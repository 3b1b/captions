[
 {
  "input": "This is a video for anyone who already knows what eigenvalues and eigenvectors are, and who might enjoy a quick way to compute them in the case of 2x2 matrices. ",
  "translatedText": "ஈஜென்வேல்யூஸ் மற்றும் ஈஜென்வெக்டர்கள் என்றால் என்ன என்பதை ஏற்கனவே அறிந்தவர்கள் மற்றும் 2x2 மெட்ரிக்குகளில் அவற்றைக் கணக்கிடுவதற்கான விரைவான வழியை அனுபவிக்கக்கூடிய எவருக்கும் இது ஒரு வீடியோ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 7.56
 },
 {
  "input": "If you’re unfamiliar with eigenvalues, take a look at this video which introduces them. ",
  "translatedText": "உங்களுக்கு ஈஜென் மதிப்புகள் பற்றித் தெரியாவிட்டால், அவற்றை அறிமுகப்படுத்தும் வகையில் இருக்கும் இந்த வீடியோவை இங்கே பாருங்கள். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.58,
  "end": 13.7
 },
 {
  "input": "You can skip ahead if you just want to see the trick, but if possible I’d like you to rediscover it for yourself, so for that let’s lay down a little background. ",
  "translatedText": "தந்திரத்தைப் பார்க்க வேண்டும் என்றால், நீங்கள் அதைத் தவிர்க்கலாம், ஆனால் முடிந்தால் அதை நீங்களே மீண்டும் கண்டுபிடிக்க வேண்டும் என்று நான் விரும்புகிறேன். எனவே, ஒரு சிறிய பின்னணியை உருவாக்குவோம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.68,
  "end": 22.38
 },
 {
  "input": "As a quick reminder, if the effect of a linear transformation on a given vector is to scale it by some constant, we call it an \"eigenvector\" of the transformation, and we call the relevant scaling factor the corresponding \"eigenvalue,\" often denoted with the letter lambda. ",
  "translatedText": "ஒரு விரைவான நினைவூட்டலாக, கொடுக்கப்பட்ட திசையன் மீது நேரியல் மாற்றத்தின் விளைவு அந்த திசையனை சில மாறிலிகளால் அளவிடுவதாக இருந்தால், அதை உருமாற்றத்தின் ஈஜென்வெக்டர் என்று அழைக்கிறோம், மேலும் தொடர்புடைய அளவிடுதல் காரணியை தொடர்புடைய ஈஜென்வேல்யூ என்று அழைக்கிறோம். லாம்ப்டா. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.26,
  "end": 38.6
 },
 {
  "input": "When you write this as an equation and you rearrange a little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the matrix (A minus lambda times the identity) must send some nonzero vector, namely the corresponding eigenvector, to the zero vector, which in turn means the determinant of this modified matrix must be 0. ",
  "translatedText": "நீங்கள் இதை ஒரு சமன்பாடாக எழுதி, சிறிது சிறிதாக மறுசீரமைக்கும்போது, நீங்கள் பார்ப்பது என்னவென்றால், லாம்ப்டா எண் மேட்ரிக்ஸ் A இன் ஈஜென்மதிப்பாக இருந்தால், அணி A மைனஸ் லாம்ப்டா மடங்கு அடையாளமானது சில பூஜ்ஜியமற்ற வெக்டரை அனுப்ப வேண்டும், அதாவது தொடர்புடைய ஈஜென்வெக்டார், பூஜ்ஜிய திசையன், இதையொட்டி இந்த மாற்றியமைக்கப்பட்ட மேட்ரிக்ஸின் தீர்மானிப்பான் பூஜ்ஜியமாக இருக்க வேண்டும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.84,
  "end": 64.58
 },
 {
  "input": "Okay, that’s all a little bit of a mouthful to say, but again, I’m assuming all of this is review for anyone watching. ",
  "translatedText": "சரி, அதெல்லாம் கொஞ்சம் வாய்விட்டுச் சொல்ல வேண்டும், ஆனால் மீண்டும், உங்களில் எவருக்கும் இது விமர்சனம் என்று நான் கருதுகிறேன். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.12,
  "end": 71.54
 },
 {
  "input": "So, the usual way to compute eigenvalues, how I used to do it, and how I believe most students are taught to carry it out, is to subtract the unknown value lambda off the diagonals and then solve for when the determinant equals 0. ",
  "translatedText": "எனவே, ஈஜென் மதிப்புகளைக் கணக்கிடுவதற்கான வழக்கமான வழி, நான் அதை எப்படிச் செய்தேன் மற்றும் பெரும்பாலான மாணவர்கள் அதைச் செயல்படுத்த கற்றுக்கொடுக்கிறார்கள் என்று நான் நம்புகிறேன், மூலைவிட்டங்களில் இருந்து அறியப்படாத மதிப்பு லாம்ப்டாவைக் கழிப்பது, பின்னர் தீர்மானிப்பான் பூஜ்ஜியத்திற்கு சமமாக இருக்கும்போது அதைத் தீர்ப்பது. . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 72.82,
  "end": 85.86
 },
 {
  "input": "Doing this always involves a few steps to expand out and simplify to get a clean quadratic polynomial, what's known as the “characteristic polynomial” of the matrix. ",
  "translatedText": "இதைச் செய்வது, மேட்ரிக்ஸின் சிறப்பியல்பு பல்லுறுப்புக்கோவை என அறியப்படும் சுத்தமான இருபடிப் பல்லுறுப்புக்கோவையைப் பெறுவதற்கு விரிவடைவதற்கும் எளிமைப்படுத்துவதற்கும் சில கூடுதல் படிகளை உள்ளடக்கியது. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.76,
  "end": 96.46
 },
 {
  "input": "The eigenvalues are the roots of this polynomial. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 97.36,
  "end": 99.9
 },
 {
  "input": "So to find them you have to apply the quadratic formula, which itself typically requires one or two more steps of simplification. ",
  "translatedText": "eigenvalues இந்த பல்லுறுப்புக்கோவையின் வேர்கள், எனவே அவற்றைக் கண்டுபிடிக்க நீங்கள் இருபடி சூத்திரத்தைப் பயன்படுத்த வேண்டும், இது பொதுவாக ஒன்று அல்லது இரண்டு படிகள் எளிமைப்படுத்தப்பட வேண்டும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.1,
  "end": 106.54
 },
 {
  "input": "Honestly, the process isn’t terrible. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 107.76,
  "end": 109.5
 },
 {
  "input": "But at least for 2x2 matrices, there’s a much more direct way to get at this answer. ",
  "translatedText": "நேர்மையாக, செயல்முறை பயங்கரமானது அல்ல, ஆனால் குறைந்தபட்சம் 2x2 மெட்ரிக்குகளுக்கு, நீங்கள் பதிலைப் பெறக்கூடிய நேரடியான வழி உள்ளது. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.58,
  "end": 114.68
 },
 {
  "input": "And if you want to rediscover this trick, there are only three relevant facts you need to know, each of which is worth knowing in its own right and can help you with other problem-solving. ",
  "translatedText": "இந்த தந்திரத்தை நீங்கள் மீண்டும் கண்டுபிடிக்க விரும்பினால், நீங்கள் தெரிந்து கொள்ள வேண்டிய மூன்று தொடர்புடைய உண்மைகள் மட்டுமே உள்ளன, அவை ஒவ்வொன்றும் அதன் சொந்த உரிமையில் தெரிந்து கொள்ள வேண்டியவை மற்றும் பிற சிக்கலைத் தீர்ப்பதில் உங்களுக்கு உதவும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 115.4,
  "end": 122.9
 },
 {
  "input": "Number 1: The trace of a matrix, which is the sum of these two diagonal entries, is equal to the sum of the eigenvalues. ",
  "translatedText": "எண் ஒன்று, இந்த இரண்டு மூலைவிட்ட உள்ளீடுகளின் கூட்டுத்தொகையான மேட்ரிக்ஸின் சுவடு, ஈஜென் மதிப்புகளின் கூட்டுத்தொகைக்கு சமம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 123.82,
  "end": 130.92
 },
 {
  "input": "Or another way to phrase it, more useful for our purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal entries. ",
  "translatedText": "அல்லது அதை சொற்றொடர் செய்வதற்கான மற்றொரு வழி, எங்கள் நோக்கங்களுக்கு மிகவும் பயனுள்ளதாக இருக்கும், இரண்டு ஈஜென் மதிப்புகளின் சராசரியும் இந்த இரண்டு மூலைவிட்ட உள்ளீடுகளின் சராசரியும் ஒன்றுதான். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.7,
  "end": 139.46
 },
 {
  "input": "Number 2: The determinant of a matrix, our usual ad-bc formula, is equal to the product of the two eigenvalues. ",
  "translatedText": "எண் இரண்டு, ஒரு மேட்ரிக்ஸின் தீர்மானிப்பான், எங்களின் வழக்கமான ad-bc சூத்திரம், இரண்டு ஈஜென் மதிப்புகளின் பெருக்கத்திற்குச் சமம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.0,
  "end": 148.96
 },
 {
  "input": "And this should kind of make sense if you understand that eigenvalues describe how much an operator stretches space in a particular direction and that the determinant describes how much an operator scales areas (or volumes) as a whole. ",
  "translatedText": "ஒரு குறிப்பிட்ட திசையில் ஒரு ஆபரேட்டர் எவ்வளவு இடத்தை நீட்டிக்கிறார் என்பதை ஈஜென்வேல்யூஸ் விவரிக்கிறது என்பதையும், ஒரு ஆபரேட்டர் பகுதிகள் அல்லது தொகுதிகளை ஒட்டுமொத்தமாக எவ்வளவு அளவிடுகிறது என்பதையும் தீர்மானிப்பவர் விவரிக்கிறார் என்பதை நீங்கள் புரிந்து கொண்டால் இது அர்த்தமுள்ளதாக இருக்க வேண்டும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.06,
  "end": 161.76
 },
 {
  "input": "Now before getting to the third fact, notice how you can essentially read these first two values out of the matrix without really writing much down. ",
  "translatedText": "இப்போது மூன்றாவது உண்மையைப் பெறுவதற்கு முன், இந்த முதல் இரண்டு மதிப்புகளை மேட்ரிக்ஸிலிருந்து எப்படி அதிகம் எழுதாமல் படிக்கலாம் என்பதைக் கவனியுங்கள். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 162.8,
  "end": 169.16
 },
 {
  "input": "Take this matrix here as an example. ",
  "translatedText": "இந்த மேட்ரிக்ஸை இங்கே உதாரணமாக எடுத்துக் கொள்ளுங்கள். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.76,
  "end": 171.32
 },
 {
  "input": "Straight away you can know that the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7. ",
  "translatedText": "நேராக, eigenvalues இன் சராசரி 8 மற்றும் 6 இன் சராசரி, 7 ஆகும் என்பதை நீங்கள் அறிந்து கொள்ளலாம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 171.82,
  "end": 177.82
 },
 {
  "input": "Likewise, most linear algebra students are pretty well-practiced at finding the determinant, which in this case works out to be 48 - 8 So right away you know that the product of our two eigenvalues is 40. ",
  "translatedText": "அதேபோல், பெரும்பாலான நேரியல் இயற்கணிதம் மாணவர்கள் தீர்மானிப்பதைக் கண்டுபிடிப்பதில் நன்கு பயிற்சி பெற்றுள்ளனர், இந்த விஷயத்தில் இது 48 மைனஸ் 8 ஆக இருக்கும். அப்போதே, இரண்டு ஈஜென்வேல்யூக்களின் பலன் 40 என்பதை நீங்கள் அறிவீர்கள். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.58,
  "end": 191.7
 },
 {
  "input": "Now take a moment to see how you can derive what will be our third relevant fact, which is how to recover two numbers when you know their mean and you know their product. ",
  "translatedText": "இப்போது எங்களின் மூன்றாவது பொருத்தமான உண்மை என்ன என்பதை உங்களால் பெற முடியுமா என்பதைப் பார்க்க சிறிது நேரம் ஒதுக்குங்கள், அதாவது இரண்டு எண்களின் சராசரியை நீங்கள் அறிந்ததும் அவற்றின் தயாரிப்பு உங்களுக்குத் தெரிந்ததும் அவற்றை விரைவாக மீட்டெடுக்கலாம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.78,
  "end": 201.56
 },
 {
  "input": "Here, let's focus on this example. ",
  "translatedText": "இங்கே, இந்த எடுத்துக்காட்டில் கவனம் செலுத்துவோம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.46,
  "end": 203.72
 },
 {
  "input": "You know the two values are evenly spaced around 7, so they look like 7 plus or minus something; let’s call that something \"d\" for distance. ",
  "translatedText": "7 என்ற எண்ணைச் சுற்றி இரண்டு மதிப்புகளும் சம இடைவெளியில் இருப்பது உங்களுக்குத் தெரியும், எனவே அவை 7 கூட்டல் அல்லது கழித்தல் போல இருக்கும், அதை தூரத்திற்கு d என்று அழைப்போம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 204.2,
  "end": 212.78
 },
 {
  "input": "You also know that the product of these two numbers is 40. ",
  "translatedText": "இந்த இரண்டு எண்களின் பலன் 40 என்பதும் உங்களுக்குத் தெரியும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.56,
  "end": 216.38
 },
 {
  "input": "Now to find d, notice that this product expands really nicely, it works out as a difference of squares. ",
  "translatedText": "இப்போது d ஐக் கண்டுபிடிக்க, இந்த தயாரிப்பு மிகவும் நன்றாக விரிவடைவதைக் கவனியுங்கள், இது சதுரங்களின் வித்தியாசமாக வேலை செய்கிறது. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.6,
  "end": 223.7
 },
 {
  "input": "So from there, you can directly find d: d^2 is 7^2 - 40, or 9, which means d itself is 3. ",
  "translatedText": "எனவே அங்கிருந்து, நீங்கள் நேரடியாக டி கண்டுபிடிக்கலாம். d ஸ்கொயர் என்பது 7 ஸ்கொயர் மைனஸ் 40 அல்லது 9, அதாவது d தானே 3 ஆகும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.56,
  "end": 233.4
 },
 {
  "input": "In other words, the two values for this very specific example work out to be 4 and 10. ",
  "translatedText": "வேறு வார்த்தைகளில் கூறுவதானால், இந்த குறிப்பிட்ட உதாரணத்திற்கான இரண்டு மதிப்புகள் 4 மற்றும் 10 ஆக செயல்படுகின்றன. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.38,
  "end": 241.1
 },
 {
  "input": "But our goal is a quick trick, and you wouldn’t want to think this through each time, so let’s wrap up what we just did in a general formula. ",
  "translatedText": "ஆனால் எங்களின் இலக்கு விரைவான தந்திரம், ஒவ்வொரு முறையும் இதைப் பற்றி நீங்கள் சிந்திக்க விரும்ப மாட்டீர்கள், எனவே நாங்கள் செய்ததை ஒரு பொதுவான சூத்திரத்தில் சுருக்கவும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 241.68,
  "end": 248.12
 },
 {
  "input": "For any mean, m and product, p, the distance squared is always going to be m^2 - p. ",
  "translatedText": "எந்த சராசரி m மற்றும் தயாரிப்பு p க்கும், சதுர தூரம் எப்போதும் m ஸ்கொயர் மைனஸ் p ஆக இருக்கும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.64,
  "end": 255.68
 },
 {
  "input": "This gives the third key fact, which is that when two numbers have a mean m and a product p, you can write those two numbers as m ± sqrt(m^2 - p) This is decently fast to rederive on the fly if you ever forget it, and it’s essentially just a rephrasing of the difference of squares formula. ",
  "translatedText": "இது மூன்றாவது முக்கிய உண்மையைத் தருகிறது, அதாவது இரண்டு எண்களுக்கு சராசரி m மற்றும் ஒரு தயாரிப்பு p இருந்தால், அந்த இரண்டு எண்களையும் m க்ளஸ் அல்லது m ஸ்கொயர்ட் மைனஸ் p இன் வர்க்க மூலத்தைக் கழிக்கலாம். நீங்கள் எப்போதாவது அதை மறந்துவிட்டால், பறக்கும்போது மீண்டும் பெறுவதற்கு இது மிகவும் விரைவானது, மேலும் இது அடிப்படையில் சதுர சூத்திரத்தின் வேறுபாட்டை மறுவடிவமைப்பதாகும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 257.56,
  "end": 277.08
 },
 {
  "input": "But even still it’s a fact worth memorizing so that you have it at the tip of your fingers. ",
  "translatedText": "ஆனால் இன்னும், இது உங்கள் விரல் நுனியில் இருப்பதால் மனப்பாடம் செய்ய வேண்டிய உண்மை. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.86,
  "end": 281.22
 },
 {
  "input": "In fact, my friend Tim from the channel acapellascience wrote us a quick jingle to make it a little more memorable. ",
  "translatedText": "உண்மையில், A Capella Science சேனலைச் சேர்ந்த எனது நண்பர் டிம், அதை இன்னும் கொஞ்சம் மறக்கமுடியாத வகையில் எங்களுக்கு ஒரு நல்ல விரைவான ஜிங்கிள் எழுதினார். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.22,
  "end": 287.16
 },
 {
  "input": "m plus or minus squaaaare root of me squared minus p (ping!) Let me show you how this works, say for the matrix [[3,1], [4,1]]. ",
  "translatedText": "மேட்ரிக்ஸ் 3, 1, 4, 1 க்கு இது எப்படி வேலை செய்கிறது என்பதைக் காட்டுகிறேன். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.9,
  "end": 297.62
 },
 {
  "input": "You start by bringing to mind the formula, maybe stating it all in your head. ",
  "translatedText": "நீங்கள் சூத்திரத்தை மனதில் கொண்டு தொடங்குகிறீர்கள், ஒருவேளை அதை உங்கள் தலையில் கூறலாம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 298.1,
  "end": 301.82
 },
 {
  "input": "But when you write it down, you fill in the appropriate values of m and p as you go. ",
  "translatedText": "ஆனால் நீங்கள் அதை எழுதும்போது, நீங்கள் செல்லும்போது m மற்றும் p க்கு பொருத்தமான மதிப்புகளை நிரப்புகிறீர்கள். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.2,
  "end": 311.62
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 312.34,
  "end": 317.74
 },
 {
  "input": "So the thing you start writing is 2 ± sqrt(2^2 - …). ",
  "translatedText": "எனவே இந்த எடுத்துக்காட்டில், eigenvalues இன் சராசரியானது 3 மற்றும் 1 இன் சராசரி, அதாவது 2 ஆகும், எனவே நீங்கள் எழுதத் தொடங்கும் விஷயம் 2 க்ளோஸ் அல்லது மைனஸ் 2 ஸ்கொயர் மைனஸின் வர்க்க மூலத்தை, பிறகு ஈஜென் மதிப்புகளின் பலன் ஆகும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 318.3,
  "end": 322.7
 },
 {
  "input": "Then the product of the eigenvalues is the determinant, which in this example is 3*1 - 1*4, or -1. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.54,
  "end": 332.14
 },
 {
  "input": "So that’s the final thing you fill in. ",
  "translatedText": "இந்த எடுத்துக்காட்டில் 3 பெருக்கல் 1 கழித்தல் 1 பெருக்கல் 4 அல்லது எதிர்மறை 1 என்பது நிர்ணயம் ஆகும், எனவே நீங்கள் நிரப்பும் இறுதி விஷயம் இதுதான், அதாவது ஈஜென் மதிப்புகள் 2 கூட்டல் அல்லது 5 இன் வர்க்க மூலத்தைக் கழித்தல். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 332.38,
  "end": 334.48
 },
 {
  "input": "This means the eigenvalues are 2±sqrt(5). ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.88,
  "end": 338.76
 },
 {
  "input": "You might recognize that this is the same matrix I was using at the beginning, but notice how much more directly we can get at the answer. ",
  "translatedText": "நான் ஆரம்பத்தில் பயன்படுத்திய அதே அணி இது என்பதை நீங்கள் அடையாளம் காணலாம், ஆனால் பதில் எவ்வளவு நேரடியாகப் பெற முடியும் என்பதைக் கவனியுங்கள். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.3,
  "end": 346.5
 },
 {
  "input": "Here, try another one. ",
  "translatedText": "இதோ, இன்னொன்றை முயற்சிக்கவும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.14,
  "end": 349.18
 },
 {
  "input": "This time the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5. ",
  "translatedText": "இந்த நேரத்தில், ஈஜென் மதிப்புகளின் சராசரி 2 மற்றும் 8 இன் சராசரி, அதாவது 5 ஆகும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 349.44,
  "end": 354.48
 },
 {
  "input": "So again, you start writing out the formula but this time writing 5 in place of m [song]. ",
  "translatedText": "எனவே மீண்டும், நீங்கள் சூத்திரத்தை எழுதத் தொடங்குகிறீர்கள், ஆனால் இந்த முறை m க்கு பதிலாக 5 ஐ எழுதுங்கள், பின்னர் தீர்மானிப்பான் 2 பெருக்கல் 8 கழித்தல் 7 பெருக்கல் 1 அல்லது 9 ஆகும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.1,
  "end": 359.22
 },
 {
  "input": "And then the determinant is 2*8 - 7*1, or 9. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.98,
  "end": 368.3
 },
 {
  "input": "So in this example, the eigenvalues look like 5 ± sqrt(16), which simplifies even further as 9 and 1. ",
  "translatedText": "எனவே இந்த எடுத்துக்காட்டில், eigenvalues 16 இன் வர்க்க மூலத்தை 5 கூட்டல் அல்லது கழித்தல் போல் இருக்கும், இது 9 மற்றும் 1 என மேலும் எளிதாக்குகிறது. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 369.52,
  "end": 378.24
 },
 {
  "input": "You see what I mean about how you can basically just start writing down the eigenvalues while staring at the matrix? ",
  "translatedText": "நீங்கள் மேட்ரிக்ஸைப் பார்த்துக்கொண்டிருக்கும்போது, அடிப்படை மதிப்புகளை எப்படி எழுதத் தொடங்கலாம் என்பதைப் பற்றி நான் என்ன சொல்கிறேன் என்று பார்க்கிறீர்களா? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 379.42,
  "end": 384.62
 },
 {
  "input": "It’s typically just the tiniest bit of simplifying at the end. ",
  "translatedText": "இது பொதுவாக முடிவில் மிகச்சிறிய எளிமைப்படுத்தல். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.28,
  "end": 388.16
 },
 {
  "input": "Honestly, I’ve found myself using this trick a lot when I’m sketching quick notes related to linear algebra and want to use small matrices as examples. ",
  "translatedText": "நேர்மையாக, நான் நேரியல் இயற்கணிதம் தொடர்பான விரைவான குறிப்புகளை வரையும்போதும், சிறிய மெட்ரிக்குகளை எடுத்துக்காட்டுகளாகப் பயன்படுத்த விரும்பும்போதும் இந்த தந்திரத்தை அதிகம் பயன்படுத்தினேன். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 389.06,
  "end": 395.72
 },
 {
  "input": "I’ve been working on a video about matrix exponents, where eigenvalues pop up a lot, and I realized it’s just very handy if students can read off the eigenvalues from small examples without losing the main line of thought by getting bogged down in a different calculation. ",
  "translatedText": "மேட்ரிக்ஸ் எக்ஸ்போனென்ட்களைப் பற்றிய வீடியோவில் நான் வேலை செய்து வருகிறேன், அங்கு ஈஜென்வேல்யூக்கள் அதிகமாக தோன்றும், மேலும் மாணவர்கள் வித்தியாசமான சிந்தனையில் மூழ்கி முக்கிய சிந்தனையை இழக்காமல் சிறிய எடுத்துக்காட்டுகளிலிருந்து ஈஜென் மதிப்புகளை படிக்க முடிந்தால் அது மிகவும் எளிது என்பதை உணர்ந்தேன். கணக்கீடு. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.18,
  "end": 408.62
 },
 {
  "input": "As another fun example, take a look at this set of three different matrices, which come up a lot in quantum mechanics, they're known as the Pauli spin matrices. ",
  "translatedText": "மற்றொரு வேடிக்கையான உதாரணமாக, குவாண்டம் இயக்கவியலில் நிறைய வரும் இந்த மூன்று வெவ்வேறு மெட்ரிக்குகளின் தொகுப்பைப் பாருங்கள். அவை பாலி ஸ்பின் மெட்ரிக்குகள் என்று அழைக்கப்படுகின்றன. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.74,
  "end": 417.52
 },
 {
  "input": "If you know quantum mechanics, you’ll know that the eigenvalues of matrices are highly relevant to the physics they describe, and if you don’t know quantum mechanics, let this just be a little glimpse of how these computations are actually relevant to real applications. ",
  "translatedText": "குவாண்டம் இயக்கவியல் உங்களுக்குத் தெரிந்திருந்தால், மெட்ரிக்குகளின் ஈஜென் மதிப்புகள் அவை விவரிக்கும் இயற்பியலுக்கு மிகவும் பொருத்தமானவை என்பதை நீங்கள் அறிவீர்கள். குவாண்டம் இயக்கவியல் உங்களுக்குத் தெரியாவிட்டால், இந்த கணக்கீடுகள் உண்மையில் உண்மையான பயன்பாடுகளுக்கு எவ்வாறு மிகவும் பொருத்தமானவை என்பதைப் பற்றிய ஒரு சிறிய பார்வையாக இருக்கட்டும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.6,
  "end": 431.22
 },
 {
  "input": "The mean of the diagonal in all three cases is 0, so the mean of the eigenvalues in all cases is 0, which makes our formula look especially simple. ",
  "translatedText": "மூன்று நிகழ்வுகளிலும் மூலைவிட்ட உள்ளீடுகளின் சராசரி பூஜ்ஜியமாகும். எனவே இந்த எல்லா நிகழ்வுகளிலும் ஈஜென் மதிப்புகளின் சராசரி பூஜ்ஜியமாகும், இது எங்கள் சூத்திரத்தை குறிப்பாக எளிதாக்குகிறது. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.54,
  "end": 443.06
 },
 {
  "input": "What about the products of the eigenvalues, the determinants of these matrices? ",
  "translatedText": "இந்த மெட்ரிக்குகளின் நிர்ணயம் செய்யும் ஈஜென்வேல்யூஸ் தயாரிப்புகள் பற்றி என்ன? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.38,
  "end": 448.8
 },
 {
  "input": "For the first one, it’s 0 - 1 or -1. ",
  "translatedText": "முதல்வருக்கு, இது 0 கழித்தல் 1 அல்லது எதிர்மறை 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.7,
  "end": 453.4
 },
 {
  "input": "The second also looks like 0 - 1, but it takes a moment more to see because of the complex numbers. ",
  "translatedText": "இரண்டாவது 0 மைனஸ் 1 போல் தெரிகிறது, ஆனால் கலப்பு எண்கள் இருப்பதால் பார்க்க இன்னும் சிறிது நேரம் ஆகும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.4,
  "end": 458.2
 },
 {
  "input": "And the final one looks like -1 - 0. ",
  "translatedText": "மேலும் இறுதியானது எதிர்மறை 1 கழித்தல் 0 போல் தெரிகிறது. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.84,
  "end": 461.36
 },
 {
  "input": "So in all cases, the eigenvalues simplify to be ±1. ",
  "translatedText": "எனவே எல்லா சந்தர்ப்பங்களிலும், ஈஜென் மதிப்புகள் கூட்டல் மற்றும் கழித்தல் 1 ஆக எளிமைப்படுத்தப்படுகின்றன. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.06,
  "end": 465.92
 },
 {
  "input": "Although in this case, you really don’t need the formula to find two values if you know theyr'e evenly spaced around 0 and their product is -1. ",
  "translatedText": "இந்த விஷயத்தில், இரண்டு மதிப்புகள் 0-ஐச் சுற்றி சம இடைவெளியில் உள்ளன மற்றும் அவற்றின் தயாரிப்பு எதிர்மறை 1 என்று உங்களுக்குத் தெரிந்தால், அவற்றைக் கண்டுபிடிக்க உங்களுக்கு சூத்திரம் தேவையில்லை. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.72,
  "end": 473.28
 },
 {
  "input": "If you’re curious, in the context of quantum mechanics, these matrices describe observations you might make about a particle's spin in the x, y or z directions. ",
  "translatedText": "நீங்கள் ஆர்வமாக இருந்தால், குவாண்டம் இயக்கவியலின் பின்னணியில், x, y அல்லது z திசையில் ஒரு துகள் சுழல்வதைப் பற்றி நீங்கள் செய்யக்கூடிய அவதானிப்புகளை இந்த மெட்ரிக்குகள் விவரிக்கின்றன. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.64,
  "end": 483.76
 },
 {
  "input": "The fact that their eigenvalues are ±1 corresponds with the idea that the values for the spin that you would observe would be either entirely in one direction or entirely in another, as opposed to something continuously ranging in between. ",
  "translatedText": "அவற்றின் ஈஜென் மதிப்புகள் கூட்டல் மற்றும் கழித்தல் 1 என்பது நீங்கள் கவனிக்கும் சுழலுக்கான மதிப்புகள் முற்றிலும் ஒரு திசையில் அல்லது முற்றிலும் வேறொரு திசையில் இருக்கும் என்ற எண்ணத்துடன் ஒத்துப்போகிறது. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.76,
  "end": 497.02
 },
 {
  "input": "Maybe you’d wonder how exactly this works, or why you would use 2x2 matrices that have complex numbers to describe spin in three dimensions. ",
  "translatedText": "இது எப்படி சரியாக வேலை செய்கிறது அல்லது முப்பரிமாணத்தில் சுழல்வதை விவரிக்க சிக்கலான எண்களைக் கொண்ட 2x2 மெட்ரிக்குகளை ஏன் பயன்படுத்துவீர்கள் என்று நீங்கள் ஆச்சரியப்படலாம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.32,
  "end": 505.52
 },
 {
  "input": "And those would be fair questions, just outside the scope of what I want to talk about here. ",
  "translatedText": "அது நியாயமான கேள்விகளாக இருக்கும், நான் இங்கு என்ன பேச விரும்புகிறேன் என்பதன் எல்லைக்கு வெளியே. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 506.1,
  "end": 509.76
 },
 {
  "input": "You know it’s funny, I wrote this section because I wanted some case where you have 2x2 matrices that are not just toy examples or homework problems, ones where they actually come up in practice, and quantum mechanics is great for that. ",
  "translatedText": "உங்களுக்கு தெரியும், இது வேடிக்கையானது, நான் இந்த பகுதியை எழுதினேன், ஏனென்றால் உங்களிடம் 2x2 மெட்ரிக்குகள் உள்ளன, அவை பொம்மை எடுத்துக்காட்டுகள் அல்ல, அல்லது வீட்டுப்பாட சிக்கல்கள், அவை உண்மையில் நடைமுறையில் வரும் மற்றும் குவாண்டம் இயக்கவியல் அதற்கு சிறந்தது. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 510.48,
  "end": 521.7
 },
 {
  "input": "But the thing is after I made it I realized that the whole example kind of undercuts the point I’m trying to make. ",
  "translatedText": "ஆனால் விஷயம் என்னவென்றால், நான் அதை உருவாக்கிய பிறகு, முழு உதாரணமும் நான் செய்ய முயற்சிக்கும் புள்ளியை குறைக்கிறது என்பதை உணர்ந்தேன். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.7,
  "end": 528.24
 },
 {
  "input": "For these specific matrices, when you use the traditional method, the one with characteristic polynomials, it’s essentially just as fast; it might actually faster. ",
  "translatedText": "இந்த குறிப்பிட்ட மெட்ரிக்குகளுக்கு, நீங்கள் பாரம்பரிய முறையைப் பயன்படுத்தும் போது, பண்புக்கூறு பல்லுறுப்புக்கோவைகள் கொண்டவை, அது அடிப்படையில் வேகமானது. இது உண்மையில் வேகமாக இருக்கலாம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.74,
  "end": 537.64
 },
 {
  "input": "I mean, take a look a the first one: The relevant determinant directly gives you a characteristic polynomial of lambda^2 - 1, and clearly, that has roots of plus and minus 1. ",
  "translatedText": "அதாவது, முதல் ஒன்றைப் பாருங்கள். தொடர்புடைய டிடர்மினன்ட், லாம்ப்டா ஸ்கொயர் மைனஸ் ஒன்னின் சிறப்பியல்பு பல்லுறுப்புக்கோவையை நேரடியாக உங்களுக்கு வழங்குகிறது, மேலும் அது பிளஸ் மற்றும் மைனஸ் ஒன்றின் வேர்களைக் கொண்டுள்ளது. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.24,
  "end": 548.2
 },
 {
  "input": "Same answer when you do the second matrix, lambda^2 - 1. ",
  "translatedText": "இரண்டாவது மேட்ரிக்ஸ், லாம்ப்டா ஸ்கொயர் மைனஸ் ஒன் செய்யும் போது அதே பதில். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 548.84,
  "end": 551.76
 },
 {
  "input": "And as for the last matrix, forget about doing any computations, traditional or otherwise, it’s already a diagonal matrix, so those diagonal entries are the eigenvalues! ",
  "translatedText": "கடைசி மேட்ரிக்ஸைப் பொறுத்தவரை, பாரம்பரியமாகவோ அல்லது வேறுவிதமாகவோ எந்த கணக்கீடுகளையும் செய்வதை மறந்துவிடுங்கள், இது ஏற்கனவே ஒரு மூலைவிட்ட அணி, எனவே அந்த மூலைவிட்ட உள்ளீடுகள் ஈஜென் மதிப்புகள். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.88,
  "end": 562.74
 },
 {
  "input": "However, the example is not totally lost to our cause. ",
  "translatedText": "எவ்வாறாயினும், இந்த உதாரணம் எங்கள் காரணத்திற்காக முற்றிலும் இழக்கப்படவில்லை. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.3,
  "end": 566.92
 },
 {
  "input": "Where you will actually feel the speed up is in the more general case where you take a linear combination of these three matrices and then try to compute the eigenvalues. ",
  "translatedText": "இந்த மூன்று மெட்ரிக்குகளின் நேரியல் கலவையை நீங்கள் எடுத்து, பின்னர் ஈஜென் மதிப்புகளைக் கணக்கிட முயற்சிக்கும்போது, வேகமானது மிகவும் பொதுவான நிலையில் இருப்பதை நீங்கள் உண்மையில் உணருவீர்கள். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 567.38,
  "end": 576.06
 },
 {
  "input": "You might write this as a times the first one, plus b times the second, plus c times the third. ",
  "translatedText": "நீங்கள் இதை முதல் முறை, பிளஸ் b முறை இரண்டாவது, கூட்டல் c பெருக்கல் மூன்றாவது என எழுதலாம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.82,
  "end": 582.42
 },
 {
  "input": "In quantum mechanics, this would describe spin observations in a general direction of a vector with coordinates [a, b, c]. ",
  "translatedText": "குவாண்டம் இயக்கவியலில், இது a, b, c ஆயத்தொலைவுகளுடன் ஒரு திசையன் பொது திசையில் சுழல் அவதானிப்புகளை விவரிக்கும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.02,
  "end": 589.28
 },
 {
  "input": "More specifically, you should assume this vector is normalized, meaning a^2 + b^2 + c^2 = 1. ",
  "translatedText": "மேலும் குறிப்பாக, இந்த திசையன் இயல்பாக்கப்பட்டதாக நீங்கள் கருத வேண்டும், அதாவது ஒரு ஸ்கொயர் பிளஸ் பி ஸ்கொயர் மற்றும் சி ஸ்கொயர் ஒன்றுக்கு சமம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.9,
  "end": 597.7
 },
 {
  "input": "When you look at this new matrix, it’s immediate to see that the mean of the eigenvalues is still zero, and you might also enjoy pausing for a brief moment to confirm that the product of those eigenvalues is still -1, and then from there concluding what the eigenvalues must be. ",
  "translatedText": "இந்த புதிய மேட்ரிக்ஸைப் பார்க்கும்போது, ஈஜென் மதிப்புகளின் சராசரி இன்னும் பூஜ்ஜியமாக இருப்பதை உடனடியாகக் காணலாம், மேலும் அந்த ஈஜென்வேல்யூக்களின் தயாரிப்பு இன்னும் எதிர்மறையாக இருப்பதை உறுதிசெய்ய சிறிது நேரம் நிறுத்தி மகிழலாம். பின்னர் அங்கிருந்து, ஐஜென் மதிப்புகள் என்னவாக இருக்க வேண்டும் என்பதை முடிவு செய்தல். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.6,
  "end": 615.92
 },
 {
  "input": "And this time, the characteristic polynomial approach would be by comparison a lot more cumbersome, definitely harder to do in your head. ",
  "translatedText": "இந்த நேரத்தில், பண்புரீதியான பல்லுறுப்புக்கோவை அணுகுமுறை ஒப்பிடுகையில் மிகவும் சிக்கலானதாக இருக்கும், நிச்சயமாக உங்கள் தலையில் செய்ய கடினமாக இருக்கும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.22,
  "end": 623.58
 },
 {
  "input": "To be clear, using the mean-product formula is not fundamentally different from finding roots of the characteristic polynomial; I mean, it can't be, they're solving the same problem. ",
  "translatedText": "தெளிவாக இருக்க, சராசரி தயாரிப்பு சூத்திரத்தைப் பயன்படுத்துவது பண்பு பல்லுறுப்புக்கோவையின் வேர்களைக் கண்டுபிடிப்பதில் இருந்து வேறுபட்டதல்ல. அதாவது, அது முடியாது, அவர்கள் அதே பிரச்சனையை தீர்க்கிறார்கள். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 625.08,
  "end": 633.44
 },
 {
  "input": "One way to think about this, actually, is that the mean-product formula is a nice way to solve quadratic in general (and some viewers of the channel may recognize this). ",
  "translatedText": "இதைப் பற்றி சிந்திக்க ஒரு வழி, உண்மையில், சராசரி தயாரிப்பு சூத்திரம் பொதுவாக இருபடிகளை தீர்க்க ஒரு சிறந்த வழியாகும், மேலும் சேனலின் சில பார்வையாளர்கள் இதை அங்கீகரிக்கலாம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.16,
  "end": 641.66
 },
 {
  "input": "This about it: When you’re trying to find the roots of a quadratic given its coefficients, that's another situation where you know the sum of two values, and you also know their product, but you’re trying to recover the original two values. ",
  "translatedText": "யோசித்துப் பாருங்கள். நீங்கள் குணகங்களின் அடிப்படையில் ஒரு இருபடியின் வேர்களைக் கண்டறிய முயற்சிக்கும்போது, இரண்டு மதிப்புகளின் கூட்டுத்தொகை உங்களுக்குத் தெரியும், மேலும் அவற்றின் தயாரிப்பையும் நீங்கள் அறிவீர்கள், ஆனால் அசல் இரண்டு மதிப்புகளை மீட்டெடுக்க முயற்சிக்கிறீர்கள். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 642.54,
  "end": 654.1
 },
 {
  "input": "Specifically, if the polynomial is normalized so that this leading coefficient is 1, then the mean of the roots will be -½ times this linear coefficient, which is -1 times the sum of those roots. ",
  "translatedText": "குறிப்பாக, பல்லுறுப்புக்கோவை இயல்பாக்கப்பட்டால், இந்த முன்னணி குணகம் ஒன்று, வேர்களின் சராசரியானது இந்த நேரியல் குணகத்தின் பாதி மடங்கு எதிர்மறையாக இருக்கும், இது அந்த வேர்களின் கூட்டுத்தொகையை விட ஒரு மடங்கு எதிர்மறையாக இருக்கும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 655.56,
  "end": 666.88
 },
 {
  "input": "For the example on the screen that makes the mean 5. ",
  "translatedText": "திரையில் உள்ள உதாரணத்திற்கு, அது ஐந்தைக் குறிக்கும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 668.02,
  "end": 670.18
 },
 {
  "input": "And the product of the roots is even easier, it’s just the constant term no adjustments needed. ",
  "translatedText": "மற்றும் வேர்களின் தயாரிப்பு இன்னும் எளிதானது, இது நிலையான சொல், சரிசெய்தல் தேவையில்லை. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.98,
  "end": 676.52
 },
 {
  "input": "So from there, you would apply the mean product formula and that gives you the roots. ",
  "translatedText": "எனவே அங்கிருந்து, நீங்கள் சராசரி தயாரிப்பு சூத்திரத்தைப் பயன்படுத்துவீர்கள், அது உங்களுக்கு வேர்களைத் தருகிறது. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.34,
  "end": 680.9
 },
 {
  "input": "On the one hand, you could think of this as a lighter-weight version of the traditional quadratic formula. ",
  "translatedText": "ஒருபுறம், இதை பாரம்பரிய இருபடி சூத்திரத்தின் இலகுவான பதிப்பாக நீங்கள் நினைக்கலாம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.14,
  "end": 690.22
 },
 {
  "input": "But the real advantage is that it's fewer symbols to memorize, it's that each one of them carries more meaning with it. ",
  "translatedText": "ஆனால் உண்மையான நன்மை என்னவென்றால், மனப்பாடம் செய்வதற்கு குறைவான குறியீடுகள் இருப்பது மட்டுமல்ல, அவை ஒவ்வொன்றும் அதனுடன் அதிக அர்த்தத்தைக் கொண்டுள்ளன. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 690.96,
  "end": 696.44
 },
 {
  "input": "The whole point of this eigenvalue trick is that because you can read out the mean and product directly from looking at the matrix, you don't need to go through the intermediate step of setting up the characteristic polynomial. ",
  "translatedText": "அதாவது, இந்த ஈஜென்வேல்யூ தந்திரத்தின் முழு அம்சம் என்னவென்றால், மேட்ரிக்ஸைப் பார்ப்பதன் மூலம் நீங்கள் சராசரி மற்றும் தயாரிப்பை நேரடியாகப் படிக்க முடியும் என்பதால், பண்புக்கூறு பல்லுறுப்புக்கோவை அமைக்கும் இடைநிலை படிநிலையை நீங்கள் கடக்க வேண்டியதில்லை. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.94,
  "end": 708.0
 },
 {
  "input": "You can jump straight to writing down the roots without ever explicitly thinking about what the polynomial looks like. ",
  "translatedText": "பல்லுறுப்புக்கோவை எப்படி இருக்கும் என்பதைப் பற்றி வெளிப்படையாகச் சிந்திக்காமல், நீங்கள் நேரடியாக வேர்களை எழுதலாம். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.42,
  "end": 713.64
 },
 {
  "input": "But to do that we need a version of the quadratic formula where the terms carry some kind of meaning. ",
  "translatedText": "ஆனால் அதைச் செய்ய, இருபடி சூத்திரத்தின் பதிப்பு நமக்குத் தேவை, அங்கு சொற்கள் சில வகையான அர்த்தங்களைக் கொண்டுள்ளன. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 713.84,
  "end": 718.82
 },
 {
  "input": "I realize that this is a very specific trick, for a very specific audience, but it’s something I wish I knew in college, so if you happen to know any students who might benefit from this, consider sharing it with them. ",
  "translatedText": "இது மிகவும் குறிப்பிட்ட பார்வையாளர்களுக்கான மிகவும் குறிப்பிட்ட தந்திரம் என்பதை நான் உணர்கிறேன், ஆனால் இது கல்லூரியில் நான் அறிந்திருக்க வேண்டும் என்று நான் விரும்புகிறேன், எனவே இதன் மூலம் பயனடையக்கூடிய எந்த மாணவர்களையும் நீங்கள் அறிந்தால், அதை அவர்களுடன் பகிர்ந்து கொள்ளுங்கள். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 720.38,
  "end": 729.7
 },
 {
  "input": "The hope is that it’s not just one more thing to memorize, but that the framing reinforces some other nice facts worth knowing, like how the trace and determinant relate to eigenvalues. ",
  "translatedText": "நம்பிக்கை என்னவென்றால், நீங்கள் மனப்பாடம் செய்யும் மற்றொரு விஷயம் மட்டும் அல்ல, ஆனால், சுவடு மற்றும் தீர்மானிப்பவர் ஈஜென் மதிப்புகளுடன் எவ்வாறு தொடர்புடையது என்பது போன்ற தெரிந்து கொள்ள வேண்டிய வேறு சில நல்ல உண்மைகளை இந்த ஃப்ரேமிங் வலுப்படுத்துகிறது. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.28,
  "end": 739.82
 },
 {
  "input": "If you want to prove those facts, by the way, take a moment to expand out the characteristic polynomial for a general matrix, and think hard about the meaning of each of these coefficients. ",
  "translatedText": "நீங்கள் அந்த உண்மைகளை நிரூபிக்க விரும்பினால், ஒரு பொது மேட்ரிக்ஸிற்கான பண்பு பல்லுறுப்புக்கோவையை விரிவுபடுத்த சிறிது நேரம் ஒதுக்குங்கள், பின்னர் இந்த குணகங்கள் ஒவ்வொன்றின் அர்த்தத்தையும் பற்றி நன்றாக சிந்திக்கவும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 740.56,
  "end": 749.62
 },
 {
  "input": "Many thanks to Tim, for ensuring that this mean-product formula will stay stuck in all of our heads for at least a few months. ",
  "translatedText": "இந்த சராசரி தயாரிப்பு சூத்திரம் குறைந்தது சில மாதங்களுக்கு நம் அனைவரின் தலையிலும் சிக்கியிருக்கும் என்பதை உறுதி செய்ததற்காக டிம்முக்கு மிக்க நன்றி. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 752.4,
  "end": 757.94
 },
 {
  "input": "If you don’t know about acapellascience, please do check it out. ",
  "translatedText": "அல்காப்பெல்லா அறிவியல் பற்றி உங்களுக்குத் தெரியாவிட்டால், தயவுசெய்து அதைப் பார்க்கவும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 761.7,
  "end": 766.0
 },
 {
  "input": "\"The Molecular Shape of You\", in particular, is one of the greatest things on the internet. ",
  "translatedText": "குறிப்பாக உங்களின் மூலக்கூறு வடிவம் இணையத்தில் உள்ள மிகப்பெரிய விஷயங்களில் ஒன்றாகும். ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.28,
  "end": 769.58
 }
]