[
 {
  "input": "Intro 00:00:01,989 --> 00:00:06,060 3   the trick, but if possible I’d like you to rediscover it for yourself, so for that 00:00:23,349 --> 00:00:28,550 8  often denoted with the letter lambda.",
  "model": "nmt",
  "translatedText": "该视频适合任何已经了解特征值和特征向量的人，并且可 能喜欢在 2x2 矩阵的情况下快速计算它们的方法。 如果您不熟悉特征值，请继续观看此处的此视频，该视频 实际上是为了介绍它们。 如果您只想了解其中的技巧，则可以 跳过，但如果可能的话，我希望您自己重新发现它。 为此，我们 先介绍一些背景知识。 快速提醒一下，如果线性变换对给 定向量的影响是将该向量缩放某个常数，我们将其称为变 换的特征向量，并将相关缩放因子称为相应的特征值，通常 用字母表示拉姆达。 当你把它写成一个方程，并稍微重新排列一 下时，你会看到，如果数字 lambda 是矩阵 A 的特征值， 那么矩阵 A 减去 lambda 乘以恒等式必须发送一些非零向 量，即相应的特征向量到零向量，这又意味着该修改矩阵的行列式必须 为零。"
 },
 {
  "input": "When you write this as an equation and you rearrange 00:00:47,320 --> 00:00:53,190 12  of this modified matrix must be 0.",
  "model": "nmt",
  "translatedText": "好吧，说起来有点拗口，但我再次假设所有这些都是 针对你们观看的人的评论。 因此，计算特征值的常用方法，我 过去是如何做的，以及我相信大多数学生都是如何进行计算的，是 从对角线上减去未知值 lambda，然后求解行列式何时等于 0。"
 },
 {
  "input": "Okay, that’s all a little bit of a mouthful 00:01:09,960 --> 00:01:12,829 00:01:12,829 --> 00:01:17,939 17  equals 0.",
  "model": "nmt",
  "translatedText": "这样做总是需要一些额外的步骤来扩展和简化以 获得干净的二次多项式，即所谓的矩阵的特征多项式。 特征 值是该多项式的根，因此要找到它们，您必须应用二次 公式，该公式本身通常需要一两个以上的简化步骤。 老实说，这个过程并不可怕，但至少对于 2x2 矩阵，有一种更直接的 方法可以得到答案。"
 },
 {
  "input": "Doing this always involves a few steps to 00:01:32,630 --> 00:01:38,069 21  more steps of simplification.",
  "model": "nmt",
  "translatedText": "如果您想重新发现这个技巧，您只需要了解 三个相关事实，每个事实本身都值得了解，并且可以帮助您解 决其他问题。 第一，矩阵的迹（即这两个对角线项之 和）等于特征值之和。 或者对我们的目的更有用的另一种 表达方式是，两个特征值的平均值与这两个对角线条目的平均值相 同。 第二，矩阵的行列式，即我们常用的 ad-bc 公式，等于两个 特征值的乘积。"
 },
 {
  "input": "Honestly, the process isn’t terrible.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "But 00:01:52,710 --> 00:01:57,049 25   the sum of these two diagonal entries, is equal to the sum of the eigenvalues.",
  "model": "nmt",
  "translatedText": "如果您了解特征值描述了算子在特定方向上拉 伸空间的程度，并且行列式描述了算子整体上缩放面积 或体积的程度，那么这应该是有意义的。 现在，在讨论第三 个事实之前，请注意如何从矩阵中读取前两个值，而无需真正 写下太多内容。 以这里的这个矩阵为例。 马上就可以知道特征 值的均值与8和6的均值相同，都是7。 同样，大多数线性 代数学生在求行列式方面都训练有素，在本例中，行列式的计算结果为 48 减 8。"
 },
 {
  "input": "Or another 00:02:14,700 --> 00:02:18,959 30 31  of make sense if you understand that eigenvalues describe how much an operator stretches space 00:02:37,720 --> 00:02:42,189 35  much down.",
  "model": "nmt",
  "translatedText": "所以您马上就知道两个特征值的乘积是 40。 现在花点时间看看你是否可以推导出我们的第三个相关事实，即当你知道 两个数字的平均值并且知道它们的乘积时，如何快速恢复它们。 在这里 ，我们重点关注这个例子。 您知道这两个值在数字 7 周围均匀分布，因此 它们看起来像 7 加上或减去某个值，我们将其称为距离 d。 您还知道这两个数字的乘积是 40。 现在要求 d，请注意该乘积展开得非常好，它的计算结果是平方 差。 所以从那里，你可以直接找到d。 d 的平方是 7 的平方减 40，即 9，这意味着 d 本身是 3。 换句话说，这个非常具体的示例的两个值分别为 4 和 1 0。"
 },
 {
  "input": "Take this matrix here as an example.",
  "model": "nmt",
  "translatedText": "但我们的目标是一个快速技巧，您不会想每次都考虑这个问题，所以让我 们用一个通用公式来总结我们刚刚所做的事情。"
 },
 {
  "input": "Straight away you can know that the mean of 00:02:55,970 --> 00:03:02,420 39  is 40.",
  "model": "nmt",
  "translatedText": "对于任何平均值 m 和乘积 p，距离的平方始终为 m 平方减去 p。 这就给出了第三个关键事实，即当两个数字具有均值 m 和乘积 p 时， 您可以将这两个数字写为 m 加上或减去 m 平方减去 p 的平方根。"
 },
 {
  "input": "Now take a moment to see how you can derive 00:03:17,079 --> 00:03:21,860 43  let’s call that something \"d\" for distance.",
  "model": "nmt",
  "translatedText": "如果您忘记了，那么可以快速重新推导它 ，而且它本质上只是平方差公式的改写。 但即便如此，这是一个值得记住的事实，因此它就在您的指尖。 事实上，我来自 A Capella Science 频道的朋友 Tim 给我们写了一首简 短的歌曲，让它更令人难忘。 让我向您展示它是如何工作的，例如矩阵 3, 1, 4, 1。 你首先要想起这个公式，也许在你的脑海里把它全部陈述出来。"
 },
 {
  "input": "You also know that the product of these two 00:03:40,239 --> 00:03:45,480 47  specific example work out to be 4 and 10.",
  "model": "nmt",
  "translatedText": "但是当您写下来时，您可以随时填写 m 和 p 的适当值。 所以在这个例子中，特征值的平均值与 3 和 1 的平均值 相同，即 2，所以你开始写的是 2 加或减 2 平方减 的平方根，然后是特征值的乘积是行列式，在本例中为 3 乘 以 1 减 1 乘以 4，或负 1，因此这是您填写的最 后一个值，这意味着特征值是 2 加或减 5 的平方根。"
 },
 {
  "input": "But our goal is a quick trick, and you wouldn’t 00:04:06,239 --> 00:04:12,610 51  you can write those two numbers as m ± sqrt(m^2 - p) This is decently fast to rederive on the fly 00:04:34,490 --> 00:04:39,360 56  wrote us a quick jingle to make it a little more memorable.",
  "model": "nmt",
  "translatedText": "您可能会意识到这与我一开始使用的矩阵相 同，但请注意我们可以更直接地获得答案。 在这里，尝试另一种。 这次，特征值的平均值与 2 和 8 的平均 值相同，即 5。 再次，你开始写出公式，但这次用 5 代替 m，然后行列式是 2 乘以 8 减去 7 乘以 1，即 9。 因此，在本例中，特征值看起来像 5 加或减 16 的平方根， 进一步简化为 9 和 1。 你明白我的意思了，当你盯 着矩阵的时候，你基本上可以开始写下特征值吗？ 这通常只是最后的一点点简化。 老实说，当我绘制与线性代数相关的快速笔记并想使用小矩阵作为 示例时，我发现自己经常使用这个技巧。"
 },
 {
  "input": "m plus or minus squaaaare root of me squared 00:04:53,880 --> 00:04:59,069 61  p as you go.",
  "model": "nmt",
  "translatedText": "我一直在制作一个关于矩 阵指数的视频，其中特征值出现了很多，我意识到，如果学生能 够从小例子中读出特征值，而不会因为陷入不同的困境而失去主 线，那就非常方便了计算。 作为另一个有趣的例子，看一下这 组三个不同的矩阵，它在量子力学中经常出现。 它们被称为泡利 自旋矩阵。"
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean 00:05:19,030 --> 00:05:26,780 65  might recognize that this is the same matrix I was using at the beginning, but notice how 00:05:47,370 --> 00:05:52,190 69  is 2*8 - 7*1, or 9.",
  "model": "nmt",
  "translatedText": "如果您了解量子力学，您就会知道矩阵的特征值 与其描述的物理高度相关。 如果您不了解量子力学，请让我 们稍微了解一下这些计算实际上如何与实际应用非常相关。 所有三种情况下对角线条目的平均值均为零。 因此，所有这些情况下特征值的平均值为零，这使得我们的公式看起来 特别简单。 特征值的乘积（这些矩阵的行列式）又如何呢？ 对于第一个，它是 0 减 1，或负 1。 第二个看起来也像 0 减 1，但由于是复数，需要更多时间才能看到。 最后一个看起来像负 1 减 0。 因此在所有情况下，特征值都简化为正负 1。 尽管在本例中，如果您知道两个值在 0 周围均匀分布并且它们的乘积为 负 1，则实际上不需要公式来查找这两个值。"
 },
 {
  "input": "So in this example, the eigenvalues look like 5 ± sqrt(16), which 00:06:19,590 --> 00:06:23,800 73  a lot when I’m sketching quick notes related to linear algebra and want to use small matrices 00:06:37,039 --> 00:06:41,800 77  a different calculation.",
  "model": "nmt",
  "translatedText": "如果您好奇，在量子力学 的背景下，这些矩阵描述了您可能对粒子在 x、y 或 z 方向 上的自旋进行的观察。 它们的特征值是正负 1 的事实与这 样的想法相对应，即您观察到的自旋值要么完全在一个方向 上，要么完全在另一个方向上，而不是在两者之间连续变 化。 也许您想知道它到底是如何工作的，或者为什么要使用具有复数的 2x2 矩阵来描述三维空间中的自旋。 这些都是公平的问题， 超出了我想在这里讨论的范围。 你知道，这很有趣，我写这一 部分是因为我想要一些 2x2 矩阵的情况，这些矩阵不仅仅是玩具示例 或家庭作业问题，而是它们在实践中实际出现的问题，而量子力学非常适 合这样做。 但问题是，在我完成之后，我意识到整个示例有点 削弱了我想要表达的观点。"
 },
 {
  "input": "As another fun example, take a look at this 00:06:54,190 --> 00:06:59,419 81  mechanics, let this just be a little glimpse of how these computations are actually relevant 00:07:12,639 --> 00:07:19,990 85  the determinants of these matrices?",
  "model": "nmt",
  "translatedText": "对于这些特定的矩阵，当您使用 传统方法（具有特征多项式的方法）时，它本质上同样快。 实际 上可能会更快。 我的意思是，看看第一个。 相关的行列式直接给出 了 lambda 平方减一的特征多项式，并且显然它有正 负一的根。 当你做第二个矩阵时，答案是相同的，即 lambda 平方减一。 至于最后一个矩阵，忘记做任何计算，无论是传统的还是其他的 ，它已经是一个对角矩阵，所以那些对角项就是特征值。 然而，我 们的事业并没有完全失去这个例子。 您实际上会感觉到加速是在更 一般的情况下，您采用这三个矩阵的线性组合，然后尝试 计算特征值。 您可以将其写为 a 乘以第一个，加上 b 乘以第二个，再加上 c 乘以第三个。"
 },
 {
  "input": "For the first one, it’s 0 - 1 or -1.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "The second 00:07:36,430 --> 00:07:42,310 89  know theyr'e evenly spaced around 0 and their product is -1.",
  "model": "nmt",
  "translatedText": "在量子力学中，这将描述坐标为 a、b、c 的 向量的一般方向上的自旋观测。 更具体地说，您应该假设该向量已标准 化，这意味着 a 的平方加上 b 的平方加上 c 的平方等于 1。 当您查看这个新矩阵时，您会立即看到特征值的平均值仍 然为零，并且您可能还喜欢暂停片刻以确认这些特征值的 乘积仍然为负数。 然后从那里得出特征值一定是什么的结论。"
 },
 {
  "input": "If you’re curious, in the context of quantum 00:07:59,759 --> 00:08:04,470 94  or entirely in another, as opposed to something continuously ranging in between.",
  "model": "nmt",
  "translatedText": "而这一次，特征多项式方法相比之下会麻烦得多，在你 的头脑中肯定更难做到。 需要明确的是，使用平均乘积公式 与求特征多项式的根没有什么不同。 我的意思是，这不可能，他们正在解 决同样的问题。 实际上，思考这个问题的一种方法是，平均乘积公式是解 决一般二次方程的好方法，该频道的一些观众可能会认识到这一点。 想一想。"
 },
 {
  "input": "Maybe you’d 00:08:20,680 --> 00:08:26,259 98  because I wanted some case where you have 2x2 matrices that are not just toy examples 00:08:38,970 --> 00:08:43,880 102  use the traditional method, the one with characteristic polynomials, it’s essentially just as fast; 00:08:58,700 --> 00:09:03,390 106  for the last matrix, forget about doing any computations, traditional or otherwise, it’s 00:09:20,940 --> 00:09:23,510 00:09:23,510 --> 00:09:28,850 111   one, plus b times the second, plus c times the third.",
  "model": "nmt",
  "translatedText": "当您尝试在给定系数的情况下找到二次方的根时，这是 另一种情况，您知道两个值的总和，并且您也知道它们的乘积 ，但您正在尝试恢复原始的两个值。 具体来说，如果对多项 式进行归一化，使得该前导系数为 1，则根的平均值将 是该线性系数的负二分之一，即这些根之和的负一倍。 对于屏幕上的示例，这意味着平均值为 5。 而根的乘积就更容易了 ，只是常数项，不需要调整。 因此，从那里开始，您将应用 平均乘积公式，这将为您提供根源。 一方面，您可以将其视 为传统二次公式的轻量级版本。 但真正的优势不仅在 于它需要记住的符号更少，还在于每个符号都承载着更多的含义。 我的意思是，这个特征值技巧的要点在于，因为您可以直 接通过查看矩阵读出平均值和乘积，所以您不需要经历设置 特征多项式的中间步骤。 您可以直接跳到写下根， 而无需明确考虑多项式是什么样的。 但要做到这一 点，我们需要一个二次公式的版本，其中的项具有某种含义。 我意识到这是针对特定受众的非常特定的技巧，但我希望我在大学时就知 道这一点，所以如果您碰巧认识任何可能从中受益的学生，请考虑与他们 分享。 我们希望这不仅仅是您记住的另一件事，而且框架还 强化了其他一些值得了解的好事实，例如迹和行列式与特 征值的关系。 顺便说一句，如果您想证明这些事实，请 花点时间展开一般矩阵的特征多项式，然后认真思考每 个系数的含义。 非常感谢蒂姆，确保这个平均产品配方将在我 们所有人的脑海中停留至少几个月。"
 },
 {
  "input": "In quantum mechanics, this would 00:09:47,360 --> 00:09:53,240 116  still zero, and you might also enjoy pausing for a brief moment to confirm that the product 00:10:13,560 --> 00:10:18,710 120   is not fundamentally different from finding roots of the characteristic polynomial; I 00:10:33,110 --> 00:10:37,160 125  the roots of a quadratic given its coefficients, that's another situation where you know the 00:10:50,950 --> 00:10:54,820 129  linear coefficient, which is -1 times the sum of those roots.",
  "model": "nmt",
  "translatedText": "如果您不了解阿尔卡贝拉科 学，请务必查看一下。 尤其是你的分子形状是 互联网上最伟大的事物之一。"
 },
 {
  "input": "For the example on the 00:11:12,820 --> 00:11:17,610 133  a lighter-weight version of the traditional quadratic formula.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "But the real advantage 00:11:34,200 --> 00:11:37,370 137  you don't need to go through the intermediate step of setting up the characteristic polynomial.",
  "model": "nmt",
  "translatedText": ""
 },
 {
  "input": "00:11:49,930 --> 00:11:54,339 141  for a very specific audience, but it’s something I wish I knew in college, so if you happen 00:12:08,240 --> 00:12:10,330 00:12:10,330 --> 00:12:15,140 146  take a moment to expand out the characteristic polynomial for a general matrix, and think 00:12:32,449 --> 00:12:36,420 150  in particular, is one of the greatest things on the internet.",
  "model": "nmt",
  "translatedText": ""
 }
]