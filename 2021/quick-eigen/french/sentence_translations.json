[
 {
  "input": "This is a video for anyone who already knows what eigenvalues and eigenvectors are, and who might enjoy a quick way to compute them in the case of 2x2 matrices. ",
  "translatedText": "Il s'agit d'une vidéo destinée à tous ceux qui savent déjà ce que sont les valeurs propres et les vecteurs propres, et qui pourraient apprécier un moyen rapide de les calculer dans le cas de matrices 2x2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 7.56
 },
 {
  "input": "If you’re unfamiliar with eigenvalues, take a look at this video which introduces them. ",
  "translatedText": "Si vous n'êtes pas familier avec les valeurs propres, allez-y et jetez un œil à cette vidéo ici, qui est en fait destinée à les présenter. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.58,
  "end": 13.7
 },
 {
  "input": "You can skip ahead if you just want to see the trick, but if possible I’d like you to rediscover it for yourself, so for that let’s lay down a little background. ",
  "translatedText": "Vous pouvez avancer si tout ce que vous voulez, c'est voir l'astuce, mais si possible, j'aimerais que vous la redécouvertiez par vous-même. Alors pour cela, décrivons un peu le contexte. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.68,
  "end": 22.38
 },
 {
  "input": "As a quick reminder, if the effect of a linear transformation on a given vector is to scale it by some constant, we call it an \"eigenvector\" of the transformation, and we call the relevant scaling factor the corresponding \"eigenvalue,\" often denoted with the letter lambda. ",
  "translatedText": "Pour rappel, si l'effet d'une transformation linéaire sur un vecteur donné est de mettre à l'échelle ce vecteur par une constante, nous l'appelons vecteur propre de la transformation, et nous appelons le facteur d'échelle pertinent la valeur propre correspondante, souvent désignée par la lettre lambda. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.26,
  "end": 38.6
 },
 {
  "input": "When you write this as an equation and you rearrange a little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the matrix (A minus lambda times the identity) must send some nonzero vector, namely the corresponding eigenvector, to the zero vector, which in turn means the determinant of this modified matrix must be 0. ",
  "translatedText": "Lorsque vous écrivez cela sous forme d'équation et que vous réorganisez un peu, ce que vous voyez, c'est que si le nombre lambda est une valeur propre d'une matrice A, alors la matrice A moins lambda fois l'identité doit envoyer un vecteur non nul, à savoir le vecteur propre correspondant, au vecteur zéro, ce qui signifie que le déterminant de cette matrice modifiée doit être nul. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.84,
  "end": 64.58
 },
 {
  "input": "Okay, that’s all a little bit of a mouthful to say, but again, I’m assuming all of this is review for anyone watching. ",
  "translatedText": "D'accord, c'est un peu long à dire, mais encore une fois, je suppose que tout ceci est une révision pour tous ceux d'entre vous qui nous regardent. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.12,
  "end": 71.54
 },
 {
  "input": "So, the usual way to compute eigenvalues, how I used to do it, and how I believe most students are taught to carry it out, is to subtract the unknown value lambda off the diagonals and then solve for when the determinant equals 0. ",
  "translatedText": "Ainsi, la manière habituelle de calculer les valeurs propres, comment je le faisais et comment je pense que la plupart des étudiants apprennent à le faire, est de soustraire la valeur inconnue lambda des diagonales, puis de déterminer quand le déterminant est égal à zéro. . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 72.82,
  "end": 85.86
 },
 {
  "input": "Doing this always involves a few steps to expand out and simplify to get a clean quadratic polynomial, what's known as the “characteristic polynomial” of the matrix. ",
  "translatedText": "Faire cela implique toujours quelques étapes supplémentaires pour développer et simplifier afin d'obtenir un polynôme quadratique propre, ce qu'on appelle le polynôme caractéristique de la matrice. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.76,
  "end": 96.46
 },
 {
  "input": "The eigenvalues are the roots of this polynomial. ",
  "translatedText": "Les valeurs propres sont les racines de ce polynôme.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 97.36,
  "end": 99.9
 },
 {
  "input": "So to find them you have to apply the quadratic formula, which itself typically requires one or two more steps of simplification. ",
  "translatedText": "Donc, pour les trouver, vous devez appliquer la formule quadratique, qui elle-même nécessite généralement une ou deux étapes de simplification supplémentaires. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.1,
  "end": 106.54
 },
 {
  "input": "Honestly, the process isn’t terrible. ",
  "translatedText": "Honnêtement, le processus n'est pas terrible. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 107.76,
  "end": 109.5
 },
 {
  "input": "But at least for 2x2 matrices, there’s a much more direct way to get at this answer. ",
  "translatedText": "Mais au moins pour les matrices 2x2, il existe un moyen beaucoup plus direct d'obtenir cette réponse. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.58,
  "end": 114.68
 },
 {
  "input": "And if you want to rediscover this trick, there are only three relevant facts you need to know, each of which is worth knowing in its own right and can help you with other problem-solving. ",
  "translatedText": "Et si vous souhaitez redécouvrir cette astuce, il n'y a que trois faits pertinents que vous devez connaître, chacun d'eux mérite d'être connu en soi et peut vous aider à résoudre d'autres problèmes. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 115.4,
  "end": 122.9
 },
 {
  "input": "Number 1: The trace of a matrix, which is the sum of these two diagonal entries, is equal to the sum of the eigenvalues. ",
  "translatedText": "Premièrement, la trace d'une matrice, qui est la somme de ces deux entrées diagonales, est égale à la somme des valeurs propres. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 123.82,
  "end": 130.92
 },
 {
  "input": "Or another way to phrase it, more useful for our purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal entries. ",
  "translatedText": "Ou une autre façon de le formuler, plus utile pour nos besoins, est que la moyenne des deux valeurs propres est la même que la moyenne de ces deux entrées diagonales. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.7,
  "end": 139.46
 },
 {
  "input": "Number 2: The determinant of a matrix, our usual ad-bc formula, is equal to the product of the two eigenvalues. ",
  "translatedText": "Numéro deux, le déterminant d'une matrice, notre formule ad-bc habituelle, est égal au produit des deux valeurs propres. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.0,
  "end": 148.96
 },
 {
  "input": "And this should kind of make sense if you understand that eigenvalues describe how much an operator stretches space in a particular direction and that the determinant describes how much an operator scales areas (or volumes) as a whole. ",
  "translatedText": "Et cela devrait avoir du sens si vous comprenez que les valeurs propres décrivent dans quelle mesure un opérateur étend l'espace dans une direction particulière, et que le déterminant décrit dans quelle mesure un opérateur met à l'échelle des zones ou des volumes dans leur ensemble. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.06,
  "end": 161.76
 },
 {
  "input": "Now before getting to the third fact, notice how you can essentially read these first two values out of the matrix without really writing much down. ",
  "translatedText": "Maintenant, avant d'aborder le troisième fait, remarquez comment vous pouvez essentiellement lire ces deux premières valeurs de la matrice sans vraiment écrire grand-chose. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 162.8,
  "end": 169.16
 },
 {
  "input": "Take this matrix here as an example. ",
  "translatedText": "Prenons cette matrice ici comme exemple. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.76,
  "end": 171.32
 },
 {
  "input": "Straight away you can know that the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7. ",
  "translatedText": "Tout de suite, vous pouvez savoir que la moyenne des valeurs propres est la même que la moyenne de 8 et 6, qui est 7. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 171.82,
  "end": 177.82
 },
 {
  "input": "Likewise, most linear algebra students are pretty well-practiced at finding the determinant, which in this case works out to be 48 - 8 So right away you know that the product of our two eigenvalues is 40. ",
  "translatedText": "De même, la plupart des étudiants en algèbre linéaire sont assez bien entraînés à trouver le déterminant, qui dans ce cas équivaut à 48 moins 8. Donc tout de suite, vous savez que le produit des deux valeurs propres est 40. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.58,
  "end": 191.7
 },
 {
  "input": "Now take a moment to see how you can derive what will be our third relevant fact, which is how to recover two numbers when you know their mean and you know their product. ",
  "translatedText": "Maintenant, prenez un moment pour voir si vous pouvez déduire quel sera notre troisième fait pertinent, à savoir comment vous pouvez récupérer rapidement deux nombres lorsque vous connaissez leur moyenne et leur produit. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.78,
  "end": 201.56
 },
 {
  "input": "Here, let's focus on this example. ",
  "translatedText": "Ici, concentrons-nous sur cet exemple. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.46,
  "end": 203.72
 },
 {
  "input": "You know the two values are evenly spaced around 7, so they look like 7 plus or minus something; let’s call that something \"d\" for distance. ",
  "translatedText": "Vous savez que les deux valeurs sont uniformément espacées autour du chiffre 7, elles ressemblent donc à 7 plus ou moins quelque chose, appelons cela quelque chose d pour la distance. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 204.2,
  "end": 212.78
 },
 {
  "input": "You also know that the product of these two numbers is 40. ",
  "translatedText": "Vous savez aussi que le produit de ces deux nombres est 40. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.56,
  "end": 216.38
 },
 {
  "input": "Now to find d, notice that this product expands really nicely, it works out as a difference of squares. ",
  "translatedText": "Maintenant, pour trouver d, notez que ce produit se développe très bien, cela se traduit par une différence de carrés. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.6,
  "end": 223.7
 },
 {
  "input": "So from there, you can directly find d: d^2 is 7^2 - 40, or 9, which means d itself is 3. ",
  "translatedText": "Donc à partir de là, vous pouvez directement trouver d. d au carré vaut 7 au carré moins 40, soit 9, ce qui signifie que d lui-même vaut 3. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.56,
  "end": 233.4
 },
 {
  "input": "In other words, the two values for this very specific example work out to be 4 and 10. ",
  "translatedText": "En d’autres termes, les deux valeurs pour cet exemple très spécifique sont 4 et 10. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.38,
  "end": 241.1
 },
 {
  "input": "But our goal is a quick trick, and you wouldn’t want to think this through each time, so let’s wrap up what we just did in a general formula. ",
  "translatedText": "Mais notre objectif est une astuce rapide, et vous ne voudriez pas y réfléchir à chaque fois, alors résumons ce que nous venons de faire dans une formule générale. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 241.68,
  "end": 248.12
 },
 {
  "input": "For any mean, m and product, p, the distance squared is always going to be m^2 - p. ",
  "translatedText": "Pour toute moyenne m et produit p, la distance au carré sera toujours m au carré moins p. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.64,
  "end": 255.68
 },
 {
  "input": "This gives the third key fact, which is that when two numbers have a mean m and a product p, you can write those two numbers as m ± sqrt(m^2 - p) This is decently fast to rederive on the fly if you ever forget it, and it’s essentially just a rephrasing of the difference of squares formula. ",
  "translatedText": "Cela donne le troisième fait clé, à savoir que lorsque deux nombres ont une moyenne m et un produit p, vous pouvez écrire ces deux nombres sous la forme m plus ou moins la racine carrée de m au carré moins p. C'est assez rapide à recréer à la volée si jamais vous l'oubliez, et il s'agit essentiellement d'une reformulation de la formule de la différence des carrés. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 257.56,
  "end": 277.08
 },
 {
  "input": "But even still it’s a fact worth memorizing so that you have it at the tip of your fingers. ",
  "translatedText": "Mais c’est quand même un fait qui mérite d’être mémorisé, donc il est au bout de vos doigts. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.86,
  "end": 281.22
 },
 {
  "input": "In fact, my friend Tim from the channel acapellascience wrote us a quick jingle to make it a little more memorable. ",
  "translatedText": "En fait, mon ami Tim de la chaîne A Capella Science nous a écrit un joli jingle rapide pour le rendre un peu plus mémorable. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.22,
  "end": 287.16
 },
 {
  "input": "m plus or minus squaaaare root of me squared minus p (ping!) Let me show you how this works, say for the matrix [[3,1], [4,1]]. ",
  "translatedText": "Laissez-moi vous montrer comment cela fonctionne, disons pour la matrice 3, 1, 4, 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.9,
  "end": 297.62
 },
 {
  "input": "You start by bringing to mind the formula, maybe stating it all in your head. ",
  "translatedText": "Vous commencez par vous rappeler la formule, peut-être en l’énonçant entièrement dans votre tête. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 298.1,
  "end": 301.82
 },
 {
  "input": "But when you write it down, you fill in the appropriate values of m and p as you go. ",
  "translatedText": "Mais lorsque vous l’écrivez, vous remplissez les valeurs appropriées de m et p au fur et à mesure. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.2,
  "end": 311.62
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2. ",
  "translatedText": "Ainsi, dans cet exemple, la moyenne des valeurs propres est la même que la moyenne de 3 et 1, qui est 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 312.34,
  "end": 317.74
 },
 {
  "input": "So the thing you start writing is 2 ± sqrt(2^2 - …). ",
  "translatedText": "Donc, la chose que vous commencez à écrire est 2 ± sqrt(2^2 - …). ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 318.3,
  "end": 322.7
 },
 {
  "input": "Then the product of the eigenvalues is the determinant, which in this example is 3*1 - 1*4, or -1. ",
  "translatedText": "Ensuite, le produit des valeurs propres est le déterminant, qui dans cet exemple est 3*1 - 1*4, ou -1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.54,
  "end": 332.14
 },
 {
  "input": "So that’s the final thing you fill in. ",
  "translatedText": "C'est donc la dernière chose que vous remplissez. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 332.38,
  "end": 334.48
 },
 {
  "input": "This means the eigenvalues are 2±sqrt(5). ",
  "translatedText": "Cela signifie que les valeurs propres sont 2 ± sqrt (5). ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.88,
  "end": 338.76
 },
 {
  "input": "You might recognize that this is the same matrix I was using at the beginning, but notice how much more directly we can get at the answer. ",
  "translatedText": "Vous reconnaîtrez peut-être qu’il s’agit de la même matrice que celle que j’utilisais au début, mais remarquez à quel point nous pouvons obtenir la réponse plus directement. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.3,
  "end": 346.5
 },
 {
  "input": "Here, try another one. ",
  "translatedText": "Tiens, essaie-en un autre. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.14,
  "end": 349.18
 },
 {
  "input": "This time the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5. ",
  "translatedText": "Cette fois, la moyenne des valeurs propres est la même que la moyenne de 2 et 8, qui est 5. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 349.44,
  "end": 354.48
 },
 {
  "input": "So again, you start writing out the formula but this time writing 5 in place of m [song]. ",
  "translatedText": "Encore une fois, vous commencez à écrire la formule mais cette fois en écrivant 5 à la place de m [chanson]. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.1,
  "end": 359.22
 },
 {
  "input": "And then the determinant is 2*8 - 7*1, or 9. ",
  "translatedText": "Et puis le déterminant est 2*8 - 7*1, ou 9. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.98,
  "end": 368.3
 },
 {
  "input": "So in this example, the eigenvalues look like 5 ± sqrt(16), which simplifies even further as 9 and 1. ",
  "translatedText": "Ainsi, dans cet exemple, les valeurs propres ressemblent à 5 ± sqrt(16), ce qui se simplifie encore davantage en 9 et 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 369.52,
  "end": 378.24
 },
 {
  "input": "You see what I mean about how you can basically just start writing down the eigenvalues while staring at the matrix? ",
  "translatedText": "Vous voyez ce que je veux dire sur la façon dont vous pouvez commencer à écrire les valeurs propres pendant que vous regardez la matrice ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 379.42,
  "end": 384.62
 },
 {
  "input": "It’s typically just the tiniest bit of simplifying at the end. ",
  "translatedText": "Il ne s’agit généralement que d’une infime simplification à la fin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.28,
  "end": 388.16
 },
 {
  "input": "Honestly, I’ve found myself using this trick a lot when I’m sketching quick notes related to linear algebra and want to use small matrices as examples. ",
  "translatedText": "Honnêtement, je me suis souvent retrouvé à utiliser cette astuce lorsque je dessine des notes rapides liées à l'algèbre linéaire et que je souhaite utiliser de petites matrices comme exemples. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 389.06,
  "end": 395.72
 },
 {
  "input": "I’ve been working on a video about matrix exponents, where eigenvalues pop up a lot, and I realized it’s just very handy if students can read off the eigenvalues from small examples without losing the main line of thought by getting bogged down in a different calculation. ",
  "translatedText": "J'ai travaillé sur une vidéo sur les exposants matriciels, où les valeurs propres apparaissent souvent, et je me rends compte que c'est tout simplement très pratique si les élèves peuvent lire les valeurs propres à partir de petits exemples sans perdre le fil de la pensée principale en s'enlisant dans un autre calcul. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.18,
  "end": 408.62
 },
 {
  "input": "As another fun example, take a look at this set of three different matrices, which come up a lot in quantum mechanics, they're known as the Pauli spin matrices. ",
  "translatedText": "Comme autre exemple amusant, jetez un œil à cet ensemble de trois matrices différentes, qui revient souvent en mécanique quantique. Elles sont connues sous le nom de matrices de spin de Pauli. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.74,
  "end": 417.52
 },
 {
  "input": "If you know quantum mechanics, you’ll know that the eigenvalues of matrices are highly relevant to the physics they describe, and if you don’t know quantum mechanics, let this just be a little glimpse of how these computations are actually relevant to real applications. ",
  "translatedText": "Si vous connaissez la mécanique quantique, vous saurez que les valeurs propres des matrices sont très pertinentes pour la physique qu'elles décrivent. Et si vous ne connaissez pas la mécanique quantique, voici juste un petit aperçu de la façon dont ces calculs sont en réalité très pertinents pour les applications réelles. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.6,
  "end": 431.22
 },
 {
  "input": "The mean of the diagonal in all three cases is 0, so the mean of the eigenvalues in all cases is 0, which makes our formula look especially simple. ",
  "translatedText": "La moyenne des entrées diagonales dans les trois cas est nulle. Ainsi, la moyenne des valeurs propres dans tous ces cas est nulle, ce qui rend notre formule particulièrement simple. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.54,
  "end": 443.06
 },
 {
  "input": "What about the products of the eigenvalues, the determinants of these matrices? ",
  "translatedText": "Qu’en est-il des produits des valeurs propres, déterminants de ces matrices ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.38,
  "end": 448.8
 },
 {
  "input": "For the first one, it’s 0 - 1 or -1. ",
  "translatedText": "Pour le premier, c’est 0 moins 1, soit moins 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.7,
  "end": 453.4
 },
 {
  "input": "The second also looks like 0 - 1, but it takes a moment more to see because of the complex numbers. ",
  "translatedText": "Le second ressemble également à 0 moins 1, mais il faut un moment de plus pour le voir en raison des nombres complexes. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.4,
  "end": 458.2
 },
 {
  "input": "And the final one looks like -1 - 0. ",
  "translatedText": "Et le dernier ressemble à moins 1 moins 0. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.84,
  "end": 461.36
 },
 {
  "input": "So in all cases, the eigenvalues simplify to be ±1. ",
  "translatedText": "Ainsi, dans tous les cas, les valeurs propres se simplifient pour être plus et moins 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.06,
  "end": 465.92
 },
 {
  "input": "Although in this case, you really don’t need the formula to find two values if you know theyr'e evenly spaced around 0 and their product is -1. ",
  "translatedText": "Bien que dans ce cas, vous n'avez vraiment pas besoin d'une formule pour trouver deux valeurs si vous savez qu'elles sont régulièrement espacées autour de 0 et que leur produit est moins 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.72,
  "end": 473.28
 },
 {
  "input": "If you’re curious, in the context of quantum mechanics, these matrices describe observations you might make about a particle's spin in the x, y or z directions. ",
  "translatedText": "Si vous êtes curieux, dans le contexte de la mécanique quantique, ces matrices décrivent les observations que vous pourriez faire sur le spin d'une particule dans la direction x, y ou z. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.64,
  "end": 483.76
 },
 {
  "input": "The fact that their eigenvalues are ±1 corresponds with the idea that the values for the spin that you would observe would be either entirely in one direction or entirely in another, as opposed to something continuously ranging in between. ",
  "translatedText": "Et le fait que leurs valeurs propres soient plus et moins 1 correspond à l'idée que les valeurs du spin que vous observeriez seraient soit entièrement dans une direction, soit entièrement dans une autre, par opposition à quelque chose qui se situe continuellement entre les deux. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.76,
  "end": 497.02
 },
 {
  "input": "Maybe you’d wonder how exactly this works, or why you would use 2x2 matrices that have complex numbers to describe spin in three dimensions. ",
  "translatedText": "Peut-être vous demandez-vous comment cela fonctionne exactement, ou pourquoi vous utiliseriez des matrices 2x2 contenant des nombres complexes pour décrire le spin en trois dimensions. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.32,
  "end": 505.52
 },
 {
  "input": "And those would be fair questions, just outside the scope of what I want to talk about here. ",
  "translatedText": "Et ce seraient des questions légitimes, qui sortent du cadre de ce dont je veux parler ici. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 506.1,
  "end": 509.76
 },
 {
  "input": "You know it’s funny, I wrote this section because I wanted some case where you have 2x2 matrices that are not just toy examples or homework problems, ones where they actually come up in practice, and quantum mechanics is great for that. ",
  "translatedText": "Vous savez, c'est drôle, j'ai écrit cette section parce que je voulais un cas où vous avez des matrices 2x2 qui ne sont pas seulement des exemples de jouets, ou des problèmes de devoirs, des cas où elles surviennent réellement dans la pratique, et la mécanique quantique est idéale pour ça. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 510.48,
  "end": 521.7
 },
 {
  "input": "But the thing is after I made it I realized that the whole example kind of undercuts the point I’m trying to make. ",
  "translatedText": "Mais le fait est qu'après l'avoir réalisé, j'ai réalisé que tout l'exemple allait à l'encontre du point que j'essaie de faire valoir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.7,
  "end": 528.24
 },
 {
  "input": "For these specific matrices, when you use the traditional method, the one with characteristic polynomials, it’s essentially just as fast; it might actually faster. ",
  "translatedText": "Pour ces matrices spécifiques, lorsque vous utilisez la méthode traditionnelle, celle des polynômes caractéristiques, c'est essentiellement tout aussi rapide. Cela pourrait en fait être plus rapide. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.74,
  "end": 537.64
 },
 {
  "input": "I mean, take a look a the first one: The relevant determinant directly gives you a characteristic polynomial of lambda^2 - 1, and clearly, that has roots of plus and minus 1. ",
  "translatedText": "Je veux dire, jetez un œil au premier. Le déterminant pertinent vous donne directement un polynôme caractéristique de lambda au carré moins un, et qui a clairement des racines de plus et moins un. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.24,
  "end": 548.2
 },
 {
  "input": "Same answer when you do the second matrix, lambda^2 - 1. ",
  "translatedText": "Même réponse lorsque vous faites la deuxième matrice, lambda au carré moins un. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 548.84,
  "end": 551.76
 },
 {
  "input": "And as for the last matrix, forget about doing any computations, traditional or otherwise, it’s already a diagonal matrix, so those diagonal entries are the eigenvalues! ",
  "translatedText": "Et comme pour la dernière matrice, oubliez de faire des calculs, traditionnels ou autres, c'est déjà une matrice diagonale, donc ces entrées diagonales sont les valeurs propres. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.88,
  "end": 562.74
 },
 {
  "input": "However, the example is not totally lost to our cause. ",
  "translatedText": "Cependant, l’exemple n’est pas totalement perdu pour notre cause. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.3,
  "end": 566.92
 },
 {
  "input": "Where you will actually feel the speed up is in the more general case where you take a linear combination of these three matrices and then try to compute the eigenvalues. ",
  "translatedText": "Là où vous ressentirez réellement l'accélération, c'est dans le cas plus général, où vous prenez une combinaison linéaire de ces trois matrices, puis essayez de calculer les valeurs propres. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 567.38,
  "end": 576.06
 },
 {
  "input": "You might write this as a times the first one, plus b times the second, plus c times the third. ",
  "translatedText": "Vous pouvez écrire ceci comme a multiplié par le premier, plus b fois le deuxième, plus c fois le troisième. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.82,
  "end": 582.42
 },
 {
  "input": "In quantum mechanics, this would describe spin observations in a general direction of a vector with coordinates [a, b, c]. ",
  "translatedText": "En mécanique quantique, cela décrirait les observations de spin dans une direction générale d'un vecteur de coordonnées a, b, c. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.02,
  "end": 589.28
 },
 {
  "input": "More specifically, you should assume this vector is normalized, meaning a^2 + b^2 + c^2 = 1. ",
  "translatedText": "Plus précisément, vous devez supposer que ce vecteur est normalisé, ce qui signifie que a au carré plus b au carré plus c au carré est égal à un. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.9,
  "end": 597.7
 },
 {
  "input": "When you look at this new matrix, it’s immediate to see that the mean of the eigenvalues is still zero, and you might also enjoy pausing for a brief moment to confirm that the product of those eigenvalues is still -1, and then from there concluding what the eigenvalues must be. ",
  "translatedText": "Lorsque vous regardez cette nouvelle matrice, il est immédiat de constater que la moyenne des valeurs propres est toujours nulle, et vous pourriez également apprécier de vous arrêter un bref instant pour confirmer que le produit de ces valeurs propres est toujours négatif. Et puis à partir de là, conclure quelles doivent être les valeurs propres. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.6,
  "end": 615.92
 },
 {
  "input": "And this time, the characteristic polynomial approach would be by comparison a lot more cumbersome, definitely harder to do in your head. ",
  "translatedText": "Et cette fois, l’approche polynomiale caractéristique serait en comparaison beaucoup plus lourde, nettement plus difficile à mettre en œuvre dans votre tête. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.22,
  "end": 623.58
 },
 {
  "input": "To be clear, using the mean-product formula is not fundamentally different from finding roots of the characteristic polynomial; I mean, it can't be, they're solving the same problem. ",
  "translatedText": "Pour être clair, utiliser la formule du produit moyen n’est pas différent de trouver les racines du polynôme caractéristique. Je veux dire, ce n'est pas possible, ils résolvent le même problème. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 625.08,
  "end": 633.44
 },
 {
  "input": "One way to think about this, actually, is that the mean-product formula is a nice way to solve quadratic in general (and some viewers of the channel may recognize this). ",
  "translatedText": "En fait, une façon d'y penser est que la formule du produit moyen est un bon moyen de résoudre les quadratiques en général, et certains téléspectateurs de la chaîne peuvent le reconnaître. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.16,
  "end": 641.66
 },
 {
  "input": "This about it: When you’re trying to find the roots of a quadratic given its coefficients, that's another situation where you know the sum of two values, and you also know their product, but you’re trying to recover the original two values. ",
  "translatedText": "Pensez-y. Lorsque vous essayez de trouver les racines d'une quadratique, étant donné les coefficients, c'est une autre situation où vous connaissez la somme de deux valeurs, et vous connaissez également leur produit, mais vous essayez de récupérer les deux valeurs d'origine. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 642.54,
  "end": 654.1
 },
 {
  "input": "Specifically, if the polynomial is normalized so that this leading coefficient is 1, then the mean of the roots will be -½ times this linear coefficient, which is -1 times the sum of those roots. ",
  "translatedText": "Plus précisément, si le polynôme est normalisé de sorte que ce coefficient principal soit égal à un, alors la moyenne des racines sera négative la moitié de ce coefficient linéaire, qui est négatif une fois la somme de ces racines. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 655.56,
  "end": 666.88
 },
 {
  "input": "For the example on the screen that makes the mean 5. ",
  "translatedText": "Pour l’exemple à l’écran, cela fait la moyenne de cinq. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 668.02,
  "end": 670.18
 },
 {
  "input": "And the product of the roots is even easier, it’s just the constant term no adjustments needed. ",
  "translatedText": "Et le produit des racines est encore plus simple, c'est juste le terme constant, aucun ajustement n'est nécessaire. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.98,
  "end": 676.52
 },
 {
  "input": "So from there, you would apply the mean product formula and that gives you the roots. ",
  "translatedText": "Donc à partir de là, vous appliqueriez la formule du produit moyen, et cela vous donnerait les racines. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.34,
  "end": 680.9
 },
 {
  "input": "On the one hand, you could think of this as a lighter-weight version of the traditional quadratic formula. ",
  "translatedText": "Et d’une part, vous pourriez considérer cela comme une version plus légère de la formule quadratique traditionnelle. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.14,
  "end": 690.22
 },
 {
  "input": "But the real advantage is that it's fewer symbols to memorize, it's that each one of them carries more meaning with it. ",
  "translatedText": "Mais le véritable avantage n’est pas seulement qu’il y a moins de symboles à mémoriser, c’est que chacun d’entre eux a plus de sens. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 690.96,
  "end": 696.44
 },
 {
  "input": "The whole point of this eigenvalue trick is that because you can read out the mean and product directly from looking at the matrix, you don't need to go through the intermediate step of setting up the characteristic polynomial. ",
  "translatedText": "Je veux dire, tout l'intérêt de cette astuce aux valeurs propres est que, comme vous pouvez lire la moyenne et le produit directement en regardant la matrice, vous n'avez pas besoin de passer par l'étape intermédiaire de configuration du polynôme caractéristique. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.94,
  "end": 708.0
 },
 {
  "input": "You can jump straight to writing down the roots without ever explicitly thinking about what the polynomial looks like. ",
  "translatedText": "Vous pouvez passer directement à l’écriture des racines sans jamais penser explicitement à quoi ressemble le polynôme. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.42,
  "end": 713.64
 },
 {
  "input": "But to do that we need a version of the quadratic formula where the terms carry some kind of meaning. ",
  "translatedText": "Mais pour ce faire, nous avons besoin d’une version de la formule quadratique où les termes ont une signification. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 713.84,
  "end": 718.82
 },
 {
  "input": "I realize that this is a very specific trick, for a very specific audience, but it’s something I wish I knew in college, so if you happen to know any students who might benefit from this, consider sharing it with them. ",
  "translatedText": "Je me rends compte qu'il s'agit d'une astuce très spécifique destinée à un public très spécifique, mais c'est quelque chose que j'aurais aimé connaître à l'université, donc si vous connaissez des étudiants qui pourraient en bénéficier, pensez à la partager avec eux. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 720.38,
  "end": 729.7
 },
 {
  "input": "The hope is that it’s not just one more thing to memorize, but that the framing reinforces some other nice facts worth knowing, like how the trace and determinant relate to eigenvalues. ",
  "translatedText": "L'espoir est que ce n'est pas seulement une chose de plus que vous mémorisez, mais que le cadrage renforce d'autres faits intéressants qui valent la peine d'être connus, comme la façon dont la trace et le déterminant sont liés aux valeurs propres. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.28,
  "end": 739.82
 },
 {
  "input": "If you want to prove those facts, by the way, take a moment to expand out the characteristic polynomial for a general matrix, and think hard about the meaning of each of these coefficients. ",
  "translatedText": "Si vous voulez prouver ces faits, prenez un moment pour développer le polynôme caractéristique d'une matrice générale, puis réfléchissez attentivement à la signification de chacun de ces coefficients. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 740.56,
  "end": 749.62
 },
 {
  "input": "Many thanks to Tim, for ensuring that this mean-product formula will stay stuck in all of our heads for at least a few months. ",
  "translatedText": "Un grand merci à Tim d'avoir veillé à ce que cette formule de produit moyenne reste gravée dans toutes nos têtes pendant au moins quelques mois. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 752.4,
  "end": 757.94
 },
 {
  "input": "If you don’t know about acapellascience, please do check it out. ",
  "translatedText": "Si vous ne connaissez pas la science alcappella, n'hésitez pas à y jeter un œil. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 761.7,
  "end": 766.0
 },
 {
  "input": "\"The Molecular Shape of You\", in particular, is one of the greatest things on the internet. ",
  "translatedText": "Votre forme moléculaire en particulier est l’une des choses les plus intéressantes sur Internet. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.28,
  "end": 769.58
 }
]