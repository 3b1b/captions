[
 {
  "input": "This is a video for anyone who already knows what eigenvalues and eigenvectors are, and who might enjoy a quick way to compute them in the case of 2x2 matrices. ",
  "translatedText": "该视频适合任何已经了解特征值和特征向量的人，并且可 能喜欢在 2x2 矩阵的情况下快速计算它们的方法。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 7.56
 },
 {
  "input": "If you’re unfamiliar with eigenvalues, take a look at this video which introduces them. ",
  "translatedText": "如果您不熟悉特征值，请继续观看此处的此视频，该视频 实际上是为了介绍它们。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.58,
  "end": 13.7
 },
 {
  "input": "You can skip ahead if you just want to see the trick, but if possible I’d like you to rediscover it for yourself, so for that let’s lay down a little background. ",
  "translatedText": "如果您只想了解其中的技巧，则可以 跳过，但如果可能的话，我希望您自己重新发现它。为此，我们 先介绍一些背景知识。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.68,
  "end": 22.38
 },
 {
  "input": "As a quick reminder, if the effect of a linear transformation on a given vector is to scale it by some constant, we call it an \"eigenvector\" of the transformation, and we call the relevant scaling factor the corresponding \"eigenvalue,\" often denoted with the letter lambda. ",
  "translatedText": "快速提醒一下，如果线性变换对给 定向量的影响是将该向量缩放某个常数，我们将其称为变 换的特征向量，并将相关缩放因子称为相应的特征值，通常 用字母表示拉姆达。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.26,
  "end": 38.6
 },
 {
  "input": "When you write this as an equation and you rearrange a little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the matrix (A minus lambda times the identity) must send some nonzero vector, namely the corresponding eigenvector, to the zero vector, which in turn means the determinant of this modified matrix must be 0. ",
  "translatedText": "当你把它写成一个方程，并稍微重新排列一 下时，你会看到，如果数字 lambda 是矩阵 A 的特征值， 那么矩阵 A 减去 lambda 乘以恒等式必须发送一些非零向 量，即相应的特征向量到零向量，这又意味着该修改矩阵的行列式必须 为零。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.84,
  "end": 64.58
 },
 {
  "input": "Okay, that’s all a little bit of a mouthful to say, but again, I’m assuming all of this is review for anyone watching. ",
  "translatedText": "好吧，说起来有点拗口，但我再次假设所有这些都是 针对你们观看的人的评论。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.12,
  "end": 71.54
 },
 {
  "input": "So, the usual way to compute eigenvalues, how I used to do it, and how I believe most students are taught to carry it out, is to subtract the unknown value lambda off the diagonals and then solve for when the determinant equals 0. ",
  "translatedText": "因此，计算特征值的常用方法，我 过去是如何做的，以及我相信大多数学生都是如何进行计算的，是 从对角线上减去未知值 lambda，然后求解行列式何时等于 0 。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 72.82,
  "end": 85.86
 },
 {
  "input": "Doing this always involves a few steps to expand out and simplify to get a clean quadratic polynomial, what's known as the “characteristic polynomial” of the matrix. ",
  "translatedText": "这样做总是需要几个步骤来扩展和简化以获得干净的二次多项式，即所谓的矩阵的“特征多项式”。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.76,
  "end": 96.46
 },
 {
  "input": "The eigenvalues are the roots of this polynomial. ",
  "translatedText": "特征值是该多项式的根。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 97.36,
  "end": 99.9
 },
 {
  "input": "So to find them you have to apply the quadratic formula, which itself typically requires one or two more steps of simplification. ",
  "translatedText": "因此，要找到它们，您必须应用二次公式，该公式本身通常需要一两个以上的简化步骤。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.1,
  "end": 106.54
 },
 {
  "input": "Honestly, the process isn’t terrible. ",
  "translatedText": "说实话，这个过程并不可怕。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 107.76,
  "end": 109.5
 },
 {
  "input": "But at least for 2x2 matrices, there’s a much more direct way to get at this answer. ",
  "translatedText": "但至少对于 2x2 矩阵，有一种更直接的方法可以得到这个答案。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.58,
  "end": 114.68
 },
 {
  "input": "And if you want to rediscover this trick, there are only three relevant facts you need to know, each of which is worth knowing in its own right and can help you with other problem-solving. ",
  "translatedText": "如果您想重新发现这个技巧，您只需要了解 三个相关事实，每个事实本身都值得了解，并且可以帮助您解 决其他问题。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 115.4,
  "end": 122.9
 },
 {
  "input": "Number 1: The trace of a matrix, which is the sum of these two diagonal entries, is equal to the sum of the eigenvalues. ",
  "translatedText": "第一，矩阵的迹（即这两个对角线项之 和）等于特征值之和。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 123.82,
  "end": 130.92
 },
 {
  "input": "Or another way to phrase it, more useful for our purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal entries. ",
  "translatedText": "或者对我们的目的更有用的另一种 表达方式是，两个特征值的平均值与这两个对角线条目的平均值相 同。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.7,
  "end": 139.46
 },
 {
  "input": "Number 2: The determinant of a matrix, our usual ad-bc formula, is equal to the product of the two eigenvalues. ",
  "translatedText": "第二，矩阵的行列式，即我们常用的 ad-bc 公式，等于两个 特征值的乘积。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.0,
  "end": 148.96
 },
 {
  "input": "And this should kind of make sense if you understand that eigenvalues describe how much an operator stretches space in a particular direction and that the determinant describes how much an operator scales areas (or volumes) as a whole. ",
  "translatedText": "如果您了解特征值描述了算子在特定方向上拉 伸空间的程度，并且行列式描述了算子整体上缩放面积 或体积的程度，那么这应该是有意义的。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.06,
  "end": 161.76
 },
 {
  "input": "Now before getting to the third fact, notice how you can essentially read these first two values out of the matrix without really writing much down. ",
  "translatedText": "现在，在讨论第三 个事实之前，请注意如何从矩阵中读取前两个值，而无需真正 写下太多内容。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 162.8,
  "end": 169.16
 },
 {
  "input": "Take this matrix here as an example. ",
  "translatedText": "以这里的这个矩阵为例。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.76,
  "end": 171.32
 },
 {
  "input": "Straight away you can know that the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7. ",
  "translatedText": "马上就可以知道特征 值的均值与8和6的均值相同，都是7。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 171.82,
  "end": 177.82
 },
 {
  "input": "Likewise, most linear algebra students are pretty well-practiced at finding the determinant, which in this case works out to be 48 - 8 So right away you know that the product of our two eigenvalues is 40. ",
  "translatedText": "同样，大多数线性 代数学生在求行列式方面都训练有素，在本例中，行列式的计算结果为 48 减 8。所以您马上就知道两个特征值的乘积是 40。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.58,
  "end": 191.7
 },
 {
  "input": "Now take a moment to see how you can derive what will be our third relevant fact, which is how to recover two numbers when you know their mean and you know their product. ",
  "translatedText": "现在花点时间看看你是否可以推导出我们的第三个相关事实，即当你知道 两个数字的平均值并且知道它们的乘积时，如何快速恢复它们。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.78,
  "end": 201.56
 },
 {
  "input": "Here, let's focus on this example. ",
  "translatedText": "在这里 ，我们重点关注这个例子。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.46,
  "end": 203.72
 },
 {
  "input": "You know the two values are evenly spaced around 7, so they look like 7 plus or minus something; let’s call that something \"d\" for distance. ",
  "translatedText": "您知道这两个值在数字 7 周围均匀分布，因此 它们看起来像 7 加上或减去某个值，我们将其称为距离 d。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 204.2,
  "end": 212.78
 },
 {
  "input": "You also know that the product of these two numbers is 40. ",
  "translatedText": "您还知道这两个数字的乘积是 40。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.56,
  "end": 216.38
 },
 {
  "input": "Now to find d, notice that this product expands really nicely, it works out as a difference of squares. ",
  "translatedText": "现在要求 d，请注意该乘积展开得非常好，它的计算结果是平方 差。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.6,
  "end": 223.7
 },
 {
  "input": "So from there, you can directly find d: d^2 is 7^2 - 40, or 9, which means d itself is 3. ",
  "translatedText": "所以从那里，你可以直接找到d。d 的平方是 7 的平方减 40，即 9，这意味着 d 本身是 3。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.56,
  "end": 233.4
 },
 {
  "input": "In other words, the two values for this very specific example work out to be 4 and 10. ",
  "translatedText": "换句话说，这个非常具体的示例的两个值分别为 4 和 1 0。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.38,
  "end": 241.1
 },
 {
  "input": "But our goal is a quick trick, and you wouldn’t want to think this through each time, so let’s wrap up what we just did in a general formula. ",
  "translatedText": "但我们的目标是一个快速技巧，您不会想每次都考虑这个问题，所以让我 们用一个通用公式来总结我们刚刚所做的事情。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 241.68,
  "end": 248.12
 },
 {
  "input": "For any mean, m and product, p, the distance squared is always going to be m^2 - p. ",
  "translatedText": "对于任何平均值 m 和乘积 p，距离的平方始终为 m 平方减去 p。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.64,
  "end": 255.68
 },
 {
  "input": "This gives the third key fact, which is that when two numbers have a mean m and a product p, you can write those two numbers as m ± sqrt(m^2 - p) This is decently fast to rederive on the fly if you ever forget it, and it’s essentially just a rephrasing of the difference of squares formula. ",
  "translatedText": "这就给出了第三个关键事实，即当两个数字具有均值 m 和乘积 p 时， 您可以将这两个数字写为 m 加上或减去 m 平方减去 p 的平方根。如果您忘记了，那么可以快速重新推导它 ，而且它本质上只是平方差公式的改写。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 257.56,
  "end": 277.08
 },
 {
  "input": "But even still it’s a fact worth memorizing so that you have it at the tip of your fingers. ",
  "translatedText": "但即便如此，这是一个值得记住的事实，因此它就在您的指尖。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.86,
  "end": 281.22
 },
 {
  "input": "In fact, my friend Tim from the channel acapellascience wrote us a quick jingle to make it a little more memorable. ",
  "translatedText": "事实上，我来自 A Capella Science 频道的朋友 Tim 给我们写了一首简 短的歌曲，让它更令人难忘。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.22,
  "end": 287.16
 },
 {
  "input": "m plus or minus squaaaare root of me squared minus p (ping!) Let me show you how this works, say for the matrix [[3,1], [4,1]]. ",
  "translatedText": "让我向您展示它是如何工作的，例如矩阵 3, 1, 4, 1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.9,
  "end": 297.62
 },
 {
  "input": "You start by bringing to mind the formula, maybe stating it all in your head. ",
  "translatedText": "你首先要想起这个公式，也许在你的脑海里把它全部陈述出来。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 298.1,
  "end": 301.82
 },
 {
  "input": "But when you write it down, you fill in the appropriate values of m and p as you go. ",
  "translatedText": "但是，当您写下来时，您可以随时填写 m 和 p 的适当值。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.2,
  "end": 311.62
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2. ",
  "translatedText": "因此，在本例中，特征值的平均值与 3 和 1 的平均值相同，即 2。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 312.34,
  "end": 317.74
 },
 {
  "input": "So the thing you start writing is 2 ± sqrt(2^2 - …). ",
  "translatedText": "所以你开始写的是 2 ± sqrt(2^2 - ...)。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 318.3,
  "end": 322.7
 },
 {
  "input": "Then the product of the eigenvalues is the determinant, which in this example is 3*1 - 1*4, or -1. ",
  "translatedText": "那么特征值的乘积就是行列式，在本例中为 3*1 - 1*4，即 -1。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.54,
  "end": 332.14
 },
 {
  "input": "So that’s the final thing you fill in. ",
  "translatedText": "这就是您填写的最后一项。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 332.38,
  "end": 334.48
 },
 {
  "input": "This means the eigenvalues are 2±sqrt(5). ",
  "translatedText": "这意味着特征值为 2±sqrt(5)。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.88,
  "end": 338.76
 },
 {
  "input": "You might recognize that this is the same matrix I was using at the beginning, but notice how much more directly we can get at the answer. ",
  "translatedText": "您可能会意识到这与我一开始使用的矩阵相同，但请注意我们可以更直接地获得答案。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.3,
  "end": 346.5
 },
 {
  "input": "Here, try another one. ",
  "translatedText": "在这里，尝试另一种。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.14,
  "end": 349.18
 },
 {
  "input": "This time the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5. ",
  "translatedText": "这次，特征值的平均值与 2 和 8 的平均 值相同，即 5。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 349.44,
  "end": 354.48
 },
 {
  "input": "So again, you start writing out the formula but this time writing 5 in place of m [song]. ",
  "translatedText": "再次，您开始写出公式，但这次写 5 代替 m [歌曲]。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.1,
  "end": 359.22
 },
 {
  "input": "And then the determinant is 2*8 - 7*1, or 9. ",
  "translatedText": "那么行列式就是 2*8 - 7*1，即 9。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.98,
  "end": 368.3
 },
 {
  "input": "So in this example, the eigenvalues look like 5 ± sqrt(16), which simplifies even further as 9 and 1. ",
  "translatedText": "因此，在本例中，特征值看起来像 5 ± sqrt(16)，进一步简化为 9 和 1。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 369.52,
  "end": 378.24
 },
 {
  "input": "You see what I mean about how you can basically just start writing down the eigenvalues while staring at the matrix? ",
  "translatedText": "你明白我的意思了，当你盯 着矩阵的时候，你基本上可以开始写下特征值吗？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 379.42,
  "end": 384.62
 },
 {
  "input": "It’s typically just the tiniest bit of simplifying at the end. ",
  "translatedText": "这通常只是最后的一点点简化。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.28,
  "end": 388.16
 },
 {
  "input": "Honestly, I’ve found myself using this trick a lot when I’m sketching quick notes related to linear algebra and want to use small matrices as examples. ",
  "translatedText": "老实说，当我绘制与线性代数相关的快速笔记并想使用小矩阵作为 示例时，我发现自己经常使用这个技巧。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 389.06,
  "end": 395.72
 },
 {
  "input": "I’ve been working on a video about matrix exponents, where eigenvalues pop up a lot, and I realized it’s just very handy if students can read off the eigenvalues from small examples without losing the main line of thought by getting bogged down in a different calculation. ",
  "translatedText": "我一直在制作一个关于矩 阵指数的视频，其中特征值出现了很多，我意识到，如果学生能 够从小例子中读出特征值，而不会因为陷入不同的困境而失去主 线，那就非常方便了计算。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.18,
  "end": 408.62
 },
 {
  "input": "As another fun example, take a look at this set of three different matrices, which come up a lot in quantum mechanics, they're known as the Pauli spin matrices. ",
  "translatedText": "作为另一个有趣的例子，看一下这 组三个不同的矩阵，它在量子力学中经常出现。它们被称为泡利 自旋矩阵。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.74,
  "end": 417.52
 },
 {
  "input": "If you know quantum mechanics, you’ll know that the eigenvalues of matrices are highly relevant to the physics they describe, and if you don’t know quantum mechanics, let this just be a little glimpse of how these computations are actually relevant to real applications. ",
  "translatedText": "如果您了解量子力学，您就会知道矩阵的特征值 与其描述的物理高度相关。如果您不了解量子力学，请让我 们稍微了解一下这些计算实际上如何与实际应用非常相关。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.6,
  "end": 431.22
 },
 {
  "input": "The mean of the diagonal in all three cases is 0, so the mean of the eigenvalues in all cases is 0, which makes our formula look especially simple. ",
  "translatedText": "所有三种情况下对角线条目的平均值均为零。因此，所有这些情况下特征值的平均值为零，这使得我们的公式看起来 特别简单。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.54,
  "end": 443.06
 },
 {
  "input": "What about the products of the eigenvalues, the determinants of these matrices? ",
  "translatedText": "特征值的乘积（这些矩阵的行列式）又如何呢？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.38,
  "end": 448.8
 },
 {
  "input": "For the first one, it’s 0 - 1 or -1. ",
  "translatedText": "对于第一个，它是 0 减 1，或负 1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.7,
  "end": 453.4
 },
 {
  "input": "The second also looks like 0 - 1, but it takes a moment more to see because of the complex numbers. ",
  "translatedText": "第二个看起来也像 0 减 1，但由于是复数，需要更多时间才能看到。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.4,
  "end": 458.2
 },
 {
  "input": "And the final one looks like -1 - 0. ",
  "translatedText": "最后一个看起来像负 1 减 0。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.84,
  "end": 461.36
 },
 {
  "input": "So in all cases, the eigenvalues simplify to be ±1. ",
  "translatedText": "因此在所有情况下，特征值都简化为正负 1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.06,
  "end": 465.92
 },
 {
  "input": "Although in this case, you really don’t need the formula to find two values if you know theyr'e evenly spaced around 0 and their product is -1. ",
  "translatedText": "尽管在本例中，如果您知道两个值在 0 周围均匀分布并且它们的乘积为 负 1，则实际上不需要公式来查找这两个值。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.72,
  "end": 473.28
 },
 {
  "input": "If you’re curious, in the context of quantum mechanics, these matrices describe observations you might make about a particle's spin in the x, y or z directions. ",
  "translatedText": "如果您好奇，在量子力学 的背景下，这些矩阵描述了您可能对粒子在 x、y 或 z 方向 上的自旋进行的观察。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.64,
  "end": 483.76
 },
 {
  "input": "The fact that their eigenvalues are ±1 corresponds with the idea that the values for the spin that you would observe would be either entirely in one direction or entirely in another, as opposed to something continuously ranging in between. ",
  "translatedText": "它们的特征值是正负 1 的事实与这 样的想法相对应，即您观察到的自旋值要么完全在一个方向 上，要么完全在另一个方向上，而不是在两者之间连续变 化。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.76,
  "end": 497.02
 },
 {
  "input": "Maybe you’d wonder how exactly this works, or why you would use 2x2 matrices that have complex numbers to describe spin in three dimensions. ",
  "translatedText": "也许您想知道它到底是如何工作的，或者为什么要使用具有复数的 2x2 矩阵来描述三维空间中的自旋。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.32,
  "end": 505.52
 },
 {
  "input": "And those would be fair questions, just outside the scope of what I want to talk about here. ",
  "translatedText": "这些都是公平的问题， 超出了我想在这里讨论的范围。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 506.1,
  "end": 509.76
 },
 {
  "input": "You know it’s funny, I wrote this section because I wanted some case where you have 2x2 matrices that are not just toy examples or homework problems, ones where they actually come up in practice, and quantum mechanics is great for that. ",
  "translatedText": "你知道，这很有趣，我写这一 部分是因为我想要一些 2x2 矩阵的情况，这些矩阵不仅仅是玩具示例 或家庭作业问题，而是它们在实践中实际出现的问题，而量子力学非常适 合这样做。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 510.48,
  "end": 521.7
 },
 {
  "input": "But the thing is after I made it I realized that the whole example kind of undercuts the point I’m trying to make. ",
  "translatedText": "但问题是，在我完成之后，我意识到整个示例有点 削弱了我想要表达的观点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.7,
  "end": 528.24
 },
 {
  "input": "For these specific matrices, when you use the traditional method, the one with characteristic polynomials, it’s essentially just as fast; it might actually faster. ",
  "translatedText": "对于这些特定的矩阵，当您使用 传统方法（具有特征多项式的方法）时，它本质上同样快。实际 上可能会更快。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.74,
  "end": 537.64
 },
 {
  "input": "I mean, take a look a the first one: The relevant determinant directly gives you a characteristic polynomial of lambda^2 - 1, and clearly, that has roots of plus and minus 1. ",
  "translatedText": "我的意思是，看看第一个。相关的行列式直接给出 了 lambda 平方减一的特征多项式，并且显然它有正 负一的根。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.24,
  "end": 548.2
 },
 {
  "input": "Same answer when you do the second matrix, lambda^2 - 1. ",
  "translatedText": "当你做第二个矩阵时，答案是相同的，即 lambda 平方减一。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 548.84,
  "end": 551.76
 },
 {
  "input": "And as for the last matrix, forget about doing any computations, traditional or otherwise, it’s already a diagonal matrix, so those diagonal entries are the eigenvalues! ",
  "translatedText": "至于最后一个矩阵，忘记做任何计算，无论是传统的还是其他的 ，它已经是一个对角矩阵，所以那些对角项就是特征值。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.88,
  "end": 562.74
 },
 {
  "input": "However, the example is not totally lost to our cause. ",
  "translatedText": "然而，我 们的事业并没有完全失去这个例子。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.3,
  "end": 566.92
 },
 {
  "input": "Where you will actually feel the speed up is in the more general case where you take a linear combination of these three matrices and then try to compute the eigenvalues. ",
  "translatedText": "您实际上会感觉到加速是在更 一般的情况下，您采用这三个矩阵的线性组合，然后尝试 计算特征值。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 567.38,
  "end": 576.06
 },
 {
  "input": "You might write this as a times the first one, plus b times the second, plus c times the third. ",
  "translatedText": "您可以将其写为 a 乘以第一个，加上 b 乘以第二个，再加上 c 乘以第三个。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.82,
  "end": 582.42
 },
 {
  "input": "In quantum mechanics, this would describe spin observations in a general direction of a vector with coordinates [a, b, c]. ",
  "translatedText": "在量子力学中，这将描述坐标为 a、b、c 的 向量的一般方向上的自旋观测。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.02,
  "end": 589.28
 },
 {
  "input": "More specifically, you should assume this vector is normalized, meaning a^2 + b^2 + c^2 = 1. ",
  "translatedText": "更具体地说，您应该假设该向量已标准 化，这意味着 a 的平方加上 b 的平方加上 c 的平方等于 1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.9,
  "end": 597.7
 },
 {
  "input": "When you look at this new matrix, it’s immediate to see that the mean of the eigenvalues is still zero, and you might also enjoy pausing for a brief moment to confirm that the product of those eigenvalues is still -1, and then from there concluding what the eigenvalues must be. ",
  "translatedText": "当您查看这个新矩阵时，您会立即看到特征值的平均值仍 然为零，并且您可能还喜欢暂停片刻以确认这些特征值的 乘积仍然为负数。然后从那里得出特征值一定是什么的结论。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.6,
  "end": 615.92
 },
 {
  "input": "And this time, the characteristic polynomial approach would be by comparison a lot more cumbersome, definitely harder to do in your head. ",
  "translatedText": "而这一次，特征多项式方法相比之下会麻烦得多，在你 的头脑中肯定更难做到。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.22,
  "end": 623.58
 },
 {
  "input": "To be clear, using the mean-product formula is not fundamentally different from finding roots of the characteristic polynomial; I mean, it can't be, they're solving the same problem. ",
  "translatedText": "需要明确的是，使用平均乘积公式 与求特征多项式的根没有什么不同。我的意思是，这不可能，他们正在解 决同样的问题。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 625.08,
  "end": 633.44
 },
 {
  "input": "One way to think about this, actually, is that the mean-product formula is a nice way to solve quadratic in general (and some viewers of the channel may recognize this). ",
  "translatedText": "实际上，思考这个问题的一种方法是，平均乘积公式是解 决一般二次方程的好方法，该频道的一些观众可能会认识到这一点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.16,
  "end": 641.66
 },
 {
  "input": "This about it: When you’re trying to find the roots of a quadratic given its coefficients, that's another situation where you know the sum of two values, and you also know their product, but you’re trying to recover the original two values. ",
  "translatedText": "想一想。当您尝试在给定系数的情况下找到二次方的根时，这是 另一种情况，您知道两个值的总和，并且您也知道它们的乘积 ，但您正在尝试恢复原始的两个值。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 642.54,
  "end": 654.1
 },
 {
  "input": "Specifically, if the polynomial is normalized so that this leading coefficient is 1, then the mean of the roots will be -½ times this linear coefficient, which is -1 times the sum of those roots. ",
  "translatedText": "具体来说，如果对多项 式进行归一化，使得该前导系数为 1，则根的平均值将 是该线性系数的负二分之一，即这些根之和的负一倍。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 655.56,
  "end": 666.88
 },
 {
  "input": "For the example on the screen that makes the mean 5. ",
  "translatedText": "对于屏幕上的示例，这意味着平均值为 5。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 668.02,
  "end": 670.18
 },
 {
  "input": "And the product of the roots is even easier, it’s just the constant term no adjustments needed. ",
  "translatedText": "而根的乘积就更容易了 ，只是常数项，不需要调整。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.98,
  "end": 676.52
 },
 {
  "input": "So from there, you would apply the mean product formula and that gives you the roots. ",
  "translatedText": "因此，从那里开始，您将应用 平均乘积公式，这将为您提供根源。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.34,
  "end": 680.9
 },
 {
  "input": "On the one hand, you could think of this as a lighter-weight version of the traditional quadratic formula. ",
  "translatedText": "一方面，您可以将其视 为传统二次公式的轻量级版本。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.14,
  "end": 690.22
 },
 {
  "input": "But the real advantage is that it's fewer symbols to memorize, it's that each one of them carries more meaning with it. ",
  "translatedText": "但真正的优势不仅在 于它需要记住的符号更少，还在于每个符号都承载着更多的含义。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 690.96,
  "end": 696.44
 },
 {
  "input": "The whole point of this eigenvalue trick is that because you can read out the mean and product directly from looking at the matrix, you don't need to go through the intermediate step of setting up the characteristic polynomial. ",
  "translatedText": "我的意思是，这个特征值技巧的要点在于，因为您可以直 接通过查看矩阵读出平均值和乘积，所以您不需要经历设置 特征多项式的中间步骤。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.94,
  "end": 708.0
 },
 {
  "input": "You can jump straight to writing down the roots without ever explicitly thinking about what the polynomial looks like. ",
  "translatedText": "您可以直接跳到写下根， 而无需明确考虑多项式是什么样的。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.42,
  "end": 713.64
 },
 {
  "input": "But to do that we need a version of the quadratic formula where the terms carry some kind of meaning. ",
  "translatedText": "但要做到这一 点，我们需要一个二次公式的版本，其中的项具有某种含义。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 713.84,
  "end": 718.82
 },
 {
  "input": "I realize that this is a very specific trick, for a very specific audience, but it’s something I wish I knew in college, so if you happen to know any students who might benefit from this, consider sharing it with them. ",
  "translatedText": "我意识到这是针对特定受众的非常特定的技巧，但我希望我在大学时就知 道这一点，所以如果您碰巧认识任何可能从中受益的学生，请考虑与他们 分享。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 720.38,
  "end": 729.7
 },
 {
  "input": "The hope is that it’s not just one more thing to memorize, but that the framing reinforces some other nice facts worth knowing, like how the trace and determinant relate to eigenvalues. ",
  "translatedText": "我们希望这不仅仅是您记住的另一件事，而且框架还 强化了其他一些值得了解的好事实，例如迹和行列式与特 征值的关系。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.28,
  "end": 739.82
 },
 {
  "input": "If you want to prove those facts, by the way, take a moment to expand out the characteristic polynomial for a general matrix, and think hard about the meaning of each of these coefficients. ",
  "translatedText": "顺便说一句，如果您想证明这些事实，请 花点时间展开一般矩阵的特征多项式，然后认真思考每 个系数的含义。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 740.56,
  "end": 749.62
 },
 {
  "input": "Many thanks to Tim, for ensuring that this mean-product formula will stay stuck in all of our heads for at least a few months. ",
  "translatedText": "非常感谢蒂姆，确保这个平均产品配方将在我 们所有人的脑海中停留至少几个月。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 752.4,
  "end": 757.94
 },
 {
  "input": "If you don’t know about acapellascience, please do check it out. ",
  "translatedText": "如果您不了解阿尔卡贝拉科 学，请务必查看一下。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 761.7,
  "end": 766.0
 },
 {
  "input": "\"The Molecular Shape of You\", in particular, is one of the greatest things on the internet. ",
  "translatedText": "尤其是你的分子形状是 互联网上最伟大的事物之一。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.28,
  "end": 769.58
 }
]