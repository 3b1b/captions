1
00:00:00,060 --> 00:00:01,989
This is a video for anyone who already knows

2
00:00:01,989 --> 00:00:06,060
what eigenvalues and eigenvectors are, and
who might enjoy a quick way to compute them

3
00:00:06,060 --> 00:00:12,080
in the case of 2x2 matrices. If you’re unfamiliar
with eigenvalues, take a look at this video

4
00:00:12,080 --> 00:00:14,870
which introduces them.

5
00:00:14,870 --> 00:00:18,349
You can skip ahead if you just want to see
the trick, but if possible I’d like you

6
00:00:18,349 --> 00:00:23,349
to rediscover it for yourself, so for that
let’s lay down a little background.

7
00:00:23,349 --> 00:00:28,550
As a quick reminder, if the effect of a linear
transformation on a given vector is to scale

8
00:00:28,550 --> 00:00:34,250
it by some constant, we call it an "eigenvector"
of the transformation, and we call the relevant

9
00:00:34,250 --> 00:00:39,981
scaling factor the corresponding "eigenvalue,"
often denoted with the letter lambda. When

10
00:00:39,981 --> 00:00:47,320
you write this as an equation and you rearrange
a little bit, what you see is that if the

11
00:00:47,320 --> 00:00:53,190
number lambda is an eigenvalue of a matrix
A, then the matrix (A minus lambda times the

12
00:00:53,190 --> 00:00:59,220
identity) must send some nonzero vector, namely
the corresponding eigenvector, to the zero

13
00:00:59,220 --> 00:01:04,999
vector, which in turn means the determinant
of this modified matrix must be 0.

14
00:01:04,999 --> 00:01:09,960
Okay, that’s all a little bit of a mouthful
to say, but again, I’m assuming all of this

15
00:01:09,960 --> 00:01:12,829
is review for anyone watching.

16
00:01:12,829 --> 00:01:17,939
So, the usual way to compute eigenvalues,
how I used to do it, and how I believe most

17
00:01:17,939 --> 00:01:22,179
students are taught to carry it out, is to
subtract the unknown value lambda off the

18
00:01:22,179 --> 00:01:27,890
diagonals and then solve for when the determinant
equals 0.

19
00:01:27,890 --> 00:01:32,630
Doing this always involves a few steps to
expand out and simplify to get a clean quadratic

20
00:01:32,630 --> 00:01:38,069
polynomial, what's known as the “characteristic
polynomial” of the matrix. The eigenvalues

21
00:01:38,069 --> 00:01:43,170
are the roots of this polynomial. So to find
them you have to apply the quadratic formula,

22
00:01:43,170 --> 00:01:46,050
which itself typically requires one or two
more steps of simplification.

23
00:01:46,050 --> 00:01:52,710
Honestly, the process isn’t terrible. But
at least for 2x2 matrices, there’s a much

24
00:01:52,710 --> 00:01:57,049
more direct way to get at this answer. And
if you want to rediscover this trick, there

25
00:01:57,049 --> 00:02:00,929
are only three relevant facts you need to
know, each of which is worth knowing in its

26
00:02:00,929 --> 00:02:03,899
own right and can help you with other problem-solving.

27
00:02:03,899 --> 00:02:08,979
Number 1: The trace of a matrix, which is
the sum of these two diagonal entries, is

28
00:02:08,979 --> 00:02:14,700
equal to the sum of the eigenvalues. Or another
way to phrase it, more useful for our purposes,

29
00:02:14,700 --> 00:02:18,959
is that the mean of the two eigenvalues is
the same as the mean of these two diagonal

30
00:02:18,959 --> 00:02:19,959
entries.

31
00:02:19,959 --> 00:02:27,810
Number 2: The determinant of a matrix, our
usual ad-bc formula, is equal to the product

32
00:02:27,810 --> 00:02:32,890
of the two eigenvalues. And this should kind
of make sense if you understand that eigenvalues

33
00:02:32,890 --> 00:02:37,720
describe how much an operator stretches space
in a particular direction and that the determinant

34
00:02:37,720 --> 00:02:42,189
describes how much an operator scales areas
(or volumes) as a whole.

35
00:02:42,189 --> 00:02:46,550
Now before getting to the third fact, notice
how you can essentially read these first two

36
00:02:46,550 --> 00:02:51,970
values out of the matrix without really writing
much down. Take this matrix here as an example.

37
00:02:51,970 --> 00:02:55,970
Straight away you can know that the mean of
the eigenvalues is the same as the mean of

38
00:02:55,970 --> 00:03:02,420
8 and 6, which is 7. Likewise, most linear
algebra students are pretty well-practiced

39
00:03:02,420 --> 00:03:08,909
at finding the determinant, which in this
case works out to be 48 - 8 So right away

40
00:03:08,909 --> 00:03:12,890
you know that the product of our two eigenvalues
is 40.

41
00:03:12,890 --> 00:03:17,079
Now take a moment to see how you can derive
what will be our third relevant fact, which

42
00:03:17,079 --> 00:03:21,860
is how to recover two numbers when you know
their mean and you know their product.

43
00:03:21,860 --> 00:03:27,959
Here, let's focus on this example. You know
the two values are evenly spaced around 7,

44
00:03:27,959 --> 00:03:33,959
so they look like 7 plus or minus something;
let’s call that something "d" for distance.

45
00:03:33,959 --> 00:03:40,239
You also know that the product of these two
numbers is 40. Now to find d, notice that

46
00:03:40,239 --> 00:03:45,480
this product expands really nicely, it works
out as a difference of squares. So from there,

47
00:03:45,480 --> 00:03:54,120
you can directly find d: d^2 is 7^2 - 40,
or 9, which means d itself is 3.

48
00:03:54,120 --> 00:04:01,930
In other words, the two values for this very
specific example work out to be 4 and 10.

49
00:04:01,930 --> 00:04:06,239
But our goal is a quick trick, and you wouldn’t
want to think this through each time, so let’s

50
00:04:06,239 --> 00:04:12,610
wrap up what we just did in a general formula.
For any mean, m and product, p, the distance

51
00:04:12,610 --> 00:04:20,209
squared is always going to be m^2 - p. This
gives the third key fact, which is that when

52
00:04:20,209 --> 00:04:27,660
two numbers have a mean m and a product p,
you can write those two numbers as m ± sqrt(m^2

53
00:04:27,660 --> 00:04:30,150
- p)

54
00:04:30,150 --> 00:04:34,490
This is decently fast to rederive on the fly
if you ever forget it, and it’s essentially

55
00:04:34,490 --> 00:04:39,360
just a rephrasing of the difference of squares
formula. But even still it’s a fact worth

56
00:04:39,360 --> 00:04:41,610
memorizing so that you have it at the tip
of your fingers.

57
00:04:41,610 --> 00:04:46,520
In fact, my friend Tim from the channel acapellascience
wrote us a quick jingle to make it a little

58
00:04:46,520 --> 00:04:48,500
more memorable.

59
00:04:48,500 --> 00:04:53,880
m plus or minus squaaaare root of me squared
minus p (ping!)

60
00:04:53,880 --> 00:04:59,069
Let me show you how this works, say for the
matrix [[3,1], [4,1]]. You start by bringing

61
00:04:59,069 --> 00:05:08,330
to mind the formula, maybe stating it all
in your head. But when you write it down,

62
00:05:08,330 --> 00:05:13,551
you fill in the appropriate values of m and
p as you go. So in this example, the mean

63
00:05:13,551 --> 00:05:19,030
of the eigenvalues is the same as the mean
of 3 and 1, which is 2. So the thing you start

64
00:05:19,030 --> 00:05:26,780
writing is 2 ± sqrt(2^2 - …). Then the
product of the eigenvalues is the determinant,

65
00:05:26,780 --> 00:05:35,090
which in this example is 3*1 - 1*4, or -1.
So that’s the final thing you fill in. This

66
00:05:35,090 --> 00:05:42,319
means the eigenvalues are 2±sqrt(5). You
might recognize that this is the same matrix

67
00:05:42,319 --> 00:05:47,370
I was using at the beginning, but notice how
much more directly we can get at the answer.

68
00:05:47,370 --> 00:05:52,190
Here, try another one. This time the mean
of the eigenvalues is the same as the mean

69
00:05:52,190 --> 00:05:57,870
of 2 and 8, which is 5. So again, you start
writing out the formula but this time writing

70
00:05:57,870 --> 00:06:10,900
5 in place of m [song]. And then the determinant
is 2*8 - 7*1, or 9. So in this example, the

71
00:06:10,900 --> 00:06:19,590
eigenvalues look like 5 ± sqrt(16), which
simplifies even further as 9 and 1.

72
00:06:19,590 --> 00:06:23,800
You see what I mean about how you can basically
just start writing down the eigenvalues while

73
00:06:23,800 --> 00:06:28,470
staring at the matrix? It’s typically just
the tiniest bit of simplifying at the end.

74
00:06:28,470 --> 00:06:32,800
Honestly, I’ve found myself using this trick
a lot when I’m sketching quick notes related

75
00:06:32,800 --> 00:06:37,039
to linear algebra and want to use small matrices
as examples. I’ve been working on a video

76
00:06:37,039 --> 00:06:41,800
about matrix exponents, where eigenvalues
pop up a lot, and I realized it’s just very

77
00:06:41,800 --> 00:06:45,960
handy if students can read off the eigenvalues
from small examples without losing the main

78
00:06:45,960 --> 00:06:50,340
line of thought by getting bogged down in
a different calculation.

79
00:06:50,340 --> 00:06:54,190
As another fun example, take a look at this
set of three different matrices, which come

80
00:06:54,190 --> 00:06:59,419
up a lot in quantum mechanics, they're known
as the Pauli spin matrices. If you know quantum

81
00:06:59,419 --> 00:07:03,710
mechanics, you’ll know that the eigenvalues
of matrices are highly relevant to the physics

82
00:07:03,710 --> 00:07:08,060
they describe, and if you don’t know quantum
mechanics, let this just be a little glimpse

83
00:07:08,060 --> 00:07:12,639
of how these computations are actually relevant
to real applications.

84
00:07:12,639 --> 00:07:19,990
The mean of the diagonal in all three cases
is 0, so the mean of the eigenvalues in all

85
00:07:19,990 --> 00:07:25,870
cases is 0, which makes our formula look especially
simple.

86
00:07:25,870 --> 00:07:29,840
What about the products of the eigenvalues,
the determinants of these matrices? For the

87
00:07:29,840 --> 00:07:36,430
first one, it’s 0 - 1 or -1. The second
also looks like 0 - 1, but it takes a moment

88
00:07:36,430 --> 00:07:42,310
more to see because of the complex numbers.
And the final one looks like -1 - 0. So in

89
00:07:42,310 --> 00:07:48,060
all cases, the eigenvalues simplify to be
±1. Although in this case, you really don’t

90
00:07:48,060 --> 00:07:52,080
need the formula to find two values if you
know theyr'e evenly spaced around 0 and their

91
00:07:52,080 --> 00:07:53,080
product is -1.

92
00:07:53,080 --> 00:07:59,759
If you’re curious, in the context of quantum
mechanics, these matrices describe observations

93
00:07:59,759 --> 00:08:04,470
you might make about a particle's spin in
the x, y or z directions. The fact that their

94
00:08:04,470 --> 00:08:09,810
eigenvalues are ±1 corresponds with the idea
that the values for the spin that you would

95
00:08:09,810 --> 00:08:15,419
observe would be either entirely in one direction
or entirely in another, as opposed to something

96
00:08:15,419 --> 00:08:20,680
continuously ranging in between. Maybe you’d
wonder how exactly this works, or why you

97
00:08:20,680 --> 00:08:26,259
would use 2x2 matrices that have complex numbers
to describe spin in three dimensions. And

98
00:08:26,259 --> 00:08:30,500
those would be fair questions, just outside
the scope of what I want to talk about here.

99
00:08:30,500 --> 00:08:33,990
You know it’s funny, I wrote this section
because I wanted some case where you have

100
00:08:33,990 --> 00:08:38,970
2x2 matrices that are not just toy examples
or homework problems, ones where they actually

101
00:08:38,970 --> 00:08:43,880
come up in practice, and quantum mechanics
is great for that. But the thing is after

102
00:08:43,880 --> 00:08:47,940
I made it I realized that the whole example
kind of undercuts the point I’m trying to

103
00:08:47,940 --> 00:08:53,420
make. For these specific matrices, when you
use the traditional method, the one with characteristic

104
00:08:53,420 --> 00:08:58,700
polynomials, it’s essentially just as fast;
it might actually faster. I mean, take a look

105
00:08:58,700 --> 00:09:03,390
a the first one: The relevant determinant
directly gives you a characteristic polynomial

106
00:09:03,390 --> 00:09:09,570
of lambda^2 - 1, and clearly, that has roots
of plus and minus 1. Same answer when you

107
00:09:09,570 --> 00:09:16,480
do the second matrix, lambda^2 - 1. And as
for the last matrix, forget about doing any

108
00:09:16,480 --> 00:09:20,940
computations, traditional or otherwise, it’s
already a diagonal matrix, so those diagonal

109
00:09:20,940 --> 00:09:23,510
entries are the eigenvalues!

110
00:09:23,510 --> 00:09:28,850
However, the example is not totally lost to
our cause. Where you will actually feel the

111
00:09:28,850 --> 00:09:33,260
speed up is in the more general case where
you take a linear combination of these three

112
00:09:33,260 --> 00:09:36,760
matrices and then try to compute the eigenvalues.

113
00:09:36,760 --> 00:09:42,000
You might write this as a times the first
one, plus b times the second, plus c times

114
00:09:42,000 --> 00:09:47,360
the third. In quantum mechanics, this would
describe spin observations in a general direction

115
00:09:47,360 --> 00:09:53,240
of a vector with coordinates [a, b, c]. More
specifically, you should assume this vector

116
00:09:53,240 --> 00:10:01,269
is normalized, meaning a^2 + b^2 + c^2 = 1.
When you look at this new matrix, it’s immediate

117
00:10:01,269 --> 00:10:05,889
to see that the mean of the eigenvalues is
still zero, and you might also enjoy pausing

118
00:10:05,889 --> 00:10:13,560
for a brief moment to confirm that the product
of those eigenvalues is still -1, and then

119
00:10:13,560 --> 00:10:18,710
from there concluding what the eigenvalues
must be. And this time, the characteristic

120
00:10:18,710 --> 00:10:22,860
polynomial approach would be by comparison
a lot more cumbersome, definitely harder to

121
00:10:22,860 --> 00:10:25,190
do in your head.

122
00:10:25,190 --> 00:10:29,220
To be clear, using the mean-product formula
is not fundamentally different from finding

123
00:10:29,220 --> 00:10:33,110
roots of the characteristic polynomial; I
mean, it can't be, they're solving the same

124
00:10:33,110 --> 00:10:37,160
problem. One way to think about this, actually,
is that the mean-product formula is a nice

125
00:10:37,160 --> 00:10:41,480
way to solve quadratic in general (and some
viewers of the channel may recognize this).

126
00:10:41,480 --> 00:10:47,140
This about it: When you’re trying to find
the roots of a quadratic given its coefficients,

127
00:10:47,140 --> 00:10:50,950
that's another situation where you know the
sum of two values, and you also know their

128
00:10:50,950 --> 00:10:54,820
product, but you’re trying to recover the
original two values.

129
00:10:54,820 --> 00:11:00,420
Specifically, if the polynomial is normalized
so that this leading coefficient is 1, then

130
00:11:00,420 --> 00:11:06,030
the mean of the roots will be -½ times this
linear coefficient, which is -1 times the

131
00:11:06,030 --> 00:11:12,820
sum of those roots. For the example on the
screen that makes the mean 5. And the product

132
00:11:12,820 --> 00:11:17,610
of the roots is even easier, it’s just the
constant term no adjustments needed. So from

133
00:11:17,610 --> 00:11:22,339
there, you would apply the mean product formula
and that gives you the roots.

134
00:11:22,339 --> 00:11:29,300
On the one hand, you could think of this as
a lighter-weight version of the traditional

135
00:11:29,300 --> 00:11:34,200
quadratic formula. But the real advantage
is that it's fewer symbols to memorize, it's

136
00:11:34,200 --> 00:11:37,370
that each one of them carries more meaning
with it.

137
00:11:37,370 --> 00:11:41,610
The whole point of this eigenvalue trick is
that because you can read out the mean and

138
00:11:41,610 --> 00:11:45,760
product directly from looking at the matrix,
you don't need to go through the intermediate

139
00:11:45,760 --> 00:11:49,930
step of setting up the characteristic polynomial.
You can jump straight to writing down the

140
00:11:49,930 --> 00:11:54,339
roots without ever explicitly thinking about
what the polynomial looks like. But to do

141
00:11:54,339 --> 00:12:00,660
that we need a version of the quadratic formula
where the terms carry some kind of meaning.

142
00:12:00,660 --> 00:12:04,550
I realize that this is a very specific trick,
for a very specific audience, but it’s something

143
00:12:04,550 --> 00:12:08,240
I wish I knew in college, so if you happen
to know any students who might benefit from

144
00:12:08,240 --> 00:12:10,330
this, consider sharing it with them.

145
00:12:10,330 --> 00:12:15,140
The hope is that it’s not just one more
thing to memorize, but that the framing reinforces

146
00:12:15,140 --> 00:12:20,660
some other nice facts worth knowing, like
how the trace and determinant relate to eigenvalues.

147
00:12:20,660 --> 00:12:24,630
If you want to prove those facts, by the way,
take a moment to expand out the characteristic

148
00:12:24,630 --> 00:12:32,449
polynomial for a general matrix, and think
hard about the meaning of each of these coefficients.

149
00:12:32,449 --> 00:12:36,420
Many thanks to Tim, for ensuring that this
mean-product formula will stay stuck in all

150
00:12:36,420 --> 00:12:45,240
of our heads for at least a few months. If
you don’t know about acapellascience, please

151
00:12:45,240 --> 00:12:49,240
do check it out. "The Molecular Shape of You",
in particular, is one of the greatest things

152
00:12:49,240 --> 00:13:04,720
on the internet.

