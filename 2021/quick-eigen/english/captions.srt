1
00:00:00,000 --> 00:00:03,969
This is a video for anyone who already knows what eigenvalues and eigenvectors are, 

2
00:00:03,969 --> 00:00:07,560
and who might enjoy a quick way to compute them in the case of 2x2 matrices.

3
00:00:08,580 --> 00:00:11,993
If you're unfamiliar with eigenvalues, go ahead and take a look at this video here, 

4
00:00:11,993 --> 00:00:13,700
which is actually meant to introduce them.

5
00:00:14,680 --> 00:00:17,390
You can skip ahead if all you want to do is see the trick, 

6
00:00:17,390 --> 00:00:20,100
but if possible I'd like you to rediscover it for yourself.

7
00:00:20,580 --> 00:00:22,380
So for that, let's lay out a little background.

8
00:00:23,260 --> 00:00:26,957
As a quick reminder, if the effect of a linear transformation on a 

9
00:00:26,957 --> 00:00:29,991
given vector is to scale that vector by some constant, 

10
00:00:29,991 --> 00:00:32,695
we call it an eigenvector of the transformation, 

11
00:00:32,695 --> 00:00:36,558
and we call the relevant scaling factor the corresponding eigenvalue, 

12
00:00:36,558 --> 00:00:38,600
often denoted with the letter lambda.

13
00:00:39,840 --> 00:00:45,466
When you write this as an equation, and you rearrange a little bit, 

14
00:00:45,466 --> 00:00:51,589
what you see is that if the number lambda is an eigenvalue of a matrix A, 

15
00:00:51,589 --> 00:00:58,126
then the eigenvector is then the corresponding eigenvector to the zero vector, 

16
00:00:58,126 --> 00:01:04,580
which in turn means that the determinant of this modified matrix must be zero.

17
00:01:06,120 --> 00:01:08,808
Okay, that's all a little bit of a mouthful to say, but again, 

18
00:01:08,808 --> 00:01:11,540
I'm assuming that all of this is review for any of you watching.

19
00:01:12,820 --> 00:01:17,297
So, the usual way to compute eigenvalues, how I used to do it and how I believe 

20
00:01:17,297 --> 00:01:21,494
most students are taught to carry it out, is to subtract the unknown value 

21
00:01:21,494 --> 00:01:25,860
lambda off the diagonals, and then solve for the determinant is equal to zero.

22
00:01:27,760 --> 00:01:31,929
Doing this always involves a few extra steps to expand out and simplify to get a 

23
00:01:31,929 --> 00:01:36,460
clean quadratic polynomial, what's known as the characteristic polynomial of the matrix.

24
00:01:37,360 --> 00:01:39,924
The eigenvalues are the roots of this polynomial, 

25
00:01:39,924 --> 00:01:42,847
so to find them you have to apply the quadratic formula, 

26
00:01:42,847 --> 00:01:46,540
which itself typically requires one or two more steps of simplification.

27
00:01:47,760 --> 00:01:51,684
Honestly, the process isn't terrible, but at least for two by two matrices, 

28
00:01:51,684 --> 00:01:54,680
there is a much more direct way you can get at the answer.

29
00:01:55,400 --> 00:01:57,859
And if you want to rediscover this trick, there's only three 

30
00:01:57,859 --> 00:02:00,440
relevant facts you need to know, each of which is worth knowing 

31
00:02:00,440 --> 00:02:02,900
in its own right and can help you with other problem solving.

32
00:02:03,820 --> 00:02:08,650
Number one, the trace of a matrix, which is the sum of these two diagonal entries, 

33
00:02:08,650 --> 00:02:10,919
is equal to the sum of the eigenvalues.

34
00:02:11,700 --> 00:02:14,723
Or, another way to phrase it, more useful for our purposes, 

35
00:02:14,723 --> 00:02:18,603
is that the mean of the two eigenvalues is the same as the mean of these two 

36
00:02:18,603 --> 00:02:19,460
diagonal entries.

37
00:02:21,000 --> 00:02:25,649
Number two, the determinant of a matrix, our usual ad-bc formula, 

38
00:02:25,649 --> 00:02:28,960
is equal to the product of the two eigenvalues.

39
00:02:30,060 --> 00:02:33,976
And this should kind of make sense if you understand that eigenvalues describe 

40
00:02:33,976 --> 00:02:37,149
how much an operator stretches space in a particular direction, 

41
00:02:37,149 --> 00:02:41,214
and that the determinant describes how much an operator scales areas, or volumes, 

42
00:02:41,214 --> 00:02:41,760
as a whole.

43
00:02:42,800 --> 00:02:45,980
Now before getting to the third fact, notice how you can essentially read 

44
00:02:45,980 --> 00:02:49,160
these first two values out of the matrix without really writing much down.

45
00:02:49,760 --> 00:02:51,320
Take this matrix here as an example.

46
00:02:51,820 --> 00:02:54,542
Straight away, you can know that the mean of the 

47
00:02:54,542 --> 00:02:57,820
eigenvalues is the same as the mean of 8 and 6, which is 7.

48
00:02:59,580 --> 00:03:03,249
Likewise, most linear algebra students are pretty well practiced at 

49
00:03:03,249 --> 00:03:07,080
finding the determinant, which in this case works out to be 48 minus 8.

50
00:03:08,240 --> 00:03:11,700
So right away, you know that the product of the two eigenvalues is 40.

51
00:03:12,780 --> 00:03:16,687
Now take a moment to see if you can derive what will be our third relevant fact, 

52
00:03:16,687 --> 00:03:19,485
which is how you can quickly recover two numbers when you 

53
00:03:19,485 --> 00:03:21,560
know their mean and you know their product.

54
00:03:22,460 --> 00:03:23,720
Here, let's focus on this example.

55
00:03:24,200 --> 00:03:27,988
You know that the two values are evenly spaced around the number 7, 

56
00:03:27,988 --> 00:03:32,780
so they look like 7 plus or minus something, let's call that something d for distance.

57
00:03:33,560 --> 00:03:36,380
You also know that the product of these two numbers is 40.

58
00:03:38,600 --> 00:03:41,719
Now to find d, notice that this product expands really nicely, 

59
00:03:41,719 --> 00:03:43,700
it works out as a difference of squares.

60
00:03:44,560 --> 00:03:46,860
So from there, you can find d.

61
00:03:48,200 --> 00:03:53,400
d squared is 7 squared minus 40, or 9, which means that d itself is 3.

62
00:03:56,380 --> 00:04:01,100
In other words, the two values for this very specific example work out to be 4 and 10.

63
00:04:01,680 --> 00:04:05,607
But our goal is a quick trick, and you wouldn't want to think through this each time, 

64
00:04:05,607 --> 00:04:08,120
so let's wrap up what we just did in a general formula.

65
00:04:08,640 --> 00:04:12,585
For any mean m and product p, the distance squared 

66
00:04:12,585 --> 00:04:15,680
is always going to be m squared minus p.

67
00:04:17,560 --> 00:04:21,293
This gives the third key fact, which is that when two numbers 

68
00:04:21,293 --> 00:04:25,087
have a mean m and a product p, you can write those two numbers 

69
00:04:25,087 --> 00:04:28,460
as m plus or minus the square root of m squared minus p.

70
00:04:30,100 --> 00:04:33,421
This is decently fast to re-derive on the fly if you ever forget it, 

71
00:04:33,421 --> 00:04:37,080
and it's essentially just a rephrasing of the difference of squares formula.

72
00:04:37,860 --> 00:04:41,220
But even still, it's a fact that's worth memorizing so it's at the tip of your fingers.

73
00:04:41,220 --> 00:04:44,237
In fact, my friend Tim from the channel A Capella Science wrote 

74
00:04:44,237 --> 00:04:47,160
us a nice quick jingle to make it a little bit more memorable.

75
00:04:51,900 --> 00:04:57,620
Let me show you how this works, say for the matrix 3 1 4 1.

76
00:04:58,100 --> 00:05:01,820
You start by bringing to mind the formula, maybe stating it all in your head.

77
00:05:06,200 --> 00:05:11,620
But when you write it down, you fill in the appropriate values for m and p as you go.

78
00:05:12,340 --> 00:05:17,147
So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, 

79
00:05:17,147 --> 00:05:20,696
which is 2, so the thing you start writing is 2 plus or minus 

80
00:05:20,696 --> 00:05:22,700
the square root of 2 squared minus.

81
00:05:23,540 --> 00:05:27,229
Then the product of the eigenvalues is the determinant, 

82
00:05:27,229 --> 00:05:31,644
which in this example is 3 times 1 minus 1 times 4, or negative 1, 

83
00:05:31,644 --> 00:05:36,783
so that's the final thing you fill in, which means the eigenvalues are 2 plus 

84
00:05:36,783 --> 00:05:38,760
or minus the square root of 5.

85
00:05:40,300 --> 00:05:43,849
You might recognize that this is the same matrix I was using at the beginning, 

86
00:05:43,849 --> 00:05:46,500
but notice how much more directly we can get at the answer.

87
00:05:48,140 --> 00:05:49,180
Here, try another one.

88
00:05:49,440 --> 00:05:54,480
This time, the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5.

89
00:05:55,100 --> 00:05:59,220
So again, you start writing out the formula, but this time writing 5 in place of m.

90
00:06:02,980 --> 00:06:08,300
And then the determinant is 2 times 8 minus 7 times 1, or 9.

91
00:06:09,520 --> 00:06:15,402
So in this example, the eigenvalues look like 5 plus or minus the square root of 16, 

92
00:06:15,402 --> 00:06:18,240
which simplifies even further as 9 and 1.

93
00:06:19,420 --> 00:06:21,914
You see what I mean about how you can basically just start 

94
00:06:21,914 --> 00:06:24,620
writing down the eigenvalues while you're staring at the matrix?

95
00:06:25,280 --> 00:06:28,160
It's typically just the tiniest bit of simplification at the end.

96
00:06:29,060 --> 00:06:32,412
Honestly, I've found myself using this trick a lot when I'm sketching quick 

97
00:06:32,412 --> 00:06:35,720
notes related to linear algebra and want to use small matrices as examples.

98
00:06:36,180 --> 00:06:38,612
I've been working on a video about matrix exponents, 

99
00:06:38,612 --> 00:06:41,688
where eigenvalues pop up a lot, and I realize it's just very handy 

100
00:06:41,688 --> 00:06:44,855
if students can read out the eigenvalues from small examples without 

101
00:06:44,855 --> 00:06:48,620
losing the main line of thought by getting bogged down in a different calculation.

102
00:06:49,740 --> 00:06:53,441
As another fun example, take a look at this set of three different matrices, 

103
00:06:53,441 --> 00:06:55,460
which comes up a lot in quantum mechanics.

104
00:06:55,760 --> 00:06:57,520
They're known as the Pauli spin matrices.

105
00:06:58,600 --> 00:07:01,465
If you know quantum mechanics, you'll know that the eigenvalues 

106
00:07:01,465 --> 00:07:04,420
of matrices are highly relevant to the physics that they describe.

107
00:07:05,220 --> 00:07:08,240
And if you don't know quantum mechanics, let this just be a little glimpse 

108
00:07:08,240 --> 00:07:11,220
of how these computations are actually very relevant to real applications.

109
00:07:12,540 --> 00:07:15,880
The mean of the diagonal entries in all three cases is zero.

110
00:07:17,560 --> 00:07:20,688
So the mean of the eigenvalues in all of these cases is zero, 

111
00:07:20,688 --> 00:07:23,060
which makes our formula look especially simple.

112
00:07:25,380 --> 00:07:28,800
What about the products of the eigenvalues, the determinants of these matrices?

113
00:07:29,700 --> 00:07:32,560
For the first one, it's 0, minus 1, or negative 1.

114
00:07:33,200 --> 00:07:35,792
The second one also looks like 0, minus 1, but it takes 

115
00:07:35,792 --> 00:07:38,200
a moment more to see because of the complex numbers.

116
00:07:38,840 --> 00:07:41,360
And the final one looks like negative 1, minus 0.

117
00:07:42,060 --> 00:07:45,920
So in all cases, the eigenvalues simplify to be plus and minus 1.

118
00:07:46,720 --> 00:07:50,000
Although in this case, you really don't need a formula to find two values if 

119
00:07:50,000 --> 00:07:53,280
you know that they're evenly spaced around 0 and their product is negative 1.

120
00:07:54,640 --> 00:07:57,608
If you're curious, in the context of quantum mechanics, 

121
00:07:57,608 --> 00:08:02,166
these matrices describe observations you might make about a particle's spin in the x, 

122
00:08:02,166 --> 00:08:03,120
y, or z direction.

123
00:08:03,560 --> 00:08:07,907
And the fact that their eigenvalues are plus and minus 1 corresponds with the idea 

124
00:08:07,907 --> 00:08:12,306
that the values for the spin that you would observe would be either entirely in one 

125
00:08:12,306 --> 00:08:17,020
direction or entirely in another, as opposed to something continuously ranging in between.

126
00:08:18,320 --> 00:08:21,817
Maybe you'd wonder how exactly this works, or why you would use 2x2 

127
00:08:21,817 --> 00:08:25,520
matrices that have complex numbers to describe spin in three dimensions.

128
00:08:26,100 --> 00:08:29,760
Those would be fair questions, just outside the scope of what I want to talk about here.

129
00:08:30,480 --> 00:08:34,093
You know, it's funny, I wrote this section because I wanted some case where 

130
00:08:34,093 --> 00:08:37,611
you have 2x2 matrices that aren't just toy examples or homework problems, 

131
00:08:37,611 --> 00:08:41,700
ones where they actually come up in practice, and quantum mechanics is great for that.

132
00:08:41,700 --> 00:08:44,886
The thing is, after I made it, I realized that the whole 

133
00:08:44,886 --> 00:08:48,240
example kind of undercuts the point that I'm trying to make.

134
00:08:48,740 --> 00:08:52,285
For these specific matrices, when you use the traditional method, 

135
00:08:52,285 --> 00:08:56,100
the one with characteristic polynomials, it's essentially just as fast.

136
00:08:56,220 --> 00:08:57,640
It might actually be faster.

137
00:08:58,240 --> 00:08:59,400
I mean, take a look at the first one.

138
00:08:59,680 --> 00:09:03,881
The relevant determinant directly gives you a characteristic polynomial 

139
00:09:03,881 --> 00:09:08,200
of lambda squared minus 1, and clearly that has roots of plus and minus 1.

140
00:09:08,840 --> 00:09:11,760
Same answer when you do the second matrix, lambda squared minus 1.

141
00:09:13,880 --> 00:09:17,287
And as for the last matrix, forget about doing any computations, 

142
00:09:17,287 --> 00:09:20,328
traditional or otherwise, it's already a diagonal matrix, 

143
00:09:20,328 --> 00:09:22,740
so those diagonal entries are the eigenvalues.

144
00:09:24,300 --> 00:09:26,920
However, the example is not totally lost to our cause.

145
00:09:27,380 --> 00:09:30,954
Where you will actually feel the speedup is in the more general case, 

146
00:09:30,954 --> 00:09:35,243
where you take a linear combination of these three matrices and then try to compute 

147
00:09:35,243 --> 00:09:36,060
the eigenvalues.

148
00:09:36,820 --> 00:09:39,590
You might write this as a times the first one, 

149
00:09:39,590 --> 00:09:42,420
plus b times the second, plus c times the third.

150
00:09:43,020 --> 00:09:46,150
In quantum mechanics, this would describe spin observations 

151
00:09:46,150 --> 00:09:49,280
in a general direction of a vector with coordinates a, b, c.

152
00:09:50,900 --> 00:09:54,481
More specifically, you should assume that this vector is normalized, 

153
00:09:54,481 --> 00:09:57,700
meaning a squared plus b squared plus c squared is equal to 1.

154
00:09:58,600 --> 00:10:01,295
When you look at this new matrix, it's immediate 

155
00:10:01,295 --> 00:10:04,100
to see that the mean of the eigenvalues is still 0.

156
00:10:04,600 --> 00:10:07,880
And you might also enjoy pausing for a brief moment to confirm 

157
00:10:07,880 --> 00:10:10,900
that the product of those eigenvalues is still negative 1.

158
00:10:13,260 --> 00:10:15,920
And then from there, concluding what the eigenvalues must be.

159
00:10:17,220 --> 00:10:20,283
And this time, the characteristic polynomial approach would be by 

160
00:10:20,283 --> 00:10:23,580
comparison a lot more cumbersome, definitely harder to do in your head.

161
00:10:25,080 --> 00:10:28,089
To be clear, using the mean product formula is not fundamentally 

162
00:10:28,089 --> 00:10:30,960
different from finding roots of the characteristic polynomial.

163
00:10:31,340 --> 00:10:33,440
I mean, it can't be, they're solving the same problem.

164
00:10:34,160 --> 00:10:36,442
One way to think about this actually is that the mean 

165
00:10:36,442 --> 00:10:39,020
product formula is a nice way to solve quadratics in general.

166
00:10:39,600 --> 00:10:41,660
And some viewers of the channel may recognize this.

167
00:10:42,540 --> 00:10:45,836
Think about it, when you're trying to find the roots of a quadratic, 

168
00:10:45,836 --> 00:10:49,991
given the coefficients, that's another situation where you know the sum of two values, 

169
00:10:49,991 --> 00:10:54,100
and you also know their product, but you're trying to recover the original two values.

170
00:10:55,560 --> 00:11:00,046
Specifically, if the polynomial is normalized, so that this leading coefficient is 1, 

171
00:11:00,046 --> 00:11:04,323
then the mean of the roots will be negative 1 half times this linear coefficient, 

172
00:11:04,323 --> 00:11:06,880
which is negative 1 times the sum of those roots.

173
00:11:08,020 --> 00:11:10,180
With the example on the screen, that makes the mean 5.

174
00:11:11,980 --> 00:11:15,479
And the product of the roots is even easier, it's just the constant term, 

175
00:11:15,479 --> 00:11:16,520
no adjustments needed.

176
00:11:17,340 --> 00:11:20,900
So from there, you would apply the mean product formula, and that gives you the roots.

177
00:11:25,140 --> 00:11:27,818
And on the one hand, you could think of this as a lighter 

178
00:11:27,818 --> 00:11:30,220
weight version of the traditional quadratic formula.

179
00:11:30,960 --> 00:11:34,042
But the real advantage is not just that it's fewer symbols to memorize, 

180
00:11:34,042 --> 00:11:36,440
it's that each one of them carries more meaning with it.

181
00:11:36,940 --> 00:11:40,610
I mean, the whole point of this eigenvalue trick is that because you can read 

182
00:11:40,610 --> 00:11:43,528
out the mean and product directly from looking at the matrix, 

183
00:11:43,528 --> 00:11:47,482
you don't need to go through the intermediate step of setting up the characteristic 

184
00:11:47,482 --> 00:11:48,000
polynomial.

185
00:11:48,420 --> 00:11:51,118
You can jump straight to writing down the roots without ever 

186
00:11:51,118 --> 00:11:53,640
explicitly thinking about what the polynomial looks like.

187
00:11:53,840 --> 00:11:56,330
But to do that, we need a version of the quadratic 

188
00:11:56,330 --> 00:11:58,820
formula where the terms carry some kind of meaning.

189
00:12:00,380 --> 00:12:03,457
I realize this is a very specific trick for a very specific audience, 

190
00:12:03,457 --> 00:12:06,534
but it's something I wish I knew in college, so if you happen to know 

191
00:12:06,534 --> 00:12:09,700
any students who might benefit from this, consider sharing it with them.

192
00:12:10,280 --> 00:12:13,246
The hope is that it's not just one more thing that you memorize, 

193
00:12:13,246 --> 00:12:16,807
but that the framing reinforces some other nice facts that are worth knowing, 

194
00:12:16,807 --> 00:12:19,820
like how the trace and the determinant are related to eigenvalues.

195
00:12:20,560 --> 00:12:23,502
If you want to prove those facts, by the way, take a moment to 

196
00:12:23,502 --> 00:12:26,444
expand out the characteristic polynomial for a general matrix, 

197
00:12:26,444 --> 00:12:29,620
and then think hard about the meaning of each of these coefficients.

198
00:12:32,400 --> 00:12:35,192
Many thanks to Tim for ensuring that this mean product formula 

199
00:12:35,192 --> 00:12:37,940
will stay stuck in all of our heads for at least a few months.

200
00:12:41,700 --> 00:12:46,000
If you don't know about alcappella science, please do check it out.

201
00:12:46,280 --> 00:12:49,580
The molecular shape of you in particular is one of the greatest things on the internet.

