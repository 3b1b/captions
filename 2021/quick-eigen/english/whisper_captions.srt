1
00:00:00,000 --> 00:00:03,960
This is a video for anyone who already knows what eigenvalues and eigenvectors are,

2
00:00:04,200 --> 00:00:07,560
and who might enjoy a quick way to compute them in the case of 2x2 matrices.

3
00:00:08,580 --> 00:00:12,000
If you're unfamiliar with eigenvalues, go ahead and take a look at this video here,

4
00:00:12,100 --> 00:00:16,680
which is actually meant to introduce them. You can skip ahead if all you want to do is see

5
00:00:16,680 --> 00:00:21,080
the trick, but if possible I'd like you to rediscover it for yourself. So for that,

6
00:00:21,140 --> 00:00:25,400
let's lay out a little background. As a quick reminder, if the effect of a linear

7
00:00:25,400 --> 00:00:31,140
transformation on a given vector is to scale that vector by some constant, we call it an

8
00:00:31,140 --> 00:00:36,640
eigenvector of the transformation, and we call the relevant scaling factor the corresponding eigenvalue,

9
00:00:37,100 --> 00:00:42,160
often denoted with the letter lambda. When you write this as an equation, and you rearrange a

10
00:00:42,160 --> 00:00:55,380
little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the

11
00:00:55,380 --> 00:01:01,420
eigenvector is then the corresponding eigenvector to the zero vector, which in turn means that the

12
00:01:01,420 --> 00:01:08,160
determinant of this modified matrix must be zero. Okay, that's all a little bit of a mouthful to say,

13
00:01:08,280 --> 00:01:11,540
but again, I'm assuming that all of this is review for any of you watching.

14
00:01:12,820 --> 00:01:18,460
So, the usual way to compute eigenvalues, how I used to do it and how I believe most students are

15
00:01:18,460 --> 00:01:24,200
taught to carry it out, is to subtract the unknown value lambda off the diagonals, and then solve for

16
00:01:24,200 --> 00:01:30,660
the determinant is equal to zero. Doing this always involves a few extra steps to expand out

17
00:01:30,660 --> 00:01:35,860
and simplify to get a clean quadratic polynomial, what's known as the characteristic polynomial of

18
00:01:35,860 --> 00:01:41,800
the matrix. The eigenvalues are the roots of this polynomial, so to find them you have to apply the

19
00:01:41,800 --> 00:01:48,060
quadratic formula, which itself typically requires one or two more steps of simplification. Honestly,

20
00:01:48,140 --> 00:01:53,520
the process isn't terrible, but at least for two by two matrices, there is a much more direct way

21
00:01:53,520 --> 00:01:57,740
you can get at the answer. And if you want to rediscover this trick, there's only three

22
00:01:57,740 --> 00:02:01,900
relevant facts you need to know, each of which is worth knowing in its own right and can help you

23
00:02:01,900 --> 00:02:07,820
with other problem solving. Number one, the trace of a matrix, which is the sum of these two diagonal

24
00:02:07,820 --> 00:02:13,860
entries, is equal to the sum of the eigenvalues. Or, another way to phrase it, more useful for our

25
00:02:13,860 --> 00:02:18,980
purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal

26
00:02:18,980 --> 00:02:27,660
entries. Number two, the determinant of a matrix, our usual ad-bc formula, is equal to the product

27
00:02:27,660 --> 00:02:32,480
of the two eigenvalues. And this should kind of make sense if you understand that eigenvalues

28
00:02:32,480 --> 00:02:37,640
describe how much an operator stretches space in a particular direction, and that the determinant

29
00:02:37,640 --> 00:02:43,600
describes how much an operator scales areas, or volumes, as a whole. Now before getting to the

30
00:02:43,600 --> 00:02:48,100
third fact, notice how you can essentially read these first two values out of the matrix without

31
00:02:48,100 --> 00:02:53,080
really writing much down. Take this matrix here as an example. Straight away, you can know that

32
00:02:53,080 --> 00:03:00,860
the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7. Likewise, most linear

33
00:03:00,860 --> 00:03:05,100
algebra students are pretty well practiced at finding the determinant, which in this case works

34
00:03:05,100 --> 00:03:11,700
out to be 48 minus 8. So right away, you know that the product of the two eigenvalues is 40.

35
00:03:12,780 --> 00:03:16,560
Now take a moment to see if you can derive what will be our third relevant fact,

36
00:03:16,560 --> 00:03:21,140
which is how you can quickly recover two numbers when you know their mean and you know their

37
00:03:21,140 --> 00:03:26,660
product. Here, let's focus on this example. You know that the two values are evenly spaced around

38
00:03:26,660 --> 00:03:32,340
the number 7, so they look like 7 plus or minus something, let's call that something d for

39
00:03:32,340 --> 00:03:40,420
distance. You also know that the product of these two numbers is 40. Now to find d, notice that this

40
00:03:40,420 --> 00:03:45,700
product expands really nicely, it works out as a difference of squares. So from there, you can

41
00:03:45,700 --> 00:03:53,400
find d. d squared is 7 squared minus 40, or 9, which means that d itself is 3.

42
00:03:56,380 --> 00:04:02,000
In other words, the two values for this very specific example work out to be 4 and 10. But

43
00:04:02,000 --> 00:04:06,400
our goal is a quick trick, and you wouldn't want to think through this each time, so let's wrap

44
00:04:06,400 --> 00:04:13,340
up what we just did in a general formula. For any mean m and product p, the distance squared is

45
00:04:13,340 --> 00:04:20,780
always going to be m squared minus p. This gives the third key fact, which is that when two numbers

46
00:04:20,780 --> 00:04:27,080
have a mean m and a product p, you can write those two numbers as m plus or minus the square root of

47
00:04:27,080 --> 00:04:33,660
m squared minus p. This is decently fast to re-derive on the fly if you ever forget it,

48
00:04:33,900 --> 00:04:37,080
and it's essentially just a rephrasing of the difference of squares formula.

49
00:04:37,860 --> 00:04:41,220
But even still, it's a fact that's worth memorizing so it's at the tip of your fingers.

50
00:04:41,220 --> 00:04:46,000
In fact, my friend Tim from the channel A Capella Science wrote us a nice quick jingle to make it

51
00:04:46,000 --> 00:04:57,620
a little bit more memorable. Let me show you how this works, say for the matrix 3 1 4 1.

52
00:04:58,100 --> 00:05:01,820
You start by bringing to mind the formula, maybe stating it all in your head.

53
00:05:06,200 --> 00:05:11,620
But when you write it down, you fill in the appropriate values for m and p as you go.

54
00:05:12,340 --> 00:05:17,740
So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2,

55
00:05:18,300 --> 00:05:22,700
so the thing you start writing is 2 plus or minus the square root of 2 squared minus.

56
00:05:23,540 --> 00:05:29,020
Then the product of the eigenvalues is the determinant, which in this example is 3 times 1

57
00:05:29,020 --> 00:05:34,480
minus 1 times 4, or negative 1, so that's the final thing you fill in,

58
00:05:34,880 --> 00:05:38,760
which means the eigenvalues are 2 plus or minus the square root of 5.

59
00:05:40,300 --> 00:05:43,560
You might recognize that this is the same matrix I was using at the beginning,

60
00:05:43,680 --> 00:05:46,500
but notice how much more directly we can get at the answer.

61
00:05:48,140 --> 00:05:53,420
Here, try another one. This time, the mean of the eigenvalues is the same as the mean of 2 and 8,

62
00:05:53,420 --> 00:05:59,220
which is 5. So again, you start writing out the formula, but this time writing 5 in place of m.

63
00:06:02,980 --> 00:06:08,300
And then the determinant is 2 times 8 minus 7 times 1, or 9.

64
00:06:09,520 --> 00:06:14,300
So in this example, the eigenvalues look like 5 plus or minus the square root of 16,

65
00:06:15,440 --> 00:06:18,240
which simplifies even further as 9 and 1.

66
00:06:19,420 --> 00:06:22,580
You see what I mean about how you can basically just start writing down the

67
00:06:22,580 --> 00:06:24,620
eigenvalues while you're staring at the matrix?

68
00:06:25,280 --> 00:06:28,160
It's typically just the tiniest bit of simplification at the end.

69
00:06:29,060 --> 00:06:32,820
Honestly, I've found myself using this trick a lot when I'm sketching quick notes related

70
00:06:32,820 --> 00:06:35,720
to linear algebra and want to use small matrices as examples.

71
00:06:36,180 --> 00:06:40,140
I've been working on a video about matrix exponents, where eigenvalues pop up a lot,

72
00:06:40,200 --> 00:06:44,840
and I realize it's just very handy if students can read out the eigenvalues from small examples

73
00:06:44,840 --> 00:06:48,620
without losing the main line of thought by getting bogged down in a different calculation.

74
00:06:49,740 --> 00:06:54,740
As another fun example, take a look at this set of three different matrices, which comes up a lot in

75
00:06:54,740 --> 00:06:57,520
quantum mechanics. They're known as the Pauli spin matrices.

76
00:06:58,600 --> 00:07:03,060
If you know quantum mechanics, you'll know that the eigenvalues of matrices are highly relevant

77
00:07:03,060 --> 00:07:06,680
to the physics that they describe. And if you don't know quantum mechanics,

78
00:07:06,900 --> 00:07:10,640
let this just be a little glimpse of how these computations are actually very relevant to real

79
00:07:10,640 --> 00:07:15,880
applications. The mean of the diagonal entries in all three cases is zero.

80
00:07:17,560 --> 00:07:22,200
So the mean of the eigenvalues in all of these cases is zero, which makes our formula look

81
00:07:22,200 --> 00:07:27,240
especially simple. What about the products of the eigenvalues,

82
00:07:27,520 --> 00:07:32,560
the determinants of these matrices? For the first one, it's 0, minus 1, or negative 1.

83
00:07:33,200 --> 00:07:37,320
The second one also looks like 0, minus 1, but it takes a moment more to see because of

84
00:07:37,320 --> 00:07:40,760
the complex numbers. And the final one looks like negative 1,

85
00:07:40,760 --> 00:07:44,460
minus 0. So in all cases, the eigenvalues simplify

86
00:07:44,460 --> 00:07:49,440
to be plus and minus 1. Although in this case, you really don't need a formula to find two

87
00:07:49,440 --> 00:07:53,280
values if you know that they're evenly spaced around 0 and their product is negative 1.

88
00:07:54,640 --> 00:07:58,940
If you're curious, in the context of quantum mechanics, these matrices describe

89
00:07:58,940 --> 00:08:03,120
observations you might make about a particle's spin in the x, y, or z direction.

90
00:08:03,560 --> 00:08:08,280
And the fact that their eigenvalues are plus and minus 1 corresponds with the idea that the

91
00:08:08,280 --> 00:08:12,660
values for the spin that you would observe would be either entirely in one direction

92
00:08:12,660 --> 00:08:17,020
or entirely in another, as opposed to something continuously ranging in between.

93
00:08:18,320 --> 00:08:22,920
Maybe you'd wonder how exactly this works, or why you would use 2x2 matrices that have complex

94
00:08:22,920 --> 00:08:28,300
numbers to describe spin in three dimensions. Those would be fair questions, just outside the

95
00:08:28,300 --> 00:08:31,980
scope of what I want to talk about here. You know, it's funny, I wrote this section

96
00:08:31,980 --> 00:08:37,100
because I wanted some case where you have 2x2 matrices that aren't just toy examples or homework

97
00:08:37,100 --> 00:08:41,700
problems, ones where they actually come up in practice, and quantum mechanics is great for that.

98
00:08:41,700 --> 00:08:47,300
The thing is, after I made it, I realized that the whole example kind of undercuts the point

99
00:08:47,300 --> 00:08:50,420
that I'm trying to make. For these specific matrices,

100
00:08:50,880 --> 00:08:54,120
when you use the traditional method, the one with characteristic polynomials,

101
00:08:54,460 --> 00:08:57,640
it's essentially just as fast. It might actually be faster.

102
00:08:58,240 --> 00:09:01,880
I mean, take a look at the first one. The relevant determinant directly gives you

103
00:09:01,880 --> 00:09:07,700
a characteristic polynomial of lambda squared minus 1, and clearly that has roots of plus and

104
00:09:07,700 --> 00:09:11,760
minus 1. Same answer when you do the second matrix, lambda squared minus 1.

105
00:09:13,880 --> 00:09:18,220
And as for the last matrix, forget about doing any computations, traditional or otherwise,

106
00:09:18,480 --> 00:09:22,740
it's already a diagonal matrix, so those diagonal entries are the eigenvalues.

107
00:09:24,300 --> 00:09:29,380
However, the example is not totally lost to our cause. Where you will actually feel the speedup

108
00:09:29,380 --> 00:09:33,800
is in the more general case, where you take a linear combination of these three matrices

109
00:09:33,800 --> 00:09:39,160
and then try to compute the eigenvalues. You might write this as a times the first one,

110
00:09:39,180 --> 00:09:45,300
plus b times the second, plus c times the third. In quantum mechanics, this would describe spin

111
00:09:45,300 --> 00:09:49,280
observations in a general direction of a vector with coordinates a, b, c.

112
00:09:50,900 --> 00:09:54,060
More specifically, you should assume that this vector is normalized,

113
00:09:54,500 --> 00:09:57,700
meaning a squared plus b squared plus c squared is equal to 1.

114
00:09:58,600 --> 00:10:03,680
When you look at this new matrix, it's immediate to see that the mean of the eigenvalues is still

115
00:10:03,680 --> 00:10:08,380
0. And you might also enjoy pausing for a brief moment to confirm that the product of those

116
00:10:08,380 --> 00:10:15,920
eigenvalues is still negative 1. And then from there, concluding what the eigenvalues must be.

117
00:10:17,220 --> 00:10:21,580
And this time, the characteristic polynomial approach would be by comparison a lot more

118
00:10:21,580 --> 00:10:27,740
cumbersome, definitely harder to do in your head. To be clear, using the mean product formula is not

119
00:10:27,740 --> 00:10:32,280
fundamentally different from finding roots of the characteristic polynomial. I mean, it can't be,

120
00:10:32,280 --> 00:10:36,380
they're solving the same problem. One way to think about this actually is that the mean product

121
00:10:36,380 --> 00:10:40,820
formula is a nice way to solve quadratics in general. And some viewers of the channel may

122
00:10:40,820 --> 00:10:45,840
recognize this. Think about it, when you're trying to find the roots of a quadratic, given the

123
00:10:45,840 --> 00:10:50,880
coefficients, that's another situation where you know the sum of two values, and you also know

124
00:10:50,880 --> 00:10:56,440
their product, but you're trying to recover the original two values. Specifically, if the

125
00:10:56,440 --> 00:11:02,020
polynomial is normalized, so that this leading coefficient is 1, then the mean of the roots will

126
00:11:02,020 --> 00:11:06,880
be negative 1 half times this linear coefficient, which is negative 1 times the sum of those roots.

127
00:11:08,020 --> 00:11:14,100
With the example on the screen, that makes the mean 5. And the product of the roots is even easier,

128
00:11:14,220 --> 00:11:18,920
it's just the constant term, no adjustments needed. So from there, you would apply the mean

129
00:11:18,920 --> 00:11:27,160
product formula, and that gives you the roots. And on the one hand, you could think of this as

130
00:11:27,160 --> 00:11:32,220
a lighter weight version of the traditional quadratic formula. But the real advantage is not

131
00:11:32,220 --> 00:11:36,220
just that it's fewer symbols to memorize, it's that each one of them carries more meaning with

132
00:11:36,220 --> 00:11:41,500
it. I mean, the whole point of this eigenvalue trick is that because you can read out the mean

133
00:11:41,500 --> 00:11:46,100
and product directly from looking at the matrix, you don't need to go through the intermediate step

134
00:11:46,100 --> 00:11:50,260
of setting up the characteristic polynomial. You can jump straight to writing down the roots

135
00:11:50,260 --> 00:11:55,280
without ever explicitly thinking about what the polynomial looks like. But to do that, we need a

136
00:11:55,280 --> 00:12:01,480
version of the quadratic formula where the terms carry some kind of meaning. I realize this is a

137
00:12:01,480 --> 00:12:06,260
very specific trick for a very specific audience, but it's something I wish I knew in college, so if

138
00:12:06,260 --> 00:12:10,640
you happen to know any students who might benefit from this, consider sharing it with them. The hope

139
00:12:10,640 --> 00:12:15,380
is that it's not just one more thing that you memorize, but that the framing reinforces some

140
00:12:15,380 --> 00:12:19,280
other nice facts that are worth knowing, like how the trace and the determinant are related to

141
00:12:19,280 --> 00:12:24,140
eigenvalues. If you want to prove those facts, by the way, take a moment to expand out the

142
00:12:24,140 --> 00:12:28,840
characteristic polynomial for a general matrix, and then think hard about the meaning of each of

143
00:12:28,840 --> 00:12:36,340
these coefficients. Many thanks to Tim for ensuring that this mean product formula will stay stuck in

144
00:12:36,340 --> 00:12:44,740
all of our heads for at least a few months. If you don't know about alcappella science,

145
00:12:45,120 --> 00:12:49,280
please do check it out. The molecular shape of you in particular is one of the greatest things

146
00:12:49,280 --> 00:12:49,880
on the internet.

