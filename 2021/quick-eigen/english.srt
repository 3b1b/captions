1
00:00:00,000 --> 00:00:04,000
This is a video for anyone who already knows what eigenvalues and eigenvectors are,

2
00:00:04,000 --> 00:00:07,680
and who might enjoy a quick way to compute them in the case of 2x2 matrices.

3
00:00:08,480 --> 00:00:11,920
If you're unfamiliar with eigenvalues, go ahead and take a look at this video here,

4
00:00:11,920 --> 00:00:16,560
which is actually meant to introduce them. You can skip ahead if all you want to do is see

5
00:00:16,560 --> 00:00:20,960
the trick, but if possible I'd like you to rediscover it for yourself. So for that,

6
00:00:20,960 --> 00:00:25,360
let's lay out a little background. As a quick reminder, if the effect of a linear

7
00:00:25,360 --> 00:00:31,040
transformation on a given vector is to scale that vector by some constant, we call it an

8
00:00:31,040 --> 00:00:36,880
eigenvector of the transformation, and we call the relevant scaling factor the corresponding eigenvalue,

9
00:00:36,880 --> 00:00:42,000
often denoted with the letter lambda. When you write this as an equation, and you rearrange a

10
00:00:42,000 --> 00:00:51,200
little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the

11
00:00:51,280 --> 00:00:57,440
matrix A minus lambda times the identity must send some non-zero vector, namely the corresponding

12
00:00:57,440 --> 00:01:03,600
eigenvector, to the zero vector, which in turn means that the determinant of this modified matrix

13
00:01:03,600 --> 00:01:09,680
must be zero. Okay, that's all a little bit of a mouthful to say, but again, I'm assuming that all

14
00:01:09,680 --> 00:01:15,920
of this is review for any of you watching. So, the usual way to compute eigenvalues,

15
00:01:15,920 --> 00:01:19,680
how I used to do it and how I believe most students are taught to carry it out,

16
00:01:19,680 --> 00:01:24,880
is to subtract the unknown value lambda off the diagonals, and then solve for when the determinant

17
00:01:24,880 --> 00:01:31,680
is equal to zero. Doing this always involves a few extra steps to expand out and simplify to get a

18
00:01:31,680 --> 00:01:37,360
clean quadratic polynomial, what's known as the characteristic polynomial of the matrix. The

19
00:01:37,360 --> 00:01:42,160
eigenvalues are the roots of this polynomial, so to find them you have to apply the quadratic

20
00:01:42,160 --> 00:01:46,480
formula, which itself typically requires one or two more steps of simplification.

21
00:01:47,440 --> 00:01:53,280
Honestly, the process isn't terrible, but at least for 2x2 matrices, there is a much more direct way

22
00:01:53,280 --> 00:01:57,920
that you can get at the answer. And if you want to rediscover this trick, there's only three relevant

23
00:01:57,920 --> 00:02:01,680
facts that you need to know, each of which is worth knowing in its own right and can help you

24
00:02:01,680 --> 00:02:07,600
with other problem solving. Number one, the trace of a matrix, which is the sum of these two diagonal

25
00:02:07,600 --> 00:02:13,520
entries, is equal to the sum of the eigenvalues. Or another way to phrase it, more useful for our

26
00:02:13,760 --> 00:02:18,960
purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal

27
00:02:18,960 --> 00:02:27,760
entries. Number two, the determinant of a matrix, our usual ad-bc formula, is equal to the product

28
00:02:27,760 --> 00:02:32,480
of the two eigenvalues. And this should kind of make sense if you understand that eigenvalues

29
00:02:32,480 --> 00:02:37,520
describe how much an operator stretches space in a particular direction, and that the determinant

30
00:02:37,520 --> 00:02:43,440
describes how much an operator scales areas, or volumes, as a whole. Now before getting to the

31
00:02:43,440 --> 00:02:48,000
third fact, notice how you can essentially read these first two values out of the matrix without

32
00:02:48,000 --> 00:02:52,960
really writing much down. Take this matrix here as an example. Straight away, you can know that

33
00:02:52,960 --> 00:03:00,720
the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7. Likewise, most linear

34
00:03:00,720 --> 00:03:05,040
algebra students are pretty well practiced at finding the determinant, which in this case works

35
00:03:05,040 --> 00:03:11,680
out to be 48 minus 8. So right away, you know that the product of the two eigenvalues is 40.

36
00:03:12,720 --> 00:03:17,360
Now take a moment to see if you can derive what will be our third relevant fact, which is how you

37
00:03:17,360 --> 00:03:22,480
can quickly recover two numbers when you know their mean and you know their product. Here,

38
00:03:22,480 --> 00:03:27,760
let's focus on this example. You know that the two values are evenly spaced around the number 7,

39
00:03:27,760 --> 00:03:32,720
so they look like 7 plus or minus something, let's call that something d for distance.

40
00:03:33,680 --> 00:03:36,320
You also know that the product of these two numbers is 40.

41
00:03:38,400 --> 00:03:43,120
Now to find d, notice that this product expands really nicely, it works out as a difference of

42
00:03:43,120 --> 00:03:51,680
squares. So from there, you can directly find d. d squared is 7 squared minus 40, or 9, which means

43
00:03:51,680 --> 00:04:00,480
that d itself is 3. In other words, the two values for this very specific example work out to be 4

44
00:04:00,560 --> 00:04:05,760
and 10. But our goal is a quick trick, and you wouldn't want to think through this each time,

45
00:04:05,760 --> 00:04:11,280
so let's wrap up what we just did in a general formula. For any mean m and product p,

46
00:04:11,920 --> 00:04:15,520
the distance squared is always going to be m squared minus p.

47
00:04:17,600 --> 00:04:22,960
This gives the third key fact, which is that when two numbers have a mean m and a product p,

48
00:04:22,960 --> 00:04:28,240
you can write those two numbers as m plus or minus the square root of m squared minus p.

49
00:04:29,040 --> 00:04:32,960
This is decently fast to re-derive on the fly if you ever forget it,

50
00:04:32,960 --> 00:04:36,160
and it's essentially just a rephrasing of the difference of squares formula.

51
00:04:36,960 --> 00:04:40,880
But even still, it's a fact that's worth memorizing so it's at the tip of your fingers.

52
00:04:40,880 --> 00:04:45,200
In fact, my friend Tim from the channel A Capella Science wrote us a nice quick jingle to make it

53
00:04:45,200 --> 00:04:56,880
a little bit more memorable. Let me show you how this works, say for the matrix 3, 1, 4, 1.

54
00:04:57,120 --> 00:05:01,600
You start by bringing to mind the formula, maybe stating it all in your head.

55
00:05:06,560 --> 00:05:11,440
But when you write it down, you fill in the appropriate values for m and p as you go.

56
00:05:12,080 --> 00:05:17,520
So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2,

57
00:05:18,080 --> 00:05:22,640
so the thing you start writing is 2 plus or minus the square root of 2 squared minus,

58
00:05:23,440 --> 00:05:29,120
then the product of the eigenvalues is the determinant, which in this example is 3 times 1

59
00:05:29,120 --> 00:05:34,000
minus 1 times 4, or negative 1, so that's the final thing you fill in,

60
00:05:34,800 --> 00:05:38,640
which means the eigenvalues are 2 plus or minus the square root of 5.

61
00:05:40,160 --> 00:05:43,840
You might recognize that this is the same matrix I was using at the beginning,

62
00:05:43,840 --> 00:05:46,400
but notice how much more directly we can get at the answer.

63
00:05:48,000 --> 00:05:52,480
Here, try another one. This time, the mean of the eigenvalues is the same as the mean of

64
00:05:53,040 --> 00:05:56,960
2 and 8, which is 5. So again, you start writing out the formula,

65
00:05:56,960 --> 00:06:06,640
but this time writing 5 in place of m, and then the determinant is 2 times 8 minus 7 times 1,

66
00:06:07,360 --> 00:06:14,240
or 9. So in this example, the eigenvalues look like 5 plus or minus the square root of 16,

67
00:06:15,440 --> 00:06:21,280
which simplifies even further as 9 and 1. You see what I mean about how you can basically

68
00:06:21,520 --> 00:06:25,040
start writing down the eigenvalues while you're staring at the matrix?

69
00:06:25,040 --> 00:06:28,000
It's typically just the tiniest bit of simplification at the end.

70
00:06:28,880 --> 00:06:32,720
Honestly, I've found myself using this trick a lot when I'm sketching quick notes related

71
00:06:32,720 --> 00:06:37,280
to linear algebra and want to use small matrices as examples. I've been working on a video about

72
00:06:37,280 --> 00:06:42,560
matrix exponents, where eigenvalues pop up a lot, and I realize it's just very handy if students

73
00:06:42,560 --> 00:06:46,880
can read out the eigenvalues from small examples without losing the main line of thought by getting

74
00:06:46,880 --> 00:06:52,160
bogged down in a different calculation. As another fun example, take a look at this

75
00:06:52,160 --> 00:06:56,240
set of three different matrices, which comes up a lot in quantum mechanics. They're known as the

76
00:06:56,240 --> 00:07:02,240
Pauli spin matrices. If you know quantum mechanics, you'll know that the eigenvalues of matrices are

77
00:07:02,240 --> 00:07:07,040
highly relevant to the physics they describe. And if you don't know quantum mechanics, let this

78
00:07:07,040 --> 00:07:11,280
just be a little glimpse of how these computations are actually very relevant to real applications.

79
00:07:11,600 --> 00:07:15,840
The mean of the diagonal entries in all three cases is zero.

80
00:07:17,680 --> 00:07:22,080
So the mean of the eigenvalues in all of these cases is zero, which makes our formula look

81
00:07:22,080 --> 00:07:28,880
especially simple. What about the products of the eigenvalues, the determinants of these matrices?

82
00:07:29,520 --> 00:07:35,440
For the first one, it's 0 minus 1, or negative 1. The second one also looks like 0 minus 1,

83
00:07:35,440 --> 00:07:39,840
but it takes a moment more to see because of the complex numbers. And the final one looks

84
00:07:39,840 --> 00:07:45,840
like negative 1 minus 0. So in all cases, the eigenvalues simplify to be plus and minus 1.

85
00:07:46,560 --> 00:07:50,320
Although in this case, you really don't need a formula to find two values if you know that

86
00:07:50,320 --> 00:07:56,320
they're evenly spaced around 0 and their product is negative 1. If you're curious, in the context

87
00:07:56,320 --> 00:08:01,040
of quantum mechanics, these matrices describe observations you might make about a particle's

88
00:08:01,040 --> 00:08:06,560
spin in the x, y, or z direction. And the fact that their eigenvalues are plus and minus 1

89
00:08:06,560 --> 00:08:11,360
corresponds with the idea that the values for the spin that you would observe would be either

90
00:08:11,360 --> 00:08:16,640
entirely in one direction or entirely in another, as opposed to something continuously ranging in

91
00:08:16,640 --> 00:08:22,240
between. Maybe you'd wonder how exactly this works, or why you would use 2x2 matrices that

92
00:08:22,240 --> 00:08:27,360
have complex numbers to describe spin in three dimensions. And those would be fair questions,

93
00:08:27,360 --> 00:08:31,600
just outside the scope of what I want to talk about here. You know, it's funny, I wrote this

94
00:08:31,600 --> 00:08:36,480
section because I wanted some case where you have 2x2 matrices that aren't just toy examples,

95
00:08:36,480 --> 00:08:40,960
or homework problems, ones where they actually come up in practice, and quantum mechanics is

96
00:08:40,960 --> 00:08:46,400
great for that. But the thing is, after I made it, I realized that the whole example kind of

97
00:08:46,400 --> 00:08:51,440
undercuts the point that I'm trying to make. For these specific matrices, when you use the

98
00:08:51,440 --> 00:08:56,400
traditional method, the one with characteristic polynomials, it's essentially just as fast. It

99
00:08:56,400 --> 00:09:01,280
might actually be faster. I mean, take a look at the first one. The relevant determinant directly

100
00:09:01,280 --> 00:09:07,120
gives you a characteristic polynomial of lambda squared minus one, and clearly that has roots of

101
00:09:07,120 --> 00:09:11,680
plus and minus one. Same answer when you do the second matrix, lambda squared minus one.

102
00:09:14,000 --> 00:09:18,400
And as for the last matrix, forget about doing any computations, traditional or otherwise,

103
00:09:18,400 --> 00:09:24,720
it's already a diagonal matrix, so those diagonal entries are the eigenvalues. However,

104
00:09:24,720 --> 00:09:29,760
the example is not totally lost to our cause. Where you will actually feel the speedup is in

105
00:09:29,760 --> 00:09:34,720
the more general case, where you take a linear combination of these three matrices, and then try

106
00:09:34,720 --> 00:09:40,960
to compute the eigenvalues. You might write this as a times the first one, plus b times the second,

107
00:09:40,960 --> 00:09:46,800
plus c times the third. In quantum mechanics, this would describe spin observations in a general

108
00:09:46,800 --> 00:09:52,960
direction of a vector with coordinates a, b, c. More specifically, you should assume that this

109
00:09:52,960 --> 00:09:57,680
vector is normalized, meaning a squared plus b squared plus c squared is equal to one.

110
00:09:58,640 --> 00:10:03,600
When you look at this new matrix, it's immediate to see that the mean of the eigenvalues is still

111
00:10:03,600 --> 00:10:08,240
zero, and you might also enjoy pausing for a brief moment to confirm that the product of those

112
00:10:08,240 --> 00:10:15,680
eigenvalues is still negative one. And then from there, concluding what the eigenvalues must be.

113
00:10:17,040 --> 00:10:21,440
And this time, the characteristic polynomial approach would be by comparison a lot more

114
00:10:21,440 --> 00:10:27,600
cumbersome, definitely harder to do in your head. To be clear, using the mean product formula is not

115
00:10:28,240 --> 00:10:32,640
different from finding roots of the characteristic polynomial. I mean, it can't be, they're solving

116
00:10:32,640 --> 00:10:36,880
the same problem. One way to think about this, actually, is that the mean product formula is

117
00:10:36,880 --> 00:10:41,520
a nice way to solve quadratics in general, and some viewers of the channel may recognize this.

118
00:10:42,320 --> 00:10:46,880
Think about it. When you're trying to find the roots of a quadratic, given the coefficients,

119
00:10:46,880 --> 00:10:51,680
that's another situation where you know the sum of two values, and you also know their product,

120
00:10:51,680 --> 00:10:57,040
but you're trying to recover the original two values. Specifically, if the polynomial is

121
00:10:57,040 --> 00:11:02,560
normalized so that this leading coefficient is one, then the mean of the roots will be negative

122
00:11:02,560 --> 00:11:06,800
one half times this linear coefficient, which is negative one times the sum of those roots.

123
00:11:07,760 --> 00:11:14,160
For the example on the screen, that makes the mean five. And the product of the roots is even easier,

124
00:11:14,160 --> 00:11:18,640
it's just the constant term, no adjustments needed. So from there, you would apply the

125
00:11:18,640 --> 00:11:26,480
mean product formula, and that gives you the roots. And on the one hand, you could think of

126
00:11:26,480 --> 00:11:32,080
this as a lighter weight version of the traditional quadratic formula. But the real advantage is not

127
00:11:32,080 --> 00:11:36,160
just that it's fewer symbols to memorize, it's that each one of them carries more meaning with

128
00:11:36,160 --> 00:11:41,440
it. I mean, the whole point of this eigenvalue trick is that because you can read out the mean

129
00:11:41,440 --> 00:11:46,080
and product directly from looking at the matrix, you don't need to go through the intermediate step

130
00:11:46,080 --> 00:11:50,240
of setting up the characteristic polynomial. You can jump straight to writing down the roots

131
00:11:50,240 --> 00:11:54,560
without ever explicitly thinking about what the polynomial looks like. But to do that,

132
00:11:54,560 --> 00:11:58,800
we need a version of the quadratic formula where the terms carry some kind of meaning.

133
00:12:00,480 --> 00:12:05,040
I realize this is a very specific trick for a very specific audience, but it's something I wish I knew

134
00:12:05,040 --> 00:12:09,200
in college, so if you happen to know any students who might benefit from this, consider sharing it

135
00:12:09,200 --> 00:12:14,320
with them. The hope is that it's not just one more thing that you memorize, but that the framing

136
00:12:14,320 --> 00:12:18,560
reinforces some other nice facts that are worth knowing, like how the trace and the determinant

137
00:12:18,560 --> 00:12:23,680
are related to eigenvalues. If you want to prove those facts, by the way, take a moment to expand

138
00:12:23,680 --> 00:12:28,320
out the characteristic polynomial for a general matrix, and then think hard about the meaning

139
00:12:28,320 --> 00:12:35,680
of each of these coefficients. Many thanks to Tim for ensuring that this mean product formula will

140
00:12:35,680 --> 00:12:44,240
stay stuck in all of our heads for at least a few months. If you don't know about alcappella

141
00:12:44,240 --> 00:12:48,960
science, please do check it out. The molecular shape of you in particular is one of the greatest

142
00:12:48,960 --> 00:12:50,640
things on the internet.

