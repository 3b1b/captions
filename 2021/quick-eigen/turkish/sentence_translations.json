[
 {
  "input": "This is a video for anyone who already knows what eigenvalues and eigenvectors are, and who might enjoy a quick way to compute them in the case of 2x2 matrices. ",
  "translatedText": "Bu, özdeğerlerin ve özvektörlerin ne olduğunu zaten bilen ve bunları 2x2 matrisler durumunda hızlı bir şekilde hesaplamanın keyfini çıkarabilecek herkes için bir videodur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 7.56
 },
 {
  "input": "If you’re unfamiliar with eigenvalues, take a look at this video which introduces them. ",
  "translatedText": "Özdeğerlere aşina değilseniz, devam edin ve buradaki videoya bir göz atın; bu video aslında onları tanıtmayı amaçlamaktadır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.58,
  "end": 13.7
 },
 {
  "input": "You can skip ahead if you just want to see the trick, but if possible I’d like you to rediscover it for yourself, so for that let’s lay down a little background. ",
  "translatedText": "Tek yapmak istediğiniz hileyi görmekse ileri atlayabilirsiniz, ancak mümkünse bunu kendi başınıza yeniden keşfetmenizi isterim. Bunun için biraz arka plan hazırlayalım. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.68,
  "end": 22.38
 },
 {
  "input": "As a quick reminder, if the effect of a linear transformation on a given vector is to scale it by some constant, we call it an \"eigenvector\" of the transformation, and we call the relevant scaling factor the corresponding \"eigenvalue,\" often denoted with the letter lambda. ",
  "translatedText": "Kısa bir hatırlatma olarak, belirli bir vektör üzerindeki doğrusal dönüşümün etkisi, bu vektörü bir sabitle ölçeklendirmekse, buna dönüşümün özvektörü adını veririz ve ilgili ölçeklendirme faktörüne karşılık gelen özdeğer adını veririz ve genellikle şu harfle gösterilir: lambda. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.26,
  "end": 38.6
 },
 {
  "input": "When you write this as an equation and you rearrange a little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the matrix (A minus lambda times the identity) must send some nonzero vector, namely the corresponding eigenvector, to the zero vector, which in turn means the determinant of this modified matrix must be 0. ",
  "translatedText": "Bunu bir denklem olarak yazdığınızda ve biraz yeniden düzenlediğinizde, eğer lambda sayısı bir A matrisinin özdeğeriyse, o zaman A matrisi eksi lambda çarpı özdeşliğin sıfırdan farklı bir vektör göndermesi gerektiğini görürsünüz. karşılık gelen özvektörün sıfır vektörüne oranı, bu da bu değiştirilmiş matrisin determinantının sıfır olması gerektiği anlamına gelir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.84,
  "end": 64.58
 },
 {
  "input": "Okay, that’s all a little bit of a mouthful to say, but again, I’m assuming all of this is review for anyone watching. ",
  "translatedText": "Tamam, bunları söylemek biraz fazla ama yine de tüm bunların izleyenleriniz için bir inceleme olduğunu varsayıyorum. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.12,
  "end": 71.54
 },
 {
  "input": "So, the usual way to compute eigenvalues, how I used to do it, and how I believe most students are taught to carry it out, is to subtract the unknown value lambda off the diagonals and then solve for when the determinant equals 0. ",
  "translatedText": "Dolayısıyla, özdeğerleri hesaplamanın genel yolu, benim bunu nasıl yaptığım ve çoğu öğrenciye bunu nasıl yapmayı öğrettiğine inandığım, bilinmeyen lambda değerini köşegenlerden çıkarmak ve sonra determinantın sıfıra eşit olduğu zamanı bulmaktır. . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 72.82,
  "end": 85.86
 },
 {
  "input": "Doing this always involves a few steps to expand out and simplify to get a clean quadratic polynomial, what's known as the “characteristic polynomial” of the matrix. ",
  "translatedText": "Bunu yapmak, matrisin karakteristik polinomu olarak bilinen, temiz bir ikinci dereceden polinom elde etmek için genişletme ve basitleştirme için her zaman birkaç ekstra adım gerektirir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.76,
  "end": 96.46
 },
 {
  "input": "The eigenvalues are the roots of this polynomial. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 97.36,
  "end": 99.9
 },
 {
  "input": "So to find them you have to apply the quadratic formula, which itself typically requires one or two more steps of simplification. ",
  "translatedText": "Özdeğerler bu polinomun kökleridir, dolayısıyla onları bulmak için ikinci dereceden formülü uygulamanız gerekir; bu da genellikle bir veya iki basitleştirme adımı daha gerektirir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.1,
  "end": 106.54
 },
 {
  "input": "Honestly, the process isn’t terrible. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 107.76,
  "end": 109.5
 },
 {
  "input": "But at least for 2x2 matrices, there’s a much more direct way to get at this answer. ",
  "translatedText": "Dürüst olmak gerekirse süreç fena değil ama en azından 2x2'lik matrisler için cevaba ulaşmanın çok daha doğrudan bir yolu var. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.58,
  "end": 114.68
 },
 {
  "input": "And if you want to rediscover this trick, there are only three relevant facts you need to know, each of which is worth knowing in its own right and can help you with other problem-solving. ",
  "translatedText": "Ve eğer bu numarayı yeniden keşfetmek istiyorsanız, bilmeniz gereken yalnızca üç gerçek var; bunların her biri kendi başına bilmeye değer ve diğer problem çözmede size yardımcı olabilir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 115.4,
  "end": 122.9
 },
 {
  "input": "Number 1: The trace of a matrix, which is the sum of these two diagonal entries, is equal to the sum of the eigenvalues. ",
  "translatedText": "Birincisi, bu iki köşegen girişin toplamı olan bir matrisin izi, özdeğerlerin toplamına eşittir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 123.82,
  "end": 130.92
 },
 {
  "input": "Or another way to phrase it, more useful for our purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal entries. ",
  "translatedText": "Ya da amaçlarımız açısından daha yararlı olan başka bir ifadeyle, iki özdeğerin ortalaması bu iki köşegen girdinin ortalaması ile aynıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.7,
  "end": 139.46
 },
 {
  "input": "Number 2: The determinant of a matrix, our usual ad-bc formula, is equal to the product of the two eigenvalues. ",
  "translatedText": "İki numara, bir matrisin determinantı, her zamanki ad-bc formülümüz, iki özdeğerin çarpımına eşittir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.0,
  "end": 148.96
 },
 {
  "input": "And this should kind of make sense if you understand that eigenvalues describe how much an operator stretches space in a particular direction and that the determinant describes how much an operator scales areas (or volumes) as a whole. ",
  "translatedText": "Ve özdeğerlerin, bir operatörün uzayı belirli bir yönde ne kadar genişlettiğini tanımladığını ve determinantın, bir operatörün alanları veya hacimleri bir bütün olarak ne kadar ölçeklendirdiğini tanımladığını anlarsanız, bu bir anlam ifade etmelidir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.06,
  "end": 161.76
 },
 {
  "input": "Now before getting to the third fact, notice how you can essentially read these first two values out of the matrix without really writing much down. ",
  "translatedText": "Şimdi üçüncü gerçeğe geçmeden önce, aslında çok fazla yazmadan bu ilk iki değeri matristen nasıl okuyabileceğinize dikkat edin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 162.8,
  "end": 169.16
 },
 {
  "input": "Take this matrix here as an example. ",
  "translatedText": "Buradaki matrisi örnek olarak alın. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.76,
  "end": 171.32
 },
 {
  "input": "Straight away you can know that the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7. ",
  "translatedText": "Özdeğerlerin ortalamasının 8 ve 6'nın ortalaması ile aynı olduğunu, yani 7 olduğunu hemen bilebilirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 171.82,
  "end": 177.82
 },
 {
  "input": "Likewise, most linear algebra students are pretty well-practiced at finding the determinant, which in this case works out to be 48 - 8 So right away you know that the product of our two eigenvalues is 40. ",
  "translatedText": "Benzer şekilde, doğrusal cebir öğrencilerinin çoğu, determinantı bulma konusunda oldukça iyi uygulamalıdır; bu durumda 48 eksi 8 elde edilir. Yani hemen iki özdeğerin çarpımının 40 olduğunu biliyorsunuz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.58,
  "end": 191.7
 },
 {
  "input": "Now take a moment to see how you can derive what will be our third relevant fact, which is how to recover two numbers when you know their mean and you know their product. ",
  "translatedText": "Şimdi bir dakikanızı ayırın ve üçüncü ilgili gerçeğimizi türetip çıkaramayacağınızı görün; bu, ortalamalarını bildiğinizde ve çarpımlarını bildiğinizde iki sayıyı nasıl hızlı bir şekilde kurtarabileceğinizdir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.78,
  "end": 201.56
 },
 {
  "input": "Here, let's focus on this example. ",
  "translatedText": "Burada bu örnek üzerinde duralım. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.46,
  "end": 203.72
 },
 {
  "input": "You know the two values are evenly spaced around 7, so they look like 7 plus or minus something; let’s call that something \"d\" for distance. ",
  "translatedText": "Biliyorsunuz, iki değer 7 sayısının etrafında eşit aralıklarla yerleştirilmiştir, yani 7 artı veya eksi gibi görünürler, buna uzaklık için d diyelim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 204.2,
  "end": 212.78
 },
 {
  "input": "You also know that the product of these two numbers is 40. ",
  "translatedText": "Bu iki sayının çarpımının 40 olduğunu da biliyorsunuz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.56,
  "end": 216.38
 },
 {
  "input": "Now to find d, notice that this product expands really nicely, it works out as a difference of squares. ",
  "translatedText": "Şimdi d'yi bulmak için, bu çarpımın gerçekten güzel bir şekilde genişlediğine dikkat edin, kareler farkı olarak sonuç verir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.6,
  "end": 223.7
 },
 {
  "input": "So from there, you can directly find d: d^2 is 7^2 - 40, or 9, which means d itself is 3. ",
  "translatedText": "Yani oradan doğrudan d'yi bulabilirsiniz. d kare 7 kare eksi 40 veya 9'dur, bu da d'nin kendisinin 3 olduğu anlamına gelir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.56,
  "end": 233.4
 },
 {
  "input": "In other words, the two values for this very specific example work out to be 4 and 10. ",
  "translatedText": "Başka bir deyişle, bu çok özel örneğin iki değeri 4 ve 10 olarak çıkıyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.38,
  "end": 241.1
 },
 {
  "input": "But our goal is a quick trick, and you wouldn’t want to think this through each time, so let’s wrap up what we just did in a general formula. ",
  "translatedText": "Ancak amacımız hızlı bir numaradır ve bunu her seferinde düşünmek istemezsiniz, o yüzden az önce yaptıklarımızı genel bir formülle özetleyelim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 241.68,
  "end": 248.12
 },
 {
  "input": "For any mean, m and product, p, the distance squared is always going to be m^2 - p. ",
  "translatedText": "Herhangi bir m ve p çarpımı için mesafenin karesi her zaman m kare eksi p olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.64,
  "end": 255.68
 },
 {
  "input": "This gives the third key fact, which is that when two numbers have a mean m and a product p, you can write those two numbers as m ± sqrt(m^2 - p) This is decently fast to rederive on the fly if you ever forget it, and it’s essentially just a rephrasing of the difference of squares formula. ",
  "translatedText": "Bu üçüncü temel gerçeği verir; iki sayının ortalaması m ve çarpımı p olduğunda, bu iki sayıyı m artı veya eksi m kare eksi p'nin karekökü olarak yazabilirsiniz. Bu, unutursanız anında yeniden türetilmesi oldukça hızlıdır ve aslında sadece kareler farkı formülünün yeniden ifade edilmesidir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 257.56,
  "end": 277.08
 },
 {
  "input": "But even still it’s a fact worth memorizing so that you have it at the tip of your fingers. ",
  "translatedText": "Ama yine de ezberlemeye değer bir gerçek olduğundan parmaklarınızın ucundadır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.86,
  "end": 281.22
 },
 {
  "input": "In fact, my friend Tim from the channel acapellascience wrote us a quick jingle to make it a little more memorable. ",
  "translatedText": "Aslında, A Capella Science kanalından arkadaşım Tim, bunu biraz daha akılda kalıcı kılmak için bize güzel bir kısa şarkı yazdı. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.22,
  "end": 287.16
 },
 {
  "input": "m plus or minus squaaaare root of me squared minus p (ping!) Let me show you how this works, say for the matrix [[3,1], [4,1]]. ",
  "translatedText": "Size bunun nasıl çalıştığını göstereyim, örneğin 3, 1, 4, 1 matrisi için. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.9,
  "end": 297.62
 },
 {
  "input": "You start by bringing to mind the formula, maybe stating it all in your head. ",
  "translatedText": "Formülü aklınıza getirerek başlıyorsunuz, belki de hepsini kafanızda ifade ediyorsunuz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 298.1,
  "end": 301.82
 },
 {
  "input": "But when you write it down, you fill in the appropriate values of m and p as you go. ",
  "translatedText": "Ancak bunu yazdığınızda m ve p için uygun değerleri giderek doldurursunuz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.2,
  "end": 311.62
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 312.34,
  "end": 317.74
 },
 {
  "input": "So the thing you start writing is 2 ± sqrt(2^2 - …). ",
  "translatedText": "Yani bu örnekte, özdeğerlerin ortalaması 3 ve 1'in ortalaması ile aynıdır, yani 2, yani yazmaya başladığınız şey 2 artı veya eksi 2'nin karesinin karekökü eksi, o zaman özdeğerlerin çarpımıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 318.3,
  "end": 322.7
 },
 {
  "input": "Then the product of the eigenvalues is the determinant, which in this example is 3*1 - 1*4, or -1. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.54,
  "end": 332.14
 },
 {
  "input": "So that’s the final thing you fill in. ",
  "translatedText": "determinanttır, bu örnekte 3 çarpı 1 eksi 1 çarpı 4 veya negatif 1, yani dolduracağınız son şey bu, yani özdeğerler 2 artı veya eksi 5'in karekökü. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 332.38,
  "end": 334.48
 },
 {
  "input": "This means the eigenvalues are 2±sqrt(5). ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.88,
  "end": 338.76
 },
 {
  "input": "You might recognize that this is the same matrix I was using at the beginning, but notice how much more directly we can get at the answer. ",
  "translatedText": "Bunun başlangıçta kullandığım matrisin aynısı olduğunu fark edebilirsiniz, ancak cevaba ne kadar doğrudan ulaşabildiğimize dikkat edin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.3,
  "end": 346.5
 },
 {
  "input": "Here, try another one. ",
  "translatedText": "İşte, başka bir tane dene. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.14,
  "end": 349.18
 },
 {
  "input": "This time the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5. ",
  "translatedText": "Bu sefer özdeğerlerin ortalaması 2 ve 8'in ortalaması ile aynı yani 5 olur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 349.44,
  "end": 354.48
 },
 {
  "input": "So again, you start writing out the formula but this time writing 5 in place of m [song]. ",
  "translatedText": "Yani yine formülü yazmaya başlıyorsunuz ama bu sefer m yerine 5 yazıyorsunuz ve sonra determinant 2 çarpı 8 eksi 7 çarpı 1 veya 9 oluyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.1,
  "end": 359.22
 },
 {
  "input": "And then the determinant is 2*8 - 7*1, or 9. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.98,
  "end": 368.3
 },
 {
  "input": "So in this example, the eigenvalues look like 5 ± sqrt(16), which simplifies even further as 9 and 1. ",
  "translatedText": "Yani bu örnekte özdeğerler 5 artı veya eksi 16'nın karekökü gibi görünüyor, bu da 9 ve 1 olarak daha da basitleşiyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 369.52,
  "end": 378.24
 },
 {
  "input": "You see what I mean about how you can basically just start writing down the eigenvalues while staring at the matrix? ",
  "translatedText": "Matrise bakarken temel olarak özdeğerleri yazmaya nasıl başlayabileceğiniz konusunda ne demek istediğimi anladınız mı? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 379.42,
  "end": 384.62
 },
 {
  "input": "It’s typically just the tiniest bit of simplifying at the end. ",
  "translatedText": "Genellikle işin sonundaki en ufak bir basitleştirmedir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.28,
  "end": 388.16
 },
 {
  "input": "Honestly, I’ve found myself using this trick a lot when I’m sketching quick notes related to linear algebra and want to use small matrices as examples. ",
  "translatedText": "Dürüst olmak gerekirse, doğrusal cebirle ilgili kısa notlar çizerken ve küçük matrisleri örnek olarak kullanmak istediğimde kendimi bu numarayı çok kullanırken buldum. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 389.06,
  "end": 395.72
 },
 {
  "input": "I’ve been working on a video about matrix exponents, where eigenvalues pop up a lot, and I realized it’s just very handy if students can read off the eigenvalues from small examples without losing the main line of thought by getting bogged down in a different calculation. ",
  "translatedText": "Özdeğerlerin çok sık ortaya çıktığı matris üsleri hakkında bir video üzerinde çalışıyordum ve öğrencilerin farklı bir çıkmaza takılıp ana düşünce çizgisini kaybetmeden küçük örneklerden özdeğerleri okuyabilmelerinin çok kullanışlı olacağını fark ettim. hesaplama. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.18,
  "end": 408.62
 },
 {
  "input": "As another fun example, take a look at this set of three different matrices, which come up a lot in quantum mechanics, they're known as the Pauli spin matrices. ",
  "translatedText": "Başka bir eğlenceli örnek olarak, kuantum mekaniğinde sıklıkla karşılaşılan bu üç farklı matris kümesine bir göz atın. Pauli spin matrisleri olarak bilinirler. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.74,
  "end": 417.52
 },
 {
  "input": "If you know quantum mechanics, you’ll know that the eigenvalues of matrices are highly relevant to the physics they describe, and if you don’t know quantum mechanics, let this just be a little glimpse of how these computations are actually relevant to real applications. ",
  "translatedText": "Kuantum mekaniğini biliyorsanız, matrislerin özdeğerlerinin tanımladıkları fizikle son derece alakalı olduğunu bilirsiniz. Ve eğer kuantum mekaniğini bilmiyorsanız, izin verin bu hesaplamaların gerçek uygulamalarla ne kadar alakalı olduğuna dair küçük bir bakış olsun. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.6,
  "end": 431.22
 },
 {
  "input": "The mean of the diagonal in all three cases is 0, so the mean of the eigenvalues in all cases is 0, which makes our formula look especially simple. ",
  "translatedText": "Her üç durumda da çapraz girişlerin ortalaması sıfırdır. Yani tüm bu durumlarda özdeğerlerin ortalaması sıfırdır, bu da formülümüzün özellikle basit görünmesini sağlar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.54,
  "end": 443.06
 },
 {
  "input": "What about the products of the eigenvalues, the determinants of these matrices? ",
  "translatedText": "Peki ya bu matrislerin belirleyicileri olan özdeğerlerin çarpımları? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.38,
  "end": 448.8
 },
 {
  "input": "For the first one, it’s 0 - 1 or -1. ",
  "translatedText": "İlki için bu 0 eksi 1 veya eksi 1'dir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.7,
  "end": 453.4
 },
 {
  "input": "The second also looks like 0 - 1, but it takes a moment more to see because of the complex numbers. ",
  "translatedText": "İkincisi de 0 eksi 1'e benziyor ama karmaşık sayılar nedeniyle görülmesi biraz daha zaman alıyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.4,
  "end": 458.2
 },
 {
  "input": "And the final one looks like -1 - 0. ",
  "translatedText": "Ve sonuncusu eksi 1 eksi 0'a benziyor. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.84,
  "end": 461.36
 },
 {
  "input": "So in all cases, the eigenvalues simplify to be ±1. ",
  "translatedText": "Yani her durumda özdeğerler artı ve eksi 1 olacak şekilde basitleştirilir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.06,
  "end": 465.92
 },
 {
  "input": "Although in this case, you really don’t need the formula to find two values if you know theyr'e evenly spaced around 0 and their product is -1. ",
  "translatedText": "Ancak bu durumda, eğer bu değerlerin 0 civarında eşit aralıklı olduğunu ve çarpımlarının negatif 1 olduğunu biliyorsanız, iki değeri bulmak için gerçekten bir formüle ihtiyacınız yoktur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.72,
  "end": 473.28
 },
 {
  "input": "If you’re curious, in the context of quantum mechanics, these matrices describe observations you might make about a particle's spin in the x, y or z directions. ",
  "translatedText": "Merak ediyorsanız, kuantum mekaniği bağlamında bu matrisler, bir parçacığın x, y veya z yönündeki dönüşü hakkında yapabileceğiniz gözlemleri tanımlar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.64,
  "end": 483.76
 },
 {
  "input": "The fact that their eigenvalues are ±1 corresponds with the idea that the values for the spin that you would observe would be either entirely in one direction or entirely in another, as opposed to something continuously ranging in between. ",
  "translatedText": "Ve onların özdeğerlerinin artı ve eksi 1 olması gerçeği, gözlemleyeceğiniz dönüş değerlerinin sürekli olarak arada değişen bir şeyin aksine ya tamamen bir yönde ya da tamamen başka bir yönde olacağı fikrine karşılık gelir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.76,
  "end": 497.02
 },
 {
  "input": "Maybe you’d wonder how exactly this works, or why you would use 2x2 matrices that have complex numbers to describe spin in three dimensions. ",
  "translatedText": "Belki bunun tam olarak nasıl çalıştığını veya üç boyutlu dönüşü tanımlamak için neden karmaşık sayılara sahip 2x2 matrisleri kullandığınızı merak edersiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.32,
  "end": 505.52
 },
 {
  "input": "And those would be fair questions, just outside the scope of what I want to talk about here. ",
  "translatedText": "Ve bunlar burada konuşmak istediklerimin kapsamı dışında kalan adil sorular olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 506.1,
  "end": 509.76
 },
 {
  "input": "You know it’s funny, I wrote this section because I wanted some case where you have 2x2 matrices that are not just toy examples or homework problems, ones where they actually come up in practice, and quantum mechanics is great for that. ",
  "translatedText": "Biliyor musunuz, çok komik, bu bölümü yazdım çünkü sadece oyuncak örnekleri ya da ev ödevi problemleri olmayan, pratikte ortaya çıkan 2x2 matrislerin olduğu bir durum istedim ve kuantum mekaniği bunun için harika. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 510.48,
  "end": 521.7
 },
 {
  "input": "But the thing is after I made it I realized that the whole example kind of undercuts the point I’m trying to make. ",
  "translatedText": "Ama sorun şu ki, bunu yaptıktan sonra, tüm örneğin benim vurgulamaya çalıştığım noktayı gölgede bıraktığını fark ettim. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.7,
  "end": 528.24
 },
 {
  "input": "For these specific matrices, when you use the traditional method, the one with characteristic polynomials, it’s essentially just as fast; it might actually faster. ",
  "translatedText": "Bu spesifik matrisler için, karakteristik polinomlara sahip geleneksel yöntemi kullandığınızda, aslında aynı derecede hızlıdır. Aslında daha hızlı olabilir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.74,
  "end": 537.64
 },
 {
  "input": "I mean, take a look a the first one: The relevant determinant directly gives you a characteristic polynomial of lambda^2 - 1, and clearly, that has roots of plus and minus 1. ",
  "translatedText": "İlkine bir bakın derim. İlgili determinant size doğrudan lambda kare eksi birin karakteristik polinomunu verir ve bunun köklerinin artı ve eksi bir olduğu açıktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.24,
  "end": 548.2
 },
 {
  "input": "Same answer when you do the second matrix, lambda^2 - 1. ",
  "translatedText": "İkinci matrisi yaptığınızda da aynı cevap, lambda kare eksi bir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 548.84,
  "end": 551.76
 },
 {
  "input": "And as for the last matrix, forget about doing any computations, traditional or otherwise, it’s already a diagonal matrix, so those diagonal entries are the eigenvalues! ",
  "translatedText": "Ve son matrise gelince, geleneksel veya başka herhangi bir hesaplama yapmayı unutun, bu zaten köşegen bir matris, yani bu köşegen girişler özdeğerlerdir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.88,
  "end": 562.74
 },
 {
  "input": "However, the example is not totally lost to our cause. ",
  "translatedText": "Ancak örnek davamız açısından tamamen kaybolmuş değil. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.3,
  "end": 566.92
 },
 {
  "input": "Where you will actually feel the speed up is in the more general case where you take a linear combination of these three matrices and then try to compute the eigenvalues. ",
  "translatedText": "Hızlanmayı gerçekten hissedeceğiniz yer, bu üç matrisin doğrusal bir kombinasyonunu aldığınız ve daha sonra özdeğerleri hesaplamaya çalıştığınız daha genel durumdur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 567.38,
  "end": 576.06
 },
 {
  "input": "You might write this as a times the first one, plus b times the second, plus c times the third. ",
  "translatedText": "Bunu a çarpı birinci artı b çarpı ikinci, artı c çarpı üçüncü olarak yazabilirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.82,
  "end": 582.42
 },
 {
  "input": "In quantum mechanics, this would describe spin observations in a general direction of a vector with coordinates [a, b, c]. ",
  "translatedText": "Kuantum mekaniğinde bu, a, b, c koordinatlarına sahip bir vektörün genel yönündeki spin gözlemlerini tanımlar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.02,
  "end": 589.28
 },
 {
  "input": "More specifically, you should assume this vector is normalized, meaning a^2 + b^2 + c^2 = 1. ",
  "translatedText": "Daha spesifik olarak, bu vektörün normalleştirilmiş olduğunu, yani a kare artı b kare artı c karenin bire eşit olduğunu varsaymalısınız. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.9,
  "end": 597.7
 },
 {
  "input": "When you look at this new matrix, it’s immediate to see that the mean of the eigenvalues is still zero, and you might also enjoy pausing for a brief moment to confirm that the product of those eigenvalues is still -1, and then from there concluding what the eigenvalues must be. ",
  "translatedText": "Bu yeni matrise baktığınızda, özdeğerlerin ortalamasının hala sıfır olduğunu hemen görürsünüz ve ayrıca bu özdeğerlerin çarpımının hala negatif olduğunu doğrulamak için kısa bir süre duraklamanın keyfini çıkarabilirsiniz. Ve oradan, özdeğerlerin ne olması gerektiği sonucuna varıyoruz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.6,
  "end": 615.92
 },
 {
  "input": "And this time, the characteristic polynomial approach would be by comparison a lot more cumbersome, definitely harder to do in your head. ",
  "translatedText": "Ve bu kez, karakteristik polinom yaklaşımı, kıyaslandığında çok daha hantal ve kafanızda yapılması kesinlikle daha zor olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.22,
  "end": 623.58
 },
 {
  "input": "To be clear, using the mean-product formula is not fundamentally different from finding roots of the characteristic polynomial; I mean, it can't be, they're solving the same problem. ",
  "translatedText": "Açık olmak gerekirse, ortalama çarpım formülünü kullanmak, karakteristik polinomun köklerini bulmaktan farklı değildir. Yani olamaz, aynı sorunu çözüyorlar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 625.08,
  "end": 633.44
 },
 {
  "input": "One way to think about this, actually, is that the mean-product formula is a nice way to solve quadratic in general (and some viewers of the channel may recognize this). ",
  "translatedText": "Aslında bunu düşünmenin bir yolu, ortalama çarpım formülünün genel olarak ikinci dereceden denklemleri çözmenin güzel bir yolu olduğu ve kanalın bazı izleyicilerinin bunu fark edebileceğidir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.16,
  "end": 641.66
 },
 {
  "input": "This about it: When you’re trying to find the roots of a quadratic given its coefficients, that's another situation where you know the sum of two values, and you also know their product, but you’re trying to recover the original two values. ",
  "translatedText": "Bunu düşün. Katsayılar göz önüne alındığında ikinci dereceden bir ifadenin köklerini bulmaya çalıştığınızda, bu, iki değerin toplamını bildiğiniz ve aynı zamanda bunların çarpımını da bildiğiniz, ancak orijinal iki değeri kurtarmaya çalıştığınız başka bir durumdur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 642.54,
  "end": 654.1
 },
 {
  "input": "Specifically, if the polynomial is normalized so that this leading coefficient is 1, then the mean of the roots will be -½ times this linear coefficient, which is -1 times the sum of those roots. ",
  "translatedText": "Spesifik olarak, eğer polinom bu baş katsayı bir olacak şekilde normalleştirilirse, köklerin ortalaması bu doğrusal katsayının yarısı kadar negatif olacaktır; bu da köklerin toplamının bir katı negatif olacaktır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 655.56,
  "end": 666.88
 },
 {
  "input": "For the example on the screen that makes the mean 5. ",
  "translatedText": "Ekrandaki örnekte bu ortalamayı beş yapar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 668.02,
  "end": 670.18
 },
 {
  "input": "And the product of the roots is even easier, it’s just the constant term no adjustments needed. ",
  "translatedText": "Ve köklerin çarpımı daha da kolaydır, sadece sabit terimdir, hiçbir düzeltmeye gerek yoktur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.98,
  "end": 676.52
 },
 {
  "input": "So from there, you would apply the mean product formula and that gives you the roots. ",
  "translatedText": "Yani oradan ortalama çarpım formülünü uygularsınız ve bu size kökleri verir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.34,
  "end": 680.9
 },
 {
  "input": "On the one hand, you could think of this as a lighter-weight version of the traditional quadratic formula. ",
  "translatedText": "Ve bir yandan bunu geleneksel ikinci dereceden formülün daha hafif bir versiyonu olarak düşünebilirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.14,
  "end": 690.22
 },
 {
  "input": "But the real advantage is that it's fewer symbols to memorize, it's that each one of them carries more meaning with it. ",
  "translatedText": "Ancak asıl avantaj, yalnızca ezberlenecek daha az sembol olması değil, aynı zamanda her birinin daha fazla anlam taşımasıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 690.96,
  "end": 696.44
 },
 {
  "input": "The whole point of this eigenvalue trick is that because you can read out the mean and product directly from looking at the matrix, you don't need to go through the intermediate step of setting up the characteristic polynomial. ",
  "translatedText": "Demek istediğim, bu özdeğer hilesinin asıl amacı, ortalamayı ve çarpımı doğrudan matrise bakarak okuyabileceğiniz için, karakteristik polinomu oluşturmanın ara adımını geçmenize gerek olmamasıdır. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.94,
  "end": 708.0
 },
 {
  "input": "You can jump straight to writing down the roots without ever explicitly thinking about what the polynomial looks like. ",
  "translatedText": "Polinomun neye benzediğini açıkça düşünmeden doğrudan kökleri yazmaya geçebilirsiniz. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.42,
  "end": 713.64
 },
 {
  "input": "But to do that we need a version of the quadratic formula where the terms carry some kind of meaning. ",
  "translatedText": "Ancak bunu yapmak için ikinci dereceden formülün, terimlerin bir tür anlam taşıdığı bir versiyonuna ihtiyacımız var. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 713.84,
  "end": 718.82
 },
 {
  "input": "I realize that this is a very specific trick, for a very specific audience, but it’s something I wish I knew in college, so if you happen to know any students who might benefit from this, consider sharing it with them. ",
  "translatedText": "Bunun çok özel bir kitleye yönelik çok özel bir numara olduğunun farkındayım, ancak bu keşke üniversitede bilseydim dediğim bir şey, dolayısıyla bundan yararlanabilecek herhangi bir öğrenci tanıyorsanız, bunu onlarla paylaşmayı düşünün. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 720.38,
  "end": 729.7
 },
 {
  "input": "The hope is that it’s not just one more thing to memorize, but that the framing reinforces some other nice facts worth knowing, like how the trace and determinant relate to eigenvalues. ",
  "translatedText": "Umudumuz, ezberleyeceğiniz tek bir şeyin daha olması değil, çerçevelemenin iz ve determinantın özdeğerlerle nasıl ilişkili olduğu gibi bilmeye değer diğer bazı güzel gerçekleri pekiştirmesidir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.28,
  "end": 739.82
 },
 {
  "input": "If you want to prove those facts, by the way, take a moment to expand out the characteristic polynomial for a general matrix, and think hard about the meaning of each of these coefficients. ",
  "translatedText": "Bu arada, bu gerçekleri kanıtlamak istiyorsanız, genel bir matris için karakteristik polinomu genişletmek için bir dakikanızı ayırın ve ardından bu katsayıların her birinin anlamı hakkında iyice düşünün. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 740.56,
  "end": 749.62
 },
 {
  "input": "Many thanks to Tim, for ensuring that this mean-product formula will stay stuck in all of our heads for at least a few months. ",
  "translatedText": "Bu ortalama ürün formülünün en az birkaç ay boyunca hepimizin aklında kalmasını sağladığı için Tim'e çok teşekkürler. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 752.4,
  "end": 757.94
 },
 {
  "input": "If you don’t know about acapellascience, please do check it out. ",
  "translatedText": "Alcappella bilimi hakkında bilginiz yoksa lütfen bir göz atın. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 761.7,
  "end": 766.0
 },
 {
  "input": "\"The Molecular Shape of You\", in particular, is one of the greatest things on the internet. ",
  "translatedText": "Özellikle moleküler şekliniz internetteki en harika şeylerden biridir. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.28,
  "end": 769.58
 }
]