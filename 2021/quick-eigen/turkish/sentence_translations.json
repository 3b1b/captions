[
 {
  "input": "This is a video for anyone who already knows what eigenvalues and eigenvectors are, and who might enjoy a quick way to compute them in the case of 2x2 matrices.",
  "translatedText": "Bu, özdeğerlerin ve özvektörlerin ne olduğunu zaten bilen ve bunları 2x2 matrisler durumunda hızlı bir şekilde hesaplamanın keyfini çıkarabilecek herkes için bir videodur. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 7.56
 },
 {
  "input": "If you're unfamiliar with eigenvalues, go ahead and take a look at this video here, which is actually meant to introduce them.",
  "translatedText": "Özdeğerlere aşina değilseniz, devam edin ve buradaki videoya bir göz atın; bu video aslında onları tanıtmayı amaçlamaktadır. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 8.58,
  "end": 13.7
 },
 {
  "input": "You can skip ahead if all you want to do is see the trick, but if possible I'd like you to rediscover it for yourself.",
  "translatedText": "Tek yapmak istediğiniz numarayı görmekse ilerleyebilirsiniz, ancak mümkünse bunu kendiniz için yeniden keşfetmenizi isterim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 14.68,
  "end": 20.1
 },
 {
  "input": "So for that, let's lay out a little background.",
  "translatedText": "Bunun için biraz arka plan hazırlayalım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.58,
  "end": 22.38
 },
 {
  "input": "As a quick reminder, if the effect of a linear transformation on a given vector is to scale that vector by some constant, we call it an eigenvector of the transformation, and we call the relevant scaling factor the corresponding eigenvalue, often denoted with the letter lambda.",
  "translatedText": "Hızlı bir hatırlatma olarak, doğrusal bir dönüşümün belirli bir vektör üzerindeki etkisi, bu vektörü bir sabitle ölçeklendirmekse, buna dönüşümün bir özvektörü diyoruz ve ilgili ölçeklendirme faktörüne, genellikle lambda harfiyle gösterilen karşılık gelen özdeğer diyoruz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.26,
  "end": 38.6
 },
 {
  "input": "When you write this as an equation, and you rearrange a little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the matrix A minus lambda times the identity must send some non-zero vector, namely the corresponding eigenvector, to the zero vector, which in turn means that the determinant of this modified matrix must be zero.",
  "translatedText": "Bunu bir denklem olarak yazdığınızda ve biraz yeniden düzenlediğinizde, lambda sayısı bir A matrisinin özdeğeri ise, A eksi lambda çarpı özdeşlik matrisinin sıfır olmayan bir vektörü, yani karşılık gelen özvektörü sıfır vektörüne göndermesi gerektiğini görürsünüz, bu da bu değiştirilmiş matrisin determinantının sıfır olması gerektiği anlamına gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.84,
  "end": 64.58
 },
 {
  "input": "Okay, that's all a little bit of a mouthful to say, but again, I'm assuming that all of this is review for any of you watching.",
  "translatedText": "Tamam, bunları söylemek biraz fazla ama yine de tüm bunların izleyenleriniz için bir inceleme olduğunu varsayıyorum. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.12,
  "end": 71.54
 },
 {
  "input": "So, the usual way to compute eigenvalues, how I used to do it and how I believe most students are taught to carry it out, is to subtract the unknown value lambda off the diagonals, and then solve for the determinant is equal to zero.",
  "translatedText": "Dolayısıyla, özdeğerleri hesaplamanın olağan yolu, benim eskiden yaptığım ve çoğu öğrenciye öğretildiğine inandığım şekilde, bilinmeyen lambda değerini köşegenlerden çıkarmak ve ardından determinantın sıfıra eşit olup olmadığını çözmektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 72.82,
  "end": 85.86
 },
 {
  "input": "Doing this always involves a few extra steps to expand out and simplify to get a clean quadratic polynomial, what's known as the characteristic polynomial of the matrix.",
  "translatedText": "Bunu yapmak, matrisin karakteristik polinomu olarak bilinen temiz bir ikinci dereceden polinom elde etmek için genişletmek ve basitleştirmek için her zaman birkaç ekstra adım içerir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 87.76,
  "end": 96.46
 },
 {
  "input": "The eigenvalues are the roots of this polynomial, so to find them you have to apply the quadratic formula, which itself typically requires one or two more steps of simplification.",
  "translatedText": "Özdeğerler bu polinomun kökleridir, bu nedenle onları bulmak için ikinci dereceden formülü uygulamanız gerekir, bu da genellikle bir veya iki basitleştirme adımı daha gerektirir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 97.36,
  "end": 106.54
 },
 {
  "input": "Honestly, the process isn't terrible, but at least for two by two matrices, there is a much more direct way you can get at the answer.",
  "translatedText": "Dürüst olmak gerekirse, süreç korkunç değildir, ancak en azından ikiye iki matrisler için, cevaba ulaşmanın çok daha doğrudan bir yolu vardır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 107.76,
  "end": 114.68
 },
 {
  "input": "And if you want to rediscover this trick, there's only three relevant facts you need to know, each of which is worth knowing in its own right and can help you with other problem solving.",
  "translatedText": "Bu hileyi yeniden keşfetmek istiyorsanız, bilmeniz gereken sadece üç önemli gerçek var, bunların her biri kendi başına bilinmeye değer ve diğer problem çözümlerinde size yardımcı olabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 115.4,
  "end": 122.9
 },
 {
  "input": "Number one, the trace of a matrix, which is the sum of these two diagonal entries, is equal to the sum of the eigenvalues.",
  "translatedText": "Birincisi, bu iki köşegen girdinin toplamı olan bir matrisin izi, öz değerlerin toplamına eşittir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 123.82,
  "end": 130.92
 },
 {
  "input": "Or, another way to phrase it, more useful for our purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal entries.",
  "translatedText": "Ya da, bizim amaçlarımız için daha kullanışlı olan bir başka ifade şekli, iki özdeğerin ortalamasının bu iki köşegen girdinin ortalamasıyla aynı olduğudur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.7,
  "end": 139.46
 },
 {
  "input": "Number two, the determinant of a matrix, our usual ad-bc formula, is equal to the product of the two eigenvalues.",
  "translatedText": "İki numara, bir matrisin determinantı, her zamanki ad-bc formülümüz, iki özdeğerin çarpımına eşittir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.0,
  "end": 148.96
 },
 {
  "input": "And this should kind of make sense if you understand that eigenvalues describe how much an operator stretches space in a particular direction, and that the determinant describes how much an operator scales areas, or volumes, as a whole.",
  "translatedText": "Özdeğerlerin bir operatörün uzayı belirli bir yönde ne kadar genişlettiğini ve determinantın bir operatörün alanları veya hacimleri bir bütün olarak ne kadar ölçeklendirdiğini tanımladığını anlarsanız, bu bir şekilde mantıklı olacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 150.06,
  "end": 161.76
 },
 {
  "input": "Now before getting to the third fact, notice how you can essentially read these first two values out of the matrix without really writing much down.",
  "translatedText": "Şimdi üçüncü gerçeğe geçmeden önce, aslında çok fazla yazmadan bu ilk iki değeri matristen nasıl okuyabileceğinize dikkat edin. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 162.8,
  "end": 169.16
 },
 {
  "input": "Take this matrix here as an example.",
  "translatedText": "Buradaki matrisi örnek olarak alın. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 169.76,
  "end": 171.32
 },
 {
  "input": "Straight away, you can know that the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7.",
  "translatedText": "Özdeğerlerin ortalamasının 8 ve 6'nın ortalaması olan 7 ile aynı olduğunu hemen anlayabilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 171.82,
  "end": 177.82
 },
 {
  "input": "Likewise, most linear algebra students are pretty well practiced at finding the determinant, which in this case works out to be 48 minus 8.",
  "translatedText": "Benzer şekilde, çoğu lineer cebir öğrencisi determinantı bulma konusunda oldukça deneyimlidir; bu durumda determinant 48 eksi 8 olarak hesaplanır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 179.58,
  "end": 187.08
 },
 {
  "input": "So right away, you know that the product of the two eigenvalues is 40.",
  "translatedText": "Yani hemen iki özdeğerin çarpımının 40 olduğunu bilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.24,
  "end": 191.7
 },
 {
  "input": "Now take a moment to see if you can derive what will be our third relevant fact, which is how you can quickly recover two numbers when you know their mean and you know their product.",
  "translatedText": "Şimdi bir dakikanızı ayırın ve üçüncü ilgili gerçeğimizi türetip çıkaramayacağınızı görün; bu, ortalamalarını bildiğinizde ve çarpımlarını bildiğinizde iki sayıyı nasıl hızlı bir şekilde kurtarabileceğinizdir. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 192.78,
  "end": 201.56
 },
 {
  "input": "Here, let's focus on this example.",
  "translatedText": "Burada bu örnek üzerinde duralım. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.46,
  "end": 203.72
 },
 {
  "input": "You know that the two values are evenly spaced around the number 7, so they look like 7 plus or minus something, let's call that something d for distance.",
  "translatedText": "Biliyorsunuz, iki değer 7 sayısının etrafında eşit aralıklarla yerleştirilmiştir, yani 7 artı veya eksi gibi görünürler, buna uzaklık için d diyelim. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 204.2,
  "end": 212.78
 },
 {
  "input": "You also know that the product of these two numbers is 40.",
  "translatedText": "Bu iki sayının çarpımının 40 olduğunu da biliyorsunuz. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 213.56,
  "end": 216.38
 },
 {
  "input": "Now to find d, notice that this product expands really nicely, it works out as a difference of squares.",
  "translatedText": "Şimdi d'yi bulmak için, bu çarpımın gerçekten güzel bir şekilde genişlediğine dikkat edin, kareler farkı olarak sonuç verir. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 218.6,
  "end": 223.7
 },
 {
  "input": "So from there, you can find d.",
  "translatedText": "Oradan da D'yi bulabilirsin.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 224.56,
  "end": 226.86
 },
 {
  "input": "d squared is 7 squared minus 40, or 9, which means that d itself is 3.",
  "translatedText": "d'nin karesi 7'nin karesi eksi 40 veya 9'dur, bu da d'nin kendisinin 3 olduğu anlamına gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 228.2,
  "end": 233.4
 },
 {
  "input": "In other words, the two values for this very specific example work out to be 4 and 10.",
  "translatedText": "Başka bir deyişle, bu çok özel örneğin iki değeri 4 ve 10 olarak çıkıyor. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.38,
  "end": 241.1
 },
 {
  "input": "But our goal is a quick trick, and you wouldn't want to think through this each time, so let's wrap up what we just did in a general formula.",
  "translatedText": "Ancak amacımız hızlı bir numara ve bunu her seferinde düşünmek istemezsiniz, bu yüzden az önce yaptığımız şeyi genel bir formülle tamamlayalım.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.68,
  "end": 248.12
 },
 {
  "input": "For any mean m and product p, the distance squared is always going to be m squared minus p.",
  "translatedText": "Herhangi bir ortalama m ve ürün p için, mesafenin karesi her zaman m'nin karesi eksi p olacaktır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.64,
  "end": 255.68
 },
 {
  "input": "This gives the third key fact, which is that when two numbers have a mean m and a product p, you can write those two numbers as m plus or minus the square root of m squared minus p.",
  "translatedText": "Bu da üçüncü temel gerçeği verir: İki sayının ortalaması m ve çarpımı p olduğunda, bu iki sayıyı m artı veya eksi m kare eksi p'nin karekökü olarak yazabilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 257.56,
  "end": 268.46
 },
 {
  "input": "This is decently fast to re-derive on the fly if you ever forget it, and it's essentially just a rephrasing of the difference of squares formula.",
  "translatedText": "Bu, unutmanız halinde anında yeniden türetmek için oldukça hızlıdır ve esasen kareler farkı formülünün yeniden ifade edilmesinden ibarettir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 270.1,
  "end": 277.08
 },
 {
  "input": "But even still, it's a fact that's worth memorizing so it's at the tip of your fingers.",
  "translatedText": "Ancak yine de, parmaklarınızın ucunda olması için ezberlemeye değer bir gerçektir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.86,
  "end": 281.22
 },
 {
  "input": "In fact, my friend Tim from the channel A Capella Science wrote us a nice quick jingle to make it a little bit more memorable.",
  "translatedText": "Aslında, A Capella Science kanalından arkadaşım Tim, bunu biraz daha akılda kalıcı kılmak için bize güzel bir kısa şarkı yazdı. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 281.22,
  "end": 287.16
 },
 {
  "input": "Let me show you how this works, say for the matrix 3 1 4 1.",
  "translatedText": "Size bunun nasıl çalıştığını göstereyim, diyelim ki 3 1 4 1 matrisi için.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 291.9,
  "end": 297.62
 },
 {
  "input": "You start by bringing to mind the formula, maybe stating it all in your head.",
  "translatedText": "Formülü aklınıza getirerek başlıyorsunuz, belki de hepsini kafanızda ifade ediyorsunuz. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 298.1,
  "end": 301.82
 },
 {
  "input": "But when you write it down, you fill in the appropriate values for m and p as you go.",
  "translatedText": "Ancak bunu yazarken, m ve p için uygun değerleri ilerledikçe doldurursunuz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.2,
  "end": 311.62
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2, so the thing you start writing is 2 plus or minus the square root of 2 squared minus.",
  "translatedText": "Bu örnekte, özdeğerlerin ortalaması 3 ve 1'in ortalamasıyla aynıdır, yani 2'dir, bu nedenle yazmaya başladığınız şey 2 artı veya eksi 2'nin karekökü eksidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 312.34,
  "end": 322.7
 },
 {
  "input": "Then the product of the eigenvalues is the determinant, which in this example is 3 times 1 minus 1 times 4, or negative 1, so that's the final thing you fill in, which means the eigenvalues are 2 plus or minus the square root of 5.",
  "translatedText": "Daha sonra özdeğerlerin çarpımı determinanttır, bu örnekte 3 kere 1 eksi 1 kere 4 veya negatif 1'dir, bu yüzden doldurduğunuz son şey budur, bu da özdeğerlerin 2 artı veya eksi 5'in karekökü olduğu anlamına gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.54,
  "end": 338.76
 },
 {
  "input": "You might recognize that this is the same matrix I was using at the beginning, but notice how much more directly we can get at the answer.",
  "translatedText": "Bunun başlangıçta kullandığım matrisin aynısı olduğunu fark edebilirsiniz, ancak cevaba ne kadar doğrudan ulaşabildiğimize dikkat edin. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 340.3,
  "end": 346.5
 },
 {
  "input": "Here, try another one.",
  "translatedText": "İşte, başka bir tane dene. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 348.14,
  "end": 349.18
 },
 {
  "input": "This time, the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5.",
  "translatedText": "Bu kez, özdeğerlerin ortalaması 2 ve 8'in ortalaması olan 5 ile aynıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 349.44,
  "end": 354.48
 },
 {
  "input": "So again, you start writing out the formula, but this time writing 5 in place of m.",
  "translatedText": "Yine formülü yazmaya başlıyorsunuz ama bu sefer m yerine 5 yazıyorsunuz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 355.1,
  "end": 359.22
 },
 {
  "input": "And then the determinant is 2 times 8 minus 7 times 1, or 9.",
  "translatedText": "",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 362.98,
  "end": 368.3
 },
 {
  "input": "So in this example, the eigenvalues look like 5 plus or minus the square root of 16, which simplifies even further as 9 and 1.",
  "translatedText": "Yani bu örnekte özdeğerler 5 artı veya eksi 16'nın karekökü gibi görünüyor, bu da 9 ve 1 olarak daha da basitleşiyor. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 369.52,
  "end": 378.24
 },
 {
  "input": "You see what I mean about how you can basically just start writing down the eigenvalues while you're staring at the matrix?",
  "translatedText": "Matrise bakarken özdeğerleri yazmaya nasıl başlayabileceğiniz hakkında ne demek istediğimi anlıyor musunuz?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 379.42,
  "end": 384.62
 },
 {
  "input": "It's typically just the tiniest bit of simplification at the end.",
  "translatedText": "Bu genellikle en sonunda yapılan küçük bir basitleştirmedir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 385.28,
  "end": 388.16
 },
 {
  "input": "Honestly, I've found myself using this trick a lot when I'm sketching quick notes related to linear algebra and want to use small matrices as examples.",
  "translatedText": "Açıkçası, lineer cebirle ilgili hızlı notlar alırken ve küçük matrisleri örnek olarak kullanmak istediğimde kendimi bu numarayı çok kullanırken buldum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 389.06,
  "end": 395.72
 },
 {
  "input": "I've been working on a video about matrix exponents, where eigenvalues pop up a lot, and I realize it's just very handy if students can read out the eigenvalues from small examples without losing the main line of thought by getting bogged down in a different calculation.",
  "translatedText": "Özdeğerlerin çokça geçtiği matris üsleri hakkında bir video üzerinde çalışıyorum ve öğrencilerin farklı bir hesaplamaya dalıp ana düşünce çizgisini kaybetmeden küçük örneklerden özdeğerleri okuyabilmelerinin çok kullanışlı olduğunu fark ettim.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.18,
  "end": 408.62
 },
 {
  "input": "As another fun example, take a look at this set of three different matrices, which comes up a lot in quantum mechanics.",
  "translatedText": "Bir başka eğlenceli örnek olarak, kuantum mekaniğinde sıkça karşımıza çıkan bu üç farklı matris kümesine bir göz atın.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 409.74,
  "end": 415.46
 },
 {
  "input": "They're known as the Pauli spin matrices.",
  "translatedText": "Pauli spin matrisleri olarak bilinirler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 415.76,
  "end": 417.52
 },
 {
  "input": "If you know quantum mechanics, you'll know that the eigenvalues of matrices are highly relevant to the physics that they describe.",
  "translatedText": "Kuantum mekaniğini biliyorsanız, matrislerin özdeğerlerinin tanımladıkları fizikle oldukça ilgili olduğunu bilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 418.6,
  "end": 424.42
 },
 {
  "input": "And if you don't know quantum mechanics, let this just be a little glimpse of how these computations are actually very relevant to real applications.",
  "translatedText": "Kuantum mekaniğini bilmiyorsanız, bu hesaplamaların aslında gerçek uygulamalarla ne kadar ilgili olduğuna dair küçük bir fikir vermesine izin verin.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.22,
  "end": 431.22
 },
 {
  "input": "The mean of the diagonal entries in all three cases is zero.",
  "translatedText": "Her üç durumda da diyagonal girişlerin ortalaması sıfırdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.54,
  "end": 435.88
 },
 {
  "input": "So the mean of the eigenvalues in all of these cases is zero, which makes our formula look especially simple.",
  "translatedText": "Dolayısıyla, tüm bu durumlarda özdeğerlerin ortalaması sıfırdır, bu da formülümüzün özellikle basit görünmesini sağlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 437.56,
  "end": 443.06
 },
 {
  "input": "What about the products of the eigenvalues, the determinants of these matrices?",
  "translatedText": "Peki ya bu matrislerin belirleyicileri olan özdeğerlerin çarpımları? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.38,
  "end": 448.8
 },
 {
  "input": "For the first one, it's 0, minus 1, or negative 1.",
  "translatedText": "İlki için 0, eksi 1 veya negatif 1'dir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.7,
  "end": 452.56
 },
 {
  "input": "The second one also looks like 0, minus 1, but it takes a moment more to see because of the complex numbers.",
  "translatedText": "İkincisi de 0, eksi 1 gibi görünüyor, ancak karmaşık sayılar nedeniyle görmek biraz daha zaman alıyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.2,
  "end": 458.2
 },
 {
  "input": "And the final one looks like negative 1, minus 0.",
  "translatedText": "Ve sonuncusu eksi 1 eksi 0'a benziyor. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.84,
  "end": 461.36
 },
 {
  "input": "So in all cases, the eigenvalues simplify to be plus and minus 1.",
  "translatedText": "Yani her durumda özdeğerler artı ve eksi 1 olacak şekilde basitleştirilir. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 462.06,
  "end": 465.92
 },
 {
  "input": "Although in this case, you really don't need a formula to find two values if you know that they're evenly spaced around 0 and their product is negative 1.",
  "translatedText": "Ancak bu durumda, eğer bu değerlerin 0 civarında eşit aralıklı olduğunu ve çarpımlarının negatif 1 olduğunu biliyorsanız, iki değeri bulmak için gerçekten bir formüle ihtiyacınız yoktur. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 466.72,
  "end": 473.28
 },
 {
  "input": "If you're curious, in the context of quantum mechanics, these matrices describe observations you might make about a particle's spin in the x, y, or z direction.",
  "translatedText": "Merak ediyorsanız, kuantum mekaniği bağlamında, bu matrisler bir parçacığın x, y veya z yönündeki dönüşü hakkında yapabileceğiniz gözlemleri tanımlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 474.64,
  "end": 483.12
 },
 {
  "input": "And the fact that their eigenvalues are plus and minus 1 corresponds with the idea that the values for the spin that you would observe would be either entirely in one direction or entirely in another, as opposed to something continuously ranging in between.",
  "translatedText": "Ve özdeğerlerinin artı ve eksi 1 olması, gözlemleyeceğiniz spin değerlerinin, sürekli olarak arada değişen bir şeyin aksine, ya tamamen bir yönde ya da tamamen başka bir yönde olacağı fikrine karşılık gelir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.56,
  "end": 497.02
 },
 {
  "input": "Maybe you'd wonder how exactly this works, or why you would use 2x2 matrices that have complex numbers to describe spin in three dimensions.",
  "translatedText": "Belki bunun tam olarak nasıl çalıştığını ya da üç boyutta spini tanımlamak için neden karmaşık sayılara sahip 2x2 matrisler kullandığınızı merak edersiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 498.32,
  "end": 505.52
 },
 {
  "input": "Those would be fair questions, just outside the scope of what I want to talk about here.",
  "translatedText": "Bunlar adil sorular olabilir, ancak burada konuşmak istediğim konunun kapsamı dışında.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 506.1,
  "end": 509.76
 },
 {
  "input": "You know, it's funny, I wrote this section because I wanted some case where you have 2x2 matrices that aren't just toy examples or homework problems, ones where they actually come up in practice, and quantum mechanics is great for that.",
  "translatedText": "Biliyor musunuz, bu bölümü yazmamın nedeni 2x2 matrislerin sadece oyuncak örnekler ya da ev ödevi problemleri değil, pratikte gerçekten karşımıza çıktıkları bir durum olmasını istememdi ve kuantum mekaniği bunun için harikadır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 510.48,
  "end": 521.7
 },
 {
  "input": "The thing is, after I made it, I realized that the whole example kind of undercuts the point that I'm trying to make.",
  "translatedText": "Mesele şu ki, bunu yaptıktan sonra fark ettim ki tüm bu örnek, anlatmaya çalıştığım noktanın altını oyuyor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 521.7,
  "end": 528.24
 },
 {
  "input": "For these specific matrices, when you use the traditional method, the one with characteristic polynomials, it's essentially just as fast.",
  "translatedText": "Bu özel matrisler için, karakteristik polinomları içeren geleneksel yöntemi kullandığınızda, aslında aynı derecede hızlıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.74,
  "end": 536.1
 },
 {
  "input": "It might actually be faster.",
  "translatedText": "Aslında daha hızlı olabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.22,
  "end": 537.64
 },
 {
  "input": "I mean, take a look at the first one.",
  "translatedText": "Yani, ilkine bir bakın.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.24,
  "end": 539.4
 },
 {
  "input": "The relevant determinant directly gives you a characteristic polynomial of lambda squared minus 1, and clearly that has roots of plus and minus 1.",
  "translatedText": "İlgili determinant size doğrudan lambda kare eksi 1'in karakteristik polinomunu verir ve açıkça bunun artı ve eksi 1 kökleri vardır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.68,
  "end": 548.2
 },
 {
  "input": "Same answer when you do the second matrix, lambda squared minus 1.",
  "translatedText": "İkinci matrisi yaptığınızda da aynı cevap, lambda kare eksi bir. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 548.84,
  "end": 551.76
 },
 {
  "input": "And as for the last matrix, forget about doing any computations, traditional or otherwise, it's already a diagonal matrix, so those diagonal entries are the eigenvalues.",
  "translatedText": "Ve son matrise gelince, geleneksel veya başka türlü herhangi bir hesaplama yapmayı unutun, zaten köşegen bir matris, bu yüzden bu köşegen girişler özdeğerlerdir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.88,
  "end": 562.74
 },
 {
  "input": "However, the example is not totally lost to our cause.",
  "translatedText": "Ancak örnek davamız açısından tamamen kaybolmuş değil. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 564.3,
  "end": 566.92
 },
 {
  "input": "Where you will actually feel the speedup is in the more general case, where you take a linear combination of these three matrices and then try to compute the eigenvalues.",
  "translatedText": "Aslında hızlanmayı hissedeceğiniz yer, bu üç matrisin doğrusal bir kombinasyonunu aldığınız ve ardından özdeğerleri hesaplamaya çalıştığınız daha genel durumdur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 567.38,
  "end": 576.06
 },
 {
  "input": "You might write this as a times the first one, plus b times the second, plus c times the third.",
  "translatedText": "Bunu a çarpı birinci artı b çarpı ikinci, artı c çarpı üçüncü olarak yazabilirsiniz. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.82,
  "end": 582.42
 },
 {
  "input": "In quantum mechanics, this would describe spin observations in a general direction of a vector with coordinates a, b, c.",
  "translatedText": "Kuantum mekaniğinde bu, a, b, c koordinatlarına sahip bir vektörün genel yönündeki spin gözlemlerini tanımlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.02,
  "end": 589.28
 },
 {
  "input": "More specifically, you should assume that this vector is normalized, meaning a squared plus b squared plus c squared is equal to 1.",
  "translatedText": "Daha spesifik olarak, bu vektörün normalleştirilmiş olduğunu, yani a kare artı b kare artı c karenin bire eşit olduğunu varsaymalısınız. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 590.9,
  "end": 597.7
 },
 {
  "input": "When you look at this new matrix, it's immediate to see that the mean of the eigenvalues is still 0.",
  "translatedText": "Bu yeni matrise baktığınızda, özdeğerlerin ortalamasının hala 0 olduğunu hemen görebilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 598.6,
  "end": 604.1
 },
 {
  "input": "And you might also enjoy pausing for a brief moment to confirm that the product of those eigenvalues is still negative 1.",
  "translatedText": "Ayrıca, bu özdeğerlerin çarpımının hala negatif 1 olduğunu doğrulamak için kısa bir süre duraklamaktan da hoşlanabilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 604.6,
  "end": 610.9
 },
 {
  "input": "And then from there, concluding what the eigenvalues must be.",
  "translatedText": "Ve buradan özdeğerlerin ne olması gerektiği sonucuna varmak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 613.26,
  "end": 615.92
 },
 {
  "input": "And this time, the characteristic polynomial approach would be by comparison a lot more cumbersome, definitely harder to do in your head.",
  "translatedText": "Ve bu kez, karakteristik polinom yaklaşımı, kıyaslandığında çok daha hantal ve kafanızda yapılması kesinlikle daha zor olacaktır. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.22,
  "end": 623.58
 },
 {
  "input": "To be clear, using the mean product formula is not fundamentally different from finding roots of the characteristic polynomial.",
  "translatedText": "Açık olmak gerekirse, ortalama çarpım formülünü kullanmak, karakteristik polinomun köklerini bulmaktan temelde farklı değildir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 625.08,
  "end": 630.96
 },
 {
  "input": "I mean, it can't be, they're solving the same problem.",
  "translatedText": "Yani, olamaz, aynı sorunu çözüyorlar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 631.34,
  "end": 633.44
 },
 {
  "input": "One way to think about this actually is that the mean product formula is a nice way to solve quadratics in general.",
  "translatedText": "Aslında bunu düşünmenin bir yolu, ortalama çarpım formülünün genel olarak kuadratikleri çözmek için güzel bir yol olduğudur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 634.16,
  "end": 639.02
 },
 {
  "input": "And some viewers of the channel may recognize this.",
  "translatedText": "Ve kanalın bazı izleyicileri bunu tanıyabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 639.6,
  "end": 641.66
 },
 {
  "input": "Think about it, when you're trying to find the roots of a quadratic, given the coefficients, that's another situation where you know the sum of two values, and you also know their product, but you're trying to recover the original two values.",
  "translatedText": "Bir düşünün, katsayıları verilen ikinci dereceden bir karenin köklerini bulmaya çalıştığınızda, bu iki değerin toplamını bildiğiniz başka bir durumdur ve aynı zamanda çarpımlarını da bilirsiniz, ancak orijinal iki değeri kurtarmaya çalışıyorsunuzdur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 642.54,
  "end": 654.1
 },
 {
  "input": "Specifically, if the polynomial is normalized, so that this leading coefficient is 1, then the mean of the roots will be negative 1 half times this linear coefficient, which is negative 1 times the sum of those roots.",
  "translatedText": "Spesifik olarak, eğer polinom bu baş katsayı bir olacak şekilde normalleştirilirse, köklerin ortalaması bu doğrusal katsayının yarısı kadar negatif olacaktır; bu da köklerin toplamının bir katı negatif olacaktır. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 655.56,
  "end": 666.88
 },
 {
  "input": "With the example on the screen, that makes the mean 5.",
  "translatedText": "Ekrandaki örnekle, bu ortalama 5 yapar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 668.02,
  "end": 670.18
 },
 {
  "input": "And the product of the roots is even easier, it's just the constant term, no adjustments needed.",
  "translatedText": "Ve köklerin çarpımı daha da kolaydır, sadece sabit terimdir, ayarlamaya gerek yoktur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 671.98,
  "end": 676.52
 },
 {
  "input": "So from there, you would apply the mean product formula, and that gives you the roots.",
  "translatedText": "Buradan ortalama çarpım formülünü uygularsınız ve bu da size kökleri verir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 677.34,
  "end": 680.9
 },
 {
  "input": "And on the one hand, you could think of this as a lighter weight version of the traditional quadratic formula.",
  "translatedText": "Bir yandan da bunu geleneksel ikinci dereceden formülün daha hafif bir versiyonu olarak düşünebilirsiniz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.14,
  "end": 690.22
 },
 {
  "input": "But the real advantage is not just that it's fewer symbols to memorize, it's that each one of them carries more meaning with it.",
  "translatedText": "Ancak asıl avantaj, yalnızca ezberlenecek daha az sembol olması değil, aynı zamanda her birinin daha fazla anlam taşımasıdır. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 690.96,
  "end": 696.44
 },
 {
  "input": "I mean, the whole point of this eigenvalue trick is that because you can read out the mean and product directly from looking at the matrix, you don't need to go through the intermediate step of setting up the characteristic polynomial.",
  "translatedText": "Yani, bu özdeğer numarasının tüm amacı, ortalama ve çarpımı doğrudan matrise bakarak okuyabildiğiniz için, karakteristik polinomu ayarlama ara adımından geçmenize gerek kalmamasıdır.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.94,
  "end": 708.0
 },
 {
  "input": "You can jump straight to writing down the roots without ever explicitly thinking about what the polynomial looks like.",
  "translatedText": "Polinomun neye benzediğini açıkça düşünmeden doğrudan kökleri yazmaya geçebilirsiniz. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 708.42,
  "end": 713.64
 },
 {
  "input": "But to do that, we need a version of the quadratic formula where the terms carry some kind of meaning.",
  "translatedText": "Ancak bunu yapmak için, terimlerin bir tür anlam taşıdığı ikinci dereceden formülün bir versiyonuna ihtiyacımız var.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 713.84,
  "end": 718.82
 },
 {
  "input": "I realize this is a very specific trick for a very specific audience, but it's something I wish I knew in college, so if you happen to know any students who might benefit from this, consider sharing it with them.",
  "translatedText": "Bunun çok özel bir kitle için çok özel bir numara olduğunun farkındayım, ancak keşke üniversitede bilseydim dediğim bir şey, bu yüzden bundan yararlanabilecek herhangi bir öğrenci tanıyorsanız, onlarla paylaşmayı düşünün.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 720.38,
  "end": 729.7
 },
 {
  "input": "The hope is that it's not just one more thing that you memorize, but that the framing reinforces some other nice facts that are worth knowing, like how the trace and the determinant are related to eigenvalues.",
  "translatedText": "Umudumuz, ezberleyeceğiniz tek bir şeyin daha olması değil, çerçevelemenin iz ve determinantın özdeğerlerle nasıl ilişkili olduğu gibi bilmeye değer diğer bazı güzel gerçekleri pekiştirmesidir. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.28,
  "end": 739.82
 },
 {
  "input": "If you want to prove those facts, by the way, take a moment to expand out the characteristic polynomial for a general matrix, and then think hard about the meaning of each of these coefficients.",
  "translatedText": "Bu arada, bu gerçekleri kanıtlamak istiyorsanız, genel bir matris için karakteristik polinomu genişletmek için biraz zaman ayırın ve ardından bu katsayıların her birinin anlamı hakkında iyice düşünün.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 740.56,
  "end": 749.62
 },
 {
  "input": "Many thanks to Tim for ensuring that this mean product formula will stay stuck in all of our heads for at least a few months.",
  "translatedText": "Bu ortalama ürün formülünün en azından birkaç ay boyunca hepimizin aklında kalmasını sağladığı için Tim'e çok teşekkürler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 752.4,
  "end": 757.94
 },
 {
  "input": "If you don't know about alcappella science, please do check it out.",
  "translatedText": "Eğer alcappella bilimini bilmiyorsanız, lütfen bir göz atın.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 761.7,
  "end": 766.0
 },
 {
  "input": "The molecular shape of you in particular is one of the greatest things on the internet.",
  "translatedText": "Özellikle sizin moleküler şekliniz internetteki en harika şeylerden biri.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.28,
  "end": 769.58
 }
]