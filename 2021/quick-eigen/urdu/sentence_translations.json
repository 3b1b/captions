[
 {
  "input": "This is a video for anyone who already knows what eigenvalues and eigenvectors are, and who might enjoy a quick way to compute them in the case of 2x2 matrices. ",
  "translatedText": "یہ ان لوگوں کے لیے ایک ویڈیو ہے جو پہلے سے جانتا ہے کہ eigenvalues اور eigenvectors کیا ہیں، اور جو 2x2 میٹرکس کے معاملے میں ان کی گنتی کرنے کے فوری طریقے سے لطف اندوز ہو سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 7.56
 },
 {
  "input": "If you’re unfamiliar with eigenvalues, take a look at this video which introduces them. ",
  "translatedText": "اگر آپ eigenvalues سے ناواقف ہیں تو آگے بڑھیں اور یہاں اس ویڈیو پر ایک نظر ڈالیں، جس کا مقصد دراصل ان کا تعارف کروانا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.58,
  "end": 13.7
 },
 {
  "input": "You can skip ahead if you just want to see the trick, but if possible I’d like you to rediscover it for yourself, so for that let’s lay down a little background. ",
  "translatedText": "اگر آپ صرف اس چال کو دیکھنا چاہتے ہیں تو آپ آگے بڑھ سکتے ہیں، لیکن اگر ممکن ہو تو میں چاہوں گا کہ آپ اسے اپنے لیے دوبارہ دریافت کریں۔تو اس کے لیے، آئیے تھوڑا سا پس منظر بیان کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.68,
  "end": 22.38
 },
 {
  "input": "As a quick reminder, if the effect of a linear transformation on a given vector is to scale it by some constant, we call it an \"eigenvector\" of the transformation, and we call the relevant scaling factor the corresponding \"eigenvalue,\" often denoted with the letter lambda. ",
  "translatedText": "ایک فوری یاد دہانی کے طور پر، اگر کسی دیے گئے ویکٹر پر لکیری تبدیلی کا اثر اس ویکٹر کو کسی مستقل سے پیمانہ کرنے کے لیے ہے، تو ہم اسے تبدیلی کا ایک ایجین ویکٹر کہتے ہیں، اور ہم متعلقہ اسکیلنگ فیکٹر کو متعلقہ eigenvalue کہتے ہیں، جو اکثر خط کے ساتھ اشارہ کیا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.26,
  "end": 38.6
 },
 {
  "input": "When you write this as an equation and you rearrange a little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the matrix (A minus lambda times the identity) must send some nonzero vector, namely the corresponding eigenvector, to the zero vector, which in turn means the determinant of this modified matrix must be 0. ",
  "translatedText": "لیمبڈا جب آپ اسے ایک مساوات کے طور پر لکھتے ہیں، اور آپ تھوڑا سا دوبارہ ترتیب دیتے ہیں، تو آپ جو دیکھتے ہیں وہ یہ ہے کہ اگر نمبر لیمبڈا میٹرکس A کی ایک ایگن ویلیو ہے، تو میٹرکس A مائنس لیمبڈا اوقات شناخت کو کچھ غیر صفر ویکٹر بھیجنا چاہیے، یعنی متعلقہ ایگن ویکٹر، صفر ویکٹر سے، جس کا مطلب یہ ہے کہ اس ترمیم شدہ میٹرکس کا تعین کنندہ صفر ہونا چاہیے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.84,
  "end": 64.58
 },
 {
  "input": "Okay, that’s all a little bit of a mouthful to say, but again, I’m assuming all of this is review for anyone watching. ",
  "translatedText": "ٹھیک ہے، کہنے کے لیے یہ سب کچھ تھوڑا سا ہے، لیکن ایک بار پھر، میں فرض کر رہا ہوں کہ یہ سب کچھ آپ میں سے کسی کے لیے بھی جائزہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.12,
  "end": 71.54
 },
 {
  "input": "So, the usual way to compute eigenvalues, how I used to do it, and how I believe most students are taught to carry it out, is to subtract the unknown value lambda off the diagonals and then solve for when the determinant equals 0. ",
  "translatedText": "لہذا، eigenvalues کی گنتی کرنے کا معمول کا طریقہ، میں اسے کیسے کرتا تھا اور کس طرح مجھے یقین ہے کہ زیادہ تر طالب علموں کو اس پر عمل کرنا سکھایا جاتا ہے، یہ ہے کہ نامعلوم قدر lambda کو اخترن سے گھٹا دیا جائے، اور پھر اس کے لیے حل کیا جائے جب تعین کنندہ صفر کے برابر ہو۔. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 72.82,
  "end": 85.86
 },
 {
  "input": "Doing this always involves a few steps to expand out and simplify to get a clean quadratic polynomial, what's known as the “characteristic polynomial” of the matrix. ",
  "translatedText": "ایسا کرنے میں ہمیشہ کچھ اضافی اقدامات شامل ہوتے ہیں تاکہ ایک صاف چوکور کثیر الثانی کو حاصل کیا جا سکے، جسے میٹرکس کی خصوصیت کے کثیر نام کے طور پر جانا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.76,
  "end": 96.46
 },
 {
  "input": "The eigenvalues are the roots of this polynomial. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 97.36,
  "end": 99.9
 },
 {
  "input": "So to find them you have to apply the quadratic formula, which itself typically requires one or two more steps of simplification. ",
  "translatedText": "eigenvalues اس کثیر الجہتی کی جڑیں ہیں، لہذا ان کو تلاش کرنے کے لیے آپ کو چوکور فارمولے کو لاگو کرنا ہوگا، جس کے لیے خود عام طور پر ایک یا دو مزید مراحل کی ضرورت ہوتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.1,
  "end": 106.54
 },
 {
  "input": "Honestly, the process isn’t terrible. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 107.76,
  "end": 109.5
 },
 {
  "input": "But at least for 2x2 matrices, there’s a much more direct way to get at this answer. ",
  "translatedText": "سچ میں، یہ عمل خوفناک نہیں ہے، لیکن کم از کم 2x2 میٹرکس کے لیے، ایک بہت زیادہ سیدھا طریقہ ہے جس سے آپ جواب حاصل کر سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.58,
  "end": 114.68
 },
 {
  "input": "And if you want to rediscover this trick, there are only three relevant facts you need to know, each of which is worth knowing in its own right and can help you with other problem-solving. ",
  "translatedText": "اور اگر آپ اس چال کو دوبارہ دریافت کرنا چاہتے ہیں، تو صرف تین متعلقہ حقائق ہیں جن کے بارے میں آپ کو جاننے کی ضرورت ہے، جن میں سے ہر ایک اپنے طور پر جاننے کے قابل ہے اور دیگر مسائل کو حل کرنے میں آپ کی مدد کر سکتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 115.4,
  "end": 122.9
 },
 {
  "input": "Number 1: The trace of a matrix, which is the sum of these two diagonal entries, is equal to the sum of the eigenvalues. ",
  "translatedText": "نمبر ایک، میٹرکس کا ٹریس، جو ان دو ترچھی اندراجات کا مجموعہ ہے، ایگن ویلیوز کے مجموعے کے برابر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 123.82,
  "end": 130.92
 },
 {
  "input": "Or another way to phrase it, more useful for our purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal entries. ",
  "translatedText": "یا اس کو جمانے کا دوسرا طریقہ، ہمارے مقاصد کے لیے زیادہ مفید ہے، یہ ہے کہ دو ایگن ویلیوز کا وسط ان دو ترچھی اندراجات کے وسط کے برابر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.7,
  "end": 139.46
 },
 {
  "input": "Number 2: The determinant of a matrix, our usual ad-bc formula, is equal to the product of the two eigenvalues. ",
  "translatedText": "نمبر دو، میٹرکس کا تعین کرنے والا، ہمارا معمول کا ایڈ-بی سی فارمولا، دو ایگن ویلیوز کی پیداوار کے برابر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.0,
  "end": 148.96
 },
 {
  "input": "And this should kind of make sense if you understand that eigenvalues describe how much an operator stretches space in a particular direction and that the determinant describes how much an operator scales areas (or volumes) as a whole. ",
  "translatedText": "اور یہ اس طرح کا معنی خیز ہونا چاہئے اگر آپ سمجھتے ہیں کہ ایگن ویلیوز یہ بتاتے ہیں کہ ایک آپریٹر کسی خاص سمت میں کتنی جگہ پھیلاتا ہے، اور یہ کہ تعین کنندہ یہ بتاتا ہے کہ ایک آپریٹر مجموعی طور پر کتنے علاقوں، یا حجم کو پیمانہ کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.06,
  "end": 161.76
 },
 {
  "input": "Now before getting to the third fact, notice how you can essentially read these first two values out of the matrix without really writing much down. ",
  "translatedText": "اب تیسری حقیقت کی طرف جانے سے پہلے، غور کریں کہ آپ ان پہلی دو قدروں کو میٹرکس سے باہر واقعی زیادہ لکھے بغیر کیسے پڑھ سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 162.8,
  "end": 169.16
 },
 {
  "input": "Take this matrix here as an example. ",
  "translatedText": "اس میٹرکس کو یہاں ایک مثال کے طور پر لیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.76,
  "end": 171.32
 },
 {
  "input": "Straight away you can know that the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7. ",
  "translatedText": "فوری طور پر، آپ جان سکتے ہیں کہ eigenvalues کا اوسط 8 اور 6 کے اوسط کے برابر ہے، جو کہ 7 ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 171.82,
  "end": 177.82
 },
 {
  "input": "Likewise, most linear algebra students are pretty well-practiced at finding the determinant, which in this case works out to be 48 - 8 So right away you know that the product of our two eigenvalues is 40. ",
  "translatedText": "اسی طرح، زیادہ تر لکیری الجبرا کے طلباء تعین کنندہ کو تلاش کرنے کے لیے بہت اچھی طرح سے مشق کر رہے ہیں، جو اس صورت میں 48 مائنس 8 پر کام کرتا ہے۔تو فوراً، آپ جانتے ہیں کہ دو ایگن ویلیوز کی پیداوار 40 ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.58,
  "end": 191.7
 },
 {
  "input": "Now take a moment to see how you can derive what will be our third relevant fact, which is how to recover two numbers when you know their mean and you know their product. ",
  "translatedText": "اب یہ دیکھنے کے لیے تھوڑا وقت نکالیں کہ کیا آپ یہ اخذ کر سکتے ہیں کہ ہماری تیسری متعلقہ حقیقت کیا ہو گی، جس کا مطلب یہ ہے کہ جب آپ کو دو نمبروں کا مطلب معلوم ہو جائے اور آپ ان کی مصنوع کو جانتے ہوں تو آپ تیزی سے بازیافت کر سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.78,
  "end": 201.56
 },
 {
  "input": "Here, let's focus on this example. ",
  "translatedText": "یہاں، اس مثال پر توجہ مرکوز کرتے ہیں. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.46,
  "end": 203.72
 },
 {
  "input": "You know the two values are evenly spaced around 7, so they look like 7 plus or minus something; let’s call that something \"d\" for distance. ",
  "translatedText": "آپ جانتے ہیں کہ دو قدریں نمبر 7 کے ارد گرد یکساں طور پر فاصلہ رکھتی ہیں، اس لیے وہ 7 جمع یا مائنس کی طرح نظر آتی ہیں، آئیے اسے فاصلہ کے لیے d کہتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 204.2,
  "end": 212.78
 },
 {
  "input": "You also know that the product of these two numbers is 40. ",
  "translatedText": "آپ یہ بھی جانتے ہیں کہ ان دو نمبروں کی پیداوار 40 ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.56,
  "end": 216.38
 },
 {
  "input": "Now to find d, notice that this product expands really nicely, it works out as a difference of squares. ",
  "translatedText": "اب d کو تلاش کرنے کے لیے، نوٹس کریں کہ یہ پروڈکٹ واقعی اچھی طرح پھیلتی ہے، یہ مربعوں کے فرق کے طور پر کام کرتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.6,
  "end": 223.7
 },
 {
  "input": "So from there, you can directly find d: d^2 is 7^2 - 40, or 9, which means d itself is 3. ",
  "translatedText": "تو وہاں سے، آپ براہ راست تلاش کر سکتے ہیں d. d مربع ہے 7 مربع مائنس 40، یا 9، جس کا مطلب ہے کہ d خود 3 ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.56,
  "end": 233.4
 },
 {
  "input": "In other words, the two values for this very specific example work out to be 4 and 10. ",
  "translatedText": "دوسرے الفاظ میں، اس خاص مثال کے لیے دو قدریں 4 اور 10 بنتی ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.38,
  "end": 241.1
 },
 {
  "input": "But our goal is a quick trick, and you wouldn’t want to think this through each time, so let’s wrap up what we just did in a general formula. ",
  "translatedText": "لیکن ہمارا مقصد ایک تیز چال ہے، اور آپ ہر بار اس کے بارے میں سوچنا نہیں چاہیں گے، تو آئیے اسے سمیٹتے ہیں جو ہم نے ابھی ایک عام فارمولے میں کیا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 241.68,
  "end": 248.12
 },
 {
  "input": "For any mean, m and product, p, the distance squared is always going to be m^2 - p. ",
  "translatedText": "کسی بھی اوسط m اور پروڈکٹ p کے لیے، فاصلہ مربع ہمیشہ m مربع مائنس p ہوگا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.64,
  "end": 255.68
 },
 {
  "input": "This gives the third key fact, which is that when two numbers have a mean m and a product p, you can write those two numbers as m ± sqrt(m^2 - p) This is decently fast to rederive on the fly if you ever forget it, and it’s essentially just a rephrasing of the difference of squares formula. ",
  "translatedText": "اس سے تیسری کلیدی حقیقت ملتی ہے، جو یہ ہے کہ جب دو نمبروں کا اوسط m اور ایک مصنوعہ p ہوتا ہے، تو آپ ان دو نمبروں کو m جمع یا مائنس m مربع مائنس p کا مربع جڑ لکھ سکتے ہیں۔اگر آپ اسے بھول جاتے ہیں تو یہ اڑان بھرنے کے لیے بہت تیز ہے، اور یہ بنیادی طور پر صرف مربع فارمولے کے فرق کو دوبارہ بیان کرنا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 257.56,
  "end": 277.08
 },
 {
  "input": "But even still it’s a fact worth memorizing so that you have it at the tip of your fingers. ",
  "translatedText": "لیکن پھر بھی، یہ ایک حقیقت ہے جو یاد رکھنے کے قابل ہے لہذا یہ آپ کی انگلیوں کی نوک پر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.86,
  "end": 281.22
 },
 {
  "input": "In fact, my friend Tim from the channel acapellascience wrote us a quick jingle to make it a little more memorable. ",
  "translatedText": "درحقیقت، چینل A Capella Science سے میرے دوست ٹم نے ہمیں ایک اچھا فوری گینگل لکھا تاکہ اسے تھوڑا سا مزید یادگار بنایا جا سکے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.22,
  "end": 287.16
 },
 {
  "input": "m plus or minus squaaaare root of me squared minus p (ping!) Let me show you how this works, say for the matrix [[3,1], [4,1]]. ",
  "translatedText": "آئیے میں آپ کو دکھاتا ہوں کہ یہ کیسے کام کرتا ہے، میٹرکس 3، 1، 4، 1 کے لیے کہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.9,
  "end": 297.62
 },
 {
  "input": "You start by bringing to mind the formula, maybe stating it all in your head. ",
  "translatedText": "آپ فارمولے کو ذہن میں لا کر شروع کرتے ہیں، شاید یہ سب کچھ آپ کے دماغ میں بیان کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 298.1,
  "end": 301.82
 },
 {
  "input": "But when you write it down, you fill in the appropriate values of m and p as you go. ",
  "translatedText": "لیکن جب آپ اسے لکھتے ہیں، تو آپ جاتے جاتے m اور p کے لیے مناسب قدریں بھرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.2,
  "end": 311.62
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 312.34,
  "end": 317.74
 },
 {
  "input": "So the thing you start writing is 2 ± sqrt(2^2 - …). ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 318.3,
  "end": 322.7
 },
 {
  "input": "Then the product of the eigenvalues is the determinant, which in this example is 3*1 - 1*4, or -1. ",
  "translatedText": "تو اس مثال میں، eigenvalues کا اوسط وہی ہے جو 3 اور 1 کا اوسط ہے، جو کہ 2 ہے، لہذا آپ جس چیز کو لکھنا شروع کرتے ہیں وہ 2 جمع یا مائنس 2 مربع مائنس کا مربع جڑ ہے، پھر eigenvalues کی پیداوار تعین کنندہ ہے، جو اس مثال میں 3 ضرب 1 منفی 1 ضرب 4، یا منفی 1 ہے، تو یہ وہ حتمی چیز ہے جسے آپ بھرتے ہیں، جس کا مطلب ہے کہ eigenvalues 2 جمع یا مائنس 5 کا مربع جڑ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.54,
  "end": 332.14
 },
 {
  "input": "So that’s the final thing you fill in. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 332.38,
  "end": 334.48
 },
 {
  "input": "This means the eigenvalues are 2±sqrt(5). ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.88,
  "end": 338.76
 },
 {
  "input": "You might recognize that this is the same matrix I was using at the beginning, but notice how much more directly we can get at the answer. ",
  "translatedText": "آپ شاید پہچان لیں کہ یہ وہی میٹرکس ہے جو میں شروع میں استعمال کر رہا تھا، لیکن غور کریں کہ ہم جواب سے کتنا براہ راست حاصل کر سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.3,
  "end": 346.5
 },
 {
  "input": "Here, try another one. ",
  "translatedText": "یہاں، ایک اور کوشش کریں. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.14,
  "end": 349.18
 },
 {
  "input": "This time the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5. ",
  "translatedText": "اس بار، eigenvalues کا اوسط وہی ہے جو 2 اور 8 کا اوسط ہے، جو کہ 5 ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 349.44,
  "end": 354.48
 },
 {
  "input": "So again, you start writing out the formula but this time writing 5 in place of m [song]. ",
  "translatedText": "تو پھر، آپ فارمولہ لکھنا شروع کریں، لیکن اس بار m کی جگہ 5 لکھیں، اور پھر تعین کنندہ 2 ضرب 8 منفی 7 ضرب 1، یا 9 ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.1,
  "end": 359.22
 },
 {
  "input": "And then the determinant is 2*8 - 7*1, or 9. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.98,
  "end": 368.3
 },
 {
  "input": "So in this example, the eigenvalues look like 5 ± sqrt(16), which simplifies even further as 9 and 1. ",
  "translatedText": "تو اس مثال میں، eigenvalues 16 کے مربع جڑ کے 5 جمع یا مائنس کی طرح نظر آتے ہیں، جو 9 اور 1 کو مزید آسان بناتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 369.52,
  "end": 378.24
 },
 {
  "input": "You see what I mean about how you can basically just start writing down the eigenvalues while staring at the matrix? ",
  "translatedText": "آپ دیکھ رہے ہیں کہ میرا کیا مطلب ہے کہ جب آپ میٹرکس کو گھور رہے ہوں تو آپ بنیادی طور پر eigenvalues کو کیسے لکھنا شروع کر سکتے ہیں؟ یہ عام طور پر آخر میں آسان بنانے کا سب سے چھوٹا سا حصہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 379.42,
  "end": 384.62
 },
 {
  "input": "It’s typically just the tiniest bit of simplifying at the end. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.28,
  "end": 388.16
 },
 {
  "input": "Honestly, I’ve found myself using this trick a lot when I’m sketching quick notes related to linear algebra and want to use small matrices as examples. ",
  "translatedText": "سچ میں، میں نے خود کو اس چال کو بہت زیادہ استعمال کرتے ہوئے پایا ہے جب میں لکیری الجبرا سے متعلق فوری نوٹوں کا خاکہ بنا رہا ہوں اور چھوٹے میٹرکس کو بطور مثال استعمال کرنا چاہتا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 389.06,
  "end": 395.72
 },
 {
  "input": "I’ve been working on a video about matrix exponents, where eigenvalues pop up a lot, and I realized it’s just very handy if students can read off the eigenvalues from small examples without losing the main line of thought by getting bogged down in a different calculation. ",
  "translatedText": "میں میٹرکس ایکسپوینٹس کے بارے میں ایک ویڈیو پر کام کر رہا ہوں، جہاں ایگن ویلیوز بہت زیادہ پاپ اپ ہوتے ہیں، اور میں سمجھتا ہوں کہ یہ بہت آسان ہے اگر طالب علم چھوٹی چھوٹی مثالوں سے ایگین ویلیوز کو پڑھ سکتے ہیں بغیر کسی دوسرے میں الجھے ہوئے سوچ کی مرکزی لائن کو کھونے کے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.18,
  "end": 408.62
 },
 {
  "input": "As another fun example, take a look at this set of three different matrices, which come up a lot in quantum mechanics, they're known as the Pauli spin matrices. ",
  "translatedText": "حساب ایک اور تفریحی مثال کے طور پر، تین مختلف میٹرکس کے اس سیٹ پر ایک نظر ڈالیں، جو کوانٹم میکانکس میں بہت زیادہ آتا ہے۔انہیں پاؤلی اسپن میٹرکس کے نام سے جانا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.74,
  "end": 417.52
 },
 {
  "input": "If you know quantum mechanics, you’ll know that the eigenvalues of matrices are highly relevant to the physics they describe, and if you don’t know quantum mechanics, let this just be a little glimpse of how these computations are actually relevant to real applications. ",
  "translatedText": "اگر آپ کوانٹم میکانکس جانتے ہیں، تو آپ کو معلوم ہو جائے گا کہ میٹرکس کی ایگن ویلیوز ان کی بیان کردہ فزکس سے بہت زیادہ متعلقہ ہیں۔اور اگر آپ کوانٹم میکینکس نہیں جانتے ہیں، تو یہ صرف اس بات کی ایک چھوٹی سی جھلک ہونے دیں کہ یہ کمپیوٹیشنز حقیقی ایپلی کیشنز سے کس طرح بہت زیادہ متعلقہ ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.6,
  "end": 431.22
 },
 {
  "input": "The mean of the diagonal in all three cases is 0, so the mean of the eigenvalues in all cases is 0, which makes our formula look especially simple. ",
  "translatedText": "تینوں صورتوں میں ترچھی اندراجات کا اوسط صفر ہے۔تو ان تمام صورتوں میں eigenvalues کا اوسط صفر ہے، جو ہمارے فارمولے کو خاص طور پر سادہ نظر آتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.54,
  "end": 443.06
 },
 {
  "input": "What about the products of the eigenvalues, the determinants of these matrices? ",
  "translatedText": "eigenvalues کی مصنوعات کے بارے میں کیا ہے، ان میٹرکس کے تعین کنندگان؟ پہلے کے لیے، یہ 0 منفی 1، یا منفی 1 ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.38,
  "end": 448.8
 },
 {
  "input": "For the first one, it’s 0 - 1 or -1. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.7,
  "end": 453.4
 },
 {
  "input": "The second also looks like 0 - 1, but it takes a moment more to see because of the complex numbers. ",
  "translatedText": "دوسرا بھی 0 مائنس 1 جیسا لگتا ہے، لیکن پیچیدہ نمبروں کی وجہ سے اسے دیکھنے میں ایک لمحہ زیادہ لگتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.4,
  "end": 458.2
 },
 {
  "input": "And the final one looks like -1 - 0. ",
  "translatedText": "اور فائنل منفی 1 مائنس 0 کی طرح لگتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.84,
  "end": 461.36
 },
 {
  "input": "So in all cases, the eigenvalues simplify to be ±1. ",
  "translatedText": "لہذا تمام صورتوں میں، eigenvalues جمع اور مائنس 1 ہونے کے لیے آسان ہو جاتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.06,
  "end": 465.92
 },
 {
  "input": "Although in this case, you really don’t need the formula to find two values if you know theyr'e evenly spaced around 0 and their product is -1. ",
  "translatedText": "اگرچہ اس صورت میں، آپ کو واقعی دو قدریں تلاش کرنے کے لیے کسی فارمولے کی ضرورت نہیں ہے اگر آپ جانتے ہیں کہ وہ یکساں طور پر 0 کے ارد گرد فاصلہ رکھتے ہیں اور ان کی مصنوع منفی 1 ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.72,
  "end": 473.28
 },
 {
  "input": "If you’re curious, in the context of quantum mechanics, these matrices describe observations you might make about a particle's spin in the x, y or z directions. ",
  "translatedText": "اگر آپ متجسس ہیں تو، کوانٹم میکانکس کے تناظر میں، یہ میٹرکس ان مشاہدات کی وضاحت کرتے ہیں جو آپ x، y، یا z سمت میں کسی ذرہ کے گھماؤ کے بارے میں کر سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.64,
  "end": 483.76
 },
 {
  "input": "The fact that their eigenvalues are ±1 corresponds with the idea that the values for the spin that you would observe would be either entirely in one direction or entirely in another, as opposed to something continuously ranging in between. ",
  "translatedText": "اور حقیقت یہ ہے کہ ان کی eigenvalues جمع اور مائنس 1 اس خیال سے مطابقت رکھتی ہے کہ آپ جس گھماؤ کی قدروں کا مشاہدہ کریں گے وہ یا تو مکمل طور پر ایک سمت میں ہوں گی یا مکمل طور پر کسی دوسری سمت میں ہوں گی، جیسا کہ درمیان میں مسلسل کسی چیز کے خلاف ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.76,
  "end": 497.02
 },
 {
  "input": "Maybe you’d wonder how exactly this works, or why you would use 2x2 matrices that have complex numbers to describe spin in three dimensions. ",
  "translatedText": "شاید آپ حیران ہوں گے کہ یہ بالکل کیسے کام کرتا ہے، یا آپ 2x2 میٹرکس کیوں استعمال کریں گے جن میں گھماؤ کو تین جہتوں میں بیان کرنے کے لیے پیچیدہ نمبر ہوتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.32,
  "end": 505.52
 },
 {
  "input": "And those would be fair questions, just outside the scope of what I want to talk about here. ",
  "translatedText": "اور یہ منصفانہ سوالات ہوں گے، اس دائرہ کار سے بالکل باہر جس کے بارے میں میں یہاں بات کرنا چاہتا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 506.1,
  "end": 509.76
 },
 {
  "input": "You know it’s funny, I wrote this section because I wanted some case where you have 2x2 matrices that are not just toy examples or homework problems, ones where they actually come up in practice, and quantum mechanics is great for that. ",
  "translatedText": "آپ جانتے ہیں، یہ مضحکہ خیز ہے، میں نے یہ سیکشن اس لیے لکھا ہے کہ میں کچھ ایسا کیس چاہتا ہوں جہاں آپ کے پاس 2x2 میٹرکس ہوں جو صرف کھلونوں کی مثالیں نہیں ہیں، یا ہوم ورک کے مسائل ہیں، جہاں وہ عملی طور پر سامنے آتے ہیں، اور کوانٹم میکینکس اس کے لیے بہت اچھا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 510.48,
  "end": 521.7
 },
 {
  "input": "But the thing is after I made it I realized that the whole example kind of undercuts the point I’m trying to make. ",
  "translatedText": "لیکن بات یہ ہے کہ میں نے اسے بنانے کے بعد، میں نے محسوس کیا کہ ساری مثال اس نکتے کو کم کرتی ہے جسے میں بنانے کی کوشش کر رہا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.7,
  "end": 528.24
 },
 {
  "input": "For these specific matrices, when you use the traditional method, the one with characteristic polynomials, it’s essentially just as fast; it might actually faster. ",
  "translatedText": "ان مخصوص میٹرکس کے لیے، جب آپ روایتی طریقہ استعمال کرتے ہیں، جس میں خصوصیت والے کثیر الثانیات ہوتے ہیں، یہ بنیادی طور پر اتنا ہی تیز ہوتا ہے۔یہ اصل میں تیز ہو سکتا ہے. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.74,
  "end": 537.64
 },
 {
  "input": "I mean, take a look a the first one: The relevant determinant directly gives you a characteristic polynomial of lambda^2 - 1, and clearly, that has roots of plus and minus 1. ",
  "translatedText": "میرا مطلب ہے، پہلے ایک پر ایک نظر ڈالیں۔متعلقہ تعین کنندہ براہ راست آپ کو لیمبڈا اسکوائر مائنس ون کا ایک خصوصیت والا کثیر الثانی دیتا ہے، اور واضح طور پر جس کی جڑیں جمع اور منفی ایک ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.24,
  "end": 548.2
 },
 {
  "input": "Same answer when you do the second matrix, lambda^2 - 1. ",
  "translatedText": "یہی جواب جب آپ دوسرا میٹرکس کرتے ہیں تو لیمبڈا اسکوائر مائنس ون۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 548.84,
  "end": 551.76
 },
 {
  "input": "And as for the last matrix, forget about doing any computations, traditional or otherwise, it’s already a diagonal matrix, so those diagonal entries are the eigenvalues! ",
  "translatedText": "اور جہاں تک آخری میٹرکس کا تعلق ہے، کوئی بھی کمپیوٹیشن کرنا بھول جائیں، روایتی یا دوسری صورت میں، یہ پہلے سے ہی ایک اخترن میٹرکس ہے، اس لیے وہ اخترن اندراجات ہی ایگن ویلیوز ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.88,
  "end": 562.74
 },
 {
  "input": "However, the example is not totally lost to our cause. ",
  "translatedText": "تاہم، مثال ہمارے مقصد سے مکمل طور پر ضائع نہیں ہوئی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.3,
  "end": 566.92
 },
 {
  "input": "Where you will actually feel the speed up is in the more general case where you take a linear combination of these three matrices and then try to compute the eigenvalues. ",
  "translatedText": "جہاں آپ حقیقت میں محسوس کریں گے کہ اسپیڈ اپ زیادہ عام صورت میں ہے، جہاں آپ ان تین میٹرکس کا ایک لکیری امتزاج لیتے ہیں، اور پھر eigenvalues کی گنتی کرنے کی کوشش کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 567.38,
  "end": 576.06
 },
 {
  "input": "You might write this as a times the first one, plus b times the second, plus c times the third. ",
  "translatedText": "آپ اسے پہلے والے بار، جمع b کے دوسرے، اور c بار تیسرے کے طور پر لکھ سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.82,
  "end": 582.42
 },
 {
  "input": "In quantum mechanics, this would describe spin observations in a general direction of a vector with coordinates [a, b, c]. ",
  "translatedText": "کوانٹم میکانکس میں، یہ ایک ویکٹر کی عمومی سمت میں سپن مشاہدات کو a, b, c کے ساتھ بیان کرے گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.02,
  "end": 589.28
 },
 {
  "input": "More specifically, you should assume this vector is normalized, meaning a^2 + b^2 + c^2 = 1. ",
  "translatedText": "مزید خاص طور پر، آپ کو فرض کرنا چاہیے کہ یہ ویکٹر نارملائز ہے، یعنی ایک مربع جمع b مربع جمع c مربع ایک کے برابر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.9,
  "end": 597.7
 },
 {
  "input": "When you look at this new matrix, it’s immediate to see that the mean of the eigenvalues is still zero, and you might also enjoy pausing for a brief moment to confirm that the product of those eigenvalues is still -1, and then from there concluding what the eigenvalues must be. ",
  "translatedText": "جب آپ اس نئے میٹرکس کو دیکھتے ہیں، تو یہ فوری طور پر نظر آتا ہے کہ eigenvalues کا اوسط اب بھی صفر ہے، اور آپ اس بات کی تصدیق کرنے کے لیے ایک مختصر لمحے کے لیے توقف بھی کر سکتے ہیں کہ ان eigenvalues کی پیداوار اب بھی منفی ہے۔اور پھر وہاں سے، یہ نتیجہ اخذ کرتے ہوئے کہ eigenvalues کیا ہونا چاہیے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.6,
  "end": 615.92
 },
 {
  "input": "And this time, the characteristic polynomial approach would be by comparison a lot more cumbersome, definitely harder to do in your head. ",
  "translatedText": "اور اس بار، خصوصیت کا کثیر الجہتی نقطہ نظر مقابلے کے لحاظ سے بہت زیادہ بوجھل ہوگا، جو یقینی طور پر آپ کے ذہن میں کرنا مشکل ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.22,
  "end": 623.58
 },
 {
  "input": "To be clear, using the mean-product formula is not fundamentally different from finding roots of the characteristic polynomial; I mean, it can't be, they're solving the same problem. ",
  "translatedText": "واضح ہونے کے لیے، اوسط پروڈکٹ فارمولے کا استعمال خصوصیت کے کثیر نام کی جڑیں تلاش کرنے سے مختلف نہیں ہے۔میرا مطلب ہے، یہ نہیں ہو سکتا، وہ ایک ہی مسئلہ کو حل کر رہے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 625.08,
  "end": 633.44
 },
 {
  "input": "One way to think about this, actually, is that the mean-product formula is a nice way to solve quadratic in general (and some viewers of the channel may recognize this). ",
  "translatedText": "اس کے بارے میں سوچنے کا ایک طریقہ، درحقیقت، یہ ہے کہ اوسط پروڈکٹ فارمولہ عام طور پر quadratics کو حل کرنے کا ایک اچھا طریقہ ہے، اور چینل کے کچھ ناظرین اسے پہچان سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.16,
  "end": 641.66
 },
 {
  "input": "This about it: When you’re trying to find the roots of a quadratic given its coefficients, that's another situation where you know the sum of two values, and you also know their product, but you’re trying to recover the original two values. ",
  "translatedText": "اس کے بارے میں سوچیں. جب آپ کواڈراٹک کی جڑیں تلاش کرنے کی کوشش کر رہے ہیں، گتانک کو دیکھتے ہوئے، یہ ایک اور صورت حال ہے جہاں آپ کو دو قدروں کا مجموعہ معلوم ہے، اور آپ ان کی پیداوار کو بھی جانتے ہیں، لیکن آپ اصل دو قدروں کو بازیافت کرنے کی کوشش کر رہے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 642.54,
  "end": 654.1
 },
 {
  "input": "Specifically, if the polynomial is normalized so that this leading coefficient is 1, then the mean of the roots will be -½ times this linear coefficient, which is -1 times the sum of those roots. ",
  "translatedText": "خاص طور پر، اگر کثیر الثانی کو نارملائز کیا جاتا ہے تاکہ یہ لیڈنگ گتانک ایک ہو، تو جڑوں کا اوسط اس لکیری گتانک کا نصف گنا منفی ہوگا، جو ان جڑوں کے مجموعے سے ایک گنا منفی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 655.56,
  "end": 666.88
 },
 {
  "input": "For the example on the screen that makes the mean 5. ",
  "translatedText": "اسکرین پر مثال کے طور پر، اس کا مطلب پانچ بنتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 668.02,
  "end": 670.18
 },
 {
  "input": "And the product of the roots is even easier, it’s just the constant term no adjustments needed. ",
  "translatedText": "اور جڑوں کی پیداوار اور بھی آسان ہے، یہ صرف مستقل اصطلاح ہے، کوئی ایڈجسٹمنٹ کی ضرورت نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.98,
  "end": 676.52
 },
 {
  "input": "So from there, you would apply the mean product formula and that gives you the roots. ",
  "translatedText": "تو وہاں سے، آپ اوسط پروڈکٹ فارمولے کو لاگو کریں گے، اور اس سے آپ کو جڑیں ملتی ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.34,
  "end": 680.9
 },
 {
  "input": "On the one hand, you could think of this as a lighter-weight version of the traditional quadratic formula. ",
  "translatedText": "اور ایک طرف، آپ اسے روایتی چوکور فارمولے کے ہلکے وزن کے ورژن کے طور پر سوچ سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.14,
  "end": 690.22
 },
 {
  "input": "But the real advantage is that it's fewer symbols to memorize, it's that each one of them carries more meaning with it. ",
  "translatedText": "لیکن اصل فائدہ صرف یہ نہیں ہے کہ حفظ کرنے کے لیے کم علامتیں ہیں، بلکہ یہ ہے کہ ان میں سے ہر ایک اپنے ساتھ زیادہ معنی رکھتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 690.96,
  "end": 696.44
 },
 {
  "input": "The whole point of this eigenvalue trick is that because you can read out the mean and product directly from looking at the matrix, you don't need to go through the intermediate step of setting up the characteristic polynomial. ",
  "translatedText": "میرا مطلب ہے، اس eigenvalue کی چال کا پورا نکتہ یہ ہے کہ چونکہ آپ میٹرکس کو دیکھ کر اوسط اور مصنوع کو براہ راست پڑھ سکتے ہیں، اس لیے آپ کو خصوصیت والی کثیر الثانی کو ترتیب دینے کے درمیانی مرحلے سے گزرنے کی ضرورت نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.94,
  "end": 708.0
 },
 {
  "input": "You can jump straight to writing down the roots without ever explicitly thinking about what the polynomial looks like. ",
  "translatedText": "آپ کبھی بھی واضح طور پر یہ سوچے بغیر کہ کثیر نام کیسا لگتا ہے، جڑوں کو لکھنے کے لیے سیدھے چھلانگ لگا سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.42,
  "end": 713.64
 },
 {
  "input": "But to do that we need a version of the quadratic formula where the terms carry some kind of meaning. ",
  "translatedText": "لیکن ایسا کرنے کے لیے، ہمیں چوکور فارمولے کے ایک ورژن کی ضرورت ہے جہاں اصطلاحات کسی قسم کے معنی رکھتی ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 713.84,
  "end": 718.82
 },
 {
  "input": "I realize that this is a very specific trick, for a very specific audience, but it’s something I wish I knew in college, so if you happen to know any students who might benefit from this, consider sharing it with them. ",
  "translatedText": "میں سمجھتا ہوں کہ یہ ایک بہت ہی مخصوص سامعین کے لیے ایک خاص چال ہے، لیکن یہ ایک ایسی چیز ہے جو کاش میں کالج میں جانتا ہوں، لہذا اگر آپ کسی ایسے طالب علم کو جانتے ہیں جو اس سے فائدہ اٹھا سکتے ہیں، تو ان کے ساتھ اشتراک کرنے پر غور کریں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 720.38,
  "end": 729.7
 },
 {
  "input": "The hope is that it’s not just one more thing to memorize, but that the framing reinforces some other nice facts worth knowing, like how the trace and determinant relate to eigenvalues. ",
  "translatedText": "امید یہ ہے کہ یہ صرف ایک اور چیز نہیں ہے جسے آپ حفظ کرتے ہیں، لیکن یہ کہ فریمنگ کچھ دوسرے اچھے حقائق کو تقویت دیتی ہے جو جاننے کے قابل ہیں، جیسے کہ ٹریس اور تعین کنندہ کا تعلق eigenvalues سے کیسے ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.28,
  "end": 739.82
 },
 {
  "input": "If you want to prove those facts, by the way, take a moment to expand out the characteristic polynomial for a general matrix, and think hard about the meaning of each of these coefficients. ",
  "translatedText": "اگر آپ ان حقائق کو ثابت کرنا چاہتے ہیں، تو ویسے، ایک عام میٹرکس کے لیے خصوصیت کے کثیر نام کو پھیلانے کے لیے تھوڑا وقت نکالیں، اور پھر ان میں سے ہر ایک کے مفہوم پر غور کریں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 740.56,
  "end": 749.62
 },
 {
  "input": "Many thanks to Tim, for ensuring that this mean-product formula will stay stuck in all of our heads for at least a few months. ",
  "translatedText": "اس بات کو یقینی بنانے کے لیے ٹِم کا بہت شکریہ کہ یہ مطلب پروڈکٹ فارمولہ کم از کم چند مہینوں تک ہمارے تمام دماغوں میں پھنسا رہے گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 752.4,
  "end": 757.94
 },
 {
  "input": "If you don’t know about acapellascience, please do check it out. ",
  "translatedText": "اگر آپ الکپیلا سائنس کے بارے میں نہیں جانتے ہیں، تو براہ کرم اسے چیک کریں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 761.7,
  "end": 766.0
 },
 {
  "input": "\"The Molecular Shape of You\", in particular, is one of the greatest things on the internet. ",
  "translatedText": "خاص طور پر آپ کی سالماتی شکل انٹرنیٹ پر سب سے بڑی چیزوں میں سے ایک ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.28,
  "end": 769.58
 }
]