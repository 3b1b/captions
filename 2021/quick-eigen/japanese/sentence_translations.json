[
 {
  "input": "This is a video for anyone who already knows what eigenvalues and eigenvectors are, and who might enjoy a quick way to compute them in the case of 2x2 matrices.",
  "translatedText": "これは、固有値と固有ベクトルが何であるかをすでに知っており、2x2 行列 の場合にそれらを簡単に計算する方法を楽しみたい人を対象としたビデオです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 7.56
 },
 {
  "input": "If you're unfamiliar with eigenvalues, go ahead and take a look at this video here, which is actually meant to introduce them.",
  "translatedText": "固有値に慣れていない場合は、実際に固有値を紹介することを目的とし たこのビデオをご覧ください。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 8.58,
  "end": 13.7
 },
 {
  "input": "You can skip ahead if all you want to do is see the trick, but if possible I'd like you to rediscover it for yourself.",
  "translatedText": "トリックを見るだけなら読み飛ばしても構わないが、できれば自分の目で再発見してほしい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 14.68,
  "end": 20.1
 },
 {
  "input": "So for that, let's lay out a little background.",
  "translatedText": "そのために、少し背景を整理しておこう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.58,
  "end": 22.38
 },
 {
  "input": "As a quick reminder, if the effect of a linear transformation on a given vector is to scale that vector by some constant, we call it an eigenvector of the transformation, and we call the relevant scaling factor the corresponding eigenvalue, often denoted with the letter lambda.",
  "translatedText": "簡単な覚え書きとして、あるベクトルに対する線形変換の効果が、そのベクトルをある定数でスケーリングすることである場合、それを変換の固有ベクトルと呼び、関連するスケーリング係数を対応する固有値と呼ぶ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.26,
  "end": 38.6
 },
 {
  "input": "When you write this as an equation, and you rearrange a little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the matrix A minus lambda times the identity must send some non-zero vector, namely the corresponding eigenvector, to the zero vector, which in turn means that the determinant of this modified matrix must be zero.",
  "translatedText": "これを方程式として書き、少し並べ替えると、λが行列Aの固有値である場合、行列Aからλを引いたものに恒等式を乗じたものは、ある非ゼロベクトル、すなわち対応する固有ベクトルをゼロベクトルに送らなければならず、このことは、この修正行列の行列式がゼロでなければならないことを意味する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.84,
  "end": 64.58
 },
 {
  "input": "Okay, that's all a little bit of a mouthful to say, but again, I'm assuming that all of this is review for any of you watching.",
  "translatedText": "さて、ここまでは少し口が裂けてしまいましたが、繰り返しになりますが、これはすべて、ご覧 になっている皆さんにとっての復習だと思います。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.12,
  "end": 71.54
 },
 {
  "input": "So, the usual way to compute eigenvalues, how I used to do it and how I believe most students are taught to carry it out, is to subtract the unknown value lambda off the diagonals, and then solve for the determinant is equal to zero.",
  "translatedText": "つまり、固有値を計算する通常の方法は、私が以前やっていた方法であり、ほとんどの学生が教えている方法だと思うが、対角線から未知の値λを引いて、行列式がゼロに等しいことを解くのである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 72.82,
  "end": 85.86
 },
 {
  "input": "Doing this always involves a few extra steps to expand out and simplify to get a clean quadratic polynomial, what's known as the characteristic polynomial of the matrix.",
  "translatedText": "行列の特性多項式と呼ばれる、きれいな2次多項式を得るために、展開と簡略化のための余分なステップが必要になる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 87.76,
  "end": 96.46
 },
 {
  "input": "The eigenvalues are the roots of this polynomial, so to find them you have to apply the quadratic formula, which itself typically requires one or two more steps of simplification.",
  "translatedText": "固有値はこの多項式の根なので、それを求めるには2次式を適用しなければならない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 97.36,
  "end": 106.54
 },
 {
  "input": "Honestly, the process isn't terrible, but at least for two by two matrices, there is a much more direct way you can get at the answer.",
  "translatedText": "正直なところ、このプロセスはひどいものではないが、少なくとも2×2の行列については、もっと直接的な方法で答えを得ることができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 107.76,
  "end": 114.68
 },
 {
  "input": "And if you want to rediscover this trick, there's only three relevant facts you need to know, each of which is worth knowing in its own right and can help you with other problem solving.",
  "translatedText": "そして、このトリックを再発見したいのなら、知っておくべき関連する事実は3つしかない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 115.4,
  "end": 122.9
 },
 {
  "input": "Number one, the trace of a matrix, which is the sum of these two diagonal entries, is equal to the sum of the eigenvalues.",
  "translatedText": "その1、行列のトレース（2つの対角エントリーの和）は固有値の和に等しい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 123.82,
  "end": 130.92
 },
 {
  "input": "Or, another way to phrase it, more useful for our purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal entries.",
  "translatedText": "あるいは、別の言い方をすれば、我々の目的により役立つのは、2つの固有値の平均は、これら2つの対角エントリーの平均と同じであるということである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.7,
  "end": 139.46
 },
 {
  "input": "Number two, the determinant of a matrix, our usual ad-bc formula, is equal to the product of the two eigenvalues.",
  "translatedText": "その2、行列の行列式は、いつものad-bc式で、2つの固有値の積に等しい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.0,
  "end": 148.96
 },
 {
  "input": "And this should kind of make sense if you understand that eigenvalues describe how much an operator stretches space in a particular direction, and that the determinant describes how much an operator scales areas, or volumes, as a whole.",
  "translatedText": "固有値が、ある作用素がどれだけ空間を特定の方向に引き伸ばすかを表し、行列式が、ある作用素がどれだけ面積や体積を全体として拡大縮小するかを表すことを理解すれば、これはなんとなく理解できるはずだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 150.06,
  "end": 161.76
 },
 {
  "input": "Now before getting to the third fact, notice how you can essentially read these first two values out of the matrix without really writing much down.",
  "translatedText": "3 番目の事実に到達する前に 、実際に多くを書き留めることなく、行列から最初の 2 つの値を本質的にどのように読み取ることができ るかに注目してください。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 162.8,
  "end": 169.16
 },
 {
  "input": "Take this matrix here as an example.",
  "translatedText": "ここでは例としてこのマトリックスを取り上げます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 169.76,
  "end": 171.32
 },
 {
  "input": "Straight away, you can know that the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7.",
  "translatedText": "すぐに、固有値の平均は8と6の平均と同じ7であることがわかる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 171.82,
  "end": 177.82
 },
 {
  "input": "Likewise, most linear algebra students are pretty well practiced at finding the determinant, which in this case works out to be 48 minus 8.",
  "translatedText": "同様に、線形代数を学ぶ学生のほとんどは、行列式を求める練習をかなり積んでいる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 179.58,
  "end": 187.08
 },
 {
  "input": "So right away, you know that the product of the two eigenvalues is 40.",
  "translatedText": "だからすぐに、2つの固有値の積が40であることがわかる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.24,
  "end": 191.7
 },
 {
  "input": "Now take a moment to see if you can derive what will be our third relevant fact, which is how you can quickly recover two numbers when you know their mean and you know their product.",
  "translatedText": "ここで、3 番目の関連事実を導き出せるかどうかを確認してください。これは、2 つの数値の平均と積 がわかっているときに、どのようにして 2 つの数値を迅速に復元できるかということです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 192.78,
  "end": 201.56
 },
 {
  "input": "Here, let's focus on this example.",
  "translatedText": "ここでは、 この例に焦点を当ててみましょう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.46,
  "end": 203.72
 },
 {
  "input": "You know that the two values are evenly spaced around the number 7, so they look like 7 plus or minus something, let's call that something d for distance.",
  "translatedText": "2 つの値は数字の 7 の周りに等間隔に配置されていることがわかります。そのため、7 プラスまたはマイナスの何かのように見えます。これを距離を表す何か d と呼びます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 204.2,
  "end": 212.78
 },
 {
  "input": "You also know that the product of these two numbers is 40.",
  "translatedText": "これら 2 つの数値の積が 40 であることもわかります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 213.56,
  "end": 216.38
 },
 {
  "input": "Now to find d, notice that this product expands really nicely, it works out as a difference of squares.",
  "translatedText": "ここで d を求めると、この積が非常にうまく拡張され、二乗の差として計算されることに注目してくだ さい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 218.6,
  "end": 223.7
 },
 {
  "input": "So from there, you can find d.",
  "translatedText": "だから、そこからdを見つけることができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 224.56,
  "end": 226.86
 },
 {
  "input": "d squared is 7 squared minus 40, or 9, which means that d itself is 3.",
  "translatedText": "dの2乗は7の2乗から40を引いた9であり、これはd自体が3であることを意味する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 228.2,
  "end": 233.4
 },
 {
  "input": "In other words, the two values for this very specific example work out to be 4 and 10.",
  "translatedText": "つまり、この非常に具体的な例の 2 つの値は 4 と 10 になりま す。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.38,
  "end": 241.1
 },
 {
  "input": "But our goal is a quick trick, and you wouldn't want to think through this each time, so let's wrap up what we just did in a general formula.",
  "translatedText": "しかし、私たちの目的は手っ取り早いトリックであり、毎回これを考えるのは面倒だろうから、今やったことを一般的な式でまとめてみよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.68,
  "end": 248.12
 },
 {
  "input": "For any mean m and product p, the distance squared is always going to be m squared minus p.",
  "translatedText": "どんな平均mと積pに対しても、距離の2乗は常にmの2乗からpを引いたものになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.64,
  "end": 255.68
 },
 {
  "input": "This gives the third key fact, which is that when two numbers have a mean m and a product p, you can write those two numbers as m plus or minus the square root of m squared minus p.",
  "translatedText": "つまり、2つの数が平均mと積pを持つとき、その2つの数をm+m乗の平方根マイナスpと書くことができるということである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 257.56,
  "end": 268.46
 },
 {
  "input": "This is decently fast to re-derive on the fly if you ever forget it, and it's essentially just a rephrasing of the difference of squares formula.",
  "translatedText": "これは、もし忘れてしまっても、その場ですぐに再導出することができるし、本質的には2乗の差の公式を言い換えただけのものだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 270.1,
  "end": 277.08
 },
 {
  "input": "But even still, it's a fact that's worth memorizing so it's at the tip of your fingers.",
  "translatedText": "しかし、それでも、指先ひとつでわかるように暗記しておく価値のある事実だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.86,
  "end": 281.22
 },
 {
  "input": "In fact, my friend Tim from the channel A Capella Science wrote us a nice quick jingle to make it a little bit more memorable.",
  "translatedText": "実際、A Capella Science チャンネルの友人の Tim が、少しでも思い出に残るように、素敵 なジングルを書いてくれました。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 281.22,
  "end": 287.16
 },
 {
  "input": "Let me show you how this works, say for the matrix 3 1 4 1.",
  "translatedText": "例えば、3 1 4 1の行列の場合、これがどのように機能するかをお見せしよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 291.9,
  "end": 297.62
 },
 {
  "input": "You start by bringing to mind the formula, maybe stating it all in your head.",
  "translatedText": "まずは公式を思い出し、頭の中ですべてを述べることから始めます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 298.1,
  "end": 301.82
 },
 {
  "input": "But when you write it down, you fill in the appropriate values for m and p as you go.",
  "translatedText": "しかし、それを書き出すときには、mとpに適切な値を記入することになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.2,
  "end": 311.62
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2, so the thing you start writing is 2 plus or minus the square root of 2 squared minus.",
  "translatedText": "つまりこの例では、固有値の平均は3と1の平均と同じであり、2であるから、書き始めるのは2プラスマイナス2の平方根マイナスである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 312.34,
  "end": 322.7
 },
 {
  "input": "Then the product of the eigenvalues is the determinant, which in this example is 3 times 1 minus 1 times 4, or negative 1, so that's the final thing you fill in, which means the eigenvalues are 2 plus or minus the square root of 5.",
  "translatedText": "そして固有値の積が行列式となり、この例では3の1倍から1の4倍を引いたもの、つまりマイナス1である。つまり、固有値は2プラスマイナス5の平方根ということになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.54,
  "end": 338.76
 },
 {
  "input": "You might recognize that this is the same matrix I was using at the beginning, but notice how much more directly we can get at the answer.",
  "translatedText": "これが私が最初に使用したのと同じ行列であることに気づくかもしれませ んが、より直接的に答えを得ることができることに注目してください。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 340.3,
  "end": 346.5
 },
 {
  "input": "Here, try another one.",
  "translatedText": "ここで、別のものを試してください。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 348.14,
  "end": 349.18
 },
 {
  "input": "This time, the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5.",
  "translatedText": "今回の固有値の平均は、2と8の平均と同じ5である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 349.44,
  "end": 354.48
 },
 {
  "input": "So again, you start writing out the formula, but this time writing 5 in place of m.",
  "translatedText": "そこでもう一度、式を書き始めるが、今度はmの代わりに5を書く。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 355.1,
  "end": 359.22
 },
 {
  "input": "And then the determinant is 2 times 8 minus 7 times 1, or 9.",
  "translatedText": "すると、行列式は 2 掛ける 8 から 7 掛ける 1、つまり 9 になります 。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 362.98,
  "end": 368.3
 },
 {
  "input": "So in this example, the eigenvalues look like 5 plus or minus the square root of 16, which simplifies even further as 9 and 1.",
  "translatedText": "したがって、この例では、固有値は 5 プラスマイナス 16 の平方根のようになり、さら に単純化して 9 と 1 になります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 369.52,
  "end": 378.24
 },
 {
  "input": "You see what I mean about how you can basically just start writing down the eigenvalues while you're staring at the matrix?",
  "translatedText": "行列とにらめっこしている間に、固有値を書き出すことができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 379.42,
  "end": 384.62
 },
 {
  "input": "It's typically just the tiniest bit of simplification at the end.",
  "translatedText": "最後にほんの少し簡略化するのが一般的だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 385.28,
  "end": 388.16
 },
 {
  "input": "Honestly, I've found myself using this trick a lot when I'm sketching quick notes related to linear algebra and want to use small matrices as examples.",
  "translatedText": "正直なところ、線形代数に関連する簡単なメモをスケッチしていて、小さな行列を例として使いたいときに、このトリックをよく使っていることに気づいた。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 389.06,
  "end": 395.72
 },
 {
  "input": "I've been working on a video about matrix exponents, where eigenvalues pop up a lot, and I realize it's just very handy if students can read out the eigenvalues from small examples without losing the main line of thought by getting bogged down in a different calculation.",
  "translatedText": "固有値がよく出てくる行列の指数についてのビデオを制作しているのだが、生徒が別の計算に没頭して思考の本筋を見失うことなく、小さな例題から固有値を読み上げることができれば、とても便利だと実感している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.18,
  "end": 408.62
 },
 {
  "input": "As another fun example, take a look at this set of three different matrices, which comes up a lot in quantum mechanics.",
  "translatedText": "別の楽しい例として、量子力学でよく出てくる3つの異なる行列のセットを見てみよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 409.74,
  "end": 415.46
 },
 {
  "input": "They're known as the Pauli spin matrices.",
  "translatedText": "これらはパウリ・スピン行列として知られている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 415.76,
  "end": 417.52
 },
 {
  "input": "If you know quantum mechanics, you'll know that the eigenvalues of matrices are highly relevant to the physics that they describe.",
  "translatedText": "量子力学を知っていれば、行列の固有値が、その行列が記述する物理学に大いに関係していることがわかるだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 418.6,
  "end": 424.42
 },
 {
  "input": "And if you don't know quantum mechanics, let this just be a little glimpse of how these computations are actually very relevant to real applications.",
  "translatedText": "量子力学をご存じない方は、これらの計算が実際のアプリケーションにどのように関係しているかを少し垣間見ていただきたい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.22,
  "end": 431.22
 },
 {
  "input": "The mean of the diagonal entries in all three cases is zero.",
  "translatedText": "3つのケースとも対角エントリーの平均はゼロである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.54,
  "end": 435.88
 },
 {
  "input": "So the mean of the eigenvalues in all of these cases is zero, which makes our formula look especially simple.",
  "translatedText": "つまり、これらの場合の固有値の平均はすべてゼロであり、この式は特に単純に見える。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 437.56,
  "end": 443.06
 },
 {
  "input": "What about the products of the eigenvalues, the determinants of these matrices?",
  "translatedText": "これらの行列の行列式である固有値の積はどうなるでしょうか? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.38,
  "end": 448.8
 },
 {
  "input": "For the first one, it's 0, minus 1, or negative 1.",
  "translatedText": "最初のものは、0、マイナス1、またはマイナス1だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.7,
  "end": 452.56
 },
 {
  "input": "The second one also looks like 0, minus 1, but it takes a moment more to see because of the complex numbers.",
  "translatedText": "2つ目も0から1を引いたように見えるが、複素数のため見るのに時間がかかる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.2,
  "end": 458.2
 },
 {
  "input": "And the final one looks like negative 1, minus 0.",
  "translatedText": "そして最後のものはマイナス 1 マ イナス 0 のように見えます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.84,
  "end": 461.36
 },
 {
  "input": "So in all cases, the eigenvalues simplify to be plus and minus 1.",
  "translatedText": "したがって、すべての場合において、固有値は単純化してプラス 1 とマイナス 1 になります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 462.06,
  "end": 465.92
 },
 {
  "input": "Although in this case, you really don't need a formula to find two values if you know that they're evenly spaced around 0 and their product is negative 1.",
  "translatedText": "ただし、この場合、2 つの値が 0 の周りに等間隔に配置されており、その積がマイナス 1 であることがわかって いる場合、実際には 2 つの値を見つけるための数式は必要ありません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 466.72,
  "end": 473.28
 },
 {
  "input": "If you're curious, in the context of quantum mechanics, these matrices describe observations you might make about a particle's spin in the x, y, or z direction.",
  "translatedText": "もし興味があるなら、量子力学の文脈では、これらの行列は粒子のx、y、z方向のスピンについての観測を記述する。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 474.64,
  "end": 483.12
 },
 {
  "input": "And the fact that their eigenvalues are plus and minus 1 corresponds with the idea that the values for the spin that you would observe would be either entirely in one direction or entirely in another, as opposed to something continuously ranging in between.",
  "translatedText": "その固有値がプラス1とマイナス1であるという事実は、観測されるスピンの値が、その中間の連続的なものであるのとは対照的に、完全に一方向か、あるいは完全に他方向のどちらかであるという考えに対応している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.56,
  "end": 497.02
 },
 {
  "input": "Maybe you'd wonder how exactly this works, or why you would use 2x2 matrices that have complex numbers to describe spin in three dimensions.",
  "translatedText": "これが具体的にどのように機能するのか、あるいはなぜ3次元のスピンを記述するのに複素数を持つ2x2の行列を使うのか、不思議に思うかもしれない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 498.32,
  "end": 505.52
 },
 {
  "input": "Those would be fair questions, just outside the scope of what I want to talk about here.",
  "translatedText": "それは公平な質問だろうが、ここで話したいことの範囲外だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 506.1,
  "end": 509.76
 },
 {
  "input": "You know, it's funny, I wrote this section because I wanted some case where you have 2x2 matrices that aren't just toy examples or homework problems, ones where they actually come up in practice, and quantum mechanics is great for that.",
  "translatedText": "おかしな話だが、このセクションを書いたのは、2x2行列が単なるおもちゃの例や宿題の問題ではなく、実際に出てくるようなケースが欲しかったからだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 510.48,
  "end": 521.7
 },
 {
  "input": "The thing is, after I made it, I realized that the whole example kind of undercuts the point that I'm trying to make.",
  "translatedText": "ただ、これを作った後で、この例は私が言いたいことを台無しにしていることに気づいたんだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 521.7,
  "end": 528.24
 },
 {
  "input": "For these specific matrices, when you use the traditional method, the one with characteristic polynomials, it's essentially just as fast.",
  "translatedText": "これらの特定の行列については、従来の方法、つまり特性多項式を使った方法を使っても、本質的には同じように速くなる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.74,
  "end": 536.1
 },
 {
  "input": "It might actually be faster.",
  "translatedText": "実際に速くなるかもしれない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.22,
  "end": 537.64
 },
 {
  "input": "I mean, take a look at the first one.",
  "translatedText": "つまり、最初の一枚を見てほしい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.24,
  "end": 539.4
 },
 {
  "input": "The relevant determinant directly gives you a characteristic polynomial of lambda squared minus 1, and clearly that has roots of plus and minus 1.",
  "translatedText": "この行列式は、ラムダの2乗から1を引いた特性多項式を直接与える。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.68,
  "end": 548.2
 },
 {
  "input": "Same answer when you do the second matrix, lambda squared minus 1.",
  "translatedText": "2 番目の行列、ラムダ 2 乗マイナス 1 を実行しても同じ答えになります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 548.84,
  "end": 551.76
 },
 {
  "input": "And as for the last matrix, forget about doing any computations, traditional or otherwise, it's already a diagonal matrix, so those diagonal entries are the eigenvalues.",
  "translatedText": "そして最後の行列に関しては、伝統的なものであろうとなかろうと、計算をすることは忘れてほしい。すでに対角行列なので、その対角エントリーが固有値となる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.88,
  "end": 562.74
 },
 {
  "input": "However, the example is not totally lost to our cause.",
  "translatedText": "しかし、この例 は私たちの大義から完全に失われているわけではありません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 564.3,
  "end": 566.92
 },
 {
  "input": "Where you will actually feel the speedup is in the more general case, where you take a linear combination of these three matrices and then try to compute the eigenvalues.",
  "translatedText": "実際にスピードアップを実感できるのは、これら3つの行列の線形結合をとり、固有値を計算しようとする、より一般的な場合である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 567.38,
  "end": 576.06
 },
 {
  "input": "You might write this as a times the first one, plus b times the second, plus c times the third.",
  "translatedText": "これは、最初の a 倍、2 番目の b 倍、3 番目の c 倍として 書くことができます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.82,
  "end": 582.42
 },
 {
  "input": "In quantum mechanics, this would describe spin observations in a general direction of a vector with coordinates a, b, c.",
  "translatedText": "量子力学では、座標a,b,cを持つベクトルの一般的な方向におけるスピン観測を記述することになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.02,
  "end": 589.28
 },
 {
  "input": "More specifically, you should assume that this vector is normalized, meaning a squared plus b squared plus c squared is equal to 1.",
  "translatedText": "より具体的には、このベクトルは正規化されている、つ まり、a の 2 乗と b の 2 乗と c の 2 乗が 1 に等しいと仮定する必要があります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 590.9,
  "end": 597.7
 },
 {
  "input": "When you look at this new matrix, it's immediate to see that the mean of the eigenvalues is still 0.",
  "translatedText": "この新しい行列を見ると、固有値の平均が依然として0であることがすぐにわかる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 598.6,
  "end": 604.1
 },
 {
  "input": "And you might also enjoy pausing for a brief moment to confirm that the product of those eigenvalues is still negative 1.",
  "translatedText": "また、これらの固有値の積がまだマイナス1であることを確認するために、ちょっと立ち止まってみるのも楽しいかもしれない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 604.6,
  "end": 610.9
 },
 {
  "input": "And then from there, concluding what the eigenvalues must be.",
  "translatedText": "そしてそこから、固有値はどうあるべきかを結論づける。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 613.26,
  "end": 615.92
 },
 {
  "input": "And this time, the characteristic polynomial approach would be by comparison a lot more cumbersome, definitely harder to do in your head.",
  "translatedText": "そして今回の特徴的な多項式アプローチは、それに比べてはるかに面倒で、頭の中 で実行するのは間違いなく困難です。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.22,
  "end": 623.58
 },
 {
  "input": "To be clear, using the mean product formula is not fundamentally different from finding roots of the characteristic polynomial.",
  "translatedText": "はっきりさせておくが、平均積の公式を使うことは、特性多項式の根を求めることと根本的に異なるわけではない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 625.08,
  "end": 630.96
 },
 {
  "input": "I mean, it can't be, they're solving the same problem.",
  "translatedText": "つまり、同じ問題を解決しているのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 631.34,
  "end": 633.44
 },
 {
  "input": "One way to think about this actually is that the mean product formula is a nice way to solve quadratics in general.",
  "translatedText": "このことを考える1つの方法として、平均積の公式は一般的な二次方程式を解くのに良い方法だということができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 634.16,
  "end": 639.02
 },
 {
  "input": "And some viewers of the channel may recognize this.",
  "translatedText": "そして、このチャンネルの視聴者の中には、これに見覚えがある人もいるだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 639.6,
  "end": 641.66
 },
 {
  "input": "Think about it, when you're trying to find the roots of a quadratic, given the coefficients, that's another situation where you know the sum of two values, and you also know their product, but you're trying to recover the original two values.",
  "translatedText": "考えてみてほしい。二次方程式の根を求めようとするとき、係数が与えられていれば、それは2つの値の和も知っているし、積も知っているが、元の2つの値を復元しようとしている状況なのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 642.54,
  "end": 654.1
 },
 {
  "input": "Specifically, if the polynomial is normalized, so that this leading coefficient is 1, then the mean of the roots will be negative 1 half times this linear coefficient, which is negative 1 times the sum of those roots.",
  "translatedText": "具体的には、この主要係数が 1 になるように多項式が正規化されている場合、根の平均はこの線形係数の 2 分 の 1 倍の負の値になり、これはこれらの根の合計の 1 倍の負になります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 655.56,
  "end": 666.88
 },
 {
  "input": "With the example on the screen, that makes the mean 5.",
  "translatedText": "画面の例では、平均は5となる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 668.02,
  "end": 670.18
 },
 {
  "input": "And the product of the roots is even easier, it's just the constant term, no adjustments needed.",
  "translatedText": "根の積はさらに簡単で、定数項だけであり、調整は必要ない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 671.98,
  "end": 676.52
 },
 {
  "input": "So from there, you would apply the mean product formula, and that gives you the roots.",
  "translatedText": "つまり、そこから平均積の公式を適用すれば、ルーツが得られるというわけだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 677.34,
  "end": 680.9
 },
 {
  "input": "And on the one hand, you could think of this as a lighter weight version of the traditional quadratic formula.",
  "translatedText": "そして一方では、これは従来の二次方程式の軽量版と考えることもできる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.14,
  "end": 690.22
 },
 {
  "input": "But the real advantage is not just that it's fewer symbols to memorize, it's that each one of them carries more meaning with it.",
  "translatedText": "しかし、本当の利点は、暗記す る記号が少ないというだけではなく、それぞれの記号がより多くの意味を持っているということです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 690.96,
  "end": 696.44
 },
 {
  "input": "I mean, the whole point of this eigenvalue trick is that because you can read out the mean and product directly from looking at the matrix, you don't need to go through the intermediate step of setting up the characteristic polynomial.",
  "translatedText": "つまり、この固有値トリックの要点は、行列を見れば平均と積が直接読み取れるので、特性多項式を設定するという中間ステップを踏む必要がないということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.94,
  "end": 708.0
 },
 {
  "input": "You can jump straight to writing down the roots without ever explicitly thinking about what the polynomial looks like.",
  "translatedText": "多項式がどのようなものであるかを明確 に考えることなく、すぐに根を書き留めることができます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 708.42,
  "end": 713.64
 },
 {
  "input": "But to do that, we need a version of the quadratic formula where the terms carry some kind of meaning.",
  "translatedText": "しかし、そのためには、項が何らかの意味を持つ2次式のバージョンが必要だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 713.84,
  "end": 718.82
 },
 {
  "input": "I realize this is a very specific trick for a very specific audience, but it's something I wish I knew in college, so if you happen to know any students who might benefit from this, consider sharing it with them.",
  "translatedText": "しかし、これは私が大学時代に知っておきたかったことなので、もしこの方法が役に立ちそうな学生をご存知なら、ぜひ教えてあげてほしい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 720.38,
  "end": 729.7
 },
 {
  "input": "The hope is that it's not just one more thing that you memorize, but that the framing reinforces some other nice facts that are worth knowing, like how the trace and the determinant are related to eigenvalues.",
  "translatedText": "単にもう 1 つ暗記するだけでなく、トレースと行列式が固有値にどのように関 係するかなど、知っておく価値のある他の素晴らしい事実がこの枠組みによって強化される ことが期待されます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.28,
  "end": 739.82
 },
 {
  "input": "If you want to prove those facts, by the way, take a moment to expand out the characteristic polynomial for a general matrix, and then think hard about the meaning of each of these coefficients.",
  "translatedText": "ところで、これらの事実を証明したければ、一般的な行列の特性多項式を展開し、それぞれの係数の意味についてじっくり考えてみるといい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 740.56,
  "end": 749.62
 },
 {
  "input": "Many thanks to Tim for ensuring that this mean product formula will stay stuck in all of our heads for at least a few months.",
  "translatedText": "少なくとも数ヶ月は、この平均的な製品フォーミュラが私たち全員の頭に残るようにしてくれたティムに感謝したい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 752.4,
  "end": 757.94
 },
 {
  "input": "If you don't know about alcappella science, please do check it out.",
  "translatedText": "アルカペラ・サイエンスを知らない人は、ぜひチェックしてほしい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 761.7,
  "end": 766.0
 },
 {
  "input": "The molecular shape of you in particular is one of the greatest things on the internet.",
  "translatedText": "特にあなたの分子形状は、インターネット上で最も素晴らしいもののひとつだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.28,
  "end": 769.58
 }
]