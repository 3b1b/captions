[
 {
  "input": "This is a video for anyone who already knows what eigenvalues and eigenvectors are, and who might enjoy a quick way to compute them in the case of 2x2 matrices. ",
  "translatedText": "これは、固有値と固有ベクトルが何であるかをすでに知っており、2x2 行列 の場合にそれらを簡単に計算する方法を楽しみたい人を対象としたビデオです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 7.56
 },
 {
  "input": "If you’re unfamiliar with eigenvalues, take a look at this video which introduces them. ",
  "translatedText": "固有値に慣れていない場合は、実際に固有値を紹介することを目的とし たこのビデオをご覧ください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.58,
  "end": 13.7
 },
 {
  "input": "You can skip ahead if you just want to see the trick, but if possible I’d like you to rediscover it for yourself, so for that let’s lay down a little background. ",
  "translatedText": "コツを知りたいだけなら読み飛ばしていただいて も構いませんが、できればご自身で再発見していただきたいと思います。そのために、 少し背景を説明しましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.68,
  "end": 22.38
 },
 {
  "input": "As a quick reminder, if the effect of a linear transformation on a given vector is to scale it by some constant, we call it an \"eigenvector\" of the transformation, and we call the relevant scaling factor the corresponding \"eigenvalue,\" often denoted with the letter lambda. ",
  "translatedText": "簡単に思い出していただきたいのですが、特定のベクト ルに対する線形変換の効果がそのベクトルをある定数でスケーリングすることである場合、そ れを変換の固有ベクトルと呼び、関連するスケーリング係数を対応する固有値と呼び、多く の場合文字で表されます。ラムダ。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.26,
  "end": 38.6
 },
 {
  "input": "When you write this as an equation and you rearrange a little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the matrix (A minus lambda times the identity) must send some nonzero vector, namely the corresponding eigenvector, to the zero vector, which in turn means the determinant of this modified matrix must be 0. ",
  "translatedText": "これを方程式として書き、少し整理すると、数値ラムダ が行列 A の固有値である場合、行列 A からラムダを掛けた恒等式はゼロ以外のベ クトルを送信する必要があることがわかります。対応する固有ベクトルをゼロ ベクトル に変換します。これは、この変更された行列の行列式がゼロでなければならないことを意味 します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.84,
  "end": 64.58
 },
 {
  "input": "Okay, that’s all a little bit of a mouthful to say, but again, I’m assuming all of this is review for anyone watching. ",
  "translatedText": "さて、ここまでは少し口が裂けてしまいましたが、繰り返しになりますが、これはすべて、ご覧 になっている皆さんにとっての復習だと思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.12,
  "end": 71.54
 },
 {
  "input": "So, the usual way to compute eigenvalues, how I used to do it, and how I believe most students are taught to carry it out, is to subtract the unknown value lambda off the diagonals and then solve for when the determinant equals 0. ",
  "translatedText": "したがって、固有値を計算する通常の方法は、私が 以前どのように計算していたのか、そしてほとんどの学生がそれを実行するように教えられてきた と信じている方法ですが、対角線から未知の値ラムダを減算し、行列式がゼロに等しい場合を解く ことです。。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 72.82,
  "end": 85.86
 },
 {
  "input": "Doing this always involves a few steps to expand out and simplify to get a clean quadratic polynomial, what's known as the “characteristic polynomial” of the matrix. ",
  "translatedText": "これを行うには、行列の「特性多項式」として知られるきれいな 2 次多項式を取得するために拡張および単純化するためのいくつかの手順が常に必要になります。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.76,
  "end": 96.46
 },
 {
  "input": "The eigenvalues are the roots of this polynomial. ",
  "translatedText": "固有値はこの多項式の根です。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 97.36,
  "end": 99.9
 },
 {
  "input": "So to find them you have to apply the quadratic formula, which itself typically requires one or two more steps of simplification. ",
  "translatedText": "したがって、それらを見つけるには、二次公式を適用する必要がありますが、通常、それ自体にさらに 1 つまたは 2 つの単純化ステップが必要です。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.1,
  "end": 106.54
 },
 {
  "input": "Honestly, the process isn’t terrible. ",
  "translatedText": "正直なところ、このプロセスはそれほどひどいものではありません。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 107.76,
  "end": 109.5
 },
 {
  "input": "But at least for 2x2 matrices, there’s a much more direct way to get at this answer. ",
  "translatedText": "しかし、少なくとも 2x2 行列の場合、この答えを得るもっと直接的な方法があります。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.58,
  "end": 114.68
 },
 {
  "input": "And if you want to rediscover this trick, there are only three relevant facts you need to know, each of which is worth knowing in its own right and can help you with other problem-solving. ",
  "translatedText": "このトリックを再発見したい場合、知っておく必要がある関連 事実は 3 つだけです。それぞれの事実はそれ自体で知っておく価値があり、他の問 題解決に役立ちます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 115.4,
  "end": 122.9
 },
 {
  "input": "Number 1: The trace of a matrix, which is the sum of these two diagonal entries, is equal to the sum of the eigenvalues. ",
  "translatedText": "第一に、これら 2 つの対角要素の合計である行列のトレースは 、固有値の合計に等しいということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 123.82,
  "end": 130.92
 },
 {
  "input": "Or another way to phrase it, more useful for our purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal entries. ",
  "translatedText": "または、私たちの目的にとってより便利な別の言 い方は、2 つの固有値の平均がこれら 2 つの対角要素の平均と同じであるということ です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.7,
  "end": 139.46
 },
 {
  "input": "Number 2: The determinant of a matrix, our usual ad-bc formula, is equal to the product of the two eigenvalues. ",
  "translatedText": "2 番目、行列の行列式、通常の ad-bc 式は、2 つの固有値の積に 等しくなります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.0,
  "end": 148.96
 },
 {
  "input": "And this should kind of make sense if you understand that eigenvalues describe how much an operator stretches space in a particular direction and that the determinant describes how much an operator scales areas (or volumes) as a whole. ",
  "translatedText": "そして、固有値はオペレーターが空間を特定の方向にどれだけ引き伸 ばすかを表し、行列式はオペレーターが全体として領域または体積をどれだけ拡大するかを 表すことを理解すれば、これはある程度意味がわかるはずです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.06,
  "end": 161.76
 },
 {
  "input": "Now before getting to the third fact, notice how you can essentially read these first two values out of the matrix without really writing much down. ",
  "translatedText": "3 番目の事実に到達する前に 、実際に多くを書き留めることなく、行列から最初の 2 つの値を本質的にどのように読み取ることができ るかに注目してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 162.8,
  "end": 169.16
 },
 {
  "input": "Take this matrix here as an example. ",
  "translatedText": "ここでは例としてこのマトリックスを取り上げます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.76,
  "end": 171.32
 },
 {
  "input": "Straight away you can know that the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7. ",
  "translatedText": "すぐに、固有値の平均が 8 と 6 の平均と同じ 7 であることがわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 171.82,
  "end": 177.82
 },
 {
  "input": "Likewise, most linear algebra students are pretty well-practiced at finding the determinant, which in this case works out to be 48 - 8 So right away you know that the product of our two eigenvalues is 40. ",
  "translatedText": "同様に、ほとんどの線形 代数学の学生は、行列式を見つける練習をかなりよく積んでいます。この場合、行列式は 48 マイナ ス 8 となります。したがって、2 つの固有値の積が 40 であることがすぐにわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.58,
  "end": 191.7
 },
 {
  "input": "Now take a moment to see how you can derive what will be our third relevant fact, which is how to recover two numbers when you know their mean and you know their product. ",
  "translatedText": "ここで、3 番目の関連事実を導き出せるかどうかを確認してください。これは、2 つの数値の平均と積 がわかっているときに、どのようにして 2 つの数値を迅速に復元できるかということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.78,
  "end": 201.56
 },
 {
  "input": "Here, let's focus on this example. ",
  "translatedText": "ここでは、 この例に焦点を当ててみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.46,
  "end": 203.72
 },
 {
  "input": "You know the two values are evenly spaced around 7, so they look like 7 plus or minus something; let’s call that something \"d\" for distance. ",
  "translatedText": "2 つの値は数字の 7 の周りに等間隔に配置されていることがわかります。そのため、7 プラスまたはマイナスの何かのように見えます。これを距離を表す何か d と呼びます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 204.2,
  "end": 212.78
 },
 {
  "input": "You also know that the product of these two numbers is 40. ",
  "translatedText": "これら 2 つの数値の積が 40 であることもわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.56,
  "end": 216.38
 },
 {
  "input": "Now to find d, notice that this product expands really nicely, it works out as a difference of squares. ",
  "translatedText": "ここで d を求めると、この積が非常にうまく拡張され、二乗の差として計算されることに注目してくだ さい。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.6,
  "end": 223.7
 },
 {
  "input": "So from there, you can directly find d: d^2 is 7^2 - 40, or 9, which means d itself is 3. ",
  "translatedText": "したがって、そこから d を直接見つけることができます。d の 2 乗は、7 の 2 乗から 40 を引いた値、つまり 9 であり、d 自体が 3 であることを意味します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.56,
  "end": 233.4
 },
 {
  "input": "In other words, the two values for this very specific example work out to be 4 and 10. ",
  "translatedText": "つまり、この非常に具体的な例の 2 つの値は 4 と 10 になりま す。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.38,
  "end": 241.1
 },
 {
  "input": "But our goal is a quick trick, and you wouldn’t want to think this through each time, so let’s wrap up what we just did in a general formula. ",
  "translatedText": "しかし、私たちの目標は簡単なトリックであり、これを毎回じっくり考えるのは望ましく ないので、今行ったことを一般的な式でまとめましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 241.68,
  "end": 248.12
 },
 {
  "input": "For any mean, m and product, p, the distance squared is always going to be m^2 - p. ",
  "translatedText": "平均 m と積 p の場 合、距離の二乗は常に m の二乗から p を引いたものになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.64,
  "end": 255.68
 },
 {
  "input": "This gives the third key fact, which is that when two numbers have a mean m and a product p, you can write those two numbers as m ± sqrt(m^2 - p) This is decently fast to rederive on the fly if you ever forget it, and it’s essentially just a rephrasing of the difference of squares formula. ",
  "translatedText": "これにより、3 番目の重要な事実が得られます。つまり、2 つの数値が平均 m と積 p を持つ場合、これら 2 つの数値は、m プラスまたはマイナス m の 2 乗の平方根から p を引いたものとして書けるということです。これは、忘れた場合にその場で再導出するのにかなり高速 であり、本質的には二乗差の公式を言い換えただけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 257.56,
  "end": 277.08
 },
 {
  "input": "But even still it’s a fact worth memorizing so that you have it at the tip of your fingers. ",
  "translatedText": "しかし、それでも、これは覚えておく価値のある事実なので、すぐに覚えておきましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.86,
  "end": 281.22
 },
 {
  "input": "In fact, my friend Tim from the channel acapellascience wrote us a quick jingle to make it a little more memorable. ",
  "translatedText": "実際、A Capella Science チャンネルの友人の Tim が、少しでも思い出に残るように、素敵 なジングルを書いてくれました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.22,
  "end": 287.16
 },
 {
  "input": "m plus or minus squaaaare root of me squared minus p (ping!) Let me show you how this works, say for the matrix [[3,1], [4,1]]. ",
  "translatedText": "たとえば行列 3、1、4、1 の場合、これがどのように機能するかを説明しましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.9,
  "end": 297.62
 },
 {
  "input": "You start by bringing to mind the formula, maybe stating it all in your head. ",
  "translatedText": "まずは公式を思い出し、頭の中ですべてを述べることから始めます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 298.1,
  "end": 301.82
 },
 {
  "input": "But when you write it down, you fill in the appropriate values of m and p as you go. ",
  "translatedText": "ただし、書き留めるときは、m と p の適切な値を入力します。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.2,
  "end": 311.62
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2. ",
  "translatedText": "したがって、この例では、固有値の平均は 3 と 1 の平均と同じ 2 になります。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 312.34,
  "end": 317.74
 },
 {
  "input": "So the thing you start writing is 2 ± sqrt(2^2 - …). ",
  "translatedText": "したがって、書き始めるのは 2 ± sqrt(2^2 - …) です。 ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 318.3,
  "end": 322.7
 },
 {
  "input": "Then the product of the eigenvalues is the determinant, which in this example is 3*1 - 1*4, or -1. ",
  "translatedText": "は行列式で、この例では 3 掛ける 1 マ イナス 1 掛ける 4、つまりマイナス 1 です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.54,
  "end": 332.14
 },
 {
  "input": "So that’s the final thing you fill in. ",
  "translatedText": "これが最後に入力するも のです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 332.38,
  "end": 334.48
 },
 {
  "input": "This means the eigenvalues are 2±sqrt(5). ",
  "translatedText": "つまり、固有値は 2 プラスまたはマイナス 5 の平方根です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.88,
  "end": 338.76
 },
 {
  "input": "You might recognize that this is the same matrix I was using at the beginning, but notice how much more directly we can get at the answer. ",
  "translatedText": "これが私が最初に使用したのと同じ行列であることに気づくかもしれませ んが、より直接的に答えを得ることができることに注目してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.3,
  "end": 346.5
 },
 {
  "input": "Here, try another one. ",
  "translatedText": "ここで、別のものを試してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.14,
  "end": 349.18
 },
 {
  "input": "This time the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5. ",
  "translatedText": "今回は、固有値の平均は 2 と 8 の平均と同じ 5 になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 349.44,
  "end": 354.48
 },
 {
  "input": "So again, you start writing out the formula but this time writing 5 in place of m [song]. ",
  "translatedText": "もう一度、式を書き始めますが、今回は m の代わりに 5 を書 きます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.1,
  "end": 359.22
 },
 {
  "input": "And then the determinant is 2*8 - 7*1, or 9. ",
  "translatedText": "すると、行列式は 2 掛ける 8 から 7 掛ける 1、つまり 9 になります 。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.98,
  "end": 368.3
 },
 {
  "input": "So in this example, the eigenvalues look like 5 ± sqrt(16), which simplifies even further as 9 and 1. ",
  "translatedText": "したがって、この例では、固有値は 5 プラスマイナス 16 の平方根のようになり、さら に単純化して 9 と 1 になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 369.52,
  "end": 378.24
 },
 {
  "input": "You see what I mean about how you can basically just start writing down the eigenvalues while staring at the matrix? ",
  "translatedText": "基本的に行列を見つめながら固有値を 書き留め始める方法について、私が言いたいことはわかりましたか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 379.42,
  "end": 384.62
 },
 {
  "input": "It’s typically just the tiniest bit of simplifying at the end. ",
  "translatedText": "通常、これは最後にほんの少し簡略化するだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.28,
  "end": 388.16
 },
 {
  "input": "Honestly, I’ve found myself using this trick a lot when I’m sketching quick notes related to linear algebra and want to use small matrices as examples. ",
  "translatedText": "正直に言うと、私は線形代数に関連する簡単なメモをスケッチしていて、例として小さな行列を使用した いときに、このトリックをよく使っていることに気づきました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 389.06,
  "end": 395.72
 },
 {
  "input": "I’ve been working on a video about matrix exponents, where eigenvalues pop up a lot, and I realized it’s just very handy if students can read off the eigenvalues from small examples without losing the main line of thought by getting bogged down in a different calculation. ",
  "translatedText": "私は固有値が頻繁に登場する行 列の指数に関するビデオに取り組んでいますが、生徒が別の話に行き詰まって 本筋の思考を見失うことなく、小さな例から固有値を読み取れれば非常に便利 であることに気づきました。計算。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.18,
  "end": 408.62
 },
 {
  "input": "As another fun example, take a look at this set of three different matrices, which come up a lot in quantum mechanics, they're known as the Pauli spin matrices. ",
  "translatedText": "もう 1 つの興味深い例として、量子力学で よく出てくる 3 つの異なる行列のセットを見てみましょう。これらはパウリ スピン行列 として知られています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.74,
  "end": 417.52
 },
 {
  "input": "If you know quantum mechanics, you’ll know that the eigenvalues of matrices are highly relevant to the physics they describe, and if you don’t know quantum mechanics, let this just be a little glimpse of how these computations are actually relevant to real applications. ",
  "translatedText": "量子力学を知っている場合は、行列の固有値が、行列が記述する物理学に非 常に関連していることがわかるでしょう。量子力学を知らない人のために、これらの計算が 実際のアプリケーションにどのように非常に関連しているかを少しだけ見てみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.6,
  "end": 431.22
 },
 {
  "input": "The mean of the diagonal in all three cases is 0, so the mean of the eigenvalues in all cases is 0, which makes our formula look especially simple. ",
  "translatedText": "3 つの場合すべての対角要素の平均は 0 です。したがって、これらすべての場合の固有値の平均はゼロであり、このため式が特に単純 に見えます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.54,
  "end": 443.06
 },
 {
  "input": "What about the products of the eigenvalues, the determinants of these matrices? ",
  "translatedText": "これらの行列の行列式である固有値の積はどうなるでしょうか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.38,
  "end": 448.8
 },
 {
  "input": "For the first one, it’s 0 - 1 or -1. ",
  "translatedText": "最初の値は 0 から 1 を引く、つまりマイナス 1 です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.7,
  "end": 453.4
 },
 {
  "input": "The second also looks like 0 - 1, but it takes a moment more to see because of the complex numbers. ",
  "translatedText": "2 番目も 0 から 1 を引いたように見 えますが、複素数であるため、確認するのに少し時間がかかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.4,
  "end": 458.2
 },
 {
  "input": "And the final one looks like -1 - 0. ",
  "translatedText": "そして最後のものはマイナス 1 マ イナス 0 のように見えます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.84,
  "end": 461.36
 },
 {
  "input": "So in all cases, the eigenvalues simplify to be ±1. ",
  "translatedText": "したがって、すべての場合において、固有値は単純化してプラス 1 とマイナス 1 になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.06,
  "end": 465.92
 },
 {
  "input": "Although in this case, you really don’t need the formula to find two values if you know theyr'e evenly spaced around 0 and their product is -1. ",
  "translatedText": "ただし、この場合、2 つの値が 0 の周りに等間隔に配置されており、その積がマイナス 1 であることがわかって いる場合、実際には 2 つの値を見つけるための数式は必要ありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.72,
  "end": 473.28
 },
 {
  "input": "If you’re curious, in the context of quantum mechanics, these matrices describe observations you might make about a particle's spin in the x, y or z directions. ",
  "translatedText": "ご興味があれば、量子力学の文 脈で、これらの行列は、x、y、または z 方向の粒子のスピンについて行う可能 性のある観察を記述します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.64,
  "end": 483.76
 },
 {
  "input": "The fact that their eigenvalues are ±1 corresponds with the idea that the values for the spin that you would observe would be either entirely in one direction or entirely in another, as opposed to something continuously ranging in between. ",
  "translatedText": "そして、それらの固有値がプラス 1 とマイナス 1 であるという事実は、観察されるスピンの値が、その間で継続的に変化するもので はなく、完全に一方向か完全に別の方向のいずれかであるという考えに対応していま す。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.76,
  "end": 497.02
 },
 {
  "input": "Maybe you’d wonder how exactly this works, or why you would use 2x2 matrices that have complex numbers to describe spin in three dimensions. ",
  "translatedText": "おそらく、これがどのように正確に機能するのか、あるいは 3 次元でスピンを記述するのになぜ複素 数を含む 2x2 行列を使用するのか疑問に思うかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.32,
  "end": 505.52
 },
 {
  "input": "And those would be fair questions, just outside the scope of what I want to talk about here. ",
  "translatedText": "これらは当然の質問ですが 、私がここで話したいことの範囲外です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 506.1,
  "end": 509.76
 },
 {
  "input": "You know it’s funny, I wrote this section because I wanted some case where you have 2x2 matrices that are not just toy examples or homework problems, ones where they actually come up in practice, and quantum mechanics is great for that. ",
  "translatedText": "面白いことに、私がこのセクシ ョンを書いたのは、単なるおもちゃの例や宿題の問題ではなく、実際に実際 に現れる 2x2 行列のケースが欲しかったからです。量子力学はそれに 最適です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 510.48,
  "end": 521.7
 },
 {
  "input": "But the thing is after I made it I realized that the whole example kind of undercuts the point I’m trying to make. ",
  "translatedText": "しかし、問題は、作成した後、この例全体が私が言おうとしている要点を やや損なっていることに気づきました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.7,
  "end": 528.24
 },
 {
  "input": "For these specific matrices, when you use the traditional method, the one with characteristic polynomials, it’s essentially just as fast; it might actually faster. ",
  "translatedText": "これらの特定の行列については、特徴的な 多項式を使用する従来の方法を使用しても、基本的には同じ速度になります。実際 にはもっと速いかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.74,
  "end": 537.64
 },
 {
  "input": "I mean, take a look a the first one: The relevant determinant directly gives you a characteristic polynomial of lambda^2 - 1, and clearly, that has roots of plus and minus 1. ",
  "translatedText": "つまり、最初のものを見てください。関連する行列式は、ラムダの 2 乗マイナス 1 の特性多項式を直接与え、明らかにプラスとマイナス 1 の 根を持ちます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.24,
  "end": 548.2
 },
 {
  "input": "Same answer when you do the second matrix, lambda^2 - 1. ",
  "translatedText": "2 番目の行列、ラムダ 2 乗マイナス 1 を実行しても同じ答えになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 548.84,
  "end": 551.76
 },
 {
  "input": "And as for the last matrix, forget about doing any computations, traditional or otherwise, it’s already a diagonal matrix, so those diagonal entries are the eigenvalues! ",
  "translatedText": "最後の行列については、伝統的であろうがそうでなかろうが、計算を行うことは忘れてくださ い。これはすでに対角行列であるため、これらの対角要素が固有値になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.88,
  "end": 562.74
 },
 {
  "input": "However, the example is not totally lost to our cause. ",
  "translatedText": "しかし、この例 は私たちの大義から完全に失われているわけではありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.3,
  "end": 566.92
 },
 {
  "input": "Where you will actually feel the speed up is in the more general case where you take a linear combination of these three matrices and then try to compute the eigenvalues. ",
  "translatedText": "実際に高速化を感じるのは、より一般的 なケースで、これら 3 つの行列の線形結合を取得し、固有値を計算しよ うとする場合です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 567.38,
  "end": 576.06
 },
 {
  "input": "You might write this as a times the first one, plus b times the second, plus c times the third. ",
  "translatedText": "これは、最初の a 倍、2 番目の b 倍、3 番目の c 倍として 書くことができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.82,
  "end": 582.42
 },
 {
  "input": "In quantum mechanics, this would describe spin observations in a general direction of a vector with coordinates [a, b, c]. ",
  "translatedText": "量子力学では、これは座標 a、b、c を持つベクトルの一般 的な方向でのスピン観測を記述します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.02,
  "end": 589.28
 },
 {
  "input": "More specifically, you should assume this vector is normalized, meaning a^2 + b^2 + c^2 = 1. ",
  "translatedText": "より具体的には、このベクトルは正規化されている、つ まり、a の 2 乗と b の 2 乗と c の 2 乗が 1 に等しいと仮定する必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.9,
  "end": 597.7
 },
 {
  "input": "When you look at this new matrix, it’s immediate to see that the mean of the eigenvalues is still zero, and you might also enjoy pausing for a brief moment to confirm that the product of those eigenvalues is still -1, and then from there concluding what the eigenvalues must be. ",
  "translatedText": "この新しい行列を見ると、固有値の平均がまだゼロであることがすぐにわかります。また 、少しの間立ち止まって、これらの固有値の積がまだ負の 1 であることを確認する のも楽しいかもしれません。そしてそこから、固有値が何であるべきかを結論付けます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.6,
  "end": 615.92
 },
 {
  "input": "And this time, the characteristic polynomial approach would be by comparison a lot more cumbersome, definitely harder to do in your head. ",
  "translatedText": "そして今回の特徴的な多項式アプローチは、それに比べてはるかに面倒で、頭の中 で実行するのは間違いなく困難です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.22,
  "end": 623.58
 },
 {
  "input": "To be clear, using the mean-product formula is not fundamentally different from finding roots of the characteristic polynomial; I mean, it can't be, they're solving the same problem. ",
  "translatedText": "明確にしておきますが、平均積公式の使用 は特性多項式の根を求めることと変わりません。つまり、そんなことはあり得ません、彼らは同じ問題 を解決しているのです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 625.08,
  "end": 633.44
 },
 {
  "input": "One way to think about this, actually, is that the mean-product formula is a nice way to solve quadratic in general (and some viewers of the channel may recognize this). ",
  "translatedText": "実際、これについて考える 1 つの方法は、平均積公式は二次関数一般を解くの に優れた方法であり、チャンネルの視聴者の中にはこれを認識している人もいるかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.16,
  "end": 641.66
 },
 {
  "input": "This about it: When you’re trying to find the roots of a quadratic given its coefficients, that's another situation where you know the sum of two values, and you also know their product, but you’re trying to recover the original two values. ",
  "translatedText": "考えてみてください。係数を指定して 2 次関数の根を求めようとする場合、これ も 2 つの値の和とその積がわかっているものの、元の 2 つの値 を回復しようとしている状況になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 642.54,
  "end": 654.1
 },
 {
  "input": "Specifically, if the polynomial is normalized so that this leading coefficient is 1, then the mean of the roots will be -½ times this linear coefficient, which is -1 times the sum of those roots. ",
  "translatedText": "具体的には、この主要係数が 1 になるように多項式が正規化されている場合、根の平均はこの線形係数の 2 分 の 1 倍の負の値になり、これはこれらの根の合計の 1 倍の負になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 655.56,
  "end": 666.88
 },
 {
  "input": "For the example on the screen that makes the mean 5. ",
  "translatedText": "画面上の例では、平均は 5 になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 668.02,
  "end": 670.18
 },
 {
  "input": "And the product of the roots is even easier, it’s just the constant term no adjustments needed. ",
  "translatedText": "そして、根の積はさらに簡単です。それ は単なる定数項であり、調整は必要ありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.98,
  "end": 676.52
 },
 {
  "input": "So from there, you would apply the mean product formula and that gives you the roots. ",
  "translatedText": "そこから、平均積の式を 適用すると、根が得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.34,
  "end": 680.9
 },
 {
  "input": "On the one hand, you could think of this as a lighter-weight version of the traditional quadratic formula. ",
  "translatedText": "一方で、これは従来の 2 次公 式の軽量バージョンと考えることもできます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.14,
  "end": 690.22
 },
 {
  "input": "But the real advantage is that it's fewer symbols to memorize, it's that each one of them carries more meaning with it. ",
  "translatedText": "しかし、本当の利点は、暗記す る記号が少ないというだけではなく、それぞれの記号がより多くの意味を持っているということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 690.96,
  "end": 696.44
 },
 {
  "input": "The whole point of this eigenvalue trick is that because you can read out the mean and product directly from looking at the matrix, you don't need to go through the intermediate step of setting up the characteristic polynomial. ",
  "translatedText": "つまり、この固有値トリックの要点は、行列を見て平均と積を直接読み 取ることができるため、特性多項式を設定するという中間ステップを 経る必要がないということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.94,
  "end": 708.0
 },
 {
  "input": "You can jump straight to writing down the roots without ever explicitly thinking about what the polynomial looks like. ",
  "translatedText": "多項式がどのようなものであるかを明確 に考えることなく、すぐに根を書き留めることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.42,
  "end": 713.64
 },
 {
  "input": "But to do that we need a version of the quadratic formula where the terms carry some kind of meaning. ",
  "translatedText": "しかし、そのた めには、項が何らかの意味を持つ二次公式のバージョンが必要です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 713.84,
  "end": 718.82
 },
 {
  "input": "I realize that this is a very specific trick, for a very specific audience, but it’s something I wish I knew in college, so if you happen to know any students who might benefit from this, consider sharing it with them. ",
  "translatedText": "これが非常に特定の対象者向けの非常に特殊なトリックであることは承知していますが、大学時代に知っておきたかっ たことなので、これから恩恵を受ける可能性のある学生を知っている場合は、その人たちと共有することを検討してく ださい。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 720.38,
  "end": 729.7
 },
 {
  "input": "The hope is that it’s not just one more thing to memorize, but that the framing reinforces some other nice facts worth knowing, like how the trace and determinant relate to eigenvalues. ",
  "translatedText": "単にもう 1 つ暗記するだけでなく、トレースと行列式が固有値にどのように関 係するかなど、知っておく価値のある他の素晴らしい事実がこの枠組みによって強化される ことが期待されます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.28,
  "end": 739.82
 },
 {
  "input": "If you want to prove those facts, by the way, take a moment to expand out the characteristic polynomial for a general matrix, and think hard about the meaning of each of these coefficients. ",
  "translatedText": "ちなみに、これらの事実を証明したい場合は、 一般行列の特性多項式を展開して、これらの各係数の意味をよく 考えてください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 740.56,
  "end": 749.62
 },
 {
  "input": "Many thanks to Tim, for ensuring that this mean-product formula will stay stuck in all of our heads for at least a few months. ",
  "translatedText": "この意地悪な製品フォーミュラが少なくとも数か月間は私たちの頭 の中に残ることを保証してくれたティムに感謝します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 752.4,
  "end": 757.94
 },
 {
  "input": "If you don’t know about acapellascience, please do check it out. ",
  "translatedText": "アルカペラサイエンスを知らない方はぜ ひチェックしてみてください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 761.7,
  "end": 766.0
 },
 {
  "input": "\"The Molecular Shape of You\", in particular, is one of the greatest things on the internet. ",
  "translatedText": "特にあなたの分子の形状は、インタ ーネット上で最も素晴らしいものの 1 つです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.28,
  "end": 769.58
 }
]