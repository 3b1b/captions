[
 {
  "input": "This is a video for anyone who already knows what eigenvalues and eigenvectors are, and who might enjoy a quick way to compute them in the case of 2x2 matrices.",
  "translatedText": "זהו סרטון לכל מי שכבר יודע מה הם ערכים עצמיים ווקטורים עצמיים, ועשוי ליהנות מדרך מהירה לחשב אותם במקרה של מטריצות 2x2. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 7.56
 },
 {
  "input": "If you're unfamiliar with eigenvalues, go ahead and take a look at this video here, which is actually meant to introduce them.",
  "translatedText": "אם אינך מכיר ערכים עצמיים, עיין בסרטון זה המציג אותם. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 8.58,
  "end": 13.7
 },
 {
  "input": "You can skip ahead if all you want to do is see the trick, but if possible I'd like you to rediscover it for yourself.",
  "translatedText": "אתה יכול לדלג קדימה אם כל מה שאתה רוצה לעשות הוא לראות את הטריק, אבל אם אפשר אני רוצה שתגלה אותו מחדש בעצמך.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 14.68,
  "end": 20.1
 },
 {
  "input": "So for that, let's lay out a little background.",
  "translatedText": "אז בשביל זה, בואו נפתור רקע קטן.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.58,
  "end": 22.38
 },
 {
  "input": "As a quick reminder, if the effect of a linear transformation on a given vector is to scale that vector by some constant, we call it an eigenvector of the transformation, and we call the relevant scaling factor the corresponding eigenvalue, often denoted with the letter lambda.",
  "translatedText": "כתזכורת מהירה, אם ההשפעה של טרנספורמציה ליניארית על וקטור נתון היא לשנות את קנה המידה של אותו וקטור לפי קבוע כלשהו, אנו קוראים לו וקטור עצמי של הטרנספורמציה, ואנו קוראים לגורם קנה המידה הרלוונטי הערך העצמי המתאים, המסומן לעתים קרובות באות למבדה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.26,
  "end": 38.6
 },
 {
  "input": "When you write this as an equation, and you rearrange a little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the matrix A minus lambda times the identity must send some non-zero vector, namely the corresponding eigenvector, to the zero vector, which in turn means that the determinant of this modified matrix must be zero.",
  "translatedText": "כשאתה כותב את זה כמשוואה, ומסדר קצת מחדש, מה שאתה רואה הוא שאם המספר למבדה הוא ערך עצמי של מטריצה A, אז המטריצה A פחות למבדה כפול הזהות חייבת לשלוח איזה וקטור שאינו אפס, כלומר הווקטור העצמי המתאים לוקטור האפס, מה שבתורו אומר שהדטרמיננטה של המטריצה המשתנה הזו חייבת להיות אפס.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.84,
  "end": 64.58
 },
 {
  "input": "Okay, that's all a little bit of a mouthful to say, but again, I'm assuming that all of this is review for any of you watching.",
  "translatedText": "אוקיי, זה הכל קצת לשון הרע לומר, אבל שוב, אני מניח שכל זה הוא סקירה לכל מי שצופה. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.12,
  "end": 71.54
 },
 {
  "input": "So, the usual way to compute eigenvalues, how I used to do it and how I believe most students are taught to carry it out, is to subtract the unknown value lambda off the diagonals, and then solve for the determinant is equal to zero.",
  "translatedText": "לכן, הדרך הרגילה לחישוב ערכים עצמיים, איך נהגתי לעשות זאת ואיך אני מאמין שמלמדים את רוב התלמידים לבצע את זה, היא להחסיר את הערך הלא ידוע למבדה מהאלכסונים, ואז לפתור שהדטרמיננטה שווה לאפס.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 72.82,
  "end": 85.86
 },
 {
  "input": "Doing this always involves a few extra steps to expand out and simplify to get a clean quadratic polynomial, what's known as the characteristic polynomial of the matrix.",
  "translatedText": "פעולה זו כרוכה תמיד בכמה שלבים נוספים כדי להרחיב ולפשט כדי לקבל פולינום ריבועי נקי, מה שמכונה הפולינום האופייני של המטריצה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 87.76,
  "end": 96.46
 },
 {
  "input": "The eigenvalues are the roots of this polynomial, so to find them you have to apply the quadratic formula, which itself typically requires one or two more steps of simplification.",
  "translatedText": "הערכים העצמיים הם השורשים של הפולינום הזה, אז כדי למצוא אותם צריך ליישם את הנוסחה הריבועית, שבעצמה דורשת בדרך כלל עוד צעד אחד או שניים של פישוט.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 97.36,
  "end": 106.54
 },
 {
  "input": "Honestly, the process isn't terrible, but at least for two by two matrices, there is a much more direct way you can get at the answer.",
  "translatedText": "בכנות, התהליך לא נורא, אבל לפחות עבור מטריצות שתיים על שתיים, יש דרך הרבה יותר ישירה שתוכל לקבל את התשובה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 107.76,
  "end": 114.68
 },
 {
  "input": "And if you want to rediscover this trick, there's only three relevant facts you need to know, each of which is worth knowing in its own right and can help you with other problem solving.",
  "translatedText": "ואם אתה רוצה לגלות מחדש את הטריק הזה, יש רק שלוש עובדות רלוונטיות שאתה צריך לדעת, שכל אחת מהן שווה לדעת בפני עצמה ויכולה לעזור לך בפתרון בעיות אחרות.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 115.4,
  "end": 122.9
 },
 {
  "input": "Number one, the trace of a matrix, which is the sum of these two diagonal entries, is equal to the sum of the eigenvalues.",
  "translatedText": "מספר אחד, עקבות המטריצה, שהיא הסכום של שני הערכים האלכסוניים הללו, שווה לסכום הערכים העצמיים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 123.82,
  "end": 130.92
 },
 {
  "input": "Or, another way to phrase it, more useful for our purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal entries.",
  "translatedText": "או, דרך אחרת לנסח את זה, שימושית יותר למטרותינו, היא שהממוצע של שני הערכים העצמיים זהה לממוצע של שני הערכים האלכסוניים האלה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.7,
  "end": 139.46
 },
 {
  "input": "Number two, the determinant of a matrix, our usual ad-bc formula, is equal to the product of the two eigenvalues.",
  "translatedText": "מספר שתיים, הקובע של מטריצה, נוסחת ad-bc הרגילה שלנו, שווה למכפלת שני הערכים העצמיים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.0,
  "end": 148.96
 },
 {
  "input": "And this should kind of make sense if you understand that eigenvalues describe how much an operator stretches space in a particular direction, and that the determinant describes how much an operator scales areas, or volumes, as a whole.",
  "translatedText": "וזה אמור להיות הגיוני במידה מסוימת אם אתה מבין שערכים עצמיים מתארים כמה אופרטור מותח את החלל בכיוון מסוים, ושהדטרמיננטה מתאר עד כמה אופרטור משנה שטחים, או נפחים, כמכלול.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 150.06,
  "end": 161.76
 },
 {
  "input": "Now before getting to the third fact, notice how you can essentially read these first two values out of the matrix without really writing much down.",
  "translatedText": "כעת, לפני שתגיעו לעובדה השלישית, שימו לב כיצד למעשה תוכלו לקרוא את שני הערכים הראשונים הללו מתוך המטריצה מבלי באמת לרשום הרבה. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 162.8,
  "end": 169.16
 },
 {
  "input": "Take this matrix here as an example.",
  "translatedText": "קח את המטריצה הזו כאן כדוגמה. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 169.76,
  "end": 171.32
 },
 {
  "input": "Straight away, you can know that the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7.",
  "translatedText": "מיד אתה יכול לדעת שהממוצע של הערכים העצמיים זהה לממוצע של 8 ו-6, שהם 7.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 171.82,
  "end": 177.82
 },
 {
  "input": "Likewise, most linear algebra students are pretty well practiced at finding the determinant, which in this case works out to be 48 minus 8.",
  "translatedText": "באופן דומה, רוב תלמידי האלגברה הלינארית מתאמנים היטב במציאת הקובע, שבמקרה זה מסתבר להיות 48 פחות 8.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 179.58,
  "end": 187.08
 },
 {
  "input": "So right away, you know that the product of the two eigenvalues is 40.",
  "translatedText": "אז מיד, אתה יודע שהמכפלה של שני הערכים העצמיים היא 40.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.24,
  "end": 191.7
 },
 {
  "input": "Now take a moment to see if you can derive what will be our third relevant fact, which is how you can quickly recover two numbers when you know their mean and you know their product.",
  "translatedText": "עכשיו קח רגע כדי לראות איך אתה יכול להסיק מה תהיה העובדה השלישית הרלוונטית שלנו, והיא איך לשחזר שני מספרים כשאתה יודע את הממוצע שלהם ואתה מכיר את המוצר שלהם. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 192.78,
  "end": 201.56
 },
 {
  "input": "Here, let's focus on this example.",
  "translatedText": "הנה, בואו נתמקד בדוגמה זו. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.46,
  "end": 203.72
 },
 {
  "input": "You know that the two values are evenly spaced around the number 7, so they look like 7 plus or minus something, let's call that something d for distance.",
  "translatedText": "אתה יודע ששני הערכים מרווחים באופן שווה סביב 7, כך שהם נראים כמו 7 פלוס מינוס משהו; בואו נקרא לזה משהו &quot;ד&quot; למרחק. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 204.2,
  "end": 212.78
 },
 {
  "input": "You also know that the product of these two numbers is 40.",
  "translatedText": "אתה גם יודע שהמכפלה של שני המספרים האלה היא 40. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 213.56,
  "end": 216.38
 },
 {
  "input": "Now to find d, notice that this product expands really nicely, it works out as a difference of squares.",
  "translatedText": "עכשיו כדי למצוא את d, שימו לב שהמוצר הזה מתרחב ממש יפה, זה עובד כהבדל של ריבועים. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 218.6,
  "end": 223.7
 },
 {
  "input": "So from there, you can find d.",
  "translatedText": "אז משם, אתה יכול למצוא את ד.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 224.56,
  "end": 226.86
 },
 {
  "input": "d squared is 7 squared minus 40, or 9, which means that d itself is 3.",
  "translatedText": "d בריבוע הוא 7 בריבוע מינוס 40, או 9, מה שאומר ש-d עצמו הוא 3.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 228.2,
  "end": 233.4
 },
 {
  "input": "In other words, the two values for this very specific example work out to be 4 and 10.",
  "translatedText": "במילים אחרות, שני הערכים עבור הדוגמה המאוד ספציפית הזו מתגלים להיות 4 ו-10. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.38,
  "end": 241.1
 },
 {
  "input": "But our goal is a quick trick, and you wouldn't want to think through this each time, so let's wrap up what we just did in a general formula.",
  "translatedText": "אבל המטרה שלנו היא טריק מהיר, ולא הייתם רוצים לחשוב על זה בכל פעם, אז בואו נסכם את מה שעשינו זה עתה בנוסחה כללית.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.68,
  "end": 248.12
 },
 {
  "input": "For any mean m and product p, the distance squared is always going to be m squared minus p.",
  "translatedText": "עבור כל m ממוצע ומכפלה p, המרחק בריבוע תמיד יהיה m בריבוע פחות p.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.64,
  "end": 255.68
 },
 {
  "input": "This gives the third key fact, which is that when two numbers have a mean m and a product p, you can write those two numbers as m plus or minus the square root of m squared minus p.",
  "translatedText": "זה נותן את עובדת המפתח השלישית, שהיא שכאשר לשני מספרים יש ממוצע m ומכפלה p, אתה יכול לכתוב את שני המספרים האלה כ-m פלוס או מינוס השורש הריבועי של m בריבוע מינוס p.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 257.56,
  "end": 268.46
 },
 {
  "input": "This is decently fast to re-derive on the fly if you ever forget it, and it's essentially just a rephrasing of the difference of squares formula.",
  "translatedText": "זה מהיר למדי לגזירה מחדש תוך כדי תנועה אם אי פעם תשכח את זה, וזה בעצם רק ניסוח מחדש של נוסחת ההבדל בין הריבועים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 270.1,
  "end": 277.08
 },
 {
  "input": "But even still, it's a fact that's worth memorizing so it's at the tip of your fingers.",
  "translatedText": "אבל אפילו בכל זאת, זו עובדה שכדאי לשנן אז היא בקצה האצבעות.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.86,
  "end": 281.22
 },
 {
  "input": "In fact, my friend Tim from the channel A Capella Science wrote us a nice quick jingle to make it a little bit more memorable.",
  "translatedText": "למעשה, חברי טים מהערוץ acapellascience כתב לנו ג'ינגל מהיר כדי להפוך אותו לקצת יותר בלתי נשכח. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 281.22,
  "end": 287.16
 },
 {
  "input": "Let me show you how this works, say for the matrix 3 1 4 1.",
  "translatedText": "תן לי להראות לך איך זה עובד, נניח עבור המטריצה 3 1 4 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 291.9,
  "end": 297.62
 },
 {
  "input": "You start by bringing to mind the formula, maybe stating it all in your head.",
  "translatedText": "אתה מתחיל בלהעלות לראש את הנוסחה, אולי לציין את הכל בראש שלך. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 298.1,
  "end": 301.82
 },
 {
  "input": "But when you write it down, you fill in the appropriate values for m and p as you go.",
  "translatedText": "אבל כשאתה כותב את זה, אתה ממלא את הערכים המתאימים עבור m ו-p תוך כדי.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.2,
  "end": 311.62
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2, so the thing you start writing is 2 plus or minus the square root of 2 squared minus.",
  "translatedText": "אז בדוגמה הזו, הממוצע של הערכים העצמיים זהה לממוצע של 3 ו-1, שהוא 2, אז הדבר שאתה מתחיל לכתוב הוא 2 פלוס מינוס השורש הריבועי של 2 בריבוע מינוס.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 312.34,
  "end": 322.7
 },
 {
  "input": "Then the product of the eigenvalues is the determinant, which in this example is 3 times 1 minus 1 times 4, or negative 1, so that's the final thing you fill in, which means the eigenvalues are 2 plus or minus the square root of 5.",
  "translatedText": "ואז המכפלה של הערכים העצמיים הוא הקובע, שבדוגמה זו הוא 3 כפול 1 פחות 1 כפול 4, או 1 שלילי, אז זה הדבר האחרון שאתה ממלא, כלומר הערכים העצמיים הם 2 פלוס או מינוס השורש הריבועי של 5 .",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.54,
  "end": 338.76
 },
 {
  "input": "You might recognize that this is the same matrix I was using at the beginning, but notice how much more directly we can get at the answer.",
  "translatedText": "אולי תזהו שזו אותה מטריצה שבה השתמשתי בהתחלה, אבל שימו לב כמה יותר ישיר אנחנו יכולים להגיע לתשובה. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 340.3,
  "end": 346.5
 },
 {
  "input": "Here, try another one.",
  "translatedText": "הנה, נסה עוד אחד. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 348.14,
  "end": 349.18
 },
 {
  "input": "This time, the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5.",
  "translatedText": "הפעם, הממוצע של הערכים העצמיים זהה לממוצע של 2 ו-8, שהוא 5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 349.44,
  "end": 354.48
 },
 {
  "input": "So again, you start writing out the formula, but this time writing 5 in place of m.",
  "translatedText": "אז שוב, אתה מתחיל לכתוב את הנוסחה, אבל הפעם כותב 5 במקום m.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 355.1,
  "end": 359.22
 },
 {
  "input": "And then the determinant is 2 times 8 minus 7 times 1, or 9.",
  "translatedText": "ואז הקובע הוא 2*8 - 7*1, או 9. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 362.98,
  "end": 368.3
 },
 {
  "input": "So in this example, the eigenvalues look like 5 plus or minus the square root of 16, which simplifies even further as 9 and 1.",
  "translatedText": "אז בדוגמה זו, הערכים העצמיים נראים כמו 5 ± sqrt(16), מה שמפשט עוד יותר כמו 9 ו-1. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 369.52,
  "end": 378.24
 },
 {
  "input": "You see what I mean about how you can basically just start writing down the eigenvalues while you're staring at the matrix?",
  "translatedText": "אתה מבין למה אני מתכוון לגבי איך אתה יכול בעצם להתחיל לרשום את הערכים העצמיים בזמן שאתה בוהה במטריצה?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 379.42,
  "end": 384.62
 },
 {
  "input": "It's typically just the tiniest bit of simplification at the end.",
  "translatedText": "זה בדרך כלל רק הפשטות הקטנות ביותר בסוף.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 385.28,
  "end": 388.16
 },
 {
  "input": "Honestly, I've found myself using this trick a lot when I'm sketching quick notes related to linear algebra and want to use small matrices as examples.",
  "translatedText": "בכנות, מצאתי את עצמי משתמש בטריק הזה הרבה כשאני משרטט הערות מהירות הקשורות לאלגברה לינארית ורוצה להשתמש במטריצות קטנות כדוגמאות.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 389.06,
  "end": 395.72
 },
 {
  "input": "I've been working on a video about matrix exponents, where eigenvalues pop up a lot, and I realize it's just very handy if students can read out the eigenvalues from small examples without losing the main line of thought by getting bogged down in a different calculation.",
  "translatedText": "עבדתי על סרטון על מעריכי מטריצה, שבהם ערכים עצמיים צצים הרבה, ואני מבין שזה פשוט מאוד שימושי אם תלמידים יכולים לקרוא את הערכים העצמיים מדוגמאות קטנות מבלי לאבד את קו המחשבה הראשי על ידי תקיעה תַחשִׁיב.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.18,
  "end": 408.62
 },
 {
  "input": "As another fun example, take a look at this set of three different matrices, which comes up a lot in quantum mechanics.",
  "translatedText": "כדוגמה מהנה נוספת, תסתכל על הסט הזה של שלוש מטריצות שונות, שעולה הרבה במכניקת הקוונטים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 409.74,
  "end": 415.46
 },
 {
  "input": "They're known as the Pauli spin matrices.",
  "translatedText": "הם ידועים בתור מטריצות הספין של פאולי.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 415.76,
  "end": 417.52
 },
 {
  "input": "If you know quantum mechanics, you'll know that the eigenvalues of matrices are highly relevant to the physics that they describe.",
  "translatedText": "אם אתה יודע מכניקת הקוונטים, תדע שהערכים העצמיים של מטריצות רלוונטיים מאוד לפיזיקה שהם מתארים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 418.6,
  "end": 424.42
 },
 {
  "input": "And if you don't know quantum mechanics, let this just be a little glimpse of how these computations are actually very relevant to real applications.",
  "translatedText": "ואם אתה לא יודע מכניקת קוונטים, תן לזה רק להיות הצצה קטנה לאופן שבו החישובים האלה באמת רלוונטיים מאוד ליישומים אמיתיים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.22,
  "end": 431.22
 },
 {
  "input": "The mean of the diagonal entries in all three cases is zero.",
  "translatedText": "הממוצע של הערכים האלכסוניים בכל שלושת המקרים הוא אפס.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.54,
  "end": 435.88
 },
 {
  "input": "So the mean of the eigenvalues in all of these cases is zero, which makes our formula look especially simple.",
  "translatedText": "אז הממוצע של הערכים העצמיים בכל המקרים האלה הוא אפס, מה שגורם לנוסחה שלנו להיראות פשוטה במיוחד.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 437.56,
  "end": 443.06
 },
 {
  "input": "What about the products of the eigenvalues, the determinants of these matrices?",
  "translatedText": "מה לגבי מכפלת הערכים העצמיים, הקובעים של המטריצות הללו? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.38,
  "end": 448.8
 },
 {
  "input": "For the first one, it's 0, minus 1, or negative 1.",
  "translatedText": "עבור הראשון, זה 0, מינוס 1 או 1 שלילי.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.7,
  "end": 452.56
 },
 {
  "input": "The second one also looks like 0, minus 1, but it takes a moment more to see because of the complex numbers.",
  "translatedText": "גם השני נראה כמו 0, מינוס 1, אבל זה לוקח עוד רגע לראות בגלל המספרים המרוכבים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.2,
  "end": 458.2
 },
 {
  "input": "And the final one looks like negative 1, minus 0.",
  "translatedText": "והאחרון נראה כמו -1 - 0. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.84,
  "end": 461.36
 },
 {
  "input": "So in all cases, the eigenvalues simplify to be plus and minus 1.",
  "translatedText": "אז בכל המקרים, הערכים העצמיים מפשטים להיות ±1. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 462.06,
  "end": 465.92
 },
 {
  "input": "Although in this case, you really don't need a formula to find two values if you know that they're evenly spaced around 0 and their product is negative 1.",
  "translatedText": "למרות שבמקרה זה, אתה באמת לא צריך את הנוסחה כדי למצוא שני ערכים אם אתה יודע שהם מרווחים באופן שווה סביב 0 והמוצר שלהם הוא -1. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 466.72,
  "end": 473.28
 },
 {
  "input": "If you're curious, in the context of quantum mechanics, these matrices describe observations you might make about a particle's spin in the x, y, or z direction.",
  "translatedText": "אם אתה סקרן, בהקשר של מכניקת הקוונטים, מטריצות אלה מתארות תצפיות שאתה עשוי לעשות לגבי סיבוב של חלקיק בכיוון x, y או z.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 474.64,
  "end": 483.12
 },
 {
  "input": "And the fact that their eigenvalues are plus and minus 1 corresponds with the idea that the values for the spin that you would observe would be either entirely in one direction or entirely in another, as opposed to something continuously ranging in between.",
  "translatedText": "והעובדה שהערכים העצמיים שלהם הם פלוס ומינוס 1 תואמת את הרעיון שהערכים של הספין שתבחין יהיו לגמרי בכיוון אחד או לגמרי בכיוון אחר, בניגוד למשהו שנע ברציפות ביניהם.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.56,
  "end": 497.02
 },
 {
  "input": "Maybe you'd wonder how exactly this works, or why you would use 2x2 matrices that have complex numbers to describe spin in three dimensions.",
  "translatedText": "אולי אתה תוהה איך בדיוק זה עובד, או למה אתה משתמש במטריצות 2x2 שיש להן מספרים מרוכבים כדי לתאר ספין בתלת מימד.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 498.32,
  "end": 505.52
 },
 {
  "input": "Those would be fair questions, just outside the scope of what I want to talk about here.",
  "translatedText": "אלו יהיו שאלות הוגנות, ממש מחוץ לתחום של מה שאני רוצה לדבר עליו כאן.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 506.1,
  "end": 509.76
 },
 {
  "input": "You know, it's funny, I wrote this section because I wanted some case where you have 2x2 matrices that aren't just toy examples or homework problems, ones where they actually come up in practice, and quantum mechanics is great for that.",
  "translatedText": "אתה יודע, זה מצחיק, כתבתי את הסעיף הזה בגלל שרציתי מקרה שבו יש לך מטריצות 2x2 שאינן רק דוגמאות צעצועים או בעיות של שיעורי בית, כאלה שהן עולות בפועל, ומכניקת הקוונטים היא מצוינת בשביל זה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 510.48,
  "end": 521.7
 },
 {
  "input": "The thing is, after I made it, I realized that the whole example kind of undercuts the point that I'm trying to make.",
  "translatedText": "העניין הוא שאחרי שהכנתי את זה, הבנתי שכל הדוגמה קצת חותרת את הנקודה שאני מנסה להבהיר.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 521.7,
  "end": 528.24
 },
 {
  "input": "For these specific matrices, when you use the traditional method, the one with characteristic polynomials, it's essentially just as fast.",
  "translatedText": "עבור מטריצות ספציפיות אלה, כאשר אתה משתמש בשיטה המסורתית, זו עם פולינומים אופייניים, זה בעצם מהיר באותה מידה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.74,
  "end": 536.1
 },
 {
  "input": "It might actually be faster.",
  "translatedText": "אולי זה באמת יהיה מהיר יותר.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.22,
  "end": 537.64
 },
 {
  "input": "I mean, take a look at the first one.",
  "translatedText": "כלומר, תסתכל על הראשון.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.24,
  "end": 539.4
 },
 {
  "input": "The relevant determinant directly gives you a characteristic polynomial of lambda squared minus 1, and clearly that has roots of plus and minus 1.",
  "translatedText": "הקובע הרלוונטי נותן לך ישירות פולינום אופייני של למבדה בריבוע מינוס 1, וברור שיש לו שורשים של פלוס ומינוס 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.68,
  "end": 548.2
 },
 {
  "input": "Same answer when you do the second matrix, lambda squared minus 1.",
  "translatedText": "אותה תשובה כשאתה עושה את המטריצה השנייה, lambda^2 - 1. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 548.84,
  "end": 551.76
 },
 {
  "input": "And as for the last matrix, forget about doing any computations, traditional or otherwise, it's already a diagonal matrix, so those diagonal entries are the eigenvalues.",
  "translatedText": "ולגבי המטריצה האחרונה, תשכחו לעשות חישובים, מסורתיים או אחרים, היא כבר מטריצה אלכסונית, אז הערכים האלכסוניים האלה הם הערכים העצמיים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.88,
  "end": 562.74
 },
 {
  "input": "However, the example is not totally lost to our cause.",
  "translatedText": "עם זאת, הדוגמה לא אבדה לחלוטין למטרה שלנו. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 564.3,
  "end": 566.92
 },
 {
  "input": "Where you will actually feel the speedup is in the more general case, where you take a linear combination of these three matrices and then try to compute the eigenvalues.",
  "translatedText": "המקום שבו אתה בעצם תרגיש את המהירות היא במקרה הכללי יותר, שבו אתה לוקח שילוב ליניארי של שלוש המטריצות האלה ואז מנסה לחשב את הערכים העצמיים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 567.38,
  "end": 576.06
 },
 {
  "input": "You might write this as a times the first one, plus b times the second, plus c times the third.",
  "translatedText": "אתה יכול לכתוב את זה ככפול הראשון, פלוס b כפול השני, פלוס c כפול השלישי. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.82,
  "end": 582.42
 },
 {
  "input": "In quantum mechanics, this would describe spin observations in a general direction of a vector with coordinates a, b, c.",
  "translatedText": "במכניקת הקוונטים, זה יתאר תצפיות ספין בכיוון כללי של וקטור עם קואורדינטות a, b, c.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.02,
  "end": 589.28
 },
 {
  "input": "More specifically, you should assume that this vector is normalized, meaning a squared plus b squared plus c squared is equal to 1.",
  "translatedText": "ליתר דיוק, עליך להניח שהווקטור הזה מנורמל, כלומר a^2 + b^2 + c^2 = 1. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 590.9,
  "end": 597.7
 },
 {
  "input": "When you look at this new matrix, it's immediate to see that the mean of the eigenvalues is still 0.",
  "translatedText": "כאשר אתה מסתכל על המטריצה החדשה הזו, זה מיידי לראות שהממוצע של הערכים העצמיים הוא עדיין 0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 598.6,
  "end": 604.1
 },
 {
  "input": "And you might also enjoy pausing for a brief moment to confirm that the product of those eigenvalues is still negative 1.",
  "translatedText": "וייתכן שתהנה גם לעצור לרגע קצר כדי לאשר שהמכפלה של הערכים העצמיים האלה עדיין שלילי 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 604.6,
  "end": 610.9
 },
 {
  "input": "And then from there, concluding what the eigenvalues must be.",
  "translatedText": "ומשם, מסקנה מה הערכים העצמיים חייבים להיות.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 613.26,
  "end": 615.92
 },
 {
  "input": "And this time, the characteristic polynomial approach would be by comparison a lot more cumbersome, definitely harder to do in your head.",
  "translatedText": "והפעם, הגישה הפולינומית האופיינית תהיה בהשוואה הרבה יותר מסורבלת, בהחלט קשה יותר לעשות בראש שלך. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.22,
  "end": 623.58
 },
 {
  "input": "To be clear, using the mean product formula is not fundamentally different from finding roots of the characteristic polynomial.",
  "translatedText": "שיהיה ברור, השימוש בנוסחת המוצר הממוצע אינו שונה מהותית ממציאת שורשים של הפולינום האופייני.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 625.08,
  "end": 630.96
 },
 {
  "input": "I mean, it can't be, they're solving the same problem.",
  "translatedText": "כלומר, זה לא יכול להיות, הם פותרים את אותה בעיה.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 631.34,
  "end": 633.44
 },
 {
  "input": "One way to think about this actually is that the mean product formula is a nice way to solve quadratics in general.",
  "translatedText": "דרך אחת לחשוב על זה בעצם היא שנוסחת המוצר הממוצע היא דרך נחמדה לפתור ריבועים באופן כללי.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 634.16,
  "end": 639.02
 },
 {
  "input": "And some viewers of the channel may recognize this.",
  "translatedText": "וחלק מהצופים בערוץ עשויים לזהות זאת.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 639.6,
  "end": 641.66
 },
 {
  "input": "Think about it, when you're trying to find the roots of a quadratic, given the coefficients, that's another situation where you know the sum of two values, and you also know their product, but you're trying to recover the original two values.",
  "translatedText": "תחשוב על זה, כשאתה מנסה למצוא את השורשים של ריבוע, בהינתן המקדמים, זה עוד מצב שבו אתה יודע את הסכום של שני ערכים, ואתה יודע גם את התוצר שלהם, אבל אתה מנסה לשחזר את השניים המקוריים ערכים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 642.54,
  "end": 654.1
 },
 {
  "input": "Specifically, if the polynomial is normalized, so that this leading coefficient is 1, then the mean of the roots will be negative 1 half times this linear coefficient, which is negative 1 times the sum of those roots.",
  "translatedText": "באופן ספציפי, אם הפולינום מנורמל כך שמקדם מוביל זה הוא 1, אז הממוצע של השורשים יהיה -½ כפול מקדם הליניארי הזה, שהוא -1 כפול מסכום השורשים הללו. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 655.56,
  "end": 666.88
 },
 {
  "input": "With the example on the screen, that makes the mean 5.",
  "translatedText": "עם הדוגמה על המסך, זה הופך את הממוצע ל-5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 668.02,
  "end": 670.18
 },
 {
  "input": "And the product of the roots is even easier, it's just the constant term, no adjustments needed.",
  "translatedText": "והתוצר של השורשים הוא אפילו יותר קל, זה רק המונח הקבוע, אין צורך בהתאמות.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 671.98,
  "end": 676.52
 },
 {
  "input": "So from there, you would apply the mean product formula, and that gives you the roots.",
  "translatedText": "אז משם, היית מיישם את נוסחת המוצר הממוצעת, וזה נותן לך את השורשים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 677.34,
  "end": 680.9
 },
 {
  "input": "And on the one hand, you could think of this as a lighter weight version of the traditional quadratic formula.",
  "translatedText": "ומצד אחד, אתה יכול לחשוב על זה כעל גרסה קלה יותר של הנוסחה הריבועית המסורתית.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.14,
  "end": 690.22
 },
 {
  "input": "But the real advantage is not just that it's fewer symbols to memorize, it's that each one of them carries more meaning with it.",
  "translatedText": "אבל היתרון האמיתי הוא שיש פחות סמלים לשנן, זה שלכל אחד מהם יש יותר משמעות. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 690.96,
  "end": 696.44
 },
 {
  "input": "I mean, the whole point of this eigenvalue trick is that because you can read out the mean and product directly from looking at the matrix, you don't need to go through the intermediate step of setting up the characteristic polynomial.",
  "translatedText": "אני מתכוון, כל העניין של טריק הערך העצמי הזה הוא שבגלל שאתה יכול לקרוא את הממוצע והמכפל ישירות מהסתכלות על המטריצה, אתה לא צריך לעבור את שלב הביניים של הגדרת הפולינום האופייני.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.94,
  "end": 708.0
 },
 {
  "input": "You can jump straight to writing down the roots without ever explicitly thinking about what the polynomial looks like.",
  "translatedText": "אתה יכול לקפוץ ישר לרשום את השורשים מבלי לחשוב באופן מפורש על איך נראה הפולינום. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 708.42,
  "end": 713.64
 },
 {
  "input": "But to do that, we need a version of the quadratic formula where the terms carry some kind of meaning.",
  "translatedText": "אבל כדי לעשות זאת, אנחנו צריכים גרסה של הנוסחה הריבועית שבה המונחים נושאים איזושהי משמעות.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 713.84,
  "end": 718.82
 },
 {
  "input": "I realize this is a very specific trick for a very specific audience, but it's something I wish I knew in college, so if you happen to know any students who might benefit from this, consider sharing it with them.",
  "translatedText": "אני מבין שזהו טריק מאוד ספציפי לקהל מאוד ספציפי, אבל זה משהו שהלוואי שידעתי בקולג&#39;, אז אם במקרה אתה מכיר סטודנטים שיכולים להפיק מכך תועלת, שקול לשתף אותו איתם.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 720.38,
  "end": 729.7
 },
 {
  "input": "The hope is that it's not just one more thing that you memorize, but that the framing reinforces some other nice facts that are worth knowing, like how the trace and the determinant are related to eigenvalues.",
  "translatedText": "התקווה היא שזה לא רק עוד דבר אחד שצריך לשנן, אלא שהמסגור מחזק עוד כמה עובדות נחמדות שכדאי לדעת, כמו איך העקבות והקביעה קשורים לערכים עצמיים. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.28,
  "end": 739.82
 },
 {
  "input": "If you want to prove those facts, by the way, take a moment to expand out the characteristic polynomial for a general matrix, and then think hard about the meaning of each of these coefficients.",
  "translatedText": "אם אתה רוצה להוכיח את העובדות האלה, אגב, הקדישו רגע להרחבת הפולינום האופייני למטריצה כללית, ואז תחשבו היטב על המשמעות של כל אחד מהמקדמים הללו.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 740.56,
  "end": 749.62
 },
 {
  "input": "Many thanks to Tim for ensuring that this mean product formula will stay stuck in all of our heads for at least a few months.",
  "translatedText": "תודה רבה לטים שהבטיח שנוסחת המוצר הממוצעת הזו תישאר תקועה בראשנו לפחות לכמה חודשים.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 752.4,
  "end": 757.94
 },
 {
  "input": "If you don't know about alcappella science, please do check it out.",
  "translatedText": "אם אינך יודע על מדע אלקפלה, אנא בדוק זאת.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 761.7,
  "end": 766.0
 },
 {
  "input": "The molecular shape of you in particular is one of the greatest things on the internet.",
  "translatedText": "הצורה המולקולרית שלך במיוחד היא אחד הדברים הגדולים ביותר באינטרנט.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.28,
  "end": 769.58
 }
]