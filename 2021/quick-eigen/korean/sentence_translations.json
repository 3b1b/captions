[
 {
  "input": "This is a video for anyone who already knows what eigenvalues and eigenvectors are, and who might enjoy a quick way to compute them in the case of 2x2 matrices. ",
  "translatedText": "이것은 고유값과 고유벡터가 무엇인지 이미 알고 있고 2x2 행렬의 경우 이를 계산하는 빠른 방법을 즐기는 모든 사람을 위한 비디오입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 7.56
 },
 {
  "input": "If you’re unfamiliar with eigenvalues, take a look at this video which introduces them. ",
  "translatedText": "고유값에 대해 잘 모르신다면 실제로 고유값을 소개하는 이 동영상을 시청해 보세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 8.58,
  "end": 13.7
 },
 {
  "input": "You can skip ahead if you just want to see the trick, but if possible I’d like you to rediscover it for yourself, so for that let’s lay down a little background. ",
  "translatedText": "요령만 보고 싶다면 건너뛰셔도 됩니다. 하지만 가능하다면 직접 재발견해 보시기 바랍니다. 이를 위해 약간의 배경 지식을 설명하겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.68,
  "end": 22.38
 },
 {
  "input": "As a quick reminder, if the effect of a linear transformation on a given vector is to scale it by some constant, we call it an \"eigenvector\" of the transformation, and we call the relevant scaling factor the corresponding \"eigenvalue,\" often denoted with the letter lambda. ",
  "translatedText": "다시 한번 말씀드리자면, 주어진 벡터에 대한 선형 변환의 효과가 해당 벡터를 어떤 상수로 스케일링하는 것이라면, 이를 변환의 고유벡터라고 부르고 관련 스케일링 인자를 해당 고유값이라고 부릅니다. 종종 문자로 표시됩니다. 람다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.26,
  "end": 38.6
 },
 {
  "input": "When you write this as an equation and you rearrange a little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the matrix (A minus lambda times the identity) must send some nonzero vector, namely the corresponding eigenvector, to the zero vector, which in turn means the determinant of this modified matrix must be 0. ",
  "translatedText": "이것을 방정식으로 작성하고 약간 재배열하면 숫자 람다가 행렬 A의 고유값이면 행렬 A에서 람다를 곱한 항등식은 0이 아닌 벡터를 보내야 한다는 것을 알 수 있습니다. 해당 고유벡터를 0 벡터로 변환합니다. 이는 수정된 행렬의 행렬식이 0이어야 함을 의미합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.84,
  "end": 64.58
 },
 {
  "input": "Okay, that’s all a little bit of a mouthful to say, but again, I’m assuming all of this is review for anyone watching. ",
  "translatedText": "좋아, 말하기엔 좀 장황하지만, 다시 한 번 말씀드리지만, 이 모든 것은 시청하시는 분들을 위한 리뷰라고 가정합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 66.12,
  "end": 71.54
 },
 {
  "input": "So, the usual way to compute eigenvalues, how I used to do it, and how I believe most students are taught to carry it out, is to subtract the unknown value lambda off the diagonals and then solve for when the determinant equals 0. ",
  "translatedText": "따라서 고유값을 계산하는 일반적인 방법, 제가 사용했던 방식과 대부분의 학생들이 이를 수행하도록 배웠다고 생각하는 방식은 대각선에서 알려지지 않은 값 람다를 뺀 다음 행렬식이 0과 같을 때를 푸는 것입니다. . ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 72.82,
  "end": 85.86
 },
 {
  "input": "Doing this always involves a few steps to expand out and simplify to get a clean quadratic polynomial, what's known as the “characteristic polynomial” of the matrix. ",
  "translatedText": "이를 수행하려면 항상 행렬의 &quot;특성 다항식&quot;으로 알려진 깨끗한 2차 다항식을 얻기 위해 확장하고 단순화하는 몇 가지 단계가 필요합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.76,
  "end": 96.46
 },
 {
  "input": "The eigenvalues are the roots of this polynomial. ",
  "translatedText": "고유값은 이 다항식의 근입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 97.36,
  "end": 99.9
 },
 {
  "input": "So to find them you have to apply the quadratic formula, which itself typically requires one or two more steps of simplification. ",
  "translatedText": "따라서 이를 찾으려면 이차 공식을 적용해야 하며, 이 공식 자체에는 일반적으로 한두 단계의 단순화 단계가 더 필요합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.1,
  "end": 106.54
 },
 {
  "input": "Honestly, the process isn’t terrible. ",
  "translatedText": "솔직히 그 과정은 나쁘지 않습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 107.76,
  "end": 109.5
 },
 {
  "input": "But at least for 2x2 matrices, there’s a much more direct way to get at this answer. ",
  "translatedText": "그러나 적어도 2x2 행렬의 경우에는 이 답을 얻을 수 있는 훨씬 더 직접적인 방법이 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.58,
  "end": 114.68
 },
 {
  "input": "And if you want to rediscover this trick, there are only three relevant facts you need to know, each of which is worth knowing in its own right and can help you with other problem-solving. ",
  "translatedText": "그리고 이 트릭을 재발견하고 싶다면 알아야 할 세 가지 관련 사실이 있습니다. 각 사실은 그 자체로 알 가치가 있고 다른 문제 해결에 도움이 될 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 115.4,
  "end": 122.9
 },
 {
  "input": "Number 1: The trace of a matrix, which is the sum of these two diagonal entries, is equal to the sum of the eigenvalues. ",
  "translatedText": "첫째, 이 두 대각선 항목의 합인 행렬의 자취는 고유값의 합과 같습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 123.82,
  "end": 130.92
 },
 {
  "input": "Or another way to phrase it, more useful for our purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal entries. ",
  "translatedText": "또는 우리의 목적에 더 유용하게 표현하는 다른 방법은 두 고유값의 평균이 이 두 대각선 항목의 평균과 동일하다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.7,
  "end": 139.46
 },
 {
  "input": "Number 2: The determinant of a matrix, our usual ad-bc formula, is equal to the product of the two eigenvalues. ",
  "translatedText": "우리의 일반적인 ad-bc 공식인 행렬의 행렬식인 두 번째는 두 고유값의 곱과 같습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 141.0,
  "end": 148.96
 },
 {
  "input": "And this should kind of make sense if you understand that eigenvalues describe how much an operator stretches space in a particular direction and that the determinant describes how much an operator scales areas (or volumes) as a whole. ",
  "translatedText": "그리고 고유값은 연산자가 특정 방향으로 공간을 얼마나 늘리는지 설명하고 행렬식은 연산자가 면적이나 부피를 전체적으로 얼마나 확장하는지 설명한다는 것을 이해하면 이해가 될 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.06,
  "end": 161.76
 },
 {
  "input": "Now before getting to the third fact, notice how you can essentially read these first two values out of the matrix without really writing much down. ",
  "translatedText": "이제 세 번째 사실에 도달하기 전에 실제로 많은 내용을 기록하지 않고도 행렬에서 처음 두 값을 본질적으로 읽을 수 있는 방법에 대해 알아보세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 162.8,
  "end": 169.16
 },
 {
  "input": "Take this matrix here as an example. ",
  "translatedText": "여기 이 행렬을 예로 들어 보겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.76,
  "end": 171.32
 },
 {
  "input": "Straight away you can know that the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7. ",
  "translatedText": "곧바로 고유값의 평균이 8과 6의 평균인 7과 동일하다는 것을 알 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 171.82,
  "end": 177.82
 },
 {
  "input": "Likewise, most linear algebra students are pretty well-practiced at finding the determinant, which in this case works out to be 48 - 8 So right away you know that the product of our two eigenvalues is 40. ",
  "translatedText": "마찬가지로, 대부분의 선형 대수학 학생들은 행렬식을 찾는 데 꽤 능숙하며, 이 경우 행렬식은 48 빼기 8이 됩니다. 따라서 즉시 두 고유값의 곱이 40이라는 것을 알 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.58,
  "end": 191.7
 },
 {
  "input": "Now take a moment to see how you can derive what will be our third relevant fact, which is how to recover two numbers when you know their mean and you know their product. ",
  "translatedText": "이제 잠시 시간을 내어 세 번째 관련 사실이 무엇인지 도출할 수 있는지 살펴보십시오. 이는 두 숫자의 평균과 곱을 알 때 두 숫자를 빠르게 복구할 수 있는 방법입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 192.78,
  "end": 201.56
 },
 {
  "input": "Here, let's focus on this example. ",
  "translatedText": "여기서는 이 예에 중점을 두겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 202.46,
  "end": 203.72
 },
 {
  "input": "You know the two values are evenly spaced around 7, so they look like 7 plus or minus something; let’s call that something \"d\" for distance. ",
  "translatedText": "두 값은 숫자 7 주위에 균등한 간격으로 배치되어 있으므로 7에 더하기 또는 빼기 값처럼 보입니다. 거리를 d라고 부르겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 204.2,
  "end": 212.78
 },
 {
  "input": "You also know that the product of these two numbers is 40. ",
  "translatedText": "또한 이 두 숫자의 곱이 40이라는 것도 알고 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.56,
  "end": 216.38
 },
 {
  "input": "Now to find d, notice that this product expands really nicely, it works out as a difference of squares. ",
  "translatedText": "이제 d를 찾으려면 이 곱이 정말 멋지게 확장되고 제곱의 차이로 계산됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 218.6,
  "end": 223.7
 },
 {
  "input": "So from there, you can directly find d: d^2 is 7^2 - 40, or 9, which means d itself is 3. ",
  "translatedText": "그래서 거기에서 d를 직접 찾을 수 있습니다. d 제곱은 7의 제곱 빼기 40, 즉 9입니다. 즉, d 자체는 3입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 224.56,
  "end": 233.4
 },
 {
  "input": "In other words, the two values for this very specific example work out to be 4 and 10. ",
  "translatedText": "즉, 이 매우 구체적인 예의 두 값은 4와 10이 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.38,
  "end": 241.1
 },
 {
  "input": "But our goal is a quick trick, and you wouldn’t want to think this through each time, so let’s wrap up what we just did in a general formula. ",
  "translatedText": "하지만 우리의 목표는 빠른 트릭이며 매번 이것을 깊이 생각하고 싶지 않을 것이므로 방금 수행한 작업을 일반적인 공식으로 마무리하겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 241.68,
  "end": 248.12
 },
 {
  "input": "For any mean, m and product, p, the distance squared is always going to be m^2 - p. ",
  "translatedText": "평균 m과 곱 p에 대해 거리 제곱은 항상 m 제곱에서 p를 뺀 값이 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.64,
  "end": 255.68
 },
 {
  "input": "This gives the third key fact, which is that when two numbers have a mean m and a product p, you can write those two numbers as m ± sqrt(m^2 - p) This is decently fast to rederive on the fly if you ever forget it, and it’s essentially just a rephrasing of the difference of squares formula. ",
  "translatedText": "이것은 세 번째 핵심 사실을 제공합니다. 즉, 두 숫자가 평균 m과 곱 p를 가질 때 이 두 숫자를 m 더하기 또는 빼기 m 제곱 빼기 p의 제곱근으로 쓸 수 있다는 것입니다. 이것은 잊어버린 경우 즉석에서 다시 파생하는 것이 상당히 빠르며 본질적으로 제곱의 차이 공식을 다시 표현한 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 257.56,
  "end": 277.08
 },
 {
  "input": "But even still it’s a fact worth memorizing so that you have it at the tip of your fingers. ",
  "translatedText": "하지만 그럼에도 불구하고 외울 가치가 있는 사실이기 때문에 아직은 미루어 두고 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.86,
  "end": 281.22
 },
 {
  "input": "In fact, my friend Tim from the channel acapellascience wrote us a quick jingle to make it a little more memorable. ",
  "translatedText": "사실 A Capella Science 채널의 내 친구 Tim이 좀 더 기억에 남을 수 있도록 멋지고 빠른 노래를 만들어 주었습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.22,
  "end": 287.16
 },
 {
  "input": "m plus or minus squaaaare root of me squared minus p (ping!) Let me show you how this works, say for the matrix [[3,1], [4,1]]. ",
  "translatedText": "행렬 3, 1, 4, 1에 대해 이것이 어떻게 작동하는지 보여드리겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 291.9,
  "end": 297.62
 },
 {
  "input": "You start by bringing to mind the formula, maybe stating it all in your head. ",
  "translatedText": "공식을 염두에 두는 것부터 시작합니다. 어쩌면 머리 속으로 모든 것을 설명할 수도 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 298.1,
  "end": 301.82
 },
 {
  "input": "But when you write it down, you fill in the appropriate values of m and p as you go. ",
  "translatedText": "하지만 적어보면 m과 p의 적절한 값을 채워가면서 쓰게 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 306.2,
  "end": 311.62
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2. ",
  "translatedText": "따라서 이 예에서 고유값의 평균은 3과 1의 평균인 2와 같습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 312.34,
  "end": 317.74
 },
 {
  "input": "So the thing you start writing is 2 ± sqrt(2^2 - …). ",
  "translatedText": "따라서 쓰기 시작하는 것은 2 ± sqrt(2^2 - …)입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 318.3,
  "end": 322.7
 },
 {
  "input": "Then the product of the eigenvalues is the determinant, which in this example is 3*1 - 1*4, or -1. ",
  "translatedText": "는 행렬식입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 323.54,
  "end": 332.14
 },
 {
  "input": "So that’s the final thing you fill in. ",
  "translatedText": "이 예에서는 3 곱하기 1 빼기 1 곱하기 4 또는 -1이므로 이것이 마지막으로 채워지는 값입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 332.38,
  "end": 334.48
 },
 {
  "input": "This means the eigenvalues are 2±sqrt(5). ",
  "translatedText": "고유값은 2 더하기 또는 빼기 5의 제곱근이라는 의미입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 334.88,
  "end": 338.76
 },
 {
  "input": "You might recognize that this is the same matrix I was using at the beginning, but notice how much more directly we can get at the answer. ",
  "translatedText": "이것이 제가 처음에 사용했던 것과 동일한 행렬이라는 것을 알 수 있을 것입니다. 그러나 우리가 답을 얼마나 더 직접적으로 얻을 수 있는지 주목하십시오. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.3,
  "end": 346.5
 },
 {
  "input": "Here, try another one. ",
  "translatedText": "여기, 다른 것을 시도해 보세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 348.14,
  "end": 349.18
 },
 {
  "input": "This time the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5. ",
  "translatedText": "이번에는 고유값의 평균이 2와 8의 평균인 5와 같습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 349.44,
  "end": 354.48
 },
 {
  "input": "So again, you start writing out the formula but this time writing 5 in place of m [song]. ",
  "translatedText": "그래서 다시 공식을 쓰기 시작합니다. 이번에는 m [노래] 자리에 5를 씁니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.1,
  "end": 359.22
 },
 {
  "input": "And then the determinant is 2*8 - 7*1, or 9. ",
  "translatedText": "그리고 행렬식은 2*8 - 7*1, 즉 9입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.98,
  "end": 368.3
 },
 {
  "input": "So in this example, the eigenvalues look like 5 ± sqrt(16), which simplifies even further as 9 and 1. ",
  "translatedText": "따라서 이 예에서 고유값은 5 ± sqrt(16)처럼 보이며 이는 9와 1로 더욱 단순화됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 369.52,
  "end": 378.24
 },
 {
  "input": "You see what I mean about how you can basically just start writing down the eigenvalues while staring at the matrix? ",
  "translatedText": "기본적으로 행렬을 쳐다보는 동안 고유값을 기록하기 시작할 수 있다는 것이 무슨 뜻인지 아시겠습니까? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 379.42,
  "end": 384.62
 },
 {
  "input": "It’s typically just the tiniest bit of simplifying at the end. ",
  "translatedText": "일반적으로 마지막에는 가장 작은 단순화에 불과합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.28,
  "end": 388.16
 },
 {
  "input": "Honestly, I’ve found myself using this trick a lot when I’m sketching quick notes related to linear algebra and want to use small matrices as examples. ",
  "translatedText": "솔직히 저는 선형 대수학에 관련된 빠른 노트를 스케치하고 작은 행렬을 예로 사용하고 싶을 때 이 트릭을 많이 사용하는 것을 발견했습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 389.06,
  "end": 395.72
 },
 {
  "input": "I’ve been working on a video about matrix exponents, where eigenvalues pop up a lot, and I realized it’s just very handy if students can read off the eigenvalues from small examples without losing the main line of thought by getting bogged down in a different calculation. ",
  "translatedText": "저는 고유값이 많이 나타나는 행렬 지수에 관한 비디오 작업을 하고 있는데, 학생들이 다른 문제에 빠져서 주된 생각을 잃지 않고 작은 예에서 고유값을 읽을 수 있다면 매우 편리하다는 것을 깨달았습니다. 계산. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 396.18,
  "end": 408.62
 },
 {
  "input": "As another fun example, take a look at this set of three different matrices, which come up a lot in quantum mechanics, they're known as the Pauli spin matrices. ",
  "translatedText": "또 다른 재미있는 예로, 양자역학에서 많이 등장하는 세 가지 서로 다른 행렬 세트를 살펴보세요. 이는 Pauli 스핀 행렬로 알려져 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.74,
  "end": 417.52
 },
 {
  "input": "If you know quantum mechanics, you’ll know that the eigenvalues of matrices are highly relevant to the physics they describe, and if you don’t know quantum mechanics, let this just be a little glimpse of how these computations are actually relevant to real applications. ",
  "translatedText": "양자 역학을 알고 있다면 행렬의 고유값이 그들이 설명하는 물리학과 매우 관련이 있다는 것을 알게 될 것입니다. 양자역학을 모른다면 이러한 계산이 실제로 실제 응용 프로그램과 어떻게 관련되는지 간단히 살펴보겠습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.6,
  "end": 431.22
 },
 {
  "input": "The mean of the diagonal in all three cases is 0, so the mean of the eigenvalues in all cases is 0, which makes our formula look especially simple. ",
  "translatedText": "세 가지 경우 모두 대각선 항목의 평균은 0입니다. 따라서 이 모든 경우의 고유값의 평균은 0이므로 공식이 특히 단순해 보입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.54,
  "end": 443.06
 },
 {
  "input": "What about the products of the eigenvalues, the determinants of these matrices? ",
  "translatedText": "이 행렬의 행렬식인 고유값의 곱은 어떻습니까? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 445.38,
  "end": 448.8
 },
 {
  "input": "For the first one, it’s 0 - 1 or -1. ",
  "translatedText": "첫 번째 값은 0 - 1 또는 -1입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.7,
  "end": 453.4
 },
 {
  "input": "The second also looks like 0 - 1, but it takes a moment more to see because of the complex numbers. ",
  "translatedText": "두 번째도 0-1처럼 보이지만 복소수 때문에 보는 데 시간이 더 걸립니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.4,
  "end": 458.2
 },
 {
  "input": "And the final one looks like -1 - 0. ",
  "translatedText": "그리고 마지막은 -1 빼기 0처럼 보입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.84,
  "end": 461.36
 },
 {
  "input": "So in all cases, the eigenvalues simplify to be ±1. ",
  "translatedText": "따라서 모든 경우에 고유값은 플러스 및 마이너스 1로 단순화됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.06,
  "end": 465.92
 },
 {
  "input": "Although in this case, you really don’t need the formula to find two values if you know theyr'e evenly spaced around 0 and their product is -1. ",
  "translatedText": "하지만 이 경우에는 두 값이 0 주위에 균일한 간격으로 있고 그 곱이 -1이라는 것을 알고 있다면 두 값을 찾는 데 공식이 실제로 필요하지 않습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 466.72,
  "end": 473.28
 },
 {
  "input": "If you’re curious, in the context of quantum mechanics, these matrices describe observations you might make about a particle's spin in the x, y or z directions. ",
  "translatedText": "궁금하다면 양자 역학의 맥락에서 이러한 행렬은 x, y 또는 z 방향의 입자 스핀에 대해 수행할 수 있는 관찰을 설명합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.64,
  "end": 483.76
 },
 {
  "input": "The fact that their eigenvalues are ±1 corresponds with the idea that the values for the spin that you would observe would be either entirely in one direction or entirely in another, as opposed to something continuously ranging in between. ",
  "translatedText": "그리고 그들의 고유값이 플러스와 마이너스 1이라는 사실은 여러분이 관찰하게 될 스핀 값이 그 사이에 연속적으로 범위를 갖는 것이 아니라 완전히 한 방향이거나 완전히 다른 방향일 것이라는 생각과 일치합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 483.76,
  "end": 497.02
 },
 {
  "input": "Maybe you’d wonder how exactly this works, or why you would use 2x2 matrices that have complex numbers to describe spin in three dimensions. ",
  "translatedText": "아마도 이것이 정확히 어떻게 작동하는지, 또는 왜 3차원 스핀을 설명하기 위해 복소수가 있는 2x2 행렬을 사용하는지 궁금할 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.32,
  "end": 505.52
 },
 {
  "input": "And those would be fair questions, just outside the scope of what I want to talk about here. ",
  "translatedText": "그리고 그것은 제가 여기서 이야기하고 싶은 범위를 벗어나는 공정한 질문이 될 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 506.1,
  "end": 509.76
 },
 {
  "input": "You know it’s funny, I wrote this section because I wanted some case where you have 2x2 matrices that are not just toy examples or homework problems, ones where they actually come up in practice, and quantum mechanics is great for that. ",
  "translatedText": "아시다시피, 재밌습니다. 제가 이 섹션을 쓴 이유는 단지 장난감 예제나 숙제 문제가 아닌 2x2 행렬이 실제로 실제로 나타나는 경우를 원했기 때문입니다. 양자 역학은 이에 적합합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 510.48,
  "end": 521.7
 },
 {
  "input": "But the thing is after I made it I realized that the whole example kind of undercuts the point I’m trying to make. ",
  "translatedText": "하지만 문제는 제가 그것을 만들고 나서 전체 예제가 제가 말하려는 요점을 약화시킨다는 것을 깨달았다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.7,
  "end": 528.24
 },
 {
  "input": "For these specific matrices, when you use the traditional method, the one with characteristic polynomials, it’s essentially just as fast; it might actually faster. ",
  "translatedText": "이러한 특정 행렬의 경우 특성 다항식을 사용하는 전통적인 방법을 사용하면 기본적으로 속도가 빠릅니다. 실제로는 더 빠를 수도 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 528.74,
  "end": 537.64
 },
 {
  "input": "I mean, take a look a the first one: The relevant determinant directly gives you a characteristic polynomial of lambda^2 - 1, and clearly, that has roots of plus and minus 1. ",
  "translatedText": "즉, 첫 번째 것을 살펴보십시오. 관련 행렬식은 람다 제곱 - 1의 특성 다항식을 직접 제공하며 분명히 플러스와 마이너스 1의 근을 갖습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.24,
  "end": 548.2
 },
 {
  "input": "Same answer when you do the second matrix, lambda^2 - 1. ",
  "translatedText": "두 번째 행렬인 람다 제곱 빼기 1을 수행할 때에도 같은 답이 나옵니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 548.84,
  "end": 551.76
 },
 {
  "input": "And as for the last matrix, forget about doing any computations, traditional or otherwise, it’s already a diagonal matrix, so those diagonal entries are the eigenvalues! ",
  "translatedText": "그리고 마지막 행렬은 전통적이든 아니든 계산을 수행하는 것을 잊어버리세요. 이는 이미 대각 행렬이므로 해당 대각선 항목은 고유값입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.88,
  "end": 562.74
 },
 {
  "input": "However, the example is not totally lost to our cause. ",
  "translatedText": "그러나 그 예가 우리의 주장에서 완전히 사라진 것은 아닙니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 564.3,
  "end": 566.92
 },
 {
  "input": "Where you will actually feel the speed up is in the more general case where you take a linear combination of these three matrices and then try to compute the eigenvalues. ",
  "translatedText": "실제로 속도 향상을 느낄 수 있는 곳은 세 가지 행렬의 선형 결합을 취한 다음 고유값을 계산하는 보다 일반적인 경우입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 567.38,
  "end": 576.06
 },
 {
  "input": "You might write this as a times the first one, plus b times the second, plus c times the third. ",
  "translatedText": "이것을 a 곱하기 첫 번째, 더하기 b 두 번째 곱하기, c 곱하기 세 번째로 쓸 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 576.82,
  "end": 582.42
 },
 {
  "input": "In quantum mechanics, this would describe spin observations in a general direction of a vector with coordinates [a, b, c]. ",
  "translatedText": "양자 역학에서 이는 좌표 a, b, c를 갖는 벡터의 일반적인 방향으로 스핀 관찰을 설명합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.02,
  "end": 589.28
 },
 {
  "input": "More specifically, you should assume this vector is normalized, meaning a^2 + b^2 + c^2 = 1. ",
  "translatedText": "더 구체적으로 말하면, 이 벡터가 정규화되어 있다고 가정해야 합니다. 즉, a 제곱 + b 제곱 + c 제곱은 1과 같습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.9,
  "end": 597.7
 },
 {
  "input": "When you look at this new matrix, it’s immediate to see that the mean of the eigenvalues is still zero, and you might also enjoy pausing for a brief moment to confirm that the product of those eigenvalues is still -1, and then from there concluding what the eigenvalues must be. ",
  "translatedText": "이 새로운 행렬을 보면 고유값의 평균이 여전히 0이라는 것을 즉시 확인할 수 있으며 고유값의 곱이 여전히 음수인지 확인하기 위해 잠시 멈춰서 즐길 수도 있습니다. 그리고 거기에서 고유값이 무엇인지 결론을 내립니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 598.6,
  "end": 615.92
 },
 {
  "input": "And this time, the characteristic polynomial approach would be by comparison a lot more cumbersome, definitely harder to do in your head. ",
  "translatedText": "그리고 이번에는 특징적인 다항식 접근 방식이 비교해 보면 훨씬 더 번거롭고 머리 속에서 수행하기가 확실히 더 어려울 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.22,
  "end": 623.58
 },
 {
  "input": "To be clear, using the mean-product formula is not fundamentally different from finding roots of the characteristic polynomial; I mean, it can't be, they're solving the same problem. ",
  "translatedText": "명확하게 말하면, 평균 곱 공식을 사용하는 것은 특성 다항식의 근을 찾는 것과 다르지 않습니다. 내 말은, 그럴 리가 없다는 것입니다. 그들은 같은 문제를 해결하고 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 625.08,
  "end": 633.44
 },
 {
  "input": "One way to think about this, actually, is that the mean-product formula is a nice way to solve quadratic in general (and some viewers of the channel may recognize this). ",
  "translatedText": "실제로 이에 대해 생각하는 한 가지 방법은 평균 곱 공식이 일반적으로 이차 방정식을 푸는 좋은 방법이며 채널의 일부 시청자가 이를 인식할 수 있다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 634.16,
  "end": 641.66
 },
 {
  "input": "This about it: When you’re trying to find the roots of a quadratic given its coefficients, that's another situation where you know the sum of two values, and you also know their product, but you’re trying to recover the original two values. ",
  "translatedText": "생각해 보세요. 계수가 주어졌을 때 이차식의 근을 찾으려고 할 때 이는 두 값의 합과 곱도 알고 있지만 원래 두 값을 복구하려고 하는 또 다른 상황입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 642.54,
  "end": 654.1
 },
 {
  "input": "Specifically, if the polynomial is normalized so that this leading coefficient is 1, then the mean of the roots will be -½ times this linear coefficient, which is -1 times the sum of those roots. ",
  "translatedText": "특히, 이 선행 계수가 1이 되도록 다항식을 정규화하면 근의 평균은 이 선형 계수의 1/2이 음수가 되며, 이는 해당 근의 합의 1배가 음수가 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 655.56,
  "end": 666.88
 },
 {
  "input": "For the example on the screen that makes the mean 5. ",
  "translatedText": "화면의 예에서는 평균이 5가 됩니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 668.02,
  "end": 670.18
 },
 {
  "input": "And the product of the roots is even easier, it’s just the constant term no adjustments needed. ",
  "translatedText": "그리고 근의 곱은 훨씬 더 쉽습니다. 단지 상수항일 뿐이며 조정이 필요하지 않습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.98,
  "end": 676.52
 },
 {
  "input": "So from there, you would apply the mean product formula and that gives you the roots. ",
  "translatedText": "따라서 거기에서 평균 곱 공식을 적용하면 뿌리가 나옵니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 677.34,
  "end": 680.9
 },
 {
  "input": "On the one hand, you could think of this as a lighter-weight version of the traditional quadratic formula. ",
  "translatedText": "한편으로는 이것을 전통적인 이차 공식의 더 가벼운 버전으로 생각할 수도 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.14,
  "end": 690.22
 },
 {
  "input": "But the real advantage is that it's fewer symbols to memorize, it's that each one of them carries more meaning with it. ",
  "translatedText": "하지만 진짜 장점은 기억해야 할 기호가 적다는 것뿐만 아니라 각 기호가 더 많은 의미를 담고 있다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 690.96,
  "end": 696.44
 },
 {
  "input": "The whole point of this eigenvalue trick is that because you can read out the mean and product directly from looking at the matrix, you don't need to go through the intermediate step of setting up the characteristic polynomial. ",
  "translatedText": "내 말은, 이 고유값 트릭의 요점은 행렬을 보면서 평균과 곱을 직접 읽을 수 있기 때문에 특성 다항식을 설정하는 중간 단계를 거칠 필요가 없다는 것입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.94,
  "end": 708.0
 },
 {
  "input": "You can jump straight to writing down the roots without ever explicitly thinking about what the polynomial looks like. ",
  "translatedText": "다항식이 어떻게 생겼는지 명시적으로 생각하지 않고도 바로 근을 쓰는 것으로 넘어갈 수 있습니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.42,
  "end": 713.64
 },
 {
  "input": "But to do that we need a version of the quadratic formula where the terms carry some kind of meaning. ",
  "translatedText": "하지만 그렇게 하려면 용어가 어떤 의미를 전달하는 이차 공식의 버전이 필요합니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 713.84,
  "end": 718.82
 },
 {
  "input": "I realize that this is a very specific trick, for a very specific audience, but it’s something I wish I knew in college, so if you happen to know any students who might benefit from this, consider sharing it with them. ",
  "translatedText": "나는 이것이 매우 특정한 청중을 위한 매우 구체적인 트릭이라는 것을 알고 있지만 대학에서 알았더라면 좋았을 것입니다. 따라서 이로부터 이익을 얻을 수 있는 학생을 알고 있다면 그들과 공유하는 것을 고려하십시오. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 720.38,
  "end": 729.7
 },
 {
  "input": "The hope is that it’s not just one more thing to memorize, but that the framing reinforces some other nice facts worth knowing, like how the trace and determinant relate to eigenvalues. ",
  "translatedText": "여러분이 기억하는 것이 단지 하나 더 있는 것이 아니라, 추적과 행렬식이 고유값과 어떻게 관련되어 있는지와 같이 알아야 할 가치가 있는 다른 좋은 사실을 프레이밍이 강화하기를 바랍니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 730.28,
  "end": 739.82
 },
 {
  "input": "If you want to prove those facts, by the way, take a moment to expand out the characteristic polynomial for a general matrix, and think hard about the meaning of each of these coefficients. ",
  "translatedText": "그런데 이러한 사실을 증명하고 싶다면 잠시 시간을 내어 일반 행렬의 특성 다항식을 확장한 다음 각 계수의 의미에 대해 열심히 생각해 보십시오. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 740.56,
  "end": 749.62
 },
 {
  "input": "Many thanks to Tim, for ensuring that this mean-product formula will stay stuck in all of our heads for at least a few months. ",
  "translatedText": "이 비열한 제품 공식이 적어도 몇 달 동안 우리 머리 속에 머물게 해준 Tim에게 많은 감사를 드립니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 752.4,
  "end": 757.94
 },
 {
  "input": "If you don’t know about acapellascience, please do check it out. ",
  "translatedText": "알카펠라 과학에 대해 모르신다면 꼭 확인해 보세요. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 761.7,
  "end": 766.0
 },
 {
  "input": "\"The Molecular Shape of You\", in particular, is one of the greatest things on the internet. ",
  "translatedText": "특히 당신의 분자 모양은 인터넷에서 가장 위대한 것 중 하나입니다. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 766.28,
  "end": 769.58
 }
]