[
 {
  "input": "This is a video for anyone who already knows what eigenvalues and eigenvectors are, and who might enjoy a quick way to compute them in the case of 2x2 matrices.",
  "translatedText": "이것은 고유값과 고유벡터가 무엇인지 이미 알고 있고 2x2 행렬의 경우 이를 계산하는 빠른 방법을 즐기는 모든 사람을 위한 비디오입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 7.56
 },
 {
  "input": "If you're unfamiliar with eigenvalues, go ahead and take a look at this video here, which is actually meant to introduce them.",
  "translatedText": "고유값에 대해 잘 모르신다면 실제로 고유값을 소개하는 이 동영상을 시청해 보세요. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 8.58,
  "end": 13.7
 },
 {
  "input": "You can skip ahead if all you want to do is see the trick, but if possible I'd like you to rediscover it for yourself.",
  "translatedText": "트릭만 보고 싶으시다면 건너뛰셔도 되지만, 가능하면 직접 재발견해 보셨으면 좋겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 14.68,
  "end": 20.1
 },
 {
  "input": "So for that, let's lay out a little background.",
  "translatedText": "이를 위해 약간의 배경 지식을 설명하겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.58,
  "end": 22.38
 },
 {
  "input": "As a quick reminder, if the effect of a linear transformation on a given vector is to scale that vector by some constant, we call it an eigenvector of the transformation, and we call the relevant scaling factor the corresponding eigenvalue, often denoted with the letter lambda.",
  "translatedText": "간단히 말해, 주어진 벡터에 대한 선형 변환의 효과가 해당 벡터를 어떤 상수만큼 스케일링하는 것이라면 이를 변환의 고유 벡터라고 하고, 관련 스케일링 계수를 해당 고유값이라고 하며, 종종 람다라는 문자로 표시합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.26,
  "end": 38.6
 },
 {
  "input": "When you write this as an equation, and you rearrange a little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the matrix A minus lambda times the identity must send some non-zero vector, namely the corresponding eigenvector, to the zero vector, which in turn means that the determinant of this modified matrix must be zero.",
  "translatedText": "이를 방정식으로 작성하고 약간 재정렬하면, 람다라는 숫자가 행렬 A의 고유값이라면, 행렬 A에서 람다를 뺀 값에 아이덴티티를 곱하면 0이 아닌 벡터, 즉 해당 고유 벡터를 0 벡터로 보내야 하며, 이는 곧 이 수정 행렬의 행렬식이 0이어야 함을 뜻합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.84,
  "end": 64.58
 },
 {
  "input": "Okay, that's all a little bit of a mouthful to say, but again, I'm assuming that all of this is review for any of you watching.",
  "translatedText": "좋아, 말하기엔 좀 장황하지만, 다시 한 번 말씀드리지만, 이 모든 것은 시청하시는 분들을 위한 리뷰라고 가정합니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.12,
  "end": 71.54
 },
 {
  "input": "So, the usual way to compute eigenvalues, how I used to do it and how I believe most students are taught to carry it out, is to subtract the unknown value lambda off the diagonals, and then solve for the determinant is equal to zero.",
  "translatedText": "따라서 고유값을 계산하는 일반적인 방법, 제가 사용했던 방법, 그리고 대부분의 학생들이 배운 방법은 대각선에서 미지의 값인 람다를 뺀 다음 행렬식이 0과 같다는 것을 푸는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 72.82,
  "end": 85.86
 },
 {
  "input": "Doing this always involves a few extra steps to expand out and simplify to get a clean quadratic polynomial, what's known as the characteristic polynomial of the matrix.",
  "translatedText": "이 작업을 수행하려면 항상 행렬의 특성 다항식으로 알려진 깨끗한 이차 다항식을 얻기 위해 확장하고 단순화하는 몇 가지 추가 단계가 필요합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 87.76,
  "end": 96.46
 },
 {
  "input": "The eigenvalues are the roots of this polynomial, so to find them you have to apply the quadratic formula, which itself typically requires one or two more steps of simplification.",
  "translatedText": "고유값은 이 다항식의 근이므로 이를 찾으려면 일반적으로 한두 단계의 단순화 과정을 더 거쳐야 하는 이차 공식을 적용해야 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 97.36,
  "end": 106.54
 },
 {
  "input": "Honestly, the process isn't terrible, but at least for two by two matrices, there is a much more direct way you can get at the answer.",
  "translatedText": "솔직히 이 과정이 나쁘지는 않지만, 적어도 2×2 행렬의 경우 답을 구할 수 있는 훨씬 더 직접적인 방법이 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 107.76,
  "end": 114.68
 },
 {
  "input": "And if you want to rediscover this trick, there's only three relevant facts you need to know, each of which is worth knowing in its own right and can help you with other problem solving.",
  "translatedText": "이 트릭을 재발견하고 싶다면 세 가지 관련 사실만 알아두면 되는데, 각각은 그 자체로 알 가치가 있고 다른 문제 해결에도 도움이 될 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 115.4,
  "end": 122.9
 },
 {
  "input": "Number one, the trace of a matrix, which is the sum of these two diagonal entries, is equal to the sum of the eigenvalues.",
  "translatedText": "첫째, 이 두 대각선 항목의 합인 행렬의 트레이스는 고유값의 합과 같습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 123.82,
  "end": 130.92
 },
 {
  "input": "Or, another way to phrase it, more useful for our purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal entries.",
  "translatedText": "또는 우리의 목적에 더 유용한 다른 표현으로, 두 고유값의 평균이 이 두 대각선 항목의 평균과 같다는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.7,
  "end": 139.46
 },
 {
  "input": "Number two, the determinant of a matrix, our usual ad-bc formula, is equal to the product of the two eigenvalues.",
  "translatedText": "두 번째, 행렬의 행렬식인 일반적인 광고-비씨 공식의 행렬식은 두 고유값의 곱과 같습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.0,
  "end": 148.96
 },
 {
  "input": "And this should kind of make sense if you understand that eigenvalues describe how much an operator stretches space in a particular direction, and that the determinant describes how much an operator scales areas, or volumes, as a whole.",
  "translatedText": "고유값은 연산자가 특정 방향으로 공간을 얼마나 늘리는지를 설명하고, 결정자는 연산자가 전체적으로 영역 또는 볼륨을 얼마나 확장하는지를 설명한다는 점을 이해한다면 어느 정도 이해가 될 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 150.06,
  "end": 161.76
 },
 {
  "input": "Now before getting to the third fact, notice how you can essentially read these first two values out of the matrix without really writing much down.",
  "translatedText": "이제 세 번째 사실에 도달하기 전에 실제로 많은 내용을 기록하지 않고도 행렬에서 처음 두 값을 본질적으로 읽을 수 있는 방법에 대해 알아보세요. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 162.8,
  "end": 169.16
 },
 {
  "input": "Take this matrix here as an example.",
  "translatedText": "여기 이 행렬을 예로 들어 보겠습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 169.76,
  "end": 171.32
 },
 {
  "input": "Straight away, you can know that the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7.",
  "translatedText": "바로 고유값의 평균이 8과 6의 평균인 7과 같다는 것을 알 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 171.82,
  "end": 177.82
 },
 {
  "input": "Likewise, most linear algebra students are pretty well practiced at finding the determinant, which in this case works out to be 48 minus 8.",
  "translatedText": "마찬가지로, 대부분의 선형 대수학 학생들은 이 경우 48에서 8을 뺀 값인 행렬식을 찾는 데 꽤 능숙하게 연습합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 179.58,
  "end": 187.08
 },
 {
  "input": "So right away, you know that the product of the two eigenvalues is 40.",
  "translatedText": "따라서 두 고유값의 곱이 40이라는 것을 바로 알 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.24,
  "end": 191.7
 },
 {
  "input": "Now take a moment to see if you can derive what will be our third relevant fact, which is how you can quickly recover two numbers when you know their mean and you know their product.",
  "translatedText": "이제 잠시 시간을 내어 세 번째 관련 사실이 무엇인지 도출할 수 있는지 살펴보십시오. 이는 두 숫자의 평균과 곱을 알 때 두 숫자를 빠르게 복구할 수 있는 방법입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 192.78,
  "end": 201.56
 },
 {
  "input": "Here, let's focus on this example.",
  "translatedText": "여기서는 이 예에 중점을 두겠습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.46,
  "end": 203.72
 },
 {
  "input": "You know that the two values are evenly spaced around the number 7, so they look like 7 plus or minus something, let's call that something d for distance.",
  "translatedText": "두 값은 숫자 7 주위에 균등한 간격으로 배치되어 있으므로 7에 더하기 또는 빼기 값처럼 보입니다. 거리를 d라고 부르겠습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 204.2,
  "end": 212.78
 },
 {
  "input": "You also know that the product of these two numbers is 40.",
  "translatedText": "또한 이 두 숫자의 곱이 40이라는 것도 알고 있습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 213.56,
  "end": 216.38
 },
 {
  "input": "Now to find d, notice that this product expands really nicely, it works out as a difference of squares.",
  "translatedText": "이제 d를 찾으려면 이 곱이 정말 멋지게 확장되고 제곱의 차이로 계산됩니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 218.6,
  "end": 223.7
 },
 {
  "input": "So from there, you can find d.",
  "translatedText": "거기에서 d를 찾을 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 224.56,
  "end": 226.86
 },
 {
  "input": "d squared is 7 squared minus 40, or 9, which means that d itself is 3.",
  "translatedText": "d 제곱은 7 제곱에서 40을 뺀 9 즉, d 자체는 3입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 228.2,
  "end": 233.4
 },
 {
  "input": "In other words, the two values for this very specific example work out to be 4 and 10.",
  "translatedText": "즉, 이 매우 구체적인 예의 두 값은 4와 10이 됩니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.38,
  "end": 241.1
 },
 {
  "input": "But our goal is a quick trick, and you wouldn't want to think through this each time, so let's wrap up what we just did in a general formula.",
  "translatedText": "그러나 우리의 목표는 빠른 트릭이며, 매번 이 과정을 생각하고 싶지 않을 것이므로 방금 한 작업을 일반적인 공식으로 정리해 보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.68,
  "end": 248.12
 },
 {
  "input": "For any mean m and product p, the distance squared is always going to be m squared minus p.",
  "translatedText": "모든 평균 m과 곱하기 p에 대해 거리 제곱은 항상 m 제곱에서 p를 뺀 값이 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.64,
  "end": 255.68
 },
 {
  "input": "This gives the third key fact, which is that when two numbers have a mean m and a product p, you can write those two numbers as m plus or minus the square root of m squared minus p.",
  "translatedText": "이것은 세 번째 핵심 사실을 알려주는데, 두 숫자의 평균이 m이고 곱이 p일 때 이 두 숫자를 m의 제곱근에서 p를 뺀 제곱근을 더하거나 뺀 m으로 쓸 수 있다는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 257.56,
  "end": 268.46
 },
 {
  "input": "This is decently fast to re-derive on the fly if you ever forget it, and it's essentially just a rephrasing of the difference of squares formula.",
  "translatedText": "이 공식은 잊어버렸을 때 즉석에서 다시 도출할 수 있을 정도로 빠르며, 기본적으로 제곱의 차의 공식을 다시 표현한 것일 뿐입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 270.1,
  "end": 277.08
 },
 {
  "input": "But even still, it's a fact that's worth memorizing so it's at the tip of your fingers.",
  "translatedText": "하지만 여전히 외울 만한 가치가 있는 사실이기 때문에 손끝으로 외울 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.86,
  "end": 281.22
 },
 {
  "input": "In fact, my friend Tim from the channel A Capella Science wrote us a nice quick jingle to make it a little bit more memorable.",
  "translatedText": "사실 A Capella Science 채널의 내 친구 Tim이 좀 더 기억에 남을 수 있도록 멋지고 빠른 노래를 만들어 주었습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 281.22,
  "end": 287.16
 },
 {
  "input": "Let me show you how this works, say for the matrix 3 1 4 1.",
  "translatedText": "행렬 3 1 14 1을 예로 들어 어떻게 작동하는지 보여드리겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 291.9,
  "end": 297.62
 },
 {
  "input": "You start by bringing to mind the formula, maybe stating it all in your head.",
  "translatedText": "공식을 염두에 두는 것부터 시작합니다. 어쩌면 머리 속으로 모든 것을 설명할 수도 있습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 298.1,
  "end": 301.82
 },
 {
  "input": "But when you write it down, you fill in the appropriate values for m and p as you go.",
  "translatedText": "하지만 적을 때는 m과 p에 적절한 값을 채워 넣으면 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.2,
  "end": 311.62
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2, so the thing you start writing is 2 plus or minus the square root of 2 squared minus.",
  "translatedText": "따라서 이 예제에서 고유값의 평균은 3과 1의 평균인 2와 같으므로 2를 더하거나 2의 제곱근을 뺀 2 제곱 마이너스가 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 312.34,
  "end": 322.7
 },
 {
  "input": "Then the product of the eigenvalues is the determinant, which in this example is 3 times 1 minus 1 times 4, or negative 1, so that's the final thing you fill in, which means the eigenvalues are 2 plus or minus the square root of 5.",
  "translatedText": "그런 다음 고유값의 곱이 행렬식이며, 이 예에서는 1의 3 곱하기 1의 곱하기 4 또는 음수 1이므로 마지막으로 입력하는 것은 고유값이 5의 제곱근을 더하거나 뺀 2라는 뜻입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.54,
  "end": 338.76
 },
 {
  "input": "You might recognize that this is the same matrix I was using at the beginning, but notice how much more directly we can get at the answer.",
  "translatedText": "이것이 제가 처음에 사용했던 것과 동일한 행렬이라는 것을 알 수 있을 것입니다. 그러나 우리가 답을 얼마나 더 직접적으로 얻을 수 있는지 주목하십시오. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 340.3,
  "end": 346.5
 },
 {
  "input": "Here, try another one.",
  "translatedText": "여기, 다른 것을 시도해 보세요. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 348.14,
  "end": 349.18
 },
 {
  "input": "This time, the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5.",
  "translatedText": "이번에는 고유값의 평균이 2와 8의 평균인 5와 동일합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 349.44,
  "end": 354.48
 },
 {
  "input": "So again, you start writing out the formula, but this time writing 5 in place of m.",
  "translatedText": "다시 공식을 작성하기 시작하지만 이번에는 m 대신 5를 작성합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 355.1,
  "end": 359.22
 },
 {
  "input": "And then the determinant is 2 times 8 minus 7 times 1, or 9.",
  "translatedText": "그리고 행렬식은 2*8 - 7*1, 즉 9입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 362.98,
  "end": 368.3
 },
 {
  "input": "So in this example, the eigenvalues look like 5 plus or minus the square root of 16, which simplifies even further as 9 and 1.",
  "translatedText": "따라서 이 예에서 고유값은 5 ± sqrt(16)처럼 보이며 이는 9와 1로 더욱 단순화됩니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 369.52,
  "end": 378.24
 },
 {
  "input": "You see what I mean about how you can basically just start writing down the eigenvalues while you're staring at the matrix?",
  "translatedText": "행렬을 보면서 기본적으로 고유값을 적는다는 것이 무슨 말인지 아시겠죠?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 379.42,
  "end": 384.62
 },
 {
  "input": "It's typically just the tiniest bit of simplification at the end.",
  "translatedText": "일반적으로 마지막에 아주 작은 단순화 작업이 이루어집니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 385.28,
  "end": 388.16
 },
 {
  "input": "Honestly, I've found myself using this trick a lot when I'm sketching quick notes related to linear algebra and want to use small matrices as examples.",
  "translatedText": "솔직히 저는 선형 대수학과 관련된 간단한 노트를 스케치할 때 작은 행렬을 예제로 사용하고 싶을 때 이 방법을 많이 사용했습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 389.06,
  "end": 395.72
 },
 {
  "input": "I've been working on a video about matrix exponents, where eigenvalues pop up a lot, and I realize it's just very handy if students can read out the eigenvalues from small examples without losing the main line of thought by getting bogged down in a different calculation.",
  "translatedText": "고유값이 많이 등장하는 행렬 지수에 관한 동영상을 제작하고 있는데, 학생들이 다른 계산에 얽매여 큰 흐름을 잃지 않고 작은 예제에서 고유값을 읽어낼 수 있다면 매우 편리하다는 것을 깨달았습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.18,
  "end": 408.62
 },
 {
  "input": "As another fun example, take a look at this set of three different matrices, which comes up a lot in quantum mechanics.",
  "translatedText": "또 다른 재미있는 예로, 양자역학에서 자주 등장하는 세 가지 행렬 세트를 살펴보세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 409.74,
  "end": 415.46
 },
 {
  "input": "They're known as the Pauli spin matrices.",
  "translatedText": "이를 폴리 스핀 행렬이라고 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 415.76,
  "end": 417.52
 },
 {
  "input": "If you know quantum mechanics, you'll know that the eigenvalues of matrices are highly relevant to the physics that they describe.",
  "translatedText": "양자역학을 안다면 행렬의 고유값이 행렬이 설명하는 물리와 매우 관련이 있다는 것을 알 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 418.6,
  "end": 424.42
 },
 {
  "input": "And if you don't know quantum mechanics, let this just be a little glimpse of how these computations are actually very relevant to real applications.",
  "translatedText": "양자역학을 잘 모르시는 분들을 위해 이러한 계산이 실제 응용 분야와 어떻게 연관되어 있는지 간략하게 설명해드리겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.22,
  "end": 431.22
 },
 {
  "input": "The mean of the diagonal entries in all three cases is zero.",
  "translatedText": "세 경우 모두 대각선 항목의 평균은 0입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.54,
  "end": 435.88
 },
 {
  "input": "So the mean of the eigenvalues in all of these cases is zero, which makes our formula look especially simple.",
  "translatedText": "따라서 이 모든 경우의 고유값의 평균은 0이므로 수식이 특히 단순해 보입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 437.56,
  "end": 443.06
 },
 {
  "input": "What about the products of the eigenvalues, the determinants of these matrices?",
  "translatedText": "이 행렬의 행렬식인 고유값의 곱은 어떻습니까? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.38,
  "end": 448.8
 },
 {
  "input": "For the first one, it's 0, minus 1, or negative 1.",
  "translatedText": "첫 번째 값은 0, 마이너스 1 또는 음수 1입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.7,
  "end": 452.56
 },
 {
  "input": "The second one also looks like 0, minus 1, but it takes a moment more to see because of the complex numbers.",
  "translatedText": "두 번째도 0에서 1을 뺀 것처럼 보이지만 복소수이기 때문에 보는 데 시간이 조금 더 걸립니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.2,
  "end": 458.2
 },
 {
  "input": "And the final one looks like negative 1, minus 0.",
  "translatedText": "그리고 마지막은 -1 빼기 0처럼 보입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.84,
  "end": 461.36
 },
 {
  "input": "So in all cases, the eigenvalues simplify to be plus and minus 1.",
  "translatedText": "따라서 모든 경우에 고유값은 플러스 및 마이너스 1로 단순화됩니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 462.06,
  "end": 465.92
 },
 {
  "input": "Although in this case, you really don't need a formula to find two values if you know that they're evenly spaced around 0 and their product is negative 1.",
  "translatedText": "하지만 이 경우에는 두 값이 0 주위에 균일한 간격으로 있고 그 곱이 -1이라는 것을 알고 있다면 두 값을 찾는 데 공식이 실제로 필요하지 않습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 466.72,
  "end": 473.28
 },
 {
  "input": "If you're curious, in the context of quantum mechanics, these matrices describe observations you might make about a particle's spin in the x, y, or z direction.",
  "translatedText": "궁금하신 분들을 위해 양자역학의 맥락에서 이러한 행렬은 입자의 스핀에 대한 x, y 또는 z 방향의 관찰을 설명하는 행렬입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 474.64,
  "end": 483.12
 },
 {
  "input": "And the fact that their eigenvalues are plus and minus 1 corresponds with the idea that the values for the spin that you would observe would be either entirely in one direction or entirely in another, as opposed to something continuously ranging in between.",
  "translatedText": "그리고 고유값이 플러스와 마이너스 1이라는 사실은 관찰할 수 있는 스핀의 값이 그 사이에 연속적으로 존재하는 것이 아니라 전적으로 한 방향 또는 전적으로 다른 방향에 있다는 생각과 일치합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.56,
  "end": 497.02
 },
 {
  "input": "Maybe you'd wonder how exactly this works, or why you would use 2x2 matrices that have complex numbers to describe spin in three dimensions.",
  "translatedText": "이것이 정확히 어떻게 작동하는지, 왜 복소수가 있는 2x2 행렬을 사용하여 스핀을 3차원으로 설명하는지 궁금할 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 498.32,
  "end": 505.52
 },
 {
  "input": "Those would be fair questions, just outside the scope of what I want to talk about here.",
  "translatedText": "이는 제가 여기서 이야기하고자 하는 범위를 벗어난 공정한 질문일 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 506.1,
  "end": 509.76
 },
 {
  "input": "You know, it's funny, I wrote this section because I wanted some case where you have 2x2 matrices that aren't just toy examples or homework problems, ones where they actually come up in practice, and quantum mechanics is great for that.",
  "translatedText": "재미있게도 이 섹션을 쓴 이유는 2x2 행렬이 장난감 예제나 숙제 문제가 아닌, 실제로 실제로 등장하는 사례를 보고 싶었기 때문인데, 양자역학은 그런 경우에 아주 좋습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 510.48,
  "end": 521.7
 },
 {
  "input": "The thing is, after I made it, I realized that the whole example kind of undercuts the point that I'm trying to make.",
  "translatedText": "문제는 이 예시를 만들고 나서 제가 말하고자 하는 요점을 약화시킨다는 것을 깨달았습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 521.7,
  "end": 528.24
 },
 {
  "input": "For these specific matrices, when you use the traditional method, the one with characteristic polynomials, it's essentially just as fast.",
  "translatedText": "이러한 특정 행렬의 경우, 특징적인 다항식을 사용하는 전통적인 방법을 사용하면 기본적으로 속도가 빨라집니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.74,
  "end": 536.1
 },
 {
  "input": "It might actually be faster.",
  "translatedText": "실제로 더 빠를 수도 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.22,
  "end": 537.64
 },
 {
  "input": "I mean, take a look at the first one.",
  "translatedText": "첫 번째 사례를 살펴보세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.24,
  "end": 539.4
 },
 {
  "input": "The relevant determinant directly gives you a characteristic polynomial of lambda squared minus 1, and clearly that has roots of plus and minus 1.",
  "translatedText": "관련 결정자는 람다 제곱 마이너스 1의 특징적인 다항식을 직접 제공하며, 분명히 더하기와 빼기 1의 근을 가지고 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.68,
  "end": 548.2
 },
 {
  "input": "Same answer when you do the second matrix, lambda squared minus 1.",
  "translatedText": "두 번째 행렬인 람다 제곱 빼기 1을 수행할 때에도 같은 답이 나옵니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 548.84,
  "end": 551.76
 },
 {
  "input": "And as for the last matrix, forget about doing any computations, traditional or otherwise, it's already a diagonal matrix, so those diagonal entries are the eigenvalues.",
  "translatedText": "마지막 행렬은 기존 행렬이든 다른 행렬이든 계산을 하지 않아도 됩니다. 이미 대각선 행렬이므로 대각선 항목이 고유값입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.88,
  "end": 562.74
 },
 {
  "input": "However, the example is not totally lost to our cause.",
  "translatedText": "그러나 그 예가 우리의 주장에서 완전히 사라진 것은 아닙니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 564.3,
  "end": 566.92
 },
 {
  "input": "Where you will actually feel the speedup is in the more general case, where you take a linear combination of these three matrices and then try to compute the eigenvalues.",
  "translatedText": "실제로 속도 향상을 체감할 수 있는 경우는 이 세 행렬의 선형 조합을 취한 다음 고유값을 계산하는 보다 일반적인 경우입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 567.38,
  "end": 576.06
 },
 {
  "input": "You might write this as a times the first one, plus b times the second, plus c times the third.",
  "translatedText": "이것을 a 곱하기 첫 번째, 더하기 b 두 번째 곱하기, c 곱하기 세 번째로 쓸 수 있습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.82,
  "end": 582.42
 },
 {
  "input": "In quantum mechanics, this would describe spin observations in a general direction of a vector with coordinates a, b, c.",
  "translatedText": "양자 역학에서 이것은 좌표 a, b, c를 가진 벡터의 일반적인 방향으로의 스핀 관측을 설명합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.02,
  "end": 589.28
 },
 {
  "input": "More specifically, you should assume that this vector is normalized, meaning a squared plus b squared plus c squared is equal to 1.",
  "translatedText": "더 구체적으로 말하면, 이 벡터가 정규화되어 있다고 가정해야 합니다. 즉, a 제곱 + b 제곱 + c 제곱은 1과 같습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 590.9,
  "end": 597.7
 },
 {
  "input": "When you look at this new matrix, it's immediate to see that the mean of the eigenvalues is still 0.",
  "translatedText": "이 새로운 행렬을 보면 고유값의 평균이 여전히 0이라는 것을 바로 알 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 598.6,
  "end": 604.1
 },
 {
  "input": "And you might also enjoy pausing for a brief moment to confirm that the product of those eigenvalues is still negative 1.",
  "translatedText": "또한 잠시 멈춰서 해당 고유값의 곱이 여전히 음수 1인지 확인할 수도 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 604.6,
  "end": 610.9
 },
 {
  "input": "And then from there, concluding what the eigenvalues must be.",
  "translatedText": "그런 다음 거기에서 고유값이 무엇인지 결론을 내립니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 613.26,
  "end": 615.92
 },
 {
  "input": "And this time, the characteristic polynomial approach would be by comparison a lot more cumbersome, definitely harder to do in your head.",
  "translatedText": "그리고 이번에는 특징적인 다항식 접근 방식이 비교해 보면 훨씬 더 번거롭고 머리 속에서 수행하기가 확실히 더 어려울 것입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.22,
  "end": 623.58
 },
 {
  "input": "To be clear, using the mean product formula is not fundamentally different from finding roots of the characteristic polynomial.",
  "translatedText": "명확하게 말하면, 평균 곱 공식을 사용하는 것은 특성 다항식의 근을 찾는 것과 근본적으로 다르지 않습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 625.08,
  "end": 630.96
 },
 {
  "input": "I mean, it can't be, they're solving the same problem.",
  "translatedText": "같은 문제를 해결하고 있으니 그럴 리가 없죠.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 631.34,
  "end": 633.44
 },
 {
  "input": "One way to think about this actually is that the mean product formula is a nice way to solve quadratics in general.",
  "translatedText": "평균 곱 공식은 일반적으로 이차방정식을 푸는 좋은 방법이라고 생각하면 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 634.16,
  "end": 639.02
 },
 {
  "input": "And some viewers of the channel may recognize this.",
  "translatedText": "채널 시청자 중 일부는 이를 알아볼 수도 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 639.6,
  "end": 641.66
 },
 {
  "input": "Think about it, when you're trying to find the roots of a quadratic, given the coefficients, that's another situation where you know the sum of two values, and you also know their product, but you're trying to recover the original two values.",
  "translatedText": "계수가 주어진 이차함수의 근을 찾으려고 할 때, 두 값의 합과 곱도 알고 있지만 원래의 두 값을 구하려고 하는 또 다른 상황이라고 생각해보십시오.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 642.54,
  "end": 654.1
 },
 {
  "input": "Specifically, if the polynomial is normalized, so that this leading coefficient is 1, then the mean of the roots will be negative 1 half times this linear coefficient, which is negative 1 times the sum of those roots.",
  "translatedText": "특히, 이 선행 계수가 1이 되도록 다항식을 정규화하면 근의 평균은 이 선형 계수의 1/2이 음수가 되며, 이는 해당 근의 합의 1배가 음수가 됩니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 655.56,
  "end": 666.88
 },
 {
  "input": "With the example on the screen, that makes the mean 5.",
  "translatedText": "화면의 예시를 사용하면 평균이 5가 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 668.02,
  "end": 670.18
 },
 {
  "input": "And the product of the roots is even easier, it's just the constant term, no adjustments needed.",
  "translatedText": "그리고 뿌리의 산물은 훨씬 더 쉬우며 조정할 필요가 없는 일정한 용어일 뿐입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 671.98,
  "end": 676.52
 },
 {
  "input": "So from there, you would apply the mean product formula, and that gives you the roots.",
  "translatedText": "따라서 거기에서 평균 제품 공식을 적용하면 루트를 얻을 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 677.34,
  "end": 680.9
 },
 {
  "input": "And on the one hand, you could think of this as a lighter weight version of the traditional quadratic formula.",
  "translatedText": "한편으로는 전통적인 이차 공식의 가벼운 버전이라고 생각할 수도 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.14,
  "end": 690.22
 },
 {
  "input": "But the real advantage is not just that it's fewer symbols to memorize, it's that each one of them carries more meaning with it.",
  "translatedText": "하지만 진짜 장점은 기억해야 할 기호가 적다는 것뿐만 아니라 각 기호가 더 많은 의미를 담고 있다는 것입니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 690.96,
  "end": 696.44
 },
 {
  "input": "I mean, the whole point of this eigenvalue trick is that because you can read out the mean and product directly from looking at the matrix, you don't need to go through the intermediate step of setting up the characteristic polynomial.",
  "translatedText": "이 고유값 트릭의 핵심은 행렬을 보고 바로 평균과 곱을 읽어낼 수 있기 때문에 특성 다항식을 설정하는 중간 단계를 거치지 않아도 된다는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.94,
  "end": 708.0
 },
 {
  "input": "You can jump straight to writing down the roots without ever explicitly thinking about what the polynomial looks like.",
  "translatedText": "다항식이 어떻게 생겼는지 명시적으로 생각하지 않고도 바로 근을 쓰는 것으로 넘어갈 수 있습니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 708.42,
  "end": 713.64
 },
 {
  "input": "But to do that, we need a version of the quadratic formula where the terms carry some kind of meaning.",
  "translatedText": "하지만 이를 위해서는 용어가 어떤 의미를 담고 있는 이차 공식의 버전이 필요합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 713.84,
  "end": 718.82
 },
 {
  "input": "I realize this is a very specific trick for a very specific audience, but it's something I wish I knew in college, so if you happen to know any students who might benefit from this, consider sharing it with them.",
  "translatedText": "매우 특정한 대상을 위한 매우 구체적인 요령이라는 것을 알고 있지만, 대학 시절에 알았더라면 좋았을 것이므로 혹시 이 요령을 활용할 수 있는 학생을 알고 있다면 그들과 공유하는 것이 좋습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 720.38,
  "end": 729.7
 },
 {
  "input": "The hope is that it's not just one more thing that you memorize, but that the framing reinforces some other nice facts that are worth knowing, like how the trace and the determinant are related to eigenvalues.",
  "translatedText": "여러분이 기억하는 것이 단지 하나 더 있는 것이 아니라, 추적과 행렬식이 고유값과 어떻게 관련되어 있는지와 같이 알아야 할 가치가 있는 다른 좋은 사실을 프레이밍이 강화하기를 바랍니다. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.28,
  "end": 739.82
 },
 {
  "input": "If you want to prove those facts, by the way, take a moment to expand out the characteristic polynomial for a general matrix, and then think hard about the meaning of each of these coefficients.",
  "translatedText": "이러한 사실을 증명하려면 잠시 시간을 내어 일반 행렬의 특성 다항식을 확장한 다음 각 계수의 의미를 잘 생각해 보세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 740.56,
  "end": 749.62
 },
 {
  "input": "Many thanks to Tim for ensuring that this mean product formula will stay stuck in all of our heads for at least a few months.",
  "translatedText": "이 비열한 제품 공식이 적어도 몇 달 동안 우리 모두의 머릿속에 남을 수 있도록 해준 팀에게 많은 감사를 드립니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 752.4,
  "end": 757.94
 },
 {
  "input": "If you don't know about alcappella science, please do check it out.",
  "translatedText": "아카펠라 과학에 대해 잘 모르신다면 꼭 한번 확인해 보세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 761.7,
  "end": 766.0
 },
 {
  "input": "The molecular shape of you in particular is one of the greatest things on the internet.",
  "translatedText": "특히 당신의 분자 모양은 인터넷에서 가장 위대한 것 중 하나입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.28,
  "end": 769.58
 }
]