[
 {
  "input": "This is a video for anyone who already knows what eigenvalues and eigenvectors are, and who might enjoy a quick way to compute them in the case of 2x2 matrices.",
  "translatedText": "Это видео для всех, кто уже знает, что такое собственные значения и собственные векторы, и кому может понравиться быстрый способ их вычисления в случае матриц 2x2. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 7.56
 },
 {
  "input": "If you're unfamiliar with eigenvalues, go ahead and take a look at this video here, which is actually meant to introduce them.",
  "translatedText": "Если вы не знакомы с собственными значениями, посмотрите это видео, которое на самом деле предназначено для ознакомления с ними. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 8.58,
  "end": 13.7
 },
 {
  "input": "You can skip ahead if all you want to do is see the trick, but if possible I'd like you to rediscover it for yourself.",
  "translatedText": "Ты можешь пропустить вперед, если все, что ты хочешь сделать, - это увидеть трюк, но если это возможно, я бы хотел, чтобы ты открыл его для себя заново.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 14.68,
  "end": 20.1
 },
 {
  "input": "So for that, let's lay out a little background.",
  "translatedText": "Поэтому для этого давай изложим небольшую предысторию.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.58,
  "end": 22.38
 },
 {
  "input": "As a quick reminder, if the effect of a linear transformation on a given vector is to scale that vector by some constant, we call it an eigenvector of the transformation, and we call the relevant scaling factor the corresponding eigenvalue, often denoted with the letter lambda.",
  "translatedText": "Напомним, что если эффект линейного преобразования на данный вектор заключается в масштабировании этого вектора на некоторую константу, то мы называем его собственным вектором преобразования, а соответствующий масштабный коэффициент - соответствующим собственным значением, которое часто обозначается буквой лямбда.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.26,
  "end": 38.6
 },
 {
  "input": "When you write this as an equation, and you rearrange a little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the matrix A minus lambda times the identity must send some non-zero vector, namely the corresponding eigenvector, to the zero vector, which in turn means that the determinant of this modified matrix must be zero.",
  "translatedText": "Если записать это в виде уравнения и немного переставить, то получится, что если число лямбда является собственным значением матрицы A, то матрица A минус лямбда, умноженная на тождество, должна направить некоторый ненулевой вектор, а именно соответствующий собственный вектор, на нулевой вектор, что в свою очередь означает, что определитель этой модифицированной матрицы должен быть равен нулю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.84,
  "end": 64.58
 },
 {
  "input": "Okay, that's all a little bit of a mouthful to say, but again, I'm assuming that all of this is review for any of you watching.",
  "translatedText": "Ладно, это все слишком громко сказано, но опять же, я предполагаю, что все это обзор для любого из вас, кто смотрит. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.12,
  "end": 71.54
 },
 {
  "input": "So, the usual way to compute eigenvalues, how I used to do it and how I believe most students are taught to carry it out, is to subtract the unknown value lambda off the diagonals, and then solve for the determinant is equal to zero.",
  "translatedText": "Итак, обычный способ вычисления собственных значений, как я делал это раньше и как, полагаю, учат большинство студентов, - вычесть неизвестное значение лямбды из диагоналей, а затем решить, что детерминант равен нулю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 72.82,
  "end": 85.86
 },
 {
  "input": "Doing this always involves a few extra steps to expand out and simplify to get a clean quadratic polynomial, what's known as the characteristic polynomial of the matrix.",
  "translatedText": "Для этого всегда нужно сделать несколько дополнительных шагов, чтобы разложить и упростить, чтобы получить чистый квадратичный многочлен, который называется характеристическим многочленом матрицы.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 87.76,
  "end": 96.46
 },
 {
  "input": "The eigenvalues are the roots of this polynomial, so to find them you have to apply the quadratic formula, which itself typically requires one or two more steps of simplification.",
  "translatedText": "Собственные значения - это корни этого многочлена, поэтому, чтобы их найти, тебе нужно применить квадратичную формулу, которая сама по себе обычно требует еще одного или двух шагов упрощения.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 97.36,
  "end": 106.54
 },
 {
  "input": "Honestly, the process isn't terrible, but at least for two by two matrices, there is a much more direct way you can get at the answer.",
  "translatedText": "Честно говоря, процесс не ужасен, но, по крайней мере, для матриц два на два есть гораздо более прямой способ получить ответ.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 107.76,
  "end": 114.68
 },
 {
  "input": "And if you want to rediscover this trick, there's only three relevant facts you need to know, each of which is worth knowing in its own right and can help you with other problem solving.",
  "translatedText": "И если ты хочешь заново открыть для себя этот трюк, то тебе нужно знать всего три значимых факта, каждый из которых стоит знать сам по себе и может помочь тебе в решении других проблем.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 115.4,
  "end": 122.9
 },
 {
  "input": "Number one, the trace of a matrix, which is the sum of these two diagonal entries, is equal to the sum of the eigenvalues.",
  "translatedText": "Во-первых, след матрицы, который является суммой этих двух диагональных записей, равен сумме собственных значений.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 123.82,
  "end": 130.92
 },
 {
  "input": "Or, another way to phrase it, more useful for our purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal entries.",
  "translatedText": "Или другой способ сформулировать это, более удобный для наших целей, заключается в том, что среднее значение двух собственных значений совпадает со средним значением этих двух диагональных записей.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.7,
  "end": 139.46
 },
 {
  "input": "Number two, the determinant of a matrix, our usual ad-bc formula, is equal to the product of the two eigenvalues.",
  "translatedText": "Второе: детерминант матрицы, наша обычная формула ad-bc, равен произведению двух собственных значений.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.0,
  "end": 148.96
 },
 {
  "input": "And this should kind of make sense if you understand that eigenvalues describe how much an operator stretches space in a particular direction, and that the determinant describes how much an operator scales areas, or volumes, as a whole.",
  "translatedText": "И это должно иметь смысл, если ты понимаешь, что собственные значения описывают, насколько оператор растягивает пространство в определенном направлении, а детерминант описывает, насколько оператор масштабирует области, или объемы, в целом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 150.06,
  "end": 161.76
 },
 {
  "input": "Now before getting to the third fact, notice how you can essentially read these first two values out of the matrix without really writing much down.",
  "translatedText": "Теперь, прежде чем перейти к третьему факту, обратите внимание, как вы можете прочитать эти первые два значения из матрицы, практически не записывая их. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 162.8,
  "end": 169.16
 },
 {
  "input": "Take this matrix here as an example.",
  "translatedText": "Возьмите эту матрицу в качестве примера. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 169.76,
  "end": 171.32
 },
 {
  "input": "Straight away, you can know that the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7.",
  "translatedText": "Сразу же можно узнать, что среднее значение собственных значений совпадает со средним значением 8 и 6, то есть равно 7.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 171.82,
  "end": 177.82
 },
 {
  "input": "Likewise, most linear algebra students are pretty well practiced at finding the determinant, which in this case works out to be 48 minus 8.",
  "translatedText": "Точно так же большинство студентов, изучающих линейную алгебру, неплохо практикуются в нахождении определителя, который в данном случае получается 48 минус 8.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 179.58,
  "end": 187.08
 },
 {
  "input": "So right away, you know that the product of the two eigenvalues is 40.",
  "translatedText": "Так что сразу же ты знаешь, что произведение двух собственных значений равно 40.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.24,
  "end": 191.7
 },
 {
  "input": "Now take a moment to see if you can derive what will be our third relevant fact, which is how you can quickly recover two numbers when you know their mean and you know their product.",
  "translatedText": "Теперь найдите время и посмотрите, сможете ли вы вывести наш третий важный факт, а именно, как вы можете быстро восстановить два числа, если вы знаете их среднее значение и их произведение. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 192.78,
  "end": 201.56
 },
 {
  "input": "Here, let's focus on this example.",
  "translatedText": "Давайте сосредоточимся на этом примере. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.46,
  "end": 203.72
 },
 {
  "input": "You know that the two values are evenly spaced around the number 7, so they look like 7 plus or minus something, let's call that something d for distance.",
  "translatedText": "Вы знаете, что два значения равномерно распределены вокруг числа 7, поэтому они выглядят как 7 плюс-минус что-то, назовем это чем-то d, обозначающим расстояние. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 204.2,
  "end": 212.78
 },
 {
  "input": "You also know that the product of these two numbers is 40.",
  "translatedText": "Вы также знаете, что произведение этих двух чисел равно 40. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 213.56,
  "end": 216.38
 },
 {
  "input": "Now to find d, notice that this product expands really nicely, it works out as a difference of squares.",
  "translatedText": "Теперь, чтобы найти d, обратите внимание, что это произведение очень хорошо расширяется, оно получается как разность квадратов. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 218.6,
  "end": 223.7
 },
 {
  "input": "So from there, you can find d.",
  "translatedText": "Оттуда ты сможешь найти D.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 224.56,
  "end": 226.86
 },
 {
  "input": "d squared is 7 squared minus 40, or 9, which means that d itself is 3.",
  "translatedText": "Квадрат d равен 7 квадратам минус 40, или 9, а это значит, что само d равно 3.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 228.2,
  "end": 233.4
 },
 {
  "input": "In other words, the two values for this very specific example work out to be 4 and 10.",
  "translatedText": "Другими словами, два значения для этого очень конкретного примера — 4 и 10. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.38,
  "end": 241.1
 },
 {
  "input": "But our goal is a quick trick, and you wouldn't want to think through this each time, so let's wrap up what we just did in a general formula.",
  "translatedText": "Но наша цель - быстрый трюк, и тебе не хотелось бы каждый раз продумывать это, поэтому давай обернем то, что мы только что сделали, в общую формулу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.68,
  "end": 248.12
 },
 {
  "input": "For any mean m and product p, the distance squared is always going to be m squared minus p.",
  "translatedText": "Для любого среднего m и произведения p расстояние в квадрате всегда будет равно m в квадрате минус p.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.64,
  "end": 255.68
 },
 {
  "input": "This gives the third key fact, which is that when two numbers have a mean m and a product p, you can write those two numbers as m plus or minus the square root of m squared minus p.",
  "translatedText": "Это дает третий ключевой факт: когда два числа имеют среднее m и произведение p, ты можешь записать эти два числа как m плюс или минус квадратный корень из m в квадрате минус p.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 257.56,
  "end": 268.46
 },
 {
  "input": "This is decently fast to re-derive on the fly if you ever forget it, and it's essentially just a rephrasing of the difference of squares formula.",
  "translatedText": "Это довольно быстро можно восстановить на лету, если ты вдруг забудешь, и по сути это просто перефразирование формулы разности квадратов.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 270.1,
  "end": 277.08
 },
 {
  "input": "But even still, it's a fact that's worth memorizing so it's at the tip of your fingers.",
  "translatedText": "Но даже несмотря на это, этот факт стоит запомнить, чтобы он был на кончике твоих пальцев.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.86,
  "end": 281.22
 },
 {
  "input": "In fact, my friend Tim from the channel A Capella Science wrote us a nice quick jingle to make it a little bit more memorable.",
  "translatedText": "На самом деле, мой друг Тим с канала A Capella Science написал нам симпатичный джингл, чтобы сделать его немного более запоминающимся. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 281.22,
  "end": 287.16
 },
 {
  "input": "Let me show you how this works, say for the matrix 3 1 4 1.",
  "translatedText": "Давай я покажу тебе, как это работает, скажем, для матрицы 3 1 4 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 291.9,
  "end": 297.62
 },
 {
  "input": "You start by bringing to mind the formula, maybe stating it all in your head.",
  "translatedText": "Вы начинаете с того, что вспоминаете формулу, возможно, проговаривая ее в уме. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 298.1,
  "end": 301.82
 },
 {
  "input": "But when you write it down, you fill in the appropriate values for m and p as you go.",
  "translatedText": "Но когда ты записываешь его, то по ходу дела подставляешь соответствующие значения для m и p.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.2,
  "end": 311.62
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2, so the thing you start writing is 2 plus or minus the square root of 2 squared minus.",
  "translatedText": "Итак, в этом примере среднее значение собственных значений равно среднему значению 3 и 1, которое равно 2, поэтому то, что ты начнешь писать, будет 2 плюс или минус квадратный корень из 2 в квадрате.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 312.34,
  "end": 322.7
 },
 {
  "input": "Then the product of the eigenvalues is the determinant, which in this example is 3 times 1 minus 1 times 4, or negative 1, so that's the final thing you fill in, which means the eigenvalues are 2 plus or minus the square root of 5.",
  "translatedText": "Затем произведение собственных значений - это детерминант, который в данном примере равен 3 умножить на 1 минус 1 умножить на 4, или отрицательная 1, так что это последнее, что ты заполняешь, что означает, что собственные значения равны 2 плюс или минус квадратный корень из 5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.54,
  "end": 338.76
 },
 {
  "input": "You might recognize that this is the same matrix I was using at the beginning, but notice how much more directly we can get at the answer.",
  "translatedText": "Возможно, вы заметили, что это та же самая матрица, которую я использовал вначале, но обратите внимание, насколько точнее мы можем получить ответ. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 340.3,
  "end": 346.5
 },
 {
  "input": "Here, try another one.",
  "translatedText": "Вот, попробуй еще один. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 348.14,
  "end": 349.18
 },
 {
  "input": "This time, the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5.",
  "translatedText": "На этот раз среднее значение собственных значений совпадает со средним значением 2 и 8, которое равно 5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 349.44,
  "end": 354.48
 },
 {
  "input": "So again, you start writing out the formula, but this time writing 5 in place of m.",
  "translatedText": "Итак, ты снова начинаешь выписывать формулу, но на этот раз вместо m пишешь 5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 355.1,
  "end": 359.22
 },
 {
  "input": "And then the determinant is 2 times 8 minus 7 times 1, or 9.",
  "translatedText": "И тогда определитель 2*8 - 7*1, или 9. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 362.98,
  "end": 368.3
 },
 {
  "input": "So in this example, the eigenvalues look like 5 plus or minus the square root of 16, which simplifies even further as 9 and 1.",
  "translatedText": "Итак, в этом примере собственные значения выглядят как 5 ± sqrt(16), что еще больше упрощается до 9 и 1. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 369.52,
  "end": 378.24
 },
 {
  "input": "You see what I mean about how you can basically just start writing down the eigenvalues while you're staring at the matrix?",
  "translatedText": "Понимаешь, что я имею в виду, говоря о том, что ты можешь просто начать записывать собственные значения, глядя на матрицу?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 379.42,
  "end": 384.62
 },
 {
  "input": "It's typically just the tiniest bit of simplification at the end.",
  "translatedText": "Как правило, это лишь крошечное упрощение в конце.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 385.28,
  "end": 388.16
 },
 {
  "input": "Honestly, I've found myself using this trick a lot when I'm sketching quick notes related to linear algebra and want to use small matrices as examples.",
  "translatedText": "Честно говоря, я часто прибегаю к этому трюку, когда набрасываю быстрые заметки, связанные с линейной алгеброй, и хочу использовать в качестве примеров небольшие матрицы.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 389.06,
  "end": 395.72
 },
 {
  "input": "I've been working on a video about matrix exponents, where eigenvalues pop up a lot, and I realize it's just very handy if students can read out the eigenvalues from small examples without losing the main line of thought by getting bogged down in a different calculation.",
  "translatedText": "Я работаю над видео о матричных экспонентах, где часто всплывают собственные значения, и понимаю, что очень удобно, если студенты могут вычитать собственные значения из небольших примеров, не теряя основной линии мысли и не увязая в других вычислениях.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.18,
  "end": 408.62
 },
 {
  "input": "As another fun example, take a look at this set of three different matrices, which comes up a lot in quantum mechanics.",
  "translatedText": "В качестве еще одного забавного примера посмотри на этот набор из трех разных матриц, который часто встречается в квантовой механике.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 409.74,
  "end": 415.46
 },
 {
  "input": "They're known as the Pauli spin matrices.",
  "translatedText": "Они известны как спиновые матрицы Паули.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 415.76,
  "end": 417.52
 },
 {
  "input": "If you know quantum mechanics, you'll know that the eigenvalues of matrices are highly relevant to the physics that they describe.",
  "translatedText": "Если ты знаком с квантовой механикой, то знаешь, что собственные значения матриц имеют огромное значение для физики, которую они описывают.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 418.6,
  "end": 424.42
 },
 {
  "input": "And if you don't know quantum mechanics, let this just be a little glimpse of how these computations are actually very relevant to real applications.",
  "translatedText": "И если ты не знаешь квантовую механику, пусть это будет просто небольшим проблеском того, как эти вычисления на самом деле очень актуальны для реальных приложений.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.22,
  "end": 431.22
 },
 {
  "input": "The mean of the diagonal entries in all three cases is zero.",
  "translatedText": "Среднее значение диагональных записей во всех трех случаях равно нулю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.54,
  "end": 435.88
 },
 {
  "input": "So the mean of the eigenvalues in all of these cases is zero, which makes our formula look especially simple.",
  "translatedText": "Так что среднее значение собственных значений во всех этих случаях равно нулю, благодаря чему наша формула выглядит особенно просто.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 437.56,
  "end": 443.06
 },
 {
  "input": "What about the products of the eigenvalues, the determinants of these matrices?",
  "translatedText": "А как насчет произведений собственных значений, определителей этих матриц? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.38,
  "end": 448.8
 },
 {
  "input": "For the first one, it's 0, minus 1, or negative 1.",
  "translatedText": "Для первого это 0, минус 1 или отрицательная 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.7,
  "end": 452.56
 },
 {
  "input": "The second one also looks like 0, minus 1, but it takes a moment more to see because of the complex numbers.",
  "translatedText": "Вторая тоже выглядит как 0, минус 1, но для того, чтобы ее разглядеть, требуется больше времени из-за комплексных чисел.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.2,
  "end": 458.2
 },
 {
  "input": "And the final one looks like negative 1, minus 0.",
  "translatedText": "И последний выглядит как минус 1 минус 0. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.84,
  "end": 461.36
 },
 {
  "input": "So in all cases, the eigenvalues simplify to be plus and minus 1.",
  "translatedText": "Таким образом, во всех случаях собственные значения упрощаются до плюса и минус 1. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 462.06,
  "end": 465.92
 },
 {
  "input": "Although in this case, you really don't need a formula to find two values if you know that they're evenly spaced around 0 and their product is negative 1.",
  "translatedText": "Хотя в этом случае вам действительно не нужна формула для нахождения двух значений, если вы знаете, что они равномерно расположены вокруг 0, а их произведение отрицательно 1. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 466.72,
  "end": 473.28
 },
 {
  "input": "If you're curious, in the context of quantum mechanics, these matrices describe observations you might make about a particle's spin in the x, y, or z direction.",
  "translatedText": "Если тебе интересно, то в контексте квантовой механики эти матрицы описывают наблюдения, которые ты можешь сделать относительно спина частицы в направлении x, y или z.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 474.64,
  "end": 483.12
 },
 {
  "input": "And the fact that their eigenvalues are plus and minus 1 corresponds with the idea that the values for the spin that you would observe would be either entirely in one direction or entirely in another, as opposed to something continuously ranging in between.",
  "translatedText": "И тот факт, что их собственные значения равны плюс и минус 1, соответствует идее, что значения спина, которые ты будешь наблюдать, будут либо полностью в одном направлении, либо полностью в другом, в отличие от чего-то постоянно колеблющегося между ними.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.56,
  "end": 497.02
 },
 {
  "input": "Maybe you'd wonder how exactly this works, or why you would use 2x2 matrices that have complex numbers to describe spin in three dimensions.",
  "translatedText": "Возможно, тебе будет интересно, как именно это работает, или зачем использовать матрицы 2х2 с комплексными числами для описания спина в трех измерениях.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 498.32,
  "end": 505.52
 },
 {
  "input": "Those would be fair questions, just outside the scope of what I want to talk about here.",
  "translatedText": "Это были бы справедливые вопросы, просто они выходят за рамки того, о чем я хочу здесь поговорить.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 506.1,
  "end": 509.76
 },
 {
  "input": "You know, it's funny, I wrote this section because I wanted some case where you have 2x2 matrices that aren't just toy examples or homework problems, ones where they actually come up in practice, and quantum mechanics is great for that.",
  "translatedText": "Знаешь, забавно, но я написал этот раздел, потому что мне нужен был какой-то случай, когда матрицы 2х2 будут не просто игрушечными примерами или домашними заданиями, а теми, где они действительно встречаются на практике, и квантовая механика отлично подходит для этого.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 510.48,
  "end": 521.7
 },
 {
  "input": "The thing is, after I made it, I realized that the whole example kind of undercuts the point that I'm trying to make.",
  "translatedText": "Дело в том, что после того, как я его сделал, я понял, что весь этот пример как бы подрывает смысл, который я пытаюсь донести.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 521.7,
  "end": 528.24
 },
 {
  "input": "For these specific matrices, when you use the traditional method, the one with characteristic polynomials, it's essentially just as fast.",
  "translatedText": "Для этих конкретных матриц, когда ты используешь традиционный метод, тот, что с характеристическими полиномами, это, по сути, так же быстро.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.74,
  "end": 536.1
 },
 {
  "input": "It might actually be faster.",
  "translatedText": "На самом деле это может быть быстрее.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.22,
  "end": 537.64
 },
 {
  "input": "I mean, take a look at the first one.",
  "translatedText": "В смысле, посмотри на первую.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.24,
  "end": 539.4
 },
 {
  "input": "The relevant determinant directly gives you a characteristic polynomial of lambda squared minus 1, and clearly that has roots of plus and minus 1.",
  "translatedText": "Соответствующий определитель напрямую дает тебе характеристический многочлен лямбда-квадрат минус 1, и очевидно, что он имеет корни плюс и минус 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.68,
  "end": 548.2
 },
 {
  "input": "Same answer when you do the second matrix, lambda squared minus 1.",
  "translatedText": "Тот же ответ, когда вы делаете вторую матрицу, лямбда в квадрате минус один. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 548.84,
  "end": 551.76
 },
 {
  "input": "And as for the last matrix, forget about doing any computations, traditional or otherwise, it's already a diagonal matrix, so those diagonal entries are the eigenvalues.",
  "translatedText": "А что касается последней матрицы, то забудь о каких-либо вычислениях, традиционных или иных, - это уже диагональная матрица, так что эти диагональные записи и есть собственные значения.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.88,
  "end": 562.74
 },
 {
  "input": "However, the example is not totally lost to our cause.",
  "translatedText": "Однако этот пример не полностью потерян для нашего дела. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 564.3,
  "end": 566.92
 },
 {
  "input": "Where you will actually feel the speedup is in the more general case, where you take a linear combination of these three matrices and then try to compute the eigenvalues.",
  "translatedText": "Где ты действительно почувствуешь ускорение, так это в более общем случае, когда ты берешь линейную комбинацию этих трех матриц, а затем пытаешься вычислить собственные значения.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 567.38,
  "end": 576.06
 },
 {
  "input": "You might write this as a times the first one, plus b times the second, plus c times the third.",
  "translatedText": "Вы можете записать это как a, умноженное на первое, плюс b, умноженное на второе, плюс c, умноженное на третье. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.82,
  "end": 582.42
 },
 {
  "input": "In quantum mechanics, this would describe spin observations in a general direction of a vector with coordinates a, b, c.",
  "translatedText": "В квантовой механике это описывало бы наблюдения за спином в общем направлении вектора с координатами a, b, c.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.02,
  "end": 589.28
 },
 {
  "input": "More specifically, you should assume that this vector is normalized, meaning a squared plus b squared plus c squared is equal to 1.",
  "translatedText": "Точнее, вы должны предположить, что этот вектор нормализован, то есть квадрат плюс b в квадрате плюс с в квадрате равны единице. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 590.9,
  "end": 597.7
 },
 {
  "input": "When you look at this new matrix, it's immediate to see that the mean of the eigenvalues is still 0.",
  "translatedText": "Когда ты смотришь на эту новую матрицу, то сразу же видишь, что среднее значение собственных значений по-прежнему равно 0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 598.6,
  "end": 604.1
 },
 {
  "input": "And you might also enjoy pausing for a brief moment to confirm that the product of those eigenvalues is still negative 1.",
  "translatedText": "А еще тебе может понравиться сделать небольшую паузу, чтобы убедиться, что произведение этих собственных значений все еще отрицательно равно 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 604.6,
  "end": 610.9
 },
 {
  "input": "And then from there, concluding what the eigenvalues must be.",
  "translatedText": "И уже оттуда сделать вывод о том, какими должны быть собственные значения.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 613.26,
  "end": 615.92
 },
 {
  "input": "And this time, the characteristic polynomial approach would be by comparison a lot more cumbersome, definitely harder to do in your head.",
  "translatedText": "И на этот раз характеристический полиномиальный подход будет намного более громоздким, и его определенно сложнее будет реализовать в уме. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.22,
  "end": 623.58
 },
 {
  "input": "To be clear, using the mean product formula is not fundamentally different from finding roots of the characteristic polynomial.",
  "translatedText": "Если говорить начистоту, то использование формулы среднего произведения ничем принципиально не отличается от нахождения корней характеристического многочлена.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 625.08,
  "end": 630.96
 },
 {
  "input": "I mean, it can't be, they're solving the same problem.",
  "translatedText": "Этого не может быть, ведь они решают одну и ту же проблему.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 631.34,
  "end": 633.44
 },
 {
  "input": "One way to think about this actually is that the mean product formula is a nice way to solve quadratics in general.",
  "translatedText": "На самом деле, об этом можно подумать так: формула среднего произведения - это хороший способ решения квадратичных задач в целом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 634.16,
  "end": 639.02
 },
 {
  "input": "And some viewers of the channel may recognize this.",
  "translatedText": "И некоторые зрители канала могут узнать это.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 639.6,
  "end": 641.66
 },
 {
  "input": "Think about it, when you're trying to find the roots of a quadratic, given the coefficients, that's another situation where you know the sum of two values, and you also know their product, but you're trying to recover the original two values.",
  "translatedText": "Подумай, когда ты пытаешься найти корни квадратичного уравнения, учитывая коэффициенты, это еще одна ситуация, когда ты знаешь сумму двух значений, а также знаешь их произведение, но пытаешься восстановить исходные два значения.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 642.54,
  "end": 654.1
 },
 {
  "input": "Specifically, if the polynomial is normalized, so that this leading coefficient is 1, then the mean of the roots will be negative 1 half times this linear coefficient, which is negative 1 times the sum of those roots.",
  "translatedText": "В частности, если полином нормализован так, что главный коэффициент равен единице, то среднее значение корней будет отрицательным в половину этого линейного коэффициента, который является отрицательным в один раз суммы этих корней. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 655.56,
  "end": 666.88
 },
 {
  "input": "With the example on the screen, that makes the mean 5.",
  "translatedText": "В примере на экране это означает, что среднее значение равно 5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 668.02,
  "end": 670.18
 },
 {
  "input": "And the product of the roots is even easier, it's just the constant term, no adjustments needed.",
  "translatedText": "А с произведением корней все еще проще, это просто постоянный член, никаких корректировок не нужно.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 671.98,
  "end": 676.52
 },
 {
  "input": "So from there, you would apply the mean product formula, and that gives you the roots.",
  "translatedText": "Отсюда ты применишь формулу среднего произведения, и это даст тебе корни.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 677.34,
  "end": 680.9
 },
 {
  "input": "And on the one hand, you could think of this as a lighter weight version of the traditional quadratic formula.",
  "translatedText": "И с одной стороны, ты можешь думать об этом как о более легкой версии традиционной квадратичной формулы.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.14,
  "end": 690.22
 },
 {
  "input": "But the real advantage is not just that it's fewer symbols to memorize, it's that each one of them carries more meaning with it.",
  "translatedText": "Но настоящее преимущество не только в том, что нужно запоминать меньше символов, но и в том, что каждый из них несет в себе больше смысла. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 690.96,
  "end": 696.44
 },
 {
  "input": "I mean, the whole point of this eigenvalue trick is that because you can read out the mean and product directly from looking at the matrix, you don't need to go through the intermediate step of setting up the characteristic polynomial.",
  "translatedText": "Я имею в виду, что весь смысл этого трюка с собственными значениями заключается в том, что, поскольку ты можешь вычитать среднее значение и произведение непосредственно из матрицы, тебе не нужно проходить через промежуточный этап задания характеристического полинома.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.94,
  "end": 708.0
 },
 {
  "input": "You can jump straight to writing down the roots without ever explicitly thinking about what the polynomial looks like.",
  "translatedText": "Вы можете сразу перейти к записи корней, даже не задумываясь явно о том, как выглядит полином. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 708.42,
  "end": 713.64
 },
 {
  "input": "But to do that, we need a version of the quadratic formula where the terms carry some kind of meaning.",
  "translatedText": "Но для этого нам нужна версия квадратичной формулы, в которой термины несут какой-то смысл.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 713.84,
  "end": 718.82
 },
 {
  "input": "I realize this is a very specific trick for a very specific audience, but it's something I wish I knew in college, so if you happen to know any students who might benefit from this, consider sharing it with them.",
  "translatedText": "Я понимаю, что это очень специфический трюк для очень специфической аудитории, но это то, что я хотел бы знать в колледже, так что если ты случайно знаешь студентов, которым это может быть полезно, подумай о том, чтобы поделиться этим с ними.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 720.38,
  "end": 729.7
 },
 {
  "input": "The hope is that it's not just one more thing that you memorize, but that the framing reinforces some other nice facts that are worth knowing, like how the trace and the determinant are related to eigenvalues.",
  "translatedText": "Мы надеемся, что это не просто еще одна вещь, которую вы запомните, но и то, что формулировка подкрепит некоторые другие интересные факты, которые стоит знать, например, как след и определитель связаны с собственными значениями. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.28,
  "end": 739.82
 },
 {
  "input": "If you want to prove those facts, by the way, take a moment to expand out the characteristic polynomial for a general matrix, and then think hard about the meaning of each of these coefficients.",
  "translatedText": "Если ты хочешь доказать эти факты, кстати, удели минутку расширению характеристического многочлена для общей матрицы, а потом хорошенько подумай над значением каждого из этих коэффициентов.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 740.56,
  "end": 749.62
 },
 {
  "input": "Many thanks to Tim for ensuring that this mean product formula will stay stuck in all of our heads for at least a few months.",
  "translatedText": "Большое спасибо Тиму за то, что эта убогая формула продукта застрянет в голове у каждого из нас как минимум на несколько месяцев.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 752.4,
  "end": 757.94
 },
 {
  "input": "If you don't know about alcappella science, please do check it out.",
  "translatedText": "Если ты не знаешь о науке алькапелла, то, пожалуйста, ознакомься с ней.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 761.7,
  "end": 766.0
 },
 {
  "input": "The molecular shape of you in particular is one of the greatest things on the internet.",
  "translatedText": "В частности, молекулярная форма тебя - одна из величайших вещей в интернете.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.28,
  "end": 769.58
 }
]