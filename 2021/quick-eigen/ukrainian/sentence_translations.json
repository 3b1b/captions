[
 {
  "input": "This is a video for anyone who already knows what eigenvalues and eigenvectors are, and who might enjoy a quick way to compute them in the case of 2x2 matrices.",
  "translatedText": "Це відео для тих, хто вже знає, що таке власні значення та власні вектори, і кому може сподобатися швидкий спосіб їх обчислення у випадку матриць 2x2. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 7.56
 },
 {
  "input": "If you're unfamiliar with eigenvalues, go ahead and take a look at this video here, which is actually meant to introduce them.",
  "translatedText": "Якщо ви не знайомі з власними значеннями, подивіться це відео, яке ознайомлює з ними. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 8.58,
  "end": 13.7
 },
 {
  "input": "You can skip ahead if all you want to do is see the trick, but if possible I'd like you to rediscover it for yourself.",
  "translatedText": "Ви можете пропустити вперед, якщо все, що ви хочете зробити, це побачити трюк, але, якщо можливо, я б хотів, щоб ви відкрили його для себе заново.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 14.68,
  "end": 20.1
 },
 {
  "input": "So for that, let's lay out a little background.",
  "translatedText": "Для цього давайте викладемо невеликий бекграунд.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 20.58,
  "end": 22.38
 },
 {
  "input": "As a quick reminder, if the effect of a linear transformation on a given vector is to scale that vector by some constant, we call it an eigenvector of the transformation, and we call the relevant scaling factor the corresponding eigenvalue, often denoted with the letter lambda.",
  "translatedText": "Нагадуємо, що якщо результатом лінійного перетворення на заданий вектор є масштабування цього вектора на деяку константу, ми називаємо його власним вектором перетворення, а відповідний коефіцієнт масштабування - власним значенням, яке часто позначають літерою лямбда.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.26,
  "end": 38.6
 },
 {
  "input": "When you write this as an equation, and you rearrange a little bit, what you see is that if the number lambda is an eigenvalue of a matrix A, then the matrix A minus lambda times the identity must send some non-zero vector, namely the corresponding eigenvector, to the zero vector, which in turn means that the determinant of this modified matrix must be zero.",
  "translatedText": "Коли ви записуєте це як рівняння і трохи переставляєте, то бачите, що якщо число лямбда є власним значенням матриці A, то матриця A мінус лямбда, помножена на тотожність, повинна відправити деякий ненульовий вектор, а саме відповідний власний вектор, до нульового вектора, що, в свою чергу, означає, що визначник цієї модифікованої матриці повинен дорівнювати нулю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.84,
  "end": 64.58
 },
 {
  "input": "Okay, that's all a little bit of a mouthful to say, but again, I'm assuming that all of this is review for any of you watching.",
  "translatedText": "Гаразд, все це трохи зайве, але знову ж таки, я припускаю, що все це огляд для всіх, хто дивиться. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 66.12,
  "end": 71.54
 },
 {
  "input": "So, the usual way to compute eigenvalues, how I used to do it and how I believe most students are taught to carry it out, is to subtract the unknown value lambda off the diagonals, and then solve for the determinant is equal to zero.",
  "translatedText": "Отже, звичайний спосіб обчислення власних значень, як це робив я, і як, на мою думку, більшість студентів вчать це робити, полягає у відніманні невідомого значення лямбда від діагоналей, а потім розв'язуванні для визначника, що дорівнює нулю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 72.82,
  "end": 85.86
 },
 {
  "input": "Doing this always involves a few extra steps to expand out and simplify to get a clean quadratic polynomial, what's known as the characteristic polynomial of the matrix.",
  "translatedText": "Це завжди передбачає кілька додаткових кроків для розкладання та спрощення, щоб отримати чистий квадратичний поліном, так званий характеристичний поліном матриці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 87.76,
  "end": 96.46
 },
 {
  "input": "The eigenvalues are the roots of this polynomial, so to find them you have to apply the quadratic formula, which itself typically requires one or two more steps of simplification.",
  "translatedText": "Власні значення - це корені цього полінома, тому для їх знаходження потрібно застосувати квадратичну формулу, яка, як правило, вимагає ще одного-двох кроків спрощення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 97.36,
  "end": 106.54
 },
 {
  "input": "Honestly, the process isn't terrible, but at least for two by two matrices, there is a much more direct way you can get at the answer.",
  "translatedText": "Чесно кажучи, процес не є жахливим, але принаймні для матриць два на два є набагато простіший спосіб отримати відповідь.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 107.76,
  "end": 114.68
 },
 {
  "input": "And if you want to rediscover this trick, there's only three relevant facts you need to know, each of which is worth knowing in its own right and can help you with other problem solving.",
  "translatedText": "І якщо ви хочете заново відкрити для себе цей трюк, вам потрібно знати лише три факти, кожен з яких вартий уваги сам по собі і може допомогти вам у вирішенні інших проблем.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 115.4,
  "end": 122.9
 },
 {
  "input": "Number one, the trace of a matrix, which is the sum of these two diagonal entries, is equal to the sum of the eigenvalues.",
  "translatedText": "По-перше, слід матриці, який є сумою цих двох діагональних елементів, дорівнює сумі власних значень.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 123.82,
  "end": 130.92
 },
 {
  "input": "Or, another way to phrase it, more useful for our purposes, is that the mean of the two eigenvalues is the same as the mean of these two diagonal entries.",
  "translatedText": "Або, якщо сформулювати це по-іншому, більш корисно для наших цілей, це означає, що середнє двох власних значень дорівнює середньому цих двох діагональних записів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.7,
  "end": 139.46
 },
 {
  "input": "Number two, the determinant of a matrix, our usual ad-bc formula, is equal to the product of the two eigenvalues.",
  "translatedText": "По-друге, визначник матриці, наша звична формула ad-bc, дорівнює добутку двох власних значень.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 141.0,
  "end": 148.96
 },
 {
  "input": "And this should kind of make sense if you understand that eigenvalues describe how much an operator stretches space in a particular direction, and that the determinant describes how much an operator scales areas, or volumes, as a whole.",
  "translatedText": "І це має бути зрозуміло, якщо ви розумієте, що власні значення описують, наскільки оператор розтягує простір у певному напрямку, а визначник описує, наскільки оператор масштабує області, або об'єми, в цілому.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 150.06,
  "end": 161.76
 },
 {
  "input": "Now before getting to the third fact, notice how you can essentially read these first two values out of the matrix without really writing much down.",
  "translatedText": "Тепер, перш ніж перейти до третього факту, зауважте, як ви можете по суті прочитати ці перші два значення з матриці, не записуючи багато чого. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 162.8,
  "end": 169.16
 },
 {
  "input": "Take this matrix here as an example.",
  "translatedText": "Візьмемо цю матрицю як приклад. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 169.76,
  "end": 171.32
 },
 {
  "input": "Straight away, you can know that the mean of the eigenvalues is the same as the mean of 8 and 6, which is 7.",
  "translatedText": "Одразу видно, що середнє власних значень дорівнює середньому значенню 8 і 6, тобто 7.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 171.82,
  "end": 177.82
 },
 {
  "input": "Likewise, most linear algebra students are pretty well practiced at finding the determinant, which in this case works out to be 48 minus 8.",
  "translatedText": "Аналогічно, більшість студентів, які вивчають лінійну алгебру, досить добре вміють знаходити визначник, який у цьому випадку дорівнює 48 мінус 8.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 179.58,
  "end": 187.08
 },
 {
  "input": "So right away, you know that the product of the two eigenvalues is 40.",
  "translatedText": "Отже, ви одразу знаєте, що добуток двох власних значень дорівнює 40.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.24,
  "end": 191.7
 },
 {
  "input": "Now take a moment to see if you can derive what will be our third relevant fact, which is how you can quickly recover two numbers when you know their mean and you know their product.",
  "translatedText": "Тепер знайдіть хвилинку, щоб побачити, як ви можете вивести наш третій релевантний факт, який полягає в тому, як відновити два числа, коли ви знаєте їх середнє значення та добуток. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 192.78,
  "end": 201.56
 },
 {
  "input": "Here, let's focus on this example.",
  "translatedText": "Зупинимося на цьому прикладі. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.46,
  "end": 203.72
 },
 {
  "input": "You know that the two values are evenly spaced around the number 7, so they look like 7 plus or minus something, let's call that something d for distance.",
  "translatedText": "Ви знаєте, що два значення рівномірно розподілені навколо 7, тому вони виглядають як 7 плюс або мінус щось; давайте назвемо це щось &quot;d&quot; для відстані. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 204.2,
  "end": 212.78
 },
 {
  "input": "You also know that the product of these two numbers is 40.",
  "translatedText": "Ви також знаєте, що добуток цих двох чисел дорівнює 40. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 213.56,
  "end": 216.38
 },
 {
  "input": "Now to find d, notice that this product expands really nicely, it works out as a difference of squares.",
  "translatedText": "Тепер, щоб знайти d, зауважте, що цей добуток дуже гарно розширюється, він працює як різниця квадратів. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 218.6,
  "end": 223.7
 },
 {
  "input": "So from there, you can find d.",
  "translatedText": "Отже, звідти можна знайти d.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 224.56,
  "end": 226.86
 },
 {
  "input": "d squared is 7 squared minus 40, or 9, which means that d itself is 3.",
  "translatedText": "d в квадраті дорівнює 7 в квадраті мінус 40, або 9, що означає, що саме d дорівнює 3.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 228.2,
  "end": 233.4
 },
 {
  "input": "In other words, the two values for this very specific example work out to be 4 and 10.",
  "translatedText": "Іншими словами, два значення для цього дуже конкретного прикладу складають 4 і 10. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.38,
  "end": 241.1
 },
 {
  "input": "But our goal is a quick trick, and you wouldn't want to think through this each time, so let's wrap up what we just did in a general formula.",
  "translatedText": "Але наша мета - швидкий трюк, і ви не захочете обдумувати це щоразу, тому давайте завершимо те, що ми щойно зробили, загальною формулою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 241.68,
  "end": 248.12
 },
 {
  "input": "For any mean m and product p, the distance squared is always going to be m squared minus p.",
  "translatedText": "Для будь-якого середнього m і добутку p відстань у квадраті завжди дорівнює m у квадраті мінус p.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 248.64,
  "end": 255.68
 },
 {
  "input": "This gives the third key fact, which is that when two numbers have a mean m and a product p, you can write those two numbers as m plus or minus the square root of m squared minus p.",
  "translatedText": "Це дає третій ключовий факт, який полягає в тому, що коли два числа мають середнє m і добуток p, ви можете записати ці два числа як m плюс або мінус квадратний корінь з m в квадраті мінус p.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 257.56,
  "end": 268.46
 },
 {
  "input": "This is decently fast to re-derive on the fly if you ever forget it, and it's essentially just a rephrasing of the difference of squares formula.",
  "translatedText": "Якщо ви забули, її досить швидко можна відновити на льоту, і це, по суті, просто перефразування формули різниці квадратів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 270.1,
  "end": 277.08
 },
 {
  "input": "But even still, it's a fact that's worth memorizing so it's at the tip of your fingers.",
  "translatedText": "Але все одно, це факт, який варто запам'ятати, щоб він був на кінчику ваших пальців.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.86,
  "end": 281.22
 },
 {
  "input": "In fact, my friend Tim from the channel A Capella Science wrote us a nice quick jingle to make it a little bit more memorable.",
  "translatedText": "Насправді мій друг Тім з каналу acapellascience написав для нас короткий джингл, щоб зробити його трохи більш запам’ятовуваним. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 281.22,
  "end": 287.16
 },
 {
  "input": "Let me show you how this works, say for the matrix 3 1 4 1.",
  "translatedText": "Дозвольте показати, як це працює, скажімо, на прикладі матриці 3 1 4 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 291.9,
  "end": 297.62
 },
 {
  "input": "You start by bringing to mind the formula, maybe stating it all in your head.",
  "translatedText": "Ви починаєте з того, що згадуєте формулу, можливо, викладаючи все це в своїй голові. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 298.1,
  "end": 301.82
 },
 {
  "input": "But when you write it down, you fill in the appropriate values for m and p as you go.",
  "translatedText": "Але коли ви записуєте його, ви вписуєте відповідні значення для m і p по ходу роботи.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 306.2,
  "end": 311.62
 },
 {
  "input": "So in this example, the mean of the eigenvalues is the same as the mean of 3 and 1, which is 2, so the thing you start writing is 2 plus or minus the square root of 2 squared minus.",
  "translatedText": "Отже, в цьому прикладі середнє власних значень дорівнює середньому значенню 3 і 1, тобто 2, тому ви починаєте писати 2 плюс або мінус квадратний корінь з 2 в квадраті мінус.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 312.34,
  "end": 322.7
 },
 {
  "input": "Then the product of the eigenvalues is the determinant, which in this example is 3 times 1 minus 1 times 4, or negative 1, so that's the final thing you fill in, which means the eigenvalues are 2 plus or minus the square root of 5.",
  "translatedText": "Потім добуток власних значень є визначником, який у цьому прикладі дорівнює 3 помножити на 1 мінус 1 помножити на 4, або від'ємне значення 1, тому це останнє, що ви заповнюєте, тобто власні значення дорівнюють 2 плюс або мінус квадратний корінь з 5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 323.54,
  "end": 338.76
 },
 {
  "input": "You might recognize that this is the same matrix I was using at the beginning, but notice how much more directly we can get at the answer.",
  "translatedText": "Можливо, ви впізнаєте, що це та сама матриця, яку я використовував на початку, але зауважте, наскільки чіткіше ми можемо отримати відповідь. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 340.3,
  "end": 346.5
 },
 {
  "input": "Here, try another one.",
  "translatedText": "Ось, спробуй інший. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 348.14,
  "end": 349.18
 },
 {
  "input": "This time, the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5.",
  "translatedText": "Цього разу середнє власних значень дорівнює середньому значенню 2 і 8, тобто 5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 349.44,
  "end": 354.48
 },
 {
  "input": "So again, you start writing out the formula, but this time writing 5 in place of m.",
  "translatedText": "Отже, ви знову починаєте виписувати формулу, але цього разу замість m пишете 5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 355.1,
  "end": 359.22
 },
 {
  "input": "And then the determinant is 2 times 8 minus 7 times 1, or 9.",
  "translatedText": "І тоді визначник 2*8 - 7*1 або 9. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 362.98,
  "end": 368.3
 },
 {
  "input": "So in this example, the eigenvalues look like 5 plus or minus the square root of 16, which simplifies even further as 9 and 1.",
  "translatedText": "Отже, у цьому прикладі власні значення виглядають як 5 ± sqrt(16), що ще більше спрощує як 9 і 1. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 369.52,
  "end": 378.24
 },
 {
  "input": "You see what I mean about how you can basically just start writing down the eigenvalues while you're staring at the matrix?",
  "translatedText": "Розумієте, що я маю на увазі, коли кажу, що ви можете просто почати записувати власні значення, дивлячись на матрицю?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 379.42,
  "end": 384.62
 },
 {
  "input": "It's typically just the tiniest bit of simplification at the end.",
  "translatedText": "Зазвичай це лише найменше спрощення в кінці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 385.28,
  "end": 388.16
 },
 {
  "input": "Honestly, I've found myself using this trick a lot when I'm sketching quick notes related to linear algebra and want to use small matrices as examples.",
  "translatedText": "Чесно кажучи, я часто використовую цей трюк, коли пишу короткі нотатки з лінійної алгебри і хочу використовувати невеликі матриці як приклади.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 389.06,
  "end": 395.72
 },
 {
  "input": "I've been working on a video about matrix exponents, where eigenvalues pop up a lot, and I realize it's just very handy if students can read out the eigenvalues from small examples without losing the main line of thought by getting bogged down in a different calculation.",
  "translatedText": "Я працюю над відео про матричні експоненти, де часто з'являються власні значення, і я розумію, що було б дуже зручно, якби студенти могли зчитувати власні значення з невеликих прикладів, не втрачаючи основної лінії думки, занурюючись в інші обчислення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.18,
  "end": 408.62
 },
 {
  "input": "As another fun example, take a look at this set of three different matrices, which comes up a lot in quantum mechanics.",
  "translatedText": "Як ще один цікавий приклад, погляньте на цей набір з трьох різних матриць, який часто зустрічається у квантовій механіці.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 409.74,
  "end": 415.46
 },
 {
  "input": "They're known as the Pauli spin matrices.",
  "translatedText": "Вони відомі як спінові матриці Паулі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 415.76,
  "end": 417.52
 },
 {
  "input": "If you know quantum mechanics, you'll know that the eigenvalues of matrices are highly relevant to the physics that they describe.",
  "translatedText": "Якщо ви знаєте квантову механіку, то знаєте, що власні значення матриць мають велике значення для фізики, яку вони описують.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 418.6,
  "end": 424.42
 },
 {
  "input": "And if you don't know quantum mechanics, let this just be a little glimpse of how these computations are actually very relevant to real applications.",
  "translatedText": "І якщо ви не знаєте квантової механіки, нехай це буде лише невеликий огляд того, як ці обчислення насправді дуже важливі для реальних застосувань.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 425.22,
  "end": 431.22
 },
 {
  "input": "The mean of the diagonal entries in all three cases is zero.",
  "translatedText": "Середнє значення діагональних записів у всіх трьох випадках дорівнює нулю.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.54,
  "end": 435.88
 },
 {
  "input": "So the mean of the eigenvalues in all of these cases is zero, which makes our formula look especially simple.",
  "translatedText": "Отже, середнє власних значень у всіх цих випадках дорівнює нулю, що робить нашу формулу особливо простою.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 437.56,
  "end": 443.06
 },
 {
  "input": "What about the products of the eigenvalues, the determinants of these matrices?",
  "translatedText": "А як щодо добутків власних значень, детермінантів цих матриць? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 445.38,
  "end": 448.8
 },
 {
  "input": "For the first one, it's 0, minus 1, or negative 1.",
  "translatedText": "Для першого це 0, мінус 1 або від'ємне значення 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.7,
  "end": 452.56
 },
 {
  "input": "The second one also looks like 0, minus 1, but it takes a moment more to see because of the complex numbers.",
  "translatedText": "Другий також виглядає як 0, мінус 1, але через комплексні числа на його розгляд потрібно більше часу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.2,
  "end": 458.2
 },
 {
  "input": "And the final one looks like negative 1, minus 0.",
  "translatedText": "І остаточний виглядає як -1 - 0. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 458.84,
  "end": 461.36
 },
 {
  "input": "So in all cases, the eigenvalues simplify to be plus and minus 1.",
  "translatedText": "Отже, у всіх випадках власні значення спрощуються до ±1. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 462.06,
  "end": 465.92
 },
 {
  "input": "Although in this case, you really don't need a formula to find two values if you know that they're evenly spaced around 0 and their product is negative 1.",
  "translatedText": "Хоча в цьому випадку вам справді не потрібна формула, щоб знайти два значення, якщо ви знаєте, що вони рівномірно розподілені навколо 0, а їхній добуток дорівнює -1. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 466.72,
  "end": 473.28
 },
 {
  "input": "If you're curious, in the context of quantum mechanics, these matrices describe observations you might make about a particle's spin in the x, y, or z direction.",
  "translatedText": "Якщо вам цікаво, в контексті квантової механіки ці матриці описують спостереження, які ви можете зробити за спіном частинки в напрямку x, y або z.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 474.64,
  "end": 483.12
 },
 {
  "input": "And the fact that their eigenvalues are plus and minus 1 corresponds with the idea that the values for the spin that you would observe would be either entirely in one direction or entirely in another, as opposed to something continuously ranging in between.",
  "translatedText": "І той факт, що їхні власні значення дорівнюють плюс і мінус 1, відповідає ідеї, що значення спіну, які ви спостерігатимете, будуть або повністю в одному напрямку, або повністю в іншому, на відміну від чогось безперервно коливного між ними.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 483.56,
  "end": 497.02
 },
 {
  "input": "Maybe you'd wonder how exactly this works, or why you would use 2x2 matrices that have complex numbers to describe spin in three dimensions.",
  "translatedText": "Можливо, вам цікаво, як саме це працює, або навіщо використовувати матриці 2х2, які мають комплексні числа, для опису спіну в трьох вимірах.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 498.32,
  "end": 505.52
 },
 {
  "input": "Those would be fair questions, just outside the scope of what I want to talk about here.",
  "translatedText": "Це були б справедливі запитання, але вони виходять за рамки того, про що я хочу тут поговорити.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 506.1,
  "end": 509.76
 },
 {
  "input": "You know, it's funny, I wrote this section because I wanted some case where you have 2x2 matrices that aren't just toy examples or homework problems, ones where they actually come up in practice, and quantum mechanics is great for that.",
  "translatedText": "Знаєте, забавно, але я написав цей розділ, тому що хотів, щоб матриці 2х2 були не просто іграшковими прикладами або домашніми завданнями, а такими, що зустрічаються на практиці, і квантова механіка чудово підходить для цього.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 510.48,
  "end": 521.7
 },
 {
  "input": "The thing is, after I made it, I realized that the whole example kind of undercuts the point that I'm trying to make.",
  "translatedText": "Справа в тому, що після того, як я його зробив, я зрозумів, що весь цей приклад не зовсім відповідає тому, що я намагаюся донести.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 521.7,
  "end": 528.24
 },
 {
  "input": "For these specific matrices, when you use the traditional method, the one with characteristic polynomials, it's essentially just as fast.",
  "translatedText": "Для цих конкретних матриць, коли ви використовуєте традиційний метод з характеристичними поліномами, він працює так само швидко.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 528.74,
  "end": 536.1
 },
 {
  "input": "It might actually be faster.",
  "translatedText": "Насправді це може бути швидше.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 536.22,
  "end": 537.64
 },
 {
  "input": "I mean, take a look at the first one.",
  "translatedText": "Я маю на увазі, погляньте на перший.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 538.24,
  "end": 539.4
 },
 {
  "input": "The relevant determinant directly gives you a characteristic polynomial of lambda squared minus 1, and clearly that has roots of plus and minus 1.",
  "translatedText": "Відповідний визначник безпосередньо дає характеристичний поліном лямбда в квадраті мінус 1, і очевидно, що він має корені плюс і мінус 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.68,
  "end": 548.2
 },
 {
  "input": "Same answer when you do the second matrix, lambda squared minus 1.",
  "translatedText": "Така сама відповідь, коли ви виконуєте другу матрицю, лямбда^2 - 1. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 548.84,
  "end": 551.76
 },
 {
  "input": "And as for the last matrix, forget about doing any computations, traditional or otherwise, it's already a diagonal matrix, so those diagonal entries are the eigenvalues.",
  "translatedText": "А щодо останньої матриці, забудьте про будь-які обчислення, традиційні чи ні, це вже діагональна матриця, тому ці діагональні записи є власними значеннями.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.88,
  "end": 562.74
 },
 {
  "input": "However, the example is not totally lost to our cause.",
  "translatedText": "Проте приклад не зовсім втрачений для нашої справи. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 564.3,
  "end": 566.92
 },
 {
  "input": "Where you will actually feel the speedup is in the more general case, where you take a linear combination of these three matrices and then try to compute the eigenvalues.",
  "translatedText": "Прискорення ви відчуєте у більш загальному випадку, коли візьмете лінійну комбінацію цих трьох матриць, а потім спробуєте обчислити власні значення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 567.38,
  "end": 576.06
 },
 {
  "input": "You might write this as a times the first one, plus b times the second, plus c times the third.",
  "translatedText": "Ви можете записати це як a помножити на перше, плюс b помножити на друге, плюс c помножити на третє. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 576.82,
  "end": 582.42
 },
 {
  "input": "In quantum mechanics, this would describe spin observations in a general direction of a vector with coordinates a, b, c.",
  "translatedText": "У квантовій механіці це описує спостереження спінів у загальному напрямку вектора з координатами a, b, c.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.02,
  "end": 589.28
 },
 {
  "input": "More specifically, you should assume that this vector is normalized, meaning a squared plus b squared plus c squared is equal to 1.",
  "translatedText": "Точніше, ви повинні припустити, що цей вектор нормалізований, тобто a^2 + b^2 + c^2 = 1. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 590.9,
  "end": 597.7
 },
 {
  "input": "When you look at this new matrix, it's immediate to see that the mean of the eigenvalues is still 0.",
  "translatedText": "Коли ви дивитеся на цю нову матрицю, то одразу бачите, що середнє власних значень все ще дорівнює 0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 598.6,
  "end": 604.1
 },
 {
  "input": "And you might also enjoy pausing for a brief moment to confirm that the product of those eigenvalues is still negative 1.",
  "translatedText": "Також вам може сподобатись зупинитись на мить, щоб переконатись, що добуток цих власних значень все ще від'ємний 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 604.6,
  "end": 610.9
 },
 {
  "input": "And then from there, concluding what the eigenvalues must be.",
  "translatedText": "А вже звідти - висновок про те, якими мають бути власні значення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 613.26,
  "end": 615.92
 },
 {
  "input": "And this time, the characteristic polynomial approach would be by comparison a lot more cumbersome, definitely harder to do in your head.",
  "translatedText": "І цього разу характерний поліноміальний підхід був би набагато більш громіздким, безперечно важчим для виконання у вашій голові. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.22,
  "end": 623.58
 },
 {
  "input": "To be clear, using the mean product formula is not fundamentally different from finding roots of the characteristic polynomial.",
  "translatedText": "Зрозуміло, що використання формули середнього арифметичного не має принципових відмінностей від знаходження коренів характеристичного полінома.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 625.08,
  "end": 630.96
 },
 {
  "input": "I mean, it can't be, they're solving the same problem.",
  "translatedText": "Я маю на увазі, що цього не може бути, вони вирішують ту саму проблему.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 631.34,
  "end": 633.44
 },
 {
  "input": "One way to think about this actually is that the mean product formula is a nice way to solve quadratics in general.",
  "translatedText": "Один із способів подумати про це полягає в тому, що формула середнього арифметичного - це гарний спосіб розв'язувати квадратні рівняння взагалі.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 634.16,
  "end": 639.02
 },
 {
  "input": "And some viewers of the channel may recognize this.",
  "translatedText": "І деякі глядачі каналу можуть це впізнати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 639.6,
  "end": 641.66
 },
 {
  "input": "Think about it, when you're trying to find the roots of a quadratic, given the coefficients, that's another situation where you know the sum of two values, and you also know their product, but you're trying to recover the original two values.",
  "translatedText": "Подумайте, коли ви намагаєтеся знайти корені квадратного рівняння, знаючи коефіцієнти, це інша ситуація, коли вам відома сума двох значень, а також їх добуток, але ви намагаєтеся відновити вихідні два значення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 642.54,
  "end": 654.1
 },
 {
  "input": "Specifically, if the polynomial is normalized, so that this leading coefficient is 1, then the mean of the roots will be negative 1 half times this linear coefficient, which is negative 1 times the sum of those roots.",
  "translatedText": "Зокрема, якщо поліном нормалізовано таким чином, що цей старший коефіцієнт дорівнює 1, тоді середнє значення коренів дорівнюватиме -½ цього лінійного коефіцієнта, що дорівнює -1 помноженій сумі цих коренів. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 655.56,
  "end": 666.88
 },
 {
  "input": "With the example on the screen, that makes the mean 5.",
  "translatedText": "У прикладі на екрані це означає середнє значення 5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 668.02,
  "end": 670.18
 },
 {
  "input": "And the product of the roots is even easier, it's just the constant term, no adjustments needed.",
  "translatedText": "А з добутком коренів ще простіше, це просто постійний термін, не потребує жодних коригувань.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 671.98,
  "end": 676.52
 },
 {
  "input": "So from there, you would apply the mean product formula, and that gives you the roots.",
  "translatedText": "Звідси ви можете застосувати формулу середнього арифметичного, і це дасть вам коріння.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 677.34,
  "end": 680.9
 },
 {
  "input": "And on the one hand, you could think of this as a lighter weight version of the traditional quadratic formula.",
  "translatedText": "З одного боку, ви можете вважати це полегшеною версією традиційної квадратичної формули.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.14,
  "end": 690.22
 },
 {
  "input": "But the real advantage is not just that it's fewer symbols to memorize, it's that each one of them carries more meaning with it.",
  "translatedText": "Але справжня перевага полягає в тому, що потрібно запам’ятати менше символів, а кожен із них має більше значення. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 690.96,
  "end": 696.44
 },
 {
  "input": "I mean, the whole point of this eigenvalue trick is that because you can read out the mean and product directly from looking at the matrix, you don't need to go through the intermediate step of setting up the characteristic polynomial.",
  "translatedText": "Я маю на увазі, що весь сенс цього трюку з власними значеннями полягає в тому, що оскільки ви можете прочитати середнє значення і добуток безпосередньо з матриці, вам не потрібно проходити через проміжний крок налаштування характеристичного полінома.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 696.94,
  "end": 708.0
 },
 {
  "input": "You can jump straight to writing down the roots without ever explicitly thinking about what the polynomial looks like.",
  "translatedText": "Ви можете відразу перейти до запису коренів, навіть не задумуючись про те, як виглядає поліном. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 708.42,
  "end": 713.64
 },
 {
  "input": "But to do that, we need a version of the quadratic formula where the terms carry some kind of meaning.",
  "translatedText": "Але для цього нам потрібна версія квадратичної формули, в якій члени мають певне значення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 713.84,
  "end": 718.82
 },
 {
  "input": "I realize this is a very specific trick for a very specific audience, but it's something I wish I knew in college, so if you happen to know any students who might benefit from this, consider sharing it with them.",
  "translatedText": "Я розумію, що це дуже специфічний прийом для дуже специфічної аудиторії, але це те, що я хотів би знати в коледжі, тому якщо ви знаєте студентів, яким це може бути корисно, подумайте про те, щоб поділитися з ними цією інформацією.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 720.38,
  "end": 729.7
 },
 {
  "input": "The hope is that it's not just one more thing that you memorize, but that the framing reinforces some other nice facts that are worth knowing, like how the trace and the determinant are related to eigenvalues.",
  "translatedText": "Сподіваємося, що це не просто ще одна річ, яку потрібно запам’ятати, а те, що фрейм підкріплює деякі інші цікаві факти, які варто знати, наприклад, як слід і визначник пов’язані з власними значеннями. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.28,
  "end": 739.82
 },
 {
  "input": "If you want to prove those facts, by the way, take a moment to expand out the characteristic polynomial for a general matrix, and then think hard about the meaning of each of these coefficients.",
  "translatedText": "Якщо ви хочете довести ці факти, до речі, знайдіть хвилинку, щоб розкласти характеристичний поліном для загальної матриці, а потім добре подумайте про значення кожного з цих коефіцієнтів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 740.56,
  "end": 749.62
 },
 {
  "input": "Many thanks to Tim for ensuring that this mean product formula will stay stuck in all of our heads for at least a few months.",
  "translatedText": "Велике спасибі Тіму за те, що він гарантує, що ця формула продукту застрягне в наших головах принаймні на кілька місяців.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 752.4,
  "end": 757.94
 },
 {
  "input": "If you don't know about alcappella science, please do check it out.",
  "translatedText": "Якщо ви не знаєте про науку алькапелли, будь ласка, ознайомтеся з нею.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 761.7,
  "end": 766.0
 },
 {
  "input": "The molecular shape of you in particular is one of the greatest things on the internet.",
  "translatedText": "Молекулярна форма вас, зокрема, є однією з найцікавіших речей в Інтернеті.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 766.28,
  "end": 769.58
 }
]