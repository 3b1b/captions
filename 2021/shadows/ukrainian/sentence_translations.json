[
 {
  "input": "In a moment I'm going to tell you about a certain really nice puzzle involving the shadow of a cube.",
  "translatedText": "За мить я розповім вам про одну дуже гарну головоломку, пов’язану з тінню куба.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 4.3
 },
 {
  "input": "But before we get to that, I should say that the point of this video is not exactly the puzzle per se, it's about two distinct problem-solving styles that are reflected in two different ways that we can tackle this problem.",
  "translatedText": "Але перш ніж ми перейдемо до цього, я повинен сказати, що сенс цього відео полягає не зовсім у головоломці як такій, це про два різних стилі вирішення проблем, які відображаються двома різними способами, якими ми можемо вирішити цю проблему.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 5.0,
  "end": 15.24
 },
 {
  "input": "In fact, let's anthropomorphize those two different styles by imagining two students, Alice and Bob, that embody each one of the approaches.",
  "translatedText": "Фактично, давайте антропоморфізуємо ці два різні стилі, уявивши двох студентів, Алісу та Боба, які втілюють кожен із підходів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 15.78,
  "end": 22.7
 },
 {
  "input": "So Bob will be the kind of student who really loves calculation.",
  "translatedText": "Тож Боб буде тим студентом, який справді любить рахувати.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.5,
  "end": 26.98
 },
 {
  "input": "As soon as there's a moment when he can dig into the details and get a very concrete view of the concrete situation in front of him, that's where he's the most pleased.",
  "translatedText": "Як тільки настає момент, коли він може вникнути в деталі та отримати дуже конкретне уявлення про конкретну ситуацію, що стоїть перед ним, це те, що йому найбільше подобається.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 26.98,
  "end": 34.34
 },
 {
  "input": "Alice, on the other hand, is more inclined to procrastinate the computations, not because she doesn't know how to do them or doesn't want to per se, but she prefers to get a nice high-level general overview of the kind of problem she's dealing with, the general shape that it has, before she digs into the computations themselves.",
  "translatedText": "Аліса, з іншого боку, більш схильна відкладати обчислення не тому, що вона не знає, як їх робити чи не хоче сама по собі, а тому, що вона вважає за краще отримати гарний загальний огляд на високому рівні. проблеми, з якою вона має справу, загальний вигляд, який вона має, перш ніж вона заглибиться в самі обчислення.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 35.12,
  "end": 51.36
 },
 {
  "input": "She's most pleased if she understands not just the specific question sitting in front of her, but also the broadest possible way that you could generalize it, and especially if the more general view can lend itself to more swift and elegant computations, once she does actually sit down to carry them out.",
  "translatedText": "Їй дуже приємно, якщо вона розуміє не лише конкретне питання, яке стоїть перед нею, але й якомога ширше його узагальнення, і особливо якщо загальніший погляд може піддаватися більш швидким і елегантним обчисленням, коли вона це зробить. сісти їх виконувати.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 52.16,
  "end": 66.94
 },
 {
  "input": "Now the puzzle that both of them are going to be faced with is to find the average area for the shadow of a cube.",
  "translatedText": "Тепер головоломка, з якою вони зіткнуться, полягає в тому, щоб знайти середню площу тіні куба.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 73.02,
  "end": 79.14
 },
 {
  "input": "So if I have a cube kind of sitting here hovering in space, there are a few things that influence the area of its shadow.",
  "translatedText": "Отже, якщо у мене є куб, який ніби сидить тут і ширяє в просторі, є кілька речей, які впливають на площу його тіні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 79.9,
  "end": 85.46
 },
 {
  "input": "One obvious one would be the size of the cube, smaller cube, smaller shadow.",
  "translatedText": "Одним із очевидних є розмір куба, менший куб, менша тінь.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.46,
  "end": 89.26
 },
 {
  "input": "But also if it's sitting at different orientations, those orientations correspond to different particular shadows with different areas.",
  "translatedText": "Але також якщо він знаходиться в різних орієнтаціях, ці орієнтації відповідають різним конкретним тіням з різними областями.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 89.88,
  "end": 96.16
 },
 {
  "input": "And when I say find the average here, what I mean is average over all possible orientations for a particular size of the cube.",
  "translatedText": "І коли я кажу тут знайти середнє значення, я маю на увазі середнє серед усіх можливих орієнтацій для певного розміру куба.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 96.78,
  "end": 103.1
 },
 {
  "input": "The astute among you might point out that it also matters a lot where the light source is.",
  "translatedText": "Проникливий серед вас може зауважити, що також дуже важливо, де знаходиться джерело світла.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 104.42,
  "end": 108.1
 },
 {
  "input": "If the light source were very low, close to the cube itself, then the shadow ends up larger.",
  "translatedText": "Якщо джерело світла було дуже низько, близько до самого куба, то тінь вийшла б більшою.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.36,
  "end": 112.66
 },
 {
  "input": "And if the light source were kind of positioned laterally off to the side, this can distort the shadow and give it a very different shape.",
  "translatedText": "І якщо джерело світла було розташоване збоку збоку, це може спотворити тінь і надати їй зовсім іншої форми.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 112.66,
  "end": 118.56
 },
 {
  "input": "Accounting for that light position stands to be highly interesting in its own right, but the puzzle is hard enough as it is, so at least initially, let's do the easiest thing we can and say that the light is directly above the cube and really far away, effectively infinitely far, so that all we're considering is a flat projection, in the sense that if you look at any coordinates, x, y, z, in space, the flat projection would be x, y, 0.",
  "translatedText": "Облік цієї позиції світла сам по собі дуже цікавий, але головоломка досить складна, тому принаймні спочатку давайте зробимо найпростіше, що можемо, і скажемо, що світло знаходиться прямо над кубом і дуже далеко далеко, фактично нескінченно далеко, тому все, що ми розглядаємо, це плоска проекція, у тому сенсі, що якщо ви подивитеся на будь-які координати, x, y, z, у просторі, плоска проекція буде x, y, 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.26,
  "end": 141.7
 },
 {
  "input": "So just to get our bearings, the easiest situation to think about would be if the cube is straight up, with two of its faces parallel to the ground.",
  "translatedText": "Отже, щоб зорієнтуватися, найлегше подумати про ситуацію, коли куб стоїть прямо, а дві його грані паралельні землі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.48,
  "end": 149.28
 },
 {
  "input": "In that case, this flat projection shadow is simply a square, and if we say the side lengths of the cube are s, then the area of that shadow is s squared.",
  "translatedText": "У цьому випадку ця плоска проекційна тінь є просто квадратом, і якщо ми скажемо, що довжини сторін куба дорівнюють s, тоді площа цієї тіні дорівнює s у квадраті.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 149.92,
  "end": 157.9
 },
 {
  "input": "And by the way, any time that I have a label up on these animations, like the one down here, I'll be assuming that the relevant cube has a side length of 1.",
  "translatedText": "І, до речі, кожного разу, коли я маю мітку на цих анімаціях, подібну до цієї, я буду припускати, що відповідний куб має довжину сторони 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.74,
  "end": 165.46
 },
 {
  "input": "Now another special case among all the orientations that's fun to think about is if the long diagonal is parallel to the direction of the light.",
  "translatedText": "Ще один особливий випадок серед усіх орієнтацій, про який цікаво думати, це якщо довга діагональ паралельна напрямку світла.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 166.24,
  "end": 173.04
 },
 {
  "input": "In that case, the shadow actually looks like a regular hexagon, and if you use some of the methods that we will develop in a few minutes, you can compute that the area of that shadow is exactly the square root of 3 times the area of one of the square faces.",
  "translatedText": "У цьому випадку тінь насправді виглядає як правильний шестикутник, і якщо ви скористаєтеся деякими методами, які ми розробимо за кілька хвилин, ви зможете обчислити, що площа цієї тіні дорівнює квадратному кореню з 3 помноженої площі одна з квадратних граней.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 173.6,
  "end": 185.82
 },
 {
  "input": "But of course, more often, the actual shadow will be not so regular as a square or a hexagon.",
  "translatedText": "Але, звичайно, найчастіше справжня тінь буде не такою правильною, як квадрат або шестикутник.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 186.66,
  "end": 191.2
 },
 {
  "input": "It's some harder to think about shape based on some harder to think about orientation for this cube.",
  "translatedText": "Дещо важче подумати про форму на основі важчих подумати про орієнтацію цього куба.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.66,
  "end": 196.24
 },
 {
  "input": "Earlier, I casually threw out this phrase of averaging over all possible orientations, but you could rightly ask, what exactly is that supposed to mean?",
  "translatedText": "Раніше я випадково викинув цю фразу про усереднення за всіма можливими орієнтаціями, але ви можете слушно запитати, що саме це має означати?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 197.06,
  "end": 205.3
 },
 {
  "input": "I think a lot of us have an intuitive feel for what we want it to mean, at least in the sense of what experiment would you do to verify it.",
  "translatedText": "Я думаю, що багато хто з нас інтуїтивно відчуває, що ми хочемо, щоб це означало, принаймні в сенсі того, який експеримент ви б провели, щоб перевірити це.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.16,
  "end": 212.86
 },
 {
  "input": "You might imagine tossing this cube in the air like a dye, freezing it at some arbitrary point, recording the area of the shadow from that position, and then repeating.",
  "translatedText": "Ви можете уявити, як підкидаєте цей куб у повітря, як барвник, заморожуєте його в якійсь довільній точці, записуєте область тіні з цього положення, а потім повторюєте.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.06,
  "end": 222.44
 },
 {
  "input": "If you do this many many times over and over, you can take the mean of your sample.",
  "translatedText": "Якщо ви робите це багато разів знову і знову, ви можете взяти середнє значення вашої вибірки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 223.64,
  "end": 228.38
 },
 {
  "input": "The number that we want to get at, the true average here, should be whatever that experimental mean approaches as you do more and more tosses, approaching infinitely many.",
  "translatedText": "Число, яке ми хочемо отримати, справжнє середнє тут, має бути таким, яким би наближалося це експериментальне середнє, коли ви робите все більше і більше кидків, наближаючись до нескінченної кількості.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 229.22,
  "end": 237.94
 },
 {
  "input": "Even still, the sticklers among you could complain that doesn't really answer the question, because it leaves open the issue of how we're defining a random toss.",
  "translatedText": "Незважаючи на це, прихильники серед вас можуть поскаржитися, що це насправді не відповідає на запитання, оскільки залишає відкритим питання про те, як ми визначаємо випадковий жеребок.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 240.44,
  "end": 247.8
 },
 {
  "input": "The proper way to answer this, if we want it to be more formal, would be to first describe the space of all possible orientations, which mathematicians have actually given a fancy name.",
  "translatedText": "Правильний спосіб відповісти на це питання, якщо ми хочемо, щоб це було більш формально, полягав би в тому, щоб спочатку описати простір усіх можливих орієнтацій, якому математики фактично дали химерну назву.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 248.3,
  "end": 257.54
 },
 {
  "input": "They call it SO3, typically defined in terms of a certain family of 3x3 matrices.",
  "translatedText": "Вони називають це SO3, зазвичай визначене в термінах певного сімейства матриць 3x3.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 257.64,
  "end": 262.44
 },
 {
  "input": "And the question we want to answer is, what probability distribution are we putting to this entire space?",
  "translatedText": "І питання, на яке ми хочемо відповісти, полягає в тому, який розподіл ймовірностей ми накладаємо на весь цей простір?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 263.1,
  "end": 268.76
 },
 {
  "input": "It's only when such a probability distribution is well-defined that we can answer a question involving an average.",
  "translatedText": "Лише тоді, коли такий розподіл ймовірностей чітко визначено, ми можемо відповісти на запитання, що містить середнє значення.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 269.1,
  "end": 274.5
 },
 {
  "input": "If you are a stickler for that kind of thing, I want you to hold off on that question until the end of the video.",
  "translatedText": "Якщо ви прихильник такого роду речей, я хочу, щоб ви не відповідали на це запитання до кінця відео.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 275.8,
  "end": 280.82
 },
 {
  "input": "You'll be surprised at how far we can get with the more heuristic, experimental idea of just repeating a bunch of random tosses without really defining the distribution.",
  "translatedText": "Ви здивуєтеся, наскільки далеко ми можемо зайти з більш евристичною, експериментальною ідеєю простого повторення купи випадкових кидків без реального визначення розподілу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 280.98,
  "end": 288.58
 },
 {
  "input": "Once we see Alice and Bob's solutions, it's actually very interesting to ask how exactly each one of them defined this distribution along their way.",
  "translatedText": "Коли ми побачимо рішення Аліси та Боба, насправді дуже цікаво запитати, як саме кожен із них визначив цей розподіл на своєму шляху.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 289.28,
  "end": 296.48
 },
 {
  "input": "And remember, this is not meant to be a lesson about cube shadows per se, but a lesson about problem solving, told through the lens of two different mindsets that we might bring to the puzzle.",
  "translatedText": "І пам’ятайте, що це не урок про кубичні тіні як такі, а урок про вирішення проблем, розказаний крізь призму двох різних способів мислення, які ми можемо застосувати до головоломки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 297.92,
  "end": 307.1
 },
 {
  "input": "And as with any lesson on problem solving, the goal here is not to get to the answer as quickly as we can, but hopefully for you to feel like you found the answer yourself.",
  "translatedText": "І, як і в будь-якому уроці з розв’язування проблем, тут мета полягає не в тому, щоб якнайшвидше знайти відповідь, а, сподіваємось, у тому, щоб ви відчули, що ви самі знайшли відповідь.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.86,
  "end": 315.72
 },
 {
  "input": "So if ever there's a point when you feel like you might have an idea, give yourself the freedom to pause and try to think it through.",
  "translatedText": "Тож якщо колись настане момент, коли ви відчуєте, що у вас може виникнути ідея, дайте собі свободу зробити паузу та спробувати її обдумати.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.02,
  "end": 320.82
 },
 {
  "input": "As a first step, and this is really independent of any particular problem solving styles, just any time you find a hard question, a good thing that you can do is ask, what's the simplest possible, non-trivial variant of the problem that you can try to solve?",
  "translatedText": "Як перший крок, і це дійсно незалежно від будь-яких конкретних стилів вирішення проблеми, просто щоразу, коли ви знаходите важке запитання, добре, що ви можете зробити, це запитати, який із можливих найпростіший, нетривіальний варіант проблеми, який ви можна спробувати вирішити?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 325.42,
  "end": 338.54
 },
 {
  "input": "So in our case, what you might say is, okay, let's forget about averaging over all the orientations.",
  "translatedText": "Тож у нашому випадку ви можете сказати: гаразд, давайте забудемо про усереднення за всіма орієнтаціями.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 339.56,
  "end": 344.0
 },
 {
  "input": "That's a tricky thing to think about.",
  "translatedText": "Думати про це складно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.12,
  "end": 345.42
 },
 {
  "input": "And let's even forget about all the different faces of the cube, because they overlap, and that's also tricky to think about.",
  "translatedText": "І давайте навіть забудемо про всі різні грані куба, тому що вони перекриваються, і про це також складно думати.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 345.68,
  "end": 350.86
 },
 {
  "input": "Just for one particular face, and one particular orientation, can we compute the area of this shadow?",
  "translatedText": "Лише для одного конкретного обличчя та однієї конкретної орієнтації, чи можемо ми обчислити площу цієї тіні?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.34,
  "end": 356.9
 },
 {
  "input": "Once more, if you want to get your bearings with some special cases, the easiest is when that face is parallel to the ground, in which case the area of the shadow is the same as the area of the face.",
  "translatedText": "Знову ж таки, якщо ви хочете зорієнтуватися в деяких особливих випадках, найпростішим буде, коли це обличчя буде паралельно землі, і в цьому випадку площа тіні буде такою ж, як і площа обличчя.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 357.66,
  "end": 366.68
 },
 {
  "input": "And on the other hand, if we were to tilt that face 90 degrees, then its shadow will be a straight line, and it has an area of zero.",
  "translatedText": "І з іншого боку, якщо ми повинні нахилити цю грань на 90 градусів, то її тінь буде прямою лінією, а її площа дорівнює нулю.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.18,
  "end": 373.44
 },
 {
  "input": "So Bob looks at this, and he wants an actual formula for that shadow.",
  "translatedText": "Отже, Боб дивиться на це, і йому потрібна справжня формула цієї тіні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 374.3,
  "end": 377.42
 },
 {
  "input": "And the way he might think about it is to consider the normal vector perpendicular off of that face.",
  "translatedText": "І те, як він міг би подумати про це, це розглянути нормальний вектор, перпендикулярний до цієї грані.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.9,
  "end": 382.7
 },
 {
  "input": "And what seems relevant is the angle that that normal vector makes with the vertical, with the direction where the light is coming from, which we might call theta.",
  "translatedText": "І те, що здається релевантним, це кут, який вектор нормалі утворює з вертикаллю, з напрямком, звідки надходить світло, який ми можемо назвати тета.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 383.18,
  "end": 390.08
 },
 {
  "input": "Now, from the two special cases we just looked at, we know that when theta is equal to zero, the area of that shadow is the same as the area of the shape itself, which is s squared if the square has side lengths s.",
  "translatedText": "Тепер із двох особливих випадків, які ми щойно розглянули, ми знаємо, що коли тета дорівнює нулю, площа цієї тіні дорівнює площі самої фігури, яка дорівнює s у квадраті, якщо квадрат має довжину сторін s.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 391.2,
  "end": 401.56
 },
 {
  "input": "And if theta is equal to 90 degrees, then the area of that shadow is zero.",
  "translatedText": "І якщо тета дорівнює 90 градусам, то площа цієї тіні дорівнює нулю.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 402.2,
  "end": 405.8
 },
 {
  "input": "And it's probably not too hard to guess that trigonometry will be somehow relevant, so anyone comfortable with their trig functions could probably hazard a guess as to what the right formula is.",
  "translatedText": "І, напевно, неважко здогадатися, що тригонометрія буде якимось чином доречна, тож будь-хто, хто добре володіє своїми тригонометричними функціями, може ризикнути здогадатися щодо правильної формули.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 406.24,
  "end": 414.62
 },
 {
  "input": "But Bob is more detail-oriented than that.",
  "translatedText": "Але Боб більше орієнтований на деталі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 414.62,
  "end": 417.12
 },
 {
  "input": "He wants to properly prove what that area should be, rather than just making a guess based on the endpoints.",
  "translatedText": "Він хоче належним чином довести, якою має бути ця область, а не просто робити припущення на основі кінцевих точок.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 417.4,
  "end": 422.02
 },
 {
  "input": "And the way you might think about it could be something like this.",
  "translatedText": "І те, як ви думаєте про це, може бути приблизно таким.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.82,
  "end": 424.74
 },
 {
  "input": "If we consider the plane that passes through the vertical as well as our normal vector, and then we consider all the different slices of our shape that are in that plane, or parallel to that plane, then we can focus our attention on a two-dimensional variant of the problem.",
  "translatedText": "Якщо ми розглянемо площину, яка проходить через вертикаль, а також наш вектор нормалі, а потім ми розглянемо всі різні зрізи нашої форми, які знаходяться в цій площині або паралельні цій площині, тоді ми можемо зосередити нашу увагу на двох розмірний варіант задачі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 424.98,
  "end": 439.04
 },
 {
  "input": "If we just look at one of those slices, who has a normal vector, an angle theta away from the vertical, its shadow might look something like this.",
  "translatedText": "Якщо ми просто подивимось на один із цих фрагментів, який має нормальний вектор, кут тета від вертикалі, його тінь може виглядати приблизно так.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 439.32,
  "end": 446.78
 },
 {
  "input": "And if we draw a vertical line up to the left here, we have ourselves a right triangle.",
  "translatedText": "І якщо ми проведемо тут вертикальну лінію вліво, ми отримаємо прямокутний трикутник.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.46,
  "end": 451.02
 },
 {
  "input": "And from here we can do a little bit of angle chasing, where we follow around what that angle theta implies about the rest of the diagram.",
  "translatedText": "І з цього моменту ми можемо зробити невеликий переслідування кута, де ми слідкуємо за тим, що означає цей кут тета щодо решти діаграми.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 451.6,
  "end": 457.52
 },
 {
  "input": "And this means the lower right angle in this triangle is precisely theta.",
  "translatedText": "А це означає, що нижній прямий кут у цьому трикутнику дорівнює точно тета.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 458.58,
  "end": 462.36
 },
 {
  "input": "So, when we want to understand the size of this shadow in comparison to the original size of the piece, we can think about the cosine of that angle, theta, which remembers the adjacent over the hypotenuse.",
  "translatedText": "Отже, коли ми хочемо зрозуміти розмір цієї тіні в порівнянні з початковим розміром частини, ми можемо подумати про косинус цього кута, тета, який запам’ятовує прилеглий до гіпотенузи кут.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 463.48,
  "end": 474.58
 },
 {
  "input": "It's literally the ratio between the size of the shadow and the size of the slice.",
  "translatedText": "Це буквально співвідношення між розміром тіні та розміром шматочка.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.7,
  "end": 478.18
 },
 {
  "input": "So, the factor by which the slice gets squished down in this direction is exactly cosine of theta.",
  "translatedText": "Отже, коефіцієнт, за допомогою якого зріз здавлюється в цьому напрямку, дорівнює точно косинусу тета.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 478.9,
  "end": 484.52
 },
 {
  "input": "And if we broaden our view to the entire square, all the slices in that direction get scaled by the same factor.",
  "translatedText": "І якщо ми розширимо наш огляд на весь квадрат, усі зрізи в цьому напрямку будуть масштабовані з однаковим коефіцієнтом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.14,
  "end": 490.18
 },
 {
  "input": "But in the other direction, in the one perpendicular to that slice, there is no stretching or squishing, because the face is not at all tilted in that direction.",
  "translatedText": "Але в іншому напрямку, в тому, що перпендикулярно до цього шматка, немає розтягування чи здавлювання, тому що обличчя зовсім не нахилене в цьому напрямку.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.38,
  "end": 498.12
 },
 {
  "input": "So overall, the two-dimensional shadow of our two-dimensional face should also be scaled down by this factor of a cosine of theta.",
  "translatedText": "Отже, загалом, двовимірна тінь нашого двовимірного обличчя також має бути зменшена на цей коефіцієнт косинуса тета.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 498.12,
  "end": 505.7
 },
 {
  "input": "It lines up with what you might intuitively guess, given the case where the angle is 0° and the case where it's 90°, but it's reassuring to see why it's true.",
  "translatedText": "Це узгоджується з тим, що ви можете інтуїтивно здогадатися, враховуючи випадок, коли кут дорівнює 0°, і випадок, коли він дорівнює 90°, але заспокоює зрозуміти, чому це правда.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 506.26,
  "end": 513.38
 },
 {
  "input": "And actually, as stated so far, this is not quite correct.",
  "translatedText": "І насправді, як сказано досі, це не зовсім правильно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.96,
  "end": 518.32
 },
 {
  "input": "There is a small problem with the formula that we've written.",
  "translatedText": "Є невелика проблема з формулою, яку ми написали.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 518.52,
  "end": 520.8
 },
 {
  "input": "In the case where theta is bigger than 90°, the cosine would actually come out to be negative.",
  "translatedText": "У випадку, коли тета більше 90°, косинус фактично виявиться від’ємним.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.34,
  "end": 526.24
 },
 {
  "input": "But of course, we don't want to consider the shadow to have negative area, at least not in a problem like this.",
  "translatedText": "Але, звісно, ми не хочемо вважати, що тінь має негативну область, принаймні в такій задачі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 526.24,
  "end": 531.4
 },
 {
  "input": "So there's two different ways you could solve this.",
  "translatedText": "Отже, ви можете вирішити це двома способами.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 531.86,
  "end": 533.3
 },
 {
  "input": "You could say we only ever want to consider the normal vector that is pointing up, that has a positive z component.",
  "translatedText": "Можна сказати, що ми хочемо розглядати тільки вектор нормалі, який спрямований вгору, який має додатну компоненту z.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 533.38,
  "end": 538.34
 },
 {
  "input": "Or, more simply, we could say, just take the absolute value of that cosine, and that gives us a valid formula.",
  "translatedText": "Або, простіше, ми могли б сказати, просто візьміть абсолютне значення цього косинуса, і це дасть нам дійсну формулу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 538.84,
  "end": 544.72
 },
 {
  "input": "So Bob's happy because he has a precise formula describing the area of the shadow.",
  "translatedText": "Отже, Боб щасливий, тому що в нього є точна формула, яка описує область тіні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 546.98,
  "end": 550.86
 },
 {
  "input": "But Alice starts to think about it a little bit differently.",
  "translatedText": "Але Аліса починає думати про це дещо інакше.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 551.5,
  "end": 554.06
 },
 {
  "input": "She says, okay, we've got some shape, and then we apply a rotation that sort of situates it into 3D space in some way.",
  "translatedText": "Вона каже: добре, у нас є певна форма, а потім ми застосовуємо обертання, яке певним чином переміщує її в 3D-простір.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.06,
  "end": 560.52
 },
 {
  "input": "And then we apply a flat projection that shoves that back into two-dimensional space.",
  "translatedText": "А потім ми застосовуємо плоску проекцію, яка повертає це назад у двовимірний простір.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 560.78,
  "end": 564.66
 },
 {
  "input": "And what stands out to her is that both of these are linear transformations.",
  "translatedText": "І що впадає в око, так це те, що обидва ці перетворення є лінійними.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.08,
  "end": 568.34
 },
 {
  "input": "That means that in principle you could describe each one of them with a matrix, and that the overall transformation would look like the product of those two matrices.",
  "translatedText": "Це означає, що в принципі ви можете описати кожну з них за допомогою матриці, і що загальне перетворення виглядатиме як добуток цих двох матриць.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 569.06,
  "end": 576.2
 },
 {
  "input": "What Alice knows from one of her favorite subjects, linear algebra, is that if you take some shape and you consider its area, then you apply some linear transformation, then the area of that output looks like some constant times the original area of the shape.",
  "translatedText": "Те, що Аліса знає з одного зі своїх улюблених предметів, лінійної алгебри, полягає в тому, що якщо ви берете певну фігуру та розглядаєте її площу, потім застосовуєте деяке лінійне перетворення, тоді площа цього виходу виглядає як деяка стала, помножена на початкову площу фігури.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 577.0,
  "end": 590.32
 },
 {
  "input": "More specifically, we have a name for that constant.",
  "translatedText": "Точніше, у нас є назва для цієї константи.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 590.9,
  "end": 592.78
 },
 {
  "input": "It's called the determinant of the transformation.",
  "translatedText": "Це називається детермінантом перетворення.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.86,
  "end": 594.96
 },
 {
  "input": "If you're not so comfortable with linear algebra, we could give a much more intuitive description and say, if you uniformly stretch the original shape in some direction, the output will also uniformly get stretched in some direction.",
  "translatedText": "Якщо вам не дуже зручно працювати з лінійною алгеброю, ми могли б дати набагато більш інтуїтивно зрозумілий опис і сказати, що якщо ви рівномірно розтягуєте вихідну форму в певному напрямку, результат також буде рівномірно розтягнутий у певному напрямку.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.26,
  "end": 607.56
 },
 {
  "input": "So the area of each of them should scale in proportion to each other.",
  "translatedText": "Отже, площа кожного з них повинна масштабуватися пропорційно один одному.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 607.56,
  "end": 611.4
 },
 {
  "input": "Now in principle, Alice could compute this determinant, but it's not really her style to do that, at least not to do so immediately.",
  "translatedText": "У принципі, Аліса могла б обчислити цей визначник, але це не зовсім її стиль робити це, принаймні не робити це відразу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 612.16,
  "end": 618.32
 },
 {
  "input": "Instead, the thing that she writes down is how this proportionality constant between our original shape and its shadow does not depend on the original shape.",
  "translatedText": "Замість цього вона записує, як ця константа пропорційності між нашою вихідною формою та її тінню не залежить від вихідної форми.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 618.88,
  "end": 627.1
 },
 {
  "input": "We could be talking about the shadow of this cat outline, or anything else, and the size of it doesn't really matter.",
  "translatedText": "Ми могли б говорити про тінь цього контуру кота чи щось інше, і її розмір насправді не має значення.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 627.26,
  "end": 632.64
 },
 {
  "input": "The only thing affecting that proportionality constant is what transformation we're applying, which in this context means we could write it down as some factor that depends on the rotation being applied to the shape.",
  "translatedText": "Єдине, що впливає на константу пропорційності, це те, яке перетворення ми застосовуємо, що в цьому контексті означає, що ми можемо записати це як певний фактор, який залежить від обертання, застосованого до форми.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.64,
  "end": 643.14
 },
 {
  "input": "In the back of our mind, because of Bob's calculation, we know what that factor looks like.",
  "translatedText": "У глибині нашої свідомості, завдяки розрахункам Боба, ми знаємо, як виглядає цей фактор.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 644.5,
  "end": 648.22
 },
 {
  "input": "You know, it's the absolute value of the cosine of the angle between the normal vector and the vertical.",
  "translatedText": "Знаєте, це абсолютне значення косинуса кута між нормальним вектором і вертикаллю.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 648.36,
  "end": 652.5
 },
 {
  "input": "But Alice right now is just saying, yeah, yeah, yeah, I can think about that eventually when I want to.",
  "translatedText": "Але зараз Аліса просто каже: так, так, так, я можу подумати про це коли захочу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 653.16,
  "end": 656.82
 },
 {
  "input": "But she knows we're about to average over all the different orientations anyway, though she holds out some hope that any specific formula about a specific orientation might get washed away in that average.",
  "translatedText": "Але вона знає, що ми збираємося усереднити всі різні орієнтації в будь-якому випадку, хоча вона має деяку надію, що будь-яка конкретна формула про конкретну орієнтацію може бути змита в цьому середньому.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 657.04,
  "end": 666.8
 },
 {
  "input": "Now it's easy to look at this and say, okay, well Alice isn't really doing anything then.",
  "translatedText": "Тепер легко подивитись на це і сказати: гаразд, тоді Аліса насправді нічого не робить.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 668.22,
  "end": 671.64
 },
 {
  "input": "Of course the area of the shadow is proportional to the area of the original shape.",
  "translatedText": "Звичайно, площа тіні пропорційна площі вихідної форми.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 671.78,
  "end": 675.44
 },
 {
  "input": "They're both two-dimensional quantities, they should both scale like two-dimensional things.",
  "translatedText": "Вони обидві є двовимірними величинами, вони повинні масштабуватися як двовимірні речі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 675.62,
  "end": 679.64
 },
 {
  "input": "But keep in mind, this would not at all be true if we were dealing with the harder case that has a closer light source.",
  "translatedText": "Але майте на увазі, що це було б зовсім не так, якби ми мали справу з більш складним випадком, який має ближче джерело світла.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 680.2,
  "end": 685.68
 },
 {
  "input": "In that case, the projection is not linear.",
  "translatedText": "У цьому випадку проекція не є лінійною.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.84,
  "end": 687.98
 },
 {
  "input": "For example, if I rotate this cat so that its tail ends up quite close to the light source, then if I stretch the original shape uniformly in the x-direction, say by a factor of 1.5, it might have a very disproportionate effect on the ultimate shadow, because the tail gets very disproportionately blown up as it gets really close to the light.",
  "translatedText": "Наприклад, якщо я обертаю цього кота так, щоб його хвіст опинився досить близько до джерела світла, тоді я рівномірно розтягну початкову форму в напрямку x, скажімо, у 1 разі.5, це може мати дуже непропорційний вплив на кінцеву тінь, тому що хвіст стає дуже непропорційним, коли він наближається до світла.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 687.98,
  "end": 706.2
 },
 {
  "input": "Again, Alice is keeping an eye out for what properties of the problem are actually relevant, because that helps her know how much she can generalize things.",
  "translatedText": "Знову ж таки, Аліса стежить за тим, які властивості проблеми є насправді актуальними, оскільки це допомагає їй знати, наскільки вона може узагальнювати речі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 706.88,
  "end": 713.44
 },
 {
  "input": "Does the fact that we're thinking about a square face and not some other shape matter?",
  "translatedText": "Чи має значення той факт, що ми думаємо про квадратне обличчя, а не про якусь іншу форму?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 713.96,
  "end": 717.26
 },
 {
  "input": "No, not really.",
  "translatedText": "Ні, не дуже.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.26,
  "end": 718.64
 },
 {
  "input": "Does the fact that the transformation is linear matter?",
  "translatedText": "Чи має значення те, що перетворення є лінійним?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 718.78,
  "end": 721.32
 },
 {
  "input": "Yes, absolutely.",
  "translatedText": "Так, точно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 721.82,
  "end": 722.84
 },
 {
  "input": "Alice can also apply a similar way of thinking about the average shadow for any shape like this.",
  "translatedText": "Аліса також може застосувати подібний спосіб мислення щодо середньої тіні для будь-якої подібної форми.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.56,
  "end": 731.76
 },
 {
  "input": "Say we have some sequence of rotations that we apply to our square face, and let's call them R1, R2, R3, and so on.",
  "translatedText": "Скажімо, у нас є певна послідовність обертань, яку ми застосовуємо до нашої квадратної грані, і давайте назвемо їх R1, R2, R3 і так далі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.02,
  "end": 739.56
 },
 {
  "input": "Then the area of the shadow in each one of those cases looks like some factor times the area of the square, and that factor depends on the rotation.",
  "translatedText": "Тоді площа тіні в кожному з цих випадків виглядає як деякий множник, помножений на площу квадрата, і цей множник залежить від повороту.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 739.72,
  "end": 747.3
 },
 {
  "input": "So if we take an empirical average for that shadow across the sample of rotations we're looking at right now, the way it looks is to add up all of those shadow areas and then divide by the total number that we have.",
  "translatedText": "Отже, якщо ми візьмемо емпіричне середнє для цієї тіні по вибірці обертань, які ми зараз дивимося, то це виглядає так: скласти всі ті тіні, а потім розділити на загальну кількість, яку ми маємо.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 748.06,
  "end": 758.32
 },
 {
  "input": "Now, because of the linearity, this area of the original square can cleanly factor out of all of that, and it ends up on the left.",
  "translatedText": "Тепер, через лінійність, цю область вихідного квадрата можна чітко вирахувати з усього цього, і вона опиниться ліворуч.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 758.9,
  "end": 766.46
 },
 {
  "input": "This isn't the exact average that we're looking for, it's just an empirical mean of a sample of rotations, but in principle what we're looking for is what this approaches as the size of our sample approaches infinity, and all the parts that depend on the size of the sample sit cleanly away from the area itself.",
  "translatedText": "Це не точне середнє значення, яке ми шукаємо, це лише емпіричне середнє значення вибірки обертань, але в принципі ми шукаємо те, до чого це наближається, коли розмір нашої вибірки наближається до нескінченності, і всі частини, які залежать від розміру зразка, розташовані чисто подалі від самої області.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 767.2,
  "end": 783.04
 },
 {
  "input": "So whatever this approaches in the limit, it's just going to be some number.",
  "translatedText": "Отже, до чого б це не наближалося в межах, це буде просто деяке число.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 783.58,
  "end": 786.46
 },
 {
  "input": "It might be a royal pain to compute, we're not sure about that yet, but the thing that Alice notes is that it's independent of the size and the shape of the particular 2D thing that we're looking at.",
  "translatedText": "Це може бути справжньою мукою обчислювати, ми ще не впевнені в цьому, але те, що зауважує Аліса, полягає в тому, що це не залежить від розміру та форми конкретного 2D-об’єкта, який ми дивимося.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 786.82,
  "end": 795.66
 },
 {
  "input": "It's a universal proportionality constant, and her hope is that that universality somehow lends itself to a more elegant way to deduce what it must be.",
  "translatedText": "Це універсальна константа пропорційності, і вона сподівається, що ця універсальність якимось чином піддається більш елегантному способу виведення того, що вона має бути.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 795.72,
  "end": 804.94
 },
 {
  "input": "Now Bob would be eager to compute this constant here and now, and in a few minutes I'll show you how he does it.",
  "translatedText": "Тепер Боб захоче обчислити цю константу тут і зараз, і за кілька хвилин я покажу вам, як він це робить.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 806.26,
  "end": 811.72
 },
 {
  "input": "But before that I do want to stay in Alice's world for a little bit more, because this is where things start to really get fun.",
  "translatedText": "Але перед цим я хочу ще трохи побути у світі Аліси, тому що тут усе починає справді веселитися.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 812.04,
  "end": 816.96
 },
 {
  "input": "In her desire to understand the overall structure of the question before diving into the details, she's curious now about how the area of the shadow of the cube relates to the area of its individual faces.",
  "translatedText": "У своєму бажанні зрозуміти загальну структуру питання, перш ніж занурюватися в деталі, їй зараз цікаво, як площа тіні куба співвідноситься з площею його окремих граней.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 820.08,
  "end": 831.1
 },
 {
  "input": "Now if we can say something about the average area of a particular face, does that tell us anything about the average area of the cube as a whole?",
  "translatedText": "Тепер, якщо ми можемо щось сказати про середню площу окремої грані, чи це скаже нам щось про середню площу куба в цілому?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 831.62,
  "end": 838.4
 },
 {
  "input": "For example, a simple thing we could say is that that area is definitely less than the sum of the areas across all the faces, because there's a meaningful amount of overlap between those shadows.",
  "translatedText": "Наприклад, можна просто сказати, що ця площа точно менша за суму площ усіх граней, тому що між цими тінями є значне перекриття.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 839.1,
  "end": 848.92
 },
 {
  "input": "But it's not entirely clear how to think about that overlap, because if we focus our attention just on two particular faces, in some orientations they don't overlap at all, but in other orientations they do have some overlap, and the specific shape and area of that overlap seems a little bit tricky to think about, much less how on Earth we would average that across all of the different orientations.",
  "translatedText": "Але не зовсім зрозуміло, як думати про це накладання, тому що, якщо ми зосереджуємо нашу увагу лише на двох конкретних обличчях, в деяких орієнтаціях вони взагалі не накладаються, але в інших орієнтаціях вони мають певне перекриття, а конкретна форма і Здається, трохи складно подумати про область цього перекриття, а тим більше про те, як на Землі ми усереднюємо це для всіх різних орієнтацій.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 849.64,
  "end": 869.82
 },
 {
  "input": "But Alice has about three clever insights through this whole problem, and this is the first one of them.",
  "translatedText": "Але в цій проблемі Аліса має три розумні ідеї, і це перша з них.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 870.66,
  "end": 875.46
 },
 {
  "input": "She says, actually, if we think about the whole cube, not just a pair of faces, we can conclude that the area of the shadow for a given orientation is exactly one half the sum of the areas of all of the faces.",
  "translatedText": "Вона каже, що насправді, якщо ми подумаємо про весь куб, а не лише про пару граней, ми можемо зробити висновок, що площа тіні для даної орієнтації рівно половині суми площ усіх граней.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 875.88,
  "end": 888.18
 },
 {
  "input": "Intuitively you can maybe guess that half of them are bathed in the light and half of them are not, but here's the way that she justifies it.",
  "translatedText": "Інтуїтивно ви можете здогадатися, що половина з них залиті світлом, а половина ні, але ось як вона це виправдовує.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 889.58,
  "end": 895.66
 },
 {
  "input": "She says for a particular ray of light, they would go from the sky and eventually hit a point in the shadow.",
  "translatedText": "Вона каже, що для конкретного променя світла вони летіли б з неба і врешті-решт потрапляли в точку в тіні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 895.82,
  "end": 901.4
 },
 {
  "input": "That ray passes through the cube at exactly two points.",
  "translatedText": "Цей промінь проходить через куб рівно в двох точках.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 902.04,
  "end": 904.86
 },
 {
  "input": "There's one moment when it enters and one moment when it exits.",
  "translatedText": "Є один момент, коли він входить, і один момент, коли він виходить.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 905.12,
  "end": 907.6
 },
 {
  "input": "So every point in that shadow corresponds to exactly two faces above it.",
  "translatedText": "Отже, кожна точка в цій тіні відповідає рівно двом граням над нею.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 907.6,
  "end": 913.78
 },
 {
  "input": "Well, okay, that's not exactly true if that beam of light happened to go through the edge of one of the squares.",
  "translatedText": "Добре, це не зовсім так, якщо промінь світла випадково пройшов через край одного з квадратів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 914.46,
  "end": 919.22
 },
 {
  "input": "There's a little bit of ambiguity on how many faces it's passing, but those account for zero area inside the shadow, so we're safe to ignore them if the thing we're trying to do is compute the area.",
  "translatedText": "Існує невелика неоднозначність щодо того, скільки граней він проходить, але вони враховують нульову площу всередині тіні, тому ми можемо ігнорувати їх, якщо ми намагаємося обчислити площу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 919.6,
  "end": 929.04
 },
 {
  "input": "If Alice is pressed and she needs to justify why exactly this is true, which is important for understanding how the problem might generalize, she can appeal to the idea of convexity.",
  "translatedText": "Якщо на Алісу тиснуть і їй потрібно обґрунтувати, чому саме це правда, що важливо для розуміння того, як проблема може бути узагальнена, вона може звернутися до ідеї опуклості.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 931.02,
  "end": 940.82
 },
 {
  "input": "Convexity is one of those properties where a lot of us have an intuitive sense for what it should mean, you know, it's shapes that just bulge out, they never dent inward.",
  "translatedText": "Опуклість — одна з тих властивостей, де багато хто з нас має інтуїтивне відчуття того, що це має означати, знаєте, це форми, які просто випирають, вони ніколи не вм’яться всередину.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.42,
  "end": 948.58
 },
 {
  "input": "But mathematicians have a pretty clever way of formalizing it that's helpful for actual proofs.",
  "translatedText": "Але математики мають досить хитрий спосіб формалізувати це, який корисний для фактичних доказів.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 949.14,
  "end": 953.02
 },
 {
  "input": "They say that a set is convex if the line that connects any two points inside that set is entirely contained within the set itself.",
  "translatedText": "Кажуть, що множина є опуклою, якщо лінія, яка з’єднує будь-які дві точки всередині цієї множини, повністю міститься в самій множині.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 953.68,
  "end": 961.66
 },
 {
  "input": "So a square is convex because no matter where you put two points inside that square, the line connecting them is entirely contained inside the square.",
  "translatedText": "Отже, квадрат є опуклим, тому що незалежно від того, де ви поставите дві точки всередині цього квадрата, лінія, що їх з’єднує, повністю міститься всередині квадрата.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 961.66,
  "end": 969.66
 },
 {
  "input": "But something like the symbol pi is not convex.",
  "translatedText": "Але щось на зразок символу пі не є опуклим.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 970.28,
  "end": 972.72
 },
 {
  "input": "I can easily find two different points so that the line connecting them has to peak outside of the set itself.",
  "translatedText": "Я можу легко знайти дві різні точки так, щоб лінія, що їх з’єднує, мала вершину за межами самого набору.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 972.84,
  "end": 978.32
 },
 {
  "input": "None of the letters in the word convex are themselves convex.",
  "translatedText": "Жодна з букв у слові convex сама по собі не є опуклою.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 978.94,
  "end": 982.6
 },
 {
  "input": "You can find two points so that the line connecting them has to pass outside of the set.",
  "translatedText": "Ви можете знайти дві точки так, щоб пряма, що їх з’єднує, проходила за межами множини.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 982.7,
  "end": 987.02
 },
 {
  "input": "It's a really clever way to formalize this idea of a shape that only bulges out, because any time that it dents inward, you can find these counterexample lines.",
  "translatedText": "Це дійсно розумний спосіб формалізувати цю ідею форми, яка лише випинається, тому що щоразу, коли вона вгинається всередину, ви можете знайти ці контрприклади ліній.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.46,
  "end": 995.54
 },
 {
  "input": "Our cube, because it's convex, between the first point of entry and the last point of exit, it has to stay entirely inside the cube by definition of convexity.",
  "translatedText": "Наш куб, оскільки він опуклий, між першою точкою входу й останньою точкою виходу, він має повністю залишатися всередині куба згідно з визначенням опуклості.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 996.1,
  "end": 1005.18
 },
 {
  "input": "But if we were dealing with some other non-convex shape, like a donut, you could find a ray of light that enters, then exits, then enters, then exits again, so you wouldn't have a clean two-to-one cover from the shadows.",
  "translatedText": "Але якби ми мали справу з якоюсь іншою неопуклою формою, як-от пончик, ви могли б знайти промінь світла, який входить, потім виходить, потім входить і знову виходить, тож у вас не буде чистого співвідношення два до одного прикриття від тіні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1005.74,
  "end": 1016.16
 },
 {
  "input": "The shadows of all of its different parts, if you were to cover this in a bunch of faces, would not be precisely two times the area of the shadow itself.",
  "translatedText": "Тіні всіх її частин, якщо ви покриєте це купою облич, не будуть рівно вдвічі більші за площу самої тіні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1016.6,
  "end": 1024.08
 },
 {
  "input": "So, that's the first key insight, the face shadows double cover the cube shadow.",
  "translatedText": "Отже, це перше ключове розуміння: тіні обличчя подвійно покривають кубічну тінь.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.76,
  "end": 1028.26
 },
 {
  "input": "And the next one is a little bit more symbolic, so let's start things off by abbreviating our notation a little to make room on the screen.",
  "translatedText": "А наступний є трохи більш символічним, тож давайте почнемо з того, що трохи скоротимо позначення, щоб звільнити місце на екрані.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1028.88,
  "end": 1034.66
 },
 {
  "input": "Instead of writing the area of the shadow of the cube, I'm just going to write s of the cube.",
  "translatedText": "Замість того, щоб писати площу тіні куба, я просто напишу s куба.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1035.36,
  "end": 1039.68
 },
 {
  "input": "And similarly, instead of the area of the shadow of a particular face, I'm just going to write s of f, where that subscript j indicates which face I'm talking about.",
  "translatedText": "І так само замість області тіні певного обличчя я просто напишу s або f, де цей індекс j вказує, про яке обличчя я говорю.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.32,
  "end": 1048.42
 },
 {
  "input": "But of course, we should really be talking about the shadow of a particular rotation applied to the cube.",
  "translatedText": "Але, звісно, ми повинні говорити про тінь конкретного обертання, застосованого до куба.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1048.42,
  "end": 1053.62
 },
 {
  "input": "So I might write this as s of some rotation applied to the cube, and likewise on the right, it's the area of the shadow of that same rotation applied to a given one of the faces.",
  "translatedText": "Отже, я міг би записати це як s деякого обертання, застосованого до куба, і так само праворуч це область тіні того самого обертання, застосованого до даної однієї з граней.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.1,
  "end": 1063.26
 },
 {
  "input": "With the more compact notation at hand, let's think about the average of this shadow area across many different rotations, some sample of r1, r2, r3, and so on.",
  "translatedText": "Маючи під рукою більш компактну нотацію, давайте подумаємо про середнє значення цієї тіньової області для багатьох різних обертань, деякі вибірки r1, r2, r3 тощо.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1063.76,
  "end": 1073.7
 },
 {
  "input": "Again, that average just involves adding up all of those shadow areas and then dividing them by n.",
  "translatedText": "Знову ж таки, це середнє значення включає лише додавання всіх тіньових областей, а потім їх ділення на n.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1074.12,
  "end": 1079.22
 },
 {
  "input": "And in principle, if we were to look at this for larger and larger samples, let n approach infinity, that would give us the average area of the shadow of the cube.",
  "translatedText": "І в принципі, якби ми дивилися на це для все більших і більших зразків, нехай n наближається до нескінченності, це дасть нам середню площу тіні куба.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.94,
  "end": 1087.36
 },
 {
  "input": "Some of you might be thinking, yes, we know this, you've said this already, but it's beneficial to write it out so that we can understand why it is that expressing the shadow area for a particular rotation of the cube as a sum across all of its faces, or one half times that sum at least, why is that beneficial?",
  "translatedText": "Дехто з вас може подумати: так, ми знаємо це, ви вже це сказали, але було б корисно записати це, щоб ми могли зрозуміти, чому виражаємо область тіні для конкретного обертання куба як суму по всіх його гранях, або принаймні в половину цього розміру, чому це вигідно?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1088.26,
  "end": 1103.42
 },
 {
  "input": "What is it going to do for us?",
  "translatedText": "Що це нам дасть?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1103.6,
  "end": 1104.76
 },
 {
  "input": "Well, let's just write it out, where for each one of these rotations of the cube, we could break down that shadow as a sum across that same rotation applied across all of the faces.",
  "translatedText": "Ну, давайте просто напишемо це, де для кожного з цих поворотів куба ми можемо розбити цю тінь як суму того самого обертання, застосованого до всіх граней.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1105.56,
  "end": 1113.9
 },
 {
  "input": "And when it's written as a grid like this, we can get to Alice's second insight, which is to shift the way that we're thinking about the sum from going row by row to instead going column by column.",
  "translatedText": "І коли це записано у вигляді сітки, подібної до цієї, ми можемо дійти до другої ідеї Аліси, яка полягає в тому, щоб змінити спосіб, у який ми думаємо про суму, від рядка за рядком до стовпця за стовпцем.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1114.54,
  "end": 1123.72
 },
 {
  "input": "For example, if we focused our attention just on the first column, what it's telling us is to add up the area of the shadow of the first face across many different orientations.",
  "translatedText": "Наприклад, якщо ми зосередили свою увагу лише на першому стовпці, це нам скаже про додавання області тіні першої грані в багатьох різних орієнтаціях.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1125.84,
  "end": 1135.08
 },
 {
  "input": "So if we were to take that sum and divide it by the size of our sample, that gives us an empirical average for the area of the shadow of this face.",
  "translatedText": "Отже, якби ми взяли цю суму та розділили її на розмір нашої вибірки, це дало б нам емпіричне середнє значення для площі тіні цього обличчя.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1135.64,
  "end": 1142.94
 },
 {
  "input": "So if we take larger and larger samples, letting that size go to infinity, this will approach the average shadow area for a square.",
  "translatedText": "Отже, якщо ми беремо все більші і більші зразки, дозволяючи цьому розміру йти до нескінченності, це наблизиться до середньої площі тіні для квадрата.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1143.8,
  "end": 1150.24
 },
 {
  "input": "Likewise, the second column can be thought of as telling us the average area for the second face of the cube, which should of course be the same number.",
  "translatedText": "Так само можна вважати, що другий стовпець повідомляє нам середню площу другої грані куба, яка, звісно, має бути однаковою.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1152.12,
  "end": 1159.78
 },
 {
  "input": "And same deal for any other column, it's telling us the average area for a particular face.",
  "translatedText": "Те ж саме для будь-якого іншого стовпця, він повідомляє нам середню площу для конкретного обличчя.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1160.44,
  "end": 1164.36
 },
 {
  "input": "So that gives us a very different way of thinking about our whole expression.",
  "translatedText": "Тож це дає нам зовсім інший спосіб думати про весь наш вираз.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1164.98,
  "end": 1168.04
 },
 {
  "input": "Instead of saying add up the areas of the cubes at all the different orientations, we could say just add up the average shadows for the six different faces and divide the total by one half.",
  "translatedText": "Замість того, щоб скласти площі кубів у всіх різних орієнтаціях, ми могли б сказати, що просто склали середні тіні для шести різних граней і розділили загальну суму на половину.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1168.38,
  "end": 1177.56
 },
 {
  "input": "The term on the left here is thinking about adding up rows first, and the term on the right is thinking about adding up columns first.",
  "translatedText": "Термін ліворуч тут думає про те, щоб спочатку додати рядки, а член праворуч думає про те, щоб спочатку додати стовпці.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1178.04,
  "end": 1183.76
 },
 {
  "input": "In short, the average of the sum of the face shadows is the same as the sum of the average of the face shadows.",
  "translatedText": "Коротше кажучи, середнє значення суми тіней на обличчі таке ж, як і сума середніх тіней на обличчі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1184.68,
  "end": 1191.14
 },
 {
  "input": "Maybe that swap seems simple, maybe it doesn't, but I can tell you that there is actually a little bit more than meets the eye to the step that we just took, but we'll get to that later.",
  "translatedText": "Можливо, цей обмін здається простим, можливо, ні, але я можу вам сказати, що насправді крок, який ми щойно зробили, є дещо більшим, ніж здається на перший погляд, але ми поговоримо про це пізніше.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1192.14,
  "end": 1199.7
 },
 {
  "input": "And remember, we know that the average area for a particular face looks like some universal proportionality constant times the area of that face.",
  "translatedText": "І пам’ятайте, ми знаємо, що середня площа конкретного обличчя виглядає як деяка універсальна константа пропорційності, помножена на площу цього обличчя.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1200.78,
  "end": 1208.22
 },
 {
  "input": "So if we're adding this up across all the faces of the cube, we could think of this as equaling some constant times the surface area of the cube.",
  "translatedText": "Отже, якщо ми додамо це по всіх гранях куба, ми могли б подумати, що це дорівнює деякій константі, помноженій на площу поверхні куба.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1208.8,
  "end": 1215.2
 },
 {
  "input": "And that's pretty interesting.",
  "translatedText": "І це досить цікаво.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1215.92,
  "end": 1216.76
 },
 {
  "input": "The average area for the shadow of this cube is going to be proportional to its surface area.",
  "translatedText": "Середня площа тіні цього куба буде пропорційна площі його поверхні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1216.98,
  "end": 1221.48
 },
 {
  "input": "But at the same time, you might complain, well Alice is just pushing around a bunch of symbols here, because none of this matters if we don't know what that proportionality constant is.",
  "translatedText": "Але в той же час ви можете поскаржитися, що Аліса просто навіває купу символів, тому що все це не має значення, якщо ми не знаємо, що таке константа пропорційності.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1222.68,
  "end": 1231.08
 },
 {
  "input": "I mean, it almost seems obvious.",
  "translatedText": "Я маю на увазі, що це майже очевидно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1231.66,
  "end": 1233.38
 },
 {
  "input": "Like, of course the average shadow area should be proportional to the surface area.",
  "translatedText": "Звичайно, середня площа тіні має бути пропорційною площі поверхні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1233.64,
  "end": 1237.62
 },
 {
  "input": "They're both two-dimensional quantities, so they should scale in lockstep with each other.",
  "translatedText": "Вони обидві є двовимірними величинами, тому вони мають масштабуватись узгоджено одна з одною.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1237.88,
  "end": 1242.26
 },
 {
  "input": "I mean, it's not obvious.",
  "translatedText": "Я маю на увазі, що це не очевидно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1243.08,
  "end": 1244.38
 },
 {
  "input": "After all, for a closer light source, it simply wouldn't be true.",
  "translatedText": "Зрештою, для ближчого джерела світла це було б просто неправдою.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1244.64,
  "end": 1247.28
 },
 {
  "input": "And also, this business where we added up the grid column by column versus row by row is a little more nuanced than it might look at first.",
  "translatedText": "Крім того, цей бізнес, де ми складали сітку стовпець за стовпцем проти рядка за рядком, є дещо складнішим, ніж може здатися на перший погляд.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1248.12,
  "end": 1254.7
 },
 {
  "input": "There's a subtle, hidden assumption underlying all of this, which carries a special significance when we choose to revisit the question of what probability distribution is being taken across the space of all orientations.",
  "translatedText": "В основі всього цього лежить тонке, приховане припущення, яке має особливе значення, коли ми вирішуємо повернутися до питання про те, який розподіл ймовірностей приймається в просторі всіх орієнтацій.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1255.22,
  "end": 1266.3
 },
 {
  "input": "But more than anything, the reason that it's not obvious is that the significance of this result right here is not merely that these two values are proportional.",
  "translatedText": "Але понад усе, причина того, що це неочевидно, полягає в тому, що значення цього результату тут не просто в тому, що ці два значення пропорційні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1267.3,
  "end": 1275.36
 },
 {
  "input": "It's that an analogous fact will hold true for any convex solids, and, crucially, the actual content of what Alice has built up so far is that it'll be the same proportionality constant across all of them.",
  "translatedText": "Справа в тому, що аналогічний факт буде справедливим для будь-яких опуклих тіл, і, що важливо, фактичний зміст того, що Аліса створила досі, полягає в тому, що це буде однакова константа пропорційності для всіх них.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1276.14,
  "end": 1287.92
 },
 {
  "input": "Now if you really mull over that, some of you may be able to predict the way that Alice is able to finish things off from here.",
  "translatedText": "Якщо ви справді подумаєте над цим, дехто з вас, можливо, зможе передбачити, як Аліса зможе завершити справу звідси.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1289.28,
  "end": 1294.18
 },
 {
  "input": "It's really delightful.",
  "translatedText": "Це справді чудово.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1294.18,
  "end": 1295.42
 },
 {
  "input": "It's honestly my main reason for covering this topic.",
  "translatedText": "Чесно кажучи, це моя основна причина висвітлювати цю тему.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1295.6,
  "end": 1297.94
 },
 {
  "input": "But before we get into it, I think it's easy to underappreciate her result unless we dig into the details of what it is that she manages to avoid.",
  "translatedText": "Але перш ніж ми перейдемо до цього, я думаю, що легко недооцінити її результат, якщо ми не розберемося в деталях того, чого їй вдається уникнути.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1298.24,
  "end": 1306.14
 },
 {
  "input": "So let's take a moment to turn our attention back into Bob's world, because while Alice has been doing all of this, he's been busy doing some computations.",
  "translatedText": "Тож давайте на хвилинку знову звернемо нашу увагу на світ Боба, тому що поки Аліса робила все це, він був зайнятий виконанням деяких обчислень.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1306.86,
  "end": 1314.4
 },
 {
  "input": "In fact, what he's been working on is finding exactly what Alice has yet to figure out, which is how to take the formula that he found for the area of a square's shadow and taking the natural next step of trying to find the average of that square's shadow averaged over all possible orientations.",
  "translatedText": "Насправді він працював над тим, щоб знайти саме те, що Алісі ще належить з’ясувати, тобто як взяти формулу, яку він знайшов для площі тіні квадрата, і зробити наступний природний крок — спробувати знайти середнє значення цього тінь квадрата, усереднена за всіма можливими орієнтаціями.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1314.98,
  "end": 1329.98
 },
 {
  "input": "So the way Bob starts, if he's thinking about all the different possible orientations for this square, is to ask, what are all the different normal vectors that that square can have in all these orientations, because everything about its shadow comes down to that normal vector.",
  "translatedText": "Отже, те, як Боб починає, якщо він розмірковує про всі різні можливі орієнтації цього квадрата, полягає в тому, щоб запитати, які всі різні нормальні вектори може мати цей квадрат у всіх цих орієнтаціях, тому що все, що стосується його тіні, зводиться до цієї нормалі вектор.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1334.62,
  "end": 1347.24
 },
 {
  "input": "It's not too hard to see that all those possible normal vectors trace out the surface of a sphere.",
  "translatedText": "Неважко побачити, що всі ці можливі нормальні вектори викреслюють поверхню сфери.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1347.8,
  "end": 1352.32
 },
 {
  "input": "If we assume it's a unit normal vector, it's a sphere with radius 1.",
  "translatedText": "Якщо ми припустимо, що це одиничний нормальний вектор, це буде сфера з радіусом 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1352.32,
  "end": 1355.56
 },
 {
  "input": "And furthermore, Bob figures that each point of this sphere should be just as likely to occur as any other.",
  "translatedText": "Крім того, Боб вважає, що кожна точка цієї сфери має бути такою ж імовірною, як і будь-яка інша.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1356.42,
  "end": 1361.58
 },
 {
  "input": "Our probabilities should be uniform in that way.",
  "translatedText": "Таким чином, наші ймовірності повинні бути однаковими.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1362.0,
  "end": 1363.98
 },
 {
  "input": "There's no reason to prefer one direction over another.",
  "translatedText": "Немає причин віддавати перевагу одному напрямку над іншим.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1364.02,
  "end": 1366.32
 },
 {
  "input": "But in the context of continuous probabilities, it's not very helpful to talk about the likelihood of a particular individual point, because in the uncountable infinity of points on the sphere, that would be zero and unhelpful.",
  "translatedText": "Але в контексті безперервних ймовірностей не дуже корисно говорити про ймовірність окремої окремої точки, тому що в незліченній нескінченності точок на сфері це буде нульовим і марним.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1367.12,
  "end": 1377.44
 },
 {
  "input": "So instead, the more precise way to phrase this uniformity would be to say the probability that our normal vector lands in any given patch of area on the sphere should be proportional to that area itself.",
  "translatedText": "Тож натомість точніше сформулювати цю однаковість було б сказати, що ймовірність того, що наш нормальний вектор потрапляє на будь-яку дану ділянку сфери на сфері, має бути пропорційною самій площі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1377.44,
  "end": 1389.44
 },
 {
  "input": "More specifically, it should equal the area of that little patch divided by the total surface area of the sphere.",
  "translatedText": "Точніше, вона повинна дорівнювати площі цієї маленької ділянки, поділеній на загальну площу поверхні сфери.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1389.96,
  "end": 1395.12
 },
 {
  "input": "If that's true, no matter what patch of area we're considering, that's what we mean by a uniform distribution on the sphere.",
  "translatedText": "Якщо це правда, незалежно від того, яку ділянку площі ми розглядаємо, це те, що ми маємо на увазі під рівномірним розподілом на сфері.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1395.68,
  "end": 1401.06
 },
 {
  "input": "Now to be clear, points on the sphere are not the same thing as orientations in 3D space, because even if you know what normal vector this square is going to have, that leaves us with another degree of freedom.",
  "translatedText": "Тепер, щоб було зрозуміло, точки на сфері – це не те саме, що орієнтації в 3D-просторі, тому що навіть якщо ви знаєте, який нормальний вектор буде мати цей квадрат, це залишає нам ще один ступінь свободи.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1402.0,
  "end": 1411.7
 },
 {
  "input": "The square could be rotated about that normal vector.",
  "translatedText": "Квадрат можна обертати навколо цього вектора нормалі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1411.9,
  "end": 1414.16
 },
 {
  "input": "But Bob doesn't actually have to care about that extra degree of freedom, because in all of those cases, the area of the shadow is the same.",
  "translatedText": "Але Бобу насправді не потрібно піклуватися про цей додатковий ступінь свободи, тому що в усіх цих випадках площа тіні однакова.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1414.96,
  "end": 1422.0
 },
 {
  "input": "It's only dependent on the cosine of the angle between that normal vector and the vertical.",
  "translatedText": "Це залежить лише від косинуса кута між вектором нормалі та вертикаллю.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1422.36,
  "end": 1426.46
 },
 {
  "input": "Which is kind of neat.",
  "translatedText": "Що начебто акуратно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1427.18,
  "end": 1427.84
 },
 {
  "input": "All those shadows are genuinely different shapes.",
  "translatedText": "Усі ці тіні мають справді різні форми.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1428.0,
  "end": 1430.06
 },
 {
  "input": "They're not the same.",
  "translatedText": "Вони не однакові.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1430.16,
  "end": 1430.9
 },
 {
  "input": "But the area of each of them will be the same.",
  "translatedText": "Але площа кожного з них буде однаковою.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1431.2,
  "end": 1433.54
 },
 {
  "input": "What this means is that when Bob wants this average shadow area over all possible orientations, all he really needs to know is the average value of this absolute value of cosine of theta for all different possible normal vectors, all different possible points on the sphere.",
  "translatedText": "Це означає, що коли Бобу потрібна ця середня площа тіні для всіх можливих орієнтацій, все, що йому дійсно потрібно знати, це середнє значення цього абсолютного значення косинуса тета для всіх різних можливих нормалей, усіх різних можливих точок на сфері.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1434.72,
  "end": 1448.44
 },
 {
  "input": "So, how do you compute an average like this?",
  "translatedText": "Отже, як обчислити таке середнє значення?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1449.12,
  "end": 1451.32
 },
 {
  "input": "Well, if we lived in some kind of discrete pixelated world, where there's only a finite number of possible angles theta that that normal vector could have, the average would be pretty straightforward.",
  "translatedText": "Ну, якби ми жили в якомусь дискретному піксельному світі, де існує лише кінцева кількість можливих кутів тета, які міг би мати цей нормальний вектор, середнє значення було б досить простим.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1452.54,
  "end": 1461.44
 },
 {
  "input": "What you do is find the probability of landing on any particular value of theta, which will tell us something like how much of the sphere do normal vectors with that angle make up, and then you multiply it by the thing we want to take the average of, this formula for the area of the shadow.",
  "translatedText": "Що ви робите, це знаходите ймовірність приземлення на будь-якому конкретному значенні тета, яке скаже нам щось на зразок того, яку частину сфери займають нормальні вектори з таким кутом, а потім ви множите це на те, що ми хочемо взяти середнє цієї формули для площі тіні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1461.44,
  "end": 1475.94
 },
 {
  "input": "And then you would add that up over all of the different possible values of theta, ranging from 0 up to 180 degrees, or pi radians.",
  "translatedText": "А потім ви б додали це до всіх різних можливих значень тета в діапазоні від 0 до 180 градусів, або пі радіан.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1476.86,
  "end": 1484.02
 },
 {
  "input": "But of course, in reality, there is a continuum of possible values of theta, this uncountable infinity, and the probability of landing on any specific particular value of theta will actually be 0.",
  "translatedText": "Але, звичайно, насправді існує безперервність можливих значень тета, ця незліченна нескінченність, і ймовірність приземлення на будь-яке конкретне значення тета фактично дорівнює 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1485.06,
  "end": 1495.98
 },
 {
  "input": "And so a sum like this unfortunately doesn't really make any sense, or if it does make sense, adding up infinitely many zeros should just give us a 0.",
  "translatedText": "І тому така сума, на жаль, насправді не має сенсу, або якщо вона має сенс, додавання нескінченної кількості нулів має просто дати нам 0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1496.68,
  "end": 1504.16
 },
 {
  "input": "The short answer for what we do instead is that we compute an integral.",
  "translatedText": "Коротка відповідь на те, що ми робимо замість цього, полягає в тому, що ми обчислюємо інтеграл.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1505.8,
  "end": 1508.88
 },
 {
  "input": "And I'll level with you, the hard part here is I'm not entirely sure what background I should be assuming from those of you watching right now.",
  "translatedText": "І я погоджуся з вами, найважче тут те, що я не зовсім впевнений, яку історію я повинен припустити від тих з вас, хто зараз дивиться.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1509.66,
  "end": 1515.26
 },
 {
  "input": "Maybe it's the case that you're quite comfortable with calculus and you don't need me to belabor the point here.",
  "translatedText": "Можливо, ви досить добре розбираєтеся в обчисленні, і вам не потрібно, щоб я наголошував на цьому.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1515.64,
  "end": 1519.8
 },
 {
  "input": "Maybe it's the case that you're not familiar with calculus and I shouldn't just be throwing down integrals like that.",
  "translatedText": "Можливо, справа в тому, що ви не знайомі з численням, і я не повинен просто так викидати інтеграли.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.8,
  "end": 1524.78
 },
 {
  "input": "Or maybe you took a calculus class a while ago but you need a little bit of a refresher.",
  "translatedText": "Або, можливо, ви нещодавно відвідували курс математики, але вам потрібно трохи відновити знання.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1524.86,
  "end": 1529.44
 },
 {
  "input": "I'm going to go with the option of setting this up as if it's a calculus lesson, because to be honest, even when you are quite comfortable with integrals, setting them up can be kind of an error-prone process, and calling back to the underlying definition is a good way to sort of check yourself in the process.",
  "translatedText": "Я збираюся налаштувати це так, ніби це урок обчислення, тому що, чесно кажучи, навіть якщо ви досить добре користуєтеся інтегралами, їх налаштування може бути певним чином схильним до помилок процесом і передзвонити до базового визначення — це хороший спосіб перевірити себе в процесі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1529.82,
  "end": 1543.04
 },
 {
  "input": "If we lived in a time before calculus existed and integrals weren't a thing, and we wanted to approximate an answer to this question, one way we could go about it is to take a sample of values for θ that ranges from 0 up to 180°.",
  "translatedText": "Якби ми жили в часи, коли ще не існувало обчислення, і інтеграли не були річчю, і ми хотіли приблизно відповісти на це запитання, один із способів зробити це — взяти вибірку значень для θ, яка коливається від 0 до 180°.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1543.78,
  "end": 1556.52
 },
 {
  "input": "We might think of them as evenly spaced with some sort of difference between each one, some delta θ.",
  "translatedText": "Ми можемо вважати їх рівномірно розташованими з певною різницею між кожним, деякою дельтою θ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1557.18,
  "end": 1562.04
 },
 {
  "input": "And it's still the case that it would be unhelpful to ask about the probability of a particular value of θ occurring, even if it's 1 in our sample.",
  "translatedText": "І все ще так, що було б марно запитувати про ймовірність появи певного значення θ, навіть якщо воно дорівнює 1 у нашій вибірці.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1562.62,
  "end": 1569.24
 },
 {
  "input": "That probability would still be 0 and it would be unhelpful.",
  "translatedText": "Ця ймовірність все одно дорівнюватиме 0 і це не допоможе.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1569.66,
  "end": 1572.36
 },
 {
  "input": "But what is helpful to ask is the probability of falling between two different values from our sample, in this little band of latitude with a width of delta θ.",
  "translatedText": "Але корисно запитати про ймовірність потрапляння між двома різними значеннями з нашої вибірки в цій невеликій смузі широти з шириною дельта θ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1572.36,
  "end": 1582.02
 },
 {
  "input": "Based on our assumption that the distribution along this sphere should be uniform, that probability comes down to knowing the area of this band.",
  "translatedText": "Виходячи з нашого припущення, що розподіл уздовж цієї сфери має бути рівномірним, ця ймовірність зводиться до знання площі цієї смуги.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1582.4,
  "end": 1589.56
 },
 {
  "input": "More specifically, the chances that a randomly chosen vector lands in that band should be that area divided by the total surface area of the sphere.",
  "translatedText": "Точніше, ймовірність того, що випадково вибраний вектор потрапить у цю смугу, має бути поділена на площу поверхні сфери.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1590.02,
  "end": 1596.72
 },
 {
  "input": "To figure out that area, let's first think of the radius of that band, which, if the radius of our sphere is 1, is definitely going to be smaller than 1.",
  "translatedText": "Щоб визначити цю площу, давайте спочатку подумаємо про радіус цієї смуги, який, якщо радіус нашої сфери дорівнює 1, точно буде меншим за 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1596.72,
  "end": 1605.28
 },
 {
  "input": "And in fact, if we draw the appropriate little right triangle here, you can see that that little radius, let's just say at the top of the band, should be the sine of our angle, the sine of θ.",
  "translatedText": "І фактично, якщо ми намалюємо тут відповідний маленький прямокутний трикутник, ви побачите, що маленький радіус, скажімо так, у верхній частині смуги, має бути синусом нашого кута, синусом θ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1605.9,
  "end": 1614.78
 },
 {
  "input": "This means that the circumference of the band should be 2π times the sine of that angle, and then the area of the band should be that circumference times its thickness, that little delta θ.",
  "translatedText": "Це означає, що окружність смуги має бути 2π, помножена на синус цього кута, а тоді площа смуги має бути дорівнює цьому окружності, помноженому на її товщину, цю маленьку дельту θ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1615.52,
  "end": 1625.52
 },
 {
  "input": "Or rather, the area of our band is approximately this quantity.",
  "translatedText": "Вірніше, площа нашої смуги приблизно дорівнює цій величині.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1625.52,
  "end": 1629.08
 },
 {
  "input": "What's important is that for a finer sample of many more values of θ, the accuracy of that approximation would get better and better.",
  "translatedText": "Важливо те, що для більш тонкої вибірки з набагато більшою кількістю значень θ точність цього наближення ставатиме все кращою.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1629.54,
  "end": 1636.32
 },
 {
  "input": "Now remember, the reason we wanted this area is to know the probability of falling into that band, which is this area divided by the surface area of the sphere, which we know to be 4π times its radius squared.",
  "translatedText": "А тепер запам’ятайте, причина, чому нам потрібна ця площа, полягає в тому, щоб знати ймовірність потрапляння в цю смугу, яка дорівнює цій площі, поділеній на площу поверхні сфери, яка, як ми знаємо, дорівнює 4π, помноженим на радіус у квадраті.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1637.54,
  "end": 1648.08
 },
 {
  "input": "That's a value that you could also compute with an integral similar to the one that we're setting up now, but for now we can take it as a given, as a standard well-known formula.",
  "translatedText": "Це значення, яке ви також можете обчислити за допомогою інтеграла, подібного до того, який ми зараз встановлюємо, але поки що ми можемо прийняти його як даність, як стандартну добре відому формулу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1648.66,
  "end": 1656.08
 },
 {
  "input": "And this probability itself is just a stepping stone in the direction of what we actually want, which is the average area for the shadow of a square.",
  "translatedText": "І сама ця ймовірність є лише сходинкою в напрямку того, чого ми насправді хочемо, тобто середньої площі тіні квадрата.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1656.84,
  "end": 1663.32
 },
 {
  "input": "To get that, we'll multiply this probability times the corresponding shadow area, which is this absolute value of cosθ expression we've seen many times up to this point.",
  "translatedText": "Щоб отримати це, ми помножимо цю ймовірність на відповідну область тіні, яка є цим абсолютним значенням виразу cosθ, яке ми бачили багато разів до цього моменту.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1664.24,
  "end": 1673.02
 },
 {
  "input": "And our estimate for this average would now come down to adding up this expression across all of the different bands, all of the different samples of θ that we've taken.",
  "translatedText": "І наша оцінка для цього середнього тепер зводиться до додавання цього виразу для всіх різних діапазонів, усіх різних зразків θ, які ми взяли.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1673.5,
  "end": 1681.7
 },
 {
  "input": "This right here, by the way, is when Bob is just totally in his element.",
  "translatedText": "До речі, саме тут Боб перебуває у своїй стихії.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1683.44,
  "end": 1686.36
 },
 {
  "input": "We've got a lot of exact formulas describing something very concrete, actually digging in on our way to a real answer.",
  "translatedText": "У нас є багато точних формул, які описують щось дуже конкретне, фактично копаючись на шляху до справжньої відповіді.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1686.58,
  "end": 1691.86
 },
 {
  "input": "And again, if it feels like a lot of detail, I want you to appreciate that fact, so that you can appreciate just how magical it is when Alice manages to somehow avoid all of this.",
  "translatedText": "І знову ж таки, якщо здається, що це багато деталей, я хочу, щоб ви оцінили цей факт, щоб ви могли оцінити, наскільки це чарівно, коли Алісі вдається якимось чином уникнути всього цього.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.52,
  "end": 1701.92
 },
 {
  "input": "Anyway, looking back at our expression, let's clean things up a little bit, like factoring out all of the terms that don't depend on θ itself.",
  "translatedText": "У будь-якому випадку, озираючись назад на наш вираз, давайте трошки очистимо речі, наприклад, вилучимо всі доданки, які не залежать від самого θ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.88,
  "end": 1709.0
 },
 {
  "input": "And we can simplify that 2π divided by 4π to simply be 1 half.",
  "translatedText": "І ми можемо спростити, що 2π, поділене на 4π, буде просто 1 половина.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1709.72,
  "end": 1713.48
 },
 {
  "input": "And to make it a little more analogous to calculus, with integrals, let me just swap the main terms inside the sum here.",
  "translatedText": "І щоб зробити це трохи більш аналогічним до обчислення з інтегралами, дозвольте мені просто поміняти місцями основні члени всередині суми.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1714.54,
  "end": 1719.46
 },
 {
  "input": "What we now have, this sum that's going to approximate the answer to our question, is almost what an integral is.",
  "translatedText": "Те, що ми зараз маємо, ця сума, яка буде наближено відповідати на наше запитання, майже є інтегралом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1719.96,
  "end": 1726.04
 },
 {
  "input": "Instead of writing the sigma for sum, we write the integral symbol, this kind of elongated Leibnizian s, showing us that we're going from 0 to π.",
  "translatedText": "Замість того, щоб писати сигму для суми, ми пишемо інтегральний символ, такий вид подовженого Лейбніца s, який показує нам, що ми переходимо від 0 до π.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1726.48,
  "end": 1733.98
 },
 {
  "input": "And instead of describing the step size as δθ, a concrete finite amount, we instead describe it as dθ, which I like to think of as signaling the fact that some kind of limit is being taken.",
  "translatedText": "І замість того, щоб описувати розмір кроку як δθ, конкретну кінцеву величину, ми натомість описуємо його як dθ, що мені подобається вважати сигналом того, що дотримується певного ліміту.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1734.72,
  "end": 1745.16
 },
 {
  "input": "What that integral means, by definition, is whatever the sum on the bottom approaches for finer and finer subdivisions, more dense samples that we might take for θ itself.",
  "translatedText": "Цей інтеграл, за визначенням, означає те, що сума в нижній частині наближається до все дрібніших підрозділів, більш щільних вибірок, які ми можемо взяти для самого θ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1746.08,
  "end": 1757.1
 },
 {
  "input": "And at this point, for those of you who do know calculus, I'll just write down the details of how you would actually carry this out, as you might see it written down in Bob's notebook.",
  "translatedText": "А зараз для тих із вас, хто знає обчислення, я просто запишу деталі того, як ви насправді це зробите, як ви можете побачити це в блокноті Боба.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1759.04,
  "end": 1766.62
 },
 {
  "input": "It's the usual anti-derivative stuff, but the one key step is to bring in a certain trig identity.",
  "translatedText": "Це звичайні антипохідні речі, але один ключовий крок полягає в тому, щоб запровадити певну ідентифікацію тригонометра.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1767.16,
  "end": 1772.16
 },
 {
  "input": "In the end, what Bob finds after doing this is the surprisingly clean fact that the average area for a square's shadow is precisely one half the area of that square.",
  "translatedText": "Зрештою, після цього Боб виявив напрочуд чіткий факт, що середня площа тіні квадрата рівно половині площі цього квадрата.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1773.06,
  "end": 1783.52
 },
 {
  "input": "This is the mystery constant, which Alice doesn't yet know.",
  "translatedText": "Це таємнича константа, про яку Аліса ще не знає.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1784.58,
  "end": 1787.56
 },
 {
  "input": "If Bob were to look over her shoulder and see the work that she's done, he could finish out the problem right now.",
  "translatedText": "Якби Боб озирнувся через її плече й побачив, яку роботу вона виконала, він міг би вирішити проблему прямо зараз.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1788.12,
  "end": 1792.78
 },
 {
  "input": "He plugs in the constant that he just found, and he knows the final answer.",
  "translatedText": "Він підключає константу, яку щойно знайшов, і знає остаточну відповідь.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1793.0,
  "end": 1796.16
 },
 {
  "input": "And now, finally, with all of this as backdrop, what is it that Alice does to carry out the final solution?",
  "translatedText": "А тепер, нарешті, на тлі всього цього, що робить Аліса, щоб знайти остаточне рішення?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1800.22,
  "end": 1806.2
 },
 {
  "input": "I introduced her as someone who really likes to generalize the results she finds.",
  "translatedText": "Я представив її як людину, яка дуже любить узагальнювати результати, які вона знаходить.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1806.86,
  "end": 1810.26
 },
 {
  "input": "And usually those generalizations end up as interesting footnotes that aren't really material for solving particular problems.",
  "translatedText": "І зазвичай ці узагальнення закінчуються цікавими виносками, які насправді не є матеріалом для вирішення конкретних проблем.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1810.84,
  "end": 1816.68
 },
 {
  "input": "But this is a case where the generalization itself draws her to a quantitative result.",
  "translatedText": "Але це той випадок, коли саме узагальнення підводить її до кількісного результату.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1817.18,
  "end": 1821.76
 },
 {
  "input": "Remember, the substance of what she's found so far is that if you look at any convex solid, then the average area for its shadow is going to be proportional to its surface area, and critically, it'll be the same proportionality constant across all of these solids.",
  "translatedText": "Пам’ятайте: суть того, що вона знайшла, полягає в тому, що якщо ви подивитеся на будь-яке опукле тіло, то середня площа його тіні буде пропорційна площі його поверхні, і, що важливо, вона буде однаковою константою пропорційності для всіх цих твердих речовин.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1821.76,
  "end": 1836.5
 },
 {
  "input": "So all Alice needs to do is find just a single convex solid out there where she already knows the average area of its shadow.",
  "translatedText": "Отже, усе, що потрібно зробити Алісі, це знайти лише одне опукле тіло, де вона вже знає середню площу його тіні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1837.1,
  "end": 1844.46
 },
 {
  "input": "And some of you may see where this is going.",
  "translatedText": "І дехто з вас може побачити, куди це веде.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1845.16,
  "end": 1846.84
 },
 {
  "input": "The most symmetric solid available to us is a sphere.",
  "translatedText": "Найбільш симетричним тілом з доступних нам є сфера.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1846.84,
  "end": 1850.06
 },
 {
  "input": "No matter what the orientation of that sphere, its shadow, the flat projection shadow, is always a circle with an area of πr².",
  "translatedText": "Незалежно від орієнтації цієї сфери, її тінь, тінь плоскої проекції, завжди є колом із площею πr².",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1850.52,
  "end": 1858.02
 },
 {
  "input": "So in particular, that's its average shadow area.",
  "translatedText": "Так, зокрема, це середня площа тіні.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1858.62,
  "end": 1861.04
 },
 {
  "input": "And the surface area of a sphere, like I mentioned before, is exactly 4πr².",
  "translatedText": "А площа поверхні сфери, як я вже згадував раніше, рівно 4πr².",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1861.78,
  "end": 1866.32
 },
 {
  "input": "By the way, I did make a video talking all about that surface area formula and how Archimedes proved it thousands of years before calculus existed, so you don't need integrals to find it.",
  "translatedText": "До речі, я зняв відео, в якому розповідаю про цю формулу площі поверхні та про те, як Архімед довів це за тисячі років до того, як існувало числення, тому вам не потрібні інтеграли, щоб знайти її.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1867.1,
  "end": 1876.34
 },
 {
  "input": "The magic of what Alice has done is that she can take this seemingly specific fact, that the shadow of a sphere has an area exactly 1⁄4 its surface area, and use it to conclude a much more general fact, that for any convex solid out there, its shadow and surface area are related in the same way, in a certain sense.",
  "translatedText": "Магія того, що зробила Аліса, полягає в тому, що вона може взяти цей, здавалося б, специфічний факт, що тінь сфери має площу рівно 1⁄4 площі її поверхні, і використати це, щоб зробити висновок про набагато більш загальний факт, що для будь-якого опуклого тіла там, його тінь і площа поверхні пов'язані таким же чином, у певному сенсі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1876.34,
  "end": 1893.58
 },
 {
  "input": "So with that, she can go and fill in the details of the particular question about a cube, and say that its average shadow area will be 1⁄4 times its surface area, 6s².",
  "translatedText": "Таким чином, вона може піти і заповнити деталі конкретного запитання про куб, і сказати, що його середня площа тіні буде 1⁄4 площі його поверхні, 6s².",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1894.64,
  "end": 1903.62
 },
 {
  "input": "But the much more memorable fact that you'll go to sleep thinking about is how it didn't really matter that we were talking about a cube at all.",
  "translatedText": "Але набагато більш пам’ятний факт, про який ви ляжете спати, думаючи про те, що це не мало значення, що ми взагалі говорили про куб.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1903.62,
  "end": 1910.8
 },
 {
  "input": "Now, that's all very pretty, but some of you might complain that this isn't really a valid argument, because spheres don't have flat faces.",
  "translatedText": "Все це дуже гарно, але дехто з вас може поскаржитися, що це не зовсім вагомий аргумент, оскільки сфери не мають плоских граней.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1912.52,
  "end": 1919.38
 },
 {
  "input": "When I said Alice's argument generalizes to any convex solid, if we actually look at the argument itself, it definitely depends on the use of a finite number of flat faces.",
  "translatedText": "Коли я сказав, що аргумент Аліси узагальнює будь-яке опукле тіло, якщо ми подивимося на сам аргумент, він точно залежить від використання кінцевої кількості плоских граней.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1920.1,
  "end": 1928.94
 },
 {
  "input": "For example, if we were mapping it to a dodecahedron, you would start by saying that the area of a particular shadow of that dodecahedron looks like exactly 1⁄2 times the sum of the areas of the shadows of all its faces.",
  "translatedText": "Наприклад, якби ми відобразили його на додекаедр, ви б почали зі слів, що площа окремої тіні цього додекаедра виглядає як рівно 1⁄2 суми площ тіней усіх його граней.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1928.94,
  "end": 1940.44
 },
 {
  "input": "Once again, you could use a certain ray of light mixed with convexity argument to draw that conclusion.",
  "translatedText": "Знову ж таки, ви можете використати певний промінь світла, змішаний з аргументом опуклості, щоб зробити такий висновок.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1941.0,
  "end": 1945.44
 },
 {
  "input": "And remember, the benefit of expressing that shadow area as a sum is that when we want to average over a bunch of different rotations, we can describe that sum as a big grid, where we can then go column by column and consider the average area for the shadow of each face.",
  "translatedText": "І пам’ятайте, перевага вираження тіньової області як суми полягає в тому, що коли ми хочемо усереднити групу різних обертань, ми можемо описати цю суму як велику сітку, де потім можемо переходити стовпчик за стовпцем і розглядати середню площу для тіні кожного обличчя.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1946.28,
  "end": 1960.82
 },
 {
  "input": "And also, a critical fact was the conclusion from much earlier, that the average shadow for any 2D object, a flat 2D object, which is important, will equal some universal proportionality constant times its area.",
  "translatedText": "Крім того, важливим фактом був висновок, зроблений набагато раніше, що середня тінь для будь-якого двовимірного об’єкта, плоского двовимірного об’єкта, що важливо, дорівнюватиме якійсь універсальній постійній пропорційності, помноженій на його площу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1961.46,
  "end": 1972.72
 },
 {
  "input": "The significance was that that constant didn't depend on the shape itself.",
  "translatedText": "Важливим було те, що ця константа не залежала від самої форми.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1973.26,
  "end": 1976.12
 },
 {
  "input": "It could have been a square, or a cat, or the pentagonal faces of our dodecahedron, whatever.",
  "translatedText": "Це міг бути квадрат, чи кіт, чи п’ятикутні грані нашого додекаедра, що завгодно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1976.22,
  "end": 1980.84
 },
 {
  "input": "So, after hastily carrying this over to a sphere that doesn't have a finite number of flat faces, you would be right to complain.",
  "translatedText": "Отже, поспішно перенісши це на сферу, яка не має кінцевої кількості плоских граней, ви маєте рацію скаржитися.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1980.84,
  "end": 1988.26
 },
 {
  "input": "But luckily, it's a pretty easy detail to fill in.",
  "translatedText": "Але, на щастя, це досить легко заповнити.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1988.9,
  "end": 1991.24
 },
 {
  "input": "What you can do is imagine a sequence of different polyhedra that successively approximate a sphere, in the sense that their faces hug tighter and tighter around the genuine surface of the sphere.",
  "translatedText": "Що ви можете зробити, це уявити послідовність різних многогранників, які послідовно наближаються до сфери, у тому сенсі, що їхні грані все міцніше й міцніше обіймають справжню поверхню сфери.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1991.64,
  "end": 2001.16
 },
 {
  "input": "For each one of those approximations, we can draw the same conclusion, that its average shadow is going to be proportional to its surface area with this universal proportionality constant.",
  "translatedText": "Для кожного з цих наближень ми можемо зробити той самий висновок, що його середня тінь буде пропорційна його площі поверхні з цією універсальною константою пропорційності.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2001.68,
  "end": 2010.78
 },
 {
  "input": "So then, if we say, okay, let's take the limit of the ratio between the average shadow area at each step and the surface area at each step, well, since that ratio is never changing, it's always equal to this constant, then in the limit, it's also going to equal that constant.",
  "translatedText": "Тоді, якщо ми скажемо, гаразд, давайте візьмемо межу співвідношення між середньою площею тіні на кожному кроці та площею поверхні на кожному кроці, добре, оскільки це співвідношення ніколи не змінюється, воно завжди дорівнює цій константі, тоді в межа, вона також дорівнюватиме цій константі.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2011.2,
  "end": 2024.62
 },
 {
  "input": "But on the other hand, by their definition, in the limit, their average shadow area should be that of a circle, which is πr², and the limit of the surface areas would be the surface area of the sphere, 4πr².",
  "translatedText": "Але з іншого боку, за їхнім визначенням, у межах їхня середня площа тіні повинна бути площею кола, яка дорівнює πr², а межа площі поверхні буде площею поверхні сфери, 4πr².",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2024.62,
  "end": 2036.98
 },
 {
  "input": "So we do genuinely get the conclusion that intuition would suggest, but, as is so common with Alice's argument here, we do have to be a little delicate in how we justify that intuition.",
  "translatedText": "Таким чином, ми справді отримуємо висновок, який може підказати інтуїція, але, як це дуже часто зустрічається в аргументах Аліси, ми повинні бути делікатними у тому, як ми виправдовуємо цю інтуїцію.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2037.66,
  "end": 2047.0
 },
 {
  "input": "It's easy for this contrast of Alice and Bob to come across like a value judgment, as if I'm saying, look how clever Alice has managed to be, she insightfully avoided all those computations that Bob had to do.",
  "translatedText": "Цей контраст Аліси та Боба легко сприйняти як оціночне судження, начебто я кажу, подивіться, якою розумною зуміла бути Аліса, вона проникливо уникала всіх тих обчислень, які доводилося робити Бобу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2052.2,
  "end": 2063.56
 },
 {
  "input": "But that would be a very, um, misguided conclusion.",
  "translatedText": "Але це був би дуже, гм, помилковий висновок.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2063.88,
  "end": 2067.9
 },
 {
  "input": "I think there's an important way that popularizations of math differ from the feeling of actually doing math.",
  "translatedText": "Я вважаю, що популяризація математики дещо відрізняється від відчуття, яке відчуваєш насправді.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2068.56,
  "end": 2074.08
 },
 {
  "input": "There's this bias towards showing the slick proofs, the arguments with some clever keen insight that lets you avoid doing calculations.",
  "translatedText": "Існує така упередженість щодо показу витончених доказів, аргументів із деяким розумним глибоким розумінням, яке дозволяє вам уникати обчислень.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2074.08,
  "end": 2080.78
 },
 {
  "input": "I could just be projecting, since I'm very guilty of this, but what I can tell you, sitting on the other side of the screen here, is that it feels a lot more attractive to make a video about Alice's approach than Bob's.",
  "translatedText": "Я міг просто проектувати, оскільки я дуже винен у цьому, але що я можу вам сказати, сидячи тут по інший бік екрану, так це те, що мені набагато привабливіше знімати відео про підхід Аліси, ніж про підхід Боба.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2081.24,
  "end": 2092.3
 },
 {
  "input": "For one thing, in Alice's approach, the line of reasoning is fun, it has these nice aha moments.",
  "translatedText": "З одного боку, у підході Аліси міркування веселі, у ньому є такі приємні моменти.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2092.46,
  "end": 2097.12
 },
 {
  "input": "But also, crucially, the way that you explain it is more or less the same for a very wide range of mathematical backgrounds.",
  "translatedText": "Але також, що важливо, спосіб, яким ви це пояснюєте, більш-менш однаковий для дуже широкого діапазону математичних знань.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2097.12,
  "end": 2103.9
 },
 {
  "input": "It's much less enticing to do a video about Bob's approach, not because the computations are all that bad, I mean, they're honestly not, but the pragmatic reality is that the appropriate pace to explain it looks very different depending on the different mathematical backgrounds in the audience.",
  "translatedText": "Набагато менш привабливо знімати відео про підхід Боба не тому, що обчислення такі погані, я маю на увазі, вони, чесно кажучи, ні, а прагматична реальність полягає в тому, що відповідний темп для пояснення виглядає дуже різним залежно від різних математичних фони в аудиторії.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2104.64,
  "end": 2118.86
 },
 {
  "input": "So, you, watching this right now, clearly consume math videos online, and I think in doing so it's worth being aware of this bias.",
  "translatedText": "Отже, ви, дивлячись це прямо зараз, явно споживаєте математичні відео онлайн, і я думаю, що при цьому варто усвідомлювати цю упередженість.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2119.82,
  "end": 2126.62
 },
 {
  "input": "If the aim is to have a genuine lesson on problem solving, too much focus on the slick proofs runs the risk of being disingenuous.",
  "translatedText": "Якщо мета полягає в тому, щоб отримати справжній урок з розв’язання проблем, занадто велика зосередженість на гладких доказах ризикує бути нещирими.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2126.62,
  "end": 2134.52
 },
 {
  "input": "For example, let's say we were to step up to challenge mode here and ask about the case with a closer light source.",
  "translatedText": "Наприклад, скажімо, ми маємо підійти до режиму виклику тут і запитати про випадок із ближчим джерелом світла.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2135.84,
  "end": 2141.02
 },
 {
  "input": "To my knowledge, there is not a similarly slick solution to Alice's here, where you can just relate to a single shape like a sphere.",
  "translatedText": "Наскільки мені відомо, тут немає такого ж витонченого рішення, як у Аліси, де ви можете просто пов’язати одну форму, як-от кулю.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2141.7,
  "end": 2148.16
 },
 {
  "input": "The much more productive warmup to have done would have been the calculus of Bob's approach.",
  "translatedText": "Набагато продуктивнішою розминкою був би розрахунок підходу Боба.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2148.86,
  "end": 2153.3
 },
 {
  "input": "And if you look at the history of this problem, it was proved by Cauchy in 1832, and if we paw through his handwritten notes, they look a lot more similar to Bob's work than Alice's work.",
  "translatedText": "І якщо ви подивитеся на історію цієї проблеми, це було доведено Коші в 1832 році, і якщо ми погортаємо його рукописні нотатки, то вони набагато більше схожі на роботу Боба, ніж на роботу Аліси.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2153.88,
  "end": 2164.48
 },
 {
  "input": "Right here at the top of page 11, you can see what is essentially the same integral that you and I set up in the middle.",
  "translatedText": "Прямо тут, у верхній частині сторінки 11, ви можете побачити, по суті, той самий інтеграл, який ми з вами встановили посередині.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2164.9,
  "end": 2170.4
 },
 {
  "input": "On the other hand, the whole framing of the paper is to find a general fact, not something specific like the case of a cube.",
  "translatedText": "З іншого боку, вся структура статті полягає в тому, щоб знайти загальний факт, а не щось конкретне, як у випадку з кубом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2171.3,
  "end": 2177.24
 },
 {
  "input": "So if we were asking the question which of these two mindsets correlates with the act of discovering new math, the right answer would almost certainly have to be a blend of both.",
  "translatedText": "Отже, якби ми поставили запитання, який із цих двох способів мислення корелює з актом відкриття нової математики, правильна відповідь майже напевно мала б поєднувати обидва.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2177.24,
  "end": 2186.4
 },
 {
  "input": "But I would suggest that many people don't sign enough weight to the part of that blend where you're eager to dive into calculations.",
  "translatedText": "Але я б припустив, що багато людей не надають належної ваги тій частині цієї суміші, де ви прагнете зануритися в розрахунки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2187.22,
  "end": 2194.18
 },
 {
  "input": "And I think there's some risk that the videos I make might contribute to that.",
  "translatedText": "І я думаю, що є певний ризик, що відео, які я знімаю, можуть цьому сприяти.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2194.72,
  "end": 2198.16
 },
 {
  "input": "In the podcast I did with the mathematician Alex Kontorovich, he talked about the often underappreciated importance of just drilling on computations to build intuition, whether you're a student engaging with a new class, or a practicing research mathematician engaging with a new field of study.",
  "translatedText": "У подкасті, який я робив із математиком Алексом Конторовичем, він говорив про те, як часто недооцінюють важливість простого вивчення обчислень для розвитку інтуїції, незалежно від того, чи ви студент, який навчається в новому класі, чи практикуючий математик-дослідник, який вивчає нову сферу знань. вивчення.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2198.96,
  "end": 2214.32
 },
 {
  "input": "A listener actually wrote in to highlight what an impression that particular section made.",
  "translatedText": "Слухач насправді написав, щоб підкреслити, яке враження справив цей розділ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2214.8,
  "end": 2219.04
 },
 {
  "input": "They're a PhD student and describe themselves as being worried that their mathematical abilities were starting to fade, which they attributed to becoming older and less sharp.",
  "translatedText": "Вони є аспірантами та описують себе як стурбовані тим, що їхні математичні здібності починають згасати, що вони пояснюють тим, що вони стають старшими та менш кмітливими.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2219.18,
  "end": 2227.64
 },
 {
  "input": "But hearing a practicing mathematician talk about the importance of doing hundreds of concrete examples in order to learn something new, evidently that changed their perspective.",
  "translatedText": "Але, почувши розмову практикуючого математика про важливість виконання сотень конкретних прикладів, щоб дізнатися щось нове, це, очевидно, змінило їх точку зору.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2227.64,
  "end": 2236.32
 },
 {
  "input": "In their own words, recognizing this completely reshaped their outlook and their results.",
  "translatedText": "За їхніми власними словами, усвідомлення цього повністю змінило їхній світогляд і результати.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2236.9,
  "end": 2241.16
 },
 {
  "input": "And if you look at the famous mathematicians through history, Newton, Euler, Gauss, all of them, they all have this seemingly infinite patience for doing tedious calculations.",
  "translatedText": "І якщо ви подивитеся на відомих математиків в історії, Ньютона, Ейлера, Гаусса, усіх них, усі вони мають це, здавалося б, нескінченне терпіння виконувати нудні обчислення.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2242.02,
  "end": 2250.58
 },
 {
  "input": "The irony of being biased to show insights that let us avoid calculations is that the way people often train up the intuitions to find those insights in the first place is by doing piles and piles of calculations.",
  "translatedText": "Іронія упередженості, щоб показати ідеї, які дозволяють нам уникнути обчислень, полягає в тому, що люди часто тренують інтуїцію, щоб знайти ці ідеї, перш за все, виконуючи купу обчислень.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2250.58,
  "end": 2262.72
 },
 {
  "input": "All that said, something would definitely be missing without the Alice mindset here.",
  "translatedText": "З усього сказаного, без мислення Аліси тут чогось би точно не вистачало.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2264.72,
  "end": 2269.42
 },
 {
  "input": "I mean, think about it, how sad would it be if we solved this problem for a cube, and we never stepped outside of the trees to see the forest and understand that this is a super general fact, it applies to a huge family of shapes.",
  "translatedText": "Я маю на увазі, подумайте про це, як було б сумно, якби ми розв’язали цю задачу для куба, і ми ніколи не виходили за межі дерев, щоб побачити ліс і зрозуміти, що це надзвичайно загальний факт, він стосується величезної родини форми.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2269.98,
  "end": 2280.32
 },
 {
  "input": "And if you consider that math is not just about answering the questions that are posed to you, but about introducing new ideas and constructs, one fun side note about Alice's approach here is that it suggests a fun way to quantify the idea of convexity.",
  "translatedText": "І якщо ви вважаєте, що математика — це не лише відповіді на поставлені перед вами запитання, а й впровадження нових ідей і конструкцій, одна цікава сторона підходу Аліси полягає в тому, що вона пропонує цікавий спосіб кількісного визначення ідеї опуклості.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2281.14,
  "end": 2294.82
 },
 {
  "input": "Rather than just having a yes-no answer, is it convex, is it not, we could put a number to it by saying, consider the average area of the shadow of some solid, multiply that by 4, divide it by the surface area, and if that number is 1, you've got a convex solid, but if it's less than 1, it's non-convex, and how close it is to 1 tells you how close it is to being convex.",
  "translatedText": "Замість того, щоб просто відповісти «так-ні», чи опукло воно, чи ні, ми могли б поставити йому число, сказавши: вважайте середню площу тіні якогось твердого тіла, помножте це на 4, розділіть на площу поверхні , і якщо це число дорівнює 1, ви маєте опукле тіло, але якщо воно менше 1, воно не опукле, і те, наскільки воно близьке до 1, говорить вам, наскільки воно близьке до опуклості.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2295.36,
  "end": 2316.46
 },
 {
  "input": "Also, one of the nice things about the Alice solution here is that it helps explain why it is that mathematicians have what can sometimes look like a bizarre infatuation with generality and with abstraction.",
  "translatedText": "Крім того, одна з приємних речей у розв’язанні Аліси полягає в тому, що воно допомагає пояснити, чому математики мають те, що іноді може виглядати як дивне захоплення загальністю та абстракцією.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2317.1,
  "end": 2328.36
 },
 {
  "input": "The more examples that you see where generalizing and abstracting actually helps you to solve a specific case, the more you start to adopt the same infatuation.",
  "translatedText": "Чим більше ви бачите прикладів, коли узагальнення й абстрагування насправді допомагають вам вирішити конкретну справу, тим більше ви починаєте захоплюватися тим самим.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2328.36,
  "end": 2337.36
 },
 {
  "input": "And as a final thought for the stalwart viewers among you who have stuck through it this far, there is still one unanswered question about the very premise of our puzzle.",
  "translatedText": "І як остання думка для відважних глядачів з-поміж вас, які застрягли через це так далеко, є ще одне запитання без відповіді щодо самої передумови нашої головоломки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2339.24,
  "end": 2347.0
 },
 {
  "input": "What exactly does it mean to choose a random orientation?",
  "translatedText": "Що саме означає вибрати випадкову орієнтацію?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2347.76,
  "end": 2350.94
 },
 {
  "input": "Now if that feels like a silly question, like of course we know what it should mean, I would encourage you to watch a video that I just did with Numberphile on a conundrum from probability known as Bertrand's paradox.",
  "translatedText": "Тепер, якщо це здається дурним запитанням, ми, звісно, знаємо, що воно має означати, я пропоную вам переглянути відео, яке я щойно зняв із Numberphile про загадку ймовірності, відому як парадокс Бертрана.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2350.94,
  "end": 2360.78
 },
 {
  "input": "After you watch it, and if you appreciate some of the nuance at play here, homework for you is to reflect on where exactly Alice and Bob implicitly answer to this question.",
  "translatedText": "Після того, як ви подивіться це, і якщо ви оціните деякі нюанси, що тут діють, домашнє завдання для вас — подумати, де саме Аліса та Боб неявно відповідають на це запитання.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2361.58,
  "end": 2370.42
 },
 {
  "input": "The case with Bob is relatively straightforward, but the point at which Alice locks down some specific distribution on the space of all orientations, well it's not at all obvious, it's actually very subtle.",
  "translatedText": "Випадок із Бобом відносно простий, але момент, у якому Аліса фіксує певний розподіл у просторі всіх орієнтацій, зовсім не очевидний, насправді він дуже тонкий.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 2370.42,
  "end": 2381.7
 }
]