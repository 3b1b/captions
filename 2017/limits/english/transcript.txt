The last several videos have been about the idea of a derivative, and before moving on to integrals I want to take some time to talk about limits. 
To be honest, the idea of a limit is not really anything new. 
If you know what the word approach means you pretty much already know what a limit is. 
You could say it's a matter of assigning fancy notation to the intuitive idea of one value that gets closer to another. 
But there are a few reasons to devote a full video to this topic. 
For one thing, it's worth showing how the way I've been describing derivatives so far lines up with the formal definition of a derivative as it's typically presented in most courses and textbooks. 
I want to give you a little confidence that thinking in terms of dx and df as concrete non-zero nudges is not just some trick for building intuition, it's backed up by the formal definition of a derivative in all its rigor. 
I also want to shed light on what exactly mathematicians mean when they say approach in terms of the epsilon-delta definition of limits. 
Then we'll finish off with a clever trick for computing limits called L'Hopital's rule. 
So, first things first, let's take a look at the formal definition of the derivative. 
As a reminder, when you have some function f of x, to think about its derivative at a particular input, maybe x equals 2, you start by imagining nudging that input some little dx away, and looking at the resulting change to the output, df. 
The ratio df divided by dx, which can be nicely thought of as the rise over run slope between the starting point on the graph and the nudged point, is almost what the derivative is. 
The actual derivative is whatever this ratio approaches as dx approaches 0. 
Just to spell out what's meant there, that nudge to the output df is the difference between f at the starting input plus dx and f at the starting input, the change to the output caused by dx. 
To express that you want to find what this ratio approaches as dx approaches 0, you write lim for limit, with dx arrow 0 below it. 
You'll almost never see terms with a lowercase d like dx inside a limit expression like this. 
Instead, the standard is to use a different variable, something like delta x, or commonly h for whatever reason. 
The way I like to think of it is that terms with this lowercase d in the typical derivative expression have built into them this idea of a limit, the idea that dx is supposed to eventually go to 0. 
In a sense, this left hand side here, df over dx, the ratio we've been thinking about for the past few videos, is just shorthand for what the right hand side here spells out in more detail, writing out exactly what we mean by df, and writing out this limit process explicitly. 
This right hand side here is the formal definition of a derivative, as you would commonly see it in any calculus textbook. 
And if you'll pardon me for a small rant here, I want to emphasize that nothing about this right hand side references the paradoxical idea of an infinitely small change. 
The point of limits is to avoid that. 
This value h is the exact same thing as the dx I've been referencing throughout the series. 
It's a nudge to the input of f with some non-zero, finitely small size, like 0.001. 
It's just that we're analyzing what happens for arbitrarily small choices of h. 
In fact, the only reason people introduce a new variable name into this formal definition, rather than just using dx, is to be extra clear that these changes to the input are just ordinary numbers that have nothing to do with infinitesimals. 
There are others who like to interpret this dx as an infinitely small change, whatever Or to just say that dx and df are nothing more than symbols that we shouldn't take too seriously. 
But by now in the series, you know I'm not really a fan of either of those views. 
I think you can and should interpret dx as a concrete, finitely small nudge, just so long as you remember to ask what happens when that thing approaches 0. 
For one thing, and I hope the past few videos have helped convince you of this, that helps to build stronger intuition for where the rules of calculus actually come from. 
But it's not just some trick for building intuitions. 
Everything I've been saying about derivatives with this concrete, finitely small nudge philosophy is just a translation of this formal definition we're staring at right now. 
Long story short, the big fuss about limits is that they let us avoid talking about infinitely small changes by instead asking what happens as the size of some small change to our variable approaches 0. 
And this brings us to goal number 2, understanding exactly what it means for one value to approach another. 
For example, consider the function 2 plus h cubed minus 2 cubed all divided by h. 
This happens to be the expression that pops out when you unravel the definition of a derivative of x cubed evaluated at x equals 2, but let's just think of it as any old function with an input h. 
Its graph is this nice continuous looking parabola, which would make sense because it's a cubic term divided by a linear term. 
But actually, if you think about what's going on at h equals 0, plugging that in you would get 0 divided by 0, which is not defined. 
So really, this graph has a hole at that point, and you have to exaggerate to draw that hole, often with an empty circle like this. 
But keep in mind, the function is perfectly well defined for inputs as close to 0 as you want. 
Wouldn't you agree that as h approaches 0, the corresponding output, the height of this graph, approaches 12? 
It doesn't matter which side you come at it from. 
That limit of this ratio as h approaches 0 is equal to 12. 
But imagine you're a mathematician inventing calculus, and someone skeptically asks you, well, what exactly do you mean by approach? 
That would be kind of an annoying question, I mean, come on, we all know what it means for one value to get closer to another. 
But let's start thinking about ways you might be able to answer that person, completely unambiguously. 
For a given range of inputs within some distance of 0, excluding the forbidden point 0 itself, look at all of the corresponding outputs, all possible heights of the graph above that range. 
As the range of input values closes in more and more tightly around 0, that range of output values closes in more and more closely around 12. 
And importantly, the size of that range of output values can be made as small as you want. 
As a counter example, consider a function that looks like this, which is also not defined at 0, but kind of jumps up at that point. 
When you approach h equals 0 from the right, the function approaches the value 2, but as you come at it from the left, it approaches 1. 
Since there's not a single clear, unambiguous value that this function approaches as h approaches 0, the limit is not defined at that point. 
One way to think of this is that when you look at any range of inputs around 0, and consider the corresponding range of outputs, as you shrink that input range, the corresponding outputs don't narrow in on any specific value. 
Instead, those outputs straddle a range that never shrinks smaller than 1, even as you make that input range as tiny as you could imagine. 
This perspective of shrinking an input range around the limiting point, and seeing whether or not you're restricted in how much that shrinks the output range, leads to something called the epsilon-delta definition of limits. 
Now I should tell you, you could argue that this is needlessly heavy duty for an introduction to calculus. 
Like I said, if you know what the word approach means, you already know what a limit means, there's nothing new on the conceptual level here. 
But this is an interesting glimpse into the field of real analysis, and gives you a taste for how mathematicians make the intuitive ideas of calculus more airtight and rigorous. 
You've already seen the main idea here. 
When a limit exists, you can make this output range as small as you want, but when the limit doesn't exist, that output range cannot get smaller than some particular value, no matter how much you shrink the input range around the limiting input. 
Let's freeze that same idea a little more precisely, maybe in the context of this example where the limiting value was 12. 
Think about any distance away from 12, where for some reason it's common to use the Greek letter epsilon to denote that distance. 
The intent here is that this distance epsilon is as small as you want. 
What it means for the limit to exist is that you will always be able to find a range of inputs around our limiting point, some distance delta around 0, so that any input within delta of 0 corresponds to an output within a distance epsilon of 12. 
The key point here is that that's true for any epsilon, no matter how small, you'll always be able to find the corresponding delta. 
In contrast, when a limit does not exist, as in this example here, you can find a sufficiently small epsilon, like 0.4, so that no matter how small you make your range around 0, no matter how tiny delta is, the corresponding range of outputs is just always too big. 
There is no limiting output where everything is within a distance epsilon of that output. 
So far, this is all pretty theory-heavy, don't you think? 
Limits being used to formally define the derivative, and epsilons and deltas being used to rigorously define the limit itself. 
So let's finish things off here with a trick for actually computing limits. 
For instance, let's say for some reason you were studying the function sin of pi times x divided by x squared minus 1. 
Maybe this was modeling some kind of dampened oscillation. 
When you plot a bunch of points to graph this, it looks pretty continuous. 
But there's a problematic value at x equals 1. 
When you plug that in, sin of pi is 0, and the denominator also comes out to 0, so the function is actually not defined at that input, and the graph should have a hole there. 
This also happens at x equals negative 1, but let's just focus our attention on a single one of these holes for now. 
The graph certainly does seem to approach a distinct value at that point, wouldn't you say? 
So you might ask, how exactly do you find what output this approaches as x approaches 1, since you can't just plug in 1? 
Well, one way to approximate it would be to plug in a number that's just really close to 1, like 1.00001. 
Doing that, you'd find that this should be a number around negative 1.57. 
But is there a way to know precisely what it is? 
Some systematic process to take an expression like this one, that looks like 0 divided by and ask, what is its limit as x approaches that input? 
After limits, so helpfully let us write the definition for derivatives, derivatives can actually come back here and return the favor to help us evaluate limits. 
Let me show you what I mean. 
Here's what the graph of sin of pi times x looks like, and here's what the graph of x squared minus 1 looks like. 
That's a lot to have up on the screen, but just focus on what's happening around x equals 1. 
The point here is that sin of pi times x and x squared minus 1 are both 0 at that point, they both cross the x axis. 
In the same spirit as plugging in a specific value near 1, like 1.00001, let's zoom in on that point and consider what happens just a tiny nudge dx away from it. 
The value sin of pi times x is bumped down, and the value of that nudge, which was caused by the nudge dx to the input, is what we might call d sin of pi x. 
And from our knowledge of derivatives, using the chain rule, that should be around cosine of pi times x times pi times dx. 
Since the starting value was x equals 1, we plug in x equals 1 to that expression. 
In other words, the amount that this sin of pi times x graph changes is roughly proportional to dx, with a proportionality constant equal to cosine of pi times pi. 
And cosine of pi, if we think back to our trig knowledge, is exactly negative 1, so we can write this whole thing as negative pi times dx. 
Similarly, the value of the x squared minus 1 graph changes by some dx squared minus 1, and taking the derivative, the size of that nudge should be 2x times dx. 
Again, we were starting at x equals 1, so we plug in x equals 1 to that expression, meaning the size of that output nudge is about 2 times 1 times dx. 
What this means is that for values of x which are just a tiny nudge dx away from 1, the ratio sin of pi x divided by x squared minus 1 is approximately negative pi times dx divided by 2 times dx. 
The dx's cancel out, so what's left is negative pi over 2. 
And importantly, those approximations get more and more accurate for smaller and smaller choices of dx, right? 
This ratio, negative pi over 2, actually tells us the precise limiting value as x approaches 1. 
Remember, what that means is that the limiting height on our original graph is evidently exactly negative pi over 2. 
What happened there is a little subtle, so I want to go through it again, but this time a little more generally. 
Instead of these two specific functions, which are both equal to 0 at x equals 1, think of any two functions, f of x and g of x, which are both 0 at some common value, x equals a. 
The only constraint is that these have to be functions where you're able to take a derivative of them at x equals a, which means they each basically look like a line when you zoom in close enough to that value. 
Even though you can't compute f divided by g at this trouble point, since both of them equal 0, you can ask about this ratio for values of x really close to a, the limit as x approaches a. 
It's helpful to think of those nearby inputs as just a tiny nudge, dx, away from a. 
The value of f at that nudged point is approximately its derivative, df over dx, evaluated at a times dx. 
Likewise, the value of g at that nudged point is approximately the derivative of g, evaluated at a times dx. 
Near that trouble point, the ratio between the outputs of f and g is actually about the same as the derivative of f at a times dx, divided by the derivative of g at a times dx. 
Those dx's cancel out, so the ratio of f and g near a is about the same as the ratio between their derivatives. 
Because each of those approximations gets more and more accurate for smaller and smaller nudges, this ratio of derivatives gives the precise value for the limit. 
This is a really handy trick for computing a lot of limits. 
Whenever you come across some expression that seems to equal 0 divided by 0 when you plug in some particular input, just try taking the derivative of the top and bottom expressions and plugging in that same trouble input. 
This clever trick is called L'Hopital's Rule. 
Interestingly, it was actually discovered by Johann Bernoulli, but L'Hopital was this wealthy dude who essentially paid Bernoulli for the rights to some of his mathematical discoveries. 
Academia is weird back then, but in a very literal way, it pays to understand these tiny nudges. 
Right now, you might be remembering that the definition of a derivative for a given function comes down to computing the limit of a certain fraction that looks like 0 divided by 0, so you might think that L'Hopital's Rule could give us a handy way to discover new derivative formulas. 
But that would actually be cheating, since presumably you don't know what the derivative of the numerator is. 
When it comes to discovering derivative formulas, something we've been doing a fair amount this series, there is no systematic plug-and-chug method. 
But that's a good thing! 
Whenever creativity is needed to solve problems like these, it's a good sign that you're doing something real, something that might give you a powerful tool to solve future problems. 
And speaking of powerful tools, up next I'm going to be talking about what an integral is, as well as the fundamental theorem of calculus, another example of where limits can be used to give a clear meaning to a pretty delicate idea that flirts with infinity. 
As you know, most support for this channel comes through Patreon, and the primary perk for patrons is early access to future series like this one, where the next one is going to be on probability. 
But for those of you who want a more tangible way to flag that you're part of the community, there is also a small 3blue1brown store. 
Links on the screen and in the description. 
I'm still debating whether or not to make a preliminary batch of plushie pie creatures, it kinda depends on how many viewers seem interested in the store more generally, but let me know in comments what other kinds of things you'd like to see in there.