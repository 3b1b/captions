[
 {
  "input": "Who doesn't like fractals?",
  "translatedText": "프랙탈을 싫어하는 사람이 있을까요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.04,
  "end": 5.36
 },
 {
  "input": "They're a beautiful blend of simplicity and complexity, often including these infinitely repeating patterns.",
  "translatedText": "무한히 반복되는 패턴을 포함하여 단순함과 복잡함이 아름답게 조화를 이루고 있습니다.",
  "model": "DeepL",
  "from_community_srt": "누가 프랙탈을 좋아하지 않겠습니까? 그것은 단순함과 복잡합의 아름다운 조화이며 흔히 무한하게 반복되는 패턴을 포함하고 있습니다.",
  "n_reviews": 0,
  "start": 5.76,
  "end": 10.6
 },
 {
  "input": "Programmers in particular tend to be especially fond of them, because it takes a shockingly small amount of code to produce images that are way more intricate than any human hand ever could hope to draw.",
  "translatedText": "특히 프로그래머가 특히 선호하는 경향이 있는데, 사람의 손으로 그릴 수 있는 것보다 훨씬 복잡한 이미지를 생성하는 데 놀라울 정도로 적은 양의 코드가 필요하기 때문입니다.",
  "model": "DeepL",
  "from_community_srt": "특히 프로그래머들은 프랙탈을 매우 좋아하는 면이 있는데, 왜냐하면 프랙탈은 보통 아주 작은 양의 프로그램 코드만으로도 사람의 손으로 도저히 그릴 수 없는 엄청나게 복잡한 그림을 그릴 수 있기 때문입니다.",
  "n_reviews": 0,
  "start": 11.42,
  "end": 20.58
 },
 {
  "input": "But a lot of people don't actually know the definition of a fractal, at least not the one Benoit Mandelbrot, the father of fractal geometry, had in mind.",
  "translatedText": "하지만 프랙탈 기하학의 아버지라 불리는 베누아 만델브로가 생각했던 프랙탈의 정의는 물론, 실제로 프랙탈의 정의를 모르는 사람도 많습니다.",
  "model": "DeepL",
  "from_community_srt": "하지만 사실은 많은 사람들은 프랙탈의 정의를 정확히는 모릅니다. 적어도 프랙탈 기하학의 아버지인 Benoit Mandelbrot 가 생각했던 식으로는요.",
  "n_reviews": 0,
  "start": 21.44,
  "end": 28.96
 },
 {
  "input": "A common misconception is that fractals are shapes that are perfectly self-similar.",
  "translatedText": "흔히 프랙탈은 완벽하게 자기 유사성이 있는 도형이라고 오해하는 경우가 많습니다.",
  "model": "DeepL",
  "from_community_srt": "가장 흔한 오해는 프랙탈은 완전히 자기닮음(self-similar)인 모양이라는 것입니다.",
  "n_reviews": 0,
  "start": 29.66,
  "end": 33.3
 },
 {
  "input": "For example, this snowflake-looking shape right here, called the Von Koch snowflake, consists of three different segments, and each one of these is perfectly self-similar, in that when you zoom in on it, you get a perfectly identical copy of the original.",
  "translatedText": "예를 들어, 폰 코흐 눈송이라고 불리는 이 눈송이 모양은 세 개의 서로 다른 세그먼트로 구성되어 있으며, 각 세그먼트는 확대하면 원본과 완벽하게 동일한 복사본을 얻을 수 있다는 점에서 완벽하게 자체 유사합니다.",
  "model": "DeepL",
  "from_community_srt": "예를 들어, 여기 있는 눈송이처럼 생긴 모양은 코흐 눈송이라고 불리는데 이것은 세 부분으로 이루어져 있고 이들은 완전히 자기닮음입니다. 이들을 확대해 보면,",
  "n_reviews": 0,
  "start": 34.12,
  "end": 47.46
 },
 {
  "input": "Likewise, the famous Sierpinski triangle consists of three smaller identical copies of itself.",
  "translatedText": "마찬가지로 유명한 시에르핀스키 삼각형은 세 개의 작은 동일한 사본으로 구성되어 있습니다.",
  "model": "DeepL",
  "from_community_srt": "당신은 완전히 원본과 같은 모양을 볼 수 있습니다. 다른 예로, 유명한 시에르핀스키 삼각형은 원본과 완전히 같은 모양 세개로 이루어집니다. 덧붙여,",
  "n_reviews": 0,
  "start": 49.52,
  "end": 54.36
 },
 {
  "input": "And don't get me wrong, self-similar shapes are definitely beautiful, and they're a good toy model for what fractals really are.",
  "translatedText": "오해하지 마세요. 자체적으로 유사한 모양은 분명 아름답고 프랙탈이 실제로 무엇인지에 대한 좋은 장난감 모델입니다.",
  "model": "DeepL",
  "from_community_srt": "오해하지 마시길 바랍니다. 자기닮음인 모양들은 확실히 아름답습니다. 그런 모양들은 프랙탈에 대한 좋은  예시이죠.",
  "n_reviews": 0,
  "start": 55.04,
  "end": 60.78
 },
 {
  "input": "But Mandelbrot had a much broader conception in mind, one motivated not by beauty, but more by a pragmatic desire to model nature in a way that actually captures roughness.",
  "translatedText": "하지만 만델브로트는 아름다움보다는 자연을 실제로 거칠게 표현하는 방식으로 모델링하려는 실용적인 욕구에 더 큰 동기를 부여한 훨씬 더 광범위한 개념을 염두에 두고 있었습니다.",
  "model": "DeepL",
  "from_community_srt": "하지만 Mandelbrot 가 생각한 프랙탈은 그보다 훨씬 넓은 의미를 가집니다. 아름다움보다는 자연의 거친 면을 수학적으로 표현하려는 좀 더 실용적인 목적에서 말이죠.",
  "n_reviews": 0,
  "start": 61.1,
  "end": 70.26
 },
 {
  "input": "In some ways, fractal geometry is a rebellion against calculus, whose central assumption is that things tend to look smooth if you zoom in far enough.",
  "translatedText": "어떤 면에서 프랙탈 기하학은 충분히 확대하면 사물이 매끄럽게 보인다는 중심 가정을 가진 미적분에 대한 반란입니다.",
  "model": "DeepL",
  "from_community_srt": "어떤 면에서는 프랙탈 기하학은 미적분학에 대한 반란이기도 합니다. 왜냐하면 미적분학의 \"어떤 불연속적인 모양도 확대한다면 연속적인 모양이 된다\"는 가정을 프랙탈 기하학은 전면으로 부정하고 있으니까요.",
  "n_reviews": 0,
  "start": 72.38,
  "end": 79.72
 },
 {
  "input": "But Mandelbrot saw this as overly idealized, or at least needlessly idealized, resulting in models that neglect the finer details of the thing they're actually modeling, which can matter.",
  "translatedText": "하지만 만델브로트는 이러한 방식이 지나치게 이상적이거나 적어도 불필요하게 이상화되어 실제로 모델링하는 대상의 세세한 디테일을 무시하는 모델을 만들 수 있다고 생각했습니다.",
  "model": "DeepL",
  "from_community_srt": "Mandelbrot는 이러한 가정이 과도하게 혹은 불필요하게 이상적이라고 봤습니다. 사물의 미세한 디테일을 무시하는 모델을 만드는 결과를 낳을 수 있는 이상성이라고 본 것입니다.",
  "n_reviews": 0,
  "start": 80.32,
  "end": 90.58
 },
 {
  "input": "What he observed is that self-similar shapes give a basis for modeling the regularity in some forms of roughness, but the popular perception that fractals only include perfectly self-similar shapes is another over-idealization, one that ironically goes against the pragmatic spirit of fractal geometry's origins.",
  "translatedText": "그가 관찰한 바에 따르면 자기 유사 도형은 어떤 형태로든 규칙성을 모델링하는 데 기초가 되지만, 프랙탈이 완벽하게 자기 유사 도형만 포함한다는 대중의 인식은 또 다른 지나친 이상화이며, 이는 프랙탈 기하학의 기원이 된 실용주의 정신에 역설적으로 반하는 것이기도 합니다.",
  "model": "DeepL",
  "from_community_srt": "사실은 그 디테일이 중요할 수도 있는데 말입니다. 그가 관찰했던 것은 자기 닮음이 어떤 종류의 거친 면 안에 있는 규칙성을 모델링 하는데에 기본이 될 수 있다는 점입니다. 그러나 프랙탈이 무조건 자기 닮음인 모양만 갖는 다는 흔한 인식은 또 다른 지나친 이상성입니다. 프랙탈이 본래 가지고 있던 실용성에 대해 반하는 아이러니한 점이죠.",
  "n_reviews": 0,
  "start": 92.04,
  "end": 109.02
 },
 {
  "input": "The real definition of fractals has to do with this idea of fractal dimension, the main topic of this video.",
  "translatedText": "프랙탈의 진정한 정의는 이 비디오의 주요 주제인 프랙탈 차원에 대한 개념과 관련이 있습니다.",
  "model": "DeepL",
  "from_community_srt": "프랙탈의 진짜 정의는 프랙탈 차원이라는 아이디어와 관련이 있습니다. 이 비디오의 주제이죠.",
  "n_reviews": 0,
  "start": 109.64,
  "end": 115.4
 },
 {
  "input": "You see, there is a sense, a certain way to define the word dimension, in which the Sierpinski triangle is approximately 1.585D, that the Von Koch curve is approximately 1.262D.",
  "translatedText": "시에르핀스키 삼각형은 약 1.585D, 폰 코흐 곡선은 약 1.262D라는 식으로 차원이라는 단어를 정의할 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "차원이라는 말을 정의하는 방법이 있습니다. 이 방법으로 시에르핀스키 삼각형의 차원은 약 1.585 차원이고, 또한 코흐 곡선의 차원은 약 1.262 차원이며.",
  "n_reviews": 0,
  "start": 116.32,
  "end": 128.66
 },
 {
  "input": "The coastline of Britain turns out to be around 1.21D, and in general it's possible to have shapes whose dimension is any positive real number, not just whole numbers.",
  "translatedText": "영국의 해안선은 약 1.21D로 밝혀졌으며, 일반적으로 정수가 아닌 양의 실수인 도형을 가질 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "브리튼 섬의 해안선은 약 1.21 차원이 됩니다. 그리고 일반적으로, 어떤 모양이든지 정수 차원 뿐만 아니라 양의 실수 차원을 갖는 것도 가능합니다.",
  "n_reviews": 0,
  "start": 129.58,
  "end": 138.88
 },
 {
  "input": "I think when I first heard someone reference fractional dimension like this, I just thought it was nonsense, right?",
  "translatedText": "처음 누군가가 이런 식으로 분수 차원을 언급하는 것을 들었을 때는 말도 안 되는 소리라고 생각했었죠?",
  "model": "DeepL",
  "from_community_srt": "사실 제가 처음 이렇게 프랙탈 차원에 관한 이야기를 들었을때는 정말 헛소리라고 생각했습니다. 그렇죠?",
  "n_reviews": 0,
  "start": 142.14,
  "end": 147.56
 },
 {
  "input": "I mean, mathematicians are clearly just making stuff up.",
  "translatedText": "수학자들은 분명히 무언가를 지어낸 것일 뿐입니다.",
  "model": "DeepL",
  "from_community_srt": "제말은, 수학자들이 분명히 지어내고 있다고 생각했습니다.",
  "n_reviews": 0,
  "start": 147.68,
  "end": 150.12
 },
 {
  "input": "Dimension is something that usually only makes sense for natural numbers, right?",
  "translatedText": "차원은 보통 자연수에게만 의미가 있는 것이죠?",
  "model": "DeepL",
  "from_community_srt": "차원은 그냥 자연수의 값을 가질때만 성립하는 것이죠,",
  "n_reviews": 0,
  "start": 150.48,
  "end": 153.78
 },
 {
  "input": "A line is one-dimensional, a plane that's two-dimensional, the space that we live in that's three-dimensional, and so on.",
  "translatedText": "선은 1차원, 평면은 2차원, 우리가 살고 있는 공간은 3차원 등 모든 것이 1차원입니다.",
  "model": "DeepL",
  "from_community_srt": "그렇죠? 선은 1차원이고, 면은 2차원이며, 우리가 살고 있는 이세상은 3차원입니다.",
  "n_reviews": 0,
  "start": 154.08,
  "end": 159.86
 },
 {
  "input": "And in fact, any linear algebra student who just learned the formal definition of dimension in that context would agree, it only makes sense for counting numbers.",
  "translatedText": "사실 이러한 맥락에서 차원의 공식적인 정의를 배운 선형대수학 학생이라면 누구나 동의하겠지만, 이는 숫자를 세는 데만 의미가 있습니다.",
  "model": "DeepL",
  "from_community_srt": "그런 거죠. 사실, 선형 대수학을 공부하고 있으며 일반적인 차원의 정의를 배운 학생들은 동의를 할 겁니다. 그것은 오직 정수에서만 성립합니다.",
  "n_reviews": 0,
  "start": 160.36,
  "end": 168.3
 },
 {
  "input": "And of course, the idea of fractal dimension is just made up.",
  "translatedText": "물론 프랙탈 차원이라는 개념은 그냥 만들어낸 것입니다.",
  "model": "DeepL",
  "from_community_srt": "또 물론 프랙탈 차원에 대한 아이디어는 그저 만들어진 개념입니다.",
  "n_reviews": 0,
  "start": 169.2,
  "end": 172.52
 },
 {
  "input": "I mean, this is math, everything's made up.",
  "translatedText": "이건 수학이고, 모든 것이 만들어져 있습니다.",
  "model": "DeepL",
  "from_community_srt": "제 말은, 이것은 수학이고, 수학은 만들어진 학문이라는 겁니다.",
  "n_reviews": 0,
  "start": 172.82,
  "end": 174.64
 },
 {
  "input": "But the question is whether or not it turns out to be a useful construct for modeling the world.",
  "translatedText": "하지만 문제는 그것이 세계를 모델링하는 데 유용한 구조로 밝혀졌는지 여부입니다.",
  "model": "DeepL",
  "from_community_srt": "하지만 논점은 그것이 만들어졌든 아니든 그것은 세계를 모델링하는데에 유용합니다.",
  "n_reviews": 0,
  "start": 175.08,
  "end": 179.46
 },
 {
  "input": "And I think you'll agree, once you learn how fractal dimension is defined, it's something that you start seeing almost everywhere that you look.",
  "translatedText": "프랙탈 차원이 어떻게 정의되는지 알게 되면 거의 모든 곳에서 프랙탈 차원을 볼 수 있게 됩니다.",
  "model": "DeepL",
  "from_community_srt": "그리고 제가 보기에 당신은 동의할 겁니다. 당신이 일단 프랙탈차원이 어떻게 정의되었는지 배운다면 말이죠. 그것은 당신이  어디를 보더라도 그것의 프랙탈 차원을 보게 되는 것입니다.",
  "n_reviews": 0,
  "start": 180.1,
  "end": 186.3
 },
 {
  "input": "It actually helps to start the discussion here by only looking at perfectly self-similar shapes.",
  "translatedText": "완벽하게 자체적으로 유사한 모양만 살펴보는 것으로 논의를 시작하는 것이 실제로 도움이 됩니다.",
  "model": "DeepL",
  "from_community_srt": "사실 오직 자기닮음인 모양만 보는 것이 이야기를 시작하는 것에 도움을 줍니다.",
  "n_reviews": 0,
  "start": 188.66,
  "end": 193.26
 },
 {
  "input": "In fact, I'm going to start with four shapes, the first three of which aren't even fractals.",
  "translatedText": "사실 처음 세 개는 프랙탈도 아닌 도형 네 개로 시작하겠습니다.",
  "model": "DeepL",
  "from_community_srt": "사실 저는 4가지 모양을 가지고 시작하려고 합니다. 그중 처음부터 3가지의 모양은 아예 프랙탈도 아닙니다.",
  "n_reviews": 0,
  "start": 193.94,
  "end": 197.64
 },
 {
  "input": "A line, a square, a cube, and a Sierpinski triangle.",
  "translatedText": "선, 정사각형, 정육면체, 시에르핀스키 삼각형이 있습니다.",
  "model": "DeepL",
  "from_community_srt": "바로 선, 정사각형, 정육면체, 그리고 시에르핀스키 삼각형입니다.",
  "n_reviews": 0,
  "start": 198.1,
  "end": 201.72
 },
 {
  "input": "All of these shapes are self-similar.",
  "translatedText": "이 모든 모양은 자체적으로 유사합니다.",
  "model": "DeepL",
  "from_community_srt": "이 모든 모양들은 자기 닮음입니다.",
  "n_reviews": 0,
  "start": 202.6,
  "end": 204.2
 },
 {
  "input": "A line can be broken up into two smaller lines, each of which is a perfect copy of the original, just scaled down by a half.",
  "translatedText": "한 줄을 두 개의 작은 줄로 나눌 수 있으며, 각 줄은 원본의 완벽한 복사본으로 절반으로 축소할 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "선은 두 개의 더 작은 선으로 분해될 수 있고, 그것은 완벽하게 길이만 1/2로 줄어든 복사본입니다.",
  "n_reviews": 0,
  "start": 204.7,
  "end": 210.96
 },
 {
  "input": "A square can be broken down into four smaller squares, each of which is a perfect copy of the original, just scaled down by a half.",
  "translatedText": "정사각형을 네 개의 작은 정사각형으로 나눌 수 있으며, 각 정사각형은 원본의 완벽한 사본으로 절반으로 축소할 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "정사각형은 네 개의 더 작은 정사각형으로 분해될 수 있고, 그것은 완벽하게 한 변의 길이만 1/2로 줄어든 복사본입니다.",
  "n_reviews": 0,
  "start": 211.54,
  "end": 218.34
 },
 {
  "input": "Likewise, a cube can be broken down into eight smaller cubes, again, each one is a scaled down version by one half.",
  "translatedText": "마찬가지로 정육면체도 8개의 작은 정육면체로 나눌 수 있으며, 각 정육면체는 절반으로 축소된 버전입니다.",
  "model": "DeepL",
  "from_community_srt": "마찬가지로, 정사각형은 여덟 개의 더 작은  정육면체로 분해될 수 있으며, 다시 한 모서리의 길이가 1/2가 된 복사본이 되죠.",
  "n_reviews": 0,
  "start": 220.0,
  "end": 225.2
 },
 {
  "input": "And the core characteristic of the Sierpinski triangle is that it's made of three smaller copies of itself, and the length of the side of one of those smaller copies is one half the side length of the original triangle.",
  "translatedText": "시에르핀스키 삼각형의 핵심 특징은 삼각형 자체가 세 개의 작은 사본으로 이루어져 있고, 그 작은 사본 중 하나의 변의 길이가 원래 삼각형의 변 길이의 절반이라는 점입니다.",
  "model": "DeepL",
  "from_community_srt": "그리고 시에르핀스키의 삼각형의 주된 특징은 그것이 세 개의 복사본으로 이루어져 있다는 것인데, 그 복사본의 한 변의 길이는 원본의 1/2입니다.",
  "n_reviews": 0,
  "start": 226.02,
  "end": 236.5
 },
 {
  "input": "Now, it's fun to compare how we measure these things.",
  "translatedText": "이제 이러한 것들을 측정하는 방법을 비교해보는 것도 재미있을 것입니다.",
  "model": "DeepL",
  "from_community_srt": "어떻게 측정하는 건지를  비교하는 것은 흥미롭습니다.",
  "n_reviews": 0,
  "start": 238.2,
  "end": 240.36
 },
 {
  "input": "We'd say that the smaller line is one half the length of the original line, the smaller square is one quarter the area of the original square, the smaller cube is one eighth the volume of the original cube, and that smaller Sierpinski triangle?",
  "translatedText": "작은 선은 원래 선의 길이의 절반, 작은 정사각형은 원래 정사각형의 면적의 1/4, 작은 정육면체는 원래 정육면체의 부피의 1/8, 그리고 작은 시에르핀스키 삼각형은 원래 삼각형의 부피의 1/8이라고 할 수 있겠죠?",
  "model": "DeepL",
  "from_community_srt": "작은 선의 길이는 원본의 1/2 이고, 작은 정사각형의 넓이는 원본의 1/4 이고, 작은 정육면체의 부피는 원본의 1/8 , 그리고 작은 sierpinski triangle은...",
  "n_reviews": 0,
  "start": 240.62,
  "end": 253.84
 },
 {
  "input": "Well, we'll talk about how to measure that in just a moment.",
  "translatedText": "이를 측정하는 방법에 대해서는 잠시 후에 설명하겠습니다.",
  "model": "DeepL",
  "from_community_srt": "이걸 어떻게 측정하는지는 잠시 후에  알아봅시다.",
  "n_reviews": 0,
  "start": 254.44,
  "end": 256.8
 },
 {
  "input": "What I want is a word that generalizes the idea of length, area, and volume, but that I can apply to all of those shapes and more.",
  "translatedText": "제가 원하는 것은 길이, 면적, 부피에 대한 개념을 일반화하면서도 이러한 모든 도형 등에 적용할 수 있는 단어입니다.",
  "model": "DeepL",
  "from_community_srt": "제가 원하는 것은 길이, 넓이, 부피의 개념을 일반화하며 이 모두에게 적용할 수 있는 단어입니다.",
  "n_reviews": 0,
  "start": 258.2,
  "end": 264.96
 },
 {
  "input": "And typically in math, the word that you'd use for this is measure, but I think it might be more intuitive to talk about mass, as in, imagine that each of these shapes is made out of metal, a thin wire, a flat sheet, a solid cube, and some kind of Sierpinski mesh.",
  "translatedText": "일반적으로 수학에서는 측정이라는 단어를 사용하지만, 이 각각의 모양이 금속, 가는 철사, 평평한 시트, 단단한 입방체, 일종의 시에르핀스키 메쉬로 만들어졌다고 상상해 보면 질량이라고 말하는 것이 더 직관적일 수 있을 것 같습니다.",
  "model": "DeepL",
  "from_community_srt": "수학에서는 이 단어를 \"Measure\"이라고 합니다. 하지만 저는 \"mass\"라고 말하는 것이 더욱 직관적이라고 생각합니다. 이 모든 도형들이 금속으로 만들어져 있다고 생각합시다. 얇은 철사, 평평한 판, 고체 덩어리, 그리고 Sierpinski 철조망 정도로 생각합시다.",
  "n_reviews": 0,
  "start": 265.7,
  "end": 279.48
 },
 {
  "input": "Fractal dimension has everything to do with understanding how the mass of these shapes changes as you scale them.",
  "translatedText": "프랙탈 차원은 도형의 크기를 조정할 때 질량이 어떻게 변하는지를 이해하는 것과 관련이 있습니다.",
  "model": "DeepL",
  "from_community_srt": "프랙탈 차원은  이 도형들의 규모에 따른 mass(거듭하여 본래 모습이 되는 수치)가 어떻게 변화하는지에 대한 이해에 있습니다.",
  "n_reviews": 0,
  "start": 280.46,
  "end": 286.3
 },
 {
  "input": "The benefit of starting the discussion with self-similar shapes is that it gives us a nice clear-cut way to compare masses.",
  "translatedText": "자체적으로 유사한 모양으로 논의를 시작하면 대중을 명확하게 비교할 수 있는 좋은 방법이 된다는 장점이 있습니다.",
  "model": "DeepL",
  "from_community_srt": "자기-닮음 도형으로 시작하는 장점은 질량을 깔끔한 방법으로 비교할 수 있다는 것입니다.",
  "n_reviews": 0,
  "start": 287.28,
  "end": 292.86
 },
 {
  "input": "When you scale down that line by one half, the mass is also scaled down by one half, which you can viscerally see because it takes two copies of that smaller one to form the whole.",
  "translatedText": "이 선을 절반으로 축소하면 질량도 절반으로 축소되는데, 작은 선의 복사본 두 개가 있어야 전체가 형성되므로 직관적으로 알 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "이 선의 규모를 1/2로 줄이면 mass 또한 1/2이 됩니다. 원본을 만들기 위해서는 복사본 2개가 필요하기 때문에 누구나 직관적으로 알 수 있습니다.",
  "n_reviews": 0,
  "start": 295.04,
  "end": 305.28
 },
 {
  "input": "When you scale down a square by one half, its mass is scaled down by one fourth, where again you can see this by piecing together four of the smaller copies to get the original.",
  "translatedText": "정사각형을 반으로 축소하면 질량이 4분의 1로 줄어들고, 다시 작은 사본 4개를 합쳐 원본을 얻으면 이를 확인할 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "이 정사각형의 규모를 1/2로 줄이면 mass는 1/4배 가 됩니다.",
  "n_reviews": 0,
  "start": 307.18,
  "end": 315.26
 },
 {
  "input": "Likewise, when you scale down that cube by one half, the mass is scaled down by one eighth, or one half cubed, because it takes eight copies of that smaller cube to rebuild the original.",
  "translatedText": "마찬가지로 정육면체를 절반으로 축소하면 원본을 재구성하는 데 작은 정육면체의 복사본 8장이 필요하므로 질량은 8분의 1, 즉 절반의 정육면체로 축소됩니다.",
  "model": "DeepL",
  "from_community_srt": "이것은 4개의 복사본을 모으는 것으로 원본을 만들어 알 수 있습니다.(mass의 정의) 같은 방법으로 이 정육면체의 규모를 1/2로 줄이면 mass는 1/8배, 즉 (1/2)^3 가 됩니다.",
  "n_reviews": 0,
  "start": 319.28,
  "end": 328.84
 },
 {
  "input": "And when you scale down the Sierpinski triangle by a factor of a half, wouldn't you agree that it makes sense to say that its mass goes down by a factor of one third?",
  "translatedText": "그리고 시에르핀스키 삼각형을 절반으로 축소하면 질량이 1/3로 줄어든다고 말하는 것이 합리적이지 않을까요?",
  "model": "DeepL",
  "from_community_srt": "원본을 만들기 위해 8개의 복사본이 필요하기 때문입니다.(mass의 정의) 그리고 이 Sierpinski triangle의 규모를 1/2로 줄이면 mass는 1/3배가 된다고 생각할 것입니다.",
  "n_reviews": 0,
  "start": 331.12,
  "end": 338.68
 },
 {
  "input": "I mean, it takes exactly three of those smaller ones to form the original.",
  "translatedText": "원본을 만들려면 정확히 세 개의 작은 그림이 필요합니다.",
  "model": "DeepL",
  "from_community_srt": "원본을 만들기 위해 3개의 복사본이 필요하기 때문입니다.",
  "n_reviews": 0,
  "start": 339.24,
  "end": 342.62
 },
 {
  "input": "But notice that for the line, the square, and the cube, the factor by which the mass changed is this nice clean integer power of one half.",
  "translatedText": "하지만 선, 정사각형, 정육면체의 경우 질량 변화의 요인이 절반의 깨끗한 정수 거듭제곱이라는 것을 알 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "하지만 선, 정사각형, 그리고 정육면체는 mas가 모두 정확하게 1/2의 거듭제곱 배가 되었다는 점에 주목하십시오.",
  "n_reviews": 0,
  "start": 345.18,
  "end": 352.1
 },
 {
  "input": "In fact, that exponent is the dimension of each shape.",
  "translatedText": "사실 이 지수는 각 도형의 치수입니다.",
  "model": "DeepL",
  "from_community_srt": "사실, 여기서 지수는 각 도형의 차원에 해당합니다.",
  "n_reviews": 0,
  "start": 353.98,
  "end": 357.04
 },
 {
  "input": "And what's more, you could say that what it means for a shape to be, for example, two-dimensional, what puts the two in two-dimensional, is that when you scale it by some factor, its mass is scaled by that factor raised to the second power.",
  "translatedText": "또한, 예를 들어 도형이 2차원이라는 의미는 도형을 2차원으로 만드는 것은 어떤 계수로 스케일링할 때 그 질량이 그 계수의 거듭제곱만큼 스케일링된다는 의미라고 말할 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "중요한 것은 어떤 도형의 차원이, 예를 들어 2차원이라고 할 때, 그 \"2\"가 의미하는 바가 무엇이냐는 겁니다. \"2\"차원의 2는 이 도형의 규모를 s배 하였을 때 mass는 s의 \"2\"제곱 배가 된다는 뜻입니다.",
  "n_reviews": 0,
  "start": 359.58,
  "end": 376.64
 },
 {
  "input": "And maybe what it means for a shape to be three-dimensional is that when you scale it by some factor, the mass is scaled by the third power of that factor.",
  "translatedText": "도형이 3차원적이라는 의미는 어떤 계수에 따라 크기를 조정할 때 질량이 해당 계수의 세 번째 거듭 제곱으로 조정된다는 뜻일 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "그리고 어떤 도형이 3차원이라는 것은 그 도형의 규모를 s배 하였을 때 그 massd의 s의 \"3\"제곱 배가 된다는 뜻이지요.",
  "n_reviews": 0,
  "start": 379.1,
  "end": 387.26
 },
 {
  "input": "So if this is our conception of dimension, what should the dimensionality of a Sierpinski triangle be?",
  "translatedText": "그렇다면 이것이 우리의 차원 개념이라면 시에르핀스키 삼각형의 차원은 무엇일까요?",
  "model": "DeepL",
  "from_community_srt": "이것이  우리의 차원의  개념이라면, Sierpinski triangle은 몇차원 일까요?",
  "n_reviews": 0,
  "start": 391.64,
  "end": 396.76
 },
 {
  "input": "You'd want to say that when you scale it down by a factor of one half, its mass goes down by one half to the power of whatever its dimension is.",
  "translatedText": "크기를 절반으로 축소하면 질량이 그 크기의 절반으로 줄어든다고 말할 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "도형의 규모를 1/2배  했을 때 도형의 mass는 1/2의... (그게 무슨 숫자든 간에) \"D\" 제곱 배가 될 것입니다.",
  "n_reviews": 0,
  "start": 398.36,
  "end": 406.12
 },
 {
  "input": "And because it's self-similar, we know that we want its mass to go down by a factor of one third.",
  "translatedText": "그리고 자체적으로 유사하기 때문에 질량을 3분의 1로 줄이기를 원한다는 것을 알고 있습니다.",
  "model": "DeepL",
  "from_community_srt": "그러나 자기-닮음 도형이기  때문에, 우리는 mass의 변화가 1/3임을 알고 있습니다.",
  "n_reviews": 0,
  "start": 406.72,
  "end": 410.84
 },
 {
  "input": "So what's the number d such that raising one half to the power of d gives you one third?",
  "translatedText": "그렇다면 절반을 d의 거듭제곱으로 올리면 3분의 1이 되는 숫자 d는 무엇일까요?",
  "model": "DeepL",
  "from_community_srt": "그렇다면 1/2의 D제곱이 1/3이 되는 D는 무엇일까요? 이것은 2의 몇제곱이 3이 되냐는",
  "n_reviews": 0,
  "start": 412.78,
  "end": 418.22
 },
 {
  "input": "Well, that's the same as asking two to the what equals three, the quintessential type of question that logarithms are meant to answer.",
  "translatedText": "이는 대수가 대답해야 하는 전형적인 유형의 질문인 '2가 3과 같은 것은 무엇인가'를 묻는 것과 같습니다.",
  "model": "DeepL",
  "from_community_srt": "질문과 같습니다. 바로 log(로그)를 통해 푸는 문제입니다.",
  "n_reviews": 0,
  "start": 419.4,
  "end": 425.46
 },
 {
  "input": "And when you go and plug in log base two of three to a calculator, what you'll find is that it's about 1.585.",
  "translatedText": "로그베이스 3 중 2를 계산기에 대면 약 1.585가 나온다는 것을 알 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "그리고 계산기에 대입을 하면 D는 약 1.585 라는 것을 알 수 있습니다.",
  "n_reviews": 0,
  "start": 425.98,
  "end": 432.18
 },
 {
  "input": "So in this way, the Sierpinski triangle is not one-dimensional, even though you could define a curve that passes through all its points, and nor is it two-dimensional, even though it lives in the plane.",
  "translatedText": "이런 식으로 시에르핀스키 삼각형은 모든 점을 통과하는 곡선을 정의할 수 있지만 1차원이 아니며, 평면에 존재하지만 2차원도 아닙니다.",
  "model": "DeepL",
  "from_community_srt": "이런 식으로, 이  Sierpinski triangle의 모든 점을 지나가는 곡선 하나를 정의할 수 있지만, 1차원은 아니며, 평면에 존재하지만,",
  "n_reviews": 0,
  "start": 433.4,
  "end": 443.46
 },
 {
  "input": "Instead, it's 1.585 dimensional.",
  "translatedText": "대신 1.585 차원입니다.",
  "model": "DeepL",
  "from_community_srt": "2차원 역시 아님을 알 수 있습니다.",
  "n_reviews": 0,
  "start": 444.18,
  "end": 446.46
 },
 {
  "input": "And if you want to describe its mass, neither length nor area seem like the fitting notions.",
  "translatedText": "그리고 질량을 설명하려면 길이도 면적도 적합한 개념이 아닌 것 같습니다.",
  "model": "DeepL",
  "from_community_srt": "대신 이 도형의 차원은 1.585 차원인 것입니다. 이 도형의 질량을 설명하기 위해서는 길이와 넓이 모두 적절한 개념이 아니라는 것을 알 수 있습니다.",
  "n_reviews": 0,
  "start": 447.24,
  "end": 451.72
 },
 {
  "input": "If you tried, its length would turn out to be infinite, and its area would turn out to be zero.",
  "translatedText": "시도해 보면 그 길이는 무한대이고 면적은 0이 될 것입니다.",
  "model": "DeepL",
  "from_community_srt": "당신이 만약 시도해보았다면 길이는 무한대로 나왔을 것이고, 넓이는 0이 나왔을 것입니다.",
  "n_reviews": 0,
  "start": 452.34,
  "end": 456.62
 },
 {
  "input": "Instead, what you want is whatever the 1.585 dimensional analog of length is.",
  "translatedText": "대신 원하는 것은 길이의 1.585 차원 아날로그가 무엇이든 상관없습니다.",
  "model": "DeepL",
  "from_community_srt": "우리가 원하는 것은 무엇이 되었든지 길이와 유사한 \"1.585차원의 값\"입니다.",
  "n_reviews": 0,
  "start": 458.92,
  "end": 464.12
 },
 {
  "input": "Here, let's look at another self-similar fractal, the von Koch curve.",
  "translatedText": "이제 또 다른 자기 유사 프랙탈인 폰 코흐 곡선을 살펴보겠습니다.",
  "model": "DeepL",
  "from_community_srt": "다른 자기닮은 도형으로 코흐 곡선을 봅시다.",
  "n_reviews": 0,
  "start": 465.82,
  "end": 469.12
 },
 {
  "input": "This one is composed of four smaller identical copies of itself, each of which is a copy of the original scaled down by one third.",
  "translatedText": "이 사본은 원본을 1/3로 축소한 사본으로, 각각 4개의 작은 동일한 사본으로 구성됩니다.",
  "model": "DeepL",
  "from_community_srt": "이 도형은 4개의 복사본으로 이루어져 있는데, 하나의 복사본은 원본 규모의 1/3입니다.",
  "n_reviews": 0,
  "start": 469.84,
  "end": 476.72
 },
 {
  "input": "So the scaling factor is one third, and the mass has gone down by a factor of one fourth.",
  "translatedText": "따라서 스케일링 계수는 3분의 1이 되고 질량은 4분의 1로 감소했습니다.",
  "model": "DeepL",
  "from_community_srt": "따라서 scaling factor (규모 계수 정도로 해석할 수 있겠죠) 는 1/3이고, mass는 원본의 1/4배 되었습니다.",
  "n_reviews": 0,
  "start": 477.58,
  "end": 481.72
 },
 {
  "input": "So that means the dimension should be some number D, so that when we raise one third to the power of D, it gives us one fourth.",
  "translatedText": "즉, 차원은 어떤 숫자 D여야 하며, 3분의 1을 D의 거듭제곱으로 올리면 4분의 1이 됩니다.",
  "model": "DeepL",
  "from_community_srt": "이것은 차원의 값이 \"1/3을 D 제곱했을 때 1/4이 되는\" 특정 숫자 D가 되어야 함을 의미합니다.",
  "n_reviews": 0,
  "start": 483.92,
  "end": 491.16
 },
 {
  "input": "Well, that's the same as saying three to the what equals four, so you can go and plug into a calculator log base three of four, and that comes out to be around 1.262.",
  "translatedText": "3을 4와 같다고 말하는 것과 같으므로 4의 3을 계산기에 대입하면 약 1.262가 나옵니다.",
  "model": "DeepL",
  "from_community_srt": "그것은 3을 몇 제곱해야 4가 되는지 묻는 것과 똑같죠. 이제 계산기에 숫자를 넣어 계산하면 약 1.262가 나옵니다.",
  "n_reviews": 0,
  "start": 492.38,
  "end": 501.86
 },
 {
  "input": "So in a sense, the von Koch curve is a 1.262 dimensional shape.",
  "translatedText": "따라서 어떤 의미에서 폰 코흐 곡선은 1.262 차원의 도형입니다.",
  "model": "DeepL",
  "from_community_srt": "따라서 이 고흐 곡선은 1.262차원을 가진 도형입니다.",
  "n_reviews": 0,
  "start": 502.8,
  "end": 507.46
 },
 {
  "input": "Here's another fun one.",
  "translatedText": "여기 또 다른 재미있는 것이 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 509.5,
  "end": 510.28
 },
 {
  "input": "This is kind of the right angled version of the Koch curve.",
  "translatedText": "이것은 일종의 코흐 곡선의 직각 버전입니다.",
  "model": "DeepL",
  "from_community_srt": "재밌는 사실이 또 있습니다. 이것은 말하자면 \"직각 버전의 코흐 곡선\"입니다.",
  "n_reviews": 0,
  "start": 510.6,
  "end": 513.5
 },
 {
  "input": "It's built up of eight scaled down copies of itself, where the scaling factor here is one fourth.",
  "translatedText": "8개의 축소된 복사본으로 구성되며, 여기서 스케일링 계수는 4분의 1입니다.",
  "model": "DeepL",
  "from_community_srt": "이것은 규모가 줄어든 복사본 8개로 이루어지는데,",
  "n_reviews": 0,
  "start": 516.64,
  "end": 521.72
 },
 {
  "input": "So if you want to know its dimension, it should be some number D, such that one fourth to the power of D equals one eighth, the factor by which the mass just decreased.",
  "translatedText": "따라서 그 크기를 알고 싶다면 질량이 방금 감소한 계수인 D의 제곱에 4분의 1이 8분의 1이 되는 어떤 숫자 D여야 합니다.",
  "model": "DeepL",
  "from_community_srt": "여기서 규모 계수는 1/4입니다. 따라서 이 도형의 차원은 \"1/4을 D제곱했을 때 1/8이 되는\" 어떤 숫자 D입니다. mass가 1/8로 줄었으니까요.",
  "n_reviews": 0,
  "start": 524.82,
  "end": 534.0
 },
 {
  "input": "And in this case, the value we want is log base four of eight, and that's exactly three halves.",
  "translatedText": "이 경우 원하는 값은 8의 로그 밑수 4이며, 정확히 세 개의 절반입니다.",
  "model": "DeepL",
  "from_community_srt": "이 경우 우리가 원하는 차원 값은 로그 4에 8입니다. 그 값은 정확히 2/3이죠.",
  "n_reviews": 0,
  "start": 536.54,
  "end": 541.88
 },
 {
  "input": "So evidently, this fractal is precisely 1.5 dimensional.",
  "translatedText": "따라서 이 프랙탈은 정확히 1.5차원입니다.",
  "model": "DeepL",
  "from_community_srt": "따라서 이 프랙탈은 1.5차원입니다.",
  "n_reviews": 0,
  "start": 542.54,
  "end": 546.8
 },
 {
  "input": "Does that kind of make sense?",
  "translatedText": "이해가 되시나요?",
  "model": "DeepL",
  "from_community_srt": "이상하지 않습니까?",
  "n_reviews": 0,
  "start": 548.06,
  "end": 548.9
 },
 {
  "input": "It's weird, but it's all just about scaling and comparing masses while you scale.",
  "translatedText": "이상하게 들릴지 모르지만, 스케일링을 하면서 질량을 비교하는 것이 전부입니다.",
  "model": "DeepL",
  "from_community_srt": "하지만 이건 전부 규모를 임의로 바꿀 때마다 mass가 어떻게 바뀌는지 알아보는 이야기입니다.",
  "n_reviews": 0,
  "start": 549.18,
  "end": 553.74
 },
 {
  "input": "And what I've described so far, everything up to this point is what you might call self-similarity dimension.",
  "translatedText": "그리고 지금까지 설명한 것은 모두 자기 유사성 차원이라고 할 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "지금까지 제가 소개한 것들은 모두 여러분이 \"자기닮음의 차원\"이라고 부를 만한 것들입니다.",
  "n_reviews": 0,
  "start": 554.98,
  "end": 560.1
 },
 {
  "input": "It does a good job making the idea of fractional dimension seem at least somewhat reasonable, but there's a problem.",
  "translatedText": "분수 차원이라는 개념을 어느 정도 합리적으로 보이게 하는 데는 효과적이지만 문제가 있습니다.",
  "model": "DeepL",
  "from_community_srt": "자기닮음의 차원은 프렉탈 차원의 아이디어가 (적어도) 합리적으로 볼 수 있게 해주지만 문제점이 하나 있습니다.",
  "n_reviews": 0,
  "start": 560.76,
  "end": 566.02
 },
 {
  "input": "It's not really a general notion.",
  "translatedText": "이는 일반적인 개념이 아닙니다.",
  "model": "DeepL",
  "from_community_srt": "문제는 그것이 일반적인 개념이 아니라는 것입니다.",
  "n_reviews": 0,
  "start": 566.32,
  "end": 568.22
 },
 {
  "input": "I mean, when we were reasoning about how a mass's shape should change, it relied on the self-similarity of the shapes, that you could build them up from smaller copies of themselves.",
  "translatedText": "덩어리의 모양이 어떻게 변해야 하는지 추론할 때, 작은 복사본으로 덩어리를 만들 수 있다는 모양의 자기 유사성에 의존했습니다.",
  "model": "DeepL",
  "from_community_srt": "다시 말해, 한 변을 m등분한 수는 자기 유사성을 기반으로 달라진다는 것입니다. 'mass'라는 단어를 풀어 이야기하자면, 자기와 닮은 작은 조각들로 채울 수 있는 양을 의미합니다.",
  "n_reviews": 0,
  "start": 568.58,
  "end": 577.42
 },
 {
  "input": "But that seems unnecessarily restrictive.",
  "translatedText": "하지만 이는 불필요한 제한으로 보입니다.",
  "model": "DeepL",
  "from_community_srt": "하지만 이 개념(자기 닮음 도형으로 자신을 채울 수 있는 수치)은 필요 이상으로 제한적이라고 생각됩니다.",
  "n_reviews": 0,
  "start": 578.08,
  "end": 580.3
 },
 {
  "input": "After all, most two-dimensional shapes are not at all self-similar.",
  "translatedText": "결국, 대부분의 2차원 도형은 자체적으로 전혀 유사하지 않습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 580.76,
  "end": 583.98
 },
 {
  "input": "Consider the disk, the interior of a circle.",
  "translatedText": "원 안쪽인 디스크를 생각해 보세요.",
  "model": "DeepL",
  "from_community_srt": "보편적인 2차원의 도형들을 예로, 그들의 자기 작은 닮음 도형들은 같지 못합니다. 이 원반을 봅시다.",
  "n_reviews": 0,
  "start": 585.32,
  "end": 587.74
 },
 {
  "input": "We know that's two-dimensional, and you could say that this is because when you scale it up by a factor of two, its mass, proportional to the area, gets scaled by the square of that factor, in this case four.",
  "translatedText": "우리는 그것이 2차원이라는 것을 알고 있으며, 2의 배율로 확대하면 면적에 비례하는 질량이 해당 배율의 제곱(이 경우 4)으로 확대되기 때문이라고 말할 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "우리는 이 원반은 2차원 도형임을 압니다. 왜냐면 우리가 2배 확대시켰을 때, 그것에게 필요한 mass(자기닮음도형의 필요갯수)는 넓이로 봤을 때 4개의 작은 원이 필요합니다.",
  "n_reviews": 0,
  "start": 588.14,
  "end": 598.7
 },
 {
  "input": "But it's not like there's some way to piece together four copies of that smaller circle to rebuild the original.",
  "translatedText": "하지만 그 작은 원의 사본 네 개를 모아 원본을 다시 만들 수 있는 방법이 있는 것도 아닙니다.",
  "model": "DeepL",
  "from_community_srt": "하지만 네 개의 자기 닮음 도형으로는 도저히 커진 원반을 채울 수 없음을 압니다.",
  "n_reviews": 0,
  "start": 599.48,
  "end": 604.94
 },
 {
  "input": "So how do we know that that bigger disk is exactly four times the mass of the original?",
  "translatedText": "그렇다면 더 큰 디스크가 원본의 질량의 정확히 4배라는 것을 어떻게 알 수 있을까요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 605.54,
  "end": 610.34
 },
 {
  "input": "Answering that requires a way to make this idea of mass a little more mathematically rigorous, since we're not dealing with physical objects made of matter, are we?",
  "translatedText": "이 질문에 답하려면 물질로 이루어진 물리적 물체를 다루는 것이 아니기 때문에 질량 개념을 좀 더 수학적으로 엄격하게 만들 수 있는 방법이 필요하겠죠?",
  "model": "DeepL",
  "from_community_srt": "그러면 우리는 커진 원반을 어떻게 정확히 떨어지는 갯수로 원본을 채울 수 있을까요? 이 질문에 답을 하기 위해서는 자기 닮음 도형으로 자신을 채울 수 있는 수(mass)을 조금 더 수학적으로 엄밀해져야 합니다. 왜냐하면 우리는 물리적으로 이 개념에 접근하는 것이 아니기 때문입니다.",
  "n_reviews": 0,
  "start": 612.36,
  "end": 620.9
 },
 {
  "input": "We're dealing with purely geometric ones living in an abstract space.",
  "translatedText": "우리는 추상적인 공간에 사는 순전히 기하학적인 것들을 다루고 있습니다.",
  "model": "DeepL",
  "from_community_srt": "우리는 순수 기하(추상적인 공간에 존재하는) 를 다루고 있습니다.",
  "n_reviews": 0,
  "start": 621.12,
  "end": 624.42
 },
 {
  "input": "And there's a couple ways to think about this, but here's a common one.",
  "translatedText": "이에 대해 생각해 볼 수 있는 몇 가지 방법이 있지만 일반적인 방법은 다음과 같습니다.",
  "model": "DeepL",
  "from_community_srt": "두 방법으로 우리는 생각해볼 수 있습니다. 첫째,",
  "n_reviews": 0,
  "start": 625.4,
  "end": 628.48
 },
 {
  "input": "Cover the plane with a grid, and highlight all of the grid squares that are touching the disk, and now count how many there are.",
  "translatedText": "평면을 격자로 덮고 디스크에 닿는 모든 격자 사각형을 강조 표시하고 이제 몇 개가 있는지 세어봅니다.",
  "model": "DeepL",
  "from_community_srt": "보편적인 것으로, 격자점을 깔아주고 도형과 겹치는 부분을 색칠합니다. 그리고 몇 개가 있는지 세면 됩니다.",
  "n_reviews": 0,
  "start": 629.06,
  "end": 636.18
 },
 {
  "input": "In the back of our minds, we already know that a disk is two-dimensional, and the number of grid squares that it touches should be proportional to its area.",
  "translatedText": "우리는 이미 디스크가 2차원이며 디스크가 닿는 격자 사각형의 수는 그 면적에 비례해야 한다는 것을 알고 있습니다.",
  "model": "DeepL",
  "from_community_srt": "예전 우리의 관점으로는 이 원반은 2차원 도형입니다. 따라서 색칠한 격자의 갯수는 구 원반이 차지하는 넓이에 비례할 것입니다.",
  "n_reviews": 0,
  "start": 637.68,
  "end": 645.0
 },
 {
  "input": "A clever way to verify this empirically is to scale up that disk by some factor, like two, and count how many grid squares touch this new scaled-up version.",
  "translatedText": "이를 경험적으로 검증하는 현명한 방법은 해당 디스크를 2와 같이 일정 비율로 확장한 다음, 확장된 새 버전에 닿는 그리드 사각형의 수를 세는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "경험적으로 이 방법을 더 정밀하게 만들고 싶다면, 원반의 크기를 키우고 예를 들어 2배로, 그리고 같은 방법으로 겹치는 부분의 갯수를 셉니다.",
  "n_reviews": 0,
  "start": 646.04,
  "end": 654.18
 },
 {
  "input": "What you should find is that that number has increased approximately in proportion to the square of our scaling factor, which in this case means about four times as many boxes.",
  "translatedText": "이 숫자는 대략 배율의 제곱에 비례하여 증가했다는 것을 알 수 있으며, 이 경우 약 4배의 상자를 의미합니다.",
  "model": "DeepL",
  "from_community_srt": "당신이 알아채야 할 것은 갯수가 대략 비례하여 증가하는데, 이도 당신이 도형을 얼마나 늘렸는지에 비례하는데, 그 수치는 격자점이 (영상의 경우) 네 배 많아 짐을 알 수 있습니다.",
  "n_reviews": 0,
  "start": 654.96,
  "end": 663.66
 },
 {
  "input": "Well, admittedly what's on the screen here might not look that convincing, but it's just because the grid is really coarse.",
  "translatedText": "물론 여기 화면에 표시된 내용이 그다지 설득력이 없어 보일 수도 있지만, 이는 그리드가 매우 거칠기 때문입니다.",
  "model": "DeepL",
  "from_community_srt": "음... 솔직하게 영상 자료가 이해하는 데 도움을 줬을 지 모르겠지만, 이는 격자점이 너무",
  "n_reviews": 0,
  "start": 664.96,
  "end": 670.46
 },
 {
  "input": "If instead you took a much finer grid, one that more tightly captures the intent we're going for here by measuring the size of the circle, that relationship of quadrupling the number of boxes touched when you scale the disk by a factor of two should shine through more clearly.",
  "translatedText": "대신 훨씬 더 세밀한 그리드, 즉 원의 크기를 측정하여 우리가 여기서 의도하는 바를 더 잘 포착할 수 있는 그리드를 사용한다면 디스크 크기를 2배로 늘릴 때 닿는 상자 수가 4배가 된다는 관계가 더 명확하게 드러날 것입니다.",
  "model": "DeepL",
  "from_community_srt": "조잡해서 그렇습니다. 만약 당신이 아주 세밀한 격자점을 깐다면, 원을 더욱 컴팩트하게 채울 수 있는, 이 관계(원반의 넓이와 격자점 갯수간의)는 격자점을 4배 세밀하게 하고 다시 색칠하기를 했을 때, 이전의 상황보다 선명해짐을",
  "n_reviews": 0,
  "start": 670.76,
  "end": 683.96
 },
 {
  "input": "I'll admit though that when I was animating this, I was surprised by just how slowly this value converges to four.",
  "translatedText": "이 애니메이션을 만들 때 이 값이 4로 수렴하는 속도가 너무 느려서 놀랐다는 점은 인정합니다.",
  "model": "DeepL",
  "from_community_srt": "느낄 수 있을 것입니다. 저도 사실 놀랐는데요, 제가 이 영상을 만들때 비율이 얼마나 천천히 4에 수렴하는지 놀라웠습니다.",
  "n_reviews": 0,
  "start": 685.24,
  "end": 690.32
 },
 {
  "input": "Here's one example.",
  "translatedText": "한 가지 예를 들어보겠습니다.",
  "model": "DeepL",
  "from_community_srt": "더 정확히 말씀드리면, 도면을 그려",
  "n_reviews": 0,
  "start": 696.48,
  "end": 697.64
 },
 {
  "input": "For larger and larger scaling values, which is actually equivalent to just looking at a finer grid, that data is going to more perfectly fit that parabola.",
  "translatedText": "스케일링 값이 점점 더 커지는 경우(실제로는 더 세밀한 격자를 보는 것과 같습니다), 해당 데이터는 포물선에 더 완벽하게 맞을 것입니다.",
  "model": "DeepL",
  "from_community_srt": "격자점의 세밀한 정도와 원반과 겹치는 격자의 갯수의 관게에 대한 데이터는 완벽한 포물선에 가까워지는데, 이는 겹치는 격자의 갯수가 대략 깔아준 격자의 세밀함과 비례함을 알 수 있습니다. 원반이 더욱 커질 수록, 더 세밀한 격자점을 깔아주는 것과 동등한,",
  "n_reviews": 0,
  "start": 707.9,
  "end": 718.76
 },
 {
  "input": "Now getting back to fractals, let's play this game with the Sierpinski triangle, counting how many boxes are touching points in that shape.",
  "translatedText": "이제 프랙탈로 돌아가서 시에르핀스키 삼각형으로 이 게임을 해보면서 해당 도형에 접하는 점의 개수를 세어보겠습니다.",
  "model": "DeepL",
  "from_community_srt": "그 데이터는 포물선을 그리는 관계에 더 잘 들어맞습니다. 다시 프랙탈로 돌아와서 시어핀스키 삼각형으로 간단한 게임을 해봅시다. 격자점과 얼마나 많은 부분 겹치는 지 봅시다.",
  "n_reviews": 0,
  "start": 722.04,
  "end": 729.02
 },
 {
  "input": "How would you imagine that number compares to scaling up the triangle by a factor of two and counting the new number of boxes touched?",
  "translatedText": "이 숫자가 삼각형을 두 배로 확대하고 새로 닿은 상자 수를 세는 것과 어떻게 비교될까요?",
  "model": "DeepL",
  "from_community_srt": "그리고 한번 더 상상해봅시다 이 삼각형을 두 배 키웠을 때 얼마나 많은 부분 겹치는 지요.",
  "n_reviews": 0,
  "start": 730.6,
  "end": 737.14
 },
 {
  "input": "Well, the proportion of boxes touched by the big one to the number of boxes touched by the small one should be about three.",
  "translatedText": "큰 상자가 닿은 상자 수와 작은 상자가 닿은 상자 수의 비율은 약 3이 되어야 합니다.",
  "model": "DeepL",
  "from_community_srt": "아마 그 커진 삼각형과 겹치는 부분의 비율은 이전 상황에 비해 3배 커졌을 것입니다.",
  "n_reviews": 0,
  "start": 739.9,
  "end": 746.04
 },
 {
  "input": "After all, that bigger version is just built up of three copies of the smaller version.",
  "translatedText": "결국, 큰 버전은 작은 버전의 복사본 세 개로 구성된 것입니다.",
  "model": "DeepL",
  "from_community_srt": "따라서, 커진 삼각형은 이전(작은 시어핀스키 삼각형)의 세배 임을 알 수 있습니다.",
  "n_reviews": 0,
  "start": 746.86,
  "end": 750.94
 },
 {
  "input": "You could also think about this as two raised to the dimension of the fractal, which we just saw is about 1.585.",
  "translatedText": "2를 프랙탈의 차원으로 올리면 약 1.585가 된다고 생각할 수도 있습니다.",
  "model": "DeepL",
  "from_community_srt": "또한 우리는 두 배 커진 공간이지만 프랙탈의 입장에서는 1.585배 커짐이라는 것을 알 수 있습니다.",
  "n_reviews": 0,
  "start": 752.34,
  "end": 758.82
 },
 {
  "input": "And so if you were to go and plot the scaling factor in this case against the number of boxes touched by the Sierpinski triangle, the data would closely fit a curve with the shape of y equals x to the power 1.585, just multiplied by some proportionality constant.",
  "translatedText": "따라서 이 경우의 배율을 시에르핀스키 삼각형이 닿은 상자 수에 대해 플롯하면, 데이터는 비례 상수를 곱한 값에 x의 거듭제곱 1.585를 곱한 값의 y 모양을 가진 곡선에 가깝게 맞출 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "이를 다시 도면을 그려서 스케일링한 정도와 겹치는 상자의 갯수의 관계를 시어핀스키 삼각형에 관해 그리면 그 테이터는 이 곡선에 수렴할 것입니다. 이 함수는 y= x^(1.585)를 만족하며 단지 비례상수만이 곱해집니다.",
  "n_reviews": 0,
  "start": 760.02,
  "end": 775.2
 },
 {
  "input": "But importantly, the whole reason that I'm talking about this is that we can play the same game with non-self-similar shapes that still have some kind of roughness.",
  "translatedText": "하지만 제가 이 이야기를 하는 중요한 이유는 자체적으로 유사하지 않은 모양으로 같은 게임을 할 수 있지만 여전히 거친 느낌이 있기 때문입니다.",
  "model": "DeepL",
  "from_community_srt": "하지만 이 이야기를 한 중요한 이유는 이 방법으로 우리는 꼭 자기 닮음 도형이 아니어도 같은 방식을 시도해볼 수 있다는 것입니다.",
  "n_reviews": 0,
  "start": 777.22,
  "end": 784.4
 },
 {
  "input": "And the classic example here is the coastline of Britain.",
  "translatedText": "대표적인 예로 영국의 해안선을 들 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "일반적으로 드는 예는 영국의 해안선입니다.",
  "n_reviews": 0,
  "start": 784.88,
  "end": 787.12
 },
 {
  "input": "If you plop that coastline into the plane and count how many boxes are touching it, and then scale it by some amount, and count how many boxes are touching that new scaled version, what you'd find is that the number of boxes touching the coastline increases approximately in proportion to the scaling factor raised to the power of 1.21.",
  "translatedText": "이 해안선을 평면에 놓고 얼마나 많은 상자가 해안선에 닿아 있는지 세고, 그 다음 일정 비율로 스케일링하고, 스케일링된 새 버전에 닿아 있는 상자 수를 세어 보면 해안선에 닿아 있는 상자 수가 1.21의 거듭제곱에 대략 비례하여 증가한다는 것을 알 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "당신이 비행기를 타고 격자점을 해안선을 따라서 깔아주고 몇개의 상자와 겹치는 지 세고, 다시 비율을 키워서 얼마나 많은 상자와 겹치는 지 다시 세면, 당신은 해안선과 상자와 겹치는 이 수치가 대략적으로 해안선의 크기비율을 늘린 수치와 지수적으로 비례함을 알 수 있습니다.",
  "n_reviews": 0,
  "start": 787.86,
  "end": 806.32
 },
 {
  "input": "Here, it's kind of fun to think about how you would actually compute that number empirically.",
  "translatedText": "여기서 실제로 이 숫자를 경험적으로 어떻게 계산할지 생각해 보는 것도 재미있을 것입니다.",
  "model": "DeepL",
  "from_community_srt": "대략 1.21으로 말이죠.",
  "n_reviews": 0,
  "start": 807.82,
  "end": 812.06
 },
 {
  "input": "As in, imagine I give you some shape, and you're a savvy programmer.",
  "translatedText": "예를 들어, 제가 여러분에게 어떤 모양을 만들어 주면 여러분은 능숙한 프로그래머라고 가정해 보겠습니다.",
  "model": "DeepL",
  "from_community_srt": "그렇다면 당신은 어떻게 이 수치를 찾을 수 있을까요? 제가 만약 상상 속의 도형을 제시하고",
  "n_reviews": 0,
  "start": 812.64,
  "end": 815.94
 },
 {
  "input": "How would you find this number?",
  "translatedText": "이 번호는 어떻게 찾을 수 있나요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.62,
  "end": 818.04
 },
 {
  "input": "So what I'm saying here is that if you scale this shape by some factor, which I'll call S, the number of boxes touching that shape should equal some constant multiplied by that scaling factor raised to whatever the dimension is, the value that we're looking for.",
  "translatedText": "제가 여기서 말하는 것은 이 도형의 크기를 S라고 하는 어떤 계수에 따라 조정하면 해당 도형에 닿는 상자의 수에 우리가 찾고 있는 값인 치수가 얼마가 되든 그 배율 계수를 곱한 상수와 같아야 한다는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "당신은 이해력이 좋은 프로그래머라면, 어떻게 찾으시겠습니까? 제가 말하려는 것은 당신이 도형을 몇 배 늘리고 싶을 것이며 늘려져 상자와 겹치는 수치를 저는 s라고 부를 것이고 이는 당신이 몇 배로 크기를 키우든, 어떤 차원의 도형을 키우든, s라는 수치는 시간에 관계하지 않을 것입니다.",
  "n_reviews": 0,
  "start": 822.42,
  "end": 836.92
 },
 {
  "input": "Now, if you have some data plot that closely fits a curve that looks like the input raised to some power, it can be hard to see exactly what that power should be.",
  "translatedText": "이제 입력값이 어떤 거듭제곱으로 올라간 것처럼 보이는 곡선에 정확히 들어맞는 데이터 플롯이 있다면, 그 거듭제곱이 정확히 무엇인지 확인하기 어려울 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "자, 이제 당신이 관찰한 데이터를 갖고 있다면, 이 커브에 맞는 그리고 이 커브가 지수함수를 만족하는 듯 보이면 우리는 그 지수가 무엇인지 찾기가 조금은 까다로울 것입니다.",
  "n_reviews": 0,
  "start": 837.92,
  "end": 847.0
 },
 {
  "input": "So a common trick is to take the logarithm of both sides.",
  "translatedText": "따라서 일반적인 요령은 양쪽의 로그를 취하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 848.02,
  "end": 851.16
 },
 {
  "input": "That way, the dimension is going to drop down from the exponent, and we'll have a nice clean linear relationship.",
  "translatedText": "이렇게 하면 지수를 통해 차원이 내려가고 깔끔한 선형 관계를 갖게 됩니다.",
  "model": "DeepL",
  "from_community_srt": "따라서 우리는 양변에 로그를 취할 것인데요, 따라서 우리는 지수를 '정수배화' 시킬 수 있습니다 따라서 우리는 선명하고 좋은 일차의 관계를 찾을 수 있습니다.",
  "n_reviews": 0,
  "start": 851.64,
  "end": 857.08
 },
 {
  "input": "What this suggests is that if you were to plot the log of the scaling factor against the log of the number of boxes touching the coastline, the relationship should look like a line, and that line should have a slope equal to the dimension.",
  "translatedText": "이것이 시사하는 바는 배율 계수의 로그와 해안선에 닿는 상자 수의 로그를 비교하면 관계가 선처럼 보이고 그 선은 치수와 같은 기울기를 가져야 한다는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "이것이 우리에게 시사하는 바는, 만약 우리가 다시 이를 함수로 표현했을 시에, 상자와 겹치는 양을 y에 대응하여 표시하면 이 함수는 직선의 형태를 가지며 이 직선은 기울기를 갖고 이 기울기(경사)는 차원에 대응합니다.",
  "n_reviews": 0,
  "start": 858.12,
  "end": 871.36
 },
 {
  "input": "So what that means is that if you tried out a whole bunch of scaling factors, counted the number of boxes touching the coast in each instant, and then plotted the points on the log-log plot, you could then do some kind of linear regression to find the best fit line to your data set, and when you look at the slope of that line, that tells you the empirical measurement for the dimension of what you're examining.",
  "translatedText": "즉, 여러 가지 스케일링 인자를 시도하고 각 순간에 해안에 닿는 상자 수를 세고 로그-로그 플롯에 점을 표시한 다음 일종의 선형 회귀를 수행하여 데이터 세트에 가장 적합한 선을 찾고 해당 선의 기울기를 보면 조사 대상의 차원에 대한 경험적 측정값을 알 수 있다는 뜻입니다.",
  "model": "DeepL",
  "from_community_srt": "만약 당신이 도형을 얼마나 늘이고 줄이든, 각각의 상황에 따라서 상자와 해안선의 겹치는 수치를 로그-로그 관계의 평면에 그렸을 때, 우리는 직선에 가까운 점들의 집합을 볼 수 있고 이 테이터를 표현하는 방법에는 이 점들을 이었을 때 생기는 직선의 기울기가 경험적으로 우리가 찾으려던 차원수가 됨을 알 수 있습니다.",
  "n_reviews": 0,
  "start": 873.44,
  "end": 893.88
 },
 {
  "input": "I just think that makes this idea of fractal dimension so much more real and visceral compared to abstract, artificially perfect shapes.",
  "translatedText": "그래서 프랙탈 차원이라는 아이디어가 추상적이고 인위적으로 완벽한 도형에 비해 훨씬 더 현실적이고 본능적이라고 생각합니다.",
  "model": "DeepL",
  "from_community_srt": "이 방법의 프랙탈에 대한 접근은 아주 현실적이고 본능적입니다. 추상적이고 표면적인 도형들보다도.",
  "n_reviews": 0,
  "start": 894.76,
  "end": 901.08
 },
 {
  "input": "And once you're comfortable thinking about dimension like this, you, my friend, have become ready to hear the definition of a fractal.",
  "translatedText": "이렇게 차원에 대해 편안하게 생각하게 되면 프랙탈의 정의를 들을 준비가 된 것입니다.",
  "model": "DeepL",
  "from_community_srt": "그리고 만약 당신이 이와 같은 차원의 개념에 익숙해졌다면 나의 친구, 당신은 이제 프랙탈의 정의를 이해하는데 준비가 끝났습니다.",
  "n_reviews": 0,
  "start": 902.04,
  "end": 908.44
 },
 {
  "input": "Essentially, fractals are shapes whose dimension is not an integer, but instead some fractional amount.",
  "translatedText": "기본적으로 프랙탈은 크기가 정수가 아닌 일부 분수인 도형입니다.",
  "model": "DeepL",
  "from_community_srt": "본직적으로, 프랙탈은 도형인데, 정수배의 차원을 가진 것이 아닌,",
  "n_reviews": 0,
  "start": 910.3,
  "end": 916.42
 },
 {
  "input": "What's cool about that is that it's a quantitative way to say that they're shapes that are rough, and that they stay rough even as you zoom in.",
  "translatedText": "멋진 점은 거친 모양을 정량적으로 표현할 수 있고, 확대해도 거친 모양이 유지된다는 점입니다.",
  "model": "DeepL",
  "from_community_srt": "프랙탈의 수치를 가진 도형입니다. 이것은 그 도형이 가진 복잡한 정도를 정량적으로 기술하는 좋고 멋진 방법입니다. 그리고 이는 당신이 확대를 하더라도 변하지 않죠.",
  "n_reviews": 0,
  "start": 917.36,
  "end": 925.04
 },
 {
  "input": "Technically, there's a slightly more accurate definition, and I've included it in the video description, but this idea here of a non-integer dimension almost entirely captures the idea of roughness that we're going for.",
  "translatedText": "기술적으로는 조금 더 정확한 정의가 있고 동영상 설명에 포함시켰지만, 정수가 아닌 차원이라는 이 아이디어는 우리가 추구하는 거칠기의 개념을 거의 완벽하게 포착하고 있습니다.",
  "model": "DeepL",
  "from_community_srt": "엄밀히 말해, 프랙탈의 정의에 조금만 보태자면, 그리고 이 비디오에 추가한 것인데, 정수배가 아닌 차원은 거친 정도(복잡한 정도)를 단번에 나타낼 수 있는 하나의 수치입니다.",
  "n_reviews": 0,
  "start": 925.96,
  "end": 937.52
 },
 {
  "input": "There is one nuance though that I haven't brought up yet, but it's worth pointing out, which is that this dimension, at least as I've described it so far using the box counting method, can sometimes change based on how far zoomed in you are.",
  "translatedText": "아직 언급하지 않은 한 가지 뉘앙스가 있지만 지적할 가치가 있는 것은 적어도 지금까지 박스 카운팅 방법을 사용하여 설명한 것처럼 이 치수는 얼마나 확대했는지에 따라 달라질 수 있다는 점입니다.",
  "model": "DeepL",
  "from_community_srt": "다만 여기에는 한 늬앙스가 있는데, 아직 제가 말하지는 않았지만 언급하기에 충분한 가치가 있습니다. 제가 지금껏 말해온 이 차원의 개념, 겹치는 상자의 갯수를 세는 그 방법말이죠, 당신이 얼마만큼 줌을 하느냐에따라 달라진다는 것입니다.",
  "n_reviews": 0,
  "start": 938.52,
  "end": 951.28
 },
 {
  "input": "For example, here's a shape sitting in three dimensions which at a distance looks like a line.",
  "translatedText": "예를 들어, 멀리서 보면 선처럼 보이는 도형이 3차원으로 놓여 있습니다.",
  "model": "DeepL",
  "from_community_srt": "예를 들어 여기 3차원공간에 거리감을 가진 직선이 있다고 합시다.",
  "n_reviews": 0,
  "start": 952.64,
  "end": 957.6
 },
 {
  "input": "In 3D, by the way, when you do a box counting you have a 3D grid full of little cubes instead of little squares, but it works the same way.",
  "translatedText": "그런데 3D에서는 상자 수를 셀 때 작은 정사각형 대신 작은 정육면체로 가득 찬 3D 그리드가 있지만 동일한 방식으로 작동합니다.",
  "model": "DeepL",
  "from_community_srt": "3차원 공간에서 보면, 당신은 상자 세기를 3차원 격자를 이용하여 입체 상자의 갯수를 세야 할 것입니다. 물론,",
  "n_reviews": 0,
  "start": 958.36,
  "end": 965.48
 },
 {
  "input": "At this scale, where the shape's thickness is smaller than the size of the boxes, it looks one-dimensional, meaning the number of boxes it touches is proportional to its length.",
  "translatedText": "도형의 두께가 상자의 크기보다 작은 이 배율에서는 1차원적으로 보이며, 이는 닿는 상자의 수가 길이에 비례한다는 의미입니다.",
  "model": "DeepL",
  "from_community_srt": "방법은 같습니다. 만약 이 선의 굵기가 상자를 관통할 정도로 얇다면, 이것은 아마 1차원으로 보일 것입니다. 이것이 지나가는 상자의 숫자는 이 3차원 선의 길이가 될 것입니다.",
  "n_reviews": 0,
  "start": 966.38,
  "end": 976.64
 },
 {
  "input": "But when you scale it up, it starts behaving a lot more like a tube, touching the boxes on the surface of that tube, and so it'll look two-dimensional, with the number of boxes touched being proportional to the square of the scaling factor.",
  "translatedText": "그러나 스케일을 확대하면 튜브처럼 작동하기 시작하여 튜브 표면의 상자에 닿기 때문에 닿는 상자 수가 스케일링 계수의 제곱에 비례하는 2차원으로 보입니다.",
  "model": "DeepL",
  "from_community_srt": "하지만 이처럼 선의 크기를 키우면 이 선은 마치 튜브처럼 보일 것입니다. 표면이 상자를 지나가는 것처럼 보이죠. 따라서 이는 2차원처럼 보입니다. 곧 상자의 배율을 늘려서 계산하는 것처럼 말이죠.",
  "n_reviews": 0,
  "start": 977.5,
  "end": 990.76
 },
 {
  "input": "But it's not really a tube, it's made of these rapidly winding little curves, so once you scale it up even more, to the point where the boxes can pick up on the details of those curves, it looks one-dimensional again, with the number of boxes touched scaling directly in proportion to the scaling constant.",
  "translatedText": "하지만 실제로는 튜브가 아니라 빠르게 구불구불한 작은 곡선으로 만들어졌기 때문에 상자가 곡선의 디테일을 포착할 수 있을 정도로 크기를 더 확대하면 다시 1차원적으로 보이며, 닿는 상자 수는 스케일링 상수에 비례하여 직접적으로 확대됩니다.",
  "model": "DeepL",
  "from_community_srt": "하지만 이 선은 정말 튜브가 아니기 때문에 영상과 같이 주기적이로 감아진 커브여서, 당신의 몇 배를 더 늘리든, 이 상자와 만나는 부분을 찾아도 이는 이 커브의 일부이기 때문에 다시 1차원처럼 보입니다. 또한 상자와 만나는 비율은 크기를 키우는 것과 비례할 것이고요.",
  "n_reviews": 0,
  "start": 991.64,
  "end": 1008.36
 },
 {
  "input": "So actually assigning a number to a shape for its dimension can be tricky, and it leaves room for differing definitions and differing conventions.",
  "translatedText": "따라서 실제로 도형에 치수에 대한 숫자를 할당하는 것은 까다로울 수 있으며, 다른 정의와 다른 규칙이 적용될 여지가 있습니다.",
  "model": "DeepL",
  "from_community_srt": "따라서, 사실 실제 도형에 그것의 차원을 정밀하게 정하는 것이 까다롭습니다.",
  "n_reviews": 0,
  "start": 1009.4,
  "end": 1018.12
 },
 {
  "input": "In a pure math setting, there are indeed numerous definitions for dimension, but all of them focus on what the limit of this dimension is at closer and closer zoom levels.",
  "translatedText": "순수한 수학 환경에서는 실제로 차원에 대한 수많은 정의가 있지만, 모두 점점 더 가까운 확대/축소 수준에서 이 차원의 한계가 무엇인지에 초점을 맞추고 있습니다.",
  "model": "DeepL",
  "from_community_srt": "따라서 이는 다른 정의법과 다른 관례법이 발생할 여지를 만듭니다. 순수 수학에서는 사실 굉장히 많은 차원에 대한 정의법들이 있습니다. 하지만 이들은 확대를 계속하여 극한의 값을 찾는 것에 초점을 두고 있습니다.",
  "n_reviews": 0,
  "start": 1020.62,
  "end": 1030.92
 },
 {
  "input": "You can think of that in terms of the plot as the limit of this slope as you move farther and farther to the right.",
  "translatedText": "오른쪽으로 점점 더 멀어질수록 이 기울기의 한계라고 생각하면 됩니다.",
  "model": "DeepL",
  "from_community_srt": "이 흐름에서 본다면, 이 경사의 기울기의 극한은 계속해서 우측을 향할 것이라고 생각할 것입니다.",
  "n_reviews": 0,
  "start": 1034.54,
  "end": 1040.1
 },
 {
  "input": "So for a purely geometric shape to be a genuine fractal, it has to continue looking rough, even as you zoom in infinitely far.",
  "translatedText": "따라서 순수한 기하학적 도형이 진정한 프랙탈이 되려면 무한히 확대해도 계속 거칠어 보여야 합니다.",
  "model": "DeepL",
  "from_community_srt": "따라서 순수한 이 모형이 프랙탈에 의해서 복잡하게 보이는 정도가 무한히 확대를 해도 계속 되어야 함을 의미할 것이라고 생각 할 것입니다.",
  "n_reviews": 0,
  "start": 1041.2,
  "end": 1048.08
 },
 {
  "input": "But in a more applied setting, like looking at the coastline of Britain, it doesn't really make sense to talk about the limit as you zoom in more and more.",
  "translatedText": "하지만 영국의 해안선을 보는 것과 같이 좀 더 응용적인 환경에서는 점점 더 확대할수록 한계에 대해 이야기하는 것은 의미가 없습니다.",
  "model": "DeepL",
  "from_community_srt": "더 엄밀히 했을 때, 영국의 해안선을 예로, 무한히 확대를 하는 것이 불가능합니다.",
  "n_reviews": 0,
  "start": 1049.96,
  "end": 1057.68
 },
 {
  "input": "I mean, at some point you'd just be hitting atoms.",
  "translatedText": "언젠가는 원자를 치게 될 것입니다.",
  "model": "DeepL",
  "from_community_srt": "그러니까 계속 확대를 진행하면 원자단위까지 확대해야 할 것입니다.",
  "n_reviews": 0,
  "start": 1058.12,
  "end": 1060.12
 },
 {
  "input": "Instead what you do is you look at a sufficiently wide range of scales from very zoomed out up to very zoomed in, and compute the dimension at each one.",
  "translatedText": "대신 매우 축소된 스케일부터 매우 확대된 스케일까지 충분히 넓은 범위의 스케일을 보고 각 스케일에서 치수를 계산하면 됩니다.",
  "model": "DeepL",
  "from_community_srt": "대신에, 충분히 넚은 시야에서 매우 축소된 시야에서 확대된 시야를 기준으로 각각의 상황에서 차원수를 구하고, 그리고 조금 더 보태서,",
  "n_reviews": 0,
  "start": 1060.96,
  "end": 1068.82
 },
 {
  "input": "And in this more applied setting, a shape is typically considered to be a fractal only when the measured dimension stays approximately constant even across multiple different scales.",
  "translatedText": "그리고 이러한 적용 환경에서 도형은 일반적으로 측정된 치수가 여러 다른 스케일에서도 거의 일정하게 유지되는 경우에만 프랙탈로 간주됩니다.",
  "model": "DeepL",
  "from_community_srt": "도형이 일반적으로 당신이 측정했을 때, 차원수의 증가가 일정하다면, 다른 크기의 상황에서 예를 들어,",
  "n_reviews": 0,
  "start": 1069.94,
  "end": 1079.94
 },
 {
  "input": "For example, the coastline of Britain doesn't just look 1.21 dimensional at a distance.",
  "translatedText": "예를 들어, 영국의 해안선은 멀리서 보면 1.21차원으로만 보이지 않습니다.",
  "model": "DeepL",
  "from_community_srt": "영국의 해안선은 거리를 두고 관찰했을 때,",
  "n_reviews": 0,
  "start": 1080.66,
  "end": 1084.94
 },
 {
  "input": "Even if you zoom in by a factor of a thousand, the level of roughness is still around 1.21.",
  "translatedText": "1000배로 확대해도 거칠기 수준은 여전히 1.21 정도입니다.",
  "model": "DeepL",
  "from_community_srt": "1.21차원으로 관찰할 수 없습니다. 1000배 확대를 시켜도, 수치는 1.21의 언저리에서 머물 것입니다.",
  "n_reviews": 0,
  "start": 1085.28,
  "end": 1090.56
 },
 {
  "input": "That right there is the sense in which many shapes from nature actually are self-similar, albeit not perfect self-similarity.",
  "translatedText": "완벽한 자기 유사성은 아니지만 자연의 많은 모양이 실제로는 자기 유사성을 지니고 있다는 의미입니다.",
  "model": "DeepL",
  "from_community_srt": "바로 그 점에서, 자연적인 도형들은 정확한 자기 닮음이 아님을 알 수 있습니다.",
  "n_reviews": 0,
  "start": 1091.52,
  "end": 1098.86
 },
 {
  "input": "Perfectly self-similar shapes do play an important role in fractal geometry.",
  "translatedText": "완벽하게 자기 유사 도형은 프랙탈 기하학에서 중요한 역할을 합니다.",
  "model": "DeepL",
  "from_community_srt": "완벽한 자기 닮음의 도형들은 프랙탈 기하학에서는 아주 중요한 역할을 합니다.",
  "n_reviews": 0,
  "start": 1099.64,
  "end": 1103.26
 },
 {
  "input": "What they give us are simple to describe, low-information examples of this phenomenon of roughness, roughness that persists at many different scales and at arbitrarily close scales.",
  "translatedText": "이러한 거칠기 현상은 다양한 스케일과 임의로 가까운 스케일에서 지속되는 거칠기 현상에 대한 설명이 간단하고 정보가 적은 예시입니다.",
  "model": "DeepL",
  "from_community_srt": "이들의 역할은 간단히 말해 영상과 같은 행위에서 거친 정도를 말해줍니다. 이 영상에서의 거친 정도는 어느 임의의 크기에서든지 일정합니다.",
  "n_reviews": 0,
  "start": 1103.66,
  "end": 1113.74
 },
 {
  "input": "And that's important, it gives us the primitive tools for modeling these fractal phenomena.",
  "translatedText": "프랙탈 현상을 모델링할 수 있는 원시적인 도구를 제공한다는 점에서 중요합니다.",
  "model": "DeepL",
  "from_community_srt": "그리고 이게 중요합겁니다!!! 왜냐면 이 방법이 이러한 프랙탈 현상을 만드는데에",
  "n_reviews": 0,
  "start": 1114.86,
  "end": 1118.98
 },
 {
  "input": "But I think it's also important not to view them as the prototypical example of fractals, since fractals in general actually have a lot more character to them.",
  "translatedText": "하지만 일반적으로 프랙탈에는 훨씬 더 많은 특징이 있기 때문에 이를 프랙탈의 전형적인 예로 보지 않는 것도 중요하다고 생각합니다.",
  "model": "DeepL",
  "from_community_srt": "기본적인 도구이기 때문이죠. 하지만, 이 방법이 프랙탈을 이해하는데에 간단한 검증이나 단편적인 핵심이 아님을 아셨으면 좋겠습니다. 왜냐하면 프랙탈은 더욱 많은 특징들을 갖고 있기 때문입니다.",
  "n_reviews": 0,
  "start": 1119.76,
  "end": 1127.66
 },
 {
  "input": "I really do think that this is one of those ideas where once you learn it, it makes you start looking at the world completely differently.",
  "translatedText": "한 번 배우면 세상을 완전히 다르게 바라보게 되는 아이디어 중 하나라고 생각합니다.",
  "model": "DeepL",
  "from_community_srt": "저는 이 아이디어가 만약 당신이 습득한다면, 당신의 세상을 바라보는 눈을",
  "n_reviews": 0,
  "start": 1130.78,
  "end": 1136.24
 },
 {
  "input": "What this number is, what this fractional dimension gives us is a quantitative way to describe roughness.",
  "translatedText": "이 숫자가 의미하는 것은, 이 분수 차원이 제공하는 것은 거칠기를 정량적으로 설명하는 방식입니다.",
  "model": "DeepL",
  "from_community_srt": "완전히 다르게 만들 것이라고 생각합니다. 이 숫자는, 그리고 프랙탈 차원이 우리에게 의미하는 바는, 이것이 거친 정도를 말하는 수치이기 때문입니다.",
  "n_reviews": 0,
  "start": 1136.9,
  "end": 1142.96
 },
 {
  "input": "For example, the coastline of Norway is about 1.52 dimensional, which is a numerical way to communicate the fact that it's way more jaggedy than Britain's coastline.",
  "translatedText": "예를 들어 노르웨이의 해안선은 약 1.52차원으로, 영국의 해안선보다 훨씬 더 울퉁불퉁하다는 사실을 수치로 전달할 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "예를 들어 노르웨이의 해안선은 약 1,51,차원인데, 이것은 영국의 해안선보다 \"얼마만큼 더 정확히 복잡한지\"에 대해 말하는 하나의 방법이라는 것입니다.",
  "n_reviews": 0,
  "start": 1143.4,
  "end": 1152.04
 },
 {
  "input": "The surface of a calm ocean might have a fractal dimension only barely above 2, while a stormy one might have a dimension closer to 2.3.",
  "translatedText": "잔잔한 바다의 표면은 프랙탈 차원이 2를 간신히 넘을 수 있지만, 폭풍우가 치는 바다의 표면은 2.3에 가까운 차원을 가질 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "아주 매끈한 바다는 프랙탈적으로 2보다 아주 조금 큰 차원을 가질 것입니다. 바람이 많이 부는 바다는 약 2.3에 수렴할 때 말이죠.",
  "n_reviews": 0,
  "start": 1152.82,
  "end": 1160.12
 },
 {
  "input": "In fact, fractal dimension doesn't just arise frequently in nature, it seems to be the core differentiator between objects that arise naturally and those that are just man-made.",
  "translatedText": "사실 프랙탈 차원은 자연에서 자주 발생하는 것이 아니라 자연적으로 발생하는 물체와 인공적으로 만들어진 물체를 구분하는 핵심적인 요소입니다.",
  "model": "DeepL",
  "from_community_srt": "사실, 프랙탈 차원은 자연에서만 생겨나는 것일 수 있습니다, 곧, 프랙탈차원은 자연에서 만들어진 물체와 인간이 만든 물체간의 핵심적인 차이가 되는 것이죠.",
  "n_reviews": 0,
  "start": 1161.3,
  "end": 1170.16
 }
]