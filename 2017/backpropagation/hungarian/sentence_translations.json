[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn.",
  "translatedText": "Itt a backpropagációval, a neurális hálózatok tanulásának alapvető algoritmusával foglalkozunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas.",
  "translatedText": "Egy gyors összefoglaló után, hogy hol tartunk, az első dolog, amit teszek, egy intuitív áttekintés arról, hogy mit csinál az algoritmus valójában, a képletekre való hivatkozás nélkül.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this.",
  "translatedText": "Azoknak, akik szeretnének belemerülni a matematikába, a következő videó a mindezek alapjául szolgáló számításokkal foglalkozik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information.",
  "translatedText": "Ha megnézted az előző két videót, vagy ha csak a megfelelő háttérrel ugrottál be, akkor tudod, mi az a neurális hálózat, és hogyan táplálja az információt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer.",
  "translatedText": "Itt a kézzel írt számjegyek felismerésének klasszikus példáját mutatjuk be, amelyek pixelértékei a 784 neuronból álló hálózat első rétegébe kerülnek, és egy olyan hálózatot mutattam be, amelynek két rejtett rétege mindössze 16 neuronból áll, és egy 10 neuronból álló kimeneti réteget, amely jelzi, hogy a hálózat melyik számjegyet választja válaszként.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function.",
  "translatedText": "Azt is elvárom, hogy megértsd a gradiens ereszkedést, ahogyan azt az előző videóban leírtuk, és azt, hogy a tanulás alatt azt értjük, hogy meg akarjuk találni, hogy mely súlyok és torzítások minimalizálnak egy bizonyos költségfüggvényt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component.",
  "translatedText": "Gyors emlékeztetőül: egyetlen képzési példa költségére a hálózat által adott kimenetet és a kívánt kimenetet veszi, és összeadja az egyes komponensek közötti különbségek négyzetét.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network.",
  "translatedText": "Ha ezt az összes több tízezer képzési példára elvégezzük, és az eredményeket átlagoljuk, akkor megkapjuk a hálózat teljes költségét.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "And as if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost.",
  "translatedText": "És mintha ez még nem lenne elég, ahogy az előző videóban is leírtuk, a dolog, amit keresünk, ennek a költségfüggvénynek a negatív gradiense, ami megmondja, hogyan kell megváltoztatni az összes súlyt és torzítást, az összes kapcsolatot, hogy a leghatékonyabban csökkentsük a költségeket.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient.",
  "translatedText": "A backpropagation, amelyről ez a videó szól, egy algoritmus ennek az őrült bonyolult gradiensnek a kiszámítására.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "And the one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it.",
  "translatedText": "És az utolsó videóban elhangzott egy gondolat, amit most nagyon szeretném, ha szilárdan a fejetekben tartanátok, hogy mivel a gradiens vektor 13 000 dimenzióban való irányként való elképzelése, hogy finoman fogalmazzak, meghaladja a képzeletünk határait, van egy másik mód, ahogyan gondolkodhattok róla.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias.",
  "translatedText": "Az egyes komponensek nagysága itt azt mutatja meg, hogy a költségfüggvény mennyire érzékeny az egyes súlyokra és torzításokra.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and you compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1.",
  "translatedText": "Tegyük fel például, hogy végigmegyünk a folyamaton, amit most le fogok írni, és kiszámítjuk a negatív gradienst, és az itt lévő él súlyához tartozó komponens 3,2 lesz, míg az itt lévő élhez tartozó komponens 0,1 lesz.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give.",
  "translatedText": "Ezt úgy értelmezhetjük, hogy a függvény költsége 32-szer érzékenyebb az első súly változására, tehát ha csak egy kicsit is megingatjuk ezt az értéket, az a költségben némi változást okoz, és ez a változás 32-szer nagyobb, mint amit ugyanez a második súly megingatása eredményezne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all.",
  "translatedText": "Személy szerint, amikor először tanultam a backpropagationről, azt hiszem, a legzavaróbb aspektus a jelölés és az index kergetése volt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other.",
  "translatedText": "De ha egyszer kibontod, hogy mit is csinál valójában az algoritmus minden egyes része, akkor az egyes hatások valójában elég intuitívak, csak sok apró kiigazítás van egymásra rétegezve.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases.",
  "translatedText": "Ezért itt most a jelölések teljes figyelmen kívül hagyásával kezdem a dolgokat, és csak végigmegyek az egyes képzési példák súlyokra és torzításokra gyakorolt hatásain.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example.",
  "translatedText": "Mivel a költségfüggvény egy bizonyos költség átlagolását jelenti példánként az összes több tízezer gyakorló példa felett, a súlyok és torzítások beállításának módja egyetlen gradiens ereszkedési lépésnél szintén minden egyes példától függ.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step.",
  "translatedText": "Vagyis elvileg kellene, de a számítási hatékonyság érdekében később egy kis trükköt alkalmazunk, hogy ne kelljen minden egyes lépésnél minden egyes példát leütni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2.",
  "translatedText": "Más esetekben, most csak egyetlen példára fogunk koncentrálni, erre a képre, amely egy 2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted?",
  "translatedText": "Milyen hatással kell lennie ennek az egy képzési példának a súlyok és torzítások beállítására?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on.",
  "translatedText": "Tegyük fel, hogy egy olyan ponton vagyunk, ahol a hálózat még nincs jól betanítva, így az aktivációk a kimeneten eléggé véletlenszerűnek fognak tűnni, talán valami 0,5, 0,8, 0,2, és így tovább.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases.",
  "translatedText": "Ezeket az aktiválásokat közvetlenül nem tudjuk megváltoztatni, csak a súlyokra és az előfeszítésekre van befolyásunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 242.52,
  "end": 247.16
 },
 {
  "input": "But it's helpful to keep track of which adjustments we wish should take place to that output layer.",
  "translatedText": "De hasznos, ha nyomon követjük, hogy milyen beállításokat szeretnénk elvégezni az adott kimeneti rétegen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 247.16,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down.",
  "translatedText": "És mivel azt akarjuk, hogy a képet 2-esnek minősítse, azt akarjuk, hogy a harmadik értéket feljebb tolja, míg a többit lefelé tolja.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value.",
  "translatedText": "Ezen túlmenően ezeknek a lökéseknek a méretének arányosnak kell lennie azzal, hogy az egyes aktuális értékek milyen messze vannak a célértéktől.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be.",
  "translatedText": "Például a 2-es számú neuron aktivációjának növekedése bizonyos értelemben fontosabb, mint a 8-as számú neuron csökkenése, amely már elég közel van ahhoz, ahol lennie kellene.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase.",
  "translatedText": "Tehát tovább nagyítva, koncentráljunk csak erre az egy neuronra, arra, amelynek az aktivációját növelni szeretnénk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU.",
  "translatedText": "Ne feledjük, hogy az aktiválás az előző rétegben lévő összes aktiválás bizonyos súlyozott összegeként van definiálva, plusz egy előfeszítés, amelyet aztán mindet valami olyan függvénybe illesztünk, mint a szigmoid squishification függvény, vagy egy ReLU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation.",
  "translatedText": "Tehát három különböző útvonal van, amelyek együttesen segíthetnek az aktiválás növelésében.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer.",
  "translatedText": "Növelheti az előfeszítést, növelheti a súlyokat, és megváltoztathatja az előző réteg aktiválását.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence.",
  "translatedText": "A súlyok beállításának módjára összpontosítva figyeljük meg, hogy a súlyok valójában különböző mértékű befolyással bírnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values.",
  "translatedText": "Az előző réteg legvilágosabb neuronjaihoz tartozó kapcsolatoknak van a legnagyobb hatása, mivel ezek a súlyok nagyobb aktiválási értékekkel vannak megszorozva.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned.",
  "translatedText": "Tehát ha növeljük az egyik ilyen súlyt, az valójában erősebb hatással van a végső költségfüggvényre, mint a halványabb neuronokkal rendelkező kapcsolatok súlyának növelése, legalábbis ami ezt az egy képzési példát illeti.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck.",
  "translatedText": "Ne feledje, amikor a gradiens süllyedésről beszélünk, nem csak az érdekel minket, hogy az egyes komponenseket felfelé vagy lefelé kell-e tolni, hanem az is, hogy melyik adja a legtöbbet a pénzéért.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together.",
  "translatedText": "Ez egyébként legalábbis némileg emlékeztet az idegtudományok egyik elméletére, amely a biológiai neuronhálózatok tanulásának módjára vonatkozik, a Hebb-elméletre, amelyet gyakran úgy foglalnak össze, hogy az együtt tüzelő neuronok összekapcsolódnak.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active.",
  "translatedText": "Itt a legnagyobb súlynövekedés, a kapcsolatok legnagyobb megerősödése a legaktívabb neuronok között történik, és azok között, amelyeket szeretnénk aktívabbá tenni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those are the ones firing when thinking about a 2.",
  "translatedText": "Bizonyos értelemben azok az idegsejtek, amelyek akkor tüzelnek, amikor egy 2-est látunk, erősebben kapcsolódnak azokhoz, amelyek akkor tüzelnek, amikor egy 2-esre gondolunk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note.",
  "translatedText": "Tisztázzunk valamit, nem vagyok abban a helyzetben, hogy így vagy úgy nyilatkozzam arról, hogy a neuronok mesterséges hálózatai úgy viselkednek-e, mint a biológiai agyak, és ez a tüzek együtt drótoznak össze, és az ötlethez tartozik egy-két értelmes csillag, de nagyon laza analógiának tekintve érdekesnek találom megjegyezni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer.",
  "translatedText": "Mindenesetre a harmadik mód, amivel segíthetünk növelni ennek a neuronnak az aktivációját, az az előző réteg összes aktivációjának megváltoztatása.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active.",
  "translatedText": "Ha ugyanis minden, ami pozitív súllyal kapcsolódik a 2-es számjegyű neuronhoz, világosabbá válna, és ha minden, ami negatív súllyal kapcsolódik, halványabbá válna, akkor a 2-es számjegyű neuron aktívabbá válna.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights.",
  "translatedText": "És a súlyváltozásokhoz hasonlóan, a legtöbbet akkor kapod a pénzedért, ha a megfelelő súlyok méretével arányos változásokat keresel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases.",
  "translatedText": "Természetesen nem tudjuk közvetlenül befolyásolni ezeket az aktiválásokat, csak a súlyok és az előfeszítések felett rendelkezünk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are.",
  "translatedText": "De az utolsó réteghez hasonlóan hasznos megjegyezni, hogy mik ezek a kívánt változások.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants.",
  "translatedText": "De ne feledjük, hogy egy lépéssel kicsinyítve, ez csak az, amit a 2-es számjegyű kimeneti neuron akar.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer.",
  "translatedText": "Ne feledjük, hogy az utolsó réteg összes többi neuronja is kevésbé aktív, és minden egyes kimeneti neuronnak megvannak a saját gondolatai arról, hogy mi történjen az utolsó előtti réteggel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So, the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change.",
  "translatedText": "Tehát ennek a 2. számjegyű neuronnak a kívánsága összeadódik az összes többi kimeneti neuron kívánságával, hogy mi történjen az utolsó előtti réteggel, ismét a megfelelő súlyok arányában, és annak arányában, hogy az egyes neuronoknak mennyit kell változniuk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in.",
  "translatedText": "Itt jön a képbe a visszafelé terjedés gondolata.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer.",
  "translatedText": "Ha ezeket a kívánt hatásokat összeadjuk, akkor alapvetően egy listát kapunk azokról a lökésekről, amelyeket az utolsó előtti réteggel szeretnénk elérni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network.",
  "translatedText": "És ha ezek megvannak, akkor ugyanezt a folyamatot rekurzívan alkalmazhatjuk a releváns súlyokra és torzításokra, amelyek meghatározzák ezeket az értékeket, megismételve ugyanazt a folyamatot, amelyen az imént végigmentem, és visszafelé haladva a hálózaton.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases.",
  "translatedText": "És ha egy kicsit tovább nagyítunk, ne feledjük, hogy mindez csak azt jelenti, hogy egyetlen gyakorló példa hogyan kívánja az egyes súlyokat és torzításokat befolyásolni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2.",
  "translatedText": "Ha csak arra figyelnénk, hogy mit akar ez a 2, a hálózatot végül arra ösztönöznénk, hogy minden képet 2-esnek minősítsen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes.",
  "translatedText": "Tehát az a teendőd, hogy ugyanezt a backprop rutint végigcsinálod minden más képzési példánál, rögzíted, hogy mindegyikük hogyan szeretné megváltoztatni a súlyokat és az előfeszítéseket, és ezeket a kívánt változásokat átlagolod.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it.",
  "translatedText": "Az egyes súlyok és torzítások átlagolt lökéseinek gyűjteménye itt az utolsó videóban említett költségfüggvény negatív gradiensét, vagy legalábbis valami ahhoz arányosat jelent.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing.",
  "translatedText": "Csak azért mondom, hogy lazán, mert még nem tudok kvantitatívan pontosabban beszélni ezekről a lökésekről, de ha megértettél minden változást, amire az imént utaltam, hogy miért arányosan nagyobbak egyesek, mint mások, és hogyan kell ezeket összeadni, akkor megérted a mechanikát, amit a backpropagation valójában csinál.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step.",
  "translatedText": "Egyébként a gyakorlatban a számítógépeknek rendkívül sok időbe telik, hogy minden egyes edzéspélda hatását összeadják minden egyes gradiens ereszkedési lépésben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead.",
  "translatedText": "Ezért a következőt szokták tenni helyette.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and then divide it into a whole bunch of mini-batches, let's say each one having 100 training examples.",
  "translatedText": "A képzési adatokat véletlenszerűen összekevered, majd egy csomó minitételre osztod, mondjuk, mindegyikben 100 képzési példa van.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch.",
  "translatedText": "Ezután kiszámít egy lépést a minitételnek megfelelően.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not going to be the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speedup.",
  "translatedText": "Ez nem lesz a költségfüggvény tényleges gradiense, amely az összes képzési adattól függ, nem pedig ettől az apró részhalmaztól, így nem ez a leghatékonyabb lépés lefelé, de minden egyes mini-batch elég jó közelítést ad, és ami még fontosabb, jelentős számítási sebességnövekedést eredményez.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction.",
  "translatedText": "Ha a hálózatod pályáját a vonatkozó költségfelület alatt ábrázolnád, az egy kicsit inkább hasonlítana egy részeg emberre, aki céltalanul botorkál lefelé a hegyről, de gyors lépéseket tesz, mint egy gondosan számító emberre, aki minden egyes lépés pontos lefelé irányuló irányát meghatározza, mielőtt nagyon lassan és óvatosan lépne abba az irányba.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent.",
  "translatedText": "Ezt a technikát sztochasztikus gradiens ereszkedésnek nevezik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we?",
  "translatedText": "Sok minden történik itt, úgyhogy foglaljuk össze magunknak, jó?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost.",
  "translatedText": "A backpropagation az az algoritmus, amely meghatározza, hogy egyetlen gyakorló példa hogyan szeretné eltolni a súlyokat és az előfeszítéseket, nem csak abból a szempontból, hogy felfelé vagy lefelé menjenek, hanem abból a szempontból is, hogy e változások milyen relatív arányban okozzák a leggyorsabb csökkenést a költségben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens of thousands of training examples and averaging the desired changes you get.",
  "translatedText": "Egy valódi gradiens süllyedés lépése azt jelentené, hogy ezt a több tízezer gyakorló példán végzi el, és átlagolja a kívánt változásokat, amelyeket kap.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.26,
  "end": 664.2
 },
 {
  "input": "But that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch.",
  "translatedText": "De ez számítási szempontból lassú, ezért ehelyett az adatokat véletlenszerűen minitételekre osztjuk, és minden egyes lépést egy minitételre vonatkoztatva számolunk ki.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 664.86,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all of the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples.",
  "translatedText": "Ha ismételten végigmegy az összes minitételen, és elvégzi ezeket a beállításokat, akkor a költségfüggvény lokális minimuma felé konvergál, ami azt jelenti, hogy a hálózat végül nagyon jó munkát fog végezni a képzési példákon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms.",
  "translatedText": "Mindezzel együtt, minden egyes sor kód, ami a backprop megvalósításához szükséges, valójában megfelel valaminek, amit most láttál, legalábbis informálisan.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing.",
  "translatedText": "De néha az, hogy tudjuk, mit csinál a matematika, csak a csata fele, és csak a dolog ábrázolása az, ahol az egész zavarossá és zavarossá válik.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources.",
  "translatedText": "Tehát azok számára, akik szeretnének mélyebbre menni, a következő videó ugyanazokat az ötleteket veszi sorra, amelyeket az imént bemutattunk, de a mögöttes számítás szempontjából, ami remélhetőleg egy kicsit ismerősebbé teszi a témát, mivel más forrásokban is látják a témát.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data.",
  "translatedText": "Előtte egy dolgot érdemes hangsúlyozni: ahhoz, hogy ez az algoritmus működjön - és ez a neurális hálózatokon túl mindenféle gépi tanulásra vonatkozik -, sok gyakorló adatra van szükség.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans.",
  "translatedText": "A mi esetünkben a kézzel írt számjegyeket az teszi ilyen szép példává, hogy létezik az MNIST adatbázis, amely nagyon sok olyan példát tartalmaz, amelyet emberek jelöltek meg.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with.",
  "translatedText": "A gépi tanulásban dolgozók számára ismerős kihívás, hogy a ténylegesen szükséges, címkézett képzési adatokhoz jussanak, legyen szó akár több tízezer kép vagy bármilyen más adattípus címkézéséről.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]