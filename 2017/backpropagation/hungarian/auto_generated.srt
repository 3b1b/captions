1
00:00:00,000 --> 00:00:04,552
Itt foglalkozunk a visszaterjesztéssel, a neurális

2
00:00:04,552 --> 00:00:09,640
hálózatok tanulási folyamatának alapvető algoritmusával.

3
00:00:09,640 --> 00:00:13,640
Miután röviden összefoglalom, hol tartunk, először egy intuitív áttekintést teszek

4
00:00:13,640 --> 00:00:17,400
arról, hogy mit is csinál az algoritmus, a képletekre való hivatkozás nélkül.

5
00:00:17,400 --> 00:00:20,647
Aztán azok számára, akik szeretnének belemerülni a matematikába, a

6
00:00:20,647 --> 00:00:24,040
következő videó a mindezek alapjául szolgáló kalkulussal foglalkozik.

7
00:00:24,040 --> 00:00:27,582
Ha megnézte az utolsó két videót, vagy ha csak a megfelelő háttérrel ugrik be,

8
00:00:27,582 --> 00:00:31,080
akkor tudja, mi az a neurális hálózat, és hogyan továbbítja az információkat.

9
00:00:31,080 --> 00:00:35,352
Itt a klasszikus példát tesszük a kézzel írt számjegyek felismerésére, amelyek

10
00:00:35,352 --> 00:00:39,678
pixelértékei a hálózat első rétegébe kerülnek, 784 neuronnal, és bemutattam egy

11
00:00:39,678 --> 00:00:44,328
hálózatot két rejtett réteggel, amelyek mindegyike mindössze 16 neuronból áll, és egy

12
00:00:44,328 --> 00:00:48,979
kimenet. 10 neuronból álló réteg, jelezve, hogy a hálózat melyik számjegyet választja

13
00:00:48,979 --> 00:00:49,520
válaszul.

14
00:00:49,520 --> 00:00:53,815
Azt is elvárom tőled, hogy megértsd a gradiens süllyedést, amint azt az utolsó

15
00:00:53,815 --> 00:00:58,165
videóban leírtuk, és hogyan értjük tanulás alatt azt, hogy meg akarjuk találni,

16
00:00:58,165 --> 00:01:02,080
mely súlyok és torzítások minimalizálnak egy bizonyos költségfüggvényt.

17
00:01:02,080 --> 00:01:06,711
Gyors emlékeztetőként, egyetlen betanítási példa költségéhez vegye

18
00:01:06,711 --> 00:01:11,274
ki a hálózat által adott kimenetet a kívánt kimenettel együtt, és

19
00:01:11,274 --> 00:01:15,560
adja össze az egyes összetevők közötti különbségek négyzetét.

20
00:01:15,560 --> 00:01:19,359
Ha ezt megteszi a több tízezer képzési példájához, és átlagolja

21
00:01:19,359 --> 00:01:23,040
az eredményeket, akkor ez megadja a hálózat teljes költségét.

22
00:01:23,040 --> 00:01:27,879
Mintha nem lenne elég ezen gondolkodni, ahogy az utolsó videóban is le

23
00:01:27,879 --> 00:01:32,991
van írva, a keresett dolog ennek a költségfüggvénynek a negatív gradiense,

24
00:01:32,991 --> 00:01:38,104
amely megmondja, hogyan kell módosítania az összes súlyozást és torzítást,

25
00:01:38,104 --> 00:01:43,080
ezeket a kapcsolatokat a költségek leghatékonyabb csökkentése érdekében.

26
00:01:43,080 --> 00:01:46,450
A backpropagation, ennek a videónak a témája, egy algoritmus

27
00:01:46,450 --> 00:01:49,600
ennek az őrülten bonyolult gradiensnek a kiszámításához.

28
00:01:49,600 --> 00:01:54,803
Az utolsó videó egyetlen gondolata, amit nagyon szeretném, ha most szilárdan a fejedben

29
00:01:54,803 --> 00:01:59,652
tartsd, az az, hogy mivel a gradiens vektort 13 000 dimenziós iránynak tekinteni,

30
00:01:59,652 --> 00:02:04,620
enyhén szólva túlmutat a képzeletünkön, van egy másik ahogyan gondolkodhatsz rajta.

31
00:02:04,620 --> 00:02:07,747
Az egyes komponensek nagysága itt megmutatja, hogy a

32
00:02:07,747 --> 00:02:11,820
költségfüggvény mennyire érzékeny az egyes súlyokra és torzításokra.

33
00:02:11,820 --> 00:02:17,178
Tegyük fel például, hogy végigmegy azon a folyamaton, amelyet leírok, és kiszámítja

34
00:02:17,178 --> 00:02:22,091
a negatív gradienst, és az ezen az élen lévő súlyhoz tartozó összetevő itt 3

35
00:02:22,091 --> 00:02:26,940
lesz. 2, míg az ehhez az élhez tartozó komponens itt 0-ként jelenik meg. 1.

36
00:02:26,940 --> 00:02:31,583
Ezt úgy értelmeznéd, hogy a függvény költsége 32-szer érzékenyebb az

37
00:02:31,583 --> 00:02:36,293
első súly változásaira, tehát ha egy kicsit mozgatnád ezt az értéket,

38
00:02:36,293 --> 00:02:41,004
az némi változást fog okozni a költségekben, és ez a változás 32-szer

39
00:02:41,004 --> 00:02:45,580
nagyobb, mint amit az adott második súlynak ugyanaz a mozgása adna.

40
00:02:45,580 --> 00:02:50,662
Személy szerint, amikor először tanultam a visszaszaporításról, azt

41
00:02:50,662 --> 00:02:55,820
hiszem, a legzavaróbb szempont az egész jelölése és indexelése volt.

42
00:02:55,820 --> 00:02:59,933
De ha egyszer kibontja, hogy ennek az algoritmusnak az egyes részei valójában

43
00:02:59,933 --> 00:03:04,047
mit csinálnak, minden egyes hatás, amelyet kifejtenek, valójában meglehetősen

44
00:03:04,047 --> 00:03:07,740
intuitív, csak arról van szó, hogy sok apró beállítás kerül egymásra.

45
00:03:07,740 --> 00:03:12,673
Úgyhogy a jelölések teljes figyelmen kívül hagyásával kezdem a dolgokat, és csak végig

46
00:03:12,673 --> 00:03:17,380
kell lépnem az egyes edzési példák súlyozására és torzításaira gyakorolt hatásain.

47
00:03:17,380 --> 00:03:22,400
Mivel a költségfüggvény magában foglalja egy bizonyos példánkénti költség átlagolását

48
00:03:22,400 --> 00:03:27,070
a több tízezer képzési példában, az is, hogy hogyan állítjuk be a súlyokat és a

49
00:03:27,070 --> 00:03:31,740
torzításokat egyetlen gradiens süllyedési lépéshez, minden egyes példától függ.

50
00:03:31,740 --> 00:03:35,722
Illetve elvileg kellene, de a számítási hatékonyság érdekében később teszünk

51
00:03:35,722 --> 00:03:39,860
egy kis trükköt, hogy ne kelljen minden egyes példát eltalálni minden lépésnél.

52
00:03:39,860 --> 00:03:44,350
Más esetekben jelenleg csak egyetlen példára összpontosítjuk

53
00:03:44,350 --> 00:03:46,780
figyelmünket, erre a 2-es képre.

54
00:03:46,780 --> 00:03:49,233
Milyen hatással lehet ennek az egyetlen edzési

55
00:03:49,233 --> 00:03:51,740
példának a súlyok és a torzítások beállítására?

56
00:03:51,740 --> 00:03:55,268
Tegyük fel, hogy egy olyan ponton vagyunk, ahol a hálózat még

57
00:03:55,268 --> 00:03:58,455
nem megfelelően képzett, így a kimenet aktiválásai elég

58
00:03:58,455 --> 00:04:02,780
véletlenszerűek lesznek, talán valami 0-nak. 5, 0.8, 0.2, tovább és tovább.

59
00:04:02,780 --> 00:04:06,202
Ezeket az aktiválásokat közvetlenül nem tudjuk megváltoztatni, csak a

60
00:04:06,202 --> 00:04:09,820
súlyokra és torzításokra van befolyásunk, de hasznos nyomon követni, hogy

61
00:04:09,820 --> 00:04:13,340
az adott kimeneti rétegen milyen módosításokat szeretnénk végrehajtani.

62
00:04:13,340 --> 00:04:17,489
És mivel azt akarjuk, hogy a képet 2-esnek minősítse, azt akarjuk,

63
00:04:17,489 --> 00:04:21,700
hogy a harmadik érték felfelé, míg az összes többi lefelé kerüljön.

64
00:04:21,700 --> 00:04:25,789
Ezen túlmenően, ezeknek a lökéseknek a méretének arányosnak kell lennie

65
00:04:25,789 --> 00:04:30,220
azzal, hogy az egyes aktuális értékek milyen távolságra vannak a célértéktől.

66
00:04:30,220 --> 00:04:36,276
Például a 2-es számú neuron aktiválásának növekedése bizonyos értelemben fontosabb, mint

67
00:04:36,276 --> 00:04:42,060
a 8-as számú neuron csökkenése, amely már elég közel van ahhoz, ahol lennie kellene.

68
00:04:42,060 --> 00:04:44,755
Tehát tovább közelítve csak erre az egyetlen neuronra

69
00:04:44,755 --> 00:04:47,900
koncentráljunk, arra, amelynek aktiválását szeretnénk növelni.

70
00:04:47,900 --> 00:04:52,432
Ne feledje, hogy az aktiválás az előző réteg összes aktiválásának egy bizonyos

71
00:04:52,432 --> 00:04:56,908
súlyozott összegeként van definiálva, plusz egy torzítás, amely azután valami

72
00:04:56,908 --> 00:05:01,900
olyasmihez van csatlakoztatva, mint a szigmoid squishification függvény vagy egy ReLU.

73
00:05:01,900 --> 00:05:05,103
Tehát három különböző út áll rendelkezésre, amelyek

74
00:05:05,103 --> 00:05:08,060
összefoghatnak az aktiválás növelése érdekében.

75
00:05:08,060 --> 00:05:15,300
Növelheti a torzítást, növelheti a súlyokat, és módosíthatja az előző réteg aktiválásait.

76
00:05:15,300 --> 00:05:18,477
Arra összpontosítva, hogyan kell beállítani a súlyokat, figyelje

77
00:05:18,477 --> 00:05:21,460
meg, hogy a súlyok valójában milyen mértékben befolyásolják.

78
00:05:21,460 --> 00:05:26,504
Az előző réteg legfényesebb neuronjaival való kapcsolatoknak van a legnagyobb

79
00:05:26,504 --> 00:05:31,420
hatása, mivel ezek a súlyok megszorozódnak a nagyobb aktiválási értékekkel.

80
00:05:31,420 --> 00:05:35,560
Tehát, ha növelné az egyik súlyt, az valójában erősebb hatással van a

81
00:05:35,560 --> 00:05:39,820
végső költségfüggvényre, mint a halványabb neuronokkal való kapcsolatok

82
00:05:39,820 --> 00:05:44,020
súlyának növelése, legalábbis ami ezt az egy gyakorlati példát illeti.

83
00:05:44,020 --> 00:05:47,403
Ne feledje, amikor gradiens süllyedésről beszélünk, nem csak azzal

84
00:05:47,403 --> 00:05:50,434
foglalkozunk, hogy az egyes komponensek felfelé vagy lefelé

85
00:05:50,434 --> 00:05:54,020
mozduljanak el, hanem az is, hogy melyik adják a legtöbbet a pénzéért.

86
00:05:54,020 --> 00:05:58,112
Ez egyébként legalább valamelyest emlékeztet az idegtudomány egy elméletére, amely

87
00:05:58,112 --> 00:06:02,205
szerint a neuronok biológiai hálózatai hogyan tanulnak, a hebbi elméletet, amelyet

88
00:06:02,205 --> 00:06:06,594
gyakran a következő kifejezéssel foglalnak össze: az idegsejtek, amelyek együtt tüzelnek

89
00:06:06,594 --> 00:06:06,940
össze.

90
00:06:06,940 --> 00:06:12,520
Itt a legnagyobb súlynövekedés, a kapcsolatok legnagyobb erősödése a

91
00:06:12,520 --> 00:06:18,100
legaktívabb és az aktívabbá tenni kívánt idegsejtek között történik.

92
00:06:18,100 --> 00:06:21,614
Bizonyos értelemben azok a neuronok, amelyek tüzelnek, miközben 2-t

93
00:06:21,614 --> 00:06:25,440
látnak, erősebben kapcsolódnak azokhoz, amelyek tüzelnek, ha rágondolunk.

94
00:06:25,440 --> 00:06:29,209
Az egyértelműség kedvéért nem vagyok abban a helyzetben, hogy ilyen vagy olyan

95
00:06:29,209 --> 00:06:33,170
kijelentéseket tegyek arról, hogy a mesterséges neuronhálózatok úgy viselkednek-e,

96
00:06:33,170 --> 00:06:36,940
mint a biológiai agyak, és ez az ötlet összekapcsolja a vezetékeket, és néhány

97
00:06:36,940 --> 00:06:41,139
jelentőségteljes csillaggal együtt jár, de nagyon laza. hasonlattal, érdekesnek találom

98
00:06:41,139 --> 00:06:41,760
megjegyezni.

99
00:06:41,760 --> 00:06:45,376
Egyébként a harmadik módja annak, hogy fokozzuk ennek a neuronnak az

100
00:06:45,376 --> 00:06:49,360
aktiválását, az az, hogy megváltoztatjuk az előző réteg összes aktiválását.

101
00:06:49,360 --> 00:06:53,627
Ugyanis, ha a pozitív súllyal rendelkező 2-es számjegyű neuronhoz

102
00:06:53,627 --> 00:06:57,830
kapcsolódó minden fényesebbé válna, és ha minden negatív súllyal

103
00:06:57,830 --> 00:07:02,680
kapcsolatos halványodna, akkor az a 2-es számjegyű neuron aktívabbá válna.

104
00:07:02,680 --> 00:07:06,731
És hasonlóan a súlyváltozásokhoz, a legtöbbet úgy érheti el, ha olyan

105
00:07:06,731 --> 00:07:10,840
változtatásokat keres, amelyek arányosak a megfelelő súlyok méretével.

106
00:07:10,840 --> 00:07:14,377
Természetesen ezeket az aktiválásokat közvetlenül nem tudjuk

107
00:07:14,377 --> 00:07:18,320
befolyásolni, csak a súlyokat és a torzításokat tudjuk ellenőrizni.

108
00:07:18,320 --> 00:07:21,624
De csakúgy, mint az utolsó rétegnél, hasznos feljegyezni,

109
00:07:21,624 --> 00:07:23,960
hogy melyek ezek a kívánt változtatások.

110
00:07:23,960 --> 00:07:26,939
De ne feledje, ha itt egy lépést kicsinyít, csak

111
00:07:26,939 --> 00:07:30,040
ez az, amit a 2-es számjegyű kimeneti neuron akar.

112
00:07:30,040 --> 00:07:34,258
Ne feledje, azt is szeretnénk, hogy az utolsó rétegben lévő összes

113
00:07:34,258 --> 00:07:38,855
többi neuron kevésbé legyen aktív, és ezeknek a többi kimeneti neuronnak

114
00:07:38,855 --> 00:07:43,200
megvan a maga gondolata arról, hogy mi történjen az utolsó réteggel.

115
00:07:43,200 --> 00:07:47,835
Tehát ennek a 2-es számjegyű neuronnak a vágya összeadódik az összes

116
00:07:47,835 --> 00:07:52,134
többi kimeneti neuron azon vágyaival, hogy mi történjen ezzel a

117
00:07:52,134 --> 00:07:56,836
második-utolsó réteggel, ismét a megfelelő súlyok arányában, és annak

118
00:07:56,836 --> 00:08:01,740
arányában, hogy mennyire van szüksége az egyes neuronoknak. változtatni.

119
00:08:01,740 --> 00:08:05,940
Itt jön a képbe a visszafelé terjedés ötlete.

120
00:08:05,940 --> 00:08:10,120
Ha ezeket a kívánt hatásokat összeadjuk, akkor alapvetően egy listát kapunk

121
00:08:10,120 --> 00:08:14,300
azokról a lökésekről, amelyeket ezzel az utolsó réteggel szeretnénk elérni.

122
00:08:14,300 --> 00:08:19,220
És ha ezek megvannak, rekurzív módon alkalmazhatja ugyanazt a folyamatot a releváns

123
00:08:19,220 --> 00:08:24,024
súlyokra és torzításokra, amelyek meghatározzák ezeket az értékeket, megismételve

124
00:08:24,024 --> 00:08:29,180
ugyanazt a folyamatot, amelyen az imént végigmentem, és visszafelé haladva a hálózaton.

125
00:08:29,180 --> 00:08:33,411
És kicsit tovább kicsinyítve, ne feledje, hogy egyetlen edzési példa

126
00:08:33,411 --> 00:08:37,520
csak így kívánja elmozdítani ezeket a súlyokat és elfogultságokat.

127
00:08:37,520 --> 00:08:40,883
Ha csak arra figyelnénk, hogy mit akar ez a 2, akkor a hálózat

128
00:08:40,883 --> 00:08:44,140
végül arra ösztönözne, hogy minden képet 2-esnek minősítsen.

129
00:08:44,140 --> 00:08:50,080
Tehát ugyanazt a backprop rutint kell végrehajtania minden más edzési

130
00:08:50,080 --> 00:08:56,020
példánál, rögzítve, hogy mindegyikük hogyan szeretné megváltoztatni a

131
00:08:56,020 --> 00:09:02,300
súlyokat és a torzításokat, és együtt átlagolja a kívánt változtatásokat.

132
00:09:02,300 --> 00:09:06,105
Az egyes súlyokra és torzításokra vonatkozó átlagolt lökések itt

133
00:09:06,105 --> 00:09:10,086
található gyűjteménye, lazán szólva, az utolsó videóban hivatkozott

134
00:09:10,086 --> 00:09:14,360
költségfüggvény negatív gradiense, vagy legalábbis valami azzal arányos.

135
00:09:14,360 --> 00:09:19,107
Csak azért mondom lazán szólva, mert még nem kell mennyiségileg pontosítani

136
00:09:19,107 --> 00:09:23,855
ezeket a lökéseket, de ha megértetted az imént hivatkozott változtatásokat,

137
00:09:23,855 --> 00:09:28,790
miért nagyobbak egyesek arányosan nagyobbak, mint mások, és hogyan kell ezeket

138
00:09:28,790 --> 00:09:34,100
összeadni, akkor megérted a mechanikát. hogy valójában mit csinál a backpropagation.

139
00:09:34,100 --> 00:09:38,640
Egyébként a gyakorlatban a számítógépeknek rendkívül sok időbe telik, hogy

140
00:09:38,640 --> 00:09:43,120
minden edzéspélda hatását összeadják minden gradiens süllyedési lépésnél.

141
00:09:43,120 --> 00:09:45,540
Tehát itt van, amit általában csinálnak helyette.

142
00:09:45,540 --> 00:09:49,460
Véletlenszerűen összekeveri az edzési adatokat, és felosztja egy

143
00:09:49,460 --> 00:09:53,380
csomó mini kötegre, mondjuk mindegyiknek 100 edzési példája van.

144
00:09:53,380 --> 00:09:56,980
Ezután kiszámít egy lépést a mini-köteg szerint.

145
00:09:56,980 --> 00:10:02,109
Ez nem a költségfüggvény tényleges gradiense, amely az összes betanítási adattól függ,

146
00:10:02,109 --> 00:10:07,239
nem ettől az apró részhalmaztól, tehát nem ez a leghatékonyabb lépés lefelé, de minden

147
00:10:07,239 --> 00:10:12,310
mini köteg elég jó közelítést ad, és ami még fontosabb. jelentős számítási sebességet

148
00:10:12,310 --> 00:10:12,900
biztosít.

149
00:10:12,900 --> 00:10:17,497
Ha a megfelelő költségfelület alatt ábrázolná a hálózatának pályáját, az egy kicsit

150
00:10:17,497 --> 00:10:22,095
inkább olyan lenne, mint egy részeg ember, aki céltalanul botorkál le a dombról, de

151
00:10:22,095 --> 00:10:26,693
gyors lépéseket tesz, nem pedig egy gondosan számító ember, aki meghatározza minden

152
00:10:26,693 --> 00:10:31,620
lépés pontos lefelé irányát. mielőtt nagyon lassú és óvatos lépést tenne abba az irányba.

153
00:10:31,620 --> 00:10:35,200
Ezt a technikát sztochasztikus gradiens süllyedésnek nevezik.

154
00:10:35,200 --> 00:10:40,400
Sok minden történik itt, úgyhogy foglaljuk össze magunknak, jó?

155
00:10:40,400 --> 00:10:44,347
A visszapropagálás az az algoritmus, amely meghatározza, hogy egy edzési példa

156
00:10:44,347 --> 00:10:48,344
hogyan kívánja eltolni a súlyokat és torzításokat, nemcsak abból a szempontból,

157
00:10:48,344 --> 00:10:52,292
hogy felfelé vagy lefelé kell-e menni, hanem abból a szempontból, hogy ezeknek

158
00:10:52,292 --> 00:10:56,240
a változásoknak milyen relatív aránya okozza a leggyorsabb csökkenést költség.

159
00:10:56,240 --> 00:11:00,680
Egy igazi gradiens süllyedési lépés azt jelentené, hogy ezt minden tíz

160
00:11:00,680 --> 00:11:04,932
és ezer képzési példánál meg kell tenni, és átlagolni kell a kívánt

161
00:11:04,932 --> 00:11:09,560
változtatásokat, de ez számításilag lassú, ezért ehelyett véletlenszerűen

162
00:11:09,560 --> 00:11:14,000
felosztja az adatokat mini kötegekre, és minden lépést egy mini-tétel.

163
00:11:14,000 --> 00:11:18,288
Az összes mini-kötegelt ismételten végignézve és végrehajtva ezeket a

164
00:11:18,288 --> 00:11:22,699
beállításokat, a költségfüggvény helyi minimuma felé közeledik, ami azt

165
00:11:22,699 --> 00:11:27,540
jelenti, hogy a hálózat végül nagyon jó munkát fog végezni a képzési példákon.

166
00:11:27,540 --> 00:11:32,845
Tehát mindezekkel együtt minden kódsor, amely a backprop megvalósításához felhasználható,

167
00:11:32,845 --> 00:11:37,680
valójában megfelel valaminek, amit most láttál, legalábbis informális értelemben.

168
00:11:37,680 --> 00:11:41,253
De néha csak a fele a sikernek tudnia, hogy mit csinál a matematika, és csak

169
00:11:41,253 --> 00:11:44,780
az átkozott dolgot képviselni az, ahol minden zavarossá és zavarossá válik.

170
00:11:44,780 --> 00:11:49,127
Tehát azok számára, akik szeretnének mélyebbre menni, a következő videó ugyanazokat

171
00:11:49,127 --> 00:11:53,112
a gondolatokat járja át, amelyeket itt bemutattunk, de a mögöttes kalkuláció

172
00:11:53,112 --> 00:11:57,460
tekintetében, ami remélhetőleg egy kicsit ismerősebbé teszi a témát egyéb források.

173
00:11:57,460 --> 00:12:00,671
Előtte érdemes hangsúlyozni, hogy ahhoz, hogy ez az algoritmus

174
00:12:00,671 --> 00:12:03,781
működjön, és ez a neurális hálózatokon kívül mindenféle gépi

175
00:12:03,781 --> 00:12:06,840
tanulásra is vonatkozik, sok betanítási adatra van szükség.

176
00:12:06,840 --> 00:12:09,686
A mi esetünkben az egyik dolog, ami a kézzel írt számjegyeket

177
00:12:09,686 --> 00:12:12,716
ilyen szép példává teszi, az az, hogy létezik az MNIST adatbázis,

178
00:12:12,716 --> 00:12:15,380
rengeteg olyan példával, amelyeket emberek címkéztek fel.

179
00:12:15,380 --> 00:12:19,370
Tehát a gépi tanulásban dolgozók számára ismert gyakori kihívás az, hogy megkapja a

180
00:12:19,370 --> 00:12:23,314
ténylegesen szükséges címkézett képzési adatokat, legyen szó akár több tízezer kép

181
00:12:23,314 --> 00:12:27,400
felcímkézéséről, vagy bármilyen más adattípusról, amellyel esetleg foglalkoznia kell.

