1
00:00:00,000 --> 00:00:04,552
Itt foglalkozunk a visszaterjesztéssel, a neurális

2
00:00:04,552 --> 00:00:09,640
hálózatok tanulási folyamatának alapvető algoritmusával.

3
00:00:09,640 --> 00:00:13,977
Miután röviden összefoglalom, hol tartunk, először egy intuitív áttekintést teszek arról,

4
00:00:13,977 --> 00:00:17,400
hogy mit is csinál az algoritmus, a képletekre való hivatkozás nélkül.

5
00:00:17,400 --> 00:00:20,550
Aztán azok számára, akik szeretnének belemerülni a matematikába,

6
00:00:20,550 --> 00:00:24,040
a következő videó a mindezek alapjául szolgáló kalkulussal foglalkozik.

7
00:00:24,040 --> 00:00:27,582
Ha megnézte az utolsó két videót, vagy ha csak a megfelelő háttérrel ugrik be,

8
00:00:27,582 --> 00:00:31,080
akkor tudja, mi az a neurális hálózat, és hogyan továbbítja az információkat.

9
00:00:31,080 --> 00:00:34,919
Itt a klasszikus példát tesszük a kézzel írt számjegyek felismerésére,

10
00:00:34,919 --> 00:00:38,704
amelyek pixelértékei a hálózat első rétegébe kerülnek, 784 neuronnal,

11
00:00:38,704 --> 00:00:41,408
és bemutattam egy hálózatot két rejtett réteggel,

12
00:00:41,408 --> 00:00:46,167
amelyek mindegyike mindössze 16 neuronból áll, és egy kimenet. 10 neuronból álló réteg,

13
00:00:46,167 --> 00:00:49,520
jelezve, hogy a hálózat melyik számjegyet választja válaszul.

14
00:00:49,520 --> 00:00:52,727
Azt is elvárom tőled, hogy megértsd a gradiens süllyedést,

15
00:00:52,727 --> 00:00:56,751
amint azt az utolsó videóban leírtuk, és hogyan értjük tanulás alatt azt,

16
00:00:56,751 --> 00:01:01,101
hogy meg akarjuk találni, mely súlyok és torzítások minimalizálnak egy bizonyos

17
00:01:01,101 --> 00:01:02,080
költségfüggvényt.

18
00:01:02,080 --> 00:01:06,711
Gyors emlékeztetőként, egyetlen betanítási példa költségéhez vegye

19
00:01:06,711 --> 00:01:11,066
ki a hálózat által adott kimenetet a kívánt kimenettel együtt,

20
00:01:11,066 --> 00:01:15,560
és adja össze az egyes összetevők közötti különbségek négyzetét.

21
00:01:15,560 --> 00:01:18,587
Ha ezt megteszi a több tízezer képzési példájához,

22
00:01:18,587 --> 00:01:23,040
és átlagolja az eredményeket, akkor ez megadja a hálózat teljes költségét.

23
00:01:23,040 --> 00:01:28,561
Mintha nem lenne elég ezen gondolkodni, ahogy az utolsó videóban is le van írva,

24
00:01:28,561 --> 00:01:32,991
a keresett dolog ennek a költségfüggvénynek a negatív gradiense,

25
00:01:32,991 --> 00:01:38,104
amely megmondja, hogyan kell módosítania az összes súlyozást és torzítást,

26
00:01:38,104 --> 00:01:43,080
ezeket a kapcsolatokat a költségek leghatékonyabb csökkentése érdekében.

27
00:01:43,080 --> 00:01:46,450
A backpropagation, ennek a videónak a témája, egy algoritmus

28
00:01:46,450 --> 00:01:49,600
ennek az őrülten bonyolult gradiensnek a kiszámításához.

29
00:01:49,600 --> 00:01:53,088
Az utolsó videó egyetlen gondolata, amit nagyon szeretném,

30
00:01:53,088 --> 00:01:57,878
ha most szilárdan a fejedben tartsd, az az, hogy mivel a gradiens vektort 13 000

31
00:01:57,878 --> 00:02:02,018
dimenziós iránynak tekinteni, enyhén szólva túlmutat a képzeletünkön,

32
00:02:02,018 --> 00:02:04,620
van egy másik ahogyan gondolkodhatsz rajta.

33
00:02:04,620 --> 00:02:07,747
Az egyes komponensek nagysága itt megmutatja, hogy a

34
00:02:07,747 --> 00:02:11,820
költségfüggvény mennyire érzékeny az egyes súlyokra és torzításokra.

35
00:02:11,820 --> 00:02:16,285
Tegyük fel például, hogy végigmegy azon a folyamaton, amelyet leírok,

36
00:02:16,285 --> 00:02:21,070
és kiszámítja a negatív gradienst, és az ezen az élen lévő súlyhoz tartozó

37
00:02:21,070 --> 00:02:26,748
összetevő itt 3 lesz. 2, míg az ehhez az élhez tartozó komponens itt 0-ként jelenik meg.

38
00:02:26,748 --> 00:02:26,940
1.

39
00:02:26,940 --> 00:02:31,583
Ezt úgy értelmeznéd, hogy a függvény költsége 32-szer érzékenyebb az

40
00:02:31,583 --> 00:02:36,293
első súly változásaira, tehát ha egy kicsit mozgatnád ezt az értéket,

41
00:02:36,293 --> 00:02:41,609
az némi változást fog okozni a költségekben, és ez a változás 32-szer nagyobb,

42
00:02:41,609 --> 00:02:45,580
mint amit az adott második súlynak ugyanaz a mozgása adna.

43
00:02:45,580 --> 00:02:50,363
Személy szerint, amikor először tanultam a visszaszaporításról,

44
00:02:50,363 --> 00:02:55,820
azt hiszem, a legzavaróbb szempont az egész jelölése és indexelése volt.

45
00:02:55,820 --> 00:02:59,933
De ha egyszer kibontja, hogy ennek az algoritmusnak az egyes részei valójában

46
00:02:59,933 --> 00:03:04,575
mit csinálnak, minden egyes hatás, amelyet kifejtenek, valójában meglehetősen intuitív,

47
00:03:04,575 --> 00:03:07,740
csak arról van szó, hogy sok apró beállítás kerül egymásra.

48
00:03:07,740 --> 00:03:11,879
Úgyhogy a jelölések teljes figyelmen kívül hagyásával kezdem a dolgokat,

49
00:03:11,879 --> 00:03:16,812
és csak végig kell lépnem az egyes edzési példák súlyozására és torzításaira gyakorolt

50
00:03:16,812 --> 00:03:17,380
hatásain.

51
00:03:17,380 --> 00:03:22,400
Mivel a költségfüggvény magában foglalja egy bizonyos példánkénti költség átlagolását

52
00:03:22,400 --> 00:03:27,070
a több tízezer képzési példában, az is, hogy hogyan állítjuk be a súlyokat és a

53
00:03:27,070 --> 00:03:31,740
torzításokat egyetlen gradiens süllyedési lépéshez, minden egyes példától függ.

54
00:03:31,740 --> 00:03:35,722
Illetve elvileg kellene, de a számítási hatékonyság érdekében később teszünk

55
00:03:35,722 --> 00:03:39,860
egy kis trükköt, hogy ne kelljen minden egyes példát eltalálni minden lépésnél.

56
00:03:39,860 --> 00:03:45,381
Más esetekben jelenleg csak egyetlen példára összpontosítjuk figyelmünket,

57
00:03:45,381 --> 00:03:46,780
erre a 2-es képre.

58
00:03:46,780 --> 00:03:49,233
Milyen hatással lehet ennek az egyetlen edzési

59
00:03:49,233 --> 00:03:51,740
példának a súlyok és a torzítások beállítására?

60
00:03:51,740 --> 00:03:56,690
Tegyük fel, hogy egy olyan ponton vagyunk, ahol a hálózat még nem megfelelően képzett,

61
00:03:56,690 --> 00:03:59,877
így a kimenet aktiválásai elég véletlenszerűek lesznek,

62
00:03:59,877 --> 00:04:02,780
talán valami 0-nak. 5, 0.8, 0.2, tovább és tovább.

63
00:04:02,780 --> 00:04:05,860
Ezeket az aktiválásokat közvetlenül nem tudjuk megváltoztatni,

64
00:04:05,860 --> 00:04:09,575
csak a súlyokra és torzításokra van befolyásunk, de hasznos nyomon követni,

65
00:04:09,575 --> 00:04:13,340
hogy az adott kimeneti rétegen milyen módosításokat szeretnénk végrehajtani.

66
00:04:13,340 --> 00:04:17,489
És mivel azt akarjuk, hogy a képet 2-esnek minősítse, azt akarjuk,

67
00:04:17,489 --> 00:04:21,700
hogy a harmadik érték felfelé, míg az összes többi lefelé kerüljön.

68
00:04:21,700 --> 00:04:26,187
Ezen túlmenően, ezeknek a lökéseknek a méretének arányosnak kell lennie azzal,

69
00:04:26,187 --> 00:04:30,220
hogy az egyes aktuális értékek milyen távolságra vannak a célértéktől.

70
00:04:30,220 --> 00:04:35,935
Például a 2-es számú neuron aktiválásának növekedése bizonyos értelemben fontosabb,

71
00:04:35,935 --> 00:04:42,060
mint a 8-as számú neuron csökkenése, amely már elég közel van ahhoz, ahol lennie kellene.

72
00:04:42,060 --> 00:04:45,554
Tehát tovább közelítve csak erre az egyetlen neuronra koncentráljunk,

73
00:04:45,554 --> 00:04:47,900
arra, amelynek aktiválását szeretnénk növelni.

74
00:04:47,900 --> 00:04:52,432
Ne feledje, hogy az aktiválás az előző réteg összes aktiválásának egy bizonyos

75
00:04:52,432 --> 00:04:55,760
súlyozott összegeként van definiálva, plusz egy torzítás,

76
00:04:55,760 --> 00:04:58,686
amely azután valami olyasmihez van csatlakoztatva,

77
00:04:58,686 --> 00:05:01,900
mint a szigmoid squishification függvény vagy egy ReLU.

78
00:05:01,900 --> 00:05:05,103
Tehát három különböző út áll rendelkezésre, amelyek

79
00:05:05,103 --> 00:05:08,060
összefoghatnak az aktiválás növelése érdekében.

80
00:05:08,060 --> 00:05:15,300
Növelheti a torzítást, növelheti a súlyokat, és módosíthatja az előző réteg aktiválásait.

81
00:05:15,300 --> 00:05:18,037
Arra összpontosítva, hogyan kell beállítani a súlyokat,

82
00:05:18,037 --> 00:05:21,460
figyelje meg, hogy a súlyok valójában milyen mértékben befolyásolják.

83
00:05:21,460 --> 00:05:27,022
Az előző réteg legfényesebb neuronjaival való kapcsolatoknak van a legnagyobb hatása,

84
00:05:27,022 --> 00:05:31,420
mivel ezek a súlyok megszorozódnak a nagyobb aktiválási értékekkel.

85
00:05:31,420 --> 00:05:35,560
Tehát, ha növelné az egyik súlyt, az valójában erősebb hatással van a

86
00:05:35,560 --> 00:05:39,820
végső költségfüggvényre, mint a halványabb neuronokkal való kapcsolatok

87
00:05:39,820 --> 00:05:44,020
súlyának növelése, legalábbis ami ezt az egy gyakorlati példát illeti.

88
00:05:44,020 --> 00:05:46,646
Ne feledje, amikor gradiens süllyedésről beszélünk,

89
00:05:46,646 --> 00:05:50,080
nem csak azzal foglalkozunk, hogy az egyes komponensek felfelé vagy

90
00:05:50,080 --> 00:05:54,020
lefelé mozduljanak el, hanem az is, hogy melyik adják a legtöbbet a pénzéért.

91
00:05:54,020 --> 00:05:57,817
Ez egyébként legalább valamelyest emlékeztet az idegtudomány egy elméletére,

92
00:05:57,817 --> 00:06:01,811
amely szerint a neuronok biológiai hálózatai hogyan tanulnak, a hebbi elméletet,

93
00:06:01,811 --> 00:06:05,411
amelyet gyakran a következő kifejezéssel foglalnak össze: az idegsejtek,

94
00:06:05,411 --> 00:06:06,940
amelyek együtt tüzelnek össze.

95
00:06:06,940 --> 00:06:12,520
Itt a legnagyobb súlynövekedés, a kapcsolatok legnagyobb erősödése a

96
00:06:12,520 --> 00:06:18,100
legaktívabb és az aktívabbá tenni kívánt idegsejtek között történik.

97
00:06:18,100 --> 00:06:22,028
Bizonyos értelemben azok a neuronok, amelyek tüzelnek, miközben 2-t látnak,

98
00:06:22,028 --> 00:06:25,440
erősebben kapcsolódnak azokhoz, amelyek tüzelnek, ha rágondolunk.

99
00:06:25,440 --> 00:06:28,160
Az egyértelműség kedvéért nem vagyok abban a helyzetben,

100
00:06:28,160 --> 00:06:30,593
hogy ilyen vagy olyan kijelentéseket tegyek arról,

101
00:06:30,593 --> 00:06:34,315
hogy a mesterséges neuronhálózatok úgy viselkednek-e, mint a biológiai agyak,

102
00:06:34,315 --> 00:06:38,276
és ez az ötlet összekapcsolja a vezetékeket, és néhány jelentőségteljes csillaggal

103
00:06:38,276 --> 00:06:41,760
együtt jár, de nagyon laza. hasonlattal, érdekesnek találom megjegyezni.

104
00:06:41,760 --> 00:06:46,057
Egyébként a harmadik módja annak, hogy fokozzuk ennek a neuronnak az aktiválását,

105
00:06:46,057 --> 00:06:49,360
az az, hogy megváltoztatjuk az előző réteg összes aktiválását.

106
00:06:49,360 --> 00:06:53,627
Ugyanis, ha a pozitív súllyal rendelkező 2-es számjegyű neuronhoz

107
00:06:53,627 --> 00:06:59,446
kapcsolódó minden fényesebbé válna, és ha minden negatív súllyal kapcsolatos halványodna,

108
00:06:59,446 --> 00:07:02,680
akkor az a 2-es számjegyű neuron aktívabbá válna.

109
00:07:02,680 --> 00:07:06,210
És hasonlóan a súlyváltozásokhoz, a legtöbbet úgy érheti el,

110
00:07:06,210 --> 00:07:10,840
ha olyan változtatásokat keres, amelyek arányosak a megfelelő súlyok méretével.

111
00:07:10,840 --> 00:07:15,188
Természetesen ezeket az aktiválásokat közvetlenül nem tudjuk befolyásolni,

112
00:07:15,188 --> 00:07:18,320
csak a súlyokat és a torzításokat tudjuk ellenőrizni.

113
00:07:18,320 --> 00:07:21,624
De csakúgy, mint az utolsó rétegnél, hasznos feljegyezni,

114
00:07:21,624 --> 00:07:23,960
hogy melyek ezek a kívánt változtatások.

115
00:07:23,960 --> 00:07:27,364
De ne feledje, ha itt egy lépést kicsinyít, csak ez az,

116
00:07:27,364 --> 00:07:30,040
amit a 2-es számjegyű kimeneti neuron akar.

117
00:07:30,040 --> 00:07:34,258
Ne feledje, azt is szeretnénk, hogy az utolsó rétegben lévő összes

118
00:07:34,258 --> 00:07:38,855
többi neuron kevésbé legyen aktív, és ezeknek a többi kimeneti neuronnak

119
00:07:38,855 --> 00:07:43,200
megvan a maga gondolata arról, hogy mi történjen az utolsó réteggel.

120
00:07:43,200 --> 00:07:47,835
Tehát ennek a 2-es számjegyű neuronnak a vágya összeadódik az összes

121
00:07:47,835 --> 00:07:53,813
többi kimeneti neuron azon vágyaival, hogy mi történjen ezzel a második-utolsó réteggel,

122
00:07:53,813 --> 00:07:57,575
ismét a megfelelő súlyok arányában, és annak arányában,

123
00:07:57,575 --> 00:08:01,740
hogy mennyire van szüksége az egyes neuronoknak. változtatni.

124
00:08:01,740 --> 00:08:05,940
Itt jön a képbe a visszafelé terjedés ötlete.

125
00:08:05,940 --> 00:08:10,120
Ha ezeket a kívánt hatásokat összeadjuk, akkor alapvetően egy listát kapunk

126
00:08:10,120 --> 00:08:14,300
azokról a lökésekről, amelyeket ezzel az utolsó réteggel szeretnénk elérni.

127
00:08:14,300 --> 00:08:19,220
És ha ezek megvannak, rekurzív módon alkalmazhatja ugyanazt a folyamatot a releváns

128
00:08:19,220 --> 00:08:23,263
súlyokra és torzításokra, amelyek meghatározzák ezeket az értékeket,

129
00:08:23,263 --> 00:08:27,129
megismételve ugyanazt a folyamatot, amelyen az imént végigmentem,

130
00:08:27,129 --> 00:08:29,180
és visszafelé haladva a hálózaton.

131
00:08:29,180 --> 00:08:33,411
És kicsit tovább kicsinyítve, ne feledje, hogy egyetlen edzési példa

132
00:08:33,411 --> 00:08:37,520
csak így kívánja elmozdítani ezeket a súlyokat és elfogultságokat.

133
00:08:37,520 --> 00:08:40,029
Ha csak arra figyelnénk, hogy mit akar ez a 2,

134
00:08:40,029 --> 00:08:44,140
akkor a hálózat végül arra ösztönözne, hogy minden képet 2-esnek minősítsen.

135
00:08:44,140 --> 00:08:50,928
Tehát ugyanazt a backprop rutint kell végrehajtania minden más edzési példánál,

136
00:08:50,928 --> 00:08:58,396
rögzítve, hogy mindegyikük hogyan szeretné megváltoztatni a súlyokat és a torzításokat,

137
00:08:58,396 --> 00:09:02,300
és együtt átlagolja a kívánt változtatásokat.

138
00:09:02,300 --> 00:09:07,451
Az egyes súlyokra és torzításokra vonatkozó átlagolt lökések itt található gyűjteménye,

139
00:09:07,451 --> 00:09:12,135
lazán szólva, az utolsó videóban hivatkozott költségfüggvény negatív gradiense,

140
00:09:12,135 --> 00:09:14,360
vagy legalábbis valami azzal arányos.

141
00:09:14,360 --> 00:09:19,107
Csak azért mondom lazán szólva, mert még nem kell mennyiségileg pontosítani

142
00:09:19,107 --> 00:09:23,855
ezeket a lökéseket, de ha megértetted az imént hivatkozott változtatásokat,

143
00:09:23,855 --> 00:09:29,477
miért nagyobbak egyesek arányosan nagyobbak, mint mások, és hogyan kell ezeket összeadni,

144
00:09:29,477 --> 00:09:34,100
akkor megérted a mechanikát. hogy valójában mit csinál a backpropagation.

145
00:09:34,100 --> 00:09:38,337
Egyébként a gyakorlatban a számítógépeknek rendkívül sok időbe telik,

146
00:09:38,337 --> 00:09:43,120
hogy minden edzéspélda hatását összeadják minden gradiens süllyedési lépésnél.

147
00:09:43,120 --> 00:09:45,540
Tehát itt van, amit általában csinálnak helyette.

148
00:09:45,540 --> 00:09:48,434
Véletlenszerűen összekeveri az edzési adatokat,

149
00:09:48,434 --> 00:09:53,380
és felosztja egy csomó mini kötegre, mondjuk mindegyiknek 100 edzési példája van.

150
00:09:53,380 --> 00:09:56,980
Ezután kiszámít egy lépést a mini-köteg szerint.

151
00:09:56,980 --> 00:10:02,109
Ez nem a költségfüggvény tényleges gradiense, amely az összes betanítási adattól függ,

152
00:10:02,109 --> 00:10:06,649
nem ettől az apró részhalmaztól, tehát nem ez a leghatékonyabb lépés lefelé,

153
00:10:06,649 --> 00:10:10,541
de minden mini köteg elég jó közelítést ad, és ami még fontosabb.

154
00:10:10,541 --> 00:10:12,900
jelentős számítási sebességet biztosít.

155
00:10:12,900 --> 00:10:16,731
Ha a megfelelő költségfelület alatt ábrázolná a hálózatának pályáját,

156
00:10:16,731 --> 00:10:19,851
az egy kicsit inkább olyan lenne, mint egy részeg ember,

157
00:10:19,851 --> 00:10:23,300
aki céltalanul botorkál le a dombról, de gyors lépéseket tesz,

158
00:10:23,300 --> 00:10:28,226
nem pedig egy gondosan számító ember, aki meghatározza minden lépés pontos lefelé irányát.

159
00:10:28,226 --> 00:10:31,620
mielőtt nagyon lassú és óvatos lépést tenne abba az irányba.

160
00:10:31,620 --> 00:10:35,200
Ezt a technikát sztochasztikus gradiens süllyedésnek nevezik.

161
00:10:35,200 --> 00:10:40,400
Sok minden történik itt, úgyhogy foglaljuk össze magunknak, jó?

162
00:10:40,400 --> 00:10:43,248
A visszapropagálás az az algoritmus, amely meghatározza,

163
00:10:43,248 --> 00:10:46,895
hogy egy edzési példa hogyan kívánja eltolni a súlyokat és torzításokat,

164
00:10:46,895 --> 00:10:50,293
nemcsak abból a szempontból, hogy felfelé vagy lefelé kell-e menni,

165
00:10:50,293 --> 00:10:54,191
hanem abból a szempontból, hogy ezeknek a változásoknak milyen relatív aránya

166
00:10:54,191 --> 00:10:56,240
okozza a leggyorsabb csökkenést költség.

167
00:10:56,240 --> 00:10:59,429
Egy igazi gradiens süllyedési lépés azt jelentené,

168
00:10:59,429 --> 00:11:03,243
hogy ezt minden tíz és ezer képzési példánál meg kell tenni,

169
00:11:03,243 --> 00:11:07,621
és átlagolni kell a kívánt változtatásokat, de ez számításilag lassú,

170
00:11:07,621 --> 00:11:11,936
ezért ehelyett véletlenszerűen felosztja az adatokat mini kötegekre,

171
00:11:11,936 --> 00:11:14,000
és minden lépést egy mini-tétel.

172
00:11:14,000 --> 00:11:19,207
Az összes mini-kötegelt ismételten végignézve és végrehajtva ezeket a beállításokat,

173
00:11:19,207 --> 00:11:23,251
a költségfüggvény helyi minimuma felé közeledik, ami azt jelenti,

174
00:11:23,251 --> 00:11:27,540
hogy a hálózat végül nagyon jó munkát fog végezni a képzési példákon.

175
00:11:27,540 --> 00:11:32,845
Tehát mindezekkel együtt minden kódsor, amely a backprop megvalósításához felhasználható,

176
00:11:32,845 --> 00:11:37,680
valójában megfelel valaminek, amit most láttál, legalábbis informális értelemben.

177
00:11:37,680 --> 00:11:40,881
De néha csak a fele a sikernek tudnia, hogy mit csinál a matematika,

178
00:11:40,881 --> 00:11:44,780
és csak az átkozott dolgot képviselni az, ahol minden zavarossá és zavarossá válik.

179
00:11:44,780 --> 00:11:47,574
Tehát azok számára, akik szeretnének mélyebbre menni,

180
00:11:47,574 --> 00:11:51,818
a következő videó ugyanazokat a gondolatokat járja át, amelyeket itt bemutattunk,

181
00:11:51,818 --> 00:11:55,907
de a mögöttes kalkuláció tekintetében, ami remélhetőleg egy kicsit ismerősebbé

182
00:11:55,907 --> 00:11:57,460
teszi a témát egyéb források.

183
00:11:57,460 --> 00:12:01,181
Előtte érdemes hangsúlyozni, hogy ahhoz, hogy ez az algoritmus működjön,

184
00:12:01,181 --> 00:12:05,055
és ez a neurális hálózatokon kívül mindenféle gépi tanulásra is vonatkozik,

185
00:12:05,055 --> 00:12:06,840
sok betanítási adatra van szükség.

186
00:12:06,840 --> 00:12:10,880
A mi esetünkben az egyik dolog, ami a kézzel írt számjegyeket ilyen szép példává teszi,

187
00:12:10,880 --> 00:12:13,864
az az, hogy létezik az MNIST adatbázis, rengeteg olyan példával,

188
00:12:13,864 --> 00:12:15,380
amelyeket emberek címkéztek fel.

189
00:12:15,380 --> 00:12:18,610
Tehát a gépi tanulásban dolgozók számára ismert gyakori kihívás az,

190
00:12:18,610 --> 00:12:21,746
hogy megkapja a ténylegesen szükséges címkézett képzési adatokat,

191
00:12:21,746 --> 00:12:25,689
legyen szó akár több tízezer kép felcímkézéséről, vagy bármilyen más adattípusról,

192
00:12:25,689 --> 00:12:27,400
amellyel esetleg foglalkoznia kell.

