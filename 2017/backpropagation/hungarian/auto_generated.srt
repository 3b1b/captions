1
00:00:04,059 --> 00:00:06,393
Itt a backpropagációval, a neurális hálózatok 

2
00:00:06,393 --> 00:00:08,880
tanulásának alapvető algoritmusával foglalkozunk.

3
00:00:09,400 --> 00:00:11,974
Egy gyors összefoglaló után, hogy hol tartunk, az első dolog, 

4
00:00:11,974 --> 00:00:15,504
amit teszek, egy intuitív áttekintés arról, hogy mit csinál az algoritmus valójában, 

5
00:00:15,504 --> 00:00:17,000
a képletekre való hivatkozás nélkül.

6
00:00:17,660 --> 00:00:19,939
Azoknak, akik szeretnének belemerülni a matematikába, 

7
00:00:19,939 --> 00:00:23,020
a következő videó a mindezek alapjául szolgáló számításokkal foglalkozik.

8
00:00:23,820 --> 00:00:27,618
Ha megnézted az előző két videót, vagy ha csak a megfelelő háttérrel ugrottál be, 

9
00:00:27,618 --> 00:00:31,000
akkor tudod, mi az a neurális hálózat, és hogyan táplálja az információt.

10
00:00:31,680 --> 00:00:35,428
Itt a kézzel írt számjegyek felismerésének klasszikus példáját mutatjuk be, 

11
00:00:35,428 --> 00:00:39,077
amelyek pixelértékei a 784 neuronból álló hálózat első rétegébe kerülnek, 

12
00:00:39,077 --> 00:00:43,368
és egy olyan hálózatot mutattam be, amelynek két rejtett rétege mindössze 16 neuronból 

13
00:00:43,368 --> 00:00:46,376
áll, és egy 10 neuronból álló kimeneti réteget, amely jelzi, 

14
00:00:46,376 --> 00:00:49,040
hogy a hálózat melyik számjegyet választja válaszként.

15
00:00:50,040 --> 00:00:52,607
Azt is elvárom, hogy megértsd a gradiens ereszkedést, 

16
00:00:52,607 --> 00:00:56,410
ahogyan azt az előző videóban leírtuk, és azt, hogy a tanulás alatt azt értjük, 

17
00:00:56,410 --> 00:01:00,023
hogy meg akarjuk találni, hogy mely súlyok és torzítások minimalizálnak egy 

18
00:01:00,023 --> 00:01:01,260
bizonyos költségfüggvényt.

19
00:01:02,040 --> 00:01:08,320
Gyors emlékeztetőül: egyetlen képzési példa költségére a hálózat által adott kimenetet és 

20
00:01:08,320 --> 00:01:14,600
a kívánt kimenetet veszi, és összeadja az egyes komponensek közötti különbségek négyzetét.

21
00:01:15,380 --> 00:01:18,376
Ha ezt az összes több tízezer képzési példára elvégezzük, 

22
00:01:18,376 --> 00:01:22,200
és az eredményeket átlagoljuk, akkor megkapjuk a hálózat teljes költségét.

23
00:01:22,200 --> 00:01:26,089
És mintha ez még nem lenne elég, ahogy az előző videóban is leírtuk, 

24
00:01:26,089 --> 00:01:30,147
a dolog, amit keresünk, ennek a költségfüggvénynek a negatív gradiense, 

25
00:01:30,147 --> 00:01:34,205
ami megmondja, hogyan kell megváltoztatni az összes súlyt és torzítást, 

26
00:01:34,205 --> 00:01:38,320
az összes kapcsolatot, hogy a leghatékonyabban csökkentsük a költségeket.

27
00:01:43,260 --> 00:01:46,084
A backpropagation, amelyről ez a videó szól, egy algoritmus 

28
00:01:46,084 --> 00:01:48,580
ennek az őrült bonyolult gradiensnek a kiszámítására.

29
00:01:49,140 --> 00:01:52,714
És az utolsó videóban elhangzott egy gondolat, amit most nagyon szeretném, 

30
00:01:52,714 --> 00:01:56,240
ha szilárdan a fejetekben tartanátok, hogy mivel a gradiens vektor 13 000 

31
00:01:56,240 --> 00:01:59,624
dimenzióban való irányként való elképzelése, hogy finoman fogalmazzak, 

32
00:01:59,624 --> 00:02:03,580
meghaladja a képzeletünk határait, van egy másik mód, ahogyan gondolkodhattok róla.

33
00:02:04,600 --> 00:02:07,166
Az egyes komponensek nagysága itt azt mutatja meg, 

34
00:02:07,166 --> 00:02:10,940
hogy a költségfüggvény mennyire érzékeny az egyes súlyokra és torzításokra.

35
00:02:11,800 --> 00:02:16,930
Tegyük fel például, hogy végigmegyünk a folyamaton, amit most le fogok írni, 

36
00:02:16,930 --> 00:02:21,662
és kiszámítjuk a negatív gradienst, és az itt lévő él súlyához tartozó 

37
00:02:21,662 --> 00:02:26,260
komponens 3,2 lesz, míg az itt lévő élhez tartozó komponens 0,1 lesz.

38
00:02:26,820 --> 00:02:30,822
Ezt úgy értelmezhetjük, hogy a függvény költsége 32-szer érzékenyebb 

39
00:02:30,822 --> 00:02:35,577
az első súly változására, tehát ha csak egy kicsit is megingatjuk ezt az értéket, 

40
00:02:35,577 --> 00:02:39,696
az a költségben némi változást okoz, és ez a változás 32-szer nagyobb, 

41
00:02:39,696 --> 00:02:43,060
mint amit ugyanez a második súly megingatása eredményezne.

42
00:02:48,420 --> 00:02:51,836
Személy szerint, amikor először tanultam a backpropagationről, 

43
00:02:51,836 --> 00:02:55,740
azt hiszem, a legzavaróbb aspektus a jelölés és az index kergetése volt.

44
00:02:56,220 --> 00:03:01,120
De ha egyszer kibontod, hogy mit is csinál valójában az algoritmus minden egyes része, 

45
00:03:01,120 --> 00:03:03,936
akkor az egyes hatások valójában elég intuitívak, 

46
00:03:03,936 --> 00:03:06,640
csak sok apró kiigazítás van egymásra rétegezve.

47
00:03:07,740 --> 00:03:11,730
Ezért itt most a jelölések teljes figyelmen kívül hagyásával kezdem a dolgokat, 

48
00:03:11,730 --> 00:03:16,120
és csak végigmegyek az egyes képzési példák súlyokra és torzításokra gyakorolt hatásain.

49
00:03:17,020 --> 00:03:21,654
Mivel a költségfüggvény egy bizonyos költség átlagolását jelenti példánként az 

50
00:03:21,654 --> 00:03:26,347
összes több tízezer gyakorló példa felett, a súlyok és torzítások beállításának 

51
00:03:26,347 --> 00:03:31,040
módja egyetlen gradiens ereszkedési lépésnél szintén minden egyes példától függ.

52
00:03:31,680 --> 00:03:35,579
Vagyis elvileg kellene, de a számítási hatékonyság érdekében később egy kis trükköt 

53
00:03:35,579 --> 00:03:39,200
alkalmazunk, hogy ne kelljen minden egyes lépésnél minden egyes példát leütni.

54
00:03:39,200 --> 00:03:45,960
Más esetekben, most csak egyetlen példára fogunk koncentrálni, erre a képre, amely egy 2.

55
00:03:46,720 --> 00:03:49,201
Milyen hatással kell lennie ennek az egy képzési 

56
00:03:49,201 --> 00:03:51,480
példának a súlyok és torzítások beállítására?

57
00:03:52,680 --> 00:03:56,688
Tegyük fel, hogy egy olyan ponton vagyunk, ahol a hálózat még nincs jól betanítva, 

58
00:03:56,688 --> 00:03:59,971
így az aktivációk a kimeneten eléggé véletlenszerűnek fognak tűnni, 

59
00:03:59,971 --> 00:04:02,000
talán valami 0,5, 0,8, 0,2, és így tovább.

60
00:04:02,520 --> 00:04:05,018
Ezeket az aktiválásokat közvetlenül nem tudjuk megváltoztatni, 

61
00:04:05,018 --> 00:04:07,160
csak a súlyokra és az előfeszítésekre van befolyásunk.

62
00:04:07,160 --> 00:04:10,153
De hasznos, ha nyomon követjük, hogy milyen beállításokat 

63
00:04:10,153 --> 00:04:12,580
szeretnénk elvégezni az adott kimeneti rétegen.

64
00:04:13,360 --> 00:04:17,369
És mivel azt akarjuk, hogy a képet 2-esnek minősítse, azt akarjuk, 

65
00:04:17,369 --> 00:04:21,260
hogy a harmadik értéket feljebb tolja, míg a többit lefelé tolja.

66
00:04:22,060 --> 00:04:26,100
Ezen túlmenően ezeknek a lökéseknek a méretének arányosnak kell lennie azzal, 

67
00:04:26,100 --> 00:04:29,520
hogy az egyes aktuális értékek milyen messze vannak a célértéktől.

68
00:04:30,220 --> 00:04:35,437
Például a 2-es számú neuron aktivációjának növekedése bizonyos értelemben fontosabb, 

69
00:04:35,437 --> 00:04:40,900
mint a 8-as számú neuron csökkenése, amely már elég közel van ahhoz, ahol lennie kellene.

70
00:04:42,040 --> 00:04:45,001
Tehát tovább nagyítva, koncentráljunk csak erre az egy neuronra, 

71
00:04:45,001 --> 00:04:47,280
arra, amelynek az aktivációját növelni szeretnénk.

72
00:04:48,180 --> 00:04:52,230
Ne feledjük, hogy az aktiválás az előző rétegben lévő összes aktiválás bizonyos 

73
00:04:52,230 --> 00:04:55,318
súlyozott összegeként van definiálva, plusz egy előfeszítés, 

74
00:04:55,318 --> 00:04:58,204
amelyet aztán mindet valami olyan függvénybe illesztünk, 

75
00:04:58,204 --> 00:05:01,040
mint a szigmoid squishification függvény, vagy egy ReLU.

76
00:05:01,640 --> 00:05:04,832
Tehát három különböző útvonal van, amelyek együttesen 

77
00:05:04,832 --> 00:05:07,020
segíthetnek az aktiválás növelésében.

78
00:05:07,440 --> 00:05:10,774
Növelheti az előfeszítést, növelheti a súlyokat, 

79
00:05:10,774 --> 00:05:14,040
és megváltoztathatja az előző réteg aktiválását.

80
00:05:14,940 --> 00:05:17,900
A súlyok beállításának módjára összpontosítva figyeljük meg, 

81
00:05:17,900 --> 00:05:20,860
hogy a súlyok valójában különböző mértékű befolyással bírnak.

82
00:05:21,440 --> 00:05:25,775
Az előző réteg legvilágosabb neuronjaihoz tartozó kapcsolatoknak van a legnagyobb hatása, 

83
00:05:25,775 --> 00:05:29,100
mivel ezek a súlyok nagyobb aktiválási értékekkel vannak megszorozva.

84
00:05:31,460 --> 00:05:35,484
Tehát ha növeljük az egyik ilyen súlyt, az valójában erősebb hatással van 

85
00:05:35,484 --> 00:05:39,183
a végső költségfüggvényre, mint a halványabb neuronokkal rendelkező 

86
00:05:39,183 --> 00:05:43,480
kapcsolatok súlyának növelése, legalábbis ami ezt az egy képzési példát illeti.

87
00:05:44,420 --> 00:05:46,832
Ne feledje, amikor a gradiens süllyedésről beszélünk, 

88
00:05:46,832 --> 00:05:50,852
nem csak az érdekel minket, hogy az egyes komponenseket felfelé vagy lefelé kell-e tolni, 

89
00:05:50,852 --> 00:05:53,220
hanem az is, hogy melyik adja a legtöbbet a pénzéért.

90
00:05:55,020 --> 00:05:58,649
Ez egyébként legalábbis némileg emlékeztet az idegtudományok egyik elméletére, 

91
00:05:58,649 --> 00:06:02,508
amely a biológiai neuronhálózatok tanulásának módjára vonatkozik, a Hebb-elméletre, 

92
00:06:02,508 --> 00:06:06,460
amelyet gyakran úgy foglalnak össze, hogy az együtt tüzelő neuronok összekapcsolódnak.

93
00:06:07,260 --> 00:06:12,423
Itt a legnagyobb súlynövekedés, a kapcsolatok legnagyobb megerősödése a legaktívabb 

94
00:06:12,423 --> 00:06:17,280
neuronok között történik, és azok között, amelyeket szeretnénk aktívabbá tenni.

95
00:06:17,940 --> 00:06:21,304
Bizonyos értelemben azok az idegsejtek, amelyek akkor tüzelnek, amikor egy 2-est látunk, 

96
00:06:21,304 --> 00:06:24,480
erősebben kapcsolódnak azokhoz, amelyek akkor tüzelnek, amikor egy 2-esre gondolunk.

97
00:06:25,400 --> 00:06:29,712
Tisztázzunk valamit, nem vagyok abban a helyzetben, hogy így vagy úgy nyilatkozzam arról, 

98
00:06:29,712 --> 00:06:33,593
hogy a neuronok mesterséges hálózatai úgy viselkednek-e, mint a biológiai agyak, 

99
00:06:33,593 --> 00:06:37,809
és ez a tüzek együtt drótoznak össze, és az ötlethez tartozik egy-két értelmes csillag, 

100
00:06:37,809 --> 00:06:41,020
de nagyon laza analógiának tekintve érdekesnek találom megjegyezni.

101
00:06:41,940 --> 00:06:45,514
Mindenesetre a harmadik mód, amivel segíthetünk növelni ennek a neuronnak 

102
00:06:45,514 --> 00:06:49,040
az aktivációját, az az előző réteg összes aktivációjának megváltoztatása.

103
00:06:49,040 --> 00:06:53,418
Ha ugyanis minden, ami pozitív súllyal kapcsolódik a 2-es számjegyű neuronhoz, 

104
00:06:53,418 --> 00:06:57,077
világosabbá válna, és ha minden, ami negatív súllyal kapcsolódik, 

105
00:06:57,077 --> 00:07:00,680
halványabbá válna, akkor a 2-es számjegyű neuron aktívabbá válna.

106
00:07:02,540 --> 00:07:06,703
És a súlyváltozásokhoz hasonlóan, a legtöbbet akkor kapod a pénzedért, 

107
00:07:06,703 --> 00:07:10,280
ha a megfelelő súlyok méretével arányos változásokat keresel.

108
00:07:12,140 --> 00:07:15,244
Természetesen nem tudjuk közvetlenül befolyásolni ezeket az aktiválásokat, 

109
00:07:15,244 --> 00:07:17,480
csak a súlyok és az előfeszítések felett rendelkezünk.

110
00:07:17,480 --> 00:07:24,120
De az utolsó réteghez hasonlóan hasznos megjegyezni, hogy mik ezek a kívánt változások.

111
00:07:24,580 --> 00:07:26,708
De ne feledjük, hogy egy lépéssel kicsinyítve, 

112
00:07:26,708 --> 00:07:29,200
ez csak az, amit a 2-es számjegyű kimeneti neuron akar.

113
00:07:29,760 --> 00:07:33,612
Ne feledjük, hogy az utolsó réteg összes többi neuronja is kevésbé aktív, 

114
00:07:33,612 --> 00:07:37,309
és minden egyes kimeneti neuronnak megvannak a saját gondolatai arról, 

115
00:07:37,309 --> 00:07:39,600
hogy mi történjen az utolsó előtti réteggel.

116
00:07:42,700 --> 00:07:48,706
Tehát ennek a 2. számjegyű neuronnak a kívánsága összeadódik az összes többi kimeneti 

117
00:07:48,706 --> 00:07:53,316
neuron kívánságával, hogy mi történjen az utolsó előtti réteggel, 

118
00:07:53,316 --> 00:07:57,227
ismét a megfelelő súlyok arányában, és annak arányában, 

119
00:07:57,227 --> 00:08:00,720
hogy az egyes neuronoknak mennyit kell változniuk.

120
00:08:01,600 --> 00:08:05,480
Itt jön a képbe a visszafelé terjedés gondolata.

121
00:08:05,820 --> 00:08:09,590
Ha ezeket a kívánt hatásokat összeadjuk, akkor alapvetően egy listát kapunk 

122
00:08:09,590 --> 00:08:13,360
azokról a lökésekről, amelyeket az utolsó előtti réteggel szeretnénk elérni.

123
00:08:14,220 --> 00:08:17,917
És ha ezek megvannak, akkor ugyanezt a folyamatot rekurzívan alkalmazhatjuk a releváns 

124
00:08:17,917 --> 00:08:20,850
súlyokra és torzításokra, amelyek meghatározzák ezeket az értékeket, 

125
00:08:20,850 --> 00:08:23,655
megismételve ugyanazt a folyamatot, amelyen az imént végigmentem, 

126
00:08:23,655 --> 00:08:25,100
és visszafelé haladva a hálózaton.

127
00:08:28,960 --> 00:08:32,670
És ha egy kicsit tovább nagyítunk, ne feledjük, hogy mindez csak azt jelenti, 

128
00:08:32,670 --> 00:08:36,381
hogy egyetlen gyakorló példa hogyan kívánja az egyes súlyokat és torzításokat 

129
00:08:36,381 --> 00:08:37,000
befolyásolni.

130
00:08:37,480 --> 00:08:39,709
Ha csak arra figyelnénk, hogy mit akar ez a 2, 

131
00:08:39,709 --> 00:08:43,220
a hálózatot végül arra ösztönöznénk, hogy minden képet 2-esnek minősítsen.

132
00:08:44,059 --> 00:08:48,074
Tehát az a teendőd, hogy ugyanezt a backprop rutint végigcsinálod minden más 

133
00:08:48,074 --> 00:08:52,037
képzési példánál, rögzíted, hogy mindegyikük hogyan szeretné megváltoztatni 

134
00:08:52,037 --> 00:08:56,000
a súlyokat és az előfeszítéseket, és ezeket a kívánt változásokat átlagolod.

135
00:09:01,720 --> 00:09:07,496
Az egyes súlyok és torzítások átlagolt lökéseinek gyűjteménye itt az utolsó videóban 

136
00:09:07,496 --> 00:09:13,204
említett költségfüggvény negatív gradiensét, vagy legalábbis valami ahhoz arányosat 

137
00:09:13,204 --> 00:09:13,680
jelent.

138
00:09:14,380 --> 00:09:18,314
Csak azért mondom, hogy lazán, mert még nem tudok kvantitatívan pontosabban 

139
00:09:18,314 --> 00:09:21,783
beszélni ezekről a lökésekről, de ha megértettél minden változást, 

140
00:09:21,783 --> 00:09:25,718
amire az imént utaltam, hogy miért arányosan nagyobbak egyesek, mint mások, 

141
00:09:25,718 --> 00:09:28,928
és hogyan kell ezeket összeadni, akkor megérted a mechanikát, 

142
00:09:28,928 --> 00:09:31,000
amit a backpropagation valójában csinál.

143
00:09:33,960 --> 00:09:37,646
Egyébként a gyakorlatban a számítógépeknek rendkívül sok időbe telik, 

144
00:09:37,646 --> 00:09:41,965
hogy minden egyes edzéspélda hatását összeadják minden egyes gradiens ereszkedési 

145
00:09:41,965 --> 00:09:42,440
lépésben.

146
00:09:43,140 --> 00:09:44,820
Ezért a következőt szokták tenni helyette.

147
00:09:45,480 --> 00:09:48,136
A képzési adatokat véletlenszerűen összekevered, 

148
00:09:48,136 --> 00:09:52,420
majd egy csomó minitételre osztod, mondjuk, mindegyikben 100 képzési példa van.

149
00:09:52,940 --> 00:09:56,200
Ezután kiszámít egy lépést a minitételnek megfelelően.

150
00:09:56,960 --> 00:10:01,596
Ez nem lesz a költségfüggvény tényleges gradiense, amely az összes képzési adattól függ, 

151
00:10:01,596 --> 00:10:05,816
nem pedig ettől az apró részhalmaztól, így nem ez a leghatékonyabb lépés lefelé, 

152
00:10:05,816 --> 00:10:09,567
de minden egyes mini-batch elég jó közelítést ad, és ami még fontosabb, 

153
00:10:09,567 --> 00:10:12,120
jelentős számítási sebességnövekedést eredményez.

154
00:10:12,820 --> 00:10:16,258
Ha a hálózatod pályáját a vonatkozó költségfelület alatt ábrázolnád, 

155
00:10:16,258 --> 00:10:18,898
az egy kicsit inkább hasonlítana egy részeg emberre, 

156
00:10:18,898 --> 00:10:22,237
aki céltalanul botorkál lefelé a hegyről, de gyors lépéseket tesz, 

157
00:10:22,237 --> 00:10:26,672
mint egy gondosan számító emberre, aki minden egyes lépés pontos lefelé irányuló irányát 

158
00:10:26,672 --> 00:10:30,160
meghatározza, mielőtt nagyon lassan és óvatosan lépne abba az irányba.

159
00:10:31,540 --> 00:10:34,660
Ezt a technikát sztochasztikus gradiens ereszkedésnek nevezik.

160
00:10:35,960 --> 00:10:39,620
Sok minden történik itt, úgyhogy foglaljuk össze magunknak, jó?

161
00:10:40,440 --> 00:10:43,037
A backpropagation az az algoritmus, amely meghatározza, 

162
00:10:43,037 --> 00:10:47,072
hogy egyetlen gyakorló példa hogyan szeretné eltolni a súlyokat és az előfeszítéseket, 

163
00:10:47,072 --> 00:10:50,087
nem csak abból a szempontból, hogy felfelé vagy lefelé menjenek, 

164
00:10:50,087 --> 00:10:53,797
hanem abból a szempontból is, hogy e változások milyen relatív arányban okozzák 

165
00:10:53,797 --> 00:10:55,560
a leggyorsabb csökkenést a költségben.

166
00:10:56,260 --> 00:10:58,976
Egy valódi gradiens süllyedés lépése azt jelentené, 

167
00:10:58,976 --> 00:11:03,468
hogy ezt a több tízezer gyakorló példán végzi el, és átlagolja a kívánt változásokat, 

168
00:11:03,468 --> 00:11:04,200
amelyeket kap.

169
00:11:04,860 --> 00:11:08,774
De ez számítási szempontból lassú, ezért ehelyett az adatokat véletlenszerűen 

170
00:11:08,774 --> 00:11:13,240
minitételekre osztjuk, és minden egyes lépést egy minitételre vonatkoztatva számolunk ki.

171
00:11:14,000 --> 00:11:18,205
Ha ismételten végigmegy az összes minitételen, és elvégzi ezeket a beállításokat, 

172
00:11:18,205 --> 00:11:22,001
akkor a költségfüggvény lokális minimuma felé konvergál, ami azt jelenti, 

173
00:11:22,001 --> 00:11:25,540
hogy a hálózat végül nagyon jó munkát fog végezni a képzési példákon.

174
00:11:27,240 --> 00:11:32,316
Mindezzel együtt, minden egyes sor kód, ami a backprop megvalósításához szükséges, 

175
00:11:32,316 --> 00:11:36,720
valójában megfelel valaminek, amit most láttál, legalábbis informálisan.

176
00:11:37,560 --> 00:11:40,725
De néha az, hogy tudjuk, mit csinál a matematika, csak a csata fele, 

177
00:11:40,725 --> 00:11:44,120
és csak a dolog ábrázolása az, ahol az egész zavarossá és zavarossá válik.

178
00:11:44,860 --> 00:11:47,129
Tehát azok számára, akik szeretnének mélyebbre menni, 

179
00:11:47,129 --> 00:11:49,483
a következő videó ugyanazokat az ötleteket veszi sorra, 

180
00:11:49,483 --> 00:11:52,426
amelyeket az imént bemutattunk, de a mögöttes számítás szempontjából, 

181
00:11:52,426 --> 00:11:54,738
ami remélhetőleg egy kicsit ismerősebbé teszi a témát, 

182
00:11:54,738 --> 00:11:56,420
mivel más forrásokban is látják a témát.

183
00:11:57,340 --> 00:11:59,514
Előtte egy dolgot érdemes hangsúlyozni: ahhoz, 

184
00:11:59,514 --> 00:12:02,383
hogy ez az algoritmus működjön - és ez a neurális hálózatokon 

185
00:12:02,383 --> 00:12:05,900
túl mindenféle gépi tanulásra vonatkozik -, sok gyakorló adatra van szükség.

186
00:12:06,420 --> 00:12:09,814
A mi esetünkben a kézzel írt számjegyeket az teszi ilyen szép példává, 

187
00:12:09,814 --> 00:12:13,353
hogy létezik az MNIST adatbázis, amely nagyon sok olyan példát tartalmaz, 

188
00:12:13,353 --> 00:12:14,740
amelyet emberek jelöltek meg.

189
00:12:15,300 --> 00:12:18,446
A gépi tanulásban dolgozók számára ismerős kihívás, 

190
00:12:18,446 --> 00:12:22,561
hogy a ténylegesen szükséges, címkézett képzési adatokhoz jussanak, 

191
00:12:22,561 --> 00:12:27,100
legyen szó akár több tízezer kép vagy bármilyen más adattípus címkézéséről.

