1
00:00:04,350 --> 00:00:06,410
여기서 우리는 신경망이 학습을 하는지에 대한

2
00:00:06,410 --> 00:00:09,400
핵심 알고리즘인 역전파에 대해 알아볼 겁니다

3
00:00:09,400 --> 00:00:10,960
우리가 알고있는 부분을 간략히 요약 하고,

4
00:00:10,960 --> 00:00:15,220
첫 번째로 알고리즘이 실제로 무엇을 하는지 어떠한 공식도 사용하지 않고

5
00:00:15,470 --> 00:00:17,260
직관적으로 살펴보겠습니다.

6
00:00:17,640 --> 00:00:20,310
수학적인 공식에 대해 더욱 알고 싶어 하는 사람들을 위해

7
00:00:20,310 --> 00:00:23,140
다음 비디오는이 모든 것을 설명해주는 미적분학에 들어갑니다 ㅎㅎ.

8
00:00:23,940 --> 00:00:25,550
마지막 두 개의 동영상을 본 경우

9
00:00:25,550 --> 00:00:27,920
또는 적당한 배경지식을 가지고 이 영상을 본다면

10
00:00:27,920 --> 00:00:31,290
당신은 신경망이 무엇인지, 그것이 어떻게 정보를 전달 하는지를 알고있을겁니다.

11
00:00:31,660 --> 00:00:35,100
여기서 우리는 손으로 쓴 숫자를 인식하는 신경망의 예를 들겠습니다.

12
00:00:35,100 --> 00:00:39,930
이 신경망은 입력층이 784개이고,

13
00:00:39,930 --> 00:00:44,000
은닉층은 각각 16개의 신경을 가지고 있으며,

14
00:00:44,000 --> 00:00:49,250
어떤 숫자인지 표시해줄 10개의 출력층이 있습니다.

15
00:00:50,020 --> 00:00:54,340
저는 여러분들이 저번 영상에서 설명한 경사 하강법에 대해서 이해하고 있고

16
00:00:54,340 --> 00:00:56,890
신경망이 배운다는 것이

17
00:00:56,890 --> 00:01:01,450
오차 함수를 최소화 시키는 가중치와 편향을 구하는 것이라는 것을 알고 있다고 가정하겠습니다.

18
00:01:02,000 --> 00:01:05,460
오차에 관한 한가지 학습데이터를 살펴봅시다.

19
00:01:05,460 --> 00:01:08,400
우리가 해야할 것은 신경망이 출력한것과

20
00:01:08,400 --> 00:01:10,860
신경망이 출력하기를 바랬던 값들을

21
00:01:11,200 --> 00:01:14,820
가져와 차이를 구한후 모두 더합니다.

22
00:01:15,360 --> 00:01:20,020
이걸 수천가지의 학습 데이터에 대해 수행하고 평균을 얻으면

23
00:01:20,020 --> 00:01:22,420
당신은 신경망의 모든 오차를 구할수 있습니다.

24
00:01:22,910 --> 00:01:26,010
그리고 저번 영상에서 설명한것 처럼

25
00:01:26,010 --> 00:01:30,870
우리가 찾고있는 것은 이 오차 함수의 음의 기울기입니다.

26
00:01:30,870 --> 00:01:35,720
이것은 모든 가중치와 편향,이 모든 연결을 어떻게 변경해야 하는지를 알려줍니다.

27
00:01:35,720 --> 00:01:38,270
이러한 방식으로 가장 효율적으로 오차함수를 줄일 수 있습니다.

28
00:01:42,950 --> 00:01:45,210
이 영상의 주제인 역전파는

29
00:01:45,210 --> 00:01:48,800
그 많은 복잡한 기울기를 계산하기위한 알고리즘입니다.

30
00:01:49,480 --> 00:01:54,000
그리고 저번 영상에서 설명한 것을 지금 잘 기억하기를 바랍니다.

31
00:01:54,010 --> 00:01:58,900
왜냐하면 기울기 벡터를 13000차원의 방향으로 생각하는것은

32
00:01:58,910 --> 00:02:02,090
우리가 상상 할 수 있는 범위를 넘어서는 것이기 때문입니다.

33
00:02:02,090 --> 00:02:03,510
그것에 대해 생각할 수있는 또 다른 방법이 있습니다.

34
00:02:04,580 --> 00:02:07,710
각 오차는 오차함수가 각 가중치 및 편차에

35
00:02:07,710 --> 00:02:11,140
얼마나 민감한지를 나타낸다고 생각하는 것입니다.

36
00:02:11,810 --> 00:02:14,580
예를 들어, 제가 설명하려고하는 과정을 거쳐서

37
00:02:14,580 --> 00:02:16,370
음의 기울기를 계산하면,

38
00:02:16,370 --> 00:02:21,470
이 가중치에 대한 기울기 계산값은 3.2로 나오고,

39
00:02:21,870 --> 00:02:26,370
이 가중치에 대한 기울기 계산값은 0.1로 나옵니다.

40
00:02:26,910 --> 00:02:28,420
당신이 해석 할 수있는 방법은

41
00:02:28,420 --> 00:02:33,080
오차함수의 출력은 기울기가 0.1인 가중치 보다 기울기가 3.2인 가중치가 32배 더 민감하다고 해석할 수 있습니다.

42
00:02:33,640 --> 00:02:35,930
그래서 만약 기울기가 3.2인 가중치의 값을 조금 조정한다면

43
00:02:35,930 --> 00:02:38,190
오차함수의 출력에 변화를 줄 것입니다.

44
00:02:38,190 --> 00:02:43,200
그리고 그 변화는두 번째 가중치의 조정이주는 것보다 32배 더 클것입니다.

45
00:02:48,520 --> 00:02:51,440
개인적으로, 제가 처음으로 역전파에 대해 배울 때

46
00:02:51,440 --> 00:02:55,740
가장 혼란스러운 부분은 표기법과 그 모든것을  나타내는 지수였다고 생각합니다.

47
00:02:56,180 --> 00:02:59,420
하지만 일단이 알고리즘의 각 부분이 실제로하고있는 것을 알면

48
00:02:59,420 --> 00:03:02,840
각 개별 효과는 실제로 꽤 직관적입니다.

49
00:03:03,180 --> 00:03:06,740
단지, 수많은 작은 조정들이 서로 겹쳐져 있을 뿐입니다.

50
00:03:07,660 --> 00:03:11,290
그래서 저는 표기법을 완전히 무시하고 시작할 것입니다.

51
00:03:11,290 --> 00:03:13,370
그리고 각각의 훈련 예제가

52
00:03:13,370 --> 00:03:16,350
가중치와 편향에 미치는 영향을 살펴보겠습니다.

53
00:03:17,090 --> 00:03:18,590
왜냐하면 오차함수는

54
00:03:18,590 --> 00:03:23,640
수만 가지의 훈련예제 대해 
예제당 특정 오차를 평균화하기 때문에,

55
00:03:23,970 --> 00:03:28,640
단일 경사하강법 단계에서 가중치와 편향를 조정하는 방식

56
00:03:28,640 --> 00:03:31,140
또한 모든 예제에 따라 다르며

57
00:03:31,660 --> 00:03:33,180
오히려 원칙적으로 그래야 합니다만

58
00:03:33,200 --> 00:03:35,930
계산 효율성을 위해 각 단계마다 모든 예를 볼 필요가 없도록

59
00:03:35,930 --> 00:03:39,370
약간의 방법을 쓸 것입니다.

60
00:03:39,790 --> 00:03:41,330
또 다른 방법으로,

61
00:03:41,330 --> 00:03:46,160
우리가 지금 하려는 것은 한 가지 예에 집중하는 것입니다.바로 이 2의 이미지 입니다.

62
00:03:46,670 --> 00:03:51,650
이 한 가지 훈련 사례가 가중치와 편향을 조정하는 방법에 어떻게 영향을 미칠까요?

63
00:03:52,680 --> 00:03:55,240
우리가 아직 네트워크가 잘 훈련되지 않은 시점에 있다고 가정 해 보겠습니다.

64
00:03:55,240 --> 00:03:57,970
결과물은 꽤 무작위 처럼 보일 것입니다.

65
00:03:57,970 --> 00:04:02,040
0.5, 0.8, 0.2와 같은 값일 수 있습니다.

66
00:04:02,640 --> 00:04:07,450
이제 우리는 이러한 출력 자체를 변화시킬수는 없으며, 가중치 및 편향에만 변화를 줄 수 있습니다.

67
00:04:07,790 --> 00:04:12,670
하지만 여기서 출력층의 어떤 값이 조정되어야 될지 아는것은 유용합니다.

68
00:04:13,270 --> 00:04:15,710
우리는 이 이미지를 2로 분류하기를 원하기 때문에,

69
00:04:16,040 --> 00:04:21,360
우리는 세 번째 값이 출력되기를 원하고 다른 모든 것은 내립니다.

70
00:04:22,040 --> 00:04:26,020
또한 이러한 조정의 크기는 다음과 비례해야합니다.

71
00:04:26,020 --> 00:04:29,630
각 현재 값이 목표 값에서 얼마나 떨어져 있는지.

72
00:04:30,220 --> 00:04:34,350
예를 들어, 그 숫자 2 뉴런 활성화에 대한 증가는,

73
00:04:34,350 --> 00:04:38,490
숫자 8 뉴런에 대한 증가 보다는 중요합니다.

74
00:04:38,490 --> 00:04:40,630
그것은 이미 있어야 할 곳에 아주 가깝습니다.

75
00:04:41,990 --> 00:04:45,250
그러니깐 더 자세히 보면서

76
00:04:45,250 --> 00:04:47,530
우리가 활성화를 원하는 이 뉴런에 초점을 맞추어 봅시다.

77
00:04:48,160 --> 00:04:50,550
그 활성화는 다음과 같이 정의됩니다.

78
00:04:50,550 --> 00:04:56,430
이전 계층의 모든 활성화에 대한 특정 가중치 합계와 편향

79
00:04:56,430 --> 00:05:01,280
은 시그모이드 함수나 다른 ReLU와 같은 함수와 연결되어 있습니다.

80
00:05:01,800 --> 00:05:07,360
따라서 활성화를 높이기 위해 함께 조화을 이룰 수있는 세 가지 방법이 있습니다.

81
00:05:07,680 --> 00:05:10,970
편향을 증가시키거나, 가중치를 증가시키거나, 또는

82
00:05:10,970 --> 00:05:14,020
이전 레이어의 활성도를 변경할 수 있습니다.

83
00:05:14,950 --> 00:05:17,770
가중치를 조정하는 방법에만 초점을 맞추고,

84
00:05:17,770 --> 00:05:21,410
가중치의 실제 영향 수준이 다른지 확인하십시오.

85
00:05:21,410 --> 00:05:25,750
앞의 레이어에서 가장 밝은 뉴런과의 연결이 가장 큰 효과를 냅니다.

86
00:05:25,750 --> 00:05:29,240
가중치에는 더 큰 활성 값이 곱해지기 때문입니다.

87
00:05:31,320 --> 00:05:34,980
그래서 만약 당신이 그 중 하나의 가중치를 늘린다면,

88
00:05:34,980 --> 00:05:37,680
어두운 뉴런과의 연결 가중치를 높이는 것보다

89
00:05:37,680 --> 00:05:40,820
오차함수에 실제로 더 강한 영향을 미칩니다.

90
00:05:40,820 --> 00:05:43,650
적어도이 한 가지 훈련 예를들 수 있습니다.

91
00:05:44,380 --> 00:05:46,890
경사 하강법에 대해서 이야기했을 때 기억하십시오.

92
00:05:46,890 --> 00:05:50,620
우리는 각 구성 요소가 어떻게 조정되어야하는지,

93
00:05:50,620 --> 00:05:53,370
우리는 오차함수의 값을 줄이는것에 관심을 둡니다.

94
00:05:55,270 --> 00:05:59,310
그건 그렇고, 적어도 신경 과학의 이론을 연상케합니다

95
00:05:59,310 --> 00:06:01,870
생물의 신경망이 어떻게 학습되는지

96
00:06:01,870 --> 00:06:06,820
Hebbian theory - 종종 "함께 연결되는 뉴런"이라는 구에서 요약됩니다.

97
00:06:07,260 --> 00:06:12,200
여기에서 가장 큰 가중치 증가, 가장 큰 연결 강화,

98
00:06:12,200 --> 00:06:14,840
가장 활동적인 뉴런 사이에서 발생하며,

99
00:06:14,840 --> 00:06:17,590
우리는 더 활성화 되기를 바랍니다.

100
00:06:18,020 --> 00:06:21,060
어떤 의미에서 볼 때, 2가 보일때 켜지는 뉴런들은

101
00:06:21,080 --> 00:06:24,700
2에 대해 생각할 때 더 밝아집니다.

102
00:06:25,420 --> 00:06:29,700
분명히 말하자면, 저는 인공신경망의 네트워크가  생물학적인 뇌와 같은 방식으로 움직이는 것과

103
00:06:29,700 --> 00:06:33,080
뉴런들이 서로 연관 되고 자극할 수 있다라는

104
00:06:33,080 --> 00:06:37,250
이 문장에 대하여 뭐라고 할 수 있는 위치에 있지는 않습니다.

105
00:06:37,250 --> 00:06:41,260
그러나 저는 아주 흥미로운 점을 발견했습니다.

106
00:06:41,890 --> 00:06:46,020
어쨌든,이 뉴런의 활성화를 증가시킬 수있는 방법 중 세 번째 방법입니다.

107
00:06:46,020 --> 00:06:49,060
이전 계층의 모든 활성화를 변경하는 것입니다.

108
00:06:49,560 --> 00:06:54,970
즉 2와 연결된 모든 양의 가중치 신경은 밝아집니다.

109
00:06:54,970 --> 00:06:57,960
음의 가중치와 관련된 모든 신경이 더 밝아지면,

110
00:06:58,340 --> 00:07:00,890
그 자리 2 뉴런은 더 활동적이 될 것입니다.

111
00:07:02,450 --> 00:07:06,130
그리고 가중치 변화와 유사하게, 당신은 당신의 돈을 위해 가장 많은 것을 얻을 것입니다.

112
00:07:06,130 --> 00:07:10,550
해당 가중치의 크기에 비례하는 변경 사항을 찾습니다.

113
00:07:12,120 --> 00:07:15,360
물론 우리는 이러한 활성화에 직접적으로 영향을 줄 수는 없지만,

114
00:07:15,360 --> 00:07:17,780
우리는 단지 가중치와 편견을 제어 할 수 있습니다.

115
00:07:18,220 --> 00:07:23,610
그러나 마지막 레이어와 마찬가지로 원하는 변경 사항이 무엇인지 메모하는 것이 좋습니다.

116
00:07:24,450 --> 00:07:29,720
하지만 여기서 한 걸음 더 자세히 살펴보면, 이것은 숫자 2 출력 뉴런이 원하는 것일뿐입니다.

117
00:07:29,720 --> 00:07:34,840
우리는 또한 마지막 레이어의 다른 모든 뉴런들이 덜 활동적이되기를 바랍니다.

118
00:07:34,840 --> 00:07:36,500
그 각각의 출력 뉴런들

119
00:07:36,500 --> 00:07:39,840
그 두 번째 - 마지막 층에서 일어날 일에 대한 생각을 가지고 있습니다.

120
00:07:43,110 --> 00:07:46,140
그래서,이 자리의 욕망은 2 뉴런

121
00:07:46,140 --> 00:07:50,520
다른 모든 출력 뉴런의 욕구와 함께 추가됩니다.

122
00:07:50,520 --> 00:07:53,240
마지막 두 번째 레이어에서 일어날 일에 대해

123
00:07:53,580 --> 00:07:56,400
다시, 대응하는 가중치에 비례하여,

124
00:07:56,400 --> 00:08:00,910
그리고 각각의 신경 세포가 변화 할 필요가있는 양에 비례하여.

125
00:08:01,480 --> 00:08:05,510
바로 여기가 거꾸로 전파하려는 아이디어가 나오는 곳입니다.

126
00:08:05,960 --> 00:08:08,730
이러한 모든 원하는 효과를 모두 합하면,

127
00:08:08,730 --> 00:08:13,560
당신은 근본적으로 당신이 두 번째에서 마지막 층으로 일어나기를 원하는 뉘앙스 목록을 얻습니다.

128
00:08:14,180 --> 00:08:15,390
그리고 일단 당신이 그것들을 가지고 있으면,

129
00:08:15,390 --> 00:08:17,850
재귀 적으로 동일한 프로세스를 적용 할 수 있습니다.

130
00:08:17,850 --> 00:08:21,180
그 값들을 결정하는 관련 가중치들과 편향들,

131
00:08:21,180 --> 00:08:25,140
방금 걸어서 돌아가서 네트워크를 통해 뒤로 이동하는 동일한 프로세스를 반복합니다.

132
00:08:29,030 --> 00:08:30,370
그리고 조금 더 축소하면,

133
00:08:30,370 --> 00:08:31,920
이것이 단지 모든 것임을 기억하십시오.

134
00:08:31,920 --> 00:08:37,400
어떻게 하나의 훈련 예가 그 무게와 편견의 각각을 조금씩 움직이기를 바랄 것인가.

135
00:08:37,400 --> 00:08:39,700
우리가 원하는 것만 듣는다면,

136
00:08:39,700 --> 00:08:43,400
네트워크는 궁극적으로 모든 이미지를 2로 분류하기 위해 인센티브가 부여됩니다.

137
00:08:44,030 --> 00:08:49,420
그래서 당신이하는 일은 다른 모든 트레이닝 예제에 대해 동일한 백 드롭 루틴을 수행하는 것입니다.

138
00:08:49,420 --> 00:08:53,200
각자가 가중치와 편견을 어떻게 바꾸고 싶은지 기록하고,

139
00:08:53,650 --> 00:08:56,220
원하는 변화를 함께 평균했습니다.

140
00:09:02,050 --> 00:09:06,940
각 체중과 편견에 대한 평균 nudges의 여기 수집은,

141
00:09:06,940 --> 00:09:11,910
느슨하게 말하면, 마지막 비디오에서 참조 된 비용 함수의 음의 기울기,

142
00:09:11,910 --> 00:09:13,740
적어도 그것에 비례하는 어떤 것.

143
00:09:14,360 --> 00:09:19,570
나는 "느슨하게 말하면서"말합니다. 왜냐하면 나는 아직 그 찌름에 대해 정량적으로 정확한 것을 얻지 못했기 때문입니다.

144
00:09:19,570 --> 00:09:22,190
그러나 제가 방금 언급 한 모든 변화를 이해한다면,

145
00:09:22,190 --> 00:09:24,770
왜 일부는 다른 것보다 비례 적으로 더 큽니다.

146
00:09:24,770 --> 00:09:27,160
그들 모두를 어떻게 함께 추가해야하는지,

147
00:09:27,160 --> 00:09:31,170
당신은 backpropagation이 실제로하고있는 것에 대한 메 커닉을 이해합니다.

148
00:09:34,050 --> 00:09:37,400
그건 그렇고, 실제로 그것은 컴퓨터를 매우 오랜 시간이 걸립니다.

149
00:09:37,400 --> 00:09:42,490
모든 단일 교육 예, 모든 단일 그래디언트 디센트 단계의 영향을 추가합니다.

150
00:09:43,010 --> 00:09:44,960
여기에 일반적으로 수행되는 작업이 있습니다.

151
00:09:45,440 --> 00:09:50,280
학습 데이터를 무작위로 섞은 다음이를 전체 배치로 나눕니다.

152
00:09:50,280 --> 00:09:52,680
각자 100 개의 훈련 예를 가지고 있다고 가정 해 봅시다.

153
00:09:53,240 --> 00:09:56,430
그런 다음 미니 배치에 따라 단계를 계산합니다.

154
00:09:56,850 --> 00:09:59,390
비용 함수의 실제 그래디언트가 될 수는 없습니다.

155
00:09:59,390 --> 00:10:02,630
이 작은 부분 집합이 아닌 모든 훈련 데이터에 의존합니다.

156
00:10:03,100 --> 00:10:05,640
따라서 내리막 길이 가장 효율적인 단계는 아닙니다.

157
00:10:06,080 --> 00:10:08,970
하지만 각 미니 배치는 꽤 좋은 근사값을 제공하지만,

158
00:10:08,970 --> 00:10:12,250
더욱 중요한 것은 계산 속도가 현저히 빠름을 의미합니다.

159
00:10:12,820 --> 00:10:16,810
관련 비용면에서 네트워크의 궤적을 그리려면,

160
00:10:16,810 --> 00:10:22,030
술 취하는 남자가 언덕을 목적없이 우연히 마주 치는 것과 조금 더 비슷하지만 빠른 발걸음을 내딛을 것입니다.

161
00:10:22,030 --> 00:10:27,180
각 단계의 정확한 내리막 방향을 결정하는 신중하게 계산하는 사람이 아니라

162
00:10:27,180 --> 00:10:30,350
그 방향으로 매우 천천히 조심스럽게 걸음.

163
00:10:31,460 --> 00:10:34,940
이 기법을 "확률 적 구배 강하"라고합니다.

164
00:10:36,000 --> 00:10:39,800
여기에 많은 일이 일어나고 있습니다. 그래서 우리 자신을 위해 요약 해 보겠습니다.

165
00:10:40,240 --> 00:10:42,270
역 전파는 알고리즘입니다.

166
00:10:42,270 --> 00:10:47,370
하나의 훈련 예가 가중치와 편향을 조금씩 움직이기를 원하는지를 결정하기 위해,

167
00:10:47,370 --> 00:10:49,930
그들이 위 또는 아래로 가야하는지에 관해서뿐만 아니라,

168
00:10:49,930 --> 00:10:55,700
그러나 그 변화에 대한 상대적인 비율이 비용을 가장 빠르게 감소시키는 측면에서 볼 때.

169
00:10:56,240 --> 00:10:58,270
진정한 그래디언트 디센트 단계

170
00:10:58,270 --> 00:11:01,820
수십, 수천 건의 교육 사례에 대해이 작업을 수행해야합니다.

171
00:11:01,820 --> 00:11:04,260
당신이 얻는 원하는 변화를 평균화하는 것입니다.

172
00:11:04,830 --> 00:11:06,340
하지만 계산 속도가 느립니다.

173
00:11:06,690 --> 00:11:10,480
그래서 대신에 데이터를 이러한 작은 배치로 무작위로 세분합니다.

174
00:11:10,480 --> 00:11:13,460
미니 배치와 관련하여 각 단계를 계산할 수 있습니다.

175
00:11:13,900 --> 00:11:17,690
반복적으로 모든 미니 배치를 검토하고 이러한 조정을 수행하면,

176
00:11:17,690 --> 00:11:21,050
당신은 비용 함수의 지역 최소값으로 수렴 할 것이며,

177
00:11:21,430 --> 00:11:25,740
말하자면 네트워크가 교육 사례에서 실제로 잘 수행 될 것입니다.

178
00:11:27,450 --> 00:11:32,290
그래서 모든 말로는, 역행을 구현할 코드의 모든 라인이

179
00:11:32,290 --> 00:11:36,970
적어도 비공식적 인면에서 지금 본 내용과 일치합니다.

180
00:11:37,570 --> 00:11:40,960
그러나 때때로 수학이하는 것이 전투의 절반에 불과하다는 것을 알기 때문에,

181
00:11:40,960 --> 00:11:44,460
그리고 그 빌어 먹을 일을 나타내는 것은 그것이 혼란스럽고 혼란스러워지는 곳입니다.

182
00:11:44,930 --> 00:11:47,620
그래서 더 깊은 곳으로 가고 싶은 당신들에게는,

183
00:11:47,620 --> 00:11:50,670
다음 비디오는 방금 여기에 제시된 것과 동일한 아이디어를 거칩니다.

184
00:11:50,670 --> 00:11:52,750
그러나 밑에있는 미적분학의 관점에서,

185
00:11:52,750 --> 00:11:56,760
다른 리소스에서이 주제를 보면서 좀 더 익숙해 져야합니다.

186
00:11:57,210 --> 00:11:59,440
그 전에 강조 할 가치가있는 것은

187
00:11:59,440 --> 00:12:04,320
이 알고리즘이 작동하려면, 이것은 모든 종류의 기계가 단지 신경 네트워크를 넘어서서 배우는 것,

188
00:12:04,320 --> 00:12:06,120
당신은 많은 훈련 데이터가 필요합니다.

189
00:12:06,430 --> 00:12:09,860
우리의 경우, 자필 자국을 만드는 좋은 예가 좋은 예입니다.

190
00:12:09,860 --> 00:12:12,110
MNIST 데이터베이스가 존재한다는 것입니다.

191
00:12:12,110 --> 00:12:15,290
인간에 의해 분류 된 수많은 사례가 있습니다.

192
00:12:15,290 --> 00:12:19,000
따라서 기계 학습 분야에서 일하고있는 사람들이

193
00:12:19,000 --> 00:12:21,930
당신이 실제로 필요로하는 분류 된 훈련 자료를 얻는 것뿐입니다.

194
00:12:22,240 --> 00:12:25,080
사람들이 수만 장의 이미지를 표시하는지 여부

195
00:12:25,080 --> 00:12:27,550
또는 다른 데이터 유형을 처리 할 수 ​​있습니다.

