1
00:00:04,060 --> 00:00:08,880
Тут ми розглядаємо зворотне поширення, основний алгоритм навчання нейронних мереж. 

2
00:00:09,400 --> 00:00:12,179
Після короткого підсумку того, де ми зараз, перше, що я зроблю, 

3
00:00:12,179 --> 00:00:15,480
це інтуїтивно зрозуміле керівництво для того, що насправді робить алгоритм, 

4
00:00:15,480 --> 00:00:17,000
без будь-яких посилань на формули. 

5
00:00:17,660 --> 00:00:19,927
Тоді для тих із вас, хто хоче зануритися в математику, 

6
00:00:19,927 --> 00:00:23,020
наступне відео розповідає про обчислення, що лежить в основі всього цього. 

7
00:00:23,820 --> 00:00:27,942
Якщо ви переглянули останні два відео або якщо ви просто переходите з відповідним фоном, 

8
00:00:27,942 --> 00:00:31,000
ви знаєте, що таке нейронна мережа та як вона передає інформацію. 

9
00:00:31,680 --> 00:00:35,313
Тут ми робимо класичний приклад розпізнавання рукописних цифр, 

10
00:00:35,313 --> 00:00:39,639
піксельні значення яких надходять на перший рівень мережі з 784 нейронами, 

11
00:00:39,639 --> 00:00:44,483
і я показую мережу з двома прихованими шарами, які мають лише по 16 нейронів кожен, 

12
00:00:44,483 --> 00:00:49,040
і вихідним шар з 10 нейронів, що вказує, яку цифру мережа обирає як відповідь. 

13
00:00:50,040 --> 00:00:54,605
Я також очікую, що ви зрозумієте градієнтний спуск, як описано в останньому відео, 

14
00:00:54,605 --> 00:00:58,235
і те, як ми маємо на увазі під навчанням те, що ми хочемо знайти, 

15
00:00:58,235 --> 00:01:01,260
які ваги та зміщення мінімізують певну функцію витрат. 

16
00:01:02,040 --> 00:01:07,197
Як швидке нагадування: для вартості одного навчального прикладу ви берете результат, 

17
00:01:07,197 --> 00:01:11,202
який дає мережа, разом із результатом, який ви хотіли б отримати, 

18
00:01:11,202 --> 00:01:14,600
і додаєте квадрати відмінностей між кожним компонентом. 

19
00:01:15,380 --> 00:01:20,692
Зробивши це для всіх ваших десятків тисяч навчальних прикладів і усереднивши результати, 

20
00:01:20,692 --> 00:01:23,020
ви отримаєте загальну вартість мережі. 

21
00:01:23,020 --> 00:01:28,219
Наче цього недостатньо для роздумів, як описано в останньому відео, те, що ми шукаємо, 

22
00:01:28,219 --> 00:01:31,925
це від’ємний градієнт цієї функції витрат, який говорить вам, 

23
00:01:31,925 --> 00:01:35,869
як вам потрібно змінити всі ваги та зміщення, усі ці підключення, 

24
00:01:35,869 --> 00:01:38,320
щоб найбільш ефективно знизити вартість. 

25
00:01:43,260 --> 00:01:46,389
Зворотне поширення, тема цього відео, — це алгоритм 

26
00:01:46,389 --> 00:01:49,580
для обчислення цього божевільно складного градієнта. 

27
00:01:49,580 --> 00:01:54,117
Одна ідея з останнього відео, яку я справді хочу, щоб ви зараз міцно запам’ятали, 

28
00:01:54,117 --> 00:01:58,987
полягає в тому, що оскільки уявлення про вектор градієнта як напрямок у 13 000 вимірах, 

29
00:01:58,987 --> 00:02:03,580
м’якше кажучи, виходить за межі нашої уяви, існує інша як ви можете думати про це. 

30
00:02:04,600 --> 00:02:07,424
Величина кожного компонента тут говорить про те, 

31
00:02:07,424 --> 00:02:10,940
наскільки функція витрат чутлива до кожної ваги та зміщення. 

32
00:02:11,800 --> 00:02:16,004
Наприклад, скажімо, ви виконуєте процес, який я збираюся описати, 

33
00:02:16,004 --> 00:02:21,163
і обчислюєте від’ємний градієнт, і компонент, пов’язаний із вагою на цьому краю, 

34
00:02:21,163 --> 00:02:26,260
дорівнює 3.2, тоді як компонент, пов’язаний із цим краєм, тут має значення 0.1. 

35
00:02:26,820 --> 00:02:32,121
Ви б це інтерпретували так: вартість функції в 32 рази чутливіша до змін у цій 

36
00:02:32,121 --> 00:02:35,812
першій вазі, отже, якщо ви трішки зміните це значення, 

37
00:02:35,812 --> 00:02:40,375
це призведе до певних змін у вартості, і ця зміна у 32 рази більше, 

38
00:02:40,375 --> 00:02:43,060
ніж те саме ворушіння цієї другої ваги. 

39
00:02:48,420 --> 00:02:51,755
Особисто, коли я вперше дізнався про зворотне розповсюдження, я вважав, 

40
00:02:51,755 --> 00:02:55,740
що найбільш заплутаним аспектом були лише нотація та погоня за індексом усього цього. 

41
00:02:56,220 --> 00:02:59,971
Але як тільки ви розгортаєте, що насправді робить кожна частина цього алгоритму, 

42
00:02:59,971 --> 00:03:03,444
кожен окремий ефект, який він має, насправді досить інтуїтивно зрозумілий, 

43
00:03:03,444 --> 00:03:06,640
просто є багато маленьких коригувань, які накладаються одне на одне. 

44
00:03:07,740 --> 00:03:11,779
Тож я почну тут із повного ігнорування нотації та просто покроково 

45
00:03:11,779 --> 00:03:16,120
розповім про вплив кожного навчального прикладу на ваги та упередження. 

46
00:03:17,020 --> 00:03:21,635
Оскільки функція витрат передбачає усереднення певної вартості кожного прикладу 

47
00:03:21,635 --> 00:03:24,347
за всіма десятками тисяч навчальних прикладів, 

48
00:03:24,347 --> 00:03:29,251
спосіб коригування ваг і зміщень для одного кроку градієнтного спуску також залежить 

49
00:03:29,251 --> 00:03:31,040
від кожного окремого прикладу. 

50
00:03:31,680 --> 00:03:34,119
Точніше, в принципі так і повинно бути, але для ефективності 

51
00:03:34,119 --> 00:03:36,720
обчислень ми пізніше зробимо невеликий трюк, щоб вам не потрібно 

52
00:03:36,720 --> 00:03:39,200
було використовувати кожен окремий приклад для кожного кроку. 

53
00:03:39,200 --> 00:03:42,500
В інших випадках, прямо зараз, все, що ми збираємося зробити, 

54
00:03:42,500 --> 00:03:45,960
це зосередити нашу увагу на одному прикладі, цьому зображенні 2. 

55
00:03:46,720 --> 00:03:51,480
Який вплив повинен мати цей навчальний приклад на те, як коригуються ваги та зміщення? 

56
00:03:52,680 --> 00:03:56,563
Припустімо, що ми перебуваємо в точці, коли мережа ще недостатньо навчена, 

57
00:03:56,563 --> 00:04:01,068
тому активації у виводі виглядатимуть досить випадковими, можливо, приблизно 0.5, 0.8, 

58
00:04:01,068 --> 00:04:02,000
0.2, далі і далі. 

59
00:04:02,520 --> 00:04:07,228
Ми не можемо напряму змінити ці активації, ми можемо лише впливати на ваги та зміщення, 

60
00:04:07,228 --> 00:04:10,332
але корисно відстежувати, які коригування, які ми хочемо, 

61
00:04:10,332 --> 00:04:12,580
мають відбутися на цьому вихідному рівні. 

62
00:04:13,360 --> 00:04:17,508
І оскільки ми хочемо, щоб воно класифікувало зображення як 2, ми хочемо, 

63
00:04:17,508 --> 00:04:21,260
щоб це третє значення було підштовхнуто вгору, а всі інші – вниз. 

64
00:04:22,060 --> 00:04:25,437
Крім того, розміри цих поштовхів мають бути пропорційними до того, 

65
00:04:25,437 --> 00:04:29,520
наскільки далеко кожне поточне значення знаходиться від його цільового значення. 

66
00:04:30,220 --> 00:04:34,989
Наприклад, збільшення активації нейрона № 2 у певному сенсі важливіше, 

67
00:04:34,989 --> 00:04:40,900
ніж зменшення активності нейрона № 8, яке вже досить близько до того, де воно має бути. 

68
00:04:42,040 --> 00:04:45,459
Отже, збільшуючи масштаб, давайте зосередимося лише на цьому одному нейроні, 

69
00:04:45,459 --> 00:04:47,280
тому, чию активацію ми хочемо збільшити. 

70
00:04:48,180 --> 00:04:52,359
Пам’ятайте, що активація визначається як певна зважена сума всіх 

71
00:04:52,359 --> 00:04:55,445
активацій на попередньому рівні, плюс зміщення, 

72
00:04:55,445 --> 00:05:01,040
яке потім підключається до чогось на зразок функції сигмоїдної сквишификации або ReLU. 

73
00:05:01,640 --> 00:05:07,020
Отже, є три різні шляхи, які можна об’єднати разом, щоб збільшити цю активність. 

74
00:05:07,440 --> 00:05:10,894
Ви можете збільшити зміщення, ви можете збільшити ваги, 

75
00:05:10,894 --> 00:05:14,040
і ви можете змінити активації з попереднього шару. 

76
00:05:14,940 --> 00:05:17,462
Зосереджуючись на тому, як слід регулювати ваги, 

77
00:05:17,462 --> 00:05:20,860
зверніть увагу на те, що ваги насправді мають різні рівні впливу. 

78
00:05:21,440 --> 00:05:25,889
Зв’язки з найяскравішими нейронами з попереднього шару мають найбільший ефект, 

79
00:05:25,889 --> 00:05:29,100
оскільки ці ваги множаться на більші значення активації. 

80
00:05:31,460 --> 00:05:35,427
Отже, якщо ви збільшите одну з цих ваг, це справді матиме сильніший 

81
00:05:35,427 --> 00:05:40,387
вплив на кінцеву функцію витрат, ніж збільшення ваг зв’язків із димерними нейронами, 

82
00:05:40,387 --> 00:05:43,480
принаймні, що стосується цього навчального прикладу. 

83
00:05:44,420 --> 00:05:46,994
Пам’ятайте, що коли ми говоримо про градієнтний спуск, 

84
00:05:46,994 --> 00:05:49,990
нам важливо не лише підштовхнути кожен компонент угору чи вниз, 

85
00:05:49,990 --> 00:05:53,220
нам важливо, які з них дають вам найбільшу віддачу від ваших грошей. 

86
00:05:55,020 --> 00:05:58,626
Це, до речі, принаймні дещо нагадує теорію в нейронауці про те, 

87
00:05:58,626 --> 00:06:01,782
як навчаються біологічні мережі нейронів, теорію Хебба, 

88
00:06:01,782 --> 00:06:06,460
яку часто підсумовують фразою: нейрони, які запускаються разом, з’єднуються разом. 

89
00:06:07,260 --> 00:06:12,948
Тут найбільше збільшення ваги, найбільше зміцнення зв’язків відбувається між нейронами, 

90
00:06:12,948 --> 00:06:17,280
які є найбільш активними, і тими, які ми хочемо стати активнішими. 

91
00:06:17,940 --> 00:06:20,880
У певному сенсі нейрони, які спрацьовують, коли бачать 2, 

92
00:06:20,880 --> 00:06:24,480
стають сильніше пов’язаними з тими, хто спрацьовує, коли думає про це. 

93
00:06:25,400 --> 00:06:29,808
Щоб було зрозуміло, я не в змозі робити твердження тим чи іншим чином щодо того, 

94
00:06:29,808 --> 00:06:33,835
чи штучні мережі нейронів поводяться хоч як біологічний мозок, і ця ідея, 

95
00:06:33,835 --> 00:06:37,482
що спрацьовує разом, супроводжується кількома значущими зірочками, 

96
00:06:37,482 --> 00:06:41,020
але сприймається як дуже вільна Мені цікаво відзначити аналогію. 

97
00:06:41,940 --> 00:06:45,439
У будь-якому разі, третій спосіб, яким ми можемо допомогти підвищити 

98
00:06:45,439 --> 00:06:49,040
активацію цього нейрона, — змінити всі активації на попередньому шарі. 

99
00:06:49,040 --> 00:06:53,471
А саме, якщо все, що пов’язано з цим нейроном цифри 2 із позитивною вагою, 

100
00:06:53,471 --> 00:06:57,193
стане яскравішим, а якщо все, що пов’язано з негативною вагою, 

101
00:06:57,193 --> 00:07:00,680
стане тьмянішим, тоді цей нейрон цифри 2 стане активнішим. 

102
00:07:02,540 --> 00:07:07,060
Подібно до змін ваги, ви отримаєте максимальну віддачу від своїх грошей, 

103
00:07:07,060 --> 00:07:10,280
шукаючи змін, пропорційних розміру відповідних ваг. 

104
00:07:12,140 --> 00:07:15,507
Тепер, звичайно, ми не можемо безпосередньо впливати на ці активації, 

105
00:07:15,507 --> 00:07:17,480
ми лише контролюємо ваги та упередження. 

106
00:07:17,480 --> 00:07:24,120
Але так само, як і з останнім шаром, корисно занотувати, які ці бажані зміни. 

107
00:07:24,580 --> 00:07:27,146
Але майте на увазі, якщо зменшити масштаб на один крок тут, 

108
00:07:27,146 --> 00:07:29,200
це лише те, що хоче вихідний нейрон з цифрою 2. 

109
00:07:29,760 --> 00:07:34,301
Пам’ятайте, ми також хочемо, щоб усі інші нейрони в останньому шарі стали менш активними, 

110
00:07:34,301 --> 00:07:37,682
і кожен із цих інших вихідних нейронів має власні думки щодо того, 

111
00:07:37,682 --> 00:07:39,600
що має статися з передостаннім шаром. 

112
00:07:42,700 --> 00:07:48,617
Отже, бажання цього нейрона з цифрою 2 додається разом із бажаннями всіх інших вихідних 

113
00:07:48,617 --> 00:07:52,785
нейронів щодо того, що має статися з цим передостаннім шаром, 

114
00:07:52,785 --> 00:07:57,290
знову ж таки пропорційно до відповідних ваг і пропорційно до того, 

115
00:07:57,290 --> 00:08:00,720
скільки потрібно кожному з цих нейронів змінювати. 

116
00:08:01,600 --> 00:08:05,480
Ось тут і виникає ідея розповсюдження назад. 

117
00:08:05,820 --> 00:08:10,344
Додавши разом усі ці бажані ефекти, ви, по суті, отримаєте список підштовхувань, 

118
00:08:10,344 --> 00:08:13,360
які ви хочете виконати на цьому передостанньому шарі. 

119
00:08:14,220 --> 00:08:17,863
І як тільки ви їх отримаєте, ви можете рекурсивно застосувати той самий 

120
00:08:17,863 --> 00:08:21,152
процес до відповідних ваг і зміщень, які визначають ці значення, 

121
00:08:21,152 --> 00:08:25,100
повторюючи той самий процес, який я щойно пройшов, і рухаючись назад мережею. 

122
00:08:28,960 --> 00:08:32,397
І якщо трохи зменшити масштаб, пам’ятайте, що це все лише те, 

123
00:08:32,397 --> 00:08:37,000
як окремий навчальний приклад хоче підштовхнути до кожного з цих ваг і упереджень. 

124
00:08:37,480 --> 00:08:39,724
Якби ми лише прислухалися до того, що хоче ця двоє, 

125
00:08:39,724 --> 00:08:43,220
мережа зрештою була б стимулювана просто класифікувати всі зображення як двійку. 

126
00:08:44,059 --> 00:08:47,979
Отже, що ви робите, це проходите ту саму процедуру підтримки для 

127
00:08:47,979 --> 00:08:52,020
кожного іншого прикладу навчання, записуючи, як кожен із них хотів 

128
00:08:52,020 --> 00:08:56,000
би змінити ваги та зміщення, і усереднюєте разом ці бажані зміни. 

129
00:09:01,720 --> 00:09:05,963
Цей набір усереднених підштовхувань до кожної ваги та зміщення є, 

130
00:09:05,963 --> 00:09:09,243
грубо кажучи, від’ємним градієнтом функції витрат, 

131
00:09:09,243 --> 00:09:13,680
згаданим в останньому відео, або принаймні чимось пропорційним йому. 

132
00:09:14,380 --> 00:09:18,380
Я кажу вільно, лише тому, що мені ще належить отримати точні кількісні 

133
00:09:18,380 --> 00:09:22,999
дані про ці підштовхи, але якщо ви зрозуміли кожну зміну, про яку я щойно згадав, 

134
00:09:22,999 --> 00:09:27,281
чому одні пропорційно більші за інші, і як їх усіх потрібно додавати разом, 

135
00:09:27,281 --> 00:09:31,000
ви розумієте механізм для що насправді робить зворотне поширення. 

136
00:09:33,960 --> 00:09:37,673
До речі, на практиці комп’ютерам потрібно надзвичайно багато часу, 

137
00:09:37,673 --> 00:09:42,440
щоб підсумувати вплив кожного навчального прикладу кожного кроку градієнтного спуску. 

138
00:09:43,140 --> 00:09:44,820
Отже, ось що зазвичай роблять замість цього. 

139
00:09:45,480 --> 00:09:49,040
Ви випадковим чином перетасовуєте свої навчальні дані та розділяєте їх на цілу 

140
00:09:49,040 --> 00:09:52,420
купу міні-пакетів, припустімо, що кожен із них має 100 прикладів навчання. 

141
00:09:52,939 --> 00:09:57,280
Потім ви обчислюєте крок відповідно до міні-серії. 

142
00:09:57,280 --> 00:10:01,942
Це не фактичний градієнт функції витрат, який залежить від усіх навчальних даних, 

143
00:10:01,942 --> 00:10:05,751
а не ця крихітна підмножина, тому це не найефективніший крок униз, 

144
00:10:05,751 --> 00:10:09,788
але кожна міні-серія дає вам досить гарне наближення, і, що важливіше, 

145
00:10:09,788 --> 00:10:12,120
це дає вам значне прискорення обчислень. 

146
00:10:12,820 --> 00:10:16,747
Якби ви побудували траєкторію вашої мережі під відповідною поверхнею витрат, 

147
00:10:16,747 --> 00:10:20,877
це було б схоже на п’яного чоловіка, який безцільно спотикається вниз з пагорба, 

148
00:10:20,877 --> 00:10:23,938
але робить швидкі кроки, а не на ретельно обчислену людину, 

149
00:10:23,938 --> 00:10:26,742
яка визначає точний напрямок спуску для кожного кроку. 

150
00:10:26,742 --> 00:10:30,160
перш ніж зробити дуже повільний і обережний крок у цьому напрямку. 

151
00:10:31,540 --> 00:10:34,660
Ця техніка називається стохастичним градієнтним спуском. 

152
00:10:35,960 --> 00:10:39,620
Тут багато чого відбувається, тож давайте просто підсумуємо це для себе, чи не так? 

153
00:10:40,440 --> 00:10:43,326
Зворотне розповсюдження — це алгоритм для визначення того, 

154
00:10:43,326 --> 00:10:47,437
як окремий навчальний приклад хотів би підштовхнути вагові коефіцієнти та зміщення, 

155
00:10:47,437 --> 00:10:50,715
не лише з точки зору того, чи мають вони зростати чи зменшуватися, 

156
00:10:50,715 --> 00:10:54,434
а й з точки зору того, які відносні пропорції цих змін викликають найшвидше 

157
00:10:54,434 --> 00:10:55,560
зменшення до вартість. 

158
00:10:56,260 --> 00:11:00,518
Справжній крок градієнтного спуску передбачав би виконання цього для всіх ваших 

159
00:11:00,518 --> 00:11:04,989
десятків і тисяч навчальних прикладів і усереднення бажаних змін, які ви отримуєте, 

160
00:11:04,989 --> 00:11:09,088
але це повільно з обчислювальної точки зору, тому замість цього ви випадково 

161
00:11:09,088 --> 00:11:13,240
розбиваєте дані на міні-пакети та обчислюєте кожен крок відносно міні-партія. 

162
00:11:14,000 --> 00:11:17,925
Неодноразово проходячи всі міні-пакети та вносячи ці коригування, 

163
00:11:17,925 --> 00:11:20,959
ви досягнете локального мінімуму функції вартості, 

164
00:11:20,959 --> 00:11:25,540
тобто ваша мережа зрештою справді добре впорається з навчальними прикладами. 

165
00:11:27,240 --> 00:11:31,846
З огляду на все сказане, кожен рядок коду, який буде використовуватися для реалізації 

166
00:11:31,846 --> 00:11:36,666
backprop, насправді відповідає тому, що ви зараз бачили, принаймні в неофіційних термінах.

167
00:11:36,666 --> 00:11:36,720
 

168
00:11:37,560 --> 00:11:40,549
Але інколи знати, що робить математика, — це лише половина успіху, 

169
00:11:40,549 --> 00:11:44,120
а просто представити прокляту річ — це те, де все стає заплутаним і заплутаним. 

170
00:11:44,860 --> 00:11:47,101
Отже, для тих із вас, хто хоче заглибитися глибше, 

171
00:11:47,101 --> 00:11:50,486
наступне відео розповідає про ті самі ідеї, які щойно були представлені тут, 

172
00:11:50,486 --> 00:11:52,947
але з точки зору основного обчислення, яке, сподіваюся, 

173
00:11:52,947 --> 00:11:56,420
має зробити його трохи більш знайомим, оскільки ви бачите тему в інші ресурси. 

174
00:11:57,340 --> 00:11:59,585
Перед цим варто підкреслити одну річ: для того, 

175
00:11:59,585 --> 00:12:03,046
щоб цей алгоритм працював, і це стосується всіх видів машинного навчання, 

176
00:12:03,046 --> 00:12:05,900
окрім нейронних мереж, вам потрібно багато навчальних даних. 

177
00:12:06,420 --> 00:12:10,107
У нашому випадку одна річ, яка робить рукописні цифри таким гарним прикладом, 

178
00:12:10,107 --> 00:12:13,463
полягає в тому, що існує база даних MNIST з такою кількістю прикладів, 

179
00:12:13,463 --> 00:12:14,740
які були позначені людьми. 

180
00:12:15,300 --> 00:12:19,160
Тож поширена проблема, з якою ті з вас, хто працює у сфері машинного навчання, знайомі, 

181
00:12:19,160 --> 00:12:22,713
— це просто отримати мічені навчальні дані, які вам дійсно потрібні, чи то люди, 

182
00:12:22,713 --> 00:12:25,784
які позначають десятки тисяч зображень, чи будь-який інший тип даних, 

183
00:12:25,784 --> 00:12:27,100
з яким ви можете мати справу. 

