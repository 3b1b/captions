1
00:00:00,000 --> 00:00:09,640
Тут ми розглядаємо зворотне поширення, основний алгоритм навчання нейронних мереж.

2
00:00:09,640 --> 00:00:12,477
Після короткого підсумку того, де ми зараз, перше, що я зроблю,

3
00:00:12,477 --> 00:00:15,848
це інтуїтивно зрозуміле керівництво для того, що насправді робить алгоритм,

4
00:00:15,848 --> 00:00:17,400
без будь-яких посилань на формули.

5
00:00:17,400 --> 00:00:20,209
Тоді для тих із вас, хто хоче зануритися в математику,

6
00:00:20,209 --> 00:00:24,040
наступне відео розповідає про обчислення, що лежить в основі всього цього.

7
00:00:24,040 --> 00:00:28,082
Якщо ви переглянули останні два відео або якщо ви просто переходите з відповідним фоном,

8
00:00:28,082 --> 00:00:31,080
ви знаєте, що таке нейронна мережа та як вона передає інформацію.

9
00:00:31,080 --> 00:00:34,939
Тут ми робимо класичний приклад розпізнавання рукописних цифр,

10
00:00:34,939 --> 00:00:39,534
піксельні значення яких надходять на перший рівень мережі з 784 нейронами,

11
00:00:39,534 --> 00:00:44,680
і я показую мережу з двома прихованими шарами, які мають лише по 16 нейронів кожен,

12
00:00:44,680 --> 00:00:49,520
і вихідним шар з 10 нейронів, що вказує, яку цифру мережа обирає як відповідь.

13
00:00:49,520 --> 00:00:54,630
Я також очікую, що ви зрозумієте градієнтний спуск, як описано в останньому відео,

14
00:00:54,630 --> 00:00:58,693
і те, як ми маємо на увазі під навчанням те, що ми хочемо знайти,

15
00:00:58,693 --> 00:01:02,080
які ваги та зміщення мінімізують певну функцію витрат.

16
00:01:02,080 --> 00:01:07,615
Як швидке нагадування: для вартості одного навчального прикладу ви берете результат,

17
00:01:07,615 --> 00:01:11,913
який дає мережа, разом із результатом, який ви хотіли б отримати,

18
00:01:11,913 --> 00:01:15,560
і додаєте квадрати відмінностей між кожним компонентом.

19
00:01:15,560 --> 00:01:20,760
Зробивши це для всіх ваших десятків тисяч навчальних прикладів і усереднивши результати,

20
00:01:20,760 --> 00:01:23,040
ви отримаєте загальну вартість мережі.

21
00:01:23,040 --> 00:01:29,850
Наче цього недостатньо для роздумів, як описано в останньому відео, те, що ми шукаємо,

22
00:01:29,850 --> 00:01:34,703
це від’ємний градієнт цієї функції витрат, який говорить вам,

23
00:01:34,703 --> 00:01:39,870
як вам потрібно змінити всі ваги та зміщення, усі ці підключення,

24
00:01:39,870 --> 00:01:43,080
щоб найбільш ефективно знизити вартість.

25
00:01:43,080 --> 00:01:46,308
Зворотне поширення, тема цього відео, — це алгоритм

26
00:01:46,308 --> 00:01:49,600
для обчислення цього божевільно складного градієнта.

27
00:01:49,600 --> 00:01:54,468
Одна ідея з останнього відео, яку я справді хочу, щоб ви зараз міцно запам’ятали,

28
00:01:54,468 --> 00:01:59,692
полягає в тому, що оскільки уявлення про вектор градієнта як напрямок у 13 000 вимірах,

29
00:01:59,692 --> 00:02:04,620
м’якше кажучи, виходить за межі нашої уяви, існує інша як ви можете думати про це.

30
00:02:04,620 --> 00:02:07,827
Величина кожного компонента тут говорить про те,

31
00:02:07,827 --> 00:02:11,820
наскільки функція витрат чутлива до кожної ваги та зміщення.

32
00:02:11,820 --> 00:02:16,216
Наприклад, скажімо, ви виконуєте процес, який я збираюся описати,

33
00:02:16,216 --> 00:02:21,611
і обчислюєте від’ємний градієнт, і компонент, пов’язаний із вагою на цьому краю,

34
00:02:21,611 --> 00:02:26,940
дорівнює 3.2, тоді як компонент, пов’язаний із цим краєм, тут має значення 0.1.

35
00:02:26,940 --> 00:02:33,024
Ви б це інтерпретували так: вартість функції в 32 рази чутливіша до змін у цій

36
00:02:33,024 --> 00:02:37,261
першій вазі, отже, якщо ви трішки зміните це значення,

37
00:02:37,261 --> 00:02:42,499
це призведе до певних змін у вартості, і ця зміна у 32 рази більше,

38
00:02:42,499 --> 00:02:45,580
ніж те саме ворушіння цієї другої ваги.

39
00:02:45,580 --> 00:02:50,246
Особисто, коли я вперше дізнався про зворотне розповсюдження, я вважав,

40
00:02:50,246 --> 00:02:55,820
що найбільш заплутаним аспектом були лише нотація та погоня за індексом усього цього.

41
00:02:55,820 --> 00:03:00,111
Але як тільки ви розгортаєте, що насправді робить кожна частина цього алгоритму,

42
00:03:00,111 --> 00:03:04,084
кожен окремий ефект, який він має, насправді досить інтуїтивно зрозумілий,

43
00:03:04,084 --> 00:03:07,740
просто є багато маленьких коригувань, які накладаються одне на одне.

44
00:03:07,740 --> 00:03:12,386
Тож я почну тут із повного ігнорування нотації та просто покроково

45
00:03:12,386 --> 00:03:17,380
розповім про вплив кожного навчального прикладу на ваги та упередження.

46
00:03:17,380 --> 00:03:22,107
Оскільки функція витрат передбачає усереднення певної вартості кожного прикладу

47
00:03:22,107 --> 00:03:24,885
за всіма десятками тисяч навчальних прикладів,

48
00:03:24,885 --> 00:03:29,908
спосіб коригування ваг і зміщень для одного кроку градієнтного спуску також залежить

49
00:03:29,908 --> 00:03:31,740
від кожного окремого прикладу.

50
00:03:31,740 --> 00:03:34,374
Точніше, в принципі так і повинно бути, але для ефективності

51
00:03:34,374 --> 00:03:37,182
обчислень ми пізніше зробимо невеликий трюк, щоб вам не потрібно

52
00:03:37,182 --> 00:03:39,860
було використовувати кожен окремий приклад для кожного кроку.

53
00:03:39,860 --> 00:03:43,238
В інших випадках, прямо зараз, все, що ми збираємося зробити,

54
00:03:43,238 --> 00:03:46,780
це зосередити нашу увагу на одному прикладі, цьому зображенні 2.

55
00:03:46,780 --> 00:03:51,740
Який вплив повинен мати цей навчальний приклад на те, як коригуються ваги та зміщення?

56
00:03:51,740 --> 00:03:56,340
Припустімо, що ми перебуваємо в точці, коли мережа ще недостатньо навчена,

57
00:03:56,340 --> 00:04:01,676
тому активації у виводі виглядатимуть досить випадковими, можливо, приблизно 0.5, 0.8,

58
00:04:01,676 --> 00:04:02,780
0.2, далі і далі.

59
00:04:02,780 --> 00:04:07,722
Ми не можемо напряму змінити ці активації, ми можемо лише впливати на ваги та зміщення,

60
00:04:07,722 --> 00:04:10,980
але корисно відстежувати, які коригування, які ми хочемо,

61
00:04:10,980 --> 00:04:13,340
мають відбутися на цьому вихідному рівні.

62
00:04:13,340 --> 00:04:17,730
І оскільки ми хочемо, щоб воно класифікувало зображення як 2, ми хочемо,

63
00:04:17,730 --> 00:04:21,700
щоб це третє значення було підштовхнуто вгору, а всі інші – вниз.

64
00:04:21,700 --> 00:04:25,557
Крім того, розміри цих поштовхів мають бути пропорційними до того,

65
00:04:25,557 --> 00:04:30,220
наскільки далеко кожне поточне значення знаходиться від його цільового значення.

66
00:04:30,220 --> 00:04:35,507
Наприклад, збільшення активації нейрона № 2 у певному сенсі важливіше,

67
00:04:35,507 --> 00:04:42,060
ніж зменшення активності нейрона № 8, яке вже досить близько до того, де воно має бути.

68
00:04:42,060 --> 00:04:45,870
Отже, збільшуючи масштаб, давайте зосередимося лише на цьому одному нейроні,

69
00:04:45,870 --> 00:04:47,900
тому, чию активацію ми хочемо збільшити.

70
00:04:47,900 --> 00:04:52,450
Пам’ятайте, що активація визначається як певна зважена сума всіх

71
00:04:52,450 --> 00:04:55,810
активацій на попередньому рівні, плюс зміщення,

72
00:04:55,810 --> 00:05:01,900
яке потім підключається до чогось на зразок функції сигмоїдної сквишификации або ReLU.

73
00:05:01,900 --> 00:05:08,060
Отже, є три різні шляхи, які можна об’єднати разом, щоб збільшити цю активність.

74
00:05:08,060 --> 00:05:11,849
Ви можете збільшити зміщення, ви можете збільшити ваги,

75
00:05:11,849 --> 00:05:15,300
і ви можете змінити активації з попереднього шару.

76
00:05:15,300 --> 00:05:17,924
Зосереджуючись на тому, як слід регулювати ваги,

77
00:05:17,924 --> 00:05:21,460
зверніть увагу на те, що ваги насправді мають різні рівні впливу.

78
00:05:21,460 --> 00:05:27,245
Зв’язки з найяскравішими нейронами з попереднього шару мають найбільший ефект,

79
00:05:27,245 --> 00:05:31,420
оскільки ці ваги множаться на більші значення активації.

80
00:05:31,420 --> 00:05:35,579
Отже, якщо ви збільшите одну з цих ваг, це справді матиме сильніший

81
00:05:35,579 --> 00:05:40,778
вплив на кінцеву функцію витрат, ніж збільшення ваг зв’язків із димерними нейронами,

82
00:05:40,778 --> 00:05:44,020
принаймні, що стосується цього навчального прикладу.

83
00:05:44,020 --> 00:05:46,945
Пам’ятайте, що коли ми говоримо про градієнтний спуск,

84
00:05:46,945 --> 00:05:50,349
нам важливо не лише підштовхнути кожен компонент угору чи вниз,

85
00:05:50,349 --> 00:05:54,020
нам важливо, які з них дають вам найбільшу віддачу від ваших грошей.

86
00:05:54,020 --> 00:05:58,093
Це, до речі, принаймні дещо нагадує теорію в нейронауці про те,

87
00:05:58,093 --> 00:06:01,657
як навчаються біологічні мережі нейронів, теорію Хебба,

88
00:06:01,657 --> 00:06:06,940
яку часто підсумовують фразою: нейрони, які запускаються разом, з’єднуються разом.

89
00:06:06,940 --> 00:06:13,276
Тут найбільше збільшення ваги, найбільше зміцнення зв’язків відбувається між нейронами,

90
00:06:13,276 --> 00:06:18,100
які є найбільш активними, і тими, які ми хочемо стати активнішими.

91
00:06:18,100 --> 00:06:21,400
У певному сенсі нейрони, які спрацьовують, коли бачать 2,

92
00:06:21,400 --> 00:06:25,440
стають сильніше пов’язаними з тими, хто спрацьовує, коли думає про це.

93
00:06:25,440 --> 00:06:30,045
Щоб було зрозуміло, я не в змозі робити твердження тим чи іншим чином щодо того,

94
00:06:30,045 --> 00:06:34,253
чи штучні мережі нейронів поводяться хоч як біологічний мозок, і ця ідея,

95
00:06:34,253 --> 00:06:38,063
що спрацьовує разом, супроводжується кількома значущими зірочками,

96
00:06:38,063 --> 00:06:41,760
але сприймається як дуже вільна Мені цікаво відзначити аналогію.

97
00:06:41,760 --> 00:06:45,505
У будь-якому разі, третій спосіб, яким ми можемо допомогти підвищити

98
00:06:45,505 --> 00:06:49,360
активацію цього нейрона, — змінити всі активації на попередньому шарі.

99
00:06:49,360 --> 00:06:54,431
А саме, якщо все, що пов’язано з цим нейроном цифри 2 із позитивною вагою,

100
00:06:54,431 --> 00:06:58,690
стане яскравішим, а якщо все, що пов’язано з негативною вагою,

101
00:06:58,690 --> 00:07:02,680
стане тьмянішим, тоді цей нейрон цифри 2 стане активнішим.

102
00:07:02,680 --> 00:07:07,445
Подібно до змін ваги, ви отримаєте максимальну віддачу від своїх грошей,

103
00:07:07,445 --> 00:07:10,840
шукаючи змін, пропорційних розміру відповідних ваг.

104
00:07:10,840 --> 00:07:15,557
Тепер, звичайно, ми не можемо безпосередньо впливати на ці активації,

105
00:07:15,557 --> 00:07:18,320
ми лише контролюємо ваги та упередження.

106
00:07:18,320 --> 00:07:23,960
Але так само, як і з останнім шаром, корисно занотувати, які ці бажані зміни.

107
00:07:23,960 --> 00:07:27,337
Але майте на увазі, якщо зменшити масштаб на один крок тут,

108
00:07:27,337 --> 00:07:30,040
це лише те, що хоче вихідний нейрон з цифрою 2.

109
00:07:30,040 --> 00:07:36,113
Пам’ятайте, ми також хочемо, щоб усі інші нейрони в останньому шарі стали менш активними,

110
00:07:36,113 --> 00:07:40,635
і кожен із цих інших вихідних нейронів має власні думки щодо того,

111
00:07:40,635 --> 00:07:43,200
що має статися з передостаннім шаром.

112
00:07:43,200 --> 00:07:49,287
Отже, бажання цього нейрона з цифрою 2 додається разом із бажаннями всіх інших вихідних

113
00:07:49,287 --> 00:07:53,576
нейронів щодо того, що має статися з цим передостаннім шаром,

114
00:07:53,576 --> 00:07:58,211
знову ж таки пропорційно до відповідних ваг і пропорційно до того,

115
00:07:58,211 --> 00:08:01,740
скільки потрібно кожному з цих нейронів змінювати.

116
00:08:01,740 --> 00:08:05,940
Ось тут і виникає ідея розповсюдження назад.

117
00:08:05,940 --> 00:08:10,956
Додавши разом усі ці бажані ефекти, ви, по суті, отримаєте список підштовхувань,

118
00:08:10,956 --> 00:08:14,300
які ви хочете виконати на цьому передостанньому шарі.

119
00:08:14,300 --> 00:08:19,283
І як тільки ви їх отримаєте, ви можете рекурсивно застосувати той самий

120
00:08:19,283 --> 00:08:23,781
процес до відповідних ваг і зміщень, які визначають ці значення,

121
00:08:23,781 --> 00:08:29,180
повторюючи той самий процес, який я щойно пройшов, і рухаючись назад мережею.

122
00:08:29,180 --> 00:08:32,746
І якщо трохи зменшити масштаб, пам’ятайте, що це все лише те,

123
00:08:32,746 --> 00:08:37,520
як окремий навчальний приклад хоче підштовхнути до кожного з цих ваг і упереджень.

124
00:08:37,520 --> 00:08:40,108
Якби ми лише прислухалися до того, що хоче ця двоє,

125
00:08:40,108 --> 00:08:44,140
мережа зрештою була б стимулювана просто класифікувати всі зображення як двійку.

126
00:08:44,140 --> 00:08:50,101
Отже, що ви робите, це проходите ту саму процедуру підтримки для

127
00:08:50,101 --> 00:08:56,246
кожного іншого прикладу навчання, записуючи, як кожен із них хотів

128
00:08:56,246 --> 00:09:02,300
би змінити ваги та зміщення, і усереднюєте разом ці бажані зміни.

129
00:09:02,300 --> 00:09:06,579
Цей набір усереднених підштовхувань до кожної ваги та зміщення є,

130
00:09:06,579 --> 00:09:09,886
грубо кажучи, від’ємним градієнтом функції витрат,

131
00:09:09,886 --> 00:09:14,360
згаданим в останньому відео, або принаймні чимось пропорційним йому.

132
00:09:14,360 --> 00:09:19,110
Я кажу вільно, лише тому, що мені ще належить отримати точні кількісні

133
00:09:19,110 --> 00:09:24,598
дані про ці підштовхи, але якщо ви зрозуміли кожну зміну, про яку я щойно згадав,

134
00:09:24,598 --> 00:09:29,683
чому одні пропорційно більші за інші, і як їх усіх потрібно додавати разом,

135
00:09:29,683 --> 00:09:34,100
ви розумієте механізм для що насправді робить зворотне поширення.

136
00:09:34,100 --> 00:09:38,049
До речі, на практиці комп’ютерам потрібно надзвичайно багато часу,

137
00:09:38,049 --> 00:09:43,120
щоб підсумувати вплив кожного навчального прикладу кожного кроку градієнтного спуску.

138
00:09:43,120 --> 00:09:45,540
Отже, ось що зазвичай роблять замість цього.

139
00:09:45,540 --> 00:09:49,561
Ви випадковим чином перетасовуєте свої навчальні дані та розділяєте їх на цілу

140
00:09:49,561 --> 00:09:53,380
купу міні-пакетів, припустімо, що кожен із них має 100 прикладів навчання.

141
00:09:53,380 --> 00:09:56,980
Потім ви обчислюєте крок відповідно до міні-серії.

142
00:09:56,980 --> 00:10:01,981
Це не фактичний градієнт функції витрат, який залежить від усіх навчальних даних,

143
00:10:01,981 --> 00:10:06,068
а не ця крихітна підмножина, тому це не найефективніший крок униз,

144
00:10:06,068 --> 00:10:10,399
але кожна міні-серія дає вам досить гарне наближення, і, що важливіше,

145
00:10:10,399 --> 00:10:12,900
це дає вам значне прискорення обчислень.

146
00:10:12,900 --> 00:10:17,139
Якби ви побудували траєкторію вашої мережі під відповідною поверхнею витрат,

147
00:10:17,139 --> 00:10:21,599
це було б схоже на п’яного чоловіка, який безцільно спотикається вниз з пагорба,

148
00:10:21,599 --> 00:10:24,902
але робить швидкі кроки, а не на ретельно обчислену людину,

149
00:10:24,902 --> 00:10:27,931
яка визначає точний напрямок спуску для кожного кроку.

150
00:10:27,931 --> 00:10:31,620
перш ніж зробити дуже повільний і обережний крок у цьому напрямку.

151
00:10:31,620 --> 00:10:35,200
Ця техніка називається стохастичним градієнтним спуском.

152
00:10:35,200 --> 00:10:40,400
Тут багато чого відбувається, тож давайте просто підсумуємо це для себе, чи не так?

153
00:10:40,400 --> 00:10:43,424
Зворотне розповсюдження — це алгоритм для визначення того,

154
00:10:43,424 --> 00:10:47,730
як окремий навчальний приклад хотів би підштовхнути вагові коефіцієнти та зміщення,

155
00:10:47,730 --> 00:10:51,165
не лише з точки зору того, чи мають вони зростати чи зменшуватися,

156
00:10:51,165 --> 00:10:55,060
а й з точки зору того, які відносні пропорції цих змін викликають найшвидше

157
00:10:55,060 --> 00:10:56,240
зменшення до вартість.

158
00:10:56,240 --> 00:11:00,693
Справжній крок градієнтного спуску передбачав би виконання цього для всіх ваших

159
00:11:00,693 --> 00:11:05,370
десятків і тисяч навчальних прикладів і усереднення бажаних змін, які ви отримуєте,

160
00:11:05,370 --> 00:11:09,657
але це повільно з обчислювальної точки зору, тому замість цього ви випадково

161
00:11:09,657 --> 00:11:14,000
розбиваєте дані на міні-пакети та обчислюєте кожен крок відносно міні-партія.

162
00:11:14,000 --> 00:11:18,606
Неодноразово проходячи всі міні-пакети та вносячи ці коригування,

163
00:11:18,606 --> 00:11:22,165
ви досягнете локального мінімуму функції вартості,

164
00:11:22,165 --> 00:11:27,540
тобто ваша мережа зрештою справді добре впорається з навчальними прикладами.

165
00:11:27,540 --> 00:11:32,466
З огляду на все сказане, кожен рядок коду, який буде використовуватися для реалізації

166
00:11:32,466 --> 00:11:37,622
backprop, насправді відповідає тому, що ви зараз бачили, принаймні в неофіційних термінах.

167
00:11:37,622 --> 00:11:37,680


168
00:11:37,680 --> 00:11:40,916
Але інколи знати, що робить математика, — це лише половина успіху,

169
00:11:40,916 --> 00:11:44,780
а просто представити прокляту річ — це те, де все стає заплутаним і заплутаним.

170
00:11:44,780 --> 00:11:47,238
Отже, для тих із вас, хто хоче заглибитися глибше,

171
00:11:47,238 --> 00:11:50,951
наступне відео розповідає про ті самі ідеї, які щойно були представлені тут,

172
00:11:50,951 --> 00:11:53,651
але з точки зору основного обчислення, яке, сподіваюся,

173
00:11:53,651 --> 00:11:57,460
має зробити його трохи більш знайомим, оскільки ви бачите тему в інші ресурси.

174
00:11:57,460 --> 00:11:59,920
Перед цим варто підкреслити одну річ: для того,

175
00:11:59,920 --> 00:12:03,713
щоб цей алгоритм працював, і це стосується всіх видів машинного навчання,

176
00:12:03,713 --> 00:12:06,840
окрім нейронних мереж, вам потрібно багато навчальних даних.

177
00:12:06,840 --> 00:12:10,624
У нашому випадку одна річ, яка робить рукописні цифри таким гарним прикладом,

178
00:12:10,624 --> 00:12:14,069
полягає в тому, що існує база даних MNIST з такою кількістю прикладів,

179
00:12:14,069 --> 00:12:15,380
які були позначені людьми.

180
00:12:15,380 --> 00:12:19,312
Тож поширена проблема, з якою ті з вас, хто працює у сфері машинного навчання, знайомі,

181
00:12:19,312 --> 00:12:22,931
— це просто отримати мічені навчальні дані, які вам дійсно потрібні, чи то люди,

182
00:12:22,931 --> 00:12:26,059
які позначають десятки тисяч зображень, чи будь-який інший тип даних,

183
00:12:26,059 --> 00:12:27,400
з яким ви можете мати справу.

