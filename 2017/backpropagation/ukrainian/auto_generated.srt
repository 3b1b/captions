1
00:00:00,000 --> 00:00:09,640
Тут ми розглядаємо зворотне поширення, основний алгоритм навчання нейронних мереж.

2
00:00:09,640 --> 00:00:13,542
Після короткого підсумку того, де ми зараз, перше, що я зроблю, це інтуїтивно зрозуміле

3
00:00:13,542 --> 00:00:17,400
керівництво для того, що насправді робить алгоритм, без будь-яких посилань на формули.

4
00:00:17,400 --> 00:00:20,668
Тоді для тих із вас, хто хоче зануритися в математику, наступне

5
00:00:20,668 --> 00:00:24,040
відео розповідає про обчислення, що лежить в основі всього цього.

6
00:00:24,040 --> 00:00:27,764
Якщо ви переглянули останні два відео або якщо ви просто переходите з відповідним

7
00:00:27,764 --> 00:00:31,080
фоном, ви знаєте, що таке нейронна мережа та як вона передає інформацію.

8
00:00:31,080 --> 00:00:35,552
Тут ми робимо класичний приклад розпізнавання рукописних цифр, піксельні

9
00:00:35,552 --> 00:00:40,269
значення яких надходять на перший рівень мережі з 784 нейронами, і я показую

10
00:00:40,269 --> 00:00:44,802
мережу з двома прихованими шарами, які мають лише по 16 нейронів кожен, і

11
00:00:44,802 --> 00:00:49,520
вихідним шар з 10 нейронів, що вказує, яку цифру мережа обирає як відповідь.

12
00:00:49,520 --> 00:00:53,521
Я також очікую, що ви зрозумієте градієнтний спуск, як описано в

13
00:00:53,521 --> 00:00:57,770
останньому відео, і те, як ми маємо на увазі під навчанням те, що ми

14
00:00:57,770 --> 00:01:02,080
хочемо знайти, які ваги та зміщення мінімізують певну функцію витрат.

15
00:01:02,080 --> 00:01:06,443
Як швидке нагадування: для вартості одного навчального прикладу ви

16
00:01:06,443 --> 00:01:11,131
берете результат, який дає мережа, разом із результатом, який ви хотіли

17
00:01:11,131 --> 00:01:15,560
б отримати, і додаєте квадрати відмінностей між кожним компонентом.

18
00:01:15,560 --> 00:01:19,241
Зробивши це для всіх ваших десятків тисяч навчальних прикладів

19
00:01:19,241 --> 00:01:23,040
і усереднивши результати, ви отримаєте загальну вартість мережі.

20
00:01:23,040 --> 00:01:29,850
Наче цього недостатньо для роздумів, як описано в останньому відео, те, що ми шукаємо,

21
00:01:29,850 --> 00:01:36,582
це від’ємний градієнт цієї функції витрат, який говорить вам, як вам потрібно змінити

22
00:01:36,582 --> 00:01:43,080
всі ваги та зміщення, усі ці підключення, щоб найбільш ефективно знизити вартість.

23
00:01:43,080 --> 00:01:46,308
Зворотне поширення, тема цього відео, — це алгоритм

24
00:01:46,308 --> 00:01:49,600
для обчислення цього божевільно складного градієнта.

25
00:01:49,600 --> 00:01:54,468
Одна ідея з останнього відео, яку я справді хочу, щоб ви зараз міцно запам’ятали,

26
00:01:54,468 --> 00:01:59,692
полягає в тому, що оскільки уявлення про вектор градієнта як напрямок у 13 000 вимірах,

27
00:01:59,692 --> 00:02:04,620
м’якше кажучи, виходить за межі нашої уяви, існує інша як ви можете думати про це.

28
00:02:04,620 --> 00:02:08,481
Величина кожного компонента тут говорить про те, наскільки

29
00:02:08,481 --> 00:02:11,820
функція витрат чутлива до кожної ваги та зміщення.

30
00:02:11,820 --> 00:02:17,082
Наприклад, скажімо, ви виконуєте процес, який я збираюся описати, і обчислюєте

31
00:02:17,082 --> 00:02:22,210
від’ємний градієнт, і компонент, пов’язаний із вагою на цьому краю, дорівнює

32
00:02:22,210 --> 00:02:26,940
3.2, тоді як компонент, пов’язаний із цим краєм, тут має значення 0.1.

33
00:02:26,940 --> 00:02:33,024
Ви б це інтерпретували так: вартість функції в 32 рази чутливіша до змін у цій

34
00:02:33,024 --> 00:02:39,340
першій вазі, отже, якщо ви трішки зміните це значення, це призведе до певних змін

35
00:02:39,340 --> 00:02:45,580
у вартості, і ця зміна у 32 рази більше, ніж те саме ворушіння цієї другої ваги.

36
00:02:45,580 --> 00:02:50,440
Особисто, коли я вперше дізнався про зворотне розповсюдження, я вважав, що

37
00:02:50,440 --> 00:02:55,820
найбільш заплутаним аспектом були лише нотація та погоня за індексом усього цього.

38
00:02:55,820 --> 00:02:59,528
Але як тільки ви розгортаєте, що насправді робить кожна частина цього

39
00:02:59,528 --> 00:03:03,448
алгоритму, кожен окремий ефект, який він має, насправді досить інтуїтивно

40
00:03:03,448 --> 00:03:07,740
зрозумілий, просто є багато маленьких коригувань, які накладаються одне на одне.

41
00:03:07,740 --> 00:03:12,386
Тож я почну тут із повного ігнорування нотації та просто покроково

42
00:03:12,386 --> 00:03:17,380
розповім про вплив кожного навчального прикладу на ваги та упередження.

43
00:03:17,380 --> 00:03:22,107
Оскільки функція витрат передбачає усереднення певної вартості кожного прикладу

44
00:03:22,107 --> 00:03:26,835
за всіма десятками тисяч навчальних прикладів, спосіб коригування ваг і зміщень

45
00:03:26,835 --> 00:03:31,740
для одного кроку градієнтного спуску також залежить від кожного окремого прикладу.

46
00:03:31,740 --> 00:03:34,374
Точніше, в принципі так і повинно бути, але для ефективності

47
00:03:34,374 --> 00:03:37,182
обчислень ми пізніше зробимо невеликий трюк, щоб вам не потрібно

48
00:03:37,182 --> 00:03:39,860
було використовувати кожен окремий приклад для кожного кроку.

49
00:03:39,860 --> 00:03:43,238
В інших випадках, прямо зараз, все, що ми збираємося зробити,

50
00:03:43,238 --> 00:03:46,780
це зосередити нашу увагу на одному прикладі, цьому зображенні 2.

51
00:03:46,780 --> 00:03:51,740
Який вплив повинен мати цей навчальний приклад на те, як коригуються ваги та зміщення?

52
00:03:51,740 --> 00:03:57,260
Припустімо, що ми перебуваємо в точці, коли мережа ще недостатньо навчена, тому активації

53
00:03:57,260 --> 00:04:02,780
у виводі виглядатимуть досить випадковими, можливо, приблизно 0.5, 0.8, 0.2, далі і далі.

54
00:04:02,780 --> 00:04:06,037
Ми не можемо напряму змінити ці активації, ми можемо лише

55
00:04:06,037 --> 00:04:09,408
впливати на ваги та зміщення, але корисно відстежувати, які

56
00:04:09,408 --> 00:04:13,340
коригування, які ми хочемо, мають відбутися на цьому вихідному рівні.

57
00:04:13,340 --> 00:04:17,249
І оскільки ми хочемо, щоб воно класифікувало зображення як 2, ми

58
00:04:17,249 --> 00:04:21,700
хочемо, щоб це третє значення було підштовхнуто вгору, а всі інші – вниз.

59
00:04:21,700 --> 00:04:26,132
Крім того, розміри цих поштовхів мають бути пропорційними до того, наскільки

60
00:04:26,132 --> 00:04:30,220
далеко кожне поточне значення знаходиться від його цільового значення.

61
00:04:30,220 --> 00:04:35,804
Наприклад, збільшення активації нейрона № 2 у певному сенсі важливіше, ніж

62
00:04:35,804 --> 00:04:42,060
зменшення активності нейрона № 8, яке вже досить близько до того, де воно має бути.

63
00:04:42,060 --> 00:04:45,078
Отже, збільшуючи масштаб, давайте зосередимося лише на цьому

64
00:04:45,078 --> 00:04:47,900
одному нейроні, тому, чию активацію ми хочемо збільшити.

65
00:04:47,900 --> 00:04:52,450
Пам’ятайте, що активація визначається як певна зважена сума всіх

66
00:04:52,450 --> 00:04:57,490
активацій на попередньому рівні, плюс зміщення, яке потім підключається

67
00:04:57,490 --> 00:05:01,900
до чогось на зразок функції сигмоїдної сквишификации або ReLU.

68
00:05:01,900 --> 00:05:08,060
Отже, є три різні шляхи, які можна об’єднати разом, щоб збільшити цю активність.

69
00:05:08,060 --> 00:05:11,443
Ви можете збільшити зміщення, ви можете збільшити

70
00:05:11,443 --> 00:05:15,300
ваги, і ви можете змінити активації з попереднього шару.

71
00:05:15,300 --> 00:05:18,406
Зосереджуючись на тому, як слід регулювати ваги, зверніть

72
00:05:18,406 --> 00:05:21,460
увагу на те, що ваги насправді мають різні рівні впливу.

73
00:05:21,460 --> 00:05:26,732
Зв’язки з найяскравішими нейронами з попереднього шару мають найбільший

74
00:05:26,732 --> 00:05:31,420
ефект, оскільки ці ваги множаться на більші значення активації.

75
00:05:31,420 --> 00:05:35,579
Отже, якщо ви збільшите одну з цих ваг, це справді матиме сильніший

76
00:05:35,579 --> 00:05:39,493
вплив на кінцеву функцію витрат, ніж збільшення ваг зв’язків із

77
00:05:39,493 --> 00:05:44,020
димерними нейронами, принаймні, що стосується цього навчального прикладу.

78
00:05:44,020 --> 00:05:47,158
Пам’ятайте, що коли ми говоримо про градієнтний спуск, нам

79
00:05:47,158 --> 00:05:50,349
важливо не лише підштовхнути кожен компонент угору чи вниз,

80
00:05:50,349 --> 00:05:54,020
нам важливо, які з них дають вам найбільшу віддачу від ваших грошей.

81
00:05:54,020 --> 00:05:58,284
Це, до речі, принаймні дещо нагадує теорію в нейронауці про те, як

82
00:05:58,284 --> 00:06:02,293
навчаються біологічні мережі нейронів, теорію Хебба, яку часто

83
00:06:02,293 --> 00:06:06,940
підсумовують фразою: нейрони, які запускаються разом, з’єднуються разом.

84
00:06:06,940 --> 00:06:12,484
Тут найбільше збільшення ваги, найбільше зміцнення зв’язків відбувається між

85
00:06:12,484 --> 00:06:18,100
нейронами, які є найбільш активними, і тими, які ми хочемо стати активнішими.

86
00:06:18,100 --> 00:06:21,798
У певному сенсі нейрони, які спрацьовують, коли бачать 2, стають

87
00:06:21,798 --> 00:06:25,440
сильніше пов’язаними з тими, хто спрацьовує, коли думає про це.

88
00:06:25,440 --> 00:06:29,420
Щоб було зрозуміло, я не в змозі робити твердження тим чи іншим чином

89
00:06:29,420 --> 00:06:33,628
щодо того, чи штучні мережі нейронів поводяться хоч як біологічний мозок,

90
00:06:33,628 --> 00:06:37,438
і ця ідея, що спрацьовує разом, супроводжується кількома значущими

91
00:06:37,438 --> 00:06:41,760
зірочками, але сприймається як дуже вільна Мені цікаво відзначити аналогію.

92
00:06:41,760 --> 00:06:45,505
У будь-якому разі, третій спосіб, яким ми можемо допомогти підвищити

93
00:06:45,505 --> 00:06:49,360
активацію цього нейрона, — змінити всі активації на попередньому шарі.

94
00:06:49,360 --> 00:06:53,957
А саме, якщо все, що пов’язано з цим нейроном цифри 2 із позитивною

95
00:06:53,957 --> 00:06:58,217
вагою, стане яскравішим, а якщо все, що пов’язано з негативною

96
00:06:58,217 --> 00:07:02,680
вагою, стане тьмянішим, тоді цей нейрон цифри 2 стане активнішим.

97
00:07:02,680 --> 00:07:06,531
Подібно до змін ваги, ви отримаєте максимальну віддачу від

98
00:07:06,531 --> 00:07:10,840
своїх грошей, шукаючи змін, пропорційних розміру відповідних ваг.

99
00:07:10,840 --> 00:07:14,613
Тепер, звичайно, ми не можемо безпосередньо впливати на

100
00:07:14,613 --> 00:07:18,320
ці активації, ми лише контролюємо ваги та упередження.

101
00:07:18,320 --> 00:07:23,960
Але так само, як і з останнім шаром, корисно занотувати, які ці бажані зміни.

102
00:07:23,960 --> 00:07:27,056
Але майте на увазі, якщо зменшити масштаб на один крок

103
00:07:27,056 --> 00:07:30,040
тут, це лише те, що хоче вихідний нейрон з цифрою 2.

104
00:07:30,040 --> 00:07:34,291
Пам’ятайте, ми також хочемо, щоб усі інші нейрони в останньому

105
00:07:34,291 --> 00:07:38,745
шарі стали менш активними, і кожен із цих інших вихідних нейронів

106
00:07:38,745 --> 00:07:43,200
має власні думки щодо того, що має статися з передостаннім шаром.

107
00:07:43,200 --> 00:07:49,287
Отже, бажання цього нейрона з цифрою 2 додається разом із бажаннями всіх інших вихідних

108
00:07:49,287 --> 00:07:55,513
нейронів щодо того, що має статися з цим передостаннім шаром, знову ж таки пропорційно до

109
00:07:55,513 --> 00:08:01,740
відповідних ваг і пропорційно до того, скільки потрібно кожному з цих нейронів змінювати.

110
00:08:01,740 --> 00:08:05,940
Ось тут і виникає ідея розповсюдження назад.

111
00:08:05,940 --> 00:08:10,027
Додавши разом усі ці бажані ефекти, ви, по суті, отримаєте список

112
00:08:10,027 --> 00:08:14,300
підштовхувань, які ви хочете виконати на цьому передостанньому шарі.

113
00:08:14,300 --> 00:08:19,283
І як тільки ви їх отримаєте, ви можете рекурсивно застосувати той самий

114
00:08:19,283 --> 00:08:24,542
процес до відповідних ваг і зміщень, які визначають ці значення, повторюючи

115
00:08:24,542 --> 00:08:29,180
той самий процес, який я щойно пройшов, і рухаючись назад мережею.

116
00:08:29,180 --> 00:08:33,378
І якщо трохи зменшити масштаб, пам’ятайте, що це все лише те, як окремий

117
00:08:33,378 --> 00:08:37,520
навчальний приклад хоче підштовхнути до кожного з цих ваг і упереджень.

118
00:08:37,520 --> 00:08:40,854
Якби ми лише прислухалися до того, що хоче ця двоє, мережа зрештою

119
00:08:40,854 --> 00:08:44,140
була б стимулювана просто класифікувати всі зображення як двійку.

120
00:08:44,140 --> 00:08:50,101
Отже, що ви робите, це проходите ту саму процедуру підтримки для

121
00:08:50,101 --> 00:08:56,246
кожного іншого прикладу навчання, записуючи, як кожен із них хотів

122
00:08:56,246 --> 00:09:02,300
би змінити ваги та зміщення, і усереднюєте разом ці бажані зміни.

123
00:09:02,300 --> 00:09:06,384
Цей набір усереднених підштовхувань до кожної ваги та зміщення

124
00:09:06,384 --> 00:09:10,469
є, грубо кажучи, від’ємним градієнтом функції витрат, згаданим

125
00:09:10,469 --> 00:09:14,360
в останньому відео, або принаймні чимось пропорційним йому.

126
00:09:14,360 --> 00:09:19,110
Я кажу вільно, лише тому, що мені ще належить отримати точні кількісні

127
00:09:19,110 --> 00:09:24,062
дані про ці підштовхи, але якщо ви зрозуміли кожну зміну, про яку я щойно

128
00:09:24,062 --> 00:09:29,215
згадав, чому одні пропорційно більші за інші, і як їх усіх потрібно додавати

129
00:09:29,215 --> 00:09:34,100
разом, ви розумієте механізм для що насправді робить зворотне поширення.

130
00:09:34,100 --> 00:09:38,285
До речі, на практиці комп’ютерам потрібно надзвичайно багато часу, щоб

131
00:09:38,285 --> 00:09:43,120
підсумувати вплив кожного навчального прикладу кожного кроку градієнтного спуску.

132
00:09:43,120 --> 00:09:45,540
Отже, ось що зазвичай роблять замість цього.

133
00:09:45,540 --> 00:09:49,561
Ви випадковим чином перетасовуєте свої навчальні дані та розділяєте їх на цілу

134
00:09:49,561 --> 00:09:53,380
купу міні-пакетів, припустімо, що кожен із них має 100 прикладів навчання.

135
00:09:53,380 --> 00:09:56,980
Потім ви обчислюєте крок відповідно до міні-серії.

136
00:09:56,980 --> 00:10:02,286
Це не фактичний градієнт функції витрат, який залежить від усіх навчальних даних, а не

137
00:10:02,286 --> 00:10:07,593
ця крихітна підмножина, тому це не найефективніший крок униз, але кожна міні-серія дає

138
00:10:07,593 --> 00:10:12,900
вам досить гарне наближення, і, що важливіше, це дає вам значне прискорення обчислень.

139
00:10:12,900 --> 00:10:17,580
Якби ви побудували траєкторію вашої мережі під відповідною поверхнею витрат, це було

140
00:10:17,580 --> 00:10:22,204
б схоже на п’яного чоловіка, який безцільно спотикається вниз з пагорба, але робить

141
00:10:22,204 --> 00:10:26,884
швидкі кроки, а не на ретельно обчислену людину, яка визначає точний напрямок спуску

142
00:10:26,884 --> 00:10:31,620
для кожного кроку. перш ніж зробити дуже повільний і обережний крок у цьому напрямку.

143
00:10:31,620 --> 00:10:35,200
Ця техніка називається стохастичним градієнтним спуском.

144
00:10:35,200 --> 00:10:40,400
Тут багато чого відбувається, тож давайте просто підсумуємо це для себе, чи не так?

145
00:10:40,400 --> 00:10:44,552
Зворотне розповсюдження — це алгоритм для визначення того, як окремий навчальний

146
00:10:44,552 --> 00:10:48,550
приклад хотів би підштовхнути вагові коефіцієнти та зміщення, не лише з точки

147
00:10:48,550 --> 00:10:52,549
зору того, чи мають вони зростати чи зменшуватися, а й з точки зору того, які

148
00:10:52,549 --> 00:10:56,240
відносні пропорції цих змін викликають найшвидше зменшення до вартість.

149
00:10:56,240 --> 00:11:00,693
Справжній крок градієнтного спуску передбачав би виконання цього для всіх ваших

150
00:11:00,693 --> 00:11:05,370
десятків і тисяч навчальних прикладів і усереднення бажаних змін, які ви отримуєте,

151
00:11:05,370 --> 00:11:09,657
але це повільно з обчислювальної точки зору, тому замість цього ви випадково

152
00:11:09,657 --> 00:11:14,000
розбиваєте дані на міні-пакети та обчислюєте кожен крок відносно міні-партія.

153
00:11:14,000 --> 00:11:18,606
Неодноразово проходячи всі міні-пакети та вносячи ці коригування,

154
00:11:18,606 --> 00:11:22,933
ви досягнете локального мінімуму функції вартості, тобто ваша

155
00:11:22,933 --> 00:11:27,540
мережа зрештою справді добре впорається з навчальними прикладами.

156
00:11:27,540 --> 00:11:32,466
З огляду на все сказане, кожен рядок коду, який буде використовуватися для реалізації

157
00:11:32,466 --> 00:11:37,107
backprop, насправді відповідає тому, що ви зараз бачили, принаймні в неофіційних

158
00:11:37,107 --> 00:11:37,680
термінах.

159
00:11:37,680 --> 00:11:41,350
Але інколи знати, що робить математика, — це лише половина успіху, а просто

160
00:11:41,350 --> 00:11:44,780
представити прокляту річ — це те, де все стає заплутаним і заплутаним.

161
00:11:44,780 --> 00:11:49,070
Отже, для тих із вас, хто хоче заглибитися глибше, наступне відео розповідає про ті самі

162
00:11:49,070 --> 00:11:53,072
ідеї, які щойно були представлені тут, але з точки зору основного обчислення, яке,

163
00:11:53,072 --> 00:11:57,026
сподіваюся, має зробити його трохи більш знайомим, оскільки ви бачите тему в інші

164
00:11:57,026 --> 00:11:57,460
ресурси.

165
00:11:57,460 --> 00:12:00,791
Перед цим варто підкреслити одну річ: для того, щоб цей алгоритм

166
00:12:00,791 --> 00:12:04,020
працював, і це стосується всіх видів машинного навчання, окрім

167
00:12:04,020 --> 00:12:06,840
нейронних мереж, вам потрібно багато навчальних даних.

168
00:12:06,840 --> 00:12:11,110
У нашому випадку одна річ, яка робить рукописні цифри таким гарним прикладом, полягає в

169
00:12:11,110 --> 00:12:15,380
тому, що існує база даних MNIST з такою кількістю прикладів, які були позначені людьми.

170
00:12:15,380 --> 00:12:19,312
Тож поширена проблема, з якою ті з вас, хто працює у сфері машинного навчання, знайомі,

171
00:12:19,312 --> 00:12:23,110
— це просто отримати мічені навчальні дані, які вам дійсно потрібні, чи то люди, які

172
00:12:23,110 --> 00:12:27,042
позначають десятки тисяч зображень, чи будь-який інший тип даних, з яким ви можете мати

173
00:12:27,042 --> 00:12:27,400
справу.

