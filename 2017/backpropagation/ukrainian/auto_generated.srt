1
00:00:00,000 --> 00:00:09,640
Тут ми розглядаємо зворотне поширення, основний алгоритм навчання нейронних мереж.

2
00:00:09,640 --> 00:00:13,320
Після короткого підсумку того, де ми зараз, перше, що я зроблю, це інтуїтивно

3
00:00:13,320 --> 00:00:17,400
зрозуміле керівництво для того, що насправді робить алгоритм, без будь-яких посилань на формули.

4
00:00:17,400 --> 00:00:21,400
Тоді для тих із вас, хто хоче зануритися в математику, наступне

5
00:00:21,400 --> 00:00:24,040
відео розповідає про обчислення, що лежить в основі всього цього.

6
00:00:24,040 --> 00:00:27,320
Якщо ви переглянули останні два відео або якщо ви просто переходите з відповідним

7
00:00:27,320 --> 00:00:31,080
фоном, ви знаєте, що таке нейронна мережа та як вона передає інформацію.

8
00:00:31,080 --> 00:00:35,520
Тут ми робимо класичний приклад розпізнавання рукописних цифр, піксельні значення яких надходять

9
00:00:35,520 --> 00:00:40,280
на перший рівень мережі з 784 нейронами, і я показую мережу з

10
00:00:40,280 --> 00:00:44,720
двома прихованими шарами, які мають лише по 16 нейронів кожен, і вихідним

11
00:00:44,720 --> 00:00:49,520
шар з 10 нейронів, що вказує, яку цифру мережа обирає як відповідь.

12
00:00:49,520 --> 00:00:54,480
Я також очікую, що ви зрозумієте градієнтний спуск, як описано в останньому

13
00:00:54,480 --> 00:01:00,160
відео, і те, як ми маємо на увазі під навчанням те, що

14
00:01:00,160 --> 00:01:02,080
ми хочемо знайти, які ваги та зміщення мінімізують певну функцію витрат.

15
00:01:02,080 --> 00:01:07,560
Як швидке нагадування: для вартості одного навчального прикладу ви берете

16
00:01:07,560 --> 00:01:12,920
результат, який дає мережа, разом із результатом, який ви хотіли

17
00:01:12,920 --> 00:01:15,560
б отримати, і додаєте квадрати відмінностей між кожним компонентом.

18
00:01:15,560 --> 00:01:20,160
Зробивши це для всіх ваших десятків тисяч навчальних прикладів

19
00:01:20,160 --> 00:01:23,040
і усереднивши результати, ви отримаєте загальну вартість мережі.

20
00:01:23,040 --> 00:01:26,320
Наче цього недостатньо для роздумів, як описано в останньому відео,

21
00:01:26,320 --> 00:01:31,700
те, що ми шукаємо, це від’ємний градієнт цієї функції витрат,

22
00:01:31,700 --> 00:01:36,000
який говорить вам, як вам потрібно змінити всі ваги та

23
00:01:36,000 --> 00:01:43,080
зміщення, усі ці підключення, щоб найбільш ефективно знизити вартість.

24
00:01:43,080 --> 00:01:48,600
Зворотне поширення, тема цього відео, — це

25
00:01:48,600 --> 00:01:49,600
алгоритм для обчислення цього божевільно складного градієнта.

26
00:01:49,600 --> 00:01:53,300
Одна ідея з останнього відео, яку я справді хочу, щоб ви

27
00:01:53,300 --> 00:01:58,280
зараз міцно запам’ятали, полягає в тому, що оскільки уявлення про вектор

28
00:01:58,280 --> 00:02:02,660
градієнта як напрямок у 13 000 вимірах, м’якше кажучи, виходить за

29
00:02:02,660 --> 00:02:04,620
межі нашої уяви, існує інша як ви можете думати про це.

30
00:02:04,620 --> 00:02:09,700
Величина кожного компонента тут говорить про те, наскільки

31
00:02:09,700 --> 00:02:11,820
функція витрат чутлива до кожної ваги та зміщення.

32
00:02:11,820 --> 00:02:15,180
Наприклад, скажімо, ви виконуєте процес, який я збираюся описати, і обчислюєте

33
00:02:15,180 --> 00:02:19,800
від’ємний градієнт, і компонент, пов’язаний із вагою на цьому краю,

34
00:02:19,800 --> 00:02:26,940
дорівнює 3. 2, тоді як компонент, пов’язаний із цим краєм, тут має значення 0. 1.

35
00:02:26,940 --> 00:02:31,520
Ви б це інтерпретували так: вартість функції в 32 рази чутливіша до

36
00:02:31,520 --> 00:02:36,100
змін у цій першій вазі, отже, якщо ви трішки зміните це

37
00:02:36,100 --> 00:02:40,780
значення, це призведе до певних змін у вартості, і ця зміна

38
00:02:40,780 --> 00:02:45,580
у 32 рази більше, ніж те саме ворушіння цієї другої ваги.

39
00:02:45,580 --> 00:02:52,500
Особисто, коли я вперше дізнався про зворотне розповсюдження, я вважав, що найбільш

40
00:02:52,500 --> 00:02:55,820
заплутаним аспектом були лише нотація та погоня за індексом усього цього.

41
00:02:55,820 --> 00:03:00,240
Але як тільки ви розгортаєте, що насправді робить кожна частина цього

42
00:03:00,240 --> 00:03:04,540
алгоритму, кожен окремий ефект, який він має, насправді досить інтуїтивно зрозумілий,

43
00:03:04,540 --> 00:03:07,740
просто є багато маленьких коригувань, які накладаються одне на одне.

44
00:03:07,740 --> 00:03:11,380
Тож я почну тут із повного ігнорування нотації та просто покроково

45
00:03:11,380 --> 00:03:17,380
розповім про вплив кожного навчального прикладу на ваги та упередження.

46
00:03:17,380 --> 00:03:21,880
Оскільки функція витрат передбачає усереднення певної вартості кожного прикладу за всіма

47
00:03:21,880 --> 00:03:26,980
десятками тисяч навчальних прикладів, спосіб коригування ваг і зміщень для

48
00:03:26,980 --> 00:03:31,740
одного кроку градієнтного спуску також залежить від кожного окремого прикладу.

49
00:03:31,740 --> 00:03:35,300
Точніше, в принципі так і повинно бути, але для ефективності обчислень ми пізніше зробимо

50
00:03:35,300 --> 00:03:39,860
невеликий трюк, щоб вам не потрібно було використовувати кожен окремий приклад для кожного кроку.

51
00:03:39,860 --> 00:03:44,460
В інших випадках, прямо зараз, все, що ми збираємося зробити,

52
00:03:44,460 --> 00:03:46,780
це зосередити нашу увагу на одному прикладі, цьому зображенні 2.

53
00:03:46,780 --> 00:03:51,740
Який вплив повинен мати цей навчальний приклад на те, як коригуються ваги та зміщення?

54
00:03:51,740 --> 00:03:56,040
Припустімо, що ми перебуваємо в точці, коли мережа ще недостатньо навчена, тому

55
00:03:56,040 --> 00:04:01,620
активації у виводі виглядатимуть досить випадковими, можливо, приблизно 0. 5, 0. 8, 0. 2,

56
00:04:01,620 --> 00:04:02,780
далі і далі.

57
00:04:02,780 --> 00:04:06,700
Ми не можемо напряму змінити ці активації, ми можемо лише

58
00:04:06,700 --> 00:04:11,380
впливати на ваги та зміщення, але корисно відстежувати, які коригування,

59
00:04:11,380 --> 00:04:13,340
які ми хочемо, мають відбутися на цьому вихідному рівні.

60
00:04:13,340 --> 00:04:18,220
І оскільки ми хочемо, щоб воно класифікувало зображення як 2, ми хочемо,

61
00:04:18,220 --> 00:04:21,700
щоб це третє значення було підштовхнуто вгору, а всі інші – вниз.

62
00:04:21,700 --> 00:04:27,620
Крім того, розміри цих поштовхів мають бути пропорційними до того,

63
00:04:27,620 --> 00:04:30,220
наскільки далеко кожне поточне значення знаходиться від його цільового значення.

64
00:04:30,220 --> 00:04:35,260
Наприклад, збільшення активації нейрона № 2 у певному сенсі

65
00:04:35,260 --> 00:04:39,620
важливіше, ніж зменшення активності нейрона № 8, яке вже

66
00:04:39,620 --> 00:04:42,060
досить близько до того, де воно має бути.

67
00:04:42,060 --> 00:04:46,260
Отже, збільшуючи масштаб, давайте зосередимося лише на цьому

68
00:04:46,260 --> 00:04:47,900
одному нейроні, тому, чию активацію ми хочемо збільшити.

69
00:04:47,900 --> 00:04:53,680
Пам’ятайте, що активація визначається як певна зважена сума всіх

70
00:04:53,680 --> 00:04:58,380
активацій на попередньому рівні, плюс зміщення, яке потім підключається

71
00:04:58,380 --> 00:05:01,900
до чогось на зразок функції сигмоїдної сквишификации або ReLU.

72
00:05:01,900 --> 00:05:07,060
Отже, є три різні шляхи, які можна

73
00:05:07,060 --> 00:05:08,060
об’єднати разом, щоб збільшити цю активність.

74
00:05:08,060 --> 00:05:12,800
Ви можете збільшити зміщення, ви можете збільшити ваги,

75
00:05:12,800 --> 00:05:15,300
і ви можете змінити активації з попереднього шару.

76
00:05:15,300 --> 00:05:19,720
Зосереджуючись на тому, як слід регулювати ваги, зверніть увагу

77
00:05:19,720 --> 00:05:21,460
на те, що ваги насправді мають різні рівні впливу.

78
00:05:21,460 --> 00:05:25,100
Зв’язки з найяскравішими нейронами з попереднього шару мають найбільший

79
00:05:25,100 --> 00:05:31,420
ефект, оскільки ці ваги множаться на більші значення активації.

80
00:05:31,420 --> 00:05:35,820
Отже, якщо ви збільшите одну з цих ваг, це справді

81
00:05:35,820 --> 00:05:40,900
матиме сильніший вплив на кінцеву функцію витрат, ніж збільшення ваг

82
00:05:40,900 --> 00:05:44,020
зв’язків із димерними нейронами, принаймні, що стосується цього навчального прикладу.

83
00:05:44,020 --> 00:05:48,700
Пам’ятайте, що коли ми говоримо про градієнтний спуск, нам важливо

84
00:05:48,700 --> 00:05:53,020
не лише підштовхнути кожен компонент угору чи вниз, нам важливо,

85
00:05:53,020 --> 00:05:54,020
які з них дають вам найбільшу віддачу від ваших грошей.

86
00:05:54,020 --> 00:06:00,260
Це, до речі, принаймні дещо нагадує теорію в нейронауці про

87
00:06:00,260 --> 00:06:04,900
те, як навчаються біологічні мережі нейронів, теорію Хебба, яку

88
00:06:04,900 --> 00:06:06,940
часто підсумовують фразою: нейрони, які запускаються разом, з’єднуються разом.

89
00:06:06,940 --> 00:06:12,460
Тут найбільше збільшення ваги, найбільше зміцнення зв’язків

90
00:06:12,460 --> 00:06:16,860
відбувається між нейронами, які є найбільш активними,

91
00:06:16,860 --> 00:06:18,100
і тими, які ми хочемо стати активнішими.

92
00:06:18,100 --> 00:06:22,520
У певному сенсі нейрони, які спрацьовують, коли бачать 2, стають

93
00:06:22,520 --> 00:06:25,440
сильніше пов’язаними з тими, хто спрацьовує, коли думає про це.

94
00:06:25,440 --> 00:06:29,240
Щоб було зрозуміло, я не в змозі робити твердження тим чи

95
00:06:29,240 --> 00:06:34,020
іншим чином щодо того, чи штучні мережі нейронів поводяться хоч як

96
00:06:34,020 --> 00:06:39,440
біологічний мозок, і ця ідея, що спрацьовує разом, супроводжується кількома значущими

97
00:06:39,440 --> 00:06:41,760
зірочками, але сприймається як дуже вільна Мені цікаво відзначити аналогію.

98
00:06:41,760 --> 00:06:46,760
У будь-якому разі, третій спосіб, яким ми можемо допомогти підвищити

99
00:06:46,760 --> 00:06:49,360
активацію цього нейрона, — змінити всі активації на попередньому шарі.

100
00:06:49,360 --> 00:06:55,080
А саме, якщо все, що пов’язано з цим нейроном цифри 2

101
00:06:55,080 --> 00:06:59,480
із позитивною вагою, стане яскравішим, а якщо все, що пов’язано з

102
00:06:59,480 --> 00:07:02,680
негативною вагою, стане тьмянішим, тоді цей нейрон цифри 2 стане активнішим.

103
00:07:02,680 --> 00:07:06,200
Подібно до змін ваги, ви отримаєте максимальну віддачу від

104
00:07:06,200 --> 00:07:10,840
своїх грошей, шукаючи змін, пропорційних розміру відповідних ваг.

105
00:07:10,840 --> 00:07:16,520
Тепер, звичайно, ми не можемо безпосередньо впливати на

106
00:07:16,520 --> 00:07:18,320
ці активації, ми лише контролюємо ваги та упередження.

107
00:07:18,320 --> 00:07:22,960
Але так само, як і з останнім

108
00:07:22,960 --> 00:07:23,960
шаром, корисно занотувати, які ці бажані зміни.

109
00:07:23,960 --> 00:07:29,040
Але майте на увазі, якщо зменшити масштаб на один крок тут,

110
00:07:29,040 --> 00:07:30,040
це лише те, що хоче вихідний нейрон з цифрою 2.

111
00:07:30,040 --> 00:07:34,960
Пам’ятайте, ми також хочемо, щоб усі інші нейрони в останньому шарі

112
00:07:34,960 --> 00:07:38,460
стали менш активними, і кожен із цих інших вихідних нейронів має

113
00:07:38,460 --> 00:07:43,200
власні думки щодо того, що має статися з передостаннім шаром.

114
00:07:43,200 --> 00:07:49,220
Отже, бажання цього нейрона з цифрою 2 додається разом із бажаннями

115
00:07:49,220 --> 00:07:54,800
всіх інших вихідних нейронів щодо того, що має статися з

116
00:07:54,800 --> 00:08:00,240
цим передостаннім шаром, знову ж таки пропорційно до відповідних ваг і

117
00:08:00,240 --> 00:08:01,740
пропорційно до того, скільки потрібно кожному з цих нейронів змінювати.

118
00:08:01,740 --> 00:08:05,940
Ось тут і виникає ідея розповсюдження назад.

119
00:08:05,940 --> 00:08:11,080
Додавши разом усі ці бажані ефекти, ви, по суті, отримаєте

120
00:08:11,080 --> 00:08:14,300
список підштовхувань, які ви хочете виконати на цьому передостанньому шарі.

121
00:08:14,300 --> 00:08:18,740
І як тільки ви їх отримаєте, ви можете рекурсивно застосувати той самий

122
00:08:18,740 --> 00:08:23,400
процес до відповідних ваг і зміщень, які визначають ці значення, повторюючи

123
00:08:23,400 --> 00:08:29,180
той самий процес, який я щойно пройшов, і рухаючись назад мережею.

124
00:08:29,180 --> 00:08:33,960
І якщо трохи зменшити масштаб, пам’ятайте, що це все лише те, як

125
00:08:33,960 --> 00:08:37,520
окремий навчальний приклад хоче підштовхнути до кожного з цих ваг і упереджень.

126
00:08:37,520 --> 00:08:41,400
Якби ми лише прислухалися до того, що хоче ця двоє, мережа

127
00:08:41,400 --> 00:08:44,140
зрештою була б стимулювана просто класифікувати всі зображення як двійку.

128
00:08:44,140 --> 00:08:49,500
Отже, що ви робите, це проходите ту саму процедуру підтримки для

129
00:08:49,500 --> 00:08:54,700
кожного іншого прикладу навчання, записуючи, як кожен із них хотів би

130
00:08:54,700 --> 00:09:02,300
змінити ваги та зміщення, і усереднюєте разом ці бажані зміни.

131
00:09:02,300 --> 00:09:08,260
Цей набір усереднених підштовхувань до кожної ваги та зміщення

132
00:09:08,260 --> 00:09:12,340
є, грубо кажучи, від’ємним градієнтом функції витрат, згаданим

133
00:09:12,340 --> 00:09:14,360
в останньому відео, або принаймні чимось пропорційним йому.

134
00:09:14,360 --> 00:09:18,980
Я кажу вільно, лише тому, що мені ще належить отримати точні кількісні дані

135
00:09:18,980 --> 00:09:23,480
про ці підштовхи, але якщо ви зрозуміли кожну зміну, про яку я

136
00:09:23,480 --> 00:09:28,740
щойно згадав, чому одні пропорційно більші за інші, і як їх усіх

137
00:09:28,740 --> 00:09:34,100
потрібно додавати разом, ви розумієте механізм для що насправді робить зворотне поширення.

138
00:09:34,100 --> 00:09:38,540
До речі, на практиці комп’ютерам потрібно надзвичайно багато часу, щоб

139
00:09:38,540 --> 00:09:43,120
підсумувати вплив кожного навчального прикладу кожного кроку градієнтного спуску.

140
00:09:43,120 --> 00:09:45,540
Отже, ось що зазвичай роблять замість цього.

141
00:09:45,540 --> 00:09:50,460
Ви випадковим чином перетасовуєте свої навчальні дані та розділяєте їх на цілу

142
00:09:50,460 --> 00:09:53,380
купу міні-пакетів, припустімо, що кожен із них має 100 прикладів навчання.

143
00:09:53,380 --> 00:09:56,980
Потім ви обчислюєте крок відповідно до міні-серії.

144
00:09:56,980 --> 00:10:00,840
Це не фактичний градієнт функції витрат, який залежить від усіх

145
00:10:00,840 --> 00:10:06,260
навчальних даних, а не ця крихітна підмножина, тому це не

146
00:10:06,260 --> 00:10:10,900
найефективніший крок униз, але кожна міні-серія дає вам досить гарне

147
00:10:10,900 --> 00:10:12,900
наближення, і, що важливіше, це дає вам значне прискорення обчислень.

148
00:10:12,900 --> 00:10:16,900
Якби ви побудували траєкторію вашої мережі під відповідною поверхнею витрат, це було б

149
00:10:16,900 --> 00:10:22,020
схоже на п’яного чоловіка, який безцільно спотикається вниз з пагорба, але робить швидкі

150
00:10:22,020 --> 00:10:26,880
кроки, а не на ретельно обчислену людину, яка визначає точний напрямок спуску для

151
00:10:26,880 --> 00:10:31,620
кожного кроку. перш ніж зробити дуже повільний і обережний крок у цьому напрямку.

152
00:10:31,620 --> 00:10:35,200
Ця техніка називається стохастичним градієнтним спуском.

153
00:10:35,200 --> 00:10:40,400
Тут багато чого відбувається, тож давайте просто підсумуємо це для себе, чи не так?

154
00:10:40,400 --> 00:10:45,480
Зворотне розповсюдження — це алгоритм для визначення того, як окремий навчальний приклад

155
00:10:45,480 --> 00:10:50,040
хотів би підштовхнути вагові коефіцієнти та зміщення, не лише з точки зору

156
00:10:50,040 --> 00:10:54,780
того, чи мають вони зростати чи зменшуватися, а й з точки зору

157
00:10:54,780 --> 00:10:56,240
того, які відносні пропорції цих змін викликають найшвидше зменшення до вартість.

158
00:10:56,240 --> 00:11:00,720
Справжній крок градієнтного спуску передбачав би виконання цього для всіх ваших десятків

159
00:11:00,720 --> 00:11:05,920
і тисяч навчальних прикладів і усереднення бажаних змін, які ви отримуєте,

160
00:11:05,920 --> 00:11:11,680
але це повільно з обчислювальної точки зору, тому замість цього ви

161
00:11:11,680 --> 00:11:14,000
випадково розбиваєте дані на міні-пакети та обчислюєте кожен крок відносно міні-партія.

162
00:11:14,000 --> 00:11:18,600
Неодноразово проходячи всі міні-пакети та вносячи ці коригування,

163
00:11:18,600 --> 00:11:23,420
ви досягнете локального мінімуму функції вартості, тобто ваша

164
00:11:23,420 --> 00:11:27,540
мережа зрештою справді добре впорається з навчальними прикладами.

165
00:11:27,540 --> 00:11:32,600
З огляду на все сказане, кожен рядок коду, який буде використовуватися для реалізації

166
00:11:32,600 --> 00:11:37,680
backprop, насправді відповідає тому, що ви зараз бачили, принаймні в неофіційних термінах.

167
00:11:37,680 --> 00:11:41,900
Але інколи знати, що робить математика, — це лише половина успіху, а просто

168
00:11:41,900 --> 00:11:44,780
представити прокляту річ — це те, де все стає заплутаним і заплутаним.

169
00:11:44,780 --> 00:11:49,360
Отже, для тих із вас, хто хоче заглибитися глибше, наступне відео розповідає про ті

170
00:11:49,360 --> 00:11:53,400
самі ідеї, які щойно були представлені тут, але з точки зору основного обчислення, яке,

171
00:11:53,400 --> 00:11:57,460
сподіваюся, має зробити його трохи більш знайомим, оскільки ви бачите тему в інші ресурси.

172
00:11:57,460 --> 00:12:01,220
Перед цим варто підкреслити одну річ: для того, щоб

173
00:12:01,220 --> 00:12:05,840
цей алгоритм працював, і це стосується всіх видів машинного

174
00:12:05,840 --> 00:12:06,840
навчання, окрім нейронних мереж, вам потрібно багато навчальних даних.

175
00:12:06,840 --> 00:12:10,740
У нашому випадку одна річ, яка робить рукописні цифри таким гарним прикладом, полягає в

176
00:12:10,740 --> 00:12:15,380
тому, що існує база даних MNIST з такою кількістю прикладів, які були позначені людьми.

177
00:12:15,380 --> 00:12:19,000
Тож поширена проблема, з якою ті з вас, хто працює у сфері машинного навчання, знайомі,

178
00:12:19,040 --> 00:12:22,880
— це просто отримати мічені навчальні дані, які вам дійсно потрібні, чи то люди, які

179
00:12:22,880 --> 00:12:27,400
позначають десятки тисяч зображень, чи будь-який інший тип даних, з яким ви можете мати справу.

