[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn. ",
  "translatedText": "Тут ми розглядаємо зворотне поширення, основний алгоритм навчання нейронних мереж. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. ",
  "translatedText": "Після короткого підсумку того, де ми зараз, перше, що я зроблю, це інтуїтивно зрозуміле керівництво для того, що насправді робить алгоритм, без будь-яких посилань на формули. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. ",
  "translatedText": "Тоді для тих із вас, хто хоче зануритися в математику, наступне відео розповідає про обчислення, що лежить в основі всього цього. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. ",
  "translatedText": "Якщо ви переглянули останні два відео або якщо ви просто переходите з відповідним фоном, ви знаєте, що таке нейронна мережа та як вона передає інформацію. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. ",
  "translatedText": "Тут ми робимо класичний приклад розпізнавання рукописних цифр, піксельні значення яких надходять на перший рівень мережі з 784 нейронами, і я показую мережу з двома прихованими шарами, які мають лише по 16 нейронів кожен, і вихідним шар з 10 нейронів, що вказує, яку цифру мережа обирає як відповідь. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. ",
  "translatedText": "Я також очікую, що ви зрозумієте градієнтний спуск, як описано в останньому відео, і те, як ми маємо на увазі під навчанням те, що ми хочемо знайти, які ваги та зміщення мінімізують певну функцію витрат. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. ",
  "translatedText": "Як швидке нагадування: для вартості одного навчального прикладу ви берете результат, який дає мережа, разом із результатом, який ви хотіли б отримати, і додаєте квадрати відмінностей між кожним компонентом. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. ",
  "translatedText": "Зробивши це для всіх ваших десятків тисяч навчальних прикладів і усереднивши результати, ви отримаєте загальну вартість мережі. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. ",
  "translatedText": "Наче цього недостатньо для роздумів, як описано в останньому відео, те, що ми шукаємо, це від’ємний градієнт цієї функції витрат, який говорить вам, як вам потрібно змінити всі ваги та зміщення, усі ці підключення, щоб найбільш ефективно знизити вартість. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. ",
  "translatedText": "Зворотне поширення, тема цього відео, — це алгоритм для обчислення цього божевільно складного градієнта. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "The one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. ",
  "translatedText": "Одна ідея з останнього відео, яку я справді хочу, щоб ви зараз міцно запам’ятали, полягає в тому, що оскільки уявлення про вектор градієнта як напрямок у 13 000 вимірах, м’якше кажучи, виходить за межі нашої уяви, існує інша як ви можете думати про це. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. ",
  "translatedText": "Величина кожного компонента тут говорить про те, наскільки функція витрат чутлива до кожної ваги та зміщення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. ",
  "translatedText": "Наприклад, скажімо, ви виконуєте процес, який я збираюся описати, і обчислюєте від’ємний градієнт, і компонент, пов’язаний із вагою на цьому краю, дорівнює 3.2, тоді як компонент, пов’язаний із цим краєм, тут має значення 0.1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. ",
  "translatedText": "Ви б це інтерпретували так: вартість функції в 32 рази чутливіша до змін у цій першій вазі, отже, якщо ви трішки зміните це значення, це призведе до певних змін у вартості, і ця зміна у 32 рази більше, ніж те саме ворушіння цієї другої ваги. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and index chasing of it all. ",
  "translatedText": "Особисто, коли я вперше дізнався про зворотне розповсюдження, я вважав, що найбільш заплутаним аспектом були лише нотація та погоня за індексом усього цього. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. ",
  "translatedText": "Але як тільки ви розгортаєте, що насправді робить кожна частина цього алгоритму, кожен окремий ефект, який він має, насправді досить інтуїтивно зрозумілий, просто є багато маленьких коригувань, які накладаються одне на одне. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. ",
  "translatedText": "Тож я почну тут із повного ігнорування нотації та просто покроково розповім про вплив кожного навчального прикладу на ваги та упередження. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. ",
  "translatedText": "Оскільки функція витрат передбачає усереднення певної вартості кожного прикладу за всіма десятками тисяч навчальних прикладів, спосіб коригування ваг і зміщень для одного кроку градієнтного спуску також залежить від кожного окремого прикладу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. ",
  "translatedText": "Точніше, в принципі так і повинно бути, але для ефективності обчислень ми пізніше зробимо невеликий трюк, щоб вам не потрібно було використовувати кожен окремий приклад для кожного кроку. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. ",
  "translatedText": "В інших випадках, прямо зараз, все, що ми збираємося зробити, це зосередити нашу увагу на одному прикладі, цьому зображенні 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted? ",
  "translatedText": "Який вплив повинен мати цей навчальний приклад на те, як коригуються ваги та зміщення? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. ",
  "translatedText": "Припустімо, що ми перебуваємо в точці, коли мережа ще недостатньо навчена, тому активації у виводі виглядатимуть досить випадковими, можливо, приблизно 0.5, 0.8, 0.2, далі і далі. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases, but it's helpful to keep track of which adjustments we wish should take place to that output layer. ",
  "translatedText": "Ми не можемо напряму змінити ці активації, ми можемо лише впливати на ваги та зміщення, але корисно відстежувати, які коригування, які ми хочемо, мають відбутися на цьому вихідному рівні. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 242.52,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. ",
  "translatedText": "І оскільки ми хочемо, щоб воно класифікувало зображення як 2, ми хочемо, щоб це третє значення було підштовхнуто вгору, а всі інші – вниз. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. ",
  "translatedText": "Крім того, розміри цих поштовхів мають бути пропорційними до того, наскільки далеко кожне поточне значення знаходиться від його цільового значення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. ",
  "translatedText": "Наприклад, збільшення активації нейрона № 2 у певному сенсі важливіше, ніж зменшення активності нейрона № 8, яке вже досить близько до того, де воно має бути. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. ",
  "translatedText": "Отже, збільшуючи масштаб, давайте зосередимося лише на цьому одному нейроні, тому, чию активацію ми хочемо збільшити. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. ",
  "translatedText": "Пам’ятайте, що активація визначається як певна зважена сума всіх активацій на попередньому рівні, плюс зміщення, яке потім підключається до чогось на зразок функції сигмоїдної сквишификации або ReLU. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation. ",
  "translatedText": "Отже, є три різні шляхи, які можна об’єднати разом, щоб збільшити цю активність. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. ",
  "translatedText": "Ви можете збільшити зміщення, ви можете збільшити ваги, і ви можете змінити активації з попереднього шару. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. ",
  "translatedText": "Зосереджуючись на тому, як слід регулювати ваги, зверніть увагу на те, що ваги насправді мають різні рівні впливу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. ",
  "translatedText": "Зв’язки з найяскравішими нейронами з попереднього шару мають найбільший ефект, оскільки ці ваги множаться на більші значення активації. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. ",
  "translatedText": "Отже, якщо ви збільшите одну з цих ваг, це справді матиме сильніший вплив на кінцеву функцію витрат, ніж збільшення ваг зв’язків із димерними нейронами, принаймні, що стосується цього навчального прикладу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck. ",
  "translatedText": "Пам’ятайте, що коли ми говоримо про градієнтний спуск, нам важливо не лише підштовхнути кожен компонент угору чи вниз, нам важливо, які з них дають вам найбільшу віддачу від ваших грошей. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together. ",
  "translatedText": "Це, до речі, принаймні дещо нагадує теорію в нейронауці про те, як навчаються біологічні мережі нейронів, теорію Хебба, яку часто підсумовують фразою: нейрони, які запускаються разом, з’єднуються разом. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active and the ones which we wish to become more active. ",
  "translatedText": "Тут найбільше збільшення ваги, найбільше зміцнення зв’язків відбувається між нейронами, які є найбільш активними, і тими, які ми хочемо стати активнішими. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those firing when thinking about it. ",
  "translatedText": "У певному сенсі нейрони, які спрацьовують, коли бачать 2, стають сильніше пов’язаними з тими, хто спрацьовує, коли думає про це. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note. ",
  "translatedText": "Щоб було зрозуміло, я не в змозі робити твердження тим чи іншим чином щодо того, чи штучні мережі нейронів поводяться хоч як біологічний мозок, і ця ідея, що спрацьовує разом, супроводжується кількома значущими зірочками, але сприймається як дуже вільна Мені цікаво відзначити аналогію. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer. ",
  "translatedText": "У будь-якому разі, третій спосіб, яким ми можемо допомогти підвищити активацію цього нейрона, — змінити всі активації на попередньому шарі. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active. ",
  "translatedText": "А саме, якщо все, що пов’язано з цим нейроном цифри 2 із позитивною вагою, стане яскравішим, а якщо все, що пов’язано з негативною вагою, стане тьмянішим, тоді цей нейрон цифри 2 стане активнішим. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. ",
  "translatedText": "Подібно до змін ваги, ви отримаєте максимальну віддачу від своїх грошей, шукаючи змін, пропорційних розміру відповідних ваг. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases. ",
  "translatedText": "Тепер, звичайно, ми не можемо безпосередньо впливати на ці активації, ми лише контролюємо ваги та упередження. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are. ",
  "translatedText": "Але так само, як і з останнім шаром, корисно занотувати, які ці бажані зміни. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants. ",
  "translatedText": "Але майте на увазі, якщо зменшити масштаб на один крок тут, це лише те, що хоче вихідний нейрон з цифрою 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. ",
  "translatedText": "Пам’ятайте, ми також хочемо, щоб усі інші нейрони в останньому шарі стали менш активними, і кожен із цих інших вихідних нейронів має власні думки щодо того, що має статися з передостаннім шаром. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. ",
  "translatedText": "Отже, бажання цього нейрона з цифрою 2 додається разом із бажаннями всіх інших вихідних нейронів щодо того, що має статися з цим передостаннім шаром, знову ж таки пропорційно до відповідних ваг і пропорційно до того, скільки потрібно кожному з цих нейронів змінювати. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in. ",
  "translatedText": "Ось тут і виникає ідея розповсюдження назад. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. ",
  "translatedText": "Додавши разом усі ці бажані ефекти, ви, по суті, отримаєте список підштовхувань, які ви хочете виконати на цьому передостанньому шарі. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. ",
  "translatedText": "І як тільки ви їх отримаєте, ви можете рекурсивно застосувати той самий процес до відповідних ваг і зміщень, які визначають ці значення, повторюючи той самий процес, який я щойно пройшов, і рухаючись назад мережею. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. ",
  "translatedText": "І якщо трохи зменшити масштаб, пам’ятайте, що це все лише те, як окремий навчальний приклад хоче підштовхнути до кожного з цих ваг і упереджень. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2. ",
  "translatedText": "Якби ми лише прислухалися до того, що хоче ця двоє, мережа зрештою була б стимулювана просто класифікувати всі зображення як двійку. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes. ",
  "translatedText": "Отже, що ви робите, це проходите ту саму процедуру підтримки для кожного іншого прикладу навчання, записуючи, як кожен із них хотів би змінити ваги та зміщення, і усереднюєте разом ці бажані зміни. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. ",
  "translatedText": "Цей набір усереднених підштовхувань до кожної ваги та зміщення є, грубо кажучи, від’ємним градієнтом функції витрат, згаданим в останньому відео, або принаймні чимось пропорційним йому. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. ",
  "translatedText": "Я кажу вільно, лише тому, що мені ще належить отримати точні кількісні дані про ці підштовхи, але якщо ви зрозуміли кожну зміну, про яку я щойно згадав, чому одні пропорційно більші за інші, і як їх усіх потрібно додавати разом, ви розумієте механізм для що насправді робить зворотне поширення. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. ",
  "translatedText": "До речі, на практиці комп’ютерам потрібно надзвичайно багато часу, щоб підсумувати вплив кожного навчального прикладу кожного кроку градієнтного спуску. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead. ",
  "translatedText": "Отже, ось що зазвичай роблять замість цього. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and divide it into a whole bunch of mini-batches, let's say each one having 100 training examples. ",
  "translatedText": "Ви випадковим чином перетасовуєте свої навчальні дані та розділяєте їх на цілу купу міні-пакетів, припустімо, що кожен із них має 100 прикладів навчання. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch. ",
  "translatedText": "Потім ви обчислюєте крок відповідно до міні-серії. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly it gives you a significant computational speedup. ",
  "translatedText": "Це не фактичний градієнт функції витрат, який залежить від усіх навчальних даних, а не ця крихітна підмножина, тому це не найефективніший крок униз, але кожна міні-серія дає вам досить гарне наближення, і, що важливіше, це дає вам значне прискорення обчислень. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. ",
  "translatedText": "Якби ви побудували траєкторію вашої мережі під відповідною поверхнею витрат, це було б схоже на п’яного чоловіка, який безцільно спотикається вниз з пагорба, але робить швидкі кроки, а не на ретельно обчислену людину, яка визначає точний напрямок спуску для кожного кроку. перш ніж зробити дуже повільний і обережний крок у цьому напрямку. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent. ",
  "translatedText": "Ця техніка називається стохастичним градієнтним спуском. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we? ",
  "translatedText": "Тут багато чого відбувається, тож давайте просто підсумуємо це для себе, чи не так? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. ",
  "translatedText": "Зворотне розповсюдження — це алгоритм для визначення того, як окремий навчальний приклад хотів би підштовхнути вагові коефіцієнти та зміщення, не лише з точки зору того, чи мають вони зростати чи зменшуватися, а й з точки зору того, які відносні пропорції цих змін викликають найшвидше зменшення до вартість. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes you get, but that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch. ",
  "translatedText": "Справжній крок градієнтного спуску передбачав би виконання цього для всіх ваших десятків і тисяч навчальних прикладів і усереднення бажаних змін, які ви отримуєте, але це повільно з обчислювальної точки зору, тому замість цього ви випадково розбиваєте дані на міні-пакети та обчислюєте кожен крок відносно міні-партія. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.26,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples. ",
  "translatedText": "Неодноразово проходячи всі міні-пакети та вносячи ці коригування, ви досягнете локального мінімуму функції вартості, тобто ваша мережа зрештою справді добре впорається з навчальними прикладами. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms. ",
  "translatedText": "З огляду на все сказане, кожен рядок коду, який буде використовуватися для реалізації backprop, насправді відповідає тому, що ви зараз бачили, принаймні в неофіційних термінах. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. ",
  "translatedText": "Але інколи знати, що робить математика, — це лише половина успіху, а просто представити прокляту річ — це те, де все стає заплутаним і заплутаним. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So, for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. ",
  "translatedText": "Отже, для тих із вас, хто хоче заглибитися глибше, наступне відео розповідає про ті самі ідеї, які щойно були представлені тут, але з точки зору основного обчислення, яке, сподіваюся, має зробити його трохи більш знайомим, оскільки ви бачите тему в інші ресурси. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. ",
  "translatedText": "Перед цим варто підкреслити одну річ: для того, щоб цей алгоритм працював, і це стосується всіх видів машинного навчання, окрім нейронних мереж, вам потрібно багато навчальних даних. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans. ",
  "translatedText": "У нашому випадку одна річ, яка робить рукописні цифри таким гарним прикладом, полягає в тому, що існує база даних MNIST з такою кількістю прикладів, які були позначені людьми. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with. ",
  "translatedText": "Тож поширена проблема, з якою ті з вас, хто працює у сфері машинного навчання, знайомі, — це просто отримати мічені навчальні дані, які вам дійсно потрібні, чи то люди, які позначають десятки тисяч зображень, чи будь-який інший тип даних, з яким ви можете мати справу. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]