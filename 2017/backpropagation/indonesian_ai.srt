1
00:00:00,000 --> 00:00:09,640
Di sini, kami menangani propagasi mundur, algoritma inti di balik cara jaringan saraf belajar.

2
00:00:09,640 --> 00:00:13,320
Setelah rekap singkat mengenai keadaan kita saat ini, hal pertama yang akan saya lakukan adalah

3
00:00:13,320 --> 00:00:17,400
penelusuran intuitif tentang apa yang sebenarnya dilakukan algoritme, tanpa referensi apa pun ke rumusnya.

4
00:00:17,400 --> 00:00:21,400
Lalu, bagi Anda yang memang ingin mendalami matematika, video

5
00:00:21,400 --> 00:00:24,040
selanjutnya akan membahas tentang kalkulus yang mendasari semua itu.

6
00:00:24,040 --> 00:00:27,320
Jika Anda menonton dua video terakhir, atau jika Anda hanya melihat latar belakang yang

7
00:00:27,320 --> 00:00:31,080
sesuai, Anda pasti tahu apa itu jaringan saraf, dan bagaimana jaringan tersebut meneruskan informasi.

8
00:00:31,080 --> 00:00:35,520
Di sini, kita melakukan contoh klasik dalam mengenali angka tulisan tangan yang nilai

9
00:00:35,520 --> 00:00:40,280
pikselnya dimasukkan ke dalam lapisan pertama jaringan dengan 784 neuron, dan saya telah

10
00:00:40,280 --> 00:00:44,720
menunjukkan jaringan dengan dua lapisan tersembunyi yang masing-masing hanya memiliki 16 neuron, dan

11
00:00:44,720 --> 00:00:49,520
sebuah keluaran. lapisan 10 neuron, menunjukkan digit mana yang dipilih jaringan sebagai jawabannya.

12
00:00:49,520 --> 00:00:54,480
Saya juga mengharapkan Anda memahami penurunan gradien, seperti yang dijelaskan dalam

13
00:00:54,480 --> 00:01:00,160
video terakhir, dan apa yang kami maksud dengan pembelajaran adalah kami

14
00:01:00,160 --> 00:01:02,080
ingin menemukan bobot dan bias mana yang meminimalkan fungsi biaya tertentu.

15
00:01:02,080 --> 00:01:07,560
Sebagai pengingat singkat, untuk biaya satu contoh pelatihan, Anda mengambil

16
00:01:07,560 --> 00:01:12,920
keluaran yang diberikan jaringan, bersama dengan keluaran yang ingin

17
00:01:12,920 --> 00:01:15,560
Anda berikan, dan menjumlahkan kuadrat selisih antara masing-masing komponen.

18
00:01:15,560 --> 00:01:20,160
Melakukan hal ini untuk puluhan ribu contoh pelatihan Anda dan

19
00:01:20,160 --> 00:01:23,040
merata-ratakan hasilnya, ini akan memberi Anda total biaya jaringan.

20
00:01:23,040 --> 00:01:26,320
Seolah-olah itu belum cukup untuk dipikirkan, seperti yang dijelaskan dalam video terakhir,

21
00:01:26,320 --> 00:01:31,700
hal yang kita cari adalah gradien negatif dari fungsi biaya ini,

22
00:01:31,700 --> 00:01:36,000
yang memberi tahu Anda bagaimana Anda perlu mengubah semua bobot dan

23
00:01:36,000 --> 00:01:43,080
bias, semuanya. koneksi ini, sehingga dapat mengurangi biaya secara paling efisien.

24
00:01:43,080 --> 00:01:48,600
Propagasi mundur, topik video ini, adalah algoritma

25
00:01:48,600 --> 00:01:49,600
untuk menghitung gradien yang sangat rumit.

26
00:01:49,600 --> 00:01:53,300
Satu gagasan dari video terakhir yang saya benar-benar ingin Anda

27
00:01:53,300 --> 00:01:58,280
ingat saat ini adalah karena memikirkan vektor gradien sebagai

28
00:01:58,280 --> 00:02:02,660
arah dalam 13.000 dimensi, secara sederhana, di luar jangkauan

29
00:02:02,660 --> 00:02:04,620
imajinasi kita, ada gagasan lain. cara Anda dapat memikirkannya.

30
00:02:04,620 --> 00:02:09,700
Besaran setiap komponen di sini menunjukkan seberapa sensitif

31
00:02:09,700 --> 00:02:11,820
fungsi biaya terhadap setiap bobot dan bias.

32
00:02:11,820 --> 00:02:15,180
Misalnya, Anda menjalani proses yang akan saya jelaskan, dan menghitung gradien

33
00:02:15,180 --> 00:02:19,800
negatif, dan komponen yang terkait dengan bobot pada tepi ini

34
00:02:19,800 --> 00:02:26,940
adalah 3. 2, sedangkan komponen yang terkait dengan tepi ini di sini keluar sebagai 0. 1.

35
00:02:26,940 --> 00:02:31,520
Cara Anda menafsirkannya adalah bahwa biaya fungsi tersebut 32 kali lebih sensitif terhadap

36
00:02:31,520 --> 00:02:36,100
perubahan bobot pertama, jadi jika Anda menggoyangkan nilai tersebut sedikit, hal ini

37
00:02:36,100 --> 00:02:40,780
akan menyebabkan beberapa perubahan pada biaya, dan perubahan itu adalah 32 kali lebih

38
00:02:40,780 --> 00:02:45,580
besar dari apa yang dihasilkan oleh goyangan yang sama pada beban kedua.

39
00:02:45,580 --> 00:02:52,500
Secara pribadi, ketika saya pertama kali belajar tentang backpropagation, menurut saya

40
00:02:52,500 --> 00:02:55,820
aspek yang paling membingungkan hanyalah notasi dan pengejaran indeks dari semuanya.

41
00:02:55,820 --> 00:03:00,240
Namun begitu Anda mengungkap apa yang sebenarnya dilakukan setiap bagian dari

42
00:03:00,240 --> 00:03:04,540
algoritme ini, setiap efek yang dimilikinya sebenarnya cukup intuitif, hanya

43
00:03:04,540 --> 00:03:07,740
saja ada banyak penyesuaian kecil yang ditumpangkan satu sama lain.

44
00:03:07,740 --> 00:03:11,380
Jadi saya akan memulai semuanya di sini dengan mengabaikan notasi, dan

45
00:03:11,380 --> 00:03:17,380
hanya menelusuri efek setiap contoh pelatihan terhadap bobot dan bias.

46
00:03:17,380 --> 00:03:21,880
Karena fungsi biaya melibatkan rata-rata biaya tertentu per contoh pada

47
00:03:21,880 --> 00:03:26,980
puluhan ribu contoh pelatihan, cara kita menyesuaikan bobot dan bias

48
00:03:26,980 --> 00:03:31,740
untuk satu langkah penurunan gradien juga bergantung pada setiap contoh.

49
00:03:31,740 --> 00:03:35,300
Atau lebih tepatnya, pada prinsipnya memang seharusnya demikian, namun untuk efisiensi komputasi, kami akan

50
00:03:35,300 --> 00:03:39,860
melakukan sedikit trik nanti agar Anda tidak perlu melakukan setiap contoh untuk setiap langkah.

51
00:03:39,860 --> 00:03:44,460
Dalam kasus lain, saat ini, yang akan kita lakukan hanyalah

52
00:03:44,460 --> 00:03:46,780
memusatkan perhatian kita pada satu contoh, gambar angka 2 ini.

53
00:03:46,780 --> 00:03:51,740
Apa pengaruh contoh pelatihan ini terhadap penyesuaian bobot dan bias?

54
00:03:51,740 --> 00:03:56,040
Katakanlah kita berada pada titik di mana jaringan belum terlatih dengan baik, sehingga

55
00:03:56,040 --> 00:04:01,620
aktivasi pada output akan terlihat acak, mungkin sekitar 0. 5, 0. 8, 0. 2,

56
00:04:01,620 --> 00:04:02,780
terus dan terus.

57
00:04:02,780 --> 00:04:06,700
Kita tidak dapat secara langsung mengubah aktivasi tersebut, kita hanya mempunyai pengaruh

58
00:04:06,700 --> 00:04:11,380
pada bobot dan bias, namun akan sangat membantu jika kita melacak

59
00:04:11,380 --> 00:04:13,340
penyesuaian mana yang kita inginkan untuk dilakukan pada lapisan keluaran tersebut.

60
00:04:13,340 --> 00:04:18,220
Dan karena kita ingin mengklasifikasikan gambar sebagai 2, kita

61
00:04:18,220 --> 00:04:21,700
ingin nilai ketiga tersebut dinaikkan sementara nilai lainnya diturunkan.

62
00:04:21,700 --> 00:04:27,620
Selain itu, ukuran dorongan ini harus sebanding dengan seberapa

63
00:04:27,620 --> 00:04:30,220
jauh jarak setiap nilai saat ini dari nilai targetnya.

64
00:04:30,220 --> 00:04:35,260
Misalnya, peningkatan aktivasi neuron nomor 2 dalam arti tertentu

65
00:04:35,260 --> 00:04:39,620
lebih penting daripada penurunan aktivasi neuron nomor 8,

66
00:04:39,620 --> 00:04:42,060
yang sudah cukup dekat dengan tempat yang seharusnya.

67
00:04:42,060 --> 00:04:46,260
Jadi jika kita perbesar lebih jauh, mari kita fokus

68
00:04:46,260 --> 00:04:47,900
pada satu neuron saja, yang aktivasinya ingin kita tingkatkan.

69
00:04:47,900 --> 00:04:53,680
Ingat, aktivasi tersebut didefinisikan sebagai jumlah tertimbang tertentu dari semua

70
00:04:53,680 --> 00:04:58,380
aktivasi di lapisan sebelumnya, ditambah bias, yang semuanya kemudian dimasukkan

71
00:04:58,380 --> 00:05:01,900
ke dalam sesuatu seperti fungsi squishification sigmoid, atau ReLU.

72
00:05:01,900 --> 00:05:07,060
Jadi ada tiga cara berbeda yang dapat

73
00:05:07,060 --> 00:05:08,060
bekerja sama untuk membantu meningkatkan aktivasi tersebut.

74
00:05:08,060 --> 00:05:12,800
Anda dapat meningkatkan bias, Anda dapat menambah bobot,

75
00:05:12,800 --> 00:05:15,300
dan Anda dapat mengubah aktivasi dari lapisan sebelumnya.

76
00:05:15,300 --> 00:05:19,720
Berfokus pada bagaimana bobot harus disesuaikan, perhatikan bagaimana

77
00:05:19,720 --> 00:05:21,460
bobot sebenarnya memiliki tingkat pengaruh yang berbeda.

78
00:05:21,460 --> 00:05:25,100
Koneksi dengan neuron paling terang dari lapisan sebelumnya memiliki pengaruh terbesar

79
00:05:25,100 --> 00:05:31,420
karena bobot tersebut dikalikan dengan nilai aktivasi yang lebih besar.

80
00:05:31,420 --> 00:05:35,820
Jadi jika Anda meningkatkan salah satu bobot tersebut, hal ini sebenarnya memiliki

81
00:05:35,820 --> 00:05:40,900
pengaruh yang lebih kuat pada fungsi biaya akhir dibandingkan meningkatkan bobot koneksi

82
00:05:40,900 --> 00:05:44,020
dengan neuron peredup, setidaknya sejauh menyangkut contoh pelatihan yang satu ini.

83
00:05:44,020 --> 00:05:48,700
Ingat, ketika kita berbicara tentang penurunan gradien, kita tidak hanya

84
00:05:48,700 --> 00:05:53,020
peduli apakah setiap komponen harus dinaikkan atau diturunkan, kita

85
00:05:53,020 --> 00:05:54,020
juga peduli tentang komponen mana yang memberikan hasil maksimal.

86
00:05:54,020 --> 00:06:00,260
Omong-omong, hal ini setidaknya mengingatkan kita pada teori dalam ilmu

87
00:06:00,260 --> 00:06:04,900
saraf tentang bagaimana jaringan biologis neuron belajar, teori Hebbian, yang

88
00:06:04,900 --> 00:06:06,940
sering diringkas dalam frasa, neuron yang menyala bersama-sama saling terhubung.

89
00:06:06,940 --> 00:06:12,460
Di sini, peningkatan bobot terbesar, penguatan koneksi terbesar,

90
00:06:12,460 --> 00:06:16,860
terjadi antara neuron yang paling aktif dan

91
00:06:16,860 --> 00:06:18,100
neuron yang ingin kita jadikan lebih aktif.

92
00:06:18,100 --> 00:06:22,520
Dalam arti tertentu, neuron yang terpicu saat melihat angka 2

93
00:06:22,520 --> 00:06:25,440
menjadi lebih terkait erat dengan neuron yang terpicu saat memikirkannya.

94
00:06:25,440 --> 00:06:29,240
Untuk lebih jelasnya, saya tidak dalam posisi untuk membuat pernyataan dengan satu

95
00:06:29,240 --> 00:06:34,020
atau lain cara tentang apakah jaringan neuron buatan berperilaku seperti otak

96
00:06:34,020 --> 00:06:39,440
biologis, dan gagasan yang menyatu ini disertai dengan beberapa tanda bintang yang

97
00:06:39,440 --> 00:06:41,760
bermakna, tetapi dianggap sangat longgar. analoginya, menurut saya menarik untuk disimak.

98
00:06:41,760 --> 00:06:46,760
Bagaimanapun, cara ketiga untuk membantu meningkatkan aktivasi neuron ini

99
00:06:46,760 --> 00:06:49,360
adalah dengan mengubah semua aktivasi di lapisan sebelumnya.

100
00:06:49,360 --> 00:06:55,080
Yaitu, jika semua yang terhubung ke neuron digit 2 yang berbobot positif

101
00:06:55,080 --> 00:06:59,480
menjadi lebih terang, dan jika semua yang terhubung dengan bobot negatif menjadi

102
00:06:59,480 --> 00:07:02,680
lebih redup, maka neuron digit 2 tersebut akan menjadi lebih aktif.

103
00:07:02,680 --> 00:07:06,200
Mirip dengan perubahan berat badan, Anda akan mendapatkan hasil maksimal

104
00:07:06,200 --> 00:07:10,840
dengan mencari perubahan yang sebanding dengan ukuran beban yang sesuai.

105
00:07:10,840 --> 00:07:16,520
Tentu saja, kami tidak dapat secara langsung mempengaruhi aktivasi

106
00:07:16,520 --> 00:07:18,320
tersebut, kami hanya memiliki kendali atas bobot dan bias.

107
00:07:18,320 --> 00:07:22,960
Namun sama seperti lapisan terakhir, ada baiknya

108
00:07:22,960 --> 00:07:23,960
untuk mencatat perubahan apa saja yang diinginkan.

109
00:07:23,960 --> 00:07:29,040
Namun perlu diingat, memperkecil satu langkah di sini, ini

110
00:07:29,040 --> 00:07:30,040
hanya yang diinginkan oleh neuron keluaran digit 2 itu.

111
00:07:30,040 --> 00:07:34,960
Ingat, kita juga ingin semua neuron lain di lapisan terakhir menjadi

112
00:07:34,960 --> 00:07:38,460
kurang aktif, dan masing-masing neuron keluaran lainnya memiliki pemikirannya sendiri tentang

113
00:07:38,460 --> 00:07:43,200
apa yang harus terjadi pada lapisan kedua hingga terakhir tersebut.

114
00:07:43,200 --> 00:07:49,220
Jadi keinginan neuron digit 2 ini dijumlahkan dengan keinginan semua neuron

115
00:07:49,220 --> 00:07:54,800
keluaran lainnya untuk apa yang seharusnya terjadi pada lapisan kedua

116
00:07:54,800 --> 00:08:00,240
hingga terakhir ini, sekali lagi sebanding dengan bobot yang sesuai, dan

117
00:08:00,240 --> 00:08:01,740
sebanding dengan seberapa banyak kebutuhan masing-masing neuron tersebut. Untuk mengganti.

118
00:08:01,740 --> 00:08:05,940
Di sinilah muncul ide untuk melakukan propaganda ke belakang.

119
00:08:05,940 --> 00:08:11,080
Dengan menambahkan semua efek yang diinginkan ini, pada dasarnya Anda mendapatkan daftar

120
00:08:11,080 --> 00:08:14,300
dorongan yang Anda inginkan terjadi pada lapisan kedua hingga terakhir ini.

121
00:08:14,300 --> 00:08:18,740
Dan setelah Anda memilikinya, Anda dapat menerapkan proses yang sama secara rekursif

122
00:08:18,740 --> 00:08:23,400
pada bobot dan bias relevan yang menentukan nilai tersebut, mengulangi proses yang

123
00:08:23,400 --> 00:08:29,180
sama yang baru saja saya lalui dan bergerak mundur melalui jaringan.

124
00:08:29,180 --> 00:08:33,960
Dan jika diperbesar sedikit lagi, ingatlah bahwa ini adalah bagaimana

125
00:08:33,960 --> 00:08:37,520
sebuah contoh pelatihan ingin mendorong setiap bobot dan bias tersebut.

126
00:08:37,520 --> 00:08:41,400
Jika kita hanya mendengarkan apa yang diinginkan oleh 2, jaringan pada

127
00:08:41,400 --> 00:08:44,140
akhirnya akan diberi insentif hanya untuk mengklasifikasikan semua gambar sebagai 2.

128
00:08:44,140 --> 00:08:49,500
Jadi yang Anda lakukan adalah melakukan rutinitas backprop yang sama

129
00:08:49,500 --> 00:08:54,700
untuk setiap contoh pelatihan lainnya, mencatat bagaimana masing-masing contoh ingin

130
00:08:54,700 --> 00:09:02,300
mengubah bobot dan bias, dan membuat rata-rata perubahan yang diinginkan.

131
00:09:02,300 --> 00:09:08,260
Kumpulan dorongan rata-rata untuk setiap bobot dan bias ini, secara

132
00:09:08,260 --> 00:09:12,340
sederhana, adalah gradien negatif dari fungsi biaya yang dirujuk

133
00:09:12,340 --> 00:09:14,360
dalam video terakhir, atau setidaknya sesuatu yang sebanding dengannya.

134
00:09:14,360 --> 00:09:18,980
Saya mengatakannya dengan santai hanya karena saya belum mengetahui secara tepat secara kuantitatif

135
00:09:18,980 --> 00:09:23,480
mengenai dorongan-dorongan tersebut, namun jika Anda memahami setiap perubahan yang baru saja saya

136
00:09:23,480 --> 00:09:28,740
referensikan, mengapa beberapa perubahan secara proporsional lebih besar daripada yang lain, dan bagaimana

137
00:09:28,740 --> 00:09:34,100
semuanya perlu dijumlahkan, Anda memahami mekanisme untuk apa yang sebenarnya dilakukan propagasi mundur.

138
00:09:34,100 --> 00:09:38,540
Ngomong-ngomong, dalam praktiknya, komputer membutuhkan waktu yang sangat lama untuk

139
00:09:38,540 --> 00:09:43,120
menjumlahkan pengaruh setiap contoh pelatihan pada setiap langkah penurunan gradien.

140
00:09:43,120 --> 00:09:45,540
Jadi inilah yang biasa dilakukan.

141
00:09:45,540 --> 00:09:50,460
Anda mengacak data pelatihan secara acak dan membaginya menjadi beberapa

142
00:09:50,460 --> 00:09:53,380
kelompok kecil, katakanlah masing-masing kelompok kecil memiliki 100 contoh pelatihan.

143
00:09:53,380 --> 00:09:56,980
Kemudian Anda menghitung langkah sesuai dengan mini-batch.

144
00:09:56,980 --> 00:10:00,840
Ini bukan gradien sebenarnya dari fungsi biaya, yang bergantung pada semua data

145
00:10:00,840 --> 00:10:06,260
pelatihan, bukan subset kecil ini, jadi ini bukan langkah menurun yang

146
00:10:06,260 --> 00:10:10,900
paling efisien, tetapi setiap mini-batch memberi Anda perkiraan yang cukup bagus,

147
00:10:10,900 --> 00:10:12,900
dan yang lebih penting itu memberi Anda kecepatan komputasi yang signifikan.

148
00:10:12,900 --> 00:10:16,900
Jika Anda merencanakan lintasan jaringan Anda di bawah permukaan biaya yang relevan, hal tersebut akan

149
00:10:16,900 --> 00:10:22,020
lebih seperti seorang pria mabuk yang tersandung tanpa tujuan menuruni bukit namun mengambil langkah

150
00:10:22,020 --> 00:10:26,880
cepat, dibandingkan dengan seorang pria yang menghitung dengan cermat yang menentukan arah penurunan yang tepat

151
00:10:26,880 --> 00:10:31,620
dari setiap langkah. sebelum mengambil langkah yang sangat lambat dan hati-hati ke arah itu.

152
00:10:31,620 --> 00:10:35,200
Teknik ini disebut sebagai penurunan gradien stokastik.

153
00:10:35,200 --> 00:10:40,400
Ada banyak hal yang terjadi di sini, jadi mari kita simpulkan sendiri, oke?

154
00:10:40,400 --> 00:10:45,480
Propagasi mundur adalah algoritma untuk menentukan bagaimana sebuah contoh pelatihan ingin mendorong

155
00:10:45,480 --> 00:10:50,040
bobot dan bias, tidak hanya dalam hal apakah bobot dan bias tersebut

156
00:10:50,040 --> 00:10:54,780
harus naik atau turun, namun juga dalam hal proporsi relatif terhadap perubahan

157
00:10:54,780 --> 00:10:56,240
tersebut yang menyebabkan penurunan paling cepat pada bobot dan bias. biaya.

158
00:10:56,240 --> 00:11:00,720
Langkah penurunan gradien yang sebenarnya akan melibatkan melakukan hal ini untuk semua puluhan

159
00:11:00,720 --> 00:11:05,920
dan ribuan contoh pelatihan Anda dan merata-ratakan perubahan yang diinginkan yang Anda

160
00:11:05,920 --> 00:11:11,680
dapatkan, tapi itu lambat secara komputasi, jadi Anda membagi data secara acak

161
00:11:11,680 --> 00:11:14,000
menjadi kumpulan kecil dan menghitung setiap langkah sehubungan dengan a kumpulan mini.

162
00:11:14,000 --> 00:11:18,600
Dengan berulang kali memeriksa semua mini-batch dan melakukan penyesuaian ini, Anda

163
00:11:18,600 --> 00:11:23,420
akan menyatu menuju fungsi biaya minimum lokal, yang berarti jaringan Anda

164
00:11:23,420 --> 00:11:27,540
pada akhirnya akan melakukan pekerjaan yang sangat baik pada contoh pelatihan.

165
00:11:27,540 --> 00:11:32,600
Jadi dengan semua hal tersebut, setiap baris kode yang digunakan untuk mengimplementasikan

166
00:11:32,600 --> 00:11:37,680
backprop sebenarnya sesuai dengan sesuatu yang telah Anda lihat, setidaknya secara informal.

167
00:11:37,680 --> 00:11:41,900
Namun terkadang, mengetahui fungsi matematika hanyalah setengah dari perjuangan, dan hanya

168
00:11:41,900 --> 00:11:44,780
mewakili matematika saja sudah membuat semuanya menjadi kacau dan membingungkan.

169
00:11:44,780 --> 00:11:49,360
Jadi, bagi Anda yang ingin mendalami lebih dalam, video berikutnya akan membahas ide-ide yang

170
00:11:49,360 --> 00:11:53,400
sama yang baru saja disajikan di sini, namun dalam kaitannya dengan kalkulus yang mendasarinya,

171
00:11:53,400 --> 00:11:57,460
yang semoga akan membuatnya lebih familiar saat Anda melihat topiknya di sumber daya lainnya.

172
00:11:57,460 --> 00:12:01,220
Sebelum itu, satu hal yang perlu ditekankan adalah agar algoritme

173
00:12:01,220 --> 00:12:05,840
ini berfungsi, dan ini berlaku untuk semua jenis pembelajaran

174
00:12:05,840 --> 00:12:06,840
mesin selain jaringan saraf, Anda memerlukan banyak data pelatihan.

175
00:12:06,840 --> 00:12:10,740
Dalam kasus kami, satu hal yang membuat angka tulisan tangan menjadi contoh yang bagus

176
00:12:10,740 --> 00:12:15,380
adalah adanya database MNIST, dengan begitu banyak contoh yang telah diberi label oleh manusia.

177
00:12:15,380 --> 00:12:19,000
Jadi, tantangan umum yang biasa dihadapi oleh Anda yang bekerja di bidang pembelajaran mesin

178
00:12:19,040 --> 00:12:22,880
hanyalah mendapatkan data pelatihan berlabel yang benar-benar Anda perlukan, apakah itu meminta orang memberi

179
00:12:22,880 --> 00:12:27,400
label pada puluhan ribu gambar, atau jenis data apa pun yang mungkin Anda hadapi.

