[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn. ",
  "translatedText": "هنا، نتناول الانتشار العكسي، وهو الخوارزمية الأساسية وراء كيفية تعلم الشبكات العصبية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. ",
  "translatedText": "بعد تلخيص سريع لما نحن فيه، أول شيء سأفعله هو إجراء إرشادات بديهية لما تفعله الخوارزمية فعليًا، دون أي إشارة إلى الصيغ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. ",
  "translatedText": "ثم، بالنسبة لأولئك منكم الذين يريدون التعمق في الرياضيات، فإن الفيديو التالي يتناول حساب التفاضل والتكامل الأساسي لكل هذا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. ",
  "translatedText": "إذا شاهدت مقطعي الفيديو الأخيرين، أو إذا كنت تقفز للتو بالخلفية المناسبة، فأنت تعرف ما هي الشبكة العصبية، وكيف تغذي المعلومات إلى الأمام. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. ",
  "translatedText": "هنا، نحن ننفذ المثال الكلاسيكي للتعرف على الأرقام المكتوبة بخط اليد والتي يتم إدخال قيم البكسل الخاصة بها في الطبقة الأولى من الشبكة التي تحتوي على 784 خلية عصبية، وقد قمت بعرض شبكة ذات طبقتين مخفيتين تحتوي كل منهما على 16 خلية عصبية فقط، ومخرج طبقة من 10 خلايا عصبية، تشير إلى الرقم الذي تختاره الشبكة كإجابة لها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. ",
  "translatedText": "أتوقع منك أيضًا أن تفهم النسب المتدرج، كما هو موضح في الفيديو الأخير، وكيف أن ما نعنيه بالتعلم هو أننا نريد العثور على الأوزان والتحيزات التي تقلل من دالة تكلفة معينة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. ",
  "translatedText": "كتذكير سريع، مقابل تكلفة مثال تدريبي واحد، فإنك تأخذ المخرجات التي تقدمها الشبكة، بالإضافة إلى المخرجات التي تريدها أن تعطيها، وتضيف مربعات الاختلافات بين كل مكون. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. ",
  "translatedText": "عند القيام بذلك لجميع عشرات الآلاف من أمثلة التدريب الخاصة بك وحساب متوسط النتائج، فإن هذا يمنحك التكلفة الإجمالية للشبكة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. ",
  "translatedText": "كما لو أن هذا لا يكفي للتفكير فيه، كما هو موضح في الفيديو الأخير، فإن الشيء الذي نبحث عنه هو التدرج السلبي لدالة التكلفة هذه، والذي يخبرك كيف تحتاج إلى تغيير كل الأوزان والتحيزات، كل هذه الاتصالات، وذلك لتقليل التكلفة بشكل أكثر كفاءة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. ",
  "translatedText": "Backpropagation، موضوع هذا الفيديو، هو خوارزمية لحساب هذا التدرج المعقد والجنوني. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "The one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. ",
  "translatedText": "الفكرة الوحيدة من الفيديو الأخير التي أريدك حقًا أن تضعها بقوة في ذهنك الآن هي أنه نظرًا لأن التفكير في متجه التدرج كاتجاه في 13000 بُعد هو، بعبارة مبسطة، خارج نطاق مخيلتنا، هناك فكرة أخرى الطريقة التي يمكنك التفكير في ذلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. ",
  "translatedText": "يخبرك حجم كل مكون هنا بمدى حساسية دالة التكلفة لكل وزن وتحيز. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. ",
  "translatedText": "على سبيل المثال، لنفترض أنك قمت بتنفيذ العملية التي أنا على وشك وصفها، وحساب التدرج السلبي، والمكون المرتبط بالوزن على هذه الحافة هنا يصبح 3.2، في حين أن المكون المرتبط بهذه الحافة هنا يخرج كـ 0.1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. ",
  "translatedText": "الطريقة التي تفسر بها ذلك هي أن تكلفة الوظيفة أكثر حساسية بمقدار 32 مرة للتغيرات في هذا الوزن الأول، لذلك إذا قمت بتحريك هذه القيمة قليلاً، فسوف يتسبب ذلك في بعض التغيير في التكلفة، وهذا التغيير أكبر بـ 32 مرة مما ستعطيه نفس الاهتزازة لهذا الوزن الثاني. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and index chasing of it all. ",
  "translatedText": "شخصيًا، عندما كنت أتعلم لأول مرة عن الانتشار العكسي، أعتقد أن الجانب الأكثر إرباكًا كان مجرد الترميز ومطاردة الفهرس لكل شيء. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. ",
  "translatedText": "ولكن بمجرد أن تكتشف ما يفعله كل جزء من هذه الخوارزمية بالفعل، فإن كل تأثير فردي يحدثه يكون في الواقع بديهيًا جدًا، الأمر فقط أن هناك الكثير من التعديلات الصغيرة التي يتم وضعها فوق بعضها البعض. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. ",
  "translatedText": "لذا، سأبدأ الأمور هنا مع التجاهل التام للتدوين، وسأنتقل فقط عبر تأثيرات كل مثال تدريبي على الأوزان والتحيزات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. ",
  "translatedText": "نظرًا لأن دالة التكلفة تتضمن حساب متوسط تكلفة معينة لكل مثال على عشرات الآلاف من أمثلة التدريب، فإن الطريقة التي نضبط بها الأوزان والتحيزات لخطوة نزول متدرجة واحدة تعتمد أيضًا على كل مثال على حدة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. ",
  "translatedText": "أو بالأحرى، من حيث المبدأ، ينبغي أن يكون الأمر كذلك، ولكن من أجل الكفاءة الحسابية، سنقوم بخدعة صغيرة لاحقًا لمنعك من الحاجة إلى ضرب كل مثال في كل خطوة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. ",
  "translatedText": "في حالات أخرى، الآن، كل ما سنفعله هو تركيز انتباهنا على مثال واحد، هذه الصورة للرقم 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted? ",
  "translatedText": "ما هو التأثير الذي يجب أن يحدثه هذا المثال التدريبي على كيفية تعديل الأوزان والتحيزات؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. ",
  "translatedText": "لنفترض أننا وصلنا إلى مرحلة لم يتم فيها تدريب الشبكة بشكل جيد بعد، وبالتالي فإن عمليات التنشيط في المخرجات ستبدو عشوائية جدًا، ربما مثل 0.5، 0.8، 0.2، وهكذا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases, but it's helpful to keep track of which adjustments we wish should take place to that output layer. ",
  "translatedText": "لا يمكننا تغيير تلك التنشيطات بشكل مباشر، لدينا فقط تأثير على الأوزان والتحيزات، ولكن من المفيد تتبع التعديلات التي نرغب في إجرائها على طبقة الإخراج تلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 242.52,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. ",
  "translatedText": "وبما أننا نريدها أن تصنف الصورة على أنها 2، فإننا نريد أن يتم دفع القيمة الثالثة للأعلى بينما يتم دفع جميع القيم الأخرى للأسفل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. ",
  "translatedText": "علاوة على ذلك، يجب أن تكون أحجام هذه الدفعات متناسبة مع مدى بعد كل قيمة حالية عن قيمتها المستهدفة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. ",
  "translatedText": "على سبيل المثال، الزيادة في تنشيط الخلية العصبية رقم 2 هي إلى حد ما أكثر أهمية من الانخفاض في الخلية العصبية رقم 8، والتي هي بالفعل قريبة جدًا من المكان الذي ينبغي أن تكون فيه. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. ",
  "translatedText": "لذا، بالتكبير أكثر، دعونا نركز فقط على هذه الخلية العصبية، تلك التي نرغب في زيادة تنشيطها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. ",
  "translatedText": "تذكر، يتم تعريف هذا التنشيط على أنه مجموع مرجح معين لجميع عمليات التنشيط في الطبقة السابقة، بالإضافة إلى التحيز، والذي يتم بعد ذلك توصيله بشيء مثل وظيفة السحق السيني، أو ReLU. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation. ",
  "translatedText": "لذلك هناك ثلاث طرق مختلفة يمكن أن تتعاون معًا للمساعدة في زيادة هذا التنشيط. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. ",
  "translatedText": "يمكنك زيادة التحيز، ويمكنك زيادة الأوزان، ويمكنك تغيير عمليات التنشيط من الطبقة السابقة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. ",
  "translatedText": "بالتركيز على كيفية ضبط الأوزان، لاحظ كيف أن للأوزان بالفعل مستويات مختلفة من التأثير. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. ",
  "translatedText": "إن الاتصالات مع الخلايا العصبية الأكثر سطوعًا من الطبقة السابقة لها التأثير الأكبر حيث يتم ضرب تلك الأوزان بقيم تنشيط أكبر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. ",
  "translatedText": "لذا، إذا قمت بزيادة أحد هذه الأوزان، فسيكون لها في الواقع تأثير أقوى على دالة التكلفة النهائية من زيادة أوزان الاتصالات مع الخلايا العصبية الخافتة، على الأقل فيما يتعلق بهذا المثال التدريبي. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck. ",
  "translatedText": "تذكر، عندما نتحدث عن النسب المتدرج، فإننا لا نهتم فقط بما إذا كان يجب دفع كل مكون لأعلى أو لأسفل، بل نهتم بالمكونات التي تمنحك أكبر قدر من المال. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together. ",
  "translatedText": "هذا، بالمناسبة، يذكرنا على الأقل إلى حد ما بنظرية في علم الأعصاب حول كيفية تعلم الشبكات البيولوجية للخلايا العصبية، وهي نظرية هيبيان، والتي غالبًا ما يتم تلخيصها في العبارة، الخلايا العصبية التي تشتعل معًا تترابط معًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active and the ones which we wish to become more active. ",
  "translatedText": "هنا، أكبر زيادات في الأوزان، وأكبر تقوية للاتصالات، تحدث بين الخلايا العصبية الأكثر نشاطًا وتلك التي نرغب في أن تصبح أكثر نشاطًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those firing when thinking about it. ",
  "translatedText": "بمعنى ما، فإن الخلايا العصبية التي تنشط أثناء رؤية الرقم 2 تصبح أكثر ارتباطًا بتلك الخلايا العصبية التي تنشط عند التفكير في الأمر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note. ",
  "translatedText": "لأكون واضحًا، لست في وضع يسمح لي بالإدلاء ببيانات بطريقة أو بأخرى حول ما إذا كانت الشبكات الاصطناعية من الخلايا العصبية تتصرف مثل العقول البيولوجية، وهذه الفكرة التي تنطلق معًا تأتي مع نجمتين ذات معنى، ولكن تم اعتبارها فضفاضة للغاية تشبيهًا، أجد أنه من المثير للاهتمام ملاحظة ذلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer. ",
  "translatedText": "على أية حال، الطريقة الثالثة التي يمكننا من خلالها المساعدة في زيادة تنشيط هذه الخلايا العصبية هي تغيير جميع عمليات التنشيط في الطبقة السابقة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active. ",
  "translatedText": "على وجه التحديد، إذا أصبح كل شيء مرتبط بالخلية العصبية ذات الرقم 2 ذات الوزن الإيجابي أكثر سطوعًا، وإذا أصبح كل شيء مرتبط بالوزن السلبي أكثر خفوتًا، فإن تلك الخلية العصبية ذات الرقم 2 ستصبح أكثر نشاطًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. ",
  "translatedText": "وكما هو الحال مع تغييرات الوزن، ستحصل على أقصى استفادة من أموالك من خلال البحث عن تغييرات تتناسب مع حجم الأوزان المقابلة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases. ",
  "translatedText": "الآن بالطبع، لا يمكننا التأثير بشكل مباشر على تلك التنشيطات، لدينا فقط السيطرة على الأوزان والتحيزات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are. ",
  "translatedText": "ولكن كما هو الحال مع الطبقة الأخيرة، من المفيد الاحتفاظ بملاحظة حول ماهية تلك التغييرات المرغوبة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants. ",
  "translatedText": "لكن ضع في اعتبارك، عند تصغير خطوة واحدة هنا، فإن هذا هو فقط ما تريده تلك الخلية العصبية الناتجة من الرقم 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. ",
  "translatedText": "تذكر أننا نريد أيضًا أن تصبح جميع الخلايا العصبية الأخرى في الطبقة الأخيرة أقل نشاطًا، ولكل من تلك الخلايا العصبية الأخرى أفكارها الخاصة حول ما يجب أن يحدث لتلك الطبقة الثانية قبل الأخيرة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. ",
  "translatedText": "لذلك يتم إضافة رغبة هذه الخلية العصبية ذات الرقم 2 مع رغبات جميع الخلايا العصبية الناتجة الأخرى فيما يتعلق بما يجب أن يحدث لهذه الطبقة الثانية إلى الأخيرة، مرة أخرى بما يتناسب مع الأوزان المقابلة، وبما يتناسب مع مقدار احتياجات كل من تلك الخلايا العصبية للتغيير. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in. ",
  "translatedText": "وهنا تأتي فكرة الانتشار العكسي. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. ",
  "translatedText": "من خلال جمع كل هذه التأثيرات المرغوبة معًا، تحصل بشكل أساسي على قائمة من التنبيهات التي تريد أن تحدث لهذه الطبقة الثانية إلى الأخيرة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. ",
  "translatedText": "وبمجرد حصولك على هذه، يمكنك تطبيق نفس العملية بشكل متكرر على الأوزان والتحيزات ذات الصلة التي تحدد تلك القيم، وتكرار نفس العملية التي مررت بها للتو والتحرك للخلف عبر الشبكة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. ",
  "translatedText": "وبالتصغير أكثر قليلاً، تذكر أن هذا كله هو الطريقة التي يرغب بها مثال تدريبي واحد في دفع كل واحد من تلك الأوزان والتحيزات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2. ",
  "translatedText": "إذا استمعنا فقط إلى ما يريده هذا الشخصان، فسيتم تحفيز الشبكة في النهاية لتصنيف جميع الصور على أنها رقم 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes. ",
  "translatedText": "إذن ما تفعله هو اتباع نفس روتين الدعم الخلفي لكل مثال تدريبي آخر، وتسجيل كيف يرغب كل منهم في تغيير الأوزان والتحيزات، وحساب متوسط تلك التغييرات المرغوبة معًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. ",
  "translatedText": "هذه المجموعة هنا من متوسط الدفعات لكل وزن وتحيز هي، بشكل فضفاض، التدرج السلبي لوظيفة التكلفة المشار إليها في الفيديو الأخير، أو على الأقل شيء يتناسب معها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. ",
  "translatedText": "أقول ذلك بشكل فضفاض فقط لأنني لم أحصل بعد على دقة كمية بشأن تلك التنبيهات، ولكن إذا فهمت كل تغيير أشرت إليه للتو، ولماذا يكون بعضها أكبر نسبيًا من البعض الآخر، وكيف يجب إضافتها جميعًا معًا، فإنك تفهم آليات حدوث ذلك. ما يفعله الانتشار العكسي في الواقع. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. ",
  "translatedText": "بالمناسبة، في الممارسة العملية، يستغرق الأمر وقتًا طويلاً للغاية من أجهزة الكمبيوتر لإضافة تأثير كل مثال تدريبي في كل خطوة نزول متدرجة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead. ",
  "translatedText": "إذن، إليك ما يتم فعله عادةً بدلاً من ذلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and divide it into a whole bunch of mini-batches, let's say each one having 100 training examples. ",
  "translatedText": "يمكنك خلط بيانات التدريب الخاصة بك عشوائيًا وتقسيمها إلى مجموعة كاملة من الدفعات الصغيرة، لنفترض أن كل واحدة تحتوي على 100 مثال تدريبي. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch. ",
  "translatedText": "ثم تقوم بحساب الخطوة وفقًا للدفعة الصغيرة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly it gives you a significant computational speedup. ",
  "translatedText": "إنه ليس التدرج الفعلي لدالة التكلفة، والذي يعتمد على جميع بيانات التدريب، وليس هذه المجموعة الفرعية الصغيرة، لذا فهي ليست الخطوة الأكثر كفاءة إلى أسفل، ولكن كل دفعة صغيرة تمنحك تقريبًا جيدًا، والأهم من ذلك أنها يمنحك تسريعًا حسابيًا كبيرًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. ",
  "translatedText": "إذا كنت تريد رسم مسار شبكتك تحت سطح التكلفة ذي الصلة، فسيكون الأمر أشبه برجل مخمور يتعثر بلا هدف أسفل التل ولكنه يتخذ خطوات سريعة، بدلاً من رجل يحسب بعناية ويحدد الاتجاه الدقيق لانحدار كل خطوة. قبل اتخاذ خطوة بطيئة وحذرة للغاية في هذا الاتجاه. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent. ",
  "translatedText": "يشار إلى هذه التقنية باسم النسب التدرج العشوائي. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we? ",
  "translatedText": "هناك الكثير مما يحدث هنا، لذا دعونا نلخص الأمر لأنفسنا، أليس كذلك؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. ",
  "translatedText": "Backpropagation هي خوارزمية لتحديد كيف يرغب مثال تدريبي واحد في دفع الأوزان والتحيزات، ليس فقط فيما يتعلق بما إذا كان ينبغي أن ترتفع أم تنخفض، ولكن من حيث النسب النسبية لتلك التغييرات التي تسبب الانخفاض الأسرع في يكلف. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes you get, but that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch. ",
  "translatedText": "قد تتضمن خطوة النزول المتدرج الحقيقية القيام بذلك لجميع العشرات والآلاف من أمثلة التدريب وحساب متوسط التغييرات المرغوبة التي تحصل عليها، ولكن هذا بطيء من الناحية الحسابية، لذلك بدلاً من ذلك تقوم بتقسيم البيانات عشوائيًا إلى دفعات صغيرة وحساب كل خطوة فيما يتعلق بـ دفعة صغيرة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.26,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples. ",
  "translatedText": "من خلال مراجعة جميع الدفعات الصغيرة بشكل متكرر وإجراء هذه التعديلات، سوف تتقارب نحو الحد الأدنى المحلي لوظيفة التكلفة، مما يعني أن شبكتك ستؤدي في النهاية عملًا جيدًا حقًا في أمثلة التدريب. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms. ",
  "translatedText": "إذن مع كل ما قيل، فإن كل سطر من التعليمات البرمجية الذي قد يدخل في تنفيذ backprop يتوافق فعليًا مع شيء رأيته الآن، على الأقل بعبارات غير رسمية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. ",
  "translatedText": "لكن في بعض الأحيان، معرفة ما تفعله الرياضيات هو نصف المعركة فقط، ومجرد تمثيل الشيء اللعين هو المكان الذي يصبح فيه الأمر مشوشًا ومربكًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So, for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. ",
  "translatedText": "لذا، بالنسبة لأولئك منكم الذين يريدون التعمق أكثر، فإن الفيديو التالي يتناول نفس الأفكار التي تم تقديمها هنا للتو، ولكن فيما يتعلق بحساب التفاضل والتكامل الأساسي، والذي نأمل أن يجعله مألوفًا أكثر قليلاً كما ترون الموضوع في مصادر أخرى. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. ",
  "translatedText": "قبل ذلك، هناك شيء واحد يستحق التأكيد عليه وهو أنه لكي تعمل هذه الخوارزمية، وهذا ينطبق على جميع أنواع التعلم الآلي خارج الشبكات العصبية فقط، فأنت بحاجة إلى الكثير من بيانات التدريب. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans. ",
  "translatedText": "في حالتنا، الشيء الوحيد الذي يجعل الأرقام المكتوبة بخط اليد مثالًا رائعًا هو وجود قاعدة بيانات MNIST، مع العديد من الأمثلة التي تم تصنيفها بواسطة البشر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with. ",
  "translatedText": "لذا فإن التحدي الشائع الذي سيكون العاملون في التعلم الآلي على دراية به هو مجرد الحصول على بيانات التدريب المصنفة التي تحتاجها بالفعل، سواء كان ذلك جعل الأشخاص يقومون بتسمية عشرات الآلاف من الصور، أو أي نوع آخر من البيانات التي قد تتعامل معها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]