1
00:00:04,350 --> 00:00:06,410
Рассмотрим обратное распространение,

2
00:00:06,410 --> 00:00:09,400
основной алгоритм обучения нейронных сетей.

3
00:00:09,400 --> 00:00:11,210
После краткого напоминания о том, что мы узнали,

4
00:00:11,210 --> 00:00:15,470
я расскажу о том, что на самом деле делает алгоритм

5
00:00:15,470 --> 00:00:17,270
без ссылок на формулы.

6
00:00:17,640 --> 00:00:20,310
Для тех из вас, кто хочет погрузиться в математику,

7
00:00:20,310 --> 00:00:23,140
следующее видео рассматривает математическое обоснование, лежащее в основе всего этого.

8
00:00:23,940 --> 00:00:25,550
Если вы посмотрели последние два видео

9
00:00:25,550 --> 00:00:27,920
или вы просто пропустили, т.к. знакомы с этим, то

10
00:00:27,920 --> 00:00:31,290
вы знаете, что такое нейронная сеть и как она передает информацию вперед.

11
00:00:31,660 --> 00:00:35,100
Здесь мы делаем классический пример распознавания рукописных цифр,

12
00:00:35,100 --> 00:00:39,930
чьи значения пикселей поступают в первый слой сети с 784 нейронами.

13
00:00:39,930 --> 00:00:44,000
Я показываю сеть с двумя скрытыми слоями, имеющими всего 16 нейронов,

14
00:00:44,000 --> 00:00:49,250
и выходной слой из 10 нейронов, указывающий, какую цифру выбирает сеть в качестве своего ответа.

15
00:00:50,020 --> 00:00:54,340
Я также надеюсь, что вы поняли метод градиентного спуска, из последнего видео,

16
00:00:54,340 --> 00:00:56,890
и как мы понимаем, что

17
00:00:56,890 --> 00:01:01,450
мы хотим найти, какие веса и смещения сводят к минимуму определенную функцию стоимости.

18
00:01:02,010 --> 00:01:05,470
В качестве быстрого напоминания о стоимости одного учебного примера,

19
00:01:05,470 --> 00:01:08,400
то, что вы делаете, - это результат, который выдает сеть,

20
00:01:08,400 --> 00:01:10,850
наряду с выходом, который вы хотели, бы получить,

21
00:01:11,200 --> 00:01:14,820
и вы просто добавляете квадраты разности между каждым компонентом.

22
00:01:15,370 --> 00:01:20,020
Выполняя это для всех ваших десятков тысяч примеров обучения и усредняя результаты,

23
00:01:20,020 --> 00:01:22,410
вы получаете общую стоимость сети.

24
00:01:22,910 --> 00:01:26,010
И, так как этого недостаточно для ответа, как описано в последнем видео,

25
00:01:26,010 --> 00:01:30,870
то, мы ищем отрицательный градиент этой функции стоимости,

26
00:01:30,870 --> 00:01:35,720
который показывает, как вам нужно изменить все веса и смещения каждого соединения,

27
00:01:35,720 --> 00:01:38,270
чтобы наиболее эффективно снизить стоимость.

28
00:01:42,950 --> 00:01:45,210
Обратное распространение, тема этого видео,

29
00:01:45,210 --> 00:01:48,800
является алгоритмом вычисления этого сумасшедшего сложного градиента.

30
00:01:49,490 --> 00:01:54,010
И одна из идей из последнего видео, которую я хочу донести проще,

31
00:01:54,010 --> 00:01:58,910
состоит в том, что, поскольку мысли о векторе градиенте как о направлениях в 13000 измерениях,

32
00:01:58,910 --> 00:02:02,090
мягко говоря, вне сферы нашего воображения,

33
00:02:02,090 --> 00:02:03,510
есть еще один способ, которым вы можете думать об этом:

34
00:02:04,580 --> 00:02:07,710
Величина каждого компонента здесь говорит вам

35
00:02:07,710 --> 00:02:11,140
насколько чувствительна функция стоимости к каждому весу и смещению.

36
00:02:11,810 --> 00:02:14,580
Например, вы повторяете процесс, который я описываю,

37
00:02:14,580 --> 00:02:16,370
и вы вычисляете отрицательный градиент,

38
00:02:16,370 --> 00:02:21,470
и компонент, связанный с весом на этом крае, составляет 3,2,

39
00:02:21,870 --> 00:02:26,370
в то время как компонент, связанный с этим ребром, отображается как 0,1.

40
00:02:26,910 --> 00:02:28,420
Таким образом,

41
00:02:28,420 --> 00:02:33,080
стоимость функции в 32 раза более чувствительна к изменениям первого веса.

42
00:02:33,640 --> 00:02:35,930
Поэтому, если вы хотите немного изменить значение,

43
00:02:35,930 --> 00:02:38,190
то это вызовет некоторое изменение стоимости,

44
00:02:38,190 --> 00:02:43,200
и это изменение в 32 раза больше, чем то, что даст такое же изменение второго веса.

45
00:02:48,520 --> 00:02:51,440
Когда я впервые узнал о обратном распространении,

46
00:02:51,440 --> 00:02:55,740
мне казалось, что самым запутанным аспектом было обозначение и индекс.

47
00:02:56,180 --> 00:02:59,450
Но как только вы разворачиваете  каждую часть этого алгоритма,

48
00:02:59,450 --> 00:03:02,870
каждый отдельный элемент на самом деле довольно понятен.

49
00:03:03,180 --> 00:03:06,740
Получается множество небольших корректировок, которые накладываются друг на друга.

50
00:03:07,660 --> 00:03:11,290
Поэтому я собираюсь пренебречь обозначениями

51
00:03:11,290 --> 00:03:13,370
и просто рассмотрю результаты того, как

52
00:03:13,370 --> 00:03:16,350
каждый пример обучения звасисит от веса и смещения.

53
00:03:17,090 --> 00:03:18,590
Поскольку функция стоимости включает

54
00:03:18,590 --> 00:03:23,640
усредную определенную стоимость по всем десяткам тысяч примеров обучения,

55
00:03:23,970 --> 00:03:28,640
способ, которым мы корректируем веса и смещения для одного шага уменьшения градиента

56
00:03:28,640 --> 00:03:31,140
также зависит от каждого отдельного примера,

57
00:03:31,680 --> 00:03:33,200
или, скорее, должен,

58
00:03:33,200 --> 00:03:35,930
но для вычислительной эффективности позже мы собираемся сделать небольшой трюк

59
00:03:35,930 --> 00:03:39,370
чтобы вы не нуждались в каждом конкретном примере для каждого отдельного шага.

60
00:03:39,790 --> 00:03:41,330
Другое дело,

61
00:03:41,330 --> 00:03:46,160
что мы собираемся сосредоточить наше внимание на одном примере: изображение цифры 2.

62
00:03:46,670 --> 00:03:51,650
Какое влияние должен иметь этот пример тренировки на то, как корректируются веса и смещения?

63
00:03:52,680 --> 00:03:55,240
Допустим, мы находимся в точке, где сеть еще недостаточно подготовлена,

64
00:03:55,240 --> 00:03:57,970
поэтому активации на выходе будут выглядеть довольно случайными,

65
00:03:57,970 --> 00:04:02,040
может быть, что-то вроде 0,5, 0,8, 0,2, и так далее.

66
00:04:02,640 --> 00:04:07,450
Мы не можем напрямую изменять эти активации, мы можем влиять только на вес и смещение,

67
00:04:07,790 --> 00:04:12,670
но полезно отслеживать, какие корректировки должны быть для этого выходного слоя,

68
00:04:13,270 --> 00:04:15,710
и поскольку мы хотим, чтобы он классифицировал изображение как 2,

69
00:04:16,040 --> 00:04:21,360
мы хотим, чтобы третье значение увеличивалось, а все остальные уменьшались.

70
00:04:22,040 --> 00:04:26,020
Более того, размеры этих изменений должны быть пропорциональны тому,

71
00:04:26,020 --> 00:04:29,630
насколько большая разница между текущим и целевым значеним.

72
00:04:30,220 --> 00:04:34,350
Например, увеличение к активации нейронов номер 2

73
00:04:34,350 --> 00:04:38,490
является более важным, чем уменьшение числа нейронов числа 8,

74
00:04:38,490 --> 00:04:40,630
который уже близок к тому, где он должен быть.

75
00:04:41,990 --> 00:04:45,250
Поэтому, увеличивая масштаб, давайте сосредоточимся только на этом нейроне,

76
00:04:45,250 --> 00:04:47,530
активация которого мы хотим увеличить.

77
00:04:48,160 --> 00:04:50,550
Помните, что активация определяется как

78
00:04:50,550 --> 00:04:56,430
определенную взвешенную сумму всех активаций в предыдущем слое плюс смещение,

79
00:04:56,430 --> 00:05:01,290
которые подключены к чему-то вроде функции сигмовидного сгибания или ReLU,

80
00:05:01,810 --> 00:05:07,360
Таким образом, есть три разных способа, которые могут объединяться, чтобы помочь увеличить эту активацию:

81
00:05:07,680 --> 00:05:10,970
вы можете увеличить смещение, вы можете увеличить вес,

82
00:05:10,970 --> 00:05:14,030
и вы можете изменить активацию нейронов в предыдущем слое.

83
00:05:14,950 --> 00:05:17,770
Сосредоточив внимание на том, как следует регулировать вес,

84
00:05:17,770 --> 00:05:21,410
обратите внимание на то, как веса имеют разные уровни влияния:

85
00:05:21,410 --> 00:05:25,750
связи с самыми яркими нейронами из предыдущего слоя имеют наибольший эффект,

86
00:05:25,750 --> 00:05:29,240
поскольку эти веса умножаются на большие значения активации.

87
00:05:31,330 --> 00:05:33,480
Поэтому, если вы должны увеличить один из этих весов,

88
00:05:33,480 --> 00:05:37,370
он фактически оказывает более сильное влияние на конечную функцию стоимости

89
00:05:37,370 --> 00:05:40,820
чем увеличение весов связей с тусклыми нейронами,

90
00:05:40,820 --> 00:05:43,650
по крайней мере, насколько это касается одного примера обучения.

91
00:05:44,380 --> 00:05:46,890
Помните, когда мы говорили о градиентном уменьшении,

92
00:05:46,890 --> 00:05:50,620
мы не просто заботимся о том, нужно ли увеличивать или уменьшать каждый компонент,

93
00:05:50,620 --> 00:05:53,370
мы заботимся о том, какие из них дают вам наибольший эффект.

94
00:05:55,270 --> 00:05:59,310
Это, кстати, несколько напоминает теорию в области нейронауки

95
00:05:59,310 --> 00:06:01,870
как изучают биологические нейронные сети

96
00:06:01,870 --> 00:06:06,820
Теория Hebbian - часто суммируется во фразе «нейроны, которые запускаются вместе,  соединяются».

97
00:06:07,260 --> 00:06:12,200
Здесь наибольшее увеличение веса, наибольшее усиление связей,

98
00:06:12,200 --> 00:06:14,840
происходит между наиболее активными нейронами,

99
00:06:14,840 --> 00:06:17,590
и те, которые мы хотим активизировать.

100
00:06:18,020 --> 00:06:21,060
В некотором смысле, нейроны, стреляющие, видя 2,

101
00:06:21,060 --> 00:06:24,680
сильнее привязывайтесь к тем, кто стреляет, думая о 2.

102
00:06:25,420 --> 00:06:28,780
Чтобы быть ясным, я действительно не в состоянии делать заявления так или иначе

103
00:06:28,780 --> 00:06:33,080
о том, как искусственные сети нейронов ведут себя как биологические мозги,

104
00:06:33,080 --> 00:06:37,250
и эта идея объединяет вместе целую пару значащих звездочек.

105
00:06:37,250 --> 00:06:41,260
Но, как очень простая аналогия, мне интересно отметить.

106
00:06:41,890 --> 00:06:46,020
Во всяком случае, третий способ, которым мы можем помочь увеличить активацию нейрона

107
00:06:46,020 --> 00:06:49,060
это изменение всех активаций в предыдущем слое,

108
00:06:49,560 --> 00:06:54,970
а именно, если все, что связано с этой цифрой 2 нейроном с положительным весом, стало ярче,

109
00:06:54,970 --> 00:06:57,960
и если все, что связано с отрицательным весом, уменьшилось,

110
00:06:58,340 --> 00:07:00,890
то эта цифра 2 нейрона станет более активной.

111
00:07:02,450 --> 00:07:06,130
И, подобно изменениям веса, вы получите максимальную отдачу от своего доллара

112
00:07:06,130 --> 00:07:10,550
путем поиска изменений, которые пропорциональны размеру соответствующих весов.

113
00:07:12,120 --> 00:07:15,360
Теперь, конечно, мы не можем напрямую влиять на эти активации,

114
00:07:15,360 --> 00:07:17,780
мы имеем только контроль над весами и предубеждениями.

115
00:07:18,220 --> 00:07:23,610
Но так же, как и в последнем слое, полезно просто отметить, что это за желаемые изменения.

116
00:07:24,450 --> 00:07:29,720
Но имейте в виду, уменьшая на один шаг здесь, это только то, что хочет эта цифра 2.

117
00:07:29,720 --> 00:07:34,840
Помните, мы также хотим, чтобы все остальные нейроны в последнем слое стали менее активными,

118
00:07:34,840 --> 00:07:36,500
и каждый из этих других выходных нейронов

119
00:07:36,500 --> 00:07:39,840
имеет свои собственные мысли о том, что должно произойти с этим вторым-последним слоем.

120
00:07:43,110 --> 00:07:46,140
Итак, желание этой цифры 2 нейрона

121
00:07:46,140 --> 00:07:50,520
добавляется вместе с желаниями всех других выходных нейронов

122
00:07:50,520 --> 00:07:53,240
что должно произойти с этим вторым-последним слоем.

123
00:07:53,580 --> 00:07:56,400
Опять же, пропорционально соответствующим весам,

124
00:07:56,400 --> 00:08:00,910
и пропорционально тому, как каждый из этих нейронов должен измениться.

125
00:08:01,480 --> 00:08:05,510
Здесь прямо возникает идея распространения назад.

126
00:08:05,960 --> 00:08:08,730
Объединив все эти желаемые эффекты,

127
00:08:08,730 --> 00:08:13,560
вы в основном получаете список подтасовки, которые вы хотите выполнить со вторым до последнего уровня.

128
00:08:14,180 --> 00:08:15,390
И как только вы их получите,

129
00:08:15,390 --> 00:08:17,850
вы можете рекурсивно применять тот же процесс

130
00:08:17,850 --> 00:08:21,180
к соответствующим весам и смещениям, которые определяют эти значения,

131
00:08:21,180 --> 00:08:25,140
повторяя тот же процесс, я просто прошел и двинулся назад по сети.

132
00:08:29,030 --> 00:08:30,370
И немного увеличивая масштаб,

133
00:08:30,370 --> 00:08:31,920
помните, что все это просто

134
00:08:31,920 --> 00:08:37,400
как один пример тренинга хочет подтолкнуть каждый из этих весов и предубеждений.

135
00:08:37,400 --> 00:08:39,700
Если мы будем слушать только то, что хотели,

136
00:08:39,700 --> 00:08:43,400
сеть в конечном итоге будет стимулировать просто классифицировать все изображения как 2.

137
00:08:44,030 --> 00:08:49,420
Итак, что вы делаете, вы проходите эту же процедуру backprop для каждого другого примера обучения,

138
00:08:49,420 --> 00:08:53,200
записывая, как каждый из них хотел бы изменить вес и предубеждения,

139
00:08:53,650 --> 00:08:56,220
и вы усреднили эти желаемые изменения.

140
00:09:02,050 --> 00:09:06,940
Эта коллекция здесь усредненных подтасовков к каждому весу и смещению,

141
00:09:06,940 --> 00:09:11,910
свободно говоря, отрицательный градиент функции стоимости, упомянутый в последнем видео,

142
00:09:11,910 --> 00:09:13,740
или, по крайней мере, что-то пропорциональное ему.

143
00:09:14,360 --> 00:09:19,570
Я говорю «свободно говоря», только потому, что мне еще предстоит получить количественную информацию об этих подтасовках.

144
00:09:19,570 --> 00:09:22,190
Но если вы понимаете все изменения, о которых я только что говорил,

145
00:09:22,190 --> 00:09:24,770
почему некоторые из них пропорционально больше других,

146
00:09:24,770 --> 00:09:27,160
и как все они должны быть объединены вместе,

147
00:09:27,160 --> 00:09:31,170
вы понимаете механику того, что на самом деле делает backpropagation.

148
00:09:34,050 --> 00:09:37,400
Кстати, на практике компьютеры занимают очень много времени

149
00:09:37,400 --> 00:09:42,490
чтобы добавить влияние каждого отдельного примера обучения, каждого шага спуска градиента.

150
00:09:43,010 --> 00:09:44,960
Итак, вот что обычно делается:

151
00:09:45,440 --> 00:09:50,280
Вы произвольно перетасовываете свои данные обучения, а затем делите его на целую кучу мини-партий,

152
00:09:50,280 --> 00:09:52,680
допустим, каждый из них имеет 100 учебных примеров.

153
00:09:53,240 --> 00:09:56,430
Затем вы вычисляете шаг в соответствии с мини-пакетом.

154
00:09:56,850 --> 00:09:59,390
Это не будет фактическим градиентом функции стоимости,

155
00:09:59,390 --> 00:10:02,630
который зависит от всех данных обучения, а не от этого крошечного подмножества.

156
00:10:03,100 --> 00:10:05,640
Так что это не самый эффективный шаг вниз.

157
00:10:06,080 --> 00:10:08,970
Но каждая мини-партия действительно дает вам довольно хорошее приближение,

158
00:10:08,970 --> 00:10:12,250
и что более важно, это дает вам значительную вычислительную скорость.

159
00:10:12,820 --> 00:10:16,810
Если бы вы построили траекторию своей сети под соответствующей ценовой поверхностью,

160
00:10:16,810 --> 00:10:22,030
это было бы немного больше, как пьяный человек, бесцельно спотыкающийся с холма, но делая быстрые шаги;

161
00:10:22,030 --> 00:10:27,180
а не тщательно вычисляющий человек, определяющий точное направление спуска на каждом шаге

162
00:10:27,180 --> 00:10:30,350
прежде чем делать очень медленный и осторожный шаг в этом направлении.

163
00:10:31,460 --> 00:10:34,940
Этот метод называется «стохастическим градиентным спуском».

164
00:10:36,000 --> 00:10:39,800
Здесь много чего происходит, поэтому давайте просто подытожим это для себя, не так ли?

165
00:10:40,240 --> 00:10:42,270
Backpropagation - алгоритм

166
00:10:42,270 --> 00:10:47,370
для определения того, как один пример тренинга хотел бы подтолкнуть веса и предубеждения,

167
00:10:47,370 --> 00:10:49,930
не только с точки зрения того, должны ли они подниматься или опускаться,

168
00:10:49,930 --> 00:10:55,700
но с точки зрения того, что относительные пропорции к этим изменениям приводят к самому быстрому снижению стоимости.

169
00:10:56,240 --> 00:10:58,270
Истинный шаг спуска градиента

170
00:10:58,270 --> 00:11:01,820
будет включать в себя выполнение этого для всех ваших десятков и тысяч учебных примеров

171
00:11:01,820 --> 00:11:04,260
и усреднение желаемых изменений, которые вы получаете.

172
00:11:04,830 --> 00:11:06,340
Но это вычислительно медленно.

173
00:11:06,690 --> 00:11:10,480
Поэтому вместо этого вы произвольно подразделяете данные на эти мини-партии

174
00:11:10,480 --> 00:11:13,460
и вычислить каждый шаг в отношении мини-партии.

175
00:11:13,900 --> 00:11:17,690
Неоднократно проходя через все мини-партии и делая эти корректировки,

176
00:11:17,690 --> 00:11:21,050
вы сходитесь к локальному минимуму функции стоимости,

177
00:11:21,430 --> 00:11:25,740
то есть ваша сеть будет в конечном итоге делать действительно хорошую работу на примерах обучения.

178
00:11:27,450 --> 00:11:32,290
Таким образом, со всем сказанным, каждая строка кода, которая будет внедрять backprop

179
00:11:32,290 --> 00:11:36,970
фактически соответствует тому, что вы сейчас видели, по крайней мере, в неформальном плане.

180
00:11:37,570 --> 00:11:40,960
Но иногда знание того, что делает математика, - это только половина битвы,

181
00:11:40,960 --> 00:11:44,460
и просто представлять чертову вещь, где она становится все запутанной и запутанной.

182
00:11:44,930 --> 00:11:47,620
Итак, для тех из вас, кто хочет глубже,

183
00:11:47,620 --> 00:11:50,670
следующее видео проходит те же идеи, которые были представлены здесь

184
00:11:50,670 --> 00:11:52,750
но в терминах основного исчисления,

185
00:11:52,750 --> 00:11:56,760
что, надеюсь, сделает его немного более знакомым, поскольку вы видите эту тему в других ресурсах.

186
00:11:57,210 --> 00:11:59,440
Прежде всего стоит подчеркнуть, что

187
00:11:59,440 --> 00:12:04,320
для того, чтобы этот алгоритм работал, и это касается всех видов машинного обучения за пределами только нейронных сетей,

188
00:12:04,320 --> 00:12:06,120
вам нужно много учебных данных.

189
00:12:06,430 --> 00:12:09,860
В нашем случае одна вещь, которая делает рукописные цифры таким приятным примером

190
00:12:09,860 --> 00:12:12,110
заключается в том, что существует база данных MNIST

191
00:12:12,110 --> 00:12:15,290
с таким количеством примеров, которые были обозначены людьми.

192
00:12:15,290 --> 00:12:19,000
Таким образом, общая задача, с которой вы работаете в машинном обучении, будет знакома с

193
00:12:19,000 --> 00:12:21,930
просто получает обозначенные данные обучения, которые вам действительно нужны,

194
00:12:22,240 --> 00:12:25,080
независимо от того, имеют ли люди метки десятки тысяч изображений

195
00:12:25,080 --> 00:12:27,550
или каким-либо другим типом данных, с которым вы можете иметь дело.

