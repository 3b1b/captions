1
00:00:00,000 --> 00:00:09,640
Itt foglalkozunk a visszaterjesztéssel, a neurális hálózatok tanulási folyamatának alapvető algoritmusával.

2
00:00:09,640 --> 00:00:13,320
Miután röviden összefoglalom, hol tartunk, először egy intuitív áttekintést teszek arról,

3
00:00:13,320 --> 00:00:17,400
hogy mit is csinál az algoritmus, a képletekre való hivatkozás nélkül.

4
00:00:17,400 --> 00:00:21,400
Aztán azok számára, akik szeretnének belemerülni a matematikába, a

5
00:00:21,400 --> 00:00:24,040
következő videó a mindezek alapjául szolgáló kalkulussal foglalkozik.

6
00:00:24,040 --> 00:00:27,320
Ha megnézte az utolsó két videót, vagy ha csak a megfelelő háttérrel ugrik

7
00:00:27,320 --> 00:00:31,080
be, akkor tudja, mi az a neurális hálózat, és hogyan továbbítja az információkat.

8
00:00:31,080 --> 00:00:35,520
Itt a klasszikus példát tesszük a kézzel írt számjegyek felismerésére, amelyek pixelértékei

9
00:00:35,520 --> 00:00:40,280
a hálózat első rétegébe kerülnek, 784 neuronnal, és bemutattam egy hálózatot két

10
00:00:40,280 --> 00:00:44,720
rejtett réteggel, amelyek mindegyike mindössze 16 neuronból áll, és egy kimenet. 10

11
00:00:44,720 --> 00:00:49,520
neuronból álló réteg, jelezve, hogy a hálózat melyik számjegyet választja válaszul.

12
00:00:49,520 --> 00:00:54,480
Azt is elvárom tőled, hogy megértsd a gradiens süllyedést, amint azt

13
00:00:54,480 --> 00:01:00,160
az utolsó videóban leírtuk, és hogyan értjük tanulás alatt azt, hogy

14
00:01:00,160 --> 00:01:02,080
meg akarjuk találni, mely súlyok és torzítások minimalizálnak egy bizonyos költségfüggvényt.

15
00:01:02,080 --> 00:01:07,560
Gyors emlékeztetőként, egyetlen betanítási példa költségéhez vegye ki a

16
00:01:07,560 --> 00:01:12,920
hálózat által adott kimenetet a kívánt kimenettel együtt, és

17
00:01:12,920 --> 00:01:15,560
adja össze az egyes összetevők közötti különbségek négyzetét.

18
00:01:15,560 --> 00:01:20,160
Ha ezt megteszi a több tízezer képzési példájához, és átlagolja

19
00:01:20,160 --> 00:01:23,040
az eredményeket, akkor ez megadja a hálózat teljes költségét.

20
00:01:23,040 --> 00:01:26,320
Mintha nem lenne elég ezen gondolkodni, ahogy az utolsó videóban is

21
00:01:26,320 --> 00:01:31,700
le van írva, a keresett dolog ennek a költségfüggvénynek a

22
00:01:31,700 --> 00:01:36,000
negatív gradiense, amely megmondja, hogyan kell módosítania az összes súlyozást

23
00:01:36,000 --> 00:01:43,080
és torzítást, ezeket a kapcsolatokat a költségek leghatékonyabb csökkentése érdekében.

24
00:01:43,080 --> 00:01:48,600
A backpropagation, ennek a videónak a témája, egy

25
00:01:48,600 --> 00:01:49,600
algoritmus ennek az őrülten bonyolult gradiensnek a kiszámításához.

26
00:01:49,600 --> 00:01:53,300
Az utolsó videó egyetlen gondolata, amit nagyon szeretném, ha most

27
00:01:53,300 --> 00:01:58,280
szilárdan a fejedben tartsd, az az, hogy mivel a

28
00:01:58,280 --> 00:02:02,660
gradiens vektort 13 000 dimenziós iránynak tekinteni, enyhén szólva

29
00:02:02,660 --> 00:02:04,620
túlmutat a képzeletünkön, van egy másik ahogyan gondolkodhatsz rajta.

30
00:02:04,620 --> 00:02:09,700
Az egyes komponensek nagysága itt megmutatja, hogy a

31
00:02:09,700 --> 00:02:11,820
költségfüggvény mennyire érzékeny az egyes súlyokra és torzításokra.

32
00:02:11,820 --> 00:02:15,180
Tegyük fel például, hogy végigmegy azon a folyamaton, amelyet leírok, és kiszámítja a

33
00:02:15,180 --> 00:02:19,800
negatív gradienst, és az ezen az élen lévő súlyhoz tartozó összetevő itt

34
00:02:19,800 --> 00:02:26,940
3 lesz. 2, míg az ehhez az élhez tartozó komponens itt 0-ként jelenik meg. 1.

35
00:02:26,940 --> 00:02:31,520
Ezt úgy értelmeznéd, hogy a függvény költsége 32-szer érzékenyebb az első

36
00:02:31,520 --> 00:02:36,100
súly változásaira, tehát ha egy kicsit mozgatnád ezt az értéket, az

37
00:02:36,100 --> 00:02:40,780
némi változást fog okozni a költségekben, és ez a változás 32-szer

38
00:02:40,780 --> 00:02:45,580
nagyobb, mint amit az adott második súlynak ugyanaz a mozgása adna.

39
00:02:45,580 --> 00:02:52,500
Személy szerint, amikor először tanultam a visszaszaporításról, azt hiszem,

40
00:02:52,500 --> 00:02:55,820
a legzavaróbb szempont az egész jelölése és indexelése volt.

41
00:02:55,820 --> 00:03:00,240
De ha egyszer kibontja, hogy ennek az algoritmusnak az egyes részei

42
00:03:00,240 --> 00:03:04,540
valójában mit csinálnak, minden egyes hatás, amelyet kifejtenek, valójában meglehetősen intuitív,

43
00:03:04,540 --> 00:03:07,740
csak arról van szó, hogy sok apró beállítás kerül egymásra.

44
00:03:07,740 --> 00:03:11,380
Úgyhogy a jelölések teljes figyelmen kívül hagyásával kezdem a dolgokat, és csak

45
00:03:11,380 --> 00:03:17,380
végig kell lépnem az egyes edzési példák súlyozására és torzításaira gyakorolt hatásain.

46
00:03:17,380 --> 00:03:21,880
Mivel a költségfüggvény magában foglalja egy bizonyos példánkénti költség átlagolását a több

47
00:03:21,880 --> 00:03:26,980
tízezer képzési példában, az is, hogy hogyan állítjuk be a súlyokat

48
00:03:26,980 --> 00:03:31,740
és a torzításokat egyetlen gradiens süllyedési lépéshez, minden egyes példától függ.

49
00:03:31,740 --> 00:03:35,300
Illetve elvileg kellene, de a számítási hatékonyság érdekében később teszünk egy

50
00:03:35,300 --> 00:03:39,860
kis trükköt, hogy ne kelljen minden egyes példát eltalálni minden lépésnél.

51
00:03:39,860 --> 00:03:44,460
Más esetekben jelenleg csak egyetlen példára

52
00:03:44,460 --> 00:03:46,780
összpontosítjuk figyelmünket, erre a 2-es képre.

53
00:03:46,780 --> 00:03:51,740
Milyen hatással lehet ennek az egyetlen edzési példának a súlyok és a torzítások beállítására?

54
00:03:51,740 --> 00:03:56,040
Tegyük fel, hogy egy olyan ponton vagyunk, ahol a hálózat még nem megfelelően képzett,

55
00:03:56,040 --> 00:04:01,620
így a kimenet aktiválásai elég véletlenszerűek lesznek, talán valami 0-nak. 5, 0. 8, 0. 2,

56
00:04:01,620 --> 00:04:02,780
tovább és tovább.

57
00:04:02,780 --> 00:04:06,700
Ezeket az aktiválásokat közvetlenül nem tudjuk megváltoztatni, csak a

58
00:04:06,700 --> 00:04:11,380
súlyokra és torzításokra van befolyásunk, de hasznos nyomon követni,

59
00:04:11,380 --> 00:04:13,340
hogy az adott kimeneti rétegen milyen módosításokat szeretnénk végrehajtani.

60
00:04:13,340 --> 00:04:18,220
És mivel azt akarjuk, hogy a képet 2-esnek minősítse, azt akarjuk,

61
00:04:18,220 --> 00:04:21,700
hogy a harmadik érték felfelé, míg az összes többi lefelé kerüljön.

62
00:04:21,700 --> 00:04:27,620
Ezen túlmenően, ezeknek a lökéseknek a méretének arányosnak kell lennie azzal,

63
00:04:27,620 --> 00:04:30,220
hogy az egyes aktuális értékek milyen távolságra vannak a célértéktől.

64
00:04:30,220 --> 00:04:35,260
Például a 2-es számú neuron aktiválásának növekedése bizonyos értelemben

65
00:04:35,260 --> 00:04:39,620
fontosabb, mint a 8-as számú neuron csökkenése, amely

66
00:04:39,620 --> 00:04:42,060
már elég közel van ahhoz, ahol lennie kellene.

67
00:04:42,060 --> 00:04:46,260
Tehát tovább közelítve csak erre az egyetlen

68
00:04:46,260 --> 00:04:47,900
neuronra koncentráljunk, arra, amelynek aktiválását szeretnénk növelni.

69
00:04:47,900 --> 00:04:53,680
Ne feledje, hogy az aktiválás az előző réteg összes aktiválásának egy

70
00:04:53,680 --> 00:04:58,380
bizonyos súlyozott összegeként van definiálva, plusz egy torzítás, amely azután valami

71
00:04:58,380 --> 00:05:01,900
olyasmihez van csatlakoztatva, mint a szigmoid squishification függvény vagy egy ReLU.

72
00:05:01,900 --> 00:05:07,060
Tehát három különböző út áll rendelkezésre,

73
00:05:07,060 --> 00:05:08,060
amelyek összefoghatnak az aktiválás növelése érdekében.

74
00:05:08,060 --> 00:05:12,800
Növelheti a torzítást, növelheti a súlyokat,

75
00:05:12,800 --> 00:05:15,300
és módosíthatja az előző réteg aktiválásait.

76
00:05:15,300 --> 00:05:19,720
Arra összpontosítva, hogyan kell beállítani a súlyokat, figyelje

77
00:05:19,720 --> 00:05:21,460
meg, hogy a súlyok valójában milyen mértékben befolyásolják.

78
00:05:21,460 --> 00:05:25,100
Az előző réteg legfényesebb neuronjaival való kapcsolatoknak van a legnagyobb

79
00:05:25,100 --> 00:05:31,420
hatása, mivel ezek a súlyok megszorozódnak a nagyobb aktiválási értékekkel.

80
00:05:31,420 --> 00:05:35,820
Tehát, ha növelné az egyik súlyt, az valójában erősebb hatással

81
00:05:35,820 --> 00:05:40,900
van a végső költségfüggvényre, mint a halványabb neuronokkal való kapcsolatok

82
00:05:40,900 --> 00:05:44,020
súlyának növelése, legalábbis ami ezt az egy gyakorlati példát illeti.

83
00:05:44,020 --> 00:05:48,700
Ne feledje, amikor gradiens süllyedésről beszélünk, nem csak azzal foglalkozunk,

84
00:05:48,700 --> 00:05:53,020
hogy az egyes komponensek felfelé vagy lefelé mozduljanak el, hanem

85
00:05:53,020 --> 00:05:54,020
az is, hogy melyik adják a legtöbbet a pénzéért.

86
00:05:54,020 --> 00:06:00,260
Ez egyébként legalább valamelyest emlékeztet az idegtudomány egy elméletére, amely szerint

87
00:06:00,260 --> 00:06:04,900
a neuronok biológiai hálózatai hogyan tanulnak, a hebbi elméletet, amelyet gyakran

88
00:06:04,900 --> 00:06:06,940
a következő kifejezéssel foglalnak össze: az idegsejtek, amelyek együtt tüzelnek össze.

89
00:06:06,940 --> 00:06:12,460
Itt a legnagyobb súlynövekedés, a kapcsolatok

90
00:06:12,460 --> 00:06:16,860
legnagyobb erősödése a legaktívabb és az

91
00:06:16,860 --> 00:06:18,100
aktívabbá tenni kívánt idegsejtek között történik.

92
00:06:18,100 --> 00:06:22,520
Bizonyos értelemben azok a neuronok, amelyek tüzelnek, miközben 2-t

93
00:06:22,520 --> 00:06:25,440
látnak, erősebben kapcsolódnak azokhoz, amelyek tüzelnek, ha rágondolunk.

94
00:06:25,440 --> 00:06:29,240
Az egyértelműség kedvéért nem vagyok abban a helyzetben, hogy ilyen vagy olyan

95
00:06:29,240 --> 00:06:34,020
kijelentéseket tegyek arról, hogy a mesterséges neuronhálózatok úgy viselkednek-e, mint a

96
00:06:34,020 --> 00:06:39,440
biológiai agyak, és ez az ötlet összekapcsolja a vezetékeket, és néhány

97
00:06:39,440 --> 00:06:41,760
jelentőségteljes csillaggal együtt jár, de nagyon laza. hasonlattal, érdekesnek találom megjegyezni.

98
00:06:41,760 --> 00:06:46,760
Egyébként a harmadik módja annak, hogy fokozzuk ennek a neuronnak az

99
00:06:46,760 --> 00:06:49,360
aktiválását, az az, hogy megváltoztatjuk az előző réteg összes aktiválását.

100
00:06:49,360 --> 00:06:55,080
Ugyanis, ha a pozitív súllyal rendelkező 2-es számjegyű neuronhoz kapcsolódó

101
00:06:55,080 --> 00:06:59,480
minden fényesebbé válna, és ha minden negatív súllyal kapcsolatos

102
00:06:59,480 --> 00:07:02,680
halványodna, akkor az a 2-es számjegyű neuron aktívabbá válna.

103
00:07:02,680 --> 00:07:06,200
És hasonlóan a súlyváltozásokhoz, a legtöbbet úgy érheti el, ha

104
00:07:06,200 --> 00:07:10,840
olyan változtatásokat keres, amelyek arányosak a megfelelő súlyok méretével.

105
00:07:10,840 --> 00:07:16,520
Természetesen ezeket az aktiválásokat közvetlenül nem tudjuk befolyásolni,

106
00:07:16,520 --> 00:07:18,320
csak a súlyokat és a torzításokat tudjuk ellenőrizni.

107
00:07:18,320 --> 00:07:22,960
De csakúgy, mint az utolsó rétegnél, hasznos

108
00:07:22,960 --> 00:07:23,960
feljegyezni, hogy melyek ezek a kívánt változtatások.

109
00:07:23,960 --> 00:07:29,040
De ne feledje, ha itt egy lépést kicsinyít, csak

110
00:07:29,040 --> 00:07:30,040
ez az, amit a 2-es számjegyű kimeneti neuron akar.

111
00:07:30,040 --> 00:07:34,960
Ne feledje, azt is szeretnénk, hogy az utolsó rétegben lévő összes

112
00:07:34,960 --> 00:07:38,460
többi neuron kevésbé legyen aktív, és ezeknek a többi kimeneti neuronnak

113
00:07:38,460 --> 00:07:43,200
megvan a maga gondolata arról, hogy mi történjen az utolsó réteggel.

114
00:07:43,200 --> 00:07:49,220
Tehát ennek a 2-es számjegyű neuronnak a vágya összeadódik az

115
00:07:49,220 --> 00:07:54,800
összes többi kimeneti neuron azon vágyaival, hogy mi történjen ezzel

116
00:07:54,800 --> 00:08:00,240
a második-utolsó réteggel, ismét a megfelelő súlyok arányában, és annak

117
00:08:00,240 --> 00:08:01,740
arányában, hogy mennyire van szüksége az egyes neuronoknak. változtatni.

118
00:08:01,740 --> 00:08:05,940
Itt jön a képbe a visszafelé terjedés ötlete.

119
00:08:05,940 --> 00:08:11,080
Ha ezeket a kívánt hatásokat összeadjuk, akkor alapvetően egy listát kapunk

120
00:08:11,080 --> 00:08:14,300
azokról a lökésekről, amelyeket ezzel az utolsó réteggel szeretnénk elérni.

121
00:08:14,300 --> 00:08:18,740
És ha ezek megvannak, rekurzív módon alkalmazhatja ugyanazt a folyamatot a

122
00:08:18,740 --> 00:08:23,400
releváns súlyokra és torzításokra, amelyek meghatározzák ezeket az értékeket, megismételve ugyanazt

123
00:08:23,400 --> 00:08:29,180
a folyamatot, amelyen az imént végigmentem, és visszafelé haladva a hálózaton.

124
00:08:29,180 --> 00:08:33,960
És kicsit tovább kicsinyítve, ne feledje, hogy egyetlen edzési példa

125
00:08:33,960 --> 00:08:37,520
csak így kívánja elmozdítani ezeket a súlyokat és elfogultságokat.

126
00:08:37,520 --> 00:08:41,400
Ha csak arra figyelnénk, hogy mit akar ez a 2, akkor

127
00:08:41,400 --> 00:08:44,140
a hálózat végül arra ösztönözne, hogy minden képet 2-esnek minősítsen.

128
00:08:44,140 --> 00:08:49,500
Tehát ugyanazt a backprop rutint kell végrehajtania minden más edzési

129
00:08:49,500 --> 00:08:54,700
példánál, rögzítve, hogy mindegyikük hogyan szeretné megváltoztatni a súlyokat

130
00:08:54,700 --> 00:09:02,300
és a torzításokat, és együtt átlagolja a kívánt változtatásokat.

131
00:09:02,300 --> 00:09:08,260
Az egyes súlyokra és torzításokra vonatkozó átlagolt lökések itt

132
00:09:08,260 --> 00:09:12,340
található gyűjteménye, lazán szólva, az utolsó videóban hivatkozott

133
00:09:12,340 --> 00:09:14,360
költségfüggvény negatív gradiense, vagy legalábbis valami azzal arányos.

134
00:09:14,360 --> 00:09:18,980
Csak azért mondom lazán szólva, mert még nem kell mennyiségileg pontosítani

135
00:09:18,980 --> 00:09:23,480
ezeket a lökéseket, de ha megértetted az imént hivatkozott változtatásokat, miért

136
00:09:23,480 --> 00:09:28,740
nagyobbak egyesek arányosan nagyobbak, mint mások, és hogyan kell ezeket összeadni,

137
00:09:28,740 --> 00:09:34,100
akkor megérted a mechanikát. hogy valójában mit csinál a backpropagation.

138
00:09:34,100 --> 00:09:38,540
Egyébként a gyakorlatban a számítógépeknek rendkívül sok időbe telik,

139
00:09:38,540 --> 00:09:43,120
hogy minden edzéspélda hatását összeadják minden gradiens süllyedési lépésnél.

140
00:09:43,120 --> 00:09:45,540
Tehát itt van, amit általában csinálnak helyette.

141
00:09:45,540 --> 00:09:50,460
Véletlenszerűen összekeveri az edzési adatokat, és felosztja egy csomó

142
00:09:50,460 --> 00:09:53,380
mini kötegre, mondjuk mindegyiknek 100 edzési példája van.

143
00:09:53,380 --> 00:09:56,980
Ezután kiszámít egy lépést a mini-köteg szerint.

144
00:09:56,980 --> 00:10:00,840
Ez nem a költségfüggvény tényleges gradiense, amely az összes betanítási

145
00:10:00,840 --> 00:10:06,260
adattól függ, nem ettől az apró részhalmaztól, tehát nem ez

146
00:10:06,260 --> 00:10:10,900
a leghatékonyabb lépés lefelé, de minden mini köteg elég jó

147
00:10:10,900 --> 00:10:12,900
közelítést ad, és ami még fontosabb. jelentős számítási sebességet biztosít.

148
00:10:12,900 --> 00:10:16,900
Ha a megfelelő költségfelület alatt ábrázolná a hálózatának pályáját, az egy kicsit inkább

149
00:10:16,900 --> 00:10:22,020
olyan lenne, mint egy részeg ember, aki céltalanul botorkál le a dombról, de

150
00:10:22,020 --> 00:10:26,880
gyors lépéseket tesz, nem pedig egy gondosan számító ember, aki meghatározza minden lépés

151
00:10:26,880 --> 00:10:31,620
pontos lefelé irányát. mielőtt nagyon lassú és óvatos lépést tenne abba az irányba.

152
00:10:31,620 --> 00:10:35,200
Ezt a technikát sztochasztikus gradiens süllyedésnek nevezik.

153
00:10:35,200 --> 00:10:40,400
Sok minden történik itt, úgyhogy foglaljuk össze magunknak, jó?

154
00:10:40,400 --> 00:10:45,480
A visszapropagálás az az algoritmus, amely meghatározza, hogy egy edzési példa

155
00:10:45,480 --> 00:10:50,040
hogyan kívánja eltolni a súlyokat és torzításokat, nemcsak abból a szempontból,

156
00:10:50,040 --> 00:10:54,780
hogy felfelé vagy lefelé kell-e menni, hanem abból a szempontból, hogy

157
00:10:54,780 --> 00:10:56,240
ezeknek a változásoknak milyen relatív aránya okozza a leggyorsabb csökkenést költség.

158
00:10:56,240 --> 00:11:00,720
Egy igazi gradiens süllyedési lépés azt jelentené, hogy ezt minden tíz

159
00:11:00,720 --> 00:11:05,920
és ezer képzési példánál meg kell tenni, és átlagolni kell

160
00:11:05,920 --> 00:11:11,680
a kívánt változtatásokat, de ez számításilag lassú, ezért ehelyett véletlenszerűen

161
00:11:11,680 --> 00:11:14,000
felosztja az adatokat mini kötegekre, és minden lépést egy mini-tétel.

162
00:11:14,000 --> 00:11:18,600
Az összes mini-kötegelt ismételten végignézve és végrehajtva ezeket a beállításokat, a

163
00:11:18,600 --> 00:11:23,420
költségfüggvény helyi minimuma felé közeledik, ami azt jelenti, hogy a

164
00:11:23,420 --> 00:11:27,540
hálózat végül nagyon jó munkát fog végezni a képzési példákon.

165
00:11:27,540 --> 00:11:32,600
Tehát mindezekkel együtt minden kódsor, amely a backprop megvalósításához felhasználható,

166
00:11:32,600 --> 00:11:37,680
valójában megfelel valaminek, amit most láttál, legalábbis informális értelemben.

167
00:11:37,680 --> 00:11:41,900
De néha csak a fele a sikernek tudnia, hogy mit csinál a matematika,

168
00:11:41,900 --> 00:11:44,780
és csak az átkozott dolgot képviselni az, ahol minden zavarossá és zavarossá válik.

169
00:11:44,780 --> 00:11:49,360
Tehát azok számára, akik szeretnének mélyebbre menni, a következő videó ugyanazokat

170
00:11:49,360 --> 00:11:53,400
a gondolatokat járja át, amelyeket itt bemutattunk, de a mögöttes kalkuláció

171
00:11:53,400 --> 00:11:57,460
tekintetében, ami remélhetőleg egy kicsit ismerősebbé teszi a témát egyéb források.

172
00:11:57,460 --> 00:12:01,220
Előtte érdemes hangsúlyozni, hogy ahhoz, hogy ez az algoritmus

173
00:12:01,220 --> 00:12:05,840
működjön, és ez a neurális hálózatokon kívül mindenféle gépi

174
00:12:05,840 --> 00:12:06,840
tanulásra is vonatkozik, sok betanítási adatra van szükség.

175
00:12:06,840 --> 00:12:10,740
A mi esetünkben az egyik dolog, ami a kézzel írt számjegyeket ilyen szép példává teszi,

176
00:12:10,740 --> 00:12:15,380
az az, hogy létezik az MNIST adatbázis, rengeteg olyan példával, amelyeket emberek címkéztek fel.

177
00:12:15,380 --> 00:12:19,000
Tehát a gépi tanulásban dolgozók számára ismert gyakori kihívás az, hogy

178
00:12:19,040 --> 00:12:22,880
megkapja a ténylegesen szükséges címkézett képzési adatokat, legyen szó akár több

179
00:12:22,880 --> 00:12:27,400
tízezer kép felcímkézéséről, vagy bármilyen más adattípusról, amellyel esetleg foglalkoznia kell.

