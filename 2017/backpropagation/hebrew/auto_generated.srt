1
00:00:00,000 --> 00:00:09,640
כאן אנו מתמודדים עם התפשטות לאחור, האלגוריתם המרכזי מאחורי האופן שבו רשתות עצביות לומדות.

2
00:00:09,640 --> 00:00:13,577
לאחר סיכום מהיר של המקום בו אנו נמצאים, הדבר הראשון שאעשה הוא הדרכה

3
00:00:13,577 --> 00:00:17,400
אינטואיטיבית למה שהאלגוריתם עושה בפועל, ללא כל התייחסות לנוסחאות.

4
00:00:17,400 --> 00:00:24,040
לאחר מכן, לאלו מכם שכן רוצים לצלול לתוך המתמטיקה, הסרטון הבא נכנס לחישוב שבבסיס כל זה.

5
00:00:24,040 --> 00:00:28,127
אם צפית בשני הסרטונים האחרונים, או אם אתה רק קופץ פנימה עם הרקע המתאים,

6
00:00:28,127 --> 00:00:31,080
אתה יודע מהי רשת עצבית וכיצד היא מזרימה מידע קדימה.

7
00:00:31,080 --> 00:00:37,374
כאן, אנחנו עושים את הדוגמה הקלאסית של זיהוי ספרות בכתב יד שערכי הפיקסלים שלהן מוזנים

8
00:00:37,374 --> 00:00:43,521
לשכבה הראשונה של הרשת עם 784 נוירונים, ואני הצגתי רשת עם שתי שכבות נסתרות עם רק 16

9
00:00:43,521 --> 00:00:49,520
נוירונים כל אחת, ופלט שכבה של 10 נוירונים, המציינת באיזו ספרה הרשת בוחרת כתשובה.

10
00:00:49,520 --> 00:00:54,609
אני גם מצפה שתבינו את הירידה בשיפוע, כפי שתואר בסרטון האחרון,

11
00:00:54,609 --> 00:01:01,997
וכיצד כוונתנו בלמידה היא שאנו רוצים למצוא אילו משקלים והטיות ממזערים פונקציית עלות מסוימת.

12
00:01:01,997 --> 00:01:02,080


13
00:01:02,080 --> 00:01:08,820
כתזכורת מהירה, בעלות של דוגמה לאימון בודד, אתה לוקח את הפלט שהרשת נותנת,

14
00:01:08,820 --> 00:01:15,560
יחד עם הפלט שרצית שהיא תיתן, ומסכמים את הריבועים של ההבדלים בין כל רכיב.

15
00:01:15,560 --> 00:01:20,400
אם תעשה זאת עבור כל עשרות אלפי דוגמאות האימון שלך וממוצע התוצאות,

16
00:01:20,400 --> 00:01:23,040
זה נותן לך את העלות הכוללת של הרשת.

17
00:01:23,040 --> 00:01:28,390
כאילו זה לא מספיק כדי לחשוב עליו, כפי שתואר בסרטון האחרון,

18
00:01:28,390 --> 00:01:33,649
הדבר שאנו מחפשים הוא השיפוע השלילי של פונקציית העלות הזו,

19
00:01:33,649 --> 00:01:39,452
שאומר לך כיצד עליך לשנות את כל המשקלים וההטיות, כל חיבורים אלה,

20
00:01:39,452 --> 00:01:43,080
כדי להפחית את העלות בצורה היעילה ביותר.

21
00:01:43,080 --> 00:01:49,600
התפשטות לאחור, הנושא של הסרטון הזה, הוא אלגוריתם לחישוב השיפוע המסובך והמטורף הזה.

22
00:01:49,600 --> 00:01:54,679
הרעיון האחד מהסרטון האחרון שאני באמת רוצה שתחזיקו בחוזקה בראשכם עכשיו

23
00:01:54,679 --> 00:01:59,976
הוא שמכיוון שחשיבה על וקטור הגרדיאנט ככיוון ב-13,000 ממדים היא, בקלילות,

24
00:01:59,976 --> 00:02:04,620
מעבר לטווח הדמיון שלנו, יש עוד רעיון איך שאתה יכול לחשוב על זה.

25
00:02:04,620 --> 00:02:11,820
הגודל של כל רכיב כאן אומר לך עד כמה פונקציית העלות רגישה לכל משקל והטיה.

26
00:02:11,820 --> 00:02:18,734
לדוגמה, נניח שאתה עובר את התהליך שאני עומד לתאר, ומחשב את הגרדיאנט השלילי,

27
00:02:18,734 --> 00:02:26,940
והרכיב המשויך למשקל בקצה הזה כאן יוצא כ-3.2, בעוד שהרכיב המשויך לקצה הזה כאן יוצא כ-0.1.

28
00:02:26,940 --> 00:02:34,272
הדרך שבה תפרשו את זה היא שהעלות של הפונקציה רגישה פי 32 לשינויים במשקל הראשון הזה,

29
00:02:34,272 --> 00:02:39,926
אז אם הייתם מתנועעים קצת בערך הזה, זה יגרום לשינוי מסוים בעלות,

30
00:02:39,926 --> 00:02:45,580
ולשינוי הזה. גדול פי 32 ממה שאותו התנודדות למשקל השני היה נותן.

31
00:02:45,580 --> 00:02:50,435
באופן אישי, כשלמדתי לראשונה על התפשטות לאחור, אני חושב

32
00:02:50,435 --> 00:02:55,820
שההיבט המבלבל ביותר היה רק המרדף אחר התווים והאינדקס של הכל.

33
00:02:55,820 --> 00:03:00,141
אבל ברגע שאתה פותח את מה שכל חלק באלגוריתם הזה באמת עושה,

34
00:03:00,141 --> 00:03:04,164
כל אפקט אינדיבידואלי שיש לו הוא למעשה די אינטואיטיבי,

35
00:03:04,164 --> 00:03:07,740
רק שיש הרבה התאמות קטנות שמשתלבות זו על גבי זו.

36
00:03:07,740 --> 00:03:12,285
אז אני אתחיל את העניינים כאן עם התעלמות מוחלטת מהסימונים,

37
00:03:12,285 --> 00:03:17,380
ופשוט יעבור על ההשפעות שיש לכל דוגמה לאימון על המשקולות וההטיות.

38
00:03:17,380 --> 00:03:22,243
מכיוון שפונקציית העלות כוללת ממוצע של עלות מסוימת לכל דוגמה על

39
00:03:22,243 --> 00:03:27,339
פני כל עשרות אלפי דוגמאות האימון, הדרך בה אנו מתאימים את המשקולות

40
00:03:27,339 --> 00:03:31,740
וההטיות לשלב ירידה בשיפוע בודד תלויה גם בכל דוגמה בודדת.

41
00:03:31,740 --> 00:03:35,441
או ליתר דיוק, באופן עקרוני זה צריך, אבל בשביל יעילות חישובית,

42
00:03:35,441 --> 00:03:39,860
אנחנו נעשה טריק קטן מאוחר יותר כדי למנוע ממך להכות בכל דוגמה עבור כל צעד.

43
00:03:39,860 --> 00:03:43,450
במקרים אחרים, כרגע, כל מה שאנחנו הולכים לעשות הוא למקד

44
00:03:43,450 --> 00:03:46,780
את תשומת הלב שלנו בדוגמה אחת בודדת, תמונה זו של 2.

45
00:03:46,780 --> 00:03:51,740
איזו השפעה צריכה להיות לדוגמה האימון האחת הזו על האופן שבו המשקולות וההטיות מתכווננות?

46
00:03:51,740 --> 00:03:55,953
נניח שאנחנו בנקודה שבה הרשת עדיין לא מאומנת היטב,

47
00:03:55,953 --> 00:04:02,780
אז ההפעלה בפלט הולכות להיראות די אקראיות, אולי משהו כמו 0.5, 0.8, 0.2, עוד ועוד.

48
00:04:02,780 --> 00:04:08,630
אנחנו לא יכולים לשנות ישירות את ההפעלות האלה, יש לנו רק השפעה על המשקלים וההטיות,

49
00:04:08,630 --> 00:04:13,340
אבל זה מועיל לעקוב אחר ההתאמות שאנו רוצים שיבוצעו בשכבת הפלט הזו.

50
00:04:13,340 --> 00:04:16,614
ומכיוון שאנחנו רוצים שהוא יסווג את התמונה כ-2,

51
00:04:16,614 --> 00:04:21,700
אנחנו רוצים שהערך השלישי הזה יזוז כלפי מעלה בזמן שכל האחרים יתנוחו למטה.

52
00:04:21,700 --> 00:04:25,699
יתר על כן, הגדלים של הדחפים הללו צריכים להיות

53
00:04:25,699 --> 00:04:30,220
פרופורציונליים למרחק של כל ערך נוכחי מערך היעד שלו.

54
00:04:30,220 --> 00:04:35,910
לדוגמה, העלייה להפעלה של אותו נוירון מספר 2 חשובה במובן מסוים

55
00:04:35,910 --> 00:04:42,060
יותר מהירידה לנוירון מספר 8, שכבר די קרוב למקום בו הוא צריך להיות.

56
00:04:42,060 --> 00:04:47,900
אז בהתקרבות נוספת, בואו נתמקד רק בנוירון האחד הזה, זה שאת ההפעלה שלו אנחנו רוצים להגביר.

57
00:04:47,900 --> 00:04:54,769
זכור, הפעלה מוגדרת כסכום משוקלל מסוים של כל ההפעלות בשכבה הקודמת, בתוספת הטיה,

58
00:04:54,769 --> 00:05:01,900
שהכל מחובר לאחר מכן למשהו כמו פונקציית ה-squishification של הסיגמואידים, או ReLU.

59
00:05:01,900 --> 00:05:08,060
אז יש שלושה אפיקים שונים שיכולים לחבור יחד כדי לעזור להגביר את ההפעלה הזו.

60
00:05:08,060 --> 00:05:12,165
אתה יכול להגדיל את ההטיה, אתה יכול להגדיל את המשקולות,

61
00:05:12,165 --> 00:05:15,300
ואתה יכול לשנות את ההפעלות מהשכבה הקודמת.

62
00:05:15,300 --> 00:05:21,460
התמקדות באופן שבו יש להתאים את המשקולות, שימו לב כיצד למשקולות יש רמות השפעה שונות.

63
00:05:21,460 --> 00:05:26,520
לחיבורים עם הנוירונים הבהירים ביותר מהשכבה הקודמת יש את ההשפעה

64
00:05:26,520 --> 00:05:31,420
הגדולה ביותר שכן משקלים אלה מוכפלים בערכי הפעלה גדולים יותר.

65
00:05:31,420 --> 00:05:35,597
אז אם הייתם מגדילים אחד מהמשקלים האלה, למעשה יש לזה השפעה חזקה

66
00:05:35,597 --> 00:05:41,367
יותר על פונקציית העלות האולטימטיבית מאשר הגדלת משקלם של קשרים עם נוירונים עמומים יותר,

67
00:05:41,367 --> 00:05:44,020
לפחות בכל הנוגע לדוגמא האימון האחת הזו.

68
00:05:44,020 --> 00:05:49,053
זכור, כאשר אנו מדברים על ירידה בשיפוע, לא אכפת לנו רק אם כל רכיב צריך להיות

69
00:05:49,053 --> 00:05:54,020
דחף למעלה או למטה, אכפת לנו אילו מהם נותנים לך הכי הרבה כסף עבור הכסף שלך.

70
00:05:54,020 --> 00:06:01,110
זה, אגב, מזכיר לפחות במידת מה תיאוריה במדעי המוח כיצד לומדים רשתות ביולוגיות של נוירונים,

71
00:06:01,110 --> 00:06:06,940
תיאוריה העברית, המסוכמת לעתים קרובות בביטוי, נוירונים שיורים יחד חוט יחד.

72
00:06:06,940 --> 00:06:12,055
כאן, העליות הגדולות ביותר למשקולות, החיזוק הגדול ביותר של הקשרים,

73
00:06:12,055 --> 00:06:18,100
מתרחשת בין נוירונים שהם הפעילים ביותר לבין אלו שאנו רוצים להפוך לפעילים יותר.

74
00:06:18,100 --> 00:06:21,731
במובן מסוים, הנוירונים שיורים בזמן שהם רואים 2

75
00:06:21,731 --> 00:06:25,440
מקבלים קשר חזק יותר לאלו היורים כשחושבים על זה.

76
00:06:25,440 --> 00:06:30,903
שיהיה ברור, אני לא בעמדה להצהיר בצורה כזו או אחרת לגבי האם רשתות מלאכותיות של

77
00:06:30,903 --> 00:06:36,506
נוירונים מתנהגות משהו כמו מוחות ביולוגיים, והרעיון הזה מתחבר יחד עם כמה כוכביות

78
00:06:36,506 --> 00:06:41,760
משמעותיות, אבל נתפס כמשהו רופף מאוד. אנלוגיה, אני מוצא את זה מעניין לציין.

79
00:06:41,760 --> 00:06:45,525
בכל מקרה, הדרך השלישית שבה נוכל לעזור להגביר את ההפעלה

80
00:06:45,525 --> 00:06:49,360
של הנוירון הזה היא על ידי שינוי כל הפעלות בשכבה הקודמת.

81
00:06:49,360 --> 00:06:55,796
כלומר, אם כל מה שקשור לאותו נוירון ספרה 2 עם משקל חיובי נעשה בהיר יותר,

82
00:06:55,796 --> 00:07:02,680
ואם כל מה שקשור למשקל שלילי נעשה עמום, אז הנוירון הספרה 2 הזה היה פעיל יותר.

83
00:07:02,680 --> 00:07:06,793
ובדומה לשינויים במשקל, אתה הולך לקבל את המרב עבור הכסף שלך על

84
00:07:06,793 --> 00:07:10,840
ידי חיפוש שינויים שהם פרופורציונליים לגודל המשקולות התואמות.

85
00:07:10,840 --> 00:07:15,466
עכשיו כמובן, אנחנו לא יכולים להשפיע ישירות על ההפעלות האלה,

86
00:07:15,466 --> 00:07:18,320
יש לנו רק שליטה על המשקולות וההטיות.

87
00:07:18,320 --> 00:07:23,960
אבל בדיוק כמו בשכבה האחרונה, זה מועיל לרשום מה הם השינויים הרצויים האלה.

88
00:07:23,960 --> 00:07:30,040
אבל זכור, בהרחקת שלב אחד כאן, זה רק מה שנוירון פלט ספרה 2 רוצה.

89
00:07:30,040 --> 00:07:35,962
זכרו, אנחנו גם רוצים שכל שאר הנוירונים בשכבה האחרונה יהפכו פחות פעילים,

90
00:07:35,962 --> 00:07:43,200
ולכל אחד מאותם נוירוני פלט אחרים יש מחשבות משלו לגבי מה צריך לקרות לשכבה השנייה אחרונה.

91
00:07:43,200 --> 00:07:49,542
אז הרצון של נוירון ספרה 2 זה מתווסף יחד עם הרצונות של כל נוירוני

92
00:07:49,542 --> 00:07:56,763
הפלט האחרים למה שיקרה לשכבה השנייה-אחרונה הזו, שוב ביחס למשקלים המתאימים,

93
00:07:56,763 --> 00:08:01,740
ובפרופורציה לכמה כל אחד מאותם נוירונים צריך לשנות.

94
00:08:01,740 --> 00:08:05,940
כאן בדיוק נכנס הרעיון של התפשטות לאחור.

95
00:08:05,940 --> 00:08:10,080
על ידי הוספת כל האפקטים הרצויים הללו, אתה בעצם מקבל

96
00:08:10,080 --> 00:08:14,300
רשימה של דחיפות שאתה רוצה שיקרה לשכבה השנייה אחרונה.

97
00:08:14,300 --> 00:08:22,094
וברגע שיש לך כאלה, אתה יכול להחיל את אותו תהליך רקורסיבי על המשקולות וההטיות הרלוונטיות

98
00:08:22,094 --> 00:08:29,180
שקובעות את הערכים האלה, לחזור על אותו תהליך שעברתי זה עתה ולנוע לאחור דרך הרשת.

99
00:08:29,180 --> 00:08:33,435
ותרחיק קצת יותר, זכרו שזה הכל בדיוק איך דוגמה אחת

100
00:08:33,435 --> 00:08:37,520
לאימון רוצה להניע כל אחד מהמשקלים וההטיות הללו.

101
00:08:37,520 --> 00:08:40,830
אם רק היינו מקשיבים למה שהשניים האלה רוצים, בסופו

102
00:08:40,830 --> 00:08:44,140
של דבר הרשת תתמרץ רק כדי לסווג את כל התמונות כ-2.

103
00:08:44,140 --> 00:08:52,151
אז מה שאתה עושה זה לעבור את אותה שגרת תמיכה בגב עבור כל דוגמה אחרת לאימון,

104
00:08:52,151 --> 00:08:58,347
לרשום כיצד כל אחד מהם היה רוצה לשנות את המשקולות וההטיות,

105
00:08:58,347 --> 00:09:02,300
ולבצע את הממוצע של השינויים הרצויים.

106
00:09:02,300 --> 00:09:07,423
האוסף הזה כאן של הדחפים הממוצעים לכל משקל והטיה הוא, באופן רופף,

107
00:09:07,423 --> 00:09:14,360
השיפוע השלילי של פונקציית העלות שהוזכרה בסרטון האחרון, או לפחות משהו פרופורציונלי אליה.

108
00:09:14,360 --> 00:09:20,273
אני אומר בצורה רופפת רק כי עדיין לא למדתי דיוק כמותי לגבי הדחפים האלה,

109
00:09:20,273 --> 00:09:27,436
אבל אם הבנת כל שינוי שהזכרתי זה עתה, מדוע חלקם גדולים יותר מאחרים באופן פרופורציונלי,

110
00:09:27,436 --> 00:09:34,100
וכיצד צריך להוסיף את כולם יחד, אתה מבין את המכניקה של מה בעצם עושה ההפצה לאחור.

111
00:09:34,100 --> 00:09:38,700
אגב, בפועל, לוקח למחשבים זמן רב במיוחד כדי לחבר את

112
00:09:38,700 --> 00:09:43,120
ההשפעה של כל דוגמה לאימון בכל צעד בירידה בשיפוע.

113
00:09:43,120 --> 00:09:45,540
אז הנה מה שנהוג לעשות במקום.

114
00:09:45,540 --> 00:09:50,790
אתה מערבב באקראי את נתוני האימון שלך ומחלק אותם לחבורה שלמה של מיני-אצט,

115
00:09:50,790 --> 00:09:53,380
נניח שלכל אחד יש 100 דוגמאות אימון.

116
00:09:53,380 --> 00:09:56,980
ואז אתה מחשב שלב לפי המיני-אצט.

117
00:09:56,980 --> 00:10:02,020
זה לא השיפוע האמיתי של פונקציית העלות, שתלוי בכל נתוני האימון,

118
00:10:02,020 --> 00:10:06,660
לא תת-הקבוצה הקטנה הזו, אז זה לא הצעד היעיל ביותר בירידה,

119
00:10:06,660 --> 00:10:12,900
אבל כל מיני-אצט נותן לך קירוב די טוב, וחשוב מכך. נותן לך זירוז חישוב משמעותי.

120
00:10:12,900 --> 00:10:17,426
אם היית מתווה את מסלול הרשת שלך מתחת למשטח העלות הרלוונטי,

121
00:10:17,426 --> 00:10:23,487
זה יהיה קצת יותר כמו אדם שיכור המועד ללא מטרה במורד גבעה אך עושה צעדים מהירים,

122
00:10:23,487 --> 00:10:28,320
במקום אדם מחושב בקפידה שקובע את כיוון הירידה המדויק של כל צעד.

123
00:10:28,320 --> 00:10:31,620
לפני שתעשה צעד איטי וזהיר מאוד בכיוון הזה.

124
00:10:31,620 --> 00:10:35,200
טכניקה זו מכונה ירידה בשיפוע סטוכסטי.

125
00:10:35,200 --> 00:10:40,400
יש פה הרבה דברים, אז בואו נסכם את זה לעצמנו, נכון?

126
00:10:40,400 --> 00:10:45,611
התפשטות לאחור הוא האלגוריתם לקביעת האופן שבו דוגמה אחת לאימון תרצה להניע את

127
00:10:45,611 --> 00:10:50,000
המשקולות וההטיות, לא רק במונחים של האם הם צריכים לעלות או לרדת,

128
00:10:50,000 --> 00:10:56,171
אלא במונחים של מה הפרופורציות היחסיות לשינויים האלה שגורמים לירידה המהירה ביותר ל- עֲלוּת.

129
00:10:56,171 --> 00:10:56,240


130
00:10:56,240 --> 00:11:02,342
שלב ירידה שיפוע אמיתי יכלול ביצוע של כל עשרות ואלפי דוגמאות האימון שלך וממוצע

131
00:11:02,342 --> 00:11:06,958
של השינויים הרצויים שאתה מקבל, אבל זה איטי מבחינה חישובית,

132
00:11:06,958 --> 00:11:14,000
אז במקום זאת אתה מחלק את הנתונים באופן אקראי למיני-אצטות ומחשב כל שלב ביחס ל- מיני אצווה.

133
00:11:14,000 --> 00:11:19,235
אם תעבור שוב ושוב על כל המיני-אצות ותבצע את ההתאמות האלה,

134
00:11:19,235 --> 00:11:26,005
תתכנס למינימום מקומי של פונקציית העלות, כלומר הרשת שלך תעשה עבודה ממש טובה

135
00:11:26,005 --> 00:11:27,540
בדוגמאות ההדרכה.

136
00:11:27,540 --> 00:11:35,168
אז עם כל זה, כל שורת קוד שתיכנס ליישום backprop למעשה מתכתבת עם משהו שראית עכשיו,

137
00:11:35,168 --> 00:11:37,680
לפחות במונחים לא פורמליים.

138
00:11:37,680 --> 00:11:40,878
אבל לפעמים לדעת מה המתמטיקה עושה זה רק חצי מהקרב,

139
00:11:40,878 --> 00:11:44,780
ורק מייצג את הדבר הארור הוא המקום שבו זה נהיה מבולבל ומבלבל.

140
00:11:44,780 --> 00:11:50,223
אז, לאלו מכם שכן רוצים להעמיק, הסרטון הבא עובר על אותם רעיונות שהוצגו כאן זה עתה,

141
00:11:50,223 --> 00:11:54,472
אבל במונחים של החשבון הבסיסי, מה שיש לקוות לעשות את זה קצת יותר

142
00:11:54,472 --> 00:11:57,460
מוכר כפי שאתם רואים את הנושא ב משאבים אחרים.

143
00:11:57,460 --> 00:12:01,433
לפני כן, דבר אחד שכדאי להדגיש הוא שכדי שהאלגוריתם הזה יעבוד,

144
00:12:01,433 --> 00:12:06,840
וזה מתאים לכל מיני למידת מכונה מעבר לרשתות עצביות בלבד, אתה צריך הרבה נתוני אימון.

145
00:12:06,840 --> 00:12:11,172
במקרה שלנו, דבר אחד שהופך ספרות בכתב יד לדוגמא כל כך נחמדה הוא שקיים

146
00:12:11,172 --> 00:12:15,380
מסד הנתונים של MNIST, עם כל כך הרבה דוגמאות שסומנו על ידי בני אדם.

147
00:12:15,380 --> 00:12:19,448
אז אתגר נפוץ שאלו מכם שעובדים בלמידת מכונה יכירו הוא פשוט לקבל את

148
00:12:19,448 --> 00:12:24,996
נתוני ההדרכה המסומנים להם אתם באמת צריכים, בין אם זה לגרום לאנשים לסמן עשרות אלפי תמונות,

149
00:12:24,996 --> 00:12:27,400
או כל סוג אחר שעמו אתם עשויים להתמודד.

