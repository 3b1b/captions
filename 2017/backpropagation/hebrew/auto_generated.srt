1
00:00:04,059 --> 00:00:06,425
כאן אנו מתמודדים עם התפשטות לאחור (Backpropagation), 

2
00:00:06,425 --> 00:00:08,880
האלגוריתם המרכזי מאחורי האופן שבו רשתות עצביות לומדות. 

3
00:00:09,400 --> 00:00:13,256
לאחר סיכום מהיר של המקום בו אנו נמצאים, הדבר הראשון שאעשה הוא הדרכה 

4
00:00:13,256 --> 00:00:17,000
אינטואיטיבית למה שהאלגוריתם עושה בפועל, ללא כל התייחסות לנוסחאות. 

5
00:00:17,660 --> 00:00:23,020
לאחר מכן, לאלו מכם שכן רוצים לצלול לתוך המתמטיקה, הסרטון הבא נכנס לחישובים שבבסיס כל זה. 

6
00:00:23,820 --> 00:00:27,641
אם צפיתם בשני הסרטונים האחרונים, או אם אתם מגיעים עם הרקע המתאים, 

7
00:00:27,641 --> 00:00:31,000
אתם יודעים מהי רשת נוירוננים וכיצד היא מזרימה מידע קדימה. 

8
00:00:31,680 --> 00:00:37,560
כאן, אנחנו משתמשים בדוגמה הקלאסית של זיהוי ספרות בכתב יד שערכי הפיקסלים שלהן מוזנים 

9
00:00:37,560 --> 00:00:43,370
לשכבה הראשונה של הרשת עם 784 נוירונים, ואני הצגתי רשת עם שתי שכבות נסתרות עם רק 16 

10
00:00:43,370 --> 00:00:49,040
נוירונים כל אחת, ושכבת פלט של 10 נוירונים, המציינת באיזו ספרה הרשת בוחרת כתשובה. 

11
00:00:50,040 --> 00:00:54,655
אני גם מצפה שתבינו את הירידה בגרדיאנט, כפי שתוארה בסרטון האחרון, 

12
00:00:54,655 --> 00:01:00,336
וכיצד כוונתנו בלמידה היא שאנחנו רוצים למצוא אילו משקלים והטיות ממזערים פונקציית 

13
00:01:00,336 --> 00:01:01,260
עלות מסוימת. 

14
00:01:02,040 --> 00:01:08,564
כתזכורת מהירה, בשביל עלות של דוגמה לאימון בודד, אתם לוקחים את הפלט שהרשת נותנת, 

15
00:01:08,564 --> 00:01:14,600
יחד עם הפלט שרציתם שהיא תיתן, ומסכמים את הריבועים של ההבדלים בין כל רכיב. 

16
00:01:15,380 --> 00:01:20,106
אם תעשו זאת עבור כל עשרות אלפי דוגמאות האימון שלכם ותמצעו את התוצאות, 

17
00:01:20,106 --> 00:01:22,200
תקבלו את העלות הכוללת של הרשת. 

18
00:01:22,200 --> 00:01:26,389
כאילו זה לא מספיק כדי לחשוב עליו, כפי שתואר בסרטון האחרון, 

19
00:01:26,389 --> 00:01:30,650
הדבר שאנו מחפשים הוא הגרדיאנט השלילי של פונקציית העלות הזו, 

20
00:01:30,650 --> 00:01:35,479
שאומרת לכם כיצד עליכם לשנות את כל המשקלים וההטיות, כל החיבורים אלה, 

21
00:01:35,479 --> 00:01:38,320
כדי להפחית את העלות בצורה היעילה ביותר. 

22
00:01:43,260 --> 00:01:48,580
התפשטות לאחור, הנושא של הסרטון הזה, הוא אלגוריתם לחישוב הגרדיאנט המסובך והמטורף הזה. 

23
00:01:49,140 --> 00:01:54,080
הרעיון האחד מהסרטון האחרון שאני באמת רוצה שתזכרו טוב הוא שמכיוון 

24
00:01:54,080 --> 00:01:58,716
שחשיבה על וקטור הגרדיאנט ככיוון ב-13,000 ממדים היא, בקלילות, 

25
00:01:58,716 --> 00:02:03,580
מעבר לטווח הדמיון שלנו, יש עוד דרך איך שאתם יכולים לחשוב על זה. 

26
00:02:04,600 --> 00:02:10,940
הגודל של כל רכיב כאן אומר לכם עד כמה פונקציית העלות רגישה לכל משקל והטיה. 

27
00:02:11,800 --> 00:02:18,765
לדוגמה, נניח שאתם עוברים את התהליך שאני עומד לתאר, ומחשבים את הגרדיאנט השלילי, 

28
00:02:18,765 --> 00:02:26,260
והרכיב המשויך למשקל בקצה הזה כאן יוצא 3.2, בעוד שהרכיב המשויך לקצה הזה כאן יוצא 0.1. 

29
00:02:26,820 --> 00:02:33,078
הדרך שבה תפרשו את זה היא שהעלות של הפונקציה רגישה פי 32 לשינויים במשקל הראשון, 

30
00:02:33,078 --> 00:02:37,910
אז אם הייתם משנים קצת הערך שלו, זה יגרום לשינוי מסוים בעלות, 

31
00:02:37,910 --> 00:02:43,060
והשינוי הזה גדול פי 32 ממה שאותו שינוי יחסי במשקל השני היה נותן. 

32
00:02:48,420 --> 00:02:54,508
באופן אישי, כשלמדתי לראשונה על התפשטות לאחור, אני חושב שההיבט המבלבל ביותר היה הסימונים, 

33
00:02:54,508 --> 00:02:55,740
בעיקר של האידקסים.

34
00:02:56,220 --> 00:02:59,859
אבל ברגע שאתם מבינים מה כל חלק באלגוריתם הזה עושה, 

35
00:02:59,859 --> 00:03:03,214
כל אפקט פרטני שיש לו הוא למעשה די אינטואיטיבי, 

36
00:03:03,214 --> 00:03:06,640
רק שיש הרבה התאמות קטנות שמשתלבות זו על גבי זו. 

37
00:03:07,740 --> 00:03:11,723
אז אני אתחיל את העניינים כאן עם התעלמות מוחלטת מהסימונים, 

38
00:03:11,723 --> 00:03:16,120
ופשוט אעבור על ההשפעות שיש לכל דוגמה לאימון על המשקלים וההטיות. 

39
00:03:17,020 --> 00:03:21,593
מכיוון שפונקציית העלות כוללת ממוצע של העלות לכל דוגמה על פני 

40
00:03:21,593 --> 00:03:26,316
כל עשרות אלפי דוגמאות האימון, הדרך בה אנחנו מתאימים את המשקלים 

41
00:03:26,316 --> 00:03:31,040
וההטיות לשלב בודד של הירידה בגרדיאנט תלויה גם בכל דוגמה בודדת. 

42
00:03:31,680 --> 00:03:35,690
או ליתר דיוק, באופן עקרוני זה מה שאמור להיות, אבל בשביל יעילות חישובית, 

43
00:03:35,690 --> 00:03:39,200
נבצע טריק קטן מאוחר יותר כדי למנוע חזרה לכל דוגמה עבור כל צעד. 

44
00:03:39,200 --> 00:03:44,857
כרגע, כל מה שאנחנו הולכים לעשות הוא למקד את תשומת הלב שלנו בדוגמה אחת בודדת, 

45
00:03:44,857 --> 00:03:45,960
תמונה זו של 2. 

46
00:03:46,720 --> 00:03:51,480
איזו השפעה צריכה להיות לדוגמת האימון האחת הזו על כיוונון המשקלים וההטיות? 

47
00:03:52,680 --> 00:03:56,210
נניח שאנחנו בנקודה שבה הרשת עדיין לא מאומנת היטב, 

48
00:03:56,210 --> 00:04:02,000
אז ההפעלות בפלט הולכות להיראות די אקראיות, אולי משהו כמו 0.5, 0.8, 0.2, עוד ועוד. 

49
00:04:02,520 --> 00:04:08,019
אנחנו לא יכולים לשנות ישירות את ההפעלות האלה, יש לנו רק השפעה על המשקלים וההטיות, 

50
00:04:08,019 --> 00:04:12,580
אבל זה מועיל לעקוב אחר ההתאמות שאנחנו רוצים שיבוצעו בשכבת הפלט הזו. 

51
00:04:13,360 --> 00:04:16,588
ומכיוון שאנחנו רוצים שהיא תסווג את התמונה כ-2, 

52
00:04:16,588 --> 00:04:21,260
אנחנו רוצים שהערך השלישי הזה ישתנה כלפי מעלה בזמן שכל האחרים יקטנו. 

53
00:04:22,060 --> 00:04:25,640
יתר על כן, הגדלים של השינויים הללו צריכים להיות 

54
00:04:25,640 --> 00:04:29,520
פרופורציונליים למרחק של כל ערך נוכחי מערך היעד שלו. 

55
00:04:30,220 --> 00:04:35,682
לדוגמה, הגידול בהפעלה של אותו נוירון מספר 2 חשובה במובן מסוים יותר 

56
00:04:35,682 --> 00:04:40,900
מהירידה של נוירון מספר 8, שכבר די קרוב למקום בו הוא צריך להיות. 

57
00:04:42,040 --> 00:04:47,280
אז בהגדלה נוספת, בואו נתמקד רק בנוירון האחד הזה, זה שאת ההפעלה שלו אנחנו רוצים להגביר. 

58
00:04:48,180 --> 00:04:54,659
זכרו, הפעלה מוגדרת כסכום משוקלל מסוים של כל ההפעלות בשכבה הקודמת, 

59
00:04:54,659 --> 00:05:01,040
בתוספת הטיה, ואז הכל עובר למשהו כמו פונקציית הסיגמואיד, או ReLU. 

60
00:05:01,640 --> 00:05:07,020
אז יש שלוש דרכים שונות שיכולות לחבור יחד כדי לעזור להגביר את ההפעלה הזו. 

61
00:05:07,440 --> 00:05:11,268
אתם יכולים להגדיל את ההטיה, אתם יכולים להגדיל את המשקלים, 

62
00:05:11,268 --> 00:05:14,040
ואתם יכול לשנות את ההפעלות מהשכבה הקודמת. 

63
00:05:14,940 --> 00:05:20,860
בהתמקדות באופן שבו יש להתאים את המשקלים, שימו לב כיצד למשקלים יש רמות השפעה שונות. 

64
00:05:21,440 --> 00:05:25,331
לחיבורים עם הנוירונים הבהירים ביותר מהשכבה הקודמת יש את ההשפעה 

65
00:05:25,331 --> 00:05:29,100
הגדולה ביותר שכן משקלים אלה מוכפלים בערכי הפעלה גדולים יותר. 

66
00:05:31,460 --> 00:05:35,553
אז אם הייתם מגדילים אחד מהמשקלים האלה, למעשה יש לזה השפעה חזקה 

67
00:05:35,553 --> 00:05:41,205
יותר על פונקציית העלות האולטימטיבית מאשר הגדלת משקלם של קשרים עם נוירונים עמומים יותר, 

68
00:05:41,205 --> 00:05:43,480
לפחות בכל הנוגע לדוגמת האימון הזו. 

69
00:05:44,420 --> 00:05:50,374
זכרו, כאשר אנחנו מדברים על ירידה בגרדיאנט, לא אכפת לנו רק אם כל רכיב צריך לגדול או לקטון, 

70
00:05:50,374 --> 00:05:53,220
אכפת לנו אילו מהם נותנים לכם הכי הרבה ערך. 

71
00:05:55,020 --> 00:06:01,076
זה, אגב, מזכיר לפחות במידת מה תיאוריה במדעי המוח כיצד לומדות רשתות ביולוגיות של נוירונים, 

72
00:06:01,076 --> 00:06:06,460
תיאוריה Hebbian, המסוכמת לעתים קרובות בביטוי "נוירונים שיורים יחד מחווטים יחד". 

73
00:06:07,260 --> 00:06:11,564
כאן, הגידול הרב ביותר למשקלים, החיזוק הגדול ביותר של הקשרים, 

74
00:06:11,564 --> 00:06:17,280
מתרחשים בין נוירונים שהם הפעילים ביותר לבין אלו שאנחנו רוצים להפוך לפעילים יותר. 

75
00:06:17,940 --> 00:06:21,210
במובן מסוים, הנוירונים שיורים בזמן שהם רואים 2 

76
00:06:21,210 --> 00:06:24,480
מקבלים קשר חזק יותר לאלו היורים כשחושבים על 2. 

77
00:06:25,400 --> 00:06:30,483
שיהיה ברור, אני לא בעמדה להצהיר שרשתות מלאכותיות של נוירונים מתנהגות 

78
00:06:30,483 --> 00:06:36,967
בדומה למוחות ביולוגיים, והרעיון של "יורים יחד מחוברים יחד" בא עם כמה כוכביות משמעותיות, 

79
00:06:36,967 --> 00:06:41,020
אבל כאנלוגיה רופפת מאוד, אני מוצא שמעניין לציין את זה. 

80
00:06:41,940 --> 00:06:45,426
בכל מקרה, הדרך השלישית שבה נוכל לעזור להגביר את ההפעלה 

81
00:06:45,426 --> 00:06:49,040
של הנוירון הזה היא על ידי שינוי כל ההפעלות בשכבה הקודמת. 

82
00:06:49,040 --> 00:06:54,664
כלומר, אם כל מה שקשור לאותו נוירון-ספרה-2 עם משקל חיובי נעשה בהיר יותר, 

83
00:06:54,664 --> 00:07:00,680
ואם כל מה שקשור למשקל שלילי נעשה עמום, אז הנוירון -ספרה-2 הזה היה פעיל יותר. 

84
00:07:02,540 --> 00:07:06,310
ובדומה לשינויים במשקל, אתם הולכים לקבל את הערך המירבי על 

85
00:07:06,310 --> 00:07:10,280
ידי חיפוש שינויים שהם פרופורציונליים לגודל המשקלים התואמים. 

86
00:07:12,140 --> 00:07:15,498
עכשיו, כמובן, אנחנו לא יכולים להשפיע ישירות על ההפעלות האלו, 

87
00:07:15,498 --> 00:07:17,480
יש לנו רק שליטה על המשקלים וההטיות. 

88
00:07:17,480 --> 00:07:24,120
אבל בדיוק כמו בשכבה האחרונה, זה מועיל לרשום מה הם השינויים הרצויים. 

89
00:07:24,580 --> 00:07:29,200
אבל זכרו, במבט רחב יותר, זה רק מה שנוירון-ספרה-2 רוצה. 

90
00:07:29,760 --> 00:07:34,215
זכרו, אנחנו גם רוצים שכל שאר הנוירונים בשכבה האחרונה יהפכו פחות פעילים, 

91
00:07:34,215 --> 00:07:39,600
ולכל אחד מאותם נוירוני פלט אחרים יש מחשבות משלו לגבי מה צריך לקרות לשכבה הלפני אחרונה. 

92
00:07:42,700 --> 00:07:48,706
אז הרצון של נוירון-ספרה-2 מתווסף יחד עם הרצונות של כל נוירוני 

93
00:07:48,706 --> 00:07:55,779
הפלט האחרים למה שיקרה לשכבה הלפני אחרונה הזו, שוב ביחס למשקלים המתאימים, 

94
00:07:55,779 --> 00:08:00,720
ובפרופורציה לכמה כל אחד מאותם נוירונים צריך לשנות. 

95
00:08:01,600 --> 00:08:05,480
כאן בדיוק נכנס הרעיון של התפשטות לאחור. 

96
00:08:05,820 --> 00:08:09,555
על ידי חיבור כל האפקטים הרצויים הללו, אתם בעצם מקבלים 

97
00:08:09,555 --> 00:08:13,360
רשימה של שינויים שאתם רוצים שיקרו לשכבה הלפני האחרונה. 

98
00:08:14,220 --> 00:08:19,980
וברגע שיש לכם אותם, אתם יכולים להחיל את אותו תהליך רקורסיבי על המשקלים וההטיות הרלוונטיות 

99
00:08:19,980 --> 00:08:25,100
שקובעות את הערכים האלה, לחזור על אותו תהליך שעברתי זה עתה ולנוע לאחור דרך הרשת. 

100
00:08:28,960 --> 00:08:33,140
ובמבט כללי יותר, זכרו שכל זה הוא רק איך דוגמת אימון 

101
00:08:33,140 --> 00:08:37,000
אחת רוצה לשנות את כל אחד מהמשקלים וההטיות הללו. 

102
00:08:37,480 --> 00:08:40,319
אם רק היינו מקשיבים למה שה-2 הזה רצה, בסופו של 

103
00:08:40,319 --> 00:08:43,220
דבר הרשת תתומרץ רק כדי לסווג את כל התמונות כ-2. 

104
00:08:44,059 --> 00:08:49,236
אז מה שאתם עושים זה לעבור אותו תהליך של התפשטות לאחור עבור כל דוגמת אימון, 

105
00:08:49,236 --> 00:08:53,308
רושמים כיצד כל אחת מהן היתה רוצה לשנות את המשקלים וההטיות, 

106
00:08:53,308 --> 00:08:56,000
ומחשבים את הממוצע של השינויים הרצויים. 

107
00:09:01,720 --> 00:09:06,823
האוסף הזה כאן של השינויים הממוצעים לכל משקל והטיה הוא, באופן רופף, 

108
00:09:06,823 --> 00:09:13,680
הגרדיאנט השלילי של פונקציית העלות שהוזכרה בסרטון האחרון, או לפחות משהו פרופורציונלי אליו. 

109
00:09:14,380 --> 00:09:19,851
אני אומר בצורה רופפת רק כי עדיין אני צריך להגיע לדיוק כמותי לגבי השינויים האלה, 

110
00:09:19,851 --> 00:09:25,323
אבל אם הבנתם כל שינוי שהזכרתי, מדוע חלקם גדולים יותר מאחרים באופן פרופורציונלי, 

111
00:09:25,323 --> 00:09:31,000
וכיצד צריך לחבר את כולם יחד, אתם מבינים את המכניקה של מה בעצם עושה ההתפשטות לאחור. 

112
00:09:33,960 --> 00:09:38,241
אגב, בפועל, לוקח למחשבים זמן רב במיוחד כדי לחבר את 

113
00:09:38,241 --> 00:09:42,440
ההשפעה של כל דוגמת אימון בכל צעד בירידה בגרדיאנט. 

114
00:09:43,140 --> 00:09:44,820
אז הנה מה שנהוג לעשות במקום. 

115
00:09:45,480 --> 00:09:50,266
אתם מערבבים באקראי את נתוני האימון שלכם ומחלקים אותם לחבורה שלמה של מיני-קבוצה, 

116
00:09:50,266 --> 00:09:52,420
נניח שלכל אחת יש 100 דוגמאות אימון. 

117
00:09:52,940 --> 00:09:56,200
ואז אתם מחשבים צעד לפי המיני-קבוצה. 

118
00:09:56,960 --> 00:10:01,674
זה לא הגרדיאנט האמיתי של פונקציית העלות, שתלוי בכל נתוני האימון, 

119
00:10:01,674 --> 00:10:06,172
לא רק בתת-הקבוצה הקטנה הזו, אז זה לא הצעד היעיל ביותר בירידה, 

120
00:10:06,172 --> 00:10:12,120
אבל כל מיני-קבוצה נותן לכם קירוב די טוב, וחשוב מכך, מזרז את החישוב באופן משמעותי. 

121
00:10:12,820 --> 00:10:17,172
אם הייתם מתווים את מסלול הרשת שלכם מתחת למשטח העלות הרלוונטי, 

122
00:10:17,172 --> 00:10:22,718
זה יהיה קצת יותר כמו אדם שיכור המועד ללא מטרה במורד גבעה אך עושה צעדים מהירים, 

123
00:10:22,718 --> 00:10:27,141
במקום אדם מחושב בקפידה שקובע את כיוון הירידה המדויק של כל צעד, 

124
00:10:27,141 --> 00:10:30,160
לפני שיעשה צעד איטי וזהיר מאוד בכיוון הזה. 

125
00:10:31,540 --> 00:10:34,660
טכניקה זו מכונה ירידה סטוכסטית בגרדיאנט (stochastic gradient descent). 

126
00:10:35,960 --> 00:10:39,620
יש פה הרבה דברים, אז בואו נסכם את זה לעצמנו, בסדר? 

127
00:10:40,440 --> 00:10:45,480
התפשטות לאחור היא האלגוריתם לקביעת האופן שבו דוגמה אחת לאימון תרצה לשנות את 

128
00:10:45,480 --> 00:10:49,657
המשקלים וההטיות, לא רק במונחים של האם הם צריכים לעלות או לרדת, 

129
00:10:49,657 --> 00:10:55,560
אלא במונחים של מה הפרופורציות היחסיות לשינויים האלה שגורמים לירידה המהירה ביותר בעֲלוּת. 

130
00:10:56,260 --> 00:11:01,705
צעד אמיתי בירידה בגרדיאנט יכלול ביצוע של כל עשרות ואלפי דוגמאות האימון שלכם 

131
00:11:01,705 --> 00:11:06,576
ומיצוע של השינויים הרצויים שאתם מקבלים, אבל זה איטי מבחינה חישובית, 

132
00:11:06,576 --> 00:11:12,308
אז במקום זאת אתם מחלקים את הנתונים באופן אקראי למיני-קבוצות ומחשבים כל שלב ביחס 

133
00:11:12,308 --> 00:11:13,240
למיני-קבוצה. 

134
00:11:14,000 --> 00:11:18,586
אם תעבורו שוב ושוב על כל המיני-קבוצות ותבצעו את ההתאמות האלה, 

135
00:11:18,586 --> 00:11:24,282
תתכנסו למינימום מקומי של פונקציית העלות, כלומר הרשת שלכם תעשה עבודה ממש טובה 

136
00:11:24,282 --> 00:11:25,540
בדוגמאות ההדרכה. 

137
00:11:27,240 --> 00:11:34,494
אז עם כל זה, כל שורת קוד שתיכנס ליישום התפשטות לאחור למעשה מתכתבת עם משהו שראיתם עכשיו, 

138
00:11:34,494 --> 00:11:36,720
לפחות במונחים לא פורמליים. 

139
00:11:37,560 --> 00:11:40,569
אבל לפעמים לדעת מה המתמטיקה עושה זה רק חצי מהקרב, 

140
00:11:40,569 --> 00:11:44,120
ורק לייצג את הדבר הזה הוא המקום שבו זה נהיה מבולבל ומבלבל. 

141
00:11:44,860 --> 00:11:50,051
אז, לאלו מכם שכן רוצים להעמיק, הסרטון הבא עובר על אותם רעיונות שהוצגו כאן, 

142
00:11:50,051 --> 00:11:54,689
אבל במונחים של החשבון הבסיסי, שיש לקוות שיעשה את זה קצת יותר מוכר, 

143
00:11:54,689 --> 00:11:56,420
כפי שמוצג במקומות אחרים. 

144
00:11:57,340 --> 00:12:00,916
לפני כן, דבר אחד שכדאי להדגיש הוא שכדי שהאלגוריתם הזה יעבוד, 

145
00:12:00,916 --> 00:12:05,900
וזה מתאים לכל מיני למידות מכונה ולא רק לרשתות נוירונים, אתם צריכים הרבה נתוני אימון. 

146
00:12:06,420 --> 00:12:11,966
במקרה שלנו, דבר אחד שהופך ספרות בכתב יד לדוגמא כל כך טובה הוא שקיים מסד הנתונים של MNIST, 

147
00:12:11,966 --> 00:12:14,740
עם כל כך הרבה דוגמאות שסומנו על ידי בני אדם. 

148
00:12:15,300 --> 00:12:19,151
אז אתגר נפוץ שאלו מכם שעובדים בלמידת מכונה יכירו הוא פשוט לקבל 

149
00:12:19,151 --> 00:12:24,654
את נתוני ההדרכה המסומנים שאתם באמת צריכים, בין אם זה לגרום לאנשים לסמן עשרות אלפי תמונות, 

150
00:12:24,654 --> 00:12:27,100
או כל סוג אחר שאיתו אתם עשויים להתמודד. 

