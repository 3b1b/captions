1
00:00:04,060 --> 00:00:08,880
כאן אנו מתמודדים עם התפשטות לאחור, האלגוריתם המרכזי מאחורי האופן שבו רשתות עצביות לומדות. 

2
00:00:09,400 --> 00:00:13,256
לאחר סיכום מהיר של המקום בו אנו נמצאים, הדבר הראשון שאעשה הוא הדרכה 

3
00:00:13,256 --> 00:00:17,000
אינטואיטיבית למה שהאלגוריתם עושה בפועל, ללא כל התייחסות לנוסחאות. 

4
00:00:17,660 --> 00:00:23,020
לאחר מכן, לאלו מכם שכן רוצים לצלול לתוך המתמטיקה, הסרטון הבא נכנס לחישוב שבבסיס כל זה. 

5
00:00:23,820 --> 00:00:27,989
אם צפית בשני הסרטונים האחרונים, או אם אתה רק קופץ פנימה עם הרקע המתאים, 

6
00:00:27,989 --> 00:00:31,000
אתה יודע מהי רשת עצבית וכיצד היא מזרימה מידע קדימה. 

7
00:00:31,680 --> 00:00:37,606
כאן, אנחנו עושים את הדוגמה הקלאסית של זיהוי ספרות בכתב יד שערכי הפיקסלים שלהן מוזנים 

8
00:00:37,606 --> 00:00:43,392
לשכבה הראשונה של הרשת עם 784 נוירונים, ואני הצגתי רשת עם שתי שכבות נסתרות עם רק 16 

9
00:00:43,392 --> 00:00:49,040
נוירונים כל אחת, ופלט שכבה של 10 נוירונים, המציינת באיזו ספרה הרשת בוחרת כתשובה. 

10
00:00:50,040 --> 00:00:54,586
אני גם מצפה שתבינו את הירידה בשיפוע, כפי שתואר בסרטון האחרון, 

11
00:00:54,586 --> 00:01:01,186
וכיצד כוונתנו בלמידה היא שאנו רוצים למצוא אילו משקלים והטיות ממזערים פונקציית עלות מסוימת.

12
00:01:01,186 --> 00:01:01,260
 

13
00:01:02,040 --> 00:01:08,320
כתזכורת מהירה, בעלות של דוגמה לאימון בודד, אתה לוקח את הפלט שהרשת נותנת, 

14
00:01:08,320 --> 00:01:14,600
יחד עם הפלט שרצית שהיא תיתן, ומסכמים את הריבועים של ההבדלים בין כל רכיב. 

15
00:01:15,380 --> 00:01:20,323
אם תעשה זאת עבור כל עשרות אלפי דוגמאות האימון שלך וממוצע התוצאות, 

16
00:01:20,323 --> 00:01:23,020
זה נותן לך את העלות הכוללת של הרשת. 

17
00:01:23,020 --> 00:01:27,104
כאילו זה לא מספיק כדי לחשוב עליו, כפי שתואר בסרטון האחרון, 

18
00:01:27,104 --> 00:01:31,119
הדבר שאנו מחפשים הוא השיפוע השלילי של פונקציית העלות הזו, 

19
00:01:31,119 --> 00:01:35,550
שאומר לך כיצד עליך לשנות את כל המשקלים וההטיות, כל חיבורים אלה, 

20
00:01:35,550 --> 00:01:38,320
כדי להפחית את העלות בצורה היעילה ביותר. 

21
00:01:43,260 --> 00:01:49,580
התפשטות לאחור, הנושא של הסרטון הזה, הוא אלגוריתם לחישוב השיפוע המסובך והמטורף הזה. 

22
00:01:49,580 --> 00:01:54,314
הרעיון האחד מהסרטון האחרון שאני באמת רוצה שתחזיקו בחוזקה בראשכם עכשיו 

23
00:01:54,314 --> 00:01:59,251
הוא שמכיוון שחשיבה על וקטור הגרדיאנט ככיוון ב-13,000 ממדים היא, בקלילות, 

24
00:01:59,251 --> 00:02:03,580
מעבר לטווח הדמיון שלנו, יש עוד רעיון איך שאתה יכול לחשוב על זה. 

25
00:02:04,600 --> 00:02:10,940
הגודל של כל רכיב כאן אומר לך עד כמה פונקציית העלות רגישה לכל משקל והטיה. 

26
00:02:11,800 --> 00:02:18,412
לדוגמה, נניח שאתה עובר את התהליך שאני עומד לתאר, ומחשב את הגרדיאנט השלילי, 

27
00:02:18,412 --> 00:02:26,260
והרכיב המשויך למשקל בקצה הזה כאן יוצא כ-3.2, בעוד שהרכיב המשויך לקצה הזה כאן יוצא כ-0.1. 

28
00:02:26,820 --> 00:02:33,208
הדרך שבה תפרשו את זה היא שהעלות של הפונקציה רגישה פי 32 לשינויים במשקל הראשון הזה, 

29
00:02:33,208 --> 00:02:38,134
אז אם הייתם מתנועעים קצת בערך הזה, זה יגרום לשינוי מסוים בעלות, 

30
00:02:38,134 --> 00:02:43,060
ולשינוי הזה. גדול פי 32 ממה שאותו התנודדות למשקל השני היה נותן. 

31
00:02:48,420 --> 00:02:51,890
באופן אישי, כשלמדתי לראשונה על התפשטות לאחור, אני חושב 

32
00:02:51,890 --> 00:02:55,740
שההיבט המבלבל ביותר היה רק המרדף אחר התווים והאינדקס של הכל. 

33
00:02:56,220 --> 00:02:59,997
אבל ברגע שאתה פותח את מה שכל חלק באלגוריתם הזה באמת עושה, 

34
00:02:59,997 --> 00:03:03,514
כל אפקט אינדיבידואלי שיש לו הוא למעשה די אינטואיטיבי, 

35
00:03:03,514 --> 00:03:06,640
רק שיש הרבה התאמות קטנות שמשתלבות זו על גבי זו. 

36
00:03:07,740 --> 00:03:11,691
אז אני אתחיל את העניינים כאן עם התעלמות מוחלטת מהסימונים, 

37
00:03:11,691 --> 00:03:16,120
ופשוט יעבור על ההשפעות שיש לכל דוגמה לאימון על המשקולות וההטיות. 

38
00:03:17,020 --> 00:03:21,768
מכיוון שפונקציית העלות כוללת ממוצע של עלות מסוימת לכל דוגמה על 

39
00:03:21,768 --> 00:03:26,743
פני כל עשרות אלפי דוגמאות האימון, הדרך בה אנו מתאימים את המשקולות 

40
00:03:26,743 --> 00:03:31,040
וההטיות לשלב ירידה בשיפוע בודד תלויה גם בכל דוגמה בודדת. 

41
00:03:31,680 --> 00:03:35,108
או ליתר דיוק, באופן עקרוני זה צריך, אבל בשביל יעילות חישובית, 

42
00:03:35,108 --> 00:03:39,200
אנחנו נעשה טריק קטן מאוחר יותר כדי למנוע ממך להכות בכל דוגמה עבור כל צעד. 

43
00:03:39,200 --> 00:03:42,707
במקרים אחרים, כרגע, כל מה שאנחנו הולכים לעשות הוא למקד 

44
00:03:42,707 --> 00:03:45,960
את תשומת הלב שלנו בדוגמה אחת בודדת, תמונה זו של 2. 

45
00:03:46,720 --> 00:03:51,480
איזו השפעה צריכה להיות לדוגמה האימון האחת הזו על האופן שבו המשקולות וההטיות מתכווננות? 

46
00:03:52,680 --> 00:03:56,237
נניח שאנחנו בנקודה שבה הרשת עדיין לא מאומנת היטב, 

47
00:03:56,237 --> 00:04:02,000
אז ההפעלה בפלט הולכות להיראות די אקראיות, אולי משהו כמו 0.5, 0.8, 0.2, עוד ועוד. 

48
00:04:02,520 --> 00:04:08,093
אנחנו לא יכולים לשנות ישירות את ההפעלות האלה, יש לנו רק השפעה על המשקלים וההטיות, 

49
00:04:08,093 --> 00:04:12,580
אבל זה מועיל לעקוב אחר ההתאמות שאנו רוצים שיבוצעו בשכבת הפלט הזו. 

50
00:04:13,360 --> 00:04:16,454
ומכיוון שאנחנו רוצים שהוא יסווג את התמונה כ-2, 

51
00:04:16,454 --> 00:04:21,260
אנחנו רוצים שהערך השלישי הזה יזוז כלפי מעלה בזמן שכל האחרים יתנוחו למטה. 

52
00:04:22,060 --> 00:04:25,561
יתר על כן, הגדלים של הדחפים הללו צריכים להיות 

53
00:04:25,561 --> 00:04:29,520
פרופורציונליים למרחק של כל ערך נוכחי מערך היעד שלו. 

54
00:04:30,220 --> 00:04:35,353
לדוגמה, העלייה להפעלה של אותו נוירון מספר 2 חשובה במובן מסוים 

55
00:04:35,353 --> 00:04:40,900
יותר מהירידה לנוירון מספר 8, שכבר די קרוב למקום בו הוא צריך להיות. 

56
00:04:42,040 --> 00:04:47,280
אז בהתקרבות נוספת, בואו נתמקד רק בנוירון האחד הזה, זה שאת ההפעלה שלו אנחנו רוצים להגביר. 

57
00:04:48,180 --> 00:04:54,490
זכור, הפעלה מוגדרת כסכום משוקלל מסוים של כל ההפעלות בשכבה הקודמת, בתוספת הטיה, 

58
00:04:54,490 --> 00:05:01,040
שהכל מחובר לאחר מכן למשהו כמו פונקציית ה-squishification של הסיגמואידים, או ReLU. 

59
00:05:01,640 --> 00:05:07,020
אז יש שלושה אפיקים שונים שיכולים לחבור יחד כדי לעזור להגביר את ההפעלה הזו. 

60
00:05:07,440 --> 00:05:11,182
אתה יכול להגדיל את ההטיה, אתה יכול להגדיל את המשקולות, 

61
00:05:11,182 --> 00:05:14,040
ואתה יכול לשנות את ההפעלות מהשכבה הקודמת. 

62
00:05:14,940 --> 00:05:20,860
התמקדות באופן שבו יש להתאים את המשקולות, שימו לב כיצד למשקולות יש רמות השפעה שונות. 

63
00:05:21,440 --> 00:05:25,331
לחיבורים עם הנוירונים הבהירים ביותר מהשכבה הקודמת יש את ההשפעה 

64
00:05:25,331 --> 00:05:29,100
הגדולה ביותר שכן משקלים אלה מוכפלים בערכי הפעלה גדולים יותר. 

65
00:05:31,460 --> 00:05:35,445
אז אם הייתם מגדילים אחד מהמשקלים האלה, למעשה יש לזה השפעה חזקה 

66
00:05:35,445 --> 00:05:40,949
יותר על פונקציית העלות האולטימטיבית מאשר הגדלת משקלם של קשרים עם נוירונים עמומים יותר, 

67
00:05:40,949 --> 00:05:43,480
לפחות בכל הנוגע לדוגמא האימון האחת הזו. 

68
00:05:44,420 --> 00:05:48,849
זכור, כאשר אנו מדברים על ירידה בשיפוע, לא אכפת לנו רק אם כל רכיב צריך להיות 

69
00:05:48,849 --> 00:05:53,220
דחף למעלה או למטה, אכפת לנו אילו מהם נותנים לך הכי הרבה כסף עבור הכסף שלך. 

70
00:05:55,020 --> 00:06:01,298
זה, אגב, מזכיר לפחות במידת מה תיאוריה במדעי המוח כיצד לומדים רשתות ביולוגיות של נוירונים, 

71
00:06:01,298 --> 00:06:06,460
תיאוריה העברית, המסוכמת לעתים קרובות בביטוי, נוירונים שיורים יחד חוט יחד. 

72
00:06:07,260 --> 00:06:11,852
כאן, העליות הגדולות ביותר למשקולות, החיזוק הגדול ביותר של הקשרים, 

73
00:06:11,852 --> 00:06:17,280
מתרחשת בין נוירונים שהם הפעילים ביותר לבין אלו שאנו רוצים להפוך לפעילים יותר. 

74
00:06:17,940 --> 00:06:21,175
במובן מסוים, הנוירונים שיורים בזמן שהם רואים 2 

75
00:06:21,175 --> 00:06:24,480
מקבלים קשר חזק יותר לאלו היורים כשחושבים על זה. 

76
00:06:25,400 --> 00:06:30,629
שיהיה ברור, אני לא בעמדה להצהיר בצורה כזו או אחרת לגבי האם רשתות מלאכותיות של 

77
00:06:30,629 --> 00:06:35,992
נוירונים מתנהגות משהו כמו מוחות ביולוגיים, והרעיון הזה מתחבר יחד עם כמה כוכביות 

78
00:06:35,992 --> 00:06:41,020
משמעותיות, אבל נתפס כמשהו רופף מאוד. אנלוגיה, אני מוצא את זה מעניין לציין. 

79
00:06:41,940 --> 00:06:45,458
בכל מקרה, הדרך השלישית שבה נוכל לעזור להגביר את ההפעלה 

80
00:06:45,458 --> 00:06:49,040
של הנוירון הזה היא על ידי שינוי כל הפעלות בשכבה הקודמת. 

81
00:06:49,040 --> 00:06:54,664
כלומר, אם כל מה שקשור לאותו נוירון ספרה 2 עם משקל חיובי נעשה בהיר יותר, 

82
00:06:54,664 --> 00:07:00,680
ואם כל מה שקשור למשקל שלילי נעשה עמום, אז הנוירון הספרה 2 הזה היה פעיל יותר. 

83
00:07:02,540 --> 00:07:06,441
ובדומה לשינויים במשקל, אתה הולך לקבל את המרב עבור הכסף שלך על 

84
00:07:06,441 --> 00:07:10,280
ידי חיפוש שינויים שהם פרופורציונליים לגודל המשקולות התואמות. 

85
00:07:12,140 --> 00:07:15,443
עכשיו כמובן, אנחנו לא יכולים להשפיע ישירות על ההפעלות האלה, 

86
00:07:15,443 --> 00:07:17,480
יש לנו רק שליטה על המשקולות וההטיות. 

87
00:07:17,480 --> 00:07:24,120
אבל בדיוק כמו בשכבה האחרונה, זה מועיל לרשום מה הם השינויים הרצויים האלה. 

88
00:07:24,580 --> 00:07:29,200
אבל זכור, בהרחקת שלב אחד כאן, זה רק מה שנוירון פלט ספרה 2 רוצה. 

89
00:07:29,760 --> 00:07:34,188
זכרו, אנחנו גם רוצים שכל שאר הנוירונים בשכבה האחרונה יהפכו פחות פעילים, 

90
00:07:34,188 --> 00:07:39,600
ולכל אחד מאותם נוירוני פלט אחרים יש מחשבות משלו לגבי מה צריך לקרות לשכבה השנייה אחרונה. 

91
00:07:42,700 --> 00:07:48,864
אז הרצון של נוירון ספרה 2 זה מתווסף יחד עם הרצונות של כל נוירוני 

92
00:07:48,864 --> 00:07:55,883
הפלט האחרים למה שיקרה לשכבה השנייה-אחרונה הזו, שוב ביחס למשקלים המתאימים, 

93
00:07:55,883 --> 00:08:00,720
ובפרופורציה לכמה כל אחד מאותם נוירונים צריך לשנות. 

94
00:08:01,600 --> 00:08:05,480
כאן בדיוק נכנס הרעיון של התפשטות לאחור. 

95
00:08:05,820 --> 00:08:09,554
על ידי הוספת כל האפקטים הרצויים הללו, אתה בעצם מקבל 

96
00:08:09,554 --> 00:08:13,360
רשימה של דחיפות שאתה רוצה שיקרה לשכבה השנייה אחרונה. 

97
00:08:14,220 --> 00:08:19,919
וברגע שיש לך כאלה, אתה יכול להחיל את אותו תהליך רקורסיבי על המשקולות וההטיות הרלוונטיות 

98
00:08:19,919 --> 00:08:25,100
שקובעות את הערכים האלה, לחזור על אותו תהליך שעברתי זה עתה ולנוע לאחור דרך הרשת. 

99
00:08:28,960 --> 00:08:33,062
ותרחיק קצת יותר, זכרו שזה הכל בדיוק איך דוגמה אחת 

100
00:08:33,062 --> 00:08:37,000
לאימון רוצה להניע כל אחד מהמשקלים וההטיות הללו. 

101
00:08:37,480 --> 00:08:40,350
אם רק היינו מקשיבים למה שהשניים האלה רוצים, בסופו 

102
00:08:40,350 --> 00:08:43,220
של דבר הרשת תתמרץ רק כדי לסווג את כל התמונות כ-2. 

103
00:08:44,059 --> 00:08:49,327
אז מה שאתה עושה זה לעבור את אותה שגרת תמיכה בגב עבור כל דוגמה אחרת לאימון, 

104
00:08:49,327 --> 00:08:53,401
לרשום כיצד כל אחד מהם היה רוצה לשנות את המשקולות וההטיות, 

105
00:08:53,401 --> 00:08:56,000
ולבצע את הממוצע של השינויים הרצויים. 

106
00:09:01,720 --> 00:09:06,801
האוסף הזה כאן של הדחפים הממוצעים לכל משקל והטיה הוא, באופן רופף, 

107
00:09:06,801 --> 00:09:13,680
השיפוע השלילי של פונקציית העלות שהוזכרה בסרטון האחרון, או לפחות משהו פרופורציונלי אליה. 

108
00:09:14,380 --> 00:09:19,358
אני אומר בצורה רופפת רק כי עדיין לא למדתי דיוק כמותי לגבי הדחפים האלה, 

109
00:09:19,358 --> 00:09:25,389
אבל אם הבנת כל שינוי שהזכרתי זה עתה, מדוע חלקם גדולים יותר מאחרים באופן פרופורציונלי, 

110
00:09:25,389 --> 00:09:31,000
וכיצד צריך להוסיף את כולם יחד, אתה מבין את המכניקה של מה בעצם עושה ההפצה לאחור. 

111
00:09:33,960 --> 00:09:38,284
אגב, בפועל, לוקח למחשבים זמן רב במיוחד כדי לחבר את 

112
00:09:38,284 --> 00:09:42,440
ההשפעה של כל דוגמה לאימון בכל צעד בירידה בשיפוע. 

113
00:09:43,140 --> 00:09:44,820
אז הנה מה שנהוג לעשות במקום. 

114
00:09:45,480 --> 00:09:50,127
אתה מערבב באקראי את נתוני האימון שלך ומחלק אותם לחבורה שלמה של מיני-אצט, 

115
00:09:50,127 --> 00:09:52,420
נניח שלכל אחד יש 100 דוגמאות אימון. 

116
00:09:52,939 --> 00:09:57,280
ואז אתה מחשב שלב לפי המיני-אצט. 

117
00:09:57,280 --> 00:10:01,978
זה לא השיפוע האמיתי של פונקציית העלות, שתלוי בכל נתוני האימון, 

118
00:10:01,978 --> 00:10:06,303
לא תת-הקבוצה הקטנה הזו, אז זה לא הצעד היעיל ביותר בירידה, 

119
00:10:06,303 --> 00:10:12,120
אבל כל מיני-אצט נותן לך קירוב די טוב, וחשוב מכך. נותן לך זירוז חישוב משמעותי. 

120
00:10:12,820 --> 00:10:17,012
אם היית מתווה את מסלול הרשת שלך מתחת למשטח העלות הרלוונטי, 

121
00:10:17,012 --> 00:10:22,627
זה יהיה קצת יותר כמו אדם שיכור המועד ללא מטרה במורד גבעה אך עושה צעדים מהירים, 

122
00:10:22,627 --> 00:10:27,104
במקום אדם מחושב בקפידה שקובע את כיוון הירידה המדויק של כל צעד. 

123
00:10:27,104 --> 00:10:30,160
לפני שתעשה צעד איטי וזהיר מאוד בכיוון הזה. 

124
00:10:31,540 --> 00:10:34,660
טכניקה זו מכונה ירידה בשיפוע סטוכסטי. 

125
00:10:35,960 --> 00:10:39,620
יש פה הרבה דברים, אז בואו נסכם את זה לעצמנו, נכון? 

126
00:10:40,440 --> 00:10:45,414
התפשטות לאחור הוא האלגוריתם לקביעת האופן שבו דוגמה אחת לאימון תרצה להניע את 

127
00:10:45,414 --> 00:10:49,603
המשקולות וההטיות, לא רק במונחים של האם הם צריכים לעלות או לרדת, 

128
00:10:49,603 --> 00:10:55,494
אלא במונחים של מה הפרופורציות היחסיות לשינויים האלה שגורמים לירידה המהירה ביותר ל- עֲלוּת.

129
00:10:55,494 --> 00:10:55,560
 

130
00:10:56,260 --> 00:11:02,094
שלב ירידה שיפוע אמיתי יכלול ביצוע של כל עשרות ואלפי דוגמאות האימון שלך וממוצע 

131
00:11:02,094 --> 00:11:06,507
של השינויים הרצויים שאתה מקבל, אבל זה איטי מבחינה חישובית, 

132
00:11:06,507 --> 00:11:13,240
אז במקום זאת אתה מחלק את הנתונים באופן אקראי למיני-אצטות ומחשב כל שלב ביחס ל- מיני אצווה. 

133
00:11:14,000 --> 00:11:18,462
אם תעבור שוב ושוב על כל המיני-אצות ותבצע את ההתאמות האלה, 

134
00:11:18,462 --> 00:11:24,232
תתכנס למינימום מקומי של פונקציית העלות, כלומר הרשת שלך תעשה עבודה ממש טובה 

135
00:11:24,232 --> 00:11:25,540
בדוגמאות ההדרכה. 

136
00:11:27,240 --> 00:11:34,371
אז עם כל זה, כל שורת קוד שתיכנס ליישום backprop למעשה מתכתבת עם משהו שראית עכשיו, 

137
00:11:34,371 --> 00:11:36,720
לפחות במונחים לא פורמליים. 

138
00:11:37,560 --> 00:11:40,514
אבל לפעמים לדעת מה המתמטיקה עושה זה רק חצי מהקרב, 

139
00:11:40,514 --> 00:11:44,120
ורק מייצג את הדבר הארור הוא המקום שבו זה נהיה מבולבל ומבלבל. 

140
00:11:44,860 --> 00:11:49,822
אז, לאלו מכם שכן רוצים להעמיק, הסרטון הבא עובר על אותם רעיונות שהוצגו כאן זה עתה, 

141
00:11:49,822 --> 00:11:53,696
אבל במונחים של החשבון הבסיסי, מה שיש לקוות לעשות את זה קצת יותר 

142
00:11:53,696 --> 00:11:56,420
מוכר כפי שאתם רואים את הנושא ב משאבים אחרים. 

143
00:11:57,340 --> 00:12:00,966
לפני כן, דבר אחד שכדאי להדגיש הוא שכדי שהאלגוריתם הזה יעבוד, 

144
00:12:00,966 --> 00:12:05,900
וזה מתאים לכל מיני למידת מכונה מעבר לרשתות עצביות בלבד, אתה צריך הרבה נתוני אימון. 

145
00:12:06,420 --> 00:12:10,641
במקרה שלנו, דבר אחד שהופך ספרות בכתב יד לדוגמא כל כך נחמדה הוא שקיים 

146
00:12:10,641 --> 00:12:14,740
מסד הנתונים של MNIST, עם כל כך הרבה דוגמאות שסומנו על ידי בני אדם. 

147
00:12:15,300 --> 00:12:19,293
אז אתגר נפוץ שאלו מכם שעובדים בלמידת מכונה יכירו הוא פשוט לקבל את 

148
00:12:19,293 --> 00:12:24,740
נתוני ההדרכה המסומנים להם אתם באמת צריכים, בין אם זה לגרום לאנשים לסמן עשרות אלפי תמונות, 

149
00:12:24,740 --> 00:12:27,100
או כל סוג אחר שעמו אתם עשויים להתמודד. 

