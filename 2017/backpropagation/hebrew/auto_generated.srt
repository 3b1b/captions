1
00:00:00,000 --> 00:00:09,640
כאן אנו מתמודדים עם התפשטות לאחור, האלגוריתם המרכזי מאחורי האופן שבו רשתות עצביות לומדות.

2
00:00:09,640 --> 00:00:13,320
לאחר סיכום מהיר של המקום בו אנו נמצאים, הדבר הראשון שאעשה

3
00:00:13,320 --> 00:00:17,400
הוא הדרכה אינטואיטיבית למה שהאלגוריתם עושה בפועל, ללא כל התייחסות לנוסחאות.

4
00:00:17,400 --> 00:00:21,400
לאחר מכן, לאלו מכם שכן רוצים לצלול לתוך

5
00:00:21,400 --> 00:00:24,040
המתמטיקה, הסרטון הבא נכנס לחישוב שבבסיס כל זה.

6
00:00:24,040 --> 00:00:27,320
אם צפית בשני הסרטונים האחרונים, או אם אתה רק קופץ פנימה עם

7
00:00:27,320 --> 00:00:31,080
הרקע המתאים, אתה יודע מהי רשת עצבית וכיצד היא מזרימה מידע קדימה.

8
00:00:31,080 --> 00:00:35,520
כאן, אנחנו עושים את הדוגמה הקלאסית של זיהוי ספרות בכתב יד שערכי

9
00:00:35,520 --> 00:00:40,280
הפיקסלים שלהן מוזנים לשכבה הראשונה של הרשת עם 784 נוירונים, ואני

10
00:00:40,280 --> 00:00:44,720
הצגתי רשת עם שתי שכבות נסתרות עם רק 16 נוירונים כל אחת,

11
00:00:44,720 --> 00:00:49,520
ופלט שכבה של 10 נוירונים, המציינת באיזו ספרה הרשת בוחרת כתשובה.

12
00:00:49,520 --> 00:00:54,480
אני גם מצפה שתבינו את הירידה בשיפוע, כפי שתואר

13
00:00:54,480 --> 00:01:00,160
בסרטון האחרון, וכיצד כוונתנו בלמידה היא שאנו רוצים

14
00:01:00,160 --> 00:01:02,080
למצוא אילו משקלים והטיות ממזערים פונקציית עלות מסוימת.

15
00:01:02,080 --> 00:01:07,560
כתזכורת מהירה, בעלות של דוגמה לאימון בודד, אתה לוקח

16
00:01:07,560 --> 00:01:12,920
את הפלט שהרשת נותנת, יחד עם הפלט שרצית שהיא

17
00:01:12,920 --> 00:01:15,560
תיתן, ומסכמים את הריבועים של ההבדלים בין כל רכיב.

18
00:01:15,560 --> 00:01:20,160
אם תעשה זאת עבור כל עשרות אלפי דוגמאות האימון שלך

19
00:01:20,160 --> 00:01:23,040
וממוצע התוצאות, זה נותן לך את העלות הכוללת של הרשת.

20
00:01:23,040 --> 00:01:26,320
כאילו זה לא מספיק כדי לחשוב עליו, כפי שתואר בסרטון

21
00:01:26,320 --> 00:01:31,700
האחרון, הדבר שאנו מחפשים הוא השיפוע השלילי של פונקציית העלות

22
00:01:31,700 --> 00:01:36,000
הזו, שאומר לך כיצד עליך לשנות את כל המשקלים וההטיות,

23
00:01:36,000 --> 00:01:43,080
כל חיבורים אלה, כדי להפחית את העלות בצורה היעילה ביותר.

24
00:01:43,080 --> 00:01:48,600
התפשטות לאחור, הנושא של הסרטון הזה, הוא

25
00:01:48,600 --> 00:01:49,600
אלגוריתם לחישוב השיפוע המסובך והמטורף הזה.

26
00:01:49,600 --> 00:01:53,300
הרעיון האחד מהסרטון האחרון שאני באמת רוצה שתחזיקו בחוזקה

27
00:01:53,300 --> 00:01:58,280
בראשכם עכשיו הוא שמכיוון שחשיבה על וקטור הגרדיאנט ככיוון

28
00:01:58,280 --> 00:02:02,660
ב-13,000 ממדים היא, בקלילות, מעבר לטווח הדמיון שלנו, יש

29
00:02:02,660 --> 00:02:04,620
עוד רעיון איך שאתה יכול לחשוב על זה.

30
00:02:04,620 --> 00:02:09,700
הגודל של כל רכיב כאן אומר לך עד

31
00:02:09,700 --> 00:02:11,820
כמה פונקציית העלות רגישה לכל משקל והטיה.

32
00:02:11,820 --> 00:02:15,180
לדוגמה, נניח שאתה עובר את התהליך שאני עומד לתאר, ומחשב

33
00:02:15,180 --> 00:02:19,800
את הגרדיאנט השלילי, והרכיב המשויך למשקל בקצה הזה כאן יוצא

34
00:02:19,800 --> 00:02:26,940
כ-3. 2, בעוד שהרכיב המשויך לקצה הזה כאן יוצא כ-0. 1.

35
00:02:26,940 --> 00:02:31,520
הדרך שבה תפרשו את זה היא שהעלות של הפונקציה רגישה

36
00:02:31,520 --> 00:02:36,100
פי 32 לשינויים במשקל הראשון הזה, אז אם הייתם מתנועעים

37
00:02:36,100 --> 00:02:40,780
קצת בערך הזה, זה יגרום לשינוי מסוים בעלות, ולשינוי הזה.

38
00:02:40,780 --> 00:02:45,580
גדול פי 32 ממה שאותו התנודדות למשקל השני היה נותן.

39
00:02:45,580 --> 00:02:52,500
באופן אישי, כשלמדתי לראשונה על התפשטות לאחור, אני חושב שההיבט

40
00:02:52,500 --> 00:02:55,820
המבלבל ביותר היה רק המרדף אחר התווים והאינדקס של הכל.

41
00:02:55,820 --> 00:03:00,240
אבל ברגע שאתה פותח את מה שכל חלק באלגוריתם הזה באמת

42
00:03:00,240 --> 00:03:04,540
עושה, כל אפקט אינדיבידואלי שיש לו הוא למעשה די אינטואיטיבי,

43
00:03:04,540 --> 00:03:07,740
רק שיש הרבה התאמות קטנות שמשתלבות זו על גבי זו.

44
00:03:07,740 --> 00:03:11,380
אז אני אתחיל את העניינים כאן עם התעלמות מוחלטת מהסימונים, ופשוט

45
00:03:11,380 --> 00:03:17,380
יעבור על ההשפעות שיש לכל דוגמה לאימון על המשקולות וההטיות.

46
00:03:17,380 --> 00:03:21,880
מכיוון שפונקציית העלות כוללת ממוצע של עלות מסוימת לכל דוגמה על

47
00:03:21,880 --> 00:03:26,980
פני כל עשרות אלפי דוגמאות האימון, הדרך בה אנו מתאימים את

48
00:03:26,980 --> 00:03:31,740
המשקולות וההטיות לשלב ירידה בשיפוע בודד תלויה גם בכל דוגמה בודדת.

49
00:03:31,740 --> 00:03:35,300
או ליתר דיוק, באופן עקרוני זה צריך, אבל בשביל יעילות חישובית, אנחנו נעשה

50
00:03:35,300 --> 00:03:39,860
טריק קטן מאוחר יותר כדי למנוע ממך להכות בכל דוגמה עבור כל צעד.

51
00:03:39,860 --> 00:03:44,460
במקרים אחרים, כרגע, כל מה שאנחנו הולכים לעשות הוא למקד את

52
00:03:44,460 --> 00:03:46,780
תשומת הלב שלנו בדוגמה אחת בודדת, תמונה זו של 2.

53
00:03:46,780 --> 00:03:51,740
איזו השפעה צריכה להיות לדוגמה האימון האחת הזו על האופן שבו המשקולות וההטיות מתכווננות?

54
00:03:51,740 --> 00:03:56,040
נניח שאנחנו בנקודה שבה הרשת עדיין לא מאומנת היטב, אז ההפעלה בפלט

55
00:03:56,040 --> 00:04:01,620
הולכות להיראות די אקראיות, אולי משהו כמו 0. 5, 0. 8, 0. 2,

56
00:04:01,620 --> 00:04:02,780
עוד ועוד.

57
00:04:02,780 --> 00:04:06,700
אנחנו לא יכולים לשנות ישירות את ההפעלות האלה, יש

58
00:04:06,700 --> 00:04:11,380
לנו רק השפעה על המשקלים וההטיות, אבל זה מועיל

59
00:04:11,380 --> 00:04:13,340
לעקוב אחר ההתאמות שאנו רוצים שיבוצעו בשכבת הפלט הזו.

60
00:04:13,340 --> 00:04:18,220
ומכיוון שאנחנו רוצים שהוא יסווג את התמונה כ-2, אנחנו רוצים שהערך

61
00:04:18,220 --> 00:04:21,700
השלישי הזה יזוז כלפי מעלה בזמן שכל האחרים יתנוחו למטה.

62
00:04:21,700 --> 00:04:27,620
יתר על כן, הגדלים של הדחפים הללו צריכים להיות

63
00:04:27,620 --> 00:04:30,220
פרופורציונליים למרחק של כל ערך נוכחי מערך היעד שלו.

64
00:04:30,220 --> 00:04:35,260
לדוגמה, העלייה להפעלה של אותו נוירון מספר 2

65
00:04:35,260 --> 00:04:39,620
חשובה במובן מסוים יותר מהירידה לנוירון מספר 8,

66
00:04:39,620 --> 00:04:42,060
שכבר די קרוב למקום בו הוא צריך להיות.

67
00:04:42,060 --> 00:04:46,260
אז בהתקרבות נוספת, בואו נתמקד רק בנוירון האחד

68
00:04:46,260 --> 00:04:47,900
הזה, זה שאת ההפעלה שלו אנחנו רוצים להגביר.

69
00:04:47,900 --> 00:04:53,680
זכור, הפעלה מוגדרת כסכום משוקלל מסוים של כל ההפעלות

70
00:04:53,680 --> 00:04:58,380
בשכבה הקודמת, בתוספת הטיה, שהכל מחובר לאחר מכן

71
00:04:58,380 --> 00:05:01,900
למשהו כמו פונקציית ה-squishification של הסיגמואידים, או ReLU.

72
00:05:01,900 --> 00:05:07,060
אז יש שלושה אפיקים שונים שיכולים לחבור

73
00:05:07,060 --> 00:05:08,060
יחד כדי לעזור להגביר את ההפעלה הזו.

74
00:05:08,060 --> 00:05:12,800
אתה יכול להגדיל את ההטיה, אתה יכול להגדיל את

75
00:05:12,800 --> 00:05:15,300
המשקולות, ואתה יכול לשנות את ההפעלות מהשכבה הקודמת.

76
00:05:15,300 --> 00:05:19,720
התמקדות באופן שבו יש להתאים את המשקולות, שימו

77
00:05:19,720 --> 00:05:21,460
לב כיצד למשקולות יש רמות השפעה שונות.

78
00:05:21,460 --> 00:05:25,100
לחיבורים עם הנוירונים הבהירים ביותר מהשכבה הקודמת יש את ההשפעה

79
00:05:25,100 --> 00:05:31,420
הגדולה ביותר שכן משקלים אלה מוכפלים בערכי הפעלה גדולים יותר.

80
00:05:31,420 --> 00:05:35,820
אז אם הייתם מגדילים אחד מהמשקלים האלה, למעשה יש לזה השפעה

81
00:05:35,820 --> 00:05:40,900
חזקה יותר על פונקציית העלות האולטימטיבית מאשר הגדלת משקלם של קשרים

82
00:05:40,900 --> 00:05:44,020
עם נוירונים עמומים יותר, לפחות בכל הנוגע לדוגמא האימון האחת הזו.

83
00:05:44,020 --> 00:05:48,700
זכור, כאשר אנו מדברים על ירידה בשיפוע, לא אכפת לנו רק

84
00:05:48,700 --> 00:05:53,020
אם כל רכיב צריך להיות דחף למעלה או למטה, אכפת לנו

85
00:05:53,020 --> 00:05:54,020
אילו מהם נותנים לך הכי הרבה כסף עבור הכסף שלך.

86
00:05:54,020 --> 00:06:00,260
זה, אגב, מזכיר לפחות במידת מה תיאוריה במדעי המוח

87
00:06:00,260 --> 00:06:04,900
כיצד לומדים רשתות ביולוגיות של נוירונים, תיאוריה העברית, המסוכמת

88
00:06:04,900 --> 00:06:06,940
לעתים קרובות בביטוי, נוירונים שיורים יחד חוט יחד.

89
00:06:06,940 --> 00:06:12,460
כאן, העליות הגדולות ביותר למשקולות, החיזוק הגדול ביותר

90
00:06:12,460 --> 00:06:16,860
של הקשרים, מתרחשת בין נוירונים שהם הפעילים ביותר

91
00:06:16,860 --> 00:06:18,100
לבין אלו שאנו רוצים להפוך לפעילים יותר.

92
00:06:18,100 --> 00:06:22,520
במובן מסוים, הנוירונים שיורים בזמן שהם רואים 2 מקבלים

93
00:06:22,520 --> 00:06:25,440
קשר חזק יותר לאלו היורים כשחושבים על זה.

94
00:06:25,440 --> 00:06:29,240
שיהיה ברור, אני לא בעמדה להצהיר בצורה כזו או אחרת לגבי

95
00:06:29,240 --> 00:06:34,020
האם רשתות מלאכותיות של נוירונים מתנהגות משהו כמו מוחות ביולוגיים,

96
00:06:34,020 --> 00:06:39,440
והרעיון הזה מתחבר יחד עם כמה כוכביות משמעותיות, אבל נתפס

97
00:06:39,440 --> 00:06:41,760
כמשהו רופף מאוד. אנלוגיה, אני מוצא את זה מעניין לציין.

98
00:06:41,760 --> 00:06:46,760
בכל מקרה, הדרך השלישית שבה נוכל לעזור להגביר את ההפעלה של

99
00:06:46,760 --> 00:06:49,360
הנוירון הזה היא על ידי שינוי כל הפעלות בשכבה הקודמת.

100
00:06:49,360 --> 00:06:55,080
כלומר, אם כל מה שקשור לאותו נוירון ספרה 2 עם משקל

101
00:06:55,080 --> 00:06:59,480
חיובי נעשה בהיר יותר, ואם כל מה שקשור למשקל שלילי

102
00:06:59,480 --> 00:07:02,680
נעשה עמום, אז הנוירון הספרה 2 הזה היה פעיל יותר.

103
00:07:02,680 --> 00:07:06,200
ובדומה לשינויים במשקל, אתה הולך לקבל את המרב עבור הכסף

104
00:07:06,200 --> 00:07:10,840
שלך על ידי חיפוש שינויים שהם פרופורציונליים לגודל המשקולות התואמות.

105
00:07:10,840 --> 00:07:16,520
עכשיו כמובן, אנחנו לא יכולים להשפיע ישירות על ההפעלות

106
00:07:16,520 --> 00:07:18,320
האלה, יש לנו רק שליטה על המשקולות וההטיות.

107
00:07:18,320 --> 00:07:22,960
אבל בדיוק כמו בשכבה האחרונה, זה מועיל

108
00:07:22,960 --> 00:07:23,960
לרשום מה הם השינויים הרצויים האלה.

109
00:07:23,960 --> 00:07:29,040
אבל זכור, בהרחקת שלב אחד כאן, זה

110
00:07:29,040 --> 00:07:30,040
רק מה שנוירון פלט ספרה 2 רוצה.

111
00:07:30,040 --> 00:07:34,960
זכרו, אנחנו גם רוצים שכל שאר הנוירונים בשכבה האחרונה יהפכו

112
00:07:34,960 --> 00:07:38,460
פחות פעילים, ולכל אחד מאותם נוירוני פלט אחרים יש

113
00:07:38,460 --> 00:07:43,200
מחשבות משלו לגבי מה צריך לקרות לשכבה השנייה אחרונה.

114
00:07:43,200 --> 00:07:49,220
אז הרצון של נוירון ספרה 2 זה מתווסף יחד

115
00:07:49,220 --> 00:07:54,800
עם הרצונות של כל נוירוני הפלט האחרים למה

116
00:07:54,800 --> 00:08:00,240
שיקרה לשכבה השנייה-אחרונה הזו, שוב ביחס למשקלים המתאימים,

117
00:08:00,240 --> 00:08:01,740
ובפרופורציה לכמה כל אחד מאותם נוירונים צריך לשנות.

118
00:08:01,740 --> 00:08:05,940
כאן בדיוק נכנס הרעיון של התפשטות לאחור.

119
00:08:05,940 --> 00:08:11,080
על ידי הוספת כל האפקטים הרצויים הללו, אתה בעצם מקבל

120
00:08:11,080 --> 00:08:14,300
רשימה של דחיפות שאתה רוצה שיקרה לשכבה השנייה אחרונה.

121
00:08:14,300 --> 00:08:18,740
וברגע שיש לך כאלה, אתה יכול להחיל את אותו תהליך

122
00:08:18,740 --> 00:08:23,400
רקורסיבי על המשקולות וההטיות הרלוונטיות שקובעות את הערכים האלה, לחזור

123
00:08:23,400 --> 00:08:29,180
על אותו תהליך שעברתי זה עתה ולנוע לאחור דרך הרשת.

124
00:08:29,180 --> 00:08:33,960
ותרחיק קצת יותר, זכרו שזה הכל בדיוק איך דוגמה

125
00:08:33,960 --> 00:08:37,520
אחת לאימון רוצה להניע כל אחד מהמשקלים וההטיות הללו.

126
00:08:37,520 --> 00:08:41,400
אם רק היינו מקשיבים למה שהשניים האלה רוצים, בסופו של

127
00:08:41,400 --> 00:08:44,140
דבר הרשת תתמרץ רק כדי לסווג את כל התמונות כ-2.

128
00:08:44,140 --> 00:08:49,500
אז מה שאתה עושה זה לעבור את אותה שגרת תמיכה בגב

129
00:08:49,500 --> 00:08:54,700
עבור כל דוגמה אחרת לאימון, לרשום כיצד כל אחד מהם היה

130
00:08:54,700 --> 00:09:02,300
רוצה לשנות את המשקולות וההטיות, ולבצע את הממוצע של השינויים הרצויים.

131
00:09:02,300 --> 00:09:08,260
האוסף הזה כאן של הדחפים הממוצעים לכל משקל והטיה

132
00:09:08,260 --> 00:09:12,340
הוא, באופן רופף, השיפוע השלילי של פונקציית העלות

133
00:09:12,340 --> 00:09:14,360
שהוזכרה בסרטון האחרון, או לפחות משהו פרופורציונלי אליה.

134
00:09:14,360 --> 00:09:18,980
אני אומר בצורה רופפת רק כי עדיין לא למדתי דיוק כמותי לגבי

135
00:09:18,980 --> 00:09:23,480
הדחפים האלה, אבל אם הבנת כל שינוי שהזכרתי זה עתה, מדוע

136
00:09:23,480 --> 00:09:28,740
חלקם גדולים יותר מאחרים באופן פרופורציונלי, וכיצד צריך להוסיף את כולם

137
00:09:28,740 --> 00:09:34,100
יחד, אתה מבין את המכניקה של מה בעצם עושה ההפצה לאחור.

138
00:09:34,100 --> 00:09:38,540
אגב, בפועל, לוקח למחשבים זמן רב במיוחד כדי לחבר את

139
00:09:38,540 --> 00:09:43,120
ההשפעה של כל דוגמה לאימון בכל צעד בירידה בשיפוע.

140
00:09:43,120 --> 00:09:45,540
אז הנה מה שנהוג לעשות במקום.

141
00:09:45,540 --> 00:09:50,460
אתה מערבב באקראי את נתוני האימון שלך ומחלק אותם לחבורה

142
00:09:50,460 --> 00:09:53,380
שלמה של מיני-אצט, נניח שלכל אחד יש 100 דוגמאות אימון.

143
00:09:53,380 --> 00:09:56,980
ואז אתה מחשב שלב לפי המיני-אצט.

144
00:09:56,980 --> 00:10:00,840
זה לא השיפוע האמיתי של פונקציית העלות, שתלוי בכל נתוני

145
00:10:00,840 --> 00:10:06,260
האימון, לא תת-הקבוצה הקטנה הזו, אז זה לא הצעד

146
00:10:06,260 --> 00:10:10,900
היעיל ביותר בירידה, אבל כל מיני-אצט נותן לך קירוב

147
00:10:10,900 --> 00:10:12,900
די טוב, וחשוב מכך. נותן לך זירוז חישוב משמעותי.

148
00:10:12,900 --> 00:10:16,900
אם היית מתווה את מסלול הרשת שלך מתחת למשטח העלות הרלוונטי, זה

149
00:10:16,900 --> 00:10:22,020
יהיה קצת יותר כמו אדם שיכור המועד ללא מטרה במורד גבעה אך

150
00:10:22,020 --> 00:10:26,880
עושה צעדים מהירים, במקום אדם מחושב בקפידה שקובע את כיוון הירידה המדויק

151
00:10:26,880 --> 00:10:31,620
של כל צעד. לפני שתעשה צעד איטי וזהיר מאוד בכיוון הזה.

152
00:10:31,620 --> 00:10:35,200
טכניקה זו מכונה ירידה בשיפוע סטוכסטי.

153
00:10:35,200 --> 00:10:40,400
יש פה הרבה דברים, אז בואו נסכם את זה לעצמנו, נכון?

154
00:10:40,400 --> 00:10:45,480
התפשטות לאחור הוא האלגוריתם לקביעת האופן שבו דוגמה אחת לאימון

155
00:10:45,480 --> 00:10:50,040
תרצה להניע את המשקולות וההטיות, לא רק במונחים של האם

156
00:10:50,040 --> 00:10:54,780
הם צריכים לעלות או לרדת, אלא במונחים של מה הפרופורציות

157
00:10:54,780 --> 00:10:56,240
היחסיות לשינויים האלה שגורמים לירידה המהירה ביותר ל- עֲלוּת.

158
00:10:56,240 --> 00:11:00,720
שלב ירידה שיפוע אמיתי יכלול ביצוע של כל עשרות ואלפי דוגמאות

159
00:11:00,720 --> 00:11:05,920
האימון שלך וממוצע של השינויים הרצויים שאתה מקבל, אבל זה

160
00:11:05,920 --> 00:11:11,680
איטי מבחינה חישובית, אז במקום זאת אתה מחלק את הנתונים

161
00:11:11,680 --> 00:11:14,000
באופן אקראי למיני-אצטות ומחשב כל שלב ביחס ל- מיני אצווה.

162
00:11:14,000 --> 00:11:18,600
אם תעבור שוב ושוב על כל המיני-אצות ותבצע את

163
00:11:18,600 --> 00:11:23,420
ההתאמות האלה, תתכנס למינימום מקומי של פונקציית העלות, כלומר

164
00:11:23,420 --> 00:11:27,540
הרשת שלך תעשה עבודה ממש טובה בדוגמאות ההדרכה.

165
00:11:27,540 --> 00:11:32,600
אז עם כל זה, כל שורת קוד שתיכנס ליישום backprop

166
00:11:32,600 --> 00:11:37,680
למעשה מתכתבת עם משהו שראית עכשיו, לפחות במונחים לא פורמליים.

167
00:11:37,680 --> 00:11:41,900
אבל לפעמים לדעת מה המתמטיקה עושה זה רק חצי מהקרב, ורק

168
00:11:41,900 --> 00:11:44,780
מייצג את הדבר הארור הוא המקום שבו זה נהיה מבולבל ומבלבל.

169
00:11:44,780 --> 00:11:49,360
אז, לאלו מכם שכן רוצים להעמיק, הסרטון הבא עובר על אותם רעיונות שהוצגו

170
00:11:49,360 --> 00:11:53,400
כאן זה עתה, אבל במונחים של החשבון הבסיסי, מה שיש לקוות לעשות את

171
00:11:53,400 --> 00:11:57,460
זה קצת יותר מוכר כפי שאתם רואים את הנושא ב משאבים אחרים.

172
00:11:57,460 --> 00:12:01,220
לפני כן, דבר אחד שכדאי להדגיש הוא שכדי שהאלגוריתם

173
00:12:01,220 --> 00:12:05,840
הזה יעבוד, וזה מתאים לכל מיני למידת מכונה מעבר

174
00:12:05,840 --> 00:12:06,840
לרשתות עצביות בלבד, אתה צריך הרבה נתוני אימון.

175
00:12:06,840 --> 00:12:10,740
במקרה שלנו, דבר אחד שהופך ספרות בכתב יד לדוגמא כל כך נחמדה הוא שקיים

176
00:12:10,740 --> 00:12:15,380
מסד הנתונים של MNIST, עם כל כך הרבה דוגמאות שסומנו על ידי בני אדם.

177
00:12:15,380 --> 00:12:19,000
אז אתגר נפוץ שאלו מכם שעובדים בלמידת מכונה יכירו הוא פשוט לקבל את

178
00:12:19,040 --> 00:12:22,880
נתוני ההדרכה המסומנים להם אתם באמת צריכים, בין אם זה לגרום לאנשים

179
00:12:22,880 --> 00:12:27,400
לסמן עשרות אלפי תמונות, או כל סוג אחר שעמו אתם עשויים להתמודד.

