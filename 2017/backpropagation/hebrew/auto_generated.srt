1
00:00:00,000 --> 00:00:09,640
כאן אנו מתמודדים עם התפשטות לאחור, האלגוריתם המרכזי מאחורי האופן שבו רשתות עצביות לומדות.

2
00:00:09,640 --> 00:00:13,577
לאחר סיכום מהיר של המקום בו אנו נמצאים, הדבר הראשון שאעשה הוא הדרכה

3
00:00:13,577 --> 00:00:17,400
אינטואיטיבית למה שהאלגוריתם עושה בפועל, ללא כל התייחסות לנוסחאות.

4
00:00:17,400 --> 00:00:24,040
לאחר מכן, לאלו מכם שכן רוצים לצלול לתוך המתמטיקה, הסרטון הבא נכנס לחישוב שבבסיס כל זה.

5
00:00:24,040 --> 00:00:27,673
אם צפית בשני הסרטונים האחרונים, או אם אתה רק קופץ פנימה עם הרקע

6
00:00:27,673 --> 00:00:31,080
המתאים, אתה יודע מהי רשת עצבית וכיצד היא מזרימה מידע קדימה.

7
00:00:31,080 --> 00:00:37,374
כאן, אנחנו עושים את הדוגמה הקלאסית של זיהוי ספרות בכתב יד שערכי הפיקסלים שלהן מוזנים

8
00:00:37,374 --> 00:00:43,521
לשכבה הראשונה של הרשת עם 784 נוירונים, ואני הצגתי רשת עם שתי שכבות נסתרות עם רק 16

9
00:00:43,521 --> 00:00:49,520
נוירונים כל אחת, ופלט שכבה של 10 נוירונים, המציינת באיזו ספרה הרשת בוחרת כתשובה.

10
00:00:49,520 --> 00:00:55,758
אני גם מצפה שתבינו את הירידה בשיפוע, כפי שתואר בסרטון האחרון, וכיצד כוונתנו

11
00:00:55,758 --> 00:01:02,080
בלמידה היא שאנו רוצים למצוא אילו משקלים והטיות ממזערים פונקציית עלות מסוימת.

12
00:01:02,080 --> 00:01:08,820
כתזכורת מהירה, בעלות של דוגמה לאימון בודד, אתה לוקח את הפלט שהרשת נותנת,

13
00:01:08,820 --> 00:01:15,560
יחד עם הפלט שרצית שהיא תיתן, ומסכמים את הריבועים של ההבדלים בין כל רכיב.

14
00:01:15,560 --> 00:01:19,226
אם תעשה זאת עבור כל עשרות אלפי דוגמאות האימון שלך

15
00:01:19,226 --> 00:01:23,040
וממוצע התוצאות, זה נותן לך את העלות הכוללת של הרשת.

16
00:01:23,040 --> 00:01:29,931
כאילו זה לא מספיק כדי לחשוב עליו, כפי שתואר בסרטון האחרון, הדבר שאנו מחפשים

17
00:01:29,931 --> 00:01:36,460
הוא השיפוע השלילי של פונקציית העלות הזו, שאומר לך כיצד עליך לשנות את כל

18
00:01:36,460 --> 00:01:43,080
המשקלים וההטיות, כל חיבורים אלה, כדי להפחית את העלות בצורה היעילה ביותר.

19
00:01:43,080 --> 00:01:49,600
התפשטות לאחור, הנושא של הסרטון הזה, הוא אלגוריתם לחישוב השיפוע המסובך והמטורף הזה.

20
00:01:49,600 --> 00:01:54,679
הרעיון האחד מהסרטון האחרון שאני באמת רוצה שתחזיקו בחוזקה בראשכם עכשיו

21
00:01:54,679 --> 00:01:59,976
הוא שמכיוון שחשיבה על וקטור הגרדיאנט ככיוון ב-13,000 ממדים היא, בקלילות,

22
00:01:59,976 --> 00:02:04,620
מעבר לטווח הדמיון שלנו, יש עוד רעיון איך שאתה יכול לחשוב על זה.

23
00:02:04,620 --> 00:02:11,820
הגודל של כל רכיב כאן אומר לך עד כמה פונקציית העלות רגישה לכל משקל והטיה.

24
00:02:11,820 --> 00:02:19,380
לדוגמה, נניח שאתה עובר את התהליך שאני עומד לתאר, ומחשב את הגרדיאנט השלילי, והרכיב

25
00:02:19,380 --> 00:02:26,940
המשויך למשקל בקצה הזה כאן יוצא כ-3.2, בעוד שהרכיב המשויך לקצה הזה כאן יוצא כ-0.1.

26
00:02:26,940 --> 00:02:33,212
הדרך שבה תפרשו את זה היא שהעלות של הפונקציה רגישה פי 32 לשינויים במשקל

27
00:02:33,212 --> 00:02:39,307
הראשון הזה, אז אם הייתם מתנועעים קצת בערך הזה, זה יגרום לשינוי מסוים

28
00:02:39,307 --> 00:02:45,580
בעלות, ולשינוי הזה. גדול פי 32 ממה שאותו התנודדות למשקל השני היה נותן.

29
00:02:45,580 --> 00:02:50,435
באופן אישי, כשלמדתי לראשונה על התפשטות לאחור, אני חושב

30
00:02:50,435 --> 00:02:55,820
שההיבט המבלבל ביותר היה רק המרדף אחר התווים והאינדקס של הכל.

31
00:02:55,820 --> 00:03:01,705
אבל ברגע שאתה פותח את מה שכל חלק באלגוריתם הזה באמת עושה, כל אפקט אינדיבידואלי

32
00:03:01,705 --> 00:03:07,740
שיש לו הוא למעשה די אינטואיטיבי, רק שיש הרבה התאמות קטנות שמשתלבות זו על גבי זו.

33
00:03:07,740 --> 00:03:12,285
אז אני אתחיל את העניינים כאן עם התעלמות מוחלטת מהסימונים,

34
00:03:12,285 --> 00:03:17,380
ופשוט יעבור על ההשפעות שיש לכל דוגמה לאימון על המשקולות וההטיות.

35
00:03:17,380 --> 00:03:22,243
מכיוון שפונקציית העלות כוללת ממוצע של עלות מסוימת לכל דוגמה על

36
00:03:22,243 --> 00:03:27,339
פני כל עשרות אלפי דוגמאות האימון, הדרך בה אנו מתאימים את המשקולות

37
00:03:27,339 --> 00:03:31,740
וההטיות לשלב ירידה בשיפוע בודד תלויה גם בכל דוגמה בודדת.

38
00:03:31,740 --> 00:03:35,800
או ליתר דיוק, באופן עקרוני זה צריך, אבל בשביל יעילות חישובית, אנחנו

39
00:03:35,800 --> 00:03:39,860
נעשה טריק קטן מאוחר יותר כדי למנוע ממך להכות בכל דוגמה עבור כל צעד.

40
00:03:39,860 --> 00:03:43,450
במקרים אחרים, כרגע, כל מה שאנחנו הולכים לעשות הוא למקד

41
00:03:43,450 --> 00:03:46,780
את תשומת הלב שלנו בדוגמה אחת בודדת, תמונה זו של 2.

42
00:03:46,780 --> 00:03:51,740
איזו השפעה צריכה להיות לדוגמה האימון האחת הזו על האופן שבו המשקולות וההטיות מתכווננות?

43
00:03:51,740 --> 00:03:57,217
נניח שאנחנו בנקודה שבה הרשת עדיין לא מאומנת היטב, אז ההפעלה בפלט

44
00:03:57,217 --> 00:04:02,780
הולכות להיראות די אקראיות, אולי משהו כמו 0.5, 0.8, 0.2, עוד ועוד.

45
00:04:02,780 --> 00:04:07,988
אנחנו לא יכולים לשנות ישירות את ההפעלות האלה, יש לנו רק השפעה על המשקלים

46
00:04:07,988 --> 00:04:13,340
וההטיות, אבל זה מועיל לעקוב אחר ההתאמות שאנו רוצים שיבוצעו בשכבת הפלט הזו.

47
00:04:13,340 --> 00:04:17,450
ומכיוון שאנחנו רוצים שהוא יסווג את התמונה כ-2, אנחנו רוצים

48
00:04:17,450 --> 00:04:21,700
שהערך השלישי הזה יזוז כלפי מעלה בזמן שכל האחרים יתנוחו למטה.

49
00:04:21,700 --> 00:04:25,699
יתר על כן, הגדלים של הדחפים הללו צריכים להיות

50
00:04:25,699 --> 00:04:30,220
פרופורציונליים למרחק של כל ערך נוכחי מערך היעד שלו.

51
00:04:30,220 --> 00:04:35,910
לדוגמה, העלייה להפעלה של אותו נוירון מספר 2 חשובה במובן מסוים

52
00:04:35,910 --> 00:04:42,060
יותר מהירידה לנוירון מספר 8, שכבר די קרוב למקום בו הוא צריך להיות.

53
00:04:42,060 --> 00:04:47,900
אז בהתקרבות נוספת, בואו נתמקד רק בנוירון האחד הזה, זה שאת ההפעלה שלו אנחנו רוצים להגביר.

54
00:04:47,900 --> 00:04:54,769
זכור, הפעלה מוגדרת כסכום משוקלל מסוים של כל ההפעלות בשכבה הקודמת, בתוספת הטיה,

55
00:04:54,769 --> 00:05:01,900
שהכל מחובר לאחר מכן למשהו כמו פונקציית ה-squishification של הסיגמואידים, או ReLU.

56
00:05:01,900 --> 00:05:08,060
אז יש שלושה אפיקים שונים שיכולים לחבור יחד כדי לעזור להגביר את ההפעלה הזו.

57
00:05:08,060 --> 00:05:12,165
אתה יכול להגדיל את ההטיה, אתה יכול להגדיל את המשקולות,

58
00:05:12,165 --> 00:05:15,300
ואתה יכול לשנות את ההפעלות מהשכבה הקודמת.

59
00:05:15,300 --> 00:05:21,460
התמקדות באופן שבו יש להתאים את המשקולות, שימו לב כיצד למשקולות יש רמות השפעה שונות.

60
00:05:21,460 --> 00:05:26,520
לחיבורים עם הנוירונים הבהירים ביותר מהשכבה הקודמת יש את ההשפעה

61
00:05:26,520 --> 00:05:31,420
הגדולה ביותר שכן משקלים אלה מוכפלים בערכי הפעלה גדולים יותר.

62
00:05:31,420 --> 00:05:35,597
אז אם הייתם מגדילים אחד מהמשקלים האלה, למעשה יש לזה השפעה חזקה

63
00:05:35,597 --> 00:05:39,709
יותר על פונקציית העלות האולטימטיבית מאשר הגדלת משקלם של קשרים

64
00:05:39,709 --> 00:05:44,020
עם נוירונים עמומים יותר, לפחות בכל הנוגע לדוגמא האימון האחת הזו.

65
00:05:44,020 --> 00:05:49,053
זכור, כאשר אנו מדברים על ירידה בשיפוע, לא אכפת לנו רק אם כל רכיב צריך להיות

66
00:05:49,053 --> 00:05:54,020
דחף למעלה או למטה, אכפת לנו אילו מהם נותנים לך הכי הרבה כסף עבור הכסף שלך.

67
00:05:54,020 --> 00:06:00,322
זה, אגב, מזכיר לפחות במידת מה תיאוריה במדעי המוח כיצד לומדים רשתות ביולוגיות של

68
00:06:00,322 --> 00:06:06,940
נוירונים, תיאוריה העברית, המסוכמת לעתים קרובות בביטוי, נוירונים שיורים יחד חוט יחד.

69
00:06:06,940 --> 00:06:12,597
כאן, העליות הגדולות ביותר למשקולות, החיזוק הגדול ביותר של הקשרים, מתרחשת

70
00:06:12,597 --> 00:06:18,100
בין נוירונים שהם הפעילים ביותר לבין אלו שאנו רוצים להפוך לפעילים יותר.

71
00:06:18,100 --> 00:06:21,731
במובן מסוים, הנוירונים שיורים בזמן שהם רואים 2

72
00:06:21,731 --> 00:06:25,440
מקבלים קשר חזק יותר לאלו היורים כשחושבים על זה.

73
00:06:25,440 --> 00:06:30,903
שיהיה ברור, אני לא בעמדה להצהיר בצורה כזו או אחרת לגבי האם רשתות מלאכותיות של

74
00:06:30,903 --> 00:06:36,506
נוירונים מתנהגות משהו כמו מוחות ביולוגיים, והרעיון הזה מתחבר יחד עם כמה כוכביות

75
00:06:36,506 --> 00:06:41,760
משמעותיות, אבל נתפס כמשהו רופף מאוד. אנלוגיה, אני מוצא את זה מעניין לציין.

76
00:06:41,760 --> 00:06:45,525
בכל מקרה, הדרך השלישית שבה נוכל לעזור להגביר את ההפעלה

77
00:06:45,525 --> 00:06:49,360
של הנוירון הזה היא על ידי שינוי כל הפעלות בשכבה הקודמת.

78
00:06:49,360 --> 00:06:55,796
כלומר, אם כל מה שקשור לאותו נוירון ספרה 2 עם משקל חיובי נעשה בהיר יותר,

79
00:06:55,796 --> 00:07:02,680
ואם כל מה שקשור למשקל שלילי נעשה עמום, אז הנוירון הספרה 2 הזה היה פעיל יותר.

80
00:07:02,680 --> 00:07:06,793
ובדומה לשינויים במשקל, אתה הולך לקבל את המרב עבור הכסף שלך על

81
00:07:06,793 --> 00:07:10,840
ידי חיפוש שינויים שהם פרופורציונליים לגודל המשקולות התואמות.

82
00:07:10,840 --> 00:07:14,387
עכשיו כמובן, אנחנו לא יכולים להשפיע ישירות על

83
00:07:14,387 --> 00:07:18,320
ההפעלות האלה, יש לנו רק שליטה על המשקולות וההטיות.

84
00:07:18,320 --> 00:07:23,960
אבל בדיוק כמו בשכבה האחרונה, זה מועיל לרשום מה הם השינויים הרצויים האלה.

85
00:07:23,960 --> 00:07:30,040
אבל זכור, בהרחקת שלב אחד כאן, זה רק מה שנוירון פלט ספרה 2 רוצה.

86
00:07:30,040 --> 00:07:36,702
זכרו, אנחנו גם רוצים שכל שאר הנוירונים בשכבה האחרונה יהפכו פחות פעילים, ולכל אחד

87
00:07:36,702 --> 00:07:43,200
מאותם נוירוני פלט אחרים יש מחשבות משלו לגבי מה צריך לקרות לשכבה השנייה אחרונה.

88
00:07:43,200 --> 00:07:49,542
אז הרצון של נוירון ספרה 2 זה מתווסף יחד עם הרצונות של כל נוירוני

89
00:07:49,542 --> 00:07:55,787
הפלט האחרים למה שיקרה לשכבה השנייה-אחרונה הזו, שוב ביחס למשקלים

90
00:07:55,787 --> 00:08:01,740
המתאימים, ובפרופורציה לכמה כל אחד מאותם נוירונים צריך לשנות.

91
00:08:01,740 --> 00:08:05,940
כאן בדיוק נכנס הרעיון של התפשטות לאחור.

92
00:08:05,940 --> 00:08:10,080
על ידי הוספת כל האפקטים הרצויים הללו, אתה בעצם מקבל

93
00:08:10,080 --> 00:08:14,300
רשימה של דחיפות שאתה רוצה שיקרה לשכבה השנייה אחרונה.

94
00:08:14,300 --> 00:08:22,094
וברגע שיש לך כאלה, אתה יכול להחיל את אותו תהליך רקורסיבי על המשקולות וההטיות הרלוונטיות

95
00:08:22,094 --> 00:08:29,180
שקובעות את הערכים האלה, לחזור על אותו תהליך שעברתי זה עתה ולנוע לאחור דרך הרשת.

96
00:08:29,180 --> 00:08:33,435
ותרחיק קצת יותר, זכרו שזה הכל בדיוק איך דוגמה אחת

97
00:08:33,435 --> 00:08:37,520
לאימון רוצה להניע כל אחד מהמשקלים וההטיות הללו.

98
00:08:37,520 --> 00:08:40,830
אם רק היינו מקשיבים למה שהשניים האלה רוצים, בסופו

99
00:08:40,830 --> 00:08:44,140
של דבר הרשת תתמרץ רק כדי לסווג את כל התמונות כ-2.

100
00:08:44,140 --> 00:08:53,326
אז מה שאתה עושה זה לעבור את אותה שגרת תמיכה בגב עבור כל דוגמה אחרת לאימון, לרשום כיצד

101
00:08:53,326 --> 00:09:02,300
כל אחד מהם היה רוצה לשנות את המשקולות וההטיות, ולבצע את הממוצע של השינויים הרצויים.

102
00:09:02,300 --> 00:09:08,527
האוסף הזה כאן של הדחפים הממוצעים לכל משקל והטיה הוא, באופן רופף, השיפוע השלילי

103
00:09:08,527 --> 00:09:14,360
של פונקציית העלות שהוזכרה בסרטון האחרון, או לפחות משהו פרופורציונלי אליה.

104
00:09:14,360 --> 00:09:20,856
אני אומר בצורה רופפת רק כי עדיין לא למדתי דיוק כמותי לגבי הדחפים האלה, אבל אם

105
00:09:20,856 --> 00:09:27,436
הבנת כל שינוי שהזכרתי זה עתה, מדוע חלקם גדולים יותר מאחרים באופן פרופורציונלי,

106
00:09:27,436 --> 00:09:34,100
וכיצד צריך להוסיף את כולם יחד, אתה מבין את המכניקה של מה בעצם עושה ההפצה לאחור.

107
00:09:34,100 --> 00:09:38,700
אגב, בפועל, לוקח למחשבים זמן רב במיוחד כדי לחבר את

108
00:09:38,700 --> 00:09:43,120
ההשפעה של כל דוגמה לאימון בכל צעד בירידה בשיפוע.

109
00:09:43,120 --> 00:09:45,540
אז הנה מה שנהוג לעשות במקום.

110
00:09:45,540 --> 00:09:49,495
אתה מערבב באקראי את נתוני האימון שלך ומחלק אותם לחבורה

111
00:09:49,495 --> 00:09:53,380
שלמה של מיני-אצט, נניח שלכל אחד יש 100 דוגמאות אימון.

112
00:09:53,380 --> 00:09:56,980
ואז אתה מחשב שלב לפי המיני-אצט.

113
00:09:56,980 --> 00:10:02,260
זה לא השיפוע האמיתי של פונקציית העלות, שתלוי בכל נתוני האימון, לא

114
00:10:02,260 --> 00:10:07,220
תת-הקבוצה הקטנה הזו, אז זה לא הצעד היעיל ביותר בירידה, אבל כל

115
00:10:07,220 --> 00:10:12,900
מיני-אצט נותן לך קירוב די טוב, וחשוב מכך. נותן לך זירוז חישוב משמעותי.

116
00:10:12,900 --> 00:10:19,037
אם היית מתווה את מסלול הרשת שלך מתחת למשטח העלות הרלוונטי, זה יהיה קצת יותר כמו

117
00:10:19,037 --> 00:10:25,252
אדם שיכור המועד ללא מטרה במורד גבעה אך עושה צעדים מהירים, במקום אדם מחושב בקפידה

118
00:10:25,252 --> 00:10:31,620
שקובע את כיוון הירידה המדויק של כל צעד. לפני שתעשה צעד איטי וזהיר מאוד בכיוון הזה.

119
00:10:31,620 --> 00:10:35,200
טכניקה זו מכונה ירידה בשיפוע סטוכסטי.

120
00:10:35,200 --> 00:10:40,400
יש פה הרבה דברים, אז בואו נסכם את זה לעצמנו, נכון?

121
00:10:40,400 --> 00:10:45,611
התפשטות לאחור הוא האלגוריתם לקביעת האופן שבו דוגמה אחת לאימון תרצה להניע את

122
00:10:45,611 --> 00:10:50,822
המשקולות וההטיות, לא רק במונחים של האם הם צריכים לעלות או לרדת, אלא במונחים

123
00:10:50,822 --> 00:10:56,240
של מה הפרופורציות היחסיות לשינויים האלה שגורמים לירידה המהירה ביותר ל- עֲלוּת.

124
00:10:56,240 --> 00:11:02,342
שלב ירידה שיפוע אמיתי יכלול ביצוע של כל עשרות ואלפי דוגמאות האימון שלך וממוצע

125
00:11:02,342 --> 00:11:08,288
של השינויים הרצויים שאתה מקבל, אבל זה איטי מבחינה חישובית, אז במקום זאת אתה

126
00:11:08,288 --> 00:11:14,000
מחלק את הנתונים באופן אקראי למיני-אצטות ומחשב כל שלב ביחס ל- מיני אצווה.

127
00:11:14,000 --> 00:11:20,589
אם תעבור שוב ושוב על כל המיני-אצות ותבצע את ההתאמות האלה, תתכנס למינימום

128
00:11:20,589 --> 00:11:27,540
מקומי של פונקציית העלות, כלומר הרשת שלך תעשה עבודה ממש טובה בדוגמאות ההדרכה.

129
00:11:27,540 --> 00:11:32,563
אז עם כל זה, כל שורת קוד שתיכנס ליישום backprop למעשה

130
00:11:32,563 --> 00:11:37,680
מתכתבת עם משהו שראית עכשיו, לפחות במונחים לא פורמליים.

131
00:11:37,680 --> 00:11:41,134
אבל לפעמים לדעת מה המתמטיקה עושה זה רק חצי מהקרב, ורק

132
00:11:41,134 --> 00:11:44,780
מייצג את הדבר הארור הוא המקום שבו זה נהיה מבולבל ומבלבל.

133
00:11:44,780 --> 00:11:48,962
אז, לאלו מכם שכן רוצים להעמיק, הסרטון הבא עובר על אותם רעיונות

134
00:11:48,962 --> 00:11:53,078
שהוצגו כאן זה עתה, אבל במונחים של החשבון הבסיסי, מה שיש לקוות

135
00:11:53,078 --> 00:11:57,460
לעשות את זה קצת יותר מוכר כפי שאתם רואים את הנושא ב משאבים אחרים.

136
00:11:57,460 --> 00:12:02,084
לפני כן, דבר אחד שכדאי להדגיש הוא שכדי שהאלגוריתם הזה יעבוד, וזה מתאים

137
00:12:02,084 --> 00:12:06,840
לכל מיני למידת מכונה מעבר לרשתות עצביות בלבד, אתה צריך הרבה נתוני אימון.

138
00:12:06,840 --> 00:12:11,172
במקרה שלנו, דבר אחד שהופך ספרות בכתב יד לדוגמא כל כך נחמדה הוא שקיים

139
00:12:11,172 --> 00:12:15,380
מסד הנתונים של MNIST, עם כל כך הרבה דוגמאות שסומנו על ידי בני אדם.

140
00:12:15,380 --> 00:12:19,448
אז אתגר נפוץ שאלו מכם שעובדים בלמידת מכונה יכירו הוא פשוט לקבל את

141
00:12:19,448 --> 00:12:23,516
נתוני ההדרכה המסומנים להם אתם באמת צריכים, בין אם זה לגרום לאנשים

142
00:12:23,516 --> 00:12:27,400
לסמן עשרות אלפי תמונות, או כל סוג אחר שעמו אתם עשויים להתמודד.

