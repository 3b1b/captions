[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn. ",
  "translatedText": "כאן אנו מתמודדים עם התפשטות לאחור (Backpropagation), האלגוריתם המרכזי מאחורי האופן שבו רשתות עצביות לומדות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. ",
  "translatedText": "לאחר סיכום מהיר של המקום בו אנו נמצאים, הדבר הראשון שאעשה הוא הדרכה אינטואיטיבית למה שהאלגוריתם עושה בפועל, ללא כל התייחסות לנוסחאות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 9.4,
  "end": 17
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. ",
  "translatedText": "לאחר מכן, לאלו מכם שכן רוצים לצלול לתוך המתמטיקה, הסרטון הבא נכנס לחישובים שבבסיס כל זה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. ",
  "translatedText": "אם צפיתם בשני הסרטונים האחרונים, או אם אתם מגיעים עם הרקע המתאים, אתם יודעים מהי רשת נוירוננים וכיצד היא מזרימה מידע קדימה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 23.82,
  "end": 31
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. ",
  "translatedText": "כאן, אנחנו משתמשים בדוגמה הקלאסית של זיהוי ספרות בכתב יד שערכי הפיקסלים שלהן מוזנים לשכבה הראשונה של הרשת עם 784 נוירונים, ואני הצגתי רשת עם שתי שכבות נסתרות עם רק 16 נוירונים כל אחת, ושכבת פלט של 10 נוירונים, המציינת באיזו ספרה הרשת בוחרת כתשובה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. ",
  "translatedText": "אני גם מצפה שתבינו את הירידה בגרדיאנט, כפי שתוארה בסרטון האחרון, וכיצד כוונתנו בלמידה היא שאנחנו רוצים למצוא אילו משקלים והטיות ממזערים פונקציית עלות מסוימת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. ",
  "translatedText": "כתזכורת מהירה, בשביל עלות של דוגמה לאימון בודד, אתם לוקחים את הפלט שהרשת נותנת, יחד עם הפלט שרציתם שהיא תיתן, ומסכמים את הריבועים של ההבדלים בין כל רכיב. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. ",
  "translatedText": "אם תעשו זאת עבור כל עשרות אלפי דוגמאות האימון שלכם ותמצעו את התוצאות, תקבלו את העלות הכוללת של הרשת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. ",
  "translatedText": "כאילו זה לא מספיק כדי לחשוב עליו, כפי שתואר בסרטון האחרון, הדבר שאנו מחפשים הוא הגרדיאנט השלילי של פונקציית העלות הזו, שאומרת לכם כיצד עליכם לשנות את כל המשקלים וההטיות, כל החיבורים אלה, כדי להפחית את העלות בצורה היעילה ביותר. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. ",
  "translatedText": "התפשטות לאחור, הנושא של הסרטון הזה, הוא אלגוריתם לחישוב הגרדיאנט המסובך והמטורף הזה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "The one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. ",
  "translatedText": "הרעיון האחד מהסרטון האחרון שאני באמת רוצה שתזכרו טוב הוא שמכיוון שחשיבה על וקטור הגרדיאנט ככיוון ב-13,000 ממדים היא, בקלילות, מעבר לטווח הדמיון שלנו, יש עוד דרך איך שאתם יכולים לחשוב על זה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. ",
  "translatedText": "הגודל של כל רכיב כאן אומר לכם עד כמה פונקציית העלות רגישה לכל משקל והטיה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. ",
  "translatedText": "לדוגמה, נניח שאתם עוברים את התהליך שאני עומד לתאר, ומחשבים את הגרדיאנט השלילי, והרכיב המשויך למשקל בקצה הזה כאן יוצא 3.2, בעוד שהרכיב המשויך לקצה הזה כאן יוצא 0.1. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. ",
  "translatedText": "הדרך שבה תפרשו את זה היא שהעלות של הפונקציה רגישה פי 32 לשינויים במשקל הראשון, אז אם הייתם משנים קצת הערך שלו, זה יגרום לשינוי מסוים בעלות, והשינוי הזה גדול פי 32 ממה שאותו שינוי יחסי במשקל השני היה נותן. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and index chasing of it all. ",
  "translatedText": "באופן אישי, כשלמדתי לראשונה על התפשטות לאחור, אני חושב שההיבט המבלבל ביותר היה הסימונים, בעיקר של האידקסים.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. ",
  "translatedText": "אבל ברגע שאתם מבינים מה כל חלק באלגוריתם הזה עושה, כל אפקט פרטני שיש לו הוא למעשה די אינטואיטיבי, רק שיש הרבה התאמות קטנות שמשתלבות זו על גבי זו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. ",
  "translatedText": "אז אני אתחיל את העניינים כאן עם התעלמות מוחלטת מהסימונים, ופשוט אעבור על ההשפעות שיש לכל דוגמה לאימון על המשקלים וההטיות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. ",
  "translatedText": "מכיוון שפונקציית העלות כוללת ממוצע של העלות לכל דוגמה על פני כל עשרות אלפי דוגמאות האימון, הדרך בה אנחנו מתאימים את המשקלים וההטיות לשלב בודד של הירידה בגרדיאנט תלויה גם בכל דוגמה בודדת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. ",
  "translatedText": "או ליתר דיוק, באופן עקרוני זה מה שאמור להיות, אבל בשביל יעילות חישובית, נבצע טריק קטן מאוחר יותר כדי למנוע חזרה לכל דוגמה עבור כל צעד. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. ",
  "translatedText": "כרגע, כל מה שאנחנו הולכים לעשות הוא למקד את תשומת הלב שלנו בדוגמה אחת בודדת, תמונה זו של 2. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted? ",
  "translatedText": "איזו השפעה צריכה להיות לדוגמת האימון האחת הזו על כיוונון המשקלים וההטיות? ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. ",
  "translatedText": "נניח שאנחנו בנקודה שבה הרשת עדיין לא מאומנת היטב, אז ההפעלות בפלט הולכות להיראות די אקראיות, אולי משהו כמו 0.5, 0.8, 0.2, עוד ועוד. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 232.68,
  "end": 242
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases, but it's helpful to keep track of which adjustments we wish should take place to that output layer. ",
  "translatedText": "אנחנו לא יכולים לשנות ישירות את ההפעלות האלה, יש לנו רק השפעה על המשקלים וההטיות, אבל זה מועיל לעקוב אחר ההתאמות שאנחנו רוצים שיבוצעו בשכבת הפלט הזו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 242.52,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. ",
  "translatedText": "ומכיוון שאנחנו רוצים שהיא תסווג את התמונה כ-2, אנחנו רוצים שהערך השלישי הזה ישתנה כלפי מעלה בזמן שכל האחרים יקטנו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. ",
  "translatedText": "יתר על כן, הגדלים של השינויים הללו צריכים להיות פרופורציונליים למרחק של כל ערך נוכחי מערך היעד שלו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. ",
  "translatedText": "לדוגמה, הגידול בהפעלה של אותו נוירון מספר 2 חשובה במובן מסוים יותר מהירידה של נוירון מספר 8, שכבר די קרוב למקום בו הוא צריך להיות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. ",
  "translatedText": "אז בהגדלה נוספת, בואו נתמקד רק בנוירון האחד הזה, זה שאת ההפעלה שלו אנחנו רוצים להגביר. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. ",
  "translatedText": "זכרו, הפעלה מוגדרת כסכום משוקלל מסוים של כל ההפעלות בשכבה הקודמת, בתוספת הטיה, ואז הכל עובר למשהו כמו פונקציית הסיגמואיד, או ReLU. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation. ",
  "translatedText": "אז יש שלוש דרכים שונות שיכולות לחבור יחד כדי לעזור להגביר את ההפעלה הזו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. ",
  "translatedText": "אתם יכולים להגדיל את ההטיה, אתם יכולים להגדיל את המשקלים, ואתם יכול לשנות את ההפעלות מהשכבה הקודמת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. ",
  "translatedText": "בהתמקדות באופן שבו יש להתאים את המשקלים, שימו לב כיצד למשקלים יש רמות השפעה שונות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. ",
  "translatedText": "לחיבורים עם הנוירונים הבהירים ביותר מהשכבה הקודמת יש את ההשפעה הגדולה ביותר שכן משקלים אלה מוכפלים בערכי הפעלה גדולים יותר. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. ",
  "translatedText": "אז אם הייתם מגדילים אחד מהמשקלים האלה, למעשה יש לזה השפעה חזקה יותר על פונקציית העלות האולטימטיבית מאשר הגדלת משקלם של קשרים עם נוירונים עמומים יותר, לפחות בכל הנוגע לדוגמת האימון הזו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck. ",
  "translatedText": "זכרו, כאשר אנחנו מדברים על ירידה בגרדיאנט, לא אכפת לנו רק אם כל רכיב צריך לגדול או לקטון, אכפת לנו אילו מהם נותנים לכם הכי הרבה ערך. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together. ",
  "translatedText": "זה, אגב, מזכיר לפחות במידת מה תיאוריה במדעי המוח כיצד לומדות רשתות ביולוגיות של נוירונים, תיאוריה Hebbian, המסוכמת לעתים קרובות בביטוי \"נוירונים שיורים יחד מחווטים יחד\". ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active and the ones which we wish to become more active. ",
  "translatedText": "כאן, הגידול הרב ביותר למשקלים, החיזוק הגדול ביותר של הקשרים, מתרחשים בין נוירונים שהם הפעילים ביותר לבין אלו שאנחנו רוצים להפוך לפעילים יותר. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those are the ones firing when thinking about a 2. ",
  "translatedText": "במובן מסוים, הנוירונים שיורים בזמן שהם רואים 2 מקבלים קשר חזק יותר לאלו היורים כשחושבים על 2. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note. ",
  "translatedText": "שיהיה ברור, אני לא בעמדה להצהיר שרשתות מלאכותיות של נוירונים מתנהגות בדומה למוחות ביולוגיים, והרעיון של \"יורים יחד מחוברים יחד\" בא עם כמה כוכביות משמעותיות, אבל כאנלוגיה רופפת מאוד, אני מוצא שמעניין לציין את זה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer. ",
  "translatedText": "בכל מקרה, הדרך השלישית שבה נוכל לעזור להגביר את ההפעלה של הנוירון הזה היא על ידי שינוי כל ההפעלות בשכבה הקודמת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active. ",
  "translatedText": "כלומר, אם כל מה שקשור לאותו נוירון-ספרה-2 עם משקל חיובי נעשה בהיר יותר, ואם כל מה שקשור למשקל שלילי נעשה עמום, אז הנוירון -ספרה-2 הזה היה פעיל יותר. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. ",
  "translatedText": "ובדומה לשינויים במשקל, אתם הולכים לקבל את הערך המירבי על ידי חיפוש שינויים שהם פרופורציונליים לגודל המשקלים התואמים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases. ",
  "translatedText": "עכשיו, כמובן, אנחנו לא יכולים להשפיע ישירות על ההפעלות האלו, יש לנו רק שליטה על המשקלים וההטיות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are. ",
  "translatedText": "אבל בדיוק כמו בשכבה האחרונה, זה מועיל לרשום מה הם השינויים הרצויים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants. ",
  "translatedText": "אבל זכרו, במבט רחב יותר, זה רק מה שנוירון-ספרה-2 רוצה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. ",
  "translatedText": "זכרו, אנחנו גם רוצים שכל שאר הנוירונים בשכבה האחרונה יהפכו פחות פעילים, ולכל אחד מאותם נוירוני פלט אחרים יש מחשבות משלו לגבי מה צריך לקרות לשכבה הלפני אחרונה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. ",
  "translatedText": "אז הרצון של נוירון-ספרה-2 מתווסף יחד עם הרצונות של כל נוירוני הפלט האחרים למה שיקרה לשכבה הלפני אחרונה הזו, שוב ביחס למשקלים המתאימים, ובפרופורציה לכמה כל אחד מאותם נוירונים צריך לשנות. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in. ",
  "translatedText": "כאן בדיוק נכנס הרעיון של התפשטות לאחור. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. ",
  "translatedText": "על ידי חיבור כל האפקטים הרצויים הללו, אתם בעצם מקבלים רשימה של שינויים שאתם רוצים שיקרו לשכבה הלפני האחרונה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. ",
  "translatedText": "וברגע שיש לכם אותם, אתם יכולים להחיל את אותו תהליך רקורסיבי על המשקלים וההטיות הרלוונטיות שקובעות את הערכים האלה, לחזור על אותו תהליך שעברתי זה עתה ולנוע לאחור דרך הרשת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. ",
  "translatedText": "ובמבט כללי יותר, זכרו שכל זה הוא רק איך דוגמת אימון אחת רוצה לשנות את כל אחד מהמשקלים וההטיות הללו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 508.96,
  "end": 517
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2. ",
  "translatedText": "אם רק היינו מקשיבים למה שה-2 הזה רצה, בסופו של דבר הרשת תתומרץ רק כדי לסווג את כל התמונות כ-2. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes. ",
  "translatedText": "אז מה שאתם עושים זה לעבור אותו תהליך של התפשטות לאחור עבור כל דוגמת אימון, רושמים כיצד כל אחת מהן היתה רוצה לשנות את המשקלים וההטיות, ומחשבים את הממוצע של השינויים הרצויים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 524.06,
  "end": 536
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. ",
  "translatedText": "האוסף הזה כאן של השינויים הממוצעים לכל משקל והטיה הוא, באופן רופף, הגרדיאנט השלילי של פונקציית העלות שהוזכרה בסרטון האחרון, או לפחות משהו פרופורציונלי אליו. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. ",
  "translatedText": "אני אומר בצורה רופפת רק כי עדיין אני צריך להגיע לדיוק כמותי לגבי השינויים האלה, אבל אם הבנתם כל שינוי שהזכרתי, מדוע חלקם גדולים יותר מאחרים באופן פרופורציונלי, וכיצד צריך לחבר את כולם יחד, אתם מבינים את המכניקה של מה בעצם עושה ההתפשטות לאחור. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 554.38,
  "end": 571
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. ",
  "translatedText": "אגב, בפועל, לוקח למחשבים זמן רב במיוחד כדי לחבר את ההשפעה של כל דוגמת אימון בכל צעד בירידה בגרדיאנט. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead. ",
  "translatedText": "אז הנה מה שנהוג לעשות במקום. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and divide it into a whole bunch of mini-batches, let's say each one having 100 training examples. ",
  "translatedText": "אתם מערבבים באקראי את נתוני האימון שלכם ומחלקים אותם לחבורה שלמה של מיני-קבוצה, נניח שלכל אחת יש 100 דוגמאות אימון. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch. ",
  "translatedText": "ואז אתם מחשבים צעד לפי המיני-קבוצה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not going to be the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speedup. ",
  "translatedText": "זה לא הגרדיאנט האמיתי של פונקציית העלות, שתלוי בכל נתוני האימון, לא רק בתת-הקבוצה הקטנה הזו, אז זה לא הצעד היעיל ביותר בירידה, אבל כל מיני-קבוצה נותן לכם קירוב די טוב, וחשוב מכך, מזרז את החישוב באופן משמעותי. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. ",
  "translatedText": "אם הייתם מתווים את מסלול הרשת שלכם מתחת למשטח העלות הרלוונטי, זה יהיה קצת יותר כמו אדם שיכור המועד ללא מטרה במורד גבעה אך עושה צעדים מהירים, במקום אדם מחושב בקפידה שקובע את כיוון הירידה המדויק של כל צעד, לפני שיעשה צעד איטי וזהיר מאוד בכיוון הזה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent. ",
  "translatedText": "טכניקה זו מכונה ירידה סטוכסטית בגרדיאנט (stochastic gradient descent). ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we? ",
  "translatedText": "יש פה הרבה דברים, אז בואו נסכם את זה לעצמנו, בסדר? ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. ",
  "translatedText": "התפשטות לאחור היא האלגוריתם לקביעת האופן שבו דוגמה אחת לאימון תרצה לשנות את המשקלים וההטיות, לא רק במונחים של האם הם צריכים לעלות או לרדת, אלא במונחים של מה הפרופורציות היחסיות לשינויים האלה שגורמים לירידה המהירה ביותר בעֲלוּת. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes you get, but that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch. ",
  "translatedText": "צעד אמיתי בירידה בגרדיאנט יכלול ביצוע של כל עשרות ואלפי דוגמאות האימון שלכם ומיצוע של השינויים הרצויים שאתם מקבלים, אבל זה איטי מבחינה חישובית, אז במקום זאת אתם מחלקים את הנתונים באופן אקראי למיני-קבוצות ומחשבים כל שלב ביחס למיני-קבוצה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 656.26,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples. ",
  "translatedText": "אם תעבורו שוב ושוב על כל המיני-קבוצות ותבצעו את ההתאמות האלה, תתכנסו למינימום מקומי של פונקציית העלות, כלומר הרשת שלכם תעשה עבודה ממש טובה בדוגמאות ההדרכה. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 674,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms. ",
  "translatedText": "אז עם כל זה, כל שורת קוד שתיכנס ליישום התפשטות לאחור למעשה מתכתבת עם משהו שראיתם עכשיו, לפחות במונחים לא פורמליים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. ",
  "translatedText": "אבל לפעמים לדעת מה המתמטיקה עושה זה רק חצי מהקרב, ורק לייצג את הדבר הזה הוא המקום שבו זה נהיה מבולבל ומבלבל. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So, for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. ",
  "translatedText": "אז, לאלו מכם שכן רוצים להעמיק, הסרטון הבא עובר על אותם רעיונות שהוצגו כאן, אבל במונחים של החשבון הבסיסי, שיש לקוות שיעשה את זה קצת יותר מוכר, כפי שמוצג במקומות אחרים. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. ",
  "translatedText": "לפני כן, דבר אחד שכדאי להדגיש הוא שכדי שהאלגוריתם הזה יעבוד, וזה מתאים לכל מיני למידות מכונה ולא רק לרשתות נוירונים, אתם צריכים הרבה נתוני אימון. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans. ",
  "translatedText": "במקרה שלנו, דבר אחד שהופך ספרות בכתב יד לדוגמא כל כך טובה הוא שקיים מסד הנתונים של MNIST, עם כל כך הרבה דוגמאות שסומנו על ידי בני אדם. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with. ",
  "translatedText": "אז אתגר נפוץ שאלו מכם שעובדים בלמידת מכונה יכירו הוא פשוט לקבל את נתוני ההדרכה המסומנים שאתם באמת צריכים, בין אם זה לגרום לאנשים לסמן עשרות אלפי תמונות, או כל סוג אחר שאיתו אתם עשויים להתמודד. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 735.3,
  "end": 747.1
 }
]
