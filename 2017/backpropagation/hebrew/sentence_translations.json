[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn. ",
  "translatedText": "כאן אנו מתמודדים עם התפשטות לאחור, האלגוריתם המרכזי מאחורי האופן שבו רשתות עצביות לומדות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. ",
  "translatedText": "לאחר סיכום מהיר של המקום בו אנו נמצאים, הדבר הראשון שאעשה הוא הדרכה אינטואיטיבית למה שהאלגוריתם עושה בפועל, ללא כל התייחסות לנוסחאות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. ",
  "translatedText": "לאחר מכן, לאלו מכם שכן רוצים לצלול לתוך המתמטיקה, הסרטון הבא נכנס לחישוב שבבסיס כל זה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. ",
  "translatedText": "אם צפית בשני הסרטונים האחרונים, או אם אתה רק קופץ פנימה עם הרקע המתאים, אתה יודע מהי רשת עצבית וכיצד היא מזרימה מידע קדימה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. ",
  "translatedText": "כאן, אנחנו עושים את הדוגמה הקלאסית של זיהוי ספרות בכתב יד שערכי הפיקסלים שלהן מוזנים לשכבה הראשונה של הרשת עם 784 נוירונים, ואני הצגתי רשת עם שתי שכבות נסתרות עם רק 16 נוירונים כל אחת, ופלט שכבה של 10 נוירונים, המציינת באיזו ספרה הרשת בוחרת כתשובה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. ",
  "translatedText": "אני גם מצפה שתבינו את הירידה בשיפוע, כפי שתואר בסרטון האחרון, וכיצד כוונתנו בלמידה היא שאנו רוצים למצוא אילו משקלים והטיות ממזערים פונקציית עלות מסוימת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. ",
  "translatedText": "כתזכורת מהירה, בעלות של דוגמה לאימון בודד, אתה לוקח את הפלט שהרשת נותנת, יחד עם הפלט שרצית שהיא תיתן, ומסכמים את הריבועים של ההבדלים בין כל רכיב. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. ",
  "translatedText": "אם תעשה זאת עבור כל עשרות אלפי דוגמאות האימון שלך וממוצע התוצאות, זה נותן לך את העלות הכוללת של הרשת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. ",
  "translatedText": "כאילו זה לא מספיק כדי לחשוב עליו, כפי שתואר בסרטון האחרון, הדבר שאנו מחפשים הוא השיפוע השלילי של פונקציית העלות הזו, שאומר לך כיצד עליך לשנות את כל המשקלים וההטיות, כל חיבורים אלה, כדי להפחית את העלות בצורה היעילה ביותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. ",
  "translatedText": "התפשטות לאחור, הנושא של הסרטון הזה, הוא אלגוריתם לחישוב השיפוע המסובך והמטורף הזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "The one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. ",
  "translatedText": "הרעיון האחד מהסרטון האחרון שאני באמת רוצה שתחזיקו בחוזקה בראשכם עכשיו הוא שמכיוון שחשיבה על וקטור הגרדיאנט ככיוון ב-13,000 ממדים היא, בקלילות, מעבר לטווח הדמיון שלנו, יש עוד רעיון איך שאתה יכול לחשוב על זה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. ",
  "translatedText": "הגודל של כל רכיב כאן אומר לך עד כמה פונקציית העלות רגישה לכל משקל והטיה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. ",
  "translatedText": "לדוגמה, נניח שאתה עובר את התהליך שאני עומד לתאר, ומחשב את הגרדיאנט השלילי, והרכיב המשויך למשקל בקצה הזה כאן יוצא כ-3.2, בעוד שהרכיב המשויך לקצה הזה כאן יוצא כ-0.1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. ",
  "translatedText": "הדרך שבה תפרשו את זה היא שהעלות של הפונקציה רגישה פי 32 לשינויים במשקל הראשון הזה, אז אם הייתם מתנועעים קצת בערך הזה, זה יגרום לשינוי מסוים בעלות, ולשינוי הזה. גדול פי 32 ממה שאותו התנודדות למשקל השני היה נותן. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and index chasing of it all. ",
  "translatedText": "באופן אישי, כשלמדתי לראשונה על התפשטות לאחור, אני חושב שההיבט המבלבל ביותר היה רק המרדף אחר התווים והאינדקס של הכל. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. ",
  "translatedText": "אבל ברגע שאתה פותח את מה שכל חלק באלגוריתם הזה באמת עושה, כל אפקט אינדיבידואלי שיש לו הוא למעשה די אינטואיטיבי, רק שיש הרבה התאמות קטנות שמשתלבות זו על גבי זו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. ",
  "translatedText": "אז אני אתחיל את העניינים כאן עם התעלמות מוחלטת מהסימונים, ופשוט יעבור על ההשפעות שיש לכל דוגמה לאימון על המשקולות וההטיות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. ",
  "translatedText": "מכיוון שפונקציית העלות כוללת ממוצע של עלות מסוימת לכל דוגמה על פני כל עשרות אלפי דוגמאות האימון, הדרך בה אנו מתאימים את המשקולות וההטיות לשלב ירידה בשיפוע בודד תלויה גם בכל דוגמה בודדת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. ",
  "translatedText": "או ליתר דיוק, באופן עקרוני זה צריך, אבל בשביל יעילות חישובית, אנחנו נעשה טריק קטן מאוחר יותר כדי למנוע ממך להכות בכל דוגמה עבור כל צעד. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. ",
  "translatedText": "במקרים אחרים, כרגע, כל מה שאנחנו הולכים לעשות הוא למקד את תשומת הלב שלנו בדוגמה אחת בודדת, תמונה זו של 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted? ",
  "translatedText": "איזו השפעה צריכה להיות לדוגמה האימון האחת הזו על האופן שבו המשקולות וההטיות מתכווננות? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. ",
  "translatedText": "נניח שאנחנו בנקודה שבה הרשת עדיין לא מאומנת היטב, אז ההפעלה בפלט הולכות להיראות די אקראיות, אולי משהו כמו 0.5, 0.8, 0.2, עוד ועוד. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases, but it's helpful to keep track of which adjustments we wish should take place to that output layer. ",
  "translatedText": "אנחנו לא יכולים לשנות ישירות את ההפעלות האלה, יש לנו רק השפעה על המשקלים וההטיות, אבל זה מועיל לעקוב אחר ההתאמות שאנו רוצים שיבוצעו בשכבת הפלט הזו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 242.52,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. ",
  "translatedText": "ומכיוון שאנחנו רוצים שהוא יסווג את התמונה כ-2, אנחנו רוצים שהערך השלישי הזה יזוז כלפי מעלה בזמן שכל האחרים יתנוחו למטה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. ",
  "translatedText": "יתר על כן, הגדלים של הדחפים הללו צריכים להיות פרופורציונליים למרחק של כל ערך נוכחי מערך היעד שלו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. ",
  "translatedText": "לדוגמה, העלייה להפעלה של אותו נוירון מספר 2 חשובה במובן מסוים יותר מהירידה לנוירון מספר 8, שכבר די קרוב למקום בו הוא צריך להיות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. ",
  "translatedText": "אז בהתקרבות נוספת, בואו נתמקד רק בנוירון האחד הזה, זה שאת ההפעלה שלו אנחנו רוצים להגביר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. ",
  "translatedText": "זכור, הפעלה מוגדרת כסכום משוקלל מסוים של כל ההפעלות בשכבה הקודמת, בתוספת הטיה, שהכל מחובר לאחר מכן למשהו כמו פונקציית ה-squishification של הסיגמואידים, או ReLU. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation. ",
  "translatedText": "אז יש שלושה אפיקים שונים שיכולים לחבור יחד כדי לעזור להגביר את ההפעלה הזו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. ",
  "translatedText": "אתה יכול להגדיל את ההטיה, אתה יכול להגדיל את המשקולות, ואתה יכול לשנות את ההפעלות מהשכבה הקודמת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. ",
  "translatedText": "התמקדות באופן שבו יש להתאים את המשקולות, שימו לב כיצד למשקולות יש רמות השפעה שונות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. ",
  "translatedText": "לחיבורים עם הנוירונים הבהירים ביותר מהשכבה הקודמת יש את ההשפעה הגדולה ביותר שכן משקלים אלה מוכפלים בערכי הפעלה גדולים יותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. ",
  "translatedText": "אז אם הייתם מגדילים אחד מהמשקלים האלה, למעשה יש לזה השפעה חזקה יותר על פונקציית העלות האולטימטיבית מאשר הגדלת משקלם של קשרים עם נוירונים עמומים יותר, לפחות בכל הנוגע לדוגמא האימון האחת הזו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck. ",
  "translatedText": "זכור, כאשר אנו מדברים על ירידה בשיפוע, לא אכפת לנו רק אם כל רכיב צריך להיות דחף למעלה או למטה, אכפת לנו אילו מהם נותנים לך הכי הרבה כסף עבור הכסף שלך. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together. ",
  "translatedText": "זה, אגב, מזכיר לפחות במידת מה תיאוריה במדעי המוח כיצד לומדים רשתות ביולוגיות של נוירונים, תיאוריה העברית, המסוכמת לעתים קרובות בביטוי, נוירונים שיורים יחד חוט יחד. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active and the ones which we wish to become more active. ",
  "translatedText": "כאן, העליות הגדולות ביותר למשקולות, החיזוק הגדול ביותר של הקשרים, מתרחשת בין נוירונים שהם הפעילים ביותר לבין אלו שאנו רוצים להפוך לפעילים יותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those firing when thinking about it. ",
  "translatedText": "במובן מסוים, הנוירונים שיורים בזמן שהם רואים 2 מקבלים קשר חזק יותר לאלו היורים כשחושבים על זה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note. ",
  "translatedText": "שיהיה ברור, אני לא בעמדה להצהיר בצורה כזו או אחרת לגבי האם רשתות מלאכותיות של נוירונים מתנהגות משהו כמו מוחות ביולוגיים, והרעיון הזה מתחבר יחד עם כמה כוכביות משמעותיות, אבל נתפס כמשהו רופף מאוד. אנלוגיה, אני מוצא את זה מעניין לציין. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer. ",
  "translatedText": "בכל מקרה, הדרך השלישית שבה נוכל לעזור להגביר את ההפעלה של הנוירון הזה היא על ידי שינוי כל הפעלות בשכבה הקודמת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active. ",
  "translatedText": "כלומר, אם כל מה שקשור לאותו נוירון ספרה 2 עם משקל חיובי נעשה בהיר יותר, ואם כל מה שקשור למשקל שלילי נעשה עמום, אז הנוירון הספרה 2 הזה היה פעיל יותר. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. ",
  "translatedText": "ובדומה לשינויים במשקל, אתה הולך לקבל את המרב עבור הכסף שלך על ידי חיפוש שינויים שהם פרופורציונליים לגודל המשקולות התואמות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases. ",
  "translatedText": "עכשיו כמובן, אנחנו לא יכולים להשפיע ישירות על ההפעלות האלה, יש לנו רק שליטה על המשקולות וההטיות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are. ",
  "translatedText": "אבל בדיוק כמו בשכבה האחרונה, זה מועיל לרשום מה הם השינויים הרצויים האלה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants. ",
  "translatedText": "אבל זכור, בהרחקת שלב אחד כאן, זה רק מה שנוירון פלט ספרה 2 רוצה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. ",
  "translatedText": "זכרו, אנחנו גם רוצים שכל שאר הנוירונים בשכבה האחרונה יהפכו פחות פעילים, ולכל אחד מאותם נוירוני פלט אחרים יש מחשבות משלו לגבי מה צריך לקרות לשכבה השנייה אחרונה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. ",
  "translatedText": "אז הרצון של נוירון ספרה 2 זה מתווסף יחד עם הרצונות של כל נוירוני הפלט האחרים למה שיקרה לשכבה השנייה-אחרונה הזו, שוב ביחס למשקלים המתאימים, ובפרופורציה לכמה כל אחד מאותם נוירונים צריך לשנות. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in. ",
  "translatedText": "כאן בדיוק נכנס הרעיון של התפשטות לאחור. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. ",
  "translatedText": "על ידי הוספת כל האפקטים הרצויים הללו, אתה בעצם מקבל רשימה של דחיפות שאתה רוצה שיקרה לשכבה השנייה אחרונה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. ",
  "translatedText": "וברגע שיש לך כאלה, אתה יכול להחיל את אותו תהליך רקורסיבי על המשקולות וההטיות הרלוונטיות שקובעות את הערכים האלה, לחזור על אותו תהליך שעברתי זה עתה ולנוע לאחור דרך הרשת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. ",
  "translatedText": "ותרחיק קצת יותר, זכרו שזה הכל בדיוק איך דוגמה אחת לאימון רוצה להניע כל אחד מהמשקלים וההטיות הללו. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2. ",
  "translatedText": "אם רק היינו מקשיבים למה שהשניים האלה רוצים, בסופו של דבר הרשת תתמרץ רק כדי לסווג את כל התמונות כ-2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes. ",
  "translatedText": "אז מה שאתה עושה זה לעבור את אותה שגרת תמיכה בגב עבור כל דוגמה אחרת לאימון, לרשום כיצד כל אחד מהם היה רוצה לשנות את המשקולות וההטיות, ולבצע את הממוצע של השינויים הרצויים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. ",
  "translatedText": "האוסף הזה כאן של הדחפים הממוצעים לכל משקל והטיה הוא, באופן רופף, השיפוע השלילי של פונקציית העלות שהוזכרה בסרטון האחרון, או לפחות משהו פרופורציונלי אליה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. ",
  "translatedText": "אני אומר בצורה רופפת רק כי עדיין לא למדתי דיוק כמותי לגבי הדחפים האלה, אבל אם הבנת כל שינוי שהזכרתי זה עתה, מדוע חלקם גדולים יותר מאחרים באופן פרופורציונלי, וכיצד צריך להוסיף את כולם יחד, אתה מבין את המכניקה של מה בעצם עושה ההפצה לאחור. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. ",
  "translatedText": "אגב, בפועל, לוקח למחשבים זמן רב במיוחד כדי לחבר את ההשפעה של כל דוגמה לאימון בכל צעד בירידה בשיפוע. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead. ",
  "translatedText": "אז הנה מה שנהוג לעשות במקום. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and divide it into a whole bunch of mini-batches, let's say each one having 100 training examples. ",
  "translatedText": "אתה מערבב באקראי את נתוני האימון שלך ומחלק אותם לחבורה שלמה של מיני-אצט, נניח שלכל אחד יש 100 דוגמאות אימון. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch. ",
  "translatedText": "ואז אתה מחשב שלב לפי המיני-אצט. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly it gives you a significant computational speedup. ",
  "translatedText": "זה לא השיפוע האמיתי של פונקציית העלות, שתלוי בכל נתוני האימון, לא תת-הקבוצה הקטנה הזו, אז זה לא הצעד היעיל ביותר בירידה, אבל כל מיני-אצט נותן לך קירוב די טוב, וחשוב מכך. נותן לך זירוז חישוב משמעותי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. ",
  "translatedText": "אם היית מתווה את מסלול הרשת שלך מתחת למשטח העלות הרלוונטי, זה יהיה קצת יותר כמו אדם שיכור המועד ללא מטרה במורד גבעה אך עושה צעדים מהירים, במקום אדם מחושב בקפידה שקובע את כיוון הירידה המדויק של כל צעד. לפני שתעשה צעד איטי וזהיר מאוד בכיוון הזה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent. ",
  "translatedText": "טכניקה זו מכונה ירידה בשיפוע סטוכסטי. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we? ",
  "translatedText": "יש פה הרבה דברים, אז בואו נסכם את זה לעצמנו, נכון? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. ",
  "translatedText": "התפשטות לאחור הוא האלגוריתם לקביעת האופן שבו דוגמה אחת לאימון תרצה להניע את המשקולות וההטיות, לא רק במונחים של האם הם צריכים לעלות או לרדת, אלא במונחים של מה הפרופורציות היחסיות לשינויים האלה שגורמים לירידה המהירה ביותר ל- עֲלוּת. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes you get, but that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch. ",
  "translatedText": "שלב ירידה שיפוע אמיתי יכלול ביצוע של כל עשרות ואלפי דוגמאות האימון שלך וממוצע של השינויים הרצויים שאתה מקבל, אבל זה איטי מבחינה חישובית, אז במקום זאת אתה מחלק את הנתונים באופן אקראי למיני-אצטות ומחשב כל שלב ביחס ל- מיני אצווה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.26,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples. ",
  "translatedText": "אם תעבור שוב ושוב על כל המיני-אצות ותבצע את ההתאמות האלה, תתכנס למינימום מקומי של פונקציית העלות, כלומר הרשת שלך תעשה עבודה ממש טובה בדוגמאות ההדרכה. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms. ",
  "translatedText": "אז עם כל זה, כל שורת קוד שתיכנס ליישום backprop למעשה מתכתבת עם משהו שראית עכשיו, לפחות במונחים לא פורמליים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. ",
  "translatedText": "אבל לפעמים לדעת מה המתמטיקה עושה זה רק חצי מהקרב, ורק מייצג את הדבר הארור הוא המקום שבו זה נהיה מבולבל ומבלבל. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So, for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. ",
  "translatedText": "אז, לאלו מכם שכן רוצים להעמיק, הסרטון הבא עובר על אותם רעיונות שהוצגו כאן זה עתה, אבל במונחים של החשבון הבסיסי, מה שיש לקוות לעשות את זה קצת יותר מוכר כפי שאתם רואים את הנושא ב משאבים אחרים. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. ",
  "translatedText": "לפני כן, דבר אחד שכדאי להדגיש הוא שכדי שהאלגוריתם הזה יעבוד, וזה מתאים לכל מיני למידת מכונה מעבר לרשתות עצביות בלבד, אתה צריך הרבה נתוני אימון. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans. ",
  "translatedText": "במקרה שלנו, דבר אחד שהופך ספרות בכתב יד לדוגמא כל כך נחמדה הוא שקיים מסד הנתונים של MNIST, עם כל כך הרבה דוגמאות שסומנו על ידי בני אדם. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with. ",
  "translatedText": "אז אתגר נפוץ שאלו מכם שעובדים בלמידת מכונה יכירו הוא פשוט לקבל את נתוני ההדרכה המסומנים להם אתם באמת צריכים, בין אם זה לגרום לאנשים לסמן עשרות אלפי תמונות, או כל סוג אחר שעמו אתם עשויים להתמודד. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]