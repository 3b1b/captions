1
00:00:04,059 --> 00:00:06,614
Здесь мы занимаемся обратным распространением ошибки 

2
00:00:06,614 --> 00:00:08,880
— основным алгоритмом обучения нейронных сетей.

3
00:00:09,400 --> 00:00:12,319
После краткого обзора того, где мы находимся, первое, что я сделаю, 

4
00:00:12,319 --> 00:00:15,583
— это интуитивно понятное описание того, что на самом деле делает алгоритм, 

5
00:00:15,583 --> 00:00:17,000
без каких-либо ссылок на формулы.

6
00:00:17,660 --> 00:00:20,187
Затем для тех из вас, кто хочет погрузиться в математику, 

7
00:00:20,187 --> 00:00:23,020
следующее видео посвящено расчетам, лежащим в основе всего этого.

8
00:00:23,820 --> 00:00:27,387
Если вы смотрели два последних видео или просто знакомитесь с соответствующей 

9
00:00:27,387 --> 00:00:31,000
информацией, вы знаете, что такое нейронная сеть и как она передает информацию.

10
00:00:31,680 --> 00:00:35,519
Здесь мы приводим классический пример распознавания рукописных цифр, 

11
00:00:35,519 --> 00:00:39,581
значения пикселей которых передаются в первый слой сети с 784 нейронами, 

12
00:00:39,581 --> 00:00:44,143
и я показываю сеть с двумя скрытыми слоями, имеющими всего по 16 нейронов каждый, 

13
00:00:44,143 --> 00:00:49,040
и выходом слой из 10 нейронов, указывающий, какую цифру сеть выбирает в качестве ответа.

14
00:00:50,040 --> 00:00:54,388
Я также ожидаю, что вы поймете градиентный спуск, как описано в последнем видео, 

15
00:00:54,388 --> 00:00:57,716
и то, что под обучением мы подразумеваем то, что хотим найти, 

16
00:00:57,716 --> 00:01:01,260
какие веса и смещения минимизируют определенную функцию стоимости.

17
00:01:02,040 --> 00:01:06,134
Напоминаем, что для расчета стоимости одного обучающего примера вы берете 

18
00:01:06,134 --> 00:01:09,509
выходные данные, которые дает сеть, а также выходные данные, 

19
00:01:09,509 --> 00:01:13,548
которые вы хотели от нее получить, и складываете квадраты различий между 

20
00:01:13,548 --> 00:01:14,600
каждым компонентом.

21
00:01:15,380 --> 00:01:20,292
Проделав это для всех ваших десятков тысяч обучающих примеров и усреднив результаты, 

22
00:01:20,292 --> 00:01:22,200
вы получите общую стоимость сети.

23
00:01:22,200 --> 00:01:27,023
И как будто этого недостаточно для размышлений, как описано в последнем видео, 

24
00:01:27,023 --> 00:01:31,664
мы ищем отрицательный градиент этой функции стоимости, которая говорит вам, 

25
00:01:31,664 --> 00:01:35,633
как вам нужно изменить все веса и смещения, все этих соединений, 

26
00:01:35,633 --> 00:01:38,320
чтобы наиболее эффективно снизить стоимость.

27
00:01:43,260 --> 00:01:45,520
Обратное распространение ошибки, тема этого видео, 

28
00:01:45,520 --> 00:01:48,580
представляет собой алгоритм расчета этого безумно сложного градиента.

29
00:01:49,140 --> 00:01:52,546
И единственная идея из последнего видео, которую я действительно хочу, 

30
00:01:52,546 --> 00:01:55,616
чтобы вы прямо сейчас твердо запомнили, заключается в том, что, 

31
00:01:55,616 --> 00:01:59,214
поскольку думать о векторе градиента как о направлении в 13 000 измерений, 

32
00:01:59,214 --> 00:02:01,469
мягко говоря, за пределами нашего воображения, 

33
00:02:01,469 --> 00:02:03,580
существует еще один способ подумать об этом.

34
00:02:04,600 --> 00:02:07,191
Величина каждого компонента здесь говорит вам, 

35
00:02:07,191 --> 00:02:10,940
насколько чувствительна функция стоимости к каждому весу и смещению.

36
00:02:11,800 --> 00:02:16,210
Например, предположим, что вы выполняете процесс, который я собираюсь описать, 

37
00:02:16,210 --> 00:02:20,844
и вычисляете отрицательный градиент, и компонент, связанный с весом на этом ребре, 

38
00:02:20,844 --> 00:02:25,031
оказывается равным 3,2, в то время как компонент, связанный с этим ребром, 

39
00:02:25,031 --> 00:02:26,260
равен выходит как 0,1.

40
00:02:26,820 --> 00:02:29,572
Вы можете интерпретировать это следующим образом: 

41
00:02:29,572 --> 00:02:33,701
стоимость функции в 32 раза более чувствительна к изменениям первого веса, 

42
00:02:33,701 --> 00:02:36,343
поэтому, если вы немного измените это значение, 

43
00:02:36,343 --> 00:02:40,802
это приведет к некоторому изменению стоимости, и это Изменение в 32 раза больше, 

44
00:02:40,802 --> 00:02:43,060
чем то же самое покачивание второго веса.

45
00:02:48,420 --> 00:02:51,892
Лично, когда я впервые узнал об обратном распространении ошибки, я думаю, 

46
00:02:51,892 --> 00:02:55,740
что самым запутанным аспектом была сама нотация и погоня за индексом во всем этом.

47
00:02:56,220 --> 00:03:00,021
Но как только вы разберетесь, что на самом деле делает каждая часть этого алгоритма, 

48
00:03:00,021 --> 00:03:03,688
каждый отдельный эффект, который он оказывает, на самом деле довольно интуитивен, 

49
00:03:03,688 --> 00:03:06,640
просто есть много маленьких настроек, накладываемых друг на друга.

50
00:03:07,740 --> 00:03:11,957
Итак, я собираюсь начать здесь с полного игнорирования обозначений и просто 

51
00:03:11,957 --> 00:03:16,120
пошагово рассмотреть влияние каждого обучающего примера на веса и смещения.

52
00:03:17,020 --> 00:03:21,526
Поскольку функция стоимости включает в себя усреднение определенной стоимости на 

53
00:03:21,526 --> 00:03:24,530
пример по всем десяткам тысяч обучающих примеров, то, 

54
00:03:24,530 --> 00:03:28,592
как мы корректируем веса и смещения для одного шага градиентного спуска, 

55
00:03:28,592 --> 00:03:31,040
также зависит от каждого отдельного примера.

56
00:03:31,680 --> 00:03:35,528
Вернее, в принципе так и должно быть, но для эффективности вычислений мы позже сделаем 

57
00:03:35,528 --> 00:03:39,200
небольшой трюк, чтобы вам не приходилось обрабатывать каждый пример на каждом шаге.

58
00:03:39,200 --> 00:03:42,041
В других случаях прямо сейчас все, что нам нужно сделать, 

59
00:03:42,041 --> 00:03:45,960
это сосредоточить внимание на одном-единственном примере, на этом изображении 2.

60
00:03:46,720 --> 00:03:51,480
Какое влияние должен оказать этот обучающий пример на корректировку весов и смещений?

61
00:03:52,680 --> 00:03:56,163
Допустим, мы находимся в точке, где сеть еще недостаточно хорошо обучена, 

62
00:03:56,163 --> 00:03:59,599
поэтому активации в выходных данных будут выглядеть довольно случайными, 

63
00:03:59,599 --> 00:04:02,000
может быть, что-то вроде 0,5, 0,8, 0,2 и так далее.

64
00:04:02,520 --> 00:04:04,967
Мы не можем напрямую изменить эти активации, мы 

65
00:04:04,967 --> 00:04:07,160
можем влиять только на веса и предвзятости.

66
00:04:07,160 --> 00:04:12,580
Но полезно отслеживать, какие изменения мы хотим внести в этот выходной слой.

67
00:04:13,360 --> 00:04:17,500
А поскольку мы хотим, чтобы изображение классифицировалось как 2, мы хотим, 

68
00:04:17,500 --> 00:04:21,260
чтобы это третье значение было смещено вверх, а все остальные — вниз.

69
00:04:22,060 --> 00:04:25,815
Более того, размеры этих подталкиваний должны быть пропорциональны тому, 

70
00:04:25,815 --> 00:04:29,520
насколько далеко каждое текущее значение находится от целевого значения.

71
00:04:30,220 --> 00:04:35,041
Например, увеличение активации нейрона номер 2 в некотором смысле более важно, 

72
00:04:35,041 --> 00:04:39,740
чем снижение активности нейрона номер 8, который уже довольно близок к тому, 

73
00:04:39,740 --> 00:04:40,900
где он должен быть.

74
00:04:42,040 --> 00:04:45,486
Итак, увеличивая масштаб, давайте сосредоточимся только на этом нейроне, 

75
00:04:45,486 --> 00:04:47,280
том, чью активацию мы хотим увеличить.

76
00:04:48,180 --> 00:04:52,344
Помните, что эта активация определяется как определенная взвешенная 

77
00:04:52,344 --> 00:04:55,834
сумма всех активаций на предыдущем уровне плюс смещение, 

78
00:04:55,834 --> 00:05:01,040
которое затем подключается к чему-то вроде функции сжатия сигмовидной кишки или ReLU.

79
00:05:01,640 --> 00:05:07,020
Итак, есть три разных направления, которые можно объединить, чтобы повысить эту активацию.

80
00:05:07,440 --> 00:05:14,040
Вы можете увеличить смещение, увеличить веса и изменить активации предыдущего слоя.

81
00:05:14,940 --> 00:05:17,551
Сосредоточив внимание на том, как следует настраивать веса, 

82
00:05:17,551 --> 00:05:20,860
обратите внимание на то, что на самом деле веса имеют разные уровни влияния.

83
00:05:21,440 --> 00:05:25,488
Связи с самыми яркими нейронами предыдущего слоя имеют наибольший эффект, 

84
00:05:25,488 --> 00:05:29,100
поскольку эти веса умножаются на более высокие значения активации.

85
00:05:31,460 --> 00:05:34,018
Таким образом, если вы увеличите один из этих весов, 

86
00:05:34,018 --> 00:05:37,783
это на самом деле окажет более сильное влияние на конечную функцию стоимости, 

87
00:05:37,783 --> 00:05:41,645
чем увеличение весов связей с более тусклыми нейронами, по крайней мере, в том, 

88
00:05:41,645 --> 00:05:43,480
что касается этого обучающего примера.

89
00:05:44,420 --> 00:05:47,477
Помните, когда мы говорим о градиентном спуске, нас волнует не только то, 

90
00:05:47,477 --> 00:05:50,369
должен ли каждый компонент сдвигаться вверх или вниз, нас заботит то, 

91
00:05:50,369 --> 00:05:53,220
какие из них принесут вам максимальную отдачу от затраченных средств.

92
00:05:55,020 --> 00:05:58,664
Это, кстати, по крайней мере чем-то напоминает теорию нейронауки о том, 

93
00:05:58,664 --> 00:06:03,169
как обучаются биологические сети нейронов, теорию Хебба, которую часто суммируют фразой: 

94
00:06:03,169 --> 00:06:06,460
«нейроны, которые срабатывают вместе, соединяются друг с другом».

95
00:06:07,260 --> 00:06:13,013
Здесь наибольшее увеличение веса, наибольшее усиление связей происходит между нейронами, 

96
00:06:13,013 --> 00:06:17,280
которые наиболее активны, и теми, которые мы хотим активизировать.

97
00:06:17,940 --> 00:06:21,210
В каком-то смысле нейроны, которые срабатывают, когда видят цифру 2, 

98
00:06:21,210 --> 00:06:24,480
становятся более прочно связанными с ними, когда мы думаем о цифре 2.

99
00:06:25,400 --> 00:06:28,996
Чтобы внести ясность, я не могу так или иначе делать заявления о том, 

100
00:06:28,996 --> 00:06:32,696
ведут ли искусственные сети нейронов что-то вроде биологического мозга, 

101
00:06:32,696 --> 00:06:37,166
и эта идея «сжигает вместе, соединяет вместе» сопровождается парой значащих звездочек, 

102
00:06:37,166 --> 00:06:41,020
но воспринимается как очень расплывчатая. аналогию, мне интересно отметить.

103
00:06:41,940 --> 00:06:45,521
В любом случае, третий способ увеличить активацию этого 

104
00:06:45,521 --> 00:06:49,040
нейрона — это изменить все активации в предыдущем слое.

105
00:06:49,040 --> 00:06:53,457
А именно, если все, что связано с нейроном цифры 2 с положительным весом, 

106
00:06:53,457 --> 00:06:57,038
станет ярче, а если все, что связано с отрицательным весом, 

107
00:06:57,038 --> 00:07:00,680
станет тусклее, то этот нейрон цифры 2 станет более активным.

108
00:07:02,540 --> 00:07:06,531
И, как и в случае с изменением веса, вы получите максимальную отдачу от вложенных 

109
00:07:06,531 --> 00:07:10,280
средств, добиваясь изменений, пропорциональных размеру соответствующих весов.

110
00:07:12,140 --> 00:07:14,910
Конечно, мы не можем напрямую влиять на эти активации, 

111
00:07:14,910 --> 00:07:17,480
мы можем контролировать только веса и предвзятости.

112
00:07:17,480 --> 00:07:24,120
Но, как и в случае с последним слоем, полезно запомнить желаемые изменения.

113
00:07:24,580 --> 00:07:27,724
Но имейте в виду, что уменьшение масштаба здесь на один шаг — это всего лишь то, 

114
00:07:27,724 --> 00:07:29,200
чего хочет выходной нейрон с цифрой 2.

115
00:07:29,760 --> 00:07:33,055
Помните, мы также хотим, чтобы все остальные нейроны последнего слоя 

116
00:07:33,055 --> 00:07:36,351
стали менее активными, и каждый из этих выходных нейронов имеет свои 

117
00:07:36,351 --> 00:07:39,600
собственные мысли о том, что должно произойти с предпоследним слоем.

118
00:07:42,700 --> 00:07:47,448
Итак, желание этого нейрона с цифрой 2 суммируется с желаниями всех остальных 

119
00:07:47,448 --> 00:07:52,684
выходных нейронов относительно того, что должно произойти с этим предпоследним слоем, 

120
00:07:52,684 --> 00:07:57,006
опять же пропорционально соответствующим весам и пропорционально тому, 

121
00:07:57,006 --> 00:08:00,720
насколько сильно каждый из этих нейронов необходимо изменить.

122
00:08:01,600 --> 00:08:05,480
Именно здесь возникает идея обратного распространения.

123
00:08:05,820 --> 00:08:10,393
Сложив все эти желаемые эффекты, вы, по сути, получаете список изменений, 

124
00:08:10,393 --> 00:08:13,360
которые вы хотите сделать с предпоследним слоем.

125
00:08:14,220 --> 00:08:17,640
И как только они у вас появятся, вы сможете рекурсивно применить тот же 

126
00:08:17,640 --> 00:08:21,346
процесс к соответствующим весам и смещениям, которые определяют эти значения, 

127
00:08:21,346 --> 00:08:25,100
повторяя тот же процесс, который я только что прошел, и двигаясь назад по сети.

128
00:08:28,960 --> 00:08:32,539
И, немного уменьшив масштаб, помните, что все это всего лишь то, 

129
00:08:32,539 --> 00:08:37,000
как один обучающий пример хочет подтолкнуть каждый из этих весов и предубеждений.

130
00:08:37,480 --> 00:08:39,617
Если бы мы слушали только то, чего хочет эта цифра 2, 

131
00:08:39,617 --> 00:08:42,507
сеть в конечном итоге была бы заинтересована просто классифицировать все 

132
00:08:42,507 --> 00:08:43,220
изображения как 2.

133
00:08:44,059 --> 00:08:48,095
Итак, вы выполняете одну и ту же процедуру обратного распространения для 

134
00:08:48,095 --> 00:08:50,693
каждого другого обучающего примера, записывая, 

135
00:08:50,693 --> 00:08:53,623
как каждый из них хотел бы изменить веса и смещения, 

136
00:08:53,623 --> 00:08:56,000
и усредняете вместе эти желаемые изменения.

137
00:09:01,720 --> 00:09:05,979
Этот набор усредненных подталкиваний к каждому весу и смещению, грубо говоря, 

138
00:09:05,979 --> 00:09:09,311
представляет собой отрицательный градиент функции стоимости, 

139
00:09:09,311 --> 00:09:13,680
упомянутой в последнем видео, или, по крайней мере, что-то пропорциональное ему.

140
00:09:14,380 --> 00:09:17,995
Я говорю в общих чертах только потому, что мне еще предстоит получить точную 

141
00:09:17,995 --> 00:09:21,657
количественную оценку этих подталкиваний, но если вы поняли каждое изменение, 

142
00:09:21,657 --> 00:09:25,741
о котором я только что упомянул, почему некоторые из них пропорционально больше других 

143
00:09:25,741 --> 00:09:29,920
и как их все нужно сложить вместе, вы поймете механизм что на самом деле делает обратное 

144
00:09:29,920 --> 00:09:31,000
распространение ошибки.

145
00:09:33,960 --> 00:09:37,784
Кстати, на практике компьютерам требуется чрезвычайно много времени, 

146
00:09:37,784 --> 00:09:42,440
чтобы сложить влияние каждого обучающего примера на каждом шаге градиентного спуска.

147
00:09:43,140 --> 00:09:44,820
Итак, вот что обычно делают вместо этого.

148
00:09:45,480 --> 00:09:48,011
Вы случайным образом перемешиваете свои обучающие данные, 

149
00:09:48,011 --> 00:09:50,368
а затем делите их на целую кучу мини-пакетов, скажем, 

150
00:09:50,368 --> 00:09:52,420
каждый из которых имеет 100 обучающих примеров.

151
00:09:52,940 --> 00:09:56,200
Затем вы вычисляете шаг в соответствии с мини-пакетом.

152
00:09:56,960 --> 00:09:59,611
Это не будет фактический градиент функции стоимости, 

153
00:09:59,611 --> 00:10:03,664
который зависит от всех обучающих данных, а не от этого крошечного подмножества, 

154
00:10:03,664 --> 00:10:07,566
так что это не самый эффективный шаг вниз, но каждый мини-пакет дает довольно 

155
00:10:07,566 --> 00:10:11,069
хорошее приближение, и что еще более важно, это дает вам значительное 

156
00:10:11,069 --> 00:10:12,120
ускорение вычислений.

157
00:10:12,820 --> 00:10:16,780
Если бы вы построили траекторию своей сети по соответствующей поверхности затрат, 

158
00:10:16,780 --> 00:10:20,982
это было бы больше похоже на пьяного мужчину, бесцельно спотыкающегося вниз по склону, 

159
00:10:20,982 --> 00:10:24,218
но делающего быстрые шаги, чем на тщательно расчетливого человека, 

160
00:10:24,218 --> 00:10:26,730
определяющего точное направление каждого шага вниз. 

161
00:10:26,730 --> 00:10:30,160
прежде чем сделать очень медленный и осторожный шаг в этом направлении.

162
00:10:31,540 --> 00:10:34,660
Этот метод называется стохастическим градиентным спуском.

163
00:10:35,960 --> 00:10:39,620
Здесь много всего происходит, так что давайте просто подведем итоги для себя, ладно?

164
00:10:40,440 --> 00:10:43,501
Обратное распространение ошибки — это алгоритм определения того, 

165
00:10:43,501 --> 00:10:46,751
как отдельный обучающий пример хотел бы подтолкнуть веса и смещения, 

166
00:10:46,751 --> 00:10:50,190
не только с точки зрения того, должны ли они повышаться или уменьшаться, 

167
00:10:50,190 --> 00:10:53,958
но и с точки зрения того, какие относительные пропорции этих изменений вызывают 

168
00:10:53,958 --> 00:10:55,560
наиболее быстрое снижение расходы.

169
00:10:56,260 --> 00:11:00,426
Настоящий шаг градиентного спуска предполагает проделать это для всех ваших десятков 

170
00:11:00,426 --> 00:11:04,200
тысяч обучающих примеров и усреднить желаемые изменения, которые вы получите.

171
00:11:04,860 --> 00:11:09,197
Но это требует больших вычислительных затрат, поэтому вместо этого вы случайным образом 

172
00:11:09,197 --> 00:11:13,240
разделяете данные на мини-пакеты и вычисляете каждый шаг относительно мини-пакета.

173
00:11:14,000 --> 00:11:17,585
Неоднократно проходя все мини-пакеты и внося эти корректировки, 

174
00:11:17,585 --> 00:11:20,554
вы дойдете до локального минимума функции стоимости, 

175
00:11:20,554 --> 00:11:25,540
то есть ваша сеть в конечном итоге действительно хорошо справится с обучающими примерами.

176
00:11:27,240 --> 00:11:29,674
Итак, несмотря на все вышесказанное, каждая строка кода, 

177
00:11:29,674 --> 00:11:32,705
которая будет использоваться для реализации обратного распространения, 

178
00:11:32,705 --> 00:11:35,823
на самом деле соответствует тому, что вы сейчас видели, по крайней мере, 

179
00:11:35,823 --> 00:11:36,720
в неформальном плане.

180
00:11:37,560 --> 00:11:40,679
Но иногда знание того, что делает математика, — это только полдела, 

181
00:11:40,679 --> 00:11:44,120
и простое представление этой чертовой штуки приводит к путанице и путанице.

182
00:11:44,860 --> 00:11:48,657
Итак, для тех из вас, кто хочет углубиться, в следующем видео рассматриваются те же идеи, 

183
00:11:48,657 --> 00:11:52,201
которые только что были представлены здесь, но с точки зрения основного исчисления, 

184
00:11:52,201 --> 00:11:54,479
что, мы надеемся, сделает его немного более знакомым, 

185
00:11:54,479 --> 00:11:56,420
поскольку вы видите эту тему в других Ресурсы.

186
00:11:57,340 --> 00:11:59,691
Перед этим стоит подчеркнуть одну вещь: для того, 

187
00:11:59,691 --> 00:12:03,219
чтобы этот алгоритм работал, и это касается всех видов машинного обучения, 

188
00:12:03,219 --> 00:12:05,900
помимо нейронных сетей, вам нужно много обучающих данных.

189
00:12:06,420 --> 00:12:09,998
В нашем случае, одна вещь, которая делает рукописные цифры таким хорошим примером, 

190
00:12:09,998 --> 00:12:13,489
— это то, что существует база данных MNIST с таким большим количеством примеров, 

191
00:12:13,489 --> 00:12:14,740
которые были помечены людьми.

192
00:12:15,300 --> 00:12:18,069
Таким образом, распространенная задача, с которой знакомы те из вас, 

193
00:12:18,069 --> 00:12:21,521
кто занимается машинным обучением, — это просто получить помеченные обучающие данные, 

194
00:12:21,521 --> 00:12:24,210
которые вам действительно нужны, будь то маркировка десятков тысяч 

195
00:12:24,210 --> 00:12:27,100
изображений или любой другой тип данных, с которым вы можете иметь дело.

