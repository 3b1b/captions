[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn.",
  "translatedText": "Aquí abordamos la retropropagación, el algoritmo central detrás de cómo aprenden las redes neuronales.",
  "from_community_srt": "Aquí vamos a abordar la \"Retropropagación\", el algoritmo central por el cual las redes neuronales aprenden.",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas.",
  "translatedText": "Después de un rápido resumen de dónde nos encontramos, lo primero que haré es un recorrido intuitivo de lo que realmente hace el algoritmo, sin ninguna referencia a las fórmulas.",
  "from_community_srt": "después de una pequeña  recapitulación de donde estábamos, la primera cosa que voy a hacer es una demostración intuitiva de lo que el algoritmo realmente esta haciendo sin referencia alguna a la fórmula.",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this.",
  "translatedText": "Luego, para aquellos de ustedes que quieran sumergirse en las matemáticas, el siguiente video analiza el cálculo subyacente a todo esto.",
  "from_community_srt": "Luego para los que quieran ir a la matemática, el siguiente vídeo va sobre la matemática detrás de esto.",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information.",
  "translatedText": "Si vio los dos últimos videos, o si simplemente está comenzando con el fondo apropiado, sabrá qué es una red neuronal y cómo transmite información.",
  "from_community_srt": "Si viste los últimos dos videos, o si los saltaste con el apropiado conocimiento previo, tú sabes que es una red neuronal y cómo se ajusta con la información.",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer.",
  "translatedText": "Aquí, estamos haciendo el ejemplo clásico de reconocer dígitos escritos a mano cuyos valores de píxeles se introducen en la primera capa de la red con 784 neuronas, y he estado mostrando una red con dos capas ocultas que tienen solo 16 neuronas cada una y una salida. capa de 10 neuronas, que indica qué dígito está eligiendo la red como respuesta.",
  "from_community_srt": "Lo que estamos haciendo aquí es el ejemplo clásico de reconocimiento de dígitos escritos a mano, cuyos valores de pixeles se ocultan  en la primera capa  de 784 neuronas. He mostrado una una red con dis capa de solo 16 neuronas cada una, y una capa de salida con 10 neuronas, indicando qué dígitos la red esa eligiendo como respuesta.",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function.",
  "translatedText": "También espero que comprenda el descenso de gradiente, como se describe en el último video, y cómo lo que queremos decir con aprendizaje es que queremos encontrar qué pesos y sesgos minimizan una determinada función de costo.",
  "from_community_srt": "También espero que entiendas que es el descenso de gradiente el cual fue descrito en el último vídeo, y lo que queremos dar a entender por \"Aprender\", es que queremos encontrar cuales pesos  y BIASES minimizan una función de coste.",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component.",
  "translatedText": "Como recordatorio rápido, por el costo de un solo ejemplo de capacitación, se toma el resultado que brinda la red, junto con el resultado que deseaba que brindara, y se suman los cuadrados de las diferencias entre cada componente.",
  "from_community_srt": "Como un pequeño recordatorio, para el coste de un entrenamiento en particular, lo que haces es tomar el output que la red da , y junto con la salida que quieres que ella de, solo elevas al cuadrado la diferencia entre cada componente.",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network.",
  "translatedText": "Al hacer esto para todas sus decenas de miles de ejemplos de capacitación y promediar los resultados, obtendrá el costo total de la red.",
  "from_community_srt": "Haciendo esto para todos los  miles ejemplos de datos de entrenamiento, y promediando los resultados, esto te da el costo total de la red.",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "And as if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost.",
  "translatedText": "Y como si eso no fuera suficiente en qué pensar, como se describe en el último video, lo que estamos buscando es el gradiente negativo de esta función de costo, que le indica cómo necesita cambiar todos los pesos y sesgos, todos de estas conexiones, para disminuir el costo de la manera más eficiente.",
  "from_community_srt": "Y si eso no es suficiente para pensar, como fue descrito en el último video, lo que estamos buscando es la gradiente negativa de esta función de coste que te diga lo que necesitas cambiar en los pesos y las BIASES en estas conexiones , de manera que se reduzca el coste mas eficientemente .",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient.",
  "translatedText": "La retropropagación, el tema de este vídeo, es un algoritmo para calcular ese gradiente increíblemente complicado.",
  "from_community_srt": "La retropropagación , el tema de este vídeo , es el algoritmo para calcular es gradiente locamente complicada",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "And the one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it.",
  "translatedText": "Y la única idea del último vídeo que realmente quiero que tengan firmemente presente en este momento es que, dado que pensar en el vector de gradiente como una dirección en 13.000 dimensiones está, para decirlo suavemente, más allá del alcance de nuestra imaginación, hay Otra forma de pensarlo.",
  "from_community_srt": "Y la idea del último vídeo que realmente quiero que tengas fresca en tu mente ahorita, es que, por pensar en la gradiente como la dirección de 13,000 dimensiones es , por ponerlo así, mas allá del alcance de nuestra imaginación, pero hay otra manera de pensarlo.",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias.",
  "translatedText": "La magnitud de cada componente aquí le indica qué tan sensible es la función de costos a cada ponderación y sesgo.",
  "from_community_srt": "La magnitud de cada componente te esta diciendo que tan sensible es la función de coste en cada peso y bias.",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and you compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1.",
  "translatedText": "Por ejemplo, digamos que sigues el proceso que estoy a punto de describir y calculas el gradiente negativo, y el componente asociado con el peso en este borde aquí resulta ser 3.2, mientras que el componente asociado con este borde aquí viene sale como 0.1.",
  "from_community_srt": "Por ejemplo, digamos que vas al proceso describiendo y calculas la gradiente negativa, y la componente asociada aquí  con este eje sale como 3.20, mientras que la componente asociada con este aquí con este eje es 0.10,",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give.",
  "translatedText": "La forma en que lo interpretarías es que el costo de la función es 32 veces más sensible a los cambios en ese primer peso, por lo que si movieras ese valor un poco, provocaría algún cambio en el costo, y eso El cambio es 32 veces mayor de lo que daría el mismo movimiento de ese segundo peso.",
  "from_community_srt": "la manera en que interpretas eso es que el coste de la función 3.2 veces mas sensible de esa manera primero, luego si meneas ese valor solo un poco le va a costar algo al coste, y ese cambio es 3.2 veces mas grande que el que produces meneando el otro valor.",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all.",
  "translatedText": "Personalmente, cuando aprendí por primera vez sobre la propagación hacia atrás, creo que el aspecto más confuso era simplemente la notación y la búsqueda de índices.",
  "from_community_srt": "Personalmente , la primea vez que aprendí de la retropropagación , creo que el aspecto mas confuso",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other.",
  "translatedText": "Pero una vez que desenvuelves lo que realmente hace cada parte de este algoritmo, cada efecto individual que tiene es en realidad bastante intuitivo, solo que hay muchos pequeños ajustes que se superponen uno encima del otro.",
  "from_community_srt": "es la notación, Pero una vez ya estés envuelto en lo que cada una de las partes de este algoritmo hacen realmente , cada efecto individual que tenga sera intuitivo, es solo que hay un montón de ajuste pequeños de una capa encima de otra.",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases.",
  "translatedText": "Así que comenzaré aquí sin tener en cuenta la notación y simplemente analizaré los efectos que cada ejemplo de entrenamiento tiene sobre los pesos y sesgos.",
  "from_community_srt": "Voy a empezar las cosas con indiferencia completa por la notación, solo ire a pasos a travez de esos efectos que cada ejemplo de entrenamiento esta teniendo en sus peso y biases",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example.",
  "translatedText": "Debido a que la función de costo implica promediar un cierto costo por ejemplo entre las decenas de miles de ejemplos de entrenamiento, la forma en que ajustamos los pesos y sesgos para un único paso de descenso de gradiente también depende de cada ejemplo.",
  "from_community_srt": "debido a que la función de coste envuelve promediar cierto coste por  todos los ejemplos de entrenamiento La manera en que ajustamos los pesos y biases para un un paso del descenso de gradiente también depende en cada ejemplo en particular.",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step.",
  "translatedText": "O mejor dicho, en principio debería hacerlo, pero para lograr eficiencia computacional haremos un pequeño truco más adelante para evitar que tengas que ejecutar cada ejemplo en cada paso.",
  "from_community_srt": "O mas bien , en principio debería, pero por eficiencia computacional vamos  a hacer un pequeño truco después",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2.",
  "translatedText": "En otros casos, ahora mismo lo único que vamos a hacer es centrar nuestra atención en un solo ejemplo, esta imagen de un 2.",
  "from_community_srt": "para alejarte de la necesidad acertar cada ejemplo para cada paso en particular.Ahora, Otro caso  , todo lo que vamos a hacer es enfocar nuestra atención en un ejemplo en particular,",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted?",
  "translatedText": "¿Qué efecto debería tener este ejemplo de entrenamiento sobre cómo se ajustan las ponderaciones y los sesgos?",
  "from_community_srt": "esta imagen , una de un  2, qué efecto debería tener este entrenamiento en cómo los pesos y biases se van a ajustar Digamos que estamos en un punto donde la red no esta bien entrenada aún.",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on.",
  "translatedText": "Digamos que estamos en un punto donde la red aún no está bien entrenada, por lo que las activaciones en la salida se verán bastante aleatorias, tal vez algo así como 0,5, 0,8, 0,2, y así sucesivamente.",
  "from_community_srt": "de manear que las activaciones son muy aleatorias , algo como 0.5, 0.8,",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases.",
  "translatedText": "No podemos cambiar directamente esas activaciones, sólo tenemos influencia sobre los pesos y sesgos.",
  "from_community_srt": "0.2 y demas, No pedemos directamente cambiar esas activaciones ,",
  "n_reviews": 0,
  "start": 242.52,
  "end": 247.16
 },
 {
  "input": "But it's helpful to keep track of which adjustments we wish should take place to that output layer.",
  "translatedText": "Pero es útil realizar un seguimiento de qué ajustes deseamos que se realicen en esa capa de salida.",
  "from_community_srt": "solo podemos influenciar los pesos y las biases , pero es de ayuda mantener rastro des los ajustes que nos gustaría poner en esa capa de salida,",
  "n_reviews": 0,
  "start": 247.16,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down.",
  "translatedText": "Y como queremos que clasifique la imagen como 2, queremos que ese tercer valor aumente mientras que todos los demás se reduzcan.",
  "from_community_srt": "Ya que queremos clasificar la imagen como un 2 , queremos que ese tercer valor sea empujado hacia arriba mientras que todos los demás sean empujados hacia abajo.",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value.",
  "translatedText": "Además, los tamaños de estos empujones deben ser proporcionales a qué tan lejos está cada valor actual de su valor objetivo.",
  "from_community_srt": "Mas aún, los tamaños de estos empujones deberían ser proporcionales a qué tan lejos cada valor es del objetivo.",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be.",
  "translatedText": "Por ejemplo, el aumento de la activación de la neurona número 2 es, en cierto sentido, más importante que la disminución de la neurona número 8, que ya está bastante cerca de donde debería estar.",
  "from_community_srt": "por ejemplo, el incremento para esas dos activaciones de neuronas es en cierto sentido mas importante que el descenso de la neurona número ocho, que de echo esta ya muy cerca a donde debería.",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase.",
  "translatedText": "Entonces, acercándonos más, centrémonos solo en esta neurona, aquella cuya activación deseamos aumentar.",
  "from_community_srt": "Asi que haciendo zoom mas profundamente, enfocandonos en solo en una neurona,",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU.",
  "translatedText": "Recuerde, esa activación se define como una cierta suma ponderada de todas las activaciones en la capa anterior, más un sesgo, que luego se conecta a algo como la función de compresión sigmoidea, o un ReLU.",
  "from_community_srt": "la que sus activaciones deseamos que incrementen , recuerda  que \"Activación \" esta definido como cierta suma ponderada de todas las activaciones en las capas previas mas un bias a la que todos estan conectados como la función sigmoid  o RALU Hay tres avenidas diferentes que se mantienen aumentando juntas",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation.",
  "translatedText": "Por lo tanto, hay tres vías diferentes que pueden unirse para ayudar a aumentar esa activación.",
  "from_community_srt": "esa activación,",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer.",
  "translatedText": "Puede aumentar el sesgo, puede aumentar los pesos y puede cambiar las activaciones de la capa anterior.",
  "from_community_srt": "tu puedes incrementar el bias , puedes incrementar los pesos, y  puedes cambiar las activaciones de la capa anterior .",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence.",
  "translatedText": "Centrándose en cómo se deben ajustar las ponderaciones, observe cómo las ponderaciones en realidad tienen diferentes niveles de influencia.",
  "from_community_srt": "Enfocándose solo en cómo los pesos deberían ser ajustados, nota como los pesos realmente tienen niveles de influencia diferenciándose.",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values.",
  "translatedText": "Las conexiones con las neuronas más brillantes de la capa anterior tienen el mayor efecto ya que esos pesos se multiplican por valores de activación mayores.",
  "from_community_srt": "Las conecciones con las neuronas mas iluminadas  de la capa precedente  tienen el mayor efecto, ya que esos pesos están multiplicados  por un valor largo de activación.",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned.",
  "translatedText": "Entonces, si aumentaras uno de esos pesos, en realidad tendría una influencia más fuerte en la función de costo final que aumentar los pesos de las conexiones con neuronas más débiles, al menos en lo que respecta a este ejemplo de entrenamiento.",
  "from_community_srt": "Así que, si fuimos incrementando uno de esos pesos, este de echo tiene una influencia fuerte en la  función de coste . mas que incrementando los pesos de las conexiones con neuronas no definidas  . al menos hasta que este ejemplo de entrenamiento esté concernido.",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck.",
  "translatedText": "Recuerde, cuando hablamos de descenso de gradiente, no solo nos importa si cada componente debe subir o bajar, sino también cuáles le brindan el mayor rendimiento por su inversión.",
  "from_community_srt": "Recuerda que cuando hablamos de el descenso de gradiente. no solo nos importaba si cada componente se empujaba hacia arriba o abjo, nos importa cuales te dan la mayor explosión para tu carga.",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together.",
  "translatedText": "Esto, por cierto, recuerda al menos en cierto modo a una teoría en neurociencia sobre cómo aprenden las redes biológicas de neuronas, la teoría hebbiana, a menudo resumida en la frase: las neuronas que se activan juntas se conectan entre sí.",
  "from_community_srt": "Esto es, por cierto, al menos algo recordativo de una teoría en neurociencia sobre cómo las redes biológicas de neuronas aprenden \"Teoría de Hebbian\"-  regularmente resumida en la frase \"Neuronas que se prenden y conectan juntas \"",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active.",
  "translatedText": "Aquí, los mayores aumentos de peso, el mayor fortalecimiento de las conexiones, ocurren entre las neuronas que son las más activas y las que deseamos que sean más activas.",
  "from_community_srt": "Aquí, el mayor incremento en los pesos y la mayor rigidez en las conexiones, ocurre entre las neuronas las cuales son las mas activas. y las que desearíamos que se volviesen mas activas.",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those are the ones firing when thinking about a 2.",
  "translatedText": "En cierto sentido, las neuronas que se activan al ver un 2 se vinculan más fuertemente con ellas son las que se activan cuando piensan en un 2.",
  "from_community_srt": "En cierto sentido, las neuronas que se entan prendiendo mientras se ven un 2 se vinculan  mas fuertemente a esas que se prenden cuando piensan en un 2.",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note.",
  "translatedText": "Para ser claros, no estoy en posición de hacer afirmaciones de una forma u otra sobre si las redes artificiales de neuronas se comportan de alguna manera como cerebros biológicos, y esta idea de &quot;disparar juntos, cablear juntos&quot; viene acompañada de un par de asteriscos significativos, pero tomada como una interpretación muy vaga. analogía, me parece interesante observarla.",
  "from_community_srt": "Para ser claro, realmente no estoy en  posición de hacer declaraciones de una manera u otra sobre si una red artificial de neuronas se comporta para nada como cerebros biológicos. Y esta idea de prenderse juntos- conectarse juntos  viene con un par asteriscos significativos. Pero tomado como una analogía libre, encuentro muy interesante notar Como sea,",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer.",
  "translatedText": "De todos modos, la tercera forma en que podemos ayudar a aumentar la activación de esta neurona es cambiando todas las activaciones de la capa anterior.",
  "from_community_srt": "La tercera manera que podemos ayudar a esta activación de neuronas Es tomar todas las activaciones en la capa previa,",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active.",
  "translatedText": "Es decir, si todo lo conectado a esa neurona del dígito 2 con un peso positivo se volviera más brillante, y si todo lo conectado con un peso negativo se volviera más tenue, entonces esa neurona del dígito 2 se volvería más activa.",
  "from_community_srt": "es decir, si todo lo conectado a esa neurona de dígito 2 ,con con un peso positivo , se iluminó y si todo lo conectado con un peso negativo se apago, entonces esa neurona con dígito 2 se volvería mas activa.",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights.",
  "translatedText": "Y de manera similar a los cambios de peso, obtendrá el máximo provecho de su inversión buscando cambios que sean proporcionales al tamaño de los pesos correspondientes.",
  "from_community_srt": "Y similarmente a los cambios de los pesos, tu vas  obtener explosión para tu carga al buscar cambios que sean proporcionales al tamaño del pesos correspondientes",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases.",
  "translatedText": "Ahora bien, por supuesto, no podemos influir directamente en esas activaciones, sólo tenemos control sobre los pesos y sesgos.",
  "from_community_srt": "Ahora, por su puesto , nosotros no podemos directamente influir  esas activaciones, solo podemos tener control sobre los pesos y biases.",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are.",
  "translatedText": "Pero al igual que con la última capa, es útil anotar cuáles son esos cambios deseados.",
  "from_community_srt": "Pero, como con la última capa, es de ayuda solo mantener una nota de cuales son eso cambios deseados.",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants.",
  "translatedText": "Pero tenga en cuenta que, si nos alejamos un paso, esto es solo lo que quiere la neurona de salida del dígito 2.",
  "from_community_srt": "Peor ten en mente,( alejándonos un paso aquí ), esto es solo lo que esa neurona de dígito 2 quiere que salga Recuerda ,",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer.",
  "translatedText": "Recuerde, también queremos que todas las demás neuronas de la última capa se vuelvan menos activas, y cada una de esas otras neuronas de salida tiene sus propios pensamientos sobre lo que debería suceder con la penúltima capa.",
  "from_community_srt": "También queremos que las todas las otras neuronas en la última capa se vuelvan menos activas. y cada una de esas otras neuronas de salida tiene su propios pensamientos acerca de lo que debería pasar a esa segunda  a última capa.",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So, the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change.",
  "translatedText": "Entonces, el deseo de esta neurona del dígito 2 se suma junto con los deseos de todas las demás neuronas de salida sobre lo que debería suceder con esta penúltima capa, nuevamente en proporción a los pesos correspondientes, y en proporción a cuánto pesa cada una de esas neuronas. necesita cambiar.",
  "from_community_srt": "Asi que, el deseo de esta neurona de dígito 2 esta sumado junto con los deseos de todas las demás neuronas de salida para lo que debería pasar  esta segunda a última capa , De nuevo, en proporción a los correspondientes pesos, Y en proporción a cómo cada una de esas neuronas necesita cambiar",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in.",
  "translatedText": "Aquí es donde entra en juego la idea de propagarse hacia atrás.",
  "from_community_srt": "Esto que esta aquí es donde viene la idea de propagación hacia atrás.",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer.",
  "translatedText": "Al sumar todos estos efectos deseados, básicamente obtienes una lista de empujones que deseas que sucedan en esta penúltima capa.",
  "from_community_srt": "Al añadir juntos todos todos estos efectos deseados, basicametes obtienes una lista de empujones que quieres que le pasen desde la segunda a la última capa,.",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network.",
  "translatedText": "Y una vez que los tenga, puede aplicar recursivamente el mismo proceso a los pesos y sesgos relevantes que determinan esos valores, repitiendo el mismo proceso que acabo de recorrer y retrocediendo a través de la red.",
  "from_community_srt": "Y una ves los tengas, tu puedes recursivamete  aplicar el mismo proceso a los pesos  y BIASES relevantes que determinan esos valores, repitiendo el mismo proceso, solo caminé a través y de regreso de la red.",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases.",
  "translatedText": "Y alejándonos un poco más, recordemos que así es como un único ejemplo de entrenamiento desea empujar cada uno de esos pesos y sesgos.",
  "from_community_srt": "Y alejandos un poco más, recuerda que todo esto es solo cómo un entrenamiento en particular desea empujar una de esos pesos y bieases.",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2.",
  "translatedText": "Si solo escucháramos lo que ese 2 quería, la red en última instancia se vería incentivada a clasificar todas las imágenes como 2.",
  "from_community_srt": "Si solo escucháramos qué quería ese 2, La red por último sería incentivada a solo clasificar todas las imágenes como un 2.",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes.",
  "translatedText": "Entonces, lo que debe hacer es realizar esta misma rutina de respaldo para todos los demás ejemplos de entrenamiento, registrando cómo a cada uno de ellos le gustaría cambiar los pesos y los sesgos, y promediar juntos esos cambios deseados.",
  "from_community_srt": "Asi que, vas a través de esta misma rutina de retropropagación para cualquier otros ejemplo de entrenamiento, grabando  cómo cada uno de ellos le gustaría cambiar los pesos y biases, y luego promedias juntos esos cambios deseados.",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it.",
  "translatedText": "Esta colección aquí de los empujones promediados para cada peso y sesgo es, en términos generales, el gradiente negativo de la función de costo a la que se hace referencia en el último video, o al menos algo proporcional a él.",
  "from_community_srt": "Aquí, esta colección de los empujones promediados para cada peso y bias es , de manera superficial, la gradiente negativa de la función de coste referida en el último video, o al menos algo proporcional.",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing.",
  "translatedText": "Digo en términos generales solo porque todavía tengo que ser cuantitativamente preciso acerca de esos empujones, pero si entendiste cada cambio al que acabo de hacer referencia, por qué algunos son proporcionalmente más grandes que otros y cómo es necesario sumarlos todos, comprenderás la mecánica para qué está haciendo realmente la retropropagación.",
  "from_community_srt": "digo \"de manera superficial\", solo porque tengo ser quantitavimente preciso acerca de esos empujones. Pero si entendiste cada cambio que referí , por qué algunos son proporcionalmente mayores que otros y cómo todos ellos necesitan ser sumados juntos , entonces tu entiendes la mecánica de la retropropagación está haciendo en realidad.",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step.",
  "translatedText": "Por cierto, en la práctica, a las computadoras les lleva mucho tiempo sumar la influencia de cada ejemplo de entrenamiento en cada paso de descenso de gradiente.",
  "from_community_srt": "Por cierto, en la práctica a las computadoras les tema un tiempo extremadamente largo para sumar la influencia de cada ejemplo de entrenamiento en partícular, cada  paso del descenso de gradiente Asi que ,",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead.",
  "translatedText": "Así que esto es lo que se hace comúnmente.",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and then divide it into a whole bunch of mini-batches, let's say each one having 100 training examples.",
  "translatedText": "Mezclas aleatoriamente tus datos de entrenamiento y luego los divides en un montón de minilotes, digamos que cada uno tiene 100 ejemplos de entrenamiento.",
  "from_community_srt": "esto es lo que  se hace comunmente en lugar : Barajas aleatoriamente tus datos de entrenamiento, y luego los divides en montón de mini lotes, digamos, cada uno teniendo 100 ejemplos de entrenamiento.",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch.",
  "translatedText": "Luego calcula un paso de acuerdo con el mini lote.",
  "from_community_srt": "Luego tu calculas un paso de acuerdo al mini lote No va a ser la gradiente real de la función de coste,",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not going to be the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speedup.",
  "translatedText": "No será el gradiente real de la función de costos, que depende de todos los datos de entrenamiento, ni de este pequeño subconjunto, por lo que no es el paso cuesta abajo más eficiente, pero cada mini lote le brinda una aproximación bastante buena, y Más importante aún, le brinda una aceleración computacional significativa.",
  "from_community_srt": "la cual depende de todos los datos de entrenamiento, no de este pequeño sub conjunto. Entonces, no es el paso mas eficiente hacia abajo de la colina, Pero cada mini lote te da una muy buena aproximación, Y mas importantemente, te da una significante aceleración computacional .",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction.",
  "translatedText": "Si tuviera que trazar la trayectoria de su red bajo la superficie de costos relevante, sería un poco más como un hombre borracho que tropezara sin rumbo colina abajo pero dando pasos rápidos, en lugar de un hombre cuidadosamente calculador que determina la dirección exacta de cada paso cuesta abajo. antes de dar un paso muy lento y cuidadoso en esa dirección.",
  "from_community_srt": "Si ubieses ido a la gráfica de la trayectoria de tu red debajo  de la superficie relevante de la función coste, Sería mas como si un hombre borracho sin objetivo descendiendo una colina, pero tomando pequeños pasos; mas que un hombre cuidadosamente calculando la dirección hacia abajo de cada paso; antes de tomar un paso muy lento y cuidadoso en esa dirección.",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent.",
  "translatedText": "Esta técnica se conoce como descenso de gradiente estocástico.",
  "from_community_srt": "Esta técnica es referida como \"Descenso de gradiente estocástico\" Hay  muncho allí,",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we?",
  "translatedText": "Están sucediendo muchas cosas aquí, así que resumámoslo nosotros mismos, ¿de acuerdo?",
  "from_community_srt": "asi que solo sumemoslo para nosotros mismos,",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost.",
  "translatedText": "La retropropagación es el algoritmo para determinar cómo un único ejemplo de entrenamiento le gustaría empujar los pesos y sesgos, no sólo en términos de si deberían subir o bajar, sino en términos de qué proporciones relativas a esos cambios causan la disminución más rápida del costo.",
  "from_community_srt": "deberiamos ? La retropropagación es el algoritmo para determinar cómo un ejemplo de entrenamiento en particular le gustaría empujar los pesos y biases, no solo en términos  de si deben ir hacia  arriba o abajo, si no que en términos de que proporciones relativas a esos cambios causan el decrecimiento mas rápido del coste.",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens of thousands of training examples and averaging the desired changes you get.",
  "translatedText": "Un verdadero paso de descenso de gradiente implicaría hacer esto para todas sus decenas de miles de ejemplos de entrenamiento y promediar los cambios deseados que obtenga.",
  "from_community_srt": "U verdadero paso de descenso de gradiente involucraría hacer esto para todos tus  decenas y miles de ejemplos de entrenamiento y promediar los cambios deseados que obtienes.",
  "n_reviews": 0,
  "start": 656.26,
  "end": 664.2
 },
 {
  "input": "But that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch.",
  "translatedText": "Pero eso es computacionalmente lento, por lo que en su lugar subdivide aleatoriamente los datos en minilotes y calcula cada paso con respecto a un minilote.",
  "from_community_srt": "Pero eso computacionalmete lento Asi que en cambio, aleatoriamente subdivides la información es estos mini lotes y coputas cada paso con respecto a un mini lote.",
  "n_reviews": 0,
  "start": 664.86,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all of the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples.",
  "translatedText": "Al revisar repetidamente todos los minilotes y realizar estos ajustes, convergerá hacia un mínimo local de la función de costo, es decir, su red terminará haciendo un trabajo realmente bueno en los ejemplos de capacitación.",
  "from_community_srt": "Repetidamente yendo a traves de todos los mini lotes y haciendo estos ajustes , llegaras hacia un mínimo local de la función de coste, lo cual quiere decir que tu red va a finalizar haciendo un muy buen trabajo en los datos de entrenamiento",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms.",
  "translatedText": "Entonces, dicho todo esto, cada línea de código que se utilizaría para implementar backprop en realidad corresponde con algo que ya ha visto, al menos en términos informales.",
  "from_community_srt": "Asi que con todo eso dicho, cada linea de código que iría en la implementación de la retropropagación de echo corresponde con algo que acabas de ver ahora,",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing.",
  "translatedText": "Pero a veces saber lo que hacen las matemáticas es sólo la mitad de la batalla, y simplemente representar la maldita cosa es donde todo se vuelve confuso y confuso.",
  "from_community_srt": "al menos términos informales , Pero a veces sabiendo donde va la mate solo hace la mitad de la batalla, y solo representando la maldita cosa es donde se vuelve confuso.",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources.",
  "translatedText": "Entonces, para aquellos de ustedes que quieran profundizar más, el siguiente video analiza las mismas ideas que se acaban de presentar aquí, pero en términos del cálculo subyacente, lo que con suerte debería hacerlo un poco más familiar a medida que ven el tema en otros. recursos.",
  "from_community_srt": "asi que para esos de ustedes que quieren ir  mas profundo, el siguiente video va a través  de las misma ideas que fueron aquí  presentadas pero en términos del cálculo subyacente el cual ojalá  debería hacer esto un poco más familiar así como lo ves en otras fuentes",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data.",
  "translatedText": "Antes de eso, una cosa que vale la pena enfatizar es que para que este algoritmo funcione, y esto se aplica a todo tipo de aprendizaje automático más allá de las redes neuronales, se necesitan muchos datos de entrenamiento.",
  "from_community_srt": "Antes de eso, una cosa que vale la pena enfatizar para trabajar este este algoritmo, y  esto es para todo tipo de máquina de aprender mas allá de redes neuronales, necesitas un monton de datos de entrenamiento",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans.",
  "translatedText": "En nuestro caso, una cosa que hace que los dígitos escritos a mano sean un buen ejemplo es que existe la base de datos MNIST, con tantos ejemplos que han sido etiquetados por humanos.",
  "from_community_srt": "en nuestro caso, una cosa que hace a los dígitos escritos a mano un buen ejemplo, es que existe la base de datos MNIST",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with.",
  "translatedText": "Entonces, un desafío común con el que aquellos de ustedes que trabajan en aprendizaje automático estarán familiarizados es simplemente obtener los datos de entrenamiento etiquetados que realmente necesitan, ya sea que las personas etiqueten decenas de miles de imágenes o cualquier otro tipo de datos con el que esté tratando.",
  "from_community_srt": "con muchísimos ejemplos que  han sido etiquetados por humanos Asi que un reto común con el que estarán familiarizados  esos de ustedes que trabajan con máquinas de aprender es solo tener etiquetados los datos de entrenamiento que realmente necesitan, Ya sea teniendo personas etiquetando  decenas de miles de imágenes O cual sea otra tipo de dato que con el que podrías tratar.",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]