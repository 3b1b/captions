1
00:00:04,180 --> 00:00:08,880
Aquí vamos a abordar la "Retropropagación", el algoritmo central por el cual las redes neuronales aprenden.

2
00:00:09,460 --> 00:00:12,815
después de una pequeña  recapitulación de donde estábamos, la primera cosa que voy a hacer

3
00:00:12,815 --> 00:00:15,855
es una demostración intuitiva de lo que el algoritmo realmente esta haciendo

4
00:00:15,855 --> 00:00:17,055
sin referencia alguna a la fórmula.

5
00:00:17,385 --> 00:00:20,325
Luego para los que quieran ir a la matemática,

6
00:00:20,325 --> 00:00:23,265
el siguiente vídeo va sobre la matemática detrás de esto.

7
00:00:23,665 --> 00:00:26,325
Si viste los últimos dos videos,

8
00:00:26,325 --> 00:00:29,445
o si los saltaste con el apropiado conocimiento previo, tú sabes que es una red neuronal

9
00:00:29,445 --> 00:00:32,185
y cómo se ajusta con la información. Lo que estamos haciendo

10
00:00:32,185 --> 00:00:35,145
aquí es el ejemplo clásico de reconocimiento de dígitos escritos a mano,

11
00:00:35,145 --> 00:00:39,300
cuyos valores de pixeles se ocultan  en la primera capa  de 784 neuronas.

12
00:00:40,000 --> 00:00:44,320
He mostrado una una red con dis capa de solo 16 neuronas cada una,  y

13
00:00:44,335 --> 00:00:47,205
una capa de salida con 10 neuronas, indicando qué

14
00:00:47,205 --> 00:00:49,345
dígitos la red esa eligiendo como respuesta.

15
00:00:49,875 --> 00:00:52,695
También espero que entiendas que es el descenso de gradiente

16
00:00:52,695 --> 00:00:55,565
el cual fue descrito en el último vídeo,

17
00:00:55,565 --> 00:00:58,705
y lo que queremos dar a entender por "Aprender", es que queremos encontrar

18
00:00:58,705 --> 00:01:01,445
cuales pesos  y BIASES minimizan una función de coste.

19
00:01:01,735 --> 00:01:04,575
Como un pequeño recordatorio, para el coste

20
00:01:04,575 --> 00:01:06,705
de un entrenamiento en particular,

21
00:01:06,705 --> 00:01:09,615
lo que haces es tomar el output que la red da ,

22
00:01:09,615 --> 00:01:12,565
y junto con la salida que quieres que ella de,  solo elevas al cuadrado

23
00:01:12,565 --> 00:01:15,505
la diferencia entre cada componente.

24
00:01:15,505 --> 00:01:18,285
Haciendo esto para todos los  miles ejemplos de datos de entrenamiento,

25
00:01:18,285 --> 00:01:21,215
y promediando los resultados,  esto te da

26
00:01:21,215 --> 00:01:22,225
el costo total de la red.

27
00:01:22,605 --> 00:01:25,395
Y si eso no es suficiente para pensar,

28
00:01:25,395 --> 00:01:28,435
como fue descrito en el último video, lo que estamos buscando

29
00:01:28,435 --> 00:01:31,095
es la gradiente negativa de esta función de coste

30
00:01:31,095 --> 00:01:34,195
que te diga lo que necesitas cambiar en los pesos y las BIASES

31
00:01:34,200 --> 00:01:37,920
en estas conexiones , de manera que se reduzca el coste mas eficientemente .

32
00:01:42,900 --> 00:01:45,975
La retropropagación , el tema de este vídeo ,

33
00:01:45,980 --> 00:01:48,620
es el algoritmo para calcular es gradiente locamente complicada

34
00:01:49,240 --> 00:01:52,035
Y la idea del último vídeo que realmente

35
00:01:52,035 --> 00:01:54,975
quiero que tengas fresca en tu mente ahorita,

36
00:01:54,975 --> 00:01:57,525
es que, por pensar en la gradiente como la dirección de 13,000 dimensiones

37
00:01:57,525 --> 00:02:00,505
es , por ponerlo así, mas allá

38
00:02:00,505 --> 00:02:03,685
del alcance de nuestra imaginación, pero hay otra manera de pensarlo.

39
00:02:04,265 --> 00:02:07,215
La magnitud de cada componente

40
00:02:07,220 --> 00:02:10,800
te esta diciendo que tan sensible es la función de coste en cada peso y bias.

41
00:02:11,840 --> 00:02:14,460
Por ejemplo,  digamos que vas al proceso describiendo

42
00:02:14,465 --> 00:02:17,535
y calculas la gradiente negativa,

43
00:02:17,535 --> 00:02:20,695
y la componente asociada aquí  con este eje

44
00:02:20,695 --> 00:02:23,725
sale como 3.20, mientras que la componente asociada con

45
00:02:23,725 --> 00:02:26,335
este aquí con este eje

46
00:02:26,575 --> 00:02:29,735
es 0.10, la manera en que interpretas eso es

47
00:02:29,735 --> 00:02:33,195
que el coste de la función 3.2 veces mas sensible

48
00:02:33,305 --> 00:02:36,385
de esa manera primero, luego si meneas ese valor solo un poco

49
00:02:36,385 --> 00:02:39,035
le va a costar algo al coste, y ese cambio

50
00:02:39,035 --> 00:02:41,845
es 3.2 veces mas grande que

51
00:02:41,845 --> 00:02:43,285
el que produces meneando el otro valor.

52
00:02:48,215 --> 00:02:51,215
Personalmente , la primea vez que aprendí de la retropropagación ,

53
00:02:51,215 --> 00:02:53,935
creo que el aspecto mas confuso

54
00:02:53,935 --> 00:02:56,835
es la notación,

55
00:02:56,835 --> 00:02:59,295
Pero una vez ya estés envuelto en lo que cada una de las partes de este algoritmo hacen realmente ,

56
00:02:59,295 --> 00:03:02,335
cada efecto individual que tenga

57
00:03:02,335 --> 00:03:05,315
sera intuitivo, es solo que hay un montón de ajuste pequeños

58
00:03:05,315 --> 00:03:06,625
de una capa encima de otra.

59
00:03:07,335 --> 00:03:10,265
Voy a empezar las cosas con indiferencia completa por

60
00:03:10,265 --> 00:03:13,075
la notación, solo ire a pasos a travez de esos efectos

61
00:03:13,075 --> 00:03:16,265
que cada ejemplo de entrenamiento esta teniendo en sus peso y biases

62
00:03:16,825 --> 00:03:19,855
debido a que la función de coste envuelve promediar

63
00:03:19,855 --> 00:03:22,815
cierto coste por  todos los ejemplos de entrenamiento

64
00:03:22,815 --> 00:03:25,775
La manera en que ajustamos

65
00:03:25,775 --> 00:03:28,355
los pesos y biases para un un paso

66
00:03:28,575 --> 00:03:30,915
del descenso de gradiente también depende en cada ejemplo en particular.

67
00:03:31,445 --> 00:03:34,365
O mas bien ,  en principio debería,

68
00:03:34,365 --> 00:03:36,975
pero por eficiencia computacional vamos  a hacer un pequeño truco después

69
00:03:36,975 --> 00:03:40,125
para alejarte de la necesidad acertar cada ejemplo

70
00:03:40,125 --> 00:03:43,125
para cada paso en particular.Ahora, Otro caso  ,

71
00:03:43,125 --> 00:03:46,185
todo lo que vamos a hacer es enfocar nuestra atención en un ejemplo en particular,

72
00:03:46,375 --> 00:03:49,395
esta imagen , una de un  2, qué efecto debería tener este entrenamiento

73
00:03:49,395 --> 00:03:51,775
en cómo los pesos y biases se van a ajustar

74
00:03:52,375 --> 00:03:55,365
Digamos que estamos en un punto donde la red no esta bien entrenada aún.

75
00:03:55,365 --> 00:03:58,265
de manear que las activaciones son muy aleatorias ,

76
00:03:58,265 --> 00:04:01,255
algo como 0.5, 0.8, 0.2

77
00:04:01,255 --> 00:04:02,255
y demas,

78
00:04:02,415 --> 00:04:05,455
No pedemos directamente cambiar

79
00:04:05,455 --> 00:04:08,455
esas activaciones , solo podemos influenciar los pesos y las biases

80
00:04:08,455 --> 00:04:11,155
, pero es de ayuda mantener rastro des los ajustes que nos gustaría

81
00:04:11,155 --> 00:04:12,665
poner en esa capa de salida,

82
00:04:13,025 --> 00:04:16,035
Ya que queremos clasificar la imagen como un 2

83
00:04:16,035 --> 00:04:19,395
, queremos que ese tercer valor sea empujado hacia arriba mientras

84
00:04:19,400 --> 00:04:21,520
que todos los demás sean empujados hacia abajo.

85
00:04:21,960 --> 00:04:25,115
Mas aún, los tamaños de estos empujones

86
00:04:25,115 --> 00:04:28,045
deberían ser proporcionales a qué tan lejos cada

87
00:04:28,045 --> 00:04:29,765
valor es del objetivo.

88
00:04:30,155 --> 00:04:32,975
por ejemplo, el incremento para esas dos activaciones de neuronas

89
00:04:32,975 --> 00:04:35,825
es en cierto sentido mas importante

90
00:04:35,825 --> 00:04:38,225
que el descenso de la neurona número ocho,

91
00:04:38,225 --> 00:04:40,685
que de echo esta ya muy cerca a donde debería.

92
00:04:41,655 --> 00:04:44,515
Asi que haciendo zoom mas profundamente, enfocandonos en solo

93
00:04:44,515 --> 00:04:47,685
en una neurona, la que sus activaciones

94
00:04:47,895 --> 00:04:50,855
deseamos que incrementen , recuerda  que "Activación " esta definido como

95
00:04:50,855 --> 00:04:52,055
cierta suma ponderada

96
00:04:52,465 --> 00:04:55,645
de todas las activaciones en las capas previas

97
00:04:55,645 --> 00:04:58,755
mas un bias a la que todos estan conectados

98
00:04:58,755 --> 00:05:02,215
como la función sigmoid  o RALU

99
00:05:02,335 --> 00:05:05,605
Hay tres avenidas diferentes que se mantienen aumentando juntas

100
00:05:05,605 --> 00:05:10,220
esa activación, tu puedes incrementar el bias ,  puedes incrementar los pesos,

101
00:05:11,000 --> 00:05:14,000
y  puedes cambiar las activaciones de la capa anterior .

102
00:05:14,620 --> 00:05:17,460
Enfocándose solo en cómo los pesos deberían ser ajustados,

103
00:05:17,460 --> 00:05:20,480
nota como los pesos realmente tienen niveles de influencia diferenciándose.

104
00:05:21,400 --> 00:05:25,020
Las conecciones con las neuronas mas iluminadas  de la capa precedente  tienen el mayor efecto,

105
00:05:25,280 --> 00:05:29,380
ya que esos pesos están multiplicados  por un valor largo de activación.

106
00:05:31,000 --> 00:05:34,040
Así que, si fuimos incrementando uno de esos pesos,

107
00:05:34,055 --> 00:05:37,045
este de echo tiene una influencia fuerte en la  función de coste .

108
00:05:37,045 --> 00:05:40,085
mas que incrementando los pesos de las conexiones con neuronas no definidas  .

109
00:05:40,085 --> 00:05:43,520
al menos hasta que este ejemplo de entrenamiento esté concernido.

110
00:05:44,520 --> 00:05:47,655
Recuerda que cuando hablamos de el descenso de gradiente.

111
00:05:47,655 --> 00:05:50,675
no solo nos importaba si cada componente se empujaba hacia arriba o abjo,

112
00:05:50,675 --> 00:05:53,425
nos importa cuales te dan la mayor explosión para tu carga.

113
00:05:55,275 --> 00:05:58,645
Esto es, por cierto, al menos algo recordativo de una teoría en neurociencia

114
00:05:58,645 --> 00:06:01,575
sobre cómo las redes biológicas de neuronas aprenden

115
00:06:01,580 --> 00:06:06,560
"Teoría de Hebbian"-  regularmente resumida en la frase "Neuronas que se prenden y conectan juntas "

116
00:06:06,980 --> 00:06:11,440
Aquí, el mayor incremento en los pesos y la mayor rigidez en las conexiones,

117
00:06:11,880 --> 00:06:14,460
ocurre entre las neuronas las cuales son las mas activas.

118
00:06:14,700 --> 00:06:17,300
y las que desearíamos que se volviesen mas activas.

119
00:06:18,100 --> 00:06:20,560
En cierto sentido, las neuronas que se entan prendiendo mientras se ven un 2

120
00:06:20,560 --> 00:06:24,300
se vinculan  mas fuertemente a esas que se prenden cuando piensan en un 2.

121
00:06:25,100 --> 00:06:28,660
Para ser claro,  realmente no estoy en  posición de hacer declaraciones de una manera u otra

122
00:06:28,940 --> 00:06:31,960
sobre si una red artificial de neuronas se comporta para nada como cerebros biológicos.

123
00:06:31,960 --> 00:06:36,600
Y esta idea de prenderse juntos- conectarse juntos  viene con un par asteriscos significativos.

124
00:06:37,360 --> 00:06:41,060
Pero tomado como una analogía libre, encuentro muy interesante notar

125
00:06:41,980 --> 00:06:45,580
Como sea, La tercera manera que podemos ayudar a esta activación de neuronas

126
00:06:46,000 --> 00:06:48,640
Es tomar todas las activaciones en la capa previa,

127
00:06:49,300 --> 00:06:52,360
es decir, si todo lo conectado a esa neurona de dígito 2 ,con con un peso positivo , se iluminó

128
00:06:54,120 --> 00:06:57,100
y si todo lo conectado con un peso negativo se apago,

129
00:06:57,460 --> 00:07:00,480
entonces esa neurona con dígito 2 se volvería mas activa.

130
00:07:02,160 --> 00:07:06,180
Y similarmente a los cambios de los pesos,  tu vas  obtener explosión para tu carga

131
00:07:06,240 --> 00:07:09,820
al buscar cambios que sean proporcionales al tamaño del pesos correspondientes

132
00:07:12,220 --> 00:07:15,400
Ahora, por su puesto , nosotros no podemos directamente influir  esas activaciones,

133
00:07:15,405 --> 00:07:18,375
solo podemos tener control sobre los pesos y biases.

134
00:07:18,380 --> 00:07:23,160
Pero, como con la última capa,  es de ayuda solo mantener una nota de cuales son eso cambios deseados.

135
00:07:24,200 --> 00:07:28,960
Peor ten en mente,( alejándonos un paso aquí ),  esto es solo lo que esa neurona de dígito 2 quiere que salga

136
00:07:29,600 --> 00:07:33,960
Recuerda , También queremos que las todas las otras neuronas en la última capa se vuelvan menos activas.

137
00:07:34,920 --> 00:07:37,940
y cada una de esas otras neuronas de salida

138
00:07:38,080 --> 00:07:39,640
tiene su propios pensamientos acerca de lo que debería pasar a esa segunda  a última capa.

139
00:07:42,800 --> 00:07:45,815
Asi que, el deseo de esta neurona de dígito 2

140
00:07:45,820 --> 00:07:49,940
esta sumado junto con los deseos de todas las demás neuronas de salida

141
00:07:50,580 --> 00:07:53,720
para lo que debería pasar  esta segunda a última capa ,

142
00:07:53,720 --> 00:07:55,760
De nuevo, en proporción a los correspondientes pesos,

143
00:07:56,120 --> 00:08:00,620
Y en proporción a cómo cada una de esas neuronas necesita cambiar

144
00:08:01,180 --> 00:08:05,060
Esto que esta aquí es donde viene la idea de propagación hacia atrás.

145
00:08:05,640 --> 00:08:08,600
Al añadir juntos todos todos estos efectos deseados,

146
00:08:08,600 --> 00:08:13,400
basicametes obtienes una lista de empujones que quieres que le pasen desde la segunda a la última capa,.

147
00:08:13,860 --> 00:08:15,700
Y una ves los tengas,

148
00:08:16,020 --> 00:08:17,540
tu puedes recursivamete  aplicar el mismo proceso

149
00:08:17,540 --> 00:08:20,720
a los pesos  y BIASES relevantes que determinan esos valores,

150
00:08:21,080 --> 00:08:24,880
repitiendo el mismo proceso, solo caminé a través y de regreso de la red.

151
00:08:28,720 --> 00:08:31,240
Y alejandos un poco más,

152
00:08:31,460 --> 00:08:34,735
recuerda que todo esto es solo

153
00:08:34,740 --> 00:08:37,220
cómo un entrenamiento en particular desea empujar una de esos pesos y bieases.

154
00:08:37,600 --> 00:08:40,635
Si solo escucháramos qué quería ese 2,

155
00:08:40,635 --> 00:08:43,395
La red por último sería incentivada a solo clasificar todas las imágenes como un 2.

156
00:08:43,780 --> 00:08:48,740
Asi que, vas a través de esta misma rutina de retropropagación para cualquier otros ejemplo de entrenamiento,

157
00:08:49,140 --> 00:08:52,400
grabando  cómo cada uno de ellos le gustaría cambiar los pesos y biases,

158
00:08:52,660 --> 00:08:55,695
y luego promedias juntos esos cambios deseados.

159
00:09:01,975 --> 00:09:05,025
Aquí, esta colección de los empujones promediados para cada peso y bias es ,

160
00:09:05,025 --> 00:09:11,440
de manera superficial, la gradiente negativa de la función de coste referida en el último video,

161
00:09:11,980 --> 00:09:13,480
o al menos algo proporcional.

162
00:09:14,360 --> 00:09:19,040
digo "de manera superficial", solo porque tengo ser quantitavimente preciso acerca de esos empujones.

163
00:09:19,340 --> 00:09:22,295
Pero si entendiste cada cambio que referí ,

164
00:09:22,295 --> 00:09:25,275
por qué algunos son proporcionalmente mayores que otros

165
00:09:25,275 --> 00:09:28,355
y cómo todos ellos necesitan ser sumados juntos ,

166
00:09:28,355 --> 00:09:30,985
entonces tu entiendes la mecánica de la retropropagación está haciendo en realidad.

167
00:09:33,835 --> 00:09:37,055
Por cierto, en la práctica a las computadoras les tema un tiempo extremadamente largo

168
00:09:37,060 --> 00:09:42,200
para sumar la influencia de cada ejemplo de entrenamiento en partícular, cada  paso del descenso de gradiente

169
00:09:43,280 --> 00:09:46,240
Asi que , esto es lo que  se hace comunmente en lugar :

170
00:09:46,240 --> 00:09:49,800
Barajas aleatoriamente tus datos de entrenamiento, y luego los divides en montón de mini lotes,

171
00:09:50,420 --> 00:09:52,260
digamos, cada uno teniendo 100 ejemplos de entrenamiento.

172
00:09:53,280 --> 00:09:55,940
Luego tu calculas un paso de acuerdo al mini lote

173
00:09:56,880 --> 00:09:59,080
No va a ser la gradiente real de la función de coste,

174
00:09:59,380 --> 00:10:02,740
la cual depende de todos los datos de entrenamiento, no de este pequeño sub conjunto.

175
00:10:03,280 --> 00:10:05,520
Entonces,  no es el paso mas eficiente hacia abajo de la colina,

176
00:10:05,800 --> 00:10:08,720
Pero cada mini lote te da una muy buena aproximación,

177
00:10:08,720 --> 00:10:11,760
Y mas importantemente, te da una significante aceleración computacional .

178
00:10:12,860 --> 00:10:16,340
Si ubieses ido a la gráfica de la trayectoria de tu red debajo  de la superficie relevante de la función coste,

179
00:10:16,940 --> 00:10:21,500
Sería mas como si un hombre borracho sin objetivo descendiendo una colina, pero tomando pequeños pasos;

180
00:10:21,960 --> 00:10:26,780
mas que un hombre cuidadosamente calculando la dirección hacia abajo de cada paso;

181
00:10:26,980 --> 00:10:30,000
antes de tomar un paso muy lento y cuidadoso en esa dirección.

182
00:10:31,200 --> 00:10:34,980
Esta técnica es referida como "Descenso de gradiente estocástico"

183
00:10:35,960 --> 00:10:39,460
Hay  muncho allí, asi que solo sumemoslo para nosotros mismos, deberiamos ?

184
00:10:40,300 --> 00:10:43,235
La retropropagación es el algoritmo

185
00:10:43,240 --> 00:10:46,700
para determinar cómo un ejemplo de entrenamiento en particular le gustaría empujar los pesos y biases,

186
00:10:46,960 --> 00:10:49,640
no solo en términos  de si deben ir hacia  arriba o abajo,

187
00:10:49,640 --> 00:10:55,300
si no que en términos de que proporciones relativas a esos cambios causan el decrecimiento mas rápido del coste.

188
00:10:55,920 --> 00:10:58,945
U verdadero paso de descenso de gradiente

189
00:10:58,945 --> 00:11:01,645
involucraría hacer esto para todos tus  decenas y miles de ejemplos de entrenamiento

190
00:11:01,645 --> 00:11:04,380
y promediar los cambios deseados que obtienes.

191
00:11:04,840 --> 00:11:06,280
Pero eso computacionalmete lento

192
00:11:06,640 --> 00:11:10,415
Asi que en cambio, aleatoriamente subdivides la información es estos mini lotes

193
00:11:10,415 --> 00:11:13,605
y coputas cada paso con respecto a un mini lote.

194
00:11:13,605 --> 00:11:17,380
Repetidamente yendo a traves de todos los mini lotes y haciendo estos ajustes ,

195
00:11:17,520 --> 00:11:20,780
llegaras hacia un mínimo local de la función de coste,

196
00:11:21,300 --> 00:11:25,200
lo cual quiere decir que tu red va a finalizar haciendo un muy buen trabajo en los datos de entrenamiento

197
00:11:27,160 --> 00:11:31,920
Asi que con todo eso dicho, cada linea de código que iría en la implementación de la retropropagación

198
00:11:32,220 --> 00:11:36,660
de echo corresponde con algo que acabas de ver ahora, al menos términos informales ,

199
00:11:37,320 --> 00:11:40,705
Pero a veces sabiendo donde va la mate solo hace la mitad de la batalla,

200
00:11:40,705 --> 00:11:43,725
y solo representando la maldita cosa es donde se vuelve confuso.

201
00:11:44,595 --> 00:11:47,635
asi que para esos de ustedes que quieren ir  mas profundo,

202
00:11:47,635 --> 00:11:50,435
el siguiente video va a través  de las misma ideas que fueron aquí  presentadas

203
00:11:50,435 --> 00:11:52,575
pero en términos del cálculo subyacente

204
00:11:52,580 --> 00:11:56,480
el cual ojalá  debería hacer esto un poco más familiar así como lo ves en otras fuentes

205
00:11:56,920 --> 00:11:59,960
Antes de eso, una cosa que vale la pena enfatizar

206
00:11:59,960 --> 00:12:04,080
para trabajar este este algoritmo, y  esto es para todo tipo de máquina de aprender mas allá de redes neuronales,

207
00:12:04,760 --> 00:12:06,340
necesitas un monton de datos de entrenamiento

208
00:12:06,345 --> 00:12:09,535
en nuestro caso, una cosa que hace a los dígitos escritos a mano un buen ejemplo,

209
00:12:09,535 --> 00:12:12,405
es que existe la base de datos MNIST

210
00:12:12,405 --> 00:12:14,975
con muchísimos ejemplos que  han sido etiquetados por humanos

211
00:12:14,980 --> 00:12:17,960
Asi que un reto común con el que estarán familiarizados  esos de ustedes que trabajan con máquinas de aprender

212
00:12:18,160 --> 00:12:21,160
es solo tener etiquetados los datos de entrenamiento que realmente necesitan,

213
00:12:21,660 --> 00:12:24,700
Ya sea teniendo personas etiquetando  decenas de miles de imágenes

214
00:12:24,700 --> 00:12:27,320
O cual sea otra tipo de dato que con el que podrías tratar.

