1
00:00:00,000 --> 00:00:09,640
Qui affrontiamo la backpropagation, l’algoritmo fondamentale alla base del modo in cui le reti neurali apprendono.

2
00:00:09,640 --> 00:00:13,320
Dopo un breve riepilogo della situazione attuale, la prima cosa che farò sarà una

3
00:00:13,320 --> 00:00:17,400
guida intuitiva su cosa sta effettivamente facendo l&#39;algoritmo, senza alcun riferimento alle formule.

4
00:00:17,400 --> 00:00:21,400
Quindi, per quelli di voi che vogliono tuffarsi nella matematica, il

5
00:00:21,400 --> 00:00:24,040
prossimo video approfondirà i calcoli alla base di tutto questo.

6
00:00:24,040 --> 00:00:27,320
Se hai guardato gli ultimi due video o se stai semplicemente entrando nel merito

7
00:00:27,320 --> 00:00:31,080
con il background appropriato, sai cos&#39;è una rete neurale e come trasmette informazioni.

8
00:00:31,080 --> 00:00:35,520
Qui stiamo facendo il classico esempio di riconoscimento di cifre scritte a mano i cui

9
00:00:35,520 --> 00:00:40,280
valori di pixel vengono immessi nel primo strato della rete con 784 neuroni, e

10
00:00:40,280 --> 00:00:44,720
ho mostrato una rete con due strati nascosti con solo 16 neuroni ciascuno e un

11
00:00:44,720 --> 00:00:49,520
output strato di 10 neuroni, che indica quale cifra la rete sceglie come risposta.

12
00:00:49,520 --> 00:00:54,480
Mi aspetto anche che tu comprenda la discesa del gradiente, come descritta

13
00:00:54,480 --> 00:01:00,160
nell&#39;ultimo video, e come ciò che intendiamo per apprendimento è che vogliamo

14
00:01:00,160 --> 00:01:02,080
scoprire quali pesi e pregiudizi minimizzano una determinata funzione di costo.

15
00:01:02,080 --> 00:01:07,560
Come rapido promemoria, per il costo di un singolo esempio di

16
00:01:07,560 --> 00:01:12,920
formazione, prendi l&#39;output fornito dalla rete, insieme all&#39;output che volevi che

17
00:01:12,920 --> 00:01:15,560
fornisse, e somma i quadrati delle differenze tra ciascun componente.

18
00:01:15,560 --> 00:01:20,160
Facendo questo per tutte le decine di migliaia di esempi di formazione e

19
00:01:20,160 --> 00:01:23,040
calcolando la media dei risultati, si ottiene il costo totale della rete.

20
00:01:23,040 --> 00:01:26,320
Come se ciò non bastasse, come descritto nell&#39;ultimo video, la cosa che

21
00:01:26,320 --> 00:01:31,700
stiamo cercando è il gradiente negativo di questa funzione di costo, che

22
00:01:31,700 --> 00:01:36,000
ti dice come devi cambiare tutti i pesi e i bias, tutti

23
00:01:36,000 --> 00:01:43,080
queste connessioni, in modo da ridurre nel modo più efficiente i costi.

24
00:01:43,080 --> 00:01:48,600
La propagazione inversa, l&#39;argomento di questo video, è

25
00:01:48,600 --> 00:01:49,600
un algoritmo per calcolare quel gradiente follemente complicato.

26
00:01:49,600 --> 00:01:53,300
L&#39;idea dell&#39;ultimo video che voglio davvero che tu tenga saldamente in mente

27
00:01:53,300 --> 00:01:58,280
in questo momento è che, poiché pensare al vettore gradiente come una

28
00:01:58,280 --> 00:02:02,660
direzione in 13.000 dimensioni è, per dirla alla leggera, oltre la portata

29
00:02:02,660 --> 00:02:04,620
della nostra immaginazione, ce n&#39;è un&#39;altra modo in cui puoi pensarci.

30
00:02:04,620 --> 00:02:09,700
L&#39;entità di ciascun componente qui indica quanto la funzione

31
00:02:09,700 --> 00:02:11,820
di costo sia sensibile a ciascun peso e bias.

32
00:02:11,820 --> 00:02:15,180
Per esempio, diciamo che segui il processo che sto per descrivere, e calcoli il

33
00:02:15,180 --> 00:02:19,800
gradiente negativo, e il componente associato al peso su questo bordo qui risulta

34
00:02:19,800 --> 00:02:26,940
essere 3. 2, mentre la componente associata a questo bordo qui risulta essere 0. 1.

35
00:02:26,940 --> 00:02:31,520
Il modo in cui lo interpreteresti è che il costo della funzione è

36
00:02:31,520 --> 00:02:36,100
32 volte più sensibile ai cambiamenti nel primo peso, quindi se dovessi spostare

37
00:02:36,100 --> 00:02:40,780
un po&#39; quel valore, causerebbe qualche cambiamento nel costo, e quel cambiamento è

38
00:02:40,780 --> 00:02:45,580
32 volte maggiore di quanto darebbe la stessa oscillazione a quel secondo peso.

39
00:02:45,580 --> 00:02:52,500
Personalmente, quando ho appreso per la prima volta della propagazione inversa, penso che

40
00:02:52,500 --> 00:02:55,820
l&#39;aspetto più confuso fosse proprio la notazione e l&#39;inseguimento dell&#39;indice di tutto ciò.

41
00:02:55,820 --> 00:03:00,240
Ma una volta che scopri cosa sta realmente facendo ogni parte di questo

42
00:03:00,240 --> 00:03:04,540
algoritmo, ogni singolo effetto che sta avendo è in realtà piuttosto intuitivo, è

43
00:03:04,540 --> 00:03:07,740
solo che ci sono molti piccoli aggiustamenti che si sovrappongono l&#39;uno sull&#39;altro.

44
00:03:07,740 --> 00:03:11,380
Quindi inizierò qui ignorando completamente la notazione e passerò semplicemente in rassegna gli

45
00:03:11,380 --> 00:03:17,380
effetti che ogni esempio di allenamento ha sui pesi e sui bias.

46
00:03:17,380 --> 00:03:21,880
Poiché la funzione di costo implica la media di un certo costo per esempio su tutte

47
00:03:21,880 --> 00:03:26,980
le decine di migliaia di esempi di addestramento, il modo in cui regoliamo i pesi e

48
00:03:26,980 --> 00:03:31,740
i bias per un singolo passaggio di discesa del gradiente dipende anche da ogni singolo esempio.

49
00:03:31,740 --> 00:03:35,300
O meglio, in linea di principio dovrebbe, ma per efficienza computazionale faremo un piccolo

50
00:03:35,300 --> 00:03:39,860
trucchetto più avanti per evitare di dover colpire ogni singolo esempio per ogni passaggio.

51
00:03:39,860 --> 00:03:44,460
In altri casi, per ora, tutto ciò che faremo è concentrare la

52
00:03:44,460 --> 00:03:46,780
nostra attenzione su un singolo esempio, questa immagine di un 2.

53
00:03:46,780 --> 00:03:51,740
Che effetto dovrebbe avere questo esempio di formazione sul modo in cui i pesi e i pregiudizi vengono adeguati?

54
00:03:51,740 --> 00:03:56,040
Diciamo che siamo a un punto in cui la rete non è ancora ben addestrata,

55
00:03:56,040 --> 00:04:01,620
quindi le attivazioni nell&#39;output sembreranno piuttosto casuali, forse qualcosa come 0. 5, 0. 8, 0. 2,

56
00:04:01,620 --> 00:04:02,780
e così via.

57
00:04:02,780 --> 00:04:06,700
Non possiamo modificare direttamente tali attivazioni, abbiamo solo influenza sui

58
00:04:06,700 --> 00:04:11,380
pesi e sui pregiudizi, ma è utile tenere traccia di

59
00:04:11,380 --> 00:04:13,340
quali aggiustamenti desideriamo vengano apportati a quel livello di output.

60
00:04:13,340 --> 00:04:18,220
E poiché vogliamo che classifichi l&#39;immagine come 2, vogliamo che il terzo valore

61
00:04:18,220 --> 00:04:21,700
venga spostato verso l&#39;alto mentre tutti gli altri vengano spostati verso il basso.

62
00:04:21,700 --> 00:04:27,620
Inoltre, le dimensioni di questi nudge dovrebbero essere proporzionali alla

63
00:04:27,620 --> 00:04:30,220
distanza di ciascun valore corrente dal suo valore target.

64
00:04:30,220 --> 00:04:35,260
Ad esempio, l&#39;aumento dell&#39;attivazione del neurone numero 2 è in un

65
00:04:35,260 --> 00:04:39,620
certo senso più importante della diminuzione dell&#39;attivazione del neurone numero

66
00:04:39,620 --> 00:04:42,060
8, che è già abbastanza vicino a dove dovrebbe essere.

67
00:04:42,060 --> 00:04:46,260
Quindi, ingrandendo ulteriormente, concentriamoci solo su questo

68
00:04:46,260 --> 00:04:47,900
neurone, quello di cui desideriamo aumentare l&#39;attivazione.

69
00:04:47,900 --> 00:04:53,680
Ricorda, che l&#39;attivazione è definita come una certa somma ponderata di tutte

70
00:04:53,680 --> 00:04:58,380
le attivazioni nel livello precedente, più un bias, che viene poi

71
00:04:58,380 --> 00:05:01,900
collegato a qualcosa come la funzione di schiacciamento sigmoide o ReLU.

72
00:05:01,900 --> 00:05:07,060
Quindi ci sono tre diverse strade che possono

73
00:05:07,060 --> 00:05:08,060
collaborare per contribuire ad aumentare tale attivazione.

74
00:05:08,060 --> 00:05:12,800
È possibile aumentare il bias, aumentare i pesi

75
00:05:12,800 --> 00:05:15,300
e modificare le attivazioni dal livello precedente.

76
00:05:15,300 --> 00:05:19,720
Concentrandosi su come dovrebbero essere adeguati i pesi, si noti

77
00:05:19,720 --> 00:05:21,460
come i pesi abbiano effettivamente diversi livelli di influenza.

78
00:05:21,460 --> 00:05:25,100
Le connessioni con i neuroni più luminosi dello strato precedente hanno l&#39;effetto

79
00:05:25,100 --> 00:05:31,420
maggiore poiché questi pesi vengono moltiplicati per valori di attivazione maggiori.

80
00:05:31,420 --> 00:05:35,820
Quindi, se dovessi aumentare uno di questi pesi, in realtà avrebbe un&#39;influenza

81
00:05:35,820 --> 00:05:40,900
maggiore sulla funzione di costo finale rispetto all&#39;aumento dei pesi delle connessioni

82
00:05:40,900 --> 00:05:44,020
con neuroni più deboli, almeno per quanto riguarda questo esempio di allenamento.

83
00:05:44,020 --> 00:05:48,700
Ricorda, quando parliamo di discesa del gradiente, non ci interessa solo

84
00:05:48,700 --> 00:05:53,020
se ciascun componente debba essere spostato verso l&#39;alto o verso il

85
00:05:53,020 --> 00:05:54,020
basso, ci interessa anche quale ti dà il massimo rapporto qualità-prezzo.

86
00:05:54,020 --> 00:06:00,260
Questo, tra l&#39;altro, ricorda almeno in qualche modo una teoria delle neuroscienze

87
00:06:00,260 --> 00:06:04,900
su come le reti biologiche di neuroni apprendono, la teoria hebbiana, spesso

88
00:06:04,900 --> 00:06:06,940
riassunta nella frase, i neuroni che si attivano insieme si collegano insieme.

89
00:06:06,940 --> 00:06:12,460
Qui i maggiori aumenti di peso, il maggiore rafforzamento

90
00:06:12,460 --> 00:06:16,860
delle connessioni, avviene tra i neuroni che sono più

91
00:06:16,860 --> 00:06:18,100
attivi e quelli che desideriamo diventino più attivi.

92
00:06:18,100 --> 00:06:22,520
In un certo senso, i neuroni che si attivano mentre vedono un 2

93
00:06:22,520 --> 00:06:25,440
si collegano più fortemente a quelli che si attivano quando ci si pensa.

94
00:06:25,440 --> 00:06:29,240
Per essere chiari, non sono nella posizione di fare affermazioni in un modo o nell&#39;altro

95
00:06:29,240 --> 00:06:34,020
sul fatto che le reti artificiali di neuroni si comportino in qualche modo come

96
00:06:34,020 --> 00:06:39,440
i cervelli biologici, e questa idea di &quot;fuochi insieme, collegamenti insieme&quot; viene fornita con un

97
00:06:39,440 --> 00:06:41,760
paio di asterischi significativi, ma presa come un&#39;idea molto vaga. analogia, trovo interessante notare.

98
00:06:41,760 --> 00:06:46,760
Comunque, il terzo modo in cui possiamo contribuire ad aumentare l&#39;attivazione

99
00:06:46,760 --> 00:06:49,360
di questo neurone è modificando tutte le attivazioni dello strato precedente.

100
00:06:49,360 --> 00:06:55,080
Vale a dire, se tutto ciò che è collegato a quel neurone della cifra 2

101
00:06:55,080 --> 00:06:59,480
con un peso positivo diventasse più luminoso, e se tutto ciò che è connesso con

102
00:06:59,480 --> 00:07:02,680
un peso negativo diventasse più fioco, allora quel neurone della cifra 2 diventerebbe più attivo.

103
00:07:02,680 --> 00:07:06,200
E in modo simile alle variazioni di peso, otterrai il massimo

104
00:07:06,200 --> 00:07:10,840
dal tuo investimento cercando cambiamenti proporzionali alla dimensione dei pesi corrispondenti.

105
00:07:10,840 --> 00:07:16,520
Ora, ovviamente, non possiamo influenzare direttamente tali attivazioni, abbiamo

106
00:07:16,520 --> 00:07:18,320
solo il controllo sui pesi e sui pregiudizi.

107
00:07:18,320 --> 00:07:22,960
Ma proprio come con l&#39;ultimo livello, è utile

108
00:07:22,960 --> 00:07:23,960
tenere nota di quali sono i cambiamenti desiderati.

109
00:07:23,960 --> 00:07:29,040
Ma tieni presente che, rimpicciolendo di un passo qui, questo è

110
00:07:29,040 --> 00:07:30,040
solo ciò che vuole quel neurone di output della cifra 2.

111
00:07:30,040 --> 00:07:34,960
Ricorda, vogliamo anche che tutti gli altri neuroni nell&#39;ultimo strato diventino

112
00:07:34,960 --> 00:07:38,460
meno attivi e ciascuno di questi altri neuroni in uscita abbia

113
00:07:38,460 --> 00:07:43,200
i propri pensieri su cosa dovrebbe accadere a quel penultimo strato.

114
00:07:43,200 --> 00:07:49,220
Quindi il desiderio di questo neurone della cifra 2 viene sommato insieme

115
00:07:49,220 --> 00:07:54,800
ai desideri di tutti gli altri neuroni di output per ciò che

116
00:07:54,800 --> 00:08:00,240
dovrebbe accadere a questo penultimo strato, sempre in proporzione ai pesi corrispondenti

117
00:08:00,240 --> 00:08:01,740
e in proporzione a quanto ciascuno di questi neuroni ha bisogno cambiare.

118
00:08:01,740 --> 00:08:05,940
È proprio qui che entra in gioco l&#39;idea della propagazione all&#39;indietro.

119
00:08:05,940 --> 00:08:11,080
Sommando insieme tutti questi effetti desiderati, ottieni sostanzialmente un elenco di

120
00:08:11,080 --> 00:08:14,300
solleciti che vuoi che si verifichino su questo penultimo livello.

121
00:08:14,300 --> 00:08:18,740
E una volta che li hai, puoi applicare ricorsivamente lo stesso processo

122
00:08:18,740 --> 00:08:23,400
ai pesi e ai pregiudizi rilevanti che determinano quei valori, ripetendo lo

123
00:08:23,400 --> 00:08:29,180
stesso processo che ho appena seguito e andando all&#39;indietro attraverso la rete.

124
00:08:29,180 --> 00:08:33,960
E zoomando ancora un po’, ricorda che questo è proprio il modo in cui

125
00:08:33,960 --> 00:08:37,520
un singolo esempio di formazione desidera spingere ciascuno di quei pesi e pregiudizi.

126
00:08:37,520 --> 00:08:41,400
Se ascoltassimo solo ciò che vogliono quei 2, la rete alla

127
00:08:41,400 --> 00:08:44,140
fine sarebbe incentivata solo a classificare tutte le immagini come 2.

128
00:08:44,140 --> 00:08:49,500
Quindi quello che fai è seguire la stessa routine di backprop per ogni

129
00:08:49,500 --> 00:08:54,700
altro esempio di allenamento, registrando come ciascuno di loro vorrebbe modificare i

130
00:08:54,700 --> 00:09:02,300
pesi e i bias e fare una media insieme dei cambiamenti desiderati.

131
00:09:02,300 --> 00:09:08,260
Questa raccolta qui degli scostamenti medi per ciascun peso e bias è,

132
00:09:08,260 --> 00:09:12,340
in parole povere, il gradiente negativo della funzione di costo a cui

133
00:09:12,340 --> 00:09:14,360
si fa riferimento nell&#39;ultimo video, o almeno qualcosa di proporzionale ad esso.

134
00:09:14,360 --> 00:09:18,980
Dico in termini approssimativi solo perché devo ancora ottenere una precisione quantitativa su

135
00:09:18,980 --> 00:09:23,480
questi stimoli, ma se hai capito ogni cambiamento a cui ho appena fatto

136
00:09:23,480 --> 00:09:28,740
riferimento, perché alcuni sono proporzionalmente più grandi di altri e come devono essere

137
00:09:28,740 --> 00:09:34,100
sommati tutti insieme, capirai i meccanismi per cosa sta effettivamente facendo la backpropagation.

138
00:09:34,100 --> 00:09:38,540
In pratica, però, i computer impiegano molto tempo per sommare l&#39;influenza di

139
00:09:38,540 --> 00:09:43,120
ogni esempio di allenamento e di ogni fase di discesa del gradiente.

140
00:09:43,120 --> 00:09:45,540
Quindi ecco cosa viene fatto comunemente invece.

141
00:09:45,540 --> 00:09:50,460
Mescoli casualmente i tuoi dati di allenamento e li dividi in

142
00:09:50,460 --> 00:09:53,380
un sacco di mini-lotti, diciamo ognuno con 100 esempi di allenamento.

143
00:09:53,380 --> 00:09:56,980
Quindi calcoli un passaggio in base al mini-batch.

144
00:09:56,980 --> 00:10:00,840
Non è il gradiente effettivo della funzione di costo, che dipende da

145
00:10:00,840 --> 00:10:06,260
tutti i dati di addestramento, non da questo piccolo sottoinsieme, quindi non

146
00:10:06,260 --> 00:10:10,900
è il passo più efficiente in discesa, ma ogni mini-batch fornisce un&#39;approssimazione

147
00:10:10,900 --> 00:10:12,900
abbastanza buona e, cosa più importante, ti dà una notevole accelerazione computazionale.

148
00:10:12,900 --> 00:10:16,900
Se dovessi tracciare la traiettoria della tua rete sotto la superficie di costo rilevante, sarebbe un

149
00:10:16,900 --> 00:10:22,020
po’ più simile a un uomo ubriaco che inciampa senza meta giù da una collina ma

150
00:10:22,020 --> 00:10:26,880
fa passi rapidi, piuttosto che a un uomo che calcola attentamente e determina l’esatta direzione in

151
00:10:26,880 --> 00:10:31,620
discesa di ogni passo. prima di fare un passo molto lento e cauto in quella direzione.

152
00:10:31,620 --> 00:10:35,200
Questa tecnica è detta discesa del gradiente stocastico.

153
00:10:35,200 --> 00:10:40,400
C&#39;è molto da fare qui, quindi riassumiamolo per noi stessi, va bene?

154
00:10:40,400 --> 00:10:45,480
La backpropagation è l&#39;algoritmo per determinare come un singolo esempio di training

155
00:10:45,480 --> 00:10:50,040
vorrebbe spostare i pesi e i bias, non solo in termini di

156
00:10:50,040 --> 00:10:54,780
se dovrebbero aumentare o diminuire, ma in termini di quali proporzioni relative

157
00:10:54,780 --> 00:10:56,240
a tali cambiamenti causano la diminuzione più rapida del valore costo.

158
00:10:56,240 --> 00:11:00,720
Un vero passaggio di discesa del gradiente implicherebbe eseguire questa operazione per tutte le decine

159
00:11:00,720 --> 00:11:05,920
e migliaia di esempi di addestramento e calcolare la media delle modifiche desiderate ottenute,

160
00:11:05,920 --> 00:11:11,680
ma è un processo lento dal punto di vista computazionale, quindi invece si suddividono

161
00:11:11,680 --> 00:11:14,000
casualmente i dati in mini-batch e si calcola ogni passaggio rispetto a un mini-lotto.

162
00:11:14,000 --> 00:11:18,600
Esaminando ripetutamente tutti i mini-batch e apportando queste modifiche, convergerai verso un

163
00:11:18,600 --> 00:11:23,420
minimo locale della funzione di costo, vale a dire che la tua

164
00:11:23,420 --> 00:11:27,540
rete finirà per fare un ottimo lavoro sugli esempi di formazione.

165
00:11:27,540 --> 00:11:32,600
Quindi, detto tutto ciò, ogni riga di codice utilizzata per implementare il backprop

166
00:11:32,600 --> 00:11:37,680
corrisponde effettivamente a qualcosa che hai visto ora, almeno in termini informali.

167
00:11:37,680 --> 00:11:41,900
Ma a volte sapere cosa fa la matematica è solo metà della battaglia,

168
00:11:41,900 --> 00:11:44,780
e solo rappresentare quella dannata cosa è dove tutto diventa confuso e confuso.

169
00:11:44,780 --> 00:11:49,360
Quindi, per quelli di voi che vogliono andare più in profondità, il prossimo video

170
00:11:49,360 --> 00:11:53,400
analizza le stesse idee appena presentate qui, ma in termini di calcolo sottostante, che

171
00:11:53,400 --> 00:11:57,460
si spera dovrebbe renderlo un po&#39; più familiare vedendo l&#39;argomento in altre risorse.

172
00:11:57,460 --> 00:12:01,220
Prima di ciò, una cosa che vale la pena sottolineare è che affinché

173
00:12:01,220 --> 00:12:05,840
questo algoritmo funzioni, e questo vale per tutti i tipi di apprendimento

174
00:12:05,840 --> 00:12:06,840
automatico oltre alle sole reti neurali, sono necessari molti dati di addestramento.

175
00:12:06,840 --> 00:12:10,740
Nel nostro caso, una cosa che rende le cifre scritte a mano un esempio così carino è

176
00:12:10,740 --> 00:12:15,380
che esiste il database MNIST, con così tanti esempi che sono stati etichettati dagli esseri umani.

177
00:12:15,380 --> 00:12:19,000
Quindi una sfida comune con cui quelli di voi che lavorano nell&#39;apprendimento automatico avranno familiarità è semplicemente ottenere

178
00:12:19,040 --> 00:12:22,880
i dati di addestramento etichettati di cui avete effettivamente bisogno, sia che si tratti di far etichettare decine

179
00:12:22,880 --> 00:12:27,400
di migliaia di immagini o qualsiasi altro tipo di dati con cui potreste avere a che fare.

