1
00:00:00,000 --> 00:00:09,640
ที่นี่ เราจัดการกับการเผยแพร่ย้อนกลับ ซึ่งเป็นอัลกอริธึมหลักที่อยู่เบื้องหลังการเรียนรู้ของโครงข่ายประสาทเทียม

2
00:00:09,640 --> 00:00:13,320
หลังจากการสรุปคร่าวๆ เกี่ยวกับจุดที่เราอยู่

3
00:00:13,320 --> 00:00:17,400
สิ่งแรกที่ฉันจะทำคือคำแนะนำแบบเข้าใจง่ายเกี่ยวกับสิ่งที่อัลกอริทึมกำลังทำอยู่ โดยไม่ต้องอ้างอิงถึงสูตรใดๆ

4
00:00:17,400 --> 00:00:21,400
จากนั้น สำหรับผู้ที่ต้องการเจาะลึกคณิตศาสตร์

5
00:00:21,400 --> 00:00:24,040
วิดีโอถัดไปจะพูดถึงแคลคูลัสที่เป็นรากฐานของทั้งหมดนี้

6
00:00:24,040 --> 00:00:27,320
หากคุณดูวิดีโอสองรายการล่าสุด หรือหากคุณเพียงแค่กระโดดเข้ามาโดยมีพื้นหลังที่เหมาะสม

7
00:00:27,320 --> 00:00:31,080
คุณจะรู้ว่าโครงข่ายประสาทเทียมคืออะไร และเครือข่ายดังกล่าวส่งต่อข้อมูลอย่างไร

8
00:00:31,080 --> 00:00:35,520
ที่นี่ เรากำลังทำตัวอย่างคลาสสิกของการจดจำตัวเลขที่เขียนด้วยลายมือซึ่งค่าพิกเซลถูกป้อนเข้าไปในเลเยอร์แรกของเครือข่ายด้วยเซลล์ประสาท 784 ตัว

9
00:00:35,520 --> 00:00:40,280
และฉันได้แสดงเครือข่ายที่มีเลเยอร์ซ่อนอยู่สองชั้น โดยแต่ละเลเยอร์มีเซลล์ประสาทเพียง 16

10
00:00:40,280 --> 00:00:44,720
ตัว และเอาต์พุต เลเยอร์ของเซลล์ประสาท

11
00:00:44,720 --> 00:00:49,520
10 ตัว ซึ่งบ่งชี้ว่าเครือข่ายเลือกหลักใดเป็นคำตอบ

12
00:00:49,520 --> 00:00:54,480
ฉันคาดหวังให้คุณเข้าใจการไล่ระดับสีแบบเกรเดียนต์ด้วย ดังที่อธิบายไว้ในวิดีโอที่แล้ว

13
00:00:54,480 --> 00:01:00,160
และสิ่งที่เราหมายถึงโดยการเรียนรู้ก็คือ

14
00:01:00,160 --> 00:01:02,080
เราต้องการค้นหาว่าน้ำหนักและอคติใดที่ลดฟังก์ชันต้นทุนบางอย่างลง

15
00:01:02,080 --> 00:01:07,560
ขอเตือนไว้ก่อนว่า สำหรับค่าใช้จ่ายของตัวอย่างการฝึกอบรมรายการเดียว

16
00:01:07,560 --> 00:01:12,920
คุณจะต้องนำเอาท์พุตที่เครือข่ายให้มา ควบคู่ไปกับเอาท์พุตที่คุณต้องการให้มัน

17
00:01:12,920 --> 00:01:15,560
และเพิ่มกำลังสองของความแตกต่างระหว่างแต่ละส่วนประกอบ

18
00:01:15,560 --> 00:01:20,160
การทำเช่นนี้กับตัวอย่างการฝึกอบรมนับหมื่นรายการของคุณและเฉลี่ยผลลัพธ์

19
00:01:20,160 --> 00:01:23,040
จะทำให้คุณมีค่าใช้จ่ายรวมของเครือข่าย

20
00:01:23,040 --> 00:01:26,320
ราวกับว่านั่นยังไม่เพียงพอ อย่างที่อธิบายไว้ในวิดีโอที่แล้ว

21
00:01:26,320 --> 00:01:31,700
สิ่งที่เรากำลังมองหาคือเกรเดียนต์เชิงลบของฟังก์ชันต้นทุนนี้

22
00:01:31,700 --> 00:01:36,000
ซึ่งบอกคุณว่าคุณต้องเปลี่ยนน้ำหนักและอคติทั้งหมดอย่างไร

23
00:01:36,000 --> 00:01:43,080
การเชื่อมต่อเหล่านี้เพื่อลดต้นทุนได้อย่างมีประสิทธิภาพสูงสุด

24
00:01:43,080 --> 00:01:48,600
Backpropagation

25
00:01:48,600 --> 00:01:49,600
ซึ่งเป็นหัวข้อของวิดีโอนี้เป็นอัลกอริทึมสำหรับการคำนวณการไล่ระดับสีที่ซับซ้อนอย่างบ้าคลั่ง

26
00:01:49,600 --> 00:01:53,300
แนวคิดหนึ่งจากวิดีโอที่แล้ว ที่ผมอยากให้คุณยึดมั่นในใจตอนนี้ก็คือ เพราะการคิดถึงเวกเตอร์เกรเดียนต์เป็นทิศทางใน

27
00:01:53,300 --> 00:01:58,280
13,000 มิติ

28
00:01:58,280 --> 00:02:02,660
พูดง่ายๆ เลย นอกเหนือขอบเขตจินตนาการของเรา

29
00:02:02,660 --> 00:02:04,620
ยังมีอีกแนวคิดหนึ่งจากวิดีโอที่แล้ว วิธีคิดเกี่ยวกับมัน

30
00:02:04,620 --> 00:02:09,700
ขนาดของแต่ละองค์ประกอบตรงนี้จะบอกคุณว่าฟังก์ชันต้นทุนมีความละเอียดอ่อนต่อน้ำหนักและอคติแต่ละรายการอย่างไร

31
00:02:09,700 --> 00:02:11,820


32
00:02:11,820 --> 00:02:15,180
ตัวอย่างเช่น สมมติว่าคุณทำตามขั้นตอนที่ฉันกำลังจะอธิบาย และคำนวณค่าเกรเดียนต์ที่เป็นลบ

33
00:02:15,180 --> 00:02:19,800
และส่วนประกอบที่เกี่ยวข้องกับน้ำหนักบนขอบนี้จะเป็น 3

34
00:02:19,800 --> 00:02:26,940
2 ในขณะที่ส่วนประกอบที่เกี่ยวข้องกับขอบนี้ที่นี่ออกมาเป็น 0 1.

35
00:02:26,940 --> 00:02:31,520
วิธีที่คุณจะตีความได้ว่าต้นทุนของฟังก์ชันนั้นไวต่อการเปลี่ยนแปลงของน้ำหนักแรกนั้นมากกว่า 32 เท่า

36
00:02:31,520 --> 00:02:36,100
ดังนั้นหากคุณขยับค่านั้นสักหน่อย มันจะทำให้เกิดการเปลี่ยนแปลงกับราคา

37
00:02:36,100 --> 00:02:40,780
และการเปลี่ยนแปลงนั้น มากกว่าน้ำหนักตัวที่สองที่กระดิกได้

38
00:02:40,780 --> 00:02:45,580
32 เท่า

39
00:02:45,580 --> 00:02:52,500
โดยส่วนตัวแล้ว ตอนที่ฉันเรียนรู้เกี่ยวกับ backpropagation

40
00:02:52,500 --> 00:02:55,820
เป็นครั้งแรก ฉันคิดว่าสิ่งที่น่าสับสนที่สุดคือเพียงสัญกรณ์และการไล่ตามดัชนีของมันทั้งหมด

41
00:02:55,820 --> 00:03:00,240
แต่เมื่อคุณแกะสิ่งที่แต่ละส่วนของอัลกอริธึมนี้ทำจริงๆ แล้ว

42
00:03:00,240 --> 00:03:04,540
เอฟเฟ็กต์แต่ละอย่างที่มีนั้นค่อนข้างจะเข้าใจได้ง่ายจริงๆ เพียงแต่มีการปรับเปลี่ยนเล็กๆ

43
00:03:04,540 --> 00:03:07,740
น้อยๆ มากมายซ้อนกันเป็นชั้นๆ

44
00:03:07,740 --> 00:03:11,380
ผมจะเริ่มต้นตรงนี้โดยไม่สนใจสัญกรณ์เลย

45
00:03:11,380 --> 00:03:17,380
และแค่ลองดูผลกระทบที่แต่ละตัวอย่างการฝึกมีต่อน้ำหนักและอคติ

46
00:03:17,380 --> 00:03:21,880
เนื่องจากฟังก์ชันต้นทุนเกี่ยวข้องกับการเฉลี่ยต้นทุนต่อตัวอย่างจากตัวอย่างการฝึกนับหมื่นทั้งหมด

47
00:03:21,880 --> 00:03:26,980
วิธีที่เราปรับน้ำหนักและอคติสำหรับขั้นตอนการไล่ระดับไล่ระดับขั้นเดียวยังขึ้นอยู่กับทุกตัวอย่างด้วย

48
00:03:26,980 --> 00:03:31,740


49
00:03:31,740 --> 00:03:35,300
หรือโดยหลักการแล้ว มันควรจะเป็น แต่เพื่อประสิทธิภาพในการคำนวณ

50
00:03:35,300 --> 00:03:39,860
เราจะใช้กลเม็ดเล็กๆ น้อยๆ ในภายหลังเพื่อป้องกันไม่ให้คุณต้องอ่านทุกตัวอย่างในทุกขั้นตอน

51
00:03:39,860 --> 00:03:44,460
ในกรณีอื่นๆ ตอนนี้ สิ่งที่เราจะทำคือมุ่งความสนใจไปที่ตัวอย่างเดียว

52
00:03:44,460 --> 00:03:46,780
นั่นคือรูป 2 นี้

53
00:03:46,780 --> 00:03:51,740
ตัวอย่างการฝึกอบรมนี้ควรมีผลกระทบอย่างไรต่อการปรับน้ำหนักและอคติ

54
00:03:51,740 --> 00:03:56,040
สมมติว่าเราอยู่ในจุดที่เครือข่ายยังไม่ได้รับการฝึกฝนมาเป็นอย่างดี ดังนั้นการเปิดใช้งานในเอาต์พุตจึงดูค่อนข้างสุ่ม อาจเป็นประมาณ

55
00:03:56,040 --> 00:04:01,620
0 5, 0. 8, 0. 2

56
00:04:01,620 --> 00:04:02,780
ต่อไปและต่อไป

57
00:04:02,780 --> 00:04:06,700
เราไม่สามารถเปลี่ยนแปลงการเปิดใช้งานเหล่านั้นได้โดยตรง

58
00:04:06,700 --> 00:04:11,380
เราเพียงแต่มีอิทธิพลต่อน้ำหนักและอคติเท่านั้น

59
00:04:11,380 --> 00:04:13,340
แต่การติดตามว่าการปรับเปลี่ยนใดที่เราต้องการให้เกิดขึ้นกับเลเยอร์เอาต์พุตนั้นก็มีประโยชน์

60
00:04:13,340 --> 00:04:18,220
และเนื่องจากเราต้องการให้มันจัดประเภทรูปภาพเป็น 2 เราจึงต้องการให้ค่าที่สามนั้นถูกปัดขึ้น

61
00:04:18,220 --> 00:04:21,700
ในขณะที่ค่าอื่นๆ ทั้งหมดจะถูกปัดลง

62
00:04:21,700 --> 00:04:27,620
นอกจากนี้

63
00:04:27,620 --> 00:04:30,220
ขนาดของการกระตุ้นเตือนเหล่านี้ควรเป็นสัดส่วนกับระยะห่างระหว่างค่าปัจจุบันแต่ละค่าจากค่าเป้าหมาย

64
00:04:30,220 --> 00:04:35,260
ตัวอย่างเช่น การเพิ่มขึ้นของการกระตุ้นเซลล์ประสาทหมายเลข

65
00:04:35,260 --> 00:04:39,620
2 นั้นมีความสำคัญมากกว่าการลดลงของเซลล์ประสาทหมายเลข

66
00:04:39,620 --> 00:04:42,060
8 ซึ่งค่อนข้างใกล้เคียงกับตำแหน่งที่ควรจะเป็นอยู่แล้ว

67
00:04:42,060 --> 00:04:46,260
ถ้าจะขยายเข้าไปอีก เรามาดูที่เซลล์ประสาทอันเดียวนี้

68
00:04:46,260 --> 00:04:47,900
ซึ่งเป็นเซลล์ประสาทที่เราอยากจะเพิ่มการกระตุ้น

69
00:04:47,900 --> 00:04:53,680
โปรดจำไว้ว่า การเปิดใช้งานนั้นถูกกำหนดให้เป็นผลรวมแบบถ่วงน้ำหนักของการเปิดใช้งานทั้งหมดในเลเยอร์ก่อนหน้า บวกกับไบแอส ซึ่งทั้งหมดจะถูกเสียบเข้ากับฟังก์ชันบางอย่าง

70
00:04:53,680 --> 00:04:58,380
เช่น ฟังก์ชัน sigmoid

71
00:04:58,380 --> 00:05:01,900
squishification หรือ ReLU

72
00:05:01,900 --> 00:05:07,060
ดังนั้นจึงมีสามช่องทางที่แตกต่างกันที่สามารถร่วมมือกันเพื่อช่วยเพิ่มการเปิดใช้งานนั้นได้

73
00:05:07,060 --> 00:05:08,060


74
00:05:08,060 --> 00:05:12,800
คุณสามารถเพิ่มอคติ คุณสามารถเพิ่มน้ำหนัก

75
00:05:12,800 --> 00:05:15,300
และคุณสามารถเปลี่ยนการเปิดใช้งานจากเลเยอร์ก่อนหน้าได้

76
00:05:15,300 --> 00:05:19,720
โดยเน้นไปที่วิธีการปรับตุ้มน้ำหนัก ให้สังเกตว่าจริงๆ

77
00:05:19,720 --> 00:05:21,460
แล้วตุ้มน้ำหนักมีระดับอิทธิพลที่แตกต่างกันอย่างไร

78
00:05:21,460 --> 00:05:25,100
การเชื่อมต่อกับเซลล์ประสาทที่สว่างที่สุดจากเลเยอร์ก่อนหน้ามีผลมากที่สุดเนื่องจากน้ำหนักเหล่านั้นจะถูกคูณด้วยค่าการเปิดใช้งานที่มากขึ้น

79
00:05:25,100 --> 00:05:31,420


80
00:05:31,420 --> 00:05:35,820
ดังนั้น หากคุณจะเพิ่มน้ำหนักตัวใดตัวหนึ่ง

81
00:05:35,820 --> 00:05:40,900
จริงๆ แล้วมันจะมีอิทธิพลมากกว่าในฟังก์ชันต้นทุนขั้นสุดท้าย

82
00:05:40,900 --> 00:05:44,020
มากกว่าการเพิ่มน้ำหนักของการเชื่อมต่อกับเซลล์ประสาทที่หรี่ลง อย่างน้อยก็เท่าที่ตัวอย่างการฝึกนี้เกี่ยวข้อง

83
00:05:44,020 --> 00:05:48,700
โปรดจำไว้ว่า เมื่อเราพูดถึงการไล่ระดับไล่ระดับ

84
00:05:48,700 --> 00:05:53,020
เราไม่เพียงแต่สนใจว่าแต่ละองค์ประกอบควรถูกดันขึ้นหรือลง

85
00:05:53,020 --> 00:05:54,020
แต่เราสนใจว่าองค์ประกอบใดที่คุ้มค่าที่สุดสำหรับคุณ

86
00:05:54,020 --> 00:06:00,260
อย่างไรก็ตาม อย่างน้อยก็ค่อนข้างชวนให้นึกถึงทฤษฎีทางประสาทวิทยาศาสตร์ว่าเครือข่ายทางชีววิทยาของเซลล์ประสาทเรียนรู้ได้อย่างไร

87
00:06:00,260 --> 00:06:04,900
ทฤษฎีฮิบเบียน ซึ่งมักสรุปไว้ในวลีนี้

88
00:06:04,900 --> 00:06:06,940
เซลล์ประสาทที่ยิงเชื่อมเข้าด้วยกัน

89
00:06:06,940 --> 00:06:12,460
การเพิ่มน้ำหนักครั้งใหญ่ที่สุด

90
00:06:12,460 --> 00:06:16,860
การเสริมสร้างการเชื่อมต่อที่ยิ่งใหญ่ที่สุดเกิดขึ้นระหว่างเซลล์ประสาทที่แอคทีฟมากที่สุดกับเซลล์ที่เราอยากให้มีความแอคทีฟมากขึ้น

91
00:06:16,860 --> 00:06:18,100


92
00:06:18,100 --> 00:06:22,520
ในแง่หนึ่ง เซลล์ประสาทที่กำลังส่งสัญญาณในขณะที่มองเห็น

93
00:06:22,520 --> 00:06:25,440
2 จะมีการเชื่อมโยงอย่างมากกับเซลล์ประสาทที่ส่งสัญญาณเมื่อคิดถึงมัน

94
00:06:25,440 --> 00:06:29,240
เพื่อให้ชัดเจน ฉันไม่อยู่ในฐานะที่จะแถลงไม่ทางใดก็ทางหนึ่งว่าเครือข่ายเซลล์ประสาทเทียมมีพฤติกรรมเหมือนสมองทางชีววิทยาหรือไม่

95
00:06:29,240 --> 00:06:34,020
และสิ่งนี้รวมเอาแนวคิดที่เชื่อมโยงเข้าด้วยกัน มาพร้อมกับเครื่องหมายดอกจันที่มีความหมายสองสามอัน

96
00:06:34,020 --> 00:06:39,440
แต่ถือเป็นการหลวมมาก การเปรียบเทียบ

97
00:06:39,440 --> 00:06:41,760
ฉันคิดว่ามันน่าสนใจที่จะทราบ

98
00:06:41,760 --> 00:06:46,760
อย่างไรก็ตาม

99
00:06:46,760 --> 00:06:49,360
วิธีที่สามที่เราสามารถช่วยเพิ่มการกระตุ้นของเซลล์ประสาทนี้คือโดยการเปลี่ยนการเปิดใช้งานทั้งหมดในเลเยอร์ก่อนหน้า

100
00:06:49,360 --> 00:06:55,080
กล่าวคือ ถ้าทุกสิ่งที่เชื่อมต่อกับเซลล์ประสาทหลัก 2

101
00:06:55,080 --> 00:06:59,480
ที่มีน้ำหนักเป็นบวกนั้นสว่างขึ้น และถ้าทุกสิ่งที่เกี่ยวข้องกับน้ำหนักลบนั้นหรี่ลง เซลล์ประสาทหลัก

102
00:06:59,480 --> 00:07:02,680
2 นั้นก็จะมีความกระตือรือร้นมากขึ้น

103
00:07:02,680 --> 00:07:06,200
และเช่นเดียวกับการเปลี่ยนแปลงน้ำหนัก

104
00:07:06,200 --> 00:07:10,840
คุณจะได้รับผลตอบแทนสูงสุดจากเงินที่เสียไปโดยการค้นหาการเปลี่ยนแปลงที่เป็นสัดส่วนกับขนาดของน้ำหนักที่สอดคล้องกัน

105
00:07:10,840 --> 00:07:16,520
แน่นอนว่าเราไม่สามารถมีอิทธิพลต่อการเปิดใช้งานเหล่านั้นได้โดยตรง

106
00:07:16,520 --> 00:07:18,320
เราทำได้เพียงควบคุมน้ำหนักและอคติเท่านั้น

107
00:07:18,320 --> 00:07:22,960
แต่เช่นเดียวกับเลเยอร์สุดท้าย

108
00:07:22,960 --> 00:07:23,960
การจดบันทึกว่าการเปลี่ยนแปลงที่ต้องการคืออะไรจะเป็นประโยชน์

109
00:07:23,960 --> 00:07:29,040
แต่โปรดจำไว้ว่า เมื่อซูมออกหนึ่งขั้นที่นี่ นี่เป็นเพียงสิ่งที่เซลล์ประสาทเอาต์พุตหลัก

110
00:07:29,040 --> 00:07:30,040
2 ต้องการเท่านั้น

111
00:07:30,040 --> 00:07:34,960
โปรดจำไว้ว่า เรายังต้องการให้เซลล์ประสาทอื่นๆ

112
00:07:34,960 --> 00:07:38,460
ทั้งหมดในเลเยอร์สุดท้ายมีการเคลื่อนไหวน้อยลง และเซลล์ประสาทเอาท์พุตอื่นๆ

113
00:07:38,460 --> 00:07:43,200
เหล่านั้นก็มีความคิดของตัวเองเกี่ยวกับสิ่งที่จะเกิดขึ้นกับเลเยอร์ที่สองถึงเลเยอร์สุดท้ายนั้น

114
00:07:43,200 --> 00:07:49,220
ดังนั้นความปรารถนาของเซลล์ประสาทหลัก 2

115
00:07:49,220 --> 00:07:54,800
นี้จึงถูกรวมเข้ากับความปรารถนาของเซลล์ประสาทเอาท์พุตอื่นๆ ทั้งหมดสำหรับสิ่งที่จะเกิดขึ้นกับเลเยอร์ที่สองถึงเลเยอร์สุดท้ายนี้

116
00:07:54,800 --> 00:08:00,240
อีกครั้งตามสัดส่วนของน้ำหนักที่สอดคล้องกัน และเป็นสัดส่วนกับจำนวนเซลล์ประสาทแต่ละอันที่ต้องการ

117
00:08:00,240 --> 00:08:01,740
เพื่อเปลี่ยน.

118
00:08:01,740 --> 00:08:05,940
นี่คือจุดที่แนวคิดเรื่องการเผยแพร่แบบย้อนกลับเข้ามา

119
00:08:05,940 --> 00:08:11,080
เมื่อรวมเอฟเฟกต์ที่ต้องการทั้งหมดเข้าด้วยกัน

120
00:08:11,080 --> 00:08:14,300
คุณจะได้รับรายการการกระตุ้นเตือนที่คุณต้องการให้เกิดขึ้นในเลเยอร์ที่สองถึงเลเยอร์สุดท้ายนี้

121
00:08:14,300 --> 00:08:18,740
และเมื่อคุณมีสิ่งเหล่านี้แล้ว คุณสามารถใช้กระบวนการเดิมซ้ำๆ

122
00:08:18,740 --> 00:08:23,400
กับน้ำหนักและอคติที่เกี่ยวข้องซึ่งกำหนดค่าเหล่านั้น

123
00:08:23,400 --> 00:08:29,180
ทำซ้ำขั้นตอนเดียวกับที่ฉันเพิ่งเดินผ่านและย้อนกลับผ่านเครือข่าย

124
00:08:29,180 --> 00:08:33,960
และเมื่อขยายออกไปอีกเล็กน้อย

125
00:08:33,960 --> 00:08:37,520
โปรดจำไว้ว่าทั้งหมดนี้เป็นเพียงวิธีที่ตัวอย่างการฝึกอบรมเดียวต้องการจะสะกิดน้ำหนักและอคติเหล่านั้น

126
00:08:37,520 --> 00:08:41,400
ถ้าเราเพียงแต่ฟังสิ่งที่ 2 นั้นต้องการ

127
00:08:41,400 --> 00:08:44,140
ในที่สุดเครือข่ายก็จะถูกจูงใจให้จัดประเภทภาพทั้งหมดเป็น 2

128
00:08:44,140 --> 00:08:49,500
ดังนั้นสิ่งที่คุณทำคือทำตามขั้นตอน backprop

129
00:08:49,500 --> 00:08:54,700
เดียวกันนี้สำหรับตัวอย่างการฝึกอบรมอื่นๆ ทั้งหมด

130
00:08:54,700 --> 00:09:02,300
โดยบันทึกว่าแต่ละคนต้องการเปลี่ยนแปลงน้ำหนักและอคติอย่างไร และเฉลี่ยการเปลี่ยนแปลงที่ต้องการเหล่านั้นรวมกัน

131
00:09:02,300 --> 00:09:08,260
คอลเลกชันนี้ของการกระตุ้นโดยเฉลี่ยต่อน้ำหนักและอคติแต่ละอย่าง พูดง่ายๆ

132
00:09:08,260 --> 00:09:12,340
ก็คือความชันเชิงลบของฟังก์ชันต้นทุนที่อ้างอิงในวิดีโอที่แล้ว

133
00:09:12,340 --> 00:09:14,360
หรืออย่างน้อยก็มีบางอย่างที่เป็นสัดส่วนกับมัน

134
00:09:14,360 --> 00:09:18,980
ฉันพูดแบบหลวมๆ เพียงเพราะฉันยังไม่ได้รับความแม่นยำในเชิงปริมาณเกี่ยวกับการกระตุ้นเตือนเหล่านั้น แต่ถ้าคุณเข้าใจทุกการเปลี่ยนแปลงที่ฉันเพิ่งอ้างอิงไป

135
00:09:18,980 --> 00:09:23,480
เหตุใดการเปลี่ยนแปลงบางอย่างจึงใหญ่กว่าการเปลี่ยนแปลงอื่นๆ ตามสัดส่วน และวิธีที่จะรวมการเปลี่ยนแปลงทั้งหมดเข้าด้วยกัน

136
00:09:23,480 --> 00:09:28,740
คุณจะเข้าใจกลไกของ จริงๆ แล้ว

137
00:09:28,740 --> 00:09:34,100
backpropagation กำลังทำอะไรอยู่

138
00:09:34,100 --> 00:09:38,540
อย่างไรก็ตาม ในทางปฏิบัติ

139
00:09:38,540 --> 00:09:43,120
คอมพิวเตอร์จะใช้เวลานานมากในการเพิ่มอิทธิพลของตัวอย่างการฝึกทุกตัวในทุกขั้นตอนการไล่ระดับสี

140
00:09:43,120 --> 00:09:45,540
ต่อไปนี้คือสิ่งที่ทำกันโดยทั่วไปแทน

141
00:09:45,540 --> 00:09:50,460
คุณสุ่มสับเปลี่ยนข้อมูลการฝึกของคุณและแบ่งเป็นกลุ่มย่อย สมมติว่าแต่ละอันมีตัวอย่างการฝึก

142
00:09:50,460 --> 00:09:53,380
100 ตัวอย่าง

143
00:09:53,380 --> 00:09:56,980
จากนั้นคุณคำนวณขั้นตอนตามมินิแบทช์

144
00:09:56,980 --> 00:10:00,840
ไม่ใช่การไล่ระดับที่แท้จริงของฟังก์ชันต้นทุน ซึ่งขึ้นอยู่กับข้อมูลการฝึกทั้งหมด

145
00:10:00,840 --> 00:10:06,260
ไม่ใช่ชุดย่อยเล็กๆ นี้

146
00:10:06,260 --> 00:10:10,900
ดังนั้นจึงไม่ใช่การลงเนินที่มีประสิทธิภาพมากที่สุด แต่ชุดย่อยแต่ละชุดจะให้การประมาณที่ดีทีเดียว

147
00:10:10,900 --> 00:10:12,900
และที่สำคัญกว่านั้น ช่วยให้คุณเร่งความเร็วในการคำนวณได้อย่างมาก

148
00:10:12,900 --> 00:10:16,900
หากคุณต้องวางแผนเส้นทางของเครือข่ายของคุณภายใต้ต้นทุนที่เกี่ยวข้อง

149
00:10:16,900 --> 00:10:22,020
มันจะเหมือนกับคนเมาที่สะดุดล้มลงเนินอย่างไร้จุดหมายแต่ทำตามขั้นตอนอย่างรวดเร็ว

150
00:10:22,020 --> 00:10:26,880
แทนที่จะเป็นคนที่คำนวณอย่างรอบคอบเพื่อกำหนดทิศทางลงเนินที่แน่นอนของแต่ละก้าว

151
00:10:26,880 --> 00:10:31,620
ก่อนที่จะก้าวไปอย่างช้าๆและระมัดระวังไปในทิศทางนั้น

152
00:10:31,620 --> 00:10:35,200
เทคนิคนี้เรียกว่าการไล่ระดับสีแบบสุ่ม

153
00:10:35,200 --> 00:10:40,400
มีเรื่องเกิดขึ้นมากมายที่นี่ งั้นเรามาสรุปกันเองเลยดีไหม?

154
00:10:40,400 --> 00:10:45,480
Backpropagation เป็นอัลกอริธึมสำหรับพิจารณาว่าตัวอย่างการฝึกเดี่ยวต้องการจะสะกิดน้ำหนักและอคติอย่างไร

155
00:10:45,480 --> 00:10:50,040
ไม่ใช่แค่ในแง่ของว่าควรจะขึ้นหรือลง

156
00:10:50,040 --> 00:10:54,780
แต่ในแง่ของสัดส่วนสัมพันธ์กับการเปลี่ยนแปลงเหล่านั้นที่ทำให้การลดลงอย่างรวดเร็วที่สุด

157
00:10:54,780 --> 00:10:56,240
ค่าใช้จ่าย.

158
00:10:56,240 --> 00:11:00,720
ขั้นตอนการไล่ระดับที่แท้จริงจะเกี่ยวข้องกับการทำสิ่งนี้กับตัวอย่างการฝึกนับหมื่นตัวอย่างทั้งหมดของคุณ และหาค่าเฉลี่ยของการเปลี่ยนแปลงที่ต้องการที่คุณได้รับ

159
00:11:00,720 --> 00:11:05,920
แต่นั่นเป็นการคำนวณที่ช้า

160
00:11:05,920 --> 00:11:11,680
ดังนั้นคุณจึงสุ่มแบ่งย่อยข้อมูลออกเป็นชุดย่อยและคำนวณแต่ละขั้นตอนโดยคำนึงถึง

161
00:11:11,680 --> 00:11:14,000
มินิแบทช์

162
00:11:14,000 --> 00:11:18,600
ทำซ้ำชุดย่อยทั้งหมดและทำการปรับเปลี่ยนเหล่านี้

163
00:11:18,600 --> 00:11:23,420
คุณจะมาบรรจบกันสู่ฟังก์ชันต้นทุนขั้นต่ำในท้องถิ่น

164
00:11:23,420 --> 00:11:27,540
ซึ่งกล่าวได้ว่าเครือข่ายของคุณจะจบลงด้วยการทำงานที่ดีมากกับตัวอย่างการฝึกอบรม

165
00:11:27,540 --> 00:11:32,600
จากที่กล่าวมาทั้งหมด โค้ดทุกบรรทัดที่จะใช้กับ backprop จริง

166
00:11:32,600 --> 00:11:37,680
ๆ แล้วสอดคล้องกับสิ่งที่คุณได้เห็นในตอนนี้ อย่างน้อยก็ในแง่ที่ไม่เป็นทางการ

167
00:11:37,680 --> 00:11:41,900
แต่บางครั้งการรู้ว่าคณิตศาสตร์ทำอะไรก็มีชัยไปเพียงครึ่งเดียว

168
00:11:41,900 --> 00:11:44,780
และการเป็นตัวแทนของสิ่งที่น่ารังเกียจก็คือจุดที่ทุกอย่างสับสนและสับสน

169
00:11:44,780 --> 00:11:49,360
ดังนั้น สำหรับผู้ที่ต้องการเจาะลึก

170
00:11:49,360 --> 00:11:53,400
วิดีโอหน้าจะพูดถึงแนวคิดเดียวกันกับที่เพิ่งนำเสนอที่นี่ แต่ในแง่ของแคลคูลัสพื้นฐาน

171
00:11:53,400 --> 00:11:57,460
ซึ่งหวังว่าจะทำให้คุ้นเคยขึ้นอีกหน่อยเมื่อคุณเห็นหัวข้อใน ทรัพยากรอื่นๆ

172
00:11:57,460 --> 00:12:01,220
ก่อนหน้านั้น สิ่งหนึ่งที่ควรเน้นย้ำคือเพื่อให้อัลกอริธึมนี้ทำงานได้

173
00:12:01,220 --> 00:12:05,840
และการเรียนรู้ของเครื่องทุกประเภทนอกเหนือจากโครงข่ายประสาทเทียม

174
00:12:05,840 --> 00:12:06,840
คุณต้องมีข้อมูลการฝึกอบรมจำนวนมาก

175
00:12:06,840 --> 00:12:10,740
ในกรณีของเรา สิ่งหนึ่งที่ทำให้ตัวเลขที่เขียนด้วยลายมือเป็นตัวอย่างที่ดีก็คือ มีฐานข้อมูล

176
00:12:10,740 --> 00:12:15,380
MNIST อยู่ โดยมีตัวอย่างมากมายที่มนุษย์ติดป้ายไว้

177
00:12:15,380 --> 00:12:19,000
ความท้าทายทั่วไปที่พวกคุณที่ทำงานด้านการเรียนรู้ของเครื่องจะคุ้นเคยก็คือการได้รับข้อมูลการฝึกอบรมที่มีป้ายกำกับที่คุณต้องการจริงๆ

178
00:12:19,040 --> 00:12:22,880
ไม่ว่าจะเป็นการที่ผู้คนติดป้ายกำกับรูปภาพนับหมื่นภาพ

179
00:12:22,880 --> 00:12:27,400
หรือข้อมูลประเภทอื่นใดก็ตามที่คุณอาจต้องเผชิญอยู่

