1
00:00:00,000 --> 00:00:09,640
ここでは、ニューラル ネットワークの学習方法の背後にある中心的なアルゴリズムであるバックプロパゲーションに取り組みます。

2
00:00:09,640 --> 00:00:13,320
ここまでの概要を簡単にまとめた後、最初に、数式を参照せずに、

3
00:00:13,320 --> 00:00:17,400
アルゴリズムが実際に何を行っているかを直感的に説明します。

4
00:00:17,400 --> 00:00:21,400
次に、数学について詳しく知りたい人のために、次のビデオで

5
00:00:21,400 --> 00:00:24,040
は、これらすべての基礎となる微積分について説明します。

6
00:00:24,040 --> 00:00:27,320
最後の 2 つのビデオをご覧になった場合、または適切な背景を理解してすぐに参加した場合は、ニューラル ネットワーク

7
00:00:27,320 --> 00:00:31,080
とは何か、またニューラル ネットワークがどのように情報をフィードフォワードするかについては理解しているでしょう。

8
00:00:31,080 --> 00:00:35,520
ここでは、ピクセル値が 784 個のニューロンを含むネットワークの最初の層に入力さ

9
00:00:35,520 --> 00:00:40,280
れる手書きの数字を認識する古典的な例を行っています。また、それぞれ 16 個のニ

10
00:00:40,280 --> 00:00:44,720
ューロンしか持たない 2 つの隠れ層と出力を含むネットワークを示しています。 10

11
00:00:44,720 --> 00:00:49,520
個のニューロンの層。ネットワークがどの桁を答えとして選択しているかを示します。

12
00:00:49,520 --> 00:00:54,480
また、前回のビデオで説明したように、勾配降下法と、学習とは、

13
00:00:54,480 --> 00:01:00,160
どの重みとバイアスが特定のコスト関数を最小化するかを見つける

14
00:01:00,160 --> 00:01:02,080
ことを意味することを理解していただくことも期待しています。

15
00:01:02,080 --> 00:01:07,560
簡単に思い出していただきたいのですが、1 つのトレーニング サンプル

16
00:01:07,560 --> 00:01:12,920
のコストとして、ネットワークが提供する出力と、ネットワークに提供して

17
00:01:12,920 --> 00:01:15,560
もらいたい出力を取得し、各コンポーネントの差の 2 乗を合計します。

18
00:01:15,560 --> 00:01:20,160
これを何万ものトレーニング例すべてに対して実行し、結

19
00:01:20,160 --> 00:01:23,040
果を平均すると、ネットワークの総コストが得られます。

20
00:01:23,040 --> 00:01:26,320
最後のビデオで説明したように、それだけでは考えるのが十分ではないか

21
00:01:26,320 --> 00:01:31,700
のように、私たちが探しているのはこのコスト関数の負の勾配です。こ

22
00:01:31,700 --> 00:01:36,000
れは、すべての重みとバイアスをどのように変更する必要があるかを示し

23
00:01:36,000 --> 00:01:43,080
ています。これらの接続により、最も効率的にコストが削減されます。

24
00:01:43,080 --> 00:01:48,600
このビデオのトピックであるバックプロパゲーションは、

25
00:01:48,600 --> 00:01:49,600
非常に複雑な勾配を計算するためのアルゴリズムです。

26
00:01:49,600 --> 00:01:53,300
最後のビデオで、今すぐにしっかりと頭の中に留めておいてほしい 1 つ

27
00:01:53,300 --> 00:01:58,280
のアイデアは、勾配ベクトルを 13,000 次元の方向として考えるこ

28
00:01:58,280 --> 00:02:02,660
とは、簡単に言えば、私たちの想像の範囲を超えているため、別のアイデア

29
00:02:02,660 --> 00:02:04,620
があるということです。あなたがそれについて考えることができる方法。

30
00:02:04,620 --> 00:02:09,700
ここでの各成分の大きさは、コスト関数が各重みとバイ

31
00:02:09,700 --> 00:02:11,820
アスに対してどの程度敏感であるかを示しています。

32
00:02:11,820 --> 00:02:15,180
たとえば、これから説明するプロセスを実行し、負の勾配を計算したところ、こ

33
00:02:15,180 --> 00:02:19,800
のエッジの重みに関連付けられたコンポーネントが 3 であることが判明し

34
00:02:19,800 --> 00:02:26,940
たとします。2 ですが、このエッジに関連付けられたコンポーネントは 0 として出力されます。1.

35
00:02:26,940 --> 00:02:31,520
これをどう解釈するかというと、関数のコストは最初の重みの変

36
00:02:31,520 --> 00:02:36,100
化に対して 32 倍敏感であるため、その値を少し変更する

37
00:02:36,100 --> 00:02:40,780
と、コストに何らかの変化が生じ、その変化は2 番目の重りに

38
00:02:40,780 --> 00:02:45,580
対する同じ揺れが与える値よりも 32 倍大きくなります。

39
00:02:45,580 --> 00:02:52,500
個人的に、私が最初にバックプロパゲーションについて学んだとき、最

40
00:02:52,500 --> 00:02:55,820
も混乱したのは単にその表記とインデックスの追跡だったと思います。

41
00:02:55,820 --> 00:03:00,240
しかし、このアルゴリズムの各部分が実際に何をしているのかを紐

42
00:03:00,240 --> 00:03:04,540
解いてみると、それがもたらす個々の効果は実際には非常に直感的

43
00:03:04,540 --> 00:03:07,740
であり、ただ多くの小さな調整が積み重ねられているだけです。

44
00:03:07,740 --> 00:03:11,380
したがって、ここでは表記を完全に無視して物事を開始し、各トレー

45
00:03:11,380 --> 00:03:17,380
ニング例が重みとバイアスに与える影響を段階的に見ていきます。

46
00:03:17,380 --> 00:03:21,880
コスト関数には、数万のトレーニング サンプル全体にわたるサンプル

47
00:03:21,880 --> 00:03:26,980
あたりの特定のコストの平均が含まれるため、単一の勾配降下ステップ

48
00:03:26,980 --> 00:03:31,740
の重みとバイアスを調整する方法も、すべてのサンプルに依存します。

49
00:03:31,740 --> 00:03:35,300
というか、原理的にはそうすべきですが、計算効率を高めるために、各ステップですべて

50
00:03:35,300 --> 00:03:39,860
のサンプルをヒットする必要がないように、後でちょっとしたトリックを実行します。

51
00:03:39,860 --> 00:03:44,460
他のケースでは、現時点では、この 2 の画

52
00:03:44,460 --> 00:03:46,780
像という 1 つの例に注目するだけです。

53
00:03:46,780 --> 00:03:51,740
この 1 つのトレーニング例は、重みとバイアスの調整方法にどのような影響を与えるでしょうか?

54
00:03:51,740 --> 00:03:56,040
ネットワークがまだ十分にトレーニングされていない段階にあるとします。そのため、出力内のアクティ

55
00:03:56,040 --> 00:04:01,620
ベーションはかなりランダムに、おそらく 0 のようなものになるとします。5、0。8、0。2

56
00:04:01,620 --> 00:04:02,780
、延々と。

57
00:04:02,780 --> 00:04:06,700
これらのアクティベーションを直接変更することはできず、影

58
00:04:06,700 --> 00:04:11,380
響を受けるのは重みとバイアスのみですが、その出力層に対し

59
00:04:11,380 --> 00:04:13,340
てどの調整を行う必要があるかを追跡するのに役立ちます。

60
00:04:13,340 --> 00:04:18,220
そして、画像を 2 として分類したいので、3 番目の値を少

61
00:04:18,220 --> 00:04:21,700
しずつ上げて、他のすべての値を少しずつ下げるようにします。

62
00:04:21,700 --> 00:04:27,620
さらに、これらのナッジのサイズは、各現在の値がその目標

63
00:04:27,620 --> 00:04:30,220
値からどれだけ離れているかに比例する必要があります。

64
00:04:30,220 --> 00:04:35,260
たとえば、2 番のニューロンの活性化の増加は、

65
00:04:35,260 --> 00:04:39,620
ある意味で、すでにあるべき状態にかなり近づいて

66
00:04:39,620 --> 00:04:42,060
いる 8 番のニューロンの減少よりも重要です。

67
00:04:42,060 --> 00:04:46,260
そこでさらにズームインして、活性化を高めたいこの

68
00:04:46,260 --> 00:04:47,900
1 つのニューロンだけに焦点を当ててみましょう。

69
00:04:47,900 --> 00:04:53,680
アクティベーションは、前の層のすべてのアクティベーションの特定の加

70
00:04:53,680 --> 00:04:58,380
重合計にバイアスを加えたものとして定義され、そのすべてがシグモイド

71
00:04:58,380 --> 00:05:01,900
潰し関数や ReLU などに組み込まれることに注意してください。

72
00:05:01,900 --> 00:05:07,060
したがって、その活性化を高めるために連携

73
00:05:07,060 --> 00:05:08,060
できる 3 つの異なる方法があります。

74
00:05:08,060 --> 00:05:12,800
バイアスを増やしたり、重みを増やしたり、前のレイヤ

75
00:05:12,800 --> 00:05:15,300
ーからのアクティベーションを変更したりできます。

76
00:05:15,300 --> 00:05:19,720
ウェイトをどのように調整するかに注目して、ウェイトが実際に

77
00:05:19,720 --> 00:05:21,460
どのように異なるレベルの影響を与えるかに注目してください。

78
00:05:21,460 --> 00:05:25,100
前の層の最も明るいニューロンとの接続は、それらの重みにより大

79
00:05:25,100 --> 00:05:31,420
きな活性化値が乗算されるため、最も大きな効果をもたらします。

80
00:05:31,420 --> 00:05:35,820
したがって、これらの重みの 1 つを増加すると、少なくともこの 1

81
00:05:35,820 --> 00:05:40,900
つのトレーニング例に関する限り、実際には、ディマー ニューロンとの接

82
00:05:40,900 --> 00:05:44,020
続の重みを増加するよりも最終的なコスト関数に強い影響を及ぼします。

83
00:05:44,020 --> 00:05:48,700
勾配降下法について話すとき、私たちは単に各コンポーネントを

84
00:05:48,700 --> 00:05:53,020
上に動かすか下に動かすかだけを気にするのではなく、どれが最

85
00:05:53,020 --> 00:05:54,020
も費用対効果が高いかを気にしていることに注意してください。

86
00:05:54,020 --> 00:06:00,260
ちなみに、これは、ニューロンの生物学的ネットワークがどのように学習するかについて

87
00:06:00,260 --> 00:06:04,900
の神経科学の理論、ヘビアン理論を少なくともいくらか思い出させます。ヘビアン理論は

88
00:06:04,900 --> 00:06:06,940
、しばしば「発火するニューロンは一緒に配線する」というフレーズに要約されます。

89
00:06:06,940 --> 00:06:12,460
ここで、重みの最大の増加、つまり接続の最大の強

90
00:06:12,460 --> 00:06:16,860
化は、最もアクティブなニューロンと、よりアク

91
00:06:16,860 --> 00:06:18,100
ティブになりたいニューロンの間で発生します。

92
00:06:18,100 --> 00:06:22,520
ある意味、「2」を見ているときに発火しているニューロンは、それについ

93
00:06:22,520 --> 00:06:25,440
て考えているときに発火しているニューロンとより強く結びついています。

94
00:06:25,440 --> 00:06:29,240
誤解のないように言っておきますが、私はニューロンの人工ネットワークが生物学的な脳のように振る

95
00:06:29,240 --> 00:06:34,020
舞うかどうかについて何らかの形で意見を言う立場にありません。そして、この「ファイア・トゥゲ

96
00:06:34,020 --> 00:06:39,440
ザー・ワイヤー・トゥゲザー」というアイデアには意味のあるアスタリスクがいくつか付いています

97
00:06:39,440 --> 00:06:41,760
が、非常に緩いものとして捉えられています。たとえて言えば、注目するのは興味深いと思います。

98
00:06:41,760 --> 00:06:46,760
とにかく、このニューロンの活性化を高める 3 番目の

99
00:06:46,760 --> 00:06:49,360
方法は、前の層のすべての活性化を変更することです。

100
00:06:49,360 --> 00:06:55,080
つまり、正の重みを持つ数字 2 のニューロンに接続されている

101
00:06:55,080 --> 00:06:59,480
すべてが明るくなり、負の重みに接続されているすべてが暗くなる

102
00:06:59,480 --> 00:07:02,680
と、その数字 2 のニューロンはよりアクティブになります。

103
00:07:02,680 --> 00:07:06,200
ウェイトの変更と同様に、対応するウェイトのサイズに比例する

104
00:07:06,200 --> 00:07:10,840
変更を求めることで、最も大きな利益を得ることができます。

105
00:07:10,840 --> 00:07:16,520
もちろん、これらのアクティベーションに直接影響を与えるこ

106
00:07:16,520 --> 00:07:18,320
とはできません。制御できるのは重みとバイアスだけです。

107
00:07:18,320 --> 00:07:22,960
ただし、最後のレイヤーと同様に、必要な変更

108
00:07:22,960 --> 00:07:23,960
が何であるかをメモしておくと役立ちます。

109
00:07:23,960 --> 00:07:29,040
ただし、ここで 1 段階ズームアウトすると、これは桁 2 の出

110
00:07:29,040 --> 00:07:30,040
力ニューロンが望んでいることだけであることに注意してください。

111
00:07:30,040 --> 00:07:34,960
最後の層にある他のすべてのニューロンもあまりアクティブにならないようにした

112
00:07:34,960 --> 00:07:38,460
いこと、そしてそれらの他の出力ニューロンはそれぞれ、最後から 2 番目の層

113
00:07:38,460 --> 00:07:43,200
に何が起こるべきかについて独自の考えを持っていることを思い出してください。

114
00:07:43,200 --> 00:07:49,220
したがって、この桁 2 ニューロンの欲求は、この最後から 2

115
00:07:49,220 --> 00:07:54,800
番目の層に何が起こるべきかについての他のすべての出力ニュ

116
00:07:54,800 --> 00:08:00,240
ーロンの欲求と加算されます。これも、対応する重みに比例し、各

117
00:08:00,240 --> 00:08:01,740
ニューロンがどれだけ必要とするかに比例します。変えること。

118
00:08:01,740 --> 00:08:05,940
ここで、逆方向に伝播するというアイデアが登場します。

119
00:08:05,940 --> 00:08:11,080
これらの必要な効果をすべて加算すると、基本的に、最後から

120
00:08:11,080 --> 00:08:14,300
2 番目のレイヤーに適用するナッジのリストが得られます。

121
00:08:14,300 --> 00:08:18,740
これらを取得したら、それらの値を決定する関連する重みとバイア

122
00:08:18,740 --> 00:08:23,400
スに同じプロセスを再帰的に適用し、先ほど説明したのと同じプロ

123
00:08:23,400 --> 00:08:29,180
セスを繰り返し、ネットワークを逆方向に進むことができます。

124
00:08:29,180 --> 00:08:33,960
さらにズームアウトすると、これはすべて、単一のトレーニング サンプルがこれらの

125
00:08:33,960 --> 00:08:37,520
重みとバイアスのそれぞれを微調整する方法にすぎないことを思い出してください。

126
00:08:37,520 --> 00:08:41,400
もし私たちがその 2 の要求にのみ耳を傾けていたとしたら、ネットワークは最終的には

127
00:08:41,400 --> 00:08:44,140
すべての画像を 2 として分類することだけにインセンティブを与えることになります。

128
00:08:44,140 --> 00:08:49,500
したがって、他のすべてのトレーニング サンプルに対して同じバックプ

129
00:08:49,500 --> 00:08:54,700
ロップ ルーチンを実行し、それぞれのサンプルで重みとバイアスをど

130
00:08:54,700 --> 00:09:02,300
のように変更したいかを記録し、それらの望ましい変更を平均します。

131
00:09:02,300 --> 00:09:08,260
ここでの各重みとバイアスの平均ナッジのコレクションは

132
00:09:08,260 --> 00:09:12,340
、大まかに言えば、最後のビデオで参照されたコスト関数

133
00:09:12,340 --> 00:09:14,360
の負の勾配、または少なくともそれに比例するものです。

134
00:09:14,360 --> 00:09:18,980
私がこれらのナッジについて定量的に正確に理解できていないから大まかに言ってるだけです

135
00:09:18,980 --> 00:09:23,480
が、私が今言及したすべての変更、なぜ一部の変更が他の変更よりも比例して大きくなるの

136
00:09:23,480 --> 00:09:28,740
か、そしてそれらすべてをどのように加算する必要があるのかを理解できたなら、あなたはそ

137
00:09:28,740 --> 00:09:34,100
のメカニズムを理解しているでしょう。バックプロパゲーションが実際に行っていること。

138
00:09:34,100 --> 00:09:38,540
ところで、実際には、コンピューターがすべての学習例の影響を勾

139
00:09:38,540 --> 00:09:43,120
配降下ステップごとに合計するには非常に長い時間がかかります。

140
00:09:43,120 --> 00:09:45,540
そこで、代わりに一般的に行われることを以下に示します。

141
00:09:45,540 --> 00:09:50,460
トレーニング データをランダムにシャッフルし、それを多数のミニバッチに分割します

142
00:09:50,460 --> 00:09:53,380
。たとえば、各ミニバッチに 100 個のトレーニング サンプルがあるとします。

143
00:09:53,380 --> 00:09:56,980
次に、ミニバッチに従ってステップを計算します。

144
00:09:56,980 --> 00:10:00,840
これはコスト関数の実際の勾配ではなく、この小さなサブセットでは

145
00:10:00,840 --> 00:10:06,260
なくすべてのトレーニング データに依存するため、最も効率的な下

146
00:10:06,260 --> 00:10:10,900
り坂のステップではありませんが、各ミニバッチからかなり良好な近

147
00:10:10,900 --> 00:10:12,900
似が得られます。さらに重要なのは、計算速度が大幅に向上します。

148
00:10:12,900 --> 00:10:16,900
関連するコスト曲面の下でネットワークの軌跡をプロットすると、それは、慎

149
00:10:16,900 --> 00:10:22,020
重に計算して各ステップの下り坂の方向を正確に決定するというよりは、目

150
00:10:22,020 --> 00:10:26,880
的もなく坂を下りながらも素早いステップを踏む酔っぱらいの男に似たもの

151
00:10:26,880 --> 00:10:31,620
になるでしょう。その方向に非常にゆっくりと慎重に一歩を踏み出す前に。

152
00:10:31,620 --> 00:10:35,200
この手法は確率的勾配降下法と呼ばれます。

153
00:10:35,200 --> 00:10:40,400
ここではたくさんのことが起こっているので、自分用にまとめてみましょう。

154
00:10:40,400 --> 00:10:45,480
バックプロパゲーションは、単一のトレーニング サンプルが重みとバイアスを

155
00:10:45,480 --> 00:10:50,040
どのように微調整するかを決定するためのアルゴリズムです。重みとバイアス

156
00:10:50,040 --> 00:10:54,780
を上昇させるか下降させるかという観点だけでなく、それらの変化に対する相対

157
00:10:54,780 --> 00:10:56,240
的な割合が最も急速な減少を引き起こすかという観点から判断します。料金。

158
00:10:56,240 --> 00:11:00,720
本当の勾配降下ステップでは、これを何万、何千ものトレーニ

159
00:11:00,720 --> 00:11:05,920
ング例すべてに対して実行し、得られる望ましい変化を平均

160
00:11:05,920 --> 00:11:11,680
する必要がありますが、計算が遅いため、代わりにデータを

161
00:11:11,680 --> 00:11:14,000
ランダムにミニバッチに分割し、各ステップをミニバッチ。

162
00:11:14,000 --> 00:11:18,600
すべてのミニバッチを繰り返し実行してこれらの調整を行うと、コスト関

163
00:11:18,600 --> 00:11:23,420
数の極小値に向かって収束します。つまり、ネットワークがトレーニング

164
00:11:23,420 --> 00:11:27,540
サンプルで非常に優れたパフォーマンスを発揮することになります。

165
00:11:27,540 --> 00:11:32,600
以上のことをすべて踏まえた上で、backprop の実装に含まれるコードのすべ

166
00:11:32,600 --> 00:11:37,680
ての行は、少なくとも非公式の用語では、実際にこれまでに見たものと一致します。

167
00:11:37,680 --> 00:11:41,900
しかし、数学が何をするのかを知ることは戦いの半分に過ぎず、単に

168
00:11:41,900 --> 00:11:44,780
それを表現するだけですべてが混乱して混乱することもあります。

169
00:11:44,780 --> 00:11:49,360
したがって、さらに詳しく知りたい人のために、次のビデオ

170
00:11:49,360 --> 00:11:53,400
では、ここで紹介したのと同じアイデアを説明しますが、

171
00:11:53,400 --> 00:11:57,460
基礎となる微積分の観点から説明します。他のリソース。

172
00:11:57,460 --> 00:12:01,220
その前に、強調しておきたいことの 1 つは、このアルゴリズムが機能するに

173
00:12:01,220 --> 00:12:05,840
は、これはニューラル ネットワークだけでなくあらゆる種類の機械学習に当

174
00:12:05,840 --> 00:12:06,840
てはまりますが、大量のトレーニング データが必要であるということです。

175
00:12:06,840 --> 00:12:10,740
私たちの場合、手書きの数字がこれほど優れた例である理由の 1 つは、人間によってラベ

176
00:12:10,740 --> 00:12:15,380
ル付けされた非常に多くの例が含まれる MNIST データベースが存在することです。

177
00:12:15,380 --> 00:12:19,000
したがって、機械学習に取り組んでいる人ならよく知っているであろう共通の課

178
00:12:19,040 --> 00:12:22,880
題は、数万枚の画像にラベルを付けることであろうと、扱う他のデータ型であ

179
00:12:22,880 --> 00:12:27,400
ろうと、実際に必要なラベル付きトレーニング データを取得することです。

