1
00:00:00,000 --> 00:00:04,820
ここでは、ニューラル ネットワークの学習方法の背後にある中心

2
00:00:04,820 --> 00:00:09,640
的なアルゴリズムであるバックプロパゲーションに取り組みます。

3
00:00:09,640 --> 00:00:13,520
ここまでの概要を簡単にまとめた後、最初に、数式を参照せずに、

4
00:00:13,520 --> 00:00:17,400
アルゴリズムが実際に何を行っているかを直感的に説明します。

5
00:00:17,400 --> 00:00:20,838
次に、数学について詳しく知りたい人のために、次のビデオで

6
00:00:20,838 --> 00:00:24,040
は、これらすべての基礎となる微積分について説明します。

7
00:00:24,040 --> 00:00:25,769
最後の 2 つのビデオをご覧になった場合、または適切な背

8
00:00:25,769 --> 00:00:27,621
景を理解してすぐに参加した場合は、ニューラル ネットワーク

9
00:00:27,621 --> 00:00:29,350
とは何か、またニューラル ネットワークがどのように情報を

10
00:00:29,350 --> 00:00:31,080
フィードフォワードするかについては理解しているでしょう。

11
00:00:31,080 --> 00:00:32,890
ここでは、ピクセル値が 784

12
00:00:32,890 --> 00:00:35,831
個のニューロンを含むネットワークの最初の層に入力さ

13
00:00:35,831 --> 00:00:38,885
れる手書きの数字を認識する古典的な例を行っています。ま

14
00:00:38,885 --> 00:00:41,940
た、それぞれ 16 個のニ ューロンしか持たない 2

15
00:00:41,940 --> 00:00:45,107
つの隠れ層と出力を含むネットワークを示しています。10

16
00:00:45,107 --> 00:00:48,162
個のニューロンの層。ネットワークがどの桁を答えとして選

17
00:00:48,162 --> 00:00:49,520
択しているかを示します。

18
00:00:49,520 --> 00:00:52,556
また、前回のビデオで説明したように、勾配降下

19
00:00:52,556 --> 00:00:55,592
法と、学習とは、 どの重みとバイアスが特定の

20
00:00:55,592 --> 00:00:58,077
コスト関数を最小化するかを見つける

21
00:00:58,077 --> 00:01:02,080
ことを意味することを理解していただくことも期待しています。

22
00:01:02,080 --> 00:01:05,968
簡単に思い出していただきたいのですが、1 つのトレーニング

23
00:01:05,968 --> 00:01:09,338
サンプル のコストとして、ネットワークが提供する出力

24
00:01:09,338 --> 00:01:12,708
と、ネットワークに提供して もらいたい出力を取得し、

25
00:01:12,708 --> 00:01:15,560
各コンポーネントの差の 2 乗を合計します。

26
00:01:15,560 --> 00:01:19,370
これを何万ものトレーニング例すべてに対して実行し、結

27
00:01:19,370 --> 00:01:23,040
果を平均すると、ネットワークの総コストが得られます。

28
00:01:23,040 --> 00:01:26,957
最後のビデオで説明したように、それだけでは考えるのが

29
00:01:26,957 --> 00:01:30,875
十分ではないか のように、私たちが探しているのはこの

30
00:01:30,875 --> 00:01:34,792
コスト関数の負の勾配です。こ れは、すべての重みとバ

31
00:01:34,792 --> 00:01:38,258
イアスをどのように変更する必要があるかを示し

32
00:01:38,258 --> 00:01:42,175
ています。これらの接続により、最も効率的にコストが削

33
00:01:42,175 --> 00:01:43,080
減されます。

34
00:01:43,080 --> 00:01:46,465
このビデオのトピックであるバックプロパゲーションは、

35
00:01:46,465 --> 00:01:49,600
非常に複雑な勾配を計算するためのアルゴリズムです。

36
00:01:49,600 --> 00:01:52,538
最後のビデオで、今すぐにしっかりと頭の中に留めておいて

37
00:01:52,538 --> 00:01:55,803
ほしい 1 つ のアイデアは、勾配ベクトルを 13,000

38
00:01:55,803 --> 00:01:58,742
次元の方向として考えるこ とは、簡単に言えば、私たちの

39
00:01:58,742 --> 00:02:01,028
想像の範囲を超えているため、別のアイデア

40
00:02:01,028 --> 00:02:03,966
があるということです。あなたがそれについて考えることが

41
00:02:03,966 --> 00:02:04,620
できる方法。

42
00:02:04,620 --> 00:02:08,364
ここでの各成分の大きさは、コスト関数が各重みとバイ

43
00:02:08,364 --> 00:02:11,820
アスに対してどの程度敏感であるかを示しています。

44
00:02:11,820 --> 00:02:14,794
たとえば、これから説明するプロセスを実行し、負の

45
00:02:14,794 --> 00:02:17,768
勾配を計算したところ、こ のエッジの重みに関連付

46
00:02:17,768 --> 00:02:20,867
けられたコンポーネントが 3 であることが判明し

47
00:02:20,867 --> 00:02:23,841
たとします。2 ですが、このエッジに関連付けられ

48
00:02:23,841 --> 00:02:26,940
たコンポーネントは 0 として出力されます。1.

49
00:02:26,940 --> 00:02:31,719
これをどう解釈するかというと、関数のコストは最初の重みの変

50
00:02:31,719 --> 00:02:36,339
化に対して 32 倍敏感であるため、その値を少し変更する

51
00:02:36,339 --> 00:02:41,119
と、コストに何らかの変化が生じ、その変化は2 番目の重りに

52
00:02:41,119 --> 00:02:45,580
対する同じ揺れが与える値よりも 32 倍大きくなります。

53
00:02:45,580 --> 00:02:48,888
個人的に、私が最初にバックプロパゲーション

54
00:02:48,888 --> 00:02:52,196
について学んだとき、最 も混乱したのは単に

55
00:02:52,196 --> 00:02:55,820
その表記とインデックスの追跡だったと思います。

56
00:02:55,820 --> 00:02:58,701
しかし、このアルゴリズムの各部分が実際に何を

57
00:02:58,701 --> 00:03:01,583
しているのかを紐 解いてみると、それがもたら

58
00:03:01,583 --> 00:03:03,941
す個々の効果は実際には非常に直感的

59
00:03:03,941 --> 00:03:07,740
であり、ただ多くの小さな調整が積み重ねられているだけです。

60
00:03:07,740 --> 00:03:10,849
したがって、ここでは表記を完全に無視して

61
00:03:10,849 --> 00:03:13,959
物事を開始し、各トレー ニング例が重みと

62
00:03:13,959 --> 00:03:17,380
バイアスに与える影響を段階的に見ていきます。

63
00:03:17,380 --> 00:03:20,017
コスト関数には、数万のトレーニング

64
00:03:20,017 --> 00:03:23,534
サンプル全体にわたるサンプル あたりの特定のコス

65
00:03:23,534 --> 00:03:27,051
トの平均が含まれるため、単一の勾配降下ステップ

66
00:03:27,051 --> 00:03:30,567
の重みとバイアスを調整する方法も、すべてのサンプ

67
00:03:30,567 --> 00:03:31,740
ルに依存します。

68
00:03:31,740 --> 00:03:34,379
というか、原理的にはそうすべきですが、計算効率を高め

69
00:03:34,379 --> 00:03:37,018
るために、各ステップですべて のサンプルをヒットする

70
00:03:37,018 --> 00:03:39,860
必要がないように、後でちょっとしたトリックを実行します。

71
00:03:39,860 --> 00:03:43,484
他のケースでは、現時点では、この 2 の画

72
00:03:43,484 --> 00:03:46,780
像という 1 つの例に注目するだけです。

73
00:03:46,780 --> 00:03:49,207
この 1 つのトレーニング例は、重みとバイアス

74
00:03:49,207 --> 00:03:51,740
の調整方法にどのような影響を与えるでしょうか?

75
00:03:51,740 --> 00:03:54,416
ネットワークがまだ十分にトレーニングされていない

76
00:03:54,416 --> 00:03:57,092
段階にあるとします。そのため、出力内のアクティ

77
00:03:57,092 --> 00:03:59,546
ベーションはかなりランダムに、おそらく 0

78
00:03:59,546 --> 00:04:02,780
のようなものになるとします。5、0。8、0。2 、延々と。

79
00:04:02,780 --> 00:04:06,382
これらのアクティベーションを直接変更することはできず、影

80
00:04:06,382 --> 00:04:09,985
響を受けるのは重みとバイアスのみですが、その出力層に対し

81
00:04:09,985 --> 00:04:13,340
てどの調整を行う必要があるかを追跡するのに役立ちます。

82
00:04:13,340 --> 00:04:17,590
そして、画像を 2 として分類したいので、3 番目の値を少

83
00:04:17,590 --> 00:04:21,700
しずつ上げて、他のすべての値を少しずつ下げるようにします。

84
00:04:21,700 --> 00:04:26,117
さらに、これらのナッジのサイズは、各現在の値がその目標

85
00:04:26,117 --> 00:04:30,220
値からどれだけ離れているかに比例する必要があります。

86
00:04:30,220 --> 00:04:34,222
たとえば、2 番のニューロンの活性化の増加は、

87
00:04:34,222 --> 00:04:38,224
ある意味で、すでにあるべき状態にかなり近づいて

88
00:04:38,224 --> 00:04:42,060
いる 8 番のニューロンの減少よりも重要です。

89
00:04:42,060 --> 00:04:45,039
そこでさらにズームインして、活性化を高めたいこの

90
00:04:45,039 --> 00:04:47,900
1 つのニューロンだけに焦点を当ててみましょう。

91
00:04:47,900 --> 00:04:51,400
アクティベーションは、前の層のすべてのアクティベー

92
00:04:51,400 --> 00:04:54,900
ションの特定の加 重合計にバイアスを加えたものとし

93
00:04:54,900 --> 00:04:58,260
て定義され、そのすべてがシグモイド 潰し関数や

94
00:04:58,260 --> 00:05:01,900
ReLU などに組み込まれることに注意してください。

95
00:05:01,900 --> 00:05:05,134
したがって、その活性化を高めるために連携

96
00:05:05,134 --> 00:05:08,060
できる 3 つの異なる方法があります。

97
00:05:08,060 --> 00:05:11,824
バイアスを増やしたり、重みを増やしたり、前のレイヤ

98
00:05:11,824 --> 00:05:15,300
ーからのアクティベーションを変更したりできます。

99
00:05:15,300 --> 00:05:18,432
ウェイトをどのように調整するかに注目して、ウェイトが実際に

100
00:05:18,432 --> 00:05:21,460
どのように異なるレベルの影響を与えるかに注目してください。

101
00:05:21,460 --> 00:05:24,725
前の層の最も明るいニューロンとの接続は、

102
00:05:24,725 --> 00:05:27,991
それらの重みにより大 きな活性化値が乗算

103
00:05:27,991 --> 00:05:31,420
されるため、最も大きな効果をもたらします。

104
00:05:31,420 --> 00:05:33,396
したがって、これらの重みの 1

105
00:05:33,396 --> 00:05:35,620
つを増加すると、少なくともこの 1

106
00:05:35,620 --> 00:05:38,831
つのトレーニング例に関する限り、実際には、ディマー

107
00:05:38,831 --> 00:05:41,920
ニューロンとの接 続の重みを増加するよりも最終的な

108
00:05:41,920 --> 00:05:44,020
コスト関数に強い影響を及ぼします。

109
00:05:44,020 --> 00:05:47,390
勾配降下法について話すとき、私たちは単に各コンポーネントを

110
00:05:47,390 --> 00:05:50,761
上に動かすか下に動かすかだけを気にするのではなく、どれが最

111
00:05:50,761 --> 00:05:54,020
も費用対効果が高いかを気にしていることに注意してください。

112
00:05:54,020 --> 00:05:56,582
ちなみに、これは、ニューロンの生物学的ネットワー

113
00:05:56,582 --> 00:05:58,397
クがどのように学習するかについて

114
00:05:58,397 --> 00:06:00,960
の神経科学の理論、ヘビアン理論を少なくともいくら

115
00:06:00,960 --> 00:06:02,775
か思い出させます。ヘビアン理論は

116
00:06:02,775 --> 00:06:05,338
、しばしば「発火するニューロンは一緒に配線する」

117
00:06:05,338 --> 00:06:06,940
というフレーズに要約されます。

118
00:06:06,940 --> 00:06:10,821
ここで、重みの最大の増加、つまり接続の最大の強

119
00:06:10,821 --> 00:06:14,541
化は、最もアクティブなニューロンと、よりアク

120
00:06:14,541 --> 00:06:18,100
ティブになりたいニューロンの間で発生します。

121
00:06:18,100 --> 00:06:20,546
ある意味、「2」を見ているときに発火しているニ

122
00:06:20,546 --> 00:06:22,993
ューロンは、それについ て考えているときに発火

123
00:06:22,993 --> 00:06:25,440
しているニューロンとより強く結びついています。

124
00:06:25,440 --> 00:06:27,746
誤解のないように言っておきますが、私はニューロンの人

125
00:06:27,746 --> 00:06:29,608
工ネットワークが生物学的な脳のように振る

126
00:06:29,608 --> 00:06:31,914
舞うかどうかについて何らかの形で意見を言う立場にあり

127
00:06:31,914 --> 00:06:33,688
ません。そして、この「ファイア・トゥゲ

128
00:06:33,688 --> 00:06:35,994
ザー・ワイヤー・トゥゲザー」というアイデアには意味の

129
00:06:35,994 --> 00:06:37,768
あるアスタリスクがいくつか付いています

130
00:06:37,768 --> 00:06:40,074
が、非常に緩いものとして捉えられています。たとえて言

131
00:06:40,074 --> 00:06:41,760
えば、注目するのは興味深いと思います。

132
00:06:41,760 --> 00:06:45,706
とにかく、このニューロンの活性化を高める 3 番目の

133
00:06:45,706 --> 00:06:49,360
方法は、前の層のすべての活性化を変更することです。

134
00:06:49,360 --> 00:06:51,701
つまり、正の重みを持つ数字 2

135
00:06:51,701 --> 00:06:54,922
のニューロンに接続されている すべてが明るく

136
00:06:54,922 --> 00:06:58,435
なり、負の重みに接続されているすべてが暗くなる

137
00:06:58,435 --> 00:07:02,680
と、その数字 2 のニューロンはよりアクティブになります。

138
00:07:02,680 --> 00:07:06,900
ウェイトの変更と同様に、対応するウェイトのサイズに比例する

139
00:07:06,900 --> 00:07:10,840
変更を求めることで、最も大きな利益を得ることができます。

140
00:07:10,840 --> 00:07:14,713
もちろん、これらのアクティベーションに直接影響を与えるこ

141
00:07:14,713 --> 00:07:18,320
とはできません。制御できるのは重みとバイアスだけです。

142
00:07:18,320 --> 00:07:21,274
ただし、最後のレイヤーと同様に、必要な変更

143
00:07:21,274 --> 00:07:23,960
が何であるかをメモしておくと役立ちます。

144
00:07:23,960 --> 00:07:26,565
ただし、ここで 1 段階ズームアウトすると、これは桁

145
00:07:26,565 --> 00:07:28,592
2 の出 力ニューロンが望んでいることだけ

146
00:07:28,592 --> 00:07:30,040
であることに注意してください。

147
00:07:30,040 --> 00:07:33,300
最後の層にある他のすべてのニューロンもあまりアクティブに

148
00:07:33,300 --> 00:07:36,561
ならないようにした いこと、そしてそれらの他の出力ニュー

149
00:07:36,561 --> 00:07:38,890
ロンはそれぞれ、最後から 2 番目の層

150
00:07:38,890 --> 00:07:42,151
に何が起こるべきかについて独自の考えを持っていることを思

151
00:07:42,151 --> 00:07:43,200
い出してください。

152
00:07:43,200 --> 00:07:47,680
したがって、この桁 2 ニューロンの欲求は、この最後から

153
00:07:47,680 --> 00:07:52,315
2 番目の層に何が起こるべきかについての他のすべての出力ニュ

154
00:07:52,315 --> 00:07:56,950
ーロンの欲求と加算されます。これも、対応する重みに比例し、

155
00:07:56,950 --> 00:08:01,585
各 ニューロンがどれだけ必要とするかに比例します。変えること

156
00:08:01,585 --> 00:08:01,740
。

157
00:08:01,740 --> 00:08:05,940
ここで、逆方向に伝播するというアイデアが登場します。

158
00:08:05,940 --> 00:08:10,193
これらの必要な効果をすべて加算すると、基本的に、最後から

159
00:08:10,193 --> 00:08:14,300
2 番目のレイヤーに適用するナッジのリストが得られます。

160
00:08:14,300 --> 00:08:17,897
これらを取得したら、それらの値を決定する関連

161
00:08:17,897 --> 00:08:21,494
する重みとバイア スに同じプロセスを再帰的に

162
00:08:21,494 --> 00:08:24,438
適用し、先ほど説明したのと同じプロ

163
00:08:24,438 --> 00:08:29,180
セスを繰り返し、ネットワークを逆方向に進むことができます。

164
00:08:29,180 --> 00:08:32,387
さらにズームアウトすると、これはすべて、単一のトレーニング

165
00:08:32,387 --> 00:08:35,167
サンプルがこれらの 重みとバイアスのそれぞれを微調整

166
00:08:35,167 --> 00:08:37,520
する方法にすぎないことを思い出してください。

167
00:08:37,520 --> 00:08:39,673
もし私たちがその 2 の要求にのみ耳を傾けていたとした

168
00:08:39,673 --> 00:08:41,667
ら、ネットワークは最終的には すべての画像を 2

169
00:08:41,667 --> 00:08:43,820
として分類することだけにインセンティブを与えることにな

170
00:08:43,820 --> 00:08:44,140
ります。

171
00:08:44,140 --> 00:08:47,625
したがって、他のすべてのトレーニング

172
00:08:47,625 --> 00:08:51,110
サンプルに対して同じバックプ ロップ

173
00:08:51,110 --> 00:08:56,430
ルーチンを実行し、それぞれのサンプルで重みとバイアスをど

174
00:08:56,430 --> 00:09:00,832
のように変更したいかを記録し、それらの望ましい変

175
00:09:00,832 --> 00:09:02,300
更を平均します。

176
00:09:02,300 --> 00:09:06,370
ここでの各重みとバイアスの平均ナッジのコレクションは

177
00:09:06,370 --> 00:09:10,440
、大まかに言えば、最後のビデオで参照されたコスト関数

178
00:09:10,440 --> 00:09:14,360
の負の勾配、または少なくともそれに比例するものです。

179
00:09:14,360 --> 00:09:17,630
私がこれらのナッジについて定量的に正確に理解できていない

180
00:09:17,630 --> 00:09:20,901
から大まかに言ってるだけです が、私が今言及したすべての

181
00:09:20,901 --> 00:09:24,288
変更、なぜ一部の変更が他の変更よりも比例して大きくなるの

182
00:09:24,288 --> 00:09:27,558
か、そしてそれらすべてをどのように加算する必要があるのか

183
00:09:27,558 --> 00:09:30,829
を理解できたなら、あなたはそ のメカニズムを理解している

184
00:09:30,829 --> 00:09:34,100
でしょう。バックプロパゲーションが実際に行っていること。

185
00:09:34,100 --> 00:09:37,057
ところで、実際には、コンピューターがすべ

186
00:09:37,057 --> 00:09:40,014
ての学習例の影響を勾 配降下ステップごと

187
00:09:40,014 --> 00:09:43,120
に合計するには非常に長い時間がかかります。

188
00:09:43,120 --> 00:09:45,540
そこで、代わりに一般的に行われることを以下に示します。

189
00:09:45,540 --> 00:09:48,088
トレーニング データをランダムにシャッフルし、それを

190
00:09:48,088 --> 00:09:50,930
多数のミニバッチに分割します 。たとえば、各ミニバッチに

191
00:09:50,930 --> 00:09:53,380
100 個のトレーニング サンプルがあるとします。

192
00:09:53,380 --> 00:09:56,980
次に、ミニバッチに従ってステップを計算します。

193
00:09:56,980 --> 00:10:00,113
これはコスト関数の実際の勾配ではなく、この小さなサ

194
00:10:00,113 --> 00:10:02,620
ブセットでは なくすべてのトレーニング

195
00:10:02,620 --> 00:10:05,002
データに依存するため、最も効率的な下

196
00:10:05,002 --> 00:10:08,136
り坂のステップではありませんが、各ミニバッチからか

197
00:10:08,136 --> 00:10:11,270
なり良好な近 似が得られます。さらに重要なのは、計

198
00:10:11,270 --> 00:10:12,900
算速度が大幅に向上します。

199
00:10:12,900 --> 00:10:16,644
関連するコスト曲面の下でネットワークの軌跡をプロットする

200
00:10:16,644 --> 00:10:20,388
と、それは、慎 重に計算して各ステップの下り坂の方向を正

201
00:10:20,388 --> 00:10:24,132
確に決定するというよりは、目 的もなく坂を下りながらも素

202
00:10:24,132 --> 00:10:27,073
早いステップを踏む酔っぱらいの男に似たもの

203
00:10:27,073 --> 00:10:30,817
になるでしょう。その方向に非常にゆっくりと慎重に一歩を踏

204
00:10:30,817 --> 00:10:31,620
み出す前に。

205
00:10:31,620 --> 00:10:35,200
この手法は確率的勾配降下法と呼ばれます。

206
00:10:35,200 --> 00:10:37,725
ここではたくさんのことが起こってい

207
00:10:37,725 --> 00:10:40,400
るので、自分用にまとめてみましょう。

208
00:10:40,400 --> 00:10:42,912
バックプロパゲーションは、単一のトレーニング

209
00:10:42,912 --> 00:10:46,080
サンプルが重みとバイアスを どのように微調整するかを決定す

210
00:10:46,080 --> 00:10:48,374
るためのアルゴリズムです。重みとバイアス

211
00:10:48,374 --> 00:10:51,542
を上昇させるか下降させるかという観点だけでなく、それらの変

212
00:10:51,542 --> 00:10:54,710
化に対する相対 的な割合が最も急速な減少を引き起こすかとい

213
00:10:54,710 --> 00:10:56,240
う観点から判断します。料金。

214
00:10:56,240 --> 00:11:00,838
本当の勾配降下ステップでは、これを何万、何千ものトレーニ

215
00:11:00,838 --> 00:11:05,278
ング例すべてに対して実行し、得られる望ましい変化を平均

216
00:11:05,278 --> 00:11:09,718
する必要がありますが、計算が遅いため、代わりにデータを

217
00:11:09,718 --> 00:11:14,000
ランダムにミニバッチに分割し、各ステップをミニバッチ。

218
00:11:14,000 --> 00:11:17,282
すべてのミニバッチを繰り返し実行してこれらの調整

219
00:11:17,282 --> 00:11:20,564
を行うと、コスト関 数の極小値に向かって収束しま

220
00:11:20,564 --> 00:11:23,300
す。つまり、ネットワークがトレーニング

221
00:11:23,300 --> 00:11:26,582
サンプルで非常に優れたパフォーマンスを発揮するこ

222
00:11:26,582 --> 00:11:27,540
とになります。

223
00:11:27,540 --> 00:11:30,790
以上のことをすべて踏まえた上で、backprop

224
00:11:30,790 --> 00:11:34,170
の実装に含まれるコードのすべ ての行は、少なくとも非

225
00:11:34,170 --> 00:11:37,680
公式の用語では、実際にこれまでに見たものと一致します。

226
00:11:37,680 --> 00:11:39,970
しかし、数学が何をするのかを知ることは戦

227
00:11:39,970 --> 00:11:42,260
いの半分に過ぎず、単に それを表現するだ

228
00:11:42,260 --> 00:11:44,780
けですべてが混乱して混乱することもあります。

229
00:11:44,780 --> 00:11:49,163
したがって、さらに詳しく知りたい人のために、次のビデオ

230
00:11:49,163 --> 00:11:53,389
では、ここで紹介したのと同じアイデアを説明しますが、

231
00:11:53,389 --> 00:11:57,460
基礎となる微積分の観点から説明します。他のリソース。

232
00:11:57,460 --> 00:11:59,110
その前に、強調しておきたいことの 1

233
00:11:59,110 --> 00:12:01,628
つは、このアルゴリズムが機能するに は、これはニューラル

234
00:12:01,628 --> 00:12:03,800
ネットワークだけでなくあらゆる種類の機械学習に当

235
00:12:03,800 --> 00:12:05,363
てはまりますが、大量のトレーニング

236
00:12:05,363 --> 00:12:06,840
データが必要であるということです。

237
00:12:06,840 --> 00:12:09,788
私たちの場合、手書きの数字がこれほど優れた例である理由の

238
00:12:09,788 --> 00:12:12,635
1 つは、人間によってラベ ル付けされた非常に多くの例が

239
00:12:12,635 --> 00:12:15,380
含まれる MNIST データベースが存在することです。

240
00:12:15,380 --> 00:12:18,385
したがって、機械学習に取り組んでいる人ならよく知ってい

241
00:12:18,385 --> 00:12:21,390
るであろう共通の課 題は、数万枚の画像にラベルを付ける

242
00:12:21,390 --> 00:12:23,504
ことであろうと、扱う他のデータ型であ

243
00:12:23,504 --> 00:12:25,953
ろうと、実際に必要なラベル付きトレーニング

244
00:12:25,953 --> 00:12:27,400
データを取得することです。

