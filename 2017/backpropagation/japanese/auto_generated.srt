1
00:00:04,060 --> 00:00:06,470
ここでは、ニューラル ネットワークの学習方法の背後にある中心

2
00:00:06,470 --> 00:00:08,880
的なアルゴリズムであるバックプロパゲーションに取り組みます。

3
00:00:09,400 --> 00:00:13,200
ここまでの概要を簡単にまとめた後、最初に、数式を参照せずに、

4
00:00:13,200 --> 00:00:17,000
 アルゴリズムが実際に何を行っているかを直感的に説明します。

5
00:00:17,660 --> 00:00:20,435
次に、数学について詳しく知りたい人のために、次のビデオで 

6
00:00:20,435 --> 00:00:23,020
は、これらすべての基礎となる微積分について説明します。

7
00:00:23,820 --> 00:00:25,583
最後の 2 つのビデオをご覧になった場合、または適切な背

8
00:00:25,583 --> 00:00:27,472
景を理解してすぐに参加した場合は、ニューラル ネットワーク 

9
00:00:27,472 --> 00:00:29,236
とは何か、またニューラル ネットワークがどのように情報を

10
00:00:29,236 --> 00:00:31,000
フィードフォワードするかについては理解しているでしょう。

11
00:00:31,680 --> 00:00:33,384
ここでは、ピクセル値が 784 

12
00:00:33,384 --> 00:00:36,153
個のニューロンを含むネットワークの最初の層に入力さ 

13
00:00:36,153 --> 00:00:38,922
れる手書きの数字を認識する古典的な例を行っています。

14
00:00:38,922 --> 00:00:41,691
また、それぞれ 16 個のニ ューロンしか持たない 

15
00:00:41,691 --> 00:00:44,566
2 つの隠れ層と出力を含むネットワークを示しています。

16
00:00:44,566 --> 00:00:47,442
10 個のニューロンの層。ネットワークがどの桁を答えと

17
00:00:47,442 --> 00:00:49,040
して選択しているかを示します。

18
00:00:50,040 --> 00:00:52,752
また、前回のビデオで説明したように、勾配降下

19
00:00:52,752 --> 00:00:55,465
法と、学習とは、 どの重みとバイアスが特定の

20
00:00:55,465 --> 00:00:57,684
コスト関数を最小化するかを見つける 

21
00:00:57,684 --> 00:01:01,260
ことを意味することを理解していただくことも期待しています。

22
00:01:02,040 --> 00:01:05,663
簡単に思い出していただきたいのですが、1 つのトレーニング 

23
00:01:05,663 --> 00:01:08,803
サンプル のコストとして、ネットワークが提供する出力

24
00:01:08,803 --> 00:01:11,943
と、ネットワークに提供して もらいたい出力を取得し、

25
00:01:11,943 --> 00:01:14,600
各コンポーネントの差の 2 乗を合計します。

26
00:01:15,380 --> 00:01:19,272
これを何万ものトレーニング例すべてに対して実行し、結 

27
00:01:19,272 --> 00:01:23,020
果を平均すると、ネットワークの総コストが得られます。

28
00:01:23,020 --> 00:01:26,010
最後のビデオで説明したように、それだけでは考えるのが

29
00:01:26,010 --> 00:01:29,001
十分ではないか のように、私たちが探しているのはこの

30
00:01:29,001 --> 00:01:31,992
コスト関数の負の勾配です。こ れは、すべての重みとバ

31
00:01:31,992 --> 00:01:35,213
イアスをどのように変更する必要があるかを示し ています。

32
00:01:35,213 --> 00:01:38,320
これらの接続により、最も効率的にコストが削減されます。

33
00:01:43,260 --> 00:01:46,541
このビデオのトピックであるバックプロパゲーションは、 

34
00:01:46,541 --> 00:01:49,580
非常に複雑な勾配を計算するためのアルゴリズムです。

35
00:01:49,580 --> 00:01:52,319
最後のビデオで、今すぐにしっかりと頭の中に留めておいて

36
00:01:52,319 --> 00:01:55,362
ほしい 1 つ のアイデアは、勾配ベクトルを 13,000 

37
00:01:55,362 --> 00:01:58,101
次元の方向として考えるこ とは、簡単に言えば、私たちの

38
00:01:58,101 --> 00:02:00,232
想像の範囲を超えているため、別のアイデア 

39
00:02:00,232 --> 00:02:02,971
があるということです。あなたがそれについて考えることが

40
00:02:02,971 --> 00:02:03,580
できる方法。

41
00:02:04,600 --> 00:02:07,896
ここでの各成分の大きさは、コスト関数が各重みとバイ 

42
00:02:07,896 --> 00:02:10,940
アスに対してどの程度敏感であるかを示しています。

43
00:02:11,800 --> 00:02:14,644
たとえば、これから説明するプロセスを実行し、負の

44
00:02:14,644 --> 00:02:17,489
勾配を計算したところ、こ のエッジの重みに関連付

45
00:02:17,489 --> 00:02:20,452
けられたコンポーネントが 3 であることが判明し 

46
00:02:20,452 --> 00:02:23,296
たとします。2 ですが、このエッジに関連付けられ

47
00:02:23,296 --> 00:02:26,260
たコンポーネントは 0 として出力されます。1. 

48
00:02:26,820 --> 00:02:30,984
これをどう解釈するかというと、関数のコストは最初の重みの変 

49
00:02:30,984 --> 00:02:35,009
化に対して 32 倍敏感であるため、その値を少し変更する 

50
00:02:35,009 --> 00:02:39,173
と、コストに何らかの変化が生じ、その変化は2 番目の重りに 

51
00:02:39,173 --> 00:02:43,060
対する同じ揺れが与える値よりも 32 倍大きくなります。

52
00:02:48,420 --> 00:02:50,784
個人的に、私が最初にバックプロパゲーション

53
00:02:50,784 --> 00:02:53,149
について学んだとき、最 も混乱したのは単に

54
00:02:53,149 --> 00:02:55,740
その表記とインデックスの追跡だったと思います。

55
00:02:56,220 --> 00:02:58,739
しかし、このアルゴリズムの各部分が実際に何を

56
00:02:58,739 --> 00:03:01,258
しているのかを紐 解いてみると、それがもたら

57
00:03:01,258 --> 00:03:03,319
す個々の効果は実際には非常に直感的 

58
00:03:03,319 --> 00:03:06,640
であり、ただ多くの小さな調整が積み重ねられているだけです。

59
00:03:07,740 --> 00:03:10,443
したがって、ここでは表記を完全に無視して

60
00:03:10,443 --> 00:03:13,146
物事を開始し、各トレー ニング例が重みと

61
00:03:13,146 --> 00:03:16,120
バイアスに与える影響を段階的に見ていきます。

62
00:03:17,020 --> 00:03:19,595
コスト関数には、数万のトレーニング 

63
00:03:19,595 --> 00:03:23,028
サンプル全体にわたるサンプル あたりの特定のコス

64
00:03:23,028 --> 00:03:26,462
トの平均が含まれるため、単一の勾配降下ステップ 

65
00:03:26,462 --> 00:03:29,895
の重みとバイアスを調整する方法も、すべてのサンプ

66
00:03:29,895 --> 00:03:31,040
ルに依存します。

67
00:03:31,680 --> 00:03:34,124
というか、原理的にはそうすべきですが、計算効率を高め

68
00:03:34,124 --> 00:03:36,567
るために、各ステップですべて のサンプルをヒットする

69
00:03:36,567 --> 00:03:39,200
必要がないように、後でちょっとしたトリックを実行します。

70
00:03:39,200 --> 00:03:42,740
他のケースでは、現時点では、この 2 の画 

71
00:03:42,740 --> 00:03:45,960
像という 1 つの例に注目するだけです。

72
00:03:46,720 --> 00:03:49,049
この 1 つのトレーニング例は、重みとバイアス

73
00:03:49,049 --> 00:03:51,480
の調整方法にどのような影響を与えるでしょうか? 

74
00:03:52,680 --> 00:03:54,939
ネットワークがまだ十分にトレーニングされていない

75
00:03:54,939 --> 00:03:57,198
段階にあるとします。そのため、出力内のアクティ 

76
00:03:57,198 --> 00:03:59,269
ベーションはかなりランダムに、おそらく 0 

77
00:03:59,269 --> 00:04:02,000
のようなものになるとします。5、0。8、0。2 、延々と。

78
00:04:02,520 --> 00:04:05,952
これらのアクティベーションを直接変更することはできず、影 

79
00:04:05,952 --> 00:04:09,384
響を受けるのは重みとバイアスのみですが、その出力層に対し 

80
00:04:09,384 --> 00:04:12,580
てどの調整を行う必要があるかを追跡するのに役立ちます。

81
00:04:13,360 --> 00:04:17,376
そして、画像を 2 として分類したいので、3 番目の値を少 

82
00:04:17,376 --> 00:04:21,260
しずつ上げて、他のすべての値を少しずつ下げるようにします。

83
00:04:22,060 --> 00:04:25,928
さらに、これらのナッジのサイズは、各現在の値がその目標 

84
00:04:25,928 --> 00:04:29,520
値からどれだけ離れているかに比例する必要があります。

85
00:04:30,220 --> 00:04:33,830
たとえば、2 番のニューロンの活性化の増加は、 

86
00:04:33,830 --> 00:04:37,440
ある意味で、すでにあるべき状態にかなり近づいて 

87
00:04:37,440 --> 00:04:40,900
いる 8 番のニューロンの減少よりも重要です。

88
00:04:42,040 --> 00:04:44,713
そこでさらにズームインして、活性化を高めたいこの 

89
00:04:44,713 --> 00:04:47,280
1 つのニューロンだけに焦点を当ててみましょう。

90
00:04:48,180 --> 00:04:51,395
アクティベーションは、前の層のすべてのアクティベー

91
00:04:51,395 --> 00:04:54,610
ションの特定の加 重合計にバイアスを加えたものとし

92
00:04:54,610 --> 00:04:57,696
て定義され、そのすべてがシグモイド 潰し関数や 

93
00:04:57,696 --> 00:05:01,040
ReLU などに組み込まれることに注意してください。

94
00:05:01,640 --> 00:05:04,464
したがって、その活性化を高めるために連携 

95
00:05:04,464 --> 00:05:07,020
できる 3 つの異なる方法があります。

96
00:05:07,440 --> 00:05:10,872
バイアスを増やしたり、重みを増やしたり、前のレイヤ 

97
00:05:10,872 --> 00:05:14,040
ーからのアクティベーションを変更したりできます。

98
00:05:14,940 --> 00:05:17,950
ウェイトをどのように調整するかに注目して、ウェイトが実際に 

99
00:05:17,950 --> 00:05:20,860
どのように異なるレベルの影響を与えるかに注目してください。

100
00:05:21,440 --> 00:05:23,951
前の層の最も明るいニューロンとの接続は、

101
00:05:23,951 --> 00:05:26,462
それらの重みにより大 きな活性化値が乗算

102
00:05:26,462 --> 00:05:29,100
されるため、最も大きな効果をもたらします。

103
00:05:31,460 --> 00:05:33,345
したがって、これらの重みの 1 

104
00:05:33,345 --> 00:05:35,466
つを増加すると、少なくともこの 1 

105
00:05:35,466 --> 00:05:38,530
つのトレーニング例に関する限り、実際には、ディマー 

106
00:05:38,530 --> 00:05:41,476
ニューロンとの接 続の重みを増加するよりも最終的な

107
00:05:41,476 --> 00:05:43,480
コスト関数に強い影響を及ぼします。

108
00:05:44,420 --> 00:05:47,386
勾配降下法について話すとき、私たちは単に各コンポーネントを 

109
00:05:47,386 --> 00:05:50,352
上に動かすか下に動かすかだけを気にするのではなく、どれが最 

110
00:05:50,352 --> 00:05:53,220
も費用対効果が高いかを気にしていることに注意してください。

111
00:05:55,020 --> 00:05:57,289
ちなみに、これは、ニューロンの生物学的ネットワー

112
00:05:57,289 --> 00:05:58,896
クがどのように学習するかについて 

113
00:05:58,896 --> 00:06:01,165
の神経科学の理論、ヘビアン理論を少なくともいくら

114
00:06:01,165 --> 00:06:02,772
か思い出させます。ヘビアン理論は 

115
00:06:02,772 --> 00:06:05,041
、しばしば「発火するニューロンは一緒に配線する」

116
00:06:05,041 --> 00:06:06,460
というフレーズに要約されます。

117
00:06:07,260 --> 00:06:10,745
ここで、重みの最大の増加、つまり接続の最大の強 

118
00:06:10,745 --> 00:06:14,085
化は、最もアクティブなニューロンと、よりアク 

119
00:06:14,085 --> 00:06:17,280
ティブになりたいニューロンの間で発生します。

120
00:06:17,940 --> 00:06:20,120
ある意味、「2」を見ているときに発火しているニ

121
00:06:20,120 --> 00:06:22,300
ューロンは、それについ て考えているときに発火

122
00:06:22,300 --> 00:06:24,480
しているニューロンとより強く結びついています。

123
00:06:25,400 --> 00:06:27,607
誤解のないように言っておきますが、私はニューロンの人

124
00:06:27,607 --> 00:06:29,389
工ネットワークが生物学的な脳のように振る 

125
00:06:29,389 --> 00:06:31,936
舞うかどうかについて何らかの形で意見を言う立場にありません。

126
00:06:31,936 --> 00:06:33,294
そして、この「ファイア・トゥゲ 

127
00:06:33,294 --> 00:06:35,502
ザー・ワイヤー・トゥゲザー」というアイデアには意味の

128
00:06:35,502 --> 00:06:37,199
あるアスタリスクがいくつか付いています 

129
00:06:37,199 --> 00:06:38,982
が、非常に緩いものとして捉えられています。

130
00:06:38,982 --> 00:06:41,020
たとえて言えば、注目するのは興味深いと思います。

131
00:06:41,940 --> 00:06:45,626
とにかく、このニューロンの活性化を高める 3 番目の 

132
00:06:45,626 --> 00:06:49,040
方法は、前の層のすべての活性化を変更することです。

133
00:06:49,040 --> 00:06:51,086
つまり、正の重みを持つ数字 2 

134
00:06:51,086 --> 00:06:53,900
のニューロンに接続されている すべてが明るく

135
00:06:53,900 --> 00:06:56,970
なり、負の重みに接続されているすべてが暗くなる 

136
00:06:56,970 --> 00:07:00,680
と、その数字 2 のニューロンはよりアクティブになります。

137
00:07:02,540 --> 00:07:06,543
ウェイトの変更と同様に、対応するウェイトのサイズに比例する 

138
00:07:06,543 --> 00:07:10,280
変更を求めることで、最も大きな利益を得ることができます。

139
00:07:12,140 --> 00:07:14,905
もちろん、これらのアクティベーションに直接影響を与えるこ 

140
00:07:14,905 --> 00:07:17,480
とはできません。制御できるのは重みとバイアスだけです。

141
00:07:17,480 --> 00:07:20,958
ただし、最後のレイヤーと同様に、必要な変更 

142
00:07:20,958 --> 00:07:24,120
が何であるかをメモしておくと役立ちます。

143
00:07:24,580 --> 00:07:26,559
ただし、ここで 1 段階ズームアウトすると、これは桁 

144
00:07:26,559 --> 00:07:28,100
2 の出 力ニューロンが望んでいることだけ

145
00:07:28,100 --> 00:07:29,200
であることに注意してください。

146
00:07:29,760 --> 00:07:32,198
最後の層にある他のすべてのニューロンもあまりアクティブに

147
00:07:32,198 --> 00:07:34,636
ならないようにした いこと、そしてそれらの他の出力ニュー

148
00:07:34,636 --> 00:07:36,378
ロンはそれぞれ、最後から 2 番目の層 

149
00:07:36,378 --> 00:07:38,816
に何が起こるべきかについて独自の考えを持っていることを思

150
00:07:38,816 --> 00:07:39,600
い出してください。

151
00:07:42,700 --> 00:07:47,054
したがって、この桁 2 ニューロンの欲求は、この最後から 

152
00:07:47,054 --> 00:07:51,559
2 番目の層に何が起こるべきかについての他のすべての出力ニュ

153
00:07:51,559 --> 00:07:56,064
 ーロンの欲求と加算されます。これも、対応する重みに比例し、

154
00:07:56,064 --> 00:07:59,819
各 ニューロンがどれだけ必要とするかに比例します。

155
00:07:59,819 --> 00:08:00,720
変えること。

156
00:08:01,600 --> 00:08:05,480
ここで、逆方向に伝播するというアイデアが登場します。

157
00:08:05,820 --> 00:08:09,656
これらの必要な効果をすべて加算すると、基本的に、最後から 

158
00:08:09,656 --> 00:08:13,360
2 番目のレイヤーに適用するナッジのリストが得られます。

159
00:08:14,220 --> 00:08:16,850
これらを取得したら、それらの値を決定する関連

160
00:08:16,850 --> 00:08:19,480
する重みとバイア スに同じプロセスを再帰的に

161
00:08:19,480 --> 00:08:21,632
適用し、先ほど説明したのと同じプロ 

162
00:08:21,632 --> 00:08:25,100
セスを繰り返し、ネットワークを逆方向に進むことができます。

163
00:08:28,960 --> 00:08:32,052
さらにズームアウトすると、これはすべて、単一のトレーニング 

164
00:08:32,052 --> 00:08:34,732
サンプルがこれらの 重みとバイアスのそれぞれを微調整

165
00:08:34,732 --> 00:08:37,000
する方法にすぎないことを思い出してください。

166
00:08:37,480 --> 00:08:39,347
もし私たちがその 2 の要求にのみ耳を傾けていたとした

167
00:08:39,347 --> 00:08:41,076
ら、ネットワークは最終的には すべての画像を 2 

168
00:08:41,076 --> 00:08:42,943
として分類することだけにインセンティブを与えることにな

169
00:08:42,943 --> 00:08:43,220
ります。

170
00:08:44,059 --> 00:08:46,351
したがって、他のすべてのトレーニング 

171
00:08:46,351 --> 00:08:48,643
サンプルに対して同じバックプ ロップ 

172
00:08:48,643 --> 00:08:52,140
ルーチンを実行し、それぞれのサンプルで重みとバイアスをど 

173
00:08:52,140 --> 00:08:55,035
のように変更したいかを記録し、それらの望ましい変

174
00:08:55,035 --> 00:08:56,000
更を平均します。

175
00:09:01,720 --> 00:09:05,756
ここでの各重みとバイアスの平均ナッジのコレクションは 

176
00:09:05,756 --> 00:09:09,793
、大まかに言えば、最後のビデオで参照されたコスト関数 

177
00:09:09,793 --> 00:09:13,680
の負の勾配、または少なくともそれに比例するものです。

178
00:09:14,380 --> 00:09:17,133
私がこれらのナッジについて定量的に正確に理解できていない

179
00:09:17,133 --> 00:09:19,887
から大まかに言ってるだけです が、私が今言及したすべての

180
00:09:19,887 --> 00:09:22,739
変更、なぜ一部の変更が他の変更よりも比例して大きくなるの 

181
00:09:22,739 --> 00:09:25,492
か、そしてそれらすべてをどのように加算する必要があるのか

182
00:09:25,492 --> 00:09:28,246
を理解できたなら、あなたはそ のメカニズムを理解している

183
00:09:28,246 --> 00:09:31,000
でしょう。バックプロパゲーションが実際に行っていること。

184
00:09:33,960 --> 00:09:36,740
ところで、実際には、コンピューターがすべ

185
00:09:36,740 --> 00:09:39,520
ての学習例の影響を勾 配降下ステップごと

186
00:09:39,520 --> 00:09:42,440
に合計するには非常に長い時間がかかります。

187
00:09:43,140 --> 00:09:44,820
そこで、代わりに一般的に行われることを以下に示します。

188
00:09:45,480 --> 00:09:47,735
トレーニング データをランダムにシャッフルし、それを

189
00:09:47,735 --> 00:09:49,123
多数のミニバッチに分割します 。

190
00:09:49,123 --> 00:09:51,379
たとえば、各ミニバッチに 100 個のトレーニング 

191
00:09:51,379 --> 00:09:52,420
サンプルがあるとします。

192
00:09:52,939 --> 00:09:57,280
次に、ミニバッチに従ってステップを計算します。

193
00:09:57,280 --> 00:10:00,201
これはコスト関数の実際の勾配ではなく、この小さなサ

194
00:10:00,201 --> 00:10:02,538
ブセットでは なくすべてのトレーニング 

195
00:10:02,538 --> 00:10:04,758
データに依存するため、最も効率的な下 

196
00:10:04,758 --> 00:10:07,679
り坂のステップではありませんが、各ミニバッチからか

197
00:10:07,679 --> 00:10:10,600
なり良好な近 似が得られます。さらに重要なのは、計

198
00:10:10,600 --> 00:10:12,120
算速度が大幅に向上します。

199
00:10:12,820 --> 00:10:16,288
関連するコスト曲面の下でネットワークの軌跡をプロットする

200
00:10:16,288 --> 00:10:19,756
と、それは、慎 重に計算して各ステップの下り坂の方向を正

201
00:10:19,756 --> 00:10:23,223
確に決定するというよりは、目 的もなく坂を下りながらも素

202
00:10:23,223 --> 00:10:26,939
早いステップを踏む酔っぱらいの男に似たもの になるでしょう。

203
00:10:26,939 --> 00:10:30,160
その方向に非常にゆっくりと慎重に一歩を踏み出す前に。

204
00:10:31,540 --> 00:10:34,660
この手法は確率的勾配降下法と呼ばれます。

205
00:10:35,960 --> 00:10:37,737
ここではたくさんのことが起こってい

206
00:10:37,737 --> 00:10:39,620
るので、自分用にまとめてみましょう。

207
00:10:40,440 --> 00:10:42,838
バックプロパゲーションは、単一のトレーニング 

208
00:10:42,838 --> 00:10:45,862
サンプルが重みとバイアスを どのように微調整するかを決定す

209
00:10:45,862 --> 00:10:48,052
るためのアルゴリズムです。重みとバイアス 

210
00:10:48,052 --> 00:10:51,076
を上昇させるか下降させるかという観点だけでなく、それらの変

211
00:10:51,076 --> 00:10:54,100
化に対する相対 的な割合が最も急速な減少を引き起こすかとい

212
00:10:54,100 --> 00:10:55,560
う観点から判断します。料金。

213
00:10:56,260 --> 00:11:00,656
本当の勾配降下ステップでは、これを何万、何千ものトレーニ 

214
00:11:00,656 --> 00:11:04,901
ング例すべてに対して実行し、得られる望ましい変化を平均 

215
00:11:04,901 --> 00:11:09,146
する必要がありますが、計算が遅いため、代わりにデータを 

216
00:11:09,146 --> 00:11:13,240
ランダムにミニバッチに分割し、各ステップをミニバッチ。

217
00:11:14,000 --> 00:11:16,797
すべてのミニバッチを繰り返し実行してこれらの調整

218
00:11:16,797 --> 00:11:19,828
を行うと、コスト関 数の極小値に向かって収束します。

219
00:11:19,828 --> 00:11:21,926
つまり、ネットワークがトレーニング 

220
00:11:21,926 --> 00:11:24,724
サンプルで非常に優れたパフォーマンスを発揮するこ

221
00:11:24,724 --> 00:11:25,540
とになります。

222
00:11:27,240 --> 00:11:30,278
以上のことをすべて踏まえた上で、backprop 

223
00:11:30,278 --> 00:11:33,438
の実装に含まれるコードのすべ ての行は、少なくとも非

224
00:11:33,438 --> 00:11:36,720
公式の用語では、実際にこれまでに見たものと一致します。

225
00:11:37,560 --> 00:11:39,676
しかし、数学が何をするのかを知ることは戦

226
00:11:39,676 --> 00:11:41,792
いの半分に過ぎず、単に それを表現するだ

227
00:11:41,792 --> 00:11:44,120
けですべてが混乱して混乱することもあります。

228
00:11:44,860 --> 00:11:48,856
したがって、さらに詳しく知りたい人のために、次のビデオ 

229
00:11:48,856 --> 00:11:52,709
では、ここで紹介したのと同じアイデアを説明しますが、 

230
00:11:52,709 --> 00:11:56,420
基礎となる微積分の観点から説明します。他のリソース。

231
00:11:57,340 --> 00:11:58,845
その前に、強調しておきたいことの 1 

232
00:11:58,845 --> 00:12:01,144
つは、このアルゴリズムが機能するに は、これはニューラル 

233
00:12:01,144 --> 00:12:03,125
ネットワークだけでなくあらゆる種類の機械学習に当 

234
00:12:03,125 --> 00:12:04,552
てはまりますが、大量のトレーニング 

235
00:12:04,552 --> 00:12:05,900
データが必要であるということです。

236
00:12:06,420 --> 00:12:09,292
私たちの場合、手書きの数字がこれほど優れた例である理由の 

237
00:12:09,292 --> 00:12:12,065
1 つは、人間によってラベ ル付けされた非常に多くの例が

238
00:12:12,065 --> 00:12:14,740
含まれる MNIST データベースが存在することです。

239
00:12:15,300 --> 00:12:18,249
したがって、機械学習に取り組んでいる人ならよく知ってい

240
00:12:18,249 --> 00:12:21,200
るであろう共通の課 題は、数万枚の画像にラベルを付ける

241
00:12:21,200 --> 00:12:23,275
ことであろうと、扱う他のデータ型であ 

242
00:12:23,275 --> 00:12:25,679
ろうと、実際に必要なラベル付きトレーニング 

243
00:12:25,679 --> 00:12:27,100
データを取得することです。

