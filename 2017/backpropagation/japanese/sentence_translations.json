[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn. ",
  "translatedText": "ここでは、ニューラル ネットワークの学習方法の背後にある中心的なアルゴリズムであるバックプロパゲーションに取り組みます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. ",
  "translatedText": "ここまでの概要を簡単にまとめた後、最初に、数式を参照せずに、 アルゴリズムが実際に何を行っているかを直感的に説明します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. ",
  "translatedText": "次に、数学について詳しく知りたい人のために、次のビデオで は、これらすべての基礎となる微積分について説明します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. ",
  "translatedText": "最後の 2 つのビデオをご覧になった場合、または適切な背景を理解してすぐに参加した場合は、ニューラル ネットワーク とは何か、またニューラル ネットワークがどのように情報をフィードフォワードするかについては理解しているでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. ",
  "translatedText": "ここでは、ピクセル値が 784 個のニューロンを含むネットワークの最初の層に入力さ れる手書きの数字を認識する古典的な例を行っています。また、それぞれ 16 個のニ ューロンしか持たない 2 つの隠れ層と出力を含むネットワークを示しています。10 個のニューロンの層。ネットワークがどの桁を答えとして選択しているかを示します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. ",
  "translatedText": "また、前回のビデオで説明したように、勾配降下法と、学習とは、 どの重みとバイアスが特定のコスト関数を最小化するかを見つける ことを意味することを理解していただくことも期待しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. ",
  "translatedText": "簡単に思い出していただきたいのですが、1 つのトレーニング サンプル のコストとして、ネットワークが提供する出力と、ネットワークに提供して もらいたい出力を取得し、各コンポーネントの差の 2 乗を合計します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. ",
  "translatedText": "これを何万ものトレーニング例すべてに対して実行し、結 果を平均すると、ネットワークの総コストが得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. ",
  "translatedText": "最後のビデオで説明したように、それだけでは考えるのが十分ではないか のように、私たちが探しているのはこのコスト関数の負の勾配です。こ れは、すべての重みとバイアスをどのように変更する必要があるかを示し ています。これらの接続により、最も効率的にコストが削減されます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. ",
  "translatedText": "このビデオのトピックであるバックプロパゲーションは、 非常に複雑な勾配を計算するためのアルゴリズムです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "The one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. ",
  "translatedText": "最後のビデオで、今すぐにしっかりと頭の中に留めておいてほしい 1 つ のアイデアは、勾配ベクトルを 13,000 次元の方向として考えるこ とは、簡単に言えば、私たちの想像の範囲を超えているため、別のアイデア があるということです。あなたがそれについて考えることができる方法。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. ",
  "translatedText": "ここでの各成分の大きさは、コスト関数が各重みとバイ アスに対してどの程度敏感であるかを示しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. ",
  "translatedText": "たとえば、これから説明するプロセスを実行し、負の勾配を計算したところ、こ のエッジの重みに関連付けられたコンポーネントが 3 であることが判明し たとします。2 ですが、このエッジに関連付けられたコンポーネントは 0 として出力されます。1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. ",
  "translatedText": "これをどう解釈するかというと、関数のコストは最初の重みの変 化に対して 32 倍敏感であるため、その値を少し変更する と、コストに何らかの変化が生じ、その変化は2 番目の重りに 対する同じ揺れが与える値よりも 32 倍大きくなります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and index chasing of it all. ",
  "translatedText": "個人的に、私が最初にバックプロパゲーションについて学んだとき、最 も混乱したのは単にその表記とインデックスの追跡だったと思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. ",
  "translatedText": "しかし、このアルゴリズムの各部分が実際に何をしているのかを紐 解いてみると、それがもたらす個々の効果は実際には非常に直感的 であり、ただ多くの小さな調整が積み重ねられているだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. ",
  "translatedText": "したがって、ここでは表記を完全に無視して物事を開始し、各トレー ニング例が重みとバイアスに与える影響を段階的に見ていきます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. ",
  "translatedText": "コスト関数には、数万のトレーニング サンプル全体にわたるサンプル あたりの特定のコストの平均が含まれるため、単一の勾配降下ステップ の重みとバイアスを調整する方法も、すべてのサンプルに依存します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. ",
  "translatedText": "というか、原理的にはそうすべきですが、計算効率を高めるために、各ステップですべて のサンプルをヒットする必要がないように、後でちょっとしたトリックを実行します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. ",
  "translatedText": "他のケースでは、現時点では、この 2 の画 像という 1 つの例に注目するだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted? ",
  "translatedText": "この 1 つのトレーニング例は、重みとバイアスの調整方法にどのような影響を与えるでしょうか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. ",
  "translatedText": "ネットワークがまだ十分にトレーニングされていない段階にあるとします。そのため、出力内のアクティ ベーションはかなりランダムに、おそらく 0 のようなものになるとします。5、0。8、0。2 、延々と。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases, but it's helpful to keep track of which adjustments we wish should take place to that output layer. ",
  "translatedText": "これらのアクティベーションを直接変更することはできず、影 響を受けるのは重みとバイアスのみですが、その出力層に対し てどの調整を行う必要があるかを追跡するのに役立ちます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 242.52,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. ",
  "translatedText": "そして、画像を 2 として分類したいので、3 番目の値を少 しずつ上げて、他のすべての値を少しずつ下げるようにします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. ",
  "translatedText": "さらに、これらのナッジのサイズは、各現在の値がその目標 値からどれだけ離れているかに比例する必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. ",
  "translatedText": "たとえば、2 番のニューロンの活性化の増加は、 ある意味で、すでにあるべき状態にかなり近づいて いる 8 番のニューロンの減少よりも重要です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. ",
  "translatedText": "そこでさらにズームインして、活性化を高めたいこの 1 つのニューロンだけに焦点を当ててみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. ",
  "translatedText": "アクティベーションは、前の層のすべてのアクティベーションの特定の加 重合計にバイアスを加えたものとして定義され、そのすべてがシグモイド 潰し関数や ReLU などに組み込まれることに注意してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation. ",
  "translatedText": "したがって、その活性化を高めるために連携 できる 3 つの異なる方法があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. ",
  "translatedText": "バイアスを増やしたり、重みを増やしたり、前のレイヤ ーからのアクティベーションを変更したりできます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. ",
  "translatedText": "ウェイトをどのように調整するかに注目して、ウェイトが実際に どのように異なるレベルの影響を与えるかに注目してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. ",
  "translatedText": "前の層の最も明るいニューロンとの接続は、それらの重みにより大 きな活性化値が乗算されるため、最も大きな効果をもたらします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. ",
  "translatedText": "したがって、これらの重みの 1 つを増加すると、少なくともこの 1 つのトレーニング例に関する限り、実際には、ディマー ニューロンとの接 続の重みを増加するよりも最終的なコスト関数に強い影響を及ぼします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck. ",
  "translatedText": "勾配降下法について話すとき、私たちは単に各コンポーネントを 上に動かすか下に動かすかだけを気にするのではなく、どれが最 も費用対効果が高いかを気にしていることに注意してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together. ",
  "translatedText": "ちなみに、これは、ニューロンの生物学的ネットワークがどのように学習するかについて の神経科学の理論、ヘビアン理論を少なくともいくらか思い出させます。ヘビアン理論は 、しばしば「発火するニューロンは一緒に配線する」というフレーズに要約されます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active and the ones which we wish to become more active. ",
  "translatedText": "ここで、重みの最大の増加、つまり接続の最大の強 化は、最もアクティブなニューロンと、よりアク ティブになりたいニューロンの間で発生します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those firing when thinking about it. ",
  "translatedText": "ある意味、「2」を見ているときに発火しているニューロンは、それについ て考えているときに発火しているニューロンとより強く結びついています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note. ",
  "translatedText": "誤解のないように言っておきますが、私はニューロンの人工ネットワークが生物学的な脳のように振る 舞うかどうかについて何らかの形で意見を言う立場にありません。そして、この「ファイア・トゥゲ ザー・ワイヤー・トゥゲザー」というアイデアには意味のあるアスタリスクがいくつか付いています が、非常に緩いものとして捉えられています。たとえて言えば、注目するのは興味深いと思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer. ",
  "translatedText": "とにかく、このニューロンの活性化を高める 3 番目の 方法は、前の層のすべての活性化を変更することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active. ",
  "translatedText": "つまり、正の重みを持つ数字 2 のニューロンに接続されている すべてが明るくなり、負の重みに接続されているすべてが暗くなる と、その数字 2 のニューロンはよりアクティブになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. ",
  "translatedText": "ウェイトの変更と同様に、対応するウェイトのサイズに比例する 変更を求めることで、最も大きな利益を得ることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases. ",
  "translatedText": "もちろん、これらのアクティベーションに直接影響を与えるこ とはできません。制御できるのは重みとバイアスだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are. ",
  "translatedText": "ただし、最後のレイヤーと同様に、必要な変更 が何であるかをメモしておくと役立ちます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants. ",
  "translatedText": "ただし、ここで 1 段階ズームアウトすると、これは桁 2 の出 力ニューロンが望んでいることだけであることに注意してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. ",
  "translatedText": "最後の層にある他のすべてのニューロンもあまりアクティブにならないようにした いこと、そしてそれらの他の出力ニューロンはそれぞれ、最後から 2 番目の層 に何が起こるべきかについて独自の考えを持っていることを思い出してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. ",
  "translatedText": "したがって、この桁 2 ニューロンの欲求は、この最後から 2 番目の層に何が起こるべきかについての他のすべての出力ニュ ーロンの欲求と加算されます。これも、対応する重みに比例し、各 ニューロンがどれだけ必要とするかに比例します。変えること。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in. ",
  "translatedText": "ここで、逆方向に伝播するというアイデアが登場します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. ",
  "translatedText": "これらの必要な効果をすべて加算すると、基本的に、最後から 2 番目のレイヤーに適用するナッジのリストが得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. ",
  "translatedText": "これらを取得したら、それらの値を決定する関連する重みとバイア スに同じプロセスを再帰的に適用し、先ほど説明したのと同じプロ セスを繰り返し、ネットワークを逆方向に進むことができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. ",
  "translatedText": "さらにズームアウトすると、これはすべて、単一のトレーニング サンプルがこれらの 重みとバイアスのそれぞれを微調整する方法にすぎないことを思い出してください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2. ",
  "translatedText": "もし私たちがその 2 の要求にのみ耳を傾けていたとしたら、ネットワークは最終的には すべての画像を 2 として分類することだけにインセンティブを与えることになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes. ",
  "translatedText": "したがって、他のすべてのトレーニング サンプルに対して同じバックプ ロップ ルーチンを実行し、それぞれのサンプルで重みとバイアスをど のように変更したいかを記録し、それらの望ましい変更を平均します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. ",
  "translatedText": "ここでの各重みとバイアスの平均ナッジのコレクションは 、大まかに言えば、最後のビデオで参照されたコスト関数 の負の勾配、または少なくともそれに比例するものです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. ",
  "translatedText": "私がこれらのナッジについて定量的に正確に理解できていないから大まかに言ってるだけです が、私が今言及したすべての変更、なぜ一部の変更が他の変更よりも比例して大きくなるの か、そしてそれらすべてをどのように加算する必要があるのかを理解できたなら、あなたはそ のメカニズムを理解しているでしょう。バックプロパゲーションが実際に行っていること。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. ",
  "translatedText": "ところで、実際には、コンピューターがすべての学習例の影響を勾 配降下ステップごとに合計するには非常に長い時間がかかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead. ",
  "translatedText": "そこで、代わりに一般的に行われることを以下に示します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and divide it into a whole bunch of mini-batches, let's say each one having 100 training examples. ",
  "translatedText": "トレーニング データをランダムにシャッフルし、それを多数のミニバッチに分割します 。たとえば、各ミニバッチに 100 個のトレーニング サンプルがあるとします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch. ",
  "translatedText": "次に、ミニバッチに従ってステップを計算します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly it gives you a significant computational speedup. ",
  "translatedText": "これはコスト関数の実際の勾配ではなく、この小さなサブセットでは なくすべてのトレーニング データに依存するため、最も効率的な下 り坂のステップではありませんが、各ミニバッチからかなり良好な近 似が得られます。さらに重要なのは、計算速度が大幅に向上します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. ",
  "translatedText": "関連するコスト曲面の下でネットワークの軌跡をプロットすると、それは、慎 重に計算して各ステップの下り坂の方向を正確に決定するというよりは、目 的もなく坂を下りながらも素早いステップを踏む酔っぱらいの男に似たもの になるでしょう。その方向に非常にゆっくりと慎重に一歩を踏み出す前に。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent. ",
  "translatedText": "この手法は確率的勾配降下法と呼ばれます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we? ",
  "translatedText": "ここではたくさんのことが起こっているので、自分用にまとめてみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. ",
  "translatedText": "バックプロパゲーションは、単一のトレーニング サンプルが重みとバイアスを どのように微調整するかを決定するためのアルゴリズムです。重みとバイアス を上昇させるか下降させるかという観点だけでなく、それらの変化に対する相対 的な割合が最も急速な減少を引き起こすかという観点から判断します。料金。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes you get, but that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch. ",
  "translatedText": "本当の勾配降下ステップでは、これを何万、何千ものトレーニ ング例すべてに対して実行し、得られる望ましい変化を平均 する必要がありますが、計算が遅いため、代わりにデータを ランダムにミニバッチに分割し、各ステップをミニバッチ。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.26,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples. ",
  "translatedText": "すべてのミニバッチを繰り返し実行してこれらの調整を行うと、コスト関 数の極小値に向かって収束します。つまり、ネットワークがトレーニング サンプルで非常に優れたパフォーマンスを発揮することになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms. ",
  "translatedText": "以上のことをすべて踏まえた上で、backprop の実装に含まれるコードのすべ ての行は、少なくとも非公式の用語では、実際にこれまでに見たものと一致します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. ",
  "translatedText": "しかし、数学が何をするのかを知ることは戦いの半分に過ぎず、単に それを表現するだけですべてが混乱して混乱することもあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So, for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. ",
  "translatedText": "したがって、さらに詳しく知りたい人のために、次のビデオ では、ここで紹介したのと同じアイデアを説明しますが、 基礎となる微積分の観点から説明します。他のリソース。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. ",
  "translatedText": "その前に、強調しておきたいことの 1 つは、このアルゴリズムが機能するに は、これはニューラル ネットワークだけでなくあらゆる種類の機械学習に当 てはまりますが、大量のトレーニング データが必要であるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans. ",
  "translatedText": "私たちの場合、手書きの数字がこれほど優れた例である理由の 1 つは、人間によってラベ ル付けされた非常に多くの例が含まれる MNIST データベースが存在することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with. ",
  "translatedText": "したがって、機械学習に取り組んでいる人ならよく知っているであろう共通の課 題は、数万枚の画像にラベルを付けることであろうと、扱う他のデータ型であ ろうと、実際に必要なラベル付きトレーニング データを取得することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]