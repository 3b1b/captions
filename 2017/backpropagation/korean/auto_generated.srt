1
00:00:04,059 --> 00:00:06,202
여기서는 신경망 학습의 핵심 

2
00:00:06,202 --> 00:00:08,880
알고리즘인 역전파에 대해 살펴봅니다.

3
00:00:09,400 --> 00:00:11,504
현재 상황을 간단히 요약한 후, 

4
00:00:11,504 --> 00:00:13,726
수식에 대한 언급 없이 알고리즘이 

5
00:00:13,726 --> 00:00:17,000
실제로 어떤 일을 하는지 직관적으로 살펴보겠습니다.

6
00:00:17,660 --> 00:00:19,527
수학에 대해 자세히 알아보고 싶은 분들을 

7
00:00:19,527 --> 00:00:21,233
위해 다음 동영상에서는 이 모든 것의 

8
00:00:21,233 --> 00:00:23,020
기초가 되는 미적분학에 대해 설명합니다.

9
00:00:23,820 --> 00:00:26,364
지난 두 개의 동영상을 보셨거나 적절한 배경지식을 

10
00:00:26,364 --> 00:00:28,546
가지고 이 글을 읽는다면 신경망이 무엇이고 

11
00:00:28,546 --> 00:00:31,000
어떻게 정보를 전달하는지 잘 알고 계실 것입니다.

12
00:00:31,680 --> 00:00:34,399
여기서는 784개의 뉴런이 있는 네트워크의 첫 

13
00:00:34,399 --> 00:00:37,013
번째 레이어에 픽셀 값이 입력되는 손으로 쓴 

14
00:00:37,013 --> 00:00:40,046
숫자를 인식하는 전형적인 예시를 보여드리고 있으며, 

15
00:00:40,046 --> 00:00:43,079
각각 16개의 뉴런이 있는 두 개의 숨겨진 레이어와 

16
00:00:43,079 --> 00:00:46,111
10개의 뉴런이 있는 출력 레이어를 통해 네트워크가 

17
00:00:46,111 --> 00:00:49,040
어떤 숫자를 답으로 선택하는지 보여드리고 있습니다.

18
00:00:50,040 --> 00:00:53,865
또한 지난 동영상에서 설명한 것처럼 그라데이션 하강을 

19
00:00:53,865 --> 00:00:57,307
이해하고, 특정 비용 함수를 최소화하는 가중치와 

20
00:00:57,307 --> 00:01:00,495
편향을 찾는 학습의 의미를 이해할 수 있기를 

21
00:01:00,495 --> 00:01:01,260
기대합니다.

22
00:01:02,040 --> 00:01:05,947
간단히 상기시켜 드리자면, 단일 학습 예제 비용의 

23
00:01:05,947 --> 00:01:09,715
경우 네트워크가 제공하는 출력과 사용자가 원하는 

24
00:01:09,715 --> 00:01:13,483
출력을 취하고 각 구성 요소 간의 차이의 제곱을 

25
00:01:13,483 --> 00:01:14,600
더하면 됩니다.

26
00:01:15,380 --> 00:01:17,653
수만 개의 훈련 예제 모두에 대해 이 

27
00:01:17,653 --> 00:01:19,926
작업을 수행하고 그 결과를 평균화하면 

28
00:01:19,926 --> 00:01:22,200
네트워크의 총 비용을 알 수 있습니다.

29
00:01:22,200 --> 00:01:26,267
지난 동영상에서 설명한 것처럼 우리가 찾고 있는 

30
00:01:26,267 --> 00:01:29,431
것은 이 비용 함수의 음의 기울기로, 

31
00:01:29,431 --> 00:01:33,649
비용을 가장 효율적으로 줄이기 위해 모든 가중치와 

32
00:01:33,649 --> 00:01:37,416
편향, 모든 연결을 어떻게 변경해야 하는지를 

33
00:01:37,416 --> 00:01:38,320
알려줍니다.

34
00:01:43,260 --> 00:01:45,742
이 동영상의 주제인 역전파는 엄청나게 

35
00:01:45,742 --> 00:01:48,580
복잡한 그라데이션을 계산하는 알고리즘입니다.

36
00:01:49,140 --> 00:01:52,116
그리고 마지막 영상에서 여러분이 지금 꼭 기억해 

37
00:01:52,116 --> 00:01:54,871
두었으면 하는 한 가지 아이디어는 그라데이션 

38
00:01:54,871 --> 00:01:57,848
벡터를 13,000차원의 방향으로 생각하는 것은 

39
00:01:57,848 --> 00:02:00,603
가볍게 말하면 상상의 범위를 넘어서는 것이기 

40
00:02:00,603 --> 00:02:03,580
때문에 다른 방식으로 생각할 수 있다는 것입니다.

41
00:02:04,600 --> 00:02:07,643
여기서 각 구성 요소의 크기는 비용 함수가 

42
00:02:07,643 --> 00:02:10,940
각 가중치와 편향에 얼마나 민감한지 알려줍니다.

43
00:02:11,800 --> 00:02:14,289
예를 들어 지금부터 설명할 프로세스를 

44
00:02:14,289 --> 00:02:17,015
진행하면서 음의 기울기를 계산했는데 여기 

45
00:02:17,015 --> 00:02:19,859
이 가장자리의 가중치와 관련된 구성 요소가 

46
00:02:19,859 --> 00:02:22,585
3.2로 나오는 반면 여기 이 가장자리와 

47
00:02:22,585 --> 00:02:25,548
관련된 구성 요소는 0.1로 나왔다고 가정해 

48
00:02:25,548 --> 00:02:26,260
보겠습니다.

49
00:02:26,820 --> 00:02:30,094
이를 해석하는 방식은 함수의 비용이 첫 번째 

50
00:02:30,094 --> 00:02:33,237
가중치의 변화에 32배 더 민감하므로 해당 

51
00:02:33,237 --> 00:02:36,249
값을 조금만 흔들면 비용에 약간의 변화가 

52
00:02:36,249 --> 00:02:39,523
발생하고 그 변화는 두 번째 가중치에 동일한 

53
00:02:39,523 --> 00:02:43,060
흔들림이 주는 것보다 32배 더 크다는 것입니다.

54
00:02:48,420 --> 00:02:52,351
개인적으로 역전파를 처음 배울 때 가장 혼란스러웠던 

55
00:02:52,351 --> 00:02:55,740
부분은 표기법과 색인을 쫓아가는 것이었습니다.

56
00:02:56,220 --> 00:02:59,230
하지만 이 알고리즘의 각 부분이 실제로 무엇을 

57
00:02:59,230 --> 00:03:02,587
하는지를 살펴보면, 각각의 개별 효과는 실제로 매우 

58
00:03:02,587 --> 00:03:06,061
직관적이며, 단지 많은 작은 조정이 서로 겹쳐져 있을 

59
00:03:06,061 --> 00:03:06,640
뿐입니다.

60
00:03:07,740 --> 00:03:10,751
그래서 여기서는 표기법을 완전히 무시하고 

61
00:03:10,751 --> 00:03:13,370
각 트레이닝 예제가 가중치와 편향에 

62
00:03:13,370 --> 00:03:16,120
미치는 영향을 단계별로 살펴보겠습니다.

63
00:03:17,020 --> 00:03:20,561
비용 함수에는 수만 개의 모든 학습 예제에 

64
00:03:20,561 --> 00:03:24,103
대해 예제당 특정 비용의 평균이 포함되므로 

65
00:03:24,103 --> 00:03:27,350
단일 경사 하강 단계의 가중치와 편향을 

66
00:03:27,350 --> 00:03:31,040
조정하는 방법도 모든 예제에 따라 달라집니다.

67
00:03:31,680 --> 00:03:34,017
원칙적으로는 그래야 하지만 계산 효율성을 

68
00:03:34,017 --> 00:03:36,456
위해 나중에 모든 단계에 대해 모든 예제를 

69
00:03:36,456 --> 00:03:39,200
치지 않아도 되도록 약간의 트릭을 적용하겠습니다.

70
00:03:39,200 --> 00:03:42,658
다른 경우에는 지금 당장 하나의 예시, 

71
00:03:42,658 --> 00:03:45,960
즉 이 2 이미지에 집중해 보겠습니다.

72
00:03:46,720 --> 00:03:49,155
이 하나의 훈련 예시가 가중치와 편향이 

73
00:03:49,155 --> 00:03:51,480
조정되는 방식에 어떤 영향을 미칠까요?

74
00:03:52,680 --> 00:03:55,745
네트워크가 아직 잘 훈련되지 않은 상태이므로 

75
00:03:55,745 --> 00:03:58,075
출력의 활성화가 0.5, 0.8, 

76
00:03:58,075 --> 00:04:01,264
0.2 등 매우 무작위로 보일 것이라고 가정해 

77
00:04:01,264 --> 00:04:02,000
보겠습니다.

78
00:04:02,520 --> 00:04:04,691
이러한 활성화는 직접 변경할 수 없으며 

79
00:04:04,691 --> 00:04:07,160
가중치와 편향성에만 영향을 미칠 수 있습니다.

80
00:04:07,160 --> 00:04:09,644
하지만 해당 출력 레이어에 어떤 조정이 

81
00:04:09,644 --> 00:04:12,580
이루어져야 하는지 추적하는 것이 도움이 됩니다.

82
00:04:13,360 --> 00:04:15,828
그리고 이미지를 2로 분류하고 싶기 

83
00:04:15,828 --> 00:04:18,297
때문에 세 번째 값은 위로 올라가고 

84
00:04:18,297 --> 00:04:21,260
다른 모든 값은 아래로 내려가기를 원합니다.

85
00:04:22,060 --> 00:04:25,586
또한 이러한 넛지의 크기는 각 현재 값이 목표 

86
00:04:25,586 --> 00:04:29,520
값에서 얼마나 멀리 떨어져 있는지에 비례해야 합니다.

87
00:04:30,220 --> 00:04:33,902
예를 들어, 2번 뉴런의 활성화가 증가하는 것이 8번 

88
00:04:33,902 --> 00:04:37,462
뉴런의 활성화가 감소하는 것보다 어떤 의미에서는 더 

89
00:04:37,462 --> 00:04:40,900
중요하며, 이미 그 위치에 상당히 근접해 있습니다.

90
00:04:42,040 --> 00:04:44,540
따라서 더 확대하여 활성화를 높이고자 

91
00:04:44,540 --> 00:04:47,280
하는 이 뉴런 하나에만 집중해 보겠습니다.

92
00:04:48,180 --> 00:04:52,614
활성화는 이전 레이어의 모든 활성화와 바이어스의 특정 

93
00:04:52,614 --> 00:04:56,753
가중치 합으로 정의되며, 이 모든 것을 시그모이드 

94
00:04:56,753 --> 00:05:01,040
스퀴시화 함수 또는 ReLU와 같은 것에 연결합니다.

95
00:05:01,640 --> 00:05:04,330
따라서 이러한 활성화를 높이기 위해 함께 

96
00:05:04,330 --> 00:05:07,020
협력할 수 있는 세 가지 방법이 있습니다.

97
00:05:07,440 --> 00:05:10,602
편향성을 높이고 가중치를 높일 수 있으며 

98
00:05:10,602 --> 00:05:14,040
이전 레이어에서 활성화를 변경할 수 있습니다.

99
00:05:14,940 --> 00:05:17,688
가중치를 조정하는 방법에 초점을 맞춰 가중치가 

100
00:05:17,688 --> 00:05:20,860
실제로 어떻게 다른 수준의 영향을 미치는지 살펴보세요.

101
00:05:21,440 --> 00:05:23,910
이전 레이어에서 가장 밝은 뉴런과의 

102
00:05:23,910 --> 00:05:26,381
연결은 가중치가 더 큰 활성화 값을 

103
00:05:26,381 --> 00:05:29,100
곱하기 때문에 가장 큰 영향을 미칩니다.

104
00:05:31,460 --> 00:05:34,465
따라서 이러한 가중치 중 하나를 증가시키면, 

105
00:05:34,465 --> 00:05:37,229
적어도 이 하나의 훈련 예제에 관한 한, 

106
00:05:37,229 --> 00:05:39,994
희미한 뉴런과의 연결 가중치를 증가시키는 

107
00:05:39,994 --> 00:05:43,480
것보다 궁극적인 비용 함수에 더 큰 영향을 미칩니다.

108
00:05:44,420 --> 00:05:46,143
경사도 하강에 대해 이야기할 때, 

109
00:05:46,143 --> 00:05:48,230
각 구성 요소를 위 또는 아래로 넛지할지 

110
00:05:48,230 --> 00:05:50,226
여부만 고려하는 것이 아니라 어떤 구성 

111
00:05:50,226 --> 00:05:52,403
요소가 가장 큰 효과를 주는지를 고려한다는 

112
00:05:52,403 --> 00:05:53,220
점을 기억하세요.

113
00:05:55,020 --> 00:05:57,880
그런데 이것은 뉴런의 생물학적 네트워크가 어떻게 

114
00:05:57,880 --> 00:06:00,528
학습하는지에 대한 신경과학의 이론인 헤비비언 

115
00:06:00,528 --> 00:06:02,434
이론을 어느 정도 연상시키는데, 

116
00:06:02,434 --> 00:06:05,506
이는 흔히 '함께 발화하는 뉴런은 서로 연결된다'는 

117
00:06:05,506 --> 00:06:06,460
말로 요약됩니다.

118
00:06:07,260 --> 00:06:09,765
여기서 가장 큰 가중치 증가, 

119
00:06:09,765 --> 00:06:12,712
가장 큰 연결 강화는 가장 활성화된 

120
00:06:12,712 --> 00:06:15,659
뉴런과 더 활성화되기를 원하는 뉴런 

121
00:06:15,659 --> 00:06:17,280
사이에서 일어납니다.

122
00:06:17,940 --> 00:06:21,037
어떤 의미에서 2를 볼 때 발화하는 뉴런은 2에 

123
00:06:21,037 --> 00:06:24,480
대해 생각할 때 발화하는 뉴런과 더 강하게 연결됩니다.

124
00:06:25,400 --> 00:06:28,174
분명히 말씀드리지만, 저는 인공 뉴런 네트워크가 

125
00:06:28,174 --> 00:06:30,949
생물학적 뇌처럼 작동하는지에 대해 어떤 식으로든 

126
00:06:30,949 --> 00:06:32,798
언급할 수 있는 입장이 아니며, 

127
00:06:32,798 --> 00:06:35,265
이 '함께 불을 붙인다'는 아이디어에는 몇 

128
00:06:35,265 --> 00:06:37,628
가지 의미 있는 별표가 붙어 있지만 매우 

129
00:06:37,628 --> 00:06:40,300
느슨한 비유로 받아들이면 흥미로운 점을 발견할 

130
00:06:40,300 --> 00:06:41,020
수 있습니다.

131
00:06:41,940 --> 00:06:45,429
어쨌든, 이 뉴런의 활성화를 높일 수 있는 세 번째 

132
00:06:45,429 --> 00:06:49,040
방법은 이전 레이어의 모든 활성화를 변경하는 것입니다.

133
00:06:49,040 --> 00:06:52,781
즉, 양수 가중치를 가진 숫자 2 뉴런에 연결된 

134
00:06:52,781 --> 00:06:56,661
모든 것이 더 밝아지고, 음수 가중치를 가진 모든 

135
00:06:56,661 --> 00:07:00,680
것이 더 어두워지면 숫자 2 뉴런이 더 활성화됩니다.

136
00:07:02,540 --> 00:07:04,671
그리고 가중치 변경과 마찬가지로, 

137
00:07:04,671 --> 00:07:07,026
해당 가중치의 크기에 비례하는 변경을 

138
00:07:07,026 --> 00:07:10,280
추구하면 투자 대비 최대의 효과를 얻을 수 있습니다.

139
00:07:12,140 --> 00:07:14,860
물론 이러한 활성화에 직접적으로 영향을 줄 수는 

140
00:07:14,860 --> 00:07:17,480
없으며, 가중치와 편향성만 제어할 수 있습니다.

141
00:07:17,480 --> 00:07:20,737
하지만 마지막 레이어와 마찬가지로 원하는 변경 

142
00:07:20,737 --> 00:07:24,120
사항이 무엇인지 메모해 두는 것이 도움이 됩니다.

143
00:07:24,580 --> 00:07:26,846
하지만 여기서 한 단계 축소하면 숫자 2 출력 

144
00:07:26,846 --> 00:07:29,200
뉴런이 원하는 것은 이것뿐이라는 점을 명심하세요.

145
00:07:29,760 --> 00:07:31,906
또한 마지막 레이어의 다른 모든 뉴런이 덜 

146
00:07:31,906 --> 00:07:34,322
활성화되기를 원하며, 다른 출력 뉴런은 각각 두 

147
00:07:34,322 --> 00:07:36,558
번째에서 마지막 레이어에 어떤 일이 일어나야 

148
00:07:36,558 --> 00:07:39,063
하는지에 대한 자체적인 생각을 가지고 있다는 것을 

149
00:07:39,063 --> 00:07:39,600
기억하세요.

150
00:07:42,700 --> 00:07:46,015
따라서 이 숫자 2 뉴런의 욕구는 이 두 

151
00:07:46,015 --> 00:07:49,619
번째에서 마지막 레이어에 어떤 일이 일어나야 

152
00:07:49,619 --> 00:07:53,223
하는지에 대한 다른 모든 출력 뉴런의 욕구와 

153
00:07:53,223 --> 00:07:56,683
합산되며, 다시 해당 가중치에 비례하고 각 

154
00:07:56,683 --> 00:08:00,720
뉴런이 변경되어야 하는 정도에 비례하여 합산됩니다.

155
00:08:01,600 --> 00:08:05,480
바로 여기서 역방향 전파라는 아이디어가 등장합니다.

156
00:08:05,820 --> 00:08:08,551
이러한 모든 원하는 효과를 합치면 기본적으로 

157
00:08:08,551 --> 00:08:11,065
이 두 번째 레이어에서 마지막 레이어까지 

158
00:08:11,065 --> 00:08:13,360
원하는 넛지 목록을 얻을 수 있습니다.

159
00:08:14,220 --> 00:08:16,914
이러한 값을 확보한 후에는 해당 값을 결정하는 

160
00:08:16,914 --> 00:08:19,815
관련 가중치와 편향에 동일한 프로세스를 재귀적으로 

161
00:08:19,815 --> 00:08:22,405
적용하여 방금 살펴본 것과 동일한 프로세스를 

162
00:08:22,405 --> 00:08:25,100
반복하고 네트워크를 거꾸로 이동할 수 있습니다.

163
00:08:28,960 --> 00:08:31,751
조금 더 확대해 보면, 이 모든 것이 하나의 

164
00:08:31,751 --> 00:08:34,319
트레이닝 예시가 각각의 가중치와 편향성을 

165
00:08:34,319 --> 00:08:37,000
조정하고자 하는 방식이라는 것을 기억하세요.

166
00:08:37,480 --> 00:08:39,424
2가 원하는 것만 듣는다면 네트워크는 

167
00:08:39,424 --> 00:08:41,460
궁극적으로 모든 이미지를 2로 분류하는 

168
00:08:41,460 --> 00:08:43,220
데만 인센티브를 받게 될 것입니다.

169
00:08:44,059 --> 00:08:46,684
따라서 다른 모든 훈련 예제에 대해 

170
00:08:46,684 --> 00:08:49,833
동일한 백그라운드 루틴을 수행하여 가중치와 

171
00:08:49,833 --> 00:08:52,588
편향성을 각각 어떻게 변경하고 싶은지 

172
00:08:52,588 --> 00:08:56,000
기록하고 원하는 변경 사항을 평균화하면 됩니다.

173
00:09:01,720 --> 00:09:05,753
여기서 각 가중치와 편향에 대한 평균 넛지의 집합은 

174
00:09:05,753 --> 00:09:09,786
느슨하게 말하면 지난 동영상에서 언급한 비용 함수의 

175
00:09:09,786 --> 00:09:13,680
음의 기울기, 또는 적어도 그에 비례하는 값입니다.

176
00:09:14,380 --> 00:09:17,245
아직 이러한 넛지에 대해 정량적으로 정확하게 파악하지 

177
00:09:17,245 --> 00:09:19,442
못했기 때문에 느슨하게 표현한 것이지만, 

178
00:09:19,442 --> 00:09:21,066
방금 언급한 모든 변경 사항, 

179
00:09:21,066 --> 00:09:23,836
일부 변경 사항이 다른 변경 사항보다 비례적으로 큰 

180
00:09:23,836 --> 00:09:26,319
이유, 모든 변경 사항을 합산해야 하는 이유를 

181
00:09:26,319 --> 00:09:29,089
이해했다면 역전파가 실제로 어떻게 작동하는지에 대한 

182
00:09:29,089 --> 00:09:31,000
메커니즘을 이해할 수 있을 것입니다.

183
00:09:33,960 --> 00:09:36,832
그런데 실제로는 컴퓨터가 경사도 하강 

184
00:09:36,832 --> 00:09:39,430
단계마다 모든 훈련 예제의 영향을 

185
00:09:39,430 --> 00:09:42,440
합산하는 데 매우 오랜 시간이 걸립니다.

186
00:09:43,140 --> 00:09:44,115
따라서 대신 일반적으로 수행되는 

187
00:09:44,115 --> 00:09:44,820
작업은 다음과 같습니다.

188
00:09:45,480 --> 00:09:47,915
훈련 데이터를 무작위로 섞은 다음, 

189
00:09:47,915 --> 00:09:51,324
각각 100개의 훈련 예시가 있는 여러 개의 미니 

190
00:09:51,324 --> 00:09:52,420
배치로 나눕니다.

191
00:09:52,940 --> 00:09:56,200
그런 다음 미니 배치에 따라 단계를 계산합니다.

192
00:09:56,960 --> 00:10:00,157
이 작은 하위 집합이 아니라 모든 학습 데이터에 

193
00:10:00,157 --> 00:10:02,881
따라 달라지는 비용 함수의 실제 기울기가 

194
00:10:02,881 --> 00:10:05,605
아니므로 가장 효율적인 방법은 아니지만, 

195
00:10:05,605 --> 00:10:08,566
각 미니 배치는 꽤 좋은 근사치를 제공하며, 

196
00:10:08,566 --> 00:10:12,120
더 중요한 것은 계산 속도가 크게 빨라진다는 것입니다.

197
00:10:12,820 --> 00:10:15,924
관련 비용 표면 아래 네트워크의 궤적을 그려본다면, 

198
00:10:15,924 --> 00:10:18,921
이는 마치 술에 취한 사람이 정처 없이 비틀거리며 

199
00:10:18,921 --> 00:10:21,811
언덕길을 내려가다가도 빠른 발걸음을 내딛는 것과 

200
00:10:21,811 --> 00:10:24,594
비슷하며, 신중하게 계산하여 각 단계의 정확한 

201
00:10:24,594 --> 00:10:27,484
내리막 방향을 결정하고 그 방향으로 매우 천천히 

202
00:10:27,484 --> 00:10:30,160
조심스럽게 한 걸음씩 나아가는 것과 같습니다.

203
00:10:31,540 --> 00:10:34,660
이 기법을 확률적 그라데이션 하강이라고 합니다.

204
00:10:35,960 --> 00:10:37,633
여기에는 많은 일이 일어나고 

205
00:10:37,633 --> 00:10:39,620
있으므로 간단히 요약해 보겠습니다.

206
00:10:40,440 --> 00:10:44,258
역전파는 단일 학습 예제에서 가중치와 편향의 

207
00:10:44,258 --> 00:10:47,618
상향 또는 하향 여부뿐만 아니라 이러한 

208
00:10:47,618 --> 00:10:51,436
변화에 대한 상대적 비율을 통해 비용을 가장 

209
00:10:51,436 --> 00:10:55,560
빠르게 감소시키는 방법을 결정하는 알고리즘입니다.

210
00:10:56,260 --> 00:10:58,945
진정한 그라데이션 하강 단계는 수만 개의 

211
00:10:58,945 --> 00:11:01,747
모든 훈련 예제에 대해 이 작업을 수행하고 

212
00:11:01,747 --> 00:11:04,200
원하는 변화의 평균을 구하는 것입니다.

213
00:11:04,860 --> 00:11:07,531
하지만 이는 계산 속도가 느리기 때문에 

214
00:11:07,531 --> 00:11:10,568
대신 데이터를 임의로 미니 배치로 세분화하고 

215
00:11:10,568 --> 00:11:13,240
각 단계를 미니 배치에 대해 계산합니다.

216
00:11:14,000 --> 00:11:16,634
모든 미니 배치를 반복적으로 검토하고 

217
00:11:16,634 --> 00:11:19,519
이러한 조정을 수행하면 비용 함수의 로컬 

218
00:11:19,519 --> 00:11:22,404
최소값에 수렴하게 되며, 이는 네트워크가 

219
00:11:22,404 --> 00:11:25,540
교육 예제에서 정말 잘 작동한다는 의미입니다.

220
00:11:27,240 --> 00:11:30,494
따라서 백그라운드를 구현하는 데 들어가는 

221
00:11:30,494 --> 00:11:33,324
모든 코드 라인은 적어도 비공식적인 

222
00:11:33,324 --> 00:11:36,720
용어로는 여러분이 지금 본 것과 일치합니다.

223
00:11:37,560 --> 00:11:39,689
하지만 때로는 수학의 원리를 아는 것만으로는 

224
00:11:39,689 --> 00:11:41,564
전투의 절반에 불과하며, 이를 표현하는 

225
00:11:41,564 --> 00:11:44,120
것만으로도 모든 것이 뒤죽박죽이 되고 혼란스러워집니다.

226
00:11:44,860 --> 00:11:47,191
더 자세히 알아보고 싶은 분들을 위해 다음 

227
00:11:47,191 --> 00:11:50,105
동영상에서는 방금 소개한 것과 동일한 아이디어를 기본 

228
00:11:50,105 --> 00:11:52,728
미적분학 측면에서 살펴보고, 다른 리소스에서 이 

229
00:11:52,728 --> 00:11:55,448
주제를 접한 적이 있다면 조금 더 친숙하게 이해할 

230
00:11:55,448 --> 00:11:56,420
수 있을 것입니다.

231
00:11:57,340 --> 00:12:00,226
그 전에 한 가지 강조하고 싶은 것은 이 알고리즘이 

232
00:12:00,226 --> 00:12:02,814
작동하려면 신경망뿐만 아니라 모든 종류의 머신 

233
00:12:02,814 --> 00:12:05,402
러닝에 적용되는 많은 학습 데이터가 필요하다는 

234
00:12:05,402 --> 00:12:05,900
점입니다.

235
00:12:06,420 --> 00:12:09,097
우리의 경우, 손으로 쓴 숫자가 좋은 예가 될 수 

236
00:12:09,097 --> 00:12:11,775
있는 한 가지 이유는 사람이 레이블을 붙인 수많은 

237
00:12:11,775 --> 00:12:14,166
예가 있는 MNIST 데이터베이스가 존재하기 

238
00:12:14,166 --> 00:12:14,740
때문입니다.

239
00:12:15,300 --> 00:12:18,109
따라서 머신 러닝 분야에서 일하시는 분이라면 

240
00:12:18,109 --> 00:12:21,143
수만 장의 이미지에 라벨을 붙이거나 다른 데이터 

241
00:12:21,143 --> 00:12:23,953
유형에 상관없이 실제로 필요한 라벨이 지정된 

242
00:12:23,953 --> 00:12:27,100
학습 데이터를 얻는 것이 일반적인 과제일 것입니다.

