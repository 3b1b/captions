[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn.",
  "translatedText": "",
  "from_community_srt": "Hier wird Rückpropagation behandelt, Der Kernalgorithmus hinter dem Lernen von neuralen Netzwerken.",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas.",
  "translatedText": "",
  "from_community_srt": "Nach einer kurzen Zusammenfassung, werde ich intuitiv erklären, was der Algorithmus eigentlich tut, ohne Formeln zu verwenden.",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this.",
  "translatedText": "",
  "from_community_srt": "Für die, die sich für die Mathematik interessieren, bespricht das nächste Video die zugrunde liegenden Berechnungen.",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information.",
  "translatedText": "",
  "from_community_srt": "Wenn du die letzten zwei Videos gesehen hast, oder du mit passendem Hintergrundwissen hier startest, dann weißt du, was ein neurales Netzwerk ist und wie es Information verarbeitet.",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer.",
  "translatedText": "",
  "from_community_srt": "Hier behandeln wir das klassische Beispiel hangeschriebener Ziffern, deren Pixelwerte in die erste Ebene des Netzwerks gefüttert werden, die 784 Neuronen hat. Ich habe ein Netzwerk mit zwei verborgenen Ebenen zu je 16 Neuronen verwendet, das eine Ausgabeebene mit 10 Neuronen hat, welche die gewählte Ziffer anzeigt.",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function.",
  "translatedText": "",
  "from_community_srt": "Ich gehe außerdem davon aus, dass du Gradientenabstiege verstehst, wie sie im letzten Video beandelt wurden und weißt was wir damit meinen, dass wir herausfinden wollen, welche Gewichtungen und Verzerrungen eine spezielle Kostenfunktion minimieren.",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component.",
  "translatedText": "",
  "from_community_srt": "Zur Erinnerung, für die Kosten eines einzelnen Trainingsbeispiels, Was Sie tun, ist die Ausgabe, die das Netzwerk gibt, zusammen mit der Ausgabe, die Sie geben wollten, und Sie addieren einfach die Quadrate der Unterschiede zwischen jeder Komponente.",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network.",
  "translatedText": "",
  "from_community_srt": "Tun Sie dies für all Ihre Zehntausende von Trainingsbeispielen und mitteln Sie die Ergebnisse, Dies gibt Ihnen die Gesamtkosten des Netzwerks.",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "And as if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost.",
  "translatedText": "",
  "from_community_srt": "Und als ob das nicht genug wäre, um darüber nachzudenken, wie im letzten Video beschrieben, die Sache, nach der wir suchen, ist der negative Gradient dieser Kostenfunktion, was sagt Ihnen, wie Sie alle Gewichte und Voreingenommenheiten ändern müssen, all diese Verbindungen, um die Kosten so effizient wie möglich zu senken.",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient.",
  "translatedText": "",
  "from_community_srt": "Backpropagation, das Thema dieses Videos, ist ein Algorithmus zur Berechnung dieses verrückten komplizierten Gradienten.",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "And the one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it.",
  "translatedText": "",
  "from_community_srt": "Und die eine Idee aus dem letzten Video, von der ich wirklich möchte, dass du dich fest im Kopf hältst ist das, weil das Denken des Gradientenvektors als eine Richtung in 13000 Dimensionen ist, um es leicht zu sagen, jenseits unserer Vorstellungen, Es gibt noch eine andere Möglichkeit,",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias.",
  "translatedText": "",
  "from_community_srt": "darüber nachzudenken: Die Größe jeder Komponente hier sagt dir wie sensibel die Kostenfunktion für jedes Gewicht und jede Abweichung ist.",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and you compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1.",
  "translatedText": "",
  "from_community_srt": "Nehmen wir an, Sie durchlaufen den Prozess, den ich beschreiben möchte, und Sie berechnen den negativen Gradienten, und die Komponente, die mit dem Gewicht an dieser Kante verbunden ist, kommt hier 3,2 heraus, während die mit dieser Kante verbundene Komponente hier als 0,1 herauskommt.",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give.",
  "translatedText": "",
  "from_community_srt": "Die Art, wie Sie das interpretieren würden, ist das Die Kosten der Funktion sind 32 Mal empfindlicher für Änderungen in diesem ersten Gewicht. Wenn du also diesen Wert nur ein bisschen wackeln würdest, es wird einige Änderungen an den Kosten verursachen, und diese Änderung ist 32-mal größer als das, was das gleiche Wackeln auf das zweite Gewicht geben würde.",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all.",
  "translatedText": "",
  "from_community_srt": "Als ich zum ersten Mal etwas über Backpropagation Ich denke, der verwirrendste Aspekt war nur die Notation und der Index, der alles jagte.",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other.",
  "translatedText": "",
  "from_community_srt": "Aber wenn du einmal entpackt hast, was jeder Teil dieses Algorithmus wirklich macht, jeder einzelne Effekt, den er hat, ist eigentlich ziemlich intuitiv. Es ist nur so, dass viele kleine Anpassungen übereinander geschichtet werden.",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases.",
  "translatedText": "",
  "from_community_srt": "Also fange ich hier mit einer völligen Missachtung der Notation an, und treten Sie einfach durch diese Effekte Jedes Trainingsbeispiel hat auf die Gewichte und Voreingenommenheiten.",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example.",
  "translatedText": "",
  "from_community_srt": "Weil die Kostenfunktion beinhaltet Durchschnitt von bestimmten Kosten pro Beispiel über alle Zehntausende von Trainingsbeispielen, die Art und Weise, wie wir die Gewichte und Neigungen für einen einzelnen Gradientabstieg anpassen hängt auch von jedem einzelnen Beispiel ab,",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step.",
  "translatedText": "",
  "from_community_srt": "oder eher im Prinzip sollte es, aber für die rechnerische Effizienz werden wir später einen kleinen Trick machen damit Sie nicht jedes einzelne Beispiel für jeden einzelnen Schritt lösen müssen.",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2.",
  "translatedText": "",
  "from_community_srt": "Ein anderer Fall gerade jetzt, Alles, was wir tun werden, ist unsere Aufmerksamkeit auf ein einziges Beispiel zu richten: dieses Bild eines 2.",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted?",
  "translatedText": "",
  "from_community_srt": "Welchen Effekt sollte dieses eine Trainingsbeispiel auf die Anpassung der Gewichte und Verzerrungen haben?",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on.",
  "translatedText": "",
  "from_community_srt": "Nehmen wir an, wir befinden uns an einem Punkt, an dem das Netzwerk noch nicht gut ausgebildet ist. also werden die Aktivierungen in der Ausgabe ziemlich zufällig aussehen, vielleicht etwas wie 0,5, 0,8, 0,2, weiter und weiter.",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases.",
  "translatedText": "",
  "from_community_srt": "Jetzt können wir diese Aktivierungen nicht direkt ändern, wir haben nur Einfluss auf die Gewichte und Verzerrungen, aber es ist hilfreich,",
  "n_reviews": 0,
  "start": 242.52,
  "end": 247.16
 },
 {
  "input": "But it's helpful to keep track of which adjustments we wish should take place to that output layer.",
  "translatedText": "",
  "from_community_srt": "zu verfolgen, welche Anpassungen wir für diese Ausgabeschicht vornehmen sollten.",
  "n_reviews": 0,
  "start": 247.16,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down.",
  "translatedText": "",
  "from_community_srt": "und da wir wollen, dass das Bild als 2 klassifiziert wird, wir wollen, dass der dritte Wert angestupst wird, während alle anderen gestoßen werden.",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value.",
  "translatedText": "",
  "from_community_srt": "Außerdem sollten die Größen dieser Nudges proportional zu sein wie weit entfernt jeder aktuelle Wert von seinem Zielwert entfernt ist.",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be.",
  "translatedText": "",
  "from_community_srt": "Zum Beispiel ist der Anstieg auf diese Anzahl 2 Neuronenaktivierung, in gewisser Hinsicht wichtiger als die Abnahme auf das Neuron Nummer 8, Das ist schon ziemlich nah dran wo es sein sollte.",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase.",
  "translatedText": "",
  "from_community_srt": "Also, weiter heranzoomen, konzentrieren wir uns nur auf dieses eine Neuron, derjenige, dessen Aktivierung wir erhöhen möchten.",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU.",
  "translatedText": "",
  "from_community_srt": "Denken Sie daran, dass die Aktivierung definiert ist als eine bestimmte gewichtete Summe aller Aktivierungen in der vorherigen Schicht plus einer Verzerrung, die alle in etwas wie die sigmoid Squishification-Funktion oder eine ReLU gesteckt wurde, Es gibt also drei verschiedene Wege,",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation.",
  "translatedText": "",
  "from_community_srt": "die sich zusammenschließen,",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer.",
  "translatedText": "",
  "from_community_srt": "um diese Aktivierung zu verstärken: Sie können die Verzerrung erhöhen, Sie können die Gewichte erhöhen, und Sie können die Aktivierungen von der vorherigen Ebene ändern.",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence.",
  "translatedText": "",
  "from_community_srt": "Konzentrieren Sie sich nur darauf, wie die Gewichte angepasst werden sollen, Beachten Sie,",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values.",
  "translatedText": "",
  "from_community_srt": "wie die Gewichte tatsächlich unterschiedliche Einflussniveaus haben: die Verbindungen mit den hellsten Neuronen aus der vorhergehenden Schicht haben den größten Effekt, da diese Gewichte mit größeren Aktivierungswerten multipliziert werden.",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned.",
  "translatedText": "",
  "from_community_srt": "Wenn Sie also eines dieser Gewichte erhöhen würden, es hat tatsächlich einen stärkeren Einfluss auf die ultimative Kostenfunktion als die Gewichte von Verbindungen mit Dimmerneuronen zu erhöhen, zumindest was dieses eine Trainingsbeispiel betrifft.",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck.",
  "translatedText": "",
  "from_community_srt": "Denken Sie daran, wenn wir über Gradientenabstieg sprachen, Wir kümmern uns nicht nur darum, ob jede Komponente nach oben oder unten geschubst wird, wir kümmern uns darum, welche Ihnen am meisten für Ihr Geld geben.",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together.",
  "translatedText": "",
  "from_community_srt": "Dies erinnert übrigens zumindest etwas an eine neurowissenschaftliche Theorie wie biologische Netzwerke von Neuronen lernen Hebbianische Theorie - oft zusammengefasst in der Phrase \"Neuronen, die zusammen Draht feuern\".",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active.",
  "translatedText": "",
  "from_community_srt": "Hier sind die größten Zunahmen zu Gewichten, die größte Stärkung der Verbindungen, passiert zwischen Neuronen, die am aktivsten sind, und diejenigen, die wir aktiver werden wollen.",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those are the ones firing when thinking about a 2.",
  "translatedText": "",
  "from_community_srt": "In gewissem Sinne sind die Neuronen, die feuern, während sie eine 2 sehen, werden stärker mit denen verbunden, die schießen, wenn sie an eine 2 denken.",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note.",
  "translatedText": "",
  "from_community_srt": "Um es klar zu sagen, ich bin wirklich nicht in der Lage, auf die eine oder andere Weise etwas zu sagen darüber, ob künstliche Netzwerke von Neuronen sich wie biologische Gehirne verhalten, und diese Feuer-zusammen-Draht-zusammen-Idee kommt mit ein paar sinnvollen Sternchen. Aber als sehr lockere Analogie finde ich es interessant zu bemerken.",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer.",
  "translatedText": "",
  "from_community_srt": "Wie auch immer, der dritte Weg, wie wir dazu beitragen können, die Aktivierung dieses Neurons zu erhöhen Durch Ändern aller Aktivierungen in der vorherigen Ebene",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active.",
  "translatedText": "",
  "from_community_srt": "wenn nämlich alles, was mit dem Neuron Nummer 2 mit einem positiven Gewicht verbunden war, heller wurde, und wenn alles, was mit einem negativen Gewicht verbunden ist, schwächer wurde, dann würde das Neuron Nummer 2 aktiver werden.",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights.",
  "translatedText": "",
  "from_community_srt": "Und ähnlich wie bei der Gewichtsveränderung wirst du den meisten Knall für dein Geld bekommen indem Sie Änderungen suchen, die proportional zur Größe der entsprechenden Gewichte sind.",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases.",
  "translatedText": "",
  "from_community_srt": "Nun können wir diese Aktivierungen natürlich nicht direkt beeinflussen, Wir haben nur Kontrolle über die Gewichte und Voreingenommenheiten.",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are.",
  "translatedText": "",
  "from_community_srt": "Aber genauso wie bei der letzten Ebene ist es hilfreich, nur die gewünschten Änderungen zu notieren.",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants.",
  "translatedText": "",
  "from_community_srt": "Aber denken Sie daran, wenn Sie hier einen Schritt herauszoomen, das ist nur das, was das Neuron mit der Ziffer 2 will.",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer.",
  "translatedText": "",
  "from_community_srt": "Denken Sie daran, wir wollen auch, dass alle anderen Neuronen in der letzten Schicht weniger aktiv werden, und jedes dieser anderen Ausgangsneuronen hat seine eigenen Gedanken darüber, was mit dieser vorletzten Schicht passieren soll.",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So, the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change.",
  "translatedText": "",
  "from_community_srt": "Also, der Wunsch dieses Digit 2 Neuron wird zusammen mit den Wünschen aller anderen Ausgangsneuronen addiert was mit dieser vorletzten Schicht passieren soll. Wiederum im Verhältnis zu den entsprechenden Gewichten, und im Verhältnis dazu, wie viel jedes dieser Neuronen ändern muss.",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in.",
  "translatedText": "",
  "from_community_srt": "Genau hier kommt die Idee der Rückwärtsverbreitung ins Spiel.",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer.",
  "translatedText": "",
  "from_community_srt": "Indem man all diese gewünschten Effekte zusammenfügt, Sie erhalten im Prinzip eine Liste von Stupsern, die Sie mit der vorletzten Ebene erreichen möchten.",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network.",
  "translatedText": "",
  "from_community_srt": "Und wenn du diese hast, Sie können den gleichen Prozess rekursiv anwenden zu den relevanten Gewichten und Verzerrungen, die diese Werte bestimmen, Ich wiederhole denselben Prozess und gehe gerade rückwärts durch das Netzwerk.",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases.",
  "translatedText": "",
  "from_community_srt": "Und etwas weiter herauszoomen, Erinnere dich, dass das alles gerecht ist wie ein einzelnes Trainingsbeispiel jede dieser Gewichte und Neigungen anstoßen möchte.",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2.",
  "translatedText": "",
  "from_community_srt": "Wenn wir nur hören, was das 2 wollte, Das Netzwerk würde letztendlich einen Anreiz erhalten, alle Bilder als 2 einzustufen.",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes.",
  "translatedText": "",
  "from_community_srt": "Also, was Sie tun, ist, dass Sie für jedes andere Trainingsbeispiel dieselbe Backprop-Routine durchlaufen. Aufzeichnung, wie jeder von ihnen die Gewichte und die Neigungen ändern möchte, und Sie gemittelt zusammen diese gewünschten Änderungen.",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it.",
  "translatedText": "",
  "from_community_srt": "Diese Sammlung hier der gemittelten Nudges zu jedem Gewicht und Bias ist, lockerer gesagt, der negative Gradient der Kostenfunktion, die im letzten Video referenziert wurde, oder zumindest etwas proportional dazu.",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing.",
  "translatedText": "",
  "from_community_srt": "Ich sage \"locker gesagt\", nur weil ich über diese Stöße noch quantitativ genau zu sein brauche. Aber wenn du jede Veränderung verstanden hast, die ich gerade angesprochen habe, warum einige proportional größer sind als andere, und wie sie alle zusammen addiert werden müssen, Sie verstehen die Mechanismen für die tatsächliche Backpropagation.",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step.",
  "translatedText": "",
  "from_community_srt": "Übrigens, in der Praxis dauert es sehr lange, bis der Computer fertig ist um den Einfluss jedes einzelnen Trainingsbeispiels, jeden einzelnen Gradientenabstiegsschritts zu addieren.",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead.",
  "translatedText": "",
  "from_community_srt": "Also, hier ist,",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and then divide it into a whole bunch of mini-batches, let's say each one having 100 training examples.",
  "translatedText": "",
  "from_community_srt": "was normalerweise getan wird: Sie mischen zufällig Ihre Trainingsdaten und teilen sie dann in eine ganze Reihe von Mini-Chargen auf, Sagen wir mal, jeder hat 100 Trainingsbeispiele.",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch.",
  "translatedText": "",
  "from_community_srt": "Dann berechnen Sie einen Schritt entsprechend dem Mini-Batch.",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not going to be the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speedup.",
  "translatedText": "",
  "from_community_srt": "Es wird nicht der tatsächliche Gradient der Kostenfunktion sein, Das hängt von allen Trainingsdaten ab, nicht von dieser kleinen Teilmenge. Es ist also nicht der effizienteste Schritt bergab. Aber jede Minibatch gibt Ihnen eine ziemlich gute Annäherung, und, noch wichtiger, es gibt Ihnen eine erhebliche Rechengeschwindigkeit.",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction.",
  "translatedText": "",
  "from_community_srt": "Wenn Sie die Flugbahn Ihres Netzwerks unter der relevanten Kostenoberfläche darstellen würden, es wäre ein wenig mehr wie ein Betrunkener, der ziellos über einen Hügel stolpert, aber schnelle Schritte unternimmt; eher als ein sorgfältig berechnender Mann, der die genaue Abwärtsrichtung jedes Schrittes bestimmt bevor Sie einen sehr langsamen und sorgfältigen Schritt in diese Richtung machen.",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent.",
  "translatedText": "",
  "from_community_srt": "Diese Technik wird als \"stochastischer Gradientenabstieg\" bezeichnet.",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we?",
  "translatedText": "",
  "from_community_srt": "Da passiert eine Menge, also fassen wir es einfach für uns zusammen,",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost.",
  "translatedText": "",
  "from_community_srt": "oder? Backpropagation ist der Algorithmus um zu bestimmen, wie ein einzelnes Trainingsbeispiel die Gewichte und Neigungen anstoßen möchte, nicht nur in Bezug darauf, ob sie nach oben oder unten gehen sollten, aber in Bezug auf die relativen Anteile zu diesen Veränderungen verursacht die schnellste Abnahme der Kosten.",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens of thousands of training examples and averaging the desired changes you get.",
  "translatedText": "",
  "from_community_srt": "Ein echter Gradientabstieg Das würde bedeuten, dies für all Ihre Zehntausende von Trainingsbeispielen zu tun und mitteln Sie die gewünschten Änderungen, die Sie erhalten.",
  "n_reviews": 0,
  "start": 656.26,
  "end": 664.2
 },
 {
  "input": "But that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch.",
  "translatedText": "",
  "from_community_srt": "Aber das ist rechnerisch langsam. Stattdessen unterteilen Sie die Daten zufällig in diese Mini-Chargen und Berechnen jedes Schrittes in Bezug auf einen Minibatch.",
  "n_reviews": 0,
  "start": 664.86,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all of the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples.",
  "translatedText": "",
  "from_community_srt": "Wiederholt alle Mini-Chargen durchlaufen und diese Anpassungen vornehmen, Sie werden auf ein lokales Minimum der Kostenfunktion konvergieren, Das heißt, Ihr Netzwerk wird am Ende eine wirklich gute Arbeit an den Trainingsbeispielen leisten.",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms.",
  "translatedText": "",
  "from_community_srt": "Also mit all dem gesagt, jede Codezeile, die in die Implementierung von Backprop einfließen würde entspricht tatsächlich etwas, was Sie jetzt gesehen haben, zumindest informell.",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing.",
  "translatedText": "",
  "from_community_srt": "Aber manchmal zu wissen, was die Mathematik macht, ist nur die halbe Miete, und nur das verdammte Ding zu repräsentieren ist, wo es alles verwirrt und verwirrend ist.",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources.",
  "translatedText": "",
  "from_community_srt": "Also für diejenigen von euch, die tiefer gehen wollen, Das nächste Video geht durch die gleichen Ideen, die hier vorgestellt wurden aber in Bezug auf den zugrunde liegenden Kalkül, Das sollte hoffentlich ein wenig vertrauter werden, wenn Sie das Thema in anderen Quellen sehen.",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data.",
  "translatedText": "",
  "from_community_srt": "Davor ist eines hervorzuheben für diesen Algorithmus zu arbeiten, und dies gilt für alle Arten von maschinellem Lernen über nur neuronale Netze hinaus, Sie benötigen eine Menge Trainingsdaten.",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans.",
  "translatedText": "",
  "from_community_srt": "In unserem Fall ist eine Sache, die handschriftliche Ziffern macht, ein schönes Beispiel ist, dass es die MNIST-Datenbank gibt mit so vielen Beispielen, die von Menschen beschriftet wurden.",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with.",
  "translatedText": "",
  "from_community_srt": "Eine gemeinsame Herausforderung, die diejenigen von Ihnen, die im maschinellen Lernen arbeiten, kennen erhält nur die etikettierten Trainingsdaten, die du tatsächlich brauchst, Ob die Leute Zehntausende von Bildern beschriften sollen oder welchen anderen Datentyp Sie auch haben mögen.",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]