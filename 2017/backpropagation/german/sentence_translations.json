[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn.",
  "translatedText": "Hier befassen wir uns mit Backpropagation, dem zentralen Algorithmus für das Lernen neuronaler Netze.",
  "model": "DeepL",
  "from_community_srt": "Hier wird Rückpropagation behandelt, Der Kernalgorithmus hinter dem Lernen von neuralen Netzwerken.",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas.",
  "translatedText": "Nach einer kurzen Zusammenfassung, wo wir uns befinden, werde ich zunächst intuitiv erklären, was der Algorithmus tatsächlich tut, ohne auf die Formeln einzugehen.",
  "model": "DeepL",
  "from_community_srt": "Nach einer kurzen Zusammenfassung, werde ich intuitiv erklären, was der Algorithmus eigentlich tut, ohne Formeln zu verwenden.",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this.",
  "translatedText": "Für diejenigen unter euch, die in die Mathematik eintauchen wollen, geht das nächste Video auf die Berechnungen ein, die all dem zugrunde liegen.",
  "model": "DeepL",
  "from_community_srt": "Für die, die sich für die Mathematik interessieren, bespricht das nächste Video die zugrunde liegenden Berechnungen.",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information.",
  "translatedText": "Wenn du die letzten beiden Videos gesehen hast oder gerade mit dem entsprechenden Hintergrundwissen einsteigst, weißt du, was ein neuronales Netz ist und wie es Informationen weiterleitet.",
  "model": "DeepL",
  "from_community_srt": "Wenn du die letzten zwei Videos gesehen hast, oder du mit passendem Hintergrundwissen hier startest, dann weißt du, was ein neurales Netzwerk ist und wie es Information verarbeitet.",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer.",
  "translatedText": "Hier machen wir das klassische Beispiel der Erkennung handgeschriebener Ziffern, deren Pixelwerte in die erste Schicht des Netzes mit 784 Neuronen eingespeist werden. Ich habe ein Netz mit zwei versteckten Schichten mit jeweils nur 16 Neuronen und einer Ausgabeschicht mit 10 Neuronen gezeigt, die anzeigt, welche Ziffer das Netz als Antwort wählt.",
  "model": "DeepL",
  "from_community_srt": "Hier behandeln wir das klassische Beispiel hangeschriebener Ziffern, deren Pixelwerte in die erste Ebene des Netzwerks gefüttert werden, die 784 Neuronen hat. Ich habe ein Netzwerk mit zwei verborgenen Ebenen zu je 16 Neuronen verwendet, das eine Ausgabeebene mit 10 Neuronen hat, welche die gewählte Ziffer anzeigt.",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function.",
  "translatedText": "Ich erwarte auch, dass du den Gradientenabstieg verstehst, wie er im letzten Video beschrieben wurde, und dass wir mit Lernen meinen, dass wir herausfinden wollen, welche Gewichte und Verzerrungen eine bestimmte Kostenfunktion minimieren.",
  "model": "DeepL",
  "from_community_srt": "Ich gehe außerdem davon aus, dass du Gradientenabstiege verstehst, wie sie im letzten Video beandelt wurden und weißt was wir damit meinen, dass wir herausfinden wollen, welche Gewichtungen und Verzerrungen eine spezielle Kostenfunktion minimieren.",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component.",
  "translatedText": "Zur Erinnerung: Für die Kosten eines einzigen Trainingsbeispiels nimmst du die Ausgabe, die das Netz liefert, und die Ausgabe, die es liefern soll, und addierst die Quadrate der Differenzen zwischen den einzelnen Komponenten.",
  "model": "DeepL",
  "from_community_srt": "Zur Erinnerung, für die Kosten eines einzelnen Trainingsbeispiels, Was Sie tun, ist die Ausgabe, die das Netzwerk gibt, zusammen mit der Ausgabe, die Sie geben wollten, und Sie addieren einfach die Quadrate der Unterschiede zwischen jeder Komponente.",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network.",
  "translatedText": "Wenn du das für alle deine zehntausend Trainingsbeispiele machst und die Ergebnisse mittelst, erhältst du die Gesamtkosten des Netzwerks.",
  "model": "DeepL",
  "from_community_srt": "Tun Sie dies für all Ihre Zehntausende von Trainingsbeispielen und mitteln Sie die Ergebnisse, Dies gibt Ihnen die Gesamtkosten des Netzwerks.",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "And as if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost.",
  "translatedText": "Und als ob das noch nicht genug wäre, suchen wir, wie im letzten Video beschrieben, nach der negativen Steigung dieser Kostenfunktion, die dir sagt, wie du die Gewichte und Vorspannungen, also alle Verbindungen, verändern musst, um die Kosten möglichst effizient zu senken.",
  "model": "DeepL",
  "from_community_srt": "Und als ob das nicht genug wäre, um darüber nachzudenken, wie im letzten Video beschrieben, die Sache, nach der wir suchen, ist der negative Gradient dieser Kostenfunktion, was sagt Ihnen, wie Sie alle Gewichte und Voreingenommenheiten ändern müssen, all diese Verbindungen, um die Kosten so effizient wie möglich zu senken.",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient.",
  "translatedText": "Die Backpropagation, um die es in diesem Video geht, ist ein Algorithmus zur Berechnung dieses verrückten und komplizierten Gradienten.",
  "model": "DeepL",
  "from_community_srt": "Backpropagation, das Thema dieses Videos, ist ein Algorithmus zur Berechnung dieses verrückten komplizierten Gradienten.",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "And the one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it.",
  "translatedText": "Die Idee aus dem letzten Video, die du dir unbedingt merken solltest, ist, dass es eine andere Möglichkeit gibt, den Gradientenvektor als eine Richtung in 13.000 Dimensionen zu betrachten, die, um es vorsichtig auszudrücken, unsere Vorstellungskraft übersteigt.",
  "model": "DeepL",
  "from_community_srt": "Und die eine Idee aus dem letzten Video, von der ich wirklich möchte, dass du dich fest im Kopf hältst ist das, weil das Denken des Gradientenvektors als eine Richtung in 13000 Dimensionen ist, um es leicht zu sagen, jenseits unserer Vorstellungen, Es gibt noch eine andere Möglichkeit,",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias.",
  "translatedText": "Die Größe der einzelnen Komponenten zeigt dir, wie empfindlich die Kostenfunktion auf die einzelnen Gewichte und Verzerrungen reagiert.",
  "model": "DeepL",
  "from_community_srt": "darüber nachzudenken: Die Größe jeder Komponente hier sagt dir wie sensibel die Kostenfunktion für jedes Gewicht und jede Abweichung ist.",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and you compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1.",
  "translatedText": "Angenommen, du gehst den Prozess durch, den ich gleich beschreiben werde, und berechnest den negativen Gradienten. Die Komponente, die mit dem Gewicht dieser Kante hier verbunden ist, beträgt 3,2, während die Komponente, die mit dieser Kante hier verbunden ist, 0,1 beträgt.",
  "model": "DeepL",
  "from_community_srt": "Nehmen wir an, Sie durchlaufen den Prozess, den ich beschreiben möchte, und Sie berechnen den negativen Gradienten, und die Komponente, die mit dem Gewicht an dieser Kante verbunden ist, kommt hier 3,2 heraus, während die mit dieser Kante verbundene Komponente hier als 0,1 herauskommt.",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give.",
  "translatedText": "Du kannst das so interpretieren, dass die Kosten der Funktion 32-mal empfindlicher auf Änderungen der ersten Gewichtung reagieren. Wenn du also diesen Wert nur ein wenig veränderst, führt das zu einer Änderung der Kosten, und diese Änderung ist 32-mal größer als die, die dieselbe Änderung der zweiten Gewichtung bewirken würde.",
  "model": "DeepL",
  "from_community_srt": "Die Art, wie Sie das interpretieren würden, ist das Die Kosten der Funktion sind 32 Mal empfindlicher für Änderungen in diesem ersten Gewicht. Wenn du also diesen Wert nur ein bisschen wackeln würdest, es wird einige Änderungen an den Kosten verursachen, und diese Änderung ist 32-mal größer als das, was das gleiche Wackeln auf das zweite Gewicht geben würde.",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all.",
  "translatedText": "Als ich zum ersten Mal etwas über Backpropagation gelernt habe, war für mich der verwirrendste Aspekt die Notation und die Indexverfolgung, die sich dahinter verbirgt.",
  "model": "DeepL",
  "from_community_srt": "Als ich zum ersten Mal etwas über Backpropagation Ich denke, der verwirrendste Aspekt war nur die Notation und der Index, der alles jagte.",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other.",
  "translatedText": "Aber wenn du erst einmal herausgefunden hast, was die einzelnen Teile dieses Algorithmus wirklich tun, ist jeder einzelne Effekt eigentlich ziemlich intuitiv, es gibt nur viele kleine Anpassungen, die übereinander geschichtet werden.",
  "model": "DeepL",
  "from_community_srt": "Aber wenn du einmal entpackt hast, was jeder Teil dieses Algorithmus wirklich macht, jeder einzelne Effekt, den er hat, ist eigentlich ziemlich intuitiv. Es ist nur so, dass viele kleine Anpassungen übereinander geschichtet werden.",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases.",
  "translatedText": "Ich fange also ohne Rücksicht auf die Notation an und gehe einfach die Auswirkungen jedes Trainingsbeispiels auf die Gewichte und Verzerrungen durch.",
  "model": "DeepL",
  "from_community_srt": "Also fange ich hier mit einer völligen Missachtung der Notation an, und treten Sie einfach durch diese Effekte Jedes Trainingsbeispiel hat auf die Gewichte und Voreingenommenheiten.",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example.",
  "translatedText": "Da die Kostenfunktion einen Durchschnittswert pro Beispiel über alle Zehntausende von Trainingsbeispielen bildet, hängt die Art und Weise, wie wir die Gewichte und Verzerrungen für einen einzelnen Gradientenabstiegsschritt anpassen, auch von jedem einzelnen Beispiel ab.",
  "model": "DeepL",
  "from_community_srt": "Weil die Kostenfunktion beinhaltet Durchschnitt von bestimmten Kosten pro Beispiel über alle Zehntausende von Trainingsbeispielen, die Art und Weise, wie wir die Gewichte und Neigungen für einen einzelnen Gradientabstieg anpassen hängt auch von jedem einzelnen Beispiel ab,",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step.",
  "translatedText": "Oder besser gesagt, im Prinzip sollte es das, aber aus Gründen der Recheneffizienz werden wir später einen kleinen Trick anwenden, damit du nicht jedes einzelne Beispiel für jeden Schritt aufrufen musst.",
  "model": "DeepL",
  "from_community_srt": "oder eher im Prinzip sollte es, aber für die rechnerische Effizienz werden wir später einen kleinen Trick machen damit Sie nicht jedes einzelne Beispiel für jeden einzelnen Schritt lösen müssen.",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2.",
  "translatedText": "In anderen Fällen werden wir uns jetzt nur auf ein einziges Beispiel konzentrieren, dieses Bild einer 2.",
  "model": "DeepL",
  "from_community_srt": "Ein anderer Fall gerade jetzt, Alles, was wir tun werden, ist unsere Aufmerksamkeit auf ein einziges Beispiel zu richten: dieses Bild eines 2.",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted?",
  "translatedText": "Welche Auswirkungen sollte dieses eine Trainingsbeispiel auf die Anpassung der Gewichte und Verzerrungen haben?",
  "model": "DeepL",
  "from_community_srt": "Welchen Effekt sollte dieses eine Trainingsbeispiel auf die Anpassung der Gewichte und Verzerrungen haben?",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on.",
  "translatedText": "Nehmen wir an, wir sind an einem Punkt, an dem das Netz noch nicht gut trainiert ist, so dass die Aktivierungen in der Ausgabe ziemlich zufällig aussehen werden, vielleicht so etwas wie 0,5, 0,8, 0,2 und so weiter.",
  "model": "DeepL",
  "from_community_srt": "Nehmen wir an, wir befinden uns an einem Punkt, an dem das Netzwerk noch nicht gut ausgebildet ist. also werden die Aktivierungen in der Ausgabe ziemlich zufällig aussehen, vielleicht etwas wie 0,5, 0,8, 0,2, weiter und weiter.",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases.",
  "translatedText": "Wir können diese Aktivierungen nicht direkt ändern, wir haben nur Einfluss auf die Gewichte und Verzerrungen.",
  "model": "DeepL",
  "from_community_srt": "Jetzt können wir diese Aktivierungen nicht direkt ändern, wir haben nur Einfluss auf die Gewichte und Verzerrungen, aber es ist hilfreich,",
  "n_reviews": 0,
  "start": 242.52,
  "end": 247.16
 },
 {
  "input": "But it's helpful to keep track of which adjustments we wish should take place to that output layer.",
  "translatedText": "Aber es ist hilfreich, den Überblick darüber zu behalten, welche Anpassungen wir an dieser Ausgabeschicht vornehmen wollen.",
  "model": "DeepL",
  "from_community_srt": "zu verfolgen, welche Anpassungen wir für diese Ausgabeschicht vornehmen sollten.",
  "n_reviews": 0,
  "start": 247.16,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down.",
  "translatedText": "Und da wir wollen, dass das Bild als 2 eingestuft wird, soll der dritte Wert nach oben verschoben werden, während alle anderen nach unten verschoben werden.",
  "model": "DeepL",
  "from_community_srt": "und da wir wollen, dass das Bild als 2 klassifiziert wird, wir wollen, dass der dritte Wert angestupst wird, während alle anderen gestoßen werden.",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value.",
  "translatedText": "Außerdem sollte die Größe dieser Anstöße proportional dazu sein, wie weit der aktuelle Wert von seinem Zielwert entfernt ist.",
  "model": "DeepL",
  "from_community_srt": "Außerdem sollten die Größen dieser Nudges proportional zu sein wie weit entfernt jeder aktuelle Wert von seinem Zielwert entfernt ist.",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be.",
  "translatedText": "So ist zum Beispiel die Erhöhung der Aktivierung des Neurons Nummer 2 in gewisser Weise wichtiger als die Verringerung des Neurons Nummer 8, das bereits ziemlich nahe an dem Wert ist, den es haben sollte.",
  "model": "DeepL",
  "from_community_srt": "Zum Beispiel ist der Anstieg auf diese Anzahl 2 Neuronenaktivierung, in gewisser Hinsicht wichtiger als die Abnahme auf das Neuron Nummer 8, Das ist schon ziemlich nah dran wo es sein sollte.",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase.",
  "translatedText": "Zoomen wir also weiter hinein und konzentrieren uns nur auf dieses eine Neuron, dessen Aktivierung wir erhöhen wollen.",
  "model": "DeepL",
  "from_community_srt": "Also, weiter heranzoomen, konzentrieren wir uns nur auf dieses eine Neuron, derjenige, dessen Aktivierung wir erhöhen möchten.",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU.",
  "translatedText": "Denke daran, dass die Aktivierung als eine bestimmte gewichtete Summe aller Aktivierungen in der vorherigen Schicht plus einer Vorspannung definiert ist, die dann in eine Funktion wie die sigmoide Squishification oder eine ReLU eingesetzt wird.",
  "model": "DeepL",
  "from_community_srt": "Denken Sie daran, dass die Aktivierung definiert ist als eine bestimmte gewichtete Summe aller Aktivierungen in der vorherigen Schicht plus einer Verzerrung, die alle in etwas wie die sigmoid Squishification-Funktion oder eine ReLU gesteckt wurde, Es gibt also drei verschiedene Wege,",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation.",
  "translatedText": "Es gibt also drei verschiedene Wege, die zusammenhelfen können, um die Aktivierung zu erhöhen.",
  "model": "DeepL",
  "from_community_srt": "die sich zusammenschließen,",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer.",
  "translatedText": "Du kannst den Bias erhöhen, du kannst die Gewichte erhöhen und du kannst die Aktivierungen der vorherigen Schicht ändern.",
  "model": "DeepL",
  "from_community_srt": "um diese Aktivierung zu verstärken: Sie können die Verzerrung erhöhen, Sie können die Gewichte erhöhen, und Sie können die Aktivierungen von der vorherigen Ebene ändern.",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence.",
  "translatedText": "Wenn du dich darauf konzentrierst, wie die Gewichte angepasst werden sollten, bemerkst du, dass die Gewichte tatsächlich einen unterschiedlichen Einfluss haben.",
  "model": "DeepL",
  "from_community_srt": "Konzentrieren Sie sich nur darauf, wie die Gewichte angepasst werden sollen, Beachten Sie,",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values.",
  "translatedText": "Die Verbindungen mit den hellsten Neuronen aus der vorangegangenen Schicht haben den größten Effekt, da diese Gewichte mit größeren Aktivierungswerten multipliziert werden.",
  "model": "DeepL",
  "from_community_srt": "wie die Gewichte tatsächlich unterschiedliche Einflussniveaus haben: die Verbindungen mit den hellsten Neuronen aus der vorhergehenden Schicht haben den größten Effekt, da diese Gewichte mit größeren Aktivierungswerten multipliziert werden.",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned.",
  "translatedText": "Wenn du also eines dieser Gewichte erhöhst, hat das einen stärkeren Einfluss auf die endgültige Kostenfunktion als die Erhöhung der Gewichte von Verbindungen mit schwächeren Neuronen, zumindest was dieses eine Trainingsbeispiel betrifft.",
  "model": "DeepL",
  "from_community_srt": "Wenn Sie also eines dieser Gewichte erhöhen würden, es hat tatsächlich einen stärkeren Einfluss auf die ultimative Kostenfunktion als die Gewichte von Verbindungen mit Dimmerneuronen zu erhöhen, zumindest was dieses eine Trainingsbeispiel betrifft.",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck.",
  "translatedText": "Vergiss nicht, dass es beim Gradientenabstieg nicht nur darum geht, ob die einzelnen Komponenten nach oben oder unten verschoben werden sollen, sondern auch darum, welche Komponenten dir das meiste Geld einbringen.",
  "model": "DeepL",
  "from_community_srt": "Denken Sie daran, wenn wir über Gradientenabstieg sprachen, Wir kümmern uns nicht nur darum, ob jede Komponente nach oben oder unten geschubst wird, wir kümmern uns darum, welche Ihnen am meisten für Ihr Geld geben.",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together.",
  "translatedText": "Das erinnert übrigens ein wenig an eine Theorie aus den Neurowissenschaften, die beschreibt, wie biologische Netzwerke von Neuronen lernen: die Hebb'sche Theorie, die oft mit dem Satz zusammengefasst wird, dass Neuronen, die zusammen feuern, auch zusammen verdrahtet sind.",
  "model": "DeepL",
  "from_community_srt": "Dies erinnert übrigens zumindest etwas an eine neurowissenschaftliche Theorie wie biologische Netzwerke von Neuronen lernen Hebbianische Theorie - oft zusammengefasst in der Phrase \"Neuronen, die zusammen Draht feuern\".",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active.",
  "translatedText": "Hier findet die größte Erhöhung der Gewichte, die größte Stärkung der Verbindungen, zwischen den aktivsten Neuronen und denjenigen statt, die wir aktiver machen wollen.",
  "model": "DeepL",
  "from_community_srt": "Hier sind die größten Zunahmen zu Gewichten, die größte Stärkung der Verbindungen, passiert zwischen Neuronen, die am aktivsten sind, und diejenigen, die wir aktiver werden wollen.",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those are the ones firing when thinking about a 2.",
  "translatedText": "In gewisser Weise werden die Neuronen, die feuern, wenn du eine 2 siehst, stärker mit denen verknüpft, die feuern, wenn du an eine 2 denkst.",
  "model": "DeepL",
  "from_community_srt": "In gewissem Sinne sind die Neuronen, die feuern, während sie eine 2 sehen, werden stärker mit denen verbunden, die schießen, wenn sie an eine 2 denken.",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note.",
  "translatedText": "Um das klarzustellen: Ich bin nicht in der Lage, eine Aussage darüber zu treffen, ob sich künstliche Netzwerke von Neuronen wie biologische Gehirne verhalten, und die Idee, dass Feuer und Draht zusammengehören, ist mit ein paar bedeutsamen Sternchen versehen.",
  "model": "DeepL",
  "from_community_srt": "Um es klar zu sagen, ich bin wirklich nicht in der Lage, auf die eine oder andere Weise etwas zu sagen darüber, ob künstliche Netzwerke von Neuronen sich wie biologische Gehirne verhalten, und diese Feuer-zusammen-Draht-zusammen-Idee kommt mit ein paar sinnvollen Sternchen. Aber als sehr lockere Analogie finde ich es interessant zu bemerken.",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer.",
  "translatedText": "Die dritte Möglichkeit, die Aktivierung dieses Neurons zu erhöhen, besteht darin, alle Aktivierungen in der vorherigen Schicht zu ändern.",
  "model": "DeepL",
  "from_community_srt": "Wie auch immer, der dritte Weg, wie wir dazu beitragen können, die Aktivierung dieses Neurons zu erhöhen Durch Ändern aller Aktivierungen in der vorherigen Ebene",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active.",
  "translatedText": "Wenn nämlich alles, was mit dem Neuron der Ziffer 2 verbunden ist und ein positives Gewicht hat, heller wird, und wenn alles, was mit einem negativen Gewicht verbunden ist, dunkler wird, dann wird das Neuron der Ziffer 2 aktiver.",
  "model": "DeepL",
  "from_community_srt": "wenn nämlich alles, was mit dem Neuron Nummer 2 mit einem positiven Gewicht verbunden war, heller wurde, und wenn alles, was mit einem negativen Gewicht verbunden ist, schwächer wurde, dann würde das Neuron Nummer 2 aktiver werden.",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights.",
  "translatedText": "Ähnlich wie bei den Gewichtsveränderungen wirst du das meiste für dein Geld bekommen, wenn du Veränderungen suchst, die proportional zur Größe der entsprechenden Gewichte sind.",
  "model": "DeepL",
  "from_community_srt": "Und ähnlich wie bei der Gewichtsveränderung wirst du den meisten Knall für dein Geld bekommen indem Sie Änderungen suchen, die proportional zur Größe der entsprechenden Gewichte sind.",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases.",
  "translatedText": "Natürlich können wir diese Aktivierungen nicht direkt beeinflussen, wir haben nur die Kontrolle über die Gewichte und Verzerrungen.",
  "model": "DeepL",
  "from_community_srt": "Nun können wir diese Aktivierungen natürlich nicht direkt beeinflussen, Wir haben nur Kontrolle über die Gewichte und Voreingenommenheiten.",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are.",
  "translatedText": "Aber genau wie bei der letzten Schicht ist es hilfreich, sich zu notieren, welche Veränderungen du dir wünschst.",
  "model": "DeepL",
  "from_community_srt": "Aber genauso wie bei der letzten Ebene ist es hilfreich, nur die gewünschten Änderungen zu notieren.",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants.",
  "translatedText": "Aber vergiss nicht: Wenn du hier einen Schritt herauszoomst, ist das nur das, was das Ausgangsneuron der Ziffer 2 will.",
  "model": "DeepL",
  "from_community_srt": "Aber denken Sie daran, wenn Sie hier einen Schritt herauszoomen, das ist nur das, was das Neuron mit der Ziffer 2 will.",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer.",
  "translatedText": "Denke daran, dass wir auch wollen, dass alle anderen Neuronen in der letzten Schicht weniger aktiv werden, und jedes dieser anderen Ausgangsneuronen hat seine eigenen Gedanken darüber, was mit der vorletzten Schicht passieren soll.",
  "model": "DeepL",
  "from_community_srt": "Denken Sie daran, wir wollen auch, dass alle anderen Neuronen in der letzten Schicht weniger aktiv werden, und jedes dieser anderen Ausgangsneuronen hat seine eigenen Gedanken darüber, was mit dieser vorletzten Schicht passieren soll.",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So, the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change.",
  "translatedText": "Der Wunsch dieses Neurons der Ziffer 2 wird also mit den Wünschen aller anderen Ausgangsneuronen für die vorletzte Schicht addiert, wiederum im Verhältnis zu den entsprechenden Gewichten und im Verhältnis dazu, wie stark sich jedes dieser Neuronen ändern muss.",
  "model": "DeepL",
  "from_community_srt": "Also, der Wunsch dieses Digit 2 Neuron wird zusammen mit den Wünschen aller anderen Ausgangsneuronen addiert was mit dieser vorletzten Schicht passieren soll. Wiederum im Verhältnis zu den entsprechenden Gewichten, und im Verhältnis dazu, wie viel jedes dieser Neuronen ändern muss.",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in.",
  "translatedText": "Genau hier kommt die Idee der Rückwärtsvermehrung ins Spiel.",
  "model": "DeepL",
  "from_community_srt": "Genau hier kommt die Idee der Rückwärtsverbreitung ins Spiel.",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer.",
  "translatedText": "Wenn du all diese gewünschten Effekte zusammenzählst, erhältst du im Grunde eine Liste von Stößen, die du für diese vorletzte Schicht haben möchtest.",
  "model": "DeepL",
  "from_community_srt": "Indem man all diese gewünschten Effekte zusammenfügt, Sie erhalten im Prinzip eine Liste von Stupsern, die Sie mit der vorletzten Ebene erreichen möchten.",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network.",
  "translatedText": "Wenn du diese Werte hast, kannst du den gleichen Prozess auf die relevanten Gewichte und Verzerrungen anwenden, die diese Werte bestimmen, indem du den Prozess wiederholst, den ich gerade beschrieben habe, und dich rückwärts durch das Netzwerk bewegst.",
  "model": "DeepL",
  "from_community_srt": "Und wenn du diese hast, Sie können den gleichen Prozess rekursiv anwenden zu den relevanten Gewichten und Verzerrungen, die diese Werte bestimmen, Ich wiederhole denselben Prozess und gehe gerade rückwärts durch das Netzwerk.",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases.",
  "translatedText": "Und wenn du noch ein bisschen weiter herauszoomst, solltest du bedenken, dass dies alles nur ein einziges Trainingsbeispiel ist, mit dem du die Gewichte und Verzerrungen beeinflussen willst.",
  "model": "DeepL",
  "from_community_srt": "Und etwas weiter herauszoomen, Erinnere dich, dass das alles gerecht ist wie ein einzelnes Trainingsbeispiel jede dieser Gewichte und Neigungen anstoßen möchte.",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2.",
  "translatedText": "Wenn wir nur darauf hören würden, was die 2 will, hätte das Netzwerk einen Anreiz, alle Bilder als 2 einzustufen.",
  "model": "DeepL",
  "from_community_srt": "Wenn wir nur hören, was das 2 wollte, Das Netzwerk würde letztendlich einen Anreiz erhalten, alle Bilder als 2 einzustufen.",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes.",
  "translatedText": "Du gehst also dieselbe Backprop-Routine für jedes andere Trainingsbeispiel durch, indem du aufzeichnest, wie jeder von ihnen die Gewichte und Verzerrungen ändern möchte, und den Durchschnitt dieser gewünschten Änderungen zusammenrechnest.",
  "model": "DeepL",
  "from_community_srt": "Also, was Sie tun, ist, dass Sie für jedes andere Trainingsbeispiel dieselbe Backprop-Routine durchlaufen. Aufzeichnung, wie jeder von ihnen die Gewichte und die Neigungen ändern möchte, und Sie gemittelt zusammen diese gewünschten Änderungen.",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it.",
  "translatedText": "Die Summe der gemittelten Zuschläge für jedes Gewicht und jede Vorspannung ist, grob gesagt, die negative Steigung der Kostenfunktion, die im letzten Video erwähnt wurde, oder zumindest etwas, das proportional dazu ist.",
  "model": "DeepL",
  "from_community_srt": "Diese Sammlung hier der gemittelten Nudges zu jedem Gewicht und Bias ist, lockerer gesagt, der negative Gradient der Kostenfunktion, die im letzten Video referenziert wurde, oder zumindest etwas proportional dazu.",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing.",
  "translatedText": "Ich spreche nur grob davon, weil ich diese Anstöße noch nicht quantitativ präzisieren kann, aber wenn du jede Änderung, die ich gerade erwähnt habe, verstanden hast, warum einige proportional größer sind als andere und wie sie alle addiert werden müssen, verstehst du die Mechanik dessen, was Backpropagation tatsächlich tut.",
  "model": "DeepL",
  "from_community_srt": "Ich sage \"locker gesagt\", nur weil ich über diese Stöße noch quantitativ genau zu sein brauche. Aber wenn du jede Veränderung verstanden hast, die ich gerade angesprochen habe, warum einige proportional größer sind als andere, und wie sie alle zusammen addiert werden müssen, Sie verstehen die Mechanismen für die tatsächliche Backpropagation.",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step.",
  "translatedText": "In der Praxis brauchen Computer übrigens extrem viel Zeit, um den Einfluss jedes Trainingsbeispiels bei jedem Schritt des Gradientenabstiegs zu addieren.",
  "model": "DeepL",
  "from_community_srt": "Übrigens, in der Praxis dauert es sehr lange, bis der Computer fertig ist um den Einfluss jedes einzelnen Trainingsbeispiels, jeden einzelnen Gradientenabstiegsschritts zu addieren.",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead.",
  "translatedText": "Hier ist, was stattdessen üblicherweise gemacht wird.",
  "model": "DeepL",
  "from_community_srt": "Also, hier ist,",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and then divide it into a whole bunch of mini-batches, let's say each one having 100 training examples.",
  "translatedText": "Du mischst deine Trainingsdaten nach dem Zufallsprinzip und teilst sie dann in eine Reihe von Mini-Batches auf, von denen jeder, sagen wir, 100 Trainingsbeispiele enthält.",
  "model": "DeepL",
  "from_community_srt": "was normalerweise getan wird: Sie mischen zufällig Ihre Trainingsdaten und teilen sie dann in eine ganze Reihe von Mini-Chargen auf, Sagen wir mal, jeder hat 100 Trainingsbeispiele.",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch.",
  "translatedText": "Dann berechnest du einen Schritt entsprechend dem Mini-Batch.",
  "model": "DeepL",
  "from_community_srt": "Dann berechnen Sie einen Schritt entsprechend dem Mini-Batch.",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not going to be the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speedup.",
  "translatedText": "Es wird nicht der tatsächliche Gradient der Kostenfunktion sein, der von allen Trainingsdaten abhängt, nicht von dieser winzigen Teilmenge. Es ist also nicht der effizienteste Schritt bergab, aber jeder Mini-Batch gibt dir eine ziemlich gute Annäherung und, was noch wichtiger ist, er beschleunigt deine Berechnungen erheblich.",
  "model": "DeepL",
  "from_community_srt": "Es wird nicht der tatsächliche Gradient der Kostenfunktion sein, Das hängt von allen Trainingsdaten ab, nicht von dieser kleinen Teilmenge. Es ist also nicht der effizienteste Schritt bergab. Aber jede Minibatch gibt Ihnen eine ziemlich gute Annäherung, und, noch wichtiger, es gibt Ihnen eine erhebliche Rechengeschwindigkeit.",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction.",
  "translatedText": "Wenn du die Flugbahn deines Netzwerks unter der entsprechenden Kostenoberfläche aufzeichnen würdest, wäre es eher wie ein betrunkener Mann, der ziellos einen Hügel hinunterstolpert und dabei schnelle Schritte macht, als ein sorgfältig kalkulierender Mann, der bei jedem Schritt die genaue Abwärtsrichtung bestimmt und dann einen sehr langsamen und vorsichtigen Schritt in diese Richtung macht.",
  "model": "DeepL",
  "from_community_srt": "Wenn Sie die Flugbahn Ihres Netzwerks unter der relevanten Kostenoberfläche darstellen würden, es wäre ein wenig mehr wie ein Betrunkener, der ziellos über einen Hügel stolpert, aber schnelle Schritte unternimmt; eher als ein sorgfältig berechnender Mann, der die genaue Abwärtsrichtung jedes Schrittes bestimmt bevor Sie einen sehr langsamen und sorgfältigen Schritt in diese Richtung machen.",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent.",
  "translatedText": "Diese Technik wird als stochastischer Gradientenabstieg bezeichnet.",
  "model": "DeepL",
  "from_community_srt": "Diese Technik wird als \"stochastischer Gradientenabstieg\" bezeichnet.",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we?",
  "translatedText": "Hier ist eine Menge los, also fassen wir es einfach für uns selbst zusammen, okay?",
  "model": "DeepL",
  "from_community_srt": "Da passiert eine Menge, also fassen wir es einfach für uns zusammen,",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost.",
  "translatedText": "Backpropagation ist ein Algorithmus, der bestimmt, wie ein einzelnes Trainingsbeispiel die Gewichte und Verzerrungen verändern soll. Dabei geht es nicht nur darum, ob sie nach oben oder unten gehen sollen, sondern auch darum, welche relativen Anteile an diesen Veränderungen den schnellsten Rückgang der Kosten verursachen.",
  "model": "DeepL",
  "from_community_srt": "oder? Backpropagation ist der Algorithmus um zu bestimmen, wie ein einzelnes Trainingsbeispiel die Gewichte und Neigungen anstoßen möchte, nicht nur in Bezug darauf, ob sie nach oben oder unten gehen sollten, aber in Bezug auf die relativen Anteile zu diesen Veränderungen verursacht die schnellste Abnahme der Kosten.",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens of thousands of training examples and averaging the desired changes you get.",
  "translatedText": "Ein echter Gradientenabstieg würde bedeuten, dass du dies für alle Zehntausende von Trainingsbeispielen machst und den Durchschnitt der gewünschten Änderungen ermittelst.",
  "model": "DeepL",
  "from_community_srt": "Ein echter Gradientabstieg Das würde bedeuten, dies für all Ihre Zehntausende von Trainingsbeispielen zu tun und mitteln Sie die gewünschten Änderungen, die Sie erhalten.",
  "n_reviews": 0,
  "start": 656.26,
  "end": 664.2
 },
 {
  "input": "But that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch.",
  "translatedText": "Das ist aber sehr rechenintensiv. Stattdessen unterteilst du die Daten nach dem Zufallsprinzip in Mini-Batches und berechnest jeden Schritt in Bezug auf einen Mini-Batch.",
  "model": "DeepL",
  "from_community_srt": "Aber das ist rechnerisch langsam. Stattdessen unterteilen Sie die Daten zufällig in diese Mini-Chargen und Berechnen jedes Schrittes in Bezug auf einen Minibatch.",
  "n_reviews": 0,
  "start": 664.86,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all of the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples.",
  "translatedText": "Wenn du alle Mini-Batches wiederholt durchgehst und diese Anpassungen vornimmst, konvergierst du gegen ein lokales Minimum der Kostenfunktion, d.h. dein Netzwerk macht am Ende einen wirklich guten Job bei den Trainingsbeispielen.",
  "model": "DeepL",
  "from_community_srt": "Wiederholt alle Mini-Chargen durchlaufen und diese Anpassungen vornehmen, Sie werden auf ein lokales Minimum der Kostenfunktion konvergieren, Das heißt, Ihr Netzwerk wird am Ende eine wirklich gute Arbeit an den Trainingsbeispielen leisten.",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms.",
  "translatedText": "Damit ist gesagt, dass jede Zeile Code, die für die Implementierung von Backprop benötigt wird, etwas entspricht, das du bereits gesehen hast, zumindest inoffiziell.",
  "model": "DeepL",
  "from_community_srt": "Also mit all dem gesagt, jede Codezeile, die in die Implementierung von Backprop einfließen würde entspricht tatsächlich etwas, was Sie jetzt gesehen haben, zumindest informell.",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing.",
  "translatedText": "Aber manchmal ist das Wissen, was die Mathematik macht, nur die halbe Miete, und wenn man das verdammte Ding einfach nur darstellt, wird es ganz verwirrend.",
  "model": "DeepL",
  "from_community_srt": "Aber manchmal zu wissen, was die Mathematik macht, ist nur die halbe Miete, und nur das verdammte Ding zu repräsentieren ist, wo es alles verwirrt und verwirrend ist.",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources.",
  "translatedText": "Für diejenigen unter euch, die tiefer einsteigen wollen, geht das nächste Video auf die gleichen Ideen ein, die hier gerade vorgestellt wurden, aber in Bezug auf die zugrundeliegende Kalkulation, was das Thema hoffentlich ein wenig vertrauter macht, wenn ihr es in anderen Quellen seht.",
  "model": "DeepL",
  "from_community_srt": "Also für diejenigen von euch, die tiefer gehen wollen, Das nächste Video geht durch die gleichen Ideen, die hier vorgestellt wurden aber in Bezug auf den zugrunde liegenden Kalkül, Das sollte hoffentlich ein wenig vertrauter werden, wenn Sie das Thema in anderen Quellen sehen.",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data.",
  "translatedText": "Damit dieser Algorithmus funktioniert - und das gilt nicht nur für neuronale Netze, sondern für alle Arten des maschinellen Lernens - brauchst du eine Menge Trainingsdaten.",
  "model": "DeepL",
  "from_community_srt": "Davor ist eines hervorzuheben für diesen Algorithmus zu arbeiten, und dies gilt für alle Arten von maschinellem Lernen über nur neuronale Netze hinaus, Sie benötigen eine Menge Trainingsdaten.",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans.",
  "translatedText": "In unserem Fall sind handgeschriebene Ziffern ein gutes Beispiel, weil es die MNIST-Datenbank mit vielen Beispielen gibt, die von Menschen beschriftet worden sind.",
  "model": "DeepL",
  "from_community_srt": "In unserem Fall ist eine Sache, die handschriftliche Ziffern macht, ein schönes Beispiel ist, dass es die MNIST-Datenbank gibt mit so vielen Beispielen, die von Menschen beschriftet wurden.",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with.",
  "translatedText": "Diejenigen unter euch, die im Bereich des maschinellen Lernens arbeiten, kennen die Herausforderung, die benötigten beschrifteten Trainingsdaten zu erhalten, sei es, dass Menschen Zehntausende von Bildern beschriften müssen oder andere Datentypen, mit denen du zu tun hast.",
  "model": "DeepL",
  "from_community_srt": "Eine gemeinsame Herausforderung, die diejenigen von Ihnen, die im maschinellen Lernen arbeiten, kennen erhält nur die etikettierten Trainingsdaten, die du tatsächlich brauchst, Ob die Leute Zehntausende von Bildern beschriften sollen oder welchen anderen Datentyp Sie auch haben mögen.",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]