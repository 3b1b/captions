[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn.",
  "translatedText": "Aici, vom aborda propagarea inversă, algoritmul de bază din spatele modului de învățare a rețelelor neuronale.",
  "model": "DeepL",
  "time_range": [
   4.06,
   8.88
  ],
  "n_reviews": 0
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas.",
  "translatedText": "După o scurtă recapitulare a punctului în care ne aflăm, primul lucru pe care îl voi face este o prezentare intuitivă a ceea ce face algoritmul, fără nicio referire la formule.",
  "model": "DeepL",
  "time_range": [
   9.4,
   17.0
  ],
  "n_reviews": 0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this.",
  "translatedText": "Apoi, pentru aceia dintre voi care doresc să se scufunde în matematică, următorul videoclip prezintă calculele care stau la baza tuturor acestor lucruri.",
  "model": "DeepL",
  "time_range": [
   17.66,
   23.02
  ],
  "n_reviews": 0
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information.",
  "translatedText": "Dacă ați vizionat ultimele două videoclipuri sau dacă ați intrat în joc cu un fundal adecvat, știți ce este o rețea neuronală și cum transmite informații.",
  "model": "DeepL",
  "time_range": [
   23.82,
   31.0
  ],
  "n_reviews": 0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer.",
  "translatedText": "Aici, facem exemplul clasic de recunoaștere a cifrelor scrise de mână, ale căror valori de pixeli sunt introduse în primul strat al rețelei cu 784 de neuroni, și am prezentat o rețea cu două straturi ascunse având doar 16 neuroni fiecare și un strat de ieșire de 10 neuroni, care indică ce cifră alege rețeaua ca răspuns.",
  "model": "DeepL",
  "time_range": [
   31.68,
   49.04
  ],
  "n_reviews": 0
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function.",
  "translatedText": "Mă aștept, de asemenea, să înțelegeți coborârea gradientului, așa cum a fost descrisă în ultimul videoclip, și să înțelegeți că prin învățare dorim să găsim ce ponderi și polarizări minimizează o anumită funcție de cost.",
  "model": "DeepL",
  "time_range": [
   50.04,
   61.26
  ],
  "n_reviews": 0
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component.",
  "translatedText": "Pentru a vă reaminti rapid, pentru costul unui singur exemplu de instruire, luați ieșirea pe care o dă rețeaua, împreună cu ieșirea pe care ați dorit să o dea și adunați pătratele diferențelor dintre fiecare componentă.",
  "model": "DeepL",
  "time_range": [
   62.04,
   74.6
  ],
  "n_reviews": 0
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network.",
  "translatedText": "Dacă faceți acest lucru pentru toate zecile de mii de exemple de antrenament și faceți media rezultatelor, veți obține costul total al rețelei.",
  "model": "DeepL",
  "time_range": [
   75.38,
   82.2
  ],
  "n_reviews": 0
 },
 {
  "input": "And as if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost.",
  "translatedText": "Și, ca și cum nu ar fi de ajuns, așa cum am descris în ultimul videoclip, ceea ce căutăm este gradientul negativ al acestei funcții de cost, care ne spune cum trebuie să modificăm toate ponderile și polarizările, toate aceste conexiuni, pentru a reduce cât mai eficient costul.",
  "model": "DeepL",
  "time_range": [
   82.2,
   98.32
  ],
  "n_reviews": 0
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient.",
  "translatedText": "Backpropagation, subiectul acestui videoclip, este un algoritm pentru calcularea acestui gradient nebunesc de complicat.",
  "model": "DeepL",
  "time_range": [
   103.26,
   108.58
  ],
  "n_reviews": 0
 },
 {
  "input": "And the one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it.",
  "translatedText": "Iar ideea din ultimul filmuleț pe care aș vrea să o rețineți cu tărie în minte acum este că, deoarece gândirea vectorului de gradient ca o direcție în 13.000 de dimensiuni este, ca să spunem așa, dincolo de imaginația noastră, există un alt mod în care vă puteți gândi la el.",
  "model": "DeepL",
  "time_range": [
   109.14,
   123.58
  ],
  "n_reviews": 0
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias.",
  "translatedText": "Magnitudinea fiecărei componente de aici vă spune cât de sensibilă este funcția de cost la fiecare pondere și polarizare.",
  "model": "DeepL",
  "time_range": [
   124.6,
   130.94
  ],
  "n_reviews": 0
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and you compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1.",
  "translatedText": "De exemplu, să presupunem că treceți prin procesul pe care urmează să îl descriu și calculați gradientul negativ, iar componenta asociată cu ponderea pe această muchie de aici este de 3,2, în timp ce componenta asociată cu această muchie de aici este de 0,1.",
  "model": "DeepL",
  "time_range": [
   131.8,
   146.26
  ],
  "n_reviews": 0
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give.",
  "translatedText": "Modul în care ați putea interpreta acest lucru este că costul funcției este de 32 de ori mai sensibil la modificările primei ponderi, astfel încât, dacă ar trebui să mișcați puțin această valoare, aceasta va cauza o anumită modificare a costului, iar această modificare este de 32 de ori mai mare decât cea pe care ar produce-o aceeași mișcare a celei de-a doua ponderi.",
  "model": "DeepL",
  "time_range": [
   146.82,
   163.06
  ],
  "n_reviews": 0
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all.",
  "translatedText": "Personal, atunci când am învățat pentru prima dată despre backpropagation, cred că cel mai confuz aspect a fost doar notarea și urmărirea indexului.",
  "model": "DeepL",
  "time_range": [
   168.42,
   175.74
  ],
  "n_reviews": 0
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other.",
  "translatedText": "Dar, odată ce ai descoperit ce face fiecare parte a acestui algoritm, fiecare efect individual pe care îl are este de fapt destul de intuitiv, doar că există o mulțime de mici ajustări care se suprapun unele peste altele.",
  "model": "DeepL",
  "time_range": [
   176.22,
   186.64
  ],
  "n_reviews": 0
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases.",
  "translatedText": "Așadar, voi începe aici fără a ține cont de notație și voi trece în revistă efectele pe care fiecare exemplu de antrenament le are asupra ponderilor și a biasurilor.",
  "model": "DeepL",
  "time_range": [
   187.74,
   196.12
  ],
  "n_reviews": 0
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example.",
  "translatedText": "Deoarece funcția de cost implică medierea unui anumit cost per exemplu pe toate zecile de mii de exemple de formare, modul în care ajustăm ponderile și polarizările pentru o singură etapă de coborâre a gradientului depinde, de asemenea, de fiecare exemplu în parte.",
  "model": "DeepL",
  "time_range": [
   197.02,
   211.04
  ],
  "n_reviews": 0
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step.",
  "translatedText": "Sau, mai degrabă, în principiu ar trebui, dar pentru eficiență computațională vom face un mic truc mai târziu pentru a nu fi nevoie să accesați fiecare exemplu pentru fiecare pas.",
  "model": "DeepL",
  "time_range": [
   211.68,
   219.2
  ],
  "n_reviews": 0
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2.",
  "translatedText": "În alte cazuri, în acest moment, ne vom concentra atenția asupra unui singur exemplu, această imagine a unui 2.",
  "model": "DeepL",
  "time_range": [
   219.2,
   225.96
  ],
  "n_reviews": 0
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted?",
  "translatedText": "Ce efect ar trebui să aibă acest exemplu de instruire asupra modului în care se ajustează ponderile și prejudecățile?",
  "model": "DeepL",
  "time_range": [
   226.72,
   231.48
  ],
  "n_reviews": 0
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on.",
  "translatedText": "Să spunem că ne aflăm într-un punct în care rețeaua nu este încă bine antrenată, așa că activările din ieșire vor arăta destul de aleatoriu, poate ceva de genul 0,5, 0,8, 0,2, și așa mai departe.",
  "model": "DeepL",
  "time_range": [
   232.68,
   242.0
  ],
  "n_reviews": 0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases.",
  "translatedText": "Nu putem modifica direct aceste activări, ci doar influența ponderile și polarizările.",
  "model": "DeepL",
  "time_range": [
   242.52,
   247.16
  ],
  "n_reviews": 0
 },
 {
  "input": "But it's helpful to keep track of which adjustments we wish should take place to that output layer.",
  "translatedText": "Dar este util să ținem evidența ajustărilor pe care dorim să le efectuăm la acel strat de ieșire.",
  "model": "DeepL",
  "time_range": [
   247.16,
   252.58
  ],
  "n_reviews": 0
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down.",
  "translatedText": "Și, din moment ce dorim să clasificăm imaginea ca fiind 2, dorim ca această a treia valoare să fie ridicată, în timp ce toate celelalte sunt coborâte.",
  "model": "DeepL",
  "time_range": [
   253.36,
   261.26
  ],
  "n_reviews": 0
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value.",
  "translatedText": "În plus, mărimea acestor impulsuri ar trebui să fie proporțională cu distanța la care se află fiecare valoare curentă față de valoarea țintă.",
  "model": "DeepL",
  "time_range": [
   262.06,
   269.52
  ],
  "n_reviews": 0
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be.",
  "translatedText": "De exemplu, creșterea activării neuronului numărul 2 este, într-un fel, mai importantă decât scăderea activării neuronului numărul 8, care este deja destul de aproape de nivelul în care ar trebui să fie.",
  "model": "DeepL",
  "time_range": [
   270.22,
   280.9
  ],
  "n_reviews": 0
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase.",
  "translatedText": "Deci, dacă mărim mai mult, să ne concentrăm doar pe acest neuron, cel a cărui activare dorim să o creștem.",
  "model": "DeepL",
  "time_range": [
   282.04,
   287.28
  ],
  "n_reviews": 0
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU.",
  "translatedText": "Amintiți-vă că activarea este definită ca o anumită sumă ponderată a tuturor activărilor din stratul anterior, plus o polarizare, care este apoi introdusă în ceva de genul funcției de squishificare sigmoidă sau ReLU.",
  "model": "DeepL",
  "time_range": [
   288.18,
   301.04
  ],
  "n_reviews": 0
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation.",
  "translatedText": "Așadar, există trei căi diferite care pot fi folosite împreună pentru a contribui la creșterea activării.",
  "model": "DeepL",
  "time_range": [
   301.64,
   307.02
  ],
  "n_reviews": 0
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer.",
  "translatedText": "Puteți crește polarizarea, puteți crește ponderile și puteți modifica activările din stratul anterior.",
  "model": "DeepL",
  "time_range": [
   307.44,
   314.04
  ],
  "n_reviews": 0
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence.",
  "translatedText": "Concentrându-ne asupra modului în care ar trebui ajustate ponderile, observați cum ponderile au de fapt niveluri diferite de influență.",
  "model": "DeepL",
  "time_range": [
   314.94,
   320.86
  ],
  "n_reviews": 0
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values.",
  "translatedText": "Conexiunile cu cei mai luminoși neuroni din stratul precedent au cel mai mare efect, deoarece aceste ponderi sunt înmulțite cu valori de activare mai mari.",
  "model": "DeepL",
  "time_range": [
   321.44,
   329.1
  ],
  "n_reviews": 0
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned.",
  "translatedText": "Deci, dacă ar fi să măriți una dintre aceste ponderi, aceasta are de fapt o influență mai puternică asupra funcției finale de cost decât creșterea ponderilor conexiunilor cu neuroni mai slabi, cel puțin în ceea ce privește acest exemplu de antrenament.",
  "model": "DeepL",
  "time_range": [
   331.46,
   343.48
  ],
  "n_reviews": 0
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck.",
  "translatedText": "Nu uitați că, atunci când vorbim despre coborârea gradientului, nu ne interesează doar dacă fiecare componentă ar trebui să crească sau să scadă, ci ne interesează care dintre ele vă oferă cel mai bun raport calitate-preț.",
  "model": "DeepL",
  "time_range": [
   344.42,
   353.22
  ],
  "n_reviews": 0
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together.",
  "translatedText": "Apropo, acest lucru amintește, cel puțin într-o oarecare măsură, de o teorie din domeniul neuroștiințelor privind modul în care rețelele biologice de neuroni învață, teoria Hebbian, adesea rezumată în expresia: \"Neuronii care trag împreună se conectează împreună\".",
  "model": "DeepL",
  "time_range": [
   355.02,
   366.46
  ],
  "n_reviews": 0
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active.",
  "translatedText": "În acest caz, cele mai mari creșteri ale greutăților, cele mai mari consolidări ale conexiunilor, au loc între neuronii care sunt cei mai activi și cei pe care dorim să devină mai activi.",
  "model": "DeepL",
  "time_range": [
   367.26,
   377.28
  ],
  "n_reviews": 0
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those are the ones firing when thinking about a 2.",
  "translatedText": "Într-un anumit sens, neuronii care se activează în timp ce văd un 2 sunt mai puternic legați de cei care se activează atunci când se gândesc la un 2.",
  "model": "DeepL",
  "time_range": [
   377.94,
   384.48
  ],
  "n_reviews": 0
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note.",
  "translatedText": "Pentru a fi clar, nu sunt în măsură să fac declarații într-un fel sau altul cu privire la faptul că rețelele artificiale de neuroni se comportă în vreun fel asemănător cu creierele biologice, iar această idee de \"fire together wire together\" vine cu câteva asteriscuri semnificative, dar, luată ca o analogie foarte liberă, mi se pare interesant de remarcat.",
  "model": "DeepL",
  "time_range": [
   385.4,
   401.02
  ],
  "n_reviews": 0
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer.",
  "translatedText": "În orice caz, al treilea mod în care putem contribui la creșterea activării acestui neuron este prin modificarea tuturor activărilor din stratul anterior.",
  "model": "DeepL",
  "time_range": [
   401.94,
   409.04
  ],
  "n_reviews": 0
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active.",
  "translatedText": "Mai exact, dacă tot ceea ce este conectat la acel neuron de cifră 2 cu o pondere pozitivă devine mai luminos, iar tot ceea ce este conectat cu o pondere negativă devine mai slab, atunci acel neuron de cifră 2 va deveni mai activ.",
  "model": "DeepL",
  "time_range": [
   409.04,
   420.68
  ],
  "n_reviews": 0
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights.",
  "translatedText": "Și, la fel ca în cazul modificărilor de greutate, veți obține cel mai bun rezultat dacă veți căuta modificări proporționale cu mărimea greutăților corespunzătoare.",
  "model": "DeepL",
  "time_range": [
   422.54,
   430.28
  ],
  "n_reviews": 0
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases.",
  "translatedText": "Desigur, nu putem influența în mod direct aceste activări, avem control doar asupra ponderilor și a polarizărilor.",
  "model": "DeepL",
  "time_range": [
   432.14,
   437.48
  ],
  "n_reviews": 0
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are.",
  "translatedText": "Dar, la fel ca în cazul ultimului strat, este util să notați care sunt modificările dorite.",
  "model": "DeepL",
  "time_range": [
   437.48,
   444.12
  ],
  "n_reviews": 0
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants.",
  "translatedText": "Dar rețineți că, dacă mărim cu un pas aici, aceasta este doar ceea ce dorește neuronii de ieșire a cifrei 2.",
  "model": "DeepL",
  "time_range": [
   444.58,
   449.2
  ],
  "n_reviews": 0
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer.",
  "translatedText": "Nu uitați că dorim ca toți ceilalți neuroni din ultimul strat să devină mai puțin activi, iar fiecare dintre acești neuroni de ieșire are propriile gânduri cu privire la ceea ce ar trebui să se întâmple cu penultimul strat.",
  "model": "DeepL",
  "time_range": [
   449.76,
   459.6
  ],
  "n_reviews": 0
 },
 {
  "input": "So, the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change.",
  "translatedText": "Așadar, dorința acestui neuron de cifră 2 este adăugată împreună cu dorințele tuturor celorlalți neuroni de ieșire pentru ceea ce ar trebui să se întâmple în acest penultim strat, din nou proporțional cu ponderile corespunzătoare și proporțional cu cât de mult trebuie să se schimbe fiecare dintre acești neuroni.",
  "model": "DeepL",
  "time_range": [
   462.7,
   480.72
  ],
  "n_reviews": 0
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in.",
  "translatedText": "Aici intervine ideea de propagare inversă.",
  "model": "DeepL",
  "time_range": [
   481.6,
   485.48
  ],
  "n_reviews": 0
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer.",
  "translatedText": "Adunând toate aceste efecte dorite, obțineți practic o listă de impulsuri pe care doriți să le obțineți pentru acest penultim strat.",
  "model": "DeepL",
  "time_range": [
   485.82,
   493.36
  ],
  "n_reviews": 0
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network.",
  "translatedText": "Și odată ce le aveți, puteți aplica recursiv același proces la ponderile și polarizările relevante care determină aceste valori, repetând același proces pe care tocmai l-am parcurs și mergând înapoi în rețea.",
  "model": "DeepL",
  "time_range": [
   494.22,
   505.1
  ],
  "n_reviews": 0
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases.",
  "translatedText": "Și dacă mărim puțin mai mult, nu uitați că toate acestea sunt doar modul în care un singur exemplu de instruire dorește să influențeze fiecare dintre aceste ponderi și prejudecăți.",
  "model": "DeepL",
  "time_range": [
   508.96,
   517.0
  ],
  "n_reviews": 0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2.",
  "translatedText": "Dacă am asculta doar ceea ce dorește acel 2, rețeaua ar fi în cele din urmă stimulată doar să clasifice toate imaginile ca fiind 2.",
  "model": "DeepL",
  "time_range": [
   517.48,
   523.22
  ],
  "n_reviews": 0
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes.",
  "translatedText": "Deci, ceea ce trebuie să faceți este să treceți prin aceeași rutină de backprop pentru fiecare alt exemplu de antrenament, înregistrând modul în care fiecare dintre ei ar dori să modifice ponderile și polarizările și să faceți o medie a acestor modificări dorite.",
  "model": "DeepL",
  "time_range": [
   524.06,
   536.0
  ],
  "n_reviews": 0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it.",
  "translatedText": "Această colecție de aici, care reprezintă media impulsurilor pentru fiecare greutate și bias, reprezintă, în linii mari, gradientul negativ al funcției de cost la care s-a făcut referire în ultimul videoclip, sau cel puțin ceva proporțional cu acesta.",
  "model": "DeepL",
  "time_range": [
   541.72,
   553.68
  ],
  "n_reviews": 0
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing.",
  "translatedText": "Spun \"în sens larg\" doar pentru că încă nu am reușit să fiu mai precis din punct de vedere cantitativ în legătură cu aceste impulsuri, dar dacă ați înțeles fiecare schimbare la care tocmai am făcut referire, de ce unele sunt proporțional mai mari decât altele și cum trebuie să fie adăugate toate împreună, ați înțeles mecanismul pentru ceea ce face de fapt backpropagation.",
  "model": "DeepL",
  "time_range": [
   554.38,
   571.0
  ],
  "n_reviews": 0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step.",
  "translatedText": "Apropo, în practică, calculatoarele au nevoie de foarte mult timp pentru a adăuga influența fiecărui exemplu de formare la fiecare pas de coborâre a gradientului.",
  "model": "DeepL",
  "time_range": [
   573.96,
   582.44
  ],
  "n_reviews": 0
 },
 {
  "input": "So here's what's commonly done instead.",
  "translatedText": "Așa că iată ce se face în mod obișnuit în loc.",
  "model": "DeepL",
  "time_range": [
   583.14,
   584.82
  ],
  "n_reviews": 0
 },
 {
  "input": "You randomly shuffle your training data and then divide it into a whole bunch of mini-batches, let's say each one having 100 training examples.",
  "translatedText": "Amestecați aleatoriu datele de instruire și apoi le împărțiți în mai multe mini loturi, să spunem că fiecare dintre ele are 100 de exemple de instruire.",
  "model": "DeepL",
  "time_range": [
   585.48,
   592.42
  ],
  "n_reviews": 0
 },
 {
  "input": "Then you compute a step according to the mini-batch.",
  "translatedText": "Apoi se calculează un pas în funcție de mini-lot.",
  "model": "DeepL",
  "time_range": [
   592.94,
   596.2
  ],
  "n_reviews": 0
 },
 {
  "input": "It's not going to be the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speedup.",
  "translatedText": "Nu va fi gradientul real al funcției de cost, care depinde de toate datele de instruire, nu de acest mic subset, deci nu este cel mai eficient pas în jos, dar fiecare mini-lot vă oferă o aproximație destul de bună și, mai important, vă oferă o viteză de calcul semnificativă.",
  "model": "DeepL",
  "time_range": [
   596.96,
   612.12
  ],
  "n_reviews": 0
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction.",
  "translatedText": "Dacă ar fi să trasați traiectoria rețelei dvs. sub suprafața de cost relevantă, aceasta ar semăna mai degrabă cu un om beat care se poticnește fără țintă pe un deal, dar care face pași rapizi, decât cu un om care își calculează cu atenție direcția exactă de coborâre a fiecărui pas înainte de a face un pas foarte lent și atent în acea direcție.",
  "model": "DeepL",
  "time_range": [
   612.82,
   630.16
  ],
  "n_reviews": 0
 },
 {
  "input": "This technique is referred to as stochastic gradient descent.",
  "translatedText": "Această tehnică este denumită coborâre stocastică a gradientului.",
  "model": "DeepL",
  "time_range": [
   631.54,
   634.66
  ],
  "n_reviews": 0
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we?",
  "translatedText": "Se întâmplă multe lucruri aici, așa că hai să le rezumăm pentru noi înșine, bine?",
  "model": "DeepL",
  "time_range": [
   635.96,
   639.62
  ],
  "n_reviews": 0
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost.",
  "translatedText": "Backpropagation este algoritmul care determină modul în care un singur exemplu de instruire ar dori să influențeze ponderile și polarizările, nu doar în ceea ce privește dacă acestea ar trebui să crească sau să scadă, ci și în ceea ce privește proporțiile relative ale acestor modificări care determină cea mai rapidă scădere a costului.",
  "model": "DeepL",
  "time_range": [
   640.44,
   655.56
  ],
  "n_reviews": 0
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens of thousands of training examples and averaging the desired changes you get.",
  "translatedText": "Un pas adevărat de coborâre a gradientului ar implica să faceți acest lucru pentru toate zecile de mii de exemple de antrenament și să calculați media schimbărilor dorite pe care le obțineți.",
  "model": "DeepL",
  "time_range": [
   656.26,
   664.2
  ],
  "n_reviews": 0
 },
 {
  "input": "But that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch.",
  "translatedText": "Dar acest lucru este lent din punct de vedere computațional, așa că, în schimb, subdivizați aleatoriu datele în mini-loturi și calculați fiecare pas cu privire la un mini-lot.",
  "model": "DeepL",
  "time_range": [
   664.86,
   673.24
  ],
  "n_reviews": 0
 },
 {
  "input": "Repeatedly going through all of the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples.",
  "translatedText": "Trecând în mod repetat prin toate mini loturile și făcând aceste ajustări, veți converge către un minim local al funcției de cost, ceea ce înseamnă că rețeaua dvs. va sfârși prin a face o treabă foarte bună cu exemplele de antrenament.",
  "model": "DeepL",
  "time_range": [
   674.0,
   685.54
  ],
  "n_reviews": 0
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms.",
  "translatedText": "Acestea fiind spuse, fiecare linie de cod care ar trebui implementată pentru a implementa backprop corespunde de fapt cu ceva ce ați văzut acum, cel puțin în termeni informali.",
  "model": "DeepL",
  "time_range": [
   687.24,
   696.72
  ],
  "n_reviews": 0
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing.",
  "translatedText": "Dar, uneori, să știi ce face matematica este doar jumătate din bătălie, iar reprezentarea lucrurilor devine confuză și încurcată.",
  "model": "DeepL",
  "time_range": [
   697.56,
   704.12
  ],
  "n_reviews": 0
 },
 {
  "input": "So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources.",
  "translatedText": "Așadar, pentru cei care doresc să aprofundeze acest subiect, videoclipul următor trece în revistă aceleași idei care tocmai au fost prezentate aici, dar în termeni de calcul de bază, ceea ce ar trebui, sperăm, să vă fie puțin mai familiar pe măsură ce vedeți acest subiect în alte resurse.",
  "model": "DeepL",
  "time_range": [
   704.86,
   716.42
  ],
  "n_reviews": 0
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data.",
  "translatedText": "Înainte de asta, un lucru care merită subliniat este că, pentru ca acest algoritm să funcționeze, și acest lucru este valabil pentru toate tipurile de învățare automată, dincolo de rețelele neuronale, aveți nevoie de o mulțime de date de antrenament.",
  "model": "DeepL",
  "time_range": [
   717.34,
   725.9
  ],
  "n_reviews": 0
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans.",
  "translatedText": "În cazul nostru, un lucru care face ca cifrele scrise de mână să fie un exemplu atât de bun este faptul că există baza de date MNIST, cu atât de multe exemple care au fost etichetate de oameni.",
  "model": "DeepL",
  "time_range": [
   726.42,
   734.74
  ],
  "n_reviews": 0
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with.",
  "translatedText": "Așadar, o provocare obișnuită cu care cei care lucrează în domeniul învățării automate sunt familiarizați este obținerea datelor de instruire etichetate de care aveți nevoie, fie că este vorba de etichetarea a zeci de mii de imagini sau de orice alt tip de date cu care aveți de-a face.",
  "model": "DeepL",
  "time_range": [
   735.3,
   747.1
  ],
  "n_reviews": 0
 }
]