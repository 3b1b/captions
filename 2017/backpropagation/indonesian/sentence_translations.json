[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn. ",
  "translatedText": "Di sini, kami menangani propagasi mundur, algoritma inti di balik cara jaringan saraf belajar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. ",
  "translatedText": "Setelah rekap singkat mengenai keadaan kita saat ini, hal pertama yang akan saya lakukan adalah penelusuran intuitif tentang apa yang sebenarnya dilakukan algoritme, tanpa referensi apa pun ke rumusnya. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. ",
  "translatedText": "Lalu, bagi Anda yang memang ingin mendalami matematika, video selanjutnya akan membahas tentang kalkulus yang mendasari semua itu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. ",
  "translatedText": "Jika Anda menonton dua video terakhir, atau jika Anda hanya melihat latar belakang yang sesuai, Anda pasti tahu apa itu jaringan saraf, dan bagaimana jaringan tersebut meneruskan informasi. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. ",
  "translatedText": "Di sini, kita melakukan contoh klasik dalam mengenali angka tulisan tangan yang nilai pikselnya dimasukkan ke dalam lapisan pertama jaringan dengan 784 neuron, dan saya telah menunjukkan jaringan dengan dua lapisan tersembunyi yang masing-masing hanya memiliki 16 neuron, dan sebuah keluaran. lapisan 10 neuron, menunjukkan digit mana yang dipilih jaringan sebagai jawabannya. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. ",
  "translatedText": "Saya juga mengharapkan Anda memahami penurunan gradien, seperti yang dijelaskan dalam video terakhir, dan apa yang kami maksud dengan pembelajaran adalah kami ingin menemukan bobot dan bias mana yang meminimalkan fungsi biaya tertentu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. ",
  "translatedText": "Sebagai pengingat singkat, untuk biaya satu contoh pelatihan, Anda mengambil keluaran yang diberikan jaringan, bersama dengan keluaran yang ingin Anda berikan, dan menjumlahkan kuadrat selisih antara masing-masing komponen. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. ",
  "translatedText": "Melakukan hal ini untuk puluhan ribu contoh pelatihan Anda dan merata-ratakan hasilnya, ini akan memberi Anda total biaya jaringan. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. ",
  "translatedText": "Seolah-olah itu belum cukup untuk dipikirkan, seperti yang dijelaskan dalam video terakhir, hal yang kita cari adalah gradien negatif dari fungsi biaya ini, yang memberi tahu Anda bagaimana Anda perlu mengubah semua bobot dan bias, semuanya. koneksi ini, sehingga dapat mengurangi biaya secara paling efisien. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. ",
  "translatedText": "Propagasi mundur, topik video ini, adalah algoritma untuk menghitung gradien yang sangat rumit. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "The one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. ",
  "translatedText": "Satu gagasan dari video terakhir yang saya benar-benar ingin Anda ingat saat ini adalah karena memikirkan vektor gradien sebagai arah dalam 13.000 dimensi, secara sederhana, di luar jangkauan imajinasi kita, ada gagasan lain. cara Anda dapat memikirkannya. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. ",
  "translatedText": "Besaran setiap komponen di sini menunjukkan seberapa sensitif fungsi biaya terhadap setiap bobot dan bias. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. ",
  "translatedText": "Misalnya, Anda menjalani proses yang akan saya jelaskan, dan menghitung gradien negatif, dan komponen yang terkait dengan bobot pada tepi ini adalah 3.2, sedangkan komponen yang terkait dengan tepi ini di sini keluar sebagai 0.1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. ",
  "translatedText": "Cara Anda menafsirkannya adalah bahwa biaya fungsi tersebut 32 kali lebih sensitif terhadap perubahan bobot pertama, jadi jika Anda menggoyangkan nilai tersebut sedikit, hal ini akan menyebabkan beberapa perubahan pada biaya, dan perubahan itu adalah 32 kali lebih besar dari apa yang dihasilkan oleh goyangan yang sama pada beban kedua. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and index chasing of it all. ",
  "translatedText": "Secara pribadi, ketika saya pertama kali belajar tentang backpropagation, menurut saya aspek yang paling membingungkan hanyalah notasi dan pengejaran indeks dari semuanya. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. ",
  "translatedText": "Namun begitu Anda mengungkap apa yang sebenarnya dilakukan setiap bagian dari algoritme ini, setiap efek yang dimilikinya sebenarnya cukup intuitif, hanya saja ada banyak penyesuaian kecil yang ditumpangkan satu sama lain. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. ",
  "translatedText": "Jadi saya akan memulai semuanya di sini dengan mengabaikan notasi, dan hanya menelusuri efek setiap contoh pelatihan terhadap bobot dan bias. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. ",
  "translatedText": "Karena fungsi biaya melibatkan rata-rata biaya tertentu per contoh pada puluhan ribu contoh pelatihan, cara kita menyesuaikan bobot dan bias untuk satu langkah penurunan gradien juga bergantung pada setiap contoh. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. ",
  "translatedText": "Atau lebih tepatnya, pada prinsipnya memang seharusnya demikian, namun untuk efisiensi komputasi, kami akan melakukan sedikit trik nanti agar Anda tidak perlu melakukan setiap contoh untuk setiap langkah. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. ",
  "translatedText": "Dalam kasus lain, saat ini, yang akan kita lakukan hanyalah memusatkan perhatian kita pada satu contoh, gambar angka 2 ini. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted? ",
  "translatedText": "Apa pengaruh contoh pelatihan ini terhadap penyesuaian bobot dan bias? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. ",
  "translatedText": "Katakanlah kita berada pada titik di mana jaringan belum terlatih dengan baik, sehingga aktivasi pada output akan terlihat acak, mungkin sekitar 0.5, 0.8, 0.2, terus dan terus. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases, but it's helpful to keep track of which adjustments we wish should take place to that output layer. ",
  "translatedText": "Kita tidak dapat secara langsung mengubah aktivasi tersebut, kita hanya mempunyai pengaruh pada bobot dan bias, namun akan sangat membantu jika kita melacak penyesuaian mana yang kita inginkan untuk dilakukan pada lapisan keluaran tersebut. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 242.52,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. ",
  "translatedText": "Dan karena kita ingin mengklasifikasikan gambar sebagai 2, kita ingin nilai ketiga tersebut dinaikkan sementara nilai lainnya diturunkan. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. ",
  "translatedText": "Selain itu, ukuran dorongan ini harus sebanding dengan seberapa jauh jarak setiap nilai saat ini dari nilai targetnya. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. ",
  "translatedText": "Misalnya, peningkatan aktivasi neuron nomor 2 dalam arti tertentu lebih penting daripada penurunan aktivasi neuron nomor 8, yang sudah cukup dekat dengan tempat yang seharusnya. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. ",
  "translatedText": "Jadi jika kita perbesar lebih jauh, mari kita fokus pada satu neuron saja, yang aktivasinya ingin kita tingkatkan. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. ",
  "translatedText": "Ingat, aktivasi tersebut didefinisikan sebagai jumlah tertimbang tertentu dari semua aktivasi di lapisan sebelumnya, ditambah bias, yang semuanya kemudian dimasukkan ke dalam sesuatu seperti fungsi squishification sigmoid, atau ReLU. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation. ",
  "translatedText": "Jadi ada tiga cara berbeda yang dapat bekerja sama untuk membantu meningkatkan aktivasi tersebut. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. ",
  "translatedText": "Anda dapat meningkatkan bias, Anda dapat menambah bobot, dan Anda dapat mengubah aktivasi dari lapisan sebelumnya. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. ",
  "translatedText": "Berfokus pada bagaimana bobot harus disesuaikan, perhatikan bagaimana bobot sebenarnya memiliki tingkat pengaruh yang berbeda. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. ",
  "translatedText": "Koneksi dengan neuron paling terang dari lapisan sebelumnya memiliki pengaruh terbesar karena bobot tersebut dikalikan dengan nilai aktivasi yang lebih besar. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. ",
  "translatedText": "Jadi jika Anda meningkatkan salah satu bobot tersebut, hal ini sebenarnya memiliki pengaruh yang lebih kuat pada fungsi biaya akhir dibandingkan meningkatkan bobot koneksi dengan neuron peredup, setidaknya sejauh menyangkut contoh pelatihan yang satu ini. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck. ",
  "translatedText": "Ingat, ketika kita berbicara tentang penurunan gradien, kita tidak hanya peduli apakah setiap komponen harus dinaikkan atau diturunkan, kita juga peduli tentang komponen mana yang memberikan hasil maksimal. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together. ",
  "translatedText": "Omong-omong, hal ini setidaknya mengingatkan kita pada teori dalam ilmu saraf tentang bagaimana jaringan biologis neuron belajar, teori Hebbian, yang sering diringkas dalam frasa, neuron yang menyala bersama-sama saling terhubung. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active and the ones which we wish to become more active. ",
  "translatedText": "Di sini, peningkatan bobot terbesar, penguatan koneksi terbesar, terjadi antara neuron yang paling aktif dan neuron yang ingin kita jadikan lebih aktif. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those firing when thinking about it. ",
  "translatedText": "Dalam arti tertentu, neuron yang terpicu saat melihat angka 2 menjadi lebih terkait erat dengan neuron yang terpicu saat memikirkannya. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note. ",
  "translatedText": "Untuk lebih jelasnya, saya tidak dalam posisi untuk membuat pernyataan dengan satu atau lain cara tentang apakah jaringan neuron buatan berperilaku seperti otak biologis, dan gagasan yang menyatu ini disertai dengan beberapa tanda bintang yang bermakna, tetapi dianggap sangat longgar. analoginya, menurut saya menarik untuk disimak. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer. ",
  "translatedText": "Bagaimanapun, cara ketiga untuk membantu meningkatkan aktivasi neuron ini adalah dengan mengubah semua aktivasi di lapisan sebelumnya. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active. ",
  "translatedText": "Yaitu, jika semua yang terhubung ke neuron digit 2 yang berbobot positif menjadi lebih terang, dan jika semua yang terhubung dengan bobot negatif menjadi lebih redup, maka neuron digit 2 tersebut akan menjadi lebih aktif. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. ",
  "translatedText": "Mirip dengan perubahan berat badan, Anda akan mendapatkan hasil maksimal dengan mencari perubahan yang sebanding dengan ukuran beban yang sesuai. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases. ",
  "translatedText": "Tentu saja, kami tidak dapat secara langsung mempengaruhi aktivasi tersebut, kami hanya memiliki kendali atas bobot dan bias. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are. ",
  "translatedText": "Namun sama seperti lapisan terakhir, ada baiknya untuk mencatat perubahan apa saja yang diinginkan. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants. ",
  "translatedText": "Namun perlu diingat, memperkecil satu langkah di sini, ini hanya yang diinginkan oleh neuron keluaran digit 2 itu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. ",
  "translatedText": "Ingat, kita juga ingin semua neuron lain di lapisan terakhir menjadi kurang aktif, dan masing-masing neuron keluaran lainnya memiliki pemikirannya sendiri tentang apa yang harus terjadi pada lapisan kedua hingga terakhir tersebut. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. ",
  "translatedText": "Jadi keinginan neuron digit 2 ini dijumlahkan dengan keinginan semua neuron keluaran lainnya untuk apa yang seharusnya terjadi pada lapisan kedua hingga terakhir ini, sekali lagi sebanding dengan bobot yang sesuai, dan sebanding dengan seberapa banyak kebutuhan masing-masing neuron tersebut. Untuk mengganti. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in. ",
  "translatedText": "Di sinilah muncul ide untuk melakukan propaganda ke belakang. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. ",
  "translatedText": "Dengan menambahkan semua efek yang diinginkan ini, pada dasarnya Anda mendapatkan daftar dorongan yang Anda inginkan terjadi pada lapisan kedua hingga terakhir ini. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. ",
  "translatedText": "Dan setelah Anda memilikinya, Anda dapat menerapkan proses yang sama secara rekursif pada bobot dan bias relevan yang menentukan nilai tersebut, mengulangi proses yang sama yang baru saja saya lalui dan bergerak mundur melalui jaringan. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. ",
  "translatedText": "Dan jika diperbesar sedikit lagi, ingatlah bahwa ini adalah bagaimana sebuah contoh pelatihan ingin mendorong setiap bobot dan bias tersebut. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2. ",
  "translatedText": "Jika kita hanya mendengarkan apa yang diinginkan oleh 2, jaringan pada akhirnya akan diberi insentif hanya untuk mengklasifikasikan semua gambar sebagai 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes. ",
  "translatedText": "Jadi yang Anda lakukan adalah melakukan rutinitas backprop yang sama untuk setiap contoh pelatihan lainnya, mencatat bagaimana masing-masing contoh ingin mengubah bobot dan bias, dan membuat rata-rata perubahan yang diinginkan. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. ",
  "translatedText": "Kumpulan dorongan rata-rata untuk setiap bobot dan bias ini, secara sederhana, adalah gradien negatif dari fungsi biaya yang dirujuk dalam video terakhir, atau setidaknya sesuatu yang sebanding dengannya. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. ",
  "translatedText": "Saya mengatakannya dengan santai hanya karena saya belum mengetahui secara tepat secara kuantitatif mengenai dorongan-dorongan tersebut, namun jika Anda memahami setiap perubahan yang baru saja saya referensikan, mengapa beberapa perubahan secara proporsional lebih besar daripada yang lain, dan bagaimana semuanya perlu dijumlahkan, Anda memahami mekanisme untuk apa yang sebenarnya dilakukan propagasi mundur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. ",
  "translatedText": "Ngomong-ngomong, dalam praktiknya, komputer membutuhkan waktu yang sangat lama untuk menjumlahkan pengaruh setiap contoh pelatihan pada setiap langkah penurunan gradien. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead. ",
  "translatedText": "Jadi inilah yang biasa dilakukan. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and divide it into a whole bunch of mini-batches, let's say each one having 100 training examples. ",
  "translatedText": "Anda mengacak data pelatihan secara acak dan membaginya menjadi beberapa kelompok kecil, katakanlah masing-masing kelompok kecil memiliki 100 contoh pelatihan. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch. ",
  "translatedText": "Kemudian Anda menghitung langkah sesuai dengan mini-batch. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly it gives you a significant computational speedup. ",
  "translatedText": "Ini bukan gradien sebenarnya dari fungsi biaya, yang bergantung pada semua data pelatihan, bukan subset kecil ini, jadi ini bukan langkah menurun yang paling efisien, tetapi setiap mini-batch memberi Anda perkiraan yang cukup bagus, dan yang lebih penting itu memberi Anda kecepatan komputasi yang signifikan. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. ",
  "translatedText": "Jika Anda merencanakan lintasan jaringan Anda di bawah permukaan biaya yang relevan, hal tersebut akan lebih seperti seorang pria mabuk yang tersandung tanpa tujuan menuruni bukit namun mengambil langkah cepat, dibandingkan dengan seorang pria yang menghitung dengan cermat yang menentukan arah penurunan yang tepat dari setiap langkah. sebelum mengambil langkah yang sangat lambat dan hati-hati ke arah itu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent. ",
  "translatedText": "Teknik ini disebut sebagai penurunan gradien stokastik. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we? ",
  "translatedText": "Ada banyak hal yang terjadi di sini, jadi mari kita simpulkan sendiri, oke? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. ",
  "translatedText": "Propagasi mundur adalah algoritma untuk menentukan bagaimana sebuah contoh pelatihan ingin mendorong bobot dan bias, tidak hanya dalam hal apakah bobot dan bias tersebut harus naik atau turun, namun juga dalam hal proporsi relatif terhadap perubahan tersebut yang menyebabkan penurunan paling cepat pada bobot dan bias. biaya. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes you get, but that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch. ",
  "translatedText": "Langkah penurunan gradien yang sebenarnya akan melibatkan melakukan hal ini untuk semua puluhan dan ribuan contoh pelatihan Anda dan merata-ratakan perubahan yang diinginkan yang Anda dapatkan, tapi itu lambat secara komputasi, jadi Anda membagi data secara acak menjadi kumpulan kecil dan menghitung setiap langkah sehubungan dengan a kumpulan mini. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.26,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples. ",
  "translatedText": "Dengan berulang kali memeriksa semua mini-batch dan melakukan penyesuaian ini, Anda akan menyatu menuju fungsi biaya minimum lokal, yang berarti jaringan Anda pada akhirnya akan melakukan pekerjaan yang sangat baik pada contoh pelatihan. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms. ",
  "translatedText": "Jadi dengan semua hal tersebut, setiap baris kode yang digunakan untuk mengimplementasikan backprop sebenarnya sesuai dengan sesuatu yang telah Anda lihat, setidaknya secara informal. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. ",
  "translatedText": "Namun terkadang, mengetahui fungsi matematika hanyalah setengah dari perjuangan, dan hanya mewakili matematika saja sudah membuat semuanya menjadi kacau dan membingungkan. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So, for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. ",
  "translatedText": "Jadi, bagi Anda yang ingin mendalami lebih dalam, video berikutnya akan membahas ide-ide yang sama yang baru saja disajikan di sini, namun dalam kaitannya dengan kalkulus yang mendasarinya, yang semoga akan membuatnya lebih familiar saat Anda melihat topiknya di sumber daya lainnya. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. ",
  "translatedText": "Sebelum itu, satu hal yang perlu ditekankan adalah agar algoritme ini berfungsi, dan ini berlaku untuk semua jenis pembelajaran mesin selain jaringan saraf, Anda memerlukan banyak data pelatihan. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans. ",
  "translatedText": "Dalam kasus kami, satu hal yang membuat angka tulisan tangan menjadi contoh yang bagus adalah adanya database MNIST, dengan begitu banyak contoh yang telah diberi label oleh manusia. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with. ",
  "translatedText": "Jadi, tantangan umum yang biasa dihadapi oleh Anda yang bekerja di bidang pembelajaran mesin hanyalah mendapatkan data pelatihan berlabel yang benar-benar Anda perlukan, apakah itu meminta orang memberi label pada puluhan ribu gambar, atau jenis data apa pun yang mungkin Anda hadapi. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]