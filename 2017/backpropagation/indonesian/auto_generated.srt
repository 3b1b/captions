1
00:00:04,060 --> 00:00:06,698
Di sini, kami menangani propagasi mundur, algoritma 

2
00:00:06,698 --> 00:00:08,880
inti di balik cara jaringan saraf belajar. 

3
00:00:09,400 --> 00:00:11,421
Setelah rekap singkat mengenai keadaan kita saat ini, 

4
00:00:11,421 --> 00:00:13,780
hal pertama yang akan saya lakukan adalah penelusuran intuitif 

5
00:00:13,780 --> 00:00:17,000
tentang apa yang sebenarnya dilakukan algoritme, tanpa referensi apa pun ke rumusnya. 

6
00:00:17,660 --> 00:00:19,951
Lalu, bagi Anda yang memang ingin mendalami matematika, 

7
00:00:19,951 --> 00:00:23,020
video selanjutnya akan membahas tentang kalkulus yang mendasari semua itu. 

8
00:00:23,820 --> 00:00:26,087
Jika Anda menonton dua video terakhir, atau jika Anda hanya 

9
00:00:26,087 --> 00:00:28,959
melihat latar belakang yang sesuai, Anda pasti tahu apa itu jaringan saraf, 

10
00:00:28,959 --> 00:00:31,000
dan bagaimana jaringan tersebut meneruskan informasi. 

11
00:00:31,680 --> 00:00:35,133
Di sini, kita melakukan contoh klasik dalam mengenali angka tulisan tangan 

12
00:00:35,133 --> 00:00:39,047
yang nilai pikselnya dimasukkan ke dalam lapisan pertama jaringan dengan 784 neuron, 

13
00:00:39,047 --> 00:00:42,363
dan saya telah menunjukkan jaringan dengan dua lapisan tersembunyi yang 

14
00:00:42,363 --> 00:00:46,046
masing-masing hanya memiliki 16 neuron, dan sebuah keluaran. lapisan 10 neuron, 

15
00:00:46,046 --> 00:00:49,040
menunjukkan digit mana yang dipilih jaringan sebagai jawabannya. 

16
00:00:50,040 --> 00:00:52,702
Saya juga mengharapkan Anda memahami penurunan gradien, 

17
00:00:52,702 --> 00:00:56,410
seperti yang dijelaskan dalam video terakhir, dan apa yang kami maksud dengan 

18
00:00:56,410 --> 00:01:00,166
pembelajaran adalah kami ingin menemukan bobot dan bias mana yang meminimalkan 

19
00:01:00,166 --> 00:01:01,260
fungsi biaya tertentu. 

20
00:01:02,040 --> 00:01:05,516
Sebagai pengingat singkat, untuk biaya satu contoh pelatihan, 

21
00:01:05,516 --> 00:01:08,263
Anda mengambil keluaran yang diberikan jaringan, 

22
00:01:08,263 --> 00:01:11,011
bersama dengan keluaran yang ingin Anda berikan, 

23
00:01:11,011 --> 00:01:14,600
dan menjumlahkan kuadrat selisih antara masing-masing komponen. 

24
00:01:15,380 --> 00:01:20,473
Melakukan hal ini untuk puluhan ribu contoh pelatihan Anda dan merata-ratakan hasilnya, 

25
00:01:20,473 --> 00:01:23,020
ini akan memberi Anda total biaya jaringan. 

26
00:01:23,020 --> 00:01:26,770
Seolah-olah itu belum cukup untuk dipikirkan, seperti yang dijelaskan dalam 

27
00:01:26,770 --> 00:01:30,768
video terakhir, hal yang kita cari adalah gradien negatif dari fungsi biaya ini, 

28
00:01:30,768 --> 00:01:34,470
yang memberi tahu Anda bagaimana Anda perlu mengubah semua bobot dan bias, 

29
00:01:34,470 --> 00:01:38,320
semuanya. koneksi ini, sehingga dapat mengurangi biaya secara paling efisien. 

30
00:01:43,260 --> 00:01:46,683
Propagasi mundur, topik video ini, adalah algoritma 

31
00:01:46,683 --> 00:01:49,580
untuk menghitung gradien yang sangat rumit. 

32
00:01:49,580 --> 00:01:54,373
Satu gagasan dari video terakhir yang saya benar-benar ingin Anda ingat saat ini adalah 

33
00:01:54,373 --> 00:01:59,058
karena memikirkan vektor gradien sebagai arah dalam 13.000 dimensi, secara sederhana, 

34
00:01:59,058 --> 00:02:03,580
di luar jangkauan imajinasi kita, ada gagasan lain. cara Anda dapat memikirkannya. 

35
00:02:04,600 --> 00:02:07,740
Besaran setiap komponen di sini menunjukkan seberapa 

36
00:02:07,740 --> 00:02:10,940
sensitif fungsi biaya terhadap setiap bobot dan bias. 

37
00:02:11,800 --> 00:02:17,395
Misalnya, Anda menjalani proses yang akan saya jelaskan, dan menghitung gradien negatif, 

38
00:02:17,395 --> 00:02:21,481
dan komponen yang terkait dengan bobot pada tepi ini adalah 3.2, 

39
00:02:21,481 --> 00:02:26,260
sedangkan komponen yang terkait dengan tepi ini di sini keluar sebagai 0.1. 

40
00:02:26,820 --> 00:02:30,807
Cara Anda menafsirkannya adalah bahwa biaya fungsi tersebut 32 kali lebih sensitif 

41
00:02:30,807 --> 00:02:34,988
terhadap perubahan bobot pertama, jadi jika Anda menggoyangkan nilai tersebut sedikit, 

42
00:02:34,988 --> 00:02:37,678
hal ini akan menyebabkan beberapa perubahan pada biaya, 

43
00:02:37,678 --> 00:02:41,714
dan perubahan itu adalah 32 kali lebih besar dari apa yang dihasilkan oleh goyangan 

44
00:02:41,714 --> 00:02:43,060
yang sama pada beban kedua. 

45
00:02:48,420 --> 00:02:51,569
Secara pribadi, ketika saya pertama kali belajar tentang backpropagation, 

46
00:02:51,569 --> 00:02:55,314
menurut saya aspek yang paling membingungkan hanyalah notasi dan pengejaran indeks dari 

47
00:02:55,314 --> 00:02:55,740
semuanya. 

48
00:02:56,220 --> 00:02:59,631
Namun begitu Anda mengungkap apa yang sebenarnya dilakukan setiap bagian 

49
00:02:59,631 --> 00:03:03,182
dari algoritme ini, setiap efek yang dimilikinya sebenarnya cukup intuitif, 

50
00:03:03,182 --> 00:03:06,640
hanya saja ada banyak penyesuaian kecil yang ditumpangkan satu sama lain. 

51
00:03:07,740 --> 00:03:11,693
Jadi saya akan memulai semuanya di sini dengan mengabaikan notasi, 

52
00:03:11,693 --> 00:03:16,120
dan hanya menelusuri efek setiap contoh pelatihan terhadap bobot dan bias. 

53
00:03:17,020 --> 00:03:21,737
Karena fungsi biaya melibatkan rata-rata biaya tertentu per contoh pada 

54
00:03:21,737 --> 00:03:26,257
puluhan ribu contoh pelatihan, cara kita menyesuaikan bobot dan bias 

55
00:03:26,257 --> 00:03:31,040
untuk satu langkah penurunan gradien juga bergantung pada setiap contoh. 

56
00:03:31,680 --> 00:03:34,064
Atau lebih tepatnya, pada prinsipnya memang seharusnya demikian, 

57
00:03:34,064 --> 00:03:36,485
namun untuk efisiensi komputasi, kami akan melakukan sedikit trik 

58
00:03:36,485 --> 00:03:39,200
nanti agar Anda tidak perlu melakukan setiap contoh untuk setiap langkah. 

59
00:03:39,200 --> 00:03:42,470
Dalam kasus lain, saat ini, yang akan kita lakukan hanyalah 

60
00:03:42,470 --> 00:03:45,960
memusatkan perhatian kita pada satu contoh, gambar angka 2 ini. 

61
00:03:46,720 --> 00:03:51,480
Apa pengaruh contoh pelatihan ini terhadap penyesuaian bobot dan bias? 

62
00:03:52,680 --> 00:03:56,839
Katakanlah kita berada pada titik di mana jaringan belum terlatih dengan baik, 

63
00:03:56,839 --> 00:04:01,104
sehingga aktivasi pada output akan terlihat acak, mungkin sekitar 0.5, 0.8, 0.2, 

64
00:04:01,104 --> 00:04:02,000
terus dan terus. 

65
00:04:02,520 --> 00:04:05,066
Kita tidak dapat secara langsung mengubah aktivasi tersebut, 

66
00:04:05,066 --> 00:04:07,195
kita hanya mempunyai pengaruh pada bobot dan bias, 

67
00:04:07,195 --> 00:04:10,576
namun akan sangat membantu jika kita melacak penyesuaian mana yang kita inginkan 

68
00:04:10,576 --> 00:04:12,580
untuk dilakukan pada lapisan keluaran tersebut. 

69
00:04:13,360 --> 00:04:16,737
Dan karena kita ingin mengklasifikasikan gambar sebagai 2, 

70
00:04:16,737 --> 00:04:21,260
kita ingin nilai ketiga tersebut dinaikkan sementara nilai lainnya diturunkan. 

71
00:04:22,060 --> 00:04:25,507
Selain itu, ukuran dorongan ini harus sebanding dengan 

72
00:04:25,507 --> 00:04:29,520
seberapa jauh jarak setiap nilai saat ini dari nilai targetnya. 

73
00:04:30,220 --> 00:04:35,560
Misalnya, peningkatan aktivasi neuron nomor 2 dalam arti tertentu lebih penting daripada 

74
00:04:35,560 --> 00:04:40,900
penurunan aktivasi neuron nomor 8, yang sudah cukup dekat dengan tempat yang seharusnya. 

75
00:04:42,040 --> 00:04:45,457
Jadi jika kita perbesar lebih jauh, mari kita fokus pada satu neuron saja, 

76
00:04:45,457 --> 00:04:47,280
yang aktivasinya ingin kita tingkatkan. 

77
00:04:48,180 --> 00:04:52,521
Ingat, aktivasi tersebut didefinisikan sebagai jumlah tertimbang tertentu dari 

78
00:04:52,521 --> 00:04:55,434
semua aktivasi di lapisan sebelumnya, ditambah bias, 

79
00:04:55,434 --> 00:04:59,940
yang semuanya kemudian dimasukkan ke dalam sesuatu seperti fungsi squishification 

80
00:04:59,940 --> 00:05:01,040
sigmoid, atau ReLU. 

81
00:05:01,640 --> 00:05:04,439
Jadi ada tiga cara berbeda yang dapat bekerja sama 

82
00:05:04,439 --> 00:05:07,020
untuk membantu meningkatkan aktivasi tersebut. 

83
00:05:07,440 --> 00:05:10,711
Anda dapat meningkatkan bias, Anda dapat menambah bobot, 

84
00:05:10,711 --> 00:05:14,040
dan Anda dapat mengubah aktivasi dari lapisan sebelumnya. 

85
00:05:14,940 --> 00:05:17,224
Berfokus pada bagaimana bobot harus disesuaikan, 

86
00:05:17,224 --> 00:05:20,860
perhatikan bagaimana bobot sebenarnya memiliki tingkat pengaruh yang berbeda. 

87
00:05:21,440 --> 00:05:25,197
Koneksi dengan neuron paling terang dari lapisan sebelumnya memiliki pengaruh 

88
00:05:25,197 --> 00:05:29,100
terbesar karena bobot tersebut dikalikan dengan nilai aktivasi yang lebih besar. 

89
00:05:31,460 --> 00:05:34,042
Jadi jika Anda meningkatkan salah satu bobot tersebut, 

90
00:05:34,042 --> 00:05:38,268
hal ini sebenarnya memiliki pengaruh yang lebih kuat pada fungsi biaya akhir dibandingkan 

91
00:05:38,268 --> 00:05:40,615
meningkatkan bobot koneksi dengan neuron peredup, 

92
00:05:40,615 --> 00:05:43,480
setidaknya sejauh menyangkut contoh pelatihan yang satu ini. 

93
00:05:44,420 --> 00:05:46,800
Ingat, ketika kita berbicara tentang penurunan gradien, 

94
00:05:46,800 --> 00:05:50,201
kita tidak hanya peduli apakah setiap komponen harus dinaikkan atau diturunkan, 

95
00:05:50,201 --> 00:05:53,220
kita juga peduli tentang komponen mana yang memberikan hasil maksimal. 

96
00:05:55,020 --> 00:05:58,882
Omong-omong, hal ini setidaknya mengingatkan kita pada teori dalam ilmu saraf 

97
00:05:58,882 --> 00:06:02,200
tentang bagaimana jaringan biologis neuron belajar, teori Hebbian, 

98
00:06:02,200 --> 00:06:06,460
yang sering diringkas dalam frasa, neuron yang menyala bersama-sama saling terhubung. 

99
00:06:07,260 --> 00:06:11,516
Di sini, peningkatan bobot terbesar, penguatan koneksi terbesar, 

100
00:06:11,516 --> 00:06:17,280
terjadi antara neuron yang paling aktif dan neuron yang ingin kita jadikan lebih aktif. 

101
00:06:17,940 --> 00:06:21,306
Dalam arti tertentu, neuron yang terpicu saat melihat angka 2 menjadi 

102
00:06:21,306 --> 00:06:24,480
lebih terkait erat dengan neuron yang terpicu saat memikirkannya. 

103
00:06:25,400 --> 00:06:29,281
Untuk lebih jelasnya, saya tidak dalam posisi untuk membuat pernyataan dengan satu 

104
00:06:29,281 --> 00:06:33,397
atau lain cara tentang apakah jaringan neuron buatan berperilaku seperti otak biologis, 

105
00:06:33,397 --> 00:06:37,278
dan gagasan yang menyatu ini disertai dengan beberapa tanda bintang yang bermakna, 

106
00:06:37,278 --> 00:06:41,020
tetapi dianggap sangat longgar. analoginya, menurut saya menarik untuk disimak. 

107
00:06:41,940 --> 00:06:45,621
Bagaimanapun, cara ketiga untuk membantu meningkatkan aktivasi neuron 

108
00:06:45,621 --> 00:06:49,040
ini adalah dengan mengubah semua aktivasi di lapisan sebelumnya. 

109
00:06:49,040 --> 00:06:52,867
Yaitu, jika semua yang terhubung ke neuron digit 2 yang berbobot positif 

110
00:06:52,867 --> 00:06:56,695
menjadi lebih terang, dan jika semua yang terhubung dengan bobot negatif 

111
00:06:56,695 --> 00:07:00,680
menjadi lebih redup, maka neuron digit 2 tersebut akan menjadi lebih aktif. 

112
00:07:02,540 --> 00:07:06,409
Mirip dengan perubahan berat badan, Anda akan mendapatkan hasil maksimal 

113
00:07:06,409 --> 00:07:10,280
dengan mencari perubahan yang sebanding dengan ukuran beban yang sesuai. 

114
00:07:12,140 --> 00:07:15,403
Tentu saja, kami tidak dapat secara langsung mempengaruhi aktivasi tersebut, 

115
00:07:15,403 --> 00:07:17,480
kami hanya memiliki kendali atas bobot dan bias. 

116
00:07:17,480 --> 00:07:20,733
Namun sama seperti lapisan terakhir, ada baiknya 

117
00:07:20,733 --> 00:07:24,120
untuk mencatat perubahan apa saja yang diinginkan. 

118
00:07:24,580 --> 00:07:26,789
Namun perlu diingat, memperkecil satu langkah di sini, 

119
00:07:26,789 --> 00:07:29,200
ini hanya yang diinginkan oleh neuron keluaran digit 2 itu. 

120
00:07:29,760 --> 00:07:33,295
Ingat, kita juga ingin semua neuron lain di lapisan terakhir menjadi kurang aktif, 

121
00:07:33,295 --> 00:07:36,703
dan masing-masing neuron keluaran lainnya memiliki pemikirannya sendiri tentang 

122
00:07:36,703 --> 00:07:39,600
apa yang harus terjadi pada lapisan kedua hingga terakhir tersebut. 

123
00:07:42,700 --> 00:07:47,117
Jadi keinginan neuron digit 2 ini dijumlahkan dengan keinginan semua neuron 

124
00:07:47,117 --> 00:07:51,593
keluaran lainnya untuk apa yang seharusnya terjadi pada lapisan kedua hingga 

125
00:07:51,593 --> 00:07:55,197
terakhir ini, sekali lagi sebanding dengan bobot yang sesuai, 

126
00:07:55,197 --> 00:07:59,731
dan sebanding dengan seberapa banyak kebutuhan masing-masing neuron tersebut. 

127
00:07:59,731 --> 00:08:00,720
Untuk mengganti. 

128
00:08:01,600 --> 00:08:05,480
Di sinilah muncul ide untuk melakukan propaganda ke belakang. 

129
00:08:05,820 --> 00:08:08,150
Dengan menambahkan semua efek yang diinginkan ini, 

130
00:08:08,150 --> 00:08:11,760
pada dasarnya Anda mendapatkan daftar dorongan yang Anda inginkan terjadi pada 

131
00:08:11,760 --> 00:08:13,360
lapisan kedua hingga terakhir ini. 

132
00:08:14,220 --> 00:08:17,708
Dan setelah Anda memilikinya, Anda dapat menerapkan proses yang sama secara 

133
00:08:17,708 --> 00:08:20,876
rekursif pada bobot dan bias relevan yang menentukan nilai tersebut, 

134
00:08:20,876 --> 00:08:24,640
mengulangi proses yang sama yang baru saja saya lalui dan bergerak mundur melalui 

135
00:08:24,640 --> 00:08:25,100
jaringan. 

136
00:08:28,960 --> 00:08:32,923
Dan jika diperbesar sedikit lagi, ingatlah bahwa ini adalah bagaimana 

137
00:08:32,923 --> 00:08:37,000
sebuah contoh pelatihan ingin mendorong setiap bobot dan bias tersebut. 

138
00:08:37,480 --> 00:08:39,577
Jika kita hanya mendengarkan apa yang diinginkan oleh 2, 

139
00:08:39,577 --> 00:08:42,336
jaringan pada akhirnya akan diberi insentif hanya untuk mengklasifikasikan 

140
00:08:42,336 --> 00:08:43,220
semua gambar sebagai 2. 

141
00:08:44,059 --> 00:08:47,987
Jadi yang Anda lakukan adalah melakukan rutinitas backprop yang sama untuk 

142
00:08:47,987 --> 00:08:51,810
setiap contoh pelatihan lainnya, mencatat bagaimana masing-masing contoh 

143
00:08:51,810 --> 00:08:56,000
ingin mengubah bobot dan bias, dan membuat rata-rata perubahan yang diinginkan. 

144
00:09:01,720 --> 00:09:05,278
Kumpulan dorongan rata-rata untuk setiap bobot dan bias ini, 

145
00:09:05,278 --> 00:09:09,012
secara sederhana, adalah gradien negatif dari fungsi biaya yang 

146
00:09:09,012 --> 00:09:13,680
dirujuk dalam video terakhir, atau setidaknya sesuatu yang sebanding dengannya. 

147
00:09:14,380 --> 00:09:17,647
Saya mengatakannya dengan santai hanya karena saya belum mengetahui secara tepat 

148
00:09:17,647 --> 00:09:19,906
secara kuantitatif mengenai dorongan-dorongan tersebut, 

149
00:09:19,906 --> 00:09:22,972
namun jika Anda memahami setiap perubahan yang baru saja saya referensikan, 

150
00:09:22,972 --> 00:09:26,159
mengapa beberapa perubahan secara proporsional lebih besar daripada yang lain, 

151
00:09:26,159 --> 00:09:29,426
dan bagaimana semuanya perlu dijumlahkan, Anda memahami mekanisme untuk apa yang 

152
00:09:29,426 --> 00:09:31,000
sebenarnya dilakukan propagasi mundur. 

153
00:09:33,960 --> 00:09:38,200
Ngomong-ngomong, dalam praktiknya, komputer membutuhkan waktu yang sangat lama untuk 

154
00:09:38,200 --> 00:09:42,440
menjumlahkan pengaruh setiap contoh pelatihan pada setiap langkah penurunan gradien. 

155
00:09:43,140 --> 00:09:44,820
Jadi inilah yang biasa dilakukan. 

156
00:09:45,480 --> 00:09:49,340
Anda mengacak data pelatihan secara acak dan membaginya menjadi beberapa kelompok kecil, 

157
00:09:49,340 --> 00:09:52,420
katakanlah masing-masing kelompok kecil memiliki 100 contoh pelatihan. 

158
00:09:52,939 --> 00:09:57,280
Kemudian Anda menghitung langkah sesuai dengan mini-batch. 

159
00:09:57,280 --> 00:09:59,577
Ini bukan gradien sebenarnya dari fungsi biaya, 

160
00:09:59,577 --> 00:10:02,785
yang bergantung pada semua data pelatihan, bukan subset kecil ini, 

161
00:10:02,785 --> 00:10:05,274
jadi ini bukan langkah menurun yang paling efisien, 

162
00:10:05,274 --> 00:10:08,433
tetapi setiap mini-batch memberi Anda perkiraan yang cukup bagus, 

163
00:10:08,433 --> 00:10:12,120
dan yang lebih penting itu memberi Anda kecepatan komputasi yang signifikan. 

164
00:10:12,820 --> 00:10:16,423
Jika Anda merencanakan lintasan jaringan Anda di bawah permukaan biaya yang relevan, 

165
00:10:16,423 --> 00:10:19,815
hal tersebut akan lebih seperti seorang pria mabuk yang tersandung tanpa tujuan 

166
00:10:19,815 --> 00:10:23,164
menuruni bukit namun mengambil langkah cepat, dibandingkan dengan seorang pria 

167
00:10:23,164 --> 00:10:26,725
yang menghitung dengan cermat yang menentukan arah penurunan yang tepat dari setiap 

168
00:10:26,725 --> 00:10:30,160
langkah. sebelum mengambil langkah yang sangat lambat dan hati-hati ke arah itu. 

169
00:10:31,540 --> 00:10:34,660
Teknik ini disebut sebagai penurunan gradien stokastik. 

170
00:10:35,960 --> 00:10:39,620
Ada banyak hal yang terjadi di sini, jadi mari kita simpulkan sendiri, oke? 

171
00:10:40,440 --> 00:10:44,370
Propagasi mundur adalah algoritma untuk menentukan bagaimana sebuah contoh pelatihan 

172
00:10:44,370 --> 00:10:48,300
ingin mendorong bobot dan bias, tidak hanya dalam hal apakah bobot dan bias tersebut 

173
00:10:48,300 --> 00:10:51,999
harus naik atau turun, namun juga dalam hal proporsi relatif terhadap perubahan 

174
00:10:51,999 --> 00:10:55,560
tersebut yang menyebabkan penurunan paling cepat pada bobot dan bias. biaya. 

175
00:10:56,260 --> 00:11:00,517
Langkah penurunan gradien yang sebenarnya akan melibatkan melakukan hal ini untuk semua 

176
00:11:00,517 --> 00:11:04,677
puluhan dan ribuan contoh pelatihan Anda dan merata-ratakan perubahan yang diinginkan 

177
00:11:04,677 --> 00:11:07,289
yang Anda dapatkan, tapi itu lambat secara komputasi, 

178
00:11:07,289 --> 00:11:11,546
jadi Anda membagi data secara acak menjadi kumpulan kecil dan menghitung setiap langkah 

179
00:11:11,546 --> 00:11:13,240
sehubungan dengan a kumpulan mini. 

180
00:11:14,000 --> 00:11:17,830
Dengan berulang kali memeriksa semua mini-batch dan melakukan penyesuaian ini, 

181
00:11:17,830 --> 00:11:20,400
Anda akan menyatu menuju fungsi biaya minimum lokal, 

182
00:11:20,400 --> 00:11:24,182
yang berarti jaringan Anda pada akhirnya akan melakukan pekerjaan yang sangat 

183
00:11:24,182 --> 00:11:25,540
baik pada contoh pelatihan. 

184
00:11:27,240 --> 00:11:30,588
Jadi dengan semua hal tersebut, setiap baris kode yang digunakan 

185
00:11:30,588 --> 00:11:33,680
untuk mengimplementasikan backprop sebenarnya sesuai dengan 

186
00:11:33,680 --> 00:11:36,720
sesuatu yang telah Anda lihat, setidaknya secara informal. 

187
00:11:37,560 --> 00:11:40,649
Namun terkadang, mengetahui fungsi matematika hanyalah setengah dari perjuangan, 

188
00:11:40,649 --> 00:11:44,081
dan hanya mewakili matematika saja sudah membuat semuanya menjadi kacau dan membingungkan.

189
00:11:44,081 --> 00:11:44,120
 

190
00:11:44,860 --> 00:11:46,895
Jadi, bagi Anda yang ingin mendalami lebih dalam, 

191
00:11:46,895 --> 00:11:50,273
video berikutnya akan membahas ide-ide yang sama yang baru saja disajikan di sini, 

192
00:11:50,273 --> 00:11:52,593
namun dalam kaitannya dengan kalkulus yang mendasarinya, 

193
00:11:52,593 --> 00:11:55,443
yang semoga akan membuatnya lebih familiar saat Anda melihat topiknya 

194
00:11:55,443 --> 00:11:56,420
di sumber daya lainnya. 

195
00:11:57,340 --> 00:12:00,877
Sebelum itu, satu hal yang perlu ditekankan adalah agar algoritme ini berfungsi, 

196
00:12:00,877 --> 00:12:04,196
dan ini berlaku untuk semua jenis pembelajaran mesin selain jaringan saraf, 

197
00:12:04,196 --> 00:12:05,900
Anda memerlukan banyak data pelatihan. 

198
00:12:06,420 --> 00:12:09,193
Dalam kasus kami, satu hal yang membuat angka tulisan tangan 

199
00:12:09,193 --> 00:12:11,739
menjadi contoh yang bagus adalah adanya database MNIST, 

200
00:12:11,739 --> 00:12:14,740
dengan begitu banyak contoh yang telah diberi label oleh manusia. 

201
00:12:15,300 --> 00:12:18,363
Jadi, tantangan umum yang biasa dihadapi oleh Anda yang bekerja di bidang 

202
00:12:18,363 --> 00:12:21,220
pembelajaran mesin hanyalah mendapatkan data pelatihan berlabel yang 

203
00:12:21,220 --> 00:12:24,160
benar-benar Anda perlukan, apakah itu meminta orang memberi label pada 

204
00:12:24,160 --> 00:12:27,100
puluhan ribu gambar, atau jenis data apa pun yang mungkin Anda hadapi. 

