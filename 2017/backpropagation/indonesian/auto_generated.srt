1
00:00:00,000 --> 00:00:05,276
Di sini, kami menangani propagasi mundur, algoritma

2
00:00:05,276 --> 00:00:09,640
inti di balik cara jaringan saraf belajar.

3
00:00:09,640 --> 00:00:12,162
Setelah rekap singkat mengenai keadaan kita saat ini, hal pertama

4
00:00:12,162 --> 00:00:14,762
yang akan saya lakukan adalah penelusuran intuitif tentang apa yang

5
00:00:14,762 --> 00:00:17,400
sebenarnya dilakukan algoritme, tanpa referensi apa pun ke rumusnya.

6
00:00:17,400 --> 00:00:20,542
Lalu, bagi Anda yang memang ingin mendalami matematika, video

7
00:00:20,542 --> 00:00:24,040
selanjutnya akan membahas tentang kalkulus yang mendasari semua itu.

8
00:00:24,040 --> 00:00:26,263
Jika Anda menonton dua video terakhir, atau jika Anda hanya

9
00:00:26,263 --> 00:00:28,486
melihat latar belakang yang sesuai, Anda pasti tahu apa itu

10
00:00:28,486 --> 00:00:31,080
jaringan saraf, dan bagaimana jaringan tersebut meneruskan informasi.

11
00:00:31,080 --> 00:00:34,748
Di sini, kita melakukan contoh klasik dalam mengenali angka tulisan tangan

12
00:00:34,748 --> 00:00:38,319
yang nilai pikselnya dimasukkan ke dalam lapisan pertama jaringan dengan

13
00:00:38,319 --> 00:00:42,183
784 neuron, dan saya telah menunjukkan jaringan dengan dua lapisan tersembunyi

14
00:00:42,183 --> 00:00:45,802
yang masing-masing hanya memiliki 16 neuron, dan sebuah keluaran. lapisan

15
00:00:45,802 --> 00:00:49,520
10 neuron, menunjukkan digit mana yang dipilih jaringan sebagai jawabannya.

16
00:00:49,520 --> 00:00:53,777
Saya juga mengharapkan Anda memahami penurunan gradien, seperti yang dijelaskan

17
00:00:53,777 --> 00:00:57,982
dalam video terakhir, dan apa yang kami maksud dengan pembelajaran adalah kami

18
00:00:57,982 --> 00:01:02,080
ingin menemukan bobot dan bias mana yang meminimalkan fungsi biaya tertentu.

19
00:01:02,080 --> 00:01:06,713
Sebagai pengingat singkat, untuk biaya satu contoh pelatihan, Anda mengambil

20
00:01:06,713 --> 00:01:11,166
keluaran yang diberikan jaringan, bersama dengan keluaran yang ingin Anda

21
00:01:11,166 --> 00:01:15,560
berikan, dan menjumlahkan kuadrat selisih antara masing-masing komponen.

22
00:01:15,560 --> 00:01:19,130
Melakukan hal ini untuk puluhan ribu contoh pelatihan Anda dan

23
00:01:19,130 --> 00:01:23,040
merata-ratakan hasilnya, ini akan memberi Anda total biaya jaringan.

24
00:01:23,040 --> 00:01:27,953
Seolah-olah itu belum cukup untuk dipikirkan, seperti yang dijelaskan dalam

25
00:01:27,953 --> 00:01:32,866
video terakhir, hal yang kita cari adalah gradien negatif dari fungsi biaya

26
00:01:32,866 --> 00:01:37,649
ini, yang memberi tahu Anda bagaimana Anda perlu mengubah semua bobot dan

27
00:01:37,649 --> 00:01:43,080
bias, semuanya. koneksi ini, sehingga dapat mengurangi biaya secara paling efisien.

28
00:01:43,080 --> 00:01:46,611
Propagasi mundur, topik video ini, adalah algoritma

29
00:01:46,611 --> 00:01:49,600
untuk menghitung gradien yang sangat rumit.

30
00:01:49,600 --> 00:01:54,743
Satu gagasan dari video terakhir yang saya benar-benar ingin Anda ingat saat ini adalah

31
00:01:54,743 --> 00:01:59,769
karena memikirkan vektor gradien sebagai arah dalam 13.000 dimensi, secara sederhana,

32
00:01:59,769 --> 00:02:04,620
di luar jangkauan imajinasi kita, ada gagasan lain. cara Anda dapat memikirkannya.

33
00:02:04,620 --> 00:02:08,186
Besaran setiap komponen di sini menunjukkan seberapa

34
00:02:08,186 --> 00:02:11,820
sensitif fungsi biaya terhadap setiap bobot dan bias.

35
00:02:11,820 --> 00:02:16,553
Misalnya, Anda menjalani proses yang akan saya jelaskan, dan menghitung

36
00:02:16,553 --> 00:02:21,615
gradien negatif, dan komponen yang terkait dengan bobot pada tepi ini adalah

37
00:02:21,615 --> 00:02:26,940
3.2, sedangkan komponen yang terkait dengan tepi ini di sini keluar sebagai 0.1.

38
00:02:26,940 --> 00:02:31,517
Cara Anda menafsirkannya adalah bahwa biaya fungsi tersebut 32 kali lebih sensitif

39
00:02:31,517 --> 00:02:36,315
terhadap perubahan bobot pertama, jadi jika Anda menggoyangkan nilai tersebut sedikit,

40
00:02:36,315 --> 00:02:40,947
hal ini akan menyebabkan beberapa perubahan pada biaya, dan perubahan itu adalah 32

41
00:02:40,947 --> 00:02:45,580
kali lebih besar dari apa yang dihasilkan oleh goyangan yang sama pada beban kedua.

42
00:02:45,580 --> 00:02:50,759
Secara pribadi, ketika saya pertama kali belajar tentang backpropagation, menurut saya

43
00:02:50,759 --> 00:02:55,820
aspek yang paling membingungkan hanyalah notasi dan pengejaran indeks dari semuanya.

44
00:02:55,820 --> 00:02:59,722
Namun begitu Anda mengungkap apa yang sebenarnya dilakukan setiap bagian

45
00:02:59,722 --> 00:03:03,784
dari algoritme ini, setiap efek yang dimilikinya sebenarnya cukup intuitif,

46
00:03:03,784 --> 00:03:07,740
hanya saja ada banyak penyesuaian kecil yang ditumpangkan satu sama lain.

47
00:03:07,740 --> 00:03:12,560
Jadi saya akan memulai semuanya di sini dengan mengabaikan notasi, dan

48
00:03:12,560 --> 00:03:17,380
hanya menelusuri efek setiap contoh pelatihan terhadap bobot dan bias.

49
00:03:17,380 --> 00:03:22,211
Karena fungsi biaya melibatkan rata-rata biaya tertentu per contoh pada

50
00:03:22,211 --> 00:03:26,841
puluhan ribu contoh pelatihan, cara kita menyesuaikan bobot dan bias

51
00:03:26,841 --> 00:03:31,740
untuk satu langkah penurunan gradien juga bergantung pada setiap contoh.

52
00:03:31,740 --> 00:03:34,314
Atau lebih tepatnya, pada prinsipnya memang seharusnya demikian,

53
00:03:34,314 --> 00:03:36,928
namun untuk efisiensi komputasi, kami akan melakukan sedikit trik

54
00:03:36,928 --> 00:03:39,860
nanti agar Anda tidak perlu melakukan setiap contoh untuk setiap langkah.

55
00:03:39,860 --> 00:03:43,208
Dalam kasus lain, saat ini, yang akan kita lakukan hanyalah

56
00:03:43,208 --> 00:03:46,780
memusatkan perhatian kita pada satu contoh, gambar angka 2 ini.

57
00:03:46,780 --> 00:03:51,740
Apa pengaruh contoh pelatihan ini terhadap penyesuaian bobot dan bias?

58
00:03:51,740 --> 00:03:57,228
Katakanlah kita berada pada titik di mana jaringan belum terlatih dengan baik, sehingga

59
00:03:57,228 --> 00:04:02,780
aktivasi pada output akan terlihat acak, mungkin sekitar 0.5, 0.8, 0.2, terus dan terus.

60
00:04:02,780 --> 00:04:06,373
Kita tidak dapat secara langsung mengubah aktivasi tersebut, kita hanya mempunyai

61
00:04:06,373 --> 00:04:09,659
pengaruh pada bobot dan bias, namun akan sangat membantu jika kita melacak

62
00:04:09,659 --> 00:04:13,340
penyesuaian mana yang kita inginkan untuk dilakukan pada lapisan keluaran tersebut.

63
00:04:13,340 --> 00:04:17,580
Dan karena kita ingin mengklasifikasikan gambar sebagai 2, kita ingin

64
00:04:17,580 --> 00:04:21,700
nilai ketiga tersebut dinaikkan sementara nilai lainnya diturunkan.

65
00:04:21,700 --> 00:04:25,637
Selain itu, ukuran dorongan ini harus sebanding dengan

66
00:04:25,637 --> 00:04:30,220
seberapa jauh jarak setiap nilai saat ini dari nilai targetnya.

67
00:04:30,220 --> 00:04:36,140
Misalnya, peningkatan aktivasi neuron nomor 2 dalam arti tertentu lebih penting daripada

68
00:04:36,140 --> 00:04:42,060
penurunan aktivasi neuron nomor 8, yang sudah cukup dekat dengan tempat yang seharusnya.

69
00:04:42,060 --> 00:04:44,954
Jadi jika kita perbesar lebih jauh, mari kita fokus pada

70
00:04:44,954 --> 00:04:47,900
satu neuron saja, yang aktivasinya ingin kita tingkatkan.

71
00:04:47,900 --> 00:04:52,626
Ingat, aktivasi tersebut didefinisikan sebagai jumlah tertimbang tertentu dari

72
00:04:52,626 --> 00:04:57,173
semua aktivasi di lapisan sebelumnya, ditambah bias, yang semuanya kemudian

73
00:04:57,173 --> 00:05:01,900
dimasukkan ke dalam sesuatu seperti fungsi squishification sigmoid, atau ReLU.

74
00:05:01,900 --> 00:05:05,105
Jadi ada tiga cara berbeda yang dapat bekerja sama

75
00:05:05,105 --> 00:05:08,060
untuk membantu meningkatkan aktivasi tersebut.

76
00:05:08,060 --> 00:05:11,648
Anda dapat meningkatkan bias, Anda dapat menambah bobot,

77
00:05:11,648 --> 00:05:15,300
dan Anda dapat mengubah aktivasi dari lapisan sebelumnya.

78
00:05:15,300 --> 00:05:18,210
Berfokus pada bagaimana bobot harus disesuaikan, perhatikan

79
00:05:18,210 --> 00:05:21,460
bagaimana bobot sebenarnya memiliki tingkat pengaruh yang berbeda.

80
00:05:21,460 --> 00:05:26,346
Koneksi dengan neuron paling terang dari lapisan sebelumnya memiliki pengaruh

81
00:05:26,346 --> 00:05:31,420
terbesar karena bobot tersebut dikalikan dengan nilai aktivasi yang lebih besar.

82
00:05:31,420 --> 00:05:35,505
Jadi jika Anda meningkatkan salah satu bobot tersebut, hal ini sebenarnya memiliki

83
00:05:35,505 --> 00:05:39,491
pengaruh yang lebih kuat pada fungsi biaya akhir dibandingkan meningkatkan bobot

84
00:05:39,491 --> 00:05:43,773
koneksi dengan neuron peredup, setidaknya sejauh menyangkut contoh pelatihan yang satu

85
00:05:43,773 --> 00:05:44,020
ini.

86
00:05:44,020 --> 00:05:47,256
Ingat, ketika kita berbicara tentang penurunan gradien, kita tidak

87
00:05:47,256 --> 00:05:50,590
hanya peduli apakah setiap komponen harus dinaikkan atau diturunkan,

88
00:05:50,590 --> 00:05:54,020
kita juga peduli tentang komponen mana yang memberikan hasil maksimal.

89
00:05:54,020 --> 00:05:58,382
Omong-omong, hal ini setidaknya mengingatkan kita pada teori dalam ilmu saraf

90
00:05:58,382 --> 00:06:02,801
tentang bagaimana jaringan biologis neuron belajar, teori Hebbian, yang sering

91
00:06:02,801 --> 00:06:06,940
diringkas dalam frasa, neuron yang menyala bersama-sama saling terhubung.

92
00:06:06,940 --> 00:06:12,264
Di sini, peningkatan bobot terbesar, penguatan koneksi terbesar, terjadi

93
00:06:12,264 --> 00:06:18,100
antara neuron yang paling aktif dan neuron yang ingin kita jadikan lebih aktif.

94
00:06:18,100 --> 00:06:21,877
Dalam arti tertentu, neuron yang terpicu saat melihat angka 2 menjadi

95
00:06:21,877 --> 00:06:25,440
lebih terkait erat dengan neuron yang terpicu saat memikirkannya.

96
00:06:25,440 --> 00:06:29,495
Untuk lebih jelasnya, saya tidak dalam posisi untuk membuat pernyataan dengan satu

97
00:06:29,495 --> 00:06:33,306
atau lain cara tentang apakah jaringan neuron buatan berperilaku seperti otak

98
00:06:33,306 --> 00:06:37,362
biologis, dan gagasan yang menyatu ini disertai dengan beberapa tanda bintang yang

99
00:06:37,362 --> 00:06:41,760
bermakna, tetapi dianggap sangat longgar. analoginya, menurut saya menarik untuk disimak.

100
00:06:41,760 --> 00:06:45,700
Bagaimanapun, cara ketiga untuk membantu meningkatkan aktivasi neuron

101
00:06:45,700 --> 00:06:49,360
ini adalah dengan mengubah semua aktivasi di lapisan sebelumnya.

102
00:06:49,360 --> 00:06:53,740
Yaitu, jika semua yang terhubung ke neuron digit 2 yang berbobot positif

103
00:06:53,740 --> 00:06:58,120
menjadi lebih terang, dan jika semua yang terhubung dengan bobot negatif

104
00:06:58,120 --> 00:07:02,680
menjadi lebih redup, maka neuron digit 2 tersebut akan menjadi lebih aktif.

105
00:07:02,680 --> 00:07:06,760
Mirip dengan perubahan berat badan, Anda akan mendapatkan hasil maksimal

106
00:07:06,760 --> 00:07:10,840
dengan mencari perubahan yang sebanding dengan ukuran beban yang sesuai.

107
00:07:10,840 --> 00:07:14,817
Tentu saja, kami tidak dapat secara langsung mempengaruhi aktivasi

108
00:07:14,817 --> 00:07:18,320
tersebut, kami hanya memiliki kendali atas bobot dan bias.

109
00:07:18,320 --> 00:07:21,083
Namun sama seperti lapisan terakhir, ada baiknya

110
00:07:21,083 --> 00:07:23,960
untuk mencatat perubahan apa saja yang diinginkan.

111
00:07:23,960 --> 00:07:26,867
Namun perlu diingat, memperkecil satu langkah di sini,

112
00:07:26,867 --> 00:07:30,040
ini hanya yang diinginkan oleh neuron keluaran digit 2 itu.

113
00:07:30,040 --> 00:07:34,369
Ingat, kita juga ingin semua neuron lain di lapisan terakhir menjadi kurang

114
00:07:34,369 --> 00:07:38,870
aktif, dan masing-masing neuron keluaran lainnya memiliki pemikirannya sendiri

115
00:07:38,870 --> 00:07:43,200
tentang apa yang harus terjadi pada lapisan kedua hingga terakhir tersebut.

116
00:07:43,200 --> 00:07:47,745
Jadi keinginan neuron digit 2 ini dijumlahkan dengan keinginan semua neuron

117
00:07:47,745 --> 00:07:52,350
keluaran lainnya untuk apa yang seharusnya terjadi pada lapisan kedua hingga

118
00:07:52,350 --> 00:07:56,895
terakhir ini, sekali lagi sebanding dengan bobot yang sesuai, dan sebanding

119
00:07:56,895 --> 00:08:01,740
dengan seberapa banyak kebutuhan masing-masing neuron tersebut. Untuk mengganti.

120
00:08:01,740 --> 00:08:05,940
Di sinilah muncul ide untuk melakukan propaganda ke belakang.

121
00:08:05,940 --> 00:08:10,094
Dengan menambahkan semua efek yang diinginkan ini, pada dasarnya Anda mendapatkan

122
00:08:10,094 --> 00:08:14,300
daftar dorongan yang Anda inginkan terjadi pada lapisan kedua hingga terakhir ini.

123
00:08:14,300 --> 00:08:19,071
Dan setelah Anda memilikinya, Anda dapat menerapkan proses yang sama secara

124
00:08:19,071 --> 00:08:24,094
rekursif pada bobot dan bias relevan yang menentukan nilai tersebut, mengulangi

125
00:08:24,094 --> 00:08:29,180
proses yang sama yang baru saja saya lalui dan bergerak mundur melalui jaringan.

126
00:08:29,180 --> 00:08:33,291
Dan jika diperbesar sedikit lagi, ingatlah bahwa ini adalah bagaimana

127
00:08:33,291 --> 00:08:37,520
sebuah contoh pelatihan ingin mendorong setiap bobot dan bias tersebut.

128
00:08:37,520 --> 00:08:40,914
Jika kita hanya mendengarkan apa yang diinginkan oleh 2, jaringan pada akhirnya

129
00:08:40,914 --> 00:08:44,140
akan diberi insentif hanya untuk mengklasifikasikan semua gambar sebagai 2.

130
00:08:44,140 --> 00:08:50,113
Jadi yang Anda lakukan adalah melakukan rutinitas backprop yang sama untuk

131
00:08:50,113 --> 00:08:55,928
setiap contoh pelatihan lainnya, mencatat bagaimana masing-masing contoh

132
00:08:55,928 --> 00:09:02,300
ingin mengubah bobot dan bias, dan membuat rata-rata perubahan yang diinginkan.

133
00:09:02,300 --> 00:09:06,300
Kumpulan dorongan rata-rata untuk setiap bobot dan bias ini, secara

134
00:09:06,300 --> 00:09:10,124
sederhana, adalah gradien negatif dari fungsi biaya yang dirujuk

135
00:09:10,124 --> 00:09:14,360
dalam video terakhir, atau setidaknya sesuatu yang sebanding dengannya.

136
00:09:14,360 --> 00:09:18,240
Saya mengatakannya dengan santai hanya karena saya belum mengetahui secara tepat

137
00:09:18,240 --> 00:09:22,121
secara kuantitatif mengenai dorongan-dorongan tersebut, namun jika Anda memahami

138
00:09:22,121 --> 00:09:26,194
setiap perubahan yang baru saja saya referensikan, mengapa beberapa perubahan secara

139
00:09:26,194 --> 00:09:30,362
proporsional lebih besar daripada yang lain, dan bagaimana semuanya perlu dijumlahkan,

140
00:09:30,362 --> 00:09:34,100
Anda memahami mekanisme untuk apa yang sebenarnya dilakukan propagasi mundur.

141
00:09:34,100 --> 00:09:38,610
Ngomong-ngomong, dalam praktiknya, komputer membutuhkan waktu yang sangat lama untuk

142
00:09:38,610 --> 00:09:43,120
menjumlahkan pengaruh setiap contoh pelatihan pada setiap langkah penurunan gradien.

143
00:09:43,120 --> 00:09:45,540
Jadi inilah yang biasa dilakukan.

144
00:09:45,540 --> 00:09:49,558
Anda mengacak data pelatihan secara acak dan membaginya menjadi beberapa kelompok

145
00:09:49,558 --> 00:09:53,380
kecil, katakanlah masing-masing kelompok kecil memiliki 100 contoh pelatihan.

146
00:09:53,380 --> 00:09:56,980
Kemudian Anda menghitung langkah sesuai dengan mini-batch.

147
00:09:56,980 --> 00:10:00,831
Ini bukan gradien sebenarnya dari fungsi biaya, yang bergantung pada semua

148
00:10:00,831 --> 00:10:04,734
data pelatihan, bukan subset kecil ini, jadi ini bukan langkah menurun yang

149
00:10:04,734 --> 00:10:08,586
paling efisien, tetapi setiap mini-batch memberi Anda perkiraan yang cukup

150
00:10:08,586 --> 00:10:12,900
bagus, dan yang lebih penting itu memberi Anda kecepatan komputasi yang signifikan.

151
00:10:12,900 --> 00:10:16,790
Jika Anda merencanakan lintasan jaringan Anda di bawah permukaan biaya yang relevan,

152
00:10:16,790 --> 00:10:20,452
hal tersebut akan lebih seperti seorang pria mabuk yang tersandung tanpa tujuan

153
00:10:20,452 --> 00:10:24,067
menuruni bukit namun mengambil langkah cepat, dibandingkan dengan seorang pria

154
00:10:24,067 --> 00:10:27,912
yang menghitung dengan cermat yang menentukan arah penurunan yang tepat dari setiap

155
00:10:27,912 --> 00:10:31,620
langkah. sebelum mengambil langkah yang sangat lambat dan hati-hati ke arah itu.

156
00:10:31,620 --> 00:10:35,200
Teknik ini disebut sebagai penurunan gradien stokastik.

157
00:10:35,200 --> 00:10:40,400
Ada banyak hal yang terjadi di sini, jadi mari kita simpulkan sendiri, oke?

158
00:10:40,400 --> 00:10:44,517
Propagasi mundur adalah algoritma untuk menentukan bagaimana sebuah contoh pelatihan

159
00:10:44,517 --> 00:10:48,634
ingin mendorong bobot dan bias, tidak hanya dalam hal apakah bobot dan bias tersebut

160
00:10:48,634 --> 00:10:52,510
harus naik atau turun, namun juga dalam hal proporsi relatif terhadap perubahan

161
00:10:52,510 --> 00:10:56,240
tersebut yang menyebabkan penurunan paling cepat pada bobot dan bias. biaya.

162
00:10:56,240 --> 00:11:00,692
Langkah penurunan gradien yang sebenarnya akan melibatkan melakukan hal ini untuk semua

163
00:11:00,692 --> 00:11:05,044
puluhan dan ribuan contoh pelatihan Anda dan merata-ratakan perubahan yang diinginkan

164
00:11:05,044 --> 00:11:09,547
yang Anda dapatkan, tapi itu lambat secara komputasi, jadi Anda membagi data secara acak

165
00:11:09,547 --> 00:11:14,000
menjadi kumpulan kecil dan menghitung setiap langkah sehubungan dengan a kumpulan mini.

166
00:11:14,000 --> 00:11:18,494
Dengan berulang kali memeriksa semua mini-batch dan melakukan penyesuaian ini,

167
00:11:18,494 --> 00:11:23,045
Anda akan menyatu menuju fungsi biaya minimum lokal, yang berarti jaringan Anda

168
00:11:23,045 --> 00:11:27,540
pada akhirnya akan melakukan pekerjaan yang sangat baik pada contoh pelatihan.

169
00:11:27,540 --> 00:11:31,122
Jadi dengan semua hal tersebut, setiap baris kode yang digunakan

170
00:11:31,122 --> 00:11:34,428
untuk mengimplementasikan backprop sebenarnya sesuai dengan

171
00:11:34,428 --> 00:11:37,680
sesuatu yang telah Anda lihat, setidaknya secara informal.

172
00:11:37,680 --> 00:11:41,188
Namun terkadang, mengetahui fungsi matematika hanyalah setengah dari perjuangan, dan

173
00:11:41,188 --> 00:11:44,780
hanya mewakili matematika saja sudah membuat semuanya menjadi kacau dan membingungkan.

174
00:11:44,780 --> 00:11:47,994
Jadi, bagi Anda yang ingin mendalami lebih dalam, video berikutnya akan

175
00:11:47,994 --> 00:11:51,253
membahas ide-ide yang sama yang baru saja disajikan di sini, namun dalam

176
00:11:51,253 --> 00:11:54,513
kaitannya dengan kalkulus yang mendasarinya, yang semoga akan membuatnya

177
00:11:54,513 --> 00:11:57,460
lebih familiar saat Anda melihat topiknya di sumber daya lainnya.

178
00:11:57,460 --> 00:12:00,618
Sebelum itu, satu hal yang perlu ditekankan adalah agar algoritme

179
00:12:00,618 --> 00:12:03,585
ini berfungsi, dan ini berlaku untuk semua jenis pembelajaran

180
00:12:03,585 --> 00:12:06,840
mesin selain jaringan saraf, Anda memerlukan banyak data pelatihan.

181
00:12:06,840 --> 00:12:09,686
Dalam kasus kami, satu hal yang membuat angka tulisan tangan

182
00:12:09,686 --> 00:12:12,626
menjadi contoh yang bagus adalah adanya database MNIST, dengan

183
00:12:12,626 --> 00:12:15,380
begitu banyak contoh yang telah diberi label oleh manusia.

184
00:12:15,380 --> 00:12:18,500
Jadi, tantangan umum yang biasa dihadapi oleh Anda yang bekerja di bidang

185
00:12:18,500 --> 00:12:21,411
pembelajaran mesin hanyalah mendapatkan data pelatihan berlabel yang

186
00:12:21,411 --> 00:12:24,405
benar-benar Anda perlukan, apakah itu meminta orang memberi label pada

187
00:12:24,405 --> 00:12:27,400
puluhan ribu gambar, atau jenis data apa pun yang mungkin Anda hadapi.

