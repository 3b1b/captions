1
00:00:00,000 --> 00:00:05,276
Di sini, kami menangani propagasi mundur, algoritma

2
00:00:05,276 --> 00:00:09,640
inti di balik cara jaringan saraf belajar.

3
00:00:09,640 --> 00:00:11,704
Setelah rekap singkat mengenai keadaan kita saat ini,

4
00:00:11,704 --> 00:00:14,112
hal pertama yang akan saya lakukan adalah penelusuran intuitif

5
00:00:14,112 --> 00:00:17,400
tentang apa yang sebenarnya dilakukan algoritme, tanpa referensi apa pun ke rumusnya.

6
00:00:17,400 --> 00:00:20,238
Lalu, bagi Anda yang memang ingin mendalami matematika,

7
00:00:20,238 --> 00:00:24,040
video selanjutnya akan membahas tentang kalkulus yang mendasari semua itu.

8
00:00:24,040 --> 00:00:26,263
Jika Anda menonton dua video terakhir, atau jika Anda hanya

9
00:00:26,263 --> 00:00:29,079
melihat latar belakang yang sesuai, Anda pasti tahu apa itu jaringan saraf,

10
00:00:29,079 --> 00:00:31,080
dan bagaimana jaringan tersebut meneruskan informasi.

11
00:00:31,080 --> 00:00:34,748
Di sini, kita melakukan contoh klasik dalam mengenali angka tulisan tangan

12
00:00:34,748 --> 00:00:38,905
yang nilai pikselnya dimasukkan ke dalam lapisan pertama jaringan dengan 784 neuron,

13
00:00:38,905 --> 00:00:42,427
dan saya telah menunjukkan jaringan dengan dua lapisan tersembunyi yang

14
00:00:42,427 --> 00:00:46,340
masing-masing hanya memiliki 16 neuron, dan sebuah keluaran. lapisan 10 neuron,

15
00:00:46,340 --> 00:00:49,520
menunjukkan digit mana yang dipilih jaringan sebagai jawabannya.

16
00:00:49,520 --> 00:00:52,500
Saya juga mengharapkan Anda memahami penurunan gradien,

17
00:00:52,500 --> 00:00:56,651
seperti yang dijelaskan dalam video terakhir, dan apa yang kami maksud dengan

18
00:00:56,651 --> 00:01:00,855
pembelajaran adalah kami ingin menemukan bobot dan bias mana yang meminimalkan

19
00:01:00,855 --> 00:01:02,080
fungsi biaya tertentu.

20
00:01:02,080 --> 00:01:05,811
Sebagai pengingat singkat, untuk biaya satu contoh pelatihan,

21
00:01:05,811 --> 00:01:08,759
Anda mengambil keluaran yang diberikan jaringan,

22
00:01:08,759 --> 00:01:11,708
bersama dengan keluaran yang ingin Anda berikan,

23
00:01:11,708 --> 00:01:15,560
dan menjumlahkan kuadrat selisih antara masing-masing komponen.

24
00:01:15,560 --> 00:01:20,546
Melakukan hal ini untuk puluhan ribu contoh pelatihan Anda dan merata-ratakan hasilnya,

25
00:01:20,546 --> 00:01:23,040
ini akan memberi Anda total biaya jaringan.

26
00:01:23,040 --> 00:01:27,953
Seolah-olah itu belum cukup untuk dipikirkan, seperti yang dijelaskan dalam

27
00:01:27,953 --> 00:01:33,189
video terakhir, hal yang kita cari adalah gradien negatif dari fungsi biaya ini,

28
00:01:33,189 --> 00:01:38,037
yang memberi tahu Anda bagaimana Anda perlu mengubah semua bobot dan bias,

29
00:01:38,037 --> 00:01:43,080
semuanya. koneksi ini, sehingga dapat mengurangi biaya secara paling efisien.

30
00:01:43,080 --> 00:01:46,611
Propagasi mundur, topik video ini, adalah algoritma

31
00:01:46,611 --> 00:01:49,600
untuk menghitung gradien yang sangat rumit.

32
00:01:49,600 --> 00:01:54,743
Satu gagasan dari video terakhir yang saya benar-benar ingin Anda ingat saat ini adalah

33
00:01:54,743 --> 00:01:59,769
karena memikirkan vektor gradien sebagai arah dalam 13.000 dimensi, secara sederhana,

34
00:01:59,769 --> 00:02:04,620
di luar jangkauan imajinasi kita, ada gagasan lain. cara Anda dapat memikirkannya.

35
00:02:04,620 --> 00:02:08,186
Besaran setiap komponen di sini menunjukkan seberapa

36
00:02:08,186 --> 00:02:11,820
sensitif fungsi biaya terhadap setiap bobot dan bias.

37
00:02:11,820 --> 00:02:17,670
Misalnya, Anda menjalani proses yang akan saya jelaskan, dan menghitung gradien negatif,

38
00:02:17,670 --> 00:02:21,943
dan komponen yang terkait dengan bobot pada tepi ini adalah 3.2,

39
00:02:21,943 --> 00:02:26,940
sedangkan komponen yang terkait dengan tepi ini di sini keluar sebagai 0.1.

40
00:02:26,940 --> 00:02:31,517
Cara Anda menafsirkannya adalah bahwa biaya fungsi tersebut 32 kali lebih sensitif

41
00:02:31,517 --> 00:02:36,315
terhadap perubahan bobot pertama, jadi jika Anda menggoyangkan nilai tersebut sedikit,

42
00:02:36,315 --> 00:02:39,403
hal ini akan menyebabkan beberapa perubahan pada biaya,

43
00:02:39,403 --> 00:02:44,035
dan perubahan itu adalah 32 kali lebih besar dari apa yang dihasilkan oleh goyangan

44
00:02:44,035 --> 00:02:45,580
yang sama pada beban kedua.

45
00:02:45,580 --> 00:02:49,985
Secara pribadi, ketika saya pertama kali belajar tentang backpropagation,

46
00:02:49,985 --> 00:02:55,224
menurut saya aspek yang paling membingungkan hanyalah notasi dan pengejaran indeks dari

47
00:02:55,224 --> 00:02:55,820
semuanya.

48
00:02:55,820 --> 00:02:59,722
Namun begitu Anda mengungkap apa yang sebenarnya dilakukan setiap bagian

49
00:02:59,722 --> 00:03:03,784
dari algoritme ini, setiap efek yang dimilikinya sebenarnya cukup intuitif,

50
00:03:03,784 --> 00:03:07,740
hanya saja ada banyak penyesuaian kecil yang ditumpangkan satu sama lain.

51
00:03:07,740 --> 00:03:12,288
Jadi saya akan memulai semuanya di sini dengan mengabaikan notasi,

52
00:03:12,288 --> 00:03:17,380
dan hanya menelusuri efek setiap contoh pelatihan terhadap bobot dan bias.

53
00:03:17,380 --> 00:03:22,211
Karena fungsi biaya melibatkan rata-rata biaya tertentu per contoh pada

54
00:03:22,211 --> 00:03:26,841
puluhan ribu contoh pelatihan, cara kita menyesuaikan bobot dan bias

55
00:03:26,841 --> 00:03:31,740
untuk satu langkah penurunan gradien juga bergantung pada setiap contoh.

56
00:03:31,740 --> 00:03:34,314
Atau lebih tepatnya, pada prinsipnya memang seharusnya demikian,

57
00:03:34,314 --> 00:03:36,928
namun untuk efisiensi komputasi, kami akan melakukan sedikit trik

58
00:03:36,928 --> 00:03:39,860
nanti agar Anda tidak perlu melakukan setiap contoh untuk setiap langkah.

59
00:03:39,860 --> 00:03:43,208
Dalam kasus lain, saat ini, yang akan kita lakukan hanyalah

60
00:03:43,208 --> 00:03:46,780
memusatkan perhatian kita pada satu contoh, gambar angka 2 ini.

61
00:03:46,780 --> 00:03:51,740
Apa pengaruh contoh pelatihan ini terhadap penyesuaian bobot dan bias?

62
00:03:51,740 --> 00:03:56,667
Katakanlah kita berada pada titik di mana jaringan belum terlatih dengan baik,

63
00:03:56,667 --> 00:04:01,719
sehingga aktivasi pada output akan terlihat acak, mungkin sekitar 0.5, 0.8, 0.2,

64
00:04:01,719 --> 00:04:02,780
terus dan terus.

65
00:04:02,780 --> 00:04:05,452
Kita tidak dapat secara langsung mengubah aktivasi tersebut,

66
00:04:05,452 --> 00:04:07,687
kita hanya mempunyai pengaruh pada bobot dan bias,

67
00:04:07,687 --> 00:04:11,236
namun akan sangat membantu jika kita melacak penyesuaian mana yang kita inginkan

68
00:04:11,236 --> 00:04:13,340
untuk dilakukan pada lapisan keluaran tersebut.

69
00:04:13,340 --> 00:04:16,914
Dan karena kita ingin mengklasifikasikan gambar sebagai 2,

70
00:04:16,914 --> 00:04:21,700
kita ingin nilai ketiga tersebut dinaikkan sementara nilai lainnya diturunkan.

71
00:04:21,700 --> 00:04:25,637
Selain itu, ukuran dorongan ini harus sebanding dengan

72
00:04:25,637 --> 00:04:30,220
seberapa jauh jarak setiap nilai saat ini dari nilai targetnya.

73
00:04:30,220 --> 00:04:36,140
Misalnya, peningkatan aktivasi neuron nomor 2 dalam arti tertentu lebih penting daripada

74
00:04:36,140 --> 00:04:42,060
penurunan aktivasi neuron nomor 8, yang sudah cukup dekat dengan tempat yang seharusnya.

75
00:04:42,060 --> 00:04:45,868
Jadi jika kita perbesar lebih jauh, mari kita fokus pada satu neuron saja,

76
00:04:45,868 --> 00:04:47,900
yang aktivasinya ingin kita tingkatkan.

77
00:04:47,900 --> 00:04:52,626
Ingat, aktivasi tersebut didefinisikan sebagai jumlah tertimbang tertentu dari

78
00:04:52,626 --> 00:04:55,797
semua aktivasi di lapisan sebelumnya, ditambah bias,

79
00:04:55,797 --> 00:05:00,703
yang semuanya kemudian dimasukkan ke dalam sesuatu seperti fungsi squishification

80
00:05:00,703 --> 00:05:01,900
sigmoid, atau ReLU.

81
00:05:01,900 --> 00:05:05,105
Jadi ada tiga cara berbeda yang dapat bekerja sama

82
00:05:05,105 --> 00:05:08,060
untuk membantu meningkatkan aktivasi tersebut.

83
00:05:08,060 --> 00:05:11,648
Anda dapat meningkatkan bias, Anda dapat menambah bobot,

84
00:05:11,648 --> 00:05:15,300
dan Anda dapat mengubah aktivasi dari lapisan sebelumnya.

85
00:05:15,300 --> 00:05:17,676
Berfokus pada bagaimana bobot harus disesuaikan,

86
00:05:17,676 --> 00:05:21,460
perhatikan bagaimana bobot sebenarnya memiliki tingkat pengaruh yang berbeda.

87
00:05:21,460 --> 00:05:26,346
Koneksi dengan neuron paling terang dari lapisan sebelumnya memiliki pengaruh

88
00:05:26,346 --> 00:05:31,420
terbesar karena bobot tersebut dikalikan dengan nilai aktivasi yang lebih besar.

89
00:05:31,420 --> 00:05:34,127
Jadi jika Anda meningkatkan salah satu bobot tersebut,

90
00:05:34,127 --> 00:05:38,556
hal ini sebenarnya memiliki pengaruh yang lebih kuat pada fungsi biaya akhir dibandingkan

91
00:05:38,556 --> 00:05:41,017
meningkatkan bobot koneksi dengan neuron peredup,

92
00:05:41,017 --> 00:05:44,020
setidaknya sejauh menyangkut contoh pelatihan yang satu ini.

93
00:05:44,020 --> 00:05:46,725
Ingat, ketika kita berbicara tentang penurunan gradien,

94
00:05:46,725 --> 00:05:50,590
kita tidak hanya peduli apakah setiap komponen harus dinaikkan atau diturunkan,

95
00:05:50,590 --> 00:05:54,020
kita juga peduli tentang komponen mana yang memberikan hasil maksimal.

96
00:05:54,020 --> 00:05:58,382
Omong-omong, hal ini setidaknya mengingatkan kita pada teori dalam ilmu saraf

97
00:05:58,382 --> 00:06:02,129
tentang bagaimana jaringan biologis neuron belajar, teori Hebbian,

98
00:06:02,129 --> 00:06:06,940
yang sering diringkas dalam frasa, neuron yang menyala bersama-sama saling terhubung.

99
00:06:06,940 --> 00:06:11,681
Di sini, peningkatan bobot terbesar, penguatan koneksi terbesar,

100
00:06:11,681 --> 00:06:18,100
terjadi antara neuron yang paling aktif dan neuron yang ingin kita jadikan lebih aktif.

101
00:06:18,100 --> 00:06:21,877
Dalam arti tertentu, neuron yang terpicu saat melihat angka 2 menjadi

102
00:06:21,877 --> 00:06:25,440
lebih terkait erat dengan neuron yang terpicu saat memikirkannya.

103
00:06:25,440 --> 00:06:29,495
Untuk lebih jelasnya, saya tidak dalam posisi untuk membuat pernyataan dengan satu

104
00:06:29,495 --> 00:06:33,795
atau lain cara tentang apakah jaringan neuron buatan berperilaku seperti otak biologis,

105
00:06:33,795 --> 00:06:37,851
dan gagasan yang menyatu ini disertai dengan beberapa tanda bintang yang bermakna,

106
00:06:37,851 --> 00:06:41,760
tetapi dianggap sangat longgar. analoginya, menurut saya menarik untuk disimak.

107
00:06:41,760 --> 00:06:45,700
Bagaimanapun, cara ketiga untuk membantu meningkatkan aktivasi neuron

108
00:06:45,700 --> 00:06:49,360
ini adalah dengan mengubah semua aktivasi di lapisan sebelumnya.

109
00:06:49,360 --> 00:06:53,740
Yaitu, jika semua yang terhubung ke neuron digit 2 yang berbobot positif

110
00:06:53,740 --> 00:06:58,120
menjadi lebih terang, dan jika semua yang terhubung dengan bobot negatif

111
00:06:58,120 --> 00:07:02,680
menjadi lebih redup, maka neuron digit 2 tersebut akan menjadi lebih aktif.

112
00:07:02,680 --> 00:07:06,760
Mirip dengan perubahan berat badan, Anda akan mendapatkan hasil maksimal

113
00:07:06,760 --> 00:07:10,840
dengan mencari perubahan yang sebanding dengan ukuran beban yang sesuai.

114
00:07:10,840 --> 00:07:15,411
Tentu saja, kami tidak dapat secara langsung mempengaruhi aktivasi tersebut,

115
00:07:15,411 --> 00:07:18,320
kami hanya memiliki kendali atas bobot dan bias.

116
00:07:18,320 --> 00:07:21,083
Namun sama seperti lapisan terakhir, ada baiknya

117
00:07:21,083 --> 00:07:23,960
untuk mencatat perubahan apa saja yang diinginkan.

118
00:07:23,960 --> 00:07:26,867
Namun perlu diingat, memperkecil satu langkah di sini,

119
00:07:26,867 --> 00:07:30,040
ini hanya yang diinginkan oleh neuron keluaran digit 2 itu.

120
00:07:30,040 --> 00:07:34,768
Ingat, kita juga ingin semua neuron lain di lapisan terakhir menjadi kurang aktif,

121
00:07:34,768 --> 00:07:39,326
dan masing-masing neuron keluaran lainnya memiliki pemikirannya sendiri tentang

122
00:07:39,326 --> 00:07:43,200
apa yang harus terjadi pada lapisan kedua hingga terakhir tersebut.

123
00:07:43,200 --> 00:07:47,745
Jadi keinginan neuron digit 2 ini dijumlahkan dengan keinginan semua neuron

124
00:07:47,745 --> 00:07:52,350
keluaran lainnya untuk apa yang seharusnya terjadi pada lapisan kedua hingga

125
00:07:52,350 --> 00:07:56,058
terakhir ini, sekali lagi sebanding dengan bobot yang sesuai,

126
00:07:56,058 --> 00:08:00,723
dan sebanding dengan seberapa banyak kebutuhan masing-masing neuron tersebut.

127
00:08:00,723 --> 00:08:01,740
Untuk mengganti.

128
00:08:01,740 --> 00:08:05,940
Di sinilah muncul ide untuk melakukan propaganda ke belakang.

129
00:08:05,940 --> 00:08:08,524
Dengan menambahkan semua efek yang diinginkan ini,

130
00:08:08,524 --> 00:08:12,526
pada dasarnya Anda mendapatkan daftar dorongan yang Anda inginkan terjadi pada

131
00:08:12,526 --> 00:08:14,300
lapisan kedua hingga terakhir ini.

132
00:08:14,300 --> 00:08:19,071
Dan setelah Anda memilikinya, Anda dapat menerapkan proses yang sama secara

133
00:08:19,071 --> 00:08:23,403
rekursif pada bobot dan bias relevan yang menentukan nilai tersebut,

134
00:08:23,403 --> 00:08:28,552
mengulangi proses yang sama yang baru saja saya lalui dan bergerak mundur melalui

135
00:08:28,552 --> 00:08:29,180
jaringan.

136
00:08:29,180 --> 00:08:33,291
Dan jika diperbesar sedikit lagi, ingatlah bahwa ini adalah bagaimana

137
00:08:33,291 --> 00:08:37,520
sebuah contoh pelatihan ingin mendorong setiap bobot dan bias tersebut.

138
00:08:37,520 --> 00:08:39,938
Jika kita hanya mendengarkan apa yang diinginkan oleh 2,

139
00:08:39,938 --> 00:08:43,121
jaringan pada akhirnya akan diberi insentif hanya untuk mengklasifikasikan

140
00:08:43,121 --> 00:08:44,140
semua gambar sebagai 2.

141
00:08:44,140 --> 00:08:50,113
Jadi yang Anda lakukan adalah melakukan rutinitas backprop yang sama untuk

142
00:08:50,113 --> 00:08:55,928
setiap contoh pelatihan lainnya, mencatat bagaimana masing-masing contoh

143
00:08:55,928 --> 00:09:02,300
ingin mengubah bobot dan bias, dan membuat rata-rata perubahan yang diinginkan.

144
00:09:02,300 --> 00:09:05,888
Kumpulan dorongan rata-rata untuk setiap bobot dan bias ini,

145
00:09:05,888 --> 00:09:09,653
secara sederhana, adalah gradien negatif dari fungsi biaya yang

146
00:09:09,653 --> 00:09:14,360
dirujuk dalam video terakhir, atau setidaknya sesuatu yang sebanding dengannya.

147
00:09:14,360 --> 00:09:18,240
Saya mengatakannya dengan santai hanya karena saya belum mengetahui secara tepat

148
00:09:18,240 --> 00:09:20,924
secara kuantitatif mengenai dorongan-dorongan tersebut,

149
00:09:20,924 --> 00:09:24,565
namun jika Anda memahami setiap perubahan yang baru saja saya referensikan,

150
00:09:24,565 --> 00:09:28,350
mengapa beberapa perubahan secara proporsional lebih besar daripada yang lain,

151
00:09:28,350 --> 00:09:32,231
dan bagaimana semuanya perlu dijumlahkan, Anda memahami mekanisme untuk apa yang

152
00:09:32,231 --> 00:09:34,100
sebenarnya dilakukan propagasi mundur.

153
00:09:34,100 --> 00:09:38,610
Ngomong-ngomong, dalam praktiknya, komputer membutuhkan waktu yang sangat lama untuk

154
00:09:38,610 --> 00:09:43,120
menjumlahkan pengaruh setiap contoh pelatihan pada setiap langkah penurunan gradien.

155
00:09:43,120 --> 00:09:45,540
Jadi inilah yang biasa dilakukan.

156
00:09:45,540 --> 00:09:49,901
Anda mengacak data pelatihan secara acak dan membaginya menjadi beberapa kelompok kecil,

157
00:09:49,901 --> 00:09:53,380
katakanlah masing-masing kelompok kecil memiliki 100 contoh pelatihan.

158
00:09:53,380 --> 00:09:56,980
Kemudian Anda menghitung langkah sesuai dengan mini-batch.

159
00:09:56,980 --> 00:09:59,445
Ini bukan gradien sebenarnya dari fungsi biaya,

160
00:09:59,445 --> 00:10:02,885
yang bergantung pada semua data pelatihan, bukan subset kecil ini,

161
00:10:02,885 --> 00:10:05,556
jadi ini bukan langkah menurun yang paling efisien,

162
00:10:05,556 --> 00:10:08,945
tetapi setiap mini-batch memberi Anda perkiraan yang cukup bagus,

163
00:10:08,945 --> 00:10:12,900
dan yang lebih penting itu memberi Anda kecepatan komputasi yang signifikan.

164
00:10:12,900 --> 00:10:16,790
Jika Anda merencanakan lintasan jaringan Anda di bawah permukaan biaya yang relevan,

165
00:10:16,790 --> 00:10:20,452
hal tersebut akan lebih seperti seorang pria mabuk yang tersandung tanpa tujuan

166
00:10:20,452 --> 00:10:24,067
menuruni bukit namun mengambil langkah cepat, dibandingkan dengan seorang pria

167
00:10:24,067 --> 00:10:27,912
yang menghitung dengan cermat yang menentukan arah penurunan yang tepat dari setiap

168
00:10:27,912 --> 00:10:31,620
langkah. sebelum mengambil langkah yang sangat lambat dan hati-hati ke arah itu.

169
00:10:31,620 --> 00:10:35,200
Teknik ini disebut sebagai penurunan gradien stokastik.

170
00:10:35,200 --> 00:10:40,400
Ada banyak hal yang terjadi di sini, jadi mari kita simpulkan sendiri, oke?

171
00:10:40,400 --> 00:10:44,517
Propagasi mundur adalah algoritma untuk menentukan bagaimana sebuah contoh pelatihan

172
00:10:44,517 --> 00:10:48,634
ingin mendorong bobot dan bias, tidak hanya dalam hal apakah bobot dan bias tersebut

173
00:10:48,634 --> 00:10:52,510
harus naik atau turun, namun juga dalam hal proporsi relatif terhadap perubahan

174
00:10:52,510 --> 00:10:56,240
tersebut yang menyebabkan penurunan paling cepat pada bobot dan bias. biaya.

175
00:10:56,240 --> 00:11:00,692
Langkah penurunan gradien yang sebenarnya akan melibatkan melakukan hal ini untuk semua

176
00:11:00,692 --> 00:11:05,044
puluhan dan ribuan contoh pelatihan Anda dan merata-ratakan perubahan yang diinginkan

177
00:11:05,044 --> 00:11:07,776
yang Anda dapatkan, tapi itu lambat secara komputasi,

178
00:11:07,776 --> 00:11:12,229
jadi Anda membagi data secara acak menjadi kumpulan kecil dan menghitung setiap langkah

179
00:11:12,229 --> 00:11:14,000
sehubungan dengan a kumpulan mini.

180
00:11:14,000 --> 00:11:18,494
Dengan berulang kali memeriksa semua mini-batch dan melakukan penyesuaian ini,

181
00:11:18,494 --> 00:11:21,509
Anda akan menyatu menuju fungsi biaya minimum lokal,

182
00:11:21,509 --> 00:11:25,947
yang berarti jaringan Anda pada akhirnya akan melakukan pekerjaan yang sangat

183
00:11:25,947 --> 00:11:27,540
baik pada contoh pelatihan.

184
00:11:27,540 --> 00:11:31,122
Jadi dengan semua hal tersebut, setiap baris kode yang digunakan

185
00:11:31,122 --> 00:11:34,428
untuk mengimplementasikan backprop sebenarnya sesuai dengan

186
00:11:34,428 --> 00:11:37,680
sesuatu yang telah Anda lihat, setidaknya secara informal.

187
00:11:37,680 --> 00:11:41,023
Namun terkadang, mengetahui fungsi matematika hanyalah setengah dari perjuangan,

188
00:11:41,023 --> 00:11:44,738
dan hanya mewakili matematika saja sudah membuat semuanya menjadi kacau dan membingungkan.

189
00:11:44,738 --> 00:11:44,780


190
00:11:44,780 --> 00:11:47,012
Jadi, bagi Anda yang ingin mendalami lebih dalam,

191
00:11:47,012 --> 00:11:50,718
video berikutnya akan membahas ide-ide yang sama yang baru saja disajikan di sini,

192
00:11:50,718 --> 00:11:53,263
namun dalam kaitannya dengan kalkulus yang mendasarinya,

193
00:11:53,263 --> 00:11:56,388
yang semoga akan membuatnya lebih familiar saat Anda melihat topiknya

194
00:11:56,388 --> 00:11:57,460
di sumber daya lainnya.

195
00:11:57,460 --> 00:12:01,336
Sebelum itu, satu hal yang perlu ditekankan adalah agar algoritme ini berfungsi,

196
00:12:01,336 --> 00:12:04,973
dan ini berlaku untuk semua jenis pembelajaran mesin selain jaringan saraf,

197
00:12:04,973 --> 00:12:06,840
Anda memerlukan banyak data pelatihan.

198
00:12:06,840 --> 00:12:09,686
Dalam kasus kami, satu hal yang membuat angka tulisan tangan

199
00:12:09,686 --> 00:12:12,300
menjadi contoh yang bagus adalah adanya database MNIST,

200
00:12:12,300 --> 00:12:15,380
dengan begitu banyak contoh yang telah diberi label oleh manusia.

201
00:12:15,380 --> 00:12:18,500
Jadi, tantangan umum yang biasa dihadapi oleh Anda yang bekerja di bidang

202
00:12:18,500 --> 00:12:21,411
pembelajaran mesin hanyalah mendapatkan data pelatihan berlabel yang

203
00:12:21,411 --> 00:12:24,405
benar-benar Anda perlukan, apakah itu meminta orang memberi label pada

204
00:12:24,405 --> 00:12:27,400
puluhan ribu gambar, atau jenis data apa pun yang mungkin Anda hadapi.

