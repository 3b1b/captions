1
00:00:00,000 --> 00:00:09,640
Here, we tackle backpropagation, the core algorithm behind how neural networks learn.

2
00:00:09,640 --> 00:00:13,320
After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough

3
00:00:13,320 --> 00:00:17,400
for what the algorithm is actually doing, without any reference to the formulas.

4
00:00:17,400 --> 00:00:21,400
Then, for those of you who do want to dive into the math, the next video goes into the

5
00:00:21,400 --> 00:00:24,040
calculus underlying all this.

6
00:00:24,040 --> 00:00:27,320
If you watched the last two videos, or if you're just jumping in with the appropriate

7
00:00:27,320 --> 00:00:31,080
background, you know what a neural network is, and how it feeds forward information.

8
00:00:31,080 --> 00:00:35,520
Here, we're doing the classic example of recognizing handwritten digits whose pixel

9
00:00:35,520 --> 00:00:40,280
values get fed into the first layer of the network with 784 neurons, and I've been

10
00:00:40,280 --> 00:00:44,720
showing a network with two hidden layers having just 16 neurons each, and an output layer

11
00:00:44,720 --> 00:00:49,520
of 10 neurons, indicating which digit the network is choosing as its answer.

12
00:00:49,520 --> 00:00:54,480
I'm also expecting you to understand gradient descent, as described in the last video, and

13
00:00:54,480 --> 00:01:00,160
how what we mean by learning is that we want to find which weights and biases minimize

14
00:01:00,160 --> 00:01:02,080
a certain cost function.

15
00:01:02,080 --> 00:01:07,560
As a quick reminder, for the cost of a single training example, you take the output the

16
00:01:07,560 --> 00:01:12,920
network gives, along with the output you wanted it to give, and add up the squares of the

17
00:01:12,920 --> 00:01:15,560
differences between each component.

18
00:01:15,560 --> 00:01:20,160
Doing this for all of your tens of thousands of training examples and averaging the results,

19
00:01:20,160 --> 00:01:23,040
this gives you the total cost of the network.

20
00:01:23,040 --> 00:01:26,320
As if that's not enough to think about, as described in the last video, the thing

21
00:01:26,320 --> 00:01:31,700
that we're looking for is the negative gradient of this cost function, which tells you how

22
00:01:31,700 --> 00:01:36,000
you need to change all of the weights and biases, all of these connections, so as to

23
00:01:36,000 --> 00:01:43,080
most efficiently decrease the cost.

24
00:01:43,080 --> 00:01:48,600
Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated

25
00:01:48,600 --> 00:01:49,600
gradient.

26
00:01:49,600 --> 00:01:53,300
The one idea from the last video that I really want you to hold firmly in your mind right

27
00:01:53,300 --> 00:01:58,280
now is that because thinking of the gradient vector as a direction in 13,000 dimensions

28
00:01:58,280 --> 00:02:02,660
is, to put it lightly, beyond the scope of our imaginations, there's another way you

29
00:02:02,660 --> 00:02:04,620
can think about it.

30
00:02:04,620 --> 00:02:09,700
The magnitude of each component here is telling you how sensitive the cost function is to

31
00:02:09,700 --> 00:02:11,820
each weight and bias.

32
00:02:11,820 --> 00:02:15,180
For example, let's say you go through the process I'm about to describe, and compute

33
00:02:15,180 --> 00:02:19,800
the negative gradient, and the component associated with the weight on this edge here

34
00:02:19,800 --> 00:02:26,940
comes out to be 3.2, while the component associated with this edge here comes out as 0.1.

35
00:02:26,940 --> 00:02:31,520
The way you would interpret that is that the cost of the function is 32 times more sensitive

36
00:02:31,520 --> 00:02:36,100
to changes in that first weight, so if you were to wiggle that value a little bit, it's

37
00:02:36,100 --> 00:02:40,780
going to cause some change to the cost, and that change is 32 times greater than what

38
00:02:40,780 --> 00:02:45,580
the same wiggle to that second weight would give.

39
00:02:45,580 --> 00:02:52,500
Personally, when I was first learning about backpropagation, I think the most confusing

40
00:02:52,500 --> 00:02:55,820
aspect was just the notation and index chasing of it all.

41
00:02:55,820 --> 00:03:00,240
But once you unwrap what each part of this algorithm is really doing, each individual

42
00:03:00,240 --> 00:03:04,540
effect it's having is actually pretty intuitive, it's just that there's a lot of little

43
00:03:04,540 --> 00:03:07,740
adjustments getting layered on top of each other.

44
00:03:07,740 --> 00:03:11,380
So I'm going to start things off here with a complete disregard for the notation, and

45
00:03:11,380 --> 00:03:17,380
just step through the effects each training example has on the weights and biases.

46
00:03:17,380 --> 00:03:21,880
Because the cost function involves averaging a certain cost per example over all the tens

47
00:03:21,880 --> 00:03:26,980
of thousands of training examples, the way we adjust the weights and biases for a single

48
00:03:26,980 --> 00:03:31,740
gradient descent step also depends on every single example.

49
00:03:31,740 --> 00:03:35,300
Or rather, in principle it should, but for computational efficiency we'll do a little

50
00:03:35,300 --> 00:03:39,860
trick later to keep you from needing to hit every single example for every step.

51
00:03:39,860 --> 00:03:44,460
In other cases, right now, all we're going to do is focus our attention on one single

52
00:03:44,460 --> 00:03:46,780
example, this image of a 2.

53
00:03:46,780 --> 00:03:51,740
What effect should this one training example have on how the weights and biases get adjusted?

54
00:03:51,740 --> 00:03:56,040
Let's say we're at a point where the network is not well trained yet, so the activations

55
00:03:56,040 --> 00:04:01,620
in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and

56
00:04:01,620 --> 00:04:02,780
on.

57
00:04:02,780 --> 00:04:06,700
We can't directly change those activations, we only have influence on the weights and

58
00:04:06,700 --> 00:04:11,380
biases, but it's helpful to keep track of which adjustments we wish should take place

59
00:04:11,380 --> 00:04:13,340
to that output layer.

60
00:04:13,340 --> 00:04:18,220
And since we want it to classify the image as a 2, we want that third value to get nudged

61
00:04:18,220 --> 00:04:21,700
up while all the others get nudged down.

62
00:04:21,700 --> 00:04:27,620
Moreover, the sizes of these nudges should be proportional to how far away each current

63
00:04:27,620 --> 00:04:30,220
value is from its target value.

64
00:04:30,220 --> 00:04:35,260
For example, the increase to that number 2 neuron's activation is in a sense more

65
00:04:35,260 --> 00:04:39,620
important than the decrease to the number 8 neuron, which is already pretty close to

66
00:04:39,620 --> 00:04:42,060
where it should be.

67
00:04:42,060 --> 00:04:46,260
So zooming in further, let's focus just on this one neuron, the one whose activation

68
00:04:46,260 --> 00:04:47,900
we wish to increase.

69
00:04:47,900 --> 00:04:53,680
Remember, that activation is defined as a certain weighted sum of all the activations

70
00:04:53,680 --> 00:04:58,380
in the previous layer, plus a bias, which is all then plugged into something like the

71
00:04:58,380 --> 00:05:01,900
sigmoid squishification function, or a ReLU.

72
00:05:01,900 --> 00:05:07,060
So there are three different avenues that can team up together to help increase that

73
00:05:07,060 --> 00:05:08,060
activation.

74
00:05:08,060 --> 00:05:12,800
You can increase the bias, you can increase the weights, and you can change the activations

75
00:05:12,800 --> 00:05:15,300
from the previous layer.

76
00:05:15,300 --> 00:05:19,720
Focusing on how the weights should be adjusted, notice how the weights actually have differing

77
00:05:19,720 --> 00:05:21,460
levels of influence.

78
00:05:21,460 --> 00:05:25,100
The connections with the brightest neurons from the preceding layer have the biggest

79
00:05:25,100 --> 00:05:31,420
effect since those weights are multiplied by larger activation values.

80
00:05:31,420 --> 00:05:35,820
So if you were to increase one of those weights, it actually has a stronger influence on the

81
00:05:35,820 --> 00:05:40,900
ultimate cost function than increasing the weights of connections with dimmer neurons,

82
00:05:40,900 --> 00:05:44,020
at least as far as this one training example is concerned.

83
00:05:44,020 --> 00:05:48,700
Remember, when we talk about gradient descent, we don't just care about whether each component

84
00:05:48,700 --> 00:05:53,020
should get nudged up or down, we care about which ones give you the most bang for your

85
00:05:53,020 --> 00:05:54,020
buck.

86
00:05:54,020 --> 00:06:00,260
This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological

87
00:06:00,260 --> 00:06:04,900
networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that

88
00:06:04,900 --> 00:06:06,940
fire together wire together.

89
00:06:06,940 --> 00:06:12,460
Here, the biggest increases to weights, the biggest strengthening of connections, happens

90
00:06:12,460 --> 00:06:16,860
between neurons which are the most active and the ones which we wish to become more

91
00:06:16,860 --> 00:06:18,100
active.

92
00:06:18,100 --> 00:06:22,520
In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those

93
00:06:22,520 --> 00:06:25,440
firing when thinking about it.

94
00:06:25,440 --> 00:06:29,240
To be clear, I'm not in a position to make statements one way or another about whether

95
00:06:29,240 --> 00:06:34,020
artificial networks of neurons behave anything like biological brains, and this fires together

96
00:06:34,020 --> 00:06:39,440
wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy,

97
00:06:39,440 --> 00:06:41,760
I find it interesting to note.

98
00:06:41,760 --> 00:06:46,760
Anyway, the third way we can help increase this neuron's activation is by changing

99
00:06:46,760 --> 00:06:49,360
all the activations in the previous layer.

100
00:06:49,360 --> 00:06:55,080
Namely, if everything connected to that digit 2 neuron with a positive weight got brighter,

101
00:06:55,080 --> 00:06:59,480
and if everything connected with a negative weight got dimmer, then that digit 2 neuron

102
00:06:59,480 --> 00:07:02,680
would become more active.

103
00:07:02,680 --> 00:07:06,200
And similar to the weight changes, you're going to get the most bang for your buck by

104
00:07:06,200 --> 00:07:10,840
seeking changes that are proportional to the size of the corresponding weights.

105
00:07:10,840 --> 00:07:16,520
Now of course, we cannot directly influence those activations, we only have control over

106
00:07:16,520 --> 00:07:18,320
the weights and biases.

107
00:07:18,320 --> 00:07:22,960
But just as with the last layer, it's helpful to keep a note of what those desired changes

108
00:07:22,960 --> 00:07:23,960
are.

109
00:07:23,960 --> 00:07:29,040
But keep in mind, zooming out one step here, this is only what that digit 2 output neuron

110
00:07:29,040 --> 00:07:30,040
wants.

111
00:07:30,040 --> 00:07:34,960
Remember, we also want all the other neurons in the last layer to become less active, and

112
00:07:34,960 --> 00:07:38,460
each of those other output neurons has its own thoughts about what should happen to that

113
00:07:38,460 --> 00:07:43,200
second to last layer.

114
00:07:43,200 --> 00:07:49,220
So the desire of this digit 2 neuron is added together with the desires of all the other

115
00:07:49,220 --> 00:07:54,800
output neurons for what should happen to this second to last layer, again in proportion

116
00:07:54,800 --> 00:08:00,240
to the corresponding weights, and in proportion to how much each of those neurons needs to

117
00:08:00,240 --> 00:08:01,740
change.

118
00:08:01,740 --> 00:08:05,940
This right here is where the idea of propagating backwards comes in.

119
00:08:05,940 --> 00:08:11,080
By adding together all these desired effects, you basically get a list of nudges that you

120
00:08:11,080 --> 00:08:14,300
want to happen to this second to last layer.

121
00:08:14,300 --> 00:08:18,740
And once you have those, you can recursively apply the same process to the relevant weights

122
00:08:18,740 --> 00:08:23,400
and biases that determine those values, repeating the same process I just walked through and

123
00:08:23,400 --> 00:08:29,180
moving backwards through the network.

124
00:08:29,180 --> 00:08:33,960
And zooming out a bit further, remember that this is all just how a single training example

125
00:08:33,960 --> 00:08:37,520
wishes to nudge each one of those weights and biases.

126
00:08:37,520 --> 00:08:41,400
If we only listened to what that 2 wanted, the network would ultimately be incentivized

127
00:08:41,400 --> 00:08:44,140
just to classify all images as a 2.

128
00:08:44,140 --> 00:08:49,500
So what you do is go through this same backprop routine for every other training example,

129
00:08:49,500 --> 00:08:54,700
recording how each of them would like to change the weights and biases, and average together

130
00:08:54,700 --> 00:09:02,300
those desired changes.

131
00:09:02,300 --> 00:09:08,260
This collection here of the averaged nudges to each weight and bias is, loosely speaking,

132
00:09:08,260 --> 00:09:12,340
the negative gradient of the cost function referenced in the last video, or at least

133
00:09:12,340 --> 00:09:14,360
something proportional to it.

134
00:09:14,360 --> 00:09:18,980
I say loosely speaking only because I have yet to get quantitatively precise about those

135
00:09:18,980 --> 00:09:23,480
nudges, but if you understood every change I just referenced, why some are proportionally

136
00:09:23,480 --> 00:09:28,740
bigger than others, and how they all need to be added together, you understand the mechanics

137
00:09:28,740 --> 00:09:34,100
for what backpropagation is actually doing.

138
00:09:34,100 --> 00:09:38,540
By the way, in practice, it takes computers an extremely long time to add up the influence

139
00:09:38,540 --> 00:09:43,120
of every training example every gradient descent step.

140
00:09:43,120 --> 00:09:45,540
So here's what's commonly done instead.

141
00:09:45,540 --> 00:09:50,460
You randomly shuffle your training data and divide it into a whole bunch of mini-batches,

142
00:09:50,460 --> 00:09:53,380
let's say each one having 100 training examples.

143
00:09:53,380 --> 00:09:56,980
Then you compute a step according to the mini-batch.

144
00:09:56,980 --> 00:10:00,840
It's not the actual gradient of the cost function, which depends on all of the training

145
00:10:00,840 --> 00:10:06,260
data, not this tiny subset, so it's not the most efficient step downhill, but each

146
00:10:06,260 --> 00:10:10,900
mini-batch does give you a pretty good approximation, and more importantly it gives you a significant

147
00:10:10,900 --> 00:10:12,900
computational speedup.

148
00:10:12,900 --> 00:10:16,900
If you were to plot the trajectory of your network under the relevant cost surface, it

149
00:10:16,900 --> 00:10:22,020
would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps,

150
00:10:22,020 --> 00:10:26,880
rather than a carefully calculating man determining the exact downhill direction of each step

151
00:10:26,880 --> 00:10:31,620
before taking a very slow and careful step in that direction.

152
00:10:31,620 --> 00:10:35,200
This technique is referred to as stochastic gradient descent.

153
00:10:35,200 --> 00:10:40,400
There's a lot going on here, so let's just sum it up for ourselves, shall we?

154
00:10:40,400 --> 00:10:45,480
Backpropagation is the algorithm for determining how a single training example would like to

155
00:10:45,480 --> 00:10:50,040
nudge the weights and biases, not just in terms of whether they should go up or down,

156
00:10:50,040 --> 00:10:54,780
but in terms of what relative proportions to those changes cause the most rapid decrease

157
00:10:54,780 --> 00:10:56,240
to the cost.

158
00:10:56,240 --> 00:11:00,720
A true gradient descent step would involve doing this for all your tens and thousands

159
00:11:00,720 --> 00:11:05,920
of training examples and averaging the desired changes you get, but that's computationally

160
00:11:05,920 --> 00:11:11,680
slow, so instead you randomly subdivide the data into mini-batches and compute each step

161
00:11:11,680 --> 00:11:14,000
with respect to a mini-batch.

162
00:11:14,000 --> 00:11:18,600
Repeatedly going through all the mini-batches and making these adjustments, you will converge

163
00:11:18,600 --> 00:11:23,420
towards a local minimum of the cost function, which is to say your network will end up doing

164
00:11:23,420 --> 00:11:27,540
a really good job on the training examples.

165
00:11:27,540 --> 00:11:32,600
So with all of that said, every line of code that would go into implementing backprop actually

166
00:11:32,600 --> 00:11:37,680
corresponds with something you have now seen, at least in informal terms.

167
00:11:37,680 --> 00:11:41,900
But sometimes knowing what the math does is only half the battle, and just representing

168
00:11:41,900 --> 00:11:44,780
the damn thing is where it gets all muddled and confusing.

169
00:11:44,780 --> 00:11:49,360
So, for those of you who do want to go deeper, the next video goes through the same ideas

170
00:11:49,360 --> 00:11:53,400
that were just presented here, but in terms of the underlying calculus, which should hopefully

171
00:11:53,400 --> 00:11:57,460
make it a little more familiar as you see the topic in other resources.

172
00:11:57,460 --> 00:12:01,220
Before that, one thing worth emphasizing is that for this algorithm to work, and this

173
00:12:01,220 --> 00:12:05,840
goes for all sorts of machine learning beyond just neural networks, you need a lot of training

174
00:12:05,840 --> 00:12:06,840
data.

175
00:12:06,840 --> 00:12:10,740
In our case, one thing that makes handwritten digits such a nice example is that there exists

176
00:12:10,740 --> 00:12:15,380
the MNIST database, with so many examples that have been labeled by humans.

177
00:12:15,380 --> 00:12:19,000
So a common challenge that those of you working in machine learning will be familiar with

178
00:12:19,040 --> 00:12:22,880
is just getting the labeled training data you actually need, whether that's having

179
00:12:22,880 --> 00:12:27,400
people label tens of thousands of images, or whatever other data type you might be dealing with.

