1
00:00:00,000 --> 00:00:09,640
ここでは、ニューラル ネットワークの学習方法の背後にある中心的なアルゴリズムであるバックプロパゲーションに取り組みます。

2
00:00:09,640 --> 00:00:13,320
ここまでの概要を簡単にまとめた後、最初に、数式を参照せずに、アルゴリズムが実際に何を行っているかを直感的に説明します。

3
00:00:13,320 --> 00:00:17,400


4
00:00:17,400 --> 00:00:21,400
次に、数学について詳しく知りたい人のために、次のビデオでは、これらすべての基礎となる微積分について説明します。

5
00:00:21,400 --> 00:00:24,040


6
00:00:24,040 --> 00:00:27,320
最後の 2 つのビデオをご覧になった場合、または適切な背景を理解してすぐに参加した場合は、ニューラル

7
00:00:27,320 --> 00:00:31,080
ネットワークとは何か、またニューラル ネットワークがどのように情報をフィードフォワードするかについては理解しているでしょう。

8
00:00:31,080 --> 00:00:35,520
ここでは、ピクセル値が 784 個のニューロンを含むネットワークの最初の層に入力される手書きの数字を認識する古典的な例を行っています。また、それぞれ

9
00:00:35,520 --> 00:00:40,280
16 個のニューロンしか持たない

10
00:00:40,280 --> 00:00:44,720
2 つの隠れ層と出力を含むネットワークを示しています。

11
00:00:44,720 --> 00:00:49,520
10 個のニューロンの層。ネットワークがどの桁を答えとして選択しているかを示します。

12
00:00:49,520 --> 00:00:54,480
また、前回のビデオで説明したように、勾配降下法と、学習とは、どの重みとバイアスが特定のコスト関数を最小化するかを見つけることを意味することを理解していただくことも期待しています。

13
00:00:54,480 --> 00:01:00,160


14
00:01:00,160 --> 00:01:02,080


15
00:01:02,080 --> 00:01:07,560
簡単に思い出していただきたいのですが、1 つのトレーニング

16
00:01:07,560 --> 00:01:12,920
サンプルのコストとして、ネットワークが提供する出力と、ネットワークに提供してもらいたい出力を取得し、各コンポーネントの差の 2

17
00:01:12,920 --> 00:01:15,560
乗を合計します。

18
00:01:15,560 --> 00:01:20,160
これを何万ものトレーニング例すべてに対して実行し、結果を平均すると、ネットワークの総コストが得られます。

19
00:01:20,160 --> 00:01:23,040


20
00:01:23,040 --> 00:01:26,320
最後のビデオで説明したように、それだけでは考えるのが十分ではないかのように、私たちが探しているのはこのコスト関数の負の勾配です。これは、すべての重みとバイアスをどのように変更する必要があるかを示しています。これらの接続により、最も効率的にコストが削減されます。

21
00:01:26,320 --> 00:01:31,700


22
00:01:31,700 --> 00:01:36,000


23
00:01:36,000 --> 00:01:43,080


24
00:01:43,080 --> 00:01:48,600
このビデオのトピックであるバックプロパゲーションは、非常に複雑な勾配を計算するためのアルゴリズムです。

25
00:01:48,600 --> 00:01:49,600


26
00:01:49,600 --> 00:01:53,300
最後のビデオで、今すぐにしっかりと頭の中に留めておいてほしい 1

27
00:01:53,300 --> 00:01:58,280
つのアイデアは、勾配ベクトルを

28
00:01:58,280 --> 00:02:02,660
13,000

29
00:02:02,660 --> 00:02:04,620
次元の方向として考えることは、簡単に言えば、私たちの想像の範囲を超えているため、別のアイデアがあるということです。あなたがそれについて考えることができる方法。

30
00:02:04,620 --> 00:02:09,700
ここでの各成分の大きさは、コスト関数が各重みとバイアスに対してどの程度敏感であるかを示しています。

31
00:02:09,700 --> 00:02:11,820


32
00:02:11,820 --> 00:02:15,180
たとえば、これから説明するプロセスを実行し、負の勾配を計算したところ、このエッジの重みに関連付けられたコンポーネントが 3

33
00:02:15,180 --> 00:02:19,800
であることが判明したとします。

34
00:02:19,800 --> 00:02:26,940
2 ですが、このエッジに関連付けられたコンポーネントは 0 として出力されます。 1.

35
00:02:26,940 --> 00:02:31,520
これをどう解釈するかというと、関数のコストは最初の重みの変化に対して 32

36
00:02:31,520 --> 00:02:36,100
倍敏感であるため、その値を少し変更すると、コストに何らかの変化が生じ、その変化は2

37
00:02:36,100 --> 00:02:40,780
番目の重りに対する同じ揺れが与える値よりも 32

38
00:02:40,780 --> 00:02:45,580
倍大きくなります。

39
00:02:45,580 --> 00:02:52,500
個人的に、私が最初にバックプロパゲーションについて学んだとき、最も混乱したのは単にその表記とインデックスの追跡だったと思います。

40
00:02:52,500 --> 00:02:55,820


41
00:02:55,820 --> 00:03:00,240
しかし、このアルゴリズムの各部分が実際に何をしているのかを紐解いてみると、それがもたらす個々の効果は実際には非常に直感的であり、ただ多くの小さな調整が積み重ねられているだけです。

42
00:03:00,240 --> 00:03:04,540


43
00:03:04,540 --> 00:03:07,740


44
00:03:07,740 --> 00:03:11,380
したがって、ここでは表記を完全に無視して物事を開始し、各トレーニング例が重みとバイアスに与える影響を段階的に見ていきます。

45
00:03:11,380 --> 00:03:17,380


46
00:03:17,380 --> 00:03:21,880
コスト関数には、数万のトレーニング

47
00:03:21,880 --> 00:03:26,980
サンプル全体にわたるサンプルあたりの特定のコストの平均が含まれるため、単一の勾配降下ステップの重みとバイアスを調整する方法も、すべてのサンプルに依存します。

48
00:03:26,980 --> 00:03:31,740


49
00:03:31,740 --> 00:03:35,300
というか、原理的にはそうすべきですが、計算効率を高めるために、各ステップですべてのサンプルをヒットする必要がないように、後でちょっとしたトリックを実行します。

50
00:03:35,300 --> 00:03:39,860


51
00:03:39,860 --> 00:03:44,460
他のケースでは、現時点では、この 2 の画像という

52
00:03:44,460 --> 00:03:46,780
1 つの例に注目するだけです。

53
00:03:46,780 --> 00:03:51,740
この 1 つのトレーニング例は、重みとバイアスの調整方法にどのような影響を与えるでしょうか?

54
00:03:51,740 --> 00:03:56,040
ネットワークがまだ十分にトレーニングされていない段階にあるとします。そのため、出力内のアクティベーションはかなりランダムに、おそらく 0

55
00:03:56,040 --> 00:04:01,620
のようなものになるとします。 5、0。 8、0。 2、延々と。

56
00:04:01,620 --> 00:04:02,780


57
00:04:02,780 --> 00:04:06,700
これらのアクティベーションを直接変更することはできず、影響を受けるのは重みとバイアスのみですが、その出力層に対してどの調整を行う必要があるかを追跡するのに役立ちます。

58
00:04:06,700 --> 00:04:11,380


59
00:04:11,380 --> 00:04:13,340


60
00:04:13,340 --> 00:04:18,220
そして、画像を 2

61
00:04:18,220 --> 00:04:21,700
として分類したいので、3 番目の値を少しずつ上げて、他のすべての値を少しずつ下げるようにします。

62
00:04:21,700 --> 00:04:27,620
さらに、これらのナッジのサイズは、各現在の値がその目標値からどれだけ離れているかに比例する必要があります。

63
00:04:27,620 --> 00:04:30,220


64
00:04:30,220 --> 00:04:35,260
たとえば、2 番のニューロンの活性化の増加は、ある意味で、すでにあるべき状態にかなり近づいている

65
00:04:35,260 --> 00:04:39,620
8

66
00:04:39,620 --> 00:04:42,060
番のニューロンの減少よりも重要です。

67
00:04:42,060 --> 00:04:46,260
そこでさらにズームインして、活性化を高めたいこの 1

68
00:04:46,260 --> 00:04:47,900
つのニューロンだけに焦点を当ててみましょう。

69
00:04:47,900 --> 00:04:53,680
アクティベーションは、前の層のすべてのアクティベーションの特定の加重合計にバイアスを加えたものとして定義され、そのすべてがシグモイド潰し関数や

70
00:04:53,680 --> 00:04:58,380
ReLU

71
00:04:58,380 --> 00:05:01,900
などに組み込まれることに注意してください。

72
00:05:01,900 --> 00:05:07,060
したがって、その活性化を高めるために連携できる 3

73
00:05:07,060 --> 00:05:08,060
つの異なる方法があります。

74
00:05:08,060 --> 00:05:12,800
バイアスを増やしたり、重みを増やしたり、前のレイヤーからのアクティベーションを変更したりできます。

75
00:05:12,800 --> 00:05:15,300


76
00:05:15,300 --> 00:05:19,720
ウェイトをどのように調整するかに注目して、ウェイトが実際にどのように異なるレベルの影響を与えるかに注目してください。

77
00:05:19,720 --> 00:05:21,460


78
00:05:21,460 --> 00:05:25,100
前の層の最も明るいニューロンとの接続は、それらの重みにより大きな活性化値が乗算されるため、最も大きな効果をもたらします。

79
00:05:25,100 --> 00:05:31,420


80
00:05:31,420 --> 00:05:35,820
したがって、これらの重みの 1

81
00:05:35,820 --> 00:05:40,900
つを増加すると、少なくともこの 1

82
00:05:40,900 --> 00:05:44,020
つのトレーニング例に関する限り、実際には、ディマー ニューロンとの接続の重みを増加するよりも最終的なコスト関数に強い影響を及ぼします。

83
00:05:44,020 --> 00:05:48,700
勾配降下法について話すとき、私たちは単に各コンポーネントを上に動かすか下に動かすかだけを気にするのではなく、どれが最も費用対効果が高いかを気にしていることに注意してください。

84
00:05:48,700 --> 00:05:53,020


85
00:05:53,020 --> 00:05:54,020


86
00:05:54,020 --> 00:06:00,260
ちなみに、これは、ニューロンの生物学的ネットワークがどのように学習するかについての神経科学の理論、ヘビアン理論を少なくともいくらか思い出させます。ヘビアン理論は、しばしば「発火するニューロンは一緒に配線する」というフレーズに要約されます。

87
00:06:00,260 --> 00:06:04,900


88
00:06:04,900 --> 00:06:06,940


89
00:06:06,940 --> 00:06:12,460
ここで、重みの最大の増加、つまり接続の最大の強化は、最もアクティブなニューロンと、よりアクティブになりたいニューロンの間で発生します。

90
00:06:12,460 --> 00:06:16,860


91
00:06:16,860 --> 00:06:18,100


92
00:06:18,100 --> 00:06:22,520
ある意味、「2」を見ているときに発火しているニューロンは、それについて考えているときに発火しているニューロンとより強く結びついています。

93
00:06:22,520 --> 00:06:25,440


94
00:06:25,440 --> 00:06:29,240
誤解のないように言っておきますが、私はニューロンの人工ネットワークが生物学的な脳のように振る舞うかどうかについて何らかの形で意見を言う立場にありません。そして、この「ファイア・トゥゲザー・ワイヤー・トゥゲザー」というアイデアには意味のあるアスタリスクがいくつか付いていますが、非常に緩いものとして捉えられています。たとえて言えば、注目するのは興味深いと思います。

95
00:06:29,240 --> 00:06:34,020


96
00:06:34,020 --> 00:06:39,440


97
00:06:39,440 --> 00:06:41,760


98
00:06:41,760 --> 00:06:46,760
とにかく、このニューロンの活性化を高める 3

99
00:06:46,760 --> 00:06:49,360
番目の方法は、前の層のすべての活性化を変更することです。

100
00:06:49,360 --> 00:06:55,080
つまり、正の重みを持つ数字 2

101
00:06:55,080 --> 00:06:59,480
のニューロンに接続されているすべてが明るくなり、負の重みに接続されているすべてが暗くなると、その数字 2

102
00:06:59,480 --> 00:07:02,680
のニューロンはよりアクティブになります。

103
00:07:02,680 --> 00:07:06,200
ウェイトの変更と同様に、対応するウェイトのサイズに比例する変更を求めることで、最も大きな利益を得ることができます。

104
00:07:06,200 --> 00:07:10,840


105
00:07:10,840 --> 00:07:16,520
もちろん、これらのアクティベーションに直接影響を与えることはできません。制御できるのは重みとバイアスだけです。

106
00:07:16,520 --> 00:07:18,320


107
00:07:18,320 --> 00:07:22,960
ただし、最後のレイヤーと同様に、必要な変更が何であるかをメモしておくと役立ちます。

108
00:07:22,960 --> 00:07:23,960


109
00:07:23,960 --> 00:07:29,040
ただし、ここで 1 段階ズームアウトすると、これは桁

110
00:07:29,040 --> 00:07:30,040
2 の出力ニューロンが望んでいることだけであることに注意してください。

111
00:07:30,040 --> 00:07:34,960
最後の層にある他のすべてのニューロンもあまりアクティブにならないようにしたいこと、そしてそれらの他の出力ニューロンはそれぞれ、最後から

112
00:07:34,960 --> 00:07:38,460
2

113
00:07:38,460 --> 00:07:43,200
番目の層に何が起こるべきかについて独自の考えを持っていることを思い出してください。

114
00:07:43,200 --> 00:07:49,220
したがって、この桁 2

115
00:07:49,220 --> 00:07:54,800
ニューロンの欲求は、この最後から

116
00:07:54,800 --> 00:08:00,240
2

117
00:08:00,240 --> 00:08:01,740
番目の層に何が起こるべきかについての他のすべての出力ニューロンの欲求と加算されます。これも、対応する重みに比例し、各ニューロンがどれだけ必要とするかに比例します。変えること。

118
00:08:01,740 --> 00:08:05,940
ここで、逆方向に伝播するというアイデアが登場します。

119
00:08:05,940 --> 00:08:11,080
これらの必要な効果をすべて加算すると、基本的に、最後から 2

120
00:08:11,080 --> 00:08:14,300
番目のレイヤーに適用するナッジのリストが得られます。

121
00:08:14,300 --> 00:08:18,740
これらを取得したら、それらの値を決定する関連する重みとバイアスに同じプロセスを再帰的に適用し、先ほど説明したのと同じプロセスを繰り返し、ネットワークを逆方向に進むことができます。

122
00:08:18,740 --> 00:08:23,400


123
00:08:23,400 --> 00:08:29,180


124
00:08:29,180 --> 00:08:33,960
さらにズームアウトすると、これはすべて、単一のトレーニング

125
00:08:33,960 --> 00:08:37,520
サンプルがこれらの重みとバイアスのそれぞれを微調整する方法にすぎないことを思い出してください。

126
00:08:37,520 --> 00:08:41,400
もし私たちがその 2 の要求にのみ耳を傾けていたとしたら、ネットワークは最終的にはすべての画像を

127
00:08:41,400 --> 00:08:44,140
2 として分類することだけにインセンティブを与えることになります。

128
00:08:44,140 --> 00:08:49,500
したがって、他のすべてのトレーニング

129
00:08:49,500 --> 00:08:54,700
サンプルに対して同じバックプロップ

130
00:08:54,700 --> 00:09:02,300
ルーチンを実行し、それぞれのサンプルで重みとバイアスをどのように変更したいかを記録し、それらの望ましい変更を平均します。

131
00:09:02,300 --> 00:09:08,260
ここでの各重みとバイアスの平均ナッジのコレクションは、大まかに言えば、最後のビデオで参照されたコスト関数の負の勾配、または少なくともそれに比例するものです。

132
00:09:08,260 --> 00:09:12,340


133
00:09:12,340 --> 00:09:14,360


134
00:09:14,360 --> 00:09:18,980
私がこれらのナッジについて定量的に正確に理解できていないから大まかに言ってるだけですが、私が今言及したすべての変更、なぜ一部の変更が他の変更よりも比例して大きくなるのか、そしてそれらすべてをどのように加算する必要があるのかを理解できたなら、あなたはそのメカニズムを理解しているでしょう。バックプロパゲーションが実際に行っていること。

135
00:09:18,980 --> 00:09:23,480


136
00:09:23,480 --> 00:09:28,740


137
00:09:28,740 --> 00:09:34,100


138
00:09:34,100 --> 00:09:38,540
ところで、実際には、コンピューターがすべての学習例の影響を勾配降下ステップごとに合計するには非常に長い時間がかかります。

139
00:09:38,540 --> 00:09:43,120


140
00:09:43,120 --> 00:09:45,540
そこで、代わりに一般的に行われることを次に示します。

141
00:09:45,540 --> 00:09:50,460
トレーニング データをランダムにシャッフルし、それを多数のミニバッチに分割します。たとえば、各ミニバッチに 100

142
00:09:50,460 --> 00:09:53,380
個のトレーニング サンプルがあるとします。

143
00:09:53,380 --> 00:09:56,980
次に、ミニバッチに従ってステップを計算します。

144
00:09:56,980 --> 00:10:00,840
これはコスト関数の実際の勾配ではなく、この小さなサブセットではなくすべてのトレーニング

145
00:10:00,840 --> 00:10:06,260


146
00:10:06,260 --> 00:10:10,900
データに依存するため、最も効率的な下り坂のステップではありませんが、各ミニバッチからかなり良好な近似が得られます。さらに重要なのは、計算速度が大幅に向上します。

147
00:10:10,900 --> 00:10:12,900


148
00:10:12,900 --> 00:10:16,900
関連するコスト曲面の下でネットワークの軌跡をプロットすると、それは、慎重に計算して各ステップの下り坂の方向を正確に決定するというよりは、目的もなく坂を下りながらも素早いステップを踏む酔っぱらいの男に似たものになるでしょう。その方向に非常にゆっくりと慎重に一歩を踏み出す前に。

149
00:10:16,900 --> 00:10:22,020


150
00:10:22,020 --> 00:10:26,880


151
00:10:26,880 --> 00:10:31,620


152
00:10:31,620 --> 00:10:35,200
この手法は確率的勾配降下法と呼ばれます。

153
00:10:35,200 --> 00:10:40,400
ここではたくさんのことが起こっているので、自分用にまとめてみましょう。

154
00:10:40,400 --> 00:10:45,480
バックプロパゲーションは、単一のトレーニング

155
00:10:45,480 --> 00:10:50,040


156
00:10:50,040 --> 00:10:54,780
サンプルが重みとバイアスをどのように微調整するかを決定するためのアルゴリズムです。重みとバイアスを上昇させるか下降させるかという観点だけでなく、それらの変化に対する相対的な割合が最も急速な減少を引き起こすかという観点から判断します。料金。

157
00:10:54,780 --> 00:10:56,240


158
00:10:56,240 --> 00:11:00,720
真の勾配降下ステップでは、これを何万、何千ものトレーニング例すべてに対して実行し、得られる望ましい変化を平均する必要がありますが、これでは計算が遅いため、代わりにデータをランダムにミニバッチに分割し、各ステップをミニバッチ。

159
00:11:00,720 --> 00:11:05,920


160
00:11:05,920 --> 00:11:11,680


161
00:11:11,680 --> 00:11:14,000


162
00:11:14,000 --> 00:11:18,600
すべてのミニバッチを繰り返し実行してこれらの調整を行うと、コスト関数の極小値に向かって収束します。つまり、ネットワークがトレーニング

163
00:11:18,600 --> 00:11:23,420
サンプルで非常に優れたパフォーマンスを発揮することになります。

164
00:11:23,420 --> 00:11:27,540


165
00:11:27,540 --> 00:11:32,600
以上のことをすべて踏まえた上で、backprop

166
00:11:32,600 --> 00:11:37,680
の実装に含まれるコードのすべての行は、少なくとも非公式の用語では、実際にこれまでに見たものと一致します。

167
00:11:37,680 --> 00:11:41,900
しかし、数学が何をするのかを知ることは戦いの半分に過ぎず、単にそれを表現するだけですべてが混乱して混乱することもあります。

168
00:11:41,900 --> 00:11:44,780


169
00:11:44,780 --> 00:11:49,360
したがって、さらに詳しく知りたい人のために、次のビデオでは、ここで紹介したのと同じアイデアを説明しますが、基礎となる微積分の観点から説明します。他のリソース。

170
00:11:49,360 --> 00:11:53,400


171
00:11:53,400 --> 00:11:57,460


172
00:11:57,460 --> 00:12:01,220
その前に、強調しておきたいことの 1

173
00:12:01,220 --> 00:12:05,840
つは、このアルゴリズムが機能するには、これはニューラル ネットワークだけでなくあらゆる種類の機械学習に当てはまりますが、大量のトレーニング

174
00:12:05,840 --> 00:12:06,840
データが必要であるということです。

175
00:12:06,840 --> 00:12:10,740
私たちの場合、手書きの数字がこれほど優れた例である理由の 1 つは、人間によってラベル付けされた非常に多くの例が含まれる

176
00:12:10,740 --> 00:12:15,380
MNIST データベースが存在することです。

177
00:12:15,380 --> 00:12:19,000
したがって、機械学習に取り組んでいる人ならよく知っているであろう共通の課題は、数万枚の画像にラベルを付けることであろうと、扱う他のデータ型であろうと、実際に必要なラベル付きトレーニング

178
00:12:19,040 --> 00:12:22,880
データを取得することです。

179
00:12:22,880 --> 00:12:27,400


