1
00:00:00,000 --> 00:00:09,640
येथे, आम्ही बॅकप्रॉपगेशन हाताळतो, न्यूरल नेटवर्क कसे शिकतात यामागील कोर अल्गोरिदम.

2
00:00:09,640 --> 00:00:13,469
आपण कोठे आहोत याचे द्रुत रीकॅप केल्यानंतर, सूत्रांचा कोणताही संदर्भ न घेता,

3
00:00:13,469 --> 00:00:17,400
अल्गोरिदम प्रत्यक्षात काय करत आहे यासाठी मी एक अंतर्ज्ञानी वॉकथ्रू करणार आहे.

4
00:00:17,400 --> 00:00:20,799
मग, तुमच्यापैकी ज्यांना गणितात उतरायचे आहे त्यांच्यासाठी, पुढील

5
00:00:20,799 --> 00:00:24,040
व्हिडिओ या सर्व गोष्टींच्या अंतर्निहित कॅल्क्युलसमध्ये जातो.

6
00:00:24,040 --> 00:00:27,410
तुम्ही शेवटचे दोन व्हिडिओ पाहिल्यास, किंवा तुम्ही योग्य पार्श्वभूमीसह उडी मारत

7
00:00:27,410 --> 00:00:31,080
असाल, तर तुम्हाला माहीत आहे की न्यूरल नेटवर्क काय आहे आणि ते माहिती फॉरवर्ड कसे करते.

8
00:00:31,080 --> 00:00:35,690
येथे, आम्ही हस्तलिखित अंक ओळखण्याचे उत्कृष्ट उदाहरण करीत आहोत ज्यांचे पिक्सेल मूल्य

9
00:00:35,690 --> 00:00:40,354
784 न्यूरॉन्स असलेल्या नेटवर्कच्या पहिल्या स्तरामध्ये दिले जाते आणि मी दोन छुपे स्तर

10
00:00:40,354 --> 00:00:44,855
असलेले नेटवर्क दाखवत आहे ज्यामध्ये प्रत्येकी फक्त 16 न्यूरॉन्स आहेत आणि एक आउटपुट

11
00:00:44,855 --> 00:00:49,520
आहे. 10 न्यूरॉन्सचा थर, नेटवर्क त्याचे उत्तर म्हणून कोणता अंक निवडत आहे हे दर्शविते.

12
00:00:49,520 --> 00:00:53,749
शेवटच्या व्हिडीओमध्ये वर्णन केल्याप्रमाणे तुम्ही ग्रेडियंट डिसेंट

13
00:00:53,749 --> 00:00:57,850
समजून घ्याल, आणि आम्ही शिकून घेतलेला अर्थ काय आहे हे आम्ही शोधू

14
00:00:57,850 --> 00:01:02,080
इच्छितो की कोणते वजन आणि पक्षपात विशिष्ट खर्चाचे कार्य कमी करतात.

15
00:01:02,080 --> 00:01:08,506
द्रुत स्मरणपत्र म्हणून, एका प्रशिक्षणाच्या उदाहरणासाठी, तुम्ही नेटवर्क देत असलेले

16
00:01:08,506 --> 00:01:15,560
आउटपुट, तुम्हाला ते देऊ इच्छित असलेल्या आउटपुटसह आणि प्रत्येक घटकातील फरकांचे वर्ग जोडता.

17
00:01:15,560 --> 00:01:18,976
तुमच्या सर्व हजारो प्रशिक्षण उदाहरणांसाठी हे केल्याने आणि

18
00:01:18,976 --> 00:01:23,040
परिणामांची सरासरी काढणे, यामुळे तुम्हाला नेटवर्कची एकूण किंमत मिळते.

19
00:01:23,040 --> 00:01:29,452
शेवटच्या व्हिडिओमध्ये वर्णन केल्याप्रमाणे विचार करणे पुरेसे नाही, आम्ही या खर्च

20
00:01:29,452 --> 00:01:35,865
कार्याचा नकारात्मक ग्रेडियंट शोधत आहोत, जी तुम्हाला सर्व वजन आणि पूर्वाग्रह कसे

21
00:01:35,865 --> 00:01:43,080
बदलण्याची आवश्यकता आहे हे सांगते. ही जोडणी, जेणेकरून सर्वात कार्यक्षमतेने किंमत कमी होईल.

22
00:01:43,080 --> 00:01:46,508
बॅकप्रोपगेशन, या व्हिडिओचा विषय, त्या वेडा क्लिष्ट

23
00:01:46,508 --> 00:01:49,600
ग्रेडियंटची गणना करण्यासाठी एक अल्गोरिदम आहे.

24
00:01:49,600 --> 00:01:53,263
शेवटच्या व्हिडीओमधली एक कल्पना जी तुम्ही आत्ता तुमच्या मनात घट्ट धरून

25
00:01:53,263 --> 00:01:56,769
ठेवावी अशी माझी इच्छा आहे ती म्हणजे 13,000 परिमाणांमध्ये ग्रेडियंट

26
00:01:56,769 --> 00:02:00,485
वेक्टरची दिशा म्हणून विचार करणे म्हणजे, आपल्या कल्पनेच्या व्याप्तीच्या

27
00:02:00,485 --> 00:02:04,620
पलीकडे, हलक्या शब्दात सांगायचे तर दुसरी गोष्ट आहे. आपण याबद्दल विचार करू शकता.

28
00:02:04,620 --> 00:02:07,943
येथे प्रत्येक घटकाचे परिमाण तुम्हाला प्रत्येक वजन आणि

29
00:02:07,943 --> 00:02:11,820
पूर्वाग्रहासाठी किमतीचे कार्य किती संवेदनशील आहे हे सांगत आहे.

30
00:02:11,820 --> 00:02:16,769
उदाहरणार्थ, मी ज्या प्रक्रियेचे वर्णन करणार आहे त्या प्रक्रियेतून तुम्ही

31
00:02:16,769 --> 00:02:21,651
गेलात आणि नकारात्मक ग्रेडियंटची गणना करा आणि या काठावरील वजनाशी संबंधित

32
00:02:21,651 --> 00:02:26,940
घटक 3 असेल असे समजा. 2, तर येथे या काठाशी संबंधित घटक 0 म्हणून बाहेर येतो. १.

33
00:02:26,940 --> 00:02:31,515
तुम्ही ज्या प्रकारे त्याचा अर्थ लावाल ते म्हणजे फंक्शनची किंमत त्या

34
00:02:31,515 --> 00:02:36,226
पहिल्या वजनातील बदलांच्या तुलनेत 32 पट अधिक संवेदनशील असते, म्हणून जर

35
00:02:36,226 --> 00:02:40,936
तुम्ही ते मूल्य थोडेसे हलवायचे असेल, तर ते खर्चात काही बदल घडवून आणेल

36
00:02:40,936 --> 00:02:45,580
आणि तो बदल ते दुसऱ्या वजनाला समान वळवळ देण्यापेक्षा 32 पट जास्त आहे.

37
00:02:45,580 --> 00:02:50,799
व्यक्तिशः, जेव्हा मी पहिल्यांदा बॅकप्रोपॅगेशनबद्दल शिकत होतो, तेव्हा मला वाटते

38
00:02:50,799 --> 00:02:55,820
की सर्वात गोंधळात टाकणारी बाब म्हणजे फक्त नोटेशन आणि इंडेक्सचा पाठलाग करणे.

39
00:02:55,820 --> 00:02:59,734
परंतु एकदा तुम्ही या अल्गोरिदमचा प्रत्येक भाग खरोखर काय करत आहे हे

40
00:02:59,734 --> 00:03:03,766
उघडल्यानंतर, त्याचा होणारा प्रत्येक वैयक्तिक प्रभाव प्रत्यक्षात खूपच

41
00:03:03,766 --> 00:03:07,740
अंतर्ज्ञानी असतो, इतकेच की एकमेकांच्या वर अनेक लहान समायोजने होतात.

42
00:03:07,740 --> 00:03:12,299
म्हणून मी नोटेशनकडे पूर्णपणे दुर्लक्ष करून गोष्टी सुरू करणार आहे, आणि

43
00:03:12,299 --> 00:03:17,380
प्रत्येक प्रशिक्षण उदाहरणाचे वजन आणि पूर्वाग्रहांवर होणारे परिणाम जाणून घ्या.

44
00:03:17,380 --> 00:03:21,952
कारण कॉस्ट फंक्शनमध्ये सर्व दहा हजार प्रशिक्षण उदाहरणांवर प्रति उदाहरण

45
00:03:21,952 --> 00:03:26,652
विशिष्ट खर्चाचा सरासरी समावेश असतो, आम्ही एका ग्रेडियंट डिसेंट पायरीसाठी

46
00:03:26,652 --> 00:03:31,740
वजन आणि पूर्वाग्रह कसे समायोजित करतो हे देखील प्रत्येक उदाहरणावर अवलंबून असते.

47
00:03:31,740 --> 00:03:35,868
किंवा त्याऐवजी, तत्त्वतः ते केले पाहिजे, परंतु संगणकीय कार्यक्षमतेसाठी आम्ही नंतर एक छोटी

48
00:03:35,868 --> 00:03:39,860
युक्ती करू ज्यामुळे तुम्हाला प्रत्येक चरणासाठी प्रत्येक उदाहरणे मारण्याची गरज पडू नये.

49
00:03:39,860 --> 00:03:43,320
इतर प्रकरणांमध्ये, आत्ता आपण फक्त आपले लक्ष एका

50
00:03:43,320 --> 00:03:46,780
उदाहरणावर केंद्रित करणार आहोत, 2 ची ही प्रतिमा.

51
00:03:46,780 --> 00:03:49,184
वजन आणि पूर्वाग्रह कसे समायोजित केले जातात यावर

52
00:03:49,184 --> 00:03:51,740
या एका प्रशिक्षण उदाहरणाचा काय परिणाम झाला पाहिजे?

53
00:03:51,740 --> 00:03:56,916
समजा आम्ही अशा टप्प्यावर आहोत जिथे नेटवर्क अद्याप चांगले प्रशिक्षित नाही, त्यामुळे

54
00:03:56,916 --> 00:04:02,281
आउटपुटमधील सक्रियता खूपच यादृच्छिक दिसत आहेत, कदाचित 0 सारखे काहीतरी. ५, ०.8, 0.2, वर

55
00:04:02,281 --> 00:04:02,780
आणि वर.

56
00:04:02,780 --> 00:04:07,846
आम्ही ती सक्रियता थेट बदलू शकत नाही, आमचा फक्त वजन आणि पूर्वाग्रहांवर प्रभाव असतो,

57
00:04:07,846 --> 00:04:13,340
परंतु त्या आउटपुट स्तरावर आम्हाला कोणते समायोजन करायचे आहे याचा मागोवा ठेवणे उपयुक्त आहे.

58
00:04:13,340 --> 00:04:17,722
आणि आम्हाला प्रतिमेचे 2 म्हणून वर्गीकरण करायचे असल्याने, आम्हाला

59
00:04:17,722 --> 00:04:21,700
ते तिसरे मूल्य वाढवायचे आहे आणि इतर सर्व खाली ढकलले जावेत.

60
00:04:21,700 --> 00:04:26,318
शिवाय, प्रत्येक वर्तमान मूल्य त्याच्या लक्ष्य मूल्यापासून

61
00:04:26,318 --> 00:04:30,220
किती दूर आहे याच्या प्रमाणात या नजचा आकार असावा.

62
00:04:30,220 --> 00:04:36,005
उदाहरणार्थ, संख्या 2 न्यूरॉनच्या सक्रियतेमध्ये वाढ होणे हे एका अर्थाने 8 क्रमांकाच्या

63
00:04:36,005 --> 00:04:42,060
न्यूरॉनच्या कमी होण्यापेक्षा अधिक महत्त्वाचे आहे, जे ते जेथे असावे त्याच्या अगदी जवळ आहे.

64
00:04:42,060 --> 00:04:44,893
म्हणून आणखी झूम करून, फक्त या एका न्यूरॉनवर लक्ष

65
00:04:44,893 --> 00:04:47,900
केंद्रित करूया, ज्याचे सक्रियकरण आपण वाढवू इच्छितो.

66
00:04:47,900 --> 00:04:52,633
लक्षात ठेवा, ते सक्रियकरण मागील लेयरमधील सर्व सक्रियतेची विशिष्ट भारित

67
00:04:52,633 --> 00:04:57,566
बेरीज म्हणून परिभाषित केले आहे, तसेच एक पूर्वाग्रह, जे सर्व नंतर सिग्मॉइड

68
00:04:57,566 --> 00:05:01,900
स्क्विशिफिकेशन फंक्शन किंवा ReLU सारखे काहीतरी प्लग इन केले आहे.

69
00:05:01,900 --> 00:05:05,569
त्यामुळे तीन भिन्न मार्ग आहेत जे ते सक्रियता वाढविण्यात

70
00:05:05,569 --> 00:05:08,060
मदत करण्यासाठी एकत्र कार्य करू शकतात.

71
00:05:08,060 --> 00:05:11,532
तुम्ही पूर्वाग्रह वाढवू शकता, तुम्ही वजन वाढवू

72
00:05:11,532 --> 00:05:15,300
शकता आणि तुम्ही मागील लेयरमधून सक्रियता बदलू शकता.

73
00:05:15,300 --> 00:05:18,293
वजन कसे समायोजित केले जावे यावर लक्ष केंद्रित करून,

74
00:05:18,293 --> 00:05:21,460
वजनाचे प्रत्यक्षात भिन्न स्तर कसे आहेत ते लक्षात घ्या.

75
00:05:21,460 --> 00:05:26,580
आधीच्या लेयरमधील सर्वात तेजस्वी न्यूरॉन्ससह कनेक्शनचा सर्वात मोठा प्रभाव

76
00:05:26,580 --> 00:05:31,420
असतो कारण त्या वजनांना मोठ्या सक्रियकरण मूल्यांनी गुणाकार केला जातो.

77
00:05:31,420 --> 00:05:35,660
त्यामुळे जर तुम्हाला यापैकी एक वजन वाढवायचे असेल तर, कमीत कमी या एका

78
00:05:35,660 --> 00:05:39,717
प्रशिक्षण उदाहरणाचा संबंध आहे तोपर्यंत, मंद न्यूरॉन्ससह कनेक्शनचे

79
00:05:39,717 --> 00:05:44,020
वजन वाढवण्यापेक्षा अंतिम खर्चाच्या कार्यावर त्याचा प्रभाव जास्त असतो.

80
00:05:44,020 --> 00:05:47,199
लक्षात ठेवा, जेव्हा आपण ग्रेडियंट डिसेंट बद्दल बोलतो तेव्हा प्रत्येक

81
00:05:47,199 --> 00:05:50,517
घटकाला वर किंवा खाली नेले जावे याकडेच आम्‍ही लक्ष देत नाही, तर तुमच्‍या

82
00:05:50,517 --> 00:05:54,020
पैशासाठी कोणता घटक तुम्‍हाला सर्वात जास्त दणका देतो याची आम्‍ही काळजी घेतो.

83
00:05:54,020 --> 00:05:58,561
हे, तसे, न्यूरॉन्सचे जैविक नेटवर्क कसे शिकतात याबद्दल न्यूरोसायन्समधील

84
00:05:58,561 --> 00:06:02,782
सिद्धांताची किमान आठवण करून देणारा आहे, हेबियन सिद्धांत, बहुतेकदा

85
00:06:02,782 --> 00:06:06,940
या वाक्यांशात सारांशित केला जातो, न्यूरॉन्स जे एकत्र वायर करतात.

86
00:06:06,940 --> 00:06:12,484
येथे, वजनात सर्वात मोठी वाढ, कनेक्शनची सर्वात मोठी मजबूती, सर्वात जास्त सक्रिय

87
00:06:12,484 --> 00:06:18,100
असलेल्या न्यूरॉन्स आणि ज्यांना आपण अधिक सक्रिय होऊ इच्छितो त्यांच्यामध्ये घडते.

88
00:06:18,100 --> 00:06:21,770
एका अर्थाने, 2 पाहताना फायरिंग होणारे न्यूरॉन्स त्याबद्दल

89
00:06:21,770 --> 00:06:25,440
विचार करताना गोळीबार करणाऱ्यांशी अधिक दृढपणे जोडले जातात.

90
00:06:25,440 --> 00:06:29,401
स्पष्टपणे सांगायचे तर, न्यूरॉन्सचे कृत्रिम नेटवर्क जैविक मेंदूसारखे काहीही

91
00:06:29,401 --> 00:06:33,520
वागतात की नाही याबद्दल एक किंवा दुसर्‍या प्रकारे विधाने करण्याच्या स्थितीत मी

92
00:06:33,520 --> 00:06:37,534
नाही, आणि हे एकत्र वायर एकत्र जमते ही कल्पना दोन अर्थपूर्ण तारकांसोबत येते,

93
00:06:37,534 --> 00:06:41,760
परंतु ती खूप सैल म्हणून घेतली जाते. साधर्म्य, मला हे लक्षात घेणे मनोरंजक वाटते.

94
00:06:41,760 --> 00:06:45,725
असं असलं तरी, या न्यूरॉनचे सक्रियकरण वाढवण्यात मदत करण्याचा

95
00:06:45,725 --> 00:06:49,360
तिसरा मार्ग म्हणजे मागील लेयरमधील सर्व सक्रियता बदलणे.

96
00:06:49,360 --> 00:06:55,866
उदाहरणार्थ, जर पॉझिटिव्ह वजनासह त्या अंक 2 न्यूरॉनशी जोडलेली प्रत्येक गोष्ट उजळ झाली

97
00:06:55,866 --> 00:07:02,680
आणि नकारात्मक वजनाशी जोडलेली प्रत्येक गोष्ट मंद झाली, तर अंक 2 न्यूरॉन अधिक सक्रिय होईल.

98
00:07:02,680 --> 00:07:06,696
आणि वजनातील बदलांप्रमाणेच, संबंधित वजनाच्या आकाराच्या प्रमाणात

99
00:07:06,696 --> 00:07:10,840
बदल शोधून तुम्हाला तुमच्या पैशासाठी सर्वात मोठा फायदा होणार आहे.

100
00:07:10,840 --> 00:07:14,649
आता अर्थातच, आम्ही त्या सक्रियतेवर थेट प्रभाव टाकू शकत

101
00:07:14,649 --> 00:07:18,320
नाही, आमचे फक्त वजन आणि पूर्वाग्रहांवर नियंत्रण आहे.

102
00:07:18,320 --> 00:07:23,960
परंतु शेवटच्या लेयरप्रमाणेच, ते इच्छित बदल काय आहेत याची नोंद ठेवणे उपयुक्त आहे.

103
00:07:23,960 --> 00:07:30,040
पण लक्षात ठेवा, येथे एक पायरी झूम आउट करा, फक्त ते अंक 2 आउटपुट न्यूरॉनला हवे आहे.

104
00:07:30,040 --> 00:07:34,135
लक्षात ठेवा, शेवटच्या लेयरमधील इतर सर्व न्यूरॉन्स कमी सक्रिय

105
00:07:34,135 --> 00:07:38,432
व्हावेत अशी आमची इच्छा आहे आणि त्या प्रत्येक आउटपुट न्यूरॉन्सचे

106
00:07:38,432 --> 00:07:43,200
स्वतःचे विचार आहेत की त्या दुसऱ्या ते शेवटच्या लेयरचे काय झाले पाहिजे.

107
00:07:43,200 --> 00:07:49,278
त्यामुळे या अंक 2 न्यूरॉनची इच्छा या दुसऱ्या ते शेवटच्या लेयरचे काय व्हायला हवे

108
00:07:49,278 --> 00:07:55,585
यासाठी इतर सर्व आउटपुट न्यूरॉन्सच्या इच्छेसोबत जोडले जाते, पुन्हा संबंधित वजनाच्या

109
00:07:55,585 --> 00:08:01,740
प्रमाणात आणि त्या प्रत्येक न्यूरॉन्सला किती आवश्यक आहे या प्रमाणात. बदलण्यासाठी.

110
00:08:01,740 --> 00:08:05,940
इथेच मागच्या बाजूने प्रचार करण्याची कल्पना येते.

111
00:08:05,940 --> 00:08:10,189
हे सर्व इच्छित प्रभाव एकत्र जोडून, तुम्हाला मुळात या दुसऱ्या

112
00:08:10,189 --> 00:08:14,300
ते शेवटच्या लेयरमध्ये घडू इच्छित असलेल्या नजची सूची मिळते.

113
00:08:14,300 --> 00:08:18,803
आणि एकदा तुमच्याकडे ती झाली की, तुम्ही तीच प्रक्रिया संबंधित वजन आणि

114
00:08:18,803 --> 00:08:23,632
पूर्वाग्रहांवर लागू करू शकता जे ती मूल्ये निर्धारित करतात, मी नुकतीच ज्या

115
00:08:23,632 --> 00:08:29,180
प्रक्रियेतून गेलो होतो आणि नेटवर्कमधून मागे सरकतो त्याच प्रक्रियेची पुनरावृत्ती करा.

116
00:08:29,180 --> 00:08:33,318
आणि थोडे पुढे झूम करून, लक्षात ठेवा की हे सर्व फक्त एक प्रशिक्षण

117
00:08:33,318 --> 00:08:37,520
उदाहरण त्या प्रत्येक वजन आणि पूर्वाग्रहांना धक्का देऊ इच्छित आहे.

118
00:08:37,520 --> 00:08:40,754
जर आम्ही फक्त त्या 2 ला काय हवे आहे ते ऐकले तर, सर्व प्रतिमांना

119
00:08:40,754 --> 00:08:44,140
2 म्हणून वर्गीकृत करण्यासाठी नेटवर्कला शेवटी प्रोत्साहन दिले जाईल.

120
00:08:44,140 --> 00:08:50,464
तर तुम्ही काय करता ते प्रत्येक इतर प्रशिक्षण उदाहरणासाठी याच बॅकप्रॉप

121
00:08:50,464 --> 00:08:56,337
रूटीनमधून जाणे, त्यांच्यापैकी प्रत्येकाला वजन आणि पूर्वाग्रह कसे

122
00:08:56,337 --> 00:09:02,300
बदलायचे आहेत याची नोंद करणे आणि इच्छित बदलांची सरासरी एकत्र करणे.

123
00:09:02,300 --> 00:09:06,279
प्रत्येक वजन आणि पूर्वाग्रहासाठी सरासरी नजचा येथे हा संग्रह, अगदी

124
00:09:06,279 --> 00:09:10,018
सहज सांगायचे तर, शेवटच्या व्हिडिओमध्ये संदर्भित केलेल्या खर्च

125
00:09:10,018 --> 00:09:14,360
कार्याचा नकारात्मक ग्रेडियंट किंवा किमान त्याच्या प्रमाणात काहीतरी आहे.

126
00:09:14,360 --> 00:09:19,430
मी सैलपणे बोलतोय कारण मला अजून त्या नडजबद्दल परिमाणवाचक तंतोतंत मिळणे बाकी

127
00:09:19,430 --> 00:09:24,365
आहे, परंतु जर तुम्हाला मी संदर्भ दिलेला प्रत्येक बदल समजला असेल, तर काही

128
00:09:24,365 --> 00:09:29,165
इतरांपेक्षा प्रमाणानुसार का मोठे आहेत आणि ते सर्व एकत्र कसे जोडले जाणे

129
00:09:29,165 --> 00:09:34,100
आवश्यक आहे, तुम्हाला समजले असेल backpropagation प्रत्यक्षात काय करत आहे.

130
00:09:34,100 --> 00:09:38,464
तसे, सराव मध्ये, प्रत्येक ग्रेडियंट डिसेंट पायरीवर प्रत्येक

131
00:09:38,464 --> 00:09:43,120
प्रशिक्षण उदाहरणाचा प्रभाव जोडण्यासाठी संगणकांना खूप वेळ लागतो.

132
00:09:43,120 --> 00:09:45,540
तर त्याऐवजी सामान्यतः काय केले जाते ते येथे आहे.

133
00:09:45,540 --> 00:09:49,696
तुम्ही तुमचा प्रशिक्षण डेटा यादृच्छिकपणे बदलता आणि त्यास संपूर्ण मिनी-बॅचमध्ये

134
00:09:49,696 --> 00:09:53,380
विभाजित करा, चला प्रत्येकाकडे 100 प्रशिक्षण उदाहरणे आहेत असे म्हणूया.

135
00:09:53,380 --> 00:09:56,980
मग आपण मिनी-बॅचनुसार एक चरण मोजा.

136
00:09:56,980 --> 00:10:00,858
हा खर्च फंक्शनचा वास्तविक ग्रेडियंट नाही, जो सर्व प्रशिक्षण डेटावर

137
00:10:00,858 --> 00:10:04,911
अवलंबून असतो, या लहान उपसंचावर नाही, म्हणून ही सर्वात कार्यक्षम पायरी

138
00:10:04,911 --> 00:10:08,847
उतरणीवर नाही, परंतु प्रत्येक मिनी-बॅच तुम्हाला एक चांगला अंदाज देते

139
00:10:08,847 --> 00:10:12,900
आणि अधिक महत्त्वाचे म्हणजे ते तुम्हाला महत्त्वपूर्ण संगणकीय गती देते.

140
00:10:12,900 --> 00:10:17,682
जर तुम्ही तुमच्या नेटवर्कचा मार्ग संबंधित खर्चाच्या पृष्ठभागाखाली प्लॉट करत असाल,

141
00:10:17,682 --> 00:10:22,405
तर प्रत्येक पायरीची अचूक उताराची दिशा ठरवणार्‍या सावधपणे मोजणार्‍या माणसापेक्षा,

142
00:10:22,405 --> 00:10:26,954
एखाद्या नशेतल्या माणसाने टेकडीवरून उद्दिष्ट न ठेवता अडखळल्यासारखे होईल, परंतु

143
00:10:26,954 --> 00:10:31,620
वेगवान पावले उचलली पाहिजेत. त्या दिशेने खूप सावकाश आणि सावध पाऊल टाकण्यापूर्वी.

144
00:10:31,620 --> 00:10:35,200
या तंत्राला स्टोकास्टिक ग्रेडियंट डिसेंट असे संबोधले जाते.

145
00:10:35,200 --> 00:10:40,400
येथे बरेच काही चालले आहे, तर आपण ते स्वतःसाठी एकत्र करूया, का?

146
00:10:40,400 --> 00:10:45,722
बॅकप्रोपॅगेशन हे एका प्रशिक्षणाच्या उदाहरणाने वजन आणि पूर्वाग्रह कसे हलवायचे आहे हे

147
00:10:45,722 --> 00:10:50,854
ठरविण्याचे अल्गोरिदम आहे, केवळ ते वर किंवा खाली जावे की नाही या संदर्भात नाही तर

148
00:10:50,854 --> 00:10:56,240
त्या बदलांच्या सापेक्ष प्रमाणात कोणत्या प्रमाणात सर्वात जलद घट होते याच्या दृष्टीने.

149
00:10:56,240 --> 00:11:00,855
खर्च खर्‍या ग्रेडियंट डिसेंट पायरीमध्ये तुमच्या सर्व दहापट आणि हजारो प्रशिक्षण

150
00:11:00,855 --> 00:11:05,061
उदाहरणांसाठी हे करणे आणि तुम्हाला मिळणाऱ्या इच्छित बदलांची सरासरी काढणे

151
00:11:05,061 --> 00:11:09,443
समाविष्ट असते, परंतु ते संगणकीयदृष्ट्या धीमे आहे, त्यामुळे त्याऐवजी तुम्ही

152
00:11:09,443 --> 00:11:14,000
यादृच्छिकपणे मिनी-बॅचेसमध्ये डेटाचे विभाजन करा आणि प्रत्येक पायरीची गणना करा.

153
00:11:14,000 --> 00:11:18,298
मिनी बॅच सर्व मिनी-बॅचेसमधून वारंवार जाणे आणि या ऍडजस्टमेंट

154
00:11:18,298 --> 00:11:22,883
केल्याने, तुम्ही स्थानिक किमान खर्चाच्या कार्याकडे एकरूप व्हाल,

155
00:11:22,883 --> 00:11:27,540
म्हणजे तुमचे नेटवर्क प्रशिक्षण उदाहरणांवर खरोखर चांगले काम करेल.

156
00:11:27,540 --> 00:11:32,641
तर या सर्व गोष्टींसह, कोडची प्रत्येक ओळ जी बॅकप्रॉपच्या अंमलबजावणीमध्ये जाईल ती

157
00:11:32,641 --> 00:11:37,680
प्रत्यक्षात आपण आता पाहिलेल्या गोष्टीशी संबंधित आहे, किमान अनौपचारिक दृष्टीने.

158
00:11:37,680 --> 00:11:41,183
परंतु काहीवेळा गणित काय करते हे जाणून घेणे ही केवळ अर्धी लढाई असते आणि केवळ

159
00:11:41,183 --> 00:11:44,780
निंदनीय गोष्टीचे प्रतिनिधित्व करणे हे सर्व गोंधळलेले आणि गोंधळात टाकणारे आहे.

160
00:11:44,780 --> 00:11:48,593
तर, तुमच्यापैकी ज्यांना अधिक खोलात जायचे आहे त्यांच्यासाठी, पुढील व्हिडिओ त्याच

161
00:11:48,593 --> 00:11:52,264
कल्पनांमधून जातो ज्या नुकत्याच येथे मांडल्या गेल्या होत्या, परंतु अंतर्निहित

162
00:11:52,264 --> 00:11:56,458
कॅल्क्युलसच्या संदर्भात, ज्याने आशा आहे की तुम्ही विषय पाहता तेव्हा ते थोडे अधिक परिचित

163
00:11:56,458 --> 00:11:57,460
व्हावे. इतर संसाधने.

164
00:11:57,460 --> 00:12:00,554
त्याआधी, एका गोष्टीवर जोर देण्यासारखे आहे की हे अल्गोरिदम कार्य

165
00:12:00,554 --> 00:12:03,745
करण्यासाठी, आणि हे फक्त न्यूरल नेटवर्कच्या पलीकडे सर्व प्रकारच्या

166
00:12:03,745 --> 00:12:06,840
मशीन लर्निंगसाठी आहे, तुम्हाला भरपूर प्रशिक्षण डेटा आवश्यक आहे.

167
00:12:06,840 --> 00:12:11,110
आमच्या बाबतीत, एक गोष्ट जी हस्तलिखीत अंकांना इतके छान उदाहरण बनवते ती म्हणजे

168
00:12:11,110 --> 00:12:15,380
MNIST डेटाबेस अस्तित्वात आहे, ज्याची अनेक उदाहरणे मानवांनी लेबल केलेली आहेत.

169
00:12:15,380 --> 00:12:19,119
त्यामुळे तुमच्यापैकी जे मशीन लर्निंगमध्ये काम करत आहेत त्यांच्याशी परिचित असलेले एक

170
00:12:19,119 --> 00:12:23,081
सामान्य आव्हान म्हणजे तुम्हाला खरोखर आवश्यक असलेला लेबल केलेला प्रशिक्षण डेटा मिळवणे, मग

171
00:12:23,081 --> 00:12:26,821
त्यात लोकांनी हजारो प्रतिमांना लेबल लावले असेल किंवा इतर कोणताही डेटा प्रकार तुम्ही

172
00:12:26,821 --> 00:12:27,400
हाताळत असाल.

