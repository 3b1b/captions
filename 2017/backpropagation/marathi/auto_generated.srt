1
00:00:00,000 --> 00:00:09,640
येथे, आम्ही बॅकप्रॉपगेशन हाताळतो, न्यूरल नेटवर्क कसे शिकतात यामागील कोर अल्गोरिदम.

2
00:00:09,640 --> 00:00:13,320
आपण कोठे आहोत याचे द्रुत रीकॅप केल्यानंतर, सूत्रांचा कोणताही संदर्भ न घेता,

3
00:00:13,320 --> 00:00:17,400
अल्गोरिदम प्रत्यक्षात काय करत आहे यासाठी मी एक अंतर्ज्ञानी वॉकथ्रू करणार आहे.

4
00:00:17,400 --> 00:00:21,400
मग, तुमच्यापैकी ज्यांना गणितात उतरायचे आहे त्यांच्यासाठी, पुढील

5
00:00:21,400 --> 00:00:24,040
व्हिडिओ या सर्व गोष्टींच्या अंतर्निहित कॅल्क्युलसमध्ये जातो.

6
00:00:24,040 --> 00:00:27,320
तुम्ही शेवटचे दोन व्हिडिओ पाहिल्यास, किंवा तुम्ही योग्य पार्श्वभूमीसह उडी मारत असाल, तर तुम्हाला

7
00:00:27,320 --> 00:00:31,080
माहीत आहे की न्यूरल नेटवर्क काय आहे आणि ते माहिती फॉरवर्ड कसे करते.

8
00:00:31,080 --> 00:00:35,520
येथे, आम्ही हस्तलिखित अंक ओळखण्याचे उत्कृष्ट उदाहरण करीत आहोत ज्यांचे पिक्सेल मूल्य 784

9
00:00:35,520 --> 00:00:40,280
न्यूरॉन्स असलेल्या नेटवर्कच्या पहिल्या स्तरामध्ये दिले जाते आणि मी दोन छुपे स्तर असलेले

10
00:00:40,280 --> 00:00:44,720
नेटवर्क दाखवत आहे ज्यामध्ये प्रत्येकी फक्त 16 न्यूरॉन्स आहेत आणि एक आउटपुट आहे.

11
00:00:44,720 --> 00:00:49,520
10 न्यूरॉन्सचा थर, नेटवर्क त्याचे उत्तर म्हणून कोणता अंक निवडत आहे हे दर्शविते.

12
00:00:49,520 --> 00:00:54,480
शेवटच्या व्हिडीओमध्ये वर्णन केल्याप्रमाणे तुम्ही ग्रेडियंट डिसेंट समजून घ्याल, आणि

13
00:00:54,480 --> 00:01:00,160
आम्ही शिकून घेतलेला अर्थ काय आहे हे आम्ही शोधू इच्छितो

14
00:01:00,160 --> 00:01:02,080
की कोणते वजन आणि पक्षपात विशिष्ट खर्चाचे कार्य कमी करतात.

15
00:01:02,080 --> 00:01:07,560
द्रुत स्मरणपत्र म्हणून, एका प्रशिक्षणाच्या उदाहरणासाठी, तुम्ही नेटवर्क

16
00:01:07,560 --> 00:01:12,920
देत असलेले आउटपुट, तुम्हाला ते देऊ इच्छित असलेल्या

17
00:01:12,920 --> 00:01:15,560
आउटपुटसह आणि प्रत्येक घटकातील फरकांचे वर्ग जोडता.

18
00:01:15,560 --> 00:01:20,160
तुमच्या सर्व हजारो प्रशिक्षण उदाहरणांसाठी हे केल्याने आणि परिणामांची

19
00:01:20,160 --> 00:01:23,040
सरासरी काढणे, यामुळे तुम्हाला नेटवर्कची एकूण किंमत मिळते.

20
00:01:23,040 --> 00:01:26,320
शेवटच्या व्हिडिओमध्ये वर्णन केल्याप्रमाणे विचार करणे पुरेसे नाही, आम्ही

21
00:01:26,320 --> 00:01:31,700
या खर्च कार्याचा नकारात्मक ग्रेडियंट शोधत आहोत, जी तुम्हाला

22
00:01:31,700 --> 00:01:36,000
सर्व वजन आणि पूर्वाग्रह कसे बदलण्याची आवश्यकता आहे हे

23
00:01:36,000 --> 00:01:43,080
सांगते. ही जोडणी, जेणेकरून सर्वात कार्यक्षमतेने किंमत कमी होईल.

24
00:01:43,080 --> 00:01:48,600
बॅकप्रोपगेशन, या व्हिडिओचा विषय, त्या वेडा क्लिष्ट

25
00:01:48,600 --> 00:01:49,600
ग्रेडियंटची गणना करण्यासाठी एक अल्गोरिदम आहे.

26
00:01:49,600 --> 00:01:53,300
शेवटच्या व्हिडीओमधली एक कल्पना जी तुम्ही आत्ता तुमच्या मनात घट्ट धरून

27
00:01:53,300 --> 00:01:58,280
ठेवावी अशी माझी इच्छा आहे ती म्हणजे 13,000 परिमाणांमध्ये ग्रेडियंट वेक्टरची

28
00:01:58,280 --> 00:02:02,660
दिशा म्हणून विचार करणे म्हणजे, आपल्या कल्पनेच्या व्याप्तीच्या पलीकडे, हलक्या शब्दात

29
00:02:02,660 --> 00:02:04,620
सांगायचे तर दुसरी गोष्ट आहे. आपण याबद्दल विचार करू शकता.

30
00:02:04,620 --> 00:02:09,700
येथे प्रत्येक घटकाचे परिमाण तुम्हाला प्रत्येक वजन आणि पूर्वाग्रहासाठी

31
00:02:09,700 --> 00:02:11,820
किमतीचे कार्य किती संवेदनशील आहे हे सांगत आहे.

32
00:02:11,820 --> 00:02:15,180
उदाहरणार्थ, मी ज्या प्रक्रियेचे वर्णन करणार आहे त्या प्रक्रियेतून तुम्ही गेलात आणि

33
00:02:15,180 --> 00:02:19,800
नकारात्मक ग्रेडियंटची गणना करा आणि या काठावरील वजनाशी संबंधित घटक 3 असेल

34
00:02:19,800 --> 00:02:26,940
असे समजा. 2, तर येथे या काठाशी संबंधित घटक 0 म्हणून बाहेर येतो. १.

35
00:02:26,940 --> 00:02:31,520
तुम्ही ज्या प्रकारे त्याचा अर्थ लावाल ते म्हणजे फंक्शनची किंमत त्या पहिल्या

36
00:02:31,520 --> 00:02:36,100
वजनातील बदलांच्या तुलनेत 32 पट अधिक संवेदनशील असते, म्हणून जर तुम्ही ते

37
00:02:36,100 --> 00:02:40,780
मूल्य थोडेसे हलवायचे असेल, तर ते खर्चात काही बदल घडवून आणेल आणि

38
00:02:40,780 --> 00:02:45,580
तो बदल ते दुसऱ्या वजनाला समान वळवळ देण्यापेक्षा 32 पट जास्त आहे.

39
00:02:45,580 --> 00:02:52,500
व्यक्तिशः, जेव्हा मी पहिल्यांदा बॅकप्रोपॅगेशनबद्दल शिकत होतो, तेव्हा मला वाटते की

40
00:02:52,500 --> 00:02:55,820
सर्वात गोंधळात टाकणारी बाब म्हणजे फक्त नोटेशन आणि इंडेक्सचा पाठलाग करणे.

41
00:02:55,820 --> 00:03:00,240
परंतु एकदा तुम्ही या अल्गोरिदमचा प्रत्येक भाग खरोखर काय करत

42
00:03:00,240 --> 00:03:04,540
आहे हे उघडल्यानंतर, त्याचा होणारा प्रत्येक वैयक्तिक प्रभाव प्रत्यक्षात खूपच

43
00:03:04,540 --> 00:03:07,740
अंतर्ज्ञानी असतो, इतकेच की एकमेकांच्या वर अनेक लहान समायोजने होतात.

44
00:03:07,740 --> 00:03:11,380
म्हणून मी नोटेशनकडे पूर्णपणे दुर्लक्ष करून गोष्टी सुरू करणार आहे, आणि

45
00:03:11,380 --> 00:03:17,380
प्रत्येक प्रशिक्षण उदाहरणाचे वजन आणि पूर्वाग्रहांवर होणारे परिणाम जाणून घ्या.

46
00:03:17,380 --> 00:03:21,880
कारण कॉस्ट फंक्शनमध्ये सर्व दहा हजार प्रशिक्षण उदाहरणांवर प्रति उदाहरण विशिष्ट

47
00:03:21,880 --> 00:03:26,980
खर्चाचा सरासरी समावेश असतो, आम्ही एका ग्रेडियंट डिसेंट पायरीसाठी वजन आणि

48
00:03:26,980 --> 00:03:31,740
पूर्वाग्रह कसे समायोजित करतो हे देखील प्रत्येक उदाहरणावर अवलंबून असते.

49
00:03:31,740 --> 00:03:35,300
किंवा त्याऐवजी, तत्त्वतः ते केले पाहिजे, परंतु संगणकीय कार्यक्षमतेसाठी आम्ही नंतर एक छोटी

50
00:03:35,300 --> 00:03:39,860
युक्ती करू ज्यामुळे तुम्हाला प्रत्येक चरणासाठी प्रत्येक उदाहरणे मारण्याची गरज पडू नये.

51
00:03:39,860 --> 00:03:44,460
इतर प्रकरणांमध्ये, आत्ता आपण फक्त आपले लक्ष एका

52
00:03:44,460 --> 00:03:46,780
उदाहरणावर केंद्रित करणार आहोत, 2 ची ही प्रतिमा.

53
00:03:46,780 --> 00:03:51,740
वजन आणि पूर्वाग्रह कसे समायोजित केले जातात यावर या एका प्रशिक्षण उदाहरणाचा काय परिणाम झाला पाहिजे?

54
00:03:51,740 --> 00:03:56,040
समजा आम्ही अशा टप्प्यावर आहोत जिथे नेटवर्क अद्याप चांगले प्रशिक्षित नाही, त्यामुळे आउटपुटमधील

55
00:03:56,040 --> 00:04:01,620
सक्रियता खूपच यादृच्छिक दिसत आहेत, कदाचित 0 सारखे काहीतरी. ५, ०. 8, 0. 2,

56
00:04:01,620 --> 00:04:02,780
वर आणि वर.

57
00:04:02,780 --> 00:04:06,700
आम्ही ती सक्रियता थेट बदलू शकत नाही, आमचा फक्त वजन

58
00:04:06,700 --> 00:04:11,380
आणि पूर्वाग्रहांवर प्रभाव असतो, परंतु त्या आउटपुट स्तरावर आम्हाला

59
00:04:11,380 --> 00:04:13,340
कोणते समायोजन करायचे आहे याचा मागोवा ठेवणे उपयुक्त आहे.

60
00:04:13,340 --> 00:04:18,220
आणि आम्हाला प्रतिमेचे 2 म्हणून वर्गीकरण करायचे असल्याने, आम्हाला ते

61
00:04:18,220 --> 00:04:21,700
तिसरे मूल्य वाढवायचे आहे आणि इतर सर्व खाली ढकलले जावेत.

62
00:04:21,700 --> 00:04:27,620
शिवाय, प्रत्येक वर्तमान मूल्य त्याच्या लक्ष्य मूल्यापासून किती

63
00:04:27,620 --> 00:04:30,220
दूर आहे याच्या प्रमाणात या नजचा आकार असावा.

64
00:04:30,220 --> 00:04:35,260
उदाहरणार्थ, संख्या 2 न्यूरॉनच्या सक्रियतेमध्ये वाढ होणे हे एका

65
00:04:35,260 --> 00:04:39,620
अर्थाने 8 क्रमांकाच्या न्यूरॉनच्या कमी होण्यापेक्षा अधिक महत्त्वाचे आहे,

66
00:04:39,620 --> 00:04:42,060
जे ते जेथे असावे त्याच्या अगदी जवळ आहे.

67
00:04:42,060 --> 00:04:46,260
म्हणून आणखी झूम करून, फक्त या एका न्यूरॉनवर

68
00:04:46,260 --> 00:04:47,900
लक्ष केंद्रित करूया, ज्याचे सक्रियकरण आपण वाढवू इच्छितो.

69
00:04:47,900 --> 00:04:53,680
लक्षात ठेवा, ते सक्रियकरण मागील लेयरमधील सर्व सक्रियतेची विशिष्ट भारित बेरीज

70
00:04:53,680 --> 00:04:58,380
म्हणून परिभाषित केले आहे, तसेच एक पूर्वाग्रह, जे सर्व नंतर सिग्मॉइड

71
00:04:58,380 --> 00:05:01,900
स्क्विशिफिकेशन फंक्शन किंवा ReLU सारखे काहीतरी प्लग इन केले आहे.

72
00:05:01,900 --> 00:05:07,060
त्यामुळे तीन भिन्न मार्ग आहेत जे ते सक्रियता

73
00:05:07,060 --> 00:05:08,060
वाढविण्यात मदत करण्यासाठी एकत्र कार्य करू शकतात.

74
00:05:08,060 --> 00:05:12,800
तुम्ही पूर्वाग्रह वाढवू शकता, तुम्ही वजन वाढवू शकता

75
00:05:12,800 --> 00:05:15,300
आणि तुम्ही मागील लेयरमधून सक्रियता बदलू शकता.

76
00:05:15,300 --> 00:05:19,720
वजन कसे समायोजित केले जावे यावर लक्ष केंद्रित करून,

77
00:05:19,720 --> 00:05:21,460
वजनाचे प्रत्यक्षात भिन्न स्तर कसे आहेत ते लक्षात घ्या.

78
00:05:21,460 --> 00:05:25,100
आधीच्या लेयरमधील सर्वात तेजस्वी न्यूरॉन्ससह कनेक्शनचा सर्वात मोठा प्रभाव असतो

79
00:05:25,100 --> 00:05:31,420
कारण त्या वजनांना मोठ्या सक्रियकरण मूल्यांनी गुणाकार केला जातो.

80
00:05:31,420 --> 00:05:35,820
त्यामुळे जर तुम्हाला यापैकी एक वजन वाढवायचे असेल तर, कमीत

81
00:05:35,820 --> 00:05:40,900
कमी या एका प्रशिक्षण उदाहरणाचा संबंध आहे तोपर्यंत, मंद न्यूरॉन्ससह

82
00:05:40,900 --> 00:05:44,020
कनेक्शनचे वजन वाढवण्यापेक्षा अंतिम खर्चाच्या कार्यावर त्याचा प्रभाव जास्त असतो.

83
00:05:44,020 --> 00:05:48,700
लक्षात ठेवा, जेव्हा आपण ग्रेडियंट डिसेंट बद्दल बोलतो तेव्हा प्रत्येक घटकाला वर

84
00:05:48,700 --> 00:05:53,020
किंवा खाली नेले जावे याकडेच आम्‍ही लक्ष देत नाही, तर तुमच्‍या पैशासाठी

85
00:05:53,020 --> 00:05:54,020
कोणता घटक तुम्‍हाला सर्वात जास्त दणका देतो याची आम्‍ही काळजी घेतो.

86
00:05:54,020 --> 00:06:00,260
हे, तसे, न्यूरॉन्सचे जैविक नेटवर्क कसे शिकतात याबद्दल न्यूरोसायन्समधील सिद्धांताची

87
00:06:00,260 --> 00:06:04,900
किमान आठवण करून देणारा आहे, हेबियन सिद्धांत, बहुतेकदा या

88
00:06:04,900 --> 00:06:06,940
वाक्यांशात सारांशित केला जातो, न्यूरॉन्स जे एकत्र वायर करतात.

89
00:06:06,940 --> 00:06:12,460
येथे, वजनात सर्वात मोठी वाढ, कनेक्शनची सर्वात मोठी

90
00:06:12,460 --> 00:06:16,860
मजबूती, सर्वात जास्त सक्रिय असलेल्या न्यूरॉन्स आणि ज्यांना

91
00:06:16,860 --> 00:06:18,100
आपण अधिक सक्रिय होऊ इच्छितो त्यांच्यामध्ये घडते.

92
00:06:18,100 --> 00:06:22,520
एका अर्थाने, 2 पाहताना फायरिंग होणारे न्यूरॉन्स त्याबद्दल

93
00:06:22,520 --> 00:06:25,440
विचार करताना गोळीबार करणाऱ्यांशी अधिक दृढपणे जोडले जातात.

94
00:06:25,440 --> 00:06:29,240
स्पष्टपणे सांगायचे तर, न्यूरॉन्सचे कृत्रिम नेटवर्क जैविक मेंदूसारखे काहीही वागतात की नाही

95
00:06:29,240 --> 00:06:34,020
याबद्दल एक किंवा दुसर्‍या प्रकारे विधाने करण्याच्या स्थितीत मी नाही, आणि हे

96
00:06:34,020 --> 00:06:39,440
एकत्र वायर एकत्र जमते ही कल्पना दोन अर्थपूर्ण तारकांसोबत येते, परंतु ती

97
00:06:39,440 --> 00:06:41,760
खूप सैल म्हणून घेतली जाते. साधर्म्य, मला हे लक्षात घेणे मनोरंजक वाटते.

98
00:06:41,760 --> 00:06:46,760
असं असलं तरी, या न्यूरॉनचे सक्रियकरण वाढवण्यात मदत करण्याचा

99
00:06:46,760 --> 00:06:49,360
तिसरा मार्ग म्हणजे मागील लेयरमधील सर्व सक्रियता बदलणे.

100
00:06:49,360 --> 00:06:55,080
उदाहरणार्थ, जर पॉझिटिव्ह वजनासह त्या अंक 2 न्यूरॉनशी जोडलेली प्रत्येक

101
00:06:55,080 --> 00:06:59,480
गोष्ट उजळ झाली आणि नकारात्मक वजनाशी जोडलेली प्रत्येक गोष्ट

102
00:06:59,480 --> 00:07:02,680
मंद झाली, तर अंक 2 न्यूरॉन अधिक सक्रिय होईल.

103
00:07:02,680 --> 00:07:06,200
आणि वजनातील बदलांप्रमाणेच, संबंधित वजनाच्या आकाराच्या प्रमाणात बदल शोधून

104
00:07:06,200 --> 00:07:10,840
तुम्हाला तुमच्या पैशासाठी सर्वात मोठा फायदा होणार आहे.

105
00:07:10,840 --> 00:07:16,520
आता अर्थातच, आम्ही त्या सक्रियतेवर थेट प्रभाव टाकू शकत

106
00:07:16,520 --> 00:07:18,320
नाही, आमचे फक्त वजन आणि पूर्वाग्रहांवर नियंत्रण आहे.

107
00:07:18,320 --> 00:07:22,960
परंतु शेवटच्या लेयरप्रमाणेच, ते इच्छित बदल काय

108
00:07:22,960 --> 00:07:23,960
आहेत याची नोंद ठेवणे उपयुक्त आहे.

109
00:07:23,960 --> 00:07:29,040
पण लक्षात ठेवा, येथे एक पायरी झूम आउट करा,

110
00:07:29,040 --> 00:07:30,040
फक्त ते अंक 2 आउटपुट न्यूरॉनला हवे आहे.

111
00:07:30,040 --> 00:07:34,960
लक्षात ठेवा, शेवटच्या लेयरमधील इतर सर्व न्यूरॉन्स कमी सक्रिय व्हावेत अशी

112
00:07:34,960 --> 00:07:38,460
आमची इच्छा आहे आणि त्या प्रत्येक आउटपुट न्यूरॉन्सचे स्वतःचे विचार

113
00:07:38,460 --> 00:07:43,200
आहेत की त्या दुसऱ्या ते शेवटच्या लेयरचे काय झाले पाहिजे.

114
00:07:43,200 --> 00:07:49,220
त्यामुळे या अंक 2 न्यूरॉनची इच्छा या दुसऱ्या ते

115
00:07:49,220 --> 00:07:54,800
शेवटच्या लेयरचे काय व्हायला हवे यासाठी इतर सर्व आउटपुट

116
00:07:54,800 --> 00:08:00,240
न्यूरॉन्सच्या इच्छेसोबत जोडले जाते, पुन्हा संबंधित वजनाच्या प्रमाणात आणि

117
00:08:00,240 --> 00:08:01,740
त्या प्रत्येक न्यूरॉन्सला किती आवश्यक आहे या प्रमाणात. बदलण्यासाठी.

118
00:08:01,740 --> 00:08:05,940
इथेच मागच्या बाजूने प्रचार करण्याची कल्पना येते.

119
00:08:05,940 --> 00:08:11,080
हे सर्व इच्छित प्रभाव एकत्र जोडून, तुम्हाला मुळात या दुसऱ्या

120
00:08:11,080 --> 00:08:14,300
ते शेवटच्या लेयरमध्ये घडू इच्छित असलेल्या नजची सूची मिळते.

121
00:08:14,300 --> 00:08:18,740
आणि एकदा तुमच्याकडे ती झाली की, तुम्ही तीच प्रक्रिया संबंधित वजन आणि

122
00:08:18,740 --> 00:08:23,400
पूर्वाग्रहांवर लागू करू शकता जे ती मूल्ये निर्धारित करतात, मी नुकतीच ज्या

123
00:08:23,400 --> 00:08:29,180
प्रक्रियेतून गेलो होतो आणि नेटवर्कमधून मागे सरकतो त्याच प्रक्रियेची पुनरावृत्ती करा.

124
00:08:29,180 --> 00:08:33,960
आणि थोडे पुढे झूम करून, लक्षात ठेवा की हे सर्व फक्त एक

125
00:08:33,960 --> 00:08:37,520
प्रशिक्षण उदाहरण त्या प्रत्येक वजन आणि पूर्वाग्रहांना धक्का देऊ इच्छित आहे.

126
00:08:37,520 --> 00:08:41,400
जर आम्ही फक्त त्या 2 ला काय हवे आहे ते ऐकले तर,

127
00:08:41,400 --> 00:08:44,140
सर्व प्रतिमांना 2 म्हणून वर्गीकृत करण्यासाठी नेटवर्कला शेवटी प्रोत्साहन दिले जाईल.

128
00:08:44,140 --> 00:08:49,500
तर तुम्ही काय करता ते प्रत्येक इतर प्रशिक्षण उदाहरणासाठी याच

129
00:08:49,500 --> 00:08:54,700
बॅकप्रॉप रूटीनमधून जाणे, त्यांच्यापैकी प्रत्येकाला वजन आणि पूर्वाग्रह कसे बदलायचे

130
00:08:54,700 --> 00:09:02,300
आहेत याची नोंद करणे आणि इच्छित बदलांची सरासरी एकत्र करणे.

131
00:09:02,300 --> 00:09:08,260
प्रत्येक वजन आणि पूर्वाग्रहासाठी सरासरी नजचा येथे हा संग्रह,

132
00:09:08,260 --> 00:09:12,340
अगदी सहज सांगायचे तर, शेवटच्या व्हिडिओमध्ये संदर्भित केलेल्या खर्च

133
00:09:12,340 --> 00:09:14,360
कार्याचा नकारात्मक ग्रेडियंट किंवा किमान त्याच्या प्रमाणात काहीतरी आहे.

134
00:09:14,360 --> 00:09:18,980
मी सैलपणे बोलतोय कारण मला अजून त्या नडजबद्दल परिमाणवाचक तंतोतंत मिळणे बाकी

135
00:09:18,980 --> 00:09:23,480
आहे, परंतु जर तुम्हाला मी संदर्भ दिलेला प्रत्येक बदल समजला असेल, तर

136
00:09:23,480 --> 00:09:28,740
काही इतरांपेक्षा प्रमाणानुसार का मोठे आहेत आणि ते सर्व एकत्र कसे जोडले

137
00:09:28,740 --> 00:09:34,100
जाणे आवश्यक आहे, तुम्हाला समजले असेल backpropagation प्रत्यक्षात काय करत आहे.

138
00:09:34,100 --> 00:09:38,540
तसे, सराव मध्ये, प्रत्येक ग्रेडियंट डिसेंट पायरीवर प्रत्येक

139
00:09:38,540 --> 00:09:43,120
प्रशिक्षण उदाहरणाचा प्रभाव जोडण्यासाठी संगणकांना खूप वेळ लागतो.

140
00:09:43,120 --> 00:09:45,540
तर त्याऐवजी सामान्यतः काय केले जाते ते येथे आहे.

141
00:09:45,540 --> 00:09:50,460
तुम्ही तुमचा प्रशिक्षण डेटा यादृच्छिकपणे बदलता आणि त्यास संपूर्ण मिनी-बॅचमध्ये

142
00:09:50,460 --> 00:09:53,380
विभाजित करा, चला प्रत्येकाकडे 100 प्रशिक्षण उदाहरणे आहेत असे म्हणूया.

143
00:09:53,380 --> 00:09:56,980
मग आपण मिनी-बॅचनुसार एक चरण मोजा.

144
00:09:56,980 --> 00:10:00,840
हा खर्च फंक्शनचा वास्तविक ग्रेडियंट नाही, जो सर्व प्रशिक्षण डेटावर अवलंबून

145
00:10:00,840 --> 00:10:06,260
असतो, या लहान उपसंचावर नाही, म्हणून ही सर्वात कार्यक्षम पायरी

146
00:10:06,260 --> 00:10:10,900
उतरणीवर नाही, परंतु प्रत्येक मिनी-बॅच तुम्हाला एक चांगला अंदाज देते

147
00:10:10,900 --> 00:10:12,900
आणि अधिक महत्त्वाचे म्हणजे ते तुम्हाला महत्त्वपूर्ण संगणकीय गती देते.

148
00:10:12,900 --> 00:10:16,900
जर तुम्ही तुमच्या नेटवर्कचा मार्ग संबंधित खर्चाच्या पृष्ठभागाखाली प्लॉट करत असाल,

149
00:10:16,900 --> 00:10:22,020
तर प्रत्येक पायरीची अचूक उताराची दिशा ठरवणार्‍या सावधपणे मोजणार्‍या माणसापेक्षा, एखाद्या

150
00:10:22,020 --> 00:10:26,880
नशेतल्या माणसाने टेकडीवरून उद्दिष्ट न ठेवता अडखळल्यासारखे होईल, परंतु वेगवान पावले

151
00:10:26,880 --> 00:10:31,620
उचलली पाहिजेत. त्या दिशेने खूप सावकाश आणि सावध पाऊल टाकण्यापूर्वी.

152
00:10:31,620 --> 00:10:35,200
या तंत्राला स्टोकास्टिक ग्रेडियंट डिसेंट असे संबोधले जाते.

153
00:10:35,200 --> 00:10:40,400
येथे बरेच काही चालले आहे, तर आपण ते स्वतःसाठी एकत्र करूया, का?

154
00:10:40,400 --> 00:10:45,480
बॅकप्रोपॅगेशन हे एका प्रशिक्षणाच्या उदाहरणाने वजन आणि पूर्वाग्रह कसे हलवायचे

155
00:10:45,480 --> 00:10:50,040
आहे हे ठरविण्याचे अल्गोरिदम आहे, केवळ ते वर किंवा खाली

156
00:10:50,040 --> 00:10:54,780
जावे की नाही या संदर्भात नाही तर त्या बदलांच्या सापेक्ष

157
00:10:54,780 --> 00:10:56,240
प्रमाणात कोणत्या प्रमाणात सर्वात जलद घट होते याच्या दृष्टीने. खर्च

158
00:10:56,240 --> 00:11:00,720
खर्‍या ग्रेडियंट डिसेंट पायरीमध्ये तुमच्या सर्व दहापट आणि हजारो प्रशिक्षण उदाहरणांसाठी

159
00:11:00,720 --> 00:11:05,920
हे करणे आणि तुम्हाला मिळणाऱ्या इच्छित बदलांची सरासरी काढणे समाविष्ट

160
00:11:05,920 --> 00:11:11,680
असते, परंतु ते संगणकीयदृष्ट्या धीमे आहे, त्यामुळे त्याऐवजी तुम्ही यादृच्छिकपणे मिनी-बॅचेसमध्ये

161
00:11:11,680 --> 00:11:14,000
डेटाचे विभाजन करा आणि प्रत्येक पायरीची गणना करा. मिनी बॅच

162
00:11:14,000 --> 00:11:18,600
सर्व मिनी-बॅचेसमधून वारंवार जाणे आणि या ऍडजस्टमेंट केल्याने,

163
00:11:18,600 --> 00:11:23,420
तुम्ही स्थानिक किमान खर्चाच्या कार्याकडे एकरूप व्हाल, म्हणजे

164
00:11:23,420 --> 00:11:27,540
तुमचे नेटवर्क प्रशिक्षण उदाहरणांवर खरोखर चांगले काम करेल.

165
00:11:27,540 --> 00:11:32,600
तर या सर्व गोष्टींसह, कोडची प्रत्येक ओळ जी बॅकप्रॉपच्या अंमलबजावणीमध्ये जाईल

166
00:11:32,600 --> 00:11:37,680
ती प्रत्यक्षात आपण आता पाहिलेल्या गोष्टीशी संबंधित आहे, किमान अनौपचारिक दृष्टीने.

167
00:11:37,680 --> 00:11:41,900
परंतु काहीवेळा गणित काय करते हे जाणून घेणे ही केवळ अर्धी लढाई असते

168
00:11:41,900 --> 00:11:44,780
आणि केवळ निंदनीय गोष्टीचे प्रतिनिधित्व करणे हे सर्व गोंधळलेले आणि गोंधळात टाकणारे आहे.

169
00:11:44,780 --> 00:11:49,360
तर, तुमच्यापैकी ज्यांना अधिक खोलात जायचे आहे त्यांच्यासाठी, पुढील व्हिडिओ त्याच कल्पनांमधून जातो

170
00:11:49,360 --> 00:11:53,400
ज्या नुकत्याच येथे मांडल्या गेल्या होत्या, परंतु अंतर्निहित कॅल्क्युलसच्या संदर्भात, ज्याने आशा आहे

171
00:11:53,400 --> 00:11:57,460
की तुम्ही विषय पाहता तेव्हा ते थोडे अधिक परिचित व्हावे. इतर संसाधने.

172
00:11:57,460 --> 00:12:01,220
त्याआधी, एका गोष्टीवर जोर देण्यासारखे आहे की हे अल्गोरिदम कार्य

173
00:12:01,220 --> 00:12:05,840
करण्यासाठी, आणि हे फक्त न्यूरल नेटवर्कच्या पलीकडे सर्व प्रकारच्या

174
00:12:05,840 --> 00:12:06,840
मशीन लर्निंगसाठी आहे, तुम्हाला भरपूर प्रशिक्षण डेटा आवश्यक आहे.

175
00:12:06,840 --> 00:12:10,740
आमच्या बाबतीत, एक गोष्ट जी हस्तलिखीत अंकांना इतके छान उदाहरण बनवते ती

176
00:12:10,740 --> 00:12:15,380
म्हणजे MNIST डेटाबेस अस्तित्वात आहे, ज्याची अनेक उदाहरणे मानवांनी लेबल केलेली आहेत.

177
00:12:15,380 --> 00:12:19,000
त्यामुळे तुमच्यापैकी जे मशीन लर्निंगमध्ये काम करत आहेत त्यांच्याशी परिचित असलेले एक सामान्य आव्हान

178
00:12:19,040 --> 00:12:22,880
म्हणजे तुम्हाला खरोखर आवश्यक असलेला लेबल केलेला प्रशिक्षण डेटा मिळवणे, मग त्यात लोकांनी

179
00:12:22,880 --> 00:12:27,400
हजारो प्रतिमांना लेबल लावले असेल किंवा इतर कोणताही डेटा प्रकार तुम्ही हाताळत असाल.

