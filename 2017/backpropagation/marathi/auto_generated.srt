1
00:00:00,000 --> 00:00:09,640
येथे, आम्ही बॅकप्रॉपगेशन हाताळतो, न्यूरल नेटवर्क कसे शिकतात यामागील कोर अल्गोरिदम.

2
00:00:09,640 --> 00:00:13,469
आपण कोठे आहोत याचे द्रुत रीकॅप केल्यानंतर, सूत्रांचा कोणताही संदर्भ न घेता,

3
00:00:13,469 --> 00:00:17,400
अल्गोरिदम प्रत्यक्षात काय करत आहे यासाठी मी एक अंतर्ज्ञानी वॉकथ्रू करणार आहे.

4
00:00:17,400 --> 00:00:20,480
मग, तुमच्यापैकी ज्यांना गणितात उतरायचे आहे त्यांच्यासाठी,

5
00:00:20,480 --> 00:00:24,040
पुढील व्हिडिओ या सर्व गोष्टींच्या अंतर्निहित कॅल्क्युलसमध्ये जातो.

6
00:00:24,040 --> 00:00:27,666
तुम्ही शेवटचे दोन व्हिडिओ पाहिल्यास, किंवा तुम्ही योग्य पार्श्वभूमीसह उडी मारत असाल,

7
00:00:27,666 --> 00:00:31,080
तर तुम्हाला माहीत आहे की न्यूरल नेटवर्क काय आहे आणि ते माहिती फॉरवर्ड कसे करते.

8
00:00:31,080 --> 00:00:35,690
येथे, आम्ही हस्तलिखित अंक ओळखण्याचे उत्कृष्ट उदाहरण करीत आहोत ज्यांचे पिक्सेल मूल्य

9
00:00:35,690 --> 00:00:40,354
784 न्यूरॉन्स असलेल्या नेटवर्कच्या पहिल्या स्तरामध्ये दिले जाते आणि मी दोन छुपे स्तर

10
00:00:40,354 --> 00:00:45,129
असलेले नेटवर्क दाखवत आहे ज्यामध्ये प्रत्येकी फक्त 16 न्यूरॉन्स आहेत आणि एक आउटपुट आहे.

11
00:00:45,129 --> 00:00:49,520
10 न्यूरॉन्सचा थर, नेटवर्क त्याचे उत्तर म्हणून कोणता अंक निवडत आहे हे दर्शविते.

12
00:00:49,520 --> 00:00:54,582
शेवटच्या व्हिडीओमध्ये वर्णन केल्याप्रमाणे तुम्ही ग्रेडियंट डिसेंट समजून घ्याल,

13
00:00:54,582 --> 00:00:58,555
आणि आम्ही शिकून घेतलेला अर्थ काय आहे हे आम्ही शोधू इच्छितो की

14
00:00:58,555 --> 00:01:02,080
कोणते वजन आणि पक्षपात विशिष्ट खर्चाचे कार्य कमी करतात.

15
00:01:02,080 --> 00:01:09,133
द्रुत स्मरणपत्र म्हणून, एका प्रशिक्षणाच्या उदाहरणासाठी, तुम्ही नेटवर्क देत असलेले आउटपुट,

16
00:01:09,133 --> 00:01:15,560
तुम्हाला ते देऊ इच्छित असलेल्या आउटपुटसह आणि प्रत्येक घटकातील फरकांचे वर्ग जोडता.

17
00:01:15,560 --> 00:01:20,448
तुमच्या सर्व हजारो प्रशिक्षण उदाहरणांसाठी हे केल्याने आणि परिणामांची सरासरी काढणे,

18
00:01:20,448 --> 00:01:23,040
यामुळे तुम्हाला नेटवर्कची एकूण किंमत मिळते.

19
00:01:23,040 --> 00:01:28,330
शेवटच्या व्हिडिओमध्ये वर्णन केल्याप्रमाणे विचार करणे पुरेसे नाही,

20
00:01:28,330 --> 00:01:32,659
आम्ही या खर्च कार्याचा नकारात्मक ग्रेडियंट शोधत आहोत,

21
00:01:32,659 --> 00:01:39,392
जी तुम्हाला सर्व वजन आणि पूर्वाग्रह कसे बदलण्याची आवश्यकता आहे हे सांगते. ही जोडणी,

22
00:01:39,392 --> 00:01:43,080
जेणेकरून सर्वात कार्यक्षमतेने किंमत कमी होईल.

23
00:01:43,080 --> 00:01:46,508
बॅकप्रोपगेशन, या व्हिडिओचा विषय, त्या वेडा क्लिष्ट

24
00:01:46,508 --> 00:01:49,600
ग्रेडियंटची गणना करण्यासाठी एक अल्गोरिदम आहे.

25
00:01:49,600 --> 00:01:53,263
शेवटच्या व्हिडीओमधली एक कल्पना जी तुम्ही आत्ता तुमच्या मनात घट्ट धरून

26
00:01:53,263 --> 00:01:56,769
ठेवावी अशी माझी इच्छा आहे ती म्हणजे 13,000 परिमाणांमध्ये ग्रेडियंट

27
00:01:56,769 --> 00:02:00,904
वेक्टरची दिशा म्हणून विचार करणे म्हणजे, आपल्या कल्पनेच्या व्याप्तीच्या पलीकडे,

28
00:02:00,904 --> 00:02:04,620
हलक्या शब्दात सांगायचे तर दुसरी गोष्ट आहे. आपण याबद्दल विचार करू शकता.

29
00:02:04,620 --> 00:02:07,943
येथे प्रत्येक घटकाचे परिमाण तुम्हाला प्रत्येक वजन आणि

30
00:02:07,943 --> 00:02:11,820
पूर्वाग्रहासाठी किमतीचे कार्य किती संवेदनशील आहे हे सांगत आहे.

31
00:02:11,820 --> 00:02:16,769
उदाहरणार्थ, मी ज्या प्रक्रियेचे वर्णन करणार आहे त्या प्रक्रियेतून तुम्ही

32
00:02:16,769 --> 00:02:21,651
गेलात आणि नकारात्मक ग्रेडियंटची गणना करा आणि या काठावरील वजनाशी संबंधित

33
00:02:21,651 --> 00:02:26,940
घटक 3 असेल असे समजा. 2, तर येथे या काठाशी संबंधित घटक 0 म्हणून बाहेर येतो. १.

34
00:02:26,940 --> 00:02:31,515
तुम्ही ज्या प्रकारे त्याचा अर्थ लावाल ते म्हणजे फंक्शनची किंमत त्या

35
00:02:31,515 --> 00:02:35,553
पहिल्या वजनातील बदलांच्या तुलनेत 32 पट अधिक संवेदनशील असते,

36
00:02:35,553 --> 00:02:38,716
म्हणून जर तुम्ही ते मूल्य थोडेसे हलवायचे असेल,

37
00:02:38,716 --> 00:02:43,224
तर ते खर्चात काही बदल घडवून आणेल आणि तो बदल ते दुसऱ्या वजनाला समान

38
00:02:43,224 --> 00:02:45,580
वळवळ देण्यापेक्षा 32 पट जास्त आहे.

39
00:02:45,580 --> 00:02:49,676
व्यक्तिशः, जेव्हा मी पहिल्यांदा बॅकप्रोपॅगेशनबद्दल शिकत होतो,

40
00:02:49,676 --> 00:02:54,961
तेव्हा मला वाटते की सर्वात गोंधळात टाकणारी बाब म्हणजे फक्त नोटेशन आणि इंडेक्सचा

41
00:02:54,961 --> 00:02:55,820
पाठलाग करणे.

42
00:02:55,820 --> 00:03:00,494
परंतु एकदा तुम्ही या अल्गोरिदमचा प्रत्येक भाग खरोखर काय करत आहे हे उघडल्यानंतर,

43
00:03:00,494 --> 00:03:04,818
त्याचा होणारा प्रत्येक वैयक्तिक प्रभाव प्रत्यक्षात खूपच अंतर्ज्ञानी असतो,

44
00:03:04,818 --> 00:03:07,740
इतकेच की एकमेकांच्या वर अनेक लहान समायोजने होतात.

45
00:03:07,740 --> 00:03:12,038
म्हणून मी नोटेशनकडे पूर्णपणे दुर्लक्ष करून गोष्टी सुरू करणार आहे,

46
00:03:12,038 --> 00:03:17,380
आणि प्रत्येक प्रशिक्षण उदाहरणाचे वजन आणि पूर्वाग्रहांवर होणारे परिणाम जाणून घ्या.

47
00:03:17,380 --> 00:03:21,952
कारण कॉस्ट फंक्शनमध्ये सर्व दहा हजार प्रशिक्षण उदाहरणांवर प्रति उदाहरण

48
00:03:21,952 --> 00:03:26,652
विशिष्ट खर्चाचा सरासरी समावेश असतो, आम्ही एका ग्रेडियंट डिसेंट पायरीसाठी

49
00:03:26,652 --> 00:03:31,740
वजन आणि पूर्वाग्रह कसे समायोजित करतो हे देखील प्रत्येक उदाहरणावर अवलंबून असते.

50
00:03:31,740 --> 00:03:35,868
किंवा त्याऐवजी, तत्त्वतः ते केले पाहिजे, परंतु संगणकीय कार्यक्षमतेसाठी आम्ही नंतर एक छोटी

51
00:03:35,868 --> 00:03:39,860
युक्ती करू ज्यामुळे तुम्हाला प्रत्येक चरणासाठी प्रत्येक उदाहरणे मारण्याची गरज पडू नये.

52
00:03:39,860 --> 00:03:45,554
इतर प्रकरणांमध्ये, आत्ता आपण फक्त आपले लक्ष एका उदाहरणावर केंद्रित करणार आहोत,

53
00:03:45,554 --> 00:03:46,780
2 ची ही प्रतिमा.

54
00:03:46,780 --> 00:03:49,184
वजन आणि पूर्वाग्रह कसे समायोजित केले जातात यावर

55
00:03:49,184 --> 00:03:51,740
या एका प्रशिक्षण उदाहरणाचा काय परिणाम झाला पाहिजे?

56
00:03:51,740 --> 00:03:56,355
समजा आम्ही अशा टप्प्यावर आहोत जिथे नेटवर्क अद्याप चांगले प्रशिक्षित नाही,

57
00:03:56,355 --> 00:04:01,782
त्यामुळे आउटपुटमधील सक्रियता खूपच यादृच्छिक दिसत आहेत, कदाचित 0 सारखे काहीतरी. ५, ०.8,

58
00:04:01,782 --> 00:04:02,780
0.2, वर आणि वर.

59
00:04:02,780 --> 00:04:07,846
आम्ही ती सक्रियता थेट बदलू शकत नाही, आमचा फक्त वजन आणि पूर्वाग्रहांवर प्रभाव असतो,

60
00:04:07,846 --> 00:04:13,340
परंतु त्या आउटपुट स्तरावर आम्हाला कोणते समायोजन करायचे आहे याचा मागोवा ठेवणे उपयुक्त आहे.

61
00:04:13,340 --> 00:04:17,182
आणि आम्हाला प्रतिमेचे 2 म्हणून वर्गीकरण करायचे असल्याने,

62
00:04:17,182 --> 00:04:21,700
आम्हाला ते तिसरे मूल्य वाढवायचे आहे आणि इतर सर्व खाली ढकलले जावेत.

63
00:04:21,700 --> 00:04:26,318
शिवाय, प्रत्येक वर्तमान मूल्य त्याच्या लक्ष्य मूल्यापासून

64
00:04:26,318 --> 00:04:30,220
किती दूर आहे याच्या प्रमाणात या नजचा आकार असावा.

65
00:04:30,220 --> 00:04:36,005
उदाहरणार्थ, संख्या 2 न्यूरॉनच्या सक्रियतेमध्ये वाढ होणे हे एका अर्थाने 8 क्रमांकाच्या

66
00:04:36,005 --> 00:04:42,060
न्यूरॉनच्या कमी होण्यापेक्षा अधिक महत्त्वाचे आहे, जे ते जेथे असावे त्याच्या अगदी जवळ आहे.

67
00:04:42,060 --> 00:04:45,818
म्हणून आणखी झूम करून, फक्त या एका न्यूरॉनवर लक्ष केंद्रित करूया,

68
00:04:45,818 --> 00:04:47,900
ज्याचे सक्रियकरण आपण वाढवू इच्छितो.

69
00:04:47,900 --> 00:04:52,633
लक्षात ठेवा, ते सक्रियकरण मागील लेयरमधील सर्व सक्रियतेची विशिष्ट भारित

70
00:04:52,633 --> 00:04:56,100
बेरीज म्हणून परिभाषित केले आहे, तसेच एक पूर्वाग्रह,

71
00:04:56,100 --> 00:05:01,900
जे सर्व नंतर सिग्मॉइड स्क्विशिफिकेशन फंक्शन किंवा ReLU सारखे काहीतरी प्लग इन केले आहे.

72
00:05:01,900 --> 00:05:05,569
त्यामुळे तीन भिन्न मार्ग आहेत जे ते सक्रियता वाढविण्यात

73
00:05:05,569 --> 00:05:08,060
मदत करण्यासाठी एकत्र कार्य करू शकतात.

74
00:05:08,060 --> 00:05:11,532
तुम्ही पूर्वाग्रह वाढवू शकता, तुम्ही वजन वाढवू

75
00:05:11,532 --> 00:05:15,300
शकता आणि तुम्ही मागील लेयरमधून सक्रियता बदलू शकता.

76
00:05:15,300 --> 00:05:18,293
वजन कसे समायोजित केले जावे यावर लक्ष केंद्रित करून,

77
00:05:18,293 --> 00:05:21,460
वजनाचे प्रत्यक्षात भिन्न स्तर कसे आहेत ते लक्षात घ्या.

78
00:05:21,460 --> 00:05:26,580
आधीच्या लेयरमधील सर्वात तेजस्वी न्यूरॉन्ससह कनेक्शनचा सर्वात मोठा प्रभाव

79
00:05:26,580 --> 00:05:31,420
असतो कारण त्या वजनांना मोठ्या सक्रियकरण मूल्यांनी गुणाकार केला जातो.

80
00:05:31,420 --> 00:05:34,677
त्यामुळे जर तुम्हाला यापैकी एक वजन वाढवायचे असेल तर,

81
00:05:34,677 --> 00:05:38,119
कमीत कमी या एका प्रशिक्षण उदाहरणाचा संबंध आहे तोपर्यंत,

82
00:05:38,119 --> 00:05:42,421
मंद न्यूरॉन्ससह कनेक्शनचे वजन वाढवण्यापेक्षा अंतिम खर्चाच्या कार्यावर

83
00:05:42,421 --> 00:05:44,020
त्याचा प्रभाव जास्त असतो.

84
00:05:44,020 --> 00:05:47,199
लक्षात ठेवा, जेव्हा आपण ग्रेडियंट डिसेंट बद्दल बोलतो तेव्हा प्रत्येक

85
00:05:47,199 --> 00:05:49,964
घटकाला वर किंवा खाली नेले जावे याकडेच आम्‍ही लक्ष देत नाही,

86
00:05:49,964 --> 00:05:54,020
तर तुमच्‍या पैशासाठी कोणता घटक तुम्‍हाला सर्वात जास्त दणका देतो याची आम्‍ही काळजी घेतो.

87
00:05:54,020 --> 00:05:58,561
हे, तसे, न्यूरॉन्सचे जैविक नेटवर्क कसे शिकतात याबद्दल न्यूरोसायन्समधील

88
00:05:58,561 --> 00:06:02,206
सिद्धांताची किमान आठवण करून देणारा आहे, हेबियन सिद्धांत,

89
00:06:02,206 --> 00:06:06,940
बहुतेकदा या वाक्यांशात सारांशित केला जातो, न्यूरॉन्स जे एकत्र वायर करतात.

90
00:06:06,940 --> 00:06:11,081
येथे, वजनात सर्वात मोठी वाढ, कनेक्शनची सर्वात मोठी मजबूती,

91
00:06:11,081 --> 00:06:16,626
सर्वात जास्त सक्रिय असलेल्या न्यूरॉन्स आणि ज्यांना आपण अधिक सक्रिय होऊ इच्छितो

92
00:06:16,626 --> 00:06:18,100
त्यांच्यामध्ये घडते.

93
00:06:18,100 --> 00:06:21,770
एका अर्थाने, 2 पाहताना फायरिंग होणारे न्यूरॉन्स त्याबद्दल

94
00:06:21,770 --> 00:06:25,440
विचार करताना गोळीबार करणाऱ्यांशी अधिक दृढपणे जोडले जातात.

95
00:06:25,440 --> 00:06:29,401
स्पष्टपणे सांगायचे तर, न्यूरॉन्सचे कृत्रिम नेटवर्क जैविक मेंदूसारखे काहीही

96
00:06:29,401 --> 00:06:33,837
वागतात की नाही याबद्दल एक किंवा दुसर्‍या प्रकारे विधाने करण्याच्या स्थितीत मी नाही,

97
00:06:33,837 --> 00:06:37,534
आणि हे एकत्र वायर एकत्र जमते ही कल्पना दोन अर्थपूर्ण तारकांसोबत येते,

98
00:06:37,534 --> 00:06:41,760
परंतु ती खूप सैल म्हणून घेतली जाते. साधर्म्य, मला हे लक्षात घेणे मनोरंजक वाटते.

99
00:06:41,760 --> 00:06:45,725
असं असलं तरी, या न्यूरॉनचे सक्रियकरण वाढवण्यात मदत करण्याचा

100
00:06:45,725 --> 00:06:49,360
तिसरा मार्ग म्हणजे मागील लेयरमधील सर्व सक्रियता बदलणे.

101
00:06:49,360 --> 00:06:55,866
उदाहरणार्थ, जर पॉझिटिव्ह वजनासह त्या अंक 2 न्यूरॉनशी जोडलेली प्रत्येक गोष्ट उजळ झाली

102
00:06:55,866 --> 00:07:02,680
आणि नकारात्मक वजनाशी जोडलेली प्रत्येक गोष्ट मंद झाली, तर अंक 2 न्यूरॉन अधिक सक्रिय होईल.

103
00:07:02,680 --> 00:07:06,696
आणि वजनातील बदलांप्रमाणेच, संबंधित वजनाच्या आकाराच्या प्रमाणात

104
00:07:06,696 --> 00:07:10,840
बदल शोधून तुम्हाला तुमच्या पैशासाठी सर्वात मोठा फायदा होणार आहे.

105
00:07:10,840 --> 00:07:15,064
आता अर्थातच, आम्ही त्या सक्रियतेवर थेट प्रभाव टाकू शकत नाही,

106
00:07:15,064 --> 00:07:18,320
आमचे फक्त वजन आणि पूर्वाग्रहांवर नियंत्रण आहे.

107
00:07:18,320 --> 00:07:23,960
परंतु शेवटच्या लेयरप्रमाणेच, ते इच्छित बदल काय आहेत याची नोंद ठेवणे उपयुक्त आहे.

108
00:07:23,960 --> 00:07:30,040
पण लक्षात ठेवा, येथे एक पायरी झूम आउट करा, फक्त ते अंक 2 आउटपुट न्यूरॉनला हवे आहे.

109
00:07:30,040 --> 00:07:34,135
लक्षात ठेवा, शेवटच्या लेयरमधील इतर सर्व न्यूरॉन्स कमी सक्रिय

110
00:07:34,135 --> 00:07:38,432
व्हावेत अशी आमची इच्छा आहे आणि त्या प्रत्येक आउटपुट न्यूरॉन्सचे

111
00:07:38,432 --> 00:07:43,200
स्वतःचे विचार आहेत की त्या दुसऱ्या ते शेवटच्या लेयरचे काय झाले पाहिजे.

112
00:07:43,200 --> 00:07:49,278
त्यामुळे या अंक 2 न्यूरॉनची इच्छा या दुसऱ्या ते शेवटच्या लेयरचे काय व्हायला हवे

113
00:07:49,278 --> 00:07:53,761
यासाठी इतर सर्व आउटपुट न्यूरॉन्सच्या इच्छेसोबत जोडले जाते,

114
00:07:53,761 --> 00:07:59,992
पुन्हा संबंधित वजनाच्या प्रमाणात आणि त्या प्रत्येक न्यूरॉन्सला किती आवश्यक आहे या

115
00:07:59,992 --> 00:08:01,740
प्रमाणात. बदलण्यासाठी.

116
00:08:01,740 --> 00:08:05,940
इथेच मागच्या बाजूने प्रचार करण्याची कल्पना येते.

117
00:08:05,940 --> 00:08:10,189
हे सर्व इच्छित प्रभाव एकत्र जोडून, तुम्हाला मुळात या दुसऱ्या

118
00:08:10,189 --> 00:08:14,300
ते शेवटच्या लेयरमध्ये घडू इच्छित असलेल्या नजची सूची मिळते.

119
00:08:14,300 --> 00:08:18,803
आणि एकदा तुमच्याकडे ती झाली की, तुम्ही तीच प्रक्रिया संबंधित वजन आणि

120
00:08:18,803 --> 00:08:22,653
पूर्वाग्रहांवर लागू करू शकता जे ती मूल्ये निर्धारित करतात,

121
00:08:22,653 --> 00:08:27,287
मी नुकतीच ज्या प्रक्रियेतून गेलो होतो आणि नेटवर्कमधून मागे सरकतो त्याच

122
00:08:27,287 --> 00:08:29,180
प्रक्रियेची पुनरावृत्ती करा.

123
00:08:29,180 --> 00:08:33,318
आणि थोडे पुढे झूम करून, लक्षात ठेवा की हे सर्व फक्त एक प्रशिक्षण

124
00:08:33,318 --> 00:08:37,520
उदाहरण त्या प्रत्येक वजन आणि पूर्वाग्रहांना धक्का देऊ इच्छित आहे.

125
00:08:37,520 --> 00:08:39,945
जर आम्ही फक्त त्या 2 ला काय हवे आहे ते ऐकले तर,

126
00:08:39,945 --> 00:08:44,140
सर्व प्रतिमांना 2 म्हणून वर्गीकृत करण्यासाठी नेटवर्कला शेवटी प्रोत्साहन दिले जाईल.

127
00:08:44,140 --> 00:08:51,909
तर तुम्ही काय करता ते प्रत्येक इतर प्रशिक्षण उदाहरणासाठी याच बॅकप्रॉप रूटीनमधून जाणे,

128
00:08:51,909 --> 00:08:57,963
त्यांच्यापैकी प्रत्येकाला वजन आणि पूर्वाग्रह कसे बदलायचे आहेत याची

129
00:08:57,963 --> 00:09:02,300
नोंद करणे आणि इच्छित बदलांची सरासरी एकत्र करणे.

130
00:09:02,300 --> 00:09:05,978
प्रत्येक वजन आणि पूर्वाग्रहासाठी सरासरी नजचा येथे हा संग्रह,

131
00:09:05,978 --> 00:09:10,018
अगदी सहज सांगायचे तर, शेवटच्या व्हिडिओमध्ये संदर्भित केलेल्या खर्च

132
00:09:10,018 --> 00:09:14,360
कार्याचा नकारात्मक ग्रेडियंट किंवा किमान त्याच्या प्रमाणात काहीतरी आहे.

133
00:09:14,360 --> 00:09:19,768
मी सैलपणे बोलतोय कारण मला अजून त्या नडजबद्दल परिमाणवाचक तंतोतंत मिळणे बाकी आहे,

134
00:09:19,768 --> 00:09:23,824
परंतु जर तुम्हाला मी संदर्भ दिलेला प्रत्येक बदल समजला असेल,

135
00:09:23,824 --> 00:09:28,826
तर काही इतरांपेक्षा प्रमाणानुसार का मोठे आहेत आणि ते सर्व एकत्र कसे जोडले

136
00:09:28,826 --> 00:09:34,100
जाणे आवश्यक आहे, तुम्हाला समजले असेल backpropagation प्रत्यक्षात काय करत आहे.

137
00:09:34,100 --> 00:09:38,464
तसे, सराव मध्ये, प्रत्येक ग्रेडियंट डिसेंट पायरीवर प्रत्येक

138
00:09:38,464 --> 00:09:43,120
प्रशिक्षण उदाहरणाचा प्रभाव जोडण्यासाठी संगणकांना खूप वेळ लागतो.

139
00:09:43,120 --> 00:09:45,540
तर त्याऐवजी सामान्यतः काय केले जाते ते येथे आहे.

140
00:09:45,540 --> 00:09:49,696
तुम्ही तुमचा प्रशिक्षण डेटा यादृच्छिकपणे बदलता आणि त्यास संपूर्ण मिनी-बॅचमध्ये

141
00:09:49,696 --> 00:09:53,380
विभाजित करा, चला प्रत्येकाकडे 100 प्रशिक्षण उदाहरणे आहेत असे म्हणूया.

142
00:09:53,380 --> 00:09:56,980
मग आपण मिनी-बॅचनुसार एक चरण मोजा.

143
00:09:56,980 --> 00:10:01,669
हा खर्च फंक्शनचा वास्तविक ग्रेडियंट नाही, जो सर्व प्रशिक्षण डेटावर अवलंबून असतो,

144
00:10:01,669 --> 00:10:05,721
या लहान उपसंचावर नाही, म्हणून ही सर्वात कार्यक्षम पायरी उतरणीवर नाही,

145
00:10:05,721 --> 00:10:09,368
परंतु प्रत्येक मिनी-बॅच तुम्हाला एक चांगला अंदाज देते आणि अधिक

146
00:10:09,368 --> 00:10:12,900
महत्त्वाचे म्हणजे ते तुम्हाला महत्त्वपूर्ण संगणकीय गती देते.

147
00:10:12,900 --> 00:10:17,682
जर तुम्ही तुमच्या नेटवर्कचा मार्ग संबंधित खर्चाच्या पृष्ठभागाखाली प्लॉट करत असाल,

148
00:10:17,682 --> 00:10:22,405
तर प्रत्येक पायरीची अचूक उताराची दिशा ठरवणार्‍या सावधपणे मोजणार्‍या माणसापेक्षा,

149
00:10:22,405 --> 00:10:26,604
एखाद्या नशेतल्या माणसाने टेकडीवरून उद्दिष्ट न ठेवता अडखळल्यासारखे होईल,

150
00:10:26,604 --> 00:10:31,620
परंतु वेगवान पावले उचलली पाहिजेत. त्या दिशेने खूप सावकाश आणि सावध पाऊल टाकण्यापूर्वी.

151
00:10:31,620 --> 00:10:35,200
या तंत्राला स्टोकास्टिक ग्रेडियंट डिसेंट असे संबोधले जाते.

152
00:10:35,200 --> 00:10:40,400
येथे बरेच काही चालले आहे, तर आपण ते स्वतःसाठी एकत्र करूया, का?

153
00:10:40,400 --> 00:10:45,722
बॅकप्रोपॅगेशन हे एका प्रशिक्षणाच्या उदाहरणाने वजन आणि पूर्वाग्रह कसे हलवायचे आहे हे

154
00:10:45,722 --> 00:10:50,854
ठरविण्याचे अल्गोरिदम आहे, केवळ ते वर किंवा खाली जावे की नाही या संदर्भात नाही तर

155
00:10:50,854 --> 00:10:56,240
त्या बदलांच्या सापेक्ष प्रमाणात कोणत्या प्रमाणात सर्वात जलद घट होते याच्या दृष्टीने.

156
00:10:56,240 --> 00:11:00,855
खर्च खर्‍या ग्रेडियंट डिसेंट पायरीमध्ये तुमच्या सर्व दहापट आणि हजारो प्रशिक्षण

157
00:11:00,855 --> 00:11:05,937
उदाहरणांसाठी हे करणे आणि तुम्हाला मिळणाऱ्या इच्छित बदलांची सरासरी काढणे समाविष्ट असते,

158
00:11:05,937 --> 00:11:10,202
परंतु ते संगणकीयदृष्ट्या धीमे आहे, त्यामुळे त्याऐवजी तुम्ही यादृच्छिकपणे

159
00:11:10,202 --> 00:11:14,000
मिनी-बॅचेसमध्ये डेटाचे विभाजन करा आणि प्रत्येक पायरीची गणना करा.

160
00:11:14,000 --> 00:11:19,014
मिनी बॅच सर्व मिनी-बॅचेसमधून वारंवार जाणे आणि या ऍडजस्टमेंट केल्याने,

161
00:11:19,014 --> 00:11:22,883
तुम्ही स्थानिक किमान खर्चाच्या कार्याकडे एकरूप व्हाल,

162
00:11:22,883 --> 00:11:27,540
म्हणजे तुमचे नेटवर्क प्रशिक्षण उदाहरणांवर खरोखर चांगले काम करेल.

163
00:11:27,540 --> 00:11:32,641
तर या सर्व गोष्टींसह, कोडची प्रत्येक ओळ जी बॅकप्रॉपच्या अंमलबजावणीमध्ये जाईल ती

164
00:11:32,641 --> 00:11:37,680
प्रत्यक्षात आपण आता पाहिलेल्या गोष्टीशी संबंधित आहे, किमान अनौपचारिक दृष्टीने.

165
00:11:37,680 --> 00:11:41,183
परंतु काहीवेळा गणित काय करते हे जाणून घेणे ही केवळ अर्धी लढाई असते आणि केवळ

166
00:11:41,183 --> 00:11:44,780
निंदनीय गोष्टीचे प्रतिनिधित्व करणे हे सर्व गोंधळलेले आणि गोंधळात टाकणारे आहे.

167
00:11:44,780 --> 00:11:47,640
तर, तुमच्यापैकी ज्यांना अधिक खोलात जायचे आहे त्यांच्यासाठी,

168
00:11:47,640 --> 00:11:51,453
पुढील व्हिडिओ त्याच कल्पनांमधून जातो ज्या नुकत्याच येथे मांडल्या गेल्या होत्या,

169
00:11:51,453 --> 00:11:55,648
परंतु अंतर्निहित कॅल्क्युलसच्या संदर्भात, ज्याने आशा आहे की तुम्ही विषय पाहता तेव्हा ते

170
00:11:55,648 --> 00:11:57,460
थोडे अधिक परिचित व्हावे. इतर संसाधने.

171
00:11:57,460 --> 00:12:01,134
त्याआधी, एका गोष्टीवर जोर देण्यासारखे आहे की हे अल्गोरिदम कार्य करण्यासाठी,

172
00:12:01,134 --> 00:12:04,809
आणि हे फक्त न्यूरल नेटवर्कच्या पलीकडे सर्व प्रकारच्या मशीन लर्निंगसाठी आहे,

173
00:12:04,809 --> 00:12:06,840
तुम्हाला भरपूर प्रशिक्षण डेटा आवश्यक आहे.

174
00:12:06,840 --> 00:12:11,110
आमच्या बाबतीत, एक गोष्ट जी हस्तलिखीत अंकांना इतके छान उदाहरण बनवते ती म्हणजे

175
00:12:11,110 --> 00:12:15,380
MNIST डेटाबेस अस्तित्वात आहे, ज्याची अनेक उदाहरणे मानवांनी लेबल केलेली आहेत.

176
00:12:15,380 --> 00:12:19,119
त्यामुळे तुमच्यापैकी जे मशीन लर्निंगमध्ये काम करत आहेत त्यांच्याशी परिचित असलेले एक

177
00:12:19,119 --> 00:12:22,948
सामान्य आव्हान म्हणजे तुम्हाला खरोखर आवश्यक असलेला लेबल केलेला प्रशिक्षण डेटा मिळवणे,

178
00:12:22,948 --> 00:12:26,821
मग त्यात लोकांनी हजारो प्रतिमांना लेबल लावले असेल किंवा इतर कोणताही डेटा प्रकार तुम्ही

179
00:12:26,821 --> 00:12:27,400
हाताळत असाल.

