1
00:00:04,060 --> 00:00:08,880
येथे, आम्ही बॅकप्रॉपगेशन हाताळतो, न्यूरल नेटवर्क कसे शिकतात यामागील कोर अल्गोरिदम. 

2
00:00:09,400 --> 00:00:13,150
आपण कोठे आहोत याचे द्रुत रीकॅप केल्यानंतर, सूत्रांचा कोणताही संदर्भ न घेता, 

3
00:00:13,150 --> 00:00:17,000
अल्गोरिदम प्रत्यक्षात काय करत आहे यासाठी मी एक अंतर्ज्ञानी वॉकथ्रू करणार आहे. 

4
00:00:17,660 --> 00:00:20,147
मग, तुमच्यापैकी ज्यांना गणितात उतरायचे आहे त्यांच्यासाठी, 

5
00:00:20,147 --> 00:00:23,020
पुढील व्हिडिओ या सर्व गोष्टींच्या अंतर्निहित कॅल्क्युलसमध्ये जातो. 

6
00:00:23,820 --> 00:00:27,518
तुम्ही शेवटचे दोन व्हिडिओ पाहिल्यास, किंवा तुम्ही योग्य पार्श्वभूमीसह उडी मारत असाल, 

7
00:00:27,518 --> 00:00:31,000
तर तुम्हाला माहीत आहे की न्यूरल नेटवर्क काय आहे आणि ते माहिती फॉरवर्ड कसे करते. 

8
00:00:31,680 --> 00:00:36,019
येथे, आम्ही हस्तलिखित अंक ओळखण्याचे उत्कृष्ट उदाहरण करीत आहोत ज्यांचे पिक्सेल मूल्य 

9
00:00:36,019 --> 00:00:40,411
784 न्यूरॉन्स असलेल्या नेटवर्कच्या पहिल्या स्तरामध्ये दिले जाते आणि मी दोन छुपे स्तर 

10
00:00:40,411 --> 00:00:44,906
असलेले नेटवर्क दाखवत आहे ज्यामध्ये प्रत्येकी फक्त 16 न्यूरॉन्स आहेत आणि एक आउटपुट आहे. 

11
00:00:44,906 --> 00:00:49,040
10 न्यूरॉन्सचा थर, नेटवर्क त्याचे उत्तर म्हणून कोणता अंक निवडत आहे हे दर्शविते. 

12
00:00:50,040 --> 00:00:54,562
शेवटच्या व्हिडीओमध्ये वर्णन केल्याप्रमाणे तुम्ही ग्रेडियंट डिसेंट समजून घ्याल, 

13
00:00:54,562 --> 00:00:58,111
आणि आम्ही शिकून घेतलेला अर्थ काय आहे हे आम्ही शोधू इच्छितो की 

14
00:00:58,111 --> 00:01:01,260
कोणते वजन आणि पक्षपात विशिष्ट खर्चाचे कार्य कमी करतात. 

15
00:01:02,040 --> 00:01:08,612
द्रुत स्मरणपत्र म्हणून, एका प्रशिक्षणाच्या उदाहरणासाठी, तुम्ही नेटवर्क देत असलेले आउटपुट, 

16
00:01:08,612 --> 00:01:14,600
तुम्हाला ते देऊ इच्छित असलेल्या आउटपुटसह आणि प्रत्येक घटकातील फरकांचे वर्ग जोडता. 

17
00:01:15,380 --> 00:01:20,373
तुमच्या सर्व हजारो प्रशिक्षण उदाहरणांसाठी हे केल्याने आणि परिणामांची सरासरी काढणे, 

18
00:01:20,373 --> 00:01:23,020
यामुळे तुम्हाला नेटवर्कची एकूण किंमत मिळते. 

19
00:01:23,020 --> 00:01:27,059
शेवटच्या व्हिडिओमध्ये वर्णन केल्याप्रमाणे विचार करणे पुरेसे नाही, 

20
00:01:27,059 --> 00:01:30,364
आम्ही या खर्च कार्याचा नकारात्मक ग्रेडियंट शोधत आहोत, 

21
00:01:30,364 --> 00:01:35,504
जी तुम्हाला सर्व वजन आणि पूर्वाग्रह कसे बदलण्याची आवश्यकता आहे हे सांगते. ही जोडणी, 

22
00:01:35,504 --> 00:01:38,320
जेणेकरून सर्वात कार्यक्षमतेने किंमत कमी होईल. 

23
00:01:43,260 --> 00:01:46,582
बॅकप्रोपगेशन, या व्हिडिओचा विषय, त्या वेडा क्लिष्ट 

24
00:01:46,582 --> 00:01:49,580
ग्रेडियंटची गणना करण्यासाठी एक अल्गोरिदम आहे. 

25
00:01:49,580 --> 00:01:52,994
शेवटच्या व्हिडीओमधली एक कल्पना जी तुम्ही आत्ता तुमच्या मनात घट्ट धरून 

26
00:01:52,994 --> 00:01:56,262
ठेवावी अशी माझी इच्छा आहे ती म्हणजे 13,000 परिमाणांमध्ये ग्रेडियंट 

27
00:01:56,262 --> 00:02:00,116
वेक्टरची दिशा म्हणून विचार करणे म्हणजे, आपल्या कल्पनेच्या व्याप्तीच्या पलीकडे, 

28
00:02:00,116 --> 00:02:03,580
हलक्या शब्दात सांगायचे तर दुसरी गोष्ट आहे. आपण याबद्दल विचार करू शकता. 

29
00:02:04,600 --> 00:02:07,526
येथे प्रत्येक घटकाचे परिमाण तुम्हाला प्रत्येक वजन आणि 

30
00:02:07,526 --> 00:02:10,940
पूर्वाग्रहासाठी किमतीचे कार्य किती संवेदनशील आहे हे सांगत आहे. 

31
00:02:11,800 --> 00:02:16,533
उदाहरणार्थ, मी ज्या प्रक्रियेचे वर्णन करणार आहे त्या प्रक्रियेतून तुम्ही 

32
00:02:16,533 --> 00:02:21,202
गेलात आणि नकारात्मक ग्रेडियंटची गणना करा आणि या काठावरील वजनाशी संबंधित 

33
00:02:21,202 --> 00:02:26,260
घटक 3 असेल असे समजा. 2, तर येथे या काठाशी संबंधित घटक 0 म्हणून बाहेर येतो. १. 

34
00:02:26,820 --> 00:02:30,806
तुम्ही ज्या प्रकारे त्याचा अर्थ लावाल ते म्हणजे फंक्शनची किंमत त्या 

35
00:02:30,806 --> 00:02:34,324
पहिल्या वजनातील बदलांच्या तुलनेत 32 पट अधिक संवेदनशील असते, 

36
00:02:34,324 --> 00:02:37,079
म्हणून जर तुम्ही ते मूल्य थोडेसे हलवायचे असेल, 

37
00:02:37,079 --> 00:02:41,008
तर ते खर्चात काही बदल घडवून आणेल आणि तो बदल ते दुसऱ्या वजनाला समान 

38
00:02:41,008 --> 00:02:43,060
वळवळ देण्यापेक्षा 32 पट जास्त आहे. 

39
00:02:48,420 --> 00:02:51,348
व्यक्तिशः, जेव्हा मी पहिल्यांदा बॅकप्रोपॅगेशनबद्दल शिकत होतो, 

40
00:02:51,348 --> 00:02:55,126
तेव्हा मला वाटते की सर्वात गोंधळात टाकणारी बाब म्हणजे फक्त नोटेशन आणि इंडेक्सचा 

41
00:02:55,126 --> 00:02:55,740
पाठलाग करणे. 

42
00:02:56,220 --> 00:03:00,306
परंतु एकदा तुम्ही या अल्गोरिदमचा प्रत्येक भाग खरोखर काय करत आहे हे उघडल्यानंतर, 

43
00:03:00,306 --> 00:03:04,086
त्याचा होणारा प्रत्येक वैयक्तिक प्रभाव प्रत्यक्षात खूपच अंतर्ज्ञानी असतो, 

44
00:03:04,086 --> 00:03:06,640
इतकेच की एकमेकांच्या वर अनेक लहान समायोजने होतात. 

45
00:03:07,740 --> 00:03:11,477
म्हणून मी नोटेशनकडे पूर्णपणे दुर्लक्ष करून गोष्टी सुरू करणार आहे, 

46
00:03:11,477 --> 00:03:16,120
आणि प्रत्येक प्रशिक्षण उदाहरणाचे वजन आणि पूर्वाग्रहांवर होणारे परिणाम जाणून घ्या. 

47
00:03:17,020 --> 00:03:21,483
कारण कॉस्ट फंक्शनमध्ये सर्व दहा हजार प्रशिक्षण उदाहरणांवर प्रति उदाहरण 

48
00:03:21,483 --> 00:03:26,073
विशिष्ट खर्चाचा सरासरी समावेश असतो, आम्ही एका ग्रेडियंट डिसेंट पायरीसाठी 

49
00:03:26,073 --> 00:03:31,040
वजन आणि पूर्वाग्रह कसे समायोजित करतो हे देखील प्रत्येक उदाहरणावर अवलंबून असते. 

50
00:03:31,680 --> 00:03:35,503
किंवा त्याऐवजी, तत्त्वतः ते केले पाहिजे, परंतु संगणकीय कार्यक्षमतेसाठी आम्ही नंतर एक छोटी 

51
00:03:35,503 --> 00:03:39,200
युक्ती करू ज्यामुळे तुम्हाला प्रत्येक चरणासाठी प्रत्येक उदाहरणे मारण्याची गरज पडू नये. 

52
00:03:39,200 --> 00:03:44,762
इतर प्रकरणांमध्ये, आत्ता आपण फक्त आपले लक्ष एका उदाहरणावर केंद्रित करणार आहोत, 

53
00:03:44,762 --> 00:03:45,960
2 ची ही प्रतिमा. 

54
00:03:46,720 --> 00:03:49,027
वजन आणि पूर्वाग्रह कसे समायोजित केले जातात यावर 

55
00:03:49,027 --> 00:03:51,480
या एका प्रशिक्षण उदाहरणाचा काय परिणाम झाला पाहिजे? 

56
00:03:52,680 --> 00:03:56,576
समजा आम्ही अशा टप्प्यावर आहोत जिथे नेटवर्क अद्याप चांगले प्रशिक्षित नाही, 

57
00:03:56,576 --> 00:04:01,157
त्यामुळे आउटपुटमधील सक्रियता खूपच यादृच्छिक दिसत आहेत, कदाचित 0 सारखे काहीतरी. ५, ०.8, 

58
00:04:01,157 --> 00:04:02,000
0.2, वर आणि वर. 

59
00:04:02,520 --> 00:04:07,346
आम्ही ती सक्रियता थेट बदलू शकत नाही, आमचा फक्त वजन आणि पूर्वाग्रहांवर प्रभाव असतो, 

60
00:04:07,346 --> 00:04:12,580
परंतु त्या आउटपुट स्तरावर आम्हाला कोणते समायोजन करायचे आहे याचा मागोवा ठेवणे उपयुक्त आहे. 

61
00:04:13,360 --> 00:04:16,991
आणि आम्हाला प्रतिमेचे 2 म्हणून वर्गीकरण करायचे असल्याने, 

62
00:04:16,991 --> 00:04:21,260
आम्हाला ते तिसरे मूल्य वाढवायचे आहे आणि इतर सर्व खाली ढकलले जावेत. 

63
00:04:22,060 --> 00:04:26,103
शिवाय, प्रत्येक वर्तमान मूल्य त्याच्या लक्ष्य मूल्यापासून 

64
00:04:26,103 --> 00:04:29,520
किती दूर आहे याच्या प्रमाणात या नजचा आकार असावा. 

65
00:04:30,220 --> 00:04:35,438
उदाहरणार्थ, संख्या 2 न्यूरॉनच्या सक्रियतेमध्ये वाढ होणे हे एका अर्थाने 8 क्रमांकाच्या 

66
00:04:35,438 --> 00:04:40,900
न्यूरॉनच्या कमी होण्यापेक्षा अधिक महत्त्वाचे आहे, जे ते जेथे असावे त्याच्या अगदी जवळ आहे. 

67
00:04:42,040 --> 00:04:45,412
म्हणून आणखी झूम करून, फक्त या एका न्यूरॉनवर लक्ष केंद्रित करूया, 

68
00:04:45,412 --> 00:04:47,280
ज्याचे सक्रियकरण आपण वाढवू इच्छितो. 

69
00:04:48,180 --> 00:04:52,527
लक्षात ठेवा, ते सक्रियकरण मागील लेयरमधील सर्व सक्रियतेची विशिष्ट भारित 

70
00:04:52,527 --> 00:04:55,712
बेरीज म्हणून परिभाषित केले आहे, तसेच एक पूर्वाग्रह, 

71
00:04:55,712 --> 00:05:01,040
जे सर्व नंतर सिग्मॉइड स्क्विशिफिकेशन फंक्शन किंवा ReLU सारखे काहीतरी प्लग इन केले आहे. 

72
00:05:01,640 --> 00:05:04,845
त्यामुळे तीन भिन्न मार्ग आहेत जे ते सक्रियता वाढविण्यात 

73
00:05:04,845 --> 00:05:07,020
मदत करण्यासाठी एकत्र कार्य करू शकतात. 

74
00:05:07,440 --> 00:05:10,605
तुम्ही पूर्वाग्रह वाढवू शकता, तुम्ही वजन वाढवू 

75
00:05:10,605 --> 00:05:14,040
शकता आणि तुम्ही मागील लेयरमधून सक्रियता बदलू शकता. 

76
00:05:14,940 --> 00:05:17,817
वजन कसे समायोजित केले जावे यावर लक्ष केंद्रित करून, 

77
00:05:17,817 --> 00:05:20,860
वजनाचे प्रत्यक्षात भिन्न स्तर कसे आहेत ते लक्षात घ्या. 

78
00:05:21,440 --> 00:05:25,377
आधीच्या लेयरमधील सर्वात तेजस्वी न्यूरॉन्ससह कनेक्शनचा सर्वात मोठा प्रभाव 

79
00:05:25,377 --> 00:05:29,100
असतो कारण त्या वजनांना मोठ्या सक्रियकरण मूल्यांनी गुणाकार केला जातो. 

80
00:05:31,460 --> 00:05:34,567
त्यामुळे जर तुम्हाला यापैकी एक वजन वाढवायचे असेल तर, 

81
00:05:34,567 --> 00:05:37,851
कमीत कमी या एका प्रशिक्षण उदाहरणाचा संबंध आहे तोपर्यंत, 

82
00:05:37,851 --> 00:05:41,955
मंद न्यूरॉन्ससह कनेक्शनचे वजन वाढवण्यापेक्षा अंतिम खर्चाच्या कार्यावर 

83
00:05:41,955 --> 00:05:43,480
त्याचा प्रभाव जास्त असतो. 

84
00:05:44,420 --> 00:05:47,218
लक्षात ठेवा, जेव्हा आपण ग्रेडियंट डिसेंट बद्दल बोलतो तेव्हा प्रत्येक 

85
00:05:47,218 --> 00:05:49,651
घटकाला वर किंवा खाली नेले जावे याकडेच आम्‍ही लक्ष देत नाही, 

86
00:05:49,651 --> 00:05:53,220
तर तुमच्‍या पैशासाठी कोणता घटक तुम्‍हाला सर्वात जास्त दणका देतो याची आम्‍ही काळजी घेतो. 

87
00:05:55,020 --> 00:05:59,040
हे, तसे, न्यूरॉन्सचे जैविक नेटवर्क कसे शिकतात याबद्दल न्यूरोसायन्समधील 

88
00:05:59,040 --> 00:06:02,269
सिद्धांताची किमान आठवण करून देणारा आहे, हेबियन सिद्धांत, 

89
00:06:02,269 --> 00:06:06,460
बहुतेकदा या वाक्यांशात सारांशित केला जातो, न्यूरॉन्स जे एकत्र वायर करतात. 

90
00:06:07,260 --> 00:06:10,978
येथे, वजनात सर्वात मोठी वाढ, कनेक्शनची सर्वात मोठी मजबूती, 

91
00:06:10,978 --> 00:06:15,956
सर्वात जास्त सक्रिय असलेल्या न्यूरॉन्स आणि ज्यांना आपण अधिक सक्रिय होऊ इच्छितो 

92
00:06:15,956 --> 00:06:17,280
त्यांच्यामध्ये घडते. 

93
00:06:17,940 --> 00:06:21,210
एका अर्थाने, 2 पाहताना फायरिंग होणारे न्यूरॉन्स त्याबद्दल 

94
00:06:21,210 --> 00:06:24,480
विचार करताना गोळीबार करणाऱ्यांशी अधिक दृढपणे जोडले जातात. 

95
00:06:25,400 --> 00:06:29,191
स्पष्टपणे सांगायचे तर, न्यूरॉन्सचे कृत्रिम नेटवर्क जैविक मेंदूसारखे काहीही 

96
00:06:29,191 --> 00:06:33,437
वागतात की नाही याबद्दल एक किंवा दुसर्‍या प्रकारे विधाने करण्याच्या स्थितीत मी नाही, 

97
00:06:33,437 --> 00:06:36,975
आणि हे एकत्र वायर एकत्र जमते ही कल्पना दोन अर्थपूर्ण तारकांसोबत येते, 

98
00:06:36,975 --> 00:06:41,020
परंतु ती खूप सैल म्हणून घेतली जाते. साधर्म्य, मला हे लक्षात घेणे मनोरंजक वाटते. 

99
00:06:41,940 --> 00:06:45,644
असं असलं तरी, या न्यूरॉनचे सक्रियकरण वाढवण्यात मदत करण्याचा 

100
00:06:45,644 --> 00:06:49,040
तिसरा मार्ग म्हणजे मागील लेयरमधील सर्व सक्रियता बदलणे. 

101
00:06:49,040 --> 00:06:54,726
उदाहरणार्थ, जर पॉझिटिव्ह वजनासह त्या अंक 2 न्यूरॉनशी जोडलेली प्रत्येक गोष्ट उजळ झाली 

102
00:06:54,726 --> 00:07:00,680
आणि नकारात्मक वजनाशी जोडलेली प्रत्येक गोष्ट मंद झाली, तर अंक 2 न्यूरॉन अधिक सक्रिय होईल. 

103
00:07:02,540 --> 00:07:06,349
आणि वजनातील बदलांप्रमाणेच, संबंधित वजनाच्या आकाराच्या प्रमाणात 

104
00:07:06,349 --> 00:07:10,280
बदल शोधून तुम्हाला तुमच्या पैशासाठी सर्वात मोठा फायदा होणार आहे. 

105
00:07:12,140 --> 00:07:15,156
आता अर्थातच, आम्ही त्या सक्रियतेवर थेट प्रभाव टाकू शकत नाही, 

106
00:07:15,156 --> 00:07:17,480
आमचे फक्त वजन आणि पूर्वाग्रहांवर नियंत्रण आहे. 

107
00:07:17,480 --> 00:07:24,120
परंतु शेवटच्या लेयरप्रमाणेच, ते इच्छित बदल काय आहेत याची नोंद ठेवणे उपयुक्त आहे. 

108
00:07:24,580 --> 00:07:29,200
पण लक्षात ठेवा, येथे एक पायरी झूम आउट करा, फक्त ते अंक 2 आउटपुट न्यूरॉनला हवे आहे. 

109
00:07:29,760 --> 00:07:32,822
लक्षात ठेवा, शेवटच्या लेयरमधील इतर सर्व न्यूरॉन्स कमी सक्रिय 

110
00:07:32,822 --> 00:07:36,035
व्हावेत अशी आमची इच्छा आहे आणि त्या प्रत्येक आउटपुट न्यूरॉन्सचे 

111
00:07:36,035 --> 00:07:39,600
स्वतःचे विचार आहेत की त्या दुसऱ्या ते शेवटच्या लेयरचे काय झाले पाहिजे. 

112
00:07:42,700 --> 00:07:48,608
त्यामुळे या अंक 2 न्यूरॉनची इच्छा या दुसऱ्या ते शेवटच्या लेयरचे काय व्हायला हवे 

113
00:07:48,608 --> 00:07:52,965
यासाठी इतर सर्व आउटपुट न्यूरॉन्सच्या इच्छेसोबत जोडले जाते, 

114
00:07:52,965 --> 00:07:59,021
पुन्हा संबंधित वजनाच्या प्रमाणात आणि त्या प्रत्येक न्यूरॉन्सला किती आवश्यक आहे या 

115
00:07:59,021 --> 00:08:00,720
प्रमाणात. बदलण्यासाठी. 

116
00:08:01,600 --> 00:08:05,480
इथेच मागच्या बाजूने प्रचार करण्याची कल्पना येते. 

117
00:08:05,820 --> 00:08:09,652
हे सर्व इच्छित प्रभाव एकत्र जोडून, तुम्हाला मुळात या दुसऱ्या 

118
00:08:09,652 --> 00:08:13,360
ते शेवटच्या लेयरमध्ये घडू इच्छित असलेल्या नजची सूची मिळते. 

119
00:08:14,220 --> 00:08:17,512
आणि एकदा तुमच्याकडे ती झाली की, तुम्ही तीच प्रक्रिया संबंधित वजन आणि 

120
00:08:17,512 --> 00:08:20,328
पूर्वाग्रहांवर लागू करू शकता जे ती मूल्ये निर्धारित करतात, 

121
00:08:20,328 --> 00:08:23,716
मी नुकतीच ज्या प्रक्रियेतून गेलो होतो आणि नेटवर्कमधून मागे सरकतो त्याच 

122
00:08:23,716 --> 00:08:25,100
प्रक्रियेची पुनरावृत्ती करा. 

123
00:08:28,960 --> 00:08:32,949
आणि थोडे पुढे झूम करून, लक्षात ठेवा की हे सर्व फक्त एक प्रशिक्षण 

124
00:08:32,949 --> 00:08:37,000
उदाहरण त्या प्रत्येक वजन आणि पूर्वाग्रहांना धक्का देऊ इच्छित आहे. 

125
00:08:37,480 --> 00:08:39,583
जर आम्ही फक्त त्या 2 ला काय हवे आहे ते ऐकले तर, 

126
00:08:39,583 --> 00:08:43,220
सर्व प्रतिमांना 2 म्हणून वर्गीकृत करण्यासाठी नेटवर्कला शेवटी प्रोत्साहन दिले जाईल. 

127
00:08:44,059 --> 00:08:49,168
तर तुम्ही काय करता ते प्रत्येक इतर प्रशिक्षण उदाहरणासाठी याच बॅकप्रॉप रूटीनमधून जाणे, 

128
00:08:49,168 --> 00:08:53,148
त्यांच्यापैकी प्रत्येकाला वजन आणि पूर्वाग्रह कसे बदलायचे आहेत याची 

129
00:08:53,148 --> 00:08:56,000
नोंद करणे आणि इच्छित बदलांची सरासरी एकत्र करणे. 

130
00:09:01,720 --> 00:09:05,367
प्रत्येक वजन आणि पूर्वाग्रहासाठी सरासरी नजचा येथे हा संग्रह, 

131
00:09:05,367 --> 00:09:09,374
अगदी सहज सांगायचे तर, शेवटच्या व्हिडिओमध्ये संदर्भित केलेल्या खर्च 

132
00:09:09,374 --> 00:09:13,680
कार्याचा नकारात्मक ग्रेडियंट किंवा किमान त्याच्या प्रमाणात काहीतरी आहे. 

133
00:09:14,380 --> 00:09:18,933
मी सैलपणे बोलतोय कारण मला अजून त्या नडजबद्दल परिमाणवाचक तंतोतंत मिळणे बाकी आहे, 

134
00:09:18,933 --> 00:09:22,348
परंतु जर तुम्हाला मी संदर्भ दिलेला प्रत्येक बदल समजला असेल, 

135
00:09:22,348 --> 00:09:26,560
तर काही इतरांपेक्षा प्रमाणानुसार का मोठे आहेत आणि ते सर्व एकत्र कसे जोडले 

136
00:09:26,560 --> 00:09:31,000
जाणे आवश्यक आहे, तुम्हाला समजले असेल backpropagation प्रत्यक्षात काय करत आहे. 

137
00:09:33,960 --> 00:09:38,063
तसे, सराव मध्ये, प्रत्येक ग्रेडियंट डिसेंट पायरीवर प्रत्येक 

138
00:09:38,063 --> 00:09:42,440
प्रशिक्षण उदाहरणाचा प्रभाव जोडण्यासाठी संगणकांना खूप वेळ लागतो. 

139
00:09:43,140 --> 00:09:44,820
तर त्याऐवजी सामान्यतः काय केले जाते ते येथे आहे. 

140
00:09:45,480 --> 00:09:49,159
तुम्ही तुमचा प्रशिक्षण डेटा यादृच्छिकपणे बदलता आणि त्यास संपूर्ण मिनी-बॅचमध्ये 

141
00:09:49,159 --> 00:09:52,420
विभाजित करा, चला प्रत्येकाकडे 100 प्रशिक्षण उदाहरणे आहेत असे म्हणूया. 

142
00:09:52,939 --> 00:09:57,280
मग आपण मिनी-बॅचनुसार एक चरण मोजा. 

143
00:09:57,280 --> 00:10:01,651
हा खर्च फंक्शनचा वास्तविक ग्रेडियंट नाही, जो सर्व प्रशिक्षण डेटावर अवलंबून असतो, 

144
00:10:01,651 --> 00:10:05,428
या लहान उपसंचावर नाही, म्हणून ही सर्वात कार्यक्षम पायरी उतरणीवर नाही, 

145
00:10:05,428 --> 00:10:08,828
परंतु प्रत्येक मिनी-बॅच तुम्हाला एक चांगला अंदाज देते आणि अधिक 

146
00:10:08,828 --> 00:10:12,120
महत्त्वाचे म्हणजे ते तुम्हाला महत्त्वपूर्ण संगणकीय गती देते. 

147
00:10:12,820 --> 00:10:17,249
जर तुम्ही तुमच्या नेटवर्कचा मार्ग संबंधित खर्चाच्या पृष्ठभागाखाली प्लॉट करत असाल, 

148
00:10:17,249 --> 00:10:21,625
तर प्रत्येक पायरीची अचूक उताराची दिशा ठरवणार्‍या सावधपणे मोजणार्‍या माणसापेक्षा, 

149
00:10:21,625 --> 00:10:25,514
एखाद्या नशेतल्या माणसाने टेकडीवरून उद्दिष्ट न ठेवता अडखळल्यासारखे होईल, 

150
00:10:25,514 --> 00:10:30,160
परंतु वेगवान पावले उचलली पाहिजेत. त्या दिशेने खूप सावकाश आणि सावध पाऊल टाकण्यापूर्वी. 

151
00:10:31,540 --> 00:10:34,660
या तंत्राला स्टोकास्टिक ग्रेडियंट डिसेंट असे संबोधले जाते. 

152
00:10:35,960 --> 00:10:39,620
येथे बरेच काही चालले आहे, तर आपण ते स्वतःसाठी एकत्र करूया, का? 

153
00:10:40,440 --> 00:10:45,520
बॅकप्रोपॅगेशन हे एका प्रशिक्षणाच्या उदाहरणाने वजन आणि पूर्वाग्रह कसे हलवायचे आहे हे 

154
00:10:45,520 --> 00:10:50,419
ठरविण्याचे अल्गोरिदम आहे, केवळ ते वर किंवा खाली जावे की नाही या संदर्भात नाही तर 

155
00:10:50,419 --> 00:10:55,560
त्या बदलांच्या सापेक्ष प्रमाणात कोणत्या प्रमाणात सर्वात जलद घट होते याच्या दृष्टीने. 

156
00:10:56,260 --> 00:11:00,672
खर्च खर्‍या ग्रेडियंट डिसेंट पायरीमध्ये तुमच्या सर्व दहापट आणि हजारो प्रशिक्षण 

157
00:11:00,672 --> 00:11:05,531
उदाहरणांसाठी हे करणे आणि तुम्हाला मिळणाऱ्या इच्छित बदलांची सरासरी काढणे समाविष्ट असते, 

158
00:11:05,531 --> 00:11:09,609
परंतु ते संगणकीयदृष्ट्या धीमे आहे, त्यामुळे त्याऐवजी तुम्ही यादृच्छिकपणे 

159
00:11:09,609 --> 00:11:13,240
मिनी-बॅचेसमध्ये डेटाचे विभाजन करा आणि प्रत्येक पायरीची गणना करा. 

160
00:11:14,000 --> 00:11:18,274
मिनी बॅच सर्व मिनी-बॅचेसमधून वारंवार जाणे आणि या ऍडजस्टमेंट केल्याने, 

161
00:11:18,274 --> 00:11:21,571
तुम्ही स्थानिक किमान खर्चाच्या कार्याकडे एकरूप व्हाल, 

162
00:11:21,571 --> 00:11:25,540
म्हणजे तुमचे नेटवर्क प्रशिक्षण उदाहरणांवर खरोखर चांगले काम करेल. 

163
00:11:27,240 --> 00:11:32,009
तर या सर्व गोष्टींसह, कोडची प्रत्येक ओळ जी बॅकप्रॉपच्या अंमलबजावणीमध्ये जाईल ती 

164
00:11:32,009 --> 00:11:36,720
प्रत्यक्षात आपण आता पाहिलेल्या गोष्टीशी संबंधित आहे, किमान अनौपचारिक दृष्टीने. 

165
00:11:37,560 --> 00:11:40,797
परंतु काहीवेळा गणित काय करते हे जाणून घेणे ही केवळ अर्धी लढाई असते आणि केवळ 

166
00:11:40,797 --> 00:11:44,120
निंदनीय गोष्टीचे प्रतिनिधित्व करणे हे सर्व गोंधळलेले आणि गोंधळात टाकणारे आहे. 

167
00:11:44,860 --> 00:11:47,467
तर, तुमच्यापैकी ज्यांना अधिक खोलात जायचे आहे त्यांच्यासाठी, 

168
00:11:47,467 --> 00:11:50,944
पुढील व्हिडिओ त्याच कल्पनांमधून जातो ज्या नुकत्याच येथे मांडल्या गेल्या होत्या, 

169
00:11:50,944 --> 00:11:54,768
परंतु अंतर्निहित कॅल्क्युलसच्या संदर्भात, ज्याने आशा आहे की तुम्ही विषय पाहता तेव्हा ते 

170
00:11:54,768 --> 00:11:56,420
थोडे अधिक परिचित व्हावे. इतर संसाधने. 

171
00:11:57,340 --> 00:12:00,693
त्याआधी, एका गोष्टीवर जोर देण्यासारखे आहे की हे अल्गोरिदम कार्य करण्यासाठी, 

172
00:12:00,693 --> 00:12:04,046
आणि हे फक्त न्यूरल नेटवर्कच्या पलीकडे सर्व प्रकारच्या मशीन लर्निंगसाठी आहे, 

173
00:12:04,046 --> 00:12:05,900
तुम्हाला भरपूर प्रशिक्षण डेटा आवश्यक आहे. 

174
00:12:06,420 --> 00:12:10,579
आमच्या बाबतीत, एक गोष्ट जी हस्तलिखीत अंकांना इतके छान उदाहरण बनवते ती म्हणजे 

175
00:12:10,579 --> 00:12:14,740
MNIST डेटाबेस अस्तित्वात आहे, ज्याची अनेक उदाहरणे मानवांनी लेबल केलेली आहेत. 

176
00:12:15,300 --> 00:12:18,971
त्यामुळे तुमच्यापैकी जे मशीन लर्निंगमध्ये काम करत आहेत त्यांच्याशी परिचित असलेले एक 

177
00:12:18,971 --> 00:12:22,729
सामान्य आव्हान म्हणजे तुम्हाला खरोखर आवश्यक असलेला लेबल केलेला प्रशिक्षण डेटा मिळवणे, 

178
00:12:22,729 --> 00:12:26,531
मग त्यात लोकांनी हजारो प्रतिमांना लेबल लावले असेल किंवा इतर कोणताही डेटा प्रकार तुम्ही 

179
00:12:26,531 --> 00:12:27,100
हाताळत असाल. 

