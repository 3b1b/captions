1
00:00:04,350 --> 00:00:06,410
本期我们来讲反向传播

2
00:00:06,410 --> 00:00:09,400
也就是神经网络学习的核心算法

3
00:00:09,400 --> 00:00:11,210
稍微回顾一下我们之前讲到哪里之后

4
00:00:11,210 --> 00:00:15,470
首先我要撇开公式不提 直观地过一遍

5
00:00:15,470 --> 00:00:17,270
这个算法到底在做什么

6
00:00:17,640 --> 00:00:20,310
然后如果你们有人想认真看里头的数学

7
00:00:20,310 --> 00:00:23,140
下一期视频我会解释这一切背后的微积分

8
00:00:23,940 --> 00:00:25,550
如果你看了前两期视频

9
00:00:25,550 --> 00:00:27,920
或者你已经有足够背景  直接空降来这一期视频的话

10
00:00:27,920 --> 00:00:31,290
你一定知道神经网络是什么  以及它如何前馈信息的

11
00:00:31,660 --> 00:00:35,100
这里我们考虑的经典例子就是手写数字识别

12
00:00:35,100 --> 00:00:39,930
数字的像素值被输入到网络第一层的784个神经元里

13
00:00:39,930 --> 00:00:44,000
这里 我展示的是有2层16个神经元隐含层

14
00:00:44,000 --> 00:00:49,250
10个神经元的输出层 代表网络最终给出的选择

15
00:00:50,020 --> 00:00:54,340
我也假设你们已经理解了上期说到的梯度下降法

16
00:00:54,340 --> 00:00:56,890
理解了所谓学习就是指

17
00:00:56,890 --> 00:01:01,450
我们要找到特定的权重偏置 从而使一个代价函数最小化

18
00:01:02,010 --> 00:01:05,470
稍许提醒一下 计算一个训练样本的代价

19
00:01:05,470 --> 00:01:08,400
你需要求出网络的输出

20
00:01:08,400 --> 00:01:10,850
与期待的输出

21
00:01:11,200 --> 00:01:14,820
之间每一项的差的平方和

22
00:01:15,370 --> 00:01:20,020
然后对于成千上万个训练样本都这么算一遍 最后取平均

23
00:01:20,020 --> 00:01:22,410
这就得到了整个网络的代价值

24
00:01:22,910 --> 00:01:26,010
如果你嫌这还不够复杂的话 上集内容也讲到了

25
00:01:26,010 --> 00:01:30,870
我们要求的是代价函数的负梯度

26
00:01:30,870 --> 00:01:35,720
它告诉你如何改变所有连线上的权重偏置

27
00:01:35,720 --> 00:01:38,270
才好让代价下降得最快

28
00:01:42,950 --> 00:01:45,210
本集的中心 反向传播算法

29
00:01:45,210 --> 00:01:48,800
正是用来求这个复杂到爆的梯度的

30
00:01:49,490 --> 00:01:54,010
我希望大家能够把上集中提到的一点牢牢记住

31
00:01:54,010 --> 00:01:58,910
毕竟13000维的梯度向量

32
00:01:58,910 --> 00:02:02,090
说它是难以想象都不为过

33
00:02:02,090 --> 00:02:03,510
所以这里大家请记住另一套思路

34
00:02:04,580 --> 00:02:07,710
梯度向量每一项的大小是在告诉大家

35
00:02:07,710 --> 00:02:11,140
代价函数对于每个参数有多敏感

36
00:02:11,810 --> 00:02:14,580
比如说 你走了一段我讲的过程

37
00:02:14,580 --> 00:02:16,370
计算了负梯度

38
00:02:16,370 --> 00:02:21,470
对应这条线上这个权重的一项等于3.2

39
00:02:21,870 --> 00:02:26,370
而对应这条边上的一项等于0.1

40
00:02:26,910 --> 00:02:28,420
你可以这么来理解

41
00:02:28,420 --> 00:02:33,080
第一个权重对代价函数的值有32倍的影响力

42
00:02:33,640 --> 00:02:35,930
如果你稍微改变一下第一个权重

43
00:02:35,930 --> 00:02:38,190
它对代价值造成的变化

44
00:02:38,190 --> 00:02:43,200
就是改变第二个权重同等大小下的32倍

45
00:02:48,520 --> 00:02:51,440
就我个人而言 我刚开始学习反向传播的时候

46
00:02:51,440 --> 00:02:55,740
我觉得最容易搞混的部分就是各种符号和上标下标

47
00:02:56,180 --> 00:02:59,450
不过 一旦你捋清了算法的思路

48
00:02:59,450 --> 00:03:02,870
算法的每一步其实都挺直观的

49
00:03:03,180 --> 00:03:06,740
其实就是把许许多多微小的调整一层进一层地进行下去而已

50
00:03:07,660 --> 00:03:11,290
所以 开始讲解时 我将完全抛弃所有的符号

51
00:03:11,290 --> 00:03:13,370
给大家一步步解释

52
00:03:13,370 --> 00:03:16,350
每一个训练样本会对权重偏置的调整造成怎样的影响

53
00:03:17,090 --> 00:03:18,590
因为代价函数牵扯到

54
00:03:18,590 --> 00:03:23,640
对成千上万个训练样本的代价取平均值

55
00:03:23,970 --> 00:03:28,640
所以我们调整每一步梯度下降用的权重偏置

56
00:03:28,640 --> 00:03:31,140
也会基于所有的训练样本

57
00:03:31,680 --> 00:03:33,200
原理上是这么说

58
00:03:33,200 --> 00:03:35,930
但为了计算效率 之后咱们会讨个巧

59
00:03:35,930 --> 00:03:39,370
从而不必每一步都非得要计算所有的训练样本

60
00:03:39,790 --> 00:03:41,330
还需要说明一点

61
00:03:41,330 --> 00:03:46,160
我们现在只关注一个训练样本 就这张2

62
00:03:46,670 --> 00:03:51,650
这一个训练样本会对调整权重和偏置造成怎样的影响呢?

63
00:03:52,680 --> 00:03:55,240
现在假设网络还没有完全训练好

64
00:03:55,240 --> 00:03:57,970
那么输出层的激活值看起来就很随机

65
00:03:57,970 --> 00:04:02,040
也许就会出现0.5 0.8 0.2 等等等等

66
00:04:02,640 --> 00:04:07,450
我们并不能直接改动这些激活值  只能改变权重和偏置值

67
00:04:07,790 --> 00:04:12,670
但记住我们想要输出层出现怎样的变动  还是很有用的

68
00:04:13,270 --> 00:04:15,710
因为我们希望图像最终的分类结果是2

69
00:04:16,040 --> 00:04:21,360
我们希望第三个输出值变大  其他数值变小

70
00:04:22,040 --> 00:04:26,020
并且变动的大小应该与现在值和目标值之间的差呈正比

71
00:04:26,020 --> 00:04:29,630
并且变动的大小应该与现在值和目标值之间的差呈正比

72
00:04:30,220 --> 00:04:34,350
举个例子 增加数字”2”神经元的激活值

73
00:04:34,350 --> 00:04:38,490
就应该比减少数字”8”神经元的激活值来得重要

74
00:04:38,490 --> 00:04:40,630
因为后者已经很接近它的目标了

75
00:04:41,990 --> 00:04:45,250
那好 我们更进一步 就来关注下这一个神经元

76
00:04:45,250 --> 00:04:47,530
我们要让这里面的激活值变大

77
00:04:48,160 --> 00:04:50,550
还记得这个激活值是

78
00:04:50,550 --> 00:04:56,430
把前一层所有激活值的加权和 加上一个偏置

79
00:04:56,430 --> 00:05:01,290
再通过sigmoid ReLU之类的挤压函数 最后算出来的吧

80
00:05:01,810 --> 00:05:07,360
所以要增加这个激活值 我们有三条大路可走

81
00:05:07,680 --> 00:05:10,970
一增加偏置 二增加权重

82
00:05:10,970 --> 00:05:14,030
或者三改变上一层的激活值

83
00:05:14,950 --> 00:05:17,770
先来看如何调整权重

84
00:05:17,770 --> 00:05:21,410
各个权重它们的影响力各不相同

85
00:05:21,410 --> 00:05:25,750
连接前一层最亮的神经元的权重 影响力也最大

86
00:05:25,750 --> 00:05:29,240
因为这些权重会与大的激活值相乘

87
00:05:31,330 --> 00:05:33,480
所以至少对于这一个训练样本而言

88
00:05:33,480 --> 00:05:37,370
增大了这几个权重值 对最终代价函数造成的影响

89
00:05:37,370 --> 00:05:40,820
就比增大连接黯淡神经元的权重所造成的影响

90
00:05:40,820 --> 00:05:43,650
要大上好多倍

91
00:05:44,380 --> 00:05:46,890
请记住当我们说到梯度下降的时候

92
00:05:46,890 --> 00:05:50,620
我们并不只看每个参数是该增大还是减小

93
00:05:50,620 --> 00:05:53,370
我们还看该哪个参数的性价比最高

94
00:05:55,270 --> 00:05:59,310
顺便一提  这有一点点像描述生物中

95
00:05:59,310 --> 00:06:01,870
神经元的网络如何学习的一个理论

96
00:06:01,870 --> 00:06:06,820
“赫布理论”  总结起来就是“一同激活的神经元关联在一起”

97
00:06:07,260 --> 00:06:12,200
这里 权重的最大增长 即连接变得更强的部分

98
00:06:12,200 --> 00:06:14,840
就会发生在已经最活跃的神经元

99
00:06:14,840 --> 00:06:17,590
和想要更多激发的神经元之间

100
00:06:18,020 --> 00:06:21,060
可以说 看见一个2时激发的神经元

101
00:06:21,060 --> 00:06:24,680
会和”想到一个2”时激发的神经元联系地更紧密

102
00:06:25,420 --> 00:06:28,780
这里解释一下 我个人对人工神经网络是否真的在

103
00:06:28,780 --> 00:06:33,080
模仿生物学上大脑的工作 没有什么发言权

104
00:06:33,080 --> 00:06:37,250
“一同激活的神经元关联在一起”这句话是要打星号注释的

105
00:06:37,250 --> 00:06:41,260
但作为一个粗略的对照 我觉得还是挺有意思的

106
00:06:41,890 --> 00:06:46,020
言归正传 第三个能够增加这个神经元激活值的方法

107
00:06:46,020 --> 00:06:49,060
就是改变前一层的激活值

108
00:06:49,560 --> 00:06:54,970
更具体地说 如果所有正权重连接的神经元更亮

109
00:06:54,970 --> 00:06:57,960
所有负权重连接的神经元更暗的话

110
00:06:58,340 --> 00:07:00,890
那么数字2的神经元就会更强烈地激发

111
00:07:02,450 --> 00:07:06,130
和改权重的时候类似 我们想造成更大的影响

112
00:07:06,130 --> 00:07:10,550
就要依据对应权重的大小 对激活值做出呈比例的改变

113
00:07:12,120 --> 00:07:15,360
当然 我们并不能直接改变激活值

114
00:07:15,360 --> 00:07:17,780
我们手头只能控制权重和偏置

115
00:07:18,220 --> 00:07:23,610
但就光对最后一层来说 记住我们期待的变化还是很有帮助的

116
00:07:24,450 --> 00:07:29,720
不过别忘了 从全局上看 这只不过是数字2的神经元所期待的变化

117
00:07:29,720 --> 00:07:34,840
我们还需要最后一层其余的神经元的激发变弱

118
00:07:34,840 --> 00:07:36,500
但这其余的每个输出神经元

119
00:07:36,500 --> 00:07:39,840
对于如何改变倒数第二层 都有各自的想法

120
00:07:43,110 --> 00:07:46,140
所以 我们会把数字2神经元的期待

121
00:07:46,140 --> 00:07:50,520
和别的输出神经元的期待全部加起来

122
00:07:50,520 --> 00:07:53,240
作为对如何改变倒数第二层神经元的指示

123
00:07:53,580 --> 00:07:56,400
这些期待变化不仅是对应的权重的倍数

124
00:07:56,400 --> 00:08:00,910
也是每个神经元激活值改变量的倍数

125
00:08:01,480 --> 00:08:05,510
这其实就是在实现”反向传播”的理念了

126
00:08:05,960 --> 00:08:08,730
我们把所有期待的改变加起来

127
00:08:08,730 --> 00:08:13,560
就得到了一串对倒数第二层改动的变化量

128
00:08:14,180 --> 00:08:15,390
有了这些

129
00:08:15,390 --> 00:08:17,850
我们就可以重复这个过程

130
00:08:17,850 --> 00:08:21,180
改变影响倒数第二层神经元激活值的相关参数

131
00:08:21,180 --> 00:08:25,140
从后一层到前一层 把这个过程一直循环到第一层

132
00:08:29,030 --> 00:08:30,370
放眼大局

133
00:08:30,370 --> 00:08:31,920
还记得我们只是在讨论

134
00:08:31,920 --> 00:08:37,400
单个训练样本对所有权重偏置的影响吗？

135
00:08:37,400 --> 00:08:39,700
如果我们只关注那个“2”的要求

136
00:08:39,700 --> 00:08:43,400
最后  网络只会把所有图像都分类成是“2”

137
00:08:44,030 --> 00:08:49,420
所以你要对其他所有的训练样本 同样地过一遍反向传播

138
00:08:49,420 --> 00:08:53,200
记录下每个样本想怎样修改权重与偏置

139
00:08:53,650 --> 00:08:56,220
最后再取一个平均值

140
00:09:02,050 --> 00:09:06,940
这里一系列的权重偏置的平均微调大小

141
00:09:06,940 --> 00:09:11,910
不严格地说  就是上期视频提到的代价函数的负梯度

142
00:09:11,910 --> 00:09:13,740
至少是其标量的倍数

143
00:09:14,360 --> 00:09:19,570
这里的不严格  指的是我还没有准确地解释如何量化这些微调

144
00:09:19,570 --> 00:09:22,190
但如果你清楚我提到的所有改动

145
00:09:22,190 --> 00:09:24,770
为什么有些数字是其他数字的好几倍

146
00:09:24,770 --> 00:09:27,160
以及最后要怎么全部加起来

147
00:09:27,160 --> 00:09:31,170
你就懂得了反向传播的真实工作原理

148
00:09:34,050 --> 00:09:37,400
顺带一提 实际操作中 如果梯度下降的每一步

149
00:09:37,400 --> 00:09:42,490
都用上每一个训练样本来计算的话 那么花的时间就太长了

150
00:09:43,010 --> 00:09:44,960
所以我们一般会这么做

151
00:09:45,440 --> 00:09:50,280
首先把训练样本打乱 然后分成很多组minibatch

152
00:09:50,280 --> 00:09:52,680
每个minibatch就当包含100个训练样本好了

153
00:09:53,240 --> 00:09:56,430
然后你算出这个minibatch下降的一步

154
00:09:56,850 --> 00:09:59,390
这不是代价函数真正的梯度

155
00:09:59,390 --> 00:10:02,630
毕竟计算真实梯度得用上所有的样本 而非这个子集

156
00:10:03,100 --> 00:10:05,640
所以这也不是下山最高效的一步

157
00:10:06,080 --> 00:10:08,970
然而 每个minibatch都会给你一个不错的近似

158
00:10:08,970 --> 00:10:12,250
而且更重要的是 你的计算量会减轻不少

159
00:10:12,820 --> 00:10:16,810
你如果想把网络沿代价函数的表面下山的路径画出来的话

160
00:10:16,810 --> 00:10:22,030
它看上去会有点像醉汉漫无目的地遛下山  但起码步伐很快

161
00:10:22,030 --> 00:10:27,180
而不像是细致入微的人 踏步之前先准确地算好下坡的方向

162
00:10:27,180 --> 00:10:30,350
然后再向那个方向谨小慎微地慢慢走一步

163
00:10:31,460 --> 00:10:34,940
这个技巧就叫做“随机梯度下降”

164
00:10:36,000 --> 00:10:39,800
内容挺多的 我们先小结一下好不好

165
00:10:40,240 --> 00:10:42,270
反向传播算法算的是

166
00:10:42,270 --> 00:10:47,370
单个训练样本想怎样修改权重与偏置

167
00:10:47,370 --> 00:10:49,930
不仅是说每个参数应该变大还是变小

168
00:10:49,930 --> 00:10:55,700
还包括了这些变化的比例是多大  才能最快地降低代价

169
00:10:56,240 --> 00:10:58,270
真正的梯度下降

170
00:10:58,270 --> 00:11:01,820
得对好几万个训练范例都这么操作

171
00:11:01,820 --> 00:11:04,260
然后对这些变化值取平均

172
00:11:04,830 --> 00:11:06,340
但算起来太慢了

173
00:11:06,690 --> 00:11:10,480
所以你会先把所有的样本分到各个minibatch中去

174
00:11:10,480 --> 00:11:13,460
计算一个minibatch来作为梯度下降的一步

175
00:11:13,900 --> 00:11:17,690
计算每个minibatch的梯度 调整参数 不断循环

176
00:11:17,690 --> 00:11:21,050
最终你就会收敛到代价函数的一个局部最小值上

177
00:11:21,430 --> 00:11:25,740
此时就可以说 你的神经网络对付训练数据已经很不错了

178
00:11:27,450 --> 00:11:32,290
总而言之 我们实现反向传播算法的每一句代码

179
00:11:32,290 --> 00:11:36,970
其实或多或少地都对应了大家已经知道的内容

180
00:11:37,570 --> 00:11:40,960
但有时 了解其中的数学原理只不过是完成了一半

181
00:11:40,960 --> 00:11:44,460
如何把这破玩意儿表示出来又会搞得人一头雾水

182
00:11:44,930 --> 00:11:47,620
那么 在座的如果想深入探讨的话

183
00:11:47,620 --> 00:11:50,670
下一期视频中我们会把本期的内容用微积分的形式呈现出来

184
00:11:50,670 --> 00:11:52,750
下一期视频中我们会把本期的内容用微积分的形式呈现出来

185
00:11:52,750 --> 00:11:56,760
希望看过以后再看其他资料时会更容易接受一些吧

186
00:11:57,210 --> 00:11:59,440
收尾之前 我想着重提一点

187
00:11:59,440 --> 00:12:04,320
反向传播算法在内 所有包括神经网络在内的机器学习 要让它们工作

188
00:12:04,320 --> 00:12:06,120
咱需要一大坨的训练数据

189
00:12:06,430 --> 00:12:09,860
我们用的手写数字的范例之所以那么方便

190
00:12:09,860 --> 00:12:12,110
是因为存在着一个MNIST数据库

191
00:12:12,110 --> 00:12:15,290
里面所有的样本都已经人为标记好了

192
00:12:15,290 --> 00:12:19,000
所以机器学习领域的人 最熟悉的一个难关

193
00:12:19,000 --> 00:12:21,930
莫过于获取标记好的训练数据了

194
00:12:22,240 --> 00:12:25,080
不管是叫别人标记成千上万个图像

195
00:12:25,080 --> 00:12:27,550
还是去标记别的类型的数据也罢

