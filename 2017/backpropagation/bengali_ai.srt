1
00:00:00,000 --> 00:00:09,640
এখানে, আমরা ব্যাকপ্রোপগেশন মোকাবেলা করি, নিউরাল নেটওয়ার্কগুলি কীভাবে শেখার পিছনে মূল অ্যালগরিদম।

2
00:00:09,640 --> 00:00:13,320
আমরা কোথায় আছি তার একটি দ্রুত সংক্ষিপ্ত বিবরণের পরে, আমি প্রথমে যা করব তা

3
00:00:13,320 --> 00:00:17,400
হল অ্যালগরিদম আসলে কী করছে তার জন্য একটি স্বজ্ঞাত ওয়াকথ্রু, সূত্রের কোনো উল্লেখ ছাড়াই।

4
00:00:17,400 --> 00:00:21,400
তারপর, আপনি যারা গণিতে ডুব দিতে চান তাদের

5
00:00:21,400 --> 00:00:24,040
জন্য, পরবর্তী ভিডিওটি এই সমস্তের অন্তর্নিহিত ক্যালকুলাসে যায়।

6
00:00:24,040 --> 00:00:27,320
আপনি যদি শেষ দুটি ভিডিও দেখে থাকেন, অথবা আপনি যদি উপযুক্ত ব্যাকগ্রাউন্ড নিয়ে ঝাঁপিয়ে

7
00:00:27,320 --> 00:00:31,080
পড়েন, তাহলে আপনি জানেন একটি নিউরাল নেটওয়ার্ক কী এবং এটি কীভাবে তথ্য ফরোয়ার্ড করে।

8
00:00:31,080 --> 00:00:35,520
এখানে, আমরা হস্তলিখিত অঙ্কগুলি সনাক্ত করার ক্লাসিক উদাহরণ করছি যার পিক্সেল মানগুলি 784 নিউরন

9
00:00:35,520 --> 00:00:40,280
সহ নেটওয়ার্কের প্রথম স্তরে খাওয়ানো হয়, এবং আমি একটি নেটওয়ার্ক দেখাচ্ছি যেখানে দুটি লুকানো

10
00:00:40,280 --> 00:00:44,720
স্তর রয়েছে যার প্রতিটিতে মাত্র 16টি নিউরন রয়েছে এবং একটি আউটপুট রয়েছে 10টি নিউরনের

11
00:00:44,720 --> 00:00:49,520
স্তর, যা নির্দেশ করে যে নেটওয়ার্কটি তার উত্তর হিসাবে কোন সংখ্যাটি বেছে নিচ্ছে।

12
00:00:49,520 --> 00:00:54,480
আমি আশা করছি আপনি গ্রেডিয়েন্ট ডিসেন্ট বুঝতে পারবেন, যেমনটি শেষ ভিডিওতে বর্ণনা করা

13
00:00:54,480 --> 00:01:00,160
হয়েছে, এবং শেখার দ্বারা আমরা কী বোঝাতে চাই তা হল আমরা খুঁজে

14
00:01:00,160 --> 00:01:02,080
পেতে চাই কোন ওজন এবং পক্ষপাতগুলি একটি নির্দিষ্ট খরচ ফাংশনকে কম করে।

15
00:01:02,080 --> 00:01:07,560
একটি দ্রুত অনুস্মারক হিসাবে, একটি একক প্রশিক্ষণের উদাহরণের খরচের জন্য, আপনি নেটওয়ার্ক

16
00:01:07,560 --> 00:01:12,920
যে আউটপুট দিতে চেয়েছিলেন তার সাথে আপনি যে আউটপুটটি দিতে চেয়েছিলেন

17
00:01:12,920 --> 00:01:15,560
তা নিয়ে যান এবং প্রতিটি উপাদানের মধ্যে পার্থক্যের স্কোয়ার যোগ করুন।

18
00:01:15,560 --> 00:01:20,160
আপনার হাজার হাজার প্রশিক্ষণের উদাহরণগুলির জন্য এটি করা এবং

19
00:01:20,160 --> 00:01:23,040
ফলাফলের গড়, এটি আপনাকে নেটওয়ার্কের মোট খরচ দেয়।

20
00:01:23,040 --> 00:01:26,320
শেষ ভিডিওতে বর্ণিত হিসাবে এটি চিন্তা করার জন্য যথেষ্ট নয়, আমরা

21
00:01:26,320 --> 00:01:31,700
যে জিনিসটি খুঁজছি তা হল এই খরচ ফাংশনের নেতিবাচক গ্রেডিয়েন্ট, যা

22
00:01:31,700 --> 00:01:36,000
আপনাকে বলে যে কীভাবে আপনাকে সমস্ত ওজন এবং পক্ষপাতগুলি পরিবর্তন করতে

23
00:01:36,000 --> 00:01:43,080
হবে, সমস্ত এই সংযোগগুলি, যাতে সবচেয়ে দক্ষতার সাথে খরচ কমাতে পারে।

24
00:01:43,080 --> 00:01:48,600
Backpropagation, এই ভিডিওর বিষয়, সেই পাগল

25
00:01:48,600 --> 00:01:49,600
জটিল গ্রেডিয়েন্ট গণনার জন্য একটি অ্যালগরিদম।

26
00:01:49,600 --> 00:01:53,300
শেষ ভিডিও থেকে একটি ধারণা যা আমি সত্যিই চাই যে আপনি এখনই

27
00:01:53,300 --> 00:01:58,280
আপনার মনে দৃঢ়ভাবে ধরে রাখুন তা হল কারণ 13,000 মাত্রায় একটি

28
00:01:58,280 --> 00:02:02,660
দিক হিসাবে গ্রেডিয়েন্ট ভেক্টরকে চিন্তা করা হল, এটিকে হালকাভাবে বলা, আমাদের কল্পনার

29
00:02:02,660 --> 00:02:04,620
সুযোগের বাইরে, আরেকটি আছে যেভাবে আপনি এটি সম্পর্কে চিন্তা করতে পারেন।

30
00:02:04,620 --> 00:02:09,700
এখানে প্রতিটি উপাদানের মাত্রা আপনাকে বলছে যে খরচ

31
00:02:09,700 --> 00:02:11,820
ফাংশন প্রতিটি ওজন এবং পক্ষপাতের জন্য কতটা সংবেদনশীল।

32
00:02:11,820 --> 00:02:15,180
উদাহরণ স্বরূপ, ধরা যাক আমি যে প্রক্রিয়াটি বর্ণনা করতে যাচ্ছি আপনি তার মধ্য দিয়ে

33
00:02:15,180 --> 00:02:19,800
যান এবং নেতিবাচক গ্রেডিয়েন্ট গণনা করেন এবং এই প্রান্তে ওজনের সাথে যুক্ত উপাদানটি এখানে

34
00:02:19,800 --> 00:02:26,940
3 হবে। 2, এখানে এই প্রান্তের সাথে যুক্ত উপাদানটি 0 হিসাবে বেরিয়ে আসে। 1.

35
00:02:26,940 --> 00:02:31,520
আপনি যেভাবে ব্যাখ্যা করবেন তা হল যে ফাংশনের খরচ সেই প্রথম ওজনের

36
00:02:31,520 --> 00:02:36,100
পরিবর্তনের জন্য 32 গুণ বেশি সংবেদনশীল, তাই আপনি যদি সেই মানটিকে

37
00:02:36,100 --> 00:02:40,780
কিছুটা নড়াচড়া করেন তবে এটি খরচে কিছুটা পরিবর্তন ঘটাবে এবং সেই পরিবর্তনটি

38
00:02:40,780 --> 00:02:45,580
32 গুণ বড় যে দ্বিতীয় ওজন একই wiggle কি দিতে হবে.

39
00:02:45,580 --> 00:02:52,500
ব্যক্তিগতভাবে, যখন আমি প্রথম ব্যাকপ্রোপাগেশন সম্পর্কে শিখছিলাম, আমি মনে করি

40
00:02:52,500 --> 00:02:55,820
সবচেয়ে বিভ্রান্তিকর দিকটি ছিল কেবলমাত্র স্বরলিপি এবং সূচকের পিছনে থাকা।

41
00:02:55,820 --> 00:03:00,240
কিন্তু একবার আপনি এই অ্যালগরিদমের প্রতিটি অংশ আসলে কী করছে তা

42
00:03:00,240 --> 00:03:04,540
খুলে ফেললে, এটির প্রতিটি স্বতন্ত্র প্রভাব আসলে বেশ স্বজ্ঞাত, এটি ঠিক

43
00:03:04,540 --> 00:03:07,740
যে একে অপরের উপরে স্তরে স্তরে অনেকগুলি সামান্য সমন্বয় রয়েছে।

44
00:03:07,740 --> 00:03:11,380
তাই আমি স্বরলিপির জন্য সম্পূর্ণ উপেক্ষা করে এখানে জিনিসগুলি শুরু করতে যাচ্ছি, এবং প্রতিটি প্রশিক্ষণের

45
00:03:11,380 --> 00:03:17,380
উদাহরণের ওজন এবং পক্ষপাতের উপর যে প্রভাব রয়েছে তার মধ্য দিয়ে কেবল ধাপে ধাপে।

46
00:03:17,380 --> 00:03:21,880
যেহেতু খরচ ফাংশনে সমস্ত হাজার হাজার প্রশিক্ষণ উদাহরণের উপর উদাহরণ প্রতি একটি নির্দিষ্ট

47
00:03:21,880 --> 00:03:26,980
খরচ গড় জড়িত থাকে, তাই আমরা যেভাবে একটি গ্রেডিয়েন্ট ডিসেন্ট ধাপের জন্য

48
00:03:26,980 --> 00:03:31,740
ওজন এবং পক্ষপাতগুলি সামঞ্জস্য করি তাও প্রতিটি একক উদাহরণের উপর নির্ভর করে।

49
00:03:31,740 --> 00:03:35,300
অথবা বরং, নীতিগতভাবে এটি করা উচিত, কিন্তু গণনাগত দক্ষতার জন্য আমরা আপনাকে প্রতিটি পদক্ষেপের

50
00:03:35,300 --> 00:03:39,860
জন্য প্রতিটি একক উদাহরণ আঘাত করার প্রয়োজন থেকে বিরত রাখতে পরে একটু কৌশল করব।

51
00:03:39,860 --> 00:03:44,460
অন্যান্য ক্ষেত্রে, এই মুহুর্তে, আমরা যা করতে যাচ্ছি তা হল

52
00:03:44,460 --> 00:03:46,780
একটি একক উদাহরণে আমাদের মনোযোগ কেন্দ্রীভূত করা, এই 2-এর চিত্র।

53
00:03:46,780 --> 00:03:51,740
ওজন এবং পক্ষপাতগুলি কীভাবে সামঞ্জস্য করা যায় তার উপর এই একটি প্রশিক্ষণ উদাহরণের কী প্রভাব থাকা উচিত?

54
00:03:51,740 --> 00:03:56,040
ধরা যাক আমরা এমন এক বিন্দুতে আছি যেখানে নেটওয়ার্ক এখনও ভালভাবে প্রশিক্ষিত নয়, তাই

55
00:03:56,040 --> 00:04:01,620
আউটপুটে অ্যাক্টিভেশনগুলি বেশ র্যান্ডম দেখা যাচ্ছে, হয়তো 0 এর মতো কিছু। 5, 0। 8, 0। 2,

56
00:04:01,620 --> 00:04:02,780
অন এবং অন।

57
00:04:02,780 --> 00:04:06,700
আমরা সরাসরি এই অ্যাক্টিভেশনগুলি পরিবর্তন করতে পারি না, আমাদের শুধুমাত্র

58
00:04:06,700 --> 00:04:11,380
ওজন এবং পক্ষপাতের উপর প্রভাব আছে, কিন্তু সেই আউটপুট স্তরে

59
00:04:11,380 --> 00:04:13,340
আমরা কোন সমন্বয় করতে চাই তা ট্র্যাক রাখা সহায়ক।

60
00:04:13,340 --> 00:04:18,220
এবং যেহেতু আমরা ইমেজটিকে 2 হিসাবে শ্রেণীবদ্ধ করতে চাই, তাই আমরা চাই

61
00:04:18,220 --> 00:04:21,700
যে তৃতীয় মানটি নাজ করা হোক যখন অন্য সবগুলি নিচে নামানো হোক।

62
00:04:21,700 --> 00:04:27,620
তদুপরি, প্রতিটি বর্তমান মান তার লক্ষ্য মান থেকে

63
00:04:27,620 --> 00:04:30,220
কতটা দূরে এই নজগুলির আকার সমানুপাতিক হওয়া উচিত।

64
00:04:30,220 --> 00:04:35,260
উদাহরণস্বরূপ, সংখ্যা 2 নিউরনের সক্রিয়তা বৃদ্ধি একটি অর্থে

65
00:04:35,260 --> 00:04:39,620
8 নম্বর নিউরনের হ্রাসের চেয়ে বেশি গুরুত্বপূর্ণ, যা

66
00:04:39,620 --> 00:04:42,060
ইতিমধ্যেই যেখানে এটি হওয়া উচিত তার কাছাকাছি।

67
00:04:42,060 --> 00:04:46,260
তাই আরও জুম করে, আসুন শুধুমাত্র এই একটি নিউরনের

68
00:04:46,260 --> 00:04:47,900
উপর ফোকাস করি, যার সক্রিয়তা আমরা বাড়াতে চাই।

69
00:04:47,900 --> 00:04:53,680
মনে রাখবেন, সেই অ্যাক্টিভেশনকে পূর্ববর্তী স্তরের সমস্ত অ্যাক্টিভেশনের একটি নির্দিষ্ট ওজনযুক্ত

70
00:04:53,680 --> 00:04:58,380
সমষ্টি হিসাবে সংজ্ঞায়িত করা হয়, এবং একটি পক্ষপাত, যা সব পরে

71
00:04:58,380 --> 00:05:01,900
সিগময়েড স্কুইশিফিকেশন ফাংশন, বা একটি ReLU-এর মতো কিছুতে প্লাগ করা হয়।

72
00:05:01,900 --> 00:05:07,060
তাই এই অ্যাক্টিভেশন বাড়ানোর জন্য তিনটি ভিন্ন

73
00:05:07,060 --> 00:05:08,060
উপায় রয়েছে যা একসাথে দলবদ্ধ হতে পারে।

74
00:05:08,060 --> 00:05:12,800
আপনি পক্ষপাত বাড়াতে পারেন, ওজন বাড়াতে পারেন এবং

75
00:05:12,800 --> 00:05:15,300
আগের স্তর থেকে অ্যাক্টিভেশন পরিবর্তন করতে পারেন।

76
00:05:15,300 --> 00:05:19,720
কীভাবে ওজনগুলি সামঞ্জস্য করা উচিত তার উপর ফোকাস করে,

77
00:05:19,720 --> 00:05:21,460
লক্ষ্য করুন কীভাবে ওজনের প্রভাবের বিভিন্ন স্তর রয়েছে।

78
00:05:21,460 --> 00:05:25,100
পূর্ববর্তী স্তর থেকে উজ্জ্বল নিউরনের সাথে সংযোগগুলি সবচেয়ে বড় প্রভাব ফেলে

79
00:05:25,100 --> 00:05:31,420
কারণ সেই ওজনগুলিকে বৃহত্তর সক্রিয়করণ মান দ্বারা গুণ করা হয়।

80
00:05:31,420 --> 00:05:35,820
সুতরাং আপনি যদি সেই ওজনগুলির মধ্যে একটি বাড়াতে চান, তবে এটি প্রকৃতপক্ষে

81
00:05:35,820 --> 00:05:40,900
ম্লান নিউরনের সাথে সংযোগের ওজন বাড়ানোর চেয়ে চূড়ান্ত ব্যয় ফাংশনের উপর

82
00:05:40,900 --> 00:05:44,020
একটি শক্তিশালী প্রভাব ফেলে, অন্তত যতদূর এই একটি প্রশিক্ষণ উদাহরণটি উদ্বিগ্ন।

83
00:05:44,020 --> 00:05:48,700
মনে রাখবেন, যখন আমরা গ্রেডিয়েন্ট ডিসেন্টের কথা বলি, তখন প্রতিটি উপাদান উপরে

84
00:05:48,700 --> 00:05:53,020
বা নিচে নামানো উচিত কিনা তা নিয়েই আমরা চিন্তা করি না, আমরা

85
00:05:53,020 --> 00:05:54,020
খেয়াল করি কোনটি আপনাকে আপনার অর্থের জন্য সবচেয়ে বেশি ধাক্কা দেয়।

86
00:05:54,020 --> 00:06:00,260
যাইহোক, এটি অন্তত কিছুটা স্নায়ুবিজ্ঞানের একটি তত্ত্বের স্মরণ করিয়ে দেয়

87
00:06:00,260 --> 00:06:04,900
যে কীভাবে নিউরনের জৈবিক নেটওয়ার্কগুলি শেখে, হেবিয়ান তত্ত্ব, প্রায়শই এই

88
00:06:04,900 --> 00:06:06,940
বাক্যাংশে সংক্ষিপ্ত হয়, নিউরন যেগুলি একসাথে তারে আগুন দেয়।

89
00:06:06,940 --> 00:06:12,460
এখানে, ওজনের সবচেয়ে বড় বৃদ্ধি, সংযোগের সবচেয়ে বড়

90
00:06:12,460 --> 00:06:16,860
বৃদ্ধি, নিউরনগুলির মধ্যে ঘটে যা সবচেয়ে সক্রিয়

91
00:06:16,860 --> 00:06:18,100
এবং যেগুলিকে আমরা আরও সক্রিয় হতে চাই৷

92
00:06:18,100 --> 00:06:22,520
এক অর্থে, একটি 2 দেখার সময় যে নিউরনগুলি ফায়ার করছে সেগুলি সম্পর্কে

93
00:06:22,520 --> 00:06:25,440
চিন্তা করার সময় গুলি চালানোর সাথে আরও দৃঢ়ভাবে যুক্ত হয়ে যায়।

94
00:06:25,440 --> 00:06:29,240
স্পষ্ট করে বলতে গেলে, নিউরনের কৃত্রিম নেটওয়ার্কগুলি জৈবিক মস্তিষ্কের মতো কিছু আচরণ

95
00:06:29,240 --> 00:06:34,020
করে কিনা সে সম্পর্কে আমি এক বা অন্যভাবে বিবৃতি দেওয়ার অবস্থানে

96
00:06:34,020 --> 00:06:39,440
নই, এবং এই অগ্নিসংযোগ একত্রিতভাবে কিছু অর্থপূর্ণ তারকাচিহ্নের সাথে আসে, তবে এটি

97
00:06:39,440 --> 00:06:41,760
খুব শিথিল হিসাবে নেওয়া হয়। সাদৃশ্য, আমি এটা নোট আকর্ষণীয় খুঁজে.

98
00:06:41,760 --> 00:06:46,760
যাই হোক, তৃতীয় যেভাবে আমরা এই নিউরনের অ্যাক্টিভেশন বাড়াতে সাহায্য

99
00:06:46,760 --> 00:06:49,360
করতে পারি তা হল আগের লেয়ারের সমস্ত অ্যাক্টিভেশন পরিবর্তন করে।

100
00:06:49,360 --> 00:06:55,080
যথা, যদি ধনাত্মক ওজনের সাথে সেই সংখ্যা 2 নিউরনের সাথে সংযুক্ত সবকিছু

101
00:06:55,080 --> 00:06:59,480
উজ্জ্বল হয়ে যায় এবং যদি নেতিবাচক ওজনের সাথে সংযুক্ত সমস্ত কিছু ম্লান

102
00:06:59,480 --> 00:07:02,680
হয়ে যায়, তবে সেই সংখ্যা 2 নিউরন আরও সক্রিয় হয়ে উঠবে।

103
00:07:02,680 --> 00:07:06,200
এবং ওজন পরিবর্তনের অনুরূপ, আপনি সংশ্লিষ্ট ওজনের আকারের সমানুপাতিক পরিবর্তনগুলি

104
00:07:06,200 --> 00:07:10,840
খোঁজার মাধ্যমে আপনার অর্থের জন্য সর্বাধিক ধাক্কা পেতে চলেছেন।

105
00:07:10,840 --> 00:07:16,520
এখন অবশ্যই, আমরা সেই সক্রিয়করণগুলিকে সরাসরি প্রভাবিত করতে পারি

106
00:07:16,520 --> 00:07:18,320
না, আমাদের কেবল ওজন এবং পক্ষপাতের উপর নিয়ন্ত্রণ আছে।

107
00:07:18,320 --> 00:07:22,960
কিন্তু শেষ স্তরের মতোই, সেই পছন্দসই পরিবর্তনগুলি

108
00:07:22,960 --> 00:07:23,960
কী তা নোট করে রাখা সহায়ক।

109
00:07:23,960 --> 00:07:29,040
কিন্তু মনে রাখবেন, এখানে এক ধাপ জুম আউট করুন,

110
00:07:29,040 --> 00:07:30,040
এটি শুধুমাত্র সেই ডিজিট 2 আউটপুট নিউরন চায়।

111
00:07:30,040 --> 00:07:34,960
মনে রাখবেন, আমরাও চাই যে শেষ স্তরের অন্যান্য সমস্ত নিউরন কম

112
00:07:34,960 --> 00:07:38,460
সক্রিয় হয়ে উঠুক, এবং সেই দ্বিতীয় থেকে শেষ স্তরে কী

113
00:07:38,460 --> 00:07:43,200
ঘটবে সে সম্পর্কে অন্যান্য আউটপুট নিউরনের প্রত্যেকটির নিজস্ব চিন্তাভাবনা রয়েছে।

114
00:07:43,200 --> 00:07:49,220
তাই এই দ্বিতীয় থেকে শেষ স্তরে কী ঘটতে হবে তার

115
00:07:49,220 --> 00:07:54,800
জন্য অন্যান্য সমস্ত আউটপুট নিউরনের ইচ্ছার সাথে এই সংখ্যা 2

116
00:07:54,800 --> 00:08:00,240
নিউরনের ইচ্ছা একত্রে যোগ করা হয়েছে, আবার সংশ্লিষ্ট ওজনের অনুপাতে,

117
00:08:00,240 --> 00:08:01,740
এবং সেই অনুপাতে প্রতিটি নিউরনের প্রয়োজন কতটুকু। পরিবর্তন করতে.

118
00:08:01,740 --> 00:08:05,940
এটি এখানেই যেখানে পিছনে প্রচারের ধারণাটি আসে।

119
00:08:05,940 --> 00:08:11,080
এই সমস্ত কাঙ্খিত প্রভাবগুলিকে একত্রে যুক্ত করার মাধ্যমে, আপনি মূলত এই

120
00:08:11,080 --> 00:08:14,300
দ্বিতীয় থেকে শেষ স্তরে ঘটতে চান এমন নজগুলির একটি তালিকা পাবেন।

121
00:08:14,300 --> 00:08:18,740
এবং একবার আপনার কাছে সেগুলি হয়ে গেলে, আপনি প্রাসঙ্গিক ওজন এবং পক্ষপাতগুলিতে একই

122
00:08:18,740 --> 00:08:23,400
প্রক্রিয়াটি পুনরাবৃত্তিমূলকভাবে প্রয়োগ করতে পারেন যা সেই মানগুলি নির্ধারণ করে, একই প্রক্রিয়াটি

123
00:08:23,400 --> 00:08:29,180
পুনরাবৃত্তি করে যা আমি এইমাত্র হেঁটেছি এবং নেটওয়ার্কের মাধ্যমে পিছনের দিকে চলেছি।

124
00:08:29,180 --> 00:08:33,960
এবং আরও একটু জুম আউট করে, মনে রাখবেন যে এটিই ঠিক যেভাবে

125
00:08:33,960 --> 00:08:37,520
একটি একক প্রশিক্ষণ উদাহরণ সেই ওজন এবং পক্ষপাতগুলির প্রতিটিকে নাজ করতে চায়।

126
00:08:37,520 --> 00:08:41,400
যদি আমরা কেবলমাত্র সেই 2টি যা চায় তা শুনি, তবে নেটওয়ার্কটি

127
00:08:41,400 --> 00:08:44,140
শেষ পর্যন্ত সমস্ত চিত্রকে 2 হিসাবে শ্রেণীবদ্ধ করতে উত্সাহিত করা হবে।

128
00:08:44,140 --> 00:08:49,500
সুতরাং আপনি যা করবেন তা হল প্রতিটি অন্যান্য প্রশিক্ষণের উদাহরণের জন্য এই

129
00:08:49,500 --> 00:08:54,700
একই ব্যাকপ্রপ রুটিনের মধ্য দিয়ে যাওয়া, তাদের প্রত্যেকে কীভাবে ওজন এবং

130
00:08:54,700 --> 00:09:02,300
পক্ষপাতগুলি পরিবর্তন করতে চায় এবং সেই পছন্দসই পরিবর্তনগুলিকে একত্রে গড় করে।

131
00:09:02,300 --> 00:09:08,260
এখানে প্রতিটি ওজন এবং পক্ষপাতের গড় নাজগুলির এই সংগ্রহটি

132
00:09:08,260 --> 00:09:12,340
হল, আলগাভাবে বলতে গেলে, শেষ ভিডিওতে উল্লেখ করা খরচ

133
00:09:12,340 --> 00:09:14,360
ফাংশনের নেতিবাচক গ্রেডিয়েন্ট, বা অন্তত এটির সমানুপাতিক কিছু।

134
00:09:14,360 --> 00:09:18,980
আমি ঢিলেঢালাভাবে বলছি শুধুমাত্র এই কারণে যে আমি এখনও সেই ধাক্কাগুলি সম্পর্কে

135
00:09:18,980 --> 00:09:23,480
পরিমাণগতভাবে সুনির্দিষ্ট করতে পারিনি, কিন্তু আপনি যদি এইমাত্র উল্লেখ করা প্রতিটি পরিবর্তন

136
00:09:23,480 --> 00:09:28,740
বুঝতে পারেন, কেন কিছু আনুপাতিকভাবে অন্যদের চেয়ে বড়, এবং কীভাবে সেগুলিকে একসাথে

137
00:09:28,740 --> 00:09:34,100
যুক্ত করা দরকার, আপনি এর যান্ত্রিকতা বোঝেন backpropagation আসলে কি করছে.

138
00:09:34,100 --> 00:09:38,540
যাইহোক, অনুশীলনে, প্রতিটি গ্রেডিয়েন্ট ডিসেন্ট ধাপে প্রতিটি প্রশিক্ষণ উদাহরণের

139
00:09:38,540 --> 00:09:43,120
প্রভাব যোগ করতে কম্পিউটারগুলিকে অত্যন্ত দীর্ঘ সময় লাগে।

140
00:09:43,120 --> 00:09:45,540
তাই এখানে এর পরিবর্তে সাধারণত কি করা হয়।

141
00:09:45,540 --> 00:09:50,460
আপনি এলোমেলোভাবে আপনার প্রশিক্ষণের ডেটা এলোমেলো করেন এবং এটিকে মিনি-ব্যাচের পুরো

142
00:09:50,460 --> 00:09:53,380
গুচ্ছে ভাগ করেন, ধরা যাক প্রতিটিতে 100টি প্রশিক্ষণের উদাহরণ রয়েছে।

143
00:09:53,380 --> 00:09:56,980
তারপর আপনি মিনি-ব্যাচ অনুযায়ী একটি ধাপ গণনা করুন।

144
00:09:56,980 --> 00:10:00,840
এটি খরচ ফাংশনের প্রকৃত গ্রেডিয়েন্ট নয়, যা প্রশিক্ষণের সমস্ত ডেটার উপর

145
00:10:00,840 --> 00:10:06,260
নির্ভর করে, এই ক্ষুদ্র উপসেট নয়, তাই এটি সবচেয়ে কার্যকর

146
00:10:06,260 --> 00:10:10,900
পদক্ষেপ নয়, তবে প্রতিটি মিনি-ব্যাচ আপনাকে একটি সুন্দর অনুমান দেয়

147
00:10:10,900 --> 00:10:12,900
এবং আরও গুরুত্বপূর্ণভাবে এটি আপনাকে একটি উল্লেখযোগ্য গণনাগত গতি দেয়।

148
00:10:12,900 --> 00:10:16,900
আপনি যদি প্রাসঙ্গিক খরচের পৃষ্ঠের নীচে আপনার নেটওয়ার্কের ট্র্যাজেক্টোরি প্লট করতে চান, তবে

149
00:10:16,900 --> 00:10:22,020
এটি একটি মাতাল লোকের মতো হবে যেটি লক্ষ্যহীনভাবে একটি পাহাড়ের নিচে হোঁচট খাচ্ছে

150
00:10:22,020 --> 00:10:26,880
কিন্তু দ্রুত পদক্ষেপ নিচ্ছে, বরং একজন সাবধানে গণনা করা লোক প্রতিটি পদক্ষেপের সঠিক

151
00:10:26,880 --> 00:10:31,620
উতরাইয়ের দিকটি নির্ধারণ করছে। সেই দিকে খুব ধীর এবং সতর্ক পদক্ষেপ নেওয়ার আগে।

152
00:10:31,620 --> 00:10:35,200
এই কৌশলটিকে স্টোকাস্টিক গ্রেডিয়েন্ট ডিসেন্ট হিসাবে উল্লেখ করা হয়।

153
00:10:35,200 --> 00:10:40,400
এখানে অনেক কিছু চলছে, তাই আসুন নিজের জন্য এটিকে সংক্ষিপ্ত করা যাক, আমরা কি করব?

154
00:10:40,400 --> 00:10:45,480
ব্যাকপ্রোপগেশন হল অ্যালগরিদম যা নির্ধারণ করার জন্য একটি একক প্রশিক্ষণের উদাহরণ

155
00:10:45,480 --> 00:10:50,040
কীভাবে ওজন এবং পক্ষপাতগুলিকে ঠেলে দিতে চায়, কেবলমাত্র সেগুলি উপরে

156
00:10:50,040 --> 00:10:54,780
বা নীচে যাওয়া উচিত কিনা তা নয়, তবে সেই পরিবর্তনগুলির সাথে

157
00:10:54,780 --> 00:10:56,240
কোন আপেক্ষিক অনুপাত সবচেয়ে দ্রুত হ্রাস ঘটায় তার পরিপ্রেক্ষিতে। খরচ

158
00:10:56,240 --> 00:11:00,720
একটি সত্যিকারের গ্রেডিয়েন্ট ডিসেন্ট ধাপে আপনার সমস্ত দশ এবং হাজার হাজার প্রশিক্ষণের

159
00:11:00,720 --> 00:11:05,920
উদাহরণের জন্য এটি করা এবং আপনি যে কাঙ্খিত পরিবর্তনগুলি পান তা

160
00:11:05,920 --> 00:11:11,680
গড় করা জড়িত, তবে এটি গণনাগতভাবে ধীর, তাই এর পরিবর্তে আপনি

161
00:11:11,680 --> 00:11:14,000
এলোমেলোভাবে ডেটাকে মিনি-ব্যাচগুলিতে উপবিভক্ত করুন এবং প্রতিটি ধাপ গণনা করুন মিনি-ব্যাচ

162
00:11:14,000 --> 00:11:18,600
বারবার সমস্ত মিনি-ব্যাচের মধ্য দিয়ে যাওয়া এবং এই সমন্বয়গুলি করা,

163
00:11:18,600 --> 00:11:23,420
আপনি স্থানীয় ন্যূনতম খরচ ফাংশনের দিকে একত্রিত হবেন, যার অর্থ

164
00:11:23,420 --> 00:11:27,540
হল আপনার নেটওয়ার্ক প্রশিক্ষণের উদাহরণগুলিতে সত্যিই একটি ভাল কাজ করবে।

165
00:11:27,540 --> 00:11:32,600
তাই যে সব বলেন সঙ্গে, কোড প্রতিটি লাইন যে ব্যাকপ্রপ বাস্তবায়নে যেতে হবে

166
00:11:32,600 --> 00:11:37,680
আসলে কিছু সঙ্গে মিলে যায় আপনি এখন দেখা হয়েছে, অন্তত অনানুষ্ঠানিক পদে.

167
00:11:37,680 --> 00:11:41,900
কিন্তু কখনও কখনও গণিত কী করে তা জানা মাত্র অর্ধেক যুদ্ধ, এবং শুধুমাত্র

168
00:11:41,900 --> 00:11:44,780
জঘন্য জিনিসটির প্রতিনিধিত্ব করা যেখানে এটি সমস্ত গোলমাল এবং বিভ্রান্তিকর হয়ে যায়।

169
00:11:44,780 --> 00:11:49,360
সুতরাং, আপনারা যারা আরও গভীরে যেতে চান তাদের জন্য, পরবর্তী ভিডিওটি একই ধারণার মধ্য

170
00:11:49,360 --> 00:11:53,400
দিয়ে যায় যা এইমাত্র এখানে উপস্থাপিত হয়েছিল, কিন্তু অন্তর্নিহিত ক্যালকুলাসের পরিপ্রেক্ষিতে, যা আশা করি

171
00:11:53,400 --> 00:11:57,460
এটিকে আরও একটু পরিচিত করে তুলবে যেহেতু আপনি বিষয়টি দেখতে পাবেন অন্যান্য উৎস.

172
00:11:57,460 --> 00:12:01,220
তার আগে, একটি বিষয়ের উপর জোর দেওয়া উচিত যে এই অ্যালগরিদমটি

173
00:12:01,220 --> 00:12:05,840
কাজ করার জন্য, এবং এটি কেবলমাত্র নিউরাল নেটওয়ার্কের বাইরে সমস্ত

174
00:12:05,840 --> 00:12:06,840
ধরণের মেশিন লার্নিংয়ের জন্য যায়, আপনার প্রচুর প্রশিক্ষণ ডেটার প্রয়োজন।

175
00:12:06,840 --> 00:12:10,740
আমাদের ক্ষেত্রে, একটি জিনিস যা হস্তলিখিত অঙ্কগুলিকে এত সুন্দর উদাহরণ তৈরি করে তা হল

176
00:12:10,740 --> 00:12:15,380
MNIST ডাটাবেস রয়েছে, যেখানে অনেক উদাহরণ রয়েছে যা মানুষের দ্বারা লেবেল করা হয়েছে।

177
00:12:15,380 --> 00:12:19,000
তাই একটি সাধারণ চ্যালেঞ্জ যেটির সাথে আপনারা যারা মেশিন লার্নিংয়ে কাজ করছেন তাদের সাথে

178
00:12:19,040 --> 00:12:22,880
পরিচিত হবেন শুধুমাত্র আপনার আসলে প্রয়োজনীয় লেবেলযুক্ত প্রশিক্ষণ ডেটা পাওয়া, তাতে মানুষ হাজার

179
00:12:22,880 --> 00:12:27,400
হাজার ইমেজ লেবেল করুক বা অন্য যেকোন ডেটা টাইপের সাথে আপনি ডিল করছেন।

