1
00:00:00,000 --> 00:00:09,640
در اینجا، ما به انتشار پس‌انداز می‌پردازیم، الگوریتم اصلی در پس چگونگی یادگیری شبکه‌های عصبی.

2
00:00:09,640 --> 00:00:13,320
پس از یک جمع بندی سریع برای جایی که در آن هستیم، اولین کاری که انجام می دهم این است

3
00:00:13,320 --> 00:00:17,400
که الگوریتم واقعاً چه کاری انجام می دهد، بدون هیچ اشاره ای به فرمول ها، یک راهنما بصری انجام دهد.

4
00:00:17,400 --> 00:00:21,400
سپس، برای کسانی از شما که می‌خواهید در ریاضیات غوطه‌ور

5
00:00:21,400 --> 00:00:24,040
شوید، ویدیوی بعدی به محاسبات زیربنای همه این‌ها می‌رود.

6
00:00:24,040 --> 00:00:27,320
اگر دو ویدیوی آخر را تماشا کرده باشید، یا اگر فقط با پس‌زمینه مناسب وارد

7
00:00:27,320 --> 00:00:31,080
آن می‌شوید، می‌دانید که شبکه عصبی چیست و چگونه اطلاعات را به جلو می‌رساند.

8
00:00:31,080 --> 00:00:35,520
در اینجا، ما مثال کلاسیک تشخیص ارقام دست‌نویس را انجام می‌دهیم که مقادیر پیکسل آنها به اولین

9
00:00:35,520 --> 00:00:40,280
لایه شبکه با 784 نورون وارد می‌شود، و من شبکه‌ای را با دو لایه پنهان نشان

10
00:00:40,280 --> 00:00:44,720
می‌دهم که هر کدام فقط 16 نورون دارند و یک خروجی دارند. لایه ای از 10

11
00:00:44,720 --> 00:00:49,520
نورون، که نشان می دهد شبکه کدام رقم را به عنوان پاسخ خود انتخاب می کند.

12
00:00:49,520 --> 00:00:54,480
همچنین از شما انتظار دارم که نزول گرادیان را همانطور که در ویدیو آخر

13
00:00:54,480 --> 00:01:00,160
توضیح داده شد، درک کنید، و اینکه منظور ما از یادگیری این است که

14
00:01:00,160 --> 00:01:02,080
می‌خواهیم بفهمیم کدام وزن‌ها و سوگیری‌ها تابع هزینه خاصی را به حداقل می‌رسانند.

15
00:01:02,080 --> 00:01:07,560
به عنوان یک یادآوری سریع، برای هزینه یک مثال آموزشی

16
00:01:07,560 --> 00:01:12,920
واحد، خروجی شبکه را به همراه خروجی که می‌خواستید ارائه

17
00:01:12,920 --> 00:01:15,560
دهد، می‌گیرید و مجذور تفاوت‌های هر جزء را جمع می‌کنید.

18
00:01:15,560 --> 00:01:20,160
انجام این کار برای همه ده ها هزار مثال آموزشی و

19
00:01:20,160 --> 00:01:23,040
میانگین گیری نتایج، هزینه کل شبکه را به شما می دهد.

20
00:01:23,040 --> 00:01:26,320
همانطور که در ویدیوی آخر توضیح داده شد، گویی برای فکر کردن در مورد آن

21
00:01:26,320 --> 00:01:31,700
کافی نیست، چیزی که ما به دنبال آن هستیم، گرادیان منفی این تابع هزینه

22
00:01:31,700 --> 00:01:36,000
است که به شما می گوید چگونه باید همه وزن ها و سوگیری ها را

23
00:01:36,000 --> 00:01:43,080
تغییر دهید. این اتصالات، به طوری که به بهترین نحو هزینه را کاهش دهد.

24
00:01:43,080 --> 00:01:48,600
پس انتشار، موضوع این ویدیو، الگوریتمی برای

25
00:01:48,600 --> 00:01:49,600
محاسبه آن گرادیان پیچیده دیوانه کننده است.

26
00:01:49,600 --> 00:01:53,300
یک ایده از آخرین ویدیو که من واقعاً از شما می خواهم در حال

27
00:01:53,300 --> 00:01:58,280
حاضر محکم در ذهن خود نگه دارید این است که از آنجایی که

28
00:01:58,280 --> 00:02:02,660
تصور بردار گرادیان به عنوان یک جهت در 13000 بعد، به بیان ساده، فراتر

29
00:02:02,660 --> 00:02:04,620
از محدوده تصورات ما است. راهی که بتوانید در مورد آن فکر کنید

30
00:02:04,620 --> 00:02:09,700
بزرگی هر جزء در اینجا به شما می گوید که تابع

31
00:02:09,700 --> 00:02:11,820
هزینه چقدر نسبت به هر وزن و سوگیری حساس است.

32
00:02:11,820 --> 00:02:15,180
به عنوان مثال، فرض کنید شما فرآیندی را که می‌خواهم توضیح دهم انجام دهید و

33
00:02:15,180 --> 00:02:19,800
گرادیان منفی را محاسبه کنید، و مؤلفه مرتبط با وزن در این لبه در اینجا

34
00:02:19,800 --> 00:02:26,940
3 می‌شود. 2، در حالی که مؤلفه مرتبط با این لبه در اینجا 0 است. 1.

35
00:02:26,940 --> 00:02:31,520
روشی که شما آن را تفسیر می کنید این است که هزینه تابع 32

36
00:02:31,520 --> 00:02:36,100
برابر بیشتر به تغییرات وزن اول حساس تر است، بنابراین اگر بخواهید کمی آن

37
00:02:36,100 --> 00:02:40,780
مقدار را تکان دهید، مقداری تغییر در هزینه ایجاد می شود و این تغییر

38
00:02:40,780 --> 00:02:45,580
تغییر می کند. 32 برابر بیشتر از همان تکان دادن وزن دوم است.

39
00:02:45,580 --> 00:02:52,500
شخصاً، زمانی که برای اولین بار در مورد پس انتشار اطلاعات یاد می‌کردم،

40
00:02:52,500 --> 00:02:55,820
فکر می‌کنم گیج‌کننده‌ترین جنبه فقط نمادگذاری و تعقیب فهرست همه آن بود.

41
00:02:55,820 --> 00:03:00,240
اما هنگامی که آنچه را که هر بخش از این الگوریتم واقعاً انجام می

42
00:03:00,240 --> 00:03:04,540
دهد را باز کنید، هر اثر فردی که دارد در واقع بسیار شهودی

43
00:03:04,540 --> 00:03:07,740
است، فقط این است که تنظیمات کوچک زیادی روی هم قرار می گیرند.

44
00:03:07,740 --> 00:03:11,380
بنابراین می‌خواهم کارها را در اینجا با بی‌توجهی کامل به نماد شروع کنم، و

45
00:03:11,380 --> 00:03:17,380
فقط از تأثیراتی که هر مثال تمرینی روی وزن‌ها و سوگیری‌ها دارد، بگذرم.

46
00:03:17,380 --> 00:03:21,880
از آنجایی که تابع هزینه شامل میانگین هزینه معین برای هر مثال

47
00:03:21,880 --> 00:03:26,980
در تمام ده‌ها هزار مثال آموزشی است، نحوه تنظیم وزن‌ها و بایاس‌ها

48
00:03:26,980 --> 00:03:31,740
برای یک مرحله نزول گرادیان منفرد نیز به هر مثال بستگی دارد.

49
00:03:31,740 --> 00:03:35,300
یا بهتر است بگوییم، در اصل باید باشد، اما برای بهره وری محاسباتی، ما بعداً ترفند کوچکی را

50
00:03:35,300 --> 00:03:39,860
انجام خواهیم داد تا از نیاز به زدن تک تک نمونه ها برای هر مرحله جلوگیری کنیم.

51
00:03:39,860 --> 00:03:44,460
در موارد دیگر، در حال حاضر، تنها کاری که می‌خواهیم انجام دهیم این است

52
00:03:44,460 --> 00:03:46,780
که توجه خود را بر روی یک مثال متمرکز کنیم، این تصویر از 2.

53
00:03:46,780 --> 00:03:51,740
این یک مثال آموزشی باید چه تأثیری بر نحوه تنظیم وزن ها و سوگیری ها داشته باشد؟

54
00:03:51,740 --> 00:03:56,040
فرض کنید در نقطه‌ای هستیم که شبکه هنوز به خوبی آموزش داده نشده است، بنابراین فعال‌سازی‌ها

55
00:03:56,040 --> 00:04:01,620
در خروجی بسیار تصادفی به نظر می‌رسند، شاید چیزی در حدود 0. 5، 0. 8، 0. 2،

56
00:04:01,620 --> 00:04:02,780
در و در.

57
00:04:02,780 --> 00:04:06,700
ما نمی‌توانیم مستقیماً آن فعال‌سازی‌ها را تغییر دهیم، ما فقط

58
00:04:06,700 --> 00:04:11,380
روی وزن‌ها و سوگیری‌ها تأثیر داریم، اما پیگیری تنظیماتی که

59
00:04:11,380 --> 00:04:13,340
می‌خواهیم در آن لایه خروجی انجام شود مفید است.

60
00:04:13,340 --> 00:04:18,220
و از آنجایی که می‌خواهیم تصویر را به‌عنوان 2 طبقه‌بندی کند، می‌خواهیم آن مقدار سوم

61
00:04:18,220 --> 00:04:21,700
به سمت بالا حرکت کند در حالی که بقیه مقادیر به پایین هدایت شوند.

62
00:04:21,700 --> 00:04:27,620
علاوه بر این، اندازه این تلنگرها باید متناسب با میزان

63
00:04:27,620 --> 00:04:30,220
فاصله هر مقدار فعلی از مقدار هدف خود باشد.

64
00:04:30,220 --> 00:04:35,260
به عنوان مثال، افزایش فعال شدن نورون شماره 2 به یک

65
00:04:35,260 --> 00:04:39,620
معنا مهمتر از کاهش به نورون شماره 8 است که در

66
00:04:39,620 --> 00:04:42,060
حال حاضر بسیار نزدیک به جایی است که باید باشد.

67
00:04:42,060 --> 00:04:46,260
بنابراین با بزرگنمایی بیشتر، بیایید فقط بر روی این یک نورون تمرکز

68
00:04:46,260 --> 00:04:47,900
کنیم، نورونی که ما می خواهیم فعال سازی آن را افزایش دهیم.

69
00:04:47,900 --> 00:04:53,680
به یاد داشته باشید که فعال‌سازی به‌عنوان مجموع وزنی معینی از تمام

70
00:04:53,680 --> 00:04:58,380
فعال‌سازی‌ها در لایه قبلی، به‌علاوه یک بایاس تعریف می‌شود، که همه

71
00:04:58,380 --> 00:05:01,900
آن‌ها به چیزی مانند تابع squishification sigmoid یا ReLU متصل می‌شوند.

72
00:05:01,900 --> 00:05:07,060
بنابراین سه راه مختلف وجود دارد که می توانند با هم

73
00:05:07,060 --> 00:05:08,060
متحد شوند تا به افزایش آن فعال سازی کمک کنند.

74
00:05:08,060 --> 00:05:12,800
شما می توانید بایاس را افزایش دهید، می توانید وزن ها را افزایش

75
00:05:12,800 --> 00:05:15,300
دهید و می توانید فعال سازی های لایه قبلی را تغییر دهید.

76
00:05:15,300 --> 00:05:19,720
با تمرکز بر نحوه تنظیم وزنه ها، توجه کنید که

77
00:05:19,720 --> 00:05:21,460
چگونه وزن ها در واقع سطوح مختلف تأثیرگذاری دارند.

78
00:05:21,460 --> 00:05:25,100
اتصالات با درخشان‌ترین نورون‌ها از لایه قبلی بیشترین تأثیر را

79
00:05:25,100 --> 00:05:31,420
دارند زیرا این وزن‌ها در مقادیر فعال‌سازی بزرگ‌تر ضرب می‌شوند.

80
00:05:31,420 --> 00:05:35,820
بنابراین، اگر بخواهید یکی از آن وزن‌ها را افزایش دهید، در واقع تأثیر

81
00:05:35,820 --> 00:05:40,900
قوی‌تری بر تابع هزینه نهایی نسبت به افزایش وزن اتصالات با نورون‌های

82
00:05:40,900 --> 00:05:44,020
کم‌نور دارد، حداقل تا آنجا که به این مثال آموزشی مربوط می‌شود.

83
00:05:44,020 --> 00:05:48,700
به یاد داشته باشید، وقتی در مورد شیب نزول صحبت می‌کنیم، فقط به این

84
00:05:48,700 --> 00:05:53,020
اهمیت نمی‌دهیم که آیا هر جزء باید به سمت بالا یا پایین حرکت کند،

85
00:05:53,020 --> 00:05:54,020
بلکه به این اهمیت می‌دهیم که کدام یک بیشترین هزینه را به شما می‌دهند.

86
00:05:54,020 --> 00:06:00,260
به هر حال، این حداقل تا حدودی یادآور نظریه ای در علوم اعصاب برای نحوه

87
00:06:00,260 --> 00:06:04,900
یادگیری شبکه های بیولوژیکی نورون ها است، نظریه هبی، که اغلب در این عبارت خلاصه

88
00:06:04,900 --> 00:06:06,940
می شود، نورون هایی که با هم شلیک می کنند به هم متصل می شوند.

89
00:06:06,940 --> 00:06:12,460
در اینجا، بیشترین افزایش وزن، بزرگترین تقویت

90
00:06:12,460 --> 00:06:16,860
اتصالات، بین نورون‌هایی که فعال‌ترین هستند و

91
00:06:16,860 --> 00:06:18,100
آن‌هایی که می‌خواهیم فعال‌تر شوند، اتفاق می‌افتد.

92
00:06:18,100 --> 00:06:22,520
به یک معنا، نورون هایی که هنگام دیدن یک 2 شلیک می کنند، هنگام فکر

93
00:06:22,520 --> 00:06:25,440
کردن به آن، ارتباط قوی تری با نورون هایی دارند که شلیک می کنند.

94
00:06:25,440 --> 00:06:29,240
برای روشن بودن، من در موقعیتی نیستم که به هر طریقی در مورد اینکه آیا شبکه‌های

95
00:06:29,240 --> 00:06:34,020
مصنوعی نورون‌ها مانند مغزهای بیولوژیکی رفتار می‌کنند یا خیر، اظهار نظر کنم یا خیر، و

96
00:06:34,020 --> 00:06:39,440
این ایده با هم به هم متصل می‌شود، اما با چند ستاره معنی‌دار همراه است،

97
00:06:39,440 --> 00:06:41,760
اما به‌عنوان یک ایده بسیار شل تلقی می‌شود. به نظر من جالب است که بدانم.

98
00:06:41,760 --> 00:06:46,760
به هر حال، سومین راهی که می‌توانیم به افزایش فعال‌سازی

99
00:06:46,760 --> 00:06:49,360
این نورون کمک کنیم، تغییر تمام فعال‌سازی‌های لایه قبلی است.

100
00:06:49,360 --> 00:06:55,080
یعنی، اگر هر چیزی که به آن نورون رقم 2 با وزن

101
00:06:55,080 --> 00:06:59,480
مثبت متصل می‌شود، روشن‌تر می‌شد، و اگر هر چیزی که با وزن

102
00:06:59,480 --> 00:07:02,680
منفی مرتبط می‌شد، تیره‌تر می‌شد، آن‌گاه آن نورون رقم 2 فعال‌تر می‌شد.

103
00:07:02,680 --> 00:07:06,200
و مشابه تغییرات وزن، با جستجوی تغییراتی که متناسب با اندازه

104
00:07:06,200 --> 00:07:10,840
وزن‌های مربوطه باشد، بیشترین سود را به دست خواهید آورد.

105
00:07:10,840 --> 00:07:16,520
البته، اکنون نمی‌توانیم مستقیماً بر آن فعال‌سازی‌ها تأثیر

106
00:07:16,520 --> 00:07:18,320
بگذاریم، فقط روی وزن‌ها و سوگیری‌ها کنترل داریم.

107
00:07:18,320 --> 00:07:22,960
اما درست مانند آخرین لایه، یادداشت برداری

108
00:07:22,960 --> 00:07:23,960
از تغییرات مورد نظر مفید است.

109
00:07:23,960 --> 00:07:29,040
اما به خاطر داشته باشید، اگر در اینجا یک مرحله بزرگنمایی کنید،

110
00:07:29,040 --> 00:07:30,040
این تنها چیزی است که آن نورون خروجی رقم 2 می خواهد.

111
00:07:30,040 --> 00:07:34,960
به یاد داشته باشید، ما همچنین می‌خواهیم همه نورون‌های دیگر در لایه آخر کمتر

112
00:07:34,960 --> 00:07:38,460
فعال شوند و هر یک از آن نورون‌های خروجی دیگر افکار خود را

113
00:07:38,460 --> 00:07:43,200
در مورد اینکه چه اتفاقی باید برای لایه دوم تا آخر بیفتد دارند.

114
00:07:43,200 --> 00:07:49,220
بنابراین میل این نورون رقم 2 با خواسته های همه نورون های

115
00:07:49,220 --> 00:07:54,800
خروجی دیگر برای اتفاقی که باید برای این لایه دوم تا آخر

116
00:07:54,800 --> 00:08:00,240
بیفتد، دوباره به نسبت وزن های مربوطه، و به نسبت میزان نیاز

117
00:08:00,240 --> 00:08:01,740
هر یک از آن نورون ها، جمع می شود. عوض شدن.

118
00:08:01,740 --> 00:08:05,940
اینجا همان جایی است که ایده انتشار به عقب مطرح می شود.

119
00:08:05,940 --> 00:08:11,080
با جمع کردن تمام این افکت‌های دلخواه، اساساً فهرستی از تلنگرها را

120
00:08:11,080 --> 00:08:14,300
دریافت می‌کنید که می‌خواهید برای این لایه دوم تا آخر اتفاق بیفتد.

121
00:08:14,300 --> 00:08:18,740
و هنگامی که آن‌ها را داشتید، می‌توانید به صورت بازگشتی همان فرآیند را برای وزن‌ها

122
00:08:18,740 --> 00:08:23,400
و بایاس‌های مربوطه اعمال کنید که آن مقادیر را تعیین می‌کنند، همان فرآیندی را

123
00:08:23,400 --> 00:08:29,180
که من قبلاً طی کردم، تکرار کنید و در شبکه به عقب حرکت کنید.

124
00:08:29,180 --> 00:08:33,960
و با کمی بزرگ‌نمایی، به یاد داشته باشید که این دقیقاً همان چیزی است که یک

125
00:08:33,960 --> 00:08:37,520
نمونه تمرینی می‌خواهد هر یک از آن وزن‌ها و سوگیری‌ها را تحت فشار قرار دهد.

126
00:08:37,520 --> 00:08:41,400
اگر ما فقط به آنچه آن 2 می خواست گوش می دادیم، در نهایت

127
00:08:41,400 --> 00:08:44,140
شبکه انگیزه می گرفت تا همه تصاویر را به عنوان 2 طبقه بندی کند.

128
00:08:44,140 --> 00:08:49,500
بنابراین کاری که شما انجام می دهید این است که برای هر نمونه تمرینی دیگر، همین روال

129
00:08:49,500 --> 00:08:54,700
پشتیبان را انجام دهید، و ثبت کنید که چگونه هر یک از آنها می خواهند وزن ها

130
00:08:54,700 --> 00:09:02,300
و سوگیری ها را تغییر دهند، و میانگین آن تغییرات مورد نظر را با هم انجام دهند.

131
00:09:02,300 --> 00:09:08,260
این مجموعه در اینجا از نوک‌های متوسط به هر وزن و سوگیری،

132
00:09:08,260 --> 00:09:12,340
به‌طور ساده، گرادیان منفی تابع هزینه است که در آخرین ویدیو به

133
00:09:12,340 --> 00:09:14,360
آن اشاره شده است، یا حداقل چیزی متناسب با آن است.

134
00:09:14,360 --> 00:09:18,980
من به راحتی می گویم فقط به این دلیل که هنوز از نظر کمی در مورد آن

135
00:09:18,980 --> 00:09:23,480
تلنگرها دقیق نشده ام، اما اگر هر تغییری را که به تازگی اشاره کردم، درک کرده

136
00:09:23,480 --> 00:09:28,740
باشید، چرا برخی از آنها به نسبت بزرگتر از بقیه هستند، و چگونه همه آنها باید

137
00:09:28,740 --> 00:09:34,100
با هم جمع شوند، مکانیک را درک می کنید. پس انتشار در واقع چه می کند.

138
00:09:34,100 --> 00:09:38,540
به هر حال، در عمل، زمان بسیار زیادی طول می کشد تا رایانه

139
00:09:38,540 --> 00:09:43,120
ها تأثیر هر نمونه آموزشی را در هر مرحله نزول گرادیان جمع کنند.

140
00:09:43,120 --> 00:09:45,540
بنابراین در اینجا چیزی است که معمولا به جای آن انجام می شود.

141
00:09:45,540 --> 00:09:50,460
شما به‌طور تصادفی داده‌های آموزشی خود را به هم می‌زنید و آن‌ها را به دسته‌ای

142
00:09:50,460 --> 00:09:53,380
کامل از مینی دسته‌ها تقسیم می‌کنید، فرض کنید هر کدام 100 نمونه آموزشی دارد.

143
00:09:53,380 --> 00:09:56,980
سپس یک مرحله را با توجه به دسته کوچک محاسبه می کنید.

144
00:09:56,980 --> 00:10:00,840
این شیب واقعی تابع هزینه نیست، که به تمام داده های آموزشی

145
00:10:00,840 --> 00:10:06,260
بستگی دارد، نه این زیر مجموعه کوچک، بنابراین کارآمدترین گام در سراشیبی

146
00:10:06,260 --> 00:10:10,900
نیست، اما هر دسته کوچک تقریب بسیار خوبی به شما می دهد،

147
00:10:10,900 --> 00:10:12,900
و مهمتر از آن سرعت محاسباتی قابل توجهی به شما می دهد.

148
00:10:12,900 --> 00:10:16,900
اگر بخواهید مسیر شبکه خود را زیر سطح هزینه مربوطه ترسیم کنید، بیشتر شبیه

149
00:10:16,900 --> 00:10:22,020
یک مرد مست است که بی‌هدف از تپه تلو تلو خوردن می‌کند اما قدم‌های

150
00:10:22,020 --> 00:10:26,880
سریعی برمی‌دارد، نه اینکه یک مرد محاسباتی دقیق جهت سراشیبی هر پله را تعیین

151
00:10:26,880 --> 00:10:31,620
کند. قبل از برداشتن یک قدم بسیار آهسته و با دقت در این مسیر.

152
00:10:31,620 --> 00:10:35,200
این تکنیک به عنوان نزول گرادیان تصادفی شناخته می شود.

153
00:10:35,200 --> 00:10:40,400
اینجا چیزهای زیادی در حال وقوع است، پس بیایید آن را برای خودمان خلاصه کنیم، درست است؟

154
00:10:40,400 --> 00:10:45,480
انتشار معکوس الگوریتمی است برای تعیین اینکه چگونه یک مثال تمرینی منفرد

155
00:10:45,480 --> 00:10:50,040
می‌خواهد وزنه‌ها و سوگیری‌ها را به حرکت درآورد، نه فقط از

156
00:10:50,040 --> 00:10:54,780
نظر این که آیا آنها باید بالا یا پایین بروند، بلکه

157
00:10:54,780 --> 00:10:56,240
از نظر نسبت نسبی آن تغییرات باعث سریع‌ترین کاهش در هزینه.

158
00:10:56,240 --> 00:11:00,720
یک مرحله شیب نزولی واقعی شامل انجام این کار برای همه ده‌ها و

159
00:11:00,720 --> 00:11:05,920
هزاران مثال آموزشی و میانگین‌گیری تغییرات دلخواه است، اما از نظر محاسباتی

160
00:11:05,920 --> 00:11:11,680
کند است، بنابراین در عوض به‌طور تصادفی داده‌ها را به دسته‌های کوچک

161
00:11:11,680 --> 00:11:14,000
تقسیم می‌کنید و هر مرحله را با توجه به یک مینی دسته

162
00:11:14,000 --> 00:11:18,600
با مرور مکرر همه دسته های کوچک و انجام این تنظیمات، به

163
00:11:18,600 --> 00:11:23,420
سمت حداقل محلی تابع هزینه همگرا می شوید، یعنی می گویند شبکه

164
00:11:23,420 --> 00:11:27,540
شما در نمونه های آموزشی واقعاً کار خوبی انجام می دهد.

165
00:11:27,540 --> 00:11:32,600
بنابراین، با همه این‌ها، هر خط کدی که به پیاده‌سازی backprop می‌رود، در

166
00:11:32,600 --> 00:11:37,680
واقع با چیزی که اکنون دیده‌اید، حداقل در شرایط غیررسمی مطابقت دارد.

167
00:11:37,680 --> 00:11:41,900
اما گاهی اوقات دانستن اینکه ریاضی چه می کند تنها نیمی از کار است، و فقط

168
00:11:41,900 --> 00:11:44,780
نشان دادن این لعنتی جایی است که همه چیز درهم و گیج کننده می شود.

169
00:11:44,780 --> 00:11:49,360
بنابراین، برای آن دسته از شما که می‌خواهید عمیق‌تر بروید، ویدیوی بعدی همان

170
00:11:49,360 --> 00:11:53,400
ایده‌هایی را که در اینجا ارائه شد، بررسی می‌کند، اما از نظر محاسبات

171
00:11:53,400 --> 00:11:57,460
اساسی، که امیدواریم همانطور که موضوع را می‌بینید کمی آشناتر شود. منابع دیگر

172
00:11:57,460 --> 00:12:01,220
قبل از آن، یک چیزی که ارزش تاکید دارد این است

173
00:12:01,220 --> 00:12:05,840
که برای کارکرد این الگوریتم، و این برای همه انواع یادگیری

174
00:12:05,840 --> 00:12:06,840
ماشینی فراتر از شبکه‌های عصبی، به داده‌های آموزشی زیادی نیاز دارید.

175
00:12:06,840 --> 00:12:10,740
در مورد ما، یکی از مواردی که ارقام دست‌نویس را به یک مثال خوب تبدیل می‌کند

176
00:12:10,740 --> 00:12:15,380
این است که پایگاه داده MNIST وجود دارد، با نمونه‌های بسیاری که توسط انسان‌ها برچسب‌گذاری شده‌اند.

177
00:12:15,380 --> 00:12:19,000
بنابراین یک چالش رایج که کسانی از شما که در یادگیری ماشین کار می کنید با آن آشنا هستید، فقط دریافت

178
00:12:19,040 --> 00:12:22,880
داده های آموزشی برچسب گذاری شده ای است که واقعاً به آن نیاز دارید، خواه این باشد که افراد ده

179
00:12:22,880 --> 00:12:27,400
ها هزار تصویر را برچسب گذاری کنند، یا هر نوع داده دیگری که ممکن است با آن سروکار داشته باشید.

