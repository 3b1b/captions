[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn. ",
  "translatedText": "در اینجا، ما به انتشار پس‌انداز می‌پردازیم، الگوریتم اصلی در پس چگونگی یادگیری شبکه‌های عصبی. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. ",
  "translatedText": "پس از یک جمع بندی سریع برای جایی که در آن هستیم، اولین کاری که انجام می دهم این است که الگوریتم واقعاً چه کاری انجام می دهد، بدون هیچ اشاره ای به فرمول ها، یک راهنما بصری انجام دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. ",
  "translatedText": "سپس، برای کسانی از شما که می‌خواهید در ریاضیات غوطه‌ور شوید، ویدیوی بعدی به محاسبات زیربنای همه این‌ها می‌رود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. ",
  "translatedText": "اگر دو ویدیوی آخر را تماشا کرده باشید، یا اگر فقط با پس‌زمینه مناسب وارد آن می‌شوید، می‌دانید که شبکه عصبی چیست و چگونه اطلاعات را به جلو می‌رساند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. ",
  "translatedText": "در اینجا، ما مثال کلاسیک تشخیص ارقام دست‌نویس را انجام می‌دهیم که مقادیر پیکسل آنها به اولین لایه شبکه با 784 نورون وارد می‌شود، و من شبکه‌ای را با دو لایه پنهان نشان می‌دهم که هر کدام فقط 16 نورون دارند و یک خروجی دارند. لایه ای از 10 نورون، که نشان می دهد شبکه کدام رقم را به عنوان پاسخ خود انتخاب می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. ",
  "translatedText": "همچنین از شما انتظار دارم که نزول گرادیان را همانطور که در ویدیو آخر توضیح داده شد، درک کنید، و اینکه منظور ما از یادگیری این است که می‌خواهیم بفهمیم کدام وزن‌ها و سوگیری‌ها تابع هزینه خاصی را به حداقل می‌رسانند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. ",
  "translatedText": "به عنوان یک یادآوری سریع، برای هزینه یک مثال آموزشی واحد، خروجی شبکه را به همراه خروجی که می‌خواستید ارائه دهد، می‌گیرید و مجذور تفاوت‌های هر جزء را جمع می‌کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. ",
  "translatedText": "انجام این کار برای همه ده ها هزار مثال آموزشی و میانگین گیری نتایج، هزینه کل شبکه را به شما می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. ",
  "translatedText": "همانطور که در ویدیوی آخر توضیح داده شد، گویی برای فکر کردن در مورد آن کافی نیست، چیزی که ما به دنبال آن هستیم، گرادیان منفی این تابع هزینه است که به شما می گوید چگونه باید همه وزن ها و سوگیری ها را تغییر دهید. این اتصالات، به طوری که به بهترین نحو هزینه را کاهش دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. ",
  "translatedText": "پس انتشار، موضوع این ویدیو، الگوریتمی برای محاسبه آن گرادیان پیچیده دیوانه کننده است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "The one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. ",
  "translatedText": "یک ایده از آخرین ویدیو که من واقعاً از شما می خواهم در حال حاضر محکم در ذهن خود نگه دارید این است که از آنجایی که تصور بردار گرادیان به عنوان یک جهت در 13000 بعد، به بیان ساده، فراتر از محدوده تصورات ما است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. ",
  "translatedText": "راهی که بتوانید در مورد آن فکر کنید بزرگی هر جزء در اینجا به شما می گوید که تابع هزینه چقدر نسبت به هر وزن و سوگیری حساس است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. ",
  "translatedText": "به عنوان مثال، فرض کنید شما فرآیندی را که می‌خواهم توضیح دهم انجام دهید و گرادیان منفی را محاسبه کنید، و مؤلفه مرتبط با وزن در این لبه در اینجا 3 می‌شود. 2، در حالی که مؤلفه مرتبط با این لبه در اینجا 0 است. 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. ",
  "translatedText": "روشی که شما آن را تفسیر می کنید این است که هزینه تابع 32 برابر بیشتر به تغییرات وزن اول حساس تر است، بنابراین اگر بخواهید کمی آن مقدار را تکان دهید، مقداری تغییر در هزینه ایجاد می شود و این تغییر تغییر می کند. 32 برابر بیشتر از همان تکان دادن وزن دوم است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and index chasing of it all. ",
  "translatedText": "شخصاً، زمانی که برای اولین بار در مورد پس انتشار اطلاعات یاد می‌کردم، فکر می‌کنم گیج‌کننده‌ترین جنبه فقط نمادگذاری و تعقیب فهرست همه آن بود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. ",
  "translatedText": "اما هنگامی که آنچه را که هر بخش از این الگوریتم واقعاً انجام می دهد را باز کنید، هر اثر فردی که دارد در واقع بسیار شهودی است، فقط این است که تنظیمات کوچک زیادی روی هم قرار می گیرند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. ",
  "translatedText": "بنابراین می‌خواهم کارها را در اینجا با بی‌توجهی کامل به نماد شروع کنم، و فقط از تأثیراتی که هر مثال تمرینی روی وزن‌ها و سوگیری‌ها دارد، بگذرم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. ",
  "translatedText": "از آنجایی که تابع هزینه شامل میانگین هزینه معین برای هر مثال در تمام ده‌ها هزار مثال آموزشی است، نحوه تنظیم وزن‌ها و بایاس‌ها برای یک مرحله نزول گرادیان منفرد نیز به هر مثال بستگی دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. ",
  "translatedText": "یا بهتر است بگوییم، در اصل باید باشد، اما برای بهره وری محاسباتی، ما بعداً ترفند کوچکی را انجام خواهیم داد تا از نیاز به زدن تک تک نمونه ها برای هر مرحله جلوگیری کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. ",
  "translatedText": "در موارد دیگر، در حال حاضر، تنها کاری که می‌خواهیم انجام دهیم این است که توجه خود را بر روی یک مثال متمرکز کنیم، این تصویر از 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. ",
  "translatedText": "این یک مثال آموزشی باید چه تأثیری بر نحوه تنظیم وزن ها و سوگیری ها داشته باشد؟ فرض کنید در نقطه‌ای هستیم که شبکه هنوز به خوبی آموزش داده نشده است، بنابراین فعال‌سازی‌ها در خروجی بسیار تصادفی به نظر می‌رسند، شاید چیزی در حدود 0.5، 0.8، 0.2، در و در. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases, but it's helpful to keep track of which adjustments we wish should take place to that output layer. ",
  "translatedText": "ما نمی‌توانیم مستقیماً آن فعال‌سازی‌ها را تغییر دهیم، ما فقط روی وزن‌ها و سوگیری‌ها تأثیر داریم، اما پیگیری تنظیماتی که می‌خواهیم در آن لایه خروجی انجام شود مفید است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 242.52,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. ",
  "translatedText": "و از آنجایی که می‌خواهیم تصویر را به‌عنوان 2 طبقه‌بندی کند، می‌خواهیم آن مقدار سوم به سمت بالا حرکت کند در حالی که بقیه مقادیر به پایین هدایت شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. ",
  "translatedText": "علاوه بر این، اندازه این تلنگرها باید متناسب با میزان فاصله هر مقدار فعلی از مقدار هدف خود باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. ",
  "translatedText": "به عنوان مثال، افزایش فعال شدن نورون شماره 2 به یک معنا مهمتر از کاهش به نورون شماره 8 است که در حال حاضر بسیار نزدیک به جایی است که باید باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. ",
  "translatedText": "بنابراین با بزرگنمایی بیشتر، بیایید فقط بر روی این یک نورون تمرکز کنیم، نورونی که ما می خواهیم فعال سازی آن را افزایش دهیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. ",
  "translatedText": "به یاد داشته باشید که فعال‌سازی به‌عنوان مجموع وزنی معینی از تمام فعال‌سازی‌ها در لایه قبلی، به‌علاوه یک بایاس تعریف می‌شود، که همه آن‌ها به چیزی مانند تابع squishification sigmoid یا ReLU متصل می‌شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation. ",
  "translatedText": "بنابراین سه راه مختلف وجود دارد که می توانند با هم متحد شوند تا به افزایش آن فعال سازی کمک کنند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. ",
  "translatedText": "شما می توانید بایاس را افزایش دهید، می توانید وزن ها را افزایش دهید و می توانید فعال سازی های لایه قبلی را تغییر دهید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. ",
  "translatedText": "با تمرکز بر نحوه تنظیم وزنه ها، توجه کنید که چگونه وزن ها در واقع سطوح مختلف تأثیرگذاری دارند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. ",
  "translatedText": "اتصالات با درخشان‌ترین نورون‌ها از لایه قبلی بیشترین تأثیر را دارند زیرا این وزن‌ها در مقادیر فعال‌سازی بزرگ‌تر ضرب می‌شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. ",
  "translatedText": "بنابراین، اگر بخواهید یکی از آن وزن‌ها را افزایش دهید، در واقع تأثیر قوی‌تری بر تابع هزینه نهایی نسبت به افزایش وزن اتصالات با نورون‌های کم‌نور دارد، حداقل تا آنجا که به این مثال آموزشی مربوط می‌شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck. ",
  "translatedText": "به یاد داشته باشید، وقتی در مورد شیب نزول صحبت می‌کنیم، فقط به این اهمیت نمی‌دهیم که آیا هر جزء باید به سمت بالا یا پایین حرکت کند، بلکه به این اهمیت می‌دهیم که کدام یک بیشترین هزینه را به شما می‌دهند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together. ",
  "translatedText": "به هر حال، این حداقل تا حدودی یادآور نظریه ای در علوم اعصاب برای نحوه یادگیری شبکه های بیولوژیکی نورون ها است، نظریه هبی، که اغلب در این عبارت خلاصه می شود، نورون هایی که با هم شلیک می کنند به هم متصل می شوند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active and the ones which we wish to become more active. ",
  "translatedText": "در اینجا، بیشترین افزایش وزن، بزرگترین تقویت اتصالات، بین نورون‌هایی که فعال‌ترین هستند و آن‌هایی که می‌خواهیم فعال‌تر شوند، اتفاق می‌افتد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those firing when thinking about it. ",
  "translatedText": "به یک معنا، نورون هایی که هنگام دیدن یک 2 شلیک می کنند، هنگام فکر کردن به آن، ارتباط قوی تری با نورون هایی دارند که شلیک می کنند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note. ",
  "translatedText": "برای روشن بودن، من در موقعیتی نیستم که به هر طریقی در مورد اینکه آیا شبکه‌های مصنوعی نورون‌ها مانند مغزهای بیولوژیکی رفتار می‌کنند یا خیر، اظهار نظر کنم یا خیر، و این ایده با هم به هم متصل می‌شود، اما با چند ستاره معنی‌دار همراه است، اما به‌عنوان یک ایده بسیار شل تلقی می‌شود. به نظر من جالب است که بدانم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer. ",
  "translatedText": "به هر حال، سومین راهی که می‌توانیم به افزایش فعال‌سازی این نورون کمک کنیم، تغییر تمام فعال‌سازی‌های لایه قبلی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active. ",
  "translatedText": "یعنی، اگر هر چیزی که به آن نورون رقم 2 با وزن مثبت متصل می‌شود، روشن‌تر می‌شد، و اگر هر چیزی که با وزن منفی مرتبط می‌شد، تیره‌تر می‌شد، آن‌گاه آن نورون رقم 2 فعال‌تر می‌شد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. ",
  "translatedText": "و مشابه تغییرات وزن، با جستجوی تغییراتی که متناسب با اندازه وزن‌های مربوطه باشد، بیشترین سود را به دست خواهید آورد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases. ",
  "translatedText": "البته، اکنون نمی‌توانیم مستقیماً بر آن فعال‌سازی‌ها تأثیر بگذاریم، فقط روی وزن‌ها و سوگیری‌ها کنترل داریم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are. ",
  "translatedText": "اما درست مانند آخرین لایه، یادداشت برداری از تغییرات مورد نظر مفید است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants. ",
  "translatedText": "اما به خاطر داشته باشید، اگر در اینجا یک مرحله بزرگنمایی کنید، این تنها چیزی است که آن نورون خروجی رقم 2 می خواهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. ",
  "translatedText": "به یاد داشته باشید، ما همچنین می‌خواهیم همه نورون‌های دیگر در لایه آخر کمتر فعال شوند و هر یک از آن نورون‌های خروجی دیگر افکار خود را در مورد اینکه چه اتفاقی باید برای لایه دوم تا آخر بیفتد دارند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. ",
  "translatedText": "بنابراین میل این نورون رقم 2 با خواسته های همه نورون های خروجی دیگر برای اتفاقی که باید برای این لایه دوم تا آخر بیفتد، دوباره به نسبت وزن های مربوطه، و به نسبت میزان نیاز هر یک از آن نورون ها، جمع می شود. عوض شدن. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in. ",
  "translatedText": "اینجا همان جایی است که ایده انتشار به عقب مطرح می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. ",
  "translatedText": "با جمع کردن تمام این افکت‌های دلخواه، اساساً فهرستی از تلنگرها را دریافت می‌کنید که می‌خواهید برای این لایه دوم تا آخر اتفاق بیفتد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. ",
  "translatedText": "و هنگامی که آن‌ها را داشتید، می‌توانید به صورت بازگشتی همان فرآیند را برای وزن‌ها و بایاس‌های مربوطه اعمال کنید که آن مقادیر را تعیین می‌کنند، همان فرآیندی را که من قبلاً طی کردم، تکرار کنید و در شبکه به عقب حرکت کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. ",
  "translatedText": "و با کمی بزرگ‌نمایی، به یاد داشته باشید که این دقیقاً همان چیزی است که یک نمونه تمرینی می‌خواهد هر یک از آن وزن‌ها و سوگیری‌ها را تحت فشار قرار دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2. ",
  "translatedText": "اگر ما فقط به آنچه آن 2 می خواست گوش می دادیم، در نهایت شبکه انگیزه می گرفت تا همه تصاویر را به عنوان 2 طبقه بندی کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes. ",
  "translatedText": "بنابراین کاری که شما انجام می دهید این است که برای هر نمونه تمرینی دیگر، همین روال پشتیبان را انجام دهید، و ثبت کنید که چگونه هر یک از آنها می خواهند وزن ها و سوگیری ها را تغییر دهند، و میانگین آن تغییرات مورد نظر را با هم انجام دهند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. ",
  "translatedText": "این مجموعه در اینجا از نوک‌های متوسط به هر وزن و سوگیری، به‌طور ساده، گرادیان منفی تابع هزینه است که در آخرین ویدیو به آن اشاره شده است، یا حداقل چیزی متناسب با آن است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. ",
  "translatedText": "من به راحتی می گویم فقط به این دلیل که هنوز از نظر کمی در مورد آن تلنگرها دقیق نشده ام، اما اگر هر تغییری را که به تازگی اشاره کردم، درک کرده باشید، چرا برخی از آنها به نسبت بزرگتر از بقیه هستند، و چگونه همه آنها باید با هم جمع شوند، مکانیک را درک می کنید. پس انتشار در واقع چه می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. ",
  "translatedText": "به هر حال، در عمل، زمان بسیار زیادی طول می کشد تا رایانه ها تأثیر هر نمونه آموزشی را در هر مرحله نزول گرادیان جمع کنند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead. ",
  "translatedText": "بنابراین در اینجا چیزی است که معمولا به جای آن انجام می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and divide it into a whole bunch of mini-batches, let's say each one having 100 training examples. ",
  "translatedText": "شما به‌طور تصادفی داده‌های آموزشی خود را به هم می‌زنید و آن‌ها را به دسته‌ای کامل از مینی دسته‌ها تقسیم می‌کنید، فرض کنید هر کدام 100 نمونه آموزشی دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch. ",
  "translatedText": "سپس یک مرحله را با توجه به دسته کوچک محاسبه می کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly it gives you a significant computational speedup. ",
  "translatedText": "این شیب واقعی تابع هزینه نیست، که به تمام داده های آموزشی بستگی دارد، نه این زیر مجموعه کوچک، بنابراین کارآمدترین گام در سراشیبی نیست، اما هر دسته کوچک تقریب بسیار خوبی به شما می دهد، و مهمتر از آن سرعت محاسباتی قابل توجهی به شما می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. ",
  "translatedText": "اگر بخواهید مسیر شبکه خود را زیر سطح هزینه مربوطه ترسیم کنید، بیشتر شبیه یک مرد مست است که بی‌هدف از تپه تلو تلو خوردن می‌کند اما قدم‌های سریعی برمی‌دارد، نه اینکه یک مرد محاسباتی دقیق جهت سراشیبی هر پله را تعیین کند. قبل از برداشتن یک قدم بسیار آهسته و با دقت در این مسیر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent. ",
  "translatedText": "این تکنیک به عنوان نزول گرادیان تصادفی شناخته می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. ",
  "translatedText": "اینجا چیزهای زیادی در حال وقوع است، پس بیایید آن را برای خودمان خلاصه کنیم، درست است؟ انتشار معکوس الگوریتمی است برای تعیین اینکه چگونه یک مثال تمرینی منفرد می‌خواهد وزنه‌ها و سوگیری‌ها را به حرکت درآورد، نه فقط از نظر این که آیا آنها باید بالا یا پایین بروند، بلکه از نظر نسبت نسبی آن تغییرات باعث سریع‌ترین کاهش در هزینه. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes you get, but that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch. ",
  "translatedText": "یک مرحله شیب نزولی واقعی شامل انجام این کار برای همه ده‌ها و هزاران مثال آموزشی و میانگین‌گیری تغییرات دلخواه است، اما از نظر محاسباتی کند است، بنابراین در عوض به‌طور تصادفی داده‌ها را به دسته‌های کوچک تقسیم می‌کنید و هر مرحله را با توجه به یک مینی دسته با مرور مکرر همه دسته های کوچک و انجام این تنظیمات، به سمت حداقل محلی تابع هزینه همگرا می شوید، یعنی می گویند شبکه شما در نمونه های آموزشی واقعاً کار خوبی انجام می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.26,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms. ",
  "translatedText": "بنابراین، با همه این‌ها، هر خط کدی که به پیاده‌سازی backprop می‌رود، در واقع با چیزی که اکنون دیده‌اید، حداقل در شرایط غیررسمی مطابقت دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. ",
  "translatedText": "اما گاهی اوقات دانستن اینکه ریاضی چه می کند تنها نیمی از کار است، و فقط نشان دادن این لعنتی جایی است که همه چیز درهم و گیج کننده می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So, for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. ",
  "translatedText": "بنابراین، برای آن دسته از شما که می‌خواهید عمیق‌تر بروید، ویدیوی بعدی همان ایده‌هایی را که در اینجا ارائه شد، بررسی می‌کند، اما از نظر محاسبات اساسی، که امیدواریم همانطور که موضوع را می‌بینید کمی آشناتر شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. ",
  "translatedText": "منابع دیگر قبل از آن، یک چیزی که ارزش تاکید دارد این است که برای کارکرد این الگوریتم، و این برای همه انواع یادگیری ماشینی فراتر از شبکه‌های عصبی، به داده‌های آموزشی زیادی نیاز دارید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans. ",
  "translatedText": "در مورد ما، یکی از مواردی که ارقام دست‌نویس را به یک مثال خوب تبدیل می‌کند این است که پایگاه داده MNIST وجود دارد، با نمونه‌های بسیاری که توسط انسان‌ها برچسب‌گذاری شده‌اند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with. ",
  "translatedText": "بنابراین یک چالش رایج که کسانی از شما که در یادگیری ماشین کار می کنید با آن آشنا هستید، فقط دریافت داده های آموزشی برچسب گذاری شده ای است که واقعاً به آن نیاز دارید، خواه این باشد که افراد ده ها هزار تصویر را برچسب گذاری کنند، یا هر نوع داده دیگری که ممکن است با آن سروکار داشته باشید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]