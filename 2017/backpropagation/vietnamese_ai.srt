1
00:00:00,000 --> 00:00:09,640
Ở đây, chúng tôi giải quyết vấn đề lan truyền ngược, thuật toán cốt lõi đằng sau cách mạng lưới thần kinh học hỏi.

2
00:00:09,640 --> 00:00:13,320
Sau khi tóm tắt nhanh về vị trí của chúng ta, điều đầu tiên tôi sẽ làm là hướng dẫn trực

3
00:00:13,320 --> 00:00:17,400
quan về những gì thuật toán thực sự đang thực hiện mà không cần tham chiếu đến các công thức.

4
00:00:17,400 --> 00:00:21,400
Sau đó, dành cho những ai muốn đi sâu vào toán học, video tiếp theo

5
00:00:21,400 --> 00:00:24,040
sẽ đi sâu vào phép tính cơ bản của tất cả những điều này.

6
00:00:24,040 --> 00:00:27,320
Nếu bạn đã xem hai video cuối cùng hoặc nếu bạn chỉ bắt đầu với nền tảng thích hợp,

7
00:00:27,320 --> 00:00:31,080
thì bạn sẽ biết mạng lưới thần kinh là gì và cách nó truyền thông tin chuyển tiếp.

8
00:00:31,080 --> 00:00:35,520
Ở đây, chúng tôi đang thực hiện một ví dụ cổ điển về việc nhận dạng các chữ số

9
00:00:35,520 --> 00:00:40,280
viết tay có giá trị pixel được đưa vào lớp đầu tiên của mạng có 784 nơ-ron và

10
00:00:40,280 --> 00:00:44,720
tôi đã hiển thị một mạng có hai lớp ẩn, mỗi lớp chỉ có 16 nơ-ron và một

11
00:00:44,720 --> 00:00:49,520
đầu ra lớp gồm 10 nơ-ron, cho biết mạng đang chọn chữ số nào làm câu trả lời.

12
00:00:49,520 --> 00:00:54,480
Tôi cũng mong bạn hiểu độ dốc giảm dần, như được mô tả trong

13
00:00:54,480 --> 00:01:00,160
video trước và ý nghĩa của việc học là chúng tôi muốn tìm ra

14
00:01:00,160 --> 00:01:02,080
trọng số và độ lệch nào giảm thiểu một hàm chi phí nhất định.

15
00:01:02,080 --> 00:01:07,560
Xin nhắc lại, với chi phí cho một ví dụ đào tạo, bạn lấy đầu

16
00:01:07,560 --> 00:01:12,920
ra mà mạng cung cấp, cùng với đầu ra mà bạn muốn nó cung

17
00:01:12,920 --> 00:01:15,560
cấp, rồi cộng các bình phương của sự khác biệt giữa mỗi thành phần.

18
00:01:15,560 --> 00:01:20,160
Thực hiện việc này cho tất cả hàng chục nghìn ví dụ đào tạo của bạn và tính

19
00:01:20,160 --> 00:01:23,040
trung bình các kết quả, điều này sẽ mang lại cho bạn tổng chi phí của mạng.

20
00:01:23,040 --> 00:01:26,320
Như thể nghĩ thế vẫn chưa đủ, như được mô tả trong video trước, thứ

21
00:01:26,320 --> 00:01:31,700
mà chúng ta đang tìm kiếm là gradient âm của hàm chi phí này, nó

22
00:01:31,700 --> 00:01:36,000
cho bạn biết cách bạn cần thay đổi tất cả trọng số và độ lệch,

23
00:01:36,000 --> 00:01:43,080
tất cả những kết nối này, để giảm chi phí một cách hiệu quả nhất.

24
00:01:43,080 --> 00:01:48,600
Lan truyền ngược, chủ đề của video này, là một thuật

25
00:01:48,600 --> 00:01:49,600
toán để tính toán độ dốc cực kỳ phức tạp đó.

26
00:01:49,600 --> 00:01:53,300
Một ý tưởng từ video trước mà tôi thực sự muốn bạn ghi nhớ

27
00:01:53,300 --> 00:01:58,280
ngay bây giờ là vì coi vectơ gradient như một hướng trong 13.000 chiều,

28
00:01:58,280 --> 00:02:02,660
nói một cách nhẹ nhàng, ngoài phạm vi trí tưởng tượng của chúng ta,

29
00:02:02,660 --> 00:02:04,620
còn có một ý tưởng khác. cách bạn có thể nghĩ về nó.

30
00:02:04,620 --> 00:02:09,700
Độ lớn của từng thành phần ở đây cho bạn biết mức độ nhạy

31
00:02:09,700 --> 00:02:11,820
cảm của hàm chi phí đối với từng trọng số và độ lệch.

32
00:02:11,820 --> 00:02:15,180
Ví dụ: giả sử bạn thực hiện quy trình mà tôi sắp mô tả và tính

33
00:02:15,180 --> 00:02:19,800
gradient âm và thành phần liên quan đến trọng số trên cạnh này ở đây

34
00:02:19,800 --> 00:02:26,940
sẽ là 3. 2, trong khi thành phần được liên kết với cạnh này ở đây là 0. 1.

35
00:02:26,940 --> 00:02:31,520
Theo cách bạn giải thích thì chi phí của hàm nhạy cảm hơn 32 lần với những

36
00:02:31,520 --> 00:02:36,100
thay đổi về trọng số đầu tiên đó, vì vậy nếu bạn thay đổi giá trị

37
00:02:36,100 --> 00:02:40,780
đó một chút, điều đó sẽ gây ra một số thay đổi về chi phí và thay

38
00:02:40,780 --> 00:02:45,580
đổi đó lớn hơn 32 lần so với lực lắc của vật nặng thứ hai đó.

39
00:02:45,580 --> 00:02:52,500
Cá nhân tôi, khi lần đầu tiên tìm hiểu về lan truyền ngược, tôi nghĩ

40
00:02:52,500 --> 00:02:55,820
khía cạnh khó hiểu nhất chỉ là ký hiệu và chỉ số của nó.

41
00:02:55,820 --> 00:03:00,240
Nhưng một khi bạn khám phá ra chức năng thực sự của từng phần của

42
00:03:00,240 --> 00:03:04,540
thuật toán này, mỗi hiệu ứng riêng lẻ mà nó mang lại thực sự khá

43
00:03:04,540 --> 00:03:07,740
trực quan, chỉ là có rất nhiều điều chỉnh nhỏ được xếp chồng lên nhau.

44
00:03:07,740 --> 00:03:11,380
Vì vậy, tôi sẽ bắt đầu mọi thứ ở đây với việc hoàn toàn không quan tâm đến ký hiệu

45
00:03:11,380 --> 00:03:17,380
và chỉ xem qua tác động của mỗi ví dụ huấn luyện đối với trọng số và độ lệch.

46
00:03:17,380 --> 00:03:21,880
Bởi vì hàm chi phí liên quan đến việc tính trung bình một chi phí nhất định cho mỗi

47
00:03:21,880 --> 00:03:26,980
ví dụ trong tất cả hàng chục nghìn ví dụ huấn luyện, nên cách chúng ta điều chỉnh

48
00:03:26,980 --> 00:03:31,740
trọng số và độ lệch cho một bước giảm độ dốc cũng phụ thuộc vào từng ví dụ.

49
00:03:31,740 --> 00:03:35,300
Hay đúng hơn, về nguyên tắc là như vậy, nhưng để đạt hiệu quả tính toán, chúng tôi sẽ thực

50
00:03:35,300 --> 00:03:39,860
hiện một thủ thuật nhỏ sau để giúp bạn không cần phải xem từng ví dụ cho mỗi bước.

51
00:03:39,860 --> 00:03:44,460
Trong các trường hợp khác, ngay bây giờ, tất cả những gì chúng ta sẽ làm là

52
00:03:44,460 --> 00:03:46,780
tập trung sự chú ý vào một ví dụ duy nhất, hình ảnh số 2 này.

53
00:03:46,780 --> 00:03:51,740
Ví dụ đào tạo này sẽ có ảnh hưởng gì đến cách điều chỉnh trọng số và độ lệch?

54
00:03:51,740 --> 00:03:56,040
Giả sử chúng ta đang ở thời điểm mạng chưa được đào tạo tốt, do đó, kích hoạt

55
00:03:56,040 --> 00:04:01,620
ở đầu ra sẽ trông khá ngẫu nhiên, có thể giống như 0. 5, 0. 8, 0. 2,

56
00:04:01,620 --> 00:04:02,780
cứ thế tiếp tục.

57
00:04:02,780 --> 00:04:06,700
Chúng tôi không thể trực tiếp thay đổi những kích hoạt đó, chúng tôi chỉ có

58
00:04:06,700 --> 00:04:11,380
ảnh hưởng đến trọng số và độ lệch, nhưng sẽ rất hữu ích khi theo dõi

59
00:04:11,380 --> 00:04:13,340
những điều chỉnh nào chúng tôi muốn sẽ diễn ra đối với lớp đầu ra đó.

60
00:04:13,340 --> 00:04:18,220
Và vì chúng tôi muốn nó phân loại hình ảnh thành 2, nên chúng tôi muốn giá trị

61
00:04:18,220 --> 00:04:21,700
thứ ba đó được nâng lên trong khi tất cả các giá trị khác bị đẩy xuống.

62
00:04:21,700 --> 00:04:27,620
Hơn nữa, kích thước của những cú hích này phải tỷ lệ thuận với khoảng

63
00:04:27,620 --> 00:04:30,220
cách giữa mỗi giá trị hiện tại với giá trị mục tiêu của nó.

64
00:04:30,220 --> 00:04:35,260
Ví dụ, việc tăng cường kích hoạt tế bào thần kinh số 2

65
00:04:35,260 --> 00:04:39,620
theo một nghĩa nào đó quan trọng hơn việc giảm kích hoạt tế

66
00:04:39,620 --> 00:04:42,060
bào thần kinh số 8, vốn đã khá gần với mức cần thiết.

67
00:04:42,060 --> 00:04:46,260
Vì vậy, hãy phóng to hơn nữa, hãy tập trung vào một

68
00:04:46,260 --> 00:04:47,900
nơ-ron này, nơ-ron mà chúng ta muốn tăng cường kích hoạt.

69
00:04:47,900 --> 00:04:53,680
Hãy nhớ rằng, kích hoạt đó được xác định là tổng có trọng số nhất định

70
00:04:53,680 --> 00:04:58,380
của tất cả các kích hoạt ở lớp trước, cộng với độ lệch, sau đó

71
00:04:58,380 --> 00:05:01,900
tất cả được cắm vào một cái gì đó như hàm sigmoid squishification hoặc ReLU.

72
00:05:01,900 --> 00:05:07,060
Vì vậy, có ba cách khác nhau có thể hợp tác

73
00:05:07,060 --> 00:05:08,060
với nhau để giúp tăng cường sự kích hoạt đó.

74
00:05:08,060 --> 00:05:12,800
Bạn có thể tăng độ lệch, có thể tăng trọng số

75
00:05:12,800 --> 00:05:15,300
và có thể thay đổi kích hoạt từ lớp trước.

76
00:05:15,300 --> 00:05:19,720
Tập trung vào cách điều chỉnh trọng số, chú ý xem các trọng

77
00:05:19,720 --> 00:05:21,460
số thực sự có mức độ ảnh hưởng khác nhau như thế nào.

78
00:05:21,460 --> 00:05:25,100
Các kết nối với các nơ-ron sáng nhất từ lớp trước có tác động lớn

79
00:05:25,100 --> 00:05:31,420
nhất vì các trọng số đó được nhân với giá trị kích hoạt lớn hơn.

80
00:05:31,420 --> 00:05:35,820
Vì vậy, nếu bạn tăng một trong những trọng số đó, nó thực sự có ảnh hưởng

81
00:05:35,820 --> 00:05:40,900
mạnh hơn đến hàm chi phí cuối cùng so với việc tăng trọng số của các kết

82
00:05:40,900 --> 00:05:44,020
nối với các nơ-ron mờ hơn, ít nhất là đối với ví dụ đào tạo này.

83
00:05:44,020 --> 00:05:48,700
Hãy nhớ rằng, khi nói về việc giảm độ dốc, chúng tôi không chỉ quan tâm

84
00:05:48,700 --> 00:05:53,020
đến việc mỗi thành phần nên được nâng lên hay giảm xuống, mà chúng tôi

85
00:05:53,020 --> 00:05:54,020
còn quan tâm đến thành phần nào mang lại cho bạn nhiều lợi ích nhất.

86
00:05:54,020 --> 00:06:00,260
Nhân tiện, điều này ít nhất gợi nhớ đến một lý thuyết trong khoa học thần kinh về

87
00:06:00,260 --> 00:06:04,900
cách mạng lưới sinh học của các tế bào thần kinh học hỏi, lý thuyết Hebbian, thường được

88
00:06:04,900 --> 00:06:06,940
tóm tắt trong cụm từ, các tế bào thần kinh hoạt động cùng nhau nối với nhau.

89
00:06:06,940 --> 00:06:12,460
Ở đây, trọng lượng tăng lên nhiều nhất, sự tăng cường lớn nhất của các

90
00:06:12,460 --> 00:06:16,860
kết nối, xảy ra giữa các tế bào thần kinh hoạt động mạnh nhất

91
00:06:16,860 --> 00:06:18,100
và những tế bào mà chúng ta mong muốn trở nên năng động hơn.

92
00:06:18,100 --> 00:06:22,520
Theo một nghĩa nào đó, các tế bào thần kinh kích hoạt khi nhìn thấy số 2 sẽ có

93
00:06:22,520 --> 00:06:25,440
mối liên kết chặt chẽ hơn với những tế bào thần kinh kích hoạt khi nghĩ về nó.

94
00:06:25,440 --> 00:06:29,240
Nói rõ hơn, tôi không có đủ tư cách để đưa ra tuyên bố theo cách này hay cách

95
00:06:29,240 --> 00:06:34,020
khác về việc liệu mạng lưới nơ-ron nhân tạo có hoạt động giống như bộ não sinh học hay

96
00:06:34,020 --> 00:06:39,440
không, và ý tưởng này kết hợp với nhau đi kèm với một vài dấu hoa thị có ý

97
00:06:39,440 --> 00:06:41,760
nghĩa, nhưng được coi là rất lỏng lẻo. sự tương tự, tôi thấy thật thú vị khi lưu ý.

98
00:06:41,760 --> 00:06:46,760
Dù sao đi nữa, cách thứ ba chúng ta có thể giúp tăng cường kích hoạt

99
00:06:46,760 --> 00:06:49,360
tế bào thần kinh này là thay đổi tất cả các kích hoạt ở lớp trước.

100
00:06:49,360 --> 00:06:55,080
Cụ thể, nếu mọi thứ kết nối với nơron số 2 có trọng số dương trở

101
00:06:55,080 --> 00:06:59,480
nên sáng hơn và nếu mọi thứ kết nối với nơron số 2 có trọng số

102
00:06:59,480 --> 00:07:02,680
âm trở nên mờ hơn thì nơron số 2 đó sẽ hoạt động mạnh hơn.

103
00:07:02,680 --> 00:07:06,200
Và tương tự như những thay đổi về trọng lượng, bạn sẽ thu được lợi ích lớn nhất bằng

104
00:07:06,200 --> 00:07:10,840
cách tìm kiếm những thay đổi tỷ lệ thuận với kích thước của các trọng lượng tương ứng.

105
00:07:10,840 --> 00:07:16,520
Tất nhiên, hiện tại, chúng tôi không thể tác động trực tiếp đến những kích

106
00:07:16,520 --> 00:07:18,320
hoạt đó, chúng tôi chỉ có quyền kiểm soát trọng số và độ lệch.

107
00:07:18,320 --> 00:07:22,960
Nhưng cũng giống như lớp cuối cùng, việc ghi lại những

108
00:07:22,960 --> 00:07:23,960
thay đổi mong muốn đó là gì sẽ rất hữu ích.

109
00:07:23,960 --> 00:07:29,040
Nhưng hãy nhớ rằng, thu nhỏ một bước ở đây, đây

110
00:07:29,040 --> 00:07:30,040
chỉ là điều mà nơ-ron đầu ra chữ số 2 muốn.

111
00:07:30,040 --> 00:07:34,960
Hãy nhớ rằng, chúng ta cũng muốn tất cả các nơ-ron khác ở lớp cuối cùng

112
00:07:34,960 --> 00:07:38,460
trở nên ít hoạt động hơn và mỗi nơ-ron đầu ra khác đó có suy nghĩ

113
00:07:38,460 --> 00:07:43,200
riêng về điều gì sẽ xảy ra với lớp thứ hai đến lớp cuối cùng.

114
00:07:43,200 --> 00:07:49,220
Vì vậy, mong muốn của nơ-ron chữ số 2 này được cộng thêm cùng với mong

115
00:07:49,220 --> 00:07:54,800
muốn của tất cả các nơ-ron đầu ra khác về điều gì sẽ xảy ra

116
00:07:54,800 --> 00:08:00,240
từ lớp thứ hai đến lớp cuối cùng, một lần nữa theo tỷ lệ với trọng

117
00:08:00,240 --> 00:08:01,740
số tương ứng và tỷ lệ với mỗi nơ-ron đó cần bao nhiêu thay đổi.

118
00:08:01,740 --> 00:08:05,940
Đây chính là nơi mà ý tưởng truyền bá ngược xuất hiện.

119
00:08:05,940 --> 00:08:11,080
Bằng cách cộng tất cả các hiệu ứng mong muốn này lại với nhau, về cơ bản bạn sẽ có

120
00:08:11,080 --> 00:08:14,300
được một danh sách các cú hích mà bạn muốn thực hiện ở lớp thứ hai đến lớp cuối cùng.

121
00:08:14,300 --> 00:08:18,740
Và khi bạn đã có những thứ đó, bạn có thể áp dụng đệ quy quy trình tương

122
00:08:18,740 --> 00:08:23,400
tự cho các trọng số và độ lệch có liên quan để xác định các giá trị đó,

123
00:08:23,400 --> 00:08:29,180
lặp lại quy trình tương tự mà tôi vừa thực hiện và di chuyển ngược lại qua mạng.

124
00:08:29,180 --> 00:08:33,960
Và thu nhỏ hơn một chút, hãy nhớ rằng đây chỉ là cách một ví

125
00:08:33,960 --> 00:08:37,520
dụ đào tạo duy nhất muốn thúc đẩy từng trọng số và thành kiến đó.

126
00:08:37,520 --> 00:08:41,400
Nếu chúng ta chỉ lắng nghe những gì thứ 2 đó muốn thì cuối cùng

127
00:08:41,400 --> 00:08:44,140
mạng sẽ được khuyến khích chỉ phân loại tất cả hình ảnh thành thứ 2.

128
00:08:44,140 --> 00:08:49,500
Vì vậy, những gì bạn làm là thực hiện quy trình hỗ trợ tương tự này

129
00:08:49,500 --> 00:08:54,700
cho mọi ví dụ đào tạo khác, ghi lại cách mỗi ví dụ muốn thay đổi

130
00:08:54,700 --> 00:09:02,300
trọng số và độ lệch, đồng thời tính trung bình những thay đổi mong muốn đó.

131
00:09:02,300 --> 00:09:08,260
Bộ sưu tập ở đây gồm các mức tăng trung bình cho từng trọng số và độ

132
00:09:08,260 --> 00:09:12,340
lệch, nói một cách lỏng lẻo, là độ dốc âm của hàm chi phí được tham

133
00:09:12,340 --> 00:09:14,360
chiếu trong video trước hoặc ít nhất là thứ gì đó tỷ lệ thuận với nó.

134
00:09:14,360 --> 00:09:18,980
Tôi nói một cách lỏng lẻo chỉ vì tôi vẫn chưa hiểu chính xác về mặt định lượng về

135
00:09:18,980 --> 00:09:23,480
những cú hích đó, nhưng nếu bạn hiểu mọi thay đổi mà tôi vừa đề cập, tại sao một

136
00:09:23,480 --> 00:09:28,740
số thay đổi lại lớn hơn những thay đổi khác theo tỷ lệ và cách tất cả chúng cần

137
00:09:28,740 --> 00:09:34,100
được cộng lại với nhau, bạn sẽ hiểu cơ chế của lan truyền ngược thực sự đang làm gì.

138
00:09:34,100 --> 00:09:38,540
Nhân tiện, trên thực tế, máy tính phải mất một thời gian rất dài để cộng dồn

139
00:09:38,540 --> 00:09:43,120
mức độ ảnh hưởng của từng ví dụ huấn luyện ở mỗi bước giảm độ dốc.

140
00:09:43,120 --> 00:09:45,540
Vì vậy, đây là những gì thường được thực hiện thay thế.

141
00:09:45,540 --> 00:09:50,460
Bạn xáo trộn ngẫu nhiên dữ liệu huấn luyện của mình và chia nó

142
00:09:50,460 --> 00:09:53,380
thành nhiều đợt nhỏ, giả sử mỗi đợt có 100 mẫu huấn luyện.

143
00:09:53,380 --> 00:09:56,980
Sau đó, bạn tính toán một bước theo lô nhỏ.

144
00:09:56,980 --> 00:10:00,840
Đó không phải là độ dốc thực tế của hàm chi phí, phụ thuộc vào tất cả dữ

145
00:10:00,840 --> 00:10:06,260
liệu huấn luyện, không phải tập hợp con nhỏ này, vì vậy đây không phải là bước xuống

146
00:10:06,260 --> 00:10:10,900
dốc hiệu quả nhất, nhưng mỗi lô nhỏ sẽ cung cấp cho bạn một xấp xỉ khá tốt

147
00:10:10,900 --> 00:10:12,900
và quan trọng hơn là nó cung cấp cho bạn một tốc độ tính toán đáng kể.

148
00:10:12,900 --> 00:10:16,900
Nếu bạn lập biểu đồ quỹ đạo của mạng theo bề mặt chi phí liên quan, thì nó sẽ giống

149
00:10:16,900 --> 00:10:22,020
một người đàn ông say rượu vấp ngã không mục đích xuống một ngọn đồi nhưng thực hiện những

150
00:10:22,020 --> 00:10:26,880
bước đi nhanh chóng, hơn là một người đàn ông tính toán cẩn thận xác định hướng xuống dốc chính

151
00:10:26,880 --> 00:10:31,620
xác của mỗi bước trước khi thực hiện bước đi thật chậm rãi và cẩn thận theo hướng đó.

152
00:10:31,620 --> 00:10:35,200
Kỹ thuật này được gọi là giảm độ dốc ngẫu nhiên.

153
00:10:35,200 --> 00:10:40,400
Có rất nhiều điều đang diễn ra ở đây, vì vậy chúng ta hãy tự tổng hợp lại nhé?

154
00:10:40,400 --> 00:10:45,480
Lan truyền ngược là thuật toán để xác định cách một ví dụ huấn

155
00:10:45,480 --> 00:10:50,040
luyện muốn điều chỉnh trọng số và độ lệch, không chỉ về việc

156
00:10:50,040 --> 00:10:54,780
chúng nên tăng hay giảm mà còn về tỷ lệ tương đối với những

157
00:10:54,780 --> 00:10:56,240
thay đổi đó gây ra sự giảm nhanh nhất đối với trị giá.

158
00:10:56,240 --> 00:11:00,720
Bước giảm độ dốc thực sự sẽ liên quan đến việc thực hiện việc này cho tất cả hàng

159
00:11:00,720 --> 00:11:05,920
chục nghìn ví dụ đào tạo của bạn và tính trung bình các thay đổi mong muốn mà

160
00:11:05,920 --> 00:11:11,680
bạn nhận được, nhưng việc đó chậm về mặt tính toán, vì vậy thay vào đó, bạn chia ngẫu

161
00:11:11,680 --> 00:11:14,000
nhiên dữ liệu thành các lô nhỏ và tính toán từng bước tương ứng với một lô nhỏ.

162
00:11:14,000 --> 00:11:18,600
Liên tục thực hiện tất cả các đợt nhỏ và thực hiện những điều chỉnh

163
00:11:18,600 --> 00:11:23,420
này, bạn sẽ hội tụ về mức tối thiểu cục bộ của hàm chi phí,

164
00:11:23,420 --> 00:11:27,540
nghĩa là mạng của bạn sẽ thực hiện rất tốt các ví dụ đào tạo.

165
00:11:27,540 --> 00:11:32,600
Vì vậy, với tất cả những gì đã nói, mọi dòng mã dùng để triển khai backprop thực

166
00:11:32,600 --> 00:11:37,680
sự tương ứng với những gì bạn đã thấy, ít nhất là theo thuật ngữ không chính thức.

167
00:11:37,680 --> 00:11:41,900
Nhưng đôi khi biết những gì toán học làm mới chỉ là một nửa trận chiến, và chỉ việc

168
00:11:41,900 --> 00:11:44,780
trình bày cái thứ chết tiệt đó là mọi thứ sẽ trở nên lộn xộn và khó hiểu.

169
00:11:44,780 --> 00:11:49,360
Vì vậy, đối với những ai muốn tìm hiểu sâu hơn, video tiếp theo sẽ trình bày những ý

170
00:11:49,360 --> 00:11:53,400
tưởng tương tự vừa được trình bày ở đây, nhưng về mặt phép tính cơ bản, hy vọng sẽ

171
00:11:53,400 --> 00:11:57,460
làm cho nó quen thuộc hơn một chút khi bạn xem chủ đề trong các nguồn lực khác.

172
00:11:57,460 --> 00:12:01,220
Trước đó, một điều đáng nhấn mạnh là để thuật toán này hoạt

173
00:12:01,220 --> 00:12:05,840
động và điều này áp dụng cho tất cả các loại máy học

174
00:12:05,840 --> 00:12:06,840
ngoài mạng lưới thần kinh, bạn cần rất nhiều dữ liệu đào tạo.

175
00:12:06,840 --> 00:12:10,740
Trong trường hợp của chúng tôi, một điều khiến các chữ số viết tay trở thành một ví dụ hay

176
00:12:10,740 --> 00:12:15,380
là tồn tại cơ sở dữ liệu MNIST, với rất nhiều ví dụ đã được con người gắn nhãn.

177
00:12:15,380 --> 00:12:19,000
Vì vậy, một thách thức chung mà những người làm việc trong lĩnh vực học máy sẽ quen thuộc là lấy dữ

178
00:12:19,040 --> 00:12:22,880
liệu huấn luyện được gắn nhãn mà bạn thực sự cần, cho dù đó là yêu cầu mọi người gắn nhãn

179
00:12:22,880 --> 00:12:27,400
cho hàng chục nghìn hình ảnh hay bất kỳ loại dữ liệu nào khác mà bạn có thể đang xử lý.

