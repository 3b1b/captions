[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn.",
  "translatedText": "Dito, tinatalakay namin ang backpropagation, ang pangunahing algorithm sa likod kung paano natututo ang mga neural network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas.",
  "translatedText": "Pagkatapos ng isang mabilis na recap para sa kung nasaan tayo, ang unang bagay na gagawin ko ay isang intuitive walkthrough para sa kung ano talaga ang ginagawa ng algorithm, nang walang anumang reference sa mga formula.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this.",
  "translatedText": "Pagkatapos, para sa inyo na gustong sumabak sa matematika, ang susunod na video ay pupunta sa calculus na pinagbabatayan ng lahat ng ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information.",
  "translatedText": "Kung napanood mo ang huling dalawang video, o kung tumatalon ka lang gamit ang naaangkop na background, alam mo kung ano ang neural network, at kung paano ito nagpapasa ng impormasyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer.",
  "translatedText": "Dito, ginagawa namin ang klasikong halimbawa ng pagkilala sa mga sulat-kamay na digit na ang mga halaga ng pixel ay ipinapasok sa unang layer ng network na may 784 neuron, at nagpakita ako ng network na may dalawang nakatagong layer na may 16 neuron lang bawat isa, at isang output. layer ng 10 neuron, na nagpapahiwatig kung aling digit ang pipiliin ng network bilang sagot nito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function.",
  "translatedText": "Inaasahan ko rin na mauunawaan mo ang gradient descent, gaya ng inilarawan sa huling video, at kung paano ang ibig naming sabihin sa pag-aaral ay gusto naming malaman kung aling mga timbang at bias ang nagpapaliit sa isang partikular na function ng gastos.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component.",
  "translatedText": "Bilang isang mabilis na paalala, para sa halaga ng isang halimbawa ng pagsasanay, kukunin mo ang output na ibinibigay ng network, kasama ang output na gusto mong ibigay nito, at idagdag ang mga parisukat ng mga pagkakaiba sa pagitan ng bawat bahagi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network.",
  "translatedText": "Ang paggawa nito para sa lahat ng iyong sampu-sampung libong mga halimbawa ng pagsasanay at pag-average ng mga resulta, binibigyan ka nito ng kabuuang halaga ng network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "And as if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost.",
  "translatedText": "At parang hindi iyon sapat na pag-isipan, gaya ng inilarawan sa huling video, ang hinahanap namin ay ang negatibong gradient ng cost function na ito, na nagsasabi sa iyo kung paano mo kailangang baguhin ang lahat ng mga timbang at bias, lahat ng mga koneksyong ito, upang mas mahusay na bawasan ang gastos.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient.",
  "translatedText": "Ang backpropagation, ang paksa ng video na ito, ay isang algorithm para sa pag-compute ng nakatutuwang kumplikadong gradient.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "And the one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it.",
  "translatedText": "At ang isang ideya mula sa huling video na talagang gusto kong hawakan mo nang mahigpit sa iyong isipan ngayon ay dahil ang pag-iisip sa gradient vector bilang isang direksyon sa 13,000 dimensyon ay, sa madaling sabi, lampas sa saklaw ng ating mga imahinasyon, mayroong ibang paraan na maiisip mo ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias.",
  "translatedText": "Ang magnitude ng bawat bahagi dito ay nagsasabi sa iyo kung gaano kasensitibo ang paggana ng gastos sa bawat timbang at bias.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and you compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1.",
  "translatedText": "Halimbawa, sabihin nating dumaan ka sa prosesong ilalarawan ko, at kino-compute mo ang negatibong gradient, at ang bahaging nauugnay sa bigat sa gilid na ito ay lalabas na 3.2, habang ang bahaging nauugnay sa gilid na ito ay darating dito. bilang 0.1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give.",
  "translatedText": "Ang paraan kung paano mo mabibigyang-kahulugan iyon ay ang halaga ng function ay 32 beses na mas sensitibo sa mga pagbabago sa unang timbang na iyon, kaya kung ililipat mo ang halagang iyon nang kaunti, ito ay magdudulot ng ilang pagbabago sa gastos, at iyon Ang pagbabago ay 32 beses na mas malaki kaysa sa kung ano ang ibibigay ng parehong pag-alog sa pangalawang timbang na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all.",
  "translatedText": "Sa personal, noong una kong natutunan ang tungkol sa backpropagation, sa palagay ko ang pinakanakalilito na aspeto ay ang notasyon at ang paghabol sa index ng lahat ng ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other.",
  "translatedText": "Ngunit sa sandaling ma-unwrap mo kung ano talaga ang ginagawa ng bawat bahagi ng algorithm na ito, ang bawat indibidwal na epekto na nararanasan nito ay talagang medyo intuitive, ito lang ay mayroong maraming maliliit na pagsasaayos na nagkakapatong-patong sa bawat isa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases.",
  "translatedText": "Kaya't sisimulan ko ang mga bagay dito na may ganap na pagwawalang-bahala sa notasyon, at hakbang lang sa mga epekto ng bawat halimbawa ng pagsasanay sa mga timbang at bias.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example.",
  "translatedText": "Dahil ang function ng gastos ay nagsasangkot ng pag-average ng isang partikular na gastos sa bawat halimbawa sa lahat ng sampu-sampung libong mga halimbawa ng pagsasanay, ang paraan ng pagsasaayos namin ng mga timbang at bias para sa isang solong gradient descent step ay nakasalalay din sa bawat isang halimbawa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step.",
  "translatedText": "O sa halip, sa prinsipyo ito ay dapat, ngunit para sa computational na kahusayan ay gagawa kami ng isang maliit na trick sa ibang pagkakataon upang pigilan ka sa pangangailangang matumbok ang bawat solong halimbawa para sa bawat hakbang.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2.",
  "translatedText": "Sa ibang mga kaso, sa ngayon, ang gagawin lang natin ay ituon ang ating atensyon sa isang halimbawa, ang larawang ito ng isang 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted?",
  "translatedText": "Ano ang dapat na epekto ng isang halimbawa ng pagsasanay na ito sa kung paano nababagay ang mga timbang at bias?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on.",
  "translatedText": "Sabihin nating nasa punto na tayo kung saan ang network ay hindi pa sanay na mabuti, kaya ang mga pag-activate sa output ay magmumukhang random, marahil ay parang 0.5, 0.8, 0.2, on and on.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases.",
  "translatedText": "Hindi namin direktang mababago ang mga pag-activate na iyon, mayroon lamang kaming impluwensya sa mga timbang at bias.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 242.52,
  "end": 247.16
 },
 {
  "input": "But it's helpful to keep track of which adjustments we wish should take place to that output layer.",
  "translatedText": "Ngunit nakakatulong na subaybayan kung aling mga pagsasaayos ang gusto naming maganap sa output layer na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 247.16,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down.",
  "translatedText": "At dahil gusto naming i-classify nito ang imahe bilang 2, gusto namin na ang pangatlong halaga ay tumaas habang ang lahat ng iba ay bumababa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value.",
  "translatedText": "Bukod dito, ang mga sukat ng mga nudge na ito ay dapat na proporsyonal sa kung gaano kalayo ang bawat kasalukuyang halaga mula sa target na halaga nito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be.",
  "translatedText": "Halimbawa, ang pagtaas sa activation ng number 2 neuron ay mas mahalaga kaysa sa pagbaba sa number 8 neuron, na medyo malapit na sa kung saan ito dapat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase.",
  "translatedText": "Kaya mag-zoom in pa, tumutok lang tayo sa isang neuron na ito, ang isa na ang pag-activate ay nais nating madagdagan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU.",
  "translatedText": "Tandaan, ang pag-activate na iyon ay tinukoy bilang isang tiyak na may timbang na kabuuan ng lahat ng mga pag-activate sa nakaraang layer, kasama ang isang bias, na lahat pagkatapos ay nakasaksak sa isang bagay tulad ng sigmoid squishification function, o isang ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation.",
  "translatedText": "Kaya mayroong tatlong magkakaibang mga paraan na maaaring magsama-sama upang makatulong na mapataas ang pag-activate na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer.",
  "translatedText": "Maaari mong dagdagan ang bias, maaari mong dagdagan ang mga timbang, at maaari mong baguhin ang mga pag-activate mula sa nakaraang layer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence.",
  "translatedText": "Sa pagtutuon sa kung paano dapat iakma ang mga timbang, pansinin kung paano aktwal na may magkakaibang antas ng impluwensya ang mga timbang.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values.",
  "translatedText": "Ang mga koneksyon sa pinakamaliwanag na mga neuron mula sa naunang layer ay may pinakamalaking epekto dahil ang mga timbang na iyon ay pinarami ng mas malalaking halaga ng activation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned.",
  "translatedText": "Kaya kung tataasan mo ang isa sa mga timbang na iyon, ito ay talagang may mas malakas na impluwensya sa pinakahuling paggana ng gastos kaysa sa pagtaas ng mga timbang ng mga koneksyon sa mga dimmer na neuron, kahit na hanggang sa isang halimbawa ng pagsasanay na ito ay nababahala.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck.",
  "translatedText": "Tandaan, kapag pinag-uusapan natin ang tungkol sa gradient descent, hindi lang natin inaalala kung dapat ba pataasin o pababa ang bawat bahagi, pinapahalagahan namin kung alin ang magbibigay sa iyo ng pinakamalaking halaga para sa iyong pera.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together.",
  "translatedText": "Ito, sa pamamagitan ng paraan, ay hindi bababa sa medyo nakapagpapaalaala ng isang teorya sa neuroscience para sa kung paano natututo ang mga biological network ng mga neuron, ang teoryang Hebbian, na madalas na summed up sa parirala, mga neuron na nagsusunog ng magkasamang wire.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active.",
  "translatedText": "Dito, ang pinakamalaking pagtaas sa mga timbang, ang pinakamalaking pagpapalakas ng mga koneksyon, ay nangyayari sa pagitan ng mga neuron na pinakaaktibo, at sa mga neuron na nais nating maging mas aktibo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those are the ones firing when thinking about a 2.",
  "translatedText": "Sa isang kahulugan, ang mga neuron na nagpapaputok habang nakikita ang isang 2 ay nagiging mas malakas na nauugnay sa mga iyon ay ang mga nagpapaputok kapag iniisip ang tungkol sa isang 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note.",
  "translatedText": "Upang maging malinaw, wala ako sa posisyon na gumawa ng mga pahayag sa isang paraan o iba pa tungkol sa kung ang mga artipisyal na network ng mga neuron ay kumikilos tulad ng mga biological na utak, at ito ay pinagsasama-sama ang ideya ng wire na may kasamang ilang makabuluhang asterisk, ngunit kinuha bilang isang napakaluwag. pagkakatulad, nakita kong kawili-wiling tandaan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer.",
  "translatedText": "Anyway, ang pangatlong paraan na makakatulong tayo sa pagtaas ng activation ng neuron na ito ay sa pamamagitan ng pagbabago ng lahat ng activation sa nakaraang layer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active.",
  "translatedText": "Ibig sabihin, kung ang lahat ng konektado sa digit 2 neuron na iyon na may positibong timbang ay naging mas maliwanag, at kung lahat ng konektado sa isang negatibong timbang ay lumabo, ang digit 2 neuron na iyon ay magiging mas aktibo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights.",
  "translatedText": "At katulad ng mga pagbabago sa timbang, mas masusulit mo ang iyong pera sa pamamagitan ng paghahanap ng mga pagbabago na proporsyonal sa laki ng kaukulang mga timbang.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases.",
  "translatedText": "Ngayon siyempre, hindi natin direktang maimpluwensyahan ang mga pag-activate na iyon, mayroon lamang tayong kontrol sa mga timbang at bias.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are.",
  "translatedText": "Ngunit tulad ng huling layer, nakakatulong na tandaan kung ano ang mga gustong pagbabagong iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants.",
  "translatedText": "Ngunit tandaan, ang pag-zoom out ng isang hakbang dito, ito lang ang gusto ng digit 2 output neuron na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer.",
  "translatedText": "Tandaan, gusto din namin na ang lahat ng iba pang mga neuron sa huling layer ay hindi gaanong aktibo, at ang bawat isa sa iba pang mga output neuron ay may sariling mga iniisip tungkol sa kung ano ang dapat mangyari sa pangalawang hanggang huling layer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So, the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change.",
  "translatedText": "Kaya, ang pagnanais ng digit na 2 neuron na ito ay idinagdag kasama ang mga pagnanasa ng lahat ng iba pang mga output neuron para sa kung ano ang dapat mangyari sa pangalawang hanggang huling layer na ito, muli sa proporsyon sa kaukulang mga timbang, at sa proporsyon sa kung magkano ang bawat isa sa mga neuron na iyon. kailangang baguhin.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in.",
  "translatedText": "Dito mismo pumapasok ang ideya ng pagpapalaganap pabalik.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer.",
  "translatedText": "Sa pamamagitan ng pagsasama-sama ng lahat ng gustong epektong ito, karaniwang nakakakuha ka ng listahan ng mga nudge na gusto mong mangyari sa pangalawang hanggang huling layer na ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network.",
  "translatedText": "At sa sandaling mayroon ka ng mga iyon, maaari mong muling ilapat ang parehong proseso sa mga nauugnay na timbang at pagkiling na tumutukoy sa mga halagang iyon, na inuulit ang parehong proseso na katatapos ko lang dumaan at lumilipat pabalik sa network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases.",
  "translatedText": "At mag-zoom out nang kaunti, tandaan na ito lang ang nais ng isang solong halimbawa ng pagsasanay na itulak ang bawat isa sa mga timbang at bias na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2.",
  "translatedText": "Kung nakinig lang tayo sa gusto ng 2 na iyon, sa huli ay mabibigyang-insentibo ang network para lang maiuri ang lahat ng larawan bilang 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes.",
  "translatedText": "Kaya't ang gagawin mo ay dumaan sa parehong backprop na gawain para sa bawat iba pang halimbawa ng pagsasanay, na nagre-record kung paano gustong baguhin ng bawat isa sa kanila ang mga timbang at bias, at pagsama-samahin ang mga gustong pagbabago.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it.",
  "translatedText": "Itong koleksyon dito ng mga na-average na nudge sa bawat timbang at bias ay, sa madaling salita, ang negatibong gradient ng cost function na na-reference sa huling video, o kahit isang bagay na proporsyonal dito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing.",
  "translatedText": "Maluwag na sinasabi ko lang dahil hindi pa ako nakakakuha ng tumpak na dami tungkol sa mga nudge na iyon, ngunit kung naunawaan mo ang bawat pagbabagong kakabanggit ko lang, kung bakit ang ilan ay proporsyonal na mas malaki kaysa sa iba, at kung paano kailangang pagsamahin ang lahat ng ito, naiintindihan mo ang mekanika para sa kung ano talaga ang ginagawa ng backpropagation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step.",
  "translatedText": "Sa pamamagitan ng paraan, sa pagsasagawa, nangangailangan ang mga computer ng napakahabang panahon upang madagdagan ang impluwensya ng bawat halimbawa ng pagsasanay sa bawat gradient descent step.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead.",
  "translatedText": "Kaya narito ang karaniwang ginagawa sa halip.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and then divide it into a whole bunch of mini-batches, let's say each one having 100 training examples.",
  "translatedText": "Random mong i-shuffle ang iyong data ng pagsasanay at pagkatapos ay hatiin ito sa isang buong grupo ng mga mini-batch, sabihin nating bawat isa ay may 100 halimbawa ng pagsasanay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch.",
  "translatedText": "Pagkatapos ay mag-compute ka ng isang hakbang ayon sa mini-batch.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not going to be the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speedup.",
  "translatedText": "Hindi ito ang magiging aktwal na gradient ng cost function, na nakadepende sa lahat ng data ng pagsasanay, hindi ang maliit na subset na ito, kaya hindi ito ang pinaka-epektibong hakbang pababa, ngunit ang bawat mini-batch ay nagbibigay sa iyo ng medyo magandang approximation, at higit sa lahat, ito ay nagbibigay sa iyo ng isang makabuluhang computational speedup.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction.",
  "translatedText": "Kung iplano mo ang trajectory ng iyong network sa ilalim ng nauugnay na ibabaw ng gastos, ito ay magiging isang maliit na katulad ng isang lasing na lalaki na natitisod nang walang patutunguhan pababa ng burol ngunit gumagawa ng mabilis na mga hakbang, sa halip na isang maingat na pagkalkula ng tao na tinutukoy ang eksaktong pababang direksyon ng bawat hakbang. bago gumawa ng napakabagal at maingat na hakbang sa direksyong iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent.",
  "translatedText": "Ang pamamaraan na ito ay tinutukoy bilang stochastic gradient descent.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we?",
  "translatedText": "Napakaraming nangyayari dito, kaya isa-isahin na lang natin ito, di ba?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost.",
  "translatedText": "Ang backpropagation ay ang algorithm para sa pagtukoy kung paano nais ng isang solong halimbawa ng pagsasanay na itulak ang mga timbang at bias, hindi lamang sa mga tuntunin kung dapat silang tumaas o pababa, ngunit sa mga tuntunin ng kung anong mga kaugnay na proporsyon sa mga pagbabagong iyon ang sanhi ng pinakamabilis na pagbaba sa gastos.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens of thousands of training examples and averaging the desired changes you get.",
  "translatedText": "Ang isang tunay na gradient descent step ay kasangkot sa paggawa nito para sa lahat ng iyong sampu-sampung libong mga halimbawa ng pagsasanay at pag-a-average ng mga ninanais na pagbabago na makukuha mo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.26,
  "end": 664.2
 },
 {
  "input": "But that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch.",
  "translatedText": "Ngunit iyon ay mabagal sa pagkalkula, kaya sa halip ay random mong i-subdivide ang data sa mga mini-batch at kino-compute ang bawat hakbang na may kinalaman sa isang mini-batch.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 664.86,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all of the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples.",
  "translatedText": "Sa paulit-ulit na pagdaan sa lahat ng mini-batch at paggawa ng mga pagsasaayos na ito, magsasama-sama ka patungo sa isang lokal na minimum ng function ng gastos, ibig sabihin, ang iyong network ay gagawa ng talagang mahusay na trabaho sa mga halimbawa ng pagsasanay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms.",
  "translatedText": "Kaya sa lahat ng sinabi, ang bawat linya ng code na pupunta sa pagpapatupad ng backprop ay talagang tumutugma sa isang bagay na nakita mo na ngayon, hindi bababa sa mga impormal na termino.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing.",
  "translatedText": "Ngunit kung minsan ang pag-alam kung ano ang ginagawa ng matematika ay kalahati lamang ng labanan, at ang kumakatawan lamang sa mapahamak na bagay ay kung saan ito ay nagiging magulo at nakakalito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources.",
  "translatedText": "Kaya para sa iyo na gustong lumalim, ang susunod na video ay dumaan sa parehong mga ideya na ipinakita dito, ngunit sa mga tuntunin ng pinagbabatayan na calculus, na sana ay gawing mas pamilyar ito habang nakikita mo ang paksa sa ibang mapagkukunan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data.",
  "translatedText": "Bago iyon, isang bagay na dapat bigyang-diin ay para gumana ang algorithm na ito, at ito ay para sa lahat ng uri ng machine learning na higit pa sa mga neural network, kailangan mo ng maraming data ng pagsasanay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans.",
  "translatedText": "Sa aming kaso, ang isang bagay na gumagawa ng mga sulat-kamay na digit na isang magandang halimbawa ay ang pagkakaroon ng database ng MNIST, na may napakaraming mga halimbawa na na-label ng mga tao.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with.",
  "translatedText": "Kaya't ang isang karaniwang hamon na magiging pamilyar sa inyo na nagtatrabaho sa machine learning ay ang pagkuha lamang ng may label na data ng pagsasanay na talagang kailangan mo, kung iyon ay ang pagkakaroon ng mga tao na mag-label ng libu-libong mga larawan, o anumang iba pang uri ng data na maaari mong harapin.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]