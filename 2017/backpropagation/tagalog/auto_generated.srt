1
00:00:04,059 --> 00:00:06,450
Dito, tinatalakay namin ang backpropagation, ang pangunahing 

2
00:00:06,450 --> 00:00:08,880
algorithm sa likod kung paano natututo ang mga neural network.

3
00:00:09,400 --> 00:00:11,576
Pagkatapos ng isang mabilis na recap para sa kung nasaan tayo, 

4
00:00:11,576 --> 00:00:14,132
ang unang bagay na gagawin ko ay isang intuitive walkthrough para sa kung 

5
00:00:14,132 --> 00:00:17,000
ano talaga ang ginagawa ng algorithm, nang walang anumang reference sa mga formula.

6
00:00:17,660 --> 00:00:19,968
Pagkatapos, para sa inyo na gustong sumabak sa matematika, 

7
00:00:19,968 --> 00:00:23,020
ang susunod na video ay pupunta sa calculus na pinagbabatayan ng lahat ng ito.

8
00:00:23,820 --> 00:00:26,200
Kung napanood mo ang huling dalawang video, o kung tumatalon 

9
00:00:26,200 --> 00:00:29,322
ka lang gamit ang naaangkop na background, alam mo kung ano ang neural network, 

10
00:00:29,322 --> 00:00:31,000
at kung paano ito nagpapasa ng impormasyon.

11
00:00:31,680 --> 00:00:35,197
Dito, ginagawa namin ang klasikong halimbawa ng pagkilala sa mga sulat-kamay 

12
00:00:35,197 --> 00:00:38,669
na digit na ang mga halaga ng pixel ay ipinapasok sa unang layer ng network 

13
00:00:38,669 --> 00:00:42,050
na may 784 neuron, at nagpakita ako ng network na may dalawang nakatagong 

14
00:00:42,050 --> 00:00:45,522
layer na may 16 neuron lang bawat isa, at isang output. layer ng 10 neuron, 

15
00:00:45,522 --> 00:00:49,040
na nagpapahiwatig kung aling digit ang pipiliin ng network bilang sagot nito.

16
00:00:50,040 --> 00:00:52,494
Inaasahan ko rin na mauunawaan mo ang gradient descent, 

17
00:00:52,494 --> 00:00:56,307
gaya ng inilarawan sa huling video, at kung paano ang ibig naming sabihin sa pag-aaral 

18
00:00:56,307 --> 00:00:59,813
ay gusto naming malaman kung aling mga timbang at bias ang nagpapaliit sa isang 

19
00:00:59,813 --> 00:01:01,260
partikular na function ng gastos.

20
00:01:02,040 --> 00:01:06,176
Bilang isang mabilis na paalala, para sa halaga ng isang halimbawa ng pagsasanay, 

21
00:01:06,176 --> 00:01:08,597
kukunin mo ang output na ibinibigay ng network, 

22
00:01:08,597 --> 00:01:12,683
kasama ang output na gusto mong ibigay nito, at idagdag ang mga parisukat ng mga 

23
00:01:12,683 --> 00:01:14,600
pagkakaiba sa pagitan ng bawat bahagi.

24
00:01:15,380 --> 00:01:18,971
Ang paggawa nito para sa lahat ng iyong sampu-sampung libong mga halimbawa ng pagsasanay 

25
00:01:18,971 --> 00:01:22,200
at pag-average ng mga resulta, binibigyan ka nito ng kabuuang halaga ng network.

26
00:01:22,200 --> 00:01:26,309
At parang hindi iyon sapat na pag-isipan, gaya ng inilarawan sa huling video, 

27
00:01:26,309 --> 00:01:30,101
ang hinahanap namin ay ang negatibong gradient ng cost function na ito, 

28
00:01:30,101 --> 00:01:34,685
na nagsasabi sa iyo kung paano mo kailangang baguhin ang lahat ng mga timbang at bias, 

29
00:01:34,685 --> 00:01:38,320
lahat ng mga koneksyong ito, upang mas mahusay na bawasan ang gastos.

30
00:01:43,260 --> 00:01:45,302
Ang backpropagation, ang paksa ng video na ito, 

31
00:01:45,302 --> 00:01:48,580
ay isang algorithm para sa pag-compute ng nakatutuwang kumplikadong gradient.

32
00:01:49,140 --> 00:01:52,690
At ang isang ideya mula sa huling video na talagang gusto kong hawakan mo 

33
00:01:52,690 --> 00:01:56,144
nang mahigpit sa iyong isipan ngayon ay dahil ang pag-iisip sa gradient 

34
00:01:56,144 --> 00:01:59,598
vector bilang isang direksyon sa 13,000 dimensyon ay, sa madaling sabi, 

35
00:01:59,598 --> 00:02:03,580
lampas sa saklaw ng ating mga imahinasyon, mayroong ibang paraan na maiisip mo ito.

36
00:02:04,600 --> 00:02:07,643
Ang magnitude ng bawat bahagi dito ay nagsasabi sa iyo kung 

37
00:02:07,643 --> 00:02:10,940
gaano kasensitibo ang paggana ng gastos sa bawat timbang at bias.

38
00:02:11,800 --> 00:02:15,487
Halimbawa, sabihin nating dumaan ka sa prosesong ilalarawan ko, 

39
00:02:15,487 --> 00:02:20,153
at kino-compute mo ang negatibong gradient, at ang bahaging nauugnay sa bigat sa 

40
00:02:20,153 --> 00:02:24,762
gilid na ito ay lalabas na 3.2, habang ang bahaging nauugnay sa gilid na ito ay 

41
00:02:24,762 --> 00:02:26,260
darating dito. bilang 0.1.

42
00:02:26,820 --> 00:02:30,136
Ang paraan kung paano mo mabibigyang-kahulugan iyon ay ang halaga ng function 

43
00:02:30,136 --> 00:02:33,196
ay 32 beses na mas sensitibo sa mga pagbabago sa unang timbang na iyon, 

44
00:02:33,196 --> 00:02:35,450
kaya kung ililipat mo ang halagang iyon nang kaunti, 

45
00:02:35,450 --> 00:02:37,490
ito ay magdudulot ng ilang pagbabago sa gastos, 

46
00:02:37,490 --> 00:02:40,849
at iyon Ang pagbabago ay 32 beses na mas malaki kaysa sa kung ano ang ibibigay 

47
00:02:40,849 --> 00:02:43,060
ng parehong pag-alog sa pangalawang timbang na iyon.

48
00:02:48,420 --> 00:02:51,399
Sa personal, noong una kong natutunan ang tungkol sa backpropagation, 

49
00:02:51,399 --> 00:02:55,059
sa palagay ko ang pinakanakalilito na aspeto ay ang notasyon at ang paghabol sa index 

50
00:02:55,059 --> 00:02:55,740
ng lahat ng ito.

51
00:02:56,220 --> 00:02:59,345
Ngunit sa sandaling ma-unwrap mo kung ano talaga ang ginagawa ng bawat bahagi ng 

52
00:02:59,345 --> 00:03:02,703
algorithm na ito, ang bawat indibidwal na epekto na nararanasan nito ay talagang medyo 

53
00:03:02,703 --> 00:03:06,138
intuitive, ito lang ay mayroong maraming maliliit na pagsasaayos na nagkakapatong-patong 

54
00:03:06,138 --> 00:03:06,640
sa bawat isa.

55
00:03:07,740 --> 00:03:11,574
Kaya't sisimulan ko ang mga bagay dito na may ganap na pagwawalang-bahala sa 

56
00:03:11,574 --> 00:03:15,741
notasyon, at hakbang lang sa mga epekto ng bawat halimbawa ng pagsasanay sa mga timbang 

57
00:03:15,741 --> 00:03:16,120
at bias.

58
00:03:17,020 --> 00:03:20,603
Dahil ang function ng gastos ay nagsasangkot ng pag-average ng isang partikular 

59
00:03:20,603 --> 00:03:24,141
na gastos sa bawat halimbawa sa lahat ng sampu-sampung libong mga halimbawa ng 

60
00:03:24,141 --> 00:03:27,501
pagsasanay, ang paraan ng pagsasaayos namin ng mga timbang at bias para sa 

61
00:03:27,501 --> 00:03:31,040
isang solong gradient descent step ay nakasalalay din sa bawat isang halimbawa.

62
00:03:31,680 --> 00:03:34,250
O sa halip, sa prinsipyo ito ay dapat, ngunit para sa computational na kahusayan 

63
00:03:34,250 --> 00:03:36,725
ay gagawa kami ng isang maliit na trick sa ibang pagkakataon upang pigilan ka 

64
00:03:36,725 --> 00:03:39,200
sa pangangailangang matumbok ang bawat solong halimbawa para sa bawat hakbang.

65
00:03:39,200 --> 00:03:42,448
Sa ibang mga kaso, sa ngayon, ang gagawin lang natin ay ituon 

66
00:03:42,448 --> 00:03:45,960
ang ating atensyon sa isang halimbawa, ang larawang ito ng isang 2.

67
00:03:46,720 --> 00:03:49,142
Ano ang dapat na epekto ng isang halimbawa ng pagsasanay 

68
00:03:49,142 --> 00:03:51,480
na ito sa kung paano nababagay ang mga timbang at bias?

69
00:03:52,680 --> 00:03:56,893
Sabihin nating nasa punto na tayo kung saan ang network ay hindi pa sanay na mabuti, 

70
00:03:56,893 --> 00:03:59,868
kaya ang mga pag-activate sa output ay magmumukhang random, 

71
00:03:59,868 --> 00:04:02,000
marahil ay parang 0.5, 0.8, 0.2, on and on.

72
00:04:02,520 --> 00:04:04,918
Hindi namin direktang mababago ang mga pag-activate na iyon, 

73
00:04:04,918 --> 00:04:07,160
mayroon lamang kaming impluwensya sa mga timbang at bias.

74
00:04:07,160 --> 00:04:09,574
Ngunit nakakatulong na subaybayan kung aling mga 

75
00:04:09,574 --> 00:04:12,580
pagsasaayos ang gusto naming maganap sa output layer na iyon.

76
00:04:13,360 --> 00:04:16,609
At dahil gusto naming i-classify nito ang imahe bilang 2, 

77
00:04:16,609 --> 00:04:21,260
gusto namin na ang pangatlong halaga ay tumaas habang ang lahat ng iba ay bumababa.

78
00:04:22,060 --> 00:04:25,691
Bukod dito, ang mga sukat ng mga nudge na ito ay dapat na proporsyonal sa 

79
00:04:25,691 --> 00:04:29,520
kung gaano kalayo ang bawat kasalukuyang halaga mula sa target na halaga nito.

80
00:04:30,220 --> 00:04:35,700
Halimbawa, ang pagtaas sa activation ng number 2 neuron ay mas mahalaga kaysa 

81
00:04:35,700 --> 00:04:40,900
sa pagbaba sa number 8 neuron, na medyo malapit na sa kung saan ito dapat.

82
00:04:42,040 --> 00:04:44,861
Kaya mag-zoom in pa, tumutok lang tayo sa isang neuron na ito, 

83
00:04:44,861 --> 00:04:47,280
ang isa na ang pag-activate ay nais nating madagdagan.

84
00:04:48,180 --> 00:04:52,514
Tandaan, ang pag-activate na iyon ay tinukoy bilang isang tiyak na may timbang na kabuuan 

85
00:04:52,514 --> 00:04:55,982
ng lahat ng mga pag-activate sa nakaraang layer, kasama ang isang bias, 

86
00:04:55,982 --> 00:04:59,932
na lahat pagkatapos ay nakasaksak sa isang bagay tulad ng sigmoid squishification 

87
00:04:59,932 --> 00:05:01,040
function, o isang ReLU.

88
00:05:01,640 --> 00:05:04,136
Kaya mayroong tatlong magkakaibang mga paraan na maaaring 

89
00:05:04,136 --> 00:05:07,020
magsama-sama upang makatulong na mapataas ang pag-activate na iyon.

90
00:05:07,440 --> 00:05:10,764
Maaari mong dagdagan ang bias, maaari mong dagdagan ang mga timbang, 

91
00:05:10,764 --> 00:05:14,040
at maaari mong baguhin ang mga pag-activate mula sa nakaraang layer.

92
00:05:14,940 --> 00:05:17,308
Sa pagtutuon sa kung paano dapat iakma ang mga timbang, 

93
00:05:17,308 --> 00:05:20,860
pansinin kung paano aktwal na may magkakaibang antas ng impluwensya ang mga timbang.

94
00:05:21,440 --> 00:05:23,909
Ang mga koneksyon sa pinakamaliwanag na mga neuron mula sa 

95
00:05:23,909 --> 00:05:26,295
naunang layer ay may pinakamalaking epekto dahil ang mga 

96
00:05:26,295 --> 00:05:29,100
timbang na iyon ay pinarami ng mas malalaking halaga ng activation.

97
00:05:31,460 --> 00:05:33,769
Kaya kung tataasan mo ang isa sa mga timbang na iyon, 

98
00:05:33,769 --> 00:05:36,806
ito ay talagang may mas malakas na impluwensya sa pinakahuling paggana 

99
00:05:36,806 --> 00:05:40,400
ng gastos kaysa sa pagtaas ng mga timbang ng mga koneksyon sa mga dimmer na neuron, 

100
00:05:40,400 --> 00:05:43,480
kahit na hanggang sa isang halimbawa ng pagsasanay na ito ay nababahala.

101
00:05:44,420 --> 00:05:46,892
Tandaan, kapag pinag-uusapan natin ang tungkol sa gradient descent, 

102
00:05:46,892 --> 00:05:49,656
hindi lang natin inaalala kung dapat ba pataasin o pababa ang bawat bahagi, 

103
00:05:49,656 --> 00:05:52,529
pinapahalagahan namin kung alin ang magbibigay sa iyo ng pinakamalaking halaga 

104
00:05:52,529 --> 00:05:53,220
para sa iyong pera.

105
00:05:55,020 --> 00:05:58,043
Ito, sa pamamagitan ng paraan, ay hindi bababa sa medyo nakapagpapaalaala 

106
00:05:58,043 --> 00:06:00,821
ng isang teorya sa neuroscience para sa kung paano natututo ang mga 

107
00:06:00,821 --> 00:06:03,109
biological network ng mga neuron, ang teoryang Hebbian, 

108
00:06:03,109 --> 00:06:06,460
na madalas na summed up sa parirala, mga neuron na nagsusunog ng magkasamang wire.

109
00:06:07,260 --> 00:06:09,655
Dito, ang pinakamalaking pagtaas sa mga timbang, 

110
00:06:09,655 --> 00:06:12,098
ang pinakamalaking pagpapalakas ng mga koneksyon, 

111
00:06:12,098 --> 00:06:14,836
ay nangyayari sa pagitan ng mga neuron na pinakaaktibo, 

112
00:06:14,836 --> 00:06:17,280
at sa mga neuron na nais nating maging mas aktibo.

113
00:06:17,940 --> 00:06:19,957
Sa isang kahulugan, ang mga neuron na nagpapaputok habang 

114
00:06:19,957 --> 00:06:22,044
nakikita ang isang 2 ay nagiging mas malakas na nauugnay sa 

115
00:06:22,044 --> 00:06:24,480
mga iyon ay ang mga nagpapaputok kapag iniisip ang tungkol sa isang 2.

116
00:06:25,400 --> 00:06:28,590
Upang maging malinaw, wala ako sa posisyon na gumawa ng mga pahayag sa isang 

117
00:06:28,590 --> 00:06:31,739
paraan o iba pa tungkol sa kung ang mga artipisyal na network ng mga neuron 

118
00:06:31,739 --> 00:06:34,763
ay kumikilos tulad ng mga biological na utak, at ito ay pinagsasama-sama 

119
00:06:34,763 --> 00:06:37,373
ang ideya ng wire na may kasamang ilang makabuluhang asterisk, 

120
00:06:37,373 --> 00:06:41,020
ngunit kinuha bilang isang napakaluwag. pagkakatulad, nakita kong kawili-wiling tandaan.

121
00:06:41,940 --> 00:06:45,618
Anyway, ang pangatlong paraan na makakatulong tayo sa pagtaas ng activation ng neuron 

122
00:06:45,618 --> 00:06:49,040
na ito ay sa pamamagitan ng pagbabago ng lahat ng activation sa nakaraang layer.

123
00:06:49,040 --> 00:06:52,754
Ibig sabihin, kung ang lahat ng konektado sa digit 2 neuron na iyon na may 

124
00:06:52,754 --> 00:06:56,717
positibong timbang ay naging mas maliwanag, at kung lahat ng konektado sa isang 

125
00:06:56,717 --> 00:07:00,680
negatibong timbang ay lumabo, ang digit 2 neuron na iyon ay magiging mas aktibo.

126
00:07:02,540 --> 00:07:06,572
At katulad ng mga pagbabago sa timbang, mas masusulit mo ang iyong pera sa pamamagitan 

127
00:07:06,572 --> 00:07:10,280
ng paghahanap ng mga pagbabago na proporsyonal sa laki ng kaukulang mga timbang.

128
00:07:12,140 --> 00:07:15,443
Ngayon siyempre, hindi natin direktang maimpluwensyahan ang mga pag-activate na iyon, 

129
00:07:15,443 --> 00:07:17,480
mayroon lamang tayong kontrol sa mga timbang at bias.

130
00:07:17,480 --> 00:07:20,661
Ngunit tulad ng huling layer, nakakatulong na 

131
00:07:20,661 --> 00:07:24,120
tandaan kung ano ang mga gustong pagbabagong iyon.

132
00:07:24,580 --> 00:07:26,975
Ngunit tandaan, ang pag-zoom out ng isang hakbang dito, 

133
00:07:26,975 --> 00:07:29,200
ito lang ang gusto ng digit 2 output neuron na iyon.

134
00:07:29,760 --> 00:07:33,104
Tandaan, gusto din namin na ang lahat ng iba pang mga neuron sa huling layer ay hindi 

135
00:07:33,104 --> 00:07:36,332
gaanong aktibo, at ang bawat isa sa iba pang mga output neuron ay may sariling mga 

136
00:07:36,332 --> 00:07:39,600
iniisip tungkol sa kung ano ang dapat mangyari sa pangalawang hanggang huling layer.

137
00:07:42,700 --> 00:07:47,258
Kaya, ang pagnanais ng digit na 2 neuron na ito ay idinagdag kasama ang mga pagnanasa 

138
00:07:47,258 --> 00:07:52,028
ng lahat ng iba pang mga output neuron para sa kung ano ang dapat mangyari sa pangalawang 

139
00:07:52,028 --> 00:07:56,003
hanggang huling layer na ito, muli sa proporsyon sa kaukulang mga timbang, 

140
00:07:56,003 --> 00:08:00,720
at sa proporsyon sa kung magkano ang bawat isa sa mga neuron na iyon. kailangang baguhin.

141
00:08:01,600 --> 00:08:05,480
Dito mismo pumapasok ang ideya ng pagpapalaganap pabalik.

142
00:08:05,820 --> 00:08:08,565
Sa pamamagitan ng pagsasama-sama ng lahat ng gustong epektong ito, 

143
00:08:08,565 --> 00:08:10,983
karaniwang nakakakuha ka ng listahan ng mga nudge na gusto 

144
00:08:10,983 --> 00:08:13,360
mong mangyari sa pangalawang hanggang huling layer na ito.

145
00:08:14,220 --> 00:08:17,946
At sa sandaling mayroon ka ng mga iyon, maaari mong muling ilapat ang parehong proseso 

146
00:08:17,946 --> 00:08:21,159
sa mga nauugnay na timbang at pagkiling na tumutukoy sa mga halagang iyon, 

147
00:08:21,159 --> 00:08:24,757
na inuulit ang parehong proseso na katatapos ko lang dumaan at lumilipat pabalik sa 

148
00:08:24,757 --> 00:08:25,100
network.

149
00:08:28,960 --> 00:08:32,848
At mag-zoom out nang kaunti, tandaan na ito lang ang nais ng isang solong 

150
00:08:32,848 --> 00:08:37,000
halimbawa ng pagsasanay na itulak ang bawat isa sa mga timbang at bias na iyon.

151
00:08:37,480 --> 00:08:40,765
Kung nakinig lang tayo sa gusto ng 2 na iyon, sa huli ay mabibigyang-insentibo 

152
00:08:40,765 --> 00:08:43,220
ang network para lang maiuri ang lahat ng larawan bilang 2.

153
00:08:44,059 --> 00:08:47,913
Kaya't ang gagawin mo ay dumaan sa parehong backprop na gawain para sa bawat 

154
00:08:47,913 --> 00:08:51,718
iba pang halimbawa ng pagsasanay, na nagre-record kung paano gustong baguhin ng 

155
00:08:51,718 --> 00:08:56,000
bawat isa sa kanila ang mga timbang at bias, at pagsama-samahin ang mga gustong pagbabago.

156
00:09:01,720 --> 00:09:06,063
Itong koleksyon dito ng mga na-average na nudge sa bawat timbang at bias ay, 

157
00:09:06,063 --> 00:09:09,674
sa madaling salita, ang negatibong gradient ng cost function na 

158
00:09:09,674 --> 00:09:13,680
na-reference sa huling video, o kahit isang bagay na proporsyonal dito.

159
00:09:14,380 --> 00:09:17,721
Maluwag na sinasabi ko lang dahil hindi pa ako nakakakuha ng tumpak na dami 

160
00:09:17,721 --> 00:09:21,107
tungkol sa mga nudge na iyon, ngunit kung naunawaan mo ang bawat pagbabagong 

161
00:09:21,107 --> 00:09:24,844
kakabanggit ko lang, kung bakit ang ilan ay proporsyonal na mas malaki kaysa sa iba, 

162
00:09:24,844 --> 00:09:27,218
at kung paano kailangang pagsamahin ang lahat ng ito, 

163
00:09:27,218 --> 00:09:31,000
naiintindihan mo ang mekanika para sa kung ano talaga ang ginagawa ng backpropagation.

164
00:09:33,960 --> 00:09:36,815
Sa pamamagitan ng paraan, sa pagsasagawa, nangangailangan ang mga 

165
00:09:36,815 --> 00:09:39,671
computer ng napakahabang panahon upang madagdagan ang impluwensya 

166
00:09:39,671 --> 00:09:42,440
ng bawat halimbawa ng pagsasanay sa bawat gradient descent step.

167
00:09:43,140 --> 00:09:44,820
Kaya narito ang karaniwang ginagawa sa halip.

168
00:09:45,480 --> 00:09:48,891
Random mong i-shuffle ang iyong data ng pagsasanay at pagkatapos ay hatiin ito sa isang 

169
00:09:48,891 --> 00:09:51,993
buong grupo ng mga mini-batch, sabihin nating bawat isa ay may 100 halimbawa ng 

170
00:09:51,993 --> 00:09:52,420
pagsasanay.

171
00:09:52,940 --> 00:09:56,200
Pagkatapos ay mag-compute ka ng isang hakbang ayon sa mini-batch.

172
00:09:56,960 --> 00:09:59,445
Hindi ito ang magiging aktwal na gradient ng cost function, 

173
00:09:59,445 --> 00:10:02,841
na nakadepende sa lahat ng data ng pagsasanay, hindi ang maliit na subset na ito, 

174
00:10:02,841 --> 00:10:05,037
kaya hindi ito ang pinaka-epektibong hakbang pababa, 

175
00:10:05,037 --> 00:10:08,474
ngunit ang bawat mini-batch ay nagbibigay sa iyo ng medyo magandang approximation, 

176
00:10:08,474 --> 00:10:12,120
at higit sa lahat, ito ay nagbibigay sa iyo ng isang makabuluhang computational speedup.

177
00:10:12,820 --> 00:10:16,432
Kung iplano mo ang trajectory ng iyong network sa ilalim ng nauugnay na ibabaw ng gastos, 

178
00:10:16,432 --> 00:10:19,804
ito ay magiging isang maliit na katulad ng isang lasing na lalaki na natitisod nang 

179
00:10:19,804 --> 00:10:22,975
walang patutunguhan pababa ng burol ngunit gumagawa ng mabilis na mga hakbang, 

180
00:10:22,975 --> 00:10:26,306
sa halip na isang maingat na pagkalkula ng tao na tinutukoy ang eksaktong pababang 

181
00:10:26,306 --> 00:10:29,517
direksyon ng bawat hakbang. bago gumawa ng napakabagal at maingat na hakbang sa 

182
00:10:29,517 --> 00:10:30,160
direksyong iyon.

183
00:10:31,540 --> 00:10:34,660
Ang pamamaraan na ito ay tinutukoy bilang stochastic gradient descent.

184
00:10:35,960 --> 00:10:39,620
Napakaraming nangyayari dito, kaya isa-isahin na lang natin ito, di ba?

185
00:10:40,440 --> 00:10:44,220
Ang backpropagation ay ang algorithm para sa pagtukoy kung paano nais ng isang solong 

186
00:10:44,220 --> 00:10:46,813
halimbawa ng pagsasanay na itulak ang mga timbang at bias, 

187
00:10:46,813 --> 00:10:49,626
hindi lamang sa mga tuntunin kung dapat silang tumaas o pababa, 

188
00:10:49,626 --> 00:10:53,450
ngunit sa mga tuntunin ng kung anong mga kaugnay na proporsyon sa mga pagbabagong iyon 

189
00:10:53,450 --> 00:10:55,560
ang sanhi ng pinakamabilis na pagbaba sa gastos.

190
00:10:56,260 --> 00:10:58,802
Ang isang tunay na gradient descent step ay kasangkot sa paggawa 

191
00:10:58,802 --> 00:11:01,383
nito para sa lahat ng iyong sampu-sampung libong mga halimbawa ng 

192
00:11:01,383 --> 00:11:04,200
pagsasanay at pag-a-average ng mga ninanais na pagbabago na makukuha mo.

193
00:11:04,860 --> 00:11:09,026
Ngunit iyon ay mabagal sa pagkalkula, kaya sa halip ay random mong i-subdivide ang data 

194
00:11:09,026 --> 00:11:13,240
sa mga mini-batch at kino-compute ang bawat hakbang na may kinalaman sa isang mini-batch.

195
00:11:14,000 --> 00:11:17,774
Sa paulit-ulit na pagdaan sa lahat ng mini-batch at paggawa ng mga pagsasaayos na ito, 

196
00:11:17,774 --> 00:11:21,635
magsasama-sama ka patungo sa isang lokal na minimum ng function ng gastos, ibig sabihin, 

197
00:11:21,635 --> 00:11:25,540
ang iyong network ay gagawa ng talagang mahusay na trabaho sa mga halimbawa ng pagsasanay.

198
00:11:27,240 --> 00:11:30,400
Kaya sa lahat ng sinabi, ang bawat linya ng code na pupunta sa 

199
00:11:30,400 --> 00:11:34,713
pagpapatupad ng backprop ay talagang tumutugma sa isang bagay na nakita mo na ngayon, 

200
00:11:34,713 --> 00:11:36,720
hindi bababa sa mga impormal na termino.

201
00:11:37,560 --> 00:11:39,824
Ngunit kung minsan ang pag-alam kung ano ang ginagawa ng matematika 

202
00:11:39,824 --> 00:11:41,822
ay kalahati lamang ng labanan, at ang kumakatawan lamang sa 

203
00:11:41,822 --> 00:11:44,120
mapahamak na bagay ay kung saan ito ay nagiging magulo at nakakalito.

204
00:11:44,860 --> 00:11:48,759
Kaya para sa iyo na gustong lumalim, ang susunod na video ay dumaan sa parehong mga 

205
00:11:48,759 --> 00:11:52,427
ideya na ipinakita dito, ngunit sa mga tuntunin ng pinagbabatayan na calculus, 

206
00:11:52,427 --> 00:11:56,420
na sana ay gawing mas pamilyar ito habang nakikita mo ang paksa sa ibang mapagkukunan.

207
00:11:57,340 --> 00:12:00,650
Bago iyon, isang bagay na dapat bigyang-diin ay para gumana ang algorithm na ito, 

208
00:12:00,650 --> 00:12:04,123
at ito ay para sa lahat ng uri ng machine learning na higit pa sa mga neural network, 

209
00:12:04,123 --> 00:12:05,900
kailangan mo ng maraming data ng pagsasanay.

210
00:12:06,420 --> 00:12:09,151
Sa aming kaso, ang isang bagay na gumagawa ng mga sulat-kamay na 

211
00:12:09,151 --> 00:12:12,344
digit na isang magandang halimbawa ay ang pagkakaroon ng database ng MNIST, 

212
00:12:12,344 --> 00:12:14,740
na may napakaraming mga halimbawa na na-label ng mga tao.

213
00:12:15,300 --> 00:12:18,351
Kaya't ang isang karaniwang hamon na magiging pamilyar sa inyo na nagtatrabaho 

214
00:12:18,351 --> 00:12:21,291
sa machine learning ay ang pagkuha lamang ng may label na data ng pagsasanay na 

215
00:12:21,291 --> 00:12:24,195
talagang kailangan mo, kung iyon ay ang pagkakaroon ng mga tao na mag-label ng 

216
00:12:24,195 --> 00:12:27,100
libu-libong mga larawan, o anumang iba pang uri ng data na maaari mong harapin.

