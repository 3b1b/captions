1
00:00:00,000 --> 00:00:09,640
在这里，我们讨论反向传播，这是神经网络学习背后的核心算法。

2
00:00:09,640 --> 00:00:13,688
快速回顾一下我们的情况后，我要做的第一件事是直

3
00:00:13,688 --> 00:00:17,400
观地演练算法实际在做什么，而不参考任何公式。

4
00:00:17,400 --> 00:00:20,886
然后，对于那些确实想深入研究数学的人，下

5
00:00:20,886 --> 00:00:24,040
一个视频将介绍所有这一切背后的微积分。

6
00:00:24,040 --> 00:00:27,700
如果您观看了最后两个视频，或者只是了解了适当的背景

7
00:00:27,700 --> 00:00:31,080
，您就会知道什么是神经网络，以及它如何前馈信息。

8
00:00:31,080 --> 00:00:34,678
在这里，我们正在做识别手写数字的经典示例，其像素

9
00:00:34,678 --> 00:00:38,276
值被输入到具 有 784 个神经元的网络的第一层

10
00:00:38,276 --> 00:00:41,874
，并且我已经展示了一个具有 两个隐藏层的网络，每

11
00:00:41,874 --> 00:00:45,472
个隐藏层只有 16 个神经元，以及一个输 出由

12
00:00:45,472 --> 00:00:49,520
10 个神经元组成的层，指示网络选择哪个数字作为答案。

13
00:00:49,520 --> 00:00:53,916
我还希望您理解梯度下降，如上一个视频中所

14
00:00:53,916 --> 00:00:58,102
述，以及我们所说的学习的意思是我们想要

15
00:00:58,102 --> 00:01:02,080
找到哪些权重和偏差最小化某个成本函数。

16
00:01:02,080 --> 00:01:06,645
快速提醒一下，对于单个训练示例的成本，您

17
00:01:06,645 --> 00:01:11,211
可以获取网络提供的输出以及您希望其提供的

18
00:01:11,211 --> 00:01:15,560
输出，并将每个组件之间的差异的平方相加。

19
00:01:15,560 --> 00:01:19,507
对所有数万个训练示例执行此操作并对结

20
00:01:19,507 --> 00:01:23,040
果进行平均，即可得出网络的总成本。

21
00:01:23,040 --> 00:01:28,172
好像这还不够考虑，正如上一个视频中所描述

22
00:01:28,172 --> 00:01:33,304
的，我们正在寻找的是这个成本函数的负梯度

23
00:01:33,304 --> 00:01:38,436
，它告诉你需要如何改变所有的权重和偏差，

24
00:01:38,436 --> 00:01:43,080
所有的这些连接，从而最有效地降低成本。

25
00:01:43,080 --> 00:01:49,600
本视频的主题反向传播是一种用 于计算疯狂复杂梯度的算法。

26
00:01:49,600 --> 00:01:53,400
上一个视频中我真正希望您现在牢牢记住的一

27
00:01:53,400 --> 00:01:57,562
个想法是，因为将梯度向量视为 13,00 0

28
00:01:57,562 --> 00:02:01,000
维中的方向，简单地说，超出了我们的想

29
00:02:01,000 --> 00:02:04,620
象范围，所以还有另一个想法你可以这样想。

30
00:02:04,620 --> 00:02:08,336
这里每个分量的大小告诉您成本函

31
00:02:08,336 --> 00:02:11,820
数对每个权重和偏差的敏感程度。

32
00:02:11,820 --> 00:02:16,630
例如，假设您经历了我将要描述的过程，并计

33
00:02:16,630 --> 00:02:21,670
算负梯度，与此边缘上的权重相关的分量为 3。

34
00:02:21,670 --> 00:02:26,940
2，而与此边相关的分量在这里显示为 0。1.

35
00:02:26,940 --> 00:02:31,651
您的解释方式是，函数的成本对第一个权重的变化

36
00:02:31,651 --> 00:02:36,362
敏感度是原来的 32 倍，因此，如果您稍微调

37
00:02:36,362 --> 00:02:41,073
整该值，就会导致成本发生一些变化，而这种变化

38
00:02:41,073 --> 00:02:45,580
比同样摆动第二个重量时产生的力大 32 倍。

39
00:02:45,580 --> 00:02:50,932
就我个人而言，当我第一次学习反向传播时，我认

40
00:02:50,932 --> 00:02:55,820
为最令人困惑的方面就是它的符号和索引追逐。

41
00:02:55,820 --> 00:02:59,992
但是，一旦你解开这个算法的每个部分的真正

42
00:02:59,992 --> 00:03:03,965
作用，它所产生的每个单独的效果实际上都

43
00:03:03,965 --> 00:03:07,740
非常直观，只是有很多小的调整相互叠加。

44
00:03:07,740 --> 00:03:12,801
因此，我将从这里开始，完全忽略符号，并逐

45
00:03:12,801 --> 00:03:17,380
步了解每个训练示例对权重和偏差的影响。

46
00:03:17,380 --> 00:03:22,374
由于成本函数涉及对所有数以万计的训练示例中每个

47
00:03:22,374 --> 00:03:27,161
示例的特定成本进行平均，因此我们调整单个梯度

48
00:03:27,161 --> 00:03:31,740
下降步骤的权重和偏差的方式也取决于每个示例。

49
00:03:31,740 --> 00:03:35,873
或者更确切地说，原则上应该如此，但为了计算效率，我们稍

50
00:03:35,873 --> 00:03:39,860
后会做一些小技巧，以防止您需要为每个步骤击中每个示例。

51
00:03:39,860 --> 00:03:43,493
在其他情况下，现在我们要做的就是将注意力

52
00:03:43,493 --> 00:03:46,780
集中在一个例子上，即这张 2 的图像。

53
00:03:46,780 --> 00:03:51,740
这一训练示例对权重和偏差的调整有何影响？

54
00:03:51,740 --> 00:03:56,296
假设我们正处于网络尚未经过良好训练的阶段，因此输出

55
00:03:56,296 --> 00:03:59,800
中的激活看起来相当随机，可能类似于 0。

56
00:03:59,800 --> 00:04:02,780
5, 0.8, 0.2 、不断地。

57
00:04:02,780 --> 00:04:06,435
我们不能直接改变这些激活，我们只能

58
00:04:06,435 --> 00:04:10,090
影响权重和偏差，但是跟踪我们希望对

59
00:04:10,090 --> 00:04:13,340
该输出层进行哪些调整是有帮助的。

60
00:04:13,340 --> 00:04:17,710
由于我们希望它将图像分类为 2，因此我们希望

61
00:04:17,710 --> 00:04:21,700
第三个值向上移动，而所有其他值都向下移动。

62
00:04:21,700 --> 00:04:30,220
此外，这些微调的大小应与每个当 前值与其目标值的距离成正比。

63
00:04:30,220 --> 00:04:36,140
例如，从某种意义上说，增加 2 号神 经元的激活比减少

64
00:04:36,140 --> 00:04:42,060
8 号神经元的激活 更重要，后者已经非常接近应有的位置。

65
00:04:42,060 --> 00:04:45,162
因此，进一步放大，让我们只关注这

66
00:04:45,162 --> 00:04:47,900
个神经元，我们希望增加其激活。

67
00:04:47,900 --> 00:04:52,635
请记住，激活被定义为前一层中所有激活的特定加

68
00:04:52,635 --> 00:04:57,370
权总和，加上偏差，然后将其全部插入 sigm

69
00:04:57,370 --> 00:05:01,900
oid 压缩函数或 ReLU 之类的函数中。

70
00:05:01,900 --> 00:05:08,060
因此，可以通过三种不同的途 径联合起来帮助提高激活率。

71
00:05:08,060 --> 00:05:15,300
您可以增加偏差，可以增加权重 ，并且可以更改上一层的激活。

72
00:05:15,300 --> 00:05:18,479
重点关注如何调整权重，注意权重

73
00:05:18,479 --> 00:05:21,460
实际上如何具有不同程度的影响。

74
00:05:21,460 --> 00:05:26,716
与前一层最亮神经元的连接具有最大的影

75
00:05:26,716 --> 00:05:31,420
响，因为这些权重乘以较大的激活值。

76
00:05:31,420 --> 00:05:35,751
因此，如果您要增加其中一个权重，它实际上对

77
00:05:35,751 --> 00:05:40,082
最终成本函数的影响比增加与较暗神经元的连接

78
00:05:40,082 --> 00:05:44,020
权重更大，至少就这一训练示例而言是这样。

79
00:05:44,020 --> 00:05:47,520
请记住，当我们谈论梯度下降时，我们不仅仅

80
00:05:47,520 --> 00:05:50,853
关心每个组件是否应该向上或向下推动，我

81
00:05:50,853 --> 00:05:54,020
们关心哪些组件可以为您带来最大的收益。

82
00:05:54,020 --> 00:05:58,387
顺便说一句，这至少在某种程度上让人想起神经科学

83
00:05:58,387 --> 00:06:02,754
中关于神经元生物网络如何学习的理论，赫布理论，

84
00:06:02,754 --> 00:06:06,940
通常用短语来概括：一起放电的神经元连接在一起。

85
00:06:06,940 --> 00:06:10,892
在这里，权重的最大增加、连接的最

86
00:06:10,892 --> 00:06:14,612
大加强发生在最活跃的神经元和我

87
00:06:14,612 --> 00:06:18,100
们希望变得更活跃的神经元之间。

88
00:06:18,100 --> 00:06:21,936
从某种意义上说，看到 2 时放电的神经元与思

89
00:06:21,936 --> 00:06:25,440
考 2 时放电的神经元之间的联系更加紧密。

90
00:06:25,440 --> 00:06:29,664
需要明确的是，我无法以某种方式对神经元的人

91
00:06:29,664 --> 00:06:33,696
工网络是否表现得像生物大脑做出陈述，这种

92
00:06:33,696 --> 00:06:37,920
将电线连接在一起的想法带有几个有意义的星号

93
00:06:37,920 --> 00:06:41,760
，但被视为非常松散的类比，我觉得很有趣。

94
00:06:41,760 --> 00:06:45,657
无论如何，我们可以帮助增加该神经元激活

95
00:06:45,657 --> 00:06:49,360
的第三种方法是更改前一层中的所有激活。

96
00:06:49,360 --> 00:06:53,916
也就是说，如果与具有正权值的数字 2 神经元相连的

97
00:06:53,916 --> 00:06:58,473
所有东西都变得更亮，而与负权值连接的所有东西都变得

98
00:06:58,473 --> 00:07:02,680
更暗，那么那个数字 2 神经元就会变得更加活跃。

99
00:07:02,680 --> 00:07:06,870
与权重变化类似，通过寻求与相应权重大

100
00:07:06,870 --> 00:07:10,840
小成比例的变化，您将获得最大的收益。

101
00:07:10,840 --> 00:07:14,700
当然，现在我们不能直接影响这些

102
00:07:14,700 --> 00:07:18,320
激活，我们只能控制权重和偏差。

103
00:07:18,320 --> 00:07:23,960
但就像最后一层一样，记下这 些所需的更改会很有帮助。

104
00:07:23,960 --> 00:07:30,040
但请记住，这里缩小一步，这只是 数字 2 输出神经元想要的。

105
00:07:30,040 --> 00:07:34,563
请记住，我们还希望最后一层中的所有其他神经

106
00:07:34,563 --> 00:07:39,087
元变得不那么活跃，并且每个其他输出神经元对

107
00:07:39,087 --> 00:07:43,200
于倒数第二层应该发生什么都有自己的想法。

108
00:07:43,200 --> 00:07:47,893
因此，这个数字 2 神经元的愿望与所有

109
00:07:47,893 --> 00:07:52,587
其他输出神经元的愿望相加，以决定倒数第

110
00:07:52,587 --> 00:07:57,281
二层应该发生什么，同样与相应的权重成比

111
00:07:57,281 --> 00:08:01,740
例，并与每个神经元需要多少成比例改变。

112
00:08:01,740 --> 00:08:05,940
这就是向后传播的想法出现的地方。

113
00:08:05,940 --> 00:08:10,212
通过将所有这些所需的效果添加在一起，您基本上

114
00:08:10,212 --> 00:08:14,300
会得到一个您想要在倒数第二层发生的微调列表。

115
00:08:14,300 --> 00:08:19,422
一旦有了这些，您就可以递归地将相同的过程

116
00:08:19,422 --> 00:08:24,545
应用于确定这些值的相关权重和偏差，重复我

117
00:08:24,545 --> 00:08:29,180
刚刚走过的相同过程并在网络中向后移动。

118
00:08:29,180 --> 00:08:33,558
再缩小一点，请记住，这只是单个训练示例希

119
00:08:33,558 --> 00:08:37,520
望推动这些权重和偏差中的每一个的方式。

120
00:08:37,520 --> 00:08:40,924
如果我们只听 2 想要的内容，网络

121
00:08:40,924 --> 00:08:44,140
最终会被激励将所有图像分类为 2。

122
00:08:44,140 --> 00:08:50,290
因此，您要做的就是对每个其他训练示例执行

123
00:08:50,290 --> 00:08:56,441
相同的反向传播例程，记录每个示例如何更改

124
00:08:56,441 --> 00:09:02,300
权重和偏差，并将这些所需的更改一起平均。

125
00:09:02,300 --> 00:09:06,458
宽松地说，这里对每个权重和偏差的平均微

126
00:09:06,458 --> 00:09:10,617
调的集合是上一个视频中引用的成本函数的

127
00:09:10,617 --> 00:09:14,360
负梯度，或者至少是与之成比例的东西。

128
00:09:14,360 --> 00:09:19,441
我之所以说粗略地说，只是因为我还没有对这些推动进行

129
00:09:19,441 --> 00:09:24,327
量化精确，但如果你理解我刚才提到的每一个变化，为

130
00:09:24,327 --> 00:09:29,409
什么有些变化比其他变化更大，以及它们如何需要加在一

131
00:09:29,409 --> 00:09:34,100
起，你就会理解其中的机制反向传播实际上在做什么。

132
00:09:34,100 --> 00:09:38,705
顺便说一句，在实践中，计算机需要花费很长时间才

133
00:09:38,705 --> 00:09:43,120
能将每个梯度下降步骤的每个训练示例的影响相加。

134
00:09:43,120 --> 00:09:45,540
所以这就是通常所做的事情。

135
00:09:45,540 --> 00:09:49,646
您随机打乱训练数据并将其分成一大堆小批量，

136
00:09:49,646 --> 00:09:53,380
假设每个小批量都有 100 个训练示例。

137
00:09:53,380 --> 00:09:56,980
然后根据小批量计算步骤。

138
00:09:56,980 --> 00:10:01,048
它不是成本函数的实际梯度，它取决于所有训练数

139
00:10:01,048 --> 00:10:05,116
据，而不是这个微小的子集，所以它不是最有效的

140
00:10:05,116 --> 00:10:09,185
下坡步骤，但每个小批量确实给你一个非常好的近

141
00:10:09,185 --> 00:10:12,900
似值，更重要的是它为您带来显着的计算加速。

142
00:10:12,900 --> 00:10:17,629
如果你要在相关成本面下绘制网络的轨迹，那么它更

143
00:10:17,629 --> 00:10:22,358
像是一个醉汉漫无目的地跌跌撞撞地下山，但步伐很

144
00:10:22,358 --> 00:10:27,087
快，而不是一个仔细计算的人确定每一步的确切下坡

145
00:10:27,087 --> 00:10:31,620
方向然后朝着这个方向迈出非常缓慢而谨慎的一步。

146
00:10:31,620 --> 00:10:35,200
该技术称为随机梯度下降。

147
00:10:35,200 --> 00:10:40,400
这里发生了很多事情，所以让我们自己总结一下，好吗？

148
00:10:40,400 --> 00:10:44,568
反向传播是一种算法，用于确定单个训练示

149
00:10:44,568 --> 00:10:48,528
例如何推动权重和偏差，不仅在于它们是

150
00:10:48,528 --> 00:10:52,488
否应该上升或下降，还在于这些变化的相

151
00:10:52,488 --> 00:10:56,240
对比例导致权重和偏差最快下降。成本。

152
00:10:56,240 --> 00:11:00,902
真正的梯度下降步骤将涉及对所有数以万计的

153
00:11:00,902 --> 00:11:05,342
训练示例执行此操作，并对获得的所需变化

154
00:11:05,342 --> 00:11:09,782
进行平均，但这在计算上很慢，因此您可以

155
00:11:09,782 --> 00:11:14,000
将数据随机细分为小批量，并根据小批量。

156
00:11:14,000 --> 00:11:18,668
反复检查所有小批量并进行这些调整，您将

157
00:11:18,668 --> 00:11:23,337
收敛到成本函数的局部最小值，也就是说您

158
00:11:23,337 --> 00:11:27,540
的网络最终将在训练示例上做得非常好。

159
00:11:27,540 --> 00:11:32,709
综上所述，用于实现反向传播的每一行代码实际上都与您

160
00:11:32,709 --> 00:11:37,680
现在所看到的内容相对应，至少在非正式术语中是这样。

161
00:11:37,680 --> 00:11:41,316
但有时知道数学的作用只是成功的一半，仅仅

162
00:11:41,316 --> 00:11:44,780
代表该死的东西就会让一切变得混乱和混乱。

163
00:11:44,780 --> 00:11:49,063
因此，对于那些确实想要深入了解的人，下一个视频将

164
00:11:49,063 --> 00:11:53,347
介绍与此处刚刚介绍的相同的想法，但就基本微积分而

165
00:11:53,347 --> 00:11:57,460
言，这应该会让您在看到主题时更加熟悉。其他资源。

166
00:11:57,460 --> 00:12:00,694
在此之前，值得强调的一件事是，要使该算

167
00:12:00,694 --> 00:12:03,928
法发挥作用，并且这适用于神经网络之外的

168
00:12:03,928 --> 00:12:06,840
各种机器学习，您需要大量的训练数据。

169
00:12:06,840 --> 00:12:11,110
在我们的例子中，手写数字成为如此好的例子的原因之一是

170
00:12:11,110 --> 00:12:15,380
MNIST 数据库的存在，其中有很多由人类标记的例子。

171
00:12:15,380 --> 00:12:19,553
因此，从事机器学习工作的人所熟悉的一个常见挑战是

172
00:12:19,553 --> 00:12:23,560
获取实际需要的标记训练数据，无论是让人标记数以

173
00:12:23,560 --> 00:12:27,400
万计的图像，还是您可能处理的任何其他数据类型。

