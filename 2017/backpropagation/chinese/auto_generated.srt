1
00:00:00,000 --> 00:00:09,640
在这里，我们讨论反向传播，这是神经网络学习背后的核心算法。

2
00:00:09,640 --> 00:00:13,320
快速回顾一下我们的情况后，我要做的第一件事是直

3
00:00:13,320 --> 00:00:17,400
观地演练算法实际在做什么，而不参考任何公式。

4
00:00:17,400 --> 00:00:21,400
然后，对于那些确实想深入研究数学的人，下

5
00:00:21,400 --> 00:00:24,040
一个视频将介绍所有这一切背后的微积分。

6
00:00:24,040 --> 00:00:27,320
如果您观看了最后两个视频，或者只是了解了适当的背景

7
00:00:27,320 --> 00:00:31,080
，您就会知道什么是神经网络，以及它如何前馈信息。

8
00:00:31,080 --> 00:00:35,520
在这里，我们正在做识别手写数字的经典示例，其像素值被输入到具

9
00:00:35,520 --> 00:00:40,280
有 784 个神经元的网络的第一层，并且我已经展示了一个具有

10
00:00:40,280 --> 00:00:44,720
两个隐藏层的网络，每个隐藏层只有 16 个神经元，以及一个输

11
00:00:44,720 --> 00:00:49,520
出由 10 个神经元组成的层，指示网络选择哪个数字作为答案。

12
00:00:49,520 --> 00:00:54,480
我还希望您理解梯度下降，如上一个视频中所

13
00:00:54,480 --> 00:01:00,160
述，以及我们所说的学习的意思是我们想要

14
00:01:00,160 --> 00:01:02,080
找到哪些权重和偏差最小化某个成本函数。

15
00:01:02,080 --> 00:01:07,560
快速提醒一下，对于单个训练示例的成本，您

16
00:01:07,560 --> 00:01:12,920
可以获取网络提供的输出以及您希望其提供的

17
00:01:12,920 --> 00:01:15,560
输出，并将每个组件之间的差异的平方相加。

18
00:01:15,560 --> 00:01:20,160
对所有数万个训练示例执行此操作并对结

19
00:01:20,160 --> 00:01:23,040
果进行平均，即可得出网络的总成本。

20
00:01:23,040 --> 00:01:26,320
好像这还不够考虑，正如上一个视频中所描述

21
00:01:26,320 --> 00:01:31,700
的，我们正在寻找的是这个成本函数的负梯度

22
00:01:31,700 --> 00:01:36,000
，它告诉你需要如何改变所有的权重和偏差，

23
00:01:36,000 --> 00:01:43,080
所有的这些连接，从而最有效地降低成本。

24
00:01:43,080 --> 00:01:48,600
本视频的主题反向传播是一种用

25
00:01:48,600 --> 00:01:49,600
于计算疯狂复杂梯度的算法。

26
00:01:49,600 --> 00:01:53,300
上一个视频中我真正希望您现在牢牢记住的一

27
00:01:53,300 --> 00:01:58,280
个想法是，因为将梯度向量视为 13,00

28
00:01:58,280 --> 00:02:02,660
0 维中的方向，简单地说，超出了我们的想

29
00:02:02,660 --> 00:02:04,620
象范围，所以还有另一个想法你可以这样想。

30
00:02:04,620 --> 00:02:09,700
这里每个分量的大小告诉您成本函

31
00:02:09,700 --> 00:02:11,820
数对每个权重和偏差的敏感程度。

32
00:02:11,820 --> 00:02:15,180
例如，假设您经历了我将要描述的过程，并计

33
00:02:15,180 --> 00:02:19,800
算负梯度，与此边缘上的权重相关的分量为

34
00:02:19,800 --> 00:02:26,940
3。2，而与此边相关的分量在这里显示为 0。1.

35
00:02:26,940 --> 00:02:31,520
您的解释方式是，函数的成本对第一个权重的变化

36
00:02:31,520 --> 00:02:36,100
敏感度是原来的 32 倍，因此，如果您稍微调

37
00:02:36,100 --> 00:02:40,780
整该值，就会导致成本发生一些变化，而这种变化

38
00:02:40,780 --> 00:02:45,580
比同样摆动第二个重量时产生的力大 32 倍。

39
00:02:45,580 --> 00:02:52,500
就我个人而言，当我第一次学习反向传播时，我认

40
00:02:52,500 --> 00:02:55,820
为最令人困惑的方面就是它的符号和索引追逐。

41
00:02:55,820 --> 00:03:00,240
但是，一旦你解开这个算法的每个部分的真正

42
00:03:00,240 --> 00:03:04,540
作用，它所产生的每个单独的效果实际上都

43
00:03:04,540 --> 00:03:07,740
非常直观，只是有很多小的调整相互叠加。

44
00:03:07,740 --> 00:03:11,380
因此，我将从这里开始，完全忽略符号，并逐

45
00:03:11,380 --> 00:03:17,380
步了解每个训练示例对权重和偏差的影响。

46
00:03:17,380 --> 00:03:21,880
由于成本函数涉及对所有数以万计的训练示例中每个

47
00:03:21,880 --> 00:03:26,980
示例的特定成本进行平均，因此我们调整单个梯度

48
00:03:26,980 --> 00:03:31,740
下降步骤的权重和偏差的方式也取决于每个示例。

49
00:03:31,740 --> 00:03:35,300
或者更确切地说，原则上应该如此，但为了计算效率，我们稍

50
00:03:35,300 --> 00:03:39,860
后会做一些小技巧，以防止您需要为每个步骤击中每个示例。

51
00:03:39,860 --> 00:03:44,460
在其他情况下，现在我们要做的就是将注意力

52
00:03:44,460 --> 00:03:46,780
集中在一个例子上，即这张 2 的图像。

53
00:03:46,780 --> 00:03:51,740
这一训练示例对权重和偏差的调整有何影响？

54
00:03:51,740 --> 00:03:56,040
假设我们正处于网络尚未经过良好训练的阶段，因此输出

55
00:03:56,040 --> 00:04:01,620
中的激活看起来相当随机，可能类似于 0。5, 0.8, 0.2

56
00:04:01,620 --> 00:04:02,780
、不断地。

57
00:04:02,780 --> 00:04:06,700
我们不能直接改变这些激活，我们只能

58
00:04:06,700 --> 00:04:11,380
影响权重和偏差，但是跟踪我们希望对

59
00:04:11,380 --> 00:04:13,340
该输出层进行哪些调整是有帮助的。

60
00:04:13,340 --> 00:04:18,220
由于我们希望它将图像分类为 2，因此我们希望

61
00:04:18,220 --> 00:04:21,700
第三个值向上移动，而所有其他值都向下移动。

62
00:04:21,700 --> 00:04:27,620
此外，这些微调的大小应与每个当

63
00:04:27,620 --> 00:04:30,220
前值与其目标值的距离成正比。

64
00:04:30,220 --> 00:04:35,260
例如，从某种意义上说，增加 2 号神

65
00:04:35,260 --> 00:04:39,620
经元的激活比减少 8 号神经元的激活

66
00:04:39,620 --> 00:04:42,060
更重要，后者已经非常接近应有的位置。

67
00:04:42,060 --> 00:04:46,260
因此，进一步放大，让我们只关注这

68
00:04:46,260 --> 00:04:47,900
个神经元，我们希望增加其激活。

69
00:04:47,900 --> 00:04:53,680
请记住，激活被定义为前一层中所有激活的特定加

70
00:04:53,680 --> 00:04:58,380
权总和，加上偏差，然后将其全部插入 sigm

71
00:04:58,380 --> 00:05:01,900
oid 压缩函数或 ReLU 之类的函数中。

72
00:05:01,900 --> 00:05:07,060
因此，可以通过三种不同的途

73
00:05:07,060 --> 00:05:08,060
径联合起来帮助提高激活率。

74
00:05:08,060 --> 00:05:12,800
您可以增加偏差，可以增加权重

75
00:05:12,800 --> 00:05:15,300
，并且可以更改上一层的激活。

76
00:05:15,300 --> 00:05:19,720
重点关注如何调整权重，注意权重

77
00:05:19,720 --> 00:05:21,460
实际上如何具有不同程度的影响。

78
00:05:21,460 --> 00:05:25,100
与前一层最亮神经元的连接具有最大的影

79
00:05:25,100 --> 00:05:31,420
响，因为这些权重乘以较大的激活值。

80
00:05:31,420 --> 00:05:35,820
因此，如果您要增加其中一个权重，它实际上对

81
00:05:35,820 --> 00:05:40,900
最终成本函数的影响比增加与较暗神经元的连接

82
00:05:40,900 --> 00:05:44,020
权重更大，至少就这一训练示例而言是这样。

83
00:05:44,020 --> 00:05:48,700
请记住，当我们谈论梯度下降时，我们不仅仅

84
00:05:48,700 --> 00:05:53,020
关心每个组件是否应该向上或向下推动，我

85
00:05:53,020 --> 00:05:54,020
们关心哪些组件可以为您带来最大的收益。

86
00:05:54,020 --> 00:06:00,260
顺便说一句，这至少在某种程度上让人想起神经科学

87
00:06:00,260 --> 00:06:04,900
中关于神经元生物网络如何学习的理论，赫布理论，

88
00:06:04,900 --> 00:06:06,940
通常用短语来概括：一起放电的神经元连接在一起。

89
00:06:06,940 --> 00:06:12,460
在这里，权重的最大增加、连接的最

90
00:06:12,460 --> 00:06:16,860
大加强发生在最活跃的神经元和我

91
00:06:16,860 --> 00:06:18,100
们希望变得更活跃的神经元之间。

92
00:06:18,100 --> 00:06:22,520
从某种意义上说，看到 2 时放电的神经元与思

93
00:06:22,520 --> 00:06:25,440
考 2 时放电的神经元之间的联系更加紧密。

94
00:06:25,440 --> 00:06:29,240
需要明确的是，我无法以某种方式对神经元的人

95
00:06:29,240 --> 00:06:34,020
工网络是否表现得像生物大脑做出陈述，这种

96
00:06:34,020 --> 00:06:39,440
将电线连接在一起的想法带有几个有意义的星号

97
00:06:39,440 --> 00:06:41,760
，但被视为非常松散的类比，我觉得很有趣。

98
00:06:41,760 --> 00:06:46,760
无论如何，我们可以帮助增加该神经元激活

99
00:06:46,760 --> 00:06:49,360
的第三种方法是更改前一层中的所有激活。

100
00:06:49,360 --> 00:06:55,080
也就是说，如果与具有正权值的数字 2 神经元相连的

101
00:06:55,080 --> 00:06:59,480
所有东西都变得更亮，而与负权值连接的所有东西都变得

102
00:06:59,480 --> 00:07:02,680
更暗，那么那个数字 2 神经元就会变得更加活跃。

103
00:07:02,680 --> 00:07:06,200
与权重变化类似，通过寻求与相应权重大

104
00:07:06,200 --> 00:07:10,840
小成比例的变化，您将获得最大的收益。

105
00:07:10,840 --> 00:07:16,520
当然，现在我们不能直接影响这些

106
00:07:16,520 --> 00:07:18,320
激活，我们只能控制权重和偏差。

107
00:07:18,320 --> 00:07:22,960
但就像最后一层一样，记下这

108
00:07:22,960 --> 00:07:23,960
些所需的更改会很有帮助。

109
00:07:23,960 --> 00:07:29,040
但请记住，这里缩小一步，这只是

110
00:07:29,040 --> 00:07:30,040
数字 2 输出神经元想要的。

111
00:07:30,040 --> 00:07:34,960
请记住，我们还希望最后一层中的所有其他神经

112
00:07:34,960 --> 00:07:38,460
元变得不那么活跃，并且每个其他输出神经元对

113
00:07:38,460 --> 00:07:43,200
于倒数第二层应该发生什么都有自己的想法。

114
00:07:43,200 --> 00:07:49,220
因此，这个数字 2 神经元的愿望与所有

115
00:07:49,220 --> 00:07:54,800
其他输出神经元的愿望相加，以决定倒数第

116
00:07:54,800 --> 00:08:00,240
二层应该发生什么，同样与相应的权重成比

117
00:08:00,240 --> 00:08:01,740
例，并与每个神经元需要多少成比例改变。

118
00:08:01,740 --> 00:08:05,940
这就是向后传播的想法出现的地方。

119
00:08:05,940 --> 00:08:11,080
通过将所有这些所需的效果添加在一起，您基本上

120
00:08:11,080 --> 00:08:14,300
会得到一个您想要在倒数第二层发生的微调列表。

121
00:08:14,300 --> 00:08:18,740
一旦有了这些，您就可以递归地将相同的过程

122
00:08:18,740 --> 00:08:23,400
应用于确定这些值的相关权重和偏差，重复我

123
00:08:23,400 --> 00:08:29,180
刚刚走过的相同过程并在网络中向后移动。

124
00:08:29,180 --> 00:08:33,960
再缩小一点，请记住，这只是单个训练示例希

125
00:08:33,960 --> 00:08:37,520
望推动这些权重和偏差中的每一个的方式。

126
00:08:37,520 --> 00:08:41,400
如果我们只听 2 想要的内容，网络

127
00:08:41,400 --> 00:08:44,140
最终会被激励将所有图像分类为 2。

128
00:08:44,140 --> 00:08:49,500
因此，您要做的就是对每个其他训练示例执行

129
00:08:49,500 --> 00:08:54,700
相同的反向传播例程，记录每个示例如何更改

130
00:08:54,700 --> 00:09:02,300
权重和偏差，并将这些所需的更改一起平均。

131
00:09:02,300 --> 00:09:08,260
宽松地说，这里对每个权重和偏差的平均微

132
00:09:08,260 --> 00:09:12,340
调的集合是上一个视频中引用的成本函数的

133
00:09:12,340 --> 00:09:14,360
负梯度，或者至少是与之成比例的东西。

134
00:09:14,360 --> 00:09:18,980
我之所以说粗略地说，只是因为我还没有对这些推动进行

135
00:09:18,980 --> 00:09:23,480
量化精确，但如果你理解我刚才提到的每一个变化，为

136
00:09:23,480 --> 00:09:28,740
什么有些变化比其他变化更大，以及它们如何需要加在一

137
00:09:28,740 --> 00:09:34,100
起，你就会理解其中的机制反向传播实际上在做什么。

138
00:09:34,100 --> 00:09:38,540
顺便说一句，在实践中，计算机需要花费很长时间才

139
00:09:38,540 --> 00:09:43,120
能将每个梯度下降步骤的每个训练示例的影响相加。

140
00:09:43,120 --> 00:09:45,540
所以这就是通常所做的事情。

141
00:09:45,540 --> 00:09:50,460
您随机打乱训练数据并将其分成一大堆小批量，

142
00:09:50,460 --> 00:09:53,380
假设每个小批量都有 100 个训练示例。

143
00:09:53,380 --> 00:09:56,980
然后根据小批量计算步骤。

144
00:09:56,980 --> 00:10:00,840
它不是成本函数的实际梯度，它取决于所有训练数

145
00:10:00,840 --> 00:10:06,260
据，而不是这个微小的子集，所以它不是最有效的

146
00:10:06,260 --> 00:10:10,900
下坡步骤，但每个小批量确实给你一个非常好的近

147
00:10:10,900 --> 00:10:12,900
似值，更重要的是它为您带来显着的计算加速。

148
00:10:12,900 --> 00:10:16,900
如果你要在相关成本面下绘制网络的轨迹，那么它更

149
00:10:16,900 --> 00:10:22,020
像是一个醉汉漫无目的地跌跌撞撞地下山，但步伐很

150
00:10:22,020 --> 00:10:26,880
快，而不是一个仔细计算的人确定每一步的确切下坡

151
00:10:26,880 --> 00:10:31,620
方向然后朝着这个方向迈出非常缓慢而谨慎的一步。

152
00:10:31,620 --> 00:10:35,200
该技术称为随机梯度下降。

153
00:10:35,200 --> 00:10:40,400
这里发生了很多事情，所以让我们自己总结一下，好吗？

154
00:10:40,400 --> 00:10:45,480
反向传播是一种算法，用于确定单个训练示

155
00:10:45,480 --> 00:10:50,040
例如何推动权重和偏差，不仅在于它们是

156
00:10:50,040 --> 00:10:54,780
否应该上升或下降，还在于这些变化的相

157
00:10:54,780 --> 00:10:56,240
对比例导致权重和偏差最快下降。成本。

158
00:10:56,240 --> 00:11:00,720
真正的梯度下降步骤将涉及对所有数以万计的

159
00:11:00,720 --> 00:11:05,920
训练示例执行此操作，并对获得的所需变化

160
00:11:05,920 --> 00:11:11,680
进行平均，但这在计算上很慢，因此您可以

161
00:11:11,680 --> 00:11:14,000
将数据随机细分为小批量，并根据小批量。

162
00:11:14,000 --> 00:11:18,600
反复检查所有小批量并进行这些调整，您将

163
00:11:18,600 --> 00:11:23,420
收敛到成本函数的局部最小值，也就是说您

164
00:11:23,420 --> 00:11:27,540
的网络最终将在训练示例上做得非常好。

165
00:11:27,540 --> 00:11:32,600
综上所述，用于实现反向传播的每一行代码实际上都与您

166
00:11:32,600 --> 00:11:37,680
现在所看到的内容相对应，至少在非正式术语中是这样。

167
00:11:37,680 --> 00:11:41,900
但有时知道数学的作用只是成功的一半，仅仅

168
00:11:41,900 --> 00:11:44,780
代表该死的东西就会让一切变得混乱和混乱。

169
00:11:44,780 --> 00:11:49,360
因此，对于那些确实想要深入了解的人，下一个视频将

170
00:11:49,360 --> 00:11:53,400
介绍与此处刚刚介绍的相同的想法，但就基本微积分而

171
00:11:53,400 --> 00:11:57,460
言，这应该会让您在看到主题时更加熟悉。其他资源。

172
00:11:57,460 --> 00:12:01,220
在此之前，值得强调的一件事是，要使该算

173
00:12:01,220 --> 00:12:05,840
法发挥作用，并且这适用于神经网络之外的

174
00:12:05,840 --> 00:12:06,840
各种机器学习，您需要大量的训练数据。

175
00:12:06,840 --> 00:12:10,740
在我们的例子中，手写数字成为如此好的例子的原因之一是

176
00:12:10,740 --> 00:12:15,380
MNIST 数据库的存在，其中有很多由人类标记的例子。

177
00:12:15,380 --> 00:12:19,000
因此，从事机器学习工作的人所熟悉的一个常见挑战是

178
00:12:19,040 --> 00:12:22,880
获取实际需要的标记训练数据，无论是让人标记数以

179
00:12:22,880 --> 00:12:27,400
万计的图像，还是您可能处理的任何其他数据类型。

