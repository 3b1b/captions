1
00:00:04,060 --> 00:00:08,880
在这里，我们讨论反向传播，这是神经网络学习背后的核心算法。

2
00:00:09,400 --> 00:00:13,365
快速回顾一下我们的情况后，我要做的第一件事是直 

3
00:00:13,365 --> 00:00:17,000
观地演练算法实际在做什么，而不参考任何公式。

4
00:00:17,660 --> 00:00:20,206
然后，对于那些确实想深入研究数学的人，

5
00:00:20,206 --> 00:00:23,020
下 一个视频将介绍所有这一切背后的微积分。

6
00:00:23,820 --> 00:00:27,697
如果您观看了最后两个视频，或者只是了解了适当的背景 ，

7
00:00:27,697 --> 00:00:31,000
您就会知道什么是神经网络，以及它如何前馈信息。

8
00:00:31,680 --> 00:00:34,643
在这里，我们正在做识别手写数字的经典示例，

9
00:00:34,643 --> 00:00:38,595
其像素值被输入到具 有 784 个神经元的网络的第一层，

10
00:00:38,595 --> 00:00:41,700
并且我已经展示了一个具有 两个隐藏层的网络，

11
00:00:41,700 --> 00:00:43,959
每个隐藏层只有 16 个神经元，

12
00:00:43,959 --> 00:00:46,922
以及一个输 出由 10 个神经元组成的层，

13
00:00:46,922 --> 00:00:49,040
指示网络选择哪个数字作为答案。

14
00:00:50,040 --> 00:00:54,341
我还希望您理解梯度下降，如上一个视频中所 述，

15
00:00:54,341 --> 00:00:57,706
以及我们所说的学习的意思是我们想要 

16
00:00:57,706 --> 00:01:01,260
找到哪些权重和偏差最小化某个成本函数。

17
00:01:02,040 --> 00:01:05,889
快速提醒一下，对于单个训练示例的成本，

18
00:01:05,889 --> 00:01:11,156
您 可以获取网络提供的输出以及您希望其提供的 输出，

19
00:01:11,156 --> 00:01:14,600
并将每个组件之间的差异的平方相加。

20
00:01:15,380 --> 00:01:20,685
对所有数万个训练示例执行此操作并对结 果进行平均，

21
00:01:20,685 --> 00:01:23,020
即可得出网络的总成本。

22
00:01:23,020 --> 00:01:27,311
好像这还不够考虑，正如上一个视频中所描述 的，

23
00:01:27,311 --> 00:01:31,043
我们正在寻找的是这个成本函数的负梯度 ，

24
00:01:31,043 --> 00:01:36,267
它告诉你需要如何改变所有的权重和偏差， 所有的这些连接，

25
00:01:36,267 --> 00:01:38,320
从而最有效地降低成本。

26
00:01:43,260 --> 00:01:49,580
本视频的主题反向传播是一种用 于计算疯狂复杂梯度的算法。

27
00:01:49,580 --> 00:01:53,965
上一个视频中我真正希望您现在牢牢记住的一 个想法是，

28
00:01:53,965 --> 00:01:58,857
因为将梯度向量视为 13,00 0 维中的方向，简单地说，

29
00:01:58,857 --> 00:02:03,580
超出了我们的想 象范围，所以还有另一个想法你可以这样想。

30
00:02:04,600 --> 00:02:07,872
这里每个分量的大小告诉您成本函 

31
00:02:07,872 --> 00:02:10,940
数对每个权重和偏差的敏感程度。

32
00:02:11,800 --> 00:02:15,743
例如，假设您经历了我将要描述的过程，

33
00:02:15,743 --> 00:02:21,220
并计 算负梯度，与此边缘上的权重相关的分量为 3。

34
00:02:21,220 --> 00:02:26,260
2，而与此边相关的分量在这里显示为 0。1. 

35
00:02:26,820 --> 00:02:30,924
您的解释方式是，函数的成本对第一个权重的变化 

36
00:02:30,924 --> 00:02:35,743
敏感度是原来的 32 倍，因此，如果您稍微调 整该值，

37
00:02:35,743 --> 00:02:39,133
就会导致成本发生一些变化，而这种变化 

38
00:02:39,133 --> 00:02:43,060
比同样摆动第二个重量时产生的力大 32 倍。

39
00:02:48,420 --> 00:02:51,747
就我个人而言，当我第一次学习反向传播时，

40
00:02:51,747 --> 00:02:55,740
我认 为最令人困惑的方面就是它的符号和索引追逐。

41
00:02:56,220 --> 00:03:00,387
但是，一旦你解开这个算法的每个部分的真正 作用，

42
00:03:00,387 --> 00:03:04,208
它所产生的每个单独的效果实际上都 非常直观，

43
00:03:04,208 --> 00:03:06,640
只是有很多小的调整相互叠加。

44
00:03:07,740 --> 00:03:11,511
因此，我将从这里开始，完全忽略符号，

45
00:03:11,511 --> 00:03:16,120
并逐 步了解每个训练示例对权重和偏差的影响。

46
00:03:17,020 --> 00:03:21,896
由于成本函数涉及对所有数以万计的训练示例中每个 

47
00:03:21,896 --> 00:03:26,569
示例的特定成本进行平均，因此我们调整单个梯度 

48
00:03:26,569 --> 00:03:31,040
下降步骤的权重和偏差的方式也取决于每个示例。

49
00:03:31,680 --> 00:03:34,961
或者更确切地说，原则上应该如此，但为了计算效率，

50
00:03:34,961 --> 00:03:38,653
我们稍 后会做一些小技巧，以防止您需要为每个步骤击中每

51
00:03:38,653 --> 00:03:39,200
个示例。

52
00:03:39,200 --> 00:03:44,269
在其他情况下，现在我们要做的就是将注意力 集中在一个例子上，

53
00:03:44,269 --> 00:03:45,960
即这张 2 的图像。

54
00:03:46,720 --> 00:03:51,480
这一训练示例对权重和偏差的调整有何影响？

55
00:03:52,680 --> 00:03:55,786
假设我们正处于网络尚未经过良好训练的阶段，

56
00:03:55,786 --> 00:03:58,301
因此输出 中的激活看起来相当随机，

57
00:03:58,301 --> 00:04:02,000
可能类似于 0。5, 0.8, 0.2 、不断地。

58
00:04:02,520 --> 00:04:07,550
我们不能直接改变这些激活，我们只能 影响权重和偏差，

59
00:04:07,550 --> 00:04:12,580
但是跟踪我们希望对 该输出层进行哪些调整是有帮助的。

60
00:04:13,360 --> 00:04:16,232
由于我们希望它将图像分类为 2，

61
00:04:16,232 --> 00:04:21,260
因此我们希望 第三个值向上移动，而所有其他值都向下移动。

62
00:04:22,060 --> 00:04:29,520
此外，这些微调的大小应与每个当 前值与其目标值的距离成正比。

63
00:04:30,220 --> 00:04:35,560
例如，从某种意义上说，增加 2 号神 经元的激活比减少 

64
00:04:35,560 --> 00:04:40,900
8 号神经元的激活 更重要，后者已经非常接近应有的位置。

65
00:04:42,040 --> 00:04:45,642
因此，进一步放大，让我们只关注这 个神经元，

66
00:04:45,642 --> 00:04:47,280
我们希望增加其激活。

67
00:04:48,180 --> 00:04:53,286
请记住，激活被定义为前一层中所有激活的特定加 权总和，

68
00:04:53,286 --> 00:04:57,635
加上偏差，然后将其全部插入 sigm oid 

69
00:04:57,635 --> 00:05:01,040
压缩函数或 ReLU 之类的函数中。

70
00:05:01,640 --> 00:05:07,020
因此，可以通过三种不同的途 径联合起来帮助提高激活率。

71
00:05:07,440 --> 00:05:14,040
您可以增加偏差，可以增加权重 ，并且可以更改上一层的激活。

72
00:05:14,940 --> 00:05:17,995
重点关注如何调整权重，注意权重 

73
00:05:17,995 --> 00:05:20,860
实际上如何具有不同程度的影响。

74
00:05:21,440 --> 00:05:25,908
与前一层最亮神经元的连接具有最大的影 响，

75
00:05:25,908 --> 00:05:29,100
因为这些权重乘以较大的激活值。

76
00:05:31,460 --> 00:05:34,465
因此，如果您要增加其中一个权重，

77
00:05:34,465 --> 00:05:39,723
它实际上对 最终成本函数的影响比增加与较暗神经元的连接 

78
00:05:39,723 --> 00:05:43,480
权重更大，至少就这一训练示例而言是这样。

79
00:05:44,420 --> 00:05:47,500
请记住，当我们谈论梯度下降时，我们不仅仅 

80
00:05:47,500 --> 00:05:50,140
关心每个组件是否应该向上或向下推动，

81
00:05:50,140 --> 00:05:53,220
我 们关心哪些组件可以为您带来最大的收益。

82
00:05:55,020 --> 00:05:58,887
顺便说一句，这至少在某种程度上让人想起神经科学 

83
00:05:58,887 --> 00:06:02,592
中关于神经元生物网络如何学习的理论，赫布理论，

84
00:06:02,592 --> 00:06:06,460
 通常用短语来概括：一起放电的神经元连接在一起。

85
00:06:07,260 --> 00:06:10,808
在这里，权重的最大增加、连接的最 

86
00:06:10,808 --> 00:06:14,148
大加强发生在最活跃的神经元和我 

87
00:06:14,148 --> 00:06:17,280
们希望变得更活跃的神经元之间。

88
00:06:17,940 --> 00:06:21,358
从某种意义上说，看到 2 时放电的神经元与思 

89
00:06:21,358 --> 00:06:24,480
考 2 时放电的神经元之间的联系更加紧密。

90
00:06:25,400 --> 00:06:29,442
需要明确的是，我无法以某种方式对神经元的人 

91
00:06:29,442 --> 00:06:32,750
工网络是否表现得像生物大脑做出陈述，

92
00:06:32,750 --> 00:06:37,528
这种 将电线连接在一起的想法带有几个有意义的星号 ，

93
00:06:37,528 --> 00:06:41,020
但被视为非常松散的类比，我觉得很有趣。

94
00:06:41,940 --> 00:06:45,581
无论如何，我们可以帮助增加该神经元激活 

95
00:06:45,581 --> 00:06:49,040
的第三种方法是更改前一层中的所有激活。

96
00:06:49,040 --> 00:06:53,022
也就是说，如果与具有正权值的数字 2 神经元相连的 

97
00:06:53,022 --> 00:06:57,463
所有东西都变得更亮，而与负权值连接的所有东西都变得 更暗，

98
00:06:57,463 --> 00:07:00,680
那么那个数字 2 神经元就会变得更加活跃。

99
00:07:02,540 --> 00:07:08,188
与权重变化类似，通过寻求与相应权重大 小成比例的变化，

100
00:07:08,188 --> 00:07:10,280
您将获得最大的收益。

101
00:07:12,140 --> 00:07:15,412
当然，现在我们不能直接影响这些 激活，

102
00:07:15,412 --> 00:07:17,480
我们只能控制权重和偏差。

103
00:07:17,480 --> 00:07:24,120
但就像最后一层一样，记下这 些所需的更改会很有帮助。

104
00:07:24,580 --> 00:07:29,200
但请记住，这里缩小一步，这只是 数字 2 输出神经元想要的。

105
00:07:29,760 --> 00:07:33,142
请记住，我们还希望最后一层中的所有其他神经 

106
00:07:33,142 --> 00:07:36,525
元变得不那么活跃，并且每个其他输出神经元对 

107
00:07:36,525 --> 00:07:39,600
于倒数第二层应该发生什么都有自己的想法。

108
00:07:42,700 --> 00:07:47,262
因此，这个数字 2 神经元的愿望与所有 

109
00:07:47,262 --> 00:07:53,876
其他输出神经元的愿望相加，以决定倒数第 二层应该发生什么，

110
00:07:53,876 --> 00:08:00,720
同样与相应的权重成比 例，并与每个神经元需要多少成比例改变。

111
00:08:01,600 --> 00:08:05,480
这就是向后传播的想法出现的地方。

112
00:08:05,820 --> 00:08:08,836
通过将所有这些所需的效果添加在一起，

113
00:08:08,836 --> 00:08:13,360
您基本上 会得到一个您想要在倒数第二层发生的微调列表。

114
00:08:14,220 --> 00:08:17,965
一旦有了这些，您就可以递归地将相同的过程 

115
00:08:17,965 --> 00:08:20,997
应用于确定这些值的相关权重和偏差，

116
00:08:20,997 --> 00:08:25,100
重复我 刚刚走过的相同过程并在网络中向后移动。

117
00:08:28,960 --> 00:08:33,181
再缩小一点，请记住，这只是单个训练示例希 

118
00:08:33,181 --> 00:08:37,000
望推动这些权重和偏差中的每一个的方式。

119
00:08:37,480 --> 00:08:40,432
如果我们只听 2 想要的内容，网络 

120
00:08:40,432 --> 00:08:43,220
最终会被激励将所有图像分类为 2。

121
00:08:44,059 --> 00:08:48,104
因此，您要做的就是对每个其他训练示例执行 

122
00:08:48,104 --> 00:08:53,303
相同的反向传播例程，记录每个示例如何更改 权重和偏差，

123
00:08:53,303 --> 00:08:56,000
并将这些所需的更改一起平均。

124
00:09:01,720 --> 00:09:05,844
宽松地说，这里对每个权重和偏差的平均微 

125
00:09:05,844 --> 00:09:10,793
调的集合是上一个视频中引用的成本函数的 负梯度，

126
00:09:10,793 --> 00:09:13,680
或者至少是与之成比例的东西。

127
00:09:14,380 --> 00:09:18,658
我之所以说粗略地说，只是因为我还没有对这些推动进行 

128
00:09:18,658 --> 00:09:22,443
量化精确，但如果你理解我刚才提到的每一个变化，

129
00:09:22,443 --> 00:09:27,379
为 什么有些变化比其他变化更大，以及它们如何需要加在一 起，

130
00:09:27,379 --> 00:09:31,000
你就会理解其中的机制反向传播实际上在做什么。

131
00:09:33,960 --> 00:09:38,290
顺便说一句，在实践中，计算机需要花费很长时间才 

132
00:09:38,290 --> 00:09:42,440
能将每个梯度下降步骤的每个训练示例的影响相加。

133
00:09:43,140 --> 00:09:44,820
所以这就是通常所做的事情。

134
00:09:45,480 --> 00:09:48,950
您随机打乱训练数据并将其分成一大堆小批量，

135
00:09:48,950 --> 00:09:52,420
 假设每个小批量都有 100 个训练示例。

136
00:09:52,939 --> 00:09:57,280
然后根据小批量计算步骤。

137
00:09:57,280 --> 00:10:01,402
它不是成本函数的实际梯度，它取决于所有训练数 据，

138
00:10:01,402 --> 00:10:05,689
而不是这个微小的子集，所以它不是最有效的 下坡步骤，

139
00:10:05,689 --> 00:10:09,152
但每个小批量确实给你一个非常好的近 似值，

140
00:10:09,152 --> 00:10:12,120
更重要的是它为您带来显着的计算加速。

141
00:10:12,820 --> 00:10:16,288
如果你要在相关成本面下绘制网络的轨迹，

142
00:10:16,288 --> 00:10:20,668
那么它更 像是一个醉汉漫无目的地跌跌撞撞地下山，

143
00:10:20,668 --> 00:10:25,961
但步伐很 快，而不是一个仔细计算的人确定每一步的确切下坡 

144
00:10:25,961 --> 00:10:30,160
方向然后朝着这个方向迈出非常缓慢而谨慎的一步。

145
00:10:31,540 --> 00:10:34,660
该技术称为随机梯度下降。

146
00:10:35,960 --> 00:10:39,620
这里发生了很多事情，所以让我们自己总结一下，好吗？

147
00:10:40,440 --> 00:10:44,418
反向传播是一种算法，用于确定单个训练示 

148
00:10:44,418 --> 00:10:49,989
例如何推动权重和偏差，不仅在于它们是 否应该上升或下降，

149
00:10:49,989 --> 00:10:55,560
还在于这些变化的相 对比例导致权重和偏差最快下降。成本。

150
00:10:56,260 --> 00:11:00,717
真正的梯度下降步骤将涉及对所有数以万计的 

151
00:11:00,717 --> 00:11:06,023
训练示例执行此操作，并对获得的所需变化 进行平均，

152
00:11:06,023 --> 00:11:11,754
但这在计算上很慢，因此您可以 将数据随机细分为小批量，

153
00:11:11,754 --> 00:11:13,240
并根据小批量。

154
00:11:14,000 --> 00:11:17,382
反复检查所有小批量并进行这些调整，

155
00:11:17,382 --> 00:11:20,764
您将 收敛到成本函数的局部最小值，

156
00:11:20,764 --> 00:11:25,540
也就是说您 的网络最终将在训练示例上做得非常好。

157
00:11:27,240 --> 00:11:32,072
综上所述，用于实现反向传播的每一行代码实际上都与您 

158
00:11:32,072 --> 00:11:36,720
现在所看到的内容相对应，至少在非正式术语中是这样。

159
00:11:37,560 --> 00:11:40,440
但有时知道数学的作用只是成功的一半，

160
00:11:40,440 --> 00:11:44,120
仅仅 代表该死的东西就会让一切变得混乱和混乱。

161
00:11:44,860 --> 00:11:47,671
因此，对于那些确实想要深入了解的人，

162
00:11:47,671 --> 00:11:51,264
下一个视频将 介绍与此处刚刚介绍的相同的想法，

163
00:11:51,264 --> 00:11:55,638
但就基本微积分而 言，这应该会让您在看到主题时更加熟悉。

164
00:11:55,638 --> 00:11:56,420
其他资源。

165
00:11:57,340 --> 00:12:01,177
在此之前，值得强调的一件事是，要使该算 法发挥作用，

166
00:12:01,177 --> 00:12:04,276
并且这适用于神经网络之外的 各种机器学习，

167
00:12:04,276 --> 00:12:05,900
您需要大量的训练数据。

168
00:12:06,420 --> 00:12:10,579
在我们的例子中，手写数字成为如此好的例子的原因之一是 

169
00:12:10,579 --> 00:12:14,740
MNIST 数据库的存在，其中有很多由人类标记的例子。

170
00:12:15,300 --> 00:12:19,397
因此，从事机器学习工作的人所熟悉的一个常见挑战是 

171
00:12:19,397 --> 00:12:24,313
获取实际需要的标记训练数据，无论是让人标记数以 万计的图像，

172
00:12:24,313 --> 00:12:27,100
还是您可能处理的任何其他数据类型。

