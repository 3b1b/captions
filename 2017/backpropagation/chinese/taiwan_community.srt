1
00:00:04,350 --> 00:00:06,410
本期我們來講反向傳播

2
00:00:06,410 --> 00:00:09,400
也就是神經網絡學習的核心算法

3
00:00:09,400 --> 00:00:11,210
稍微回顧一下我們之前講到哪裡之後

4
00:00:11,210 --> 00:00:15,470
首先我要撇開公式不提 直觀地過一遍

5
00:00:15,470 --> 00:00:17,270
這個演算法到底在做什麼

6
00:00:17,640 --> 00:00:20,310
然後如果你們有人想認真看裡頭的數學

7
00:00:20,310 --> 00:00:23,140
下一期影片我會解釋這一切背後的微積分

8
00:00:23,940 --> 00:00:25,550
如果你看了前兩期影片

9
00:00:25,550 --> 00:00:27,920
或者你已經有足夠背景知識 直接空降來這一期影片的話

10
00:00:27,920 --> 00:00:31,290
你一定知道神經網絡是什麼 以及它如何前饋信息的

11
00:00:31,660 --> 00:00:35,100
這裡我們考慮的經典例子就是手寫數字識別

12
00:00:35,100 --> 00:00:39,930
數字的像素值被輸入到網絡第一層的784個神經元裡

13
00:00:39,930 --> 00:00:44,000
這裡 我展示的是有2層16個神經元隱含層

14
00:00:44,000 --> 00:00:49,250
10個神經元的輸出層 代表網絡最終給出的選擇

15
00:00:50,020 --> 00:00:54,340
我也假設你們已經理解了上期說到的梯度下降法

16
00:00:54,340 --> 00:00:56,890
理解了所謂學習就是指

17
00:00:56,890 --> 00:01:01,450
我們要找到特定的權重偏置 從而使一個代價函數最小化

18
00:01:02,010 --> 00:01:05,470
稍許提醒一下 計算一個訓練樣本的代價

19
00:01:05,470 --> 00:01:08,400
你需要求出網絡的輸出

20
00:01:08,400 --> 00:01:10,850
與期待的輸出

21
00:01:11,200 --> 00:01:14,820
之間每一項的差的平方和

22
00:01:15,370 --> 00:01:20,020
然後對於成千上萬個訓練樣本都這麼算一遍 最後取平均

23
00:01:20,020 --> 00:01:22,410
這就得到了整個網絡的代價值

24
00:01:22,910 --> 00:01:26,010
如果你嫌這還不夠複雜的話 上集內容也講到了

25
00:01:26,010 --> 00:01:30,870
我們要求的是代價函數的負梯度

26
00:01:30,870 --> 00:01:35,720
它告訴你如何改變所有連線上的權重偏置

27
00:01:35,720 --> 00:01:38,270
才好讓代價下降得最快

28
00:01:42,950 --> 00:01:45,210
本集的核心 反向傳播算法

29
00:01:45,210 --> 00:01:48,800
正是用來求這個複雜到爆的梯度的

30
00:01:49,490 --> 00:01:54,010
我希望大家能夠把上集中提到的一點牢牢記住

31
00:01:54,010 --> 00:01:58,910
畢竟13000維的梯度向量

32
00:01:58,910 --> 00:02:02,090
說它是難以想像都不為過

33
00:02:02,090 --> 00:02:03,510
所以這裡大家請記住另一套思路

34
00:02:04,580 --> 00:02:07,710
梯度向量每一項的大小是在告訴大家

35
00:02:07,710 --> 00:02:11,140
代價函數對於每個參數有多敏感

36
00:02:11,810 --> 00:02:14,580
比如說 你走了一段我講的過程

37
00:02:14,580 --> 00:02:16,370
計算了負梯度

38
00:02:16,370 --> 00:02:21,470
對應這條線上這個權重的一項等於3.2

39
00:02:21,870 --> 00:02:26,370
而對應這條邊上的一項等於0.1

40
00:02:26,910 --> 00:02:28,420
你可以這麼來理解

41
00:02:28,420 --> 00:02:33,080
第一個權重對代價函數的值有32倍的影響力

42
00:02:33,640 --> 00:02:35,930
如果你稍微改變一下第一個權重

43
00:02:35,930 --> 00:02:38,190
它對代價值造成的變化

44
00:02:38,190 --> 00:02:43,200
就是改變第二個權重同等大小下的32倍

45
00:02:48,520 --> 00:02:51,440
就我個人而言 我剛開始學習反向傳播的時候

46
00:02:51,440 --> 00:02:55,740
我覺得最容易搞混的部分就是各種符號和上標下標

47
00:02:56,180 --> 00:02:59,450
不過 一旦你釐清了算法的思路

48
00:02:59,450 --> 00:03:02,870
算法的每一步其實都挺直觀的

49
00:03:03,180 --> 00:03:06,740
其實就是把許許多多微小的調整一層進一層地進行下去而已

50
00:03:07,660 --> 00:03:11,290
所以 開始講解時 我將完全拋棄所有的符號

51
00:03:11,290 --> 00:03:13,370
給大家一步步解釋

52
00:03:13,370 --> 00:03:16,350
每一個訓練樣本會對權重偏置的調整造成怎樣的影響

53
00:03:17,090 --> 00:03:18,590
因為代價函數牽扯到

54
00:03:18,590 --> 00:03:23,640
對成千上萬個訓練樣本的代價取平均值

55
00:03:23,970 --> 00:03:28,640
所以我們調整每一步梯度下降用的權重偏置

56
00:03:28,640 --> 00:03:31,140
也會基於所有的訓練樣本

57
00:03:31,680 --> 00:03:33,200
原理上是這麼說

58
00:03:33,200 --> 00:03:35,930
但為了計算效率 之後咱們會討個巧

59
00:03:35,930 --> 00:03:39,370
從而不必每一步都非得要計算所有的訓練樣本

60
00:03:39,790 --> 00:03:41,330
還需要說明一點

61
00:03:41,330 --> 00:03:46,160
我們現在只關註一個訓練樣本 就這張2

62
00:03:46,670 --> 00:03:51,650
這一個訓練樣本會對調整權重和偏置造成怎樣的影響呢?

63
00:03:52,680 --> 00:03:55,240
現在假設網絡還沒有完全訓練好

64
00:03:55,240 --> 00:03:57,970
那麼輸出層的激活值看起來就很隨機

65
00:03:57,970 --> 00:04:02,040
也許就會出現0.5、0.8、0.2 等等等等

66
00:04:02,640 --> 00:04:07,450
我們並不能直接改動這些激活值 只能改變權重和偏置值

67
00:04:07,790 --> 00:04:12,670
但記住我們想要輸出層出現怎樣的變動 還是很有用的

68
00:04:13,270 --> 00:04:15,710
因為我們希望圖像最終的分類結果是2

69
00:04:16,040 --> 00:04:21,360
我們希望第三個輸出值變大 其他數值變小

70
00:04:22,040 --> 00:04:26,020
並且變動的大小應該與現在值和目標值之間的差呈正比

71
00:04:26,020 --> 00:04:29,630
並且變動的大小應該與現在值和目標值之間的差呈正比

72
00:04:30,220 --> 00:04:34,350
舉個例子 增加數字”2”神經元的激活值

73
00:04:34,350 --> 00:04:38,490
就應該比減少數字”8”神經元的激活值來得重要

74
00:04:38,490 --> 00:04:40,630
因為後者已經很接近它的目標了

75
00:04:41,990 --> 00:04:45,250
那好 我們更進一步 就來關注下這一個神經元

76
00:04:45,250 --> 00:04:47,530
我們要讓這裡面的激活值變大

77
00:04:48,160 --> 00:04:50,550
還記得這個激活值是

78
00:04:50,550 --> 00:04:56,430
把前一層所有激活值的加權和 加上一個偏置

79
00:04:56,430 --> 00:05:01,290
再通過sigmoid ReLU之類的擠壓函數 最後算出來的吧

80
00:05:01,810 --> 00:05:07,360
所以要增加這個激活值 我們有三條大路可走

81
00:05:07,680 --> 00:05:10,970
一增加偏置 二增加權重

82
00:05:10,970 --> 00:05:14,030
或者三改變上一層的激活值

83
00:05:14,950 --> 00:05:17,770
先來看如何調整權重

84
00:05:17,770 --> 00:05:21,410
各個權重它們的影響力各不相同

85
00:05:21,410 --> 00:05:25,750
連接前一層最亮的神經元的權重 影響力也最大

86
00:05:25,750 --> 00:05:29,240
因為這些權重會與大的激活值相乘

87
00:05:31,330 --> 00:05:33,480
所以至少對於這一個訓練樣本而言

88
00:05:33,480 --> 00:05:37,370
增大了這幾個權重值 對最終代價函數造成的影響

89
00:05:37,370 --> 00:05:40,820
就比增大連接黯淡神經元的權重所造成的影響

90
00:05:40,820 --> 00:05:43,650
要大上好多倍

91
00:05:44,380 --> 00:05:46,890
請記住當我們說到梯度下降的時候

92
00:05:46,890 --> 00:05:50,620
我們並不只看每個參數是該增大還是減小

93
00:05:50,620 --> 00:05:53,370
我們還看該哪個參數的性價比最高

94
00:05:55,270 --> 00:05:59,310
順便一提 這有一點點像描述生物中

95
00:05:59,310 --> 00:06:01,870
神經元的網絡如何學習的一個理論

96
00:06:01,870 --> 00:06:06,820
“赫布理論” 總結起來就是“一同激活的神經元關聯在一起”

97
00:06:07,260 --> 00:06:12,200
這裡 權重的最大增長 即連接變得更強的部分

98
00:06:12,200 --> 00:06:14,840
就會發生在已經最活躍的神經元

99
00:06:14,840 --> 00:06:17,590
和想要更多激發的神經元之間

100
00:06:18,020 --> 00:06:21,060
可以說 看見一個"2"時激發的神經元

101
00:06:21,060 --> 00:06:24,680
會和”想到一個2”時激發的神經元聯繫地更緊密

102
00:06:25,420 --> 00:06:28,780
這裡解釋一下 我個人對人工神經網絡是否真的在

103
00:06:28,780 --> 00:06:33,080
模仿生物學上大腦的工作 沒有什麼發言權

104
00:06:33,080 --> 00:06:37,250
“一同激活的神經元關聯在一起”這句話是要打星號註釋的

105
00:06:37,250 --> 00:06:41,260
但作為一個粗略的對照 我覺得還是挺有意思的

106
00:06:41,890 --> 00:06:46,020
言歸正傳 第三個能夠增加這個神經元激活值的方法

107
00:06:46,020 --> 00:06:49,060
就是改變前一層的激活值

108
00:06:49,560 --> 00:06:54,970
更具體地說 如果所有正權重連接的神經元更亮

109
00:06:54,970 --> 00:06:57,960
所有負權重連接的神經元更暗的話

110
00:06:58,340 --> 00:07:00,890
那麼數字2的神經元就會更強烈地激發

111
00:07:02,450 --> 00:07:06,130
和改權重的時候類似 我們想造成更大的影響

112
00:07:06,130 --> 00:07:10,550
就要依據對應權重的大小 對激活值做出呈比例的改變

113
00:07:12,120 --> 00:07:15,360
當然 我們並不能直接改變激活值

114
00:07:15,360 --> 00:07:17,780
我們手頭只能控制權重和偏置

115
00:07:18,220 --> 00:07:23,610
但就光對最後一層來說 記住我們期待的變化還是很有幫助的

116
00:07:24,450 --> 00:07:29,720
不過別忘了 從全局上看 這只不過是數字2的神經元所期待的變化

117
00:07:29,720 --> 00:07:34,840
我們還需要最後一層其餘的神經元的激發變弱

118
00:07:34,840 --> 00:07:36,500
但這其餘的每個輸出神經元

119
00:07:36,500 --> 00:07:39,840
對於如何改變倒數第二層 都有各自的想法

120
00:07:43,110 --> 00:07:46,140
所以 我們會把數字2神經元的期待

121
00:07:46,140 --> 00:07:50,520
和別的輸出神經元的期待全部加起來

122
00:07:50,520 --> 00:07:53,240
作為對如何改變倒數第二層神經元的指示

123
00:07:53,580 --> 00:07:56,400
這些期待變化不僅是對應的權重的倍數

124
00:07:56,400 --> 00:08:00,910
也是每個神經元激活值改變量的倍數

125
00:08:01,480 --> 00:08:05,510
這其實就是在實現”反向傳播”的理念了

126
00:08:05,960 --> 00:08:08,730
我們把所有期待的改變加起來

127
00:08:08,730 --> 00:08:13,560
就得到了一串對倒數第二層改動的變化量

128
00:08:14,180 --> 00:08:15,390
有了這些

129
00:08:15,390 --> 00:08:17,850
我們就可以重複這個過程

130
00:08:17,850 --> 00:08:21,180
改變影響倒數第二層神經元激活值的相關參數

131
00:08:21,180 --> 00:08:25,140
從後一層到前一層 把這個過程一直循環到第一層

132
00:08:29,030 --> 00:08:30,370
著眼大局

133
00:08:30,370 --> 00:08:31,920
還記得我們只是在討論

134
00:08:31,920 --> 00:08:37,400
單個訓練樣本對所有權重偏置的影響嗎？

135
00:08:37,400 --> 00:08:39,700
如果我們只關注那個“2”的要求

136
00:08:39,700 --> 00:08:43,400
最後 網絡只會把所有圖像都分類成是“2”

137
00:08:44,030 --> 00:08:49,420
所以你要對其他所有的訓練樣本 同樣地過一遍反向傳播

138
00:08:49,420 --> 00:08:53,200
記錄下每個樣本想怎樣修改權重與偏置

139
00:08:53,650 --> 00:08:56,220
最後再取一個平均值

140
00:09:02,050 --> 00:09:06,940
這裡一系列的權重偏置的平均微調大小

141
00:09:06,940 --> 00:09:11,910
不嚴格地說 就是上期影片提到的代價函數的負梯度

142
00:09:11,910 --> 00:09:13,740
至少是其標量的倍數

143
00:09:14,360 --> 00:09:19,570
這裡的不嚴格 指的是我還沒有準確地解釋如何量化這些微調

144
00:09:19,570 --> 00:09:22,190
但如果你清楚我提到的所有改動

145
00:09:22,190 --> 00:09:24,770
為什麼有些數字是其他數字的好幾倍

146
00:09:24,770 --> 00:09:27,160
以及最後要怎麼全部加起來

147
00:09:27,160 --> 00:09:31,170
你就懂得了反向傳播的真實工作原理

148
00:09:34,050 --> 00:09:37,400
順帶一提 實際操作中 如果梯度下降的每一步

149
00:09:37,400 --> 00:09:42,490
都用上每一個訓練樣本來計算的話 那麼花的時間就太長了

150
00:09:43,010 --> 00:09:44,960
所以我們一般會這麼做

151
00:09:45,440 --> 00:09:50,280
首先把訓練樣本打亂 然後分成很多組minibatch

152
00:09:50,280 --> 00:09:52,680
每個minibatch就當包含100個訓練樣本好了

153
00:09:53,240 --> 00:09:56,430
然後你算出這個minibatch下降的一步

154
00:09:56,850 --> 00:09:59,390
這不是代價函數真正的梯度

155
00:09:59,390 --> 00:10:02,630
畢竟計算真實梯度得用上所有的樣本 而非這個子集

156
00:10:03,100 --> 00:10:05,640
所以這也不是下山最高效的一步

157
00:10:06,080 --> 00:10:08,970
然而 每個minibatch都會給你一個不錯的近似

158
00:10:08,970 --> 00:10:12,250
而且更重要的是 你的計算量會減輕不少

159
00:10:12,820 --> 00:10:16,810
你如果想把網絡沿代價函數的表面下山的路徑畫出來的話

160
00:10:16,810 --> 00:10:22,030
它看上去會有點像醉漢漫無目的地遛下山 但起碼步伐很快

161
00:10:22,030 --> 00:10:27,180
而不像是細緻入微的人 踏步之前先準確地算好下坡的方向

162
00:10:27,180 --> 00:10:30,350
然後再向那個方向謹小慎微地慢慢走一步

163
00:10:31,460 --> 00:10:34,940
這個技巧就叫做“隨機梯度下降”

164
00:10:36,000 --> 00:10:39,800
內容挺多的 我們先小結一下好不好

165
00:10:40,240 --> 00:10:42,270
反向傳播算法算的是

166
00:10:42,270 --> 00:10:47,370
單個訓練樣本想怎樣修改權重與偏置

167
00:10:47,370 --> 00:10:49,930
不僅是說每個參數應該變大還是變小

168
00:10:49,930 --> 00:10:55,700
還包括了這些變化的比例是多大 才能最快地降低代價

169
00:10:56,240 --> 00:10:58,270
真正的梯度下降

170
00:10:58,270 --> 00:11:01,820
得對好幾萬個訓練範例都這麼操作

171
00:11:01,820 --> 00:11:04,260
然後對這些變化值取平均

172
00:11:04,830 --> 00:11:06,340
但算起來太慢了

173
00:11:06,690 --> 00:11:10,480
所以你會先把所有的樣本分到各個minibatch中去

174
00:11:10,480 --> 00:11:13,460
計算一個minibatch來作為梯度下降的一步

175
00:11:13,900 --> 00:11:17,690
計算每個minibatch的梯度 調整參數 不斷循環

176
00:11:17,690 --> 00:11:21,050
最終你就會收斂到代價函數的一個局部最小值上

177
00:11:21,430 --> 00:11:25,740
此時就可以說 你的神經網絡對付訓練數據已經很不錯了

178
00:11:27,450 --> 00:11:32,290
總而言之 我們實現反向傳播算法的每一句代碼

179
00:11:32,290 --> 00:11:36,970
其實或多或少地都對應了大家已經知道的內容

180
00:11:37,570 --> 00:11:40,960
但有時 了解其中的數學原理只不過是完成了一半

181
00:11:40,960 --> 00:11:44,460
如何把這破玩意兒表示出來又會搞得人一頭霧水

182
00:11:44,930 --> 00:11:47,620
那麼 在座的如果想深入探討的話

183
00:11:47,620 --> 00:11:50,670
下一期影片中我們會把本期的內容用微積分的形式呈現出來

184
00:11:50,670 --> 00:11:52,750
下一期影片中我們會把本期的內容用微積分的形式呈現出來

185
00:11:52,750 --> 00:11:56,760
希望看過以後再看其他資料時會更容易接受一些吧

186
00:11:57,210 --> 00:11:59,440
收尾之前 我想著重提一點

187
00:11:59,440 --> 00:12:04,320
反向傳播算法在內 所有包括神經網絡在內的機器學習 要讓它們工作

188
00:12:04,320 --> 00:12:06,120
咱需要一大坨的訓練數據

189
00:12:06,430 --> 00:12:09,860
我們用的手寫數字的範例之所以那麼方便

190
00:12:09,860 --> 00:12:12,110
是因為存在著一個MNIST數據庫

191
00:12:12,110 --> 00:12:15,290
裡面所有的樣本都已經人為標記好了

192
00:12:15,290 --> 00:12:19,000
所以機器學習領域的人 最熟悉的一個難關

193
00:12:19,000 --> 00:12:21,930
莫過於獲取標記好的訓練數據了

194
00:12:22,240 --> 00:12:25,080
不管是叫別人標記成千上萬個圖像

195
00:12:25,080 --> 00:12:27,550
還是去標記別的類型的數據也罷

