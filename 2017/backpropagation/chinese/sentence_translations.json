[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn. ",
  "translatedText": "在这里，我们讨论反向传播，这是神经网络学习背后的核心算法。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. ",
  "translatedText": "快速回顾一下我们的情况后，我要做的第一件事是直 观地演练算法实际在做什么，而不参考任何公式。",
  "model": "google_nmt",
  "from_community_srt": "本期我们来讲反向传播 也就是神经网络学习的核心算法 稍微回顾一下我们之前讲到哪里之后 首先我要撇开公式不提 直观地过一遍 这个算法到底在做什么 然后如果你们有人想认真看里头的数学 下一期视频我会解释这一切背后的微积分",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. ",
  "translatedText": "然后，对于那些确实想深入研究数学的人，下 一个视频将介绍所有这一切背后的微积分。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. ",
  "translatedText": "如果您观看了最后两个视频，或者只是了解了适当的背景 ，您就会知道什么是神经网络，以及它如何前馈信息。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. ",
  "translatedText": "在这里，我们正在做识别手写数字的经典示例，其像素值被输入到具 有 784 个神经元的网络的第一层，并且我已经展示了一个具有 两个隐藏层的网络，每个隐藏层只有 16 个神经元，以及一个输 出由 10 个神经元组成的层，指示网络选择哪个数字作为答案。",
  "model": "google_nmt",
  "from_community_srt": "如果你看了前两期视频 或者你已经有足够背景  直接空降来这一期视频的话 你一定知道神经网络是什么  以及它如何前馈信息的 这里我们考虑的经典例子就是手写数字识别 数字的像素值被输入到网络第一层的784个神经元里",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. ",
  "translatedText": "我还希望您理解梯度下降，如上一个视频中所 述，以及我们所说的学习的意思是我们想要 找到哪些权重和偏差最小化某个成本函数。",
  "model": "google_nmt",
  "from_community_srt": "这里 我展示的是有2层16个神经元隐含层 10个神经元的输出层 代表网络最终给出的选择 我也假设你们已经理解了上期说到的梯度下降法 理解了所谓学习就是指 我们要找到特定的权重偏置 从而使一个代价函数最小化",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. ",
  "translatedText": "快速提醒一下，对于单个训练示例的成本，您 可以获取网络提供的输出以及您希望其提供的 输出，并将每个组件之间的差异的平方相加。",
  "model": "google_nmt",
  "from_community_srt": "稍许提醒一下 计算一个训练样本的代价 你需要求出网络的输出 与期待的输出 之间每一项的差的平方和 然后对于成千上万个训练样本都这么算一遍 最后取平均 这就得到了整个网络的代价值 如果你嫌这还不够复杂的话 上集内容也讲到了",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. ",
  "translatedText": "对所有数万个训练示例执行此操作并对结 果进行平均，即可得出网络的总成本。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. ",
  "translatedText": "好像这还不够考虑，正如上一个视频中所描述 的，我们正在寻找的是这个成本函数的负梯度 ，它告诉你需要如何改变所有的权重和偏差， 所有的这些连接，从而最有效地降低成本。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. ",
  "translatedText": "本视频的主题反向传播是一种用 于计算疯狂复杂梯度的算法。",
  "model": "google_nmt",
  "from_community_srt": "我们要求的是代价函数的负梯度 它告诉你如何改变所有连线上的权重偏置 才好让代价下降得最快 本集的中心 反向传播算法 正是用来求这个复杂到爆的梯度的 我希望大家能够把上集中提到的一点牢牢记住",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "The one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. ",
  "translatedText": "上一个视频中我真正希望您现在牢牢记住的一 个想法是，因为将梯度向量视为 13,00 0 维中的方向，简单地说，超出了我们的想 象范围，所以还有另一个想法你可以这样想。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. ",
  "translatedText": "这里每个分量的大小告诉您成本函 数对每个权重和偏差的敏感程度。",
  "model": "google_nmt",
  "from_community_srt": "毕竟13000维的梯度向量 说它是难以想象都不为过 所以这里大家请记住另一套思路 梯度向量每一项的大小是在告诉大家 代价函数对于每个参数有多敏感 比如说 你走了一段我讲的过程 计算了负梯度",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. ",
  "translatedText": "例如，假设您经历了我将要描述的过程，并计 算负梯度，与此边缘上的权重相关的分量为 3。2，而与此边相关的分量在这里显示为 0。1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. ",
  "translatedText": "您的解释方式是，函数的成本对第一个权重的变化 敏感度是原来的 32 倍，因此，如果您稍微调 整该值，就会导致成本发生一些变化，而这种变化 比同样摆动第二个重量时产生的力大 32 倍。",
  "model": "google_nmt",
  "from_community_srt": "对应这条线上这个权重的一项等于3.2 而对应这条边上的一项等于0.1 你可以这么来理解 第一个权重对代价函数的值有32倍的影响力 如果你稍微改变一下第一个权重 它对代价值造成的变化 就是改变第二个权重同等大小下的32倍",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and index chasing of it all. ",
  "translatedText": "就我个人而言，当我第一次学习反向传播时，我认 为最令人困惑的方面就是它的符号和索引追逐。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. ",
  "translatedText": "但是，一旦你解开这个算法的每个部分的真正 作用，它所产生的每个单独的效果实际上都 非常直观，只是有很多小的调整相互叠加。",
  "model": "google_nmt",
  "from_community_srt": "就我个人而言 我刚开始学习反向传播的时候 我觉得最容易搞混的部分就是各种符号和上标下标 不过 一旦你捋清了算法的思路 算法的每一步其实都挺直观的 其实就是把许许多多微小的调整一层进一层地进行下去而已",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. ",
  "translatedText": "因此，我将从这里开始，完全忽略符号，并逐 步了解每个训练示例对权重和偏差的影响。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. ",
  "translatedText": "由于成本函数涉及对所有数以万计的训练示例中每个 示例的特定成本进行平均，因此我们调整单个梯度 下降步骤的权重和偏差的方式也取决于每个示例。",
  "model": "google_nmt",
  "from_community_srt": "所以 开始讲解时 我将完全抛弃所有的符号 给大家一步步解释 每一个训练样本会对权重偏置的调整造成怎样的影响 因为代价函数牵扯到 对成千上万个训练样本的代价取平均值 所以我们调整每一步梯度下降用的权重偏置",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. ",
  "translatedText": "或者更确切地说，原则上应该如此，但为了计算效率，我们稍 后会做一些小技巧，以防止您需要为每个步骤击中每个示例。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. ",
  "translatedText": "在其他情况下，现在我们要做的就是将注意力 集中在一个例子上，即这张 2 的图像。",
  "model": "google_nmt",
  "from_community_srt": "也会基于所有的训练样本 原理上是这么说 但为了计算效率 之后咱们会讨个巧 从而不必每一步都非得要计算所有的训练样本 还需要说明一点 我们现在只关注一个训练样本 就这张2 这一个训练样本会对调整权重和偏置造成怎样的影响呢?",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted? ",
  "translatedText": "这一训练示例对权重和偏差的调整有何影响？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. ",
  "translatedText": "假设我们正处于网络尚未经过良好训练的阶段，因此输出 中的激活看起来相当随机，可能类似于 0。5, 0.8, 0.2 、不断地。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases, but it's helpful to keep track of which adjustments we wish should take place to that output layer. ",
  "translatedText": "我们不能直接改变这些激活，我们只能 影响权重和偏差，但是跟踪我们希望对 该输出层进行哪些调整是有帮助的。",
  "model": "google_nmt",
  "from_community_srt": "现在假设网络还没有完全训练好 那么输出层的激活值看起来就很随机 也许就会出现0.5 0.8 0.2 等等等等 我们并不能直接改动这些激活值  只能改变权重和偏置值 但记住我们想要输出层出现怎样的变动  还是很有用的",
  "n_reviews": 0,
  "start": 242.52,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. ",
  "translatedText": "由于我们希望它将图像分类为 2，因此我们希望 第三个值向上移动，而所有其他值都向下移动。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. ",
  "translatedText": "此外，这些微调的大小应与每个当 前值与其目标值的距离成正比。",
  "model": "google_nmt",
  "from_community_srt": "因为我们希望图像最终的分类结果是2 我们希望第三个输出值变大  其他数值变小 并且变动的大小应该与现在值和目标值之间的差呈正比 并且变动的大小应该与现在值和目标值之间的差呈正比 举个例子 增加数字”2”神经元的激活值",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. ",
  "translatedText": "例如，从某种意义上说，增加 2 号神 经元的激活比减少 8 号神经元的激活 更重要，后者已经非常接近应有的位置。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. ",
  "translatedText": "因此，进一步放大，让我们只关注这 个神经元，我们希望增加其激活。",
  "model": "google_nmt",
  "from_community_srt": "就应该比减少数字”8”神经元的激活值来得重要 因为后者已经很接近它的目标了 那好 我们更进一步 就来关注下这一个神经元 我们要让这里面的激活值变大 还记得这个激活值是 把前一层所有激活值的加权和 加上一个偏置",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. ",
  "translatedText": "请记住，激活被定义为前一层中所有激活的特定加 权总和，加上偏差，然后将其全部插入 sigm oid 压缩函数或 ReLU 之类的函数中。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation. ",
  "translatedText": "因此，可以通过三种不同的途 径联合起来帮助提高激活率。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. ",
  "translatedText": "您可以增加偏差，可以增加权重 ，并且可以更改上一层的激活。",
  "model": "google_nmt",
  "from_community_srt": "再通过sigmoid ReLU之类的挤压函数 最后算出来的吧 所以要增加这个激活值 我们有三条大路可走 一增加偏置 二增加权重 或者三改变上一层的激活值 先来看如何调整权重 各个权重它们的影响力各不相同",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. ",
  "translatedText": "重点关注如何调整权重，注意权重 实际上如何具有不同程度的影响。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. ",
  "translatedText": "与前一层最亮神经元的连接具有最大的影 响，因为这些权重乘以较大的激活值。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. ",
  "translatedText": "因此，如果您要增加其中一个权重，它实际上对 最终成本函数的影响比增加与较暗神经元的连接 权重更大，至少就这一训练示例而言是这样。",
  "model": "google_nmt",
  "from_community_srt": "连接前一层最亮的神经元的权重 影响力也最大 因为这些权重会与大的激活值相乘 所以至少对于这一个训练样本而言 增大了这几个权重值 对最终代价函数造成的影响 就比增大连接黯淡神经元的权重所造成的影响",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck. ",
  "translatedText": "请记住，当我们谈论梯度下降时，我们不仅仅 关心每个组件是否应该向上或向下推动，我 们关心哪些组件可以为您带来最大的收益。",
  "model": "google_nmt",
  "from_community_srt": "要大上好多倍 请记住当我们说到梯度下降的时候 我们并不只看每个参数是该增大还是减小 我们还看该哪个参数的性价比最高 顺便一提  这有一点点像描述生物中 神经元的网络如何学习的一个理论",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together. ",
  "translatedText": "顺便说一句，这至少在某种程度上让人想起神经科学 中关于神经元生物网络如何学习的理论，赫布理论， 通常用短语来概括：一起放电的神经元连接在一起。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active and the ones which we wish to become more active. ",
  "translatedText": "在这里，权重的最大增加、连接的最 大加强发生在最活跃的神经元和我 们希望变得更活跃的神经元之间。",
  "model": "google_nmt",
  "from_community_srt": "“赫布理论”  总结起来就是“一同激活的神经元关联在一起” 这里 权重的最大增长 即连接变得更强的部分 就会发生在已经最活跃的神经元 和想要更多激发的神经元之间 可以说 看见一个2时激发的神经元",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those firing when thinking about it. ",
  "translatedText": "从某种意义上说，看到 2 时放电的神经元与思 考 2 时放电的神经元之间的联系更加紧密。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note. ",
  "translatedText": "需要明确的是，我无法以某种方式对神经元的人 工网络是否表现得像生物大脑做出陈述，这种 将电线连接在一起的想法带有几个有意义的星号 ，但被视为非常松散的类比，我觉得很有趣。",
  "model": "google_nmt",
  "from_community_srt": "会和”想到一个2”时激发的神经元联系地更紧密 这里解释一下 我个人对人工神经网络是否真的在 模仿生物学上大脑的工作 没有什么发言权 “一同激活的神经元关联在一起”这句话是要打星号注释的",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer. ",
  "translatedText": "无论如何，我们可以帮助增加该神经元激活 的第三种方法是更改前一层中的所有激活。",
  "model": "google_nmt",
  "from_community_srt": "但作为一个粗略的对照 我觉得还是挺有意思的 言归正传 第三个能够增加这个神经元激活值的方法 就是改变前一层的激活值 更具体地说 如果所有正权重连接的神经元更亮 所有负权重连接的神经元更暗的话",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active. ",
  "translatedText": "也就是说，如果与具有正权值的数字 2 神经元相连的 所有东西都变得更亮，而与负权值连接的所有东西都变得 更暗，那么那个数字 2 神经元就会变得更加活跃。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. ",
  "translatedText": "与权重变化类似，通过寻求与相应权重大 小成比例的变化，您将获得最大的收益。",
  "model": "google_nmt",
  "from_community_srt": "那么数字2的神经元就会更强烈地激发 和改权重的时候类似 我们想造成更大的影响 就要依据对应权重的大小 对激活值做出呈比例的改变 当然 我们并不能直接改变激活值 我们手头只能控制权重和偏置",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases. ",
  "translatedText": "当然，现在我们不能直接影响这些 激活，我们只能控制权重和偏差。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are. ",
  "translatedText": "但就像最后一层一样，记下这 些所需的更改会很有帮助。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants. ",
  "translatedText": "但请记住，这里缩小一步，这只是 数字 2 输出神经元想要的。",
  "model": "google_nmt",
  "from_community_srt": "但就光对最后一层来说 记住我们期待的变化还是很有帮助的 不过别忘了 从全局上看 这只不过是数字2的神经元所期待的变化 我们还需要最后一层其余的神经元的激发变弱 但这其余的每个输出神经元",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. ",
  "translatedText": "请记住，我们还希望最后一层中的所有其他神经 元变得不那么活跃，并且每个其他输出神经元对 于倒数第二层应该发生什么都有自己的想法。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. ",
  "translatedText": "因此，这个数字 2 神经元的愿望与所有 其他输出神经元的愿望相加，以决定倒数第 二层应该发生什么，同样与相应的权重成比 例，并与每个神经元需要多少成比例改变。",
  "model": "google_nmt",
  "from_community_srt": "对于如何改变倒数第二层 都有各自的想法 所以 我们会把数字2神经元的期待 和别的输出神经元的期待全部加起来 作为对如何改变倒数第二层神经元的指示 这些期待变化不仅是对应的权重的倍数 也是每个神经元激活值改变量的倍数",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in. ",
  "translatedText": "这就是向后传播的想法出现的地方。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. ",
  "translatedText": "通过将所有这些所需的效果添加在一起，您基本上 会得到一个您想要在倒数第二层发生的微调列表。",
  "model": "google_nmt",
  "from_community_srt": "这其实就是在实现”反向传播”的理念了 我们把所有期待的改变加起来 就得到了一串对倒数第二层改动的变化量 有了这些 我们就可以重复这个过程 改变影响倒数第二层神经元激活值的相关参数 从后一层到前一层 把这个过程一直循环到第一层",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. ",
  "translatedText": "一旦有了这些，您就可以递归地将相同的过程 应用于确定这些值的相关权重和偏差，重复我 刚刚走过的相同过程并在网络中向后移动。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. ",
  "translatedText": "再缩小一点，请记住，这只是单个训练示例希 望推动这些权重和偏差中的每一个的方式。",
  "model": "google_nmt",
  "from_community_srt": "放眼大局 还记得我们只是在讨论 单个训练样本对所有权重偏置的影响吗？",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2. ",
  "translatedText": "如果我们只听 2 想要的内容，网络 最终会被激励将所有图像分类为 2。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes. ",
  "translatedText": "因此，您要做的就是对每个其他训练示例执行 相同的反向传播例程，记录每个示例如何更改 权重和偏差，并将这些所需的更改一起平均。",
  "model": "google_nmt",
  "from_community_srt": "如果我们只关注那个“2”的要求 最后  网络只会把所有图像都分类成是“2” 所以你要对其他所有的训练样本 同样地过一遍反向传播 记录下每个样本想怎样修改权重与偏置 最后再取一个平均值",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. ",
  "translatedText": "宽松地说，这里对每个权重和偏差的平均微 调的集合是上一个视频中引用的成本函数的 负梯度，或者至少是与之成比例的东西。",
  "model": "google_nmt",
  "from_community_srt": "这里一系列的权重偏置的平均微调大小 不严格地说  就是上期视频提到的代价函数的负梯度 至少是其标量的倍数 这里的不严格  指的是我还没有准确地解释如何量化这些微调 但如果你清楚我提到的所有改动",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. ",
  "translatedText": "我之所以说粗略地说，只是因为我还没有对这些推动进行 量化精确，但如果你理解我刚才提到的每一个变化，为 什么有些变化比其他变化更大，以及它们如何需要加在一 起，你就会理解其中的机制反向传播实际上在做什么。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. ",
  "translatedText": "顺便说一句，在实践中，计算机需要花费很长时间才 能将每个梯度下降步骤的每个训练示例的影响相加。",
  "model": "google_nmt",
  "from_community_srt": "为什么有些数字是其他数字的好几倍 以及最后要怎么全部加起来 你就懂得了反向传播的真实工作原理 顺带一提 实际操作中 如果梯度下降的每一步 都用上每一个训练样本来计算的话 那么花的时间就太长了",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead. ",
  "translatedText": "所以这就是通常所做的事情。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and divide it into a whole bunch of mini-batches, let's say each one having 100 training examples. ",
  "translatedText": "您随机打乱训练数据并将其分成一大堆小批量， 假设每个小批量都有 100 个训练示例。",
  "model": "google_nmt",
  "from_community_srt": "所以我们一般会这么做 首先把训练样本打乱 然后分成很多组minibatch 每个minibatch就当包含100个训练样本好了 然后你算出这个minibatch下降的一步 这不是代价函数真正的梯度",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch. ",
  "translatedText": "然后根据小批量计算步骤。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly it gives you a significant computational speedup. ",
  "translatedText": "它不是成本函数的实际梯度，它取决于所有训练数 据，而不是这个微小的子集，所以它不是最有效的 下坡步骤，但每个小批量确实给你一个非常好的近 似值，更重要的是它为您带来显着的计算加速。",
  "model": "google_nmt",
  "from_community_srt": "毕竟计算真实梯度得用上所有的样本 而非这个子集 所以这也不是下山最高效的一步 然而 每个minibatch都会给你一个不错的近似 而且更重要的是 你的计算量会减轻不少 你如果想把网络沿代价函数的表面下山的路径画出来的话",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. ",
  "translatedText": "如果你要在相关成本面下绘制网络的轨迹，那么它更 像是一个醉汉漫无目的地跌跌撞撞地下山，但步伐很 快，而不是一个仔细计算的人确定每一步的确切下坡 方向然后朝着这个方向迈出非常缓慢而谨慎的一步。",
  "model": "google_nmt",
  "from_community_srt": "它看上去会有点像醉汉漫无目的地遛下山  但起码步伐很快 而不像是细致入微的人 踏步之前先准确地算好下坡的方向 然后再向那个方向谨小慎微地慢慢走一步 这个技巧就叫做“随机梯度下降” 内容挺多的 我们先小结一下好不好",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent. ",
  "translatedText": "该技术称为随机梯度下降。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we? ",
  "translatedText": "这里发生了很多事情，所以让我们自己总结一下，好吗？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. ",
  "translatedText": "反向传播是一种算法，用于确定单个训练示 例如何推动权重和偏差，不仅在于它们是 否应该上升或下降，还在于这些变化的相 对比例导致权重和偏差最快下降。成本。",
  "model": "google_nmt",
  "from_community_srt": "反向传播算法算的是 单个训练样本想怎样修改权重与偏置 不仅是说每个参数应该变大还是变小 还包括了这些变化的比例是多大  才能最快地降低代价 真正的梯度下降 得对好几万个训练范例都这么操作",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes you get, but that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch. ",
  "translatedText": "真正的梯度下降步骤将涉及对所有数以万计的 训练示例执行此操作，并对获得的所需变化 进行平均，但这在计算上很慢，因此您可以 将数据随机细分为小批量，并根据小批量。",
  "model": "google_nmt",
  "from_community_srt": "然后对这些变化值取平均 但算起来太慢了 所以你会先把所有的样本分到各个minibatch中去 计算一个minibatch来作为梯度下降的一步 计算每个minibatch的梯度 调整参数 不断循环",
  "n_reviews": 0,
  "start": 656.26,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples. ",
  "translatedText": "反复检查所有小批量并进行这些调整，您将 收敛到成本函数的局部最小值，也就是说您 的网络最终将在训练示例上做得非常好。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms. ",
  "translatedText": "综上所述，用于实现反向传播的每一行代码实际上都与您 现在所看到的内容相对应，至少在非正式术语中是这样。",
  "model": "google_nmt",
  "from_community_srt": "最终你就会收敛到代价函数的一个局部最小值上 此时就可以说 你的神经网络对付训练数据已经很不错了 总而言之 我们实现反向传播算法的每一句代码 其实或多或少地都对应了大家已经知道的内容 但有时 了解其中的数学原理只不过是完成了一半",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. ",
  "translatedText": "但有时知道数学的作用只是成功的一半，仅仅 代表该死的东西就会让一切变得混乱和混乱。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So, for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. ",
  "translatedText": "因此，对于那些确实想要深入了解的人，下一个视频将 介绍与此处刚刚介绍的相同的想法，但就基本微积分而 言，这应该会让您在看到主题时更加熟悉。其他资源。",
  "model": "google_nmt",
  "from_community_srt": "如何把这破玩意儿表示出来又会搞得人一头雾水 那么 在座的如果想深入探讨的话 下一期视频中我们会把本期的内容用微积分的形式呈现出来 下一期视频中我们会把本期的内容用微积分的形式呈现出来",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. ",
  "translatedText": "在此之前，值得强调的一件事是，要使该算 法发挥作用，并且这适用于神经网络之外的 各种机器学习，您需要大量的训练数据。",
  "model": "google_nmt",
  "from_community_srt": "希望看过以后再看其他资料时会更容易接受一些吧 收尾之前 我想着重提一点 反向传播算法在内 所有包括神经网络在内的机器学习 要让它们工作 咱需要一大坨的训练数据 我们用的手写数字的范例之所以那么方便",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans. ",
  "translatedText": "在我们的例子中，手写数字成为如此好的例子的原因之一是 MNIST 数据库的存在，其中有很多由人类标记的例子。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with. ",
  "translatedText": "因此，从事机器学习工作的人所熟悉的一个常见挑战是 获取实际需要的标记训练数据，无论是让人标记数以 万计的图像，还是您可能处理的任何其他数据类型。",
  "model": "google_nmt",
  "from_community_srt": "是因为存在着一个MNIST数据库 里面所有的样本都已经人为标记好了 所以机器学习领域的人 最熟悉的一个难关 莫过于获取标记好的训练数据了 不管是叫别人标记成千上万个图像 还是去标记别的类型的数据也罢",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]