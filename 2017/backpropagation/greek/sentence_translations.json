[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn.",
  "translatedText": "Εδώ, θα ασχοληθούμε με την οπισθοδιάδοση, τον βασικό αλγόριθμο πίσω από τον τρόπο με τον οποίο τα νευρωνικά δίκτυα μαθαίνουν.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas.",
  "translatedText": "Μετά από μια σύντομη ανακεφαλαίωση για το πού βρισκόμαστε, το πρώτο πράγμα που θα κάνω είναι μια διαισθητική περιήγηση για το τι πραγματικά κάνει ο αλγόριθμος, χωρίς καμία αναφορά στους τύπους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this.",
  "translatedText": "Στη συνέχεια, για όσους από εσάς θέλετε να βουτήξετε στα μαθηματικά, το επόμενο βίντεο αναλύει τους υπολογισμούς που διέπουν όλα αυτά.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information.",
  "translatedText": "Αν παρακολουθήσατε τα δύο τελευταία βίντεο ή αν μόλις μπήκατε στο θέμα με το κατάλληλο υπόβαθρο, γνωρίζετε τι είναι ένα νευρωνικό δίκτυο και πώς τροφοδοτεί πληροφορίες.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer.",
  "translatedText": "Εδώ, κάνουμε το κλασικό παράδειγμα της αναγνώρισης χειρόγραφων ψηφίων, των οποίων οι τιμές των εικονοστοιχείων τροφοδοτούνται στο πρώτο στρώμα του δικτύου με 784 νευρώνες, και έχω δείξει ένα δίκτυο με δύο κρυφά στρώματα που έχουν μόλις 16 νευρώνες το καθένα, και ένα στρώμα εξόδου με 10 νευρώνες, το οποίο υποδεικνύει ποιο ψηφίο επιλέγει το δίκτυο ως απάντηση.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function.",
  "translatedText": "Περιμένω επίσης να κατανοήσετε την κάθοδο κλίσης, όπως περιγράφεται στο τελευταίο βίντεο, και πώς αυτό που εννοούμε με τη μάθηση είναι ότι θέλουμε να βρούμε ποια βάρη και προκαταλήψεις ελαχιστοποιούν μια συγκεκριμένη συνάρτηση κόστους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component.",
  "translatedText": "Ως γρήγορη υπενθύμιση, για το κόστος ενός μόνο παραδείγματος εκπαίδευσης, παίρνετε την έξοδο που δίνει το δίκτυο, μαζί με την έξοδο που θέλατε να δώσει, και προσθέτετε τα τετράγωνα των διαφορών μεταξύ κάθε συνιστώσας.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network.",
  "translatedText": "Αν το κάνετε αυτό για όλα τα δεκάδες χιλιάδες παραδείγματα εκπαίδευσης και υπολογίσετε το μέσο όρο των αποτελεσμάτων, θα έχετε το συνολικό κόστος του δικτύου.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "And as if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost.",
  "translatedText": "Και σαν να μην έφτανε αυτό, όπως περιγράψαμε στο τελευταίο βίντεο, αυτό που ψάχνουμε είναι η αρνητική κλίση αυτής της συνάρτησης κόστους, η οποία σας λέει πώς πρέπει να αλλάξετε όλα τα βάρη και τις προκαταλήψεις, όλες αυτές τις συνδέσεις, ώστε να μειώσετε το κόστος πιο αποτελεσματικά.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient.",
  "translatedText": "Η οπισθοδιάδοση, το θέμα αυτού του βίντεο, είναι ένας αλγόριθμος για τον υπολογισμό αυτής της τρελά περίπλοκης κλίσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "And the one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it.",
  "translatedText": "Και η μία ιδέα από το τελευταίο βίντεο που πραγματικά θέλω να κρατήσετε σταθερά στο μυαλό σας αυτή τη στιγμή είναι ότι επειδή η σκέψη του διανύσματος κλίσης ως κατεύθυνση σε 13.000 διαστάσεις είναι, για να το θέσω ευγενικά, πέρα από το πεδίο της φαντασίας μας, υπάρχει ένας άλλος τρόπος να το σκεφτείτε.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias.",
  "translatedText": "Το μέγεθος κάθε συνιστώσας εδώ σας λέει πόσο ευαίσθητη είναι η συνάρτηση κόστους σε κάθε βάρος και προκατάληψη.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and you compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1.",
  "translatedText": "Για παράδειγμα, ας πούμε ότι ακολουθείτε τη διαδικασία που θα περιγράψω και υπολογίζετε την αρνητική κλίση και η συνιστώσα που σχετίζεται με το βάρος αυτής της ακμής εδώ είναι 3,2, ενώ η συνιστώσα που σχετίζεται με αυτή την ακμή εδώ είναι 0,1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give.",
  "translatedText": "Ο τρόπος με τον οποίο θα το ερμηνεύατε αυτό είναι ότι το κόστος της συνάρτησης είναι 32 φορές πιο ευαίσθητο στις αλλαγές του πρώτου βάρους, οπότε αν κουνήσετε αυτή την τιμή έστω και λίγο, αυτό θα προκαλέσει κάποια αλλαγή στο κόστος, και αυτή η αλλαγή είναι 32 φορές μεγαλύτερη από ό,τι θα έδινε η ίδια αλλαγή στο δεύτερο βάρος.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all.",
  "translatedText": "Προσωπικά, όταν έμαθα για πρώτη φορά για την οπισθοδιάδοση, νομίζω ότι η πιο συγκεχυμένη πτυχή ήταν απλώς η σημειογραφία και το κυνήγι των δεικτών.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other.",
  "translatedText": "Αλλά μόλις ξετυλίξετε το τι πραγματικά κάνει κάθε μέρος αυτού του αλγορίθμου, κάθε επιμέρους αποτέλεσμα που έχει είναι στην πραγματικότητα αρκετά διαισθητικό, απλώς υπάρχουν πολλές μικρές προσαρμογές που τοποθετούνται η μία πάνω στην άλλη.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases.",
  "translatedText": "Έτσι, θα ξεκινήσω τα πράγματα εδώ, αδιαφορώντας πλήρως για τον συμβολισμό, και θα εξετάσω βήμα προς βήμα τις επιπτώσεις που έχει κάθε παράδειγμα εκπαίδευσης στα βάρη και τις προκαταλήψεις.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example.",
  "translatedText": "Επειδή η συνάρτηση κόστους περιλαμβάνει τη μέση τιμή ενός συγκεκριμένου κόστους ανά παράδειγμα για όλες τις δεκάδες χιλιάδες των παραδειγμάτων εκπαίδευσης, ο τρόπος με τον οποίο ρυθμίζουμε τα βάρη και τις προκαταλήψεις για ένα μόνο βήμα βαθμωτής καθόδου εξαρτάται επίσης από κάθε παράδειγμα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step.",
  "translatedText": "Ή μάλλον, κατ' αρχήν θα έπρεπε, αλλά για λόγους υπολογιστικής αποδοτικότητας θα κάνουμε ένα μικρό κόλπο αργότερα για να μην χρειάζεται να χτυπάτε κάθε παράδειγμα για κάθε βήμα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2.",
  "translatedText": "Σε άλλες περιπτώσεις, αυτή τη στιγμή, το μόνο που θα κάνουμε είναι να εστιάσουμε την προσοχή μας σε ένα μόνο παράδειγμα, αυτή την εικόνα ενός 2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted?",
  "translatedText": "Ποια επίδραση θα πρέπει να έχει αυτό το ένα παράδειγμα εκπαίδευσης στον τρόπο προσαρμογής των βαρών και των προκαταλήψεων;",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on.",
  "translatedText": "Ας πούμε ότι βρισκόμαστε σε ένα σημείο όπου το δίκτυο δεν έχει εκπαιδευτεί καλά ακόμα, οπότε οι ενεργοποιήσεις στην έξοδο θα φαίνονται αρκετά τυχαίες, ίσως κάτι σαν 0,5, 0,8, 0,2, και πάει λέγοντας.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases.",
  "translatedText": "Δεν μπορούμε να αλλάξουμε άμεσα αυτές τις ενεργοποιήσεις, έχουμε επιρροή μόνο στα βάρη και τις προκαταλήψεις.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 242.52,
  "end": 247.16
 },
 {
  "input": "But it's helpful to keep track of which adjustments we wish should take place to that output layer.",
  "translatedText": "Αλλά είναι χρήσιμο να παρακολουθούμε ποιες προσαρμογές θέλουμε να γίνουν σε αυτό το επίπεδο εξόδου.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 247.16,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down.",
  "translatedText": "Και αφού θέλουμε να ταξινομήσει την εικόνα ως 2, θέλουμε αυτή η τρίτη τιμή να ανέβει, ενώ όλες οι άλλες να κατέβουν.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value.",
  "translatedText": "Επιπλέον, τα μεγέθη αυτών των ωθήσεων θα πρέπει να είναι ανάλογα με το πόσο μακριά βρίσκεται κάθε τρέχουσα τιμή από την τιμή-στόχο της.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be.",
  "translatedText": "Για παράδειγμα, η αύξηση της ενεργοποίησης του νευρώνα με τον αριθμό 2 είναι κατά μία έννοια πιο σημαντική από τη μείωση του νευρώνα με τον αριθμό 8, ο οποίος είναι ήδη αρκετά κοντά στο σημείο που θα έπρεπε να είναι.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase.",
  "translatedText": "Έτσι, μεγεθύνοντας περισσότερο, ας επικεντρωθούμε μόνο σε αυτόν τον ένα νευρώνα, εκείνον του οποίου την ενεργοποίηση θέλουμε να αυξήσουμε.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU.",
  "translatedText": "Θυμηθείτε, ότι η ενεργοποίηση ορίζεται ως ένα ορισμένο σταθμισμένο άθροισμα όλων των ενεργοποιήσεων στο προηγούμενο επίπεδο, συν μια προκατάληψη, η οποία στη συνέχεια εισάγεται σε κάτι όπως η σιγμοειδής συνάρτηση squishification ή μια ReLU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation.",
  "translatedText": "Έτσι, υπάρχουν τρεις διαφορετικοί τρόποι που μπορούν να συνεργαστούν για να βοηθήσουν στην αύξηση της ενεργοποίησης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer.",
  "translatedText": "Μπορείτε να αυξήσετε την προκατάληψη, να αυξήσετε τα βάρη και να αλλάξετε τις ενεργοποιήσεις από το προηγούμενο επίπεδο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence.",
  "translatedText": "Εστιάζοντας στον τρόπο προσαρμογής των βαρών, παρατηρήστε πώς τα βάρη έχουν στην πραγματικότητα διαφορετικά επίπεδα επιρροής.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values.",
  "translatedText": "Οι συνδέσεις με τους πιο φωτεινούς νευρώνες από το προηγούμενο επίπεδο έχουν τη μεγαλύτερη επίδραση, καθώς τα βάρη αυτά πολλαπλασιάζονται με μεγαλύτερες τιμές ενεργοποίησης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned.",
  "translatedText": "Έτσι, αν αυξήσετε ένα από αυτά τα βάρη, στην πραγματικότητα έχει ισχυρότερη επίδραση στην τελική συνάρτηση κόστους από ό,τι η αύξηση των βαρών των συνδέσεων με τους πιο αμυδρούς νευρώνες, τουλάχιστον όσον αφορά αυτό το ένα παράδειγμα εκπαίδευσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck.",
  "translatedText": "Θυμηθείτε, όταν μιλάμε για την κάθοδο κλίσης, δεν μας ενδιαφέρει μόνο αν κάθε στοιχείο πρέπει να ανεβαίνει ή να κατεβαίνει, μας ενδιαφέρει ποια από αυτά σας δίνουν το μεγαλύτερο όφελος για το χρήμα σας.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together.",
  "translatedText": "Αυτό, παρεμπιπτόντως, θυμίζει τουλάχιστον κάπως μια θεωρία της νευροεπιστήμης για το πώς μαθαίνουν τα βιολογικά δίκτυα νευρώνων, τη θεωρία του Χέμπιαν, η οποία συχνά συνοψίζεται στη φράση, νευρώνες που πυροδοτούνται μαζί συνδέονται μεταξύ τους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active.",
  "translatedText": "Εδώ, οι μεγαλύτερες αυξήσεις στα βάρη, η μεγαλύτερη ενίσχυση των συνδέσεων, συμβαίνει μεταξύ των νευρώνων που είναι οι πιο ενεργοί και εκείνων που θέλουμε να γίνουν πιο ενεργοί.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those are the ones firing when thinking about a 2.",
  "translatedText": "Κατά μία έννοια, οι νευρώνες που πυροδοτούνται όταν βλέπουμε ένα 2 συνδέονται πιο ισχυρά με εκείνους που πυροδοτούνται όταν σκεφτόμαστε ένα 2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note.",
  "translatedText": "Για να είμαι σαφής, δεν είμαι σε θέση να κάνω δηλώσεις με τον ένα ή τον άλλο τρόπο σχετικά με το αν τα τεχνητά δίκτυα νευρώνων συμπεριφέρονται όπως οι βιολογικοί εγκέφαλοι, και αυτή η ιδέα του \"fires together wire together\" συνοδεύεται από μερικούς σημαντικούς αστερίσκους, αλλά αν το πάρουμε ως μια πολύ χαλαρή αναλογία, το βρίσκω ενδιαφέρον να το σημειώσουμε.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer.",
  "translatedText": "Τέλος πάντων, ο τρίτος τρόπος με τον οποίο μπορούμε να βοηθήσουμε στην αύξηση της ενεργοποίησης αυτού του νευρώνα είναι να αλλάξουμε όλες τις ενεργοποιήσεις στο προηγούμενο επίπεδο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active.",
  "translatedText": "Δηλαδή, αν όλα όσα συνδέονται με αυτόν τον νευρώνα του ψηφίου 2 με θετικό βάρος γίνουν πιο φωτεινά και αν όλα όσα συνδέονται με αρνητικό βάρος γίνουν πιο σκοτεινά, τότε αυτός ο νευρώνας του ψηφίου 2 θα γίνει πιο ενεργός.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights.",
  "translatedText": "Και παρόμοια με τις αλλαγές βάρους, θα έχετε το μεγαλύτερο όφελος για το χρήμα σας επιδιώκοντας αλλαγές που είναι ανάλογες με το μέγεθος των αντίστοιχων βαρών.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases.",
  "translatedText": "Τώρα, φυσικά, δεν μπορούμε να επηρεάσουμε άμεσα αυτές τις ενεργοποιήσεις, έχουμε μόνο τον έλεγχο των βαρών και των προκαταλήψεων.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are.",
  "translatedText": "Αλλά όπως και με το τελευταίο επίπεδο, είναι χρήσιμο να κρατάτε σημειώσεις σχετικά με τις επιθυμητές αλλαγές.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants.",
  "translatedText": "Αλλά να θυμάστε, μεγεθύνοντας ένα βήμα προς τα έξω, ότι αυτό είναι μόνο αυτό που θέλει ο νευρώνας εξόδου του ψηφίου 2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer.",
  "translatedText": "Θυμηθείτε, θέλουμε επίσης όλοι οι άλλοι νευρώνες του τελευταίου στρώματος να γίνουν λιγότερο ενεργοί, και κάθε ένας από αυτούς τους άλλους νευρώνες εξόδου έχει τις δικές του σκέψεις σχετικά με το τι πρέπει να συμβεί σε αυτό το προτελευταίο στρώμα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So, the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change.",
  "translatedText": "Έτσι, η επιθυμία αυτού του νευρώνα του ψηφίου 2 προστίθεται μαζί με τις επιθυμίες όλων των άλλων νευρώνων εξόδου για το τι πρέπει να συμβεί σε αυτό το προτελευταίο στρώμα, και πάλι σε αναλογία με τα αντίστοιχα βάρη και σε αναλογία με το πόσο πρέπει να αλλάξει ο καθένας από αυτούς τους νευρώνες.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in.",
  "translatedText": "Εδώ ακριβώς έρχεται η ιδέα της διάδοσης προς τα πίσω.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer.",
  "translatedText": "Προσθέτοντας όλα αυτά τα επιθυμητά αποτελέσματα, έχετε ουσιαστικά μια λίστα με τις κινήσεις που θέλετε να συμβούν σε αυτό το προτελευταίο στρώμα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network.",
  "translatedText": "Και μόλις τα έχετε αυτά, μπορείτε να εφαρμόσετε αναδρομικά την ίδια διαδικασία στα σχετικά βάρη και τις προκαταλήψεις που καθορίζουν αυτές τις τιμές, επαναλαμβάνοντας την ίδια διαδικασία που μόλις προχώρησα και προχωρώντας προς τα πίσω μέσα στο δίκτυο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases.",
  "translatedText": "Και μεγεθύνοντας λίγο περισσότερο, να θυμάστε ότι όλα αυτά είναι απλώς το πώς ένα μόνο παράδειγμα εκπαίδευσης επιθυμεί να ωθήσει κάθε ένα από αυτά τα βάρη και τις προκαταλήψεις.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2.",
  "translatedText": "Αν ακούγαμε μόνο τι ήθελε αυτό το 2, το δίκτυο θα είχε τελικά κίνητρο να ταξινομεί όλες τις εικόνες ως 2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes.",
  "translatedText": "Έτσι, αυτό που κάνετε είναι να ακολουθήσετε την ίδια ρουτίνα backprop για κάθε άλλο παράδειγμα εκπαίδευσης, καταγράφοντας πώς θα ήθελε ο καθένας από αυτούς να αλλάξει τα βάρη και τις προκαταλήψεις, και να υπολογίσετε τον μέσο όρο αυτών των επιθυμητών αλλαγών.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it.",
  "translatedText": "Αυτή η συλλογή εδώ των μέσων όρων των ωθήσεων σε κάθε βάρος και προκατάληψη είναι, χαλαρά μιλώντας, η αρνητική κλίση της συνάρτησης κόστους που αναφέρθηκε στο τελευταίο βίντεο, ή τουλάχιστον κάτι ανάλογο με αυτήν.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing.",
  "translatedText": "Λέω χαλαρά μιλώντας μόνο επειδή δεν έχω ακόμη να γίνω ποσοτικά ακριβής σχετικά με αυτές τις ωθήσεις, αλλά αν καταλάβετε κάθε αλλαγή που μόλις ανέφερα, γιατί κάποιες είναι αναλογικά μεγαλύτερες από άλλες και πώς πρέπει να προστεθούν όλες μαζί, καταλαβαίνετε τους μηχανισμούς για το τι κάνει στην πραγματικότητα η οπισθοδιάδοση.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step.",
  "translatedText": "Παρεμπιπτόντως, στην πράξη, οι υπολογιστές χρειάζονται εξαιρετικά πολύ χρόνο για να προσθέσουν την επιρροή κάθε παραδείγματος εκπαίδευσης σε κάθε βήμα καθόδου κλίσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead.",
  "translatedText": "Οπότε εδώ είναι αυτό που συνήθως γίνεται αντί γι' αυτό.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and then divide it into a whole bunch of mini-batches, let's say each one having 100 training examples.",
  "translatedText": "Ανακατεύετε τυχαία τα δεδομένα εκπαίδευσής σας και στη συνέχεια τα χωρίζετε σε ένα σωρό μίνι-πακέτα, ας πούμε ότι το καθένα έχει 100 παραδείγματα εκπαίδευσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch.",
  "translatedText": "Στη συνέχεια, υπολογίζετε ένα βήμα σύμφωνα με τη μίνι-παρτίδα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not going to be the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speedup.",
  "translatedText": "Δεν πρόκειται να είναι η πραγματική κλίση της συνάρτησης κόστους, η οποία εξαρτάται από όλα τα δεδομένα εκπαίδευσης, όχι από αυτό το μικροσκοπικό υποσύνολο, οπότε δεν είναι το πιο αποτελεσματικό βήμα προς τα κάτω, αλλά κάθε μίνι-πακέτο σας δίνει μια αρκετά καλή προσέγγιση και, το σημαντικότερο, σας δίνει μια σημαντική υπολογιστική επιτάχυνση.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction.",
  "translatedText": "Αν σχεδιάζατε την τροχιά του δικτύου σας κάτω από τη σχετική επιφάνεια κόστους, θα ήταν λίγο περισσότερο σαν έναν μεθυσμένο άνθρωπο που παραπατάει άσκοπα σε έναν λόφο, αλλά κάνει γρήγορα βήματα, παρά σαν έναν προσεκτικά υπολογίζοντα άνθρωπο που καθορίζει την ακριβή κατεύθυνση κάθε βήματος προς τα κάτω πριν κάνει ένα πολύ αργό και προσεκτικό βήμα προς αυτή την κατεύθυνση.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent.",
  "translatedText": "Αυτή η τεχνική αναφέρεται ως στοχαστική κάθοδος κλίσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we?",
  "translatedText": "Συμβαίνουν πολλά εδώ, οπότε ας τα συνοψίσουμε για τον εαυτό μας, εντάξει;",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost.",
  "translatedText": "Η οπισθοδιάδοση είναι ο αλγόριθμος για τον προσδιορισμό του τρόπου με τον οποίο ένα μεμονωμένο παράδειγμα εκπαίδευσης θα ήθελε να ωθήσει τα βάρη και τις προκαταλήψεις, όχι μόνο ως προς το αν θα πρέπει να ανέβουν ή να κατέβουν, αλλά και ως προς το ποιες σχετικές αναλογίες αυτών των αλλαγών προκαλούν την ταχύτερη μείωση του κόστους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens of thousands of training examples and averaging the desired changes you get.",
  "translatedText": "Ένα πραγματικό βήμα κλίσης θα περιελάμβανε την εκτέλεση αυτού του βήματος για όλες τις δεκάδες χιλιάδες των παραδειγμάτων εκπαίδευσης και τον υπολογισμό του μέσου όρου των επιθυμητών αλλαγών που θα λαμβάνατε.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 656.26,
  "end": 664.2
 },
 {
  "input": "But that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch.",
  "translatedText": "Αλλά αυτό είναι υπολογιστικά αργό, οπότε, αντί γι' αυτό, υποδιαιρείτε τυχαία τα δεδομένα σε μίνι παρτίδες και υπολογίζετε κάθε βήμα σε σχέση με μια μίνι παρτίδα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 664.86,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all of the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples.",
  "translatedText": "Αν περάσετε επανειλημμένα από όλες τις μίνι παρτίδες και κάνετε αυτές τις προσαρμογές, θα συγκλίνετε προς ένα τοπικό ελάχιστο της συνάρτησης κόστους, πράγμα που σημαίνει ότι το δίκτυό σας θα καταλήξει να κάνει πολύ καλή δουλειά στα παραδείγματα εκπαίδευσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms.",
  "translatedText": "Έτσι, με όλα αυτά ειπωμένα, κάθε γραμμή κώδικα που θα πήγαινε στην υλοποίηση του backprop στην πραγματικότητα αντιστοιχεί σε κάτι που έχετε δει τώρα, τουλάχιστον σε ανεπίσημους όρους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing.",
  "translatedText": "Αλλά μερικές φορές το να ξέρεις τι κάνουν τα μαθηματικά είναι μόνο το ήμισυ της μάχης, και η απλή αναπαράσταση του καταραμένου πράγματος είναι το σημείο όπου τα πράγματα μπερδεύονται και δημιουργούν σύγχυση.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources.",
  "translatedText": "Έτσι, για όσους από εσάς θέλετε να εμβαθύνετε, το επόμενο βίντεο εξετάζει τις ίδιες ιδέες που μόλις παρουσιάστηκαν εδώ, αλλά από την άποψη του υποκείμενου λογισμού, ο οποίος ελπίζουμε ότι θα το κάνει λίγο πιο οικείο καθώς θα βλέπετε το θέμα σε άλλες πηγές.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data.",
  "translatedText": "Πριν από αυτό, ένα πράγμα που αξίζει να τονιστεί είναι ότι για να λειτουργήσει αυτός ο αλγόριθμος, και αυτό ισχύει για όλα τα είδη μηχανικής μάθησης πέρα από τα νευρωνικά δίκτυα, χρειάζεστε πολλά δεδομένα εκπαίδευσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans.",
  "translatedText": "Στην περίπτωσή μας, ένα πράγμα που κάνει τα χειρόγραφα ψηφία ένα τόσο καλό παράδειγμα είναι ότι υπάρχει η βάση δεδομένων MNIST, με τόσα πολλά παραδείγματα που έχουν επισημανθεί από ανθρώπους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with.",
  "translatedText": "Έτσι, μια κοινή πρόκληση που όσοι εργάζεστε στη μηχανική μάθηση θα γνωρίζετε, είναι να αποκτήσετε τα δεδομένα εκπαίδευσης με ετικέτες που πραγματικά χρειάζεστε, είτε πρόκειται για την τοποθέτηση ετικετών σε δεκάδες χιλιάδες εικόνες, είτε για οποιονδήποτε άλλο τύπο δεδομένων που μπορεί να έχετε να αντιμετωπίσετε.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]