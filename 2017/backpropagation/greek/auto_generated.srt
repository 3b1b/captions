1
00:00:04,059 --> 00:00:06,489
Εδώ, θα ασχοληθούμε με την οπισθοδιάδοση, τον βασικό αλγόριθμο 

2
00:00:06,489 --> 00:00:08,880
πίσω από τον τρόπο με τον οποίο τα νευρωνικά δίκτυα μαθαίνουν.

3
00:00:09,400 --> 00:00:11,723
Μετά από μια σύντομη ανακεφαλαίωση για το πού βρισκόμαστε, 

4
00:00:11,723 --> 00:00:14,243
το πρώτο πράγμα που θα κάνω είναι μια διαισθητική περιήγηση για 

5
00:00:14,243 --> 00:00:17,000
το τι πραγματικά κάνει ο αλγόριθμος, χωρίς καμία αναφορά στους τύπους.

6
00:00:17,660 --> 00:00:20,420
Στη συνέχεια, για όσους από εσάς θέλετε να βουτήξετε στα μαθηματικά, 

7
00:00:20,420 --> 00:00:23,020
το επόμενο βίντεο αναλύει τους υπολογισμούς που διέπουν όλα αυτά.

8
00:00:23,820 --> 00:00:27,538
Αν παρακολουθήσατε τα δύο τελευταία βίντεο ή αν μόλις μπήκατε στο θέμα με το κατάλληλο 

9
00:00:27,538 --> 00:00:31,000
υπόβαθρο, γνωρίζετε τι είναι ένα νευρωνικό δίκτυο και πώς τροφοδοτεί πληροφορίες.

10
00:00:31,680 --> 00:00:35,103
Εδώ, κάνουμε το κλασικό παράδειγμα της αναγνώρισης χειρόγραφων ψηφίων, 

11
00:00:35,103 --> 00:00:39,299
των οποίων οι τιμές των εικονοστοιχείων τροφοδοτούνται στο πρώτο στρώμα του δικτύου με 

12
00:00:39,299 --> 00:00:43,639
784 νευρώνες, και έχω δείξει ένα δίκτυο με δύο κρυφά στρώματα που έχουν μόλις 16 νευρώνες 

13
00:00:43,639 --> 00:00:46,002
το καθένα, και ένα στρώμα εξόδου με 10 νευρώνες, 

14
00:00:46,002 --> 00:00:49,040
το οποίο υποδεικνύει ποιο ψηφίο επιλέγει το δίκτυο ως απάντηση.

15
00:00:50,040 --> 00:00:54,337
Περιμένω επίσης να κατανοήσετε την κάθοδο κλίσης, όπως περιγράφεται στο τελευταίο βίντεο, 

16
00:00:54,337 --> 00:00:58,013
και πώς αυτό που εννοούμε με τη μάθηση είναι ότι θέλουμε να βρούμε ποια βάρη 

17
00:00:58,013 --> 00:01:01,260
και προκαταλήψεις ελαχιστοποιούν μια συγκεκριμένη συνάρτηση κόστους.

18
00:01:02,040 --> 00:01:06,323
Ως γρήγορη υπενθύμιση, για το κόστος ενός μόνο παραδείγματος εκπαίδευσης, 

19
00:01:06,323 --> 00:01:10,895
παίρνετε την έξοδο που δίνει το δίκτυο, μαζί με την έξοδο που θέλατε να δώσει, 

20
00:01:10,895 --> 00:01:14,600
και προσθέτετε τα τετράγωνα των διαφορών μεταξύ κάθε συνιστώσας.

21
00:01:15,380 --> 00:01:18,617
Αν το κάνετε αυτό για όλα τα δεκάδες χιλιάδες παραδείγματα εκπαίδευσης και 

22
00:01:18,617 --> 00:01:22,200
υπολογίσετε το μέσο όρο των αποτελεσμάτων, θα έχετε το συνολικό κόστος του δικτύου.

23
00:01:22,200 --> 00:01:25,989
Και σαν να μην έφτανε αυτό, όπως περιγράψαμε στο τελευταίο βίντεο, 

24
00:01:25,989 --> 00:01:30,005
αυτό που ψάχνουμε είναι η αρνητική κλίση αυτής της συνάρτησης κόστους, 

25
00:01:30,005 --> 00:01:34,247
η οποία σας λέει πώς πρέπει να αλλάξετε όλα τα βάρη και τις προκαταλήψεις, 

26
00:01:34,247 --> 00:01:38,320
όλες αυτές τις συνδέσεις, ώστε να μειώσετε το κόστος πιο αποτελεσματικά.

27
00:01:43,260 --> 00:01:45,694
Η οπισθοδιάδοση, το θέμα αυτού του βίντεο, είναι ένας 

28
00:01:45,694 --> 00:01:48,580
αλγόριθμος για τον υπολογισμό αυτής της τρελά περίπλοκης κλίσης.

29
00:01:49,140 --> 00:01:52,571
Και η μία ιδέα από το τελευταίο βίντεο που πραγματικά θέλω να κρατήσετε 

30
00:01:52,571 --> 00:01:56,288
σταθερά στο μυαλό σας αυτή τη στιγμή είναι ότι επειδή η σκέψη του διανύσματος 

31
00:01:56,288 --> 00:01:59,815
κλίσης ως κατεύθυνση σε 13.000 διαστάσεις είναι, για να το θέσω ευγενικά, 

32
00:01:59,815 --> 00:02:03,580
πέρα από το πεδίο της φαντασίας μας, υπάρχει ένας άλλος τρόπος να το σκεφτείτε.

33
00:02:04,600 --> 00:02:07,741
Το μέγεθος κάθε συνιστώσας εδώ σας λέει πόσο ευαίσθητη 

34
00:02:07,741 --> 00:02:10,940
είναι η συνάρτηση κόστους σε κάθε βάρος και προκατάληψη.

35
00:02:11,800 --> 00:02:16,322
Για παράδειγμα, ας πούμε ότι ακολουθείτε τη διαδικασία που θα περιγράψω και 

36
00:02:16,322 --> 00:02:21,082
υπολογίζετε την αρνητική κλίση και η συνιστώσα που σχετίζεται με το βάρος αυτής 

37
00:02:21,082 --> 00:02:26,260
της ακμής εδώ είναι 3,2, ενώ η συνιστώσα που σχετίζεται με αυτή την ακμή εδώ είναι 0,1.

38
00:02:26,820 --> 00:02:30,743
Ο τρόπος με τον οποίο θα το ερμηνεύατε αυτό είναι ότι το κόστος της συνάρτησης 

39
00:02:30,743 --> 00:02:33,772
είναι 32 φορές πιο ευαίσθητο στις αλλαγές του πρώτου βάρους, 

40
00:02:33,772 --> 00:02:36,156
οπότε αν κουνήσετε αυτή την τιμή έστω και λίγο, 

41
00:02:36,156 --> 00:02:40,030
αυτό θα προκαλέσει κάποια αλλαγή στο κόστος, και αυτή η αλλαγή είναι 32 φορές 

42
00:02:40,030 --> 00:02:43,060
μεγαλύτερη από ό,τι θα έδινε η ίδια αλλαγή στο δεύτερο βάρος.

43
00:02:48,420 --> 00:02:51,407
Προσωπικά, όταν έμαθα για πρώτη φορά για την οπισθοδιάδοση, 

44
00:02:51,407 --> 00:02:55,740
νομίζω ότι η πιο συγκεχυμένη πτυχή ήταν απλώς η σημειογραφία και το κυνήγι των δεικτών.

45
00:02:56,220 --> 00:02:59,620
Αλλά μόλις ξετυλίξετε το τι πραγματικά κάνει κάθε μέρος αυτού του αλγορίθμου, 

46
00:02:59,620 --> 00:03:03,152
κάθε επιμέρους αποτέλεσμα που έχει είναι στην πραγματικότητα αρκετά διαισθητικό, 

47
00:03:03,152 --> 00:03:06,640
απλώς υπάρχουν πολλές μικρές προσαρμογές που τοποθετούνται η μία πάνω στην άλλη.

48
00:03:07,740 --> 00:03:11,083
Έτσι, θα ξεκινήσω τα πράγματα εδώ, αδιαφορώντας πλήρως για τον συμβολισμό, 

49
00:03:11,083 --> 00:03:13,712
και θα εξετάσω βήμα προς βήμα τις επιπτώσεις που έχει κάθε 

50
00:03:13,712 --> 00:03:16,120
παράδειγμα εκπαίδευσης στα βάρη και τις προκαταλήψεις.

51
00:03:17,020 --> 00:03:20,488
Επειδή η συνάρτηση κόστους περιλαμβάνει τη μέση τιμή ενός συγκεκριμένου 

52
00:03:20,488 --> 00:03:24,535
κόστους ανά παράδειγμα για όλες τις δεκάδες χιλιάδες των παραδειγμάτων εκπαίδευσης, 

53
00:03:24,535 --> 00:03:27,956
ο τρόπος με τον οποίο ρυθμίζουμε τα βάρη και τις προκαταλήψεις για ένα 

54
00:03:27,956 --> 00:03:31,040
μόνο βήμα βαθμωτής καθόδου εξαρτάται επίσης από κάθε παράδειγμα.

55
00:03:31,680 --> 00:03:35,440
Ή μάλλον, κατ' αρχήν θα έπρεπε, αλλά για λόγους υπολογιστικής αποδοτικότητας θα κάνουμε 

56
00:03:35,440 --> 00:03:39,200
ένα μικρό κόλπο αργότερα για να μην χρειάζεται να χτυπάτε κάθε παράδειγμα για κάθε βήμα.

57
00:03:39,200 --> 00:03:42,486
Σε άλλες περιπτώσεις, αυτή τη στιγμή, το μόνο που θα κάνουμε είναι να 

58
00:03:42,486 --> 00:03:45,960
εστιάσουμε την προσοχή μας σε ένα μόνο παράδειγμα, αυτή την εικόνα ενός 2.

59
00:03:46,720 --> 00:03:48,865
Ποια επίδραση θα πρέπει να έχει αυτό το ένα παράδειγμα 

60
00:03:48,865 --> 00:03:51,480
εκπαίδευσης στον τρόπο προσαρμογής των βαρών και των προκαταλήψεων;

61
00:03:52,680 --> 00:03:56,775
Ας πούμε ότι βρισκόμαστε σε ένα σημείο όπου το δίκτυο δεν έχει εκπαιδευτεί καλά ακόμα, 

62
00:03:56,775 --> 00:03:59,787
οπότε οι ενεργοποιήσεις στην έξοδο θα φαίνονται αρκετά τυχαίες, 

63
00:03:59,787 --> 00:04:02,000
ίσως κάτι σαν 0,5, 0,8, 0,2, και πάει λέγοντας.

64
00:04:02,520 --> 00:04:04,988
Δεν μπορούμε να αλλάξουμε άμεσα αυτές τις ενεργοποιήσεις, 

65
00:04:04,988 --> 00:04:07,160
έχουμε επιρροή μόνο στα βάρη και τις προκαταλήψεις.

66
00:04:07,160 --> 00:04:10,171
Αλλά είναι χρήσιμο να παρακολουθούμε ποιες προσαρμογές 

67
00:04:10,171 --> 00:04:12,580
θέλουμε να γίνουν σε αυτό το επίπεδο εξόδου.

68
00:04:13,360 --> 00:04:16,697
Και αφού θέλουμε να ταξινομήσει την εικόνα ως 2, 

69
00:04:16,697 --> 00:04:21,260
θέλουμε αυτή η τρίτη τιμή να ανέβει, ενώ όλες οι άλλες να κατέβουν.

70
00:04:22,060 --> 00:04:25,817
Επιπλέον, τα μεγέθη αυτών των ωθήσεων θα πρέπει να είναι ανάλογα με 

71
00:04:25,817 --> 00:04:29,520
το πόσο μακριά βρίσκεται κάθε τρέχουσα τιμή από την τιμή-στόχο της.

72
00:04:30,220 --> 00:04:33,730
Για παράδειγμα, η αύξηση της ενεργοποίησης του νευρώνα με τον αριθμό 2 

73
00:04:33,730 --> 00:04:37,636
είναι κατά μία έννοια πιο σημαντική από τη μείωση του νευρώνα με τον αριθμό 8, 

74
00:04:37,636 --> 00:04:40,900
ο οποίος είναι ήδη αρκετά κοντά στο σημείο που θα έπρεπε να είναι.

75
00:04:42,040 --> 00:04:45,115
Έτσι, μεγεθύνοντας περισσότερο, ας επικεντρωθούμε μόνο σε αυτόν τον ένα νευρώνα, 

76
00:04:45,115 --> 00:04:47,280
εκείνον του οποίου την ενεργοποίηση θέλουμε να αυξήσουμε.

77
00:04:48,180 --> 00:04:52,520
Θυμηθείτε, ότι η ενεργοποίηση ορίζεται ως ένα ορισμένο σταθμισμένο άθροισμα όλων 

78
00:04:52,520 --> 00:04:56,003
των ενεργοποιήσεων στο προηγούμενο επίπεδο, συν μια προκατάληψη, 

79
00:04:56,003 --> 00:05:00,450
η οποία στη συνέχεια εισάγεται σε κάτι όπως η σιγμοειδής συνάρτηση squishification 

80
00:05:00,450 --> 00:05:01,040
ή μια ReLU.

81
00:05:01,640 --> 00:05:04,237
Έτσι, υπάρχουν τρεις διαφορετικοί τρόποι που μπορούν να 

82
00:05:04,237 --> 00:05:07,020
συνεργαστούν για να βοηθήσουν στην αύξηση της ενεργοποίησης.

83
00:05:07,440 --> 00:05:10,630
Μπορείτε να αυξήσετε την προκατάληψη, να αυξήσετε τα βάρη 

84
00:05:10,630 --> 00:05:14,040
και να αλλάξετε τις ενεργοποιήσεις από το προηγούμενο επίπεδο.

85
00:05:14,940 --> 00:05:17,876
Εστιάζοντας στον τρόπο προσαρμογής των βαρών, παρατηρήστε πώς 

86
00:05:17,876 --> 00:05:20,860
τα βάρη έχουν στην πραγματικότητα διαφορετικά επίπεδα επιρροής.

87
00:05:21,440 --> 00:05:24,982
Οι συνδέσεις με τους πιο φωτεινούς νευρώνες από το προηγούμενο επίπεδο έχουν τη 

88
00:05:24,982 --> 00:05:28,480
μεγαλύτερη επίδραση, καθώς τα βάρη αυτά πολλαπλασιάζονται με μεγαλύτερες τιμές 

89
00:05:28,480 --> 00:05:29,100
ενεργοποίησης.

90
00:05:31,460 --> 00:05:35,613
Έτσι, αν αυξήσετε ένα από αυτά τα βάρη, στην πραγματικότητα έχει ισχυρότερη επίδραση 

91
00:05:35,613 --> 00:05:39,522
στην τελική συνάρτηση κόστους από ό,τι η αύξηση των βαρών των συνδέσεων με τους 

92
00:05:39,522 --> 00:05:43,480
πιο αμυδρούς νευρώνες, τουλάχιστον όσον αφορά αυτό το ένα παράδειγμα εκπαίδευσης.

93
00:05:44,420 --> 00:05:47,251
Θυμηθείτε, όταν μιλάμε για την κάθοδο κλίσης, δεν μας ενδιαφέρει 

94
00:05:47,251 --> 00:05:49,821
μόνο αν κάθε στοιχείο πρέπει να ανεβαίνει ή να κατεβαίνει, 

95
00:05:49,821 --> 00:05:53,220
μας ενδιαφέρει ποια από αυτά σας δίνουν το μεγαλύτερο όφελος για το χρήμα σας.

96
00:05:55,020 --> 00:05:58,864
Αυτό, παρεμπιπτόντως, θυμίζει τουλάχιστον κάπως μια θεωρία της νευροεπιστήμης για 

97
00:05:58,864 --> 00:06:02,146
το πώς μαθαίνουν τα βιολογικά δίκτυα νευρώνων, τη θεωρία του Χέμπιαν, 

98
00:06:02,146 --> 00:06:05,897
η οποία συχνά συνοψίζεται στη φράση, νευρώνες που πυροδοτούνται μαζί συνδέονται 

99
00:06:05,897 --> 00:06:06,460
μεταξύ τους.

100
00:06:07,260 --> 00:06:11,562
Εδώ, οι μεγαλύτερες αυξήσεις στα βάρη, η μεγαλύτερη ενίσχυση των συνδέσεων, 

101
00:06:11,562 --> 00:06:16,600
συμβαίνει μεταξύ των νευρώνων που είναι οι πιο ενεργοί και εκείνων που θέλουμε να γίνουν 

102
00:06:16,600 --> 00:06:17,280
πιο ενεργοί.

103
00:06:17,940 --> 00:06:21,025
Κατά μία έννοια, οι νευρώνες που πυροδοτούνται όταν βλέπουμε ένα 2 

104
00:06:21,025 --> 00:06:24,480
συνδέονται πιο ισχυρά με εκείνους που πυροδοτούνται όταν σκεφτόμαστε ένα 2.

105
00:06:25,400 --> 00:06:28,515
Για να είμαι σαφής, δεν είμαι σε θέση να κάνω δηλώσεις με τον ένα ή τον 

106
00:06:28,515 --> 00:06:31,587
άλλο τρόπο σχετικά με το αν τα τεχνητά δίκτυα νευρώνων συμπεριφέρονται 

107
00:06:31,587 --> 00:06:34,659
όπως οι βιολογικοί εγκέφαλοι, και αυτή η ιδέα του "fires together wire 

108
00:06:34,659 --> 00:06:37,255
together" συνοδεύεται από μερικούς σημαντικούς αστερίσκους, 

109
00:06:37,255 --> 00:06:41,020
αλλά αν το πάρουμε ως μια πολύ χαλαρή αναλογία, το βρίσκω ενδιαφέρον να το σημειώσουμε.

110
00:06:41,940 --> 00:06:44,097
Τέλος πάντων, ο τρίτος τρόπος με τον οποίο μπορούμε να 

111
00:06:44,097 --> 00:06:46,411
βοηθήσουμε στην αύξηση της ενεργοποίησης αυτού του νευρώνα 

112
00:06:46,411 --> 00:06:49,040
είναι να αλλάξουμε όλες τις ενεργοποιήσεις στο προηγούμενο επίπεδο.

113
00:06:49,040 --> 00:06:52,955
Δηλαδή, αν όλα όσα συνδέονται με αυτόν τον νευρώνα του ψηφίου 2 με θετικό 

114
00:06:52,955 --> 00:06:57,717
βάρος γίνουν πιο φωτεινά και αν όλα όσα συνδέονται με αρνητικό βάρος γίνουν πιο σκοτεινά, 

115
00:06:57,717 --> 00:07:00,680
τότε αυτός ο νευρώνας του ψηφίου 2 θα γίνει πιο ενεργός.

116
00:07:02,540 --> 00:07:06,385
Και παρόμοια με τις αλλαγές βάρους, θα έχετε το μεγαλύτερο όφελος για το χρήμα 

117
00:07:06,385 --> 00:07:10,280
σας επιδιώκοντας αλλαγές που είναι ανάλογες με το μέγεθος των αντίστοιχων βαρών.

118
00:07:12,140 --> 00:07:15,203
Τώρα, φυσικά, δεν μπορούμε να επηρεάσουμε άμεσα αυτές τις ενεργοποιήσεις, 

119
00:07:15,203 --> 00:07:17,480
έχουμε μόνο τον έλεγχο των βαρών και των προκαταλήψεων.

120
00:07:17,480 --> 00:07:20,708
Αλλά όπως και με το τελευταίο επίπεδο, είναι χρήσιμο 

121
00:07:20,708 --> 00:07:24,120
να κρατάτε σημειώσεις σχετικά με τις επιθυμητές αλλαγές.

122
00:07:24,580 --> 00:07:26,615
Αλλά να θυμάστε, μεγεθύνοντας ένα βήμα προς τα έξω, 

123
00:07:26,615 --> 00:07:29,200
ότι αυτό είναι μόνο αυτό που θέλει ο νευρώνας εξόδου του ψηφίου 2.

124
00:07:29,760 --> 00:07:33,106
Θυμηθείτε, θέλουμε επίσης όλοι οι άλλοι νευρώνες του τελευταίου στρώματος να γίνουν 

125
00:07:33,106 --> 00:07:36,293
λιγότερο ενεργοί, και κάθε ένας από αυτούς τους άλλους νευρώνες εξόδου έχει τις 

126
00:07:36,293 --> 00:07:39,600
δικές του σκέψεις σχετικά με το τι πρέπει να συμβεί σε αυτό το προτελευταίο στρώμα.

127
00:07:42,700 --> 00:07:47,098
Έτσι, η επιθυμία αυτού του νευρώνα του ψηφίου 2 προστίθεται μαζί με τις 

128
00:07:47,098 --> 00:07:51,435
επιθυμίες όλων των άλλων νευρώνων εξόδου για το τι πρέπει να συμβεί σε 

129
00:07:51,435 --> 00:07:55,833
αυτό το προτελευταίο στρώμα, και πάλι σε αναλογία με τα αντίστοιχα βάρη 

130
00:07:55,833 --> 00:08:00,720
και σε αναλογία με το πόσο πρέπει να αλλάξει ο καθένας από αυτούς τους νευρώνες.

131
00:08:01,600 --> 00:08:05,480
Εδώ ακριβώς έρχεται η ιδέα της διάδοσης προς τα πίσω.

132
00:08:05,820 --> 00:08:08,367
Προσθέτοντας όλα αυτά τα επιθυμητά αποτελέσματα, 

133
00:08:08,367 --> 00:08:12,164
έχετε ουσιαστικά μια λίστα με τις κινήσεις που θέλετε να συμβούν σε αυτό 

134
00:08:12,164 --> 00:08:13,360
το προτελευταίο στρώμα.

135
00:08:14,220 --> 00:08:17,803
Και μόλις τα έχετε αυτά, μπορείτε να εφαρμόσετε αναδρομικά την ίδια διαδικασία στα 

136
00:08:17,803 --> 00:08:20,696
σχετικά βάρη και τις προκαταλήψεις που καθορίζουν αυτές τις τιμές, 

137
00:08:20,696 --> 00:08:24,409
επαναλαμβάνοντας την ίδια διαδικασία που μόλις προχώρησα και προχωρώντας προς τα πίσω 

138
00:08:24,409 --> 00:08:25,100
μέσα στο δίκτυο.

139
00:08:28,960 --> 00:08:32,911
Και μεγεθύνοντας λίγο περισσότερο, να θυμάστε ότι όλα αυτά είναι απλώς το πώς ένα μόνο 

140
00:08:32,911 --> 00:08:37,000
παράδειγμα εκπαίδευσης επιθυμεί να ωθήσει κάθε ένα από αυτά τα βάρη και τις προκαταλήψεις.

141
00:08:37,480 --> 00:08:40,213
Αν ακούγαμε μόνο τι ήθελε αυτό το 2, το δίκτυο θα 

142
00:08:40,213 --> 00:08:43,220
είχε τελικά κίνητρο να ταξινομεί όλες τις εικόνες ως 2.

143
00:08:44,059 --> 00:08:47,932
Έτσι, αυτό που κάνετε είναι να ακολουθήσετε την ίδια ρουτίνα backprop για κάθε άλλο 

144
00:08:47,932 --> 00:08:51,897
παράδειγμα εκπαίδευσης, καταγράφοντας πώς θα ήθελε ο καθένας από αυτούς να αλλάξει τα 

145
00:08:51,897 --> 00:08:56,000
βάρη και τις προκαταλήψεις, και να υπολογίσετε τον μέσο όρο αυτών των επιθυμητών αλλαγών.

146
00:09:01,720 --> 00:09:06,380
Αυτή η συλλογή εδώ των μέσων όρων των ωθήσεων σε κάθε βάρος και προκατάληψη είναι, 

147
00:09:06,380 --> 00:09:10,423
χαλαρά μιλώντας, η αρνητική κλίση της συνάρτησης κόστους που αναφέρθηκε 

148
00:09:10,423 --> 00:09:13,680
στο τελευταίο βίντεο, ή τουλάχιστον κάτι ανάλογο με αυτήν.

149
00:09:14,380 --> 00:09:18,560
Λέω χαλαρά μιλώντας μόνο επειδή δεν έχω ακόμη να γίνω ποσοτικά ακριβής σχετικά με 

150
00:09:18,560 --> 00:09:22,078
αυτές τις ωθήσεις, αλλά αν καταλάβετε κάθε αλλαγή που μόλις ανέφερα, 

151
00:09:22,078 --> 00:09:26,207
γιατί κάποιες είναι αναλογικά μεγαλύτερες από άλλες και πώς πρέπει να προστεθούν 

152
00:09:26,207 --> 00:09:30,286
όλες μαζί, καταλαβαίνετε τους μηχανισμούς για το τι κάνει στην πραγματικότητα η 

153
00:09:30,286 --> 00:09:31,000
οπισθοδιάδοση.

154
00:09:33,960 --> 00:09:38,251
Παρεμπιπτόντως, στην πράξη, οι υπολογιστές χρειάζονται εξαιρετικά πολύ χρόνο για να 

155
00:09:38,251 --> 00:09:42,440
προσθέσουν την επιρροή κάθε παραδείγματος εκπαίδευσης σε κάθε βήμα καθόδου κλίσης.

156
00:09:43,140 --> 00:09:44,820
Οπότε εδώ είναι αυτό που συνήθως γίνεται αντί γι' αυτό.

157
00:09:45,480 --> 00:09:48,950
Ανακατεύετε τυχαία τα δεδομένα εκπαίδευσής σας και στη συνέχεια τα χωρίζετε σε 

158
00:09:48,950 --> 00:09:52,420
ένα σωρό μίνι-πακέτα, ας πούμε ότι το καθένα έχει 100 παραδείγματα εκπαίδευσης.

159
00:09:52,940 --> 00:09:56,200
Στη συνέχεια, υπολογίζετε ένα βήμα σύμφωνα με τη μίνι-παρτίδα.

160
00:09:56,960 --> 00:09:59,868
Δεν πρόκειται να είναι η πραγματική κλίση της συνάρτησης κόστους, 

161
00:09:59,868 --> 00:10:02,116
η οποία εξαρτάται από όλα τα δεδομένα εκπαίδευσης, 

162
00:10:02,116 --> 00:10:05,994
όχι από αυτό το μικροσκοπικό υποσύνολο, οπότε δεν είναι το πιο αποτελεσματικό βήμα προς 

163
00:10:05,994 --> 00:10:09,211
τα κάτω, αλλά κάθε μίνι-πακέτο σας δίνει μια αρκετά καλή προσέγγιση και, 

164
00:10:09,211 --> 00:10:12,120
το σημαντικότερο, σας δίνει μια σημαντική υπολογιστική επιτάχυνση.

165
00:10:12,820 --> 00:10:16,539
Αν σχεδιάζατε την τροχιά του δικτύου σας κάτω από τη σχετική επιφάνεια κόστους, 

166
00:10:16,539 --> 00:10:20,629
θα ήταν λίγο περισσότερο σαν έναν μεθυσμένο άνθρωπο που παραπατάει άσκοπα σε έναν λόφο, 

167
00:10:20,629 --> 00:10:24,023
αλλά κάνει γρήγορα βήματα, παρά σαν έναν προσεκτικά υπολογίζοντα άνθρωπο 

168
00:10:24,023 --> 00:10:27,417
που καθορίζει την ακριβή κατεύθυνση κάθε βήματος προς τα κάτω πριν κάνει 

169
00:10:27,417 --> 00:10:30,160
ένα πολύ αργό και προσεκτικό βήμα προς αυτή την κατεύθυνση.

170
00:10:31,540 --> 00:10:34,660
Αυτή η τεχνική αναφέρεται ως στοχαστική κάθοδος κλίσης.

171
00:10:35,960 --> 00:10:39,620
Συμβαίνουν πολλά εδώ, οπότε ας τα συνοψίσουμε για τον εαυτό μας, εντάξει;

172
00:10:40,440 --> 00:10:44,265
Η οπισθοδιάδοση είναι ο αλγόριθμος για τον προσδιορισμό του τρόπου με τον οποίο ένα 

173
00:10:44,265 --> 00:10:48,091
μεμονωμένο παράδειγμα εκπαίδευσης θα ήθελε να ωθήσει τα βάρη και τις προκαταλήψεις, 

174
00:10:48,091 --> 00:10:50,778
όχι μόνο ως προς το αν θα πρέπει να ανέβουν ή να κατέβουν, 

175
00:10:50,778 --> 00:10:54,694
αλλά και ως προς το ποιες σχετικές αναλογίες αυτών των αλλαγών προκαλούν την ταχύτερη 

176
00:10:54,694 --> 00:10:55,560
μείωση του κόστους.

177
00:10:56,260 --> 00:10:58,779
Ένα πραγματικό βήμα κλίσης θα περιελάμβανε την εκτέλεση αυτού του 

178
00:10:58,779 --> 00:11:01,375
βήματος για όλες τις δεκάδες χιλιάδες των παραδειγμάτων εκπαίδευσης 

179
00:11:01,375 --> 00:11:04,200
και τον υπολογισμό του μέσου όρου των επιθυμητών αλλαγών που θα λαμβάνατε.

180
00:11:04,860 --> 00:11:07,826
Αλλά αυτό είναι υπολογιστικά αργό, οπότε, αντί γι' αυτό, 

181
00:11:07,826 --> 00:11:11,886
υποδιαιρείτε τυχαία τα δεδομένα σε μίνι παρτίδες και υπολογίζετε κάθε βήμα σε 

182
00:11:11,886 --> 00:11:13,240
σχέση με μια μίνι παρτίδα.

183
00:11:14,000 --> 00:11:17,906
Αν περάσετε επανειλημμένα από όλες τις μίνι παρτίδες και κάνετε αυτές τις προσαρμογές, 

184
00:11:17,906 --> 00:11:20,735
θα συγκλίνετε προς ένα τοπικό ελάχιστο της συνάρτησης κόστους, 

185
00:11:20,735 --> 00:11:24,417
πράγμα που σημαίνει ότι το δίκτυό σας θα καταλήξει να κάνει πολύ καλή δουλειά στα 

186
00:11:24,417 --> 00:11:25,540
παραδείγματα εκπαίδευσης.

187
00:11:27,240 --> 00:11:30,451
Έτσι, με όλα αυτά ειπωμένα, κάθε γραμμή κώδικα που θα πήγαινε 

188
00:11:30,451 --> 00:11:35,010
στην υλοποίηση του backprop στην πραγματικότητα αντιστοιχεί σε κάτι που έχετε δει τώρα, 

189
00:11:35,010 --> 00:11:36,720
τουλάχιστον σε ανεπίσημους όρους.

190
00:11:37,560 --> 00:11:40,317
Αλλά μερικές φορές το να ξέρεις τι κάνουν τα μαθηματικά είναι μόνο το ήμισυ της μάχης, 

191
00:11:40,317 --> 00:11:42,567
και η απλή αναπαράσταση του καταραμένου πράγματος είναι το σημείο όπου 

192
00:11:42,567 --> 00:11:44,120
τα πράγματα μπερδεύονται και δημιουργούν σύγχυση.

193
00:11:44,860 --> 00:11:46,982
Έτσι, για όσους από εσάς θέλετε να εμβαθύνετε, 

194
00:11:46,982 --> 00:11:50,278
το επόμενο βίντεο εξετάζει τις ίδιες ιδέες που μόλις παρουσιάστηκαν εδώ, 

195
00:11:50,278 --> 00:11:54,117
αλλά από την άποψη του υποκείμενου λογισμού, ο οποίος ελπίζουμε ότι θα το κάνει λίγο 

196
00:11:54,117 --> 00:11:56,420
πιο οικείο καθώς θα βλέπετε το θέμα σε άλλες πηγές.

197
00:11:57,340 --> 00:11:59,943
Πριν από αυτό, ένα πράγμα που αξίζει να τονιστεί είναι ότι για να 

198
00:11:59,943 --> 00:12:02,902
λειτουργήσει αυτός ο αλγόριθμος, και αυτό ισχύει για όλα τα είδη μηχανικής 

199
00:12:02,902 --> 00:12:05,900
μάθησης πέρα από τα νευρωνικά δίκτυα, χρειάζεστε πολλά δεδομένα εκπαίδευσης.

200
00:12:06,420 --> 00:12:09,077
Στην περίπτωσή μας, ένα πράγμα που κάνει τα χειρόγραφα ψηφία 

201
00:12:09,077 --> 00:12:11,995
ένα τόσο καλό παράδειγμα είναι ότι υπάρχει η βάση δεδομένων MNIST, 

202
00:12:11,995 --> 00:12:14,740
με τόσα πολλά παραδείγματα που έχουν επισημανθεί από ανθρώπους.

203
00:12:15,300 --> 00:12:18,240
Έτσι, μια κοινή πρόκληση που όσοι εργάζεστε στη μηχανική μάθηση θα γνωρίζετε, 

204
00:12:18,240 --> 00:12:21,369
είναι να αποκτήσετε τα δεδομένα εκπαίδευσης με ετικέτες που πραγματικά χρειάζεστε, 

205
00:12:21,369 --> 00:12:24,084
είτε πρόκειται για την τοποθέτηση ετικετών σε δεκάδες χιλιάδες εικόνες, 

206
00:12:24,084 --> 00:12:27,100
είτε για οποιονδήποτε άλλο τύπο δεδομένων που μπορεί να έχετε να αντιμετωπίσετε.

