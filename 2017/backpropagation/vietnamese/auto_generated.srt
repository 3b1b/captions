1
00:00:00,000 --> 00:00:04,945
Ở đây, chúng tôi giải quyết vấn đề lan truyền ngược, thuật

2
00:00:04,945 --> 00:00:09,640
toán cốt lõi đằng sau cách mạng lưới thần kinh học hỏi.

3
00:00:09,640 --> 00:00:12,184
Sau khi tóm tắt nhanh về vị trí của chúng ta, điều đầu tiên

4
00:00:12,184 --> 00:00:14,813
tôi sẽ làm là hướng dẫn trực quan về những gì thuật toán thực

5
00:00:14,813 --> 00:00:17,400
sự đang thực hiện mà không cần tham chiếu đến các công thức.

6
00:00:17,400 --> 00:00:20,720
Sau đó, dành cho những ai muốn đi sâu vào toán học, video tiếp

7
00:00:20,720 --> 00:00:24,040
theo sẽ đi sâu vào phép tính cơ bản của tất cả những điều này.

8
00:00:24,040 --> 00:00:27,581
Nếu bạn đã xem hai video cuối cùng hoặc nếu bạn chỉ bắt đầu với nền tảng thích hợp,

9
00:00:27,581 --> 00:00:31,080
thì bạn sẽ biết mạng lưới thần kinh là gì và cách nó truyền thông tin chuyển tiếp.

10
00:00:31,080 --> 00:00:35,644
Ở đây, chúng tôi đang thực hiện một ví dụ cổ điển về việc nhận dạng các chữ

11
00:00:35,644 --> 00:00:40,330
số viết tay có giá trị pixel được đưa vào lớp đầu tiên của mạng có 784 nơ-ron

12
00:00:40,330 --> 00:00:44,834
và tôi đã hiển thị một mạng có hai lớp ẩn, mỗi lớp chỉ có 16 nơ-ron và một

13
00:00:44,834 --> 00:00:49,520
đầu ra lớp gồm 10 nơ-ron, cho biết mạng đang chọn chữ số nào làm câu trả lời.

14
00:00:49,520 --> 00:00:53,683
Tôi cũng mong bạn hiểu độ dốc giảm dần, như được mô tả trong

15
00:00:53,683 --> 00:00:57,847
video trước và ý nghĩa của việc học là chúng tôi muốn tìm ra

16
00:00:57,847 --> 00:01:02,080
trọng số và độ lệch nào giảm thiểu một hàm chi phí nhất định.

17
00:01:02,080 --> 00:01:06,500
Xin nhắc lại, với chi phí cho một ví dụ đào tạo, bạn lấy đầu

18
00:01:06,500 --> 00:01:10,994
ra mà mạng cung cấp, cùng với đầu ra mà bạn muốn nó cung cấp,

19
00:01:10,994 --> 00:01:15,560
rồi cộng các bình phương của sự khác biệt giữa mỗi thành phần.

20
00:01:15,560 --> 00:01:19,300
Thực hiện việc này cho tất cả hàng chục nghìn ví dụ đào tạo của bạn và tính

21
00:01:19,300 --> 00:01:23,040
trung bình các kết quả, điều này sẽ mang lại cho bạn tổng chi phí của mạng.

22
00:01:23,040 --> 00:01:29,541
Như thể nghĩ thế vẫn chưa đủ, như được mô tả trong video trước, thứ mà chúng ta đang

23
00:01:29,541 --> 00:01:36,196
tìm kiếm là gradient âm của hàm chi phí này, nó cho bạn biết cách bạn cần thay đổi tất

24
00:01:36,196 --> 00:01:43,080
cả trọng số và độ lệch, tất cả những kết nối này, để giảm chi phí một cách hiệu quả nhất.

25
00:01:43,080 --> 00:01:46,206
Lan truyền ngược, chủ đề của video này, là một

26
00:01:46,206 --> 00:01:49,600
thuật toán để tính toán độ dốc cực kỳ phức tạp đó.

27
00:01:49,600 --> 00:01:54,487
Một ý tưởng từ video trước mà tôi thực sự muốn bạn ghi nhớ ngay bây giờ là vì coi

28
00:01:54,487 --> 00:01:59,494
vectơ gradient như một hướng trong 13.000 chiều, nói một cách nhẹ nhàng, ngoài phạm

29
00:01:59,494 --> 00:02:04,620
vi trí tưởng tượng của chúng ta, còn có một ý tưởng khác. cách bạn có thể nghĩ về nó.

30
00:02:04,620 --> 00:02:08,348
Độ lớn của từng thành phần ở đây cho bạn biết mức độ nhạy

31
00:02:08,348 --> 00:02:11,820
cảm của hàm chi phí đối với từng trọng số và độ lệch.

32
00:02:11,820 --> 00:02:17,130
Ví dụ: giả sử bạn thực hiện quy trình mà tôi sắp mô tả và tính gradient

33
00:02:17,130 --> 00:02:21,998
âm và thành phần liên quan đến trọng số trên cạnh này ở đây sẽ là

34
00:02:21,998 --> 00:02:26,940
3.2, trong khi thành phần được liên kết với cạnh này ở đây là 0.1.

35
00:02:26,940 --> 00:02:32,968
Theo cách bạn giải thích thì chi phí của hàm nhạy cảm hơn 32 lần với những thay đổi về

36
00:02:32,968 --> 00:02:39,135
trọng số đầu tiên đó, vì vậy nếu bạn thay đổi giá trị đó một chút, điều đó sẽ gây ra một

37
00:02:39,135 --> 00:02:45,302
số thay đổi về chi phí và thay đổi đó lớn hơn 32 lần so với lực lắc của vật nặng thứ hai

38
00:02:45,302 --> 00:02:45,580
đó.

39
00:02:45,580 --> 00:02:50,781
Cá nhân tôi, khi lần đầu tiên tìm hiểu về lan truyền ngược, tôi

40
00:02:50,781 --> 00:02:55,820
nghĩ khía cạnh khó hiểu nhất chỉ là ký hiệu và chỉ số của nó.

41
00:02:55,820 --> 00:02:59,714
Nhưng một khi bạn khám phá ra chức năng thực sự của từng phần của

42
00:02:59,714 --> 00:03:03,550
thuật toán này, mỗi hiệu ứng riêng lẻ mà nó mang lại thực sự khá

43
00:03:03,550 --> 00:03:07,740
trực quan, chỉ là có rất nhiều điều chỉnh nhỏ được xếp chồng lên nhau.

44
00:03:07,740 --> 00:03:12,410
Vì vậy, tôi sẽ bắt đầu mọi thứ ở đây với việc hoàn toàn không quan tâm đến ký

45
00:03:12,410 --> 00:03:17,380
hiệu và chỉ xem qua tác động của mỗi ví dụ huấn luyện đối với trọng số và độ lệch.

46
00:03:17,380 --> 00:03:22,127
Bởi vì hàm chi phí liên quan đến việc tính trung bình một chi phí nhất định cho

47
00:03:22,127 --> 00:03:26,874
mỗi ví dụ trong tất cả hàng chục nghìn ví dụ huấn luyện, nên cách chúng ta điều

48
00:03:26,874 --> 00:03:31,740
chỉnh trọng số và độ lệch cho một bước giảm độ dốc cũng phụ thuộc vào từng ví dụ.

49
00:03:31,740 --> 00:03:35,753
Hay đúng hơn, về nguyên tắc là như vậy, nhưng để đạt hiệu quả tính toán, chúng tôi sẽ

50
00:03:35,753 --> 00:03:39,860
thực hiện một thủ thuật nhỏ sau để giúp bạn không cần phải xem từng ví dụ cho mỗi bước.

51
00:03:39,860 --> 00:03:43,320
Trong các trường hợp khác, ngay bây giờ, tất cả những gì chúng ta sẽ

52
00:03:43,320 --> 00:03:46,780
làm là tập trung sự chú ý vào một ví dụ duy nhất, hình ảnh số 2 này.

53
00:03:46,780 --> 00:03:51,740
Ví dụ đào tạo này sẽ có ảnh hưởng gì đến cách điều chỉnh trọng số và độ lệch?

54
00:03:51,740 --> 00:03:57,225
Giả sử chúng ta đang ở thời điểm mạng chưa được đào tạo tốt, do đó, kích hoạt ở

55
00:03:57,225 --> 00:04:02,780
đầu ra sẽ trông khá ngẫu nhiên, có thể giống như 0.5, 0.8, 0.2, cứ thế tiếp tục.

56
00:04:02,780 --> 00:04:06,169
Chúng tôi không thể trực tiếp thay đổi những kích hoạt đó, chúng tôi

57
00:04:06,169 --> 00:04:09,705
chỉ có ảnh hưởng đến trọng số và độ lệch, nhưng sẽ rất hữu ích khi theo

58
00:04:09,705 --> 00:04:13,340
dõi những điều chỉnh nào chúng tôi muốn sẽ diễn ra đối với lớp đầu ra đó.

59
00:04:13,340 --> 00:04:17,492
Và vì chúng tôi muốn nó phân loại hình ảnh thành 2, nên chúng tôi muốn giá

60
00:04:17,492 --> 00:04:21,700
trị thứ ba đó được nâng lên trong khi tất cả các giá trị khác bị đẩy xuống.

61
00:04:21,700 --> 00:04:25,828
Hơn nữa, kích thước của những cú hích này phải tỷ lệ thuận với

62
00:04:25,828 --> 00:04:30,220
khoảng cách giữa mỗi giá trị hiện tại với giá trị mục tiêu của nó.

63
00:04:30,220 --> 00:04:35,964
Ví dụ, việc tăng cường kích hoạt tế bào thần kinh số 2 theo một nghĩa nào đó quan

64
00:04:35,964 --> 00:04:42,060
trọng hơn việc giảm kích hoạt tế bào thần kinh số 8, vốn đã khá gần với mức cần thiết.

65
00:04:42,060 --> 00:04:44,820
Vì vậy, hãy phóng to hơn nữa, hãy tập trung vào một

66
00:04:44,820 --> 00:04:47,900
nơ-ron này, nơ-ron mà chúng ta muốn tăng cường kích hoạt.

67
00:04:47,900 --> 00:04:52,610
Hãy nhớ rằng, kích hoạt đó được xác định là tổng có trọng số nhất định

68
00:04:52,610 --> 00:04:57,255
của tất cả các kích hoạt ở lớp trước, cộng với độ lệch, sau đó tất cả

69
00:04:57,255 --> 00:05:01,900
được cắm vào một cái gì đó như hàm sigmoid squishification hoặc ReLU.

70
00:05:01,900 --> 00:05:08,060
Vì vậy, có ba cách khác nhau có thể hợp tác với nhau để giúp tăng cường sự kích hoạt đó.

71
00:05:08,060 --> 00:05:15,300
Bạn có thể tăng độ lệch, có thể tăng trọng số và có thể thay đổi kích hoạt từ lớp trước.

72
00:05:15,300 --> 00:05:18,217
Tập trung vào cách điều chỉnh trọng số, chú ý xem các

73
00:05:18,217 --> 00:05:21,460
trọng số thực sự có mức độ ảnh hưởng khác nhau như thế nào.

74
00:05:21,460 --> 00:05:26,478
Các kết nối với các nơ-ron sáng nhất từ lớp trước có tác động lớn

75
00:05:26,478 --> 00:05:31,420
nhất vì các trọng số đó được nhân với giá trị kích hoạt lớn hơn.

76
00:05:31,420 --> 00:05:35,461
Vì vậy, nếu bạn tăng một trong những trọng số đó, nó thực sự có ảnh

77
00:05:35,461 --> 00:05:39,681
hưởng mạnh hơn đến hàm chi phí cuối cùng so với việc tăng trọng số của

78
00:05:39,681 --> 00:05:44,020
các kết nối với các nơ-ron mờ hơn, ít nhất là đối với ví dụ đào tạo này.

79
00:05:44,020 --> 00:05:47,227
Hãy nhớ rằng, khi nói về việc giảm độ dốc, chúng tôi không chỉ quan

80
00:05:47,227 --> 00:05:50,576
tâm đến việc mỗi thành phần nên được nâng lên hay giảm xuống, mà chúng

81
00:05:50,576 --> 00:05:54,020
tôi còn quan tâm đến thành phần nào mang lại cho bạn nhiều lợi ích nhất.

82
00:05:54,020 --> 00:05:58,291
Nhân tiện, điều này ít nhất gợi nhớ đến một lý thuyết trong khoa học thần kinh về

83
00:05:58,291 --> 00:06:02,668
cách mạng lưới sinh học của các tế bào thần kinh học hỏi, lý thuyết Hebbian, thường

84
00:06:02,668 --> 00:06:06,940
được tóm tắt trong cụm từ, các tế bào thần kinh hoạt động cùng nhau nối với nhau.

85
00:06:06,940 --> 00:06:10,564
Ở đây, trọng lượng tăng lên nhiều nhất, sự tăng cường lớn nhất

86
00:06:10,564 --> 00:06:14,303
của các kết nối, xảy ra giữa các tế bào thần kinh hoạt động mạnh

87
00:06:14,303 --> 00:06:18,100
nhất và những tế bào mà chúng ta mong muốn trở nên năng động hơn.

88
00:06:18,100 --> 00:06:21,770
Theo một nghĩa nào đó, các tế bào thần kinh kích hoạt khi nhìn thấy số 2 sẽ có

89
00:06:21,770 --> 00:06:25,440
mối liên kết chặt chẽ hơn với những tế bào thần kinh kích hoạt khi nghĩ về nó.

90
00:06:25,440 --> 00:06:29,620
Nói rõ hơn, tôi không có đủ tư cách để đưa ra tuyên bố theo cách này hay cách khác

91
00:06:29,620 --> 00:06:33,751
về việc liệu mạng lưới nơ-ron nhân tạo có hoạt động giống như bộ não sinh học hay

92
00:06:33,751 --> 00:06:37,881
không, và ý tưởng này kết hợp với nhau đi kèm với một vài dấu hoa thị có ý nghĩa,

93
00:06:37,881 --> 00:06:41,760
nhưng được coi là rất lỏng lẻo. sự tương tự, tôi thấy thật thú vị khi lưu ý.

94
00:06:41,760 --> 00:06:45,615
Dù sao đi nữa, cách thứ ba chúng ta có thể giúp tăng cường kích hoạt

95
00:06:45,615 --> 00:06:49,360
tế bào thần kinh này là thay đổi tất cả các kích hoạt ở lớp trước.

96
00:06:49,360 --> 00:06:53,705
Cụ thể, nếu mọi thứ kết nối với nơron số 2 có trọng số dương

97
00:06:53,705 --> 00:06:58,263
trở nên sáng hơn và nếu mọi thứ kết nối với nơron số 2 có trọng

98
00:06:58,263 --> 00:07:02,680
số âm trở nên mờ hơn thì nơron số 2 đó sẽ hoạt động mạnh hơn.

99
00:07:02,680 --> 00:07:06,712
Và tương tự như những thay đổi về trọng lượng, bạn sẽ thu được lợi ích lớn nhất bằng

100
00:07:06,712 --> 00:07:10,840
cách tìm kiếm những thay đổi tỷ lệ thuận với kích thước của các trọng lượng tương ứng.

101
00:07:10,840 --> 00:07:14,634
Tất nhiên, hiện tại, chúng tôi không thể tác động trực tiếp đến những

102
00:07:14,634 --> 00:07:18,320
kích hoạt đó, chúng tôi chỉ có quyền kiểm soát trọng số và độ lệch.

103
00:07:18,320 --> 00:07:21,111
Nhưng cũng giống như lớp cuối cùng, việc ghi lại

104
00:07:21,111 --> 00:07:23,960
những thay đổi mong muốn đó là gì sẽ rất hữu ích.

105
00:07:23,960 --> 00:07:27,132
Nhưng hãy nhớ rằng, thu nhỏ một bước ở đây, đây

106
00:07:27,132 --> 00:07:30,040
chỉ là điều mà nơ-ron đầu ra chữ số 2 muốn.

107
00:07:30,040 --> 00:07:34,448
Hãy nhớ rằng, chúng ta cũng muốn tất cả các nơ-ron khác ở lớp cuối

108
00:07:34,448 --> 00:07:38,791
cùng trở nên ít hoạt động hơn và mỗi nơ-ron đầu ra khác đó có suy

109
00:07:38,791 --> 00:07:43,200
nghĩ riêng về điều gì sẽ xảy ra với lớp thứ hai đến lớp cuối cùng.

110
00:07:43,200 --> 00:07:49,356
Vì vậy, mong muốn của nơ-ron chữ số 2 này được cộng thêm cùng với mong muốn của tất cả

111
00:07:49,356 --> 00:07:55,442
các nơ-ron đầu ra khác về điều gì sẽ xảy ra từ lớp thứ hai đến lớp cuối cùng, một lần

112
00:07:55,442 --> 00:08:01,740
nữa theo tỷ lệ với trọng số tương ứng và tỷ lệ với mỗi nơ-ron đó cần bao nhiêu thay đổi.

113
00:08:01,740 --> 00:08:05,940
Đây chính là nơi mà ý tưởng truyền bá ngược xuất hiện.

114
00:08:05,940 --> 00:08:10,045
Bằng cách cộng tất cả các hiệu ứng mong muốn này lại với nhau, về cơ bản bạn sẽ có

115
00:08:10,045 --> 00:08:14,300
được một danh sách các cú hích mà bạn muốn thực hiện ở lớp thứ hai đến lớp cuối cùng.

116
00:08:14,300 --> 00:08:19,303
Và khi bạn đã có những thứ đó, bạn có thể áp dụng đệ quy quy trình tương tự

117
00:08:19,303 --> 00:08:24,110
cho các trọng số và độ lệch có liên quan để xác định các giá trị đó, lặp

118
00:08:24,110 --> 00:08:29,180
lại quy trình tương tự mà tôi vừa thực hiện và di chuyển ngược lại qua mạng.

119
00:08:29,180 --> 00:08:33,382
Và thu nhỏ hơn một chút, hãy nhớ rằng đây chỉ là cách một ví dụ

120
00:08:33,382 --> 00:08:37,520
đào tạo duy nhất muốn thúc đẩy từng trọng số và thành kiến đó.

121
00:08:37,520 --> 00:08:40,705
Nếu chúng ta chỉ lắng nghe những gì thứ 2 đó muốn thì cuối cùng

122
00:08:40,705 --> 00:08:44,140
mạng sẽ được khuyến khích chỉ phân loại tất cả hình ảnh thành thứ 2.

123
00:08:44,140 --> 00:08:50,048
Vì vậy, những gì bạn làm là thực hiện quy trình hỗ trợ tương tự này

124
00:08:50,048 --> 00:08:56,217
cho mọi ví dụ đào tạo khác, ghi lại cách mỗi ví dụ muốn thay đổi trọng

125
00:08:56,217 --> 00:09:02,300
số và độ lệch, đồng thời tính trung bình những thay đổi mong muốn đó.

126
00:09:02,300 --> 00:09:06,320
Bộ sưu tập ở đây gồm các mức tăng trung bình cho từng trọng số và độ

127
00:09:06,320 --> 00:09:10,281
lệch, nói một cách lỏng lẻo, là độ dốc âm của hàm chi phí được tham

128
00:09:10,281 --> 00:09:14,360
chiếu trong video trước hoặc ít nhất là thứ gì đó tỷ lệ thuận với nó.

129
00:09:14,360 --> 00:09:19,295
Tôi nói một cách lỏng lẻo chỉ vì tôi vẫn chưa hiểu chính xác về mặt định lượng về

130
00:09:19,295 --> 00:09:24,169
những cú hích đó, nhưng nếu bạn hiểu mọi thay đổi mà tôi vừa đề cập, tại sao một

131
00:09:24,169 --> 00:09:28,984
số thay đổi lại lớn hơn những thay đổi khác theo tỷ lệ và cách tất cả chúng cần

132
00:09:28,984 --> 00:09:34,100
được cộng lại với nhau, bạn sẽ hiểu cơ chế của lan truyền ngược thực sự đang làm gì.

133
00:09:34,100 --> 00:09:38,672
Nhân tiện, trên thực tế, máy tính phải mất một thời gian rất dài để cộng

134
00:09:38,672 --> 00:09:43,120
dồn mức độ ảnh hưởng của từng ví dụ huấn luyện ở mỗi bước giảm độ dốc.

135
00:09:43,120 --> 00:09:45,540
Vì vậy, đây là những gì thường được thực hiện thay thế.

136
00:09:45,540 --> 00:09:49,395
Bạn xáo trộn ngẫu nhiên dữ liệu huấn luyện của mình và chia

137
00:09:49,395 --> 00:09:53,380
nó thành nhiều đợt nhỏ, giả sử mỗi đợt có 100 mẫu huấn luyện.

138
00:09:53,380 --> 00:09:56,980
Sau đó, bạn tính toán một bước theo lô nhỏ.

139
00:09:56,980 --> 00:10:01,064
Đó không phải là độ dốc thực tế của hàm chi phí, phụ thuộc vào tất cả dữ liệu

140
00:10:01,064 --> 00:10:04,940
huấn luyện, không phải tập hợp con nhỏ này, vì vậy đây không phải là bước

141
00:10:04,940 --> 00:10:08,972
xuống dốc hiệu quả nhất, nhưng mỗi lô nhỏ sẽ cung cấp cho bạn một xấp xỉ khá

142
00:10:08,972 --> 00:10:12,900
tốt và quan trọng hơn là nó cung cấp cho bạn một tốc độ tính toán đáng kể.

143
00:10:12,900 --> 00:10:17,514
Nếu bạn lập biểu đồ quỹ đạo của mạng theo bề mặt chi phí liên quan, thì nó sẽ giống một

144
00:10:17,514 --> 00:10:22,076
người đàn ông say rượu vấp ngã không mục đích xuống một ngọn đồi nhưng thực hiện những

145
00:10:22,076 --> 00:10:26,795
bước đi nhanh chóng, hơn là một người đàn ông tính toán cẩn thận xác định hướng xuống dốc

146
00:10:26,795 --> 00:10:31,410
chính xác của mỗi bước trước khi thực hiện bước đi thật chậm rãi và cẩn thận theo hướng

147
00:10:31,410 --> 00:10:31,620
đó.

148
00:10:31,620 --> 00:10:35,200
Kỹ thuật này được gọi là giảm độ dốc ngẫu nhiên.

149
00:10:35,200 --> 00:10:40,400
Có rất nhiều điều đang diễn ra ở đây, vì vậy chúng ta hãy tự tổng hợp lại nhé?

150
00:10:40,400 --> 00:10:45,635
Lan truyền ngược là thuật toán để xác định cách một ví dụ huấn luyện muốn điều

151
00:10:45,635 --> 00:10:50,871
chỉnh trọng số và độ lệch, không chỉ về việc chúng nên tăng hay giảm mà còn về

152
00:10:50,871 --> 00:10:56,240
tỷ lệ tương đối với những thay đổi đó gây ra sự giảm nhanh nhất đối với trị giá.

153
00:10:56,240 --> 00:11:00,707
Bước giảm độ dốc thực sự sẽ liên quan đến việc thực hiện việc này cho tất cả hàng

154
00:11:00,707 --> 00:11:05,174
chục nghìn ví dụ đào tạo của bạn và tính trung bình các thay đổi mong muốn mà bạn

155
00:11:05,174 --> 00:11:09,641
nhận được, nhưng việc đó chậm về mặt tính toán, vì vậy thay vào đó, bạn chia ngẫu

156
00:11:09,641 --> 00:11:14,000
nhiên dữ liệu thành các lô nhỏ và tính toán từng bước tương ứng với một lô nhỏ.

157
00:11:14,000 --> 00:11:18,418
Liên tục thực hiện tất cả các đợt nhỏ và thực hiện những điều

158
00:11:18,418 --> 00:11:22,765
chỉnh này, bạn sẽ hội tụ về mức tối thiểu cục bộ của hàm chi

159
00:11:22,765 --> 00:11:27,540
phí, nghĩa là mạng của bạn sẽ thực hiện rất tốt các ví dụ đào tạo.

160
00:11:27,540 --> 00:11:32,548
Vì vậy, với tất cả những gì đã nói, mọi dòng mã dùng để triển khai backprop thực

161
00:11:32,548 --> 00:11:37,680
sự tương ứng với những gì bạn đã thấy, ít nhất là theo thuật ngữ không chính thức.

162
00:11:37,680 --> 00:11:41,230
Nhưng đôi khi biết những gì toán học làm mới chỉ là một nửa trận chiến, và chỉ

163
00:11:41,230 --> 00:11:44,780
việc trình bày cái thứ chết tiệt đó là mọi thứ sẽ trở nên lộn xộn và khó hiểu.

164
00:11:44,780 --> 00:11:49,023
Vì vậy, đối với những ai muốn tìm hiểu sâu hơn, video tiếp theo sẽ trình bày những

165
00:11:49,023 --> 00:11:53,216
ý tưởng tương tự vừa được trình bày ở đây, nhưng về mặt phép tính cơ bản, hy vọng

166
00:11:53,216 --> 00:11:57,460
sẽ làm cho nó quen thuộc hơn một chút khi bạn xem chủ đề trong các nguồn lực khác.

167
00:11:57,460 --> 00:12:02,016
Trước đó, một điều đáng nhấn mạnh là để thuật toán này hoạt động và điều này áp dụng

168
00:12:02,016 --> 00:12:06,840
cho tất cả các loại máy học ngoài mạng lưới thần kinh, bạn cần rất nhiều dữ liệu đào tạo.

169
00:12:06,840 --> 00:12:11,035
Trong trường hợp của chúng tôi, một điều khiến các chữ số viết tay trở thành một ví

170
00:12:11,035 --> 00:12:15,380
dụ hay là tồn tại cơ sở dữ liệu MNIST, với rất nhiều ví dụ đã được con người gắn nhãn.

171
00:12:15,380 --> 00:12:18,320
Vì vậy, một thách thức chung mà những người làm việc trong lĩnh vực

172
00:12:18,320 --> 00:12:21,303
học máy sẽ quen thuộc là lấy dữ liệu huấn luyện được gắn nhãn mà bạn

173
00:12:21,303 --> 00:12:24,200
thực sự cần, cho dù đó là yêu cầu mọi người gắn nhãn cho hàng chục

174
00:12:24,200 --> 00:12:27,400
nghìn hình ảnh hay bất kỳ loại dữ liệu nào khác mà bạn có thể đang xử lý.

