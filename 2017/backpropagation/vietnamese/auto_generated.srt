1
00:00:00,000 --> 00:00:04,442
Ở đây, chúng tôi giải quyết vấn đề lan truyền ngược,

2
00:00:04,442 --> 00:00:09,640
thuật toán cốt lõi đằng sau cách mạng lưới thần kinh học hỏi.

3
00:00:09,640 --> 00:00:12,184
Sau khi tóm tắt nhanh về vị trí của chúng ta, điều đầu tiên

4
00:00:12,184 --> 00:00:14,813
tôi sẽ làm là hướng dẫn trực quan về những gì thuật toán thực

5
00:00:14,813 --> 00:00:17,400
sự đang thực hiện mà không cần tham chiếu đến các công thức.

6
00:00:17,400 --> 00:00:20,140
Sau đó, dành cho những ai muốn đi sâu vào toán học,

7
00:00:20,140 --> 00:00:24,040
video tiếp theo sẽ đi sâu vào phép tính cơ bản của tất cả những điều này.

8
00:00:24,040 --> 00:00:27,581
Nếu bạn đã xem hai video cuối cùng hoặc nếu bạn chỉ bắt đầu với nền tảng thích hợp,

9
00:00:27,581 --> 00:00:31,080
thì bạn sẽ biết mạng lưới thần kinh là gì và cách nó truyền thông tin chuyển tiếp.

10
00:00:31,080 --> 00:00:35,644
Ở đây, chúng tôi đang thực hiện một ví dụ cổ điển về việc nhận dạng các chữ

11
00:00:35,644 --> 00:00:40,330
số viết tay có giá trị pixel được đưa vào lớp đầu tiên của mạng có 784 nơ-ron

12
00:00:40,330 --> 00:00:44,834
và tôi đã hiển thị một mạng có hai lớp ẩn, mỗi lớp chỉ có 16 nơ-ron và một

13
00:00:44,834 --> 00:00:49,520
đầu ra lớp gồm 10 nơ-ron, cho biết mạng đang chọn chữ số nào làm câu trả lời.

14
00:00:49,520 --> 00:00:53,683
Tôi cũng mong bạn hiểu độ dốc giảm dần, như được mô tả trong

15
00:00:53,683 --> 00:00:57,847
video trước và ý nghĩa của việc học là chúng tôi muốn tìm ra

16
00:00:57,847 --> 00:01:02,080
trọng số và độ lệch nào giảm thiểu một hàm chi phí nhất định.

17
00:01:02,080 --> 00:01:05,631
Xin nhắc lại, với chi phí cho một ví dụ đào tạo,

18
00:01:05,631 --> 00:01:10,994
bạn lấy đầu ra mà mạng cung cấp, cùng với đầu ra mà bạn muốn nó cung cấp,

19
00:01:10,994 --> 00:01:15,560
rồi cộng các bình phương của sự khác biệt giữa mỗi thành phần.

20
00:01:15,560 --> 00:01:19,300
Thực hiện việc này cho tất cả hàng chục nghìn ví dụ đào tạo của bạn và tính

21
00:01:19,300 --> 00:01:23,040
trung bình các kết quả, điều này sẽ mang lại cho bạn tổng chi phí của mạng.

22
00:01:23,040 --> 00:01:27,935
Như thể nghĩ thế vẫn chưa đủ, như được mô tả trong video trước,

23
00:01:27,935 --> 00:01:32,983
thứ mà chúng ta đang tìm kiếm là gradient âm của hàm chi phí này,

24
00:01:32,983 --> 00:01:38,031
nó cho bạn biết cách bạn cần thay đổi tất cả trọng số và độ lệch,

25
00:01:38,031 --> 00:01:43,080
tất cả những kết nối này, để giảm chi phí một cách hiệu quả nhất.

26
00:01:43,080 --> 00:01:46,206
Lan truyền ngược, chủ đề của video này, là một

27
00:01:46,206 --> 00:01:49,600
thuật toán để tính toán độ dốc cực kỳ phức tạp đó.

28
00:01:49,600 --> 00:01:54,487
Một ý tưởng từ video trước mà tôi thực sự muốn bạn ghi nhớ ngay bây giờ là vì coi

29
00:01:54,487 --> 00:01:58,838
vectơ gradient như một hướng trong 13.000 chiều, nói một cách nhẹ nhàng,

30
00:01:58,838 --> 00:02:02,951
ngoài phạm vi trí tưởng tượng của chúng ta, còn có một ý tưởng khác.

31
00:02:02,951 --> 00:02:04,620
cách bạn có thể nghĩ về nó.

32
00:02:04,620 --> 00:02:08,348
Độ lớn của từng thành phần ở đây cho bạn biết mức độ nhạy

33
00:02:08,348 --> 00:02:11,820
cảm của hàm chi phí đối với từng trọng số và độ lệch.

34
00:02:11,820 --> 00:02:17,130
Ví dụ: giả sử bạn thực hiện quy trình mà tôi sắp mô tả và tính gradient

35
00:02:17,130 --> 00:02:22,367
âm và thành phần liên quan đến trọng số trên cạnh này ở đây sẽ là 3.2,

36
00:02:22,367 --> 00:02:26,940
trong khi thành phần được liên kết với cạnh này ở đây là 0.1.

37
00:02:26,940 --> 00:02:32,968
Theo cách bạn giải thích thì chi phí của hàm nhạy cảm hơn 32 lần với những thay đổi về

38
00:02:32,968 --> 00:02:37,611
trọng số đầu tiên đó, vì vậy nếu bạn thay đổi giá trị đó một chút,

39
00:02:37,611 --> 00:02:43,847
điều đó sẽ gây ra một số thay đổi về chi phí và thay đổi đó lớn hơn 32 lần so với lực lắc

40
00:02:43,847 --> 00:02:45,580
của vật nặng thứ hai đó.

41
00:02:45,580 --> 00:02:50,456
Cá nhân tôi, khi lần đầu tiên tìm hiểu về lan truyền ngược,

42
00:02:50,456 --> 00:02:55,820
tôi nghĩ khía cạnh khó hiểu nhất chỉ là ký hiệu và chỉ số của nó.

43
00:02:55,820 --> 00:03:00,658
Nhưng một khi bạn khám phá ra chức năng thực sự của từng phần của thuật toán này,

44
00:03:00,658 --> 00:03:04,199
mỗi hiệu ứng riêng lẻ mà nó mang lại thực sự khá trực quan,

45
00:03:04,199 --> 00:03:07,740
chỉ là có rất nhiều điều chỉnh nhỏ được xếp chồng lên nhau.

46
00:03:07,740 --> 00:03:12,410
Vì vậy, tôi sẽ bắt đầu mọi thứ ở đây với việc hoàn toàn không quan tâm đến ký

47
00:03:12,410 --> 00:03:17,380
hiệu và chỉ xem qua tác động của mỗi ví dụ huấn luyện đối với trọng số và độ lệch.

48
00:03:17,380 --> 00:03:22,127
Bởi vì hàm chi phí liên quan đến việc tính trung bình một chi phí nhất định cho

49
00:03:22,127 --> 00:03:25,509
mỗi ví dụ trong tất cả hàng chục nghìn ví dụ huấn luyện,

50
00:03:25,509 --> 00:03:30,197
nên cách chúng ta điều chỉnh trọng số và độ lệch cho một bước giảm độ dốc cũng

51
00:03:30,197 --> 00:03:31,740
phụ thuộc vào từng ví dụ.

52
00:03:31,740 --> 00:03:35,146
Hay đúng hơn, về nguyên tắc là như vậy, nhưng để đạt hiệu quả tính toán,

53
00:03:35,146 --> 00:03:39,206
chúng tôi sẽ thực hiện một thủ thuật nhỏ sau để giúp bạn không cần phải xem từng ví dụ

54
00:03:39,206 --> 00:03:39,860
cho mỗi bước.

55
00:03:39,860 --> 00:03:43,320
Trong các trường hợp khác, ngay bây giờ, tất cả những gì chúng ta sẽ

56
00:03:43,320 --> 00:03:46,780
làm là tập trung sự chú ý vào một ví dụ duy nhất, hình ảnh số 2 này.

57
00:03:46,780 --> 00:03:51,740
Ví dụ đào tạo này sẽ có ảnh hưởng gì đến cách điều chỉnh trọng số và độ lệch?

58
00:03:51,740 --> 00:03:56,402
Giả sử chúng ta đang ở thời điểm mạng chưa được đào tạo tốt, do đó,

59
00:03:56,402 --> 00:04:01,614
kích hoạt ở đầu ra sẽ trông khá ngẫu nhiên, có thể giống như 0.5, 0.8, 0.2,

60
00:04:01,614 --> 00:04:02,780
cứ thế tiếp tục.

61
00:04:02,780 --> 00:04:05,677
Chúng tôi không thể trực tiếp thay đổi những kích hoạt đó,

62
00:04:05,677 --> 00:04:08,231
chúng tôi chỉ có ảnh hưởng đến trọng số và độ lệch,

63
00:04:08,231 --> 00:04:11,670
nhưng sẽ rất hữu ích khi theo dõi những điều chỉnh nào chúng tôi muốn

64
00:04:11,670 --> 00:04:13,340
sẽ diễn ra đối với lớp đầu ra đó.

65
00:04:13,340 --> 00:04:16,218
Và vì chúng tôi muốn nó phân loại hình ảnh thành 2,

66
00:04:16,218 --> 00:04:20,426
nên chúng tôi muốn giá trị thứ ba đó được nâng lên trong khi tất cả các giá

67
00:04:20,426 --> 00:04:21,700
trị khác bị đẩy xuống.

68
00:04:21,700 --> 00:04:25,828
Hơn nữa, kích thước của những cú hích này phải tỷ lệ thuận với

69
00:04:25,828 --> 00:04:30,220
khoảng cách giữa mỗi giá trị hiện tại với giá trị mục tiêu của nó.

70
00:04:30,220 --> 00:04:35,964
Ví dụ, việc tăng cường kích hoạt tế bào thần kinh số 2 theo một nghĩa nào đó quan

71
00:04:35,964 --> 00:04:42,060
trọng hơn việc giảm kích hoạt tế bào thần kinh số 8, vốn đã khá gần với mức cần thiết.

72
00:04:42,060 --> 00:04:45,457
Vì vậy, hãy phóng to hơn nữa, hãy tập trung vào một nơ-ron này,

73
00:04:45,457 --> 00:04:47,900
nơ-ron mà chúng ta muốn tăng cường kích hoạt.

74
00:04:47,900 --> 00:04:52,610
Hãy nhớ rằng, kích hoạt đó được xác định là tổng có trọng số nhất định

75
00:04:52,610 --> 00:04:56,326
của tất cả các kích hoạt ở lớp trước, cộng với độ lệch,

76
00:04:56,326 --> 00:05:01,900
sau đó tất cả được cắm vào một cái gì đó như hàm sigmoid squishification hoặc ReLU.

77
00:05:01,900 --> 00:05:08,060
Vì vậy, có ba cách khác nhau có thể hợp tác với nhau để giúp tăng cường sự kích hoạt đó.

78
00:05:08,060 --> 00:05:15,300
Bạn có thể tăng độ lệch, có thể tăng trọng số và có thể thay đổi kích hoạt từ lớp trước.

79
00:05:15,300 --> 00:05:18,217
Tập trung vào cách điều chỉnh trọng số, chú ý xem các

80
00:05:18,217 --> 00:05:21,460
trọng số thực sự có mức độ ảnh hưởng khác nhau như thế nào.

81
00:05:21,460 --> 00:05:26,478
Các kết nối với các nơ-ron sáng nhất từ lớp trước có tác động lớn

82
00:05:26,478 --> 00:05:31,420
nhất vì các trọng số đó được nhân với giá trị kích hoạt lớn hơn.

83
00:05:31,420 --> 00:05:34,391
Vì vậy, nếu bạn tăng một trong những trọng số đó,

84
00:05:34,391 --> 00:05:38,611
nó thực sự có ảnh hưởng mạnh hơn đến hàm chi phí cuối cùng so với việc

85
00:05:38,611 --> 00:05:41,761
tăng trọng số của các kết nối với các nơ-ron mờ hơn,

86
00:05:41,761 --> 00:05:44,020
ít nhất là đối với ví dụ đào tạo này.

87
00:05:44,020 --> 00:05:47,227
Hãy nhớ rằng, khi nói về việc giảm độ dốc, chúng tôi không chỉ quan

88
00:05:47,227 --> 00:05:50,152
tâm đến việc mỗi thành phần nên được nâng lên hay giảm xuống,

89
00:05:50,152 --> 00:05:54,020
mà chúng tôi còn quan tâm đến thành phần nào mang lại cho bạn nhiều lợi ích nhất.

90
00:05:54,020 --> 00:05:58,291
Nhân tiện, điều này ít nhất gợi nhớ đến một lý thuyết trong khoa học thần kinh về

91
00:05:58,291 --> 00:06:02,303
cách mạng lưới sinh học của các tế bào thần kinh học hỏi, lý thuyết Hebbian,

92
00:06:02,303 --> 00:06:06,940
thường được tóm tắt trong cụm từ, các tế bào thần kinh hoạt động cùng nhau nối với nhau.

93
00:06:06,940 --> 00:06:11,542
Ở đây, trọng lượng tăng lên nhiều nhất, sự tăng cường lớn nhất của các kết nối,

94
00:06:11,542 --> 00:06:15,281
xảy ra giữa các tế bào thần kinh hoạt động mạnh nhất và những tế

95
00:06:15,281 --> 00:06:18,100
bào mà chúng ta mong muốn trở nên năng động hơn.

96
00:06:18,100 --> 00:06:21,770
Theo một nghĩa nào đó, các tế bào thần kinh kích hoạt khi nhìn thấy số 2 sẽ có

97
00:06:21,770 --> 00:06:25,440
mối liên kết chặt chẽ hơn với những tế bào thần kinh kích hoạt khi nghĩ về nó.

98
00:06:25,440 --> 00:06:29,620
Nói rõ hơn, tôi không có đủ tư cách để đưa ra tuyên bố theo cách này hay cách khác

99
00:06:29,620 --> 00:06:34,103
về việc liệu mạng lưới nơ-ron nhân tạo có hoạt động giống như bộ não sinh học hay không,

100
00:06:34,103 --> 00:06:37,881
và ý tưởng này kết hợp với nhau đi kèm với một vài dấu hoa thị có ý nghĩa,

101
00:06:37,881 --> 00:06:41,760
nhưng được coi là rất lỏng lẻo. sự tương tự, tôi thấy thật thú vị khi lưu ý.

102
00:06:41,760 --> 00:06:45,615
Dù sao đi nữa, cách thứ ba chúng ta có thể giúp tăng cường kích hoạt

103
00:06:45,615 --> 00:06:49,360
tế bào thần kinh này là thay đổi tất cả các kích hoạt ở lớp trước.

104
00:06:49,360 --> 00:06:53,705
Cụ thể, nếu mọi thứ kết nối với nơron số 2 có trọng số dương

105
00:06:53,705 --> 00:06:58,263
trở nên sáng hơn và nếu mọi thứ kết nối với nơron số 2 có trọng

106
00:06:58,263 --> 00:07:02,680
số âm trở nên mờ hơn thì nơron số 2 đó sẽ hoạt động mạnh hơn.

107
00:07:02,680 --> 00:07:04,909
Và tương tự như những thay đổi về trọng lượng,

108
00:07:04,909 --> 00:07:09,084
bạn sẽ thu được lợi ích lớn nhất bằng cách tìm kiếm những thay đổi tỷ lệ thuận với kích

109
00:07:09,084 --> 00:07:10,840
thước của các trọng lượng tương ứng.

110
00:07:10,840 --> 00:07:15,393
Tất nhiên, hiện tại, chúng tôi không thể tác động trực tiếp đến những kích hoạt đó,

111
00:07:15,393 --> 00:07:18,320
chúng tôi chỉ có quyền kiểm soát trọng số và độ lệch.

112
00:07:18,320 --> 00:07:21,111
Nhưng cũng giống như lớp cuối cùng, việc ghi lại

113
00:07:21,111 --> 00:07:23,960
những thay đổi mong muốn đó là gì sẽ rất hữu ích.

114
00:07:23,960 --> 00:07:27,132
Nhưng hãy nhớ rằng, thu nhỏ một bước ở đây, đây

115
00:07:27,132 --> 00:07:30,040
chỉ là điều mà nơ-ron đầu ra chữ số 2 muốn.

116
00:07:30,040 --> 00:07:34,448
Hãy nhớ rằng, chúng ta cũng muốn tất cả các nơ-ron khác ở lớp cuối

117
00:07:34,448 --> 00:07:38,791
cùng trở nên ít hoạt động hơn và mỗi nơ-ron đầu ra khác đó có suy

118
00:07:38,791 --> 00:07:43,200
nghĩ riêng về điều gì sẽ xảy ra với lớp thứ hai đến lớp cuối cùng.

119
00:07:43,200 --> 00:07:49,356
Vì vậy, mong muốn của nơ-ron chữ số 2 này được cộng thêm cùng với mong muốn của tất cả

120
00:07:49,356 --> 00:07:54,875
các nơ-ron đầu ra khác về điều gì sẽ xảy ra từ lớp thứ hai đến lớp cuối cùng,

121
00:07:54,875 --> 00:08:01,032
một lần nữa theo tỷ lệ với trọng số tương ứng và tỷ lệ với mỗi nơ-ron đó cần bao nhiêu

122
00:08:01,032 --> 00:08:01,740
thay đổi.

123
00:08:01,740 --> 00:08:05,940
Đây chính là nơi mà ý tưởng truyền bá ngược xuất hiện.

124
00:08:05,940 --> 00:08:09,056
Bằng cách cộng tất cả các hiệu ứng mong muốn này lại với nhau,

125
00:08:09,056 --> 00:08:13,162
về cơ bản bạn sẽ có được một danh sách các cú hích mà bạn muốn thực hiện ở lớp thứ

126
00:08:13,162 --> 00:08:14,300
hai đến lớp cuối cùng.

127
00:08:14,300 --> 00:08:19,303
Và khi bạn đã có những thứ đó, bạn có thể áp dụng đệ quy quy trình tương tự

128
00:08:19,303 --> 00:08:23,846
cho các trọng số và độ lệch có liên quan để xác định các giá trị đó,

129
00:08:23,846 --> 00:08:29,180
lặp lại quy trình tương tự mà tôi vừa thực hiện và di chuyển ngược lại qua mạng.

130
00:08:29,180 --> 00:08:33,382
Và thu nhỏ hơn một chút, hãy nhớ rằng đây chỉ là cách một ví dụ

131
00:08:33,382 --> 00:08:37,520
đào tạo duy nhất muốn thúc đẩy từng trọng số và thành kiến đó.

132
00:08:37,520 --> 00:08:40,705
Nếu chúng ta chỉ lắng nghe những gì thứ 2 đó muốn thì cuối cùng

133
00:08:40,705 --> 00:08:44,140
mạng sẽ được khuyến khích chỉ phân loại tất cả hình ảnh thành thứ 2.

134
00:08:44,140 --> 00:08:50,048
Vì vậy, những gì bạn làm là thực hiện quy trình hỗ trợ tương tự này

135
00:08:50,048 --> 00:08:57,521
cho mọi ví dụ đào tạo khác, ghi lại cách mỗi ví dụ muốn thay đổi trọng số và độ lệch,

136
00:08:57,521 --> 00:09:02,300
đồng thời tính trung bình những thay đổi mong muốn đó.

137
00:09:02,300 --> 00:09:06,669
Bộ sưu tập ở đây gồm các mức tăng trung bình cho từng trọng số và độ lệch,

138
00:09:06,669 --> 00:09:10,631
nói một cách lỏng lẻo, là độ dốc âm của hàm chi phí được tham chiếu

139
00:09:10,631 --> 00:09:14,360
trong video trước hoặc ít nhất là thứ gì đó tỷ lệ thuận với nó.

140
00:09:14,360 --> 00:09:19,295
Tôi nói một cách lỏng lẻo chỉ vì tôi vẫn chưa hiểu chính xác về mặt định lượng về

141
00:09:19,295 --> 00:09:23,447
những cú hích đó, nhưng nếu bạn hiểu mọi thay đổi mà tôi vừa đề cập,

142
00:09:23,447 --> 00:09:28,382
tại sao một số thay đổi lại lớn hơn những thay đổi khác theo tỷ lệ và cách tất cả

143
00:09:28,382 --> 00:09:33,317
chúng cần được cộng lại với nhau, bạn sẽ hiểu cơ chế của lan truyền ngược thực sự

144
00:09:33,317 --> 00:09:34,100
đang làm gì.

145
00:09:34,100 --> 00:09:38,672
Nhân tiện, trên thực tế, máy tính phải mất một thời gian rất dài để cộng

146
00:09:38,672 --> 00:09:43,120
dồn mức độ ảnh hưởng của từng ví dụ huấn luyện ở mỗi bước giảm độ dốc.

147
00:09:43,120 --> 00:09:45,540
Vì vậy, đây là những gì thường được thực hiện thay thế.

148
00:09:45,540 --> 00:09:50,938
Bạn xáo trộn ngẫu nhiên dữ liệu huấn luyện của mình và chia nó thành nhiều đợt nhỏ,

149
00:09:50,938 --> 00:09:53,380
giả sử mỗi đợt có 100 mẫu huấn luyện.

150
00:09:53,380 --> 00:09:56,980
Sau đó, bạn tính toán một bước theo lô nhỏ.

151
00:09:56,980 --> 00:10:01,693
Đó không phải là độ dốc thực tế của hàm chi phí, phụ thuộc vào tất cả dữ liệu huấn luyện,

152
00:10:01,693 --> 00:10:06,249
không phải tập hợp con nhỏ này, vì vậy đây không phải là bước xuống dốc hiệu quả nhất,

153
00:10:06,249 --> 00:10:10,281
nhưng mỗi lô nhỏ sẽ cung cấp cho bạn một xấp xỉ khá tốt và quan trọng hơn là

154
00:10:10,281 --> 00:10:12,900
nó cung cấp cho bạn một tốc độ tính toán đáng kể.

155
00:10:12,900 --> 00:10:16,465
Nếu bạn lập biểu đồ quỹ đạo của mạng theo bề mặt chi phí liên quan,

156
00:10:16,465 --> 00:10:20,922
thì nó sẽ giống một người đàn ông say rượu vấp ngã không mục đích xuống một ngọn đồi

157
00:10:20,922 --> 00:10:25,484
nhưng thực hiện những bước đi nhanh chóng, hơn là một người đàn ông tính toán cẩn thận

158
00:10:25,484 --> 00:10:30,204
xác định hướng xuống dốc chính xác của mỗi bước trước khi thực hiện bước đi thật chậm rãi

159
00:10:30,204 --> 00:10:31,620
và cẩn thận theo hướng đó.

160
00:10:31,620 --> 00:10:35,200
Kỹ thuật này được gọi là giảm độ dốc ngẫu nhiên.

161
00:10:35,200 --> 00:10:40,400
Có rất nhiều điều đang diễn ra ở đây, vì vậy chúng ta hãy tự tổng hợp lại nhé?

162
00:10:40,400 --> 00:10:45,635
Lan truyền ngược là thuật toán để xác định cách một ví dụ huấn luyện muốn điều

163
00:10:45,635 --> 00:10:50,871
chỉnh trọng số và độ lệch, không chỉ về việc chúng nên tăng hay giảm mà còn về

164
00:10:50,871 --> 00:10:56,240
tỷ lệ tương đối với những thay đổi đó gây ra sự giảm nhanh nhất đối với trị giá.

165
00:10:56,240 --> 00:11:00,707
Bước giảm độ dốc thực sự sẽ liên quan đến việc thực hiện việc này cho tất cả hàng

166
00:11:00,707 --> 00:11:05,174
chục nghìn ví dụ đào tạo của bạn và tính trung bình các thay đổi mong muốn mà bạn

167
00:11:05,174 --> 00:11:08,879
nhận được, nhưng việc đó chậm về mặt tính toán, vì vậy thay vào đó,

168
00:11:08,879 --> 00:11:13,346
bạn chia ngẫu nhiên dữ liệu thành các lô nhỏ và tính toán từng bước tương ứng với

169
00:11:13,346 --> 00:11:14,000
một lô nhỏ.

170
00:11:14,000 --> 00:11:19,202
Liên tục thực hiện tất cả các đợt nhỏ và thực hiện những điều chỉnh này,

171
00:11:19,202 --> 00:11:23,121
bạn sẽ hội tụ về mức tối thiểu cục bộ của hàm chi phí,

172
00:11:23,121 --> 00:11:27,540
nghĩa là mạng của bạn sẽ thực hiện rất tốt các ví dụ đào tạo.

173
00:11:27,540 --> 00:11:32,548
Vì vậy, với tất cả những gì đã nói, mọi dòng mã dùng để triển khai backprop thực

174
00:11:32,548 --> 00:11:37,680
sự tương ứng với những gì bạn đã thấy, ít nhất là theo thuật ngữ không chính thức.

175
00:11:37,680 --> 00:11:40,915
Nhưng đôi khi biết những gì toán học làm mới chỉ là một nửa trận chiến,

176
00:11:40,915 --> 00:11:44,780
và chỉ việc trình bày cái thứ chết tiệt đó là mọi thứ sẽ trở nên lộn xộn và khó hiểu.

177
00:11:44,780 --> 00:11:47,234
Vì vậy, đối với những ai muốn tìm hiểu sâu hơn,

178
00:11:47,234 --> 00:11:51,222
video tiếp theo sẽ trình bày những ý tưởng tương tự vừa được trình bày ở đây,

179
00:11:51,222 --> 00:11:55,363
nhưng về mặt phép tính cơ bản, hy vọng sẽ làm cho nó quen thuộc hơn một chút khi

180
00:11:55,363 --> 00:11:57,460
bạn xem chủ đề trong các nguồn lực khác.

181
00:11:57,460 --> 00:12:02,016
Trước đó, một điều đáng nhấn mạnh là để thuật toán này hoạt động và điều này áp dụng

182
00:12:02,016 --> 00:12:06,840
cho tất cả các loại máy học ngoài mạng lưới thần kinh, bạn cần rất nhiều dữ liệu đào tạo.

183
00:12:06,840 --> 00:12:11,035
Trong trường hợp của chúng tôi, một điều khiến các chữ số viết tay trở thành một ví

184
00:12:11,035 --> 00:12:15,380
dụ hay là tồn tại cơ sở dữ liệu MNIST, với rất nhiều ví dụ đã được con người gắn nhãn.

185
00:12:15,380 --> 00:12:18,320
Vì vậy, một thách thức chung mà những người làm việc trong lĩnh vực

186
00:12:18,320 --> 00:12:21,865
học máy sẽ quen thuộc là lấy dữ liệu huấn luyện được gắn nhãn mà bạn thực sự cần,

187
00:12:21,865 --> 00:12:24,848
cho dù đó là yêu cầu mọi người gắn nhãn cho hàng chục nghìn hình ảnh

188
00:12:24,848 --> 00:12:27,400
hay bất kỳ loại dữ liệu nào khác mà bạn có thể đang xử lý.

