[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn. ",
  "translatedText": "Ở đây, chúng ta sẽ giải quyết 'lan truyền ngược', thuật toán cốt lõi đằng sau cách mạng lưới thần kinh học hỏi. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. ",
  "translatedText": "Sau khi tóm tắt nhanh về những điều chúng ta đã biết, điều đầu tiên tôi sẽ làm là hướng dẫn một cách trực quan về những gì thuật toán thực sự đang thực hiện mà không cần nhắc đến đến các công thức. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. ",
  "translatedText": "Sau đó, dành cho những ai muốn đi sâu vào toán học đứng sau, video tiếp theo sẽ đi sâu vào các tính toán của tất cả điều này. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. ",
  "translatedText": "Nếu bạn đã xem hai video cuối cùng hoặc nếu bạn chỉ bắt đầu với nền tảng thích hợp, thì bạn sẽ biết mạng lưới thần kinh là gì và cách nó truyền thông tin đi. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. ",
  "translatedText": "Ở đây, chúng tôi đang thực hiện một ví dụ kinh điển về việc nhận dạng các chữ số viết tay có giá trị pixel được đưa vào lớp đầu tiên của mạng có 784 nơ-ron và tôi đã sử dụng một mạng có hai lớp ẩn, mỗi lớp chỉ có 16 nơ-ron và một đầu ra lớp gồm 10 nơ-ron, cho biết mạng đang chọn chữ số nào làm câu trả lời. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. ",
  "translatedText": "Tôi cũng mong bạn hiểu khái niệm hạ gradient, như đã được mô tả trong video trước và ý nghĩa của việc học là tìm ra trọng số và độ lệch thích hợp để giảm thiểu một hàm chi phí nhất định. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. ",
  "translatedText": "Xin nhắc lại, để tính chi phí cho một mẫu huấn luyện, bạn lấy đầu ra mà mạng đưa ra, cùng với đầu ra mà bạn muốn nó đưa ra, rồi cộng các bình phương của hiệu giữa mỗi thành phần. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. ",
  "translatedText": "Thực hiện việc này cho tất cả hàng chục nghìn mẫu huấn luyện của bạn và tính trung bình các kết quả, điều này sẽ mang lại cho bạn tổng chi phí của mạng. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. ",
  "translatedText": "Chưa hết, như được mô tả trong video trước, thứ mà chúng ta đang tìm kiếm là gradient âm của hàm chi phí này, nó cho bạn biết cách bạn cần thay đổi tất cả trọng số và độ lệch, tất cả những kết nối này, để giảm chi phí một cách hiệu quả nhất. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. ",
  "translatedText": "Lan truyền ngược, chủ đề của video này, là một thuật toán để tính toán giá trị gradient cực kỳ phức tạp đó. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "The one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. ",
  "translatedText": "Một ý tưởng từ video trước mà tôi thực sự muốn bạn ghi nhớ ngay bây giờ là bởi vì việc coi vectơ gradient như một hướng trong không gian 13000 chiều, nói một cách nhẹ nhàng, ngoài phạm vi trí tưởng tượng của chúng ta, còn một cách khác bạn có thể nghĩ về nó. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. ",
  "translatedText": "Độ lớn của từng thành phần ở đây cho bạn biết mức độ nhạy cảm của hàm chi phí đối với từng trọng số và độ lệch. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. ",
  "translatedText": "Ví dụ: giả sử bạn thực hiện quy trình mà tôi sắp mô tả và tính gradient âm và thành phần liên quan đến trọng số trên cạnh này là 3.2, trong khi thành phần được liên kết với cạnh này là 0.1. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. ",
  "translatedText": "Bạn có thể hiểu là chi phí của hàm nhạy cảm hơn 32 lần với những thay đổi về trọng số đầu tiên đó, vì vậy nếu bạn thay đổi giá trị đó một chút, điều đó sẽ gây ra một số thay đổi về chi phí và thay đổi đó lớn hơn 32 lần so với thay đổi mà việc chỉnh sửa trọng số thứ hai gây ra. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and index chasing of it all. ",
  "translatedText": "Cá nhân tôi, khi lần đầu tiên tìm hiểu về lan truyền ngược, tôi nghĩ khía cạnh khó hiểu nhất chỉ là ký hiệu và chỉ số của nó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. ",
  "translatedText": "Nhưng một khi bạn khám phá ra chức năng thực sự của từng phần của thuật toán này, mỗi hiệu ứng riêng lẻ mà nó mang lại thực sự khá trực quan, chỉ là nó có rất nhiều điều chỉnh nhỏ được xếp chồng lên nhau. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. ",
  "translatedText": "Vì vậy, tôi sẽ bắt đầu mọi thứ ở đây mà hoàn toàn không quan tâm đến ký hiệu và chỉ xem qua tác động của mỗi mẫu huấn luyện đối với trọng số và độ lệch. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. ",
  "translatedText": "Bởi vì hàm chi phí liên quan đến việc tính trung bình một chi phí nhất định cho mỗi mẫu trong tất cả hàng chục nghìn mẫu huấn luyện, nên cách chúng ta điều chỉnh trọng số và độ lệch cho một bước giảm gradient cũng phụ thuộc vào từng mẫu. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. ",
  "translatedText": "Hay đúng hơn, về nguyên tắc là như vậy, nhưng để đạt hiệu quả tính toán, chúng ta sẽ thực hiện một thủ thuật nhỏ sau để giúp bạn không cần phải xem từng mẫu cho mỗi bước. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. ",
  "translatedText": "Trong các trường hợp khác, ngay bây giờ, tất cả những gì chúng ta sẽ làm là tập trung sự chú ý vào một mẫu duy nhất, hình ảnh số 2 này. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted? ",
  "translatedText": "Mẫu huấn luyện này sẽ có ảnh hưởng gì đến cách điều chỉnh trọng số và độ lệch? ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. ",
  "translatedText": "Giả sử chúng ta đang ở thời điểm mạng chưa được đào tạo tốt, do đó, giá trị kích hoạt ở đầu ra sẽ trông khá ngẫu nhiên, có thể giống như 0.5, 0.8, 0.2, cứ thế tiếp tục. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases, but it's helpful to keep track of which adjustments we wish should take place to that output layer. ",
  "translatedText": "Chúng ta không thể trực tiếp thay đổi những giá trị kích hoạt đó, chúng ta chỉ có thể thay đổi trọng số và độ lệch, nhưng sẽ rất hữu ích khi theo dõi những thay đổi nào chúng ta muốn diễn ra đối với lớp đầu ra đó. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 242.52,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. ",
  "translatedText": "Và vì chúng ta muốn nó phân loại hình ảnh thành số 2, nên chúng ta muốn giá trị thứ ba đó được nâng lên trong khi tất cả các giá trị khác bị đẩy xuống. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. ",
  "translatedText": "Hơn nữa, kích thước của những thay đổi này phải tỷ lệ thuận với khoảng cách giữa mỗi giá trị hiện tại với giá trị mục tiêu của nó. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. ",
  "translatedText": "Ví dụ, việc tăng giá trị kích hoạt nơ-ron số 2 theo một nghĩa nào đó quan trọng hơn việc giảm giá trị kích hoạt nơ-ron số 8, vốn đã khá gần với mức cần thiết. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. ",
  "translatedText": "Vì vậy, nhìn sâu hơn nữa, hãy tập trung vào một nơ-ron này, nơ-ron mà chúng ta muốn tăng giá trị kích hoạt. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. ",
  "translatedText": "Hãy nhớ rằng, giá trị kích hoạt đó được xác định là tổng có trọng số của tất cả các giá trị kích hoạt ở lớp trước, cộng với độ lệch, sau đó tất cả được đưa vào một hàm nào đó như hàm sigmoid squishification hoặc ReLU. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation. ",
  "translatedText": "Vì vậy, có ba cách khác nhau có thể cùng nhau tăng cường giá trị kích hoạt đó. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. ",
  "translatedText": "Bạn có thể tăng độ lệch, có thể tăng trọng số và có thể thay đổi giá trị kích hoạt từ lớp trước. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. ",
  "translatedText": "Bây giờ hãy tập trung vào cách điều chỉnh trọng số, chú ý xem các trọng số thực sự có mức độ ảnh hưởng khác nhau như thế nào. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. ",
  "translatedText": "Các kết nối với các nơ-ron sáng nhất từ lớp trước có tác động lớn nhất vì các trọng số đó được nhân với giá trị kích hoạt lớn hơn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. ",
  "translatedText": "Vì vậy, nếu bạn tăng một trong những trọng số đó, nó thực sự có ảnh hưởng mạnh hơn đến hàm chi phí cuối cùng so với việc tăng trọng số của các kết nối với các nơ-ron mờ hơn, ít nhất là đối với mẫu huấn luyện này. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck. ",
  "translatedText": "Hãy nhớ rằng, khi nói về việc giảm gradient, chúng ta không chỉ quan tâm đến việc mỗi thành phần nên được nâng lên hay giảm xuống, mà chúng ta còn quan tâm đến thành phần nào mang lại cho bạn nhiều lợi ích nhất. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together. ",
  "translatedText": "Nhân tiện, điều này ít nhất gợi nhớ đến một lý thuyết trong khoa học thần kinh về cách mạng lưới sinh học của các nơ-ron học hỏi, lý thuyết Hebbian, thường được tóm tắt trong cụm từ, các nơ-ron hoạt động cùng nhau thì nối với nhau. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active and the ones which we wish to become more active. ",
  "translatedText": "Ở đây, sự tăng lên lớn nhất của trọng số, sự tăng cường lớn nhất của các kết nối, xảy ra giữa các nơ-ron hoạt động mạnh nhất và những tế bào mà chúng ta mong muốn hoạt động mạnh hơn. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those firing when thinking about it. ",
  "translatedText": "Theo một nghĩa nào đó, các nơ-ron kích hoạt khi nhìn thấy số 2 sẽ có mối liên kết chặt chẽ hơn với những nơ-ron kích hoạt khi nghĩ về nó. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note. ",
  "translatedText": "Nói rõ hơn, tôi không có đủ tư cách để đưa ra tuyên bố theo cách này hay cách khác về việc liệu mạng lưới nơ-ron nhân tạo có hoạt động giống như bộ não sinh học hay không, và ý tưởng hoạt động cùng nhau thì nối với nhau đi kèm với một vài dấu hoa thị, nhưng nếu coi như là một sự so sánh lỏng lẻo, tôi thấy thú vị khi lưu ý về điều này. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer. ",
  "translatedText": "Dù sao đi nữa, cách thứ ba chúng ta có thể giúp tăng giá trị kích hoạt nơ-ron này là thay đổi tất cả các giá trị kích hoạt ở lớp trước. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active. ",
  "translatedText": "Cụ thể, nếu mọi thứ kết nối với nơ-ron số 2 có trọng số dương trở nên sáng hơn và nếu mọi thứ kết nối với nơ-ron số 2 có trọng số âm trở nên mờ đi thì nơ-ron số 2 đó sẽ hoạt động mạnh hơn. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. ",
  "translatedText": "Và tương tự như những thay đổi về trọng số, bạn sẽ thu được lợi ích lớn nhất bằng cách tìm kiếm những thay đổi tỷ lệ thuận với kích thước của các trọng số tương ứng. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases. ",
  "translatedText": "Tất nhiên, hiện tại, chúng ta không thể tác động trực tiếp đến những giá trị kích hoạt đó, chúng ta chỉ có quyền kiểm soát trọng số và độ lệch. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are. ",
  "translatedText": "Nhưng cũng giống như lớp cuối cùng, việc ghi lại những thay đổi mong muốn đó là gì sẽ rất hữu ích. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants. ",
  "translatedText": "Nhưng hãy nhớ rằng, đi ngược lại một bước ở đây, đây chỉ là điều mà nơ-ron đầu ra chữ số 2 muốn. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. ",
  "translatedText": "Hãy nhớ rằng, chúng ta cũng muốn tất cả các nơ-ron khác ở lớp cuối cùng trở nên ít hoạt động hơn và mỗi nơ-ron đầu ra khác đó có suy nghĩ riêng về điều gì nên xảy ra với lớp gần cuối đó. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. ",
  "translatedText": "Vì vậy, mong muốn của nơ-ron chữ số 2 này được cộng thêm cùng với mong muốn của tất cả các nơ-ron đầu ra khác về điều gì nên xảy ra đến lớp gần cuối này, một lần nữa theo tỷ lệ với trọng số tương ứng và tỷ lệ với mỗi nơ-ron đó cần bao nhiêu thay đổi. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in. ",
  "translatedText": "Đây chính là nơi mà ý tưởng lan truyền ngược xuất hiện. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. ",
  "translatedText": "Bằng cách kết hợp tất cả các hiệu ứng mong muốn này lại với nhau, về cơ bản bạn sẽ có được một danh sách các thay đổi mà bạn muốn thực hiện ở lớp gần cuối này. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. ",
  "translatedText": "Và khi bạn đã có những thứ đó, bạn có thể áp dụng quy trình tương tự cho các trọng số và độ lệch có liên quan để xác định các giá trị đó, lặp lại quy trình tương tự mà tôi vừa thực hiện và di chuyển ngược lại qua mạng. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. ",
  "translatedText": "Và thu nhỏ hơn một chút, hãy nhớ rằng đây chỉ là cách một mẫu huấn luyện duy nhất muốn thay đổi từng trọng số và độ lệch đó. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2. ",
  "translatedText": "Nếu chúng ta chỉ xem xét những gì số 2 đó muốn thì cuối cùng mạng sẽ được khuyến khích chỉ phân loại tất cả hình ảnh thành số 2. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes. ",
  "translatedText": "Vì vậy, những gì bạn làm là thực hiện quy trình tương tự này cho mọi mẫu huấn luyện khác, ghi lại cách mỗi mẫu muốn thay đổi trọng số và độ lệch, đồng thời tính trung bình những thay đổi mong muốn đó. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. ",
  "translatedText": "Bộ sưu tập gồm các mức tăng trung bình cho từng trọng số và độ lệch ở đây, nói một cách lỏng lẻo, là gradient âm của hàm chi phí được nói đến trong video trước hoặc ít nhất là thứ gì đó tỷ lệ thuận với nó. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. ",
  "translatedText": "Tôi nói một cách lỏng lẻo chỉ vì tôi vẫn chưa hiểu chính xác về mặt định lượng về những thay đổi đó, nhưng nếu bạn hiểu mọi thay đổi mà tôi vừa đề cập, tại sao một số thay đổi lại lớn hơn những thay đổi khác theo tỷ lệ và cách tất cả chúng cần được kết hợp lại với nhau, bạn sẽ hiểu cơ chế của lan truyền ngược thực sự đang làm gì. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. ",
  "translatedText": "Nhân tiện, trên thực tế, máy tính phải mất một thời gian rất dài để cộng dồn mức độ ảnh hưởng của từng mẫu huấn luyện ở mỗi bước giảm gradient. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead. ",
  "translatedText": "Vì vậy, đây là những gì thường được lảm thay vào đó. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and divide it into a whole bunch of mini-batches, let's say each one having 100 training examples. ",
  "translatedText": "Bạn xáo trộn ngẫu nhiên dữ liệu huấn luyện của mình và chia nó thành nhiều đợt nhỏ, giả sử mỗi đợt có 100 mẫu huấn luyện. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch. ",
  "translatedText": "Sau đó, bạn tính toán một bước theo lô nhỏ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly it gives you a significant computational speedup. ",
  "translatedText": "Đó không phải là gradient thực tế của hàm chi phí, nó phụ thuộc vào tất cả dữ liệu huấn luyện, không phải tập hợp con nhỏ này, vì vậy đây không phải là bước xuống dốc hiệu quả nhất, nhưng mỗi lô nhỏ sẽ đưa ra cho bạn một ước lượng khá tốt và quan trọng hơn là nó cho bạn một tốc độ tính toán đáng kể. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. ",
  "translatedText": "Nếu bạn lập biểu đồ quỹ đạo của mạng trên bề mặt của hàm chi phí, thì nó sẽ giống một người đàn ông say rượu ngã không định hướng xuống một ngọn đồi nhưng thực hiện được những bước đi nhanh chóng, hơn là một người đàn ông tính toán cẩn thận để xác định hướng xuống dốc chính xác của mỗi bước trước khi thực hiện bước đi thật chậm rãi và cẩn thận theo hướng đó. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent. ",
  "translatedText": "Kỹ thuật này được gọi là giảm gradient ngẫu nhiên. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we? ",
  "translatedText": "Có rất nhiều điều đang diễn ra ở đây, vì vậy chúng ta hãy tổng hợp lại nhé? ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. ",
  "translatedText": "Lan truyền ngược là thuật toán để xác định cách một mẫu huấn luyện muốn điều chỉnh trọng số và độ lệch, không chỉ về việc chúng nên tăng hay giảm mà còn là tỷ lệ tương đối của những thay đổi đó gây ra sự giảm nhanh nhất đối với chi phí. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes you get, but that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch. ",
  "translatedText": "Bước giảm gradient thực sự sẽ liên quan đến việc thực hiện việc này cho tất cả hàng chục nghìn mẫu huấn luyện của bạn và tính trung bình các thay đổi mong muốn mà bạn nhận được, nhưng việc đó chậm về mặt tính toán, vì vậy thay vào đó, bạn chia ngẫu nhiên dữ liệu thành các lô nhỏ và tính toán từng bước tương ứng với một lô nhỏ. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 656.26,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples. ",
  "translatedText": "Liên tục thực hiện tất cả các đợt nhỏ và những điều chỉnh này, bạn sẽ đạt được cực tiểu cục bộ của hàm chi phí, nghĩa là mạng của bạn sẽ thực hiện rất tốt các mẫu huấn luyện. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms. ",
  "translatedText": "Vì vậy, với tất cả những gì đã nói, mọi dòng mã dùng để triển khai lan truyền ngược thực sự tương ứng với những gì bạn đã thấy, ít nhất là theo thuật ngữ không chính thức. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. ",
  "translatedText": "Nhưng đôi khi biết những gì toán học đằng sau làm mới chỉ là một nửa trận chiến, và chỉ việc trình bày cái thứ chết tiệt đó là lúc mọi thứ sẽ trở nên lộn xộn và khó hiểu. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So, for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. ",
  "translatedText": "Vì vậy, đối với những ai muốn tìm hiểu sâu hơn, video tiếp theo sẽ trình bày những ý tưởng tương tự vừa được trình bày ở đây, nhưng bằng các phép tính, hy vọng sẽ làm cho nó quen thuộc hơn một chút khi bạn xem chủ đề này trong các nguồn khác. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. ",
  "translatedText": "Trước đó, một điều đáng nhấn mạnh là để thuật toán này hoạt động và điều này áp dụng cho tất cả các loại học máy ngoài mạng lưới thần kinh, bạn cần rất nhiều dữ liệu đào tạo. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans. ",
  "translatedText": "Trong trường hợp của chúng ta, một điều khiến các chữ số viết tay trở thành một ví dụ hay là tồn tại cơ sở dữ liệu MNIST, với rất nhiều mẫu đã được con người phân loại. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with. ",
  "translatedText": "Vì vậy, một thách thức chung mà những người làm việc trong lĩnh vực học máy sẽ quen thuộc là lấy dữ liệu huấn luyện được đã được phân loại mà bạn thực sự cần, cho dù đó là yêu cầu nhiều người phân loại cho hàng chục nghìn hình ảnh hay bất kỳ loại dữ liệu nào khác mà bạn có thể đang xử lý. ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 735.3,
  "end": 747.1
 }
]