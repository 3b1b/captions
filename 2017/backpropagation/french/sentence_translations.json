[
 {
  "input": "Here, we tackle backpropagation, the core algorithm behind how neural networks learn. ",
  "translatedText": "Ici, nous abordons la rétropropagation, l'algorithme de base derrière la façon dont les réseaux de neurones apprennent. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.06,
  "end": 8.88
 },
 {
  "input": "After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. ",
  "translatedText": "Après un bref récapitulatif de notre situation, la première chose que je ferai est une présentation intuitive de ce que fait réellement l'algorithme, sans aucune référence aux formules. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 9.4,
  "end": 17.0
 },
 {
  "input": "Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. ",
  "translatedText": "Ensuite, pour ceux d’entre vous qui souhaitent se plonger dans les mathématiques, la vidéo suivante aborde le calcul qui sous-tend tout cela. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 17.66,
  "end": 23.02
 },
 {
  "input": "If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. ",
  "translatedText": "Si vous avez regardé les deux dernières vidéos, ou si vous vous lancez simplement dans le contexte approprié, vous savez ce qu'est un réseau neuronal et comment il transmet les informations. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.82,
  "end": 31.0
 },
 {
  "input": "Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. ",
  "translatedText": "Ici, nous faisons l'exemple classique de reconnaissance de chiffres manuscrits dont les valeurs de pixels sont introduites dans la première couche du réseau avec 784 neurones, et j'ai montré un réseau avec deux couches cachées n'ayant que 16 neurones chacune, et une sortie couche de 10 neurones, indiquant quel chiffre le réseau choisit comme réponse. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 31.68,
  "end": 49.04
 },
 {
  "input": "I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. ",
  "translatedText": "J'attends également de vous que vous compreniez la descente de gradient, comme décrit dans la dernière vidéo, et que ce que nous entendons par apprentissage, c'est que nous voulons trouver quels poids et biais minimisent une certaine fonction de coût. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 50.04,
  "end": 61.26
 },
 {
  "input": "As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. ",
  "translatedText": "Pour rappel, pour le coût d'un seul exemple de formation, vous prenez le résultat fourni par le réseau, ainsi que le résultat que vous souhaitiez qu'il donne, et additionnez les carrés des différences entre chaque composant. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 62.04,
  "end": 74.6
 },
 {
  "input": "Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. ",
  "translatedText": "En faisant cela pour l'ensemble de vos dizaines de milliers d'exemples de formation et en faisant la moyenne des résultats, vous obtenez le coût total du réseau. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 75.38,
  "end": 82.2
 },
 {
  "input": "As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. ",
  "translatedText": "Comme si cela ne suffisait pas, comme décrit dans la dernière vidéo, ce que nous recherchons est le gradient négatif de cette fonction de coût, qui vous indique comment modifier tous les poids et biais, tous ces connexions, afin de réduire le coût le plus efficacement possible. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.2,
  "end": 98.32
 },
 {
  "input": "Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. ",
  "translatedText": "La rétropropagation, le sujet de cette vidéo, est un algorithme permettant de calculer ce gradient complexe et fou. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 103.26,
  "end": 108.58
 },
 {
  "input": "The one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. ",
  "translatedText": "La seule idée de la dernière vidéo que je veux vraiment que vous gardiez fermement à l'esprit en ce moment est que parce que considérer le vecteur gradient comme une direction dans 13 000 dimensions est, pour le dire légèrement, au-delà de la portée de notre imagination, il y en a une autre. façon dont vous pouvez y penser. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.14,
  "end": 123.58
 },
 {
  "input": "The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. ",
  "translatedText": "L'ampleur de chaque composante ici vous indique à quel point la fonction de coût est sensible à chaque pondération et biais. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 124.6,
  "end": 130.94
 },
 {
  "input": "For example, let's say you go through the process I'm about to describe, and compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. ",
  "translatedText": "Par exemple, disons que vous suivez le processus que je suis sur le point de décrire et que vous calculez le gradient négatif, et que la composante associée au poids sur cette arête est ici de 3.2, tandis que la composante associée à cette arête apparaît ici comme 0.1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.8,
  "end": 146.26
 },
 {
  "input": "The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. ",
  "translatedText": "La façon dont vous interpréteriez cela est que le coût de la fonction est 32 fois plus sensible aux changements de ce premier poids, donc si vous deviez modifier un peu cette valeur, cela entraînerait une modification du coût, et ce changement est 32 fois supérieur à ce que donnerait la même variation de ce deuxième poids. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 146.82,
  "end": 163.06
 },
 {
  "input": "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and index chasing of it all. ",
  "translatedText": "Personnellement, lorsque j'ai découvert la rétropropagation pour la première fois, je pense que l'aspect le plus déroutant était simplement la notation et la recherche d'index. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.42,
  "end": 175.74
 },
 {
  "input": "But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. ",
  "translatedText": "Mais une fois que vous avez compris ce que fait réellement chaque partie de cet algorithme, chaque effet individuel qu'il produit est en fait assez intuitif, c'est juste qu'il y a beaucoup de petits ajustements superposés. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 176.22,
  "end": 186.64
 },
 {
  "input": "So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. ",
  "translatedText": "Je vais donc commencer ici en ignorant complètement la notation, et en passant simplement en revue les effets de chaque exemple d'entraînement sur les poids et les biais. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.74,
  "end": 196.12
 },
 {
  "input": "Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. ",
  "translatedText": "Étant donné que la fonction de coût implique de faire la moyenne d'un certain coût par exemple sur des dizaines de milliers d'exemples de formation, la manière dont nous ajustons les poids et les biais pour une seule étape de descente de gradient dépend également de chaque exemple. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 197.02,
  "end": 211.04
 },
 {
  "input": "Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. ",
  "translatedText": "Ou plutôt, en principe, cela devrait le faire, mais pour des raisons d'efficacité de calcul, nous ferons une petite astuce plus tard pour vous éviter d'avoir besoin de frapper chaque exemple à chaque étape. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.68,
  "end": 219.2
 },
 {
  "input": "In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. ",
  "translatedText": "Dans d'autres cas, pour le moment, tout ce que nous allons faire est de concentrer notre attention sur un seul exemple, cette image d'un 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.2,
  "end": 225.96
 },
 {
  "input": "What effect should this one training example have on how the weights and biases get adjusted? ",
  "translatedText": "Quel effet cet exemple de formation devrait-il avoir sur la manière dont les pondérations et les biais sont ajustés ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 226.72,
  "end": 231.48
 },
 {
  "input": "Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. ",
  "translatedText": "Disons que nous sommes à un point où le réseau n'est pas encore bien formé, donc les activations dans la sortie vont paraître assez aléatoires, peut-être quelque chose comme 0.5, 0.8, 0.2, encore et encore. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 232.68,
  "end": 242.0
 },
 {
  "input": "We can't directly change those activations, we only have influence on the weights and biases, but it's helpful to keep track of which adjustments we wish should take place to that output layer. ",
  "translatedText": "Nous ne pouvons pas modifier directement ces activations, nous n'avons d'influence que sur les pondérations et les biais, mais il est utile de garder une trace des ajustements que nous souhaitons apporter à cette couche de sortie. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 242.52,
  "end": 252.58
 },
 {
  "input": "And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. ",
  "translatedText": "Et puisque nous voulons qu'il classe l'image comme 2, nous voulons que cette troisième valeur soit augmentée tandis que toutes les autres sont augmentées. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.36,
  "end": 261.26
 },
 {
  "input": "Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. ",
  "translatedText": "De plus, la taille de ces nudges doit être proportionnelle à la distance entre chaque valeur actuelle et sa valeur cible. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 262.06,
  "end": 269.52
 },
 {
  "input": "For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. ",
  "translatedText": "Par exemple, l'augmentation de l'activation du neurone numéro 2 est en un sens plus importante que la diminution de l'activation du neurone numéro 8, qui est déjà assez proche de là où il devrait être. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.22,
  "end": 280.9
 },
 {
  "input": "So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. ",
  "translatedText": "Alors en zoomant davantage, concentrons-nous uniquement sur ce neurone, celui dont nous souhaitons augmenter l'activation. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 282.04,
  "end": 287.28
 },
 {
  "input": "Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. ",
  "translatedText": "N'oubliez pas que cette activation est définie comme une certaine somme pondérée de toutes les activations de la couche précédente, plus un biais, qui est ensuite connecté à quelque chose comme la fonction de squishification sigmoïde, ou un ReLU. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 288.18,
  "end": 301.04
 },
 {
  "input": "So there are three different avenues that can team up together to help increase that activation. ",
  "translatedText": "Il existe donc trois voies différentes qui peuvent s’associer pour contribuer à accroître cette activation. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 301.64,
  "end": 307.02
 },
 {
  "input": "You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. ",
  "translatedText": "Vous pouvez augmenter le biais, augmenter les poids et modifier les activations de la couche précédente. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 307.44,
  "end": 314.04
 },
 {
  "input": "Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. ",
  "translatedText": "En vous concentrant sur la façon dont les poids doivent être ajustés, remarquez comment les poids ont en réalité différents niveaux d'influence. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 314.94,
  "end": 320.86
 },
 {
  "input": "The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. ",
  "translatedText": "Les connexions avec les neurones les plus brillants de la couche précédente ont le plus grand effet puisque ces poids sont multipliés par des valeurs d’activation plus grandes. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 321.44,
  "end": 329.1
 },
 {
  "input": "So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. ",
  "translatedText": "Donc, si vous deviez augmenter l'un de ces poids, cela aurait en fait une plus grande influence sur la fonction de coût ultime que l'augmentation du poids des connexions avec des neurones plus faibles, du moins en ce qui concerne cet exemple d'entraînement. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.46,
  "end": 343.48
 },
 {
  "input": "Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck. ",
  "translatedText": "N'oubliez pas que lorsque nous parlons de descente de gradient, nous ne nous soucions pas seulement de savoir si chaque composant doit être poussé vers le haut ou vers le bas, nous nous soucions de ceux qui vous en donnent le plus pour votre argent. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.42,
  "end": 353.22
 },
 {
  "input": "This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together. ",
  "translatedText": "Ceci, soit dit en passant, rappelle au moins quelque peu une théorie des neurosciences sur la manière dont les réseaux biologiques de neurones apprennent, la théorie Hebbian, souvent résumée dans l'expression « les neurones qui s'allument ensemble se connectent ». ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 355.02,
  "end": 366.46
 },
 {
  "input": "Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active and the ones which we wish to become more active. ",
  "translatedText": "Ici, les plus grandes augmentations de poids, le plus grand renforcement des connexions, se produisent entre les neurones les plus actifs et ceux que l'on souhaite devenir plus actifs. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.26,
  "end": 377.28
 },
 {
  "input": "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those firing when thinking about it. ",
  "translatedText": "Dans un sens, les neurones qui s’activent en voyant un 2 sont plus fortement liés à ceux qui s’activent lorsqu’on y pense. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 377.94,
  "end": 384.48
 },
 {
  "input": "To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note. ",
  "translatedText": "Pour être clair, je ne suis pas en mesure de faire des déclarations d'une manière ou d'une autre sur la question de savoir si les réseaux artificiels de neurones se comportent comme des cerveaux biologiques, et cette idée de connexion est accompagnée de quelques astérisques significatifs, mais considérée comme une idée très vague. analogie, je trouve intéressant de le noter. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.4,
  "end": 401.02
 },
 {
  "input": "Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer. ",
  "translatedText": "Quoi qu'il en soit, la troisième façon dont nous pouvons contribuer à augmenter l'activation de ce neurone consiste à modifier toutes les activations de la couche précédente. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.94,
  "end": 409.04
 },
 {
  "input": "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active. ",
  "translatedText": "À savoir, si tout ce qui est connecté à ce neurone du chiffre 2 avec un poids positif devenait plus brillant, et si tout ce qui est connecté avec un poids négatif devenait plus faible, alors ce neurone du chiffre 2 deviendrait plus actif. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 409.04,
  "end": 420.68
 },
 {
  "input": "And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. ",
  "translatedText": "Et comme pour les changements de poids, vous en aurez pour votre argent en recherchant des changements proportionnels à la taille des poids correspondants. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 422.54,
  "end": 430.28
 },
 {
  "input": "Now of course, we cannot directly influence those activations, we only have control over the weights and biases. ",
  "translatedText": "Bien entendu, nous ne pouvons pas influencer directement ces activations, nous contrôlons uniquement les poids et les biais. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 432.14,
  "end": 437.48
 },
 {
  "input": "But just as with the last layer, it's helpful to keep a note of what those desired changes are. ",
  "translatedText": "Mais tout comme pour la dernière couche, il est utile de noter les modifications souhaitées. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 437.48,
  "end": 444.12
 },
 {
  "input": "But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants. ",
  "translatedText": "Mais gardez à l’esprit qu’en effectuant un zoom arrière ici, c’est uniquement ce que veut ce neurone de sortie du chiffre 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 444.58,
  "end": 449.2
 },
 {
  "input": "Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. ",
  "translatedText": "N'oubliez pas que nous voulons également que tous les autres neurones de la dernière couche deviennent moins actifs, et chacun de ces autres neurones de sortie a ses propres idées sur ce qui devrait arriver à cette avant-dernière couche. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 449.76,
  "end": 459.6
 },
 {
  "input": "So the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. ",
  "translatedText": "Ainsi, le désir de ce neurone du chiffre 2 est ajouté aux désirs de tous les autres neurones de sortie pour ce qui devrait arriver à cette avant-dernière couche, encore une fois proportionnellement aux poids correspondants, et proportionnellement aux besoins de chacun de ces neurones. changer. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 462.7,
  "end": 480.72
 },
 {
  "input": "This right here is where the idea of propagating backwards comes in. ",
  "translatedText": "C’est ici qu’intervient l’idée de propagation à rebours. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 481.6,
  "end": 485.48
 },
 {
  "input": "By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. ",
  "translatedText": "En additionnant tous ces effets souhaités, vous obtenez essentiellement une liste de coups de pouce que vous souhaitez appliquer à cette avant-dernière couche. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 485.82,
  "end": 493.36
 },
 {
  "input": "And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. ",
  "translatedText": "Et une fois que vous les avez, vous pouvez appliquer de manière récursive le même processus aux poids et biais pertinents qui déterminent ces valeurs, en répétant le même processus que je viens de suivre et en reculant dans le réseau. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 494.22,
  "end": 505.1
 },
 {
  "input": "And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. ",
  "translatedText": "Et en zoomant un peu plus loin, rappelez-vous que c'est exactement ainsi qu'un seul exemple de formation souhaite pousser chacun de ces poids et biais. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.96,
  "end": 517.0
 },
 {
  "input": "If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2. ",
  "translatedText": "Si nous écoutions seulement ce que ce 2 voulait, le réseau serait finalement incité à simplement classer toutes les images dans la catégorie 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.48,
  "end": 523.22
 },
 {
  "input": "So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes. ",
  "translatedText": "Donc, ce que vous faites, c'est suivre cette même routine de backprop pour tous les autres exemples de formation, en enregistrant comment chacun d'entre eux souhaite modifier les poids et les biais, et en faisant la moyenne de ces changements souhaités. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.06,
  "end": 536.0
 },
 {
  "input": "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. ",
  "translatedText": "Cette collection ici des coups de pouce moyennés pour chaque poids et biais est, en gros, le gradient négatif de la fonction de coût référencé dans la dernière vidéo, ou du moins quelque chose de proportionnel à celui-ci. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 541.72,
  "end": 553.68
 },
 {
  "input": "I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. ",
  "translatedText": "Je dis vaguement uniquement parce que je n'ai pas encore été quantitativement précis sur ces coups de pouce, mais si vous comprenez chaque changement auquel je viens de faire référence, pourquoi certains sont proportionnellement plus grands que d'autres et comment ils doivent tous être additionnés, vous comprenez les mécanismes pour ce que fait réellement la rétropropagation. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 554.38,
  "end": 571.0
 },
 {
  "input": "By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. ",
  "translatedText": "Soit dit en passant, dans la pratique, il faut extrêmement longtemps aux ordinateurs pour additionner l'influence de chaque exemple d'entraînement à chaque étape de descente de gradient. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 573.96,
  "end": 582.44
 },
 {
  "input": "So here's what's commonly done instead. ",
  "translatedText": "Voici donc ce qui est couramment fait à la place. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 583.14,
  "end": 584.82
 },
 {
  "input": "You randomly shuffle your training data and divide it into a whole bunch of mini-batches, let's say each one having 100 training examples. ",
  "translatedText": "Vous mélangez aléatoirement vos données d'entraînement et les divisez en tout un tas de mini-lots, disons que chacun contient 100 exemples d'entraînement. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 585.48,
  "end": 592.42
 },
 {
  "input": "Then you compute a step according to the mini-batch. ",
  "translatedText": "Ensuite vous calculez une étape en fonction du mini-lot. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 592.94,
  "end": 596.2
 },
 {
  "input": "It's not the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly it gives you a significant computational speedup. ",
  "translatedText": "Ce n'est pas le gradient réel de la fonction de coût, qui dépend de toutes les données d'entraînement, ni de ce petit sous-ensemble, ce n'est donc pas l'étape de descente la plus efficace, mais chaque mini-lot vous donne une assez bonne approximation, et plus important encore. vous donne une accélération de calcul significative. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 596.96,
  "end": 612.12
 },
 {
  "input": "If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. ",
  "translatedText": "Si vous deviez tracer la trajectoire de votre réseau sous la surface de coût pertinente, cela ressemblerait un peu plus à un homme ivre trébuchant sans but sur une colline mais faisant des pas rapides, plutôt qu'à un homme soigneusement calculateur déterminant la direction exacte de chaque pas en descente. avant de faire un pas très lent et prudent dans cette direction. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 612.82,
  "end": 630.16
 },
 {
  "input": "This technique is referred to as stochastic gradient descent. ",
  "translatedText": "Cette technique est appelée descente de gradient stochastique. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 631.54,
  "end": 634.66
 },
 {
  "input": "There's a lot going on here, so let's just sum it up for ourselves, shall we? ",
  "translatedText": "Il se passe beaucoup de choses ici, alors résumons-le par nous-mêmes, d'accord ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.96,
  "end": 639.62
 },
 {
  "input": "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. ",
  "translatedText": "La rétropropagation est l'algorithme permettant de déterminer comment un exemple d'entraînement unique souhaite augmenter les poids et les biais, non seulement en termes de hausse ou de baisse, mais également en termes de proportions relatives à ces changements qui provoquent la diminution la plus rapide de la valeur. coût. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 640.44,
  "end": 655.56
 },
 {
  "input": "A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes you get, but that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch. ",
  "translatedText": "Une véritable étape de descente de gradient impliquerait de faire cela pour tous vos dizaines et milliers d'exemples de formation et de faire la moyenne des changements souhaités que vous obtenez, mais cela est lent en termes de calcul, donc à la place, vous subdivisez aléatoirement les données en mini-lots et calculez chaque étape par rapport à un mini-lot. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 656.26,
  "end": 673.24
 },
 {
  "input": "Repeatedly going through all the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples. ",
  "translatedText": "En parcourant à plusieurs reprises tous les mini-lots et en effectuant ces ajustements, vous convergerez vers un minimum local de la fonction de coût, c'est-à-dire que votre réseau finira par faire un très bon travail sur les exemples de formation. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.0,
  "end": 685.54
 },
 {
  "input": "So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms. ",
  "translatedText": "Cela dit, chaque ligne de code nécessaire à l'implémentation de backprop correspond en fait à quelque chose que vous avez vu maintenant, du moins en termes informels. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 687.24,
  "end": 696.72
 },
 {
  "input": "But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. ",
  "translatedText": "Mais parfois, savoir ce que font les mathématiques ne représente que la moitié de la bataille, et le simple fait de représenter cette foutue chose est là où tout devient confus et déroutant. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.56,
  "end": 704.12
 },
 {
  "input": "So, for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. ",
  "translatedText": "Donc, pour ceux d'entre vous qui souhaitent approfondir, la vidéo suivante reprend les mêmes idées que celles qui viennent d'être présentées ici, mais en termes de calcul sous-jacent, ce qui devrait, espérons-le, le rendre un peu plus familier à mesure que vous verrez le sujet dans autres ressources. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.86,
  "end": 716.42
 },
 {
  "input": "Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. ",
  "translatedText": "Avant cela, il convient de souligner que pour que cet algorithme fonctionne, et cela vaut pour toutes sortes d’apprentissage automatique au-delà des seuls réseaux de neurones, vous avez besoin de beaucoup de données d’entraînement. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 717.34,
  "end": 725.9
 },
 {
  "input": "In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans. ",
  "translatedText": "Dans notre cas, une chose qui fait des chiffres manuscrits un si bel exemple est qu’il existe la base de données MNIST, avec de nombreux exemples étiquetés par des humains. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 726.42,
  "end": 734.74
 },
 {
  "input": "So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with. ",
  "translatedText": "Ainsi, un défi commun que ceux d'entre vous qui travaillent dans le domaine de l'apprentissage automatique sont familiers est simplement d'obtenir les données d'entraînement étiquetées dont vous avez réellement besoin, qu'il s'agisse de demander aux gens d'étiqueter des dizaines de milliers d'images ou de tout autre type de données que vous pourriez traiter. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 735.3,
  "end": 747.1
 }
]