1
00:00:04,060 --> 00:00:06,510
Ici, nous abordons la rétropropagation, l'algorithme de base 

2
00:00:06,510 --> 00:00:08,880
derrière la façon dont les réseaux de neurones apprennent. 

3
00:00:09,400 --> 00:00:11,361
Après un bref récapitulatif de notre situation, 

4
00:00:11,361 --> 00:00:13,894
la première chose que je ferai est une présentation intuitive 

5
00:00:13,894 --> 00:00:17,000
de ce que fait réellement l'algorithme, sans aucune référence aux formules. 

6
00:00:17,660 --> 00:00:20,755
Ensuite, pour ceux d’entre vous qui souhaitent se plonger dans les mathématiques, 

7
00:00:20,755 --> 00:00:23,020
la vidéo suivante aborde le calcul qui sous-tend tout cela. 

8
00:00:23,820 --> 00:00:25,624
Si vous avez regardé les deux dernières vidéos, 

9
00:00:25,624 --> 00:00:27,955
ou si vous vous lancez simplement dans le contexte approprié, 

10
00:00:27,955 --> 00:00:31,000
vous savez ce qu'est un réseau neuronal et comment il transmet les informations. 

11
00:00:31,680 --> 00:00:36,007
Ici, nous faisons l'exemple classique de reconnaissance de chiffres manuscrits dont les 

12
00:00:36,007 --> 00:00:40,335
valeurs de pixels sont introduites dans la première couche du réseau avec 784 neurones, 

13
00:00:40,335 --> 00:00:44,466
et j'ai montré un réseau avec deux couches cachées n'ayant que 16 neurones chacune, 

14
00:00:44,466 --> 00:00:48,597
et une sortie couche de 10 neurones, indiquant quel chiffre le réseau choisit comme 

15
00:00:48,597 --> 00:00:49,040
réponse. 

16
00:00:50,040 --> 00:00:53,290
J'attends également de vous que vous compreniez la descente de gradient, 

17
00:00:53,290 --> 00:00:57,074
comme décrit dans la dernière vidéo, et que ce que nous entendons par apprentissage, 

18
00:00:57,074 --> 00:01:00,859
c'est que nous voulons trouver quels poids et biais minimisent une certaine fonction 

19
00:01:00,859 --> 00:01:01,260
de coût. 

20
00:01:02,040 --> 00:01:05,277
Pour rappel, pour le coût d'un seul exemple de formation, 

21
00:01:05,277 --> 00:01:09,576
vous prenez le résultat fourni par le réseau, ainsi que le résultat que vous 

22
00:01:09,576 --> 00:01:14,600
souhaitiez qu'il donne, et additionnez les carrés des différences entre chaque composant. 

23
00:01:15,380 --> 00:01:19,341
En faisant cela pour l'ensemble de vos dizaines de milliers d'exemples de formation 

24
00:01:19,341 --> 00:01:23,020
et en faisant la moyenne des résultats, vous obtenez le coût total du réseau. 

25
00:01:23,020 --> 00:01:26,803
Comme si cela ne suffisait pas, comme décrit dans la dernière vidéo, 

26
00:01:26,803 --> 00:01:30,916
ce que nous recherchons est le gradient négatif de cette fonction de coût, 

27
00:01:30,916 --> 00:01:34,152
qui vous indique comment modifier tous les poids et biais, 

28
00:01:34,152 --> 00:01:38,320
tous ces connexions, afin de réduire le coût le plus efficacement possible. 

29
00:01:43,260 --> 00:01:46,147
La rétropropagation, le sujet de cette vidéo, est un 

30
00:01:46,147 --> 00:01:49,580
algorithme permettant de calculer ce gradient complexe et fou. 

31
00:01:49,580 --> 00:01:53,144
La seule idée de la dernière vidéo que je veux vraiment que vous gardiez fermement 

32
00:01:53,144 --> 00:01:56,537
à l'esprit en ce moment est que parce que considérer le vecteur gradient comme 

33
00:01:56,537 --> 00:01:59,414
une direction dans 13 000 dimensions est, pour le dire légèrement, 

34
00:01:59,414 --> 00:02:02,162
au-delà de la portée de notre imagination, il y en a une autre. 

35
00:02:02,162 --> 00:02:03,580
façon dont vous pouvez y penser. 

36
00:02:04,600 --> 00:02:07,693
L'ampleur de chaque composante ici vous indique à quel point 

37
00:02:07,693 --> 00:02:10,940
la fonction de coût est sensible à chaque pondération et biais. 

38
00:02:11,800 --> 00:02:16,512
Par exemple, disons que vous suivez le processus que je suis sur le point de décrire et 

39
00:02:16,512 --> 00:02:21,225
que vous calculez le gradient négatif, et que la composante associée au poids sur cette 

40
00:02:21,225 --> 00:02:25,992
arête est ici de 3.2, tandis que la composante associée à cette arête apparaît ici comme 

41
00:02:25,992 --> 00:02:26,260
0.1. 

42
00:02:26,820 --> 00:02:30,917
La façon dont vous interpréteriez cela est que le coût de la fonction est 32 fois 

43
00:02:30,917 --> 00:02:33,465
plus sensible aux changements de ce premier poids, 

44
00:02:33,465 --> 00:02:35,964
donc si vous deviez modifier un peu cette valeur, 

45
00:02:35,964 --> 00:02:40,111
cela entraînerait une modification du coût, et ce changement est 32 fois supérieur 

46
00:02:40,111 --> 00:02:43,060
à ce que donnerait la même variation de ce deuxième poids. 

47
00:02:48,420 --> 00:02:51,852
Personnellement, lorsque j'ai découvert la rétropropagation pour la première fois, 

48
00:02:51,852 --> 00:02:55,367
je pense que l'aspect le plus déroutant était simplement la notation et la recherche 

49
00:02:55,367 --> 00:02:55,740
d'index. 

50
00:02:56,220 --> 00:02:59,631
Mais une fois que vous avez compris ce que fait réellement chaque partie 

51
00:02:59,631 --> 00:03:03,602
de cet algorithme, chaque effet individuel qu'il produit est en fait assez intuitif, 

52
00:03:03,602 --> 00:03:06,640
c'est juste qu'il y a beaucoup de petits ajustements superposés. 

53
00:03:07,740 --> 00:03:10,925
Je vais donc commencer ici en ignorant complètement la notation, 

54
00:03:10,925 --> 00:03:15,139
et en passant simplement en revue les effets de chaque exemple d'entraînement sur les 

55
00:03:15,139 --> 00:03:16,120
poids et les biais. 

56
00:03:17,020 --> 00:03:20,487
Étant donné que la fonction de coût implique de faire la moyenne d'un 

57
00:03:20,487 --> 00:03:24,401
certain coût par exemple sur des dizaines de milliers d'exemples de formation, 

58
00:03:24,401 --> 00:03:27,770
la manière dont nous ajustons les poids et les biais pour une seule 

59
00:03:27,770 --> 00:03:31,040
étape de descente de gradient dépend également de chaque exemple. 

60
00:03:31,680 --> 00:03:33,387
Ou plutôt, en principe, cela devrait le faire, 

61
00:03:33,387 --> 00:03:35,894
mais pour des raisons d'efficacité de calcul, nous ferons une petite 

62
00:03:35,894 --> 00:03:39,163
astuce plus tard pour vous éviter d'avoir besoin de frapper chaque exemple à chaque étape.

63
00:03:39,163 --> 00:03:39,200
 

64
00:03:39,200 --> 00:03:42,531
Dans d'autres cas, pour le moment, tout ce que nous allons faire est 

65
00:03:42,531 --> 00:03:45,960
de concentrer notre attention sur un seul exemple, cette image d'un 2. 

66
00:03:46,720 --> 00:03:49,140
Quel effet cet exemple de formation devrait-il avoir sur la 

67
00:03:49,140 --> 00:03:51,480
manière dont les pondérations et les biais sont ajustés ? 

68
00:03:52,680 --> 00:03:56,101
Disons que nous sommes à un point où le réseau n'est pas encore bien formé, 

69
00:03:56,101 --> 00:03:59,163
donc les activations dans la sortie vont paraître assez aléatoires, 

70
00:03:59,163 --> 00:04:02,000
peut-être quelque chose comme 0.5, 0.8, 0.2, encore et encore. 

71
00:04:02,520 --> 00:04:05,045
Nous ne pouvons pas modifier directement ces activations, 

72
00:04:05,045 --> 00:04:07,833
nous n'avons d'influence que sur les pondérations et les biais, 

73
00:04:07,833 --> 00:04:11,055
mais il est utile de garder une trace des ajustements que nous souhaitons 

74
00:04:11,055 --> 00:04:12,580
apporter à cette couche de sortie. 

75
00:04:13,360 --> 00:04:16,112
Et puisque nous voulons qu'il classe l'image comme 2, 

76
00:04:16,112 --> 00:04:20,036
nous voulons que cette troisième valeur soit augmentée tandis que toutes les 

77
00:04:20,036 --> 00:04:21,260
autres sont augmentées. 

78
00:04:22,060 --> 00:04:25,789
De plus, la taille de ces nudges doit être proportionnelle à 

79
00:04:25,789 --> 00:04:29,520
la distance entre chaque valeur actuelle et sa valeur cible. 

80
00:04:30,220 --> 00:04:33,815
Par exemple, l'augmentation de l'activation du neurone numéro 2 est 

81
00:04:33,815 --> 00:04:38,150
en un sens plus importante que la diminution de l'activation du neurone numéro 8, 

82
00:04:38,150 --> 00:04:40,900
qui est déjà assez proche de là où il devrait être. 

83
00:04:42,040 --> 00:04:45,107
Alors en zoomant davantage, concentrons-nous uniquement sur ce neurone, 

84
00:04:45,107 --> 00:04:47,280
celui dont nous souhaitons augmenter l'activation. 

85
00:04:48,180 --> 00:04:52,397
N'oubliez pas que cette activation est définie comme une certaine somme pondérée 

86
00:04:52,397 --> 00:04:55,833
de toutes les activations de la couche précédente, plus un biais, 

87
00:04:55,833 --> 00:05:00,415
qui est ensuite connecté à quelque chose comme la fonction de squishification sigmoïde, 

88
00:05:00,415 --> 00:05:01,040
ou un ReLU. 

89
00:05:01,640 --> 00:05:04,180
Il existe donc trois voies différentes qui peuvent 

90
00:05:04,180 --> 00:05:07,020
s’associer pour contribuer à accroître cette activation. 

91
00:05:07,440 --> 00:05:10,708
Vous pouvez augmenter le biais, augmenter les poids 

92
00:05:10,708 --> 00:05:14,040
et modifier les activations de la couche précédente. 

93
00:05:14,940 --> 00:05:17,797
En vous concentrant sur la façon dont les poids doivent être ajustés, 

94
00:05:17,797 --> 00:05:20,860
remarquez comment les poids ont en réalité différents niveaux d'influence. 

95
00:05:21,440 --> 00:05:25,248
Les connexions avec les neurones les plus brillants de la couche précédente ont le plus 

96
00:05:25,248 --> 00:05:29,100
grand effet puisque ces poids sont multipliés par des valeurs d’activation plus grandes. 

97
00:05:31,460 --> 00:05:33,789
Donc, si vous deviez augmenter l'un de ces poids, 

98
00:05:33,789 --> 00:05:37,563
cela aurait en fait une plus grande influence sur la fonction de coût ultime que 

99
00:05:37,563 --> 00:05:40,871
l'augmentation du poids des connexions avec des neurones plus faibles, 

100
00:05:40,871 --> 00:05:43,480
du moins en ce qui concerne cet exemple d'entraînement. 

101
00:05:44,420 --> 00:05:46,672
N'oubliez pas que lorsque nous parlons de descente de gradient, 

102
00:05:46,672 --> 00:05:49,594
nous ne nous soucions pas seulement de savoir si chaque composant doit être poussé 

103
00:05:49,594 --> 00:05:52,551
vers le haut ou vers le bas, nous nous soucions de ceux qui vous en donnent le plus 

104
00:05:52,551 --> 00:05:53,220
pour votre argent. 

105
00:05:55,020 --> 00:05:58,775
Ceci, soit dit en passant, rappelle au moins quelque peu une théorie des neurosciences 

106
00:05:58,775 --> 00:06:02,574
sur la manière dont les réseaux biologiques de neurones apprennent, la théorie Hebbian, 

107
00:06:02,574 --> 00:06:06,460
souvent résumée dans l'expression « les neurones qui s'allument ensemble se connectent ». 

108
00:06:07,260 --> 00:06:12,080
Ici, les plus grandes augmentations de poids, le plus grand renforcement des connexions, 

109
00:06:12,080 --> 00:06:15,384
se produisent entre les neurones les plus actifs et ceux que 

110
00:06:15,384 --> 00:06:17,280
l'on souhaite devenir plus actifs. 

111
00:06:17,940 --> 00:06:21,236
Dans un sens, les neurones qui s’activent en voyant un 2 sont 

112
00:06:21,236 --> 00:06:24,480
plus fortement liés à ceux qui s’activent lorsqu’on y pense. 

113
00:06:25,400 --> 00:06:28,457
Pour être clair, je ne suis pas en mesure de faire des déclarations d'une 

114
00:06:28,457 --> 00:06:31,598
manière ou d'une autre sur la question de savoir si les réseaux artificiels 

115
00:06:31,598 --> 00:06:33,995
de neurones se comportent comme des cerveaux biologiques, 

116
00:06:33,995 --> 00:06:37,383
et cette idée de connexion est accompagnée de quelques astérisques significatifs, 

117
00:06:37,383 --> 00:06:41,020
mais considérée comme une idée très vague. analogie, je trouve intéressant de le noter. 

118
00:06:41,940 --> 00:06:45,185
Quoi qu'il en soit, la troisième façon dont nous pouvons contribuer à augmenter 

119
00:06:45,185 --> 00:06:48,553
l'activation de ce neurone consiste à modifier toutes les activations de la couche 

120
00:06:48,553 --> 00:06:49,040
précédente. 

121
00:06:49,040 --> 00:06:52,838
À savoir, si tout ce qui est connecté à ce neurone du chiffre 2 avec un poids 

122
00:06:52,838 --> 00:06:56,588
positif devenait plus brillant, et si tout ce qui est connecté avec un poids 

123
00:06:56,588 --> 00:07:00,680
négatif devenait plus faible, alors ce neurone du chiffre 2 deviendrait plus actif. 

124
00:07:02,540 --> 00:07:06,261
Et comme pour les changements de poids, vous en aurez pour votre argent en 

125
00:07:06,261 --> 00:07:10,280
recherchant des changements proportionnels à la taille des poids correspondants. 

126
00:07:12,140 --> 00:07:15,301
Bien entendu, nous ne pouvons pas influencer directement ces activations, 

127
00:07:15,301 --> 00:07:17,480
nous contrôlons uniquement les poids et les biais. 

128
00:07:17,480 --> 00:07:20,907
Mais tout comme pour la dernière couche, il est 

129
00:07:20,907 --> 00:07:24,120
utile de noter les modifications souhaitées. 

130
00:07:24,580 --> 00:07:26,834
Mais gardez à l’esprit qu’en effectuant un zoom arrière ici, 

131
00:07:26,834 --> 00:07:29,200
c’est uniquement ce que veut ce neurone de sortie du chiffre 2. 

132
00:07:29,760 --> 00:07:32,902
N'oubliez pas que nous voulons également que tous les autres neurones de la 

133
00:07:32,902 --> 00:07:36,085
dernière couche deviennent moins actifs, et chacun de ces autres neurones de 

134
00:07:36,085 --> 00:07:39,600
sortie a ses propres idées sur ce qui devrait arriver à cette avant-dernière couche. 

135
00:07:42,700 --> 00:07:47,159
Ainsi, le désir de ce neurone du chiffre 2 est ajouté aux désirs de tous 

136
00:07:47,159 --> 00:07:52,595
les autres neurones de sortie pour ce qui devrait arriver à cette avant-dernière couche, 

137
00:07:52,595 --> 00:07:56,382
encore une fois proportionnellement aux poids correspondants, 

138
00:07:56,382 --> 00:08:00,720
et proportionnellement aux besoins de chacun de ces neurones. changer. 

139
00:08:01,600 --> 00:08:05,480
C’est ici qu’intervient l’idée de propagation à rebours. 

140
00:08:05,820 --> 00:08:09,684
En additionnant tous ces effets souhaités, vous obtenez essentiellement une liste 

141
00:08:09,684 --> 00:08:13,360
de coups de pouce que vous souhaitez appliquer à cette avant-dernière couche. 

142
00:08:14,220 --> 00:08:17,784
Et une fois que vous les avez, vous pouvez appliquer de manière récursive le 

143
00:08:17,784 --> 00:08:21,210
même processus aux poids et biais pertinents qui déterminent ces valeurs, 

144
00:08:21,210 --> 00:08:25,100
en répétant le même processus que je viens de suivre et en reculant dans le réseau. 

145
00:08:28,960 --> 00:08:32,821
Et en zoomant un peu plus loin, rappelez-vous que c'est exactement ainsi 

146
00:08:32,821 --> 00:08:37,000
qu'un seul exemple de formation souhaite pousser chacun de ces poids et biais. 

147
00:08:37,480 --> 00:08:39,433
Si nous écoutions seulement ce que ce 2 voulait, 

148
00:08:39,433 --> 00:08:42,382
le réseau serait finalement incité à simplement classer toutes les images 

149
00:08:42,382 --> 00:08:43,220
dans la catégorie 2. 

150
00:08:44,059 --> 00:08:48,008
Donc, ce que vous faites, c'est suivre cette même routine de backprop pour tous les 

151
00:08:48,008 --> 00:08:51,863
autres exemples de formation, en enregistrant comment chacun d'entre eux souhaite 

152
00:08:51,863 --> 00:08:56,000
modifier les poids et les biais, et en faisant la moyenne de ces changements souhaités. 

153
00:09:01,720 --> 00:09:06,083
Cette collection ici des coups de pouce moyennés pour chaque poids et biais est, 

154
00:09:06,083 --> 00:09:10,716
en gros, le gradient négatif de la fonction de coût référencé dans la dernière vidéo, 

155
00:09:10,716 --> 00:09:13,680
ou du moins quelque chose de proportionnel à celui-ci. 

156
00:09:14,380 --> 00:09:17,800
Je dis vaguement uniquement parce que je n'ai pas encore été quantitativement 

157
00:09:17,800 --> 00:09:20,957
précis sur ces coups de pouce, mais si vous comprenez chaque changement 

158
00:09:20,957 --> 00:09:24,422
auquel je viens de faire référence, pourquoi certains sont proportionnellement 

159
00:09:24,422 --> 00:09:27,535
plus grands que d'autres et comment ils doivent tous être additionnés, 

160
00:09:27,535 --> 00:09:31,000
vous comprenez les mécanismes pour ce que fait réellement la rétropropagation. 

161
00:09:33,960 --> 00:09:36,635
Soit dit en passant, dans la pratique, il faut extrêmement 

162
00:09:36,635 --> 00:09:39,583
longtemps aux ordinateurs pour additionner l'influence de chaque 

163
00:09:39,583 --> 00:09:42,440
exemple d'entraînement à chaque étape de descente de gradient. 

164
00:09:43,140 --> 00:09:44,820
Voici donc ce qui est couramment fait à la place. 

165
00:09:45,480 --> 00:09:48,972
Vous mélangez aléatoirement vos données d'entraînement et les divisez en tout 

166
00:09:48,972 --> 00:09:52,420
un tas de mini-lots, disons que chacun contient 100 exemples d'entraînement. 

167
00:09:52,939 --> 00:09:57,280
Ensuite vous calculez une étape en fonction du mini-lot. 

168
00:09:57,280 --> 00:09:59,701
Ce n'est pas le gradient réel de la fonction de coût, 

169
00:09:59,701 --> 00:10:03,242
qui dépend de toutes les données d'entraînement, ni de ce petit sous-ensemble, 

170
00:10:03,242 --> 00:10:05,753
ce n'est donc pas l'étape de descente la plus efficace, 

171
00:10:05,753 --> 00:10:09,743
mais chaque mini-lot vous donne une assez bonne approximation, et plus important encore. 

172
00:10:09,743 --> 00:10:12,120
vous donne une accélération de calcul significative. 

173
00:10:12,820 --> 00:10:16,957
Si vous deviez tracer la trajectoire de votre réseau sous la surface de coût pertinente, 

174
00:10:16,957 --> 00:10:20,444
cela ressemblerait un peu plus à un homme ivre trébuchant sans but sur une 

175
00:10:20,444 --> 00:10:23,837
colline mais faisant des pas rapides, plutôt qu'à un homme soigneusement 

176
00:10:23,837 --> 00:10:27,138
calculateur déterminant la direction exacte de chaque pas en descente. 

177
00:10:27,138 --> 00:10:30,160
avant de faire un pas très lent et prudent dans cette direction. 

178
00:10:31,540 --> 00:10:34,660
Cette technique est appelée descente de gradient stochastique. 

179
00:10:35,960 --> 00:10:39,620
Il se passe beaucoup de choses ici, alors résumons-le par nous-mêmes, d'accord ? 

180
00:10:40,440 --> 00:10:44,196
La rétropropagation est l'algorithme permettant de déterminer comment un exemple 

181
00:10:44,196 --> 00:10:47,211
d'entraînement unique souhaite augmenter les poids et les biais, 

182
00:10:47,211 --> 00:10:49,437
non seulement en termes de hausse ou de baisse, 

183
00:10:49,437 --> 00:10:53,287
mais également en termes de proportions relatives à ces changements qui provoquent 

184
00:10:53,287 --> 00:10:55,560
la diminution la plus rapide de la valeur. coût. 

185
00:10:56,260 --> 00:10:59,599
Une véritable étape de descente de gradient impliquerait de faire cela 

186
00:10:59,599 --> 00:11:02,939
pour tous vos dizaines et milliers d'exemples de formation et de faire 

187
00:11:02,939 --> 00:11:05,526
la moyenne des changements souhaités que vous obtenez, 

188
00:11:05,526 --> 00:11:08,207
mais cela est lent en termes de calcul, donc à la place, 

189
00:11:08,207 --> 00:11:11,687
vous subdivisez aléatoirement les données en mini-lots et calculez chaque 

190
00:11:11,687 --> 00:11:13,240
étape par rapport à un mini-lot. 

191
00:11:14,000 --> 00:11:18,078
En parcourant à plusieurs reprises tous les mini-lots et en effectuant ces ajustements, 

192
00:11:18,078 --> 00:11:20,998
vous convergerez vers un minimum local de la fonction de coût, 

193
00:11:20,998 --> 00:11:24,891
c'est-à-dire que votre réseau finira par faire un très bon travail sur les exemples 

194
00:11:24,891 --> 00:11:25,540
de formation. 

195
00:11:27,240 --> 00:11:32,008
Cela dit, chaque ligne de code nécessaire à l'implémentation de backprop correspond 

196
00:11:32,008 --> 00:11:36,720
en fait à quelque chose que vous avez vu maintenant, du moins en termes informels. 

197
00:11:37,560 --> 00:11:39,792
Mais parfois, savoir ce que font les mathématiques ne représente 

198
00:11:39,792 --> 00:11:41,956
que la moitié de la bataille, et le simple fait de représenter 

199
00:11:41,956 --> 00:11:44,120
cette foutue chose est là où tout devient confus et déroutant. 

200
00:11:44,860 --> 00:11:47,041
Donc, pour ceux d'entre vous qui souhaitent approfondir, 

201
00:11:47,041 --> 00:11:50,448
la vidéo suivante reprend les mêmes idées que celles qui viennent d'être présentées ici, 

202
00:11:50,448 --> 00:11:53,013
mais en termes de calcul sous-jacent, ce qui devrait, espérons-le, 

203
00:11:53,013 --> 00:11:56,420
le rendre un peu plus familier à mesure que vous verrez le sujet dans autres ressources. 

204
00:11:57,340 --> 00:12:00,181
Avant cela, il convient de souligner que pour que cet algorithme fonctionne, 

205
00:12:00,181 --> 00:12:03,058
et cela vaut pour toutes sortes d’apprentissage automatique au-delà des seuls 

206
00:12:03,058 --> 00:12:05,900
réseaux de neurones, vous avez besoin de beaucoup de données d’entraînement. 

207
00:12:06,420 --> 00:12:10,604
Dans notre cas, une chose qui fait des chiffres manuscrits un si bel exemple est qu’il 

208
00:12:10,604 --> 00:12:14,740
existe la base de données MNIST, avec de nombreux exemples étiquetés par des humains. 

209
00:12:15,300 --> 00:12:17,522
Ainsi, un défi commun que ceux d'entre vous qui travaillent dans le 

210
00:12:17,522 --> 00:12:19,778
domaine de l'apprentissage automatique sont familiers est simplement 

211
00:12:19,778 --> 00:12:22,458
d'obtenir les données d'entraînement étiquetées dont vous avez réellement besoin, 

212
00:12:22,458 --> 00:12:24,844
qu'il s'agisse de demander aux gens d'étiqueter des dizaines de milliers 

213
00:12:24,844 --> 00:12:27,100
d'images ou de tout autre type de données que vous pourriez traiter. 

