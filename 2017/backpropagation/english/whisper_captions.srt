1
00:00:04,060 --> 00:00:08,880
Here, we tackle backpropagation, the core algorithm behind how neural networks learn.

2
00:00:09,400 --> 00:00:13,340
After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough

3
00:00:13,340 --> 00:00:17,000
for what the algorithm is actually doing, without any reference to the formulas.

4
00:00:17,660 --> 00:00:21,520
Then, for those of you who do want to dive into the math, the next video goes into the

5
00:00:21,520 --> 00:00:23,020
calculus underlying all this.

6
00:00:23,820 --> 00:00:27,140
If you watched the last two videos, or if you're just jumping in with the appropriate

7
00:00:27,140 --> 00:00:31,000
background, you know what a neural network is, and how it feeds forward information.

8
00:00:31,680 --> 00:00:35,520
Here, we're doing the classic example of recognizing handwritten digits whose pixel

9
00:00:35,520 --> 00:00:40,320
values get fed into the first layer of the network with 784 neurons, and I've been

10
00:00:40,320 --> 00:00:44,760
showing a network with two hidden layers having just 16 neurons each, and an output layer

11
00:00:44,760 --> 00:00:49,040
of 10 neurons, indicating which digit the network is choosing as its answer.

12
00:00:50,040 --> 00:00:54,460
I'm also expecting you to understand gradient descent, as described in the last video, and

13
00:00:54,460 --> 00:00:59,860
how what we mean by learning is that we want to find which weights and biases minimize

14
00:00:59,860 --> 00:01:01,260
a certain cost function.

15
00:01:02,040 --> 00:01:07,200
As a quick reminder, for the cost of a single training example, you take the output the

16
00:01:07,200 --> 00:01:13,080
network gives, along with the output you wanted it to give, and add up the squares of the

17
00:01:13,080 --> 00:01:14,600
differences between each component.

18
00:01:15,380 --> 00:01:19,620
Doing this for all of your tens of thousands of training examples and averaging the results,

19
00:01:20,080 --> 00:01:22,200
this gives you the total cost of the network.

20
00:01:22,200 --> 00:01:26,420
And as if that's not enough to think about, as described in the last video, the thing

21
00:01:26,420 --> 00:01:31,760
that we're looking for is the negative gradient of this cost function, which tells you how

22
00:01:31,760 --> 00:01:36,080
you need to change all of the weights and biases, all of these connections, so as to

23
00:01:36,080 --> 00:01:38,320
most efficiently decrease the cost.

24
00:01:43,260 --> 00:01:48,180
Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated

25
00:01:48,180 --> 00:01:48,580
gradient.

26
00:01:49,140 --> 00:01:53,180
And the one idea from the last video that I really want you to hold firmly in your mind

27
00:01:53,180 --> 00:01:58,020
right now is that because thinking of the gradient vector as a direction in 13,000 dimensions

28
00:01:58,020 --> 00:02:02,720
is, to put it lightly, beyond the scope of our imaginations, there's another way you

29
00:02:02,720 --> 00:02:03,580
can think about it.

30
00:02:04,600 --> 00:02:09,820
The magnitude of each component here is telling you how sensitive the cost function is to

31
00:02:09,820 --> 00:02:10,940
each weight and bias.

32
00:02:11,800 --> 00:02:15,220
For example, let's say you go through the process I'm about to describe, and you compute

33
00:02:15,220 --> 00:02:19,420
the negative gradient, and the component associated with the weight on this edge here

34
00:02:19,420 --> 00:02:26,260
comes out to be 3.2, while the component associated with this edge here comes out as 0.1.

35
00:02:26,820 --> 00:02:31,520
The way you would interpret that is that the cost of the function is 32 times more sensitive

36
00:02:31,520 --> 00:02:35,700
to changes in that first weight, so if you were to wiggle that value just a little bit,

37
00:02:35,860 --> 00:02:40,860
it's going to cause some change to the cost, and that change is 32 times greater than what

38
00:02:40,860 --> 00:02:43,060
the same wiggle to that second weight would give.

39
00:02:48,420 --> 00:02:52,500
Personally, when I was first learning about backpropagation, I think the most confusing

40
00:02:52,500 --> 00:02:55,740
aspect was just the notation and the index chasing of it all.

41
00:02:56,220 --> 00:03:00,260
But once you unwrap what each part of this algorithm is really doing, each individual

42
00:03:00,260 --> 00:03:05,100
effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments

43
00:03:05,100 --> 00:03:06,640
getting layered on top of each other.

44
00:03:07,740 --> 00:03:11,580
So I'm going to start things off here with a complete disregard for the notation, and

45
00:03:11,580 --> 00:03:16,120
just step through the effects each training example has on the weights and biases.

46
00:03:17,020 --> 00:03:21,860
Because the cost function involves averaging a certain cost per example over all the tens

47
00:03:21,860 --> 00:03:27,060
of thousands of training examples, the way we adjust the weights and biases for a single

48
00:03:27,060 --> 00:03:31,040
gradient descent step also depends on every single example.

49
00:03:31,680 --> 00:03:35,400
Or rather, in principle it should, but for computational efficiency we'll do a little

50
00:03:35,400 --> 00:03:39,200
trick later to keep you from needing to hit every single example for every step.

51
00:03:39,200 --> 00:03:44,020
In other cases, right now, all we're going to do is focus our attention on one single

52
00:03:44,020 --> 00:03:45,960
example, this image of a 2.

53
00:03:46,720 --> 00:03:51,480
What effect should this one training example have on how the weights and biases get adjusted?

54
00:03:52,680 --> 00:03:56,040
Let's say we're at a point where the network is not well trained yet, so the activations

55
00:03:56,040 --> 00:04:02,000
in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on.

56
00:04:02,520 --> 00:04:07,160
We can't directly change those activations, we only have influence on the weights and biases.

57
00:04:07,160 --> 00:04:11,760
But it's helpful to keep track of which adjustments we wish should take place to that

58
00:04:11,760 --> 00:04:12,580
output layer.

59
00:04:13,360 --> 00:04:18,160
And since we want it to classify the image as a 2, we want that third value to get nudged

60
00:04:18,160 --> 00:04:21,260
up while all the others get nudged down.

61
00:04:22,060 --> 00:04:27,640
Moreover, the sizes of these nudges should be proportional to how far away each current

62
00:04:27,640 --> 00:04:29,520
value is from its target value.

63
00:04:30,220 --> 00:04:35,920
For example, the increase to that number 2 neuron's activation is in a sense more important

64
00:04:35,920 --> 00:04:40,900
than the decrease to the number 8 neuron, which is already pretty close to where it should be.

65
00:04:42,040 --> 00:04:46,240
So zooming in further, let's focus just on this one neuron, the one whose activation

66
00:04:46,240 --> 00:04:47,280
we wish to increase.

67
00:04:48,180 --> 00:04:53,660
Remember, that activation is defined as a certain weighted sum of all the activations

68
00:04:53,660 --> 00:04:58,600
in the previous layer, plus a bias, which is all then plugged into something like the

69
00:04:58,600 --> 00:05:01,040
sigmoid squishification function, or a ReLU.

70
00:05:01,640 --> 00:05:06,460
So there are three different avenues that can team up together to help increase that

71
00:05:06,460 --> 00:05:07,020
activation.

72
00:05:07,440 --> 00:05:12,700
You can increase the bias, you can increase the weights, and you can change the activations

73
00:05:12,700 --> 00:05:14,040
from the previous layer.

74
00:05:14,940 --> 00:05:19,720
Focusing on how the weights should be adjusted, notice how the weights actually have differing

75
00:05:19,720 --> 00:05:20,860
levels of influence.

76
00:05:21,440 --> 00:05:25,080
The connections with the brightest neurons from the preceding layer have the biggest

77
00:05:25,080 --> 00:05:29,100
effect since those weights are multiplied by larger activation values.

78
00:05:31,460 --> 00:05:35,860
So if you were to increase one of those weights, it actually has a stronger influence on the

79
00:05:35,860 --> 00:05:40,400
ultimate cost function than increasing the weights of connections with dimmer neurons,

80
00:05:40,880 --> 00:05:43,480
at least as far as this one training example is concerned.

81
00:05:44,420 --> 00:05:48,640
Remember, when we talk about gradient descent, we don't just care about whether each component

82
00:05:48,640 --> 00:05:53,080
should get nudged up or down, we care about which ones give you the most bang for your

83
00:05:53,080 --> 00:05:53,220
buck.

84
00:05:55,020 --> 00:06:00,240
This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological

85
00:06:00,240 --> 00:06:05,020
networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that

86
00:06:05,020 --> 00:06:06,460
fire together wire together.

87
00:06:07,260 --> 00:06:12,460
Here, the biggest increases to weights, the biggest strengthening of connections, happens

88
00:06:12,460 --> 00:06:16,960
between neurons which are the most active, and the ones which we wish to become more

89
00:06:16,960 --> 00:06:17,280
active.

90
00:06:17,940 --> 00:06:22,600
In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those

91
00:06:22,600 --> 00:06:24,480
are the ones firing when thinking about a 2.

92
00:06:25,400 --> 00:06:29,320
To be clear, I'm not in a position to make statements one way or another about whether

93
00:06:29,320 --> 00:06:34,100
artificial networks of neurons behave anything like biological brains, and this fires together

94
00:06:34,100 --> 00:06:39,220
wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy,

95
00:06:39,520 --> 00:06:41,020
I find it interesting to note.

96
00:06:41,940 --> 00:06:46,760
Anyway, the third way we can help increase this neuron's activation is by changing

97
00:06:46,760 --> 00:06:49,040
all the activations in the previous layer.

98
00:06:49,040 --> 00:06:54,400
Namely, if everything connected to that digit 2 neuron with a positive weight got brighter,

99
00:06:55,000 --> 00:06:59,440
and if everything connected with a negative weight got dimmer, then that digit 2 neuron

100
00:06:59,440 --> 00:07:00,680
would become more active.

101
00:07:02,540 --> 00:07:06,280
And similar to the weight changes, you're going to get the most bang for your buck by

102
00:07:06,280 --> 00:07:10,280
seeking changes that are proportional to the size of the corresponding weights.

103
00:07:12,140 --> 00:07:16,540
Now of course, we cannot directly influence those activations, we only have control over

104
00:07:16,540 --> 00:07:17,480
the weights and biases.

105
00:07:17,480 --> 00:07:22,940
But just as with the last layer, it's helpful to keep a note of what those desired changes

106
00:07:23,660 --> 00:07:24,120
are.

107
00:07:24,580 --> 00:07:28,720
But keep in mind, zooming out one step here, this is only what that digit 2 output neuron

108
00:07:28,720 --> 00:07:29,200
wants.

109
00:07:29,760 --> 00:07:34,960
Remember, we also want all the other neurons in the last layer to become less active, and

110
00:07:34,960 --> 00:07:38,520
each of those other output neurons has its own thoughts about what should happen to that

111
00:07:38,520 --> 00:07:39,600
second to last layer.

112
00:07:42,700 --> 00:07:49,300
So, the desire of this digit 2 neuron is added together with the desires of all the other

113
00:07:49,300 --> 00:07:54,740
output neurons for what should happen to this second to last layer, again in proportion

114
00:07:54,740 --> 00:08:00,280
to the corresponding weights, and in proportion to how much each of those neurons needs to

115
00:08:00,280 --> 00:08:00,720
change.

116
00:08:01,600 --> 00:08:05,480
This right here is where the idea of propagating backwards comes in.

117
00:08:05,820 --> 00:08:11,200
By adding together all these desired effects, you basically get a list of nudges that you

118
00:08:11,200 --> 00:08:13,360
want to happen to this second to last layer.

119
00:08:14,220 --> 00:08:18,720
And once you have those, you can recursively apply the same process to the relevant weights

120
00:08:18,720 --> 00:08:23,440
and biases that determine those values, repeating the same process I just walked through and

121
00:08:23,440 --> 00:08:25,100
moving backwards through the network.

122
00:08:28,960 --> 00:08:33,640
And zooming out a bit further, remember that this is all just how a single training example

123
00:08:33,640 --> 00:08:37,000
wishes to nudge each one of those weights and biases.

124
00:08:37,480 --> 00:08:41,380
If we only listened to what that 2 wanted, the network would ultimately be incentivized

125
00:08:41,380 --> 00:08:43,220
just to classify all images as a 2.

126
00:08:44,060 --> 00:08:49,060
So what you do is go through this same backprop routine for every other training example,

127
00:08:49,420 --> 00:08:54,720
recording how each of them would like to change the weights and biases, and average together

128
00:08:54,720 --> 00:08:56,000
those desired changes.

129
00:09:01,720 --> 00:09:07,660
This collection here of the averaged nudges to each weight and bias is, loosely speaking,

130
00:09:08,240 --> 00:09:12,260
the negative gradient of the cost function referenced in the last video, or at least

131
00:09:12,260 --> 00:09:13,680
something proportional to it.

132
00:09:14,380 --> 00:09:18,800
I say loosely speaking only because I have yet to get quantitatively precise about those

133
00:09:18,800 --> 00:09:23,540
nudges, but if you understood every change I just referenced, why some are proportionally

134
00:09:23,540 --> 00:09:28,660
bigger than others, and how they all need to be added together, you understand the mechanics

135
00:09:28,660 --> 00:09:31,000
for what backpropagation is actually doing.

136
00:09:33,960 --> 00:09:38,520
By the way, in practice, it takes computers an extremely long time to add up the influence

137
00:09:38,520 --> 00:09:42,440
of every training example every gradient descent step.

138
00:09:43,140 --> 00:09:44,820
So here's what's commonly done instead.

139
00:09:45,480 --> 00:09:49,820
You randomly shuffle your training data and then divide it into a whole bunch of mini-batches,

140
00:09:50,320 --> 00:09:52,420
let's say each one having 100 training examples.

141
00:09:52,940 --> 00:09:56,200
Then you compute a step according to the mini-batch.

142
00:09:56,960 --> 00:10:00,340
It's not going to be the actual gradient of the cost function, which depends on all

143
00:10:00,340 --> 00:10:05,340
of the training data, not this tiny subset, so it's not the most efficient step downhill,

144
00:10:05,780 --> 00:10:09,560
but each mini-batch does give you a pretty good approximation, and more importantly,

145
00:10:09,800 --> 00:10:12,120
it gives you a significant computational speedup.

146
00:10:12,820 --> 00:10:16,920
If you were to plot the trajectory of your network under the relevant cost surface, it

147
00:10:16,920 --> 00:10:21,680
would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps,

148
00:10:21,680 --> 00:10:26,980
rather than a carefully calculating man determining the exact downhill direction of each step

149
00:10:26,980 --> 00:10:30,160
before taking a very slow and careful step in that direction.

150
00:10:31,540 --> 00:10:34,660
This technique is referred to as stochastic gradient descent.

151
00:10:35,960 --> 00:10:39,620
There's a lot going on here, so let's just sum it up for ourselves, shall we?

152
00:10:40,440 --> 00:10:45,560
Backpropagation is the algorithm for determining how a single training example would like to

153
00:10:45,560 --> 00:10:49,580
nudge the weights and biases, not just in terms of whether they should go up or down,

154
00:10:49,580 --> 00:10:54,780
but in terms of what relative proportions to those changes cause the most rapid decrease

155
00:10:54,780 --> 00:10:55,560
to the cost.

156
00:10:56,260 --> 00:11:00,660
A true gradient descent step would involve doing this for all your tens of thousands

157
00:11:00,660 --> 00:11:04,200
of training examples and averaging the desired changes you get.

158
00:11:04,860 --> 00:11:09,980
But that's computationally slow, so instead you randomly subdivide the data into mini-batches

159
00:11:10,320 --> 00:11:13,240
and compute each step with respect to a mini-batch.

160
00:11:14,000 --> 00:11:18,620
Repeatedly going through all of the mini-batches and making these adjustments, you will converge

161
00:11:18,620 --> 00:11:23,380
towards a local minimum of the cost function, which is to say your network will end up doing

162
00:11:23,380 --> 00:11:25,540
a really good job on the training examples.

163
00:11:27,240 --> 00:11:32,600
So with all of that said, every line of code that would go into implementing backprop actually

164
00:11:32,600 --> 00:11:36,720
corresponds with something you have now seen, at least in informal terms.

165
00:11:37,560 --> 00:11:41,820
But sometimes knowing what the math does is only half the battle, and just representing

166
00:11:41,820 --> 00:11:44,120
the damn thing is where it gets all muddled and confusing.

167
00:11:44,860 --> 00:11:49,360
So for those of you who do want to go deeper, the next video goes through the same ideas

168
00:11:49,360 --> 00:11:53,400
that were just presented here, but in terms of the underlying calculus, which should hopefully

169
00:11:53,400 --> 00:11:56,420
make it a little more familiar as you see the topic in other resources.

170
00:11:57,340 --> 00:12:01,280
Before that, one thing worth emphasizing is that for this algorithm to work, and this

171
00:12:01,280 --> 00:12:05,560
goes for all sorts of machine learning beyond just neural networks, you need a lot of training

172
00:12:05,560 --> 00:12:05,900
data.

173
00:12:06,420 --> 00:12:10,660
In our case, one thing that makes handwritten digits such a nice example is that there exists

174
00:12:10,660 --> 00:12:14,740
the MNIST database, with so many examples that have been labeled by humans.

175
00:12:15,300 --> 00:12:18,860
So a common challenge that those of you working in machine learning will be familiar with

176
00:12:18,860 --> 00:12:22,920
is just getting the labeled training data you actually need, whether that's having

177
00:12:22,920 --> 00:12:27,100
people label tens of thousands of images, or whatever other data type you might be dealing

178
00:12:27,100 --> 00:12:27,940
with.

