1
00:00:04,059 --> 00:00:08,880
Here, we tackle backpropagation, the core algorithm behind how neural networks learn.

2
00:00:09,400 --> 00:00:13,423
After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough 

3
00:00:13,423 --> 00:00:17,000
for what the algorithm is actually doing, without any reference to the formulas.

4
00:00:17,660 --> 00:00:20,340
Then, for those of you who do want to dive into the math, 

5
00:00:20,340 --> 00:00:23,020
the next video goes into the calculus underlying all this.

6
00:00:23,820 --> 00:00:27,452
If you watched the last two videos, or if you're just jumping in with the appropriate 

7
00:00:27,452 --> 00:00:31,000
background, you know what a neural network is, and how it feeds forward information.

8
00:00:31,680 --> 00:00:36,059
Here, we're doing the classic example of recognizing handwritten digits whose pixel 

9
00:00:36,059 --> 00:00:39,656
values get fed into the first layer of the network with 784 neurons, 

10
00:00:39,656 --> 00:00:44,035
and I've been showing a network with two hidden layers having just 16 neurons each, 

11
00:00:44,035 --> 00:00:48,310
and an output layer of 10 neurons, indicating which digit the network is choosing 

12
00:00:48,310 --> 00:00:49,040
as its answer.

13
00:00:50,040 --> 00:00:53,094
I'm also expecting you to understand gradient descent, 

14
00:00:53,094 --> 00:00:56,871
as described in the last video, and how what we mean by learning is 

15
00:00:56,871 --> 00:01:01,260
that we want to find which weights and biases minimize a certain cost function.

16
00:01:02,040 --> 00:01:05,813
As a quick reminder, for the cost of a single training example, 

17
00:01:05,813 --> 00:01:10,767
you take the output the network gives, along with the output you wanted it to give, 

18
00:01:10,767 --> 00:01:14,600
and add up the squares of the differences between each component.

19
00:01:15,380 --> 00:01:18,839
Doing this for all of your tens of thousands of training examples and 

20
00:01:18,839 --> 00:01:22,200
averaging the results, this gives you the total cost of the network.

21
00:01:22,200 --> 00:01:26,324
And as if that's not enough to think about, as described in the last video, 

22
00:01:26,324 --> 00:01:30,721
the thing that we're looking for is the negative gradient of this cost function, 

23
00:01:30,721 --> 00:01:34,520
which tells you how you need to change all of the weights and biases, 

24
00:01:34,520 --> 00:01:38,320
all of these connections, so as to most efficiently decrease the cost.

25
00:01:43,260 --> 00:01:45,715
Backpropagation, the topic of this video, is an 

26
00:01:45,715 --> 00:01:48,580
algorithm for computing that crazy complicated gradient.

27
00:01:49,140 --> 00:01:52,573
And the one idea from the last video that I really want you to hold 

28
00:01:52,573 --> 00:01:56,158
firmly in your mind right now is that because thinking of the gradient 

29
00:01:56,158 --> 00:01:59,490
vector as a direction in 13,000 dimensions is, to put it lightly, 

30
00:01:59,490 --> 00:02:03,580
beyond the scope of our imaginations, there's another way you can think about it.

31
00:02:04,600 --> 00:02:07,798
The magnitude of each component here is telling you how 

32
00:02:07,798 --> 00:02:10,940
sensitive the cost function is to each weight and bias.

33
00:02:11,800 --> 00:02:15,798
For example, let's say you go through the process I'm about to describe, 

34
00:02:15,798 --> 00:02:20,563
and you compute the negative gradient, and the component associated with the weight on 

35
00:02:20,563 --> 00:02:25,328
this edge here comes out to be 3.2, while the component associated with this edge here 

36
00:02:25,328 --> 00:02:26,260
comes out as 0.1.

37
00:02:26,820 --> 00:02:30,766
The way you would interpret that is that the cost of the function is 32 times 

38
00:02:30,766 --> 00:02:33,194
more sensitive to changes in that first weight, 

39
00:02:33,194 --> 00:02:35,977
so if you were to wiggle that value just a little bit, 

40
00:02:35,977 --> 00:02:40,075
it's going to cause some change to the cost, and that change is 32 times greater 

41
00:02:40,075 --> 00:02:43,060
than what the same wiggle to that second weight would give.

42
00:02:48,420 --> 00:02:51,416
Personally, when I was first learning about backpropagation, 

43
00:02:51,416 --> 00:02:55,740
I think the most confusing aspect was just the notation and the index chasing of it all.

44
00:02:56,220 --> 00:02:59,490
But once you unwrap what each part of this algorithm is really doing, 

45
00:02:59,490 --> 00:03:02,528
each individual effect it's having is actually pretty intuitive, 

46
00:03:02,528 --> 00:03:06,640
it's just that there's a lot of little adjustments getting layered on top of each other.

47
00:03:07,740 --> 00:03:11,830
So I'm going to start things off here with a complete disregard for the notation, 

48
00:03:11,830 --> 00:03:16,120
and just step through the effects each training example has on the weights and biases.

49
00:03:17,020 --> 00:03:21,555
Because the cost function involves averaging a certain cost per example over 

50
00:03:21,555 --> 00:03:24,383
all the tens of thousands of training examples, 

51
00:03:24,383 --> 00:03:29,154
the way we adjust the weights and biases for a single gradient descent step also 

52
00:03:29,154 --> 00:03:31,040
depends on every single example.

53
00:03:31,680 --> 00:03:35,575
Or rather, in principle it should, but for computational efficiency we'll do a little 

54
00:03:35,575 --> 00:03:39,200
trick later to keep you from needing to hit every single example for every step.

55
00:03:39,200 --> 00:03:42,669
In other cases, right now, all we're going to do is focus 

56
00:03:42,669 --> 00:03:45,960
our attention on one single example, this image of a 2.

57
00:03:46,720 --> 00:03:49,279
What effect should this one training example have 

58
00:03:49,279 --> 00:03:51,480
on how the weights and biases get adjusted?

59
00:03:52,680 --> 00:03:56,264
Let's say we're at a point where the network is not well trained yet, 

60
00:03:56,264 --> 00:03:59,644
so the activations in the output are going to look pretty random, 

61
00:03:59,644 --> 00:04:02,000
maybe something like 0.5, 0.8, 0.2, on and on.

62
00:04:02,520 --> 00:04:04,864
We can't directly change those activations, we 

63
00:04:04,864 --> 00:04:07,160
only have influence on the weights and biases.

64
00:04:07,160 --> 00:04:10,006
But it's helpful to keep track of which adjustments 

65
00:04:10,006 --> 00:04:12,580
we wish should take place to that output layer.

66
00:04:13,360 --> 00:04:16,459
And since we want it to classify the image as a 2, 

67
00:04:16,459 --> 00:04:21,260
we want that third value to get nudged up while all the others get nudged down.

68
00:04:22,060 --> 00:04:25,758
Moreover, the sizes of these nudges should be proportional 

69
00:04:25,758 --> 00:04:29,520
to how far away each current value is from its target value.

70
00:04:30,220 --> 00:04:33,837
For example, the increase to that number 2 neuron's activation 

71
00:04:33,837 --> 00:04:37,914
is in a sense more important than the decrease to the number 8 neuron, 

72
00:04:37,914 --> 00:04:40,900
which is already pretty close to where it should be.

73
00:04:42,040 --> 00:04:45,034
So zooming in further, let's focus just on this one neuron, 

74
00:04:45,034 --> 00:04:47,280
the one whose activation we wish to increase.

75
00:04:48,180 --> 00:04:52,347
Remember, that activation is defined as a certain weighted sum of all 

76
00:04:52,347 --> 00:04:55,443
the activations in the previous layer, plus a bias, 

77
00:04:55,443 --> 00:05:00,444
which is all then plugged into something like the sigmoid squishification function, 

78
00:05:00,444 --> 00:05:01,040
or a ReLU.

79
00:05:01,640 --> 00:05:04,217
So there are three different avenues that can 

80
00:05:04,217 --> 00:05:07,020
team up together to help increase that activation.

81
00:05:07,440 --> 00:05:10,683
You can increase the bias, you can increase the weights, 

82
00:05:10,683 --> 00:05:14,040
and you can change the activations from the previous layer.

83
00:05:14,940 --> 00:05:17,410
Focusing on how the weights should be adjusted, 

84
00:05:17,410 --> 00:05:20,860
notice how the weights actually have differing levels of influence.

85
00:05:21,440 --> 00:05:25,245
The connections with the brightest neurons from the preceding layer have the 

86
00:05:25,245 --> 00:05:29,100
biggest effect since those weights are multiplied by larger activation values.

87
00:05:31,460 --> 00:05:33,934
So if you were to increase one of those weights, 

88
00:05:33,934 --> 00:05:38,126
it actually has a stronger influence on the ultimate cost function than increasing 

89
00:05:38,126 --> 00:05:40,550
the weights of connections with dimmer neurons, 

90
00:05:40,550 --> 00:05:43,480
at least as far as this one training example is concerned.

91
00:05:44,420 --> 00:05:46,631
Remember, when we talk about gradient descent, 

92
00:05:46,631 --> 00:05:50,302
we don't just care about whether each component should get nudged up or down, 

93
00:05:50,302 --> 00:05:53,220
we care about which ones give you the most bang for your buck.

94
00:05:55,020 --> 00:05:58,564
This, by the way, is at least somewhat reminiscent of a theory in 

95
00:05:58,564 --> 00:06:02,592
neuroscience for how biological networks of neurons learn, Hebbian theory, 

96
00:06:02,592 --> 00:06:06,460
often summed up in the phrase, neurons that fire together wire together.

97
00:06:07,260 --> 00:06:11,774
Here, the biggest increases to weights, the biggest strengthening of connections, 

98
00:06:11,774 --> 00:06:14,582
happens between neurons which are the most active, 

99
00:06:14,582 --> 00:06:17,280
and the ones which we wish to become more active.

100
00:06:17,940 --> 00:06:21,137
In a sense, the neurons that are firing while seeing a 2 get more 

101
00:06:21,137 --> 00:06:24,480
strongly linked to those are the ones firing when thinking about a 2.

102
00:06:25,400 --> 00:06:29,406
To be clear, I'm not in a position to make statements one way or another about 

103
00:06:29,406 --> 00:06:33,412
whether artificial networks of neurons behave anything like biological brains, 

104
00:06:33,412 --> 00:06:37,723
and this fires together wire together idea comes with a couple meaningful asterisks, 

105
00:06:37,723 --> 00:06:41,020
but taken as a very loose analogy, I find it interesting to note.

106
00:06:41,940 --> 00:06:45,177
Anyway, the third way we can help increase this neuron's 

107
00:06:45,177 --> 00:06:49,040
activation is by changing all the activations in the previous layer.

108
00:06:49,040 --> 00:06:53,071
Namely, if everything connected to that digit 2 neuron with a positive 

109
00:06:53,071 --> 00:06:57,840
weight got brighter, and if everything connected with a negative weight got dimmer, 

110
00:06:57,840 --> 00:07:00,680
then that digit 2 neuron would become more active.

111
00:07:02,540 --> 00:07:06,433
And similar to the weight changes, you're going to get the most bang for your buck 

112
00:07:06,433 --> 00:07:10,280
by seeking changes that are proportional to the size of the corresponding weights.

113
00:07:12,140 --> 00:07:15,143
Now of course, we cannot directly influence those activations, 

114
00:07:15,143 --> 00:07:17,480
we only have control over the weights and biases.

115
00:07:17,480 --> 00:07:20,695
But just as with the last layer, it's helpful 

116
00:07:20,695 --> 00:07:24,120
to keep a note of what those desired changes are.

117
00:07:24,580 --> 00:07:26,986
But keep in mind, zooming out one step here, this 

118
00:07:26,986 --> 00:07:29,200
is only what that digit 2 output neuron wants.

119
00:07:29,760 --> 00:07:33,991
Remember, we also want all the other neurons in the last layer to become less active, 

120
00:07:33,991 --> 00:07:37,238
and each of those other output neurons has its own thoughts about 

121
00:07:37,238 --> 00:07:39,600
what should happen to that second to last layer.

122
00:07:42,700 --> 00:07:46,943
So, the desire of this digit 2 neuron is added together with the 

123
00:07:46,943 --> 00:07:51,579
desires of all the other output neurons for what should happen to this 

124
00:07:51,579 --> 00:07:56,280
second to last layer, again in proportion to the corresponding weights, 

125
00:07:56,280 --> 00:08:00,720
and in proportion to how much each of those neurons needs to change.

126
00:08:01,600 --> 00:08:05,480
This right here is where the idea of propagating backwards comes in.

127
00:08:05,820 --> 00:08:09,533
By adding together all these desired effects, you basically get a 

128
00:08:09,533 --> 00:08:13,360
list of nudges that you want to happen to this second to last layer.

129
00:08:14,220 --> 00:08:17,895
And once you have those, you can recursively apply the same process to the 

130
00:08:17,895 --> 00:08:20,689
relevant weights and biases that determine those values, 

131
00:08:20,689 --> 00:08:25,100
repeating the same process I just walked through and moving backwards through the network.

132
00:08:28,960 --> 00:08:33,118
And zooming out a bit further, remember that this is all just how a single 

133
00:08:33,118 --> 00:08:37,000
training example wishes to nudge each one of those weights and biases.

134
00:08:37,480 --> 00:08:40,326
If we only listened to what that 2 wanted, the network would 

135
00:08:40,326 --> 00:08:43,220
ultimately be incentivized just to classify all images as a 2.

136
00:08:44,059 --> 00:08:49,301
So what you do is go through this same backprop routine for every other training example, 

137
00:08:49,301 --> 00:08:53,495
recording how each of them would like to change the weights and biases, 

138
00:08:53,495 --> 00:08:56,000
and average together those desired changes.

139
00:09:01,720 --> 00:09:05,941
This collection here of the averaged nudges to each weight and bias is, 

140
00:09:05,941 --> 00:09:10,162
loosely speaking, the negative gradient of the cost function referenced 

141
00:09:10,162 --> 00:09:13,680
in the last video, or at least something proportional to it.

142
00:09:14,380 --> 00:09:18,442
I say loosely speaking only because I have yet to get quantitatively precise 

143
00:09:18,442 --> 00:09:22,347
about those nudges, but if you understood every change I just referenced, 

144
00:09:22,347 --> 00:09:24,879
why some are proportionally bigger than others, 

145
00:09:24,879 --> 00:09:28,942
and how they all need to be added together, you understand the mechanics for 

146
00:09:28,942 --> 00:09:31,000
what backpropagation is actually doing.

147
00:09:33,960 --> 00:09:38,053
By the way, in practice, it takes computers an extremely long time to 

148
00:09:38,053 --> 00:09:42,440
add up the influence of every training example every gradient descent step.

149
00:09:43,140 --> 00:09:44,820
So here's what's commonly done instead.

150
00:09:45,480 --> 00:09:48,974
You randomly shuffle your training data and then divide it into a whole 

151
00:09:48,974 --> 00:09:52,420
bunch of mini-batches, let's say each one having 100 training examples.

152
00:09:52,940 --> 00:09:56,200
Then you compute a step according to the mini-batch.

153
00:09:56,960 --> 00:10:00,060
It's not going to be the actual gradient of the cost function, 

154
00:10:00,060 --> 00:10:03,260
which depends on all of the training data, not this tiny subset, 

155
00:10:03,260 --> 00:10:07,001
so it's not the most efficient step downhill, but each mini-batch does give 

156
00:10:07,001 --> 00:10:09,708
you a pretty good approximation, and more importantly, 

157
00:10:09,708 --> 00:10:12,120
it gives you a significant computational speedup.

158
00:10:12,820 --> 00:10:17,129
If you were to plot the trajectory of your network under the relevant cost surface, 

159
00:10:17,129 --> 00:10:21,541
it would be a little more like a drunk man stumbling aimlessly down a hill but taking 

160
00:10:21,541 --> 00:10:25,850
quick steps, rather than a carefully calculating man determining the exact downhill 

161
00:10:25,850 --> 00:10:30,160
direction of each step before taking a very slow and careful step in that direction.

162
00:10:31,540 --> 00:10:34,660
This technique is referred to as stochastic gradient descent.

163
00:10:35,960 --> 00:10:39,620
There's a lot going on here, so let's just sum it up for ourselves, shall we?

164
00:10:40,440 --> 00:10:44,274
Backpropagation is the algorithm for determining how a single training 

165
00:10:44,274 --> 00:10:47,082
example would like to nudge the weights and biases, 

166
00:10:47,082 --> 00:10:50,106
not just in terms of whether they should go up or down, 

167
00:10:50,106 --> 00:10:53,832
but in terms of what relative proportions to those changes cause the 

168
00:10:53,832 --> 00:10:55,560
most rapid decrease to the cost.

169
00:10:56,260 --> 00:11:00,283
A true gradient descent step would involve doing this for all your tens of 

170
00:11:00,283 --> 00:11:04,200
thousands of training examples and averaging the desired changes you get.

171
00:11:04,860 --> 00:11:08,963
But that's computationally slow, so instead you randomly subdivide the 

172
00:11:08,963 --> 00:11:13,240
data into mini-batches and compute each step with respect to a mini-batch.

173
00:11:14,000 --> 00:11:17,912
Repeatedly going through all of the mini-batches and making these adjustments, 

174
00:11:17,912 --> 00:11:21,082
you will converge towards a local minimum of the cost function, 

175
00:11:21,082 --> 00:11:25,540
which is to say your network will end up doing a really good job on the training examples.

176
00:11:27,240 --> 00:11:32,092
So with all of that said, every line of code that would go into implementing backprop 

177
00:11:32,092 --> 00:11:36,720
actually corresponds with something you have now seen, at least in informal terms.

178
00:11:37,560 --> 00:11:40,525
But sometimes knowing what the math does is only half the battle, 

179
00:11:40,525 --> 00:11:44,120
and just representing the damn thing is where it gets all muddled and confusing.

180
00:11:44,860 --> 00:11:48,622
So for those of you who do want to go deeper, the next video goes through the same 

181
00:11:48,622 --> 00:11:52,158
ideas that were just presented here, but in terms of the underlying calculus, 

182
00:11:52,158 --> 00:11:55,966
which should hopefully make it a little more familiar as you see the topic in other 

183
00:11:55,966 --> 00:11:56,420
resources.

184
00:11:57,340 --> 00:12:00,883
Before that, one thing worth emphasizing is that for this algorithm to work, 

185
00:12:00,883 --> 00:12:04,427
and this goes for all sorts of machine learning beyond just neural networks, 

186
00:12:04,427 --> 00:12:05,900
you need a lot of training data.

187
00:12:06,420 --> 00:12:10,407
In our case, one thing that makes handwritten digits such a nice example is that 

188
00:12:10,407 --> 00:12:14,740
there exists the MNIST database, with so many examples that have been labeled by humans.

189
00:12:15,300 --> 00:12:19,247
So a common challenge that those of you working in machine learning will be familiar with 

190
00:12:19,247 --> 00:12:21,923
is just getting the labeled training data you actually need, 

191
00:12:21,923 --> 00:12:24,731
whether that's having people label tens of thousands of images, 

192
00:12:24,731 --> 00:12:27,100
or whatever other data type you might be dealing with.

