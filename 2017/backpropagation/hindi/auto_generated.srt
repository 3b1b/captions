1
00:00:00,000 --> 00:00:04,719
यहां, हम बैकप्रॉपैगेशन से निपटते हैं, तंत्रिका

2
00:00:04,719 --> 00:00:09,640
नेटवर्क कैसे सीखते हैं इसके पीछे मुख्य एल्गोरिदम।

3
00:00:09,640 --> 00:00:11,782
हम कहां हैं, इसके बारे में एक त्वरित पुनर्कथन के बाद,

4
00:00:11,782 --> 00:00:14,692
पहली चीज जो मैं करूंगा वह यह है कि एल्गोरिदम वास्तव में क्या कर रहा है,

5
00:00:14,692 --> 00:00:17,400
सूत्रों के किसी भी संदर्भ के बिना, एक सहज ज्ञान युक्त पूर्वाभ्यास।

6
00:00:17,400 --> 00:00:20,603
फिर, आपमें से जो लोग गणित में गहराई से उतरना चाहते हैं,

7
00:00:20,603 --> 00:00:24,040
उनके लिए अगला वीडियो इस सब के अंतर्निहित कलन पर आधारित है।

8
00:00:24,040 --> 00:00:27,477
यदि आपने पिछले दो वीडियो देखे हैं, या यदि आप उचित पृष्ठभूमि के साथ आगे बढ़ रहे हैं,

9
00:00:27,477 --> 00:00:31,080
तो आप जानते हैं कि तंत्रिका नेटवर्क क्या है, और यह आगे की जानकारी कैसे प्रदान करता है।

10
00:00:31,080 --> 00:00:34,494
यहां, हम हस्तलिखित अंकों को पहचानने का क्लासिक उदाहरण दे रहे हैं,

11
00:00:34,494 --> 00:00:38,697
जिनके पिक्सेल मान 784 न्यूरॉन्स के साथ नेटवर्क की पहली परत में फीड हो जाते हैं,

12
00:00:38,697 --> 00:00:41,587
और मैं दो छिपी हुई परतों वाला एक नेटवर्क दिखा रहा हूं,

13
00:00:41,587 --> 00:00:45,842
जिनमें से प्रत्येक में केवल 16 न्यूरॉन हैं, और एक आउटपुट है 10 न्यूरॉन्स की परत,

14
00:00:45,842 --> 00:00:49,520
यह दर्शाती है कि नेटवर्क अपने उत्तर के रूप में कौन सा अंक चुन रहा है।

15
00:00:49,520 --> 00:00:52,950
मैं यह भी उम्मीद कर रहा हूं कि आप ग्रेडिएंट डिसेंट को समझेंगे,

16
00:00:52,950 --> 00:00:57,100
जैसा कि पिछले वीडियो में बताया गया है, और सीखने से हमारा मतलब यह है कि हम

17
00:00:57,100 --> 00:01:02,080
यह पता लगाना चाहते हैं कि कौन से वजन और पूर्वाग्रह एक निश्चित लागत फ़ंक्शन को कम करते हैं।

18
00:01:02,080 --> 00:01:06,615
एक त्वरित अनुस्मारक के रूप में, एक एकल प्रशिक्षण उदाहरण की लागत के लिए,

19
00:01:06,615 --> 00:01:12,174
आप उस आउटपुट को लेते हैं जो नेटवर्क देता है, उस आउटपुट के साथ जो आप उसे देना चाहते थे,

20
00:01:12,174 --> 00:01:15,560
और प्रत्येक घटक के बीच अंतर के वर्गों को जोड़ते हैं।

21
00:01:15,560 --> 00:01:19,106
अपने सभी हज़ारों प्रशिक्षण उदाहरणों के लिए ऐसा करने और

22
00:01:19,106 --> 00:01:23,040
परिणामों का औसत निकालने से आपको नेटवर्क की कुल लागत मिलती है।

23
00:01:23,040 --> 00:01:28,360
जैसे कि यह सोचने के लिए पर्याप्त नहीं है, जैसा कि पिछले वीडियो में वर्णित है,

24
00:01:28,360 --> 00:01:33,612
जिस चीज की हम तलाश कर रहे हैं वह इस लागत फ़ंक्शन का नकारात्मक ग्रेडिएंट है,

25
00:01:33,612 --> 00:01:39,832
जो आपको बताता है कि आपको सभी वजन और पूर्वाग्रहों को कैसे बदलने की आवश्यकता है ये कनेक्शन,

26
00:01:39,832 --> 00:01:43,080
ताकि लागत को सबसे कुशलतापूर्वक कम किया जा सके।

27
00:01:43,080 --> 00:01:49,600
बैकप्रॉपैगेशन, इस वीडियो का विषय, उस जटिल जटिल ग्रेडिएंट की गणना के लिए एक एल्गोरिदम है।

28
00:01:49,600 --> 00:01:53,406
पिछले वीडियो से एक विचार जो मैं वास्तव में चाहता हूं कि आप अभी अपने दिमाग

29
00:01:53,406 --> 00:01:57,264
में मजबूती से रखें, क्योंकि 13,000 आयामों में एक दिशा के रूप में ग्रेडिएंट

30
00:01:57,264 --> 00:02:01,688
वेक्टर के बारे में सोचना, इसे हल्के ढंग से कहें तो, हमारी कल्पनाओं के दायरे से परे है,

31
00:02:01,688 --> 00:02:04,620
एक और विचार है जिस तरह से आप इसके बारे में सोच सकते हैं.

32
00:02:04,620 --> 00:02:08,376
यहां प्रत्येक घटक का परिमाण आपको बता रहा है कि लागत फ़ंक्शन

33
00:02:08,376 --> 00:02:11,820
प्रत्येक भार और पूर्वाग्रह के प्रति कितना संवेदनशील है।

34
00:02:11,820 --> 00:02:16,411
उदाहरण के लिए, मान लें कि आप उस प्रक्रिया से गुजरते हैं जिसका मैं वर्णन करने जा रहा हूं,

35
00:02:16,411 --> 00:02:21,054
और नकारात्मक ग्रेडिएंट की गणना करते हैं, और यहां इस किनारे पर वजन से जुड़ा घटक 3 निकलता

36
00:02:21,054 --> 00:02:21,211
है।

37
00:02:21,211 --> 00:02:26,856
2, जबकि इस किनारे से जुड़ा घटक यहाँ 0 के रूप में सामने आता है।

38
00:02:26,856 --> 00:02:26,940
1.

39
00:02:26,940 --> 00:02:31,479
जिस तरह से आप इसकी व्याख्या करेंगे वह यह है कि फ़ंक्शन की लागत उस

40
00:02:31,479 --> 00:02:35,400
पहले वजन में परिवर्तन के प्रति 32 गुना अधिक संवेदनशील है,

41
00:02:35,400 --> 00:02:40,834
इसलिए यदि आप उस मूल्य को थोड़ा सा हिलाते हैं, तो इससे लागत में कुछ बदलाव आएगा,

42
00:02:40,834 --> 00:02:45,580
और वह परिवर्तन होगा यह उस दूसरे वजन के समान झटके से 32 गुना अधिक है।

43
00:02:45,580 --> 00:02:49,997
निजी तौर पर, जब मैं पहली बार बैकप्रॉपैगेशन के बारे में सीख रहा था,

44
00:02:49,997 --> 00:02:55,820
तो मुझे लगता है कि सबसे भ्रमित करने वाला पहलू सिर्फ नोटेशन और इंडेक्स का पीछा करना था।

45
00:02:55,820 --> 00:02:59,655
लेकिन एक बार जब आप यह समझ लेते हैं कि इस एल्गोरिदम का प्रत्येक भाग वास्तव

46
00:02:59,655 --> 00:03:03,645
में क्या कर रहा है, तो इसका प्रत्येक व्यक्तिगत प्रभाव वास्तव में बहुत सहज है,

47
00:03:03,645 --> 00:03:07,740
बात बस इतनी है कि बहुत सारे छोटे-छोटे समायोजन एक-दूसरे के ऊपर परत चढ़ रहे हैं।

48
00:03:07,740 --> 00:03:11,712
इसलिए मैं यहां नोटेशन की पूरी उपेक्षा के साथ चीजों को शुरू करने जा रहा हूं,

49
00:03:11,712 --> 00:03:14,996
और प्रत्येक प्रशिक्षण उदाहरण के वजन और पूर्वाग्रहों पर पड़ने

50
00:03:14,996 --> 00:03:17,380
वाले प्रभावों के बारे में विस्तार से बताऊंगा।

51
00:03:17,380 --> 00:03:22,290
क्योंकि लागत फ़ंक्शन में हजारों प्रशिक्षण उदाहरणों में प्रति उदाहरण एक निश्चित

52
00:03:22,290 --> 00:03:27,015
लागत का औसत शामिल होता है, जिस तरह से हम एक एकल ग्रेडिएंट डिसेंट चरण के लिए

53
00:03:27,015 --> 00:03:31,740
वजन और पूर्वाग्रह को समायोजित करते हैं वह भी हर एक उदाहरण पर निर्भर करता है।

54
00:03:31,740 --> 00:03:35,754
या बल्कि, सिद्धांत रूप में यह होना चाहिए, लेकिन कम्प्यूटेशनल दक्षता के लिए हम बाद में एक

55
00:03:35,754 --> 00:03:39,634
छोटी सी तरकीब अपनाएंगे ताकि आपको हर चरण के लिए हर एक उदाहरण को हिट करने की आवश्यकता न

56
00:03:39,634 --> 00:03:39,860
पड़े।

57
00:03:39,860 --> 00:03:43,696
अन्य मामलों में, अभी, हम अपना ध्यान केवल एक उदाहरण,

58
00:03:43,696 --> 00:03:46,780
2 की इस छवि पर केंद्रित करने जा रहे हैं।

59
00:03:46,780 --> 00:03:49,307
इस एक प्रशिक्षण उदाहरण का इस बात पर क्या प्रभाव होना

60
00:03:49,307 --> 00:03:51,740
चाहिए कि वज़न और पूर्वाग्रह कैसे समायोजित होते हैं?

61
00:03:51,740 --> 00:03:56,755
मान लीजिए कि हम एक ऐसे बिंदु पर हैं जहां नेटवर्क अभी तक अच्छी तरह से प्रशिक्षित नहीं है,

62
00:03:56,755 --> 00:04:00,516
इसलिए आउटपुट में सक्रियण काफी यादृच्छिक दिखेंगे, शायद 0 जैसा कुछ।

63
00:04:00,516 --> 00:04:00,776
5, 0.

64
00:04:00,776 --> 00:04:01,036
8, 0.

65
00:04:01,036 --> 00:04:02,780
2, पर और आगे.

66
00:04:02,780 --> 00:04:05,420
हम उन सक्रियणों को सीधे तौर पर नहीं बदल सकते हैं,

67
00:04:05,420 --> 00:04:08,113
हम केवल वज़न और पूर्वाग्रहों पर प्रभाव डालते हैं,

68
00:04:08,113 --> 00:04:11,562
लेकिन यह ट्रैक रखना सहायक होता है कि हम चाहते हैं कि उस आउटपुट

69
00:04:11,562 --> 00:04:13,340
परत पर कौन सा समायोजन होना चाहिए।

70
00:04:13,340 --> 00:04:17,232
और चूँकि हम चाहते हैं कि यह छवि को 2 के रूप में वर्गीकृत करे,

71
00:04:17,232 --> 00:04:21,700
हम चाहते हैं कि तीसरा मान ऊपर की ओर हो, जबकि अन्य सभी नीचे की ओर हों।

72
00:04:21,700 --> 00:04:26,071
इसके अलावा, इन नज़ों का आकार इस बात पर आनुपातिक होना चाहिए

73
00:04:26,071 --> 00:04:30,220
कि प्रत्येक वर्तमान मान अपने लक्ष्य मान से कितनी दूर है।

74
00:04:30,220 --> 00:04:35,960
उदाहरण के लिए, उस संख्या 2 न्यूरॉन की सक्रियता में वृद्धि एक मायने में संख्या 8

75
00:04:35,960 --> 00:04:42,060
न्यूरॉन की कमी से अधिक महत्वपूर्ण है, जो पहले से ही काफी करीब है जहां इसे होना चाहिए।

76
00:04:42,060 --> 00:04:45,797
तो आगे बढ़ते हुए, आइए केवल इस एक न्यूरॉन पर ध्यान केंद्रित करें,

77
00:04:45,797 --> 00:04:47,900
जिसकी सक्रियता हम बढ़ाना चाहते हैं।

78
00:04:47,900 --> 00:04:52,996
याद रखें, उस सक्रियण को पिछली परत में सभी सक्रियणों के एक निश्चित भारित योग के साथ-

79
00:04:52,996 --> 00:04:56,066
साथ एक पूर्वाग्रह के रूप में परिभाषित किया गया है,

80
00:04:56,066 --> 00:04:58,891
जिसे बाद में सिग्मॉइड स्क्विशिफिकेशन फ़ंक्शन,

81
00:04:58,891 --> 00:05:01,900
या एक ReLU जैसी किसी चीज़ में प्लग किया जाता है।

82
00:05:01,900 --> 00:05:08,060
इसलिए तीन अलग-अलग रास्ते हैं जो एक साथ मिलकर उस सक्रियता को बढ़ाने में मदद कर सकते हैं।

83
00:05:08,060 --> 00:05:11,994
आप पूर्वाग्रह बढ़ा सकते हैं, आप वजन बढ़ा सकते हैं,

84
00:05:11,994 --> 00:05:15,300
और आप पिछली परत से सक्रियता बदल सकते हैं।

85
00:05:15,300 --> 00:05:18,357
वज़न को कैसे समायोजित किया जाना चाहिए, इस पर ध्यान केंद्रित करते हुए,

86
00:05:18,357 --> 00:05:21,460
ध्यान दें कि वज़न का वास्तव में प्रभाव के विभिन्न स्तर कैसे होते हैं।

87
00:05:21,460 --> 00:05:26,440
पिछली परत से सबसे चमकीले न्यूरॉन्स के साथ कनेक्शन का सबसे बड़ा प्रभाव

88
00:05:26,440 --> 00:05:31,420
होता है क्योंकि उन भारों को बड़े सक्रियण मूल्यों से गुणा किया जाता है।

89
00:05:31,420 --> 00:05:34,502
इसलिए यदि आप उन भारों में से किसी एक को बढ़ाना चाहते हैं,

90
00:05:34,502 --> 00:05:38,612
तो इसका वास्तव में डिमर न्यूरॉन्स के साथ कनेक्शन के भार को बढ़ाने की तुलना

91
00:05:38,612 --> 00:05:41,153
में अंतिम लागत फ़ंक्शन पर अधिक प्रभाव पड़ता है,

92
00:05:41,153 --> 00:05:44,020
कम से कम जहां तक इस एक प्रशिक्षण उदाहरण का संबंध है।

93
00:05:44,020 --> 00:05:46,597
याद रखें, जब हम ग्रेडिएंट डिसेंट के बारे में बात करते हैं,

94
00:05:46,597 --> 00:05:50,375
तो हम सिर्फ इस बात की परवाह नहीं करते हैं कि प्रत्येक घटक को ऊपर या नीचे जाना चाहिए,

95
00:05:50,375 --> 00:05:54,020
हम इसकी परवाह करते हैं कि कौन सा घटक आपको आपके पैसे के लिए सबसे अधिक लाभ देता है।

96
00:05:54,020 --> 00:05:58,345
वैसे, यह कम से कम कुछ हद तक तंत्रिका विज्ञान के एक सिद्धांत की याद दिलाता है

97
00:05:58,345 --> 00:06:01,940
कि न्यूरॉन्स के जैविक नेटवर्क कैसे सीखते हैं, हेब्बियन सिद्धांत,

98
00:06:01,940 --> 00:06:06,940
जिसे अक्सर वाक्यांश में अभिव्यक्त किया जाता है, न्यूरॉन्स जो एक साथ तार से आग लगाते हैं।

99
00:06:06,940 --> 00:06:11,099
यहां, वजन में सबसे बड़ी वृद्धि, कनेक्शन की सबसे बड़ी मजबूती,

100
00:06:11,099 --> 00:06:16,575
उन न्यूरॉन्स के बीच होती है जो सबसे अधिक सक्रिय हैं और जिनके बारे में हम अधिक

101
00:06:16,575 --> 00:06:18,100
सक्रिय होना चाहते हैं।

102
00:06:18,100 --> 00:06:21,005
एक अर्थ में, 2 को देखते समय जो न्यूरॉन्स सक्रिय होते हैं,

103
00:06:21,005 --> 00:06:25,440
वे इसके बारे में सोचते समय सक्रिय होने वाले न्यूरॉन्स से अधिक मजबूती से जुड़ जाते हैं।

104
00:06:25,440 --> 00:06:29,386
स्पष्ट होने के लिए, मैं इस बारे में एक या दूसरे तरीके से बयान देने की स्थिति में

105
00:06:29,386 --> 00:06:33,429
नहीं हूं कि क्या न्यूरॉन्स के कृत्रिम नेटवर्क जैविक मस्तिष्क की तरह कुछ भी व्यवहार

106
00:06:33,429 --> 00:06:37,375
करते हैं, और यह तारों को एक साथ जोड़ता है विचार कुछ सार्थक तारांकन के साथ आता है,

107
00:06:37,375 --> 00:06:41,760
लेकिन इसे बहुत ही ढीले के रूप में लिया जाता है सादृश्य, मुझे यह नोट करना दिलचस्प लगता है।

108
00:06:41,760 --> 00:06:45,621
वैसे भी, तीसरा तरीका जिससे हम इस न्यूरॉन की सक्रियता को बढ़ाने

109
00:06:45,621 --> 00:06:49,360
में मदद कर सकते हैं वह है पिछली परत की सभी सक्रियता को बदलना।

110
00:06:49,360 --> 00:06:55,435
अर्थात्, यदि सकारात्मक भार वाले उस अंक 2 न्यूरॉन से जुड़ी हर चीज चमकीली हो गई,

111
00:06:55,435 --> 00:06:59,408
और यदि नकारात्मक भार से जुड़ी हर चीज धुंधली हो गई,

112
00:06:59,408 --> 00:07:02,680
तो वह अंक 2 न्यूरॉन अधिक सक्रिय हो जाएगा।

113
00:07:02,680 --> 00:07:06,437
और वज़न परिवर्तनों के समान, आप संबंधित वज़न के आकार के आनुपातिक

114
00:07:06,437 --> 00:07:10,840
परिवर्तनों की तलाश करके अपने पैसे का सबसे अधिक लाभ प्राप्त करने जा रहे हैं।

115
00:07:10,840 --> 00:07:15,302
अब निःसंदेह, हम सीधे तौर पर उन सक्रियताओं को प्रभावित नहीं कर सकते हैं,

116
00:07:15,302 --> 00:07:18,320
हमारा नियंत्रण केवल वज़न और पूर्वाग्रहों पर है।

117
00:07:18,320 --> 00:07:23,960
लेकिन पिछली परत की तरह ही, यह ध्यान रखना भी उपयोगी है कि वे वांछित परिवर्तन क्या हैं।

118
00:07:23,960 --> 00:07:27,167
लेकिन ध्यान रखें, यहां एक कदम ज़ूम आउट करना, यह

119
00:07:27,167 --> 00:07:30,040
वही है जो वह अंक 2 आउटपुट न्यूरॉन चाहता है।

120
00:07:30,040 --> 00:07:35,631
याद रखें, हम यह भी चाहते हैं कि अंतिम परत के अन्य सभी न्यूरॉन्स कम सक्रिय हो जाएं,

121
00:07:35,631 --> 00:07:39,927
और उन अन्य आउटपुट न्यूरॉन्स में से प्रत्येक के अपने विचार हैं

122
00:07:39,927 --> 00:07:43,200
कि उस दूसरी से अंतिम परत के साथ क्या होना चाहिए।

123
00:07:43,200 --> 00:07:49,229
तो इस अंक 2 न्यूरॉन की इच्छा को अन्य सभी आउटपुट न्यूरॉन की इच्छाओं के साथ जोड़ा

124
00:07:49,229 --> 00:07:56,012
जाता है कि इस दूसरी से आखिरी परत के साथ क्या होना चाहिए, फिर से संबंधित वजन के अनुपात में,

125
00:07:56,012 --> 00:08:01,740
और उनमें से प्रत्येक न्यूरॉन को कितनी आवश्यकता है इसके अनुपात में को बदलने।

126
00:08:01,740 --> 00:08:05,940
यहीं वह जगह है जहां पीछे की ओर प्रचार करने का विचार आता है।

127
00:08:05,940 --> 00:08:10,029
इन सभी वांछित प्रभावों को एक साथ जोड़कर, आपको मूल रूप से उन संकेतों

128
00:08:10,029 --> 00:08:14,300
की एक सूची मिलती है जिन्हें आप इस दूसरी से आखिरी परत तक करना चाहते हैं।

129
00:08:14,300 --> 00:08:18,741
और एक बार जब आपके पास वे होते हैं, तो आप उसी प्रक्रिया को उन प्रासंगिक भारों और

130
00:08:18,741 --> 00:08:23,572
पूर्वाग्रहों पर पुनरावर्ती रूप से लागू कर सकते हैं जो उन मूल्यों को निर्धारित करते हैं,

131
00:08:23,572 --> 00:08:28,513
उसी प्रक्रिया को दोहराते हुए जिससे मैं अभी गुजरा हूं और नेटवर्क के माध्यम से पीछे की ओर

132
00:08:28,513 --> 00:08:29,180
बढ़ रहा हूं।

133
00:08:29,180 --> 00:08:33,141
और थोड़ा और ज़ूम आउट करते हुए, याद रखें कि यह सब ठीक उसी तरह है जैसे एक एकल

134
00:08:33,141 --> 00:08:37,520
प्रशिक्षण उदाहरण उन वज़न और पूर्वाग्रहों में से प्रत्येक को नियंत्रित करना चाहता है।

135
00:08:37,520 --> 00:08:40,854
यदि हम केवल वही सुनते हैं जो वह 2 चाहता था, तो नेटवर्क को अंततः सभी

136
00:08:40,854 --> 00:08:44,140
छवियों को 2 के रूप में वर्गीकृत करने के लिए प्रोत्साहित किया जाएगा।

137
00:08:44,140 --> 00:08:51,354
तो आप जो करते हैं वह हर दूसरे प्रशिक्षण उदाहरण के लिए इसी बैकप्रॉप रूटीन से गुजरते हैं,

138
00:08:51,354 --> 00:08:58,319
यह रिकॉर्ड करते हुए कि उनमें से प्रत्येक वजन और पूर्वाग्रह को कैसे बदलना चाहते हैं,

139
00:08:58,319 --> 00:09:02,300
और उन वांछित परिवर्तनों को एक साथ औसत करते हैं।

140
00:09:02,300 --> 00:09:06,190
यहां प्रत्येक वजन और पूर्वाग्रह के औसत संकेतों का यह संग्रह,

141
00:09:06,190 --> 00:09:11,896
शिथिल रूप से कहें तो, पिछले वीडियो में संदर्भित लागत फ़ंक्शन का नकारात्मक ग्रेडिएंट है,

142
00:09:11,896 --> 00:09:14,360
या कम से कम इसके लिए आनुपातिक कुछ है।

143
00:09:14,360 --> 00:09:18,329
मैं शिथिल रूप से केवल इसलिए कह रहा हूं क्योंकि मुझे अभी तक उन संकेतों के

144
00:09:18,329 --> 00:09:20,994
बारे में मात्रात्मक रूप से सटीक नहीं मिल पाया है,

145
00:09:20,994 --> 00:09:24,148
लेकिन अगर आप मेरे द्वारा संदर्भित हर बदलाव को समझ गए हैं,

146
00:09:24,148 --> 00:09:27,356
तो कुछ दूसरों की तुलना में आनुपातिक रूप से बड़े क्यों हैं,

147
00:09:27,356 --> 00:09:29,967
और उन सभी को एक साथ जोड़ने की आवश्यकता कैसे है,

148
00:09:29,967 --> 00:09:34,100
आप इसके लिए यांत्रिकी को समझते हैं बैकप्रोपेगेशन वास्तव में क्या कर रहा है।

149
00:09:34,100 --> 00:09:38,543
वैसे, व्यवहार में, प्रत्येक प्रशिक्षण उदाहरण के प्रत्येक ग्रेडिएंट

150
00:09:38,543 --> 00:09:43,120
डिसेंट चरण के प्रभाव को जोड़ने में कंप्यूटर को बहुत लंबा समय लगता है।

151
00:09:43,120 --> 00:09:45,540
तो इसके बजाय यहाँ वह है जो आमतौर पर किया जाता है।

152
00:09:45,540 --> 00:09:48,879
आप अपने प्रशिक्षण डेटा को बेतरतीब ढंग से फेरबदल करते हैं और इसे मिनी-

153
00:09:48,879 --> 00:09:52,847
बैचों के एक पूरे समूह में विभाजित करते हैं, मान लें कि प्रत्येक में 100 प्रशिक्षण

154
00:09:52,847 --> 00:09:53,380
उदाहरण हैं।

155
00:09:53,380 --> 00:09:56,980
फिर आप मिनी-बैच के अनुसार एक चरण की गणना करें।

156
00:09:56,980 --> 00:10:01,643
यह लागत फ़ंक्शन का वास्तविक ग्रेडिएंट नहीं है, जो सभी प्रशिक्षण डेटा पर निर्भर करता है,

157
00:10:01,643 --> 00:10:05,395
न कि इस छोटे उपसमुच्चय पर, इसलिए यह डाउनहिल का सबसे कुशल कदम नहीं है,

158
00:10:05,395 --> 00:10:08,558
लेकिन प्रत्येक मिनी-बैच आपको एक बहुत अच्छा अनुमान देता है,

159
00:10:08,558 --> 00:10:12,900
और इससे भी महत्वपूर्ण बात यह है आपको एक महत्वपूर्ण कम्प्यूटेशनल स्पीडअप देता है।

160
00:10:12,900 --> 00:10:17,014
यदि आप प्रासंगिक लागत सतह के तहत अपने नेटवर्क के प्रक्षेप पथ की योजना बनाते हैं,

161
00:10:17,014 --> 00:10:20,717
तो यह कुछ हद तक एक नशे में धुत आदमी की तरह होगा जो लक्ष्यहीन रूप से एक

162
00:10:20,717 --> 00:10:23,597
पहाड़ी से नीचे गिर रहा है, लेकिन तेजी से कदम उठा रहा है,

163
00:10:23,597 --> 00:10:27,454
न कि एक सावधानीपूर्वक गणना करने वाला व्यक्ति प्रत्येक कदम की सटीक डाउनहिल

164
00:10:27,454 --> 00:10:31,620
दिशा निर्धारित करता है। उस दिशा में बहुत धीमा और सावधानीपूर्वक कदम उठाने से पहले।

165
00:10:31,620 --> 00:10:35,200
इस तकनीक को स्टोकेस्टिक ग्रेडिएंट डिसेंट कहा जाता है।

166
00:10:35,200 --> 00:10:40,400
यहाँ बहुत कुछ चल रहा है, तो आइए इसे अपने लिए संक्षेप में प्रस्तुत करें, क्या हम करेंगे?

167
00:10:40,400 --> 00:10:45,540
बैकप्रॉपैगेशन यह निर्धारित करने के लिए एल्गोरिदम है कि एक एकल प्रशिक्षण उदाहरण वजन और

168
00:10:45,540 --> 00:10:50,681
पूर्वाग्रहों को कैसे कम करना चाहेगा, न केवल इस संदर्भ में कि उन्हें ऊपर जाना चाहिए या

169
00:10:50,681 --> 00:10:55,881
नीचे जाना चाहिए, बल्कि उन परिवर्तनों के सापेक्ष अनुपात के कारण सबसे तेजी से कमी आती है।

170
00:10:55,881 --> 00:10:56,240
लागत।

171
00:10:56,240 --> 00:11:00,639
एक सच्चे ग्रेडिएंट डिसेंट चरण में आपके सभी दसियों और हजारों प्रशिक्षण उदाहरणों के

172
00:11:00,639 --> 00:11:04,502
लिए ऐसा करना और आपके द्वारा प्राप्त वांछित परिवर्तनों का औसत शामिल होगा,

173
00:11:04,502 --> 00:11:08,312
लेकिन यह कम्प्यूटेशनल रूप से धीमा है, इसलिए इसके बजाय आप डेटा को मिनी-

174
00:11:08,312 --> 00:11:12,765
बैचों में यादृच्छिक रूप से उप-विभाजित करते हैं और प्रत्येक चरण की गणना एक के संबंध

175
00:11:12,765 --> 00:11:14,000
में करते हैं। मिनी-बैच।

176
00:11:14,000 --> 00:11:18,583
बार-बार सभी मिनी-बैचों से गुजरते हुए और इन समायोजनों को करते हुए,

177
00:11:18,583 --> 00:11:21,898
आप स्थानीय न्यूनतम लागत फ़ंक्शन की ओर जुटेंगे,

178
00:11:21,898 --> 00:11:27,540
जिसका अर्थ है कि आपका नेटवर्क प्रशिक्षण उदाहरणों पर वास्तव में अच्छा काम करेगा।

179
00:11:27,540 --> 00:11:32,695
तो जो कुछ कहा गया है, उसके साथ, कोड की प्रत्येक पंक्ति जो बैकप्रॉप को लागू करने में जाएगी,

180
00:11:32,695 --> 00:11:37,680
वास्तव में उस चीज़ से मेल खाती है जिसे आपने अब देखा है, कम से कम अनौपचारिक शब्दों में।

181
00:11:37,680 --> 00:11:40,702
लेकिन कभी-कभी यह जानना कि गणित क्या करता है, केवल आधी लड़ाई है,

182
00:11:40,702 --> 00:11:44,780
और केवल उस लानत चीज़ का प्रतिनिधित्व करना है जहां यह सब गड़बड़ और भ्रमित हो जाता है।

183
00:11:44,780 --> 00:11:47,076
तो, आप में से जो लोग गहराई में जाना चाहते हैं,

184
00:11:47,076 --> 00:11:51,120
उनके लिए अगला वीडियो उन्हीं विचारों पर आधारित है जो अभी यहां प्रस्तुत किए गए थे,

185
00:11:51,120 --> 00:11:55,313
लेकिन अंतर्निहित गणना के संदर्भ में, उम्मीद है कि जैसे ही आप विषय को देखेंगे तो यह

186
00:11:55,313 --> 00:11:57,460
थोड़ा और अधिक परिचित हो जाएगा। अन्य संसाधन।

187
00:11:57,460 --> 00:12:01,050
इससे पहले, एक बात पर जोर देने लायक बात यह है कि इस एल्गोरिदम को काम करने के लिए,

188
00:12:01,050 --> 00:12:04,551
और यह केवल तंत्रिका नेटवर्क से परे सभी प्रकार की मशीन लर्निंग के लिए जाता है,

189
00:12:04,551 --> 00:12:06,840
आपको बहुत सारे प्रशिक्षण डेटा की आवश्यकता होती है।

190
00:12:06,840 --> 00:12:10,256
हमारे मामले में, एक चीज़ जो हस्तलिखित अंकों को इतना अच्छा उदाहरण बनाती है,

191
00:12:10,256 --> 00:12:12,979
वह यह है कि एमएनआईएसटी डेटाबेस मौजूद है, जिसमें बहुत सारे

192
00:12:12,979 --> 00:12:15,380
उदाहरण हैं जिन्हें मनुष्यों द्वारा लेबल किया गया है।

193
00:12:15,380 --> 00:12:17,494
तो एक आम चुनौती जिससे आपमें से मशीन लर्निंग में काम करने वाले परिचित होंगे,

194
00:12:17,494 --> 00:12:20,004
वह है केवल लेबल किए गए प्रशिक्षण डेटा को प्राप्त करना जिसकी आपको वास्तव में आवश्यकता है,

195
00:12:20,004 --> 00:12:21,357
चाहे वह लोगों को हजारों छवियों को लेबल करना हो,

196
00:12:21,357 --> 00:12:22,880
या किसी भी अन्य डेटा प्रकार के साथ आप काम कर रहे हों।

