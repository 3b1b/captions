1
00:00:00,000 --> 00:00:09,640
यहां, हम बैकप्रॉपैगेशन से निपटते हैं, तंत्रिका नेटवर्क कैसे सीखते हैं इसके पीछे मुख्य एल्गोरिदम।

2
00:00:09,640 --> 00:00:13,320
हम कहां हैं, इसके बारे में एक त्वरित पुनर्कथन के बाद, पहली चीज जो मैं करूंगा वह यह है कि

3
00:00:13,320 --> 00:00:17,400
एल्गोरिदम वास्तव में क्या कर रहा है, सूत्रों के किसी भी संदर्भ के बिना, एक सहज ज्ञान युक्त पूर्वाभ्यास।

4
00:00:17,400 --> 00:00:21,400
फिर, आपमें से जो लोग गणित में गहराई से उतरना चाहते हैं,

5
00:00:21,400 --> 00:00:24,040
उनके लिए अगला वीडियो इस सब के अंतर्निहित कलन पर आधारित है।

6
00:00:24,040 --> 00:00:27,320
यदि आपने पिछले दो वीडियो देखे हैं, या यदि आप उचित पृष्ठभूमि के साथ आगे बढ़ रहे हैं,

7
00:00:27,320 --> 00:00:31,080
तो आप जानते हैं कि तंत्रिका नेटवर्क क्या है, और यह आगे की जानकारी कैसे प्रदान करता है।

8
00:00:31,080 --> 00:00:35,520
यहां, हम हस्तलिखित अंकों को पहचानने का क्लासिक उदाहरण दे रहे हैं, जिनके पिक्सेल मान 784 न्यूरॉन्स के

9
00:00:35,520 --> 00:00:40,280
साथ नेटवर्क की पहली परत में फीड हो जाते हैं, और मैं दो छिपी हुई परतों वाला एक

10
00:00:40,280 --> 00:00:44,720
नेटवर्क दिखा रहा हूं, जिनमें से प्रत्येक में केवल 16 न्यूरॉन हैं, और एक आउटपुट है 10 न्यूरॉन्स

11
00:00:44,720 --> 00:00:49,520
की परत, यह दर्शाती है कि नेटवर्क अपने उत्तर के रूप में कौन सा अंक चुन रहा है।

12
00:00:49,520 --> 00:00:54,480
मैं यह भी उम्मीद कर रहा हूं कि आप ग्रेडिएंट डिसेंट को समझेंगे, जैसा कि पिछले वीडियो

13
00:00:54,480 --> 00:01:00,160
में बताया गया है, और सीखने से हमारा मतलब यह है कि हम यह पता लगाना

14
00:01:00,160 --> 00:01:02,080
चाहते हैं कि कौन से वजन और पूर्वाग्रह एक निश्चित लागत फ़ंक्शन को कम करते हैं।

15
00:01:02,080 --> 00:01:07,560
एक त्वरित अनुस्मारक के रूप में, एक एकल प्रशिक्षण उदाहरण की लागत के लिए, आप

16
00:01:07,560 --> 00:01:12,920
उस आउटपुट को लेते हैं जो नेटवर्क देता है, उस आउटपुट के साथ जो आप

17
00:01:12,920 --> 00:01:15,560
उसे देना चाहते थे, और प्रत्येक घटक के बीच अंतर के वर्गों को जोड़ते हैं।

18
00:01:15,560 --> 00:01:20,160
अपने सभी हज़ारों प्रशिक्षण उदाहरणों के लिए ऐसा करने और परिणामों

19
00:01:20,160 --> 00:01:23,040
का औसत निकालने से आपको नेटवर्क की कुल लागत मिलती है।

20
00:01:23,040 --> 00:01:26,320
जैसे कि यह सोचने के लिए पर्याप्त नहीं है, जैसा कि पिछले वीडियो में वर्णित

21
00:01:26,320 --> 00:01:31,700
है, जिस चीज की हम तलाश कर रहे हैं वह इस लागत फ़ंक्शन का नकारात्मक

22
00:01:31,700 --> 00:01:36,000
ग्रेडिएंट है, जो आपको बताता है कि आपको सभी वजन और पूर्वाग्रहों को कैसे बदलने

23
00:01:36,000 --> 00:01:43,080
की आवश्यकता है ये कनेक्शन, ताकि लागत को सबसे कुशलतापूर्वक कम किया जा सके।

24
00:01:43,080 --> 00:01:48,600
बैकप्रॉपैगेशन, इस वीडियो का विषय, उस जटिल जटिल

25
00:01:48,600 --> 00:01:49,600
ग्रेडिएंट की गणना के लिए एक एल्गोरिदम है।

26
00:01:49,600 --> 00:01:53,300
पिछले वीडियो से एक विचार जो मैं वास्तव में चाहता हूं कि आप अभी अपने दिमाग

27
00:01:53,300 --> 00:01:58,280
में मजबूती से रखें, क्योंकि 13,000 आयामों में एक दिशा के रूप में ग्रेडिएंट वेक्टर

28
00:01:58,280 --> 00:02:02,660
के बारे में सोचना, इसे हल्के ढंग से कहें तो, हमारी कल्पनाओं के दायरे से परे

29
00:02:02,660 --> 00:02:04,620
है, एक और विचार है जिस तरह से आप इसके बारे में सोच सकते हैं.

30
00:02:04,620 --> 00:02:09,700
यहां प्रत्येक घटक का परिमाण आपको बता रहा है कि लागत

31
00:02:09,700 --> 00:02:11,820
फ़ंक्शन प्रत्येक भार और पूर्वाग्रह के प्रति कितना संवेदनशील है।

32
00:02:11,820 --> 00:02:15,180
उदाहरण के लिए, मान लीजिए कि आप उस प्रक्रिया से गुजरते हैं जिसका मैं वर्णन करने जा रहा

33
00:02:15,180 --> 00:02:19,800
हूं, और नकारात्मक ग्रेडिएंट की गणना करते हैं, और यहां इस किनारे पर वजन से जुड़ा घटक

34
00:02:19,800 --> 00:02:26,940
3 निकलता है। 2, जबकि इस किनारे से जुड़ा घटक यहाँ 0 के रूप में सामने आता है। 1.

35
00:02:26,940 --> 00:02:31,520
जिस तरह से आप इसकी व्याख्या करेंगे वह यह है कि फ़ंक्शन की लागत उस

36
00:02:31,520 --> 00:02:36,100
पहले वजन में परिवर्तन के प्रति 32 गुना अधिक संवेदनशील है, इसलिए यदि आप उस

37
00:02:36,100 --> 00:02:40,780
मूल्य को थोड़ा सा हिलाते हैं, तो इससे लागत में कुछ बदलाव आएगा, और वह

38
00:02:40,780 --> 00:02:45,580
परिवर्तन होगा यह उस दूसरे वजन के समान झटके से 32 गुना अधिक है।

39
00:02:45,580 --> 00:02:52,500
व्यक्तिगत रूप से, जब मैं पहली बार बैकप्रॉपैगेशन के बारे में सीख रहा था, तो मुझे

40
00:02:52,500 --> 00:02:55,820
लगता है कि सबसे भ्रमित करने वाला पहलू सिर्फ नोटेशन और इंडेक्स का पीछा करना था।

41
00:02:55,820 --> 00:03:00,240
लेकिन एक बार जब आप यह समझ लेते हैं कि इस एल्गोरिदम का प्रत्येक भाग वास्तव

42
00:03:00,240 --> 00:03:04,540
में क्या कर रहा है, तो इसका प्रत्येक व्यक्तिगत प्रभाव वास्तव में बहुत सहज है, यह

43
00:03:04,540 --> 00:03:07,740
सिर्फ इतना है कि बहुत सारे छोटे समायोजन एक-दूसरे के ऊपर परत चढ़ रहे हैं।

44
00:03:07,740 --> 00:03:11,380
इसलिए मैं यहां नोटेशन की पूरी उपेक्षा के साथ चीजों को शुरू करने जा रहा हूं, और

45
00:03:11,380 --> 00:03:17,380
प्रत्येक प्रशिक्षण उदाहरण के वजन और पूर्वाग्रहों पर पड़ने वाले प्रभावों के बारे में विस्तार से बताऊंगा।

46
00:03:17,380 --> 00:03:21,880
क्योंकि लागत फ़ंक्शन में हजारों प्रशिक्षण उदाहरणों में प्रति उदाहरण एक निश्चित लागत का औसत

47
00:03:21,880 --> 00:03:26,980
शामिल होता है, जिस तरह से हम एक एकल ग्रेडिएंट डिसेंट चरण के लिए वजन

48
00:03:26,980 --> 00:03:31,740
और पूर्वाग्रह को समायोजित करते हैं वह भी हर एक उदाहरण पर निर्भर करता है।

49
00:03:31,740 --> 00:03:35,300
या बल्कि, सिद्धांत रूप में यह होना चाहिए, लेकिन कम्प्यूटेशनल दक्षता के लिए हम बाद में एक छोटी सी

50
00:03:35,300 --> 00:03:39,860
तरकीब अपनाएंगे ताकि आपको हर चरण के लिए हर एक उदाहरण को हिट करने की आवश्यकता न पड़े।

51
00:03:39,860 --> 00:03:44,460
अन्य मामलों में, अभी, हम अपना ध्यान केवल एक उदाहरण,

52
00:03:44,460 --> 00:03:46,780
2 की इस छवि पर केंद्रित करने जा रहे हैं।

53
00:03:46,780 --> 00:03:51,740
इस एक प्रशिक्षण उदाहरण का इस बात पर क्या प्रभाव होना चाहिए कि वज़न और पूर्वाग्रह कैसे समायोजित होते हैं?

54
00:03:51,740 --> 00:03:56,040
मान लीजिए कि हम एक ऐसे बिंदु पर हैं जहां नेटवर्क अभी तक अच्छी तरह से प्रशिक्षित

55
00:03:56,040 --> 00:04:01,620
नहीं है, इसलिए आउटपुट में सक्रियण काफी यादृच्छिक दिखेंगे, शायद 0 जैसा कुछ। 5, 0. 8, 0. 2,

56
00:04:01,620 --> 00:04:02,780
पर और आगे.

57
00:04:02,780 --> 00:04:06,700
हम उन सक्रियणों को सीधे तौर पर नहीं बदल सकते हैं, हम केवल वज़न

58
00:04:06,700 --> 00:04:11,380
और पूर्वाग्रहों पर प्रभाव डालते हैं, लेकिन यह ट्रैक रखना सहायक होता है कि

59
00:04:11,380 --> 00:04:13,340
हम चाहते हैं कि उस आउटपुट परत पर कौन सा समायोजन होना चाहिए।

60
00:04:13,340 --> 00:04:18,220
और चूँकि हम चाहते हैं कि यह छवि को 2 के रूप में वर्गीकृत करे, हम

61
00:04:18,220 --> 00:04:21,700
चाहते हैं कि तीसरा मान ऊपर की ओर हो, जबकि अन्य सभी नीचे की ओर हों।

62
00:04:21,700 --> 00:04:27,620
इसके अलावा, इन नज़ों का आकार इस बात पर आनुपातिक होना चाहिए

63
00:04:27,620 --> 00:04:30,220
कि प्रत्येक वर्तमान मान अपने लक्ष्य मान से कितनी दूर है।

64
00:04:30,220 --> 00:04:35,260
उदाहरण के लिए, उस संख्या 2 न्यूरॉन की सक्रियता में वृद्धि एक

65
00:04:35,260 --> 00:04:39,620
मायने में संख्या 8 न्यूरॉन की कमी से अधिक महत्वपूर्ण है,

66
00:04:39,620 --> 00:04:42,060
जो पहले से ही काफी करीब है जहां इसे होना चाहिए।

67
00:04:42,060 --> 00:04:46,260
तो आगे बढ़ते हुए, आइए केवल इस एक न्यूरॉन पर

68
00:04:46,260 --> 00:04:47,900
ध्यान केंद्रित करें, जिसकी सक्रियता हम बढ़ाना चाहते हैं।

69
00:04:47,900 --> 00:04:53,680
याद रखें, उस सक्रियण को पिछली परत में सभी सक्रियणों के एक निश्चित भारित योग

70
00:04:53,680 --> 00:04:58,380
के साथ-साथ एक पूर्वाग्रह के रूप में परिभाषित किया गया है, जिसे बाद में

71
00:04:58,380 --> 00:05:01,900
सिग्मॉइड स्क्विशिफिकेशन फ़ंक्शन, या एक ReLU जैसी किसी चीज़ में प्लग किया जाता है।

72
00:05:01,900 --> 00:05:07,060
इसलिए तीन अलग-अलग रास्ते हैं जो एक साथ मिलकर

73
00:05:07,060 --> 00:05:08,060
उस सक्रियता को बढ़ाने में मदद कर सकते हैं।

74
00:05:08,060 --> 00:05:12,800
आप पूर्वाग्रह बढ़ा सकते हैं, आप वज़न बढ़ा सकते हैं,

75
00:05:12,800 --> 00:05:15,300
और आप पिछली परत से सक्रियता बदल सकते हैं।

76
00:05:15,300 --> 00:05:19,720
वज़न को कैसे समायोजित किया जाना चाहिए, इस पर ध्यान केंद्रित करते हुए, ध्यान

77
00:05:19,720 --> 00:05:21,460
दें कि वज़न का वास्तव में प्रभाव के विभिन्न स्तर कैसे होते हैं।

78
00:05:21,460 --> 00:05:25,100
पिछली परत के सबसे चमकीले न्यूरॉन्स के साथ कनेक्शन का सबसे बड़ा प्रभाव होता

79
00:05:25,100 --> 00:05:31,420
है क्योंकि उन भारों को बड़े सक्रियण मूल्यों से गुणा किया जाता है।

80
00:05:31,420 --> 00:05:35,820
इसलिए यदि आप उन भारों में से किसी एक को बढ़ाना चाहते हैं, तो इसका वास्तव में

81
00:05:35,820 --> 00:05:40,900
डिमर न्यूरॉन्स के साथ कनेक्शन के भार को बढ़ाने की तुलना में अंतिम लागत फ़ंक्शन पर

82
00:05:40,900 --> 00:05:44,020
अधिक प्रभाव पड़ता है, कम से कम जहां तक इस एक प्रशिक्षण उदाहरण का संबंध है।

83
00:05:44,020 --> 00:05:48,700
याद रखें, जब हम ग्रेडिएंट डिसेंट के बारे में बात करते हैं, तो हम केवल इस बात

84
00:05:48,700 --> 00:05:53,020
की परवाह नहीं करते हैं कि प्रत्येक घटक को ऊपर या नीचे जाना चाहिए, हम इसकी परवाह

85
00:05:53,020 --> 00:05:54,020
करते हैं कि कौन सा घटक आपको आपके पैसे के लिए सबसे अधिक लाभ देता है।

86
00:05:54,020 --> 00:06:00,260
वैसे, यह कम से कम कुछ हद तक तंत्रिका विज्ञान के एक सिद्धांत की याद

87
00:06:00,260 --> 00:06:04,900
दिलाता है कि न्यूरॉन्स के जैविक नेटवर्क कैसे सीखते हैं, हेब्बियन सिद्धांत, जिसे अक्सर वाक्यांश

88
00:06:04,900 --> 00:06:06,940
में अभिव्यक्त किया जाता है, न्यूरॉन्स जो एक साथ तार से आग लगाते हैं।

89
00:06:06,940 --> 00:06:12,460
यहां, वजन में सबसे बड़ी वृद्धि, कनेक्शन की सबसे बड़ी मजबूती,

90
00:06:12,460 --> 00:06:16,860
उन न्यूरॉन्स के बीच होती है जो सबसे अधिक सक्रिय हैं

91
00:06:16,860 --> 00:06:18,100
और जिनके बारे में हम अधिक सक्रिय होना चाहते हैं।

92
00:06:18,100 --> 00:06:22,520
एक अर्थ में, 2 को देखते समय जो न्यूरॉन्स सक्रिय होते हैं, वे इसके बारे

93
00:06:22,520 --> 00:06:25,440
में सोचते समय सक्रिय होने वाले न्यूरॉन्स से अधिक मजबूती से जुड़ जाते हैं।

94
00:06:25,440 --> 00:06:29,240
स्पष्ट होने के लिए, मैं इस बारे में एक या दूसरे तरीके से बयान देने की स्थिति में

95
00:06:29,240 --> 00:06:34,020
नहीं हूं कि क्या न्यूरॉन्स के कृत्रिम नेटवर्क जैविक मस्तिष्क की तरह कुछ भी व्यवहार करते हैं,

96
00:06:34,020 --> 00:06:39,440
और यह तारों को एक साथ जोड़ता है विचार कुछ सार्थक तारांकन के साथ आता है, लेकिन इसे

97
00:06:39,440 --> 00:06:41,760
बहुत ही ढीले के रूप में लिया जाता है सादृश्य, मुझे यह नोट करना दिलचस्प लगता है।

98
00:06:41,760 --> 00:06:46,760
वैसे भी, तीसरा तरीका जिससे हम इस न्यूरॉन की सक्रियता को बढ़ाने में

99
00:06:46,760 --> 00:06:49,360
मदद कर सकते हैं वह है पिछली परत की सभी सक्रियता को बदलना।

100
00:06:49,360 --> 00:06:55,080
अर्थात्, यदि सकारात्मक भार वाले उस अंक 2 न्यूरॉन से जुड़ी हर

101
00:06:55,080 --> 00:06:59,480
चीज चमकीली हो गई, और यदि नकारात्मक भार से जुड़ी हर चीज

102
00:06:59,480 --> 00:07:02,680
मंद हो गई, तो वह अंक 2 न्यूरॉन अधिक सक्रिय हो जाएगा।

103
00:07:02,680 --> 00:07:06,200
और वज़न परिवर्तनों के समान, आप संबंधित वज़न के आकार के आनुपातिक परिवर्तनों की

104
00:07:06,200 --> 00:07:10,840
तलाश करके अपने पैसे का सबसे अधिक लाभ प्राप्त करने जा रहे हैं।

105
00:07:10,840 --> 00:07:16,520
अब निःसंदेह, हम उन सक्रियताओं को सीधे प्रभावित नहीं कर

106
00:07:16,520 --> 00:07:18,320
सकते हैं, हमारा नियंत्रण केवल भार और पूर्वाग्रहों पर है।

107
00:07:18,320 --> 00:07:22,960
लेकिन पिछली परत की तरह ही, यह नोट करना

108
00:07:22,960 --> 00:07:23,960
उपयोगी है कि वे वांछित परिवर्तन क्या हैं।

109
00:07:23,960 --> 00:07:29,040
लेकिन ध्यान रखें, यहां एक कदम ज़ूम आउट करना, यह

110
00:07:29,040 --> 00:07:30,040
वही है जो वह अंक 2 आउटपुट न्यूरॉन चाहता है।

111
00:07:30,040 --> 00:07:34,960
याद रखें, हम यह भी चाहते हैं कि अंतिम परत के अन्य सभी न्यूरॉन्स

112
00:07:34,960 --> 00:07:38,460
कम सक्रिय हो जाएं, और उन अन्य आउटपुट न्यूरॉन्स में से प्रत्येक के अपने

113
00:07:38,460 --> 00:07:43,200
विचार हैं कि उस दूसरी से अंतिम परत के साथ क्या होना चाहिए।

114
00:07:43,200 --> 00:07:49,220
तो इस अंक 2 न्यूरॉन की इच्छा को अन्य सभी आउटपुट न्यूरॉन की

115
00:07:49,220 --> 00:07:54,800
इच्छाओं के साथ जोड़ा जाता है कि इस दूसरी से आखिरी परत के

116
00:07:54,800 --> 00:08:00,240
साथ क्या होना चाहिए, फिर से संबंधित वजन के अनुपात में, और उनमें

117
00:08:00,240 --> 00:08:01,740
से प्रत्येक न्यूरॉन को कितनी आवश्यकता है इसके अनुपात में को बदलने।

118
00:08:01,740 --> 00:08:05,940
यहीं वह जगह है जहां पीछे की ओर प्रचार करने का विचार आता है।

119
00:08:05,940 --> 00:08:11,080
इन सभी वांछित प्रभावों को एक साथ जोड़कर, आपको मूल रूप से उन संकेतों की

120
00:08:11,080 --> 00:08:14,300
एक सूची मिलती है जिन्हें आप इस दूसरी से आखिरी परत तक करना चाहते हैं।

121
00:08:14,300 --> 00:08:18,740
और एक बार जब आपके पास वे होते हैं, तो आप उसी प्रक्रिया को उन प्रासंगिक भारों और पूर्वाग्रहों

122
00:08:18,740 --> 00:08:23,400
पर पुनरावर्ती रूप से लागू कर सकते हैं जो उन मूल्यों को निर्धारित करते हैं, उसी प्रक्रिया को

123
00:08:23,400 --> 00:08:29,180
दोहराते हुए जिससे मैं अभी गुजरा हूं और नेटवर्क के माध्यम से पीछे की ओर बढ़ रहा हूं।

124
00:08:29,180 --> 00:08:33,960
और थोड़ा और ज़ूम आउट करते हुए, याद रखें कि यह सब ठीक उसी तरह है जैसे

125
00:08:33,960 --> 00:08:37,520
एक एकल प्रशिक्षण उदाहरण उन वज़न और पूर्वाग्रहों में से प्रत्येक को नियंत्रित करना चाहता है।

126
00:08:37,520 --> 00:08:41,400
यदि हम केवल वही सुनते हैं जो वह 2 चाहता था, तो नेटवर्क को अंततः

127
00:08:41,400 --> 00:08:44,140
सभी छवियों को 2 के रूप में वर्गीकृत करने के लिए प्रोत्साहित किया जाएगा।

128
00:08:44,140 --> 00:08:49,500
तो आप जो करते हैं वह हर दूसरे प्रशिक्षण उदाहरण के लिए इसी बैकप्रॉप रूटीन

129
00:08:49,500 --> 00:08:54,700
से गुजरते हैं, यह रिकॉर्ड करते हुए कि उनमें से प्रत्येक वजन और पूर्वाग्रह को

130
00:08:54,700 --> 00:09:02,300
कैसे बदलना चाहते हैं, और उन वांछित परिवर्तनों को एक साथ औसत करते हैं।

131
00:09:02,300 --> 00:09:08,260
यहां प्रत्येक वजन और पूर्वाग्रह के औसत संकेतों का यह संग्रह, शिथिल

132
00:09:08,260 --> 00:09:12,340
रूप से कहें तो, पिछले वीडियो में संदर्भित लागत फ़ंक्शन का नकारात्मक

133
00:09:12,340 --> 00:09:14,360
ग्रेडिएंट है, या कम से कम इसके लिए आनुपातिक कुछ है।

134
00:09:14,360 --> 00:09:18,980
मैं शिथिल रूप से केवल इसलिए कह रहा हूं क्योंकि मुझे अभी तक उन संकेतों के बारे में मात्रात्मक

135
00:09:18,980 --> 00:09:23,480
रूप से सटीक नहीं मिल पाया है, लेकिन अगर आप मेरे द्वारा संदर्भित हर बदलाव को समझ गए हैं,

136
00:09:23,480 --> 00:09:28,740
तो कुछ दूसरों की तुलना में आनुपातिक रूप से बड़े क्यों हैं, और उन सभी को एक साथ जोड़ने

137
00:09:28,740 --> 00:09:34,100
की आवश्यकता कैसे है, आप इसके लिए यांत्रिकी को समझते हैं बैकप्रोपेगेशन वास्तव में क्या कर रहा है।

138
00:09:34,100 --> 00:09:38,540
वैसे, व्यवहार में, प्रत्येक प्रशिक्षण उदाहरण के प्रत्येक ग्रेडिएंट डिसेंट चरण के

139
00:09:38,540 --> 00:09:43,120
प्रभाव को जोड़ने में कंप्यूटर को बहुत लंबा समय लगता है।

140
00:09:43,120 --> 00:09:45,540
तो इसके बजाय यहाँ वह है जो आमतौर पर किया जाता है।

141
00:09:45,540 --> 00:09:50,460
आप अपने प्रशिक्षण डेटा को बेतरतीब ढंग से फेरबदल करते हैं और इसे मिनी-बैचों के एक

142
00:09:50,460 --> 00:09:53,380
पूरे समूह में विभाजित करते हैं, मान लें कि प्रत्येक में 100 प्रशिक्षण उदाहरण हैं।

143
00:09:53,380 --> 00:09:56,980
फिर आप मिनी-बैच के अनुसार एक चरण की गणना करें।

144
00:09:56,980 --> 00:10:00,840
यह लागत फ़ंक्शन का वास्तविक ग्रेडिएंट नहीं है, जो सभी प्रशिक्षण डेटा पर निर्भर

145
00:10:00,840 --> 00:10:06,260
करता है, न कि इस छोटे उपसमुच्चय पर, इसलिए यह डाउनहिल का सबसे कुशल

146
00:10:06,260 --> 00:10:10,900
कदम नहीं है, लेकिन प्रत्येक मिनी-बैच आपको एक बहुत अच्छा अनुमान देता है, और

147
00:10:10,900 --> 00:10:12,900
इससे भी महत्वपूर्ण बात यह है आपको एक महत्वपूर्ण कम्प्यूटेशनल स्पीडअप देता है।

148
00:10:12,900 --> 00:10:16,900
यदि आप प्रासंगिक लागत सतह के तहत अपने नेटवर्क के प्रक्षेप पथ की योजना बनाते हैं, तो यह कुछ

149
00:10:16,900 --> 00:10:22,020
हद तक एक नशे में धुत आदमी की तरह होगा जो लक्ष्यहीन रूप से एक पहाड़ी से नीचे गिर

150
00:10:22,020 --> 00:10:26,880
रहा है, लेकिन तेजी से कदम उठा रहा है, न कि एक सावधानीपूर्वक गणना करने वाला व्यक्ति प्रत्येक कदम

151
00:10:26,880 --> 00:10:31,620
की सटीक डाउनहिल दिशा निर्धारित करता है। उस दिशा में बहुत धीमा और सावधानीपूर्वक कदम उठाने से पहले।

152
00:10:31,620 --> 00:10:35,200
इस तकनीक को स्टोकेस्टिक ग्रेडिएंट डिसेंट कहा जाता है।

153
00:10:35,200 --> 00:10:40,400
यहाँ बहुत कुछ चल रहा है, तो आइए इसे अपने लिए संक्षेप में प्रस्तुत करें, क्या हम करेंगे?

154
00:10:40,400 --> 00:10:45,480
बैकप्रॉपैगेशन यह निर्धारित करने के लिए एल्गोरिथ्म है कि एक एकल प्रशिक्षण उदाहरण

155
00:10:45,480 --> 00:10:50,040
वजन और पूर्वाग्रहों को कैसे कम करना चाहेगा, न केवल इस संदर्भ

156
00:10:50,040 --> 00:10:54,780
में कि उन्हें ऊपर जाना चाहिए या नीचे जाना चाहिए, बल्कि उन परिवर्तनों

157
00:10:54,780 --> 00:10:56,240
के सापेक्ष अनुपात के कारण सबसे तेजी से कमी आती है। लागत।

158
00:10:56,240 --> 00:11:00,720
एक सच्चे ग्रेडिएंट डिसेंट चरण में आपके सभी दसियों और हजारों प्रशिक्षण उदाहरणों के लिए

159
00:11:00,720 --> 00:11:05,920
ऐसा करना और आपके द्वारा प्राप्त वांछित परिवर्तनों का औसत शामिल होगा, लेकिन यह कम्प्यूटेशनल

160
00:11:05,920 --> 00:11:11,680
रूप से धीमा है, इसलिए इसके बजाय आप डेटा को मिनी-बैचों में यादृच्छिक रूप से

161
00:11:11,680 --> 00:11:14,000
उप-विभाजित करते हैं और प्रत्येक चरण की गणना एक के संबंध में करते हैं। मिनी-बैच।

162
00:11:14,000 --> 00:11:18,600
बार-बार सभी मिनी-बैचों से गुजरते हुए और इन समायोजनों को करते हुए,

163
00:11:18,600 --> 00:11:23,420
आप स्थानीय न्यूनतम लागत फ़ंक्शन की ओर जुटेंगे, जिसका अर्थ है

164
00:11:23,420 --> 00:11:27,540
कि आपका नेटवर्क प्रशिक्षण उदाहरणों पर वास्तव में अच्छा काम करेगा।

165
00:11:27,540 --> 00:11:32,600
तो जो कुछ कहा गया है, उसके साथ, कोड की प्रत्येक पंक्ति जो बैकप्रॉप को लागू करने में जाएगी,

166
00:11:32,600 --> 00:11:37,680
वास्तव में उस चीज़ से मेल खाती है जिसे आपने अब देखा है, कम से कम अनौपचारिक शब्दों में।

167
00:11:37,680 --> 00:11:41,900
लेकिन कभी-कभी यह जानना कि गणित क्या करता है, केवल आधी लड़ाई है, और केवल उस

168
00:11:41,900 --> 00:11:44,780
लानत चीज़ का प्रतिनिधित्व करना है जहां यह सब गड़बड़ और भ्रमित हो जाता है।

169
00:11:44,780 --> 00:11:49,360
तो, आप में से जो लोग गहराई में जाना चाहते हैं, उनके लिए अगला वीडियो उन्हीं विचारों पर

170
00:11:49,360 --> 00:11:53,400
आधारित है जो अभी यहां प्रस्तुत किए गए थे, लेकिन अंतर्निहित गणना के संदर्भ में, उम्मीद है

171
00:11:53,400 --> 00:11:57,460
कि जैसे ही आप विषय को देखेंगे तो यह थोड़ा और अधिक परिचित हो जाएगा। अन्य संसाधन।

172
00:11:57,460 --> 00:12:01,220
इससे पहले, एक बात पर जोर देने लायक बात यह है कि इस एल्गोरिदम को

173
00:12:01,220 --> 00:12:05,840
काम करने के लिए, और यह केवल तंत्रिका नेटवर्क से परे सभी प्रकार की मशीन

174
00:12:05,840 --> 00:12:06,840
लर्निंग के लिए जाता है, आपको बहुत सारे प्रशिक्षण डेटा की आवश्यकता होती है।

175
00:12:06,840 --> 00:12:10,740
हमारे मामले में, एक चीज़ जो हस्तलिखित अंकों को इतना अच्छा उदाहरण बनाती है, वह यह है

176
00:12:10,740 --> 00:12:15,380
कि एमएनआईएसटी डेटाबेस मौजूद है, जिसमें बहुत सारे उदाहरण हैं जिन्हें मनुष्यों द्वारा लेबल किया गया है।

177
00:12:15,380 --> 00:12:19,000
तो एक आम चुनौती जिससे आपमें से मशीन लर्निंग में काम करने वाले परिचित होंगे, वह है केवल लेबल

178
00:12:19,040 --> 00:12:22,880
किए गए प्रशिक्षण डेटा को प्राप्त करना जिसकी आपको वास्तव में आवश्यकता है, चाहे वह लोगों को हजारों

179
00:12:22,880 --> 00:12:27,400
छवियों को लेबल करना हो, या किसी भी अन्य डेटा प्रकार के साथ आप काम कर रहे हों।

