1
00:00:00,000 --> 00:00:04,719
यहां, हम बैकप्रॉपैगेशन से निपटते हैं, तंत्रिका

2
00:00:04,719 --> 00:00:09,640
नेटवर्क कैसे सीखते हैं इसके पीछे मुख्य एल्गोरिदम।

3
00:00:09,640 --> 00:00:12,186
हम कहां हैं, इसके बारे में एक त्वरित पुनर्कथन के बाद, पहली चीज

4
00:00:12,186 --> 00:00:14,732
जो मैं करूंगा वह यह है कि एल्गोरिदम वास्तव में क्या कर रहा है,

5
00:00:14,732 --> 00:00:17,400
सूत्रों के किसी भी संदर्भ के बिना, एक सहज ज्ञान युक्त पूर्वाभ्यास।

6
00:00:17,400 --> 00:00:20,661
फिर, आपमें से जो लोग गणित में गहराई से उतरना चाहते हैं,

7
00:00:20,661 --> 00:00:24,040
उनके लिए अगला वीडियो इस सब के अंतर्निहित कलन पर आधारित है।

8
00:00:24,040 --> 00:00:27,518
यदि आपने पिछले दो वीडियो देखे हैं, या यदि आप उचित पृष्ठभूमि के साथ आगे बढ़ रहे हैं,

9
00:00:27,518 --> 00:00:31,080
तो आप जानते हैं कि तंत्रिका नेटवर्क क्या है, और यह आगे की जानकारी कैसे प्रदान करता है।

10
00:00:31,080 --> 00:00:35,703
यहां, हम हस्तलिखित अंकों को पहचानने का क्लासिक उदाहरण दे रहे हैं, जिनके पिक्सेल मान 784

11
00:00:35,703 --> 00:00:40,326
न्यूरॉन्स के साथ नेटवर्क की पहली परत में फीड हो जाते हैं, और मैं दो छिपी हुई परतों वाला

12
00:00:40,326 --> 00:00:44,949
एक नेटवर्क दिखा रहा हूं, जिनमें से प्रत्येक में केवल 16 न्यूरॉन हैं, और एक आउटपुट है 10

13
00:00:44,949 --> 00:00:49,520
न्यूरॉन्स की परत, यह दर्शाती है कि नेटवर्क अपने उत्तर के रूप में कौन सा अंक चुन रहा है।

14
00:00:49,520 --> 00:00:53,780
मैं यह भी उम्मीद कर रहा हूं कि आप ग्रेडिएंट डिसेंट को समझेंगे, जैसा कि पिछले

15
00:00:53,780 --> 00:00:57,819
वीडियो में बताया गया है, और सीखने से हमारा मतलब यह है कि हम यह पता लगाना

16
00:00:57,819 --> 00:01:02,080
चाहते हैं कि कौन से वजन और पूर्वाग्रह एक निश्चित लागत फ़ंक्शन को कम करते हैं।

17
00:01:02,080 --> 00:01:06,679
एक त्वरित अनुस्मारक के रूप में, एक एकल प्रशिक्षण उदाहरण की लागत के लिए,

18
00:01:06,679 --> 00:01:11,024
आप उस आउटपुट को लेते हैं जो नेटवर्क देता है, उस आउटपुट के साथ जो आप

19
00:01:11,024 --> 00:01:15,560
उसे देना चाहते थे, और प्रत्येक घटक के बीच अंतर के वर्गों को जोड़ते हैं।

20
00:01:15,560 --> 00:01:19,106
अपने सभी हज़ारों प्रशिक्षण उदाहरणों के लिए ऐसा करने और

21
00:01:19,106 --> 00:01:23,040
परिणामों का औसत निकालने से आपको नेटवर्क की कुल लागत मिलती है।

22
00:01:23,040 --> 00:01:28,153
जैसे कि यह सोचने के लिए पर्याप्त नहीं है, जैसा कि पिछले वीडियो में वर्णित

23
00:01:28,153 --> 00:01:33,405
है, जिस चीज की हम तलाश कर रहे हैं वह इस लागत फ़ंक्शन का नकारात्मक ग्रेडिएंट

24
00:01:33,405 --> 00:01:38,242
है, जो आपको बताता है कि आपको सभी वजन और पूर्वाग्रहों को कैसे बदलने की

25
00:01:38,242 --> 00:01:43,080
आवश्यकता है ये कनेक्शन, ताकि लागत को सबसे कुशलतापूर्वक कम किया जा सके।

26
00:01:43,080 --> 00:01:49,600
बैकप्रॉपैगेशन, इस वीडियो का विषय, उस जटिल जटिल ग्रेडिएंट की गणना के लिए एक एल्गोरिदम है।

27
00:01:49,600 --> 00:01:53,406
पिछले वीडियो से एक विचार जो मैं वास्तव में चाहता हूं कि आप अभी अपने दिमाग

28
00:01:53,406 --> 00:01:57,264
में मजबूती से रखें, क्योंकि 13,000 आयामों में एक दिशा के रूप में ग्रेडिएंट

29
00:01:57,264 --> 00:02:00,865
वेक्टर के बारे में सोचना, इसे हल्के ढंग से कहें तो, हमारी कल्पनाओं के

30
00:02:00,865 --> 00:02:04,620
दायरे से परे है, एक और विचार है जिस तरह से आप इसके बारे में सोच सकते हैं.

31
00:02:04,620 --> 00:02:08,376
यहां प्रत्येक घटक का परिमाण आपको बता रहा है कि लागत फ़ंक्शन

32
00:02:08,376 --> 00:02:11,820
प्रत्येक भार और पूर्वाग्रह के प्रति कितना संवेदनशील है।

33
00:02:11,820 --> 00:02:16,463
उदाहरण के लिए, मान लें कि आप उस प्रक्रिया से गुजरते हैं जिसका मैं वर्णन करने जा रहा हूं,

34
00:02:16,463 --> 00:02:21,054
और नकारात्मक ग्रेडिएंट की गणना करते हैं, और यहां इस किनारे पर वजन से जुड़ा घटक 3 निकलता

35
00:02:21,054 --> 00:02:21,211
है।

36
00:02:21,211 --> 00:02:26,856
2, जबकि इस किनारे से जुड़ा घटक यहाँ 0 के रूप में सामने आता है।

37
00:02:26,856 --> 00:02:26,940
1.

38
00:02:26,940 --> 00:02:31,479
जिस तरह से आप इसकी व्याख्या करेंगे वह यह है कि फ़ंक्शन की लागत उस

39
00:02:31,479 --> 00:02:36,156
पहले वजन में परिवर्तन के प्रति 32 गुना अधिक संवेदनशील है, इसलिए यदि

40
00:02:36,156 --> 00:02:40,902
आप उस मूल्य को थोड़ा सा हिलाते हैं, तो इससे लागत में कुछ बदलाव आएगा,

41
00:02:40,902 --> 00:02:45,580
और वह परिवर्तन होगा यह उस दूसरे वजन के समान झटके से 32 गुना अधिक है।

42
00:02:45,580 --> 00:02:50,599
निजी तौर पर, जब मैं पहली बार बैकप्रॉपैगेशन के बारे में सीख रहा था, तो मुझे

43
00:02:50,599 --> 00:02:55,820
लगता है कि सबसे भ्रमित करने वाला पहलू सिर्फ नोटेशन और इंडेक्स का पीछा करना था।

44
00:02:55,820 --> 00:02:59,655
लेकिन एक बार जब आप यह समझ लेते हैं कि इस एल्गोरिदम का प्रत्येक भाग वास्तव

45
00:02:59,655 --> 00:03:03,490
में क्या कर रहा है, तो इसका प्रत्येक व्यक्तिगत प्रभाव वास्तव में बहुत सहज

46
00:03:03,490 --> 00:03:07,740
है, बात बस इतनी है कि बहुत सारे छोटे-छोटे समायोजन एक-दूसरे के ऊपर परत चढ़ रहे हैं।

47
00:03:07,740 --> 00:03:10,865
इसलिए मैं यहां नोटेशन की पूरी उपेक्षा के साथ चीजों को शुरू

48
00:03:10,865 --> 00:03:13,831
करने जा रहा हूं, और प्रत्येक प्रशिक्षण उदाहरण के वजन और

49
00:03:13,831 --> 00:03:17,380
पूर्वाग्रहों पर पड़ने वाले प्रभावों के बारे में विस्तार से बताऊंगा।

50
00:03:17,380 --> 00:03:22,290
क्योंकि लागत फ़ंक्शन में हजारों प्रशिक्षण उदाहरणों में प्रति उदाहरण एक निश्चित

51
00:03:22,290 --> 00:03:27,015
लागत का औसत शामिल होता है, जिस तरह से हम एक एकल ग्रेडिएंट डिसेंट चरण के लिए

52
00:03:27,015 --> 00:03:31,740
वजन और पूर्वाग्रह को समायोजित करते हैं वह भी हर एक उदाहरण पर निर्भर करता है।

53
00:03:31,740 --> 00:03:35,754
या बल्कि, सिद्धांत रूप में यह होना चाहिए, लेकिन कम्प्यूटेशनल दक्षता के लिए हम बाद में एक

54
00:03:35,754 --> 00:03:39,634
छोटी सी तरकीब अपनाएंगे ताकि आपको हर चरण के लिए हर एक उदाहरण को हिट करने की आवश्यकता न

55
00:03:39,634 --> 00:03:39,860
पड़े।

56
00:03:39,860 --> 00:03:43,771
अन्य मामलों में, अभी, हम अपना ध्यान केवल एक उदाहरण,

57
00:03:43,771 --> 00:03:46,780
2 की इस छवि पर केंद्रित करने जा रहे हैं।

58
00:03:46,780 --> 00:03:49,307
इस एक प्रशिक्षण उदाहरण का इस बात पर क्या प्रभाव होना

59
00:03:49,307 --> 00:03:51,740
चाहिए कि वज़न और पूर्वाग्रह कैसे समायोजित होते हैं?

60
00:03:51,740 --> 00:03:56,299
मान लीजिए कि हम एक ऐसे बिंदु पर हैं जहां नेटवर्क अभी तक अच्छी तरह से प्रशिक्षित

61
00:03:56,299 --> 00:04:00,516
नहीं है, इसलिए आउटपुट में सक्रियण काफी यादृच्छिक दिखेंगे, शायद 0 जैसा कुछ।

62
00:04:00,516 --> 00:04:00,776
5, 0.

63
00:04:00,776 --> 00:04:01,036
8, 0.

64
00:04:01,036 --> 00:04:02,780
2, पर और आगे.

65
00:04:02,780 --> 00:04:06,335
हम उन सक्रियणों को सीधे तौर पर नहीं बदल सकते हैं, हम केवल वज़न और

66
00:04:06,335 --> 00:04:09,837
पूर्वाग्रहों पर प्रभाव डालते हैं, लेकिन यह ट्रैक रखना सहायक होता

67
00:04:09,837 --> 00:04:13,340
है कि हम चाहते हैं कि उस आउटपुट परत पर कौन सा समायोजन होना चाहिए।

68
00:04:13,340 --> 00:04:17,488
और चूँकि हम चाहते हैं कि यह छवि को 2 के रूप में वर्गीकृत करे, हम

69
00:04:17,488 --> 00:04:21,700
चाहते हैं कि तीसरा मान ऊपर की ओर हो, जबकि अन्य सभी नीचे की ओर हों।

70
00:04:21,700 --> 00:04:26,071
इसके अलावा, इन नज़ों का आकार इस बात पर आनुपातिक होना चाहिए

71
00:04:26,071 --> 00:04:30,220
कि प्रत्येक वर्तमान मान अपने लक्ष्य मान से कितनी दूर है।

72
00:04:30,220 --> 00:04:35,960
उदाहरण के लिए, उस संख्या 2 न्यूरॉन की सक्रियता में वृद्धि एक मायने में संख्या 8

73
00:04:35,960 --> 00:04:42,060
न्यूरॉन की कमी से अधिक महत्वपूर्ण है, जो पहले से ही काफी करीब है जहां इसे होना चाहिए।

74
00:04:42,060 --> 00:04:44,980
तो आगे बढ़ते हुए, आइए केवल इस एक न्यूरॉन पर ध्यान

75
00:04:44,980 --> 00:04:47,900
केंद्रित करें, जिसकी सक्रियता हम बढ़ाना चाहते हैं।

76
00:04:47,900 --> 00:04:52,566
याद रखें, उस सक्रियण को पिछली परत में सभी सक्रियणों के एक निश्चित भारित योग

77
00:04:52,566 --> 00:04:57,478
के साथ-साथ एक पूर्वाग्रह के रूप में परिभाषित किया गया है, जिसे बाद में सिग्मॉइड

78
00:04:57,478 --> 00:05:01,900
स्क्विशिफिकेशन फ़ंक्शन, या एक ReLU जैसी किसी चीज़ में प्लग किया जाता है।

79
00:05:01,900 --> 00:05:08,060
इसलिए तीन अलग-अलग रास्ते हैं जो एक साथ मिलकर उस सक्रियता को बढ़ाने में मदद कर सकते हैं।

80
00:05:08,060 --> 00:05:11,680
आप पूर्वाग्रह बढ़ा सकते हैं, आप वजन बढ़ा सकते

81
00:05:11,680 --> 00:05:15,300
हैं, और आप पिछली परत से सक्रियता बदल सकते हैं।

82
00:05:15,300 --> 00:05:18,402
वज़न को कैसे समायोजित किया जाना चाहिए, इस पर ध्यान केंद्रित करते हुए,

83
00:05:18,402 --> 00:05:21,460
ध्यान दें कि वज़न का वास्तव में प्रभाव के विभिन्न स्तर कैसे होते हैं।

84
00:05:21,460 --> 00:05:26,440
पिछली परत से सबसे चमकीले न्यूरॉन्स के साथ कनेक्शन का सबसे बड़ा प्रभाव

85
00:05:26,440 --> 00:05:31,420
होता है क्योंकि उन भारों को बड़े सक्रियण मूल्यों से गुणा किया जाता है।

86
00:05:31,420 --> 00:05:35,583
इसलिए यदि आप उन भारों में से किसी एक को बढ़ाना चाहते हैं, तो इसका वास्तव में

87
00:05:35,583 --> 00:05:39,856
डिमर न्यूरॉन्स के साथ कनेक्शन के भार को बढ़ाने की तुलना में अंतिम लागत फ़ंक्शन

88
00:05:39,856 --> 00:05:44,020
पर अधिक प्रभाव पड़ता है, कम से कम जहां तक इस एक प्रशिक्षण उदाहरण का संबंध है।

89
00:05:44,020 --> 00:05:47,308
याद रखें, जब हम ग्रेडिएंट डिसेंट के बारे में बात करते हैं, तो हम सिर्फ इस

90
00:05:47,308 --> 00:05:50,553
बात की परवाह नहीं करते हैं कि प्रत्येक घटक को ऊपर या नीचे जाना चाहिए, हम

91
00:05:50,553 --> 00:05:54,020
इसकी परवाह करते हैं कि कौन सा घटक आपको आपके पैसे के लिए सबसे अधिक लाभ देता है।

92
00:05:54,020 --> 00:05:58,345
वैसे, यह कम से कम कुछ हद तक तंत्रिका विज्ञान के एक सिद्धांत की याद दिलाता है

93
00:05:58,345 --> 00:06:02,614
कि न्यूरॉन्स के जैविक नेटवर्क कैसे सीखते हैं, हेब्बियन सिद्धांत, जिसे अक्सर

94
00:06:02,614 --> 00:06:06,940
वाक्यांश में अभिव्यक्त किया जाता है, न्यूरॉन्स जो एक साथ तार से आग लगाते हैं।

95
00:06:06,940 --> 00:06:12,554
यहां, वजन में सबसे बड़ी वृद्धि, कनेक्शन की सबसे बड़ी मजबूती, उन न्यूरॉन्स के बीच

96
00:06:12,554 --> 00:06:18,100
होती है जो सबसे अधिक सक्रिय हैं और जिनके बारे में हम अधिक सक्रिय होना चाहते हैं।

97
00:06:18,100 --> 00:06:21,719
एक अर्थ में, 2 को देखते समय जो न्यूरॉन्स सक्रिय होते हैं, वे इसके बारे

98
00:06:21,719 --> 00:06:25,440
में सोचते समय सक्रिय होने वाले न्यूरॉन्स से अधिक मजबूती से जुड़ जाते हैं।

99
00:06:25,440 --> 00:06:29,386
स्पष्ट होने के लिए, मैं इस बारे में एक या दूसरे तरीके से बयान देने की स्थिति में

100
00:06:29,386 --> 00:06:33,429
नहीं हूं कि क्या न्यूरॉन्स के कृत्रिम नेटवर्क जैविक मस्तिष्क की तरह कुछ भी व्यवहार

101
00:06:33,429 --> 00:06:37,424
करते हैं, और यह तारों को एक साथ जोड़ता है विचार कुछ सार्थक तारांकन के साथ आता है,

102
00:06:37,424 --> 00:06:41,760
लेकिन इसे बहुत ही ढीले के रूप में लिया जाता है सादृश्य, मुझे यह नोट करना दिलचस्प लगता है।

103
00:06:41,760 --> 00:06:45,621
वैसे भी, तीसरा तरीका जिससे हम इस न्यूरॉन की सक्रियता को बढ़ाने

104
00:06:45,621 --> 00:06:49,360
में मदद कर सकते हैं वह है पिछली परत की सभी सक्रियता को बदलना।

105
00:06:49,360 --> 00:06:56,058
अर्थात्, यदि सकारात्मक भार वाले उस अंक 2 न्यूरॉन से जुड़ी हर चीज चमकीली हो गई, और यदि

106
00:06:56,058 --> 00:07:02,680
नकारात्मक भार से जुड़ी हर चीज धुंधली हो गई, तो वह अंक 2 न्यूरॉन अधिक सक्रिय हो जाएगा।

107
00:07:02,680 --> 00:07:06,437
और वज़न परिवर्तनों के समान, आप संबंधित वज़न के आकार के आनुपातिक

108
00:07:06,437 --> 00:07:10,840
परिवर्तनों की तलाश करके अपने पैसे का सबसे अधिक लाभ प्राप्त करने जा रहे हैं।

109
00:07:10,840 --> 00:07:14,548
अब निःसंदेह, हम सीधे तौर पर उन सक्रियताओं को प्रभावित नहीं

110
00:07:14,548 --> 00:07:18,320
कर सकते हैं, हमारा नियंत्रण केवल वज़न और पूर्वाग्रहों पर है।

111
00:07:18,320 --> 00:07:23,960
लेकिन पिछली परत की तरह ही, यह ध्यान रखना भी उपयोगी है कि वे वांछित परिवर्तन क्या हैं।

112
00:07:23,960 --> 00:07:27,167
लेकिन ध्यान रखें, यहां एक कदम ज़ूम आउट करना, यह

113
00:07:27,167 --> 00:07:30,040
वही है जो वह अंक 2 आउटपुट न्यूरॉन चाहता है।

114
00:07:30,040 --> 00:07:34,403
याद रखें, हम यह भी चाहते हैं कि अंतिम परत के अन्य सभी न्यूरॉन्स

115
00:07:34,403 --> 00:07:38,699
कम सक्रिय हो जाएं, और उन अन्य आउटपुट न्यूरॉन्स में से प्रत्येक

116
00:07:38,699 --> 00:07:43,200
के अपने विचार हैं कि उस दूसरी से अंतिम परत के साथ क्या होना चाहिए।

117
00:07:43,200 --> 00:07:49,229
तो इस अंक 2 न्यूरॉन की इच्छा को अन्य सभी आउटपुट न्यूरॉन की इच्छाओं के साथ जोड़ा

118
00:07:49,229 --> 00:07:55,183
जाता है कि इस दूसरी से आखिरी परत के साथ क्या होना चाहिए, फिर से संबंधित वजन के

119
00:07:55,183 --> 00:08:01,740
अनुपात में, और उनमें से प्रत्येक न्यूरॉन को कितनी आवश्यकता है इसके अनुपात में को बदलने।

120
00:08:01,740 --> 00:08:05,940
यहीं वह जगह है जहां पीछे की ओर प्रचार करने का विचार आता है।

121
00:08:05,940 --> 00:08:10,029
इन सभी वांछित प्रभावों को एक साथ जोड़कर, आपको मूल रूप से उन संकेतों

122
00:08:10,029 --> 00:08:14,300
की एक सूची मिलती है जिन्हें आप इस दूसरी से आखिरी परत तक करना चाहते हैं।

123
00:08:14,300 --> 00:08:18,741
और एक बार जब आपके पास वे होते हैं, तो आप उसी प्रक्रिया को उन प्रासंगिक भारों और

124
00:08:18,741 --> 00:08:23,627
पूर्वाग्रहों पर पुनरावर्ती रूप से लागू कर सकते हैं जो उन मूल्यों को निर्धारित करते हैं,

125
00:08:23,627 --> 00:08:28,513
उसी प्रक्रिया को दोहराते हुए जिससे मैं अभी गुजरा हूं और नेटवर्क के माध्यम से पीछे की ओर

126
00:08:28,513 --> 00:08:29,180
बढ़ रहा हूं।

127
00:08:29,180 --> 00:08:33,141
और थोड़ा और ज़ूम आउट करते हुए, याद रखें कि यह सब ठीक उसी तरह है जैसे एक एकल

128
00:08:33,141 --> 00:08:37,520
प्रशिक्षण उदाहरण उन वज़न और पूर्वाग्रहों में से प्रत्येक को नियंत्रित करना चाहता है।

129
00:08:37,520 --> 00:08:40,854
यदि हम केवल वही सुनते हैं जो वह 2 चाहता था, तो नेटवर्क को अंततः सभी

130
00:08:40,854 --> 00:08:44,140
छवियों को 2 के रूप में वर्गीकृत करने के लिए प्रोत्साहित किया जाएगा।

131
00:08:44,140 --> 00:08:50,193
तो आप जो करते हैं वह हर दूसरे प्रशिक्षण उदाहरण के लिए इसी बैकप्रॉप रूटीन

132
00:08:50,193 --> 00:08:56,329
से गुजरते हैं, यह रिकॉर्ड करते हुए कि उनमें से प्रत्येक वजन और पूर्वाग्रह

133
00:08:56,329 --> 00:09:02,300
को कैसे बदलना चाहते हैं, और उन वांछित परिवर्तनों को एक साथ औसत करते हैं।

134
00:09:02,300 --> 00:09:06,255
यहां प्रत्येक वजन और पूर्वाग्रह के औसत संकेतों का यह संग्रह,

135
00:09:06,255 --> 00:09:10,210
शिथिल रूप से कहें तो, पिछले वीडियो में संदर्भित लागत फ़ंक्शन

136
00:09:10,210 --> 00:09:14,360
का नकारात्मक ग्रेडिएंट है, या कम से कम इसके लिए आनुपातिक कुछ है।

137
00:09:14,360 --> 00:09:18,329
मैं शिथिल रूप से केवल इसलिए कह रहा हूं क्योंकि मुझे अभी तक उन संकेतों के

138
00:09:18,329 --> 00:09:22,408
बारे में मात्रात्मक रूप से सटीक नहीं मिल पाया है, लेकिन अगर आप मेरे द्वारा

139
00:09:22,408 --> 00:09:26,378
संदर्भित हर बदलाव को समझ गए हैं, तो कुछ दूसरों की तुलना में आनुपातिक रूप

140
00:09:26,378 --> 00:09:30,184
से बड़े क्यों हैं, और उन सभी को एक साथ जोड़ने की आवश्यकता कैसे है, आप

141
00:09:30,184 --> 00:09:34,100
इसके लिए यांत्रिकी को समझते हैं बैकप्रोपेगेशन वास्तव में क्या कर रहा है।

142
00:09:34,100 --> 00:09:38,543
वैसे, व्यवहार में, प्रत्येक प्रशिक्षण उदाहरण के प्रत्येक ग्रेडिएंट

143
00:09:38,543 --> 00:09:43,120
डिसेंट चरण के प्रभाव को जोड़ने में कंप्यूटर को बहुत लंबा समय लगता है।

144
00:09:43,120 --> 00:09:45,540
तो इसके बजाय यहाँ वह है जो आमतौर पर किया जाता है।

145
00:09:45,540 --> 00:09:49,460
आप अपने प्रशिक्षण डेटा को बेतरतीब ढंग से फेरबदल करते हैं और इसे मिनी-बैचों के एक

146
00:09:49,460 --> 00:09:53,380
पूरे समूह में विभाजित करते हैं, मान लें कि प्रत्येक में 100 प्रशिक्षण उदाहरण हैं।

147
00:09:53,380 --> 00:09:56,980
फिर आप मिनी-बैच के अनुसार एक चरण की गणना करें।

148
00:09:56,980 --> 00:10:00,839
यह लागत फ़ंक्शन का वास्तविक ग्रेडिएंट नहीं है, जो सभी प्रशिक्षण डेटा पर

149
00:10:00,839 --> 00:10:04,752
निर्भर करता है, न कि इस छोटे उपसमुच्चय पर, इसलिए यह डाउनहिल का सबसे कुशल

150
00:10:04,752 --> 00:10:08,772
कदम नहीं है, लेकिन प्रत्येक मिनी-बैच आपको एक बहुत अच्छा अनुमान देता है, और

151
00:10:08,772 --> 00:10:12,900
इससे भी महत्वपूर्ण बात यह है आपको एक महत्वपूर्ण कम्प्यूटेशनल स्पीडअप देता है।

152
00:10:12,900 --> 00:10:16,500
यदि आप प्रासंगिक लागत सतह के तहत अपने नेटवर्क के प्रक्षेप पथ की योजना

153
00:10:16,500 --> 00:10:20,202
बनाते हैं, तो यह कुछ हद तक एक नशे में धुत आदमी की तरह होगा जो लक्ष्यहीन

154
00:10:20,202 --> 00:10:23,905
रूप से एक पहाड़ी से नीचे गिर रहा है, लेकिन तेजी से कदम उठा रहा है, न कि

155
00:10:23,905 --> 00:10:27,711
एक सावधानीपूर्वक गणना करने वाला व्यक्ति प्रत्येक कदम की सटीक डाउनहिल दिशा

156
00:10:27,711 --> 00:10:31,620
निर्धारित करता है। उस दिशा में बहुत धीमा और सावधानीपूर्वक कदम उठाने से पहले।

157
00:10:31,620 --> 00:10:35,200
इस तकनीक को स्टोकेस्टिक ग्रेडिएंट डिसेंट कहा जाता है।

158
00:10:35,200 --> 00:10:40,400
यहाँ बहुत कुछ चल रहा है, तो आइए इसे अपने लिए संक्षेप में प्रस्तुत करें, क्या हम करेंगे?

159
00:10:40,400 --> 00:10:45,540
बैकप्रॉपैगेशन यह निर्धारित करने के लिए एल्गोरिदम है कि एक एकल प्रशिक्षण उदाहरण वजन और

160
00:10:45,540 --> 00:10:50,681
पूर्वाग्रहों को कैसे कम करना चाहेगा, न केवल इस संदर्भ में कि उन्हें ऊपर जाना चाहिए या

161
00:10:50,681 --> 00:10:55,941
नीचे जाना चाहिए, बल्कि उन परिवर्तनों के सापेक्ष अनुपात के कारण सबसे तेजी से कमी आती है।

162
00:10:55,941 --> 00:10:56,240
लागत।

163
00:10:56,240 --> 00:11:00,639
एक सच्चे ग्रेडिएंट डिसेंट चरण में आपके सभी दसियों और हजारों प्रशिक्षण उदाहरणों के

164
00:11:00,639 --> 00:11:05,039
लिए ऐसा करना और आपके द्वारा प्राप्त वांछित परिवर्तनों का औसत शामिल होगा, लेकिन यह

165
00:11:05,039 --> 00:11:09,385
कम्प्यूटेशनल रूप से धीमा है, इसलिए इसके बजाय आप डेटा को मिनी-बैचों में यादृच्छिक

166
00:11:09,385 --> 00:11:14,000
रूप से उप-विभाजित करते हैं और प्रत्येक चरण की गणना एक के संबंध में करते हैं। मिनी-बैच।

167
00:11:14,000 --> 00:11:18,654
बार-बार सभी मिनी-बैचों से गुजरते हुए और इन समायोजनों को करते हुए,

168
00:11:18,654 --> 00:11:23,167
आप स्थानीय न्यूनतम लागत फ़ंक्शन की ओर जुटेंगे, जिसका अर्थ है कि

169
00:11:23,167 --> 00:11:27,540
आपका नेटवर्क प्रशिक्षण उदाहरणों पर वास्तव में अच्छा काम करेगा।

170
00:11:27,540 --> 00:11:32,352
तो जो कुछ कहा गया है, उसके साथ, कोड की प्रत्येक पंक्ति जो बैकप्रॉप को लागू करने में

171
00:11:32,352 --> 00:11:37,450
जाएगी, वास्तव में उस चीज़ से मेल खाती है जिसे आपने अब देखा है, कम से कम अनौपचारिक शब्दों

172
00:11:37,450 --> 00:11:37,680
में।

173
00:11:37,680 --> 00:11:41,277
लेकिन कभी-कभी यह जानना कि गणित क्या करता है, केवल आधी लड़ाई है, और केवल उस

174
00:11:41,277 --> 00:11:44,780
लानत चीज़ का प्रतिनिधित्व करना है जहां यह सब गड़बड़ और भ्रमित हो जाता है।

175
00:11:44,780 --> 00:11:48,923
तो, आप में से जो लोग गहराई में जाना चाहते हैं, उनके लिए अगला वीडियो उन्हीं विचारों

176
00:11:48,923 --> 00:11:53,017
पर आधारित है जो अभी यहां प्रस्तुत किए गए थे, लेकिन अंतर्निहित गणना के संदर्भ में,

177
00:11:53,017 --> 00:11:57,460
उम्मीद है कि जैसे ही आप विषय को देखेंगे तो यह थोड़ा और अधिक परिचित हो जाएगा। अन्य संसाधन।

178
00:11:57,460 --> 00:12:00,511
इससे पहले, एक बात पर जोर देने लायक बात यह है कि इस एल्गोरिदम को काम

179
00:12:00,511 --> 00:12:03,518
करने के लिए, और यह केवल तंत्रिका नेटवर्क से परे सभी प्रकार की मशीन

180
00:12:03,518 --> 00:12:06,840
लर्निंग के लिए जाता है, आपको बहुत सारे प्रशिक्षण डेटा की आवश्यकता होती है।

181
00:12:06,840 --> 00:12:09,517
हमारे मामले में, एक चीज़ जो हस्तलिखित अंकों को इतना अच्छा

182
00:12:09,517 --> 00:12:12,194
उदाहरण बनाती है, वह यह है कि एमएनआईएसटी डेटाबेस मौजूद है,

183
00:12:12,194 --> 00:12:15,380
जिसमें बहुत सारे उदाहरण हैं जिन्हें मनुष्यों द्वारा लेबल किया गया है।

184
00:12:15,380 --> 00:12:17,833
तो एक आम चुनौती जिससे आपमें से मशीन लर्निंग में काम करने वाले परिचित होंगे, वह है केवल

185
00:12:17,833 --> 00:12:20,257
लेबल किए गए प्रशिक्षण डेटा को प्राप्त करना जिसकी आपको वास्तव में आवश्यकता है, चाहे वह

186
00:12:20,257 --> 00:12:22,767
लोगों को हजारों छवियों को लेबल करना हो, या किसी भी अन्य डेटा प्रकार के साथ आप काम कर रहे

187
00:12:22,767 --> 00:12:22,880
हों।

