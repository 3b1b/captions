1
00:00:04,060 --> 00:00:06,419
यहां, हम बैकप्रॉपैगेशन से निपटते हैं, तंत्रिका 

2
00:00:06,419 --> 00:00:08,880
नेटवर्क कैसे सीखते हैं इसके पीछे मुख्य एल्गोरिदम।

3
00:00:09,400 --> 00:00:11,537
हम कहां हैं, इसके बारे में एक त्वरित पुनर्कथन के बाद, 

4
00:00:11,537 --> 00:00:14,387
पहली चीज जो मैं करूंगा वह यह है कि एल्गोरिदम वास्तव में क्या कर रहा है, 

5
00:00:14,387 --> 00:00:17,000
सूत्रों के किसी भी संदर्भ के बिना, एक सहज ज्ञान युक्त पूर्वाभ्यास।

6
00:00:17,660 --> 00:00:20,292
फिर, आपमें से जो लोग गणित में गहराई से उतरना चाहते हैं, 

7
00:00:20,292 --> 00:00:23,020
उनके लिए अगला वीडियो इस सब के अंतर्निहित कलन पर आधारित है।

8
00:00:23,820 --> 00:00:27,367
यदि आपने पिछले दो वीडियो देखे हैं, या यदि आप उचित पृष्ठभूमि के साथ आगे बढ़ रहे हैं, 

9
00:00:27,367 --> 00:00:31,000
तो आप जानते हैं कि तंत्रिका नेटवर्क क्या है, और यह आगे की जानकारी कैसे प्रदान करता है।

10
00:00:31,680 --> 00:00:34,944
यहां, हम हस्तलिखित अंकों को पहचानने का क्लासिक उदाहरण दे रहे हैं, 

11
00:00:34,944 --> 00:00:38,900
जिनके पिक्सेल मान 784 न्यूरॉन्स के साथ नेटवर्क की पहली परत में फीड हो जाते हैं, 

12
00:00:38,900 --> 00:00:41,621
और मैं दो छिपी हुई परतों वाला एक नेटवर्क दिखा रहा हूं, 

13
00:00:41,621 --> 00:00:45,627
जिनमें से प्रत्येक में केवल 16 न्यूरॉन हैं, और एक आउटपुट है 10 न्यूरॉन्स की परत, 

14
00:00:45,627 --> 00:00:49,040
यह दर्शाती है कि नेटवर्क अपने उत्तर के रूप में कौन सा अंक चुन रहा है।

15
00:00:50,040 --> 00:00:53,153
मैं यह भी उम्मीद कर रहा हूं कि आप ग्रेडिएंट डिसेंट को समझेंगे, 

16
00:00:53,153 --> 00:00:56,811
जैसा कि पिछले वीडियो में बताया गया है, और सीखने से हमारा मतलब यह है कि हम 

17
00:00:56,811 --> 00:01:01,260
यह पता लगाना चाहते हैं कि कौन से वजन और पूर्वाग्रह एक निश्चित लागत फ़ंक्शन को कम करते हैं।

18
00:01:02,040 --> 00:01:06,325
एक त्वरित अनुस्मारक के रूप में, एक एकल प्रशिक्षण उदाहरण की लागत के लिए, 

19
00:01:06,325 --> 00:01:11,504
आप उस आउटपुट को लेते हैं जो नेटवर्क देता है, उस आउटपुट के साथ जो आप उसे देना चाहते थे, 

20
00:01:11,504 --> 00:01:14,600
और प्रत्येक घटक के बीच अंतर के वर्गों को जोड़ते हैं।

21
00:01:15,380 --> 00:01:19,002
अपने सभी हज़ारों प्रशिक्षण उदाहरणों के लिए ऐसा करने और 

22
00:01:19,002 --> 00:01:23,020
परिणामों का औसत निकालने से आपको नेटवर्क की कुल लागत मिलती है।

23
00:01:23,020 --> 00:01:27,135
जैसे कि यह सोचने के लिए पर्याप्त नहीं है, जैसा कि पिछले वीडियो में वर्णित है, 

24
00:01:27,135 --> 00:01:31,144
जिस चीज की हम तलाश कर रहे हैं वह इस लागत फ़ंक्शन का नकारात्मक ग्रेडिएंट है, 

25
00:01:31,144 --> 00:01:35,893
जो आपको बताता है कि आपको सभी वजन और पूर्वाग्रहों को कैसे बदलने की आवश्यकता है ये कनेक्शन, 

26
00:01:35,893 --> 00:01:38,320
ताकि लागत को सबसे कुशलतापूर्वक कम किया जा सके।

27
00:01:43,260 --> 00:01:49,580
बैकप्रॉपैगेशन, इस वीडियो का विषय, उस जटिल जटिल ग्रेडिएंट की गणना के लिए एक एल्गोरिदम है।

28
00:01:49,580 --> 00:01:53,127
पिछले वीडियो से एक विचार जो मैं वास्तव में चाहता हूं कि आप अभी अपने दिमाग 

29
00:01:53,127 --> 00:01:56,723
में मजबूती से रखें, क्योंकि 13,000 आयामों में एक दिशा के रूप में ग्रेडिएंट 

30
00:01:56,723 --> 00:02:00,895
वेक्टर के बारे में सोचना, इसे हल्के ढंग से कहें तो, हमारी कल्पनाओं के दायरे से परे है, 

31
00:02:00,895 --> 00:02:03,580
एक और विचार है जिस तरह से आप इसके बारे में सोच सकते हैं.

32
00:02:04,600 --> 00:02:07,907
यहां प्रत्येक घटक का परिमाण आपको बता रहा है कि लागत फ़ंक्शन 

33
00:02:07,907 --> 00:02:10,940
प्रत्येक भार और पूर्वाग्रह के प्रति कितना संवेदनशील है।

34
00:02:11,800 --> 00:02:17,074
उदाहरण के लिए, मान लें कि आप उस प्रक्रिया से गुजरते हैं जिसका मैं वर्णन करने जा रहा हूं, 

35
00:02:17,074 --> 00:02:21,874
और नकारात्मक ग्रेडिएंट की गणना करते हैं, और यहां इस किनारे पर वजन से जुड़ा घटक 3 

36
00:02:21,874 --> 00:02:26,260
निकलता है।2, जबकि इस किनारे से जुड़ा घटक यहाँ 0 के रूप में सामने आता है।1.

37
00:02:26,820 --> 00:02:30,775
जिस तरह से आप इसकी व्याख्या करेंगे वह यह है कि फ़ंक्शन की लागत उस 

38
00:02:30,775 --> 00:02:34,250
पहले वजन में परिवर्तन के प्रति 32 गुना अधिक संवेदनशील है, 

39
00:02:34,250 --> 00:02:38,985
इसलिए यदि आप उस मूल्य को थोड़ा सा हिलाते हैं, तो इससे लागत में कुछ बदलाव आएगा, 

40
00:02:38,985 --> 00:02:43,060
और वह परिवर्तन होगा यह उस दूसरे वजन के समान झटके से 32 गुना अधिक है।

41
00:02:48,420 --> 00:02:51,625
निजी तौर पर, जब मैं पहली बार बैकप्रॉपैगेशन के बारे में सीख रहा था, 

42
00:02:51,625 --> 00:02:55,740
तो मुझे लगता है कि सबसे भ्रमित करने वाला पहलू सिर्फ नोटेशन और इंडेक्स का पीछा करना था।

43
00:02:56,220 --> 00:02:59,572
लेकिन एक बार जब आप यह समझ लेते हैं कि इस एल्गोरिदम का प्रत्येक भाग वास्तव 

44
00:02:59,572 --> 00:03:03,106
में क्या कर रहा है, तो इसका प्रत्येक व्यक्तिगत प्रभाव वास्तव में बहुत सहज है, 

45
00:03:03,106 --> 00:03:06,640
बात बस इतनी है कि बहुत सारे छोटे-छोटे समायोजन एक-दूसरे के ऊपर परत चढ़ रहे हैं।

46
00:03:07,740 --> 00:03:11,239
इसलिए मैं यहां नोटेशन की पूरी उपेक्षा के साथ चीजों को शुरू करने जा रहा हूं, 

47
00:03:11,239 --> 00:03:14,048
और प्रत्येक प्रशिक्षण उदाहरण के वजन और पूर्वाग्रहों पर पड़ने 

48
00:03:14,048 --> 00:03:16,120
वाले प्रभावों के बारे में विस्तार से बताऊंगा।

49
00:03:17,020 --> 00:03:21,814
क्योंकि लागत फ़ंक्शन में हजारों प्रशिक्षण उदाहरणों में प्रति उदाहरण एक निश्चित 

50
00:03:21,814 --> 00:03:26,427
लागत का औसत शामिल होता है, जिस तरह से हम एक एकल ग्रेडिएंट डिसेंट चरण के लिए 

51
00:03:26,427 --> 00:03:31,040
वजन और पूर्वाग्रह को समायोजित करते हैं वह भी हर एक उदाहरण पर निर्भर करता है।

52
00:03:31,680 --> 00:03:35,398
या बल्कि, सिद्धांत रूप में यह होना चाहिए, लेकिन कम्प्यूटेशनल दक्षता के लिए हम बाद में एक 

53
00:03:35,398 --> 00:03:38,991
छोटी सी तरकीब अपनाएंगे ताकि आपको हर चरण के लिए हर एक उदाहरण को हिट करने की आवश्यकता न 

54
00:03:38,991 --> 00:03:39,200
पड़े।

55
00:03:39,200 --> 00:03:43,020
अन्य मामलों में, अभी, हम अपना ध्यान केवल एक उदाहरण, 

56
00:03:43,020 --> 00:03:45,960
2 की इस छवि पर केंद्रित करने जा रहे हैं।

57
00:03:46,720 --> 00:03:49,145
इस एक प्रशिक्षण उदाहरण का इस बात पर क्या प्रभाव होना 

58
00:03:49,145 --> 00:03:51,480
चाहिए कि वज़न और पूर्वाग्रह कैसे समायोजित होते हैं?

59
00:03:52,680 --> 00:03:57,366
मान लीजिए कि हम एक ऐसे बिंदु पर हैं जहां नेटवर्क अभी तक अच्छी तरह से प्रशिक्षित नहीं है, 

60
00:03:57,366 --> 00:04:02,000
इसलिए आउटपुट में सक्रियण काफी यादृच्छिक दिखेंगे, शायद 0 जैसा कुछ।5, 0.8, 0.2, पर और आगे.

61
00:04:02,520 --> 00:04:05,086
हम उन सक्रियणों को सीधे तौर पर नहीं बदल सकते हैं, 

62
00:04:05,086 --> 00:04:07,652
हम केवल वज़न और पूर्वाग्रहों पर प्रभाव डालते हैं, 

63
00:04:07,652 --> 00:04:10,886
लेकिन यह ट्रैक रखना सहायक होता है कि हम चाहते हैं कि उस आउटपुट 

64
00:04:10,886 --> 00:04:12,580
परत पर कौन सा समायोजन होना चाहिए।

65
00:04:13,360 --> 00:04:17,098
और चूँकि हम चाहते हैं कि यह छवि को 2 के रूप में वर्गीकृत करे, 

66
00:04:17,098 --> 00:04:21,260
हम चाहते हैं कि तीसरा मान ऊपर की ओर हो, जबकि अन्य सभी नीचे की ओर हों।

67
00:04:22,060 --> 00:04:25,887
इसके अलावा, इन नज़ों का आकार इस बात पर आनुपातिक होना चाहिए 

68
00:04:25,887 --> 00:04:29,520
कि प्रत्येक वर्तमान मान अपने लक्ष्य मान से कितनी दूर है।

69
00:04:30,220 --> 00:04:35,398
उदाहरण के लिए, उस संख्या 2 न्यूरॉन की सक्रियता में वृद्धि एक मायने में संख्या 8 

70
00:04:35,398 --> 00:04:40,900
न्यूरॉन की कमी से अधिक महत्वपूर्ण है, जो पहले से ही काफी करीब है जहां इसे होना चाहिए।

71
00:04:42,040 --> 00:04:45,446
तो आगे बढ़ते हुए, आइए केवल इस एक न्यूरॉन पर ध्यान केंद्रित करें, 

72
00:04:45,446 --> 00:04:47,280
जिसकी सक्रियता हम बढ़ाना चाहते हैं।

73
00:04:48,180 --> 00:04:52,466
याद रखें, उस सक्रियण को पिछली परत में सभी सक्रियणों के एक निश्चित भारित योग 

74
00:04:52,466 --> 00:04:55,738
के साथ-साथ एक पूर्वाग्रह के रूप में परिभाषित किया गया है, 

75
00:04:55,738 --> 00:05:00,024
जिसे बाद में सिग्मॉइड स्क्विशिफिकेशन फ़ंक्शन, या एक ReLU जैसी किसी चीज़ में 

76
00:05:00,024 --> 00:05:01,040
प्लग किया जाता है।

77
00:05:01,640 --> 00:05:07,020
इसलिए तीन अलग-अलग रास्ते हैं जो एक साथ मिलकर उस सक्रियता को बढ़ाने में मदद कर सकते हैं।

78
00:05:07,440 --> 00:05:11,098
आप पूर्वाग्रह बढ़ा सकते हैं, आप वजन बढ़ा सकते हैं, 

79
00:05:11,098 --> 00:05:14,040
और आप पिछली परत से सक्रियता बदल सकते हैं।

80
00:05:14,940 --> 00:05:17,921
वज़न को कैसे समायोजित किया जाना चाहिए, इस पर ध्यान केंद्रित करते हुए, 

81
00:05:17,921 --> 00:05:20,860
ध्यान दें कि वज़न का वास्तव में प्रभाव के विभिन्न स्तर कैसे होते हैं।

82
00:05:21,440 --> 00:05:25,270
पिछली परत से सबसे चमकीले न्यूरॉन्स के साथ कनेक्शन का सबसे बड़ा प्रभाव 

83
00:05:25,270 --> 00:05:29,100
होता है क्योंकि उन भारों को बड़े सक्रियण मूल्यों से गुणा किया जाता है।

84
00:05:31,460 --> 00:05:34,452
इसलिए यदि आप उन भारों में से किसी एक को बढ़ाना चाहते हैं, 

85
00:05:34,452 --> 00:05:38,321
तो इसका वास्तव में डिमर न्यूरॉन्स के साथ कनेक्शन के भार को बढ़ाने की तुलना 

86
00:05:38,321 --> 00:05:40,797
में अंतिम लागत फ़ंक्शन पर अधिक प्रभाव पड़ता है, 

87
00:05:40,797 --> 00:05:43,480
कम से कम जहां तक इस एक प्रशिक्षण उदाहरण का संबंध है।

88
00:05:44,420 --> 00:05:46,727
याद रखें, जब हम ग्रेडिएंट डिसेंट के बारे में बात करते हैं, 

89
00:05:46,727 --> 00:05:50,052
तो हम सिर्फ इस बात की परवाह नहीं करते हैं कि प्रत्येक घटक को ऊपर या नीचे जाना चाहिए, 

90
00:05:50,052 --> 00:05:53,220
हम इसकी परवाह करते हैं कि कौन सा घटक आपको आपके पैसे के लिए सबसे अधिक लाभ देता है।

91
00:05:55,020 --> 00:05:58,849
वैसे, यह कम से कम कुछ हद तक तंत्रिका विज्ञान के एक सिद्धांत की याद दिलाता है 

92
00:05:58,849 --> 00:06:02,082
कि न्यूरॉन्स के जैविक नेटवर्क कैसे सीखते हैं, हेब्बियन सिद्धांत, 

93
00:06:02,082 --> 00:06:06,460
जिसे अक्सर वाक्यांश में अभिव्यक्त किया जाता है, न्यूरॉन्स जो एक साथ तार से आग लगाते हैं।

94
00:06:07,260 --> 00:06:11,056
यहां, वजन में सबसे बड़ी वृद्धि, कनेक्शन की सबसे बड़ी मजबूती, 

95
00:06:11,056 --> 00:06:15,910
उन न्यूरॉन्स के बीच होती है जो सबसे अधिक सक्रिय हैं और जिनके बारे में हम अधिक 

96
00:06:15,910 --> 00:06:17,280
सक्रिय होना चाहते हैं।

97
00:06:17,940 --> 00:06:20,574
एक अर्थ में, 2 को देखते समय जो न्यूरॉन्स सक्रिय होते हैं, 

98
00:06:20,574 --> 00:06:24,480
वे इसके बारे में सोचते समय सक्रिय होने वाले न्यूरॉन्स से अधिक मजबूती से जुड़ जाते हैं।

99
00:06:25,400 --> 00:06:29,176
स्पष्ट होने के लिए, मैं इस बारे में एक या दूसरे तरीके से बयान देने की स्थिति में 

100
00:06:29,176 --> 00:06:33,046
नहीं हूं कि क्या न्यूरॉन्स के कृत्रिम नेटवर्क जैविक मस्तिष्क की तरह कुछ भी व्यवहार 

101
00:06:33,046 --> 00:06:36,870
करते हैं, और यह तारों को एक साथ जोड़ता है विचार कुछ सार्थक तारांकन के साथ आता है, 

102
00:06:36,870 --> 00:06:41,020
लेकिन इसे बहुत ही ढीले के रूप में लिया जाता है सादृश्य, मुझे यह नोट करना दिलचस्प लगता है।

103
00:06:41,940 --> 00:06:45,547
वैसे भी, तीसरा तरीका जिससे हम इस न्यूरॉन की सक्रियता को बढ़ाने 

104
00:06:45,547 --> 00:06:49,040
में मदद कर सकते हैं वह है पिछली परत की सभी सक्रियता को बदलना।

105
00:06:49,040 --> 00:06:54,417
अर्थात्, यदि सकारात्मक भार वाले उस अंक 2 न्यूरॉन से जुड़ी हर चीज चमकीली हो गई, 

106
00:06:54,417 --> 00:06:57,889
और यदि नकारात्मक भार से जुड़ी हर चीज धुंधली हो गई, 

107
00:06:57,889 --> 00:07:00,680
तो वह अंक 2 न्यूरॉन अधिक सक्रिय हो जाएगा।

108
00:07:02,540 --> 00:07:06,103
और वज़न परिवर्तनों के समान, आप संबंधित वज़न के आकार के आनुपातिक 

109
00:07:06,103 --> 00:07:10,280
परिवर्तनों की तलाश करके अपने पैसे का सबसे अधिक लाभ प्राप्त करने जा रहे हैं।

110
00:07:12,140 --> 00:07:15,370
अब निःसंदेह, हम सीधे तौर पर उन सक्रियताओं को प्रभावित नहीं कर सकते हैं, 

111
00:07:15,370 --> 00:07:17,480
हमारा नियंत्रण केवल वज़न और पूर्वाग्रहों पर है।

112
00:07:17,480 --> 00:07:24,120
लेकिन पिछली परत की तरह ही, यह ध्यान रखना भी उपयोगी है कि वे वांछित परिवर्तन क्या हैं।

113
00:07:24,580 --> 00:07:27,016
लेकिन ध्यान रखें, यहां एक कदम ज़ूम आउट करना, यह 

114
00:07:27,016 --> 00:07:29,200
वही है जो वह अंक 2 आउटपुट न्यूरॉन चाहता है।

115
00:07:29,760 --> 00:07:33,991
याद रखें, हम यह भी चाहते हैं कि अंतिम परत के अन्य सभी न्यूरॉन्स कम सक्रिय हो जाएं, 

116
00:07:33,991 --> 00:07:37,305
और उन अन्य आउटपुट न्यूरॉन्स में से प्रत्येक के अपने विचार हैं कि 

117
00:07:37,305 --> 00:07:39,600
उस दूसरी से अंतिम परत के साथ क्या होना चाहिए।

118
00:07:42,700 --> 00:07:48,560
तो इस अंक 2 न्यूरॉन की इच्छा को अन्य सभी आउटपुट न्यूरॉन की इच्छाओं के साथ जोड़ा 

119
00:07:48,560 --> 00:07:52,735
जाता है कि इस दूसरी से आखिरी परत के साथ क्या होना चाहिए, 

120
00:07:52,735 --> 00:07:58,668
फिर से संबंधित वजन के अनुपात में, और उनमें से प्रत्येक न्यूरॉन को कितनी आवश्यकता 

121
00:07:58,668 --> 00:08:00,720
है इसके अनुपात में को बदलने।

122
00:08:01,600 --> 00:08:05,480
यहीं वह जगह है जहां पीछे की ओर प्रचार करने का विचार आता है।

123
00:08:05,820 --> 00:08:09,508
इन सभी वांछित प्रभावों को एक साथ जोड़कर, आपको मूल रूप से उन संकेतों 

124
00:08:09,508 --> 00:08:13,360
की एक सूची मिलती है जिन्हें आप इस दूसरी से आखिरी परत तक करना चाहते हैं।

125
00:08:14,220 --> 00:08:17,467
और एक बार जब आपके पास वे होते हैं, तो आप उसी प्रक्रिया को उन प्रासंगिक भारों और 

126
00:08:17,467 --> 00:08:21,040
पूर्वाग्रहों पर पुनरावर्ती रूप से लागू कर सकते हैं जो उन मूल्यों को निर्धारित करते हैं, 

127
00:08:21,040 --> 00:08:24,612
उसी प्रक्रिया को दोहराते हुए जिससे मैं अभी गुजरा हूं और नेटवर्क के माध्यम से पीछे की ओर 

128
00:08:24,612 --> 00:08:25,100
बढ़ रहा हूं।

129
00:08:28,960 --> 00:08:32,779
और थोड़ा और ज़ूम आउट करते हुए, याद रखें कि यह सब ठीक उसी तरह है जैसे एक एकल 

130
00:08:32,779 --> 00:08:37,000
प्रशिक्षण उदाहरण उन वज़न और पूर्वाग्रहों में से प्रत्येक को नियंत्रित करना चाहता है।

131
00:08:37,480 --> 00:08:40,371
यदि हम केवल वही सुनते हैं जो वह 2 चाहता था, तो नेटवर्क को अंततः सभी 

132
00:08:40,371 --> 00:08:43,220
छवियों को 2 के रूप में वर्गीकृत करने के लिए प्रोत्साहित किया जाएगा।

133
00:08:44,059 --> 00:08:48,857
तो आप जो करते हैं वह हर दूसरे प्रशिक्षण उदाहरण के लिए इसी बैकप्रॉप रूटीन से गुजरते हैं, 

134
00:08:48,857 --> 00:08:53,437
यह रिकॉर्ड करते हुए कि उनमें से प्रत्येक वजन और पूर्वाग्रह को कैसे बदलना चाहते हैं, 

135
00:08:53,437 --> 00:08:56,000
और उन वांछित परिवर्तनों को एक साथ औसत करते हैं।

136
00:09:01,720 --> 00:09:05,642
यहां प्रत्येक वजन और पूर्वाग्रह के औसत संकेतों का यह संग्रह, 

137
00:09:05,642 --> 00:09:11,300
शिथिल रूप से कहें तो, पिछले वीडियो में संदर्भित लागत फ़ंक्शन का नकारात्मक ग्रेडिएंट है, 

138
00:09:11,300 --> 00:09:13,680
या कम से कम इसके लिए आनुपातिक कुछ है।

139
00:09:14,380 --> 00:09:17,722
मैं शिथिल रूप से केवल इसलिए कह रहा हूं क्योंकि मुझे अभी तक उन संकेतों के 

140
00:09:17,722 --> 00:09:20,011
बारे में मात्रात्मक रूप से सटीक नहीं मिल पाया है, 

141
00:09:20,011 --> 00:09:22,667
लेकिन अगर आप मेरे द्वारा संदर्भित हर बदलाव को समझ गए हैं, 

142
00:09:22,667 --> 00:09:25,368
तो कुछ दूसरों की तुलना में आनुपातिक रूप से बड़े क्यों हैं, 

143
00:09:25,368 --> 00:09:27,566
और उन सभी को एक साथ जोड़ने की आवश्यकता कैसे है, 

144
00:09:27,566 --> 00:09:31,000
आप इसके लिए यांत्रिकी को समझते हैं बैकप्रोपेगेशन वास्तव में क्या कर रहा है।

145
00:09:33,960 --> 00:09:38,137
वैसे, व्यवहार में, प्रत्येक प्रशिक्षण उदाहरण के प्रत्येक ग्रेडिएंट 

146
00:09:38,137 --> 00:09:42,440
डिसेंट चरण के प्रभाव को जोड़ने में कंप्यूटर को बहुत लंबा समय लगता है।

147
00:09:43,140 --> 00:09:44,820
तो इसके बजाय यहाँ वह है जो आमतौर पर किया जाता है।

148
00:09:45,480 --> 00:09:48,950
आप अपने प्रशिक्षण डेटा को बेतरतीब ढंग से फेरबदल करते हैं और इसे मिनी-बैचों के एक 

149
00:09:48,950 --> 00:09:52,420
पूरे समूह में विभाजित करते हैं, मान लें कि प्रत्येक में 100 प्रशिक्षण उदाहरण हैं।

150
00:09:52,939 --> 00:09:57,280
फिर आप मिनी-बैच के अनुसार एक चरण की गणना करें।

151
00:09:57,280 --> 00:10:01,677
यह लागत फ़ंक्शन का वास्तविक ग्रेडिएंट नहीं है, जो सभी प्रशिक्षण डेटा पर निर्भर करता है, 

152
00:10:01,677 --> 00:10:05,174
न कि इस छोटे उपसमुच्चय पर, इसलिए यह डाउनहिल का सबसे कुशल कदम नहीं है, 

153
00:10:05,174 --> 00:10:08,122
लेकिन प्रत्येक मिनी-बैच आपको एक बहुत अच्छा अनुमान देता है, 

154
00:10:08,122 --> 00:10:12,120
और इससे भी महत्वपूर्ण बात यह है आपको एक महत्वपूर्ण कम्प्यूटेशनल स्पीडअप देता है।

155
00:10:12,820 --> 00:10:16,678
यदि आप प्रासंगिक लागत सतह के तहत अपने नेटवर्क के प्रक्षेप पथ की योजना बनाते हैं, 

156
00:10:16,678 --> 00:10:20,060
तो यह कुछ हद तक एक नशे में धुत आदमी की तरह होगा जो लक्ष्यहीन रूप से एक 

157
00:10:20,060 --> 00:10:22,776
पहाड़ी से नीचे गिर रहा है, लेकिन तेजी से कदम उठा रहा है, 

158
00:10:22,776 --> 00:10:26,301
न कि एक सावधानीपूर्वक गणना करने वाला व्यक्ति प्रत्येक कदम की सटीक डाउनहिल 

159
00:10:26,301 --> 00:10:30,160
दिशा निर्धारित करता है। उस दिशा में बहुत धीमा और सावधानीपूर्वक कदम उठाने से पहले।

160
00:10:31,540 --> 00:10:34,660
इस तकनीक को स्टोकेस्टिक ग्रेडिएंट डिसेंट कहा जाता है।

161
00:10:35,960 --> 00:10:39,620
यहाँ बहुत कुछ चल रहा है, तो आइए इसे अपने लिए संक्षेप में प्रस्तुत करें, क्या हम करेंगे?

162
00:10:40,440 --> 00:10:45,346
बैकप्रॉपैगेशन यह निर्धारित करने के लिए एल्गोरिदम है कि एक एकल प्रशिक्षण उदाहरण वजन और 

163
00:10:45,346 --> 00:10:50,253
पूर्वाग्रहों को कैसे कम करना चाहेगा, न केवल इस संदर्भ में कि उन्हें ऊपर जाना चाहिए या 

164
00:10:50,253 --> 00:10:55,217
नीचे जाना चाहिए, बल्कि उन परिवर्तनों के सापेक्ष अनुपात के कारण सबसे तेजी से कमी आती है।

165
00:10:55,217 --> 00:10:55,560
 लागत।

166
00:10:56,260 --> 00:11:00,466
एक सच्चे ग्रेडिएंट डिसेंट चरण में आपके सभी दसियों और हजारों प्रशिक्षण उदाहरणों के 

167
00:11:00,466 --> 00:11:04,211
लिए ऐसा करना और आपके द्वारा प्राप्त वांछित परिवर्तनों का औसत शामिल होगा, 

168
00:11:04,211 --> 00:11:08,315
लेकिन यह कम्प्यूटेशनल रूप से धीमा है, इसलिए इसके बजाय आप डेटा को मिनी-बैचों में 

169
00:11:08,315 --> 00:11:12,727
यादृच्छिक रूप से उप-विभाजित करते हैं और प्रत्येक चरण की गणना एक के संबंध में करते हैं।

170
00:11:12,727 --> 00:11:13,240
 मिनी-बैच।

171
00:11:14,000 --> 00:11:17,966
बार-बार सभी मिनी-बैचों से गुजरते हुए और इन समायोजनों को करते हुए, 

172
00:11:17,966 --> 00:11:20,791
आप स्थानीय न्यूनतम लागत फ़ंक्शन की ओर जुटेंगे, 

173
00:11:20,791 --> 00:11:25,540
जिसका अर्थ है कि आपका नेटवर्क प्रशिक्षण उदाहरणों पर वास्तव में अच्छा काम करेगा।

174
00:11:27,240 --> 00:11:31,738
तो जो कुछ कहा गया है, उसके साथ, कोड की प्रत्येक पंक्ति जो बैकप्रॉप को लागू करने में 

175
00:11:31,738 --> 00:11:35,113
जाएगी, वास्तव में उस चीज़ से मेल खाती है जिसे आपने अब देखा है, 

176
00:11:35,113 --> 00:11:36,720
कम से कम अनौपचारिक शब्दों में।

177
00:11:37,560 --> 00:11:40,396
लेकिन कभी-कभी यह जानना कि गणित क्या करता है, केवल आधी लड़ाई है, 

178
00:11:40,396 --> 00:11:44,120
और केवल उस लानत चीज़ का प्रतिनिधित्व करना है जहां यह सब गड़बड़ और भ्रमित हो जाता है।

179
00:11:44,860 --> 00:11:46,999
तो, आप में से जो लोग गहराई में जाना चाहते हैं, 

180
00:11:46,999 --> 00:11:50,685
उनके लिए अगला वीडियो उन्हीं विचारों पर आधारित है जो अभी यहां प्रस्तुत किए गए थे, 

181
00:11:50,685 --> 00:11:54,462
लेकिन अंतर्निहित गणना के संदर्भ में, उम्मीद है कि जैसे ही आप विषय को देखेंगे तो यह 

182
00:11:54,462 --> 00:11:56,420
थोड़ा और अधिक परिचित हो जाएगा। अन्य संसाधन।

183
00:11:57,340 --> 00:12:00,657
इससे पहले, एक बात पर जोर देने लायक बात यह है कि इस एल्गोरिदम को काम करने के लिए, 

184
00:12:00,657 --> 00:12:03,852
और यह केवल तंत्रिका नेटवर्क से परे सभी प्रकार की मशीन लर्निंग के लिए जाता है, 

185
00:12:03,852 --> 00:12:05,900
आपको बहुत सारे प्रशिक्षण डेटा की आवश्यकता होती है।

186
00:12:06,420 --> 00:12:09,792
हमारे मामले में, एक चीज़ जो हस्तलिखित अंकों को इतना अच्छा उदाहरण बनाती है, 

187
00:12:09,792 --> 00:12:12,401
वह यह है कि एमएनआईएसटी डेटाबेस मौजूद है, जिसमें बहुत सारे 

188
00:12:12,401 --> 00:12:14,740
उदाहरण हैं जिन्हें मनुष्यों द्वारा लेबल किया गया है।

189
00:12:15,300 --> 00:12:18,671
तो एक आम चुनौती जिससे आपमें से मशीन लर्निंग में काम करने वाले परिचित होंगे, 

190
00:12:18,671 --> 00:12:22,619
वह है केवल लेबल किए गए प्रशिक्षण डेटा को प्राप्त करना जिसकी आपको वास्तव में आवश्यकता है, 

191
00:12:22,619 --> 00:12:24,748
चाहे वह लोगों को हजारों छवियों को लेबल करना हो, 

192
00:12:24,748 --> 00:12:27,100
या किसी भी अन्य डेटा प्रकार के साथ आप काम कर रहे हों।

