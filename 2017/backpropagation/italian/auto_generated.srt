1
00:00:00,000 --> 00:00:05,113
Qui affrontiamo la backpropagation, l’algoritmo fondamentale

2
00:00:05,113 --> 00:00:09,640
alla base del modo in cui le reti neurali apprendono.

3
00:00:09,640 --> 00:00:12,281
Dopo un breve riepilogo della situazione attuale, la prima cosa

4
00:00:12,281 --> 00:00:14,799
che farò sarà una guida intuitiva su cosa sta effettivamente

5
00:00:14,799 --> 00:00:17,400
facendo l&#39;algoritmo, senza alcun riferimento alle formule.

6
00:00:17,400 --> 00:00:20,670
Quindi, per quelli di voi che vogliono tuffarsi nella matematica,

7
00:00:20,670 --> 00:00:24,040
il prossimo video approfondirà i calcoli alla base di tutto questo.

8
00:00:24,040 --> 00:00:27,600
Se hai guardato gli ultimi due video o se stai semplicemente entrando nel merito con il

9
00:00:27,600 --> 00:00:31,080
background appropriato, sai cos&#39;è una rete neurale e come trasmette informazioni.

10
00:00:31,080 --> 00:00:35,633
Qui stiamo facendo il classico esempio di riconoscimento di cifre scritte a mano

11
00:00:35,633 --> 00:00:40,300
i cui valori di pixel vengono immessi nel primo strato della rete con 784 neuroni,

12
00:00:40,300 --> 00:00:44,853
e ho mostrato una rete con due strati nascosti con solo 16 neuroni ciascuno e un

13
00:00:44,853 --> 00:00:49,520
output strato di 10 neuroni, che indica quale cifra la rete sceglie come risposta.

14
00:00:49,520 --> 00:00:53,494
Mi aspetto anche che tu comprenda la discesa del gradiente, come descritta

15
00:00:53,494 --> 00:00:57,840
nell&#39;ultimo video, e come ciò che intendiamo per apprendimento è che vogliamo

16
00:00:57,840 --> 00:01:02,080
scoprire quali pesi e pregiudizi minimizzano una determinata funzione di costo.

17
00:01:02,080 --> 00:01:06,573
Come rapido promemoria, per il costo di un singolo esempio di formazione,

18
00:01:06,573 --> 00:01:11,066
prendi l&#39;output fornito dalla rete, insieme all&#39;output che volevi

19
00:01:11,066 --> 00:01:15,560
che fornisse, e somma i quadrati delle differenze tra ciascun componente.

20
00:01:15,560 --> 00:01:19,274
Facendo questo per tutte le decine di migliaia di esempi di formazione e

21
00:01:19,274 --> 00:01:23,040
calcolando la media dei risultati, si ottiene il costo totale della rete.

22
00:01:23,040 --> 00:01:27,887
Come se ciò non bastasse, come descritto nell&#39;ultimo video, la

23
00:01:27,887 --> 00:01:32,806
cosa che stiamo cercando è il gradiente negativo di questa funzione

24
00:01:32,806 --> 00:01:37,871
di costo, che ti dice come devi cambiare tutti i pesi e i bias, tutti

25
00:01:37,871 --> 00:01:43,080
queste connessioni, in modo da ridurre nel modo più efficiente i costi.

26
00:01:43,080 --> 00:01:46,366
La propagazione inversa, l&#39;argomento di questo video, è un

27
00:01:46,366 --> 00:01:49,600
algoritmo per calcolare quel gradiente follemente complicato.

28
00:01:49,600 --> 00:01:53,294
L&#39;idea dell&#39;ultimo video che voglio davvero che tu tenga saldamente

29
00:01:53,294 --> 00:01:56,939
in mente in questo momento è che, poiché pensare al vettore gradiente come

30
00:01:56,939 --> 00:02:00,779
una direzione in 13.000 dimensioni è, per dirla alla leggera, oltre la portata

31
00:02:00,779 --> 00:02:04,620
della nostra immaginazione, ce n&#39;è un&#39;altra modo in cui puoi pensarci.

32
00:02:04,620 --> 00:02:08,252
L&#39;entità di ciascun componente qui indica quanto la

33
00:02:08,252 --> 00:02:11,820
funzione di costo sia sensibile a ciascun peso e bias.

34
00:02:11,820 --> 00:02:16,777
Per esempio, diciamo che segui il processo che sto per descrivere, e calcoli il

35
00:02:16,777 --> 00:02:21,858
gradiente negativo, e il componente associato al peso su questo bordo qui risulta

36
00:02:21,858 --> 00:02:26,940
essere 3.2, mentre la componente associata a questo bordo qui risulta essere 0.1.

37
00:02:26,940 --> 00:02:31,629
Il modo in cui lo interpreteresti è che il costo della funzione è 32 volte più

38
00:02:31,629 --> 00:02:36,319
sensibile ai cambiamenti nel primo peso, quindi se dovessi spostare un po&#39;

39
00:02:36,319 --> 00:02:41,009
quel valore, causerebbe qualche cambiamento nel costo, e quel cambiamento è 32

40
00:02:41,009 --> 00:02:45,580
volte maggiore di quanto darebbe la stessa oscillazione a quel secondo peso.

41
00:02:45,580 --> 00:02:48,641
Personalmente, quando ho appreso per la prima volta della

42
00:02:48,641 --> 00:02:52,019
propagazione inversa, penso che l&#39;aspetto più confuso fosse

43
00:02:52,019 --> 00:02:55,820
proprio la notazione e l&#39;inseguimento dell&#39;indice di tutto ciò.

44
00:02:55,820 --> 00:02:59,841
Ma una volta che scopri cosa sta realmente facendo ogni parte di questo algoritmo,

45
00:02:59,841 --> 00:03:03,863
ogni singolo effetto che sta avendo è in realtà piuttosto intuitivo, è solo che ci

46
00:03:03,863 --> 00:03:07,740
sono molti piccoli aggiustamenti che si sovrappongono l&#39;uno sull&#39;altro.

47
00:03:07,740 --> 00:03:12,589
Quindi inizierò qui ignorando completamente la notazione e passerò semplicemente

48
00:03:12,589 --> 00:03:17,380
in rassegna gli effetti che ogni esempio di allenamento ha sui pesi e sui bias.

49
00:03:17,380 --> 00:03:22,112
Poiché la funzione di costo implica la media di un certo costo per esempio su tutte le

50
00:03:22,112 --> 00:03:26,898
decine di migliaia di esempi di addestramento, il modo in cui regoliamo i pesi e i bias

51
00:03:26,898 --> 00:03:31,740
per un singolo passaggio di discesa del gradiente dipende anche da ogni singolo esempio.

52
00:03:31,740 --> 00:03:34,373
O meglio, in linea di principio dovrebbe, ma per efficienza

53
00:03:34,373 --> 00:03:36,963
computazionale faremo un piccolo trucchetto più avanti per

54
00:03:36,963 --> 00:03:39,860
evitare di dover colpire ogni singolo esempio per ogni passaggio.

55
00:03:39,860 --> 00:03:43,211
In altri casi, per ora, tutto ciò che faremo è concentrare la

56
00:03:43,211 --> 00:03:46,780
nostra attenzione su un singolo esempio, questa immagine di un 2.

57
00:03:46,780 --> 00:03:49,260
Che effetto dovrebbe avere questo esempio di formazione

58
00:03:49,260 --> 00:03:51,740
sul modo in cui i pesi e i pregiudizi vengono adeguati?

59
00:03:51,740 --> 00:03:55,303
Diciamo che siamo a un punto in cui la rete non è ancora ben

60
00:03:55,303 --> 00:03:58,924
addestrata, quindi le attivazioni nell&#39;output sembreranno

61
00:03:58,924 --> 00:04:02,780
piuttosto casuali, forse qualcosa come 0.5, 0.8, 0.2, e così via.

62
00:04:02,780 --> 00:04:06,199
Non possiamo modificare direttamente tali attivazioni, abbiamo solo

63
00:04:06,199 --> 00:04:09,820
influenza sui pesi e sui pregiudizi, ma è utile tenere traccia di quali

64
00:04:09,820 --> 00:04:13,340
aggiustamenti desideriamo vengano apportati a quel livello di output.

65
00:04:13,340 --> 00:04:17,471
E poiché vogliamo che classifichi l&#39;immagine come 2, vogliamo che il terzo valore

66
00:04:17,471 --> 00:04:21,700
venga spostato verso l&#39;alto mentre tutti gli altri vengano spostati verso il basso.

67
00:04:21,700 --> 00:04:26,180
Inoltre, le dimensioni di questi nudge dovrebbero essere proporzionali

68
00:04:26,180 --> 00:04:30,220
alla distanza di ciascun valore corrente dal suo valore target.

69
00:04:30,220 --> 00:04:34,221
Ad esempio, l&#39;aumento dell&#39;attivazione del neurone numero 2 è in

70
00:04:34,221 --> 00:04:38,222
un certo senso più importante della diminuzione dell&#39;attivazione del

71
00:04:38,222 --> 00:04:42,060
neurone numero 8, che è già abbastanza vicino a dove dovrebbe essere.

72
00:04:42,060 --> 00:04:45,002
Quindi, ingrandendo ulteriormente, concentriamoci solo su questo

73
00:04:45,002 --> 00:04:47,900
neurone, quello di cui desideriamo aumentare l&#39;attivazione.

74
00:04:47,900 --> 00:04:52,524
Ricorda, che l&#39;attivazione è definita come una certa somma ponderata

75
00:04:52,524 --> 00:04:57,022
di tutte le attivazioni nel livello precedente, più un bias, che viene

76
00:04:57,022 --> 00:05:01,900
poi collegato a qualcosa come la funzione di schiacciamento sigmoide o ReLU.

77
00:05:01,900 --> 00:05:04,598
Quindi ci sono tre diverse strade che possono

78
00:05:04,598 --> 00:05:08,060
collaborare per contribuire ad aumentare tale attivazione.

79
00:05:08,060 --> 00:05:11,680
È possibile aumentare il bias, aumentare i pesi e

80
00:05:11,680 --> 00:05:15,300
modificare le attivazioni dal livello precedente.

81
00:05:15,300 --> 00:05:18,403
Concentrandosi su come dovrebbero essere adeguati i pesi, si noti

82
00:05:18,403 --> 00:05:21,460
come i pesi abbiano effettivamente diversi livelli di influenza.

83
00:05:21,460 --> 00:05:26,469
Le connessioni con i neuroni più luminosi dello strato precedente hanno l&#39;effetto

84
00:05:26,469 --> 00:05:31,420
maggiore poiché questi pesi vengono moltiplicati per valori di attivazione maggiori.

85
00:05:31,420 --> 00:05:35,522
Quindi, se dovessi aumentare uno di questi pesi, in realtà avrebbe un&#39;influenza

86
00:05:35,522 --> 00:05:39,429
maggiore sulla funzione di costo finale rispetto all&#39;aumento dei pesi delle

87
00:05:39,429 --> 00:05:43,385
connessioni con neuroni più deboli, almeno per quanto riguarda questo esempio di

88
00:05:43,385 --> 00:05:44,020
allenamento.

89
00:05:44,020 --> 00:05:47,353
Ricorda, quando parliamo di discesa del gradiente, non ci interessa solo

90
00:05:47,353 --> 00:05:50,641
se ciascun componente debba essere spostato verso l&#39;alto o verso il

91
00:05:50,641 --> 00:05:54,020
basso, ci interessa anche quale ti dà il massimo rapporto qualità-prezzo.

92
00:05:54,020 --> 00:05:58,611
Questo, tra l&#39;altro, ricorda almeno in qualche modo una teoria delle neuroscienze

93
00:05:58,611 --> 00:06:02,722
su come le reti biologiche di neuroni apprendono, la teoria hebbiana, spesso

94
00:06:02,722 --> 00:06:06,940
riassunta nella frase, i neuroni che si attivano insieme si collegano insieme.

95
00:06:06,940 --> 00:06:12,654
Qui i maggiori aumenti di peso, il maggiore rafforzamento delle connessioni, avviene

96
00:06:12,654 --> 00:06:18,100
tra i neuroni che sono più attivi e quelli che desideriamo diventino più attivi.

97
00:06:18,100 --> 00:06:21,689
In un certo senso, i neuroni che si attivano mentre vedono un 2 si

98
00:06:21,689 --> 00:06:25,440
collegano più fortemente a quelli che si attivano quando ci si pensa.

99
00:06:25,440 --> 00:06:28,695
Per essere chiari, non sono nella posizione di fare affermazioni in un modo o

100
00:06:28,695 --> 00:06:31,909
nell&#39;altro sul fatto che le reti artificiali di neuroni si comportino in

101
00:06:31,909 --> 00:06:35,206
qualche modo come i cervelli biologici, e questa idea di &quot;fuochi insieme,

102
00:06:35,206 --> 00:06:38,629
collegamenti insieme&quot; viene fornita con un paio di asterischi significativi,

103
00:06:38,629 --> 00:06:41,760
ma presa come un&#39;idea molto vaga. analogia, trovo interessante notare.

104
00:06:41,760 --> 00:06:45,678
Comunque, il terzo modo in cui possiamo contribuire ad aumentare l&#39;attivazione

105
00:06:45,678 --> 00:06:49,360
di questo neurone è modificando tutte le attivazioni dello strato precedente.

106
00:06:49,360 --> 00:06:53,800
Vale a dire, se tutto ciò che è collegato a quel neurone della cifra 2 con un peso

107
00:06:53,800 --> 00:06:58,346
positivo diventasse più luminoso, e se tutto ciò che è connesso con un peso negativo

108
00:06:58,346 --> 00:07:02,680
diventasse più fioco, allora quel neurone della cifra 2 diventerebbe più attivo.

109
00:07:02,680 --> 00:07:06,939
E in modo simile alle variazioni di peso, otterrai il massimo dal tuo investimento

110
00:07:06,939 --> 00:07:10,840
cercando cambiamenti proporzionali alla dimensione dei pesi corrispondenti.

111
00:07:10,840 --> 00:07:14,401
Ora, ovviamente, non possiamo influenzare direttamente tali

112
00:07:14,401 --> 00:07:18,320
attivazioni, abbiamo solo il controllo sui pesi e sui pregiudizi.

113
00:07:18,320 --> 00:07:21,084
Ma proprio come con l&#39;ultimo livello, è utile

114
00:07:21,084 --> 00:07:23,960
tenere nota di quali sono i cambiamenti desiderati.

115
00:07:23,960 --> 00:07:27,075
Ma tieni presente che, rimpicciolendo di un passo qui, questo

116
00:07:27,075 --> 00:07:30,040
è solo ciò che vuole quel neurone di output della cifra 2.

117
00:07:30,040 --> 00:07:34,588
Ricorda, vogliamo anche che tutti gli altri neuroni nell&#39;ultimo strato

118
00:07:34,588 --> 00:07:39,015
diventino meno attivi e ciascuno di questi altri neuroni in uscita abbia

119
00:07:39,015 --> 00:07:43,200
i propri pensieri su cosa dovrebbe accadere a quel penultimo strato.

120
00:07:43,200 --> 00:07:47,788
Quindi il desiderio di questo neurone della cifra 2 viene sommato insieme

121
00:07:47,788 --> 00:07:52,128
ai desideri di tutti gli altri neuroni di output per ciò che dovrebbe

122
00:07:52,128 --> 00:07:57,151
accadere a questo penultimo strato, sempre in proporzione ai pesi corrispondenti

123
00:07:57,151 --> 00:08:01,740
e in proporzione a quanto ciascuno di questi neuroni ha bisogno cambiare.

124
00:08:01,740 --> 00:08:05,940
È proprio qui che entra in gioco l&#39;idea della propagazione all&#39;indietro.

125
00:08:05,940 --> 00:08:10,120
Sommando insieme tutti questi effetti desiderati, ottieni sostanzialmente un

126
00:08:10,120 --> 00:08:14,300
elenco di solleciti che vuoi che si verifichino su questo penultimo livello.

127
00:08:14,300 --> 00:08:19,132
E una volta che li hai, puoi applicare ricorsivamente lo stesso processo ai

128
00:08:19,132 --> 00:08:24,220
pesi e ai pregiudizi rilevanti che determinano quei valori, ripetendo lo stesso

129
00:08:24,220 --> 00:08:29,180
processo che ho appena seguito e andando all&#39;indietro attraverso la rete.

130
00:08:29,180 --> 00:08:33,482
E zoomando ancora un po’, ricorda che questo è proprio il modo in cui un singolo

131
00:08:33,482 --> 00:08:37,520
esempio di formazione desidera spingere ciascuno di quei pesi e pregiudizi.

132
00:08:37,520 --> 00:08:40,779
Se ascoltassimo solo ciò che vogliono quei 2, la rete alla fine

133
00:08:40,779 --> 00:08:44,140
sarebbe incentivata solo a classificare tutte le immagini come 2.

134
00:08:44,140 --> 00:08:49,896
Quindi quello che fai è seguire la stessa routine di backprop per ogni

135
00:08:49,896 --> 00:08:55,814
altro esempio di allenamento, registrando come ciascuno di loro vorrebbe

136
00:08:55,814 --> 00:09:02,300
modificare i pesi e i bias e fare una media insieme dei cambiamenti desiderati.

137
00:09:02,300 --> 00:09:06,212
Questa raccolta qui degli scostamenti medi per ciascun peso e bias è, in

138
00:09:06,212 --> 00:09:10,125
parole povere, il gradiente negativo della funzione di costo a cui si fa

139
00:09:10,125 --> 00:09:14,360
riferimento nell&#39;ultimo video, o almeno qualcosa di proporzionale ad esso.

140
00:09:14,360 --> 00:09:18,716
Dico in termini approssimativi solo perché devo ancora ottenere una precisione

141
00:09:18,716 --> 00:09:23,568
quantitativa su questi stimoli, ma se hai capito ogni cambiamento a cui ho appena fatto

142
00:09:23,568 --> 00:09:28,200
riferimento, perché alcuni sono proporzionalmente più grandi di altri e come devono

143
00:09:28,200 --> 00:09:33,162
essere sommati tutti insieme, capirai i meccanismi per cosa sta effettivamente facendo la

144
00:09:33,162 --> 00:09:34,100
backpropagation.

145
00:09:34,100 --> 00:09:38,819
In pratica, però, i computer impiegano molto tempo per sommare l&#39;influenza

146
00:09:38,819 --> 00:09:43,120
di ogni esempio di allenamento e di ogni fase di discesa del gradiente.

147
00:09:43,120 --> 00:09:45,540
Quindi ecco cosa viene fatto comunemente invece.

148
00:09:45,540 --> 00:09:49,400
Mescoli casualmente i tuoi dati di allenamento e li dividi in un

149
00:09:49,400 --> 00:09:53,380
sacco di mini-lotti, diciamo ognuno con 100 esempi di allenamento.

150
00:09:53,380 --> 00:09:56,980
Quindi calcoli un passaggio in base al mini-batch.

151
00:09:56,980 --> 00:10:00,984
Non è il gradiente effettivo della funzione di costo, che dipende da tutti i dati

152
00:10:00,984 --> 00:10:04,891
di addestramento, non da questo piccolo sottoinsieme, quindi non è il passo più

153
00:10:04,891 --> 00:10:09,042
efficiente in discesa, ma ogni mini-batch fornisce un&#39;approssimazione abbastanza

154
00:10:09,042 --> 00:10:12,900
buona e, cosa più importante, ti dà una notevole accelerazione computazionale.

155
00:10:12,900 --> 00:10:16,664
Se dovessi tracciare la traiettoria della tua rete sotto la superficie di

156
00:10:16,664 --> 00:10:20,428
costo rilevante, sarebbe un po’ più simile a un uomo ubriaco che inciampa

157
00:10:20,428 --> 00:10:24,193
senza meta giù da una collina ma fa passi rapidi, piuttosto che a un uomo

158
00:10:24,193 --> 00:10:28,008
che calcola attentamente e determina l’esatta direzione in discesa di ogni

159
00:10:28,008 --> 00:10:31,620
passo. prima di fare un passo molto lento e cauto in quella direzione.

160
00:10:31,620 --> 00:10:35,200
Questa tecnica è detta discesa del gradiente stocastico.

161
00:10:35,200 --> 00:10:40,400
C&#39;è molto da fare qui, quindi riassumiamolo per noi stessi, va bene?

162
00:10:40,400 --> 00:10:44,479
La backpropagation è l&#39;algoritmo per determinare come un singolo esempio

163
00:10:44,479 --> 00:10:48,346
di training vorrebbe spostare i pesi e i bias, non solo in termini di se

164
00:10:48,346 --> 00:10:52,478
dovrebbero aumentare o diminuire, ma in termini di quali proporzioni relative

165
00:10:52,478 --> 00:10:56,240
a tali cambiamenti causano la diminuzione più rapida del valore costo.

166
00:10:56,240 --> 00:10:59,706
Un vero passaggio di discesa del gradiente implicherebbe eseguire questa

167
00:10:59,706 --> 00:11:03,078
operazione per tutte le decine e migliaia di esempi di addestramento e

168
00:11:03,078 --> 00:11:06,544
calcolare la media delle modifiche desiderate ottenute, ma è un processo

169
00:11:06,544 --> 00:11:09,868
lento dal punto di vista computazionale, quindi invece si suddividono

170
00:11:09,868 --> 00:11:14,000
casualmente i dati in mini-batch e si calcola ogni passaggio rispetto a un mini-lotto.

171
00:11:14,000 --> 00:11:18,533
Esaminando ripetutamente tutti i mini-batch e apportando queste modifiche,

172
00:11:18,533 --> 00:11:22,885
convergerai verso un minimo locale della funzione di costo, vale a dire

173
00:11:22,885 --> 00:11:27,540
che la tua rete finirà per fare un ottimo lavoro sugli esempi di formazione.

174
00:11:27,540 --> 00:11:32,580
Quindi, detto tutto ciò, ogni riga di codice utilizzata per implementare il backprop

175
00:11:32,580 --> 00:11:37,680
corrisponde effettivamente a qualcosa che hai visto ora, almeno in termini informali.

176
00:11:37,680 --> 00:11:41,277
Ma a volte sapere cosa fa la matematica è solo metà della battaglia, e solo

177
00:11:41,277 --> 00:11:44,780
rappresentare quella dannata cosa è dove tutto diventa confuso e confuso.

178
00:11:44,780 --> 00:11:48,736
Quindi, per quelli di voi che vogliono andare più in profondità, il prossimo video

179
00:11:48,736 --> 00:11:52,979
analizza le stesse idee appena presentate qui, ma in termini di calcolo sottostante, che

180
00:11:52,979 --> 00:11:57,030
si spera dovrebbe renderlo un po&#39; più familiare vedendo l&#39;argomento in altre

181
00:11:57,030 --> 00:11:57,460
risorse.

182
00:11:57,460 --> 00:12:00,545
Prima di ciò, una cosa che vale la pena sottolineare è che affinché questo

183
00:12:00,545 --> 00:12:03,795
algoritmo funzioni, e questo vale per tutti i tipi di apprendimento automatico

184
00:12:03,795 --> 00:12:06,840
oltre alle sole reti neurali, sono necessari molti dati di addestramento.

185
00:12:06,840 --> 00:12:09,624
Nel nostro caso, una cosa che rende le cifre scritte a mano

186
00:12:09,624 --> 00:12:12,363
un esempio così carino è che esiste il database MNIST, con

187
00:12:12,363 --> 00:12:15,380
così tanti esempi che sono stati etichettati dagli esseri umani.

188
00:12:15,380 --> 00:12:18,236
Quindi una sfida comune con cui quelli di voi che lavorano nell&#39;apprendimento

189
00:12:18,236 --> 00:12:21,024
automatico avranno familiarità è semplicemente ottenere i dati di addestramento

190
00:12:21,024 --> 00:12:24,020
etichettati di cui avete effettivamente bisogno, sia che si tratti di far etichettare

191
00:12:24,020 --> 00:12:26,981
decine di migliaia di immagini o qualsiasi altro tipo di dati con cui potreste avere

192
00:12:26,981 --> 00:12:27,400
a che fare.

