1
00:00:00,000 --> 00:00:05,113
Qui affrontiamo la backpropagation, l’algoritmo fondamentale

2
00:00:05,113 --> 00:00:09,640
alla base del modo in cui le reti neurali apprendono.

3
00:00:09,640 --> 00:00:11,703
Dopo un breve riepilogo della situazione attuale,

4
00:00:11,703 --> 00:00:14,180
la prima cosa che farò sarà una guida intuitiva su cosa sta

5
00:00:14,180 --> 00:00:17,400
effettivamente facendo l&#39;algoritmo, senza alcun riferimento alle formule.

6
00:00:17,400 --> 00:00:20,670
Quindi, per quelli di voi che vogliono tuffarsi nella matematica,

7
00:00:20,670 --> 00:00:24,040
il prossimo video approfondirà i calcoli alla base di tutto questo.

8
00:00:24,040 --> 00:00:27,600
Se hai guardato gli ultimi due video o se stai semplicemente entrando nel merito con il

9
00:00:27,600 --> 00:00:31,080
background appropriato, sai cos&#39;è una rete neurale e come trasmette informazioni.

10
00:00:31,080 --> 00:00:35,633
Qui stiamo facendo il classico esempio di riconoscimento di cifre scritte a mano

11
00:00:35,633 --> 00:00:40,300
i cui valori di pixel vengono immessi nel primo strato della rete con 784 neuroni,

12
00:00:40,300 --> 00:00:44,853
e ho mostrato una rete con due strati nascosti con solo 16 neuroni ciascuno e un

13
00:00:44,853 --> 00:00:49,520
output strato di 10 neuroni, che indica quale cifra la rete sceglie come risposta.

14
00:00:49,520 --> 00:00:52,699
Mi aspetto anche che tu comprenda la discesa del gradiente,

15
00:00:52,699 --> 00:00:57,045
come descritta nell&#39;ultimo video, e come ciò che intendiamo per apprendimento

16
00:00:57,045 --> 00:01:01,073
è che vogliamo scoprire quali pesi e pregiudizi minimizzano una determinata

17
00:01:01,073 --> 00:01:02,080
funzione di costo.

18
00:01:02,080 --> 00:01:06,573
Come rapido promemoria, per il costo di un singolo esempio di formazione,

19
00:01:06,573 --> 00:01:11,916
prendi l&#39;output fornito dalla rete, insieme all&#39;output che volevi che fornisse,

20
00:01:11,916 --> 00:01:15,560
e somma i quadrati delle differenze tra ciascun componente.

21
00:01:15,560 --> 00:01:19,274
Facendo questo per tutte le decine di migliaia di esempi di formazione e

22
00:01:19,274 --> 00:01:23,040
calcolando la media dei risultati, si ottiene il costo totale della rete.

23
00:01:23,040 --> 00:01:27,670
Come se ciò non bastasse, come descritto nell&#39;ultimo video,

24
00:01:27,670 --> 00:01:33,530
la cosa che stiamo cercando è il gradiente negativo di questa funzione di costo,

25
00:01:33,530 --> 00:01:39,317
che ti dice come devi cambiare tutti i pesi e i bias, tutti queste connessioni,

26
00:01:39,317 --> 00:01:43,080
in modo da ridurre nel modo più efficiente i costi.

27
00:01:43,080 --> 00:01:46,105
La propagazione inversa, l&#39;argomento di questo video,

28
00:01:46,105 --> 00:01:49,600
è un algoritmo per calcolare quel gradiente follemente complicato.

29
00:01:49,600 --> 00:01:53,294
L&#39;idea dell&#39;ultimo video che voglio davvero che tu tenga saldamente

30
00:01:53,294 --> 00:01:56,939
in mente in questo momento è che, poiché pensare al vettore gradiente come

31
00:01:56,939 --> 00:01:59,953
una direzione in 13.000 dimensioni è, per dirla alla leggera,

32
00:01:59,953 --> 00:02:03,696
oltre la portata della nostra immaginazione, ce n&#39;è un&#39;altra modo in

33
00:02:03,696 --> 00:02:04,620
cui puoi pensarci.

34
00:02:04,620 --> 00:02:08,252
L&#39;entità di ciascun componente qui indica quanto la

35
00:02:08,252 --> 00:02:11,820
funzione di costo sia sensibile a ciascun peso e bias.

36
00:02:11,820 --> 00:02:15,971
Per esempio, diciamo che segui il processo che sto per descrivere,

37
00:02:15,971 --> 00:02:21,115
e calcoli il gradiente negativo, e il componente associato al peso su questo bordo

38
00:02:21,115 --> 00:02:26,196
qui risulta essere 3.2, mentre la componente associata a questo bordo qui risulta

39
00:02:26,196 --> 00:02:26,940
essere 0.1.

40
00:02:26,940 --> 00:02:31,629
Il modo in cui lo interpreteresti è che il costo della funzione è 32 volte più

41
00:02:31,629 --> 00:02:36,319
sensibile ai cambiamenti nel primo peso, quindi se dovessi spostare un po&#39;

42
00:02:36,319 --> 00:02:39,584
quel valore, causerebbe qualche cambiamento nel costo,

43
00:02:39,584 --> 00:02:44,333
e quel cambiamento è 32 volte maggiore di quanto darebbe la stessa oscillazione

44
00:02:44,333 --> 00:02:45,580
a quel secondo peso.

45
00:02:45,580 --> 00:02:49,802
Personalmente, quando ho appreso per la prima volta della propagazione inversa,

46
00:02:49,802 --> 00:02:53,128
penso che l&#39;aspetto più confuso fosse proprio la notazione

47
00:02:53,128 --> 00:02:55,820
e l&#39;inseguimento dell&#39;indice di tutto ciò.

48
00:02:55,820 --> 00:02:59,841
Ma una volta che scopri cosa sta realmente facendo ogni parte di questo algoritmo,

49
00:02:59,841 --> 00:03:03,185
ogni singolo effetto che sta avendo è in realtà piuttosto intuitivo,

50
00:03:03,185 --> 00:03:06,964
è solo che ci sono molti piccoli aggiustamenti che si sovrappongono l&#39;uno

51
00:03:06,964 --> 00:03:07,740
sull&#39;altro.

52
00:03:07,740 --> 00:03:12,589
Quindi inizierò qui ignorando completamente la notazione e passerò semplicemente

53
00:03:12,589 --> 00:03:17,380
in rassegna gli effetti che ogni esempio di allenamento ha sui pesi e sui bias.

54
00:03:17,380 --> 00:03:22,112
Poiché la funzione di costo implica la media di un certo costo per esempio su tutte le

55
00:03:22,112 --> 00:03:24,668
decine di migliaia di esempi di addestramento,

56
00:03:24,668 --> 00:03:29,074
il modo in cui regoliamo i pesi e i bias per un singolo passaggio di discesa del

57
00:03:29,074 --> 00:03:31,740
gradiente dipende anche da ogni singolo esempio.

58
00:03:31,740 --> 00:03:34,373
O meglio, in linea di principio dovrebbe, ma per efficienza

59
00:03:34,373 --> 00:03:36,963
computazionale faremo un piccolo trucchetto più avanti per

60
00:03:36,963 --> 00:03:39,860
evitare di dover colpire ogni singolo esempio per ogni passaggio.

61
00:03:39,860 --> 00:03:43,211
In altri casi, per ora, tutto ciò che faremo è concentrare la

62
00:03:43,211 --> 00:03:46,780
nostra attenzione su un singolo esempio, questa immagine di un 2.

63
00:03:46,780 --> 00:03:49,260
Che effetto dovrebbe avere questo esempio di formazione

64
00:03:49,260 --> 00:03:51,740
sul modo in cui i pesi e i pregiudizi vengono adeguati?

65
00:03:51,740 --> 00:03:56,004
Diciamo che siamo a un punto in cui la rete non è ancora ben addestrata,

66
00:03:56,004 --> 00:04:00,034
quindi le attivazioni nell&#39;output sembreranno piuttosto casuali,

67
00:04:00,034 --> 00:04:02,780
forse qualcosa come 0.5, 0.8, 0.2, e così via.

68
00:04:02,780 --> 00:04:05,545
Non possiamo modificare direttamente tali attivazioni,

69
00:04:05,545 --> 00:04:08,060
abbiamo solo influenza sui pesi e sui pregiudizi,

70
00:04:08,060 --> 00:04:11,529
ma è utile tenere traccia di quali aggiustamenti desideriamo vengano

71
00:04:11,529 --> 00:04:13,340
apportati a quel livello di output.

72
00:04:13,340 --> 00:04:16,078
E poiché vogliamo che classifichi l&#39;immagine come 2,

73
00:04:16,078 --> 00:04:20,114
vogliamo che il terzo valore venga spostato verso l&#39;alto mentre tutti gli altri

74
00:04:20,114 --> 00:04:21,700
vengano spostati verso il basso.

75
00:04:21,700 --> 00:04:26,180
Inoltre, le dimensioni di questi nudge dovrebbero essere proporzionali

76
00:04:26,180 --> 00:04:30,220
alla distanza di ciascun valore corrente dal suo valore target.

77
00:04:30,220 --> 00:04:34,221
Ad esempio, l&#39;aumento dell&#39;attivazione del neurone numero 2 è in

78
00:04:34,221 --> 00:04:38,222
un certo senso più importante della diminuzione dell&#39;attivazione del

79
00:04:38,222 --> 00:04:42,060
neurone numero 8, che è già abbastanza vicino a dove dovrebbe essere.

80
00:04:42,060 --> 00:04:45,410
Quindi, ingrandendo ulteriormente, concentriamoci solo su questo neurone,

81
00:04:45,410 --> 00:04:47,900
quello di cui desideriamo aumentare l&#39;attivazione.

82
00:04:47,900 --> 00:04:52,524
Ricorda, che l&#39;attivazione è definita come una certa somma ponderata

83
00:04:52,524 --> 00:04:56,388
di tutte le attivazioni nel livello precedente, più un bias,

84
00:04:56,388 --> 00:05:01,900
che viene poi collegato a qualcosa come la funzione di schiacciamento sigmoide o ReLU.

85
00:05:01,900 --> 00:05:04,598
Quindi ci sono tre diverse strade che possono

86
00:05:04,598 --> 00:05:08,060
collaborare per contribuire ad aumentare tale attivazione.

87
00:05:08,060 --> 00:05:11,680
È possibile aumentare il bias, aumentare i pesi e

88
00:05:11,680 --> 00:05:15,300
modificare le attivazioni dal livello precedente.

89
00:05:15,300 --> 00:05:18,027
Concentrandosi su come dovrebbero essere adeguati i pesi,

90
00:05:18,027 --> 00:05:21,460
si noti come i pesi abbiano effettivamente diversi livelli di influenza.

91
00:05:21,460 --> 00:05:26,469
Le connessioni con i neuroni più luminosi dello strato precedente hanno l&#39;effetto

92
00:05:26,469 --> 00:05:31,420
maggiore poiché questi pesi vengono moltiplicati per valori di attivazione maggiori.

93
00:05:31,420 --> 00:05:33,813
Quindi, se dovessi aumentare uno di questi pesi,

94
00:05:33,813 --> 00:05:37,915
in realtà avrebbe un&#39;influenza maggiore sulla funzione di costo finale rispetto

95
00:05:37,915 --> 00:05:41,187
all&#39;aumento dei pesi delle connessioni con neuroni più deboli,

96
00:05:41,187 --> 00:05:44,020
almeno per quanto riguarda questo esempio di allenamento.

97
00:05:44,020 --> 00:05:46,348
Ricorda, quando parliamo di discesa del gradiente,

98
00:05:46,348 --> 00:05:49,636
non ci interessa solo se ciascun componente debba essere spostato verso

99
00:05:49,636 --> 00:05:52,878
l&#39;alto o verso il basso, ci interessa anche quale ti dà il massimo

100
00:05:52,878 --> 00:05:54,020
rapporto qualità-prezzo.

101
00:05:54,020 --> 00:05:58,611
Questo, tra l&#39;altro, ricorda almeno in qualche modo una teoria delle neuroscienze

102
00:05:58,611 --> 00:06:02,348
su come le reti biologiche di neuroni apprendono, la teoria hebbiana,

103
00:06:02,348 --> 00:06:06,940
spesso riassunta nella frase, i neuroni che si attivano insieme si collegano insieme.

104
00:06:06,940 --> 00:06:12,116
Qui i maggiori aumenti di peso, il maggiore rafforzamento delle connessioni,

105
00:06:12,116 --> 00:06:18,100
avviene tra i neuroni che sono più attivi e quelli che desideriamo diventino più attivi.

106
00:06:18,100 --> 00:06:21,689
In un certo senso, i neuroni che si attivano mentre vedono un 2 si

107
00:06:21,689 --> 00:06:25,440
collegano più fortemente a quelli che si attivano quando ci si pensa.

108
00:06:25,440 --> 00:06:28,695
Per essere chiari, non sono nella posizione di fare affermazioni in un modo o

109
00:06:28,695 --> 00:06:31,909
nell&#39;altro sul fatto che le reti artificiali di neuroni si comportino in

110
00:06:31,909 --> 00:06:35,206
qualche modo come i cervelli biologici, e questa idea di &quot;fuochi insieme,

111
00:06:35,206 --> 00:06:38,629
collegamenti insieme&quot; viene fornita con un paio di asterischi significativi,

112
00:06:38,629 --> 00:06:41,760
ma presa come un&#39;idea molto vaga. analogia, trovo interessante notare.

113
00:06:41,760 --> 00:06:45,678
Comunque, il terzo modo in cui possiamo contribuire ad aumentare l&#39;attivazione

114
00:06:45,678 --> 00:06:49,360
di questo neurone è modificando tutte le attivazioni dello strato precedente.

115
00:06:49,360 --> 00:06:53,800
Vale a dire, se tutto ciò che è collegato a quel neurone della cifra 2 con un peso

116
00:06:53,800 --> 00:06:58,346
positivo diventasse più luminoso, e se tutto ciò che è connesso con un peso negativo

117
00:06:58,346 --> 00:07:02,680
diventasse più fioco, allora quel neurone della cifra 2 diventerebbe più attivo.

118
00:07:02,680 --> 00:07:06,939
E in modo simile alle variazioni di peso, otterrai il massimo dal tuo investimento

119
00:07:06,939 --> 00:07:10,840
cercando cambiamenti proporzionali alla dimensione dei pesi corrispondenti.

120
00:07:10,840 --> 00:07:15,173
Ora, ovviamente, non possiamo influenzare direttamente tali attivazioni,

121
00:07:15,173 --> 00:07:18,320
abbiamo solo il controllo sui pesi e sui pregiudizi.

122
00:07:18,320 --> 00:07:21,084
Ma proprio come con l&#39;ultimo livello, è utile

123
00:07:21,084 --> 00:07:23,960
tenere nota di quali sono i cambiamenti desiderati.

124
00:07:23,960 --> 00:07:26,723
Ma tieni presente che, rimpicciolendo di un passo qui,

125
00:07:26,723 --> 00:07:30,040
questo è solo ciò che vuole quel neurone di output della cifra 2.

126
00:07:30,040 --> 00:07:34,588
Ricorda, vogliamo anche che tutti gli altri neuroni nell&#39;ultimo strato

127
00:07:34,588 --> 00:07:39,015
diventino meno attivi e ciascuno di questi altri neuroni in uscita abbia

128
00:07:39,015 --> 00:07:43,200
i propri pensieri su cosa dovrebbe accadere a quel penultimo strato.

129
00:07:43,200 --> 00:07:47,788
Quindi il desiderio di questo neurone della cifra 2 viene sommato insieme

130
00:07:47,788 --> 00:07:52,128
ai desideri di tutti gli altri neuroni di output per ciò che dovrebbe

131
00:07:52,128 --> 00:07:57,151
accadere a questo penultimo strato, sempre in proporzione ai pesi corrispondenti

132
00:07:57,151 --> 00:08:01,740
e in proporzione a quanto ciascuno di questi neuroni ha bisogno cambiare.

133
00:08:01,740 --> 00:08:05,940
È proprio qui che entra in gioco l&#39;idea della propagazione all&#39;indietro.

134
00:08:05,940 --> 00:08:08,654
Sommando insieme tutti questi effetti desiderati,

135
00:08:08,654 --> 00:08:12,888
ottieni sostanzialmente un elenco di solleciti che vuoi che si verifichino su

136
00:08:12,888 --> 00:08:14,300
questo penultimo livello.

137
00:08:14,300 --> 00:08:19,132
E una volta che li hai, puoi applicare ricorsivamente lo stesso processo ai

138
00:08:19,132 --> 00:08:22,948
pesi e ai pregiudizi rilevanti che determinano quei valori,

139
00:08:22,948 --> 00:08:27,908
ripetendo lo stesso processo che ho appena seguito e andando all&#39;indietro

140
00:08:27,908 --> 00:08:29,180
attraverso la rete.

141
00:08:29,180 --> 00:08:33,482
E zoomando ancora un po’, ricorda che questo è proprio il modo in cui un singolo

142
00:08:33,482 --> 00:08:37,520
esempio di formazione desidera spingere ciascuno di quei pesi e pregiudizi.

143
00:08:37,520 --> 00:08:40,779
Se ascoltassimo solo ciò che vogliono quei 2, la rete alla fine

144
00:08:40,779 --> 00:08:44,140
sarebbe incentivata solo a classificare tutte le immagini come 2.

145
00:08:44,140 --> 00:08:49,896
Quindi quello che fai è seguire la stessa routine di backprop per ogni

146
00:08:49,896 --> 00:08:55,814
altro esempio di allenamento, registrando come ciascuno di loro vorrebbe

147
00:08:55,814 --> 00:09:02,300
modificare i pesi e i bias e fare una media insieme dei cambiamenti desiderati.

148
00:09:02,300 --> 00:09:06,052
Questa raccolta qui degli scostamenti medi per ciascun peso e bias è,

149
00:09:06,052 --> 00:09:10,125
in parole povere, il gradiente negativo della funzione di costo a cui si fa

150
00:09:10,125 --> 00:09:14,360
riferimento nell&#39;ultimo video, o almeno qualcosa di proporzionale ad esso.

151
00:09:14,360 --> 00:09:18,716
Dico in termini approssimativi solo perché devo ancora ottenere una precisione

152
00:09:18,716 --> 00:09:23,568
quantitativa su questi stimoli, ma se hai capito ogni cambiamento a cui ho appena fatto

153
00:09:23,568 --> 00:09:28,200
riferimento, perché alcuni sono proporzionalmente più grandi di altri e come devono

154
00:09:28,200 --> 00:09:33,162
essere sommati tutti insieme, capirai i meccanismi per cosa sta effettivamente facendo la

155
00:09:33,162 --> 00:09:34,100
backpropagation.

156
00:09:34,100 --> 00:09:38,819
In pratica, però, i computer impiegano molto tempo per sommare l&#39;influenza

157
00:09:38,819 --> 00:09:43,120
di ogni esempio di allenamento e di ogni fase di discesa del gradiente.

158
00:09:43,120 --> 00:09:45,540
Quindi ecco cosa viene fatto comunemente invece.

159
00:09:45,540 --> 00:09:50,647
Mescoli casualmente i tuoi dati di allenamento e li dividi in un sacco di mini-lotti,

160
00:09:50,647 --> 00:09:53,380
diciamo ognuno con 100 esempi di allenamento.

161
00:09:53,380 --> 00:09:56,980
Quindi calcoli un passaggio in base al mini-batch.

162
00:09:56,980 --> 00:09:59,617
Non è il gradiente effettivo della funzione di costo,

163
00:09:59,617 --> 00:10:03,621
che dipende da tutti i dati di addestramento, non da questo piccolo sottoinsieme,

164
00:10:03,621 --> 00:10:06,014
quindi non è il passo più efficiente in discesa,

165
00:10:06,014 --> 00:10:09,481
ma ogni mini-batch fornisce un&#39;approssimazione abbastanza buona e,

166
00:10:09,481 --> 00:10:12,900
cosa più importante, ti dà una notevole accelerazione computazionale.

167
00:10:12,900 --> 00:10:16,664
Se dovessi tracciare la traiettoria della tua rete sotto la superficie di

168
00:10:16,664 --> 00:10:20,428
costo rilevante, sarebbe un po’ più simile a un uomo ubriaco che inciampa

169
00:10:20,428 --> 00:10:22,972
senza meta giù da una collina ma fa passi rapidi,

170
00:10:22,972 --> 00:10:26,533
piuttosto che a un uomo che calcola attentamente e determina l’esatta

171
00:10:26,533 --> 00:10:30,246
direzione in discesa di ogni passo. prima di fare un passo molto lento e

172
00:10:30,246 --> 00:10:31,620
cauto in quella direzione.

173
00:10:31,620 --> 00:10:35,200
Questa tecnica è detta discesa del gradiente stocastico.

174
00:10:35,200 --> 00:10:40,400
C&#39;è molto da fare qui, quindi riassumiamolo per noi stessi, va bene?

175
00:10:40,400 --> 00:10:44,479
La backpropagation è l&#39;algoritmo per determinare come un singolo esempio

176
00:10:44,479 --> 00:10:46,969
di training vorrebbe spostare i pesi e i bias,

177
00:10:46,969 --> 00:10:50,147
non solo in termini di se dovrebbero aumentare o diminuire,

178
00:10:50,147 --> 00:10:54,067
ma in termini di quali proporzioni relative a tali cambiamenti causano la

179
00:10:54,067 --> 00:10:56,240
diminuzione più rapida del valore costo.

180
00:10:56,240 --> 00:10:59,706
Un vero passaggio di discesa del gradiente implicherebbe eseguire questa

181
00:10:59,706 --> 00:11:03,078
operazione per tutte le decine e migliaia di esempi di addestramento e

182
00:11:03,078 --> 00:11:05,737
calcolare la media delle modifiche desiderate ottenute,

183
00:11:05,737 --> 00:11:08,491
ma è un processo lento dal punto di vista computazionale,

184
00:11:08,491 --> 00:11:12,053
quindi invece si suddividono casualmente i dati in mini-batch e si calcola

185
00:11:12,053 --> 00:11:14,000
ogni passaggio rispetto a un mini-lotto.

186
00:11:14,000 --> 00:11:18,533
Esaminando ripetutamente tutti i mini-batch e apportando queste modifiche,

187
00:11:18,533 --> 00:11:22,160
convergerai verso un minimo locale della funzione di costo,

188
00:11:22,160 --> 00:11:27,540
vale a dire che la tua rete finirà per fare un ottimo lavoro sugli esempi di formazione.

189
00:11:27,540 --> 00:11:32,580
Quindi, detto tutto ciò, ogni riga di codice utilizzata per implementare il backprop

190
00:11:32,580 --> 00:11:37,680
corrisponde effettivamente a qualcosa che hai visto ora, almeno in termini informali.

191
00:11:37,680 --> 00:11:40,946
Ma a volte sapere cosa fa la matematica è solo metà della battaglia,

192
00:11:40,946 --> 00:11:44,780
e solo rappresentare quella dannata cosa è dove tutto diventa confuso e confuso.

193
00:11:44,780 --> 00:11:47,878
Quindi, per quelli di voi che vogliono andare più in profondità,

194
00:11:47,878 --> 00:11:50,976
il prossimo video analizza le stesse idee appena presentate qui,

195
00:11:50,976 --> 00:11:54,790
ma in termini di calcolo sottostante, che si spera dovrebbe renderlo un po&#39;

196
00:11:54,790 --> 00:11:57,460
più familiare vedendo l&#39;argomento in altre risorse.

197
00:11:57,460 --> 00:12:00,545
Prima di ciò, una cosa che vale la pena sottolineare è che affinché questo

198
00:12:00,545 --> 00:12:03,795
algoritmo funzioni, e questo vale per tutti i tipi di apprendimento automatico

199
00:12:03,795 --> 00:12:06,840
oltre alle sole reti neurali, sono necessari molti dati di addestramento.

200
00:12:06,840 --> 00:12:09,624
Nel nostro caso, una cosa che rende le cifre scritte a mano

201
00:12:09,624 --> 00:12:12,177
un esempio così carino è che esiste il database MNIST,

202
00:12:12,177 --> 00:12:15,380
con così tanti esempi che sono stati etichettati dagli esseri umani.

203
00:12:15,380 --> 00:12:18,236
Quindi una sfida comune con cui quelli di voi che lavorano nell&#39;apprendimento

204
00:12:18,236 --> 00:12:21,024
automatico avranno familiarità è semplicemente ottenere i dati di addestramento

205
00:12:21,024 --> 00:12:22,731
etichettati di cui avete effettivamente bisogno,

206
00:12:22,731 --> 00:12:25,727
sia che si tratti di far etichettare decine di migliaia di immagini o qualsiasi altro

207
00:12:25,727 --> 00:12:27,400
tipo di dati con cui potreste avere a che fare.

