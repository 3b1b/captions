1
00:00:00,000 --> 00:00:08,420
สมมติฐานที่ยากที่นี่คือคุณได้ดูตอนที่ 3

2
00:00:08,420 --> 00:00:11,160
แล้ว ซึ่งให้คำแนะนำแบบเข้าใจง่ายของอัลกอริธึมการเผยแพร่กลับ

3
00:00:11,160 --> 00:00:14,920
ต่อไปนี้เราจะดูเป็นทางการมากขึ้นอีกหน่อยและเจาะลึกแคลคูลัสที่เกี่ยวข้อง

4
00:00:14,920 --> 00:00:18,560
เป็นเรื่องปกติที่เรื่องนี้จะทำให้สับสนเล็กน้อย ดังนั้นบทสวดให้หยุดและไตร่ตรองเป็นประจำจึงใช้ได้กับที่นี่ไม่แพ้ที่อื่นๆ

5
00:00:18,560 --> 00:00:22,000
อย่างแน่นอน

6
00:00:22,000 --> 00:00:26,620
เป้าหมายหลักของเราคือการแสดงให้เห็นว่าผู้คนในแมชชีนเลิร์นนิงคิดอย่างไรเกี่ยวกับกฎลูกโซ่จากแคลคูลัสในบริบทของเครือข่าย

7
00:00:26,620 --> 00:00:31,900
ซึ่งมีความรู้สึกที่แตกต่างจากวิธีที่หลักสูตรแคลคูลัสเบื้องต้นเข้าใกล้วิชานี้อย่างไร

8
00:00:31,900 --> 00:00:34,580


9
00:00:34,580 --> 00:00:38,300
สำหรับผู้ที่ไม่สบายใจกับแคลคูลัสที่เกี่ยวข้อง

10
00:00:38,300 --> 00:00:39,300
ฉันก็มีบทความทั้งชุดในหัวข้อนี้

11
00:00:39,300 --> 00:00:44,840
เรามาเริ่มกันที่เครือข่ายที่เรียบง่ายที่สุด

12
00:00:44,840 --> 00:00:46,780
โดยแต่ละเลเยอร์จะมีเซลล์ประสาทเพียงตัวเดียวอยู่ในนั้น

13
00:00:46,780 --> 00:00:51,880
เครือข่ายนี้ถูกกำหนดโดยน้ำหนักสามค่าและอคติสามค่า

14
00:00:51,880 --> 00:00:55,640
และเป้าหมายของเราคือการทำความเข้าใจว่าฟังก์ชันต้นทุนมีความอ่อนไหวต่อตัวแปรเหล่านี้อย่างไร

15
00:00:55,640 --> 00:00:59,780
ด้วยวิธีนี้เราจะรู้ว่าการปรับเปลี่ยนเงื่อนไขใดจะทำให้ฟังก์ชันต้นทุนลดลงอย่างมีประสิทธิภาพมากที่สุด

16
00:00:59,780 --> 00:01:01,100


17
00:01:01,100 --> 00:01:05,360
เราจะเน้นไปที่การเชื่อมโยงระหว่างเซลล์ประสาทสองอันสุดท้าย

18
00:01:05,360 --> 00:01:10,400
เรามาติดป้ายกำกับการเปิดใช้งานของเซลล์ประสาทสุดท้ายนั้นด้วยตัวยก L

19
00:01:10,400 --> 00:01:11,800
เพื่อระบุว่าเซลล์ประสาทนั้นอยู่ในเลเยอร์ใด

20
00:01:11,800 --> 00:01:16,560
ดังนั้นการกระตุ้นของเซลล์ประสาทก่อนหน้าคือ AL-1

21
00:01:16,560 --> 00:01:20,120
สิ่งเหล่านี้ไม่ใช่เลขชี้กำลัง แต่เป็นเพียงวิธีการจัดทำดัชนีสิ่งที่เรากำลังพูดถึง

22
00:01:20,120 --> 00:01:23,120
เนื่องจากฉันต้องการบันทึกตัวห้อยสำหรับดัชนีต่างๆ ในภายหลัง

23
00:01:23,600 --> 00:01:28,880
สมมติว่าค่าที่เราต้องการให้การเปิดใช้งานครั้งล่าสุดนี้เป็นสำหรับตัวอย่างการฝึกที่กำหนดคือ y ตัวอย่างเช่น y

24
00:01:28,880 --> 00:01:33,020
อาจเป็น 0 หรือ 1

25
00:01:33,020 --> 00:01:39,040
ดังนั้นค่าใช้จ่ายของเครือข่ายนี้สำหรับตัวอย่างการฝึกอบรมเดียวคือ AL-y2

26
00:01:39,040 --> 00:01:46,120
เราจะแสดงต้นทุนของตัวอย่างการฝึกอบรมหนึ่งตัวอย่างเป็น c0

27
00:01:46,120 --> 00:01:51,920
เพื่อเป็นการเตือนความจำ การเปิดใช้งานครั้งล่าสุดนี้ถูกกำหนดโดยน้ำหนัก ซึ่งฉันจะเรียกว่า wL

28
00:01:51,920 --> 00:01:57,600
คูณกับการกระตุ้นของเซลล์ประสาทก่อนหน้า บวกกับอคติบางอย่าง ซึ่งฉันจะเรียกว่า bL

29
00:01:57,600 --> 00:02:01,560
จากนั้นคุณปั๊มสิ่งนั้นผ่านฟังก์ชันไม่เชิงเส้นพิเศษบางอย่าง เช่น ซิกมอยด์หรือ ReLU

30
00:02:01,560 --> 00:02:05,400
มันจะทำให้ทุกอย่างง่ายขึ้นสำหรับเรา ถ้าเราตั้งชื่อพิเศษให้กับผลรวมถ่วงนี้ เช่น

31
00:02:05,400 --> 00:02:10,600
z โดยใช้ตัวยกเดียวกันกับการเปิดใช้งานที่เกี่ยวข้อง

32
00:02:10,600 --> 00:02:15,320
นี่เป็นคำศัพท์จำนวนมาก และวิธีที่คุณอาจกำหนดกรอบความคิดก็คือ น้ำหนัก การกระทำก่อนหน้า

33
00:02:15,320 --> 00:02:21,800
และอคติทั้งหมดมารวมกันเพื่อคำนวณ z ซึ่งในทางกลับกันช่วยให้เราคำนวณ a

34
00:02:21,800 --> 00:02:27,360
ซึ่งสุดท้ายพร้อมกับค่าคงที่ y ให้ เราคำนวณต้นทุน

35
00:02:27,360 --> 00:02:33,440
และแน่นอนว่า AL-1

36
00:02:33,440 --> 00:02:35,920
ได้รับอิทธิพลจากน้ำหนักและอคติของมันเอง แต่เราจะไม่เน้นไปที่เรื่องนั้นในตอนนี้

37
00:02:35,920 --> 00:02:38,120
ทั้งหมดนี้เป็นเพียงตัวเลขใช่ไหม?

38
00:02:38,120 --> 00:02:41,960
และเป็นเรื่องดีที่จะคิดว่าแต่ละตัวมีเส้นจำนวนเล็กๆ ของตัวเอง

39
00:02:41,960 --> 00:02:47,480
เป้าหมายแรกของเราคือการทำความเข้าใจว่าฟังก์ชันต้นทุนมีความอ่อนไหวต่อการเปลี่ยนแปลงเล็กน้อยของน้ำหนัก wL

40
00:02:47,480 --> 00:02:49,820
ของเราอย่างไร

41
00:02:49,820 --> 00:02:55,740
หรือวลีต่างกัน อนุพันธ์ของ c เทียบกับ wL คืออะไร?

42
00:02:55,740 --> 00:03:01,220
เมื่อคุณเห็นเทอม del w นี้ ให้คิดว่ามันหมายถึงการขยับ w เล็กๆ น้อยๆ เช่นการเปลี่ยนแปลงทีละ 0

43
00:03:01,220 --> 00:03:08,820
01 และคิดว่าคำ del c นี้เป็นความหมายอะไรก็ตามที่ผลักดันให้เกิดต้นทุน

44
00:03:08,820 --> 00:03:10,900
สิ่งที่เราต้องการคืออัตราส่วนของพวกเขา

45
00:03:10,900 --> 00:03:17,740
ตามแนวคิดแล้ว การเขยิบเล็กๆ ไปยัง wL นี้ทำให้เกิดการเขยิบไปที่

46
00:03:17,740 --> 00:03:23,220
zL ซึ่งจะทำให้การเขยิบไปที่ AL ซึ่งส่งผลโดยตรงต่อต้นทุน

47
00:03:23,220 --> 00:03:28,020
ดังนั้นเราจึงแยกสิ่งต่าง ๆ โดยดูที่อัตราส่วนของการเปลี่ยนแปลงเล็กน้อยต่อ zL ต่อการเปลี่ยนแปลงเล็กน้อยนี้

48
00:03:28,020 --> 00:03:33,340
w นั่นคืออนุพันธ์ของ zL เทียบกับ wL

49
00:03:33,340 --> 00:03:38,820
ในทำนองเดียวกัน คุณจะพิจารณาอัตราส่วนของการเปลี่ยนแปลงต่อ AL ต่อการเปลี่ยนแปลงเล็กน้อยใน

50
00:03:38,820 --> 00:03:43,900
zL ที่ทำให้เกิดการเปลี่ยนแปลงดังกล่าว ตลอดจนอัตราส่วนระหว่างการกระตุ้นเตือนครั้งสุดท้ายต่อ

51
00:03:43,900 --> 00:03:45,900
c และการกระตุ้นตรงกลางต่อ AL

52
00:03:45,900 --> 00:03:51,880
นี่คือกฎลูกโซ่ โดยที่การคูณอัตราส่วนทั้งสามนี้ทำให้เรามีความไวของ c

53
00:03:51,880 --> 00:03:57,340
ต่อการเปลี่ยนแปลงเล็กน้อยใน wL

54
00:03:57,340 --> 00:04:01,620
ดังนั้นบนหน้าจอตอนนี้ มีสัญลักษณ์มากมาย

55
00:04:01,620 --> 00:04:07,460
และใช้เวลาสักครู่เพื่อให้แน่ใจว่ามันชัดเจนว่ามันคืออะไร เพราะตอนนี้เราจะคำนวณอนุพันธ์ที่เกี่ยวข้องกัน

56
00:04:07,460 --> 00:04:14,220
อนุพันธ์ของ c เทียบกับ AL จะได้ 2AL-y

57
00:04:14,220 --> 00:04:19,300
ซึ่งหมายความว่าขนาดของมันจะเป็นสัดส่วนกับความแตกต่างระหว่างเอาท์พุตของเครือข่ายกับสิ่งที่เราต้องการให้เป็น

58
00:04:19,300 --> 00:04:24,480
ดังนั้นหากเอาท์พุตนั้นแตกต่างกันมาก

59
00:04:24,480 --> 00:04:28,380
การเปลี่ยนแปลงเพียงเล็กน้อยก็ส่งผลกระทบอย่างมากต่อฟังก์ชันต้นทุนขั้นสุดท้าย

60
00:04:28,380 --> 00:04:33,860
อนุพันธ์ของ AL เทียบกับ zL

61
00:04:33,860 --> 00:04:37,420
เป็นเพียงอนุพันธ์ของฟังก์ชันซิกมอยด์ หรือความไม่เชิงเส้นใดๆ ก็ตามที่คุณเลือกใช้

62
00:04:37,420 --> 00:04:46,180
อนุพันธ์ของ zL เทียบกับ wL ออกมาเป็น AL-1

63
00:04:46,180 --> 00:04:49,460
ฉันไม่รู้เกี่ยวกับคุณ แต่ฉันคิดว่ามันง่ายที่จะจมอยู่กับสูตรต่างๆ

64
00:04:49,460 --> 00:04:54,180
โดยไม่ต้องเสียเวลานั่งและเตือนตัวเองว่ามันหมายถึงอะไร

65
00:04:54,180 --> 00:04:58,860
ในกรณีของอนุพันธ์อันสุดท้ายนี้

66
00:04:58,860 --> 00:05:03,220
ปริมาณที่การขยับเล็กน้อยต่อน้ำหนักจะส่งผลต่อเลเยอร์สุดท้ายนั้นขึ้นอยู่กับความแข็งแกร่งของเซลล์ประสาทก่อนหน้า

67
00:05:03,220 --> 00:05:09,320
โปรดจำไว้ว่า นี่คือที่มาของแนวคิดของเซลล์ประสาทที่ไฟเข้าด้วยกัน

68
00:05:09,320 --> 00:05:14,840
และทั้งหมดนี้เป็นอนุพันธ์ที่เกี่ยวข้องกับ wL

69
00:05:14,840 --> 00:05:16,580
ของต้นทุนสำหรับตัวอย่างการฝึกอบรมเดี่ยวที่เฉพาะเจาะจงเท่านั้น

70
00:05:16,580 --> 00:05:20,940
เนื่องจากฟังก์ชันต้นทุนเต็มเกี่ยวข้องกับการเฉลี่ยต้นทุนทั้งหมดเหล่านั้นจากตัวอย่างการฝึกอบรมที่แตกต่างกันจำนวนมาก

71
00:05:20,940 --> 00:05:27,300
อนุพันธ์ของฟังก์ชันจึงจำเป็นต้องมีการเฉลี่ยนิพจน์นี้กับตัวอย่างการฝึกอบรมทั้งหมด

72
00:05:27,300 --> 00:05:28,540


73
00:05:28,540 --> 00:05:33,860
แน่นอนว่า นั่นเป็นเพียงส่วนประกอบหนึ่งของเวกเตอร์เกรเดียนต์

74
00:05:33,860 --> 00:05:40,780
ซึ่งสร้างขึ้นจากอนุพันธ์ย่อยของฟังก์ชันต้นทุน เทียบกับน้ำหนักและอคติเหล่านั้น

75
00:05:40,780 --> 00:05:44,340
แม้ว่านั่นเป็นเพียงหนึ่งในอนุพันธ์ย่อยที่เราต้องการ แต่มันก็มีมากกว่า

76
00:05:44,340 --> 00:05:46,460
50% ของงานทั้งหมด

77
00:05:46,460 --> 00:05:50,300
ตัวอย่างเช่น ความไวต่ออคติเกือบจะเหมือนกัน

78
00:05:50,300 --> 00:05:58,980
เราแค่ต้องเปลี่ยนเทอม del z del w นี้เป็น del z del b

79
00:05:58,980 --> 00:06:04,700
และถ้าคุณดูสูตรที่เกี่ยวข้อง อนุพันธ์นั้นออกมาเป็น 1

80
00:06:04,700 --> 00:06:11,700
นอกจากนี้ และนี่คือที่มาของแนวคิดในการแพร่กระจายแบบย้อนกลับ

81
00:06:11,700 --> 00:06:16,180
คุณจะเห็นว่าฟังก์ชันต้นทุนนี้มีความละเอียดอ่อนเพียงใดต่อการเปิดใช้งานเลเยอร์ก่อนหน้า

82
00:06:16,180 --> 00:06:21,380
กล่าวคือ อนุพันธ์ตั้งต้นในนิพจน์กฎลูกโซ่ ความไวของ z

83
00:06:21,380 --> 00:06:25,420
ต่อการกระตุ้นครั้งก่อน ออกมาเป็นน้ำหนัก wL

84
00:06:25,420 --> 00:06:30,100
และอีกครั้ง แม้ว่าเราจะไม่สามารถมีอิทธิพลโดยตรงต่อการเปิดใช้งานเลเยอร์ก่อนหน้านั้นได้

85
00:06:30,100 --> 00:06:35,280
แต่ก็มีประโยชน์ในการติดตาม

86
00:06:35,280 --> 00:06:40,780
เพราะตอนนี้เราสามารถวนซ้ำแนวคิดกฎลูกโซ่เดียวกันนี้ย้อนหลังเพื่อดูว่าฟังก์ชันต้นทุนมีความละเอียดอ่อนเพียงใด

87
00:06:40,780 --> 00:06:43,680
น้ำหนักก่อนหน้าและอคติก่อนหน้า

88
00:06:43,680 --> 00:06:47,940
และคุณอาจคิดว่านี่เป็นตัวอย่างที่ง่ายเกินไป เนื่องจากทุกเลเยอร์มีเซลล์ประสาทเดียว

89
00:06:47,940 --> 00:06:51,320
และสิ่งต่างๆ จะมีความซับซ้อนมากขึ้นแบบทวีคูณสำหรับเครือข่ายจริง

90
00:06:51,320 --> 00:06:56,560
แต่จริงๆ แล้ว เราไม่ได้เปลี่ยนแปลงมากนักเมื่อเราให้เซลล์ประสาทหลายตัวแก่เลเยอร์ จริงๆ

91
00:06:56,560 --> 00:06:59,320
แล้ว เป็นเพียงดัชนีเพิ่มเติมเล็กๆ น้อยๆ ที่ต้องติดตาม

92
00:06:59,320 --> 00:07:03,580
แทนที่จะเปิดใช้งานเลเยอร์ที่กำหนดเพียงแค่เป็น AL

93
00:07:03,580 --> 00:07:07,920
มันก็จะมีตัวห้อยที่ระบุว่าเป็นเซลล์ประสาทใดของเลเยอร์นั้น

94
00:07:07,920 --> 00:07:15,280
ลองใช้ตัวอักษร k เพื่อสร้างดัชนีเลเยอร์ L-1 และ j เพื่อสร้างดัชนีเลเยอร์ L

95
00:07:15,280 --> 00:07:20,720
สำหรับค่าใช้จ่าย เราจะดูอีกครั้งว่าเอาต์พุตที่ต้องการคืออะไร

96
00:07:20,720 --> 00:07:26,120
แต่คราวนี้เราจะรวมกำลังสองของความแตกต่างระหว่างการเปิดใช้งานเลเยอร์สุดท้ายเหล่านี้กับเอาต์พุตที่ต้องการ

97
00:07:26,120 --> 00:07:33,280
นั่นคือ, คุณหาผลรวมส่วน ALj ลบ yj กำลังสอง

98
00:07:33,280 --> 00:07:36,500
เนื่องจากมีน้ำหนักมากกว่ามาก แต่ละตัวจึงต้องมีดัชนีอีกสองสามดัชนีเพื่อติดตามว่าอยู่ที่ไหน ดังนั้นลองเรียกน้ำหนักของขอบที่เชื่อมต่อเซลล์ประสาท

99
00:07:36,500 --> 00:07:41,380
k นี้กับเซลล์ประสาท jth

100
00:07:41,380 --> 00:07:45,740
กันดีกว่า WLjk

101
00:07:45,740 --> 00:07:49,820
ดัชนีเหล่านั้นอาจรู้สึกถอยหลังเล็กน้อยในตอนแรก แต่จะสอดคล้องกับวิธีที่คุณจัดทำดัชนีเมทริกซ์น้ำหนักที่ฉันพูดถึงในวิดีโอตอนที่

102
00:07:49,820 --> 00:07:53,800
1

103
00:07:53,800 --> 00:07:57,660
เช่นเดียวกับเมื่อก่อน ยังคงเป็นเรื่องดีที่จะตั้งชื่อให้กับผลรวมถ่วงน้ำหนักที่เกี่ยวข้อง เช่น

104
00:07:57,660 --> 00:08:03,540
z เพื่อให้การเปิดใช้งานเลเยอร์สุดท้ายเป็นเพียงฟังก์ชันพิเศษของคุณ เช่น

105
00:08:03,540 --> 00:08:04,980
sigmoid ที่นำไปใช้กับ z

106
00:08:04,980 --> 00:08:09,100
คุณสามารถเห็นสิ่งที่ฉันหมายถึง โดยที่ทั้งหมดนี้โดยพื้นฐานแล้วเป็นสมการเดียวกับที่เราเคยทำมาก่อน

107
00:08:09,100 --> 00:08:15,420
ในกรณีของหนึ่งเซลล์ประสาทต่อเลเยอร์ เพียงแต่ว่ามันดูซับซ้อนขึ้นเล็กน้อย

108
00:08:15,420 --> 00:08:20,620
และแท้จริงแล้ว นิพจน์อนุพันธ์ของกฎลูกโซ่ที่อธิบายว่าต้นทุนมีความอ่อนไหวต่อน้ำหนักเฉพาะอย่างไร

109
00:08:20,620 --> 00:08:23,540
โดยพื้นฐานแล้วดูจะเหมือนกัน

110
00:08:23,540 --> 00:08:29,420
ฉันจะปล่อยให้คุณหยุดและคิดถึงแต่ละคำศัพท์เหล่านั้นถ้าคุณต้องการ

111
00:08:29,420 --> 00:08:34,900
แต่สิ่งที่เปลี่ยนแปลงตรงนี้คืออนุพันธ์ของต้นทุนที่เกี่ยวข้องกับการเปิดใช้งานอย่างใดอย่างหนึ่งในเลเยอร์

112
00:08:34,900 --> 00:08:37,820
L-1

113
00:08:37,820 --> 00:08:42,000
ในกรณีนี้

114
00:08:42,000 --> 00:08:43,540
ความแตกต่างก็คือเซลล์ประสาทมีอิทธิพลต่อฟังก์ชันต้นทุนผ่านเส้นทางที่แตกต่างกันหลายเส้นทาง

115
00:08:43,540 --> 00:08:51,200
นั่นคือในอีกด้านหนึ่ง มันมีอิทธิพลต่อ AL0

116
00:08:51,200 --> 00:08:56,460
ซึ่งมีบทบาทในฟังก์ชันต้นทุน แต่ก็มีอิทธิพลต่อ AL1

117
00:08:56,460 --> 00:09:00,340
ซึ่งมีบทบาทในฟังก์ชันต้นทุนด้วย และคุณต้องบวกสิ่งเหล่านั้นเข้าด้วยกัน

118
00:09:00,340 --> 00:09:03,680
และนั่นก็ค่อนข้างจะเป็นเช่นนั้น

119
00:09:03,680 --> 00:09:08,240
เมื่อคุณทราบแล้วว่าฟังก์ชันต้นทุนมีความอ่อนไหวต่อการเปิดใช้งานในเลเยอร์ที่สองจากสุดท้ายนี้เพียงใด

120
00:09:08,240 --> 00:09:12,520
คุณก็สามารถทำซ้ำขั้นตอนนี้สำหรับน้ำหนักและอคติทั้งหมดที่ป้อนเข้าไปในเลเยอร์นั้นได้

121
00:09:12,520 --> 00:09:13,920


122
00:09:13,920 --> 00:09:15,420
ดังนั้นตบหลังตัวเองซะ!

123
00:09:15,420 --> 00:09:20,480
หากทั้งหมดนี้สมเหตุสมผล ตอนนี้คุณได้มองลึกเข้าไปในหัวใจของการแพร่กระจายกลับ

124
00:09:20,480 --> 00:09:23,700
ซึ่งเป็นกลไกเบื้องหลังการเรียนรู้ของโครงข่ายประสาทเทียม

125
00:09:23,700 --> 00:09:27,960
นิพจน์กฎลูกโซ่เหล่านี้ให้อนุพันธ์ที่กำหนดแต่ละส่วนประกอบในการไล่ระดับสี

126
00:09:27,960 --> 00:09:35,020
ซึ่งช่วยลดต้นทุนของเครือข่ายโดยการก้าวลงเนินซ้ำๆ

127
00:09:35,020 --> 00:09:38,960
หากคุณนั่งลงและคิดเกี่ยวกับเรื่องทั้งหมดนั้น นี่เป็นเรื่องที่ซับซ้อนหลายชั้นที่ห่อหุ้มจิตใจของคุณ

128
00:09:38,960 --> 00:09:42,840
ดังนั้นอย่ากังวลหากจิตใจของคุณต้องใช้เวลาในการย่อยทั้งหมด

