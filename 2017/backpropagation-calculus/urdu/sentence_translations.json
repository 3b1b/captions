[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm. ",
  "translatedText": "یہاں مشکل مفروضہ یہ ہے کہ آپ نے حصہ 3 دیکھا ہے، بیک پروپیگیشن الگورتھم کی ایک بدیہی واک تھرو پیش کرتے ہوئے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus. ",
  "translatedText": "یہاں ہم تھوڑا سا رسمی ہو جاتے ہیں اور متعلقہ حساب کتاب میں غوطہ لگاتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else. ",
  "translatedText": "اس کے لیے کم از کم تھوڑا سا الجھا ہوا ہونا معمول کی بات ہے، اس لیے باقاعدگی سے توقف کرنے اور غور کرنے کا منتر یہاں بھی اتنا ہی لاگو ہوتا ہے جتنا کہیں اور۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject. ",
  "translatedText": "ہمارا بنیادی مقصد یہ بتانا ہے کہ مشین لرننگ میں لوگ عام طور پر نیٹ ورکس کے تناظر میں کیلکولس کے سلسلہ اصول کے بارے میں کس طرح سوچتے ہیں، جس کا احساس اس سے مختلف ہوتا ہے کہ زیادہ تر تعارفی کیلکولس کورس کس طرح موضوع تک پہنچتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic. ",
  "translatedText": "آپ میں سے ان لوگوں کے لیے جو متعلقہ کیلکولس سے ناخوش ہیں، میرے پاس اس موضوع پر ایک پوری سیریز ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it. ",
  "translatedText": "آئیے ایک انتہائی سادہ نیٹ ورک کے ساتھ شروع کریں، جہاں ہر ایک پرت میں ایک نیوران ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables. ",
  "translatedText": "اس نیٹ ورک کا تعین تین وزنوں اور تین تعصبات سے ہوتا ہے، اور ہمارا مقصد یہ سمجھنا ہے کہ لاگت کا فنکشن ان متغیرات کے لیے کتنا حساس ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way we know which adjustments to those terms will cause the most efficient decrease to the cost function. ",
  "translatedText": "اس طرح ہم جانتے ہیں کہ ان شرائط میں کون سی ایڈجسٹمنٹ لاگت کے فنکشن میں سب سے زیادہ مؤثر کمی کا سبب بنے گی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "We'll just focus on the connection between the last two neurons. ",
  "translatedText": "ہم صرف آخری دو نیوران کے درمیان تعلق پر توجہ مرکوز کریں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in. ",
  "translatedText": "آئیے اس آخری نیوران کی ایکٹیویشن کو سپر اسکرپٹ L کے ساتھ لیبل لگاتے ہیں، یہ بتاتا ہے کہ یہ کس پرت میں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.98,
  "end": 71.36
 },
 {
  "input": "So the activation of the previous neuron is AL-1. ",
  "translatedText": "تو پچھلے نیوران کی ایکٹیویشن AL-1 ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.68,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on. ",
  "translatedText": "یہ ایکسپونینٹس نہیں ہیں، یہ صرف انڈیکس کرنے کا ایک طریقہ ہیں جس کے بارے میں ہم بات کر رہے ہیں، کیونکہ میں بعد میں مختلف انڈیکسز کے لیے سبسکرپٹس کو محفوظ کرنا چاہتا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1. ",
  "translatedText": "آئیے کہتے ہیں کہ جو قدر ہم چاہتے ہیں کہ یہ آخری ایکٹیویشن دی گئی تربیتی مثال کے لیے ہو وہ y ہے، مثال کے طور پر، y 0 یا 1 ہو سکتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is AL-y2. ",
  "translatedText": "لہذا اس نیٹ ورک کی ایک واحد تربیتی مثال کے لیے لاگت AL-y2 ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0. ",
  "translatedText": "ہم اس ایک تربیتی مثال کی قیمت کو c0 کے طور پر بیان کریں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call wL, times the previous neuron's activation plus some bias, which I'll call bL. ",
  "translatedText": "یاد دہانی کے طور پر، اس آخری ایکٹیویشن کا تعین ایک وزن سے ہوتا ہے، جسے میں wL کہوں گا، پچھلے نیوران کی ایکٹیویشن کے علاوہ کچھ تعصب، جسے میں bL کہوں گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "Then you pump that through some special nonlinear function like the sigmoid or ReLU. ",
  "translatedText": "پھر آپ اسے کچھ خاص نان لائنر فنکشن جیسے سگمائیڈ یا ReLU کے ذریعے پمپ کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations. ",
  "translatedText": "یہ دراصل ہمارے لیے چیزوں کو آسان بنائے گا اگر ہم اس وزنی رقم کو ایک خاص نام دیں، جیسے z، اسی سپر اسکرپٹ کے ساتھ متعلقہ ایکٹیویشنز۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action, and bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost. ",
  "translatedText": "یہ بہت ساری اصطلاحات ہیں، اور جس طرح سے آپ اسے تصور کر سکتے ہیں وہ یہ ہے کہ وزن، سابقہ عمل، اور تعصب سب کو ملا کر z کی گنتی کے لیے استعمال کیا جاتا ہے، جس کے نتیجے میں ہمیں a کی گنتی کرنے دیتا ہے، جو آخر میں، مستقل y کے ساتھ، اجازت دیتا ہے۔ہم لاگت کا حساب لگاتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course, AL-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now. ",
  "translatedText": "اور یقیناً، AL-1 اپنے وزن اور تعصب وغیرہ سے متاثر ہے، لیکن ہم ابھی اس پر توجہ نہیں دیں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line. ",
  "translatedText": "یہ سب صرف نمبر ہیں، ٹھیک ہے؟ اور یہ سوچنا اچھا ہو سکتا ہے کہ ہر ایک کی اپنی چھوٹی نمبر لائن ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight wL. ",
  "translatedText": "ہمارا پہلا مقصد یہ سمجھنا ہے کہ لاگت کا فنکشن ہمارے وزن میں چھوٹی تبدیلیوں کے لیے کتنا حساس ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to wL? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del w term, think of it as meaning some tiny nudge to w, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is. ",
  "translatedText": "یا فقرہ مختلف طور پر، wL کے حوالے سے c کا مشتق کیا ہے؟ جب آپ اس ڈیل ڈبلیو اصطلاح کو دیکھتے ہیں، تو اس کے بارے میں سوچیں کہ اس کا مطلب w کے لیے کچھ چھوٹا جھٹکا ہے، جیسے 0 کی تبدیلی۔01، اور اس ڈیل سی اصطلاح کے معنی کے طور پر سوچیں جو کچھ بھی نتیجہ میں لاگت کا نتیجہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio. ",
  "translatedText": "ہم جو چاہتے ہیں وہ ان کا تناسب ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to wL causes some nudge to zL, which in turn causes some nudge to AL, which directly influences the cost. ",
  "translatedText": "تصوراتی طور پر، wL پر یہ چھوٹا سا جھٹکا zL کو کچھ جھٹکا دیتا ہے، جس کے نتیجے میں AL کو کچھ جھٹکا پڑتا ہے، جو لاگت کو براہ راست متاثر کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to zL to this tiny change w, that is, the derivative of zL with respect to wL. ",
  "translatedText": "لہذا ہم چیزوں کو پہلے zL سے اس چھوٹی تبدیلی کے تناسب کو دیکھ کر توڑ دیتے ہیں، یعنی wL کے حوالے سے zL کا مشتق۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in zL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL. ",
  "translatedText": "اسی طرح، آپ پھر AL میں تبدیلی اور zL میں ہونے والی چھوٹی تبدیلی کے تناسب پر غور کرتے ہیں جس کی وجہ سے یہ ہوا، اور ساتھ ہی حتمی nudge to c اور اس انٹرمیڈیٹ nudge اور AL کے درمیان تناسب۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying these three ratios gives us the sensitivity of c to small changes in wL. ",
  "translatedText": "یہاں یہ سلسلہ اصول ہے، جہاں ان تینوں تناسب کو ضرب کرنے سے ہمیں wL میں چھوٹی تبدیلیوں کے لیے c کی حساسیت ملتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives. ",
  "translatedText": "تو اس وقت اسکرین پر، بہت ساری علامتیں ہیں، اور اس بات کو یقینی بنانے کے لیے ایک لمحہ نکالیں کہ یہ واضح ہے کہ وہ سب کیا ہیں، کیونکہ اب ہم متعلقہ مشتقات کی گنتی کرنے جا رہے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y. ",
  "translatedText": "AL کے حوالے سے c کا مشتق 2AL-y ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "This means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function. ",
  "translatedText": "اس کا مطلب ہے کہ اس کا سائز نیٹ ورک کے آؤٹ پٹ اور اس چیز کے درمیان فرق کے متناسب ہے جو ہم اسے بننا چاہتے ہیں، لہذا اگر وہ آؤٹ پٹ بہت مختلف تھا، تو معمولی تبدیلیاں بھی حتمی لاگت کے فنکشن پر بڑا اثر ڈالتی ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to zL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use. ",
  "translatedText": "zL کے حوالے سے AL کا مشتق صرف ہمارے سگمائیڈ فنکشن کا مشتق ہے، یا آپ جو بھی نان لائنیرٹی استعمال کرنے کا انتخاب کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "The derivative of zL with respect to wL comes out to be AL-1. ",
  "translatedText": "wL کے حوالے سے zL کا مشتق AL-1 نکلتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself what they all mean. ",
  "translatedText": "میں آپ کے بارے میں نہیں جانتا، لیکن مجھے لگتا ہے کہ فارمولوں میں ایک لمحے کے بغیر بیٹھنا اور اپنے آپ کو یاد دلانا آسان ہے کہ ان سب کا کیا مطلب ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is. ",
  "translatedText": "اس آخری مشتق کی صورت میں، وزن کے چھوٹے جھکاؤ نے آخری تہہ کو متاثر کیا اس کا انحصار اس بات پر ہے کہ پچھلا نیوران کتنا مضبوط ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in. ",
  "translatedText": "یاد رکھیں، یہ وہ جگہ ہے جہاں نیوران-وہ-آگ-ایک ساتھ-وائر-ایک ساتھ آئیڈیا آتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to wL only of the cost for a specific single training example. ",
  "translatedText": "اور یہ سب صرف ایک مخصوص واحد تربیتی مثال کے لیے لاگت کے wL کے حوالے سے مشتق ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples. ",
  "translatedText": "چونکہ مکمل لاگت کے فنکشن میں بہت سی مختلف تربیتی مثالوں میں ان تمام اخراجات کا ایک ساتھ اوسط شامل ہوتا ہے، اس لیے اس کے مشتق کے لیے تمام تربیتی مثالوں پر اس اظہار کی اوسط کی ضرورت ہوتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "Of course, that's just one component of the gradient vector, which is built up from the partial derivatives of the cost function with respect to all those weights and biases. ",
  "translatedText": "بلاشبہ، یہ گریڈینٹ ویکٹر کا صرف ایک جزو ہے، جو ان تمام وزنوں اور تعصبات کے حوالے سے لاگت کے فنکشن کے جزوی مشتقات سے بنایا گیا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work. ",
  "translatedText": "لیکن اگرچہ یہ بہت سے جزوی مشتقات میں سے ایک ہے جن کی ہمیں ضرورت ہے، یہ 50% سے زیادہ کام ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical. ",
  "translatedText": "تعصب کی حساسیت، مثال کے طور پر، تقریباً ایک جیسی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b. ",
  "translatedText": "ہمیں صرف اس ڈیل زیڈ ڈیل ڈبلیو اصطلاح کو ڈیل زیڈ ڈیل بی کے لیے تبدیل کرنے کی ضرورت ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1. ",
  "translatedText": "اور اگر آپ متعلقہ فارمولے کو دیکھیں تو وہ مشتق 1 نکلتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer. ",
  "translatedText": "اس کے علاوہ، اور یہ وہ جگہ ہے جہاں پیچھے کی طرف پھیلانے کا خیال آتا ہے، آپ دیکھ سکتے ہیں کہ یہ لاگت کا فنکشن پچھلی پرت کو چالو کرنے کے لیے کتنا حساس ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight wL. ",
  "translatedText": "یعنی، سلسلہ اصول کے اظہار میں یہ ابتدائی مشتق، گزشتہ ایکٹیویشن کے لیے z کی حساسیت، وزن wL کے طور پر سامنے آتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases. ",
  "translatedText": "اور ایک بار پھر، اگرچہ ہم اس پچھلی پرت کی ایکٹیویشن پر براہ راست اثر انداز ہونے کے قابل نہیں ہوں گے، لیکن اس پر نظر رکھنا مددگار ہے، کیونکہ اب ہم صرف اسی سلسلہ اصول کے خیال کو پیچھے کی طرف دہراتے رہ سکتے ہیں تاکہ یہ معلوم ہو سکے کہ لاگت کا فنکشن کتنا حساس ہے۔پچھلے وزن اور پچھلے تعصبات۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network. ",
  "translatedText": "اور آپ کو لگتا ہے کہ یہ ایک حد سے زیادہ سادہ مثال ہے، کیونکہ تمام تہوں میں ایک نیوران ہوتا ہے، اور چیزیں ایک حقیقی نیٹ ورک کے لیے تیزی سے پیچیدہ ہوتی جا رہی ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of. ",
  "translatedText": "لیکن ایمانداری سے، جب ہم تہوں کو ایک سے زیادہ نیوران دیتے ہیں تو اتنی تبدیلیاں نہیں ہوتیں، حقیقت میں یہ صرف چند اور اشاریوں پر نظر رکھنے کے لیے ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is. ",
  "translatedText": "کسی دی گئی پرت کو صرف AL ہونے کی وجہ سے ایکٹیویشن کرنے کے بجائے، اس میں ایک سب اسکرپٹ بھی ہوگا جو اس پرت کا کون سا نیورون ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L. ",
  "translatedText": "آئیے لیئر L-1 کو انڈیکس کرنے کے لیے حرف k کا استعمال کریں، اور L کو انڈیکس کرنے کے لیے j کا استعمال کریں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output. ",
  "translatedText": "لاگت کے لیے، ہم دوبارہ دیکھتے ہیں کہ مطلوبہ آؤٹ پٹ کیا ہے، لیکن اس بار ہم ان آخری پرت کی ایکٹیویشن اور مطلوبہ آؤٹ پٹ کے درمیان فرق کے مربع کو شامل کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus yj squared. ",
  "translatedText": "یعنی، آپ ALj مائنس yj مربع سے زیادہ رقم لیتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk. ",
  "translatedText": "چونکہ بہت زیادہ وزن ہے، اس لیے ہر ایک کے پاس دو دو مزید اشاریے ہونے چاہئیں تاکہ یہ معلوم ہو سکے کہ یہ کہاں ہے، تو آئیے اس kth نیورون کو jth نیورون، WLjk سے جوڑنے والے کنارے کے وزن کو کہتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video. ",
  "translatedText": "وہ اشاریے پہلے تو تھوڑا پیچھے کی طرف محسوس کر سکتے ہیں، لیکن یہ اس بات کے مطابق ہے کہ آپ کس طرح وزن کے میٹرکس کو انڈیکس کریں گے جس کے بارے میں میں نے حصہ 1 ویڈیو میں بات کی تھی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z. ",
  "translatedText": "بالکل پہلے کی طرح، متعلقہ وزنی رقم کو ایک نام دینا اب بھی اچھا ہے، جیسا کہ z، تاکہ آخری تہہ کا ایکٹیویشن صرف آپ کا خاص فنکشن ہو، جیسا کہ sigmoid، z پر لاگو ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated. ",
  "translatedText": "آپ دیکھ سکتے ہیں کہ میرا کیا مطلب ہے، جہاں یہ سب بنیادی طور پر وہی مساوات ہیں جو ہمارے پاس ون نیورون فی پرت کے معاملے میں پہلے تھے، یہ صرف اتنا ہے کہ یہ تھوڑا زیادہ پیچیدہ لگتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain rule derivative expression describing how sensitive the cost is to a specific weight looks essentially the same. ",
  "translatedText": "اور درحقیقت، سلسلہ اصول اخذ کرنے والا اظہار یہ بیان کرتا ہے کہ قیمت ایک مخصوص وزن کے لیے کتنی حساس ہے بنیادی طور پر ایک جیسی نظر آتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want. ",
  "translatedText": "میں اسے آپ پر چھوڑ دوں گا کہ اگر آپ چاہیں تو ان شرائط میں سے ہر ایک کے بارے میں توقف کریں اور سوچیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1. ",
  "translatedText": "یہاں کیا تبدیلی آتی ہے، اگرچہ، L-1 پرت میں ایک ایکٹیویشن کے حوالے سے لاگت کا اخذ کیا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths. ",
  "translatedText": "اس معاملے میں، فرق یہ ہے کہ نیورون متعدد مختلف راستوں سے لاگت کے فنکشن کو متاثر کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up. ",
  "translatedText": "یعنی، ایک طرف، یہ AL0 کو متاثر کرتا ہے، جو لاگت کے فنکشن میں کردار ادا کرتا ہے، لیکن اس کا اثر AL1 پر بھی ہوتا ہے، جو لاگت کے فنکشن میں بھی کردار ادا کرتا ہے، اور آپ کو ان کو شامل کرنا ہوگا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it. ",
  "translatedText": "اور یہ، ٹھیک ہے، یہ بہت زیادہ ہے. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer. ",
  "translatedText": "ایک بار جب آپ جان لیں کہ لاگت کا فنکشن اس دوسری سے آخری پرت میں ایکٹیویشن کے لیے کتنا حساس ہے، تو آپ اس پرت میں شامل تمام وزن اور تعصبات کے لیے عمل کو دہرا سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back! ",
  "translatedText": "تو اپنے آپ کو پیٹھ پر تھپتھپائیں! ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn. ",
  "translatedText": "اگر یہ سب کچھ سمجھ میں آتا ہے، تو اب آپ نے بیک پروپیگیشن کے دل کی گہرائی میں دیکھا ہے، جو کہ اعصابی نیٹ ورک کیسے سیکھتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill. ",
  "translatedText": "یہ سلسلہ اصول اظہار آپ کو مشتقات فراہم کرتے ہیں جو گریڈینٹ میں ہر ایک جز کا تعین کرتے ہیں جو بار بار نیچے کی طرف قدم بڑھا کر نیٹ ورک کی لاگت کو کم کرنے میں مدد کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all. ",
  "translatedText": "اگر آپ بیٹھ کر ان سب کے بارے میں سوچتے ہیں، تو یہ آپ کے دماغ کو لپیٹنے کے لیے پیچیدگی کی بہت سی تہیں ہیں، اس لیے فکر نہ کریں کہ کیا آپ کے دماغ کو یہ سب ہضم ہونے میں وقت لگتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]