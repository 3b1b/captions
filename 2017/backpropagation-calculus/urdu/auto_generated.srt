1
00:00:00,000 --> 00:00:08,420
یہاں مشکل مفروضہ یہ ہے کہ آپ نے حصہ 3 دیکھا ہے،

2
00:00:08,420 --> 00:00:11,160
بیک پروپیگیشن الگورتھم کی ایک بدیہی واک تھرو پیش کرتے ہوئے۔

3
00:00:11,160 --> 00:00:14,920
یہاں ہم تھوڑا سا رسمی ہو جاتے ہیں اور متعلقہ حساب کتاب میں غوطہ لگاتے ہیں۔

4
00:00:14,920 --> 00:00:18,560
اس کے لیے کم از کم تھوڑا سا الجھا ہوا ہونا معمول کی بات ہے، اس لیے باقاعدگی

5
00:00:18,560 --> 00:00:22,000
سے توقف کرنے اور غور کرنے کا منتر یہاں بھی اتنا ہی لاگو ہوتا ہے جتنا کہیں اور۔

6
00:00:22,000 --> 00:00:26,620
ہمارا بنیادی مقصد یہ بتانا ہے کہ مشین لرننگ میں لوگ عام طور پر نیٹ ورکس کے

7
00:00:26,620 --> 00:00:31,900
تناظر میں کیلکولس کے سلسلہ اصول کے بارے میں کس طرح سوچتے ہیں، جس کا احساس اس

8
00:00:31,900 --> 00:00:34,580
سے مختلف ہوتا ہے کہ زیادہ تر تعارفی کیلکولس کورس کس طرح موضوع تک پہنچتے ہیں۔

9
00:00:34,580 --> 00:00:38,300
آپ میں سے ان لوگوں کے لیے جو متعلقہ کیلکولس سے

10
00:00:38,300 --> 00:00:39,300
ناخوش ہیں، میرے پاس اس موضوع پر ایک پوری سیریز ہے۔

11
00:00:39,300 --> 00:00:44,840
آئیے ایک انتہائی سادہ نیٹ ورک کے ساتھ شروع کریں،

12
00:00:44,840 --> 00:00:46,780
جہاں ہر ایک پرت میں ایک نیوران ہوتا ہے۔

13
00:00:46,780 --> 00:00:51,880
اس نیٹ ورک کا تعین تین وزنوں اور تین تعصبات سے ہوتا ہے، اور ہمارا

14
00:00:51,880 --> 00:00:55,640
مقصد یہ سمجھنا ہے کہ لاگت کا فنکشن ان متغیرات کے لیے کتنا حساس ہے۔

15
00:00:55,640 --> 00:00:59,780
اس طرح ہم جانتے ہیں کہ ان شرائط میں کون سی ایڈجسٹمنٹ لاگت

16
00:00:59,780 --> 00:01:01,100
کے فنکشن میں سب سے زیادہ مؤثر کمی کا سبب بنے گی۔

17
00:01:01,100 --> 00:01:05,360
ہم صرف آخری دو نیوران کے درمیان تعلق پر توجہ مرکوز کریں گے۔

18
00:01:05,360 --> 00:01:10,400
آئیے اس آخری نیوران کی ایکٹیویشن کو سپر اسکرپٹ L کے ساتھ

19
00:01:10,400 --> 00:01:11,800
لیبل لگاتے ہیں، یہ بتاتا ہے کہ یہ کس پرت میں ہے۔

20
00:01:11,800 --> 00:01:16,560
تو پچھلے نیوران کی ایکٹیویشن AL-1 ہے۔

21
00:01:16,560 --> 00:01:20,120
یہ ایکسپونینٹس نہیں ہیں، یہ صرف انڈیکس کرنے کا ایک طریقہ ہیں جس کے بارے میں ہم بات

22
00:01:20,120 --> 00:01:23,120
کر رہے ہیں، کیونکہ میں بعد میں مختلف انڈیکسز کے لیے سبسکرپٹس کو محفوظ کرنا چاہتا ہوں۔

23
00:01:23,600 --> 00:01:28,880
آئیے کہتے ہیں کہ جو قدر ہم چاہتے ہیں کہ یہ آخری ایکٹیویشن دی گئی تربیتی مثال

24
00:01:28,880 --> 00:01:33,020
کے لیے ہو وہ y ہے، مثال کے طور پر، y 0 یا 1 ہو سکتا ہے۔

25
00:01:33,020 --> 00:01:39,040
لہذا اس نیٹ ورک کی ایک واحد تربیتی مثال کے لیے لاگت AL-y2 ہے۔

26
00:01:39,040 --> 00:01:46,120
ہم اس ایک تربیتی مثال کی قیمت کو c0 کے طور پر بیان کریں گے۔

27
00:01:46,120 --> 00:01:51,920
یاد دہانی کے طور پر، اس آخری ایکٹیویشن کا تعین ایک وزن سے ہوتا ہے، جسے میں

28
00:01:51,920 --> 00:01:57,600
wL کہوں گا، پچھلے نیوران کی ایکٹیویشن کے علاوہ کچھ تعصب، جسے میں bL کہوں گا۔

29
00:01:57,600 --> 00:02:01,560
پھر آپ اسے کچھ خاص نان لائنر فنکشن جیسے سگمائیڈ یا ReLU کے ذریعے پمپ کرتے ہیں۔

30
00:02:01,560 --> 00:02:05,400
یہ دراصل ہمارے لیے چیزوں کو آسان بنائے گا اگر ہم اس وزنی رقم

31
00:02:05,400 --> 00:02:10,600
کو ایک خاص نام دیں، جیسے z، اسی سپر اسکرپٹ کے ساتھ متعلقہ ایکٹیویشنز۔

32
00:02:10,600 --> 00:02:15,320
یہ بہت ساری اصطلاحات ہیں، اور جس طرح سے آپ اسے تصور کر سکتے ہیں وہ یہ ہے کہ وزن، سابقہ عمل،

33
00:02:15,320 --> 00:02:21,800
اور تعصب سب کو ملا کر z کی گنتی کے لیے استعمال کیا جاتا ہے، جس کے نتیجے میں ہمیں a

34
00:02:21,800 --> 00:02:27,360
کی گنتی کرنے دیتا ہے، جو آخر میں، مستقل y کے ساتھ، اجازت دیتا ہے۔ ہم لاگت کا حساب لگاتے ہیں۔

35
00:02:27,360 --> 00:02:33,440
اور یقیناً، AL-1 اپنے وزن اور تعصب وغیرہ سے متاثر

36
00:02:33,440 --> 00:02:35,920
ہے، لیکن ہم ابھی اس پر توجہ نہیں دیں گے۔

37
00:02:35,920 --> 00:02:38,120
یہ سب صرف نمبر ہیں، ٹھیک ہے؟

38
00:02:38,120 --> 00:02:41,960
اور یہ سوچنا اچھا ہو سکتا ہے کہ ہر ایک کی اپنی چھوٹی نمبر لائن ہے۔

39
00:02:41,960 --> 00:02:47,480
ہمارا پہلا مقصد یہ سمجھنا ہے کہ لاگت کا فنکشن

40
00:02:47,480 --> 00:02:49,820
ہمارے وزن میں چھوٹی تبدیلیوں کے لیے کتنا حساس ہے۔

41
00:02:49,820 --> 00:02:55,740
یا فقرہ مختلف طور پر، wL کے حوالے سے c کا مشتق کیا ہے؟

42
00:02:55,740 --> 00:03:01,220
جب آپ اس ڈیل ڈبلیو اصطلاح کو دیکھتے ہیں، تو اس کے بارے میں سوچیں کہ اس کا مطلب w کے لیے کچھ چھوٹا جھٹکا ہے، جیسے 0 کی

43
00:03:01,220 --> 00:03:08,820
تبدیلی۔ 01، اور اس ڈیل سی اصطلاح کے معنی کے طور پر سوچیں جو کچھ بھی نتیجہ میں لاگت کا نتیجہ ہے۔

44
00:03:08,820 --> 00:03:10,900
ہم جو چاہتے ہیں وہ ان کا تناسب ہے۔

45
00:03:10,900 --> 00:03:17,740
تصوراتی طور پر، wL پر یہ چھوٹا سا جھٹکا zL کو کچھ جھٹکا دیتا ہے، جس کے

46
00:03:17,740 --> 00:03:23,220
نتیجے میں AL کو کچھ جھٹکا پڑتا ہے، جو لاگت کو براہ راست متاثر کرتا ہے۔

47
00:03:23,220 --> 00:03:28,020
لہذا ہم چیزوں کو پہلے zL سے اس چھوٹی تبدیلی کے تناسب کو

48
00:03:28,020 --> 00:03:33,340
دیکھ کر توڑ دیتے ہیں، یعنی wL کے حوالے سے zL کا مشتق۔

49
00:03:33,340 --> 00:03:38,820
اسی طرح، آپ پھر AL میں تبدیلی اور zL میں ہونے والی چھوٹی تبدیلی

50
00:03:38,820 --> 00:03:43,900
کے تناسب پر غور کرتے ہیں جس کی وجہ سے یہ ہوا، اور ساتھ

51
00:03:43,900 --> 00:03:45,900
ہی حتمی nudge to c اور اس انٹرمیڈیٹ nudge اور AL کے درمیان تناسب۔

52
00:03:45,900 --> 00:03:51,880
یہاں یہ سلسلہ اصول ہے، جہاں ان تینوں تناسب کو ضرب کرنے سے

53
00:03:51,880 --> 00:03:57,340
ہمیں wL میں چھوٹی تبدیلیوں کے لیے c کی حساسیت ملتی ہے۔

54
00:03:57,340 --> 00:04:01,620
تو اس وقت اسکرین پر، بہت ساری علامتیں ہیں، اور اس بات کو یقینی بنانے کے لیے ایک لمحہ نکالیں

55
00:04:01,620 --> 00:04:07,460
کہ یہ واضح ہے کہ وہ سب کیا ہیں، کیونکہ اب ہم متعلقہ مشتقات کی گنتی کرنے جا رہے ہیں۔

56
00:04:07,460 --> 00:04:14,220
AL کے حوالے سے c کا مشتق 2AL-y ہے۔

57
00:04:14,220 --> 00:04:19,300
اس کا مطلب ہے کہ اس کا سائز نیٹ ورک کے آؤٹ پٹ اور اس چیز کے

58
00:04:19,300 --> 00:04:24,480
درمیان فرق کے متناسب ہے جو ہم اسے بننا چاہتے ہیں، لہذا اگر وہ آؤٹ پٹ

59
00:04:24,480 --> 00:04:28,380
بہت مختلف تھا، تو معمولی تبدیلیاں بھی حتمی لاگت کے فنکشن پر بڑا اثر ڈالتی ہیں۔

60
00:04:28,380 --> 00:04:33,860
zL کے حوالے سے AL کا مشتق صرف ہمارے سگمائیڈ فنکشن کا مشتق

61
00:04:33,860 --> 00:04:37,420
ہے، یا آپ جو بھی نان لائنیرٹی استعمال کرنے کا انتخاب کرتے ہیں۔

62
00:04:37,420 --> 00:04:46,180
wL کے حوالے سے zL کا مشتق AL-1 نکلتا ہے۔

63
00:04:46,180 --> 00:04:49,460
میں آپ کے بارے میں نہیں جانتا، لیکن مجھے لگتا ہے کہ فارمولوں میں ایک لمحے کے

64
00:04:49,460 --> 00:04:54,180
بغیر بیٹھنا اور اپنے آپ کو یاد دلانا آسان ہے کہ ان سب کا کیا مطلب ہے۔

65
00:04:54,180 --> 00:04:58,860
اس آخری مشتق کی صورت میں، وزن کے چھوٹے جھکاؤ نے آخری تہہ کو متاثر

66
00:04:58,860 --> 00:05:03,220
کیا اس کا انحصار اس بات پر ہے کہ پچھلا نیوران کتنا مضبوط ہے۔

67
00:05:03,220 --> 00:05:09,320
یاد رکھیں، یہ وہ جگہ ہے جہاں نیوران-وہ-آگ-ایک ساتھ-وائر-ایک ساتھ آئیڈیا آتا ہے۔

68
00:05:09,320 --> 00:05:14,840
اور یہ سب صرف ایک مخصوص واحد تربیتی مثال کے

69
00:05:14,840 --> 00:05:16,580
لیے لاگت کے wL کے حوالے سے مشتق ہے۔

70
00:05:16,580 --> 00:05:20,940
چونکہ مکمل لاگت کے فنکشن میں بہت سی مختلف تربیتی مثالوں میں ان تمام

71
00:05:20,940 --> 00:05:27,300
اخراجات کا ایک ساتھ اوسط شامل ہوتا ہے، اس لیے اس کے مشتق کے

72
00:05:27,300 --> 00:05:28,540
لیے تمام تربیتی مثالوں پر اس اظہار کی اوسط کی ضرورت ہوتی ہے۔

73
00:05:28,540 --> 00:05:33,860
بلاشبہ، یہ گریڈینٹ ویکٹر کا صرف ایک جزو ہے، جو ان تمام وزنوں اور

74
00:05:33,860 --> 00:05:40,780
تعصبات کے حوالے سے لاگت کے فنکشن کے جزوی مشتقات سے بنایا گیا ہے۔

75
00:05:40,780 --> 00:05:44,340
لیکن اگرچہ یہ بہت سے جزوی مشتقات میں سے ایک ہے

76
00:05:44,340 --> 00:05:46,460
جن کی ہمیں ضرورت ہے، یہ 50% سے زیادہ کام ہے۔

77
00:05:46,460 --> 00:05:50,300
تعصب کی حساسیت، مثال کے طور پر، تقریباً ایک جیسی ہے۔

78
00:05:50,300 --> 00:05:58,980
ہمیں صرف اس ڈیل زیڈ ڈیل ڈبلیو اصطلاح کو ڈیل زیڈ ڈیل بی کے لیے تبدیل کرنے کی ضرورت ہے۔

79
00:05:58,980 --> 00:06:04,700
اور اگر آپ متعلقہ فارمولے کو دیکھیں تو وہ مشتق 1 نکلتا ہے۔

80
00:06:04,700 --> 00:06:11,700
اس کے علاوہ، اور یہ وہ جگہ ہے جہاں پیچھے کی طرف پھیلانے کا خیال آتا ہے، آپ

81
00:06:11,700 --> 00:06:16,180
دیکھ سکتے ہیں کہ یہ لاگت کا فنکشن پچھلی پرت کو چالو کرنے کے لیے کتنا حساس ہے۔

82
00:06:16,180 --> 00:06:21,380
یعنی، سلسلہ اصول کے اظہار میں یہ ابتدائی مشتق، گزشتہ ایکٹیویشن کے

83
00:06:21,380 --> 00:06:25,420
لیے z کی حساسیت، وزن wL کے طور پر سامنے آتی ہے۔

84
00:06:25,420 --> 00:06:30,100
اور ایک بار پھر، اگرچہ ہم اس پچھلی پرت کی ایکٹیویشن پر براہ راست اثر انداز

85
00:06:30,100 --> 00:06:35,280
ہونے کے قابل نہیں ہوں گے، لیکن اس پر نظر رکھنا مددگار ہے، کیونکہ اب ہم

86
00:06:35,280 --> 00:06:40,780
صرف اسی سلسلہ اصول کے خیال کو پیچھے کی طرف دہراتے رہ سکتے ہیں تاکہ یہ

87
00:06:40,780 --> 00:06:43,680
معلوم ہو سکے کہ لاگت کا فنکشن کتنا حساس ہے۔ پچھلے وزن اور پچھلے تعصبات۔

88
00:06:43,680 --> 00:06:47,940
اور آپ کو لگتا ہے کہ یہ ایک حد سے زیادہ سادہ مثال ہے، کیونکہ تمام تہوں میں ایک

89
00:06:47,940 --> 00:06:51,320
نیوران ہوتا ہے، اور چیزیں ایک حقیقی نیٹ ورک کے لیے تیزی سے پیچیدہ ہوتی جا رہی ہیں۔

90
00:06:51,320 --> 00:06:56,560
لیکن ایمانداری سے، جب ہم تہوں کو ایک سے زیادہ نیوران دیتے ہیں تو اتنی تبدیلیاں

91
00:06:56,560 --> 00:06:59,320
نہیں ہوتیں، حقیقت میں یہ صرف چند اور اشاریوں پر نظر رکھنے کے لیے ہے۔

92
00:06:59,320 --> 00:07:03,580
کسی دی گئی پرت کو صرف AL ہونے کی وجہ سے ایکٹیویشن کرنے کے بجائے،

93
00:07:03,580 --> 00:07:07,920
اس میں ایک سب اسکرپٹ بھی ہوگا جو اس پرت کا کون سا نیورون ہے۔

94
00:07:07,920 --> 00:07:15,280
آئیے لیئر L-1 کو انڈیکس کرنے کے لیے حرف k کا استعمال کریں، اور L کو انڈیکس کرنے کے لیے j کا استعمال کریں۔

95
00:07:15,280 --> 00:07:20,720
لاگت کے لیے، ہم دوبارہ دیکھتے ہیں کہ مطلوبہ آؤٹ پٹ کیا ہے، لیکن اس بار ہم ان

96
00:07:20,720 --> 00:07:26,120
آخری پرت کی ایکٹیویشن اور مطلوبہ آؤٹ پٹ کے درمیان فرق کے مربع کو شامل کرتے ہیں۔

97
00:07:26,120 --> 00:07:33,280
یعنی، آپ ALj مائنس yj مربع سے زیادہ رقم لیتے ہیں۔

98
00:07:33,280 --> 00:07:36,500
چونکہ بہت زیادہ وزن ہے، اس لیے ہر ایک کے پاس دو دو مزید اشاریے

99
00:07:36,500 --> 00:07:41,380
ہونے چاہئیں تاکہ یہ معلوم ہو سکے کہ یہ کہاں ہے، تو آئیے اس kth

100
00:07:41,380 --> 00:07:45,740
نیورون کو jth نیورون، WLjk سے جوڑنے والے کنارے کے وزن کو کہتے ہیں۔

101
00:07:45,740 --> 00:07:49,820
وہ اشاریے پہلے تو تھوڑا پیچھے کی طرف محسوس کر سکتے ہیں، لیکن یہ اس بات کے مطابق ہے کہ آپ کس

102
00:07:49,820 --> 00:07:53,800
طرح وزن کے میٹرکس کو انڈیکس کریں گے جس کے بارے میں میں نے حصہ 1 ویڈیو میں بات کی تھی۔

103
00:07:53,800 --> 00:07:57,660
بالکل پہلے کی طرح، متعلقہ وزنی رقم کو ایک نام دینا اب بھی

104
00:07:57,660 --> 00:08:03,540
اچھا ہے، جیسا کہ z، تاکہ آخری تہہ کا ایکٹیویشن صرف آپ

105
00:08:03,540 --> 00:08:04,980
کا خاص فنکشن ہو، جیسا کہ sigmoid، z پر لاگو ہوتا ہے۔

106
00:08:04,980 --> 00:08:09,100
آپ دیکھ سکتے ہیں کہ میرا کیا مطلب ہے، جہاں یہ سب بنیادی طور پر وہی مساوات ہیں جو ہمارے پاس

107
00:08:09,100 --> 00:08:15,420
ون نیورون فی پرت کے معاملے میں پہلے تھے، یہ صرف اتنا ہے کہ یہ تھوڑا زیادہ پیچیدہ لگتا ہے۔

108
00:08:15,420 --> 00:08:20,620
اور درحقیقت، سلسلہ اصول اخذ کرنے والا اظہار یہ بیان کرتا ہے کہ قیمت ایک

109
00:08:20,620 --> 00:08:23,540
مخصوص وزن کے لیے کتنی حساس ہے بنیادی طور پر ایک جیسی نظر آتی ہے۔

110
00:08:23,540 --> 00:08:29,420
میں اسے آپ پر چھوڑ دوں گا کہ اگر آپ چاہیں تو ان شرائط میں سے ہر ایک کے بارے میں توقف کریں اور سوچیں۔

111
00:08:29,420 --> 00:08:34,900
یہاں کیا تبدیلی آتی ہے، اگرچہ، L-1 پرت میں ایک

112
00:08:34,900 --> 00:08:37,820
ایکٹیویشن کے حوالے سے لاگت کا اخذ کیا جاتا ہے۔

113
00:08:37,820 --> 00:08:42,000
اس معاملے میں، فرق یہ ہے کہ نیورون متعدد مختلف

114
00:08:42,000 --> 00:08:43,540
راستوں سے لاگت کے فنکشن کو متاثر کرتا ہے۔

115
00:08:43,540 --> 00:08:51,200
یعنی، ایک طرف، یہ AL0 کو متاثر کرتا ہے، جو لاگت کے فنکشن میں کردار

116
00:08:51,200 --> 00:08:56,460
ادا کرتا ہے، لیکن اس کا اثر AL1 پر بھی ہوتا ہے، جو لاگت کے

117
00:08:56,460 --> 00:09:00,340
فنکشن میں بھی کردار ادا کرتا ہے، اور آپ کو ان کو شامل کرنا ہوگا۔

118
00:09:00,340 --> 00:09:03,680
اور یہ، ٹھیک ہے، یہ بہت زیادہ ہے.

119
00:09:03,680 --> 00:09:08,240
ایک بار جب آپ جان لیں کہ لاگت کا فنکشن اس دوسری سے

120
00:09:08,240 --> 00:09:12,520
آخری پرت میں ایکٹیویشن کے لیے کتنا حساس ہے، تو آپ اس پرت

121
00:09:12,520 --> 00:09:13,920
میں شامل تمام وزن اور تعصبات کے لیے عمل کو دہرا سکتے ہیں۔

122
00:09:13,920 --> 00:09:15,420
تو اپنے آپ کو پیٹھ پر تھپتھپائیں!

123
00:09:15,420 --> 00:09:20,480
اگر یہ سب کچھ سمجھ میں آتا ہے، تو اب آپ نے بیک پروپیگیشن کے

124
00:09:20,480 --> 00:09:23,700
دل کی گہرائی میں دیکھا ہے، جو کہ اعصابی نیٹ ورک کیسے سیکھتے ہیں۔

125
00:09:23,700 --> 00:09:27,960
یہ سلسلہ اصول اظہار آپ کو مشتقات فراہم کرتے ہیں جو گریڈینٹ میں ہر ایک جز کا تعین کرتے ہیں

126
00:09:27,960 --> 00:09:35,020
جو بار بار نیچے کی طرف قدم بڑھا کر نیٹ ورک کی لاگت کو کم کرنے میں مدد کرتا ہے۔

127
00:09:35,020 --> 00:09:38,960
اگر آپ بیٹھ کر ان سب کے بارے میں سوچتے ہیں، تو یہ آپ کے دماغ کو لپیٹنے کے لیے پیچیدگی کی بہت

128
00:09:38,960 --> 00:09:42,840
سی تہیں ہیں، اس لیے فکر نہ کریں کہ کیا آپ کے دماغ کو یہ سب ہضم ہونے میں وقت لگتا ہے۔

