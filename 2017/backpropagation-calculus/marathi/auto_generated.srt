1
00:00:04,020 --> 00:00:06,817
बॅकप्रोपॅगेशन अल्गोरिदमचा अंतर्ज्ञानी वॉकथ्रू 

2
00:00:06,817 --> 00:00:09,920
देत तुम्ही भाग 3 पाहिला आहे हे येथे कठीण गृहीत आहे.

3
00:00:11,040 --> 00:00:14,220
येथे आपण थोडे अधिक औपचारिक मिळवू आणि संबंधित कॅल्क्युलसमध्ये जाऊ.

4
00:00:14,820 --> 00:00:17,370
हे कमीतकमी थोडे गोंधळात टाकणारे असणे सामान्य आहे, 

5
00:00:17,370 --> 00:00:21,400
म्हणून नियमितपणे विराम द्या आणि विचार करा हा मंत्र इतर कोठेही तितकाच लागू होतो.

6
00:00:21,940 --> 00:00:25,559
मशीन लर्निंगमधील लोक सामान्यतः नेटवर्कच्या संदर्भात कॅल्क्युलसच्या साखळी 

7
00:00:25,559 --> 00:00:28,781
नियमाबद्दल कसे विचार करतात हे दाखवणे हे आमचे मुख्य उद्दिष्ट आहे, 

8
00:00:28,781 --> 00:00:32,846
जे बहुतेक प्रास्ताविक कॅल्क्युलस अभ्यासक्रम या विषयाशी कसे संपर्क साधतात यापेक्षा 

9
00:00:32,846 --> 00:00:33,640
वेगळी भावना आहे.

10
00:00:34,340 --> 00:00:37,044
तुमच्यापैकी जे संबंधित कॅल्क्युलसमध्ये अस्वस्थ आहेत त्यांच्यासाठी, 

11
00:00:37,044 --> 00:00:38,740
माझ्याकडे या विषयावरील संपूर्ण मालिका आहे.

12
00:00:39,960 --> 00:00:43,289
चला एका अत्यंत सोप्या नेटवर्कपासून सुरुवात करूया, 

13
00:00:43,289 --> 00:00:46,020
जिथे प्रत्येक लेयरमध्ये एकच न्यूरॉन असतो.

14
00:00:46,320 --> 00:00:50,570
हे नेटवर्क तीन वजन आणि तीन पूर्वाग्रहांद्वारे निर्धारित केले जाते आणि या 

15
00:00:50,570 --> 00:00:54,880
चलांसाठी किमतीचे कार्य किती संवेदनशील आहे हे समजून घेणे हे आमचे ध्येय आहे.

16
00:00:55,419 --> 00:00:58,935
अशा प्रकारे आम्हाला माहित आहे की त्या अटींमध्ये कोणते 

17
00:00:58,935 --> 00:01:02,320
समायोजन खर्च कार्यामध्ये सर्वात प्रभावीपणे कमी करेल.

18
00:01:02,320 --> 00:01:04,840
आम्ही फक्त शेवटच्या दोन न्यूरॉन्समधील कनेक्शनवर लक्ष केंद्रित करू.

19
00:01:05,980 --> 00:01:09,499
चला त्या शेवटच्या न्यूरॉनच्या सक्रियतेला सुपरस्क्रिप्ट L सह लेबल करू, 

20
00:01:09,499 --> 00:01:11,360
ते कोणत्या लेयरमध्ये आहे हे दर्शविते.

21
00:01:11,680 --> 00:01:15,560
तर मागील न्यूरॉनचे सक्रियकरण AL-1 आहे.

22
00:01:16,360 --> 00:01:19,976
हे घातांक नाहीत, ते फक्त आपण ज्याबद्दल बोलत आहोत ते अनुक्रमित करण्याचा एक मार्ग आहे, 

23
00:01:19,976 --> 00:01:23,040
कारण मला नंतर वेगवेगळ्या निर्देशांकांसाठी सबस्क्रिप्ट जतन करायच्या आहेत.

24
00:01:23,720 --> 00:01:29,618
दिलेल्या प्रशिक्षण उदाहरणासाठी हे शेवटचे सक्रियकरण व्हायचे आहे असे मानू या, 

25
00:01:29,618 --> 00:01:32,180
उदाहरणार्थ, y 0 किंवा 1 असू शकते.

26
00:01:32,840 --> 00:01:39,240
त्यामुळे एका प्रशिक्षण उदाहरणासाठी या नेटवर्कची किंमत AL-y2 आहे.

27
00:01:40,260 --> 00:01:44,380
आम्ही त्या एका प्रशिक्षण उदाहरणाची किंमत c0 म्हणून दर्शवू.

28
00:01:45,900 --> 00:01:51,961
एक स्मरणपत्र म्हणून, हे शेवटचे सक्रियकरण वजनाने ठरवले जाते, ज्याला मी wL म्हणणार आहे, 

29
00:01:51,961 --> 00:01:57,600
पूर्वीच्या न्यूरॉनच्या सक्रियतेच्या पट आणि काही पूर्वाग्रह, ज्याला मी bL म्हणेन.

30
00:01:57,600 --> 00:02:01,320
मग तुम्ही ते सिग्मॉइड किंवा ReLU सारख्या काही विशेष नॉनलाइनर फंक्शनद्वारे पंप करता.

31
00:02:01,800 --> 00:02:05,050
या भारित रकमेला z सारखे, संबंधित सक्रियतेच्या समान 

32
00:02:05,050 --> 00:02:09,320
सुपरस्क्रिप्टसह एक विशेष नाव दिल्यास ते आपल्यासाठी खरोखर सोपे होईल.

33
00:02:10,380 --> 00:02:15,087
हे अनेक अटी आहेत, आणि तुम्‍ही याची कल्पना करण्‍याचा एक मार्ग असा आहे की वजन, 

34
00:02:15,087 --> 00:02:19,977
मागील कृती आणि पूर्वाग्रह या सर्वांचा एकत्रितपणे z मोजण्‍यासाठी वापर केला जातो, 

35
00:02:19,977 --> 00:02:25,480
ज्यामुळे आम्‍हाला a ची गणना करू देते, जे शेवटी स्थिरांक y सह, करू देते. आम्ही किंमत मोजतो.

36
00:02:27,340 --> 00:02:32,020
आणि अर्थातच, AL-1 स्वतःचे वजन आणि पूर्वाग्रह आणि अशा गोष्टींनी प्रभावित आहे, 

37
00:02:32,020 --> 00:02:35,060
परंतु आम्ही आत्ता त्यावर लक्ष केंद्रित करणार नाही.

38
00:02:35,700 --> 00:02:37,620
हे सर्व फक्त संख्या आहेत, बरोबर?

39
00:02:38,060 --> 00:02:41,040
आणि प्रत्येकाची स्वतःची छोटी संख्यारेषा आहे असा विचार करणे छान आहे.

40
00:02:41,720 --> 00:02:45,468
आमचे पहिले ध्येय हे समजून घेणे आहे की खर्चाचे कार्य 

41
00:02:45,468 --> 00:02:49,000
आमच्या वजनातील लहान बदलांसाठी किती संवेदनशील आहे.

42
00:02:49,540 --> 00:02:54,860
किंवा वाक्प्रचार वेगळ्या पद्धतीने, wL च्या संदर्भात c चे व्युत्पन्न काय आहे?

43
00:02:55,600 --> 00:03:00,722
जेव्हा तुम्ही हे डेल w शब्द पाहता, तेव्हा याचा अर्थ w ला काही लहान धक्का, 

44
00:03:00,722 --> 00:03:06,606
जसे की 0 ने बदल असा विचार करा.01, आणि या del c शब्दाचा अर्थ असा विचार करा की परिणामी 

45
00:03:06,606 --> 00:03:08,060
किंमतीला धक्का लागेल.

46
00:03:08,060 --> 00:03:10,220
त्यांचे प्रमाण आपल्याला हवे आहे.

47
00:03:11,260 --> 00:03:16,169
वैचारिकदृष्ट्या, wL ला या लहान नजमुळे zL ला काही धक्का बसतो, 

48
00:03:16,169 --> 00:03:21,240
ज्यामुळे AL ला काही धक्का बसतो, ज्याचा थेट खर्चावर परिणाम होतो.

49
00:03:23,120 --> 00:03:27,476
म्हणून आपण प्रथम zL आणि w या लहान बदलाचे गुणोत्तर, 

50
00:03:27,476 --> 00:03:33,200
म्हणजे wL च्या संदर्भात zL चे डेरिव्हेटिव्ह बघून गोष्टी खंडित करतो.

51
00:03:33,200 --> 00:03:39,051
त्याचप्रमाणे, तुम्ही नंतर AL मधील बदल आणि zL मधील लहान बदलाचे गुणोत्तर, 

52
00:03:39,051 --> 00:03:44,660
तसेच अंतिम नज ते c आणि हे मध्यवर्ती नज AL मधील गुणोत्तर विचारात घ्या.

53
00:03:45,740 --> 00:03:50,610
येथे हा साखळी नियम आहे, जेथे या तीन गुणोत्तरांचा गुणाकार 

54
00:03:50,610 --> 00:03:55,140
केल्याने wL मधील लहान बदलांना c ची संवेदनशीलता मिळते.

55
00:03:56,880 --> 00:04:01,842
तर आत्ता स्क्रीनवर, बरीच चिन्हे आहेत आणि ते सर्व काय आहेत हे स्पष्ट करण्यासाठी 

56
00:04:01,842 --> 00:04:06,240
थोडा वेळ घ्या, कारण आता आपण संबंधित डेरिव्हेटिव्ह्जची गणना करणार आहोत.

57
00:04:07,440 --> 00:04:14,180
AL च्या संदर्भात c चे व्युत्पन्न 2AL-y आहे.

58
00:04:14,180 --> 00:04:18,436
याचा अर्थ त्याचा आकार नेटवर्कचे आउटपुट आणि आम्हाला पाहिजे असलेल्या 

59
00:04:18,436 --> 00:04:22,883
गोष्टीमधील फरकाच्या प्रमाणात आहे, म्हणून जर ते आउटपुट खूप वेगळे असेल, 

60
00:04:22,883 --> 00:04:27,140
तर अगदी थोडासा बदल देखील अंतिम खर्चाच्या कार्यावर मोठा परिणाम करेल.

61
00:04:27,840 --> 00:04:32,829
zL च्या संदर्भात AL चे व्युत्पन्न फक्त आमच्या सिग्मॉइड फंक्शनचे व्युत्पन्न 

62
00:04:32,829 --> 00:04:37,420
आहे किंवा तुम्ही वापरण्यासाठी निवडलेल्या कोणत्याही नॉनलाइनरिटीचे आहे.

63
00:04:37,420 --> 00:04:46,160
wL च्या संदर्भात zL चे व्युत्पन्न AL-1 आहे.

64
00:04:46,160 --> 00:04:49,744
मला तुमच्याबद्दल माहिती नाही, पण मला वाटतं की फॉर्म्युलामध्ये डोकं अडकवणं सोपं 

65
00:04:49,744 --> 00:04:53,420
आहे आणि एक क्षणही मागे न बसता आणि त्या सर्वांचा अर्थ काय आहे याची आठवण करून द्या.

66
00:04:53,920 --> 00:04:58,497
या शेवटच्या डेरिव्हेटिव्हच्या बाबतीत, वजनाच्या लहान नजने शेवटच्या थरावर 

67
00:04:58,497 --> 00:05:02,820
किती प्रभाव टाकला हे मागील न्यूरॉन किती मजबूत आहे यावर अवलंबून असते.

68
00:05:03,380 --> 00:05:08,280
लक्षात ठेवा, इथेच न्यूरॉन्स-त्या-फायर-टूगेदर-वायर-टूगेदरची कल्पना येते.

69
00:05:09,200 --> 00:05:12,775
आणि हे सर्व केवळ एका विशिष्ट प्रशिक्षण उदाहरणासाठी 

70
00:05:12,775 --> 00:05:15,720
खर्चाच्या wL च्या संदर्भात व्युत्पन्न आहे.

71
00:05:16,440 --> 00:05:20,612
संपूर्ण किमतीच्या कार्यामध्ये अनेक भिन्न प्रशिक्षण उदाहरणांमध्ये त्या 

72
00:05:20,612 --> 00:05:23,772
सर्व खर्चांची एकत्रित सरासरी करणे समाविष्ट असल्याने, 

73
00:05:23,772 --> 00:05:28,660
त्याच्या व्युत्पन्नास सर्व प्रशिक्षण उदाहरणांवर या अभिव्यक्तीची सरासरी आवश्यक आहे.

74
00:05:28,660 --> 00:05:31,775
अर्थात, ग्रेडियंट व्हेक्टरचा हा फक्त एक घटक आहे, 

75
00:05:31,775 --> 00:05:36,161
जो त्या सर्व वजन आणि पूर्वाग्रहांच्या संदर्भात खर्च फंक्शनच्या आंशिक 

76
00:05:36,161 --> 00:05:38,260
डेरिव्हेटिव्हमधून तयार केला जातो.

77
00:05:40,640 --> 00:05:44,084
परंतु आपल्याला आवश्यक असलेल्या अनेक आंशिक डेरिव्हेटिव्हजपैकी हे फक्त एक असले तरी, 

78
00:05:44,084 --> 00:05:45,260
ते 50% पेक्षा जास्त काम आहे.

79
00:05:46,340 --> 00:05:49,720
पूर्वाग्रहाची संवेदनशीलता, उदाहरणार्थ, जवळजवळ समान आहे.

80
00:05:50,040 --> 00:05:55,020
आम्हाला फक्त डेल झेड डेल बी साठी ही डेल झेड डेल डब्ल्यू संज्ञा बदलण्याची आवश्यकता आहे.

81
00:05:58,420 --> 00:06:02,400
आणि तुम्ही संबंधित सूत्र पाहिल्यास, ते व्युत्पन्न 1 आहे.

82
00:06:06,140 --> 00:06:09,774
तसेच, आणि इथेच पाठीमागे प्रचार करण्याची कल्पना येते, 

83
00:06:09,774 --> 00:06:15,740
तुम्ही हे पाहू शकता की हे खर्चाचे कार्य मागील लेयरच्या सक्रियतेसाठी किती संवेदनशील आहे.

84
00:06:15,740 --> 00:06:20,413
अर्थात, चेन नियम अभिव्यक्तीमधील हे प्रारंभिक व्युत्पन्न, 

85
00:06:20,413 --> 00:06:25,660
मागील सक्रियकरणासाठी z ची संवेदनशीलता, वजन wL म्हणून बाहेर येते.

86
00:06:26,640 --> 00:06:32,011
आणि पुन्हा, जरी आम्ही त्या मागील लेयर सक्रियतेवर थेट प्रभाव पाडण्यास सक्षम नसलो तरी, 

87
00:06:32,011 --> 00:06:37,320
त्याचा मागोवा ठेवणे उपयुक्त आहे, कारण आता आम्ही खर्चाचे कार्य किती संवेदनशील आहे हे 

88
00:06:37,320 --> 00:06:42,440
पाहण्यासाठी हीच साखळी नियम कल्पना मागे ठेवू शकतो. मागील वजन आणि मागील पूर्वाग्रह.

89
00:06:43,180 --> 00:06:45,991
आणि तुम्हाला वाटेल की हे एक अत्यंत साधे उदाहरण आहे, 

90
00:06:45,991 --> 00:06:49,992
कारण सर्व स्तरांमध्ये एक न्यूरॉन आहे आणि वास्तविक नेटवर्कसाठी गोष्टी अधिक 

91
00:06:49,992 --> 00:06:51,020
क्लिष्ट होणार आहेत.

92
00:06:51,700 --> 00:06:55,580
पण प्रामाणिकपणे, जेव्हा आपण लेयर्सना अनेक न्यूरॉन्स देतो तेव्हा इतके बदल होत नाहीत, 

93
00:06:55,580 --> 00:06:58,860
खरोखर हे फक्त काही अधिक निर्देशांक आहेत ज्याचा मागोवा ठेवणे आवश्यक आहे.

94
00:06:59,380 --> 00:07:03,041
दिलेल्या लेयरला फक्त AL असण्याऐवजी, त्या लेयरचा 

95
00:07:03,041 --> 00:07:07,160
न्यूरॉन कोणता आहे हे दर्शवणारी सबस्क्रिप्ट देखील असेल.

96
00:07:07,160 --> 00:07:14,420
लेयर L-1 इंडेक्स करण्यासाठी k हे अक्षर वापरू आणि L लेयर इंडेक्स करण्यासाठी j.

97
00:07:15,260 --> 00:07:19,113
खर्चासाठी, इच्छित आउटपुट काय आहे ते आपण पुन्हा पाहतो, 

98
00:07:19,113 --> 00:07:25,180
परंतु यावेळी आपण या शेवटच्या लेयर सक्रियकरण आणि इच्छित आउटपुटमधील फरकांचे वर्ग जोडतो.

99
00:07:26,080 --> 00:07:30,840
म्हणजेच, तुम्ही ALj वजा yj वर्गाची बेरीज करता.

100
00:07:33,040 --> 00:07:36,936
बरेच जास्त वजन असल्याने, ते कुठे आहे याचा मागोवा ठेवण्यासाठी 

101
00:07:36,936 --> 00:07:40,129
प्रत्येकाकडे आणखी दोन निर्देशांक असणे आवश्यक आहे, 

102
00:07:40,129 --> 00:07:44,920
म्हणून या kth न्यूरॉनला jth न्यूरॉन, WLjk शी जोडणाऱ्या काठाचे वजन म्हणू या.

103
00:07:45,620 --> 00:07:47,970
ते निर्देशांक सुरुवातीला थोडे मागे वाटू शकतात, 

104
00:07:47,970 --> 00:07:51,570
परंतु मी भाग 1 व्हिडिओमध्ये ज्या वेट मॅट्रिक्सबद्दल बोललो ते तुम्ही कसे 

105
00:07:51,570 --> 00:07:53,120
इंडेक्स कराल याच्याशी ते जुळते.

106
00:07:53,620 --> 00:07:58,026
पूर्वीप्रमाणेच, z प्रमाणे, संबंधित भारित बेरीजला नाव देणे अजूनही छान आहे, 

107
00:07:58,026 --> 00:08:02,969
जेणेकरून शेवटच्या लेयरचे सक्रियकरण हे फक्त तुमचे विशेष कार्य आहे, जसे की सिग्मॉइड, 

108
00:08:02,969 --> 00:08:04,160
z ला लागू केले जाते.

109
00:08:04,660 --> 00:08:08,279
मला काय म्हणायचे आहे ते तुम्ही पाहू शकता, जिथे ही सर्व मूलत: 

110
00:08:08,279 --> 00:08:11,781
समान समीकरणे आहेत जी एक-न्यूरॉन-पर-लेयर प्रकरणात आधी होती, 

111
00:08:11,781 --> 00:08:13,680
फक्त ते थोडे अधिक क्लिष्ट दिसते.

112
00:08:15,440 --> 00:08:19,230
आणि खरंच, विशिष्ट वजनासाठी किंमत किती संवेदनशील आहे याचे 

113
00:08:19,230 --> 00:08:23,420
वर्णन करणारी साखळी नियम व्युत्पन्न अभिव्यक्ती मूलत: समान दिसते.

114
00:08:23,920 --> 00:08:26,840
मी ते तुमच्यावर सोडेन आणि तुम्हाला हवे असल्यास त्या प्रत्येक अटींबद्दल विचार करा.

115
00:08:28,979 --> 00:08:36,659
येथे काय बदल होतो, तथापि, लेयर L-1 मधील एका सक्रियतेच्या संदर्भात किमतीचे व्युत्पन्न आहे.

116
00:08:37,780 --> 00:08:40,357
या प्रकरणात, फरक असा आहे की न्यूरॉन अनेक भिन्न 

117
00:08:40,357 --> 00:08:42,880
मार्गांद्वारे खर्चाच्या कार्यावर प्रभाव पाडतो.

118
00:08:44,680 --> 00:08:50,021
म्हणजेच, एकीकडे, ते AL0 वर प्रभाव टाकते, जे किमतीच्या कार्यामध्ये भूमिका बजावते, 

119
00:08:50,021 --> 00:08:54,242
परंतु त्याचा AL1 वर देखील प्रभाव असतो, जो किमतीच्या कार्यामध्ये 

120
00:08:54,242 --> 00:08:57,540
देखील भूमिका बजावतो आणि तुम्हाला ते जोडावे लागतील.

121
00:08:59,820 --> 00:09:03,040
आणि ते, तसेच, ते खूपच जास्त आहे.

122
00:09:03,500 --> 00:09:06,570
या दुसर्‍या-ते-अंतिम लेयरमधील सक्रियतेसाठी किमतीचे कार्य किती 

123
00:09:06,570 --> 00:09:09,740
संवेदनशील आहे हे एकदा तुम्हाला कळले की, त्या लेयरमध्ये भरणाऱ्या 

124
00:09:09,740 --> 00:09:12,860
सर्व वजन आणि पूर्वाग्रहांसाठी तुम्ही प्रक्रिया पुन्हा करू शकता.

125
00:09:13,900 --> 00:09:14,960
म्हणून पाठीवर थाप द्या!

126
00:09:15,300 --> 00:09:20,238
जर या सर्व गोष्टींचा अर्थ असेल तर, तुम्ही आता बॅकप्रोपॅगेशनच्या हृदयात खोलवर पाहिले आहे, 

127
00:09:20,238 --> 00:09:22,680
न्यूरल नेटवर्क कसे शिकतात यामागील वर्कहोर्स.

128
00:09:23,300 --> 00:09:28,238
हे साखळी नियम अभिव्यक्ती तुम्हाला डेरिव्हेटिव्ह देतात जे ग्रेडियंटमधील प्रत्येक 

129
00:09:28,238 --> 00:09:33,300
घटक निर्धारित करतात जे वारंवार उतारावर जाऊन नेटवर्कची किंमत कमी करण्यात मदत करतात.

130
00:09:34,300 --> 00:09:36,575
जर तुम्ही बसून या सर्व गोष्टींचा विचार केला तर, 

131
00:09:36,575 --> 00:09:39,420
तुमच्या मनाला गुंडाळण्यासाठी हे अनेक गुंतागुंतीचे पदर आहेत, 

132
00:09:39,420 --> 00:09:42,740
त्यामुळे तुमच्या मनाला ते सर्व पचवायला वेळ लागत असेल तर काळजी करू नका.

