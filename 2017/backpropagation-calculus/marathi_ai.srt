1
00:00:00,000 --> 00:00:08,420
बॅकप्रोपॅगेशन अल्गोरिदमचा अंतर्ज्ञानी वॉकथ्रू देत तुम्ही भाग 3

2
00:00:08,420 --> 00:00:11,160
पाहिला आहे हे येथे कठीण गृहीत आहे.

3
00:00:11,160 --> 00:00:14,920
येथे आपण थोडे अधिक औपचारिक मिळवू आणि संबंधित कॅल्क्युलसमध्ये जाऊ.

4
00:00:14,920 --> 00:00:18,560
हे कमीतकमी थोडे गोंधळात टाकणारे असणे सामान्य आहे, म्हणून नियमितपणे विराम

5
00:00:18,560 --> 00:00:22,000
द्या आणि विचार करा हा मंत्र इतर कोठेही तितकाच लागू होतो.

6
00:00:22,000 --> 00:00:26,620
मशीन लर्निंगमधील लोक सामान्यतः नेटवर्कच्या संदर्भात कॅल्क्युलसच्या साखळी नियमाबद्दल कसे विचार

7
00:00:26,620 --> 00:00:31,900
करतात हे दाखवणे हे आमचे मुख्य उद्दिष्ट आहे, जे बहुतेक प्रास्ताविक

8
00:00:31,900 --> 00:00:34,580
कॅल्क्युलस अभ्यासक्रम या विषयाशी कसे संपर्क साधतात यापेक्षा वेगळी भावना आहे.

9
00:00:34,580 --> 00:00:38,300
तुमच्यापैकी जे संबंधित कॅल्क्युलसमध्ये अस्वस्थ आहेत त्यांच्यासाठी,

10
00:00:38,300 --> 00:00:39,300
माझ्याकडे या विषयावरील संपूर्ण मालिका आहे.

11
00:00:39,300 --> 00:00:44,840
चला एका अत्यंत सोप्या नेटवर्कपासून सुरुवात करूया,

12
00:00:44,840 --> 00:00:46,780
जिथे प्रत्येक लेयरमध्ये एकच न्यूरॉन असतो.

13
00:00:46,780 --> 00:00:51,880
हे नेटवर्क तीन वजन आणि तीन पूर्वाग्रहांद्वारे निर्धारित केले जाते आणि या चलांसाठी

14
00:00:51,880 --> 00:00:55,640
किमतीचे कार्य किती संवेदनशील आहे हे समजून घेणे हे आमचे ध्येय आहे.

15
00:00:55,640 --> 00:00:59,780
अशा प्रकारे आम्हाला माहित आहे की त्या अटींमध्ये

16
00:00:59,780 --> 00:01:01,100
कोणते समायोजन खर्च कार्यामध्ये सर्वात प्रभावीपणे कमी करेल.

17
00:01:01,100 --> 00:01:05,360
आम्ही फक्त शेवटच्या दोन न्यूरॉन्समधील कनेक्शनवर लक्ष केंद्रित करू.

18
00:01:05,360 --> 00:01:10,400
चला त्या शेवटच्या न्यूरॉनच्या सक्रियतेला सुपरस्क्रिप्ट L सह

19
00:01:10,400 --> 00:01:11,800
लेबल करू, ते कोणत्या लेयरमध्ये आहे हे दर्शविते.

20
00:01:11,800 --> 00:01:16,560
तर मागील न्यूरॉनचे सक्रियकरण AL-1 आहे.

21
00:01:16,560 --> 00:01:20,120
हे घातांक नाहीत, ते फक्त आपण ज्याबद्दल बोलत आहोत ते अनुक्रमित करण्याचा

22
00:01:20,120 --> 00:01:23,120
एक मार्ग आहे, कारण मला नंतर वेगवेगळ्या निर्देशांकांसाठी सबस्क्रिप्ट जतन करायच्या आहेत.

23
00:01:23,600 --> 00:01:28,880
दिलेल्या प्रशिक्षण उदाहरणासाठी हे शेवटचे सक्रियकरण व्हायचे आहे असे

24
00:01:28,880 --> 00:01:33,020
मानू या, उदाहरणार्थ, y 0 किंवा 1 असू शकते.

25
00:01:33,020 --> 00:01:39,040
त्यामुळे एका प्रशिक्षण उदाहरणासाठी या नेटवर्कची किंमत AL-y2 आहे.

26
00:01:39,040 --> 00:01:46,120
आम्ही त्या एका प्रशिक्षण उदाहरणाची किंमत c0 म्हणून दर्शवू.

27
00:01:46,120 --> 00:01:51,920
एक स्मरणपत्र म्हणून, हे शेवटचे सक्रियकरण वजनाने ठरवले जाते, ज्याला मी wL म्हणणार

28
00:01:51,920 --> 00:01:57,600
आहे, पूर्वीच्या न्यूरॉनच्या सक्रियतेच्या पट आणि काही पूर्वाग्रह, ज्याला मी bL म्हणेन.

29
00:01:57,600 --> 00:02:01,560
मग तुम्ही ते सिग्मॉइड किंवा ReLU सारख्या काही विशेष नॉनलाइनर फंक्शनद्वारे पंप करता.

30
00:02:01,560 --> 00:02:05,400
या भारित रकमेला z सारखे, संबंधित सक्रियतेच्या समान सुपरस्क्रिप्टसह

31
00:02:05,400 --> 00:02:10,600
एक विशेष नाव दिल्यास ते आपल्यासाठी खरोखर सोपे होईल.

32
00:02:10,600 --> 00:02:15,320
हे अनेक अटी आहेत, आणि तुम्‍ही याची कल्पना करण्‍याचा एक मार्ग असा आहे की वजन,

33
00:02:15,320 --> 00:02:21,800
मागील कृती आणि पूर्वाग्रह या सर्वांचा एकत्रितपणे z मोजण्‍यासाठी वापर केला जातो, ज्यामुळे आम्‍हाला a

34
00:02:21,800 --> 00:02:27,360
ची गणना करू देते, जे शेवटी स्थिरांक y सह, करू देते. आम्ही किंमत मोजतो.

35
00:02:27,360 --> 00:02:33,440
आणि अर्थातच, AL-1 स्वतःचे वजन आणि पूर्वाग्रह आणि अशा गोष्टींनी

36
00:02:33,440 --> 00:02:35,920
प्रभावित आहे, परंतु आम्ही आत्ता त्यावर लक्ष केंद्रित करणार नाही.

37
00:02:35,920 --> 00:02:38,120
हे सर्व फक्त संख्या आहेत, बरोबर?

38
00:02:38,120 --> 00:02:41,960
आणि प्रत्येकाची स्वतःची छोटी संख्यारेषा आहे असा विचार करणे छान आहे.

39
00:02:41,960 --> 00:02:47,480
आमचे पहिले ध्येय हे समजून घेणे आहे की खर्चाचे

40
00:02:47,480 --> 00:02:49,820
कार्य आमच्या वजनातील लहान बदलांसाठी किती संवेदनशील आहे.

41
00:02:49,820 --> 00:02:55,740
किंवा वाक्प्रचार वेगळ्या पद्धतीने, wL च्या संदर्भात c चे व्युत्पन्न काय आहे?

42
00:02:55,740 --> 00:03:01,220
जेव्हा तुम्ही हे डेल w शब्द पाहता, तेव्हा याचा अर्थ w ला काही लहान धक्का, जसे की 0 ने बदल असा विचार करा.

43
00:03:01,220 --> 00:03:08,820
01, आणि या del c शब्दाचा अर्थ असा विचार करा की परिणामी किंमतीला धक्का लागेल.

44
00:03:08,820 --> 00:03:10,900
त्यांचे प्रमाण आपल्याला हवे आहे.

45
00:03:10,900 --> 00:03:17,740
वैचारिकदृष्ट्या, wL ला या लहान नजमुळे zL ला काही धक्का बसतो,

46
00:03:17,740 --> 00:03:23,220
ज्यामुळे AL ला काही धक्का बसतो, ज्याचा थेट खर्चावर परिणाम होतो.

47
00:03:23,220 --> 00:03:28,020
म्हणून आपण प्रथम zL आणि w या लहान बदलाचे गुणोत्तर, म्हणजे

48
00:03:28,020 --> 00:03:33,340
wL च्या संदर्भात zL चे डेरिव्हेटिव्ह बघून गोष्टी खंडित करतो.

49
00:03:33,340 --> 00:03:38,820
त्याचप्रमाणे, तुम्ही नंतर AL मधील बदल आणि zL मधील

50
00:03:38,820 --> 00:03:43,900
लहान बदलाचे गुणोत्तर, तसेच अंतिम नज ते c आणि

51
00:03:43,900 --> 00:03:45,900
हे मध्यवर्ती नज AL मधील गुणोत्तर विचारात घ्या.

52
00:03:45,900 --> 00:03:51,880
येथे हा साखळी नियम आहे, जेथे या तीन गुणोत्तरांचा गुणाकार

53
00:03:51,880 --> 00:03:57,340
केल्याने wL मधील लहान बदलांना c ची संवेदनशीलता मिळते.

54
00:03:57,340 --> 00:04:01,620
तर आत्ता स्क्रीनवर, बरीच चिन्हे आहेत आणि ते सर्व काय आहेत हे स्पष्ट

55
00:04:01,620 --> 00:04:07,460
करण्यासाठी थोडा वेळ घ्या, कारण आता आपण संबंधित डेरिव्हेटिव्ह्जची गणना करणार आहोत.

56
00:04:07,460 --> 00:04:14,220
AL च्या संदर्भात c चे व्युत्पन्न 2AL-y आहे.

57
00:04:14,220 --> 00:04:19,300
याचा अर्थ त्याचा आकार नेटवर्कचे आउटपुट आणि आम्हाला पाहिजे असलेल्या गोष्टीमधील

58
00:04:19,300 --> 00:04:24,480
फरकाच्या प्रमाणात आहे, म्हणून जर ते आउटपुट खूप वेगळे असेल, तर

59
00:04:24,480 --> 00:04:28,380
अगदी थोडासा बदल देखील अंतिम खर्चाच्या कार्यावर मोठा परिणाम करेल.

60
00:04:28,380 --> 00:04:33,860
zL च्या संदर्भात AL चे व्युत्पन्न फक्त आमच्या सिग्मॉइड फंक्शनचे

61
00:04:33,860 --> 00:04:37,420
व्युत्पन्न आहे किंवा तुम्ही वापरण्यासाठी निवडलेल्या कोणत्याही नॉनलाइनरिटीचे आहे.

62
00:04:37,420 --> 00:04:46,180
wL च्या संदर्भात zL चे व्युत्पन्न AL-1 आहे.

63
00:04:46,180 --> 00:04:49,460
मला तुमच्याबद्दल माहिती नाही, पण मला वाटतं की फॉर्म्युलामध्ये डोकं अडकवणं सोपं आहे आणि एक

64
00:04:49,460 --> 00:04:54,180
क्षणही मागे न बसता आणि त्या सर्वांचा अर्थ काय आहे याची आठवण करून द्या.

65
00:04:54,180 --> 00:04:58,860
या शेवटच्या डेरिव्हेटिव्हच्या बाबतीत, वजनाच्या लहान नजने शेवटच्या थरावर किती प्रभाव

66
00:04:58,860 --> 00:05:03,220
टाकला हे मागील न्यूरॉन किती मजबूत आहे यावर अवलंबून असते.

67
00:05:03,220 --> 00:05:09,320
लक्षात ठेवा, इथेच न्यूरॉन्स-त्या-फायर-टूगेदर-वायर-टूगेदरची कल्पना येते.

68
00:05:09,320 --> 00:05:14,840
आणि हे सर्व केवळ एका विशिष्ट प्रशिक्षण

69
00:05:14,840 --> 00:05:16,580
उदाहरणासाठी खर्चाच्या wL च्या संदर्भात व्युत्पन्न आहे.

70
00:05:16,580 --> 00:05:20,940
संपूर्ण किमतीच्या कार्यामध्ये अनेक भिन्न प्रशिक्षण उदाहरणांमध्ये त्या सर्व

71
00:05:20,940 --> 00:05:27,300
खर्चांची एकत्रित सरासरी करणे समाविष्ट असल्याने, त्याच्या व्युत्पन्नास

72
00:05:27,300 --> 00:05:28,540
सर्व प्रशिक्षण उदाहरणांवर या अभिव्यक्तीची सरासरी आवश्यक आहे.

73
00:05:28,540 --> 00:05:33,860
अर्थात, ग्रेडियंट व्हेक्टरचा हा फक्त एक घटक आहे, जो त्या सर्व

74
00:05:33,860 --> 00:05:40,780
वजन आणि पूर्वाग्रहांच्या संदर्भात खर्च फंक्शनच्या आंशिक डेरिव्हेटिव्हमधून तयार केला जातो.

75
00:05:40,780 --> 00:05:44,340
परंतु आपल्याला आवश्यक असलेल्या अनेक आंशिक डेरिव्हेटिव्हजपैकी हे फक्त

76
00:05:44,340 --> 00:05:46,460
एक असले तरी, ते 50% पेक्षा जास्त काम आहे.

77
00:05:46,460 --> 00:05:50,300
पूर्वाग्रहाची संवेदनशीलता, उदाहरणार्थ, जवळजवळ समान आहे.

78
00:05:50,300 --> 00:05:58,980
आम्हाला फक्त डेल झेड डेल बी साठी ही डेल झेड डेल डब्ल्यू संज्ञा बदलण्याची आवश्यकता आहे.

79
00:05:58,980 --> 00:06:04,700
आणि तुम्ही संबंधित सूत्र पाहिल्यास, ते व्युत्पन्न 1 आहे.

80
00:06:04,700 --> 00:06:11,700
तसेच, आणि इथेच पाठीमागे प्रचार करण्याची कल्पना येते, तुम्ही हे पाहू

81
00:06:11,700 --> 00:06:16,180
शकता की हे खर्चाचे कार्य मागील लेयरच्या सक्रियतेसाठी किती संवेदनशील आहे.

82
00:06:16,180 --> 00:06:21,380
अर्थात, चेन नियम अभिव्यक्तीमधील हे प्रारंभिक व्युत्पन्न, मागील सक्रियकरणासाठी

83
00:06:21,380 --> 00:06:25,420
z ची संवेदनशीलता, वजन wL म्हणून बाहेर येते.

84
00:06:25,420 --> 00:06:30,100
आणि पुन्हा, जरी आम्ही त्या मागील लेयर सक्रियतेवर थेट प्रभाव पाडण्यास

85
00:06:30,100 --> 00:06:35,280
सक्षम नसलो तरी, त्याचा मागोवा ठेवणे उपयुक्त आहे, कारण आता

86
00:06:35,280 --> 00:06:40,780
आम्ही खर्चाचे कार्य किती संवेदनशील आहे हे पाहण्यासाठी हीच साखळी

87
00:06:40,780 --> 00:06:43,680
नियम कल्पना मागे ठेवू शकतो. मागील वजन आणि मागील पूर्वाग्रह.

88
00:06:43,680 --> 00:06:47,940
आणि तुम्हाला वाटेल की हे एक अत्यंत साधे उदाहरण आहे, कारण सर्व

89
00:06:47,940 --> 00:06:51,320
स्तरांमध्ये एक न्यूरॉन आहे आणि वास्तविक नेटवर्कसाठी गोष्टी अधिक क्लिष्ट होणार आहेत.

90
00:06:51,320 --> 00:06:56,560
पण प्रामाणिकपणे, जेव्हा आपण लेयर्सना अनेक न्यूरॉन्स देतो तेव्हा इतके बदल होत नाहीत,

91
00:06:56,560 --> 00:06:59,320
खरोखर हे फक्त काही अधिक निर्देशांक आहेत ज्याचा मागोवा ठेवणे आवश्यक आहे.

92
00:06:59,320 --> 00:07:03,580
दिलेल्या लेयरला फक्त AL असण्याऐवजी, त्या लेयरचा न्यूरॉन

93
00:07:03,580 --> 00:07:07,920
कोणता आहे हे दर्शवणारी सबस्क्रिप्ट देखील असेल.

94
00:07:07,920 --> 00:07:15,280
लेयर L-1 इंडेक्स करण्यासाठी k हे अक्षर वापरू आणि L लेयर इंडेक्स करण्यासाठी j.

95
00:07:15,280 --> 00:07:20,720
खर्चासाठी, इच्छित आउटपुट काय आहे ते आपण पुन्हा पाहतो, परंतु यावेळी

96
00:07:20,720 --> 00:07:26,120
आपण या शेवटच्या लेयर सक्रियकरण आणि इच्छित आउटपुटमधील फरकांचे वर्ग जोडतो.

97
00:07:26,120 --> 00:07:33,280
म्हणजेच, तुम्ही ALj वजा yj वर्गाची बेरीज करता.

98
00:07:33,280 --> 00:07:36,500
बरेच जास्त वजन असल्याने, ते कुठे आहे याचा मागोवा ठेवण्यासाठी

99
00:07:36,500 --> 00:07:41,380
प्रत्येकाकडे आणखी दोन निर्देशांक असणे आवश्यक आहे, म्हणून या kth

100
00:07:41,380 --> 00:07:45,740
न्यूरॉनला jth न्यूरॉन, WLjk शी जोडणाऱ्या काठाचे वजन म्हणू या.

101
00:07:45,740 --> 00:07:49,820
ते निर्देशांक सुरुवातीला थोडे मागे वाटू शकतात, परंतु मी भाग 1 व्हिडिओमध्ये

102
00:07:49,820 --> 00:07:53,800
ज्या वेट मॅट्रिक्सबद्दल बोललो ते तुम्ही कसे इंडेक्स कराल याच्याशी ते जुळते.

103
00:07:53,800 --> 00:07:57,660
पूर्वीप्रमाणेच, z प्रमाणे, संबंधित भारित बेरीजला नाव देणे अजूनही छान

104
00:07:57,660 --> 00:08:03,540
आहे, जेणेकरून शेवटच्या लेयरचे सक्रियकरण हे फक्त तुमचे विशेष कार्य

105
00:08:03,540 --> 00:08:04,980
आहे, जसे की सिग्मॉइड, z ला लागू केले जाते.

106
00:08:04,980 --> 00:08:09,100
मला काय म्हणायचे आहे ते तुम्ही पाहू शकता, जिथे ही सर्व मूलत: समान

107
00:08:09,100 --> 00:08:15,420
समीकरणे आहेत जी एक-न्यूरॉन-पर-लेयर प्रकरणात आधी होती, फक्त ते थोडे अधिक क्लिष्ट दिसते.

108
00:08:15,420 --> 00:08:20,620
आणि खरंच, विशिष्ट वजनासाठी किंमत किती संवेदनशील आहे याचे

109
00:08:20,620 --> 00:08:23,540
वर्णन करणारी साखळी नियम व्युत्पन्न अभिव्यक्ती मूलत: समान दिसते.

110
00:08:23,540 --> 00:08:29,420
मी ते तुमच्यावर सोडेन आणि तुम्हाला हवे असल्यास त्या प्रत्येक अटींबद्दल विचार करा.

111
00:08:29,420 --> 00:08:34,900
येथे काय बदल होतो, तथापि, लेयर L-1

112
00:08:34,900 --> 00:08:37,820
मधील एका सक्रियतेच्या संदर्भात किमतीचे व्युत्पन्न आहे.

113
00:08:37,820 --> 00:08:42,000
या प्रकरणात, फरक असा आहे की न्यूरॉन

114
00:08:42,000 --> 00:08:43,540
अनेक भिन्न मार्गांद्वारे खर्चाच्या कार्यावर प्रभाव पाडतो.

115
00:08:43,540 --> 00:08:51,200
म्हणजेच, एकीकडे, ते AL0 वर प्रभाव टाकते, जे किमतीच्या कार्यामध्ये

116
00:08:51,200 --> 00:08:56,460
भूमिका बजावते, परंतु त्याचा AL1 वर देखील प्रभाव असतो, जो

117
00:08:56,460 --> 00:09:00,340
किमतीच्या कार्यामध्ये देखील भूमिका बजावतो आणि तुम्हाला ते जोडावे लागतील.

118
00:09:00,340 --> 00:09:03,680
आणि ते, तसेच, ते खूपच जास्त आहे.

119
00:09:03,680 --> 00:09:08,240
या दुसर्‍या-ते-अंतिम लेयरमधील सक्रियतेसाठी किमतीचे कार्य किती संवेदनशील आहे

120
00:09:08,240 --> 00:09:12,520
हे एकदा तुम्हाला कळले की, त्या लेयरमध्ये भरणाऱ्या सर्व

121
00:09:12,520 --> 00:09:13,920
वजन आणि पूर्वाग्रहांसाठी तुम्ही प्रक्रिया पुन्हा करू शकता.

122
00:09:13,920 --> 00:09:15,420
म्हणून पाठीवर थाप द्या!

123
00:09:15,420 --> 00:09:20,480
जर या सर्व गोष्टींचा अर्थ असेल तर, तुम्ही आता बॅकप्रोपॅगेशनच्या

124
00:09:20,480 --> 00:09:23,700
हृदयात खोलवर पाहिले आहे, न्यूरल नेटवर्क कसे शिकतात यामागील वर्कहोर्स.

125
00:09:23,700 --> 00:09:27,960
हे साखळी नियम अभिव्यक्ती तुम्हाला डेरिव्हेटिव्ह देतात जे ग्रेडियंटमधील प्रत्येक घटक निर्धारित

126
00:09:27,960 --> 00:09:35,020
करतात जे वारंवार उतारावर जाऊन नेटवर्कची किंमत कमी करण्यात मदत करतात.

127
00:09:35,020 --> 00:09:38,960
जर तुम्ही बसून या सर्व गोष्टींचा विचार केला तर, तुमच्या मनाला गुंडाळण्यासाठी हे अनेक गुंतागुंतीचे

128
00:09:38,960 --> 00:09:42,840
पदर आहेत, त्यामुळे तुमच्या मनाला ते सर्व पचवायला वेळ लागत असेल तर काळजी करू नका.

