[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "L'hypothèse difficile ici est que vous avez regardé la partie 3, qui donne une présentation intuitive de l'algorithme de rétropropagation.",
  "from_community_srt": "Je suppose ici que vous ayez regardé la partie 3, vous donnant l'intuition du fonctionnement de l'algorithme de rétropropagation.",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Ici, nous devenons un peu plus formels et plongeons dans le calcul pertinent.",
  "from_community_srt": "Ici, nous allons être un peu plus formel et plonger dans le calcul sous-jacent.",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "Il est normal que cela soit au moins un peu déroutant, donc le mantra de faire régulièrement une pause et de réfléchir s'applique certainement autant ici que partout ailleurs.",
  "from_community_srt": "Il est normal que cela soit un peu déroutant, donc n'hésitez pas à faire pause régulièrement pour prendre le temps de réfléchir",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "Notre objectif principal est de montrer comment les personnes travaillant dans le domaine de l'apprentissage automatique pensent généralement à la règle de chaîne du calcul dans le contexte des réseaux, ce qui a une sensation différente de la façon dont la plupart des cours d'introduction au calcul abordent le sujet.",
  "from_community_srt": "Notre objectif principal est de montrer comment les gens en apprentissage automatique se conçoivent la règle de la chaîne (théorème de dérivation des fonctions composées) dans le contexte des réseaux, qui diffère de la façon dont la plupart des cours introductifs en analyse présentent le sujet.",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Pour ceux d’entre vous qui ne sont pas à l’aise avec le calcul pertinent, j’ai toute une série sur le sujet.",
  "from_community_srt": "Pour ceux d'entre vous qui sont mal à l'aise avec l'analyse, j'ai fait toute une série sur le sujet.",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Commençons par un réseau extrêmement simple, dans lequel chaque couche contient un seul neurone.",
  "from_community_srt": "Commençons par un réseau extrêmement simple, celui où chaque couche est composé d'un seul neurone.",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Ce réseau est déterminé par trois poids et trois biais, et notre objectif est de comprendre dans quelle mesure la fonction de coût est sensible à ces variables.",
  "from_community_srt": "Donc ce réseau particulier est déterminé par 3 poids et 3 biais, et notre objectif est de comprendre à quel point la fonction de coût est sensible à ces variables.",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way, we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "De cette façon, nous savons quels ajustements de ces termes entraîneront la diminution la plus efficace de la fonction de coût.",
  "from_community_srt": "De cette façon, nous savons quelles modifications de ces termes va entraîner la baisse la plus efficace de la fonction de coût.",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "And we're just going to focus on the connection between the last two neurons.",
  "translatedText": "Et nous allons juste nous concentrer sur la connexion entre les deux derniers neurones.",
  "from_community_srt": "Et nous nous concentrons sur la connexion entre les deux derniers neurones.",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in, so the activation of the previous neuron is Al-1.",
  "translatedText": "Marquons l'activation de ce dernier neurone avec un exposant L, indiquant dans quelle couche il se trouve, donc l'activation du neurone précédent est Al-1.",
  "from_community_srt": "Notons l'activation de ce dernier neurone avec un exposant L, indiquant dans quelle couche il se trouve, donc l'activation du neurone précédent est L-1.",
  "n_reviews": 0,
  "start": 65.98,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Ce ne sont pas des exposants, ils sont juste un moyen d'indexer ce dont nous parlons, puisque je souhaite enregistrer ultérieurement les indices de différents indices.",
  "from_community_srt": "Ce ne sont pas des exposants, c'est juste un indiçage, et je souhaite utiliser l'indiçage inférieur plus tard pour une autre dénotation.",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Disons que la valeur que nous souhaitons que cette dernière activation ait pour un exemple de formation donné est y, par exemple, y peut être 0 ou 1.",
  "from_community_srt": "Disons que la valeur que nous voulons que cette dernière activation soit pour un exemple d'entraînement donné est y. Par exemple, y pourrait être 0 ou 1.",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is Al-y2.",
  "translatedText": "Le coût de ce réseau pour un seul exemple de formation est donc Al-y2.",
  "from_community_srt": "Ainsi, le coût de ce réseau simple pour un seul exemple d'apprentissage est (a^(L) - y)^2.",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Nous désignerons le coût de cet exemple de formation par c0.",
  "from_community_srt": "Nous noterons le coût de cet exemple d'entraînement C_0.",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neuron's activation plus some bias, which I'll call BL.",
  "translatedText": "Pour rappel, cette dernière activation est déterminée par un poids, que j'appellerai WL, multiplié par l'activation du neurone précédent plus un biais, que j'appellerai BL.",
  "from_community_srt": "On se rappelle que cette dernière activation est déterminée par un poids, que je vais appeler w^L multiplié par l'activation du neurone précédent, plus un certain biais,",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "Et puis vous pompez cela via une fonction non linéaire spéciale comme le sigmoïde ou ReLU.",
  "from_community_srt": "que j'appellerai b^L, puis on injecte cela dans une fonction non linéaire spécifique comme un sigmoïde ou un ReLU.",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "Cela va en fait nous faciliter la tâche si nous donnons un nom spécial à cette somme pondérée, comme z, avec le même exposant que les activations concernées.",
  "from_community_srt": "Cela nous facilitera la tâche si nous donnons un nom spécial à cette somme pondérée, comme z, avec le même exposant que l'activation correspondante.",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action and the bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "Cela fait beaucoup de termes, et une façon dont vous pourriez le conceptualiser est que le poids, l'action précédente et le biais sont tous utilisés pour calculer z, ce qui nous permet à son tour de calculer a, qui finalement, avec un y constant, permet nous calculons le coût.",
  "from_community_srt": "Il y a donc beaucoup de termes. Et une façon de conceptualiser cela est que le poids, l'activation précédente, et le biais sont combinés pour calculer z, qui à son tour nous permet de calculer a, qui, avec la constante y, nous permet de calculer le coût.",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course Al-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "Et bien sûr, l'Al-1 est influencé par son propre poids et ses préjugés, mais nous n'allons pas nous concentrer là-dessus pour le moment.",
  "from_community_srt": "Et bien sûr, un a^(L-1) est influencé par son propre poids et son propre biais, et autres. Mais nous n'allons pas nous concentrer là-dessus maintenant.",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Tout cela ne sont que des chiffres, n'est-ce pas ?",
  "from_community_srt": "Tous ces termes ne ne sont que des chiffres,",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "Et il peut être agréable de penser que chacun a sa propre petite droite numérique.",
  "from_community_srt": "n'est-ce pas? Et ça peut être utile de penser que chacun a sa propre petite ligne de valeurs possibles.",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight WL.",
  "translatedText": "Notre premier objectif est de comprendre à quel point la fonction de coût est sensible aux petits changements de notre poids WL.",
  "from_community_srt": "Notre premier objectif est de comprendre à quel point la fonction de coût est sensible aux petits changements de notre poids w^(L).",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to WL?",
  "translatedText": "Ou une expression différente, quelle est la dérivée de c par rapport à WL ?",
  "from_community_srt": "En d'autres termes, quelle est la dérivée partielle de C par rapport à w^(L).",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del W term, think of it as meaning some tiny nudge to W, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Lorsque vous voyez ce terme del W, pensez-y comme signifiant un petit coup de pouce à W, comme un changement de 0,01, et pensez à ce terme del c comme signifiant quel que soit le coup de pouce résultant du coût.",
  "from_community_srt": "Quand vous voyez ce terme \"∂w\", pensez-y comme un tout petit décalage dans la valeur de w, par exemple de 0.01. Et pensez à ce terme \"∂C\" comme le décalage provoqué sur la valeur du coût C.",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "Ce que nous voulons, c'est leur ratio.",
  "from_community_srt": "Ce que nous voulons, c'est le ratio des deux.",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Conceptuellement, ce petit coup de pouce vers WL entraîne un certain coup de pouce vers ZL, qui à son tour provoque un certain coup de pouce vers AL, ce qui influence directement le coût.",
  "from_community_srt": "Conceptuellement, ce minuscule décalage de w^(L) provoque un certain décalage de z^(L) ce qui provoque à son tour un décalage de a^(L), ce qui influence directement le coût.",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is, the derivative of ZL with respect to WL.",
  "translatedText": "Nous décomposons donc les choses en examinant d’abord le rapport d’un petit changement de ZL à ce petit changement W, c’est-à-dire la dérivée de ZL par rapport à WL.",
  "from_community_srt": "Nous décomposons donc ceci en examinant d'abord le ratio d'un minuscule changement de z^(L) sur un minuscule changement de w ^ (L). C'est-à-dire, la dérivée partielle de z^(L) par rapport à w^(L).",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "De même, vous considérez ensuite le rapport entre le changement de AL et le petit changement de ZL qui l'a provoqué, ainsi que le rapport entre le coup de pouce final vers c et ce coup de pouce intermédiaire vers AL.",
  "from_community_srt": "De même, vous considérez alors le quotient d'un changement de a^(L) sur un minuscule changement de z^(L) qui l'a causé, ainsi que le quotient entre le décalage final de C et le décalage intermédiaire de a^(L).",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of c to small changes in WL.",
  "translatedText": "C'est ici la règle de la chaîne, où la multiplication de ces trois ratios nous donne la sensibilité de c aux petits changements de WL.",
  "from_community_srt": "Ceci est la règle de la chaîne (FR = théorème de dérivation des fonctions composées), qui nous permet, en multipliant ces trois fractions, d'obtenir la sensibilité de C aux petits changements de w^L.",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "Donc, à l'écran en ce moment, il y a beaucoup de symboles, et prenez un moment pour vous assurer que ce qu'ils sont tous est clair, car nous allons maintenant calculer les dérivées pertinentes.",
  "from_community_srt": "A l'écran en ce moment, il y a un tas de symboles, alors prenez un moment pour vous assurer d'avoir compris ce que chacun représente, parce que maintenant nous allons calculer chacune des dérivées partielles.",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "La dérivée de c par rapport à AL s’avère être 2AL-y.",
  "from_community_srt": "La dérivée partielle de C par rapport à a^(L) est 2*(a^(L) - y).",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "Notice this means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Notez que cela signifie que sa taille est proportionnelle à la différence entre la production du réseau et ce que nous voulons qu'il soit, donc si cette production était très différente, même de légers changements risquent d'avoir un impact important sur la fonction de coût finale.",
  "from_community_srt": "Notez que cela signifie que sa taille est proportionnelle à la différence entre la sortie du réseau et la valeur que nous voulons qu'elle soit. Donc, si cette sortie était très différente, même de légères modifications peuvent avoir un impact important sur la fonction de coût.",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "La dérivée de AL par rapport à ZL est simplement la dérivée de notre fonction sigmoïde, ou quelle que soit la non-linéarité que vous choisissez d'utiliser.",
  "from_community_srt": "La dérivée partielle de a^(L) par rapport à z^(L) n'est que la dérivée de notre fonction sigmoïde, ou quelle que soit la fonction d'activation que vous choisissez d'utiliser.",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "And the derivative of ZL with respect to WL comes out to be AL-1.",
  "translatedText": "Et la dérivée de ZL par rapport à WL s'avère être AL-1.",
  "from_community_srt": "Et la dérivée partielle de z^(L) par rapport à w^(L), est tout simplement a^(L-1).",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all mean.",
  "translatedText": "Maintenant, je ne sais pas pour vous, mais je pense qu'il est facile de rester coincé tête baissée dans les formules sans prendre un moment pour s'asseoir et se rappeler ce qu'elles signifient toutes.",
  "from_community_srt": "Maintenant, je ne sais pas pour vous, mais je pense qu'il est facile de ne plus rien y voir là-dedans sans prendre un moment pour s'asseoir et se rappeler de ce que chaque terme représente.",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "Dans le cas de cette dernière dérivée, la mesure dans laquelle le petit coup de pouce a influencé la dernière couche dépend de la force du neurone précédent.",
  "from_community_srt": "Dans le cas de cette dernière dérivée partielle, l'influence sur la dernière couche d'un petit décalage dans la valeur de ce poids dépend de la force du neurone précédent.",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "N’oubliez pas que c’est là qu’intervient l’idée des neurones qui s’allument ensemble.",
  "from_community_srt": "Rappelez-vous, c'est ici qu'intervient l'idée « Les neurones qui s'excitent ensemble se lient entre eux. » (D.Hebb, 1949).",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to WL only of the cost for a specific single training example.",
  "translatedText": "Et tout cela n'est que la dérivée par rapport à WL du coût d'un exemple de formation unique spécifique.",
  "from_community_srt": "Et tout cela est la dérivée partielle par rapport à w^(L) du coût pour un seul exemple d'entraînement spécifique.",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Étant donné que la fonction de coût complet implique de faire la moyenne de tous ces coûts sur de nombreux exemples de formation différents, sa dérivée nécessite de faire la moyenne de cette expression sur tous les exemples de formation.",
  "from_community_srt": "Puisque la fonction de coût complète implique de moyenniser tous ces coûts pour de nombreux exemples d'entraînement, sa dérivée partielle nécessite de moyenniser l'expression que nous avons trouvée sur tous les exemples d'entraînement.",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "And of course, that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "Et bien sûr, ce n’est qu’une composante du vecteur gradient, qui lui-même est construit à partir des dérivées partielles de la fonction de coût par rapport à tous ces poids et biais.",
  "from_community_srt": "Et bien sûr, ce n'est qu'une composante du vecteur de gradient, qui est lui-même construit à partir des dérivées partielles de la fonction de coût par rapport à tous ces poids et biais.",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "Mais même si ce n’est qu’une des nombreuses dérivées partielles dont nous avons besoin, cela représente plus de 50 % du travail.",
  "from_community_srt": "Mais même si ce n'était que d'une de ces dérivés partielles dont nous avons besoin, c'est plus de 50% du travail.",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "La sensibilité au biais, par exemple, est quasiment identique.",
  "from_community_srt": "La sensibilité au biais, par exemple, est presque identique.",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Nous avons juste besoin de remplacer ce terme del z del w par un del z del b.",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "Et si vous regardez la formule pertinente, cette dérivée s’avère être 1.",
  "from_community_srt": "Nous avons juste besoin de changer ce terme ∂z/∂w pour un ∂z/∂b, Et si vous regardez cette formule, on voit que la dérivée est égale à 1.",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "Aussi, et c'est là qu'intervient l'idée de propagation vers l'arrière, vous pouvez voir à quel point cette fonction de coût est sensible à l'activation de la couche précédente.",
  "from_community_srt": "Par ailleurs, et c'est maintenant que l'idée de rétro-propagation entre en jeu, vous pouvez voir à quel point cette fonction de coût est sensible à l'activation de la couche précédente;",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight WL.",
  "translatedText": "À savoir, cette dérivée initiale dans l’expression de la règle de chaîne, la sensibilité de z à l’activation précédente, s’avère être le poids WL.",
  "from_community_srt": "à savoir, cette dérivée initiale dans le développement de la règle de la chaîne, la sensibilité de z à l'activation précédente est le poids w^(L).",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "Et encore une fois, même si nous ne pourrons pas influencer directement l'activation de la couche précédente, il est utile d'en garder la trace, car maintenant nous pouvons simplement continuer à répéter cette même idée de règle de chaîne à l'envers pour voir à quel point la fonction de coût est sensible à pondérations précédentes et biais antérieurs.",
  "from_community_srt": "Encore une fois, même si nous ne serons pas en mesure d'influencer directement cette activation, il est utile d'en garder une trace parce que maintenant nous pouvons continuer à répéter cette idée de règle de chaîne, à rebours, pour voir à quel point la fonction de coût est sensible aux poids précédents et aux biais antérieurs.",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "Et vous pourriez penser qu’il s’agit d’un exemple trop simple, puisque toutes les couches ont un neurone, et les choses vont devenir exponentiellement plus compliquées pour un réseau réel.",
  "from_community_srt": "Et vous pourriez penser que c'est un exemple trop simple, puisque toutes les couches n'ont qu'un neurone, et les choses vont devenir exponentiellement plus compliquées dans le vrai réseau.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "Mais honnêtement, cela ne change pas beaucoup lorsque nous donnons plusieurs neurones aux couches, ce ne sont en réalité que quelques indices supplémentaires à suivre.",
  "from_community_srt": "Mais honnêtement, il n'y a pas tellement de changements lorsque nous mettons plusieurs neurones dans chaque couche. C'est juste l'histoire de quelques indices en plus.",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "Plutôt que l'activation d'une couche donnée soit simplement AL, elle aura également un indice indiquant de quel neurone de cette couche il s'agit.",
  "from_community_srt": "Au lieu que l'activation d'une couche donnée soit simplement a^(L), il va aussi y avoir un indice sur l'emplacement du neurone dans la couche.",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Utilisons la lettre k pour indexer le calque L-1, et j pour indexer le calque L.",
  "from_community_srt": "Allons-y, utilisons la lettre k pour indexer la couche (L-1), et j pour indexer la couche (L).",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "Pour le coût, nous regardons encore une fois quel est le résultat souhaité, mais cette fois nous additionnons les carrés des différences entre ces activations de dernière couche et le résultat souhaité.",
  "from_community_srt": "Pour le coût, encore une fois nous regardons ce que la sortie désirée est. Mais cette fois nous additionnons les carrés des différences entre ces activations de la dernière couche et la sortie désirée.",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus Yj squared.",
  "translatedText": "Autrement dit, vous prenez une somme supérieure à ALj moins Yj au carré.",
  "from_community_srt": "Autrement dit,",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Comme il y a beaucoup plus de poids, chacun doit avoir quelques indices supplémentaires pour savoir où il se trouve, appelons donc le poids du bord reliant ce kème neurone au jème neurone, WLjk.",
  "from_community_srt": "vous prenez une somme sur (a_j^(L) - y_j)^2 Comme il y a beaucoup plus de poids, chacun doit avoir des indices supplémentaires pour savoir où il se trouve. Alors appelons le poids de l'arête reliant ce k-ème neurone au j-ème neurone w_ {jk}^(L).",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "Ces indices peuvent sembler un peu rétrogrades au début, mais cela correspond à la façon dont vous indexeriez la matrice de pondération dont j'ai parlé dans la vidéo de la première partie.",
  "from_community_srt": "Ces indices pourraient donner l'impression d'être dans la mauvais sens, mais cela correspond à la façon dont vous indexeriez la matrice des poids dont j'ai parlé dans la vidéo de la partie 1.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "Comme avant, il est toujours agréable de donner un nom à la somme pondérée pertinente, comme z, afin que l'activation de la dernière couche soit simplement votre fonction spéciale, comme la sigmoïde, appliquée à z.",
  "from_community_srt": "Comme auparavant, il est toujours agréable de donner un nom à la somme pondérée, comme z, de sorte que l'activation de la dernière couche est juste votre fonction d'activation, comme le sigmoïde, appliquée à z.",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Vous pouvez voir ce que je veux dire, où toutes ces équations sont essentiellement les mêmes que celles que nous avions auparavant dans le cas d'un neurone par couche, c'est juste que cela semble un peu plus compliqué.",
  "from_community_srt": "Vous pouvez voir ce que je veux dire, non? Ce sont fondamentalement les mêmes équations que celles que nous avions auparavant dans le cas d'un neurone par couche. Ca a juste l'air un peu plus compliqué.",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain-ruled derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "Et en effet, l’expression dérivée en chaîne décrivant la sensibilité du coût à un poids spécifique semble essentiellement la même.",
  "from_community_srt": "Et en effet, le développement de la dérivée par la règle de la chaîne décrivant la sensibilité du coût à un poids spécifique a fondamentalement la même tête.",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Je vous laisse le soin de faire une pause et de réfléchir à chacun de ces termes si vous le souhaitez.",
  "from_community_srt": "Je vais vous laisser faire une pause et réfléchir à chacun de ces termes si vous le souhaitez.",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "Ce qui change ici, cependant, c'est la dérivée du coût par rapport à l'une des activations de la couche L-1.",
  "from_community_srt": "Ce qui change ici, cependant, est la dérivée du coût par rapport à l'une des activations dans la couche (L-1).",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "Dans ce cas, la différence est que le neurone influence la fonction de coût par plusieurs voies différentes.",
  "from_community_srt": "Dans ce cas, la différence est que le neurone influence la fonction de coût sur plusieurs chemins.",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "Autrement dit, d’une part, cela influence AL0, qui joue un rôle dans la fonction de coût, mais cela a également une influence sur AL1, qui joue également un rôle dans la fonction de coût, et il faut les additionner.",
  "from_community_srt": "C'est-à-dire, d'une part, il influence a_0^(L), qui joue un rôle dans la fonction de coût, mais il a aussi une influence sur a_1^(L), qui joue aussi un rôle dans la fonction de coût. Et vous devez les ajouter.",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "Et ça, eh bien, c'est à peu près tout.",
  "from_community_srt": "Et ça ... et bien c'est à peu près tout.",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Une fois que vous savez à quel point la fonction de coût est sensible aux activations de cette avant-dernière couche, vous pouvez simplement répéter le processus pour tous les poids et biais alimentant cette couche.",
  "from_community_srt": "Une fois que vous savez à quel point la fonction de coût est sensible aux activations de la deuxième à la dernière couche, vous pouvez simplement répéter le processus pour tous les poids et les biais alimentant cette couche.",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "Alors félicitez-vous !",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Si tout cela a du sens, vous avez maintenant approfondi le cœur de la rétropropagation, le cheval de bataille derrière la façon dont les réseaux neuronaux apprennent.",
  "from_community_srt": "Alors réjouissez-vous ! Si tout cela a du sens à vos yeux, et bien vous avez regardé en détail le mécanisme interne de la rétro-propagation, l'appareillage qui permet aux réseaux de neurones d'apprendre.",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Ces expressions de règles de chaîne vous donnent les dérivées qui déterminent chaque composant du gradient qui permet de minimiser le coût du réseau en descendant à plusieurs reprises.",
  "from_community_srt": "Ces développements par la règle de la chaîne vous fournissent les dérivées qui déterminent chaque composante dans le gradient qui permet de minimiser le coût du réseau en descendant d'un cran de manière répétée.",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Si vous vous asseyez et réfléchissez à tout cela, cela représente beaucoup de niveaux de complexité à comprendre, alors ne vous inquiétez pas s'il faut du temps à votre esprit pour tout digérer.",
  "from_community_srt": "Pfiouuu. Si vous vous asseyez et pensez à tout cela, C'est beaucoup de niveaux de complexité auxquels s'habituer. Donc, ne vous inquiétez pas s'il vous faut du temps pour digérer tout cela.",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]