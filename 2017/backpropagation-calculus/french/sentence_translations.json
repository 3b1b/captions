[
 {
  "translatedText": "L'hypothèse difficile ici est que vous avez regardé la partie 3, qui donne une présentation intuitive de l'algorithme de rétropropagation.",
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "time_range": [
   4.02,
   9.92
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ici, nous devenons un peu plus formels et plongeons dans le calcul pertinent.",
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "time_range": [
   11.04,
   14.22
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Il est normal que cela soit au moins un peu déroutant, donc le mantra de faire régulièrement une pause et de réfléchir s'applique certainement autant ici que partout ailleurs.",
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "time_range": [
   14.82,
   21.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Notre objectif principal est de montrer comment les personnes travaillant dans le domaine de l'apprentissage automatique pensent généralement à la règle de chaîne du calcul dans le contexte des réseaux, ce qui a une sensation différente de la façon dont la plupart des cours d'introduction au calcul abordent le sujet.",
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "time_range": [
   21.94,
   33.64
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Pour ceux d’entre vous qui ne sont pas à l’aise avec le calcul pertinent, j’ai toute une série sur le sujet.",
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "time_range": [
   34.34,
   38.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Commençons par un réseau extrêmement simple, dans lequel chaque couche contient un seul neurone.",
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "time_range": [
   39.96,
   46.02
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ce réseau est déterminé par trois poids et trois biais, et notre objectif est de comprendre dans quelle mesure la fonction de coût est sensible à ces variables.",
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "time_range": [
   46.32,
   54.88
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "De cette façon, nous savons quels ajustements de ces termes entraîneront la diminution la plus efficace de la fonction de coût.",
  "input": "That way, we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "time_range": [
   55.42,
   60.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Et nous allons juste nous concentrer sur la connexion entre les deux derniers neurones.",
  "input": "And we're just going to focus on the connection between the last two neurons.",
  "time_range": [
   61.96,
   64.84
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Marquons l'activation de ce dernier neurone avec un exposant L, indiquant dans quelle couche il se trouve, donc l'activation du neurone précédent est Al-1.",
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in, so the activation of the previous neuron is Al-1.",
  "time_range": [
   65.98,
   75.56
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ce ne sont pas des exposants, ils sont juste un moyen d'indexer ce dont nous parlons, puisque je souhaite enregistrer ultérieurement les indices de différents indices.",
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "time_range": [
   76.36,
   83.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Disons que la valeur que nous souhaitons que cette dernière activation ait pour un exemple de formation donné est y, par exemple, y peut être 0 ou 1.",
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "time_range": [
   83.72,
   92.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Le coût de ce réseau pour un seul exemple de formation est donc Al-y2.",
  "input": "So the cost of this network for a single training example is Al-y2.",
  "time_range": [
   92.84,
   99.24
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Nous désignerons le coût de cet exemple de formation par c0.",
  "input": "We'll denote the cost of that one training example as c0.",
  "time_range": [
   100.26,
   104.38
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Pour rappel, cette dernière activation est déterminée par un poids, que j'appellerai WL, multiplié par l'activation du neurone précédent plus un biais, que j'appellerai BL.",
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neuron's activation plus some bias, which I'll call BL.",
  "time_range": [
   105.9,
   116.64
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Et puis vous pompez cela via une fonction non linéaire spéciale comme le sigmoïde ou ReLU.",
  "input": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "time_range": [
   117.42,
   121.32
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Cela va en fait nous faciliter la tâche si nous donnons un nom spécial à cette somme pondérée, comme z, avec le même exposant que les activations concernées.",
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "time_range": [
   121.8,
   129.32
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Cela fait beaucoup de termes, et une façon dont vous pourriez le conceptualiser est que le poids, l'action précédente et le biais sont tous utilisés pour calculer z, ce qui nous permet à son tour de calculer a, qui finalement, avec un y constant, permet nous calculons le coût.",
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action and the bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "time_range": [
   130.38,
   145.48
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Et bien sûr, l'Al-1 est influencé par son propre poids et ses préjugés, mais nous n'allons pas nous concentrer là-dessus pour le moment.",
  "input": "And of course Al-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "time_range": [
   147.34,
   155.06
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Tout cela ne sont que des chiffres, n'est-ce pas ?",
  "input": "All of these are just numbers, right?",
  "time_range": [
   155.7,
   157.62
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Et il peut être agréable de penser que chacun a sa propre petite droite numérique.",
  "input": "And it can be nice to think of each one as having its own little number line.",
  "time_range": [
   158.06,
   161.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Notre premier objectif est de comprendre à quel point la fonction de coût est sensible aux petits changements de notre poids WL.",
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight WL.",
  "time_range": [
   161.72,
   169.0
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ou une expression différente, quelle est la dérivée de c par rapport à WL ?",
  "input": "Or phrase differently, what is the derivative of c with respect to WL?",
  "time_range": [
   169.54,
   174.86
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Lorsque vous voyez ce terme del W, pensez-y comme signifiant un petit coup de pouce à W, comme un changement de 0,01, et pensez à ce terme del c comme signifiant quel que soit le coup de pouce résultant du coût.",
  "input": "When you see this del W term, think of it as meaning some tiny nudge to W, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "time_range": [
   175.6,
   188.06
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ce que nous voulons, c'est leur ratio.",
  "input": "What we want is their ratio.",
  "time_range": [
   188.06,
   190.22
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Conceptuellement, ce petit coup de pouce vers WL entraîne un certain coup de pouce vers ZL, qui à son tour provoque un certain coup de pouce vers AL, ce qui influence directement le coût.",
  "input": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost.",
  "time_range": [
   191.26,
   201.24
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Nous décomposons donc les choses en examinant d’abord le rapport d’un petit changement de ZL à ce petit changement W, c’est-à-dire la dérivée de ZL par rapport à WL.",
  "input": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is, the derivative of ZL with respect to WL.",
  "time_range": [
   203.12,
   213.2
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "De même, vous considérez ensuite le rapport entre le changement de AL et le petit changement de ZL qui l'a provoqué, ainsi que le rapport entre le coup de pouce final vers c et ce coup de pouce intermédiaire vers AL.",
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "time_range": [
   213.2,
   224.66
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "C'est ici la règle de la chaîne, où la multiplication de ces trois ratios nous donne la sensibilité de c aux petits changements de WL.",
  "input": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of c to small changes in WL.",
  "time_range": [
   225.74,
   235.14
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Donc, à l'écran en ce moment, il y a beaucoup de symboles, et prenez un moment pour vous assurer que ce qu'ils sont tous est clair, car nous allons maintenant calculer les dérivées pertinentes.",
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "time_range": [
   236.88,
   246.24
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "La dérivée de c par rapport à AL s’avère être 2AL-y.",
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "time_range": [
   247.44,
   253.16
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Notez que cela signifie que sa taille est proportionnelle à la différence entre la production du réseau et ce que nous voulons qu'il soit, donc si cette production était très différente, même de légers changements risquent d'avoir un impact important sur la fonction de coût finale.",
  "input": "Notice this means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "time_range": [
   253.98,
   267.14
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "La dérivée de AL par rapport à ZL est simplement la dérivée de notre fonction sigmoïde, ou quelle que soit la non-linéarité que vous choisissez d'utiliser.",
  "input": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "time_range": [
   267.84,
   276.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Et la dérivée de ZL par rapport à WL s'avère être AL-1.",
  "input": "And the derivative of ZL with respect to WL comes out to be AL-1.",
  "time_range": [
   277.22,
   284.66
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Maintenant, je ne sais pas pour vous, mais je pense qu'il est facile de rester coincé tête baissée dans les formules sans prendre un moment pour s'asseoir et se rappeler ce qu'elles signifient toutes.",
  "input": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all mean.",
  "time_range": [
   285.76,
   293.42
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Dans le cas de cette dernière dérivée, la mesure dans laquelle le petit coup de pouce a influencé la dernière couche dépend de la force du neurone précédent.",
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "time_range": [
   293.92,
   302.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "N’oubliez pas que c’est là qu’intervient l’idée des neurones qui s’allument ensemble.",
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "time_range": [
   303.38,
   308.28
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Et tout cela n'est que la dérivée par rapport à WL du coût d'un exemple de formation unique spécifique.",
  "input": "And all of this is the derivative with respect to WL only of the cost for a specific single training example.",
  "time_range": [
   309.2,
   315.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Étant donné que la fonction de coût complet implique de faire la moyenne de tous ces coûts sur de nombreux exemples de formation différents, sa dérivée nécessite de faire la moyenne de cette expression sur tous les exemples de formation.",
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "time_range": [
   316.44,
   327.46
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Et bien sûr, ce n’est qu’une composante du vecteur gradient, qui lui-même est construit à partir des dérivées partielles de la fonction de coût par rapport à tous ces poids et biais.",
  "input": "And of course, that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "time_range": [
   328.38,
   338.26
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Mais même si ce n’est qu’une des nombreuses dérivées partielles dont nous avons besoin, cela représente plus de 50 % du travail.",
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "time_range": [
   340.64,
   345.26
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "La sensibilité au biais, par exemple, est quasiment identique.",
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "time_range": [
   346.34,
   349.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Nous avons juste besoin de remplacer ce terme del z del w par un del z del b.",
  "input": "We just need to change out this del z del w term for a del z del b.",
  "time_range": [
   350.04,
   355.02
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Et si vous regardez la formule pertinente, cette dérivée s’avère être 1.",
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "time_range": [
   358.42,
   362.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Aussi, et c'est là qu'intervient l'idée de propagation vers l'arrière, vous pouvez voir à quel point cette fonction de coût est sensible à l'activation de la couche précédente.",
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "time_range": [
   366.14,
   375.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "À savoir, cette dérivée initiale dans l’expression de la règle de chaîne, la sensibilité de z à l’activation précédente, s’avère être le poids WL.",
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight WL.",
  "time_range": [
   375.74,
   385.66
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Et encore une fois, même si nous ne pourrons pas influencer directement l'activation de la couche précédente, il est utile d'en garder la trace, car maintenant nous pouvons simplement continuer à répéter cette même idée de règle de chaîne à l'envers pour voir à quel point la fonction de coût est sensible à pondérations précédentes et biais antérieurs.",
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "time_range": [
   386.64,
   402.44
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Et vous pourriez penser qu’il s’agit d’un exemple trop simple, puisque toutes les couches ont un neurone, et les choses vont devenir exponentiellement plus compliquées pour un réseau réel.",
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "time_range": [
   403.18,
   411.02
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Mais honnêtement, cela ne change pas beaucoup lorsque nous donnons plusieurs neurones aux couches, ce ne sont en réalité que quelques indices supplémentaires à suivre.",
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "time_range": [
   411.7,
   418.86
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Plutôt que l'activation d'une couche donnée soit simplement AL, elle aura également un indice indiquant de quel neurone de cette couche il s'agit.",
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "time_range": [
   419.38,
   427.16
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Utilisons la lettre k pour indexer le calque L-1, et j pour indexer le calque L.",
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "time_range": [
   427.16,
   434.42
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Pour le coût, nous regardons encore une fois quel est le résultat souhaité, mais cette fois nous additionnons les carrés des différences entre ces activations de dernière couche et le résultat souhaité.",
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "time_range": [
   435.26,
   445.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Autrement dit, vous prenez une somme supérieure à ALj moins Yj au carré.",
  "input": "That is, you take a sum over ALj minus Yj squared.",
  "time_range": [
   446.08,
   450.84
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Comme il y a beaucoup plus de poids, chacun doit avoir quelques indices supplémentaires pour savoir où il se trouve, appelons donc le poids du bord reliant ce kème neurone au jème neurone, WLjk.",
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "time_range": [
   453.04,
   464.92
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ces indices peuvent sembler un peu rétrogrades au début, mais cela correspond à la façon dont vous indexeriez la matrice de pondération dont j'ai parlé dans la vidéo de la première partie.",
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "time_range": [
   465.62,
   473.12
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Comme avant, il est toujours agréable de donner un nom à la somme pondérée pertinente, comme z, afin que l'activation de la dernière couche soit simplement votre fonction spéciale, comme la sigmoïde, appliquée à z.",
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "time_range": [
   473.62,
   484.16
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Vous pouvez voir ce que je veux dire, où toutes ces équations sont essentiellement les mêmes que celles que nous avions auparavant dans le cas d'un neurone par couche, c'est juste que cela semble un peu plus compliqué.",
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "time_range": [
   484.66,
   493.68
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Et en effet, l’expression dérivée en chaîne décrivant la sensibilité du coût à un poids spécifique semble essentiellement la même.",
  "input": "And indeed, the chain-ruled derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "time_range": [
   495.44,
   503.42
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Je vous laisse le soin de faire une pause et de réfléchir à chacun de ces termes si vous le souhaitez.",
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "time_range": [
   503.92,
   506.84
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ce qui change ici, cependant, c'est la dérivée du coût par rapport à l'une des activations de la couche L-1.",
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "time_range": [
   508.98,
   516.66
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Dans ce cas, la différence est que le neurone influence la fonction de coût par plusieurs voies différentes.",
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "time_range": [
   517.78,
   522.88
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Autrement dit, d’une part, cela influence AL0, qui joue un rôle dans la fonction de coût, mais cela a également une influence sur AL1, qui joue également un rôle dans la fonction de coût, et il faut les additionner.",
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "time_range": [
   524.68,
   537.54
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Et ça, eh bien, c'est à peu près tout.",
  "input": "And that, well, that's pretty much it.",
  "time_range": [
   539.82,
   543.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Une fois que vous savez à quel point la fonction de coût est sensible aux activations de cette avant-dernière couche, vous pouvez simplement répéter le processus pour tous les poids et biais alimentant cette couche.",
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "time_range": [
   543.5,
   552.86
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Alors félicitez-vous !",
  "input": "So pat yourself on the back!",
  "time_range": [
   553.9,
   554.96
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Si tout cela a du sens, vous avez maintenant approfondi le cœur de la rétropropagation, le cheval de bataille derrière la façon dont les réseaux neuronaux apprennent.",
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "time_range": [
   555.3,
   562.68
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ces expressions de règles de chaîne vous donnent les dérivées qui déterminent chaque composant du gradient qui permet de minimiser le coût du réseau en descendant à plusieurs reprises.",
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "time_range": [
   563.3,
   573.3
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Si vous vous asseyez et réfléchissez à tout cela, cela représente beaucoup de niveaux de complexité à comprendre, alors ne vous inquiétez pas s'il faut du temps à votre esprit pour tout digérer.",
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "time_range": [
   574.3,
   582.74
  ],
  "n_reviews": 0
 }
]