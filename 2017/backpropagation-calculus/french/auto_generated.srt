1
00:00:04,019 --> 00:00:06,808
L'hypothèse difficile ici est que vous avez regardé la partie 3, 

2
00:00:06,808 --> 00:00:09,920
qui donne une présentation intuitive de l'algorithme de rétropropagation.

3
00:00:11,040 --> 00:00:14,220
Ici, nous devenons un peu plus formels et plongeons dans le calcul pertinent.

4
00:00:14,820 --> 00:00:16,841
Il est normal que cela soit au moins un peu déroutant, 

5
00:00:16,841 --> 00:00:19,745
donc le mantra de faire régulièrement une pause et de réfléchir s'applique 

6
00:00:19,745 --> 00:00:21,400
certainement autant ici que partout ailleurs.

7
00:00:21,940 --> 00:00:24,882
Notre objectif principal est de montrer comment les personnes travaillant dans le 

8
00:00:24,882 --> 00:00:27,682
domaine de l'apprentissage automatique pensent généralement à la règle de 

9
00:00:27,682 --> 00:00:29,369
chaîne du calcul dans le contexte des réseaux, 

10
00:00:29,369 --> 00:00:31,953
ce qui a une sensation différente de la façon dont la plupart des cours 

11
00:00:31,953 --> 00:00:33,640
d'introduction au calcul abordent le sujet.

12
00:00:34,340 --> 00:00:37,354
Pour ceux d’entre vous qui ne sont pas à l’aise avec le calcul pertinent, 

13
00:00:37,354 --> 00:00:38,740
j’ai toute une série sur le sujet.

14
00:00:39,960 --> 00:00:43,116
Commençons par un réseau extrêmement simple, dans 

15
00:00:43,116 --> 00:00:46,020
lequel chaque couche contient un seul neurone.

16
00:00:46,320 --> 00:00:49,316
Ce réseau est déterminé par trois poids et trois biais, 

17
00:00:49,316 --> 00:00:53,542
et notre objectif est de comprendre dans quelle mesure la fonction de coût est 

18
00:00:53,542 --> 00:00:54,880
sensible à ces variables.

19
00:00:55,420 --> 00:00:57,971
De cette façon, nous savons quels ajustements de ces termes 

20
00:00:57,971 --> 00:01:00,820
entraîneront la diminution la plus efficace de la fonction de coût.

21
00:01:01,960 --> 00:01:04,840
Et nous allons juste nous concentrer sur la connexion entre les deux derniers neurones.

22
00:01:05,980 --> 00:01:09,976
Marquons l'activation de ce dernier neurone avec un exposant L, 

23
00:01:09,976 --> 00:01:14,443
indiquant dans quelle couche il se trouve, donc l'activation du neurone 

24
00:01:14,443 --> 00:01:15,560
précédent est Al-1.

25
00:01:16,360 --> 00:01:19,875
Ce ne sont pas des exposants, ils sont juste un moyen d'indexer ce dont nous parlons, 

26
00:01:19,875 --> 00:01:23,040
puisque je souhaite enregistrer ultérieurement les indices de différents indices.

27
00:01:23,720 --> 00:01:27,978
Disons que la valeur que nous souhaitons que cette dernière activation ait 

28
00:01:27,978 --> 00:01:32,180
pour un exemple de formation donné est y, par exemple, y peut être 0 ou 1.

29
00:01:32,840 --> 00:01:39,240
Le coût de ce réseau pour un seul exemple de formation est donc Al-y2.

30
00:01:40,260 --> 00:01:44,380
Nous désignerons le coût de cet exemple de formation par c0.

31
00:01:45,900 --> 00:01:49,869
Pour rappel, cette dernière activation est déterminée par un poids, 

32
00:01:49,869 --> 00:01:53,312
que j'appellerai WL, multiplié par l'activation du 

33
00:01:53,312 --> 00:01:56,640
neurone précédent plus un biais, que j'appellerai BL.

34
00:01:57,420 --> 00:02:01,320
Et puis vous pompez cela via une fonction non linéaire spéciale comme le sigmoïde ou ReLU.

35
00:02:01,800 --> 00:02:05,583
Cela va en fait nous faciliter la tâche si nous donnons un nom spécial à cette 

36
00:02:05,583 --> 00:02:09,320
somme pondérée, comme z, avec le même exposant que les activations concernées.

37
00:02:10,380 --> 00:02:13,872
Cela fait beaucoup de termes, et une façon dont vous pourriez le 

38
00:02:13,872 --> 00:02:17,580
conceptualiser est que le poids, l'action précédente et le biais 

39
00:02:17,580 --> 00:02:21,933
sont tous utilisés pour calculer z, ce qui nous permet à son tour de calculer a, 

40
00:02:21,933 --> 00:02:25,480
qui finalement, avec un y constant, permet nous calculons le coût.

41
00:02:27,340 --> 00:02:31,414
Et bien sûr, l'Al-1 est influencé par son propre poids et ses préjugés, 

42
00:02:31,414 --> 00:02:35,060
mais nous n'allons pas nous concentrer là-dessus pour le moment.

43
00:02:35,700 --> 00:02:37,620
Tout cela ne sont que des chiffres, n'est-ce pas ?

44
00:02:38,060 --> 00:02:41,040
Et il peut être agréable de penser que chacun a sa propre petite droite numérique.

45
00:02:41,720 --> 00:02:45,473
Notre premier objectif est de comprendre à quel point la fonction 

46
00:02:45,473 --> 00:02:49,000
de coût est sensible aux petits changements de notre poids WL.

47
00:02:49,540 --> 00:02:54,860
Ou une expression différente, quelle est la dérivée de c par rapport à WL ?

48
00:02:55,600 --> 00:03:00,855
Lorsque vous voyez ce terme del W, pensez-y comme signifiant un petit coup de pouce à W, 

49
00:03:00,855 --> 00:03:05,166
comme un changement de 0,01, et pensez à ce terme del c comme signifiant 

50
00:03:05,166 --> 00:03:08,060
quel que soit le coup de pouce résultant du coût.

51
00:03:08,060 --> 00:03:10,220
Ce que nous voulons, c'est leur ratio.

52
00:03:11,260 --> 00:03:14,515
Conceptuellement, ce petit coup de pouce vers WL entraîne un 

53
00:03:14,515 --> 00:03:19,265
certain coup de pouce vers ZL, qui à son tour provoque un certain coup de pouce vers AL, 

54
00:03:19,265 --> 00:03:21,240
ce qui influence directement le coût.

55
00:03:23,120 --> 00:03:28,434
Nous décomposons donc les choses en examinant d’abord le rapport d’un petit changement 

56
00:03:28,434 --> 00:03:33,200
de ZL à ce petit changement W, c’est-à-dire la dérivée de ZL par rapport à WL.

57
00:03:33,200 --> 00:03:37,002
De même, vous considérez ensuite le rapport entre le changement de AL et 

58
00:03:37,002 --> 00:03:39,503
le petit changement de ZL qui l'a provoqué, 

59
00:03:39,503 --> 00:03:43,201
ainsi que le rapport entre le coup de pouce final vers c et ce coup de 

60
00:03:43,201 --> 00:03:44,660
pouce intermédiaire vers AL.

61
00:03:45,740 --> 00:03:50,576
C'est ici la règle de la chaîne, où la multiplication de ces trois 

62
00:03:50,576 --> 00:03:55,140
ratios nous donne la sensibilité de c aux petits changements de WL.

63
00:03:56,880 --> 00:03:59,813
Donc, à l'écran en ce moment, il y a beaucoup de symboles, 

64
00:03:59,813 --> 00:04:03,399
et prenez un moment pour vous assurer que ce qu'ils sont tous est clair, 

65
00:04:03,399 --> 00:04:06,240
car nous allons maintenant calculer les dérivées pertinentes.

66
00:04:07,440 --> 00:04:13,160
La dérivée de c par rapport à AL s’avère être 2AL-y.

67
00:04:13,980 --> 00:04:17,338
Notez que cela signifie que sa taille est proportionnelle à la différence 

68
00:04:17,338 --> 00:04:20,469
entre la production du réseau et ce que nous voulons qu'il soit, 

69
00:04:20,469 --> 00:04:22,647
donc si cette production était très différente, 

70
00:04:22,647 --> 00:04:25,914
même de légers changements risquent d'avoir un impact important sur 

71
00:04:25,914 --> 00:04:27,140
la fonction de coût finale.

72
00:04:27,840 --> 00:04:32,455
La dérivée de AL par rapport à ZL est simplement la dérivée de notre fonction sigmoïde, 

73
00:04:32,455 --> 00:04:36,180
ou quelle que soit la non-linéarité que vous choisissez d'utiliser.

74
00:04:37,220 --> 00:04:44,660
Et la dérivée de ZL par rapport à WL s'avère être AL-1.

75
00:04:45,760 --> 00:04:48,397
Maintenant, je ne sais pas pour vous, mais je pense qu'il est facile 

76
00:04:48,397 --> 00:04:50,963
de rester coincé tête baissée dans les formules sans prendre un moment 

77
00:04:50,963 --> 00:04:53,420
pour s'asseoir et se rappeler ce qu'elles signifient toutes.

78
00:04:53,920 --> 00:04:58,284
Dans le cas de cette dernière dérivée, la mesure dans laquelle le petit coup 

79
00:04:58,284 --> 00:05:02,820
de pouce a influencé la dernière couche dépend de la force du neurone précédent.

80
00:05:03,380 --> 00:05:08,280
N’oubliez pas que c’est là qu’intervient l’idée des neurones qui s’allument ensemble.

81
00:05:09,200 --> 00:05:12,430
Et tout cela n'est que la dérivée par rapport à WL 

82
00:05:12,430 --> 00:05:15,720
du coût d'un exemple de formation unique spécifique.

83
00:05:16,440 --> 00:05:20,206
Étant donné que la fonction de coût complet implique de faire la moyenne de tous 

84
00:05:20,206 --> 00:05:22,996
ces coûts sur de nombreux exemples de formation différents, 

85
00:05:22,996 --> 00:05:26,855
sa dérivée nécessite de faire la moyenne de cette expression sur tous les exemples 

86
00:05:26,855 --> 00:05:27,460
de formation.

87
00:05:28,380 --> 00:05:31,691
Et bien sûr, ce n’est qu’une composante du vecteur gradient, 

88
00:05:31,691 --> 00:05:34,948
qui lui-même est construit à partir des dérivées partielles 

89
00:05:34,948 --> 00:05:38,260
de la fonction de coût par rapport à tous ces poids et biais.

90
00:05:40,640 --> 00:05:43,816
Mais même si ce n’est qu’une des nombreuses dérivées partielles dont nous avons besoin, 

91
00:05:43,816 --> 00:05:45,260
cela représente plus de 50 % du travail.

92
00:05:46,340 --> 00:05:49,720
La sensibilité au biais, par exemple, est quasiment identique.

93
00:05:50,040 --> 00:05:55,020
Nous avons juste besoin de remplacer ce terme del z del w par un del z del b.

94
00:05:58,420 --> 00:06:02,400
Et si vous regardez la formule pertinente, cette dérivée s’avère être 1.

95
00:06:06,140 --> 00:06:10,401
Aussi, et c'est là qu'intervient l'idée de propagation vers l'arrière, 

96
00:06:10,401 --> 00:06:13,633
vous pouvez voir à quel point cette fonction de coût est sensible 

97
00:06:13,633 --> 00:06:15,740
à l'activation de la couche précédente.

98
00:06:15,740 --> 00:06:20,767
À savoir, cette dérivée initiale dans l’expression de la règle de chaîne, 

99
00:06:20,767 --> 00:06:25,660
la sensibilité de z à l’activation précédente, s’avère être le poids WL.

100
00:06:26,640 --> 00:06:29,756
Et encore une fois, même si nous ne pourrons pas influencer directement 

101
00:06:29,756 --> 00:06:33,263
l'activation de la couche précédente, il est utile d'en garder la trace, 

102
00:06:33,263 --> 00:06:36,509
car maintenant nous pouvons simplement continuer à répéter cette même idée 

103
00:06:36,509 --> 00:06:39,626
de règle de chaîne à l'envers pour voir à quel point la fonction de 

104
00:06:39,626 --> 00:06:42,440
coût est sensible à pondérations précédentes et biais antérieurs.

105
00:06:43,180 --> 00:06:45,807
Et vous pourriez penser qu’il s’agit d’un exemple trop simple, 

106
00:06:45,807 --> 00:06:48,392
puisque toutes les couches ont un neurone, et les choses vont 

107
00:06:48,392 --> 00:06:51,020
devenir exponentiellement plus compliquées pour un réseau réel.

108
00:06:51,700 --> 00:06:55,387
Mais honnêtement, cela ne change pas beaucoup lorsque nous donnons plusieurs neurones 

109
00:06:55,387 --> 00:06:58,860
aux couches, ce ne sont en réalité que quelques indices supplémentaires à suivre.

110
00:06:59,380 --> 00:07:02,925
Plutôt que l'activation d'une couche donnée soit simplement AL, 

111
00:07:02,925 --> 00:07:07,160
elle aura également un indice indiquant de quel neurone de cette couche il s'agit.

112
00:07:07,160 --> 00:07:14,420
Utilisons la lettre k pour indexer le calque L-1, et j pour indexer le calque L.

113
00:07:15,260 --> 00:07:18,992
Pour le coût, nous regardons encore une fois quel est le résultat souhaité, 

114
00:07:18,992 --> 00:07:22,282
mais cette fois nous additionnons les carrés des différences entre 

115
00:07:22,282 --> 00:07:25,180
ces activations de dernière couche et le résultat souhaité.

116
00:07:26,080 --> 00:07:30,840
Autrement dit, vous prenez une somme supérieure à ALj moins Yj au carré.

117
00:07:33,040 --> 00:07:36,959
Comme il y a beaucoup plus de poids, chacun doit avoir quelques 

118
00:07:36,959 --> 00:07:40,204
indices supplémentaires pour savoir où il se trouve, 

119
00:07:40,204 --> 00:07:44,920
appelons donc le poids du bord reliant ce kème neurone au jème neurone, WLjk.

120
00:07:45,620 --> 00:07:47,846
Ces indices peuvent sembler un peu rétrogrades au début, 

121
00:07:47,846 --> 00:07:50,346
mais cela correspond à la façon dont vous indexeriez la matrice 

122
00:07:50,346 --> 00:07:53,120
de pondération dont j'ai parlé dans la vidéo de la première partie.

123
00:07:53,620 --> 00:07:57,826
Comme avant, il est toujours agréable de donner un nom à la somme pondérée pertinente, 

124
00:07:57,826 --> 00:08:01,355
comme z, afin que l'activation de la dernière couche soit simplement 

125
00:08:01,355 --> 00:08:04,160
votre fonction spéciale, comme la sigmoïde, appliquée à z.

126
00:08:04,660 --> 00:08:07,334
Vous pouvez voir ce que je veux dire, où toutes ces équations sont 

127
00:08:07,334 --> 00:08:10,367
essentiellement les mêmes que celles que nous avions auparavant dans le cas 

128
00:08:10,367 --> 00:08:13,680
d'un neurone par couche, c'est juste que cela semble un peu plus compliqué.

129
00:08:15,440 --> 00:08:19,675
Et en effet, l’expression dérivée en chaîne décrivant la sensibilité 

130
00:08:19,675 --> 00:08:23,420
du coût à un poids spécifique semble essentiellement la même.

131
00:08:23,920 --> 00:08:25,294
Je vous laisse le soin de faire une pause et de 

132
00:08:25,294 --> 00:08:26,840
réfléchir à chacun de ces termes si vous le souhaitez.

133
00:08:28,980 --> 00:08:32,886
Ce qui change ici, cependant, c'est la dérivée du coût 

134
00:08:32,886 --> 00:08:36,659
par rapport à l'une des activations de la couche L-1.

135
00:08:37,780 --> 00:08:40,424
Dans ce cas, la différence est que le neurone influence 

136
00:08:40,424 --> 00:08:42,880
la fonction de coût par plusieurs voies différentes.

137
00:08:44,680 --> 00:08:50,063
Autrement dit, d’une part, cela influence AL0, qui joue un rôle dans la fonction de coût, 

138
00:08:50,063 --> 00:08:54,369
mais cela a également une influence sur AL1, qui joue également un rôle 

139
00:08:54,369 --> 00:08:57,540
dans la fonction de coût, et il faut les additionner.

140
00:08:59,820 --> 00:09:03,040
Et ça, eh bien, c'est à peu près tout.

141
00:09:03,500 --> 00:09:06,547
Une fois que vous savez à quel point la fonction de coût est sensible 

142
00:09:06,547 --> 00:09:08,637
aux activations de cette avant-dernière couche, 

143
00:09:08,637 --> 00:09:11,815
vous pouvez simplement répéter le processus pour tous les poids et biais 

144
00:09:11,815 --> 00:09:12,860
alimentant cette couche.

145
00:09:13,900 --> 00:09:14,960
Alors félicitez-vous !

146
00:09:15,300 --> 00:09:19,212
Si tout cela a du sens, vous avez maintenant approfondi le cœur de la rétropropagation, 

147
00:09:19,212 --> 00:09:22,680
le cheval de bataille derrière la façon dont les réseaux neuronaux apprennent.

148
00:09:23,300 --> 00:09:26,669
Ces expressions de règles de chaîne vous donnent les dérivées 

149
00:09:26,669 --> 00:09:29,876
qui déterminent chaque composant du gradient qui permet de 

150
00:09:29,876 --> 00:09:33,300
minimiser le coût du réseau en descendant à plusieurs reprises.

151
00:09:34,300 --> 00:09:36,431
Si vous vous asseyez et réfléchissez à tout cela, 

152
00:09:36,431 --> 00:09:39,159
cela représente beaucoup de niveaux de complexité à comprendre, 

153
00:09:39,159 --> 00:09:42,740
alors ne vous inquiétez pas s'il faut du temps à votre esprit pour tout digérer.

