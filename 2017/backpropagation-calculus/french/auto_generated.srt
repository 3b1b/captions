1
00:00:04,019 --> 00:00:06,798
L'hypothèse difficile ici est que vous avez regardé la partie 3, 

2
00:00:06,798 --> 00:00:09,920
qui donne une présentation intuitive de l'algorithme de rétropropagation.

3
00:00:11,040 --> 00:00:14,220
Ici, nous devenons un peu plus formels et plongeons dans le calcul pertinent.

4
00:00:14,820 --> 00:00:16,887
Il est normal que cela soit au moins un peu déroutant, 

5
00:00:16,887 --> 00:00:20,196
donc le mantra de faire régulièrement une pause et de réfléchir s'applique certainement 

6
00:00:20,196 --> 00:00:21,400
autant ici que partout ailleurs.

7
00:00:21,940 --> 00:00:24,846
Notre objectif principal est de montrer comment les personnes travaillant dans 

8
00:00:24,846 --> 00:00:27,679
le domaine de l'apprentissage automatique pensent généralement à la règle de 

9
00:00:27,679 --> 00:00:29,408
chaîne du calcul dans le contexte des réseaux, 

10
00:00:29,408 --> 00:00:32,057
ce qui a une sensation différente de la façon dont la plupart des cours 

11
00:00:32,057 --> 00:00:33,640
d'introduction au calcul abordent le sujet.

12
00:00:34,340 --> 00:00:37,354
Pour ceux d’entre vous qui ne sont pas à l’aise avec le calcul pertinent, 

13
00:00:37,354 --> 00:00:38,740
j’ai toute une série sur le sujet.

14
00:00:39,960 --> 00:00:43,116
Commençons par un réseau extrêmement simple, dans 

15
00:00:43,116 --> 00:00:46,020
lequel chaque couche contient un seul neurone.

16
00:00:46,320 --> 00:00:49,316
Ce réseau est déterminé par trois poids et trois biais, 

17
00:00:49,316 --> 00:00:53,542
et notre objectif est de comprendre dans quelle mesure la fonction de coût est 

18
00:00:53,542 --> 00:00:54,880
sensible à ces variables.

19
00:00:55,420 --> 00:00:57,971
De cette façon, nous savons quels ajustements de ces termes 

20
00:00:57,971 --> 00:01:00,820
entraîneront la diminution la plus efficace de la fonction de coût.

21
00:01:01,960 --> 00:01:04,840
Et nous allons juste nous concentrer sur la connexion entre les deux derniers neurones.

22
00:01:05,980 --> 00:01:09,935
Marquons l'activation de ce dernier neurone avec un exposant L, 

23
00:01:09,935 --> 00:01:14,385
indiquant dans quelle couche il se trouve, donc l'activation du neurone 

24
00:01:14,385 --> 00:01:15,560
précédent est Al-1.

25
00:01:16,360 --> 00:01:19,800
Ce ne sont pas des exposants, ils sont juste un moyen d'indexer ce dont nous parlons, 

26
00:01:19,800 --> 00:01:23,040
puisque je souhaite enregistrer ultérieurement les indices de différents indices.

27
00:01:23,720 --> 00:01:27,978
Disons que la valeur que nous souhaitons que cette dernière activation ait 

28
00:01:27,978 --> 00:01:32,180
pour un exemple de formation donné est y, par exemple, y peut être 0 ou 1.

29
00:01:32,840 --> 00:01:39,240
Le coût de ce réseau pour un seul exemple de formation est donc Al-y2.

30
00:01:40,260 --> 00:01:44,380
Nous désignerons le coût de cet exemple de formation par c0.

31
00:01:45,900 --> 00:01:51,457
Pour rappel, cette dernière activation est déterminée par un poids, que j'appellerai WL, 

32
00:01:51,457 --> 00:01:56,640
multiplié par l'activation du neurone précédent plus un biais, que j'appellerai BL.

33
00:01:57,420 --> 00:02:01,320
Et puis vous pompez cela via une fonction non linéaire spéciale comme le sigmoïde ou ReLU.

34
00:02:01,800 --> 00:02:05,583
Cela va en fait nous faciliter la tâche si nous donnons un nom spécial à cette 

35
00:02:05,583 --> 00:02:09,320
somme pondérée, comme z, avec le même exposant que les activations concernées.

36
00:02:10,380 --> 00:02:13,923
Cela fait beaucoup de termes, et une façon dont vous pourriez le 

37
00:02:13,923 --> 00:02:17,739
conceptualiser est que le poids, l'action précédente et le biais sont 

38
00:02:17,739 --> 00:02:21,882
tous utilisés pour calculer z, ce qui nous permet à son tour de calculer a, 

39
00:02:21,882 --> 00:02:25,480
qui finalement, avec un y constant, permet nous calculons le coût.

40
00:02:27,340 --> 00:02:31,427
Et bien sûr, l'Al-1 est influencé par son propre poids et ses préjugés, 

41
00:02:31,427 --> 00:02:35,060
mais nous n'allons pas nous concentrer là-dessus pour le moment.

42
00:02:35,700 --> 00:02:37,620
Tout cela ne sont que des chiffres, n'est-ce pas ?

43
00:02:38,060 --> 00:02:41,040
Et il peut être agréable de penser que chacun a sa propre petite droite numérique.

44
00:02:41,720 --> 00:02:45,473
Notre premier objectif est de comprendre à quel point la fonction 

45
00:02:45,473 --> 00:02:49,000
de coût est sensible aux petits changements de notre poids WL.

46
00:02:49,540 --> 00:02:54,860
Ou une expression différente, quelle est la dérivée de c par rapport à WL ?

47
00:02:55,600 --> 00:03:00,855
Lorsque vous voyez ce terme del W, pensez-y comme signifiant un petit coup de pouce à W, 

48
00:03:00,855 --> 00:03:05,166
comme un changement de 0,01, et pensez à ce terme del c comme signifiant 

49
00:03:05,166 --> 00:03:08,060
quel que soit le coup de pouce résultant du coût.

50
00:03:08,060 --> 00:03:10,220
Ce que nous voulons, c'est leur ratio.

51
00:03:11,260 --> 00:03:14,515
Conceptuellement, ce petit coup de pouce vers WL entraîne un 

52
00:03:14,515 --> 00:03:19,265
certain coup de pouce vers ZL, qui à son tour provoque un certain coup de pouce vers AL, 

53
00:03:19,265 --> 00:03:21,240
ce qui influence directement le coût.

54
00:03:23,120 --> 00:03:28,434
Nous décomposons donc les choses en examinant d’abord le rapport d’un petit changement 

55
00:03:28,434 --> 00:03:33,200
de ZL à ce petit changement W, c’est-à-dire la dérivée de ZL par rapport à WL.

56
00:03:33,200 --> 00:03:37,073
De même, vous considérez ensuite le rapport entre le changement de AL et 

57
00:03:37,073 --> 00:03:40,839
le petit changement de ZL qui l'a provoqué, ainsi que le rapport entre 

58
00:03:40,839 --> 00:03:44,660
le coup de pouce final vers c et ce coup de pouce intermédiaire vers AL.

59
00:03:45,740 --> 00:03:50,440
C'est ici la règle de la chaîne, où la multiplication de ces trois 

60
00:03:50,440 --> 00:03:55,140
ratios nous donne la sensibilité de c aux petits changements de WL.

61
00:03:56,880 --> 00:03:59,741
Donc, à l'écran en ce moment, il y a beaucoup de symboles, 

62
00:03:59,741 --> 00:04:03,281
et prenez un moment pour vous assurer que ce qu'ils sont tous est clair, 

63
00:04:03,281 --> 00:04:06,240
car nous allons maintenant calculer les dérivées pertinentes.

64
00:04:07,440 --> 00:04:13,160
La dérivée de c par rapport à AL s’avère être 2AL-y.

65
00:04:13,980 --> 00:04:17,433
Notez que cela signifie que sa taille est proportionnelle à la différence 

66
00:04:17,433 --> 00:04:20,466
entre la production du réseau et ce que nous voulons qu'il soit, 

67
00:04:20,466 --> 00:04:22,706
donc si cette production était très différente, 

68
00:04:22,706 --> 00:04:26,020
même de légers changements risquent d'avoir un impact important sur la 

69
00:04:26,020 --> 00:04:27,140
fonction de coût finale.

70
00:04:27,840 --> 00:04:32,574
La dérivée de AL par rapport à ZL est simplement la dérivée de notre fonction sigmoïde, 

71
00:04:32,574 --> 00:04:36,180
ou quelle que soit la non-linéarité que vous choisissez d'utiliser.

72
00:04:37,220 --> 00:04:44,660
Et la dérivée de ZL par rapport à WL s'avère être AL-1.

73
00:04:45,760 --> 00:04:48,402
Maintenant, je ne sais pas pour vous, mais je pense qu'il est facile 

74
00:04:48,402 --> 00:04:50,853
de rester coincé tête baissée dans les formules sans prendre un 

75
00:04:50,853 --> 00:04:53,420
moment pour s'asseoir et se rappeler ce qu'elles signifient toutes.

76
00:04:53,920 --> 00:04:58,284
Dans le cas de cette dernière dérivée, la mesure dans laquelle le petit coup 

77
00:04:58,284 --> 00:05:02,820
de pouce a influencé la dernière couche dépend de la force du neurone précédent.

78
00:05:03,380 --> 00:05:08,280
N’oubliez pas que c’est là qu’intervient l’idée des neurones qui s’allument ensemble.

79
00:05:09,200 --> 00:05:12,428
Et tout cela n'est que la dérivée par rapport à WL 

80
00:05:12,428 --> 00:05:15,720
du coût d'un exemple de formation unique spécifique.

81
00:05:16,440 --> 00:05:20,206
Étant donné que la fonction de coût complet implique de faire la moyenne de tous 

82
00:05:20,206 --> 00:05:22,996
ces coûts sur de nombreux exemples de formation différents, 

83
00:05:22,996 --> 00:05:26,855
sa dérivée nécessite de faire la moyenne de cette expression sur tous les exemples 

84
00:05:26,855 --> 00:05:27,460
de formation.

85
00:05:28,380 --> 00:05:31,691
Et bien sûr, ce n’est qu’une composante du vecteur gradient, 

86
00:05:31,691 --> 00:05:34,948
qui lui-même est construit à partir des dérivées partielles 

87
00:05:34,948 --> 00:05:38,260
de la fonction de coût par rapport à tous ces poids et biais.

88
00:05:40,640 --> 00:05:43,816
Mais même si ce n’est qu’une des nombreuses dérivées partielles dont nous avons besoin, 

89
00:05:43,816 --> 00:05:45,260
cela représente plus de 50 % du travail.

90
00:05:46,340 --> 00:05:49,720
La sensibilité au biais, par exemple, est quasiment identique.

91
00:05:50,040 --> 00:05:55,020
Nous avons juste besoin de remplacer ce terme del z del w par un del z del b.

92
00:05:58,420 --> 00:06:02,400
Et si vous regardez la formule pertinente, cette dérivée s’avère être 1.

93
00:06:06,140 --> 00:06:10,012
Aussi, et c'est là qu'intervient l'idée de propagation vers l'arrière, 

94
00:06:10,012 --> 00:06:14,758
vous pouvez voir à quel point cette fonction de coût est sensible à l'activation de la 

95
00:06:14,758 --> 00:06:15,740
couche précédente.

96
00:06:15,740 --> 00:06:20,767
À savoir, cette dérivée initiale dans l’expression de la règle de chaîne, 

97
00:06:20,767 --> 00:06:25,660
la sensibilité de z à l’activation précédente, s’avère être le poids WL.

98
00:06:26,640 --> 00:06:30,578
Et encore une fois, même si nous ne pourrons pas influencer directement l'activation de 

99
00:06:30,578 --> 00:06:33,130
la couche précédente, il est utile d'en garder la trace, 

100
00:06:33,130 --> 00:06:37,024
car maintenant nous pouvons simplement continuer à répéter cette même idée de règle de 

101
00:06:37,024 --> 00:06:41,007
chaîne à l'envers pour voir à quel point la fonction de coût est sensible à pondérations 

102
00:06:41,007 --> 00:06:42,440
précédentes et biais antérieurs.

103
00:06:43,180 --> 00:06:45,807
Et vous pourriez penser qu’il s’agit d’un exemple trop simple, 

104
00:06:45,807 --> 00:06:48,392
puisque toutes les couches ont un neurone, et les choses vont 

105
00:06:48,392 --> 00:06:51,020
devenir exponentiellement plus compliquées pour un réseau réel.

106
00:06:51,700 --> 00:06:55,387
Mais honnêtement, cela ne change pas beaucoup lorsque nous donnons plusieurs neurones 

107
00:06:55,387 --> 00:06:58,860
aux couches, ce ne sont en réalité que quelques indices supplémentaires à suivre.

108
00:06:59,380 --> 00:07:02,790
Plutôt que l'activation d'une couche donnée soit simplement AL, 

109
00:07:02,790 --> 00:07:07,160
elle aura également un indice indiquant de quel neurone de cette couche il s'agit.

110
00:07:07,160 --> 00:07:14,420
Utilisons la lettre k pour indexer le calque L-1, et j pour indexer le calque L.

111
00:07:15,260 --> 00:07:18,992
Pour le coût, nous regardons encore une fois quel est le résultat souhaité, 

112
00:07:18,992 --> 00:07:22,282
mais cette fois nous additionnons les carrés des différences entre 

113
00:07:22,282 --> 00:07:25,180
ces activations de dernière couche et le résultat souhaité.

114
00:07:26,080 --> 00:07:30,840
Autrement dit, vous prenez une somme supérieure à ALj moins Yj au carré.

115
00:07:33,040 --> 00:07:36,959
Comme il y a beaucoup plus de poids, chacun doit avoir quelques 

116
00:07:36,959 --> 00:07:40,204
indices supplémentaires pour savoir où il se trouve, 

117
00:07:40,204 --> 00:07:44,920
appelons donc le poids du bord reliant ce kème neurone au jème neurone, WLjk.

118
00:07:45,620 --> 00:07:47,893
Ces indices peuvent sembler un peu rétrogrades au début, 

119
00:07:47,893 --> 00:07:50,447
mais cela correspond à la façon dont vous indexeriez la matrice 

120
00:07:50,447 --> 00:07:53,120
de pondération dont j'ai parlé dans la vidéo de la première partie.

121
00:07:53,620 --> 00:07:57,904
Comme avant, il est toujours agréable de donner un nom à la somme pondérée pertinente, 

122
00:07:57,904 --> 00:08:01,303
comme z, afin que l'activation de la dernière couche soit simplement 

123
00:08:01,303 --> 00:08:04,160
votre fonction spéciale, comme la sigmoïde, appliquée à z.

124
00:08:04,660 --> 00:08:07,432
Vous pouvez voir ce que je veux dire, où toutes ces équations sont 

125
00:08:07,432 --> 00:08:10,411
essentiellement les mêmes que celles que nous avions auparavant dans le 

126
00:08:10,411 --> 00:08:13,680
cas d'un neurone par couche, c'est juste que cela semble un peu plus compliqué.

127
00:08:15,440 --> 00:08:19,675
Et en effet, l’expression dérivée en chaîne décrivant la sensibilité 

128
00:08:19,675 --> 00:08:23,420
du coût à un poids spécifique semble essentiellement la même.

129
00:08:23,920 --> 00:08:25,294
Je vous laisse le soin de faire une pause et de 

130
00:08:25,294 --> 00:08:26,840
réfléchir à chacun de ces termes si vous le souhaitez.

131
00:08:28,980 --> 00:08:32,891
Ce qui change ici, cependant, c'est la dérivée du coût 

132
00:08:32,891 --> 00:08:36,659
par rapport à l'une des activations de la couche L-1.

133
00:08:37,780 --> 00:08:40,424
Dans ce cas, la différence est que le neurone influence 

134
00:08:40,424 --> 00:08:42,880
la fonction de coût par plusieurs voies différentes.

135
00:08:44,680 --> 00:08:50,063
Autrement dit, d’une part, cela influence AL0, qui joue un rôle dans la fonction de coût, 

136
00:08:50,063 --> 00:08:54,369
mais cela a également une influence sur AL1, qui joue également un rôle 

137
00:08:54,369 --> 00:08:57,540
dans la fonction de coût, et il faut les additionner.

138
00:08:59,820 --> 00:09:03,040
Et ça, eh bien, c'est à peu près tout.

139
00:09:03,500 --> 00:09:06,547
Une fois que vous savez à quel point la fonction de coût est sensible 

140
00:09:06,547 --> 00:09:08,637
aux activations de cette avant-dernière couche, 

141
00:09:08,637 --> 00:09:11,815
vous pouvez simplement répéter le processus pour tous les poids et biais 

142
00:09:11,815 --> 00:09:12,860
alimentant cette couche.

143
00:09:13,900 --> 00:09:14,960
Alors félicitez-vous !

144
00:09:15,300 --> 00:09:19,212
Si tout cela a du sens, vous avez maintenant approfondi le cœur de la rétropropagation, 

145
00:09:19,212 --> 00:09:22,680
le cheval de bataille derrière la façon dont les réseaux neuronaux apprennent.

146
00:09:23,300 --> 00:09:26,669
Ces expressions de règles de chaîne vous donnent les dérivées 

147
00:09:26,669 --> 00:09:29,876
qui déterminent chaque composant du gradient qui permet de 

148
00:09:29,876 --> 00:09:33,300
minimiser le coût du réseau en descendant à plusieurs reprises.

149
00:09:34,300 --> 00:09:36,475
Si vous vous asseyez et réfléchissez à tout cela, 

150
00:09:36,475 --> 00:09:39,259
cela représente beaucoup de niveaux de complexité à comprendre, 

151
00:09:39,259 --> 00:09:42,740
alors ne vous inquiétez pas s'il faut du temps à votre esprit pour tout digérer.

