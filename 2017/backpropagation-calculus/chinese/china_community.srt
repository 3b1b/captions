1
00:00:04,230 --> 00:00:07,120
这集开始我们就假设你已经看过第三集了

2
00:00:07,120 --> 00:00:10,230
那集让大家直观上感受反向传播算法的原理

3
00:00:11,040 --> 00:00:14,770
在这集里  我们会更深入讲解一些其中的微积分理论

4
00:00:14,770 --> 00:00:17,040
这个看不太懂很正常

5
00:00:17,040 --> 00:00:21,480
所以  我们的六字格言“停一停想一想”在这依旧管用

6
00:00:21,920 --> 00:00:25,180
这集我们的目标  是给大家展示在机器学习中

7
00:00:25,180 --> 00:00:29,440
我们一般是怎么理解链式法则的

8
00:00:29,440 --> 00:00:33,820
这点跟别的基础微积分课讲得会有点不一样

9
00:00:34,500 --> 00:00:36,890
对于微积分不够熟悉的观众

10
00:00:36,890 --> 00:00:39,040
我之前已经做了一整个系列了

11
00:00:40,340 --> 00:00:43,150
我们从最最简单的网络讲起吧

12
00:00:43,150 --> 00:00:45,730
每层只有一个神经元

13
00:00:46,270 --> 00:00:50,680
图上这个网络就是由3个权重和3个偏置决定的

14
00:00:50,680 --> 00:00:55,070
我们的目标是理解代价函数对于这些变量有多敏感

15
00:00:55,550 --> 00:00:57,830
这样 我们就知道怎么调整这些变量

16
00:00:57,830 --> 00:01:00,940
才可以使得代价降低得最快

17
00:01:01,920 --> 00:01:05,170
我们先来关注最后两个神经元吧

18
00:01:05,880 --> 00:01:11,370
我给最后一个神经元的激活值一个上标L  表示它处在第L层

19
00:01:11,690 --> 00:01:15,720
那么  前一个神经元的激活值就是a^(L-1)

20
00:01:16,430 --> 00:01:20,030
这不是指数  而是用来标记我们正在讨论哪一层

21
00:01:20,030 --> 00:01:22,970
过一会我会用到下标来表示别的意思

22
00:01:23,740 --> 00:01:29,710
给定一个训练样本  我们把这个最终层激活值要接近的目标叫做y

23
00:01:30,170 --> 00:01:32,360
那么y就要么是0  要么是1

24
00:01:32,940 --> 00:01:39,470
那么这个简易网络对于单个训练样本的代价  就等于 (a^(L) - y)^2

25
00:01:40,250 --> 00:01:44,650
对于这个样本  我们把这个代价值标记为C_0

26
00:01:46,030 --> 00:01:51,520
还记得吗  最终层的激活值是这么算出来的—— 一个权重 w^(L)

27
00:01:51,980 --> 00:01:54,220
乘上前一个神经元的激活值

28
00:01:54,530 --> 00:01:56,940
再加上一个偏置b^(L)

29
00:01:57,480 --> 00:01:59,900
最后把加权和塞进一个特定的非线性函数

30
00:01:59,900 --> 00:02:01,520
例如sigmoid ReLU之类的

31
00:02:01,850 --> 00:02:06,980
给这个加权和一个名字会方便很多  就叫它z好了

32
00:02:06,980 --> 00:02:09,550
跟对应的激活值用同一个上标

33
00:02:10,390 --> 00:02:11,480
这里的项挺多

34
00:02:11,480 --> 00:02:16,960
概括起来 我们拿这个权重 前一个激活值 和这个偏置值

35
00:02:16,960 --> 00:02:21,400
一起来算出z  再算出a

36
00:02:21,740 --> 00:02:25,610
最后再用上常量y  算出代价

37
00:02:27,260 --> 00:02:31,660
当然  a^(L-1)是由它自己的权重和偏置决定的  以此类推

38
00:02:32,810 --> 00:02:34,840
但我们现在重点不在那里

39
00:02:35,680 --> 00:02:38,040
这些东西都是数字  没错吧

40
00:02:38,040 --> 00:02:41,230
我们可以想象每个数字都对应一个数轴

41
00:02:41,900 --> 00:02:43,990
我们第一个目标是理解

42
00:02:43,990 --> 00:02:48,940
代价函数对权重w^(L)的微小变化有多敏感

43
00:02:49,640 --> 00:02:54,880
或者换句话讲  求C对w^(L)的导数

44
00:02:55,630 --> 00:02:58,070
当你看到∂w之类的项时

45
00:02:58,070 --> 00:03:02,750
请把它当做这是对w的微小扰动  好比变个0.01

46
00:03:03,150 --> 00:03:08,210
然后把∂C当做 “改变w对C的值造成的变化”

47
00:03:08,710 --> 00:03:10,420
我们求的是这两个数的比值

48
00:03:11,210 --> 00:03:16,520
概念上说  w^(L)的微小变化会导致z^(L)产生些变化

49
00:03:16,520 --> 00:03:21,380
然后会导致a^(L)产生变化  最终影响到代价值

50
00:03:23,100 --> 00:03:28,930
那么 我们把式子拆开  首先求z^(L)的变化量比上w^(L)的变化量

51
00:03:29,290 --> 00:03:33,030
也就是求z^(L)关于w^(L)的导数

52
00:03:33,760 --> 00:03:39,410
同理 考虑a^(L)的变化量 比上因变量z^(L)的变化量

53
00:03:39,850 --> 00:03:44,880
以及最终的C的变化量 比上直接改动a^(L)产生的变化量

54
00:03:45,670 --> 00:03:47,850
这不就是链式法则么

55
00:03:47,850 --> 00:03:54,950
把三个比相乘  就可以算出C对w^(L)的微小变化有多敏感

56
00:03:57,190 --> 00:04:00,040
现在屏幕上多了一大坨符号

57
00:04:00,040 --> 00:04:03,000
稍稍花点时间理解一下每个符号都什么意思吧

58
00:04:03,600 --> 00:04:06,560
马上我们就要对各个求导了

59
00:04:07,400 --> 00:04:13,230
C关于a^(L)的导数  就是2(a^(L) - y)

60
00:04:13,960 --> 00:04:16,880
这也就意味着  导数的大小

61
00:04:16,880 --> 00:04:20,880
跟网络最终输出减目标结果的差成正比

62
00:04:21,360 --> 00:04:23,340
如果网络的输出差别很大

63
00:04:23,340 --> 00:04:27,150
即使w稍稍变一点  代价也会改变非常大

64
00:04:28,300 --> 00:04:33,880
a^(L)对z^(L)求导就是求sigmoid的导数

65
00:04:33,880 --> 00:04:36,370
或就你选择的非线性激活函数

66
00:04:37,310 --> 00:04:40,370
而z^(L)对w^(L)求导

67
00:04:41,470 --> 00:04:44,530
结果就是a^(L-1)

68
00:04:46,070 --> 00:04:49,570
对我自己来说   这里如果不退一步

69
00:04:49,570 --> 00:04:53,690
好好想想这些公式的含义  很容易卡住

70
00:04:54,120 --> 00:04:56,040
就最后这个导数来说

71
00:04:56,040 --> 00:05:00,060
这个权重的改变量∂w对最后一层的影响有多大

72
00:05:00,060 --> 00:05:02,850
取决于之前一层的神经元

73
00:05:03,310 --> 00:05:07,520
所谓“一同激活的神经元关联在一起”的出处即来源于此

74
00:05:09,210 --> 00:05:15,940
不过这只是包含一个训练样本的代价对w^(L)的导数

75
00:05:16,410 --> 00:05:22,150
由于总的代价函数是许许多多训练样本所有代价的总平均

76
00:05:22,150 --> 00:05:27,610
它对w^(L)的导数就需要求  这个表达式之于每一个训练样本的平均

77
00:05:28,430 --> 00:05:31,930
当然这只是梯度向量∇C的一个分量

78
00:05:31,930 --> 00:05:33,890
而梯度向量∇C本身

79
00:05:33,890 --> 00:05:38,480
则由代价函数对每一个权重和每一个偏置求偏导构成

80
00:05:40,710 --> 00:05:43,550
求出这些偏导中的一个

81
00:05:43,550 --> 00:05:45,390
就完成了一大半的工作量

82
00:05:46,420 --> 00:05:49,940
对偏置的求导步骤也基本相同

83
00:05:50,250 --> 00:05:55,120
只要把∂z/∂w替换成∂z/∂b即可

84
00:05:58,760 --> 00:06:02,590
对应的公式中可以看出 导数∂z/∂b等于1

85
00:06:06,210 --> 00:06:09,880
这里也涉及到了反向传播的概念

86
00:06:10,230 --> 00:06:15,670
我们来看下这个代价函数对上一层的激活值的敏感度

87
00:06:16,250 --> 00:06:19,650
展开来说 链式法则的第一项

88
00:06:19,650 --> 00:06:23,100
z对上一层激活值的敏感度

89
00:06:23,480 --> 00:06:25,670
就是权重w^(L)

90
00:06:26,580 --> 00:06:31,500
虽然 说过 我们不能直接改变激活值

91
00:06:31,500 --> 00:06:33,080
但我们很有必要关注这个值

92
00:06:33,080 --> 00:06:38,200
因为我们可以反向应用链式法则

93
00:06:38,200 --> 00:06:42,750
来计算代价函数对之前的权重和偏置的敏感度

94
00:06:43,630 --> 00:06:45,980
你可能觉得这个例子举得太简单了

95
00:06:45,980 --> 00:06:47,880
毕竟每层只有一个神经元

96
00:06:47,880 --> 00:06:51,220
而真实的神经网络会比这个例子复杂百倍

97
00:06:51,680 --> 00:06:56,270
然而说真的 每层多加若干个神经元并不会复杂很多

98
00:06:56,270 --> 00:06:58,710
真的 只不过多写一些下标罢了

99
00:06:59,340 --> 00:07:02,880
我们用加上下标的神经元来表示L层的若干个神经元

100
00:07:02,880 --> 00:07:07,210
而不是用a^(L)统称L层的激活值

101
00:07:07,780 --> 00:07:14,470
现在用k来标注(L-1)层的神经元 j则是L层的神经元

102
00:07:15,290 --> 00:07:18,910
要求代价函数 我们从期望的输出着手

103
00:07:18,910 --> 00:07:19,380
计算上一层激活值和期望输出的差值的平方 然后求和

104
00:07:19,380 --> 00:07:25,260
计算上一层激活值和期望输出的差值的平方 然后求和

105
00:07:26,060 --> 00:07:31,070
即求(a_j^(L) - y_j)^2的和

106
00:07:33,110 --> 00:07:34,520
由于权重的数量多了不少

107
00:07:34,520 --> 00:07:37,650
那么每个权重要多用几个下标

108
00:07:38,010 --> 00:07:44,990
我们记连接第k个神经元和第j个神经元的连线为w_{jk}^(L)

109
00:07:45,660 --> 00:07:48,260
这些下标感觉像标反了 可能有点别扭

110
00:07:48,260 --> 00:07:52,940
不过和第一集视频中的权重矩阵的下标是一致的

111
00:07:53,680 --> 00:07:58,350
同样的 把加权和记为z 总是很方便

112
00:07:58,350 --> 00:08:04,310
那么最后一层的激活值依然等于指定的函数在z处的函数值

113
00:08:05,040 --> 00:08:06,230
你懂我的意思吧

114
00:08:06,230 --> 00:08:11,680
现在的方程式和之前每层只有一个神经元的时候本质是一样的

115
00:08:11,680 --> 00:08:13,870
只是看着复杂一些

116
00:08:15,370 --> 00:08:18,220
链式法则形式的导数表达式所描述的

117
00:08:18,220 --> 00:08:21,980
代价对某个权重的敏感度

118
00:08:21,980 --> 00:08:23,890
也是一样的

119
00:08:23,890 --> 00:08:26,880
这里观众可以暂停推导一下每一项的含义

120
00:08:29,310 --> 00:08:31,320
唯一改变的是

121
00:08:31,320 --> 00:08:36,830
代价对(L-1)层激活值的导数

122
00:08:37,760 --> 00:08:43,120
此时  激活值可以通过不同的途径影响代价函数

123
00:08:44,660 --> 00:08:50,540
就是说 神经元一边通过a_0^(L)来影响代价函数

124
00:08:51,010 --> 00:08:56,320
另一边通过a_1^(L)来影响代价函数

125
00:08:56,320 --> 00:08:57,410
得把这些都加起来

126
00:09:00,170 --> 00:09:02,980
然后……就搞定了

127
00:09:03,560 --> 00:09:08,520
只要计算出倒数第二层代价函数对激活值的敏感度

128
00:09:08,840 --> 00:09:12,940
接下来只要重复上述过程 计算喂给倒数第二层的权重和偏置 就好了

129
00:09:13,850 --> 00:09:15,360
现在长吁一口气吧！

130
00:09:15,360 --> 00:09:16,950
如果这里明白了

131
00:09:16,950 --> 00:09:20,440
那你就看明白了神经网络的主力——反向传播

132
00:09:20,440 --> 00:09:22,830
那你就看明白了神经网络的主力——反向传播

133
00:09:23,590 --> 00:09:29,300
链式法则表达式给出了决定梯度每个分量的偏导

134
00:09:29,300 --> 00:09:33,550
使得我们能不断下探  最小化神经网络的代价

135
00:09:34,280 --> 00:09:36,850
乌啦啦  光是静下来想一想

136
00:09:36,850 --> 00:09:40,090
这些复杂的层层叠叠就很烧脑

137
00:09:40,090 --> 00:09:43,090
消化这些知识会花一些时间  别气馁了

