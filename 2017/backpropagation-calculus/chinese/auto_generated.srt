1
00:00:00,000 --> 00:00:05,270
这里的硬假设是您已经观看了第 3

2
00:00:05,270 --> 00:00:11,160
部 分，其中直观地介绍了反向传播算法。

3
00:00:11,160 --> 00:00:14,920
在这里，我们变得更正式一些，并深入研究相关的演算。

4
00:00:14,920 --> 00:00:18,542
这至少有点令人困惑是正常的，因此定期停下来

5
00:00:18,542 --> 00:00:22,000
思考的咒语当然适用于此，也适用于其他地方。

6
00:00:22,000 --> 00:00:26,375
我们的主要目标是展示机器学习领域的人们如何在网

7
00:00:26,375 --> 00:00:30,568
络背景下普遍思考微积分的链式法则，这与大多数

8
00:00:30,568 --> 00:00:34,580
微积分入门课程处理该主题的方式有不同的感觉。

9
00:00:34,580 --> 00:00:37,071
对于那些对相关微积分感到不舒服的人，

10
00:00:37,071 --> 00:00:39,300
我确实有一个关于该主题的完整系列。

11
00:00:39,300 --> 00:00:46,780
让我们从一个极其简单的网络开始 ，其中每一层都有一个神经元。

12
00:00:46,780 --> 00:00:51,431
该网络由三个权重和三个偏差决定，我们的目

13
00:00:51,431 --> 00:00:55,640
标是了解成本函数对这些变量的敏感程度。

14
00:00:55,640 --> 00:00:58,458
这样我们就知道对这些项的哪些调

15
00:00:58,458 --> 00:01:01,100
整将导致成本函数最有效的降低。

16
00:01:01,100 --> 00:01:05,360
我们将只关注最后两个神经元之间的连接。

17
00:01:05,360 --> 00:01:08,677
让我们用上标 L 来标记最后一个

18
00:01:08,677 --> 00:01:11,800
神经元的激活，指示它位于哪一层。

19
00:01:11,800 --> 00:01:16,560
所以前一个神经元的激活是AL-1。

20
00:01:16,560 --> 00:01:20,226
这些不是指数，它们只是对我们正在讨论的内容进行索

21
00:01:20,226 --> 00:01:23,600
引的一种方式，因为我想稍后保存不同索引的下标。

22
00:01:23,600 --> 00:01:28,524
假设对于给定的训练示例，我们希望最后一次激活

23
00:01:28,524 --> 00:01:33,020
的值是 y，例如，y 可能是 0 或 1。

24
00:01:33,020 --> 00:01:39,040
因此，该网络对于单个训练示例的成本是 AL-y2。

25
00:01:39,040 --> 00:01:46,120
我们将该训练示例的成本表示为 c0。

26
00:01:46,120 --> 00:01:51,960
提醒一下，最后的激活是由权重（我将其称为 wL）乘以前一

27
00:01:51,960 --> 00:01:57,600
个神经元的激活加上一些偏差（我将其称为 bL）来确定的。

28
00:01:57,600 --> 00:01:59,475
然后通过一些特殊的非线性函数（例如

29
00:01:59,475 --> 00:02:01,560
sigmoid 或 ReLU）将其泵送。

30
00:02:01,560 --> 00:02:06,235
如果我们为这个加权和指定一个特殊的名称（例如 z），并且具

31
00:02:06,235 --> 00:02:10,600
有与相关激活相同的上标，实际上会让我们的事情变得更容易。

32
00:02:10,600 --> 00:02:16,333
这是很多术语，您可以将其概念化的一种方式是，权重、

33
00:02:16,333 --> 00:02:22,067
先前的操作和偏差一起用于计算 z，这反过来让我们计

34
00:02:22,067 --> 00:02:27,360
算 a，最后，与常数 y 一起，让我们计算成本。

35
00:02:27,360 --> 00:02:31,749
当然，AL-1 会受到其自身重量和偏差

36
00:02:31,749 --> 00:02:35,920
等的影响，但我们现在不打算关注这一点。

37
00:02:35,920 --> 00:02:38,120
所有这些都只是数字，对吧？

38
00:02:38,120 --> 00:02:41,960
认为每个数都有自己的小数轴是件好事。

39
00:02:41,960 --> 00:02:46,009
我们的第一个目标是了解成本函数对

40
00:02:46,009 --> 00:02:49,820
权重 wL 的微小变化有多敏感。

41
00:02:49,820 --> 00:02:55,740
或者换句话说，c 对 wL 的导数是多少？

42
00:02:55,740 --> 00:02:58,985
当您看到这个 del w 术语时，请将其视为对

43
00:02:58,985 --> 00:03:01,554
w 的一些微小推动，例如 0 的更改。

44
00:03:01,554 --> 00:03:05,083
01，并认为这个 del c 术语

45
00:03:05,083 --> 00:03:08,820
的含义是无论最终对成本的影响是什么。

46
00:03:08,820 --> 00:03:10,900
我们想要的是它们的比例。

47
00:03:10,900 --> 00:03:17,288
从概念上讲，对 wL 的微小推动会导致对 zL 的一些

48
00:03:17,288 --> 00:03:23,220
推动，进而对 AL 产生一些推动，从而直接影响成本。

49
00:03:23,220 --> 00:03:27,957
因此，我们首先查看 zL 的微小变化与 w

50
00:03:27,957 --> 00:03:33,340
的 微小变化之比，即 zL 相对于 wL 的导数。

51
00:03:33,340 --> 00:03:38,154
同样，然后考虑 AL 的变化与引起它的 zL

52
00:03:38,154 --> 00:03:43,597
的微小变化的比率，以及最终对 c 的微调与对 AL

53
00:03:43,597 --> 00:03:45,900
的中间微调之间的比率。

54
00:03:45,900 --> 00:03:51,921
这就是链式法则，将这三个比率相乘即可得

55
00:03:51,921 --> 00:03:57,340
出 c 对 wL 微小变化的敏感度。

56
00:03:57,340 --> 00:04:02,640
现在屏幕上有很多符号，请花点时间确保清楚它

57
00:04:02,640 --> 00:04:07,460
们是什么，因为现在我们要计算相关的导数。

58
00:04:07,460 --> 00:04:14,220
c 对 AL 的导数为 2AL-y。

59
00:04:14,220 --> 00:04:19,145
这意味着它的大小与网络输出和我们想要的结果之间

60
00:04:19,145 --> 00:04:23,865
的差异成正比，因此如果输出非常不同，即使是微

61
00:04:23,865 --> 00:04:28,380
小的变化也会对最终的成本函数产生很大的影响。

62
00:04:28,380 --> 00:04:33,080
AL 相对于 zL 的导数只是我们的 sigmoi

63
00:04:33,080 --> 00:04:37,420
d 函数的导数，或者您选择使用的任何非线性函数。

64
00:04:37,420 --> 00:04:46,180
zL 对 wL 的导数为 AL-1。

65
00:04:46,180 --> 00:04:50,273
我不了解你的情况，但我认为你很容易陷入公式

66
00:04:50,273 --> 00:04:54,180
中，而不花点时间坐下来提醒自己它们的含义。

67
00:04:54,180 --> 00:04:58,810
在最后一个导数的情况下，权重的微小推动对

68
00:04:58,810 --> 00:05:03,220
最后一层的影响取决于前一个神经元的强度。

69
00:05:03,220 --> 00:05:09,320
请记住，这就是“神经元一起发射、连线在一起”这一想法的由来。

70
00:05:09,320 --> 00:05:16,580
所有这些都是特定单个训练示例 的成本相对于 wL 的导数。

71
00:05:16,580 --> 00:05:20,697
由于完整的成本函数涉及将许多不同训练示例

72
00:05:20,697 --> 00:05:24,814
中的所有这些成本平均在一起，因此其导数需

73
00:05:24,814 --> 00:05:28,540
要对所有训练示例上的该表达式进行平均。

74
00:05:28,540 --> 00:05:34,904
当然，这只是梯度向量的一个组成部分，梯度向量是根据

75
00:05:34,904 --> 00:05:40,780
成本函数相对于所有这些权重和偏差的偏导数构建的。

76
00:05:40,780 --> 00:05:43,696
尽管这只是我们需要的众多偏导数之一，

77
00:05:43,696 --> 00:05:46,460
但它已经完成了超过 50% 的工作。

78
00:05:46,460 --> 00:05:50,300
例如，对偏差的敏感性几乎相同。

79
00:05:50,300 --> 00:05:54,640
我们只需要将这个 del z del w

80
00:05:54,640 --> 00:05:58,980
项改为 a del z del b 即可。

81
00:05:58,980 --> 00:06:04,700
如果你查看相关公式，就会发现该导数为 1。

82
00:06:04,700 --> 00:06:10,573
此外，这就是向后传播的想法出现的地方，您可

83
00:06:10,573 --> 00:06:16,180
以看到这个成本函数对前一层的激活有多敏感。

84
00:06:16,180 --> 00:06:20,912
也就是说，链式法则表达式中的初始导数，即

85
00:06:20,912 --> 00:06:25,420
z 对先前激活的敏感度，就是权重 wL。

86
00:06:25,420 --> 00:06:30,154
再说一次，即使我们无法直接影响前一层的激

87
00:06:30,154 --> 00:06:34,662
活，但跟踪它还是很有帮助的，因为现在我

88
00:06:34,662 --> 00:06:39,396
们可以不断向后迭代这个相同的链规则思想，

89
00:06:39,396 --> 00:06:43,680
看看成本函数对之前的权重和之前的偏差。

90
00:06:43,680 --> 00:06:47,641
你可能会认为这是一个过于简单的例子，因为所有层都有一个

91
00:06:47,641 --> 00:06:51,320
神经元，对于真实的网络来说，事情会变得指数级地复杂。

92
00:06:51,320 --> 00:06:55,408
但老实说，当我们为各层提供多个神经元时，变化

93
00:06:55,408 --> 00:06:59,320
并没有那么大，实际上只是需要跟踪更多的索引。

94
00:06:59,320 --> 00:07:03,736
给定层的激活不仅仅是 AL，它还会有

95
00:07:03,736 --> 00:07:07,920
一个下标来指示它是该层的哪个神经元。

96
00:07:07,920 --> 00:07:13,143
让我们使用字母 k 来索引层 L-1，使用

97
00:07:13,143 --> 00:07:15,280
j 来索引层 L。

98
00:07:15,280 --> 00:07:20,916
对于成本，我们再次查看所需的输出是什么，但这一次我

99
00:07:20,916 --> 00:07:26,120
们将最后一层激活与所需输出之间的差异的平方相加。

100
00:07:26,120 --> 00:07:33,280
也就是说，将 ALj 减去 yj 平方求和。

101
00:07:33,280 --> 00:07:37,622
由于权重更多，每个权重都必须有更多索引来跟踪

102
00:07:37,622 --> 00:07:41,775
它的位置，因此我们将连接第 k 个神经元和

103
00:07:41,775 --> 00:07:45,740
第 j 个神经元的边的权重称为 WLjk。

104
00:07:45,740 --> 00:07:49,953
这些索引一开始可能感觉有点倒退，但它与我在第

105
00:07:49,953 --> 00:07:53,800
1 部分视频中讨论的权重矩阵索引方式一致。

106
00:07:53,800 --> 00:07:57,584
和以前一样，给相关的加权和命名（比如 z）

107
00:07:57,584 --> 00:08:01,368
还是不错的，这样最后一层的激活就只是你的特

108
00:08:01,368 --> 00:08:04,980
殊函数，比如 sigmoid，应用于 z。

109
00:08:04,980 --> 00:08:10,294
你可以明白我的意思，所有这些基本上与我们之前在每层一个

110
00:08:10,294 --> 00:08:15,420
神经元的情况下使用的方程相同，只是它看起来更复杂一些。

111
00:08:15,420 --> 00:08:19,584
事实上，描述成本对特定权重有多敏感的链

112
00:08:19,584 --> 00:08:23,540
式法则导数表达式看起来本质上是相同的。

113
00:08:23,540 --> 00:08:29,420
如果您愿意，我将让您停下来思考每个术语。

114
00:08:29,420 --> 00:08:33,882
不过，这里发生的变化是成本相对于

115
00:08:33,882 --> 00:08:37,820
L-1 层中的激活之一的导数。

116
00:08:37,820 --> 00:08:40,766
在这种情况下，不同之处在于神经元

117
00:08:40,766 --> 00:08:43,540
通过多个不同的路径影响成本函数。

118
00:08:43,540 --> 00:08:49,420
也就是说，它一方面影响在成本函数中起作用

119
00:08:49,420 --> 00:08:55,020
的AL0，但它也对也在成本函数中起作用

120
00:08:55,020 --> 00:09:00,340
的AL1产生影响，并且必须将它们相加。

121
00:09:00,340 --> 00:09:03,680
嗯，差不多就是这样了。

122
00:09:03,680 --> 00:09:07,232
一旦您知道成本函数对倒数第二层中

123
00:09:07,232 --> 00:09:10,785
的激活有多敏感，您就可以对输入该

124
00:09:10,785 --> 00:09:13,920
层的所有权重和偏差重复该过程。

125
00:09:13,920 --> 00:09:15,420
所以拍拍自己的背吧！

126
00:09:15,420 --> 00:09:19,652
如果所有这些都有意义，那么您现在已经深入了解

127
00:09:19,652 --> 00:09:23,700
了反向传播的核心，即神经网络学习背后的主力。

128
00:09:23,700 --> 00:09:29,606
这些链式法则表达式为您提供了确定梯度中每个分量

129
00:09:29,606 --> 00:09:35,020
的导数，有助于通过重复下坡来最小化网络成本。

130
00:09:35,020 --> 00:09:36,311
如果你坐下来思考所有这些，你会发现这有很

131
00:09:36,311 --> 00:09:37,603
多层复杂的内容需要你 的大脑来思考，所以

132
00:09:37,603 --> 00:09:38,960
不要担心你的大脑是否需要时间来消化这一切。

