1
00:00:04,020 --> 00:00:06,806
这里的硬假设是您已经观看了第 3 

2
00:00:06,806 --> 00:00:09,920
部 分，其中直观地介绍了反向传播算法。

3
00:00:11,040 --> 00:00:14,220
在这里，我们变得更正式一些，并深入研究相关的演算。

4
00:00:14,820 --> 00:00:18,186
这至少有点令人困惑是正常的，因此定期停下来 

5
00:00:18,186 --> 00:00:21,400
思考的咒语当然适用于此，也适用于其他地方。

6
00:00:21,940 --> 00:00:26,009
我们的主要目标是展示机器学习领域的人们如何在网 

7
00:00:26,009 --> 00:00:29,909
络背景下普遍思考微积分的链式法则，这与大多数 

8
00:00:29,909 --> 00:00:33,640
微积分入门课程处理该主题的方式有不同的感觉。

9
00:00:34,340 --> 00:00:36,662
对于那些对相关微积分感到不舒服的人， 

10
00:00:36,662 --> 00:00:38,740
我确实有一个关于该主题的完整系列。

11
00:00:39,960 --> 00:00:46,020
让我们从一个极其简单的网络开始 ，其中每一层都有一个神经元。

12
00:00:46,320 --> 00:00:50,814
该网络由三个权重和三个偏差决定，我们的目 

13
00:00:50,814 --> 00:00:54,880
标是了解成本函数对这些变量的敏感程度。

14
00:00:55,419 --> 00:00:58,981
这样我们就知道对这些项的哪些调 

15
00:00:58,981 --> 00:01:02,320
整将导致成本函数最有效的降低。

16
00:01:02,320 --> 00:01:04,840
我们将只关注最后两个神经元之间的连接。

17
00:01:05,980 --> 00:01:08,751
让我们用上标 L 来标记最后一个 

18
00:01:08,751 --> 00:01:11,360
神经元的激活，指示它位于哪一层。

19
00:01:11,680 --> 00:01:15,560
所以前一个神经元的激活是AL-1。

20
00:01:16,360 --> 00:01:19,839
这些不是指数，它们只是对我们正在讨论的内容进行索 

21
00:01:19,839 --> 00:01:23,040
引的一种方式，因为我想稍后保存不同索引的下标。

22
00:01:23,720 --> 00:01:28,142
假设对于给定的训练示例，我们希望最后一次激活 

23
00:01:28,142 --> 00:01:32,180
的值是 y，例如，y 可能是 0 或 1。

24
00:01:32,840 --> 00:01:39,240
因此，该网络对于单个训练示例的成本是 AL-y2。

25
00:01:40,260 --> 00:01:44,380
我们将该训练示例的成本表示为 c0。

26
00:01:45,900 --> 00:01:51,852
提醒一下，最后的激活是由权重（我将其称为 wL）乘以前一 

27
00:01:51,852 --> 00:01:57,600
个神经元的激活加上一些偏差（我将其称为 bL）来确定的。

28
00:01:57,600 --> 00:01:59,362
然后通过一些特殊的非线性函数（例如 

29
00:01:59,362 --> 00:02:01,320
sigmoid 或 ReLU）将其泵送。

30
00:02:01,800 --> 00:02:05,689
如果我们为这个加权和指定一个特殊的名称（例如 z），并且具 

31
00:02:05,689 --> 00:02:09,320
有与相关激活相同的上标，实际上会让我们的事情变得更容易。

32
00:02:10,380 --> 00:02:15,545
这是很多术语，您可以将其概念化的一种方式是，权重、 

33
00:02:15,545 --> 00:02:20,711
先前的操作和偏差一起用于计算 z，这反过来让我们计 

34
00:02:20,711 --> 00:02:25,480
算 a，最后，与常数 y 一起，让我们计算成本。

35
00:02:27,340 --> 00:02:31,298
当然，AL-1 会受到其自身重量和偏差 

36
00:02:31,298 --> 00:02:35,060
等的影响，但我们现在不打算关注这一点。

37
00:02:35,700 --> 00:02:37,620
所有这些都只是数字，对吧？

38
00:02:38,060 --> 00:02:41,040
认为每个数都有自己的小数轴是件好事。

39
00:02:41,720 --> 00:02:45,470
我们的第一个目标是了解成本函数对 

40
00:02:45,470 --> 00:02:49,000
权重 wL 的微小变化有多敏感。

41
00:02:49,540 --> 00:02:54,860
或者换句话说，c 对 wL 的导数是多少？

42
00:02:55,600 --> 00:02:59,753
当您看到这个 del w 术语时，请将其视为对 w 

43
00:02:59,753 --> 00:03:02,468
的一些微小推动，例如 0 的更改。

44
00:03:02,468 --> 00:03:06,622
01，并认为这个 del c 术语的含义是无论最终对

45
00:03:06,622 --> 00:03:08,060
成本的影响是什么。

46
00:03:08,060 --> 00:03:10,220
我们想要的是它们的比例。

47
00:03:11,260 --> 00:03:16,434
从概念上讲，对 wL 的微小推动会导致对 zL 的一些 

48
00:03:16,434 --> 00:03:21,240
推动，进而对 AL 产生一些推动，从而直接影响成本。

49
00:03:23,120 --> 00:03:27,838
因此，我们首先查看 zL 的微小变化与 w 

50
00:03:27,838 --> 00:03:33,200
的 微小变化之比，即 zL 相对于 wL 的导数。

51
00:03:33,200 --> 00:03:37,593
同样，然后考虑 AL 的变化与引起它的 zL 

52
00:03:37,593 --> 00:03:42,559
的微小变化的比率，以及最终对 c 的微调与对 AL 

53
00:03:42,559 --> 00:03:44,660
的中间微调之间的比率。

54
00:03:45,740 --> 00:03:50,687
这就是链式法则，将这三个比率相乘即可得 

55
00:03:50,687 --> 00:03:55,140
出 c 对 wL 微小变化的敏感度。

56
00:03:56,880 --> 00:04:01,782
现在屏幕上有很多符号，请花点时间确保清楚它 

57
00:04:01,782 --> 00:04:06,240
们是什么，因为现在我们要计算相关的导数。

58
00:04:07,440 --> 00:04:14,180
c 对 AL 的导数为 2AL-y。

59
00:04:14,180 --> 00:04:18,687
这意味着它的大小与网络输出和我们想要的结果之间 

60
00:04:18,687 --> 00:04:23,007
的差异成正比，因此如果输出非常不同，即使是微 

61
00:04:23,007 --> 00:04:27,140
小的变化也会对最终的成本函数产生很大的影响。

62
00:04:27,840 --> 00:04:32,821
AL 相对于 zL 的导数只是我们的 sigmoi 

63
00:04:32,821 --> 00:04:37,420
d 函数的导数，或者您选择使用的任何非线性函数。

64
00:04:37,420 --> 00:04:46,160
zL 对 wL 的导数为 AL-1。

65
00:04:46,160 --> 00:04:49,874
我不了解你的情况，但我认为你很容易陷入公式 

66
00:04:49,874 --> 00:04:53,420
中，而不花点时间坐下来提醒自己它们的含义。

67
00:04:53,920 --> 00:04:58,478
在最后一个导数的情况下，权重的微小推动对 

68
00:04:58,478 --> 00:05:02,820
最后一层的影响取决于前一个神经元的强度。

69
00:05:03,380 --> 00:05:08,280
请记住，这就是“神经元一起发射、连线在一起”这一想法的由来。

70
00:05:09,200 --> 00:05:15,720
所有这些都是特定单个训练示例 的成本相对于 wL 的导数。

71
00:05:16,440 --> 00:05:20,646
由于完整的成本函数涉及将许多不同训练示例 

72
00:05:20,646 --> 00:05:24,853
中的所有这些成本平均在一起，因此其导数需 

73
00:05:24,853 --> 00:05:28,660
要对所有训练示例上的该表达式进行平均。

74
00:05:28,660 --> 00:05:33,652
当然，这只是梯度向量的一个组成部分，梯度向量是根据 

75
00:05:33,652 --> 00:05:38,260
成本函数相对于所有这些权重和偏差的偏导数构建的。

76
00:05:40,640 --> 00:05:43,012
尽管这只是我们需要的众多偏导数之一， 

77
00:05:43,012 --> 00:05:45,260
但它已经完成了超过 50% 的工作。

78
00:05:46,340 --> 00:05:49,720
例如，对偏差的敏感性几乎相同。

79
00:05:50,040 --> 00:05:52,530
我们只需要将这个 del z del w 

80
00:05:52,530 --> 00:05:55,020
项改为 a del z del b 即可。

81
00:05:58,420 --> 00:06:02,400
如果你查看相关公式，就会发现该导数为 1。

82
00:06:06,140 --> 00:06:11,051
此外，这就是向后传播的想法出现的地方，您可 

83
00:06:11,051 --> 00:06:15,740
以看到这个成本函数对前一层的激活有多敏感。

84
00:06:15,740 --> 00:06:20,820
也就是说，链式法则表达式中的初始导数，即 

85
00:06:20,820 --> 00:06:25,660
z 对先前激活的敏感度，就是权重 wL。

86
00:06:26,640 --> 00:06:30,736
再说一次，即使我们无法直接影响前一层的激 

87
00:06:30,736 --> 00:06:34,637
活，但跟踪它还是很有帮助的，因为现在我 

88
00:06:34,637 --> 00:06:38,733
们可以不断向后迭代这个相同的链规则思想， 

89
00:06:38,733 --> 00:06:42,440
看看成本函数对之前的权重和之前的偏差。

90
00:06:43,180 --> 00:06:47,245
你可能会认为这是一个过于简单的例子，因为所有层都有一个 

91
00:06:47,245 --> 00:06:51,020
神经元，对于真实的网络来说，事情会变得指数级地复杂。

92
00:06:51,700 --> 00:06:55,359
但老实说，当我们为各层提供多个神经元时，变化 

93
00:06:55,359 --> 00:06:58,860
并没有那么大，实际上只是需要跟踪更多的索引。

94
00:06:59,380 --> 00:07:03,375
给定层的激活不仅仅是 AL，它还会有 

95
00:07:03,375 --> 00:07:07,160
一个下标来指示它是该层的哪个神经元。

96
00:07:07,160 --> 00:07:12,312
让我们使用字母 k 来索引层 L-1，使用 

97
00:07:12,312 --> 00:07:14,420
j 来索引层 L。

98
00:07:15,260 --> 00:07:20,418
对于成本，我们再次查看所需的输出是什么，但这一次我 

99
00:07:20,418 --> 00:07:25,180
们将最后一层激活与所需输出之间的差异的平方相加。

100
00:07:26,080 --> 00:07:30,840
也就是说，将 ALj 减去 yj 平方求和。

101
00:07:33,040 --> 00:07:37,180
由于权重更多，每个权重都必须有更多索引来跟踪 

102
00:07:37,180 --> 00:07:41,140
它的位置，因此我们将连接第 k 个神经元和 

103
00:07:41,140 --> 00:07:44,920
第 j 个神经元的边的权重称为 WLjk。

104
00:07:45,620 --> 00:07:49,540
这些索引一开始可能感觉有点倒退，但它与我在第 

105
00:07:49,540 --> 00:07:53,120
1 部分视频中讨论的权重矩阵索引方式一致。

106
00:07:53,620 --> 00:07:57,187
和以前一样，给相关的加权和命名（比如 z） 

107
00:07:57,187 --> 00:08:00,754
还是不错的，这样最后一层的激活就只是你的特 

108
00:08:00,754 --> 00:08:04,160
殊函数，比如 sigmoid，应用于 z。

109
00:08:04,660 --> 00:08:09,252
你可以明白我的意思，所有这些基本上与我们之前在每层一个 

110
00:08:09,252 --> 00:08:13,680
神经元的情况下使用的方程相同，只是它看起来更复杂一些。

111
00:08:15,440 --> 00:08:19,532
事实上，描述成本对特定权重有多敏感的链 

112
00:08:19,532 --> 00:08:23,420
式法则导数表达式看起来本质上是相同的。

113
00:08:23,920 --> 00:08:26,840
如果您愿意，我将让您停下来思考每个术语。

114
00:08:28,979 --> 00:08:33,059
不过，这里发生的变化是成本相对于 

115
00:08:33,059 --> 00:08:36,659
L-1 层中的激活之一的导数。

116
00:08:37,780 --> 00:08:40,407
在这种情况下，不同之处在于神经元 

117
00:08:40,407 --> 00:08:42,880
通过多个不同的路径影响成本函数。

118
00:08:44,680 --> 00:08:49,180
也就是说，它一方面影响在成本函数中起作用 

119
00:08:49,180 --> 00:08:53,467
的AL0，但它也对也在成本函数中起作用 

120
00:08:53,467 --> 00:08:57,540
的AL1产生影响，并且必须将它们相加。

121
00:08:59,820 --> 00:09:03,040
嗯，差不多就是这样了。

122
00:09:03,500 --> 00:09:06,747
一旦您知道成本函数对倒数第二层中 

123
00:09:06,747 --> 00:09:09,994
的激活有多敏感，您就可以对输入该 

124
00:09:09,994 --> 00:09:12,860
层的所有权重和偏差重复该过程。

125
00:09:13,900 --> 00:09:14,960
所以拍拍自己的背吧！

126
00:09:15,300 --> 00:09:19,071
如果所有这些都有意义，那么您现在已经深入了解 

127
00:09:19,071 --> 00:09:22,680
了反向传播的核心，即神经网络学习背后的主力。

128
00:09:23,300 --> 00:09:28,517
这些链式法则表达式为您提供了确定梯度中每个分量 

129
00:09:28,517 --> 00:09:33,300
的导数，有助于通过重复下坡来最小化网络成本。

130
00:09:34,300 --> 00:09:37,067
如果你坐下来思考所有这些，你会发现这有很

131
00:09:37,067 --> 00:09:39,834
多层复杂的内容需要你 的大脑来思考，所以

132
00:09:39,834 --> 00:09:42,740
不要担心你的大脑是否需要时间来消化这一切。

