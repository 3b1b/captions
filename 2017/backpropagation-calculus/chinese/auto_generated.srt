1
00:00:04,020 --> 00:00:07,461
这里的硬假设是您已经观看了第 3 部 分，

2
00:00:07,461 --> 00:00:09,920
其中直观地介绍了反向传播算法。

3
00:00:11,040 --> 00:00:14,220
在这里，我们变得更正式一些，并深入研究相关的演算。

4
00:00:14,820 --> 00:00:18,186
这至少有点令人困惑是正常的，因此定期停下来 

5
00:00:18,186 --> 00:00:21,400
思考的咒语当然适用于此，也适用于其他地方。

6
00:00:21,940 --> 00:00:26,009
我们的主要目标是展示机器学习领域的人们如何在网 

7
00:00:26,009 --> 00:00:28,892
络背景下普遍思考微积分的链式法则，

8
00:00:28,892 --> 00:00:33,640
这与大多数 微积分入门课程处理该主题的方式有不同的感觉。

9
00:00:34,340 --> 00:00:36,540
对于那些对相关微积分感到不舒服的人，

10
00:00:36,540 --> 00:00:38,740
 我确实有一个关于该主题的完整系列。

11
00:00:39,960 --> 00:00:46,020
让我们从一个极其简单的网络开始 ，其中每一层都有一个神经元。

12
00:00:46,320 --> 00:00:49,744
该网络由三个权重和三个偏差决定，

13
00:00:49,744 --> 00:00:54,880
我们的目 标是了解成本函数对这些变量的敏感程度。

14
00:00:55,419 --> 00:00:58,981
这样我们就知道对这些项的哪些调 

15
00:00:58,981 --> 00:01:02,320
整将导致成本函数最有效的降低。

16
00:01:02,320 --> 00:01:04,840
我们将只关注最后两个神经元之间的连接。

17
00:01:05,980 --> 00:01:09,892
让我们用上标 L 来标记最后一个 神经元的激活，

18
00:01:09,892 --> 00:01:11,360
指示它位于哪一层。

19
00:01:11,680 --> 00:01:15,560
所以前一个神经元的激活是AL-1。

20
00:01:16,360 --> 00:01:19,839
这些不是指数，它们只是对我们正在讨论的内容进行索 

21
00:01:19,839 --> 00:01:23,040
引的一种方式，因为我想稍后保存不同索引的下标。

22
00:01:23,720 --> 00:01:29,295
假设对于给定的训练示例，我们希望最后一次激活 的值是 y，

23
00:01:29,295 --> 00:01:32,180
例如，y 可能是 0 或 1。

24
00:01:32,840 --> 00:01:39,240
因此，该网络对于单个训练示例的成本是 AL-y2。

25
00:01:40,260 --> 00:01:44,380
我们将该训练示例的成本表示为 c0。

26
00:01:45,900 --> 00:01:51,852
提醒一下，最后的激活是由权重（我将其称为 wL）乘以前一 

27
00:01:51,852 --> 00:01:57,600
个神经元的激活加上一些偏差（我将其称为 bL）来确定的。

28
00:01:57,600 --> 00:01:59,362
然后通过一些特殊的非线性函数（例如 

29
00:01:59,362 --> 00:02:01,320
sigmoid 或 ReLU）将其泵送。

30
00:02:01,800 --> 00:02:05,171
如果我们为这个加权和指定一个特殊的名称（例如 z），

31
00:02:05,171 --> 00:02:07,245
并且具 有与相关激活相同的上标，

32
00:02:07,245 --> 00:02:09,320
实际上会让我们的事情变得更容易。

33
00:02:10,380 --> 00:02:14,751
这是很多术语，您可以将其概念化的一种方式是，

34
00:02:14,751 --> 00:02:18,923
权重、 先前的操作和偏差一起用于计算 z，

35
00:02:18,923 --> 00:02:23,890
这反过来让我们计 算 a，最后，与常数 y 一起，

36
00:02:23,890 --> 00:02:25,480
让我们计算成本。

37
00:02:27,340 --> 00:02:32,288
当然，AL-1 会受到其自身重量和偏差 等的影响，

38
00:02:32,288 --> 00:02:35,060
但我们现在不打算关注这一点。

39
00:02:35,700 --> 00:02:37,620
所有这些都只是数字，对吧？

40
00:02:38,060 --> 00:02:41,040
认为每个数都有自己的小数轴是件好事。

41
00:02:41,720 --> 00:02:45,470
我们的第一个目标是了解成本函数对 

42
00:02:45,470 --> 00:02:49,000
权重 wL 的微小变化有多敏感。

43
00:02:49,540 --> 00:02:54,860
或者换句话说，c 对 wL 的导数是多少？

44
00:02:55,600 --> 00:02:58,315
当您看到这个 del w 术语时，

45
00:02:58,315 --> 00:03:02,468
请将其视为对 w 的一些微小推动，例如 0 的更改。

46
00:03:02,468 --> 00:03:06,622
01，并认为这个 del c 术语的含义是无论最终对

47
00:03:06,622 --> 00:03:08,060
成本的影响是什么。

48
00:03:08,060 --> 00:03:10,220
我们想要的是它们的比例。

49
00:03:11,260 --> 00:03:16,434
从概念上讲，对 wL 的微小推动会导致对 zL 的一些 

50
00:03:16,434 --> 00:03:21,240
推动，进而对 AL 产生一些推动，从而直接影响成本。

51
00:03:23,120 --> 00:03:27,838
因此，我们首先查看 zL 的微小变化与 w 

52
00:03:27,838 --> 00:03:33,200
的 微小变化之比，即 zL 相对于 wL 的导数。

53
00:03:33,200 --> 00:03:37,593
同样，然后考虑 AL 的变化与引起它的 zL 

54
00:03:37,593 --> 00:03:42,559
的微小变化的比率，以及最终对 c 的微调与对 AL 

55
00:03:42,559 --> 00:03:44,660
的中间微调之间的比率。

56
00:03:45,740 --> 00:03:50,687
这就是链式法则，将这三个比率相乘即可得 

57
00:03:50,687 --> 00:03:55,140
出 c 对 wL 微小变化的敏感度。

58
00:03:56,880 --> 00:04:02,897
现在屏幕上有很多符号，请花点时间确保清楚它 们是什么，

59
00:04:02,897 --> 00:04:06,240
因为现在我们要计算相关的导数。

60
00:04:07,440 --> 00:04:14,180
c 对 AL 的导数为 2AL-y。

61
00:04:14,180 --> 00:04:18,687
这意味着它的大小与网络输出和我们想要的结果之间 

62
00:04:18,687 --> 00:04:22,068
的差异成正比，因此如果输出非常不同，

63
00:04:22,068 --> 00:04:27,140
即使是微 小的变化也会对最终的成本函数产生很大的影响。

64
00:04:27,840 --> 00:04:32,821
AL 相对于 zL 的导数只是我们的 sigmoi 

65
00:04:32,821 --> 00:04:37,420
d 函数的导数，或者您选择使用的任何非线性函数。

66
00:04:37,420 --> 00:04:46,160
zL 对 wL 的导数为 AL-1。

67
00:04:46,160 --> 00:04:50,212
我不了解你的情况，但我认为你很容易陷入公式 中，

68
00:04:50,212 --> 00:04:53,420
而不花点时间坐下来提醒自己它们的含义。

69
00:04:53,920 --> 00:04:58,478
在最后一个导数的情况下，权重的微小推动对 

70
00:04:58,478 --> 00:05:02,820
最后一层的影响取决于前一个神经元的强度。

71
00:05:03,380 --> 00:05:08,280
请记住，这就是“神经元一起发射、连线在一起”这一想法的由来。

72
00:05:09,200 --> 00:05:15,720
所有这些都是特定单个训练示例 的成本相对于 wL 的导数。

73
00:05:16,440 --> 00:05:20,646
由于完整的成本函数涉及将许多不同训练示例 

74
00:05:20,646 --> 00:05:24,853
中的所有这些成本平均在一起，因此其导数需 

75
00:05:24,853 --> 00:05:28,660
要对所有训练示例上的该表达式进行平均。

76
00:05:28,660 --> 00:05:32,116
当然，这只是梯度向量的一个组成部分，

77
00:05:32,116 --> 00:05:36,916
梯度向量是根据 成本函数相对于所有这些权重和偏差的

78
00:05:36,916 --> 00:05:38,260
偏导数构建的。

79
00:05:40,640 --> 00:05:42,887
尽管这只是我们需要的众多偏导数之一，

80
00:05:42,887 --> 00:05:45,260
 但它已经完成了超过 50% 的工作。

81
00:05:46,340 --> 00:05:49,720
例如，对偏差的敏感性几乎相同。

82
00:05:50,040 --> 00:05:52,530
我们只需要将这个 del z del w 

83
00:05:52,530 --> 00:05:55,020
项改为 a del z del b 即可。

84
00:05:58,420 --> 00:06:02,400
如果你查看相关公式，就会发现该导数为 1。

85
00:06:06,140 --> 00:06:10,381
此外，这就是向后传播的想法出现的地方，

86
00:06:10,381 --> 00:06:15,740
您可 以看到这个成本函数对前一层的激活有多敏感。

87
00:06:15,740 --> 00:06:20,337
也就是说，链式法则表达式中的初始导数，

88
00:06:20,337 --> 00:06:25,660
即 z 对先前激活的敏感度，就是权重 wL。

89
00:06:26,640 --> 00:06:31,126
再说一次，即使我们无法直接影响前一层的激 活，

90
00:06:31,126 --> 00:06:34,637
但跟踪它还是很有帮助的，因为现在我 

91
00:06:34,637 --> 00:06:38,538
们可以不断向后迭代这个相同的链规则思想，

92
00:06:38,538 --> 00:06:42,440
 看看成本函数对之前的权重和之前的偏差。

93
00:06:43,180 --> 00:06:45,793
你可能会认为这是一个过于简单的例子，

94
00:06:45,793 --> 00:06:49,277
因为所有层都有一个 神经元，对于真实的网络来说，

95
00:06:49,277 --> 00:06:51,020
事情会变得指数级地复杂。

96
00:06:51,700 --> 00:06:54,882
但老实说，当我们为各层提供多个神经元时，

97
00:06:54,882 --> 00:06:58,860
变化 并没有那么大，实际上只是需要跟踪更多的索引。

98
00:06:59,380 --> 00:07:03,375
给定层的激活不仅仅是 AL，它还会有 

99
00:07:03,375 --> 00:07:07,160
一个下标来指示它是该层的哪个神经元。

100
00:07:07,160 --> 00:07:11,609
让我们使用字母 k 来索引层 L-1，

101
00:07:11,609 --> 00:07:14,420
使用 j 来索引层 L。

102
00:07:15,260 --> 00:07:19,228
对于成本，我们再次查看所需的输出是什么，

103
00:07:19,228 --> 00:07:25,180
但这一次我 们将最后一层激活与所需输出之间的差异的平方相加。

104
00:07:26,080 --> 00:07:30,840
也就是说，将 ALj 减去 yj 平方求和。

105
00:07:33,040 --> 00:07:38,080
由于权重更多，每个权重都必须有更多索引来跟踪 它的位置，

106
00:07:38,080 --> 00:07:41,860
因此我们将连接第 k 个神经元和 第 j 

107
00:07:41,860 --> 00:07:44,920
个神经元的边的权重称为 WLjk。

108
00:07:45,620 --> 00:07:48,347
这些索引一开始可能感觉有点倒退，

109
00:07:48,347 --> 00:07:53,120
但它与我在第 1 部分视频中讨论的权重矩阵索引方式一致。

110
00:07:53,620 --> 00:07:58,160
和以前一样，给相关的加权和命名（比如 z） 还是不错的，

111
00:07:58,160 --> 00:08:01,403
这样最后一层的激活就只是你的特 殊函数，

112
00:08:01,403 --> 00:08:04,160
比如 sigmoid，应用于 z。

113
00:08:04,660 --> 00:08:09,252
你可以明白我的意思，所有这些基本上与我们之前在每层一个 

114
00:08:09,252 --> 00:08:13,680
神经元的情况下使用的方程相同，只是它看起来更复杂一些。

115
00:08:15,440 --> 00:08:19,532
事实上，描述成本对特定权重有多敏感的链 

116
00:08:19,532 --> 00:08:23,420
式法则导数表达式看起来本质上是相同的。

117
00:08:23,920 --> 00:08:26,840
如果您愿意，我将让您停下来思考每个术语。

118
00:08:28,979 --> 00:08:33,059
不过，这里发生的变化是成本相对于 

119
00:08:33,059 --> 00:08:36,659
L-1 层中的激活之一的导数。

120
00:08:37,780 --> 00:08:40,407
在这种情况下，不同之处在于神经元 

121
00:08:40,407 --> 00:08:42,880
通过多个不同的路径影响成本函数。

122
00:08:44,680 --> 00:08:50,252
也就是说，它一方面影响在成本函数中起作用 的AL0，

123
00:08:50,252 --> 00:08:55,396
但它也对也在成本函数中起作用 的AL1产生影响，

124
00:08:55,396 --> 00:08:57,540
并且必须将它们相加。

125
00:08:59,820 --> 00:09:03,040
嗯，差不多就是这样了。

126
00:09:03,500 --> 00:09:08,275
一旦您知道成本函数对倒数第二层中 的激活有多敏感，

127
00:09:08,275 --> 00:09:12,860
您就可以对输入该 层的所有权重和偏差重复该过程。

128
00:09:13,900 --> 00:09:14,960
所以拍拍自己的背吧！

129
00:09:15,300 --> 00:09:19,071
如果所有这些都有意义，那么您现在已经深入了解 

130
00:09:19,071 --> 00:09:22,680
了反向传播的核心，即神经网络学习背后的主力。

131
00:09:23,300 --> 00:09:29,386
这些链式法则表达式为您提供了确定梯度中每个分量 的导数，

132
00:09:29,386 --> 00:09:33,300
有助于通过重复下坡来最小化网络成本。

133
00:09:34,300 --> 00:09:37,067
如果你坐下来思考所有这些，你会发现这有很

134
00:09:37,067 --> 00:09:39,557
多层复杂的内容需要你 的大脑来思考，

135
00:09:39,557 --> 00:09:42,740
所以不要担心你的大脑是否需要时间来消化这一切。

