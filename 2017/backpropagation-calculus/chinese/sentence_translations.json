[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "这里的硬假设是您已经观看了第 3 部 分，其中直观地介绍了反向传播算法。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "在这里，我们变得更正式一些，并深入研究相关的演算。",
  "model": "google_nmt",
  "from_community_srt": "这集开始我们就假设你已经看过第三集了 那集让大家直观上感受反向传播算法的原理 在这集里  我们会更深入讲解一些其中的微积分理论 这个看不太懂很正常 所以  我们的六字格言“停一停想一想”在这依旧管用",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "这至少有点令人困惑是正常的，因此定期停下来 思考的咒语当然适用于此，也适用于其他地方。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "我们的主要目标是展示机器学习领域的人们如何在网 络背景下普遍思考微积分的链式法则，这与大多数 微积分入门课程处理该主题的方式有不同的感觉。",
  "model": "google_nmt",
  "from_community_srt": "这集我们的目标  是给大家展示在机器学习中 我们一般是怎么理解链式法则的 这点跟别的基础微积分课讲得会有点不一样 对于微积分不够熟悉的观众 我之前已经做了一整个系列了 我们从最最简单的网络讲起吧",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "对于那些对相关微积分感到不舒服的人， 我确实有一个关于该主题的完整系列。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "让我们从一个极其简单的网络开始 ，其中每一层都有一个神经元。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "该网络由三个权重和三个偏差决定，我们的目 标是了解成本函数对这些变量的敏感程度。",
  "model": "google_nmt",
  "from_community_srt": "每层只有一个神经元 图上这个网络就是由3个权重和3个偏置决定的 我们的目标是理解代价函数对于这些变量有多敏感 这样 我们就知道怎么调整这些变量 才可以使得代价降低得最快 我们先来关注最后两个神经元吧",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "这样我们就知道对这些项的哪些调 整将导致成本函数最有效的降低。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "We'll just focus on the connection between the last two neurons.",
  "translatedText": "我们将只关注最后两个神经元之间的连接。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in.",
  "translatedText": "让我们用上标 L 来标记最后一个 神经元的激活，指示它位于哪一层。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.98,
  "end": 71.36
 },
 {
  "input": "So the activation of the previous neuron is AL-1.",
  "translatedText": "所以前一个神经元的激活是AL-1。",
  "model": "google_nmt",
  "from_community_srt": "我给最后一个神经元的激活值一个上标L  表示它处在第L层 那么  前一个神经元的激活值就是a^(L-1) 这不是指数  而是用来标记我们正在讨论哪一层 过一会我会用到下标来表示别的意思",
  "n_reviews": 0,
  "start": 71.68,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "这些不是指数，它们只是对我们正在讨论的内容进行索 引的一种方式，因为我想稍后保存不同索引的下标。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "假设对于给定的训练示例，我们希望最后一次激活 的值是 y，例如，y 可能是 0 或 1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is AL-y2.",
  "translatedText": "因此，该网络对于单个训练示例的成本是 AL-y2。",
  "model": "google_nmt",
  "from_community_srt": "给定一个训练样本  我们把这个最终层激活值要接近的目标叫做y 那么y就要么是0  要么是1 那么这个简易网络对于单个训练样本的代价  就等于 (a^(L) - y)^2 对于这个样本  我们把这个代价值标记为C_0",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "我们将该训练示例的成本表示为 c0。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call wL, times the previous neuron's activation plus some bias, which I'll call bL.",
  "translatedText": "提醒一下，最后的激活是由权重（我将其称为 wL）乘以前一 个神经元的激活加上一些偏差（我将其称为 bL）来确定的。",
  "model": "google_nmt",
  "from_community_srt": "还记得吗  最终层的激活值是这么算出来的—— 一个权重 w^(L) 乘上前一个神经元的激活值 再加上一个偏置b^(L) 最后把加权和塞进一个特定的非线性函数 例如sigmoid ReLU之类的",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "Then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "然后通过一些特殊的非线性函数（例如 sigmoid 或 ReLU）将其泵送。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "如果我们为这个加权和指定一个特殊的名称（例如 z），并且具 有与相关激活相同的上标，实际上会让我们的事情变得更容易。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action, and bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "这是很多术语，您可以将其概念化的一种方式是，权重、 先前的操作和偏差一起用于计算 z，这反过来让我们计 算 a，最后，与常数 y 一起，让我们计算成本。",
  "model": "google_nmt",
  "from_community_srt": "给这个加权和一个名字会方便很多  就叫它z好了 跟对应的激活值用同一个上标 这里的项挺多 概括起来 我们拿这个权重 前一个激活值 和这个偏置值 一起来算出z  再算出a 最后再用上常量y  算出代价",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course, AL-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "当然，AL-1 会受到其自身重量和偏差 等的影响，但我们现在不打算关注这一点。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "所有这些都只是数字，对吧？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "认为每个数都有自己的小数轴是件好事。",
  "model": "google_nmt",
  "from_community_srt": "当然  a^(L-1)是由它自己的权重和偏置决定的  以此类推 但我们现在重点不在那里 这些东西都是数字  没错吧 我们可以想象每个数字都对应一个数轴 我们第一个目标是理解 代价函数对权重w^(L)的微小变化有多敏感",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight wL.",
  "translatedText": "我们的第一个目标是了解成本函数对 权重 wL 的微小变化有多敏感。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to wL?",
  "translatedText": "或者换句话说，c 对 wL 的导数是多少？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del w term, think of it as meaning some tiny nudge to w, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "当您看到这个 del w 术语时，请将其视为对 w 的一些微小推动，例如 0 的更改。01，并认为这个 del c 术语的含义是无论最终对成本的影响是什么。",
  "model": "google_nmt",
  "from_community_srt": "或者换句话讲  求C对w^(L)的导数 当你看到∂w之类的项时 请把它当做这是对w的微小扰动  好比变个0.01 然后把∂C当做 “改变w对C的值造成的变化” 我们求的是这两个数的比值",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "我们想要的是它们的比例。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to wL causes some nudge to zL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "从概念上讲，对 wL 的微小推动会导致对 zL 的一些 推动，进而对 AL 产生一些推动，从而直接影响成本。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to zL to this tiny change w, that is, the derivative of zL with respect to wL.",
  "translatedText": "因此，我们首先查看 zL 的微小变化与 w 的 微小变化之比，即 zL 相对于 wL 的导数。",
  "model": "google_nmt",
  "from_community_srt": "概念上说  w^(L)的微小变化会导致z^(L)产生些变化 然后会导致a^(L)产生变化  最终影响到代价值 那么 我们把式子拆开  首先求z^(L)的变化量比上w^(L)的变化量 也就是求z^(L)关于w^(L)的导数",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in zL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "同样，然后考虑 AL 的变化与引起它的 zL 的微小变化的比率，以及最终对 c 的微调与对 AL 的中间微调之间的比率。",
  "model": "google_nmt",
  "from_community_srt": "同理 考虑a^(L)的变化量 比上因变量z^(L)的变化量 以及最终的C的变化量 比上直接改动a^(L)产生的变化量 这不就是链式法则么 把三个比相乘  就可以算出C对w^(L)的微小变化有多敏感",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying these three ratios gives us the sensitivity of c to small changes in wL.",
  "translatedText": "这就是链式法则，将这三个比率相乘即可得 出 c 对 wL 微小变化的敏感度。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "现在屏幕上有很多符号，请花点时间确保清楚它 们是什么，因为现在我们要计算相关的导数。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "c 对 AL 的导数为 2AL-y。",
  "model": "google_nmt",
  "from_community_srt": "现在屏幕上多了一大坨符号 稍稍花点时间理解一下每个符号都什么意思吧 马上我们就要对各个求导了 C关于a^(L)的导数  就是2(a^(L) - y) 这也就意味着  导数的大小 跟网络最终输出减目标结果的差成正比",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "This means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "这意味着它的大小与网络输出和我们想要的结果之间 的差异成正比，因此如果输出非常不同，即使是微 小的变化也会对最终的成本函数产生很大的影响。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to zL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "AL 相对于 zL 的导数只是我们的 sigmoi d 函数的导数，或者您选择使用的任何非线性函数。",
  "model": "google_nmt",
  "from_community_srt": "如果网络的输出差别很大 即使w稍稍变一点  代价也会改变非常大 a^(L)对z^(L)求导就是求sigmoid的导数 或就你选择的非线性激活函数 而z^(L)对w^(L)求导 结果就是a^(L-1)",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "The derivative of zL with respect to wL comes out to be AL-1.",
  "translatedText": "zL 对 wL 的导数为 AL-1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself what they all mean.",
  "translatedText": "我不了解你的情况，但我认为你很容易陷入公式 中，而不花点时间坐下来提醒自己它们的含义。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "在最后一个导数的情况下，权重的微小推动对 最后一层的影响取决于前一个神经元的强度。",
  "model": "google_nmt",
  "from_community_srt": "对我自己来说   这里如果不退一步 好好想想这些公式的含义  很容易卡住 就最后这个导数来说 这个权重的改变量∂w对最后一层的影响有多大 取决于之前一层的神经元 所谓“一同激活的神经元关联在一起”的出处即来源于此",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "请记住，这就是“神经元一起发射、连线在一起”这一想法的由来。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to wL only of the cost for a specific single training example.",
  "translatedText": "所有这些都是特定单个训练示例 的成本相对于 wL 的导数。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "由于完整的成本函数涉及将许多不同训练示例 中的所有这些成本平均在一起，因此其导数需 要对所有训练示例上的该表达式进行平均。",
  "model": "google_nmt",
  "from_community_srt": "不过这只是包含一个训练样本的代价对w^(L)的导数 由于总的代价函数是许许多多训练样本所有代价的总平均 它对w^(L)的导数就需要求  这个表达式之于每一个训练样本的平均 当然这只是梯度向量∇C的一个分量",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "Of course, that's just one component of the gradient vector, which is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "当然，这只是梯度向量的一个组成部分，梯度向量是根据 成本函数相对于所有这些权重和偏差的偏导数构建的。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "尽管这只是我们需要的众多偏导数之一， 但它已经完成了超过 50% 的工作。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "例如，对偏差的敏感性几乎相同。",
  "model": "google_nmt",
  "from_community_srt": "而梯度向量∇C本身 则由代价函数对每一个权重和每一个偏置求偏导构成 求出这些偏导中的一个 就完成了一大半的工作量 对偏置的求导步骤也基本相同 只要把∂z/∂w替换成∂z/∂b即可 对应的公式中可以看出 导数∂z/∂b等于1",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "我们只需要将这个 del z del w 项改为 a del z del b 即可。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "如果你查看相关公式，就会发现该导数为 1。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "此外，这就是向后传播的想法出现的地方，您可 以看到这个成本函数对前一层的激活有多敏感。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight wL.",
  "translatedText": "也就是说，链式法则表达式中的初始导数，即 z 对先前激活的敏感度，就是权重 wL。",
  "model": "google_nmt",
  "from_community_srt": "这里也涉及到了反向传播的概念 我们来看下这个代价函数对上一层的激活值的敏感度 展开来说 链式法则的第一项 z对上一层激活值的敏感度 就是权重w^(L) 虽然 说过 我们不能直接改变激活值",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "再说一次，即使我们无法直接影响前一层的激 活，但跟踪它还是很有帮助的，因为现在我 们可以不断向后迭代这个相同的链规则思想， 看看成本函数对之前的权重和之前的偏差。",
  "model": "google_nmt",
  "from_community_srt": "但我们很有必要关注这个值 因为我们可以反向应用链式法则 来计算代价函数对之前的权重和偏置的敏感度 你可能觉得这个例子举得太简单了 毕竟每层只有一个神经元 而真实的神经网络会比这个例子复杂百倍",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "你可能会认为这是一个过于简单的例子，因为所有层都有一个 神经元，对于真实的网络来说，事情会变得指数级地复杂。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "但老实说，当我们为各层提供多个神经元时，变化 并没有那么大，实际上只是需要跟踪更多的索引。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "给定层的激活不仅仅是 AL，它还会有 一个下标来指示它是该层的哪个神经元。",
  "model": "google_nmt",
  "from_community_srt": "然而说真的 每层多加若干个神经元并不会复杂很多 真的 只不过多写一些下标罢了 我们用加上下标的神经元来表示L层的若干个神经元 而不是用a^(L)统称L层的激活值 现在用k来标注(L-1)层的神经元 j则是L层的神经元",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "让我们使用字母 k 来索引层 L-1，使用 j 来索引层 L。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "对于成本，我们再次查看所需的输出是什么，但这一次我 们将最后一层激活与所需输出之间的差异的平方相加。",
  "model": "google_nmt",
  "from_community_srt": "要求代价函数 我们从期望的输出着手 计算上一层激活值和期望输出的差值的平方 然后求和 计算上一层激活值和期望输出的差值的平方 然后求和 即求(a_j^(L) - y_j)^2的和 由于权重的数量多了不少",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus yj squared.",
  "translatedText": "也就是说，将 ALj 减去 yj 平方求和。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "由于权重更多，每个权重都必须有更多索引来跟踪 它的位置，因此我们将连接第 k 个神经元和 第 j 个神经元的边的权重称为 WLjk。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "这些索引一开始可能感觉有点倒退，但它与我在第 1 部分视频中讨论的权重矩阵索引方式一致。",
  "model": "google_nmt",
  "from_community_srt": "那么每个权重要多用几个下标 我们记连接第k个神经元和第j个神经元的连线为w_{jk}^(L) 这些下标感觉像标反了 可能有点别扭 不过和第一集视频中的权重矩阵的下标是一致的 同样的 把加权和记为z 总是很方便",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "和以前一样，给相关的加权和命名（比如 z） 还是不错的，这样最后一层的激活就只是你的特 殊函数，比如 sigmoid，应用于 z。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "你可以明白我的意思，所有这些基本上与我们之前在每层一个 神经元的情况下使用的方程相同，只是它看起来更复杂一些。",
  "model": "google_nmt",
  "from_community_srt": "那么最后一层的激活值依然等于指定的函数在z处的函数值 你懂我的意思吧 现在的方程式和之前每层只有一个神经元的时候本质是一样的 只是看着复杂一些 链式法则形式的导数表达式所描述的 代价对某个权重的敏感度",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain rule derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "事实上，描述成本对特定权重有多敏感的链 式法则导数表达式看起来本质上是相同的。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "如果您愿意，我将让您停下来思考每个术语。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "不过，这里发生的变化是成本相对于 L-1 层中的激活之一的导数。",
  "model": "google_nmt",
  "from_community_srt": "也是一样的 这里观众可以暂停推导一下每一项的含义 唯一改变的是 代价对(L-1)层激活值的导数 此时  激活值可以通过不同的途径影响代价函数 就是说 神经元一边通过a_0^(L)来影响代价函数",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "在这种情况下，不同之处在于神经元 通过多个不同的路径影响成本函数。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "也就是说，它一方面影响在成本函数中起作用 的AL0，但它也对也在成本函数中起作用 的AL1产生影响，并且必须将它们相加。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "嗯，差不多就是这样了。",
  "model": "google_nmt",
  "from_community_srt": "另一边通过a_1^(L)来影响代价函数 得把这些都加起来 然后……就搞定了 只要计算出倒数第二层代价函数对激活值的敏感度 接下来只要重复上述过程 计算喂给倒数第二层的权重和偏置 就好了",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "一旦您知道成本函数对倒数第二层中 的激活有多敏感，您就可以对输入该 层的所有权重和偏差重复该过程。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "所以拍拍自己的背吧！",
  "model": "google_nmt",
  "from_community_srt": "现在长吁一口气吧！",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "如果所有这些都有意义，那么您现在已经深入了解 了反向传播的核心，即神经网络学习背后的主力。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "这些链式法则表达式为您提供了确定梯度中每个分量 的导数，有助于通过重复下坡来最小化网络成本。",
  "model": "google_nmt",
  "from_community_srt": "如果这里明白了 那你就看明白了神经网络的主力——反向传播 那你就看明白了神经网络的主力——反向传播 链式法则表达式给出了决定梯度每个分量的偏导 使得我们能不断下探  最小化神经网络的代价",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "如果你坐下来思考所有这些，你会发现这有很多层复杂的内容需要你 的大脑来思考，所以不要担心你的大脑是否需要时间来消化这一切。",
  "model": "google_nmt",
  "from_community_srt": "乌啦啦  光是静下来想一想 这些复杂的层层叠叠就很烧脑 消化这些知识会花一些时间  别气馁了",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]