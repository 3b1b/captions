1
00:00:00,000 --> 00:00:08,420
এখানে কঠিন অনুমান হল যে আপনি পার্ট 3

2
00:00:08,420 --> 00:00:11,160
দেখেছেন, ব্যাকপ্রপাগেশন অ্যালগরিদমের একটি স্বজ্ঞাত ওয়াকথ্রু প্রদান করেছেন।

3
00:00:11,160 --> 00:00:14,920
এখানে আমরা একটু বেশি আনুষ্ঠানিকতা পাই এবং প্রাসঙ্গিক ক্যালকুলাসে ডুব দিই।

4
00:00:14,920 --> 00:00:18,560
এটির জন্য কমপক্ষে কিছুটা বিভ্রান্তিকর হওয়া স্বাভাবিক, তাই নিয়মিত বিরতি

5
00:00:18,560 --> 00:00:22,000
এবং চিন্তা করার মন্ত্রটি অবশ্যই অন্য কোথাও যতটা প্রযোজ্য।

6
00:00:22,000 --> 00:00:26,620
আমাদের মূল লক্ষ্য হল মেশিন লার্নিংয়ে লোকেরা কীভাবে নেটওয়ার্কের প্রেক্ষাপটে ক্যালকুলাস থেকে

7
00:00:26,620 --> 00:00:31,900
চেইন নিয়ম সম্পর্কে চিন্তা করে তা দেখানো, যা বেশিরভাগ প্রাথমিক ক্যালকুলাস

8
00:00:31,900 --> 00:00:34,580
কোর্সগুলি কীভাবে বিষয়ের সাথে যোগাযোগ করে তার থেকে আলাদা অনুভূতি রয়েছে।

9
00:00:34,580 --> 00:00:38,300
আপনি যারা প্রাসঙ্গিক ক্যালকুলাসে অস্বস্তিকর, আমার কাছে

10
00:00:38,300 --> 00:00:39,300
এই বিষয়ে একটি সম্পূর্ণ সিরিজ আছে।

11
00:00:39,300 --> 00:00:44,840
চলুন শুরু করা যাক একটি অতি সাধারণ নেটওয়ার্ক

12
00:00:44,840 --> 00:00:46,780
দিয়ে, যেখানে প্রতিটি স্তরে একটি করে নিউরন থাকে।

13
00:00:46,780 --> 00:00:51,880
এই নেটওয়ার্কটি তিনটি ওজন এবং তিনটি পক্ষপাত দ্বারা নির্ধারিত হয় এবং আমাদের

14
00:00:51,880 --> 00:00:55,640
লক্ষ্য হল এই ভেরিয়েবলগুলির জন্য খরচ ফাংশন কতটা সংবেদনশীল তা বোঝা।

15
00:00:55,640 --> 00:00:59,780
এইভাবে আমরা জানি যে এই শর্তগুলির কোন

16
00:00:59,780 --> 00:01:01,100
সমন্বয়গুলি খরচ ফাংশনে সবচেয়ে কার্যকরী হ্রাস ঘটাবে।

17
00:01:01,100 --> 00:01:05,360
আমরা শুধু শেষ দুটি নিউরনের মধ্যে সংযোগের উপর ফোকাস করব।

18
00:01:05,360 --> 00:01:10,400
আসুন একটি সুপারস্ক্রিপ্ট L দিয়ে সেই শেষ নিউরনের অ্যাক্টিভেশনটিকে

19
00:01:10,400 --> 00:01:11,800
লেবেল করি, এটি কোন স্তরে রয়েছে তা নির্দেশ করে।

20
00:01:11,800 --> 00:01:16,560
তাই আগের নিউরনের সক্রিয়তা হল AL-1।

21
00:01:16,560 --> 00:01:20,120
এগুলি এক্সপোনেন্ট নয়, এগুলি আমরা যা বলছি তা সূচীকরণের একটি উপায়,

22
00:01:20,120 --> 00:01:23,120
যেহেতু আমি পরবর্তীতে বিভিন্ন সূচকের জন্য সাবস্ক্রিপ্টগুলি সংরক্ষণ করতে চাই৷

23
00:01:23,600 --> 00:01:28,880
ধরা যাক যে প্রদত্ত প্রশিক্ষণ উদাহরণের জন্য আমরা এই শেষ অ্যাক্টিভেশনটি হতে

24
00:01:28,880 --> 00:01:33,020
চাই তা হল y, উদাহরণস্বরূপ, y 0 বা 1 হতে পারে।

25
00:01:33,020 --> 00:01:39,040
সুতরাং একটি একক প্রশিক্ষণ উদাহরণের জন্য এই নেটওয়ার্কের খরচ হল AL-y2।

26
00:01:39,040 --> 00:01:46,120
আমরা সেই একটি প্রশিক্ষণ উদাহরণের খরচকে c0 হিসাবে চিহ্নিত করব।

27
00:01:46,120 --> 00:01:51,920
একটি অনুস্মারক হিসাবে, এই শেষ অ্যাক্টিভেশনটি একটি ওজন দ্বারা নির্ধারিত হয়, যাকে আমি

28
00:01:51,920 --> 00:01:57,600
wL বলতে যাচ্ছি, পূর্ববর্তী নিউরনের সক্রিয়করণ এবং কিছু পক্ষপাত, যাকে আমি bL বলব।

29
00:01:57,600 --> 00:02:01,560
তারপরে আপনি এটিকে কিছু বিশেষ ননলাইনার ফাংশনের মাধ্যমে পাম্প করেন যেমন সিগমায়েড বা ReLU।

30
00:02:01,560 --> 00:02:05,400
এটি আসলে আমাদের জন্য জিনিসগুলিকে আরও সহজ করে তুলবে যদি আমরা প্রাসঙ্গিক অ্যাক্টিভেশনগুলির মতো

31
00:02:05,400 --> 00:02:10,600
একই সুপারস্ক্রিপ্ট সহ z এর মতো এই ওজনযুক্ত সমষ্টিকে একটি বিশেষ নাম দিই।

32
00:02:10,600 --> 00:02:15,320
এটি অনেকগুলি পদ, এবং একটি উপায় যা আপনি ধারণা করতে পারেন তা হল ওজন, পূর্ববর্তী

33
00:02:15,320 --> 00:02:21,800
ক্রিয়া এবং পক্ষপাত সব একসাথে z গণনা করার জন্য ব্যবহৃত হয়, যার ফলে আমরা

34
00:02:21,800 --> 00:02:27,360
একটি গণনা করতে দেয়, যা অবশেষে, একটি ধ্রুবক y সহ, দেয় আমরা খরচ গণনা.

35
00:02:27,360 --> 00:02:33,440
এবং অবশ্যই, AL-1 তার নিজস্ব ওজন এবং পক্ষপাত এবং এই জাতীয়

36
00:02:33,440 --> 00:02:35,920
দ্বারা প্রভাবিত, কিন্তু আমরা এখনই এটিতে ফোকাস করতে যাচ্ছি না।

37
00:02:35,920 --> 00:02:38,120
এই সব শুধু সংখ্যা, তাই না?

38
00:02:38,120 --> 00:02:41,960
এবং প্রতিটির নিজস্ব ছোট সংখ্যা রেখা আছে বলে ভাবতে ভালো লাগতে পারে।

39
00:02:41,960 --> 00:02:47,480
আমাদের প্রথম লক্ষ্য হল আমাদের ওজন wL এর ছোট

40
00:02:47,480 --> 00:02:49,820
পরিবর্তনের জন্য খরচ ফাংশন কতটা সংবেদনশীল তা বোঝা।

41
00:02:49,820 --> 00:02:55,740
অথবা ভিন্নভাবে বাক্যাংশ, wL সাপেক্ষে c এর ডেরিভেটিভ কি?

42
00:02:55,740 --> 00:03:01,220
যখন আপনি এই del w শব্দটি দেখেন, তখন এটিকে w এর সাথে কিছু ক্ষুদ্র ধাক্কা, 0 দ্বারা পরিবর্তনের মতো অর্থ হিসেবে ভাবুন।

43
00:03:01,220 --> 00:03:08,820
01, এবং এই del c টার্মটিকে অর্থ হিসাবে ভাবুন যার ফলে খরচের দিকে ধাবিত হয়।

44
00:03:08,820 --> 00:03:10,900
আমরা কি চাই তাদের অনুপাত.

45
00:03:10,900 --> 00:03:17,740
ধারণাগতভাবে, wL-এর এই ক্ষুদ্র নজটি zL-এ কিছু নাজ সৃষ্টি করে, যা

46
00:03:17,740 --> 00:03:23,220
ফলস্বরূপ AL-কে কিছু নাজ করে, যা সরাসরি খরচকে প্রভাবিত করে।

47
00:03:23,220 --> 00:03:28,020
তাই আমরা প্রথমে zL থেকে এই ক্ষুদ্র পরিবর্তন w এর

48
00:03:28,020 --> 00:03:33,340
অনুপাত দেখে জিনিসগুলিকে ভেঙে ফেলি, অর্থাৎ wL-এর ক্ষেত্রে zL-এর ডেরিভেটিভ।

49
00:03:33,340 --> 00:03:38,820
একইভাবে, আপনি তারপরে AL-এ পরিবর্তনের অনুপাত এবং zL-তে ক্ষুদ্র

50
00:03:38,820 --> 00:03:43,900
পরিবর্তনের অনুপাত বিবেচনা করুন, সেইসাথে চূড়ান্ত নাজ থেকে

51
00:03:43,900 --> 00:03:45,900
c এবং এই মধ্যবর্তী নাজের AL-এর মধ্যে অনুপাত।

52
00:03:45,900 --> 00:03:51,880
এটি এখানে চেইন নিয়ম, যেখানে এই তিনটি অনুপাতকে গুণ

53
00:03:51,880 --> 00:03:57,340
করলে wL-তে ছোট পরিবর্তনের প্রতি c-এর সংবেদনশীলতা পাওয়া যায়।

54
00:03:57,340 --> 00:04:01,620
তাই এখনই স্ক্রিনে, অনেকগুলি প্রতীক রয়েছে এবং সেগুলি কী তা স্পষ্ট তা নিশ্চিত

55
00:04:01,620 --> 00:04:07,460
করতে কিছুক্ষণ সময় নিন, কারণ এখন আমরা প্রাসঙ্গিক ডেরিভেটিভগুলি গণনা করতে যাচ্ছি।

56
00:04:07,460 --> 00:04:14,220
AL সাপেক্ষে c এর ডেরিভেটিভ 2AL-y হিসাবে কাজ করে।

57
00:04:14,220 --> 00:04:19,300
এর অর্থ হল নেটওয়ার্কের আউটপুট এবং আমরা যে জিনিসটি হতে চাই তার

58
00:04:19,300 --> 00:04:24,480
মধ্যে পার্থক্যের সাথে এর আকার সমানুপাতিক, তাই যদি সেই আউটপুটটি খুব আলাদা

59
00:04:24,480 --> 00:04:28,380
হয়, এমনকি সামান্য পরিবর্তনও চূড়ান্ত খরচ ফাংশনের উপর বড় প্রভাব ফেলে।

60
00:04:28,380 --> 00:04:33,860
zL-এর সাপেক্ষে AL-এর ডেরিভেটিভ হল আমাদের সিগমায়েড ফাংশনের ডেরিভেটিভ,

61
00:04:33,860 --> 00:04:37,420
অথবা আপনি যে কোনও ননলাইন্যারিটি ব্যবহার করতে চান।

62
00:04:37,420 --> 00:04:46,180
wL এর সাপেক্ষে zL এর ডেরিভেটিভ AL-1 হয়।

63
00:04:46,180 --> 00:04:49,460
আমি আপনার সম্বন্ধে জানি না, কিন্তু আমি মনে করি একটা মুহূর্ত না নিয়েই ফর্মুলায়

64
00:04:49,460 --> 00:04:54,180
মাথা নিচু করে বসে থাকা সহজ এবং মনে করিয়ে দেওয়া যে সেগুলির অর্থ কী।

65
00:04:54,180 --> 00:04:58,860
এই শেষ ডেরিভেটিভের ক্ষেত্রে, ওজনের ছোট ধাক্কা শেষ স্তরটিকে প্রভাবিত

66
00:04:58,860 --> 00:05:03,220
করে তা নির্ভর করে আগের নিউরন কতটা শক্তিশালী তার উপর।

67
00:05:03,220 --> 00:05:09,320
মনে রাখবেন, এখানেই নিউরন-দ্যাট-ফায়ার-টুগেদার-ওয়্যার-টুগেদার আইডিয়া আসে।

68
00:05:09,320 --> 00:05:14,840
এবং এই সবই একটি নির্দিষ্ট একক প্রশিক্ষণ

69
00:05:14,840 --> 00:05:16,580
উদাহরণের জন্য শুধুমাত্র খরচ wL সাপেক্ষে ডেরিভেটিভ।

70
00:05:16,580 --> 00:05:20,940
যেহেতু সম্পূর্ণ খরচ ফাংশনে অনেকগুলি বিভিন্ন প্রশিক্ষণ উদাহরণ জুড়ে সেই

71
00:05:20,940 --> 00:05:27,300
সমস্ত খরচগুলিকে একত্রে গড় করা জড়িত, তাই এর ডেরিভেটিভের

72
00:05:27,300 --> 00:05:28,540
জন্য সমস্ত প্রশিক্ষণ উদাহরণের উপর এই অভিব্যক্তির গড় প্রয়োজন।

73
00:05:28,540 --> 00:05:33,860
অবশ্যই, এটি গ্রেডিয়েন্ট ভেক্টরের শুধুমাত্র একটি উপাদান, যা সেই সমস্ত ওজন এবং

74
00:05:33,860 --> 00:05:40,780
পক্ষপাতের সাথে সাপেক্ষে খরচ ফাংশনের আংশিক ডেরিভেটিভ থেকে তৈরি করা হয়েছে।

75
00:05:40,780 --> 00:05:44,340
কিন্তু যদিও এটি আমাদের প্রয়োজনীয় অনেক আংশিক ডেরিভেটিভের

76
00:05:44,340 --> 00:05:46,460
মধ্যে একটি মাত্র, এটি কাজের 50% এরও বেশি।

77
00:05:46,460 --> 00:05:50,300
পক্ষপাতের সংবেদনশীলতা, উদাহরণস্বরূপ, প্রায় অভিন্ন।

78
00:05:50,300 --> 00:05:58,980
আমাদের শুধু এই del z del w শব্দটিকে একটি del z del b এর জন্য পরিবর্তন করতে হবে।

79
00:05:58,980 --> 00:06:04,700
এবং আপনি যদি প্রাসঙ্গিক সূত্রটি দেখেন তবে সেই ডেরিভেটিভটি 1 হবে।

80
00:06:04,700 --> 00:06:11,700
এছাড়াও, এবং এখানেই পিছনের দিকে প্রচার করার ধারণাটি আসে, আপনি দেখতে

81
00:06:11,700 --> 00:06:16,180
পারেন যে এই খরচ ফাংশনটি আগের স্তরের সক্রিয়করণের জন্য কতটা সংবেদনশীল।

82
00:06:16,180 --> 00:06:21,380
যথা, চেইন রুল এক্সপ্রেশনে এই প্রাথমিক ডেরিভেটিভ, পূর্ববর্তী অ্যাক্টিভেশনের

83
00:06:21,380 --> 00:06:25,420
জন্য z-এর সংবেদনশীলতা, ওজন wL হিসাবে বেরিয়ে আসে।

84
00:06:25,420 --> 00:06:30,100
এবং আবার, যদিও আমরা সেই আগের লেয়ার অ্যাক্টিভেশনকে সরাসরি প্রভাবিত করতে

85
00:06:30,100 --> 00:06:35,280
সক্ষম হব না, এটি ট্র্যাক রাখা সহায়ক, কারণ এখন আমরা

86
00:06:35,280 --> 00:06:40,780
এই একই চেইন নিয়মের ধারণাটিকে পিছনের দিকে পুনরাবৃত্তি করতে পারি তা

87
00:06:40,780 --> 00:06:43,680
দেখতে খরচ ফাংশনটি কতটা সংবেদনশীল। পূর্ববর্তী ওজন এবং পূর্বের পক্ষপাত।

88
00:06:43,680 --> 00:06:47,940
এবং আপনি ভাবতে পারেন এটি একটি অতি সাধারণ উদাহরণ, যেহেতু সমস্ত স্তরে একটি

89
00:06:47,940 --> 00:06:51,320
নিউরন রয়েছে এবং একটি বাস্তব নেটওয়ার্কের জন্য জিনিসগুলি দ্রুতগতিতে আরও জটিল হতে চলেছে।

90
00:06:51,320 --> 00:06:56,560
কিন্তু সত্যি কথা বলতে কি, আমরা যখন স্তরগুলিকে একাধিক নিউরন দিই তখন

91
00:06:56,560 --> 00:06:59,320
এতটা পরিবর্তন হয় না, সত্যিই এটির ট্র্যাক রাখা আরও কয়েকটি সূচক।

92
00:06:59,320 --> 00:07:03,580
একটি প্রদত্ত স্তরটিকে কেবল AL হিসাবে সক্রিয় করার পরিবর্তে, এটিতে একটি

93
00:07:03,580 --> 00:07:07,920
সাবস্ক্রিপ্টও থাকবে যা নির্দেশ করে যে স্তরটির কোন নিউরন এটি।

94
00:07:07,920 --> 00:07:15,280
লেয়ার L-1 সূচী করতে k অক্ষরটি ব্যবহার করি এবং লেয়ার L-এর সূচী করতে j ব্যবহার করি।

95
00:07:15,280 --> 00:07:20,720
খরচের জন্য, আমরা আবার দেখি কাঙ্খিত আউটপুট কি, কিন্তু এবার আমরা এই

96
00:07:20,720 --> 00:07:26,120
শেষ লেয়ার অ্যাক্টিভেশন এবং কাঙ্ক্ষিত আউটপুটের মধ্যে পার্থক্যের স্কোয়ার যোগ করি।

97
00:07:26,120 --> 00:07:33,280
অর্থাৎ, আপনি ALj বিয়োগ yj বর্গক্ষেত্রের উপর একটি যোগফল নিন।

98
00:07:33,280 --> 00:07:36,500
যেহেতু অনেক বেশি ওজন আছে, প্রত্যেকের কাছে এটি কোথায় রয়েছে তা

99
00:07:36,500 --> 00:07:41,380
ট্র্যাক করার জন্য আরও কয়েকটি সূচক থাকতে হবে, তাই আসুন এই

100
00:07:41,380 --> 00:07:45,740
কেটিএইচ নিউরনকে জেটিএইচ নিউরনের সাথে সংযোগকারী প্রান্তের ওজন বলি, ডব্লিউএলজেকে।

101
00:07:45,740 --> 00:07:49,820
এই সূচকগুলি প্রথমে কিছুটা পিছনের দিকে মনে হতে পারে, তবে এটি আপনি কীভাবে

102
00:07:49,820 --> 00:07:53,800
ওজন ম্যাট্রিক্সকে সূচক করতে চান তার সাথে আমি অংশ 1 ভিডিওতে কথা বলেছি।

103
00:07:53,800 --> 00:07:57,660
ঠিক আগের মতোই, z এর মতো প্রাসঙ্গিক ওজনযুক্ত যোগফলকে একটি

104
00:07:57,660 --> 00:08:03,540
নাম দেওয়া এখনও ভাল, যাতে শেষ স্তরের সক্রিয়করণটি কেবল আপনার

105
00:08:03,540 --> 00:08:04,980
বিশেষ ফাংশন, যেমন সিগমায়েড, z এ প্রয়োগ করা হয়।

106
00:08:04,980 --> 00:08:09,100
আপনি দেখতে পাচ্ছেন আমি কী বলতে চাইছি, যেখানে এই সবগুলিই মূলত একই সমীকরণ

107
00:08:09,100 --> 00:08:15,420
যা আমরা আগে এক-নিউরন-প্রতি-লেয়ার ক্ষেত্রে ছিল, এটি কেবল এটি একটু বেশি জটিল দেখায়।

108
00:08:15,420 --> 00:08:20,620
এবং প্রকৃতপক্ষে, একটি নির্দিষ্ট ওজনের জন্য খরচ কতটা সংবেদনশীল তা

109
00:08:20,620 --> 00:08:23,540
বর্ণনা করে চেইন রুল ডেরিভেটিভ এক্সপ্রেশনটি মূলত একই রকম দেখায়।

110
00:08:23,540 --> 00:08:29,420
আপনি যদি চান তবে এই পদগুলির প্রতিটি সম্পর্কে বিরাম দিতে এবং চিন্তা করার জন্য আমি এটি আপনার উপর ছেড়ে দেব।

111
00:08:29,420 --> 00:08:34,900
এখানে কি পরিবর্তন হয়, যদিও, লেয়ার

112
00:08:34,900 --> 00:08:37,820
L-1-এর একটি অ্যাক্টিভেশনের সাপেক্ষে খরচের ডেরিভেটিভ।

113
00:08:37,820 --> 00:08:42,000
এই ক্ষেত্রে, পার্থক্য হল যে নিউরন একাধিক

114
00:08:42,000 --> 00:08:43,540
ভিন্ন পথের মাধ্যমে খরচ ফাংশনকে প্রভাবিত করে।

115
00:08:43,540 --> 00:08:51,200
অর্থাৎ, একদিকে, এটি AL0 কে প্রভাবিত করে, যা খরচ ফাংশনে একটি

116
00:08:51,200 --> 00:08:56,460
ভূমিকা পালন করে, কিন্তু এটি AL1-এর উপরও প্রভাব ফেলে, যা খরচ

117
00:08:56,460 --> 00:09:00,340
ফাংশনেও একটি ভূমিকা পালন করে এবং আপনাকে সেগুলি যোগ করতে হবে।

118
00:09:00,340 --> 00:09:03,680
এবং যে, ভাল, যে বেশ এটা.

119
00:09:03,680 --> 00:09:08,240
এই দ্বিতীয়-থেকে-শেষ স্তরে সক্রিয়করণের জন্য খরচ ফাংশন কতটা সংবেদনশীল

120
00:09:08,240 --> 00:09:12,520
তা আপনি একবার জানলে, আপনি সেই স্তরে থাকা সমস্ত

121
00:09:12,520 --> 00:09:13,920
ওজন এবং পক্ষপাতের জন্য প্রক্রিয়াটি পুনরাবৃত্তি করতে পারেন।

122
00:09:13,920 --> 00:09:15,420
তাই পিঠে চাপ দিন!

123
00:09:15,420 --> 00:09:20,480
যদি এই সমস্ত কিছু বোঝা যায়, আপনি এখন ব্যাকপ্রোপাগেশনের হৃদয়ের

124
00:09:20,480 --> 00:09:23,700
গভীরে দেখেছেন, কীভাবে নিউরাল নেটওয়ার্কগুলি শেখে তার পিছনে কাজের ঘোড়া।

125
00:09:23,700 --> 00:09:27,960
এই চেইন রুল এক্সপ্রেশনগুলি আপনাকে ডেরিভেটিভস দেয় যা গ্রেডিয়েন্টের প্রতিটি উপাদান নির্ধারণ

126
00:09:27,960 --> 00:09:35,020
করে যা বারবার নিচের দিকে ধাপে ধাপে নেটওয়ার্কের খরচ কমাতে সাহায্য করে।

127
00:09:35,020 --> 00:09:38,960
আপনি যদি পিছনে বসে এই সমস্ত কিছু নিয়ে চিন্তা করেন, এটি আপনার মনকে ঘিরে রাখার জন্য

128
00:09:38,960 --> 00:09:42,840
জটিলতার অনেক স্তর, তাই চিন্তা করবেন না যদি আপনার মনের সমস্ত কিছু হজম হতে সময় লাগে।

