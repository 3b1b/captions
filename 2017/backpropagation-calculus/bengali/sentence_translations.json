[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm. ",
  "translatedText": "এখানে কঠিন অনুমান হল যে আপনি পার্ট 3 দেখেছেন, ব্যাকপ্রপাগেশন অ্যালগরিদমের একটি স্বজ্ঞাত ওয়াকথ্রু প্রদান করেছেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus. ",
  "translatedText": "এখানে আমরা একটু বেশি আনুষ্ঠানিকতা পাই এবং প্রাসঙ্গিক ক্যালকুলাসে ডুব দিই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else. ",
  "translatedText": "এটির জন্য কমপক্ষে কিছুটা বিভ্রান্তিকর হওয়া স্বাভাবিক, তাই নিয়মিত বিরতি এবং চিন্তা করার মন্ত্রটি অবশ্যই অন্য কোথাও যতটা প্রযোজ্য।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject. ",
  "translatedText": "আমাদের মূল লক্ষ্য হল মেশিন লার্নিংয়ে লোকেরা কীভাবে নেটওয়ার্কের প্রেক্ষাপটে ক্যালকুলাস থেকে চেইন নিয়ম সম্পর্কে চিন্তা করে তা দেখানো, যা বেশিরভাগ প্রাথমিক ক্যালকুলাস কোর্সগুলি কীভাবে বিষয়ের সাথে যোগাযোগ করে তার থেকে আলাদা অনুভূতি রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic. ",
  "translatedText": "আপনি যারা প্রাসঙ্গিক ক্যালকুলাসে অস্বস্তিকর, আমার কাছে এই বিষয়ে একটি সম্পূর্ণ সিরিজ আছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it. ",
  "translatedText": "চলুন শুরু করা যাক একটি অতি সাধারণ নেটওয়ার্ক দিয়ে, যেখানে প্রতিটি স্তরে একটি করে নিউরন থাকে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables. ",
  "translatedText": "এই নেটওয়ার্কটি তিনটি ওজন এবং তিনটি পক্ষপাত দ্বারা নির্ধারিত হয় এবং আমাদের লক্ষ্য হল এই ভেরিয়েবলগুলির জন্য খরচ ফাংশন কতটা সংবেদনশীল তা বোঝা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way we know which adjustments to those terms will cause the most efficient decrease to the cost function. ",
  "translatedText": "এইভাবে আমরা জানি যে এই শর্তগুলির কোন সমন্বয়গুলি খরচ ফাংশনে সবচেয়ে কার্যকরী হ্রাস ঘটাবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "We'll just focus on the connection between the last two neurons. ",
  "translatedText": "আমরা শুধু শেষ দুটি নিউরনের মধ্যে সংযোগের উপর ফোকাস করব।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in. ",
  "translatedText": "আসুন একটি সুপারস্ক্রিপ্ট L দিয়ে সেই শেষ নিউরনের অ্যাক্টিভেশনটিকে লেবেল করি, এটি কোন স্তরে রয়েছে তা নির্দেশ করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.98,
  "end": 71.36
 },
 {
  "input": "So the activation of the previous neuron is AL-1. ",
  "translatedText": "তাই আগের নিউরনের সক্রিয়তা হল AL-1।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.68,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1. ",
  "translatedText": "এগুলি এক্সপোনেন্ট নয়, এগুলি আমরা যা বলছি তা সূচীকরণের একটি উপায়, যেহেতু আমি পরবর্তীতে বিভিন্ন সূচকের জন্য সাবস্ক্রিপ্টগুলি সংরক্ষণ করতে চাই৷ ধরা যাক যে প্রদত্ত প্রশিক্ষণ উদাহরণের জন্য আমরা এই শেষ অ্যাক্টিভেশনটি হতে চাই তা হল y, উদাহরণস্বরূপ, y 0 বা 1 হতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is AL-y2. ",
  "translatedText": "সুতরাং একটি একক প্রশিক্ষণ উদাহরণের জন্য এই নেটওয়ার্কের খরচ হল AL-y2।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0. ",
  "translatedText": "আমরা সেই একটি প্রশিক্ষণ উদাহরণের খরচকে c0 হিসাবে চিহ্নিত করব।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call wL, times the previous neuron's activation plus some bias, which I'll call bL. ",
  "translatedText": "একটি অনুস্মারক হিসাবে, এই শেষ অ্যাক্টিভেশনটি একটি ওজন দ্বারা নির্ধারিত হয়, যাকে আমি wL বলতে যাচ্ছি, পূর্ববর্তী নিউরনের সক্রিয়করণ এবং কিছু পক্ষপাত, যাকে আমি bL বলব।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "Then you pump that through some special nonlinear function like the sigmoid or ReLU. ",
  "translatedText": "তারপরে আপনি এটিকে কিছু বিশেষ ননলাইনার ফাংশনের মাধ্যমে পাম্প করেন যেমন সিগমায়েড বা ReLU।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations. ",
  "translatedText": "এটি আসলে আমাদের জন্য জিনিসগুলিকে আরও সহজ করে তুলবে যদি আমরা প্রাসঙ্গিক অ্যাক্টিভেশনগুলির মতো একই সুপারস্ক্রিপ্ট সহ z এর মতো এই ওজনযুক্ত সমষ্টিকে একটি বিশেষ নাম দিই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action, and bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost. ",
  "translatedText": "এটি অনেকগুলি পদ, এবং একটি উপায় যা আপনি ধারণা করতে পারেন তা হল ওজন, পূর্ববর্তী ক্রিয়া এবং পক্ষপাত সব একসাথে z গণনা করার জন্য ব্যবহৃত হয়, যার ফলে আমরা একটি গণনা করতে দেয়, যা অবশেষে, একটি ধ্রুবক y সহ, দেয় আমরা খরচ গণনা. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course, AL-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now. ",
  "translatedText": "এবং অবশ্যই, AL-1 তার নিজস্ব ওজন এবং পক্ষপাত এবং এই জাতীয় দ্বারা প্রভাবিত, কিন্তু আমরা এখনই এটিতে ফোকাস করতে যাচ্ছি না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right? ",
  "translatedText": "এই সব শুধু সংখ্যা, তাই না? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line. ",
  "translatedText": "এবং প্রতিটির নিজস্ব ছোট সংখ্যা রেখা আছে বলে ভাবতে ভালো লাগতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight wL. ",
  "translatedText": "আমাদের প্রথম লক্ষ্য হল আমাদের ওজন wL এর ছোট পরিবর্তনের জন্য খরচ ফাংশন কতটা সংবেদনশীল তা বোঝা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to wL? ",
  "translatedText": "অথবা ভিন্নভাবে বাক্যাংশ, wL সাপেক্ষে c এর ডেরিভেটিভ কি? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del w term, think of it as meaning some tiny nudge to w, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is. ",
  "translatedText": "যখন আপনি এই del w শব্দটি দেখেন, তখন এটিকে w এর সাথে কিছু ক্ষুদ্র ধাক্কা, 0 দ্বারা পরিবর্তনের মতো অর্থ হিসেবে ভাবুন।01, এবং এই del c টার্মটিকে অর্থ হিসাবে ভাবুন যার ফলে খরচের দিকে ধাবিত হয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio. ",
  "translatedText": "আমরা কি চাই তাদের অনুপাত. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to wL causes some nudge to zL, which in turn causes some nudge to AL, which directly influences the cost. ",
  "translatedText": "ধারণাগতভাবে, wL-এর এই ক্ষুদ্র নজটি zL-এ কিছু নাজ সৃষ্টি করে, যা ফলস্বরূপ AL-কে কিছু নাজ করে, যা সরাসরি খরচকে প্রভাবিত করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to zL to this tiny change w, that is, the derivative of zL with respect to wL. ",
  "translatedText": "তাই আমরা প্রথমে zL থেকে এই ক্ষুদ্র পরিবর্তন w এর অনুপাত দেখে জিনিসগুলিকে ভেঙে ফেলি, অর্থাৎ wL-এর ক্ষেত্রে zL-এর ডেরিভেটিভ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in zL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL. ",
  "translatedText": "একইভাবে, আপনি তারপরে AL-এ পরিবর্তনের অনুপাত এবং zL-তে ক্ষুদ্র পরিবর্তনের অনুপাত বিবেচনা করুন, সেইসাথে চূড়ান্ত নাজ থেকে c এবং এই মধ্যবর্তী নাজের AL-এর মধ্যে অনুপাত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying these three ratios gives us the sensitivity of c to small changes in wL. ",
  "translatedText": "এটি এখানে চেইন নিয়ম, যেখানে এই তিনটি অনুপাতকে গুণ করলে wL-তে ছোট পরিবর্তনের প্রতি c-এর সংবেদনশীলতা পাওয়া যায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives. ",
  "translatedText": "তাই এখনই স্ক্রিনে, অনেকগুলি প্রতীক রয়েছে এবং সেগুলি কী তা স্পষ্ট তা নিশ্চিত করতে কিছুক্ষণ সময় নিন, কারণ এখন আমরা প্রাসঙ্গিক ডেরিভেটিভগুলি গণনা করতে যাচ্ছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y. ",
  "translatedText": "AL সাপেক্ষে c এর ডেরিভেটিভ 2AL-y হিসাবে কাজ করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "This means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function. ",
  "translatedText": "এর অর্থ হল নেটওয়ার্কের আউটপুট এবং আমরা যে জিনিসটি হতে চাই তার মধ্যে পার্থক্যের সাথে এর আকার সমানুপাতিক, তাই যদি সেই আউটপুটটি খুব আলাদা হয়, এমনকি সামান্য পরিবর্তনও চূড়ান্ত খরচ ফাংশনের উপর বড় প্রভাব ফেলে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to zL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use. ",
  "translatedText": "zL-এর সাপেক্ষে AL-এর ডেরিভেটিভ হল আমাদের সিগমায়েড ফাংশনের ডেরিভেটিভ, অথবা আপনি যে কোনও ননলাইন্যারিটি ব্যবহার করতে চান।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "The derivative of zL with respect to wL comes out to be AL-1. ",
  "translatedText": "wL এর সাপেক্ষে zL এর ডেরিভেটিভ AL-1 হয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself what they all mean. ",
  "translatedText": "আমি আপনার সম্বন্ধে জানি না, কিন্তু আমি মনে করি একটা মুহূর্ত না নিয়েই ফর্মুলায় মাথা নিচু করে বসে থাকা সহজ এবং মনে করিয়ে দেওয়া যে সেগুলির অর্থ কী।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is. ",
  "translatedText": "এই শেষ ডেরিভেটিভের ক্ষেত্রে, ওজনের ছোট ধাক্কা শেষ স্তরটিকে প্রভাবিত করে তা নির্ভর করে আগের নিউরন কতটা শক্তিশালী তার উপর।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in. ",
  "translatedText": "মনে রাখবেন, এখানেই নিউরন-দ্যাট-ফায়ার-টুগেদার-ওয়্যার-টুগেদার আইডিয়া আসে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to wL only of the cost for a specific single training example. ",
  "translatedText": "এবং এই সবই একটি নির্দিষ্ট একক প্রশিক্ষণ উদাহরণের জন্য শুধুমাত্র খরচ wL সাপেক্ষে ডেরিভেটিভ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples. ",
  "translatedText": "যেহেতু সম্পূর্ণ খরচ ফাংশনে অনেকগুলি বিভিন্ন প্রশিক্ষণ উদাহরণ জুড়ে সেই সমস্ত খরচগুলিকে একত্রে গড় করা জড়িত, তাই এর ডেরিভেটিভের জন্য সমস্ত প্রশিক্ষণ উদাহরণের উপর এই অভিব্যক্তির গড় প্রয়োজন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "Of course, that's just one component of the gradient vector, which is built up from the partial derivatives of the cost function with respect to all those weights and biases. ",
  "translatedText": "অবশ্যই, এটি গ্রেডিয়েন্ট ভেক্টরের শুধুমাত্র একটি উপাদান, যা সেই সমস্ত ওজন এবং পক্ষপাতের সাথে সাপেক্ষে খরচ ফাংশনের আংশিক ডেরিভেটিভ থেকে তৈরি করা হয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work. ",
  "translatedText": "কিন্তু যদিও এটি আমাদের প্রয়োজনীয় অনেক আংশিক ডেরিভেটিভের মধ্যে একটি মাত্র, এটি কাজের 50% এরও বেশি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical. ",
  "translatedText": "পক্ষপাতের সংবেদনশীলতা, উদাহরণস্বরূপ, প্রায় অভিন্ন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b. ",
  "translatedText": "আমাদের শুধু এই del z del w শব্দটিকে একটি del z del b এর জন্য পরিবর্তন করতে হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1. ",
  "translatedText": "এবং আপনি যদি প্রাসঙ্গিক সূত্রটি দেখেন তবে সেই ডেরিভেটিভটি 1 হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer. ",
  "translatedText": "এছাড়াও, এবং এখানেই পিছনের দিকে প্রচার করার ধারণাটি আসে, আপনি দেখতে পারেন যে এই খরচ ফাংশনটি আগের স্তরের সক্রিয়করণের জন্য কতটা সংবেদনশীল।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight wL. ",
  "translatedText": "যথা, চেইন রুল এক্সপ্রেশনে এই প্রাথমিক ডেরিভেটিভ, পূর্ববর্তী অ্যাক্টিভেশনের জন্য z-এর সংবেদনশীলতা, ওজন wL হিসাবে বেরিয়ে আসে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases. ",
  "translatedText": "এবং আবার, যদিও আমরা সেই আগের লেয়ার অ্যাক্টিভেশনকে সরাসরি প্রভাবিত করতে সক্ষম হব না, এটি ট্র্যাক রাখা সহায়ক, কারণ এখন আমরা এই একই চেইন নিয়মের ধারণাটিকে পিছনের দিকে পুনরাবৃত্তি করতে পারি তা দেখতে খরচ ফাংশনটি কতটা সংবেদনশীল।পূর্ববর্তী ওজন এবং পূর্বের পক্ষপাত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network. ",
  "translatedText": "এবং আপনি ভাবতে পারেন এটি একটি অতি সাধারণ উদাহরণ, যেহেতু সমস্ত স্তরে একটি নিউরন রয়েছে এবং একটি বাস্তব নেটওয়ার্কের জন্য জিনিসগুলি দ্রুতগতিতে আরও জটিল হতে চলেছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of. ",
  "translatedText": "কিন্তু সত্যি কথা বলতে কি, আমরা যখন স্তরগুলিকে একাধিক নিউরন দিই তখন এতটা পরিবর্তন হয় না, সত্যিই এটির ট্র্যাক রাখা আরও কয়েকটি সূচক।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is. ",
  "translatedText": "একটি প্রদত্ত স্তরটিকে কেবল AL হিসাবে সক্রিয় করার পরিবর্তে, এটিতে একটি সাবস্ক্রিপ্টও থাকবে যা নির্দেশ করে যে স্তরটির কোন নিউরন এটি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L. ",
  "translatedText": "লেয়ার L-1 সূচী করতে k অক্ষরটি ব্যবহার করি এবং লেয়ার L-এর সূচী করতে j ব্যবহার করি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output. ",
  "translatedText": "খরচের জন্য, আমরা আবার দেখি কাঙ্খিত আউটপুট কি, কিন্তু এবার আমরা এই শেষ লেয়ার অ্যাক্টিভেশন এবং কাঙ্ক্ষিত আউটপুটের মধ্যে পার্থক্যের স্কোয়ার যোগ করি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus yj squared. ",
  "translatedText": "অর্থাৎ, আপনি ALj বিয়োগ yj বর্গক্ষেত্রের উপর একটি যোগফল নিন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk. ",
  "translatedText": "যেহেতু অনেক বেশি ওজন আছে, প্রত্যেকের কাছে এটি কোথায় রয়েছে তা ট্র্যাক করার জন্য আরও কয়েকটি সূচক থাকতে হবে, তাই আসুন এই কেটিএইচ নিউরনকে জেটিএইচ নিউরনের সাথে সংযোগকারী প্রান্তের ওজন বলি, ডব্লিউএলজেকে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video. ",
  "translatedText": "এই সূচকগুলি প্রথমে কিছুটা পিছনের দিকে মনে হতে পারে, তবে এটি আপনি কীভাবে ওজন ম্যাট্রিক্সকে সূচক করতে চান তার সাথে আমি অংশ 1 ভিডিওতে কথা বলেছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z. ",
  "translatedText": "ঠিক আগের মতোই, z এর মতো প্রাসঙ্গিক ওজনযুক্ত যোগফলকে একটি নাম দেওয়া এখনও ভাল, যাতে শেষ স্তরের সক্রিয়করণটি কেবল আপনার বিশেষ ফাংশন, যেমন সিগমায়েড, z এ প্রয়োগ করা হয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated. ",
  "translatedText": "আপনি দেখতে পাচ্ছেন আমি কী বলতে চাইছি, যেখানে এই সবগুলিই মূলত একই সমীকরণ যা আমরা আগে এক-নিউরন-প্রতি-লেয়ার ক্ষেত্রে ছিল, এটি কেবল এটি একটু বেশি জটিল দেখায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain rule derivative expression describing how sensitive the cost is to a specific weight looks essentially the same. ",
  "translatedText": "এবং প্রকৃতপক্ষে, একটি নির্দিষ্ট ওজনের জন্য খরচ কতটা সংবেদনশীল তা বর্ণনা করে চেইন রুল ডেরিভেটিভ এক্সপ্রেশনটি মূলত একই রকম দেখায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want. ",
  "translatedText": "আপনি যদি চান তবে এই পদগুলির প্রতিটি সম্পর্কে বিরাম দিতে এবং চিন্তা করার জন্য আমি এটি আপনার উপর ছেড়ে দেব।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1. ",
  "translatedText": "এখানে কি পরিবর্তন হয়, যদিও, লেয়ার L-1-এর একটি অ্যাক্টিভেশনের সাপেক্ষে খরচের ডেরিভেটিভ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths. ",
  "translatedText": "এই ক্ষেত্রে, পার্থক্য হল যে নিউরন একাধিক ভিন্ন পথের মাধ্যমে খরচ ফাংশনকে প্রভাবিত করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up. ",
  "translatedText": "অর্থাৎ, একদিকে, এটি AL0 কে প্রভাবিত করে, যা খরচ ফাংশনে একটি ভূমিকা পালন করে, কিন্তু এটি AL1-এর উপরও প্রভাব ফেলে, যা খরচ ফাংশনেও একটি ভূমিকা পালন করে এবং আপনাকে সেগুলি যোগ করতে হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it. ",
  "translatedText": "এবং যে, ভাল, যে বেশ এটা. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer. ",
  "translatedText": "এই দ্বিতীয়-থেকে-শেষ স্তরে সক্রিয়করণের জন্য খরচ ফাংশন কতটা সংবেদনশীল তা আপনি একবার জানলে, আপনি সেই স্তরে থাকা সমস্ত ওজন এবং পক্ষপাতের জন্য প্রক্রিয়াটি পুনরাবৃত্তি করতে পারেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back! ",
  "translatedText": "তাই পিঠে চাপ দিন! ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn. ",
  "translatedText": "যদি এই সমস্ত কিছু বোঝা যায়, আপনি এখন ব্যাকপ্রোপাগেশনের হৃদয়ের গভীরে দেখেছেন, কীভাবে নিউরাল নেটওয়ার্কগুলি শেখে তার পিছনে কাজের ঘোড়া।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill. ",
  "translatedText": "এই চেইন রুল এক্সপ্রেশনগুলি আপনাকে ডেরিভেটিভস দেয় যা গ্রেডিয়েন্টের প্রতিটি উপাদান নির্ধারণ করে যা বারবার নিচের দিকে ধাপে ধাপে নেটওয়ার্কের খরচ কমাতে সাহায্য করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all. ",
  "translatedText": "আপনি যদি পিছনে বসে এই সমস্ত কিছু নিয়ে চিন্তা করেন, এটি আপনার মনকে ঘিরে রাখার জন্য জটিলতার অনেক স্তর, তাই চিন্তা করবেন না যদি আপনার মনের সমস্ত কিছু হজম হতে সময় লাগে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]