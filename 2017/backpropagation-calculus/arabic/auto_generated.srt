1
00:00:04,020 --> 00:00:06,789
الافتراض الصعب هنا هو أنك شاهدت الجزء الثالث، 

2
00:00:06,789 --> 00:00:09,920
والذي يقدم شرحًا بديهيًا لخوارزمية الانتشار العكسي. 

3
00:00:11,040 --> 00:00:14,220
هنا نحصل على المزيد من الرسمية ونغوص في حسابات التفاضل والتكامل ذات الصلة. 

4
00:00:14,820 --> 00:00:18,181
من الطبيعي أن يكون هذا مربكًا بعض الشيء على الأقل، لذا فإن شعار التوقف 

5
00:00:18,181 --> 00:00:21,400
والتأمل بشكل منتظم ينطبق بالتأكيد هنا بقدر ما ينطبق في أي مكان آخر. 

6
00:00:21,940 --> 00:00:25,740
هدفنا الرئيسي هو إظهار كيف يفكر الأشخاص في التعلم الآلي بشكل شائع حول قاعدة 

7
00:00:25,740 --> 00:00:29,490
السلسلة من حساب التفاضل والتكامل في سياق الشبكات، والتي لها إحساس مختلف عن 

8
00:00:29,490 --> 00:00:33,640
الطريقة التي تتعامل بها معظم دورات حساب التفاضل والتكامل التمهيدية مع هذا الموضوع. 

9
00:00:34,340 --> 00:00:36,639
لأولئك منكم الذين لا يشعرون بالارتياح تجاه حسابات التفاضل 

10
00:00:36,639 --> 00:00:38,740
والتكامل ذات الصلة، لدي سلسلة كاملة حول هذا الموضوع. 

11
00:00:39,960 --> 00:00:46,020
لنبدأ بشبكة بسيطة للغاية، حيث تحتوي كل طبقة على خلية عصبية واحدة. 

12
00:00:46,320 --> 00:00:50,477
يتم تحديد هذه الشبكة بثلاثة أوزان وثلاثة انحيازات، 

13
00:00:50,477 --> 00:00:54,880
وهدفنا هو فهم مدى حساسية دالة التكلفة لهذه المتغيرات. 

14
00:00:55,419 --> 00:00:59,320
وبهذه الطريقة نعرف أي تعديلات على هذه الشروط ستتسبب 

15
00:00:59,320 --> 00:01:02,320
في التخفيض الأكثر كفاءة لوظيفة التكلفة. 

16
00:01:02,320 --> 00:01:04,840
سنركز فقط على الاتصال بين آخر خليتين عصبيتين. 

17
00:01:05,980 --> 00:01:08,808
دعونا نسمي تنشيط تلك الخلية العصبية الأخيرة بالحرف 

18
00:01:08,808 --> 00:01:11,360
L المرتفع، للإشارة إلى الطبقة التي توجد فيها. 

19
00:01:11,680 --> 00:01:15,560
وبالتالي فإن تنشيط الخلية العصبية السابقة هو AL-1. 

20
00:01:16,360 --> 00:01:19,631
هذه ليست أسسًا، إنها مجرد وسيلة لفهرسة ما نتحدث 

21
00:01:19,631 --> 00:01:23,040
عنه، حيث أريد حفظ اشتراكات لمؤشرات مختلفة لاحقًا. 

22
00:01:23,720 --> 00:01:27,798
لنفترض أن القيمة التي نريد أن يكون هذا التنشيط الأخير 

23
00:01:27,798 --> 00:01:32,180
لمثال تدريب معين هي y، على سبيل المثال، قد تكون y 0 أو 1. 

24
00:01:32,840 --> 00:01:39,240
وبالتالي فإن تكلفة هذه الشبكة لمثال تدريبي واحد هي AL-y2. 

25
00:01:40,260 --> 00:01:44,380
سنشير إلى تكلفة هذا المثال التدريبي بالرمز c0. 

26
00:01:45,900 --> 00:01:51,903
للتذكير، يتم تحديد هذا التنشيط الأخير من خلال الوزن، والذي سأسميه wL، مضروبًا 

27
00:01:51,903 --> 00:01:57,600
في تنشيط الخلية العصبية السابقة بالإضافة إلى بعض التحيز، والذي سأسميه bL. 

28
00:01:57,600 --> 00:02:01,320
ثم تقوم بضخ ذلك من خلال بعض الوظائف غير الخطية الخاصة مثل السيني أو ReLU. 

29
00:02:01,800 --> 00:02:05,452
في الواقع، سيكون الأمر أسهل بالنسبة لنا إذا أعطينا اسمًا خاصًا لهذا 

30
00:02:05,452 --> 00:02:09,320
المجموع المرجح، مثل z، بنفس الحرف المرتفع مثل عمليات التنشيط ذات الصلة. 

31
00:02:10,380 --> 00:02:15,661
هذا كثير من المصطلحات، والطريقة التي يمكنك تصورها هي أن الوزن والإجراء 

32
00:02:15,661 --> 00:02:20,570
السابق والتحيز معًا يُستخدم لحساب z، والذي بدوره يتيح لنا حساب a، 

33
00:02:20,570 --> 00:02:25,480
والذي أخيرًا، جنبًا إلى جنب مع ثابت y، يتيح لنا لنا حساب التكلفة. 

34
00:02:27,340 --> 00:02:35,060
وبطبيعة الحال، يتأثر AL-1 بوزنه وتحيزه وما إلى ذلك، لكننا لن نركز على ذلك الآن. 

35
00:02:35,700 --> 00:02:37,620
كل هذه مجرد أرقام، أليس كذلك؟ 

36
00:02:38,060 --> 00:02:41,040
وقد يكون من الجيد التفكير في أن كل واحدة لها خط أعداد صغير خاص بها. 

37
00:02:41,720 --> 00:02:49,000
هدفنا الأول هو فهم مدى حساسية دالة التكلفة للتغيرات الصغيرة في وزننا. 

38
00:02:49,540 --> 00:02:52,162
أو قم بالعبارة بشكل مختلف، ما هو مشتق c بالنسبة لـ wL؟ عندما ترى هذا 

39
00:02:52,162 --> 00:02:54,860
المصطلح del w، فكر في أنه يعني دفعة صغيرة إلى w، مثل التغيير بمقدار 0. 

40
00:02:55,600 --> 00:03:08,060
01، وفكر في هذا المصطلح على أنه يعني مهما كانت الدفعة الناتجة إلى التكلفة. 

41
00:03:08,060 --> 00:03:10,220
ما نريده هو نسبتهم. 

42
00:03:11,260 --> 00:03:16,284
من الناحية النظرية، تؤدي هذه الدفعة الصغيرة إلى wL إلى بعض الدفع إلى zL، 

43
00:03:16,284 --> 00:03:21,240
والذي يؤدي بدوره إلى بعض الدفع إلى AL، مما يؤثر بشكل مباشر على التكلفة. 

44
00:03:23,120 --> 00:03:27,920
لذلك نقوم بتقسيم الأمور من خلال النظر أولاً إلى نسبة التغير 

45
00:03:27,920 --> 00:03:33,200
الطفيف في zL إلى هذا التغير الطفيف w، أي مشتقة zL بالنسبة إلى wL. 

46
00:03:33,200 --> 00:03:38,829
وبالمثل، عليك أن تأخذ في الاعتبار نسبة التغيير إلى AL إلى التغيير الطفيف في zL الذي 

47
00:03:38,829 --> 00:03:44,660
تسبب في ذلك، بالإضافة إلى النسبة بين الدفعة النهائية إلى c وهذه الدفعة الوسيطة إلى AL. 

48
00:03:45,740 --> 00:03:55,140
هذه هي قاعدة السلسلة، حيث أن ضرب هذه النسب الثلاث يعطينا حساسية c للتغيرات الصغيرة في wL. 

49
00:03:56,880 --> 00:04:01,449
إذن على الشاشة الآن، هناك الكثير من الرموز، وتوقف لحظة للتأكد 

50
00:04:01,449 --> 00:04:06,240
من أنها واضحة جميعًا، لأننا الآن سنقوم بحساب المشتقات ذات الصلة. 

51
00:04:07,440 --> 00:04:14,180
مشتق c بالنسبة لـ AL يصبح 2AL-y. 

52
00:04:14,180 --> 00:04:18,666
وهذا يعني أن حجمها يتناسب مع الفرق بين مخرجات الشبكة والشيء الذي نريدها 

53
00:04:18,666 --> 00:04:22,965
أن تكون عليه، لذلك إذا كان هذا الناتج مختلفًا تمامًا، فحتى التغييرات 

54
00:04:22,965 --> 00:04:27,140
الطفيفة من شأنها أن يكون لها تأثير كبير على دالة التكلفة النهائية. 

55
00:04:27,840 --> 00:04:37,420
مشتق AL بالنسبة إلى zL هو مجرد مشتق للدالة السينية، أو أي دالة غير خطية تختار استخدامها. 

56
00:04:37,420 --> 00:04:46,160
مشتق zL بالنسبة إلى wL يصبح AL-1. 

57
00:04:46,160 --> 00:04:49,889
لا أعرف عنك، ولكني أعتقد أنه من السهل أن تتعثر في الصيغ 

58
00:04:49,889 --> 00:04:53,420
دون أن تأخذ لحظة لتجلس وتذكّر نفسك بما تعنيه جميعها. 

59
00:04:53,920 --> 00:04:58,476
في حالة هذا المشتق الأخير، فإن مقدار تأثير الدفعة الصغيرة للوزن 

60
00:04:58,476 --> 00:05:02,820
على الطبقة الأخيرة يعتمد على مدى قوة الخلية العصبية السابقة. 

61
00:05:03,380 --> 00:05:08,280
تذكر، هذا هو المكان الذي تأتي فيه فكرة الخلايا العصبية التي تشتعل معًا. 

62
00:05:09,200 --> 00:05:15,720
وكل هذا هو مشتق فيما يتعلق بـ wL فقط من تكلفة مثال تدريبي واحد محدد. 

63
00:05:16,440 --> 00:05:22,695
بما أن دالة التكلفة الكاملة تتضمن حساب متوسط كل تلك التكاليف معًا عبر العديد من أمثلة 

64
00:05:22,695 --> 00:05:28,660
التدريب المختلفة، فإن مشتقها يتطلب حساب متوسط هذا التعبير على جميع أمثلة التدريب. 

65
00:05:28,660 --> 00:05:33,203
بالطبع، هذا مجرد عنصر واحد من متجه التدرج، والذي تم إنشاؤه من 

66
00:05:33,203 --> 00:05:38,260
المشتقات الجزئية لدالة التكلفة فيما يتعلق بكل تلك الأوزان والتحيزات. 

67
00:05:40,640 --> 00:05:42,888
لكن على الرغم من أن هذه مجرد واحدة من المشتقات الجزئية 

68
00:05:42,888 --> 00:05:45,260
العديدة التي نحتاجها، إلا أنها تمثل أكثر من 50% من العمل. 

69
00:05:46,340 --> 00:05:49,720
فالحساسية تجاه التحيز، على سبيل المثال، تكاد تكون متطابقة. 

70
00:05:50,040 --> 00:05:55,020
نحتاج فقط إلى تغيير مصطلح del z del w هذا إلى del z del b. 

71
00:05:58,420 --> 00:06:02,400
وإذا نظرت إلى الصيغة ذات الصلة، فستجد أن هذا المشتق يساوي 1. 

72
00:06:06,140 --> 00:06:10,700
أيضًا، وهذا هو المكان الذي تأتي فيه فكرة الانتشار للخلف، 

73
00:06:10,700 --> 00:06:15,740
يمكنك معرفة مدى حساسية دالة التكلفة هذه لتنشيط الطبقة السابقة. 

74
00:06:15,740 --> 00:06:25,660
أي أن هذا المشتق الأولي في تعبير قاعدة السلسلة، حساسية z للتنشيط السابق، يصبح الوزن wL. 

75
00:06:26,640 --> 00:06:31,927
ومرة أخرى، على الرغم من أننا لن نكون قادرين على التأثير بشكل مباشر على تنشيط الطبقة 

76
00:06:31,927 --> 00:06:37,278
السابقة، فمن المفيد تتبع ذلك، لأنه يمكننا الآن الاستمرار في تكرار فكرة قاعدة السلسلة 

77
00:06:37,278 --> 00:06:42,440
نفسها إلى الوراء لنرى مدى حساسية وظيفة التكلفة الأوزان السابقة والتحيزات السابقة. 

78
00:06:43,180 --> 00:06:47,047
وقد تعتقد أن هذا مثال بسيط للغاية، نظرًا لأن جميع الطبقات تحتوي على خلية 

79
00:06:47,047 --> 00:06:51,020
عصبية واحدة، وستصبح الأمور أكثر تعقيدًا بشكل كبير بالنسبة للشبكة الحقيقية. 

80
00:06:51,700 --> 00:06:55,099
لكن بصراحة، ليس هناك الكثير من التغييرات عندما نعطي الطبقات خلايا 

81
00:06:55,099 --> 00:06:58,860
عصبية متعددة، إنها في الحقيقة مجرد عدد قليل من المؤشرات التي يجب تتبعها. 

82
00:06:59,380 --> 00:07:03,270
بدلًا من تنشيط طبقة معينة لتكون AL، سيكون لها أيضًا 

83
00:07:03,270 --> 00:07:07,160
رمز منخفض يشير إلى أي خلية عصبية تنتمي لتلك الطبقة. 

84
00:07:07,160 --> 00:07:14,420
لنستخدم الحرف k لفهرسة الطبقة L-1، وj لفهرسة الطبقة L. 

85
00:07:15,260 --> 00:07:20,328
بالنسبة للتكلفة، ننظر مرة أخرى إلى الناتج المطلوب، لكن هذه المرة نضيف 

86
00:07:20,328 --> 00:07:25,180
مربعات الاختلافات بين عمليات تنشيط الطبقة الأخيرة والمخرج المطلوب. 

87
00:07:26,080 --> 00:07:30,840
وهذا يعني أنك تأخذ مجموعًا على ALj ناقص yj تربيع. 

88
00:07:33,040 --> 00:07:38,870
نظرًا لوجود الكثير من الأوزان، يجب أن يكون لكل واحد مؤشرين إضافيين لتتبع مكانه، 

89
00:07:38,870 --> 00:07:44,920
لذلك دعونا نسمي وزن الحافة التي تربط هذه الخلية العصبية k بالخلية العصبية j، WLjk. 

90
00:07:45,620 --> 00:07:49,256
قد تبدو هذه المؤشرات متخلفة قليلاً في البداية، ولكنها تتوافق مع 

91
00:07:49,256 --> 00:07:53,120
كيفية فهرسة مصفوفة الوزن التي تحدثت عنها في الجزء الأول من الفيديو. 

92
00:07:53,620 --> 00:07:58,820
كما كان من قبل، لا يزال من الجيد إعطاء اسم للمجموع المرجح ذي الصلة، مثل z، 

93
00:07:58,820 --> 00:08:04,160
بحيث يكون تنشيط الطبقة الأخيرة مجرد وظيفتك الخاصة، مثل السيني، المطبق على z. 

94
00:08:04,660 --> 00:08:09,086
يمكنك أن ترى ما أعنيه، حيث أن كل هذه هي في الأساس نفس المعادلات التي كانت لدينا 

95
00:08:09,086 --> 00:08:13,680
من قبل في حالة خلية عصبية واحدة لكل طبقة، الأمر فقط أنها تبدو أكثر تعقيدًا قليلاً. 

96
00:08:15,440 --> 00:08:19,430
وبالفعل، فإن التعبير المشتق لقاعدة السلسلة الذي يصف 

97
00:08:19,430 --> 00:08:23,420
مدى حساسية التكلفة لوزن معين يبدو كما هو في الأساس. 

98
00:08:23,920 --> 00:08:26,840
سأترك الأمر لك للتوقف والتفكير في كل مصطلح من هذه المصطلحات إذا كنت تريد ذلك. 

99
00:08:28,979 --> 00:08:36,659
لكن ما يتغير هنا هو مشتق التكلفة فيما يتعلق بأحد عمليات التنشيط في الطبقة L-1. 

100
00:08:37,780 --> 00:08:40,329
في هذه الحالة، الفرق هو أن الخلية العصبية تؤثر 

101
00:08:40,329 --> 00:08:42,880
على دالة التكلفة من خلال مسارات مختلفة متعددة. 

102
00:08:44,680 --> 00:08:50,949
وهذا يعني، من ناحية، أنها تؤثر على AL0، التي تلعب دورًا في دالة التكلفة، ولكن 

103
00:08:50,949 --> 00:08:57,540
لها أيضًا تأثير على AL1، والتي تلعب أيضًا دورًا في دالة التكلفة، ويجب عليك جمعها. 

104
00:08:59,820 --> 00:09:03,040
وهذا، حسنًا، هذا كل شيء تقريبًا. 

105
00:09:03,500 --> 00:09:08,064
بمجرد أن تعرف مدى حساسية دالة التكلفة لعمليات التنشيط في هذه الطبقة من الثانية 

106
00:09:08,064 --> 00:09:12,860
إلى الأخيرة، يمكنك فقط تكرار العملية لجميع الأوزان والتحيزات التي تغذي تلك الطبقة. 

107
00:09:13,900 --> 00:09:14,960
لذا ربت على ظهرك! 

108
00:09:15,300 --> 00:09:18,990
إذا كان كل هذا منطقيًا، فقد نظرت الآن بعمق في قلب الانتشار 

109
00:09:18,990 --> 00:09:22,680
العكسي، وهو العمود الفقري وراء كيفية تعلم الشبكات العصبية. 

110
00:09:23,300 --> 00:09:28,069
تمنحك تعبيرات قواعد السلسلة هذه المشتقات التي تحدد كل مكون في 

111
00:09:28,069 --> 00:09:33,300
التدرج الذي يساعد على تقليل تكلفة الشبكة عن طريق النزول بشكل متكرر. 

112
00:09:34,300 --> 00:09:38,613
إذا جلست وفكرت في كل ذلك، فستجد أن هناك الكثير من طبقات التعقيد التي 

113
00:09:38,613 --> 00:09:42,740
يحيط بها عقلك، لذا لا تقلق إذا استغرق عقلك وقتًا لاستيعاب كل ذلك. 

