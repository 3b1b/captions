1
00:00:00,000 --> 00:00:05,238
الافتراض الصعب هنا هو أنك شاهدت الجزء الثالث،

2
00:00:05,238 --> 00:00:11,160
والذي يقدم شرحًا بديهيًا لخوارزمية الانتشار العكسي.

3
00:00:11,160 --> 00:00:14,920
هنا نحصل على المزيد من الرسمية ونغوص في حسابات التفاضل والتكامل ذات الصلة.

4
00:00:14,920 --> 00:00:18,536
من الطبيعي أن يكون هذا مربكًا بعض الشيء على الأقل، لذا فإن شعار التوقف

5
00:00:18,536 --> 00:00:22,000
والتأمل بشكل منتظم ينطبق بالتأكيد هنا بقدر ما ينطبق في أي مكان آخر.

6
00:00:22,000 --> 00:00:26,085
هدفنا الرئيسي هو إظهار كيف يفكر الأشخاص في التعلم الآلي بشكل شائع حول قاعدة

7
00:00:26,085 --> 00:00:30,117
السلسلة من حساب التفاضل والتكامل في سياق الشبكات، والتي لها إحساس مختلف عن

8
00:00:30,117 --> 00:00:34,580
الطريقة التي تتعامل بها معظم دورات حساب التفاضل والتكامل التمهيدية مع هذا الموضوع.

9
00:00:34,580 --> 00:00:37,046
لأولئك منكم الذين لا يشعرون بالارتياح تجاه حسابات التفاضل

10
00:00:37,046 --> 00:00:39,300
والتكامل ذات الصلة، لدي سلسلة كاملة حول هذا الموضوع.

11
00:00:39,300 --> 00:00:46,780
لنبدأ بشبكة بسيطة للغاية، حيث تحتوي كل طبقة على خلية عصبية واحدة.

12
00:00:46,780 --> 00:00:51,083
يتم تحديد هذه الشبكة بثلاثة أوزان وثلاثة انحيازات،

13
00:00:51,083 --> 00:00:55,640
وهدفنا هو فهم مدى حساسية دالة التكلفة لهذه المتغيرات.

14
00:00:55,640 --> 00:00:58,726
وبهذه الطريقة نعرف أي تعديلات على هذه الشروط ستتسبب

15
00:00:58,726 --> 00:01:01,100
في التخفيض الأكثر كفاءة لوظيفة التكلفة.

16
00:01:01,100 --> 00:01:05,360
سنركز فقط على الاتصال بين آخر خليتين عصبيتين.

17
00:01:05,360 --> 00:01:08,745
دعونا نسمي تنشيط تلك الخلية العصبية الأخيرة بالحرف

18
00:01:08,745 --> 00:01:11,800
L المرتفع، للإشارة إلى الطبقة التي توجد فيها.

19
00:01:11,800 --> 00:01:16,560
وبالتالي فإن تنشيط الخلية العصبية السابقة هو AL-1.

20
00:01:16,560 --> 00:01:19,773
هذه ليست أسسًا، إنها مجرد وسيلة لفهرسة ما نتحدث

21
00:01:19,773 --> 00:01:23,120
عنه، حيث أريد حفظ اشتراكات لمؤشرات مختلفة لاحقًا.

22
00:01:23,120 --> 00:01:27,893
لنفترض أن القيمة التي نريد أن يكون هذا التنشيط الأخير

23
00:01:27,893 --> 00:01:33,020
لمثال تدريب معين هي y، على سبيل المثال، قد تكون y 0 أو 1.

24
00:01:33,020 --> 00:01:39,040
وبالتالي فإن تكلفة هذه الشبكة لمثال تدريبي واحد هي AL-y2.

25
00:01:39,040 --> 00:01:46,120
سنشير إلى تكلفة هذا المثال التدريبي بالرمز c0.

26
00:01:46,120 --> 00:01:52,011
للتذكير، يتم تحديد هذا التنشيط الأخير من خلال الوزن، والذي سأسميه wL، مضروبًا

27
00:01:52,011 --> 00:01:57,600
في تنشيط الخلية العصبية السابقة بالإضافة إلى بعض التحيز، والذي سأسميه bL.

28
00:01:57,600 --> 00:02:01,560
ثم تقوم بضخ ذلك من خلال بعض الوظائف غير الخطية الخاصة مثل السيني أو ReLU.

29
00:02:01,560 --> 00:02:05,950
في الواقع، سيكون الأمر أسهل بالنسبة لنا إذا أعطينا اسمًا خاصًا لهذا

30
00:02:05,950 --> 00:02:10,600
المجموع المرجح، مثل z، بنفس الحرف المرتفع مثل عمليات التنشيط ذات الصلة.

31
00:02:10,600 --> 00:02:16,461
هذا كثير من المصطلحات، والطريقة التي يمكنك تصورها هي أن الوزن والإجراء

32
00:02:16,461 --> 00:02:21,910
السابق والتحيز معًا يُستخدم لحساب z، والذي بدوره يتيح لنا حساب a،

33
00:02:21,910 --> 00:02:27,360
والذي أخيرًا، جنبًا إلى جنب مع ثابت y، يتيح لنا لنا حساب التكلفة.

34
00:02:27,360 --> 00:02:35,920
وبطبيعة الحال، يتأثر AL-1 بوزنه وتحيزه وما إلى ذلك، لكننا لن نركز على ذلك الآن.

35
00:02:35,920 --> 00:02:38,120
كل هذه مجرد أرقام، أليس كذلك؟

36
00:02:38,120 --> 00:02:41,960
وقد يكون من الجيد التفكير في أن كل واحدة لها خط أعداد صغير خاص بها.

37
00:02:41,960 --> 00:02:49,820
هدفنا الأول هو فهم مدى حساسية دالة التكلفة للتغيرات الصغيرة في وزننا.

38
00:02:49,820 --> 00:02:52,737
أو قم بالعبارة بشكل مختلف، ما هو مشتق c بالنسبة لـ wL؟ عندما ترى هذا

39
00:02:52,737 --> 00:02:55,740
المصطلح del w، فكر في أنه يعني دفعة صغيرة إلى w، مثل التغيير بمقدار 0.

40
00:02:55,740 --> 00:03:08,820
01، وفكر في هذا المصطلح على أنه يعني مهما كانت الدفعة الناتجة إلى التكلفة.

41
00:03:08,820 --> 00:03:10,900
ما نريده هو نسبتهم.

42
00:03:10,900 --> 00:03:17,102
من الناحية النظرية، تؤدي هذه الدفعة الصغيرة إلى wL إلى بعض الدفع إلى zL،

43
00:03:17,102 --> 00:03:23,220
والذي يؤدي بدوره إلى بعض الدفع إلى AL، مما يؤثر بشكل مباشر على التكلفة.

44
00:03:23,220 --> 00:03:28,039
لذلك نقوم بتقسيم الأمور من خلال النظر أولاً إلى نسبة التغير

45
00:03:28,039 --> 00:03:33,340
الطفيف في zL إلى هذا التغير الطفيف w، أي مشتقة zL بالنسبة إلى wL.

46
00:03:33,340 --> 00:03:39,509
وبالمثل، عليك أن تأخذ في الاعتبار نسبة التغيير إلى AL إلى التغيير الطفيف في zL الذي

47
00:03:39,509 --> 00:03:45,900
تسبب في ذلك، بالإضافة إلى النسبة بين الدفعة النهائية إلى c وهذه الدفعة الوسيطة إلى AL.

48
00:03:45,900 --> 00:03:57,340
هذه هي قاعدة السلسلة، حيث أن ضرب هذه النسب الثلاث يعطينا حساسية c للتغيرات الصغيرة في wL.

49
00:03:57,340 --> 00:04:02,280
إذن على الشاشة الآن، هناك الكثير من الرموز، وتوقف لحظة للتأكد

50
00:04:02,280 --> 00:04:07,460
من أنها واضحة جميعًا، لأننا الآن سنقوم بحساب المشتقات ذات الصلة.

51
00:04:07,460 --> 00:04:14,220
مشتق c بالنسبة لـ AL يصبح 2AL-y.

52
00:04:14,220 --> 00:04:19,121
وهذا يعني أن حجمها يتناسب مع الفرق بين مخرجات الشبكة والشيء الذي نريدها

53
00:04:19,121 --> 00:04:23,818
أن تكون عليه، لذلك إذا كان هذا الناتج مختلفًا تمامًا، فحتى التغييرات

54
00:04:23,818 --> 00:04:28,380
الطفيفة من شأنها أن يكون لها تأثير كبير على دالة التكلفة النهائية.

55
00:04:28,380 --> 00:04:37,420
مشتق AL بالنسبة إلى zL هو مجرد مشتق للدالة السينية، أو أي دالة غير خطية تختار استخدامها.

56
00:04:37,420 --> 00:04:46,180
مشتق zL بالنسبة إلى wL يصبح AL-1.

57
00:04:46,180 --> 00:04:50,290
لا أعرف عنك، ولكني أعتقد أنه من السهل أن تتعثر في الصيغ

58
00:04:50,290 --> 00:04:54,180
دون أن تأخذ لحظة لتجلس وتذكّر نفسك بما تعنيه جميعها.

59
00:04:54,180 --> 00:04:58,808
في حالة هذا المشتق الأخير، فإن مقدار تأثير الدفعة الصغيرة للوزن

60
00:04:58,808 --> 00:05:03,220
على الطبقة الأخيرة يعتمد على مدى قوة الخلية العصبية السابقة.

61
00:05:03,220 --> 00:05:09,320
تذكر، هذا هو المكان الذي تأتي فيه فكرة الخلايا العصبية التي تشتعل معًا.

62
00:05:09,320 --> 00:05:16,580
وكل هذا هو مشتق فيما يتعلق بـ wL فقط من تكلفة مثال تدريبي واحد محدد.

63
00:05:16,580 --> 00:05:22,702
بما أن دالة التكلفة الكاملة تتضمن حساب متوسط كل تلك التكاليف معًا عبر العديد من أمثلة

64
00:05:22,702 --> 00:05:28,540
التدريب المختلفة، فإن مشتقها يتطلب حساب متوسط هذا التعبير على جميع أمثلة التدريب.

65
00:05:28,540 --> 00:05:34,332
بالطبع، هذا مجرد عنصر واحد من متجه التدرج، والذي تم إنشاؤه من

66
00:05:34,332 --> 00:05:40,780
المشتقات الجزئية لدالة التكلفة فيما يتعلق بكل تلك الأوزان والتحيزات.

67
00:05:40,780 --> 00:05:43,544
لكن على الرغم من أن هذه مجرد واحدة من المشتقات الجزئية

68
00:05:43,544 --> 00:05:46,460
العديدة التي نحتاجها، إلا أنها تمثل أكثر من 50% من العمل.

69
00:05:46,460 --> 00:05:50,300
فالحساسية تجاه التحيز، على سبيل المثال، تكاد تكون متطابقة.

70
00:05:50,300 --> 00:05:58,980
نحتاج فقط إلى تغيير مصطلح del z del w هذا إلى del z del b.

71
00:05:58,980 --> 00:06:04,700
وإذا نظرت إلى الصيغة ذات الصلة، فستجد أن هذا المشتق يساوي 1.

72
00:06:04,700 --> 00:06:10,153
أيضًا، وهذا هو المكان الذي تأتي فيه فكرة الانتشار للخلف،

73
00:06:10,153 --> 00:06:16,180
يمكنك معرفة مدى حساسية دالة التكلفة هذه لتنشيط الطبقة السابقة.

74
00:06:16,180 --> 00:06:25,420
أي أن هذا المشتق الأولي في تعبير قاعدة السلسلة، حساسية z للتنشيط السابق، يصبح الوزن wL.

75
00:06:25,420 --> 00:06:31,530
ومرة أخرى، على الرغم من أننا لن نكون قادرين على التأثير بشكل مباشر على تنشيط الطبقة

76
00:06:31,530 --> 00:06:37,714
السابقة، فمن المفيد تتبع ذلك، لأنه يمكننا الآن الاستمرار في تكرار فكرة قاعدة السلسلة

77
00:06:37,714 --> 00:06:43,680
نفسها إلى الوراء لنرى مدى حساسية وظيفة التكلفة الأوزان السابقة والتحيزات السابقة.

78
00:06:43,680 --> 00:06:47,448
وقد تعتقد أن هذا مثال بسيط للغاية، نظرًا لأن جميع الطبقات تحتوي على خلية

79
00:06:47,448 --> 00:06:51,320
عصبية واحدة، وستصبح الأمور أكثر تعقيدًا بشكل كبير بالنسبة للشبكة الحقيقية.

80
00:06:51,320 --> 00:06:55,118
لكن بصراحة، ليس هناك الكثير من التغييرات عندما نعطي الطبقات خلايا

81
00:06:55,118 --> 00:06:59,320
عصبية متعددة، إنها في الحقيقة مجرد عدد قليل من المؤشرات التي يجب تتبعها.

82
00:06:59,320 --> 00:07:03,620
بدلًا من تنشيط طبقة معينة لتكون AL، سيكون لها أيضًا

83
00:07:03,620 --> 00:07:07,920
رمز منخفض يشير إلى أي خلية عصبية تنتمي لتلك الطبقة.

84
00:07:07,920 --> 00:07:15,280
لنستخدم الحرف k لفهرسة الطبقة L-1، وj لفهرسة الطبقة L.

85
00:07:15,280 --> 00:07:20,818
بالنسبة للتكلفة، ننظر مرة أخرى إلى الناتج المطلوب، لكن هذه المرة نضيف

86
00:07:20,818 --> 00:07:26,120
مربعات الاختلافات بين عمليات تنشيط الطبقة الأخيرة والمخرج المطلوب.

87
00:07:26,120 --> 00:07:33,280
وهذا يعني أنك تأخذ مجموعًا على ALj ناقص yj تربيع.

88
00:07:33,280 --> 00:07:39,395
نظرًا لوجود الكثير من الأوزان، يجب أن يكون لكل واحد مؤشرين إضافيين لتتبع مكانه،

89
00:07:39,395 --> 00:07:45,740
لذلك دعونا نسمي وزن الحافة التي تربط هذه الخلية العصبية k بالخلية العصبية j، WLjk.

90
00:07:45,740 --> 00:07:49,647
قد تبدو هذه المؤشرات متخلفة قليلاً في البداية، ولكنها تتوافق مع

91
00:07:49,647 --> 00:07:53,800
كيفية فهرسة مصفوفة الوزن التي تحدثت عنها في الجزء الأول من الفيديو.

92
00:07:53,800 --> 00:07:59,316
كما كان من قبل، لا يزال من الجيد إعطاء اسم للمجموع المرجح ذي الصلة، مثل z،

93
00:07:59,316 --> 00:08:04,980
بحيث يكون تنشيط الطبقة الأخيرة مجرد وظيفتك الخاصة، مثل السيني، المطبق على z.

94
00:08:04,980 --> 00:08:10,103
يمكنك أن ترى ما أعنيه، حيث أن كل هذه هي في الأساس نفس المعادلات التي كانت لدينا

95
00:08:10,103 --> 00:08:15,420
من قبل في حالة خلية عصبية واحدة لكل طبقة، الأمر فقط أنها تبدو أكثر تعقيدًا قليلاً.

96
00:08:15,420 --> 00:08:19,480
وبالفعل، فإن التعبير المشتق لقاعدة السلسلة الذي يصف

97
00:08:19,480 --> 00:08:23,540
مدى حساسية التكلفة لوزن معين يبدو كما هو في الأساس.

98
00:08:23,540 --> 00:08:29,420
سأترك الأمر لك للتوقف والتفكير في كل مصطلح من هذه المصطلحات إذا كنت تريد ذلك.

99
00:08:29,420 --> 00:08:37,820
لكن ما يتغير هنا هو مشتق التكلفة فيما يتعلق بأحد عمليات التنشيط في الطبقة L-1.

100
00:08:37,820 --> 00:08:40,680
في هذه الحالة، الفرق هو أن الخلية العصبية تؤثر

101
00:08:40,680 --> 00:08:43,540
على دالة التكلفة من خلال مسارات مختلفة متعددة.

102
00:08:43,540 --> 00:08:51,730
وهذا يعني، من ناحية، أنها تؤثر على AL0، التي تلعب دورًا في دالة التكلفة، ولكن

103
00:08:51,730 --> 00:09:00,340
لها أيضًا تأثير على AL1، والتي تلعب أيضًا دورًا في دالة التكلفة، ويجب عليك جمعها.

104
00:09:00,340 --> 00:09:03,680
وهذا، حسنًا، هذا كل شيء تقريبًا.

105
00:09:03,680 --> 00:09:08,673
بمجرد أن تعرف مدى حساسية دالة التكلفة لعمليات التنشيط في هذه الطبقة من الثانية

106
00:09:08,673 --> 00:09:13,920
إلى الأخيرة، يمكنك فقط تكرار العملية لجميع الأوزان والتحيزات التي تغذي تلك الطبقة.

107
00:09:13,920 --> 00:09:15,420
لذا ربت على ظهرك!

108
00:09:15,420 --> 00:09:19,560
إذا كان كل هذا منطقيًا، فقد نظرت الآن بعمق في قلب الانتشار

109
00:09:19,560 --> 00:09:23,700
العكسي، وهو العمود الفقري وراء كيفية تعلم الشبكات العصبية.

110
00:09:23,700 --> 00:09:29,098
تمنحك تعبيرات قواعد السلسلة هذه المشتقات التي تحدد كل مكون في

111
00:09:29,098 --> 00:09:35,020
التدرج الذي يساعد على تقليل تكلفة الشبكة عن طريق النزول بشكل متكرر.

112
00:09:35,020 --> 00:09:39,016
إذا جلست وفكرت في كل ذلك، فستجد أن هناك الكثير من طبقات التعقيد التي

113
00:09:39,016 --> 00:09:42,840
يحيط بها عقلك، لذا لا تقلق إذا استغرق عقلك وقتًا لاستيعاب كل ذلك.

