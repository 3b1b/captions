[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm. ",
  "translatedText": "الافتراض الصعب هنا هو أنك شاهدت الجزء الثالث، والذي يقدم شرحًا بديهيًا لخوارزمية الانتشار العكسي. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus. ",
  "translatedText": "هنا نحصل على المزيد من الرسمية ونغوص في حسابات التفاضل والتكامل ذات الصلة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else. ",
  "translatedText": "من الطبيعي أن يكون هذا مربكًا بعض الشيء على الأقل، لذا فإن شعار التوقف والتأمل بشكل منتظم ينطبق بالتأكيد هنا بقدر ما ينطبق في أي مكان آخر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject. ",
  "translatedText": "هدفنا الرئيسي هو إظهار كيف يفكر الأشخاص في التعلم الآلي بشكل شائع حول قاعدة السلسلة من حساب التفاضل والتكامل في سياق الشبكات، والتي لها إحساس مختلف عن الطريقة التي تتعامل بها معظم دورات حساب التفاضل والتكامل التمهيدية مع هذا الموضوع. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic. ",
  "translatedText": "لأولئك منكم الذين لا يشعرون بالارتياح تجاه حسابات التفاضل والتكامل ذات الصلة، لدي سلسلة كاملة حول هذا الموضوع. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it. ",
  "translatedText": "لنبدأ بشبكة بسيطة للغاية، حيث تحتوي كل طبقة على خلية عصبية واحدة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables. ",
  "translatedText": "يتم تحديد هذه الشبكة بثلاثة أوزان وثلاثة انحيازات، وهدفنا هو فهم مدى حساسية دالة التكلفة لهذه المتغيرات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way we know which adjustments to those terms will cause the most efficient decrease to the cost function. ",
  "translatedText": "وبهذه الطريقة نعرف أي تعديلات على هذه الشروط ستتسبب في التخفيض الأكثر كفاءة لوظيفة التكلفة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "We'll just focus on the connection between the last two neurons. ",
  "translatedText": "سنركز فقط على الاتصال بين آخر خليتين عصبيتين. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in. ",
  "translatedText": "دعونا نسمي تنشيط تلك الخلية العصبية الأخيرة بالحرف L المرتفع، للإشارة إلى الطبقة التي توجد فيها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.98,
  "end": 71.36
 },
 {
  "input": "So the activation of the previous neuron is AL-1. ",
  "translatedText": "وبالتالي فإن تنشيط الخلية العصبية السابقة هو AL-1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.68,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on. ",
  "translatedText": "هذه ليست أسسًا، إنها مجرد وسيلة لفهرسة ما نتحدث عنه، حيث أريد حفظ اشتراكات لمؤشرات مختلفة لاحقًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1. ",
  "translatedText": "لنفترض أن القيمة التي نريد أن يكون هذا التنشيط الأخير لمثال تدريب معين هي y، على سبيل المثال، قد تكون y 0 أو 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is AL-y2. ",
  "translatedText": "وبالتالي فإن تكلفة هذه الشبكة لمثال تدريبي واحد هي AL-y2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0. ",
  "translatedText": "سنشير إلى تكلفة هذا المثال التدريبي بالرمز c0. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call wL, times the previous neuron's activation plus some bias, which I'll call bL. ",
  "translatedText": "للتذكير، يتم تحديد هذا التنشيط الأخير من خلال الوزن، والذي سأسميه wL، مضروبًا في تنشيط الخلية العصبية السابقة بالإضافة إلى بعض التحيز، والذي سأسميه bL. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "Then you pump that through some special nonlinear function like the sigmoid or ReLU. ",
  "translatedText": "ثم تقوم بضخ ذلك من خلال بعض الوظائف غير الخطية الخاصة مثل السيني أو ReLU. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations. ",
  "translatedText": "في الواقع، سيكون الأمر أسهل بالنسبة لنا إذا أعطينا اسمًا خاصًا لهذا المجموع المرجح، مثل z، بنفس الحرف المرتفع مثل عمليات التنشيط ذات الصلة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action, and bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost. ",
  "translatedText": "هذا كثير من المصطلحات، والطريقة التي يمكنك تصورها هي أن الوزن والإجراء السابق والتحيز معًا يُستخدم لحساب z، والذي بدوره يتيح لنا حساب a، والذي أخيرًا، جنبًا إلى جنب مع ثابت y، يتيح لنا لنا حساب التكلفة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course, AL-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now. ",
  "translatedText": "وبطبيعة الحال، يتأثر AL-1 بوزنه وتحيزه وما إلى ذلك، لكننا لن نركز على ذلك الآن. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right? ",
  "translatedText": "كل هذه مجرد أرقام، أليس كذلك؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line. ",
  "translatedText": "وقد يكون من الجيد التفكير في أن كل واحدة لها خط أعداد صغير خاص بها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight wL. ",
  "translatedText": "هدفنا الأول هو فهم مدى حساسية دالة التكلفة للتغيرات الصغيرة في وزننا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to wL? ",
  "translatedText": "أو قم بالعبارة بشكل مختلف، ما هو مشتق c بالنسبة لـ wL؟ عندما ترى هذا المصطلح del w، فكر في أنه يعني دفعة صغيرة إلى w، مثل التغيير بمقدار 0. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del w term, think of it as meaning some tiny nudge to w, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is. ",
  "translatedText": "01، وفكر في هذا المصطلح على أنه يعني مهما كانت الدفعة الناتجة إلى التكلفة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio. ",
  "translatedText": "ما نريده هو نسبتهم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to wL causes some nudge to zL, which in turn causes some nudge to AL, which directly influences the cost. ",
  "translatedText": "من الناحية النظرية، تؤدي هذه الدفعة الصغيرة إلى wL إلى بعض الدفع إلى zL، والذي يؤدي بدوره إلى بعض الدفع إلى AL، مما يؤثر بشكل مباشر على التكلفة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to zL to this tiny change w, that is, the derivative of zL with respect to wL. ",
  "translatedText": "لذلك نقوم بتقسيم الأمور من خلال النظر أولاً إلى نسبة التغير الطفيف في zL إلى هذا التغير الطفيف w، أي مشتقة zL بالنسبة إلى wL. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in zL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL. ",
  "translatedText": "وبالمثل، عليك أن تأخذ في الاعتبار نسبة التغيير إلى AL إلى التغيير الطفيف في zL الذي تسبب في ذلك، بالإضافة إلى النسبة بين الدفعة النهائية إلى c وهذه الدفعة الوسيطة إلى AL. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying these three ratios gives us the sensitivity of c to small changes in wL. ",
  "translatedText": "هذه هي قاعدة السلسلة، حيث أن ضرب هذه النسب الثلاث يعطينا حساسية c للتغيرات الصغيرة في wL. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives. ",
  "translatedText": "إذن على الشاشة الآن، هناك الكثير من الرموز، وتوقف لحظة للتأكد من أنها واضحة جميعًا، لأننا الآن سنقوم بحساب المشتقات ذات الصلة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y. ",
  "translatedText": "مشتق c بالنسبة لـ AL يصبح 2AL-y. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "This means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function. ",
  "translatedText": "وهذا يعني أن حجمها يتناسب مع الفرق بين مخرجات الشبكة والشيء الذي نريدها أن تكون عليه، لذلك إذا كان هذا الناتج مختلفًا تمامًا، فحتى التغييرات الطفيفة من شأنها أن يكون لها تأثير كبير على دالة التكلفة النهائية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to zL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use. ",
  "translatedText": "مشتق AL بالنسبة إلى zL هو مجرد مشتق للدالة السينية، أو أي دالة غير خطية تختار استخدامها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "The derivative of zL with respect to wL comes out to be AL-1. ",
  "translatedText": "مشتق zL بالنسبة إلى wL يصبح AL-1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself what they all mean. ",
  "translatedText": "لا أعرف عنك، ولكني أعتقد أنه من السهل أن تتعثر في الصيغ دون أن تأخذ لحظة لتجلس وتذكّر نفسك بما تعنيه جميعها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is. ",
  "translatedText": "في حالة هذا المشتق الأخير، فإن مقدار تأثير الدفعة الصغيرة للوزن على الطبقة الأخيرة يعتمد على مدى قوة الخلية العصبية السابقة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in. ",
  "translatedText": "تذكر، هذا هو المكان الذي تأتي فيه فكرة الخلايا العصبية التي تشتعل معًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to wL only of the cost for a specific single training example. ",
  "translatedText": "وكل هذا هو مشتق فيما يتعلق بـ wL فقط من تكلفة مثال تدريبي واحد محدد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples. ",
  "translatedText": "بما أن دالة التكلفة الكاملة تتضمن حساب متوسط كل تلك التكاليف معًا عبر العديد من أمثلة التدريب المختلفة، فإن مشتقها يتطلب حساب متوسط هذا التعبير على جميع أمثلة التدريب. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "Of course, that's just one component of the gradient vector, which is built up from the partial derivatives of the cost function with respect to all those weights and biases. ",
  "translatedText": "بالطبع، هذا مجرد عنصر واحد من متجه التدرج، والذي تم إنشاؤه من المشتقات الجزئية لدالة التكلفة فيما يتعلق بكل تلك الأوزان والتحيزات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work. ",
  "translatedText": "لكن على الرغم من أن هذه مجرد واحدة من المشتقات الجزئية العديدة التي نحتاجها، إلا أنها تمثل أكثر من 50% من العمل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical. ",
  "translatedText": "فالحساسية تجاه التحيز، على سبيل المثال، تكاد تكون متطابقة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b. ",
  "translatedText": "نحتاج فقط إلى تغيير مصطلح del z del w هذا إلى del z del b. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1. ",
  "translatedText": "وإذا نظرت إلى الصيغة ذات الصلة، فستجد أن هذا المشتق يساوي 1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer. ",
  "translatedText": "أيضًا، وهذا هو المكان الذي تأتي فيه فكرة الانتشار للخلف، يمكنك معرفة مدى حساسية دالة التكلفة هذه لتنشيط الطبقة السابقة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight wL. ",
  "translatedText": "أي أن هذا المشتق الأولي في تعبير قاعدة السلسلة، حساسية z للتنشيط السابق، يصبح الوزن wL. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases. ",
  "translatedText": "ومرة أخرى، على الرغم من أننا لن نكون قادرين على التأثير بشكل مباشر على تنشيط الطبقة السابقة، فمن المفيد تتبع ذلك، لأنه يمكننا الآن الاستمرار في تكرار فكرة قاعدة السلسلة نفسها إلى الوراء لنرى مدى حساسية وظيفة التكلفة الأوزان السابقة والتحيزات السابقة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network. ",
  "translatedText": "وقد تعتقد أن هذا مثال بسيط للغاية، نظرًا لأن جميع الطبقات تحتوي على خلية عصبية واحدة، وستصبح الأمور أكثر تعقيدًا بشكل كبير بالنسبة للشبكة الحقيقية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of. ",
  "translatedText": "لكن بصراحة، ليس هناك الكثير من التغييرات عندما نعطي الطبقات خلايا عصبية متعددة، إنها في الحقيقة مجرد عدد قليل من المؤشرات التي يجب تتبعها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is. ",
  "translatedText": "بدلًا من تنشيط طبقة معينة لتكون AL، سيكون لها أيضًا رمز منخفض يشير إلى أي خلية عصبية تنتمي لتلك الطبقة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L. ",
  "translatedText": "لنستخدم الحرف k لفهرسة الطبقة L-1، وj لفهرسة الطبقة L. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output. ",
  "translatedText": "بالنسبة للتكلفة، ننظر مرة أخرى إلى الناتج المطلوب، لكن هذه المرة نضيف مربعات الاختلافات بين عمليات تنشيط الطبقة الأخيرة والمخرج المطلوب. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus yj squared. ",
  "translatedText": "وهذا يعني أنك تأخذ مجموعًا على ALj ناقص yj تربيع. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk. ",
  "translatedText": "نظرًا لوجود الكثير من الأوزان، يجب أن يكون لكل واحد مؤشرين إضافيين لتتبع مكانه، لذلك دعونا نسمي وزن الحافة التي تربط هذه الخلية العصبية k بالخلية العصبية j، WLjk. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video. ",
  "translatedText": "قد تبدو هذه المؤشرات متخلفة قليلاً في البداية، ولكنها تتوافق مع كيفية فهرسة مصفوفة الوزن التي تحدثت عنها في الجزء الأول من الفيديو. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z. ",
  "translatedText": "كما كان من قبل، لا يزال من الجيد إعطاء اسم للمجموع المرجح ذي الصلة، مثل z، بحيث يكون تنشيط الطبقة الأخيرة مجرد وظيفتك الخاصة، مثل السيني، المطبق على z. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated. ",
  "translatedText": "يمكنك أن ترى ما أعنيه، حيث أن كل هذه هي في الأساس نفس المعادلات التي كانت لدينا من قبل في حالة خلية عصبية واحدة لكل طبقة، الأمر فقط أنها تبدو أكثر تعقيدًا قليلاً. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain rule derivative expression describing how sensitive the cost is to a specific weight looks essentially the same. ",
  "translatedText": "وبالفعل، فإن التعبير المشتق لقاعدة السلسلة الذي يصف مدى حساسية التكلفة لوزن معين يبدو كما هو في الأساس. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want. ",
  "translatedText": "سأترك الأمر لك للتوقف والتفكير في كل مصطلح من هذه المصطلحات إذا كنت تريد ذلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1. ",
  "translatedText": "لكن ما يتغير هنا هو مشتق التكلفة فيما يتعلق بأحد عمليات التنشيط في الطبقة L-1. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths. ",
  "translatedText": "في هذه الحالة، الفرق هو أن الخلية العصبية تؤثر على دالة التكلفة من خلال مسارات مختلفة متعددة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up. ",
  "translatedText": "وهذا يعني، من ناحية، أنها تؤثر على AL0، التي تلعب دورًا في دالة التكلفة، ولكن لها أيضًا تأثير على AL1، والتي تلعب أيضًا دورًا في دالة التكلفة، ويجب عليك جمعها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it. ",
  "translatedText": "وهذا، حسنًا، هذا كل شيء تقريبًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer. ",
  "translatedText": "بمجرد أن تعرف مدى حساسية دالة التكلفة لعمليات التنشيط في هذه الطبقة من الثانية إلى الأخيرة، يمكنك فقط تكرار العملية لجميع الأوزان والتحيزات التي تغذي تلك الطبقة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back! ",
  "translatedText": "لذا ربت على ظهرك! ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn. ",
  "translatedText": "إذا كان كل هذا منطقيًا، فقد نظرت الآن بعمق في قلب الانتشار العكسي، وهو العمود الفقري وراء كيفية تعلم الشبكات العصبية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill. ",
  "translatedText": "تمنحك تعبيرات قواعد السلسلة هذه المشتقات التي تحدد كل مكون في التدرج الذي يساعد على تقليل تكلفة الشبكة عن طريق النزول بشكل متكرر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all. ",
  "translatedText": "إذا جلست وفكرت في كل ذلك، فستجد أن هناك الكثير من طبقات التعقيد التي يحيط بها عقلك، لذا لا تقلق إذا استغرق عقلك وقتًا لاستيعاب كل ذلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]