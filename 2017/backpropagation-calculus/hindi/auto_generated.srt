1
00:00:00,000 --> 00:00:08,420
यहां कठिन धारणा यह है कि आपने भाग 3 देखा है,

2
00:00:08,420 --> 00:00:11,160
जिसमें बैकप्रोपेगेशन एल्गोरिदम का सहज ज्ञान युक्त विवरण दिया गया है।

3
00:00:11,160 --> 00:00:14,920
यहां हम थोड़ा और अधिक औपचारिक होते हैं और प्रासंगिक गणना में गोता लगाते हैं।

4
00:00:14,920 --> 00:00:18,560
इसमें कम से कम थोड़ा भ्रमित होना सामान्य बात है, इसलिए नियमित रूप से रुककर विचार

5
00:00:18,560 --> 00:00:22,000
करने का मंत्र निश्चित रूप से यहां भी उतना ही लागू होता है जितना कहीं और।

6
00:00:22,000 --> 00:00:26,620
हमारा मुख्य लक्ष्य यह दिखाना है कि मशीन लर्निंग में लोग आमतौर पर नेटवर्क के

7
00:00:26,620 --> 00:00:31,900
संदर्भ में कैलकुलस से श्रृंखला नियम के बारे में कैसे सोचते हैं, जो कि

8
00:00:31,900 --> 00:00:34,580
अधिकांश परिचयात्मक कैलकुलस पाठ्यक्रम विषय को कैसे देखते हैं, उससे एक अलग अनुभव है।

9
00:00:34,580 --> 00:00:38,300
आपमें से जो लोग प्रासंगिक गणना से असहज हैं, उनके

10
00:00:38,300 --> 00:00:39,300
लिए मेरे पास इस विषय पर एक पूरी श्रृंखला है।

11
00:00:39,300 --> 00:00:44,840
आइए एक बेहद सरल नेटवर्क से शुरुआत करें,

12
00:00:44,840 --> 00:00:46,780
जहां प्रत्येक परत में एक न्यूरॉन होता है।

13
00:00:46,780 --> 00:00:51,880
यह नेटवर्क तीन भारों और तीन पूर्वाग्रहों द्वारा निर्धारित होता है, और हमारा लक्ष्य

14
00:00:51,880 --> 00:00:55,640
यह समझना है कि लागत फ़ंक्शन इन चरों के प्रति कितना संवेदनशील है।

15
00:00:55,640 --> 00:00:59,780
इस तरह हम जानते हैं कि उन शर्तों में कौन सा

16
00:00:59,780 --> 00:01:01,100
समायोजन लागत फ़ंक्शन में सबसे कुशल कमी का कारण बनेगा।

17
00:01:01,100 --> 00:01:05,360
हम केवल अंतिम दो न्यूरॉन्स के बीच संबंध पर ध्यान केंद्रित करेंगे।

18
00:01:05,360 --> 00:01:10,400
आइए उस अंतिम न्यूरॉन की सक्रियता को सुपरस्क्रिप्ट एल के साथ

19
00:01:10,400 --> 00:01:11,800
लेबल करें, यह दर्शाता है कि यह किस परत में है।

20
00:01:11,800 --> 00:01:16,560
तो पिछले न्यूरॉन की सक्रियता AL-1 है।

21
00:01:16,560 --> 00:01:20,120
ये प्रतिपादक नहीं हैं, हम जिस बारे में बात कर रहे हैं उसे अनुक्रमित करने का

22
00:01:20,120 --> 00:01:23,120
एक तरीका मात्र हैं, क्योंकि मैं बाद में विभिन्न सूचकांकों के लिए सबस्क्रिप्ट सहेजना चाहता हूं।

23
00:01:23,600 --> 00:01:28,880
मान लीजिए कि किसी दिए गए प्रशिक्षण उदाहरण के लिए हम इस अंतिम सक्रियण को जो

24
00:01:28,880 --> 00:01:33,020
मान चाहते हैं वह y है, उदाहरण के लिए, y 0 या 1 हो सकता है।

25
00:01:33,020 --> 00:01:39,040
तो एकल प्रशिक्षण उदाहरण के लिए इस नेटवर्क की लागत AL-y2 है।

26
00:01:39,040 --> 00:01:46,120
हम उस एक प्रशिक्षण उदाहरण की लागत को c0 के रूप में दर्शाएँगे।

27
00:01:46,120 --> 00:01:51,920
एक अनुस्मारक के रूप में, यह अंतिम सक्रियण एक वजन से निर्धारित होता है, जिसे मैं डब्ल्यूएल

28
00:01:51,920 --> 00:01:57,600
कहने जा रहा हूं, पिछले न्यूरॉन के सक्रियण का समय और कुछ पूर्वाग्रह, जिसे मैं बीएल कहूंगा।

29
00:01:57,600 --> 00:02:01,560
फिर आप उसे सिग्मॉइड या ReLU जैसे कुछ विशेष नॉनलाइनियर फ़ंक्शन के माध्यम से पंप करते हैं।

30
00:02:01,560 --> 00:02:05,400
यह वास्तव में हमारे लिए चीजों को आसान बनाने जा रहा है यदि हम इस भारित

31
00:02:05,400 --> 00:02:10,600
राशि को एक विशेष नाम देते हैं, जैसे z, प्रासंगिक सक्रियणों के समान सुपरस्क्रिप्ट के साथ।

32
00:02:10,600 --> 00:02:15,320
यह बहुत सारे शब्द हैं, और जिस तरह से आप इसकी संकल्पना कर सकते हैं वह यह है कि वजन, पिछली

33
00:02:15,320 --> 00:02:21,800
कार्रवाई और पूर्वाग्रह सभी को एक साथ z की गणना करने के लिए उपयोग किया जाता है, जो बदले में हमें

34
00:02:21,800 --> 00:02:27,360
a की गणना करने देता है, जो अंततः, एक निरंतर y के साथ, देता है हम लागत की गणना करते हैं।

35
00:02:27,360 --> 00:02:33,440
और निश्चित रूप से, AL-1 अपने स्वयं के वजन और पूर्वाग्रह आदि से प्रभावित

36
00:02:33,440 --> 00:02:35,920
होता है, लेकिन हम अभी उस पर ध्यान केंद्रित नहीं करने जा रहे हैं।

37
00:02:35,920 --> 00:02:38,120
ये सभी केवल संख्याएँ हैं, है ना?

38
00:02:38,120 --> 00:02:41,960
और यह सोचना अच्छा हो सकता है कि प्रत्येक की अपनी छोटी संख्या रेखा है।

39
00:02:41,960 --> 00:02:47,480
हमारा पहला लक्ष्य यह समझना है कि लागत फ़ंक्शन हमारे

40
00:02:47,480 --> 00:02:49,820
वजन डब्ल्यूएल में छोटे बदलावों के प्रति कितना संवेदनशील है।

41
00:02:49,820 --> 00:02:55,740
या वाक्यांश अलग ढंग से, wL के संबंध में c का व्युत्पन्न क्या है?

42
00:02:55,740 --> 00:03:01,220
जब आप इस डेल डब्ल्यू शब्द को देखते हैं, तो इसे डब्ल्यू के लिए कुछ छोटे संकेत के रूप में सोचें, जैसे कि 0 से

43
00:03:01,220 --> 00:03:08,820
बदलाव। 01, और इस डेल सी शब्द को इस अर्थ के रूप में सोचें कि लागत पर परिणामी प्रभाव कुछ भी हो।

44
00:03:08,820 --> 00:03:10,900
हम जो चाहते हैं वह उनका अनुपात है।

45
00:03:10,900 --> 00:03:17,740
वैचारिक रूप से, डब्ल्यूएल के लिए यह छोटा सा धक्का जेडएल के लिए कुछ दबाव का कारण बनता है,

46
00:03:17,740 --> 00:03:23,220
जो बदले में एएल के लिए कुछ दबाव का कारण बनता है, जो सीधे लागत को प्रभावित करता है।

47
00:03:23,220 --> 00:03:28,020
इसलिए हम सबसे पहले zL में एक छोटे परिवर्तन और इस छोटे परिवर्तन w के

48
00:03:28,020 --> 00:03:33,340
अनुपात को देखकर चीजों को तोड़ते हैं, यानी, wL के संबंध में zL का व्युत्पन्न।

49
00:03:33,340 --> 00:03:38,820
इसी तरह, फिर आप AL में परिवर्तन और zL में उस छोटे परिवर्तन के

50
00:03:38,820 --> 00:03:43,900
अनुपात पर विचार करते हैं जिसके कारण यह हुआ, साथ ही अंतिम नज से

51
00:03:43,900 --> 00:03:45,900
c और इस मध्यवर्ती नज से AL के बीच के अनुपात पर विचार करें।

52
00:03:45,900 --> 00:03:51,880
यहीं श्रृंखला नियम है, जहां इन तीन अनुपातों को गुणा करने से

53
00:03:51,880 --> 00:03:57,340
हमें डब्ल्यूएल में छोटे बदलावों के प्रति सी की संवेदनशीलता मिलती है।

54
00:03:57,340 --> 00:04:01,620
तो अभी स्क्रीन पर, बहुत सारे प्रतीक हैं, और यह सुनिश्चित करने के लिए थोड़ा समय लें कि यह

55
00:04:01,620 --> 00:04:07,460
स्पष्ट है कि वे सभी क्या हैं, क्योंकि अब हम प्रासंगिक डेरिवेटिव की गणना करने जा रहे हैं।

56
00:04:07,460 --> 00:04:14,220
AL के संबंध में c का व्युत्पन्न 2AL-y होता है।

57
00:04:14,220 --> 00:04:19,300
इसका मतलब यह है कि इसका आकार नेटवर्क के आउटपुट और जिस चीज को हम

58
00:04:19,300 --> 00:04:24,480
चाहते हैं, उसके बीच अंतर के समानुपाती होता है, इसलिए यदि वह आउटपुट बहुत

59
00:04:24,480 --> 00:04:28,380
अलग था, तो मामूली बदलाव भी अंतिम लागत फ़ंक्शन पर बड़ा प्रभाव डालते हैं।

60
00:04:28,380 --> 00:04:33,860
ZL के संबंध में AL का व्युत्पन्न हमारे सिग्मॉइड फ़ंक्शन का

61
00:04:33,860 --> 00:04:37,420
व्युत्पन्न है, या जो भी गैर-रैखिकता आप उपयोग करना चुनते हैं।

62
00:04:37,420 --> 00:04:46,180
wL के संबंध में zL का व्युत्पन्न AL-1 निकलता है।

63
00:04:46,180 --> 00:04:49,460
मैं आपके बारे में नहीं जानता, लेकिन मुझे लगता है कि बिना एक पल भी आराम से बैठे

64
00:04:49,460 --> 00:04:54,180
और खुद को याद दिलाए कि उन सभी का क्या मतलब है, सूत्रों में फंस जाना आसान है।

65
00:04:54,180 --> 00:04:58,860
इस अंतिम व्युत्पन्न के मामले में, वजन की छोटी सी हलचल ने अंतिम परत को कितना

66
00:04:58,860 --> 00:05:03,220
प्रभावित किया, यह इस बात पर निर्भर करता है कि पिछला न्यूरॉन कितना मजबूत है।

67
00:05:03,220 --> 00:05:09,320
याद रखें, यह वह जगह है जहां न्यूरॉन्स-वह-फायर-टुगेदर-वायर-टुगेदर विचार आता है।

68
00:05:09,320 --> 00:05:14,840
और यह सब केवल एक विशिष्ट एकल प्रशिक्षण उदाहरण के

69
00:05:14,840 --> 00:05:16,580
लिए लागत के डब्ल्यूएल के संबंध में व्युत्पन्न है।

70
00:05:16,580 --> 00:05:20,940
चूँकि पूर्ण लागत फ़ंक्शन में कई अलग-अलग प्रशिक्षण उदाहरणों में उन सभी लागतों

71
00:05:20,940 --> 00:05:27,300
का एक साथ औसत शामिल होता है, इसलिए इसके व्युत्पन्न के लिए

72
00:05:27,300 --> 00:05:28,540
सभी प्रशिक्षण उदाहरणों पर इस अभिव्यक्ति के औसत की आवश्यकता होती है।

73
00:05:28,540 --> 00:05:33,860
बेशक, यह ग्रेडिएंट वेक्टर का सिर्फ एक घटक है, जो उन सभी भारों और

74
00:05:33,860 --> 00:05:40,780
पूर्वाग्रहों के संबंध में लागत फ़ंक्शन के आंशिक डेरिवेटिव से बनाया गया है।

75
00:05:40,780 --> 00:05:44,340
लेकिन भले ही यह हमारे लिए आवश्यक कई आंशिक डेरिवेटिवों में

76
00:05:44,340 --> 00:05:46,460
से एक है, यह काम का 50% से अधिक है।

77
00:05:46,460 --> 00:05:50,300
उदाहरण के लिए, पूर्वाग्रह के प्रति संवेदनशीलता लगभग समान है।

78
00:05:50,300 --> 00:05:58,980
हमें बस इस डेल ज़ेड डेल डब्ल्यू शब्द को ए डेल ज़ेड डेल बी के लिए बदलने की जरूरत है।

79
00:05:58,980 --> 00:06:04,700
और यदि आप प्रासंगिक सूत्र को देखें, तो वह व्युत्पन्न 1 निकलता है।

80
00:06:04,700 --> 00:06:11,700
इसके अलावा, और यहीं पर पीछे की ओर प्रचार करने का विचार आता है, आप देख

81
00:06:11,700 --> 00:06:16,180
सकते हैं कि यह लागत फ़ंक्शन पिछली परत की सक्रियता के प्रति कितना संवेदनशील है।

82
00:06:16,180 --> 00:06:21,380
अर्थात्, श्रृंखला नियम अभिव्यक्ति में यह प्रारंभिक व्युत्पन्न, पिछले सक्रियण के लिए

83
00:06:21,380 --> 00:06:25,420
z की संवेदनशीलता, वजन wL के रूप में सामने आती है।

84
00:06:25,420 --> 00:06:30,100
और फिर, भले ही हम उस पिछली परत सक्रियण को सीधे प्रभावित करने में

85
00:06:30,100 --> 00:06:35,280
सक्षम नहीं होंगे, लेकिन इसका ट्रैक रखना उपयोगी है, क्योंकि अब हम केवल

86
00:06:35,280 --> 00:06:40,780
उसी श्रृंखला नियम विचार को पीछे की ओर दोहराते रह सकते हैं यह देखने

87
00:06:40,780 --> 00:06:43,680
के लिए कि लागत फ़ंक्शन कितना संवेदनशील है पिछले भार और पिछले पूर्वाग्रह।

88
00:06:43,680 --> 00:06:47,940
और आप सोच सकते हैं कि यह एक अत्यधिक सरल उदाहरण है, क्योंकि सभी परतों में एक

89
00:06:47,940 --> 00:06:51,320
न्यूरॉन होता है, और वास्तविक नेटवर्क के लिए चीजें तेजी से अधिक जटिल होती जा रही हैं।

90
00:06:51,320 --> 00:06:56,560
लेकिन ईमानदारी से कहूं तो, जब हम परतों को कई न्यूरॉन्स देते हैं तो उतना बदलाव

91
00:06:56,560 --> 00:06:59,320
नहीं होता है, वास्तव में यह ट्रैक रखने के लिए बस कुछ और सूचकांक हैं।

92
00:06:59,320 --> 00:07:03,580
किसी दी गई परत के केवल AL सक्रिय होने के बजाय, इसमें एक सबस्क्रिप्ट

93
00:07:03,580 --> 00:07:07,920
भी होगी जो यह बताएगी कि यह उस परत का कौन सा न्यूरॉन है।

94
00:07:07,920 --> 00:07:15,280
आइए परत L-1 को अनुक्रमित करने के लिए अक्षर k का उपयोग करें, और परत L को अनुक्रमित करने के लिए j अक्षर का उपयोग करें।

95
00:07:15,280 --> 00:07:20,720
लागत के लिए, हम फिर से देखते हैं कि वांछित आउटपुट क्या है, लेकिन इस बार

96
00:07:20,720 --> 00:07:26,120
हम इन अंतिम परत सक्रियणों और वांछित आउटपुट के बीच अंतर के वर्गों को जोड़ते हैं।

97
00:07:26,120 --> 00:07:33,280
अर्थात्, आप ALj घटा yj वर्ग से अधिक राशि लेते हैं।

98
00:07:33,280 --> 00:07:36,500
चूँकि बहुत अधिक वजन है, प्रत्येक को यह पता लगाने के लिए कि

99
00:07:36,500 --> 00:07:41,380
वह कहाँ है, कुछ और सूचकांक रखने होंगे, तो चलिए इस kth न्यूरॉन

100
00:07:41,380 --> 00:07:45,740
को jth न्यूरॉन से जोड़ने वाले किनारे के वजन को WLjk कहते हैं।

101
00:07:45,740 --> 00:07:49,820
वे सूचकांक पहले थोड़ा पीछे की ओर लग सकते हैं, लेकिन यह इस बात से मेल खाता है कि

102
00:07:49,820 --> 00:07:53,800
आप उस वजन मैट्रिक्स को कैसे अनुक्रमित करेंगे जिसके बारे में मैंने भाग 1 वीडियो में बात की थी।

103
00:07:53,800 --> 00:07:57,660
पहले की तरह, प्रासंगिक भारित योग को z जैसा नाम

104
00:07:57,660 --> 00:08:03,540
देना अभी भी अच्छा है, ताकि अंतिम परत का सक्रियण

105
00:08:03,540 --> 00:08:04,980
सिर्फ आपका विशेष कार्य हो, जैसे z पर लागू सिग्मॉइड।

106
00:08:04,980 --> 00:08:09,100
आप देख सकते हैं कि मेरा क्या मतलब है, जहां ये सभी अनिवार्य रूप से वही समीकरण हैं जो

107
00:08:09,100 --> 00:08:15,420
हमारे पास पहले एक-न्यूरॉन-प्रति-परत मामले में थे, यह सिर्फ इतना है कि यह थोड़ा अधिक जटिल दिखता है।

108
00:08:15,420 --> 00:08:20,620
और वास्तव में, श्रृंखला नियम व्युत्पन्न अभिव्यक्ति यह बताती है कि किसी

109
00:08:20,620 --> 00:08:23,540
विशिष्ट भार के प्रति लागत कितनी संवेदनशील है, मूलतः वही दिखती है।

110
00:08:23,540 --> 00:08:29,420
यदि आप चाहें तो मैं इसे आप पर छोड़ता हूँ कि आप रुकें और उनमें से प्रत्येक शब्द के बारे में सोचें।

111
00:08:29,420 --> 00:08:34,900
हालाँकि, यहाँ जो परिवर्तन होता है, वह परत L-1 में सक्रियणों

112
00:08:34,900 --> 00:08:37,820
में से एक के संबंध में लागत का व्युत्पन्न है।

113
00:08:37,820 --> 00:08:42,000
इस मामले में, अंतर यह है कि न्यूरॉन कई

114
00:08:42,000 --> 00:08:43,540
अलग-अलग रास्तों से लागत फ़ंक्शन को प्रभावित करता है।

115
00:08:43,540 --> 00:08:51,200
यानी, एक ओर, यह AL0 को प्रभावित करता है, जो लागत फ़ंक्शन में

116
00:08:51,200 --> 00:08:56,460
भूमिका निभाता है, लेकिन इसका AL1 पर भी प्रभाव पड़ता है, जो

117
00:08:56,460 --> 00:09:00,340
लागत फ़ंक्शन में भी भूमिका निभाता है, और आपको उन्हें जोड़ना होगा।

118
00:09:00,340 --> 00:09:03,680
और वह, ठीक है, बस इतना ही।

119
00:09:03,680 --> 00:09:08,240
एक बार जब आप जान जाते हैं कि लागत फ़ंक्शन इस दूसरी-से-अंतिम परत

120
00:09:08,240 --> 00:09:12,520
में सक्रियणों के प्रति कितना संवेदनशील है, तो आप उस परत में आने

121
00:09:12,520 --> 00:09:13,920
वाले सभी भार और पूर्वाग्रहों के लिए प्रक्रिया को दोहरा सकते हैं।

122
00:09:13,920 --> 00:09:15,420
तो अपनी पीठ थपथपाओ!

123
00:09:15,420 --> 00:09:20,480
यदि यह सब समझ में आता है, तो अब आपने बैकप्रॉपैगेशन के मूल में

124
00:09:20,480 --> 00:09:23,700
गहराई से देखा है, तंत्रिका नेटवर्क कैसे सीखते हैं इसके पीछे का कामगार।

125
00:09:23,700 --> 00:09:27,960
ये श्रृंखला नियम अभिव्यक्ति आपको डेरिवेटिव देते हैं जो ग्रेडिएंट में प्रत्येक घटक को निर्धारित करते हैं

126
00:09:27,960 --> 00:09:35,020
जो बार-बार नीचे की ओर कदम बढ़ाकर नेटवर्क की लागत को कम करने में मदद करता है।

127
00:09:35,020 --> 00:09:38,960
यदि आप आराम से बैठते हैं और उस सब के बारे में सोचते हैं, तो यह आपके दिमाग को घेरने वाली

128
00:09:38,960 --> 00:09:42,840
जटिलता की बहुत सारी परतें हैं, इसलिए चिंता न करें अगर आपके दिमाग को यह सब पचाने में समय लगता है।

