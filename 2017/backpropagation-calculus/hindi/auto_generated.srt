1
00:00:04,020 --> 00:00:06,735
यहां कठिन धारणा यह है कि आपने भाग 3 देखा है, जिसमें 

2
00:00:06,735 --> 00:00:09,920
बैकप्रोपेगेशन एल्गोरिदम का सहज ज्ञान युक्त विवरण दिया गया है।

3
00:00:11,040 --> 00:00:14,220
यहां हम थोड़ा और अधिक औपचारिक होते हैं और प्रासंगिक गणना में गोता लगाते हैं।

4
00:00:14,820 --> 00:00:16,927
इसमें कम से कम थोड़ा भ्रमित होना सामान्य बात है, 

5
00:00:16,927 --> 00:00:20,238
इसलिए नियमित रूप से रुककर विचार करने का मंत्र निश्चित रूप से यहां भी उतना ही 

6
00:00:20,238 --> 00:00:21,400
लागू होता है जितना कहीं और।

7
00:00:21,940 --> 00:00:25,840
हमारा मुख्य लक्ष्य यह दिखाना है कि मशीन लर्निंग में लोग आमतौर पर नेटवर्क के 

8
00:00:25,840 --> 00:00:29,124
संदर्भ में कैलकुलस से श्रृंखला नियम के बारे में कैसे सोचते हैं, 

9
00:00:29,124 --> 00:00:33,640
जो कि अधिकांश परिचयात्मक कैलकुलस पाठ्यक्रम विषय को कैसे देखते हैं, उससे एक अलग अनुभव है।

10
00:00:34,340 --> 00:00:36,658
आपमें से जो लोग प्रासंगिक गणना से असहज हैं, उनके 

11
00:00:36,658 --> 00:00:38,740
लिए मेरे पास इस विषय पर एक पूरी श्रृंखला है।

12
00:00:39,960 --> 00:00:46,020
आइए एक बेहद सरल नेटवर्क से शुरुआत करें, जहां प्रत्येक परत में एक न्यूरॉन होता है।

13
00:00:46,320 --> 00:00:50,221
यह नेटवर्क तीन भारों और तीन पूर्वाग्रहों द्वारा निर्धारित होता है, 

14
00:00:50,221 --> 00:00:54,880
और हमारा लक्ष्य यह समझना है कि लागत फ़ंक्शन इन चरों के प्रति कितना संवेदनशील है।

15
00:00:55,419 --> 00:00:59,118
इस तरह हम जानते हैं कि उन शर्तों में कौन सा समायोजन 

16
00:00:59,118 --> 00:01:02,320
लागत फ़ंक्शन में सबसे कुशल कमी का कारण बनेगा।

17
00:01:02,320 --> 00:01:04,840
हम केवल अंतिम दो न्यूरॉन्स के बीच संबंध पर ध्यान केंद्रित करेंगे।

18
00:01:05,980 --> 00:01:09,583
आइए उस अंतिम न्यूरॉन की सक्रियता को सुपरस्क्रिप्ट एल के साथ लेबल करें, 

19
00:01:09,583 --> 00:01:11,360
यह दर्शाता है कि यह किस परत में है।

20
00:01:11,680 --> 00:01:15,560
तो पिछले न्यूरॉन की सक्रियता AL-1 है।

21
00:01:16,360 --> 00:01:19,700
ये प्रतिपादक नहीं हैं, हम जिस बारे में बात कर रहे हैं उसे अनुक्रमित करने का एक तरीका 

22
00:01:19,700 --> 00:01:23,040
मात्र हैं, क्योंकि मैं बाद में विभिन्न सूचकांकों के लिए सबस्क्रिप्ट सहेजना चाहता हूं।

23
00:01:23,720 --> 00:01:28,109
मान लीजिए कि किसी दिए गए प्रशिक्षण उदाहरण के लिए हम इस अंतिम सक्रियण 

24
00:01:28,109 --> 00:01:32,180
को जो मान चाहते हैं वह y है, उदाहरण के लिए, y 0 या 1 हो सकता है।

25
00:01:32,840 --> 00:01:39,240
तो एकल प्रशिक्षण उदाहरण के लिए इस नेटवर्क की लागत AL-y2 है।

26
00:01:40,260 --> 00:01:44,380
हम उस एक प्रशिक्षण उदाहरण की लागत को c0 के रूप में दर्शाएँगे।

27
00:01:45,900 --> 00:01:50,540
एक अनुस्मारक के रूप में, यह अंतिम सक्रियण एक वजन से निर्धारित होता है, 

28
00:01:50,540 --> 00:01:56,227
जिसे मैं डब्ल्यूएल कहने जा रहा हूं, पिछले न्यूरॉन के सक्रियण का समय और कुछ पूर्वाग्रह, 

29
00:01:56,227 --> 00:01:57,600
जिसे मैं बीएल कहूंगा।

30
00:01:57,600 --> 00:02:01,320
फिर आप उसे सिग्मॉइड या ReLU जैसे कुछ विशेष नॉनलाइनियर फ़ंक्शन के माध्यम से पंप करते हैं।

31
00:02:01,800 --> 00:02:05,512
यह वास्तव में हमारे लिए चीजों को आसान बनाने जा रहा है यदि हम इस भारित राशि को 

32
00:02:05,512 --> 00:02:09,320
एक विशेष नाम देते हैं, जैसे z, प्रासंगिक सक्रियणों के समान सुपरस्क्रिप्ट के साथ।

33
00:02:10,380 --> 00:02:14,987
यह बहुत सारे शब्द हैं, और जिस तरह से आप इसकी संकल्पना कर सकते हैं वह यह है कि वजन, 

34
00:02:14,987 --> 00:02:19,706
पिछली कार्रवाई और पूर्वाग्रह सभी को एक साथ z की गणना करने के लिए उपयोग किया जाता है, 

35
00:02:19,706 --> 00:02:23,648
जो बदले में हमें a की गणना करने देता है, जो अंततः, एक निरंतर y के साथ, 

36
00:02:23,648 --> 00:02:25,480
देता है हम लागत की गणना करते हैं।

37
00:02:27,340 --> 00:02:31,937
और निश्चित रूप से, AL-1 अपने स्वयं के वजन और पूर्वाग्रह आदि से प्रभावित होता है, 

38
00:02:31,937 --> 00:02:35,060
लेकिन हम अभी उस पर ध्यान केंद्रित नहीं करने जा रहे हैं।

39
00:02:35,700 --> 00:02:37,620
ये सभी केवल संख्याएँ हैं, है ना?

40
00:02:38,060 --> 00:02:41,040
और यह सोचना अच्छा हो सकता है कि प्रत्येक की अपनी छोटी संख्या रेखा है।

41
00:02:41,720 --> 00:02:45,392
हमारा पहला लक्ष्य यह समझना है कि लागत फ़ंक्शन हमारे वजन 

42
00:02:45,392 --> 00:02:49,000
डब्ल्यूएल में छोटे बदलावों के प्रति कितना संवेदनशील है।

43
00:02:49,540 --> 00:02:54,860
या वाक्यांश अलग ढंग से, wL के संबंध में c का व्युत्पन्न क्या है?

44
00:02:55,600 --> 00:02:59,711
जब आप इस डेल डब्ल्यू शब्द को देखते हैं, तो इसे डब्ल्यू के लिए कुछ 

45
00:02:59,711 --> 00:03:02,951
छोटे संकेत के रूप में सोचें, जैसे कि 0 से बदलाव।01, 

46
00:03:02,951 --> 00:03:08,060
और इस डेल सी शब्द को इस अर्थ के रूप में सोचें कि लागत पर परिणामी प्रभाव कुछ भी हो।

47
00:03:08,060 --> 00:03:10,220
हम जो चाहते हैं वह उनका अनुपात है।

48
00:03:11,260 --> 00:03:16,454
वैचारिक रूप से, डब्ल्यूएल के लिए यह छोटा सा धक्का जेडएल के लिए कुछ दबाव का कारण बनता है, 

49
00:03:16,454 --> 00:03:21,240
जो बदले में एएल के लिए कुछ दबाव का कारण बनता है, जो सीधे लागत को प्रभावित करता है।

50
00:03:23,120 --> 00:03:28,370
इसलिए हम सबसे पहले zL में एक छोटे परिवर्तन और इस छोटे परिवर्तन w के अनुपात 

51
00:03:28,370 --> 00:03:33,200
को देखकर चीजों को तोड़ते हैं, यानी, wL के संबंध में zL का व्युत्पन्न।

52
00:03:33,200 --> 00:03:37,082
इसी तरह, फिर आप AL में परिवर्तन और zL में उस छोटे परिवर्तन के 

53
00:03:37,082 --> 00:03:40,839
अनुपात पर विचार करते हैं जिसके कारण यह हुआ, साथ ही अंतिम नज 

54
00:03:40,839 --> 00:03:44,660
से c और इस मध्यवर्ती नज से AL के बीच के अनुपात पर विचार करें।

55
00:03:45,740 --> 00:03:50,513
यहीं श्रृंखला नियम है, जहां इन तीन अनुपातों को गुणा करने से हमें 

56
00:03:50,513 --> 00:03:55,140
डब्ल्यूएल में छोटे बदलावों के प्रति सी की संवेदनशीलता मिलती है।

57
00:03:56,880 --> 00:04:01,586
तो अभी स्क्रीन पर, बहुत सारे प्रतीक हैं, और यह सुनिश्चित करने के लिए थोड़ा समय लें कि यह 

58
00:04:01,586 --> 00:04:06,240
स्पष्ट है कि वे सभी क्या हैं, क्योंकि अब हम प्रासंगिक डेरिवेटिव की गणना करने जा रहे हैं।

59
00:04:07,440 --> 00:04:14,180
AL के संबंध में c का व्युत्पन्न 2AL-y होता है।

60
00:04:14,180 --> 00:04:18,875
इसका मतलब यह है कि इसका आकार नेटवर्क के आउटपुट और जिस चीज को हम चाहते हैं, 

61
00:04:18,875 --> 00:04:23,195
उसके बीच अंतर के समानुपाती होता है, इसलिए यदि वह आउटपुट बहुत अलग था, 

62
00:04:23,195 --> 00:04:27,140
तो मामूली बदलाव भी अंतिम लागत फ़ंक्शन पर बड़ा प्रभाव डालते हैं।

63
00:04:27,840 --> 00:04:33,797
ZL के संबंध में AL का व्युत्पन्न हमारे सिग्मॉइड फ़ंक्शन का व्युत्पन्न है, 

64
00:04:33,797 --> 00:04:37,420
या जो भी गैर-रैखिकता आप उपयोग करना चुनते हैं।

65
00:04:37,420 --> 00:04:46,160
wL के संबंध में zL का व्युत्पन्न AL-1 निकलता है।

66
00:04:46,160 --> 00:04:49,860
मैं आपके बारे में नहीं जानता, लेकिन मुझे लगता है कि बिना एक पल भी आराम से बैठे 

67
00:04:49,860 --> 00:04:53,420
और खुद को याद दिलाए कि उन सभी का क्या मतलब है, सूत्रों में फंस जाना आसान है।

68
00:04:53,920 --> 00:04:58,399
इस अंतिम व्युत्पन्न के मामले में, वजन की छोटी सी हलचल ने अंतिम परत को कितना 

69
00:04:58,399 --> 00:05:02,820
प्रभावित किया, यह इस बात पर निर्भर करता है कि पिछला न्यूरॉन कितना मजबूत है।

70
00:05:03,380 --> 00:05:08,280
याद रखें, यह वह जगह है जहां न्यूरॉन्स-वह-फायर-टुगेदर-वायर-टुगेदर विचार आता है।

71
00:05:09,200 --> 00:05:12,460
और यह सब केवल एक विशिष्ट एकल प्रशिक्षण उदाहरण के 

72
00:05:12,460 --> 00:05:15,720
लिए लागत के डब्ल्यूएल के संबंध में व्युत्पन्न है।

73
00:05:16,440 --> 00:05:20,432
चूँकि पूर्ण लागत फ़ंक्शन में कई अलग-अलग प्रशिक्षण उदाहरणों में उन 

74
00:05:20,432 --> 00:05:24,364
सभी लागतों का एक साथ औसत शामिल होता है, इसलिए इसके व्युत्पन्न के 

75
00:05:24,364 --> 00:05:28,660
लिए सभी प्रशिक्षण उदाहरणों पर इस अभिव्यक्ति के औसत की आवश्यकता होती है।

76
00:05:28,660 --> 00:05:33,149
बेशक, यह ग्रेडिएंट वेक्टर का सिर्फ एक घटक है, जो उन सभी भारों और 

77
00:05:33,149 --> 00:05:38,260
पूर्वाग्रहों के संबंध में लागत फ़ंक्शन के आंशिक डेरिवेटिव से बनाया गया है।

78
00:05:40,640 --> 00:05:44,018
लेकिन भले ही यह हमारे लिए आवश्यक कई आंशिक डेरिवेटिवों में से एक है, 

79
00:05:44,018 --> 00:05:45,260
यह काम का 50% से अधिक है।

80
00:05:46,340 --> 00:05:49,720
उदाहरण के लिए, पूर्वाग्रह के प्रति संवेदनशीलता लगभग समान है।

81
00:05:50,040 --> 00:05:55,020
हमें बस इस डेल ज़ेड डेल डब्ल्यू शब्द को ए डेल ज़ेड डेल बी के लिए बदलने की जरूरत है।

82
00:05:58,420 --> 00:06:02,400
और यदि आप प्रासंगिक सूत्र को देखें, तो वह व्युत्पन्न 1 निकलता है।

83
00:06:06,140 --> 00:06:10,226
इसके अलावा, और यहीं पर पीछे की ओर प्रचार करने का विचार आता है, 

84
00:06:10,226 --> 00:06:15,740
आप देख सकते हैं कि यह लागत फ़ंक्शन पिछली परत की सक्रियता के प्रति कितना संवेदनशील है।

85
00:06:15,740 --> 00:06:20,438
अर्थात्, श्रृंखला नियम अभिव्यक्ति में यह प्रारंभिक व्युत्पन्न, 

86
00:06:20,438 --> 00:06:25,660
पिछले सक्रियण के लिए z की संवेदनशीलता, वजन wL के रूप में सामने आती है।

87
00:06:26,640 --> 00:06:31,426
और फिर, भले ही हम उस पिछली परत सक्रियण को सीधे प्रभावित करने में सक्षम नहीं होंगे, 

88
00:06:31,426 --> 00:06:35,462
लेकिन इसका ट्रैक रखना उपयोगी है, क्योंकि अब हम केवल उसी श्रृंखला नियम 

89
00:06:35,462 --> 00:06:39,153
विचार को पीछे की ओर दोहराते रह सकते हैं यह देखने के लिए कि लागत 

90
00:06:39,153 --> 00:06:42,440
फ़ंक्शन कितना संवेदनशील है पिछले भार और पिछले पूर्वाग्रह।

91
00:06:43,180 --> 00:06:45,679
और आप सोच सकते हैं कि यह एक अत्यधिक सरल उदाहरण है, 

92
00:06:45,679 --> 00:06:49,598
क्योंकि सभी परतों में एक न्यूरॉन होता है, और वास्तविक नेटवर्क के लिए चीजें तेजी 

93
00:06:49,598 --> 00:06:51,020
से अधिक जटिल होती जा रही हैं।

94
00:06:51,700 --> 00:06:55,230
लेकिन ईमानदारी से कहूं तो, जब हम परतों को कई न्यूरॉन्स देते हैं तो उतना 

95
00:06:55,230 --> 00:06:58,860
बदलाव नहीं होता है, वास्तव में यह ट्रैक रखने के लिए बस कुछ और सूचकांक हैं।

96
00:06:59,380 --> 00:07:02,352
किसी दी गई परत के केवल AL सक्रिय होने के बजाय, 

97
00:07:02,352 --> 00:07:07,160
इसमें एक सबस्क्रिप्ट भी होगी जो यह बताएगी कि यह उस परत का कौन सा न्यूरॉन है।

98
00:07:07,160 --> 00:07:10,915
आइए परत L-1 को अनुक्रमित करने के लिए अक्षर k का उपयोग करें, 

99
00:07:10,915 --> 00:07:14,420
और परत L को अनुक्रमित करने के लिए j अक्षर का उपयोग करें।

100
00:07:15,260 --> 00:07:19,136
लागत के लिए, हम फिर से देखते हैं कि वांछित आउटपुट क्या है, 

101
00:07:19,136 --> 00:07:24,260
लेकिन इस बार हम इन अंतिम परत सक्रियणों और वांछित आउटपुट के बीच अंतर के वर्गों 

102
00:07:24,260 --> 00:07:25,180
को जोड़ते हैं।

103
00:07:26,080 --> 00:07:30,840
अर्थात्, आप ALj घटा yj वर्ग से अधिक राशि लेते हैं।

104
00:07:33,040 --> 00:07:37,674
चूँकि बहुत अधिक वजन है, प्रत्येक को यह पता लगाने के लिए कि वह कहाँ है, 

105
00:07:37,674 --> 00:07:41,395
कुछ और सूचकांक रखने होंगे, तो चलिए इस kth न्यूरॉन को jth 

106
00:07:41,395 --> 00:07:44,920
न्यूरॉन से जोड़ने वाले किनारे के वजन को WLjk कहते हैं।

107
00:07:45,620 --> 00:07:49,348
वे सूचकांक पहले थोड़ा पीछे की ओर लग सकते हैं, लेकिन यह इस बात से मेल खाता है कि आप उस 

108
00:07:49,348 --> 00:07:53,120
वजन मैट्रिक्स को कैसे अनुक्रमित करेंगे जिसके बारे में मैंने भाग 1 वीडियो में बात की थी।

109
00:07:53,620 --> 00:07:58,601
पहले की तरह, प्रासंगिक भारित योग को z जैसा नाम देना अभी भी अच्छा है, 

110
00:07:58,601 --> 00:08:04,160
ताकि अंतिम परत का सक्रियण सिर्फ आपका विशेष कार्य हो, जैसे z पर लागू सिग्मॉइड।

111
00:08:04,660 --> 00:08:07,584
आप देख सकते हैं कि मेरा क्या मतलब है, जहां ये सभी अनिवार्य 

112
00:08:07,584 --> 00:08:11,301
रूप से वही समीकरण हैं जो हमारे पास पहले एक-न्यूरॉन-प्रति-परत मामले में थे, 

113
00:08:11,301 --> 00:08:13,680
यह सिर्फ इतना है कि यह थोड़ा अधिक जटिल दिखता है।

114
00:08:15,440 --> 00:08:19,312
और वास्तव में, श्रृंखला नियम व्युत्पन्न अभिव्यक्ति यह बताती है कि 

115
00:08:19,312 --> 00:08:23,420
किसी विशिष्ट भार के प्रति लागत कितनी संवेदनशील है, मूलतः वही दिखती है।

116
00:08:23,920 --> 00:08:25,334
यदि आप चाहें तो मैं इसे आप पर छोड़ता हूँ कि आप 

117
00:08:25,334 --> 00:08:26,840
रुकें और उनमें से प्रत्येक शब्द के बारे में सोचें।

118
00:08:28,979 --> 00:08:32,637
हालाँकि, यहाँ जो परिवर्तन होता है, वह परत L-1 में 

119
00:08:32,637 --> 00:08:36,659
सक्रियणों में से एक के संबंध में लागत का व्युत्पन्न है।

120
00:08:37,780 --> 00:08:40,414
इस मामले में, अंतर यह है कि न्यूरॉन कई अलग-अलग 

121
00:08:40,414 --> 00:08:42,880
रास्तों से लागत फ़ंक्शन को प्रभावित करता है।

122
00:08:44,680 --> 00:08:50,171
यानी, एक ओर, यह AL0 को प्रभावित करता है, जो लागत फ़ंक्शन में भूमिका निभाता है, 

123
00:08:50,171 --> 00:08:55,663
लेकिन इसका AL1 पर भी प्रभाव पड़ता है, जो लागत फ़ंक्शन में भी भूमिका निभाता है, 

124
00:08:55,663 --> 00:08:57,540
और आपको उन्हें जोड़ना होगा।

125
00:08:59,820 --> 00:09:03,040
और वह, ठीक है, बस इतना ही।

126
00:09:03,500 --> 00:09:06,620
एक बार जब आप जान जाते हैं कि लागत फ़ंक्शन इस दूसरी-से-अंतिम परत 

127
00:09:06,620 --> 00:09:09,740
में सक्रियणों के प्रति कितना संवेदनशील है, तो आप उस परत में आने 

128
00:09:09,740 --> 00:09:12,860
वाले सभी भार और पूर्वाग्रहों के लिए प्रक्रिया को दोहरा सकते हैं।

129
00:09:13,900 --> 00:09:14,960
तो अपनी पीठ थपथपाओ!

130
00:09:15,300 --> 00:09:19,772
यदि यह सब समझ में आता है, तो अब आपने बैकप्रॉपैगेशन के मूल में गहराई से देखा है, 

131
00:09:19,772 --> 00:09:22,680
तंत्रिका नेटवर्क कैसे सीखते हैं इसके पीछे का कामगार।

132
00:09:23,300 --> 00:09:28,022
ये श्रृंखला नियम अभिव्यक्ति आपको डेरिवेटिव देते हैं जो ग्रेडिएंट में प्रत्येक घटक को 

133
00:09:28,022 --> 00:09:32,855
निर्धारित करते हैं जो बार-बार नीचे की ओर कदम बढ़ाकर नेटवर्क की लागत को कम करने में मदद 

134
00:09:32,855 --> 00:09:33,300
करता है।

135
00:09:34,300 --> 00:09:36,914
यदि आप आराम से बैठते हैं और उस सब के बारे में सोचते हैं, 

136
00:09:36,914 --> 00:09:39,758
तो यह आपके दिमाग को घेरने वाली जटिलता की बहुत सारी परतें हैं, 

137
00:09:39,758 --> 00:09:42,740
इसलिए चिंता न करें अगर आपके दिमाग को यह सब पचाने में समय लगता है।

