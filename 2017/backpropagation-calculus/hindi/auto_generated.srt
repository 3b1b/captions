1
00:00:00,000 --> 00:00:05,135
यहां कठिन धारणा यह है कि आपने भाग 3 देखा है, जिसमें

2
00:00:05,135 --> 00:00:11,160
बैकप्रोपेगेशन एल्गोरिदम का सहज ज्ञान युक्त विवरण दिया गया है।

3
00:00:11,160 --> 00:00:14,920
यहां हम थोड़ा और अधिक औपचारिक होते हैं और प्रासंगिक गणना में गोता लगाते हैं।

4
00:00:14,920 --> 00:00:18,390
इसमें कम से कम थोड़ा भ्रमित होना सामान्य बात है, इसलिए नियमित रूप से रुककर

5
00:00:18,390 --> 00:00:22,000
विचार करने का मंत्र निश्चित रूप से यहां भी उतना ही लागू होता है जितना कहीं और।

6
00:00:22,000 --> 00:00:26,193
हमारा मुख्य लक्ष्य यह दिखाना है कि मशीन लर्निंग में लोग आमतौर पर नेटवर्क के

7
00:00:26,193 --> 00:00:30,497
संदर्भ में कैलकुलस से श्रृंखला नियम के बारे में कैसे सोचते हैं, जो कि अधिकांश

8
00:00:30,497 --> 00:00:34,580
परिचयात्मक कैलकुलस पाठ्यक्रम विषय को कैसे देखते हैं, उससे एक अलग अनुभव है।

9
00:00:34,580 --> 00:00:37,066
आपमें से जो लोग प्रासंगिक गणना से असहज हैं, उनके

10
00:00:37,066 --> 00:00:39,300
लिए मेरे पास इस विषय पर एक पूरी श्रृंखला है।

11
00:00:39,300 --> 00:00:46,780
आइए एक बेहद सरल नेटवर्क से शुरुआत करें, जहां प्रत्येक परत में एक न्यूरॉन होता है।

12
00:00:46,780 --> 00:00:50,999
यह नेटवर्क तीन भारों और तीन पूर्वाग्रहों द्वारा निर्धारित होता है, और

13
00:00:50,999 --> 00:00:55,640
हमारा लक्ष्य यह समझना है कि लागत फ़ंक्शन इन चरों के प्रति कितना संवेदनशील है।

14
00:00:55,640 --> 00:00:58,567
इस तरह हम जानते हैं कि उन शर्तों में कौन सा समायोजन

15
00:00:58,567 --> 00:01:01,100
लागत फ़ंक्शन में सबसे कुशल कमी का कारण बनेगा।

16
00:01:01,100 --> 00:01:05,360
हम केवल अंतिम दो न्यूरॉन्स के बीच संबंध पर ध्यान केंद्रित करेंगे।

17
00:01:05,360 --> 00:01:08,580
आइए उस अंतिम न्यूरॉन की सक्रियता को सुपरस्क्रिप्ट एल

18
00:01:08,580 --> 00:01:11,800
के साथ लेबल करें, यह दर्शाता है कि यह किस परत में है।

19
00:01:11,800 --> 00:01:16,560
तो पिछले न्यूरॉन की सक्रियता AL-1 है।

20
00:01:16,560 --> 00:01:20,080
ये प्रतिपादक नहीं हैं, हम जिस बारे में बात कर रहे हैं उसे अनुक्रमित करने का एक तरीका

21
00:01:20,080 --> 00:01:23,600
मात्र हैं, क्योंकि मैं बाद में विभिन्न सूचकांकों के लिए सबस्क्रिप्ट सहेजना चाहता हूं।

22
00:01:23,600 --> 00:01:28,487
मान लीजिए कि किसी दिए गए प्रशिक्षण उदाहरण के लिए हम इस अंतिम सक्रियण

23
00:01:28,487 --> 00:01:33,020
को जो मान चाहते हैं वह y है, उदाहरण के लिए, y 0 या 1 हो सकता है।

24
00:01:33,020 --> 00:01:39,040
तो एकल प्रशिक्षण उदाहरण के लिए इस नेटवर्क की लागत AL-y2 है।

25
00:01:39,040 --> 00:01:46,120
हम उस एक प्रशिक्षण उदाहरण की लागत को c0 के रूप में दर्शाएँगे।

26
00:01:46,120 --> 00:01:51,892
एक अनुस्मारक के रूप में, यह अंतिम सक्रियण एक वजन से निर्धारित होता है, जिसे मैं डब्ल्यूएल

27
00:01:51,892 --> 00:01:57,600
कहने जा रहा हूं, पिछले न्यूरॉन के सक्रियण का समय और कुछ पूर्वाग्रह, जिसे मैं बीएल कहूंगा।

28
00:01:57,600 --> 00:02:01,560
फिर आप उसे सिग्मॉइड या ReLU जैसे कुछ विशेष नॉनलाइनियर फ़ंक्शन के माध्यम से पंप करते हैं।

29
00:02:01,560 --> 00:02:06,022
यह वास्तव में हमारे लिए चीजों को आसान बनाने जा रहा है यदि हम इस भारित राशि को

30
00:02:06,022 --> 00:02:10,600
एक विशेष नाम देते हैं, जैसे z, प्रासंगिक सक्रियणों के समान सुपरस्क्रिप्ट के साथ।

31
00:02:10,600 --> 00:02:14,851
यह बहुत सारे शब्द हैं, और जिस तरह से आप इसकी संकल्पना कर सकते हैं वह

32
00:02:14,851 --> 00:02:18,980
यह है कि वजन, पिछली कार्रवाई और पूर्वाग्रह सभी को एक साथ z की गणना

33
00:02:18,980 --> 00:02:23,231
करने के लिए उपयोग किया जाता है, जो बदले में हमें a की गणना करने देता

34
00:02:23,231 --> 00:02:27,360
है, जो अंततः, एक निरंतर y के साथ, देता है हम लागत की गणना करते हैं।

35
00:02:27,360 --> 00:02:31,891
और निश्चित रूप से, AL-1 अपने स्वयं के वजन और पूर्वाग्रह आदि से प्रभावित

36
00:02:31,891 --> 00:02:35,920
होता है, लेकिन हम अभी उस पर ध्यान केंद्रित नहीं करने जा रहे हैं।

37
00:02:35,920 --> 00:02:38,120
ये सभी केवल संख्याएँ हैं, है ना?

38
00:02:38,120 --> 00:02:41,960
और यह सोचना अच्छा हो सकता है कि प्रत्येक की अपनी छोटी संख्या रेखा है।

39
00:02:41,960 --> 00:02:45,925
हमारा पहला लक्ष्य यह समझना है कि लागत फ़ंक्शन हमारे वजन

40
00:02:45,925 --> 00:02:49,820
डब्ल्यूएल में छोटे बदलावों के प्रति कितना संवेदनशील है।

41
00:02:49,820 --> 00:02:55,740
या वाक्यांश अलग ढंग से, wL के संबंध में c का व्युत्पन्न क्या है?

42
00:02:55,740 --> 00:02:58,698
जब आप इस डेल डब्ल्यू शब्द को देखते हैं, तो इसे डब्ल्यू के

43
00:02:58,698 --> 00:03:01,554
लिए कुछ छोटे संकेत के रूप में सोचें, जैसे कि 0 से बदलाव।

44
00:03:01,554 --> 00:03:08,820
01, और इस डेल सी शब्द को इस अर्थ के रूप में सोचें कि लागत पर परिणामी प्रभाव कुछ भी हो।

45
00:03:08,820 --> 00:03:10,900
हम जो चाहते हैं वह उनका अनुपात है।

46
00:03:10,900 --> 00:03:17,023
वैचारिक रूप से, डब्ल्यूएल के लिए यह छोटा सा धक्का जेडएल के लिए कुछ दबाव का कारण बनता

47
00:03:17,023 --> 00:03:23,220
है, जो बदले में एएल के लिए कुछ दबाव का कारण बनता है, जो सीधे लागत को प्रभावित करता है।

48
00:03:23,220 --> 00:03:28,490
इसलिए हम सबसे पहले zL में एक छोटे परिवर्तन और इस छोटे परिवर्तन w के अनुपात

49
00:03:28,490 --> 00:03:33,340
को देखकर चीजों को तोड़ते हैं, यानी, wL के संबंध में zL का व्युत्पन्न।

50
00:03:33,340 --> 00:03:37,595
इसी तरह, फिर आप AL में परिवर्तन और zL में उस छोटे परिवर्तन के

51
00:03:37,595 --> 00:03:41,713
अनुपात पर विचार करते हैं जिसके कारण यह हुआ, साथ ही अंतिम नज

52
00:03:41,713 --> 00:03:45,900
से c और इस मध्यवर्ती नज से AL के बीच के अनुपात पर विचार करें।

53
00:03:45,900 --> 00:03:51,709
यहीं श्रृंखला नियम है, जहां इन तीन अनुपातों को गुणा करने से हमें

54
00:03:51,709 --> 00:03:57,340
डब्ल्यूएल में छोटे बदलावों के प्रति सी की संवेदनशीलता मिलती है।

55
00:03:57,340 --> 00:04:02,428
तो अभी स्क्रीन पर, बहुत सारे प्रतीक हैं, और यह सुनिश्चित करने के लिए थोड़ा समय लें कि यह

56
00:04:02,428 --> 00:04:07,460
स्पष्ट है कि वे सभी क्या हैं, क्योंकि अब हम प्रासंगिक डेरिवेटिव की गणना करने जा रहे हैं।

57
00:04:07,460 --> 00:04:14,220
AL के संबंध में c का व्युत्पन्न 2AL-y होता है।

58
00:04:14,220 --> 00:04:19,008
इसका मतलब यह है कि इसका आकार नेटवर्क के आउटपुट और जिस चीज को हम चाहते

59
00:04:19,008 --> 00:04:23,796
हैं, उसके बीच अंतर के समानुपाती होता है, इसलिए यदि वह आउटपुट बहुत अलग

60
00:04:23,796 --> 00:04:28,380
था, तो मामूली बदलाव भी अंतिम लागत फ़ंक्शन पर बड़ा प्रभाव डालते हैं।

61
00:04:28,380 --> 00:04:32,862
ZL के संबंध में AL का व्युत्पन्न हमारे सिग्मॉइड फ़ंक्शन का

62
00:04:32,862 --> 00:04:37,420
व्युत्पन्न है, या जो भी गैर-रैखिकता आप उपयोग करना चुनते हैं।

63
00:04:37,420 --> 00:04:46,180
wL के संबंध में zL का व्युत्पन्न AL-1 निकलता है।

64
00:04:46,180 --> 00:04:50,257
मैं आपके बारे में नहीं जानता, लेकिन मुझे लगता है कि बिना एक पल भी आराम से बैठे

65
00:04:50,257 --> 00:04:54,180
और खुद को याद दिलाए कि उन सभी का क्या मतलब है, सूत्रों में फंस जाना आसान है।

66
00:04:54,180 --> 00:04:58,729
इस अंतिम व्युत्पन्न के मामले में, वजन की छोटी सी हलचल ने अंतिम परत को कितना

67
00:04:58,729 --> 00:05:03,220
प्रभावित किया, यह इस बात पर निर्भर करता है कि पिछला न्यूरॉन कितना मजबूत है।

68
00:05:03,220 --> 00:05:09,320
याद रखें, यह वह जगह है जहां न्यूरॉन्स-वह-फायर-टुगेदर-वायर-टुगेदर विचार आता है।

69
00:05:09,320 --> 00:05:12,950
और यह सब केवल एक विशिष्ट एकल प्रशिक्षण उदाहरण के

70
00:05:12,950 --> 00:05:16,580
लिए लागत के डब्ल्यूएल के संबंध में व्युत्पन्न है।

71
00:05:16,580 --> 00:05:20,487
चूँकि पूर्ण लागत फ़ंक्शन में कई अलग-अलग प्रशिक्षण उदाहरणों में उन

72
00:05:20,487 --> 00:05:24,336
सभी लागतों का एक साथ औसत शामिल होता है, इसलिए इसके व्युत्पन्न के

73
00:05:24,336 --> 00:05:28,540
लिए सभी प्रशिक्षण उदाहरणों पर इस अभिव्यक्ति के औसत की आवश्यकता होती है।

74
00:05:28,540 --> 00:05:34,263
बेशक, यह ग्रेडिएंट वेक्टर का सिर्फ एक घटक है, जो उन सभी भारों और

75
00:05:34,263 --> 00:05:40,780
पूर्वाग्रहों के संबंध में लागत फ़ंक्शन के आंशिक डेरिवेटिव से बनाया गया है।

76
00:05:40,780 --> 00:05:44,078
लेकिन भले ही यह हमारे लिए आवश्यक कई आंशिक डेरिवेटिवों

77
00:05:44,078 --> 00:05:46,460
में से एक है, यह काम का 50% से अधिक है।

78
00:05:46,460 --> 00:05:50,300
उदाहरण के लिए, पूर्वाग्रह के प्रति संवेदनशीलता लगभग समान है।

79
00:05:50,300 --> 00:05:58,980
हमें बस इस डेल ज़ेड डेल डब्ल्यू शब्द को ए डेल ज़ेड डेल बी के लिए बदलने की जरूरत है।

80
00:05:58,980 --> 00:06:04,700
और यदि आप प्रासंगिक सूत्र को देखें, तो वह व्युत्पन्न 1 निकलता है।

81
00:06:04,700 --> 00:06:10,517
इसके अलावा, और यहीं पर पीछे की ओर प्रचार करने का विचार आता है, आप देख सकते

82
00:06:10,517 --> 00:06:16,180
हैं कि यह लागत फ़ंक्शन पिछली परत की सक्रियता के प्रति कितना संवेदनशील है।

83
00:06:16,180 --> 00:06:20,556
अर्थात्, श्रृंखला नियम अभिव्यक्ति में यह प्रारंभिक व्युत्पन्न,

84
00:06:20,556 --> 00:06:25,420
पिछले सक्रियण के लिए z की संवेदनशीलता, वजन wL के रूप में सामने आती है।

85
00:06:25,420 --> 00:06:29,751
और फिर, भले ही हम उस पिछली परत सक्रियण को सीधे प्रभावित करने में

86
00:06:29,751 --> 00:06:34,416
सक्षम नहीं होंगे, लेकिन इसका ट्रैक रखना उपयोगी है, क्योंकि अब हम केवल

87
00:06:34,416 --> 00:06:38,881
उसी श्रृंखला नियम विचार को पीछे की ओर दोहराते रह सकते हैं यह देखने

88
00:06:38,881 --> 00:06:43,680
के लिए कि लागत फ़ंक्शन कितना संवेदनशील है पिछले भार और पिछले पूर्वाग्रह।

89
00:06:43,680 --> 00:06:47,309
और आप सोच सकते हैं कि यह एक अत्यधिक सरल उदाहरण है, क्योंकि सभी परतों में एक

90
00:06:47,309 --> 00:06:51,320
न्यूरॉन होता है, और वास्तविक नेटवर्क के लिए चीजें तेजी से अधिक जटिल होती जा रही हैं।

91
00:06:51,320 --> 00:06:55,265
लेकिन ईमानदारी से कहूं तो, जब हम परतों को कई न्यूरॉन्स देते हैं तो उतना

92
00:06:55,265 --> 00:06:59,320
बदलाव नहीं होता है, वास्तव में यह ट्रैक रखने के लिए बस कुछ और सूचकांक हैं।

93
00:06:59,320 --> 00:07:03,235
किसी दी गई परत के केवल AL सक्रिय होने के बजाय, इसमें एक

94
00:07:03,235 --> 00:07:07,920
सबस्क्रिप्ट भी होगी जो यह बताएगी कि यह उस परत का कौन सा न्यूरॉन है।

95
00:07:07,920 --> 00:07:11,726
आइए परत L-1 को अनुक्रमित करने के लिए अक्षर k का उपयोग करें,

96
00:07:11,726 --> 00:07:15,280
और परत L को अनुक्रमित करने के लिए j अक्षर का उपयोग करें।

97
00:07:15,280 --> 00:07:20,664
लागत के लिए, हम फिर से देखते हैं कि वांछित आउटपुट क्या है, लेकिन इस बार हम

98
00:07:20,664 --> 00:07:26,120
इन अंतिम परत सक्रियणों और वांछित आउटपुट के बीच अंतर के वर्गों को जोड़ते हैं।

99
00:07:26,120 --> 00:07:33,280
अर्थात्, आप ALj घटा yj वर्ग से अधिक राशि लेते हैं।

100
00:07:33,280 --> 00:07:37,319
चूँकि बहुत अधिक वजन है, प्रत्येक को यह पता लगाने के लिए कि

101
00:07:37,319 --> 00:07:41,563
वह कहाँ है, कुछ और सूचकांक रखने होंगे, तो चलिए इस kth न्यूरॉन

102
00:07:41,563 --> 00:07:45,740
को jth न्यूरॉन से जोड़ने वाले किनारे के वजन को WLjk कहते हैं।

103
00:07:45,740 --> 00:07:49,746
वे सूचकांक पहले थोड़ा पीछे की ओर लग सकते हैं, लेकिन यह इस बात से मेल खाता है कि आप उस

104
00:07:49,746 --> 00:07:53,800
वजन मैट्रिक्स को कैसे अनुक्रमित करेंगे जिसके बारे में मैंने भाग 1 वीडियो में बात की थी।

105
00:07:53,800 --> 00:07:59,466
पहले की तरह, प्रासंगिक भारित योग को z जैसा नाम देना अभी भी अच्छा है, ताकि

106
00:07:59,466 --> 00:08:04,980
अंतिम परत का सक्रियण सिर्फ आपका विशेष कार्य हो, जैसे z पर लागू सिग्मॉइड।

107
00:08:04,980 --> 00:08:08,364
आप देख सकते हैं कि मेरा क्या मतलब है, जहां ये सभी अनिवार्य

108
00:08:08,364 --> 00:08:11,863
रूप से वही समीकरण हैं जो हमारे पास पहले एक-न्यूरॉन-प्रति-परत

109
00:08:11,863 --> 00:08:15,420
मामले में थे, यह सिर्फ इतना है कि यह थोड़ा अधिक जटिल दिखता है।

110
00:08:15,420 --> 00:08:19,360
और वास्तव में, श्रृंखला नियम व्युत्पन्न अभिव्यक्ति यह बताती है कि

111
00:08:19,360 --> 00:08:23,540
किसी विशिष्ट भार के प्रति लागत कितनी संवेदनशील है, मूलतः वही दिखती है।

112
00:08:23,540 --> 00:08:26,389
यदि आप चाहें तो मैं इसे आप पर छोड़ता हूँ कि आप

113
00:08:26,389 --> 00:08:29,420
रुकें और उनमें से प्रत्येक शब्द के बारे में सोचें।

114
00:08:29,420 --> 00:08:33,420
हालाँकि, यहाँ जो परिवर्तन होता है, वह परत L-1 में

115
00:08:33,420 --> 00:08:37,820
सक्रियणों में से एक के संबंध में लागत का व्युत्पन्न है।

116
00:08:37,820 --> 00:08:40,774
इस मामले में, अंतर यह है कि न्यूरॉन कई अलग-अलग

117
00:08:40,774 --> 00:08:43,540
रास्तों से लागत फ़ंक्शन को प्रभावित करता है।

118
00:08:43,540 --> 00:08:49,079
यानी, एक ओर, यह AL0 को प्रभावित करता है, जो लागत फ़ंक्शन में

119
00:08:49,079 --> 00:08:54,437
भूमिका निभाता है, लेकिन इसका AL1 पर भी प्रभाव पड़ता है, जो

120
00:08:54,437 --> 00:09:00,340
लागत फ़ंक्शन में भी भूमिका निभाता है, और आपको उन्हें जोड़ना होगा।

121
00:09:00,340 --> 00:09:03,680
और वह, ठीक है, बस इतना ही।

122
00:09:03,680 --> 00:09:07,093
एक बार जब आप जान जाते हैं कि लागत फ़ंक्शन इस दूसरी-से-अंतिम परत

123
00:09:07,093 --> 00:09:10,506
में सक्रियणों के प्रति कितना संवेदनशील है, तो आप उस परत में आने

124
00:09:10,506 --> 00:09:13,920
वाले सभी भार और पूर्वाग्रहों के लिए प्रक्रिया को दोहरा सकते हैं।

125
00:09:13,920 --> 00:09:15,420
तो अपनी पीठ थपथपाओ!

126
00:09:15,420 --> 00:09:19,685
यदि यह सब समझ में आता है, तो अब आपने बैकप्रॉपैगेशन के मूल में गहराई

127
00:09:19,685 --> 00:09:23,700
से देखा है, तंत्रिका नेटवर्क कैसे सीखते हैं इसके पीछे का कामगार।

128
00:09:23,700 --> 00:09:29,045
ये श्रृंखला नियम अभिव्यक्ति आपको डेरिवेटिव देते हैं जो ग्रेडिएंट में प्रत्येक घटक को

129
00:09:29,045 --> 00:09:34,516
निर्धारित करते हैं जो बार-बार नीचे की ओर कदम बढ़ाकर नेटवर्क की लागत को कम करने में मदद

130
00:09:34,516 --> 00:09:35,020
करता है।

131
00:09:35,020 --> 00:09:36,304
यदि आप आराम से बैठते हैं और उस सब के बारे में सोचते हैं, तो

132
00:09:36,304 --> 00:09:37,568
यह आपके दिमाग को घेरने वाली जटिलता की बहुत सारी परतें हैं,

133
00:09:37,568 --> 00:09:38,960
इसलिए चिंता न करें अगर आपके दिमाग को यह सब पचाने में समय लगता है।

