[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "A suposição difícil aqui é que você assistiu à parte 3, que fornece um passo a passo intuitivo do algoritmo de retropropagação.",
  "model": "google_nmt",
  "from_community_srt": "A suposição difícil aqui é que você assistiu à parte 3, que fornece um guia intuitivo do algoritmo de retropropação.",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Aqui ficamos um pouco mais formais e mergulhamos no cálculo relevante.",
  "model": "google_nmt",
  "from_community_srt": "Aqui, somos um pouco mais formais e aprofundamos no cálculo relevante.",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "É normal que isso seja pelo menos um pouco confuso, então o mantra de pausar e ponderar regularmente certamente se aplica tanto aqui quanto em qualquer outro lugar.",
  "model": "google_nmt",
  "from_community_srt": "É normal que isto seja um pouco confuso, então o mantra de pausar de vez em quando pra pensar se aplica do mesmo jeito.",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "Nosso principal objetivo é mostrar como as pessoas que trabalham com aprendizado de máquina comumente pensam sobre a regra da cadeia do cálculo no contexto de redes, o que tem uma sensação diferente de como a maioria dos cursos introdutórios ao cálculo abordam o assunto.",
  "model": "google_nmt",
  "from_community_srt": "Nosso principal objetivo é mostrar como o povo da aprendizagem de máquina costuma pensar na regra da cadeia do cálculo no contexto de redes, que é um pouco diferente de como a maioria dos cursos introdutórios de cálculo abordam o assunto.",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Para aqueles que não se sentem à vontade com o cálculo relevante, tenho uma série completa sobre o assunto.",
  "model": "google_nmt",
  "from_community_srt": "Para aqueles que não se dão muito bem com cálculo, eu tenho uma série inteira sobre o assunto.",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Vamos começar com uma rede extremamente simples, onde cada camada contém um único neurônio.",
  "model": "google_nmt",
  "from_community_srt": "Vamos começar com uma rede extremamente simples, uma onde cada camada tem apenas um único neurônio.",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Esta rede é determinada por três pesos e três vieses, e nosso objetivo é entender o quão sensível é a função custo a essas variáveis.",
  "model": "google_nmt",
  "from_community_srt": "Então essa rede específica é determinada por 3 pesos e 3 vieses, e nosso objetivo é entender o quão sensível a função de custo é para essas variáveis.",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way, we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "Dessa forma, sabemos quais ajustes nesses termos causarão a redução mais eficiente na função custo.",
  "model": "google_nmt",
  "from_community_srt": "Assim nós sabemos quais ajustes a esses termos vai diminuir a função de custo mais rapidamente.",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "And we're just going to focus on the connection between the last two neurons.",
  "translatedText": "E vamos nos concentrar apenas na conexão entre os dois últimos neurônios.",
  "model": "google_nmt",
  "from_community_srt": "Vamos focar somente na conexão entre os dois últimos neurônios.",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in, so the activation of the previous neuron is Al-1.",
  "translatedText": "Vamos rotular a ativação desse último neurônio com um L sobrescrito, indicando em qual camada ele está, então a ativação do neurônio anterior é Al-1.",
  "model": "google_nmt",
  "from_community_srt": "Vamos rotular a ativação desse último neurônio com um sobrescrito L, indicando a camada em que ele está, então a ativação desse neurônio anterior é a^(L-1);",
  "n_reviews": 0,
  "start": 65.98,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Não são expoentes, são apenas uma forma de indexar o que estamos falando, já que quero salvar subscritos para índices diferentes posteriormente.",
  "model": "google_nmt",
  "from_community_srt": "Eles não são expoentes, são só um jeito de indexar o que estamos falando, uma vez que quero salvar os subscritos para índices diferentes mais tarde.",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Digamos que o valor que queremos que esta última ativação tenha para um determinado exemplo de treinamento seja y, por exemplo, y pode ser 0 ou 1.",
  "model": "google_nmt",
  "from_community_srt": "Digamos que o valor que queremos que esta última ativação tenha para um exemplo de treinamento é y. Por exemplo, y pode ser 0 ou 1.",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is Al-y2.",
  "translatedText": "Portanto, o custo desta rede para um único exemplo de treinamento é Al-y2.",
  "model": "google_nmt",
  "from_community_srt": "Então o custo desta rede simples para um único exemplo de treinamento é  (a^(L) - y)^2.",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Denotaremos o custo desse exemplo de treinamento como c0.",
  "model": "google_nmt",
  "from_community_srt": "Vamos indicar o custo deste exemplo de treinamento como C_0.",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neuron's activation plus some bias, which I'll call BL.",
  "translatedText": "Lembrando que esta última ativação é determinada por um peso, que chamarei de WL, vezes a ativação do neurônio anterior mais algum viés, que chamarei de BL.",
  "model": "google_nmt",
  "from_community_srt": "Lembrando que esta última ativação é determinada por um peso, que chamarei de w^(L) vezes a ativação do neurônio anterior, mais algum viés,",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "E então você bombeia isso através de alguma função não linear especial como o sigmóide ou ReLU.",
  "model": "google_nmt",
  "from_community_srt": "que vou chamar de b^(L), então você passa isso por alguma função não-linear especial como uma sigmoide ou uma ReLU.",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "Na verdade, as coisas ficarão mais fáceis para nós se dermos um nome especial a essa soma ponderada, como z, com o mesmo sobrescrito das ativações relevantes.",
  "model": "google_nmt",
  "from_community_srt": "Na verdade vai ficar mais fácil pra nós se dermos um nome especial para essa soma ponderada, como z, com o mesmo sobrescrito das ativações relevantes.",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action and the bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "São muitos termos, e uma maneira de conceituar isso é que o peso, a ação anterior e a tendência, todos juntos, são usados para calcular z, o que por sua vez nos permite calcular a, que finalmente, junto com uma constante y, permite nós calculamos o custo.",
  "model": "google_nmt",
  "from_community_srt": "É termo pra caramba. E uma forma de pensar nisso é que o peso, a ativação anterior, e o viés juntos são usados pra calcular z, que por sua vez nos deixa calcular a, que finalmente, junto da constante y, nos deixa calcular o custo.",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course Al-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "E é claro que o Al-1 é influenciado pelo seu próprio peso e preconceito e tal, mas não vamos nos concentrar nisso agora.",
  "model": "google_nmt",
  "from_community_srt": "E claro, a^(L-1) é influenciado por seu próprio peso e viés, e tal. Mas não vamos focar nisso agora.",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Tudo isso são apenas números, certo?",
  "model": "google_nmt",
  "from_community_srt": "Tudo isso são apenas números,",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "E pode ser bom pensar que cada um tem a sua própria reta numérica.",
  "model": "google_nmt",
  "from_community_srt": "certo? E pode ser legal pensar em cada um como tendo sua própria reta numérica.",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight WL.",
  "translatedText": "Nosso primeiro objetivo é entender quão sensível é a função custo a pequenas mudanças em nosso peso WL.",
  "model": "google_nmt",
  "from_community_srt": "Nosso primeiro objetivo é entender o quão sensível a função de custo é a pequenas mudanças em nosso peso w^(L).",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to WL?",
  "translatedText": "Ou, de outra forma, qual é a derivada de c em relação a WL?",
  "model": "google_nmt",
  "from_community_srt": "Em outras palavras, qual é a derivada de C em relação a w^(L).",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del W term, think of it as meaning some tiny nudge to W, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Quando você vir esse termo del W, pense nele como um pequeno empurrão para W, como uma mudança de 0,01, e pense nesse termo del c como significando qualquer que seja o empurrão resultante no custo.",
  "model": "google_nmt",
  "from_community_srt": "Quando você ver esse termo “∂w\", pense nele como significando \"uma pequena mexida em w\", algo como 0.01 E pense nesse \"∂C\" como \"qualquer mexida resultante no custo\".",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "O que queremos é a proporção deles.",
  "model": "google_nmt",
  "from_community_srt": "O que queremos é a razão disso.",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Conceitualmente, esse pequeno empurrão no WL causa algum empurrão no ZL, o que por sua vez causa algum empurrão no AL, o que influencia diretamente o custo.",
  "model": "google_nmt",
  "from_community_srt": "Teoricamente, essa pequena mexida em w^(L) causa alguma mexida em z^(L) que por sua vez causa alguma alteração em a^(L), que influencia o custo diretamente.",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is, the derivative of ZL with respect to WL.",
  "translatedText": "Portanto, dividimos as coisas observando primeiro a razão entre uma pequena alteração em ZL e esta pequena alteração W, ou seja, a derivada de ZL em relação a WL.",
  "model": "google_nmt",
  "from_community_srt": "Então vemos isso em partes, olhando primeiro a razão de uma pequena mudança em z^(L) vindo da pequena mudança em w^(L) Ou seja, a derivada de z^(L) em relação a w^(L).",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "Da mesma forma, você considera a razão entre a mudança para AL e a pequena mudança em ZL que a causou, bem como a razão entre o empurrão final para c e esse empurrão intermediário para AL.",
  "model": "google_nmt",
  "from_community_srt": "Da mesma forma, consideramos a razão de uma mudança em a^(L) para a pequena mudança em z^(L) que a causou bem como a razão entre a última mexida em C e essa mexida intermediária em a^(L).",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of c to small changes in WL.",
  "translatedText": "Isso aqui é a regra da cadeia, onde a multiplicação dessas três proporções nos dá a sensibilidade de c a pequenas mudanças no WL.",
  "model": "google_nmt",
  "from_community_srt": "Isso bem aqui é a regra da cadeia, onde o produto entre essas razões nos dá a sensibilidade de C a pequenas mudanças em w^(L).",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "Então, na tela agora, há muitos símbolos, e reserve um momento para ter certeza de que está claro quais são todos eles, porque agora vamos calcular as derivadas relevantes.",
  "model": "google_nmt",
  "from_community_srt": "Tem muitos símbolos na tela agora, então tenha certeza de que está claro o que todos eles são, porque agora vamos calcular as derivadas relevantes.",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "A derivada de c em relação a AL resulta em 2AL-y.",
  "model": "google_nmt",
  "from_community_srt": "A derivada de C em relação a a^(L) acontece de ser 2(a^(L) - y).",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "Notice this means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Observe que isso significa que seu tamanho é proporcional à diferença entre a produção da rede e o que queremos que ela seja; portanto, se essa produção for muito diferente, mesmo pequenas alterações podem ter um grande impacto na função de custo final.",
  "model": "google_nmt",
  "from_community_srt": "Observe que isso quer dizer que seu tamanho é proporcional à diferença entre a saída atual da rede, e a saída esperada. Então se essa saída foi muito diferente, mesmo pequenas mudanças acabam tendo um grande impacto na função de custo.",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "A derivada de AL em relação a ZL é apenas a derivada de nossa função sigmóide, ou qualquer não-linearidade que você escolher usar.",
  "model": "google_nmt",
  "from_community_srt": "A derivada de a^(L) em relação a z^(L) é apenas a derivada de nossa função sigmoide, ou qualquer outra não-linearidade que você escolher.",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "And the derivative of ZL with respect to WL comes out to be AL-1.",
  "translatedText": "E a derivada de ZL em relação a WL é AL-1.",
  "model": "google_nmt",
  "from_community_srt": "E a derivada de z^(L) em relação a w^(L), neste caso acontece de ser a^(L-1).",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all mean.",
  "translatedText": "Bem, não sei sobre você, mas acho que é fácil ficar preso de cabeça nas fórmulas sem parar um momento para sentar e se lembrar do que todas elas significam.",
  "model": "google_nmt",
  "from_community_srt": "Agora não sei você, mas eu acho que é fácil ficar perdido nessas fórmulas se você não tirar um tempo pra se lembrar do que elas significam.",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "No caso desta última derivada, a quantidade que o pequeno empurrão no peso influenciou a última camada depende de quão forte é o neurônio anterior.",
  "model": "google_nmt",
  "from_community_srt": "No caso desta última derivada, a quantidade de influência que uma pequena mexida neste peso tem na última camada depende da força do neurônio anterior.",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "Lembre-se de que é aqui que entra a ideia dos neurônios que disparam juntos.",
  "model": "google_nmt",
  "from_community_srt": "Lembre-se que é aqui que surge aquela ideia de que \"neurônios que disparam juntos permanecem juntos\".",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to WL only of the cost for a specific single training example.",
  "translatedText": "E tudo isso é a derivada em relação ao WL apenas do custo de um único exemplo específico de treinamento.",
  "model": "google_nmt",
  "from_community_srt": "E tudo isso é a derivada em relação a w^(L) do custo de apenas um exemplo de treinamento.",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Como a função de custo total envolve a média de todos esses custos em muitos exemplos de treinamento diferentes, sua derivada requer a média dessa expressão em todos os exemplos de treinamento.",
  "model": "google_nmt",
  "from_community_srt": "Uma vez que a função de custo completa envolve tirar a média de todos os custos entre muitos exemplos, sua derivada requer tirar a média desta expressão que encontramos sobre todos os exemplos.",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "And of course, that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "E, claro, esta é apenas uma componente do vetor gradiente, que é construído a partir das derivadas parciais da função de custo em relação a todos esses pesos e tendências.",
  "model": "google_nmt",
  "from_community_srt": "E é claro que isso é apenas um componente do vetor-gradiente, que é feito a partir das derivadas parciais da função de custo em relação a todos aqueles pesos e vieses.",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "Mas mesmo que esta seja apenas uma das muitas derivadas parciais de que precisamos, é mais de 50% do trabalho.",
  "model": "google_nmt",
  "from_community_srt": "Mas apesar de ser apenas uma das derivadas parciais que precisamos, é mais de 50% do trabalho.",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "A sensibilidade ao viés, por exemplo, é quase idêntica.",
  "model": "google_nmt",
  "from_community_srt": "A sensitividade do viés, por exemplo, é quase idêntica.",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Só precisamos trocar esse termo del z del w por a del z del b.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "E se você olhar para a fórmula relevante, essa derivada será 1.",
  "model": "google_nmt",
  "from_community_srt": "Só temos que mudar esse termo ∂z/∂w para ∂z/∂b, E se você olhar a fórmula relevante, essa derivada acontece de ser 1.",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "Além disso, e é aí que entra a ideia de propagação para trás, você pode ver o quão sensível essa função de custo é à ativação da camada anterior.",
  "model": "google_nmt",
  "from_community_srt": "Além disso, e agora é onde entra a ideia de propagar para trás, você pode ver o quão sensível esta função de custo é para a ativação da camada anterior.",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight WL.",
  "translatedText": "Ou seja, esta derivada inicial na expressão da regra da cadeia, a sensibilidade de z à ativação anterior, resulta ser o peso WL.",
  "model": "google_nmt",
  "from_community_srt": "ou seja, esta derivada inicial na expensão da regra da cadeia, a sensibilidade de z à ativação anterior, acontece de ser o peso w^(L).",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "E, novamente, mesmo que não seremos capazes de influenciar diretamente a ativação da camada anterior, é útil acompanhar, porque agora podemos simplesmente continuar iterando essa mesma ideia de regra da cadeia de trás para frente para ver quão sensível é a função de custo para pesos anteriores e vieses anteriores.",
  "model": "google_nmt",
  "from_community_srt": "Novamente, mesmo não sendo capazes de influenciar a ativação diretamente, ajuda se a acompanharmos, porque agora é só continuarmos iterando essa ideia da regra da cadeia pra trás para ver o quão sensível a função de custo é para os pesos e vieses anteriores.",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "E você pode pensar que este é um exemplo muito simples, já que todas as camadas têm um neurônio, e as coisas ficarão exponencialmente mais complicadas para uma rede real.",
  "model": "google_nmt",
  "from_community_srt": "Você pode pensar que este é um exemplo super simplificado, porque todas as camadas tem só 1 neurônio, e as coisas vão ficar exponencialmente mais complicadas na rede verdadeira.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "Mas, honestamente, não muda muita coisa quando damos vários neurônios às camadas; na verdade, são apenas mais alguns índices para acompanhar.",
  "model": "google_nmt",
  "from_community_srt": "Mas honestamente, não muda tanto assim quando damos vários neurônios para as camadas. Na verdade são só mais alguns índices que temos de acompanhar.",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "Em vez de a ativação de uma determinada camada ser simplesmente AL, ela também terá um subscrito indicando qual neurônio dessa camada se trata.",
  "model": "google_nmt",
  "from_community_srt": "Em vez da ativação de uma determinada camada ser apenas a^(L), ela também vai ter um subscrito indicando de qual neurônio dessa camada ela é.",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Vamos usar a letra k para indexar a camada L-1 e j para indexar a camada L.",
  "model": "google_nmt",
  "from_community_srt": "Vamos usar a letra k para indexar a camada (L-1), e j para indexar a camada (L).",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "Para o custo, novamente olhamos qual é a saída desejada, mas desta vez somamos os quadrados das diferenças entre as ativações da última camada e a saída desejada.",
  "model": "google_nmt",
  "from_community_srt": "Para o custo, novamente nós vemos qual é a saída desejada. Só que dessavez nós somamos os quadrados das diferenças entre essas últimas ativações das camadas e a saída desejada.",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus Yj squared.",
  "translatedText": "Ou seja, você soma ALj menos Yj ao quadrado.",
  "model": "google_nmt",
  "from_community_srt": "Ou seja,",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Como há muito mais pesos, cada um precisa ter mais alguns índices para acompanhar onde está, então vamos chamar o peso da aresta que conecta esse k-ésimo neurônio ao j-ésimo neurônio, WLjk.",
  "model": "google_nmt",
  "from_community_srt": "você pega a soma sobre  (a_j^(L) - y_j)^2 Uma vez que agora temos muito mais pesos, cada um precisa ter alguns índices a mais para saber onde está. Vamos chamar o peso da borda que conecta desse k-ésimo ao j-ésimo neurônio de  w_{jk}^(L).",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "Esses índices podem parecer um pouco atrasados no início, mas estão de acordo com a forma como você indexaria a matriz de peso de que falei na parte 1 do vídeo.",
  "model": "google_nmt",
  "from_community_srt": "Esses índices podem parecer estar invertidos a princípio, mas eles se alinham com a forma que você deve indexar a matriz de pesos que falei no vídeo da Parte 1.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "Assim como antes, ainda é bom dar um nome à soma ponderada relevante, como z, para que a ativação da última camada seja apenas sua função especial, como o sigmóide, aplicado a z.",
  "model": "google_nmt",
  "from_community_srt": "Assim como antes, ainda é bom dar um nome à soma ponderada em questão, tipo z, para que a ativação da última camada seja apenas sua função especial, como a sigmoide, aplicada a z.",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Você pode entender o que quero dizer, onde todas essas são essencialmente as mesmas equações que tínhamos antes no caso de um neurônio por camada, só que parece um pouco mais complicado.",
  "model": "google_nmt",
  "from_community_srt": "Dá pra ver o que quero dizer, certo? Essas são praticamente as mesmas equações que tínhamos antes com as camadas de 1 neurônio; só parece um pouco mais complicado.",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain-ruled derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "E, de fato, a expressão derivada em cadeia que descreve quão sensível é o custo a um peso específico parece essencialmente a mesma.",
  "model": "google_nmt",
  "from_community_srt": "E de fato, a expressão de derivada da regra da cadeia que descreve o quão sensível o custo é para um peso específico está praticamente do mesmo jeito.",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Deixo para você fazer uma pausa e pensar sobre cada um desses termos, se quiser.",
  "model": "google_nmt",
  "from_community_srt": "Vou deixar que você pause e pense sobre cada um desses termos, se quiser.",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "O que muda aqui, porém, é a derivada do custo em relação a uma das ativações na camada L-1.",
  "model": "google_nmt",
  "from_community_srt": "Mas uma coisa que realmente muda aqui é a derivada do custo em relação a uma das ativações na camada (L-1).",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "Neste caso, a diferença é que o neurônio influencia a função de custo através de múltiplos caminhos diferentes.",
  "model": "google_nmt",
  "from_community_srt": "Neste caso, a diferença é que o neurônio influencia a função de custo por vários caminhos.",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "Ou seja, por um lado influencia AL0, que desempenha um papel na função custo, mas também influencia AL1, que também desempenha um papel na função custo, e você tem que somar isso.",
  "model": "google_nmt",
  "from_community_srt": "Ou seja, de um lado, ele influencia a_0^(L), que desempenha um papel na função de custo, mas por outro lado ele influencia a_1^(L) que também desempenha um papel na função de custo. E você tem que somar os dois.",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "E isso, bem, é basicamente isso.",
  "model": "google_nmt",
  "from_community_srt": "E isso... é basicamente tudo.",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Depois de saber o quão sensível a função de custo é às ativações nesta penúltima camada, você pode simplesmente repetir o processo para todos os pesos e tendências que alimentam essa camada.",
  "model": "google_nmt",
  "from_community_srt": "Uma vez que você souber o quão sensível a função de custo é a ativação da segunda camada para a última, é só repetir o processo para todos os pesos e vieses que entram nessa camada.",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "Então dê um tapinha nas costas!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Se tudo isso faz sentido, você agora examinou profundamente o cerne da retropropagação, o carro-chefe por trás de como as redes neurais aprendem.",
  "model": "google_nmt",
  "from_community_srt": "Então se dê um tapinha nas costas! Se tudo isso fizer sentido, você agora olhou fundo no coração da retropropagação, o motor por trás de como as redes neurais aprendem.",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Essas expressões de regras da cadeia fornecem as derivadas que determinam cada componente no gradiente que ajuda a minimizar o custo da rede ao descer repetidamente a ladeira.",
  "model": "google_nmt",
  "from_community_srt": "Essas expressões da regra da cadeia te dá as derivadas que determinam cada componente no gradiente que ajuda a minimizar o custo da rede, descendo a ladeira repetidamente.",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Se você sentar e pensar sobre tudo isso, verá que há muitas camadas de complexidade para envolver sua mente, então não se preocupe se levar tempo para sua mente digerir tudo.",
  "model": "google_nmt",
  "from_community_srt": "Hhhhpf. Se você parar pra pensar nisso tudo, são muitas camadas de complexidade pra processar. Então não se preocupe se precisar de um tempinho pra digerir tudo.",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]