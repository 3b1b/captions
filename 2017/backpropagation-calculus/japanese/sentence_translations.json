[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "ここでの厳密な前提条件は、バックプロパゲーション アルゴリズムの直 感的なウォークスルーを提供するパート 3 を視聴していることです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "ここではもう少し形式的に、関連する微積分について詳しく説明します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "少なくとも少し混乱するのは普通のことなので、定期的に立ち止まって熟考 するという信条は、他の場所と同じようにここでも確かに当てはまります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "私たちの主な目標は、機械学習の人々がネットワークの文脈で微積分の連鎖則につ いてどのように一般的に考えているかを示すことです。 これは、ほとんどの微積分 入門コースがこの主題にアプローチする方法とは異なる雰囲気を持っています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "関連する微積分に抵抗がある人のために、このト ピックに関するシリーズ全体を用意しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "各層に 1 つのニューロンが含まれる、非 常に単純なネットワークから始めましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "このネットワークは 3 つの重みと 3 つのバイアスによって決定されます。 私たちの 目標は、コスト関数がこれらの変数に対してどの程度敏感であるかを理解することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "そうすることで、これらの項に対するどの調整がコスト関数 の最も効率的な減少を引き起こすかを知ることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "We'll just focus on the connection between the last two neurons.",
  "translatedText": "最後の 2 つのニューロン間の接続にのみ焦点を当てます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in.",
  "translatedText": "最後のニューロンの活性化に上付き文字 L を付けて、どの層にあるかを示しましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.98,
  "end": 71.36
 },
 {
  "input": "So the activation of the previous neuron is AL-1.",
  "translatedText": "したがって、前のニューロンの活性化は AL-1 です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.68,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "これらは指数ではなく、後で別のインデックスの添字を保存したいので、 ここで話しているものにインデックスを付けるための単なる方法です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "特定のトレーニング例でこの最後のアクティベーションに必要な値が y であ るとします。 たとえば、y は 0 または 1 である可能性があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is AL-y2.",
  "translatedText": "したがって、単一のトレーニング例に対するこのネットワークのコストは AL-y2 です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "その 1 つのトレーニング例のコストを c0 と表記します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call wL, times the previous neuron's activation plus some bias, which I'll call bL.",
  "translatedText": "念のため言っておきますが、この最後の活性化は、前のニューロンの活性化とバイアス (bL と呼ぶことにします) を掛けた重み (wL と呼ぶことにします) によって決定されます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "Then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "次に、それをシグモイドや ReLU などの特別な非線形関数を通してポンプします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "実際、この重み付けされた合計に、関連するアクティベーションと同じ上付き 文字を付けた z のような特別な名前を付けると、作業が簡単になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action, and bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "これには多くの用語が含まれますが、これを概念化すると、重み、以前のアクション、バ イアスがすべて一緒になって z を計算するために使用され、それによって a が計 算され、最後に定数 y とともに次のようになります。 私たちがコストを計算します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course, AL-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "そしてもちろん、AL-1 はそれ自体の重みやバイアスなどの影 響を受けますが、今はそれに焦点を当てるつもりはありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "これらはすべて単なる数字ですよね？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "そして、それぞれが独自の小さな数直線を持っていると考えるとよいでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight wL.",
  "translatedText": "私たちの最初の目標は、重み wL の小さな変化に対して コスト関数がどの程度敏感であるかを理解することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to wL?",
  "translatedText": "別の言い方をすると、wL に関する c の導関数は何ですか?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del w term, think of it as meaning some tiny nudge to w, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "この del w という用語を見たときは、0 による変更など、w に対する小さなナッジを意味すると考えてくださ い。01 であり、この del c という用語は、結果として生じるコストの微調整を意味すると考えてください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "私たちが知りたいのはそれらの比率です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to wL causes some nudge to zL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "概念的には、wL へのこの小さなナッジは zL へのナッジを引き起こし 、それが今度は AL へのナッジを引き起こし、コストに直接影響します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to zL to this tiny change w, that is, the derivative of zL with respect to wL.",
  "translatedText": "そこで、最初に zL に対する小さな変化とこの小さな変化 w の比、つ まり wL に対する zL の導関数を調べることで物事を細分化します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in zL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "同様に、次に、AL への変化とそれを引き起こした z L の小さな変化の比率、および c への最後のナッジ と AL へのこの中間のナッジとの比率を考慮します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying these three ratios gives us the sensitivity of c to small changes in wL.",
  "translatedText": "これは連鎖則であり、これら 3 つの比率を乗算すると、 wL の小さな変化に対する c の感度が得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "現在、画面上にはたくさんのシンボルが表示されていますが、これから関連する導関数を計 算するので、それらがすべて明確であることを確認するために少し時間を取ってください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "AL に関する c の導関数は、2AL-y となります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "This means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "これは、そのサイズがネットワークの出力と私たちが望むものとの差に比例 することを意味します。 そのため、その出力が大きく異なる場合、わずか な変更でも最終的なコスト関数に大きな影響を与える可能性があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to zL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "zL に関する AL の導関数は、シグモイド関数 、または使用する非線形性の導関数にすぎません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "The derivative of zL with respect to wL comes out to be AL-1.",
  "translatedText": "wL に関する zL の導関数は AL-1 になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself what they all mean.",
  "translatedText": "あなたはどうか知りませんが、じっくりと時間をかけて公式の意味を 思い出さずに、公式にどっぷりと浸かってしまいがちだと思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "この最後の導関数の場合、重みへの小さな微調整が最後の層 に与える影響の量は、前のニューロンの強さに依存します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "覚えておいてください、ここで、ニューロンが一緒に発火し、一緒にワイヤリングするというアイデアが登場します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to wL only of the cost for a specific single training example.",
  "translatedText": "そして、これはすべて、特定の 1 つのトレーニング例 のコストのみを wL に関して導関数したものです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "フルコスト関数には、多くの異なるトレーニング例にわたるすべ てのコストの平均が含まれるため、その導関数では、すべての トレーニング例にわたってこの式を平均する必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "Of course, that's just one component of the gradient vector, which is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "もちろん、これは勾配ベクトルの 1 つのコンポーネントにすぎず、す べての重みとバイアスに関するコスト関数の偏導関数から構築されます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "しかし、これは必要な偏導関数の 1 つにすぎ ませんが、作業の 50% 以上を占めます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "たとえば、バイアスに対する感度はほぼ同じです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "この del z del w という用語を del z del b に変更するだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "関連する式を見ると、その微分値は 1 になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "また、ここで逆方向に伝播するというアイデアが登場しますが、このコスト 関数が前の層のアクティブ化に対してどれほど敏感であるかがわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight wL.",
  "translatedText": "つまり、連鎖ルール式におけるこの初期導関数、つまり前 の起動に対する z の感度が重み wL になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "繰り返しますが、前の層のアクティブ化に直接影響を与えることは できませんが、同じチェーン ルールのアイデアを逆方向に反復し 続けるだけで、コスト関数がどの程度敏感であるかを確認できるた め、追跡するのには役立ちます。 以前の重みと以前のバイアス。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "すべての層には 1 つのニューロンがあり、実際のネットワークでは事態は 指数関数的に複雑になるため、これは単純すぎる例だと思うかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "しかし、正直に言うと、レイヤーに複数のニューロンを与えてもそれほど大きな変化 はありません。 実際には、追跡するインデックスがさらにいくつか増えるだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "特定の層の活性化が単に AL であるのではなく、その層 のどのニューロンであるかを示す添え字も付けられます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "文字 k を使用してレイヤー L-1 にインデックスを付け、j を使用してレイヤー L にインデックスを付けましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "コストについては、再び望ましい出力が何であるかを調べますが、今回は、 これらの最後の層のアクティブ化と望ましい出力の差の二乗を合計します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus yj squared.",
  "translatedText": "つまり、ALj から yj の 2 乗を引いた合計を計算します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "さらに多くの重みがあるため、それぞれがどこにあるかを追跡するためにさらに いくつかのインデックスを持つ必要があるため、この k 番目のニューロンを j 番目のニューロンに接続するエッジの重みを WLjk と呼びます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "これらのインデックスは最初は少し時代遅れに感じるかもしれませんが、パート 1 のビデオで説明した重み行列のインデックス付け方法と一致しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "前と同じように、関連する重み付けされた合計に z などの名 前を付けると、最後の層のアクティブ化が z に適用される シグモイドのような特別な関数にすぎなくなるので便利です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "私が言いたいことはわかると思いますが、これらはすべて、レイヤーごとに 1 ニュー ロンの場合に以前に作成した方程式と本質的に同じですが、少し複雑に見えるだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain rule derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "そして実際、特定の重みに対するコストの感度を表す連 鎖ルールの導関数式は、本質的に同じように見えます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "必要に応じて、立ち止まってそれぞれの用語について考えてみてください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "ただし、ここで変わるのは、レイヤー L-1 のアクテ ィベーションの 1 つに関するコストの導関数です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "この場合の違いは、ニューロンが複数の異なるパ スを通じてコスト関数に影響を与えることです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "つまり、コスト関数の役割を果たす AL0 に影響を与え る一方で、同じくコスト関数の役割を果たす AL1 に も影響を与えるため、それらを合計する必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "そして、まあ、それだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "この最後から 2 番目の層のアクティベーションに対するコ スト関数の感度がわかったら、その層に入力されるすべての 重みとバイアスに対してこのプロセスを繰り返すだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "だから自分の背中を押してください！",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "これらすべてが理解できるのであれば、ニューラル ネットワークの学習方法の背後 にある主力であるバックプロパゲーションの中心部を深く調べたことになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "これらのチェーン ルール式により、勾配内の各コンポーネントを決定する導関数が得 られ、下り坂を繰り返してネットワークのコストを最小限に抑えることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "座ってこれらすべてについて考えると、これはあなたの心を包み込む複雑な層にな るため、頭がすべてを消化するのに時間がかかっても心配する必要はありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]