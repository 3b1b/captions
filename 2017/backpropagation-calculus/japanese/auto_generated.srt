1
00:00:00,000 --> 00:00:04,164
ここでの厳密な前提条件は、バックプロパゲーション

2
00:00:04,164 --> 00:00:08,828
アルゴリズムの直 感的なウォークスルーを提供するパート

3
00:00:08,828 --> 00:00:11,160
3 を視聴していることです。

4
00:00:11,160 --> 00:00:12,983
ここではもう少し形式的に、関連す

5
00:00:12,983 --> 00:00:14,920
る微積分について詳しく説明します。

6
00:00:14,920 --> 00:00:17,280
少なくとも少し混乱するのは普通のことなので、定

7
00:00:17,280 --> 00:00:19,640
期的に立ち止まって熟考 するという信条は、他の

8
00:00:19,640 --> 00:00:22,000
場所と同じようにここでも確かに当てはまります。

9
00:00:22,000 --> 00:00:25,117
私たちの主な目標は、機械学習の人々がネットワークの文脈で

10
00:00:25,117 --> 00:00:28,234
微積分の連鎖則につ いてどのように一般的に考えているかを

11
00:00:28,234 --> 00:00:30,572
示すことです。 これは、ほとんどの微積分

12
00:00:30,572 --> 00:00:33,689
入門コースがこの主題にアプローチする方法とは異なる雰囲気

13
00:00:33,689 --> 00:00:34,580
を持っています。

14
00:00:34,580 --> 00:00:36,992
関連する微積分に抵抗がある人のために、このト

15
00:00:36,992 --> 00:00:39,300
ピックに関するシリーズ全体を用意しています。

16
00:00:39,300 --> 00:00:43,131
各層に 1 つのニューロンが含まれる、非

17
00:00:43,131 --> 00:00:46,780
常に単純なネットワークから始めましょう。

18
00:00:46,780 --> 00:00:48,914
このネットワークは 3 つの重みと 3

19
00:00:48,914 --> 00:00:51,370
つのバイアスによって決定されます。 私たちの

20
00:00:51,370 --> 00:00:54,252
目標は、コスト関数がこれらの変数に対してどの程度敏感で

21
00:00:54,252 --> 00:00:55,640
あるかを理解することです。

22
00:00:55,640 --> 00:00:58,419
そうすることで、これらの項に対するどの調整がコスト関数

23
00:00:58,419 --> 00:01:01,100
の最も効率的な減少を引き起こすかを知ることができます。

24
00:01:01,100 --> 00:01:05,360
最後の 2 つのニューロン間の接続にのみ焦点を当てます。

25
00:01:05,360 --> 00:01:08,344
最後のニューロンの活性化に上付き文字

26
00:01:08,344 --> 00:01:11,800
L を付けて、どの層にあるかを示しましょう。

27
00:01:11,800 --> 00:01:16,560
したがって、前のニューロンの活性化は AL-1 です。

28
00:01:16,560 --> 00:01:18,906
これらは指数ではなく、後で別のインデックスの

29
00:01:18,906 --> 00:01:21,253
添字を保存したいので、 ここで話しているもの

30
00:01:21,253 --> 00:01:23,600
にインデックスを付けるための単なる方法です。

31
00:01:23,600 --> 00:01:26,696
特定のトレーニング例でこの最後のアクティベーショ

32
00:01:26,696 --> 00:01:30,181
ンに必要な値が y であ るとします。 たとえば、y

33
00:01:30,181 --> 00:01:33,020
は 0 または 1 である可能性があります。

34
00:01:33,020 --> 00:01:35,960
したがって、単一のトレーニング例に対するこ

35
00:01:35,960 --> 00:01:39,040
のネットワークのコストは AL-y2 です。

36
00:01:39,040 --> 00:01:46,120
その 1 つのトレーニング例のコストを c0 と表記します。

37
00:01:46,120 --> 00:01:49,903
念のため言っておきますが、この最後の活性化は、前のニューロ

38
00:01:49,903 --> 00:01:53,295
ンの活性化とバイアス (bL と呼ぶことにします)

39
00:01:53,295 --> 00:01:56,165
を掛けた重み (wL と呼ぶことにします)

40
00:01:56,165 --> 00:01:57,600
によって決定されます。

41
00:01:57,600 --> 00:01:59,382
次に、それをシグモイドや ReLU

42
00:01:59,382 --> 00:02:01,560
などの特別な非線形関数を通してポンプします。

43
00:02:01,560 --> 00:02:04,488
実際、この重み付けされた合計に、関連するアクテ

44
00:02:04,488 --> 00:02:07,289
ィベーションと同じ上付き 文字を付けた z

45
00:02:07,289 --> 00:02:10,600
のような特別な名前を付けると、作業が簡単になります。

46
00:02:10,600 --> 00:02:13,870
これには多くの用語が含まれますが、これを概念化す

47
00:02:13,870 --> 00:02:16,186
ると、重み、以前のアクション、バ

48
00:02:16,186 --> 00:02:18,366
イアスがすべて一緒になって z

49
00:02:18,366 --> 00:02:21,773
を計算するために使用され、それによって a が計

50
00:02:21,773 --> 00:02:25,452
算され、最後に定数 y とともに次のようになります。

51
00:02:25,452 --> 00:02:27,360
私たちがコストを計算します。

52
00:02:27,360 --> 00:02:31,640
そしてもちろん、AL-1 はそれ自体の重みやバイアスなどの影

53
00:02:31,640 --> 00:02:35,920
響を受けますが、今はそれに焦点を当てるつもりはありません。

54
00:02:35,920 --> 00:02:38,120
これらはすべて単なる数字ですよね？

55
00:02:38,120 --> 00:02:40,040
そして、それぞれが独自の小さな数直線

56
00:02:40,040 --> 00:02:41,960
を持っていると考えるとよいでしょう。

57
00:02:41,960 --> 00:02:46,035
私たちの最初の目標は、重み wL の小さな変化に対して

58
00:02:46,035 --> 00:02:49,820
コスト関数がどの程度敏感であるかを理解することです。

59
00:02:49,820 --> 00:02:55,740
別の言い方をすると、wL に関する c の導関数は何ですか?

60
00:02:55,740 --> 00:02:58,045
この del w という用語を見たときは、0

61
00:02:58,045 --> 00:03:00,952
による変更など、w に対する小さなナッジを意味すると考えて

62
00:03:00,952 --> 00:03:01,554
くださ い。

63
00:03:01,554 --> 00:03:03,789
01 であり、この del c

64
00:03:03,789 --> 00:03:07,422
という用語は、結果として生じるコストの微調整を意味す

65
00:03:07,422 --> 00:03:08,820
ると考えてください。

66
00:03:08,820 --> 00:03:10,900
私たちが知りたいのはそれらの比率です。

67
00:03:10,900 --> 00:03:15,064
概念的には、wL へのこの小さなナッジは zL

68
00:03:15,064 --> 00:03:19,055
へのナッジを引き起こし 、それが今度は AL

69
00:03:19,055 --> 00:03:23,220
へのナッジを引き起こし、コストに直接影響します。

70
00:03:23,220 --> 00:03:27,353
そこで、最初に zL に対する小さな変化とこの小さな変化

71
00:03:27,353 --> 00:03:30,346
w の比、つ まり wL に対する zL

72
00:03:30,346 --> 00:03:33,340
の導関数を調べることで物事を細分化します。

73
00:03:33,340 --> 00:03:37,265
同様に、次に、AL への変化とそれを引き起こした

74
00:03:37,265 --> 00:03:41,818
z L の小さな変化の比率、および c への最後のナッジ

75
00:03:41,818 --> 00:03:45,900
と AL へのこの中間のナッジとの比率を考慮します。

76
00:03:45,900 --> 00:03:51,831
これは連鎖則であり、これら 3 つの比率を乗算すると、

77
00:03:51,831 --> 00:03:57,340
wL の小さな変化に対する c の感度が得られます。

78
00:03:57,340 --> 00:04:00,632
現在、画面上にはたくさんのシンボルが表示されていますが

79
00:04:00,632 --> 00:04:03,924
、これから関連する導関数を計 算するので、それらがすべ

80
00:04:03,924 --> 00:04:07,460
て明確であることを確認するために少し時間を取ってください。

81
00:04:07,460 --> 00:04:14,220
AL に関する c の導関数は、2AL-y となります。

82
00:04:14,220 --> 00:04:17,656
これは、そのサイズがネットワークの出力と私たちが望

83
00:04:17,656 --> 00:04:20,681
むものとの差に比例 することを意味します。

84
00:04:20,681 --> 00:04:23,843
そのため、その出力が大きく異なる場合、わずか

85
00:04:23,843 --> 00:04:27,280
な変更でも最終的なコスト関数に大きな影響を与える可

86
00:04:27,280 --> 00:04:28,380
能性があります。

87
00:04:28,380 --> 00:04:33,088
zL に関する AL の導関数は、シグモイド関数

88
00:04:33,088 --> 00:04:37,420
、または使用する非線形性の導関数にすぎません。

89
00:04:37,420 --> 00:04:46,180
wL に関する zL の導関数は AL-1 になります。

90
00:04:46,180 --> 00:04:48,846
あなたはどうか知りませんが、じっくりと時間

91
00:04:48,846 --> 00:04:51,513
をかけて公式の意味を 思い出さずに、公式に

92
00:04:51,513 --> 00:04:54,180
どっぷりと浸かってしまいがちだと思います。

93
00:04:54,180 --> 00:04:58,782
この最後の導関数の場合、重みへの小さな微調整が最後の層

94
00:04:58,782 --> 00:05:03,220
に与える影響の量は、前のニューロンの強さに依存します。

95
00:05:03,220 --> 00:05:06,212
覚えておいてください、ここで、ニューロンが一緒に発火

96
00:05:06,212 --> 00:05:09,320
し、一緒にワイヤリングするというアイデアが登場します。

97
00:05:09,320 --> 00:05:13,089
そして、これはすべて、特定の 1 つのトレーニング例

98
00:05:13,089 --> 00:05:16,580
のコストのみを wL に関して導関数したものです。

99
00:05:16,580 --> 00:05:20,704
フルコスト関数には、多くの異なるトレーニング例にわたるすべ

100
00:05:20,704 --> 00:05:24,690
てのコストの平均が含まれるため、その導関数では、すべての

101
00:05:24,690 --> 00:05:28,540
トレーニング例にわたってこの式を平均する必要があります。

102
00:05:28,540 --> 00:05:31,828
もちろん、これは勾配ベクトルの 1

103
00:05:31,828 --> 00:05:34,751
つのコンポーネントにすぎず、す

104
00:05:34,751 --> 00:05:38,770
べての重みとバイアスに関するコスト関数の偏導

105
00:05:38,770 --> 00:05:40,780
関数から構築されます。

106
00:05:40,780 --> 00:05:43,749
しかし、これは必要な偏導関数の 1 つにすぎ

107
00:05:43,749 --> 00:05:46,460
ませんが、作業の 50% 以上を占めます。

108
00:05:46,460 --> 00:05:50,300
たとえば、バイアスに対する感度はほぼ同じです。

109
00:05:50,300 --> 00:05:54,640
この del z del w という用語を

110
00:05:54,640 --> 00:05:58,980
del z del b に変更するだけです。

111
00:05:58,980 --> 00:06:04,700
関連する式を見ると、その微分値は 1 になります。

112
00:06:04,700 --> 00:06:08,526
また、ここで逆方向に伝播するというアイデアが登

113
00:06:08,526 --> 00:06:12,353
場しますが、このコスト 関数が前の層のアクティ

114
00:06:12,353 --> 00:06:16,180
ブ化に対してどれほど敏感であるかがわかります。

115
00:06:16,180 --> 00:06:20,887
つまり、連鎖ルール式におけるこの初期導関数、つまり前

116
00:06:20,887 --> 00:06:25,420
の起動に対する z の感度が重み wL になります。

117
00:06:25,420 --> 00:06:28,982
繰り返しますが、前の層のアクティブ化に直接影響を

118
00:06:28,982 --> 00:06:32,100
与えることは できませんが、同じチェーン

119
00:06:32,100 --> 00:06:34,624
ルールのアイデアを逆方向に反復し

120
00:06:34,624 --> 00:06:38,187
続けるだけで、コスト関数がどの程度敏感であるかを

121
00:06:38,187 --> 00:06:41,601
確認できるた め、追跡するのには役立ちます。

122
00:06:41,601 --> 00:06:43,680
以前の重みと以前のバイアス。

123
00:06:43,680 --> 00:06:46,154
すべての層には 1 つのニューロンがあり、実際

124
00:06:46,154 --> 00:06:48,629
のネットワークでは事態は 指数関数的に複雑にな

125
00:06:48,629 --> 00:06:51,320
るため、これは単純すぎる例だと思うかもしれません。

126
00:06:51,320 --> 00:06:53,986
しかし、正直に言うと、レイヤーに複数のニューロンを与

127
00:06:53,986 --> 00:06:56,140
えてもそれほど大きな変化 はありません。

128
00:06:56,140 --> 00:06:58,807
実際には、追跡するインデックスがさらにいくつか増える

129
00:06:58,807 --> 00:06:59,320
だけです。

130
00:06:59,320 --> 00:07:03,779
特定の層の活性化が単に AL であるのではなく、その層

131
00:07:03,779 --> 00:07:07,920
のどのニューロンであるかを示す添え字も付けられます。

132
00:07:07,920 --> 00:07:10,290
文字 k を使用してレイヤー L-1

133
00:07:10,290 --> 00:07:13,408
にインデックスを付け、j を使用してレイヤー L

134
00:07:13,408 --> 00:07:15,280
にインデックスを付けましょう。

135
00:07:15,280 --> 00:07:18,893
コストについては、再び望ましい出力が何であるか

136
00:07:18,893 --> 00:07:22,506
を調べますが、今回は、 これらの最後の層のアク

137
00:07:22,506 --> 00:07:26,120
ティブ化と望ましい出力の差の二乗を合計します。

138
00:07:26,120 --> 00:07:29,700
つまり、ALj から yj の

139
00:07:29,700 --> 00:07:33,280
2 乗を引いた合計を計算します。

140
00:07:33,280 --> 00:07:36,395
さらに多くの重みがあるため、それぞれがどこにあるかを追

141
00:07:36,395 --> 00:07:39,510
跡するためにさらに いくつかのインデックスを持つ必要が

142
00:07:39,510 --> 00:07:42,048
あるため、この k 番目のニューロンを j

143
00:07:42,048 --> 00:07:45,047
番目のニューロンに接続するエッジの重みを WLjk

144
00:07:45,047 --> 00:07:45,740
と呼びます。

145
00:07:45,740 --> 00:07:48,426
これらのインデックスは最初は少し時代遅れに感じる

146
00:07:48,426 --> 00:07:51,113
かもしれませんが、パート 1 のビデオで説明した

147
00:07:51,113 --> 00:07:53,800
重み行列のインデックス付け方法と一致しています。

148
00:07:53,800 --> 00:07:57,655
前と同じように、関連する重み付けされた合計に z などの名

149
00:07:57,655 --> 00:08:01,381
前を付けると、最後の層のアクティブ化が z に適用される

150
00:08:01,381 --> 00:08:04,980
シグモイドのような特別な関数にすぎなくなるので便利です。

151
00:08:04,980 --> 00:08:08,460
私が言いたいことはわかると思いますが、これらはすべて、

152
00:08:08,460 --> 00:08:11,940
レイヤーごとに 1 ニュー ロンの場合に以前に作成した

153
00:08:11,940 --> 00:08:15,420
方程式と本質的に同じですが、少し複雑に見えるだけです。

154
00:08:15,420 --> 00:08:19,559
そして実際、特定の重みに対するコストの感度を表す連

155
00:08:19,559 --> 00:08:23,540
鎖ルールの導関数式は、本質的に同じように見えます。

156
00:08:23,540 --> 00:08:26,480
必要に応じて、立ち止まってそれぞれ

157
00:08:26,480 --> 00:08:29,420
の用語について考えてみてください。

158
00:08:29,420 --> 00:08:33,781
ただし、ここで変わるのは、レイヤー L-1 のアクテ

159
00:08:33,781 --> 00:08:37,820
ィベーションの 1 つに関するコストの導関数です。

160
00:08:37,820 --> 00:08:40,743
この場合の違いは、ニューロンが複数の異なるパ

161
00:08:40,743 --> 00:08:43,540
スを通じてコスト関数に影響を与えることです。

162
00:08:43,540 --> 00:08:49,347
つまり、コスト関数の役割を果たす AL0 に影響を与え

163
00:08:49,347 --> 00:08:54,947
る一方で、同じくコスト関数の役割を果たす AL1 に

164
00:08:54,947 --> 00:09:00,340
も影響を与えるため、それらを合計する必要があります。

165
00:09:00,340 --> 00:09:03,680
そして、まあ、それだけです。

166
00:09:03,680 --> 00:09:07,215
この最後から 2 番目の層のアクティベーションに対するコ

167
00:09:07,215 --> 00:09:10,628
スト関数の感度がわかったら、その層に入力されるすべての

168
00:09:10,628 --> 00:09:13,920
重みとバイアスに対してこのプロセスを繰り返すだけです。

169
00:09:13,920 --> 00:09:15,420
だから自分の背中を押してください！

170
00:09:15,420 --> 00:09:18,034
これらすべてが理解できるのであれば、ニューラル

171
00:09:18,034 --> 00:09:20,758
ネットワークの学習方法の背後 にある主力であるバッ

172
00:09:20,758 --> 00:09:23,700
クプロパゲーションの中心部を深く調べたことになります。

173
00:09:23,700 --> 00:09:27,473
これらのチェーン ルール式により、勾配内の各コンポー

174
00:09:27,473 --> 00:09:31,246
ネントを決定する導関数が得 られ、下り坂を繰り返して

175
00:09:31,246 --> 00:09:35,020
ネットワークのコストを最小限に抑えることができます。

176
00:09:35,020 --> 00:09:36,333
座ってこれらすべてについて考えると、これはあなたの

177
00:09:36,333 --> 00:09:37,646
心を包み込む複雑な層にな るため、頭がすべてを消化

178
00:09:37,646 --> 00:09:38,960
するのに時間がかかっても心配する必要はありません。

