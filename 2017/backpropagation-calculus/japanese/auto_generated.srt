1
00:00:04,020 --> 00:00:06,221
ここでの厳密な前提条件は、バックプロパゲーション 

2
00:00:06,221 --> 00:00:08,687
アルゴリズムの直 感的なウォークスルーを提供するパート 

3
00:00:08,687 --> 00:00:09,920
3 を視聴していることです。

4
00:00:11,040 --> 00:00:12,581
ここではもう少し形式的に、関連す

5
00:00:12,581 --> 00:00:14,220
る微積分について詳しく説明します。

6
00:00:14,820 --> 00:00:17,013
少なくとも少し混乱するのは普通のことなので、定

7
00:00:17,013 --> 00:00:19,206
期的に立ち止まって熟考 するという信条は、他の

8
00:00:19,206 --> 00:00:21,400
場所と同じようにここでも確かに当てはまります。

9
00:00:21,940 --> 00:00:24,839
私たちの主な目標は、機械学習の人々がネットワークの文脈で

10
00:00:24,839 --> 00:00:27,738
微積分の連鎖則につ いてどのように一般的に考えているかを

11
00:00:27,738 --> 00:00:29,912
示すことです。 これは、ほとんどの微積分 

12
00:00:29,912 --> 00:00:32,811
入門コースがこの主題にアプローチする方法とは異なる雰囲気

13
00:00:32,811 --> 00:00:33,640
を持っています。

14
00:00:34,340 --> 00:00:36,588
関連する微積分に抵抗がある人のために、このト 

15
00:00:36,588 --> 00:00:38,740
ピックに関するシリーズ全体を用意しています。

16
00:00:39,960 --> 00:00:43,063
各層に 1 つのニューロンが含まれる、非 

17
00:00:43,063 --> 00:00:46,020
常に単純なネットワークから始めましょう。

18
00:00:46,320 --> 00:00:48,382
このネットワークは 3 つの重みと 3 

19
00:00:48,382 --> 00:00:50,135
つのバイアスによって決定されます。

20
00:00:50,135 --> 00:00:52,920
 私たちの 目標は、コスト関数がこれらの変数に対してど

21
00:00:52,920 --> 00:00:54,880
の程度敏感であるかを理解することです。

22
00:00:55,419 --> 00:00:58,932
そうすることで、これらの項に対するどの調整がコスト関数 

23
00:00:58,932 --> 00:01:02,320
の最も効率的な減少を引き起こすかを知ることができます。

24
00:01:02,320 --> 00:01:04,840
最後の 2 つのニューロン間の接続にのみ焦点を当てます。

25
00:01:05,980 --> 00:01:08,473
最後のニューロンの活性化に上付き文字 

26
00:01:08,473 --> 00:01:11,360
L を付けて、どの層にあるかを示しましょう。

27
00:01:11,680 --> 00:01:15,560
したがって、前のニューロンの活性化は AL-1 です。

28
00:01:16,360 --> 00:01:18,586
これらは指数ではなく、後で別のインデックスの

29
00:01:18,586 --> 00:01:20,813
添字を保存したいので、 ここで話しているもの

30
00:01:20,813 --> 00:01:23,040
にインデックスを付けるための単なる方法です。

31
00:01:23,720 --> 00:01:26,501
特定のトレーニング例でこの最後のアクティベーショ

32
00:01:26,501 --> 00:01:28,703
ンに必要な値が y であ るとします。

33
00:01:28,703 --> 00:01:32,180
 たとえば、y は 0 または 1 である可能性があります。

34
00:01:32,840 --> 00:01:35,965
したがって、単一のトレーニング例に対するこ

35
00:01:35,965 --> 00:01:39,240
のネットワークのコストは AL-y2 です。

36
00:01:40,260 --> 00:01:44,380
その 1 つのトレーニング例のコストを c0 と表記します。

37
00:01:45,900 --> 00:01:49,755
念のため言っておきますが、この最後の活性化は、前のニューロ

38
00:01:49,755 --> 00:01:53,212
ンの活性化とバイアス (bL と呼ぶことにします) 

39
00:01:53,212 --> 00:01:56,137
を掛けた重み (wL と呼ぶことにします) 

40
00:01:56,137 --> 00:01:57,600
によって決定されます。

41
00:01:57,600 --> 00:01:59,274
次に、それをシグモイドや ReLU 

42
00:01:59,274 --> 00:02:01,320
などの特別な非線形関数を通してポンプします。

43
00:02:01,800 --> 00:02:04,236
実際、この重み付けされた合計に、関連するアクテ

44
00:02:04,236 --> 00:02:06,566
ィベーションと同じ上付き 文字を付けた z 

45
00:02:06,566 --> 00:02:09,320
のような特別な名前を付けると、作業が簡単になります。

46
00:02:10,380 --> 00:02:13,326
これには多くの用語が含まれますが、これを概念化す

47
00:02:13,326 --> 00:02:15,413
ると、重み、以前のアクション、バ 

48
00:02:15,413 --> 00:02:17,377
イアスがすべて一緒になって z 

49
00:02:17,377 --> 00:02:20,446
を計算するために使用され、それによって a が計 

50
00:02:20,446 --> 00:02:23,638
算され、最後に定数 y とともに次のようになります。

51
00:02:23,638 --> 00:02:25,480
 私たちがコストを計算します。

52
00:02:27,340 --> 00:02:31,200
そしてもちろん、AL-1 はそれ自体の重みやバイアスなどの影

53
00:02:31,200 --> 00:02:35,060
 響を受けますが、今はそれに焦点を当てるつもりはありません。

54
00:02:35,700 --> 00:02:37,620
これらはすべて単なる数字ですよね？

55
00:02:38,060 --> 00:02:39,550
そして、それぞれが独自の小さな数直線

56
00:02:39,550 --> 00:02:41,040
を持っていると考えるとよいでしょう。

57
00:02:41,720 --> 00:02:45,494
私たちの最初の目標は、重み wL の小さな変化に対して 

58
00:02:45,494 --> 00:02:49,000
コスト関数がどの程度敏感であるかを理解することです。

59
00:02:49,540 --> 00:02:54,860
別の言い方をすると、wL に関する c の導関数は何ですか?

60
00:02:55,600 --> 00:02:58,205
この del w という用語を見たときは、0 

61
00:02:58,205 --> 00:03:01,263
による変更など、w に対する小さなナッジを意味すると考

62
00:03:01,263 --> 00:03:03,982
えてくださ い。01 であり、この del c 

63
00:03:03,982 --> 00:03:07,040
という用語は、結果として生じるコストの微調整を意味する

64
00:03:07,040 --> 00:03:08,060
と考えてください。

65
00:03:08,060 --> 00:03:10,220
私たちが知りたいのはそれらの比率です。

66
00:03:11,260 --> 00:03:14,633
概念的には、wL へのこの小さなナッジは zL 

67
00:03:14,633 --> 00:03:17,866
へのナッジを引き起こし 、それが今度は AL 

68
00:03:17,866 --> 00:03:21,240
へのナッジを引き起こし、コストに直接影響します。

69
00:03:23,120 --> 00:03:27,237
そこで、最初に zL に対する小さな変化とこの小さな変化 

70
00:03:27,237 --> 00:03:30,218
w の比、つ まり wL に対する zL 

71
00:03:30,218 --> 00:03:33,200
の導関数を調べることで物事を細分化します。

72
00:03:33,200 --> 00:03:36,781
同様に、次に、AL への変化とそれを引き起こした 

73
00:03:36,781 --> 00:03:40,935
z L の小さな変化の比率、および c への最後のナッジ 

74
00:03:40,935 --> 00:03:44,660
と AL へのこの中間のナッジとの比率を考慮します。

75
00:03:45,740 --> 00:03:50,614
これは連鎖則であり、これら 3 つの比率を乗算すると、 

76
00:03:50,614 --> 00:03:55,140
wL の小さな変化に対する c の感度が得られます。

77
00:03:56,880 --> 00:03:59,924
現在、画面上にはたくさんのシンボルが表示されていますが

78
00:03:59,924 --> 00:04:02,969
、これから関連する導関数を計 算するので、それらがすべ

79
00:04:02,969 --> 00:04:06,240
て明確であることを確認するために少し時間を取ってください。

80
00:04:07,440 --> 00:04:14,180
AL に関する c の導関数は、2AL-y となります。

81
00:04:14,180 --> 00:04:17,325
これは、そのサイズがネットワークの出力と私たちが望

82
00:04:17,325 --> 00:04:19,967
むものとの差に比例 することを意味します。

83
00:04:19,967 --> 00:04:22,987
 そのため、その出力が大きく異なる場合、わずか 

84
00:04:22,987 --> 00:04:26,133
な変更でも最終的なコスト関数に大きな影響を与える可

85
00:04:26,133 --> 00:04:27,140
能性があります。

86
00:04:27,840 --> 00:04:32,829
zL に関する AL の導関数は、シグモイド関数 

87
00:04:32,829 --> 00:04:37,420
、または使用する非線形性の導関数にすぎません。

88
00:04:37,420 --> 00:04:46,160
wL に関する zL の導関数は AL-1 になります。

89
00:04:46,160 --> 00:04:48,580
あなたはどうか知りませんが、じっくりと時間

90
00:04:48,580 --> 00:04:51,000
をかけて公式の意味を 思い出さずに、公式に

91
00:04:51,000 --> 00:04:53,420
どっぷりと浸かってしまいがちだと思います。

92
00:04:53,920 --> 00:04:58,450
この最後の導関数の場合、重みへの小さな微調整が最後の層 

93
00:04:58,450 --> 00:05:02,820
に与える影響の量は、前のニューロンの強さに依存します。

94
00:05:03,380 --> 00:05:05,783
覚えておいてください、ここで、ニューロンが一緒に発火

95
00:05:05,783 --> 00:05:08,280
し、一緒にワイヤリングするというアイデアが登場します。

96
00:05:09,200 --> 00:05:12,585
そして、これはすべて、特定の 1 つのトレーニング例 

97
00:05:12,585 --> 00:05:15,720
のコストのみを wL に関して導関数したものです。

98
00:05:16,440 --> 00:05:20,653
フルコスト関数には、多くの異なるトレーニング例にわたるすべ 

99
00:05:20,653 --> 00:05:24,727
てのコストの平均が含まれるため、その導関数では、すべての 

100
00:05:24,727 --> 00:05:28,660
トレーニング例にわたってこの式を平均する必要があります。

101
00:05:28,660 --> 00:05:31,239
もちろん、これは勾配ベクトルの 1 

102
00:05:31,239 --> 00:05:33,531
つのコンポーネントにすぎず、す 

103
00:05:33,531 --> 00:05:36,683
べての重みとバイアスに関するコスト関数の偏導

104
00:05:36,683 --> 00:05:38,260
関数から構築されます。

105
00:05:40,640 --> 00:05:43,055
しかし、これは必要な偏導関数の 1 つにすぎ 

106
00:05:43,055 --> 00:05:45,260
ませんが、作業の 50% 以上を占めます。

107
00:05:46,340 --> 00:05:49,720
たとえば、バイアスに対する感度はほぼ同じです。

108
00:05:50,040 --> 00:05:52,530
この del z del w という用語を 

109
00:05:52,530 --> 00:05:55,020
del z del b に変更するだけです。

110
00:05:58,420 --> 00:06:02,400
関連する式を見ると、その微分値は 1 になります。

111
00:06:06,140 --> 00:06:09,340
また、ここで逆方向に伝播するというアイデアが登

112
00:06:09,340 --> 00:06:12,540
場しますが、このコスト 関数が前の層のアクティ

113
00:06:12,540 --> 00:06:15,740
ブ化に対してどれほど敏感であるかがわかります。

114
00:06:15,740 --> 00:06:20,793
つまり、連鎖ルール式におけるこの初期導関数、つまり前 

115
00:06:20,793 --> 00:06:25,660
の起動に対する z の感度が重み wL になります。

116
00:06:26,640 --> 00:06:29,722
繰り返しますが、前の層のアクティブ化に直接影響を

117
00:06:29,722 --> 00:06:32,420
与えることは できませんが、同じチェーン 

118
00:06:32,420 --> 00:06:34,604
ルールのアイデアを逆方向に反復し 

119
00:06:34,604 --> 00:06:37,687
続けるだけで、コスト関数がどの程度敏感であるかを

120
00:06:37,687 --> 00:06:40,513
確認できるた め、追跡するのには役立ちます。

121
00:06:40,513 --> 00:06:42,440
 以前の重みと以前のバイアス。

122
00:06:43,180 --> 00:06:45,719
すべての層には 1 つのニューロンがあり、実際

123
00:06:45,719 --> 00:06:48,259
のネットワークでは事態は 指数関数的に複雑にな

124
00:06:48,259 --> 00:06:51,020
るため、これは単純すぎる例だと思うかもしれません。

125
00:06:51,700 --> 00:06:54,086
しかし、正直に言うと、レイヤーに複数のニューロンを与

126
00:06:54,086 --> 00:06:55,922
えてもそれほど大きな変化 はありません。

127
00:06:55,922 --> 00:06:58,309
 実際には、追跡するインデックスがさらにいくつか増え

128
00:06:58,309 --> 00:06:58,860
るだけです。

129
00:06:59,380 --> 00:07:03,414
特定の層の活性化が単に AL であるのではなく、その層 

130
00:07:03,414 --> 00:07:07,160
のどのニューロンであるかを示す添え字も付けられます。

131
00:07:07,160 --> 00:07:09,497
文字 k を使用してレイヤー L-1 

132
00:07:09,497 --> 00:07:12,574
にインデックスを付け、j を使用してレイヤー L 

133
00:07:12,574 --> 00:07:14,420
にインデックスを付けましょう。

134
00:07:15,260 --> 00:07:18,566
コストについては、再び望ましい出力が何であるか

135
00:07:18,566 --> 00:07:21,873
を調べますが、今回は、 これらの最後の層のアク

136
00:07:21,873 --> 00:07:25,180
ティブ化と望ましい出力の差の二乗を合計します。

137
00:07:26,080 --> 00:07:28,460
つまり、ALj から yj の 

138
00:07:28,460 --> 00:07:30,840
2 乗を引いた合計を計算します。

139
00:07:33,040 --> 00:07:36,010
さらに多くの重みがあるため、それぞれがどこにあるかを追

140
00:07:36,010 --> 00:07:38,980
跡するためにさらに いくつかのインデックスを持つ必要が

141
00:07:38,980 --> 00:07:41,400
あるため、この k 番目のニューロンを j 

142
00:07:41,400 --> 00:07:44,260
番目のニューロンに接続するエッジの重みを WLjk 

143
00:07:44,260 --> 00:07:44,920
と呼びます。

144
00:07:45,620 --> 00:07:48,120
これらのインデックスは最初は少し時代遅れに感じる

145
00:07:48,120 --> 00:07:50,620
かもしれませんが、パート 1 のビデオで説明した

146
00:07:50,620 --> 00:07:53,120
重み行列のインデックス付け方法と一致しています。

147
00:07:53,620 --> 00:07:57,254
前と同じように、関連する重み付けされた合計に z などの名 

148
00:07:57,254 --> 00:08:00,767
前を付けると、最後の層のアクティブ化が z に適用される 

149
00:08:00,767 --> 00:08:04,160
シグモイドのような特別な関数にすぎなくなるので便利です。

150
00:08:04,660 --> 00:08:07,666
私が言いたいことはわかると思いますが、これらはすべて、

151
00:08:07,666 --> 00:08:10,673
レイヤーごとに 1 ニュー ロンの場合に以前に作成した

152
00:08:10,673 --> 00:08:13,680
方程式と本質的に同じですが、少し複雑に見えるだけです。

153
00:08:15,440 --> 00:08:19,508
そして実際、特定の重みに対するコストの感度を表す連 

154
00:08:19,508 --> 00:08:23,420
鎖ルールの導関数式は、本質的に同じように見えます。

155
00:08:23,920 --> 00:08:25,380
必要に応じて、立ち止まってそれぞれ

156
00:08:25,380 --> 00:08:26,840
の用語について考えてみてください。

157
00:08:28,979 --> 00:08:32,967
ただし、ここで変わるのは、レイヤー L-1 のアクテ 

158
00:08:32,967 --> 00:08:36,659
ィベーションの 1 つに関するコストの導関数です。

159
00:08:37,780 --> 00:08:40,386
この場合の違いは、ニューロンが複数の異なるパ 

160
00:08:40,386 --> 00:08:42,880
スを通じてコスト関数に影響を与えることです。

161
00:08:44,680 --> 00:08:49,125
つまり、コスト関数の役割を果たす AL0 に影響を与え 

162
00:08:49,125 --> 00:08:53,412
る一方で、同じくコスト関数の役割を果たす AL1 に 

163
00:08:53,412 --> 00:08:57,540
も影響を与えるため、それらを合計する必要があります。

164
00:08:59,820 --> 00:09:03,040
そして、まあ、それだけです。

165
00:09:03,500 --> 00:09:06,731
この最後から 2 番目の層のアクティベーションに対するコ 

166
00:09:06,731 --> 00:09:09,851
スト関数の感度がわかったら、その層に入力されるすべての 

167
00:09:09,851 --> 00:09:12,860
重みとバイアスに対してこのプロセスを繰り返すだけです。

168
00:09:13,900 --> 00:09:14,960
だから自分の背中を押してください！

169
00:09:15,300 --> 00:09:17,630
これらすべてが理解できるのであれば、ニューラル 

170
00:09:17,630 --> 00:09:20,058
ネットワークの学習方法の背後 にある主力であるバッ

171
00:09:20,058 --> 00:09:22,680
クプロパゲーションの中心部を深く調べたことになります。

172
00:09:23,300 --> 00:09:26,633
これらのチェーン ルール式により、勾配内の各コンポー

173
00:09:26,633 --> 00:09:29,966
ネントを決定する導関数が得 られ、下り坂を繰り返して

174
00:09:29,966 --> 00:09:33,300
ネットワークのコストを最小限に抑えることができます。

175
00:09:34,300 --> 00:09:37,113
座ってこれらすべてについて考えると、これはあなたの

176
00:09:37,113 --> 00:09:39,926
心を包み込む複雑な層にな るため、頭がすべてを消化

177
00:09:39,926 --> 00:09:42,740
するのに時間がかかっても心配する必要はありません。

