[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "La suposición difícil aquí es que ha visto la parte 3, que ofrece un recorrido intuitivo por el algoritmo de retropropagación.",
  "from_community_srt": "Aqui la suposición dificil es que tu has mirado la parte 3, dando una demostración intuitiva del algoritmo de la retropropagación.",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Aquí nos volvemos un poco más formales y nos sumergimos en el cálculo relevante.",
  "from_community_srt": "Aquí, obtenermos algo un pocomo mas formal y profundo en lo relavanta al cálculo.",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "Es normal que esto sea al menos un poco confuso, por lo que el mantra de hacer una pausa y reflexionar con regularidad ciertamente se aplica tanto aquí como en cualquier otro lugar.",
  "from_community_srt": "Es normal estar un poco confundido para esto, asi que el mantra, pausa y piensa regularmente, ciertamente se aplica mucho aquí como en ningun otro lugar.",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "Nuestro objetivo principal es mostrar cómo las personas en el aprendizaje automático piensan comúnmente sobre la regla de la cadena del cálculo en el contexto de las redes, lo cual tiene una sensación diferente de cómo la mayoría de los cursos de introducción al cálculo abordan el tema.",
  "from_community_srt": "Nuestra meta principal es enseñarle a la gente el aprendizaje de máquinas, comummente se piensa en la regla de la cadena del cálculo tiene una diferente sensación por cuanto los cursos introductorios de calculo tratan el tema .",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Para aquellos de ustedes que no se sienten cómodos con el cálculo relevante, tengo una serie completa sobre el tema.",
  "from_community_srt": "para los que se sientas incomodos con la relación del cálculo, tengo una serie completa sobre el tema.",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Comencemos con una red extremadamente simple, donde cada capa tiene una sola neurona.",
  "from_community_srt": "Digamos que inicias con una red extremadamente simple, una en la cual cada capa tiene una sola  neurona dentro.",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Esta red está determinada por tres ponderaciones y tres sesgos, y nuestro objetivo es comprender qué tan sensible es la función de costos a estas variables.",
  "from_community_srt": "Entonces esta red en particular es determinada por 3 pesos y 3 biases, y nuestra meta es entender que tan sensitiva es la función de coste para estas variables",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way, we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "De esa manera, sabemos qué ajustes a esos términos causarán la disminución más eficiente de la función de costos.",
  "from_community_srt": "de esa manera sabemos qué ajustes a estos términos van a causar el decresimiento mas eficiente a la función de coste.",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "And we're just going to focus on the connection between the last two neurons.",
  "translatedText": "Y sólo nos centraremos en la conexión entre las dos últimas neuronas.",
  "from_community_srt": "Y solo nos estamos enfocando en la conecxión entre las dos neuronas.",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in, so the activation of the previous neuron is Al-1.",
  "translatedText": "Etiquetemos la activación de esa última neurona con un superíndice L, indicando en qué capa se encuentra, por lo que la activación de la neurona anterior es Al-1.",
  "from_community_srt": "Etiquetemos la activación de la última neurona  a con un superscript L, indicando en que capas esta (a^(L)), asi que la activación de esta neurona previa es a^(L-1)",
  "n_reviews": 0,
  "start": 65.98,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Estos no son exponentes, son sólo una forma de indexar lo que estamos hablando, ya que quiero guardar subíndices para diferentes índices más adelante.",
  "from_community_srt": "NO hay exponente, solo es una manera de indexar lo que estamos hablando, ya que quiero guardar los subscripts para diferentes indices  para después,",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Digamos que el valor que queremos que tenga esta última activación para un ejemplo de entrenamiento determinado es y, por ejemplo, y podría ser 0 o 1.",
  "from_community_srt": "Digamos que el valor que queremos, esta última activación sea para un ejemplo de entrenamiento dado es   y. Por ejemplo, y podría ser 0 o 1.",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is Al-y2.",
  "translatedText": "Entonces, el costo de esta red para un solo ejemplo de entrenamiento es Al-y2.",
  "from_community_srt": "Así que el coste de esta red simple para un ejemplo de entrenamiento en particular es (a^(L) - y)^2.",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Denotaremos el costo de ese ejemplo de capacitación como c0.",
  "from_community_srt": "Denotaremos el coste de este ejemplo de entrenamiento como C_0.",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neuron's activation plus some bias, which I'll call BL.",
  "translatedText": "Como recordatorio, esta última activación está determinada por un peso, que llamaré WL, multiplicado por la activación de la neurona anterior más un sesgo, que llamaré BL.",
  "from_community_srt": "Como un recordatorio, esta última activación es determinada por el peso, el cual voy a llamar w^(L) multiplicado por la activación previa de la neurona, mas algún bias,",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "Y luego lo bombeas a través de alguna función no lineal especial como el sigmoide o ReLU.",
  "from_community_srt": "el cual llamaré b^(L), luego bombeas eso a travez de algúna función especial no linear como  una sigmoid o una ReLU.",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "De hecho, nos facilitará las cosas si le damos un nombre especial a esta suma ponderada, como z, con el mismo superíndice que las activaciones relevantes.",
  "from_community_srt": "Eso de echo va a hacer las cosas mas fáciles para nosotros si nostros le damos  un nombre especial a esta suma ponderada, algo como Z, con el mismo superscript como las activaciones relevantes,",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action and the bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "Son muchos términos, y una forma de conceptualizarlos es que el peso, la acción previa y el sesgo, todos juntos, se usan para calcular z, lo que a su vez nos permite calcular a, que finalmente, junto con una constante y, nos permite calculemos el costo.",
  "from_community_srt": "Asi que hay un mont{on de términos. Y una manera en la que podrías conceptualizar esto es que el peso, la activación previa, y el bias. en conjunto son usadas para calcular z, que luego nos permite calcular a, que finalmente, junto con la constante y, nos permite calcular el coste.",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course Al-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "Y, por supuesto, Al-1 está influenciado por su propio peso y sesgo y demás, pero no nos vamos a centrar en eso ahora.",
  "from_community_srt": "Y por su puesto, a^(L-1)  es influenciado por su propio peso y bias, y asi como los demás. Pero nosotros no nos vamos a enfocar eso justo ahora.",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Todos estos son sólo números, ¿verdad?",
  "from_community_srt": "Todos estos son solo números,",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "Y puede ser agradable pensar que cada uno tiene su propia recta numérica.",
  "from_community_srt": "verdad? y puede ser bueno pensar en cada uno como que si tubiesen su priopia línea de números pequeña.",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight WL.",
  "translatedText": "Nuestro primer objetivo es comprender qué tan sensible es la función de costos a pequeños cambios en nuestro peso WL.",
  "from_community_srt": "Nuesta primera meta es entender cuan sensitiva es la función de coste para un cambio pequeño cambio en nuestro peso w^(L).",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to WL?",
  "translatedText": "O dicho de otro modo, ¿cuál es la derivada de c con respecto a WL?",
  "from_community_srt": "O parafraseado diferentemente, cuál es la derivada de C respecto de w^(L).",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del W term, think of it as meaning some tiny nudge to W, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Cuando vea este término del W, piense que significa un pequeño empujón hacia W, como un cambio de 0,01, y piense que este término del c significa cualquier empujón resultante al costo.",
  "from_community_srt": "Cuando veas este término “∂w”, piensa que significa \"Algún pequeño empujon para w\", uno como de 0.01. Y piensa que este  término “∂C”  esta diciendo \"cualsea  el empujon resultante  al coste nosotros queremos su proporción.",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "Lo que queremos es su proporción.",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Conceptualmente, este pequeño empujón hacia WL provoca algún empujón hacia ZL, que a su vez provoca algún empujón hacia AL, lo que influye directamente en el costo.",
  "from_community_srt": "Conceptualmente, este pequeño empujón a w“∂C”  causa algun empujón a  z^(L) que luego causa algún cambio a a^(L), que directamente influye el costo.",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is, the derivative of ZL with respect to WL.",
  "translatedText": "Así que dividimos las cosas observando primero la relación entre un pequeño cambio en ZL y este pequeño cambio W, es decir, la derivada de ZL con respecto a WL.",
  "from_community_srt": "Asi que descomponemos ,primero mirando la proporción entre el cambio pequeño a z^(L) y el cambio pequeño en w^(L). Eso es, la derivada de  z^(L)  con respecto de w^(L).",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "Del mismo modo, luego se considera la relación entre el cambio a AL y el pequeño cambio en ZL que lo causó, así como la relación entre el empujón final a c y este empujón intermedio a AL.",
  "from_community_srt": "De la misma manera, luego consideras la proporción entre un cambio para a^(L)  y el cambio pequeño en z^(L)  que lo causo, también como la proporción entre el empujón final para C y este empujón intermedio para a^(L).",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of c to small changes in WL.",
  "translatedText": "Esta es la regla de la cadena, donde multiplicar estas tres razones nos da la sensibilidad de c a pequeños cambios en WL.",
  "from_community_srt": "Esto aquí es exactamente la regla de la cadena, donde multiplicando juntos estas tres proporcionas nos da la sensibilidad de C para un pequeño cambio en w^(L).",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "Entonces, en la pantalla ahora hay muchos símbolos, y tómate un momento para asegurarte de que está claro cuáles son, porque ahora vamos a calcular las derivadas relevantes.",
  "from_community_srt": "Asi que en la pantalla ahora mismo, hay un monton de símbolos asi que tomate un tiempo para asegurarte de que esta en claro todo lo que ellos son, porque ahora vamos a calcular las derivadas relevantes.",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "La derivada de c con respecto a AL resulta ser 2AL-y.",
  "from_community_srt": "La derivada de C con respecto de a^(L)  se transforma para ser 2(a^(L) - y).",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "Notice this means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Tenga en cuenta que esto significa que su tamaño es proporcional a la diferencia entre la producción de la red y lo que queremos que sea, por lo que si esa producción fue muy diferente, incluso los cambios más pequeños pueden tener un gran impacto en la función de costo final.",
  "from_community_srt": "Nota, esto significa que su tamaño es proporcional a a la diferencia entre la salida de la red, y la cosa que queremos que sea. Asi que si esa salida fue muy diferente, incluso ligeros cambios, tienden a tener un gran impacto en función de coste.",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "La derivada de AL con respecto a ZL es simplemente la derivada de nuestra función sigmoidea, o cualquier no linealidad que elijas usar.",
  "from_community_srt": "La derivada de a^(L)  con respecto de z^(L) es solo la derivada de nuestra función sigmoid. o cualquier cosa no lineal que escojas usar.",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "And the derivative of ZL with respect to WL comes out to be AL-1.",
  "translatedText": "Y la derivada de ZL respecto a WL resulta ser AL-1.",
  "from_community_srt": "Y la derivada de  z^(L)  con respecto de w^(L), en este caso viene solo a  ser a^(L-1).",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all mean.",
  "translatedText": "Ahora, no sé ustedes, pero creo que es fácil quedarse atrapado en las fórmulas sin tomarse un momento para sentarse y recordar lo que significan todas.",
  "from_community_srt": "Ahora no se tu, pero pienso que es facil simentar estar formulas en tu cabeza. sin tomar un momento para sentarte y recordad tu mismo lo que significan realmente.",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "En el caso de esta última derivada, la cantidad en que el pequeño empujón al peso influyó en la última capa depende de qué tan fuerte sea la neurona anterior.",
  "from_community_srt": "En el caso de esta última derivada, la medida de un empujon pequeño para este peso que influye la última capa depende en cuan fuerte la neurona previa es.",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "Recuerde, aquí es donde entra en juego la idea de neuronas que se activan juntas y se conectan entre sí.",
  "from_community_srt": "Recuerda, esto es donde  la idea de \"neuronas que se  prenden juntas se enlazan juntas\".",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to WL only of the cost for a specific single training example.",
  "translatedText": "Y todo esto es la derivada con respecto a WL únicamente del costo de un ejemplo de entrenamiento específico.",
  "from_community_srt": "Y todo esto es la derivda con respecto de w^(L)  solo para el costo, para un ejemplo de entrenamiento en concreto.",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Dado que la función de costo total implica promediar todos esos costos en muchos ejemplos de capacitación diferentes, su derivada requiere promediar esta expresión en todos los ejemplos de capacitación.",
  "from_community_srt": "Ya que la función de coste completa involucra promediar juntos todos estos costes a través de muchos ejemplos de entrenamiento, su derivada requiere promediar esta expresión que encontramos en todos los ejemplos de entrenamiento.",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "And of course, that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "Y, por supuesto, ese es sólo un componente del vector gradiente, que a su vez se construye a partir de las derivadas parciales de la función de costos con respecto a todas esas ponderaciones y sesgos.",
  "from_community_srt": "Y por su puesto eso es solo una componente de vector gradiente, que en si mismo esta contruido desde  las derivada parciales de la función coste con respecto a todos los peso y biases.",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "Pero aunque esa es sólo una de las muchas derivadas parciales que necesitamos, representa más del 50% del trabajo.",
  "from_community_srt": "Pero a pesar de que esto fue solo una de esas derivadas parcias que necesitamos, es mas del 50%  del trabajo.",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "La sensibilidad al sesgo, por ejemplo, es casi idéntica.",
  "from_community_srt": "La sensibilidad del bias, por ejemplo, es casi idéntica.",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Sólo necesitamos cambiar este término del z del w por a del z del b.",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "Y si nos fijamos en la fórmula correspondiente, esa derivada resulta ser 1.",
  "from_community_srt": "Solo necesitamos reemplezar este término  ∂z/∂w  por  ∂z/∂b, y  si  ves en la fórmula pertinente,",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "Además, y aquí es donde entra la idea de propagarse hacia atrás, se puede ver cuán sensible es esta función de costos a la activación de la capa anterior.",
  "from_community_srt": "esa derivada viene siendo 1 también, aquí es donde la idea de propagación hacia atras viene, puedes ver cuan sensible es la función de coste para la activación de la capa previa,",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight WL.",
  "translatedText": "Es decir, esta derivada inicial en la expresión de la regla de la cadena, la sensibilidad de z a la activación previa, resulta ser el peso WL.",
  "from_community_srt": "es decir, esta derivada inicial en la expansión de la regla de la cadena, la sensibilidad de z a la activación previa, viene siendo el peso w^(L).",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "Y nuevamente, aunque no vamos a poder influir directamente en la activación de esa capa anterior, es útil realizar un seguimiento, porque ahora podemos seguir iterando esta misma idea de la regla de la cadena hacia atrás para ver qué tan sensible es la función de costos a ponderaciones previas y sesgos previos.",
  "from_community_srt": "Y de nuevo, incluso si no seremos capaces de influenciar directamente esa activación, Es de ayuda  mantenerle el rastro, porque ahora solo podemos manterner iterando esta regla de la cadena hacia atras para ver cuan sensible es la función de costo para los pesos y biases previos.",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "Y podría pensar que este es un ejemplo demasiado simple, ya que todas las capas tienen una neurona y las cosas se volverán exponencialmente más complicadas para una red real.",
  "from_community_srt": "Y tu podrías pensar que esto es  un ejemplo demasiado simple ya que todas las capas solo tienen 1 neurona, y las cosas solo se van a poner exponencialmente más complicadas en la red verdadera.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "Pero, honestamente, no hay muchos cambios cuando le damos a las capas múltiples neuronas; en realidad, son solo algunos índices más para realizar un seguimiento.",
  "from_community_srt": "Pero honestamente, no cambia mucho cuando damos múltiples neuronas a las capas. Realmente solo es unos índices mas para mantenerles el rastro.",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "En lugar de que la activación de una capa determinada sea simplemente AL, también tendrá un subíndice que indica qué neurona de esa capa es.",
  "from_community_srt": "Mas bien, la activación de una capa dada viene siendo  a^(L), También va a tener un subscript indicando cuál neurona de la capa es.",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Usemos la letra k para indexar la capa L-1 y j para indexar la capa L.",
  "from_community_srt": "Vamos a adelantarnos y usar la letra K para el índice en la capa (L-1), y j para el indice de la capa (L).",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "Para el costo, nuevamente miramos cuál es el resultado deseado, pero esta vez sumamos los cuadrados de las diferencias entre estas últimas activaciones de capa y el resultado deseado.",
  "from_community_srt": "Para el coste, de nuevo miramos cuál es el resultado deseado. Pero esta vez sumamos los cuadrados de las diferencias entre esta capa de activaciones y el resultado deseado.",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus Yj squared.",
  "translatedText": "Es decir, se toma una suma sobre ALj menos Yj al cuadrado.",
  "from_community_srt": "Eso es,",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Dado que hay muchos más pesos, cada uno tiene que tener un par de índices más para realizar un seguimiento de dónde está, así que llamemos al peso del borde que conecta esta k-ésima neurona con la j-ésima neurona, WLjk.",
  "from_community_srt": "tomas la suma de (a_j^(L) - y_j)^2 Ya que hay un montón de pesos más, cada uno tiene que tener un par más indices para mantenerles rastro de donde está. Así que llamemos al peso del borde  conectando esta neurona k-th a la neurona  j-th Esos índices podrían persivir  un  pequeño retroceso al principio,",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "Esos índices pueden parecer un poco al revés al principio, pero se alinean con la forma en que indexarías la matriz de peso de la que hablé en el video de la parte 1.",
  "from_community_srt": "pero se alínea con cuanto tu indexaste  , la  Matriz ponderada de la que hable en el video 1.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "Al igual que antes, sigue siendo bueno darle un nombre a la suma ponderada relevante, como z, de modo que la activación de la última capa sea solo su función especial, como el sigmoide, aplicada a z.",
  "from_community_srt": "justo  como antes, todavía es bueno darle un nombre a la suma ponderada, como z, asi que la activación de la última capa es solo tu función especial, como la sigmoid, aplicada a z.",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Puedes ver lo que quiero decir, donde todas estas son esencialmente las mismas ecuaciones que teníamos antes en el caso de una neurona por capa, solo que parece un poco más complicado.",
  "from_community_srt": "Puedes ver lo que quiero decir ,verdad Todos estos son esencialmente la misma ecuación que tuvimos antes en el caso de la capa uno a uno; Solo que se ve un poco mas complicada.",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain-ruled derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "Y, de hecho, la expresión derivada regida por cadena que describe cuán sensible es el costo a un peso específico parece esencialmente la misma.",
  "from_community_srt": "y en efecto, la expresión regla de la cadena de la derivada describiendo cuan sensible es el coste para un peso en específico se ve esencialmente lo mismo.",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Te dejaré hacer una pausa y pensar en cada uno de esos términos si lo deseas.",
  "from_community_srt": "Lo dejaré para que pauses y piense cada uno de estos términos si quieres.",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "Lo que sí cambia aquí, sin embargo, es la derivada del coste respecto de una de las activaciones en la capa L-1.",
  "from_community_srt": "Qué es lo que cambia aquí, reflexionando, es la derivada del coste con respecto de una de las activaciones en la capa (L-1).",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "En este caso, la diferencia es que la neurona influye en la función de costos a través de múltiples caminos diferentes.",
  "from_community_srt": "en este caso, la diferencia en la neurona influye la función de costo a través de múltiples caminos.",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "Es decir, por un lado, influye en AL0, que desempeña un papel en la función de costos, pero también influye en AL1, que también desempeña un papel en la función de costos, y hay que sumarlos.",
  "from_community_srt": "Eso es, por un lado, esto influye a_0^(L), que juega un role en la función de coste, Pero también influye a a_1^(L), que también juega un role en la función de coste. Y tu tienes que hacerles sentido.",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "Y eso, bueno, eso es todo.",
  "from_community_srt": "y eso.... bueno eso mucho.",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Una vez que sepa qué tan sensible es la función de costo a las activaciones en esta penúltima capa, puede simplemente repetir el proceso para todos los pesos y sesgos que ingresan a esa capa.",
  "from_community_srt": "Una vez sepas cuan sensible  es la función de coste en las activaciones de esta segunda capa, tu puedes solo repetir el proceso para todos los pesos y bias Asi que date una palmada tu mismo en la espalda!",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "¡Así que date una palmadita en la espalda!",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Si todo esto tiene sentido, ahora ha profundizado en el corazón de la retropropagación, el caballo de batalla detrás de cómo aprenden las redes neuronales.",
  "from_community_srt": "Si todo esto tiene sentido, Tu has ahora visto a profundida en el corazón de la retropropagación, EL caballo de trabajo detrás de cómo las rede aprenden.",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Estas expresiones de reglas de la cadena le brindan las derivadas que determinan cada componente en el gradiente que ayuda a minimizar el costo de la red al descender repetidamente.",
  "from_community_srt": "Estas expresiones de la regla de la cadena te dan las derivadas que determinan cada componente en la gradiente",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Si te sientas y piensas en todo eso, verás que hay muchas capas de complejidad que tu mente debe comprender, así que no te preocupes si tu mente necesita tiempo para digerirlo todo.",
  "from_community_srt": "que ayudan a minimizar el costo de la red al n Si regresas y piensas en todo eso, es un montón capas de capas de complejidad para envolver alrededor de tu mente. Asi que no te preocupes si le toma tiempo a tu mente digerirlo todo.",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]