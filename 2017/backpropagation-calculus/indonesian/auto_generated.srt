1
00:00:00,000 --> 00:00:08,420
Asumsi sulitnya di sini adalah Anda telah menonton bagian

2
00:00:08,420 --> 00:00:11,160
3, yang memberikan panduan intuitif tentang algoritma propagasi mundur.

3
00:00:11,160 --> 00:00:14,920
Di sini kita menjadi sedikit lebih formal dan mendalami kalkulus yang relevan.

4
00:00:14,920 --> 00:00:18,560
Wajar jika hal ini setidaknya sedikit membingungkan, jadi mantra untuk berhenti sejenak

5
00:00:18,560 --> 00:00:22,000
dan merenung secara teratur pasti berlaku di sini dan di mana pun.

6
00:00:22,000 --> 00:00:26,620
Tujuan utama kami adalah untuk menunjukkan bagaimana orang-orang dalam pembelajaran mesin umumnya

7
00:00:26,620 --> 00:00:31,900
berpikir tentang aturan rantai dari kalkulus dalam konteks jaringan, yang memiliki nuansa

8
00:00:31,900 --> 00:00:34,580
berbeda dari pendekatan sebagian besar kursus pengantar kalkulus terhadap subjek tersebut.

9
00:00:34,580 --> 00:00:38,300
Bagi Anda yang merasa tidak nyaman dengan kalkulus yang

10
00:00:38,300 --> 00:00:39,300
relevan, saya memiliki serangkaian topik lengkap tentang topik tersebut.

11
00:00:39,300 --> 00:00:44,840
Mari kita mulai dengan jaringan yang sangat sederhana,

12
00:00:44,840 --> 00:00:46,780
dimana setiap lapisan memiliki satu neuron di dalamnya.

13
00:00:46,780 --> 00:00:51,880
Jaringan ini ditentukan oleh tiga bobot dan tiga bias, dan tujuan

14
00:00:51,880 --> 00:00:55,640
kami adalah memahami seberapa sensitif fungsi biaya terhadap variabel-variabel ini.

15
00:00:55,640 --> 00:00:59,780
Dengan begitu kita mengetahui penyesuaian mana pada ketentuan tersebut

16
00:00:59,780 --> 00:01:01,100
yang akan menyebabkan penurunan fungsi biaya yang paling efisien.

17
00:01:01,100 --> 00:01:05,360
Kami hanya akan fokus pada hubungan antara dua neuron terakhir.

18
00:01:05,360 --> 00:01:10,400
Mari beri label aktivasi neuron terakhir dengan superskrip L,

19
00:01:10,400 --> 00:01:11,800
yang menunjukkan di lapisan mana neuron tersebut berada.

20
00:01:11,800 --> 00:01:16,560
Jadi aktivasi neuron sebelumnya adalah AL-1.

21
00:01:16,560 --> 00:01:20,120
Ini bukan eksponen, ini hanyalah cara mengindeks apa yang sedang kita

22
00:01:20,120 --> 00:01:23,120
bicarakan, karena saya ingin menyimpan subskrip untuk indeks yang berbeda nanti.

23
00:01:23,600 --> 00:01:28,880
Katakanlah nilai yang kita inginkan untuk aktivasi terakhir ini untuk contoh

24
00:01:28,880 --> 00:01:33,020
pelatihan tertentu adalah y, misalnya, y mungkin 0 atau 1.

25
00:01:33,020 --> 00:01:39,040
Jadi biaya jaringan ini untuk satu contoh pelatihan adalah AL-y2.

26
00:01:39,040 --> 00:01:46,120
Kami akan menyatakan biaya satu contoh pelatihan tersebut sebagai c0.

27
00:01:46,120 --> 00:01:51,920
Sebagai pengingat, aktivasi terakhir ini ditentukan oleh bobot, yang saya sebut wL,

28
00:01:51,920 --> 00:01:57,600
dikalikan aktivasi neuron sebelumnya ditambah beberapa bias, yang saya sebut bL.

29
00:01:57,600 --> 00:02:01,560
Kemudian Anda memompanya melalui beberapa fungsi nonlinier khusus seperti sigmoid atau ReLU.

30
00:02:01,560 --> 00:02:05,400
Sebenarnya akan lebih mudah bagi kita jika kita memberi nama khusus untuk jumlah

31
00:02:05,400 --> 00:02:10,600
tertimbang ini, seperti z, dengan superskrip yang sama dengan aktivasi yang relevan.

32
00:02:10,600 --> 00:02:15,320
Istilahnya sangat banyak, dan cara untuk mengonsepnya adalah dengan menggunakan bobot, tindakan

33
00:02:15,320 --> 00:02:21,800
sebelumnya, dan bias untuk menghitung z, yang pada gilirannya memungkinkan kita menghitung

34
00:02:21,800 --> 00:02:27,360
a, yang pada akhirnya, bersama dengan konstanta y, memungkinkan kita menghitung biayanya.

35
00:02:27,360 --> 00:02:33,440
Dan tentu saja, AL-1 dipengaruhi oleh bobotnya sendiri, biasnya, dan semacamnya,

36
00:02:33,440 --> 00:02:35,920
namun kami tidak akan fokus pada hal tersebut saat ini.

37
00:02:35,920 --> 00:02:38,120
Semua ini hanyalah angka, bukan?

38
00:02:38,120 --> 00:02:41,960
Dan akan sangat menyenangkan jika kita menganggap masing-masing mempunyai garis bilangan kecilnya sendiri.

39
00:02:41,960 --> 00:02:47,480
Tujuan pertama kita adalah memahami seberapa sensitif fungsi

40
00:02:47,480 --> 00:02:49,820
biaya terhadap perubahan kecil pada berat wL kita.

41
00:02:49,820 --> 00:02:55,740
Atau dengan kata lain, apa turunan dari c terhadap wL?

42
00:02:55,740 --> 00:03:01,220
Saat Anda melihat istilah del w ini, anggaplah itu berarti dorongan kecil ke w, seperti perubahan sebesar 0.

43
00:03:01,220 --> 00:03:08,820
01, dan anggaplah istilah del c ini berarti apa pun dampak yang dihasilkan terhadap biaya.

44
00:03:08,820 --> 00:03:10,900
Yang kami inginkan adalah rasionya.

45
00:03:10,900 --> 00:03:17,740
Secara konseptual, dorongan kecil terhadap wL ini menyebabkan sejumlah dorongan terhadap zL,

46
00:03:17,740 --> 00:03:23,220
yang selanjutnya menyebabkan sejumlah dorongan terhadap AL, yang secara langsung mempengaruhi biaya.

47
00:03:23,220 --> 00:03:28,020
Jadi kita memecahnya dengan terlebih dahulu melihat rasio perubahan kecil

48
00:03:28,020 --> 00:03:33,340
zL terhadap perubahan kecil w, yaitu turunan zL terhadap wL.

49
00:03:33,340 --> 00:03:38,820
Demikian pula, Anda kemudian mempertimbangkan rasio perubahan pada AL dengan

50
00:03:38,820 --> 00:03:43,900
perubahan kecil pada zL yang menyebabkannya, serta rasio antara

51
00:03:43,900 --> 00:03:45,900
dorongan terakhir ke c dan dorongan perantara ke AL.

52
00:03:45,900 --> 00:03:51,880
Ini adalah aturan rantai, di mana mengalikan ketiga rasio ini

53
00:03:51,880 --> 00:03:57,340
memberi kita sensitivitas c terhadap perubahan kecil pada wL.

54
00:03:57,340 --> 00:04:01,620
Jadi di layar saat ini, ada banyak simbol, dan luangkan waktu sejenak

55
00:04:01,620 --> 00:04:07,460
untuk memastikan semuanya jelas, karena sekarang kita akan menghitung turunan yang relevan.

56
00:04:07,460 --> 00:04:14,220
Turunan c terhadap AL ternyata 2AL-y.

57
00:04:14,220 --> 00:04:19,300
Artinya ukurannya sebanding dengan perbedaan antara keluaran jaringan dan keluaran

58
00:04:19,300 --> 00:04:24,480
yang kita inginkan, jadi jika keluaran tersebut sangat berbeda, perubahan

59
00:04:24,480 --> 00:04:28,380
sekecil apa pun akan berdampak besar pada fungsi biaya akhir.

60
00:04:28,380 --> 00:04:33,860
Turunan AL terhadap zL hanyalah turunan dari fungsi sigmoid kita,

61
00:04:33,860 --> 00:04:37,420
atau nonlinier apa pun yang Anda pilih untuk digunakan.

62
00:04:37,420 --> 00:04:46,180
Turunan dari zL terhadap wL menjadi AL-1.

63
00:04:46,180 --> 00:04:49,460
Saya tidak tahu tentang Anda, tapi menurut saya sangat mudah untuk terpaku pada rumus tanpa

64
00:04:49,460 --> 00:04:54,180
meluangkan waktu sejenak untuk duduk santai dan mengingatkan diri sendiri apa maksud semua rumus tersebut.

65
00:04:54,180 --> 00:04:58,860
Dalam kasus turunan terakhir ini, besar kecilnya pengaruh dorongan kecil terhadap

66
00:04:58,860 --> 00:05:03,220
bobot pada lapisan terakhir bergantung pada seberapa kuat neuron sebelumnya.

67
00:05:03,220 --> 00:05:09,320
Ingat, di sinilah gagasan neuron-yang-api-bersama-kawat-bersama muncul.

68
00:05:09,320 --> 00:05:14,840
Dan semua ini merupakan turunan dari wL saja

69
00:05:14,840 --> 00:05:16,580
dari biaya untuk satu contoh pelatihan tertentu.

70
00:05:16,580 --> 00:05:20,940
Karena fungsi biaya penuh melibatkan rata-rata semua biaya

71
00:05:20,940 --> 00:05:27,300
tersebut di banyak contoh pelatihan yang berbeda, turunannya

72
00:05:27,300 --> 00:05:28,540
memerlukan rata-rata ekspresi ini di seluruh contoh pelatihan.

73
00:05:28,540 --> 00:05:33,860
Tentu saja, itu hanyalah salah satu komponen vektor gradien, yang dibangun

74
00:05:33,860 --> 00:05:40,780
dari turunan parsial fungsi biaya terhadap semua bobot dan bias tersebut.

75
00:05:40,780 --> 00:05:44,340
Namun meskipun itu hanya salah satu dari sekian banyak turunan parsial

76
00:05:44,340 --> 00:05:46,460
yang kami perlukan, ini sudah lebih dari 50% pekerjaan yang berhasil.

77
00:05:46,460 --> 00:05:50,300
Sensitivitas terhadap bias, misalnya, hampir sama.

78
00:05:50,300 --> 00:05:58,980
Kita hanya perlu mengubah istilah del z del w ini menjadi del z del b.

79
00:05:58,980 --> 00:06:04,700
Dan jika dilihat dari rumus yang relevan, turunannya adalah 1.

80
00:06:04,700 --> 00:06:11,700
Selain itu, dan di sinilah gagasan untuk melakukan propagasi mundur muncul, Anda

81
00:06:11,700 --> 00:06:16,180
dapat melihat betapa sensitifnya fungsi biaya ini terhadap aktivasi lapisan sebelumnya.

82
00:06:16,180 --> 00:06:21,380
Yakni, turunan awal dalam ekspresi aturan rantai, sensitivitas

83
00:06:21,380 --> 00:06:25,420
z terhadap aktivasi sebelumnya, menjadi bobot wL.

84
00:06:25,420 --> 00:06:30,100
Dan sekali lagi, meskipun kita tidak akan dapat secara langsung mempengaruhi aktivasi

85
00:06:30,100 --> 00:06:35,280
lapisan sebelumnya, akan sangat membantu jika kita terus memantaunya, karena sekarang

86
00:06:35,280 --> 00:06:40,780
kita dapat terus mengulangi gagasan aturan rantai yang sama ke belakang untuk

87
00:06:40,780 --> 00:06:43,680
melihat seberapa sensitif fungsi biaya terhadap bobot sebelumnya dan bias sebelumnya.

88
00:06:43,680 --> 00:06:47,940
Dan Anda mungkin berpikir ini adalah contoh yang terlalu sederhana, karena semua lapisan memiliki

89
00:06:47,940 --> 00:06:51,320
satu neuron, dan segalanya akan menjadi lebih rumit secara eksponensial untuk jaringan nyata.

90
00:06:51,320 --> 00:06:56,560
Tapi sejujurnya, tidak banyak perubahan ketika kita memberikan beberapa neuron pada

91
00:06:56,560 --> 00:06:59,320
lapisan tersebut, sebenarnya itu hanya beberapa indeks lagi yang perlu dilacak.

92
00:06:59,320 --> 00:07:03,580
Daripada aktivasi lapisan tertentu hanya menjadi AL, ia juga akan

93
00:07:03,580 --> 00:07:07,920
memiliki subskrip yang menunjukkan neuron mana pada lapisan tersebut.

94
00:07:07,920 --> 00:07:15,280
Mari kita gunakan huruf k untuk mengindeks layer L-1, dan j untuk mengindeks layer L.

95
00:07:15,280 --> 00:07:20,720
Untuk biayanya, sekali lagi kita lihat berapa keluaran yang diinginkan, namun kali ini

96
00:07:20,720 --> 00:07:26,120
kita menjumlahkan kuadrat selisih antara aktivasi lapisan terakhir ini dan keluaran yang diinginkan.

97
00:07:26,120 --> 00:07:33,280
Artinya, Anda mengambil jumlah ALj dikurangi yj kuadrat.

98
00:07:33,280 --> 00:07:36,500
Karena ada lebih banyak bobot, masing-masing bobot harus memiliki beberapa

99
00:07:36,500 --> 00:07:41,380
indeks lagi untuk melacak posisinya, jadi sebut saja bobot tepi

100
00:07:41,380 --> 00:07:45,740
yang menghubungkan neuron ke-k ini ke neuron ke-j, WLjk.

101
00:07:45,740 --> 00:07:49,820
Indeks tersebut mungkin terasa sedikit mundur pada awalnya, tetapi hal ini sejalan dengan

102
00:07:49,820 --> 00:07:53,800
cara Anda mengindeks matriks bobot yang saya bicarakan di video bagian 1.

103
00:07:53,800 --> 00:07:57,660
Sama seperti sebelumnya, masih bagus untuk memberi nama pada jumlah

104
00:07:57,660 --> 00:08:03,540
tertimbang yang relevan, seperti z, sehingga aktivasi lapisan terakhir hanyalah

105
00:08:03,540 --> 00:08:04,980
fungsi khusus Anda, seperti sigmoid, yang diterapkan pada z.

106
00:08:04,980 --> 00:08:09,100
Anda dapat memahami apa yang saya maksud, dimana semua persamaan ini pada dasarnya sama dengan persamaan yang

107
00:08:09,100 --> 00:08:15,420
kita miliki sebelumnya dalam kasus satu neuron per lapisan, hanya saja persamaan ini terlihat sedikit lebih rumit.

108
00:08:15,420 --> 00:08:20,620
Dan memang benar, ekspresi turunan aturan rantai yang menggambarkan seberapa

109
00:08:20,620 --> 00:08:23,540
sensitif biaya terhadap bobot tertentu pada dasarnya terlihat sama.

110
00:08:23,540 --> 00:08:29,420
Saya serahkan kepada Anda untuk berhenti sejenak dan memikirkan masing-masing istilah tersebut jika Anda mau.

111
00:08:29,420 --> 00:08:34,900
Namun yang berubah di sini adalah turunan biaya

112
00:08:34,900 --> 00:08:37,820
terhadap salah satu aktivasi di lapisan L-1.

113
00:08:37,820 --> 00:08:42,000
Dalam hal ini, perbedaannya adalah neuron mempengaruhi

114
00:08:42,000 --> 00:08:43,540
fungsi biaya melalui berbagai jalur berbeda.

115
00:08:43,540 --> 00:08:51,200
Artinya, di satu sisi mempengaruhi AL0 yang berperan dalam

116
00:08:51,200 --> 00:08:56,460
fungsi biaya, tetapi juga berpengaruh terhadap AL1 yang

117
00:08:56,460 --> 00:09:00,340
juga berperan dalam fungsi biaya dan harus dijumlahkan.

118
00:09:00,340 --> 00:09:03,680
Dan itu, cukup banyak.

119
00:09:03,680 --> 00:09:08,240
Setelah Anda mengetahui seberapa sensitif fungsi biaya terhadap aktivasi di

120
00:09:08,240 --> 00:09:12,520
lapisan kedua hingga terakhir ini, Anda dapat mengulangi proses untuk

121
00:09:12,520 --> 00:09:13,920
semua bobot dan bias yang dimasukkan ke dalam lapisan tersebut.

122
00:09:13,920 --> 00:09:15,420
Jadi tepuk-tepuk punggungmu!

123
00:09:15,420 --> 00:09:20,480
Jika semua ini masuk akal, Anda sekarang telah melihat jauh ke dalam

124
00:09:20,480 --> 00:09:23,700
inti propagasi mundur, pekerja keras di balik cara jaringan saraf belajar.

125
00:09:23,700 --> 00:09:27,960
Ekspresi aturan rantai ini memberi Anda turunan yang menentukan setiap komponen dalam

126
00:09:27,960 --> 00:09:35,020
gradien yang membantu meminimalkan biaya jaringan dengan berulang kali menuruni bukit.

127
00:09:35,020 --> 00:09:38,960
Jika Anda duduk santai dan memikirkan semua itu, ada banyak lapisan kerumitan yang menyelimuti

128
00:09:38,960 --> 00:09:42,840
pikiran Anda, jadi jangan khawatir jika pikiran Anda memerlukan waktu untuk mencerna semuanya.

