1
00:00:00,000 --> 00:00:05,709
Asumsi sulitnya di sini adalah Anda telah menonton bagian 3, yang

2
00:00:05,709 --> 00:00:11,160
memberikan panduan intuitif tentang algoritma propagasi mundur.

3
00:00:11,160 --> 00:00:14,920
Di sini kita menjadi sedikit lebih formal dan mendalami kalkulus yang relevan.

4
00:00:14,920 --> 00:00:18,597
Wajar jika hal ini setidaknya sedikit membingungkan, jadi mantra untuk berhenti

5
00:00:18,597 --> 00:00:22,000
sejenak dan merenung secara teratur pasti berlaku di sini dan di mana pun.

6
00:00:22,000 --> 00:00:25,212
Tujuan utama kami adalah untuk menunjukkan bagaimana orang-orang dalam

7
00:00:25,212 --> 00:00:28,471
pembelajaran mesin umumnya berpikir tentang aturan rantai dari kalkulus

8
00:00:28,471 --> 00:00:31,593
dalam konteks jaringan, yang memiliki nuansa berbeda dari pendekatan

9
00:00:31,593 --> 00:00:34,580
sebagian besar kursus pengantar kalkulus terhadap subjek tersebut.

10
00:00:34,580 --> 00:00:36,976
Bagi Anda yang merasa tidak nyaman dengan kalkulus yang relevan,

11
00:00:36,976 --> 00:00:39,300
saya memiliki serangkaian topik lengkap tentang topik tersebut.

12
00:00:39,300 --> 00:00:43,040
Mari kita mulai dengan jaringan yang sangat sederhana,

13
00:00:43,040 --> 00:00:46,780
dimana setiap lapisan memiliki satu neuron di dalamnya.

14
00:00:46,780 --> 00:00:51,001
Jaringan ini ditentukan oleh tiga bobot dan tiga bias, dan tujuan kami

15
00:00:51,001 --> 00:00:55,640
adalah memahami seberapa sensitif fungsi biaya terhadap variabel-variabel ini.

16
00:00:55,640 --> 00:00:58,490
Dengan begitu kita mengetahui penyesuaian mana pada ketentuan tersebut

17
00:00:58,490 --> 00:01:01,100
yang akan menyebabkan penurunan fungsi biaya yang paling efisien.

18
00:01:01,100 --> 00:01:05,360
Kami hanya akan fokus pada hubungan antara dua neuron terakhir.

19
00:01:05,360 --> 00:01:08,580
Mari beri label aktivasi neuron terakhir dengan superskrip

20
00:01:08,580 --> 00:01:11,800
L, yang menunjukkan di lapisan mana neuron tersebut berada.

21
00:01:11,800 --> 00:01:16,560
Jadi aktivasi neuron sebelumnya adalah AL-1.

22
00:01:16,560 --> 00:01:19,845
Ini bukan eksponen, ini hanyalah cara mengindeks apa yang sedang kita

23
00:01:19,845 --> 00:01:23,600
bicarakan, karena saya ingin menyimpan subskrip untuk indeks yang berbeda nanti.

24
00:01:23,600 --> 00:01:28,065
Katakanlah nilai yang kita inginkan untuk aktivasi terakhir ini

25
00:01:28,065 --> 00:01:33,020
untuk contoh pelatihan tertentu adalah y, misalnya, y mungkin 0 atau 1.

26
00:01:33,020 --> 00:01:39,040
Jadi biaya jaringan ini untuk satu contoh pelatihan adalah AL-y2.

27
00:01:39,040 --> 00:01:46,120
Kami akan menyatakan biaya satu contoh pelatihan tersebut sebagai c0.

28
00:01:46,120 --> 00:01:51,720
Sebagai pengingat, aktivasi terakhir ini ditentukan oleh bobot, yang saya sebut

29
00:01:51,720 --> 00:01:57,600
wL, dikalikan aktivasi neuron sebelumnya ditambah beberapa bias, yang saya sebut bL.

30
00:01:57,600 --> 00:01:59,709
Kemudian Anda memompanya melalui beberapa fungsi

31
00:01:59,709 --> 00:02:01,560
nonlinier khusus seperti sigmoid atau ReLU.

32
00:02:01,560 --> 00:02:05,997
Sebenarnya akan lebih mudah bagi kita jika kita memberi nama khusus untuk jumlah

33
00:02:05,997 --> 00:02:10,600
tertimbang ini, seperti z, dengan superskrip yang sama dengan aktivasi yang relevan.

34
00:02:10,600 --> 00:02:14,729
Istilahnya sangat banyak, dan cara untuk mengonsepnya adalah dengan

35
00:02:14,729 --> 00:02:18,919
menggunakan bobot, tindakan sebelumnya, dan bias untuk menghitung z,

36
00:02:18,919 --> 00:02:23,352
yang pada gilirannya memungkinkan kita menghitung a, yang pada akhirnya,

37
00:02:23,352 --> 00:02:27,360
bersama dengan konstanta y, memungkinkan kita menghitung biayanya.

38
00:02:27,360 --> 00:02:31,702
Dan tentu saja, AL-1 dipengaruhi oleh bobotnya sendiri, biasnya, dan

39
00:02:31,702 --> 00:02:35,920
semacamnya, namun kami tidak akan fokus pada hal tersebut saat ini.

40
00:02:35,920 --> 00:02:38,120
Semua ini hanyalah angka, bukan?

41
00:02:38,120 --> 00:02:39,931
Dan akan sangat menyenangkan jika kita menganggap

42
00:02:39,931 --> 00:02:41,960
masing-masing mempunyai garis bilangan kecilnya sendiri.

43
00:02:41,960 --> 00:02:45,783
Tujuan pertama kita adalah memahami seberapa sensitif

44
00:02:45,783 --> 00:02:49,820
fungsi biaya terhadap perubahan kecil pada berat wL kita.

45
00:02:49,820 --> 00:02:55,740
Atau dengan kata lain, apa turunan dari c terhadap wL?

46
00:02:55,740 --> 00:02:58,485
Saat Anda melihat istilah del w ini, anggaplah itu

47
00:02:58,485 --> 00:03:01,554
berarti dorongan kecil ke w, seperti perubahan sebesar 0.

48
00:03:01,554 --> 00:03:08,820
01, dan anggaplah istilah del c ini berarti apa pun dampak yang dihasilkan terhadap biaya.

49
00:03:08,820 --> 00:03:10,900
Yang kami inginkan adalah rasionya.

50
00:03:10,900 --> 00:03:14,857
Secara konseptual, dorongan kecil terhadap wL ini menyebabkan

51
00:03:14,857 --> 00:03:18,687
sejumlah dorongan terhadap zL, yang selanjutnya menyebabkan

52
00:03:18,687 --> 00:03:23,220
sejumlah dorongan terhadap AL, yang secara langsung mempengaruhi biaya.

53
00:03:23,220 --> 00:03:28,355
Jadi kita memecahnya dengan terlebih dahulu melihat rasio perubahan

54
00:03:28,355 --> 00:03:33,340
kecil zL terhadap perubahan kecil w, yaitu turunan zL terhadap wL.

55
00:03:33,340 --> 00:03:37,374
Demikian pula, Anda kemudian mempertimbangkan rasio perubahan

56
00:03:37,374 --> 00:03:41,669
pada AL dengan perubahan kecil pada zL yang menyebabkannya, serta

57
00:03:41,669 --> 00:03:45,900
rasio antara dorongan terakhir ke c dan dorongan perantara ke AL.

58
00:03:45,900 --> 00:03:51,666
Ini adalah aturan rantai, di mana mengalikan ketiga rasio ini

59
00:03:51,666 --> 00:03:57,340
memberi kita sensitivitas c terhadap perubahan kecil pada wL.

60
00:03:57,340 --> 00:04:02,117
Jadi di layar saat ini, ada banyak simbol, dan luangkan waktu sejenak untuk

61
00:04:02,117 --> 00:04:07,460
memastikan semuanya jelas, karena sekarang kita akan menghitung turunan yang relevan.

62
00:04:07,460 --> 00:04:14,220
Turunan c terhadap AL ternyata 2AL-y.

63
00:04:14,220 --> 00:04:18,766
Artinya ukurannya sebanding dengan perbedaan antara keluaran jaringan

64
00:04:18,766 --> 00:04:23,183
dan keluaran yang kita inginkan, jadi jika keluaran tersebut sangat

65
00:04:23,183 --> 00:04:28,380
berbeda, perubahan sekecil apa pun akan berdampak besar pada fungsi biaya akhir.

66
00:04:28,380 --> 00:04:32,862
Turunan AL terhadap zL hanyalah turunan dari fungsi sigmoid

67
00:04:32,862 --> 00:04:37,420
kita, atau nonlinier apa pun yang Anda pilih untuk digunakan.

68
00:04:37,420 --> 00:04:46,180
Turunan dari zL terhadap wL menjadi AL-1.

69
00:04:46,180 --> 00:04:48,887
Saya tidak tahu tentang Anda, tapi menurut saya sangat mudah untuk

70
00:04:48,887 --> 00:04:51,674
terpaku pada rumus tanpa meluangkan waktu sejenak untuk duduk santai

71
00:04:51,674 --> 00:04:54,180
dan mengingatkan diri sendiri apa maksud semua rumus tersebut.

72
00:04:54,180 --> 00:04:58,871
Dalam kasus turunan terakhir ini, besar kecilnya pengaruh dorongan kecil terhadap

73
00:04:58,871 --> 00:05:03,220
bobot pada lapisan terakhir bergantung pada seberapa kuat neuron sebelumnya.

74
00:05:03,220 --> 00:05:09,320
Ingat, di sinilah gagasan neuron-yang-api-bersama-kawat-bersama muncul.

75
00:05:09,320 --> 00:05:13,223
Dan semua ini merupakan turunan dari wL saja dari

76
00:05:13,223 --> 00:05:16,580
biaya untuk satu contoh pelatihan tertentu.

77
00:05:16,580 --> 00:05:20,457
Karena fungsi biaya penuh melibatkan rata-rata semua biaya

78
00:05:20,457 --> 00:05:24,465
tersebut di banyak contoh pelatihan yang berbeda, turunannya

79
00:05:24,465 --> 00:05:28,540
memerlukan rata-rata ekspresi ini di seluruh contoh pelatihan.

80
00:05:28,540 --> 00:05:34,742
Tentu saja, itu hanyalah salah satu komponen vektor gradien, yang dibangun

81
00:05:34,742 --> 00:05:40,780
dari turunan parsial fungsi biaya terhadap semua bobot dan bias tersebut.

82
00:05:40,780 --> 00:05:43,660
Namun meskipun itu hanya salah satu dari sekian banyak turunan parsial

83
00:05:43,660 --> 00:05:46,460
yang kami perlukan, ini sudah lebih dari 50% pekerjaan yang berhasil.

84
00:05:46,460 --> 00:05:50,300
Sensitivitas terhadap bias, misalnya, hampir sama.

85
00:05:50,300 --> 00:05:58,980
Kita hanya perlu mengubah istilah del z del w ini menjadi del z del b.

86
00:05:58,980 --> 00:06:04,700
Dan jika dilihat dari rumus yang relevan, turunannya adalah 1.

87
00:06:04,700 --> 00:06:10,235
Selain itu, dan di sinilah gagasan untuk melakukan propagasi mundur muncul, Anda

88
00:06:10,235 --> 00:06:16,180
dapat melihat betapa sensitifnya fungsi biaya ini terhadap aktivasi lapisan sebelumnya.

89
00:06:16,180 --> 00:06:20,305
Yakni, turunan awal dalam ekspresi aturan rantai,

90
00:06:20,305 --> 00:06:25,420
sensitivitas z terhadap aktivasi sebelumnya, menjadi bobot wL.

91
00:06:25,420 --> 00:06:30,107
Dan sekali lagi, meskipun kita tidak akan dapat secara langsung mempengaruhi aktivasi

92
00:06:30,107 --> 00:06:34,795
lapisan sebelumnya, akan sangat membantu jika kita terus memantaunya, karena sekarang

93
00:06:34,795 --> 00:06:39,482
kita dapat terus mengulangi gagasan aturan rantai yang sama ke belakang untuk melihat

94
00:06:39,482 --> 00:06:43,680
seberapa sensitif fungsi biaya terhadap bobot sebelumnya dan bias sebelumnya.

95
00:06:43,680 --> 00:06:46,400
Dan Anda mungkin berpikir ini adalah contoh yang terlalu sederhana,

96
00:06:46,400 --> 00:06:48,880
karena semua lapisan memiliki satu neuron, dan segalanya akan

97
00:06:48,880 --> 00:06:51,320
menjadi lebih rumit secara eksponensial untuk jaringan nyata.

98
00:06:51,320 --> 00:06:55,197
Tapi sejujurnya, tidak banyak perubahan ketika kita memberikan beberapa neuron

99
00:06:55,197 --> 00:06:59,320
pada lapisan tersebut, sebenarnya itu hanya beberapa indeks lagi yang perlu dilacak.

100
00:06:59,320 --> 00:07:03,524
Daripada aktivasi lapisan tertentu hanya menjadi AL, ia juga akan

101
00:07:03,524 --> 00:07:07,920
memiliki subskrip yang menunjukkan neuron mana pada lapisan tersebut.

102
00:07:07,920 --> 00:07:15,280
Mari kita gunakan huruf k untuk mengindeks layer L-1, dan j untuk mengindeks layer L.

103
00:07:15,280 --> 00:07:18,758
Untuk biayanya, sekali lagi kita lihat berapa keluaran yang

104
00:07:18,758 --> 00:07:22,294
diinginkan, namun kali ini kita menjumlahkan kuadrat selisih

105
00:07:22,294 --> 00:07:26,120
antara aktivasi lapisan terakhir ini dan keluaran yang diinginkan.

106
00:07:26,120 --> 00:07:33,280
Artinya, Anda mengambil jumlah ALj dikurangi yj kuadrat.

107
00:07:33,280 --> 00:07:37,497
Karena ada lebih banyak bobot, masing-masing bobot harus memiliki

108
00:07:37,497 --> 00:07:41,458
beberapa indeks lagi untuk melacak posisinya, jadi sebut saja

109
00:07:41,458 --> 00:07:45,740
bobot tepi yang menghubungkan neuron ke-k ini ke neuron ke-j, WLjk.

110
00:07:45,740 --> 00:07:49,844
Indeks tersebut mungkin terasa sedikit mundur pada awalnya, tetapi hal ini sejalan

111
00:07:49,844 --> 00:07:53,800
dengan cara Anda mengindeks matriks bobot yang saya bicarakan di video bagian 1.

112
00:07:53,800 --> 00:07:57,455
Sama seperti sebelumnya, masih bagus untuk memberi nama pada jumlah

113
00:07:57,455 --> 00:08:01,271
tertimbang yang relevan, seperti z, sehingga aktivasi lapisan terakhir

114
00:08:01,271 --> 00:08:04,980
hanyalah fungsi khusus Anda, seperti sigmoid, yang diterapkan pada z.

115
00:08:04,980 --> 00:08:08,428
Anda dapat memahami apa yang saya maksud, dimana semua persamaan ini pada

116
00:08:08,428 --> 00:08:11,971
dasarnya sama dengan persamaan yang kita miliki sebelumnya dalam kasus satu

117
00:08:11,971 --> 00:08:15,420
neuron per lapisan, hanya saja persamaan ini terlihat sedikit lebih rumit.

118
00:08:15,420 --> 00:08:19,254
Dan memang benar, ekspresi turunan aturan rantai yang menggambarkan

119
00:08:19,254 --> 00:08:23,540
seberapa sensitif biaya terhadap bobot tertentu pada dasarnya terlihat sama.

120
00:08:23,540 --> 00:08:26,399
Saya serahkan kepada Anda untuk berhenti sejenak dan

121
00:08:26,399 --> 00:08:29,420
memikirkan masing-masing istilah tersebut jika Anda mau.

122
00:08:29,420 --> 00:08:33,802
Namun yang berubah di sini adalah turunan biaya

123
00:08:33,802 --> 00:08:37,820
terhadap salah satu aktivasi di lapisan L-1.

124
00:08:37,820 --> 00:08:40,997
Dalam hal ini, perbedaannya adalah neuron mempengaruhi

125
00:08:40,997 --> 00:08:43,540
fungsi biaya melalui berbagai jalur berbeda.

126
00:08:43,540 --> 00:08:51,940
Artinya, di satu sisi mempengaruhi AL0 yang berperan dalam fungsi biaya, tetapi juga

127
00:08:51,940 --> 00:09:00,340
berpengaruh terhadap AL1 yang juga berperan dalam fungsi biaya dan harus dijumlahkan.

128
00:09:00,340 --> 00:09:03,680
Dan itu, cukup banyak.

129
00:09:03,680 --> 00:09:07,256
Setelah Anda mengetahui seberapa sensitif fungsi biaya terhadap aktivasi

130
00:09:07,256 --> 00:09:10,539
di lapisan kedua hingga terakhir ini, Anda dapat mengulangi proses

131
00:09:10,539 --> 00:09:13,920
untuk semua bobot dan bias yang dimasukkan ke dalam lapisan tersebut.

132
00:09:13,920 --> 00:09:15,420
Jadi tepuk-tepuk punggungmu!

133
00:09:15,420 --> 00:09:19,415
Jika semua ini masuk akal, Anda sekarang telah melihat jauh ke dalam

134
00:09:19,415 --> 00:09:23,700
inti propagasi mundur, pekerja keras di balik cara jaringan saraf belajar.

135
00:09:23,700 --> 00:09:29,360
Ekspresi aturan rantai ini memberi Anda turunan yang menentukan setiap komponen dalam

136
00:09:29,360 --> 00:09:35,020
gradien yang membantu meminimalkan biaya jaringan dengan berulang kali menuruni bukit.

137
00:09:35,020 --> 00:09:36,270
Jika Anda duduk santai dan memikirkan semua itu, ada banyak

138
00:09:36,270 --> 00:09:37,542
lapisan kerumitan yang menyelimuti pikiran Anda, jadi jangan

139
00:09:37,542 --> 00:09:38,960
khawatir jika pikiran Anda memerlukan waktu untuk mencerna semuanya.

