1
00:00:00,000 --> 00:00:05,277
Asumsi sulitnya di sini adalah Anda telah menonton bagian 3,

2
00:00:05,277 --> 00:00:11,160
yang memberikan panduan intuitif tentang algoritma propagasi mundur.

3
00:00:11,160 --> 00:00:14,920
Di sini kita menjadi sedikit lebih formal dan mendalami kalkulus yang relevan.

4
00:00:14,920 --> 00:00:17,356
Wajar jika hal ini setidaknya sedikit membingungkan,

5
00:00:17,356 --> 00:00:20,896
jadi mantra untuk berhenti sejenak dan merenung secara teratur pasti berlaku

6
00:00:20,896 --> 00:00:22,000
di sini dan di mana pun.

7
00:00:22,000 --> 00:00:25,212
Tujuan utama kami adalah untuk menunjukkan bagaimana orang-orang dalam

8
00:00:25,212 --> 00:00:28,471
pembelajaran mesin umumnya berpikir tentang aturan rantai dari kalkulus

9
00:00:28,471 --> 00:00:31,593
dalam konteks jaringan, yang memiliki nuansa berbeda dari pendekatan

10
00:00:31,593 --> 00:00:34,580
sebagian besar kursus pengantar kalkulus terhadap subjek tersebut.

11
00:00:34,580 --> 00:00:36,976
Bagi Anda yang merasa tidak nyaman dengan kalkulus yang relevan,

12
00:00:36,976 --> 00:00:39,300
saya memiliki serangkaian topik lengkap tentang topik tersebut.

13
00:00:39,300 --> 00:00:43,040
Mari kita mulai dengan jaringan yang sangat sederhana,

14
00:00:43,040 --> 00:00:46,780
dimana setiap lapisan memiliki satu neuron di dalamnya.

15
00:00:46,780 --> 00:00:50,050
Jaringan ini ditentukan oleh tiga bobot dan tiga bias,

16
00:00:50,050 --> 00:00:54,331
dan tujuan kami adalah memahami seberapa sensitif fungsi biaya terhadap

17
00:00:54,331 --> 00:00:55,640
variabel-variabel ini.

18
00:00:55,640 --> 00:00:58,490
Dengan begitu kita mengetahui penyesuaian mana pada ketentuan tersebut

19
00:00:58,490 --> 00:01:01,100
yang akan menyebabkan penurunan fungsi biaya yang paling efisien.

20
00:01:01,100 --> 00:01:05,360
Kami hanya akan fokus pada hubungan antara dua neuron terakhir.

21
00:01:05,360 --> 00:01:08,743
Mari beri label aktivasi neuron terakhir dengan superskrip L,

22
00:01:08,743 --> 00:01:11,800
yang menunjukkan di lapisan mana neuron tersebut berada.

23
00:01:11,800 --> 00:01:16,560
Jadi aktivasi neuron sebelumnya adalah AL-1.

24
00:01:16,560 --> 00:01:20,361
Ini bukan eksponen, ini hanyalah cara mengindeks apa yang sedang kita bicarakan,

25
00:01:20,361 --> 00:01:23,600
karena saya ingin menyimpan subskrip untuk indeks yang berbeda nanti.

26
00:01:23,600 --> 00:01:28,065
Katakanlah nilai yang kita inginkan untuk aktivasi terakhir ini

27
00:01:28,065 --> 00:01:33,020
untuk contoh pelatihan tertentu adalah y, misalnya, y mungkin 0 atau 1.

28
00:01:33,020 --> 00:01:39,040
Jadi biaya jaringan ini untuk satu contoh pelatihan adalah AL-y2.

29
00:01:39,040 --> 00:01:46,120
Kami akan menyatakan biaya satu contoh pelatihan tersebut sebagai c0.

30
00:01:46,120 --> 00:01:52,000
Sebagai pengingat, aktivasi terakhir ini ditentukan oleh bobot, yang saya sebut wL,

31
00:01:52,000 --> 00:01:57,600
dikalikan aktivasi neuron sebelumnya ditambah beberapa bias, yang saya sebut bL.

32
00:01:57,600 --> 00:01:59,709
Kemudian Anda memompanya melalui beberapa fungsi

33
00:01:59,709 --> 00:02:01,560
nonlinier khusus seperti sigmoid atau ReLU.

34
00:02:01,560 --> 00:02:05,997
Sebenarnya akan lebih mudah bagi kita jika kita memberi nama khusus untuk jumlah

35
00:02:05,997 --> 00:02:10,600
tertimbang ini, seperti z, dengan superskrip yang sama dengan aktivasi yang relevan.

36
00:02:10,600 --> 00:02:15,883
Istilahnya sangat banyak, dan cara untuk mengonsepnya adalah dengan menggunakan bobot,

37
00:02:15,883 --> 00:02:18,919
tindakan sebelumnya, dan bias untuk menghitung z,

38
00:02:18,919 --> 00:02:23,352
yang pada gilirannya memungkinkan kita menghitung a, yang pada akhirnya,

39
00:02:23,352 --> 00:02:27,360
bersama dengan konstanta y, memungkinkan kita menghitung biayanya.

40
00:02:27,360 --> 00:02:31,451
Dan tentu saja, AL-1 dipengaruhi oleh bobotnya sendiri, biasnya,

41
00:02:31,451 --> 00:02:35,920
dan semacamnya, namun kami tidak akan fokus pada hal tersebut saat ini.

42
00:02:35,920 --> 00:02:38,120
Semua ini hanyalah angka, bukan?

43
00:02:38,120 --> 00:02:39,931
Dan akan sangat menyenangkan jika kita menganggap

44
00:02:39,931 --> 00:02:41,960
masing-masing mempunyai garis bilangan kecilnya sendiri.

45
00:02:41,960 --> 00:02:45,783
Tujuan pertama kita adalah memahami seberapa sensitif

46
00:02:45,783 --> 00:02:49,820
fungsi biaya terhadap perubahan kecil pada berat wL kita.

47
00:02:49,820 --> 00:02:55,740
Atau dengan kata lain, apa turunan dari c terhadap wL?

48
00:02:55,740 --> 00:03:01,024
Saat Anda melihat istilah del w ini, anggaplah itu berarti dorongan kecil ke w,

49
00:03:01,024 --> 00:03:05,252
seperti perubahan sebesar 0.01, dan anggaplah istilah del c ini

50
00:03:05,252 --> 00:03:08,820
berarti apa pun dampak yang dihasilkan terhadap biaya.

51
00:03:08,820 --> 00:03:10,900
Yang kami inginkan adalah rasionya.

52
00:03:10,900 --> 00:03:14,857
Secara konseptual, dorongan kecil terhadap wL ini menyebabkan

53
00:03:14,857 --> 00:03:18,687
sejumlah dorongan terhadap zL, yang selanjutnya menyebabkan

54
00:03:18,687 --> 00:03:23,220
sejumlah dorongan terhadap AL, yang secara langsung mempengaruhi biaya.

55
00:03:23,220 --> 00:03:28,355
Jadi kita memecahnya dengan terlebih dahulu melihat rasio perubahan

56
00:03:28,355 --> 00:03:33,340
kecil zL terhadap perubahan kecil w, yaitu turunan zL terhadap wL.

57
00:03:33,340 --> 00:03:37,374
Demikian pula, Anda kemudian mempertimbangkan rasio perubahan

58
00:03:37,374 --> 00:03:41,279
pada AL dengan perubahan kecil pada zL yang menyebabkannya,

59
00:03:41,279 --> 00:03:45,900
serta rasio antara dorongan terakhir ke c dan dorongan perantara ke AL.

60
00:03:45,900 --> 00:03:51,666
Ini adalah aturan rantai, di mana mengalikan ketiga rasio ini

61
00:03:51,666 --> 00:03:57,340
memberi kita sensitivitas c terhadap perubahan kecil pada wL.

62
00:03:57,340 --> 00:04:02,117
Jadi di layar saat ini, ada banyak simbol, dan luangkan waktu sejenak untuk

63
00:04:02,117 --> 00:04:07,460
memastikan semuanya jelas, karena sekarang kita akan menghitung turunan yang relevan.

64
00:04:07,460 --> 00:04:14,220
Turunan c terhadap AL ternyata 2AL-y.

65
00:04:14,220 --> 00:04:18,766
Artinya ukurannya sebanding dengan perbedaan antara keluaran jaringan

66
00:04:18,766 --> 00:04:23,768
dan keluaran yang kita inginkan, jadi jika keluaran tersebut sangat berbeda,

67
00:04:23,768 --> 00:04:28,380
perubahan sekecil apa pun akan berdampak besar pada fungsi biaya akhir.

68
00:04:28,380 --> 00:04:33,310
Turunan AL terhadap zL hanyalah turunan dari fungsi sigmoid kita,

69
00:04:33,310 --> 00:04:37,420
atau nonlinier apa pun yang Anda pilih untuk digunakan.

70
00:04:37,420 --> 00:04:46,180
Turunan dari zL terhadap wL menjadi AL-1.

71
00:04:46,180 --> 00:04:48,887
Saya tidak tahu tentang Anda, tapi menurut saya sangat mudah untuk

72
00:04:48,887 --> 00:04:51,674
terpaku pada rumus tanpa meluangkan waktu sejenak untuk duduk santai

73
00:04:51,674 --> 00:04:54,180
dan mengingatkan diri sendiri apa maksud semua rumus tersebut.

74
00:04:54,180 --> 00:04:58,871
Dalam kasus turunan terakhir ini, besar kecilnya pengaruh dorongan kecil terhadap

75
00:04:58,871 --> 00:05:03,220
bobot pada lapisan terakhir bergantung pada seberapa kuat neuron sebelumnya.

76
00:05:03,220 --> 00:05:09,320
Ingat, di sinilah gagasan neuron-yang-api-bersama-kawat-bersama muncul.

77
00:05:09,320 --> 00:05:13,223
Dan semua ini merupakan turunan dari wL saja dari

78
00:05:13,223 --> 00:05:16,580
biaya untuk satu contoh pelatihan tertentu.

79
00:05:16,580 --> 00:05:20,457
Karena fungsi biaya penuh melibatkan rata-rata semua biaya

80
00:05:20,457 --> 00:05:23,742
tersebut di banyak contoh pelatihan yang berbeda,

81
00:05:23,742 --> 00:05:28,540
turunannya memerlukan rata-rata ekspresi ini di seluruh contoh pelatihan.

82
00:05:28,540 --> 00:05:33,584
Tentu saja, itu hanyalah salah satu komponen vektor gradien,

83
00:05:33,584 --> 00:05:40,780
yang dibangun dari turunan parsial fungsi biaya terhadap semua bobot dan bias tersebut.

84
00:05:40,780 --> 00:05:43,660
Namun meskipun itu hanya salah satu dari sekian banyak turunan parsial

85
00:05:43,660 --> 00:05:46,460
yang kami perlukan, ini sudah lebih dari 50% pekerjaan yang berhasil.

86
00:05:46,460 --> 00:05:50,300
Sensitivitas terhadap bias, misalnya, hampir sama.

87
00:05:50,300 --> 00:05:58,980
Kita hanya perlu mengubah istilah del z del w ini menjadi del z del b.

88
00:05:58,980 --> 00:06:04,700
Dan jika dilihat dari rumus yang relevan, turunannya adalah 1.

89
00:06:04,700 --> 00:06:09,893
Selain itu, dan di sinilah gagasan untuk melakukan propagasi mundur muncul,

90
00:06:09,893 --> 00:06:15,428
Anda dapat melihat betapa sensitifnya fungsi biaya ini terhadap aktivasi lapisan

91
00:06:15,428 --> 00:06:16,180
sebelumnya.

92
00:06:16,180 --> 00:06:20,305
Yakni, turunan awal dalam ekspresi aturan rantai,

93
00:06:20,305 --> 00:06:25,420
sensitivitas z terhadap aktivasi sebelumnya, menjadi bobot wL.

94
00:06:25,420 --> 00:06:30,107
Dan sekali lagi, meskipun kita tidak akan dapat secara langsung mempengaruhi aktivasi

95
00:06:30,107 --> 00:06:33,923
lapisan sebelumnya, akan sangat membantu jika kita terus memantaunya,

96
00:06:33,923 --> 00:06:38,229
karena sekarang kita dapat terus mengulangi gagasan aturan rantai yang sama ke

97
00:06:38,229 --> 00:06:42,807
belakang untuk melihat seberapa sensitif fungsi biaya terhadap bobot sebelumnya dan

98
00:06:42,807 --> 00:06:43,680
bias sebelumnya.

99
00:06:43,680 --> 00:06:46,400
Dan Anda mungkin berpikir ini adalah contoh yang terlalu sederhana,

100
00:06:46,400 --> 00:06:48,880
karena semua lapisan memiliki satu neuron, dan segalanya akan

101
00:06:48,880 --> 00:06:51,320
menjadi lebih rumit secara eksponensial untuk jaringan nyata.

102
00:06:51,320 --> 00:06:55,197
Tapi sejujurnya, tidak banyak perubahan ketika kita memberikan beberapa neuron

103
00:06:55,197 --> 00:06:59,320
pada lapisan tersebut, sebenarnya itu hanya beberapa indeks lagi yang perlu dilacak.

104
00:06:59,320 --> 00:07:02,696
Daripada aktivasi lapisan tertentu hanya menjadi AL,

105
00:07:02,696 --> 00:07:07,920
ia juga akan memiliki subskrip yang menunjukkan neuron mana pada lapisan tersebut.

106
00:07:07,920 --> 00:07:15,280
Mari kita gunakan huruf k untuk mengindeks layer L-1, dan j untuk mengindeks layer L.

107
00:07:15,280 --> 00:07:19,453
Untuk biayanya, sekali lagi kita lihat berapa keluaran yang diinginkan,

108
00:07:19,453 --> 00:07:23,221
namun kali ini kita menjumlahkan kuadrat selisih antara aktivasi

109
00:07:23,221 --> 00:07:26,120
lapisan terakhir ini dan keluaran yang diinginkan.

110
00:07:26,120 --> 00:07:33,280
Artinya, Anda mengambil jumlah ALj dikurangi yj kuadrat.

111
00:07:33,280 --> 00:07:37,497
Karena ada lebih banyak bobot, masing-masing bobot harus memiliki

112
00:07:37,497 --> 00:07:41,458
beberapa indeks lagi untuk melacak posisinya, jadi sebut saja

113
00:07:41,458 --> 00:07:45,740
bobot tepi yang menghubungkan neuron ke-k ini ke neuron ke-j, WLjk.

114
00:07:45,740 --> 00:07:48,706
Indeks tersebut mungkin terasa sedikit mundur pada awalnya,

115
00:07:48,706 --> 00:07:52,909
tetapi hal ini sejalan dengan cara Anda mengindeks matriks bobot yang saya bicarakan

116
00:07:52,909 --> 00:07:53,800
di video bagian 1.

117
00:07:53,800 --> 00:07:57,455
Sama seperti sebelumnya, masih bagus untuk memberi nama pada jumlah

118
00:07:57,455 --> 00:08:01,271
tertimbang yang relevan, seperti z, sehingga aktivasi lapisan terakhir

119
00:08:01,271 --> 00:08:04,980
hanyalah fungsi khusus Anda, seperti sigmoid, yang diterapkan pada z.

120
00:08:04,980 --> 00:08:08,428
Anda dapat memahami apa yang saya maksud, dimana semua persamaan ini pada

121
00:08:08,428 --> 00:08:11,971
dasarnya sama dengan persamaan yang kita miliki sebelumnya dalam kasus satu

122
00:08:11,971 --> 00:08:15,420
neuron per lapisan, hanya saja persamaan ini terlihat sedikit lebih rumit.

123
00:08:15,420 --> 00:08:19,254
Dan memang benar, ekspresi turunan aturan rantai yang menggambarkan

124
00:08:19,254 --> 00:08:23,540
seberapa sensitif biaya terhadap bobot tertentu pada dasarnya terlihat sama.

125
00:08:23,540 --> 00:08:26,399
Saya serahkan kepada Anda untuk berhenti sejenak dan

126
00:08:26,399 --> 00:08:29,420
memikirkan masing-masing istilah tersebut jika Anda mau.

127
00:08:29,420 --> 00:08:33,802
Namun yang berubah di sini adalah turunan biaya

128
00:08:33,802 --> 00:08:37,820
terhadap salah satu aktivasi di lapisan L-1.

129
00:08:37,820 --> 00:08:40,997
Dalam hal ini, perbedaannya adalah neuron mempengaruhi

130
00:08:40,997 --> 00:08:43,540
fungsi biaya melalui berbagai jalur berbeda.

131
00:08:43,540 --> 00:08:50,754
Artinya, di satu sisi mempengaruhi AL0 yang berperan dalam fungsi biaya,

132
00:08:50,754 --> 00:08:59,154
tetapi juga berpengaruh terhadap AL1 yang juga berperan dalam fungsi biaya dan harus

133
00:08:59,154 --> 00:09:00,340
dijumlahkan.

134
00:09:00,340 --> 00:09:03,680
Dan itu, cukup banyak.

135
00:09:03,680 --> 00:09:07,256
Setelah Anda mengetahui seberapa sensitif fungsi biaya terhadap aktivasi

136
00:09:07,256 --> 00:09:10,539
di lapisan kedua hingga terakhir ini, Anda dapat mengulangi proses

137
00:09:10,539 --> 00:09:13,920
untuk semua bobot dan bias yang dimasukkan ke dalam lapisan tersebut.

138
00:09:13,920 --> 00:09:15,420
Jadi tepuk-tepuk punggungmu!

139
00:09:15,420 --> 00:09:19,415
Jika semua ini masuk akal, Anda sekarang telah melihat jauh ke dalam

140
00:09:19,415 --> 00:09:23,700
inti propagasi mundur, pekerja keras di balik cara jaringan saraf belajar.

141
00:09:23,700 --> 00:09:29,360
Ekspresi aturan rantai ini memberi Anda turunan yang menentukan setiap komponen dalam

142
00:09:29,360 --> 00:09:35,020
gradien yang membantu meminimalkan biaya jaringan dengan berulang kali menuruni bukit.

143
00:09:35,020 --> 00:09:36,041
Jika Anda duduk santai dan memikirkan semua itu,

144
00:09:36,041 --> 00:09:37,292
ada banyak lapisan kerumitan yang menyelimuti pikiran Anda,

145
00:09:37,292 --> 00:09:38,960
jadi jangan khawatir jika pikiran Anda memerlukan waktu untuk mencerna semuanya.

