1
00:00:04,020 --> 00:00:06,809
Asumsi sulitnya di sini adalah Anda telah menonton bagian 3, 

2
00:00:06,809 --> 00:00:09,920
yang memberikan panduan intuitif tentang algoritma propagasi mundur.

3
00:00:11,040 --> 00:00:14,220
Di sini kita menjadi sedikit lebih formal dan mendalami kalkulus yang relevan.

4
00:00:14,820 --> 00:00:17,084
Wajar jika hal ini setidaknya sedikit membingungkan, 

5
00:00:17,084 --> 00:00:20,374
jadi mantra untuk berhenti sejenak dan merenung secara teratur pasti berlaku 

6
00:00:20,374 --> 00:00:21,400
di sini dan di mana pun.

7
00:00:21,940 --> 00:00:24,928
Tujuan utama kami adalah untuk menunjukkan bagaimana orang-orang dalam 

8
00:00:24,928 --> 00:00:27,958
pembelajaran mesin umumnya berpikir tentang aturan rantai dari kalkulus 

9
00:00:27,958 --> 00:00:30,862
dalam konteks jaringan, yang memiliki nuansa berbeda dari pendekatan 

10
00:00:30,862 --> 00:00:33,640
sebagian besar kursus pengantar kalkulus terhadap subjek tersebut.

11
00:00:34,340 --> 00:00:36,574
Bagi Anda yang merasa tidak nyaman dengan kalkulus yang relevan, 

12
00:00:36,574 --> 00:00:38,740
saya memiliki serangkaian topik lengkap tentang topik tersebut.

13
00:00:39,960 --> 00:00:42,990
Mari kita mulai dengan jaringan yang sangat sederhana, 

14
00:00:42,990 --> 00:00:46,020
dimana setiap lapisan memiliki satu neuron di dalamnya.

15
00:00:46,320 --> 00:00:49,479
Jaringan ini ditentukan oleh tiga bobot dan tiga bias, 

16
00:00:49,479 --> 00:00:53,616
dan tujuan kami adalah memahami seberapa sensitif fungsi biaya terhadap 

17
00:00:53,616 --> 00:00:54,880
variabel-variabel ini.

18
00:00:55,419 --> 00:00:59,022
Dengan begitu kita mengetahui penyesuaian mana pada ketentuan tersebut 

19
00:00:59,022 --> 00:01:02,320
yang akan menyebabkan penurunan fungsi biaya yang paling efisien.

20
00:01:02,320 --> 00:01:04,840
Kami hanya akan fokus pada hubungan antara dua neuron terakhir.

21
00:01:05,980 --> 00:01:08,806
Mari beri label aktivasi neuron terakhir dengan superskrip L, 

22
00:01:08,806 --> 00:01:11,360
yang menunjukkan di lapisan mana neuron tersebut berada.

23
00:01:11,680 --> 00:01:15,560
Jadi aktivasi neuron sebelumnya adalah AL-1.

24
00:01:16,360 --> 00:01:19,967
Ini bukan eksponen, ini hanyalah cara mengindeks apa yang sedang kita bicarakan, 

25
00:01:19,967 --> 00:01:23,040
karena saya ingin menyimpan subskrip untuk indeks yang berbeda nanti.

26
00:01:23,720 --> 00:01:27,730
Katakanlah nilai yang kita inginkan untuk aktivasi terakhir ini 

27
00:01:27,730 --> 00:01:32,180
untuk contoh pelatihan tertentu adalah y, misalnya, y mungkin 0 atau 1.

28
00:01:32,840 --> 00:01:39,240
Jadi biaya jaringan ini untuk satu contoh pelatihan adalah AL-y2.

29
00:01:40,260 --> 00:01:44,380
Kami akan menyatakan biaya satu contoh pelatihan tersebut sebagai c0.

30
00:01:45,900 --> 00:01:51,892
Sebagai pengingat, aktivasi terakhir ini ditentukan oleh bobot, yang saya sebut wL, 

31
00:01:51,892 --> 00:01:57,600
dikalikan aktivasi neuron sebelumnya ditambah beberapa bias, yang saya sebut bL.

32
00:01:57,600 --> 00:01:59,581
Kemudian Anda memompanya melalui beberapa fungsi 

33
00:01:59,581 --> 00:02:01,320
nonlinier khusus seperti sigmoid atau ReLU.

34
00:02:01,800 --> 00:02:05,491
Sebenarnya akan lebih mudah bagi kita jika kita memberi nama khusus untuk jumlah 

35
00:02:05,491 --> 00:02:09,320
tertimbang ini, seperti z, dengan superskrip yang sama dengan aktivasi yang relevan.

36
00:02:10,380 --> 00:02:15,139
Istilahnya sangat banyak, dan cara untuk mengonsepnya adalah dengan menggunakan bobot, 

37
00:02:15,139 --> 00:02:17,875
tindakan sebelumnya, dan bias untuk menghitung z, 

38
00:02:17,875 --> 00:02:21,869
yang pada gilirannya memungkinkan kita menghitung a, yang pada akhirnya, 

39
00:02:21,869 --> 00:02:25,480
bersama dengan konstanta y, memungkinkan kita menghitung biayanya.

40
00:02:27,340 --> 00:02:31,029
Dan tentu saja, AL-1 dipengaruhi oleh bobotnya sendiri, biasnya, 

41
00:02:31,029 --> 00:02:35,060
dan semacamnya, namun kami tidak akan fokus pada hal tersebut saat ini.

42
00:02:35,700 --> 00:02:37,620
Semua ini hanyalah angka, bukan?

43
00:02:38,060 --> 00:02:39,465
Dan akan sangat menyenangkan jika kita menganggap 

44
00:02:39,465 --> 00:02:41,040
masing-masing mempunyai garis bilangan kecilnya sendiri.

45
00:02:41,720 --> 00:02:45,261
Tujuan pertama kita adalah memahami seberapa sensitif 

46
00:02:45,261 --> 00:02:49,000
fungsi biaya terhadap perubahan kecil pada berat wL kita.

47
00:02:49,540 --> 00:02:54,860
Atau dengan kata lain, apa turunan dari c terhadap wL?

48
00:02:55,600 --> 00:03:00,634
Saat Anda melihat istilah del w ini, anggaplah itu berarti dorongan kecil ke w, 

49
00:03:00,634 --> 00:03:04,661
seperti perubahan sebesar 0.01, dan anggaplah istilah del c ini 

50
00:03:04,661 --> 00:03:08,060
berarti apa pun dampak yang dihasilkan terhadap biaya.

51
00:03:08,060 --> 00:03:10,220
Yang kami inginkan adalah rasionya.

52
00:03:11,260 --> 00:03:14,466
Secara konseptual, dorongan kecil terhadap wL ini menyebabkan 

53
00:03:14,466 --> 00:03:17,568
sejumlah dorongan terhadap zL, yang selanjutnya menyebabkan 

54
00:03:17,568 --> 00:03:21,240
sejumlah dorongan terhadap AL, yang secara langsung mempengaruhi biaya.

55
00:03:23,120 --> 00:03:28,235
Jadi kita memecahnya dengan terlebih dahulu melihat rasio perubahan 

56
00:03:28,235 --> 00:03:33,200
kecil zL terhadap perubahan kecil w, yaitu turunan zL terhadap wL.

57
00:03:33,200 --> 00:03:36,881
Demikian pula, Anda kemudian mempertimbangkan rasio perubahan 

58
00:03:36,881 --> 00:03:40,444
pada AL dengan perubahan kecil pada zL yang menyebabkannya, 

59
00:03:40,444 --> 00:03:44,660
serta rasio antara dorongan terakhir ke c dan dorongan perantara ke AL.

60
00:03:45,740 --> 00:03:50,478
Ini adalah aturan rantai, di mana mengalikan ketiga rasio ini 

61
00:03:50,478 --> 00:03:55,140
memberi kita sensitivitas c terhadap perubahan kecil pada wL.

62
00:03:56,880 --> 00:04:01,298
Jadi di layar saat ini, ada banyak simbol, dan luangkan waktu sejenak untuk 

63
00:04:01,298 --> 00:04:06,240
memastikan semuanya jelas, karena sekarang kita akan menghitung turunan yang relevan.

64
00:04:07,440 --> 00:04:14,180
Turunan c terhadap AL ternyata 2AL-y.

65
00:04:14,180 --> 00:04:18,341
Artinya ukurannya sebanding dengan perbedaan antara keluaran jaringan 

66
00:04:18,341 --> 00:04:22,919
dan keluaran yang kita inginkan, jadi jika keluaran tersebut sangat berbeda, 

67
00:04:22,919 --> 00:04:27,140
perubahan sekecil apa pun akan berdampak besar pada fungsi biaya akhir.

68
00:04:27,840 --> 00:04:33,065
Turunan AL terhadap zL hanyalah turunan dari fungsi sigmoid kita, 

69
00:04:33,065 --> 00:04:37,420
atau nonlinier apa pun yang Anda pilih untuk digunakan.

70
00:04:37,420 --> 00:04:46,160
Turunan dari zL terhadap wL menjadi AL-1.

71
00:04:46,160 --> 00:04:48,616
Saya tidak tahu tentang Anda, tapi menurut saya sangat mudah untuk 

72
00:04:48,616 --> 00:04:51,146
terpaku pada rumus tanpa meluangkan waktu sejenak untuk duduk santai 

73
00:04:51,146 --> 00:04:53,420
dan mengingatkan diri sendiri apa maksud semua rumus tersebut.

74
00:04:53,920 --> 00:04:58,538
Dalam kasus turunan terakhir ini, besar kecilnya pengaruh dorongan kecil terhadap 

75
00:04:58,538 --> 00:05:02,820
bobot pada lapisan terakhir bergantung pada seberapa kuat neuron sebelumnya.

76
00:05:03,380 --> 00:05:08,280
Ingat, di sinilah gagasan neuron-yang-api-bersama-kawat-bersama muncul.

77
00:05:09,200 --> 00:05:12,705
Dan semua ini merupakan turunan dari wL saja dari 

78
00:05:12,705 --> 00:05:15,720
biaya untuk satu contoh pelatihan tertentu.

79
00:05:16,440 --> 00:05:20,401
Karena fungsi biaya penuh melibatkan rata-rata semua biaya 

80
00:05:20,401 --> 00:05:23,758
tersebut di banyak contoh pelatihan yang berbeda, 

81
00:05:23,758 --> 00:05:28,660
turunannya memerlukan rata-rata ekspresi ini di seluruh contoh pelatihan.

82
00:05:28,660 --> 00:05:32,616
Tentu saja, itu hanyalah salah satu komponen vektor gradien, 

83
00:05:32,616 --> 00:05:38,260
yang dibangun dari turunan parsial fungsi biaya terhadap semua bobot dan bias tersebut.

84
00:05:40,640 --> 00:05:42,982
Namun meskipun itu hanya salah satu dari sekian banyak turunan parsial 

85
00:05:42,982 --> 00:05:45,260
yang kami perlukan, ini sudah lebih dari 50% pekerjaan yang berhasil.

86
00:05:46,340 --> 00:05:49,720
Sensitivitas terhadap bias, misalnya, hampir sama.

87
00:05:50,040 --> 00:05:55,020
Kita hanya perlu mengubah istilah del z del w ini menjadi del z del b.

88
00:05:58,420 --> 00:06:02,400
Dan jika dilihat dari rumus yang relevan, turunannya adalah 1.

89
00:06:06,140 --> 00:06:10,482
Selain itu, dan di sinilah gagasan untuk melakukan propagasi mundur muncul, 

90
00:06:10,482 --> 00:06:15,111
Anda dapat melihat betapa sensitifnya fungsi biaya ini terhadap aktivasi lapisan 

91
00:06:15,111 --> 00:06:15,740
sebelumnya.

92
00:06:15,740 --> 00:06:20,168
Yakni, turunan awal dalam ekspresi aturan rantai, 

93
00:06:20,168 --> 00:06:25,660
sensitivitas z terhadap aktivasi sebelumnya, menjadi bobot wL.

94
00:06:26,640 --> 00:06:30,696
Dan sekali lagi, meskipun kita tidak akan dapat secara langsung mempengaruhi aktivasi 

95
00:06:30,696 --> 00:06:33,997
lapisan sebelumnya, akan sangat membantu jika kita terus memantaunya, 

96
00:06:33,997 --> 00:06:37,723
karena sekarang kita dapat terus mengulangi gagasan aturan rantai yang sama ke 

97
00:06:37,723 --> 00:06:41,685
belakang untuk melihat seberapa sensitif fungsi biaya terhadap bobot sebelumnya dan 

98
00:06:41,685 --> 00:06:42,440
bias sebelumnya.

99
00:06:43,180 --> 00:06:45,971
Dan Anda mungkin berpikir ini adalah contoh yang terlalu sederhana, 

100
00:06:45,971 --> 00:06:48,516
karena semua lapisan memiliki satu neuron, dan segalanya akan 

101
00:06:48,516 --> 00:06:51,020
menjadi lebih rumit secara eksponensial untuk jaringan nyata.

102
00:06:51,700 --> 00:06:55,170
Tapi sejujurnya, tidak banyak perubahan ketika kita memberikan beberapa neuron 

103
00:06:55,170 --> 00:06:58,860
pada lapisan tersebut, sebenarnya itu hanya beberapa indeks lagi yang perlu dilacak.

104
00:06:59,380 --> 00:07:02,434
Daripada aktivasi lapisan tertentu hanya menjadi AL, 

105
00:07:02,434 --> 00:07:07,160
ia juga akan memiliki subskrip yang menunjukkan neuron mana pada lapisan tersebut.

106
00:07:07,160 --> 00:07:14,420
Mari kita gunakan huruf k untuk mengindeks layer L-1, dan j untuk mengindeks layer L.

107
00:07:15,260 --> 00:07:19,079
Untuk biayanya, sekali lagi kita lihat berapa keluaran yang diinginkan, 

108
00:07:19,079 --> 00:07:22,527
namun kali ini kita menjumlahkan kuadrat selisih antara aktivasi 

109
00:07:22,527 --> 00:07:25,180
lapisan terakhir ini dan keluaran yang diinginkan.

110
00:07:26,080 --> 00:07:30,840
Artinya, Anda mengambil jumlah ALj dikurangi yj kuadrat.

111
00:07:33,040 --> 00:07:37,060
Karena ada lebih banyak bobot, masing-masing bobot harus memiliki 

112
00:07:37,060 --> 00:07:40,838
beberapa indeks lagi untuk melacak posisinya, jadi sebut saja 

113
00:07:40,838 --> 00:07:44,920
bobot tepi yang menghubungkan neuron ke-k ini ke neuron ke-j, WLjk.

114
00:07:45,620 --> 00:07:48,380
Indeks tersebut mungkin terasa sedikit mundur pada awalnya, 

115
00:07:48,380 --> 00:07:52,291
tetapi hal ini sejalan dengan cara Anda mengindeks matriks bobot yang saya bicarakan 

116
00:07:52,291 --> 00:07:53,120
di video bagian 1.

117
00:07:53,620 --> 00:07:57,065
Sama seperti sebelumnya, masih bagus untuk memberi nama pada jumlah 

118
00:07:57,065 --> 00:08:00,663
tertimbang yang relevan, seperti z, sehingga aktivasi lapisan terakhir 

119
00:08:00,663 --> 00:08:04,160
hanyalah fungsi khusus Anda, seperti sigmoid, yang diterapkan pada z.

120
00:08:04,660 --> 00:08:07,639
Anda dapat memahami apa yang saya maksud, dimana semua persamaan ini pada 

121
00:08:07,639 --> 00:08:10,700
dasarnya sama dengan persamaan yang kita miliki sebelumnya dalam kasus satu 

122
00:08:10,700 --> 00:08:13,680
neuron per lapisan, hanya saja persamaan ini terlihat sedikit lebih rumit.

123
00:08:15,440 --> 00:08:19,208
Dan memang benar, ekspresi turunan aturan rantai yang menggambarkan 

124
00:08:19,208 --> 00:08:23,420
seberapa sensitif biaya terhadap bobot tertentu pada dasarnya terlihat sama.

125
00:08:23,920 --> 00:08:25,339
Saya serahkan kepada Anda untuk berhenti sejenak dan 

126
00:08:25,339 --> 00:08:26,840
memikirkan masing-masing istilah tersebut jika Anda mau.

127
00:08:28,979 --> 00:08:32,986
Namun yang berubah di sini adalah turunan biaya 

128
00:08:32,986 --> 00:08:36,659
terhadap salah satu aktivasi di lapisan L-1.

129
00:08:37,780 --> 00:08:40,613
Dalam hal ini, perbedaannya adalah neuron mempengaruhi 

130
00:08:40,613 --> 00:08:42,880
fungsi biaya melalui berbagai jalur berbeda.

131
00:08:44,680 --> 00:08:50,202
Artinya, di satu sisi mempengaruhi AL0 yang berperan dalam fungsi biaya, 

132
00:08:50,202 --> 00:08:56,632
tetapi juga berpengaruh terhadap AL1 yang juga berperan dalam fungsi biaya dan harus 

133
00:08:56,632 --> 00:08:57,540
dijumlahkan.

134
00:08:59,820 --> 00:09:03,040
Dan itu, cukup banyak.

135
00:09:03,500 --> 00:09:06,769
Setelah Anda mengetahui seberapa sensitif fungsi biaya terhadap aktivasi 

136
00:09:06,769 --> 00:09:09,769
di lapisan kedua hingga terakhir ini, Anda dapat mengulangi proses 

137
00:09:09,769 --> 00:09:12,860
untuk semua bobot dan bias yang dimasukkan ke dalam lapisan tersebut.

138
00:09:13,900 --> 00:09:14,960
Jadi tepuk-tepuk punggungmu!

139
00:09:15,300 --> 00:09:18,860
Jika semua ini masuk akal, Anda sekarang telah melihat jauh ke dalam 

140
00:09:18,860 --> 00:09:22,680
inti propagasi mundur, pekerja keras di balik cara jaringan saraf belajar.

141
00:09:23,300 --> 00:09:28,300
Ekspresi aturan rantai ini memberi Anda turunan yang menentukan setiap komponen dalam 

142
00:09:28,300 --> 00:09:33,300
gradien yang membantu meminimalkan biaya jaringan dengan berulang kali menuruni bukit.

143
00:09:34,300 --> 00:09:36,488
Jika Anda duduk santai dan memikirkan semua itu, 

144
00:09:36,488 --> 00:09:39,167
ada banyak lapisan kerumitan yang menyelimuti pikiran Anda, 

145
00:09:39,167 --> 00:09:42,740
jadi jangan khawatir jika pikiran Anda memerlukan waktu untuk mencerna semuanya.

