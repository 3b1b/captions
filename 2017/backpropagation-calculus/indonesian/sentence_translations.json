[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "Asumsi sulitnya di sini adalah Anda telah menonton bagian 3, yang memberikan panduan intuitif tentang algoritma propagasi mundur.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Di sini kita menjadi sedikit lebih formal dan mendalami kalkulus yang relevan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "Wajar jika hal ini setidaknya sedikit membingungkan, jadi mantra untuk berhenti sejenak dan merenung secara teratur pasti berlaku di sini dan di mana pun.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "Tujuan utama kami adalah untuk menunjukkan bagaimana orang-orang dalam pembelajaran mesin umumnya berpikir tentang aturan rantai dari kalkulus dalam konteks jaringan, yang memiliki nuansa berbeda dari pendekatan sebagian besar kursus pengantar kalkulus terhadap subjek tersebut.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Bagi Anda yang merasa tidak nyaman dengan kalkulus yang relevan, saya memiliki serangkaian topik lengkap tentang topik tersebut.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Mari kita mulai dengan jaringan yang sangat sederhana, dimana setiap lapisan memiliki satu neuron di dalamnya.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Jaringan ini ditentukan oleh tiga bobot dan tiga bias, dan tujuan kami adalah memahami seberapa sensitif fungsi biaya terhadap variabel-variabel ini.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "Dengan begitu kita mengetahui penyesuaian mana pada ketentuan tersebut yang akan menyebabkan penurunan fungsi biaya yang paling efisien.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "We'll just focus on the connection between the last two neurons.",
  "translatedText": "Kami hanya akan fokus pada hubungan antara dua neuron terakhir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in.",
  "translatedText": "Mari beri label aktivasi neuron terakhir dengan superskrip L, yang menunjukkan di lapisan mana neuron tersebut berada.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.98,
  "end": 71.36
 },
 {
  "input": "So the activation of the previous neuron is AL-1.",
  "translatedText": "Jadi aktivasi neuron sebelumnya adalah AL-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.68,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Ini bukan eksponen, ini hanyalah cara mengindeks apa yang sedang kita bicarakan, karena saya ingin menyimpan subskrip untuk indeks yang berbeda nanti.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Katakanlah nilai yang kita inginkan untuk aktivasi terakhir ini untuk contoh pelatihan tertentu adalah y, misalnya, y mungkin 0 atau 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is AL-y2.",
  "translatedText": "Jadi biaya jaringan ini untuk satu contoh pelatihan adalah AL-y2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Kami akan menyatakan biaya satu contoh pelatihan tersebut sebagai c0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call wL, times the previous neuron's activation plus some bias, which I'll call bL.",
  "translatedText": "Sebagai pengingat, aktivasi terakhir ini ditentukan oleh bobot, yang saya sebut wL, dikalikan aktivasi neuron sebelumnya ditambah beberapa bias, yang saya sebut bL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "Then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "Kemudian Anda memompanya melalui beberapa fungsi nonlinier khusus seperti sigmoid atau ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "Sebenarnya akan lebih mudah bagi kita jika kita memberi nama khusus untuk jumlah tertimbang ini, seperti z, dengan superskrip yang sama dengan aktivasi yang relevan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action, and bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "Istilahnya sangat banyak, dan cara untuk mengonsepnya adalah dengan menggunakan bobot, tindakan sebelumnya, dan bias untuk menghitung z, yang pada gilirannya memungkinkan kita menghitung a, yang pada akhirnya, bersama dengan konstanta y, memungkinkan kita menghitung biayanya.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course, AL-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "Dan tentu saja, AL-1 dipengaruhi oleh bobotnya sendiri, biasnya, dan semacamnya, namun kami tidak akan fokus pada hal tersebut saat ini.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Semua ini hanyalah angka, bukan?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "Dan akan sangat menyenangkan jika kita menganggap masing-masing mempunyai garis bilangan kecilnya sendiri.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight wL.",
  "translatedText": "Tujuan pertama kita adalah memahami seberapa sensitif fungsi biaya terhadap perubahan kecil pada berat wL kita.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to wL?",
  "translatedText": "Atau dengan kata lain, apa turunan dari c terhadap wL?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del w term, think of it as meaning some tiny nudge to w, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Saat Anda melihat istilah del w ini, anggaplah itu berarti dorongan kecil ke w, seperti perubahan sebesar 0.01, dan anggaplah istilah del c ini berarti apa pun dampak yang dihasilkan terhadap biaya.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "Yang kami inginkan adalah rasionya.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to wL causes some nudge to zL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Secara konseptual, dorongan kecil terhadap wL ini menyebabkan sejumlah dorongan terhadap zL, yang selanjutnya menyebabkan sejumlah dorongan terhadap AL, yang secara langsung mempengaruhi biaya.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to zL to this tiny change w, that is, the derivative of zL with respect to wL.",
  "translatedText": "Jadi kita memecahnya dengan terlebih dahulu melihat rasio perubahan kecil zL terhadap perubahan kecil w, yaitu turunan zL terhadap wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in zL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "Demikian pula, Anda kemudian mempertimbangkan rasio perubahan pada AL dengan perubahan kecil pada zL yang menyebabkannya, serta rasio antara dorongan terakhir ke c dan dorongan perantara ke AL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying these three ratios gives us the sensitivity of c to small changes in wL.",
  "translatedText": "Ini adalah aturan rantai, di mana mengalikan ketiga rasio ini memberi kita sensitivitas c terhadap perubahan kecil pada wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "Jadi di layar saat ini, ada banyak simbol, dan luangkan waktu sejenak untuk memastikan semuanya jelas, karena sekarang kita akan menghitung turunan yang relevan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "Turunan c terhadap AL ternyata 2AL-y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "This means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Artinya ukurannya sebanding dengan perbedaan antara keluaran jaringan dan keluaran yang kita inginkan, jadi jika keluaran tersebut sangat berbeda, perubahan sekecil apa pun akan berdampak besar pada fungsi biaya akhir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to zL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "Turunan AL terhadap zL hanyalah turunan dari fungsi sigmoid kita, atau nonlinier apa pun yang Anda pilih untuk digunakan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "The derivative of zL with respect to wL comes out to be AL-1.",
  "translatedText": "Turunan dari zL terhadap wL menjadi AL-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself what they all mean.",
  "translatedText": "Saya tidak tahu tentang Anda, tapi menurut saya sangat mudah untuk terpaku pada rumus tanpa meluangkan waktu sejenak untuk duduk santai dan mengingatkan diri sendiri apa maksud semua rumus tersebut.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "Dalam kasus turunan terakhir ini, besar kecilnya pengaruh dorongan kecil terhadap bobot pada lapisan terakhir bergantung pada seberapa kuat neuron sebelumnya.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "Ingat, di sinilah gagasan neuron-yang-api-bersama-kawat-bersama muncul.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to wL only of the cost for a specific single training example.",
  "translatedText": "Dan semua ini merupakan turunan dari wL saja dari biaya untuk satu contoh pelatihan tertentu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Karena fungsi biaya penuh melibatkan rata-rata semua biaya tersebut di banyak contoh pelatihan yang berbeda, turunannya memerlukan rata-rata ekspresi ini di seluruh contoh pelatihan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "Of course, that's just one component of the gradient vector, which is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "Tentu saja, itu hanyalah salah satu komponen vektor gradien, yang dibangun dari turunan parsial fungsi biaya terhadap semua bobot dan bias tersebut.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "Namun meskipun itu hanya salah satu dari sekian banyak turunan parsial yang kami perlukan, ini sudah lebih dari 50% pekerjaan yang berhasil.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "Sensitivitas terhadap bias, misalnya, hampir sama.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Kita hanya perlu mengubah istilah del z del w ini menjadi del z del b.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "Dan jika dilihat dari rumus yang relevan, turunannya adalah 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "Selain itu, dan di sinilah gagasan untuk melakukan propagasi mundur muncul, Anda dapat melihat betapa sensitifnya fungsi biaya ini terhadap aktivasi lapisan sebelumnya.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight wL.",
  "translatedText": "Yakni, turunan awal dalam ekspresi aturan rantai, sensitivitas z terhadap aktivasi sebelumnya, menjadi bobot wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "Dan sekali lagi, meskipun kita tidak akan dapat secara langsung mempengaruhi aktivasi lapisan sebelumnya, akan sangat membantu jika kita terus memantaunya, karena sekarang kita dapat terus mengulangi gagasan aturan rantai yang sama ke belakang untuk melihat seberapa sensitif fungsi biaya terhadap bobot sebelumnya dan bias sebelumnya.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "Dan Anda mungkin berpikir ini adalah contoh yang terlalu sederhana, karena semua lapisan memiliki satu neuron, dan segalanya akan menjadi lebih rumit secara eksponensial untuk jaringan nyata.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "Tapi sejujurnya, tidak banyak perubahan ketika kita memberikan beberapa neuron pada lapisan tersebut, sebenarnya itu hanya beberapa indeks lagi yang perlu dilacak.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "Daripada aktivasi lapisan tertentu hanya menjadi AL, ia juga akan memiliki subskrip yang menunjukkan neuron mana pada lapisan tersebut.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Mari kita gunakan huruf k untuk mengindeks layer L-1, dan j untuk mengindeks layer L.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "Untuk biayanya, sekali lagi kita lihat berapa keluaran yang diinginkan, namun kali ini kita menjumlahkan kuadrat selisih antara aktivasi lapisan terakhir ini dan keluaran yang diinginkan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus yj squared.",
  "translatedText": "Artinya, Anda mengambil jumlah ALj dikurangi yj kuadrat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Karena ada lebih banyak bobot, masing-masing bobot harus memiliki beberapa indeks lagi untuk melacak posisinya, jadi sebut saja bobot tepi yang menghubungkan neuron ke-k ini ke neuron ke-j, WLjk.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "Indeks tersebut mungkin terasa sedikit mundur pada awalnya, tetapi hal ini sejalan dengan cara Anda mengindeks matriks bobot yang saya bicarakan di video bagian 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "Sama seperti sebelumnya, masih bagus untuk memberi nama pada jumlah tertimbang yang relevan, seperti z, sehingga aktivasi lapisan terakhir hanyalah fungsi khusus Anda, seperti sigmoid, yang diterapkan pada z.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Anda dapat memahami apa yang saya maksud, dimana semua persamaan ini pada dasarnya sama dengan persamaan yang kita miliki sebelumnya dalam kasus satu neuron per lapisan, hanya saja persamaan ini terlihat sedikit lebih rumit.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain rule derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "Dan memang benar, ekspresi turunan aturan rantai yang menggambarkan seberapa sensitif biaya terhadap bobot tertentu pada dasarnya terlihat sama.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Saya serahkan kepada Anda untuk berhenti sejenak dan memikirkan masing-masing istilah tersebut jika Anda mau.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "Namun yang berubah di sini adalah turunan biaya terhadap salah satu aktivasi di lapisan L-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "Dalam hal ini, perbedaannya adalah neuron mempengaruhi fungsi biaya melalui berbagai jalur berbeda.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "Artinya, di satu sisi mempengaruhi AL0 yang berperan dalam fungsi biaya, tetapi juga berpengaruh terhadap AL1 yang juga berperan dalam fungsi biaya dan harus dijumlahkan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "Dan itu, cukup banyak.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Setelah Anda mengetahui seberapa sensitif fungsi biaya terhadap aktivasi di lapisan kedua hingga terakhir ini, Anda dapat mengulangi proses untuk semua bobot dan bias yang dimasukkan ke dalam lapisan tersebut.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "Jadi tepuk-tepuk punggungmu!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Jika semua ini masuk akal, Anda sekarang telah melihat jauh ke dalam inti propagasi mundur, pekerja keras di balik cara jaringan saraf belajar.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Ekspresi aturan rantai ini memberi Anda turunan yang menentukan setiap komponen dalam gradien yang membantu meminimalkan biaya jaringan dengan berulang kali menuruni bukit.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Jika Anda duduk santai dan memikirkan semua itu, ada banyak lapisan kerumitan yang menyelimuti pikiran Anda, jadi jangan khawatir jika pikiran Anda memerlukan waktu untuk mencerna semuanya.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]