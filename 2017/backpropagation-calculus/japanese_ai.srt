1
00:00:00,000 --> 00:00:08,420
ここでの厳密な前提条件は、バックプロパゲーション アルゴリズムの直

2
00:00:08,420 --> 00:00:11,160
感的なウォークスルーを提供するパート 3 を視聴していることです。

3
00:00:11,160 --> 00:00:14,920
ここではもう少し形式的に、関連する微積分について詳しく説明します。

4
00:00:14,920 --> 00:00:18,560
少なくとも少し混乱するのは普通のことなので、定期的に立ち止まって熟考

5
00:00:18,560 --> 00:00:22,000
するという信条は、他の場所と同じようにここでも確かに当てはまります。

6
00:00:22,000 --> 00:00:26,620
私たちの主な目標は、機械学習の人々がネットワークの文脈で微積分の連鎖則につ

7
00:00:26,620 --> 00:00:31,900
いてどのように一般的に考えているかを示すことです。これは、ほとんどの微積分

8
00:00:31,900 --> 00:00:34,580
入門コースがこの主題にアプローチする方法とは異なる雰囲気を持っています。

9
00:00:34,580 --> 00:00:38,300
関連する微積分に抵抗がある人のために、このト

10
00:00:38,300 --> 00:00:39,300
ピックに関するシリーズ全体を用意しています。

11
00:00:39,300 --> 00:00:44,840
各層に 1 つのニューロンが含まれる、非

12
00:00:44,840 --> 00:00:46,780
常に単純なネットワークから始めましょう。

13
00:00:46,780 --> 00:00:51,880
このネットワークは 3 つの重みと 3 つのバイアスによって決定されます。私たちの

14
00:00:51,880 --> 00:00:55,640
目標は、コスト関数がこれらの変数に対してどの程度敏感であるかを理解することです。

15
00:00:55,640 --> 00:00:59,780
そうすることで、これらの項に対するどの調整がコスト関数

16
00:00:59,780 --> 00:01:01,100
の最も効率的な減少を引き起こすかを知ることができます。

17
00:01:01,100 --> 00:01:05,360
最後の 2 つのニューロン間の接続にのみ焦点を当てます。

18
00:01:05,360 --> 00:01:10,400
最後のニューロンの活性化に上付き文字 L

19
00:01:10,400 --> 00:01:11,800
を付けて、どの層にあるかを示しましょう。

20
00:01:11,800 --> 00:01:16,560
したがって、前のニューロンの活性化は AL-1 です。

21
00:01:16,560 --> 00:01:20,120
これらは指数ではなく、後で別のインデックスの添字を保存したいので、

22
00:01:20,120 --> 00:01:23,120
ここで話しているものにインデックスを付けるための単なる方法です。

23
00:01:23,600 --> 00:01:28,880
特定のトレーニング例でこの最後のアクティベーションに必要な値が y であ

24
00:01:28,880 --> 00:01:33,020
るとします。たとえば、y は 0 または 1 である可能性があります。

25
00:01:33,020 --> 00:01:39,040
したがって、単一のトレーニング例に対するこのネットワークのコストは AL-y2 です。

26
00:01:39,040 --> 00:01:46,120
その 1 つのトレーニング例のコストを c0 と表記します。

27
00:01:46,120 --> 00:01:51,920
念のため言っておきますが、この最後の活性化は、前のニューロンの活性化とバイアス (bL

28
00:01:51,920 --> 00:01:57,600
と呼ぶことにします) を掛けた重み (wL と呼ぶことにします) によって決定されます。

29
00:01:57,600 --> 00:02:01,560
次に、それをシグモイドや ReLU などの特別な非線形関数を通してポンプします。

30
00:02:01,560 --> 00:02:05,400
実際、この重み付けされた合計に、関連するアクティベーションと同じ上付き

31
00:02:05,400 --> 00:02:10,600
文字を付けた z のような特別な名前を付けると、作業が簡単になります。

32
00:02:10,600 --> 00:02:15,320
これには多くの用語が含まれますが、これを概念化すると、重み、以前のアクション、バ

33
00:02:15,320 --> 00:02:21,800
イアスがすべて一緒になって z を計算するために使用され、それによって a が計

34
00:02:21,800 --> 00:02:27,360
算され、最後に定数 y とともに次のようになります。私たちがコストを計算します。

35
00:02:27,360 --> 00:02:33,440
そしてもちろん、AL-1 はそれ自体の重みやバイアスなどの影

36
00:02:33,440 --> 00:02:35,920
響を受けますが、今はそれに焦点を当てるつもりはありません。

37
00:02:35,920 --> 00:02:38,120
これらはすべて単なる数字ですよね？

38
00:02:38,120 --> 00:02:41,960
そして、それぞれが独自の小さな数直線を持っていると考えるとよいでしょう。

39
00:02:41,960 --> 00:02:47,480
私たちの最初の目標は、重み wL の小さな変化に対して

40
00:02:47,480 --> 00:02:49,820
コスト関数がどの程度敏感であるかを理解することです。

41
00:02:49,820 --> 00:02:55,740
別の言い方をすると、wL に関する c の導関数は何ですか?

42
00:02:55,740 --> 00:03:01,220
この del w という用語を見たときは、0 による変更など、w に対する小さなナッジを意味すると考えてくださ

43
00:03:01,220 --> 00:03:08,820
い。01 であり、この del c という用語は、結果として生じるコストの微調整を意味すると考えてください。

44
00:03:08,820 --> 00:03:10,900
私たちが知りたいのはそれらの比率です。

45
00:03:10,900 --> 00:03:17,740
概念的には、wL へのこの小さなナッジは zL へのナッジを引き起こし

46
00:03:17,740 --> 00:03:23,220
、それが今度は AL へのナッジを引き起こし、コストに直接影響します。

47
00:03:23,220 --> 00:03:28,020
そこで、最初に zL に対する小さな変化とこの小さな変化 w の比、つ

48
00:03:28,020 --> 00:03:33,340
まり wL に対する zL の導関数を調べることで物事を細分化します。

49
00:03:33,340 --> 00:03:38,820
同様に、次に、AL への変化とそれを引き起こした z

50
00:03:38,820 --> 00:03:43,900
L の小さな変化の比率、および c への最後のナッジ

51
00:03:43,900 --> 00:03:45,900
と AL へのこの中間のナッジとの比率を考慮します。

52
00:03:45,900 --> 00:03:51,880
これは連鎖則であり、これら 3 つの比率を乗算すると、

53
00:03:51,880 --> 00:03:57,340
wL の小さな変化に対する c の感度が得られます。

54
00:03:57,340 --> 00:04:01,620
現在、画面上にはたくさんのシンボルが表示されていますが、これから関連する導関数を計

55
00:04:01,620 --> 00:04:07,460
算するので、それらがすべて明確であることを確認するために少し時間を取ってください。

56
00:04:07,460 --> 00:04:14,220
AL に関する c の導関数は、2AL-y となります。

57
00:04:14,220 --> 00:04:19,300
これは、そのサイズがネットワークの出力と私たちが望むものとの差に比例

58
00:04:19,300 --> 00:04:24,480
することを意味します。そのため、その出力が大きく異なる場合、わずか

59
00:04:24,480 --> 00:04:28,380
な変更でも最終的なコスト関数に大きな影響を与える可能性があります。

60
00:04:28,380 --> 00:04:33,860
zL に関する AL の導関数は、シグモイド関数

61
00:04:33,860 --> 00:04:37,420
、または使用する非線形性の導関数にすぎません。

62
00:04:37,420 --> 00:04:46,180
wL に関する zL の導関数は AL-1 になります。

63
00:04:46,180 --> 00:04:49,460
あなたはどうか知りませんが、じっくりと時間をかけて公式の意味を

64
00:04:49,460 --> 00:04:54,180
思い出さずに、公式にどっぷりと浸かってしまいがちだと思います。

65
00:04:54,180 --> 00:04:58,860
この最後の導関数の場合、重みへの小さな微調整が最後の層

66
00:04:58,860 --> 00:05:03,220
に与える影響の量は、前のニューロンの強さに依存します。

67
00:05:03,220 --> 00:05:09,320
覚えておいてください、ここで、ニューロンが一緒に発火し、一緒にワイヤリングするというアイデアが登場します。

68
00:05:09,320 --> 00:05:14,840
そして、これはすべて、特定の 1 つのトレーニング例

69
00:05:14,840 --> 00:05:16,580
のコストのみを wL に関して導関数したものです。

70
00:05:16,580 --> 00:05:20,940
フルコスト関数には、多くの異なるトレーニング例にわたるすべ

71
00:05:20,940 --> 00:05:27,300
てのコストの平均が含まれるため、その導関数では、すべての

72
00:05:27,300 --> 00:05:28,540
トレーニング例にわたってこの式を平均する必要があります。

73
00:05:28,540 --> 00:05:33,860
もちろん、これは勾配ベクトルの 1 つのコンポーネントにすぎず、す

74
00:05:33,860 --> 00:05:40,780
べての重みとバイアスに関するコスト関数の偏導関数から構築されます。

75
00:05:40,780 --> 00:05:44,340
しかし、これは必要な偏導関数の 1 つにすぎ

76
00:05:44,340 --> 00:05:46,460
ませんが、作業の 50% 以上を占めます。

77
00:05:46,460 --> 00:05:50,300
たとえば、バイアスに対する感度はほぼ同じです。

78
00:05:50,300 --> 00:05:58,980
この del z del w という用語を del z del b に変更するだけです。

79
00:05:58,980 --> 00:06:04,700
関連する式を見ると、その微分値は 1 になります。

80
00:06:04,700 --> 00:06:11,700
また、ここで逆方向に伝播するというアイデアが登場しますが、このコスト

81
00:06:11,700 --> 00:06:16,180
関数が前の層のアクティブ化に対してどれほど敏感であるかがわかります。

82
00:06:16,180 --> 00:06:21,380
つまり、連鎖ルール式におけるこの初期導関数、つまり前

83
00:06:21,380 --> 00:06:25,420
の起動に対する z の感度が重み wL になります。

84
00:06:25,420 --> 00:06:30,100
繰り返しますが、前の層のアクティブ化に直接影響を与えることは

85
00:06:30,100 --> 00:06:35,280
できませんが、同じチェーン ルールのアイデアを逆方向に反復し

86
00:06:35,280 --> 00:06:40,780
続けるだけで、コスト関数がどの程度敏感であるかを確認できるた

87
00:06:40,780 --> 00:06:43,680
め、追跡するのには役立ちます。以前の重みと以前のバイアス。

88
00:06:43,680 --> 00:06:47,940
すべての層には 1 つのニューロンがあり、実際のネットワークでは事態は

89
00:06:47,940 --> 00:06:51,320
指数関数的に複雑になるため、これは単純すぎる例だと思うかもしれません。

90
00:06:51,320 --> 00:06:56,560
しかし、正直に言うと、レイヤーに複数のニューロンを与えてもそれほど大きな変化

91
00:06:56,560 --> 00:06:59,320
はありません。実際には、追跡するインデックスがさらにいくつか増えるだけです。

92
00:06:59,320 --> 00:07:03,580
特定の層の活性化が単に AL であるのではなく、その層

93
00:07:03,580 --> 00:07:07,920
のどのニューロンであるかを示す添え字も付けられます。

94
00:07:07,920 --> 00:07:15,280
文字 k を使用してレイヤー L-1 にインデックスを付け、j を使用してレイヤー L にインデックスを付けましょう。

95
00:07:15,280 --> 00:07:20,720
コストについては、再び望ましい出力が何であるかを調べますが、今回は、

96
00:07:20,720 --> 00:07:26,120
これらの最後の層のアクティブ化と望ましい出力の差の二乗を合計します。

97
00:07:26,120 --> 00:07:33,280
つまり、ALj から yj の 2 乗を引いた合計を計算します。

98
00:07:33,280 --> 00:07:36,500
さらに多くの重みがあるため、それぞれがどこにあるかを追跡するためにさらに

99
00:07:36,500 --> 00:07:41,380
いくつかのインデックスを持つ必要があるため、この k 番目のニューロンを

100
00:07:41,380 --> 00:07:45,740
j 番目のニューロンに接続するエッジの重みを WLjk と呼びます。

101
00:07:45,740 --> 00:07:49,820
これらのインデックスは最初は少し時代遅れに感じるかもしれませんが、パート

102
00:07:49,820 --> 00:07:53,800
1 のビデオで説明した重み行列のインデックス付け方法と一致しています。

103
00:07:53,800 --> 00:07:57,660
前と同じように、関連する重み付けされた合計に z などの名

104
00:07:57,660 --> 00:08:03,540
前を付けると、最後の層のアクティブ化が z に適用される

105
00:08:03,540 --> 00:08:04,980
シグモイドのような特別な関数にすぎなくなるので便利です。

106
00:08:04,980 --> 00:08:09,100
私が言いたいことはわかると思いますが、これらはすべて、レイヤーごとに 1 ニュー

107
00:08:09,100 --> 00:08:15,420
ロンの場合に以前に作成した方程式と本質的に同じですが、少し複雑に見えるだけです。

108
00:08:15,420 --> 00:08:20,620
そして実際、特定の重みに対するコストの感度を表す連

109
00:08:20,620 --> 00:08:23,540
鎖ルールの導関数式は、本質的に同じように見えます。

110
00:08:23,540 --> 00:08:29,420
必要に応じて、立ち止まってそれぞれの用語について考えてみてください。

111
00:08:29,420 --> 00:08:34,900
ただし、ここで変わるのは、レイヤー L-1 のアクテ

112
00:08:34,900 --> 00:08:37,820
ィベーションの 1 つに関するコストの導関数です。

113
00:08:37,820 --> 00:08:42,000
この場合の違いは、ニューロンが複数の異なるパ

114
00:08:42,000 --> 00:08:43,540
スを通じてコスト関数に影響を与えることです。

115
00:08:43,540 --> 00:08:51,200
つまり、コスト関数の役割を果たす AL0 に影響を与え

116
00:08:51,200 --> 00:08:56,460
る一方で、同じくコスト関数の役割を果たす AL1 に

117
00:08:56,460 --> 00:09:00,340
も影響を与えるため、それらを合計する必要があります。

118
00:09:00,340 --> 00:09:03,680
そして、まあ、それだけです。

119
00:09:03,680 --> 00:09:08,240
この最後から 2 番目の層のアクティベーションに対するコ

120
00:09:08,240 --> 00:09:12,520
スト関数の感度がわかったら、その層に入力されるすべての

121
00:09:12,520 --> 00:09:13,920
重みとバイアスに対してこのプロセスを繰り返すだけです。

122
00:09:13,920 --> 00:09:15,420
だから自分の背中を押してください！

123
00:09:15,420 --> 00:09:20,480
これらすべてが理解できるのであれば、ニューラル ネットワークの学習方法の背後

124
00:09:20,480 --> 00:09:23,700
にある主力であるバックプロパゲーションの中心部を深く調べたことになります。

125
00:09:23,700 --> 00:09:27,960
これらのチェーン ルール式により、勾配内の各コンポーネントを決定する導関数が得

126
00:09:27,960 --> 00:09:35,020
られ、下り坂を繰り返してネットワークのコストを最小限に抑えることができます。

127
00:09:35,020 --> 00:09:38,960
座ってこれらすべてについて考えると、これはあなたの心を包み込む複雑な層にな

128
00:09:38,960 --> 00:09:42,840
るため、頭がすべてを消化するのに時間がかかっても心配する必要はありません。

