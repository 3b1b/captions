1
00:00:00,000 --> 00:00:08,420
فرض سخت در اینجا این است که شما قسمت 3 را

2
00:00:08,420 --> 00:00:11,160
تماشا کرده‌اید، که یک راهنما بصری از الگوریتم انتشار پس‌انداز است.

3
00:00:11,160 --> 00:00:14,920
در اینجا کمی رسمی تر می شویم و به محاسبات مربوطه می پردازیم.

4
00:00:14,920 --> 00:00:18,560
طبیعی است که این حداقل کمی گیج کننده باشد، بنابراین مانترا برای مکث و

5
00:00:18,560 --> 00:00:22,000
اندیشیدن منظم مطمئناً در اینجا نیز مانند هر جای دیگر اعمال می شود.

6
00:00:22,000 --> 00:00:26,620
هدف اصلی ما این است که نشان دهیم مردم در یادگیری ماشین معمولاً در مورد قانون

7
00:00:26,620 --> 00:00:31,900
زنجیره ای از حساب دیفرانسیل و انتگرال در زمینه شبکه ها فکر می کنند، که احساسی

8
00:00:31,900 --> 00:00:34,580
متفاوت از نحوه برخورد بیشتر دوره های حساب دیفرانسیل و انتگرال مقدماتی به موضوع دارد.

9
00:00:34,580 --> 00:00:38,300
برای کسانی از شما که از محاسبه مربوطه ناراحت

10
00:00:38,300 --> 00:00:39,300
هستید، من یک سری کامل در این زمینه دارم.

11
00:00:39,300 --> 00:00:44,840
بیایید با یک شبکه بسیار ساده شروع کنیم، شبکه

12
00:00:44,840 --> 00:00:46,780
ای که در آن هر لایه یک نورون دارد.

13
00:00:46,780 --> 00:00:51,880
این شبکه با سه وزن و سه سوگیری تعیین می‌شود و هدف ما این

14
00:00:51,880 --> 00:00:55,640
است که بفهمیم تابع هزینه تا چه حد به این متغیرها حساس است.

15
00:00:55,640 --> 00:00:59,780
به این ترتیب ما می دانیم که کدام تعدیل در

16
00:00:59,780 --> 00:01:01,100
آن شرایط باعث کاهش کارآمدترین تابع هزینه می شود.

17
00:01:01,100 --> 00:01:05,360
ما فقط بر ارتباط بین دو نورون آخر تمرکز خواهیم کرد.

18
00:01:05,360 --> 00:01:10,400
بیایید فعال شدن آخرین نورون را با علامت L برچسب گذاری

19
00:01:10,400 --> 00:01:11,800
کنیم، که نشان می دهد در کدام لایه قرار دارد.

20
00:01:11,800 --> 00:01:16,560
بنابراین فعال شدن نورون قبلی AL-1 است.

21
00:01:16,560 --> 00:01:20,120
اینها نما نیستند، آنها فقط راهی برای نمایه سازی چیزی هستند که در مورد آن صحبت

22
00:01:20,120 --> 00:01:23,120
می کنیم، زیرا می خواهم در آینده زیرنویس ها را برای شاخص های مختلف ذخیره کنم.

23
00:01:23,600 --> 00:01:28,880
فرض کنید مقداری که می‌خواهیم این آخرین فعال‌سازی برای مثال آموزشی داده شده

24
00:01:28,880 --> 00:01:33,020
باشد y است، برای مثال، y ممکن است 0 یا 1 باشد.

25
00:01:33,020 --> 00:01:39,040
بنابراین هزینه این شبکه برای یک نمونه آموزشی AL-y2 است.

26
00:01:39,040 --> 00:01:46,120
ما هزینه آن یک مثال آموزشی را به عنوان c0 نشان خواهیم داد.

27
00:01:46,120 --> 00:01:51,920
برای یادآوری، این آخرین فعال‌سازی با وزنی تعیین می‌شود که من آن را wL می‌نامم،

28
00:01:51,920 --> 00:01:57,600
ضربدر فعال‌سازی نورون قبلی به اضافه مقداری سوگیری، که من آن را bL می‌نامم.

29
00:01:57,600 --> 00:02:01,560
سپس آن را از طریق یک تابع غیرخطی خاص مانند سیگموئید یا ReLU پمپ می کنید.

30
00:02:01,560 --> 00:02:05,400
در واقع کار را برای ما آسان‌تر می‌کند اگر به این مجموع

31
00:02:05,400 --> 00:02:10,600
وزنی، مانند z، با همان بالانویس فعال‌سازی‌های مربوطه، یک نام خاص بدهیم.

32
00:02:10,600 --> 00:02:15,320
این اصطلاحات زیادی است، و راهی که می‌توانید آن را مفهوم‌سازی کنید این است که وزن، عملکرد قبلی و

33
00:02:15,320 --> 00:02:21,800
بایاس با هم برای محاسبه z استفاده می‌شوند، که به نوبه خود به ما اجازه می‌دهد a را

34
00:02:21,800 --> 00:02:27,360
محاسبه کنیم، که در نهایت، همراه با یک ثابت y، اجازه می‌دهد ما هزینه را محاسبه می کنیم

35
00:02:27,360 --> 00:02:33,440
و البته، AL-1 تحت تأثیر وزن و تعصب و مواردی از این

36
00:02:33,440 --> 00:02:35,920
قبیل است، اما ما در حال حاضر روی آن تمرکز نداریم.

37
00:02:35,920 --> 00:02:38,120
همه اینها فقط اعداد هستند، درست است؟

38
00:02:38,120 --> 00:02:41,960
و این می تواند خوب باشد که هر یک را به عنوان یک خط اعداد کوچک خود در نظر بگیریم.

39
00:02:41,960 --> 00:02:47,480
اولین هدف ما این است که بفهمیم تابع هزینه چقدر

40
00:02:47,480 --> 00:02:49,820
نسبت به تغییرات کوچک در وزن وزن ما حساس است.

41
00:02:49,820 --> 00:02:55,740
یا عبارت دیگر، مشتق c نسبت به wL چیست؟

42
00:02:55,740 --> 00:03:01,220
وقتی این عبارت del w را می بینید، آن را به معنای یک حرکت کوچک به w، مانند تغییر 0 در نظر بگیرید.

43
00:03:01,220 --> 00:03:08,820
01، و این اصطلاح del c را به این معنا در نظر بگیرید که هر گونه فشار حاصله به هزینه است.

44
00:03:08,820 --> 00:03:10,900
آنچه ما می خواهیم نسبت آنهاست.

45
00:03:10,900 --> 00:03:17,740
از نظر مفهومی، این حرکت کوچک به wL باعث ایجاد مقداری ضربه به zL می شود، که به

46
00:03:17,740 --> 00:03:23,220
نوبه خود باعث ایجاد مقداری ضربه به AL می شود که مستقیماً بر هزینه تأثیر می گذارد.

47
00:03:23,220 --> 00:03:28,020
بنابراین، ابتدا با نگاه کردن به نسبت یک تغییر کوچک به zL به این

48
00:03:28,020 --> 00:03:33,340
تغییر کوچک w، یعنی مشتق zL نسبت به wL، چیزها را تجزیه می کنیم.

49
00:03:33,340 --> 00:03:38,820
به همین ترتیب، سپس نسبت تغییر به AL را به تغییر کوچک

50
00:03:38,820 --> 00:03:43,900
zL که باعث آن شده است، و همچنین نسبت بین ضربه نهایی

51
00:03:43,900 --> 00:03:45,900
به c و این حرکت میانی به AL را در نظر بگیرید.

52
00:03:45,900 --> 00:03:51,880
این دقیقاً در اینجا قانون زنجیره است، که در آن ضرب این سه

53
00:03:51,880 --> 00:03:57,340
نسبت به ما حساسیت c را به تغییرات کوچک در wL می دهد.

54
00:03:57,340 --> 00:04:01,620
بنابراین در حال حاضر روی صفحه، نمادهای زیادی وجود دارد، و کمی وقت بگذارید تا

55
00:04:01,620 --> 00:04:07,460
مطمئن شوید که همه آنها چیستند، زیرا اکنون ما مشتقات مربوطه را محاسبه می کنیم.

56
00:04:07,460 --> 00:04:14,220
مشتق c نسبت به AL 2AL-y است.

57
00:04:14,220 --> 00:04:19,300
این بدان معناست که اندازه آن متناسب با تفاوت بین خروجی شبکه و

58
00:04:19,300 --> 00:04:24,480
چیزی است که می‌خواهیم باشد، بنابراین اگر آن خروجی بسیار متفاوت بود،

59
00:04:24,480 --> 00:04:28,380
حتی تغییرات جزئی نیز تأثیر زیادی بر تابع هزینه نهایی خواهد داشت.

60
00:04:28,380 --> 00:04:33,860
مشتق AL با توجه به zL فقط مشتق تابع سیگموئید

61
00:04:33,860 --> 00:04:37,420
ما یا هر غیرخطی دیگری است که انتخاب می کنید.

62
00:04:37,420 --> 00:04:46,180
مشتق zL نسبت به wL AL-1 است.

63
00:04:46,180 --> 00:04:49,460
من شما را نمی‌دانم، اما فکر می‌کنم به راحتی می‌توان در فرمول‌ها گیر کرد بدون

64
00:04:49,460 --> 00:04:54,180
اینکه لحظه‌ای بنشینید و به خود یادآوری کنید که همه آنها چه معنایی دارند.

65
00:04:54,180 --> 00:04:58,860
در مورد این آخرین مشتق، مقداری که ضربه کوچک به وزن بر

66
00:04:58,860 --> 00:05:03,220
آخرین لایه تأثیر می‌گذارد به میزان قوی بودن نورون قبلی بستگی دارد.

67
00:05:03,220 --> 00:05:09,320
به یاد داشته باشید، اینجاست که ایده نورون‌ها-که-با هم-سیم-با هم آتش می‌زنند- به وجود می‌آید.

68
00:05:09,320 --> 00:05:14,840
و همه اینها مشتق با توجه به wL

69
00:05:14,840 --> 00:05:16,580
فقط هزینه یک نمونه آموزشی خاص است.

70
00:05:16,580 --> 00:05:20,940
از آنجایی که تابع هزینه کامل شامل میانگین گیری همه آن هزینه

71
00:05:20,940 --> 00:05:27,300
ها در بسیاری از مثال های آموزشی مختلف است، مشتق آن

72
00:05:27,300 --> 00:05:28,540
مستلزم میانگین گیری این عبارت در تمام نمونه های آموزشی است.

73
00:05:28,540 --> 00:05:33,860
البته، این فقط یک جزء از بردار گرادیان است که از مشتقات جزئی تابع

74
00:05:33,860 --> 00:05:40,780
هزینه با توجه به تمام آن وزن ها و سوگیری ها ساخته شده است.

75
00:05:40,780 --> 00:05:44,340
اما اگرچه این تنها یکی از مشتقات جزئی بسیاری است که ما به

76
00:05:44,340 --> 00:05:46,460
آن نیاز داریم، بیش از 50 درصد کار را تشکیل می دهد.

77
00:05:46,460 --> 00:05:50,300
برای مثال، حساسیت به سوگیری تقریباً یکسان است.

78
00:05:50,300 --> 00:05:58,980
ما فقط باید این عبارت del z del w را برای a del z del b تغییر دهیم.

79
00:05:58,980 --> 00:06:04,700
و اگر به فرمول مربوطه نگاه کنید، آن مشتق 1 می شود.

80
00:06:04,700 --> 00:06:11,700
همچنین، و اینجاست که ایده انتشار به عقب مطرح می شود، می توانید ببینید

81
00:06:11,700 --> 00:06:16,180
که این تابع هزینه چقدر نسبت به فعال سازی لایه قبلی حساس است.

82
00:06:16,180 --> 00:06:21,380
یعنی، این مشتق اولیه در بیان قانون زنجیره ای،

83
00:06:21,380 --> 00:06:25,420
حساسیت z به فعال سازی قبلی، وزن wL است.

84
00:06:25,420 --> 00:06:30,100
و دوباره، حتی اگر نمی‌توانیم مستقیماً بر فعال‌سازی لایه قبلی تأثیر

85
00:06:30,100 --> 00:06:35,280
بگذاریم، پیگیری آن مفید است، زیرا اکنون می‌توانیم همین ایده

86
00:06:35,280 --> 00:06:40,780
قانون زنجیره‌ای را به عقب تکرار کنیم تا ببینیم تابع هزینه

87
00:06:40,780 --> 00:06:43,680
چقدر حساس است. وزن های قبلی و سوگیری های قبلی

88
00:06:43,680 --> 00:06:47,940
و ممکن است فکر کنید که این یک مثال بسیار ساده است، زیرا همه لایه‌ها

89
00:06:47,940 --> 00:06:51,320
یک نورون دارند و همه چیز برای یک شبکه واقعی به طور تصاعدی پیچیده‌تر می‌شود.

90
00:06:51,320 --> 00:06:56,560
اما صادقانه بگویم، زمانی که به لایه‌ها نورون‌های متعددی می‌دهیم، تغییرات چندانی ایجاد نمی‌شود،

91
00:06:56,560 --> 00:06:59,320
در واقع این فقط چند شاخص دیگر است که باید آنها را پیگیری کرد.

92
00:06:59,320 --> 00:07:03,580
به جای فعال شدن یک لایه مشخص به سادگی AL، یک زیرنویس نیز

93
00:07:03,580 --> 00:07:07,920
خواهد داشت که نشان می دهد کدام نورون از آن لایه است.

94
00:07:07,920 --> 00:07:15,280
از حرف k برای نمایه سازی لایه L-1 و j برای نمایه سازی لایه L استفاده می کنیم.

95
00:07:15,280 --> 00:07:20,720
برای هزینه، دوباره بررسی می کنیم که خروجی مورد نظر چیست، اما این بار مربع

96
00:07:20,720 --> 00:07:26,120
تفاوت بین این آخرین فعال سازی لایه و خروجی مورد نظر را جمع می کنیم.

97
00:07:26,120 --> 00:07:33,280
یعنی مجموع را بر ALj منهای yj مجذور می گیرید.

98
00:07:33,280 --> 00:07:36,500
از آنجایی که وزن‌های بسیار بیشتری وجود دارد، هر کدام باید چند شاخص

99
00:07:36,500 --> 00:07:41,380
بیشتر داشته باشند تا موقعیت آن را پیگیری کنند، بنابراین بیایید وزن لبه‌ای

100
00:07:41,380 --> 00:07:45,740
را که این نورون k‌ام را به نورون j‌ام متصل می‌کند، WLjk بنامیم.

101
00:07:45,740 --> 00:07:49,820
این شاخص‌ها ممکن است در ابتدا کمی عقب‌تر به نظر برسند، اما با نحوه

102
00:07:49,820 --> 00:07:53,800
نمایه‌سازی ماتریس وزنی که در ویدیوی قسمت 1 درباره آن صحبت کردم، مطابقت دارد.

103
00:07:53,800 --> 00:07:57,660
درست مانند قبل، هنوز هم خوب است که برای مجموع وزنی مربوطه، مانند

104
00:07:57,660 --> 00:08:03,540
z، یک نام بگذارید، به طوری که فعال شدن آخرین لایه فقط

105
00:08:03,540 --> 00:08:04,980
تابع خاص شما باشد، مانند سیگموید، که برای z اعمال می شود.

106
00:08:04,980 --> 00:08:09,100
می توانید منظور من را ببینید، جایی که همه اینها اساساً همان معادلاتی هستند که قبلاً

107
00:08:09,100 --> 00:08:15,420
در مورد یک نورون در هر لایه داشتیم، فقط کمی پیچیده تر به نظر می رسد.

108
00:08:15,420 --> 00:08:20,620
و در واقع، عبارت مشتق قانون زنجیره ای که توضیح می دهد چقدر

109
00:08:20,620 --> 00:08:23,540
هزینه به یک وزن خاص حساس است، اساساً یکسان به نظر می رسد.

110
00:08:23,540 --> 00:08:29,420
من این را به شما واگذار می کنم که در صورت تمایل در مورد هر یک از این اصطلاحات فکر کنید.

111
00:08:29,420 --> 00:08:34,900
با این حال، آنچه در اینجا تغییر می‌کند، مشتق هزینه

112
00:08:34,900 --> 00:08:37,820
با توجه به یکی از فعال‌سازی‌های لایه L-1 است.

113
00:08:37,820 --> 00:08:42,000
در این مورد، تفاوت این است که نورون از طریق

114
00:08:42,000 --> 00:08:43,540
چندین مسیر مختلف بر تابع هزینه تأثیر می گذارد.

115
00:08:43,540 --> 00:08:51,200
یعنی از یک طرف روی AL0 که در تابع هزینه ایفای نقش می

116
00:08:51,200 --> 00:08:56,460
کند تأثیر می گذارد، اما روی AL1 نیز تأثیر دارد که در

117
00:08:56,460 --> 00:09:00,340
تابع هزینه نیز نقش دارد و باید آن ها را جمع کنید.

118
00:09:00,340 --> 00:09:03,680
و این، خوب، تقریباً همین است.

119
00:09:03,680 --> 00:09:08,240
هنگامی که متوجه شدید که تابع هزینه نسبت به فعال‌سازی‌های این لایه

120
00:09:08,240 --> 00:09:12,520
دوم تا آخر چقدر حساس است، می‌توانید این فرآیند را برای همه

121
00:09:12,520 --> 00:09:13,920
وزن‌ها و سوگیری‌هایی که به آن لایه وارد می‌شوند تکرار کنید.

122
00:09:13,920 --> 00:09:15,420
پس به پشت خود دست بزن!

123
00:09:15,420 --> 00:09:20,480
اگر همه اینها منطقی باشد، اکنون عمیقاً به قلب انتشار پس زمینه، یعنی

124
00:09:20,480 --> 00:09:23,700
نیروی کار در پس چگونگی یادگیری شبکه های عصبی نگاه کرده اید.

125
00:09:23,700 --> 00:09:27,960
این عبارات قانون زنجیره ای مشتقاتی را به شما می دهد که هر جزء را در گرادیان تعیین

126
00:09:27,960 --> 00:09:35,020
می کند که به حداقل رساندن هزینه شبکه با گام برداشتن مکرر در سراشیبی کمک می کند.

127
00:09:35,020 --> 00:09:38,960
اگر عقب بنشینید و به همه اینها فکر کنید، این لایه‌های پیچیدگی زیادی است که ذهن شما

128
00:09:38,960 --> 00:09:42,840
را به دور خود می‌پیچد، بنابراین نگران نباشید اگر هضم همه آن‌ها به زمان نیاز دارد.

