[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm. ",
  "translatedText": "فرض سخت در اینجا این است که شما قسمت 3 را تماشا کرده‌اید، که یک راهنما بصری از الگوریتم انتشار پس‌انداز است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus. ",
  "translatedText": "در اینجا کمی رسمی تر می شویم و به محاسبات مربوطه می پردازیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else. ",
  "translatedText": "طبیعی است که این حداقل کمی گیج کننده باشد، بنابراین مانترا برای مکث و اندیشیدن منظم مطمئناً در اینجا نیز مانند هر جای دیگر اعمال می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject. ",
  "translatedText": "هدف اصلی ما این است که نشان دهیم مردم در یادگیری ماشین معمولاً در مورد قانون زنجیره ای از حساب دیفرانسیل و انتگرال در زمینه شبکه ها فکر می کنند، که احساسی متفاوت از نحوه برخورد بیشتر دوره های حساب دیفرانسیل و انتگرال مقدماتی به موضوع دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic. ",
  "translatedText": "برای کسانی از شما که از محاسبه مربوطه ناراحت هستید، من یک سری کامل در این زمینه دارم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it. ",
  "translatedText": "بیایید با یک شبکه بسیار ساده شروع کنیم، شبکه ای که در آن هر لایه یک نورون دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables. ",
  "translatedText": "این شبکه با سه وزن و سه سوگیری تعیین می‌شود و هدف ما این است که بفهمیم تابع هزینه تا چه حد به این متغیرها حساس است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way we know which adjustments to those terms will cause the most efficient decrease to the cost function. ",
  "translatedText": "به این ترتیب ما می دانیم که کدام تعدیل در آن شرایط باعث کاهش کارآمدترین تابع هزینه می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "We'll just focus on the connection between the last two neurons. ",
  "translatedText": "ما فقط بر ارتباط بین دو نورون آخر تمرکز خواهیم کرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in. ",
  "translatedText": "بیایید فعال شدن آخرین نورون را با علامت L برچسب گذاری کنیم، که نشان می دهد در کدام لایه قرار دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.98,
  "end": 71.36
 },
 {
  "input": "So the activation of the previous neuron is AL-1. ",
  "translatedText": "بنابراین فعال شدن نورون قبلی AL-1 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.68,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on. ",
  "translatedText": "اینها نما نیستند، آنها فقط راهی برای نمایه سازی چیزی هستند که در مورد آن صحبت می کنیم، زیرا می خواهم در آینده زیرنویس ها را برای شاخص های مختلف ذخیره کنم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1. ",
  "translatedText": "فرض کنید مقداری که می‌خواهیم این آخرین فعال‌سازی برای مثال آموزشی داده شده باشد y است، برای مثال، y ممکن است 0 یا 1 باشد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is AL-y2. ",
  "translatedText": "بنابراین هزینه این شبکه برای یک نمونه آموزشی AL-y2 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0. ",
  "translatedText": "ما هزینه آن یک مثال آموزشی را به عنوان c0 نشان خواهیم داد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call wL, times the previous neuron's activation plus some bias, which I'll call bL. ",
  "translatedText": "برای یادآوری، این آخرین فعال‌سازی با وزنی تعیین می‌شود که من آن را wL می‌نامم، ضربدر فعال‌سازی نورون قبلی به اضافه مقداری سوگیری، که من آن را bL می‌نامم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "Then you pump that through some special nonlinear function like the sigmoid or ReLU. ",
  "translatedText": "سپس آن را از طریق یک تابع غیرخطی خاص مانند سیگموئید یا ReLU پمپ می کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations. ",
  "translatedText": "در واقع کار را برای ما آسان‌تر می‌کند اگر به این مجموع وزنی، مانند z، با همان بالانویس فعال‌سازی‌های مربوطه، یک نام خاص بدهیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action, and bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost. ",
  "translatedText": "این اصطلاحات زیادی است، و راهی که می‌توانید آن را مفهوم‌سازی کنید این است که وزن، عملکرد قبلی و بایاس با هم برای محاسبه z استفاده می‌شوند، که به نوبه خود به ما اجازه می‌دهد a را محاسبه کنیم، که در نهایت، همراه با یک ثابت y، اجازه می‌دهد ما هزینه را محاسبه می کنیم و البته، AL-1 تحت تأثیر وزن و تعصب و مواردی از این قبیل است، اما ما در حال حاضر روی آن تمرکز نداریم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course, AL-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line. ",
  "translatedText": "همه اینها فقط اعداد هستند، درست است؟ و این می تواند خوب باشد که هر یک را به عنوان یک خط اعداد کوچک خود در نظر بگیریم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight wL. ",
  "translatedText": "اولین هدف ما این است که بفهمیم تابع هزینه چقدر نسبت به تغییرات کوچک در وزن وزن ما حساس است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to wL? ",
  "translatedText": "یا عبارت دیگر، مشتق c نسبت به wL چیست؟ وقتی این عبارت del w را می بینید، آن را به معنای یک حرکت کوچک به w، مانند تغییر 0 در نظر بگیرید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del w term, think of it as meaning some tiny nudge to w, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is. ",
  "translatedText": "01، و این اصطلاح del c را به این معنا در نظر بگیرید که هر گونه فشار حاصله به هزینه است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio. ",
  "translatedText": "آنچه ما می خواهیم نسبت آنهاست. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to wL causes some nudge to zL, which in turn causes some nudge to AL, which directly influences the cost. ",
  "translatedText": "از نظر مفهومی، این حرکت کوچک به wL باعث ایجاد مقداری ضربه به zL می شود، که به نوبه خود باعث ایجاد مقداری ضربه به AL می شود که مستقیماً بر هزینه تأثیر می گذارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to zL to this tiny change w, that is, the derivative of zL with respect to wL. ",
  "translatedText": "بنابراین، ابتدا با نگاه کردن به نسبت یک تغییر کوچک به zL به این تغییر کوچک w، یعنی مشتق zL نسبت به wL، چیزها را تجزیه می کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in zL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL. ",
  "translatedText": "به همین ترتیب، سپس نسبت تغییر به AL را به تغییر کوچک zL که باعث آن شده است، و همچنین نسبت بین ضربه نهایی به c و این حرکت میانی به AL را در نظر بگیرید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying these three ratios gives us the sensitivity of c to small changes in wL. ",
  "translatedText": "این دقیقاً در اینجا قانون زنجیره است، که در آن ضرب این سه نسبت به ما حساسیت c را به تغییرات کوچک در wL می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives. ",
  "translatedText": "بنابراین در حال حاضر روی صفحه، نمادهای زیادی وجود دارد، و کمی وقت بگذارید تا مطمئن شوید که همه آنها چیستند، زیرا اکنون ما مشتقات مربوطه را محاسبه می کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y. ",
  "translatedText": "مشتق c نسبت به AL 2AL-y است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "This means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function. ",
  "translatedText": "این بدان معناست که اندازه آن متناسب با تفاوت بین خروجی شبکه و چیزی است که می‌خواهیم باشد، بنابراین اگر آن خروجی بسیار متفاوت بود، حتی تغییرات جزئی نیز تأثیر زیادی بر تابع هزینه نهایی خواهد داشت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to zL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use. ",
  "translatedText": "مشتق AL با توجه به zL فقط مشتق تابع سیگموئید ما یا هر غیرخطی دیگری است که انتخاب می کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "The derivative of zL with respect to wL comes out to be AL-1. ",
  "translatedText": "مشتق zL نسبت به wL AL-1 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself what they all mean. ",
  "translatedText": "من شما را نمی‌دانم، اما فکر می‌کنم به راحتی می‌توان در فرمول‌ها گیر کرد بدون اینکه لحظه‌ای بنشینید و به خود یادآوری کنید که همه آنها چه معنایی دارند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is. ",
  "translatedText": "در مورد این آخرین مشتق، مقداری که ضربه کوچک به وزن بر آخرین لایه تأثیر می‌گذارد به میزان قوی بودن نورون قبلی بستگی دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in. ",
  "translatedText": "به یاد داشته باشید، اینجاست که ایده نورون‌ها-که-با هم-سیم-با هم آتش می‌زنند- به وجود می‌آید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to wL only of the cost for a specific single training example. ",
  "translatedText": "و همه اینها مشتق با توجه به wL فقط هزینه یک نمونه آموزشی خاص است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples. ",
  "translatedText": "از آنجایی که تابع هزینه کامل شامل میانگین گیری همه آن هزینه ها در بسیاری از مثال های آموزشی مختلف است، مشتق آن مستلزم میانگین گیری این عبارت در تمام نمونه های آموزشی است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "Of course, that's just one component of the gradient vector, which is built up from the partial derivatives of the cost function with respect to all those weights and biases. ",
  "translatedText": "البته، این فقط یک جزء از بردار گرادیان است که از مشتقات جزئی تابع هزینه با توجه به تمام آن وزن ها و سوگیری ها ساخته شده است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work. ",
  "translatedText": "اما اگرچه این تنها یکی از مشتقات جزئی بسیاری است که ما به آن نیاز داریم، بیش از 50 درصد کار را تشکیل می دهد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical. ",
  "translatedText": "برای مثال، حساسیت به سوگیری تقریباً یکسان است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b. ",
  "translatedText": "ما فقط باید این عبارت del z del w را برای a del z del b تغییر دهیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1. ",
  "translatedText": "و اگر به فرمول مربوطه نگاه کنید، آن مشتق 1 می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer. ",
  "translatedText": "همچنین، و اینجاست که ایده انتشار به عقب مطرح می شود، می توانید ببینید که این تابع هزینه چقدر نسبت به فعال سازی لایه قبلی حساس است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight wL. ",
  "translatedText": "یعنی، این مشتق اولیه در بیان قانون زنجیره ای، حساسیت z به فعال سازی قبلی، وزن wL است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases. ",
  "translatedText": "و دوباره، حتی اگر نمی‌توانیم مستقیماً بر فعال‌سازی لایه قبلی تأثیر بگذاریم، پیگیری آن مفید است، زیرا اکنون می‌توانیم همین ایده قانون زنجیره‌ای را به عقب تکرار کنیم تا ببینیم تابع هزینه چقدر حساس است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network. ",
  "translatedText": "وزن های قبلی و سوگیری های قبلی و ممکن است فکر کنید که این یک مثال بسیار ساده است، زیرا همه لایه‌ها یک نورون دارند و همه چیز برای یک شبکه واقعی به طور تصاعدی پیچیده‌تر می‌شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of. ",
  "translatedText": "اما صادقانه بگویم، زمانی که به لایه‌ها نورون‌های متعددی می‌دهیم، تغییرات چندانی ایجاد نمی‌شود، در واقع این فقط چند شاخص دیگر است که باید آنها را پیگیری کرد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is. ",
  "translatedText": "به جای فعال شدن یک لایه مشخص به سادگی AL، یک زیرنویس نیز خواهد داشت که نشان می دهد کدام نورون از آن لایه است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L. ",
  "translatedText": "از حرف k برای نمایه سازی لایه L-1 و j برای نمایه سازی لایه L استفاده می کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output. ",
  "translatedText": "برای هزینه، دوباره بررسی می کنیم که خروجی مورد نظر چیست، اما این بار مربع تفاوت بین این آخرین فعال سازی لایه و خروجی مورد نظر را جمع می کنیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus yj squared. ",
  "translatedText": "یعنی مجموع را بر ALj منهای yj مجذور می گیرید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk. ",
  "translatedText": "از آنجایی که وزن‌های بسیار بیشتری وجود دارد، هر کدام باید چند شاخص بیشتر داشته باشند تا موقعیت آن را پیگیری کنند، بنابراین بیایید وزن لبه‌ای را که این نورون k‌ام را به نورون j‌ام متصل می‌کند، WLjk بنامیم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video. ",
  "translatedText": "این شاخص‌ها ممکن است در ابتدا کمی عقب‌تر به نظر برسند، اما با نحوه نمایه‌سازی ماتریس وزنی که در ویدیوی قسمت 1 درباره آن صحبت کردم، مطابقت دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z. ",
  "translatedText": "درست مانند قبل، هنوز هم خوب است که برای مجموع وزنی مربوطه، مانند z، یک نام بگذارید، به طوری که فعال شدن آخرین لایه فقط تابع خاص شما باشد، مانند سیگموید، که برای z اعمال می شود. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated. ",
  "translatedText": "می توانید منظور من را ببینید، جایی که همه اینها اساساً همان معادلاتی هستند که قبلاً در مورد یک نورون در هر لایه داشتیم، فقط کمی پیچیده تر به نظر می رسد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain rule derivative expression describing how sensitive the cost is to a specific weight looks essentially the same. ",
  "translatedText": "و در واقع، عبارت مشتق قانون زنجیره ای که توضیح می دهد چقدر هزینه به یک وزن خاص حساس است، اساساً یکسان به نظر می رسد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want. ",
  "translatedText": "من این را به شما واگذار می کنم که در صورت تمایل در مورد هر یک از این اصطلاحات فکر کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1. ",
  "translatedText": "با این حال، آنچه در اینجا تغییر می‌کند، مشتق هزینه با توجه به یکی از فعال‌سازی‌های لایه L-1 است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths. ",
  "translatedText": "در این مورد، تفاوت این است که نورون از طریق چندین مسیر مختلف بر تابع هزینه تأثیر می گذارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up. ",
  "translatedText": "یعنی از یک طرف روی AL0 که در تابع هزینه ایفای نقش می کند تأثیر می گذارد، اما روی AL1 نیز تأثیر دارد که در تابع هزینه نیز نقش دارد و باید آن ها را جمع کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it. ",
  "translatedText": "و این، خوب، تقریباً همین است. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer. ",
  "translatedText": "هنگامی که متوجه شدید که تابع هزینه نسبت به فعال‌سازی‌های این لایه دوم تا آخر چقدر حساس است، می‌توانید این فرآیند را برای همه وزن‌ها و سوگیری‌هایی که به آن لایه وارد می‌شوند تکرار کنید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back! ",
  "translatedText": "پس به پشت خود دست بزن! ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn. ",
  "translatedText": "اگر همه اینها منطقی باشد، اکنون عمیقاً به قلب انتشار پس زمینه، یعنی نیروی کار در پس چگونگی یادگیری شبکه های عصبی نگاه کرده اید. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill. ",
  "translatedText": "این عبارات قانون زنجیره ای مشتقاتی را به شما می دهد که هر جزء را در گرادیان تعیین می کند که به حداقل رساندن هزینه شبکه با گام برداشتن مکرر در سراشیبی کمک می کند. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all. ",
  "translatedText": "اگر عقب بنشینید و به همه اینها فکر کنید، این لایه‌های پیچیدگی زیادی است که ذهن شما را به دور خود می‌پیچد، بنابراین نگران نباشید اگر هضم همه آن‌ها به زمان نیاز دارد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]