[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "Wir gehen davon aus, dass du Teil 3 gesehen hast, in dem du den Backpropagation-Algorithmus intuitiv nachvollziehen konntest.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Hier werden wir ein wenig formaler und tauchen in die relevante Kalkulation ein.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "Es ist normal, dass das zumindest ein bisschen verwirrend ist, also gilt das Mantra, regelmäßig innezuhalten und nachzudenken, hier genauso wie anderswo.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "Unser Hauptziel ist es zu zeigen, wie Menschen im Bereich des maschinellen Lernens über die Kettenregel aus der Infinitesimalrechnung im Kontext von Netzwerken denken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Für diejenigen unter euch, die sich mit den entsprechenden Berechnungen nicht auskennen, habe ich eine ganze Serie zu diesem Thema.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Beginnen wir mit einem sehr einfachen Netzwerk, in dem jede Schicht ein einzelnes Neuron enthält.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Dieses Netz wird durch drei Gewichte und drei Verzerrungen bestimmt, und unser Ziel ist es zu verstehen, wie empfindlich die Kostenfunktion auf diese Variablen reagiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "Auf diese Weise wissen wir, welche Anpassungen an diesen Begriffen die effizienteste Senkung der Kostenfunktion bewirken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "We'll just focus on the connection between the last two neurons.",
  "translatedText": "Wir konzentrieren uns nur auf die Verbindung zwischen den letzten beiden Neuronen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in.",
  "translatedText": "Beschriften wir die Aktivierung des letzten Neurons mit einem hochgestellten L, das angibt, in welcher Schicht es sich befindet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 65.98,
  "end": 71.36
 },
 {
  "input": "So the activation of the previous neuron is AL-1.",
  "translatedText": "Die Aktivierung des vorherigen Neurons ist also AL-1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 71.68,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Das sind keine Exponenten, sondern nur eine Möglichkeit, das, worüber wir reden, zu indizieren, da ich mir später die Indizes sparen will.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Nehmen wir an, der Wert, den wir für diese letzte Aktivierung für ein bestimmtes Trainingsbeispiel haben wollen, ist y. y kann zum Beispiel 0 oder 1 sein.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is AL-y2.",
  "translatedText": "Die Kosten dieses Netzes für ein einziges Trainingsbeispiel sind also AL-y2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Wir bezeichnen die Kosten für dieses eine Trainingsbeispiel als c0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call wL, times the previous neuron's activation plus some bias, which I'll call bL.",
  "translatedText": "Zur Erinnerung: Diese letzte Aktivierung wird durch eine Gewichtung bestimmt, die ich wL nenne, mal der vorherigen Neuronenaktivierung plus einer Vorspannung, die ich bL nenne.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "Then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "Dann pumpst du das durch eine spezielle nichtlineare Funktion wie das Sigmoid oder ReLU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "Es macht die Sache einfacher, wenn wir dieser gewichteten Summe einen eigenen Namen geben, z. B. z, mit demselben Hochkomma wie die entsprechenden Aktivierungen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action, and bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "Das sind viele Begriffe, und du kannst dir das so vorstellen, dass das Gewicht, die vorherige Aktion und die Vorspannung zusammen verwendet werden, um z zu berechnen, was uns wiederum erlaubt, a zu berechnen, was uns schließlich zusammen mit einer Konstante y die Kosten berechnet.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course, AL-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "Und natürlich wird AL-1 durch sein eigenes Gewicht, seine Vorspannung und so weiter beeinflusst, aber darauf wollen wir uns jetzt nicht konzentrieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Das sind doch alles nur Zahlen, oder?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "Und es kann schön sein, sich vorzustellen, dass jeder seine eigene kleine Zahlenreihe hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight wL.",
  "translatedText": "Unser erstes Ziel ist es, zu verstehen, wie empfindlich die Kostenfunktion auf kleine Änderungen unseres Gewichts wL reagiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to wL?",
  "translatedText": "Oder anders ausgedrückt: Was ist die Ableitung von c nach wL?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del w term, think of it as meaning some tiny nudge to w, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Wenn du den Begriff del w siehst, kannst du dir vorstellen, dass er eine winzige Änderung von w bedeutet, z. B. eine Änderung um 0,01, und der Begriff del c steht für die daraus resultierende Änderung der Kosten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "Was wir wollen, ist ihr Verhältnis.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to wL causes some nudge to zL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Diese winzige Veränderung von wL bewirkt eine Veränderung von zL, die wiederum eine Veränderung von AL bewirkt, was sich direkt auf die Kosten auswirkt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to zL to this tiny change w, that is, the derivative of zL with respect to wL.",
  "translatedText": "Also lösen wir die Sache auf, indem wir zuerst das Verhältnis zwischen einer winzigen Änderung von zL und dieser winzigen Änderung w betrachten, also die Ableitung von zL nach wL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in zL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "Ebenso berücksichtigst du das Verhältnis zwischen der Änderung von AL und der winzigen Änderung von zL, die sie verursacht hat, sowie das Verhältnis zwischen dem endgültigen Anstoß an c und diesem Zwischenanstoß an AL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying these three ratios gives us the sensitivity of c to small changes in wL.",
  "translatedText": "Das hier ist die Kettenregel, bei der die Multiplikation dieser drei Verhältnisse die Empfindlichkeit von c auf kleine Änderungen von wL ergibt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "Auf dem Bildschirm siehst du jetzt eine Menge Symbole. Nimm dir einen Moment Zeit, um sicherzustellen, dass du sie alle verstehst, denn jetzt werden wir die entsprechenden Ableitungen berechnen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "Die Ableitung von c nach AL ergibt sich als 2AL-y.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "This means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Das bedeutet, dass seine Größe proportional zur Differenz zwischen dem Ausgang des Netzes und dem gewünschten Ergebnis ist. Wenn das Ergebnis also sehr unterschiedlich ist, haben selbst kleine Änderungen einen großen Einfluss auf die endgültige Kostenfunktion.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to zL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "Die Ableitung von AL nach zL ist einfach die Ableitung unserer Sigmoidfunktion, oder welche Nichtlinearität du auch immer verwenden willst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "The derivative of zL with respect to wL comes out to be AL-1.",
  "translatedText": "Die Ableitung von zL nach wL ist dann AL-1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself what they all mean.",
  "translatedText": "Ich weiß nicht, wie es dir geht, aber ich glaube, es ist leicht, mit dem Kopf in den Formeln zu stecken, ohne sich einen Moment zurückzulehnen und sich daran zu erinnern, was sie alle bedeuten.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "Im Fall dieser letzten Ableitung hängt der Einfluss der kleinen Änderung des Gewichts auf die letzte Schicht davon ab, wie stark das vorherige Neuron ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "Denke daran, dass hier die Idee der Neuronen, die zusammen feuern und zusammen verdrahtet sind, zum Tragen kommt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to wL only of the cost for a specific single training example.",
  "translatedText": "Und all dies ist die Ableitung der Kosten für ein bestimmtes einzelnes Trainingsbeispiel in Bezug auf wL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Da bei der vollständigen Kostenfunktion alle Kosten über viele verschiedene Trainingsbeispiele hinweg gemittelt werden müssen, muss die Ableitung dieses Ausdrucks über alle Trainingsbeispiele gemittelt werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "Of course, that's just one component of the gradient vector, which is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "Das ist natürlich nur eine Komponente des Gradientenvektors, der sich aus den partiellen Ableitungen der Kostenfunktion in Bezug auf all diese Gewichte und Verzerrungen zusammensetzt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "Aber auch wenn das nur eine der vielen Teilableitungen ist, die wir brauchen, macht das mehr als 50% der Arbeit aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "Die Empfindlichkeit gegenüber der Verzerrung ist zum Beispiel fast identisch.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Wir müssen nur den Begriff del z del w gegen einen Begriff del z del b austauschen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "Und wenn du dir die entsprechende Formel ansiehst, ergibt die Ableitung 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "Außerdem - und hier kommt die Idee der Rückwärtspropagierung ins Spiel - kannst du sehen, wie empfindlich diese Kostenfunktion auf die Aktivierung der vorherigen Schicht reagiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight wL.",
  "translatedText": "Die anfängliche Ableitung in der Kettenregel, also die Empfindlichkeit von z gegenüber der vorherigen Aktivierung, ist das Gewicht wL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "Auch wenn wir nicht in der Lage sind, die Aktivierung der vorherigen Schicht direkt zu beeinflussen, ist es hilfreich, sie im Auge zu behalten, denn jetzt können wir dieselbe Kettenregel rückwärts iterieren, um zu sehen, wie empfindlich die Kostenfunktion auf die vorherigen Gewichte und die vorherigen Voreinstellungen reagiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "Du denkst vielleicht, dass dies ein zu einfaches Beispiel ist, da alle Schichten ein Neuron haben, aber in einem echten Netzwerk werden die Dinge exponentiell komplizierter.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "Aber ehrlich gesagt ändert sich gar nicht so viel, wenn wir den Schichten mehrere Neuronen geben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "Die Aktivierung einer bestimmten Schicht ist nicht einfach AL, sondern hat auch einen Index, der angibt, um welches Neuron der Schicht es sich handelt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Wir verwenden den Buchstaben k, um die Schicht L-1 zu kennzeichnen, und j, um die Schicht L zu kennzeichnen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "Für die Kosten schauen wir uns wieder an, wie hoch der gewünschte Output ist, aber dieses Mal addieren wir die Quadrate der Differenzen zwischen diesen letzten Schichtaktivierungen und dem gewünschten Output.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus yj squared.",
  "translatedText": "Das heißt, du nimmst eine Summe über ALj minus yj zum Quadrat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Da es viel mehr Gewichte gibt, muss jedes einzelne ein paar mehr Indizes haben, um zu wissen, wo es ist. Nennen wir also das Gewicht der Kante, die das k-te Neuron mit dem j-ten Neuron verbindet, WLjk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "Diese Indizes mögen sich zunächst ein wenig verkehrt anfühlen, aber sie stimmen mit den Indizes der Gewichtsmatrix überein, über die ich in Teil 1 des Videos gesprochen habe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "Es ist nach wie vor sinnvoll, der entsprechenden gewichteten Summe einen Namen zu geben, z. B. z, so dass die Aktivierung der letzten Schicht einfach deine spezielle Funktion ist, z. B. das Sigmoid, das auf z angewendet wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Du siehst, was ich meine: Das sind im Wesentlichen dieselben Gleichungen, die wir schon für den Fall eines Neurons pro Schicht hatten, nur dass es etwas komplizierter aussieht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain rule derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "Und tatsächlich sieht der abgeleitete Ausdruck der Kettenregel, der beschreibt, wie empfindlich die Kosten auf ein bestimmtes Gewicht reagieren, im Wesentlichen gleich aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Ich überlasse es dir, innezuhalten und über jeden dieser Begriffe nachzudenken, wenn du willst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "Was sich hier jedoch ändert, ist die Ableitung der Kosten in Bezug auf eine der Aktivierungen in der Schicht L-1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "In diesem Fall besteht der Unterschied darin, dass das Neuron die Kostenfunktion über mehrere verschiedene Wege beeinflusst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "Das heißt, sie beeinflusst einerseits AL0, das eine Rolle in der Kostenfunktion spielt, aber sie hat auch einen Einfluss auf AL1, das ebenfalls eine Rolle in der Kostenfunktion spielt, und diese beiden Faktoren musst du addieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "Und das, na ja, das war's auch schon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Sobald du weißt, wie empfindlich die Kostenfunktion auf die Aktivierungen in dieser vorletzten Schicht reagiert, kannst du den Prozess für alle Gewichte und Vorspannungen wiederholen, die in diese Schicht einfließen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "Also klopf dir selbst auf die Schulter!",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Wenn dir das alles einleuchtet, hast du jetzt einen tiefen Einblick in das Herz der Backpropagation bekommen, dem Arbeitspferd, mit dem neuronale Netze lernen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Mit diesen Kettenregelausdrücken erhältst du die Ableitungen, die jede Komponente der Steigung bestimmen, die dazu beiträgt, die Kosten des Netzes zu minimieren, indem es immer wieder bergab geht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Wenn du dich zurücklehnst und über all das nachdenkst, ist das eine ganze Menge an Komplexität, die du in deinem Kopf verarbeiten musst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]