[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "Die harte Annahme hier ist, dass Sie Teil 3 gesehen haben, der eine intuitive Anleitung zum Backpropagation-Algorithmus gibt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Hier werden wir etwas formeller und tauchen in die relevante Infinitesimalrechnung ein.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "Es ist normal, dass dies zumindest ein wenig verwirrend ist, daher gilt das Mantra, regelmäßig innezuhalten und nachzudenken, hier genauso wie anderswo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "Unser Hauptziel besteht darin, zu zeigen, wie Menschen, die sich mit maschinellem Lernen befassen, üblicherweise über die Kettenregel aus der Analysis im Kontext von Netzwerken denken, was ein anderes Gefühl vermittelt als die Herangehensweise der meisten Einführungskurse in Analysis an das Thema.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Für diejenigen unter Ihnen, die sich mit der relevanten Infinitesimalrechnung nicht auskennen, habe ich eine ganze Reihe zu diesem Thema.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Beginnen wir mit einem äußerst einfachen Netzwerk, bei dem jede Schicht ein einzelnes Neuron enthält.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Dieses Netzwerk wird durch drei Gewichte und drei Verzerrungen bestimmt. Unser Ziel ist es zu verstehen, wie empfindlich die Kostenfunktion auf diese Variablen reagiert.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "Auf diese Weise wissen wir, welche Anpassungen dieser Bedingungen die effizienteste Verringerung der Kostenfunktion bewirken.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "We'll just focus on the connection between the last two neurons.",
  "translatedText": "Wir konzentrieren uns nur auf die Verbindung zwischen den letzten beiden Neuronen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in.",
  "translatedText": "Beschriften wir die Aktivierung dieses letzten Neurons mit einem hochgestellten L, das angibt, in welcher Schicht es sich befindet.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.98,
  "end": 71.36
 },
 {
  "input": "So the activation of the previous neuron is AL-1.",
  "translatedText": "Die Aktivierung des vorherigen Neurons ist also AL-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.68,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Dabei handelt es sich nicht um Exponenten, sondern nur um eine Möglichkeit, das, worüber wir sprechen, zu indizieren, da ich später Indizes für verschiedene Indizes speichern möchte.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Nehmen wir an, dass der Wert, den diese letzte Aktivierung für ein bestimmtes Trainingsbeispiel haben soll, y ist. Beispielsweise könnte y 0 oder 1 sein.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is AL-y2.",
  "translatedText": "Die Kosten dieses Netzwerks für ein einzelnes Trainingsbeispiel betragen also AL-y2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Wir bezeichnen die Kosten dieses einen Trainingsbeispiels als c0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call wL, times the previous neuron's activation plus some bias, which I'll call bL.",
  "translatedText": "Zur Erinnerung: Diese letzte Aktivierung wird durch ein Gewicht bestimmt, das ich wL nenne, multipliziert mit der Aktivierung des vorherigen Neurons plus einer gewissen Abweichung, die ich bL nenne.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "Then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "Dann pumpen Sie das durch eine spezielle nichtlineare Funktion wie Sigmoid oder ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "Es wird uns tatsächlich die Arbeit erleichtern, wenn wir dieser gewichteten Summe einen speziellen Namen geben, z. B. z, mit demselben hochgestellten Index wie die relevanten Aktivierungen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action, and bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "Das sind viele Begriffe, und Sie könnten sich das so vorstellen, dass das Gewicht, die vorherige Aktion und der Bias zusammen verwendet werden, um z zu berechnen, was uns wiederum die Berechnung von a ermöglicht, was uns schließlich zusammen mit einer Konstante y ermöglicht Wir berechnen die Kosten.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course, AL-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "Und natürlich wird AL-1 durch sein eigenes Gewicht, seine Voreingenommenheit usw. beeinflusst, aber darauf werden wir uns jetzt nicht konzentrieren.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Das sind doch alles nur Zahlen, oder?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "Und es kann schön sein, sich vorzustellen, dass jede einzelne ihre eigene kleine Zahlenreihe hat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight wL.",
  "translatedText": "Unser erstes Ziel besteht darin zu verstehen, wie empfindlich die Kostenfunktion auf kleine Änderungen unseres Gewichts wL reagiert.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to wL?",
  "translatedText": "Oder anders ausgedrückt: Was ist die Ableitung von c nach wL?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del w term, think of it as meaning some tiny nudge to w, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Wenn Sie diesen del w-Begriff sehen, stellen Sie sich vor, dass er einen kleinen Anstoß an w bedeutet, etwa eine Änderung um 0.01, und stellen Sie sich diesen del c-Begriff so vor, dass er bedeutet, was auch immer der daraus resultierende Kostenschub ist.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "Was wir wollen, ist ihr Verhältnis.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to wL causes some nudge to zL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Konzeptionell führt dieser kleine Schub für wL zu einem gewissen Schub für zL, was wiederum einen gewissen Schub für AL verursacht, was sich direkt auf die Kosten auswirkt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to zL to this tiny change w, that is, the derivative of zL with respect to wL.",
  "translatedText": "Also unterteilen wir die Sache, indem wir zunächst das Verhältnis einer winzigen Änderung von zL zu dieser winzigen Änderung w betrachten, also die Ableitung von zL nach wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in zL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "Ebenso berücksichtigen Sie dann das Verhältnis der Änderung von AL zu der winzigen Änderung von zL, die sie verursacht hat, sowie das Verhältnis zwischen dem endgültigen Anstoß an c und diesem Zwischenanstoß an AL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying these three ratios gives us the sensitivity of c to small changes in wL.",
  "translatedText": "Das hier ist die Kettenregel, bei der die Multiplikation dieser drei Verhältnisse die Empfindlichkeit von c gegenüber kleinen Änderungen in wL ergibt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "Auf dem Bildschirm sind also gerade viele Symbole zu sehen, und nehmen Sie sich einen Moment Zeit, um sich zu vergewissern, dass sie alle klar sind, denn jetzt werden wir die relevanten Ableitungen berechnen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "Die Ableitung von c nach AL ergibt 2AL-y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "This means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Das bedeutet, dass seine Größe proportional zur Differenz zwischen der Ausgabe des Netzwerks und dem, was wir wollen, ist. Wenn diese Ausgabe also sehr unterschiedlich ist, können selbst geringfügige Änderungen einen großen Einfluss auf die endgültige Kostenfunktion haben.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to zL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "Die Ableitung von AL nach zL ist einfach die Ableitung unserer Sigmoidfunktion oder der von Ihnen gewählten Nichtlinearität.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "The derivative of zL with respect to wL comes out to be AL-1.",
  "translatedText": "Die Ableitung von zL nach wL ergibt AL-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself what they all mean.",
  "translatedText": "Ich weiß nicht, wie es Ihnen geht, aber ich denke, es ist leicht, mit dem Kopf in den Formeln stecken zu bleiben, ohne sich einen Moment Zeit zu nehmen, sich zurückzulehnen und sich daran zu erinnern, was sie alle bedeuten.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "Im Fall dieser letzten Ableitung hängt der Einfluss des kleinen Gewichtsschubs auf die letzte Schicht davon ab, wie stark das vorherige Neuron ist.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "Denken Sie daran, hier kommt die Idee „Neuronen, die gemeinsam feuern, miteinander verdrahten“ ins Spiel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to wL only of the cost for a specific single training example.",
  "translatedText": "Und all dies ist lediglich die Ableitung der Kosten für ein bestimmtes einzelnes Trainingsbeispiel in Bezug auf wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Da die Vollkostenfunktion die Mittelung aller dieser Kosten über viele verschiedene Trainingsbeispiele hinweg beinhaltet, erfordert ihre Ableitung die Mittelung dieses Ausdrucks über alle Trainingsbeispiele.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "Of course, that's just one component of the gradient vector, which is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "Das ist natürlich nur eine Komponente des Gradientenvektors, der aus den partiellen Ableitungen der Kostenfunktion in Bezug auf all diese Gewichte und Verzerrungen aufgebaut wird.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "Aber auch wenn das nur eine der vielen partiellen Ableitungen ist, die wir brauchen, macht es mehr als 50 % der Arbeit aus.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "Die Empfindlichkeit gegenüber der Voreingenommenheit ist beispielsweise nahezu identisch.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Wir müssen nur diesen del z del w-Term durch a del z del b ersetzen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "Und wenn Sie sich die relevante Formel ansehen, ergibt sich für diese Ableitung 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "Außerdem, und hier kommt die Idee der Rückwärtsausbreitung ins Spiel, können Sie sehen, wie empfindlich diese Kostenfunktion auf die Aktivierung der vorherigen Schicht reagiert.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight wL.",
  "translatedText": "Diese anfängliche Ableitung im Kettenregelausdruck, die Empfindlichkeit von z gegenüber der vorherigen Aktivierung, ergibt sich nämlich als Gewicht wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "Und auch wenn wir die Aktivierung der vorherigen Ebene nicht direkt beeinflussen können, ist es dennoch hilfreich, den Überblick zu behalten, denn jetzt können wir dieselbe Kettenregelidee einfach weiter rückwärts iterieren, um zu sehen, wie empfindlich die Kostenfunktion darauf reagiert frühere Gewichtungen und frühere Vorurteile.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "Und Sie könnten denken, dass dies ein zu einfaches Beispiel ist, da alle Schichten ein Neuron haben und die Dinge für ein echtes Netzwerk exponentiell komplizierter werden.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "Aber ehrlich gesagt ändert sich nicht so viel, wenn wir den Schichten mehrere Neuronen geben, es sind eigentlich nur ein paar weitere Indizes, die man im Auge behalten muss.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "Anstatt dass die Aktivierung einer bestimmten Schicht einfach AL ist, wird sie auch einen Index haben, der angibt, um welches Neuron dieser Schicht es sich handelt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Verwenden wir den Buchstaben k, um die Ebene L-1 zu indizieren, und j, um die Ebene L zu indizieren.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "Für die Kosten schauen wir uns erneut an, wie hoch die gewünschte Ausgabe ist, aber dieses Mal addieren wir die Quadrate der Differenzen zwischen diesen Aktivierungen der letzten Ebene und der gewünschten Ausgabe.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus yj squared.",
  "translatedText": "Das heißt, Sie bilden die Summe über ALj minus yj im Quadrat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Da es viel mehr Gewichte gibt, muss jedes über ein paar weitere Indizes verfügen, um den Überblick zu behalten, wo es sich befindet. Nennen wir also das Gewicht der Kante, die dieses k-te Neuron mit dem j-ten Neuron verbindet, WLjk.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "Diese Indizes wirken zunächst vielleicht etwas rückständig, aber sie stimmen mit der Art und Weise überein, wie Sie die Gewichtsmatrix indizieren würden, über die ich im Video zu Teil 1 gesprochen habe.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "Nach wie vor ist es immer noch schön, der relevanten gewichteten Summe einen Namen zu geben, z. B. z, sodass die Aktivierung der letzten Ebene nur Ihre spezielle Funktion ist, z. B. das Sigmoid, das auf z angewendet wird.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Sie können sehen, was ich meine, wenn es sich bei all diesen Gleichungen im Wesentlichen um dieselben Gleichungen handelt, die wir zuvor im Fall von einem Neuron pro Schicht hatten, sieht es nur etwas komplizierter aus.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain rule derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "Und tatsächlich sieht der abgeleitete Ausdruck der Kettenregel, der beschreibt, wie empfindlich die Kosten auf ein bestimmtes Gewicht reagieren, im Wesentlichen gleich aus.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Ich überlasse es Ihnen, innezuhalten und über jeden dieser Begriffe nachzudenken, wenn Sie möchten.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "Was sich hier jedoch ändert, ist die Ableitung der Kosten in Bezug auf eine der Aktivierungen in der Schicht L-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "Der Unterschied besteht in diesem Fall darin, dass das Neuron die Kostenfunktion über mehrere unterschiedliche Wege beeinflusst.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "Das heißt, es beeinflusst einerseits AL0, das in der Kostenfunktion eine Rolle spielt, aber es hat auch Einfluss auf AL1, das ebenfalls in der Kostenfunktion eine Rolle spielt, und das muss man addieren.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "Und das ist so ziemlich alles.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Sobald Sie wissen, wie empfindlich die Kostenfunktion auf die Aktivierungen in dieser vorletzten Ebene reagiert, können Sie den Vorgang einfach für alle Gewichtungen und Bias wiederholen, die in diese Ebene eingespeist werden.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "Also klopfen Sie sich selbst auf die Schulter!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Wenn das alles Sinn macht, haben Sie jetzt tief in den Kern der Backpropagation geschaut, dem Arbeitstier hinter dem Lernen neuronaler Netze.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Diese Kettenregelausdrücke liefern Ihnen die Ableitungen, die jede Komponente im Gradienten bestimmen, was dazu beiträgt, die Kosten des Netzwerks durch wiederholte Abwärtsschritte zu minimieren.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Wenn Sie sich zurücklehnen und über all das nachdenken, werden Sie feststellen, dass dies eine Menge Komplexitätsebenen ist, mit denen Sie sich befassen müssen. Machen Sie sich also keine Sorgen, wenn Ihr Verstand einige Zeit braucht, um alles zu verdauen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]