1
00:00:04,019 --> 00:00:06,379
Wir gehen davon aus, dass du Teil 3 gesehen hast, 

2
00:00:06,379 --> 00:00:09,920
in dem du den Backpropagation-Algorithmus intuitiv nachvollziehen konntest.

3
00:00:11,040 --> 00:00:14,220
Hier werden wir ein wenig formaler und tauchen in die relevante Kalkulation ein.

4
00:00:14,820 --> 00:00:18,475
Es ist normal, dass das zumindest ein bisschen verwirrend ist, also gilt das Mantra, 

5
00:00:18,475 --> 00:00:21,400
regelmäßig innezuhalten und nachzudenken, hier genauso wie anderswo.

6
00:00:21,940 --> 00:00:27,754
Unser Hauptziel ist es zu zeigen, wie Menschen im Bereich des maschinellen Lernens 

7
00:00:27,754 --> 00:00:33,640
über die Kettenregel aus der Infinitesimalrechnung im Kontext von Netzwerken denken.

8
00:00:34,340 --> 00:00:37,329
Für diejenigen unter euch, die sich mit den entsprechenden Berechnungen nicht auskennen, 

9
00:00:37,329 --> 00:00:38,740
habe ich eine ganze Serie zu diesem Thema.

10
00:00:39,960 --> 00:00:42,958
Beginnen wir mit einem sehr einfachen Netzwerk, 

11
00:00:42,958 --> 00:00:46,020
in dem jede Schicht ein einzelnes Neuron enthält.

12
00:00:46,320 --> 00:00:49,814
Dieses Netz wird durch drei Gewichte und drei Verzerrungen bestimmt, 

13
00:00:49,814 --> 00:00:53,917
und unser Ziel ist es zu verstehen, wie empfindlich die Kostenfunktion auf diese 

14
00:00:53,917 --> 00:00:54,880
Variablen reagiert.

15
00:00:55,420 --> 00:00:57,963
Auf diese Weise wissen wir, welche Anpassungen an diesen 

16
00:00:57,963 --> 00:01:00,820
Begriffen die effizienteste Senkung der Kostenfunktion bewirken.

17
00:01:01,960 --> 00:01:04,840
Wir konzentrieren uns nur auf die Verbindung zwischen den letzten beiden Neuronen.

18
00:01:05,980 --> 00:01:09,342
Beschriften wir die Aktivierung des letzten Neurons mit einem hochgestellten L, 

19
00:01:09,342 --> 00:01:11,360
das angibt, in welcher Schicht es sich befindet.

20
00:01:11,680 --> 00:01:15,560
Die Aktivierung des vorherigen Neurons ist also AL-1.

21
00:01:16,360 --> 00:01:19,361
Das sind keine Exponenten, sondern nur eine Möglichkeit, das, 

22
00:01:19,361 --> 00:01:23,040
worüber wir reden, zu indizieren, da ich mir später die Indizes sparen will.

23
00:01:23,720 --> 00:01:28,169
Nehmen wir an, der Wert, den wir für diese letzte Aktivierung für ein bestimmtes 

24
00:01:28,169 --> 00:01:32,180
Trainingsbeispiel haben wollen, ist y. y kann zum Beispiel 0 oder 1 sein.

25
00:01:32,840 --> 00:01:39,240
Die Kosten dieses Netzes für ein einziges Trainingsbeispiel sind also AL-y2.

26
00:01:40,260 --> 00:01:44,380
Wir bezeichnen die Kosten für dieses eine Trainingsbeispiel als c0.

27
00:01:45,900 --> 00:01:50,659
Zur Erinnerung: Diese letzte Aktivierung wird durch eine Gewichtung bestimmt, 

28
00:01:50,659 --> 00:01:55,602
die ich wL nenne, mal der vorherigen Neuronenaktivierung plus einer Vorspannung, 

29
00:01:55,602 --> 00:01:56,640
die ich bL nenne.

30
00:01:57,420 --> 00:02:01,320
Dann pumpst du das durch eine spezielle nichtlineare Funktion wie das Sigmoid oder ReLU.

31
00:02:01,800 --> 00:02:05,443
Es macht die Sache einfacher, wenn wir dieser gewichteten Summe einen eigenen 

32
00:02:05,443 --> 00:02:09,320
Namen geben, z. B. z, mit demselben Hochkomma wie die entsprechenden Aktivierungen.

33
00:02:10,380 --> 00:02:13,711
Das sind viele Begriffe, und du kannst dir das so vorstellen, 

34
00:02:13,711 --> 00:02:18,333
dass das Gewicht, die vorherige Aktion und die Vorspannung zusammen verwendet werden, 

35
00:02:18,333 --> 00:02:21,610
um z zu berechnen, was uns wiederum erlaubt, a zu berechnen, 

36
00:02:21,610 --> 00:02:25,480
was uns schließlich zusammen mit einer Konstante y die Kosten berechnet.

37
00:02:27,340 --> 00:02:30,016
Und natürlich wird AL-1 durch sein eigenes Gewicht, 

38
00:02:30,016 --> 00:02:33,721
seine Vorspannung und so weiter beeinflusst, aber darauf wollen wir uns 

39
00:02:33,721 --> 00:02:35,060
jetzt nicht konzentrieren.

40
00:02:35,700 --> 00:02:37,620
Das sind doch alles nur Zahlen, oder?

41
00:02:38,060 --> 00:02:41,040
Und es kann schön sein, sich vorzustellen, dass jeder seine eigene kleine Zahlenreihe hat.

42
00:02:41,720 --> 00:02:45,186
Unser erstes Ziel ist es, zu verstehen, wie empfindlich die 

43
00:02:45,186 --> 00:02:49,000
Kostenfunktion auf kleine Änderungen unseres Gewichts wL reagiert.

44
00:02:49,540 --> 00:02:54,860
Oder anders ausgedrückt: Was ist die Ableitung von c nach wL?

45
00:02:55,600 --> 00:02:59,126
Wenn du den Begriff del w siehst, kannst du dir vorstellen, 

46
00:02:59,126 --> 00:03:03,534
dass er eine winzige Änderung von w bedeutet, z. B. eine Änderung um 0,01, 

47
00:03:03,534 --> 00:03:08,060
und der Begriff del c steht für die daraus resultierende Änderung der Kosten.

48
00:03:08,060 --> 00:03:10,220
Was wir wollen, ist ihr Verhältnis.

49
00:03:11,260 --> 00:03:15,593
Diese winzige Veränderung von wL bewirkt eine Veränderung von zL, 

50
00:03:15,593 --> 00:03:21,240
die wiederum eine Veränderung von AL bewirkt, was sich direkt auf die Kosten auswirkt.

51
00:03:23,120 --> 00:03:27,962
Also lösen wir die Sache auf, indem wir zuerst das Verhältnis zwischen einer winzigen 

52
00:03:27,962 --> 00:03:31,285
Änderung von zL und dieser winzigen Änderung w betrachten, 

53
00:03:31,285 --> 00:03:33,200
also die Ableitung von zL nach wL.

54
00:03:33,200 --> 00:03:36,879
Ebenso berücksichtigst du das Verhältnis zwischen der Änderung von AL 

55
00:03:36,879 --> 00:03:39,928
und der winzigen Änderung von zL, die sie verursacht hat, 

56
00:03:39,928 --> 00:03:44,660
sowie das Verhältnis zwischen dem endgültigen Anstoß an c und diesem Zwischenanstoß an AL.

57
00:03:45,740 --> 00:03:50,244
Das hier ist die Kettenregel, bei der die Multiplikation dieser drei 

58
00:03:50,244 --> 00:03:55,140
Verhältnisse die Empfindlichkeit von c auf kleine Änderungen von wL ergibt.

59
00:03:56,880 --> 00:03:59,533
Auf dem Bildschirm siehst du jetzt eine Menge Symbole. 

60
00:03:59,533 --> 00:04:03,200
Nimm dir einen Moment Zeit, um sicherzustellen, dass du sie alle verstehst, 

61
00:04:03,200 --> 00:04:06,240
denn jetzt werden wir die entsprechenden Ableitungen berechnen.

62
00:04:07,440 --> 00:04:13,160
Die Ableitung von c nach AL ergibt sich als 2AL-y.

63
00:04:13,980 --> 00:04:18,181
Das bedeutet, dass seine Größe proportional zur Differenz zwischen dem Ausgang des 

64
00:04:18,181 --> 00:04:22,736
Netzes und dem gewünschten Ergebnis ist. Wenn das Ergebnis also sehr unterschiedlich ist, 

65
00:04:22,736 --> 00:04:27,140
haben selbst kleine Änderungen einen großen Einfluss auf die endgültige Kostenfunktion.

66
00:04:27,840 --> 00:04:32,640
Die Ableitung von AL nach zL ist einfach die Ableitung unserer Sigmoidfunktion, 

67
00:04:32,640 --> 00:04:36,180
oder welche Nichtlinearität du auch immer verwenden willst.

68
00:04:37,220 --> 00:04:44,660
Die Ableitung von zL nach wL ist dann AL-1.

69
00:04:45,760 --> 00:04:48,339
Ich weiß nicht, wie es dir geht, aber ich glaube, es ist leicht, 

70
00:04:48,339 --> 00:04:50,840
mit dem Kopf in den Formeln zu stecken, ohne sich einen Moment 

71
00:04:50,840 --> 00:04:53,420
zurückzulehnen und sich daran zu erinnern, was sie alle bedeuten.

72
00:04:53,920 --> 00:04:58,370
Im Fall dieser letzten Ableitung hängt der Einfluss der kleinen Änderung des 

73
00:04:58,370 --> 00:05:02,820
Gewichts auf die letzte Schicht davon ab, wie stark das vorherige Neuron ist.

74
00:05:03,380 --> 00:05:05,938
Denke daran, dass hier die Idee der Neuronen, die zusammen 

75
00:05:05,938 --> 00:05:08,280
feuern und zusammen verdrahtet sind, zum Tragen kommt.

76
00:05:09,200 --> 00:05:12,304
Und all dies ist die Ableitung der Kosten für ein 

77
00:05:12,304 --> 00:05:15,720
bestimmtes einzelnes Trainingsbeispiel in Bezug auf wL.

78
00:05:16,440 --> 00:05:19,761
Da bei der vollständigen Kostenfunktion alle Kosten über viele 

79
00:05:19,761 --> 00:05:23,136
verschiedene Trainingsbeispiele hinweg gemittelt werden müssen, 

80
00:05:23,136 --> 00:05:27,460
muss die Ableitung dieses Ausdrucks über alle Trainingsbeispiele gemittelt werden.

81
00:05:28,380 --> 00:05:31,673
Das ist natürlich nur eine Komponente des Gradientenvektors, 

82
00:05:31,673 --> 00:05:35,020
der sich aus den partiellen Ableitungen der Kostenfunktion in 

83
00:05:35,020 --> 00:05:38,260
Bezug auf all diese Gewichte und Verzerrungen zusammensetzt.

84
00:05:40,640 --> 00:05:43,029
Aber auch wenn das nur eine der vielen Teilableitungen ist, 

85
00:05:43,029 --> 00:05:45,260
die wir brauchen, macht das mehr als 50% der Arbeit aus.

86
00:05:46,340 --> 00:05:49,720
Die Empfindlichkeit gegenüber der Verzerrung ist zum Beispiel fast identisch.

87
00:05:50,040 --> 00:05:55,020
Wir müssen nur den Begriff del z del w gegen einen Begriff del z del b austauschen.

88
00:05:58,420 --> 00:06:02,400
Und wenn du dir die entsprechende Formel ansiehst, ergibt die Ableitung 1.

89
00:06:06,140 --> 00:06:10,966
Außerdem - und hier kommt die Idee der Rückwärtspropagierung ins Spiel - kannst du sehen, 

90
00:06:10,966 --> 00:06:15,740
wie empfindlich diese Kostenfunktion auf die Aktivierung der vorherigen Schicht reagiert.

91
00:06:15,740 --> 00:06:20,996
Die anfängliche Ableitung in der Kettenregel, also die Empfindlichkeit 

92
00:06:20,996 --> 00:06:25,660
von z gegenüber der vorherigen Aktivierung, ist das Gewicht wL.

93
00:06:26,640 --> 00:06:30,674
Auch wenn wir nicht in der Lage sind, die Aktivierung der vorherigen Schicht direkt 

94
00:06:30,674 --> 00:06:33,555
zu beeinflussen, ist es hilfreich, sie im Auge zu behalten, 

95
00:06:33,555 --> 00:06:37,253
denn jetzt können wir dieselbe Kettenregel rückwärts iterieren, um zu sehen, 

96
00:06:37,253 --> 00:06:41,191
wie empfindlich die Kostenfunktion auf die vorherigen Gewichte und die vorherigen 

97
00:06:41,191 --> 00:06:42,440
Voreinstellungen reagiert.

98
00:06:43,180 --> 00:06:46,035
Du denkst vielleicht, dass dies ein zu einfaches Beispiel ist, 

99
00:06:46,035 --> 00:06:49,796
da alle Schichten ein Neuron haben, aber in einem echten Netzwerk werden die Dinge 

100
00:06:49,796 --> 00:06:51,020
exponentiell komplizierter.

101
00:06:51,700 --> 00:06:55,464
Aber ehrlich gesagt ändert sich gar nicht so viel, 

102
00:06:55,464 --> 00:06:58,860
wenn wir den Schichten mehrere Neuronen geben.

103
00:06:59,380 --> 00:07:02,625
Die Aktivierung einer bestimmten Schicht ist nicht einfach AL, 

104
00:07:02,625 --> 00:07:07,160
sondern hat auch einen Index, der angibt, um welches Neuron der Schicht es sich handelt.

105
00:07:07,160 --> 00:07:11,731
Wir verwenden den Buchstaben k, um die Schicht L-1 zu kennzeichnen, 

106
00:07:11,731 --> 00:07:14,420
und j, um die Schicht L zu kennzeichnen.

107
00:07:15,260 --> 00:07:18,980
Für die Kosten schauen wir uns wieder an, wie hoch der gewünschte Output ist, 

108
00:07:18,980 --> 00:07:22,175
aber dieses Mal addieren wir die Quadrate der Differenzen zwischen 

109
00:07:22,175 --> 00:07:25,180
diesen letzten Schichtaktivierungen und dem gewünschten Output.

110
00:07:26,080 --> 00:07:30,840
Das heißt, du nimmst eine Summe über ALj minus yj zum Quadrat.

111
00:07:33,040 --> 00:07:37,768
Da es viel mehr Gewichte gibt, muss jedes einzelne ein paar mehr Indizes haben, 

112
00:07:37,768 --> 00:07:41,551
um zu wissen, wo es ist. Nennen wir also das Gewicht der Kante, 

113
00:07:41,551 --> 00:07:44,920
die das k-te Neuron mit dem j-ten Neuron verbindet, WLjk.

114
00:07:45,620 --> 00:07:48,335
Diese Indizes mögen sich zunächst ein wenig verkehrt anfühlen, 

115
00:07:48,335 --> 00:07:50,964
aber sie stimmen mit den Indizes der Gewichtsmatrix überein, 

116
00:07:50,964 --> 00:07:53,120
über die ich in Teil 1 des Videos gesprochen habe.

117
00:07:53,620 --> 00:07:57,789
Es ist nach wie vor sinnvoll, der entsprechenden gewichteten Summe einen Namen zu geben, 

118
00:07:57,789 --> 00:08:01,396
z. B. z, so dass die Aktivierung der letzten Schicht einfach deine spezielle 

119
00:08:01,396 --> 00:08:04,160
Funktion ist, z. B. das Sigmoid, das auf z angewendet wird.

120
00:08:04,660 --> 00:08:08,452
Du siehst, was ich meine: Das sind im Wesentlichen dieselben Gleichungen, 

121
00:08:08,452 --> 00:08:11,578
die wir schon für den Fall eines Neurons pro Schicht hatten, 

122
00:08:11,578 --> 00:08:13,680
nur dass es etwas komplizierter aussieht.

123
00:08:15,440 --> 00:08:19,151
Und tatsächlich sieht der abgeleitete Ausdruck der Kettenregel, der beschreibt, 

124
00:08:19,151 --> 00:08:22,167
wie empfindlich die Kosten auf ein bestimmtes Gewicht reagieren, 

125
00:08:22,167 --> 00:08:23,420
im Wesentlichen gleich aus.

126
00:08:23,920 --> 00:08:26,378
Ich überlasse es dir, innezuhalten und über jeden dieser Begriffe nachzudenken, 

127
00:08:26,378 --> 00:08:26,840
wenn du willst.

128
00:08:28,980 --> 00:08:32,921
Was sich hier jedoch ändert, ist die Ableitung der Kosten 

129
00:08:32,921 --> 00:08:36,659
in Bezug auf eine der Aktivierungen in der Schicht L-1.

130
00:08:37,780 --> 00:08:40,329
In diesem Fall besteht der Unterschied darin, dass das Neuron 

131
00:08:40,329 --> 00:08:42,880
die Kostenfunktion über mehrere verschiedene Wege beeinflusst.

132
00:08:44,680 --> 00:08:49,621
Das heißt, sie beeinflusst einerseits AL0, das eine Rolle in der Kostenfunktion spielt, 

133
00:08:49,621 --> 00:08:53,777
aber sie hat auch einen Einfluss auf AL1, das ebenfalls eine Rolle in der 

134
00:08:53,777 --> 00:08:57,540
Kostenfunktion spielt, und diese beiden Faktoren musst du addieren.

135
00:08:59,820 --> 00:09:03,040
Und das, na ja, das war's auch schon.

136
00:09:03,500 --> 00:09:06,706
Sobald du weißt, wie empfindlich die Kostenfunktion auf die Aktivierungen 

137
00:09:06,706 --> 00:09:09,740
in dieser vorletzten Schicht reagiert, kannst du den Prozess für alle 

138
00:09:09,740 --> 00:09:12,860
Gewichte und Vorspannungen wiederholen, die in diese Schicht einfließen.

139
00:09:13,900 --> 00:09:14,960
Also klopf dir selbst auf die Schulter!

140
00:09:15,300 --> 00:09:19,013
Wenn dir das alles einleuchtet, hast du jetzt einen tiefen Einblick in das Herz 

141
00:09:19,013 --> 00:09:22,680
der Backpropagation bekommen, dem Arbeitspferd, mit dem neuronale Netze lernen.

142
00:09:23,300 --> 00:09:26,463
Mit diesen Kettenregelausdrücken erhältst du die Ableitungen, 

143
00:09:26,463 --> 00:09:29,677
die jede Komponente der Steigung bestimmen, die dazu beiträgt, 

144
00:09:29,677 --> 00:09:33,300
die Kosten des Netzes zu minimieren, indem es immer wieder bergab geht.

145
00:09:34,300 --> 00:09:37,713
Wenn du dich zurücklehnst und über all das nachdenkst, 

146
00:09:37,713 --> 00:09:42,740
ist das eine ganze Menge an Komplexität, die du in deinem Kopf verarbeiten musst.

