1
00:00:00,000 --> 00:00:08,420
Die harte Annahme hier ist, dass Sie Teil 3

2
00:00:08,420 --> 00:00:11,160
gesehen haben, der eine intuitive Anleitung zum Backpropagation-Algorithmus gibt.

3
00:00:11,160 --> 00:00:14,920
Hier werden wir etwas formeller und tauchen in die relevante Infinitesimalrechnung ein.

4
00:00:14,920 --> 00:00:18,560
Es ist normal, dass dies zumindest ein wenig verwirrend ist, daher

5
00:00:18,560 --> 00:00:22,000
gilt das Mantra, regelmäßig innezuhalten und nachzudenken, hier genauso wie anderswo.

6
00:00:22,000 --> 00:00:26,620
Unser Hauptziel besteht darin, zu zeigen, wie Menschen, die sich mit maschinellem Lernen befassen,

7
00:00:26,620 --> 00:00:31,900
üblicherweise über die Kettenregel aus der Analysis im Kontext von Netzwerken denken, was ein

8
00:00:31,900 --> 00:00:34,580
anderes Gefühl vermittelt als die Herangehensweise der meisten Einführungskurse in Analysis an das Thema.

9
00:00:34,580 --> 00:00:38,300
Für diejenigen unter Ihnen, die sich mit der relevanten Infinitesimalrechnung

10
00:00:38,300 --> 00:00:39,300
nicht auskennen, habe ich eine ganze Reihe zu diesem Thema.

11
00:00:39,300 --> 00:00:44,840
Beginnen wir mit einem äußerst einfachen Netzwerk, bei

12
00:00:44,840 --> 00:00:46,780
dem jede Schicht ein einzelnes Neuron enthält.

13
00:00:46,780 --> 00:00:51,880
Dieses Netzwerk wird durch drei Gewichte und drei Verzerrungen bestimmt. Unser Ziel

14
00:00:51,880 --> 00:00:55,640
ist es zu verstehen, wie empfindlich die Kostenfunktion auf diese Variablen reagiert.

15
00:00:55,640 --> 00:00:59,780
Auf diese Weise wissen wir, welche Anpassungen dieser

16
00:00:59,780 --> 00:01:01,100
Bedingungen die effizienteste Verringerung der Kostenfunktion bewirken.

17
00:01:01,100 --> 00:01:05,360
Wir konzentrieren uns nur auf die Verbindung zwischen den letzten beiden Neuronen.

18
00:01:05,360 --> 00:01:10,400
Beschriften wir die Aktivierung dieses letzten Neurons mit einem hochgestellten

19
00:01:10,400 --> 00:01:11,800
L, das angibt, in welcher Schicht es sich befindet.

20
00:01:11,800 --> 00:01:16,560
Die Aktivierung des vorherigen Neurons ist also AL-1.

21
00:01:16,560 --> 00:01:20,120
Dabei handelt es sich nicht um Exponenten, sondern nur um eine Möglichkeit, das, worüber

22
00:01:20,120 --> 00:01:23,120
wir sprechen, zu indizieren, da ich später Indizes für verschiedene Indizes speichern möchte.

23
00:01:23,600 --> 00:01:28,880
Nehmen wir an, dass der Wert, den diese letzte Aktivierung für ein bestimmtes

24
00:01:28,880 --> 00:01:33,020
Trainingsbeispiel haben soll, y ist. Beispielsweise könnte y 0 oder 1 sein.

25
00:01:33,020 --> 00:01:39,040
Die Kosten dieses Netzwerks für ein einzelnes Trainingsbeispiel betragen also AL-y2.

26
00:01:39,040 --> 00:01:46,120
Wir bezeichnen die Kosten dieses einen Trainingsbeispiels als c0.

27
00:01:46,120 --> 00:01:51,920
Zur Erinnerung: Diese letzte Aktivierung wird durch ein Gewicht bestimmt, das ich wL nenne, multipliziert

28
00:01:51,920 --> 00:01:57,600
mit der Aktivierung des vorherigen Neurons plus einer gewissen Abweichung, die ich bL nenne.

29
00:01:57,600 --> 00:02:01,560
Dann pumpen Sie das durch eine spezielle nichtlineare Funktion wie Sigmoid oder ReLU.

30
00:02:01,560 --> 00:02:05,400
Es wird uns tatsächlich die Arbeit erleichtern, wenn wir dieser gewichteten Summe einen speziellen

31
00:02:05,400 --> 00:02:10,600
Namen geben, z. B. z, mit demselben hochgestellten Index wie die relevanten Aktivierungen.

32
00:02:10,600 --> 00:02:15,320
Das sind viele Begriffe, und Sie könnten sich das so vorstellen, dass das Gewicht, die vorherige

33
00:02:15,320 --> 00:02:21,800
Aktion und der Bias zusammen verwendet werden, um z zu berechnen, was uns wiederum die Berechnung

34
00:02:21,800 --> 00:02:27,360
von a ermöglicht, was uns schließlich zusammen mit einer Konstante y ermöglicht Wir berechnen die Kosten.

35
00:02:27,360 --> 00:02:33,440
Und natürlich wird AL-1 durch sein eigenes Gewicht, seine Voreingenommenheit

36
00:02:33,440 --> 00:02:35,920
usw. beeinflusst, aber darauf werden wir uns jetzt nicht konzentrieren.

37
00:02:35,920 --> 00:02:38,120
Das sind doch alles nur Zahlen, oder?

38
00:02:38,120 --> 00:02:41,960
Und es kann schön sein, sich vorzustellen, dass jede einzelne ihre eigene kleine Zahlenreihe hat.

39
00:02:41,960 --> 00:02:47,480
Unser erstes Ziel besteht darin zu verstehen, wie empfindlich

40
00:02:47,480 --> 00:02:49,820
die Kostenfunktion auf kleine Änderungen unseres Gewichts wL reagiert.

41
00:02:49,820 --> 00:02:55,740
Oder anders ausgedrückt: Was ist die Ableitung von c nach wL?

42
00:02:55,740 --> 00:03:01,220
Wenn Sie diesen del w-Begriff sehen, stellen Sie sich vor, dass er einen kleinen Anstoß an w bedeutet, etwa eine Änderung um 0.

43
00:03:01,220 --> 00:03:08,820
01, und stellen Sie sich diesen del c-Begriff so vor, dass er bedeutet, was auch immer der daraus resultierende Kostenschub ist.

44
00:03:08,820 --> 00:03:10,900
Was wir wollen, ist ihr Verhältnis.

45
00:03:10,900 --> 00:03:17,740
Konzeptionell führt dieser kleine Schub für wL zu einem gewissen Schub für zL, was

46
00:03:17,740 --> 00:03:23,220
wiederum einen gewissen Schub für AL verursacht, was sich direkt auf die Kosten auswirkt.

47
00:03:23,220 --> 00:03:28,020
Also unterteilen wir die Sache, indem wir zunächst das Verhältnis einer winzigen Änderung von

48
00:03:28,020 --> 00:03:33,340
zL zu dieser winzigen Änderung w betrachten, also die Ableitung von zL nach wL.

49
00:03:33,340 --> 00:03:38,820
Ebenso berücksichtigen Sie dann das Verhältnis der Änderung von AL zu der

50
00:03:38,820 --> 00:03:43,900
winzigen Änderung von zL, die sie verursacht hat, sowie das Verhältnis

51
00:03:43,900 --> 00:03:45,900
zwischen dem endgültigen Anstoß an c und diesem Zwischenanstoß an AL.

52
00:03:45,900 --> 00:03:51,880
Das hier ist die Kettenregel, bei der die Multiplikation dieser drei

53
00:03:51,880 --> 00:03:57,340
Verhältnisse die Empfindlichkeit von c gegenüber kleinen Änderungen in wL ergibt.

54
00:03:57,340 --> 00:04:01,620
Auf dem Bildschirm sind also gerade viele Symbole zu sehen, und nehmen Sie sich einen Moment Zeit,

55
00:04:01,620 --> 00:04:07,460
um sich zu vergewissern, dass sie alle klar sind, denn jetzt werden wir die relevanten Ableitungen berechnen.

56
00:04:07,460 --> 00:04:14,220
Die Ableitung von c nach AL ergibt 2AL-y.

57
00:04:14,220 --> 00:04:19,300
Das bedeutet, dass seine Größe proportional zur Differenz zwischen der Ausgabe des Netzwerks

58
00:04:19,300 --> 00:04:24,480
und dem, was wir wollen, ist. Wenn diese Ausgabe also sehr unterschiedlich ist,

59
00:04:24,480 --> 00:04:28,380
können selbst geringfügige Änderungen einen großen Einfluss auf die endgültige Kostenfunktion haben.

60
00:04:28,380 --> 00:04:33,860
Die Ableitung von AL nach zL ist einfach die

61
00:04:33,860 --> 00:04:37,420
Ableitung unserer Sigmoidfunktion oder der von Ihnen gewählten Nichtlinearität.

62
00:04:37,420 --> 00:04:46,180
Die Ableitung von zL nach wL ergibt AL-1.

63
00:04:46,180 --> 00:04:49,460
Ich weiß nicht, wie es Ihnen geht, aber ich denke, es ist leicht, mit dem Kopf in den Formeln stecken

64
00:04:49,460 --> 00:04:54,180
zu bleiben, ohne sich einen Moment Zeit zu nehmen, sich zurückzulehnen und sich daran zu erinnern, was sie alle bedeuten.

65
00:04:54,180 --> 00:04:58,860
Im Fall dieser letzten Ableitung hängt der Einfluss des kleinen Gewichtsschubs auf

66
00:04:58,860 --> 00:05:03,220
die letzte Schicht davon ab, wie stark das vorherige Neuron ist.

67
00:05:03,220 --> 00:05:09,320
Denken Sie daran, hier kommt die Idee „Neuronen, die gemeinsam feuern, miteinander verdrahten“ ins Spiel.

68
00:05:09,320 --> 00:05:14,840
Und all dies ist lediglich die Ableitung der Kosten

69
00:05:14,840 --> 00:05:16,580
für ein bestimmtes einzelnes Trainingsbeispiel in Bezug auf wL.

70
00:05:16,580 --> 00:05:20,940
Da die Vollkostenfunktion die Mittelung aller dieser Kosten

71
00:05:20,940 --> 00:05:27,300
über viele verschiedene Trainingsbeispiele hinweg beinhaltet, erfordert ihre

72
00:05:27,300 --> 00:05:28,540
Ableitung die Mittelung dieses Ausdrucks über alle Trainingsbeispiele.

73
00:05:28,540 --> 00:05:33,860
Das ist natürlich nur eine Komponente des Gradientenvektors, der aus den partiellen Ableitungen

74
00:05:33,860 --> 00:05:40,780
der Kostenfunktion in Bezug auf all diese Gewichte und Verzerrungen aufgebaut wird.

75
00:05:40,780 --> 00:05:44,340
Aber auch wenn das nur eine der vielen partiellen Ableitungen ist, die

76
00:05:44,340 --> 00:05:46,460
wir brauchen, macht es mehr als 50 % der Arbeit aus.

77
00:05:46,460 --> 00:05:50,300
Die Empfindlichkeit gegenüber der Voreingenommenheit ist beispielsweise nahezu identisch.

78
00:05:50,300 --> 00:05:58,980
Wir müssen nur diesen del z del w-Term durch a del z del b ersetzen.

79
00:05:58,980 --> 00:06:04,700
Und wenn Sie sich die relevante Formel ansehen, ergibt sich für diese Ableitung 1.

80
00:06:04,700 --> 00:06:11,700
Außerdem, und hier kommt die Idee der Rückwärtsausbreitung ins Spiel, können Sie

81
00:06:11,700 --> 00:06:16,180
sehen, wie empfindlich diese Kostenfunktion auf die Aktivierung der vorherigen Schicht reagiert.

82
00:06:16,180 --> 00:06:21,380
Diese anfängliche Ableitung im Kettenregelausdruck, die Empfindlichkeit von z gegenüber

83
00:06:21,380 --> 00:06:25,420
der vorherigen Aktivierung, ergibt sich nämlich als Gewicht wL.

84
00:06:25,420 --> 00:06:30,100
Und auch wenn wir die Aktivierung der vorherigen Ebene nicht direkt beeinflussen

85
00:06:30,100 --> 00:06:35,280
können, ist es dennoch hilfreich, den Überblick zu behalten, denn jetzt

86
00:06:35,280 --> 00:06:40,780
können wir dieselbe Kettenregelidee einfach weiter rückwärts iterieren, um zu sehen,

87
00:06:40,780 --> 00:06:43,680
wie empfindlich die Kostenfunktion darauf reagiert frühere Gewichtungen und frühere Vorurteile.

88
00:06:43,680 --> 00:06:47,940
Und Sie könnten denken, dass dies ein zu einfaches Beispiel ist, da alle Schichten

89
00:06:47,940 --> 00:06:51,320
ein Neuron haben und die Dinge für ein echtes Netzwerk exponentiell komplizierter werden.

90
00:06:51,320 --> 00:06:56,560
Aber ehrlich gesagt ändert sich nicht so viel, wenn wir den Schichten mehrere Neuronen geben,

91
00:06:56,560 --> 00:06:59,320
es sind eigentlich nur ein paar weitere Indizes, die man im Auge behalten muss.

92
00:06:59,320 --> 00:07:03,580
Anstatt dass die Aktivierung einer bestimmten Schicht einfach AL ist, wird sie auch

93
00:07:03,580 --> 00:07:07,920
einen Index haben, der angibt, um welches Neuron dieser Schicht es sich handelt.

94
00:07:07,920 --> 00:07:15,280
Verwenden wir den Buchstaben k, um die Ebene L-1 zu indizieren, und j, um die Ebene L zu indizieren.

95
00:07:15,280 --> 00:07:20,720
Für die Kosten schauen wir uns erneut an, wie hoch die gewünschte Ausgabe ist, aber dieses Mal

96
00:07:20,720 --> 00:07:26,120
addieren wir die Quadrate der Differenzen zwischen diesen Aktivierungen der letzten Ebene und der gewünschten Ausgabe.

97
00:07:26,120 --> 00:07:33,280
Das heißt, Sie bilden die Summe über ALj minus yj im Quadrat.

98
00:07:33,280 --> 00:07:36,500
Da es viel mehr Gewichte gibt, muss jedes über ein paar weitere Indizes verfügen,

99
00:07:36,500 --> 00:07:41,380
um den Überblick zu behalten, wo es sich befindet. Nennen wir also das

100
00:07:41,380 --> 00:07:45,740
Gewicht der Kante, die dieses k-te Neuron mit dem j-ten Neuron verbindet, WLjk.

101
00:07:45,740 --> 00:07:49,820
Diese Indizes wirken zunächst vielleicht etwas rückständig, aber sie stimmen mit der Art und Weise überein,

102
00:07:49,820 --> 00:07:53,800
wie Sie die Gewichtsmatrix indizieren würden, über die ich im Video zu Teil 1 gesprochen habe.

103
00:07:53,800 --> 00:07:57,660
Nach wie vor ist es immer noch schön, der relevanten gewichteten Summe einen

104
00:07:57,660 --> 00:08:03,540
Namen zu geben, z. B. z, sodass die Aktivierung der letzten Ebene nur

105
00:08:03,540 --> 00:08:04,980
Ihre spezielle Funktion ist, z. B. das Sigmoid, das auf z angewendet wird.

106
00:08:04,980 --> 00:08:09,100
Sie können sehen, was ich meine, wenn es sich bei all diesen Gleichungen im Wesentlichen um dieselben Gleichungen

107
00:08:09,100 --> 00:08:15,420
handelt, die wir zuvor im Fall von einem Neuron pro Schicht hatten, sieht es nur etwas komplizierter aus.

108
00:08:15,420 --> 00:08:20,620
Und tatsächlich sieht der abgeleitete Ausdruck der Kettenregel, der beschreibt, wie empfindlich

109
00:08:20,620 --> 00:08:23,540
die Kosten auf ein bestimmtes Gewicht reagieren, im Wesentlichen gleich aus.

110
00:08:23,540 --> 00:08:29,420
Ich überlasse es Ihnen, innezuhalten und über jeden dieser Begriffe nachzudenken, wenn Sie möchten.

111
00:08:29,420 --> 00:08:34,900
Was sich hier jedoch ändert, ist die Ableitung der Kosten

112
00:08:34,900 --> 00:08:37,820
in Bezug auf eine der Aktivierungen in der Schicht L-1.

113
00:08:37,820 --> 00:08:42,000
Der Unterschied besteht in diesem Fall darin, dass das

114
00:08:42,000 --> 00:08:43,540
Neuron die Kostenfunktion über mehrere unterschiedliche Wege beeinflusst.

115
00:08:43,540 --> 00:08:51,200
Das heißt, es beeinflusst einerseits AL0, das in der Kostenfunktion eine

116
00:08:51,200 --> 00:08:56,460
Rolle spielt, aber es hat auch Einfluss auf AL1, das ebenfalls

117
00:08:56,460 --> 00:09:00,340
in der Kostenfunktion eine Rolle spielt, und das muss man addieren.

118
00:09:00,340 --> 00:09:03,680
Und das ist so ziemlich alles.

119
00:09:03,680 --> 00:09:08,240
Sobald Sie wissen, wie empfindlich die Kostenfunktion auf die Aktivierungen in

120
00:09:08,240 --> 00:09:12,520
dieser vorletzten Ebene reagiert, können Sie den Vorgang einfach für alle

121
00:09:12,520 --> 00:09:13,920
Gewichtungen und Bias wiederholen, die in diese Ebene eingespeist werden.

122
00:09:13,920 --> 00:09:15,420
Also klopfen Sie sich selbst auf die Schulter!

123
00:09:15,420 --> 00:09:20,480
Wenn das alles Sinn macht, haben Sie jetzt tief in den

124
00:09:20,480 --> 00:09:23,700
Kern der Backpropagation geschaut, dem Arbeitstier hinter dem Lernen neuronaler Netze.

125
00:09:23,700 --> 00:09:27,960
Diese Kettenregelausdrücke liefern Ihnen die Ableitungen, die jede Komponente im Gradienten bestimmen,

126
00:09:27,960 --> 00:09:35,020
was dazu beiträgt, die Kosten des Netzwerks durch wiederholte Abwärtsschritte zu minimieren.

127
00:09:35,020 --> 00:09:38,960
Wenn Sie sich zurücklehnen und über all das nachdenken, werden Sie feststellen, dass dies eine Menge Komplexitätsebenen ist, mit denen

128
00:09:38,960 --> 00:09:42,840
Sie sich befassen müssen. Machen Sie sich also keine Sorgen, wenn Ihr Verstand einige Zeit braucht, um alles zu verdauen.

