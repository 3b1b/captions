1
00:00:00,000 --> 00:00:05,267
Die harte Annahme hier ist, dass Sie Teil 3 gesehen haben,

2
00:00:05,267 --> 00:00:11,160
der eine intuitive Anleitung zum Backpropagation-Algorithmus gibt.

3
00:00:11,160 --> 00:00:14,920
Hier werden wir etwas formeller und tauchen in die relevante Infinitesimalrechnung ein.

4
00:00:14,920 --> 00:00:18,832
Es ist normal, dass dies zumindest ein wenig verwirrend ist, daher gilt das Mantra,

5
00:00:18,832 --> 00:00:22,000
regelmäßig innezuhalten und nachzudenken, hier genauso wie anderswo.

6
00:00:22,000 --> 00:00:24,364
Unser Hauptziel besteht darin, zu zeigen, wie Menschen,

7
00:00:24,364 --> 00:00:27,656
die sich mit maschinellem Lernen befassen, üblicherweise über die Kettenregel

8
00:00:27,656 --> 00:00:29,809
aus der Analysis im Kontext von Netzwerken denken,

9
00:00:29,809 --> 00:00:32,806
was ein anderes Gefühl vermittelt als die Herangehensweise der meisten

10
00:00:32,806 --> 00:00:34,580
Einführungskurse in Analysis an das Thema.

11
00:00:34,580 --> 00:00:37,267
Für diejenigen unter Ihnen, die sich mit der relevanten Infinitesimalrechnung

12
00:00:37,267 --> 00:00:39,300
nicht auskennen, habe ich eine ganze Reihe zu diesem Thema.

13
00:00:39,300 --> 00:00:43,077
Beginnen wir mit einem äußerst einfachen Netzwerk,

14
00:00:43,077 --> 00:00:46,780
bei dem jede Schicht ein einzelnes Neuron enthält.

15
00:00:46,780 --> 00:00:50,607
Dieses Netzwerk wird durch drei Gewichte und drei Verzerrungen bestimmt.

16
00:00:50,607 --> 00:00:55,168
Unser Ziel ist es zu verstehen, wie empfindlich die Kostenfunktion auf diese Variablen

17
00:00:55,168 --> 00:00:55,640
reagiert.

18
00:00:55,640 --> 00:00:58,522
Auf diese Weise wissen wir, welche Anpassungen dieser Bedingungen

19
00:00:58,522 --> 00:01:01,100
die effizienteste Verringerung der Kostenfunktion bewirken.

20
00:01:01,100 --> 00:01:05,360
Wir konzentrieren uns nur auf die Verbindung zwischen den letzten beiden Neuronen.

21
00:01:05,360 --> 00:01:09,440
Beschriften wir die Aktivierung dieses letzten Neurons mit einem hochgestellten L,

22
00:01:09,440 --> 00:01:11,800
das angibt, in welcher Schicht es sich befindet.

23
00:01:11,800 --> 00:01:16,560
Die Aktivierung des vorherigen Neurons ist also AL-1.

24
00:01:16,560 --> 00:01:19,499
Dabei handelt es sich nicht um Exponenten, sondern nur um eine Möglichkeit,

25
00:01:19,499 --> 00:01:21,665
das, worüber wir sprechen, zu indizieren, da ich später

26
00:01:21,665 --> 00:01:23,600
Indizes für verschiedene Indizes speichern möchte.

27
00:01:23,600 --> 00:01:28,402
Nehmen wir an, dass der Wert, den diese letzte Aktivierung für ein bestimmtes

28
00:01:28,402 --> 00:01:33,020
Trainingsbeispiel haben soll, y ist. Beispielsweise könnte y 0 oder 1 sein.

29
00:01:33,020 --> 00:01:39,040
Die Kosten dieses Netzwerks für ein einzelnes Trainingsbeispiel betragen also AL-y2.

30
00:01:39,040 --> 00:01:46,120
Wir bezeichnen die Kosten dieses einen Trainingsbeispiels als c0.

31
00:01:46,120 --> 00:01:50,410
Zur Erinnerung: Diese letzte Aktivierung wird durch ein Gewicht bestimmt,

32
00:01:50,410 --> 00:01:54,295
das ich wL nenne, multipliziert mit der Aktivierung des vorherigen

33
00:01:54,295 --> 00:01:57,600
Neurons plus einer gewissen Abweichung, die ich bL nenne.

34
00:01:57,600 --> 00:02:01,560
Dann pumpen Sie das durch eine spezielle nichtlineare Funktion wie Sigmoid oder ReLU.

35
00:02:01,560 --> 00:02:03,855
Es wird uns tatsächlich die Arbeit erleichtern,

36
00:02:03,855 --> 00:02:06,917
wenn wir dieser gewichteten Summe einen speziellen Namen geben,

37
00:02:06,917 --> 00:02:10,600
z. B. z, mit demselben hochgestellten Index wie die relevanten Aktivierungen.

38
00:02:10,600 --> 00:02:15,236
Das sind viele Begriffe, und Sie könnten sich das so vorstellen, dass das Gewicht,

39
00:02:15,236 --> 00:02:19,706
die vorherige Aktion und der Bias zusammen verwendet werden, um z zu berechnen,

40
00:02:19,706 --> 00:02:22,499
was uns wiederum die Berechnung von a ermöglicht,

41
00:02:22,499 --> 00:02:27,360
was uns schließlich zusammen mit einer Konstante y ermöglicht Wir berechnen die Kosten.

42
00:02:27,360 --> 00:02:32,102
Und natürlich wird AL-1 durch sein eigenes Gewicht, seine Voreingenommenheit usw.

43
00:02:32,102 --> 00:02:35,920
beeinflusst, aber darauf werden wir uns jetzt nicht konzentrieren.

44
00:02:35,920 --> 00:02:38,120
Das sind doch alles nur Zahlen, oder?

45
00:02:38,120 --> 00:02:40,020
Und es kann schön sein, sich vorzustellen, dass

46
00:02:40,020 --> 00:02:41,960
jede einzelne ihre eigene kleine Zahlenreihe hat.

47
00:02:41,960 --> 00:02:45,890
Unser erstes Ziel besteht darin zu verstehen, wie empfindlich die

48
00:02:45,890 --> 00:02:49,820
Kostenfunktion auf kleine Änderungen unseres Gewichts wL reagiert.

49
00:02:49,820 --> 00:02:55,740
Oder anders ausgedrückt: Was ist die Ableitung von c nach wL?

50
00:02:55,740 --> 00:02:58,766
Wenn Sie diesen del w-Begriff sehen, stellen Sie sich vor,

51
00:02:58,766 --> 00:03:02,459
dass er einen kleinen Anstoß an w bedeutet, etwa eine Änderung um 0.01,

52
00:03:02,459 --> 00:03:05,947
und stellen Sie sich diesen del c-Begriff so vor, dass er bedeutet,

53
00:03:05,947 --> 00:03:08,820
was auch immer der daraus resultierende Kostenschub ist.

54
00:03:08,820 --> 00:03:10,900
Was wir wollen, ist ihr Verhältnis.

55
00:03:10,900 --> 00:03:16,558
Konzeptionell führt dieser kleine Schub für wL zu einem gewissen Schub für zL,

56
00:03:16,558 --> 00:03:20,354
was wiederum einen gewissen Schub für AL verursacht,

57
00:03:20,354 --> 00:03:23,220
was sich direkt auf die Kosten auswirkt.

58
00:03:23,220 --> 00:03:28,484
Also unterteilen wir die Sache, indem wir zunächst das Verhältnis einer winzigen Änderung

59
00:03:28,484 --> 00:03:33,340
von zL zu dieser winzigen Änderung w betrachten, also die Ableitung von zL nach wL.

60
00:03:33,340 --> 00:03:37,448
Ebenso berücksichtigen Sie dann das Verhältnis der Änderung von AL zu

61
00:03:37,448 --> 00:03:40,617
der winzigen Änderung von zL, die sie verursacht hat,

62
00:03:40,617 --> 00:03:45,900
sowie das Verhältnis zwischen dem endgültigen Anstoß an c und diesem Zwischenanstoß an AL.

63
00:03:45,900 --> 00:03:51,162
Das hier ist die Kettenregel, bei der die Multiplikation dieser drei

64
00:03:51,162 --> 00:03:57,340
Verhältnisse die Empfindlichkeit von c gegenüber kleinen Änderungen in wL ergibt.

65
00:03:57,340 --> 00:04:00,259
Auf dem Bildschirm sind also gerade viele Symbole zu sehen,

66
00:04:00,259 --> 00:04:03,373
und nehmen Sie sich einen Moment Zeit, um sich zu vergewissern,

67
00:04:03,373 --> 00:04:07,460
dass sie alle klar sind, denn jetzt werden wir die relevanten Ableitungen berechnen.

68
00:04:07,460 --> 00:04:14,220
Die Ableitung von c nach AL ergibt 2AL-y.

69
00:04:14,220 --> 00:04:17,695
Das bedeutet, dass seine Größe proportional zur Differenz zwischen

70
00:04:17,695 --> 00:04:20,599
der Ausgabe des Netzwerks und dem, was wir wollen, ist.

71
00:04:20,599 --> 00:04:23,193
Wenn diese Ausgabe also sehr unterschiedlich ist,

72
00:04:23,193 --> 00:04:26,720
können selbst geringfügige Änderungen einen großen Einfluss auf die

73
00:04:26,720 --> 00:04:28,380
endgültige Kostenfunktion haben.

74
00:04:28,380 --> 00:04:32,972
Die Ableitung von AL nach zL ist einfach die Ableitung unserer

75
00:04:32,972 --> 00:04:37,420
Sigmoidfunktion oder der von Ihnen gewählten Nichtlinearität.

76
00:04:37,420 --> 00:04:46,180
Die Ableitung von zL nach wL ergibt AL-1.

77
00:04:46,180 --> 00:04:48,547
Ich weiß nicht, wie es Ihnen geht, aber ich denke, es ist leicht,

78
00:04:48,547 --> 00:04:51,668
mit dem Kopf in den Formeln stecken zu bleiben, ohne sich einen Moment Zeit zu nehmen,

79
00:04:51,668 --> 00:04:54,180
sich zurückzulehnen und sich daran zu erinnern, was sie alle bedeuten.

80
00:04:54,180 --> 00:04:59,038
Im Fall dieser letzten Ableitung hängt der Einfluss des kleinen Gewichtsschubs

81
00:04:59,038 --> 00:05:03,220
auf die letzte Schicht davon ab, wie stark das vorherige Neuron ist.

82
00:05:03,220 --> 00:05:06,066
Denken Sie daran, hier kommt die Idee „Neuronen,

83
00:05:06,066 --> 00:05:09,320
die gemeinsam feuern, miteinander verdrahten“ ins Spiel.

84
00:05:09,320 --> 00:05:12,855
Und all dies ist lediglich die Ableitung der Kosten für

85
00:05:12,855 --> 00:05:16,580
ein bestimmtes einzelnes Trainingsbeispiel in Bezug auf wL.

86
00:05:16,580 --> 00:05:20,682
Da die Vollkostenfunktion die Mittelung aller dieser Kosten über viele

87
00:05:20,682 --> 00:05:23,628
verschiedene Trainingsbeispiele hinweg beinhaltet,

88
00:05:23,628 --> 00:05:28,540
erfordert ihre Ableitung die Mittelung dieses Ausdrucks über alle Trainingsbeispiele.

89
00:05:28,540 --> 00:05:32,711
Das ist natürlich nur eine Komponente des Gradientenvektors,

90
00:05:32,711 --> 00:05:38,865
der aus den partiellen Ableitungen der Kostenfunktion in Bezug auf all diese Gewichte und

91
00:05:38,865 --> 00:05:40,780
Verzerrungen aufgebaut wird.

92
00:05:40,780 --> 00:05:43,873
Aber auch wenn das nur eine der vielen partiellen Ableitungen ist,

93
00:05:43,873 --> 00:05:46,460
die wir brauchen, macht es mehr als 50 % der Arbeit aus.

94
00:05:46,460 --> 00:05:50,300
Die Empfindlichkeit gegenüber der Voreingenommenheit ist beispielsweise nahezu identisch.

95
00:05:50,300 --> 00:05:58,980
Wir müssen nur diesen del z del w-Term durch a del z del b ersetzen.

96
00:05:58,980 --> 00:06:04,700
Und wenn Sie sich die relevante Formel ansehen, ergibt sich für diese Ableitung 1.

97
00:06:04,700 --> 00:06:10,407
Außerdem, und hier kommt die Idee der Rückwärtsausbreitung ins Spiel, können Sie sehen,

98
00:06:10,407 --> 00:06:16,180
wie empfindlich diese Kostenfunktion auf die Aktivierung der vorherigen Schicht reagiert.

99
00:06:16,180 --> 00:06:19,361
Diese anfängliche Ableitung im Kettenregelausdruck,

100
00:06:19,361 --> 00:06:23,278
die Empfindlichkeit von z gegenüber der vorherigen Aktivierung,

101
00:06:23,278 --> 00:06:25,420
ergibt sich nämlich als Gewicht wL.

102
00:06:25,420 --> 00:06:30,300
Und auch wenn wir die Aktivierung der vorherigen Ebene nicht direkt beeinflussen können,

103
00:06:30,300 --> 00:06:33,206
ist es dennoch hilfreich, den Überblick zu behalten,

104
00:06:33,206 --> 00:06:37,757
denn jetzt können wir dieselbe Kettenregelidee einfach weiter rückwärts iterieren,

105
00:06:37,757 --> 00:06:42,418
um zu sehen, wie empfindlich die Kostenfunktion darauf reagiert frühere Gewichtungen

106
00:06:42,418 --> 00:06:43,680
und frühere Vorurteile.

107
00:06:43,680 --> 00:06:46,567
Und Sie könnten denken, dass dies ein zu einfaches Beispiel ist,

108
00:06:46,567 --> 00:06:50,387
da alle Schichten ein Neuron haben und die Dinge für ein echtes Netzwerk exponentiell

109
00:06:50,387 --> 00:06:51,320
komplizierter werden.

110
00:06:51,320 --> 00:06:53,493
Aber ehrlich gesagt ändert sich nicht so viel,

111
00:06:53,493 --> 00:06:55,666
wenn wir den Schichten mehrere Neuronen geben,

112
00:06:55,666 --> 00:06:59,320
es sind eigentlich nur ein paar weitere Indizes, die man im Auge behalten muss.

113
00:06:59,320 --> 00:07:02,990
Anstatt dass die Aktivierung einer bestimmten Schicht einfach AL ist,

114
00:07:02,990 --> 00:07:07,238
wird sie auch einen Index haben, der angibt, um welches Neuron dieser Schicht es

115
00:07:07,238 --> 00:07:07,920
sich handelt.

116
00:07:07,920 --> 00:07:12,630
Verwenden wir den Buchstaben k, um die Ebene L-1 zu indizieren,

117
00:07:12,630 --> 00:07:15,280
und j, um die Ebene L zu indizieren.

118
00:07:15,280 --> 00:07:19,300
Für die Kosten schauen wir uns erneut an, wie hoch die gewünschte Ausgabe ist,

119
00:07:19,300 --> 00:07:23,066
aber dieses Mal addieren wir die Quadrate der Differenzen zwischen diesen

120
00:07:23,066 --> 00:07:26,120
Aktivierungen der letzten Ebene und der gewünschten Ausgabe.

121
00:07:26,120 --> 00:07:33,280
Das heißt, Sie bilden die Summe über ALj minus yj im Quadrat.

122
00:07:33,280 --> 00:07:37,683
Da es viel mehr Gewichte gibt, muss jedes über ein paar weitere Indizes verfügen,

123
00:07:37,683 --> 00:07:42,517
um den Überblick zu behalten, wo es sich befindet. Nennen wir also das Gewicht der Kante,

124
00:07:42,517 --> 00:07:45,740
die dieses k-te Neuron mit dem j-ten Neuron verbindet, WLjk.

125
00:07:45,740 --> 00:07:48,134
Diese Indizes wirken zunächst vielleicht etwas rückständig,

126
00:07:48,134 --> 00:07:50,049
aber sie stimmen mit der Art und Weise überein,

127
00:07:50,049 --> 00:07:52,762
wie Sie die Gewichtsmatrix indizieren würden, über die ich im Video

128
00:07:52,762 --> 00:07:53,800
zu Teil 1 gesprochen habe.

129
00:07:53,800 --> 00:07:57,391
Nach wie vor ist es immer noch schön, der relevanten gewichteten Summe

130
00:07:57,391 --> 00:08:01,034
einen Namen zu geben, z. B. z, sodass die Aktivierung der letzten Ebene

131
00:08:01,034 --> 00:08:04,980
nur Ihre spezielle Funktion ist, z. B. das Sigmoid, das auf z angewendet wird.

132
00:08:04,980 --> 00:08:08,460
Sie können sehen, was ich meine, wenn es sich bei all diesen Gleichungen

133
00:08:08,460 --> 00:08:10,843
im Wesentlichen um dieselben Gleichungen handelt,

134
00:08:10,843 --> 00:08:13,656
die wir zuvor im Fall von einem Neuron pro Schicht hatten,

135
00:08:13,656 --> 00:08:15,420
sieht es nur etwas komplizierter aus.

136
00:08:15,420 --> 00:08:19,196
Und tatsächlich sieht der abgeleitete Ausdruck der Kettenregel, der beschreibt,

137
00:08:19,196 --> 00:08:22,265
wie empfindlich die Kosten auf ein bestimmtes Gewicht reagieren,

138
00:08:22,265 --> 00:08:23,540
im Wesentlichen gleich aus.

139
00:08:23,540 --> 00:08:28,410
Ich überlasse es Ihnen, innezuhalten und über jeden dieser Begriffe nachzudenken,

140
00:08:28,410 --> 00:08:29,420
wenn Sie möchten.

141
00:08:29,420 --> 00:08:33,731
Was sich hier jedoch ändert, ist die Ableitung der Kosten

142
00:08:33,731 --> 00:08:37,820
in Bezug auf eine der Aktivierungen in der Schicht L-1.

143
00:08:37,820 --> 00:08:40,590
Der Unterschied besteht in diesem Fall darin, dass das Neuron

144
00:08:40,590 --> 00:08:43,540
die Kostenfunktion über mehrere unterschiedliche Wege beeinflusst.

145
00:08:43,540 --> 00:08:50,740
Das heißt, es beeinflusst einerseits AL0, das in der Kostenfunktion eine Rolle spielt,

146
00:08:50,740 --> 00:08:58,188
aber es hat auch Einfluss auf AL1, das ebenfalls in der Kostenfunktion eine Rolle spielt,

147
00:08:58,188 --> 00:09:00,340
und das muss man addieren.

148
00:09:00,340 --> 00:09:03,680
Und das ist so ziemlich alles.

149
00:09:03,680 --> 00:09:07,123
Sobald Sie wissen, wie empfindlich die Kostenfunktion auf die Aktivierungen

150
00:09:07,123 --> 00:09:10,612
in dieser vorletzten Ebene reagiert, können Sie den Vorgang einfach für alle

151
00:09:10,612 --> 00:09:13,920
Gewichtungen und Bias wiederholen, die in diese Ebene eingespeist werden.

152
00:09:13,920 --> 00:09:15,420
Also klopfen Sie sich selbst auf die Schulter!

153
00:09:15,420 --> 00:09:20,705
Wenn das alles Sinn macht, haben Sie jetzt tief in den Kern der Backpropagation geschaut,

154
00:09:20,705 --> 00:09:23,700
dem Arbeitstier hinter dem Lernen neuronaler Netze.

155
00:09:23,700 --> 00:09:27,066
Diese Kettenregelausdrücke liefern Ihnen die Ableitungen,

156
00:09:27,066 --> 00:09:30,782
die jede Komponente im Gradienten bestimmen, was dazu beiträgt,

157
00:09:30,782 --> 00:09:35,020
die Kosten des Netzwerks durch wiederholte Abwärtsschritte zu minimieren.

158
00:09:35,020 --> 00:09:36,251
Wenn Sie sich zurücklehnen und über all das nachdenken, werden Sie feststellen,

159
00:09:36,251 --> 00:09:37,497
dass dies eine Menge Komplexitätsebenen ist, mit denen Sie sich befassen müssen.

160
00:09:37,497 --> 00:09:38,636
Machen Sie sich also keine Sorgen, wenn Ihr Verstand einige Zeit braucht,

161
00:09:38,636 --> 00:09:38,960
um alles zu verdauen.

