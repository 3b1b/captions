1
00:00:00,000 --> 00:00:05,624
Die harte Annahme hier ist, dass Sie Teil 3 gesehen haben, der

2
00:00:05,624 --> 00:00:11,160
eine intuitive Anleitung zum Backpropagation-Algorithmus gibt.

3
00:00:11,160 --> 00:00:14,920
Hier werden wir etwas formeller und tauchen in die relevante Infinitesimalrechnung ein.

4
00:00:14,920 --> 00:00:18,460
Es ist normal, dass dies zumindest ein wenig verwirrend ist, daher gilt das

5
00:00:18,460 --> 00:00:22,000
Mantra, regelmäßig innezuhalten und nachzudenken, hier genauso wie anderswo.

6
00:00:22,000 --> 00:00:24,912
Unser Hauptziel besteht darin, zu zeigen, wie Menschen, die sich mit

7
00:00:24,912 --> 00:00:27,994
maschinellem Lernen befassen, üblicherweise über die Kettenregel aus der

8
00:00:27,994 --> 00:00:31,245
Analysis im Kontext von Netzwerken denken, was ein anderes Gefühl vermittelt

9
00:00:31,245 --> 00:00:34,580
als die Herangehensweise der meisten Einführungskurse in Analysis an das Thema.

10
00:00:34,580 --> 00:00:37,267
Für diejenigen unter Ihnen, die sich mit der relevanten Infinitesimalrechnung

11
00:00:37,267 --> 00:00:39,300
nicht auskennen, habe ich eine ganze Reihe zu diesem Thema.

12
00:00:39,300 --> 00:00:43,077
Beginnen wir mit einem äußerst einfachen Netzwerk,

13
00:00:43,077 --> 00:00:46,780
bei dem jede Schicht ein einzelnes Neuron enthält.

14
00:00:46,780 --> 00:00:51,183
Dieses Netzwerk wird durch drei Gewichte und drei Verzerrungen bestimmt. Unser Ziel

15
00:00:51,183 --> 00:00:55,640
ist es zu verstehen, wie empfindlich die Kostenfunktion auf diese Variablen reagiert.

16
00:00:55,640 --> 00:00:58,522
Auf diese Weise wissen wir, welche Anpassungen dieser Bedingungen

17
00:00:58,522 --> 00:01:01,100
die effizienteste Verringerung der Kostenfunktion bewirken.

18
00:01:01,100 --> 00:01:05,360
Wir konzentrieren uns nur auf die Verbindung zwischen den letzten beiden Neuronen.

19
00:01:05,360 --> 00:01:08,555
Beschriften wir die Aktivierung dieses letzten Neurons mit einem

20
00:01:08,555 --> 00:01:11,800
hochgestellten L, das angibt, in welcher Schicht es sich befindet.

21
00:01:11,800 --> 00:01:16,560
Die Aktivierung des vorherigen Neurons ist also AL-1.

22
00:01:16,560 --> 00:01:18,803
Dabei handelt es sich nicht um Exponenten, sondern nur um

23
00:01:18,803 --> 00:01:21,124
eine Möglichkeit, das, worüber wir sprechen, zu indizieren,

24
00:01:21,124 --> 00:01:23,600
da ich später Indizes für verschiedene Indizes speichern möchte.

25
00:01:23,600 --> 00:01:28,402
Nehmen wir an, dass der Wert, den diese letzte Aktivierung für ein bestimmtes

26
00:01:28,402 --> 00:01:33,020
Trainingsbeispiel haben soll, y ist. Beispielsweise könnte y 0 oder 1 sein.

27
00:01:33,020 --> 00:01:39,040
Die Kosten dieses Netzwerks für ein einzelnes Trainingsbeispiel betragen also AL-y2.

28
00:01:39,040 --> 00:01:46,120
Wir bezeichnen die Kosten dieses einen Trainingsbeispiels als c0.

29
00:01:46,120 --> 00:01:49,830
Zur Erinnerung: Diese letzte Aktivierung wird durch ein Gewicht

30
00:01:49,830 --> 00:01:53,657
bestimmt, das ich wL nenne, multipliziert mit der Aktivierung des

31
00:01:53,657 --> 00:01:57,600
vorherigen Neurons plus einer gewissen Abweichung, die ich bL nenne.

32
00:01:57,600 --> 00:02:01,560
Dann pumpen Sie das durch eine spezielle nichtlineare Funktion wie Sigmoid oder ReLU.

33
00:02:01,560 --> 00:02:04,621
Es wird uns tatsächlich die Arbeit erleichtern, wenn wir dieser

34
00:02:04,621 --> 00:02:07,538
gewichteten Summe einen speziellen Namen geben, z. B. z, mit

35
00:02:07,538 --> 00:02:10,600
demselben hochgestellten Index wie die relevanten Aktivierungen.

36
00:02:10,600 --> 00:02:14,734
Das sind viele Begriffe, und Sie könnten sich das so vorstellen, dass das

37
00:02:14,734 --> 00:02:18,924
Gewicht, die vorherige Aktion und der Bias zusammen verwendet werden, um z

38
00:02:18,924 --> 00:02:22,946
zu berechnen, was uns wiederum die Berechnung von a ermöglicht, was uns

39
00:02:22,946 --> 00:02:27,360
schließlich zusammen mit einer Konstante y ermöglicht Wir berechnen die Kosten.

40
00:02:27,360 --> 00:02:31,813
Und natürlich wird AL-1 durch sein eigenes Gewicht, seine Voreingenommenheit

41
00:02:31,813 --> 00:02:35,920
usw. beeinflusst, aber darauf werden wir uns jetzt nicht konzentrieren.

42
00:02:35,920 --> 00:02:38,120
Das sind doch alles nur Zahlen, oder?

43
00:02:38,120 --> 00:02:40,020
Und es kann schön sein, sich vorzustellen, dass

44
00:02:40,020 --> 00:02:41,960
jede einzelne ihre eigene kleine Zahlenreihe hat.

45
00:02:41,960 --> 00:02:45,890
Unser erstes Ziel besteht darin zu verstehen, wie empfindlich die

46
00:02:45,890 --> 00:02:49,820
Kostenfunktion auf kleine Änderungen unseres Gewichts wL reagiert.

47
00:02:49,820 --> 00:02:55,740
Oder anders ausgedrückt: Was ist die Ableitung von c nach wL?

48
00:02:55,740 --> 00:02:58,669
Wenn Sie diesen del w-Begriff sehen, stellen Sie sich vor, dass

49
00:02:58,669 --> 00:03:01,554
er einen kleinen Anstoß an w bedeutet, etwa eine Änderung um 0.

50
00:03:01,554 --> 00:03:05,073
01, und stellen Sie sich diesen del c-Begriff so vor, dass er

51
00:03:05,073 --> 00:03:08,820
bedeutet, was auch immer der daraus resultierende Kostenschub ist.

52
00:03:08,820 --> 00:03:10,900
Was wir wollen, ist ihr Verhältnis.

53
00:03:10,900 --> 00:03:16,845
Konzeptionell führt dieser kleine Schub für wL zu einem gewissen Schub für zL, was

54
00:03:16,845 --> 00:03:23,220
wiederum einen gewissen Schub für AL verursacht, was sich direkt auf die Kosten auswirkt.

55
00:03:23,220 --> 00:03:28,484
Also unterteilen wir die Sache, indem wir zunächst das Verhältnis einer winzigen Änderung

56
00:03:28,484 --> 00:03:33,340
von zL zu dieser winzigen Änderung w betrachten, also die Ableitung von zL nach wL.

57
00:03:33,340 --> 00:03:37,448
Ebenso berücksichtigen Sie dann das Verhältnis der Änderung von AL zu

58
00:03:37,448 --> 00:03:41,850
der winzigen Änderung von zL, die sie verursacht hat, sowie das Verhältnis

59
00:03:41,850 --> 00:03:45,900
zwischen dem endgültigen Anstoß an c und diesem Zwischenanstoß an AL.

60
00:03:45,900 --> 00:03:51,162
Das hier ist die Kettenregel, bei der die Multiplikation dieser drei

61
00:03:51,162 --> 00:03:57,340
Verhältnisse die Empfindlichkeit von c gegenüber kleinen Änderungen in wL ergibt.

62
00:03:57,340 --> 00:04:00,794
Auf dem Bildschirm sind also gerade viele Symbole zu sehen, und nehmen

63
00:04:00,794 --> 00:04:04,054
Sie sich einen Moment Zeit, um sich zu vergewissern, dass sie alle

64
00:04:04,054 --> 00:04:07,460
klar sind, denn jetzt werden wir die relevanten Ableitungen berechnen.

65
00:04:07,460 --> 00:04:14,220
Die Ableitung von c nach AL ergibt 2AL-y.

66
00:04:14,220 --> 00:04:17,695
Das bedeutet, dass seine Größe proportional zur Differenz zwischen

67
00:04:17,695 --> 00:04:21,170
der Ausgabe des Netzwerks und dem, was wir wollen, ist. Wenn diese

68
00:04:21,170 --> 00:04:24,593
Ausgabe also sehr unterschiedlich ist, können selbst geringfügige

69
00:04:24,593 --> 00:04:28,380
Änderungen einen großen Einfluss auf die endgültige Kostenfunktion haben.

70
00:04:28,380 --> 00:04:32,972
Die Ableitung von AL nach zL ist einfach die Ableitung unserer

71
00:04:32,972 --> 00:04:37,420
Sigmoidfunktion oder der von Ihnen gewählten Nichtlinearität.

72
00:04:37,420 --> 00:04:46,180
Die Ableitung von zL nach wL ergibt AL-1.

73
00:04:46,180 --> 00:04:48,834
Ich weiß nicht, wie es Ihnen geht, aber ich denke, es ist leicht, mit dem

74
00:04:48,834 --> 00:04:51,381
Kopf in den Formeln stecken zu bleiben, ohne sich einen Moment Zeit zu

75
00:04:51,381 --> 00:04:54,180
nehmen, sich zurückzulehnen und sich daran zu erinnern, was sie alle bedeuten.

76
00:04:54,180 --> 00:04:59,038
Im Fall dieser letzten Ableitung hängt der Einfluss des kleinen Gewichtsschubs

77
00:04:59,038 --> 00:05:03,220
auf die letzte Schicht davon ab, wie stark das vorherige Neuron ist.

78
00:05:03,220 --> 00:05:06,299
Denken Sie daran, hier kommt die Idee „Neuronen, die

79
00:05:06,299 --> 00:05:09,320
gemeinsam feuern, miteinander verdrahten“ ins Spiel.

80
00:05:09,320 --> 00:05:12,855
Und all dies ist lediglich die Ableitung der Kosten für

81
00:05:12,855 --> 00:05:16,580
ein bestimmtes einzelnes Trainingsbeispiel in Bezug auf wL.

82
00:05:16,580 --> 00:05:20,682
Da die Vollkostenfunktion die Mittelung aller dieser Kosten über viele

83
00:05:20,682 --> 00:05:24,495
verschiedene Trainingsbeispiele hinweg beinhaltet, erfordert ihre

84
00:05:24,495 --> 00:05:28,540
Ableitung die Mittelung dieses Ausdrucks über alle Trainingsbeispiele.

85
00:05:28,540 --> 00:05:34,283
Das ist natürlich nur eine Komponente des Gradientenvektors, der aus den partiellen

86
00:05:34,283 --> 00:05:40,438
Ableitungen der Kostenfunktion in Bezug auf all diese Gewichte und Verzerrungen aufgebaut

87
00:05:40,438 --> 00:05:40,780
wird.

88
00:05:40,780 --> 00:05:43,643
Aber auch wenn das nur eine der vielen partiellen Ableitungen

89
00:05:43,643 --> 00:05:46,460
ist, die wir brauchen, macht es mehr als 50 % der Arbeit aus.

90
00:05:46,460 --> 00:05:50,300
Die Empfindlichkeit gegenüber der Voreingenommenheit ist beispielsweise nahezu identisch.

91
00:05:50,300 --> 00:05:58,980
Wir müssen nur diesen del z del w-Term durch a del z del b ersetzen.

92
00:05:58,980 --> 00:06:04,700
Und wenn Sie sich die relevante Formel ansehen, ergibt sich für diese Ableitung 1.

93
00:06:04,700 --> 00:06:10,407
Außerdem, und hier kommt die Idee der Rückwärtsausbreitung ins Spiel, können Sie sehen,

94
00:06:10,407 --> 00:06:16,180
wie empfindlich diese Kostenfunktion auf die Aktivierung der vorherigen Schicht reagiert.

95
00:06:16,180 --> 00:06:20,830
Diese anfängliche Ableitung im Kettenregelausdruck, die Empfindlichkeit von

96
00:06:20,830 --> 00:06:25,420
z gegenüber der vorherigen Aktivierung, ergibt sich nämlich als Gewicht wL.

97
00:06:25,420 --> 00:06:29,861
Und auch wenn wir die Aktivierung der vorherigen Ebene nicht direkt beeinflussen

98
00:06:29,861 --> 00:06:34,412
können, ist es dennoch hilfreich, den Überblick zu behalten, denn jetzt können wir

99
00:06:34,412 --> 00:06:38,690
dieselbe Kettenregelidee einfach weiter rückwärts iterieren, um zu sehen, wie

100
00:06:38,690 --> 00:06:43,076
empfindlich die Kostenfunktion darauf reagiert frühere Gewichtungen und frühere

101
00:06:43,076 --> 00:06:43,680
Vorurteile.

102
00:06:43,680 --> 00:06:47,544
Und Sie könnten denken, dass dies ein zu einfaches Beispiel ist, da alle Schichten ein

103
00:06:47,544 --> 00:06:51,320
Neuron haben und die Dinge für ein echtes Netzwerk exponentiell komplizierter werden.

104
00:06:51,320 --> 00:06:55,343
Aber ehrlich gesagt ändert sich nicht so viel, wenn wir den Schichten mehrere Neuronen

105
00:06:55,343 --> 00:06:59,320
geben, es sind eigentlich nur ein paar weitere Indizes, die man im Auge behalten muss.

106
00:06:59,320 --> 00:07:03,724
Anstatt dass die Aktivierung einer bestimmten Schicht einfach AL ist, wird sie auch

107
00:07:03,724 --> 00:07:07,920
einen Index haben, der angibt, um welches Neuron dieser Schicht es sich handelt.

108
00:07:07,920 --> 00:07:11,526
Verwenden wir den Buchstaben k, um die Ebene L-1

109
00:07:11,526 --> 00:07:15,280
zu indizieren, und j, um die Ebene L zu indizieren.

110
00:07:15,280 --> 00:07:19,046
Für die Kosten schauen wir uns erneut an, wie hoch die gewünschte Ausgabe

111
00:07:19,046 --> 00:07:22,710
ist, aber dieses Mal addieren wir die Quadrate der Differenzen zwischen

112
00:07:22,710 --> 00:07:26,120
diesen Aktivierungen der letzten Ebene und der gewünschten Ausgabe.

113
00:07:26,120 --> 00:07:33,280
Das heißt, Sie bilden die Summe über ALj minus yj im Quadrat.

114
00:07:33,280 --> 00:07:37,146
Da es viel mehr Gewichte gibt, muss jedes über ein paar weitere Indizes

115
00:07:37,146 --> 00:07:41,282
verfügen, um den Überblick zu behalten, wo es sich befindet. Nennen wir also

116
00:07:41,282 --> 00:07:45,740
das Gewicht der Kante, die dieses k-te Neuron mit dem j-ten Neuron verbindet, WLjk.

117
00:07:45,740 --> 00:07:48,333
Diese Indizes wirken zunächst vielleicht etwas rückständig, aber

118
00:07:48,333 --> 00:07:51,126
sie stimmen mit der Art und Weise überein, wie Sie die Gewichtsmatrix

119
00:07:51,126 --> 00:07:53,800
indizieren würden, über die ich im Video zu Teil 1 gesprochen habe.

120
00:07:53,800 --> 00:07:57,391
Nach wie vor ist es immer noch schön, der relevanten gewichteten Summe

121
00:07:57,391 --> 00:08:01,034
einen Namen zu geben, z. B. z, sodass die Aktivierung der letzten Ebene

122
00:08:01,034 --> 00:08:04,980
nur Ihre spezielle Funktion ist, z. B. das Sigmoid, das auf z angewendet wird.

123
00:08:04,980 --> 00:08:08,460
Sie können sehen, was ich meine, wenn es sich bei all diesen Gleichungen

124
00:08:08,460 --> 00:08:11,892
im Wesentlichen um dieselben Gleichungen handelt, die wir zuvor im Fall

125
00:08:11,892 --> 00:08:15,420
von einem Neuron pro Schicht hatten, sieht es nur etwas komplizierter aus.

126
00:08:15,420 --> 00:08:19,385
Und tatsächlich sieht der abgeleitete Ausdruck der Kettenregel, der beschreibt, wie

127
00:08:19,385 --> 00:08:23,540
empfindlich die Kosten auf ein bestimmtes Gewicht reagieren, im Wesentlichen gleich aus.

128
00:08:23,540 --> 00:08:26,272
Ich überlasse es Ihnen, innezuhalten und über

129
00:08:26,272 --> 00:08:29,420
jeden dieser Begriffe nachzudenken, wenn Sie möchten.

130
00:08:29,420 --> 00:08:33,731
Was sich hier jedoch ändert, ist die Ableitung der Kosten

131
00:08:33,731 --> 00:08:37,820
in Bezug auf eine der Aktivierungen in der Schicht L-1.

132
00:08:37,820 --> 00:08:40,590
Der Unterschied besteht in diesem Fall darin, dass das Neuron

133
00:08:40,590 --> 00:08:43,540
die Kostenfunktion über mehrere unterschiedliche Wege beeinflusst.

134
00:08:43,540 --> 00:08:49,167
Das heißt, es beeinflusst einerseits AL0, das in der Kostenfunktion

135
00:08:49,167 --> 00:08:54,795
eine Rolle spielt, aber es hat auch Einfluss auf AL1, das ebenfalls

136
00:08:54,795 --> 00:09:00,340
in der Kostenfunktion eine Rolle spielt, und das muss man addieren.

137
00:09:00,340 --> 00:09:03,680
Und das ist so ziemlich alles.

138
00:09:03,680 --> 00:09:07,123
Sobald Sie wissen, wie empfindlich die Kostenfunktion auf die Aktivierungen

139
00:09:07,123 --> 00:09:10,612
in dieser vorletzten Ebene reagiert, können Sie den Vorgang einfach für alle

140
00:09:10,612 --> 00:09:13,920
Gewichtungen und Bias wiederholen, die in diese Ebene eingespeist werden.

141
00:09:13,920 --> 00:09:15,420
Also klopfen Sie sich selbst auf die Schulter!

142
00:09:15,420 --> 00:09:19,178
Wenn das alles Sinn macht, haben Sie jetzt tief in den Kern der

143
00:09:19,178 --> 00:09:23,700
Backpropagation geschaut, dem Arbeitstier hinter dem Lernen neuronaler Netze.

144
00:09:23,700 --> 00:09:27,589
Diese Kettenregelausdrücke liefern Ihnen die Ableitungen, die jede

145
00:09:27,589 --> 00:09:31,420
Komponente im Gradienten bestimmen, was dazu beiträgt, die Kosten

146
00:09:31,420 --> 00:09:35,020
des Netzwerks durch wiederholte Abwärtsschritte zu minimieren.

147
00:09:35,020 --> 00:09:36,328
Wenn Sie sich zurücklehnen und über all das nachdenken, werden Sie feststellen, dass

148
00:09:36,328 --> 00:09:37,605
dies eine Menge Komplexitätsebenen ist, mit denen Sie sich befassen müssen. Machen

149
00:09:37,605 --> 00:09:38,960
Sie sich also keine Sorgen, wenn Ihr Verstand einige Zeit braucht, um alles zu verdauen.

