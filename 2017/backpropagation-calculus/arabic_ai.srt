1
00:00:00,000 --> 00:00:08,420
الافتراض الصعب هنا هو أنك شاهدت الجزء الثالث،

2
00:00:08,420 --> 00:00:11,160
والذي يقدم شرحًا بديهيًا لخوارزمية الانتشار العكسي.

3
00:00:11,160 --> 00:00:14,920
هنا نحصل على المزيد من الرسمية ونغوص في حسابات التفاضل والتكامل ذات الصلة.

4
00:00:14,920 --> 00:00:18,560
من الطبيعي أن يكون هذا مربكًا بعض الشيء على الأقل، لذا فإن شعار التوقف

5
00:00:18,560 --> 00:00:22,000
والتأمل بشكل منتظم ينطبق بالتأكيد هنا بقدر ما ينطبق في أي مكان آخر.

6
00:00:22,000 --> 00:00:26,620
هدفنا الرئيسي هو إظهار كيف يفكر الأشخاص في التعلم الآلي بشكل شائع حول قاعدة

7
00:00:26,620 --> 00:00:31,900
السلسلة من حساب التفاضل والتكامل في سياق الشبكات، والتي لها إحساس مختلف عن

8
00:00:31,900 --> 00:00:34,580
الطريقة التي تتعامل بها معظم دورات حساب التفاضل والتكامل التمهيدية مع هذا الموضوع.

9
00:00:34,580 --> 00:00:38,300
لأولئك منكم الذين لا يشعرون بالارتياح تجاه حسابات التفاضل

10
00:00:38,300 --> 00:00:39,300
والتكامل ذات الصلة، لدي سلسلة كاملة حول هذا الموضوع.

11
00:00:39,300 --> 00:00:44,840
لنبدأ بشبكة بسيطة للغاية، حيث تحتوي

12
00:00:44,840 --> 00:00:46,780
كل طبقة على خلية عصبية واحدة.

13
00:00:46,780 --> 00:00:51,880
يتم تحديد هذه الشبكة بثلاثة أوزان وثلاثة انحيازات، وهدفنا

14
00:00:51,880 --> 00:00:55,640
هو فهم مدى حساسية دالة التكلفة لهذه المتغيرات.

15
00:00:55,640 --> 00:00:59,780
وبهذه الطريقة نعرف أي تعديلات على هذه الشروط

16
00:00:59,780 --> 00:01:01,100
ستتسبب في التخفيض الأكثر كفاءة لوظيفة التكلفة.

17
00:01:01,100 --> 00:01:05,360
سنركز فقط على الاتصال بين آخر خليتين عصبيتين.

18
00:01:05,360 --> 00:01:10,400
دعونا نسمي تنشيط تلك الخلية العصبية الأخيرة بالحرف

19
00:01:10,400 --> 00:01:11,800
L المرتفع، للإشارة إلى الطبقة التي توجد فيها.

20
00:01:11,800 --> 00:01:16,560
وبالتالي فإن تنشيط الخلية العصبية السابقة هو AL-1.

21
00:01:16,560 --> 00:01:20,120
هذه ليست أسسًا، إنها مجرد وسيلة لفهرسة ما نتحدث

22
00:01:20,120 --> 00:01:23,120
عنه، حيث أريد حفظ اشتراكات لمؤشرات مختلفة لاحقًا.

23
00:01:23,600 --> 00:01:28,880
لنفترض أن القيمة التي نريد أن يكون هذا التنشيط الأخير لمثال تدريب

24
00:01:28,880 --> 00:01:33,020
معين هي y، على سبيل المثال، قد تكون y 0 أو 1.

25
00:01:33,020 --> 00:01:39,040
وبالتالي فإن تكلفة هذه الشبكة لمثال تدريبي واحد هي AL-y2.

26
00:01:39,040 --> 00:01:46,120
سنشير إلى تكلفة هذا المثال التدريبي بالرمز c0.

27
00:01:46,120 --> 00:01:51,920
للتذكير، يتم تحديد هذا التنشيط الأخير من خلال الوزن، والذي سأسميه wL، مضروبًا

28
00:01:51,920 --> 00:01:57,600
في تنشيط الخلية العصبية السابقة بالإضافة إلى بعض التحيز، والذي سأسميه bL.

29
00:01:57,600 --> 00:02:01,560
ثم تقوم بضخ ذلك من خلال بعض الوظائف غير الخطية الخاصة مثل السيني أو ReLU.

30
00:02:01,560 --> 00:02:05,400
في الواقع، سيكون الأمر أسهل بالنسبة لنا إذا أعطينا اسمًا خاصًا لهذا

31
00:02:05,400 --> 00:02:10,600
المجموع المرجح، مثل z، بنفس الحرف المرتفع مثل عمليات التنشيط ذات الصلة.

32
00:02:10,600 --> 00:02:15,320
هذا كثير من المصطلحات، والطريقة التي يمكنك تصورها هي أن الوزن والإجراء السابق

33
00:02:15,320 --> 00:02:21,800
والتحيز معًا يُستخدم لحساب z، والذي بدوره يتيح لنا حساب a، والذي

34
00:02:21,800 --> 00:02:27,360
أخيرًا، جنبًا إلى جنب مع ثابت y، يتيح لنا لنا حساب التكلفة.

35
00:02:27,360 --> 00:02:33,440
وبطبيعة الحال، يتأثر AL-1 بوزنه وتحيزه وما إلى

36
00:02:33,440 --> 00:02:35,920
ذلك، لكننا لن نركز على ذلك الآن.

37
00:02:35,920 --> 00:02:38,120
كل هذه مجرد أرقام، أليس كذلك؟

38
00:02:38,120 --> 00:02:41,960
وقد يكون من الجيد التفكير في أن كل واحدة لها خط أعداد صغير خاص بها.

39
00:02:41,960 --> 00:02:47,480
هدفنا الأول هو فهم مدى حساسية

40
00:02:47,480 --> 00:02:49,820
دالة التكلفة للتغيرات الصغيرة في وزننا.

41
00:02:49,820 --> 00:02:55,740
أو قم بالعبارة بشكل مختلف، ما هو مشتق c بالنسبة لـ wL؟

42
00:02:55,740 --> 00:03:01,220
عندما ترى هذا المصطلح del w، فكر في أنه يعني دفعة صغيرة إلى w، مثل التغيير بمقدار 0.

43
00:03:01,220 --> 00:03:08,820
01، وفكر في هذا المصطلح على أنه يعني مهما كانت الدفعة الناتجة إلى التكلفة.

44
00:03:08,820 --> 00:03:10,900
ما نريده هو نسبتهم.

45
00:03:10,900 --> 00:03:17,740
من الناحية النظرية، تؤدي هذه الدفعة الصغيرة إلى wL إلى بعض الدفع إلى zL،

46
00:03:17,740 --> 00:03:23,220
والذي يؤدي بدوره إلى بعض الدفع إلى AL، مما يؤثر بشكل مباشر على التكلفة.

47
00:03:23,220 --> 00:03:28,020
لذلك نقوم بتقسيم الأمور من خلال النظر أولاً إلى نسبة التغير الطفيف في

48
00:03:28,020 --> 00:03:33,340
zL إلى هذا التغير الطفيف w، أي مشتقة zL بالنسبة إلى wL.

49
00:03:33,340 --> 00:03:38,820
وبالمثل، عليك أن تأخذ في الاعتبار نسبة التغيير إلى AL إلى

50
00:03:38,820 --> 00:03:43,900
التغيير الطفيف في zL الذي تسبب في ذلك، بالإضافة إلى النسبة

51
00:03:43,900 --> 00:03:45,900
بين الدفعة النهائية إلى c وهذه الدفعة الوسيطة إلى AL.

52
00:03:45,900 --> 00:03:51,880
هذه هي قاعدة السلسلة، حيث أن ضرب هذه النسب

53
00:03:51,880 --> 00:03:57,340
الثلاث يعطينا حساسية c للتغيرات الصغيرة في wL.

54
00:03:57,340 --> 00:04:01,620
إذن على الشاشة الآن، هناك الكثير من الرموز، وتوقف لحظة للتأكد

55
00:04:01,620 --> 00:04:07,460
من أنها واضحة جميعًا، لأننا الآن سنقوم بحساب المشتقات ذات الصلة.

56
00:04:07,460 --> 00:04:14,220
مشتق c بالنسبة لـ AL يصبح 2AL-y.

57
00:04:14,220 --> 00:04:19,300
وهذا يعني أن حجمها يتناسب مع الفرق بين مخرجات الشبكة والشيء الذي نريدها

58
00:04:19,300 --> 00:04:24,480
أن تكون عليه، لذلك إذا كان هذا الناتج مختلفًا تمامًا، فحتى التغييرات

59
00:04:24,480 --> 00:04:28,380
الطفيفة من شأنها أن يكون لها تأثير كبير على دالة التكلفة النهائية.

60
00:04:28,380 --> 00:04:33,860
مشتق AL بالنسبة إلى zL هو مجرد مشتق للدالة

61
00:04:33,860 --> 00:04:37,420
السينية، أو أي دالة غير خطية تختار استخدامها.

62
00:04:37,420 --> 00:04:46,180
مشتق zL بالنسبة إلى wL يصبح AL-1.

63
00:04:46,180 --> 00:04:49,460
لا أعرف عنك، ولكني أعتقد أنه من السهل أن تتعثر في

64
00:04:49,460 --> 00:04:54,180
الصيغ دون أن تأخذ لحظة لتجلس وتذكّر نفسك بما تعنيه جميعها.

65
00:04:54,180 --> 00:04:58,860
في حالة هذا المشتق الأخير، فإن مقدار تأثير الدفعة الصغيرة للوزن

66
00:04:58,860 --> 00:05:03,220
على الطبقة الأخيرة يعتمد على مدى قوة الخلية العصبية السابقة.

67
00:05:03,220 --> 00:05:09,320
تذكر، هذا هو المكان الذي تأتي فيه فكرة الخلايا العصبية التي تشتعل معًا.

68
00:05:09,320 --> 00:05:14,840
وكل هذا هو مشتق فيما يتعلق بـ wL

69
00:05:14,840 --> 00:05:16,580
فقط من تكلفة مثال تدريبي واحد محدد.

70
00:05:16,580 --> 00:05:20,940
بما أن دالة التكلفة الكاملة تتضمن حساب متوسط كل تلك

71
00:05:20,940 --> 00:05:27,300
التكاليف معًا عبر العديد من أمثلة التدريب المختلفة، فإن مشتقها

72
00:05:27,300 --> 00:05:28,540
يتطلب حساب متوسط هذا التعبير على جميع أمثلة التدريب.

73
00:05:28,540 --> 00:05:33,860
بالطبع، هذا مجرد عنصر واحد من متجه التدرج، والذي تم إنشاؤه

74
00:05:33,860 --> 00:05:40,780
من المشتقات الجزئية لدالة التكلفة فيما يتعلق بكل تلك الأوزان والتحيزات.

75
00:05:40,780 --> 00:05:44,340
لكن على الرغم من أن هذه مجرد واحدة من المشتقات الجزئية

76
00:05:44,340 --> 00:05:46,460
العديدة التي نحتاجها، إلا أنها تمثل أكثر من 50% من العمل.

77
00:05:46,460 --> 00:05:50,300
فالحساسية تجاه التحيز، على سبيل المثال، تكاد تكون متطابقة.

78
00:05:50,300 --> 00:05:58,980
نحتاج فقط إلى تغيير مصطلح del z del w هذا إلى del z del b.

79
00:05:58,980 --> 00:06:04,700
وإذا نظرت إلى الصيغة ذات الصلة، فستجد أن هذا المشتق يساوي 1.

80
00:06:04,700 --> 00:06:11,700
أيضًا، وهذا هو المكان الذي تأتي فيه فكرة الانتشار للخلف،

81
00:06:11,700 --> 00:06:16,180
يمكنك معرفة مدى حساسية دالة التكلفة هذه لتنشيط الطبقة السابقة.

82
00:06:16,180 --> 00:06:21,380
أي أن هذا المشتق الأولي في تعبير قاعدة

83
00:06:21,380 --> 00:06:25,420
السلسلة، حساسية z للتنشيط السابق، يصبح الوزن wL.

84
00:06:25,420 --> 00:06:30,100
ومرة أخرى، على الرغم من أننا لن نكون قادرين على التأثير

85
00:06:30,100 --> 00:06:35,280
بشكل مباشر على تنشيط الطبقة السابقة، فمن المفيد تتبع ذلك،

86
00:06:35,280 --> 00:06:40,780
لأنه يمكننا الآن الاستمرار في تكرار فكرة قاعدة السلسلة نفسها إلى

87
00:06:40,780 --> 00:06:43,680
الوراء لنرى مدى حساسية وظيفة التكلفة الأوزان السابقة والتحيزات السابقة.

88
00:06:43,680 --> 00:06:47,940
وقد تعتقد أن هذا مثال بسيط للغاية، نظرًا لأن جميع الطبقات تحتوي على

89
00:06:47,940 --> 00:06:51,320
خلية عصبية واحدة، وستصبح الأمور أكثر تعقيدًا بشكل كبير بالنسبة للشبكة الحقيقية.

90
00:06:51,320 --> 00:06:56,560
لكن بصراحة، ليس هناك الكثير من التغييرات عندما نعطي الطبقات خلايا عصبية

91
00:06:56,560 --> 00:06:59,320
متعددة، إنها في الحقيقة مجرد عدد قليل من المؤشرات التي يجب تتبعها.

92
00:06:59,320 --> 00:07:03,580
بدلًا من تنشيط طبقة معينة لتكون AL، سيكون لها أيضًا

93
00:07:03,580 --> 00:07:07,920
رمز منخفض يشير إلى أي خلية عصبية تنتمي لتلك الطبقة.

94
00:07:07,920 --> 00:07:15,280
لنستخدم الحرف k لفهرسة الطبقة L-1، وj لفهرسة الطبقة L.

95
00:07:15,280 --> 00:07:20,720
بالنسبة للتكلفة، ننظر مرة أخرى إلى الناتج المطلوب، لكن هذه المرة

96
00:07:20,720 --> 00:07:26,120
نضيف مربعات الاختلافات بين عمليات تنشيط الطبقة الأخيرة والمخرج المطلوب.

97
00:07:26,120 --> 00:07:33,280
وهذا يعني أنك تأخذ مجموعًا على ALj ناقص yj تربيع.

98
00:07:33,280 --> 00:07:36,500
نظرًا لوجود الكثير من الأوزان، يجب أن يكون لكل واحد

99
00:07:36,500 --> 00:07:41,380
مؤشرين إضافيين لتتبع مكانه، لذلك دعونا نسمي وزن الحافة التي

100
00:07:41,380 --> 00:07:45,740
تربط هذه الخلية العصبية k بالخلية العصبية j، WLjk.

101
00:07:45,740 --> 00:07:49,820
قد تبدو هذه المؤشرات متخلفة قليلاً في البداية، ولكنها تتوافق مع كيفية

102
00:07:49,820 --> 00:07:53,800
فهرسة مصفوفة الوزن التي تحدثت عنها في الجزء الأول من الفيديو.

103
00:07:53,800 --> 00:07:57,660
كما كان من قبل، لا يزال من الجيد إعطاء اسم

104
00:07:57,660 --> 00:08:03,540
للمجموع المرجح ذي الصلة، مثل z، بحيث يكون تنشيط الطبقة

105
00:08:03,540 --> 00:08:04,980
الأخيرة مجرد وظيفتك الخاصة، مثل السيني، المطبق على z.

106
00:08:04,980 --> 00:08:09,100
يمكنك أن ترى ما أعنيه، حيث أن كل هذه هي في الأساس نفس المعادلات التي كانت لدينا

107
00:08:09,100 --> 00:08:15,420
من قبل في حالة خلية عصبية واحدة لكل طبقة، الأمر فقط أنها تبدو أكثر تعقيدًا قليلاً.

108
00:08:15,420 --> 00:08:20,620
وبالفعل، فإن التعبير المشتق لقاعدة السلسلة الذي يصف مدى

109
00:08:20,620 --> 00:08:23,540
حساسية التكلفة لوزن معين يبدو كما هو في الأساس.

110
00:08:23,540 --> 00:08:29,420
سأترك الأمر لك للتوقف والتفكير في كل مصطلح من هذه المصطلحات إذا كنت تريد ذلك.

111
00:08:29,420 --> 00:08:34,900
لكن ما يتغير هنا هو مشتق التكلفة فيما

112
00:08:34,900 --> 00:08:37,820
يتعلق بأحد عمليات التنشيط في الطبقة L-1.

113
00:08:37,820 --> 00:08:42,000
في هذه الحالة، الفرق هو أن الخلية العصبية تؤثر

114
00:08:42,000 --> 00:08:43,540
على دالة التكلفة من خلال مسارات مختلفة متعددة.

115
00:08:43,540 --> 00:08:51,200
وهذا يعني، من ناحية، أنها تؤثر على AL0، التي تلعب

116
00:08:51,200 --> 00:08:56,460
دورًا في دالة التكلفة، ولكن لها أيضًا تأثير على AL1،

117
00:08:56,460 --> 00:09:00,340
والتي تلعب أيضًا دورًا في دالة التكلفة، ويجب عليك جمعها.

118
00:09:00,340 --> 00:09:03,680
وهذا، حسنًا، هذا كل شيء تقريبًا.

119
00:09:03,680 --> 00:09:08,240
بمجرد أن تعرف مدى حساسية دالة التكلفة لعمليات التنشيط

120
00:09:08,240 --> 00:09:12,520
في هذه الطبقة من الثانية إلى الأخيرة، يمكنك فقط

121
00:09:12,520 --> 00:09:13,920
تكرار العملية لجميع الأوزان والتحيزات التي تغذي تلك الطبقة.

122
00:09:13,920 --> 00:09:15,420
لذا ربت على ظهرك!

123
00:09:15,420 --> 00:09:20,480
إذا كان كل هذا منطقيًا، فقد نظرت الآن بعمق في قلب

124
00:09:20,480 --> 00:09:23,700
الانتشار العكسي، وهو العمود الفقري وراء كيفية تعلم الشبكات العصبية.

125
00:09:23,700 --> 00:09:27,960
تمنحك تعبيرات قواعد السلسلة هذه المشتقات التي تحدد كل مكون في التدرج

126
00:09:27,960 --> 00:09:35,020
الذي يساعد على تقليل تكلفة الشبكة عن طريق النزول بشكل متكرر.

127
00:09:35,020 --> 00:09:38,960
إذا جلست وفكرت في كل ذلك، فستجد أن هناك الكثير من طبقات التعقيد التي

128
00:09:38,960 --> 00:09:42,840
يحيط بها عقلك، لذا لا تقلق إذا استغرق عقلك وقتًا لاستيعاب كل ذلك.

