[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "Presupunem că ați vizionat partea a treia, care oferă o prezentare intuitivă a algoritmului de backpropagation.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Aici devenim un pic mai formali și intrăm în calculele relevante.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "Este normal ca acest lucru să fie cel puțin puțin puțin derutant, așa că mantra de a lua o pauză și de a reflecta în mod regulat se aplică cu siguranță la fel de mult aici ca oriunde altundeva.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "Scopul nostru principal este de a arăta cum se gândesc oamenii din domeniul învățării automate la regula lanțului din calcul în contextul rețelelor, ceea ce are un aspect diferit de modul în care majoritatea cursurilor introductive de calcul abordează subiectul.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Pentru aceia dintre voi care nu se simt confortabil cu calculele relevante, am o serie întreagă pe această temă.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Să începem cu o rețea extrem de simplă, una în care fiecare strat are un singur neuron.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Această rețea este determinată de trei ponderi și trei polarizări, iar scopul nostru este de a înțelege cât de sensibilă este funcția de cost la aceste variabile.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way, we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "În acest fel, știm ce ajustări ale acestor termeni vor determina cea mai eficientă scădere a funcției de cost.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "And we're just going to focus on the connection between the last two neurons.",
  "translatedText": "Și ne vom concentra doar pe conexiunea dintre ultimii doi neuroni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in, so the activation of the previous neuron is Al-1.",
  "translatedText": "Să etichetăm activarea ultimului neuron cu un superscript L, indicând stratul în care se află, astfel încât activarea neuronului anterior să fie Al-1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 65.98,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Aceștia nu sunt exponenți, ci doar un mod de a indexa ceea ce vorbim, deoarece vreau să păstrez subscriptele pentru diferiți indici mai târziu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Să spunem că valoarea pe care dorim să o aibă această ultimă activare pentru un anumit exemplu de formare este y, de exemplu, y poate fi 0 sau 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is Al-y2.",
  "translatedText": "Astfel, costul acestei rețele pentru un singur exemplu de instruire este Al-y2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Vom numi costul acelui exemplu de formare ca fiind c0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neuron's activation plus some bias, which I'll call BL.",
  "translatedText": "Ca o reamintire, această ultimă activare este determinată de o pondere, pe care o voi numi WL, înmulțită cu activarea neuronului anterior plus o anumită distorsiune, pe care o voi numi BL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "Și apoi se pompează prin intermediul unei funcții neliniare speciale, cum ar fi Sigmoid sau ReLU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "De fapt, ne va fi mai ușor dacă vom da un nume special acestei sume ponderate, cum ar fi z, cu același indice ca și activările relevante.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action and the bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "Este vorba de o mulțime de termeni, iar un mod în care ați putea conceptualiza acest lucru este că ponderea, acțiunea anterioară și tendința sunt folosite împreună pentru a calcula z, care la rândul său ne permite să calculăm a, care, în final, împreună cu o constantă y, ne permite să calculăm costul.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course Al-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "Și, desigur, Al-1 este influențat de propria sa greutate și de prejudecăți și altele, dar nu ne vom concentra asupra acestui aspect acum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Toate acestea sunt doar numere, nu-i așa?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "Și poate fi plăcut să ne gândim că fiecare dintre ele are propria linie numerică.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight WL.",
  "translatedText": "Primul nostru obiectiv este să înțelegem cât de sensibilă este funcția de cost la mici modificări ale ponderii noastre WL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to WL?",
  "translatedText": "Sau, altfel spus, care este derivata lui c în raport cu WL?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del W term, think of it as meaning some tiny nudge to W, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Atunci când vedeți acest termen del W, gândiți-vă la el ca la o mică modificare a lui W, cum ar fi o schimbare de 0,01, iar termenul del c ca la o modificare a costului care rezultă.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "Ceea ce dorim este raportul lor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Din punct de vedere conceptual, acest mic impuls asupra lui WL determină un impuls asupra lui ZL, care, la rândul său, determină un impuls asupra lui AL, ceea ce influențează direct costul.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is, the derivative of ZL with respect to WL.",
  "translatedText": "Astfel, vom analiza mai întâi raportul dintre o mică schimbare a lui ZL și această mică schimbare W, adică derivata lui ZL în raport cu WL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "De asemenea, trebuie să luați în considerare raportul dintre modificarea lui AL și micuța modificare a lui ZL care a cauzat-o, precum și raportul dintre modificarea finală a lui c și această modificare intermediară a lui AL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of c to small changes in WL.",
  "translatedText": "Aceasta de aici este regula lanțului, unde înmulțirea acestor trei rapoarte ne dă sensibilitatea lui c la mici schimbări în WL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "Pe ecran sunt o mulțime de simboluri și asigurați-vă că este clar ce reprezintă toate acestea, pentru că acum vom calcula derivatele relevante.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "Derivata lui c în raport cu AL este 2AL-y.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "Notice this means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Observați că acest lucru înseamnă că dimensiunea sa este proporțională cu diferența dintre rezultatul rețelei și ceea ce dorim să fie, astfel încât, dacă acel rezultat a fost foarte diferit, chiar și schimbările ușoare pot avea un impact mare asupra funcției de cost finale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "Derivata lui AL în raport cu ZL este doar derivata funcției noastre sigmoide, sau orice altă neliniaritate pe care alegeți să o utilizați.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "And the derivative of ZL with respect to WL comes out to be AL-1.",
  "translatedText": "Iar derivata lui ZL în raport cu WL rezultă a fi AL-1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all mean.",
  "translatedText": "Nu știu ce părere aveți voi, dar eu cred că este ușor să te blochezi cu capul în formule, fără să te gândești puțin și să-ți amintești ce înseamnă toate acestea.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "În cazul acestei ultime derivate, valoarea în care micul impuls dat ponderii influențează ultimul strat depinde de cât de puternic este neuronii anteriori.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "Nu uitați, aici intervine ideea neuronilor care trag împreună, se conectează împreună.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to WL only of the cost for a specific single training example.",
  "translatedText": "Și toate acestea reprezintă derivația în raport cu WL doar a costului pentru un singur exemplu specific de formare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Deoarece funcția de cost complet implică calcularea mediei tuturor acestor costuri pentru mai multe exemple de formare diferite, derivata sa necesită calcularea mediei acestei expresii pentru toate exemplele de formare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "And of course, that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "Și, desigur, aceasta este doar o componentă a vectorului de gradient, care la rândul său este construit din derivatele parțiale ale funcției de cost în raport cu toate aceste ponderi și polarizări.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "Dar chiar dacă aceasta este doar una dintre multele derivate parțiale de care avem nevoie, reprezintă mai mult de 50% din muncă.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "De exemplu, sensibilitatea față de polarizare este aproape identică.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Trebuie doar să schimbăm acest termen del z del w cu un del z del b.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "Și dacă vă uitați la formula relevantă, derivata este 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "De asemenea, și aici intervine ideea de propagare inversă, puteți vedea cât de sensibilă este această funcție de cost la activarea stratului anterior.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight WL.",
  "translatedText": "Mai exact, această derivată inițială din expresia regulii în lanț, sensibilitatea lui z la activarea anterioară, se dovedește a fi ponderea WL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "Și, din nou, chiar dacă nu vom putea influența direct activarea stratului anterior, este util să ținem evidența, deoarece acum putem continua să iterăm aceeași idee de regulă în lanț în sens invers pentru a vedea cât de sensibilă este funcția de cost la ponderile anterioare și la polarizările anterioare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "S-ar putea să credeți că acesta este un exemplu prea simplu, deoarece toate straturile au un singur neuron, iar lucrurile vor deveni exponențial mai complicate pentru o rețea reală.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "Dar, sincer, nu se schimbă prea mult atunci când oferim mai mulți neuroni straturilor, ci doar câțiva indici în plus de care trebuie să ținem cont.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "În loc ca activarea unui anumit strat să fie pur și simplu AL, va avea și un subscript care va indica ce neuron din acel strat este.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Să folosim litera k pentru a indexa stratul L-1, iar j pentru a indexa stratul L.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "Pentru cost, ne uităm din nou la rezultatul dorit, dar de data aceasta adunăm pătratele diferențelor dintre activările ultimului strat și rezultatul dorit.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus Yj squared.",
  "translatedText": "Adică, se face o sumă peste ALj minus Yj la pătrat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Deoarece există mult mai multe greutăți, fiecare dintre ele trebuie să aibă câțiva indici în plus pentru a ține evidența locului în care se află, astfel încât să numim greutatea muchiei care leagă acest al k-lea neuron de al j-lea neuron, WLjk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "Acești indici ar putea părea un pic pe dos la început, dar se aliniază cu modul în care ați indexa matricea de greutate despre care am vorbit în partea 1 a videoclipului.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "La fel ca înainte, este bine să dați un nume sumei ponderate relevante, cum ar fi z, astfel încât activarea ultimului strat să fie doar funcția dvs. specială, cum ar fi sigmoidul, aplicată la z.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Puteți vedea la ce mă refer, unde toate acestea sunt, în esență, aceleași ecuații pe care le aveam înainte în cazul unui singur neuron pe strat, doar că pare puțin mai complicat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain-ruled derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "Și, într-adevăr, expresia derivată în lanț care descrie cât de sensibil este costul la o anumită greutate arată în esență la fel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Vă las pe voi să vă gândiți la fiecare dintre acești termeni, dacă doriți.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "Ceea ce se schimbă aici, totuși, este derivata costului în raport cu una dintre activările din stratul L-1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "În acest caz, diferența constă în faptul că neuronii influențează funcția de cost prin mai multe căi diferite.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "Adică, pe de o parte, influențează AL0, care joacă un rol în funcția de cost, dar are, de asemenea, o influență asupra AL1, care joacă, de asemenea, un rol în funcția de cost, și trebuie să le adunăm.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "Și asta, ei bine, cam asta e tot.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Odată ce știți cât de sensibilă este funcția de cost la activările din penultimul strat, puteți repeta procesul pentru toate ponderile și polarizările care alimentează acel strat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "Așa că bate-te pe spate!",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Dacă toate acestea au sens, înseamnă că ați pătruns adânc în inima backpropagation, motorul de lucru din spatele modului de învățare a rețelelor neuronale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Aceste expresii ale regulilor în lanț vă oferă derivatele care determină fiecare componentă a gradientului care ajută la minimizarea costului rețelei prin coborârea repetată a pantei.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Dacă stai și te gândești la toate acestea, vei avea de înțeles că sunt multe straturi de complexitate, așa că nu-ți face griji dacă îți ia ceva timp să le asimilezi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]