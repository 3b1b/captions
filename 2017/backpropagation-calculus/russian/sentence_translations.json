[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "Трудное предположение здесь состоит в том, что вы просмотрели часть 3, в которой дается интуитивное описание алгоритма обратного распространения ошибки.",
  "from_community_srt": "Трудно предположить, что вы смотрели часть 3, дать интуитивно понятное прохождение алгоритма обратного распространения.",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Здесь мы станем немного более формальными и углубимся в соответствующие вычисления.",
  "from_community_srt": "Здесь мы немного более формальны и углубимся в соответствующее исчисление.",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "Это нормально, хотя бы немного сбивать с толку, поэтому мантра о регулярной паузе и размышлении, безусловно, применима здесь так же, как и везде.",
  "from_community_srt": "Это нормально, что это немного сбивает с толку, поэтому мантра для регулярной паузы и размышления, безусловно, применима и здесь, и везде.",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "Наша главная цель — показать, как люди, занимающиеся машинным обучением, обычно думают о цепном правиле исчисления в контексте сетей, которое отличается от того, как большинство вводных курсов исчисления подходят к этому предмету.",
  "from_community_srt": "Наша главная цель - показать, как люди в машинном обучении обычно думают о правиле цепи из исчисления в контексте сетей, который по-разному понимает, насколько большинство вводных курсов по исчислению подходят к предмету.",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Для тех из вас, кому некомфортны соответствующие вычисления, у меня есть целая серия статей на эту тему.",
  "from_community_srt": "Для тех из вас, кого не устраивает соответствующее исчисление, У меня есть целая серия на эту тему.",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Начнем с чрезвычайно простой сети, в которой каждый слой имеет один нейрон.",
  "from_community_srt": "Давайте начнем с очень простой сети, один, где каждый слой имеет один нейрон в нем.",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Эта сеть определяется тремя весами и тремя смещениями, и наша цель — понять, насколько чувствительна функция стоимости к этим переменным.",
  "from_community_srt": "Таким образом, эта конкретная сеть определяется 3 весами и 3 поправками, и наша цель - понять, насколько чувствительна функция стоимости к этим переменным.",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way, we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "Таким образом, мы знаем, какие корректировки этих условий приведут к наиболее эффективному уменьшению функции затрат.",
  "from_community_srt": "Таким образом, мы знаем, какие корректировки этих условий собирается вызвать наиболее эффективное снижение функции стоимости.",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "And we're just going to focus on the connection between the last two neurons.",
  "translatedText": "И мы просто сосредоточимся на связи между двумя последними нейронами.",
  "from_community_srt": "И мы просто сосредоточены на связи между двумя последними нейронами.",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in, so the activation of the previous neuron is Al-1.",
  "translatedText": "Давайте отметим активацию этого последнего нейрона верхним индексом L, указывающим, в каком слое он находится, поэтому активация предыдущего нейрона — это Al-1.",
  "from_community_srt": "Давайте обозначим активацию этого последнего нейрона \"a\" с верхним индексом \"L\", указывая, в каком слое он находится, Таким образом, активация этого предыдущего нейрона является \"a^(L-1)\" Здесь нет показателей,",
  "n_reviews": 0,
  "start": 65.98,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Это не показатели степени, это просто способ индексации того, о чем мы говорим, поскольку позже я хочу сохранить индексы для разных индексов.",
  "from_community_srt": "это просто способ индексации того, о чем мы говорим, так как я хочу сохранить подписки для различных индексов позже.",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Предположим, что значение, которое мы хотим, чтобы эта последняя активация была для данного обучающего примера, равно y, например, y может быть 0 или 1.",
  "from_community_srt": "Давайте предположим, что значение, которое мы хотим, чтобы эта последняя активация была для данного примера обучения, равно y. Например, у может быть 0 или 1.",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is Al-y2.",
  "translatedText": "Таким образом, стоимость этой сети для одного обучающего примера составляет Al-y2.",
  "from_community_srt": "Таким образом, стоимость этой простой сети для одного примера обучения составляет (a ^ (L) - y) ^ 2.",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Мы обозначим стоимость этого одного обучающего примера как c0.",
  "from_community_srt": "Мы обозначим стоимость этого одного учебного примера как C_0.",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neuron's activation plus some bias, which I'll call BL.",
  "translatedText": "Напоминаем, что эта последняя активация определяется весом, который я назову WL, умноженным на активацию предыдущего нейрона плюс некоторое смещение, которое я назову BL.",
  "from_community_srt": "Как напоминание, эта последняя активация определяется весом, который я собираюсь назвать w ^ (L) раз активация предыдущего нейрона, плюс некоторый уклон,",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "А затем вы пропускаете это через какую-то специальную нелинейную функцию, например, сигмовидную или ReLU.",
  "from_community_srt": "который я назову б ^ (L), затем вы прокачиваете это через какую-то специальную нелинейную функцию как сигмоид или ReLU.",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "На самом деле нам будет проще, если мы дадим этой взвешенной сумме специальное имя, например z, с тем же верхним индексом, что и соответствующие активации.",
  "from_community_srt": "На самом деле нам будет легче, если мы дадим специальное имя этой взвешенной сумме, например, z, с тем же верхним индексом, что и соответствующие активации.",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action and the bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "Это много терминов, и вы можете это представить следующим образом: вес, предыдущее действие и смещение все вместе используются для вычисления z, что, в свою очередь, позволяет нам вычислить a, что, наконец, вместе с константой y, позволяет мы рассчитаем стоимость.",
  "from_community_srt": "Так что здесь много терминов. И способ, которым вы могли бы осмыслить это то, что вес, предыдущая активация и уклон в целом используются для вычисления Z, что в свою очередь позволяет нам вычислить, который, наконец, вместе с константой у, вычислим стоимость.",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course Al-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "И, конечно, на Ал-1 влияет его собственный вес, предвзятость и тому подобное, но мы не собираемся сейчас на этом сосредотачиваться.",
  "from_community_srt": "И, конечно, на ^ (L-1) влияет его собственный вес и уклон, и тому подобное. Но мы не собираемся концентрироваться на этом прямо сейчас.",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Все это всего лишь цифры, верно?",
  "from_community_srt": "Все это просто цифры,",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "И было бы неплохо представить, что у каждого из них есть своя маленькая числовая линия.",
  "from_community_srt": "верно? И может быть приятно думать, что каждый из них имеет свою собственную маленькую числовую линию.",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight WL.",
  "translatedText": "Наша первая цель — понять, насколько чувствительна функция стоимости к небольшим изменениям нашего веса WL.",
  "from_community_srt": "Наша первая цель - понять насколько чувствительна функция стоимости к небольшим изменениям нашего веса w ^ (L).",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to WL?",
  "translatedText": "Или, другими словами, какова производная c по отношению к WL?",
  "from_community_srt": "Или по-другому, что является производной от С по w ^ (L).",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del W term, think of it as meaning some tiny nudge to W, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Когда вы видите этот термин del C, думайте о нем как о некотором небольшом подталкивании к W, например, об изменении на 0,01, и думайте об этом термине del c как означающем, каким бы ни было результирующее подталкивание к стоимости.",
  "from_community_srt": "Когда вы видите этот термин «∂w», Думайте об этом как о значении «какое-то крошечное толчок к w», как изменение на 0,01. И думайте об этом термине «∂C» как означающем «каким бы ни был получаемый в результате толчок к стоимости».",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "Нам нужно их соотношение.",
  "from_community_srt": "То, что мы хотим, это их соотношение.",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Концептуально, этот небольшой толчок к WL вызывает некоторый толчок к ZL, который, в свою очередь, вызывает некоторый толчок к AL, что напрямую влияет на стоимость.",
  "from_community_srt": "Концептуально, этот крошечный толчок к w ^ (L) вызывает некоторый толчок к z ^ (L) что, в свою очередь, приводит к некоторому изменению ^ (L), что напрямую влияет на стоимость.",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is, the derivative of ZL with respect to WL.",
  "translatedText": "Итак, мы разбираем ситуацию, сначала рассматривая отношение крошечного изменения ZL к этому крошечному изменению W, то есть производной ZL по отношению к WL.",
  "from_community_srt": "Поэтому мы разбиваем это, сначала посмотрев на отношение крошечного изменения к z ^ (L) к крошечному изменению w ^ (L). То есть производная z ^ (L) по w ^ (L).",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "Аналогично, затем вы учитываете отношение изменения AL к крошечному изменению ZL, которое его вызвало, а также соотношение между окончательным подталкиванием к c и этим промежуточным подталкиванием к AL.",
  "from_community_srt": "Аналогично, вы затем учитываете отношение изменения к ^ (L) к крошечному изменению z ^ (L), которое вызвало его, а также отношение между последним толчком к C и этим промежуточным толчком к a ^ (L).",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of c to small changes in WL.",
  "translatedText": "Вот это и есть цепное правило, согласно которому умножение этих трех коэффициентов дает нам чувствительность c к небольшим изменениям WL.",
  "from_community_srt": "Это прямо здесь - цепное правило, где умножение этих трех соотношений дает нам чувствительность C к небольшим изменениям w ^ (L).",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "Итак, сейчас на экране много символов, и вам понадобится минутка, чтобы убедиться, что все они ясны, потому что сейчас мы собираемся вычислить соответствующие производные.",
  "from_community_srt": "Так что на экране прямо сейчас есть много символов, так что найдите время, чтобы убедиться, что они все потому что теперь мы собираемся вычислить соответствующие производные.",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "Производная c по AL равна 2AL-y.",
  "from_community_srt": "Производная C по отношению к ^ (L) оказывается равной 2 (a ^ (L) - y).",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "Notice this means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Обратите внимание: это означает, что его размер пропорционален разнице между выходными данными сети и тем, что мы хотим, поэтому, если эти выходные данные сильно отличаются, даже небольшие изменения окажут большое влияние на конечную функцию стоимости.",
  "from_community_srt": "Обратите внимание, это означает, что его размер пропорционален разница между выходом сети и тем, что мы хотим, чтобы это было. Так что, если этот результат был совсем другим, даже незначительные изменения могут оказать большое влияние на функцию стоимости.",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "Производная AL по ZL — это просто производная нашей сигмовидной функции или любой другой нелинейности, которую вы решите использовать.",
  "from_community_srt": "Производная a ^ (L) по z ^ (L) является просто производной нашей сигмоидальной функции, или любую нелинейность, которую вы решите использовать.",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "And the derivative of ZL with respect to WL comes out to be AL-1.",
  "translatedText": "А производная от ZL по WL получается AL-1.",
  "from_community_srt": "И производная от z ^ (L) по w ^ (L), в этом случае получается просто ^ (L-1).",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all mean.",
  "translatedText": "Не знаю, как вы, но я думаю, что легко застрять в формулах, не тратя ни минуты на то, чтобы расслабиться и напомнить себе, что все они означают.",
  "from_community_srt": "Теперь я не знаю о вас, но я думаю, что легко застрять с головой в этих формулах не тратя времени на то, чтобы расслабиться и напомнить себе, что они на самом деле означают.",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "В случае с этой последней производной степень влияния небольшого увеличения веса на последний слой зависит от того, насколько силен предыдущий нейрон.",
  "from_community_srt": "В случае этой последней производной, количество, на которое небольшой толчок к этому весу влияет на последний слой зависит от того, насколько сильный предыдущий нейрон.",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "Помните, именно здесь на помощь приходит идея нейронов, которые срабатывают вместе.",
  "from_community_srt": "Помните, что именно здесь возникает идея «нейроны, которые сжигают вместе проволоку».",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to WL only of the cost for a specific single training example.",
  "translatedText": "И все это является производной по WL лишь стоимости конкретного единичного обучающего примера.",
  "from_community_srt": "И все это является производной по отношению к w ^ (L) только стоимости для конкретного обучающего примера.",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Поскольку функция полной стоимости включает в себя усреднение всех этих затрат по множеству различных обучающих примеров, ее производная требует усреднения этого выражения по всем обучающим примерам.",
  "from_community_srt": "Поскольку функция полной стоимости включает в себя усреднение всех этих затрат по многим учебным примерам, его производная требует усреднения этого выражения, которое мы нашли во всех обучающих примерах.",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "And of course, that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "И, конечно же, это всего лишь один компонент вектора градиента, который сам по себе состоит из частных производных функции стоимости по всем этим весам и смещениям.",
  "from_community_srt": "И, конечно, это только один компонент вектора градиента, который сам построен из частные производные функции стоимости относительно всех этих весов и смещений.",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "Но даже несмотря на то, что это всего лишь одна из многих частных производных, которые нам нужны, это более 50% работы.",
  "from_community_srt": "Но даже если это был только один из тех частных производных, которые нам нужны, это более 50% работы.",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "Например, чувствительность к предвзятости практически одинакова.",
  "from_community_srt": "Чувствительность к смещению, например, практически одинакова.",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Нам просто нужно заменить этот термин del z del w на del z del b.",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "И если вы посмотрите на соответствующую формулу, эта производная окажется равной 1.",
  "from_community_srt": "Нам просто нужно заменить этот термин termz / ∂w на ∂z / ∂b, И если вы посмотрите на соответствующую формулу, эта производная становится 1.",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "Кроме того, и здесь возникает идея обратного распространения, вы можете увидеть, насколько чувствительна эта функция стоимости к активации предыдущего слоя.",
  "from_community_srt": "Кроме того, и вот тут-то и возникает идея распространения в обратном направлении, вы можете увидеть, насколько чувствительна эта функция стоимости к активации предыдущего слоя; а именно,",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight WL.",
  "translatedText": "А именно, эта начальная производная в выражении цепного правила, чувствительность z к предыдущей активации, оказывается весом WL.",
  "from_community_srt": "эта начальная производная в разложении правила цепочки, чувствительность z к предыдущей активации, выходит вес w ^ (L).",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "И снова, даже несмотря на то, что мы не сможем напрямую влиять на активацию предыдущего слоя, полезно отслеживать это, потому что теперь мы можем просто продолжать повторять ту же самую идею цепного правила в обратном порядке, чтобы увидеть, насколько чувствительна функция стоимости к предыдущие веса и предыдущие смещения.",
  "from_community_srt": "И снова, хотя мы не сможем напрямую повлиять на эту активацию, полезно отслеживать, потому что теперь мы можем просто продолжать повторять эту идею правила цепи в обратном направлении чтобы увидеть, насколько чувствительна функция стоимости к предыдущим весам и к предыдущим отклонениям.",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "И вы можете подумать, что это слишком простой пример, поскольку все слои имеют один нейрон, и в реальной сети все будет экспоненциально сложнее.",
  "from_community_srt": "И вы можете подумать, что это слишком простой пример, поскольку все слои имеют только 1 нейрон, и все станет намного сложнее в реальной сети.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "Но, честно говоря, не так уж много изменений, когда мы даем слоям несколько нейронов, на самом деле это всего лишь несколько дополнительных индексов, которые нужно отслеживать.",
  "from_community_srt": "Но, честно говоря, не так много изменений, когда мы даем слоям несколько нейронов. На самом деле это всего лишь несколько индексов, которые нужно отслеживать.",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "Вместо того, чтобы активировать данный слой просто как AL, он также будет иметь нижний индекс, указывающий, какой это нейрон этого слоя.",
  "from_community_srt": "Вместо того, чтобы активировать данный слой просто будучи ^ (L), он также будет иметь индекс, указывающий, какой это нейрон этого слоя.",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Давайте использовать букву k для индексации слоя L-1 и букву j для индексации слоя L.",
  "from_community_srt": "Давайте продолжим и будем использовать букву k для индексации слоя (L-1) и j для индексации слоя (L).",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "Что касается стоимости, мы снова смотрим на желаемый результат, но на этот раз мы суммируем квадраты разностей между активациями последнего слоя и желаемым результатом.",
  "from_community_srt": "Что касается стоимости, снова мы смотрим на то, каков желаемый результат. Но в это время мы складываем квадраты различий между этими последними активациями слоя и желаемым результатом.",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus Yj squared.",
  "translatedText": "То есть вы берете сумму ALj минус Yj в квадрате.",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Поскольку весов намного больше, каждый из них должен иметь еще пару индексов, чтобы отслеживать, где он находится, поэтому давайте назовем вес ребра, соединяющего этот k-й нейрон с j-м нейроном, WLjk.",
  "from_community_srt": "То есть вы берете сумму за (a_j ^ (L) - y_j) ^ 2 Так как весов намного больше, каждый должен иметь еще пару индексов, чтобы отслеживать, где он находится. Итак, назовем вес ребра, соединяющего этот k-й нейрон с j-м нейроном, w_ {jk} ^ (L).",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "Поначалу эти индексы могут показаться немного обратными, но они соответствуют тому, как вы индексируете матрицу весов, о которой я говорил в видео части 1.",
  "from_community_srt": "Поначалу эти индексы могут показаться немного отсталыми, но это соответствует тому, как вы будете индексировать матрицу весов, о которой я говорил в видео части 1.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "Как и раньше, полезно дать имя соответствующей взвешенной сумме, например z, чтобы активация последнего слоя была просто вашей специальной функцией, такой как сигмоида, примененной к z.",
  "from_community_srt": "Как и прежде, все еще приятно дать имя соответствующей взвешенной сумме, например, z, так что активация последнего слоя - это просто ваша специальная функция, такая как сигмоида, примененная к z.",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Вы понимаете, что я имею в виду: все это, по сути, те же уравнения, которые мы использовали ранее в случае одного нейрона на слой, просто это выглядит немного сложнее.",
  "from_community_srt": "Вы можете понять, что я имею в виду, верно? Все это, по сути, те же уравнения, которые мы имели ранее в случае с одним нейроном на слой; это выглядит немного сложнее.",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain-ruled derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "И действительно, выражение производной цепочки, описывающее, насколько чувствительность стоимости к определенному весу, выглядит по существу одинаково.",
  "from_community_srt": "И действительно, производное выражение цепочки правил описывая, насколько чувствительна стоимость к определенному весу выглядит по сути одинаково.",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Я оставлю вам возможность сделать паузу и подумать о каждом из этих терминов, если хотите.",
  "from_community_srt": "Я оставлю это вам, чтобы сделать паузу и подумать о каждом из этих терминов, если хотите.",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "Однако здесь меняется производная стоимости по отношению к одной из активаций в слое L-1.",
  "from_community_srt": "Что здесь меняется, хотя, является производной стоимости по отношению к одной из активаций в слое (L-1).",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "В этом случае разница в том, что нейрон влияет на функцию стоимости несколькими разными путями.",
  "from_community_srt": "В этом случае разница в том, что нейрон влияет на функцию стоимости через несколько путей.",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "То есть, с одной стороны, он влияет на AL0, который играет роль в функции затрат, но он также влияет на AL1, который также играет роль в функции затрат, и вам придется их складывать.",
  "from_community_srt": "То есть, с одной стороны, это влияет на a_0 ^ (L), который играет роль в функции стоимости, но это также влияет на a_1 ^ (L), который также играет роль в функции стоимости. И вы должны добавить их.",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "И это, ну, это почти все.",
  "from_community_srt": "И это ... хорошо, это в значительной степени это.",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Как только вы узнаете, насколько чувствительна функция стоимости к активациям на предпоследнем слое, вы можете просто повторить процесс для всех весов и смещений, поступающих в этот слой.",
  "from_community_srt": "Как только вы узнаете, насколько чувствительна функция стоимости к активациям этого второго до последнего слоя, Вы можете просто повторить процесс для всех весов и смещений, подаваемых в этот слой.",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "Так что похлопайте себя по спине!",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Если все это имеет смысл, то вы теперь глубоко заглянули в суть обратного распространения ошибки — рабочей лошадки, лежащей в основе обучения нейронных сетей.",
  "from_community_srt": "Так похлопайте себя по спине! Если это все имеет смысл, Вы теперь заглянули глубоко в сердце обратного распространения, рабочая лошадка в изучении нейронных сетей.",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Эти выражения цепных правил дают вам производные, которые определяют каждый компонент градиента, который помогает минимизировать стоимость сети за счет неоднократного спуска.",
  "from_community_srt": "Эти выражения правила цепочки дают вам производные, которые определяют каждый компонент в градиенте это помогает минимизировать стоимость сети, постоянно снижаясь.",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Если вы сядете и подумаете обо всем этом, вам придется охватить множество уровней сложности, поэтому не волнуйтесь, если вашему разуму потребуется время, чтобы все это переварить.",
  "from_community_srt": "Hhhhpf. Если вы будете сидеть сложа руки и думать обо всем этом, это много уровней сложности, чтобы обернуть ваш разум вокруг. Так что не волнуйтесь, если вашему разуму потребуется время, чтобы все это переварить.",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]