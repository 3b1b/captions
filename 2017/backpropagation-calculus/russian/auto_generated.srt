1
00:00:04,019 --> 00:00:06,775
Трудное предположение здесь состоит в том, что вы просмотрели часть 3, 

2
00:00:06,775 --> 00:00:09,920
в которой дается интуитивное описание алгоритма обратного распространения ошибки.

3
00:00:11,040 --> 00:00:14,220
Здесь мы станем немного более формальными и углубимся в соответствующие вычисления.

4
00:00:14,820 --> 00:00:16,998
Это нормально, хотя бы немного сбивать с толку, 

5
00:00:16,998 --> 00:00:19,766
поэтому мантра о регулярной паузе и размышлении, безусловно, 

6
00:00:19,766 --> 00:00:21,400
применима здесь так же, как и везде.

7
00:00:21,940 --> 00:00:25,653
Наша главная цель — показать, как люди, занимающиеся машинным обучением, 

8
00:00:25,653 --> 00:00:30,180
обычно думают о цепном правиле исчисления в контексте сетей, которое отличается от того, 

9
00:00:30,180 --> 00:00:33,640
как большинство вводных курсов исчисления подходят к этому предмету.

10
00:00:34,340 --> 00:00:36,920
Для тех из вас, кому некомфортны соответствующие вычисления, 

11
00:00:36,920 --> 00:00:38,740
у меня есть целая серия статей на эту тему.

12
00:00:39,960 --> 00:00:46,020
Начнем с чрезвычайно простой сети, в которой каждый слой имеет один нейрон.

13
00:00:46,320 --> 00:00:51,131
Эта сеть определяется тремя весами и тремя смещениями, и наша цель — понять, 

14
00:00:51,131 --> 00:00:54,880
насколько чувствительна функция стоимости к этим переменным.

15
00:00:55,420 --> 00:00:58,096
Таким образом, мы знаем, какие корректировки этих условий 

16
00:00:58,096 --> 00:01:00,820
приведут к наиболее эффективному уменьшению функции затрат.

17
00:01:01,960 --> 00:01:04,840
И мы просто сосредоточимся на связи между двумя последними нейронами.

18
00:01:05,980 --> 00:01:11,009
Давайте отметим активацию этого последнего нейрона верхним индексом L, указывающим, 

19
00:01:11,009 --> 00:01:15,560
в каком слое он находится, поэтому активация предыдущего нейрона — это Al-1.

20
00:01:16,360 --> 00:01:19,297
Это не показатели степени, это просто способ индексации того, 

21
00:01:19,297 --> 00:01:23,040
о чем мы говорим, поскольку позже я хочу сохранить индексы для разных индексов.

22
00:01:23,720 --> 00:01:27,894
Предположим, что значение, которое мы хотим, чтобы эта последняя активация 

23
00:01:27,894 --> 00:01:32,180
была для данного обучающего примера, равно y, например, y может быть 0 или 1.

24
00:01:32,840 --> 00:01:39,240
Таким образом, стоимость этой сети для одного обучающего примера составляет Al-y2.

25
00:01:40,260 --> 00:01:44,380
Мы обозначим стоимость этого одного обучающего примера как c0.

26
00:01:45,900 --> 00:01:51,017
Напоминаем, что эта последняя активация определяется весом, который я назову WL, 

27
00:01:51,017 --> 00:01:56,640
умноженным на активацию предыдущего нейрона плюс некоторое смещение, которое я назову BL.

28
00:01:57,420 --> 00:02:00,168
А затем вы пропускаете это через какую-то специальную нелинейную функцию, 

29
00:02:00,168 --> 00:02:01,320
например, сигмовидную или ReLU.

30
00:02:01,800 --> 00:02:05,875
На самом деле нам будет проще, если мы дадим этой взвешенной сумме специальное имя, 

31
00:02:05,875 --> 00:02:09,320
например z, с тем же верхним индексом, что и соответствующие активации.

32
00:02:10,380 --> 00:02:14,498
Это много терминов, и вы можете это представить следующим образом: вес, 

33
00:02:14,498 --> 00:02:18,959
предыдущее действие и смещение все вместе используются для вычисления z, что, 

34
00:02:18,959 --> 00:02:23,535
в свою очередь, позволяет нам вычислить a, что, наконец, вместе с константой y, 

35
00:02:23,535 --> 00:02:25,480
позволяет мы рассчитаем стоимость.

36
00:02:27,340 --> 00:02:31,936
И, конечно, на Ал-1 влияет его собственный вес, предвзятость и тому подобное, 

37
00:02:31,936 --> 00:02:35,060
но мы не собираемся сейчас на этом сосредотачиваться.

38
00:02:35,700 --> 00:02:37,620
Все это всего лишь цифры, верно?

39
00:02:38,060 --> 00:02:41,040
И было бы неплохо представить, что у каждого из них есть своя маленькая числовая линия.

40
00:02:41,720 --> 00:02:45,189
Наша первая цель — понять, насколько чувствительна 

41
00:02:45,189 --> 00:02:49,000
функция стоимости к небольшим изменениям нашего веса WL.

42
00:02:49,540 --> 00:02:54,860
Или, другими словами, какова производная c по отношению к WL?

43
00:02:55,600 --> 00:02:59,628
Когда вы видите этот термин del C, думайте о нем как о некотором небольшом 

44
00:02:59,628 --> 00:03:02,367
подталкивании к W, например, об изменении на 0,01, 

45
00:03:02,367 --> 00:03:04,945
и думайте об этом термине del c как означающем, 

46
00:03:04,945 --> 00:03:08,060
каким бы ни было результирующее подталкивание к стоимости.

47
00:03:08,060 --> 00:03:10,220
Нам нужно их соотношение.

48
00:03:11,260 --> 00:03:16,310
Концептуально, этот небольшой толчок к WL вызывает некоторый толчок к ZL, который, 

49
00:03:16,310 --> 00:03:21,240
в свою очередь, вызывает некоторый толчок к AL, что напрямую влияет на стоимость.

50
00:03:23,120 --> 00:03:28,320
Итак, мы разбираем ситуацию, сначала рассматривая отношение крошечного изменения 

51
00:03:28,320 --> 00:03:33,200
ZL к этому крошечному изменению W, то есть производной ZL по отношению к WL.

52
00:03:33,200 --> 00:03:37,829
Аналогично, затем вы учитываете отношение изменения AL к крошечному изменению ZL, 

53
00:03:37,829 --> 00:03:41,272
которое его вызвало, а также соотношение между окончательным 

54
00:03:41,272 --> 00:03:44,660
подталкиванием к c и этим промежуточным подталкиванием к AL.

55
00:03:45,740 --> 00:03:50,474
Вот это и есть цепное правило, согласно которому умножение этих трех 

56
00:03:50,474 --> 00:03:55,140
коэффициентов дает нам чувствительность c к небольшим изменениям WL.

57
00:03:56,880 --> 00:04:01,423
Итак, сейчас на экране много символов, и вам понадобится минутка, чтобы убедиться, 

58
00:04:01,423 --> 00:04:06,240
что все они ясны, потому что сейчас мы собираемся вычислить соответствующие производные.

59
00:04:07,440 --> 00:04:13,160
Производная c по AL равна 2AL-y.

60
00:04:13,980 --> 00:04:18,523
Обратите внимание: это означает, что его размер пропорционален разнице между выходными 

61
00:04:18,523 --> 00:04:23,066
данными сети и тем, что мы хотим, поэтому, если эти выходные данные сильно отличаются, 

62
00:04:23,066 --> 00:04:27,140
даже небольшие изменения окажут большое влияние на конечную функцию стоимости.

63
00:04:27,840 --> 00:04:31,823
Производная AL по ZL — это просто производная нашей сигмовидной 

64
00:04:31,823 --> 00:04:36,180
функции или любой другой нелинейности, которую вы решите использовать.

65
00:04:37,220 --> 00:04:44,660
А производная от ZL по WL получается AL-1.

66
00:04:45,760 --> 00:04:48,951
Не знаю, как вы, но я думаю, что легко застрять в формулах, 

67
00:04:48,951 --> 00:04:53,420
не тратя ни минуты на то, чтобы расслабиться и напомнить себе, что все они означают.

68
00:04:53,920 --> 00:04:58,429
В случае с этой последней производной степень влияния небольшого увеличения 

69
00:04:58,429 --> 00:05:02,820
веса на последний слой зависит от того, насколько силен предыдущий нейрон.

70
00:05:03,380 --> 00:05:08,280
Помните, именно здесь на помощь приходит идея нейронов, которые срабатывают вместе.

71
00:05:09,200 --> 00:05:12,806
И все это является производной по WL лишь стоимости 

72
00:05:12,806 --> 00:05:15,720
конкретного единичного обучающего примера.

73
00:05:16,440 --> 00:05:20,150
Поскольку функция полной стоимости включает в себя усреднение всех 

74
00:05:20,150 --> 00:05:23,195
этих затрат по множеству различных обучающих примеров, 

75
00:05:23,195 --> 00:05:27,460
ее производная требует усреднения этого выражения по всем обучающим примерам.

76
00:05:28,380 --> 00:05:32,235
И, конечно же, это всего лишь один компонент вектора градиента, 

77
00:05:32,235 --> 00:05:37,175
который сам по себе состоит из частных производных функции стоимости по всем этим 

78
00:05:37,175 --> 00:05:38,260
весам и смещениям.

79
00:05:40,640 --> 00:05:43,707
Но даже несмотря на то, что это всего лишь одна из многих частных производных, 

80
00:05:43,707 --> 00:05:45,260
которые нам нужны, это более 50% работы.

81
00:05:46,340 --> 00:05:49,720
Например, чувствительность к предвзятости практически одинакова.

82
00:05:50,040 --> 00:05:55,020
Нам просто нужно заменить этот термин del z del w на del z del b.

83
00:05:58,420 --> 00:06:02,400
И если вы посмотрите на соответствующую формулу, эта производная окажется равной 1.

84
00:06:06,140 --> 00:06:11,124
Кроме того, и здесь возникает идея обратного распространения, вы можете увидеть, 

85
00:06:11,124 --> 00:06:15,740
насколько чувствительна эта функция стоимости к активации предыдущего слоя.

86
00:06:15,740 --> 00:06:20,738
А именно, эта начальная производная в выражении цепного правила, 

87
00:06:20,738 --> 00:06:25,660
чувствительность z к предыдущей активации, оказывается весом WL.

88
00:06:26,640 --> 00:06:30,346
И снова, даже несмотря на то, что мы не сможем напрямую влиять на активацию 

89
00:06:30,346 --> 00:06:34,101
предыдущего слоя, полезно отслеживать это, потому что теперь мы можем просто 

90
00:06:34,101 --> 00:06:37,709
продолжать повторять ту же самую идею цепного правила в обратном порядке, 

91
00:06:37,709 --> 00:06:41,464
чтобы увидеть, насколько чувствительна функция стоимости к предыдущие веса и 

92
00:06:41,464 --> 00:06:42,440
предыдущие смещения.

93
00:06:43,180 --> 00:06:46,120
И вы можете подумать, что это слишком простой пример, 

94
00:06:46,120 --> 00:06:51,020
поскольку все слои имеют один нейрон, и в реальной сети все будет экспоненциально сложнее.

95
00:06:51,700 --> 00:06:55,198
Но, честно говоря, не так уж много изменений, когда мы даем слоям несколько нейронов, 

96
00:06:55,198 --> 00:06:58,860
на самом деле это всего лишь несколько дополнительных индексов, которые нужно отслеживать.

97
00:06:59,380 --> 00:07:02,755
Вместо того, чтобы активировать данный слой просто как AL, 

98
00:07:02,755 --> 00:07:07,160
он также будет иметь нижний индекс, указывающий, какой это нейрон этого слоя.

99
00:07:07,160 --> 00:07:14,420
Давайте использовать букву k для индексации слоя L-1 и букву j для индексации слоя L.

100
00:07:15,260 --> 00:07:19,039
Что касается стоимости, мы снова смотрим на желаемый результат, 

101
00:07:19,039 --> 00:07:23,940
но на этот раз мы суммируем квадраты разностей между активациями последнего слоя и 

102
00:07:23,940 --> 00:07:25,180
желаемым результатом.

103
00:07:26,080 --> 00:07:30,840
То есть вы берете сумму ALj минус Yj в квадрате.

104
00:07:33,040 --> 00:07:37,673
Поскольку весов намного больше, каждый из них должен иметь еще пару индексов, 

105
00:07:37,673 --> 00:07:41,950
чтобы отслеживать, где он находится, поэтому давайте назовем вес ребра, 

106
00:07:41,950 --> 00:07:44,920
соединяющего этот k-й нейрон с j-м нейроном, WLjk.

107
00:07:45,620 --> 00:07:49,684
Поначалу эти индексы могут показаться немного обратными, но они соответствуют тому, 

108
00:07:49,684 --> 00:07:53,120
как вы индексируете матрицу весов, о которой я говорил в видео части 1.

109
00:07:53,620 --> 00:07:57,323
Как и раньше, полезно дать имя соответствующей взвешенной сумме, 

110
00:07:57,323 --> 00:08:02,108
например z, чтобы активация последнего слоя была просто вашей специальной функцией, 

111
00:08:02,108 --> 00:08:04,160
такой как сигмоида, примененной к z.

112
00:08:04,660 --> 00:08:08,332
Вы понимаете, что я имею в виду: все это, по сути, те же уравнения, 

113
00:08:08,332 --> 00:08:11,735
которые мы использовали ранее в случае одного нейрона на слой, 

114
00:08:11,735 --> 00:08:13,680
просто это выглядит немного сложнее.

115
00:08:15,440 --> 00:08:18,663
И действительно, выражение производной цепочки, описывающее, 

116
00:08:18,663 --> 00:08:23,420
насколько чувствительность стоимости к определенному весу, выглядит по существу одинаково.

117
00:08:23,920 --> 00:08:26,840
Я оставлю вам возможность сделать паузу и подумать о каждом из этих терминов, если хотите.

118
00:08:28,980 --> 00:08:36,659
Однако здесь меняется производная стоимости по отношению к одной из активаций в слое L-1.

119
00:08:37,780 --> 00:08:40,303
В этом случае разница в том, что нейрон влияет 

120
00:08:40,303 --> 00:08:42,880
на функцию стоимости несколькими разными путями.

121
00:08:44,680 --> 00:08:50,474
То есть, с одной стороны, он влияет на AL0, который играет роль в функции затрат, 

122
00:08:50,474 --> 00:08:55,490
но он также влияет на AL1, который также играет роль в функции затрат, 

123
00:08:55,490 --> 00:08:57,540
и вам придется их складывать.

124
00:08:59,820 --> 00:09:03,040
И это, ну, это почти все.

125
00:09:03,500 --> 00:09:06,753
Как только вы узнаете, насколько чувствительна функция стоимости 

126
00:09:06,753 --> 00:09:09,906
к активациям на предпоследнем слое, вы можете просто повторить 

127
00:09:09,906 --> 00:09:12,860
процесс для всех весов и смещений, поступающих в этот слой.

128
00:09:13,900 --> 00:09:14,960
Так что похлопайте себя по спине!

129
00:09:15,300 --> 00:09:18,756
Если все это имеет смысл, то вы теперь глубоко заглянули в суть обратного 

130
00:09:18,756 --> 00:09:22,680
распространения ошибки — рабочей лошадки, лежащей в основе обучения нейронных сетей.

131
00:09:23,300 --> 00:09:26,173
Эти выражения цепных правил дают вам производные, 

132
00:09:26,173 --> 00:09:28,874
которые определяют каждый компонент градиента, 

133
00:09:28,874 --> 00:09:33,300
который помогает минимизировать стоимость сети за счет неоднократного спуска.

134
00:09:34,300 --> 00:09:38,166
Если вы сядете и подумаете обо всем этом, вам придется охватить множество уровней 

135
00:09:38,166 --> 00:09:41,561
сложности, поэтому не волнуйтесь, если вашему разуму потребуется время, 

136
00:09:41,561 --> 00:09:42,740
чтобы все это переварить.

