1
00:00:00,000 --> 00:00:08,420
A kemény feltételezés az, hogy megnézted a 3.

2
00:00:08,420 --> 00:00:11,160
részt, amely intuitív áttekintést ad a visszaterjesztési algoritmusról.

3
00:00:11,160 --> 00:00:14,920
Itt egy kicsit formálisabbak vagyunk, és belemerülünk a vonatkozó számításba.

4
00:00:14,920 --> 00:00:18,560
Normális, hogy ez legalább egy kicsit zavaró, így a rendszeres szünet

5
00:00:18,560 --> 00:00:22,000
és töprengés mantra itt is ugyanúgy érvényes, mint bárhol máshol.

6
00:00:22,000 --> 00:00:26,620
Fő célunk, hogy bemutassuk, hogyan gondolkodnak a gépi tanulásban részt vevők

7
00:00:26,620 --> 00:00:31,900
általában a hálózatok kontextusában a számításból származó láncszabályról, amely másképp

8
00:00:31,900 --> 00:00:34,580
hat, mint a legtöbb bevezető számítástechnikai kurzus megközelítése a témához.

9
00:00:34,580 --> 00:00:38,300
Azok számára, akik nem érzik jól magukat a

10
00:00:38,300 --> 00:00:39,300
vonatkozó kalkulusban, egy egész sorozatom van a témában.

11
00:00:39,300 --> 00:00:44,840
Kezdjük egy rendkívül egyszerű hálózattal, ahol

12
00:00:44,840 --> 00:00:46,780
minden rétegben egyetlen neuron található.

13
00:00:46,780 --> 00:00:51,880
Ezt a hálózatot három súlyozás és három torzítás határozza meg, és

14
00:00:51,880 --> 00:00:55,640
célunk annak megértése, hogy a költségfüggvény mennyire érzékeny ezekre a változókra.

15
00:00:55,640 --> 00:00:59,780
Így tudjuk, hogy ezeknek a feltételeknek mely

16
00:00:59,780 --> 00:01:01,100
módosításai okozzák a költségfüggvény leghatékonyabb csökkentését.

17
00:01:01,100 --> 00:01:05,360
Csak az utolsó két neuron közötti kapcsolatra koncentrálunk.

18
00:01:05,360 --> 00:01:10,400
Jelöljük az utolsó neuron aktiválását L felső

19
00:01:10,400 --> 00:01:11,800
indexszel, jelezve, hogy melyik rétegben van.

20
00:01:11,800 --> 00:01:16,560
Tehát az előző neuron aktiválása AL-1.

21
00:01:16,560 --> 00:01:20,120
Ezek nem exponensek, csak egy módja annak, hogy indexeljük azt, amiről beszélünk,

22
00:01:20,120 --> 00:01:23,120
mivel a későbbiekben szeretném elmenteni az alsó indexeket a különböző indexekhez.

23
00:01:23,600 --> 00:01:28,880
Tegyük fel, hogy egy adott képzési példánál az utolsó

24
00:01:28,880 --> 00:01:33,020
aktiválás értéke y, például y lehet 0 vagy 1.

25
00:01:33,020 --> 00:01:39,040
Tehát ennek a hálózatnak a költsége egyetlen képzési példa esetében AL-y2.

26
00:01:39,040 --> 00:01:46,120
Ennek az egy képzési példának a költségét c0-val jelöljük.

27
00:01:46,120 --> 00:01:51,920
Emlékeztetőül, ezt az utolsó aktiválást egy súly határozza meg, amelyet wL-nek fogok

28
00:01:51,920 --> 00:01:57,600
nevezni, szorozva az előző neuron aktiválódásával, plusz némi torzítással, amit bL-nek nevezek.

29
00:01:57,600 --> 00:02:01,560
Ezután ezt valamilyen speciális nemlineáris függvényen keresztül pumpálja, mint például a szigmoid vagy a ReLU.

30
00:02:01,560 --> 00:02:05,400
Valójában megkönnyíti a dolgunkat, ha ennek a súlyozott összegnek külön nevet

31
00:02:05,400 --> 00:02:10,600
adunk, például z-t, ugyanazzal a felső indexszel, mint a vonatkozó aktiválások.

32
00:02:10,600 --> 00:02:15,320
Ez egy csomó kifejezés, és úgy lehet elképzelni, hogy a súlyt, az előző műveletet

33
00:02:15,320 --> 00:02:21,800
és a torzítást együtt használják a z kiszámítására, ami viszont lehetővé teszi számunkra

34
00:02:21,800 --> 00:02:27,360
a kiszámítását, ami végül egy konstans y-val együtt lehetővé teszi kiszámoljuk a költségeket.

35
00:02:27,360 --> 00:02:33,440
És természetesen az AL-1-et befolyásolja a saját súlya, elfogultsága

36
00:02:33,440 --> 00:02:35,920
és hasonlók, de most nem erre fogunk koncentrálni.

37
00:02:35,920 --> 00:02:38,120
Ezek mind csak számok, igaz?

38
00:02:38,120 --> 00:02:41,960
És jó lehet úgy gondolni, hogy mindegyiknek megvan a maga kis számsora.

39
00:02:41,960 --> 00:02:47,480
Első célunk annak megértése, hogy a költségfüggvény

40
00:02:47,480 --> 00:02:49,820
mennyire érzékeny a súlyunk kis változásaira wL.

41
00:02:49,820 --> 00:02:55,740
Vagy fogalmazz másképp, mi a c deriváltja wL-hez képest?

42
00:02:55,740 --> 00:03:01,220
Ha látja ezt a del w kifejezést, gondolja úgy, hogy ez egy apró lökést jelent w-hez, például 0-val való változtatást.

43
00:03:01,220 --> 00:03:08,820
01, és gondolja úgy, hogy ez a del c kifejezés bármit is jelent a költségekhez képest.

44
00:03:08,820 --> 00:03:10,900
Amit szeretnénk, az az arányuk.

45
00:03:10,900 --> 00:03:17,740
Elméletileg ez az apró lökés a wL felé némi lökést okoz a zL-nek,

46
00:03:17,740 --> 00:03:23,220
ami viszont némi lökést okoz az AL-nak, ami közvetlenül befolyásolja a költségeket.

47
00:03:23,220 --> 00:03:28,020
Tehát a dolgokat úgy bontjuk szét, hogy először megnézzük a zL-hez viszonyított apró változás

48
00:03:28,020 --> 00:03:33,340
és ehhez a kis w változáshoz viszonyított arányát, vagyis zL deriváltját wL-hez képest.

49
00:03:33,340 --> 00:03:38,820
Hasonlóképpen, akkor vegye figyelembe az AL-hoz való változás és a zL-ben

50
00:03:38,820 --> 00:03:43,900
bekövetkezett apró változás arányát, amely ezt okozta, valamint a c-hez való

51
00:03:43,900 --> 00:03:45,900
végső lökést és az AL-hez való közbenső lökést közötti arányt.

52
00:03:45,900 --> 00:03:51,880
Ez itt a láncszabály, ahol ezt a három arányt

53
00:03:51,880 --> 00:03:57,340
megszorozva megkapjuk c érzékenységét a wL kis változásaira.

54
00:03:57,340 --> 00:04:01,620
Tehát a képernyőn jelenleg nagyon sok szimbólum van, és szánjon egy percet, hogy

55
00:04:01,620 --> 00:04:07,460
megbizonyosodjon arról, hogy világos, mi ez, mert most a vonatkozó származékokat fogjuk kiszámítani.

56
00:04:07,460 --> 00:04:14,220
A c deriváltja az AL-hez viszonyítva 2AL-y.

57
00:04:14,220 --> 00:04:19,300
Ez azt jelenti, hogy mérete arányos a hálózat kimenete és a kívánt

58
00:04:19,300 --> 00:04:24,480
dolog közötti különbséggel, tehát ha ez a kimenet nagyon eltérő volt, akkor

59
00:04:24,480 --> 00:04:28,380
még az enyhe változtatások is nagy hatással vannak a végső költségfüggvényre.

60
00:04:28,380 --> 00:04:33,860
Az AL zL-hez viszonyított deriváltja csak a szigmoid

61
00:04:33,860 --> 00:04:37,420
függvényünk deriváltja, vagy bármilyen nemlinearitás, amelyet használni szeretne.

62
00:04:37,420 --> 00:04:46,180
A zL deriváltja wL-hez viszonyítva AL-1 lesz.

63
00:04:46,180 --> 00:04:49,460
Nem tudom, ti hogy vagytok vele, de azt hiszem, könnyű beleragadni a

64
00:04:49,460 --> 00:04:54,180
képletekbe anélkül, hogy egy pillanatra is hátradőlne, és emlékezne, mit jelentenek ezek.

65
00:04:54,180 --> 00:04:58,860
Ennek az utolsó származéknak az esetében, hogy a súlyhoz való kis lökések milyen

66
00:04:58,860 --> 00:05:03,220
mértékben befolyásolták az utolsó réteget, attól függ, milyen erős az előző neuron.

67
00:05:03,220 --> 00:05:09,320
Ne feledje, itt jön a képbe a neuronok-az-együtt tüzel-huzal-összeköttetés ötlet.

68
00:05:09,320 --> 00:05:14,840
És mindez a wL vonatkozásában csak

69
00:05:14,840 --> 00:05:16,580
egy konkrét képzési példa költségének deriváltja.

70
00:05:16,580 --> 00:05:20,940
Mivel a teljes költségfüggvény magában foglalja az összes költségnek a

71
00:05:20,940 --> 00:05:27,300
sok különböző betanítási példában való együttes átlagolását, a származéka megköveteli

72
00:05:27,300 --> 00:05:28,540
ennek a kifejezésnek az átlagolását az összes képzési példában.

73
00:05:28,540 --> 00:05:33,860
Természetesen ez csak egy komponense a gradiensvektornak, amely a költségfüggvény

74
00:05:33,860 --> 00:05:40,780
parciális deriváltjaiból épül fel, tekintettel az összes súlyozásra és torzításra.

75
00:05:40,780 --> 00:05:44,340
De bár ez csak egy a sok részleges származék közül,

76
00:05:44,340 --> 00:05:46,460
amelyekre szükségünk van, ez a munka több mint 50%-a.

77
00:05:46,460 --> 00:05:50,300
A torzításra való érzékenység például majdnem azonos.

78
00:05:50,300 --> 00:05:58,980
Csak ki kell cserélnünk ezt a del z del w kifejezést egy del z del b-re.

79
00:05:58,980 --> 00:06:04,700
És ha megnézzük a vonatkozó képletet, a származéka 1 lesz.

80
00:06:04,700 --> 00:06:11,700
Illetve, és itt jön be a visszafelé terjedés ötlete, láthatjuk,

81
00:06:11,700 --> 00:06:16,180
hogy ez a költségfüggvény mennyire érzékeny az előző réteg aktiválására.

82
00:06:16,180 --> 00:06:21,380
Ugyanis ez a kezdeti derivált a láncszabály kifejezésben, a z

83
00:06:21,380 --> 00:06:25,420
érzékenysége az előző aktiválásra, a wL súlynak jön ki.

84
00:06:25,420 --> 00:06:30,100
És még egyszer, bár nem leszünk képesek közvetlenül befolyásolni

85
00:06:30,100 --> 00:06:35,280
az előző réteg aktiválását, hasznos nyomon követni, mert most

86
00:06:35,280 --> 00:06:40,780
már csak ismételhetjük ugyanezt a láncszabály-ötletet visszafelé, hogy meglássuk,

87
00:06:40,780 --> 00:06:43,680
mennyire érzékeny a költségfüggvény korábbi súlyok és korábbi torzítások.

88
00:06:43,680 --> 00:06:47,940
És azt gondolhatja, hogy ez egy túlságosan egyszerű példa, mivel minden rétegnek van

89
00:06:47,940 --> 00:06:51,320
egy neuronja, és a dolgok exponenciálisan bonyolultabbak lesznek egy valódi hálózat esetében.

90
00:06:51,320 --> 00:06:56,560
De őszintén szólva, nem sok változás történik, ha több neuront

91
00:06:56,560 --> 00:06:59,320
adunk a rétegeknek, valójában csak néhány indexet kell követni.

92
00:06:59,320 --> 00:07:03,580
Ahelyett, hogy egy adott réteg aktiválása egyszerűen AL lenne, annak egy alsó indexe

93
00:07:03,580 --> 00:07:07,920
is lesz, amely jelzi, hogy az adott réteg melyik neuronjáról van szó.

94
00:07:07,920 --> 00:07:15,280
Használjuk a k betűt az L-1 réteg, a j betűt pedig az L réteg indexeléséhez.

95
00:07:15,280 --> 00:07:20,720
A költségekhez ismét megnézzük, hogy mi a kívánt kimenet, de ezúttal

96
00:07:20,720 --> 00:07:26,120
összeadjuk az utolsó rétegaktiválások és a kívánt kimenet közötti különbségek négyzetét.

97
00:07:26,120 --> 00:07:33,280
Vagyis veszel egy összeget ALj mínusz yj négyzetével.

98
00:07:33,280 --> 00:07:36,500
Mivel sokkal több a súlyozás, mindegyiknek több indexnek kell lennie,

99
00:07:36,500 --> 00:07:41,380
hogy nyomon tudja követni, hol van, ezért nevezzük a

100
00:07:41,380 --> 00:07:45,740
k-adik neuront a j-edik neuronnal összekötő él súlyát WLjk-nek.

101
00:07:45,740 --> 00:07:49,820
Ezek az indexek eleinte kissé elmaradottaknak tűnhetnek, de ez összhangban van

102
00:07:49,820 --> 00:07:53,800
azzal, hogyan indexelné a súlymátrixot, amelyről az 1. rész videójában beszéltem.

103
00:07:53,800 --> 00:07:57,660
Csakúgy, mint korábban, továbbra is jó nevet adni a megfelelő

104
00:07:57,660 --> 00:08:03,540
súlyozott összegnek, például z-nek, így az utolsó réteg aktiválása

105
00:08:03,540 --> 00:08:04,980
csak a speciális függvény, mint a z-re alkalmazott szigmoid.

106
00:08:04,980 --> 00:08:09,100
Láthatja, mire gondolok, ahol ezek lényegében ugyanazok az egyenletek, mint korábban

107
00:08:09,100 --> 00:08:15,420
a rétegenkénti egy neuron esetében, csak ez egy kicsit bonyolultabbnak tűnik.

108
00:08:15,420 --> 00:08:20,620
És valóban, a láncszabály derivált kifejezés, amely leírja, hogy a

109
00:08:20,620 --> 00:08:23,540
költség mennyire érzékeny egy adott súlyra, lényegében ugyanúgy néz ki.

110
00:08:23,540 --> 00:08:29,420
Meghagyom neked, hogy állj meg, és gondold át mindegyik kifejezést, ha akarod.

111
00:08:29,420 --> 00:08:34,900
Ami azonban itt változik, az az

112
00:08:34,900 --> 00:08:37,820
L-1 réteg egyik aktiválásának költségének deriváltja.

113
00:08:37,820 --> 00:08:42,000
Ebben az esetben a különbség az, hogy a

114
00:08:42,000 --> 00:08:43,540
neuron több különböző úton befolyásolja a költségfüggvényt.

115
00:08:43,540 --> 00:08:51,200
Vagyis egyrészt befolyásolja az AL0-t, ami a költségfüggvényben játszik

116
00:08:51,200 --> 00:08:56,460
szerepet, de hatással van az AL1-re is, ami szintén

117
00:08:56,460 --> 00:09:00,340
szerepet játszik a költségfüggvényben, és ezeket össze kell adni.

118
00:09:00,340 --> 00:09:03,680
És nagyjából ennyi.

119
00:09:03,680 --> 00:09:08,240
Ha tudja, hogy a költségfüggvény mennyire érzékeny az

120
00:09:08,240 --> 00:09:12,520
utolsó előtti réteg aktiválásaira, megismételheti a folyamatot az

121
00:09:12,520 --> 00:09:13,920
adott rétegbe betáplált összes súlyozásra és torzításra.

122
00:09:13,920 --> 00:09:15,420
Szóval veregesd meg magad!

123
00:09:15,420 --> 00:09:20,480
Ha mindennek van értelme, akkor most mélyen belenézett a

124
00:09:20,480 --> 00:09:23,700
backpropagation szívébe, a neurális hálózatok tanulási folyamatának igáslójába.

125
00:09:23,700 --> 00:09:27,960
Ezek a láncszabály-kifejezések megadják azokat a származékokat, amelyek meghatározzák a gradiens egyes

126
00:09:27,960 --> 00:09:35,020
komponenseit, amelyek segítenek minimalizálni a hálózat költségeit azáltal, hogy ismételten lefelé lépkednek.

127
00:09:35,020 --> 00:09:38,960
Ha hátradől, és végiggondolja az egészet, akkor ez egy csomó összetett réteg körül kell járnia

128
00:09:38,960 --> 00:09:42,840
az elméjének, szóval ne aggódjon, ha időbe telik, amíg az elméje megemészti az egészet.

