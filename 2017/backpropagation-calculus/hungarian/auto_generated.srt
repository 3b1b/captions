1
00:00:04,019 --> 00:00:06,440
Erősen feltételezem, hogy megnézted a 3. részt, 

2
00:00:06,440 --> 00:00:09,920
amely a visszaterjesztés algoritmus intuitív bemutatását tartalmazza.

3
00:00:11,040 --> 00:00:14,220
Itt sokkal formálisabbá válunk, és belemerülünk a matematikai leírásba.

4
00:00:14,820 --> 00:00:17,676
Normális, ha nem tiszta minden azonnal, ezért a mantra, 

5
00:00:17,676 --> 00:00:21,400
hogy rendszeresen tarts szünetet és gondolkodj, itt is ugyanúgy érvényes.

6
00:00:21,940 --> 00:00:25,567
A fő célom az, hogy megmutassam, hogyan gondolkodnak a gépi tanulással 

7
00:00:25,567 --> 00:00:29,552
foglalkozók általában a matematikai láncszabályról a hálózatok kontextusában, 

8
00:00:29,552 --> 00:00:33,640
ami eltér attól, ahogyan a legtöbb bevezető számtani kurzus megközelíti a témát.

9
00:00:34,340 --> 00:00:37,113
Azoknak, akiknek a matematikai kalkulus nem az erősségük, 

10
00:00:37,113 --> 00:00:38,740
van egy egész sorozatom a témában.

11
00:00:39,960 --> 00:00:46,020
Kezdjük egy rendkívül egyszerű hálózattal, ahol minden rétegben egyetlen neuron van.

12
00:00:46,320 --> 00:00:50,687
Ezt a hálózatot három súly és három eltolósúly határozza meg. A célunk az, 

13
00:00:50,687 --> 00:00:54,880
hogy megértsük, mennyire érzékeny a költségfüggvény ezekre a változókra.

14
00:00:55,420 --> 00:00:58,267
Így tudni fogjuk, hogy az említett tagok mely módosításai 

15
00:00:58,267 --> 00:01:00,820
okozzák a költségfüggvény legmeredekebb csökkenését.

16
00:01:01,960 --> 00:01:04,840
És most koncentráljunk csak az utolsó két neuron közötti kapcsolatra.

17
00:01:05,980 --> 00:01:10,731
Az utolsó neuron aktivációját jelöljük egy L felső indexszel, 

18
00:01:10,731 --> 00:01:15,560
ami a réteg számát jelzi, így az előző neuron aktivációja AL-1.

19
00:01:16,360 --> 00:01:20,036
Ezek nem kitevők, csak egy módja annak, hogy jelöljük miről beszélünk, 

20
00:01:20,036 --> 00:01:23,040
mivel a későbbiekben más jelöléseket is akarunk használni.

21
00:01:23,720 --> 00:01:28,154
Az utolsó neuron akvtivációjának értékét mondjuk jelöljük y-nal. 

22
00:01:28,154 --> 00:01:32,180
Például egy adott képzési példa esetében, y lehet 0 vagy 1.

23
00:01:32,840 --> 00:01:39,240
Tehát ennek a hálózatnak a költsége egyetlen képzési példa esetén AL-y a másodikon.

24
00:01:40,260 --> 00:01:44,380
Ennek az egy képzési példának a költségét C0-nak nevezzük.

25
00:01:45,900 --> 00:01:49,517
Emlékeztetőül, ezt az utolsó aktivációt egy súly határozza meg, 

26
00:01:49,517 --> 00:01:53,474
amelyet WL-nek fogok nevezni, szorozva az előző neuron aktivációjával 

27
00:01:53,474 --> 00:01:56,640
plusz egy kis eltolósúllyal, amelyet BL-nek fogok hívni.

28
00:01:57,420 --> 00:02:01,320
Aztán valamilyen speciális nemlineáris függvénybe kell rakni, mint a szigmoid vagy a ReLU.

29
00:02:01,800 --> 00:02:05,933
Megkönnyíti a dolgunkat, ha ennek a súlyozott összegnek egy speciális nevet adunk, 

30
00:02:05,933 --> 00:02:09,320
például "z"-t, ugyanazzal az indexszel, mint a vonatkozó aktivációk.

31
00:02:10,380 --> 00:02:14,612
Ez nagyon sok kifejezés, de talán úgy szemléletesebb, ha vesszük a súlyt, 

32
00:02:14,612 --> 00:02:18,444
az előző aktivációt és az eltolósúlyt, azokkal a "z" kiszámítható, 

33
00:02:18,444 --> 00:02:21,190
ami viszont lehetővé teszi az "a" kiszámítását, 

34
00:02:21,190 --> 00:02:25,480
ami végül egy "y" konstanssal együtt lehetővé teszi a költség kiszámítását.

35
00:02:27,340 --> 00:02:30,620
És persze az "AL-1"-et befolyásolja a saját súlya, 

36
00:02:30,620 --> 00:02:35,060
a korábbi aktiváció és a többi, de most nem erre fogunk koncentrálni.

37
00:02:35,700 --> 00:02:37,620
Ezek mind csak számok, igaz?

38
00:02:38,060 --> 00:02:41,040
És könnyű úgy gondolni rájuk, mintha saját kis számsorral rendelkeznének.

39
00:02:41,720 --> 00:02:45,540
Az első célunk az, hogy megértsük, mennyire érzékeny 

40
00:02:45,540 --> 00:02:49,000
a költségfüggvény a WL súlyunk kis változásaira.

41
00:02:49,540 --> 00:02:54,860
Vagy másképp fogalmazva: mi a C deriváltja WL függvényében?

42
00:02:55,600 --> 00:03:00,203
Amikor ezt a delta W kifejezést látod, gondolj arra, hogy ez a W egy apró, 

43
00:03:00,203 --> 00:03:03,824
például 0,01-es változást jelent, és ugyanígy gondold azt, 

44
00:03:03,824 --> 00:03:08,060
hogy ez a delta C kifejezés a költség ebből eredő változását jelenti.

45
00:03:08,060 --> 00:03:10,220
Amit mi akarunk, az az arányuk.

46
00:03:11,260 --> 00:03:14,677
Az elképzelésünknek megfelelően ha kicsit odébb lökjük a WL-t, 

47
00:03:14,677 --> 00:03:18,907
az egy kis lökést okoz a ZL-nek, ami viszont egy kicsit odébb löki az "AL"-t, 

48
00:03:18,907 --> 00:03:21,240
ami közvetlenül befolyásolja a költségeket.

49
00:03:23,120 --> 00:03:28,246
Tehát ezt úgy tudjuk megvalósítani, hogy először a ZL apró változását és az általa W-ben 

50
00:03:28,246 --> 00:03:33,200
okozott apró változás arányát nézzük, vagyis a ZL-nek a WL általi deriváltját vesszük.

51
00:03:33,200 --> 00:03:38,769
Hasonlóképpen, ezután figyelembe kell venni az AL változásának és az azt okozó ZL apró 

52
00:03:38,769 --> 00:03:44,211
változásának arányát, valamint a C végső elmozdulása és a közbenső AL lökése közötti 

53
00:03:44,211 --> 00:03:44,660
arányt.

54
00:03:45,740 --> 00:03:50,788
Ez igazából a láncszabály, ahol e három arány szorzatával 

55
00:03:50,788 --> 00:03:55,140
megkapjuk a C érzékenységét a WL kis változásaira.

56
00:03:56,880 --> 00:04:01,200
A képernyőn most egy csomó szimbólum látható, és javasolom szánj rá pár pillanatot, 

57
00:04:01,200 --> 00:04:04,234
hogy tisztázd magadban mik is ezek valójában, mert most ki 

58
00:04:04,234 --> 00:04:06,240
fogjuk számítani releváns deriváltakat.

59
00:04:07,440 --> 00:04:13,160
A "C" deriváltja az AL függvényében 2AL mínusz y.

60
00:04:13,980 --> 00:04:18,178
Vegyük észre, hogy ez azt jelenti, hogy a mérete arányos a hálózat kimenete és az 

61
00:04:18,178 --> 00:04:22,633
általunk kívánt kimenet közötti különbséggel, így ha ez a kimenet nagyon eltérő lenne, 

62
00:04:22,633 --> 00:04:27,140
akkor még a legkisebb változtatások is nagy hatással lennének a végső költségfüggvényre.

63
00:04:27,840 --> 00:04:32,010
Az AL ZL általi deriváltja nem más, mint a szigmoid függvényünk deriváltja, 

64
00:04:32,010 --> 00:04:36,180
vagy a helyette használt bármelyik más nemlinearitást megvalósító függvényé.

65
00:04:37,220 --> 00:04:44,660
És a ZL WL-hez tartozó deriváltja "AL-1"-nek adódik.

66
00:04:45,760 --> 00:04:50,087
Nem tudom, te hogy vagy vele, de szerintem könnyű belefeledkezni a képletekbe anélkül, 

67
00:04:50,087 --> 00:04:53,420
hogy emlékeztetnénk magunkat arra, hogy mit is jelentenek pontosan.

68
00:04:53,920 --> 00:04:58,592
Az utolsó derivált esetében az, hogy a súlyra adott kis lökés mennyire befolyásolja 

69
00:04:58,592 --> 00:05:02,820
az utolsó réteget, attól függ, hogy az előző neuron aktivációja milyen erős.

70
00:05:03,380 --> 00:05:05,645
Ne feledjük, hogy itt valósul meg a neuronoknál, 

71
00:05:05,645 --> 00:05:08,280
hogy amelyek együtt tüzelnek azok kapcsolata megerősödik.

72
00:05:09,200 --> 00:05:15,720
És mindez csak egyetlen adott képzési példa költségének a WL szerinti deriváltja.

73
00:05:16,440 --> 00:05:20,243
Mivel a teljes költségfüggvény a sok különböző képzési példa összes 

74
00:05:20,243 --> 00:05:23,823
ilyen költségét átlagolja, ezért a deriváltjának kiszámolásához 

75
00:05:23,823 --> 00:05:27,460
is az összes képzési példán átlagára kell elvégezni a deriválást.

76
00:05:28,380 --> 00:05:32,175
És természetesen ez csak egy összetevője a gradiens vektornak, 

77
00:05:32,175 --> 00:05:36,814
amely maga is a költségfüggvény összes súly és eltolósúly szerinti parciális 

78
00:05:36,814 --> 00:05:38,260
deriváltjaiból épül fel.

79
00:05:40,640 --> 00:05:43,430
És bár ez csak egy a sok szükséges parciális derivált közül, 

80
00:05:43,430 --> 00:05:45,260
mégis a munka több mint 50%-át teszi ki.

81
00:05:46,340 --> 00:05:49,720
Az eltolósúlyra való érzékenység például szinte azonos.

82
00:05:50,040 --> 00:05:53,925
Csak ki kell cserélnünk ezt a delta z, delta w kifejezést egy delta z, 

83
00:05:53,925 --> 00:05:55,020
delta b kifejezésre.

84
00:05:58,420 --> 00:06:02,400
És ha megnézzük a vonatkozó képletet, a derivált értéke 1 lesz.

85
00:06:06,140 --> 00:06:11,162
Továbbá, és itt jön a képbe a visszafelé történő terjesztés ötlete, láthatjuk, 

86
00:06:11,162 --> 00:06:15,740
hogy ez a költségfüggvény mennyire érzékeny az előző réteg aktivációira.

87
00:06:15,740 --> 00:06:20,204
Szóval ha ezt az első deriválást elvégezzük a láncszabály kifejezésben, 

88
00:06:20,204 --> 00:06:25,660
ami a "z" érzékenysége az előző aktivációra, azt kapjuk, hogy ez a WL súlynak felel meg.

89
00:06:26,640 --> 00:06:31,402
És még egyszer, bár nem tudjuk közvetlenül befolyásolni az előző réteg aktivációját, 

90
00:06:31,402 --> 00:06:34,820
hasznos számon tartani, mert innentől ismételni tudjuk ezt a 

91
00:06:34,820 --> 00:06:37,789
láncszabály-ötletet visszafelé haladva, hogy lássuk, 

92
00:06:37,789 --> 00:06:42,440
mennyire érzékeny a költségfüggvény a korábbi súlyokra és a korábbi eltolósúlyokra.

93
00:06:43,180 --> 00:06:46,918
És gondolhatnád, hogy ez egy túlságosan egyszerű példa kevés neuronnal, 

94
00:06:46,918 --> 00:06:51,020
és a dolgok exponenciálisan bonyolultabbak lesznek egy valódi hálózat esetében.

95
00:06:51,700 --> 00:06:55,842
De őszintén szólva, nem sok minden változik, ha a rétegeknek több neuront adunk, 

96
00:06:55,842 --> 00:06:58,860
valójában csak néhány indexszel többet kell számon tartani.

97
00:06:59,380 --> 00:07:03,180
Ahelyett, hogy egy adott réteg aktivációja egyszerűen AL lenne, 

98
00:07:03,180 --> 00:07:07,160
egy index is jelzi, hogy az adott réteg melyik neuronjáról van szó.

99
00:07:07,160 --> 00:07:14,420
Használjuk a k betűt az L-1 réteg indexelésére, és a j betűt az L réteg indexelésére.

100
00:07:15,260 --> 00:07:19,144
A költségek esetében ismét azt nézzük, hogy mi a kívánt kimenet, 

101
00:07:19,144 --> 00:07:23,865
de ezúttal az utolsó réteg aktivációit és a kívánt kimenet közötti különbségek 

102
00:07:23,865 --> 00:07:25,180
négyzetét adjuk össze.

103
00:07:26,080 --> 00:07:30,840
Azaz, az ALj mínusz Yj négyzeteinek összegét képezzük.

104
00:07:33,040 --> 00:07:37,338
Mivel még egy adag súly van, így mindegyiknek több indexet kell kapnia, 

105
00:07:37,338 --> 00:07:40,382
hogy nyomon követhessük, hogy hol helyezkednek el, 

106
00:07:40,382 --> 00:07:44,920
ezért nevezzük a k-ik neuront a j-ik neuronnal összekötő él súlyát WLjk-nak.

107
00:07:45,620 --> 00:07:49,654
Ezek az indexek elsőre kissé fordítva tűnhetnek, de összhangban vannak azzal, 

108
00:07:49,654 --> 00:07:53,120
ahogyan az 1. rész videójában említett súlymátrixot indexelni kell.

109
00:07:53,620 --> 00:07:57,610
Ahogy korábban, most is adunk kéne egy nevet a vonatkozó súlyozott összegnek, 

110
00:07:57,610 --> 00:08:02,113
például "z", így az utolsó réteg aktivációit az erre alkalmazott speciális függvényünk, 

111
00:08:02,113 --> 00:08:04,160
például a szigmoid eredményeként kapjuk.

112
00:08:04,660 --> 00:08:08,827
Láthatjátok mire gondolok, hogy ezek lényegében ugyanazok az egyenletek, 

113
00:08:08,827 --> 00:08:13,680
mint korábban az egy neuron per réteg esetére, csak egy kicsit bonyolultabbnak tűnik.

114
00:08:15,440 --> 00:08:19,270
És valóban, a láncszabályban szereplő derivált kifejezés, amely leírja, 

115
00:08:19,270 --> 00:08:23,420
hogy a költség mennyire érzékeny egy adott súlyra, lényegében ugyanígy néz ki.

116
00:08:23,920 --> 00:08:26,840
Állj meg és gondolkodj el ezeken a képleteken, ha akarsz.

117
00:08:28,980 --> 00:08:32,938
Ami itt azonban változik, az a költség deriváltja 

118
00:08:32,938 --> 00:08:36,659
az L-1 réteg egyik aktivációjának függvényében.

119
00:08:37,780 --> 00:08:40,213
Ebben az esetben a különbség az, hogy a neuron több 

120
00:08:40,213 --> 00:08:42,880
különböző úton keresztül befolyásolja a költségfüggvényt.

121
00:08:44,680 --> 00:08:50,277
Vagyis egyrészt befolyásolja az AL0-t, amely szerepet játszik a költségfüggvényben, 

122
00:08:50,277 --> 00:08:55,807
de hatással van az AL1-re is, amely szintén szerepet játszik a költségfüggvényben, 

123
00:08:55,807 --> 00:08:57,540
és ezeket össze kell adni.

124
00:08:59,820 --> 00:09:03,040
És ez, nos... Nagyjából ennyi az egész.

125
00:09:03,500 --> 00:09:07,983
Onnantól, hogy tudod mennyire érzékeny a költségfüggvény az utolsó előtti réteg 

126
00:09:07,983 --> 00:09:12,860
aktivációira, akkor ezeket a lépéseket kell csak megismételned az előtte lévő súlyokra.

127
00:09:13,900 --> 00:09:14,960
Szóval veregesd meg a válladat!

128
00:09:15,300 --> 00:09:19,996
Ha mindezt megértetted, akkor most már mélyen belelátsz a visszaterjesztés lelkébe, 

129
00:09:19,996 --> 00:09:22,680
amely a neurális hálózatok tanulásának igáslova.

130
00:09:23,300 --> 00:09:25,949
Ezek a láncszabály-képletek megadják a deriváltakat, 

131
00:09:25,949 --> 00:09:28,900
amelyek meghatározzák a gradiens minden egyes komponensét, 

132
00:09:28,900 --> 00:09:33,300
amely segít minimalizálni a hálózat költségét azáltal, hogy ismételten lefelé lépkedünk.

133
00:09:34,300 --> 00:09:38,520
Ha hátradőlsz, és végiggondolod mindezt, akkor jó sok összetett réteget kell tudnod 

134
00:09:38,520 --> 00:09:42,740
fejben tartani, úgyhogy ne aggódj, ha időbe telik, amíg az elméd megemészti mindezt.

