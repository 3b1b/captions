1
00:00:00,000 --> 00:00:05,627
A kemény feltételezés az, hogy megnézted a 3. részt, amely

2
00:00:05,627 --> 00:00:11,160
intuitív áttekintést ad a visszaterjesztési algoritmusról.

3
00:00:11,160 --> 00:00:14,920
Itt egy kicsit formálisabbak vagyunk, és belemerülünk a vonatkozó számításba.

4
00:00:14,920 --> 00:00:18,591
Normális, hogy ez legalább egy kicsit zavaró, így a rendszeres szünet

5
00:00:18,591 --> 00:00:22,000
és töprengés mantra itt is ugyanúgy érvényes, mint bárhol máshol.

6
00:00:22,000 --> 00:00:26,005
Fő célunk, hogy bemutassuk, hogyan gondolkodnak a gépi tanulásban részt vevők

7
00:00:26,005 --> 00:00:30,164
általában a hálózatok kontextusában a számításból származó láncszabályról, amely

8
00:00:30,164 --> 00:00:34,580
másképp hat, mint a legtöbb bevezető számítástechnikai kurzus megközelítése a témához.

9
00:00:34,580 --> 00:00:37,081
Azok számára, akik nem érzik jól magukat a vonatkozó

10
00:00:37,081 --> 00:00:39,300
kalkulusban, egy egész sorozatom van a témában.

11
00:00:39,300 --> 00:00:46,780
Kezdjük egy rendkívül egyszerű hálózattal, ahol minden rétegben egyetlen neuron található.

12
00:00:46,780 --> 00:00:51,093
Ezt a hálózatot három súlyozás és három torzítás határozza meg, és célunk

13
00:00:51,093 --> 00:00:55,640
annak megértése, hogy a költségfüggvény mennyire érzékeny ezekre a változókra.

14
00:00:55,640 --> 00:00:58,467
Így tudjuk, hogy ezeknek a feltételeknek mely módosításai

15
00:00:58,467 --> 00:01:01,100
okozzák a költségfüggvény leghatékonyabb csökkentését.

16
00:01:01,100 --> 00:01:05,360
Csak az utolsó két neuron közötti kapcsolatra koncentrálunk.

17
00:01:05,360 --> 00:01:08,615
Jelöljük az utolsó neuron aktiválását L felső

18
00:01:08,615 --> 00:01:11,800
indexszel, jelezve, hogy melyik rétegben van.

19
00:01:11,800 --> 00:01:16,560
Tehát az előző neuron aktiválása AL-1.

20
00:01:16,560 --> 00:01:20,080
Ezek nem exponensek, csak egy módja annak, hogy indexeljük azt, amiről beszélünk,

21
00:01:20,080 --> 00:01:23,600
mivel a későbbiekben szeretném elmenteni az alsó indexeket a különböző indexekhez.

22
00:01:23,600 --> 00:01:28,072
Tegyük fel, hogy egy adott képzési példánál az

23
00:01:28,072 --> 00:01:33,020
utolsó aktiválás értéke y, például y lehet 0 vagy 1.

24
00:01:33,020 --> 00:01:39,040
Tehát ennek a hálózatnak a költsége egyetlen képzési példa esetében AL-y2.

25
00:01:39,040 --> 00:01:46,120
Ennek az egy képzési példának a költségét c0-val jelöljük.

26
00:01:46,120 --> 00:01:51,541
Emlékeztetőül, ezt az utolsó aktiválást egy súly határozza meg, amelyet wL-nek fogok

27
00:01:51,541 --> 00:01:57,089
nevezni, szorozva az előző neuron aktiválódásával, plusz némi torzítással, amit bL-nek

28
00:01:57,089 --> 00:01:57,600
nevezek.

29
00:01:57,600 --> 00:01:59,562
Ezután ezt valamilyen speciális nemlineáris függvényen

30
00:01:59,562 --> 00:02:01,560
keresztül pumpálja, mint például a szigmoid vagy a ReLU.

31
00:02:01,560 --> 00:02:06,051
Valójában megkönnyíti a dolgunkat, ha ennek a súlyozott összegnek külön nevet

32
00:02:06,051 --> 00:02:10,600
adunk, például z-t, ugyanazzal a felső indexszel, mint a vonatkozó aktiválások.

33
00:02:10,600 --> 00:02:16,123
Ez egy csomó kifejezés, és úgy lehet elképzelni, hogy a súlyt, az előző műveletet és a

34
00:02:16,123 --> 00:02:21,582
torzítást együtt használják a z kiszámítására, ami viszont lehetővé teszi számunkra a

35
00:02:21,582 --> 00:02:26,598
kiszámítását, ami végül egy konstans y-val együtt lehetővé teszi kiszámoljuk a

36
00:02:26,598 --> 00:02:27,360
költségeket.

37
00:02:27,360 --> 00:02:31,388
És természetesen az AL-1-et befolyásolja a saját súlya,

38
00:02:31,388 --> 00:02:35,920
elfogultsága és hasonlók, de most nem erre fogunk koncentrálni.

39
00:02:35,920 --> 00:02:38,120
Ezek mind csak számok, igaz?

40
00:02:38,120 --> 00:02:41,960
És jó lehet úgy gondolni, hogy mindegyiknek megvan a maga kis számsora.

41
00:02:41,960 --> 00:02:46,047
Első célunk annak megértése, hogy a költségfüggvény

42
00:02:46,047 --> 00:02:49,820
mennyire érzékeny a súlyunk kis változásaira wL.

43
00:02:49,820 --> 00:02:55,740
Vagy fogalmazz másképp, mi a c deriváltja wL-hez képest?

44
00:02:55,740 --> 00:02:58,671
Ha látja ezt a del w kifejezést, gondolja úgy, hogy ez egy

45
00:02:58,671 --> 00:03:01,554
apró lökést jelent w-hez, például 0-val való változtatást.

46
00:03:01,554 --> 00:03:08,820
01, és gondolja úgy, hogy ez a del c kifejezés bármit is jelent a költségekhez képest.

47
00:03:08,820 --> 00:03:10,900
Amit szeretnénk, az az arányuk.

48
00:03:10,900 --> 00:03:16,687
Elméletileg ez az apró lökés a wL felé némi lökést okoz a zL-nek, ami

49
00:03:16,687 --> 00:03:23,220
viszont némi lökést okoz az AL-nak, ami közvetlenül befolyásolja a költségeket.

50
00:03:23,220 --> 00:03:26,574
Tehát a dolgokat úgy bontjuk szét, hogy először megnézzük a

51
00:03:26,574 --> 00:03:30,097
zL-hez viszonyított apró változás és ehhez a kis w változáshoz

52
00:03:30,097 --> 00:03:33,340
viszonyított arányát, vagyis zL deriváltját wL-hez képest.

53
00:03:33,340 --> 00:03:37,703
Hasonlóképpen, akkor vegye figyelembe az AL-hoz való változás és a zL-ben

54
00:03:37,703 --> 00:03:41,949
bekövetkezett apró változás arányát, amely ezt okozta, valamint a c-hez

55
00:03:41,949 --> 00:03:45,900
való végső lökést és az AL-hez való közbenső lökést közötti arányt.

56
00:03:45,900 --> 00:03:52,159
Ez itt a láncszabály, ahol ezt a három arányt megszorozva

57
00:03:52,159 --> 00:03:57,340
megkapjuk c érzékenységét a wL kis változásaira.

58
00:03:57,340 --> 00:04:00,471
Tehát a képernyőn jelenleg nagyon sok szimbólum van, és

59
00:04:00,471 --> 00:04:03,993
szánjon egy percet, hogy megbizonyosodjon arról, hogy világos,

60
00:04:03,993 --> 00:04:07,460
mi ez, mert most a vonatkozó származékokat fogjuk kiszámítani.

61
00:04:07,460 --> 00:04:14,220
A c deriváltja az AL-hez viszonyítva 2AL-y.

62
00:04:14,220 --> 00:04:18,918
Ez azt jelenti, hogy mérete arányos a hálózat kimenete és a kívánt dolog

63
00:04:18,918 --> 00:04:23,681
közötti különbséggel, tehát ha ez a kimenet nagyon eltérő volt, akkor még

64
00:04:23,681 --> 00:04:28,380
az enyhe változtatások is nagy hatással vannak a végső költségfüggvényre.

65
00:04:28,380 --> 00:04:32,765
Az AL zL-hez viszonyított deriváltja csak a szigmoid függvényünk

66
00:04:32,765 --> 00:04:37,420
deriváltja, vagy bármilyen nemlinearitás, amelyet használni szeretne.

67
00:04:37,420 --> 00:04:46,180
A zL deriváltja wL-hez viszonyítva AL-1 lesz.

68
00:04:46,180 --> 00:04:50,230
Nem tudom, ti hogy vagytok vele, de azt hiszem, könnyű beleragadni a képletekbe

69
00:04:50,230 --> 00:04:54,180
anélkül, hogy egy pillanatra is hátradőlne, és emlékezne, mit jelentenek ezek.

70
00:04:54,180 --> 00:04:58,644
Ennek az utolsó származéknak az esetében, hogy a súlyhoz való kis lökések milyen

71
00:04:58,644 --> 00:05:03,220
mértékben befolyásolták az utolsó réteget, attól függ, milyen erős az előző neuron.

72
00:05:03,220 --> 00:05:09,320
Ne feledje, itt jön a képbe a neuronok-az-együtt tüzel-huzal-összeköttetés ötlet.

73
00:05:09,320 --> 00:05:16,580
És mindez a wL vonatkozásában csak egy konkrét képzési példa költségének deriváltja.

74
00:05:16,580 --> 00:05:20,439
Mivel a teljes költségfüggvény magában foglalja az összes költségnek a

75
00:05:20,439 --> 00:05:24,462
sok különböző betanítási példában való együttes átlagolását, a származéka

76
00:05:24,462 --> 00:05:28,540
megköveteli ennek a kifejezésnek az átlagolását az összes képzési példában.

77
00:05:28,540 --> 00:05:34,660
Természetesen ez csak egy komponense a gradiensvektornak, amely a költségfüggvény

78
00:05:34,660 --> 00:05:40,780
parciális deriváltjaiból épül fel, tekintettel az összes súlyozásra és torzításra.

79
00:05:40,780 --> 00:05:43,592
De bár ez csak egy a sok részleges származék közül,

80
00:05:43,592 --> 00:05:46,460
amelyekre szükségünk van, ez a munka több mint 50%-a.

81
00:05:46,460 --> 00:05:50,300
A torzításra való érzékenység például majdnem azonos.

82
00:05:50,300 --> 00:05:58,980
Csak ki kell cserélnünk ezt a del z del w kifejezést egy del z del b-re.

83
00:05:58,980 --> 00:06:04,700
És ha megnézzük a vonatkozó képletet, a származéka 1 lesz.

84
00:06:04,700 --> 00:06:10,524
Illetve, és itt jön be a visszafelé terjedés ötlete, láthatjuk, hogy

85
00:06:10,524 --> 00:06:16,180
ez a költségfüggvény mennyire érzékeny az előző réteg aktiválására.

86
00:06:16,180 --> 00:06:20,760
Ugyanis ez a kezdeti derivált a láncszabály kifejezésben,

87
00:06:20,760 --> 00:06:25,420
a z érzékenysége az előző aktiválásra, a wL súlynak jön ki.

88
00:06:25,420 --> 00:06:29,823
És még egyszer, bár nem leszünk képesek közvetlenül befolyásolni az

89
00:06:29,823 --> 00:06:34,226
előző réteg aktiválását, hasznos nyomon követni, mert most már csak

90
00:06:34,226 --> 00:06:38,953
ismételhetjük ugyanezt a láncszabály-ötletet visszafelé, hogy meglássuk,

91
00:06:38,953 --> 00:06:43,680
mennyire érzékeny a költségfüggvény korábbi súlyok és korábbi torzítások.

92
00:06:43,680 --> 00:06:47,500
És azt gondolhatja, hogy ez egy túlságosan egyszerű példa, mivel minden rétegnek van egy

93
00:06:47,500 --> 00:06:51,320
neuronja, és a dolgok exponenciálisan bonyolultabbak lesznek egy valódi hálózat esetében.

94
00:06:51,320 --> 00:06:55,320
De őszintén szólva, nem sok változás történik, ha több neuront

95
00:06:55,320 --> 00:06:59,320
adunk a rétegeknek, valójában csak néhány indexet kell követni.

96
00:06:59,320 --> 00:07:03,675
Ahelyett, hogy egy adott réteg aktiválása egyszerűen AL lenne, annak egy alsó

97
00:07:03,675 --> 00:07:07,920
indexe is lesz, amely jelzi, hogy az adott réteg melyik neuronjáról van szó.

98
00:07:07,920 --> 00:07:15,280
Használjuk a k betűt az L-1 réteg, a j betűt pedig az L réteg indexeléséhez.

99
00:07:15,280 --> 00:07:20,803
A költségekhez ismét megnézzük, hogy mi a kívánt kimenet, de ezúttal összeadjuk

100
00:07:20,803 --> 00:07:26,120
az utolsó rétegaktiválások és a kívánt kimenet közötti különbségek négyzetét.

101
00:07:26,120 --> 00:07:33,280
Vagyis veszel egy összeget ALj mínusz yj négyzetével.

102
00:07:33,280 --> 00:07:37,433
Mivel sokkal több a súlyozás, mindegyiknek több indexnek kell

103
00:07:37,433 --> 00:07:41,519
lennie, hogy nyomon tudja követni, hol van, ezért nevezzük a

104
00:07:41,519 --> 00:07:45,740
k-adik neuront a j-edik neuronnal összekötő él súlyát WLjk-nek.

105
00:07:45,740 --> 00:07:49,719
Ezek az indexek eleinte kissé elmaradottaknak tűnhetnek, de ez összhangban van

106
00:07:49,719 --> 00:07:53,800
azzal, hogyan indexelné a súlymátrixot, amelyről az 1. rész videójában beszéltem.

107
00:07:53,800 --> 00:07:57,467
Csakúgy, mint korábban, továbbra is jó nevet adni a megfelelő

108
00:07:57,467 --> 00:08:01,430
súlyozott összegnek, például z-nek, így az utolsó réteg aktiválása

109
00:08:01,430 --> 00:08:04,980
csak a speciális függvény, mint a z-re alkalmazott szigmoid.

110
00:08:04,980 --> 00:08:10,457
Láthatja, mire gondolok, ahol ezek lényegében ugyanazok az egyenletek, mint korábban

111
00:08:10,457 --> 00:08:15,420
a rétegenkénti egy neuron esetében, csak ez egy kicsit bonyolultabbnak tűnik.

112
00:08:15,420 --> 00:08:19,362
És valóban, a láncszabály derivált kifejezés, amely leírja, hogy a

113
00:08:19,362 --> 00:08:23,540
költség mennyire érzékeny egy adott súlyra, lényegében ugyanúgy néz ki.

114
00:08:23,540 --> 00:08:29,420
Meghagyom neked, hogy állj meg, és gondold át mindegyik kifejezést, ha akarod.

115
00:08:29,420 --> 00:08:37,820
Ami azonban itt változik, az az L-1 réteg egyik aktiválásának költségének deriváltja.

116
00:08:37,820 --> 00:08:40,535
Ebben az esetben a különbség az, hogy a neuron

117
00:08:40,535 --> 00:08:43,540
több különböző úton befolyásolja a költségfüggvényt.

118
00:08:43,540 --> 00:08:49,228
Vagyis egyrészt befolyásolja az AL0-t, ami a költségfüggvényben

119
00:08:49,228 --> 00:08:54,562
játszik szerepet, de hatással van az AL1-re is, ami szintén

120
00:08:54,562 --> 00:09:00,340
szerepet játszik a költségfüggvényben, és ezeket össze kell adni.

121
00:09:00,340 --> 00:09:03,680
És nagyjából ennyi.

122
00:09:03,680 --> 00:09:08,858
Ha tudja, hogy a költségfüggvény mennyire érzékeny az utolsó előtti réteg aktiválásaira,

123
00:09:08,858 --> 00:09:13,920
megismételheti a folyamatot az adott rétegbe betáplált összes súlyozásra és torzításra.

124
00:09:13,920 --> 00:09:15,420
Szóval veregesd meg magad!

125
00:09:15,420 --> 00:09:19,864
Ha mindennek van értelme, akkor most mélyen belenézett a backpropagation

126
00:09:19,864 --> 00:09:23,700
szívébe, a neurális hálózatok tanulási folyamatának igáslójába.

127
00:09:23,700 --> 00:09:27,544
Ezek a láncszabály-kifejezések megadják azokat a származékokat, amelyek

128
00:09:27,544 --> 00:09:30,908
meghatározzák a gradiens egyes komponenseit, amelyek segítenek

129
00:09:30,908 --> 00:09:35,020
minimalizálni a hálózat költségeit azáltal, hogy ismételten lefelé lépkednek.

130
00:09:35,020 --> 00:09:36,340
Ha hátradől, és végiggondolja az egészet, akkor ez egy csomó

131
00:09:36,340 --> 00:09:37,596
összetett réteg körül kell járnia az elméjének, szóval ne

132
00:09:37,596 --> 00:09:38,960
aggódjon, ha időbe telik, amíg az elméje megemészti az egészet.

