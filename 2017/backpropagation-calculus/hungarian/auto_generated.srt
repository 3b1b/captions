1
00:00:04,020 --> 00:00:06,692
A kemény feltételezés az, hogy megnézted a 3. részt, 

2
00:00:06,692 --> 00:00:09,920
amely intuitív áttekintést ad a visszaterjesztési algoritmusról.

3
00:00:11,040 --> 00:00:14,220
Itt egy kicsit formálisabbak vagyunk, és belemerülünk a vonatkozó számításba.

4
00:00:14,820 --> 00:00:18,231
Normális, hogy ez legalább egy kicsit zavaró, így a rendszeres szünet 

5
00:00:18,231 --> 00:00:21,400
és töprengés mantra itt is ugyanúgy érvényes, mint bárhol máshol.

6
00:00:21,940 --> 00:00:25,664
Fő célunk, hogy bemutassuk, hogyan gondolkodnak a gépi tanulásban részt vevők 

7
00:00:25,664 --> 00:00:29,246
általában a hálózatok kontextusában a számításból származó láncszabályról, 

8
00:00:29,246 --> 00:00:33,162
amely másképp hat, mint a legtöbb bevezető számítástechnikai kurzus megközelítése 

9
00:00:33,162 --> 00:00:33,640
a témához.

10
00:00:34,340 --> 00:00:37,244
Azok számára, akik nem érzik jól magukat a vonatkozó kalkulusban, 

11
00:00:37,244 --> 00:00:38,740
egy egész sorozatom van a témában.

12
00:00:39,960 --> 00:00:46,020
Kezdjük egy rendkívül egyszerű hálózattal, ahol minden rétegben egyetlen neuron található.

13
00:00:46,320 --> 00:00:49,924
Ezt a hálózatot három súlyozás és három torzítás határozza meg, 

14
00:00:49,924 --> 00:00:54,880
és célunk annak megértése, hogy a költségfüggvény mennyire érzékeny ezekre a változókra.

15
00:00:55,419 --> 00:00:58,993
Így tudjuk, hogy ezeknek a feltételeknek mely módosításai 

16
00:00:58,993 --> 00:01:02,320
okozzák a költségfüggvény leghatékonyabb csökkentését.

17
00:01:02,320 --> 00:01:04,840
Csak az utolsó két neuron közötti kapcsolatra koncentrálunk.

18
00:01:05,980 --> 00:01:09,349
Jelöljük az utolsó neuron aktiválását L felső indexszel, 

19
00:01:09,349 --> 00:01:11,360
jelezve, hogy melyik rétegben van.

20
00:01:11,680 --> 00:01:15,560
Tehát az előző neuron aktiválása AL-1.

21
00:01:16,360 --> 00:01:19,700
Ezek nem exponensek, csak egy módja annak, hogy indexeljük azt, amiről beszélünk, 

22
00:01:19,700 --> 00:01:23,040
mivel a későbbiekben szeretném elmenteni az alsó indexeket a különböző indexekhez.

23
00:01:23,720 --> 00:01:30,043
Tegyük fel, hogy egy adott képzési példánál az utolsó aktiválás értéke y, 

24
00:01:30,043 --> 00:01:32,180
például y lehet 0 vagy 1.

25
00:01:32,840 --> 00:01:39,240
Tehát ennek a hálózatnak a költsége egyetlen képzési példa esetében AL-y2.

26
00:01:40,260 --> 00:01:44,380
Ennek az egy képzési példának a költségét c0-val jelöljük.

27
00:01:45,900 --> 00:01:50,060
Emlékeztetőül, ezt az utolsó aktiválást egy súly határozza meg, 

28
00:01:50,060 --> 00:01:54,740
amelyet wL-nek fogok nevezni, szorozva az előző neuron aktiválódásával, 

29
00:01:54,740 --> 00:01:57,600
plusz némi torzítással, amit bL-nek nevezek.

30
00:01:57,600 --> 00:02:00,113
Ezután ezt valamilyen speciális nemlineáris függvényen keresztül pumpálja, 

31
00:02:00,113 --> 00:02:01,320
mint például a szigmoid vagy a ReLU.

32
00:02:01,800 --> 00:02:05,871
Valójában megkönnyíti a dolgunkat, ha ennek a súlyozott összegnek külön nevet adunk, 

33
00:02:05,871 --> 00:02:09,320
például z-t, ugyanazzal a felső indexszel, mint a vonatkozó aktiválások.

34
00:02:10,380 --> 00:02:13,983
Ez egy csomó kifejezés, és úgy lehet elképzelni, hogy a súlyt, 

35
00:02:13,983 --> 00:02:18,044
az előző műveletet és a torzítást együtt használják a z kiszámítására, 

36
00:02:18,044 --> 00:02:21,075
ami viszont lehetővé teszi számunkra a kiszámítását, 

37
00:02:21,075 --> 00:02:25,480
ami végül egy konstans y-val együtt lehetővé teszi kiszámoljuk a költségeket.

38
00:02:27,340 --> 00:02:30,972
És természetesen az AL-1-et befolyásolja a saját súlya, 

39
00:02:30,972 --> 00:02:35,060
elfogultsága és hasonlók, de most nem erre fogunk koncentrálni.

40
00:02:35,700 --> 00:02:37,620
Ezek mind csak számok, igaz?

41
00:02:38,060 --> 00:02:41,040
És jó lehet úgy gondolni, hogy mindegyiknek megvan a maga kis számsora.

42
00:02:41,720 --> 00:02:45,505
Első célunk annak megértése, hogy a költségfüggvény 

43
00:02:45,505 --> 00:02:49,000
mennyire érzékeny a súlyunk kis változásaira wL.

44
00:02:49,540 --> 00:02:54,860
Vagy fogalmazz másképp, mi a c deriváltja wL-hez képest?

45
00:02:55,600 --> 00:03:00,817
Ha látja ezt a del w kifejezést, gondolja úgy, hogy ez egy apró lökést jelent w-hez, 

46
00:03:00,817 --> 00:03:04,070
például 0-val való változtatást.01, és gondolja úgy, 

47
00:03:04,070 --> 00:03:08,060
hogy ez a del c kifejezés bármit is jelent a költségekhez képest.

48
00:03:08,060 --> 00:03:10,220
Amit szeretnénk, az az arányuk.

49
00:03:11,260 --> 00:03:15,680
Elméletileg ez az apró lökés a wL felé némi lökést okoz a zL-nek, 

50
00:03:15,680 --> 00:03:21,240
ami viszont némi lökést okoz az AL-nak, ami közvetlenül befolyásolja a költségeket.

51
00:03:23,120 --> 00:03:26,461
Tehát a dolgokat úgy bontjuk szét, hogy először megnézzük a 

52
00:03:26,461 --> 00:03:31,195
zL-hez viszonyított apró változás és ehhez a kis w változáshoz viszonyított arányát, 

53
00:03:31,195 --> 00:03:33,200
vagyis zL deriváltját wL-hez képest.

54
00:03:33,200 --> 00:03:37,181
Hasonlóképpen, akkor vegye figyelembe az AL-hoz való változás és a zL-ben 

55
00:03:37,181 --> 00:03:40,140
bekövetkezett apró változás arányát, amely ezt okozta, 

56
00:03:40,140 --> 00:03:44,660
valamint a c-hez való végső lökést és az AL-hez való közbenső lökést közötti arányt.

57
00:03:45,740 --> 00:03:50,883
Ez itt a láncszabály, ahol ezt a három arányt megszorozva 

58
00:03:50,883 --> 00:03:55,140
megkapjuk c érzékenységét a wL kis változásaira.

59
00:03:56,880 --> 00:03:59,620
Tehát a képernyőn jelenleg nagyon sok szimbólum van, 

60
00:03:59,620 --> 00:04:03,033
és szánjon egy percet, hogy megbizonyosodjon arról, hogy világos, 

61
00:04:03,033 --> 00:04:06,240
mi ez, mert most a vonatkozó származékokat fogjuk kiszámítani.

62
00:04:07,440 --> 00:04:14,180
A c deriváltja az AL-hez viszonyítva 2AL-y.

63
00:04:14,180 --> 00:04:18,480
Ez azt jelenti, hogy mérete arányos a hálózat kimenete és a kívánt dolog 

64
00:04:18,480 --> 00:04:22,250
közötti különbséggel, tehát ha ez a kimenet nagyon eltérő volt, 

65
00:04:22,250 --> 00:04:27,140
akkor még az enyhe változtatások is nagy hatással vannak a végső költségfüggvényre.

66
00:04:27,840 --> 00:04:33,344
Az AL zL-hez viszonyított deriváltja csak a szigmoid függvényünk deriváltja, 

67
00:04:33,344 --> 00:04:37,420
vagy bármilyen nemlinearitás, amelyet használni szeretne.

68
00:04:37,420 --> 00:04:46,160
A zL deriváltja wL-hez viszonyítva AL-1 lesz.

69
00:04:46,160 --> 00:04:50,249
Nem tudom, ti hogy vagytok vele, de azt hiszem, könnyű beleragadni a képletekbe anélkül, 

70
00:04:50,249 --> 00:04:53,420
hogy egy pillanatra is hátradőlne, és emlékezne, mit jelentenek ezek.

71
00:04:53,920 --> 00:04:58,315
Ennek az utolsó származéknak az esetében, hogy a súlyhoz való kis lökések milyen 

72
00:04:58,315 --> 00:05:02,820
mértékben befolyásolták az utolsó réteget, attól függ, milyen erős az előző neuron.

73
00:05:03,380 --> 00:05:08,280
Ne feledje, itt jön a képbe a neuronok-az-együtt tüzel-huzal-összeköttetés ötlet.

74
00:05:09,200 --> 00:05:15,720
És mindez a wL vonatkozásában csak egy konkrét képzési példa költségének deriváltja.

75
00:05:16,440 --> 00:05:20,383
Mivel a teljes költségfüggvény magában foglalja az összes költségnek a 

76
00:05:20,383 --> 00:05:23,772
sok különböző betanítási példában való együttes átlagolását, 

77
00:05:23,772 --> 00:05:28,660
a származéka megköveteli ennek a kifejezésnek az átlagolását az összes képzési példában.

78
00:05:28,660 --> 00:05:32,055
Természetesen ez csak egy komponense a gradiensvektornak, 

79
00:05:32,055 --> 00:05:35,508
amely a költségfüggvény parciális deriváltjaiból épül fel, 

80
00:05:35,508 --> 00:05:38,260
tekintettel az összes súlyozásra és torzításra.

81
00:05:40,640 --> 00:05:42,928
De bár ez csak egy a sok részleges származék közül, 

82
00:05:42,928 --> 00:05:45,260
amelyekre szükségünk van, ez a munka több mint 50%-a.

83
00:05:46,340 --> 00:05:49,720
A torzításra való érzékenység például majdnem azonos.

84
00:05:50,040 --> 00:05:55,020
Csak ki kell cserélnünk ezt a del z del w kifejezést egy del z del b-re.

85
00:05:58,420 --> 00:06:02,400
És ha megnézzük a vonatkozó képletet, a származéka 1 lesz.

86
00:06:06,140 --> 00:06:10,657
Illetve, és itt jön be a visszafelé terjedés ötlete, láthatjuk, 

87
00:06:10,657 --> 00:06:15,740
hogy ez a költségfüggvény mennyire érzékeny az előző réteg aktiválására.

88
00:06:15,740 --> 00:06:20,657
Ugyanis ez a kezdeti derivált a láncszabály kifejezésben, 

89
00:06:20,657 --> 00:06:25,660
a z érzékenysége az előző aktiválásra, a wL súlynak jön ki.

90
00:06:26,640 --> 00:06:30,449
És még egyszer, bár nem leszünk képesek közvetlenül befolyásolni az 

91
00:06:30,449 --> 00:06:33,195
előző réteg aktiválását, hasznos nyomon követni, 

92
00:06:33,195 --> 00:06:37,453
mert most már csak ismételhetjük ugyanezt a láncszabály-ötletet visszafelé, 

93
00:06:37,453 --> 00:06:42,440
hogy meglássuk, mennyire érzékeny a költségfüggvény korábbi súlyok és korábbi torzítások.

94
00:06:43,180 --> 00:06:45,778
És azt gondolhatja, hogy ez egy túlságosan egyszerű példa, 

95
00:06:45,778 --> 00:06:49,434
mivel minden rétegnek van egy neuronja, és a dolgok exponenciálisan bonyolultabbak 

96
00:06:49,434 --> 00:06:51,020
lesznek egy valódi hálózat esetében.

97
00:06:51,700 --> 00:06:54,370
De őszintén szólva, nem sok változás történik, 

98
00:06:54,370 --> 00:06:58,860
ha több neuront adunk a rétegeknek, valójában csak néhány indexet kell követni.

99
00:06:59,380 --> 00:07:02,562
Ahelyett, hogy egy adott réteg aktiválása egyszerűen AL lenne, 

100
00:07:02,562 --> 00:07:06,149
annak egy alsó indexe is lesz, amely jelzi, hogy az adott réteg melyik 

101
00:07:06,149 --> 00:07:07,160
neuronjáról van szó.

102
00:07:07,160 --> 00:07:14,420
Használjuk a k betűt az L-1 réteg, a j betűt pedig az L réteg indexeléséhez.

103
00:07:15,260 --> 00:07:18,924
A költségekhez ismét megnézzük, hogy mi a kívánt kimenet, 

104
00:07:18,924 --> 00:07:23,789
de ezúttal összeadjuk az utolsó rétegaktiválások és a kívánt kimenet közötti 

105
00:07:23,789 --> 00:07:25,180
különbségek négyzetét.

106
00:07:26,080 --> 00:07:30,840
Vagyis veszel egy összeget ALj mínusz yj négyzetével.

107
00:07:33,040 --> 00:07:37,510
Mivel sokkal több a súlyozás, mindegyiknek több indexnek kell lennie, 

108
00:07:37,510 --> 00:07:41,343
hogy nyomon tudja követni, hol van, ezért nevezzük a k-adik 

109
00:07:41,343 --> 00:07:44,920
neuront a j-edik neuronnal összekötő él súlyát WLjk-nek.

110
00:07:45,620 --> 00:07:49,651
Ezek az indexek eleinte kissé elmaradottaknak tűnhetnek, de ez összhangban van azzal, 

111
00:07:49,651 --> 00:07:53,120
hogyan indexelné a súlymátrixot, amelyről az 1. rész videójában beszéltem.

112
00:07:53,620 --> 00:07:58,248
Csakúgy, mint korábban, továbbra is jó nevet adni a megfelelő súlyozott összegnek, 

113
00:07:58,248 --> 00:08:02,319
például z-nek, így az utolsó réteg aktiválása csak a speciális függvény, 

114
00:08:02,319 --> 00:08:04,160
mint a z-re alkalmazott szigmoid.

115
00:08:04,660 --> 00:08:08,613
Láthatja, mire gondolok, ahol ezek lényegében ugyanazok az egyenletek, 

116
00:08:08,613 --> 00:08:11,397
mint korábban a rétegenkénti egy neuron esetében, 

117
00:08:11,397 --> 00:08:13,680
csak ez egy kicsit bonyolultabbnak tűnik.

118
00:08:15,440 --> 00:08:18,909
És valóban, a láncszabály derivált kifejezés, amely leírja, 

119
00:08:18,909 --> 00:08:23,420
hogy a költség mennyire érzékeny egy adott súlyra, lényegében ugyanúgy néz ki.

120
00:08:23,920 --> 00:08:26,840
Meghagyom neked, hogy állj meg, és gondold át mindegyik kifejezést, ha akarod.

121
00:08:28,979 --> 00:08:36,659
Ami azonban itt változik, az az L-1 réteg egyik aktiválásának költségének deriváltja.

122
00:08:37,780 --> 00:08:40,201
Ebben az esetben a különbség az, hogy a neuron 

123
00:08:40,201 --> 00:08:42,880
több különböző úton befolyásolja a költségfüggvényt.

124
00:08:44,680 --> 00:08:50,259
Vagyis egyrészt befolyásolja az AL0-t, ami a költségfüggvényben játszik szerepet, 

125
00:08:50,259 --> 00:08:55,770
de hatással van az AL1-re is, ami szintén szerepet játszik a költségfüggvényben, 

126
00:08:55,770 --> 00:08:57,540
és ezeket össze kell adni.

127
00:08:59,820 --> 00:09:03,040
És nagyjából ennyi.

128
00:09:03,500 --> 00:09:08,233
Ha tudja, hogy a költségfüggvény mennyire érzékeny az utolsó előtti réteg aktiválásaira, 

129
00:09:08,233 --> 00:09:12,860
megismételheti a folyamatot az adott rétegbe betáplált összes súlyozásra és torzításra.

130
00:09:13,900 --> 00:09:14,960
Szóval veregesd meg magad!

131
00:09:15,300 --> 00:09:19,749
Ha mindennek van értelme, akkor most mélyen belenézett a backpropagation szívébe, 

132
00:09:19,749 --> 00:09:22,680
a neurális hálózatok tanulási folyamatának igáslójába.

133
00:09:23,300 --> 00:09:26,318
Ezek a láncszabály-kifejezések megadják azokat a származékokat, 

134
00:09:26,318 --> 00:09:28,818
amelyek meghatározzák a gradiens egyes komponenseit, 

135
00:09:28,818 --> 00:09:31,743
amelyek segítenek minimalizálni a hálózat költségeit azáltal, 

136
00:09:31,743 --> 00:09:33,300
hogy ismételten lefelé lépkednek.

137
00:09:34,300 --> 00:09:37,128
Ha hátradől, és végiggondolja az egészet, akkor ez egy csomó 

138
00:09:37,128 --> 00:09:40,282
összetett réteg körül kell járnia az elméjének, szóval ne aggódjon, 

139
00:09:40,282 --> 00:09:42,740
ha időbe telik, amíg az elméje megemészti az egészet.

