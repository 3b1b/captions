1
00:00:00,000 --> 00:00:05,055
A kemény feltételezés az, hogy megnézted a 3. részt,

2
00:00:05,055 --> 00:00:11,160
amely intuitív áttekintést ad a visszaterjesztési algoritmusról.

3
00:00:11,160 --> 00:00:14,920
Itt egy kicsit formálisabbak vagyunk, és belemerülünk a vonatkozó számításba.

4
00:00:14,920 --> 00:00:18,591
Normális, hogy ez legalább egy kicsit zavaró, így a rendszeres szünet

5
00:00:18,591 --> 00:00:22,000
és töprengés mantra itt is ugyanúgy érvényes, mint bárhol máshol.

6
00:00:22,000 --> 00:00:26,005
Fő célunk, hogy bemutassuk, hogyan gondolkodnak a gépi tanulásban részt vevők

7
00:00:26,005 --> 00:00:29,856
általában a hálózatok kontextusában a számításból származó láncszabályról,

8
00:00:29,856 --> 00:00:34,066
amely másképp hat, mint a legtöbb bevezető számítástechnikai kurzus megközelítése

9
00:00:34,066 --> 00:00:34,580
a témához.

10
00:00:34,580 --> 00:00:37,695
Azok számára, akik nem érzik jól magukat a vonatkozó kalkulusban,

11
00:00:37,695 --> 00:00:39,300
egy egész sorozatom van a témában.

12
00:00:39,300 --> 00:00:46,780
Kezdjük egy rendkívül egyszerű hálózattal, ahol minden rétegben egyetlen neuron található.

13
00:00:46,780 --> 00:00:50,510
Ezt a hálózatot három súlyozás és három torzítás határozza meg,

14
00:00:50,510 --> 00:00:55,640
és célunk annak megértése, hogy a költségfüggvény mennyire érzékeny ezekre a változókra.

15
00:00:55,640 --> 00:00:58,467
Így tudjuk, hogy ezeknek a feltételeknek mely módosításai

16
00:00:58,467 --> 00:01:01,100
okozzák a költségfüggvény leghatékonyabb csökkentését.

17
00:01:01,100 --> 00:01:05,360
Csak az utolsó két neuron közötti kapcsolatra koncentrálunk.

18
00:01:05,360 --> 00:01:09,393
Jelöljük az utolsó neuron aktiválását L felső indexszel,

19
00:01:09,393 --> 00:01:11,800
jelezve, hogy melyik rétegben van.

20
00:01:11,800 --> 00:01:16,560
Tehát az előző neuron aktiválása AL-1.

21
00:01:16,560 --> 00:01:20,080
Ezek nem exponensek, csak egy módja annak, hogy indexeljük azt, amiről beszélünk,

22
00:01:20,080 --> 00:01:23,600
mivel a későbbiekben szeretném elmenteni az alsó indexeket a különböző indexekhez.

23
00:01:23,600 --> 00:01:30,641
Tegyük fel, hogy egy adott képzési példánál az utolsó aktiválás értéke y,

24
00:01:30,641 --> 00:01:33,020
például y lehet 0 vagy 1.

25
00:01:33,020 --> 00:01:39,040
Tehát ennek a hálózatnak a költsége egyetlen képzési példa esetében AL-y2.

26
00:01:39,040 --> 00:01:46,120
Ennek az egy képzési példának a költségét c0-val jelöljük.

27
00:01:46,120 --> 00:01:50,201
Emlékeztetőül, ezt az utolsó aktiválást egy súly határozza meg,

28
00:01:50,201 --> 00:01:54,793
amelyet wL-nek fogok nevezni, szorozva az előző neuron aktiválódásával,

29
00:01:54,793 --> 00:01:57,600
plusz némi torzítással, amit bL-nek nevezek.

30
00:01:57,600 --> 00:02:00,275
Ezután ezt valamilyen speciális nemlineáris függvényen keresztül pumpálja,

31
00:02:00,275 --> 00:02:01,560
mint például a szigmoid vagy a ReLU.

32
00:02:01,560 --> 00:02:06,454
Valójában megkönnyíti a dolgunkat, ha ennek a súlyozott összegnek külön nevet adunk,

33
00:02:06,454 --> 00:02:10,600
például z-t, ugyanazzal a felső indexszel, mint a vonatkozó aktiválások.

34
00:02:10,600 --> 00:02:14,599
Ez egy csomó kifejezés, és úgy lehet elképzelni, hogy a súlyt,

35
00:02:14,599 --> 00:02:19,106
az előző műveletet és a torzítást együtt használják a z kiszámítására,

36
00:02:19,106 --> 00:02:22,471
ami viszont lehetővé teszi számunkra a kiszámítását,

37
00:02:22,471 --> 00:02:27,360
ami végül egy konstans y-val együtt lehetővé teszi kiszámoljuk a költségeket.

38
00:02:27,360 --> 00:02:31,388
És természetesen az AL-1-et befolyásolja a saját súlya,

39
00:02:31,388 --> 00:02:35,920
elfogultsága és hasonlók, de most nem erre fogunk koncentrálni.

40
00:02:35,920 --> 00:02:38,120
Ezek mind csak számok, igaz?

41
00:02:38,120 --> 00:02:41,960
És jó lehet úgy gondolni, hogy mindegyiknek megvan a maga kis számsora.

42
00:02:41,960 --> 00:02:46,047
Első célunk annak megértése, hogy a költségfüggvény

43
00:02:46,047 --> 00:02:49,820
mennyire érzékeny a súlyunk kis változásaira wL.

44
00:02:49,820 --> 00:02:55,740
Vagy fogalmazz másképp, mi a c deriváltja wL-hez képest?

45
00:02:55,740 --> 00:03:01,216
Ha látja ezt a del w kifejezést, gondolja úgy, hogy ez egy apró lökést jelent w-hez,

46
00:03:01,216 --> 00:03:04,631
például 0-val való változtatást.01, és gondolja úgy,

47
00:03:04,631 --> 00:03:08,820
hogy ez a del c kifejezés bármit is jelent a költségekhez képest.

48
00:03:08,820 --> 00:03:10,900
Amit szeretnénk, az az arányuk.

49
00:03:10,900 --> 00:03:16,357
Elméletileg ez az apró lökés a wL felé némi lökést okoz a zL-nek,

50
00:03:16,357 --> 00:03:23,220
ami viszont némi lökést okoz az AL-nak, ami közvetlenül befolyásolja a költségeket.

51
00:03:23,220 --> 00:03:26,574
Tehát a dolgokat úgy bontjuk szét, hogy először megnézzük a

52
00:03:26,574 --> 00:03:31,327
zL-hez viszonyított apró változás és ehhez a kis w változáshoz viszonyított arányát,

53
00:03:31,327 --> 00:03:33,340
vagyis zL deriváltját wL-hez képest.

54
00:03:33,340 --> 00:03:37,703
Hasonlóképpen, akkor vegye figyelembe az AL-hoz való változás és a zL-ben

55
00:03:37,703 --> 00:03:40,946
bekövetkezett apró változás arányát, amely ezt okozta,

56
00:03:40,946 --> 00:03:45,900
valamint a c-hez való végső lökést és az AL-hez való közbenső lökést közötti arányt.

57
00:03:45,900 --> 00:03:52,159
Ez itt a láncszabály, ahol ezt a három arányt megszorozva

58
00:03:52,159 --> 00:03:57,340
megkapjuk c érzékenységét a wL kis változásaira.

59
00:03:57,340 --> 00:04:00,303
Tehát a képernyőn jelenleg nagyon sok szimbólum van,

60
00:04:00,303 --> 00:04:03,993
és szánjon egy percet, hogy megbizonyosodjon arról, hogy világos,

61
00:04:03,993 --> 00:04:07,460
mi ez, mert most a vonatkozó származékokat fogjuk kiszámítani.

62
00:04:07,460 --> 00:04:14,220
A c deriváltja az AL-hez viszonyítva 2AL-y.

63
00:04:14,220 --> 00:04:18,918
Ez azt jelenti, hogy mérete arányos a hálózat kimenete és a kívánt dolog

64
00:04:18,918 --> 00:04:23,037
közötti különbséggel, tehát ha ez a kimenet nagyon eltérő volt,

65
00:04:23,037 --> 00:04:28,380
akkor még az enyhe változtatások is nagy hatással vannak a végső költségfüggvényre.

66
00:04:28,380 --> 00:04:33,574
Az AL zL-hez viszonyított deriváltja csak a szigmoid függvényünk deriváltja,

67
00:04:33,574 --> 00:04:37,420
vagy bármilyen nemlinearitás, amelyet használni szeretne.

68
00:04:37,420 --> 00:04:46,180
A zL deriváltja wL-hez viszonyítva AL-1 lesz.

69
00:04:46,180 --> 00:04:50,686
Nem tudom, ti hogy vagytok vele, de azt hiszem, könnyű beleragadni a képletekbe anélkül,

70
00:04:50,686 --> 00:04:54,180
hogy egy pillanatra is hátradőlne, és emlékezne, mit jelentenek ezek.

71
00:04:54,180 --> 00:04:58,644
Ennek az utolsó származéknak az esetében, hogy a súlyhoz való kis lökések milyen

72
00:04:58,644 --> 00:05:03,220
mértékben befolyásolták az utolsó réteget, attól függ, milyen erős az előző neuron.

73
00:05:03,220 --> 00:05:09,320
Ne feledje, itt jön a képbe a neuronok-az-együtt tüzel-huzal-összeköttetés ötlet.

74
00:05:09,320 --> 00:05:16,580
És mindez a wL vonatkozásában csak egy konkrét képzési példa költségének deriváltja.

75
00:05:16,580 --> 00:05:20,439
Mivel a teljes költségfüggvény magában foglalja az összes költségnek a

76
00:05:20,439 --> 00:05:23,756
sok különböző betanítási példában való együttes átlagolását,

77
00:05:23,756 --> 00:05:28,540
a származéka megköveteli ennek a kifejezésnek az átlagolását az összes képzési példában.

78
00:05:28,540 --> 00:05:32,868
Természetesen ez csak egy komponense a gradiensvektornak,

79
00:05:32,868 --> 00:05:37,272
amely a költségfüggvény parciális deriváltjaiból épül fel,

80
00:05:37,272 --> 00:05:40,780
tekintettel az összes súlyozásra és torzításra.

81
00:05:40,780 --> 00:05:43,592
De bár ez csak egy a sok részleges származék közül,

82
00:05:43,592 --> 00:05:46,460
amelyekre szükségünk van, ez a munka több mint 50%-a.

83
00:05:46,460 --> 00:05:50,300
A torzításra való érzékenység például majdnem azonos.

84
00:05:50,300 --> 00:05:58,980
Csak ki kell cserélnünk ezt a del z del w kifejezést egy del z del b-re.

85
00:05:58,980 --> 00:06:04,700
És ha megnézzük a vonatkozó képletet, a származéka 1 lesz.

86
00:06:04,700 --> 00:06:10,102
Illetve, és itt jön be a visszafelé terjedés ötlete, láthatjuk,

87
00:06:10,102 --> 00:06:16,180
hogy ez a költségfüggvény mennyire érzékeny az előző réteg aktiválására.

88
00:06:16,180 --> 00:06:20,760
Ugyanis ez a kezdeti derivált a láncszabály kifejezésben,

89
00:06:20,760 --> 00:06:25,420
a z érzékenysége az előző aktiválásra, a wL súlynak jön ki.

90
00:06:25,420 --> 00:06:29,823
És még egyszer, bár nem leszünk képesek közvetlenül befolyásolni az

91
00:06:29,823 --> 00:06:32,995
előző réteg aktiválását, hasznos nyomon követni,

92
00:06:32,995 --> 00:06:37,917
mert most már csak ismételhetjük ugyanezt a láncszabály-ötletet visszafelé,

93
00:06:37,917 --> 00:06:43,680
hogy meglássuk, mennyire érzékeny a költségfüggvény korábbi súlyok és korábbi torzítások.

94
00:06:43,680 --> 00:06:46,212
És azt gondolhatja, hogy ez egy túlságosan egyszerű példa,

95
00:06:46,212 --> 00:06:49,774
mivel minden rétegnek van egy neuronja, és a dolgok exponenciálisan bonyolultabbak

96
00:06:49,774 --> 00:06:51,320
lesznek egy valódi hálózat esetében.

97
00:06:51,320 --> 00:06:54,304
De őszintén szólva, nem sok változás történik,

98
00:06:54,304 --> 00:06:59,320
ha több neuront adunk a rétegeknek, valójában csak néhány indexet kell követni.

99
00:06:59,320 --> 00:07:02,838
Ahelyett, hogy egy adott réteg aktiválása egyszerűen AL lenne,

100
00:07:02,838 --> 00:07:06,803
annak egy alsó indexe is lesz, amely jelzi, hogy az adott réteg melyik

101
00:07:06,803 --> 00:07:07,920
neuronjáról van szó.

102
00:07:07,920 --> 00:07:15,280
Használjuk a k betűt az L-1 réteg, a j betűt pedig az L réteg indexeléséhez.

103
00:07:15,280 --> 00:07:19,284
A költségekhez ismét megnézzük, hogy mi a kívánt kimenet,

104
00:07:19,284 --> 00:07:24,601
de ezúttal összeadjuk az utolsó rétegaktiválások és a kívánt kimenet közötti

105
00:07:24,601 --> 00:07:26,120
különbségek négyzetét.

106
00:07:26,120 --> 00:07:33,280
Vagyis veszel egy összeget ALj mínusz yj négyzetével.

107
00:07:33,280 --> 00:07:37,969
Mivel sokkal több a súlyozás, mindegyiknek több indexnek kell lennie,

108
00:07:37,969 --> 00:07:41,988
hogy nyomon tudja követni, hol van, ezért nevezzük a k-adik

109
00:07:41,988 --> 00:07:45,740
neuront a j-edik neuronnal összekötő él súlyát WLjk-nek.

110
00:07:45,740 --> 00:07:50,072
Ezek az indexek eleinte kissé elmaradottaknak tűnhetnek, de ez összhangban van azzal,

111
00:07:50,072 --> 00:07:53,800
hogyan indexelné a súlymátrixot, amelyről az 1. rész videójában beszéltem.

112
00:07:53,800 --> 00:07:58,709
Csakúgy, mint korábban, továbbra is jó nevet adni a megfelelő súlyozott összegnek,

113
00:07:58,709 --> 00:08:03,027
például z-nek, így az utolsó réteg aktiválása csak a speciális függvény,

114
00:08:03,027 --> 00:08:04,980
mint a z-re alkalmazott szigmoid.

115
00:08:04,980 --> 00:08:09,555
Láthatja, mire gondolok, ahol ezek lényegében ugyanazok az egyenletek,

116
00:08:09,555 --> 00:08:12,777
mint korábban a rétegenkénti egy neuron esetében,

117
00:08:12,777 --> 00:08:15,420
csak ez egy kicsit bonyolultabbnak tűnik.

118
00:08:15,420 --> 00:08:18,950
És valóban, a láncszabály derivált kifejezés, amely leírja,

119
00:08:18,950 --> 00:08:23,540
hogy a költség mennyire érzékeny egy adott súlyra, lényegében ugyanúgy néz ki.

120
00:08:23,540 --> 00:08:29,420
Meghagyom neked, hogy állj meg, és gondold át mindegyik kifejezést, ha akarod.

121
00:08:29,420 --> 00:08:37,820
Ami azonban itt változik, az az L-1 réteg egyik aktiválásának költségének deriváltja.

122
00:08:37,820 --> 00:08:40,535
Ebben az esetben a különbség az, hogy a neuron

123
00:08:40,535 --> 00:08:43,540
több különböző úton befolyásolja a költségfüggvényt.

124
00:08:43,540 --> 00:08:50,828
Vagyis egyrészt befolyásolja az AL0-t, ami a költségfüggvényben játszik szerepet,

125
00:08:50,828 --> 00:08:58,028
de hatással van az AL1-re is, ami szintén szerepet játszik a költségfüggvényben,

126
00:08:58,028 --> 00:09:00,340
és ezeket össze kell adni.

127
00:09:00,340 --> 00:09:03,680
És nagyjából ennyi.

128
00:09:03,680 --> 00:09:08,858
Ha tudja, hogy a költségfüggvény mennyire érzékeny az utolsó előtti réteg aktiválásaira,

129
00:09:08,858 --> 00:09:13,920
megismételheti a folyamatot az adott rétegbe betáplált összes súlyozásra és torzításra.

130
00:09:13,920 --> 00:09:15,420
Szóval veregesd meg magad!

131
00:09:15,420 --> 00:09:20,412
Ha mindennek van értelme, akkor most mélyen belenézett a backpropagation szívébe,

132
00:09:20,412 --> 00:09:23,700
a neurális hálózatok tanulási folyamatának igáslójába.

133
00:09:23,700 --> 00:09:27,117
Ezek a láncszabály-kifejezések megadják azokat a származékokat,

134
00:09:27,117 --> 00:09:29,947
amelyek meghatározzák a gradiens egyes komponenseit,

135
00:09:29,947 --> 00:09:33,257
amelyek segítenek minimalizálni a hálózat költségeit azáltal,

136
00:09:33,257 --> 00:09:35,020
hogy ismételten lefelé lépkednek.

137
00:09:35,020 --> 00:09:36,340
Ha hátradől, és végiggondolja az egészet, akkor ez egy csomó

138
00:09:36,340 --> 00:09:37,812
összetett réteg körül kell járnia az elméjének, szóval ne aggódjon,

139
00:09:37,812 --> 00:09:38,960
ha időbe telik, amíg az elméje megemészti az egészet.

