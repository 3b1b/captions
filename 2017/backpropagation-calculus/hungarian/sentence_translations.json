[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "Erősen feltételezem, hogy megnézted a 3. részt, amely a visszaterjesztés algoritmus intuitív bemutatását tartalmazza.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Itt sokkal formálisabbá válunk, és belemerülünk a matematikai leírásba.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "Normális, ha nem tiszta minden azonnal, ezért a mantra, hogy rendszeresen tarts szünetet és gondolkodj, itt is ugyanúgy érvényes.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "A fő célom az, hogy megmutassam, hogyan gondolkodnak a gépi tanulással foglalkozók általában a matematikai láncszabályról a hálózatok kontextusában, ami eltér attól, ahogyan a legtöbb bevezető számtani kurzus megközelíti a témát.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Azoknak, akiknek a matematikai kalkulus nem az erősségük, van egy egész sorozatom a témában.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Kezdjük egy rendkívül egyszerű hálózattal, ahol minden rétegben egyetlen neuron van.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Ezt a hálózatot három súly és három eltolósúly határozza meg. A célunk az, hogy megértsük, mennyire érzékeny a költségfüggvény ezekre a változókra.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way, we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "Így tudni fogjuk, hogy az említett tagok mely módosításai okozzák a költségfüggvény legmeredekebb csökkenését.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "And we're just going to focus on the connection between the last two neurons.",
  "translatedText": "És most koncentráljunk csak az utolsó két neuron közötti kapcsolatra.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in, so the activation of the previous neuron is Al-1.",
  "translatedText": "Az utolsó neuron aktivációját jelöljük egy L felső indexszel, ami a réteg számát jelzi, így az előző neuron aktivációja AL-1.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 65.98,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Ezek nem kitevők, csak egy módja annak, hogy jelöljük miről beszélünk, mivel a későbbiekben más jelöléseket is akarunk használni.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Az utolsó neuron akvtivációjának értékét mondjuk jelöljük y-nal. Például egy adott képzési példa esetében, y lehet 0 vagy 1.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is Al-y2.",
  "translatedText": "Tehát ennek a hálózatnak a költsége egyetlen képzési példa esetén AL-y a másodikon.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Ennek az egy képzési példának a költségét C0-nak nevezzük.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neuron's activation plus some bias, which I'll call BL.",
  "translatedText": "Emlékeztetőül, ezt az utolsó aktivációt egy súly határozza meg, amelyet WL-nek fogok nevezni, szorozva az előző neuron aktivációjával plusz egy kis eltolósúllyal, amelyet BL-nek fogok hívni.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "Aztán valamilyen speciális nemlineáris függvénybe kell rakni, mint a szigmoid vagy a ReLU.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "Megkönnyíti a dolgunkat, ha ennek a súlyozott összegnek egy speciális nevet adunk, például \"z\"-t, ugyanazzal az indexszel, mint a vonatkozó aktivációk.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action and the bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "Ez nagyon sok kifejezés, de talán úgy szemléletesebb, ha vesszük a súlyt, az előző aktivációt és az eltolósúlyt, azokkal a \"z\" kiszámítható, ami viszont lehetővé teszi az \"a\" kiszámítását, ami végül egy \"y\" konstanssal együtt lehetővé teszi a költség kiszámítását.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course Al-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "És persze az \"AL-1\"-et befolyásolja a saját súlya, a korábbi aktiváció és a többi, de most nem erre fogunk koncentrálni.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Ezek mind csak számok, igaz?",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "És könnyű úgy gondolni rájuk, mintha saját kis számsorral rendelkeznének.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight WL.",
  "translatedText": "Az első célunk az, hogy megértsük, mennyire érzékeny a költségfüggvény a WL súlyunk kis változásaira.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 161.72,
  "end": 169
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to WL?",
  "translatedText": "Vagy másképp fogalmazva: mi a C deriváltja WL függvényében?",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del W term, think of it as meaning some tiny nudge to W, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Amikor ezt a delta W kifejezést látod, gondolj arra, hogy ez a W egy apró, például 0,01-es változást jelent, és ugyanígy gondold azt, hogy ez a delta C kifejezés a költség ebből eredő változását jelenti.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "Amit mi akarunk, az az arányuk.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Az elképzelésünknek megfelelően ha kicsit odébb lökjük a WL-t, az egy kis lökést okoz a ZL-nek, ami viszont egy kicsit odébb löki az \"AL\"-t, ami közvetlenül befolyásolja a költségeket.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is, the derivative of ZL with respect to WL.",
  "translatedText": "Tehát ezt úgy tudjuk megvalósítani, hogy először a ZL apró változását és az általa W-ben okozott apró változás arányát nézzük, vagyis a ZL-nek a WL általi deriváltját vesszük.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "Hasonlóképpen, ezután figyelembe kell venni az AL változásának és az azt okozó ZL apró változásának arányát, valamint a C végső elmozdulása és a közbenső AL lökése közötti arányt.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of c to small changes in WL.",
  "translatedText": "Ez igazából a láncszabály, ahol e három arány szorzatával megkapjuk a C érzékenységét a WL kis változásaira.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "A képernyőn most egy csomó szimbólum látható, és javasolom szánj rá pár pillanatot, hogy tisztázd magadban mik is ezek valójában, mert most ki fogjuk számítani releváns deriváltakat.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "A \"C\" deriváltja az AL függvényében 2AL mínusz y.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "Notice this means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Vegyük észre, hogy ez azt jelenti, hogy a mérete arányos a hálózat kimenete és az általunk kívánt kimenet közötti különbséggel, így ha ez a kimenet nagyon eltérő lenne, akkor még a legkisebb változtatások is nagy hatással lennének a végső költségfüggvényre.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "Az AL ZL általi deriváltja nem más, mint a szigmoid függvényünk deriváltja, vagy a helyette használt bármelyik más nemlinearitást megvalósító függvényé.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "And the derivative of ZL with respect to WL comes out to be AL-1.",
  "translatedText": "És a ZL WL-hez tartozó deriváltja \"AL-1\"-nek adódik.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all mean.",
  "translatedText": "Nem tudom, te hogy vagy vele, de szerintem könnyű belefeledkezni a képletekbe anélkül, hogy emlékeztetnénk magunkat arra, hogy mit is jelentenek pontosan.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "Az utolsó derivált esetében az, hogy a súlyra adott kis lökés mennyire befolyásolja az utolsó réteget, attól függ, hogy az előző neuron aktivációja milyen erős.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "Ne feledjük, hogy itt valósul meg a neuronoknál, hogy amelyek együtt tüzelnek azok kapcsolata megerősödik.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to WL only of the cost for a specific single training example.",
  "translatedText": "És mindez csak egyetlen adott képzési példa költségének a WL szerinti deriváltja.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Mivel a teljes költségfüggvény a sok különböző képzési példa összes ilyen költségét átlagolja, ezért a deriváltjának kiszámolásához is az összes képzési példán átlagára kell elvégezni a deriválást.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "And of course, that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "És természetesen ez csak egy összetevője a gradiens vektornak, amely maga is a költségfüggvény összes súly és eltolósúly szerinti parciális deriváltjaiból épül fel.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "És bár ez csak egy a sok szükséges parciális derivált közül, mégis a munka több mint 50%-át teszi ki.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "Az eltolósúlyra való érzékenység például szinte azonos.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Csak ki kell cserélnünk ezt a delta z, delta w kifejezést egy delta z, delta b kifejezésre.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "És ha megnézzük a vonatkozó képletet, a derivált értéke 1 lesz.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "Továbbá, és itt jön a képbe a visszafelé történő terjesztés ötlete, láthatjuk, hogy ez a költségfüggvény mennyire érzékeny az előző réteg aktivációira.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight WL.",
  "translatedText": "Szóval ha ezt az első deriválást elvégezzük a láncszabály kifejezésben, ami a \"z\" érzékenysége az előző aktivációra, azt kapjuk, hogy ez a WL súlynak felel meg.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "És még egyszer, bár nem tudjuk közvetlenül befolyásolni az előző réteg aktivációját, hasznos számon tartani, mert innentől ismételni tudjuk ezt a láncszabály-ötletet visszafelé haladva, hogy lássuk, mennyire érzékeny a költségfüggvény a korábbi súlyokra és a korábbi eltolósúlyokra.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "És gondolhatnád, hogy ez egy túlságosan egyszerű példa kevés neuronnal, és a dolgok exponenciálisan bonyolultabbak lesznek egy valódi hálózat esetében.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "De őszintén szólva, nem sok minden változik, ha a rétegeknek több neuront adunk, valójában csak néhány indexszel többet kell számon tartani.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "Ahelyett, hogy egy adott réteg aktivációja egyszerűen AL lenne, egy index is jelzi, hogy az adott réteg melyik neuronjáról van szó.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Használjuk a k betűt az L-1 réteg indexelésére, és a j betűt az L réteg indexelésére.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "A költségek esetében ismét azt nézzük, hogy mi a kívánt kimenet, de ezúttal az utolsó réteg aktivációit és a kívánt kimenet közötti különbségek négyzetét adjuk össze.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus Yj squared.",
  "translatedText": "Azaz, az ALj mínusz Yj négyzeteinek összegét képezzük.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Mivel még egy adag súly van, így mindegyiknek több indexet kell kapnia, hogy nyomon követhessük, hogy hol helyezkednek el, ezért nevezzük a k-ik neuront a j-ik neuronnal összekötő él súlyát WLjk-nak.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "Ezek az indexek elsőre kissé fordítva tűnhetnek, de összhangban vannak azzal, ahogyan az 1. rész videójában említett súlymátrixot indexelni kell.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "Ahogy korábban, most is adunk kéne egy nevet a vonatkozó súlyozott összegnek, például \"z\", így az utolsó réteg aktivációit az erre alkalmazott speciális függvényünk, például a szigmoid eredményeként kapjuk.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Láthatjátok mire gondolok, hogy ezek lényegében ugyanazok az egyenletek, mint korábban az egy neuron per réteg esetére, csak egy kicsit bonyolultabbnak tűnik.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain-ruled derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "És valóban, a láncszabályban szereplő derivált kifejezés, amely leírja, hogy a költség mennyire érzékeny egy adott súlyra, lényegében ugyanígy néz ki.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Állj meg és gondolkodj el ezeken a képleteken, ha akarsz.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "Ami itt azonban változik, az a költség deriváltja az L-1 réteg egyik aktivációjának függvényében.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "Ebben az esetben a különbség az, hogy a neuron több különböző úton keresztül befolyásolja a költségfüggvényt.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "Vagyis egyrészt befolyásolja az AL0-t, amely szerepet játszik a költségfüggvényben, de hatással van az AL1-re is, amely szintén szerepet játszik a költségfüggvényben, és ezeket össze kell adni.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "És ez, nos... Nagyjából ennyi az egész.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Onnantól, hogy tudod mennyire érzékeny a költségfüggvény az utolsó előtti réteg aktivációira, akkor ezeket a lépéseket kell csak megismételned az előtte lévő súlyokra.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "Szóval veregesd meg a válladat!",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Ha mindezt megértetted, akkor most már mélyen belelátsz a visszaterjesztés lelkébe, amely a neurális hálózatok tanulásának igáslova.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Ezek a láncszabály-képletek megadják a deriváltakat, amelyek meghatározzák a gradiens minden egyes komponensét, amely segít minimalizálni a hálózat költségét azáltal, hogy ismételten lefelé lépkedünk.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Ha hátradőlsz, és végiggondolod mindezt, akkor jó sok összetett réteget kell tudnod fejben tartani, úgyhogy ne aggódj, ha időbe telik, amíg az elméd megemészti mindezt.",
  "model": "DeepL",
  "n_reviews": 1,
  "start": 574.3,
  "end": 582.74
 }
]
