1
00:00:04,220 --> 00:00:07,120
Трудно предположить, что вы смотрели часть 3,

2
00:00:07,120 --> 00:00:10,240
дать интуитивно понятное прохождение алгоритма обратного распространения.

3
00:00:11,040 --> 00:00:14,770
Здесь мы немного более формальны и углубимся в соответствующее исчисление.

4
00:00:14,770 --> 00:00:17,040
Это нормально, что это немного сбивает с толку,

5
00:00:17,040 --> 00:00:21,480
поэтому мантра для регулярной паузы и размышления, безусловно, применима и здесь, и везде.

6
00:00:21,920 --> 00:00:25,180
Наша главная цель - показать, как люди в машинном обучении

7
00:00:25,180 --> 00:00:29,440
обычно думают о правиле цепи из исчисления в контексте сетей,

8
00:00:29,440 --> 00:00:33,820
который по-разному понимает, насколько большинство вводных курсов по исчислению подходят к предмету.

9
00:00:34,500 --> 00:00:36,890
Для тех из вас, кого не устраивает соответствующее исчисление,

10
00:00:36,890 --> 00:00:39,040
У меня есть целая серия на эту тему.

11
00:00:40,340 --> 00:00:43,150
Давайте начнем с очень простой сети,

12
00:00:43,150 --> 00:00:45,730
один, где каждый слой имеет один нейрон в нем.

13
00:00:46,270 --> 00:00:50,680
Таким образом, эта конкретная сеть определяется 3 весами и 3 поправками,

14
00:00:50,680 --> 00:00:55,070
и наша цель - понять, насколько чувствительна функция стоимости к этим переменным.

15
00:00:55,550 --> 00:00:57,830
Таким образом, мы знаем, какие корректировки этих условий

16
00:00:57,830 --> 00:01:00,940
собирается вызвать наиболее эффективное снижение функции стоимости.

17
00:01:01,920 --> 00:01:05,170
И мы просто сосредоточены на связи между двумя последними нейронами.

18
00:01:05,880 --> 00:01:11,370
Давайте обозначим активацию этого последнего нейрона "a" с верхним индексом "L", указывая, в каком слое он находится,

19
00:01:11,690 --> 00:01:15,720
Таким образом, активация этого предыдущего нейрона является "a^(L-1)"

20
00:01:16,430 --> 00:01:20,030
Здесь нет показателей, это просто способ индексации того, о чем мы говорим,

21
00:01:20,030 --> 00:01:22,970
так как я хочу сохранить подписки для различных индексов позже.

22
00:01:23,740 --> 00:01:29,710
Давайте предположим, что значение, которое мы хотим, чтобы эта последняя активация была для данного примера обучения, равно y.

23
00:01:30,170 --> 00:01:32,360
Например, у может быть 0 или 1.

24
00:01:32,940 --> 00:01:39,470
Таким образом, стоимость этой простой сети для одного примера обучения составляет (a ^ (L) - y) ^ 2.

25
00:01:40,250 --> 00:01:44,650
Мы обозначим стоимость этого одного учебного примера как C_0.

26
00:01:46,030 --> 00:01:51,520
Как напоминание, эта последняя активация определяется весом, который я собираюсь назвать w ^ (L)

27
00:01:51,980 --> 00:01:54,220
раз активация предыдущего нейрона,

28
00:01:54,530 --> 00:01:56,940
плюс некоторый уклон, который я назову б ^ (L),

29
00:01:57,480 --> 00:01:59,900
затем вы прокачиваете это через какую-то специальную нелинейную функцию

30
00:01:59,900 --> 00:02:01,520
как сигмоид или ReLU.

31
00:02:01,850 --> 00:02:06,980
На самом деле нам будет легче, если мы дадим специальное имя этой взвешенной сумме, например, z,

32
00:02:06,980 --> 00:02:09,550
с тем же верхним индексом, что и соответствующие активации.

33
00:02:10,390 --> 00:02:11,480
Так что здесь много терминов.

34
00:02:11,480 --> 00:02:16,960
И способ, которым вы могли бы осмыслить это то, что вес, предыдущая активация и уклон

35
00:02:16,960 --> 00:02:21,400
в целом используются для вычисления Z, что в свою очередь позволяет нам вычислить,

36
00:02:21,740 --> 00:02:25,610
который, наконец, вместе с константой у, вычислим стоимость.

37
00:02:27,260 --> 00:02:31,660
И, конечно, на ^ (L-1) влияет его собственный вес и уклон, и тому подобное.

38
00:02:32,810 --> 00:02:34,840
Но мы не собираемся концентрироваться на этом прямо сейчас.

39
00:02:35,680 --> 00:02:38,040
Все это просто цифры, верно?

40
00:02:38,040 --> 00:02:41,230
И может быть приятно думать, что каждый из них имеет свою собственную маленькую числовую линию.

41
00:02:41,900 --> 00:02:43,990
Наша первая цель - понять

42
00:02:43,990 --> 00:02:48,940
насколько чувствительна функция стоимости к небольшим изменениям нашего веса w ^ (L).

43
00:02:49,640 --> 00:02:54,880
Или по-другому, что является производной от С по w ^ (L).

44
00:02:55,630 --> 00:02:58,070
Когда вы видите этот термин «∂w»,

45
00:02:58,070 --> 00:03:02,750
Думайте об этом как о значении «какое-то крошечное толчок к w», как изменение на 0,01.

46
00:03:03,150 --> 00:03:08,210
И думайте об этом термине «∂C» как означающем «каким бы ни был получаемый в результате толчок к стоимости».

47
00:03:08,710 --> 00:03:10,420
То, что мы хотим, это их соотношение.

48
00:03:11,210 --> 00:03:16,520
Концептуально, этот крошечный толчок к w ^ (L) вызывает некоторый толчок к z ^ (L)

49
00:03:16,520 --> 00:03:21,380
что, в свою очередь, приводит к некоторому изменению ^ (L), что напрямую влияет на стоимость.

50
00:03:23,100 --> 00:03:28,930
Поэтому мы разбиваем это, сначала посмотрев на отношение крошечного изменения к z ^ (L) к крошечному изменению w ^ (L).

51
00:03:29,290 --> 00:03:33,030
То есть производная z ^ (L) по w ^ (L).

52
00:03:33,760 --> 00:03:39,410
Аналогично, вы затем учитываете отношение изменения к ^ (L) к крошечному изменению z ^ (L), которое вызвало его,

53
00:03:39,850 --> 00:03:44,880
а также отношение между последним толчком к C и этим промежуточным толчком к a ^ (L).

54
00:03:45,670 --> 00:03:47,850
Это прямо здесь - цепное правило,

55
00:03:47,850 --> 00:03:54,950
где умножение этих трех соотношений дает нам чувствительность C к небольшим изменениям w ^ (L).

56
00:03:57,190 --> 00:04:00,040
Так что на экране прямо сейчас есть много символов,

57
00:04:00,040 --> 00:04:03,000
так что найдите время, чтобы убедиться, что они все

58
00:04:03,600 --> 00:04:06,560
потому что теперь мы собираемся вычислить соответствующие производные.

59
00:04:07,400 --> 00:04:13,230
Производная C по отношению к ^ (L) оказывается равной 2 (a ^ (L) - y).

60
00:04:13,960 --> 00:04:16,880
Обратите внимание, это означает, что его размер пропорционален

61
00:04:16,880 --> 00:04:20,880
разница между выходом сети и тем, что мы хотим, чтобы это было.

62
00:04:21,360 --> 00:04:23,340
Так что, если этот результат был совсем другим,

63
00:04:23,340 --> 00:04:27,150
даже незначительные изменения могут оказать большое влияние на функцию стоимости.

64
00:04:28,300 --> 00:04:33,880
Производная a ^ (L) по z ^ (L) является просто производной нашей сигмоидальной функции,

65
00:04:33,880 --> 00:04:36,370
или любую нелинейность, которую вы решите использовать.

66
00:04:37,310 --> 00:04:40,370
И производная от z ^ (L) по w ^ (L),

67
00:04:41,470 --> 00:04:44,530
в этом случае получается просто ^ (L-1).

68
00:04:46,070 --> 00:04:49,570
Теперь я не знаю о вас, но я думаю, что легко застрять с головой в этих формулах

69
00:04:49,570 --> 00:04:53,690
не тратя времени на то, чтобы расслабиться и напомнить себе, что они на самом деле означают.

70
00:04:54,120 --> 00:04:56,040
В случае этой последней производной,

71
00:04:56,040 --> 00:05:00,060
количество, на которое небольшой толчок к этому весу влияет на последний слой

72
00:05:00,060 --> 00:05:02,850
зависит от того, насколько сильный предыдущий нейрон.

73
00:05:03,310 --> 00:05:07,520
Помните, что именно здесь возникает идея «нейроны, которые сжигают вместе проволоку».

74
00:05:09,210 --> 00:05:15,940
И все это является производной по отношению к w ^ (L) только стоимости для конкретного обучающего примера.

75
00:05:16,410 --> 00:05:22,150
Поскольку функция полной стоимости включает в себя усреднение всех этих затрат по многим учебным примерам,

76
00:05:22,150 --> 00:05:27,610
его производная требует усреднения этого выражения, которое мы нашли во всех обучающих примерах.

77
00:05:28,430 --> 00:05:31,930
И, конечно, это только один компонент вектора градиента,

78
00:05:31,930 --> 00:05:33,890
который сам построен из

79
00:05:33,890 --> 00:05:38,480
частные производные функции стоимости относительно всех этих весов и смещений.

80
00:05:40,710 --> 00:05:43,550
Но даже если это был только один из тех частных производных, которые нам нужны,

81
00:05:43,550 --> 00:05:45,390
это более 50% работы.

82
00:05:46,420 --> 00:05:49,940
Чувствительность к смещению, например, практически одинакова.

83
00:05:50,250 --> 00:05:55,120
Нам просто нужно заменить этот термин termz / ∂w на ∂z / ∂b,

84
00:05:58,760 --> 00:06:02,590
И если вы посмотрите на соответствующую формулу, эта производная становится 1.

85
00:06:06,210 --> 00:06:09,880
Кроме того, и вот тут-то и возникает идея распространения в обратном направлении,

86
00:06:10,230 --> 00:06:15,670
вы можете увидеть, насколько чувствительна эта функция стоимости к активации предыдущего слоя;

87
00:06:16,250 --> 00:06:19,650
а именно, эта начальная производная в разложении правила цепочки,

88
00:06:19,650 --> 00:06:23,100
чувствительность z к предыдущей активации,

89
00:06:23,480 --> 00:06:25,670
выходит вес w ^ (L).

90
00:06:26,580 --> 00:06:31,500
И снова, хотя мы не сможем напрямую повлиять на эту активацию,

91
00:06:31,500 --> 00:06:33,080
полезно отслеживать,

92
00:06:33,080 --> 00:06:38,200
потому что теперь мы можем просто продолжать повторять эту идею правила цепи в обратном направлении

93
00:06:38,200 --> 00:06:42,750
чтобы увидеть, насколько чувствительна функция стоимости к предыдущим весам и к предыдущим отклонениям.

94
00:06:43,630 --> 00:06:45,980
И вы можете подумать, что это слишком простой пример,

95
00:06:45,980 --> 00:06:47,880
поскольку все слои имеют только 1 нейрон,

96
00:06:47,880 --> 00:06:51,220
и все станет намного сложнее в реальной сети.

97
00:06:51,680 --> 00:06:56,270
Но, честно говоря, не так много изменений, когда мы даем слоям несколько нейронов.

98
00:06:56,270 --> 00:06:58,710
На самом деле это всего лишь несколько индексов, которые нужно отслеживать.

99
00:06:59,340 --> 00:07:02,880
Вместо того, чтобы активировать данный слой просто будучи ^ (L),

100
00:07:02,880 --> 00:07:07,210
он также будет иметь индекс, указывающий, какой это нейрон этого слоя.

101
00:07:07,780 --> 00:07:14,470
Давайте продолжим и будем использовать букву k для индексации слоя (L-1) и j для индексации слоя (L).

102
00:07:15,290 --> 00:07:18,910
Что касается стоимости, снова мы смотрим на то, каков желаемый результат.

103
00:07:18,910 --> 00:07:19,380
Но в это время

104
00:07:19,380 --> 00:07:25,260
мы складываем квадраты различий между этими последними активациями слоя и желаемым результатом.

105
00:07:26,060 --> 00:07:31,070
То есть вы берете сумму за (a_j ^ (L) - y_j) ^ 2

106
00:07:33,110 --> 00:07:34,520
Так как весов намного больше,

107
00:07:34,520 --> 00:07:37,650
каждый должен иметь еще пару индексов, чтобы отслеживать, где он находится.

108
00:07:38,010 --> 00:07:44,990
Итак, назовем вес ребра, соединяющего этот k-й нейрон с j-м нейроном, w_ {jk} ^ (L).

109
00:07:45,660 --> 00:07:48,260
Поначалу эти индексы могут показаться немного отсталыми,

110
00:07:48,260 --> 00:07:52,940
но это соответствует тому, как вы будете индексировать матрицу весов, о которой я говорил в видео части 1.

111
00:07:53,680 --> 00:07:58,350
Как и прежде, все еще приятно дать имя соответствующей взвешенной сумме, например, z,

112
00:07:58,350 --> 00:08:04,310
так что активация последнего слоя - это просто ваша специальная функция, такая как сигмоида, примененная к z.

113
00:08:05,040 --> 00:08:06,230
Вы можете понять, что я имею в виду, верно?

114
00:08:06,230 --> 00:08:11,680
Все это, по сути, те же уравнения, которые мы имели ранее в случае с одним нейроном на слой;

115
00:08:11,680 --> 00:08:13,870
это выглядит немного сложнее.

116
00:08:15,370 --> 00:08:18,220
И действительно, производное выражение цепочки правил

117
00:08:18,220 --> 00:08:21,980
описывая, насколько чувствительна стоимость к определенному весу

118
00:08:21,980 --> 00:08:23,890
выглядит по сути одинаково.

119
00:08:23,890 --> 00:08:26,880
Я оставлю это вам, чтобы сделать паузу и подумать о каждом из этих терминов, если хотите.

120
00:08:29,310 --> 00:08:31,320
Что здесь меняется, хотя,

121
00:08:31,320 --> 00:08:36,830
является производной стоимости по отношению к одной из активаций в слое (L-1).

122
00:08:37,760 --> 00:08:43,120
В этом случае разница в том, что нейрон влияет на функцию стоимости через несколько путей.

123
00:08:44,660 --> 00:08:50,540
То есть, с одной стороны, это влияет на a_0 ^ (L), который играет роль в функции стоимости,

124
00:08:51,010 --> 00:08:56,320
но это также влияет на a_1 ^ (L), который также играет роль в функции стоимости.

125
00:08:56,320 --> 00:08:57,410
И вы должны добавить их.

126
00:09:00,170 --> 00:09:02,980
И это ... хорошо, это в значительной степени это.

127
00:09:03,560 --> 00:09:08,520
Как только вы узнаете, насколько чувствительна функция стоимости к активациям этого второго до последнего слоя,

128
00:09:08,840 --> 00:09:12,940
Вы можете просто повторить процесс для всех весов и смещений, подаваемых в этот слой.

129
00:09:13,850 --> 00:09:15,360
Так похлопайте себя по спине!

130
00:09:15,360 --> 00:09:16,950
Если это все имеет смысл,

131
00:09:16,950 --> 00:09:20,440
Вы теперь заглянули глубоко в сердце обратного распространения,

132
00:09:20,440 --> 00:09:22,830
рабочая лошадка в изучении нейронных сетей.

133
00:09:23,590 --> 00:09:29,300
Эти выражения правила цепочки дают вам производные, которые определяют каждый компонент в градиенте

134
00:09:29,300 --> 00:09:33,550
это помогает минимизировать стоимость сети, постоянно снижаясь.

135
00:09:34,280 --> 00:09:36,850
Hhhhpf. Если вы будете сидеть сложа руки и думать обо всем этом,

136
00:09:36,850 --> 00:09:40,090
это много уровней сложности, чтобы обернуть ваш разум вокруг.

137
00:09:40,090 --> 00:09:43,090
Так что не волнуйтесь, если вашему разуму потребуется время, чтобы все это переварить.

