1
00:00:00,000 --> 00:00:08,420
Важке припущення полягає в тому, що ви переглянули частину 3,

2
00:00:08,420 --> 00:00:11,160
яка дає інтуїтивно зрозумілу інструкцію з алгоритму зворотного поширення.

3
00:00:11,160 --> 00:00:14,920
Тут ми станемо більш формальними та зануримося у відповідне обчислення.

4
00:00:14,920 --> 00:00:18,560
Це нормально, що це принаймні трохи заплутає, тому мантра регулярно зупинятися

5
00:00:18,560 --> 00:00:22,000
та розмірковувати, безумовно, застосовна тут так само, як і будь-де.

6
00:00:22,000 --> 00:00:26,620
Наша головна мета — показати, як люди, які займаються машинним навчанням, зазвичай

7
00:00:26,620 --> 00:00:31,900
думають про правило ланцюга з обчислення в контексті мереж, яке має інше

8
00:00:31,900 --> 00:00:34,580
відчуття від того, як більшість початкових курсів обчислення підходять до цього предмета.

9
00:00:34,580 --> 00:00:38,300
Для тих із вас, кому незручно відповідне обчислення,

10
00:00:38,300 --> 00:00:39,300
у мене є ціла серія на цю тему.

11
00:00:39,300 --> 00:00:44,840
Давайте почнемо з надзвичайно простої мережі,

12
00:00:44,840 --> 00:00:46,780
де кожен рівень містить один нейрон.

13
00:00:46,780 --> 00:00:51,880
Ця мережа визначається трьома вагами та трьома зміщеннями, і наша

14
00:00:51,880 --> 00:00:55,640
мета — зрозуміти, наскільки функція витрат чутлива до цих змінних.

15
00:00:55,640 --> 00:00:59,780
Таким чином ми знаємо, які коригування цих

16
00:00:59,780 --> 00:01:01,100
умов спричинять найефективніше зниження функції витрат.

17
00:01:01,100 --> 00:01:05,360
Ми просто зосередимося на зв’язку між двома останніми нейронами.

18
00:01:05,360 --> 00:01:10,400
Давайте позначимо активацію цього останнього нейрона верхнім індексом

19
00:01:10,400 --> 00:01:11,800
L, вказуючи, на якому шарі він знаходиться.

20
00:01:11,800 --> 00:01:16,560
Таким чином, активація попереднього нейрона AL-1.

21
00:01:16,560 --> 00:01:20,120
Це не експоненти, це лише спосіб індексування того, про що ми

22
00:01:20,120 --> 00:01:23,120
говоримо, оскільки пізніше я хочу зберегти індекси для різних індексів.

23
00:01:23,600 --> 00:01:28,880
Припустімо, що значення, яке ми хочемо мати для цієї останньої активації для

24
00:01:28,880 --> 00:01:33,020
даного прикладу навчання, дорівнює y, наприклад, y може бути 0 або 1.

25
00:01:33,020 --> 00:01:39,040
Отже, вартість цієї мережі для одного навчального прикладу становить AL-y2.

26
00:01:39,040 --> 00:01:46,120
Ми позначимо вартість одного навчального прикладу як c0.

27
00:01:46,120 --> 00:01:51,920
Нагадаю, що ця остання активація визначається вагою, яку я називатиму wL, помноженою

28
00:01:51,920 --> 00:01:57,600
на активацію попереднього нейрона плюс деяке зміщення, яке я називатиму bL.

29
00:01:57,600 --> 00:02:01,560
Потім ви прокачуєте це через якусь спеціальну нелінійну функцію, як-от сигмоїда або ReLU.

30
00:02:01,560 --> 00:02:05,400
Насправді нам стане простіше, якщо ми дамо спеціальну назву цій зваженій сумі,

31
00:02:05,400 --> 00:02:10,600
як-от z, з тим самим верхнім індексом, що й відповідні активації.

32
00:02:10,600 --> 00:02:15,320
Це багато термінів, і ви можете концептуалізувати це так: вага, попередня дія та

33
00:02:15,320 --> 00:02:21,800
зміщення разом використовуються для обчислення z, що, у свою чергу, дозволяє нам

34
00:02:21,800 --> 00:02:27,360
обчислити a, яке, нарешті, разом із константою y, дозволяє ми розрахуємо вартість.

35
00:02:27,360 --> 00:02:33,440
І, звісно, на AL-1 впливає власна вага, упередженість тощо,

36
00:02:33,440 --> 00:02:35,920
але ми не збираємося на цьому зосереджуватися зараз.

37
00:02:35,920 --> 00:02:38,120
Усе це лише цифри, чи не так?

38
00:02:38,120 --> 00:02:41,960
І може бути приємно уявити, що кожна з них має власну маленьку числову лінію.

39
00:02:41,960 --> 00:02:47,480
Наша перша мета — зрозуміти, наскільки функція витрат

40
00:02:47,480 --> 00:02:49,820
чутлива до невеликих змін нашої ваги wL.

41
00:02:49,820 --> 00:02:55,740
Або інакше сформулюйте, яка похідна c відносно wL?

42
00:02:55,740 --> 00:03:01,220
Коли ви бачите цей термін del w, уявіть, що це означає деякий крихітний поштовх до w, як зміна на 0.

43
00:03:01,220 --> 00:03:08,820
01, і подумайте про те, що цей термін del c означає будь-яке кінцеве підвищення вартості.

44
00:03:08,820 --> 00:03:10,900
Ми хочемо їх співвідношення.

45
00:03:10,900 --> 00:03:17,740
Концептуально цей невеликий поштовх до wL спричиняє деякий поштовх до zL, який, у

46
00:03:17,740 --> 00:03:23,220
свою чергу, викликає певний поштовх до AL, що безпосередньо впливає на вартість.

47
00:03:23,220 --> 00:03:28,020
Отже, ми розділяємо речі, спочатку дивлячись на відношення незначної зміни zL

48
00:03:28,020 --> 00:03:33,340
до цієї незначної зміни w, тобто похідну від zL відносно wL.

49
00:03:33,340 --> 00:03:38,820
Подібним чином ви розглядаєте співвідношення зміни до AL до крихітної

50
00:03:38,820 --> 00:03:43,900
зміни в zL, яка її спричинила, а також співвідношення між

51
00:03:43,900 --> 00:03:45,900
остаточним підштовхуванням до c і цим проміжним підштовхуванням до AL.

52
00:03:45,900 --> 00:03:51,880
Це ланцюгове правило, де множення цих трьох співвідношень

53
00:03:51,880 --> 00:03:57,340
дає нам чутливість c до невеликих змін wL.

54
00:03:57,340 --> 00:04:01,620
Тож зараз на екрані є багато символів, і знайдіть хвилинку, щоб переконатися,

55
00:04:01,620 --> 00:04:07,460
що вони всі зрозумілі, тому що зараз ми збираємося обчислити відповідні похідні.

56
00:04:07,460 --> 00:04:14,220
Похідна від c відносно AL виявляється 2AL-y.

57
00:04:14,220 --> 00:04:19,300
Це означає, що його розмір пропорційний різниці між виходом мережі та тим,

58
00:04:19,300 --> 00:04:24,480
що ми хочемо, щоб він був, тому, якщо цей вивід сильно відрізняється,

59
00:04:24,480 --> 00:04:28,380
навіть незначні зміни можуть мати великий вплив на кінцеву функцію витрат.

60
00:04:28,380 --> 00:04:33,860
Похідна AL відносно zL — це просто похідна нашої

61
00:04:33,860 --> 00:04:37,420
сигмоїдної функції або будь-якої нелінійності, яку ви вирішите використовувати.

62
00:04:37,420 --> 00:04:46,180
Похідна від zL відносно wL дорівнює AL-1.

63
00:04:46,180 --> 00:04:49,460
Я не знаю, як ви, але я думаю, що легко застрягти головою в формулах,

64
00:04:49,460 --> 00:04:54,180
не витрачаючи хвилини, щоб сісти склавши руки та нагадати собі, що вони означають.

65
00:04:54,180 --> 00:04:58,860
У випадку цієї останньої похідної величина впливу невеликого поштовху на вагу

66
00:04:58,860 --> 00:05:03,220
на останній шар залежить від того, наскільки сильним є попередній нейрон.

67
00:05:03,220 --> 00:05:09,320
Пам’ятайте, що саме тут з’являється ідея нейронів, які спрацьовують разом.

68
00:05:09,320 --> 00:05:14,840
І все це є похідною по відношенню до

69
00:05:14,840 --> 00:05:16,580
wL лише від вартості окремого прикладу навчання.

70
00:05:16,580 --> 00:05:20,940
Оскільки функція повних витрат передбачає усереднення всіх цих

71
00:05:20,940 --> 00:05:27,300
витрат у багатьох різних прикладах навчання, її похідна

72
00:05:27,300 --> 00:05:28,540
вимагає усереднення цього виразу для всіх прикладів навчання.

73
00:05:28,540 --> 00:05:33,860
Звичайно, це лише один компонент вектора градієнта, який складається з

74
00:05:33,860 --> 00:05:40,780
часткових похідних функції вартості щодо всіх цих ваг і зміщень.

75
00:05:40,780 --> 00:05:44,340
Але хоча це лише одна з багатьох частинних

76
00:05:44,340 --> 00:05:46,460
похідних, які нам потрібні, це понад 50% роботи.

77
00:05:46,460 --> 00:05:50,300
Чутливість до зсуву, наприклад, практично однакова.

78
00:05:50,300 --> 00:05:58,980
Нам просто потрібно змінити цей термін del z del w на a del z del b.

79
00:05:58,980 --> 00:06:04,700
І якщо ви подивитеся на відповідну формулу, ця похідна виявиться рівною 1.

80
00:06:04,700 --> 00:06:11,700
Крім того, і саме тут виникає ідея поширення назад, ви можете

81
00:06:11,700 --> 00:06:16,180
побачити, наскільки ця функція вартості чутлива до активації попереднього рівня.

82
00:06:16,180 --> 00:06:21,380
А саме, ця початкова похідна у виразі правила ланцюга,

83
00:06:21,380 --> 00:06:25,420
чутливість z до попередньої активації, виявляється вагою wL.

84
00:06:25,420 --> 00:06:30,100
І знову ж таки, хоча ми не зможемо напряму вплинути на

85
00:06:30,100 --> 00:06:35,280
активацію попереднього рівня, це корисно відслідковувати, тому що тепер ми можемо

86
00:06:35,280 --> 00:06:40,780
просто продовжувати повторювати ту саму ідею правила ланцюга назад, щоб побачити,

87
00:06:40,780 --> 00:06:43,680
наскільки чутлива функція витрат до попередні ваги та попередні упередження.

88
00:06:43,680 --> 00:06:47,940
І ви можете подумати, що це надто простий приклад, оскільки всі рівні

89
00:06:47,940 --> 00:06:51,320
мають один нейрон, а для реальної мережі все стане експоненціально складнішим.

90
00:06:51,320 --> 00:06:56,560
Але, чесно кажучи, не так багато змін, коли ми надаємо шарам

91
00:06:56,560 --> 00:06:59,320
кілька нейронів, насправді це лише кілька індексів, які потрібно відстежувати.

92
00:06:59,320 --> 00:07:03,580
Замість того, щоб активація даного шару була просто AL, він

93
00:07:03,580 --> 00:07:07,920
також матиме індекс, який вказує, який нейрон цього шару це.

94
00:07:07,920 --> 00:07:15,280
Використовуємо букву k для індексування шару L-1, а j для індексування шару L.

95
00:07:15,280 --> 00:07:20,720
Що стосується вартості, ми знову дивимося на бажаний результат, але цього разу

96
00:07:20,720 --> 00:07:26,120
ми складаємо квадрати різниць між цими останніми активаціями шару та бажаним результатом.

97
00:07:26,120 --> 00:07:33,280
Тобто ви берете суму на ALj мінус yj у квадраті.

98
00:07:33,280 --> 00:07:36,500
Оскільки ваг набагато більше, кожен з них повинен мати ще пару

99
00:07:36,500 --> 00:07:41,380
індексів, щоб відстежувати, де він знаходиться, тому давайте назвемо вагу

100
00:07:41,380 --> 00:07:45,740
ребра, що з’єднує цей k-й нейрон із j-м нейроном, WLjk.

101
00:07:45,740 --> 00:07:49,820
Спочатку ці індекси можуть здатися трохи зворотними, але вони узгоджуються з тим, як

102
00:07:49,820 --> 00:07:53,800
ви індексуєте вагову матрицю, про яку я говорив у першій частині відео.

103
00:07:53,800 --> 00:07:57,660
Як і раніше, доцільно дати ім’я відповідній зваженій сумі,

104
00:07:57,660 --> 00:08:03,540
як-от z, щоб активація останнього шару була просто

105
00:08:03,540 --> 00:08:04,980
вашою спеціальною функцією, як-от сигмоід, застосована до z.

106
00:08:04,980 --> 00:08:09,100
Ви розумієте, що я маю на увазі, де всі ці рівняння, по суті, ті самі рівняння,

107
00:08:09,100 --> 00:08:15,420
які ми мали раніше у випадку одного нейрона на шар, просто це виглядає трохи складніше.

108
00:08:15,420 --> 00:08:20,620
І справді, похідний вираз ланцюгового правила, що описує, наскільки

109
00:08:20,620 --> 00:08:23,540
чутлива вартість до конкретної ваги, виглядає практично однаково.

110
00:08:23,540 --> 00:08:29,420
Я залишу вам зробити паузу та подумати над кожним із цих термінів, якщо хочете.

111
00:08:29,420 --> 00:08:34,900
Що тут змінюється, так це похідна вартості

112
00:08:34,900 --> 00:08:37,820
відносно однієї з активацій на рівні L-1.

113
00:08:37,820 --> 00:08:42,000
У цьому випадку різниця полягає в тому, що

114
00:08:42,000 --> 00:08:43,540
нейрон впливає на функцію витрат кількома різними шляхами.

115
00:08:43,540 --> 00:08:51,200
Тобто, з одного боку, це впливає на AL0, який відіграє певну

116
00:08:51,200 --> 00:08:56,460
роль у функції витрат, але він також впливає на AL1, який

117
00:08:56,460 --> 00:09:00,340
також відіграє роль у функції витрат, і ви повинні додати їх.

118
00:09:00,340 --> 00:09:03,680
І це, ну, це майже все.

119
00:09:03,680 --> 00:09:08,240
Коли ви дізнаєтеся, наскільки функція вартості чутлива до активацій на

120
00:09:08,240 --> 00:09:12,520
цьому передостанньому шарі, ви можете просто повторити процес для

121
00:09:12,520 --> 00:09:13,920
всіх ваг і зміщень, що надходять у цей шар.

122
00:09:13,920 --> 00:09:15,420
Тож погладьте себе по плечу!

123
00:09:15,420 --> 00:09:20,480
Якщо все це має сенс, то тепер ви зазирнули глибоко в серце зворотного

124
00:09:20,480 --> 00:09:23,700
поширення, робочої конячки, яка лежить в основі того, як нейронні мережі навчаються.

125
00:09:23,700 --> 00:09:27,960
Ці вирази правил ланцюга дають вам похідні, які визначають кожен компонент

126
00:09:27,960 --> 00:09:35,020
у градієнті, що допомагає мінімізувати вартість мережі шляхом повторного кроку вниз.

127
00:09:35,020 --> 00:09:38,960
Якщо ви сидите склавши руки і думаєте про все це, це багато рівнів складності, щоб

128
00:09:38,960 --> 00:09:42,840
охопити свій розум, тож не хвилюйтеся, якщо вашому розуму потрібен час, щоб усе це переварити.

