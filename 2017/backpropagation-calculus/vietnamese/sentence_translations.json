[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "Giả định khó khăn ở đây là bạn đã xem phần 3, đưa ra hướng dẫn trực quan về thuật toán lan truyền ngược.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Ở đây chúng ta trang trọng hơn một chút và đi sâu vào phép tính có liên quan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "Việc điều này hơi khó hiểu một chút là điều bình thường, vì vậy câu thần chú thường xuyên tạm dừng và suy ngẫm chắc chắn được áp dụng nhiều ở đây cũng như bất kỳ nơi nào khác.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "Mục tiêu chính của chúng tôi là cho thấy cách mọi người trong lĩnh vực học máy thường nghĩ về quy tắc dây chuyền từ phép tính trong bối cảnh mạng, điều này có cảm giác khác với cách hầu hết các khóa học tính toán cơ bản tiếp cận chủ đề.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Đối với những người không thoải mái với phép tính liên quan, tôi có cả một loạt bài về chủ đề này.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Hãy bắt đầu với một mạng cực kỳ đơn giản, trong đó mỗi lớp có một nơ-ron duy nhất.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Mạng này được xác định bởi ba trọng số và ba độ lệch và mục tiêu của chúng tôi là hiểu mức độ nhạy cảm của hàm chi phí đối với các biến này.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "Bằng cách đó, chúng tôi biết những điều chỉnh nào đối với các điều khoản đó sẽ làm giảm hàm chi phí một cách hiệu quả nhất.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "We'll just focus on the connection between the last two neurons.",
  "translatedText": "Chúng ta sẽ chỉ tập trung vào kết nối giữa hai nơ-ron cuối cùng.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in.",
  "translatedText": "Hãy gắn nhãn kích hoạt của nơ-ron cuối cùng đó bằng chữ L siêu hạng, cho biết nó nằm ở lớp nào.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.98,
  "end": 71.36
 },
 {
  "input": "So the activation of the previous neuron is AL-1.",
  "translatedText": "Vậy sự kích hoạt của nơron trước đó là AL-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.68,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Đây không phải là số mũ, chúng chỉ là một cách lập chỉ mục những gì chúng ta đang nói đến, vì sau này tôi muốn lưu chỉ số dưới cho các chỉ số khác nhau.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Giả sử giá trị mà chúng ta muốn lần kích hoạt cuối cùng này dành cho một ví dụ huấn luyện nhất định là y, ví dụ: y có thể là 0 hoặc 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is AL-y2.",
  "translatedText": "Vì vậy, chi phí của mạng này cho một ví dụ huấn luyện là AL-y2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Chúng ta sẽ biểu thị chi phí của một ví dụ đào tạo đó là c0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call wL, times the previous neuron's activation plus some bias, which I'll call bL.",
  "translatedText": "Xin nhắc lại, lần kích hoạt cuối cùng này được xác định bởi trọng số, mà tôi sẽ gọi là wL, nhân với lần kích hoạt của nơ-ron trước đó cộng với một số sai lệch, mà tôi sẽ gọi là bL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "Then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "Sau đó, bạn bơm nó thông qua một số hàm phi tuyến đặc biệt như sigmoid hoặc ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "Thực ra, mọi việc sẽ dễ dàng hơn cho chúng ta nếu chúng ta đặt một tên đặc biệt cho tổng có trọng số này, chẳng hạn như z, với cùng chỉ số trên như các kích hoạt có liên quan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action, and bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "Đây là rất nhiều thuật ngữ và bạn có thể khái niệm hóa nó bằng cách sử dụng trọng số, tác động trước đó và độ lệch để tính z, từ đó cho phép chúng ta tính a, cuối cùng, cùng với hằng số y, hãy chúng tôi tính toán chi phí.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course, AL-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "Và tất nhiên, AL-1 bị ảnh hưởng bởi trọng lượng và độ lệch của chính nó, v. v. , nhưng chúng ta sẽ không tập trung vào điều đó ngay bây giờ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Tất cả chỉ là những con số thôi phải không?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "Và thật tuyệt khi nghĩ mỗi người đều có trục số nhỏ của riêng mình.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight wL.",
  "translatedText": "Mục tiêu đầu tiên của chúng ta là hiểu mức độ nhạy cảm của hàm chi phí đối với những thay đổi nhỏ trong trọng số wL của chúng ta.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to wL?",
  "translatedText": "Hay nói cách khác, đạo hàm của c theo wL bằng bao nhiêu?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del w term, think of it as meaning some tiny nudge to w, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Khi bạn nhìn thấy thuật ngữ del w này, hãy nghĩ nó có nghĩa là một sự dịch chuyển nhỏ nào đó tới w, chẳng hạn như sự thay đổi bằng 0.01, và coi thuật ngữ del c này có nghĩa là bất kể tác động lên chi phí là gì.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "Những gì chúng tôi muốn là tỷ lệ của họ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to wL causes some nudge to zL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Về mặt khái niệm, sự tác động nhỏ tới wL này gây ra một số tác động tới zL, từ đó gây ra một số tác động tới AL, điều này ảnh hưởng trực tiếp đến chi phí.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to zL to this tiny change w, that is, the derivative of zL with respect to wL.",
  "translatedText": "Vì vậy, trước tiên chúng ta chia nhỏ mọi thứ bằng cách xem xét tỉ số của một thay đổi nhỏ của zL với thay đổi nhỏ này w, tức là đạo hàm của zL đối với wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in zL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "Tương tự như vậy, sau đó bạn xem xét tỷ lệ giữa sự thay đổi của AL với sự thay đổi nhỏ trong zL đã gây ra nó, cũng như tỷ lệ giữa cú hích cuối cùng với c và cú hích trung gian này với AL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying these three ratios gives us the sensitivity of c to small changes in wL.",
  "translatedText": "Đây chính là quy tắc dây chuyền, trong đó việc nhân ba tỷ lệ này cho chúng ta độ nhạy của c với những thay đổi nhỏ trong wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "Vì vậy, trên màn hình ngay bây giờ, có rất nhiều ký hiệu, và hãy dành chút thời gian để đảm bảo rằng chúng rõ ràng là gì, bởi vì bây giờ chúng ta sẽ tính đạo hàm có liên quan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "Đạo hàm của c theo AL là 2AL-y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "This means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Điều này có nghĩa là kích thước của nó tỷ lệ thuận với sự khác biệt giữa đầu ra của mạng và thứ chúng ta mong muốn, vì vậy nếu đầu ra đó rất khác nhau thì ngay cả những thay đổi nhỏ cũng có tác động lớn đến hàm chi phí cuối cùng.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to zL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "Đạo hàm của AL theo zL chỉ là đạo hàm của hàm sigmoid của chúng tôi hoặc bất kỳ tính phi tuyến nào mà bạn chọn sử dụng.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "The derivative of zL with respect to wL comes out to be AL-1.",
  "translatedText": "Đạo hàm của zL theo wL là AL-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself what they all mean.",
  "translatedText": "Không biết bạn thế nào, nhưng tôi nghĩ bạn rất dễ bị mắc kẹt trong các công thức mà không dành một chút thời gian để ngồi lại và nhắc nhở bản thân ý nghĩa của chúng.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "Trong trường hợp của đạo hàm cuối cùng này, mức độ ảnh hưởng của trọng lượng nhỏ đến lớp cuối cùng phụ thuộc vào mức độ mạnh của nơ-ron trước đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "Hãy nhớ rằng, đây chính là lúc ý tưởng kết hợp các nơ-ron thần kinh với nhau xuất hiện.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to wL only of the cost for a specific single training example.",
  "translatedText": "Và tất cả những điều này chỉ là đạo hàm của wL chi phí cho một ví dụ đào tạo cụ thể.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Vì hàm chi phí đầy đủ liên quan đến việc tính trung bình tất cả các chi phí đó trên nhiều ví dụ đào tạo khác nhau, nên đạo hàm của nó yêu cầu tính trung bình biểu thức này trên tất cả các ví dụ đào tạo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "Of course, that's just one component of the gradient vector, which is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "Tất nhiên, đó chỉ là một thành phần của vectơ gradient, được xây dựng từ đạo hàm riêng của hàm chi phí đối với tất cả các trọng số và độ lệch đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "Nhưng mặc dù đó chỉ là một trong nhiều đạo hàm riêng phần mà chúng ta cần, nhưng nó cũng chiếm hơn 50% công việc.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "Ví dụ, độ nhạy đối với sự thiên vị gần như giống hệt nhau.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Chúng ta chỉ cần đổi số hạng del z del w này thành a del z del b.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "Và nếu bạn nhìn vào công thức liên quan, đạo hàm đó sẽ bằng 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "Ngoài ra, và đây là lúc nảy sinh ý tưởng truyền ngược, bạn có thể thấy hàm chi phí này nhạy cảm như thế nào đối với việc kích hoạt lớp trước đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight wL.",
  "translatedText": "Cụ thể, đạo hàm ban đầu này trong biểu thức quy tắc dây chuyền, độ nhạy của z đối với lần kích hoạt trước đó, sẽ là trọng số wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "Và một lần nữa, mặc dù chúng ta sẽ không thể ảnh hưởng trực tiếp đến việc kích hoạt lớp trước đó, nhưng việc theo dõi vẫn rất hữu ích vì bây giờ chúng ta có thể tiếp tục lặp lại ý tưởng quy tắc chuỗi tương tự này để xem hàm chi phí nhạy cảm như thế nào đối với trọng số trước đó và độ lệch trước đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "Và bạn có thể nghĩ rằng đây là một ví dụ quá đơn giản, vì tất cả các lớp đều có một nơ-ron và mọi thứ sẽ trở nên phức tạp hơn theo cấp số nhân đối với một mạng thực.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "Nhưng thành thật mà nói, không có nhiều thay đổi khi chúng tôi cung cấp cho các lớp nhiều nơ-ron, thực sự đó chỉ là một vài chỉ số cần theo dõi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "Thay vì kích hoạt một lớp nhất định chỉ đơn giản là AL, nó cũng sẽ có chỉ số dưới cho biết đó là nơ-ron nào của lớp đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Hãy sử dụng chữ k để lập chỉ mục cho lớp L-1 và j để lập chỉ mục cho lớp L.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "Về chi phí, một lần nữa chúng ta xem xét đầu ra mong muốn là bao nhiêu, nhưng lần này chúng ta cộng bình phương của sự khác biệt giữa các lần kích hoạt lớp cuối cùng này và đầu ra mong muốn.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus yj squared.",
  "translatedText": "Nghĩa là, bạn lấy tổng trên ALj trừ yj bình phương.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Vì có nhiều trọng số hơn nên mỗi cái phải có thêm một vài chỉ số để theo dõi vị trí của nó, vì vậy hãy gọi trọng số của cạnh nối nơ-ron thứ k này với nơ-ron thứ j, WLjk.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "Ban đầu, các chỉ số đó có thể hơi ngược một chút, nhưng nó phù hợp với cách bạn lập chỉ mục cho ma trận trọng số mà tôi đã nói trong video phần 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "Cũng như trước đây, bạn vẫn nên đặt tên cho tổng có trọng số liên quan, chẳng hạn như z, để việc kích hoạt lớp cuối cùng chỉ là hàm đặc biệt của bạn, như sigmoid, áp dụng cho z.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Bạn có thể hiểu ý tôi, trong đó tất cả những phương trình này về cơ bản đều giống các phương trình mà chúng ta đã có trước đây trong trường hợp một nơ-ron trên mỗi lớp, chỉ là nó trông phức tạp hơn một chút.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain rule derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "Và thực sự, biểu thức đạo hàm quy tắc dây chuyền mô tả mức độ nhạy cảm của chi phí đối với một trọng số cụ thể về cơ bản là giống nhau.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Tôi sẽ để bạn tạm dừng và suy nghĩ về từng điều khoản đó nếu bạn muốn.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "Tuy nhiên, điều thay đổi ở đây là đạo hàm của chi phí đối với một trong các kích hoạt trong lớp L-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "Trong trường hợp này, sự khác biệt là tế bào thần kinh ảnh hưởng đến hàm chi phí thông qua nhiều con đường khác nhau.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "Nghĩa là, một mặt, nó ảnh hưởng đến AL0, vốn đóng một vai trò trong hàm chi phí, nhưng nó cũng có ảnh hưởng đến AL1, cũng đóng một vai trò trong hàm chi phí, và bạn phải cộng chúng lại.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "Và đó, ồ, đại khái là như vậy.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Khi bạn biết mức độ nhạy cảm của hàm chi phí đối với các kích hoạt trong lớp thứ hai đến lớp cuối cùng này, bạn chỉ cần lặp lại quy trình cho tất cả các trọng số và độ lệch đưa vào lớp đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "Vì vậy hãy vỗ nhẹ vào lưng mình!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Nếu tất cả những điều này đều hợp lý thì giờ đây bạn đã tìm hiểu sâu về cốt lõi của lan truyền ngược, nền tảng đằng sau cách mạng lưới thần kinh học hỏi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Các biểu thức quy tắc chuỗi này cung cấp cho bạn các đạo hàm xác định từng thành phần trong gradient giúp giảm thiểu chi phí của mạng bằng cách liên tục giảm dần độ dốc.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Nếu bạn ngồi lại và suy nghĩ về tất cả những điều đó, thì đây là rất nhiều lớp phức tạp bao trùm tâm trí bạn, vì vậy đừng lo lắng nếu tâm trí bạn cần thời gian để tiêu hóa tất cả.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]