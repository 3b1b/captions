1
00:00:04,020 --> 00:00:07,026
Giả định khó khăn ở đây là bạn đã xem phần 3, đưa ra 

2
00:00:07,026 --> 00:00:09,920
hướng dẫn trực quan về thuật toán lan truyền ngược.

3
00:00:11,040 --> 00:00:14,220
Ở đây chúng ta trang trọng hơn một chút và đi sâu vào phép tính có liên quan.

4
00:00:14,820 --> 00:00:16,963
Việc điều này hơi khó hiểu một chút là điều bình thường, 

5
00:00:16,963 --> 00:00:20,159
vì vậy câu thần chú thường xuyên tạm dừng và suy ngẫm chắc chắn được áp dụng nhiều ở 

6
00:00:20,159 --> 00:00:21,400
đây cũng như bất kỳ nơi nào khác.

7
00:00:21,940 --> 00:00:25,856
Mục tiêu chính của chúng tôi là cho thấy cách mọi người trong lĩnh vực học máy 

8
00:00:25,856 --> 00:00:29,227
thường nghĩ về quy tắc dây chuyền từ phép tính trong bối cảnh mạng, 

9
00:00:29,227 --> 00:00:33,640
điều này có cảm giác khác với cách hầu hết các khóa học tính toán cơ bản tiếp cận chủ đề.

10
00:00:34,340 --> 00:00:37,078
Đối với những người không thoải mái với phép tính liên quan, 

11
00:00:37,078 --> 00:00:38,740
tôi có cả một loạt bài về chủ đề này.

12
00:00:39,960 --> 00:00:46,020
Hãy bắt đầu với một mạng cực kỳ đơn giản, trong đó mỗi lớp có một nơ-ron duy nhất.

13
00:00:46,320 --> 00:00:50,538
Mạng này được xác định bởi ba trọng số và ba độ lệch và mục tiêu của 

14
00:00:50,538 --> 00:00:54,880
chúng tôi là hiểu mức độ nhạy cảm của hàm chi phí đối với các biến này.

15
00:00:55,419 --> 00:00:58,898
Bằng cách đó, chúng tôi biết những điều chỉnh nào đối với các 

16
00:00:58,898 --> 00:01:02,320
điều khoản đó sẽ làm giảm hàm chi phí một cách hiệu quả nhất.

17
00:01:02,320 --> 00:01:04,840
Chúng ta sẽ chỉ tập trung vào kết nối giữa hai nơ-ron cuối cùng.

18
00:01:05,980 --> 00:01:09,887
Hãy gắn nhãn kích hoạt của nơ-ron cuối cùng đó bằng chữ L siêu hạng, 

19
00:01:09,887 --> 00:01:11,360
cho biết nó nằm ở lớp nào.

20
00:01:11,680 --> 00:01:15,560
Vậy sự kích hoạt của nơron trước đó là AL-1.

21
00:01:16,360 --> 00:01:19,743
Đây không phải là số mũ, chúng chỉ là một cách lập chỉ mục những gì chúng ta 

22
00:01:19,743 --> 00:01:23,040
đang nói đến, vì sau này tôi muốn lưu chỉ số dưới cho các chỉ số khác nhau.

23
00:01:23,720 --> 00:01:27,823
Giả sử giá trị mà chúng ta muốn lần kích hoạt cuối cùng này dành 

24
00:01:27,823 --> 00:01:32,180
cho một ví dụ huấn luyện nhất định là y, ví dụ: y có thể là 0 hoặc 1.

25
00:01:32,840 --> 00:01:39,240
Vì vậy, chi phí của mạng này cho một ví dụ huấn luyện là AL-y2.

26
00:01:40,260 --> 00:01:44,380
Chúng ta sẽ biểu thị chi phí của một ví dụ đào tạo đó là c0.

27
00:01:45,900 --> 00:01:50,450
Xin nhắc lại, lần kích hoạt cuối cùng này được xác định bởi trọng số, 

28
00:01:50,450 --> 00:01:56,299
mà tôi sẽ gọi là wL, nhân với lần kích hoạt của nơ-ron trước đó cộng với một số sai lệch, 

29
00:01:56,299 --> 00:01:57,600
mà tôi sẽ gọi là bL.

30
00:01:57,600 --> 00:02:01,320
Sau đó, bạn bơm nó thông qua một số hàm phi tuyến đặc biệt như sigmoid hoặc ReLU.

31
00:02:01,800 --> 00:02:05,624
Thực ra, mọi việc sẽ dễ dàng hơn cho chúng ta nếu chúng ta đặt một tên đặc biệt cho tổng 

32
00:02:05,624 --> 00:02:09,320
có trọng số này, chẳng hạn như z, với cùng chỉ số trên như các kích hoạt có liên quan.

33
00:02:10,380 --> 00:02:16,256
Đây là rất nhiều thuật ngữ và bạn có thể khái niệm hóa nó bằng cách sử dụng trọng số, 

34
00:02:16,256 --> 00:02:21,175
tác động trước đó và độ lệch để tính z, từ đó cho phép chúng ta tính a, 

35
00:02:21,175 --> 00:02:25,480
cuối cùng, cùng với hằng số y, hãy chúng tôi tính toán chi phí.

36
00:02:27,340 --> 00:02:31,365
Và tất nhiên, AL-1 bị ảnh hưởng bởi trọng lượng và độ lệch của chính nó, 

37
00:02:31,365 --> 00:02:35,060
v. v. , nhưng chúng ta sẽ không tập trung vào điều đó ngay bây giờ.

38
00:02:35,700 --> 00:02:37,620
Tất cả chỉ là những con số thôi phải không?

39
00:02:38,060 --> 00:02:41,040
Và thật tuyệt khi nghĩ mỗi người đều có trục số nhỏ của riêng mình.

40
00:02:41,720 --> 00:02:45,275
Mục tiêu đầu tiên của chúng ta là hiểu mức độ nhạy cảm của hàm 

41
00:02:45,275 --> 00:02:49,000
chi phí đối với những thay đổi nhỏ trong trọng số wL của chúng ta.

42
00:02:49,540 --> 00:02:54,860
Hay nói cách khác, đạo hàm của c theo wL bằng bao nhiêu?

43
00:02:55,600 --> 00:02:59,753
Khi bạn nhìn thấy thuật ngữ del w này, hãy nghĩ nó có nghĩa là một sự 

44
00:02:59,753 --> 00:03:03,728
dịch chuyển nhỏ nào đó tới w, chẳng hạn như sự thay đổi bằng 0.01, 

45
00:03:03,728 --> 00:03:08,060
và coi thuật ngữ del c này có nghĩa là bất kể tác động lên chi phí là gì.

46
00:03:08,060 --> 00:03:10,220
Những gì chúng tôi muốn là tỷ lệ của họ.

47
00:03:11,260 --> 00:03:16,185
Về mặt khái niệm, sự tác động nhỏ tới wL này gây ra một số tác động tới zL, 

48
00:03:16,185 --> 00:03:21,240
từ đó gây ra một số tác động tới AL, điều này ảnh hưởng trực tiếp đến chi phí.

49
00:03:23,120 --> 00:03:28,160
Vì vậy, trước tiên chúng ta chia nhỏ mọi thứ bằng cách xem xét tỉ số của một 

50
00:03:28,160 --> 00:03:33,200
thay đổi nhỏ của zL với thay đổi nhỏ này w, tức là đạo hàm của zL đối với wL.

51
00:03:33,200 --> 00:03:36,877
Tương tự như vậy, sau đó bạn xem xét tỷ lệ giữa sự thay đổi 

52
00:03:36,877 --> 00:03:39,941
của AL với sự thay đổi nhỏ trong zL đã gây ra nó, 

53
00:03:39,941 --> 00:03:44,660
cũng như tỷ lệ giữa cú hích cuối cùng với c và cú hích trung gian này với AL.

54
00:03:45,740 --> 00:03:50,364
Đây chính là quy tắc dây chuyền, trong đó việc nhân ba tỷ lệ 

55
00:03:50,364 --> 00:03:55,140
này cho chúng ta độ nhạy của c với những thay đổi nhỏ trong wL.

56
00:03:56,880 --> 00:03:59,982
Vì vậy, trên màn hình ngay bây giờ, có rất nhiều ký hiệu, 

57
00:03:59,982 --> 00:04:03,405
và hãy dành chút thời gian để đảm bảo rằng chúng rõ ràng là gì, 

58
00:04:03,405 --> 00:04:06,240
bởi vì bây giờ chúng ta sẽ tính đạo hàm có liên quan.

59
00:04:07,440 --> 00:04:14,180
Đạo hàm của c theo AL là 2AL-y.

60
00:04:14,180 --> 00:04:18,537
Điều này có nghĩa là kích thước của nó tỷ lệ thuận với sự khác biệt giữa đầu 

61
00:04:18,537 --> 00:04:22,725
ra của mạng và thứ chúng ta mong muốn, vì vậy nếu đầu ra đó rất khác nhau 

62
00:04:22,725 --> 00:04:27,140
thì ngay cả những thay đổi nhỏ cũng có tác động lớn đến hàm chi phí cuối cùng.

63
00:04:27,840 --> 00:04:32,509
Đạo hàm của AL theo zL chỉ là đạo hàm của hàm sigmoid của 

64
00:04:32,509 --> 00:04:37,420
chúng tôi hoặc bất kỳ tính phi tuyến nào mà bạn chọn sử dụng.

65
00:04:37,420 --> 00:04:46,160
Đạo hàm của zL theo wL là AL-1.

66
00:04:46,160 --> 00:04:49,724
Không biết bạn thế nào, nhưng tôi nghĩ bạn rất dễ bị mắc kẹt trong các công thức 

67
00:04:49,724 --> 00:04:53,420
mà không dành một chút thời gian để ngồi lại và nhắc nhở bản thân ý nghĩa của chúng.

68
00:04:53,920 --> 00:04:58,277
Trong trường hợp của đạo hàm cuối cùng này, mức độ ảnh hưởng của trọng 

69
00:04:58,277 --> 00:05:02,820
lượng nhỏ đến lớp cuối cùng phụ thuộc vào mức độ mạnh của nơ-ron trước đó.

70
00:05:03,380 --> 00:05:08,280
Hãy nhớ rằng, đây chính là lúc ý tưởng kết hợp các nơ-ron thần kinh với nhau xuất hiện.

71
00:05:09,200 --> 00:05:15,720
Và tất cả những điều này chỉ là đạo hàm của wL chi phí cho một ví dụ đào tạo cụ thể.

72
00:05:16,440 --> 00:05:20,553
Vì hàm chi phí đầy đủ liên quan đến việc tính trung bình tất cả các 

73
00:05:20,553 --> 00:05:23,396
chi phí đó trên nhiều ví dụ đào tạo khác nhau, 

74
00:05:23,396 --> 00:05:28,660
nên đạo hàm của nó yêu cầu tính trung bình biểu thức này trên tất cả các ví dụ đào tạo.

75
00:05:28,660 --> 00:05:32,367
Tất nhiên, đó chỉ là một thành phần của vectơ gradient, 

76
00:05:32,367 --> 00:05:38,260
được xây dựng từ đạo hàm riêng của hàm chi phí đối với tất cả các trọng số và độ lệch đó.

77
00:05:40,640 --> 00:05:43,706
Nhưng mặc dù đó chỉ là một trong nhiều đạo hàm riêng phần mà chúng ta cần, 

78
00:05:43,706 --> 00:05:45,260
nhưng nó cũng chiếm hơn 50% công việc.

79
00:05:46,340 --> 00:05:49,720
Ví dụ, độ nhạy đối với sự thiên vị gần như giống hệt nhau.

80
00:05:50,040 --> 00:05:55,020
Chúng ta chỉ cần đổi số hạng del z del w này thành a del z del b.

81
00:05:58,420 --> 00:06:02,400
Và nếu bạn nhìn vào công thức liên quan, đạo hàm đó sẽ bằng 1.

82
00:06:06,140 --> 00:06:09,806
Ngoài ra, và đây là lúc nảy sinh ý tưởng truyền ngược, 

83
00:06:09,806 --> 00:06:15,740
bạn có thể thấy hàm chi phí này nhạy cảm như thế nào đối với việc kích hoạt lớp trước đó.

84
00:06:15,740 --> 00:06:20,700
Cụ thể, đạo hàm ban đầu này trong biểu thức quy tắc dây chuyền, 

85
00:06:20,700 --> 00:06:25,660
độ nhạy của z đối với lần kích hoạt trước đó, sẽ là trọng số wL.

86
00:06:26,640 --> 00:06:30,550
Và một lần nữa, mặc dù chúng ta sẽ không thể ảnh hưởng trực tiếp đến việc 

87
00:06:30,550 --> 00:06:34,302
kích hoạt lớp trước đó, nhưng việc theo dõi vẫn rất hữu ích vì bây giờ 

88
00:06:34,302 --> 00:06:38,265
chúng ta có thể tiếp tục lặp lại ý tưởng quy tắc chuỗi tương tự này để xem 

89
00:06:38,265 --> 00:06:42,440
hàm chi phí nhạy cảm như thế nào đối với trọng số trước đó và độ lệch trước đó.

90
00:06:43,180 --> 00:06:45,793
Và bạn có thể nghĩ rằng đây là một ví dụ quá đơn giản, 

91
00:06:45,793 --> 00:06:49,737
vì tất cả các lớp đều có một nơ-ron và mọi thứ sẽ trở nên phức tạp hơn theo cấp số 

92
00:06:49,737 --> 00:06:51,020
nhân đối với một mạng thực.

93
00:06:51,700 --> 00:06:55,280
Nhưng thành thật mà nói, không có nhiều thay đổi khi chúng tôi cung cấp 

94
00:06:55,280 --> 00:06:58,860
cho các lớp nhiều nơ-ron, thực sự đó chỉ là một vài chỉ số cần theo dõi.

95
00:06:59,380 --> 00:07:03,041
Thay vì kích hoạt một lớp nhất định chỉ đơn giản là AL, 

96
00:07:03,041 --> 00:07:07,160
nó cũng sẽ có chỉ số dưới cho biết đó là nơ-ron nào của lớp đó.

97
00:07:07,160 --> 00:07:14,420
Hãy sử dụng chữ k để lập chỉ mục cho lớp L-1 và j để lập chỉ mục cho lớp L.

98
00:07:15,260 --> 00:07:19,019
Về chi phí, một lần nữa chúng ta xem xét đầu ra mong muốn là bao nhiêu, 

99
00:07:19,019 --> 00:07:22,256
nhưng lần này chúng ta cộng bình phương của sự khác biệt giữa 

100
00:07:22,256 --> 00:07:25,180
các lần kích hoạt lớp cuối cùng này và đầu ra mong muốn.

101
00:07:26,080 --> 00:07:30,840
Nghĩa là, bạn lấy tổng trên ALj trừ yj bình phương.

102
00:07:33,040 --> 00:07:38,944
Vì có nhiều trọng số hơn nên mỗi cái phải có thêm một vài chỉ số để theo dõi vị trí 

103
00:07:38,944 --> 00:07:44,920
của nó, vì vậy hãy gọi trọng số của cạnh nối nơ-ron thứ k này với nơ-ron thứ j, WLjk.

104
00:07:45,620 --> 00:07:48,188
Ban đầu, các chỉ số đó có thể hơi ngược một chút, 

105
00:07:48,188 --> 00:07:51,938
nhưng nó phù hợp với cách bạn lập chỉ mục cho ma trận trọng số mà tôi đã 

106
00:07:51,938 --> 00:07:53,120
nói trong video phần 1.

107
00:07:53,620 --> 00:07:58,919
Cũng như trước đây, bạn vẫn nên đặt tên cho tổng có trọng số liên quan, chẳng hạn như z, 

108
00:07:58,919 --> 00:08:04,160
để việc kích hoạt lớp cuối cùng chỉ là hàm đặc biệt của bạn, như sigmoid, áp dụng cho z.

109
00:08:04,660 --> 00:08:07,623
Bạn có thể hiểu ý tôi, trong đó tất cả những phương trình này về cơ 

110
00:08:07,623 --> 00:08:10,760
bản đều giống các phương trình mà chúng ta đã có trước đây trong trường 

111
00:08:10,760 --> 00:08:13,680
hợp một nơ-ron trên mỗi lớp, chỉ là nó trông phức tạp hơn một chút.

112
00:08:15,440 --> 00:08:19,400
Và thực sự, biểu thức đạo hàm quy tắc dây chuyền mô tả mức độ nhạy 

113
00:08:19,400 --> 00:08:23,420
cảm của chi phí đối với một trọng số cụ thể về cơ bản là giống nhau.

114
00:08:23,920 --> 00:08:26,840
Tôi sẽ để bạn tạm dừng và suy nghĩ về từng điều khoản đó nếu bạn muốn.

115
00:08:28,979 --> 00:08:32,819
Tuy nhiên, điều thay đổi ở đây là đạo hàm của chi 

116
00:08:32,819 --> 00:08:36,659
phí đối với một trong các kích hoạt trong lớp L-1.

117
00:08:37,780 --> 00:08:40,351
Trong trường hợp này, sự khác biệt là tế bào thần kinh ảnh 

118
00:08:40,351 --> 00:08:42,880
hưởng đến hàm chi phí thông qua nhiều con đường khác nhau.

119
00:08:44,680 --> 00:08:50,310
Nghĩa là, một mặt, nó ảnh hưởng đến AL0, vốn đóng một vai trò trong hàm chi phí, 

120
00:08:50,310 --> 00:08:55,663
nhưng nó cũng có ảnh hưởng đến AL1, cũng đóng một vai trò trong hàm chi phí, 

121
00:08:55,663 --> 00:08:57,540
và bạn phải cộng chúng lại.

122
00:08:59,820 --> 00:09:03,040
Và đó, ồ, đại khái là như vậy.

123
00:09:03,500 --> 00:09:06,586
Khi bạn biết mức độ nhạy cảm của hàm chi phí đối với các kích 

124
00:09:06,586 --> 00:09:09,673
hoạt trong lớp thứ hai đến lớp cuối cùng này, bạn chỉ cần lặp 

125
00:09:09,673 --> 00:09:12,860
lại quy trình cho tất cả các trọng số và độ lệch đưa vào lớp đó.

126
00:09:13,900 --> 00:09:14,960
Vì vậy hãy vỗ nhẹ vào lưng mình!

127
00:09:15,300 --> 00:09:18,965
Nếu tất cả những điều này đều hợp lý thì giờ đây bạn đã tìm hiểu sâu về cốt 

128
00:09:18,965 --> 00:09:22,680
lõi của lan truyền ngược, nền tảng đằng sau cách mạng lưới thần kinh học hỏi.

129
00:09:23,300 --> 00:09:28,388
Các biểu thức quy tắc chuỗi này cung cấp cho bạn các đạo hàm xác định từng thành phần 

130
00:09:28,388 --> 00:09:33,300
trong gradient giúp giảm thiểu chi phí của mạng bằng cách liên tục giảm dần độ dốc.

131
00:09:34,300 --> 00:09:36,846
Nếu bạn ngồi lại và suy nghĩ về tất cả những điều đó, 

132
00:09:36,846 --> 00:09:39,486
thì đây là rất nhiều lớp phức tạp bao trùm tâm trí bạn, 

133
00:09:39,486 --> 00:09:42,740
vì vậy đừng lo lắng nếu tâm trí bạn cần thời gian để tiêu hóa tất cả.

