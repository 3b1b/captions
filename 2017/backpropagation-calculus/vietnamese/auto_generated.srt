1
00:00:00,000 --> 00:00:05,687
Giả định khó khăn ở đây là bạn đã xem phần 3, đưa ra

2
00:00:05,687 --> 00:00:11,160
hướng dẫn trực quan về thuật toán lan truyền ngược.

3
00:00:11,160 --> 00:00:14,920
Ở đây chúng ta trang trọng hơn một chút và đi sâu vào phép tính có liên quan.

4
00:00:14,920 --> 00:00:17,226
Việc điều này hơi khó hiểu một chút là điều bình thường,

5
00:00:17,226 --> 00:00:20,664
vì vậy câu thần chú thường xuyên tạm dừng và suy ngẫm chắc chắn được áp dụng nhiều ở

6
00:00:20,664 --> 00:00:22,000
đây cũng như bất kỳ nơi nào khác.

7
00:00:22,000 --> 00:00:26,211
Mục tiêu chính của chúng tôi là cho thấy cách mọi người trong lĩnh vực học máy

8
00:00:26,211 --> 00:00:29,835
thường nghĩ về quy tắc dây chuyền từ phép tính trong bối cảnh mạng,

9
00:00:29,835 --> 00:00:34,580
điều này có cảm giác khác với cách hầu hết các khóa học tính toán cơ bản tiếp cận chủ đề.

10
00:00:34,580 --> 00:00:37,517
Đối với những người không thoải mái với phép tính liên quan,

11
00:00:37,517 --> 00:00:39,300
tôi có cả một loạt bài về chủ đề này.

12
00:00:39,300 --> 00:00:46,780
Hãy bắt đầu với một mạng cực kỳ đơn giản, trong đó mỗi lớp có một nơ-ron duy nhất.

13
00:00:46,780 --> 00:00:51,146
Mạng này được xác định bởi ba trọng số và ba độ lệch và mục tiêu của

14
00:00:51,146 --> 00:00:55,640
chúng tôi là hiểu mức độ nhạy cảm của hàm chi phí đối với các biến này.

15
00:00:55,640 --> 00:00:58,392
Bằng cách đó, chúng tôi biết những điều chỉnh nào đối với các

16
00:00:58,392 --> 00:01:01,100
điều khoản đó sẽ làm giảm hàm chi phí một cách hiệu quả nhất.

17
00:01:01,100 --> 00:01:05,360
Chúng ta sẽ chỉ tập trung vào kết nối giữa hai nơ-ron cuối cùng.

18
00:01:05,360 --> 00:01:10,037
Hãy gắn nhãn kích hoạt của nơ-ron cuối cùng đó bằng chữ L siêu hạng,

19
00:01:10,037 --> 00:01:11,800
cho biết nó nằm ở lớp nào.

20
00:01:11,800 --> 00:01:16,560
Vậy sự kích hoạt của nơron trước đó là AL-1.

21
00:01:16,560 --> 00:01:20,126
Đây không phải là số mũ, chúng chỉ là một cách lập chỉ mục những gì chúng ta

22
00:01:20,126 --> 00:01:23,600
đang nói đến, vì sau này tôi muốn lưu chỉ số dưới cho các chỉ số khác nhau.

23
00:01:23,600 --> 00:01:28,169
Giả sử giá trị mà chúng ta muốn lần kích hoạt cuối cùng này dành

24
00:01:28,169 --> 00:01:33,020
cho một ví dụ huấn luyện nhất định là y, ví dụ: y có thể là 0 hoặc 1.

25
00:01:33,020 --> 00:01:39,040
Vì vậy, chi phí của mạng này cho một ví dụ huấn luyện là AL-y2.

26
00:01:39,040 --> 00:01:46,120
Chúng ta sẽ biểu thị chi phí của một ví dụ đào tạo đó là c0.

27
00:01:46,120 --> 00:01:50,584
Xin nhắc lại, lần kích hoạt cuối cùng này được xác định bởi trọng số,

28
00:01:50,584 --> 00:01:56,324
mà tôi sẽ gọi là wL, nhân với lần kích hoạt của nơ-ron trước đó cộng với một số sai lệch,

29
00:01:56,324 --> 00:01:57,600
mà tôi sẽ gọi là bL.

30
00:01:57,600 --> 00:02:01,560
Sau đó, bạn bơm nó thông qua một số hàm phi tuyến đặc biệt như sigmoid hoặc ReLU.

31
00:02:01,560 --> 00:02:06,157
Thực ra, mọi việc sẽ dễ dàng hơn cho chúng ta nếu chúng ta đặt một tên đặc biệt cho tổng

32
00:02:06,157 --> 00:02:10,600
có trọng số này, chẳng hạn như z, với cùng chỉ số trên như các kích hoạt có liên quan.

33
00:02:10,600 --> 00:02:17,121
Đây là rất nhiều thuật ngữ và bạn có thể khái niệm hóa nó bằng cách sử dụng trọng số,

34
00:02:17,121 --> 00:02:22,582
tác động trước đó và độ lệch để tính z, từ đó cho phép chúng ta tính a,

35
00:02:22,582 --> 00:02:27,360
cuối cùng, cùng với hằng số y, hãy chúng tôi tính toán chi phí.

36
00:02:27,360 --> 00:02:31,823
Và tất nhiên, AL-1 bị ảnh hưởng bởi trọng lượng và độ lệch của chính nó,

37
00:02:31,823 --> 00:02:35,920
v. v. , nhưng chúng ta sẽ không tập trung vào điều đó ngay bây giờ.

38
00:02:35,920 --> 00:02:38,120
Tất cả chỉ là những con số thôi phải không?

39
00:02:38,120 --> 00:02:41,960
Và thật tuyệt khi nghĩ mỗi người đều có trục số nhỏ của riêng mình.

40
00:02:41,960 --> 00:02:45,798
Mục tiêu đầu tiên của chúng ta là hiểu mức độ nhạy cảm của hàm

41
00:02:45,798 --> 00:02:49,820
chi phí đối với những thay đổi nhỏ trong trọng số wL của chúng ta.

42
00:02:49,820 --> 00:02:55,740
Hay nói cách khác, đạo hàm của c theo wL bằng bao nhiêu?

43
00:02:55,740 --> 00:03:00,100
Khi bạn nhìn thấy thuật ngữ del w này, hãy nghĩ nó có nghĩa là một sự

44
00:03:00,100 --> 00:03:04,273
dịch chuyển nhỏ nào đó tới w, chẳng hạn như sự thay đổi bằng 0.01,

45
00:03:04,273 --> 00:03:08,820
và coi thuật ngữ del c này có nghĩa là bất kể tác động lên chi phí là gì.

46
00:03:08,820 --> 00:03:10,900
Những gì chúng tôi muốn là tỷ lệ của họ.

47
00:03:10,900 --> 00:03:16,980
Về mặt khái niệm, sự tác động nhỏ tới wL này gây ra một số tác động tới zL,

48
00:03:16,980 --> 00:03:23,220
từ đó gây ra một số tác động tới AL, điều này ảnh hưởng trực tiếp đến chi phí.

49
00:03:23,220 --> 00:03:28,280
Vì vậy, trước tiên chúng ta chia nhỏ mọi thứ bằng cách xem xét tỉ số của một

50
00:03:28,280 --> 00:03:33,340
thay đổi nhỏ của zL với thay đổi nhỏ này w, tức là đạo hàm của zL đối với wL.

51
00:03:33,340 --> 00:03:37,369
Tương tự như vậy, sau đó bạn xem xét tỷ lệ giữa sự thay đổi

52
00:03:37,369 --> 00:03:40,728
của AL với sự thay đổi nhỏ trong zL đã gây ra nó,

53
00:03:40,728 --> 00:03:45,900
cũng như tỷ lệ giữa cú hích cuối cùng với c và cú hích trung gian này với AL.

54
00:03:45,900 --> 00:03:51,527
Đây chính là quy tắc dây chuyền, trong đó việc nhân ba tỷ lệ

55
00:03:51,527 --> 00:03:57,340
này cho chúng ta độ nhạy của c với những thay đổi nhỏ trong wL.

56
00:03:57,340 --> 00:04:00,694
Vì vậy, trên màn hình ngay bây giờ, có rất nhiều ký hiệu,

57
00:04:00,694 --> 00:04:04,395
và hãy dành chút thời gian để đảm bảo rằng chúng rõ ràng là gì,

58
00:04:04,395 --> 00:04:07,460
bởi vì bây giờ chúng ta sẽ tính đạo hàm có liên quan.

59
00:04:07,460 --> 00:04:14,220
Đạo hàm của c theo AL là 2AL-y.

60
00:04:14,220 --> 00:04:18,981
Điều này có nghĩa là kích thước của nó tỷ lệ thuận với sự khác biệt giữa đầu

61
00:04:18,981 --> 00:04:23,556
ra của mạng và thứ chúng ta mong muốn, vì vậy nếu đầu ra đó rất khác nhau

62
00:04:23,556 --> 00:04:28,380
thì ngay cả những thay đổi nhỏ cũng có tác động lớn đến hàm chi phí cuối cùng.

63
00:04:28,380 --> 00:04:32,786
Đạo hàm của AL theo zL chỉ là đạo hàm của hàm sigmoid của

64
00:04:32,786 --> 00:04:37,420
chúng tôi hoặc bất kỳ tính phi tuyến nào mà bạn chọn sử dụng.

65
00:04:37,420 --> 00:04:46,180
Đạo hàm của zL theo wL là AL-1.

66
00:04:46,180 --> 00:04:50,107
Không biết bạn thế nào, nhưng tôi nghĩ bạn rất dễ bị mắc kẹt trong các công thức

67
00:04:50,107 --> 00:04:54,180
mà không dành một chút thời gian để ngồi lại và nhắc nhở bản thân ý nghĩa của chúng.

68
00:04:54,180 --> 00:04:58,606
Trong trường hợp của đạo hàm cuối cùng này, mức độ ảnh hưởng của trọng

69
00:04:58,606 --> 00:05:03,220
lượng nhỏ đến lớp cuối cùng phụ thuộc vào mức độ mạnh của nơ-ron trước đó.

70
00:05:03,220 --> 00:05:09,320
Hãy nhớ rằng, đây chính là lúc ý tưởng kết hợp các nơ-ron thần kinh với nhau xuất hiện.

71
00:05:09,320 --> 00:05:16,580
Và tất cả những điều này chỉ là đạo hàm của wL chi phí cho một ví dụ đào tạo cụ thể.

72
00:05:16,580 --> 00:05:20,606
Vì hàm chi phí đầy đủ liên quan đến việc tính trung bình tất cả các

73
00:05:20,606 --> 00:05:23,388
chi phí đó trên nhiều ví dụ đào tạo khác nhau,

74
00:05:23,388 --> 00:05:28,540
nên đạo hàm của nó yêu cầu tính trung bình biểu thức này trên tất cả các ví dụ đào tạo.

75
00:05:28,540 --> 00:05:33,267
Tất nhiên, đó chỉ là một thành phần của vectơ gradient,

76
00:05:33,267 --> 00:05:40,780
được xây dựng từ đạo hàm riêng của hàm chi phí đối với tất cả các trọng số và độ lệch đó.

77
00:05:40,780 --> 00:05:44,549
Nhưng mặc dù đó chỉ là một trong nhiều đạo hàm riêng phần mà chúng ta cần,

78
00:05:44,549 --> 00:05:46,460
nhưng nó cũng chiếm hơn 50% công việc.

79
00:05:46,460 --> 00:05:50,300
Ví dụ, độ nhạy đối với sự thiên vị gần như giống hệt nhau.

80
00:05:50,300 --> 00:05:58,980
Chúng ta chỉ cần đổi số hạng del z del w này thành a del z del b.

81
00:05:58,980 --> 00:06:04,700
Và nếu bạn nhìn vào công thức liên quan, đạo hàm đó sẽ bằng 1.

82
00:06:04,700 --> 00:06:09,084
Ngoài ra, và đây là lúc nảy sinh ý tưởng truyền ngược,

83
00:06:09,084 --> 00:06:16,180
bạn có thể thấy hàm chi phí này nhạy cảm như thế nào đối với việc kích hoạt lớp trước đó.

84
00:06:16,180 --> 00:06:20,800
Cụ thể, đạo hàm ban đầu này trong biểu thức quy tắc dây chuyền,

85
00:06:20,800 --> 00:06:25,420
độ nhạy của z đối với lần kích hoạt trước đó, sẽ là trọng số wL.

86
00:06:25,420 --> 00:06:29,939
Và một lần nữa, mặc dù chúng ta sẽ không thể ảnh hưởng trực tiếp đến việc

87
00:06:29,939 --> 00:06:34,275
kích hoạt lớp trước đó, nhưng việc theo dõi vẫn rất hữu ích vì bây giờ

88
00:06:34,275 --> 00:06:38,855
chúng ta có thể tiếp tục lặp lại ý tưởng quy tắc chuỗi tương tự này để xem

89
00:06:38,855 --> 00:06:43,680
hàm chi phí nhạy cảm như thế nào đối với trọng số trước đó và độ lệch trước đó.

90
00:06:43,680 --> 00:06:46,226
Và bạn có thể nghĩ rằng đây là một ví dụ quá đơn giản,

91
00:06:46,226 --> 00:06:50,069
vì tất cả các lớp đều có một nơ-ron và mọi thứ sẽ trở nên phức tạp hơn theo cấp số

92
00:06:50,069 --> 00:06:51,320
nhân đối với một mạng thực.

93
00:06:51,320 --> 00:06:55,320
Nhưng thành thật mà nói, không có nhiều thay đổi khi chúng tôi cung cấp

94
00:06:55,320 --> 00:06:59,320
cho các lớp nhiều nơ-ron, thực sự đó chỉ là một vài chỉ số cần theo dõi.

95
00:06:59,320 --> 00:07:03,367
Thay vì kích hoạt một lớp nhất định chỉ đơn giản là AL,

96
00:07:03,367 --> 00:07:07,920
nó cũng sẽ có chỉ số dưới cho biết đó là nơ-ron nào của lớp đó.

97
00:07:07,920 --> 00:07:15,280
Hãy sử dụng chữ k để lập chỉ mục cho lớp L-1 và j để lập chỉ mục cho lớp L.

98
00:07:15,280 --> 00:07:19,387
Về chi phí, một lần nữa chúng ta xem xét đầu ra mong muốn là bao nhiêu,

99
00:07:19,387 --> 00:07:22,925
nhưng lần này chúng ta cộng bình phương của sự khác biệt giữa

100
00:07:22,925 --> 00:07:26,120
các lần kích hoạt lớp cuối cùng này và đầu ra mong muốn.

101
00:07:26,120 --> 00:07:33,280
Nghĩa là, bạn lấy tổng trên ALj trừ yj bình phương.

102
00:07:33,280 --> 00:07:39,473
Vì có nhiều trọng số hơn nên mỗi cái phải có thêm một vài chỉ số để theo dõi vị trí

103
00:07:39,473 --> 00:07:45,740
của nó, vì vậy hãy gọi trọng số của cạnh nối nơ-ron thứ k này với nơ-ron thứ j, WLjk.

104
00:07:45,740 --> 00:07:48,500
Ban đầu, các chỉ số đó có thể hơi ngược một chút,

105
00:07:48,500 --> 00:07:52,530
nhưng nó phù hợp với cách bạn lập chỉ mục cho ma trận trọng số mà tôi đã

106
00:07:52,530 --> 00:07:53,800
nói trong video phần 1.

107
00:07:53,800 --> 00:07:59,421
Cũng như trước đây, bạn vẫn nên đặt tên cho tổng có trọng số liên quan, chẳng hạn như z,

108
00:07:59,421 --> 00:08:04,980
để việc kích hoạt lớp cuối cùng chỉ là hàm đặc biệt của bạn, như sigmoid, áp dụng cho z.

109
00:08:04,980 --> 00:08:08,409
Bạn có thể hiểu ý tôi, trong đó tất cả những phương trình này về cơ

110
00:08:08,409 --> 00:08:12,040
bản đều giống các phương trình mà chúng ta đã có trước đây trong trường

111
00:08:12,040 --> 00:08:15,420
hợp một nơ-ron trên mỗi lớp, chỉ là nó trông phức tạp hơn một chút.

112
00:08:15,420 --> 00:08:19,449
Và thực sự, biểu thức đạo hàm quy tắc dây chuyền mô tả mức độ nhạy

113
00:08:19,449 --> 00:08:23,540
cảm của chi phí đối với một trọng số cụ thể về cơ bản là giống nhau.

114
00:08:23,540 --> 00:08:29,420
Tôi sẽ để bạn tạm dừng và suy nghĩ về từng điều khoản đó nếu bạn muốn.

115
00:08:29,420 --> 00:08:33,620
Tuy nhiên, điều thay đổi ở đây là đạo hàm của chi

116
00:08:33,620 --> 00:08:37,820
phí đối với một trong các kích hoạt trong lớp L-1.

117
00:08:37,820 --> 00:08:40,704
Trong trường hợp này, sự khác biệt là tế bào thần kinh ảnh

118
00:08:40,704 --> 00:08:43,540
hưởng đến hàm chi phí thông qua nhiều con đường khác nhau.

119
00:08:43,540 --> 00:08:50,895
Nghĩa là, một mặt, nó ảnh hưởng đến AL0, vốn đóng một vai trò trong hàm chi phí,

120
00:08:50,895 --> 00:08:57,888
nhưng nó cũng có ảnh hưởng đến AL1, cũng đóng một vai trò trong hàm chi phí,

121
00:08:57,888 --> 00:09:00,340
và bạn phải cộng chúng lại.

122
00:09:00,340 --> 00:09:03,680
Và đó, ồ, đại khái là như vậy.

123
00:09:03,680 --> 00:09:07,057
Khi bạn biết mức độ nhạy cảm của hàm chi phí đối với các kích

124
00:09:07,057 --> 00:09:10,434
hoạt trong lớp thứ hai đến lớp cuối cùng này, bạn chỉ cần lặp

125
00:09:10,434 --> 00:09:13,920
lại quy trình cho tất cả các trọng số và độ lệch đưa vào lớp đó.

126
00:09:13,920 --> 00:09:15,420
Vì vậy hãy vỗ nhẹ vào lưng mình!

127
00:09:15,420 --> 00:09:19,532
Nếu tất cả những điều này đều hợp lý thì giờ đây bạn đã tìm hiểu sâu về cốt

128
00:09:19,532 --> 00:09:23,700
lõi của lan truyền ngược, nền tảng đằng sau cách mạng lưới thần kinh học hỏi.

129
00:09:23,700 --> 00:09:29,460
Các biểu thức quy tắc chuỗi này cung cấp cho bạn các đạo hàm xác định từng thành phần

130
00:09:29,460 --> 00:09:35,020
trong gradient giúp giảm thiểu chi phí của mạng bằng cách liên tục giảm dần độ dốc.

131
00:09:35,020 --> 00:09:36,208
Nếu bạn ngồi lại và suy nghĩ về tất cả những điều đó,

132
00:09:36,208 --> 00:09:37,441
thì đây là rất nhiều lớp phức tạp bao trùm tâm trí bạn,

133
00:09:37,441 --> 00:09:38,960
vì vậy đừng lo lắng nếu tâm trí bạn cần thời gian để tiêu hóa tất cả.

