1
00:00:04,019 --> 00:00:07,026
Giả định khó khăn ở đây là bạn đã xem phần 3, đưa ra 

2
00:00:07,026 --> 00:00:09,920
hướng dẫn trực quan về thuật toán lan truyền ngược.

3
00:00:11,040 --> 00:00:14,220
Ở đây ta trang trọng hơn chút và đi sâu vào phép giải tích liên quan.

4
00:00:14,820 --> 00:00:18,089
Điều này hơi chút khó hiểu là bình thường, vậy câu thần chú thường xuyên là tạm 

5
00:00:18,089 --> 00:00:21,400
dừng và suy ngẫm chắc chắn được áp dụng nhiều ở đây cũng như bất kỳ nơi nào khác.

6
00:00:21,940 --> 00:00:25,857
Mục tiêu chính của ta là cho thấy cách người ta thường nghĩ trong học máy 

7
00:00:25,857 --> 00:00:29,034
về quy tắc dây chuyền từ giải tích trong bối cảnh của mạng, 

8
00:00:29,034 --> 00:00:33,640
nó có cảm giác khác với cách hầu hết tiếp cận chủ đề của các khóa học giải tích cơ bản.

9
00:00:34,340 --> 00:00:37,008
Với những người không thoải mái khi liên quan giải tích, 

10
00:00:37,008 --> 00:00:38,740
tôi có cả một loạt bài về chủ đề này.

11
00:00:39,960 --> 00:00:46,020
Hãy bắt đầu với một mạng cực kỳ đơn giản, trong đó mỗi lớp có một nơ-ron duy nhất.

12
00:00:46,320 --> 00:00:50,569
Mạng này được xác định bởi ba trọng số và ba độ lệch và mục tiêu của 

13
00:00:50,569 --> 00:00:54,880
chúng ta là hiểu mức độ nhạy cảm của hàm chi phí đối với các biến này.

14
00:00:55,420 --> 00:00:58,144
Với cách đó ta biết những điều chỉnh nào đối với các số 

15
00:00:58,144 --> 00:01:00,820
hạng đó sẽ làm giảm hàm chi phí một cách hiệu quả nhất.

16
00:01:01,960 --> 00:01:04,840
Chúng ta sẽ chỉ tập trung vào kết nối giữa hai nơ-ron cuối cùng.

17
00:01:05,980 --> 00:01:09,887
Hãy gắn nhãn kích hoạt của nơ-ron cuối cùng đó bằng chữ L siêu hạng, 

18
00:01:09,887 --> 00:01:11,360
cho biết nó nằm ở lớp nào.

19
00:01:11,680 --> 00:01:15,560
Vậy sự kích hoạt của nơron trước đó là AL-1.

20
00:01:16,360 --> 00:01:20,415
Đây không phải là số mũ, chúng chỉ là một cách lập chỉ mục những gì ta đang nói đến, 

21
00:01:20,415 --> 00:01:23,040
vì tôi muốn lưu chỉ số dưới với các chỉ số khác sau đó.

22
00:01:23,720 --> 00:01:27,823
Giả sử giá trị mà chúng ta muốn lần kích hoạt cuối cùng này dành 

23
00:01:27,823 --> 00:01:32,180
cho một ví dụ huấn luyện nhất định là y, ví dụ: y có thể là 0 hoặc 1.

24
00:01:32,840 --> 00:01:39,240
Vì vậy, chi phí của mạng này cho một ví dụ huấn luyện là AL-y2.

25
00:01:40,260 --> 00:01:44,380
Chúng ta sẽ biểu thị chi phí của một ví dụ đào tạo đó là c0.

26
00:01:45,900 --> 00:01:50,076
Xin nhắc lại, lần kích hoạt cuối cùng này được xác định bởi trọng số, 

27
00:01:50,076 --> 00:01:55,446
mà tôi sẽ gọi là wL, nhân với lần kích hoạt của nơ-ron trước đó cộng với một số sai lệch, 

28
00:01:55,446 --> 00:01:56,640
mà tôi sẽ gọi là bL.

29
00:01:57,420 --> 00:02:01,320
Sau đó, bạn đưa nó qua một số hàm phi tuyến đặc biệt như sigmoid hoặc ReLU.

30
00:02:01,800 --> 00:02:05,624
Thực ra, mọi việc sẽ dễ dàng hơn cho chúng ta nếu chúng ta đặt một tên đặc biệt cho tổng 

31
00:02:05,624 --> 00:02:09,320
có trọng số này, chẳng hạn như z, với cùng chỉ số trên như các kích hoạt có liên quan.

32
00:02:10,380 --> 00:02:16,198
Đây là rất nhiều số hạng và bạn có thể khái niệm hóa nó bằng cách sử dụng trọng số, 

33
00:02:16,198 --> 00:02:21,185
tác động trước đó và độ lệch để tính z, từ đó cho phép chúng ta tính a, 

34
00:02:21,185 --> 00:02:25,480
cuối cùng, cùng với hằng số y, cho chúng ta tính toán chi phí.

35
00:02:27,340 --> 00:02:31,284
Và tất nhiên, AL-1 bị ảnh hưởng bởi trọng số và độ lệch của chính nó, 

36
00:02:31,284 --> 00:02:35,060
v. v. , nhưng chúng ta sẽ không tập trung vào điều đó ngay bây giờ.

37
00:02:35,700 --> 00:02:37,620
Tất cả chỉ là những con số thôi phải không?

38
00:02:38,060 --> 00:02:41,040
Và thật tuyệt khi nghĩ mỗi người đều có trục số nhỏ của riêng mình.

39
00:02:41,720 --> 00:02:45,275
Mục tiêu đầu tiên của chúng ta là hiểu mức độ nhạy cảm của hàm 

40
00:02:45,275 --> 00:02:49,000
chi phí đối với những thay đổi nhỏ trong trọng số wL của chúng ta.

41
00:02:49,540 --> 00:02:54,860
Hay nói cách khác, đạo hàm của c theo wL bằng bao nhiêu?

42
00:02:55,600 --> 00:02:59,713
Khi bạn nhìn thấy số hạng del w này, hãy nghĩ nó có nghĩa là một sự 

43
00:02:59,713 --> 00:03:03,765
dịch chuyển nhỏ nào đó tới w, chẳng hạn như sự thay đổi bằng 0.01, 

44
00:03:03,765 --> 00:03:08,060
và coi số hạng del c này có nghĩa là bất kể tác động lên chi phí là gì.

45
00:03:08,060 --> 00:03:10,220
Những gì chúng ta muốn là tỷ lệ của chúng.

46
00:03:11,260 --> 00:03:16,185
Về mặt khái niệm, sự tác động nhỏ tới wL này gây ra một số tác động tới zL, 

47
00:03:16,185 --> 00:03:21,240
từ đó gây ra một số tác động tới AL, điều này ảnh hưởng trực tiếp đến chi phí.

48
00:03:23,120 --> 00:03:28,160
Vì vậy, trước tiên chúng ta chia nhỏ mọi thứ bằng cách xem xét tỉ số của một 

49
00:03:28,160 --> 00:03:33,200
thay đổi nhỏ của zL với thay đổi nhỏ này w, tức là đạo hàm của zL đối với wL.

50
00:03:33,200 --> 00:03:37,080
Tương tự như vậy, sau đó bạn xem xét tỷ lệ giữa sự thay đổi của 

51
00:03:37,080 --> 00:03:40,779
AL với sự thay đổi nhỏ trong zL đã gây ra nó, cũng như tỷ lệ 

52
00:03:40,779 --> 00:03:44,660
giữa tác động cuối cùng với c và tác động trung gian này với AL.

53
00:03:45,740 --> 00:03:50,364
Đây chính là quy tắc dây chuyền, trong đó việc nhân ba tỷ lệ 

54
00:03:50,364 --> 00:03:55,140
này cho chúng ta độ nhạy của c với những thay đổi nhỏ trong wL.

55
00:03:56,880 --> 00:03:59,982
Vì vậy, trên màn hình ngay bây giờ, có rất nhiều ký hiệu, 

56
00:03:59,982 --> 00:04:03,405
và hãy dành chút thời gian để đảm bảo rằng chúng rõ ràng là gì, 

57
00:04:03,405 --> 00:04:06,240
bởi vì bây giờ chúng ta sẽ tính đạo hàm có liên quan.

58
00:04:07,440 --> 00:04:13,160
Đạo hàm của c theo AL là 2AL-y.

59
00:04:13,980 --> 00:04:18,404
Điều này có nghĩa là kích thước của nó tỷ lệ thuận với sự khác biệt giữa đầu 

60
00:04:18,404 --> 00:04:22,657
ra của mạng và thứ chúng ta mong muốn, vì vậy nếu đầu ra đó rất khác nhau 

61
00:04:22,657 --> 00:04:27,140
thì ngay cả những thay đổi nhỏ cũng có tác động lớn đến hàm chi phí cuối cùng.

62
00:04:27,840 --> 00:04:31,939
Đạo hàm của AL theo zL chỉ là đạo hàm của hàm sigmoid của 

63
00:04:31,939 --> 00:04:36,180
chúng ta hoặc bất kỳ tính phi tuyến nào mà bạn chọn sử dụng.

64
00:04:37,220 --> 00:04:44,660
Đạo hàm của zL theo wL là AL-1.

65
00:04:45,760 --> 00:04:49,615
Không biết bạn thế nào, nhưng tôi nghĩ bạn rất dễ bị mắc kẹt trong các công 

66
00:04:49,615 --> 00:04:53,420
thức mà không dành chút thời gian để ngồi lại và ghi nhớ ý nghĩa của chúng.

67
00:04:53,920 --> 00:04:58,370
Trong trường hợp của đạo hàm cuối cùng này, mức độ ảnh hưởng của trọng 

68
00:04:58,370 --> 00:05:02,820
số nhỏ đến lớp cuối cùng phụ thuộc vào mức độ mạnh của nơ-ron trước đó.

69
00:05:03,380 --> 00:05:08,280
Hãy nhớ rằng, đây chính là lúc ý tưởng kết hợp các nơ-ron thần kinh với nhau xuất hiện.

70
00:05:09,200 --> 00:05:15,720
Và tất cả những điều này chỉ là đạo hàm của wL chi phí cho một ví dụ đào tạo cụ thể.

71
00:05:16,440 --> 00:05:20,149
Vì hàm chi phí đầy đủ liên quan đến việc tính trung bình tất cả các 

72
00:05:20,149 --> 00:05:22,713
chi phí đó trên nhiều ví dụ đào tạo khác nhau, 

73
00:05:22,713 --> 00:05:27,460
nên đạo hàm của nó yêu cầu tính trung bình biểu thức này trên tất cả các ví dụ đào tạo.

74
00:05:28,380 --> 00:05:32,195
Tất nhiên, đó chỉ là một thành phần của vectơ gradient, 

75
00:05:32,195 --> 00:05:38,260
được xây dựng từ đạo hàm riêng của hàm chi phí đối với tất cả các trọng số và độ lệch đó.

76
00:05:40,640 --> 00:05:43,555
Nhưng dù đó chỉ là một trong nhiều đạo hàm riêng phần mà ta cần, 

77
00:05:43,555 --> 00:05:45,260
nhưng nó cũng chiếm hơn 50% công việc.

78
00:05:46,340 --> 00:05:49,720
Ví dụ, độ nhạy đối với độ lệch gần như giống hệt nhau.

79
00:05:50,040 --> 00:05:55,020
Chúng ta chỉ cần đổi số hạng del z del w này thành a del z del b.

80
00:05:58,420 --> 00:06:02,400
Và nếu bạn nhìn vào công thức liên quan, đạo hàm đó sẽ bằng 1.

81
00:06:06,140 --> 00:06:09,806
Ngoài ra, và đây là lúc nảy sinh ý tưởng truyền ngược, 

82
00:06:09,806 --> 00:06:15,740
bạn có thể thấy hàm chi phí này nhạy cảm như thế nào đối với việc kích hoạt lớp trước đó.

83
00:06:15,740 --> 00:06:20,700
Cụ thể, đạo hàm ban đầu này trong biểu thức quy tắc dây chuyền, 

84
00:06:20,700 --> 00:06:25,660
độ nhạy của z đối với lần kích hoạt trước đó, sẽ là trọng số wL.

85
00:06:26,640 --> 00:06:30,520
Và lần nữa, dù chúng ta sẽ không thể gây ảnh hưởng trực tiếp đến việc 

86
00:06:30,520 --> 00:06:34,456
kích hoạt lớp trước đó, nhưng việc theo dõi vẫn rất hữu ích vì bây giờ 

87
00:06:34,456 --> 00:06:38,337
ta có thể tiếp tục lặp lại ý tưởng quy tắc dây chuyền tương tự này để 

88
00:06:38,337 --> 00:06:42,440
xem cái cách mà hàm chi phí nhạy cảm đối với trọng số và độ lệch trước đó.

89
00:06:43,180 --> 00:06:45,793
Và bạn có thể nghĩ rằng đây là một ví dụ quá đơn giản, 

90
00:06:45,793 --> 00:06:49,737
vì tất cả các lớp đều có một nơ-ron và mọi thứ sẽ trở nên phức tạp hơn theo cấp số 

91
00:06:49,737 --> 00:06:51,020
nhân đối với một mạng thực.

92
00:06:51,700 --> 00:06:55,254
Nhưng thành thật mà nói, không có nhiều thay đổi khi chúng ta cung cấp 

93
00:06:55,254 --> 00:06:58,860
cho các lớp nhiều nơ-ron, thực sự đó chỉ là một vài chỉ số cần theo dõi.

94
00:06:59,380 --> 00:07:03,041
Thay vì kích hoạt một lớp nhất định chỉ đơn giản là AL, 

95
00:07:03,041 --> 00:07:07,160
nó cũng sẽ có chỉ số dưới cho biết đó là nơ-ron nào của lớp đó.

96
00:07:07,160 --> 00:07:14,420
Hãy sử dụng chữ k để lập chỉ mục cho lớp L-1 và j để lập chỉ mục cho lớp L.

97
00:07:15,260 --> 00:07:19,019
Về chi phí, một lần nữa chúng ta xem xét đầu ra mong muốn là bao nhiêu, 

98
00:07:19,019 --> 00:07:22,256
nhưng lần này chúng ta cộng bình phương của sự khác biệt giữa 

99
00:07:22,256 --> 00:07:25,180
các lần kích hoạt lớp cuối cùng này và đầu ra mong muốn.

100
00:07:26,080 --> 00:07:30,840
Nghĩa là, bạn lấy tổng trên ALj trừ yj bình phương.

101
00:07:33,040 --> 00:07:38,944
Vì có nhiều trọng số hơn nên mỗi cái phải có thêm một vài chỉ số để theo dõi vị trí 

102
00:07:38,944 --> 00:07:44,920
của nó, vì vậy hãy gọi trọng số của cạnh nối nơ-ron thứ k này với nơ-ron thứ j, WLjk.

103
00:07:45,620 --> 00:07:48,188
Ban đầu, các chỉ số đó có thể hơi ngược một chút, 

104
00:07:48,188 --> 00:07:51,938
nhưng nó phù hợp với cách bạn lập chỉ mục cho ma trận trọng số mà tôi đã 

105
00:07:51,938 --> 00:07:53,120
nói trong video phần 1.

106
00:07:53,620 --> 00:07:58,919
Cũng như trước đây, bạn vẫn nên đặt tên cho tổng có trọng số liên quan, chẳng hạn như z, 

107
00:07:58,919 --> 00:08:04,160
để việc kích hoạt lớp cuối cùng chỉ là hàm đặc biệt của bạn, như sigmoid, áp dụng cho z.

108
00:08:04,660 --> 00:08:07,623
Bạn có thể hiểu ý tôi, trong đó tất cả những phương trình này về cơ 

109
00:08:07,623 --> 00:08:10,760
bản đều giống các phương trình mà chúng ta đã có trước đây trong trường 

110
00:08:10,760 --> 00:08:13,680
hợp một nơ-ron trên mỗi lớp, chỉ là nó trông phức tạp hơn một chút.

111
00:08:15,440 --> 00:08:19,400
Và thực sự, biểu thức đạo hàm quy tắc dây chuyền mô tả mức độ nhạy 

112
00:08:19,400 --> 00:08:23,420
cảm của chi phí đối với một trọng số cụ thể về cơ bản là giống nhau.

113
00:08:23,920 --> 00:08:26,840
Tôi sẽ để bạn tạm dừng và suy nghĩ về từng số hạng đó nếu bạn muốn.

114
00:08:28,980 --> 00:08:32,819
Tuy nhiên, điều thay đổi ở đây là đạo hàm của chi 

115
00:08:32,819 --> 00:08:36,659
phí đối với một trong các kích hoạt trong lớp L-1.

116
00:08:37,780 --> 00:08:40,351
Trong trường hợp này, sự khác biệt là tế bào thần kinh ảnh 

117
00:08:40,351 --> 00:08:42,880
hưởng đến hàm chi phí thông qua nhiều con đường khác nhau.

118
00:08:44,680 --> 00:08:50,310
Nghĩa là, một mặt, nó ảnh hưởng đến AL0, vốn đóng một vai trò trong hàm chi phí, 

119
00:08:50,310 --> 00:08:55,663
nhưng nó cũng có ảnh hưởng đến AL1, cũng đóng một vai trò trong hàm chi phí, 

120
00:08:55,663 --> 00:08:57,540
và bạn phải cộng chúng lại.

121
00:08:59,820 --> 00:09:03,040
Và đó, ồ, đại khái là như vậy.

122
00:09:03,500 --> 00:09:06,586
Khi bạn biết mức độ nhạy cảm của hàm chi phí đối với các kích 

123
00:09:06,586 --> 00:09:09,673
hoạt trong lớp thứ hai đến lớp cuối cùng này, bạn chỉ cần lặp 

124
00:09:09,673 --> 00:09:12,860
lại quy trình cho tất cả các trọng số và độ lệch đưa vào lớp đó.

125
00:09:13,900 --> 00:09:14,960
Vì vậy hãy vỗ nhẹ vào lưng mình!

126
00:09:15,300 --> 00:09:18,965
Nếu tất cả những điều này đều hợp lý thì giờ đây bạn đã tìm hiểu sâu về cốt 

127
00:09:18,965 --> 00:09:22,680
lõi của lan truyền ngược, nền tảng đằng sau cách mạng lưới thần kinh học hỏi.

128
00:09:23,300 --> 00:09:28,242
Các biểu thức quy tắc dây chuyền này cung cấp cho bạn các đạo hàm xác định từng thành 

129
00:09:28,242 --> 00:09:33,300
phần trong gradient giúp giảm thiểu chi phí của mạng bằng cách liên tục giảm dần độ dốc.

130
00:09:34,300 --> 00:09:38,441
Nếu bạn ngồi lại và nghĩ về tất cả, thì đây là rất nhiều lớp phức tạp bao trùm 

131
00:09:38,441 --> 00:09:42,740
tâm trí bạn, vì vậy đừng lo lắng nếu tâm trí bạn cần thời gian để tiêu hóa tất cả.

