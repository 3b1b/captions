1
00:00:00,000 --> 00:00:08,420
Giả định khó khăn ở đây là bạn đã xem phần 3,

2
00:00:08,420 --> 00:00:11,160
đưa ra hướng dẫn trực quan về thuật toán lan truyền ngược.

3
00:00:11,160 --> 00:00:14,920
Ở đây chúng ta trang trọng hơn một chút và đi sâu vào phép tính có liên quan.

4
00:00:14,920 --> 00:00:18,560
Việc điều này hơi khó hiểu một chút là điều bình thường, vì vậy câu thần chú thường xuyên tạm

5
00:00:18,560 --> 00:00:22,000
dừng và suy ngẫm chắc chắn được áp dụng nhiều ở đây cũng như bất kỳ nơi nào khác.

6
00:00:22,000 --> 00:00:26,620
Mục tiêu chính của chúng tôi là cho thấy cách mọi người trong lĩnh vực học máy thường

7
00:00:26,620 --> 00:00:31,900
nghĩ về quy tắc dây chuyền từ phép tính trong bối cảnh mạng, điều này có cảm

8
00:00:31,900 --> 00:00:34,580
giác khác với cách hầu hết các khóa học tính toán cơ bản tiếp cận chủ đề.

9
00:00:34,580 --> 00:00:38,300
Đối với những người không thoải mái với phép tính liên

10
00:00:38,300 --> 00:00:39,300
quan, tôi có cả một loạt bài về chủ đề này.

11
00:00:39,300 --> 00:00:44,840
Hãy bắt đầu với một mạng cực kỳ đơn giản,

12
00:00:44,840 --> 00:00:46,780
trong đó mỗi lớp có một nơ-ron duy nhất.

13
00:00:46,780 --> 00:00:51,880
Mạng này được xác định bởi ba trọng số và ba độ lệch và mục tiêu của

14
00:00:51,880 --> 00:00:55,640
chúng tôi là hiểu mức độ nhạy cảm của hàm chi phí đối với các biến này.

15
00:00:55,640 --> 00:00:59,780
Bằng cách đó, chúng tôi biết những điều chỉnh nào đối với các điều

16
00:00:59,780 --> 00:01:01,100
khoản đó sẽ làm giảm hàm chi phí một cách hiệu quả nhất.

17
00:01:01,100 --> 00:01:05,360
Chúng ta sẽ chỉ tập trung vào kết nối giữa hai nơ-ron cuối cùng.

18
00:01:05,360 --> 00:01:10,400
Hãy gắn nhãn kích hoạt của nơ-ron cuối cùng đó bằng

19
00:01:10,400 --> 00:01:11,800
chữ L siêu hạng, cho biết nó nằm ở lớp nào.

20
00:01:11,800 --> 00:01:16,560
Vậy sự kích hoạt của nơron trước đó là AL-1.

21
00:01:16,560 --> 00:01:20,120
Đây không phải là số mũ, chúng chỉ là một cách lập chỉ mục những gì chúng ta

22
00:01:20,120 --> 00:01:23,120
đang nói đến, vì sau này tôi muốn lưu chỉ số dưới cho các chỉ số khác nhau.

23
00:01:23,600 --> 00:01:28,880
Giả sử giá trị mà chúng ta muốn lần kích hoạt cuối cùng này dành cho một

24
00:01:28,880 --> 00:01:33,020
ví dụ huấn luyện nhất định là y, ví dụ: y có thể là 0 hoặc 1.

25
00:01:33,020 --> 00:01:39,040
Vì vậy, chi phí của mạng này cho một ví dụ huấn luyện là AL-y2.

26
00:01:39,040 --> 00:01:46,120
Chúng ta sẽ biểu thị chi phí của một ví dụ đào tạo đó là c0.

27
00:01:46,120 --> 00:01:51,920
Xin nhắc lại, lần kích hoạt cuối cùng này được xác định bởi trọng số, mà tôi sẽ gọi là wL,

28
00:01:51,920 --> 00:01:57,600
nhân với lần kích hoạt của nơ-ron trước đó cộng với một số sai lệch, mà tôi sẽ gọi là bL.

29
00:01:57,600 --> 00:02:01,560
Sau đó, bạn bơm nó thông qua một số hàm phi tuyến đặc biệt như sigmoid hoặc ReLU.

30
00:02:01,560 --> 00:02:05,400
Thực ra, mọi việc sẽ dễ dàng hơn cho chúng ta nếu chúng ta đặt một tên đặc biệt cho tổng

31
00:02:05,400 --> 00:02:10,600
có trọng số này, chẳng hạn như z, với cùng chỉ số trên như các kích hoạt có liên quan.

32
00:02:10,600 --> 00:02:15,320
Đây là rất nhiều thuật ngữ và bạn có thể khái niệm hóa nó bằng cách sử dụng

33
00:02:15,320 --> 00:02:21,800
trọng số, tác động trước đó và độ lệch để tính z, từ đó cho phép chúng

34
00:02:21,800 --> 00:02:27,360
ta tính a, cuối cùng, cùng với hằng số y, hãy chúng tôi tính toán chi phí.

35
00:02:27,360 --> 00:02:33,440
Và tất nhiên, AL-1 bị ảnh hưởng bởi trọng lượng và độ lệch của chính

36
00:02:33,440 --> 00:02:35,920
nó, v.v., nhưng chúng ta sẽ không tập trung vào điều đó ngay bây giờ.

37
00:02:35,920 --> 00:02:38,120
Tất cả chỉ là những con số thôi phải không?

38
00:02:38,120 --> 00:02:41,960
Và thật tuyệt khi nghĩ mỗi người đều có trục số nhỏ của riêng mình.

39
00:02:41,960 --> 00:02:47,480
Mục tiêu đầu tiên của chúng ta là hiểu mức độ nhạy cảm của hàm

40
00:02:47,480 --> 00:02:49,820
chi phí đối với những thay đổi nhỏ trong trọng số wL của chúng ta.

41
00:02:49,820 --> 00:02:55,740
Hay nói cách khác, đạo hàm của c theo wL bằng bao nhiêu?

42
00:02:55,740 --> 00:03:01,220
Khi bạn nhìn thấy thuật ngữ del w này, hãy nghĩ nó có nghĩa là một sự dịch chuyển nhỏ nào đó tới w, chẳng hạn như sự thay đổi bằng

43
00:03:01,220 --> 00:03:08,820
0. 01, và coi thuật ngữ del c này có nghĩa là bất kể tác động lên chi phí là gì.

44
00:03:08,820 --> 00:03:10,900
Những gì chúng tôi muốn là tỷ lệ của họ.

45
00:03:10,900 --> 00:03:17,740
Về mặt khái niệm, sự tác động nhỏ tới wL này gây ra một số tác động tới zL,

46
00:03:17,740 --> 00:03:23,220
từ đó gây ra một số tác động tới AL, điều này ảnh hưởng trực tiếp đến chi phí.

47
00:03:23,220 --> 00:03:28,020
Vì vậy, trước tiên chúng ta chia nhỏ mọi thứ bằng cách xem xét tỉ số của một thay

48
00:03:28,020 --> 00:03:33,340
đổi nhỏ của zL với thay đổi nhỏ này w, tức là đạo hàm của zL đối với wL.

49
00:03:33,340 --> 00:03:38,820
Tương tự như vậy, sau đó bạn xem xét tỷ lệ giữa sự thay đổi của

50
00:03:38,820 --> 00:03:43,900
AL với sự thay đổi nhỏ trong zL đã gây ra nó, cũng như tỷ lệ

51
00:03:43,900 --> 00:03:45,900
giữa cú hích cuối cùng với c và cú hích trung gian này với AL.

52
00:03:45,900 --> 00:03:51,880
Đây chính là quy tắc dây chuyền, trong đó việc nhân ba tỷ lệ này

53
00:03:51,880 --> 00:03:57,340
cho chúng ta độ nhạy của c với những thay đổi nhỏ trong wL.

54
00:03:57,340 --> 00:04:01,620
Vì vậy, trên màn hình ngay bây giờ, có rất nhiều ký hiệu, và hãy dành chút thời gian để đảm

55
00:04:01,620 --> 00:04:07,460
bảo rằng chúng rõ ràng là gì, bởi vì bây giờ chúng ta sẽ tính đạo hàm có liên quan.

56
00:04:07,460 --> 00:04:14,220
Đạo hàm của c theo AL là 2AL-y.

57
00:04:14,220 --> 00:04:19,300
Điều này có nghĩa là kích thước của nó tỷ lệ thuận với sự khác biệt giữa đầu

58
00:04:19,300 --> 00:04:24,480
ra của mạng và thứ chúng ta mong muốn, vì vậy nếu đầu ra đó rất khác nhau

59
00:04:24,480 --> 00:04:28,380
thì ngay cả những thay đổi nhỏ cũng có tác động lớn đến hàm chi phí cuối cùng.

60
00:04:28,380 --> 00:04:33,860
Đạo hàm của AL theo zL chỉ là đạo hàm của hàm sigmoid của

61
00:04:33,860 --> 00:04:37,420
chúng tôi hoặc bất kỳ tính phi tuyến nào mà bạn chọn sử dụng.

62
00:04:37,420 --> 00:04:46,180
Đạo hàm của zL theo wL là AL-1.

63
00:04:46,180 --> 00:04:49,460
Không biết bạn thế nào, nhưng tôi nghĩ bạn rất dễ bị mắc kẹt trong các công thức mà

64
00:04:49,460 --> 00:04:54,180
không dành một chút thời gian để ngồi lại và nhắc nhở bản thân ý nghĩa của chúng.

65
00:04:54,180 --> 00:04:58,860
Trong trường hợp của đạo hàm cuối cùng này, mức độ ảnh hưởng của trọng lượng

66
00:04:58,860 --> 00:05:03,220
nhỏ đến lớp cuối cùng phụ thuộc vào mức độ mạnh của nơ-ron trước đó.

67
00:05:03,220 --> 00:05:09,320
Hãy nhớ rằng, đây chính là lúc ý tưởng kết hợp các nơ-ron thần kinh với nhau xuất hiện.

68
00:05:09,320 --> 00:05:14,840
Và tất cả những điều này chỉ là đạo hàm của

69
00:05:14,840 --> 00:05:16,580
wL chi phí cho một ví dụ đào tạo cụ thể.

70
00:05:16,580 --> 00:05:20,940
Vì hàm chi phí đầy đủ liên quan đến việc tính trung bình tất cả các

71
00:05:20,940 --> 00:05:27,300
chi phí đó trên nhiều ví dụ đào tạo khác nhau, nên đạo hàm của nó

72
00:05:27,300 --> 00:05:28,540
yêu cầu tính trung bình biểu thức này trên tất cả các ví dụ đào tạo.

73
00:05:28,540 --> 00:05:33,860
Tất nhiên, đó chỉ là một thành phần của vectơ gradient, được xây dựng từ đạo hàm

74
00:05:33,860 --> 00:05:40,780
riêng của hàm chi phí đối với tất cả các trọng số và độ lệch đó.

75
00:05:40,780 --> 00:05:44,340
Nhưng mặc dù đó chỉ là một trong nhiều đạo hàm riêng phần

76
00:05:44,340 --> 00:05:46,460
mà chúng ta cần, nhưng nó cũng chiếm hơn 50% công việc.

77
00:05:46,460 --> 00:05:50,300
Ví dụ, độ nhạy đối với sự thiên vị gần như giống hệt nhau.

78
00:05:50,300 --> 00:05:58,980
Chúng ta chỉ cần đổi số hạng del z del w này thành a del z del b.

79
00:05:58,980 --> 00:06:04,700
Và nếu bạn nhìn vào công thức liên quan, đạo hàm đó sẽ bằng 1.

80
00:06:04,700 --> 00:06:11,700
Ngoài ra, và đây là lúc nảy sinh ý tưởng truyền ngược, bạn có thể thấy hàm

81
00:06:11,700 --> 00:06:16,180
chi phí này nhạy cảm như thế nào đối với việc kích hoạt lớp trước đó.

82
00:06:16,180 --> 00:06:21,380
Cụ thể, đạo hàm ban đầu này trong biểu thức quy tắc dây chuyền, độ

83
00:06:21,380 --> 00:06:25,420
nhạy của z đối với lần kích hoạt trước đó, sẽ là trọng số wL.

84
00:06:25,420 --> 00:06:30,100
Và một lần nữa, mặc dù chúng ta sẽ không thể ảnh hưởng trực tiếp đến việc kích

85
00:06:30,100 --> 00:06:35,280
hoạt lớp trước đó, nhưng việc theo dõi vẫn rất hữu ích vì bây giờ chúng ta

86
00:06:35,280 --> 00:06:40,780
có thể tiếp tục lặp lại ý tưởng quy tắc chuỗi tương tự này để xem hàm chi

87
00:06:40,780 --> 00:06:43,680
phí nhạy cảm như thế nào đối với trọng số trước đó và độ lệch trước đó.

88
00:06:43,680 --> 00:06:47,940
Và bạn có thể nghĩ rằng đây là một ví dụ quá đơn giản, vì tất cả các lớp đều có

89
00:06:47,940 --> 00:06:51,320
một nơ-ron và mọi thứ sẽ trở nên phức tạp hơn theo cấp số nhân đối với một mạng thực.

90
00:06:51,320 --> 00:06:56,560
Nhưng thành thật mà nói, không có nhiều thay đổi khi chúng tôi cung cấp cho

91
00:06:56,560 --> 00:06:59,320
các lớp nhiều nơ-ron, thực sự đó chỉ là một vài chỉ số cần theo dõi.

92
00:06:59,320 --> 00:07:03,580
Thay vì kích hoạt một lớp nhất định chỉ đơn giản là AL, nó cũng

93
00:07:03,580 --> 00:07:07,920
sẽ có chỉ số dưới cho biết đó là nơ-ron nào của lớp đó.

94
00:07:07,920 --> 00:07:15,280
Hãy sử dụng chữ k để lập chỉ mục cho lớp L-1 và j để lập chỉ mục cho lớp L.

95
00:07:15,280 --> 00:07:20,720
Về chi phí, một lần nữa chúng ta xem xét đầu ra mong muốn là bao nhiêu, nhưng lần này chúng ta

96
00:07:20,720 --> 00:07:26,120
cộng bình phương của sự khác biệt giữa các lần kích hoạt lớp cuối cùng này và đầu ra mong muốn.

97
00:07:26,120 --> 00:07:33,280
Nghĩa là, bạn lấy tổng trên ALj trừ yj bình phương.

98
00:07:33,280 --> 00:07:36,500
Vì có nhiều trọng số hơn nên mỗi cái phải có thêm một vài

99
00:07:36,500 --> 00:07:41,380
chỉ số để theo dõi vị trí của nó, vì vậy hãy gọi trọng

100
00:07:41,380 --> 00:07:45,740
số của cạnh nối nơ-ron thứ k này với nơ-ron thứ j, WLjk.

101
00:07:45,740 --> 00:07:49,820
Ban đầu, các chỉ số đó có thể hơi ngược một chút, nhưng nó phù hợp với cách

102
00:07:49,820 --> 00:07:53,800
bạn lập chỉ mục cho ma trận trọng số mà tôi đã nói trong video phần 1.

103
00:07:53,800 --> 00:07:57,660
Cũng như trước đây, bạn vẫn nên đặt tên cho tổng có trọng số

104
00:07:57,660 --> 00:08:03,540
liên quan, chẳng hạn như z, để việc kích hoạt lớp cuối cùng

105
00:08:03,540 --> 00:08:04,980
chỉ là hàm đặc biệt của bạn, như sigmoid, áp dụng cho z.

106
00:08:04,980 --> 00:08:09,100
Bạn có thể hiểu ý tôi, trong đó tất cả những phương trình này về cơ bản đều giống các phương trình mà

107
00:08:09,100 --> 00:08:15,420
chúng ta đã có trước đây trong trường hợp một nơ-ron trên mỗi lớp, chỉ là nó trông phức tạp hơn một chút.

108
00:08:15,420 --> 00:08:20,620
Và thực sự, biểu thức đạo hàm quy tắc dây chuyền mô tả mức độ nhạy cảm

109
00:08:20,620 --> 00:08:23,540
của chi phí đối với một trọng số cụ thể về cơ bản là giống nhau.

110
00:08:23,540 --> 00:08:29,420
Tôi sẽ để bạn tạm dừng và suy nghĩ về từng điều khoản đó nếu bạn muốn.

111
00:08:29,420 --> 00:08:34,900
Tuy nhiên, điều thay đổi ở đây là đạo hàm của chi

112
00:08:34,900 --> 00:08:37,820
phí đối với một trong các kích hoạt trong lớp L-1.

113
00:08:37,820 --> 00:08:42,000
Trong trường hợp này, sự khác biệt là tế bào thần kinh ảnh

114
00:08:42,000 --> 00:08:43,540
hưởng đến hàm chi phí thông qua nhiều con đường khác nhau.

115
00:08:43,540 --> 00:08:51,200
Nghĩa là, một mặt, nó ảnh hưởng đến AL0, vốn đóng một vai trò

116
00:08:51,200 --> 00:08:56,460
trong hàm chi phí, nhưng nó cũng có ảnh hưởng đến AL1, cũng đóng

117
00:08:56,460 --> 00:09:00,340
một vai trò trong hàm chi phí, và bạn phải cộng chúng lại.

118
00:09:00,340 --> 00:09:03,680
Và đó, ồ, đại khái là như vậy.

119
00:09:03,680 --> 00:09:08,240
Khi bạn biết mức độ nhạy cảm của hàm chi phí đối với các kích

120
00:09:08,240 --> 00:09:12,520
hoạt trong lớp thứ hai đến lớp cuối cùng này, bạn chỉ cần lặp lại

121
00:09:12,520 --> 00:09:13,920
quy trình cho tất cả các trọng số và độ lệch đưa vào lớp đó.

122
00:09:13,920 --> 00:09:15,420
Vì vậy hãy vỗ nhẹ vào lưng mình!

123
00:09:15,420 --> 00:09:20,480
Nếu tất cả những điều này đều hợp lý thì giờ đây bạn đã tìm hiểu sâu về

124
00:09:20,480 --> 00:09:23,700
cốt lõi của lan truyền ngược, nền tảng đằng sau cách mạng lưới thần kinh học hỏi.

125
00:09:23,700 --> 00:09:27,960
Các biểu thức quy tắc chuỗi này cung cấp cho bạn các đạo hàm xác định từng thành

126
00:09:27,960 --> 00:09:35,020
phần trong gradient giúp giảm thiểu chi phí của mạng bằng cách liên tục giảm dần độ dốc.

127
00:09:35,020 --> 00:09:38,960
Nếu bạn ngồi lại và suy nghĩ về tất cả những điều đó, thì đây là rất nhiều lớp phức tạp bao

128
00:09:38,960 --> 00:09:42,840
trùm tâm trí bạn, vì vậy đừng lo lắng nếu tâm trí bạn cần thời gian để tiêu hóa tất cả.

