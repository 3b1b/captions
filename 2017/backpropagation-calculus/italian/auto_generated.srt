1
00:00:04,019 --> 00:00:06,925
Il difficile presupposto qui è che tu abbia guardato la parte 3, 

2
00:00:06,925 --> 00:00:09,920
che fornisce una guida intuitiva dell'algoritmo di backpropagation.

3
00:00:11,040 --> 00:00:14,220
Qui diventiamo un po’ più formali e ci tuffiamo nel calcolo rilevante.

4
00:00:14,820 --> 00:00:17,110
È normale che questo crei almeno un po' di confusione, 

5
00:00:17,110 --> 00:00:20,358
quindi il mantra di fermarsi e riflettere regolarmente si applica sicuramente 

6
00:00:20,358 --> 00:00:21,400
tanto qui quanto altrove.

7
00:00:21,940 --> 00:00:24,875
Il nostro obiettivo principale è mostrare come le persone che lavorano 

8
00:00:24,875 --> 00:00:27,769
nel machine learning comunemente pensano alla regola della catena del 

9
00:00:27,769 --> 00:00:30,663
calcolo nel contesto delle reti, che ha un aspetto diverso da come la 

10
00:00:30,663 --> 00:00:33,640
maggior parte dei corsi introduttivi sul calcolo affrontano l'argomento.

11
00:00:34,340 --> 00:00:37,392
Per quelli di voi che non si sentono a proprio agio con i calcoli rilevanti, 

12
00:00:37,392 --> 00:00:38,740
ho un'intera serie sull'argomento.

13
00:00:39,960 --> 00:00:42,990
Cominciamo con una rete estremamente semplice, 

14
00:00:42,990 --> 00:00:46,020
in cui ogni strato contiene un singolo neurone.

15
00:00:46,320 --> 00:00:50,831
Questa rete è determinata da tre pesi e tre distorsioni e il nostro obiettivo 

16
00:00:50,831 --> 00:00:54,880
è capire quanto sia sensibile la funzione di costo a queste variabili.

17
00:00:55,420 --> 00:00:58,031
In questo modo sappiamo quali aggiustamenti a tali termini 

18
00:00:58,031 --> 00:01:00,820
causeranno la riduzione più efficiente della funzione di costo.

19
00:01:01,960 --> 00:01:04,840
Ci concentreremo solo sulla connessione tra gli ultimi due neuroni.

20
00:01:05,980 --> 00:01:09,479
Etichettiamo l'attivazione dell'ultimo neurone con una L in apice, 

21
00:01:09,479 --> 00:01:11,360
che indica in quale strato si trova.

22
00:01:11,680 --> 00:01:15,560
Quindi l'attivazione del neurone precedente è AL-1.

23
00:01:16,360 --> 00:01:20,271
Questi non sono esponenti, sono solo un modo per indicizzare ciò di cui stiamo parlando, 

24
00:01:20,271 --> 00:01:23,040
poiché in seguito voglio salvare gli indici per diversi indici.

25
00:01:23,720 --> 00:01:27,890
Diciamo che il valore che vogliamo che quest'ultima attivazione abbia 

26
00:01:27,890 --> 00:01:32,180
per un dato esempio di training è y, ad esempio y potrebbe essere 0 o 1.

27
00:01:32,840 --> 00:01:39,240
Quindi il costo di questa rete per un singolo esempio di formazione è AL-y2.

28
00:01:40,260 --> 00:01:44,380
Indicheremo il costo di quell'esempio di formazione come c0.

29
00:01:45,900 --> 00:01:51,176
Come promemoria, quest'ultima attivazione è determinata da un peso, che chiamerò wL, 

30
00:01:51,176 --> 00:01:56,640
moltiplicato per l'attivazione del neurone precedente più qualche bias, che chiamerò bL.

31
00:01:57,420 --> 00:02:01,320
Quindi lo pompi attraverso una speciale funzione non lineare come il sigmoide o ReLU.

32
00:02:01,800 --> 00:02:05,636
In realtà ci renderà le cose più facili se diamo un nome speciale a questa 

33
00:02:05,636 --> 00:02:09,320
somma ponderata, come z, con lo stesso apice delle relative attivazioni.

34
00:02:10,380 --> 00:02:15,068
Si tratta di molti termini e un modo in cui potresti concettualizzarlo è che il peso, 

35
00:02:15,068 --> 00:02:19,429
l'azione precedente e il bias tutti insieme vengono utilizzati per calcolare z, 

36
00:02:19,429 --> 00:02:22,481
che a sua volta ci consente di calcolare a, che infine, 

37
00:02:22,481 --> 00:02:25,480
insieme a una costante y, consente calcoliamo il costo.

38
00:02:27,340 --> 00:02:32,018
E, naturalmente, AL-1 è influenzato dal suo peso, dai suoi pregiudizi e simili, 

39
00:02:32,018 --> 00:02:35,060
ma non ci concentreremo su questo in questo momento.

40
00:02:35,700 --> 00:02:37,620
Tutti questi sono solo numeri, giusto?

41
00:02:38,060 --> 00:02:41,040
E può essere bello pensare che ognuno di essi abbia la propria piccola linea numerica.

42
00:02:41,720 --> 00:02:45,360
Il nostro primo obiettivo è capire quanto sia sensibile la 

43
00:02:45,360 --> 00:02:49,000
funzione di costo a piccoli cambiamenti nel nostro peso wL.

44
00:02:49,540 --> 00:02:54,860
Oppure, in altre parole, qual è la derivata di c rispetto a wL?

45
00:02:55,600 --> 00:02:59,867
Quando vedi questo termine del w, pensalo come se significasse una piccola 

46
00:02:59,867 --> 00:03:03,963
spinta verso w, come un cambiamento di 0.01, e pensare a questo termine 

47
00:03:03,963 --> 00:03:08,060
del c con il significato di qualunque sia la spinta risultante al costo.

48
00:03:08,060 --> 00:03:10,220
Ciò che vogliamo è il loro rapporto.

49
00:03:11,260 --> 00:03:16,337
Concettualmente, questo piccolo spostamento verso wL provoca uno spostamento verso zL, 

50
00:03:16,337 --> 00:03:21,240
che a sua volta causa uno spostamento verso AL, che influenza direttamente il costo.

51
00:03:23,120 --> 00:03:27,811
Quindi suddividiamo il tutto esaminando prima il rapporto tra una piccola 

52
00:03:27,811 --> 00:03:33,200
variazione di zL e questa piccola variazione w, cioè la derivata di zL rispetto a wL.

53
00:03:33,200 --> 00:03:36,982
Allo stesso modo, si considera quindi il rapporto tra la variazione 

54
00:03:36,982 --> 00:03:39,986
in AL e la piccola variazione in zL che l'ha causata, 

55
00:03:39,986 --> 00:03:44,660
nonché il rapporto tra la spinta finale verso c e questa spinta intermedia verso AL.

56
00:03:45,740 --> 00:03:50,295
Questa qui è la regola della catena, dove moltiplicando questi 

57
00:03:50,295 --> 00:03:55,140
tre rapporti ci dà la sensibilità di c a piccoli cambiamenti in wL.

58
00:03:56,880 --> 00:04:00,158
Quindi sullo schermo in questo momento ci sono molti simboli, 

59
00:04:00,158 --> 00:04:03,860
e prenditi un momento per assicurarti che sia chiaro cosa sono tutti, 

60
00:04:03,860 --> 00:04:06,240
perché ora calcoleremo le derivate rilevanti.

61
00:04:07,440 --> 00:04:13,160
La derivata di c rispetto ad AL risulta essere 2AL-y.

62
00:04:13,980 --> 00:04:18,469
Ciò significa che la sua dimensione è proporzionale alla differenza tra l'output della 

63
00:04:18,469 --> 00:04:22,443
rete e ciò che vogliamo che sia, quindi se quell'output fosse molto diverso, 

64
00:04:22,443 --> 00:04:26,778
anche cambiamenti minimi potrebbero avere un grande impatto sulla funzione di costo 

65
00:04:26,778 --> 00:04:27,140
finale.

66
00:04:27,840 --> 00:04:32,157
La derivata di AL rispetto a zL è semplicemente la derivata della nostra 

67
00:04:32,157 --> 00:04:36,180
funzione sigmoide, o qualunque nonlinearità tu scelga di utilizzare.

68
00:04:37,220 --> 00:04:44,660
La derivata di zL rispetto a wL risulta essere AL-1.

69
00:04:45,760 --> 00:04:49,440
Non so voi, ma penso che sia facile rimanere bloccati nelle formule senza 

70
00:04:49,440 --> 00:04:53,420
prendersi un momento per sedersi e ricordare a se stessi cosa significano tutte.

71
00:04:53,920 --> 00:04:58,341
Nel caso di quest'ultima derivata, la misura in cui la piccola spinta al peso 

72
00:04:58,341 --> 00:05:02,820
ha influenzato l'ultimo strato dipende da quanto è forte il neurone precedente.

73
00:05:03,380 --> 00:05:08,280
Ricorda, è qui che entra in gioco l'idea dei neuroni che si attivano insieme.

74
00:05:09,200 --> 00:05:12,460
E tutto ciò è la derivata rispetto a wL solo del 

75
00:05:12,460 --> 00:05:15,720
costo per un singolo esempio formativo specifico.

76
00:05:16,440 --> 00:05:20,040
Poiché la funzione di costo completo comporta la media di tutti i 

77
00:05:20,040 --> 00:05:23,913
costi tra molti esempi di formazione diversi, la sua derivata richiede 

78
00:05:23,913 --> 00:05:27,460
la media di questa espressione su tutti gli esempi di formazione.

79
00:05:28,380 --> 00:05:32,287
Naturalmente, questa è solo una componente del vettore del gradiente, 

80
00:05:32,287 --> 00:05:37,199
che è costruito dalle derivate parziali della funzione di costo rispetto a tutti questi 

81
00:05:37,199 --> 00:05:38,260
pesi e distorsioni.

82
00:05:40,640 --> 00:05:43,816
Ma anche se è solo una delle tante derivate parziali di cui abbiamo bisogno, 

83
00:05:43,816 --> 00:05:45,260
rappresenta più del 50% del lavoro.

84
00:05:46,340 --> 00:05:49,720
La sensibilità al bias, ad esempio, è quasi identica.

85
00:05:50,040 --> 00:05:55,020
Dobbiamo solo cambiare questo termine del z del w con a del z del b.

86
00:05:58,420 --> 00:06:02,400
E se guardi la formula rilevante, la derivata risulta essere 1.

87
00:06:06,140 --> 00:06:10,364
Inoltre, ed è qui che entra in gioco l’idea della propagazione all’indietro, 

88
00:06:10,364 --> 00:06:15,136
puoi vedere quanto questa funzione di costo sia sensibile all’attivazione dello strato 

89
00:06:15,136 --> 00:06:15,740
precedente.

90
00:06:15,740 --> 00:06:20,954
Vale a dire, questa derivata iniziale nell'espressione della regola della catena, 

91
00:06:20,954 --> 00:06:25,660
la sensibilità di z all'attivazione precedente, risulta essere il peso wL.

92
00:06:26,640 --> 00:06:30,471
E ancora, anche se non saremo in grado di influenzare direttamente l'attivazione 

93
00:06:30,471 --> 00:06:32,789
del livello precedente, è utile tenerne traccia, 

94
00:06:32,789 --> 00:06:36,810
perché ora possiamo semplicemente continuare a ripetere questa stessa idea di regola 

95
00:06:36,810 --> 00:06:40,784
della catena all'indietro per vedere quanto è sensibile la funzione di costo a pesi 

96
00:06:40,784 --> 00:06:42,440
precedenti e pregiudizi precedenti.

97
00:06:43,180 --> 00:06:46,099
E potresti pensare che questo sia un esempio eccessivamente semplice, 

98
00:06:46,099 --> 00:06:48,893
dato che tutti gli strati hanno un neurone, e le cose diventeranno 

99
00:06:48,893 --> 00:06:51,020
esponenzialmente più complicate per una rete reale.

100
00:06:51,700 --> 00:06:55,493
Ma onestamente, non cambia molto quando diamo agli strati più neuroni, 

101
00:06:55,493 --> 00:06:58,860
in realtà sono solo alcuni indici in più di cui tenere traccia.

102
00:06:59,380 --> 00:07:03,441
Piuttosto che l'attivazione di un dato strato essere semplicemente AL, 

103
00:07:03,441 --> 00:07:07,160
avrà anche un pedice che indica quale neurone di quello strato è.

104
00:07:07,160 --> 00:07:14,420
Usiamo la lettera k per indicizzare il livello L-1 e j per indicizzare il livello L.

105
00:07:15,260 --> 00:07:19,019
Per il costo, ancora una volta guardiamo quale sia l'output desiderato, 

106
00:07:19,019 --> 00:07:22,360
ma questa volta sommiamo i quadrati delle differenze tra queste 

107
00:07:22,360 --> 00:07:25,180
attivazioni dell'ultimo livello e l'output desiderato.

108
00:07:26,080 --> 00:07:30,840
Cioè, prendi una somma su ALj meno yj al quadrato.

109
00:07:33,040 --> 00:07:37,038
Dato che ci sono molti più pesi, ognuno deve avere un paio di indici 

110
00:07:37,038 --> 00:07:41,037
in più per tenere traccia di dove si trova, quindi chiamiamo WLjk il 

111
00:07:41,037 --> 00:07:44,920
peso del bordo che collega questo neurone kesimo al neurone jesimo.

112
00:07:45,620 --> 00:07:48,304
All'inizio questi indici potrebbero sembrare un po' arretrati, 

113
00:07:48,304 --> 00:07:52,097
ma sono in linea con il modo in cui indicizzeresti la matrice dei pesi di cui ho parlato 

114
00:07:52,097 --> 00:07:53,120
nel video della parte 1.

115
00:07:53,620 --> 00:07:57,929
Proprio come prima, è comunque carino dare un nome alla somma ponderata rilevante, 

116
00:07:57,929 --> 00:08:02,498
come z, in modo che l'attivazione dell'ultimo strato sia solo la tua funzione speciale, 

117
00:08:02,498 --> 00:08:04,160
come il sigmoide, applicata a z.

118
00:08:04,660 --> 00:08:09,195
Potete capire cosa intendo, dove tutte queste sono essenzialmente le stesse equazioni che 

119
00:08:09,195 --> 00:08:13,680
avevamo prima nel caso di un neurone per strato, è solo che sembra un po' più complicato.

120
00:08:15,440 --> 00:08:19,278
E in effetti, l’espressione derivata della regola della catena che descrive 

121
00:08:19,278 --> 00:08:23,420
quanto il costo sia sensibile a un peso specifico sembra essenzialmente la stessa.

122
00:08:23,920 --> 00:08:26,840
Lascerò a te la possibilità di fermarti e pensare a ciascuno di questi termini, se vuoi.

123
00:08:28,980 --> 00:08:32,781
Ciò che cambia qui, però, è la derivata del costo 

124
00:08:32,781 --> 00:08:36,659
rispetto ad una delle attivazioni nello strato L-1.

125
00:08:37,780 --> 00:08:40,264
In questo caso, la differenza è che il neurone influenza 

126
00:08:40,264 --> 00:08:42,880
la funzione di costo attraverso molteplici percorsi diversi.

127
00:08:44,680 --> 00:08:50,264
Cioè, da un lato influenza AL0, che gioca un ruolo nella funzione di costo, 

128
00:08:50,264 --> 00:08:56,364
ma ha anche un'influenza su AL1, che gioca anche un ruolo nella funzione di costo, 

129
00:08:56,364 --> 00:08:57,540
e devi sommarli.

130
00:08:59,820 --> 00:09:03,040
E questo, beh, è più o meno tutto.

131
00:09:03,500 --> 00:09:06,390
Una volta che sai quanto è sensibile la funzione di costo alle 

132
00:09:06,390 --> 00:09:09,510
attivazioni in questo penultimo strato, puoi semplicemente ripetere 

133
00:09:09,510 --> 00:09:12,860
il processo per tutti i pesi e i pregiudizi che alimentano quello strato.

134
00:09:13,900 --> 00:09:14,960
Quindi datti una pacca sulle spalle!

135
00:09:15,300 --> 00:09:19,312
Se tutto ciò ha senso, ora hai esaminato in profondità il cuore della backpropagation, 

136
00:09:19,312 --> 00:09:22,680
il cavallo di battaglia dietro il modo in cui le reti neurali apprendono.

137
00:09:23,300 --> 00:09:26,735
Queste espressioni delle regole della catena forniscono i derivati 

138
00:09:26,735 --> 00:09:29,864
che determinano ciascun componente nel gradiente che aiuta a 

139
00:09:29,864 --> 00:09:33,300
minimizzare il costo della rete scendendo ripetutamente in discesa.

140
00:09:34,300 --> 00:09:38,520
Se ti siedi e pensi a tutto ciò, ci sono molti strati di complessità su cui avvolgere la 

141
00:09:38,520 --> 00:09:42,740
tua mente, quindi non preoccuparti se ci vuole tempo perché la tua mente digerisca tutto.

