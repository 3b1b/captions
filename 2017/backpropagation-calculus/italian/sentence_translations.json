[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "Il difficile presupposto qui è che tu abbia guardato la parte 3, che fornisce una guida intuitiva dell'algoritmo di backpropagation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Qui diventiamo un po’ più formali e ci tuffiamo nel calcolo rilevante.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "È normale che questo crei almeno un po' di confusione, quindi il mantra di fermarsi e riflettere regolarmente si applica sicuramente tanto qui quanto altrove.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "Il nostro obiettivo principale è mostrare come le persone che lavorano nel machine learning comunemente pensano alla regola della catena del calcolo nel contesto delle reti, che ha un aspetto diverso da come la maggior parte dei corsi introduttivi sul calcolo affrontano l'argomento.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Per quelli di voi che non si sentono a proprio agio con i calcoli rilevanti, ho un'intera serie sull'argomento.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Cominciamo con una rete estremamente semplice, in cui ogni strato contiene un singolo neurone.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Questa rete è determinata da tre pesi e tre distorsioni e il nostro obiettivo è capire quanto sia sensibile la funzione di costo a queste variabili.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "In questo modo sappiamo quali aggiustamenti a tali termini causeranno la riduzione più efficiente della funzione di costo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "We'll just focus on the connection between the last two neurons.",
  "translatedText": "Ci concentreremo solo sulla connessione tra gli ultimi due neuroni.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in.",
  "translatedText": "Etichettiamo l'attivazione dell'ultimo neurone con una L in apice, che indica in quale strato si trova.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.98,
  "end": 71.36
 },
 {
  "input": "So the activation of the previous neuron is AL-1.",
  "translatedText": "Quindi l'attivazione del neurone precedente è AL-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.68,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Questi non sono esponenti, sono solo un modo per indicizzare ciò di cui stiamo parlando, poiché in seguito voglio salvare gli indici per diversi indici.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Diciamo che il valore che vogliamo che quest'ultima attivazione abbia per un dato esempio di training è y, ad esempio y potrebbe essere 0 o 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is AL-y2.",
  "translatedText": "Quindi il costo di questa rete per un singolo esempio di formazione è AL-y2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Indicheremo il costo di quell'esempio di formazione come c0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call wL, times the previous neuron's activation plus some bias, which I'll call bL.",
  "translatedText": "Come promemoria, quest'ultima attivazione è determinata da un peso, che chiamerò wL, moltiplicato per l'attivazione del neurone precedente più qualche bias, che chiamerò bL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "Then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "Quindi lo pompi attraverso una speciale funzione non lineare come il sigmoide o ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "In realtà ci renderà le cose più facili se diamo un nome speciale a questa somma ponderata, come z, con lo stesso apice delle relative attivazioni.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action, and bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "Si tratta di molti termini e un modo in cui potresti concettualizzarlo è che il peso, l'azione precedente e il bias tutti insieme vengono utilizzati per calcolare z, che a sua volta ci consente di calcolare a, che infine, insieme a una costante y, consente calcoliamo il costo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course, AL-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "E, naturalmente, AL-1 è influenzato dal suo peso, dai suoi pregiudizi e simili, ma non ci concentreremo su questo in questo momento.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Tutti questi sono solo numeri, giusto?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "E può essere bello pensare che ognuno di essi abbia la propria piccola linea numerica.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight wL.",
  "translatedText": "Il nostro primo obiettivo è capire quanto sia sensibile la funzione di costo a piccoli cambiamenti nel nostro peso wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to wL?",
  "translatedText": "Oppure, in altre parole, qual è la derivata di c rispetto a wL?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del w term, think of it as meaning some tiny nudge to w, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Quando vedi questo termine del w, pensalo come se significasse una piccola spinta verso w, come un cambiamento di 0.01, e pensare a questo termine del c con il significato di qualunque sia la spinta risultante al costo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "Ciò che vogliamo è il loro rapporto.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to wL causes some nudge to zL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Concettualmente, questo piccolo spostamento verso wL provoca uno spostamento verso zL, che a sua volta causa uno spostamento verso AL, che influenza direttamente il costo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to zL to this tiny change w, that is, the derivative of zL with respect to wL.",
  "translatedText": "Quindi suddividiamo il tutto esaminando prima il rapporto tra una piccola variazione di zL e questa piccola variazione w, cioè la derivata di zL rispetto a wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in zL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "Allo stesso modo, si considera quindi il rapporto tra la variazione in AL e la piccola variazione in zL che l'ha causata, nonché il rapporto tra la spinta finale verso c e questa spinta intermedia verso AL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying these three ratios gives us the sensitivity of c to small changes in wL.",
  "translatedText": "Questa qui è la regola della catena, dove moltiplicando questi tre rapporti ci dà la sensibilità di c a piccoli cambiamenti in wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "Quindi sullo schermo in questo momento ci sono molti simboli, e prenditi un momento per assicurarti che sia chiaro cosa sono tutti, perché ora calcoleremo le derivate rilevanti.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "La derivata di c rispetto ad AL risulta essere 2AL-y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "This means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Ciò significa che la sua dimensione è proporzionale alla differenza tra l'output della rete e ciò che vogliamo che sia, quindi se quell'output fosse molto diverso, anche cambiamenti minimi potrebbero avere un grande impatto sulla funzione di costo finale.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to zL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "La derivata di AL rispetto a zL è semplicemente la derivata della nostra funzione sigmoide, o qualunque nonlinearità tu scelga di utilizzare.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "The derivative of zL with respect to wL comes out to be AL-1.",
  "translatedText": "La derivata di zL rispetto a wL risulta essere AL-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself what they all mean.",
  "translatedText": "Non so voi, ma penso che sia facile rimanere bloccati nelle formule senza prendersi un momento per sedersi e ricordare a se stessi cosa significano tutte.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "Nel caso di quest'ultima derivata, la misura in cui la piccola spinta al peso ha influenzato l'ultimo strato dipende da quanto è forte il neurone precedente.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "Ricorda, è qui che entra in gioco l'idea dei neuroni che si attivano insieme.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to wL only of the cost for a specific single training example.",
  "translatedText": "E tutto ciò è la derivata rispetto a wL solo del costo per un singolo esempio formativo specifico.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Poiché la funzione di costo completo comporta la media di tutti i costi tra molti esempi di formazione diversi, la sua derivata richiede la media di questa espressione su tutti gli esempi di formazione.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "Of course, that's just one component of the gradient vector, which is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "Naturalmente, questa è solo una componente del vettore del gradiente, che è costruito dalle derivate parziali della funzione di costo rispetto a tutti questi pesi e distorsioni.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "Ma anche se è solo una delle tante derivate parziali di cui abbiamo bisogno, rappresenta più del 50% del lavoro.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "La sensibilità al bias, ad esempio, è quasi identica.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Dobbiamo solo cambiare questo termine del z del w con a del z del b.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "E se guardi la formula rilevante, la derivata risulta essere 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "Inoltre, ed è qui che entra in gioco l’idea della propagazione all’indietro, puoi vedere quanto questa funzione di costo sia sensibile all’attivazione dello strato precedente.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight wL.",
  "translatedText": "Vale a dire, questa derivata iniziale nell'espressione della regola della catena, la sensibilità di z all'attivazione precedente, risulta essere il peso wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "E ancora, anche se non saremo in grado di influenzare direttamente l'attivazione del livello precedente, è utile tenerne traccia, perché ora possiamo semplicemente continuare a ripetere questa stessa idea di regola della catena all'indietro per vedere quanto è sensibile la funzione di costo a pesi precedenti e pregiudizi precedenti.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "E potresti pensare che questo sia un esempio eccessivamente semplice, dato che tutti gli strati hanno un neurone, e le cose diventeranno esponenzialmente più complicate per una rete reale.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "Ma onestamente, non cambia molto quando diamo agli strati più neuroni, in realtà sono solo alcuni indici in più di cui tenere traccia.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "Piuttosto che l'attivazione di un dato strato essere semplicemente AL, avrà anche un pedice che indica quale neurone di quello strato è.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Usiamo la lettera k per indicizzare il livello L-1 e j per indicizzare il livello L.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "Per il costo, ancora una volta guardiamo quale sia l'output desiderato, ma questa volta sommiamo i quadrati delle differenze tra queste attivazioni dell'ultimo livello e l'output desiderato.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus yj squared.",
  "translatedText": "Cioè, prendi una somma su ALj meno yj al quadrato.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Dato che ci sono molti più pesi, ognuno deve avere un paio di indici in più per tenere traccia di dove si trova, quindi chiamiamo WLjk il peso del bordo che collega questo neurone kesimo al neurone jesimo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "All'inizio questi indici potrebbero sembrare un po' arretrati, ma sono in linea con il modo in cui indicizzeresti la matrice dei pesi di cui ho parlato nel video della parte 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "Proprio come prima, è comunque carino dare un nome alla somma ponderata rilevante, come z, in modo che l'attivazione dell'ultimo strato sia solo la tua funzione speciale, come il sigmoide, applicata a z.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Potete capire cosa intendo, dove tutte queste sono essenzialmente le stesse equazioni che avevamo prima nel caso di un neurone per strato, è solo che sembra un po' più complicato.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain rule derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "E in effetti, l’espressione derivata della regola della catena che descrive quanto il costo sia sensibile a un peso specifico sembra essenzialmente la stessa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Lascerò a te la possibilità di fermarti e pensare a ciascuno di questi termini, se vuoi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "Ciò che cambia qui, però, è la derivata del costo rispetto ad una delle attivazioni nello strato L-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "In questo caso, la differenza è che il neurone influenza la funzione di costo attraverso molteplici percorsi diversi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "Cioè, da un lato influenza AL0, che gioca un ruolo nella funzione di costo, ma ha anche un'influenza su AL1, che gioca anche un ruolo nella funzione di costo, e devi sommarli.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "E questo, beh, è più o meno tutto.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Una volta che sai quanto è sensibile la funzione di costo alle attivazioni in questo penultimo strato, puoi semplicemente ripetere il processo per tutti i pesi e i pregiudizi che alimentano quello strato.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "Quindi datti una pacca sulle spalle!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Se tutto ciò ha senso, ora hai esaminato in profondità il cuore della backpropagation, il cavallo di battaglia dietro il modo in cui le reti neurali apprendono.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Queste espressioni delle regole della catena forniscono i derivati che determinano ciascun componente nel gradiente che aiuta a minimizzare il costo della rete scendendo ripetutamente in discesa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Se ti siedi e pensi a tutto ciò, ci sono molti strati di complessità su cui avvolgere la tua mente, quindi non preoccuparti se ci vuole tempo perché la tua mente digerisca tutto.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]