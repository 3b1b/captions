1
00:00:04,230 --> 00:00:07,120
여러분이 파트 3를 보셨다는 가정을 하겠습니다.

2
00:00:07,120 --> 00:00:10,230
역전파 알고리즘을 직관적인 방법으로 설명한 영상이었죠.

3
00:00:11,040 --> 00:00:14,770
이제 조금 더 격식을 차려서, 관련된 미적분에 대해 살펴보려 합니다.

4
00:00:14,770 --> 00:00:17,040
이것이 약간 혼란스러울수 있습니다.

5
00:00:17,040 --> 00:00:21,480
그러니 중간중간 멈추고 숙고하라는 진리는 다른 곳에서와 마찬가지로 이 영상에도 적용되겠죠.

6
00:00:21,920 --> 00:00:25,180
이 영상의 주 목표는 머신 러닝에서 일하는 사람들이

7
00:00:25,180 --> 00:00:29,440
네트워크의 관점에서 연쇄 법칙(chain rule)을 생각하는지 보여주는 것입니다.

8
00:00:29,440 --> 00:00:33,820
대부분의 미적분학 기초에서 접근하는 방법과는 꽤 다른 느낌이 들겁니다.

9
00:00:33,820 --> 00:00:34,500
 

10
00:00:34,500 --> 00:00:36,890
관련된 미적분이 불편하신 분들은

11
00:00:36,890 --> 00:00:39,040
제가 만든 미적분에 관한 영상 시리즈를 보시면 됩니다.

12
00:00:40,340 --> 00:00:43,150
아주 단순한 네트워크를 가지고 시작합시다.

13
00:00:43,150 --> 00:00:45,730
한 층에 하나의 뉴런만 있는 네트워크죠.

14
00:00:46,270 --> 00:00:50,680
이 네트워크는 3개의 가중치(weight)와 3개의 편향(bias)만으로 결정됩니다.

15
00:00:50,680 --> 00:00:55,070
우리의 목표는 비용 함수(cost function)가 이런 변수에 대해서 얼마나 민감하게 변하는지 이해하는 것입니다.

16
00:00:55,550 --> 00:00:57,830
그렇게 한다면 이런 것들(가중치, 편향)을 어떻게 바꾸는 것이

17
00:00:57,830 --> 00:01:00,940
비용 함수를 가장 효율적으로 낮추는지 알게 될 것입니다.

18
00:01:01,920 --> 00:01:05,170
그리고 우리는 마지막 2개의 뉴런 사이의 연결에만 집중할 것입니다.

19
00:01:05,880 --> 00:01:11,370
마지막 층의 활성화 정도를 a에다 위첨자로 어느 층인지를 나타내는 L을 붙여 표현합시다.

20
00:01:11,690 --> 00:01:15,720
그렇다면 그 전 뉴런의 활성화 정도는 a^(L-1)이 되겠죠.

21
00:01:16,430 --> 00:01:20,030
지수가 아니라, 어느 층을 나타내는지 번호를 붙여주는 방법입니다.

22
00:01:20,030 --> 00:01:22,970
아랫첨자는 나중에 다른 번호를 붙여줄 생각이기 때문에 그렇게 표시한 것입니다.

23
00:01:23,740 --> 00:01:29,710
주어진 학습 예제에 대해 이 마지막 활성화 정도가 y가 되기를 원한다고 해보죠.

24
00:01:30,170 --> 00:01:32,360
예를 들어, y는 0이거나 1일 수 있습니다.

25
00:01:32,940 --> 00:01:39,470
그렇다면 하나의 학습 예제에 대한 이 단순한 네트워크의 비용은 (a^(L) - y)^2입니다.

26
00:01:40,250 --> 00:01:44,650
이 하나의 학습 예제에 대한 비용을 C_0라 표기하겠습니다.

27
00:01:46,030 --> 00:01:51,520
다시 말씀드리자면, 이 마지막 활성화 정도는 앞으로 w^(L)이라 부를 가중치(weight)와

28
00:01:51,980 --> 00:01:54,220
이전 뉴런의 활성화 정도를 곱한것,

29
00:01:54,530 --> 00:01:56,940
거기에 앞으로 b^(L)이라 부를 편향(bias)를 더한 것에 의해 결정됩니다.

30
00:01:57,480 --> 00:02:01,500
이제 이걸 시그모이드(sigmoid)나 ReLU같은 특별한 비선형 함수에 집어넣는거죠.

31
00:02:01,850 --> 00:02:06,980
만약 이 가중합(weighted sum)에 z같은 특별한 이름을 붙여준다면 편리할겁니다.

32
00:02:06,980 --> 00:02:09,550
거기에 관련된 활성화 정도와 똑같은 윗첨자를 붙여주는거죠.

33
00:02:10,390 --> 00:02:11,480
꽤 많은 표기법들이 나왔습니다.

34
00:02:11,480 --> 00:02:16,960
아마 이렇게 이해하셨을겁니다. 가중치(weight), 이전 활성화 정도, 그리고 편향(bias)이

35
00:02:16,960 --> 00:02:21,400
모두 사용돼 z를 계산하고, 이를 이용해 a를 계산하며,

36
00:02:21,740 --> 00:02:25,610
최종적으로, 상수 y와 함께 쓰여, 비용을 계산한다는 것이죠.

37
00:02:27,260 --> 00:02:31,660
그리고 물론  a^(L-1)은 자기 자신의 가중치(weight)와 편향(bias)에 영향을 받고, 하는 식이죠.

38
00:02:32,810 --> 00:02:34,840
하지만 지금 당장 여기에 초점을 두진 않을겁니다.

39
00:02:35,680 --> 00:02:38,040
이것들 전부 그냥 숫자들 맞죠?

40
00:02:38,040 --> 00:02:41,230
각각이 모두 작은 수직선을 가지고 있다고 생각하면 좋을 것입니다.

41
00:02:41,900 --> 00:02:45,820
우리의 첫 목적은 가중치 w^(L)의 작은 변화에

42
00:02:45,840 --> 00:02:48,940
비용 함수가 얼마나 민감하게 반응하는지 이해하는 것입니다.

43
00:02:49,640 --> 00:02:54,880
다르게 말하자면, C의 w^(L)에 대한 미분값이 무엇인가죠.

44
00:02:55,630 --> 00:02:58,070
여러분이 “∂w”을 보면

45
00:02:58,070 --> 00:03:02,750
그것이 0.01같이  "w에 가해진 아주 약간의 변화"라는 뜻이라고 생각하세요.

46
00:03:03,150 --> 00:03:08,210
그리고이 "∂C"라는 용어는 "그 결과로 비용이 바뀌는 정도"라고 생각하시고요.

47
00:03:08,710 --> 00:03:10,420
그 비율을 알고 싶은 겁니다.

48
00:03:11,210 --> 00:03:16,520
개념적으로 보면, w^(L)의 약간의 변화는  z^(L)가 조금 변하게 하겠죠.

49
00:03:16,520 --> 00:03:21,380
그렇다면 a^(L)에 변화가 조금 생길꺼고,  그 변화는 비용에 직접적인 영향을 미칠 것입니다.

50
00:03:23,100 --> 00:03:28,930
그러면 이걸 쪼개기 위해 먼저 z^(L)의 작은 변화와 w^(L)의 작은 변화의 비율을 살펴볼 수 있겠죠.

51
00:03:29,290 --> 00:03:33,030
그건 z^(L)의 w^(L)에 대한 미분값이 됩니다.

52
00:03:33,760 --> 00:03:39,410
마찬가지로, 그 다음엔 z^(L)의 작은 변화와 그로 인한 a^(L)의 변화의 비율을 살펴보죠.

53
00:03:39,850 --> 00:03:44,880
거기에 a^(L)에 생긴 중간 변화와 최종적으로 C에 생긴 변화의 비율 또한 살펴봅니다.

54
00:03:45,670 --> 00:03:47,850
이것이 바로 연쇄 법칙(chain rule)입니다.

55
00:03:47,850 --> 00:03:54,950
이 세 비율을 곱하는 것으로 C의 w^(L)의 작은 변화에 대한 민감도를 알 수 있는 것이죠.

56
00:03:57,190 --> 00:04:00,040
이 화면을 보면, 꽤 많은 기호들이 많이 있습니다.

57
00:04:00,040 --> 00:04:03,000
잠시 시간을 가지고 어느게 어느 것인지 정리해보세요.

58
00:04:03,600 --> 00:04:06,560
이제 이걸 가지고 연관된 미분을 계산할 것입니다.

59
00:04:07,400 --> 00:04:13,230
C의 a^(L)에 대한 도함수는 2(a^(L) - y)이 됩니다.

60
00:04:13,960 --> 00:04:19,040
참고로, 이 크기가 네트워크의 출력과 비례한다는 뜻입니다.

61
00:04:19,040 --> 00:04:20,880
바라던대로죠.

62
00:04:21,360 --> 00:04:23,340
그래서 출력이 크게 다르다면,

63
00:04:23,340 --> 00:04:27,150
아주 약간의 변화도 비용 함수에 큰 영향을 미치겠죠.

64
00:04:28,300 --> 00:04:33,880
a^(L)를 z^(L)에 대해 미분한 것은 그냥 시그모이드 함수의 도함수입니다.

65
00:04:33,880 --> 00:04:36,370
아님 쓰기로 한 다른 비선형 함수거나요.

66
00:04:37,310 --> 00:04:40,370
그리고 Z^(L)을 w^(L)에 대해 미분한 것은

67
00:04:41,470 --> 00:04:44,530
이 경우는 그냥 a^(L-1)이 되네요.

68
00:04:46,060 --> 00:04:51,220
모르긴 모르지만, 이것들이 실제로는 다 무엇일지 잠깐 시간을 내어 생각해보지  않았다면

69
00:04:51,220 --> 00:04:53,680
여기에서 막히기 딱 좋습니다.

70
00:04:54,120 --> 00:04:56,040
마지막 미분을 보면,

71
00:04:56,040 --> 00:05:00,060
이 가중치에 생긴 작은 변화가 마지막 층에 미치는 영향의 양은

72
00:05:00,060 --> 00:05:02,850
그 전 뉴런이 얼마나 강한지에 달려 있습니다.

73
00:05:03,310 --> 00:05:07,520
기억해보면, "함께 발화하는 뉴런이 함께 연결된다"라는 아이디어가 여기에 적용된 것입니다.

74
00:05:09,210 --> 00:05:15,940
이 모든 것은 w^(L)에 대해 특정한 학습 예제의 비용만을 미분한 것입니다.

75
00:05:16,410 --> 00:05:22,150
전체 비용 함수는 수많은 예제의 비용들을 전부 평균낸 것과 관련이 있기 때문에,

76
00:05:22,150 --> 00:05:27,610
그 미분을 구하려면 지금껏 찾은 식을 모든 학습 예제에 적용한 것을 평균내야 합니다.

77
00:05:28,430 --> 00:05:31,930
물론 그건 그라디언트 벡터를 이루는 단 하나의 구성 요소일 뿐이고,

78
00:05:31,930 --> 00:05:33,890
그 그라디언트 벡터는 비용 함수의

79
00:05:33,890 --> 00:05:38,480
그 모든 가중치와 편향에 대한 편미분으로 이루어져 있습니다.

80
00:05:40,710 --> 00:05:43,550
이게 비록 우리가 필요로 하는 편미분들 중 단 하나뿐이었지만,

81
00:05:43,550 --> 00:05:45,390
우리가 할 일의 절반 이상은 한 셈입니다.

82
00:05:46,420 --> 00:05:49,940
편향(bias)에 대한 민감도는, 예컨데, 거의 동일합니다.

83
00:05:50,250 --> 00:05:55,120
우리가 해야할 것은  ∂z/∂w 부분을 ∂z/∂b로 바꾸는 것 뿐이고,

84
00:05:58,760 --> 00:06:02,590
관련된 공식을 보면, 그 도함수는 1이 됩니다.

85
00:06:06,210 --> 00:06:09,880
그리고, '역으로 전파한다'는 개념이 여기에 적용되어,

86
00:06:10,230 --> 00:06:15,670
비용 함수가 이전 층의 활성화 정도에 얼마나 민감한지 알 수 있습니다.

87
00:06:16,250 --> 00:06:19,650
이름하여, 연쇄 법칙을 이용한 확장의 첫번째 도함수는,

88
00:06:19,650 --> 00:06:23,100
이전 활성화 정도에 대한 z의 민감도인데,

89
00:06:23,480 --> 00:06:25,670
가중치 w^(L)이 됩니다.

90
00:06:26,580 --> 00:06:31,500
또다시, 그 직전의 활성화 정도에 직접적으로 영항을 줄 수는 없더라도,

91
00:06:31,500 --> 00:06:33,080
추적하는데 도움이 됩니다.

92
00:06:33,080 --> 00:06:38,200
왜냐면 이젠 이 연쇄 법칙이라는 발상을 거꾸로 반복해 나가며

93
00:06:38,200 --> 00:06:42,750
비용 함수가 그 이전 가중치와 이전 편향에 얼마나 민감한지 알 수 있기 때문입니다.

94
00:06:43,630 --> 00:06:45,980
이게 지나치게 단순화된 예제라고 생각하실 수도 있습니다.

95
00:06:45,980 --> 00:06:47,880
왜냐면 모든 레이어가 단 하나의 뉴런만을 가지고 있기 때문이죠.

96
00:06:47,880 --> 00:06:51,220
그리고 실제 네트워크에서는 지수적으로 복잡해질 거라고 말입니다.

97
00:06:51,680 --> 00:06:56,270
하지만 솔직히, 각 층에 여러 뉴런이 있어도 크게 바뀌는건 아닙니다.

98
00:06:56,270 --> 00:06:58,710
그냥 번호만 몇개 더 추적하는겁니다.

99
00:06:59,340 --> 00:07:02,880
주어진 층의 활성화 정도를 단순하게 a^(L)이라고 표기하는 대신,

100
00:07:02,880 --> 00:07:07,210
그 층의 어느 뉴런인지를 표기하는 아랫첨자를 붙이는 겁니다.

101
00:07:07,780 --> 00:07:14,470
(L-1)번 레이어에 k를 이용해 번호를 매기고, (L)번 레이어는 j를 이용해 번호를 매겨봅시다.

102
00:07:15,290 --> 00:07:18,910
비용을 알아보기 위해, 다시 한번 원하는 출력이 무엇인지 확인합시다.

103
00:07:18,910 --> 00:07:19,380
그러나 이번엔

104
00:07:19,380 --> 00:07:25,260
마지막 레이어의 활성화 정도와 원하는 출력 사이의 차이의 제곱을 더할 겁니다.

105
00:07:26,060 --> 00:07:31,070
즉,  (a_j^(L) - y_j)^2의 합을 구한다는 것입니다.

106
00:07:33,110 --> 00:07:34,520
가중치가 훨씬 많으므로,

107
00:07:34,520 --> 00:07:37,650
각각이 어느 것인지 추적하기 위해 번호가 몇 개 더 필요합니다.

108
00:07:38,010 --> 00:07:44,990
그러니 k번째 뉴런과 j번째 뉴런을 연결하는 간선(edge)의 가중치를 w_{jk}^(L) 라고 표기합시다.

109
00:07:45,660 --> 00:07:48,260
이 번호는 처음 볼 땐 거꾸로 쓰인 것 같지만,

110
00:07:48,260 --> 00:07:52,940
Part 1 비디오에서 어떻게 가중치 행렬에 번호를 매길지 말했던 것과 연관이 있습니다.

111
00:07:53,680 --> 00:07:58,350
그 전처럼, 관련된 가중합에 z같은 이름을 주는게 좋고,

112
00:07:58,350 --> 00:08:04,310
그러면 마지막 층의 활성화 정도는 시그모이드같은 특별한 함수에 z를 적용한게 됩니다.

113
00:08:05,040 --> 00:08:06,230
제가 무슨 말을 하고 있는지 아시겠죠?

114
00:08:06,230 --> 00:08:11,680
이건 한 레이어당 하나의 뉴런이 있던 경우와 본질적으로 같은 공식입니다.

115
00:08:11,680 --> 00:08:13,870
약간 더 복잡해보일 뿐이죠.

116
00:08:15,370 --> 00:08:18,220
그리고 확실히, 비용이 얼마나 특정한 가중치에

117
00:08:18,220 --> 00:08:21,980
민감한지를 보여주는 연쇄법칙 도함수 표현은

118
00:08:21,980 --> 00:08:23,890
본질적으로 똑같아 보입니다.

119
00:08:23,890 --> 00:08:26,880
각 표현이 무슨 뜻인지 잠시 영상을 멈추고 생각해봐도 좋습니다.

120
00:08:29,310 --> 00:08:31,320
다만, 여기에서 바뀐 것은,

121
00:08:31,320 --> 00:08:36,830
비용의 (L-1)번 층 중 하나의 활성화 정도에 대한 도함수입니다.

122
00:08:37,760 --> 00:08:43,120
이 경우, 뉴런이 비용 함수에 여러 경로를 통해 영향을 준다는 차이점이 생깁니다.

123
00:08:44,660 --> 00:08:50,540
이 말은, 한편으론, 비용 함수에 한 역할을 하는 a_0^(L)에 영향을 미치기도 하지만.

124
00:08:51,010 --> 00:08:56,320
마찬가지로 비용 함수에 한 역할을 하는 a_1^(L)에도 영향을 준다는 뜻입니다.

125
00:08:56,320 --> 00:08:57,410
그리고 이걸 다 더하면 됩니다.

126
00:09:00,170 --> 00:09:02,980
그리고... 뭐 이게 다입니다.

127
00:09:03,560 --> 00:09:08,520
비용 함수가 이 두번째와 마지막 층 사이의 활성화 정도에 얼마나 민감한지를 알게 된다면,

128
00:09:08,840 --> 00:09:12,940
그 층에 들어가는 모든 가중치와 편향에 이 과정을 반복해주면 됩니다.

129
00:09:13,850 --> 00:09:15,360
자, 이제 스스로를 칭찬해도 됩니다.

130
00:09:15,360 --> 00:09:16,950
이걸 전부 이해했다면,

131
00:09:16,950 --> 00:09:20,440
역전파의 핵심을 깊이 파고들어본 것입니다.

132
00:09:20,440 --> 00:09:22,830
그리고 역전파는 인공 신경망 학습의 심장이죠.

133
00:09:23,590 --> 00:09:29,300
이런 연쇄 법칙 표현로 그라디언트의 각 구성 요소를 결정하는 도함수를 구할 수 있고,

134
00:09:29,300 --> 00:09:33,550
이는 언덕 아래로 반복적으로 걸어 내려가며 네트워크의 비용을 최소화하는데 도움을 줍니다.

135
00:09:34,280 --> 00:09:36,850
흐으으음. 편안히 앉아서 이걸 전부 생각해본다면,

136
00:09:36,850 --> 00:09:40,090
정리해야할 복잡성의 층이 꽤나 많습니다.

137
00:09:40,090 --> 00:09:43,090
이걸 다 이해하는데 시간이 걸린다고 걱정하진 마세요.

