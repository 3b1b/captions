1
00:00:04,019 --> 00:00:06,751
여기서 어려운 가정은 역전파 알고리즘에 대한 

2
00:00:06,751 --> 00:00:09,920
직관적인 안내를 제공하는 3부를 시청했다는 것입니다.

3
00:00:11,040 --> 00:00:14,220
여기서는 좀 더 공식적으로 관련 계산을 살펴봅니다.

4
00:00:14,820 --> 00:00:17,287
약간 혼란스러울 수 있는 것은 당연한 일이므로, 

5
00:00:17,287 --> 00:00:19,572
정기적으로 잠시 멈추고 숙고하라는 말은 다른 

6
00:00:19,572 --> 00:00:21,400
곳과 마찬가지로 여기에도 적용됩니다.

7
00:00:21,940 --> 00:00:24,840
이 강좌의 주요 목표는 대부분의 미적분 입문 강좌가 

8
00:00:24,840 --> 00:00:26,840
이 주제에 접근하는 방식과는 다른, 

9
00:00:26,840 --> 00:00:29,840
네트워크의 맥락에서 미적분학의 연쇄 법칙에 대해 머신 

10
00:00:29,840 --> 00:00:32,640
러닝 분야의 사람들이 일반적으로 어떻게 생각하는지 

11
00:00:32,640 --> 00:00:33,640
보여주는 것입니다.

12
00:00:34,340 --> 00:00:36,399
관련 미적분학이 어려운 분들을 위해 이 

13
00:00:36,399 --> 00:00:38,740
주제에 대한 전체 시리즈가 준비되어 있습니다.

14
00:00:39,960 --> 00:00:43,141
각 레이어에 뉴런이 하나씩 있는 매우 

15
00:00:43,141 --> 00:00:46,020
간단한 네트워크부터 시작하겠습니다.

16
00:00:46,320 --> 00:00:48,910
이 네트워크는 세 가지 가중치와 세 가지 

17
00:00:48,910 --> 00:00:51,838
편향으로 결정되며, 우리의 목표는 비용 함수가 

18
00:00:51,838 --> 00:00:54,880
이러한 변수에 얼마나 민감한지 이해하는 것입니다.

19
00:00:55,420 --> 00:00:58,069
이렇게 하면 어떤 조건을 조정하면 비용 함수를 

20
00:00:58,069 --> 00:01:00,820
가장 효율적으로 줄일 수 있는지 알 수 있습니다.

21
00:01:01,960 --> 00:01:04,840
그리고 마지막 두 뉴런 사이의 연결에 집중하겠습니다.

22
00:01:05,980 --> 00:01:09,021
마지막 뉴런의 활성화에 위첨자 L을 

23
00:01:09,021 --> 00:01:12,214
붙여서 어느 층에 있는지를 나타내므로 

24
00:01:12,214 --> 00:01:15,560
이전 뉴런의 활성화는 Al-1이 됩니다.

25
00:01:16,360 --> 00:01:18,683
이것은 지수가 아니라 나중에 다른 인덱스에 

26
00:01:18,683 --> 00:01:20,910
대한 구독을 저장하고 싶기 때문에 우리가 

27
00:01:20,910 --> 00:01:23,040
말하는 것을 인덱싱하는 방법일 뿐입니다.

28
00:01:23,720 --> 00:01:26,495
예를 들어, 주어진 훈련 예제에 대해 

29
00:01:26,495 --> 00:01:29,404
이 마지막 활성화의 값이 0 또는 1이 

30
00:01:29,404 --> 00:01:32,180
될 수 있는 y라고 가정해 보겠습니다.

31
00:01:32,840 --> 00:01:36,039
따라서 단일 훈련 예제에 대한 이 

32
00:01:36,039 --> 00:01:39,240
네트워크의 비용은 Al-y2입니다.

33
00:01:40,260 --> 00:01:44,380
이 트레이닝 예제의 비용을 c0으로 표시하겠습니다.

34
00:01:45,900 --> 00:01:49,673
이 마지막 활성화는 WL이라고 부르는 가중치와 

35
00:01:49,673 --> 00:01:53,156
이전 뉴런의 활성화에 약간의 편향성을 더한 

36
00:01:53,156 --> 00:01:56,640
값에 의해 결정됩니다(BL이라고 부릅니다).

37
00:01:57,420 --> 00:01:59,411
그런 다음 시그모이드 또는 ReLU와 같은 

38
00:01:59,411 --> 00:02:01,320
특수 비선형 함수를 통해 이를 펌핑합니다.

39
00:02:01,800 --> 00:02:04,163
이 가중치 합계에 관련 활성화와 동일한 

40
00:02:04,163 --> 00:02:06,741
위첨자를 사용하여 Z와 같은 특별한 이름을 

41
00:02:06,741 --> 00:02:09,320
지정하면 실제로 작업이 더 쉬워질 것입니다.

42
00:02:10,380 --> 00:02:13,780
많은 용어가 있지만, 이를 개념화할 수 있는 

43
00:02:13,780 --> 00:02:17,725
방법은 가중치, 이전 행동, 편향성을 모두 사용하여 

44
00:02:17,725 --> 00:02:21,262
z를 계산하고, 이를 통해 a를 계산하여 상수 

45
00:02:21,262 --> 00:02:24,799
y와 함께 최종적으로 비용을 계산할 수 있다는 

46
00:02:24,799 --> 00:02:25,480
것입니다.

47
00:02:27,340 --> 00:02:30,861
물론 Al-1은 그 자체의 무게와 편향성 등에 

48
00:02:30,861 --> 00:02:34,247
영향을 받지만, 지금은 여기에 초점을 맞추지 

49
00:02:34,247 --> 00:02:35,060
않겠습니다.

50
00:02:35,700 --> 00:02:37,620
이 모든 것은 숫자에 불과하죠?

51
00:02:38,060 --> 00:02:39,812
그리고 각각에 고유한 작은 번호선이 

52
00:02:39,812 --> 00:02:41,040
있다고 생각하면 좋습니다.

53
00:02:41,720 --> 00:02:45,214
첫 번째 목표는 비용 함수가 가중치 WL의 

54
00:02:45,214 --> 00:02:49,000
작은 변화에 얼마나 민감한지 이해하는 것입니다.

55
00:02:49,540 --> 00:02:52,124
또는 다르게 표현하면, WL과 

56
00:02:52,124 --> 00:02:54,860
관련하여 c의 미분은 무엇인가요?

57
00:02:55,600 --> 00:02:58,444
이 델 W 용어를 볼 때는 0.01의 

58
00:02:58,444 --> 00:03:01,152
변화와 같이 W에 대한 작은 넛지를 

59
00:03:01,152 --> 00:03:04,403
의미한다고 생각하고, 델 c 용어는 비용에 

60
00:03:04,403 --> 00:03:08,060
대한 결과적인 넛지를 의미한다고 생각하면 됩니다.

61
00:03:08,060 --> 00:03:10,220
우리가 원하는 것은 그들의 비율입니다.

62
00:03:11,260 --> 00:03:14,463
개념적으로 WL에 대한 이 작은 넛지는 ZL에 

63
00:03:14,463 --> 00:03:17,913
약간의 넛지를 유발하고, 이는 다시 AL에 약간의 

64
00:03:17,913 --> 00:03:21,240
넛지를 유발하여 비용에 직접적인 영향을 미칩니다.

65
00:03:23,120 --> 00:03:26,385
따라서 먼저 ZL에 대한 작은 변화와 이 

66
00:03:26,385 --> 00:03:29,508
작은 변화 W의 비율, 즉 WL에 대한 

67
00:03:29,508 --> 00:03:33,200
ZL의 미분을 살펴봄으로써 상황을 세분화합니다.

68
00:03:33,200 --> 00:03:36,916
마찬가지로, AL에 대한 변화와 그 원인이 

69
00:03:36,916 --> 00:03:39,549
된 ZL의 작은 변화의 비율, 

70
00:03:39,549 --> 00:03:43,111
그리고 최종 넛지와 이 중간 넛지 사이의 

71
00:03:43,111 --> 00:03:44,660
비율을 고려합니다.

72
00:03:45,740 --> 00:03:48,487
바로 여기에 연쇄 법칙이 있는데, 

73
00:03:48,487 --> 00:03:51,669
이 세 가지 비율을 곱하면 WL의 작은 

74
00:03:51,669 --> 00:03:55,140
변화에 대한 c의 민감도를 알 수 있습니다.

75
00:03:56,880 --> 00:04:00,000
지금 화면에는 많은 기호가 표시되어 있는데, 

76
00:04:00,000 --> 00:04:02,995
이제 관련 파생상품을 계산할 것이므로 잠시 

77
00:04:02,995 --> 00:04:06,240
시간을 내어 기호가 무엇인지 명확히 알아두세요.

78
00:04:07,440 --> 00:04:13,160
AL에 대한 c의 도함수는 2AL-y로 계산됩니다.

79
00:04:13,980 --> 00:04:17,269
이는 네트워크의 출력과 우리가 원하는 것 

80
00:04:17,269 --> 00:04:20,273
사이의 차이에 비례한다는 의미이므로, 

81
00:04:20,273 --> 00:04:23,420
그 출력이 매우 다르다면 약간의 변화도 

82
00:04:23,420 --> 00:04:27,140
최종 비용 함수에 큰 영향을 미칠 수 있습니다.

83
00:04:27,840 --> 00:04:32,010
ZL에 대한 AL의 도함수는 시그모이드 함수 또는 

84
00:04:32,010 --> 00:04:36,180
사용자가 선택한 비선형성에 대한 도함수일 뿐입니다.

85
00:04:37,220 --> 00:04:42,020
그리고 WL에 대한 ZL의 도함수는 

86
00:04:42,020 --> 00:04:44,660
AL-1로 나옵니다.

87
00:04:45,760 --> 00:04:48,133
여러분은 어떤지 모르겠지만, 저는 잠시 

88
00:04:48,133 --> 00:04:50,614
시간을 내서 그 공식의 의미를 되새겨보지 

89
00:04:50,614 --> 00:04:53,420
않고는 그 공식에 갇혀버리기 쉽다고 생각합니다.

90
00:04:53,920 --> 00:04:56,886
이 마지막 파생물의 경우, 가중치에 대한 

91
00:04:56,886 --> 00:04:59,724
작은 넛지가 마지막 층에 영향을 미치는 

92
00:04:59,724 --> 00:05:02,820
정도는 이전 뉴런의 강도에 따라 달라집니다.

93
00:05:03,380 --> 00:05:05,773
뉴런이 함께 불을 붙인다는 아이디어가 

94
00:05:05,773 --> 00:05:08,280
바로 여기에서 나온다는 것을 기억하세요.

95
00:05:09,200 --> 00:05:12,460
그리고 이 모든 것은 특정 단일 교육 예제에 

96
00:05:12,460 --> 00:05:15,720
대한 비용만 WL과 관련하여 파생된 것입니다.

97
00:05:16,440 --> 00:05:20,017
전체 비용 함수는 다양한 훈련 예제에서 모든 

98
00:05:20,017 --> 00:05:22,594
비용의 평균을 구하는 것이므로, 

99
00:05:22,594 --> 00:05:26,315
그 미분은 모든 훈련 예제에서 이 식의 평균을 

100
00:05:26,315 --> 00:05:27,460
구해야 합니다.

101
00:05:28,380 --> 00:05:31,476
물론 이는 모든 가중치와 편향에 대한 

102
00:05:31,476 --> 00:05:34,720
비용 함수의 부분 미분으로부터 구축되는 

103
00:05:34,720 --> 00:05:38,260
그라디언트 벡터의 한 구성 요소일 뿐입니다.

104
00:05:40,640 --> 00:05:42,180
하지만 이는 우리가 필요로 하는 많은 

105
00:05:42,180 --> 00:05:43,939
부분적인 파생 상품 중 하나에 불과하지만, 

106
00:05:43,939 --> 00:05:45,260
작업의 50% 이상을 차지합니다.

107
00:05:46,340 --> 00:05:49,720
예를 들어 편향에 대한 민감도는 거의 동일합니다.

108
00:05:50,040 --> 00:05:52,316
이 델 z 델 w 용어를 델 

109
00:05:52,316 --> 00:05:55,020
z 델 b로 바꾸기만 하면 됩니다.

110
00:05:58,420 --> 00:06:02,400
관련 공식을 살펴보면 그 미분은 1로 나옵니다.

111
00:06:06,140 --> 00:06:09,980
또한, 역전파라는 아이디어가 여기서 나오는데, 

112
00:06:09,980 --> 00:06:13,081
이 비용 함수가 이전 계층의 활성화에 

113
00:06:13,081 --> 00:06:15,740
얼마나 민감한지 알 수 있습니다.

114
00:06:15,740 --> 00:06:19,943
즉, 체인 규칙 표현식에서 이 초기 도함수, 

115
00:06:19,943 --> 00:06:24,819
즉 이전 활성화에 대한 z의 민감도가 가중치 WL로 

116
00:06:24,819 --> 00:06:25,660
나옵니다.

117
00:06:26,640 --> 00:06:29,824
다시 말하지만, 이전 레이어 활성화에 직접적인 

118
00:06:29,824 --> 00:06:32,764
영향을 줄 수는 없지만, 이제 동일한 체인 

119
00:06:32,764 --> 00:06:35,826
규칙 아이디어를 거꾸로 반복하여 비용 함수가 

120
00:06:35,826 --> 00:06:38,765
이전 가중치와 이전 편향에 얼마나 민감한지 

121
00:06:38,765 --> 00:06:42,440
확인할 수 있으므로 계속 추적하는 것이 도움이 됩니다.

122
00:06:43,180 --> 00:06:45,491
모든 레이어에는 하나의 뉴런이 있고 실제 

123
00:06:45,491 --> 00:06:48,105
네트워크에서는 상황이 기하급수적으로 복잡해지기 

124
00:06:48,105 --> 00:06:51,020
때문에 지나치게 단순한 예라고 생각할 수도 있습니다.

125
00:06:51,700 --> 00:06:53,925
하지만 솔직히 레이어에 여러 개의 뉴런을 

126
00:06:53,925 --> 00:06:55,860
부여해도 그다지 큰 변화는 없으며, 

127
00:06:55,860 --> 00:06:58,279
추적해야 할 지표가 몇 개 더 늘어나는 것에 

128
00:06:58,279 --> 00:06:58,860
불과합니다.

129
00:06:59,380 --> 00:07:02,058
특정 레이어의 활성화가 단순히 AL이 

130
00:07:02,058 --> 00:07:04,736
아니라 해당 레이어의 어떤 뉴런인지를 

131
00:07:04,736 --> 00:07:07,160
나타내는 아래 첨자가 붙게 됩니다.

132
00:07:07,160 --> 00:07:10,790
문자 k를 사용하여 레이어 L-1의 색인을 생성하고 

133
00:07:10,790 --> 00:07:14,420
j를 사용하여 레이어 L의 색인을 생성해 보겠습니다.

134
00:07:15,260 --> 00:07:18,434
비용의 경우, 다시 원하는 출력이 무엇인지 

135
00:07:18,434 --> 00:07:21,476
살펴보지만 이번에는 이러한 마지막 레이어 

136
00:07:21,476 --> 00:07:25,180
활성화와 원하는 출력 간의 차이의 제곱을 더합니다.

137
00:07:26,080 --> 00:07:30,840
즉, ALj에서 Yj 제곱을 뺀 합계를 취합니다.

138
00:07:33,040 --> 00:07:36,128
가중치가 훨씬 더 많으므로 각 가중치는 위치를 

139
00:07:36,128 --> 00:07:38,980
추적하기 위해 몇 개의 인덱스를 더 가져야 

140
00:07:38,980 --> 00:07:42,187
하므로 이 k번째 뉴런과 j번째 뉴런을 연결하는 

141
00:07:42,187 --> 00:07:44,920
에지의 가중치를 WLjk라고 부르겠습니다.

142
00:07:45,620 --> 00:07:47,870
이러한 인덱스는 처음에는 약간 거꾸로 

143
00:07:47,870 --> 00:07:50,334
느껴질 수 있지만 1부 동영상에서 설명한 

144
00:07:50,334 --> 00:07:53,120
가중치 매트릭스를 인덱싱하는 방법과 일치합니다.

145
00:07:53,620 --> 00:07:56,168
이전과 마찬가지로 z와 같이 관련 가중 

146
00:07:56,168 --> 00:07:58,716
합계에 이름을 지정하여 마지막 레이어의 

147
00:07:58,716 --> 00:08:01,264
활성화가 z에 적용된 시그모이드와 같은 

148
00:08:01,264 --> 00:08:04,160
특수 함수만 활성화되도록 하는 것이 좋습니다.

149
00:08:04,660 --> 00:08:07,416
레이어당 하나의 뉴런을 사용하는 경우와 

150
00:08:07,416 --> 00:08:10,422
본질적으로 동일한 방정식을 사용하지만 조금 

151
00:08:10,422 --> 00:08:13,680
더 복잡해 보일 뿐이라는 것을 알 수 있습니다.

152
00:08:15,440 --> 00:08:19,640
실제로 비용이 특정 무게에 얼마나 민감한지를 설명하는 

153
00:08:19,640 --> 00:08:23,420
체인 규칙 파생식도 본질적으로 동일하게 보입니다.

154
00:08:23,920 --> 00:08:25,505
원하신다면 잠시 멈춰서 각 용어에 

155
00:08:25,505 --> 00:08:26,840
대해 생각해 보시기 바랍니다.

156
00:08:28,980 --> 00:08:32,980
하지만 여기서 변경되는 것은 레이어 L-1의 

157
00:08:32,980 --> 00:08:36,659
활성화 중 하나에 대한 비용의 미분입니다.

158
00:08:37,780 --> 00:08:40,219
이 경우 뉴런이 여러 다른 경로를 통해 

159
00:08:40,219 --> 00:08:42,880
비용 함수에 영향을 미친다는 점이 다릅니다.

160
00:08:44,680 --> 00:08:48,823
즉, 한편으로는 비용 함수에서 역할을 하는 AL0에 

161
00:08:48,823 --> 00:08:52,681
영향을 미치지만, 다른 한편으로는 비용 함수에서 

162
00:08:52,681 --> 00:08:56,968
역할을 하는 AL1에도 영향을 미치므로 이를 합산해야 

163
00:08:56,968 --> 00:08:57,540
합니다.

164
00:08:59,820 --> 00:09:03,040
그리고 그게 거의 전부입니다.

165
00:09:03,500 --> 00:09:06,655
이 두 번째에서 마지막 레이어의 활성화에 비용 함수가 

166
00:09:06,655 --> 00:09:09,704
얼마나 민감한지 알게 되면, 해당 레이어에 공급되는 

167
00:09:09,704 --> 00:09:12,860
모든 가중치와 편향에 대해 이 과정을 반복하면 됩니다.

168
00:09:13,900 --> 00:09:14,960
그러니 스스로를 칭찬해 주세요!

169
00:09:15,300 --> 00:09:18,641
이 모든 것이 이해가 되셨다면 이제 신경망 

170
00:09:18,641 --> 00:09:22,680
학습의 핵심인 역전파의 핵심을 깊이 들여다보셨습니다.

171
00:09:23,300 --> 00:09:26,752
이러한 체인 규칙 표현식은 경사도의 각 구성 요소를 

172
00:09:26,752 --> 00:09:30,204
결정하는 파생물을 제공하여 반복적으로 내리막을 밟아 

173
00:09:30,204 --> 00:09:33,300
네트워크의 비용을 최소화하는 데 도움이 됩니다.

174
00:09:34,300 --> 00:09:37,113
가만히 앉아 이 모든 것을 생각해보면 복잡하게 

175
00:09:37,113 --> 00:09:39,926
얽혀 있는 내용들이 많기 때문에 이 모든 것을 

176
00:09:39,926 --> 00:09:42,740
소화하는 데 시간이 걸리더라도 걱정하지 마세요.

