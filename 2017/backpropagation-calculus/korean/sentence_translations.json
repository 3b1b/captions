[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "",
  "from_community_srt": "여러분이 파트 3를 보셨다는 가정을 하겠습니다. 역전파 알고리즘을 직관적인 방법으로 설명한 영상이었죠.",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "",
  "from_community_srt": "이제 조금 더 격식을 차려서, 관련된 미적분에 대해 살펴보려 합니다.",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "",
  "from_community_srt": "이것이 약간 혼란스러울수 있습니다. 그러니 중간중간 멈추고 숙고하라는 진리는 다른 곳에서와 마찬가지로 이 영상에도 적용되겠죠.",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "",
  "from_community_srt": "이 영상의 주 목표는 머신 러닝에서 일하는 사람들이 네트워크의 관점에서 연쇄 법칙(chain rule)을 생각하는지 보여주는 것입니다. 대부분의 미적분학 기초에서 접근하는 방법과는 꽤 다른 느낌이 들겁니다.",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "",
  "from_community_srt": "관련된 미적분이 불편하신 분들은 제가 만든 미적분에 관한 영상 시리즈를 보시면 됩니다.",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "",
  "from_community_srt": "아주 단순한 네트워크를 가지고 시작합시다. 한 층에 하나의 뉴런만 있는 네트워크죠.",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "",
  "from_community_srt": "이 네트워크는 3개의 가중치(weight)와 3개의 편향(bias)만으로 결정됩니다. 우리의 목표는 비용 함수(cost function)가 이런 변수에 대해서 얼마나 민감하게 변하는지 이해하는 것입니다.",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way, we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "",
  "from_community_srt": "그렇게 한다면 이런 것들(가중치, 편향)을 어떻게 바꾸는 것이 비용 함수를 가장 효율적으로 낮추는지 알게 될 것입니다.",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "And we're just going to focus on the connection between the last two neurons.",
  "translatedText": "",
  "from_community_srt": "그리고 우리는 마지막 2개의 뉴런 사이의 연결에만 집중할 것입니다.",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in, so the activation of the previous neuron is Al-1.",
  "translatedText": "",
  "from_community_srt": "마지막 층의 활성화 정도를 a에다 위첨자로 어느 층인지를 나타내는 L을 붙여 표현합시다. 그렇다면 그 전 뉴런의 활성화 정도는 a^(L-1)이 되겠죠.",
  "n_reviews": 0,
  "start": 65.98,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "",
  "from_community_srt": "지수가 아니라, 어느 층을 나타내는지 번호를 붙여주는 방법입니다. 아랫첨자는 나중에 다른 번호를 붙여줄 생각이기 때문에 그렇게 표시한 것입니다.",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "",
  "from_community_srt": "주어진 학습 예제에 대해 이 마지막 활성화 정도가 y가 되기를 원한다고 해보죠. 예를 들어, y는 0이거나 1일 수 있습니다.",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is Al-y2.",
  "translatedText": "",
  "from_community_srt": "그렇다면 하나의 학습 예제에 대한 이 단순한 네트워크의 비용은 (a^(L) - y)^2입니다.",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "",
  "from_community_srt": "이 하나의 학습 예제에 대한 비용을 C_0라 표기하겠습니다.",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neuron's activation plus some bias, which I'll call BL.",
  "translatedText": "",
  "from_community_srt": "다시 말씀드리자면, 이 마지막 활성화 정도는 앞으로 w^(L)이라 부를 가중치(weight)와 이전 뉴런의 활성화 정도를 곱한것, 거기에 앞으로 b^(L)이라 부를 편향(bias)를 더한 것에 의해 결정됩니다.",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "",
  "from_community_srt": "이제 이걸 시그모이드(sigmoid)나 ReLU같은 특별한 비선형 함수에 집어넣는거죠.",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "",
  "from_community_srt": "만약 이 가중합(weighted sum)에 z같은 특별한 이름을 붙여준다면 편리할겁니다. 거기에 관련된 활성화 정도와 똑같은 윗첨자를 붙여주는거죠.",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action and the bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "",
  "from_community_srt": "꽤 많은 표기법들이 나왔습니다. 아마 이렇게 이해하셨을겁니다. 가중치(weight), 이전 활성화 정도, 그리고 편향(bias)이 모두 사용돼 z를 계산하고, 이를 이용해 a를 계산하며, 최종적으로, 상수 y와 함께 쓰여, 비용을 계산한다는 것이죠.",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course Al-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "",
  "from_community_srt": "그리고 물론  a^(L-1)은 자기 자신의 가중치(weight)와 편향(bias)에 영향을 받고, 하는 식이죠. 하지만 지금 당장 여기에 초점을 두진 않을겁니다.",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "",
  "from_community_srt": "이것들 전부 그냥 숫자들 맞죠? 각각이 모두 작은 수직선을 가지고 있다고 생각하면 좋을 것입니다.",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight WL.",
  "translatedText": "",
  "from_community_srt": "우리의 첫 목적은 가중치 w^(L)의 작은 변화에 비용 함수가 얼마나 민감하게 반응하는지 이해하는 것입니다.",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to WL?",
  "translatedText": "",
  "from_community_srt": "다르게 말하자면, C의 w^(L)에 대한 미분값이 무엇인가죠.",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del W term, think of it as meaning some tiny nudge to W, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "",
  "from_community_srt": "여러분이 “∂w”을 보면 그것이 0.01같이  \"w에 가해진 아주 약간의 변화\"라는 뜻이라고 생각하세요. 그리고이 \"∂C\"라는 용어는 \"그 결과로 비용이 바뀌는 정도\"라고 생각하시고요.",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "",
  "from_community_srt": "그 비율을 알고 싶은 겁니다.",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "",
  "from_community_srt": "개념적으로 보면, w^(L)의 약간의 변화는  z^(L)가 조금 변하게 하겠죠. 그렇다면 a^(L)에 변화가 조금 생길꺼고, 그 변화는 비용에 직접적인 영향을 미칠 것입니다.",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is, the derivative of ZL with respect to WL.",
  "translatedText": "",
  "from_community_srt": "그러면 이걸 쪼개기 위해 먼저 z^(L)의 작은 변화와 w^(L)의 작은 변화의 비율을 살펴볼 수 있겠죠. 그건 z^(L)의 w^(L)에 대한 미분값이 됩니다.",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "",
  "from_community_srt": "마찬가지로, 그 다음엔 z^(L)의 작은 변화와 그로 인한 a^(L)의 변화의 비율을 살펴보죠. 거기에 a^(L)에 생긴 중간 변화와 최종적으로 C에 생긴 변화의 비율 또한 살펴봅니다.",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of c to small changes in WL.",
  "translatedText": "",
  "from_community_srt": "이것이 바로 연쇄 법칙(chain rule)입니다. 이 세 비율을 곱하는 것으로 C의 w^(L)의 작은 변화에 대한 민감도를 알 수 있는 것이죠.",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "",
  "from_community_srt": "이 화면을 보면, 꽤 많은 기호들이 많이 있습니다. 잠시 시간을 가지고 어느게 어느 것인지 정리해보세요. 이제 이걸 가지고 연관된 미분을 계산할 것입니다.",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "",
  "from_community_srt": "C의 a^(L)에 대한 도함수는 2(a^(L) - y)이 됩니다.",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "Notice this means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "",
  "from_community_srt": "참고로, 이 크기가 네트워크의 출력과 비례한다는 뜻입니다. 바라던대로죠. 그래서 출력이 크게 다르다면, 아주 약간의 변화도 비용 함수에 큰 영향을 미치겠죠.",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "",
  "from_community_srt": "a^(L)를 z^(L)에 대해 미분한 것은 그냥 시그모이드 함수의 도함수입니다. 아님 쓰기로 한 다른 비선형 함수거나요.",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "And the derivative of ZL with respect to WL comes out to be AL-1.",
  "translatedText": "",
  "from_community_srt": "그리고 Z^(L)을 w^(L)에 대해 미분한 것은 이 경우는 그냥 a^(L-1)이 되네요.",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all mean.",
  "translatedText": "",
  "from_community_srt": "모르긴 모르지만, 이것들이 실제로는 다 무엇일지 잠깐 시간을 내어 생각해보지  않았다면 여기에서 막히기 딱 좋습니다.",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "",
  "from_community_srt": "마지막 미분을 보면, 이 가중치에 생긴 작은 변화가 마지막 층에 미치는 영향의 양은 그 전 뉴런이 얼마나 강한지에 달려 있습니다.",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "",
  "from_community_srt": "기억해보면, \"함께 발화하는 뉴런이 함께 연결된다\"라는 아이디어가 여기에 적용된 것입니다.",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to WL only of the cost for a specific single training example.",
  "translatedText": "",
  "from_community_srt": "이 모든 것은 w^(L)에 대해 특정한 학습 예제의 비용만을 미분한 것입니다.",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "",
  "from_community_srt": "전체 비용 함수는 수많은 예제의 비용들을 전부 평균낸 것과 관련이 있기 때문에, 그 미분을 구하려면 지금껏 찾은 식을 모든 학습 예제에 적용한 것을 평균내야 합니다.",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "And of course, that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "",
  "from_community_srt": "물론 그건 그라디언트 벡터를 이루는 단 하나의 구성 요소일 뿐이고, 그 그라디언트 벡터는 비용 함수의 그 모든 가중치와 편향에 대한 편미분으로 이루어져 있습니다.",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "",
  "from_community_srt": "이게 비록 우리가 필요로 하는 편미분들 중 단 하나뿐이었지만, 우리가 할 일의 절반 이상은 한 셈입니다.",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "",
  "from_community_srt": "편향(bias)에 대한 민감도는, 예컨데, 거의 동일합니다.",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "",
  "from_community_srt": "우리가 해야할 것은  ∂z/∂w 부분을 ∂z/∂b로 바꾸는 것 뿐이고, 관련된 공식을 보면, 그 도함수는 1이 됩니다.",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "",
  "from_community_srt": "그리고, '역으로 전파한다'는 개념이 여기에 적용되어, 비용 함수가 이전 층의 활성화 정도에 얼마나 민감한지 알 수 있습니다.",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight WL.",
  "translatedText": "",
  "from_community_srt": "이름하여, 연쇄 법칙을 이용한 확장의 첫번째 도함수는, 이전 활성화 정도에 대한 z의 민감도인데, 가중치 w^(L)이 됩니다.",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "",
  "from_community_srt": "또다시, 그 직전의 활성화 정도에 직접적으로 영항을 줄 수는 없더라도, 추적하는데 도움이 됩니다. 왜냐면 이젠 이 연쇄 법칙이라는 발상을 거꾸로 반복해 나가며 비용 함수가 그 이전 가중치와 이전 편향에 얼마나 민감한지 알 수 있기 때문입니다.",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "",
  "from_community_srt": "이게 지나치게 단순화된 예제라고 생각하실 수도 있습니다. 왜냐면 모든 레이어가 단 하나의 뉴런만을 가지고 있기 때문이죠. 그리고 실제 네트워크에서는 지수적으로 복잡해질 거라고 말입니다.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "",
  "from_community_srt": "하지만 솔직히, 각 층에 여러 뉴런이 있어도 크게 바뀌는건 아닙니다. 그냥 번호만 몇개 더 추적하는겁니다.",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "",
  "from_community_srt": "주어진 층의 활성화 정도를 단순하게 a^(L)이라고 표기하는 대신, 그 층의 어느 뉴런인지를 표기하는 아랫첨자를 붙이는 겁니다.",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "",
  "from_community_srt": "(L-1)번 레이어에 k를 이용해 번호를 매기고, (L)번 레이어는 j를 이용해 번호를 매겨봅시다.",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "",
  "from_community_srt": "비용을 알아보기 위해, 다시 한번 원하는 출력이 무엇인지 확인합시다. 그러나 이번엔 마지막 레이어의 활성화 정도와 원하는 출력 사이의 차이의 제곱을 더할 겁니다.",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus Yj squared.",
  "translatedText": "",
  "from_community_srt": "즉, (a_j^(L) - y_j)^2의 합을 구한다는 것입니다.",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "",
  "from_community_srt": "가중치가 훨씬 많으므로, 각각이 어느 것인지 추적하기 위해 번호가 몇 개 더 필요합니다. 그러니 k번째 뉴런과 j번째 뉴런을 연결하는 간선(edge)의 가중치를 w_{jk}^(L) 라고 표기합시다.",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "",
  "from_community_srt": "이 번호는 처음 볼 땐 거꾸로 쓰인 것 같지만, Part 1 비디오에서 어떻게 가중치 행렬에 번호를 매길지 말했던 것과 연관이 있습니다.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "",
  "from_community_srt": "그 전처럼, 관련된 가중합에 z같은 이름을 주는게 좋고, 그러면 마지막 층의 활성화 정도는 시그모이드같은 특별한 함수에 z를 적용한게 됩니다.",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "",
  "from_community_srt": "제가 무슨 말을 하고 있는지 아시겠죠? 이건 한 레이어당 하나의 뉴런이 있던 경우와 본질적으로 같은 공식입니다. 약간 더 복잡해보일 뿐이죠.",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain-ruled derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "",
  "from_community_srt": "그리고 확실히, 비용이 얼마나 특정한 가중치에 민감한지를 보여주는 연쇄법칙 도함수 표현은 본질적으로 똑같아 보입니다.",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "",
  "from_community_srt": "각 표현이 무슨 뜻인지 잠시 영상을 멈추고 생각해봐도 좋습니다.",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "",
  "from_community_srt": "다만, 여기에서 바뀐 것은, 비용의 (L-1)번 층 중 하나의 활성화 정도에 대한 도함수입니다.",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "",
  "from_community_srt": "이 경우, 뉴런이 비용 함수에 여러 경로를 통해 영향을 준다는 차이점이 생깁니다.",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "",
  "from_community_srt": "이 말은, 한편으론, 비용 함수에 한 역할을 하는 a_0^(L)에 영향을 미치기도 하지만. 마찬가지로 비용 함수에 한 역할을 하는 a_1^(L)에도 영향을 준다는 뜻입니다. 그리고 이걸 다 더하면 됩니다.",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "",
  "from_community_srt": "그리고... 뭐 이게 다입니다.",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "",
  "from_community_srt": "비용 함수가 이 두번째와 마지막 층 사이의 활성화 정도에 얼마나 민감한지를 알게 된다면, 그 층에 들어가는 모든 가중치와 편향에 이 과정을 반복해주면 됩니다.",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "",
  "from_community_srt": "자, 이제 스스로를 칭찬해도 됩니다.",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "",
  "from_community_srt": "이걸 전부 이해했다면, 역전파의 핵심을 깊이 파고들어본 것입니다. 그리고 역전파는 인공 신경망 학습의 심장이죠.",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "",
  "from_community_srt": "이런 연쇄 법칙 표현로 그라디언트의 각 구성 요소를 결정하는 도함수를 구할 수 있고, 이는 언덕 아래로 반복적으로 걸어 내려가며 네트워크의 비용을 최소화하는데 도움을 줍니다.",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "",
  "from_community_srt": "흐으으음. 편안히 앉아서 이걸 전부 생각해본다면, 정리해야할 복잡성의 층이 꽤나 많습니다. 이걸 다 이해하는데 시간이 걸린다고 걱정하진 마세요.",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]