[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "여기서 어려운 가정은 역전파 알고리즘에 대한 직관적인 안내를 제공하는 3부를 시청했다는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "여러분이 파트 3를 보셨다는 가정을 하겠습니다. 역전파 알고리즘을 직관적인 방법으로 설명한 영상이었죠.",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "여기서는 좀 더 공식적으로 관련 계산을 살펴봅니다.",
  "model": "DeepL",
  "from_community_srt": "이제 조금 더 격식을 차려서, 관련된 미적분에 대해 살펴보려 합니다.",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "약간 혼란스러울 수 있는 것은 당연한 일이므로, 정기적으로 잠시 멈추고 숙고하라는 말은 다른 곳과 마찬가지로 여기에도 적용됩니다.",
  "model": "DeepL",
  "from_community_srt": "이것이 약간 혼란스러울수 있습니다. 그러니 중간중간 멈추고 숙고하라는 진리는 다른 곳에서와 마찬가지로 이 영상에도 적용되겠죠.",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "이 강좌의 주요 목표는 대부분의 미적분 입문 강좌가 이 주제에 접근하는 방식과는 다른, 네트워크의 맥락에서 미적분학의 연쇄 법칙에 대해 머신 러닝 분야의 사람들이 일반적으로 어떻게 생각하는지 보여주는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "이 영상의 주 목표는 머신 러닝에서 일하는 사람들이 네트워크의 관점에서 연쇄 법칙(chain rule)을 생각하는지 보여주는 것입니다. 대부분의 미적분학 기초에서 접근하는 방법과는 꽤 다른 느낌이 들겁니다.",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "관련 미적분학이 어려운 분들을 위해 이 주제에 대한 전체 시리즈가 준비되어 있습니다.",
  "model": "DeepL",
  "from_community_srt": "관련된 미적분이 불편하신 분들은 제가 만든 미적분에 관한 영상 시리즈를 보시면 됩니다.",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "각 레이어에 뉴런이 하나씩 있는 매우 간단한 네트워크부터 시작하겠습니다.",
  "model": "DeepL",
  "from_community_srt": "아주 단순한 네트워크를 가지고 시작합시다. 한 층에 하나의 뉴런만 있는 네트워크죠.",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "이 네트워크는 세 가지 가중치와 세 가지 편향으로 결정되며, 우리의 목표는 비용 함수가 이러한 변수에 얼마나 민감한지 이해하는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "이 네트워크는 3개의 가중치(weight)와 3개의 편향(bias)만으로 결정됩니다. 우리의 목표는 비용 함수(cost function)가 이런 변수에 대해서 얼마나 민감하게 변하는지 이해하는 것입니다.",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way, we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "이렇게 하면 어떤 조건을 조정하면 비용 함수를 가장 효율적으로 줄일 수 있는지 알 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "그렇게 한다면 이런 것들(가중치, 편향)을 어떻게 바꾸는 것이 비용 함수를 가장 효율적으로 낮추는지 알게 될 것입니다.",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "And we're just going to focus on the connection between the last two neurons.",
  "translatedText": "그리고 마지막 두 뉴런 사이의 연결에 집중하겠습니다.",
  "model": "DeepL",
  "from_community_srt": "그리고 우리는 마지막 2개의 뉴런 사이의 연결에만 집중할 것입니다.",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in, so the activation of the previous neuron is Al-1.",
  "translatedText": "마지막 뉴런의 활성화에 위첨자 L을 붙여서 어느 층에 있는지를 나타내므로 이전 뉴런의 활성화는 Al-1이 됩니다.",
  "model": "DeepL",
  "from_community_srt": "마지막 층의 활성화 정도를 a에다 위첨자로 어느 층인지를 나타내는 L을 붙여 표현합시다. 그렇다면 그 전 뉴런의 활성화 정도는 a^(L-1)이 되겠죠.",
  "n_reviews": 0,
  "start": 65.98,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "이것은 지수가 아니라 나중에 다른 인덱스에 대한 구독을 저장하고 싶기 때문에 우리가 말하는 것을 인덱싱하는 방법일 뿐입니다.",
  "model": "DeepL",
  "from_community_srt": "지수가 아니라, 어느 층을 나타내는지 번호를 붙여주는 방법입니다. 아랫첨자는 나중에 다른 번호를 붙여줄 생각이기 때문에 그렇게 표시한 것입니다.",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "예를 들어, 주어진 훈련 예제에 대해 이 마지막 활성화의 값이 0 또는 1이 될 수 있는 y라고 가정해 보겠습니다.",
  "model": "DeepL",
  "from_community_srt": "주어진 학습 예제에 대해 이 마지막 활성화 정도가 y가 되기를 원한다고 해보죠. 예를 들어, y는 0이거나 1일 수 있습니다.",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is Al-y2.",
  "translatedText": "따라서 단일 훈련 예제에 대한 이 네트워크의 비용은 Al-y2입니다.",
  "model": "DeepL",
  "from_community_srt": "그렇다면 하나의 학습 예제에 대한 이 단순한 네트워크의 비용은 (a^(L) - y)^2입니다.",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "이 트레이닝 예제의 비용을 c0으로 표시하겠습니다.",
  "model": "DeepL",
  "from_community_srt": "이 하나의 학습 예제에 대한 비용을 C_0라 표기하겠습니다.",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neuron's activation plus some bias, which I'll call BL.",
  "translatedText": "이 마지막 활성화는 WL이라고 부르는 가중치와 이전 뉴런의 활성화에 약간의 편향성을 더한 값에 의해 결정됩니다(BL이라고 부릅니다).",
  "model": "DeepL",
  "from_community_srt": "다시 말씀드리자면, 이 마지막 활성화 정도는 앞으로 w^(L)이라 부를 가중치(weight)와 이전 뉴런의 활성화 정도를 곱한것, 거기에 앞으로 b^(L)이라 부를 편향(bias)를 더한 것에 의해 결정됩니다.",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "그런 다음 시그모이드 또는 ReLU와 같은 특수 비선형 함수를 통해 이를 펌핑합니다.",
  "model": "DeepL",
  "from_community_srt": "이제 이걸 시그모이드(sigmoid)나 ReLU같은 특별한 비선형 함수에 집어넣는거죠.",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "이 가중치 합계에 관련 활성화와 동일한 위첨자를 사용하여 Z와 같은 특별한 이름을 지정하면 실제로 작업이 더 쉬워질 것입니다.",
  "model": "DeepL",
  "from_community_srt": "만약 이 가중합(weighted sum)에 z같은 특별한 이름을 붙여준다면 편리할겁니다. 거기에 관련된 활성화 정도와 똑같은 윗첨자를 붙여주는거죠.",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action and the bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "많은 용어가 있지만, 이를 개념화할 수 있는 방법은 가중치, 이전 행동, 편향성을 모두 사용하여 z를 계산하고, 이를 통해 a를 계산하여 상수 y와 함께 최종적으로 비용을 계산할 수 있다는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "꽤 많은 표기법들이 나왔습니다. 아마 이렇게 이해하셨을겁니다. 가중치(weight), 이전 활성화 정도, 그리고 편향(bias)이 모두 사용돼 z를 계산하고, 이를 이용해 a를 계산하며, 최종적으로, 상수 y와 함께 쓰여, 비용을 계산한다는 것이죠.",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course Al-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "물론 Al-1은 그 자체의 무게와 편향성 등에 영향을 받지만, 지금은 여기에 초점을 맞추지 않겠습니다.",
  "model": "DeepL",
  "from_community_srt": "그리고 물론  a^(L-1)은 자기 자신의 가중치(weight)와 편향(bias)에 영향을 받고, 하는 식이죠. 하지만 지금 당장 여기에 초점을 두진 않을겁니다.",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "이 모든 것은 숫자에 불과하죠?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "그리고 각각에 고유한 작은 번호선이 있다고 생각하면 좋습니다.",
  "model": "DeepL",
  "from_community_srt": "이것들 전부 그냥 숫자들 맞죠? 각각이 모두 작은 수직선을 가지고 있다고 생각하면 좋을 것입니다.",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight WL.",
  "translatedText": "첫 번째 목표는 비용 함수가 가중치 WL의 작은 변화에 얼마나 민감한지 이해하는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "우리의 첫 목적은 가중치 w^(L)의 작은 변화에 비용 함수가 얼마나 민감하게 반응하는지 이해하는 것입니다.",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to WL?",
  "translatedText": "또는 다르게 표현하면, WL과 관련하여 c의 미분은 무엇인가요?",
  "model": "DeepL",
  "from_community_srt": "다르게 말하자면, C의 w^(L)에 대한 미분값이 무엇인가죠.",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del W term, think of it as meaning some tiny nudge to W, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "이 델 W 용어를 볼 때는 0.01의 변화와 같이 W에 대한 작은 넛지를 의미한다고 생각하고, 델 c 용어는 비용에 대한 결과적인 넛지를 의미한다고 생각하면 됩니다.",
  "model": "DeepL",
  "from_community_srt": "여러분이 “∂w”을 보면 그것이 0.01같이  \"w에 가해진 아주 약간의 변화\"라는 뜻이라고 생각하세요. 그리고이 \"∂C\"라는 용어는 \"그 결과로 비용이 바뀌는 정도\"라고 생각하시고요.",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "우리가 원하는 것은 그들의 비율입니다.",
  "model": "DeepL",
  "from_community_srt": "그 비율을 알고 싶은 겁니다.",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "개념적으로 WL에 대한 이 작은 넛지는 ZL에 약간의 넛지를 유발하고, 이는 다시 AL에 약간의 넛지를 유발하여 비용에 직접적인 영향을 미칩니다.",
  "model": "DeepL",
  "from_community_srt": "개념적으로 보면, w^(L)의 약간의 변화는  z^(L)가 조금 변하게 하겠죠. 그렇다면 a^(L)에 변화가 조금 생길꺼고, 그 변화는 비용에 직접적인 영향을 미칠 것입니다.",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is, the derivative of ZL with respect to WL.",
  "translatedText": "따라서 먼저 ZL에 대한 작은 변화와 이 작은 변화 W의 비율, 즉 WL에 대한 ZL의 미분을 살펴봄으로써 상황을 세분화합니다.",
  "model": "DeepL",
  "from_community_srt": "그러면 이걸 쪼개기 위해 먼저 z^(L)의 작은 변화와 w^(L)의 작은 변화의 비율을 살펴볼 수 있겠죠. 그건 z^(L)의 w^(L)에 대한 미분값이 됩니다.",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "마찬가지로, AL에 대한 변화와 그 원인이 된 ZL의 작은 변화의 비율, 그리고 최종 넛지와 이 중간 넛지 사이의 비율을 고려합니다.",
  "model": "DeepL",
  "from_community_srt": "마찬가지로, 그 다음엔 z^(L)의 작은 변화와 그로 인한 a^(L)의 변화의 비율을 살펴보죠. 거기에 a^(L)에 생긴 중간 변화와 최종적으로 C에 생긴 변화의 비율 또한 살펴봅니다.",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of c to small changes in WL.",
  "translatedText": "바로 여기에 연쇄 법칙이 있는데, 이 세 가지 비율을 곱하면 WL의 작은 변화에 대한 c의 민감도를 알 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "이것이 바로 연쇄 법칙(chain rule)입니다. 이 세 비율을 곱하는 것으로 C의 w^(L)의 작은 변화에 대한 민감도를 알 수 있는 것이죠.",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "지금 화면에는 많은 기호가 표시되어 있는데, 이제 관련 파생상품을 계산할 것이므로 잠시 시간을 내어 기호가 무엇인지 명확히 알아두세요.",
  "model": "DeepL",
  "from_community_srt": "이 화면을 보면, 꽤 많은 기호들이 많이 있습니다. 잠시 시간을 가지고 어느게 어느 것인지 정리해보세요. 이제 이걸 가지고 연관된 미분을 계산할 것입니다.",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "AL에 대한 c의 도함수는 2AL-y로 계산됩니다.",
  "model": "DeepL",
  "from_community_srt": "C의 a^(L)에 대한 도함수는 2(a^(L) - y)이 됩니다.",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "Notice this means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "이는 네트워크의 출력과 우리가 원하는 것 사이의 차이에 비례한다는 의미이므로, 그 출력이 매우 다르다면 약간의 변화도 최종 비용 함수에 큰 영향을 미칠 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "참고로, 이 크기가 네트워크의 출력과 비례한다는 뜻입니다. 바라던대로죠. 그래서 출력이 크게 다르다면, 아주 약간의 변화도 비용 함수에 큰 영향을 미치겠죠.",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "ZL에 대한 AL의 도함수는 시그모이드 함수 또는 사용자가 선택한 비선형성에 대한 도함수일 뿐입니다.",
  "model": "DeepL",
  "from_community_srt": "a^(L)를 z^(L)에 대해 미분한 것은 그냥 시그모이드 함수의 도함수입니다. 아님 쓰기로 한 다른 비선형 함수거나요.",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "And the derivative of ZL with respect to WL comes out to be AL-1.",
  "translatedText": "그리고 WL에 대한 ZL의 도함수는 AL-1로 나옵니다.",
  "model": "DeepL",
  "from_community_srt": "그리고 Z^(L)을 w^(L)에 대해 미분한 것은 이 경우는 그냥 a^(L-1)이 되네요.",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all mean.",
  "translatedText": "여러분은 어떤지 모르겠지만, 저는 잠시 시간을 내서 그 공식의 의미를 되새겨보지 않고는 그 공식에 갇혀버리기 쉽다고 생각합니다.",
  "model": "DeepL",
  "from_community_srt": "모르긴 모르지만, 이것들이 실제로는 다 무엇일지 잠깐 시간을 내어 생각해보지  않았다면 여기에서 막히기 딱 좋습니다.",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "이 마지막 파생물의 경우, 가중치에 대한 작은 넛지가 마지막 층에 영향을 미치는 정도는 이전 뉴런의 강도에 따라 달라집니다.",
  "model": "DeepL",
  "from_community_srt": "마지막 미분을 보면, 이 가중치에 생긴 작은 변화가 마지막 층에 미치는 영향의 양은 그 전 뉴런이 얼마나 강한지에 달려 있습니다.",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "뉴런이 함께 불을 붙인다는 아이디어가 바로 여기에서 나온다는 것을 기억하세요.",
  "model": "DeepL",
  "from_community_srt": "기억해보면, \"함께 발화하는 뉴런이 함께 연결된다\"라는 아이디어가 여기에 적용된 것입니다.",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to WL only of the cost for a specific single training example.",
  "translatedText": "그리고 이 모든 것은 특정 단일 교육 예제에 대한 비용만 WL과 관련하여 파생된 것입니다.",
  "model": "DeepL",
  "from_community_srt": "이 모든 것은 w^(L)에 대해 특정한 학습 예제의 비용만을 미분한 것입니다.",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "전체 비용 함수는 다양한 훈련 예제에서 모든 비용의 평균을 구하는 것이므로, 그 미분은 모든 훈련 예제에서 이 식의 평균을 구해야 합니다.",
  "model": "DeepL",
  "from_community_srt": "전체 비용 함수는 수많은 예제의 비용들을 전부 평균낸 것과 관련이 있기 때문에, 그 미분을 구하려면 지금껏 찾은 식을 모든 학습 예제에 적용한 것을 평균내야 합니다.",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "And of course, that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "물론 이는 모든 가중치와 편향에 대한 비용 함수의 부분 미분으로부터 구축되는 그라디언트 벡터의 한 구성 요소일 뿐입니다.",
  "model": "DeepL",
  "from_community_srt": "물론 그건 그라디언트 벡터를 이루는 단 하나의 구성 요소일 뿐이고, 그 그라디언트 벡터는 비용 함수의 그 모든 가중치와 편향에 대한 편미분으로 이루어져 있습니다.",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "하지만 이는 우리가 필요로 하는 많은 부분적인 파생 상품 중 하나에 불과하지만, 작업의 50% 이상을 차지합니다.",
  "model": "DeepL",
  "from_community_srt": "이게 비록 우리가 필요로 하는 편미분들 중 단 하나뿐이었지만, 우리가 할 일의 절반 이상은 한 셈입니다.",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "예를 들어 편향에 대한 민감도는 거의 동일합니다.",
  "model": "DeepL",
  "from_community_srt": "편향(bias)에 대한 민감도는, 예컨데, 거의 동일합니다.",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "이 델 z 델 w 용어를 델 z 델 b로 바꾸기만 하면 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "관련 공식을 살펴보면 그 미분은 1로 나옵니다.",
  "model": "DeepL",
  "from_community_srt": "우리가 해야할 것은  ∂z/∂w 부분을 ∂z/∂b로 바꾸는 것 뿐이고, 관련된 공식을 보면, 그 도함수는 1이 됩니다.",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "또한, 역전파라는 아이디어가 여기서 나오는데, 이 비용 함수가 이전 계층의 활성화에 얼마나 민감한지 알 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "그리고, '역으로 전파한다'는 개념이 여기에 적용되어, 비용 함수가 이전 층의 활성화 정도에 얼마나 민감한지 알 수 있습니다.",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight WL.",
  "translatedText": "즉, 체인 규칙 표현식에서 이 초기 도함수, 즉 이전 활성화에 대한 z의 민감도가 가중치 WL로 나옵니다.",
  "model": "DeepL",
  "from_community_srt": "이름하여, 연쇄 법칙을 이용한 확장의 첫번째 도함수는, 이전 활성화 정도에 대한 z의 민감도인데, 가중치 w^(L)이 됩니다.",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "다시 말하지만, 이전 레이어 활성화에 직접적인 영향을 줄 수는 없지만, 이제 동일한 체인 규칙 아이디어를 거꾸로 반복하여 비용 함수가 이전 가중치와 이전 편향에 얼마나 민감한지 확인할 수 있으므로 계속 추적하는 것이 도움이 됩니다.",
  "model": "DeepL",
  "from_community_srt": "또다시, 그 직전의 활성화 정도에 직접적으로 영항을 줄 수는 없더라도, 추적하는데 도움이 됩니다. 왜냐면 이젠 이 연쇄 법칙이라는 발상을 거꾸로 반복해 나가며 비용 함수가 그 이전 가중치와 이전 편향에 얼마나 민감한지 알 수 있기 때문입니다.",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "모든 레이어에는 하나의 뉴런이 있고 실제 네트워크에서는 상황이 기하급수적으로 복잡해지기 때문에 지나치게 단순한 예라고 생각할 수도 있습니다.",
  "model": "DeepL",
  "from_community_srt": "이게 지나치게 단순화된 예제라고 생각하실 수도 있습니다. 왜냐면 모든 레이어가 단 하나의 뉴런만을 가지고 있기 때문이죠. 그리고 실제 네트워크에서는 지수적으로 복잡해질 거라고 말입니다.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "하지만 솔직히 레이어에 여러 개의 뉴런을 부여해도 그다지 큰 변화는 없으며, 추적해야 할 지표가 몇 개 더 늘어나는 것에 불과합니다.",
  "model": "DeepL",
  "from_community_srt": "하지만 솔직히, 각 층에 여러 뉴런이 있어도 크게 바뀌는건 아닙니다. 그냥 번호만 몇개 더 추적하는겁니다.",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "특정 레이어의 활성화가 단순히 AL이 아니라 해당 레이어의 어떤 뉴런인지를 나타내는 아래 첨자가 붙게 됩니다.",
  "model": "DeepL",
  "from_community_srt": "주어진 층의 활성화 정도를 단순하게 a^(L)이라고 표기하는 대신, 그 층의 어느 뉴런인지를 표기하는 아랫첨자를 붙이는 겁니다.",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "문자 k를 사용하여 레이어 L-1의 색인을 생성하고 j를 사용하여 레이어 L의 색인을 생성해 보겠습니다.",
  "model": "DeepL",
  "from_community_srt": "(L-1)번 레이어에 k를 이용해 번호를 매기고, (L)번 레이어는 j를 이용해 번호를 매겨봅시다.",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "비용의 경우, 다시 원하는 출력이 무엇인지 살펴보지만 이번에는 이러한 마지막 레이어 활성화와 원하는 출력 간의 차이의 제곱을 더합니다.",
  "model": "DeepL",
  "from_community_srt": "비용을 알아보기 위해, 다시 한번 원하는 출력이 무엇인지 확인합시다. 그러나 이번엔 마지막 레이어의 활성화 정도와 원하는 출력 사이의 차이의 제곱을 더할 겁니다.",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus Yj squared.",
  "translatedText": "즉, ALj에서 Yj 제곱을 뺀 합계를 취합니다.",
  "model": "DeepL",
  "from_community_srt": "즉, (a_j^(L) - y_j)^2의 합을 구한다는 것입니다.",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "가중치가 훨씬 더 많으므로 각 가중치는 위치를 추적하기 위해 몇 개의 인덱스를 더 가져야 하므로 이 k번째 뉴런과 j번째 뉴런을 연결하는 에지의 가중치를 WLjk라고 부르겠습니다.",
  "model": "DeepL",
  "from_community_srt": "가중치가 훨씬 많으므로, 각각이 어느 것인지 추적하기 위해 번호가 몇 개 더 필요합니다. 그러니 k번째 뉴런과 j번째 뉴런을 연결하는 간선(edge)의 가중치를 w_{jk}^(L) 라고 표기합시다.",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "이러한 인덱스는 처음에는 약간 거꾸로 느껴질 수 있지만 1부 동영상에서 설명한 가중치 매트릭스를 인덱싱하는 방법과 일치합니다.",
  "model": "DeepL",
  "from_community_srt": "이 번호는 처음 볼 땐 거꾸로 쓰인 것 같지만, Part 1 비디오에서 어떻게 가중치 행렬에 번호를 매길지 말했던 것과 연관이 있습니다.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "이전과 마찬가지로 z와 같이 관련 가중 합계에 이름을 지정하여 마지막 레이어의 활성화가 z에 적용된 시그모이드와 같은 특수 함수만 활성화되도록 하는 것이 좋습니다.",
  "model": "DeepL",
  "from_community_srt": "그 전처럼, 관련된 가중합에 z같은 이름을 주는게 좋고, 그러면 마지막 층의 활성화 정도는 시그모이드같은 특별한 함수에 z를 적용한게 됩니다.",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "레이어당 하나의 뉴런을 사용하는 경우와 본질적으로 동일한 방정식을 사용하지만 조금 더 복잡해 보일 뿐이라는 것을 알 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "제가 무슨 말을 하고 있는지 아시겠죠? 이건 한 레이어당 하나의 뉴런이 있던 경우와 본질적으로 같은 공식입니다. 약간 더 복잡해보일 뿐이죠.",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain-ruled derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "실제로 비용이 특정 무게에 얼마나 민감한지를 설명하는 체인 규칙 파생식도 본질적으로 동일하게 보입니다.",
  "model": "DeepL",
  "from_community_srt": "그리고 확실히, 비용이 얼마나 특정한 가중치에 민감한지를 보여주는 연쇄법칙 도함수 표현은 본질적으로 똑같아 보입니다.",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "원하신다면 잠시 멈춰서 각 용어에 대해 생각해 보시기 바랍니다.",
  "model": "DeepL",
  "from_community_srt": "각 표현이 무슨 뜻인지 잠시 영상을 멈추고 생각해봐도 좋습니다.",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "하지만 여기서 변경되는 것은 레이어 L-1의 활성화 중 하나에 대한 비용의 미분입니다.",
  "model": "DeepL",
  "from_community_srt": "다만, 여기에서 바뀐 것은, 비용의 (L-1)번 층 중 하나의 활성화 정도에 대한 도함수입니다.",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "이 경우 뉴런이 여러 다른 경로를 통해 비용 함수에 영향을 미친다는 점이 다릅니다.",
  "model": "DeepL",
  "from_community_srt": "이 경우, 뉴런이 비용 함수에 여러 경로를 통해 영향을 준다는 차이점이 생깁니다.",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "즉, 한편으로는 비용 함수에서 역할을 하는 AL0에 영향을 미치지만, 다른 한편으로는 비용 함수에서 역할을 하는 AL1에도 영향을 미치므로 이를 합산해야 합니다.",
  "model": "DeepL",
  "from_community_srt": "이 말은, 한편으론, 비용 함수에 한 역할을 하는 a_0^(L)에 영향을 미치기도 하지만. 마찬가지로 비용 함수에 한 역할을 하는 a_1^(L)에도 영향을 준다는 뜻입니다. 그리고 이걸 다 더하면 됩니다.",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "그리고 그게 거의 전부입니다.",
  "model": "DeepL",
  "from_community_srt": "그리고... 뭐 이게 다입니다.",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "이 두 번째에서 마지막 레이어의 활성화에 비용 함수가 얼마나 민감한지 알게 되면, 해당 레이어에 공급되는 모든 가중치와 편향에 대해 이 과정을 반복하면 됩니다.",
  "model": "DeepL",
  "from_community_srt": "비용 함수가 이 두번째와 마지막 층 사이의 활성화 정도에 얼마나 민감한지를 알게 된다면, 그 층에 들어가는 모든 가중치와 편향에 이 과정을 반복해주면 됩니다.",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "그러니 스스로를 칭찬해 주세요!",
  "model": "DeepL",
  "from_community_srt": "자, 이제 스스로를 칭찬해도 됩니다.",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "이 모든 것이 이해가 되셨다면 이제 신경망 학습의 핵심인 역전파의 핵심을 깊이 들여다보셨습니다.",
  "model": "DeepL",
  "from_community_srt": "이걸 전부 이해했다면, 역전파의 핵심을 깊이 파고들어본 것입니다. 그리고 역전파는 인공 신경망 학습의 심장이죠.",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "이러한 체인 규칙 표현식은 경사도의 각 구성 요소를 결정하는 파생물을 제공하여 반복적으로 내리막을 밟아 네트워크의 비용을 최소화하는 데 도움이 됩니다.",
  "model": "DeepL",
  "from_community_srt": "이런 연쇄 법칙 표현로 그라디언트의 각 구성 요소를 결정하는 도함수를 구할 수 있고, 이는 언덕 아래로 반복적으로 걸어 내려가며 네트워크의 비용을 최소화하는데 도움을 줍니다.",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "가만히 앉아 이 모든 것을 생각해보면 복잡하게 얽혀 있는 내용들이 많기 때문에 이 모든 것을 소화하는 데 시간이 걸리더라도 걱정하지 마세요.",
  "model": "DeepL",
  "from_community_srt": "흐으으음. 편안히 앉아서 이걸 전부 생각해본다면, 정리해야할 복잡성의 층이 꽤나 많습니다. 이걸 다 이해하는데 시간이 걸린다고 걱정하진 마세요.",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]