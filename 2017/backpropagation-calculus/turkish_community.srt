1
00:00:04,230 --> 00:00:07,120
Buradaki zor varsayım, bölüm 3'ü izlediğinizdir.

2
00:00:07,120 --> 00:00:10,230
geri yayılım algoritmasının sezgisel bir adımını vermek.

3
00:00:11,040 --> 00:00:14,770
Burada biraz daha resmi olacağız ve ilgili matematiğe dalacağız.

4
00:00:14,770 --> 00:00:17,040
Bunun biraz kafa karıştırıcı olması normal.

5
00:00:17,040 --> 00:00:21,480
bu yüzden düzenli aralıklarla durmak ve düşünmek için mantra kesinlikle burada herhangi bir yerde olduğu kadar geçerlidir.

6
00:00:21,920 --> 00:00:25,180
Temel hedefimiz, insanların makine öğrenmesinde nasıl olduğunu göstermek.

7
00:00:25,180 --> 00:00:29,440
ağlar bağlamında matematikten zincir kuralını genel olarak düşünmek,

8
00:00:29,440 --> 00:00:33,820
hangi giriş kurslarının en çok konuya yaklaştığı konusunda farklı bir havası vardır.

9
00:00:34,500 --> 00:00:36,890
İlgili hesaplardan rahatsız olanlar için,

10
00:00:36,890 --> 00:00:39,040
Konuyla ilgili bir dizi var.

11
00:00:40,340 --> 00:00:43,150
Son derece basit bir ağla başlayalım,

12
00:00:43,150 --> 00:00:45,730
her katmanın içinde tek bir nöron bulunduğu bir tane.

13
00:00:46,270 --> 00:00:50,680
Yani bu özel ağ 3 ağırlık ve 3 önyargı ile belirlenir,

14
00:00:50,680 --> 00:00:55,070
ve amacımız, maliyet fonksiyonunun bu değişkenlere ne kadar hassas olduğunu anlamaktır.

15
00:00:55,550 --> 00:00:57,830
Bu şekilde, bu şartlarda hangi ayarların yapıldığını biliyoruz.

16
00:00:57,830 --> 00:01:00,940
maliyet fonksiyonunda en etkin düşüşe neden olacaktır.

17
00:01:01,920 --> 00:01:05,170
Ve biz sadece son iki nöron arasındaki bağlantıya odaklanıyoruz.

18
00:01:05,880 --> 00:01:11,370
En son nöron a'nın aktivasyonunu, içinde L katmanı ile işaretleyelim, hangi katmanda olduğunu gösterelim.

19
00:01:11,690 --> 00:01:15,720
bu önceki nöronun aktivasyonu bir ^ (L-1).

20
00:01:16,430 --> 00:01:20,030
Üstadlar yok, onlar sadece neden bahsettiğimizi endekslemenin bir yolu.

21
00:01:20,030 --> 00:01:22,970
Zira daha sonra farklı endekslere abone kaydetmek istiyorum.

22
00:01:23,740 --> 00:01:29,710
Diyelim ki bu son aktivasyonun belirli bir eğitim örneği için olmasını istediğimiz değer y.

23
00:01:30,170 --> 00:01:32,360
Örneğin, y 0 veya 1 olabilir.

24
00:01:32,940 --> 00:01:39,470
Yani bu basit ağın tek bir eğitim örneği için maliyeti (a ^ (L) - y) ^ 2.

25
00:01:40,250 --> 00:01:44,650
Bu eğitim örneğinin maliyetini C_0 olarak göstereceğiz.

26
00:01:46,030 --> 00:01:51,520
Bir hatırlatıcı olarak, bu son aktivasyon w ^ (L) olarak adlandıracağım bir ağırlıkla belirlenir.

27
00:01:51,980 --> 00:01:54,220
önceki nöronun aktivasyonunun birkaç katı,

28
00:01:54,530 --> 00:01:56,940
artı b ^ (L) diyeceğim bazı önyargılar,

29
00:01:57,480 --> 00:01:59,900
o zaman bunu bazı özel doğrusal olmayan fonksiyonlar vasıtasıyla pompalarsınız.

30
00:01:59,900 --> 00:02:01,520
Bir sigmoid veya bir ReLU gibi.

31
00:02:01,850 --> 00:02:06,980
Eğer bu z toplam gibi özel bir isim verirsek, bu bizim için işleri kolaylaştıracak,

32
00:02:06,980 --> 00:02:09,550
ilgili aktivasyonlarla aynı üst simge ile.

33
00:02:10,390 --> 00:02:11,480
Yani bir çok terim var.

34
00:02:11,480 --> 00:02:16,960
Kavramsallaştırmanın bir yolu da, ağırlık, önceki aktivasyon ve önyargı.

35
00:02:16,960 --> 00:02:21,400
tamamen z'yi hesaplamak için kullanılır, ki bu da sırayla a.

36
00:02:21,740 --> 00:02:25,610
ve nihayet, y sabiti ile birlikte, maliyeti hesaplayalım.

37
00:02:27,260 --> 00:02:31,660
Ve elbette, bir ^ (L-1) kendi ağırlığından ve önyargısından etkilenir ve böyledir.

38
00:02:32,810 --> 00:02:34,840
Ama şu anda buna odaklanmayacağız.

39
00:02:35,680 --> 00:02:38,040
Bunların hepsi sadece sayılar, değil mi?

40
00:02:38,040 --> 00:02:41,230
Ve her birinin kendi küçük sayı çizgisine sahip olduğunu düşünmek güzel olabilir.

41
00:02:41,900 --> 00:02:43,990
İlk hedefimiz anlamaktır

42
00:02:43,990 --> 00:02:48,940
Maliyet fonksiyonunun ağırlığımızdaki küçük değişikliklere ne kadar duyarlı olduğu w ^ (L).

43
00:02:49,640 --> 00:02:54,880
Veya farklı ifadelerle, C ^ 'nin w ^ (L)' ye göre türevi nedir.

44
00:02:55,630 --> 00:02:58,070
Bu “∂w” terimini gördüğünüzde,

45
00:02:58,070 --> 00:03:02,750
0,01 gibi bir değişiklik gibi, "w için küçük bir dürtmek" anlamını düşünün.

46
00:03:03,150 --> 00:03:08,210
Ve bu “∂C” terimini “maliyete bağlı dürtüsü ne olursa olsun” olarak düşünün.

47
00:03:08,710 --> 00:03:10,420
İstediğimiz şey onların oranı.

48
00:03:11,210 --> 00:03:16,520
Kavramsal olarak, bu küçük dürtmek w ^ (L) 'ye biraz dürtmek z ^ (L)' ye neden olur

49
00:03:16,520 --> 00:03:21,380
ki bu da maliyeti doğrudan etkileyen bir ^ (L) 'de değişikliğe neden olur.

50
00:03:23,100 --> 00:03:28,930
Bu yüzden bunu önce küçük bir değişikliğin z ^ (L) 'ye olan küçük değişimin w ^ (L)' deki küçük değişime oranına bakarak çözüyoruz.

51
00:03:29,290 --> 00:03:33,030
Yani, z ^ (L) 'nin w ^ (L)' ye göre türevidir.

52
00:03:33,760 --> 00:03:39,410
Aynı şekilde, ^ (L) 'deki bir değişikliğin, z ^ (L)' deki küçük değişime oranını,

53
00:03:39,850 --> 00:03:44,880
ve ayrıca son dürtme ile C arasındaki oran ve bu ara dürtme bir ^ (L) 'ye olan oran.

54
00:03:45,670 --> 00:03:47,850
Buradaki zincir kuralı.

55
00:03:47,850 --> 00:03:54,950
bu üç oranın birlikte çarpılması bize C nin w ^ (L) 'deki küçük değişikliklere duyarlılığını verir.

56
00:03:57,190 --> 00:04:00,040
Şu anda ekranda, bir sürü sembol var.

57
00:04:00,040 --> 00:04:03,000
o yüzden, hepsinin ne olduğundan emin olmak için bir dakikanızı ayırın,

58
00:04:03,600 --> 00:04:06,560
çünkü şimdi ilgili türevleri hesaplayacağız.

59
00:04:07,400 --> 00:04:13,230
^ (L) 'ye göre C'nin türevi, 2 (a ^ (L) - y) olarak hesaplanır.

60
00:04:13,960 --> 00:04:16,880
Dikkat, bu, boyutunun orantılı olduğu anlamına gelir.

61
00:04:16,880 --> 00:04:20,880
Ağın çıktısı ile olmasını istediğimiz şey arasındaki fark.

62
00:04:21,360 --> 00:04:23,340
Yani bu çıktı çok farklı olsaydı,

63
00:04:23,340 --> 00:04:27,150
Küçük değişiklikler bile maliyet fonksiyonu üzerinde büyük bir etkiye sahiptir.

64
00:04:28,300 --> 00:04:33,880
Z ^ (L) 'ye göre bir ^ (L)' nin türevi sigmoid fonksiyonumuzun bir türevidir,

65
00:04:33,880 --> 00:04:36,370
ya da hangi doğrusallığı seçerseniz seçin.

66
00:04:37,310 --> 00:04:40,370
Ve z ^ (L) 'nin w ^ (L)' ye göre türevi,

67
00:04:41,470 --> 00:04:44,530
Bu durumda sadece bir ^ (L-1) olduğu ortaya çıkıyor.

68
00:04:46,070 --> 00:04:49,570
Şimdi seni bilmiyorum, ama bence bu formüllerde baş aşağı durmak kolay.

69
00:04:49,570 --> 00:04:53,690
arkanıza yaslanıp, gerçekte ne anlama geldiklerini kendinize hatırlatın.

70
00:04:54,120 --> 00:04:56,040
Bu son türev durumunda,

71
00:04:56,040 --> 00:05:00,060
bu ağırlığa küçük bir dürtmenin son katmanı etkilediği miktar

72
00:05:00,060 --> 00:05:02,850
önceki nöronun ne kadar güçlü olduğuna bağlıdır.

73
00:05:03,310 --> 00:05:07,520
Unutma, burası “birlikte telleri birleştiren nöronlar” fikrinin geldiği yer.

74
00:05:09,210 --> 00:05:15,940
Ve bunların tümü, sadece belirli bir eğitim örneğinin maliyetinin “^” (L) cinsinden türevidir.

75
00:05:16,410 --> 00:05:22,150
Tam maliyet işlevi, tüm eğitim maliyetlerinde tüm bu maliyetlerin ortalama alınmasını içerdiğinden,

76
00:05:22,150 --> 00:05:27,610
türevi, tüm eğitim örneklerinde bulduğumuz bu ifadenin ortalamasını gerektirir.

77
00:05:28,430 --> 00:05:31,930
Ve elbette bu, gradyan vektörünün sadece bir bileşenidir,

78
00:05:31,930 --> 00:05:33,890
hangi inşa edilmiştir

79
00:05:33,890 --> 00:05:38,480
Maliyetin kısmi türevleri, tüm bu ağırlıklar ve önyargılara göre işlev görür.

80
00:05:40,710 --> 00:05:43,550
Fakat ihtiyaç duyduğumuz kısmi türevlerden sadece biri olmasına rağmen,

81
00:05:43,550 --> 00:05:45,390
işin% 50'sinden fazlası.

82
00:05:46,420 --> 00:05:49,940
Önyargıya duyarlılık, örneğin, neredeyse aynıdır.

83
00:05:50,250 --> 00:05:55,120
Sadece ∂z / ∂b için bu /z / ∂w terimini değiştirmemiz gerekiyor,

84
00:05:58,760 --> 00:06:02,590
Ve ilgili formüle bakarsanız, bu türev 1 olur.

85
00:06:06,210 --> 00:06:09,880
Ayrıca, işte bu, geriye doğru yayılma fikrinin devreye girdiği yerdir.

86
00:06:10,230 --> 00:06:15,670
Bu maliyet fonksiyonunun önceki katmanın aktivasyonuna ne kadar hassas olduğunu görebilirsiniz;

87
00:06:16,250 --> 00:06:19,650
yani, zincir kuralı genişlemesinde bu ilk türev,

88
00:06:19,650 --> 00:06:23,100
z'nin önceki aktivasyona duyarlılığı,

89
00:06:23,480 --> 00:06:25,670
w ^ (L) ağırlık olarak çıkıyor.

90
00:06:26,580 --> 00:06:31,500
Ve yine, bu aktivasyonu doğrudan etkilememize rağmen,

91
00:06:31,500 --> 00:06:33,080
takip etmek yararlı olur

92
00:06:33,080 --> 00:06:38,200
çünkü şimdi bu zincir kuralı fikrini geriye doğru yinelemeye devam edebiliriz

93
00:06:38,200 --> 00:06:42,750
Maliyet fonksiyonunun önceki ağırlıklara ve önceki önyargılara ne kadar hassas olduğunu görmek için.

94
00:06:43,630 --> 00:06:45,980
Ve bunun çok basit bir örnek olduğunu düşünebilirsiniz.

95
00:06:45,980 --> 00:06:47,880
çünkü tüm katmanlar sadece 1 nöron içerdiğinden,

96
00:06:47,880 --> 00:06:51,220
ve gerçek ağda işler katlanarak daha da karmaşıklaşacak.

97
00:06:51,680 --> 00:06:56,270
Ama dürüst olmak gerekirse, katmanlara çoklu nöronlar verdiğimizde pek fazla bir değişiklik olmaz.

98
00:06:56,270 --> 00:06:58,710
Gerçekten takip etmesi gereken birkaç endeks var.

99
00:06:59,340 --> 00:07:02,880
Belirli bir katmanın aktivasyonu yerine, sadece bir ^ (L) olması,

100
00:07:02,880 --> 00:07:07,210
Aynı zamanda, o katmanın hangi nöronu olduğunu belirten bir aboneye sahip olacak.

101
00:07:07,780 --> 00:07:14,470
Devam edelim ve katmanı (L-1) indekslemek için k harfini ve katmanı (L) indekslemek için j harfini kullanalım.

102
00:07:15,290 --> 00:07:18,910
Maliyet için, yine istenen çıktının ne olduğuna bakarız.

103
00:07:18,910 --> 00:07:19,380
Ama bu sefer

104
00:07:19,380 --> 00:07:25,260
bu son katman aktivasyonları ile istenen çıktı arasındaki farkların karelerini toplarız.

105
00:07:26,060 --> 00:07:31,070
Yani, (a_j ^ (L) - y_j) ^ 2 değerinden bir miktar alırsınız.

106
00:07:33,110 --> 00:07:34,520
Çok fazla ağırlık olduğundan,

107
00:07:34,520 --> 00:07:37,650
her birinin nerede olduğunu takip etmek için birkaç endeksi olmalı.

108
00:07:38,010 --> 00:07:44,990
Öyleyse bu kthth nöronunu j-th nöronuna w_ {jk} ^ (L) bağlayan kenarın ağırlığını diyelim.

109
00:07:45,660 --> 00:07:48,260
Bu endeksler ilk başta biraz geri kalmış olabilir.

110
00:07:48,260 --> 00:07:52,940
ancak Bölüm 1 videoda bahsettiğim ağırlık matrisini nasıl indeksleyeceğinize göre sıralanıyor.

111
00:07:53,680 --> 00:07:58,350
Daha önce olduğu gibi, yine z gibi ilgili toplamlara bir isim vermek güzel

112
00:07:58,350 --> 00:08:04,310
Böylece son katmanın aktivasyonu, z'ye uygulanan sigmoid gibi sadece sizin özel fonksiyonunuzdur.

113
00:08:05,040 --> 00:08:06,230
Ne demek istediğimi anlayabilirsin, değil mi?

114
00:08:06,230 --> 00:08:11,680
Bunların hepsi esasen, her katman başına bir nöron durumunda daha önce sahip olduğumuz denklemlerdir;

115
00:08:11,680 --> 00:08:13,870
sadece biraz daha karmaşık görünüyor.

116
00:08:15,370 --> 00:08:18,220
Ve gerçekten de, zincir kuralı türevi ifadesi

117
00:08:18,220 --> 00:08:21,980
maliyetin belirli bir ağırlığa ne kadar hassas olduğunu açıklamak

118
00:08:21,980 --> 00:08:23,890
aslında aynı görünüyor.

119
00:08:23,890 --> 00:08:26,880
İsterseniz bu terimlerin her birini duraklatmayı ve düşünmeyi size bırakıyorum.

120
00:08:29,310 --> 00:08:31,320
Yine de burada ne değişiyor?

121
00:08:31,320 --> 00:08:36,830
maliyetin katmandaki aktivasyonlardan birine göre türevidir (L-1).

122
00:08:37,760 --> 00:08:43,120
Bu durumda, fark, nöronun maliyet fonksiyonunu birçok yoldan etkilemesidir.

123
00:08:44,660 --> 00:08:50,540
Yani, bir yandan, maliyet fonksiyonunda rol oynayan a_0 ^ (L) 'yi etkiler,

124
00:08:51,010 --> 00:08:56,320
ancak aynı zamanda maliyet fonksiyonunda da rol oynayan bir ^ ^ (L) üzerinde bir etkiye sahiptir.

125
00:08:56,320 --> 00:08:57,410
Ve bunları eklemelisin.

126
00:09:00,170 --> 00:09:02,980
Ve bu ... pekala işte bu kadar.

127
00:09:03,560 --> 00:09:08,520
Maliyet işlevinin bu ikinci ve son kattaki aktivasyonlara ne kadar hassas olduğunu öğrendikten sonra,

128
00:09:08,840 --> 00:09:12,940
bu katmana beslenen tüm ağırlıklar ve önyargılar için işlemi tekrarlayabilirsiniz.

129
00:09:13,850 --> 00:09:15,360
Bu yüzden arkana yaslan!

130
00:09:15,360 --> 00:09:16,950
Bunların hepsi mantıklı geliyorsa,

131
00:09:16,950 --> 00:09:20,440
şimdi geri yayılımın kalbine derin baktın,

132
00:09:20,440 --> 00:09:22,830
sinir ağlarının nasıl öğrendiğinin ardındaki işgücü.

133
00:09:23,590 --> 00:09:29,300
Bu zincir kuralı ifadeleri, gradyandaki her bileşeni belirleyen türevleri verir

134
00:09:29,300 --> 00:09:33,550
Bu, ağ kullanımının art arda yokuş aşağı çekilerek maliyetini en aza indirmesine yardımcı olur.

135
00:09:34,280 --> 00:09:36,850
Hhhhpf. Arkanıza yaslanıp bütün bunları düşünürseniz,

136
00:09:36,850 --> 00:09:40,090
aklını sarmak için bir sürü karmaşıklık katmanı var.

137
00:09:40,090 --> 00:09:43,090
Bu yüzden zihninizin hepsini sindirmesi zaman alırsa endişelenmeyin.

