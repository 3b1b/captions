[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "ההנחה הקשה כאן היא שצפיתם בחלק 3, מה שנותן הדרכה אינטואיטיבית של אלגוריתם ההפצה לאחור.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "כאן נהיה קצת יותר פורמליים וצוללים לתוך החשבון הרלוונטי.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "זה נורמלי שזה לפחות קצת מבלבל, אז המנטרה לעצור ולהרהר באופן קבוע חלה כאן כמו בכל מקום אחר.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "המטרה העיקרית שלנו היא להראות כיצד אנשים בלימוד מכונה בדרך כלל חושבים על כלל השרשרת מחשבון בהקשר של רשתות, שיש לו תחושה שונה מהאופן שבו רוב קורסי החישוב המבוא ניגשים לנושא.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "לאלו מכם שלא מרגישים בנוח עם החשבון הרלוונטי, יש לי סדרה שלמה בנושא.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "נתחיל עם רשת פשוטה ביותר, כזו שבה בכל שכבה יש נוירון בודד.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "רשת זו נקבעת על ידי שלושה משקלים ושלוש הטיות, והמטרה שלנו היא להבין עד כמה פונקציית העלות רגישה למשתנים אלו.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "כך אנו יודעים אילו התאמות למונחים אלו יגרמו לירידה היעילה ביותר בפונקציית העלות.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "We'll just focus on the connection between the last two neurons.",
  "translatedText": "אנחנו רק נתמקד בקשר בין שני הנוירונים האחרונים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in.",
  "translatedText": "בוא נסמן את ההפעלה של הנוירון האחרון הזה עם L, המציין באיזו שכבה הוא נמצא.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.98,
  "end": 71.36
 },
 {
  "input": "So the activation of the previous neuron is AL-1.",
  "translatedText": "אז ההפעלה של הנוירון הקודם היא AL-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 71.68,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "אלה לא מעריכים, הם רק דרך להוסיף לאינדקס את מה שאנחנו מדברים עליו, מכיוון שאני רוצה לשמור מנויים למדדים שונים בהמשך.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "נניח שהערך שאנו רוצים שההפעלה האחרונה תהיה עבור דוגמה לאימון נתונה הוא y, לדוגמה, y עשוי להיות 0 או 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is AL-y2.",
  "translatedText": "אז העלות של רשת זו עבור דוגמה אחת לאימון היא AL-y2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "נסמן את העלות של דוגמה אחת לאימון כ-c0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call wL, times the previous neuron's activation plus some bias, which I'll call bL.",
  "translatedText": "להזכירך, ההפעלה האחרונה הזו נקבעת על ידי משקל, שאקרא לו wL, כפול ההפעלה של הנוירון הקודם פלוס הטיה כלשהי, שאכנה אותה bL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "Then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "ואז אתה שואב את זה דרך איזו פונקציה לא ליניארית מיוחדת כמו הסיגמואיד או ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "למעשה, זה יקל עלינו אם ניתן שם מיוחד לסכום המשוקלל הזה, כמו z, עם אותו כתב עילית כמו ההפעלה הרלוונטית.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action, and bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "זה הרבה מונחים, ודרך שבה אתה יכול להמשיג את זה היא שהמשקל, הפעולה הקודמת וההטיה ביחד משמשים לחישוב z, שבתורו מאפשר לנו לחשב את a, אשר לבסוף, יחד עם y קבוע, מאפשר אנחנו מחשבים את העלות.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course, AL-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "וכמובן, AL-1 מושפע מהמשקל שלו ומההטיה שלו וכאלה, אבל אנחנו לא מתכוונים להתמקד בזה עכשיו.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "כל אלה הם רק מספרים, נכון?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "וזה יכול להיות נחמד לחשוב שלכל אחד מהם יש שורת מספרים קטנה משלו.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight wL.",
  "translatedText": "המטרה הראשונה שלנו היא להבין עד כמה רגישה פונקציית העלות לשינויים קטנים במשקל שלנו wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to wL?",
  "translatedText": "או לנסח אחרת, מהי הנגזרת של c ביחס ל-wL?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del w term, think of it as meaning some tiny nudge to w, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "כשאתה רואה את המונח del w הזה, תחשוב על זה כעל איזה דחיפה זעירה ל-w, כמו שינוי ב-0.01, וחשבו על המונח Del c הזה כמשמעותי מה הדחיפה שתתקבל לעלות.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "מה שאנחנו רוצים זה היחס שלהם.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to wL causes some nudge to zL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "מבחינה קונספטואלית, דחיפה זעירה זו ל-wL גורמת לדחיפה כלשהי ל-zL, אשר בתורה גורמת לדחיפה כלשהי ל-AL, אשר משפיעה ישירות על העלות.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to zL to this tiny change w, that is, the derivative of zL with respect to wL.",
  "translatedText": "אז אנחנו מפרקים את הדברים על ידי הסתכלות תחילה על היחס של שינוי זעיר ל-zL לשינוי הזעיר הזה w, כלומר, הנגזרת של zL ביחס ל-wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in zL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "באופן דומה, אתה מחשיב את היחס בין השינוי ל-AL לשינוי הזעיר ב-zL שגרם לו, כמו גם את היחס בין הדחיפה הסופי ל-c לבין הדחיפה הבינונית הזו ל-AL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying these three ratios gives us the sensitivity of c to small changes in wL.",
  "translatedText": "זה ממש כאן הוא כלל השרשרת, שבו הכפלת שלושת היחסים הללו נותנת לנו את הרגישות של c לשינויים קטנים ב-wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "אז על המסך כרגע, יש הרבה סמלים, וקח רגע כדי לוודא שזה ברור מה הם כולם, כי עכשיו אנחנו הולכים לחשב את הנגזרות הרלוונטיות.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "הנגזרת של c ביחס ל-AL מתבררת להיות 2AL-y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "This means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "המשמעות היא שהגודל שלו פרופורציונלי להבדל בין התפוקה של הרשת לבין הדבר שאנחנו רוצים שהיא תהיה, כך שאם הפלט הזה היה שונה מאוד, אפילו לשינויים קלים יש השפעה גדולה על פונקציית העלות הסופית.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to zL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "הנגזרת של AL ביחס ל-zL היא רק הנגזרת של הפונקציה הסיגמואידית שלנו, או כל אי-לינאריות שתבחר להשתמש בה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "The derivative of zL with respect to wL comes out to be AL-1.",
  "translatedText": "הנגזרת של zL ביחס ל-wL יוצאת AL-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself what they all mean.",
  "translatedText": "אני לא יודע מה איתכם, אבל אני חושב שקל להיתקע עם ראש למטה בנוסחאות מבלי לקחת רגע לשבת ולהזכיר לעצמכם מה כולן אומרות.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "במקרה של נגזרת אחרונה זו, הכמות שהדחיפה הקטנה למשקל השפיעה על השכבה האחרונה תלויה במידת העוצמה של הנוירון הקודם.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "זכרו, כאן נכנס הרעיון של נוירונים-שה-יורים-ביחד-חווט-יחד.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to wL only of the cost for a specific single training example.",
  "translatedText": "וכל זה הוא הנגזרת ביחס ל-wL בלבד של העלות עבור דוגמה ספציפית לאימון בודד.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "מכיוון שפונקציית העלות המלאה כוללת ממוצע של כל העלויות הללו על פני הרבה דוגמאות אימון שונות, הנגזרת שלה דורשת ממוצע של ביטוי זה על פני כל דוגמאות ההדרכה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "Of course, that's just one component of the gradient vector, which is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "כמובן, זה רק מרכיב אחד של וקטור הגרדיאנט, אשר בנוי מהנגזרות החלקיות של פונקציית העלות ביחס לכל אותם משקלים והטיות.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "אבל למרות שזו רק אחת מהנגזרות החלקיות הרבות שאנחנו צריכים, זה יותר מ-50% מהעבודה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "הרגישות להטיה, למשל, כמעט זהה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "אנחנו רק צריכים לשנות את המונח del z del w עבור a del z del b.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "ואם אתה מסתכל על הנוסחה הרלוונטית, הנגזרת הזו יוצאת כ-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "כמו כן, וכאן נכנס הרעיון של התפשטות לאחור, ניתן לראות עד כמה פונקציית העלות הזו רגישה להפעלת השכבה הקודמת.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight wL.",
  "translatedText": "כלומר, הנגזרת הראשונית הזו בביטוי כלל השרשרת, הרגישות של z להפעלה הקודמת, יוצאת כמשקל wL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "ושוב, למרות שלא נוכל להשפיע ישירות על הפעלת השכבה הקודמת, זה מועיל לעקוב אחריה, כי עכשיו אנחנו יכולים פשוט להמשיך ולחזור על אותו רעיון כללי שרשרת לאחור כדי לראות עד כמה רגישה פונקציית העלות ל משקלים קודמים והטיות קודמות.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "ואולי תחשבו שזו דוגמה פשוטה מדי, מכיוון שלכל השכבות יש נוירון אחד, והדברים הולכים להיות מסובכים באופן אקספוננציאלי עבור רשת אמיתית.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "אבל בכנות, לא כל כך הרבה משתנה כשאנחנו נותנים לשכבות מספר נוירונים, באמת שיש רק עוד כמה מדדים שצריך לעקוב אחריהם.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "במקום שההפעלה של שכבה נתונה פשוט תהיה AL, יהיה לה גם מנוי שמציין באיזה נוירון בשכבה זו.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "בוא נשתמש באות k כדי לאינדקס את השכבה L-1, וב-j כדי להוסיף את השכבה L.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "לגבי העלות, שוב אנו מסתכלים מהי הפלט הרצוי, אך הפעם נחבר את הריבועים של ההבדלים בין הפעלת השכבה האחרונה הללו לבין הפלט הרצוי.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus yj squared.",
  "translatedText": "כלומר, אתה לוקח סכום מעל ALj מינוס yj בריבוע.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "מכיוון שיש הרבה יותר משקלים, לכל אחד צריך להיות עוד כמה מדדים כדי לעקוב אחר היכן הוא נמצא, אז בואו נקרא למשקל הקצה המחבר את הנוירון ה-k הזה לנוירון ה-j', WLjk.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "המדדים האלה אולי מרגישים קצת לאחור בהתחלה, אבל הם תואמים את האופן שבו היית מוסיף את מטריצת המשקל שעליה דיברתי בסרטון חלק 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "ממש כמו קודם, עדיין נחמד לתת שם לסכום המשוקלל הרלוונטי, כמו z, כך שההפעלה של השכבה האחרונה היא רק הפונקציה המיוחדת שלך, כמו הסיגמואיד, המוחל על z.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "אתה יכול לראות למה אני מתכוון, כאשר כל אלה הם בעצם אותן משוואות שהיו לנו קודם במקרה של נוירון-לשכבה, רק שזה נראה קצת יותר מסובך.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain rule derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "ואכן, הביטוי הנגזר של כלל השרשרת המתאר עד כמה העלות רגישה למשקל ספציפי נראה זהה בעצם.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "אני אשאיר לך לעצור ולחשוב על כל אחד מהמונחים האלה אם תרצה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "מה שכן משתנה כאן הוא הנגזרת של העלות ביחס לאחת ההפעלה בשכבה L-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "במקרה זה, ההבדל הוא שהנוירון משפיע על תפקוד העלות דרך מספר נתיבים שונים.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "כלומר, מצד אחד, זה משפיע על AL0, שמשחק תפקיד בפונקציית העלות, אבל יש לו השפעה גם על AL1, שגם משחק תפקיד בפונקציית העלות, וצריך להוסיף את אלה.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "וזה, ובכן, זה פחות או יותר.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "ברגע שאתה יודע עד כמה פונקציית העלות רגישה להפעלות בשכבה שנייה אחרונה זו, אתה יכול פשוט לחזור על התהליך עבור כל המשקולות וההטיות המוזנות לשכבה זו.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "אז טפחו לעצמכם על השכם!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "אם כל זה הגיוני, עכשיו הסתכלת עמוק לתוך לב ליבה של התפשטות לאחור, סוס העבודה מאחורי האופן שבו רשתות עצביות לומדות.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "ביטויי כלל שרשרת אלו נותנים לך את הנגזרות הקובעות כל רכיב בשיפוע שעוזר למזער את עלות הרשת על ידי ירידה חוזרת ונשנית.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "אם אתה יושב בחיבוק ידיים וחושב על כל זה, מדובר בהרבה שכבות של מורכבות לעטוף את דעתך, אז אל תדאג אם ייקח למוח שלך זמן לעכל את הכל.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]