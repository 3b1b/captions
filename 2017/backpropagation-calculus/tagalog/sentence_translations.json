[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "Ang mahirap na palagay dito ay napanood mo na ang bahagi 3, na nagbibigay ng intuitive na walkthrough ng backpropagation algorithm.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Dito ay nakakakuha tayo ng kaunti pang pormal at sumisid sa nauugnay na calculus.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "Normal lang na medyo nakakalito ito, kaya ang mantra na regular na mag-pause at magnilay-nilay ay tiyak na nalalapat dito gaya ng kahit saan pa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "Ang aming pangunahing layunin ay ipakita kung paano karaniwang iniisip ng mga tao sa machine learning ang chain rule mula sa calculus sa konteksto ng mga network, na may kakaibang pakiramdam sa kung paano lumalapit sa paksa ang karamihan sa mga panimulang kurso sa calculus.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Para sa inyo na hindi komportable sa nauugnay na calculus, mayroon akong isang buong serye sa paksa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Magsimula tayo sa isang napakasimpleng network, kung saan ang bawat layer ay may isang neuron sa loob nito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Ang network na ito ay tinutukoy ng tatlong timbang at tatlong bias, at ang aming layunin ay maunawaan kung gaano kasensitibo ang paggana ng gastos sa mga variable na ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way, we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "Sa ganoong paraan, alam namin kung aling mga pagsasaayos sa mga tuntuning iyon ang magdudulot ng pinakamabisang pagbaba sa function ng gastos.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "And we're just going to focus on the connection between the last two neurons.",
  "translatedText": "At magtutuon lang kami ng pansin sa koneksyon sa pagitan ng huling dalawang neuron.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in, so the activation of the previous neuron is Al-1.",
  "translatedText": "Lagyan natin ng label ang activation ng huling neuron na iyon na may superscript na L, na nagpapahiwatig kung saang layer ito naroroon, kaya ang activation ng nakaraang neuron ay Al-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.98,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Hindi ito mga exponents, isa lang silang paraan ng pag-index ng pinag-uusapan natin, dahil gusto kong mag-save ng mga subscript para sa iba't ibang mga indeks sa susunod.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Sabihin nating ang value na gusto nating maging ang huling activation na ito para sa isang ibinigay na halimbawa ng pagsasanay ay y, halimbawa, ang y ay maaaring 0 o 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is Al-y2.",
  "translatedText": "Kaya ang halaga ng network na ito para sa isang halimbawa ng pagsasanay ay Al-y2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Ipapahiwatig namin ang halaga ng isang halimbawa ng pagsasanay na iyon bilang c0.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neuron's activation plus some bias, which I'll call BL.",
  "translatedText": "Bilang paalala, ang huling pag-activate na ito ay tinutukoy ng isang timbang, na tatawagin kong WL, mga beses sa pag-activate ng nakaraang neuron at ilang bias, na tatawagin kong BL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "At pagkatapos ay i-pump mo iyon sa pamamagitan ng ilang espesyal na nonlinear function tulad ng sigmoid o ReLU.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "Talagang gagawing mas madali para sa amin ang mga bagay kung bibigyan namin ng espesyal na pangalan ang may timbang na kabuuan na ito, tulad ng z, na may parehong superscript sa mga nauugnay na activation.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action and the bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "Ito ay maraming termino, at isang paraan na maaari mong maisip na ang bigat, nakaraang aksyon at ang bias na magkakasama ay ginagamit upang kalkulahin ang z, na nagbibigay-daan sa amin na mag-compute ng a, na sa wakas, kasama ang isang pare-parehong y, ay nagbibigay-daan sa kalkulahin natin ang gastos.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course Al-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "At siyempre ang Al-1 ay naiimpluwensyahan ng sarili nitong timbang at bias at iba pa, ngunit hindi kami magtutuon sa ngayon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Ang lahat ng ito ay mga numero lamang, tama ba?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "At maaaring maganda na isipin na ang bawat isa ay may sariling maliit na linya ng numero.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight WL.",
  "translatedText": "Ang aming unang layunin ay maunawaan kung gaano kasensitibo ang paggana ng gastos sa maliliit na pagbabago sa aming timbang na WL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to WL?",
  "translatedText": "O parirala sa ibang paraan, ano ang derivative ng c na may paggalang sa WL?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del W term, think of it as meaning some tiny nudge to W, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Kapag nakita mo ang terminong del W na ito, isipin na ito ay nangangahulugan ng ilang maliit na siko sa W, tulad ng pagbabago ng 0.01, at isipin ang del c term na ito bilang ibig sabihin anuman ang resultang siko sa gastos.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "Ang gusto natin ay ang ratio nila.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Sa konsepto, ang maliit na siko sa WL na ito ay nagdudulot ng ilang siko sa ZL, na nagiging sanhi naman ng ilang siko sa AL, na direktang nakakaimpluwensya sa gastos.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is, the derivative of ZL with respect to WL.",
  "translatedText": "Kaya't sinisira natin ang mga bagay sa pamamagitan ng unang pagtingin sa ratio ng isang maliit na pagbabago sa ZL sa maliit na pagbabagong ito W, iyon ay, ang hinango ng ZL na may paggalang sa WL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "Gayundin, isasaalang-alang mo ang ratio ng pagbabago sa AL sa maliit na pagbabago sa ZL na nagdulot nito, pati na rin ang ratio sa pagitan ng huling siko sa c at ang intermediate na siko sa AL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of c to small changes in WL.",
  "translatedText": "Dito mismo ang chain rule, kung saan ang pag-multiply ng tatlong ratio na ito ay nagbibigay sa atin ng sensitivity ng c sa maliliit na pagbabago sa WL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "Kaya sa screen ngayon, maraming mga simbolo, at maglaan ng ilang sandali upang matiyak na malinaw kung ano ang lahat ng ito, dahil ngayon ay kukuwentahin natin ang mga nauugnay na derivatives.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "Ang derivative ng c na may kinalaman sa AL ay gumagana na 2AL-y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "Notice this means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Pansinin na ang ibig sabihin nito ay proporsyonal ang laki nito sa pagkakaiba sa pagitan ng output ng network at ng bagay na gusto natin, kaya kung ibang-iba ang output na iyon, kahit na ang kaunting pagbabago ay magkakaroon ng malaking epekto sa panghuling function ng gastos.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "Ang derivative ng AL na may paggalang sa ZL ay ang derivative lang ng aming sigmoid function, o anumang nonlinearity na pipiliin mong gamitin.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "And the derivative of ZL with respect to WL comes out to be AL-1.",
  "translatedText": "At ang derivative ng ZL na may paggalang sa WL ay lumalabas na AL-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all mean.",
  "translatedText": "Ngayon, hindi ko alam ang tungkol sa iyo, ngunit sa palagay ko ay madaling maipit ang ulo sa mga formula nang hindi naglalaan ng ilang sandali upang umupo at paalalahanan ang iyong sarili kung ano ang ibig sabihin ng lahat ng ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "Sa kaso ng huling derivative na ito, ang halaga na naiimpluwensyahan ng maliit na nudge sa bigat sa huling layer ay depende sa kung gaano kalakas ang nakaraang neuron.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "Tandaan, dito pumapasok ang ideya ng neurons-that-fire-together-wire-together.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to WL only of the cost for a specific single training example.",
  "translatedText": "At ang lahat ng ito ay ang hinalaw na may kinalaman sa WL lamang ng gastos para sa isang partikular na halimbawa ng pagsasanay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Dahil ang function ng buong gastos ay nagsasangkot ng pag-average ng lahat ng mga gastos na iyon sa maraming iba't ibang mga halimbawa ng pagsasanay, ang hinango nito ay nangangailangan ng pag-average ng expression na ito sa lahat ng mga halimbawa ng pagsasanay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "And of course, that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "At siyempre, iyon ay isa lamang bahagi ng gradient vector, na mismo ay binuo mula sa mga partial derivatives ng cost function na may kinalaman sa lahat ng mga timbang at bias na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "Ngunit kahit na iyon ay isa lamang sa maraming mga partial derivatives na kailangan natin, ito ay higit sa 50% ng trabaho.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "Ang sensitivity sa bias, halimbawa, ay halos magkapareho.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Kailangan lang nating baguhin ang del z del w term na ito para sa a del z del b.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "At kung titingnan mo ang nauugnay na formula, ang derivative na iyon ay lalabas na 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "Gayundin, at dito pumapasok ang ideya ng pagpapalaganap pabalik, makikita mo kung gaano kasensitibo ang function ng gastos na ito sa pag-activate ng nakaraang layer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight WL.",
  "translatedText": "Ibig sabihin, ang paunang derivative na ito sa expression ng chain rule, ang sensitivity ng z sa nakaraang activation, ay lumalabas na weight WL.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "At muli, kahit na hindi natin direktang maimpluwensyahan ang nakaraang pag-activate ng layer, nakakatulong na subaybayan, dahil maaari na nating ituloy ang parehong ideya ng panuntunan sa chain pabalik upang makita kung gaano kasensitibo ang pag-andar ng gastos sa nakaraang mga timbang at nakaraang mga bias.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "At maaari mong isipin na ito ay isang napakasimpleng halimbawa, dahil ang lahat ng mga layer ay may isang neuron, at ang mga bagay ay magiging mas kumplikado para sa isang tunay na network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "Ngunit sa totoo lang, hindi ganoon karaming pagbabago kapag binibigyan natin ang mga layer ng maraming neuron, talagang ilan pa lang itong mga indeks na dapat subaybayan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "Sa halip na ang pag-activate ng isang partikular na layer ay pagiging AL lang, magkakaroon din ito ng isang subscript na nagsasaad kung aling neuron ng layer na iyon ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Gamitin natin ang letrang k upang i-index ang layer L-1, at j upang i-index ang layer L.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "Para sa gastos, muli naming tinitingnan kung ano ang nais na output, ngunit sa pagkakataong ito ay idinaragdag namin ang mga parisukat ng mga pagkakaiba sa pagitan ng mga huling pag-activate ng layer na ito at ang nais na output.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus Yj squared.",
  "translatedText": "Ibig sabihin, kukuha ka ng kabuuan sa ALj minus Yj squared.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Dahil marami pang timbang, ang bawat isa ay kailangang magkaroon ng ilang higit pang mga indeks upang masubaybayan kung nasaan ito, kaya tawagin natin ang bigat ng gilid na nagkokonekta sa kth neuron na ito sa jth neuron, WLjk.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "Ang mga indeks na iyon ay maaaring medyo pabalik-balik sa simula, ngunit ito ay naaayon sa kung paano mo i-index ang weight matrix na binanggit ko sa bahagi 1 na video.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "Tulad ng dati, maganda pa rin na bigyan ng pangalan ang nauugnay na weighted sum, tulad ng z, upang ang pag-activate ng huling layer ay ang iyong espesyal na function, tulad ng sigmoid, na inilapat sa z.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Maaari mong makita kung ano ang ibig kong sabihin, kung saan ang lahat ng ito ay mahalagang parehong mga equation na mayroon kami dati sa one-neuron-per-layer case, ito ay mukhang medyo mas kumplikado.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain-ruled derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "At sa katunayan, ang derivative expression na pinamumunuan ng chain na naglalarawan kung gaano kasensitibo ang gastos sa isang partikular na timbang ay halos pareho.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Ipaubaya ko sa iyo na i-pause at isipin ang bawat isa sa mga terminong iyon kung gusto mo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "Ano ang nagbabago dito, gayunpaman, ay ang derivative ng gastos na may paggalang sa isa sa mga activation sa layer L-1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "Sa kasong ito, ang pagkakaiba ay ang neuron ay nakakaimpluwensya sa paggana ng gastos sa pamamagitan ng maraming iba't ibang mga landas.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "Ibig sabihin, sa isang banda, nakakaimpluwensya ito sa AL0, na gumaganap ng papel sa cost function, ngunit mayroon din itong impluwensya sa AL1, na gumaganap din sa cost function, at kailangan mong dagdagan ang mga iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "At iyon, well, iyon ay halos ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Kapag nalaman mo na kung gaano kasensitibo ang paggana ng gastos sa mga pag-activate sa pangalawa hanggang sa huling layer na ito, maaari mo lang ulitin ang proseso para sa lahat ng mga timbang at bias na pumapasok sa layer na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "Kaya tapik ka sa likod mo!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Kung ang lahat ng ito ay may katuturan, tumingin ka na ngayon nang malalim sa puso ng backpropagation, ang workhorse sa likod kung paano natututo ang mga neural network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Ang mga chain rule expression na ito ay nagbibigay sa iyo ng mga derivative na tumutukoy sa bawat bahagi sa gradient na tumutulong na mabawasan ang gastos ng network sa pamamagitan ng paulit-ulit na paghakbang pababa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Kung uupo ka at pag-isipan ang lahat ng iyon, ito ay maraming mga layer ng pagiging kumplikado upang ibalot ang iyong isip sa paligid, kaya huwag mag-alala kung kailangan ng oras para sa iyong isip upang matunaw ang lahat ng ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]