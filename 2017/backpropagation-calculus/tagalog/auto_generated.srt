1
00:00:04,019 --> 00:00:06,722
Ang mahirap na palagay dito ay napanood mo na ang bahagi 3, 

2
00:00:06,722 --> 00:00:09,920
na nagbibigay ng intuitive na walkthrough ng backpropagation algorithm.

3
00:00:11,040 --> 00:00:14,220
Dito ay nakakakuha tayo ng kaunti pang pormal at sumisid sa nauugnay na calculus.

4
00:00:14,820 --> 00:00:17,881
Normal lang na medyo nakakalito ito, kaya ang mantra na regular na 

5
00:00:17,881 --> 00:00:21,400
mag-pause at magnilay-nilay ay tiyak na nalalapat dito gaya ng kahit saan pa.

6
00:00:21,940 --> 00:00:24,971
Ang aming pangunahing layunin ay ipakita kung paano karaniwang iniisip 

7
00:00:24,971 --> 00:00:27,790
ng mga tao sa machine learning ang chain rule mula sa calculus sa 

8
00:00:27,790 --> 00:00:30,693
konteksto ng mga network, na may kakaibang pakiramdam sa kung paano 

9
00:00:30,693 --> 00:00:33,640
lumalapit sa paksa ang karamihan sa mga panimulang kurso sa calculus.

10
00:00:34,340 --> 00:00:36,936
Para sa inyo na hindi komportable sa nauugnay na calculus, 

11
00:00:36,936 --> 00:00:38,740
mayroon akong isang buong serye sa paksa.

12
00:00:39,960 --> 00:00:42,678
Magsimula tayo sa isang napakasimpleng network, 

13
00:00:42,678 --> 00:00:46,020
kung saan ang bawat layer ay may isang neuron sa loob nito.

14
00:00:46,320 --> 00:00:49,744
Ang network na ito ay tinutukoy ng tatlong timbang at tatlong bias, 

15
00:00:49,744 --> 00:00:54,074
at ang aming layunin ay maunawaan kung gaano kasensitibo ang paggana ng gastos sa mga 

16
00:00:54,074 --> 00:00:54,880
variable na ito.

17
00:00:55,420 --> 00:00:58,234
Sa ganoong paraan, alam namin kung aling mga pagsasaayos sa mga tuntuning 

18
00:00:58,234 --> 00:01:00,820
iyon ang magdudulot ng pinakamabisang pagbaba sa function ng gastos.

19
00:01:01,960 --> 00:01:04,840
At magtutuon lang kami ng pansin sa koneksyon sa pagitan ng huling dalawang neuron.

20
00:01:05,980 --> 00:01:10,509
Lagyan natin ng label ang activation ng huling neuron na iyon na may superscript na L, 

21
00:01:10,509 --> 00:01:13,060
na nagpapahiwatig kung saang layer ito naroroon, 

22
00:01:13,060 --> 00:01:15,560
kaya ang activation ng nakaraang neuron ay Al-1.

23
00:01:16,360 --> 00:01:19,623
Hindi ito mga exponents, isa lang silang paraan ng pag-index ng pinag-uusapan natin, 

24
00:01:19,623 --> 00:01:23,040
dahil gusto kong mag-save ng mga subscript para sa iba't ibang mga indeks sa susunod.

25
00:01:23,720 --> 00:01:28,000
Sabihin nating ang value na gusto nating maging ang huling activation na ito para sa 

26
00:01:28,000 --> 00:01:32,180
isang ibinigay na halimbawa ng pagsasanay ay y, halimbawa, ang y ay maaaring 0 o 1.

27
00:01:32,840 --> 00:01:39,240
Kaya ang halaga ng network na ito para sa isang halimbawa ng pagsasanay ay Al-y2.

28
00:01:40,260 --> 00:01:44,380
Ipapahiwatig namin ang halaga ng isang halimbawa ng pagsasanay na iyon bilang c0.

29
00:01:45,900 --> 00:01:50,502
Bilang paalala, ang huling pag-activate na ito ay tinutukoy ng isang timbang, 

30
00:01:50,502 --> 00:01:55,400
na tatawagin kong WL, mga beses sa pag-activate ng nakaraang neuron at ilang bias, 

31
00:01:55,400 --> 00:01:56,640
na tatawagin kong BL.

32
00:01:57,420 --> 00:01:59,387
At pagkatapos ay i-pump mo iyon sa pamamagitan ng ilang 

33
00:01:59,387 --> 00:02:01,320
espesyal na nonlinear function tulad ng sigmoid o ReLU.

34
00:02:01,800 --> 00:02:04,367
Talagang gagawing mas madali para sa amin ang mga bagay kung bibigyan 

35
00:02:04,367 --> 00:02:06,752
namin ng espesyal na pangalan ang may timbang na kabuuan na ito, 

36
00:02:06,752 --> 00:02:09,320
tulad ng z, na may parehong superscript sa mga nauugnay na activation.

37
00:02:10,380 --> 00:02:14,217
Ito ay maraming termino, at isang paraan na maaari mong maisip na ang bigat, 

38
00:02:14,217 --> 00:02:18,253
nakaraang aksyon at ang bias na magkakasama ay ginagamit upang kalkulahin ang z, 

39
00:02:18,253 --> 00:02:21,293
na nagbibigay-daan sa amin na mag-compute ng a, na sa wakas, 

40
00:02:21,293 --> 00:02:25,480
kasama ang isang pare-parehong y, ay nagbibigay-daan sa kalkulahin natin ang gastos.

41
00:02:27,340 --> 00:02:32,694
At siyempre ang Al-1 ay naiimpluwensyahan ng sarili nitong timbang at bias at iba pa, 

42
00:02:32,694 --> 00:02:35,060
ngunit hindi kami magtutuon sa ngayon.

43
00:02:35,700 --> 00:02:37,620
Ang lahat ng ito ay mga numero lamang, tama ba?

44
00:02:38,060 --> 00:02:41,040
At maaaring maganda na isipin na ang bawat isa ay may sariling maliit na linya ng numero.

45
00:02:41,720 --> 00:02:45,304
Ang aming unang layunin ay maunawaan kung gaano kasensitibo ang 

46
00:02:45,304 --> 00:02:49,000
paggana ng gastos sa maliliit na pagbabago sa aming timbang na WL.

47
00:02:49,540 --> 00:02:54,860
O parirala sa ibang paraan, ano ang derivative ng c na may paggalang sa WL?

48
00:02:55,600 --> 00:02:59,902
Kapag nakita mo ang terminong del W na ito, isipin na ito ay nangangahulugan 

49
00:02:59,902 --> 00:03:03,143
ng ilang maliit na siko sa W, tulad ng pagbabago ng 0.01, 

50
00:03:03,143 --> 00:03:08,060
at isipin ang del c term na ito bilang ibig sabihin anuman ang resultang siko sa gastos.

51
00:03:08,060 --> 00:03:10,220
Ang gusto natin ay ang ratio nila.

52
00:03:11,260 --> 00:03:16,069
Sa konsepto, ang maliit na siko sa WL na ito ay nagdudulot ng ilang siko sa ZL, 

53
00:03:16,069 --> 00:03:21,240
na nagiging sanhi naman ng ilang siko sa AL, na direktang nakakaimpluwensya sa gastos.

54
00:03:23,120 --> 00:03:26,345
Kaya't sinisira natin ang mga bagay sa pamamagitan ng unang 

55
00:03:26,345 --> 00:03:30,679
pagtingin sa ratio ng isang maliit na pagbabago sa ZL sa maliit na pagbabagong ito W, 

56
00:03:30,679 --> 00:03:33,200
iyon ay, ang hinango ng ZL na may paggalang sa WL.

57
00:03:33,200 --> 00:03:36,881
Gayundin, isasaalang-alang mo ang ratio ng pagbabago sa AL sa 

58
00:03:36,881 --> 00:03:40,800
maliit na pagbabago sa ZL na nagdulot nito, pati na rin ang ratio 

59
00:03:40,800 --> 00:03:44,660
sa pagitan ng huling siko sa c at ang intermediate na siko sa AL.

60
00:03:45,740 --> 00:03:50,346
Dito mismo ang chain rule, kung saan ang pag-multiply ng tatlong ratio na 

61
00:03:50,346 --> 00:03:55,140
ito ay nagbibigay sa atin ng sensitivity ng c sa maliliit na pagbabago sa WL.

62
00:03:56,880 --> 00:04:00,048
Kaya sa screen ngayon, maraming mga simbolo, at maglaan ng ilang 

63
00:04:00,048 --> 00:04:02,973
sandali upang matiyak na malinaw kung ano ang lahat ng ito, 

64
00:04:02,973 --> 00:04:06,240
dahil ngayon ay kukuwentahin natin ang mga nauugnay na derivatives.

65
00:04:07,440 --> 00:04:13,160
Ang derivative ng c na may kinalaman sa AL ay gumagana na 2AL-y.

66
00:04:13,980 --> 00:04:17,163
Pansinin na ang ibig sabihin nito ay proporsyonal ang laki nito sa 

67
00:04:17,163 --> 00:04:20,536
pagkakaiba sa pagitan ng output ng network at ng bagay na gusto natin, 

68
00:04:20,536 --> 00:04:23,956
kaya kung ibang-iba ang output na iyon, kahit na ang kaunting pagbabago 

69
00:04:23,956 --> 00:04:27,140
ay magkakaroon ng malaking epekto sa panghuling function ng gastos.

70
00:04:27,840 --> 00:04:31,951
Ang derivative ng AL na may paggalang sa ZL ay ang derivative lang ng 

71
00:04:31,951 --> 00:04:36,180
aming sigmoid function, o anumang nonlinearity na pipiliin mong gamitin.

72
00:04:37,220 --> 00:04:44,660
At ang derivative ng ZL na may paggalang sa WL ay lumalabas na AL-1.

73
00:04:45,760 --> 00:04:48,257
Ngayon, hindi ko alam ang tungkol sa iyo, ngunit sa palagay ko ay madaling 

74
00:04:48,257 --> 00:04:50,722
maipit ang ulo sa mga formula nang hindi naglalaan ng ilang sandali upang 

75
00:04:50,722 --> 00:04:53,420
umupo at paalalahanan ang iyong sarili kung ano ang ibig sabihin ng lahat ng ito.

76
00:04:53,920 --> 00:04:58,290
Sa kaso ng huling derivative na ito, ang halaga na naiimpluwensyahan ng maliit na 

77
00:04:58,290 --> 00:05:02,820
nudge sa bigat sa huling layer ay depende sa kung gaano kalakas ang nakaraang neuron.

78
00:05:03,380 --> 00:05:08,280
Tandaan, dito pumapasok ang ideya ng neurons-that-fire-together-wire-together.

79
00:05:09,200 --> 00:05:12,537
At ang lahat ng ito ay ang hinalaw na may kinalaman sa WL lamang 

80
00:05:12,537 --> 00:05:15,720
ng gastos para sa isang partikular na halimbawa ng pagsasanay.

81
00:05:16,440 --> 00:05:20,127
Dahil ang function ng buong gastos ay nagsasangkot ng pag-average ng lahat ng mga gastos 

82
00:05:20,127 --> 00:05:22,820
na iyon sa maraming iba't ibang mga halimbawa ng pagsasanay, 

83
00:05:22,820 --> 00:05:26,465
ang hinango nito ay nangangailangan ng pag-average ng expression na ito sa lahat ng mga 

84
00:05:26,465 --> 00:05:27,460
halimbawa ng pagsasanay.

85
00:05:28,380 --> 00:05:31,565
At siyempre, iyon ay isa lamang bahagi ng gradient vector, 

86
00:05:31,565 --> 00:05:34,696
na mismo ay binuo mula sa mga partial derivatives ng cost 

87
00:05:34,696 --> 00:05:38,260
function na may kinalaman sa lahat ng mga timbang at bias na iyon.

88
00:05:40,640 --> 00:05:42,874
Ngunit kahit na iyon ay isa lamang sa maraming mga partial 

89
00:05:42,874 --> 00:05:45,260
derivatives na kailangan natin, ito ay higit sa 50% ng trabaho.

90
00:05:46,340 --> 00:05:49,720
Ang sensitivity sa bias, halimbawa, ay halos magkapareho.

91
00:05:50,040 --> 00:05:55,020
Kailangan lang nating baguhin ang del z del w term na ito para sa a del z del b.

92
00:05:58,420 --> 00:06:02,400
At kung titingnan mo ang nauugnay na formula, ang derivative na iyon ay lalabas na 1.

93
00:06:06,140 --> 00:06:09,921
Gayundin, at dito pumapasok ang ideya ng pagpapalaganap pabalik, 

94
00:06:09,921 --> 00:06:14,634
makikita mo kung gaano kasensitibo ang function ng gastos na ito sa pag-activate 

95
00:06:14,634 --> 00:06:15,740
ng nakaraang layer.

96
00:06:15,740 --> 00:06:20,734
Ibig sabihin, ang paunang derivative na ito sa expression ng chain rule, 

97
00:06:20,734 --> 00:06:25,660
ang sensitivity ng z sa nakaraang activation, ay lumalabas na weight WL.

98
00:06:26,640 --> 00:06:30,270
At muli, kahit na hindi natin direktang maimpluwensyahan ang nakaraang 

99
00:06:30,270 --> 00:06:32,878
pag-activate ng layer, nakakatulong na subaybayan, 

100
00:06:32,878 --> 00:06:36,968
dahil maaari na nating ituloy ang parehong ideya ng panuntunan sa chain pabalik 

101
00:06:36,968 --> 00:06:40,906
upang makita kung gaano kasensitibo ang pag-andar ng gastos sa nakaraang mga 

102
00:06:40,906 --> 00:06:42,440
timbang at nakaraang mga bias.

103
00:06:43,180 --> 00:06:45,834
At maaari mong isipin na ito ay isang napakasimpleng halimbawa, 

104
00:06:45,834 --> 00:06:47,908
dahil ang lahat ng mga layer ay may isang neuron, 

105
00:06:47,908 --> 00:06:51,020
at ang mga bagay ay magiging mas kumplikado para sa isang tunay na network.

106
00:06:51,700 --> 00:06:55,280
Ngunit sa totoo lang, hindi ganoon karaming pagbabago kapag binibigyan natin ang mga 

107
00:06:55,280 --> 00:06:58,860
layer ng maraming neuron, talagang ilan pa lang itong mga indeks na dapat subaybayan.

108
00:06:59,380 --> 00:07:02,995
Sa halip na ang pag-activate ng isang partikular na layer ay pagiging AL lang, 

109
00:07:02,995 --> 00:07:06,976
magkakaroon din ito ng isang subscript na nagsasaad kung aling neuron ng layer na iyon 

110
00:07:06,976 --> 00:07:07,160
ito.

111
00:07:07,160 --> 00:07:14,420
Gamitin natin ang letrang k upang i-index ang layer L-1, at j upang i-index ang layer L.

112
00:07:15,260 --> 00:07:18,248
Para sa gastos, muli naming tinitingnan kung ano ang nais na output, 

113
00:07:18,248 --> 00:07:21,411
ngunit sa pagkakataong ito ay idinaragdag namin ang mga parisukat ng mga 

114
00:07:21,411 --> 00:07:25,180
pagkakaiba sa pagitan ng mga huling pag-activate ng layer na ito at ang nais na output.

115
00:07:26,080 --> 00:07:30,840
Ibig sabihin, kukuha ka ng kabuuan sa ALj minus Yj squared.

116
00:07:33,040 --> 00:07:36,912
Dahil marami pang timbang, ang bawat isa ay kailangang magkaroon ng ilang 

117
00:07:36,912 --> 00:07:39,948
higit pang mga indeks upang masubaybayan kung nasaan ito, 

118
00:07:39,948 --> 00:07:44,658
kaya tawagin natin ang bigat ng gilid na nagkokonekta sa kth neuron na ito sa jth neuron, 

119
00:07:44,658 --> 00:07:44,920
WLjk.

120
00:07:45,620 --> 00:07:48,566
Ang mga indeks na iyon ay maaaring medyo pabalik-balik sa simula, 

121
00:07:48,566 --> 00:07:52,316
ngunit ito ay naaayon sa kung paano mo i-index ang weight matrix na binanggit ko sa 

122
00:07:52,316 --> 00:07:53,120
bahagi 1 na video.

123
00:07:53,620 --> 00:07:57,877
Tulad ng dati, maganda pa rin na bigyan ng pangalan ang nauugnay na weighted sum, 

124
00:07:57,877 --> 00:08:02,342
tulad ng z, upang ang pag-activate ng huling layer ay ang iyong espesyal na function, 

125
00:08:02,342 --> 00:08:04,160
tulad ng sigmoid, na inilapat sa z.

126
00:08:04,660 --> 00:08:06,948
Maaari mong makita kung ano ang ibig kong sabihin, 

127
00:08:06,948 --> 00:08:09,910
kung saan ang lahat ng ito ay mahalagang parehong mga equation na 

128
00:08:09,910 --> 00:08:13,680
mayroon kami dati sa one-neuron-per-layer case, ito ay mukhang medyo mas kumplikado.

129
00:08:15,440 --> 00:08:19,502
At sa katunayan, ang derivative expression na pinamumunuan ng chain na naglalarawan 

130
00:08:19,502 --> 00:08:23,420
kung gaano kasensitibo ang gastos sa isang partikular na timbang ay halos pareho.

131
00:08:23,920 --> 00:08:26,840
Ipaubaya ko sa iyo na i-pause at isipin ang bawat isa sa mga terminong iyon kung gusto mo.

132
00:08:28,980 --> 00:08:32,658
Ano ang nagbabago dito, gayunpaman, ay ang derivative ng 

133
00:08:32,658 --> 00:08:36,659
gastos na may paggalang sa isa sa mga activation sa layer L-1.

134
00:08:37,780 --> 00:08:40,257
Sa kasong ito, ang pagkakaiba ay ang neuron ay nakakaimpluwensya sa 

135
00:08:40,257 --> 00:08:42,880
paggana ng gastos sa pamamagitan ng maraming iba't ibang mga landas.

136
00:08:44,680 --> 00:08:48,187
Ibig sabihin, sa isang banda, nakakaimpluwensya ito sa AL0, 

137
00:08:48,187 --> 00:08:53,155
na gumaganap ng papel sa cost function, ngunit mayroon din itong impluwensya sa AL1, 

138
00:08:53,155 --> 00:08:57,540
na gumaganap din sa cost function, at kailangan mong dagdagan ang mga iyon.

139
00:08:59,820 --> 00:09:03,040
At iyon, well, iyon ay halos ito.

140
00:09:03,500 --> 00:09:06,404
Kapag nalaman mo na kung gaano kasensitibo ang paggana ng gastos sa mga 

141
00:09:06,404 --> 00:09:08,785
pag-activate sa pangalawa hanggang sa huling layer na ito, 

142
00:09:08,785 --> 00:09:11,770
maaari mo lang ulitin ang proseso para sa lahat ng mga timbang at bias na 

143
00:09:11,770 --> 00:09:12,860
pumapasok sa layer na iyon.

144
00:09:13,900 --> 00:09:14,960
Kaya tapik ka sa likod mo!

145
00:09:15,300 --> 00:09:18,924
Kung ang lahat ng ito ay may katuturan, tumingin ka na ngayon nang malalim sa puso 

146
00:09:18,924 --> 00:09:22,680
ng backpropagation, ang workhorse sa likod kung paano natututo ang mga neural network.

147
00:09:23,300 --> 00:09:26,802
Ang mga chain rule expression na ito ay nagbibigay sa iyo ng mga derivative 

148
00:09:26,802 --> 00:09:30,120
na tumutukoy sa bawat bahagi sa gradient na tumutulong na mabawasan ang 

149
00:09:30,120 --> 00:09:33,300
gastos ng network sa pamamagitan ng paulit-ulit na paghakbang pababa.

150
00:09:34,300 --> 00:09:37,064
Kung uupo ka at pag-isipan ang lahat ng iyon, ito ay maraming mga layer ng 

151
00:09:37,064 --> 00:09:39,275
pagiging kumplikado upang ibalot ang iyong isip sa paligid, 

152
00:09:39,275 --> 00:09:42,113
kaya huwag mag-alala kung kailangan ng oras para sa iyong isip upang matunaw 

153
00:09:42,113 --> 00:09:42,740
ang lahat ng ito.

