1
00:00:04,019 --> 00:00:06,925
Η δύσκολη υπόθεση εδώ είναι ότι έχετε παρακολουθήσει το μέρος 3, 

2
00:00:06,925 --> 00:00:09,920
που δίνει μια διαισθητική περιήγηση του αλγορίθμου backpropagation.

3
00:00:11,040 --> 00:00:14,220
Εδώ γινόμαστε λίγο πιο τυπικοί και εμβαθύνουμε στους σχετικούς υπολογισμούς.

4
00:00:14,820 --> 00:00:17,267
Είναι φυσιολογικό αυτό να προκαλεί τουλάχιστον λίγη σύγχυση, 

5
00:00:17,267 --> 00:00:20,557
οπότε η μαντινάδα για τακτική παύση και προβληματισμό ισχύει σίγουρα τόσο εδώ όσο 

6
00:00:20,557 --> 00:00:21,400
και οπουδήποτε αλλού.

7
00:00:21,940 --> 00:00:24,808
Ο κύριος στόχος μας είναι να δείξουμε πώς οι άνθρωποι που ασχολούνται με τη 

8
00:00:24,808 --> 00:00:27,790
μηχανική μάθηση συνήθως σκέφτονται για τον κανόνα της αλυσίδας από τον λογισμό 

9
00:00:27,790 --> 00:00:30,696
στο πλαίσιο των δικτύων, ο οποίος έχει μια διαφορετική αίσθηση από τον τρόπο 

10
00:00:30,696 --> 00:00:33,640
με τον οποίο τα περισσότερα εισαγωγικά μαθήματα λογισμού προσεγγίζουν το θέμα.

11
00:00:34,340 --> 00:00:37,259
Για όσους από εσάς δεν νιώθετε άνετα με τους σχετικούς υπολογισμούς, 

12
00:00:37,259 --> 00:00:38,740
έχω μια ολόκληρη σειρά για το θέμα.

13
00:00:39,960 --> 00:00:46,020
Ας ξεκινήσουμε με ένα εξαιρετικά απλό δίκτυο, όπου κάθε επίπεδο έχει έναν μόνο νευρώνα.

14
00:00:46,320 --> 00:00:50,420
Αυτό το δίκτυο καθορίζεται από τρία βάρη και τρεις προκαταλήψεις και στόχος μας 

15
00:00:50,420 --> 00:00:54,880
είναι να κατανοήσουμε πόσο ευαίσθητη είναι η συνάρτηση κόστους σε αυτές τις μεταβλητές.

16
00:00:55,420 --> 00:00:58,159
Με αυτόν τον τρόπο, γνωρίζουμε ποιες προσαρμογές σε αυτούς τους όρους 

17
00:00:58,159 --> 00:01:00,820
θα προκαλέσουν την πιο αποτελεσματική μείωση της συνάρτησης κόστους.

18
00:01:01,960 --> 00:01:04,840
Και θα επικεντρωθούμε μόνο στη σύνδεση μεταξύ των δύο τελευταίων νευρώνων.

19
00:01:05,980 --> 00:01:10,077
Ας επισημάνουμε την ενεργοποίηση του τελευταίου νευρώνα με έναν δείκτη L, 

20
00:01:10,077 --> 00:01:14,950
υποδεικνύοντας σε ποιο επίπεδο βρίσκεται, οπότε η ενεργοποίηση του προηγούμενου νευρώνα 

21
00:01:14,950 --> 00:01:15,560
είναι Al-1.

22
00:01:16,360 --> 00:01:19,875
Αυτοί δεν είναι εκθέτες, είναι απλώς ένας τρόπος για να δείξουμε για τι μιλάμε, 

23
00:01:19,875 --> 00:01:23,040
αφού θέλω να αποθηκεύσω τους δείκτες για διαφορετικούς δείκτες αργότερα.

24
00:01:23,720 --> 00:01:27,795
Ας πούμε ότι η τιμή που θέλουμε να είναι αυτή η τελευταία ενεργοποίηση για ένα 

25
00:01:27,795 --> 00:01:32,180
δεδομένο παράδειγμα εκπαίδευσης είναι η y, για παράδειγμα, η y μπορεί να είναι 0 ή 1.

26
00:01:32,840 --> 00:01:39,240
Έτσι, το κόστος αυτού του δικτύου για ένα μόνο παράδειγμα εκπαίδευσης είναι Al-y2.

27
00:01:40,260 --> 00:01:44,380
Θα συμβολίσουμε το κόστος αυτού του ενός παραδείγματος εκπαίδευσης ως c0.

28
00:01:45,900 --> 00:01:49,969
Ως υπενθύμιση, αυτή η τελευταία ενεργοποίηση καθορίζεται από ένα βάρος, 

29
00:01:49,969 --> 00:01:53,474
το οποίο θα ονομάσω WL, επί την ενεργοποίηση του προηγούμενου 

30
00:01:53,474 --> 00:01:56,640
νευρώνα συν κάποια προκατάληψη, την οποία θα ονομάσω BL.

31
00:01:57,420 --> 00:01:59,350
Και στη συνέχεια το αντλείτε μέσω κάποιας ειδικής 

32
00:01:59,350 --> 00:02:01,320
μη γραμμικής συνάρτησης όπως η σιγμοειδής ή η ReLU.

33
00:02:01,800 --> 00:02:05,512
Στην πραγματικότητα, θα διευκολυνθούμε αν δώσουμε ένα ειδικό όνομα σε αυτό το 

34
00:02:05,512 --> 00:02:09,320
σταθμισμένο άθροισμα, όπως z, με τον ίδιο δείκτη με τις σχετικές ενεργοποιήσεις.

35
00:02:10,380 --> 00:02:14,166
Πρόκειται για πολλούς όρους και ένας τρόπος που μπορείτε να το αντιληφθείτε είναι 

36
00:02:14,166 --> 00:02:17,953
ότι το βάρος, η προηγούμενη δράση και η προκατάληψη χρησιμοποιούνται όλα μαζί για 

37
00:02:17,953 --> 00:02:21,785
τον υπολογισμό του z, το οποίο με τη σειρά του μας επιτρέπει να υπολογίσουμε το a, 

38
00:02:21,785 --> 00:02:25,480
το οποίο τελικά, μαζί με μια σταθερά y, μας επιτρέπει να υπολογίσουμε το κόστος.

39
00:02:27,340 --> 00:02:30,451
Και φυσικά το Al-1 επηρεάζεται από το δικό του βάρος, 

40
00:02:30,451 --> 00:02:35,060
την προκατάληψη και τα λοιπά, αλλά δεν πρόκειται να επικεντρωθούμε σε αυτό τώρα.

41
00:02:35,700 --> 00:02:37,620
Όλα αυτά είναι απλά αριθμοί, σωστά;

42
00:02:38,060 --> 00:02:39,518
Και μπορεί να είναι ωραίο να σκέφτεσαι ότι το 

43
00:02:39,518 --> 00:02:41,040
καθένα έχει τη δική του μικρή αριθμητική γραμμή.

44
00:02:41,720 --> 00:02:45,360
Ο πρώτος μας στόχος είναι να κατανοήσουμε πόσο ευαίσθητη 

45
00:02:45,360 --> 00:02:49,000
είναι η συνάρτηση κόστους σε μικρές αλλαγές στο βάρος WL.

46
00:02:49,540 --> 00:02:54,860
Ή διαφορετικά, ποια είναι η παράγωγος του c ως προς το WL;

47
00:02:55,600 --> 00:03:00,827
Όταν βλέπετε αυτόν τον όρο del W, σκεφτείτε ότι σημαίνει κάποια μικρή μεταβολή στο W, 

48
00:03:00,827 --> 00:03:04,717
όπως μια αλλαγή κατά 0,01, και σκεφτείτε ότι αυτός ο όρος del c 

49
00:03:04,717 --> 00:03:08,060
σημαίνει όποια είναι η προκύπτουσα μεταβολή στο κόστος.

50
00:03:08,060 --> 00:03:10,220
Αυτό που θέλουμε είναι η αναλογία τους.

51
00:03:11,260 --> 00:03:15,780
Εννοιολογικά, αυτό το μικρό σπρώξιμο στο WL προκαλεί κάποιο σπρώξιμο στο ZL, 

52
00:03:15,780 --> 00:03:19,185
το οποίο με τη σειρά του προκαλεί κάποιο σπρώξιμο στο AL, 

53
00:03:19,185 --> 00:03:21,240
το οποίο επηρεάζει άμεσα το κόστος.

54
00:03:23,120 --> 00:03:28,189
Έτσι, διαχωρίζουμε τα πράγματα εξετάζοντας πρώτα τον λόγο μιας μικροσκοπικής μεταβολής 

55
00:03:28,189 --> 00:03:33,200
του ZL προς αυτή τη μικροσκοπική μεταβολή W, δηλαδή την παράγωγο του ZL ως προς το WL.

56
00:03:33,200 --> 00:03:37,038
Ομοίως, στη συνέχεια εξετάζετε την αναλογία της αλλαγής στην AL προς 

57
00:03:37,038 --> 00:03:40,654
τη μικρή αλλαγή στο ZL που την προκάλεσε, καθώς και την αναλογία 

58
00:03:40,654 --> 00:03:44,660
μεταξύ της τελικής ώθησης στο c και αυτής της ενδιάμεσης ώθησης στην AL.

59
00:03:45,740 --> 00:03:50,345
Αυτός εδώ είναι ο κανόνας της αλυσίδας, όπου πολλαπλασιάζοντας αυτές τις 

60
00:03:50,345 --> 00:03:55,140
τρεις αναλογίες μαζί, προκύπτει η ευαισθησία του c σε μικρές αλλαγές στο WL.

61
00:03:56,880 --> 00:03:59,733
Έτσι, στην οθόνη αυτή τη στιγμή, υπάρχουν πολλά σύμβολα, 

62
00:03:59,733 --> 00:04:03,687
και αφιερώστε λίγο χρόνο για να βεβαιωθείτε ότι είναι σαφές τι είναι όλα αυτά, 

63
00:04:03,687 --> 00:04:06,240
γιατί τώρα θα υπολογίσουμε τις σχετικές παραγώγους.

64
00:04:07,440 --> 00:04:13,160
Η παράγωγος του c ως προς την AL είναι 2AL-y.

65
00:04:13,980 --> 00:04:18,233
Σημειώστε ότι αυτό σημαίνει ότι το μέγεθός του είναι ανάλογο της διαφοράς μεταξύ της 

66
00:04:18,233 --> 00:04:20,785
εξόδου του δικτύου και αυτού που θέλουμε να είναι, 

67
00:04:20,785 --> 00:04:25,138
οπότε αν αυτή η έξοδος ήταν πολύ διαφορετική, ακόμη και μικρές αλλαγές θα έχουν μεγάλο 

68
00:04:25,138 --> 00:04:27,140
αντίκτυπο στην τελική συνάρτηση κόστους.

69
00:04:27,840 --> 00:04:32,889
Η παράγωγος της AL ως προς το ZL είναι απλώς η παράγωγος της σιγμοειδούς συνάρτησης μας, 

70
00:04:32,889 --> 00:04:36,180
ή όποια άλλη μη γραμμικότητα επιλέξετε να χρησιμοποιήσετε.

71
00:04:37,220 --> 00:04:44,660
Και η παράγωγος του ZL ως προς το WL είναι AL-1.

72
00:04:45,760 --> 00:04:48,379
Τώρα δεν ξέρω για εσάς, αλλά νομίζω ότι είναι εύκολο να κολλήσετε 

73
00:04:48,379 --> 00:04:50,959
με το κεφάλι σας στους τύπους χωρίς να αφιερώσετε μια στιγμή για 

74
00:04:50,959 --> 00:04:53,420
να καθίσετε και να θυμηθείτε τι σημαίνουν όλοι αυτοί οι τύποι.

75
00:04:53,920 --> 00:04:56,395
Στην περίπτωση αυτής της τελευταίας παραγώγου, 

76
00:04:56,395 --> 00:05:00,818
το πόσο επηρεάζει το τελευταίο στρώμα η μικρή ώθηση στο βάρος εξαρτάται από το πόσο 

77
00:05:00,818 --> 00:05:02,820
ισχυρός είναι ο προηγούμενος νευρώνας.

78
00:05:03,380 --> 00:05:05,901
Θυμηθείτε, εδώ είναι που έρχεται η ιδέα των νευρώνων 

79
00:05:05,901 --> 00:05:08,280
που πυροδοτούνται μαζί και συνδέονται μεταξύ τους.

80
00:05:09,200 --> 00:05:12,430
Και όλα αυτά είναι η παράγωγος σε σχέση με την WL μόνο 

81
00:05:12,430 --> 00:05:15,720
του κόστους για ένα συγκεκριμένο παράδειγμα εκπαίδευσης.

82
00:05:16,440 --> 00:05:20,210
Δεδομένου ότι η πλήρης συνάρτηση κόστους περιλαμβάνει τον μέσο όρο όλων αυτών 

83
00:05:20,210 --> 00:05:23,061
των δαπανών σε πολλά διαφορετικά παραδείγματα εκπαίδευσης, 

84
00:05:23,061 --> 00:05:26,880
η παράγωγός της απαιτεί τον μέσο όρο αυτής της έκφρασης σε όλα τα παραδείγματα 

85
00:05:26,880 --> 00:05:27,460
εκπαίδευσης.

86
00:05:28,380 --> 00:05:31,639
Και φυσικά, αυτό είναι μόνο ένα στοιχείο του διανύσματος κλίσης, 

87
00:05:31,639 --> 00:05:34,949
το οποίο και το ίδιο δημιουργείται από τις μερικές παραγώγους της 

88
00:05:34,949 --> 00:05:38,260
συνάρτησης κόστους ως προς όλα αυτά τα βάρη και τις προκαταλήψεις.

89
00:05:40,640 --> 00:05:43,945
Αλλά παρόλο που αυτή είναι μόνο μία από τις πολλές μερικές παραγώγους που χρειαζόμαστε, 

90
00:05:43,945 --> 00:05:45,260
είναι πάνω από το 50% της εργασίας.

91
00:05:46,340 --> 00:05:49,720
Η ευαισθησία στη μεροληψία, για παράδειγμα, είναι σχεδόν πανομοιότυπη.

92
00:05:50,040 --> 00:05:55,020
Πρέπει απλώς να αντικαταστήσουμε αυτόν τον όρο del z del w με έναν όρο del z del b.

93
00:05:58,420 --> 00:06:02,400
Και αν κοιτάξετε τον σχετικό τύπο, η παράγωγος αυτή είναι 1.

94
00:06:06,140 --> 00:06:09,980
Επίσης, και σε αυτό το σημείο έρχεται η ιδέα της διάδοσης προς τα πίσω, 

95
00:06:09,980 --> 00:06:14,566
μπορείτε να δείτε πόσο ευαίσθητη είναι αυτή η συνάρτηση κόστους στην ενεργοποίηση του 

96
00:06:14,566 --> 00:06:15,740
προηγούμενου επιπέδου.

97
00:06:15,740 --> 00:06:20,457
Δηλαδή, αυτή η αρχική παράγωγος στην έκφραση του αλυσιδωτού κανόνα, 

98
00:06:20,457 --> 00:06:25,660
η ευαισθησία του z στην προηγούμενη ενεργοποίηση, προκύπτει ως το βάρος WL.

99
00:06:26,640 --> 00:06:30,255
Και πάλι, παρόλο που δεν θα μπορέσουμε να επηρεάσουμε άμεσα την ενεργοποίηση του 

100
00:06:30,255 --> 00:06:32,933
προηγούμενου στρώματος, είναι χρήσιμο να το παρακολουθούμε, 

101
00:06:32,933 --> 00:06:36,950
γιατί τώρα μπορούμε να συνεχίσουμε να επαναλαμβάνουμε την ίδια ιδέα του αλυσιδωτού κανόνα 

102
00:06:36,950 --> 00:06:40,877
προς τα πίσω για να δούμε πόσο ευαίσθητη είναι η συνάρτηση κόστους στα προηγούμενα βάρη 

103
00:06:40,877 --> 00:06:42,440
και τις προηγούμενες προκαταλήψεις.

104
00:06:43,180 --> 00:06:46,146
Και μπορεί να νομίζετε ότι αυτό είναι ένα υπερβολικά απλό παράδειγμα, 

105
00:06:46,146 --> 00:06:48,646
αφού όλα τα επίπεδα έχουν έναν νευρώνα, ενώ τα πράγματα θα 

106
00:06:48,646 --> 00:06:51,020
γίνουν εκθετικά πιο περίπλοκα για ένα πραγματικό δίκτυο.

107
00:06:51,700 --> 00:06:55,410
Αλλά ειλικρινά, δεν αλλάζουν και πολλά όταν δίνουμε στα επίπεδα πολλαπλούς νευρώνες, 

108
00:06:55,410 --> 00:06:58,860
στην πραγματικότητα είναι απλώς μερικοί δείκτες παραπάνω για να παρακολουθούμε.

109
00:06:59,380 --> 00:07:02,625
Αντί η ενεργοποίηση ενός συγκεκριμένου στρώματος να είναι απλώς AL, 

110
00:07:02,625 --> 00:07:06,396
θα έχει επίσης έναν δείκτη που θα υποδεικνύει ποιος νευρώνας του συγκεκριμένου 

111
00:07:06,396 --> 00:07:07,160
στρώματος είναι.

112
00:07:07,160 --> 00:07:10,680
Ας χρησιμοποιήσουμε το γράμμα k για να δείξουμε 

113
00:07:10,680 --> 00:07:14,420
το στρώμα L-1 και το j για να δείξουμε το στρώμα L.

114
00:07:15,260 --> 00:07:18,550
Για το κόστος, εξετάζουμε και πάλι ποια είναι η επιθυμητή έξοδος, 

115
00:07:18,550 --> 00:07:21,889
αλλά αυτή τη φορά προσθέτουμε τα τετράγωνα των διαφορών μεταξύ των 

116
00:07:21,889 --> 00:07:25,180
ενεργοποιήσεων του τελευταίου στρώματος και της επιθυμητής εξόδου.

117
00:07:26,080 --> 00:07:30,840
Δηλαδή, λαμβάνετε ένα άθροισμα επί του ALj μείον το Yj στο τετράγωνο.

118
00:07:33,040 --> 00:07:35,488
Δεδομένου ότι υπάρχουν πολλά περισσότερα βάρη, 

119
00:07:35,488 --> 00:07:39,813
το καθένα πρέπει να έχει μερικούς ακόμη δείκτες για να παρακολουθεί πού βρίσκεται, 

120
00:07:39,813 --> 00:07:43,825
οπότε ας ονομάσουμε το βάρος της ακμής που συνδέει τον k-οστό νευρώνα με τον 

121
00:07:43,825 --> 00:07:44,920
j-οστό νευρώνα, WLjk.

122
00:07:45,620 --> 00:07:48,174
Αυτοί οι δείκτες μπορεί να φαίνονται λίγο ανάποδοι στην αρχή, 

123
00:07:48,174 --> 00:07:50,771
αλλά ταιριάζουν με τον τρόπο που θα αναπροσαρμόζατε τον πίνακα 

124
00:07:50,771 --> 00:07:53,120
βάρους για τον οποίο μίλησα στο βίντεο του πρώτου μέρους.

125
00:07:53,620 --> 00:07:57,452
Όπως και πριν, είναι καλό να δώσετε ένα όνομα στο σχετικό σταθμισμένο άθροισμα, 

126
00:07:57,452 --> 00:08:01,045
όπως το z, έτσι ώστε η ενεργοποίηση του τελευταίου επιπέδου να είναι απλώς 

127
00:08:01,045 --> 00:08:04,160
η ειδική σας συνάρτηση, όπως η σιγμοειδής, που εφαρμόζεται στο z.

128
00:08:04,660 --> 00:08:07,699
Μπορείτε να δείτε τι εννοώ, όπου όλες αυτές είναι ουσιαστικά 

129
00:08:07,699 --> 00:08:12,035
οι ίδιες εξισώσεις που είχαμε προηγουμένως στην περίπτωση του ενός νευρώνα ανά στρώμα, 

130
00:08:12,035 --> 00:08:13,680
απλά φαίνεται λίγο πιο περίπλοκο.

131
00:08:15,440 --> 00:08:19,571
Και πράγματι, η αλυσιδωτή παράγωγη έκφραση που περιγράφει πόσο ευαίσθητο 

132
00:08:19,571 --> 00:08:23,420
είναι το κόστος σε ένα συγκεκριμένο βάρος μοιάζει ουσιαστικά η ίδια.

133
00:08:23,920 --> 00:08:26,538
Θα σας αφήσω να κάνετε μια παύση και να σκεφτείτε για κάθε έναν από αυτούς τους όρους, 

134
00:08:26,538 --> 00:08:26,840
αν θέλετε.

135
00:08:28,980 --> 00:08:32,921
Αυτό που αλλάζει εδώ, όμως, είναι η παράγωγος του κόστους 

136
00:08:32,921 --> 00:08:36,659
σε σχέση με μία από τις ενεργοποιήσεις στο επίπεδο L-1.

137
00:08:37,780 --> 00:08:40,393
Στην περίπτωση αυτή, η διαφορά είναι ότι ο νευρώνας επηρεάζει 

138
00:08:40,393 --> 00:08:42,880
τη συνάρτηση κόστους μέσω πολλαπλών διαφορετικών διαδρομών.

139
00:08:44,680 --> 00:08:50,318
Δηλαδή, από τη μία πλευρά, επηρεάζει την AL0, η οποία παίζει ρόλο στη συνάρτηση κόστους, 

140
00:08:50,318 --> 00:08:55,449
αλλά επηρεάζει επίσης την AL1, η οποία επίσης παίζει ρόλο στη συνάρτηση κόστους, 

141
00:08:55,449 --> 00:08:57,540
και πρέπει να τα προσθέσετε αυτά.

142
00:08:59,820 --> 00:09:03,040
Και αυτό, λοιπόν, είναι λίγο πολύ αυτό.

143
00:09:03,500 --> 00:09:06,721
Μόλις μάθετε πόσο ευαίσθητη είναι η συνάρτηση κόστους στις ενεργοποιήσεις 

144
00:09:06,721 --> 00:09:09,812
σε αυτό το προτελευταίο επίπεδο, μπορείτε να επαναλάβετε τη διαδικασία 

145
00:09:09,812 --> 00:09:12,860
για όλα τα βάρη και τις προκαταλήψεις που τροφοδοτούν αυτό το επίπεδο.

146
00:09:13,900 --> 00:09:14,960
Χτυπήστε λοιπόν τον εαυτό σας στην πλάτη!

147
00:09:15,300 --> 00:09:18,673
Αν όλα αυτά βγάζουν νόημα, τότε έχετε πλέον κοιτάξει βαθιά μέσα στην καρδιά της 

148
00:09:18,673 --> 00:09:22,258
οπισθοδιάδοσης, το άρμα εργασίας πίσω από τον τρόπο με τον οποίο τα νευρωνικά δίκτυα 

149
00:09:22,258 --> 00:09:22,680
μαθαίνουν.

150
00:09:23,300 --> 00:09:26,569
Αυτές οι εκφράσεις των αλυσιδωτών κανόνων σας δίνουν τις παραγώγους 

151
00:09:26,569 --> 00:09:30,175
που καθορίζουν κάθε συνιστώσα της κλίσης που συμβάλλει στην ελαχιστοποίηση 

152
00:09:30,175 --> 00:09:33,300
του κόστους του δικτύου με το επαναλαμβανόμενο βήμα προς τα κάτω.

153
00:09:34,300 --> 00:09:37,058
Αν καθίσετε και τα σκεφτείτε όλα αυτά, πρόκειται για πολλά επίπεδα 

154
00:09:37,058 --> 00:09:39,405
πολυπλοκότητας που πρέπει να περιτυλίξετε στο μυαλό σας, 

155
00:09:39,405 --> 00:09:42,740
οπότε μην ανησυχείτε αν το μυαλό σας χρειάζεται χρόνο για να τα χωνέψει όλα αυτά.

