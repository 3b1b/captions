[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "Η δύσκολη υπόθεση εδώ είναι ότι έχετε παρακολουθήσει το μέρος 3, που δίνει μια διαισθητική περιήγηση του αλγορίθμου backpropagation.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Εδώ γινόμαστε λίγο πιο τυπικοί και εμβαθύνουμε στους σχετικούς υπολογισμούς.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "Είναι φυσιολογικό αυτό να προκαλεί τουλάχιστον λίγη σύγχυση, οπότε η μαντινάδα για τακτική παύση και προβληματισμό ισχύει σίγουρα τόσο εδώ όσο και οπουδήποτε αλλού.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "Ο κύριος στόχος μας είναι να δείξουμε πώς οι άνθρωποι που ασχολούνται με τη μηχανική μάθηση συνήθως σκέφτονται για τον κανόνα της αλυσίδας από τον λογισμό στο πλαίσιο των δικτύων, ο οποίος έχει μια διαφορετική αίσθηση από τον τρόπο με τον οποίο τα περισσότερα εισαγωγικά μαθήματα λογισμού προσεγγίζουν το θέμα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Για όσους από εσάς δεν νιώθετε άνετα με τους σχετικούς υπολογισμούς, έχω μια ολόκληρη σειρά για το θέμα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Ας ξεκινήσουμε με ένα εξαιρετικά απλό δίκτυο, όπου κάθε επίπεδο έχει έναν μόνο νευρώνα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Αυτό το δίκτυο καθορίζεται από τρία βάρη και τρεις προκαταλήψεις και στόχος μας είναι να κατανοήσουμε πόσο ευαίσθητη είναι η συνάρτηση κόστους σε αυτές τις μεταβλητές.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way, we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "Με αυτόν τον τρόπο, γνωρίζουμε ποιες προσαρμογές σε αυτούς τους όρους θα προκαλέσουν την πιο αποτελεσματική μείωση της συνάρτησης κόστους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "And we're just going to focus on the connection between the last two neurons.",
  "translatedText": "Και θα επικεντρωθούμε μόνο στη σύνδεση μεταξύ των δύο τελευταίων νευρώνων.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in, so the activation of the previous neuron is Al-1.",
  "translatedText": "Ας επισημάνουμε την ενεργοποίηση του τελευταίου νευρώνα με έναν δείκτη L, υποδεικνύοντας σε ποιο επίπεδο βρίσκεται, οπότε η ενεργοποίηση του προηγούμενου νευρώνα είναι Al-1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 65.98,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Αυτοί δεν είναι εκθέτες, είναι απλώς ένας τρόπος για να δείξουμε για τι μιλάμε, αφού θέλω να αποθηκεύσω τους δείκτες για διαφορετικούς δείκτες αργότερα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Ας πούμε ότι η τιμή που θέλουμε να είναι αυτή η τελευταία ενεργοποίηση για ένα δεδομένο παράδειγμα εκπαίδευσης είναι η y, για παράδειγμα, η y μπορεί να είναι 0 ή 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is Al-y2.",
  "translatedText": "Έτσι, το κόστος αυτού του δικτύου για ένα μόνο παράδειγμα εκπαίδευσης είναι Al-y2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Θα συμβολίσουμε το κόστος αυτού του ενός παραδείγματος εκπαίδευσης ως c0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neuron's activation plus some bias, which I'll call BL.",
  "translatedText": "Ως υπενθύμιση, αυτή η τελευταία ενεργοποίηση καθορίζεται από ένα βάρος, το οποίο θα ονομάσω WL, επί την ενεργοποίηση του προηγούμενου νευρώνα συν κάποια προκατάληψη, την οποία θα ονομάσω BL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "Και στη συνέχεια το αντλείτε μέσω κάποιας ειδικής μη γραμμικής συνάρτησης όπως η σιγμοειδής ή η ReLU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "Στην πραγματικότητα, θα διευκολυνθούμε αν δώσουμε ένα ειδικό όνομα σε αυτό το σταθμισμένο άθροισμα, όπως z, με τον ίδιο δείκτη με τις σχετικές ενεργοποιήσεις.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action and the bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "Πρόκειται για πολλούς όρους και ένας τρόπος που μπορείτε να το αντιληφθείτε είναι ότι το βάρος, η προηγούμενη δράση και η προκατάληψη χρησιμοποιούνται όλα μαζί για τον υπολογισμό του z, το οποίο με τη σειρά του μας επιτρέπει να υπολογίσουμε το a, το οποίο τελικά, μαζί με μια σταθερά y, μας επιτρέπει να υπολογίσουμε το κόστος.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course Al-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "Και φυσικά το Al-1 επηρεάζεται από το δικό του βάρος, την προκατάληψη και τα λοιπά, αλλά δεν πρόκειται να επικεντρωθούμε σε αυτό τώρα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Όλα αυτά είναι απλά αριθμοί, σωστά;",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "Και μπορεί να είναι ωραίο να σκέφτεσαι ότι το καθένα έχει τη δική του μικρή αριθμητική γραμμή.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight WL.",
  "translatedText": "Ο πρώτος μας στόχος είναι να κατανοήσουμε πόσο ευαίσθητη είναι η συνάρτηση κόστους σε μικρές αλλαγές στο βάρος WL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to WL?",
  "translatedText": "Ή διαφορετικά, ποια είναι η παράγωγος του c ως προς το WL;",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del W term, think of it as meaning some tiny nudge to W, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Όταν βλέπετε αυτόν τον όρο del W, σκεφτείτε ότι σημαίνει κάποια μικρή μεταβολή στο W, όπως μια αλλαγή κατά 0,01, και σκεφτείτε ότι αυτός ο όρος del c σημαίνει όποια είναι η προκύπτουσα μεταβολή στο κόστος.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "Αυτό που θέλουμε είναι η αναλογία τους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Εννοιολογικά, αυτό το μικρό σπρώξιμο στο WL προκαλεί κάποιο σπρώξιμο στο ZL, το οποίο με τη σειρά του προκαλεί κάποιο σπρώξιμο στο AL, το οποίο επηρεάζει άμεσα το κόστος.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is, the derivative of ZL with respect to WL.",
  "translatedText": "Έτσι, διαχωρίζουμε τα πράγματα εξετάζοντας πρώτα τον λόγο μιας μικροσκοπικής μεταβολής του ZL προς αυτή τη μικροσκοπική μεταβολή W, δηλαδή την παράγωγο του ZL ως προς το WL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "Ομοίως, στη συνέχεια εξετάζετε την αναλογία της αλλαγής στην AL προς τη μικρή αλλαγή στο ZL που την προκάλεσε, καθώς και την αναλογία μεταξύ της τελικής ώθησης στο c και αυτής της ενδιάμεσης ώθησης στην AL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of c to small changes in WL.",
  "translatedText": "Αυτός εδώ είναι ο κανόνας της αλυσίδας, όπου πολλαπλασιάζοντας αυτές τις τρεις αναλογίες μαζί, προκύπτει η ευαισθησία του c σε μικρές αλλαγές στο WL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "Έτσι, στην οθόνη αυτή τη στιγμή, υπάρχουν πολλά σύμβολα, και αφιερώστε λίγο χρόνο για να βεβαιωθείτε ότι είναι σαφές τι είναι όλα αυτά, γιατί τώρα θα υπολογίσουμε τις σχετικές παραγώγους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "Η παράγωγος του c ως προς την AL είναι 2AL-y.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "Notice this means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Σημειώστε ότι αυτό σημαίνει ότι το μέγεθός του είναι ανάλογο της διαφοράς μεταξύ της εξόδου του δικτύου και αυτού που θέλουμε να είναι, οπότε αν αυτή η έξοδος ήταν πολύ διαφορετική, ακόμη και μικρές αλλαγές θα έχουν μεγάλο αντίκτυπο στην τελική συνάρτηση κόστους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "Η παράγωγος της AL ως προς το ZL είναι απλώς η παράγωγος της σιγμοειδούς συνάρτησης μας, ή όποια άλλη μη γραμμικότητα επιλέξετε να χρησιμοποιήσετε.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "And the derivative of ZL with respect to WL comes out to be AL-1.",
  "translatedText": "Και η παράγωγος του ZL ως προς το WL είναι AL-1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all mean.",
  "translatedText": "Τώρα δεν ξέρω για εσάς, αλλά νομίζω ότι είναι εύκολο να κολλήσετε με το κεφάλι σας στους τύπους χωρίς να αφιερώσετε μια στιγμή για να καθίσετε και να θυμηθείτε τι σημαίνουν όλοι αυτοί οι τύποι.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "Στην περίπτωση αυτής της τελευταίας παραγώγου, το πόσο επηρεάζει το τελευταίο στρώμα η μικρή ώθηση στο βάρος εξαρτάται από το πόσο ισχυρός είναι ο προηγούμενος νευρώνας.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "Θυμηθείτε, εδώ είναι που έρχεται η ιδέα των νευρώνων που πυροδοτούνται μαζί και συνδέονται μεταξύ τους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to WL only of the cost for a specific single training example.",
  "translatedText": "Και όλα αυτά είναι η παράγωγος σε σχέση με την WL μόνο του κόστους για ένα συγκεκριμένο παράδειγμα εκπαίδευσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Δεδομένου ότι η πλήρης συνάρτηση κόστους περιλαμβάνει τον μέσο όρο όλων αυτών των δαπανών σε πολλά διαφορετικά παραδείγματα εκπαίδευσης, η παράγωγός της απαιτεί τον μέσο όρο αυτής της έκφρασης σε όλα τα παραδείγματα εκπαίδευσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "And of course, that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "Και φυσικά, αυτό είναι μόνο ένα στοιχείο του διανύσματος κλίσης, το οποίο και το ίδιο δημιουργείται από τις μερικές παραγώγους της συνάρτησης κόστους ως προς όλα αυτά τα βάρη και τις προκαταλήψεις.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "Αλλά παρόλο που αυτή είναι μόνο μία από τις πολλές μερικές παραγώγους που χρειαζόμαστε, είναι πάνω από το 50% της εργασίας.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "Η ευαισθησία στη μεροληψία, για παράδειγμα, είναι σχεδόν πανομοιότυπη.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Πρέπει απλώς να αντικαταστήσουμε αυτόν τον όρο del z del w με έναν όρο del z del b.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "Και αν κοιτάξετε τον σχετικό τύπο, η παράγωγος αυτή είναι 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "Επίσης, και σε αυτό το σημείο έρχεται η ιδέα της διάδοσης προς τα πίσω, μπορείτε να δείτε πόσο ευαίσθητη είναι αυτή η συνάρτηση κόστους στην ενεργοποίηση του προηγούμενου επιπέδου.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight WL.",
  "translatedText": "Δηλαδή, αυτή η αρχική παράγωγος στην έκφραση του αλυσιδωτού κανόνα, η ευαισθησία του z στην προηγούμενη ενεργοποίηση, προκύπτει ως το βάρος WL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "Και πάλι, παρόλο που δεν θα μπορέσουμε να επηρεάσουμε άμεσα την ενεργοποίηση του προηγούμενου στρώματος, είναι χρήσιμο να το παρακολουθούμε, γιατί τώρα μπορούμε να συνεχίσουμε να επαναλαμβάνουμε την ίδια ιδέα του αλυσιδωτού κανόνα προς τα πίσω για να δούμε πόσο ευαίσθητη είναι η συνάρτηση κόστους στα προηγούμενα βάρη και τις προηγούμενες προκαταλήψεις.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "Και μπορεί να νομίζετε ότι αυτό είναι ένα υπερβολικά απλό παράδειγμα, αφού όλα τα επίπεδα έχουν έναν νευρώνα, ενώ τα πράγματα θα γίνουν εκθετικά πιο περίπλοκα για ένα πραγματικό δίκτυο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "Αλλά ειλικρινά, δεν αλλάζουν και πολλά όταν δίνουμε στα επίπεδα πολλαπλούς νευρώνες, στην πραγματικότητα είναι απλώς μερικοί δείκτες παραπάνω για να παρακολουθούμε.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "Αντί η ενεργοποίηση ενός συγκεκριμένου στρώματος να είναι απλώς AL, θα έχει επίσης έναν δείκτη που θα υποδεικνύει ποιος νευρώνας του συγκεκριμένου στρώματος είναι.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Ας χρησιμοποιήσουμε το γράμμα k για να δείξουμε το στρώμα L-1 και το j για να δείξουμε το στρώμα L.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "Για το κόστος, εξετάζουμε και πάλι ποια είναι η επιθυμητή έξοδος, αλλά αυτή τη φορά προσθέτουμε τα τετράγωνα των διαφορών μεταξύ των ενεργοποιήσεων του τελευταίου στρώματος και της επιθυμητής εξόδου.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus Yj squared.",
  "translatedText": "Δηλαδή, λαμβάνετε ένα άθροισμα επί του ALj μείον το Yj στο τετράγωνο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Δεδομένου ότι υπάρχουν πολλά περισσότερα βάρη, το καθένα πρέπει να έχει μερικούς ακόμη δείκτες για να παρακολουθεί πού βρίσκεται, οπότε ας ονομάσουμε το βάρος της ακμής που συνδέει τον k-οστό νευρώνα με τον j-οστό νευρώνα, WLjk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "Αυτοί οι δείκτες μπορεί να φαίνονται λίγο ανάποδοι στην αρχή, αλλά ταιριάζουν με τον τρόπο που θα αναπροσαρμόζατε τον πίνακα βάρους για τον οποίο μίλησα στο βίντεο του πρώτου μέρους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "Όπως και πριν, είναι καλό να δώσετε ένα όνομα στο σχετικό σταθμισμένο άθροισμα, όπως το z, έτσι ώστε η ενεργοποίηση του τελευταίου επιπέδου να είναι απλώς η ειδική σας συνάρτηση, όπως η σιγμοειδής, που εφαρμόζεται στο z.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Μπορείτε να δείτε τι εννοώ, όπου όλες αυτές είναι ουσιαστικά οι ίδιες εξισώσεις που είχαμε προηγουμένως στην περίπτωση του ενός νευρώνα ανά στρώμα, απλά φαίνεται λίγο πιο περίπλοκο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain-ruled derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "Και πράγματι, η αλυσιδωτή παράγωγη έκφραση που περιγράφει πόσο ευαίσθητο είναι το κόστος σε ένα συγκεκριμένο βάρος μοιάζει ουσιαστικά η ίδια.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Θα σας αφήσω να κάνετε μια παύση και να σκεφτείτε για κάθε έναν από αυτούς τους όρους, αν θέλετε.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "Αυτό που αλλάζει εδώ, όμως, είναι η παράγωγος του κόστους σε σχέση με μία από τις ενεργοποιήσεις στο επίπεδο L-1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "Στην περίπτωση αυτή, η διαφορά είναι ότι ο νευρώνας επηρεάζει τη συνάρτηση κόστους μέσω πολλαπλών διαφορετικών διαδρομών.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "Δηλαδή, από τη μία πλευρά, επηρεάζει την AL0, η οποία παίζει ρόλο στη συνάρτηση κόστους, αλλά επηρεάζει επίσης την AL1, η οποία επίσης παίζει ρόλο στη συνάρτηση κόστους, και πρέπει να τα προσθέσετε αυτά.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "Και αυτό, λοιπόν, είναι λίγο πολύ αυτό.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Μόλις μάθετε πόσο ευαίσθητη είναι η συνάρτηση κόστους στις ενεργοποιήσεις σε αυτό το προτελευταίο επίπεδο, μπορείτε να επαναλάβετε τη διαδικασία για όλα τα βάρη και τις προκαταλήψεις που τροφοδοτούν αυτό το επίπεδο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "Χτυπήστε λοιπόν τον εαυτό σας στην πλάτη!",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Αν όλα αυτά βγάζουν νόημα, τότε έχετε πλέον κοιτάξει βαθιά μέσα στην καρδιά της οπισθοδιάδοσης, το άρμα εργασίας πίσω από τον τρόπο με τον οποίο τα νευρωνικά δίκτυα μαθαίνουν.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Αυτές οι εκφράσεις των αλυσιδωτών κανόνων σας δίνουν τις παραγώγους που καθορίζουν κάθε συνιστώσα της κλίσης που συμβάλλει στην ελαχιστοποίηση του κόστους του δικτύου με το επαναλαμβανόμενο βήμα προς τα κάτω.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Αν καθίσετε και τα σκεφτείτε όλα αυτά, πρόκειται για πολλά επίπεδα πολυπλοκότητας που πρέπει να περιτυλίξετε στο μυαλό σας, οπότε μην ανησυχείτε αν το μυαλό σας χρειάζεται χρόνο για να τα χωνέψει όλα αυτά.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]