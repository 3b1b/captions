[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "Важке припущення полягає в тому, що ви переглянули частину 3, яка дає інтуїтивно зрозумілу інструкцію з алгоритму зворотного поширення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "Тут ми станемо більш формальними та зануримося у відповідне обчислення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "Це нормально, що це принаймні трохи заплутає, тому мантра регулярно зупинятися та розмірковувати, безумовно, застосовна тут так само, як і будь-де.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "Наша головна мета — показати, як люди, які займаються машинним навчанням, зазвичай думають про правило ланцюга з обчислення в контексті мереж, яке має інше відчуття від того, як більшість початкових курсів обчислення підходять до цього предмета.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "Для тих із вас, кому незручно відповідне обчислення, у мене є ціла серія на цю тему.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "Давайте почнемо з надзвичайно простої мережі, де кожен рівень містить один нейрон.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "Ця мережа визначається трьома вагами та трьома зміщеннями, і наша мета — зрозуміти, наскільки функція витрат чутлива до цих змінних.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way, we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "Таким чином, ми знаємо, які коригування цих умов призведуть до найефективнішого зниження функції витрат.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "And we're just going to focus on the connection between the last two neurons.",
  "translatedText": "Ми просто зосередимося на зв’язку між двома останніми нейронами.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in, so the activation of the previous neuron is Al-1.",
  "translatedText": "Позначимо активацію останнього нейрона надрядковою літерою L, яка вказує, в якому шарі він знаходиться, тому активація попереднього нейрона - Al-1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 65.98,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "Це не експоненти, це лише спосіб індексування того, про що ми говоримо, оскільки пізніше я хочу зберегти індекси для різних індексів.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "Припустімо, що значення, яке ми хочемо мати для цієї останньої активації для даного прикладу навчання, дорівнює y, наприклад, y може бути 0 або 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is Al-y2.",
  "translatedText": "Отже, вартість цієї мережі для одного навчального прикладу становить Al-y2.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "Ми позначимо вартість одного навчального прикладу як c0.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neuron's activation plus some bias, which I'll call BL.",
  "translatedText": "Нагадаю, що ця остання активація визначається вагою, яку я назву WL, помноженою на активацію попереднього нейрона плюс деяке зміщення, яке я назву BL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "А потім ви прокачуєте це через спеціальну нелінійну функцію, таку як сигмоїд або ReLU.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "Насправді нам стане простіше, якщо ми дамо спеціальну назву цій зваженій сумі, як-от z, з тим самим верхнім індексом, що й відповідні активації.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action and the bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "Це дуже багато термінів, і ви можете уявити собі, що вага, попередня дія і зміщення разом використовуються для обчислення z, яка, в свою чергу, дозволяє нам обчислити a, яка, нарешті, разом з константою y, дозволяє нам обчислити вартість.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course Al-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "Звичайно, на Al-1 впливає його власна вага, упередженість тощо, але зараз ми не будемо на цьому зосереджуватися.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "Усе це лише цифри, чи не так?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "І може бути приємно уявити, що кожна з них має власну маленьку числову лінію.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight WL.",
  "translatedText": "Наша перша мета - зрозуміти, наскільки чутливою є функція витрат до невеликих змін нашої ваги WL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to WL?",
  "translatedText": "Або інакше кажучи, чому дорівнює похідна від c по відношенню до WL?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del W term, think of it as meaning some tiny nudge to W, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "Коли ви бачите цей член del W, думайте, що він означає якийсь крихітний поштовх до W, наприклад, зміну на 0,01, і думайте, що цей член del c означає будь-який результуючий поштовх до собівартості.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "Ми хочемо їх співвідношення.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "Концептуально, цей крихітний поштовх до WL спричиняє певний поштовх до ZL, який, в свою чергу, спричиняє певний поштовх до AL, що безпосередньо впливає на вартість.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is, the derivative of ZL with respect to WL.",
  "translatedText": "Отже, ми розбиваємо речі на частини, спочатку розглядаючи відношення крихітної зміни ZL до цієї крихітної зміни W, тобто похідну від ZL по відношенню до WL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "Аналогічно, ви розглядаєте відношення зміни до AL до крихітної зміни в ZL, яка її спричинила, а також відношення між остаточним поштовхом до c і цим проміжним поштовхом до AL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of c to small changes in WL.",
  "translatedText": "Це ланцюгове правило, де множення цих трьох співвідношень дає нам чутливість c до невеликих змін wL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "Тож зараз на екрані є багато символів, і знайдіть хвилинку, щоб переконатися, що вони всі зрозумілі, тому що зараз ми збираємося обчислити відповідні похідні.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "Похідна від c відносно AL виявляється 2AL-y.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "Notice this means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "Зауважте, що це означає, що його розмір пропорційний різниці між випуском мережі та тим, що ми хочемо отримати, тому, якщо цей випуск сильно відрізняється, навіть незначні зміни матимуть великий вплив на кінцеву функцію витрат.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "Похідна AL по відношенню до ZL - це просто похідна нашої сигмоїдної функції, або будь-якої іншої нелінійності, яку ви вирішите використати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "And the derivative of ZL with respect to WL comes out to be AL-1.",
  "translatedText": "А похідна від ZL по відношенню до WL дорівнює AL-1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all mean.",
  "translatedText": "Не знаю, як вам, а мені здається, що дуже легко застрягти з головою у формулах, не знайшовши часу, щоб присісти і нагадати собі, що всі вони означають.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "У випадку цієї останньої похідної величина впливу невеликого поштовху на вагу на останній шар залежить від того, наскільки сильним є попередній нейрон.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "Пам’ятайте, що саме тут з’являється ідея нейронів, які спрацьовують разом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to WL only of the cost for a specific single training example.",
  "translatedText": "І все це є похідною по відношенню до WL лише від вартості конкретного окремого навчального прикладу.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "Оскільки функція повних витрат передбачає усереднення всіх цих витрат у багатьох різних прикладах навчання, її похідна вимагає усереднення цього виразу для всіх прикладів навчання.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "And of course, that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "Звичайно, це лише один компонент вектора градієнта, який складається з часткових похідних функції вартості щодо всіх цих ваг і зміщень.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "Але хоча це лише одна з багатьох частинних похідних, які нам потрібні, це понад 50% роботи.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "Чутливість до зсуву, наприклад, практично однакова.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "Нам просто потрібно змінити цей термін del z del w на a del z del b.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "І якщо ви подивитеся на відповідну формулу, ця похідна виявиться рівною 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "Крім того, і саме тут виникає ідея поширення назад, ви можете побачити, наскільки ця функція вартості чутлива до активації попереднього рівня.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight WL.",
  "translatedText": "А саме, ця початкова похідна у виразі ланцюгового правила, чутливість z до попередньої активації, виявляється вагою WL.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "І знову ж таки, хоча ми не зможемо напряму вплинути на активацію попереднього рівня, це корисно відслідковувати, тому що тепер ми можемо просто продовжувати повторювати ту саму ідею правила ланцюга назад, щоб побачити, наскільки чутлива функція витрат до попередні ваги та попередні упередження.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "І ви можете подумати, що це надто простий приклад, оскільки всі рівні мають один нейрон, а для реальної мережі все стане експоненціально складнішим.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "Але, чесно кажучи, не так багато змін, коли ми надаємо шарам кілька нейронів, насправді це лише кілька індексів, які потрібно відстежувати.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "Замість того, щоб активація даного шару була просто AL, він також матиме індекс, який вказує, який нейрон цього шару це.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "Використовуємо букву k для індексування шару L-1, а j для індексування шару L.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "Що стосується вартості, ми знову дивимося на бажаний результат, але цього разу ми складаємо квадрати різниць між цими останніми активаціями шару та бажаним результатом.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus Yj squared.",
  "translatedText": "Тобто, ви берете суму над ALj мінус Yj в квадраті.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "Оскільки ваг набагато більше, кожен з них повинен мати ще пару індексів, щоб відстежувати, де він знаходиться, тому давайте назвемо вагу ребра, що з’єднує цей k-й нейрон із j-м нейроном, WLjk.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "Спочатку ці індекси можуть здатися трохи зворотними, але вони узгоджуються з тим, як ви індексуєте вагову матрицю, про яку я говорив у першій частині відео.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "Як і раніше, доцільно дати ім’я відповідній зваженій сумі, як-от z, щоб активація останнього шару була просто вашою спеціальною функцією, як-от сигмоід, застосована до z.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "Ви розумієте, що я маю на увазі, де всі ці рівняння, по суті, ті самі рівняння, які ми мали раніше у випадку одного нейрона на шар, просто це виглядає трохи складніше.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain-ruled derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "І справді, ланцюговий похідний вираз, що описує, наскільки чутливою є вартість до питомої ваги, виглядає практично так само.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "Я залишу вам зробити паузу та подумати над кожним із цих термінів, якщо хочете.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "Що тут змінюється, так це похідна вартості відносно однієї з активацій на рівні L-1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "У цьому випадку різниця полягає в тому, що нейрон впливає на функцію витрат кількома різними шляхами.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "Тобто, з одного боку, це впливає на AL0, який відіграє певну роль у функції витрат, але він також впливає на AL1, який також відіграє роль у функції витрат, і ви повинні додати їх.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "І це, ну, це майже все.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "Коли ви дізнаєтеся, наскільки функція вартості чутлива до активацій на цьому передостанньому шарі, ви можете просто повторити процес для всіх ваг і зміщень, що надходять у цей шар.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "Тож погладьте себе по плечу!",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "Якщо все це має сенс, то тепер ви зазирнули глибоко в серце зворотного поширення, робочої конячки, яка лежить в основі того, як нейронні мережі навчаються.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "Ці вирази правил ланцюга дають вам похідні, які визначають кожен компонент у градієнті, що допомагає мінімізувати вартість мережі шляхом повторного кроку вниз.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "Якщо ви сидите склавши руки і думаєте про все це, це багато рівнів складності, щоб охопити свій розум, тож не хвилюйтеся, якщо вашому розуму потрібен час, щоб усе це переварити.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]