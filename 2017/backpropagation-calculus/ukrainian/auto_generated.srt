1
00:00:00,000 --> 00:00:05,125
Важке припущення полягає в тому, що ви переглянули частину 3,

2
00:00:05,125 --> 00:00:11,160
яка дає інтуїтивно зрозумілу інструкцію з алгоритму зворотного поширення.

3
00:00:11,160 --> 00:00:14,920
Тут ми станемо більш формальними та зануримося у відповідне обчислення.

4
00:00:14,920 --> 00:00:18,195
Це нормально, що це принаймні трохи заплутає, тому мантра регулярно

5
00:00:18,195 --> 00:00:22,000
зупинятися та розмірковувати, безумовно, застосовна тут так само, як і будь-де.

6
00:00:22,000 --> 00:00:25,799
Наша головна мета — показати, як люди, які займаються машинним навчанням,

7
00:00:25,799 --> 00:00:29,342
зазвичай думають про правило ланцюга з обчислення в контексті мереж,

8
00:00:29,342 --> 00:00:33,655
яке має інше відчуття від того, як більшість початкових курсів обчислення підходять

9
00:00:33,655 --> 00:00:34,580
до цього предмета.

10
00:00:34,580 --> 00:00:39,300
Для тих із вас, кому незручно відповідне обчислення, у мене є ціла серія на цю тему.

11
00:00:39,300 --> 00:00:46,780
Давайте почнемо з надзвичайно простої мережі, де кожен рівень містить один нейрон.

12
00:00:46,780 --> 00:00:50,740
Ця мережа визначається трьома вагами та трьома зміщеннями,

13
00:00:50,740 --> 00:00:55,640
і наша мета — зрозуміти, наскільки функція витрат чутлива до цих змінних.

14
00:00:55,640 --> 00:00:58,314
Таким чином ми знаємо, які коригування цих умов

15
00:00:58,314 --> 00:01:01,100
спричинять найефективніше зниження функції витрат.

16
00:01:01,100 --> 00:01:05,360
Ми просто зосередимося на зв’язку між двома останніми нейронами.

17
00:01:05,360 --> 00:01:09,520
Давайте позначимо активацію цього останнього нейрона верхнім індексом L,

18
00:01:09,520 --> 00:01:11,800
вказуючи, на якому шарі він знаходиться.

19
00:01:11,800 --> 00:01:16,560
Таким чином, активація попереднього нейрона AL-1.

20
00:01:16,560 --> 00:01:20,371
Це не експоненти, це лише спосіб індексування того, про що ми говоримо,

21
00:01:20,371 --> 00:01:23,600
оскільки пізніше я хочу зберегти індекси для різних індексів.

22
00:01:23,600 --> 00:01:28,310
Припустімо, що значення, яке ми хочемо мати для цієї останньої активації

23
00:01:28,310 --> 00:01:33,020
для даного прикладу навчання, дорівнює y, наприклад, y може бути 0 або 1.

24
00:01:33,020 --> 00:01:39,040
Отже, вартість цієї мережі для одного навчального прикладу становить AL-y2.

25
00:01:39,040 --> 00:01:46,120
Ми позначимо вартість одного навчального прикладу як c0.

26
00:01:46,120 --> 00:01:51,429
Нагадаю, що ця остання активація визначається вагою, яку я називатиму wL,

27
00:01:51,429 --> 00:01:57,600
помноженою на активацію попереднього нейрона плюс деяке зміщення, яке я називатиму bL.

28
00:01:57,600 --> 00:02:01,560
Потім ви прокачуєте це через якусь спеціальну нелінійну функцію, як-от сигмоїда або ReLU.

29
00:02:01,560 --> 00:02:06,519
Насправді нам стане простіше, якщо ми дамо спеціальну назву цій зваженій сумі,

30
00:02:06,519 --> 00:02:10,600
як-от z, з тим самим верхнім індексом, що й відповідні активації.

31
00:02:10,600 --> 00:02:14,996
Це багато термінів, і ви можете концептуалізувати це так: вага,

32
00:02:14,996 --> 00:02:20,834
попередня дія та зміщення разом використовуються для обчислення z, що, у свою чергу,

33
00:02:20,834 --> 00:02:25,161
дозволяє нам обчислити a, яке, нарешті, разом із константою y,

34
00:02:25,161 --> 00:02:27,360
дозволяє ми розрахуємо вартість.

35
00:02:27,360 --> 00:02:31,945
І, звісно, на AL-1 впливає власна вага, упередженість тощо,

36
00:02:31,945 --> 00:02:35,920
але ми не збираємося на цьому зосереджуватися зараз.

37
00:02:35,920 --> 00:02:38,120
Усе це лише цифри, чи не так?

38
00:02:38,120 --> 00:02:41,960
І може бути приємно уявити, що кожна з них має власну маленьку числову лінію.

39
00:02:41,960 --> 00:02:45,890
Наша перша мета — зрозуміти, наскільки функція

40
00:02:45,890 --> 00:02:49,820
витрат чутлива до невеликих змін нашої ваги wL.

41
00:02:49,820 --> 00:02:55,740
Або інакше сформулюйте, яка похідна c відносно wL?

42
00:02:55,740 --> 00:03:01,691
Коли ви бачите цей термін del w, уявіть, що це означає деякий крихітний поштовх до w,

43
00:03:01,691 --> 00:03:06,259
як зміна на 0.01, і подумайте про те, що цей термін del c означає

44
00:03:06,259 --> 00:03:08,820
будь-яке кінцеве підвищення вартості.

45
00:03:08,820 --> 00:03:10,900
Ми хочемо їх співвідношення.

46
00:03:10,900 --> 00:03:16,983
Концептуально цей невеликий поштовх до wL спричиняє деякий поштовх до zL, який,

47
00:03:16,983 --> 00:03:23,220
у свою чергу, викликає певний поштовх до AL, що безпосередньо впливає на вартість.

48
00:03:23,220 --> 00:03:28,280
Отже, ми розділяємо речі, спочатку дивлячись на відношення незначної

49
00:03:28,280 --> 00:03:33,340
зміни zL до цієї незначної зміни w, тобто похідну від zL відносно wL.

50
00:03:33,340 --> 00:03:38,541
Подібним чином ви розглядаєте співвідношення зміни до AL до крихітної зміни в zL,

51
00:03:38,541 --> 00:03:43,172
яка її спричинила, а також співвідношення між остаточним підштовхуванням

52
00:03:43,172 --> 00:03:45,900
до c і цим проміжним підштовхуванням до AL.

53
00:03:45,900 --> 00:03:52,535
Це ланцюгове правило, де множення цих трьох співвідношень

54
00:03:52,535 --> 00:03:57,340
дає нам чутливість c до невеликих змін wL.

55
00:03:57,340 --> 00:04:02,335
Тож зараз на екрані є багато символів, і знайдіть хвилинку, щоб переконатися,

56
00:04:02,335 --> 00:04:07,460
що вони всі зрозумілі, тому що зараз ми збираємося обчислити відповідні похідні.

57
00:04:07,460 --> 00:04:14,220
Похідна від c відносно AL виявляється 2AL-y.

58
00:04:14,220 --> 00:04:19,069
Це означає, що його розмір пропорційний різниці між виходом мережі та тим,

59
00:04:19,069 --> 00:04:23,595
що ми хочемо, щоб він був, тому, якщо цей вивід сильно відрізняється,

60
00:04:23,595 --> 00:04:28,380
навіть незначні зміни можуть мати великий вплив на кінцеву функцію витрат.

61
00:04:28,380 --> 00:04:32,617
Похідна AL відносно zL — це просто похідна нашої сигмоїдної

62
00:04:32,617 --> 00:04:37,420
функції або будь-якої нелінійності, яку ви вирішите використовувати.

63
00:04:37,420 --> 00:04:46,180
Похідна від zL відносно wL дорівнює AL-1.

64
00:04:46,180 --> 00:04:49,864
Я не знаю, як ви, але я думаю, що легко застрягти головою в формулах,

65
00:04:49,864 --> 00:04:54,180
не витрачаючи хвилини, щоб сісти склавши руки та нагадати собі, що вони означають.

66
00:04:54,180 --> 00:04:58,550
У випадку цієї останньої похідної величина впливу невеликого поштовху на

67
00:04:58,550 --> 00:05:03,220
вагу на останній шар залежить від того, наскільки сильним є попередній нейрон.

68
00:05:03,220 --> 00:05:09,320
Пам’ятайте, що саме тут з’являється ідея нейронів, які спрацьовують разом.

69
00:05:09,320 --> 00:05:16,580
І все це є похідною по відношенню до wL лише від вартості окремого прикладу навчання.

70
00:05:16,580 --> 00:05:22,427
Оскільки функція повних витрат передбачає усереднення всіх цих витрат у багатьох різних

71
00:05:22,427 --> 00:05:27,942
прикладах навчання, її похідна вимагає усереднення цього виразу для всіх прикладів

72
00:05:27,942 --> 00:05:28,540
навчання.

73
00:05:28,540 --> 00:05:33,254
Звичайно, це лише один компонент вектора градієнта,

74
00:05:33,254 --> 00:05:40,780
який складається з часткових похідних функції вартості щодо всіх цих ваг і зміщень.

75
00:05:40,780 --> 00:05:44,088
Але хоча це лише одна з багатьох частинних похідних,

76
00:05:44,088 --> 00:05:46,460
які нам потрібні, це понад 50% роботи.

77
00:05:46,460 --> 00:05:50,300
Чутливість до зсуву, наприклад, практично однакова.

78
00:05:50,300 --> 00:05:58,980
Нам просто потрібно змінити цей термін del z del w на a del z del b.

79
00:05:58,980 --> 00:06:04,700
І якщо ви подивитеся на відповідну формулу, ця похідна виявиться рівною 1.

80
00:06:04,700 --> 00:06:10,520
Крім того, і саме тут виникає ідея поширення назад, ви можете побачити,

81
00:06:10,520 --> 00:06:16,180
наскільки ця функція вартості чутлива до активації попереднього рівня.

82
00:06:16,180 --> 00:06:20,599
А саме, ця початкова похідна у виразі правила ланцюга,

83
00:06:20,599 --> 00:06:25,420
чутливість z до попередньої активації, виявляється вагою wL.

84
00:06:25,420 --> 00:06:30,681
І знову ж таки, хоча ми не зможемо напряму вплинути на активацію попереднього рівня,

85
00:06:30,681 --> 00:06:35,138
це корисно відслідковувати, тому що тепер ми можемо просто продовжувати

86
00:06:35,138 --> 00:06:38,975
повторювати ту саму ідею правила ланцюга назад, щоб побачити,

87
00:06:38,975 --> 00:06:43,680
наскільки чутлива функція витрат до попередні ваги та попередні упередження.

88
00:06:43,680 --> 00:06:48,274
І ви можете подумати, що це надто простий приклад, оскільки всі рівні мають один нейрон,

89
00:06:48,274 --> 00:06:51,320
а для реальної мережі все стане експоненціально складнішим.

90
00:06:51,320 --> 00:06:55,809
Але, чесно кажучи, не так багато змін, коли ми надаємо шарам кілька нейронів,

91
00:06:55,809 --> 00:06:59,320
насправді це лише кілька індексів, які потрібно відстежувати.

92
00:06:59,320 --> 00:07:03,333
Замість того, щоб активація даного шару була просто AL,

93
00:07:03,333 --> 00:07:07,920
він також матиме індекс, який вказує, який нейрон цього шару це.

94
00:07:07,920 --> 00:07:15,280
Використовуємо букву k для індексування шару L-1, а j для індексування шару L.

95
00:07:15,280 --> 00:07:19,409
Що стосується вартості, ми знову дивимося на бажаний результат,

96
00:07:19,409 --> 00:07:24,829
але цього разу ми складаємо квадрати різниць між цими останніми активаціями шару та

97
00:07:24,829 --> 00:07:26,120
бажаним результатом.

98
00:07:26,120 --> 00:07:33,280
Тобто ви берете суму на ALj мінус yj у квадраті.

99
00:07:33,280 --> 00:07:38,017
Оскільки ваг набагато більше, кожен з них повинен мати ще пару індексів,

100
00:07:38,017 --> 00:07:42,625
щоб відстежувати, де він знаходиться, тому давайте назвемо вагу ребра,

101
00:07:42,625 --> 00:07:45,740
що з’єднує цей k-й нейрон із j-м нейроном, WLjk.

102
00:07:45,740 --> 00:07:49,976
Спочатку ці індекси можуть здатися трохи зворотними, але вони узгоджуються з тим,

103
00:07:49,976 --> 00:07:53,800
як ви індексуєте вагову матрицю, про яку я говорив у першій частині відео.

104
00:07:53,800 --> 00:07:58,272
Як і раніше, доцільно дати ім’я відповідній зваженій сумі, як-от z,

105
00:07:58,272 --> 00:08:03,862
щоб активація останнього шару була просто вашою спеціальною функцією, як-от сигмоід,

106
00:08:03,862 --> 00:08:04,980
застосована до z.

107
00:08:04,980 --> 00:08:09,981
Ви розумієте, що я маю на увазі, де всі ці рівняння, по суті, ті самі рівняння,

108
00:08:09,981 --> 00:08:15,420
які ми мали раніше у випадку одного нейрона на шар, просто це виглядає трохи складніше.

109
00:08:15,420 --> 00:08:18,961
І справді, похідний вираз ланцюгового правила, що описує,

110
00:08:18,961 --> 00:08:23,540
наскільки чутлива вартість до конкретної ваги, виглядає практично однаково.

111
00:08:23,540 --> 00:08:29,420
Я залишу вам зробити паузу та подумати над кожним із цих термінів, якщо хочете.

112
00:08:29,420 --> 00:08:37,820
Що тут змінюється, так це похідна вартості відносно однієї з активацій на рівні L-1.

113
00:08:37,820 --> 00:08:40,651
У цьому випадку різниця полягає в тому, що нейрон

114
00:08:40,651 --> 00:08:43,540
впливає на функцію витрат кількома різними шляхами.

115
00:08:43,540 --> 00:08:51,380
Тобто, з одного боку, це впливає на AL0, який відіграє певну роль у функції витрат,

116
00:08:51,380 --> 00:08:58,193
але він також впливає на AL1, який також відіграє роль у функції витрат,

117
00:08:58,193 --> 00:09:00,340
і ви повинні додати їх.

118
00:09:00,340 --> 00:09:03,680
І це, ну, це майже все.

119
00:09:03,680 --> 00:09:08,060
Коли ви дізнаєтеся, наскільки функція вартості чутлива до активацій на цьому

120
00:09:08,060 --> 00:09:12,611
передостанньому шарі, ви можете просто повторити процес для всіх ваг і зміщень,

121
00:09:12,611 --> 00:09:13,920
що надходять у цей шар.

122
00:09:13,920 --> 00:09:15,420
Тож погладьте себе по плечу!

123
00:09:15,420 --> 00:09:19,800
Якщо все це має сенс, то тепер ви зазирнули глибоко в серце зворотного поширення,

124
00:09:19,800 --> 00:09:23,700
робочої конячки, яка лежить в основі того, як нейронні мережі навчаються.

125
00:09:23,700 --> 00:09:29,965
Ці вирази правил ланцюга дають вам похідні, які визначають кожен компонент у градієнті,

126
00:09:29,965 --> 00:09:35,020
що допомагає мінімізувати вартість мережі шляхом повторного кроку вниз.

127
00:09:35,020 --> 00:09:36,778
Якщо ви сидите склавши руки і думаєте про все це, це багато рівнів складності,

128
00:09:36,778 --> 00:09:38,470
щоб охопити свій розум, тож не хвилюйтеся, якщо вашому розуму потрібен час,

129
00:09:38,470 --> 00:09:38,960
щоб усе це переварити.

