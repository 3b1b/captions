1
00:00:04,020 --> 00:00:06,729
Важке припущення полягає в тому, що ви переглянули частину 3, 

2
00:00:06,729 --> 00:00:09,920
яка дає інтуїтивно зрозумілу інструкцію з алгоритму зворотного поширення.

3
00:00:11,040 --> 00:00:14,220
Тут ми станемо більш формальними та зануримося у відповідне обчислення.

4
00:00:14,820 --> 00:00:17,863
Це нормально, що це принаймні трохи заплутає, тому мантра регулярно 

5
00:00:17,863 --> 00:00:21,400
зупинятися та розмірковувати, безумовно, застосовна тут так само, як і будь-де.

6
00:00:21,940 --> 00:00:25,473
Наша головна мета — показати, як люди, які займаються машинним навчанням, 

7
00:00:25,473 --> 00:00:28,768
зазвичай думають про правило ланцюга з обчислення в контексті мереж, 

8
00:00:28,768 --> 00:00:32,780
яке має інше відчуття від того, як більшість початкових курсів обчислення підходять 

9
00:00:32,780 --> 00:00:33,640
до цього предмета.

10
00:00:34,340 --> 00:00:38,740
Для тих із вас, кому незручно відповідне обчислення, у мене є ціла серія на цю тему.

11
00:00:39,960 --> 00:00:46,020
Давайте почнемо з надзвичайно простої мережі, де кожен рівень містить один нейрон.

12
00:00:46,320 --> 00:00:50,146
Ця мережа визначається трьома вагами та трьома зміщеннями, 

13
00:00:50,146 --> 00:00:54,880
і наша мета — зрозуміти, наскільки функція витрат чутлива до цих змінних.

14
00:00:55,419 --> 00:00:58,799
Таким чином ми знаємо, які коригування цих умов 

15
00:00:58,799 --> 00:01:02,320
спричинять найефективніше зниження функції витрат.

16
00:01:02,320 --> 00:01:04,840
Ми просто зосередимося на зв’язку між двома останніми нейронами.

17
00:01:05,980 --> 00:01:09,455
Давайте позначимо активацію цього останнього нейрона верхнім індексом L, 

18
00:01:09,455 --> 00:01:11,360
вказуючи, на якому шарі він знаходиться.

19
00:01:11,680 --> 00:01:15,560
Таким чином, активація попереднього нейрона AL-1.

20
00:01:16,360 --> 00:01:19,976
Це не експоненти, це лише спосіб індексування того, про що ми говоримо, 

21
00:01:19,976 --> 00:01:23,040
оскільки пізніше я хочу зберегти індекси для різних індексів.

22
00:01:23,720 --> 00:01:27,950
Припустімо, що значення, яке ми хочемо мати для цієї останньої активації 

23
00:01:27,950 --> 00:01:32,180
для даного прикладу навчання, дорівнює y, наприклад, y може бути 0 або 1.

24
00:01:32,840 --> 00:01:39,240
Отже, вартість цієї мережі для одного навчального прикладу становить AL-y2.

25
00:01:40,260 --> 00:01:44,380
Ми позначимо вартість одного навчального прикладу як c0.

26
00:01:45,900 --> 00:01:51,311
Нагадаю, що ця остання активація визначається вагою, яку я називатиму wL, 

27
00:01:51,311 --> 00:01:57,600
помноженою на активацію попереднього нейрона плюс деяке зміщення, яке я називатиму bL.

28
00:01:57,600 --> 00:02:01,320
Потім ви прокачуєте це через якусь спеціальну нелінійну функцію, як-от сигмоїда або ReLU.

29
00:02:01,800 --> 00:02:05,925
Насправді нам стане простіше, якщо ми дамо спеціальну назву цій зваженій сумі, 

30
00:02:05,925 --> 00:02:09,320
як-от z, з тим самим верхнім індексом, що й відповідні активації.

31
00:02:10,380 --> 00:02:14,340
Це багато термінів, і ви можете концептуалізувати це так: вага, 

32
00:02:14,340 --> 00:02:19,600
попередня дія та зміщення разом використовуються для обчислення z, що, у свою чергу, 

33
00:02:19,600 --> 00:02:23,499
дозволяє нам обчислити a, яке, нарешті, разом із константою y, 

34
00:02:23,499 --> 00:02:25,480
дозволяє ми розрахуємо вартість.

35
00:02:27,340 --> 00:02:31,475
І, звісно, на AL-1 впливає власна вага, упередженість тощо, 

36
00:02:31,475 --> 00:02:35,060
але ми не збираємося на цьому зосереджуватися зараз.

37
00:02:35,700 --> 00:02:37,620
Усе це лише цифри, чи не так?

38
00:02:38,060 --> 00:02:41,040
І може бути приємно уявити, що кожна з них має власну маленьку числову лінію.

39
00:02:41,720 --> 00:02:45,360
Наша перша мета — зрозуміти, наскільки функція 

40
00:02:45,360 --> 00:02:49,000
витрат чутлива до невеликих змін нашої ваги wL.

41
00:02:49,540 --> 00:02:54,860
Або інакше сформулюйте, яка похідна c відносно wL?

42
00:02:55,600 --> 00:03:01,269
Коли ви бачите цей термін del w, уявіть, що це означає деякий крихітний поштовх до w, 

43
00:03:01,269 --> 00:03:05,620
як зміна на 0.01, і подумайте про те, що цей термін del c означає 

44
00:03:05,620 --> 00:03:08,060
будь-яке кінцеве підвищення вартості.

45
00:03:08,060 --> 00:03:10,220
Ми хочемо їх співвідношення.

46
00:03:11,260 --> 00:03:16,188
Концептуально цей невеликий поштовх до wL спричиняє деякий поштовх до zL, який, 

47
00:03:16,188 --> 00:03:21,240
у свою чергу, викликає певний поштовх до AL, що безпосередньо впливає на вартість.

48
00:03:23,120 --> 00:03:28,160
Отже, ми розділяємо речі, спочатку дивлячись на відношення незначної 

49
00:03:28,160 --> 00:03:33,200
зміни zL до цієї незначної зміни w, тобто похідну від zL відносно wL.

50
00:03:33,200 --> 00:03:37,946
Подібним чином ви розглядаєте співвідношення зміни до AL до крихітної зміни в zL, 

51
00:03:37,946 --> 00:03:42,171
яка її спричинила, а також співвідношення між остаточним підштовхуванням 

52
00:03:42,171 --> 00:03:44,660
до c і цим проміжним підштовхуванням до AL.

53
00:03:45,740 --> 00:03:51,192
Це ланцюгове правило, де множення цих трьох співвідношень 

54
00:03:51,192 --> 00:03:55,140
дає нам чутливість c до невеликих змін wL.

55
00:03:56,880 --> 00:04:01,500
Тож зараз на екрані є багато символів, і знайдіть хвилинку, щоб переконатися, 

56
00:04:01,500 --> 00:04:06,240
що вони всі зрозумілі, тому що зараз ми збираємося обчислити відповідні похідні.

57
00:04:07,440 --> 00:04:14,180
Похідна від c відносно AL виявляється 2AL-y.

58
00:04:14,180 --> 00:04:18,618
Це означає, що його розмір пропорційний різниці між виходом мережі та тим, 

59
00:04:18,618 --> 00:04:22,760
що ми хочемо, щоб він був, тому, якщо цей вивід сильно відрізняється, 

60
00:04:22,760 --> 00:04:27,140
навіть незначні зміни можуть мати великий вплив на кінцеву функцію витрат.

61
00:04:27,840 --> 00:04:32,330
Похідна AL відносно zL — це просто похідна нашої сигмоїдної 

62
00:04:32,330 --> 00:04:37,420
функції або будь-якої нелінійності, яку ви вирішите використовувати.

63
00:04:37,420 --> 00:04:46,160
Похідна від zL відносно wL дорівнює AL-1.

64
00:04:46,160 --> 00:04:49,503
Я не знаю, як ви, але я думаю, що легко застрягти головою в формулах, 

65
00:04:49,503 --> 00:04:53,420
не витрачаючи хвилини, щоб сісти склавши руки та нагадати собі, що вони означають.

66
00:04:53,920 --> 00:04:58,222
У випадку цієї останньої похідної величина впливу невеликого поштовху на 

67
00:04:58,222 --> 00:05:02,820
вагу на останній шар залежить від того, наскільки сильним є попередній нейрон.

68
00:05:03,380 --> 00:05:08,280
Пам’ятайте, що саме тут з’являється ідея нейронів, які спрацьовують разом.

69
00:05:09,200 --> 00:05:15,720
І все це є похідною по відношенню до wL лише від вартості окремого прикладу навчання.

70
00:05:16,440 --> 00:05:22,414
Оскільки функція повних витрат передбачає усереднення всіх цих витрат у багатьох різних 

71
00:05:22,414 --> 00:05:28,049
прикладах навчання, її похідна вимагає усереднення цього виразу для всіх прикладів 

72
00:05:28,049 --> 00:05:28,660
навчання.

73
00:05:28,660 --> 00:05:32,357
Звичайно, це лише один компонент вектора градієнта, 

74
00:05:32,357 --> 00:05:38,260
який складається з часткових похідних функції вартості щодо всіх цих ваг і зміщень.

75
00:05:40,640 --> 00:05:43,330
Але хоча це лише одна з багатьох частинних похідних, 

76
00:05:43,330 --> 00:05:45,260
які нам потрібні, це понад 50% роботи.

77
00:05:46,340 --> 00:05:49,720
Чутливість до зсуву, наприклад, практично однакова.

78
00:05:50,040 --> 00:05:55,020
Нам просто потрібно змінити цей термін del z del w на a del z del b.

79
00:05:58,420 --> 00:06:02,400
І якщо ви подивитеся на відповідну формулу, ця похідна виявиться рівною 1.

80
00:06:06,140 --> 00:06:11,007
Крім того, і саме тут виникає ідея поширення назад, ви можете побачити, 

81
00:06:11,007 --> 00:06:15,740
наскільки ця функція вартості чутлива до активації попереднього рівня.

82
00:06:15,740 --> 00:06:20,484
А саме, ця початкова похідна у виразі правила ланцюга, 

83
00:06:20,484 --> 00:06:25,660
чутливість z до попередньої активації, виявляється вагою wL.

84
00:06:26,640 --> 00:06:31,192
І знову ж таки, хоча ми не зможемо напряму вплинути на активацію попереднього рівня, 

85
00:06:31,192 --> 00:06:35,048
це корисно відслідковувати, тому що тепер ми можемо просто продовжувати 

86
00:06:35,048 --> 00:06:38,369
повторювати ту саму ідею правила ланцюга назад, щоб побачити, 

87
00:06:38,369 --> 00:06:42,440
наскільки чутлива функція витрат до попередні ваги та попередні упередження.

88
00:06:43,180 --> 00:06:47,894
І ви можете подумати, що це надто простий приклад, оскільки всі рівні мають один нейрон, 

89
00:06:47,894 --> 00:06:51,020
а для реальної мережі все стане експоненціально складнішим.

90
00:06:51,700 --> 00:06:55,717
Але, чесно кажучи, не так багато змін, коли ми надаємо шарам кілька нейронів, 

91
00:06:55,717 --> 00:06:58,860
насправді це лише кілька індексів, які потрібно відстежувати.

92
00:06:59,380 --> 00:07:03,010
Замість того, щоб активація даного шару була просто AL, 

93
00:07:03,010 --> 00:07:07,160
він також матиме індекс, який вказує, який нейрон цього шару це.

94
00:07:07,160 --> 00:07:14,420
Використовуємо букву k для індексування шару L-1, а j для індексування шару L.

95
00:07:15,260 --> 00:07:19,039
Що стосується вартості, ми знову дивимося на бажаний результат, 

96
00:07:19,039 --> 00:07:23,999
але цього разу ми складаємо квадрати різниць між цими останніми активаціями шару та 

97
00:07:23,999 --> 00:07:25,180
бажаним результатом.

98
00:07:26,080 --> 00:07:30,840
Тобто ви берете суму на ALj мінус yj у квадраті.

99
00:07:33,040 --> 00:07:37,556
Оскільки ваг набагато більше, кожен з них повинен мати ще пару індексів, 

100
00:07:37,556 --> 00:07:41,950
щоб відстежувати, де він знаходиться, тому давайте назвемо вагу ребра, 

101
00:07:41,950 --> 00:07:44,920
що з’єднує цей k-й нейрон із j-м нейроном, WLjk.

102
00:07:45,620 --> 00:07:49,562
Спочатку ці індекси можуть здатися трохи зворотними, але вони узгоджуються з тим, 

103
00:07:49,562 --> 00:07:53,120
як ви індексуєте вагову матрицю, про яку я говорив у першій частині відео.

104
00:07:53,620 --> 00:07:57,836
Як і раніше, доцільно дати ім’я відповідній зваженій сумі, як-от z, 

105
00:07:57,836 --> 00:08:03,106
щоб активація останнього шару була просто вашою спеціальною функцією, як-от сигмоід, 

106
00:08:03,106 --> 00:08:04,160
застосована до z.

107
00:08:04,660 --> 00:08:08,980
Ви розумієте, що я маю на увазі, де всі ці рівняння, по суті, ті самі рівняння, 

108
00:08:08,980 --> 00:08:13,680
які ми мали раніше у випадку одного нейрона на шар, просто це виглядає трохи складніше.

109
00:08:15,440 --> 00:08:18,919
І справді, похідний вираз ланцюгового правила, що описує, 

110
00:08:18,919 --> 00:08:23,420
наскільки чутлива вартість до конкретної ваги, виглядає практично однаково.

111
00:08:23,920 --> 00:08:26,840
Я залишу вам зробити паузу та подумати над кожним із цих термінів, якщо хочете.

112
00:08:28,979 --> 00:08:36,659
Що тут змінюється, так це похідна вартості відносно однієї з активацій на рівні L-1.

113
00:08:37,780 --> 00:08:40,304
У цьому випадку різниця полягає в тому, що нейрон 

114
00:08:40,304 --> 00:08:42,880
впливає на функцію витрат кількома різними шляхами.

115
00:08:44,680 --> 00:08:50,681
Тобто, з одного боку, це впливає на AL0, який відіграє певну роль у функції витрат, 

116
00:08:50,681 --> 00:08:55,896
але він також впливає на AL1, який також відіграє роль у функції витрат, 

117
00:08:55,896 --> 00:08:57,540
і ви повинні додати їх.

118
00:08:59,820 --> 00:09:03,040
І це, ну, це майже все.

119
00:09:03,500 --> 00:09:07,503
Коли ви дізнаєтеся, наскільки функція вартості чутлива до активацій на цьому 

120
00:09:07,503 --> 00:09:11,664
передостанньому шарі, ви можете просто повторити процес для всіх ваг і зміщень, 

121
00:09:11,664 --> 00:09:12,860
що надходять у цей шар.

122
00:09:13,900 --> 00:09:14,960
Тож погладьте себе по плечу!

123
00:09:15,300 --> 00:09:19,204
Якщо все це має сенс, то тепер ви зазирнули глибоко в серце зворотного поширення, 

124
00:09:19,204 --> 00:09:22,680
робочої конячки, яка лежить в основі того, як нейронні мережі навчаються.

125
00:09:23,300 --> 00:09:28,834
Ці вирази правил ланцюга дають вам похідні, які визначають кожен компонент у градієнті, 

126
00:09:28,834 --> 00:09:33,300
що допомагає мінімізувати вартість мережі шляхом повторного кроку вниз.

127
00:09:34,300 --> 00:09:38,067
Якщо ви сидите склавши руки і думаєте про все це, це багато рівнів складності, 

128
00:09:38,067 --> 00:09:41,690
щоб охопити свій розум, тож не хвилюйтеся, якщо вашому розуму потрібен час, 

129
00:09:41,690 --> 00:09:42,740
щоб усе це переварити.

