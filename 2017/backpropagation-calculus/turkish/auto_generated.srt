1
00:00:04,019 --> 00:00:08,586
Buradaki zor varsayım, geriye yayılma algoritmasının sezgisel bir şekilde anlatıldığı 3. 

2
00:00:08,586 --> 00:00:09,920
bölümü izlemiş olmanızdır.

3
00:00:11,040 --> 00:00:14,220
Burada biraz daha resmi olacağız ve ilgili hesaplamalara dalacağız.

4
00:00:14,820 --> 00:00:17,190
Bunun en azından biraz kafa karıştırıcı olması normaldir, 

5
00:00:17,190 --> 00:00:20,541
bu nedenle düzenli olarak durup düşünme mantrası kesinlikle her yerde olduğu gibi 

6
00:00:20,541 --> 00:00:21,400
burada da geçerlidir.

7
00:00:21,940 --> 00:00:25,461
Temel amacımız, makine öğrenimi alanındaki kişilerin kalkülüs 

8
00:00:25,461 --> 00:00:29,323
derslerindeki zincir kuralını ağlar bağlamında nasıl düşündüklerini 

9
00:00:29,323 --> 00:00:33,640
göstermektir ki bu da çoğu kalkülüs dersinin konuya yaklaşımından farklıdır.

10
00:00:34,340 --> 00:00:38,740
İlgili hesaplamalardan rahatsız olanlar için konuyla ilgili bütün bir serim var.

11
00:00:39,960 --> 00:00:46,020
Her katmanda tek bir nöronun bulunduğu son derece basit bir ağ ile başlayalım.

12
00:00:46,320 --> 00:00:50,693
Bu ağ üç ağırlık ve üç sapma ile belirlenmektedir ve amacımız maliyet 

13
00:00:50,693 --> 00:00:54,880
fonksiyonunun bu değişkenlere ne kadar duyarlı olduğunu anlamaktır.

14
00:00:55,420 --> 00:00:57,940
Bu şekilde, bu terimlerde yapılacak hangi ayarlamaların 

15
00:00:57,940 --> 00:01:00,820
maliyet fonksiyonunda en verimli düşüşe neden olacağını biliriz.

16
00:01:01,960 --> 00:01:04,840
Ve biz sadece son iki nöron arasındaki bağlantıya odaklanacağız.

17
00:01:05,980 --> 00:01:10,736
Bu son nöronun aktivasyonunu, hangi katmanda olduğunu gösteren bir üst 

18
00:01:10,736 --> 00:01:15,560
simge L ile etiketleyelim, böylece önceki nöronun aktivasyonu Al-1 olur.

19
00:01:16,360 --> 00:01:19,700
Bunlar üs değildir, sadece bahsettiğimiz şeyi indekslemenin bir yoludur, 

20
00:01:19,700 --> 00:01:23,040
çünkü daha sonra farklı indeksler için alt simgeleri kaydetmek istiyorum.

21
00:01:23,720 --> 00:01:27,882
Diyelim ki belirli bir eğitim örneği için bu son aktivasyonun 

22
00:01:27,882 --> 00:01:32,180
olmasını istediğimiz değer y olsun, örneğin y 0 veya 1 olabilir.

23
00:01:32,840 --> 00:01:39,240
Dolayısıyla bu ağın tek bir eğitim örneği için maliyeti Al-y2'dir.

24
00:01:40,260 --> 00:01:44,380
Bu tek eğitim örneğinin maliyetini c0 olarak göstereceğiz.

25
00:01:45,900 --> 00:01:50,924
Bir hatırlatma olarak, bu son aktivasyon, WL olarak adlandıracağım bir ağırlık, 

26
00:01:50,924 --> 00:01:55,949
önceki nöronun aktivasyonu artı BL olarak adlandıracağım bir miktar önyargı ile 

27
00:01:55,949 --> 00:01:56,640
belirlenir.

28
00:01:57,420 --> 00:01:59,390
Ve sonra bunu sigmoid veya ReLU gibi bazı özel 

29
00:01:59,390 --> 00:02:01,320
doğrusal olmayan fonksiyonlarla pompalarsınız.

30
00:02:01,800 --> 00:02:05,529
Aslında bu ağırlıklı toplama, ilgili aktivasyonlarla aynı üst 

31
00:02:05,529 --> 00:02:09,320
simge ile z gibi özel bir isim verirsek işimiz kolaylaşacaktır.

32
00:02:10,380 --> 00:02:14,624
Bu çok fazla terimdir ve bunu kavramsallaştırmanın bir yolu, ağırlık, 

33
00:02:14,624 --> 00:02:19,355
önceki eylem ve önyargının hep birlikte z'yi hesaplamak için kullanılmasıdır, 

34
00:02:19,355 --> 00:02:24,206
bu da a'yı hesaplamamızı sağlar ve son olarak sabit bir y ile birlikte maliyeti 

35
00:02:24,206 --> 00:02:25,480
hesaplamamızı sağlar.

36
00:02:27,340 --> 00:02:32,664
Ve tabii ki Al-1 kendi ağırlığından, önyargısından ve benzerlerinden etkilenir, 

37
00:02:32,664 --> 00:02:35,060
ancak şu anda buna odaklanmayacağız.

38
00:02:35,700 --> 00:02:37,620
Bunların hepsi sadece rakam, değil mi?

39
00:02:38,060 --> 00:02:41,040
Ve her birinin kendi küçük sayı çizgisine sahip olduğunu düşünmek güzel olabilir.

40
00:02:41,720 --> 00:02:45,145
İlk hedefimiz, maliyet fonksiyonunun WL ağırlığımızdaki 

41
00:02:45,145 --> 00:02:49,000
küçük değişikliklere karşı ne kadar hassas olduğunu anlamaktır.

42
00:02:49,540 --> 00:02:54,860
Ya da farklı bir şekilde ifade edersek, c'nin WL'ye göre türevi nedir?

43
00:02:55,600 --> 00:02:59,881
Bu del W terimini gördüğünüzde, bunun W'de 0,01'lik bir değişiklik 

44
00:02:59,881 --> 00:03:03,778
gibi küçük bir dürtme anlamına geldiğini düşünün ve bu del c 

45
00:03:03,778 --> 00:03:08,060
teriminin maliyette ortaya çıkan dürtme anlamına geldiğini düşünün.

46
00:03:08,060 --> 00:03:10,220
Bizim istediğimiz şey onların oranı.

47
00:03:11,260 --> 00:03:16,313
Kavramsal olarak, WL'deki bu küçük dürtme ZL'de bir miktar dürtmeye neden olur, 

48
00:03:16,313 --> 00:03:21,240
bu da AL'de bir miktar dürtmeye neden olur ve bu da maliyeti doğrudan etkiler.

49
00:03:23,120 --> 00:03:28,806
Bu nedenle, ilk olarak ZL'deki küçük bir değişikliğin bu küçük W değişikliğine oranına, 

50
00:03:28,806 --> 00:03:33,200
yani ZL'nin WL'ye göre türevine bakarak işleri parçalara ayırıyoruz.

51
00:03:33,200 --> 00:03:37,020
Aynı şekilde, AL'deki değişikliğin ZL'de buna neden olan küçük 

52
00:03:37,020 --> 00:03:40,961
değişikliğe oranını ve c'ye yapılan son dürtme ile AL'ye yapılan 

53
00:03:40,961 --> 00:03:44,660
bu ara dürtme arasındaki oranı da göz önünde bulundurursunuz.

54
00:03:45,740 --> 00:03:50,440
Buradaki zincir kuralı, bu üç oranın çarpımının bize c'nin 

55
00:03:50,440 --> 00:03:55,140
WL'deki küçük değişikliklere olan duyarlılığını vermesidir.

56
00:03:56,880 --> 00:04:01,655
Şu anda ekranda çok sayıda sembol var ve bunların ne olduğunun anlaşılması 

57
00:04:01,655 --> 00:04:06,240
için bir dakikanızı ayırın, çünkü şimdi ilgili türevleri hesaplayacağız.

58
00:04:07,440 --> 00:04:13,160
c'nin AL'ye göre türevi 2AL-y olarak hesaplanır.

59
00:04:13,980 --> 00:04:18,242
Bunun, büyüklüğünün ağın çıktısı ile olmasını istediğimiz şey arasındaki farkla 

60
00:04:18,242 --> 00:04:22,664
orantılı olduğu anlamına geldiğine dikkat edin, bu nedenle bu çıktı çok farklıysa, 

61
00:04:22,664 --> 00:04:27,140
küçük değişikliklerin bile nihai maliyet işlevi üzerinde büyük bir etkisi olacaktır.

62
00:04:27,840 --> 00:04:31,863
AL'nin ZL'ye göre türevi, sigmoid fonksiyonumuzun veya 

63
00:04:31,863 --> 00:04:36,180
kullanmayı seçtiğiniz doğrusal olmayan özelliğin türevidir.

64
00:04:37,220 --> 00:04:44,660
Ve ZL'nin WL'ye göre türevi AL-1 olarak ortaya çıkar.

65
00:04:45,760 --> 00:04:49,617
Sizi bilmem ama bence bir an bile arkanıza yaslanıp bunların ne anlama 

66
00:04:49,617 --> 00:04:53,420
geldiğini kendinize hatırlatmadan formüllere takılıp kalmak çok kolay.

67
00:04:53,920 --> 00:04:59,490
Bu son türev durumunda, ağırlığa yapılan küçük dürtmenin son katmanı etkileme miktarı, 

68
00:04:59,490 --> 00:05:02,820
bir önceki nöronun ne kadar güçlü olduğuna bağlıdır.

69
00:05:03,380 --> 00:05:08,280
Unutmayın, nöronlar-birlikte-ateşler-birlikte-teller fikri burada ortaya çıkıyor.

70
00:05:09,200 --> 00:05:15,720
Ve tüm bunlar, yalnızca belirli bir tek eğitim örneği için maliyetin WL'ye göre türevidir.

71
00:05:16,440 --> 00:05:19,948
Tam maliyet fonksiyonu, birçok farklı eğitim örneğindeki tüm bu 

72
00:05:19,948 --> 00:05:22,744
maliyetlerin ortalamasını almayı gerektirdiğinden, 

73
00:05:22,744 --> 00:05:27,460
türevi bu ifadenin tüm eğitim örnekleri üzerinden ortalamasının alınmasını gerektirir.

74
00:05:28,380 --> 00:05:33,182
Ve tabii ki bu, maliyet fonksiyonunun tüm bu ağırlıklara ve sapmalara 

75
00:05:33,182 --> 00:05:38,260
göre kısmi türevlerinden oluşan gradyan vektörünün sadece bir bileşenidir.

76
00:05:40,640 --> 00:05:43,634
Ancak bu, ihtiyacımız olan birçok kısmi türevden sadece biri olsa da, 

77
00:05:43,634 --> 00:05:45,260
işin %50'sinden fazlasını oluşturuyor.

78
00:05:46,340 --> 00:05:49,720
Örneğin önyargıya karşı duyarlılık neredeyse aynıdır.

79
00:05:50,040 --> 00:05:55,020
Sadece bu del z del w terimini bir del z del b ile değiştirmemiz gerekiyor.

80
00:05:58,420 --> 00:06:02,400
Ve ilgili formüle bakarsanız, bu türevin 1 olduğunu görürsünüz.

81
00:06:06,140 --> 00:06:09,702
Ayrıca, geriye doğru yayılma fikri burada devreye giriyor, 

82
00:06:09,702 --> 00:06:14,290
bu maliyet fonksiyonunun bir önceki katmanın aktivasyonuna ne kadar duyarlı 

83
00:06:14,290 --> 00:06:15,740
olduğunu görebilirsiniz.

84
00:06:15,740 --> 00:06:19,724
Yani, zincir kuralı ifadesindeki bu ilk türev, 

85
00:06:19,724 --> 00:06:25,660
z'nin önceki aktivasyona duyarlılığı, WL ağırlığı olarak ortaya çıkar.

86
00:06:26,640 --> 00:06:30,746
Ve yine, önceki katman aktivasyonunu doğrudan etkileyemeyecek olsak da, 

87
00:06:30,746 --> 00:06:34,454
takip etmek faydalıdır, çünkü şimdi maliyet fonksiyonunun önceki 

88
00:06:34,454 --> 00:06:38,276
ağırlıklara ve önceki önyargılara ne kadar duyarlı olduğunu görmek 

89
00:06:38,276 --> 00:06:42,440
için aynı zincir kuralı fikrini geriye doğru yinelemeye devam edebiliriz.

90
00:06:43,180 --> 00:06:45,825
Bunun aşırı basit bir örnek olduğunu düşünebilirsiniz, 

91
00:06:45,825 --> 00:06:49,817
çünkü tüm katmanlarda bir nöron vardır ve gerçek bir ağ için işler katlanarak daha 

92
00:06:49,817 --> 00:06:51,020
karmaşık hale gelecektir.

93
00:06:51,700 --> 00:06:55,280
Ancak dürüst olmak gerekirse, katmanlara birden fazla nöron verdiğimizde çok fazla 

94
00:06:55,280 --> 00:06:58,860
bir şey değişmiyor, gerçekten sadece takip edilmesi gereken birkaç endeks daha var.

95
00:06:59,380 --> 00:07:02,396
Belirli bir katmanın aktivasyonu sadece AL olmak yerine, 

96
00:07:02,396 --> 00:07:07,160
aynı zamanda o katmanın hangi nöronu olduğunu gösteren bir alt simgeye de sahip olacaktır.

97
00:07:07,160 --> 00:07:10,712
L-1 katmanını indekslemek için k harfini ve L 

98
00:07:10,712 --> 00:07:14,420
katmanını indekslemek için j harfini kullanalım.

99
00:07:15,260 --> 00:07:18,753
Maliyet için yine istenen çıktının ne olduğuna bakarız, 

100
00:07:18,753 --> 00:07:23,932
ancak bu sefer bu son katman aktivasyonları ile istenen çıktı arasındaki farkların 

101
00:07:23,932 --> 00:07:25,180
karelerini toplarız.

102
00:07:26,080 --> 00:07:30,840
Yani, ALj eksi Yj karesi üzerinden bir toplam alırsınız.

103
00:07:33,040 --> 00:07:36,839
Çok daha fazla ağırlık olduğundan, her birinin nerede olduğunu 

104
00:07:36,839 --> 00:07:40,397
takip etmek için birkaç indekse daha sahip olması gerekir, 

105
00:07:40,397 --> 00:07:44,920
bu nedenle bu k. nöronu j. nörona bağlayan kenarın ağırlığına WLjk diyelim.

106
00:07:45,620 --> 00:07:49,480
Bu endeksler ilk başta biraz ters gelebilir, ancak bölüm 1 videosunda 

107
00:07:49,480 --> 00:07:53,120
bahsettiğim ağırlık matrisini nasıl endeksleyeceğinizle uyumludur.

108
00:07:53,620 --> 00:07:58,802
Daha önce olduğu gibi, ilgili ağırlıklı toplama z gibi bir isim vermek yine de güzeldir, 

109
00:07:58,802 --> 00:08:02,471
böylece son katmanın aktivasyonu sadece z'ye uygulanan sigmoid 

110
00:08:02,471 --> 00:08:04,160
gibi özel fonksiyonunuz olur.

111
00:08:04,660 --> 00:08:09,273
Ne demek istediğimi görebilirsiniz, tüm bunlar aslında daha önce katman başına bir nöron 

112
00:08:09,273 --> 00:08:13,680
durumunda sahip olduğumuz denklemlerle aynıdır, sadece biraz daha karmaşık görünüyor.

113
00:08:15,440 --> 00:08:19,210
Ve aslında, maliyetin belirli bir ağırlığa ne kadar duyarlı 

114
00:08:19,210 --> 00:08:23,420
olduğunu açıklayan zincirleme türev ifadesi temelde aynı görünüyor.

115
00:08:23,920 --> 00:08:26,840
İsterseniz bu terimlerin her biri üzerinde durup düşünmeyi size bırakıyorum.

116
00:08:28,980 --> 00:08:33,404
Ancak burada değişen şey, maliyetin L-1 katmanındaki 

117
00:08:33,404 --> 00:08:36,659
aktivasyonlardan birine göre türevidir.

118
00:08:37,780 --> 00:08:40,275
Bu durumda fark, nöronun maliyet fonksiyonunu 

119
00:08:40,275 --> 00:08:42,880
birden fazla farklı yol üzerinden etkilemesidir.

120
00:08:44,680 --> 00:08:49,432
Yani, bir yandan maliyet fonksiyonunda rol oynayan AL0'ı etkiliyor, 

121
00:08:49,432 --> 00:08:53,416
ancak aynı zamanda maliyet fonksiyonunda rol oynayan AL1 

122
00:08:53,416 --> 00:08:57,540
üzerinde de bir etkisi var ve bunları toplamanız gerekiyor.

123
00:08:59,820 --> 00:09:03,040
Ve bu, şey, hemen hemen bu kadar.

124
00:09:03,500 --> 00:09:06,668
Maliyet fonksiyonunun bu sondan ikinci katmandaki aktivasyonlara 

125
00:09:06,668 --> 00:09:09,837
ne kadar duyarlı olduğunu öğrendikten sonra, bu katmana beslenen 

126
00:09:09,837 --> 00:09:12,860
tüm ağırlıklar ve önyargılar için işlemi tekrarlayabilirsiniz.

127
00:09:13,900 --> 00:09:14,960
O yüzden sırtınızı sıvazlayın!

128
00:09:15,300 --> 00:09:18,896
Tüm bunlar size mantıklı geliyorsa, artık sinir ağlarının nasıl öğrendiğinin 

129
00:09:18,896 --> 00:09:22,680
arkasındaki beygir olan geriye yayılımın kalbine derinlemesine baktınız demektir.

130
00:09:23,300 --> 00:09:28,239
Bu zincir kuralı ifadeleri, yokuş aşağı tekrar tekrar adım atarak ağın maliyetini 

131
00:09:28,239 --> 00:09:33,300
en aza indirmeye yardımcı olan eğimdeki her bir bileşeni belirleyen türevleri verir.

132
00:09:34,300 --> 00:09:37,113
Arkanıza yaslanıp tüm bunları düşünürseniz, zihninizi saracak 

133
00:09:37,113 --> 00:09:39,427
çok fazla karmaşıklık katmanı olduğunu görürsünüz, 

134
00:09:39,427 --> 00:09:42,740
bu nedenle zihninizin tüm bunları sindirmesi zaman alırsa endişelenmeyin.

