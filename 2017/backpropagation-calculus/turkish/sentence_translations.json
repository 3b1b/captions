[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "translatedText": "",
  "from_community_srt": "Buradaki zor varsayım, bölüm 3'ü izlediğinizdir. geri yayılım algoritmasının sezgisel bir adımını vermek.",
  "n_reviews": 0,
  "start": 4.02,
  "end": 9.92
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "translatedText": "",
  "from_community_srt": "Burada biraz daha resmi olacağız ve ilgili matematiğe dalacağız.",
  "n_reviews": 0,
  "start": 11.04,
  "end": 14.22
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "translatedText": "",
  "from_community_srt": "Bunun biraz kafa karıştırıcı olması normal. bu yüzden düzenli aralıklarla durmak ve düşünmek için mantra kesinlikle burada herhangi bir yerde olduğu kadar geçerlidir.",
  "n_reviews": 0,
  "start": 14.82,
  "end": 21.4
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "translatedText": "",
  "from_community_srt": "Temel hedefimiz, insanların makine öğrenmesinde nasıl olduğunu göstermek. ağlar bağlamında matematikten zincir kuralını genel olarak düşünmek, hangi giriş kurslarının en çok konuya yaklaştığı konusunda farklı bir havası vardır.",
  "n_reviews": 0,
  "start": 21.94,
  "end": 33.64
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "translatedText": "",
  "from_community_srt": "İlgili hesaplardan rahatsız olanlar için, Konuyla ilgili bir dizi var.",
  "n_reviews": 0,
  "start": 34.34,
  "end": 38.74
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "translatedText": "",
  "from_community_srt": "Son derece basit bir ağla başlayalım, her katmanın içinde tek bir nöron bulunduğu bir tane.",
  "n_reviews": 0,
  "start": 39.96,
  "end": 46.02
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "translatedText": "",
  "from_community_srt": "Yani bu özel ağ 3 ağırlık ve 3 önyargı ile belirlenir, ve amacımız, maliyet fonksiyonunun bu değişkenlere ne kadar hassas olduğunu anlamaktır.",
  "n_reviews": 0,
  "start": 46.32,
  "end": 54.88
 },
 {
  "input": "That way, we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "translatedText": "",
  "from_community_srt": "Bu şekilde, bu şartlarda hangi ayarların yapıldığını biliyoruz. maliyet fonksiyonunda en etkin düşüşe neden olacaktır.",
  "n_reviews": 0,
  "start": 55.42,
  "end": 60.82
 },
 {
  "input": "And we're just going to focus on the connection between the last two neurons.",
  "translatedText": "",
  "from_community_srt": "Ve biz sadece son iki nöron arasındaki bağlantıya odaklanıyoruz.",
  "n_reviews": 0,
  "start": 61.96,
  "end": 64.84
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in, so the activation of the previous neuron is Al-1.",
  "translatedText": "",
  "from_community_srt": "En son nöron a'nın aktivasyonunu, içinde L katmanı ile işaretleyelim, hangi katmanda olduğunu gösterelim. bu önceki nöronun aktivasyonu bir ^ (L-1).",
  "n_reviews": 0,
  "start": 65.98,
  "end": 75.56
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "translatedText": "",
  "from_community_srt": "Üstadlar yok, onlar sadece neden bahsettiğimizi endekslemenin bir yolu. Zira daha sonra farklı endekslere abone kaydetmek istiyorum.",
  "n_reviews": 0,
  "start": 76.36,
  "end": 83.04
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "translatedText": "",
  "from_community_srt": "Diyelim ki bu son aktivasyonun belirli bir eğitim örneği için olmasını istediğimiz değer y. Örneğin, y 0 veya 1 olabilir.",
  "n_reviews": 0,
  "start": 83.72,
  "end": 92.18
 },
 {
  "input": "So the cost of this network for a single training example is Al-y2.",
  "translatedText": "",
  "from_community_srt": "Yani bu basit ağın tek bir eğitim örneği için maliyeti (a ^ (L) - y) ^ 2.",
  "n_reviews": 0,
  "start": 92.84,
  "end": 99.24
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "translatedText": "",
  "from_community_srt": "Bu eğitim örneğinin maliyetini C_0 olarak göstereceğiz.",
  "n_reviews": 0,
  "start": 100.26,
  "end": 104.38
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neuron's activation plus some bias, which I'll call BL.",
  "translatedText": "",
  "from_community_srt": "Bir hatırlatıcı olarak, bu son aktivasyon w ^ (L) olarak adlandıracağım bir ağırlıkla belirlenir. önceki nöronun aktivasyonunun birkaç katı, artı b ^ (L) diyeceğim bazı önyargılar,",
  "n_reviews": 0,
  "start": 105.9,
  "end": 116.64
 },
 {
  "input": "And then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "translatedText": "",
  "from_community_srt": "o zaman bunu bazı özel doğrusal olmayan fonksiyonlar vasıtasıyla pompalarsınız. Bir sigmoid veya bir ReLU gibi.",
  "n_reviews": 0,
  "start": 117.42,
  "end": 121.32
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "translatedText": "",
  "from_community_srt": "Eğer bu z toplam gibi özel bir isim verirsek, bu bizim için işleri kolaylaştıracak, ilgili aktivasyonlarla aynı üst simge ile.",
  "n_reviews": 0,
  "start": 121.8,
  "end": 129.32
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action and the bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "translatedText": "",
  "from_community_srt": "Yani bir çok terim var. Kavramsallaştırmanın bir yolu da, ağırlık, önceki aktivasyon ve önyargı. tamamen z'yi hesaplamak için kullanılır, ki bu da sırayla a. ve nihayet, y sabiti ile birlikte, maliyeti hesaplayalım.",
  "n_reviews": 0,
  "start": 130.38,
  "end": 145.48
 },
 {
  "input": "And of course Al-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "translatedText": "",
  "from_community_srt": "Ve elbette, bir ^ (L-1) kendi ağırlığından ve önyargısından etkilenir ve böyledir. Ama şu anda buna odaklanmayacağız.",
  "n_reviews": 0,
  "start": 147.34,
  "end": 155.06
 },
 {
  "input": "All of these are just numbers, right?",
  "translatedText": "",
  "from_community_srt": "Bunların hepsi sadece sayılar,",
  "n_reviews": 0,
  "start": 155.7,
  "end": 157.62
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "translatedText": "",
  "from_community_srt": "değil mi? Ve her birinin kendi küçük sayı çizgisine sahip olduğunu düşünmek güzel olabilir.",
  "n_reviews": 0,
  "start": 158.06,
  "end": 161.04
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight WL.",
  "translatedText": "",
  "from_community_srt": "İlk hedefimiz anlamaktır Maliyet fonksiyonunun ağırlığımızdaki küçük değişikliklere ne kadar duyarlı olduğu w ^ (L).",
  "n_reviews": 0,
  "start": 161.72,
  "end": 169.0
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to WL?",
  "translatedText": "",
  "from_community_srt": "Veya farklı ifadelerle, C ^ 'nin w ^ (L)' ye göre türevi nedir.",
  "n_reviews": 0,
  "start": 169.54,
  "end": 174.86
 },
 {
  "input": "When you see this del W term, think of it as meaning some tiny nudge to W, like a change by 0.01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "translatedText": "",
  "from_community_srt": "Bu “∂w” terimini gördüğünüzde, 0,01 gibi bir değişiklik gibi, \"w için küçük bir dürtmek\" anlamını düşünün. Ve bu “∂C” terimini “maliyete bağlı dürtüsü ne olursa olsun” olarak düşünün.",
  "n_reviews": 0,
  "start": 175.6,
  "end": 188.06
 },
 {
  "input": "What we want is their ratio.",
  "translatedText": "",
  "from_community_srt": "İstediğimiz şey onların oranı.",
  "n_reviews": 0,
  "start": 188.06,
  "end": 190.22
 },
 {
  "input": "Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost.",
  "translatedText": "",
  "from_community_srt": "Kavramsal olarak, bu küçük dürtmek w ^ (L) 'ye biraz dürtmek z ^ (L)' ye neden olur ki bu da maliyeti doğrudan etkileyen bir ^ (L) 'de değişikliğe neden olur.",
  "n_reviews": 0,
  "start": 191.26,
  "end": 201.24
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is, the derivative of ZL with respect to WL.",
  "translatedText": "",
  "from_community_srt": "Bu yüzden bunu önce küçük bir değişikliğin z ^ (L) 'ye olan küçük değişimin w ^ (L)' deki küçük değişime oranına bakarak çözüyoruz. Yani, z ^ (L) 'nin w ^ (L)' ye göre türevidir.",
  "n_reviews": 0,
  "start": 203.12,
  "end": 213.2
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "translatedText": "",
  "from_community_srt": "Aynı şekilde, ^ (L) 'deki bir değişikliğin, z ^ (L)' deki küçük değişime oranını, ve ayrıca son dürtme ile C arasındaki oran ve bu ara dürtme bir ^ (L) 'ye olan oran.",
  "n_reviews": 0,
  "start": 213.2,
  "end": 224.66
 },
 {
  "input": "This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of c to small changes in WL.",
  "translatedText": "",
  "from_community_srt": "Buradaki zincir kuralı. bu üç oranın birlikte çarpılması bize C nin w ^ (L) 'deki küçük değişikliklere duyarlılığını verir.",
  "n_reviews": 0,
  "start": 225.74,
  "end": 235.14
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "translatedText": "",
  "from_community_srt": "Şu anda ekranda, bir sürü sembol var. o yüzden, hepsinin ne olduğundan emin olmak için bir dakikanızı ayırın, çünkü şimdi ilgili türevleri hesaplayacağız.",
  "n_reviews": 0,
  "start": 236.88,
  "end": 246.24
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "translatedText": "",
  "from_community_srt": "^ (L) 'ye göre C'nin türevi, 2 (a ^ (L) - y) olarak hesaplanır.",
  "n_reviews": 0,
  "start": 247.44,
  "end": 253.16
 },
 {
  "input": "Notice this means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "translatedText": "",
  "from_community_srt": "Dikkat, bu, boyutunun orantılı olduğu anlamına gelir. Ağın çıktısı ile olmasını istediğimiz şey arasındaki fark. Yani bu çıktı çok farklı olsaydı, Küçük değişiklikler bile maliyet fonksiyonu üzerinde büyük bir etkiye sahiptir.",
  "n_reviews": 0,
  "start": 253.98,
  "end": 267.14
 },
 {
  "input": "The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "translatedText": "",
  "from_community_srt": "Z ^ (L) 'ye göre bir ^ (L)' nin türevi sigmoid fonksiyonumuzun bir türevidir, ya da hangi doğrusallığı seçerseniz seçin.",
  "n_reviews": 0,
  "start": 267.84,
  "end": 276.18
 },
 {
  "input": "And the derivative of ZL with respect to WL comes out to be AL-1.",
  "translatedText": "",
  "from_community_srt": "Ve z ^ (L) 'nin w ^ (L)' ye göre türevi, Bu durumda sadece bir ^ (L-1) olduğu ortaya çıkıyor.",
  "n_reviews": 0,
  "start": 277.22,
  "end": 284.66
 },
 {
  "input": "Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all mean.",
  "translatedText": "",
  "from_community_srt": "Şimdi seni bilmiyorum, ama bence bu formüllerde baş aşağı durmak kolay. arkanıza yaslanıp, gerçekte ne anlama geldiklerini kendinize hatırlatın.",
  "n_reviews": 0,
  "start": 285.76,
  "end": 293.42
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "translatedText": "",
  "from_community_srt": "Bu son türev durumunda, bu ağırlığa küçük bir dürtmenin son katmanı etkilediği miktar önceki nöronun ne kadar güçlü olduğuna bağlıdır.",
  "n_reviews": 0,
  "start": 293.92,
  "end": 302.82
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "translatedText": "",
  "from_community_srt": "Unutma, burası “birlikte telleri birleştiren nöronlar” fikrinin geldiği yer.",
  "n_reviews": 0,
  "start": 303.38,
  "end": 308.28
 },
 {
  "input": "And all of this is the derivative with respect to WL only of the cost for a specific single training example.",
  "translatedText": "",
  "from_community_srt": "Ve bunların tümü, sadece belirli bir eğitim örneğinin maliyetinin “^” (L) cinsinden türevidir.",
  "n_reviews": 0,
  "start": 309.2,
  "end": 315.72
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "translatedText": "",
  "from_community_srt": "Tam maliyet işlevi, tüm eğitim maliyetlerinde tüm bu maliyetlerin ortalama alınmasını içerdiğinden, türevi, tüm eğitim örneklerinde bulduğumuz bu ifadenin ortalamasını gerektirir.",
  "n_reviews": 0,
  "start": 316.44,
  "end": 327.46
 },
 {
  "input": "And of course, that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "translatedText": "",
  "from_community_srt": "Ve elbette bu, gradyan vektörünün sadece bir bileşenidir, hangi inşa edilmiştir Maliyetin kısmi türevleri, tüm bu ağırlıklar ve önyargılara göre işlev görür.",
  "n_reviews": 0,
  "start": 328.38,
  "end": 338.26
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "translatedText": "",
  "from_community_srt": "Fakat ihtiyaç duyduğumuz kısmi türevlerden sadece biri olmasına rağmen, işin% 50'sinden fazlası.",
  "n_reviews": 0,
  "start": 340.64,
  "end": 345.26
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "translatedText": "",
  "from_community_srt": "Önyargıya duyarlılık, örneğin, neredeyse aynıdır.",
  "n_reviews": 0,
  "start": 346.34,
  "end": 349.72
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 350.04,
  "end": 355.02
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "translatedText": "",
  "from_community_srt": "Sadece ∂z / ∂b için bu /z / ∂w terimini değiştirmemiz gerekiyor, Ve ilgili formüle bakarsanız, bu türev 1 olur.",
  "n_reviews": 0,
  "start": 358.42,
  "end": 362.4
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "translatedText": "",
  "from_community_srt": "Ayrıca, işte bu, geriye doğru yayılma fikrinin devreye girdiği yerdir. Bu maliyet fonksiyonunun önceki katmanın aktivasyonuna ne kadar hassas olduğunu görebilirsiniz;",
  "n_reviews": 0,
  "start": 366.14,
  "end": 375.74
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight WL.",
  "translatedText": "",
  "from_community_srt": "yani, zincir kuralı genişlemesinde bu ilk türev, z'nin önceki aktivasyona duyarlılığı, w ^ (L) ağırlık olarak çıkıyor.",
  "n_reviews": 0,
  "start": 375.74,
  "end": 385.66
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "translatedText": "",
  "from_community_srt": "Ve yine, bu aktivasyonu doğrudan etkilememize rağmen, takip etmek yararlı olur çünkü şimdi bu zincir kuralı fikrini geriye doğru yinelemeye devam edebiliriz Maliyet fonksiyonunun önceki ağırlıklara ve önceki önyargılara ne kadar hassas olduğunu görmek için.",
  "n_reviews": 0,
  "start": 386.64,
  "end": 402.44
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "translatedText": "",
  "from_community_srt": "Ve bunun çok basit bir örnek olduğunu düşünebilirsiniz. çünkü tüm katmanlar sadece 1 nöron içerdiğinden, ve gerçek ağda işler katlanarak daha da karmaşıklaşacak.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 411.02
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "translatedText": "",
  "from_community_srt": "Ama dürüst olmak gerekirse, katmanlara çoklu nöronlar verdiğimizde pek fazla bir değişiklik olmaz. Gerçekten takip etmesi gereken birkaç endeks var.",
  "n_reviews": 0,
  "start": 411.7,
  "end": 418.86
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "translatedText": "",
  "from_community_srt": "Belirli bir katmanın aktivasyonu yerine, sadece bir ^ (L) olması, Aynı zamanda, o katmanın hangi nöronu olduğunu belirten bir aboneye sahip olacak.",
  "n_reviews": 0,
  "start": 419.38,
  "end": 427.16
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "translatedText": "",
  "from_community_srt": "Devam edelim ve katmanı (L-1) indekslemek için k harfini ve katmanı (L) indekslemek için j harfini kullanalım.",
  "n_reviews": 0,
  "start": 427.16,
  "end": 434.42
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "translatedText": "",
  "from_community_srt": "Maliyet için, yine istenen çıktının ne olduğuna bakarız. Ama bu sefer bu son katman aktivasyonları ile istenen çıktı arasındaki farkların karelerini toplarız.",
  "n_reviews": 0,
  "start": 435.26,
  "end": 445.18
 },
 {
  "input": "That is, you take a sum over ALj minus Yj squared.",
  "translatedText": "",
  "from_community_srt": "Yani, (a_j ^ (L) - y_j) ^ 2 değerinden bir miktar alırsınız.",
  "n_reviews": 0,
  "start": 446.08,
  "end": 450.84
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "translatedText": "",
  "from_community_srt": "Çok fazla ağırlık olduğundan, her birinin nerede olduğunu takip etmek için birkaç endeksi olmalı. Öyleyse bu kthth nöronunu j-th nöronuna w_ {jk} ^ (L) bağlayan kenarın ağırlığını diyelim.",
  "n_reviews": 0,
  "start": 453.04,
  "end": 464.92
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "translatedText": "",
  "from_community_srt": "Bu endeksler ilk başta biraz geri kalmış olabilir. ancak Bölüm 1 videoda bahsettiğim ağırlık matrisini nasıl indeksleyeceğinize göre sıralanıyor.",
  "n_reviews": 0,
  "start": 465.62,
  "end": 473.12
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "translatedText": "",
  "from_community_srt": "Daha önce olduğu gibi, yine z gibi ilgili toplamlara bir isim vermek güzel Böylece son katmanın aktivasyonu, z'ye uygulanan sigmoid gibi sadece sizin özel fonksiyonunuzdur.",
  "n_reviews": 0,
  "start": 473.62,
  "end": 484.16
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "translatedText": "",
  "from_community_srt": "Ne demek istediğimi anlayabilirsin, değil mi? Bunların hepsi esasen, her katman başına bir nöron durumunda daha önce sahip olduğumuz denklemlerdir; sadece biraz daha karmaşık görünüyor.",
  "n_reviews": 0,
  "start": 484.66,
  "end": 493.68
 },
 {
  "input": "And indeed, the chain-ruled derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "translatedText": "",
  "from_community_srt": "Ve gerçekten de, zincir kuralı türevi ifadesi maliyetin belirli bir ağırlığa ne kadar hassas olduğunu açıklamak aslında aynı görünüyor.",
  "n_reviews": 0,
  "start": 495.44,
  "end": 503.42
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "translatedText": "",
  "from_community_srt": "İsterseniz bu terimlerin her birini duraklatmayı ve düşünmeyi size bırakıyorum.",
  "n_reviews": 0,
  "start": 503.92,
  "end": 506.84
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "translatedText": "",
  "from_community_srt": "Yine de burada ne değişiyor? maliyetin katmandaki aktivasyonlardan birine göre türevidir (L-1).",
  "n_reviews": 0,
  "start": 508.98,
  "end": 516.66
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "translatedText": "",
  "from_community_srt": "Bu durumda, fark, nöronun maliyet fonksiyonunu birçok yoldan etkilemesidir.",
  "n_reviews": 0,
  "start": 517.78,
  "end": 522.88
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "translatedText": "",
  "from_community_srt": "Yani, bir yandan, maliyet fonksiyonunda rol oynayan a_0 ^ (L) 'yi etkiler, ancak aynı zamanda maliyet fonksiyonunda da rol oynayan bir ^ ^ (L) üzerinde bir etkiye sahiptir. Ve bunları eklemelisin.",
  "n_reviews": 0,
  "start": 524.68,
  "end": 537.54
 },
 {
  "input": "And that, well, that's pretty much it.",
  "translatedText": "",
  "from_community_srt": "Ve bu ... pekala işte bu kadar.",
  "n_reviews": 0,
  "start": 539.82,
  "end": 543.04
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "translatedText": "",
  "from_community_srt": "Maliyet işlevinin bu ikinci ve son kattaki aktivasyonlara ne kadar hassas olduğunu öğrendikten sonra, bu katmana beslenen tüm ağırlıklar ve önyargılar için işlemi tekrarlayabilirsiniz.",
  "n_reviews": 0,
  "start": 543.5,
  "end": 552.86
 },
 {
  "input": "So pat yourself on the back!",
  "translatedText": "",
  "n_reviews": 0,
  "start": 553.9,
  "end": 554.96
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "translatedText": "",
  "from_community_srt": "Bu yüzden arkana yaslan! Bunların hepsi mantıklı geliyorsa, şimdi geri yayılımın kalbine derin baktın, sinir ağlarının nasıl öğrendiğinin ardındaki işgücü.",
  "n_reviews": 0,
  "start": 555.3,
  "end": 562.68
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "translatedText": "",
  "from_community_srt": "Bu zincir kuralı ifadeleri, gradyandaki her bileşeni belirleyen türevleri verir Bu, ağ kullanımının art arda yokuş aşağı çekilerek maliyetini en aza indirmesine yardımcı olur.",
  "n_reviews": 0,
  "start": 563.3,
  "end": 573.3
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "translatedText": "",
  "from_community_srt": "Hhhhpf. Arkanıza yaslanıp bütün bunları düşünürseniz, aklını sarmak için bir sürü karmaşıklık katmanı var. Bu yüzden zihninizin hepsini sindirmesi zaman alırsa endişelenmeyin.",
  "n_reviews": 0,
  "start": 574.3,
  "end": 582.74
 }
]