[
 {
  "input": "The hard assumption here is that you've watched part 3, giving an intuitive walkthrough of the backpropagation algorithm.",
  "model": "nmt",
  "translatedText": "A kemény feltételezés az, hogy megnézted a 3. részt, amely intuitív áttekintést ad a visszaterjesztési algoritmusról.",
  "time_range": [
   0.0,
   11.16
  ]
 },
 {
  "input": "Here we get a little more formal and dive into the relevant calculus.",
  "model": "nmt",
  "translatedText": "Itt egy kicsit formálisabbak vagyunk, és belemerülünk a vonatkozó számításba.",
  "time_range": [
   11.16,
   14.92
  ]
 },
 {
  "input": "It's normal for this to be at least a little confusing, so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.",
  "model": "nmt",
  "translatedText": "Normális, hogy ez legalább egy kicsit zavaró, így a rendszeres szünet és töprengés mantra itt is ugyanúgy érvényes, mint bárhol máshol.",
  "time_range": [
   14.92,
   22.0
  ]
 },
 {
  "input": "Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has a different feel from how most introductory calculus courses approach the subject.",
  "model": "nmt",
  "translatedText": "Fő célunk, hogy bemutassuk, hogyan gondolkodnak a gépi tanulásban részt vevők általában a hálózatok kontextusában a számításból származó láncszabályról, amely másképp hat, mint a legtöbb bevezető számítástechnikai kurzus megközelítése a témához.",
  "time_range": [
   22.0,
   34.58
  ]
 },
 {
  "input": "For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic.",
  "model": "nmt",
  "translatedText": "Azok számára, akik nem érzik jól magukat a vonatkozó kalkulusban, egy egész sorozatom van a témában.",
  "time_range": [
   34.58,
   39.3
  ]
 },
 {
  "input": "Let's start off with an extremely simple network, one where each layer has a single neuron in it.",
  "model": "nmt",
  "translatedText": "Kezdjük egy rendkívül egyszerű hálózattal, ahol minden rétegben egyetlen neuron található.",
  "time_range": [
   39.3,
   46.78
  ]
 },
 {
  "input": "This network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables.",
  "model": "nmt",
  "translatedText": "Ezt a hálózatot három súlyozás és három torzítás határozza meg, és célunk annak megértése, hogy a költségfüggvény mennyire érzékeny ezekre a változókra.",
  "time_range": [
   46.78,
   55.64
  ]
 },
 {
  "input": "That way we know which adjustments to those terms will cause the most efficient decrease to the cost function.",
  "model": "nmt",
  "translatedText": "Így tudjuk, hogy ezeknek a feltételeknek mely módosításai okozzák a költségfüggvény leghatékonyabb csökkentését.",
  "time_range": [
   55.64,
   61.1
  ]
 },
 {
  "input": "We'll just focus on the connection between the last two neurons.",
  "model": "nmt",
  "translatedText": "Csak az utolsó két neuron közötti kapcsolatra koncentrálunk.",
  "time_range": [
   61.1,
   65.36
  ]
 },
 {
  "input": "Let's label the activation of that last neuron with a superscript L, indicating which layer it's in.",
  "model": "nmt",
  "translatedText": "Jelöljük az utolsó neuron aktiválását L felső indexszel, jelezve, hogy melyik rétegben van.",
  "time_range": [
   65.36,
   71.8
  ]
 },
 {
  "input": "So the activation of the previous neuron is AL-1.",
  "model": "nmt",
  "translatedText": "Tehát az előző neuron aktiválása AL-1.",
  "time_range": [
   71.8,
   76.56
  ]
 },
 {
  "input": "These are not exponents, they're just a way of indexing what we're talking about, since I want to save subscripts for different indices later on.",
  "model": "nmt",
  "translatedText": "Ezek nem exponensek, csak egy módja annak, hogy indexeljük azt, amiről beszélünk, mivel a későbbiekben szeretném elmenteni az alsó indexeket a különböző indexekhez.",
  "time_range": [
   76.56,
   83.6
  ]
 },
 {
  "input": "Let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1.",
  "model": "nmt",
  "translatedText": "Tegyük fel, hogy egy adott képzési példánál az utolsó aktiválás értéke y, például y lehet 0 vagy 1.",
  "time_range": [
   83.6,
   93.02
  ]
 },
 {
  "input": "So the cost of this network for a single training example is AL-y2.",
  "model": "nmt",
  "translatedText": "Tehát ennek a hálózatnak a költsége egyetlen képzési példa esetében AL-y2.",
  "time_range": [
   93.02,
   99.04
  ]
 },
 {
  "input": "We'll denote the cost of that one training example as c0.",
  "model": "nmt",
  "translatedText": "Ennek az egy képzési példának a költségét c0-val jelöljük.",
  "time_range": [
   99.04,
   106.12
  ]
 },
 {
  "input": "As a reminder, this last activation is determined by a weight, which I'm going to call wL, times the previous neuron's activation plus some bias, which I'll call bL.",
  "model": "nmt",
  "translatedText": "Emlékeztetőül, ezt az utolsó aktiválást egy súly határozza meg, amelyet wL-nek fogok nevezni, szorozva az előző neuron aktiválódásával, plusz némi torzítással, amit bL-nek nevezek.",
  "time_range": [
   106.12,
   117.6
  ]
 },
 {
  "input": "Then you pump that through some special nonlinear function like the sigmoid or ReLU.",
  "model": "nmt",
  "translatedText": "Ezután ezt valamilyen speciális nemlineáris függvényen keresztül pumpálja, mint például a szigmoid vagy a ReLU.",
  "time_range": [
   117.6,
   121.56
  ]
 },
 {
  "input": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z, with the same superscript as the relevant activations.",
  "model": "nmt",
  "translatedText": "Valójában megkönnyíti a dolgunkat, ha ennek a súlyozott összegnek külön nevet adunk, például z-t, ugyanazzal a felső indexszel, mint a vonatkozó aktiválások.",
  "time_range": [
   121.56,
   130.6
  ]
 },
 {
  "input": "This is a lot of terms, and a way you might conceptualize it is that the weight, previous action, and bias all together are used to compute z, which in turn lets us compute a, which finally, along with a constant y, lets us compute the cost.",
  "model": "nmt",
  "translatedText": "Ez egy csomó kifejezés, és úgy lehet elképzelni, hogy a súlyt, az előző műveletet és a torzítást együtt használják a z kiszámítására, ami viszont lehetővé teszi számunkra a kiszámítását, ami végül egy konstans y-val együtt lehetővé teszi kiszámoljuk a költségeket.",
  "time_range": [
   130.6,
   147.36
  ]
 },
 {
  "input": "And of course, AL-1 is influenced by its own weight and bias and such, but we're not going to focus on that right now.",
  "model": "nmt",
  "translatedText": "És természetesen az AL-1-et befolyásolja a saját súlya, elfogultsága és hasonlók, de most nem erre fogunk koncentrálni.",
  "time_range": [
   147.36,
   155.92
  ]
 },
 {
  "input": "All of these are just numbers, right?",
  "model": "nmt",
  "translatedText": "Ezek mind csak számok, igaz?",
  "time_range": [
   155.92,
   158.12
  ]
 },
 {
  "input": "And it can be nice to think of each one as having its own little number line.",
  "model": "nmt",
  "translatedText": "És jó lehet úgy gondolni, hogy mindegyiknek megvan a maga kis számsora.",
  "time_range": [
   158.12,
   161.96
  ]
 },
 {
  "input": "Our first goal is to understand how sensitive the cost function is to small changes in our weight wL.",
  "model": "nmt",
  "translatedText": "Első célunk annak megértése, hogy a költségfüggvény mennyire érzékeny a súlyunk kis változásaira wL.",
  "time_range": [
   161.96,
   169.82
  ]
 },
 {
  "input": "Or phrase differently, what is the derivative of c with respect to wL?",
  "model": "nmt",
  "translatedText": "Vagy fogalmazz másképp, mi a c deriváltja wL-hez képest?",
  "time_range": [
   169.82,
   175.74
  ]
 },
 {
  "input": "When you see this del w term, think of it as meaning some tiny nudge to w, like a change by 0.",
  "model": "nmt",
  "translatedText": "Ha látja ezt a del w kifejezést, gondolja úgy, hogy ez egy apró lökést jelent w-hez, például 0-val való változtatást.",
  "time_range": [
   175.74,
   181.55406593406593
  ]
 },
 {
  "input": "01, and think of this del c term as meaning whatever the resulting nudge to the cost is.",
  "model": "nmt",
  "translatedText": "01, és gondolja úgy, hogy ez a del c kifejezés bármit is jelent a költségekhez képest.",
  "time_range": [
   181.55406593406593,
   188.82
  ]
 },
 {
  "input": "What we want is their ratio.",
  "model": "nmt",
  "translatedText": "Amit szeretnénk, az az arányuk.",
  "time_range": [
   188.82,
   190.9
  ]
 },
 {
  "input": "Conceptually, this tiny nudge to wL causes some nudge to zL, which in turn causes some nudge to AL, which directly influences the cost.",
  "model": "nmt",
  "translatedText": "Elméletileg ez az apró lökés a wL felé némi lökést okoz a zL-nek, ami viszont némi lökést okoz az AL-nak, ami közvetlenül befolyásolja a költségeket.",
  "time_range": [
   190.9,
   203.22
  ]
 },
 {
  "input": "So we break things up by first looking at the ratio of a tiny change to zL to this tiny change w, that is, the derivative of zL with respect to wL.",
  "model": "nmt",
  "translatedText": "Tehát a dolgokat úgy bontjuk szét, hogy először megnézzük a zL-hez viszonyított apró változás és ehhez a kis w változáshoz viszonyított arányát, vagyis zL deriváltját wL-hez képest.",
  "time_range": [
   203.22,
   213.34
  ]
 },
 {
  "input": "Likewise, you then consider the ratio of the change to AL to the tiny change in zL that caused it, as well as the ratio between the final nudge to c and this intermediate nudge to AL.",
  "model": "nmt",
  "translatedText": "Hasonlóképpen, akkor vegye figyelembe az AL-hoz való változás és a zL-ben bekövetkezett apró változás arányát, amely ezt okozta, valamint a c-hez való végső lökést és az AL-hez való közbenső lökést közötti arányt.",
  "time_range": [
   213.34,
   225.9
  ]
 },
 {
  "input": "This right here is the chain rule, where multiplying these three ratios gives us the sensitivity of c to small changes in wL.",
  "model": "nmt",
  "translatedText": "Ez itt a láncszabály, ahol ezt a három arányt megszorozva megkapjuk c érzékenységét a wL kis változásaira.",
  "time_range": [
   225.9,
   237.34
  ]
 },
 {
  "input": "So on screen right now, there's a lot of symbols, and take a moment to make sure it's clear what they all are, because now we're going to compute the relevant derivatives.",
  "model": "nmt",
  "translatedText": "Tehát a képernyőn jelenleg nagyon sok szimbólum van, és szánjon egy percet, hogy megbizonyosodjon arról, hogy világos, mi ez, mert most a vonatkozó származékokat fogjuk kiszámítani.",
  "time_range": [
   237.34,
   247.46
  ]
 },
 {
  "input": "The derivative of c with respect to AL works out to be 2AL-y.",
  "model": "nmt",
  "translatedText": "A c deriváltja az AL-hez viszonyítva 2AL-y.",
  "time_range": [
   247.46,
   254.22
  ]
 },
 {
  "input": "This means its size is proportional to the difference between the network's output and the thing we want it to be, so if that output was very different, even slight changes stand to have a big impact on the final cost function.",
  "model": "nmt",
  "translatedText": "Ez azt jelenti, hogy mérete arányos a hálózat kimenete és a kívánt dolog közötti különbséggel, tehát ha ez a kimenet nagyon eltérő volt, akkor még az enyhe változtatások is nagy hatással vannak a végső költségfüggvényre.",
  "time_range": [
   254.22,
   268.38
  ]
 },
 {
  "input": "The derivative of AL with respect to zL is just the derivative of our sigmoid function, or whatever nonlinearity you choose to use.",
  "model": "nmt",
  "translatedText": "Az AL zL-hez viszonyított deriváltja csak a szigmoid függvényünk deriváltja, vagy bármilyen nemlinearitás, amelyet használni szeretne.",
  "time_range": [
   268.38,
   277.42
  ]
 },
 {
  "input": "The derivative of zL with respect to wL comes out to be AL-1.",
  "model": "nmt",
  "translatedText": "A zL deriváltja wL-hez viszonyítva AL-1 lesz.",
  "time_range": [
   277.42,
   286.18
  ]
 },
 {
  "input": "I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself what they all mean.",
  "model": "nmt",
  "translatedText": "Nem tudom, ti hogy vagytok vele, de azt hiszem, könnyű beleragadni a képletekbe anélkül, hogy egy pillanatra is hátradőlne, és emlékezne, mit jelentenek ezek.",
  "time_range": [
   286.18,
   294.18
  ]
 },
 {
  "input": "In the case of this last derivative, the amount that the small nudge to the weight influenced the last layer depends on how strong the previous neuron is.",
  "model": "nmt",
  "translatedText": "Ennek az utolsó származéknak az esetében, hogy a súlyhoz való kis lökések milyen mértékben befolyásolták az utolsó réteget, attól függ, milyen erős az előző neuron.",
  "time_range": [
   294.18,
   303.22
  ]
 },
 {
  "input": "Remember, this is where the neurons-that-fire-together-wire-together idea comes in.",
  "model": "nmt",
  "translatedText": "Ne feledje, itt jön a képbe a neuronok-az-együtt tüzel-huzal-összeköttetés ötlet.",
  "time_range": [
   303.22,
   309.32
  ]
 },
 {
  "input": "And all of this is the derivative with respect to wL only of the cost for a specific single training example.",
  "model": "nmt",
  "translatedText": "És mindez a wL vonatkozásában csak egy konkrét képzési példa költségének deriváltja.",
  "time_range": [
   309.32,
   316.58
  ]
 },
 {
  "input": "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression over all training examples.",
  "model": "nmt",
  "translatedText": "Mivel a teljes költségfüggvény magában foglalja az összes költségnek a sok különböző betanítási példában való együttes átlagolását, a származéka megköveteli ennek a kifejezésnek az átlagolását az összes képzési példában.",
  "time_range": [
   316.58,
   328.54
  ]
 },
 {
  "input": "Of course, that's just one component of the gradient vector, which is built up from the partial derivatives of the cost function with respect to all those weights and biases.",
  "model": "nmt",
  "translatedText": "Természetesen ez csak egy komponense a gradiensvektornak, amely a költségfüggvény parciális deriváltjaiból épül fel, tekintettel az összes súlyozásra és torzításra.",
  "time_range": [
   328.54,
   340.78
  ]
 },
 {
  "input": "But even though that's just one of the many partial derivatives we need, it's more than 50% of the work.",
  "model": "nmt",
  "translatedText": "De bár ez csak egy a sok részleges származék közül, amelyekre szükségünk van, ez a munka több mint 50%-a.",
  "time_range": [
   340.78,
   346.46
  ]
 },
 {
  "input": "The sensitivity to the bias, for example, is almost identical.",
  "model": "nmt",
  "translatedText": "A torzításra való érzékenység például majdnem azonos.",
  "time_range": [
   346.46,
   350.3
  ]
 },
 {
  "input": "We just need to change out this del z del w term for a del z del b.",
  "model": "nmt",
  "translatedText": "Csak ki kell cserélnünk ezt a del z del w kifejezést egy del z del b-re.",
  "time_range": [
   350.3,
   358.98
  ]
 },
 {
  "input": "And if you look at the relevant formula, that derivative comes out to be 1.",
  "model": "nmt",
  "translatedText": "És ha megnézzük a vonatkozó képletet, a származéka 1 lesz.",
  "time_range": [
   358.98,
   364.7
  ]
 },
 {
  "input": "Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer.",
  "model": "nmt",
  "translatedText": "Illetve, és itt jön be a visszafelé terjedés ötlete, láthatjuk, hogy ez a költségfüggvény mennyire érzékeny az előző réteg aktiválására.",
  "time_range": [
   364.7,
   376.18
  ]
 },
 {
  "input": "Namely, this initial derivative in the chain rule expression, the sensitivity of z to the previous activation, comes out to be the weight wL.",
  "model": "nmt",
  "translatedText": "Ugyanis ez a kezdeti derivált a láncszabály kifejezésben, a z érzékenysége az előző aktiválásra, a wL súlynak jön ki.",
  "time_range": [
   376.18,
   385.42
  ]
 },
 {
  "input": "And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of, because now we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases.",
  "model": "nmt",
  "translatedText": "És még egyszer, bár nem leszünk képesek közvetlenül befolyásolni az előző réteg aktiválását, hasznos nyomon követni, mert most már csak ismételhetjük ugyanezt a láncszabály-ötletet visszafelé, hogy meglássuk, mennyire érzékeny a költségfüggvény korábbi súlyok és korábbi torzítások.",
  "time_range": [
   385.42,
   403.68
  ]
 },
 {
  "input": "And you might think this is an overly simple example, since all layers have one neuron, and things are going to get exponentially more complicated for a real network.",
  "model": "nmt",
  "translatedText": "És azt gondolhatja, hogy ez egy túlságosan egyszerű példa, mivel minden rétegnek van egy neuronja, és a dolgok exponenciálisan bonyolultabbak lesznek egy valódi hálózat esetében.",
  "time_range": [
   403.68,
   411.32
  ]
 },
 {
  "input": "But honestly, not that much changes when we give the layers multiple neurons, really it's just a few more indices to keep track of.",
  "model": "nmt",
  "translatedText": "De őszintén szólva, nem sok változás történik, ha több neuront adunk a rétegeknek, valójában csak néhány indexet kell követni.",
  "time_range": [
   411.32,
   419.32
  ]
 },
 {
  "input": "Rather than the activation of a given layer simply being AL, it's also going to have a subscript indicating which neuron of that layer it is.",
  "model": "nmt",
  "translatedText": "Ahelyett, hogy egy adott réteg aktiválása egyszerűen AL lenne, annak egy alsó indexe is lesz, amely jelzi, hogy az adott réteg melyik neuronjáról van szó.",
  "time_range": [
   419.32,
   427.92
  ]
 },
 {
  "input": "Let's use the letter k to index the layer L-1, and j to index the layer L.",
  "model": "nmt",
  "translatedText": "Használjuk a k betűt az L-1 réteg, a j betűt pedig az L réteg indexeléséhez.",
  "time_range": [
   427.92,
   435.28
  ]
 },
 {
  "input": "For the cost, again we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output.",
  "model": "nmt",
  "translatedText": "A költségekhez ismét megnézzük, hogy mi a kívánt kimenet, de ezúttal összeadjuk az utolsó rétegaktiválások és a kívánt kimenet közötti különbségek négyzetét.",
  "time_range": [
   435.28,
   446.12
  ]
 },
 {
  "input": "That is, you take a sum over ALj minus yj squared.",
  "model": "nmt",
  "translatedText": "Vagyis veszel egy összeget ALj mínusz yj négyzetével.",
  "time_range": [
   446.12,
   453.28
  ]
 },
 {
  "input": "Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is, so let's call the weight of the edge connecting this kth neuron to the jth neuron, WLjk.",
  "model": "nmt",
  "translatedText": "Mivel sokkal több a súlyozás, mindegyiknek több indexnek kell lennie, hogy nyomon tudja követni, hol van, ezért nevezzük a k-adik neuront a j-edik neuronnal összekötő él súlyát WLjk-nek.",
  "time_range": [
   453.28,
   465.74
  ]
 },
 {
  "input": "Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix I talked about in the part 1 video.",
  "model": "nmt",
  "translatedText": "Ezek az indexek eleinte kissé elmaradottaknak tűnhetnek, de ez összhangban van azzal, hogyan indexelné a súlymátrixot, amelyről az 1. rész videójában beszéltem.",
  "time_range": [
   465.74,
   473.8
  ]
 },
 {
  "input": "Just as before, it's still nice to give a name to the relevant weighted sum, like z, so that the activation of the last layer is just your special function, like the sigmoid, applied to z.",
  "model": "nmt",
  "translatedText": "Csakúgy, mint korábban, továbbra is jó nevet adni a megfelelő súlyozott összegnek, például z-nek, így az utolsó réteg aktiválása csak a speciális függvény, mint a z-re alkalmazott szigmoid.",
  "time_range": [
   473.8,
   484.98
  ]
 },
 {
  "input": "You can see what I mean, where all of these are essentially the same equations we had before in the one-neuron-per-layer case, it's just that it looks a little more complicated.",
  "model": "nmt",
  "translatedText": "Láthatja, mire gondolok, ahol ezek lényegében ugyanazok az egyenletek, mint korábban a rétegenkénti egy neuron esetében, csak ez egy kicsit bonyolultabbnak tűnik.",
  "time_range": [
   484.98,
   495.42
  ]
 },
 {
  "input": "And indeed, the chain rule derivative expression describing how sensitive the cost is to a specific weight looks essentially the same.",
  "model": "nmt",
  "translatedText": "És valóban, a láncszabály derivált kifejezés, amely leírja, hogy a költség mennyire érzékeny egy adott súlyra, lényegében ugyanúgy néz ki.",
  "time_range": [
   495.42,
   503.54
  ]
 },
 {
  "input": "I'll leave it to you to pause and think about each of those terms if you want.",
  "model": "nmt",
  "translatedText": "Meghagyom neked, hogy állj meg, és gondold át mindegyik kifejezést, ha akarod.",
  "time_range": [
   503.54,
   509.42
  ]
 },
 {
  "input": "What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L-1.",
  "model": "nmt",
  "translatedText": "Ami azonban itt változik, az az L-1 réteg egyik aktiválásának költségének deriváltja.",
  "time_range": [
   509.42,
   517.82
  ]
 },
 {
  "input": "In this case, the difference is that the neuron influences the cost function through multiple different paths.",
  "model": "nmt",
  "translatedText": "Ebben az esetben a különbség az, hogy a neuron több különböző úton befolyásolja a költségfüggvényt.",
  "time_range": [
   517.82,
   523.54
  ]
 },
 {
  "input": "That is, on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up.",
  "model": "nmt",
  "translatedText": "Vagyis egyrészt befolyásolja az AL0-t, ami a költségfüggvényben játszik szerepet, de hatással van az AL1-re is, ami szintén szerepet játszik a költségfüggvényben, és ezeket össze kell adni.",
  "time_range": [
   523.54,
   540.34
  ]
 },
 {
  "input": "And that, well, that's pretty much it.",
  "model": "nmt",
  "translatedText": "És nagyjából ennyi.",
  "time_range": [
   540.34,
   543.68
  ]
 },
 {
  "input": "Once you know how sensitive the cost function is to the activations in this second-to-last layer, you can just repeat the process for all the weights and biases feeding into that layer.",
  "model": "nmt",
  "translatedText": "Ha tudja, hogy a költségfüggvény mennyire érzékeny az utolsó előtti réteg aktiválásaira, megismételheti a folyamatot az adott rétegbe betáplált összes súlyozásra és torzításra.",
  "time_range": [
   543.68,
   553.92
  ]
 },
 {
  "input": "So pat yourself on the back!",
  "model": "nmt",
  "translatedText": "Szóval veregesd meg magad!",
  "time_range": [
   553.92,
   555.42
  ]
 },
 {
  "input": "If all of this makes sense, you have now looked deep into the heart of backpropagation, the workhorse behind how neural networks learn.",
  "model": "nmt",
  "translatedText": "Ha mindennek van értelme, akkor most mélyen belenézett a backpropagation szívébe, a neurális hálózatok tanulási folyamatának igáslójába.",
  "time_range": [
   555.42,
   563.7
  ]
 },
 {
  "input": "These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill.",
  "model": "nmt",
  "translatedText": "Ezek a láncszabály-kifejezések megadják azokat a származékokat, amelyek meghatározzák a gradiens egyes komponenseit, amelyek segítenek minimalizálni a hálózat költségeit azáltal, hogy ismételten lefelé lépkednek.",
  "time_range": [
   563.7,
   575.02
  ]
 },
 {
  "input": "If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around, so don't worry if it takes time for your mind to digest it all.",
  "model": "nmt",
  "translatedText": "Ha hátradől, és végiggondolja az egészet, akkor ez egy csomó összetett réteg körül kell járnia az elméjének, szóval ne aggódjon, ha időbe telik, amíg az elméje megemészti az egészet.",
  "time_range": [
   575.02,
   578.96
  ]
 }
]