[
 {
  "input": "In the last videos I talked about the derivatives of simple functions, and the goal was to have a clear picture or intuition to hold in your mind that actually explains where these formulas come from.",
  "translatedText": "지난 비디오에서 나는 단순 함수의 파생물에 대해 이야기했고, 목표는 이러한 공식이 실제로 어디서 나오는지 설명하는 명확한 그림이나 직관을 마음 속에 담아두는 것이었습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 14.5,
  "end": 26.2
 },
 {
  "input": "But most of the functions you deal with in modeling the world involve mixing, combining, or tweaking these simple functions in some other way, so our next step is to understand how you take derivatives of more complicated combinations.",
  "translatedText": "그러나 세계 모델링에서 다루는 대부분의 기능에는 이러한 간단한 기능을 다른 방식으로 혼합, 결합 또는 조정하는 작업이 포함되므로 다음 단계는 더 복잡한 조합의 파생물을 얻는 방법을 이해하는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 26.84,
  "end": 40.54
 },
 {
  "input": "Again, I don't want these to be something to memorize, I want you to have a clear picture in mind for where each one comes from.",
  "translatedText": "다시 말하지만, 저는 이것들을 외워야 할 것이 아니라, 여러분이 각각의 출처가 어디인지에 대해 명확한 그림을 갖고 있기를 바랍니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 41.28,
  "end": 47.6
 },
 {
  "input": "Now, this really boils down into three basic ways to combine functions.",
  "translatedText": "이제 기능을 결합하는 세 가지 기본 방법으로 요약할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 49.52,
  "end": 53.6
 },
 {
  "input": "You can add them together, you can multiply them, and you can throw one inside the other, known as composing them.",
  "translatedText": "그것들을 더할 수도 있고, 곱할 수도 있고, 하나를 다른 하나 안에 넣을 수도 있습니다. 이를 구성이라고 합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 54.1,
  "end": 59.78
 },
 {
  "input": "Sure, you could say subtracting them, but really that's just multiplying the second by negative one and adding them together.",
  "translatedText": "물론, 뺄셈이라고 말할 수도 있지만 실제로는 두 번째 값에 음수 값을 곱하고 더하는 것뿐입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 60.6,
  "end": 67.22
 },
 {
  "input": "Likewise, dividing functions doesn't really add anything, because that's the same as plugging one inside the function, one over x, and then multiplying the two together.",
  "translatedText": "마찬가지로 함수를 나누는 것은 함수 안에 하나를 더하고, 하나를 x에 더한 다음 둘을 곱하는 것과 같기 때문에 실제로는 아무것도 추가되지 않습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 68.24,
  "end": 76.76
 },
 {
  "input": "So really, most functions you come across just involve layering together these three different types of combinations, though there's not really a bound on how monstrous things can become.",
  "translatedText": "따라서 실제로 접하는 대부분의 기능은 이 세 가지 유형의 조합을 겹쳐서 사용하는 것이지만, 얼마나 괴물이 될 수 있는지에 대한 제한은 없습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 77.66,
  "end": 86.44
 },
 {
  "input": "But as long as you know how derivatives play with just those three combination types, you'll always be able to take it step by step and peel through the layers for any kind of monstrous expression.",
  "translatedText": "그러나 이 세 가지 조합 유형만으로 파생 상품이 어떻게 작동하는지 아는 한, 항상 단계별로 진행하여 모든 종류의 괴물 같은 표현을 위해 레이어를 벗겨낼 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 87.1,
  "end": 96.72
 },
 {
  "input": "So the question is, if you know the derivative of two functions, what is the derivative of their sum, of their product, and of the function composition between them?",
  "translatedText": "따라서 두 함수의 미분을 알고 있다면, 그 합과 곱, 그리고 두 함수 사이의 함수 구성의 미분은 무엇일까요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 98.72,
  "end": 108.42
 },
 {
  "input": "The sum rule is easiest, if somewhat tongue-twisting to say out loud.",
  "translatedText": "큰 소리로 말하기가 다소 어색하더라도 합계 규칙이 가장 쉽습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.32,
  "end": 114.26
 },
 {
  "input": "The derivative of a sum of two functions is the sum of their derivatives.",
  "translatedText": "두 함수의 합의 미분은 해당 미분의 합입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 114.84,
  "end": 118.6
 },
 {
  "input": "But it's worth warming up with this example by really thinking through what it means to take a derivative of a sum of two functions, since the derivative patterns for products and function composition won't be so straightforward, and they're going to require this kind of deeper thinking.",
  "translatedText": "하지만 두 함수의 합에 대한 도함수를 구하는 것이 무엇을 의미하는지 곰곰이 생각해 보면 이 예를 통해 워밍업할 가치가 있습니다. 곱셈과 함수 구성에 대한 도함수 패턴은 그렇게 간단하지 않고 이런 유형의 함수가 필요하기 때문입니다. 더 깊은 생각.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 119.8,
  "end": 135.62
 },
 {
  "input": "For example, let's think about this function f of x equals sine of x plus x squared.",
  "translatedText": "예를 들어, 이 함수 f(x)는 사인(x) 더하기 x 제곱과 같다고 생각해 봅시다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 136.7,
  "end": 141.2
 },
 {
  "input": "It's a function where, for every input, you add together the values of sine of x and x squared at that point.",
  "translatedText": "이는 모든 입력에 대해 x의 사인 값과 해당 지점의 x 제곱을 더하는 함수입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.2,
  "end": 147.96
 },
 {
  "input": "For example, let's say at x equals 0.5, the height of the sine graph is given by this vertical bar, and the height of the x squared parabola is given by this slightly smaller vertical bar.",
  "translatedText": "예를 들어 x가 0이라고 가정해 보겠습니다.도 5에서, 사인 그래프의 높이는 이 수직 막대에 의해 주어지고, x 제곱 포물선의 높이는 이 작은 수직 막대에 의해 주어진다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 149.76,
  "end": 162.56
 },
 {
  "input": "And their sum is the length you get by just stacking them together.",
  "translatedText": "그리고 그 합은 그것들을 함께 쌓아서 얻는 길이입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 164.38,
  "end": 167.32
 },
 {
  "input": "For the derivative, you want to ask what happens as you nudge that input slightly, maybe increasing it up to 0.5 plus dx.",
  "translatedText": "미분의 경우 해당 입력을 약간 움직여서 0까지 늘리면 어떤 일이 발생하는지 묻고 싶습니다.5 더하기 dx.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 168.52,
  "end": 176.42
 },
 {
  "input": "The difference in the value of f between those two places is what we call df.",
  "translatedText": "이 두 위치 사이의 f 값의 차이를 우리는 df라고 부릅니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 177.56,
  "end": 182.92
 },
 {
  "input": "And when you picture it like this, I think you'll agree that the total change in the height is whatever the change to the sine graph is, what we might call d sine of x, plus whatever the change to x squared is, dx squared.",
  "translatedText": "그리고 여러분이 이것을 이렇게 묘사할 때, 높이의 전체 변화는 사인 그래프의 변화, 즉 x의 d 사인이라고 부르는 것과 x 제곱의 변화가 무엇이든 dx라는 점에 동의하실 것입니다. 제곱.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 184.36,
  "end": 198.8
 },
 {
  "input": "We know that the derivative of sine is cosine, and remember what that means.",
  "translatedText": "우리는 사인의 도함수가 코사인이라는 것을 알고 있으며, 이것이 무엇을 의미하는지 기억하고 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 202.24,
  "end": 207.54
 },
 {
  "input": "It means that this little change, d sine of x, is about cosine of x times dx.",
  "translatedText": "이는 x의 d 사인이라는 이 작은 변화가 대략 코사인 x 곱하기 dx와 같다는 것을 의미합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 207.92,
  "end": 213.3
 },
 {
  "input": "It's proportional to the size of our initial nudge dx, and the proportionality constant equals cosine of whatever input we started at.",
  "translatedText": "이는 초기 넛지 dx의 크기에 비례하며 비례 상수는 우리가 시작한 모든 입력의 코사인과 같습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 213.78,
  "end": 223.36
 },
 {
  "input": "Likewise, because the derivative of x squared is 2x, the change in the height of the x squared graph is 2x times whatever dx was.",
  "translatedText": "마찬가지로 x 제곱의 도함수는 2x이므로 x 제곱 그래프의 높이 변화는 dx의 2배입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 223.98,
  "end": 233.94
 },
 {
  "input": "So rearranging df divided by dx, the ratio of the tiny change to the sum function to the tiny change in x that caused it, is indeed cosine of x plus 2x, the sum of the derivatives of its parts.",
  "translatedText": "따라서 작은 변화의 합 함수에 대한 작은 변화와 그 원인이 된 x의 작은 변화의 비율인 df를 dx로 다시 정렬하면 실제로 x에 그 부분의 미분의 합인 2x를 더한 코사인이 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 235.6,
  "end": 250.08
 },
 {
  "input": "But like I said, things are a bit different for products, and let's think through why in terms of tiny nudges again.",
  "translatedText": "하지만 제가 말했듯이 제품마다 상황이 조금 다르므로 다시 작은 넛지 측면에서 그 이유를 생각해 보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 251.52,
  "end": 259.14
 },
 {
  "input": "In this case, I don't think graphs are our best bet for visualizing things.",
  "translatedText": "이 경우 그래프가 사물을 시각화하는 데 가장 좋은 방법은 아니라고 생각합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 260.06,
  "end": 263.14
 },
 {
  "input": "Pretty commonly in math, at a lot of levels of math really, if you're dealing with a product of two things, it helps to understand it as some kind of area.",
  "translatedText": "수학에서 흔히 볼 수 있는 일입니다. 수학의 많은 수준에서 두 가지의 곱을 다루는 경우 이를 일종의 영역으로 이해하는 것이 도움이 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 263.82,
  "end": 272.14
 },
 {
  "input": "In this case, maybe you try to configure some mental setup of a box where the side lengths are sine of x and x squared.",
  "translatedText": "이 경우 측면 길이가 x와 x 제곱의 사인인 상자의 정신적 설정을 구성하려고 할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 273.08,
  "end": 279.0
 },
 {
  "input": "But what would that mean?",
  "translatedText": "하지만 그게 무슨 뜻일까요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 279.88,
  "end": 281.04
 },
 {
  "input": "Well, since these are functions, you might think of those sides as adjustable, dependent on the value of x, which maybe you think of as this number that you can just freely adjust up and down.",
  "translatedText": "글쎄요, 이것들은 함수이기 때문에 x 값에 따라 조정 가능한 변이라고 생각할 수도 있습니다. x 값을 자유롭게 위아래로 조정할 수 있는 숫자라고 생각할 수도 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 282.32,
  "end": 292.74
 },
 {
  "input": "So getting a feel for what this means, focus on that top side who changes as the function sine of x.",
  "translatedText": "따라서 이것이 의미하는 바를 이해하려면 x의 함수 사인으로 변경되는 윗면에 집중하세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 293.74,
  "end": 300.14
 },
 {
  "input": "As you change this value of x up from 0, it increases up to a length of 1 as sine of x moves up towards its peak, and after that it starts to decrease as sine of x comes down from 1.",
  "translatedText": "x의 값을 0에서 위로 변경하면 x의 사인이 최고점을 향해 올라갈 때 길이가 1까지 증가하고 그 후 x의 사인이 1에서 내려갈 때 길이가 감소하기 시작합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 301.06,
  "end": 313.94
 },
 {
  "input": "And in the same way, that height there is always changing as x squared.",
  "translatedText": "그리고 같은 방식으로, 그 높이는 항상 x 제곱으로 변합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 315.1,
  "end": 318.58
 },
 {
  "input": "So f of x, defined as the product of these two functions, is the area of this box.",
  "translatedText": "따라서 이 두 함수의 곱으로 정의되는 x의 f는 이 상자의 면적입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 320.08,
  "end": 325.8
 },
 {
  "input": "And for the derivative, let's think about how a tiny change to x by dx influences that area.",
  "translatedText": "그리고 미분의 경우, x x dx의 작은 변화가 해당 영역에 어떻게 영향을 미치는지 생각해 봅시다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 327.06,
  "end": 333.18
 },
 {
  "input": "What is that resulting change in area df?",
  "translatedText": "면적 df의 결과적인 변화는 무엇입니까?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 333.84,
  "end": 336.28
 },
 {
  "input": "Well, the nudge dx caused that width to change by some small d sine of x, and it caused that height to change by some dx squared.",
  "translatedText": "넛지 dx로 인해 너비가 x의 작은 d 사인만큼 변하고 높이가 dx 제곱만큼 변했습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 339.0,
  "end": 347.92
 },
 {
  "input": "And this gives us three little snippets of new area, a thin rectangle on the bottom whose area is its width, sine of x, times its thin height, dx squared.",
  "translatedText": "이렇게 하면 새로운 영역의 세 가지 작은 조각, 즉 하단의 얇은 직사각형의 너비, x의 사인에 얇은 높이의 제곱인 dx 제곱의 면적을 얻을 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 350.18,
  "end": 360.26
 },
 {
  "input": "And there's this thin rectangle on the right, whose area is its height, x squared, times its thin width, d sine of x.",
  "translatedText": "그리고 오른쪽에 얇은 직사각형이 있는데, 이 직사각형의 면적은 높이 x의 제곱에 얇은 너비 x의 사인인 d를 곱한 값입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 361.78,
  "end": 369.3
 },
 {
  "input": "And there's also this little bit in the corner, but we can ignore that.",
  "translatedText": "그리고 구석에 약간의 문제가 있지만 무시해도 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 370.74,
  "end": 374.14
 },
 {
  "input": "Its area is ultimately proportional to dx squared, and as we've seen before, that becomes negligible as dx goes to zero.",
  "translatedText": "그 면적은 궁극적으로 dx 제곱에 비례하며, 앞서 살펴본 것처럼 dx가 0이 되면 무시할 수 있는 크기가 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 374.44,
  "end": 382.48
 },
 {
  "input": "I mean, this whole setup is very similar to what I showed last video, with the x squared diagram.",
  "translatedText": "이 전체 설정은 지난 동영상에서 보여드린 X 제곱 다이어그램과 매우 유사합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 383.94,
  "end": 388.7
 },
 {
  "input": "And just like then, keep in mind that I'm using somewhat beefy changes here to draw things, just so we can actually see them.",
  "translatedText": "그리고 그때와 마찬가지로 여기에서도 사물을 그릴 때 다소 거친 변화를 사용하여 실제로 볼 수 있도록 하고 있다는 점을 기억하세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 389.46,
  "end": 395.9
 },
 {
  "input": "But in principle, dx is something very very small, and that means that dx squared and d sine of x are also very very small.",
  "translatedText": "하지만 원칙적으로 dx는 매우 작은 값이며, 이는 dx의 제곱과 x의 사인도 매우 작다는 것을 의미합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 396.36,
  "end": 404.7
 },
 {
  "input": "So, applying what we know about the derivative of sine and of x squared, that tiny change, dx squared, is going to be about 2x times dx.",
  "translatedText": "따라서 사인의 도함수와 x 제곱의 도함수에 대해 알고 있는 것을 적용하면, 그 작은 변화인 dx 제곱은 약 2배의 dx가 될 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 405.98,
  "end": 415.66
 },
 {
  "input": "And that tiny change, d sine of x, well that's going to be about cosine of x times dx.",
  "translatedText": "그리고 그 작은 변화, x의 사인인 d는 x의 코사인에 dx를 곱한 값 정도입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.36,
  "end": 421.58
 },
 {
  "input": "As usual, we divide out by that dx to see that the ratio we want, df divided by dx, is sine of x times the derivative of x squared, plus x squared times the derivative of sine.",
  "translatedText": "평소와 같이 dx로 나누면 원하는 비율인 df를 dx로 나눈 값은 x의 제곱에 x의 제곱을 곱한 값에 사인의 제곱을 곱한 값에 사인의 제곱을 곱한 값이라는 것을 알 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 422.92,
  "end": 435.7
 },
 {
  "input": "And nothing we've done here is specific to sine or to x squared.",
  "translatedText": "그리고 여기서 우리가 한 일은 사인이나 x 제곱에 특정한 것이 아닙니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 437.96,
  "end": 441.26
 },
 {
  "input": "This same line of reasoning would work for any two functions, g and h.",
  "translatedText": "이와 동일한 추론은 두 함수 g와 h에 대해 적용됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 441.58,
  "end": 445.36
 },
 {
  "input": "And sometimes people like to remember this pattern with a certain mnemonic that you kind of sing in your head.",
  "translatedText": "그리고 때때로 사람들은 이 패턴을 머릿속에서 부르는 특정 니모닉으로 기억하는 것을 좋아합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 447.0,
  "end": 451.54
 },
 {
  "input": "Left d right, right d left.",
  "translatedText": "왼쪽에서 오른쪽으로, 오른쪽에서 왼쪽으로.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 452.22,
  "end": 453.68
 },
 {
  "input": "In this example, where we have sine of x times x squared, left d right, means you take that left function, sine of x, times the derivative of the right, in this case 2x.",
  "translatedText": "이 예에서 x의 사인에 x의 제곱을 곱한 왼쪽 d 오른쪽은 왼쪽 함수인 x의 사인에 오른쪽의 도함수(이 경우 2x)를 곱한다는 의미입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 454.4,
  "end": 464.76
 },
 {
  "input": "Then you add on right d left, that right function, x squared, times the derivative of the left one, cosine of x.",
  "translatedText": "그런 다음 오른쪽 d 왼쪽에 오른쪽 함수 x 제곱을 곱하고 왼쪽 함수의 도함수인 x의 코사인을 더합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 465.48,
  "end": 472.94
 },
 {
  "input": "Now out of context, presented as a rule to remember, I think this would feel pretty strange, don't you?",
  "translatedText": "이제 문맥에서 벗어나 기억해야 할 규칙으로 제시되었으니 꽤 이상하게 느껴질 것 같지 않나요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 474.36,
  "end": 480.02
 },
 {
  "input": "But when you actually think of this adjustable box, you can see what each of those terms represents.",
  "translatedText": "하지만 실제로 이 조정 가능한 상자를 생각해 보면 각 용어가 무엇을 나타내는지 알 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 480.74,
  "end": 485.82
 },
 {
  "input": "Left d right is the area of that little bottom rectangle, and right d left is the area of that rectangle on the side.",
  "translatedText": "왼쪽 d 오른쪽은 작은 아래쪽 직사각형의 면적이고 오른쪽 d 왼쪽은 측면에 있는 직사각형의 면적입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 486.58,
  "end": 495.44
 },
 {
  "input": "By the way, I should mention that if you multiply by a constant, say 2 times sine of x, things end up a lot simpler.",
  "translatedText": "그건 그렇고, 상수를 곱하면, 예를 들어 x의 사인 2배를 곱하면 상황이 훨씬 더 단순해진다는 점을 언급하고 싶습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 500.16,
  "end": 506.74
 },
 {
  "input": "The derivative is just the same as the constant multiplied by the derivative of the function, in this case 2 times cosine of x.",
  "translatedText": "도함수는 상수에 함수의 도함수를 곱한 것과 같습니다. 이 경우에는 x의 코사인 2배입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.4,
  "end": 514.52
 },
 {
  "input": "I'll leave it to you to pause and ponder and verify that makes sense.",
  "translatedText": "나는 그것이 의미가 있는지 잠시 멈추고 숙고하고 검증하는 것을 여러분에게 맡길 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 515.56,
  "end": 520.18
 },
 {
  "input": "Aside from addition and multiplication, the other common way to combine functions, and believe me, this one comes up all the time, is to shove one inside the other, function composition.",
  "translatedText": "덧셈과 곱셈을 제외하고, 함수를 결합하는 또 다른 일반적인 방법은 항상 나오는 이 방법은 하나를 다른 하나 안에 밀어넣는 것, 즉 함수 합성입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 521.92,
  "end": 532.26
 },
 {
  "input": "For example, maybe we take the function x squared and shove it inside sine of x to get this new function, sine of x squared.",
  "translatedText": "예를 들어, x 제곱 함수를 x의 사인 안에 밀어 넣으면 새로운 함수인 x 제곱의 사인을 얻을 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 533.22,
  "end": 540.46
 },
 {
  "input": "What do you think the derivative of that new function is?",
  "translatedText": "그 새로운 함수의 미분은 무엇이라고 생각하시나요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 541.4,
  "end": 544.08
 },
 {
  "input": "To think this one through, I'll choose yet another way to visualize things, just to emphasize that in creative math, we've got lots of options.",
  "translatedText": "창의적인 수학에는 다양한 옵션이 있다는 점을 강조하기 위해 시각화할 수 있는 또 다른 방법을 선택하겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 545.3,
  "end": 552.54
 },
 {
  "input": "I'll put up three different number lines, the top one is going to hold the value of x, the second one is going to hold the x squared, and the third line is going to hold the value of sine of x squared.",
  "translatedText": "세 개의 숫자 줄을 세우고 맨 위 줄에는 x 값을, 두 번째 줄에는 x 제곱을, 세 번째 줄에는 x 제곱의 사인 값을 표시하겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 553.32,
  "end": 565.5
 },
 {
  "input": "That is, the function x squared gets you from line 1 to line 2, and the function sine gets you from line 2 to line 3.",
  "translatedText": "즉, x 제곱 함수는 1행에서 2행으로, 사인 함수는 2행에서 3행으로 이동합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 566.46,
  "end": 573.5
 },
 {
  "input": "As I shift around this value of x, maybe moving it up to the value 3, that second value stays pegged to whatever x squared is, in this case moving up to 9.",
  "translatedText": "이 x 값을 이동하면서, 예를 들어 3 값으로 이동하면 두 번째 값은 x의 제곱이 무엇이든, 이 경우 9까지 이동하는 것에 고정됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 574.84,
  "end": 585.34
 },
 {
  "input": "That bottom value, being sine of x squared, is going to go to whatever sine of 9 happens to be.",
  "translatedText": "X의 제곱의 사인인 이 하단 값은 9의 사인이 무엇이든 간에 9가 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 586.2,
  "end": 592.58
 },
 {
  "input": "So, for the derivative, let's again start by nudging that x value by some little dx.",
  "translatedText": "따라서 도함수의 경우 x 값을 작은 dx만큼 조금씩 움직여서 다시 시작하겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 594.9,
  "end": 600.4
 },
 {
  "input": "I always think that it's helpful to think of x as starting at some actual concrete number, maybe 1.5 in this case.",
  "translatedText": "저는 항상 x를 실제 구체적인 숫자(이 경우에는 1.5)에서 시작하는 것으로 생각하는 것이 도움이 된다고 생각합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 601.54,
  "end": 607.84
 },
 {
  "input": "The resulting nudge to that second value, the change in x squared caused by such a dx, is dx squared.",
  "translatedText": "두 번째 값으로의 결과 이동, 그러한 dx로 인한 x 제곱의 변화는 dx 제곱입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 608.76,
  "end": 615.7
 },
 {
  "input": "We could expand this like we have before, as 2x times dx, which for our specific input would be 2 times 1.5 times dx, but it helps to keep things written as dx squared, at least for now.",
  "translatedText": "이를 2x dx로 확장할 수 있으며, 특정 입력의 경우 2x 1이 됩니다.5 곱하기 dx. 하지만 적어도 지금은 dx 제곱으로 기록하는 것이 도움이 됩니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 616.96,
  "end": 630.12
 },
 {
  "input": "In fact, I'm going to go one step further, give a new name to this x squared, maybe h, so instead of writing dx squared for this nudge, we write dh.",
  "translatedText": "사실 한 걸음 더 나아가 이 x 제곱에 새로운 이름, 즉 h를 붙여서 이 넛지에 대해 dx 제곱을 쓰는 대신 dh를 쓸 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 631.02,
  "end": 641.2
 },
 {
  "input": "This makes it easier to think about that third value, which is now pegged at sine of h.",
  "translatedText": "이렇게 하면 이제 사인 h에 고정된 세 번째 값에 대해 생각하기가 더 쉬워집니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 642.62,
  "end": 647.26
 },
 {
  "input": "Its change is d sine of h, the tiny change caused by the nudge dh.",
  "translatedText": "그 변화는 h의 d 사인, 즉 넛지 dh로 인한 작은 변화입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 648.2,
  "end": 653.68
 },
 {
  "input": "By the way, the fact that it's moving to the left while the dh bump is going to the right just means that this change, d sine of h, is going to be some kind of negative number.",
  "translatedText": "그건 그렇고, dh 범프가 오른쪽으로 이동하는 동안 왼쪽으로 이동한다는 사실은 이 변화, 즉 h의 d 사인이 일종의 음수가 될 것임을 의미합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 655.0,
  "end": 665.04
 },
 {
  "input": "Once again, we can use our knowledge of the derivative of the sine.",
  "translatedText": "다시 한번, 우리는 사인의 도함수에 대한 지식을 사용할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 666.14,
  "end": 669.64
 },
 {
  "input": "This d sine of h is going to be about cosine of h times dh.",
  "translatedText": "이 h의 d 사인은 대략 코사인 h 곱하기 dh가 될 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 670.5,
  "end": 674.42
 },
 {
  "input": "That's what it means for the derivative of sine to be cosine.",
  "translatedText": "이것이 바로 사인의 미분값이 코사인이라는 뜻입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 675.24,
  "end": 678.64
 },
 {
  "input": "Unfolding things, we can replace that h with x squared again, so we know that the bottom nudge will be a size of cosine of x squared times dx squared.",
  "translatedText": "사물을 펼치면 h를 다시 x 제곱으로 바꿀 수 있으므로 하단 넛지는 x 제곱 곱하기 dx 제곱의 코사인 크기가 된다는 것을 알 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 679.54,
  "end": 689.78
 },
 {
  "input": "Let's unfold things even further.",
  "translatedText": "더 많은 것을 펼쳐보겠습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 691.04,
  "end": 692.48
 },
 {
  "input": "That intermediate nudge dx squared is going to be about 2x times dx.",
  "translatedText": "그 중간 넛지 dx 제곱은 dx의 약 2배가 될 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 692.84,
  "end": 698.1
 },
 {
  "input": "It's always a good habit to remind yourself of what an expression like this actually means.",
  "translatedText": "항상 이런 표현이 실제로 무엇을 의미하는지 상기하는 것은 좋은 습관입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 699.06,
  "end": 703.68
 },
 {
  "input": "In this case, where we started at x equals 1.5 up top, this whole expression is telling us that the size of the nudge on that third line is going to be about cosine of 1.5 squared times 2 times 1.5 times whatever the size of dx was.",
  "translatedText": "이 경우 x에서 시작한 곳은 1입니다.위의 5를 보면 이 전체 표현식은 세 번째 줄의 넛지 크기가 대략 코사인 1이 될 것임을 알려줍니다.5의 제곱 곱하기 2 곱하기 1.dx의 크기에 관계없이 5배입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 704.34,
  "end": 722.22
 },
 {
  "input": "It's proportional to the size of dx, and this derivative is giving us that proportionality constant.",
  "translatedText": "이는 dx의 크기에 비례하며, 이 도함수는 우리에게 비례 상수를 제공합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.72,
  "end": 727.92
 },
 {
  "input": "Notice what we came out with here.",
  "translatedText": "여기서 우리가 무엇을 얻었는지 주목하세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 730.92,
  "end": 732.56
 },
 {
  "input": "We have the derivative of the outside function, and it's still taking in the unaltered inside function, and then multiplying it by the derivative of that inside function.",
  "translatedText": "우리는 외부 함수의 도함수를 갖고 있고, 여전히 변경되지 않은 내부 함수를 받아들인 다음, 내부 함수의 도함수를 곱합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 732.96,
  "end": 743.22
 },
 {
  "input": "Again, there's nothing special about sine of x or x squared.",
  "translatedText": "다시 말하지만, x의 사인이나 x의 제곱에는 특별한 것이 없습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 745.82,
  "end": 749.22
 },
 {
  "input": "If you have any two functions, g of x and h of x, the derivative of their composition, g of h of x, is going to be the derivative of g evaluated on h, multiplied by the derivative of h.",
  "translatedText": "두 개의 함수(x의 g와 x의 h)가 있는 경우 해당 구성의 도함수인 x의 h의 g는 h에서 평가된 g의 도함수에 h의 도함수를 곱한 값입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 749.74,
  "end": 763.66
 },
 {
  "input": "This pattern right here is what we usually call the chain rule.",
  "translatedText": "이 패턴은 우리가 일반적으로 체인 규칙이라고 부르는 패턴입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 767.14,
  "end": 770.9
 },
 {
  "input": "Notice for the derivative of g, I'm writing it as dg dh instead of dg dx.",
  "translatedText": "g의 도함수의 경우, dg dx가 아닌 dg dh로 표기하고 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 772.04,
  "end": 777.68
 },
 {
  "input": "On the symbolic level, this is a reminder that the thing you plug into that derivative is still going to be that intermediary function h.",
  "translatedText": "기호적 수준에서 이것은 파생상품에 연결되는 것이 여전히 중간 함수 h라는 것을 상기시켜 줍니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 778.68,
  "end": 786.06
 },
 {
  "input": "But more than that, it's an important reflection of what this derivative of the outer function actually represents.",
  "translatedText": "그러나 그 이상으로, 이는 외부 함수의 파생물이 실제로 무엇을 나타내는지 보여주는 중요한 반영입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 787.02,
  "end": 792.52
 },
 {
  "input": "Remember, in our three line setup, when we took the derivative of the sine on that bottom, we expanded the size of that nudge, d sine, as cosine of h times dh.",
  "translatedText": "세 줄 설정에서 바닥에 있는 사인의 도함수를 취했을 때 그 넛지의 크기인 d 사인을 코사인 h 곱하기 dh로 확장했다는 것을 기억하세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 793.2,
  "end": 803.9
 },
 {
  "input": "This was because we didn't immediately know how the size of that bottom nudge depended on x.",
  "translatedText": "이는 하단 넛지의 크기가 x에 어떻게 의존하는지 즉시 알 수 없었기 때문입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 804.94,
  "end": 809.84
 },
 {
  "input": "That's kind of the whole thing we were trying to figure out.",
  "translatedText": "이것이 바로 우리가 알아내려고 했던 모든 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 810.42,
  "end": 812.6
 },
 {
  "input": "But we could take the derivative with respect to that intermediate variable, h.",
  "translatedText": "하지만 중간 변수인 h에 대한 도함수를 취할 수 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 813.26,
  "end": 817.36
 },
 {
  "input": "That is, figure out how to express the size of that nudge on the third line as some multiple of dh, the size of the nudge on the second line.",
  "translatedText": "즉, 세 번째 줄의 넛지 크기를 두 번째 줄의 넛지 크기인 dh의 배수로 표현하는 방법을 알아보세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 818.1,
  "end": 825.68
 },
 {
  "input": "It was only after that that we unfolded further by figuring out what dh was.",
  "translatedText": "그 이후에야 우리는 dh가 무엇인지 알아내면서 더 많은 것을 펼쳤습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 826.58,
  "end": 830.7
 },
 {
  "input": "In this chain rule expression, we're saying, look at the ratio between a tiny change in g, the final output, to a tiny change in h that caused it, h being the value we plug into g.",
  "translatedText": "이 연쇄 규칙 표현에서 최종 출력인 g의 작은 변화와 이를 유발한 h의 작은 변화 사이의 비율을 살펴보겠습니다. h는 g에 연결하는 값입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 833.32,
  "end": 844.38
 },
 {
  "input": "Then multiply that by the tiny change in h, divided by the tiny change in x that caused it.",
  "translatedText": "그런 다음 이를 h의 작은 변화로 곱하고 이를 유발한 x의 작은 변화로 나눕니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 845.32,
  "end": 851.2
 },
 {
  "input": "So notice, those dh's cancel out, and they give us a ratio between the change in that final output and the change to the input that, through a certain chain of events, brought it about.",
  "translatedText": "dh는 상쇄되어 최종 출력의 변화와 특정 일련의 사건을 통해 발생하는 입력의 변화 사이의 비율을 제공합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 852.3,
  "end": 862.28
 },
 {
  "input": "And that cancellation of dh is not just a notational trick.",
  "translatedText": "그리고 dh의 취소는 단순한 표기법상의 트릭이 아닙니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 863.86,
  "end": 866.98
 },
 {
  "input": "That is a genuine reflection of what's going on with the tiny nudges that underpin everything we do with derivatives.",
  "translatedText": "이는 우리가 파생상품으로 하는 모든 일의 근간을 이루는 작은 넛지들이 어떻게 작동하는지를 진정으로 반영한 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 866.98,
  "end": 873.9
 },
 {
  "input": "So those are the three basic tools to have in your belt to handle derivatives of functions that combine a lot of smaller things.",
  "translatedText": "이 세 가지 기본 도구는 여러 가지 작은 것들을 결합한 파생 함수를 처리하기 위해 기본적으로 갖추어야 할 도구입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 876.3,
  "end": 883.24
 },
 {
  "input": "You've got the sum rule, the product rule, and the chain rule.",
  "translatedText": "합의 법칙, 곱의 법칙, 연쇄의 법칙이 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 883.84,
  "end": 887.38
 },
 {
  "input": "And I'll be honest with you, there is a big difference between knowing what the chain rule is and what the product rule is, and actually being fluent with applying them in even the most hairy of situations.",
  "translatedText": "그리고 솔직하게 말씀드리자면, 체인 규칙이 무엇인지 아는 것과 제품 규칙이 무엇인지 아는 것과 실제로 가장 까다로운 상황에서도 이를 유창하게 적용하는 것 사이에는 큰 차이가 있습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 888.4,
  "end": 898.62
 },
 {
  "input": "Watching videos, any videos, about the mechanics of calculus is never going to substitute for practicing those mechanics yourself, and building up the muscles to do these computations yourself.",
  "translatedText": "미적분학의 역학에 관한 비디오를 보는 것은 결코 그 역학을 직접 연습하고 이러한 계산을 직접 수행할 수 있는 근육을 키우는 것을 대체할 수 없습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 899.48,
  "end": 910.4
 },
 {
  "input": "I really wish I could offer to do that for you, but I'm afraid the ball is in your court, my friend, to seek out the practice.",
  "translatedText": "나는 정말로 당신을 위해 그렇게 하겠다고 제안하고 싶지만, 친구여, 연습을 모색해야 할 공이 당신 코트에 있는 것이 두렵습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 911.24,
  "end": 917.44
 },
 {
  "input": "What I can offer, and what I hope I have offered, is to show you where these rules actually come from.",
  "translatedText": "제가 제안할 수 있는 것, 그리고 제가 제안하고 싶은 것은 이러한 규칙이 실제로 어디서 나오는지 보여주는 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 918.04,
  "end": 923.96
 },
 {
  "input": "To show that they're not just something to be memorized and hammered away, but they're natural patterns, things that you too could have discovered just by patiently thinking through what a derivative actually means.",
  "translatedText": "그것들이 단지 외워서 버려야 할 것이 아니라 자연스러운 패턴이라는 것을 보여주기 위해, 파생어가 실제로 무엇을 의미하는지 인내심을 갖고 생각함으로써 여러분도 발견할 수 있었던 것입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 924.14,
  "end": 934.56
 }
]