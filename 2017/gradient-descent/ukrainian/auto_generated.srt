1
00:00:04,180 --> 00:00:07,280
У минулому відео я описав структуру нейронної мережі.

2
00:00:07,680 --> 00:00:10,460
Я дам тут короткий підсумок, щоб це було свіжим у нашій пам’яті, 

3
00:00:10,460 --> 00:00:12,600
а потім у мене є дві головні цілі для цього відео.

4
00:00:13,100 --> 00:00:15,720
Перший полягає в тому, щоб представити ідею градієнтного спуску, 

5
00:00:15,720 --> 00:00:18,422
яка лежить в основі не тільки того, як навчаються нейронні мережі, 

6
00:00:18,422 --> 00:00:20,600
але й того, як працює багато інших машинного навчання.

7
00:00:21,120 --> 00:00:25,437
Після цього ми детальніше розглянемо, як працює ця конкретна мережа, 

8
00:00:25,437 --> 00:00:27,940
і що шукають ці приховані шари нейронів.

9
00:00:28,979 --> 00:00:34,311
Нагадуємо, що нашою метою тут є класичний приклад розпізнавання рукописних цифр, 

10
00:00:34,311 --> 00:00:36,220
привіт, світ нейронних мереж.

11
00:00:37,020 --> 00:00:40,035
Ці цифри відображаються на сітці 28x28 пікселів, 

12
00:00:40,035 --> 00:00:43,420
кожен піксель має значення відтінків сірого від 0 до 1.

13
00:00:43,820 --> 00:00:50,040
Саме вони визначають активацію 784 нейронів на вхідному рівні мережі.

14
00:00:51,180 --> 00:00:56,055
А потім активація для кожного нейрона в наступних шарах базується на зваженій сумі всіх 

15
00:00:56,055 --> 00:01:00,820
активацій на попередньому шарі плюс якесь спеціальне число, яке називається зміщенням.

16
00:01:02,160 --> 00:01:05,273
Потім ви складаєте цю суму за допомогою якоїсь іншої функції, 

17
00:01:05,273 --> 00:01:08,940
як-от сигмоподібна сквишифікація або relu, як я пройшов у минулому відео.

18
00:01:09,480 --> 00:01:15,385
Загалом, враховуючи дещо довільний вибір двох прихованих шарів із 16 нейронами кожен, 

19
00:01:15,385 --> 00:01:20,054
мережа має близько 13 000 ваг і зміщень, які ми можемо налаштувати, 

20
00:01:20,054 --> 00:01:24,380
і саме ці значення визначають, що саме мережа насправді робить.

21
00:01:24,880 --> 00:01:29,237
Коли ми кажемо, що ця мережа класифікує дану цифру, ми маємо на увазі те, 

22
00:01:29,237 --> 00:01:33,300
що найяскравіший із 10 нейронів останнього шару відповідає цій цифрі.

23
00:01:34,100 --> 00:01:38,369
І пам’ятайте, мотивація, яку ми тут мали на увазі для багатошарової структури, 

24
00:01:38,369 --> 00:01:41,828
полягала в тому, що, можливо, другий шар міг би підхопити краї, 

25
00:01:41,828 --> 00:01:45,233
а третій шар міг би підхопити візерунки, як-от петлі та лінії, 

26
00:01:45,233 --> 00:01:48,800
і останній міг просто з’єднати їх. шаблони для розпізнавання цифр.

27
00:01:49,800 --> 00:01:52,240
Отже, ми дізнаємося, як навчається мережа.

28
00:01:52,640 --> 00:01:56,869
Те, що ми хочемо, — це алгоритм, за допомогою якого ви можете показати цій 

29
00:01:56,869 --> 00:02:00,985
мережі цілий набір навчальних даних, які надходять у формі набору різних 

30
00:02:00,985 --> 00:02:04,763
зображень рукописних цифр разом із мітками, якими вони мають бути, 

31
00:02:04,763 --> 00:02:07,526
і це буде відкоригувати ці 13 000 ваг і зміщень, 

32
00:02:07,526 --> 00:02:10,120
щоб покращити ефективність тренувальних даних.

33
00:02:10,720 --> 00:02:13,715
Сподіваємось, ця багаторівнева структура означатиме, що те, 

34
00:02:13,715 --> 00:02:16,860
що вона вивчає, узагальнює зображення за межами даних навчання.

35
00:02:17,640 --> 00:02:20,688
Спосіб тестування полягає в тому, що після того, як ви навчите мережу, 

36
00:02:20,688 --> 00:02:23,951
ви показуєте їй більше позначених даних, яких вона ніколи не бачила раніше, 

37
00:02:23,951 --> 00:02:26,700
і ви бачите, наскільки точно вона класифікує ці нові зображення.

38
00:02:31,120 --> 00:02:35,616
На наше щастя, і те, що робить цей приклад таким поширеним для початку, полягає в тому, 

39
00:02:35,616 --> 00:02:38,017
що добрі люди, що стоять за базою даних MNIST, 

40
00:02:38,017 --> 00:02:41,134
зібрали колекцію з десятків тисяч рукописних зображень цифр, 

41
00:02:41,134 --> 00:02:44,200
кожне з яких позначено номерами, які вони повинні мати бути.

42
00:02:44,900 --> 00:02:48,538
І як би не було провокаційно описувати машину як навчальну, коли ви бачите, 

43
00:02:48,538 --> 00:02:51,698
як вона працює, це стає набагато менше схожим на якусь божевільну 

44
00:02:51,698 --> 00:02:55,480
науково-фантастичну передумову, а набагато більше схоже на вправу з обчислення.

45
00:02:56,200 --> 00:02:59,960
Я маю на увазі, що в основному це зводиться до пошуку мінімуму певної функції.

46
00:03:01,939 --> 00:03:07,235
Пам’ятайте, концептуально ми вважаємо, що кожен нейрон пов’язаний з усіма нейронами 

47
00:03:07,235 --> 00:03:11,710
попереднього шару, і ваги у зваженій сумі, що визначає його активацію, 

48
00:03:11,710 --> 00:03:17,194
схожі на силу цих зв’язків, а зміщення є певним показником чи цей нейрон має тенденцію 

49
00:03:17,194 --> 00:03:18,960
бути активним чи неактивним.

50
00:03:19,720 --> 00:03:24,400
І для початку ми просто ініціалізуємо всі ці ваги та зміщення абсолютно випадковим чином.

51
00:03:24,940 --> 00:03:28,822
Зайве говорити, що ця мережа працюватиме досить жахливо на даному навчальному прикладі, 

52
00:03:28,822 --> 00:03:30,720
оскільки вона просто робить щось випадкове.

53
00:03:31,040 --> 00:03:36,020
Наприклад, ви подаєте на це зображення 3, а вихідний шар виглядає просто безладом.

54
00:03:36,600 --> 00:03:41,378
Отже, що ви робите, це визначаєте функцію вартості, спосіб повідомити комп’ютеру, 

55
00:03:41,378 --> 00:03:44,699
ні, поганий комп’ютер, цей вихід повинен мати активації, 

56
00:03:44,699 --> 00:03:49,769
які дорівнюють 0 для більшості нейронів, але 1 для цього нейрона, те, що ви мені дали, 

57
00:03:49,769 --> 00:03:50,760
є повним сміттям.

58
00:03:51,720 --> 00:03:55,929
Якщо говорити трохи математичніше, ви складаєте квадрати різниць між 

59
00:03:55,929 --> 00:04:00,139
кожною з цих активацій виведення сміття та значенням, яке ви хочете, 

60
00:04:00,139 --> 00:04:05,020
щоб вони мали, і це те, що ми називатимемо вартістю одного навчального прикладу.

61
00:04:05,960 --> 00:04:12,038
Зауважте, що ця сума невелика, коли мережа впевнено правильно класифікує зображення, 

62
00:04:12,038 --> 00:04:16,399
але велика, коли здається, що мережа не знає, що вона робить.

63
00:04:18,640 --> 00:04:21,791
Отже, що ви робите, це розглядаєте середню вартість усіх 

64
00:04:21,791 --> 00:04:25,440
десятків тисяч навчальних прикладів, які є у вашому розпорядженні.

65
00:04:27,040 --> 00:04:29,742
Ця середня вартість є нашим показником того, наскільки 

66
00:04:29,742 --> 00:04:32,740
поганою є мережа та наскільки погано має працювати комп’ютер.

67
00:04:33,420 --> 00:04:34,600
І це складна річ.

68
00:04:35,040 --> 00:04:38,480
Пам’ятаєте, як сама мережа була в основному функцією, 

69
00:04:38,480 --> 00:04:42,111
яка приймає 784 числа як вхідні дані, значення пікселів, 

70
00:04:42,111 --> 00:04:46,761
і викидає 10 чисел як свій вихід, і в певному сенсі вона параметризована 

71
00:04:46,761 --> 00:04:48,800
всіма цими вагами та зміщеннями?

72
00:04:49,500 --> 00:04:52,820
Крім того, функція витрат є ще одним шаром складності.

73
00:04:53,100 --> 00:04:57,914
Він бере на вхід приблизно 13 000 ваг і упереджень і видає одне число, 

74
00:04:57,914 --> 00:05:01,508
яке описує, наскільки погані ці ваги та упередження, 

75
00:05:01,508 --> 00:05:06,594
і спосіб його визначення залежить від поведінки мережі над усіма десятками 

76
00:05:06,594 --> 00:05:08,900
тисяч фрагментів навчальних даних.

77
00:05:09,520 --> 00:05:11,000
Це багато про що думати.

78
00:05:12,400 --> 00:05:15,820
Але просто говорити комп’ютеру, яку погану роботу він робить, не дуже корисно.

79
00:05:16,220 --> 00:05:20,060
Ви хочете сказати йому, як змінити ці ваги та упередження, щоб воно стало кращим.

80
00:05:20,780 --> 00:05:25,778
Щоб зробити це легше, замість того, щоб намагатися уявити функцію з 13 000 входами, 

81
00:05:25,778 --> 00:05:30,480
просто уявіть просту функцію, яка має одне число як вхід і одне число як вихід.

82
00:05:31,480 --> 00:05:35,300
Як знайти вхідні дані, які мінімізують значення цієї функції?

83
00:05:36,460 --> 00:05:41,351
Студенти, які вивчають обчислення, знатимуть, що іноді можна чітко визначити цей мінімум, 

84
00:05:41,351 --> 00:05:44,340
але це не завжди можливо для справді складних функцій, 

85
00:05:44,340 --> 00:05:49,232
а особливо не у версії цієї ситуації з 13 000 вхідних даних для нашої божевільно складної 

86
00:05:49,232 --> 00:05:51,080
функції вартості нейронної мережі.

87
00:05:51,580 --> 00:05:56,152
Більш гнучка тактика полягає в тому, щоб почати з будь-якого входу та визначити, 

88
00:05:56,152 --> 00:05:59,200
у якому напрямку слід рухатися, щоб знизити цей вихід.

89
00:06:00,080 --> 00:06:03,789
Зокрема, якщо ви можете визначити нахил функції, де ви знаходитесь, 

90
00:06:03,789 --> 00:06:08,536
тоді перемістіть ліворуч, якщо цей нахил додатний, і перемістіть вхідні дані праворуч, 

91
00:06:08,536 --> 00:06:09,900
якщо цей нахил від’ємний.

92
00:06:11,960 --> 00:06:15,719
Якщо ви робите це неодноразово, у кожній точці перевіряючи новий нахил і 

93
00:06:15,719 --> 00:06:19,840
роблячи відповідний крок, ви наблизитесь до деякого локального мінімуму функції.

94
00:06:20,640 --> 00:06:23,800
Зображення, яке ви можете мати на увазі, це м’яч, що котиться з пагорба.

95
00:06:24,620 --> 00:06:28,165
Зауважте, що навіть для цієї дійсно спрощеної функції єдиного введення 

96
00:06:28,165 --> 00:06:31,860
існує багато можливих долин, у які ви можете потрапити, залежно від того, 

97
00:06:31,860 --> 00:06:35,904
з якого випадкового введення ви почнете, і немає гарантії, що локальний мінімум, 

98
00:06:35,904 --> 00:06:39,400
у який ви потрапите, буде найменшим можливим значенням функції витрат.

99
00:06:40,220 --> 00:06:42,620
Це також перенесеться на нашу нейронну мережу.

100
00:06:43,180 --> 00:06:47,982
Я також хочу, щоб ви помітили, що якщо ви робите розмір кроку пропорційним схилу, 

101
00:06:47,982 --> 00:06:52,257
тоді, коли схил вирівнюється до мінімуму, ваші кроки стають все меншими, 

102
00:06:52,257 --> 00:06:54,600
і це допомагає вам уникнути перевищення.

103
00:06:55,940 --> 00:07:00,980
Трохи збільшуючи складність, уявіть натомість функцію з двома входами та одним виходом.

104
00:07:01,500 --> 00:07:05,470
Ви можете подумати про вхідний простір як про площину xy, 

105
00:07:05,470 --> 00:07:08,140
а функцію вартості як поверхню над нею.

106
00:07:08,760 --> 00:07:12,851
Замість того, щоб запитувати про нахил функції, ви повинні запитати, 

107
00:07:12,851 --> 00:07:16,765
у якому напрямку вам слід зробити крок у цьому вхідному просторі, 

108
00:07:16,765 --> 00:07:18,960
щоб найшвидше зменшити вихід функції.

109
00:07:19,720 --> 00:07:21,760
Іншими словами, який напрямок спуску?

110
00:07:22,380 --> 00:07:25,560
Знову ж таки, корисно уявити м’яч, що котиться з пагорба.

111
00:07:26,660 --> 00:07:30,635
Ті з вас, хто знайомий із численням багатьох змінних, знають, 

112
00:07:30,635 --> 00:07:34,419
що градієнт функції дає вам напрямок найкрутішого підйому, 

113
00:07:34,419 --> 00:07:38,780
у якому напрямку слід зробити крок, щоб збільшити функцію найшвидше.

114
00:07:39,560 --> 00:07:44,213
Цілком природно, що негативне значення цього градієнта дає вам напрямок кроку, 

115
00:07:44,213 --> 00:07:46,040
який найшвидше зменшує функцію.

116
00:07:47,240 --> 00:07:51,619
Навіть більше того, довжина цього вектора градієнта є показником того, 

117
00:07:51,619 --> 00:07:53,840
наскільки крутим є найкрутіший схил.

118
00:07:54,540 --> 00:07:57,440
Якщо ви не знайомі з численням багатьох змінних і хочете дізнатися більше, 

119
00:07:57,440 --> 00:08:00,340
ознайомтеся з деякими роботами, які я виконав для Академії Хана на цю тему.

120
00:08:00,860 --> 00:08:04,243
Але, чесно кажучи, для нас з вами зараз важливо лише те, 

121
00:08:04,243 --> 00:08:07,863
що в принципі існує спосіб обчислити цей вектор, цей вектор, 

122
00:08:07,863 --> 00:08:11,900
який повідомляє вам, яким є напрямок спуску та наскільки він крутий.

123
00:08:12,400 --> 00:08:16,120
З тобою все буде добре, якщо це все, що ти знаєш, і ти не розбираєшся в деталях.

124
00:08:17,200 --> 00:08:21,202
Якщо ви можете це отримати, алгоритм мінімізації функції полягає в тому, 

125
00:08:21,202 --> 00:08:25,972
щоб обчислити цей напрямок градієнта, потім зробити невеликий крок вниз і повторити це 

126
00:08:25,972 --> 00:08:26,740
знову і знову.

127
00:08:27,700 --> 00:08:32,820
Це та сама основна ідея для функції, яка має 13 000 входів замість 2 входів.

128
00:08:33,400 --> 00:08:36,658
Уявіть собі організацію всіх 13 000 ваг і зміщень 

129
00:08:36,658 --> 00:08:39,460
нашої мережі у гігантський вектор-стовпець.

130
00:08:40,140 --> 00:08:43,741
Від’ємний градієнт функції витрат — це просто вектор, 

131
00:08:43,741 --> 00:08:48,076
це певний напрямок у цьому шалено величезному просторі введення, 

132
00:08:48,076 --> 00:08:53,212
який говорить вам, які підштовхи до всіх цих чисел призведуть до найшвидшого 

133
00:08:53,212 --> 00:08:54,880
зменшення функції витрат.

134
00:08:55,640 --> 00:08:59,256
І, звісно, завдяки нашій спеціально розробленій функції вартості зміна 

135
00:08:59,256 --> 00:09:02,160
вагових коефіцієнтів і зміщень для їх зменшення означає, 

136
00:09:02,160 --> 00:09:05,878
що вихідні дані мережі для кожної частини навчальних даних виглядатимуть 

137
00:09:05,878 --> 00:09:09,546
не так як випадковий масив із 10 значень, а більше як фактичне рішення, 

138
00:09:09,546 --> 00:09:10,820
яке ми хочемо це зробити.

139
00:09:11,440 --> 00:09:14,583
Важливо пам’ятати, що ця функція вартості передбачає середнє 

140
00:09:14,583 --> 00:09:17,984
значення за всіма навчальними даними, тож якщо ви мінімізуєте її, 

141
00:09:17,984 --> 00:09:21,180
це означає, що для всіх цих зразків буде краща продуктивність.

142
00:09:23,820 --> 00:09:26,512
Алгоритм для ефективного обчислення цього градієнта, 

143
00:09:26,512 --> 00:09:29,611
який фактично є основою того, як нейронна мережа навчається, 

144
00:09:29,611 --> 00:09:33,980
називається зворотним поширенням, і саме про нього я буду говорити в наступному відео.

145
00:09:34,660 --> 00:09:37,418
Там я справді хочу витратити час, щоб пройти через те, 

146
00:09:37,418 --> 00:09:41,833
що саме відбувається з кожною вагою та зміщенням для певної частини тренувальних даних, 

147
00:09:41,833 --> 00:09:46,146
намагаючись дати інтуїтивне відчуття того, що відбувається за межами купи відповідних 

148
00:09:46,146 --> 00:09:47,100
обчислень і формул.

149
00:09:47,780 --> 00:09:50,773
Прямо тут, прямо зараз, головне, що я хочу, щоб ви знали, 

150
00:09:50,773 --> 00:09:54,024
незалежно від деталей реалізації, це те, що ми маємо на увазі, 

151
00:09:54,024 --> 00:09:58,360
коли говоримо про мережеве навчання, це те, що це просто мінімізація функції витрат.

152
00:09:59,300 --> 00:10:03,800
І зауважте, одним із наслідків цього є те, що для цієї функції витрат важливо мати гарний 

153
00:10:03,800 --> 00:10:08,100
плавний результат, щоб ми могли знайти локальний мінімум, роблячи невеликі кроки вниз.

154
00:10:09,260 --> 00:10:14,166
Ось чому, до речі, штучні нейрони мають безперервний діапазон активацій, 

155
00:10:14,166 --> 00:10:19,140
а не просто активні чи неактивні бінарним способом, як біологічні нейрони.

156
00:10:20,220 --> 00:10:23,365
Цей процес багаторазового підштовхування вхідних даних функції 

157
00:10:23,365 --> 00:10:26,760
деяким кратним від’ємним градієнтом називається градієнтним спуском.

158
00:10:27,300 --> 00:10:30,871
Це спосіб сходитися до деякого локального мінімуму функції вартості, 

159
00:10:30,871 --> 00:10:32,580
по суті, долини на цьому графіку.

160
00:10:33,440 --> 00:10:36,971
Звичайно, я все ще показую зображення функції з двома входами, 

161
00:10:36,971 --> 00:10:41,456
тому що підштовхування в 13 000-вимірному просторі введення трохи важко уявити, 

162
00:10:41,456 --> 00:10:44,260
але є гарний непросторовий спосіб подумати про це.

163
00:10:45,080 --> 00:10:48,440
Кожен компонент негативного градієнта говорить нам про дві речі.

164
00:10:49,060 --> 00:10:52,337
Знак, звичайно, говорить нам про те, чи потрібно підштовхнути 

165
00:10:52,337 --> 00:10:55,140
відповідний компонент вхідного вектора вгору чи вниз.

166
00:10:55,800 --> 00:11:01,322
Але важливо те, що відносні величини всіх цих компонентів ніби підказують вам, 

167
00:11:01,322 --> 00:11:02,720
які зміни важливіші.

168
00:11:05,220 --> 00:11:08,889
Розумієте, у нашій мережі коригування однієї з ваг може мати 

169
00:11:08,889 --> 00:11:13,040
набагато більший вплив на функцію витрат, ніж коригування іншої ваги.

170
00:11:14,800 --> 00:11:18,200
Деякі з цих зв’язків просто важливіші для наших навчальних даних.

171
00:11:19,320 --> 00:11:24,085
Таким чином, ви можете подумати про цей вектор градієнта нашої величезної функції витрат, 

172
00:11:24,085 --> 00:11:28,799
що спотворює розум, так це те, що він кодує відносну важливість кожної ваги та зміщення, 

173
00:11:28,799 --> 00:11:32,400
тобто те, яка з цих змін принесе найбільшу віддачу від ваших грошей.

174
00:11:33,620 --> 00:11:36,640
Це просто інший спосіб думати про напрямок.

175
00:11:37,100 --> 00:11:42,656
Щоб взяти простіший приклад, якщо у вас є якась функція з двома змінними як вхідні дані, 

176
00:11:42,656 --> 00:11:48,087
і ви обчислюєте, що її градієнт у певній точці виходить рівним 3,1, то, з одного боку, 

177
00:11:48,087 --> 00:11:52,707
ви можете інтерпретувати це як те, що коли ви перебуваючи на цьому вході, 

178
00:11:52,707 --> 00:11:56,141
рух у цьому напрямку збільшує функцію найшвидше, тому, 

179
00:11:56,141 --> 00:11:59,825
коли ви будуєте графік функції над площиною вхідних точок, 

180
00:11:59,825 --> 00:12:02,260
цей вектор дає вам напрям прямо в гору.

181
00:12:02,860 --> 00:12:07,597
Але інший спосіб прочитати це – сказати, що зміни цієї першої змінної мають утричі 

182
00:12:07,597 --> 00:12:12,105
більше значення, ніж зміни другої змінної, що принаймні в околицях відповідних 

183
00:12:12,105 --> 00:12:16,900
вхідних даних, підштовхування значення x несе набагато більше удару для вашого бакс.

184
00:12:19,880 --> 00:12:22,340
Давайте зменшимо масштаб і підведемо підсумок, де ми зараз.

185
00:12:22,840 --> 00:12:26,894
Сама мережа є цією функцією з 784 входами та 10 виходами, 

186
00:12:26,894 --> 00:12:30,040
визначеними в термінах усіх цих зважених сум.

187
00:12:30,640 --> 00:12:33,680
Функція витрат є ще одним шаром складності.

188
00:12:33,980 --> 00:12:38,057
Він бере 13 000 ваг і упереджень як вхідні дані та викидає 

189
00:12:38,057 --> 00:12:41,720
єдину міру паршивості на основі навчальних прикладів.

190
00:12:42,440 --> 00:12:46,900
А градієнт функції витрат — це ще один рівень складності.

191
00:12:47,360 --> 00:12:50,601
Він говорить нам, які підштовхи до всіх цих ваг і упереджень 

192
00:12:50,601 --> 00:12:53,523
спричиняють найшвидшу зміну значення функції вартості, 

193
00:12:53,523 --> 00:12:57,880
що ви можете інтерпретувати як те, які зміни до яких ваг мають найбільше значення.

194
00:13:02,560 --> 00:13:05,887
Отже, коли ви ініціалізуєте мережу випадковими вагами та зміщеннями та 

195
00:13:05,887 --> 00:13:09,356
налаштовуєте їх багато разів на основі цього процесу градієнтного спуску, 

196
00:13:09,356 --> 00:13:13,200
наскільки добре вона насправді працює на зображеннях, яких ніколи раніше не бачив?

197
00:13:14,100 --> 00:13:18,279
Той, який я описав тут, із двома прихованими шарами по 16 нейронів кожен, 

198
00:13:18,279 --> 00:13:21,441
обраними здебільшого з естетичних міркувань, непоганий, 

199
00:13:21,441 --> 00:13:25,960
оскільки він класифікує приблизно 96% нових зображень, які він бачить правильно.

200
00:13:26,680 --> 00:13:29,482
І, чесно кажучи, якщо ви подивіться на деякі приклади, 

201
00:13:29,482 --> 00:13:32,540
з якими він зіпсувався, ви відчуєте потребу трохи послабити.

202
00:13:36,220 --> 00:13:39,056
Тепер, якщо ви поекспериментуєте зі структурою прихованого шару 

203
00:13:39,056 --> 00:13:41,760
та зробите кілька налаштувань, ви зможете отримати це до 98%.

204
00:13:41,760 --> 00:13:42,720
І це дуже добре!

205
00:13:43,020 --> 00:13:46,228
Це не найкраще, ви, звичайно, можете отримати кращу продуктивність, 

206
00:13:46,228 --> 00:13:49,813
якщо стати більш складною, ніж ця звичайна ванільна мережа, але враховуючи, 

207
00:13:49,813 --> 00:13:53,588
наскільки складним є початкове завдання, я вважаю, що є щось неймовірне в тому, 

208
00:13:53,588 --> 00:13:56,371
щоб будь-яка мережа так добре справлялася із зображеннями, 

209
00:13:56,371 --> 00:14:00,523
яких вона ніколи раніше не бачила, враховуючи, що ми ніколи конкретно не говорили йому, 

210
00:14:00,523 --> 00:14:01,420
які шаблони шукати.

211
00:14:02,560 --> 00:14:06,753
Спочатку я мотивував цю структуру, описуючи надію, яку ми могли б мати, 

212
00:14:06,753 --> 00:14:11,471
що другий шар може підхопити маленькі краї, що третій шар з’єднає ці краї разом, 

213
00:14:11,471 --> 00:14:15,840
щоб розпізнати петлі та довші лінії, і що вони можуть бути складені разом, 

214
00:14:15,840 --> 00:14:17,180
щоб розпізнавати цифри.

215
00:14:17,960 --> 00:14:20,400
Отже, це те, чим насправді займається наша мережа?

216
00:14:21,079 --> 00:14:24,400
Ну, принаймні для цього, зовсім ні.

217
00:14:24,820 --> 00:14:27,778
Пам’ятаєте, як у минулому відео ми дивилися на те, 

218
00:14:27,778 --> 00:14:31,607
як ваги зв’язків від усіх нейронів першого шару до даного нейрона 

219
00:14:31,607 --> 00:14:35,145
другого шару можна візуалізувати як даний піксельний шаблон, 

220
00:14:35,145 --> 00:14:37,060
який нейрон другого шару вловлює?

221
00:14:37,780 --> 00:14:42,105
Ну, коли ми фактично робимо це для ваг, пов’язаних із цими переходами, 

222
00:14:42,105 --> 00:14:47,405
від першого шару до наступного, замість того, щоб підбирати окремі маленькі краї тут і 

223
00:14:47,405 --> 00:14:52,766
там, вони виглядають, ну, майже випадковими, просто з деякими дуже вільними візерунками 

224
00:14:52,766 --> 00:14:53,680
в середина там.

225
00:14:53,760 --> 00:14:58,348
Здавалося б, що в незбагненно великому 13 000-вимірному просторі можливих ваг і 

226
00:14:58,348 --> 00:15:02,650
упереджень наша мережа знайшла щасливий маленький локальний мінімум, який, 

227
00:15:02,650 --> 00:15:07,353
незважаючи на успішну класифікацію більшості зображень, не точно вловлює шаблони, 

228
00:15:07,353 --> 00:15:08,960
на які ми могли сподіватися.

229
00:15:09,780 --> 00:15:12,195
Щоб переконатися в цьому, подивіться, що відбувається, 

230
00:15:12,195 --> 00:15:13,820
коли ви вводите випадкове зображення.

231
00:15:14,320 --> 00:15:19,436
Якби система була розумною, ви могли б очікувати, що вона буде відчувати себе невпевнено, 

232
00:15:19,436 --> 00:15:24,382
можливо, насправді не активуючи жоден із цих 10 вихідних нейронів або активуючи їх усі 

233
00:15:24,382 --> 00:15:28,645
рівномірно, але натомість вона впевнено дає вам якусь безглузду відповідь, 

234
00:15:28,645 --> 00:15:32,227
ніби вона відчуває себе впевненою, що цей випадковий шум це 5, 

235
00:15:32,227 --> 00:15:34,160
тому що фактичне зображення 5 є 5.

236
00:15:34,540 --> 00:15:38,771
Іншими словами, навіть якщо ця мережа досить добре розпізнає цифри, 

237
00:15:38,771 --> 00:15:40,700
вона не знає, як їх намалювати.

238
00:15:41,420 --> 00:15:45,240
Багато в чому це тому, що це жорстко обмежена система навчання.

239
00:15:45,880 --> 00:15:47,740
Я маю на увазі, поставте себе на місце мережі.

240
00:15:48,140 --> 00:15:52,726
З його точки зору, весь Всесвіт складається лише з чітко визначених нерухомих цифр, 

241
00:15:52,726 --> 00:15:57,094
зосереджених у крихітній сітці, і його функція вартості ніколи не давала жодних 

242
00:15:57,094 --> 00:16:01,080
стимулів бути чимось іншим, крім як абсолютно впевненим у своїх рішеннях.

243
00:16:02,120 --> 00:16:05,430
Отже, маючи це як образ того, що насправді роблять нейрони другого шару, 

244
00:16:05,430 --> 00:16:09,421
ви можете задатися питанням, чому я представив цю мережу з мотивацією підхоплення країв 

245
00:16:09,421 --> 00:16:09,920
і шаблонів.

246
00:16:09,920 --> 00:16:12,300
Я маю на увазі, що це зовсім не те, що це закінчується.

247
00:16:13,380 --> 00:16:17,180
Що ж, це не наша кінцева мета, а натомість відправна точка.

248
00:16:17,640 --> 00:16:21,651
Відверто кажучи, це стара технологія, яку досліджували у 80-х і 90-х роках, 

249
00:16:21,651 --> 00:16:25,978
і вам потрібно її зрозуміти, перш ніж ви зможете зрозуміти більш детальні сучасні 

250
00:16:25,978 --> 00:16:29,198
варіанти, і вона явно здатна вирішити деякі цікаві проблеми, 

251
00:16:29,198 --> 00:16:33,156
але чим більше ви заглиблюєтесь у те, що ті приховані шари дійсно роблять, 

252
00:16:33,156 --> 00:16:34,740
тим менш розумним це здається.

253
00:16:38,480 --> 00:16:42,093
Якщо на мить перенести фокус із того, як мережі навчаються, на те, 

254
00:16:42,093 --> 00:16:46,300
як навчаєтеся ви, це станеться лише за умови активного вивчення матеріалу тут.

255
00:16:47,060 --> 00:16:49,627
Я хочу, щоб ви зробили одну досить просту річ: 

256
00:16:49,627 --> 00:16:52,904
просто зупиніться зараз і на мить глибоко подумайте про те, 

257
00:16:52,904 --> 00:16:57,711
які зміни ви можете внести в цю систему та як вона сприймає зображення, якщо ви хочете, 

258
00:16:57,711 --> 00:17:00,880
щоб вона краще вловлювала такі речі, як краї та візерунки.

259
00:17:01,479 --> 00:17:04,560
Але краще за все, щоб справді ознайомитися з матеріалом, 

260
00:17:04,560 --> 00:17:09,099
я настійно рекомендую книгу Майкла Нільсена про глибоке навчання та нейронні мережі.

261
00:17:09,680 --> 00:17:13,876
У ньому ви можете знайти код і дані для завантаження та використання для 

262
00:17:13,876 --> 00:17:18,359
цього точного прикладу, і книга проведе вас крок за кроком, що цей код робить.

263
00:17:19,300 --> 00:17:22,424
Що чудово, так це те, що ця книга є безкоштовною та загальнодоступною, 

264
00:17:22,424 --> 00:17:24,887
тому, якщо ви щось отримаєте від неї, подумайте про те, 

265
00:17:24,887 --> 00:17:27,660
щоб приєднатися до мене та зробити пожертву на користь Nielsen.

266
00:17:27,660 --> 00:17:31,939
Я також пов’язав кілька інших ресурсів, які мені дуже подобаються, в описі, 

267
00:17:31,939 --> 00:17:36,500
включно з феноменальним і красивим дописом у блозі Кріса Оли та статті в Distill.

268
00:17:38,280 --> 00:17:42,687
Щоб завершити це на останні кілька хвилин, я хочу повернутися до фрагменту інтерв’ю, 

269
00:17:42,687 --> 00:17:43,880
яке я мав із Лейшею Лі.

270
00:17:44,300 --> 00:17:46,044
Можливо, ви пам’ятаєте її з останнього відео, вона 

271
00:17:46,044 --> 00:17:47,720
захистила докторську роботу з глибокого навчання.

272
00:17:48,300 --> 00:17:51,387
У цьому невеликому фрагменті вона розповідає про дві нещодавні статті, 

273
00:17:51,387 --> 00:17:55,301
які дійсно досліджують, як деякі з більш сучасних мереж розпізнавання зображень насправді 

274
00:17:55,301 --> 00:17:55,780
навчаються.

275
00:17:56,120 --> 00:17:58,530
Просто щоб визначити, де ми знаходимося в розмові, 

276
00:17:58,530 --> 00:18:01,602
перша стаття взяла одну з цих особливо глибоких нейронних мереж, 

277
00:18:01,602 --> 00:18:04,344
які справді добре розпізнають зображення, і замість того, 

278
00:18:04,344 --> 00:18:06,896
щоб навчати її на правильно позначеному наборі даних, 

279
00:18:06,896 --> 00:18:08,740
перетасувала всі мітки перед навчанням.

280
00:18:09,480 --> 00:18:13,206
Очевидно, що точність тестування тут була не кращою, ніж випадкова, 

281
00:18:13,206 --> 00:18:17,043
оскільки все просто випадково позначено, але все одно вдалося досягти 

282
00:18:17,043 --> 00:18:20,880
такої ж точності навчання, як і на правильно позначеному наборі даних.

283
00:18:21,600 --> 00:18:25,400
По суті, мільйонів ваг для цієї конкретної мережі було достатньо, 

284
00:18:25,400 --> 00:18:29,374
щоб вона просто запам’ятовувала випадкові дані, що піднімає питання, 

285
00:18:29,374 --> 00:18:33,981
чи насправді мінімізація цієї функції вартості відповідає будь-якій структурі в 

286
00:18:33,981 --> 00:18:36,400
зображенні, чи це просто запам’ятовування?

287
00:18:51,440 --> 00:18:56,578
Якщо ви подивіться на цю криву точності, якби ви просто тренувалися на 

288
00:18:56,578 --> 00:19:01,500
випадковому наборі даних, ця крива начебто опускалася дуже повільно 

289
00:19:01,500 --> 00:19:07,797
майже лінійним способом, тож вам справді важко знайти той локальний мінімум можливого, 

290
00:19:07,797 --> 00:19:12,140
ви знаєте правильні ваги, які забезпечать вам таку точність.

291
00:19:12,240 --> 00:19:16,575
Тоді як якщо ви насправді тренуєтеся на структурованому наборі даних, 

292
00:19:16,575 --> 00:19:21,964
який має правильні мітки, ви трохи возитеся на початку, але потім дуже швидко падаєте, 

293
00:19:21,964 --> 00:19:27,043
щоб досягти такого рівня точності, і тому в певному сенсі це було легше знайти ці 

294
00:19:27,043 --> 00:19:28,220
локальні максимуми.

295
00:19:28,540 --> 00:19:32,178
І що також було цікаво в цьому, це висвітлює іншу статтю, 

296
00:19:32,178 --> 00:19:37,007
написану фактично пару років тому, яка містить набагато більше спрощень щодо 

297
00:19:37,007 --> 00:19:40,645
мережевих рівнів, але один із результатів полягав у тому, 

298
00:19:40,645 --> 00:19:46,291
що якщо ви подивитеся на ландшафт оптимізації, локальні мінімуми, які вивчають ці мережі, 

299
00:19:46,291 --> 00:19:49,741
насправді мають однакову якість, тому в певному сенсі, 

300
00:19:49,741 --> 00:19:54,320
якщо ваш набір даних структурований, ви зможете знайти це набагато легше.

301
00:19:58,160 --> 00:20:01,180
Я дякую, як завжди, тим із вас, хто підтримує на Patreon.

302
00:20:01,520 --> 00:20:04,480
Раніше я вже говорив про те, що Patreon змінив правила гри, 

303
00:20:04,480 --> 00:20:06,800
але ці відео дійсно були б неможливими без вас.

304
00:20:07,460 --> 00:20:10,213
Я також хочу особливо подякувати фірмі венчурного капіталу 

305
00:20:10,213 --> 00:20:12,780
Amplify Partners за підтримку цих перших відео в серії.

