1
00:00:00,000 --> 00:00:07,240
У минулому відео я описав структуру нейронної мережі.

2
00:00:07,240 --> 00:00:10,097
Я дам тут короткий підсумок, щоб це було свіжим у нашій

3
00:00:10,097 --> 00:00:13,160
пам’яті, а потім у мене є дві головні цілі для цього відео.

4
00:00:13,160 --> 00:00:15,815
Перший полягає в тому, щоб представити ідею градієнтного спуску,

5
00:00:15,815 --> 00:00:18,226
яка лежить в основі не тільки того, як навчаються нейронні

6
00:00:18,226 --> 00:00:20,800
мережі, але й того, як працює багато інших машинного навчання.

7
00:00:20,800 --> 00:00:24,861
Після цього ми детальніше розглянемо, як працює ця

8
00:00:24,861 --> 00:00:29,560
конкретна мережа, і що шукають ці приховані шари нейронів.

9
00:00:29,560 --> 00:00:32,947
Нагадуємо, що нашою метою тут є класичний приклад

10
00:00:32,947 --> 00:00:37,080
розпізнавання рукописних цифр, привіт, світ нейронних мереж.

11
00:00:37,080 --> 00:00:40,430
Ці цифри відображаються на сітці 28x28 пікселів,

12
00:00:40,430 --> 00:00:44,260
кожен піксель має значення відтінків сірого від 0 до 1.

13
00:00:44,260 --> 00:00:51,400
Саме вони визначають активацію 784 нейронів на вхідному рівні мережі.

14
00:00:51,400 --> 00:00:56,621
Активація для кожного нейрона в наступних шарах базується на зваженій сумі всіх

15
00:00:56,621 --> 00:01:02,300
активацій у попередньому шарі, плюс деяке спеціальне число, яке називається зміщенням.

16
00:01:02,300 --> 00:01:05,937
Ви складаєте цю суму за допомогою якоїсь іншої функції,

17
00:01:05,937 --> 00:01:09,640
як-от сигмоїда, або ReLU, як я пройшов у минулому відео.

18
00:01:09,640 --> 00:01:14,602
Загалом, враховуючи дещо довільний вибір двох прихованих шарів із 16

19
00:01:14,602 --> 00:01:19,781
нейронами кожен, мережа має близько 13 000 ваг і зміщень, які ми можемо

20
00:01:19,781 --> 00:01:25,320
налаштувати, і саме ці значення визначають, що саме мережа насправді робить.

21
00:01:25,320 --> 00:01:29,609
Коли ми кажемо, що ця мережа класифікує певну цифру, ми маємо на увазі

22
00:01:29,609 --> 00:01:34,080
те, що найяскравіший із 10 нейронів останнього шару відповідає цій цифрі.

23
00:01:34,080 --> 00:01:37,741
І пам’ятайте, мотивація, яку ми мали на увазі для багатошарової

24
00:01:37,741 --> 00:01:41,688
структури, полягала в тому, що, можливо, другий шар міг би підхопити

25
00:01:41,688 --> 00:01:45,521
краї, третій шар міг би підхопити візерунки, як-от петлі та лінії,

26
00:01:45,521 --> 00:01:49,640
а останній міг би просто з’єднати ці візерунки, щоб розпізнавати цифри.

27
00:01:49,640 --> 00:01:52,880
Отже, ми дізнаємося, як навчається мережа.

28
00:01:52,880 --> 00:01:57,191
Те, що ми хочемо, — це алгоритм, за допомогою якого ви можете показати цій

29
00:01:57,191 --> 00:02:01,388
мережі цілий набір навчальних даних, які надходять у формі набору різних

30
00:02:01,388 --> 00:02:05,815
зображень рукописних цифр разом із мітками, якими вони мають бути, і це буде

31
00:02:05,815 --> 00:02:10,760
відкоригувати ці 13 000 ваг і зміщень, щоб покращити ефективність тренувальних даних.

32
00:02:10,760 --> 00:02:14,242
Сподіваюся, ця багаторівнева структура означатиме, що те, що

33
00:02:14,242 --> 00:02:17,840
вона вивчає, узагальнює зображення за межами навчальних даних.

34
00:02:17,840 --> 00:02:24,001
Ми перевіряємо це так: після навчання мережі ви показуєте їй більше

35
00:02:24,001 --> 00:02:31,160
позначених даних і бачите, наскільки точно вона класифікує ці нові зображення.

36
00:02:31,160 --> 00:02:35,781
На щастя для нас, і те, що робить цей приклад типовим для початку, полягає в тому,

37
00:02:35,781 --> 00:02:40,347
що хороші люди, що стоять за базою даних MNIST, зібрали колекцію з десятків тисяч

38
00:02:40,347 --> 00:02:45,080
рукописних зображень цифр, кожне з яких позначено номерами, якими вони повинні бути.

39
00:02:45,080 --> 00:02:48,667
І як би не було провокаційно описувати машину як навчальну, коли ви бачите,

40
00:02:48,667 --> 00:02:51,783
як вона працює, це стає набагато менше схожим на якусь божевільну

41
00:02:51,783 --> 00:02:55,560
науково-фантастичну передумову, а набагато більше схоже на вправу з обчислення.

42
00:02:55,560 --> 00:03:01,040
Я маю на увазі, що в основному це зводиться до пошуку мінімуму певної функції.

43
00:03:01,040 --> 00:03:05,911
Пам’ятайте, концептуально ми думаємо про те, що кожен нейрон пов’язаний

44
00:03:05,911 --> 00:03:10,849
з усіма нейронами попереднього шару, і ваги у зваженій сумі, що визначає

45
00:03:10,849 --> 00:03:15,179
його активацію, схожі на силу цих зв’язків, а зміщення є певним

46
00:03:15,179 --> 00:03:19,780
показником чи цей нейрон має тенденцію бути активним чи неактивним.

47
00:03:19,780 --> 00:03:25,020
І для початку ми просто ініціалізуємо всі ці ваги та зміщення абсолютно випадковим чином.

48
00:03:25,020 --> 00:03:27,927
Зайве говорити, що ця мережа працюватиме жахливо на даному

49
00:03:27,927 --> 00:03:31,180
навчальному прикладі, оскільки вона просто робить щось випадкове.

50
00:03:31,180 --> 00:03:36,820
Наприклад, ви подаєте на це зображення 3, а вихідний шар виглядає просто безладом.

51
00:03:36,820 --> 00:03:40,942
Отже, що ви робите, це визначаєте функцію вартості, спосіб сказати

52
00:03:40,942 --> 00:03:44,879
комп’ютеру, ні, поганий комп’ютер, що вихід має мати активації,

53
00:03:44,879 --> 00:03:48,940
які дорівнюють 0 для більшості нейронів, але 1 для цього нейрона.

54
00:03:48,940 --> 00:03:51,740
Те, що ти мені дав, це чисте сміття.

55
00:03:51,740 --> 00:03:56,695
Якщо говорити трохи математичніше, ви складаєте квадрати різниць між кожною

56
00:03:56,695 --> 00:04:01,325
з цих активацій виведення сміття та значенням, яке ви хочете, щоб вони

57
00:04:01,325 --> 00:04:06,020
мали, і це те, що ми називатимемо вартістю одного навчального прикладу.

58
00:04:06,020 --> 00:04:12,376
Зауважте, що ця сума невелика, коли мережа впевнено правильно класифікує

59
00:04:12,376 --> 00:04:18,820
зображення, але велика, коли здається, що мережа не знає, що вона робить.

60
00:04:18,820 --> 00:04:23,482
Отже, що ви робите, це розглядаєте середню вартість усіх десятків

61
00:04:23,482 --> 00:04:27,580
тисяч навчальних прикладів, які є у вашому розпорядженні.

62
00:04:27,580 --> 00:04:30,268
Ця середня вартість є нашим показником того, наскільки

63
00:04:30,268 --> 00:04:33,300
поганою є мережа та наскільки погано має працювати комп’ютер.

64
00:04:33,300 --> 00:04:35,300
І це складна річ.

65
00:04:35,300 --> 00:04:39,945
Пам’ятаєте, як сама мережа була в основному функцією, яка приймає 784

66
00:04:39,945 --> 00:04:44,922
числа як вхідні дані, значення пікселів, і викидає 10 чисел як свій вихід,

67
00:04:44,922 --> 00:04:49,700
і в певному сенсі вона параметризована всіма цими вагами та зміщеннями?

68
00:04:49,700 --> 00:04:53,340
Функція витрат є ще одним шаром складності.

69
00:04:53,340 --> 00:04:58,404
Він бере на вхід приблизно 13 000 ваг і упереджень і видає одне число, яке

70
00:04:58,404 --> 00:05:03,400
описує, наскільки погані ці ваги та упередження, і спосіб його визначення

71
00:05:03,400 --> 00:05:09,140
залежить від поведінки мережі над усіма десятками тисяч фрагментів навчальних даних.

72
00:05:09,140 --> 00:05:12,460
Це багато про що думати.

73
00:05:12,460 --> 00:05:16,380
Але просто говорити комп’ютеру, яку погану роботу він робить, не дуже корисно.

74
00:05:16,380 --> 00:05:21,300
Ви хочете сказати йому, як змінити ці ваги та упередження, щоб воно стало кращим.

75
00:05:21,300 --> 00:05:26,493
Щоб зробити це легше, замість того, щоб намагатися уявити функцію з 13 000 входами,

76
00:05:26,493 --> 00:05:31,440
просто уявіть просту функцію, яка має одне число як вхід і одне число як вихід.

77
00:05:31,440 --> 00:05:36,420
Як знайти вхідні дані, які мінімізують значення цієї функції?

78
00:05:36,420 --> 00:05:41,493
Студенти, які вивчають обчислення, знатимуть, що іноді можна чітко визначити цей мінімум,

79
00:05:41,493 --> 00:05:46,172
але це не завжди можливо для справді складних функцій, а особливо не у версії цієї

80
00:05:46,172 --> 00:05:51,189
ситуації з 13 000 вхідних даних для нашої божевільно складної функції вартості нейронної

81
00:05:51,189 --> 00:05:51,640
мережі.

82
00:05:51,640 --> 00:05:55,689
Більш гнучка тактика полягає в тому, щоб почати з будь-якого входу

83
00:05:55,689 --> 00:05:59,860
та визначити, у якому напрямку слід рухатися, щоб знизити цей вихід.

84
00:05:59,860 --> 00:06:03,767
Зокрема, якщо ви можете визначити нахил функції, де ви

85
00:06:03,767 --> 00:06:08,314
знаходитесь, тоді перемістіть ліворуч, якщо цей нахил додатний,

86
00:06:08,314 --> 00:06:12,720
і перемістіть вхідні дані праворуч, якщо цей нахил від’ємний.

87
00:06:12,720 --> 00:06:16,493
Якщо ви робите це неодноразово, у кожній точці перевіряючи новий нахил і

88
00:06:16,493 --> 00:06:20,680
роблячи відповідний крок, ви наблизитесь до деякого локального мінімуму функції.

89
00:06:20,680 --> 00:06:24,600
І зображення, яке ви можете мати тут на увазі, це м’яч, що котиться з пагорба.

90
00:06:24,600 --> 00:06:28,265
І зауважте, що навіть для цієї справді спрощеної функції єдиного введення

91
00:06:28,265 --> 00:06:31,930
існує багато можливих долин, у які ви можете потрапити, залежно від того,

92
00:06:31,930 --> 00:06:35,497
з якого випадкового введення ви почнете, і немає гарантії, що локальний

93
00:06:35,497 --> 00:06:39,460
мінімум, у який ви потрапите, буде найменшим можливим значенням функції витрат.

94
00:06:39,460 --> 00:06:43,180
Це також перенесеться на нашу нейронну мережу.

95
00:06:43,180 --> 00:06:47,130
І я також хочу, щоб ви помітили, що якщо ви робите розмір кроку

96
00:06:47,130 --> 00:06:51,266
пропорційним схилу, тоді, коли схил вирівнюється до мінімуму, ваші

97
00:06:51,266 --> 00:06:56,020
кроки стають все меншими і меншими, і це допомагає вам уникнути перевищення.

98
00:06:56,020 --> 00:07:01,640
Трохи збільшуючи складність, уявіть натомість функцію з двома входами та одним виходом.

99
00:07:01,640 --> 00:07:05,104
Ви можете подумати про вхідний простір як про

100
00:07:05,104 --> 00:07:09,020
площину xy, а функцію вартості як поверхню над нею.

101
00:07:09,020 --> 00:07:14,368
Замість того, щоб запитувати про нахил функції, ви повинні запитати, у якому напрямку

102
00:07:14,368 --> 00:07:19,780
вам слід зробити крок у цьому вхідному просторі, щоб найшвидше зменшити вихід функції.

103
00:07:19,780 --> 00:07:22,340
Іншими словами, який напрямок спуску?

104
00:07:22,340 --> 00:07:26,740
І знову ж таки, корисно подумати про м’яч, що котиться з того пагорба.

105
00:07:26,740 --> 00:07:30,877
Ті з вас, хто знайомий із численням багатьох змінних, знають,

106
00:07:30,877 --> 00:07:34,948
що градієнт функції дає вам напрямок найкрутішого підйому, у

107
00:07:34,948 --> 00:07:39,420
якому напрямку слід зробити крок, щоб збільшити функцію найшвидше.

108
00:07:39,420 --> 00:07:43,440
Цілком природно, що негативний градієнт дає вам

109
00:07:43,440 --> 00:07:47,460
напрямок кроку, який найшвидше зменшує функцію.

110
00:07:47,460 --> 00:07:51,020
Навіть більше того, довжина цього вектора градієнта є

111
00:07:51,020 --> 00:07:54,580
показником того, наскільки крутим є найкрутіший схил.

112
00:07:54,580 --> 00:07:57,672
Тепер, якщо ви не знайомі з численням багатьох змінних і хочете дізнатися

113
00:07:57,672 --> 00:08:01,100
більше, ознайомтеся з деякою роботою, яку я виконав для Академії Хана на цю тему.

114
00:08:01,100 --> 00:08:04,727
Але, чесно кажучи, для нас з вами зараз важливо лише те, що в

115
00:08:04,727 --> 00:08:08,295
принципі існує спосіб обчислити цей вектор, цей вектор, який

116
00:08:08,295 --> 00:08:12,040
повідомляє вам, яким є напрямок спуску та наскільки він крутий.

117
00:08:12,040 --> 00:08:17,280
З тобою все буде добре, якщо це все, що ти знаєш, і ти не розбираєшся в деталях.

118
00:08:17,280 --> 00:08:20,617
Тому що, якщо ви можете отримати це, алгоритм для мінімізації

119
00:08:20,617 --> 00:08:23,954
функції полягає в тому, щоб обчислити цей напрямок градієнта,

120
00:08:23,954 --> 00:08:27,400
потім зробити невеликий крок вниз і повторити це знову і знову.

121
00:08:27,400 --> 00:08:33,700
Це та сама основна ідея для функції, яка має 13 000 входів замість 2 входів.

122
00:08:33,700 --> 00:08:37,146
Уявіть собі організацію всіх 13 000 ваг і зміщень

123
00:08:37,146 --> 00:08:40,180
нашої мережі у гігантський вектор-стовпець.

124
00:08:40,180 --> 00:08:45,349
Від’ємний градієнт функції витрат — це просто вектор, це певний напрямок

125
00:08:45,349 --> 00:08:50,235
у цьому шалено величезному просторі введення, який говорить вам, які

126
00:08:50,235 --> 00:08:55,900
підштовхи до всіх цих чисел призведуть до найшвидшого зменшення функції витрат.

127
00:08:55,900 --> 00:08:59,552
І, звісно, завдяки нашій спеціально розробленій функції вартості зміна

128
00:08:59,552 --> 00:09:03,307
вагових коефіцієнтів і зміщень для їх зменшення означає, що вихідні дані

129
00:09:03,307 --> 00:09:07,319
мережі для кожної частини навчальних даних виглядатимуть не так як випадковий

130
00:09:07,319 --> 00:09:11,280
масив із 10 значень, а більше як фактичне рішення, яке ми хочемо це зробити.

131
00:09:11,280 --> 00:09:15,447
Важливо пам’ятати, що ця функція вартості передбачає середнє

132
00:09:15,447 --> 00:09:19,682
значення за всіма навчальними даними, тож якщо ви мінімізуєте

133
00:09:19,682 --> 00:09:24,260
її, це означає, що для всіх цих зразків буде краща продуктивність.

134
00:09:24,260 --> 00:09:27,520
Алгоритм для ефективного обчислення цього градієнта, який фактично

135
00:09:27,520 --> 00:09:30,877
є основою того, як нейронна мережа навчається, називається зворотним

136
00:09:30,877 --> 00:09:34,040
поширенням, і саме про нього я буду говорити в наступному відео.

137
00:09:34,040 --> 00:09:38,798
Там я справді хочу витратити час, щоб пройти через те, що саме відбувається з кожною

138
00:09:38,798 --> 00:09:43,613
вагою та зміщенням для певної частини тренувальних даних, намагаючись дати інтуїтивне

139
00:09:43,613 --> 00:09:47,980
відчуття того, що відбувається за межами купи відповідних обчислень і формул.

140
00:09:47,980 --> 00:09:51,723
Прямо тут, прямо зараз, головне, що я хочу, щоб ви знали, незалежно

141
00:09:51,723 --> 00:09:55,411
від деталей реалізації, це те, що ми маємо на увазі, коли говоримо

142
00:09:55,411 --> 00:09:59,320
про мережеве навчання, це те, що це просто мінімізація функції витрат.

143
00:09:59,320 --> 00:10:04,414
І зауважте, одним із наслідків цього є те, що для цієї функції витрат важливо мати гарний

144
00:10:04,414 --> 00:10:09,340
плавний результат, щоб ми могли знайти локальний мінімум, роблячи невеликі кроки вниз.

145
00:10:09,340 --> 00:10:14,890
Ось чому, до речі, штучні нейрони мають безперервний діапазон активацій, а

146
00:10:14,890 --> 00:10:20,440
не просто активні чи неактивні у бінарному порядку, як біологічні нейрони.

147
00:10:20,440 --> 00:10:23,551
Цей процес багаторазового підштовхування вхідних даних функції

148
00:10:23,551 --> 00:10:26,960
деяким кратним від’ємним градієнтом називається градієнтним спуском.

149
00:10:26,960 --> 00:10:29,950
Це спосіб сходитися до деякого локального мінімуму

150
00:10:29,950 --> 00:10:33,000
функції вартості, по суті, долини на цьому графіку.

151
00:10:33,000 --> 00:10:37,073
Звичайно, я все ще показую зображення функції з двома входами, тому

152
00:10:37,073 --> 00:10:41,086
що підштовхування у 13 000-вимірному просторі введення трохи важко

153
00:10:41,086 --> 00:10:45,220
уявити, але насправді є гарний непросторовий спосіб подумати про це.

154
00:10:45,220 --> 00:10:49,100
Кожен компонент негативного градієнта говорить нам про дві речі.

155
00:10:49,100 --> 00:10:52,713
Знак, звичайно, говорить нам про те, чи потрібно підштовхнути

156
00:10:52,713 --> 00:10:55,860
відповідний компонент вхідного вектора вгору чи вниз.

157
00:10:55,860 --> 00:11:00,349
Але важливо те, що відносні величини всіх цих

158
00:11:00,349 --> 00:11:05,620
компонентів ніби підказують вам, які зміни важливіші.

159
00:11:05,620 --> 00:11:09,978
Розумієте, у нашій мережі коригування однієї з ваг може мати

160
00:11:09,978 --> 00:11:14,980
набагато більший вплив на функцію витрат, ніж коригування іншої ваги.

161
00:11:14,980 --> 00:11:19,440
Деякі з цих зв’язків просто важливіші для наших навчальних даних.

162
00:11:19,440 --> 00:11:24,287
Таким чином, ви можете подумати про цей вектор градієнта нашої величезної функції

163
00:11:24,287 --> 00:11:29,252
витрат, що спотворює розум, так це те, що він кодує відносну важливість кожної ваги

164
00:11:29,252 --> 00:11:34,100
та зміщення, тобто те, яка з цих змін принесе найбільшу віддачу від ваших грошей.

165
00:11:34,100 --> 00:11:37,360
Це просто інший спосіб думати про напрямок.

166
00:11:37,360 --> 00:11:42,668
Щоб взяти простіший приклад, якщо у вас є деяка функція з двома змінними як вхідні

167
00:11:42,668 --> 00:11:47,657
дані, і ви обчислюєте, що її градієнт у певній точці виходить як 3,1, тоді, з

168
00:11:47,657 --> 00:11:52,902
одного боку, ви можете інтерпретувати це як те, що коли ви стоячи на цьому вході,

169
00:11:52,902 --> 00:11:58,211
рух уздовж цього напрямку збільшує функцію найшвидше, тому, коли ви будуєте графік

170
00:11:58,211 --> 00:12:03,200
функції над площиною вхідних точок, цей вектор дає вам прямий напрямок угору.

171
00:12:03,200 --> 00:12:07,951
Але інший спосіб прочитати це – сказати, що зміни цієї першої змінної мають утричі

172
00:12:07,951 --> 00:12:12,645
більшу важливість, ніж зміни другої змінної, що принаймні в околицях відповідного

173
00:12:12,645 --> 00:12:17,740
вхідного значення, підштовхування значення x несе набагато більше удару для вашого бакс.

174
00:12:17,740 --> 00:12:22,880
Гаразд, давайте зменшимо масштаб і підведемо підсумок, де ми зараз.

175
00:12:22,880 --> 00:12:26,563
Сама мережа є цією функцією з 784 входами та 10

176
00:12:26,563 --> 00:12:30,860
виходами, визначеними в термінах усіх цих зважених сум.

177
00:12:30,860 --> 00:12:34,160
Функція витрат є ще одним шаром складності.

178
00:12:34,160 --> 00:12:38,107
Він приймає 13 000 ваг і упереджень як вхідні дані та

179
00:12:38,107 --> 00:12:42,640
викидає єдину міру паршивості на основі навчальних прикладів.

180
00:12:42,640 --> 00:12:47,520
Градієнт функції витрат — це ще один рівень складності.

181
00:12:47,520 --> 00:12:52,374
Він говорить нам, які підштовхи до всіх цих ваг і упереджень

182
00:12:52,374 --> 00:12:57,468
спричиняють найшвидшу зміну значення функції вартості, що можна

183
00:12:57,468 --> 00:13:03,040
інтерпретувати як те, які зміни до яких ваг мають найбільше значення.

184
00:13:03,040 --> 00:13:06,671
Отже, коли ви ініціалізуєте мережу випадковими вагами та зміщеннями та

185
00:13:06,671 --> 00:13:10,455
багато разів налаштовуєте їх на основі цього процесу градієнтного спуску,

186
00:13:10,455 --> 00:13:14,240
наскільки добре вона справді працює на зображеннях, яких раніше не бачив?

187
00:13:14,240 --> 00:13:18,266
Той, який я описав тут, із двома прихованими шарами по 16 нейронів

188
00:13:18,266 --> 00:13:22,593
кожен, обраними здебільшого з естетичних міркувань, непоганий, оскільки

189
00:13:22,593 --> 00:13:26,920
він класифікує приблизно 96% нових зображень, які він бачить правильно.

190
00:13:26,920 --> 00:13:31,529
І, чесно кажучи, якщо ви подивіться на деякі приклади, з

191
00:13:31,529 --> 00:13:36,300
якими він зіпсувався, ви відчуєте потребу трохи послабити.

192
00:13:36,300 --> 00:13:38,602
Якщо ви пограєте зі структурою прихованого шару та

193
00:13:38,602 --> 00:13:41,220
зробите кілька налаштувань, ви можете отримати це до 98%.

194
00:13:41,220 --> 00:13:42,900
І це дуже добре!

195
00:13:42,900 --> 00:13:46,659
Це не найкраще, ви, звичайно, можете отримати кращу продуктивність, ставши

196
00:13:46,659 --> 00:13:50,419
більш складною, ніж ця звичайна ванільна мережа, але враховуючи, наскільки

197
00:13:50,419 --> 00:13:54,129
складним є початкове завдання, я вважаю, що є щось неймовірне в тому, щоб

198
00:13:54,129 --> 00:13:58,089
будь-яка мережа так добре справлялася із зображеннями, яких вона ніколи раніше

199
00:13:58,089 --> 00:14:02,000
не бачила, враховуючи, що ми ніколи конкретно не говорив, які шаблони шукати.

200
00:14:02,000 --> 00:14:07,277
Спочатку я мотивував цю структуру, описуючи надію, яку ми могли б мати, що другий

201
00:14:07,277 --> 00:14:12,813
шар може підхопити маленькі краї, що третій шар з’єднає ці краї разом, щоб розпізнати

202
00:14:12,813 --> 00:14:18,220
петлі та довші лінії, і що вони можуть бути складені разом, щоб розпізнавати цифри.

203
00:14:18,220 --> 00:14:21,040
Отже, це те, що насправді робить наша мережа?

204
00:14:21,040 --> 00:14:24,880
Ну, принаймні для цього, зовсім ні.

205
00:14:24,880 --> 00:14:28,908
Пам’ятаєте, як у минулому відео ми дивилися на те, як ваги зв’язків

206
00:14:28,908 --> 00:14:32,937
від усіх нейронів першого шару до даного нейрона другого шару можна

207
00:14:32,937 --> 00:14:37,440
візуалізувати як даний піксельний шаблон, який нейрон другого шару вловлює?

208
00:14:37,440 --> 00:14:43,026
Що ж, коли ми робимо це для ваг, пов’язаних із цими переходами, замість

209
00:14:43,026 --> 00:14:48,458
того, щоб шукати окремі маленькі краї тут і там, вони виглядають, ну,

210
00:14:48,458 --> 00:14:54,200
майже випадковими, просто з деякими дуже вільними візерунками посередині.

211
00:14:54,200 --> 00:14:58,903
Здавалося б, що в незбагненно великому 13 000-вимірному просторі можливих ваг і

212
00:14:58,903 --> 00:15:04,136
упереджень наша мережа знайшла щасливий маленький локальний мінімум, який, незважаючи на

213
00:15:04,136 --> 00:15:09,075
успішну класифікацію більшості зображень, не точно вловлює шаблони, на які ми могли

214
00:15:09,075 --> 00:15:09,840
сподіватися.

215
00:15:09,840 --> 00:15:12,655
Щоб переконатися в цьому, подивіться, що відбувається,

216
00:15:12,655 --> 00:15:14,600
коли ви вводите випадкове зображення.

217
00:15:14,600 --> 00:15:18,961
Якби система була розумною, ви могли б очікувати, що вона буде відчувати себе

218
00:15:18,961 --> 00:15:23,657
невизначеною, можливо, насправді не активуючи жоден із цих 10 вихідних нейронів або

219
00:15:23,657 --> 00:15:28,186
активуючи їх усі рівномірно, але натомість вона впевнено дає вам якусь безглузду

220
00:15:28,186 --> 00:15:32,882
відповідь, ніби вона відчуває себе впевненою, що цей випадковий Шум – це 5, тому що

221
00:15:32,882 --> 00:15:34,560
фактичне зображення 5 – це 5.

222
00:15:34,560 --> 00:15:38,252
Іншими словами, навіть якщо ця мережа досить добре

223
00:15:38,252 --> 00:15:41,800
розпізнає цифри, вона не знає, як їх намалювати.

224
00:15:41,800 --> 00:15:45,400
Багато в чому це тому, що це жорстко обмежена система навчання.

225
00:15:45,400 --> 00:15:48,220
Я маю на увазі, поставте себе на місце мережі.

226
00:15:48,220 --> 00:15:52,788
З його точки зору, весь Всесвіт складається лише з чітко визначених нерухомих

227
00:15:52,788 --> 00:15:57,415
цифр, зосереджених у крихітній сітці, і його функція вартості ніколи не давала

228
00:15:57,415 --> 00:16:02,160
жодних стимулів бути чимось іншим, крім як абсолютно впевненим у своїх рішеннях.

229
00:16:02,160 --> 00:16:06,074
Отже, маючи це як образ того, що насправді роблять нейрони другого шару, ви можете

230
00:16:06,074 --> 00:16:10,320
задатися питанням, чому я представив цю мережу з мотивацією підхоплення країв і шаблонів.

231
00:16:10,320 --> 00:16:13,040
Я маю на увазі, що це зовсім не те, що це закінчується.

232
00:16:13,040 --> 00:16:17,480
Що ж, це не наша кінцева мета, а натомість відправна точка.

233
00:16:17,480 --> 00:16:22,839
Відверто кажучи, це стара технологія, яку досліджували у 80-х і 90-х роках, і вам

234
00:16:22,839 --> 00:16:27,805
потрібно її зрозуміти, перш ніж ви зможете зрозуміти більш детальні сучасні

235
00:16:27,805 --> 00:16:32,968
варіанти, і вона явно здатна вирішити деякі цікаві проблеми, але чим більше ви

236
00:16:32,968 --> 00:16:38,720
заглиблюєтесь у те, що ті приховані шари дійсно роблять, тим менш розумним це здається.

237
00:16:38,720 --> 00:16:42,766
Якщо на мить перенести фокус із того, як мережі навчаються, на те, як

238
00:16:42,766 --> 00:16:47,160
навчаєтеся ви, це станеться лише за умови активного вивчення матеріалу тут.

239
00:16:47,160 --> 00:16:51,854
Я хочу, щоб ви зробили одну досить просту річ: просто зупиніться зараз і на мить

240
00:16:51,854 --> 00:16:56,838
глибоко подумайте про те, які зміни ви можете внести в цю систему та як вона сприймає

241
00:16:56,838 --> 00:17:01,880
зображення, якщо ви хочете, щоб вона краще вловлювала такі речі, як краї та візерунки.

242
00:17:01,880 --> 00:17:05,634
Але краще за все, щоб справді ознайомитися з матеріалом, я настійно

243
00:17:05,634 --> 00:17:09,720
рекомендую книгу Майкла Нільсена про глибоке навчання та нейронні мережі.

244
00:17:09,720 --> 00:17:14,349
У ньому ви можете знайти код і дані для завантаження та використання для

245
00:17:14,349 --> 00:17:19,360
цього точного прикладу, і книга проведе вас крок за кроком, що цей код робить.

246
00:17:19,360 --> 00:17:22,586
Що чудово, так це те, що ця книга є безкоштовною та загальнодоступною,

247
00:17:22,586 --> 00:17:25,313
тому, якщо ви щось отримаєте від неї, подумайте про те, щоб

248
00:17:25,313 --> 00:17:28,040
приєднатися до мене та зробити пожертву на користь Nielsen.

249
00:17:28,040 --> 00:17:33,177
Я також пов’язав кілька інших ресурсів, які мені дуже подобаються, в описі,

250
00:17:33,177 --> 00:17:38,720
включно з феноменальним і красивим дописом у блозі Кріса Оли та статті в Distill.

251
00:17:38,720 --> 00:17:41,343
Щоб завершити це на останні кілька хвилин, я хочу

252
00:17:41,343 --> 00:17:44,440
повернутися до фрагменту інтерв’ю, яке я мав із Лейшею Лі.

253
00:17:44,440 --> 00:17:46,500
Можливо, ви пам’ятаєте її з останнього відео, вона

254
00:17:46,500 --> 00:17:48,520
захистила докторську роботу з глибокого навчання.

255
00:17:48,520 --> 00:17:52,269
У цьому невеличкому фрагменті вона розповідає про дві нещодавні статті, які дійсно

256
00:17:52,269 --> 00:17:55,837
досліджують, як деякі з більш сучасних мереж розпізнавання зображень насправді

257
00:17:55,837 --> 00:17:56,380
навчаються.

258
00:17:56,380 --> 00:17:59,454
Просто щоб визначити, де ми знаходимося в розмові, перша стаття

259
00:17:59,454 --> 00:18:02,529
взяла одну з цих особливо глибоких нейронних мереж, яка справді

260
00:18:02,529 --> 00:18:05,508
добре розпізнає зображення, і замість того, щоб навчати її на

261
00:18:05,508 --> 00:18:09,400
правильно позначеному наборі даних, вона перетасувала всі мітки перед навчанням.

262
00:18:09,400 --> 00:18:12,411
Очевидно, що точність тестування тут мала бути не кращою,

263
00:18:12,411 --> 00:18:15,320
ніж випадкова, оскільки все просто випадково позначено.

264
00:18:15,320 --> 00:18:18,167
Але він все одно зміг досягти такої ж точності

265
00:18:18,167 --> 00:18:21,440
навчання, як і на правильно позначеному наборі даних.

266
00:18:21,440 --> 00:18:26,296
По суті, мільйонів ваг для цієї конкретної мережі було достатньо, щоб вона просто

267
00:18:26,296 --> 00:18:31,212
запам’ятовувала випадкові дані, що піднімає питання, чи насправді мінімізація цієї

268
00:18:31,212 --> 00:18:35,653
функції вартості відповідає будь-якій структурі в зображенні, чи це просто

269
00:18:35,653 --> 00:18:36,720
запам’ятовування?

270
00:18:36,720 --> 00:18:40,120
. . . щоб запам’ятати весь набір даних, що таке правильна класифікація.

271
00:18:40,120 --> 00:18:43,967
І ось кілька, ви знаєте, півроку пізніше на ICML цього року, була не

272
00:18:43,967 --> 00:18:48,093
зовсім спростувальна стаття, а стаття, в якій розглядалися деякі аспекти,

273
00:18:48,093 --> 00:18:52,220
наприклад, ага, насправді ці мережі роблять щось трохи розумніше, ніж це.

274
00:18:52,220 --> 00:18:58,803
Якщо ви подивитеся на цю криву точності, якби ви просто тренувалися на випадковому наборі

275
00:18:58,803 --> 00:19:05,240
даних, ця крива начебто опускалася дуже, знаєте, дуже повільно майже лінійним способом.

276
00:19:05,240 --> 00:19:08,875
Тож вам справді важко знайти локальні мінімуми можливих,

277
00:19:08,875 --> 00:19:12,320
знаєте, правильних ваг, які дадуть вам таку точність.

278
00:19:12,320 --> 00:19:16,083
У той час як якщо ви насправді тренуєтеся на структурованому наборі даних,

279
00:19:16,083 --> 00:19:19,546
який має правильні мітки, ви знаєте, ви знаєте, ви трохи возитеся на

280
00:19:19,546 --> 00:19:23,360
початку, але потім ви дуже швидко впали, щоб досягти такого рівня точності.

281
00:19:23,360 --> 00:19:28,580
І тому в якомусь сенсі було легше знайти ці локальні максимуми.

282
00:19:28,580 --> 00:19:34,360
І що також було цікаво в цьому, це висвітлює іншу статтю, фактично пару

283
00:19:34,360 --> 00:19:40,140
років тому, яка містить набагато більше спрощень щодо мережевих рівнів.

284
00:19:40,140 --> 00:19:44,552
Але один із результатів показав, що, якщо ви подивитеся на ландшафт оптимізації,

285
00:19:44,552 --> 00:19:49,400
локальні мінімуми, які ці мережі, як правило, вивчають, насправді мають однакову якість.

286
00:19:49,400 --> 00:19:52,391
Тож у певному сенсі, якщо ваш набір даних структурований,

287
00:19:52,391 --> 00:19:54,300
ви зможете знайти це набагато легше.

288
00:19:54,300 --> 00:20:01,140
Я, як завжди, дякую тим із вас, хто підтримує на Patreon.

289
00:20:01,140 --> 00:20:04,323
Раніше я вже говорив про те, що змінює гру на Patreon,

290
00:20:04,323 --> 00:20:07,160
але ці відео справді були б неможливими без вас.

291
00:20:07,160 --> 00:20:10,347
Я також хочу висловити особливу подяку фірмі венчурного капіталу

292
00:20:10,347 --> 00:20:13,240
Amplify Partners та її підтримці цих перших відео в серії.

293
00:20:13,240 --> 00:20:33,140
Дякую тобі.

