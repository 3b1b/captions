1
00:00:00,000 --> 00:00:07,240
У минулому відео я описав структуру нейронної мережі.

2
00:00:07,240 --> 00:00:10,557
Я дам тут короткий підсумок, щоб це було свіжим у нашій пам’яті,

3
00:00:10,557 --> 00:00:13,160
а потім у мене є дві головні цілі для цього відео.

4
00:00:13,160 --> 00:00:15,815
Перший полягає в тому, щоб представити ідею градієнтного спуску,

5
00:00:15,815 --> 00:00:18,552
яка лежить в основі не тільки того, як навчаються нейронні мережі,

6
00:00:18,552 --> 00:00:20,800
але й того, як працює багато інших машинного навчання.

7
00:00:20,800 --> 00:00:26,294
Після цього ми детальніше розглянемо, як працює ця конкретна мережа,

8
00:00:26,294 --> 00:00:29,560
і що шукають ці приховані шари нейронів.

9
00:00:29,560 --> 00:00:35,047
Нагадуємо, що нашою метою тут є класичний приклад розпізнавання рукописних цифр,

10
00:00:35,047 --> 00:00:37,080
привіт, світ нейронних мереж.

11
00:00:37,080 --> 00:00:40,430
Ці цифри відображаються на сітці 28x28 пікселів,

12
00:00:40,430 --> 00:00:44,260
кожен піксель має значення відтінків сірого від 0 до 1.

13
00:00:44,260 --> 00:00:51,400
Саме вони визначають активацію 784 нейронів на вхідному рівні мережі.

14
00:00:51,400 --> 00:00:56,621
Активація для кожного нейрона в наступних шарах базується на зваженій сумі всіх

15
00:00:56,621 --> 00:01:02,300
активацій у попередньому шарі, плюс деяке спеціальне число, яке називається зміщенням.

16
00:01:02,300 --> 00:01:05,937
Ви складаєте цю суму за допомогою якоїсь іншої функції,

17
00:01:05,937 --> 00:01:09,640
як-от сигмоїда, або ReLU, як я пройшов у минулому відео.

18
00:01:09,640 --> 00:01:15,825
Загалом, враховуючи дещо довільний вибір двох прихованих шарів із 16 нейронами кожен,

19
00:01:15,825 --> 00:01:20,716
мережа має близько 13 000 ваг і зміщень, які ми можемо налаштувати,

20
00:01:20,716 --> 00:01:25,320
і саме ці значення визначають, що саме мережа насправді робить.

21
00:01:25,320 --> 00:01:29,851
Коли ми кажемо, що ця мережа класифікує певну цифру, ми маємо на увазі те,

22
00:01:29,851 --> 00:01:34,080
що найяскравіший із 10 нейронів останнього шару відповідає цій цифрі.

23
00:01:34,080 --> 00:01:38,370
І пам’ятайте, мотивація, яку ми мали на увазі для багатошарової структури,

24
00:01:38,370 --> 00:01:42,031
полягала в тому, що, можливо, другий шар міг би підхопити краї,

25
00:01:42,031 --> 00:01:45,521
третій шар міг би підхопити візерунки, як-от петлі та лінії,

26
00:01:45,521 --> 00:01:49,640
а останній міг би просто з’єднати ці візерунки, щоб розпізнавати цифри.

27
00:01:49,640 --> 00:01:52,880
Отже, ми дізнаємося, як навчається мережа.

28
00:01:52,880 --> 00:01:57,191
Те, що ми хочемо, — це алгоритм, за допомогою якого ви можете показати цій

29
00:01:57,191 --> 00:02:01,388
мережі цілий набір навчальних даних, які надходять у формі набору різних

30
00:02:01,388 --> 00:02:05,240
зображень рукописних цифр разом із мітками, якими вони мають бути,

31
00:02:05,240 --> 00:02:08,057
і це буде відкоригувати ці 13 000 ваг і зміщень,

32
00:02:08,057 --> 00:02:10,760
щоб покращити ефективність тренувальних даних.

33
00:02:10,760 --> 00:02:14,071
Сподіваюся, ця багаторівнева структура означатиме, що те,

34
00:02:14,071 --> 00:02:17,840
що вона вивчає, узагальнює зображення за межами навчальних даних.

35
00:02:17,840 --> 00:02:24,001
Ми перевіряємо це так: після навчання мережі ви показуєте їй більше

36
00:02:24,001 --> 00:02:31,160
позначених даних і бачите, наскільки точно вона класифікує ці нові зображення.

37
00:02:31,160 --> 00:02:35,781
На щастя для нас, і те, що робить цей приклад типовим для початку, полягає в тому,

38
00:02:35,781 --> 00:02:38,454
що хороші люди, що стоять за базою даних MNIST,

39
00:02:38,454 --> 00:02:41,850
зібрали колекцію з десятків тисяч рукописних зображень цифр,

40
00:02:41,850 --> 00:02:45,080
кожне з яких позначено номерами, якими вони повинні бути.

41
00:02:45,080 --> 00:02:48,667
І як би не було провокаційно описувати машину як навчальну, коли ви бачите,

42
00:02:48,667 --> 00:02:51,783
як вона працює, це стає набагато менше схожим на якусь божевільну

43
00:02:51,783 --> 00:02:55,560
науково-фантастичну передумову, а набагато більше схоже на вправу з обчислення.

44
00:02:55,560 --> 00:03:01,040
Я маю на увазі, що в основному це зводиться до пошуку мінімуму певної функції.

45
00:03:01,040 --> 00:03:05,911
Пам’ятайте, концептуально ми думаємо про те, що кожен нейрон пов’язаний

46
00:03:05,911 --> 00:03:10,037
з усіма нейронами попереднього шару, і ваги у зваженій сумі,

47
00:03:10,037 --> 00:03:13,826
що визначає його активацію, схожі на силу цих зв’язків,

48
00:03:13,826 --> 00:03:19,780
а зміщення є певним показником чи цей нейрон має тенденцію бути активним чи неактивним.

49
00:03:19,780 --> 00:03:25,020
І для початку ми просто ініціалізуємо всі ці ваги та зміщення абсолютно випадковим чином.

50
00:03:25,020 --> 00:03:29,011
Зайве говорити, що ця мережа працюватиме жахливо на даному навчальному прикладі,

51
00:03:29,011 --> 00:03:31,180
оскільки вона просто робить щось випадкове.

52
00:03:31,180 --> 00:03:36,820
Наприклад, ви подаєте на це зображення 3, а вихідний шар виглядає просто безладом.

53
00:03:36,820 --> 00:03:40,019
Отже, що ви робите, це визначаєте функцію вартості,

54
00:03:40,019 --> 00:03:44,879
спосіб сказати комп’ютеру, ні, поганий комп’ютер, що вихід має мати активації,

55
00:03:44,879 --> 00:03:48,940
які дорівнюють 0 для більшості нейронів, але 1 для цього нейрона.

56
00:03:48,940 --> 00:03:51,740
Те, що ти мені дав, це чисте сміття.

57
00:03:51,740 --> 00:03:56,695
Якщо говорити трохи математичніше, ви складаєте квадрати різниць між кожною

58
00:03:56,695 --> 00:04:01,716
з цих активацій виведення сміття та значенням, яке ви хочете, щоб вони мали,

59
00:04:01,716 --> 00:04:06,020
і це те, що ми називатимемо вартістю одного навчального прикладу.

60
00:04:06,020 --> 00:04:13,421
Зауважте, що ця сума невелика, коли мережа впевнено правильно класифікує зображення,

61
00:04:13,421 --> 00:04:18,820
але велика, коли здається, що мережа не знає, що вона робить.

62
00:04:18,820 --> 00:04:23,482
Отже, що ви робите, це розглядаєте середню вартість усіх десятків

63
00:04:23,482 --> 00:04:27,580
тисяч навчальних прикладів, які є у вашому розпорядженні.

64
00:04:27,580 --> 00:04:30,268
Ця середня вартість є нашим показником того, наскільки

65
00:04:30,268 --> 00:04:33,300
поганою є мережа та наскільки погано має працювати комп’ютер.

66
00:04:33,300 --> 00:04:35,300
І це складна річ.

67
00:04:35,300 --> 00:04:38,883
Пам’ятаєте, як сама мережа була в основному функцією,

68
00:04:38,883 --> 00:04:42,665
яка приймає 784 числа як вхідні дані, значення пікселів,

69
00:04:42,665 --> 00:04:47,510
і викидає 10 чисел як свій вихід, і в певному сенсі вона параметризована

70
00:04:47,510 --> 00:04:49,700
всіма цими вагами та зміщеннями?

71
00:04:49,700 --> 00:04:53,340
Функція витрат є ще одним шаром складності.

72
00:04:53,340 --> 00:04:58,944
Він бере на вхід приблизно 13 000 ваг і упереджень і видає одне число, яке описує,

73
00:04:58,944 --> 00:05:04,278
наскільки погані ці ваги та упередження, і спосіб його визначення залежить від

74
00:05:04,278 --> 00:05:09,140
поведінки мережі над усіма десятками тисяч фрагментів навчальних даних.

75
00:05:09,140 --> 00:05:12,460
Це багато про що думати.

76
00:05:12,460 --> 00:05:16,380
Але просто говорити комп’ютеру, яку погану роботу він робить, не дуже корисно.

77
00:05:16,380 --> 00:05:21,300
Ви хочете сказати йому, як змінити ці ваги та упередження, щоб воно стало кращим.

78
00:05:21,300 --> 00:05:26,493
Щоб зробити це легше, замість того, щоб намагатися уявити функцію з 13 000 входами,

79
00:05:26,493 --> 00:05:31,440
просто уявіть просту функцію, яка має одне число як вхід і одне число як вихід.

80
00:05:31,440 --> 00:05:36,420
Як знайти вхідні дані, які мінімізують значення цієї функції?

81
00:05:36,420 --> 00:05:41,493
Студенти, які вивчають обчислення, знатимуть, що іноді можна чітко визначити цей мінімум,

82
00:05:41,493 --> 00:05:44,593
але це не завжди можливо для справді складних функцій,

83
00:05:44,593 --> 00:05:49,667
а особливо не у версії цієї ситуації з 13 000 вхідних даних для нашої божевільно складної

84
00:05:49,667 --> 00:05:51,640
функції вартості нейронної мережі.

85
00:05:51,640 --> 00:05:56,535
Більш гнучка тактика полягає в тому, щоб почати з будь-якого входу та визначити,

86
00:05:56,535 --> 00:05:59,860
у якому напрямку слід рухатися, щоб знизити цей вихід.

87
00:05:59,860 --> 00:06:04,691
Зокрема, якщо ви можете визначити нахил функції, де ви знаходитесь,

88
00:06:04,691 --> 00:06:08,314
тоді перемістіть ліворуч, якщо цей нахил додатний,

89
00:06:08,314 --> 00:06:12,720
і перемістіть вхідні дані праворуч, якщо цей нахил від’ємний.

90
00:06:12,720 --> 00:06:16,493
Якщо ви робите це неодноразово, у кожній точці перевіряючи новий нахил і

91
00:06:16,493 --> 00:06:20,680
роблячи відповідний крок, ви наблизитесь до деякого локального мінімуму функції.

92
00:06:20,680 --> 00:06:24,600
І зображення, яке ви можете мати тут на увазі, це м’яч, що котиться з пагорба.

93
00:06:24,600 --> 00:06:28,265
І зауважте, що навіть для цієї справді спрощеної функції єдиного введення

94
00:06:28,265 --> 00:06:31,930
існує багато можливих долин, у які ви можете потрапити, залежно від того,

95
00:06:31,930 --> 00:06:35,943
з якого випадкового введення ви почнете, і немає гарантії, що локальний мінімум,

96
00:06:35,943 --> 00:06:39,460
у який ви потрапите, буде найменшим можливим значенням функції витрат.

97
00:06:39,460 --> 00:06:43,180
Це також перенесеться на нашу нейронну мережу.

98
00:06:43,180 --> 00:06:48,365
І я також хочу, щоб ви помітили, що якщо ви робите розмір кроку пропорційним схилу,

99
00:06:48,365 --> 00:06:53,489
тоді, коли схил вирівнюється до мінімуму, ваші кроки стають все меншими і меншими,

100
00:06:53,489 --> 00:06:56,020
і це допомагає вам уникнути перевищення.

101
00:06:56,020 --> 00:07:01,640
Трохи збільшуючи складність, уявіть натомість функцію з двома входами та одним виходом.

102
00:07:01,640 --> 00:07:06,007
Ви можете подумати про вхідний простір як про площину xy,

103
00:07:06,007 --> 00:07:09,020
а функцію вартості як поверхню над нею.

104
00:07:09,020 --> 00:07:13,311
Замість того, щоб запитувати про нахил функції, ви повинні запитати,

105
00:07:13,311 --> 00:07:17,416
у якому напрямку вам слід зробити крок у цьому вхідному просторі,

106
00:07:17,416 --> 00:07:19,780
щоб найшвидше зменшити вихід функції.

107
00:07:19,780 --> 00:07:22,340
Іншими словами, який напрямок спуску?

108
00:07:22,340 --> 00:07:26,740
І знову ж таки, корисно подумати про м’яч, що котиться з того пагорба.

109
00:07:26,740 --> 00:07:30,877
Ті з вас, хто знайомий із численням багатьох змінних, знають,

110
00:07:30,877 --> 00:07:34,815
що градієнт функції дає вам напрямок найкрутішого підйому,

111
00:07:34,815 --> 00:07:39,420
у якому напрямку слід зробити крок, щоб збільшити функцію найшвидше.

112
00:07:39,420 --> 00:07:44,780
Цілком природно, що негативний градієнт дає вам напрямок кроку,

113
00:07:44,780 --> 00:07:47,460
який найшвидше зменшує функцію.

114
00:07:47,460 --> 00:07:52,140
Навіть більше того, довжина цього вектора градієнта є показником того,

115
00:07:52,140 --> 00:07:54,580
наскільки крутим є найкрутіший схил.

116
00:07:54,580 --> 00:07:58,007
Тепер, якщо ви не знайомі з численням багатьох змінних і хочете дізнатися більше,

117
00:07:58,007 --> 00:08:01,100
ознайомтеся з деякою роботою, яку я виконав для Академії Хана на цю тему.

118
00:08:01,100 --> 00:08:04,434
Але, чесно кажучи, для нас з вами зараз важливо лише те,

119
00:08:04,434 --> 00:08:08,003
що в принципі існує спосіб обчислити цей вектор, цей вектор,

120
00:08:08,003 --> 00:08:12,040
який повідомляє вам, яким є напрямок спуску та наскільки він крутий.

121
00:08:12,040 --> 00:08:17,280
З тобою все буде добре, якщо це все, що ти знаєш, і ти не розбираєшся в деталях.

122
00:08:17,280 --> 00:08:21,909
Тому що, якщо ви можете отримати це, алгоритм для мінімізації функції полягає в тому,

123
00:08:21,909 --> 00:08:25,246
щоб обчислити цей напрямок градієнта, потім зробити невеликий

124
00:08:25,246 --> 00:08:27,400
крок вниз і повторити це знову і знову.

125
00:08:27,400 --> 00:08:33,700
Це та сама основна ідея для функції, яка має 13 000 входів замість 2 входів.

126
00:08:33,700 --> 00:08:37,146
Уявіть собі організацію всіх 13 000 ваг і зміщень

127
00:08:37,146 --> 00:08:40,180
нашої мережі у гігантський вектор-стовпець.

128
00:08:40,180 --> 00:08:44,003
Від’ємний градієнт функції витрат — це просто вектор,

129
00:08:44,003 --> 00:08:48,606
це певний напрямок у цьому шалено величезному просторі введення,

130
00:08:48,606 --> 00:08:54,058
який говорить вам, які підштовхи до всіх цих чисел призведуть до найшвидшого

131
00:08:54,058 --> 00:08:55,900
зменшення функції витрат.

132
00:08:55,900 --> 00:08:59,552
І, звісно, завдяки нашій спеціально розробленій функції вартості зміна

133
00:08:59,552 --> 00:09:02,484
вагових коефіцієнтів і зміщень для їх зменшення означає,

134
00:09:02,484 --> 00:09:06,239
що вихідні дані мережі для кожної частини навчальних даних виглядатимуть

135
00:09:06,239 --> 00:09:09,942
не так як випадковий масив із 10 значень, а більше як фактичне рішення,

136
00:09:09,942 --> 00:09:11,280
яке ми хочемо це зробити.

137
00:09:11,280 --> 00:09:15,447
Важливо пам’ятати, що ця функція вартості передбачає середнє

138
00:09:15,447 --> 00:09:19,956
значення за всіма навчальними даними, тож якщо ви мінімізуєте її,

139
00:09:19,956 --> 00:09:24,260
це означає, що для всіх цих зразків буде краща продуктивність.

140
00:09:24,260 --> 00:09:26,838
Алгоритм для ефективного обчислення цього градієнта,

141
00:09:26,838 --> 00:09:29,806
який фактично є основою того, як нейронна мережа навчається,

142
00:09:29,806 --> 00:09:34,040
називається зворотним поширенням, і саме про нього я буду говорити в наступному відео.

143
00:09:34,040 --> 00:09:37,119
Там я справді хочу витратити час, щоб пройти через те,

144
00:09:37,119 --> 00:09:42,045
що саме відбувається з кожною вагою та зміщенням для певної частини тренувальних даних,

145
00:09:42,045 --> 00:09:46,860
намагаючись дати інтуїтивне відчуття того, що відбувається за межами купи відповідних

146
00:09:46,860 --> 00:09:47,980
обчислень і формул.

147
00:09:47,980 --> 00:09:51,172
Прямо тут, прямо зараз, головне, що я хочу, щоб ви знали,

148
00:09:51,172 --> 00:09:54,640
незалежно від деталей реалізації, це те, що ми маємо на увазі,

149
00:09:54,640 --> 00:09:59,320
коли говоримо про мережеве навчання, це те, що це просто мінімізація функції витрат.

150
00:09:59,320 --> 00:10:04,414
І зауважте, одним із наслідків цього є те, що для цієї функції витрат важливо мати гарний

151
00:10:04,414 --> 00:10:09,340
плавний результат, щоб ми могли знайти локальний мінімум, роблячи невеликі кроки вниз.

152
00:10:09,340 --> 00:10:14,742
Ось чому, до речі, штучні нейрони мають безперервний діапазон активацій,

153
00:10:14,742 --> 00:10:20,440
а не просто активні чи неактивні у бінарному порядку, як біологічні нейрони.

154
00:10:20,440 --> 00:10:23,551
Цей процес багаторазового підштовхування вхідних даних функції

155
00:10:23,551 --> 00:10:26,960
деяким кратним від’ємним градієнтом називається градієнтним спуском.

156
00:10:26,960 --> 00:10:31,006
Це спосіб сходитися до деякого локального мінімуму функції вартості,

157
00:10:31,006 --> 00:10:33,000
по суті, долини на цьому графіку.

158
00:10:33,000 --> 00:10:36,773
Звичайно, я все ще показую зображення функції з двома входами,

159
00:10:36,773 --> 00:10:41,565
тому що підштовхування у 13 000-вимірному просторі введення трохи важко уявити,

160
00:10:41,565 --> 00:10:45,220
але насправді є гарний непросторовий спосіб подумати про це.

161
00:10:45,220 --> 00:10:49,100
Кожен компонент негативного градієнта говорить нам про дві речі.

162
00:10:49,100 --> 00:10:52,713
Знак, звичайно, говорить нам про те, чи потрібно підштовхнути

163
00:10:52,713 --> 00:10:55,860
відповідний компонент вхідного вектора вгору чи вниз.

164
00:10:55,860 --> 00:11:03,570
Але важливо те, що відносні величини всіх цих компонентів ніби підказують вам,

165
00:11:03,570 --> 00:11:05,620
які зміни важливіші.

166
00:11:05,620 --> 00:11:09,978
Розумієте, у нашій мережі коригування однієї з ваг може мати

167
00:11:09,978 --> 00:11:14,980
набагато більший вплив на функцію витрат, ніж коригування іншої ваги.

168
00:11:14,980 --> 00:11:19,440
Деякі з цих зв’язків просто важливіші для наших навчальних даних.

169
00:11:19,440 --> 00:11:24,760
Таким чином, ви можете подумати про цей вектор градієнта нашої величезної функції витрат,

170
00:11:24,760 --> 00:11:30,021
що спотворює розум, так це те, що він кодує відносну важливість кожної ваги та зміщення,

171
00:11:30,021 --> 00:11:34,100
тобто те, яка з цих змін принесе найбільшу віддачу від ваших грошей.

172
00:11:34,100 --> 00:11:37,360
Це просто інший спосіб думати про напрямок.

173
00:11:37,360 --> 00:11:43,052
Щоб взяти простіший приклад, якщо у вас є деяка функція з двома змінними як вхідні дані,

174
00:11:43,052 --> 00:11:48,489
і ви обчислюєте, що її градієнт у певній точці виходить як 3,1, тоді, з одного боку,

175
00:11:48,489 --> 00:11:52,902
ви можете інтерпретувати це як те, що коли ви стоячи на цьому вході,

176
00:11:52,902 --> 00:11:56,740
рух уздовж цього напрямку збільшує функцію найшвидше, тому,

177
00:11:56,740 --> 00:12:00,513
коли ви будуєте графік функції над площиною вхідних точок,

178
00:12:00,513 --> 00:12:03,200
цей вектор дає вам прямий напрямок угору.

179
00:12:03,200 --> 00:12:07,951
Але інший спосіб прочитати це – сказати, що зміни цієї першої змінної мають утричі

180
00:12:07,951 --> 00:12:12,645
більшу важливість, ніж зміни другої змінної, що принаймні в околицях відповідного

181
00:12:12,645 --> 00:12:17,740
вхідного значення, підштовхування значення x несе набагато більше удару для вашого бакс.

182
00:12:17,740 --> 00:12:22,880
Гаразд, давайте зменшимо масштаб і підведемо підсумок, де ми зараз.

183
00:12:22,880 --> 00:12:27,330
Сама мережа є цією функцією з 784 входами та 10 виходами,

184
00:12:27,330 --> 00:12:30,860
визначеними в термінах усіх цих зважених сум.

185
00:12:30,860 --> 00:12:34,160
Функція витрат є ще одним шаром складності.

186
00:12:34,160 --> 00:12:38,107
Він приймає 13 000 ваг і упереджень як вхідні дані та

187
00:12:38,107 --> 00:12:42,640
викидає єдину міру паршивості на основі навчальних прикладів.

188
00:12:42,640 --> 00:12:47,520
Градієнт функції витрат — це ще один рівень складності.

189
00:12:47,520 --> 00:12:52,374
Він говорить нам, які підштовхи до всіх цих ваг і упереджень

190
00:12:52,374 --> 00:12:56,752
спричиняють найшвидшу зміну значення функції вартості,

191
00:12:56,752 --> 00:13:03,040
що можна інтерпретувати як те, які зміни до яких ваг мають найбільше значення.

192
00:13:03,040 --> 00:13:06,671
Отже, коли ви ініціалізуєте мережу випадковими вагами та зміщеннями та

193
00:13:06,671 --> 00:13:10,455
багато разів налаштовуєте їх на основі цього процесу градієнтного спуску,

194
00:13:10,455 --> 00:13:14,240
наскільки добре вона справді працює на зображеннях, яких раніше не бачив?

195
00:13:14,240 --> 00:13:18,687
Той, який я описав тут, із двома прихованими шарами по 16 нейронів кожен,

196
00:13:18,687 --> 00:13:22,052
обраними здебільшого з естетичних міркувань, непоганий,

197
00:13:22,052 --> 00:13:26,920
оскільки він класифікує приблизно 96% нових зображень, які він бачить правильно.

198
00:13:26,920 --> 00:13:31,367
І, чесно кажучи, якщо ви подивіться на деякі приклади,

199
00:13:31,367 --> 00:13:36,300
з якими він зіпсувався, ви відчуєте потребу трохи послабити.

200
00:13:36,300 --> 00:13:39,865
Якщо ви пограєте зі структурою прихованого шару та зробите кілька налаштувань,

201
00:13:39,865 --> 00:13:41,220
ви можете отримати це до 98%.

202
00:13:41,220 --> 00:13:42,900
І це дуже добре!

203
00:13:42,900 --> 00:13:46,308
Це не найкраще, ви, звичайно, можете отримати кращу продуктивність,

204
00:13:46,308 --> 00:13:49,918
ставши більш складною, ніж ця звичайна ванільна мережа, але враховуючи,

205
00:13:49,918 --> 00:13:53,928
наскільки складним є початкове завдання, я вважаю, що є щось неймовірне в тому,

206
00:13:53,928 --> 00:13:56,886
щоб будь-яка мережа так добре справлялася із зображеннями,

207
00:13:56,886 --> 00:14:00,997
яких вона ніколи раніше не бачила, враховуючи, що ми ніколи конкретно не говорив,

208
00:14:00,997 --> 00:14:02,000
які шаблони шукати.

209
00:14:02,000 --> 00:14:06,634
Спочатку я мотивував цю структуру, описуючи надію, яку ми могли б мати,

210
00:14:06,634 --> 00:14:11,847
що другий шар може підхопити маленькі краї, що третій шар з’єднає ці краї разом,

211
00:14:11,847 --> 00:14:16,675
щоб розпізнати петлі та довші лінії, і що вони можуть бути складені разом,

212
00:14:16,675 --> 00:14:18,220
щоб розпізнавати цифри.

213
00:14:18,220 --> 00:14:21,040
Отже, це те, що насправді робить наша мережа?

214
00:14:21,040 --> 00:14:24,880
Ну, принаймні для цього, зовсім ні.

215
00:14:24,880 --> 00:14:27,901
Пам’ятаєте, як у минулому відео ми дивилися на те,

216
00:14:27,901 --> 00:14:31,811
як ваги зв’язків від усіх нейронів першого шару до даного нейрона

217
00:14:31,811 --> 00:14:35,425
другого шару можна візуалізувати як даний піксельний шаблон,

218
00:14:35,425 --> 00:14:37,440
який нейрон другого шару вловлює?

219
00:14:37,440 --> 00:14:43,492
Що ж, коли ми робимо це для ваг, пов’язаних із цими переходами, замість того,

220
00:14:43,492 --> 00:14:48,458
щоб шукати окремі маленькі краї тут і там, вони виглядають, ну,

221
00:14:48,458 --> 00:14:54,200
майже випадковими, просто з деякими дуже вільними візерунками посередині.

222
00:14:54,200 --> 00:14:58,903
Здавалося б, що в незбагненно великому 13 000-вимірному просторі можливих ваг і

223
00:14:58,903 --> 00:15:03,313
упереджень наша мережа знайшла щасливий маленький локальний мінімум, який,

224
00:15:03,313 --> 00:15:08,134
незважаючи на успішну класифікацію більшості зображень, не точно вловлює шаблони,

225
00:15:08,134 --> 00:15:09,840
на які ми могли сподіватися.

226
00:15:09,840 --> 00:15:12,655
Щоб переконатися в цьому, подивіться, що відбувається,

227
00:15:12,655 --> 00:15:14,600
коли ви вводите випадкове зображення.

228
00:15:14,600 --> 00:15:17,395
Якби система була розумною, ви могли б очікувати,

229
00:15:17,395 --> 00:15:20,246
що вона буде відчувати себе невизначеною, можливо,

230
00:15:20,246 --> 00:15:25,278
насправді не активуючи жоден із цих 10 вихідних нейронів або активуючи їх усі рівномірно,

231
00:15:25,278 --> 00:15:28,801
але натомість вона впевнено дає вам якусь безглузду відповідь,

232
00:15:28,801 --> 00:15:32,435
ніби вона відчуває себе впевненою, що цей випадковий Шум – це 5,

233
00:15:32,435 --> 00:15:34,560
тому що фактичне зображення 5 – це 5.

234
00:15:34,560 --> 00:15:39,483
Іншими словами, навіть якщо ця мережа досить добре розпізнає цифри,

235
00:15:39,483 --> 00:15:41,800
вона не знає, як їх намалювати.

236
00:15:41,800 --> 00:15:45,400
Багато в чому це тому, що це жорстко обмежена система навчання.

237
00:15:45,400 --> 00:15:48,220
Я маю на увазі, поставте себе на місце мережі.

238
00:15:48,220 --> 00:15:53,140
З його точки зору, весь Всесвіт складається лише з чітко визначених нерухомих цифр,

239
00:15:53,140 --> 00:15:57,825
зосереджених у крихітній сітці, і його функція вартості ніколи не давала жодних

240
00:15:57,825 --> 00:16:02,160
стимулів бути чимось іншим, крім як абсолютно впевненим у своїх рішеннях.

241
00:16:02,160 --> 00:16:05,603
Отже, маючи це як образ того, що насправді роблять нейрони другого шару,

242
00:16:05,603 --> 00:16:09,753
ви можете задатися питанням, чому я представив цю мережу з мотивацією підхоплення країв

243
00:16:09,753 --> 00:16:10,320
і шаблонів.

244
00:16:10,320 --> 00:16:13,040
Я маю на увазі, що це зовсім не те, що це закінчується.

245
00:16:13,040 --> 00:16:17,480
Що ж, це не наша кінцева мета, а натомість відправна точка.

246
00:16:17,480 --> 00:16:22,446
Відверто кажучи, це стара технологія, яку досліджували у 80-х і 90-х роках,

247
00:16:22,446 --> 00:16:27,805
і вам потрібно її зрозуміти, перш ніж ви зможете зрозуміти більш детальні сучасні

248
00:16:27,805 --> 00:16:31,792
варіанти, і вона явно здатна вирішити деякі цікаві проблеми,

249
00:16:31,792 --> 00:16:36,694
але чим більше ви заглиблюєтесь у те, що ті приховані шари дійсно роблять,

250
00:16:36,694 --> 00:16:38,720
тим менш розумним це здається.

251
00:16:38,720 --> 00:16:42,593
Якщо на мить перенести фокус із того, як мережі навчаються, на те,

252
00:16:42,593 --> 00:16:47,160
як навчаєтеся ви, це станеться лише за умови активного вивчення матеріалу тут.

253
00:16:47,160 --> 00:16:49,883
Я хочу, щоб ви зробили одну досить просту річ:

254
00:16:49,883 --> 00:16:53,360
просто зупиніться зараз і на мить глибоко подумайте про те,

255
00:16:53,360 --> 00:16:58,460
які зміни ви можете внести в цю систему та як вона сприймає зображення, якщо ви хочете,

256
00:16:58,460 --> 00:17:01,880
щоб вона краще вловлювала такі речі, як краї та візерунки.

257
00:17:01,880 --> 00:17:05,027
Але краще за все, щоб справді ознайомитися з матеріалом,

258
00:17:05,027 --> 00:17:09,720
я настійно рекомендую книгу Майкла Нільсена про глибоке навчання та нейронні мережі.

259
00:17:09,720 --> 00:17:14,349
У ньому ви можете знайти код і дані для завантаження та використання для

260
00:17:14,349 --> 00:17:19,360
цього точного прикладу, і книга проведе вас крок за кроком, що цей код робить.

261
00:17:19,360 --> 00:17:22,586
Що чудово, так це те, що ця книга є безкоштовною та загальнодоступною,

262
00:17:22,586 --> 00:17:25,131
тому, якщо ви щось отримаєте від неї, подумайте про те,

263
00:17:25,131 --> 00:17:28,040
щоб приєднатися до мене та зробити пожертву на користь Nielsen.

264
00:17:28,040 --> 00:17:33,177
Я також пов’язав кілька інших ресурсів, які мені дуже подобаються, в описі,

265
00:17:33,177 --> 00:17:38,720
включно з феноменальним і красивим дописом у блозі Кріса Оли та статті в Distill.

266
00:17:38,720 --> 00:17:43,180
Щоб завершити це на останні кілька хвилин, я хочу повернутися до фрагменту інтерв’ю,

267
00:17:43,180 --> 00:17:44,440
яке я мав із Лейшею Лі.

268
00:17:44,440 --> 00:17:46,500
Можливо, ви пам’ятаєте її з останнього відео, вона

269
00:17:46,500 --> 00:17:48,520
захистила докторську роботу з глибокого навчання.

270
00:17:48,520 --> 00:17:51,772
У цьому невеличкому фрагменті вона розповідає про дві нещодавні статті,

271
00:17:51,772 --> 00:17:55,837
які дійсно досліджують, як деякі з більш сучасних мереж розпізнавання зображень насправді

272
00:17:55,837 --> 00:17:56,380
навчаються.

273
00:17:56,380 --> 00:17:58,830
Просто щоб визначити, де ми знаходимося в розмові,

274
00:17:58,830 --> 00:18:01,953
перша стаття взяла одну з цих особливо глибоких нейронних мереж,

275
00:18:01,953 --> 00:18:04,643
яка справді добре розпізнає зображення, і замість того,

276
00:18:04,643 --> 00:18:07,238
щоб навчати її на правильно позначеному наборі даних,

277
00:18:07,238 --> 00:18:09,400
вона перетасувала всі мітки перед навчанням.

278
00:18:09,400 --> 00:18:12,411
Очевидно, що точність тестування тут мала бути не кращою,

279
00:18:12,411 --> 00:18:15,320
ніж випадкова, оскільки все просто випадково позначено.

280
00:18:15,320 --> 00:18:18,773
Але він все одно зміг досягти такої ж точності навчання,

281
00:18:18,773 --> 00:18:21,440
як і на правильно позначеному наборі даних.

282
00:18:21,440 --> 00:18:25,348
По суті, мільйонів ваг для цієї конкретної мережі було достатньо,

283
00:18:25,348 --> 00:18:29,435
щоб вона просто запам’ятовувала випадкові дані, що піднімає питання,

284
00:18:29,435 --> 00:18:34,173
чи насправді мінімізація цієї функції вартості відповідає будь-якій структурі в

285
00:18:34,173 --> 00:18:36,720
зображенні, чи це просто запам’ятовування?

286
00:18:36,720 --> 00:18:40,120
. . . щоб запам’ятати весь набір даних, що таке правильна класифікація.

287
00:18:40,120 --> 00:18:43,521
І ось кілька, ви знаєте, півроку пізніше на ICML цього року,

288
00:18:43,521 --> 00:18:48,093
була не зовсім спростувальна стаття, а стаття, в якій розглядалися деякі аспекти,

289
00:18:48,093 --> 00:18:52,220
наприклад, ага, насправді ці мережі роблять щось трохи розумніше, ніж це.

290
00:18:52,220 --> 00:18:58,803
Якщо ви подивитеся на цю криву точності, якби ви просто тренувалися на випадковому наборі

291
00:18:58,803 --> 00:19:05,240
даних, ця крива начебто опускалася дуже, знаєте, дуже повільно майже лінійним способом.

292
00:19:05,240 --> 00:19:08,875
Тож вам справді важко знайти локальні мінімуми можливих,

293
00:19:08,875 --> 00:19:12,320
знаєте, правильних ваг, які дадуть вам таку точність.

294
00:19:12,320 --> 00:19:16,083
У той час як якщо ви насправді тренуєтеся на структурованому наборі даних,

295
00:19:16,083 --> 00:19:19,997
який має правильні мітки, ви знаєте, ви знаєте, ви трохи возитеся на початку,

296
00:19:19,997 --> 00:19:23,360
але потім ви дуже швидко впали, щоб досягти такого рівня точності.

297
00:19:23,360 --> 00:19:28,580
І тому в якомусь сенсі було легше знайти ці локальні максимуми.

298
00:19:28,580 --> 00:19:35,323
І що також було цікаво в цьому, це висвітлює іншу статтю, фактично пару років тому,

299
00:19:35,323 --> 00:19:40,140
яка містить набагато більше спрощень щодо мережевих рівнів.

300
00:19:40,140 --> 00:19:44,552
Але один із результатів показав, що, якщо ви подивитеся на ландшафт оптимізації,

301
00:19:44,552 --> 00:19:49,400
локальні мінімуми, які ці мережі, як правило, вивчають, насправді мають однакову якість.

302
00:19:49,400 --> 00:19:52,391
Тож у певному сенсі, якщо ваш набір даних структурований,

303
00:19:52,391 --> 00:19:54,300
ви зможете знайти це набагато легше.

304
00:19:54,300 --> 00:20:01,140
Я, як завжди, дякую тим із вас, хто підтримує на Patreon.

305
00:20:01,140 --> 00:20:04,323
Раніше я вже говорив про те, що змінює гру на Patreon,

306
00:20:04,323 --> 00:20:07,160
але ці відео справді були б неможливими без вас.

307
00:20:07,160 --> 00:20:10,347
Я також хочу висловити особливу подяку фірмі венчурного капіталу

308
00:20:10,347 --> 00:20:13,240
Amplify Partners та її підтримці цих перших відео в серії.

309
00:20:13,240 --> 00:20:33,140
Дякую тобі.

