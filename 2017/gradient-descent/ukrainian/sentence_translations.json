[
 {
  "translatedText": "У минулому відео я описав структуру нейронної мережі.",
  "input": "Last video I laid out the structure of a neural network.",
  "time_range": [
   4.180000000000002,
   7.28
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Я дам тут короткий підсумок, щоб це було свіжим у нашій пам’яті, а потім у мене є дві головні цілі для цього відео.",
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "time_range": [
   7.68,
   12.6
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Перший полягає в тому, щоб представити ідею градієнтного спуску, яка лежить в основі не тільки того, як навчаються нейронні мережі, але й того, як працює багато інших машинного навчання.",
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "time_range": [
   13.1,
   20.6
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Після цього ми детальніше розглянемо, як працює ця конкретна мережа, і що шукають ці приховані шари нейронів.",
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "time_range": [
   21.12,
   27.94
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Нагадуємо, що нашою метою тут є класичний приклад розпізнавання рукописних цифр, привіт, світ нейронних мереж.",
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "time_range": [
   28.979999999999997,
   36.22
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ці цифри відображаються на сітці 28x28 пікселів, кожен піксель має значення відтінків сірого від 0 до 1.",
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "time_range": [
   37.02,
   43.42
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Саме вони визначають активацію 784 нейронів на вхідному рівні мережі.",
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "time_range": [
   43.82,
   50.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "А потім активація для кожного нейрона в наступних шарах базується на зваженій сумі всіх активацій на попередньому шарі плюс якесь спеціальне число, яке називається зміщенням.",
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "time_range": [
   51.18,
   60.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Потім ви складаєте цю суму за допомогою якоїсь іншої функції, як-от сигмоподібна сквишифікація або relu, як я пройшов у минулому відео.",
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "time_range": [
   62.16,
   68.94
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Загалом, враховуючи дещо довільний вибір двох прихованих шарів із 16 нейронами кожен, мережа має близько 13 000 ваг і зміщень, які ми можемо налаштувати, і саме ці значення визначають, що саме мережа насправді робить.",
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "time_range": [
   69.48,
   84.38
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Коли ми кажемо, що ця мережа класифікує дану цифру, ми маємо на увазі те, що найяскравіший із 10 нейронів останнього шару відповідає цій цифрі.",
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "time_range": [
   84.88,
   93.3
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "І пам’ятайте, мотивація, яку ми тут мали на увазі для багатошарової структури, полягала в тому, що, можливо, другий шар міг би підхопити краї, а третій шар міг би підхопити візерунки, як-от петлі та лінії, і останній міг просто з’єднати їх. шаблони для розпізнавання цифр.",
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "time_range": [
   94.1,
   108.8
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Отже, ми дізнаємося, як навчається мережа.",
  "input": "So here, we learn how the network learns.",
  "time_range": [
   109.8,
   112.24
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Те, що ми хочемо, — це алгоритм, за допомогою якого ви можете показати цій мережі цілий набір навчальних даних, які надходять у формі набору різних зображень рукописних цифр разом із мітками, якими вони мають бути, і це буде відкоригувати ці 13 000 ваг і зміщень, щоб покращити ефективність тренувальних даних.",
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "time_range": [
   112.64,
   130.12
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Сподіваємось, ця багаторівнева структура означатиме, що те, що вона вивчає, узагальнює зображення за межами даних навчання.",
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "time_range": [
   130.72,
   136.86
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Спосіб тестування полягає в тому, що після того, як ви навчите мережу, ви показуєте їй більше позначених даних, яких вона ніколи не бачила раніше, і ви бачите, наскільки точно вона класифікує ці нові зображення.",
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "time_range": [
   137.64000000000001,
   146.7
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "На наше щастя, і те, що робить цей приклад таким поширеним для початку, полягає в тому, що добрі люди, що стоять за базою даних MNIST, зібрали колекцію з десятків тисяч рукописних зображень цифр, кожне з яких позначено номерами, які вони повинні мати бути.",
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "time_range": [
   151.12,
   164.2
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "І як би не було провокаційно описувати машину як навчальну, коли ви бачите, як вона працює, це стає набагато менше схожим на якусь божевільну науково-фантастичну передумову, а набагато більше схоже на вправу з обчислення.",
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "time_range": [
   164.9,
   175.48
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Я маю на увазі, що в основному це зводиться до пошуку мінімуму певної функції.",
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "time_range": [
   176.2,
   179.96
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Пам’ятайте, концептуально ми вважаємо, що кожен нейрон пов’язаний з усіма нейронами попереднього шару, і ваги у зваженій сумі, що визначає його активацію, схожі на силу цих зв’язків, а зміщення є певним показником чи цей нейрон має тенденцію бути активним чи неактивним.",
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "time_range": [
   181.93999999999997,
   198.96
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "І для початку ми просто ініціалізуємо всі ці ваги та зміщення абсолютно випадковим чином.",
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "time_range": [
   199.72,
   204.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Зайве говорити, що ця мережа працюватиме досить жахливо на даному навчальному прикладі, оскільки вона просто робить щось випадкове.",
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "time_range": [
   204.94,
   210.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Наприклад, ви подаєте на це зображення 3, а вихідний шар виглядає просто безладом.",
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "time_range": [
   211.04,
   216.02
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Отже, що ви робите, це визначаєте функцію вартості, спосіб повідомити комп’ютеру, ні, поганий комп’ютер, цей вихід повинен мати активації, які дорівнюють 0 для більшості нейронів, але 1 для цього нейрона, те, що ви мені дали, є повним сміттям.",
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "time_range": [
   216.6,
   230.76
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Якщо говорити трохи математичніше, ви складаєте квадрати різниць між кожною з цих активацій виведення сміття та значенням, яке ви хочете, щоб вони мали, і це те, що ми називатимемо вартістю одного навчального прикладу.",
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "time_range": [
   231.72,
   245.02
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Зауважте, що ця сума невелика, коли мережа впевнено правильно класифікує зображення, але велика, коли здається, що мережа не знає, що вона робить.",
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "time_range": [
   245.96,
   256.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Отже, що ви робите, це розглядаєте середню вартість усіх десятків тисяч навчальних прикладів, які є у вашому розпорядженні.",
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "time_range": [
   258.64,
   265.44
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ця середня вартість є нашим показником того, наскільки поганою є мережа та наскільки погано має працювати комп’ютер.",
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "time_range": [
   267.04,
   272.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "І це складна річ.",
  "input": "And that's a complicated thing.",
  "time_range": [
   273.42,
   274.6
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Пам’ятаєте, як сама мережа була в основному функцією, яка приймає 784 числа як вхідні дані, значення пікселів, і викидає 10 чисел як свій вихід, і в певному сенсі вона параметризована всіма цими вагами та зміщеннями?",
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "time_range": [
   275.04,
   288.8
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Крім того, функція витрат є ще одним шаром складності.",
  "input": "Well the cost function is a layer of complexity on top of that.",
  "time_range": [
   289.5,
   292.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Він бере на вхід приблизно 13 000 ваг і упереджень і видає одне число, яке описує, наскільки погані ці ваги та упередження, і спосіб його визначення залежить від поведінки мережі над усіма десятками тисяч фрагментів навчальних даних.",
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "time_range": [
   293.1,
   308.9
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Це багато про що думати.",
  "input": "That's a lot to think about.",
  "time_range": [
   309.52,
   311.0
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Але просто говорити комп’ютеру, яку погану роботу він робить, не дуже корисно.",
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "time_range": [
   312.4,
   315.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ви хочете сказати йому, як змінити ці ваги та упередження, щоб воно стало кращим.",
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "time_range": [
   316.22,
   320.06
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Щоб зробити це легше, замість того, щоб намагатися уявити функцію з 13 000 входами, просто уявіть просту функцію, яка має одне число як вхід і одне число як вихід.",
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "time_range": [
   320.78,
   330.48
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Як знайти вхідні дані, які мінімізують значення цієї функції?",
  "input": "How do you find an input that minimizes the value of this function?",
  "time_range": [
   331.48,
   335.3
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Студенти, які вивчають обчислення, знатимуть, що іноді можна чітко визначити цей мінімум, але це не завжди можливо для справді складних функцій, а особливо не у версії цієї ситуації з 13 000 вхідних даних для нашої божевільно складної функції вартості нейронної мережі.",
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "time_range": [
   336.46,
   351.08
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Більш гнучка тактика полягає в тому, щоб почати з будь-якого входу та визначити, у якому напрямку слід рухатися, щоб знизити цей вихід.",
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "time_range": [
   351.58,
   359.2
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Зокрема, якщо ви можете визначити нахил функції, де ви знаходитесь, тоді перемістіть ліворуч, якщо цей нахил додатний, і перемістіть вхідні дані праворуч, якщо цей нахил від’ємний.",
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "time_range": [
   360.08,
   369.9
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Якщо ви робите це неодноразово, у кожній точці перевіряючи новий нахил і роблячи відповідний крок, ви наблизитесь до деякого локального мінімуму функції.",
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "time_range": [
   371.96,
   379.84
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Зображення, яке ви можете мати на увазі, це м’яч, що котиться з пагорба.",
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "time_range": [
   380.64,
   383.8
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Зауважте, що навіть для цієї дійсно спрощеної функції єдиного введення існує багато можливих долин, у які ви можете потрапити, залежно від того, з якого випадкового введення ви почнете, і немає гарантії, що локальний мінімум, у який ви потрапите, буде найменшим можливим значенням функції витрат.",
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "time_range": [
   384.62,
   399.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Це також перенесеться на нашу нейронну мережу.",
  "input": "That will carry over to our neural network case as well.",
  "time_range": [
   400.22,
   402.62
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Я також хочу, щоб ви помітили, що якщо ви робите розмір кроку пропорційним схилу, тоді, коли схил вирівнюється до мінімуму, ваші кроки стають все меншими, і це допомагає вам уникнути перевищення.",
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "time_range": [
   403.18,
   414.6
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Трохи збільшуючи складність, уявіть натомість функцію з двома входами та одним виходом.",
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "time_range": [
   415.94,
   420.98
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ви можете подумати про вхідний простір як про площину xy, а функцію вартості як поверхню над нею.",
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "time_range": [
   421.5,
   428.14
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Замість того, щоб запитувати про нахил функції, ви повинні запитати, у якому напрямку вам слід зробити крок у цьому вхідному просторі, щоб найшвидше зменшити вихід функції.",
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "time_range": [
   428.76,
   438.96
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Іншими словами, який напрямок спуску?",
  "input": "In other words, what's the downhill direction?",
  "time_range": [
   439.72,
   441.76
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Знову ж таки, корисно уявити м’яч, що котиться з пагорба.",
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "time_range": [
   442.38,
   445.56
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ті з вас, хто знайомий із численням багатьох змінних, знають, що градієнт функції дає вам напрямок найкрутішого підйому, у якому напрямку слід зробити крок, щоб збільшити функцію найшвидше.",
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "time_range": [
   446.66,
   458.78
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Цілком природно, що негативне значення цього градієнта дає вам напрямок кроку, який найшвидше зменшує функцію.",
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "time_range": [
   459.56,
   466.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Навіть більше того, довжина цього вектора градієнта є показником того, наскільки крутим є найкрутіший схил.",
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "time_range": [
   467.24,
   473.84
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Якщо ви не знайомі з численням багатьох змінних і хочете дізнатися більше, ознайомтеся з деякими роботами, які я виконав для Академії Хана на цю тему.",
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "time_range": [
   474.54,
   480.34
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Але, чесно кажучи, для нас з вами зараз важливо лише те, що в принципі існує спосіб обчислити цей вектор, цей вектор, який повідомляє вам, яким є напрямок спуску та наскільки він крутий.",
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "time_range": [
   480.86,
   491.9
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "З тобою все буде добре, якщо це все, що ти знаєш, і ти не розбираєшся в деталях.",
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "time_range": [
   492.4,
   496.12
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Якщо ви можете це отримати, алгоритм мінімізації функції полягає в тому, щоб обчислити цей напрямок градієнта, потім зробити невеликий крок вниз і повторити це знову і знову.",
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "time_range": [
   497.2,
   506.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Це та сама основна ідея для функції, яка має 13 000 входів замість 2 входів.",
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "time_range": [
   507.7,
   512.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Уявіть собі організацію всіх 13 000 ваг і зміщень нашої мережі у гігантський вектор-стовпець.",
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "time_range": [
   513.4,
   519.46
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Від’ємний градієнт функції витрат — це просто вектор, це певний напрямок у цьому шалено величезному просторі введення, який говорить вам, які підштовхи до всіх цих чисел призведуть до найшвидшого зменшення функції витрат.",
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "time_range": [
   520.14,
   534.88
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "І, звісно, завдяки нашій спеціально розробленій функції вартості зміна вагових коефіцієнтів і зміщень для їх зменшення означає, що вихідні дані мережі для кожної частини навчальних даних виглядатимуть не так як випадковий масив із 10 значень, а більше як фактичне рішення, яке ми хочемо це зробити.",
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "time_range": [
   535.64,
   550.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Важливо пам’ятати, що ця функція вартості передбачає середнє значення за всіма навчальними даними, тож якщо ви мінімізуєте її, це означає, що для всіх цих зразків буде краща продуктивність.",
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "time_range": [
   551.44,
   561.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Алгоритм для ефективного обчислення цього градієнта, який фактично є основою того, як нейронна мережа навчається, називається зворотним поширенням, і саме про нього я буду говорити в наступному відео.",
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "time_range": [
   563.82,
   573.98
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Там я справді хочу витратити час, щоб пройти через те, що саме відбувається з кожною вагою та зміщенням для певної частини тренувальних даних, намагаючись дати інтуїтивне відчуття того, що відбувається за межами купи відповідних обчислень і формул.",
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "time_range": [
   574.66,
   587.1
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Прямо тут, прямо зараз, головне, що я хочу, щоб ви знали, незалежно від деталей реалізації, це те, що ми маємо на увазі, коли говоримо про мережеве навчання, це те, що це просто мінімізація функції витрат.",
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "time_range": [
   587.78,
   598.36
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "І зауважте, одним із наслідків цього є те, що для цієї функції витрат важливо мати гарний плавний результат, щоб ми могли знайти локальний мінімум, роблячи невеликі кроки вниз.",
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "time_range": [
   599.3,
   608.1
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ось чому, до речі, штучні нейрони мають безперервний діапазон активацій, а не просто активні чи неактивні бінарним способом, як біологічні нейрони.",
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "time_range": [
   609.26,
   619.14
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Цей процес багаторазового підштовхування вхідних даних функції деяким кратним від’ємним градієнтом називається градієнтним спуском.",
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "time_range": [
   620.22,
   626.76
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Це спосіб сходитися до деякого локального мінімуму функції вартості, по суті, долини на цьому графіку.",
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "time_range": [
   627.3,
   632.58
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Звичайно, я все ще показую зображення функції з двома входами, тому що підштовхування в 13 000-вимірному просторі введення трохи важко уявити, але є гарний непросторовий спосіб подумати про це.",
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "time_range": [
   633.44,
   644.26
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Кожен компонент негативного градієнта говорить нам про дві речі.",
  "input": "Each component of the negative gradient tells us two things.",
  "time_range": [
   645.08,
   648.44
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Знак, звичайно, говорить нам про те, чи потрібно підштовхнути відповідний компонент вхідного вектора вгору чи вниз.",
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "time_range": [
   649.06,
   655.14
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Але важливо те, що відносні величини всіх цих компонентів ніби підказують вам, які зміни важливіші.",
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "time_range": [
   655.8,
   662.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Розумієте, у нашій мережі коригування однієї з ваг може мати набагато більший вплив на функцію витрат, ніж коригування іншої ваги.",
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "time_range": [
   665.22,
   673.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Деякі з цих зв’язків просто важливіші для наших навчальних даних.",
  "input": "Some of these connections just matter more for our training data.",
  "time_range": [
   674.8,
   678.2
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Таким чином, ви можете подумати про цей вектор градієнта нашої величезної функції витрат, що спотворює розум, так це те, що він кодує відносну важливість кожної ваги та зміщення, тобто те, яка з цих змін принесе найбільшу віддачу від ваших грошей.",
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "time_range": [
   679.32,
   692.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Це просто інший спосіб думати про напрямок.",
  "input": "This really is just another way of thinking about direction.",
  "time_range": [
   693.62,
   696.64
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Щоб взяти простіший приклад, якщо у вас є якась функція з двома змінними як вхідні дані, і ви обчислюєте, що її градієнт у певній точці виходить рівним 3,1, то, з одного боку, ви можете інтерпретувати це як те, що коли ви перебуваючи на цьому вході, рух у цьому напрямку збільшує функцію найшвидше, тому, коли ви будуєте графік функції над площиною вхідних точок, цей вектор дає вам напрям прямо в гору.",
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "time_range": [
   697.1,
   722.26
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Але інший спосіб прочитати це – сказати, що зміни цієї першої змінної мають утричі більше значення, ніж зміни другої змінної, що принаймні в околицях відповідних вхідних даних, підштовхування значення x несе набагато більше удару для вашого бакс.",
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "time_range": [
   722.86,
   736.9
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Давайте зменшимо масштаб і підведемо підсумок, де ми зараз.",
  "input": "Let's zoom out and sum up where we are so far.",
  "time_range": [
   739.88,
   742.34
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Сама мережа є цією функцією з 784 входами та 10 виходами, визначеними в термінах усіх цих зважених сум.",
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "time_range": [
   742.84,
   750.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Функція витрат є ще одним шаром складності.",
  "input": "The cost function is a layer of complexity on top of that.",
  "time_range": [
   750.64,
   753.68
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Він бере 13 000 ваг і упереджень як вхідні дані та викидає єдину міру паршивості на основі навчальних прикладів.",
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "time_range": [
   753.98,
   761.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "А градієнт функції витрат — це ще один рівень складності.",
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "time_range": [
   762.44,
   766.9
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Він говорить нам, які підштовхи до всіх цих ваг і упереджень спричиняють найшвидшу зміну значення функції вартості, що ви можете інтерпретувати як те, які зміни до яких ваг мають найбільше значення.",
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "time_range": [
   767.36,
   777.88
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Отже, коли ви ініціалізуєте мережу випадковими вагами та зміщеннями та налаштовуєте їх багато разів на основі цього процесу градієнтного спуску, наскільки добре вона насправді працює на зображеннях, яких ніколи раніше не бачив?",
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "time_range": [
   782.56,
   793.2
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Той, який я описав тут, із двома прихованими шарами по 16 нейронів кожен, обраними здебільшого з естетичних міркувань, непоганий, оскільки він класифікує приблизно 96% нових зображень, які він бачить правильно.",
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "time_range": [
   794.1,
   805.96
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "І, чесно кажучи, якщо ви подивіться на деякі приклади, з якими він зіпсувався, ви відчуєте потребу трохи послабити.",
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "time_range": [
   806.68,
   812.54
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Тепер, якщо ви поекспериментуєте зі структурою прихованого шару та зробите кілька налаштувань, ви зможете отримати це до 98%.",
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "time_range": [
   816.22,
   821.76
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "І це дуже добре!",
  "input": "And that's pretty good!",
  "time_range": [
   821.76,
   822.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Це не найкраще, ви, звичайно, можете отримати кращу продуктивність, якщо стати більш складною, ніж ця звичайна ванільна мережа, але враховуючи, наскільки складним є початкове завдання, я вважаю, що є щось неймовірне в тому, щоб будь-яка мережа так добре справлялася із зображеннями, яких вона ніколи раніше не бачила, враховуючи, що ми ніколи конкретно не говорили йому, які шаблони шукати.",
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "time_range": [
   823.02,
   841.42
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Спочатку я мотивував цю структуру, описуючи надію, яку ми могли б мати, що другий шар може підхопити маленькі краї, що третій шар з’єднає ці краї разом, щоб розпізнати петлі та довші лінії, і що вони можуть бути складені разом, щоб розпізнавати цифри.",
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "time_range": [
   842.5600000000001,
   857.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Отже, це те, чим насправді займається наша мережа?",
  "input": "So is this what our network is actually doing?",
  "time_range": [
   857.96,
   860.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ну, принаймні для цього, зовсім ні.",
  "input": "Well, for this one at least, not at all.",
  "time_range": [
   861.0799999999999,
   864.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Пам’ятаєте, як у минулому відео ми дивилися на те, як ваги зв’язків від усіх нейронів першого шару до даного нейрона другого шару можна візуалізувати як даний піксельний шаблон, який нейрон другого шару вловлює?",
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "time_range": [
   864.82,
   877.06
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Ну, коли ми фактично робимо це для ваг, пов’язаних із цими переходами, від першого шару до наступного, замість того, щоб підбирати окремі маленькі краї тут і там, вони виглядають, ну, майже випадковими, просто з деякими дуже вільними візерунками в середина там.",
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "time_range": [
   877.78,
   893.68
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Здавалося б, що в незбагненно великому 13 000-вимірному просторі можливих ваг і упереджень наша мережа знайшла щасливий маленький локальний мінімум, який, незважаючи на успішну класифікацію більшості зображень, не точно вловлює шаблони, на які ми могли сподіватися.",
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "time_range": [
   893.76,
   908.96
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Щоб переконатися в цьому, подивіться, що відбувається, коли ви вводите випадкове зображення.",
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "time_range": [
   909.78,
   913.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Якби система була розумною, ви могли б очікувати, що вона буде відчувати себе невпевнено, можливо, насправді не активуючи жоден із цих 10 вихідних нейронів або активуючи їх усі рівномірно, але натомість вона впевнено дає вам якусь безглузду відповідь, ніби вона відчуває себе впевненою, що цей випадковий шум це 5, тому що фактичне зображення 5 є 5.",
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "time_range": [
   914.32,
   934.16
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Іншими словами, навіть якщо ця мережа досить добре розпізнає цифри, вона не знає, як їх намалювати.",
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "time_range": [
   934.54,
   940.7
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Багато в чому це тому, що це жорстко обмежена система навчання.",
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "time_range": [
   941.42,
   945.24
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Я маю на увазі, поставте себе на місце мережі.",
  "input": "I mean, put yourself in the network's shoes here.",
  "time_range": [
   945.88,
   947.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "З його точки зору, весь Всесвіт складається лише з чітко визначених нерухомих цифр, зосереджених у крихітній сітці, і його функція вартості ніколи не давала жодних стимулів бути чимось іншим, крім як абсолютно впевненим у своїх рішеннях.",
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "time_range": [
   948.14,
   961.08
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Отже, маючи це як образ того, що насправді роблять нейрони другого шару, ви можете задатися питанням, чому я представив цю мережу з мотивацією підхоплення країв і шаблонів.",
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "time_range": [
   962.12,
   969.92
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Я маю на увазі, що це зовсім не те, що це закінчується.",
  "input": "I mean, that's just not at all what it ends up doing.",
  "time_range": [
   969.92,
   972.3
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Що ж, це не наша кінцева мета, а натомість відправна точка.",
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "time_range": [
   973.38,
   977.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Відверто кажучи, це стара технологія, яку досліджували у 80-х і 90-х роках, і вам потрібно її зрозуміти, перш ніж ви зможете зрозуміти більш детальні сучасні варіанти, і вона явно здатна вирішити деякі цікаві проблеми, але чим більше ви заглиблюєтесь у те, що ті приховані шари дійсно роблять, тим менш розумним це здається.",
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "time_range": [
   977.64,
   994.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Якщо на мить перенести фокус із того, як мережі навчаються, на те, як навчаєтеся ви, це станеться лише за умови активного вивчення матеріалу тут.",
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "time_range": [
   998.48,
   1006.3
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Я хочу, щоб ви зробили одну досить просту річ: просто зупиніться зараз і на мить глибоко подумайте про те, які зміни ви можете внести в цю систему та як вона сприймає зображення, якщо ви хочете, щоб вона краще вловлювала такі речі, як краї та візерунки.",
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "time_range": [
   1007.06,
   1020.88
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Але краще за все, щоб справді ознайомитися з матеріалом, я настійно рекомендую книгу Майкла Нільсена про глибоке навчання та нейронні мережі.",
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "time_range": [
   1021.4799999999999,
   1029.1
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "У ньому ви можете знайти код і дані для завантаження та використання для цього точного прикладу, і книга проведе вас крок за кроком, що цей код робить.",
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "time_range": [
   1029.68,
   1038.36
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Що чудово, так це те, що ця книга є безкоштовною та загальнодоступною, тому, якщо ви щось отримаєте від неї, подумайте про те, щоб приєднатися до мене та зробити пожертву на користь Nielsen.",
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "time_range": [
   1039.3,
   1047.66
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Я також пов’язав кілька інших ресурсів, які мені дуже подобаються, в описі, включно з феноменальним і красивим дописом у блозі Кріса Оли та статті в Distill.",
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "time_range": [
   1047.66,
   1056.5
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Щоб завершити це на останні кілька хвилин, я хочу повернутися до фрагменту інтерв’ю, яке я мав із Лейшею Лі.",
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "time_range": [
   1058.28,
   1063.88
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Можливо, ви пам’ятаєте її з останнього відео, вона захистила докторську роботу з глибокого навчання.",
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "time_range": [
   1064.3,
   1067.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "У цьому невеликому фрагменті вона розповідає про дві нещодавні статті, які дійсно досліджують, як деякі з більш сучасних мереж розпізнавання зображень насправді навчаються.",
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "time_range": [
   1068.3,
   1075.78
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Просто щоб визначити, де ми знаходимося в розмові, перша стаття взяла одну з цих особливо глибоких нейронних мереж, які справді добре розпізнають зображення, і замість того, щоб навчати її на правильно позначеному наборі даних, перетасувала всі мітки перед навчанням.",
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "time_range": [
   1076.1200000000001,
   1088.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Очевидно, що точність тестування тут була не кращою, ніж випадкова, оскільки все просто випадково позначено, але все одно вдалося досягти такої ж точності навчання, як і на правильно позначеному наборі даних.",
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "time_range": [
   1089.48,
   1100.88
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "По суті, мільйонів ваг для цієї конкретної мережі було достатньо, щоб вона просто запам’ятовувала випадкові дані, що піднімає питання, чи насправді мінімізація цієї функції вартості відповідає будь-якій структурі в зображенні, чи це просто запам’ятовування?",
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "time_range": [
   1101.6000000000001,
   1116.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Якщо ви подивіться на цю криву точності, якби ви просто тренувалися на випадковому наборі даних, ця крива начебто опускалася дуже повільно майже лінійним способом, тож вам справді важко знайти той локальний мінімум можливого, ви знаєте правильні ваги, які забезпечать вам таку точність.",
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "time_range": [
   1131.44,
   1152.14
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Тоді як якщо ви насправді тренуєтеся на структурованому наборі даних, який має правильні мітки, ви трохи возитеся на початку, але потім дуже швидко падаєте, щоб досягти такого рівня точності, і тому в певному сенсі це було легше знайти ці локальні максимуми.",
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "time_range": [
   1152.24,
   1168.22
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "І що також було цікаво в цьому, це висвітлює іншу статтю, написану фактично пару років тому, яка містить набагато більше спрощень щодо мережевих рівнів, але один із результатів полягав у тому, що якщо ви подивитеся на ландшафт оптимізації, локальні мінімуми, які вивчають ці мережі, насправді мають однакову якість, тому в певному сенсі, якщо ваш набір даних структурований, ви зможете знайти це набагато легше.",
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "time_range": [
   1168.54,
   1194.32
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Я дякую, як завжди, тим із вас, хто підтримує на Patreon.",
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "time_range": [
   1198.16,
   1201.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Раніше я вже говорив про те, що Patreon змінив правила гри, але ці відео дійсно були б неможливими без вас.",
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "time_range": [
   1201.52,
   1206.8
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "Я також хочу особливо подякувати фірмі венчурного капіталу Amplify Partners за підтримку цих перших відео в серії.",
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "time_range": [
   1207.46,
   1212.78
  ],
  "n_reviews": 0
 }
]