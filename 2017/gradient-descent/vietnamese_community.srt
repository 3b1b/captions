1
00:00:04,070 --> 00:00:07,059
Video cuối cùng tôi đã trình bày cấu trúc của mạng thần kinh

2
00:00:07,160 --> 00:00:10,089
Tôi sẽ cung cấp một bản tóm tắt nhanh chóng ở đây chỉ để nó tươi trong tâm trí của chúng tôi

3
00:00:10,089 --> 00:00:15,368
Và sau đó tôi có hai mục tiêu chính cho video này. Đầu tiên là giới thiệu ý tưởng về độ dốc,

4
00:00:15,650 --> 00:00:18,219
không chỉ là cách mạng lưới thần kinh học hỏi,

5
00:00:18,220 --> 00:00:20,439
nhưng cũng có rất nhiều công cụ học máy khác hoạt động tốt

6
00:00:20,660 --> 00:00:24,609
Sau đó, sau đó chúng ta sẽ tìm hiểu thêm về cách mạng này hoạt động

7
00:00:24,609 --> 00:00:27,758
Và những lớp tế bào thần kinh ẩn giấu đó thực sự đang tìm kiếm

8
00:00:28,999 --> 00:00:33,489
Nhắc nhở, mục tiêu của chúng tôi ở đây là ví dụ điển hình về nhận dạng chữ viết tay

9
00:00:34,129 --> 00:00:36,129
thế giới hello của mạng thần kinh

10
00:00:36,500 --> 00:00:43,090
các chữ số này được hiển thị trên lưới 28 x 28 pixel mỗi pixel với một số giá trị thang độ xám giữa 0 và 1

11
00:00:43,610 --> 00:00:46,089
đó là những gì xác định kích hoạt của

12
00:00:46,850 --> 00:00:50,199
784 tế bào thần kinh trong lớp đầu vào của mạng và

13
00:00:50,840 --> 00:00:55,719
Sau đó kích hoạt cho mỗi nơron trong các lớp sau dựa trên tổng trọng số của

14
00:00:56,000 --> 00:01:00,639
Tất cả các hoạt động trong lớp trước cộng với một số số đặc biệt được gọi là thiên vị

15
00:01:01,699 --> 00:01:06,338
sau đó bạn soạn tổng hợp đó với một số hàm khác như squigification sigmoid hoặc

16
00:01:06,400 --> 00:01:08,769
một cách thích hợp theo cách tôi đã xem qua video cuối cùng

17
00:01:09,110 --> 00:01:15,729
Trong tổng số cho sự lựa chọn phần nào tùy ý của hai lớp ẩn ở đây với 16 tế bào thần kinh mỗi mạng có khoảng

18
00:01:16,579 --> 00:01:24,159
13.000 trọng số và thành kiến ​​mà chúng tôi có thể điều chỉnh và đó là những giá trị này xác định chính xác mạng bạn biết thực sự

19
00:01:24,799 --> 00:01:28,328
Sau đó, điều chúng tôi muốn nói khi chúng tôi nói rằng mạng này phân loại một số đã cho

20
00:01:28,329 --> 00:01:33,429
Có phải đó là 10 tế bào thần kinh trong lớp cuối cùng tương ứng với chữ số đó

21
00:01:33,950 --> 00:01:38,589
Và hãy nhớ động lực mà chúng tôi đã nghĩ đến ở đây cho cấu trúc phân lớp là có thể

22
00:01:38,780 --> 00:01:44,680
Lớp thứ hai có thể nhận trên các cạnh và lớp thứ ba có thể nhận các mẫu như vòng lặp và đường kẻ

23
00:01:44,930 --> 00:01:48,729
Và người cuối cùng chỉ có thể ghép các mẫu đó để nhận ra các chữ số

24
00:01:49,369 --> 00:01:52,029
Vì vậy, ở đây chúng ta tìm hiểu cách mạng học

25
00:01:52,399 --> 00:01:57,099
Những gì chúng tôi muốn là một thuật toán nơi bạn có thể hiển thị mạng này toàn bộ dữ liệu đào tạo

26
00:01:57,229 --> 00:02:03,669
trong hình thức của một loạt các hình ảnh khác nhau của chữ số viết tay cùng với nhãn cho những gì họ đang có nghĩa vụ phải và

27
00:02:03,890 --> 00:02:05,659
Nó sẽ điều chỉnh những

28
00:02:05,659 --> 00:02:09,789
13000 trọng số và thành kiến ​​để cải thiện hiệu suất của nó trên dữ liệu đào tạo

29
00:02:10,730 --> 00:02:13,569
Hy vọng rằng cấu trúc lớp này sẽ có nghĩa là những gì nó học

30
00:02:14,269 --> 00:02:16,719
tổng quát hóa các hình ảnh ngoài dữ liệu đào tạo đó

31
00:02:16,720 --> 00:02:20,289
Và cách chúng tôi kiểm tra đó là sau khi bạn đào tạo mạng

32
00:02:20,290 --> 00:02:26,560
Bạn hiển thị nó theta có nhãn nhiều hơn mà nó chưa từng thấy trước đây và bạn thấy chính xác nó phân loại những hình ảnh mới như thế nào

33
00:02:31,040 --> 00:02:37,000
May mắn thay cho chúng ta và những gì làm cho một ví dụ phổ biến như vậy để bắt đầu là những người tốt đằng sau cơ sở MNIST có

34
00:02:37,000 --> 00:02:44,289
tập hợp hàng chục ngàn hình ảnh chữ viết tay mỗi hình ảnh được gắn nhãn với các con số mà chúng được cho là

35
00:02:44,720 --> 00:02:49,539
Đó là khiêu khích vì nó là để mô tả một máy như học tập một khi bạn thực sự thấy nó hoạt động như thế nào

36
00:02:49,540 --> 00:02:55,359
Nó cảm thấy ít hơn nhiều so với một số tiền đề khoa học viễn tưởng điên rồ và nhiều hơn nữa cũng như một bài tập tính toán

37
00:02:55,390 --> 00:02:59,589
Tôi có nghĩa là về cơ bản nó đi xuống để tìm tối thiểu của một chức năng nhất định

38
00:03:01,519 --> 00:03:05,199
Hãy nhớ về khái niệm chúng ta đang nghĩ về mỗi tế bào thần kinh như đang được kết nối

39
00:03:05,390 --> 00:03:12,309
cho tất cả các tế bào thần kinh trong lớp trước và trọng số trong tổng trọng số xác định kích hoạt của nó giống như

40
00:03:12,440 --> 00:03:14,060
điểm mạnh của những kết nối đó

41
00:03:14,060 --> 00:03:20,440
Và sự thiên vị là một số dấu hiệu cho thấy liệu nơron đó có xu hướng hoạt động hay không hoạt động và để bắt đầu mọi thứ

42
00:03:20,440 --> 00:03:26,919
Chúng tôi sẽ chỉ khởi tạo tất cả những trọng số và thành kiến ​​đó hoàn toàn không cần thiết để nói rằng mạng này sẽ thực hiện

43
00:03:26,919 --> 00:03:33,759
khá khủng khiếp trên một ví dụ đào tạo nhất định vì nó chỉ làm một cái gì đó ngẫu nhiên ví dụ bạn ăn trong hình ảnh này của một 3 và

44
00:03:33,760 --> 00:03:35,799
Lớp đầu ra nó trông giống như một mớ hỗn độn

45
00:03:36,349 --> 00:03:42,518
Vì vậy, những gì bạn làm là bạn xác định một hàm chi phí một cách để nói với máy tính: "Không có máy tính xấu!

46
00:03:42,739 --> 00:03:50,529
Đầu ra đó phải có kích hoạt bằng không cho hầu hết các nơ-ron, nhưng một cho nơron này những gì bạn đã cho tôi là rác hoàn toàn "

47
00:03:51,260 --> 00:03:56,530
Để nói rằng một chút toán học hơn những gì bạn làm là thêm lên các ô vuông của sự khác biệt giữa

48
00:03:56,720 --> 00:04:01,419
mỗi kích hoạt đầu ra thùng rác và giá trị mà bạn muốn chúng có và

49
00:04:01,489 --> 00:04:04,599
Đây là những gì chúng tôi gọi là chi phí của một ví dụ đào tạo duy nhất

50
00:04:05,599 --> 00:04:10,749
Lưu ý rằng số tiền này nhỏ khi mạng tự tin phân loại hình ảnh một cách chính xác

51
00:04:12,199 --> 00:04:15,639
Nhưng nó lớn khi mạng có vẻ như nó không thực sự biết nó đang làm gì

52
00:04:18,330 --> 00:04:25,249
Vì vậy, sau đó những gì bạn làm là xem xét chi phí trung bình trên tất cả hàng chục ngàn ví dụ đào tạo theo ý của bạn

53
00:04:27,060 --> 00:04:34,310
Chi phí trung bình này là thước đo của chúng tôi về mức độ tệ hại của mạng và mức độ cảm nhận của máy tính, và đó là một điều phức tạp

54
00:04:34,830 --> 00:04:38,960
Hãy nhớ rằng bản thân mạng cơ bản là một chức năng mà

55
00:04:39,540 --> 00:04:45,890
784 con số là đầu vào giá trị pixel và spits ra mười con số như đầu ra của nó và trong một ý nghĩa

56
00:04:45,890 --> 00:04:48,770
Nó được tham số hóa bởi tất cả các trọng số và thành kiến ​​này

57
00:04:49,140 --> 00:04:54,020
Trong khi hàm chi phí là một lớp phức tạp trên đầu trang mà nó lấy làm đầu vào của nó

58
00:04:54,450 --> 00:05:02,059
mười ba nghìn trọng lượng và thành kiến ​​đó và nó phun ra một con số duy nhất mô tả những trọng số và thành kiến ​​xấu như thế nào và

59
00:05:02,340 --> 00:05:08,749
Cách nó được xác định phụ thuộc vào hành vi của mạng trên tất cả hàng chục nghìn mẩu dữ liệu đào tạo

60
00:05:09,150 --> 00:05:11,150
Đó là rất nhiều suy nghĩ về

61
00:05:12,000 --> 00:05:15,619
Nhưng chỉ cần nói với máy tính những gì một công việc crappy, nó đang làm không phải là rất hữu ích

62
00:05:15,900 --> 00:05:19,819
Bạn muốn nói với nó như thế nào để thay đổi những trọng số và thiên vị để nó được tốt hơn?

63
00:05:20,820 --> 00:05:25,129
Để làm cho nó dễ dàng hơn là đấu tranh để tưởng tượng một chức năng với 13.000 đầu vào

64
00:05:25,130 --> 00:05:30,409
Chỉ cần hình dung một hàm đơn giản có một số làm đầu vào và một số làm đầu ra

65
00:05:30,960 --> 00:05:34,999
Làm thế nào để bạn tìm thấy một đầu vào giảm thiểu giá trị của chức năng này?

66
00:05:36,270 --> 00:05:40,039
Sinh viên Calculus sẽ biết rằng đôi khi bạn có thể tìm ra rằng tối thiểu một cách rõ ràng

67
00:05:40,260 --> 00:05:43,879
Nhưng điều đó không phải lúc nào cũng khả thi đối với các chức năng thực sự phức tạp

68
00:05:44,310 --> 00:05:52,160
Chắc chắn không phải trong mười ba nghìn phiên bản đầu vào của tình huống này cho chức năng mạng thần kinh phức tạp điên rồ của chúng ta

69
00:05:52,350 --> 00:05:59,029
Một chiến thuật linh hoạt hơn là bắt đầu ở bất kỳ đầu vào cũ nào và tìm ra hướng mà bạn nên thực hiện để làm cho đầu ra đó thấp hơn

70
00:06:00,120 --> 00:06:03,710
Cụ thể nếu bạn có thể tìm ra độ dốc của hàm bạn đang ở đâu

71
00:06:04,020 --> 00:06:09,619
Sau đó chuyển sang bên trái nếu độ dốc đó là dương và dịch chuyển đầu vào sang phải nếu độ dốc đó là âm

72
00:06:12,130 --> 00:06:16,799
Nếu bạn làm điều này nhiều lần tại mỗi điểm kiểm tra độ dốc mới và thực hiện bước thích hợp

73
00:06:16,800 --> 00:06:20,039
bạn sẽ tiếp cận một số hàm địa phương tối thiểu và

74
00:06:20,280 --> 00:06:24,080
hình ảnh bạn có thể có trong tâm trí ở đây là một quả bóng lăn xuống một ngọn đồi và

75
00:06:24,400 --> 00:06:30,900
Lưu ý ngay cả đối với chức năng nhập đơn lẻ thực sự đơn giản này, có nhiều thung lũng có thể bạn có thể

76
00:06:31,540 --> 00:06:36,220
Tùy thuộc vào đầu vào ngẫu nhiên nào bạn bắt đầu và không đảm bảo rằng mức tối thiểu địa phương

77
00:06:36,580 --> 00:06:39,040
Bạn hạ cánh sẽ là giá trị nhỏ nhất có thể của hàm chi phí

78
00:06:39,610 --> 00:06:44,009
Điều đó cũng sẽ chuyển sang trường hợp mạng thần kinh của chúng tôi, và tôi cũng muốn bạn chú ý

79
00:06:44,010 --> 00:06:47,190
Làm thế nào nếu bạn thực hiện kích thước bước của bạn tỷ lệ thuận với độ dốc

80
00:06:47,620 --> 00:06:54,540
Sau đó, khi độ dốc được làm phẳng về phía mức tối thiểu, các bước của bạn càng nhỏ hơn và loại đó giúp bạn vượt quá mức

81
00:06:55,720 --> 00:07:00,449
Bumping lên sự phức tạp một chút tưởng tượng thay vì một chức năng với hai yếu tố đầu vào và một đầu ra

82
00:07:01,120 --> 00:07:07,739
Bạn có thể nghĩ về không gian đầu vào như mặt phẳng XY và hàm chi phí như được vẽ đồ thị như một bề mặt phía trên nó

83
00:07:08,230 --> 00:07:15,060
Bây giờ thay vì hỏi về độ dốc của hàm bạn phải hỏi bạn nên bước theo hướng nào trong không gian đầu vào này?

84
00:07:15,310 --> 00:07:22,440
Vì vậy, để giảm đầu ra của chức năng nhanh nhất trong các từ khác. Hướng xuống dốc là gì?

85
00:07:22,440 --> 00:07:25,379
Và một lần nữa sẽ rất hữu ích khi nghĩ đến một quả bóng lăn xuống ngọn đồi đó

86
00:07:26,260 --> 00:07:34,080
Những người bạn quen thuộc với tính toán đa biến sẽ biết rằng độ dốc của một hàm cho bạn hướng đi lên dốc nhất

87
00:07:34,750 --> 00:07:38,459
Về cơ bản, bạn nên bước hướng nào để tăng chức năng nhanh nhất

88
00:07:39,100 --> 00:07:46,439
đủ tự nhiên lấy âm của gradient đó cho bạn hướng đến bước làm giảm chức năng nhanh nhất và

89
00:07:47,020 --> 00:07:53,400
Thậm chí nhiều hơn chiều dài của vector gradient này thực sự là một dấu hiệu cho thấy độ dốc dốc lớn nhất là bao nhiêu

90
00:07:54,130 --> 00:07:56,280
Bây giờ nếu bạn không quen với phép tính đa biến

91
00:07:56,280 --> 00:08:00,239
Và bạn muốn tìm hiểu thêm về một số công việc mà tôi đã làm cho Khan Academy về chủ đề này

92
00:08:00,910 --> 00:08:03,779
Thành thật mà nói, mặc dù tất cả những gì quan trọng đối với bạn và tôi ngay bây giờ

93
00:08:03,780 --> 00:08:09,419
Về nguyên tắc có tồn tại một cách để tính toán vectơ này không. Vector này cho bạn biết

94
00:08:09,520 --> 00:08:15,900
Hướng xuống dốc và độ dốc bạn sẽ ổn nếu đó là tất cả những gì bạn biết và bạn không chắc chắn về chi tiết

95
00:08:16,790 --> 00:08:24,580
bởi vì nếu bạn có thể nhận được rằng thuật toán từ việc giảm thiểu hàm là tính toán hướng gradient sau đó thực hiện một bước nhỏ xuống dốc và

96
00:08:24,740 --> 00:08:26,740
Chỉ cần lặp đi lặp lại điều đó nhiều lần

97
00:08:27,800 --> 00:08:34,600
Đó là ý tưởng cơ bản tương tự cho một hàm có 13.000 đầu vào thay vì hai đầu vào tưởng tượng tổ chức tất cả

98
00:08:35,330 --> 00:08:39,400
13.000 trọng lượng và thành kiến ​​của mạng lưới của chúng tôi thành một vector cột khổng lồ

99
00:08:39,680 --> 00:08:43,870
Độ dốc âm của hàm chi phí chỉ là một vectơ

100
00:08:43,880 --> 00:08:49,299
Đó là một số hướng bên trong không gian đầu vào cực kỳ lớn này cho bạn biết

101
00:08:49,400 --> 00:08:55,030
nudges cho tất cả những con số này sẽ làm giảm nhanh nhất chức năng chi phí và

102
00:08:55,460 --> 00:08:58,150
tất nhiên với chức năng chi phí được thiết kế đặc biệt của chúng tôi

103
00:08:58,580 --> 00:09:04,900
Thay đổi trọng số và thành kiến ​​để giảm nó có nghĩa là làm cho đầu ra của mạng trên mỗi phần dữ liệu huấn luyện

104
00:09:05,180 --> 00:09:10,599
Nhìn giống như một mảng ngẫu nhiên gồm mười giá trị và giống như một quyết định thực tế mà chúng tôi muốn nó thực hiện

105
00:09:11,030 --> 00:09:16,030
Điều quan trọng cần nhớ là hàm chi phí này liên quan đến mức trung bình trên tất cả dữ liệu đào tạo

106
00:09:16,370 --> 00:09:20,590
Vì vậy, nếu bạn giảm thiểu nó có nghĩa là nó là một hiệu suất tốt hơn trên tất cả những mẫu

107
00:09:23,780 --> 00:09:30,849
Thuật toán để tính toán độ dốc này một cách hiệu quả, đó chính là tâm điểm của cách mà một mạng nơron học được gọi là truyền lại

108
00:09:31,190 --> 00:09:34,690
Và đó là những gì tôi sẽ nói về video tiếp theo

109
00:09:34,690 --> 00:09:36,690
Ở đó tôi thực sự muốn dành thời gian để đi qua

110
00:09:36,830 --> 00:09:41,439
Chính xác điều gì sẽ xảy ra đối với mỗi trọng lượng và từng thiên vị đối với một đoạn dữ liệu đào tạo nhất định?

111
00:09:41,810 --> 00:09:46,960
Cố gắng tạo cảm giác trực quan cho những gì đang diễn ra ngoài đống tích phân và công thức có liên quan

112
00:09:47,510 --> 00:09:52,179
Ngay tại đây ngay bây giờ là điều chính. Tôi muốn bạn biết độc lập về chi tiết triển khai

113
00:09:52,180 --> 00:09:58,479
là những gì chúng tôi muốn nói khi chúng ta nói về việc học mạng là nó chỉ giảm thiểu chức năng chi phí và

114
00:09:58,940 --> 00:10:04,479
Lưu ý một hệ quả của điều đó là nó quan trọng đối với hàm chi phí này để có đầu ra mượt mà

115
00:10:04,480 --> 00:10:07,810
Để chúng tôi có thể tìm thấy mức tối thiểu địa phương bằng cách thực hiện các bước nhỏ xuống dốc

116
00:10:08,810 --> 00:10:10,520
Đây là lý do tại sao bằng cách này

117
00:10:10,520 --> 00:10:16,749
Tế bào thần kinh nhân tạo liên tục kích hoạt khác nhau hơn là chỉ hoạt động hoặc không hoạt động theo cách nhị phân

118
00:10:16,750 --> 00:10:18,750
nếu cách mà các tế bào thần kinh sinh học

119
00:10:19,940 --> 00:10:26,770
Quá trình này liên tục nudging một đầu vào của một hàm bởi một số bội số của gradient âm được gọi là gradient gốc

120
00:10:26,930 --> 00:10:32,380
Đó là một cách để hội tụ hướng tới một số hàm tối thiểu địa phương của hàm chi phí về cơ bản là một thung lũng trong biểu đồ này

121
00:10:32,930 --> 00:10:38,890
Tôi vẫn hiển thị hình ảnh của một hàm với hai yếu tố đầu vào của khóa học bởi vì nudges trong một đầu vào mười ba nghìn chiều

122
00:10:38,890 --> 00:10:44,049
Không gian có một chút khó khăn để bao quanh tâm trí bạn, nhưng thực sự có một cách không gian không gian đẹp để suy nghĩ về điều này

123
00:10:44,630 --> 00:10:51,340
Mỗi thành phần của gradient âm cho chúng ta biết hai điều dấu hiệu của khóa học cho chúng ta biết liệu tương ứng

124
00:10:51,830 --> 00:10:59,139
Thành phần của vector đầu vào phải được đẩy lên hoặc xuống, nhưng quan trọng là độ lớn tương đối của tất cả các thành phần này

125
00:10:59,840 --> 00:11:02,530
Loại cho bạn biết những thay đổi nào quan trọng hơn

126
00:11:05,150 --> 00:11:09,340
Bạn thấy trong mạng của chúng tôi, việc điều chỉnh một trong các trọng số có thể lớn hơn nhiều

127
00:11:09,710 --> 00:11:12,939
tác động đến chức năng chi phí so với việc điều chỉnh trọng lượng khác

128
00:11:14,450 --> 00:11:17,950
Một số kết nối này chỉ quan trọng hơn đối với dữ liệu đào tạo của chúng tôi

129
00:11:18,920 --> 00:11:22,690
Vì vậy, một cách mà bạn có thể suy nghĩ về vector gradient của tâm trí của chúng tôi-cong vênh

130
00:11:22,690 --> 00:11:27,999
chức năng chi phí lớn là nó mã hóa tầm quan trọng tương đối của mỗi trọng lượng và độ lệch

131
00:11:28,250 --> 00:11:32,200
Đó là thay đổi nào trong số những thay đổi này sẽ mang lại nhiều lợi nhuận nhất cho bạn

132
00:11:33,560 --> 00:11:36,460
Đây thực sự chỉ là một cách nghĩ khác về hướng

133
00:11:36,860 --> 00:11:41,290
Để có một ví dụ đơn giản hơn nếu bạn có một số hàm với hai biến làm đầu vào và bạn

134
00:11:41,690 --> 00:11:46,540
Tính toán rằng độ dốc của nó tại một số điểm cụ thể xuất hiện dưới dạng (3,1)

135
00:11:47,420 --> 00:11:51,670
Sau đó, một mặt bạn có thể giải thích điều đó khi nói rằng khi bạn đang đứng ở đầu vào đó

136
00:11:52,070 --> 00:11:55,150
di chuyển theo hướng này làm tăng chức năng nhanh nhất

137
00:11:55,460 --> 00:12:02,229
Khi bạn vẽ đồ thị hàm trên mặt phẳng của điểm đầu vào mà vector là thứ cho bạn hướng đi lên trên thẳng

138
00:12:02,600 --> 00:12:06,580
Nhưng một cách khác để đọc đó là nói rằng những thay đổi đối với biến đầu tiên này

139
00:12:06,740 --> 00:12:13,390
Có ba lần tầm quan trọng như những thay đổi đối với biến thứ hai mà ít nhất là trong vùng lân cận của đầu vào có liên quan

140
00:12:13,520 --> 00:12:16,689
Nudging giá trị x mang nhiều bang hơn cho buck của bạn

141
00:12:19,310 --> 00:12:19,930
Được rồi

142
00:12:19,930 --> 00:12:24,940
Hãy thu nhỏ và tổng hợp vị trí của chúng ta cho đến nay chính mạng là chức năng này

143
00:12:25,400 --> 00:12:29,859
784 đầu vào và 10 đầu ra được xác định theo tất cả các khoản tiền có trọng số này

144
00:12:30,350 --> 00:12:34,780
hàm chi phí là một lớp phức tạp trên đầu trang

145
00:12:35,120 --> 00:12:41,870
13.000 trọng lượng và thành kiến ​​như đầu vào và phun ra một thước đo duy nhất về sự tệ hại dựa trên các ví dụ đào tạo và

146
00:12:42,180 --> 00:12:47,930
Độ dốc của hàm chi phí là một lớp phức tạp hơn, nó vẫn cho chúng ta biết

147
00:12:47,930 --> 00:12:53,839
Nudges cho tất cả các trọng số và thiên vị gây ra sự thay đổi nhanh nhất cho giá trị của hàm chi phí

148
00:12:53,970 --> 00:12:57,680
Điều bạn có thể giải thích là nói những thay đổi nào đối với trọng số nào quan trọng nhất

149
00:13:02,550 --> 00:13:09,289
Vì vậy, khi bạn khởi tạo mạng với các trọng số và độ lệch ngẫu nhiên và điều chỉnh chúng nhiều lần dựa trên quá trình tô đậm này

150
00:13:09,420 --> 00:13:12,949
Nó thực sự hoạt động tốt như thế nào trên những hình ảnh chưa từng thấy trước đây?

151
00:13:13,680 --> 00:13:19,609
Vâng, cái mà tôi đã mô tả ở đây với hai lớp tế bào thần kinh mười sáu ẩn được lựa chọn chủ yếu vì lý do thẩm mỹ

152
00:13:20,579 --> 00:13:26,089
tốt, nó không tệ, nó phân loại khoảng 96% hình ảnh mới mà nó thấy chính xác và

153
00:13:26,759 --> 00:13:32,239
Thành thật mà nói, nếu bạn nhìn vào một số ví dụ mà nó messes lên trên bạn loại cảm thấy bắt buộc phải cắt nó một chút slack

154
00:13:35,759 --> 00:13:39,079
Bây giờ nếu bạn chơi xung quanh với cấu trúc lớp ẩn và thực hiện một vài chỉnh sửa

155
00:13:39,079 --> 00:13:43,698
Bạn có thể nhận được điều này lên đến 98% và điều đó khá tốt. Nó không phải là tốt nhất

156
00:13:43,740 --> 00:13:48,409
Bạn chắc chắn có thể có được hiệu suất tốt hơn bằng cách tinh vi hơn mạng lưới vanilla thuần túy này

157
00:13:48,569 --> 00:13:52,669
Nhưng với cách làm nản chí nhiệm vụ ban đầu, tôi chỉ nghĩ có điều gì đó?

158
00:13:52,889 --> 00:13:56,929
Không thể tin được về bất kỳ mạng nào hoạt động tốt trên những hình ảnh mà nó chưa từng thấy trước đây

159
00:13:57,389 --> 00:14:00,919
Cho rằng chúng tôi không bao giờ cụ thể nói với nó những gì mô hình để tìm kiếm

160
00:14:02,579 --> 00:14:07,068
Ban đầu cách mà tôi thúc đẩy cấu trúc này là bằng cách mô tả một hy vọng rằng chúng ta có thể có

161
00:14:07,259 --> 00:14:09,739
Đó là lớp thứ hai có thể nhặt trên các cạnh nhỏ

162
00:14:09,809 --> 00:14:17,089
Rằng lớp thứ ba sẽ ghép các cạnh đó để nhận ra các vòng lặp và các đường dài hơn và các đường đó có thể được ghép lại với nhau để nhận ra các chữ số

163
00:14:17,699 --> 00:14:22,729
Vậy đây có phải là điều mà mạng của chúng tôi thực sự đang thực hiện không? Vâng cho điều này ít nhất

164
00:14:23,339 --> 00:14:24,449
Không có gì

165
00:14:24,449 --> 00:14:27,409
hãy nhớ xem video cuối cùng chúng tôi đã xem xét các trọng số của

166
00:14:27,480 --> 00:14:31,849
Kết nối từ tất cả các nơron trong lớp đầu tiên đến một nơron đã cho trong lớp thứ hai

167
00:14:31,980 --> 00:14:36,829
Có thể được hiển thị dưới dạng mẫu pixel nhất định mà nơron lớp thứ hai đang chọn

168
00:14:37,350 --> 00:14:43,309
Vâng, khi chúng tôi thực sự làm điều đó cho các trọng số liên quan đến các chuyển tiếp này từ lớp đầu tiên đến lớp tiếp theo

169
00:14:43,709 --> 00:14:50,209
Thay vì chọn lên các cạnh nhỏ bị cô lập ở đây và ở đó. Họ nhìn gần như ngẫu nhiên

170
00:14:50,370 --> 00:14:56,399
Chỉ cần đặt một số mô hình rất lỏng lẻo ở giữa có vẻ như là trong không thể tưởng tượng lớn

171
00:14:56,920 --> 00:15:02,580
13.000 chiều không gian có thể có trọng lượng và thiên vị của mạng lưới của chúng tôi nhận thấy chính nó là một địa phương ít hài lòng nhất

172
00:15:02,860 --> 00:15:08,940
mặc dù phân loại thành công hầu hết các hình ảnh không chính xác nhận các mẫu mà chúng tôi có thể hy vọng và

173
00:15:09,430 --> 00:15:13,709
Để thực sự thúc đẩy việc xem trang chủ này, điều gì sẽ xảy ra khi bạn nhập một hình ảnh ngẫu nhiên

174
00:15:14,019 --> 00:15:21,449
nếu hệ thống thông minh, bạn có thể mong đợi nó hoặc cảm thấy không chắc chắn có thể không thực sự kích hoạt bất kỳ 10 nơron đầu ra nào hoặc

175
00:15:21,579 --> 00:15:23,200
Kích hoạt tất cả đều

176
00:15:23,200 --> 00:15:24,820
Nhưng thay vào đó

177
00:15:24,820 --> 00:15:32,010
Tự tin cung cấp cho bạn một số câu trả lời vô nghĩa như thể nó cảm thấy chắc chắn rằng tiếng ồn ngẫu nhiên này là 5 vì nó thực sự là một

178
00:15:32,010 --> 00:15:34,010
hình ảnh của 5 là 5

179
00:15:34,180 --> 00:15:40,829
cụm từ khác nhau ngay cả khi mạng này có thể nhận ra chữ số khá tốt, nó không có ý tưởng làm thế nào để vẽ chúng

180
00:15:41,500 --> 00:15:45,149
Rất nhiều điều này là bởi vì đó là một thiết lập đào tạo hạn chế chặt chẽ

181
00:15:45,149 --> 00:15:51,479
Ý tôi là đặt mình vào vị trí của mạng lưới ở đây từ quan điểm của nó, toàn thể vũ trụ không có gì cả

182
00:15:51,480 --> 00:15:57,539
Nhưng đã xác định rõ ràng các chữ số chưa được canh giữa trong một lưới nhỏ và hàm chi phí của nó chưa bao giờ cho nó

183
00:15:57,700 --> 00:16:00,959
Khuyến khích là bất cứ điều gì, nhưng hoàn toàn tự tin trong quyết định của mình

184
00:16:01,690 --> 00:16:05,070
Vì vậy, nếu đây là hình ảnh của những gì các tế bào thần kinh lớp thứ hai đang thực sự làm

185
00:16:05,140 --> 00:16:09,839
Bạn có thể tự hỏi tại sao tôi sẽ giới thiệu mạng này với động cơ chọn lên các cạnh và các mẫu

186
00:16:09,839 --> 00:16:11,969
Ý tôi là, đó không phải là tất cả những gì nó kết thúc

187
00:16:13,029 --> 00:16:17,909
Vâng, đây không phải là mục tiêu cuối cùng của chúng tôi, nhưng thay vào đó là một điểm khởi đầu thẳng thắn

188
00:16:17,910 --> 00:16:19,120
Đây là công nghệ cũ

189
00:16:19,120 --> 00:16:21,510
loại nghiên cứu trong những năm 80 và 90 và

190
00:16:21,640 --> 00:16:29,129
Bạn cần phải hiểu nó trước khi bạn có thể hiểu các biến thể hiện đại chi tiết hơn và rõ ràng là có khả năng giải quyết một số vấn đề thú vị

191
00:16:29,410 --> 00:16:34,110
Nhưng càng có nhiều bạn đào sâu vào những gì những lớp ẩn thực sự làm ít thông minh hơn

192
00:16:38,530 --> 00:16:42,359
Chuyển tiêu điểm trong giây lát từ cách mạng tìm hiểu cách bạn học

193
00:16:42,580 --> 00:16:46,139
Điều đó sẽ chỉ xảy ra nếu bạn tham gia tích cực với tài liệu ở đây bằng cách nào đó

194
00:16:46,660 --> 00:16:53,100
Một điều khá đơn giản mà tôi muốn bạn làm là tạm dừng ngay bây giờ và suy nghĩ sâu sắc một chút về những gì

195
00:16:53,440 --> 00:16:55,230
Những thay đổi bạn có thể thực hiện đối với hệ thống này

196
00:16:55,230 --> 00:17:00,719
Và làm thế nào nó cảm nhận được hình ảnh nếu bạn muốn nó nhận được tốt hơn về những thứ như các cạnh và các mẫu?

197
00:17:01,360 --> 00:17:04,410
Nhưng tốt hơn là để thực sự tương tác với vật liệu

198
00:17:04,410 --> 00:17:05,079
tôi

199
00:17:05,079 --> 00:17:08,969
Đề xuất cuốn sách của Michael Nielsen về mạng học tập và mạng thần kinh

200
00:17:09,190 --> 00:17:14,369
Trong đó, bạn có thể tìm thấy mã và dữ liệu để tải xuống và chơi với ví dụ chính xác này

201
00:17:14,410 --> 00:17:18,089
Và cuốn sách sẽ hướng dẫn bạn từng bước những gì mã đó đang làm

202
00:17:18,910 --> 00:17:21,749
Điều tuyệt vời là cuốn sách này miễn phí và có sẵn công khai

203
00:17:22,360 --> 00:17:27,540
Vì vậy, nếu bạn nhận được một cái gì đó ra khỏi nó xem xét tham gia với tôi trong việc đóng góp cho những nỗ lực của Nielsen

204
00:17:27,910 --> 00:17:32,219
Tôi cũng đã liên kết một vài tài nguyên khác mà tôi thích rất nhiều trong mô tả bao gồm

205
00:17:32,470 --> 00:17:36,390
bài đăng blog tuyệt vời và đẹp đẽ của Chris Ola và các bài viết trong chưng cất

206
00:17:38,230 --> 00:17:40,200
Để đóng mọi thứ ở đây trong vài phút cuối

207
00:17:40,200 --> 00:17:43,740
Tôi muốn trở lại một đoạn phỏng vấn mà tôi đã có với Leisha Lee

208
00:17:43,930 --> 00:17:49,079
Bạn có thể nhớ cô ấy từ video cuối cùng. Cô đã làm công việc tiến sĩ của mình trong việc học sâu và trong đoạn trích nhỏ này

209
00:17:49,080 --> 00:17:55,530
Cô ấy nói về hai bài báo gần đây thực sự tìm hiểu cách một số mạng công nhận hình ảnh hiện đại hơn thực sự đang học

210
00:17:55,810 --> 00:18:01,349
Chỉ để thiết lập nơi chúng ta đang ở trong cuộc trò chuyện, bài báo đầu tiên đã lấy một trong những mạng thần kinh đặc biệt sâu này

211
00:18:01,350 --> 00:18:05,910
Điều đó thực sự tốt khi nhận dạng hình ảnh và thay vì đào tạo nó trên một dữ liệu được dán nhãn đúng cách

212
00:18:05,910 --> 00:18:08,579
Đặt nó xáo trộn tất cả các nhãn xung quanh trước khi đào tạo

213
00:18:08,800 --> 00:18:14,669
Rõ ràng độ chính xác thử nghiệm ở đây sẽ không tốt hơn ngẫu nhiên vì mọi thứ chỉ được dán nhãn ngẫu nhiên

214
00:18:14,800 --> 00:18:20,879
Nhưng nó vẫn có thể đạt được độ chính xác đào tạo giống như bạn có trên một tập dữ liệu được dán nhãn đúng cách

215
00:18:21,490 --> 00:18:27,540
Về cơ bản, hàng triệu trọng lượng cho mạng cụ thể này là đủ để nó chỉ ghi nhớ dữ liệu ngẫu nhiên

216
00:18:27,820 --> 00:18:34,379
Loại câu hỏi đặt ra cho việc giảm thiểu chức năng chi phí này có thực sự tương ứng với bất kỳ loại cấu trúc nào trong hình ảnh không?

217
00:18:34,380 --> 00:18:36,380
Hay chỉ là bạn biết?

218
00:18:36,520 --> 00:18:37,420
ghi nhớ toàn bộ

219
00:18:37,420 --> 00:18:43,859
Tập dữ liệu về phân loại chính xác là gì và vì vậy một vài bạn biết nửa năm sau tại ICML năm nay

220
00:18:44,470 --> 00:18:49,039
Không có giấy giấy chính xác nào đề cập đến một số người được hỏi như này

221
00:18:49,470 --> 00:18:55,279
Trên thực tế, các mạng này đang hoạt động thông minh hơn một chút nếu bạn nhìn vào đường cong chính xác đó

222
00:18:55,279 --> 00:18:57,499
nếu bạn chỉ tập luyện

223
00:18:58,259 --> 00:19:05,179
Bộ dữ liệu ngẫu nhiên mà đường cong sắp xếp đi xuống rất bạn biết rất chậm trong hầu hết các loại thời trang tuyến tính

224
00:19:05,179 --> 00:19:09,589
Vì vậy, bạn đang thực sự đấu tranh để tìm thấy rằng minima địa phương có thể

225
00:19:09,590 --> 00:19:15,289
bạn biết đúng trọng lượng sẽ giúp bạn có được độ chính xác đó trong khi nếu bạn đang thực sự đào tạo trên bộ dữ liệu có cấu trúc có

226
00:19:15,289 --> 00:19:21,439
Nhãn phải. Bạn biết bạn fiddle xung quanh một chút trong đầu, nhưng sau đó bạn loại bỏ rất nhanh để có được điều đó

227
00:19:22,200 --> 00:19:26,149
Mức độ chính xác và vì vậy theo một nghĩa nào đó, việc tìm ra

228
00:19:26,759 --> 00:19:33,949
Địa phương tối đa và do đó, nó cũng thú vị về điều đó là nó bị bắt đưa vào ánh sáng giấy khác từ thực tế một vài năm trước đây

229
00:19:34,080 --> 00:19:36,080
Trong đó có nhiều hơn

230
00:19:36,990 --> 00:19:39,169
đơn giản hóa về các lớp mạng

231
00:19:39,169 --> 00:19:46,788
Nhưng một trong những kết quả đã nói rằng nếu bạn nhìn vào cảnh quan tối ưu hóa, minima địa phương mà các mạng này có xu hướng tìm hiểu là

232
00:19:47,340 --> 00:19:54,079
Trên thực tế có chất lượng như nhau vì vậy trong một số ý nghĩa nếu tập dữ liệu của bạn là cấu trúc và bạn sẽ có thể tìm thấy dễ dàng hơn nhiều

233
00:19:58,139 --> 00:20:01,189
Lời cảm ơn của tôi như mọi khi trong số các bạn ủng hộ trên patreon

234
00:20:01,190 --> 00:20:06,950
Tôi đã nói trước đó chỉ là những gì một cuộc đổi tên game là nhưng những đoạn video này thực sự sẽ không thể nếu không có bạn tôi

235
00:20:07,230 --> 00:20:12,889
Cũng muốn đưa ra một đặc biệt. Cảm ơn các đối tác amplifi của VC trong việc hỗ trợ những video ban đầu này trong bộ phim

