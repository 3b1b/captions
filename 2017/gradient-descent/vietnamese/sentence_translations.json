[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "Video cuối cùng tôi đã trình bày cấu trúc của mạng lưới thần kinh.",
  "model": "google_nmt",
  "from_community_srt": "Video cuối cùng tôi đã trình bày cấu trúc của mạng thần kinh Tôi sẽ cung cấp một bản tóm tắt nhanh chóng ở đây chỉ để nó tươi trong tâm trí của chúng tôi",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "Tôi sẽ tóm tắt nhanh ở đây để chúng ta dễ nhớ hơn và sau đó tôi có hai mục tiêu chính cho video này.",
  "model": "google_nmt",
  "from_community_srt": "Và sau đó tôi có hai mục tiêu chính cho video này.",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "Đầu tiên là giới thiệu ý tưởng về độ dốc giảm dần, không chỉ làm nền tảng cho cách mạng lưới thần kinh học mà còn làm cơ sở cho nhiều hoạt động học máy khác.",
  "model": "google_nmt",
  "from_community_srt": "Đầu tiên là giới thiệu ý tưởng về độ dốc, không chỉ là cách mạng lưới thần kinh học hỏi, nhưng cũng có rất nhiều công cụ học máy khác hoạt động tốt Sau đó,",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "Sau đó, chúng ta sẽ tìm hiểu thêm một chút về cách thức hoạt động của mạng cụ thể này và những lớp tế bào thần kinh ẩn đó đang tìm kiếm điều gì.",
  "model": "google_nmt",
  "from_community_srt": "sau đó chúng ta sẽ tìm hiểu thêm về cách mạng này hoạt động Và những lớp tế bào thần kinh ẩn giấu đó thực sự đang tìm kiếm",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "Xin nhắc lại, mục tiêu của chúng tôi ở đây là ví dụ cổ điển về nhận dạng chữ số viết tay, thế giới xin chào của mạng lưới thần kinh.",
  "model": "google_nmt",
  "from_community_srt": "Nhắc nhở, mục tiêu của chúng tôi ở đây là ví dụ điển hình về nhận dạng chữ viết tay thế giới hello của mạng thần kinh",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "Các chữ số này được hiển thị trên lưới 28x28 pixel, mỗi pixel có một số giá trị thang độ xám trong khoảng từ 0 đến 1.",
  "model": "google_nmt",
  "from_community_srt": "các chữ số này được hiển thị trên lưới 28 x 28 pixel mỗi pixel với một số giá trị thang độ xám giữa 0 và 1",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "Đó là những gì quyết định sự kích hoạt của 784 nơ-ron trong lớp đầu vào của mạng.",
  "model": "google_nmt",
  "from_community_srt": "đó là những gì xác định kích hoạt của 784 tế bào thần kinh trong lớp đầu vào của mạng và Sau đó kích hoạt cho mỗi nơron trong các lớp sau dựa trên tổng trọng số của",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "Và sau đó, việc kích hoạt mỗi nơ-ron trong các lớp sau dựa trên tổng trọng số của tất cả các lần kích hoạt ở lớp trước, cộng với một số đặc biệt gọi là độ lệch.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "Sau đó, bạn tính số tiền đó bằng một số hàm khác, chẳng hạn như tính năng chia nhỏ sigmoid hoặc relu, như cách tôi đã xem qua video trước.",
  "model": "google_nmt",
  "from_community_srt": "Tất cả các hoạt động trong lớp trước cộng với một số số đặc biệt được gọi là thiên vị sau đó bạn soạn tổng hợp đó với một số hàm khác như squigification sigmoid hoặc",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "Tổng cộng, với sự lựa chọn có phần tùy tiện của hai lớp ẩn với 16 nơ-ron mỗi lớp, mạng có khoảng 13.000 trọng số và độ lệch mà chúng ta có thể điều chỉnh và chính những giá trị này sẽ xác định chính xác những gì mạng thực sự làm.",
  "model": "google_nmt",
  "from_community_srt": "một cách thích hợp theo cách tôi đã xem qua video cuối cùng Trong tổng số cho sự lựa chọn phần nào tùy ý của hai lớp ẩn ở đây với 16 tế bào thần kinh mỗi mạng có khoảng 13.000 trọng số và thành kiến ​​mà chúng tôi có thể điều chỉnh và đó là những giá trị này xác định chính xác mạng bạn biết thực sự",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "Vậy thì điều chúng tôi muốn nói khi nói rằng mạng này phân loại một chữ số nhất định là điểm sáng nhất trong số 10 nơ-ron ở lớp cuối cùng tương ứng với chữ số đó.",
  "model": "google_nmt",
  "from_community_srt": "Sau đó, điều chúng tôi muốn nói khi chúng tôi nói rằng mạng này phân loại một số đã cho Có phải đó là 10 tế bào thần kinh trong lớp cuối cùng tương ứng với chữ số đó",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "Và hãy nhớ rằng, động lực mà chúng tôi nghĩ đến ở đây đối với cấu trúc phân lớp là có thể lớp thứ hai có thể xử lý các cạnh và lớp thứ ba có thể xử lý các mẫu như vòng và đường, và lớp cuối cùng có thể ghép những thứ đó lại với nhau. mẫu để nhận biết chữ số.",
  "model": "google_nmt",
  "from_community_srt": "Và hãy nhớ động lực mà chúng tôi đã nghĩ đến ở đây cho cấu trúc phân lớp là có thể Lớp thứ hai có thể nhận trên các cạnh và lớp thứ ba có thể nhận các mẫu như vòng lặp và đường kẻ Và người cuối cùng chỉ có thể ghép các mẫu đó để nhận ra các chữ số Vì vậy,",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "Vì vậy, ở đây chúng ta tìm hiểu cách mạng học.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "Điều chúng tôi muốn là một thuật toán mà bạn có thể hiển thị cho mạng này toàn bộ dữ liệu huấn luyện, dưới dạng một loạt các hình ảnh khác nhau của các chữ số viết tay, cùng với các nhãn cho biết chúng phải là gì, và nó sẽ điều chỉnh 13.000 trọng số và độ lệch đó để cải thiện hiệu suất của nó trên dữ liệu huấn luyện.",
  "model": "google_nmt",
  "from_community_srt": "ở đây chúng ta tìm hiểu cách mạng học Những gì chúng tôi muốn là một thuật toán nơi bạn có thể hiển thị mạng này toàn bộ dữ liệu đào tạo trong hình thức của một loạt các hình ảnh khác nhau của chữ số viết tay cùng với nhãn cho những gì họ đang có nghĩa vụ phải và Nó sẽ điều chỉnh những 13000 trọng số và thành kiến ​​để cải thiện hiệu suất của nó trên dữ liệu đào tạo",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "Hy vọng rằng cấu trúc phân lớp này sẽ có nghĩa là những gì nó học được sẽ khái quát hóa thành các hình ảnh ngoài dữ liệu huấn luyện đó.",
  "model": "google_nmt",
  "from_community_srt": "Hy vọng rằng cấu trúc lớp này sẽ có nghĩa là những gì nó học tổng quát hóa các hình ảnh ngoài dữ liệu đào tạo đó",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "Cách chúng tôi kiểm tra là sau khi bạn huấn luyện mạng, bạn hiển thị cho nó nhiều dữ liệu được gắn nhãn hơn mà nó chưa từng thấy trước đây và bạn thấy nó phân loại những hình ảnh mới đó chính xác như thế nào.",
  "model": "google_nmt",
  "from_community_srt": "Và cách chúng tôi kiểm tra đó là sau khi bạn đào tạo mạng Bạn hiển thị nó theta có nhãn nhiều hơn mà nó chưa từng thấy trước đây và bạn thấy chính xác nó phân loại những hình ảnh mới như thế nào",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "Thật may mắn cho chúng ta, và điều khiến đây trở thành một ví dụ phổ biến để bắt đầu, đó là những người tốt đằng sau cơ sở dữ liệu MNIST đã tập hợp một bộ sưu tập gồm hàng chục nghìn hình ảnh chữ số viết tay, mỗi hình ảnh được dán nhãn bằng những con số mà chúng phải ghi. là.",
  "model": "google_nmt",
  "from_community_srt": "May mắn thay cho chúng ta và những gì làm cho một ví dụ phổ biến như vậy để bắt đầu là những người tốt đằng sau cơ sở MNIST có tập hợp hàng chục ngàn hình ảnh chữ viết tay mỗi hình ảnh được gắn nhãn với các con số mà chúng được cho là",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "Và thật khiêu khích khi mô tả một cỗ máy đang học hỏi, một khi bạn thấy nó hoạt động như thế nào, nó có cảm giác không giống một tiền đề khoa học viễn tưởng điên rồ nào đó mà giống một bài tập tính toán hơn rất nhiều.",
  "model": "google_nmt",
  "from_community_srt": "Đó là khiêu khích vì nó là để mô tả một máy như học tập một khi bạn thực sự thấy nó hoạt động như thế nào Nó cảm thấy ít hơn nhiều so với một số tiền đề khoa học viễn tưởng điên rồ và nhiều hơn nữa cũng như một bài tập tính toán",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "Ý tôi là, về cơ bản, vấn đề là tìm giá trị nhỏ nhất của một hàm nào đó.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "Hãy nhớ rằng, về mặt khái niệm, chúng ta đang nghĩ mỗi nơ-ron được kết nối với tất cả các nơ-ron ở lớp trước và các trọng số trong tổng trọng số xác định sự kích hoạt của nó giống như độ mạnh của các kết nối đó và độ lệch là một số dấu hiệu cho thấy tế bào thần kinh đó có xu hướng hoạt động hay không hoạt động.",
  "model": "google_nmt",
  "from_community_srt": "Tôi có nghĩa là về cơ bản nó đi xuống để tìm tối thiểu của một chức năng nhất định Hãy nhớ về khái niệm chúng ta đang nghĩ về mỗi tế bào thần kinh như đang được kết nối cho tất cả các tế bào thần kinh trong lớp trước và trọng số trong tổng trọng số xác định kích hoạt của nó giống như điểm mạnh của những kết nối đó Và sự thiên vị là một số dấu hiệu cho thấy liệu nơron đó có xu hướng hoạt động hay không hoạt động và để bắt đầu mọi thứ",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "Và để bắt đầu mọi thứ, chúng ta sẽ khởi tạo tất cả các trọng số và độ lệch hoàn toàn ngẫu nhiên.",
  "model": "google_nmt",
  "from_community_srt": "Chúng tôi sẽ chỉ khởi tạo tất cả những trọng số và thành kiến ​​đó hoàn toàn không cần thiết để nói rằng mạng này sẽ thực hiện",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "Không cần phải nói, mạng này sẽ hoạt động khá tệ trên một ví dụ huấn luyện nhất định, vì nó chỉ thực hiện điều gì đó ngẫu nhiên.",
  "model": "google_nmt",
  "from_community_srt": "khá khủng khiếp trên một ví dụ đào tạo nhất định vì nó chỉ làm một cái gì đó ngẫu nhiên ví dụ bạn ăn trong hình ảnh này của một 3 và",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "Ví dụ: bạn nạp hình ảnh số 3 này và lớp đầu ra trông giống như một mớ hỗn độn.",
  "model": "google_nmt",
  "from_community_srt": "Lớp đầu ra nó trông giống như một mớ hỗn độn Vì vậy,",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "Vì vậy, những gì bạn làm là xác định hàm chi phí, một cách để nói với máy tính, không, máy tính xấu, đầu ra đó phải có số lần kích hoạt là 0 đối với hầu hết các nơ-ron, nhưng là 1 đối với nơ-ron này, những gì bạn đưa cho tôi hoàn toàn là rác rưởi.",
  "model": "google_nmt",
  "from_community_srt": "những gì bạn làm là bạn xác định một hàm chi phí một cách để nói với máy tính: \"Không có máy tính xấu! Đầu ra đó phải có kích hoạt bằng không cho hầu hết các nơ-ron,",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "Nói một cách toán học hơn một chút, bạn cộng các bình phương của sự khác biệt giữa mỗi lần kích hoạt đầu ra rác đó với giá trị mà bạn muốn chúng có, và đây là cái mà chúng tôi gọi là chi phí của một ví dụ đào tạo.",
  "model": "google_nmt",
  "from_community_srt": "nhưng một cho nơron này những gì bạn đã cho tôi là rác hoàn toàn \" Để nói rằng một chút toán học hơn những gì bạn làm là thêm lên các ô vuông của sự khác biệt giữa mỗi kích hoạt đầu ra thùng rác và giá trị mà bạn muốn chúng có và Đây là những gì chúng tôi gọi là chi phí của một ví dụ đào tạo duy nhất",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "Lưu ý rằng tổng này nhỏ khi mạng tự tin phân loại hình ảnh một cách chính xác, nhưng nó lớn khi mạng có vẻ như không biết mình đang làm gì.",
  "model": "google_nmt",
  "from_community_srt": "Lưu ý rằng số tiền này nhỏ khi mạng tự tin phân loại hình ảnh một cách chính xác Nhưng nó lớn khi mạng có vẻ như nó không thực sự biết nó đang làm gì",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "Vì vậy, điều bạn làm là xem xét chi phí trung bình của tất cả hàng chục nghìn ví dụ đào tạo mà bạn có thể tùy ý sử dụng.",
  "model": "google_nmt",
  "from_community_srt": "Vì vậy, sau đó những gì bạn làm là xem xét chi phí trung bình trên tất cả hàng chục ngàn ví dụ đào tạo theo ý của bạn",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "Chi phí trung bình này là thước đo của chúng tôi để đánh giá mức độ tệ hại của mạng và mức độ tồi tệ của máy tính.",
  "model": "google_nmt",
  "from_community_srt": "Chi phí trung bình này là thước đo của chúng tôi về mức độ tệ hại của mạng và mức độ cảm nhận của máy tính,",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "Và đó là một điều phức tạp.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "Hãy nhớ rằng bản thân mạng về cơ bản là một hàm, một hàm lấy 784 số làm đầu vào, giá trị pixel và đưa ra 10 số làm đầu ra và theo một nghĩa nào đó, nó được tham số hóa bởi tất cả các trọng số và độ lệch này?",
  "model": "google_nmt",
  "from_community_srt": "và đó là một điều phức tạp Hãy nhớ rằng bản thân mạng cơ bản là một chức năng mà 784 con số là đầu vào giá trị pixel và spits ra mười con số như đầu ra của nó và trong một ý nghĩa",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "Vâng, hàm chi phí là một lớp phức tạp trên đó.",
  "model": "google_nmt",
  "from_community_srt": "Nó được tham số hóa bởi tất cả các trọng số và thành kiến ​​này Trong khi hàm chi phí là một lớp phức tạp trên đầu trang mà nó lấy làm đầu vào của nó",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "Nó lấy khoảng 13.000 trọng số và độ lệch làm đầu vào, đồng thời đưa ra một con số duy nhất mô tả mức độ nghiêm trọng của các trọng số và độ lệch đó, đồng thời cách xác định nó phụ thuộc vào hành vi của mạng đối với hàng chục nghìn phần dữ liệu huấn luyện.",
  "model": "google_nmt",
  "from_community_srt": "mười ba nghìn trọng lượng và thành kiến ​​đó và nó phun ra một con số duy nhất mô tả những trọng số và thành kiến ​​xấu như thế nào và Cách nó được xác định phụ thuộc vào hành vi của mạng trên tất cả hàng chục nghìn mẩu dữ liệu đào tạo",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "Đó là rất nhiều điều để suy nghĩ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "Nhưng chỉ nói cho máy tính biết nó đang làm công việc tồi tệ như thế nào thì không hữu ích lắm.",
  "model": "google_nmt",
  "from_community_srt": "Đó là rất nhiều suy nghĩ về Nhưng chỉ cần nói với máy tính những gì một công việc crappy,",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "Bạn muốn nói với nó cách thay đổi những trọng số và thành kiến đó để nó trở nên tốt hơn.",
  "model": "google_nmt",
  "from_community_srt": "nó đang làm không phải là rất hữu ích Bạn muốn nói với nó như thế nào để thay đổi những trọng số và thiên vị để nó được tốt hơn?",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "Để dễ dàng hơn, thay vì cố gắng tưởng tượng một hàm có 13.000 đầu vào, hãy tưởng tượng một hàm đơn giản có một số làm đầu vào và một số làm đầu ra.",
  "model": "google_nmt",
  "from_community_srt": "Để làm cho nó dễ dàng hơn là đấu tranh để tưởng tượng một chức năng với 13.000 đầu vào Chỉ cần hình dung một hàm đơn giản có một số làm đầu vào và một số làm đầu ra",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "Làm thế nào để bạn tìm thấy đầu vào giảm thiểu giá trị của hàm này?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "Sinh viên giải tích sẽ biết rằng đôi khi bạn có thể tính ra mức tối thiểu đó một cách rõ ràng, nhưng điều đó không phải lúc nào cũng khả thi đối với các hàm thực sự phức tạp, chắc chắn không phải trong phiên bản 13.000 đầu vào của tình huống này đối với hàm chi phí mạng thần kinh cực kỳ phức tạp của chúng ta.",
  "model": "google_nmt",
  "from_community_srt": "Làm thế nào để bạn tìm thấy một đầu vào giảm thiểu giá trị của chức năng này? Sinh viên Calculus sẽ biết rằng đôi khi bạn có thể tìm ra rằng tối thiểu một cách rõ ràng Nhưng điều đó không phải lúc nào cũng khả thi đối với các chức năng thực sự phức tạp Chắc chắn không phải trong mười ba nghìn phiên bản đầu vào của tình huống này cho chức năng mạng thần kinh phức tạp điên rồ của chúng ta",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "Một chiến thuật linh hoạt hơn là bắt đầu ở bất kỳ đầu vào nào và tìm ra hướng bạn nên bước để làm cho đầu ra đó thấp hơn.",
  "model": "google_nmt",
  "from_community_srt": "Một chiến thuật linh hoạt hơn là bắt đầu ở bất kỳ đầu vào cũ nào và tìm ra hướng mà bạn nên thực hiện để làm cho đầu ra đó thấp hơn",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "Cụ thể, nếu bạn có thể tìm ra độ dốc của hàm số hiện tại, thì hãy dịch sang trái nếu độ dốc đó là dương và dịch chuyển đầu vào sang phải nếu độ dốc đó là âm.",
  "model": "google_nmt",
  "from_community_srt": "Cụ thể nếu bạn có thể tìm ra độ dốc của hàm bạn đang ở đâu Sau đó chuyển sang bên trái nếu độ dốc đó là dương và dịch chuyển đầu vào sang phải nếu độ dốc đó là âm",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "Nếu bạn làm điều này nhiều lần, tại mỗi điểm kiểm tra hệ số góc mới và thực hiện bước thích hợp, bạn sẽ tiến tới điểm cực tiểu cục bộ nào đó của hàm.",
  "model": "google_nmt",
  "from_community_srt": "Nếu bạn làm điều này nhiều lần tại mỗi điểm kiểm tra độ dốc mới và thực hiện bước thích hợp",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "Hình ảnh mà bạn có thể nghĩ đến ở đây là một quả bóng lăn xuống một ngọn đồi.",
  "model": "google_nmt",
  "from_community_srt": "bạn sẽ tiếp cận một số hàm địa phương tối thiểu và hình ảnh bạn có thể có trong tâm trí ở đây là một quả bóng lăn xuống một ngọn đồi và",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "Lưu ý, ngay cả đối với hàm đầu vào đơn thực sự được đơn giản hóa này, có rất nhiều điểm có thể xảy ra mà bạn có thể rơi vào, tùy thuộc vào đầu vào ngẫu nhiên nào bạn bắt đầu và không có gì đảm bảo rằng mức tối thiểu cục bộ mà bạn đạt được sẽ là giá trị nhỏ nhất có thể của hàm chi phí.",
  "model": "google_nmt",
  "from_community_srt": "Lưu ý ngay cả đối với chức năng nhập đơn lẻ thực sự đơn giản này, có nhiều thung lũng có thể bạn có thể Tùy thuộc vào đầu vào ngẫu nhiên nào bạn bắt đầu và không đảm bảo rằng mức tối thiểu địa phương",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "Điều đó cũng sẽ được chuyển sang trường hợp mạng lưới thần kinh của chúng ta.",
  "model": "google_nmt",
  "from_community_srt": "Bạn hạ cánh sẽ là giá trị nhỏ nhất có thể của hàm chi phí Điều đó cũng sẽ chuyển sang trường hợp mạng thần kinh của chúng tôi,",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "Tôi cũng muốn bạn lưu ý rằng nếu bạn làm cho kích thước bước của mình tỷ lệ thuận với độ dốc, thì khi độ dốc giảm dần về mức tối thiểu, các bước của bạn sẽ ngày càng nhỏ hơn và điều đó giúp bạn không bị vượt quá.",
  "model": "google_nmt",
  "from_community_srt": "và tôi cũng muốn bạn chú ý Làm thế nào nếu bạn thực hiện kích thước bước của bạn tỷ lệ thuận với độ dốc Sau đó, khi độ dốc được làm phẳng về phía mức tối thiểu,",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "Tăng độ phức tạp lên một chút, thay vào đó hãy tưởng tượng một hàm có hai đầu vào và một đầu ra.",
  "model": "google_nmt",
  "from_community_srt": "các bước của bạn càng nhỏ hơn và loại đó giúp bạn vượt quá mức Bumping lên sự phức tạp một chút tưởng tượng thay vì một chức năng với hai yếu tố đầu vào và một đầu ra",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "Bạn có thể coi không gian đầu vào là mặt phẳng xy và hàm chi phí được biểu đồ dưới dạng một bề mặt phía trên nó.",
  "model": "google_nmt",
  "from_community_srt": "Bạn có thể nghĩ về không gian đầu vào như mặt phẳng XY và hàm chi phí như được vẽ đồ thị như một bề mặt phía trên nó",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "Thay vì hỏi về độ dốc của hàm số, bạn phải hỏi mình nên bước vào không gian đầu vào này theo hướng nào để giảm đầu ra của hàm nhanh nhất.",
  "model": "google_nmt",
  "from_community_srt": "Bây giờ thay vì hỏi về độ dốc của hàm bạn phải hỏi bạn nên bước theo hướng nào trong không gian đầu vào này? Vì vậy, để giảm đầu ra của chức năng nhanh nhất trong các từ khác.",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "Nói cách khác, hướng xuống dốc là gì?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "Một lần nữa, thật hữu ích khi nghĩ đến một quả bóng lăn xuống ngọn đồi đó.",
  "model": "google_nmt",
  "from_community_srt": "Hướng xuống dốc là gì? Và một lần nữa sẽ rất hữu ích khi nghĩ đến một quả bóng lăn xuống ngọn đồi đó",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "Những ai quen thuộc với phép tính nhiều biến sẽ biết rằng gradient của hàm số cho bạn hướng đi lên dốc nhất, bạn nên bước theo hướng nào để tăng hàm số nhanh nhất.",
  "model": "google_nmt",
  "from_community_srt": "Những người bạn quen thuộc với tính toán đa biến sẽ biết rằng độ dốc của một hàm cho bạn hướng đi lên dốc nhất Về cơ bản,",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "Đương nhiên, việc lấy giá trị âm của gradient đó sẽ cho bạn hướng bước để giảm hàm số nhanh nhất.",
  "model": "google_nmt",
  "from_community_srt": "bạn nên bước hướng nào để tăng chức năng nhanh nhất đủ tự nhiên lấy âm của gradient đó cho bạn hướng đến bước làm giảm chức năng nhanh nhất và",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "Hơn thế nữa, độ dài của vectơ gradient này là một dấu hiệu cho biết độ dốc lớn nhất đó là bao nhiêu.",
  "model": "google_nmt",
  "from_community_srt": "Thậm chí nhiều hơn chiều dài của vector gradient này thực sự là một dấu hiệu cho thấy độ dốc dốc lớn nhất là bao nhiêu",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "Nếu bạn chưa quen với phép tính đa biến và muốn tìm hiểu thêm, hãy xem một số công việc tôi đã làm cho Khan Academy về chủ đề này.",
  "model": "google_nmt",
  "from_community_srt": "Bây giờ nếu bạn không quen với phép tính đa biến Và bạn muốn tìm hiểu thêm về một số công việc mà tôi đã làm cho Khan Academy về chủ đề này",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "Thành thật mà nói, tất cả những gì quan trọng đối với bạn và tôi lúc này là về nguyên tắc tồn tại một cách để tính vectơ này, vectơ này cho bạn biết hướng xuống dốc là gì và độ dốc của nó.",
  "model": "google_nmt",
  "from_community_srt": "Thành thật mà nói, mặc dù tất cả những gì quan trọng đối với bạn và tôi ngay bây giờ Về nguyên tắc có tồn tại một cách để tính toán vectơ này không.",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "Sẽ ổn thôi nếu đó là tất cả những gì bạn biết và bạn không nắm chắc các chi tiết.",
  "model": "google_nmt",
  "from_community_srt": "Vector này cho bạn biết Hướng xuống dốc và độ dốc bạn sẽ ổn nếu đó là tất cả những gì bạn biết và bạn không chắc chắn về chi tiết",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "Nếu bạn có thể hiểu được điều đó, thì thuật toán để thu nhỏ hàm số là tính hướng gradient này, sau đó thực hiện một bước nhỏ xuống dốc và lặp đi lặp lại điều đó.",
  "model": "google_nmt",
  "from_community_srt": "bởi vì nếu bạn có thể nhận được rằng thuật toán từ việc giảm thiểu hàm là tính toán hướng gradient sau đó thực hiện một bước nhỏ xuống dốc và",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "Đó là ý tưởng cơ bản tương tự đối với một hàm có 13.000 đầu vào thay vì 2 đầu vào.",
  "model": "google_nmt",
  "from_community_srt": "Chỉ cần lặp đi lặp lại điều đó nhiều lần Đó là ý tưởng cơ bản tương tự cho một hàm có 13.000 đầu vào thay vì hai đầu vào tưởng tượng tổ chức tất cả",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "Hãy tưởng tượng sắp xếp tất cả 13.000 trọng số và độ lệch của mạng của chúng ta thành một vectơ cột khổng lồ.",
  "model": "google_nmt",
  "from_community_srt": "13.000 trọng lượng và thành kiến ​​của mạng lưới của chúng tôi thành một vector cột khổng lồ",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "Độ dốc âm của hàm chi phí chỉ là một vectơ, đó là một số hướng bên trong không gian đầu vào cực kỳ lớn này cho bạn biết lực đẩy nào đối với tất cả các số đó sẽ khiến hàm chi phí giảm nhanh nhất.",
  "model": "google_nmt",
  "from_community_srt": "Độ dốc âm của hàm chi phí chỉ là một vectơ Đó là một số hướng bên trong không gian đầu vào cực kỳ lớn này cho bạn biết nudges cho tất cả những con số này sẽ làm giảm nhanh nhất chức năng chi phí và tất nhiên với chức năng chi phí được thiết kế đặc biệt của chúng tôi",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "Và tất nhiên, với hàm chi phí được thiết kế đặc biệt của chúng tôi, việc thay đổi trọng số và độ lệch để giảm có nghĩa là làm cho đầu ra của mạng trên mỗi phần dữ liệu huấn luyện trông không giống một mảng ngẫu nhiên gồm 10 giá trị mà giống một quyết định thực tế mà chúng ta muốn hơn nó để làm.",
  "model": "google_nmt",
  "from_community_srt": "Thay đổi trọng số và thành kiến ​​để giảm nó có nghĩa là làm cho đầu ra của mạng trên mỗi phần dữ liệu huấn luyện Nhìn giống như một mảng ngẫu nhiên gồm mười giá trị và giống như một quyết định thực tế mà chúng tôi muốn nó thực hiện",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "Điều quan trọng cần nhớ là hàm chi phí này bao gồm giá trị trung bình trên tất cả dữ liệu huấn luyện, vì vậy nếu bạn giảm thiểu nó, điều đó có nghĩa là nó có hiệu suất tốt hơn trên tất cả các mẫu đó.",
  "model": "google_nmt",
  "from_community_srt": "Điều quan trọng cần nhớ là hàm chi phí này liên quan đến mức trung bình trên tất cả dữ liệu đào tạo Vì vậy,",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "Thuật toán để tính toán độ dốc này một cách hiệu quả, vốn là trung tâm của cách mạng nơ-ron học hỏi, được gọi là lan truyền ngược và đó là điều tôi sẽ nói trong video tiếp theo.",
  "model": "google_nmt",
  "from_community_srt": "nếu bạn giảm thiểu nó có nghĩa là nó là một hiệu suất tốt hơn trên tất cả những mẫu Thuật toán để tính toán độ dốc này một cách hiệu quả, đó chính là tâm điểm của cách mà một mạng nơron học được gọi là truyền lại Và đó là những gì tôi sẽ nói về video tiếp theo",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "Ở đó, tôi thực sự muốn dành thời gian để tìm hiểu chính xác điều gì sẽ xảy ra với từng trọng lượng và độ lệch đối với một phần dữ liệu huấn luyện nhất định, cố gắng mang lại cảm giác trực quan về những gì đang xảy ra ngoài đống phép tính và công thức có liên quan.",
  "model": "google_nmt",
  "from_community_srt": "Ở đó tôi thực sự muốn dành thời gian để đi qua Chính xác điều gì sẽ xảy ra đối với mỗi trọng lượng và từng thiên vị đối với một đoạn dữ liệu đào tạo nhất định? Cố gắng tạo cảm giác trực quan cho những gì đang diễn ra ngoài đống tích phân và công thức có liên quan",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "Ngay tại đây, ngay bây giờ, điều chính mà tôi muốn bạn biết, không phụ thuộc vào chi tiết triển khai, đó là điều chúng tôi muốn nói khi nói về việc học mạng là nó chỉ giảm thiểu hàm chi phí.",
  "model": "google_nmt",
  "from_community_srt": "Ngay tại đây ngay bây giờ là điều chính. Tôi muốn bạn biết độc lập về chi tiết triển khai là những gì chúng tôi muốn nói khi chúng ta nói về việc học mạng là nó chỉ giảm thiểu chức năng chi phí và",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "Và lưu ý, một hệ quả của điều đó là điều quan trọng là hàm chi phí này phải có đầu ra mượt mà, sao cho chúng ta có thể tìm được mức tối thiểu cục bộ bằng cách thực hiện từng bước nhỏ xuống dốc.",
  "model": "google_nmt",
  "from_community_srt": "Lưu ý một hệ quả của điều đó là nó quan trọng đối với hàm chi phí này để có đầu ra mượt mà Để chúng tôi có thể tìm thấy mức tối thiểu địa phương bằng cách thực hiện các bước nhỏ xuống dốc",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "Nhân tiện, đây là lý do tại sao các nơ-ron nhân tạo có các mức kích hoạt liên tục, thay vì chỉ đơn giản là hoạt động hoặc không hoạt động theo cách nhị phân như các nơ-ron sinh học.",
  "model": "google_nmt",
  "from_community_srt": "Đây là lý do tại sao bằng cách này Tế bào thần kinh nhân tạo liên tục kích hoạt khác nhau hơn là chỉ hoạt động hoặc không hoạt động theo cách nhị phân",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "Quá trình liên tục dịch chuyển đầu vào của một hàm theo bội số của gradient âm được gọi là giảm gradient.",
  "model": "google_nmt",
  "from_community_srt": "nếu cách mà các tế bào thần kinh sinh học Quá trình này liên tục nudging một đầu vào của một hàm bởi một số bội số của gradient âm được gọi là gradient gốc",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "Đó là một cách để hội tụ về một hàm chi phí tối thiểu cục bộ nào đó, về cơ bản là một thung lũng trong biểu đồ này.",
  "model": "google_nmt",
  "from_community_srt": "Đó là một cách để hội tụ hướng tới một số hàm tối thiểu địa phương của hàm chi phí về cơ bản là một thung lũng trong biểu đồ này",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "Tất nhiên, tôi vẫn đang hiển thị hình ảnh của một hàm có hai đầu vào, bởi vì những cú huých trong không gian đầu vào 13.000 chiều hơi khó để bạn hiểu được, nhưng có một cách hay không phải không gian để nghĩ về điều này.",
  "model": "google_nmt",
  "from_community_srt": "Tôi vẫn hiển thị hình ảnh của một hàm với hai yếu tố đầu vào của khóa học bởi vì nudges trong một đầu vào mười ba nghìn chiều Không gian có một chút khó khăn để bao quanh tâm trí bạn,",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "Mỗi thành phần của gradient âm cho chúng ta biết hai điều.",
  "model": "google_nmt",
  "from_community_srt": "nhưng thực sự có một cách không gian không gian đẹp để suy nghĩ về điều này Mỗi thành phần của gradient âm cho chúng ta biết hai điều dấu hiệu của khóa học cho chúng ta biết liệu tương ứng",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "Tất nhiên, dấu hiệu cho chúng ta biết thành phần tương ứng của vectơ đầu vào nên được nâng lên hay hạ xuống.",
  "model": "google_nmt",
  "from_community_srt": "Thành phần của vector đầu vào phải được đẩy lên hoặc xuống,",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "Nhưng quan trọng là, mức độ tương đối của tất cả các thành phần này sẽ cho bạn biết những thay đổi nào quan trọng hơn.",
  "model": "google_nmt",
  "from_community_srt": "nhưng quan trọng là độ lớn tương đối của tất cả các thành phần này Loại cho bạn biết những thay đổi nào quan trọng hơn",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "Bạn thấy đấy, trong mạng lưới của chúng tôi, việc điều chỉnh một trong các trọng số có thể có tác động lớn hơn nhiều đến hàm chi phí so với việc điều chỉnh một số trọng số khác.",
  "model": "google_nmt",
  "from_community_srt": "Bạn thấy trong mạng của chúng tôi, việc điều chỉnh một trong các trọng số có thể lớn hơn nhiều tác động đến chức năng chi phí so với việc điều chỉnh trọng lượng khác",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "Một số kết nối này quan trọng hơn đối với dữ liệu đào tạo của chúng tôi.",
  "model": "google_nmt",
  "from_community_srt": "Một số kết nối này chỉ quan trọng hơn đối với dữ liệu đào tạo của chúng tôi Vì vậy,",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "Vì vậy, bạn có thể nghĩ về vectơ gradient này của hàm chi phí khổng lồ đáng kinh ngạc của chúng ta là nó mã hóa tầm quan trọng tương đối của từng trọng số và độ lệch, nghĩa là, những thay đổi nào trong số này sẽ mang lại nhiều lợi ích nhất cho bạn.",
  "model": "google_nmt",
  "from_community_srt": "một cách mà bạn có thể suy nghĩ về vector gradient của tâm trí của chúng tôi-cong vênh chức năng chi phí lớn là nó mã hóa tầm quan trọng tương đối của mỗi trọng lượng và độ lệch Đó là thay đổi nào trong số những thay đổi này sẽ mang lại nhiều lợi nhuận nhất cho bạn Đây thực sự chỉ là một cách nghĩ khác về hướng",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "Đây thực sự chỉ là một cách suy nghĩ khác về phương hướng.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "Lấy một ví dụ đơn giản hơn, nếu bạn có một hàm nào đó với hai biến làm đầu vào và bạn tính toán rằng độ dốc của nó tại một điểm cụ thể nào đó sẽ là 3,1, thì một mặt bạn có thể hiểu điều đó là nói rằng khi bạn &#39; đang đứng ở đầu vào đó, di chuyển dọc theo hướng này sẽ làm tăng hàm số nhanh nhất, khi bạn vẽ đồ thị hàm số phía trên mặt phẳng của các điểm đầu vào, vectơ đó là thứ mang lại cho bạn hướng đi thẳng lên trên.",
  "model": "google_nmt",
  "from_community_srt": "Để có một ví dụ đơn giản hơn nếu bạn có một số hàm với hai biến làm đầu vào và bạn Tính toán rằng độ dốc của nó tại một số điểm cụ thể xuất hiện dưới dạng (3,1) Sau đó, một mặt bạn có thể giải thích điều đó khi nói rằng khi bạn đang đứng ở đầu vào đó di chuyển theo hướng này làm tăng chức năng nhanh nhất Khi bạn vẽ đồ thị hàm trên mặt phẳng của điểm đầu vào mà vector là thứ cho bạn hướng đi lên trên thẳng",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "Nhưng một cách khác để đọc điều đó là nói rằng những thay đổi đối với biến đầu tiên này có tầm quan trọng gấp 3 lần so với những thay đổi đối với biến thứ hai, ít nhất là trong vùng lân cận của đầu vào có liên quan, việc thúc đẩy giá trị x mang lại nhiều lợi ích hơn cho bạn. Cái xô.",
  "model": "google_nmt",
  "from_community_srt": "Nhưng một cách khác để đọc đó là nói rằng những thay đổi đối với biến đầu tiên này Có ba lần tầm quan trọng như những thay đổi đối với biến thứ hai mà ít nhất là trong vùng lân cận của đầu vào có liên quan",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "Hãy thu nhỏ và tóm tắt vị trí của chúng ta cho đến nay.",
  "model": "google_nmt",
  "from_community_srt": "Nudging giá trị x mang nhiều bang hơn cho buck của bạn Được rồi Hãy thu nhỏ và tổng hợp vị trí của chúng ta cho đến nay chính mạng là chức năng này",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "Bản thân mạng là chức năng này với 784 đầu vào và 10 đầu ra, được xác định theo tất cả các tổng trọng số này.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "Trên hết, hàm chi phí là một lớp phức tạp.",
  "model": "google_nmt",
  "from_community_srt": "784 đầu vào và 10 đầu ra được xác định theo tất cả các khoản tiền có trọng số này hàm chi phí là một lớp phức tạp trên đầu trang",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "Nó lấy 13.000 trọng số và độ lệch làm đầu vào và đưa ra một thước đo duy nhất về mức độ tệ hại dựa trên các ví dụ đào tạo.",
  "model": "google_nmt",
  "from_community_srt": "13.000 trọng lượng và thành kiến ​​như đầu vào và phun ra một thước đo duy nhất về sự tệ hại dựa trên các ví dụ đào tạo và",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "Và độ dốc của hàm chi phí vẫn còn một lớp phức tạp nữa.",
  "model": "google_nmt",
  "from_community_srt": "Độ dốc của hàm chi phí là một lớp phức tạp hơn,",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "Nó cho chúng ta biết điều gì tác động đến tất cả các trọng số và độ lệch này gây ra sự thay đổi nhanh nhất đối với giá trị của hàm chi phí, mà bạn có thể hiểu là cho biết những thay đổi nào đối với trọng số nào là quan trọng nhất.",
  "model": "google_nmt",
  "from_community_srt": "nó vẫn cho chúng ta biết Nudges cho tất cả các trọng số và thiên vị gây ra sự thay đổi nhanh nhất cho giá trị của hàm chi phí",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "Vì vậy, khi bạn khởi tạo mạng với các trọng số và độ lệch ngẫu nhiên, đồng thời điều chỉnh chúng nhiều lần dựa trên quy trình giảm độ dốc này, mạng thực sự hoạt động tốt như thế nào trên các hình ảnh mà nó chưa từng thấy trước đây?",
  "model": "google_nmt",
  "from_community_srt": "Điều bạn có thể giải thích là nói những thay đổi nào đối với trọng số nào quan trọng nhất Vì vậy, khi bạn khởi tạo mạng với các trọng số và độ lệch ngẫu nhiên và điều chỉnh chúng nhiều lần dựa trên quá trình tô đậm này Nó thực sự hoạt động tốt như thế nào trên những hình ảnh chưa từng thấy trước đây? Vâng,",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "Cái mà tôi đã mô tả ở đây, với hai lớp ẩn, mỗi lớp gồm 16 nơ-ron, được chọn chủ yếu vì lý do thẩm mỹ, không tệ, phân loại chính xác khoảng 96% hình ảnh mới mà nó nhìn thấy.",
  "model": "google_nmt",
  "from_community_srt": "cái mà tôi đã mô tả ở đây với hai lớp tế bào thần kinh mười sáu ẩn được lựa chọn chủ yếu vì lý do thẩm mỹ tốt, nó không tệ, nó phân loại khoảng 96% hình ảnh mới mà nó thấy chính xác và Thành thật mà nói,",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "Và thành thật mà nói, nếu bạn nhìn vào một số ví dụ mà nó gây nhầm lẫn, bạn cảm thấy buộc phải cắt giảm nó một chút.",
  "model": "google_nmt",
  "from_community_srt": "nếu bạn nhìn vào một số ví dụ mà nó messes lên trên bạn loại cảm thấy bắt buộc phải cắt nó một chút slack",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "Bây giờ nếu bạn thử nghiệm với cấu trúc lớp ẩn và thực hiện một vài chỉnh sửa, bạn có thể đạt được tỷ lệ này lên tới 98%.",
  "model": "google_nmt",
  "from_community_srt": "Bây giờ nếu bạn chơi xung quanh với cấu trúc lớp ẩn và thực hiện một vài chỉnh sửa Bạn có thể nhận được điều này lên đến 98% và điều đó khá tốt.",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "Và điều đó khá tốt!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "Nó không phải là tốt nhất, bạn chắc chắn có thể có được hiệu suất tốt hơn bằng cách phức tạp hơn mạng vanilla đơn giản này, nhưng với nhiệm vụ ban đầu khó khăn như thế nào, tôi nghĩ có điều gì đó đáng kinh ngạc về bất kỳ mạng nào hoạt động tốt như vậy trên các hình ảnh mà nó chưa từng thấy trước đây, vì điều đó chúng tôi chưa bao giờ nói cụ thể với nó những mẫu cần tìm.",
  "model": "google_nmt",
  "from_community_srt": "Nó không phải là tốt nhất Bạn chắc chắn có thể có được hiệu suất tốt hơn bằng cách tinh vi hơn mạng lưới vanilla thuần túy này Nhưng với cách làm nản chí nhiệm vụ ban đầu, tôi chỉ nghĩ có điều gì đó? Không thể tin được về bất kỳ mạng nào hoạt động tốt trên những hình ảnh mà nó chưa từng thấy trước đây",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "Ban đầu, cách tôi thúc đẩy cấu trúc này là bằng cách mô tả niềm hy vọng mà chúng ta có thể có, rằng lớp thứ hai có thể thu được các cạnh nhỏ, lớp thứ ba sẽ ghép các cạnh đó lại với nhau để nhận ra các vòng lặp và các đường dài hơn, và rằng chúng có thể được ghép lại với nhau. cùng nhau nhận biết chữ số.",
  "model": "google_nmt",
  "from_community_srt": "Cho rằng chúng tôi không bao giờ cụ thể nói với nó những gì mô hình để tìm kiếm Ban đầu cách mà tôi thúc đẩy cấu trúc này là bằng cách mô tả một hy vọng rằng chúng ta có thể có Đó là lớp thứ hai có thể nhặt trên các cạnh nhỏ Rằng lớp thứ ba sẽ ghép các cạnh đó để nhận ra các vòng lặp và các đường dài hơn và các đường đó có thể được ghép lại với nhau để nhận ra các chữ số",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "Vậy đây có phải là điều mà mạng lưới của chúng ta thực sự đang làm?",
  "model": "google_nmt",
  "from_community_srt": "Vậy đây có phải là điều mà mạng của chúng tôi thực sự đang thực hiện không?",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "Vâng, ít nhất là đối với điều này thì không hề.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "Hãy nhớ video trước chúng ta đã xem xét trọng số của các kết nối từ tất cả các nơ-ron ở lớp đầu tiên đến một nơ-ron nhất định ở lớp thứ hai có thể được hình dung như thế nào dưới dạng một mẫu pixel nhất định mà nơ-ron lớp thứ hai đang tiếp nhận?",
  "model": "google_nmt",
  "from_community_srt": "Vâng cho điều này ít nhất Không có gì hãy nhớ xem video cuối cùng chúng tôi đã xem xét các trọng số của Kết nối từ tất cả các nơron trong lớp đầu tiên đến một nơron đã cho trong lớp thứ hai Có thể được hiển thị dưới dạng mẫu pixel nhất định mà nơron lớp thứ hai đang chọn",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "Chà, khi chúng tôi thực sự làm điều đó đối với các trọng số liên quan đến những chuyển đổi này, từ lớp đầu tiên sang lớp tiếp theo, thay vì chọn các cạnh nhỏ biệt lập ở đây và ở đó, chúng trông gần như ngẫu nhiên, chỉ với một số mẫu rất lỏng lẻo trong chính giữa đó.",
  "model": "google_nmt",
  "from_community_srt": "Vâng, khi chúng tôi thực sự làm điều đó cho các trọng số liên quan đến các chuyển tiếp này từ lớp đầu tiên đến lớp tiếp theo Thay vì chọn lên các cạnh nhỏ bị cô lập ở đây và ở đó. Họ nhìn gần như ngẫu nhiên Chỉ cần đặt một số mô hình rất lỏng lẻo ở giữa có vẻ như là trong không thể tưởng tượng lớn",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "Có vẻ như trong không gian rộng lớn không thể đo lường được 13.000 chiều với các trọng số và độ lệch có thể xảy ra, mạng của chúng tôi đã tìm thấy một mức tối thiểu cục bộ nhỏ đáng mừng, mặc dù đã phân loại thành công hầu hết các hình ảnh, nhưng không chọn chính xác các mẫu mà chúng tôi có thể mong đợi.",
  "model": "google_nmt",
  "from_community_srt": "13.000 chiều không gian có thể có trọng lượng và thiên vị của mạng lưới của chúng tôi nhận thấy chính nó là một địa phương ít hài lòng nhất mặc dù phân loại thành công hầu hết các hình ảnh không chính xác nhận các mẫu mà chúng tôi có thể hy vọng và",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "Và để thực sự hiểu được điểm này, hãy xem điều gì xảy ra khi bạn nhập một hình ảnh ngẫu nhiên.",
  "model": "google_nmt",
  "from_community_srt": "Để thực sự thúc đẩy việc xem trang chủ này, điều gì sẽ xảy ra khi bạn nhập một hình ảnh ngẫu nhiên nếu hệ thống thông minh,",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "Nếu hệ thống thông minh, bạn có thể cho rằng nó sẽ có cảm giác không chắc chắn, có thể không thực sự kích hoạt bất kỳ nơ-ron đầu ra nào trong số 10 nơ-ron đầu ra đó hoặc kích hoạt tất cả chúng một cách đồng đều, nhưng thay vào đó, nó tự tin đưa ra cho bạn một số câu trả lời vô nghĩa, như thể nó cảm thấy chắc chắn rằng tiếng ồn ngẫu nhiên này là số 5 vì nó cho thấy hình ảnh thực của số 5 là số 5.",
  "model": "google_nmt",
  "from_community_srt": "bạn có thể mong đợi nó hoặc cảm thấy không chắc chắn có thể không thực sự kích hoạt bất kỳ 10 nơron đầu ra nào hoặc Kích hoạt tất cả đều Nhưng thay vào đó Tự tin cung cấp cho bạn một số câu trả lời vô nghĩa như thể nó cảm thấy chắc chắn rằng tiếng ồn ngẫu nhiên này là 5 vì nó thực sự là một",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "Nói theo cách khác, ngay cả khi mạng này có thể nhận dạng các chữ số khá tốt, nó cũng không biết cách vẽ chúng.",
  "model": "google_nmt",
  "from_community_srt": "hình ảnh của 5 là 5 cụm từ khác nhau ngay cả khi mạng này có thể nhận ra chữ số khá tốt,",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "Phần lớn điều này là do đây là một thiết lập đào tạo bị ràng buộc chặt chẽ.",
  "model": "google_nmt",
  "from_community_srt": "nó không có ý tưởng làm thế nào để vẽ chúng Rất nhiều điều này là bởi vì đó là một thiết lập đào tạo hạn chế chặt chẽ",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "Ý tôi là, hãy đặt mình vào vị trí của mạng lưới ở đây.",
  "model": "google_nmt",
  "from_community_srt": "Ý tôi là đặt mình vào vị trí của mạng lưới ở đây từ quan điểm của nó,",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "Theo quan điểm của nó, toàn bộ vũ trụ không bao gồm gì ngoài những chữ số bất động được xác định rõ ràng tập trung vào một mạng lưới nhỏ, và hàm chi phí của nó không bao giờ mang lại cho nó bất kỳ động lực nào để trở thành bất cứ thứ gì ngoại trừ sự hoàn toàn tin tưởng vào các quyết định của mình.",
  "model": "google_nmt",
  "from_community_srt": "toàn thể vũ trụ không có gì cả Nhưng đã xác định rõ ràng các chữ số chưa được canh giữa trong một lưới nhỏ và hàm chi phí của nó chưa bao giờ cho nó Khuyến khích là bất cứ điều gì, nhưng hoàn toàn tự tin trong quyết định của mình Vì vậy,",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "Vì vậy, với đây là hình ảnh về những gì các nơ-ron lớp thứ hai thực sự đang làm, bạn có thể thắc mắc tại sao tôi lại giới thiệu mạng lưới này với động cơ là tìm hiểu các cạnh và khuôn mẫu.",
  "model": "google_nmt",
  "from_community_srt": "nếu đây là hình ảnh của những gì các tế bào thần kinh lớp thứ hai đang thực sự làm Bạn có thể tự hỏi tại sao tôi sẽ giới thiệu mạng này với động cơ chọn lên các cạnh và các mẫu",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "Ý tôi là, đó hoàn toàn không phải là điều nó sẽ làm.",
  "model": "google_nmt",
  "from_community_srt": "Ý tôi là, đó không phải là tất cả những gì nó kết thúc Vâng,",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "Chà, đây không phải là mục tiêu cuối cùng của chúng tôi mà thay vào đó là điểm khởi đầu.",
  "model": "google_nmt",
  "from_community_srt": "đây không phải là mục tiêu cuối cùng của chúng tôi,",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "Thành thật mà nói, đây là công nghệ cũ, loại được nghiên cứu vào những năm 80 và 90, và bạn cần phải hiểu nó trước khi có thể hiểu các biến thể hiện đại chi tiết hơn và rõ ràng nó có khả năng giải quyết một số vấn đề thú vị, nhưng bạn càng đào sâu vào những gì những lớp ẩn đó thực sự đang hoạt động thì có vẻ như nó càng kém thông minh hơn.",
  "model": "google_nmt",
  "from_community_srt": "nhưng thay vào đó là một điểm khởi đầu thẳng thắn Đây là công nghệ cũ loại nghiên cứu trong những năm 80 và 90 và Bạn cần phải hiểu nó trước khi bạn có thể hiểu các biến thể hiện đại chi tiết hơn và rõ ràng là có khả năng giải quyết một số vấn đề thú vị",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "Chuyển trọng tâm trong giây lát từ cách mạng học sang cách bạn học, điều đó sẽ chỉ xảy ra nếu bạn tương tác tích cực với tài liệu ở đây bằng cách nào đó.",
  "model": "google_nmt",
  "from_community_srt": "Nhưng càng có nhiều bạn đào sâu vào những gì những lớp ẩn thực sự làm ít thông minh hơn Chuyển tiêu điểm trong giây lát từ cách mạng tìm hiểu cách bạn học",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "Một điều khá đơn giản mà tôi muốn bạn làm là tạm dừng ngay bây giờ và suy nghĩ sâu sắc một chút về những thay đổi bạn có thể thực hiện đối với hệ thống này và cách hệ thống cảm nhận hình ảnh nếu bạn muốn nó tiếp thu tốt hơn những thứ như cạnh và hoa văn.",
  "model": "google_nmt",
  "from_community_srt": "Điều đó sẽ chỉ xảy ra nếu bạn tham gia tích cực với tài liệu ở đây bằng cách nào đó Một điều khá đơn giản mà tôi muốn bạn làm là tạm dừng ngay bây giờ và suy nghĩ sâu sắc một chút về những gì Những thay đổi bạn có thể thực hiện đối với hệ thống này Và làm thế nào nó cảm nhận được hình ảnh nếu bạn muốn nó nhận được tốt hơn về những thứ như các cạnh và các mẫu?",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "Nhưng tốt hơn thế, để thực sự tương tác với tài liệu, tôi đặc biệt giới thiệu cuốn sách của Michael Nielsen về học sâu và mạng lưới thần kinh.",
  "model": "google_nmt",
  "from_community_srt": "Nhưng tốt hơn là để thực sự tương tác với vật liệu tôi Đề xuất cuốn sách của Michael Nielsen về mạng học tập và mạng thần kinh",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "Trong đó, bạn có thể tìm thấy mã và dữ liệu để tải xuống và sử dụng cho ví dụ chính xác này và cuốn sách sẽ hướng dẫn bạn từng bước về chức năng của mã đó.",
  "model": "google_nmt",
  "from_community_srt": "Trong đó, bạn có thể tìm thấy mã và dữ liệu để tải xuống và chơi với ví dụ chính xác này Và cuốn sách sẽ hướng dẫn bạn từng bước những gì mã đó đang làm",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "Điều tuyệt vời là cuốn sách này được cung cấp miễn phí và công khai, vì vậy nếu bạn nhận được điều gì đó từ cuốn sách, hãy cân nhắc tham gia cùng tôi để quyên góp cho những nỗ lực của Nielsen.",
  "model": "google_nmt",
  "from_community_srt": "Điều tuyệt vời là cuốn sách này miễn phí và có sẵn công khai Vì vậy, nếu bạn nhận được một cái gì đó ra khỏi nó xem xét tham gia với tôi trong việc đóng góp cho những nỗ lực của Nielsen",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "Tôi cũng đã liên kết một số tài nguyên khác mà tôi rất thích trong phần mô tả, bao gồm bài đăng blog hay và ấn tượng của Chris Ola và các bài viết trên Distill.",
  "model": "google_nmt",
  "from_community_srt": "Tôi cũng đã liên kết một vài tài nguyên khác mà tôi thích rất nhiều trong mô tả bao gồm bài đăng blog tuyệt vời và đẹp đẽ của Chris Ola và các bài viết trong chưng cất",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "Để kết thúc mọi chuyện ở đây trong vài phút vừa qua, tôi muốn quay lại một đoạn trong cuộc phỏng vấn tôi đã thực hiện với Leisha Lee.",
  "model": "google_nmt",
  "from_community_srt": "Để đóng mọi thứ ở đây trong vài phút cuối Tôi muốn trở lại một đoạn phỏng vấn mà tôi đã có với Leisha Lee",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "Bạn có thể nhớ đến cô ấy từ video trước, cô ấy đã làm luận án tiến sĩ về học sâu.",
  "model": "google_nmt",
  "from_community_srt": "Bạn có thể nhớ cô ấy từ video cuối cùng.",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "Trong đoạn trích nhỏ này, cô ấy nói về hai bài báo gần đây thực sự đi sâu vào cách một số mạng nhận dạng hình ảnh hiện đại hơn đang thực sự học hỏi.",
  "model": "google_nmt",
  "from_community_srt": "Cô đã làm công việc tiến sĩ của mình trong việc học sâu và trong đoạn trích nhỏ này Cô ấy nói về hai bài báo gần đây thực sự tìm hiểu cách một số mạng công nhận hình ảnh hiện đại hơn thực sự đang học",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "Để xác định vị trí của chúng tôi trong cuộc trò chuyện, bài báo đầu tiên đã sử dụng một trong những mạng lưới thần kinh đặc biệt sâu có khả năng nhận dạng hình ảnh thực sự tốt và thay vì huấn luyện nó trên một tập dữ liệu được dán nhãn chính xác, hãy xáo trộn tất cả các nhãn xung quanh trước khi huấn luyện.",
  "model": "google_nmt",
  "from_community_srt": "Chỉ để thiết lập nơi chúng ta đang ở trong cuộc trò chuyện, bài báo đầu tiên đã lấy một trong những mạng thần kinh đặc biệt sâu này Điều đó thực sự tốt khi nhận dạng hình ảnh và thay vì đào tạo nó trên một dữ liệu được dán nhãn đúng cách",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "Rõ ràng độ chính xác của thử nghiệm ở đây không tốt hơn ngẫu nhiên, vì mọi thứ chỉ được gắn nhãn ngẫu nhiên, nhưng nó vẫn có thể đạt được độ chính xác huấn luyện giống như bạn làm trên một tập dữ liệu được gắn nhãn chính xác.",
  "model": "google_nmt",
  "from_community_srt": "Đặt nó xáo trộn tất cả các nhãn xung quanh trước khi đào tạo Rõ ràng độ chính xác thử nghiệm ở đây sẽ không tốt hơn ngẫu nhiên vì mọi thứ chỉ được dán nhãn ngẫu nhiên Nhưng nó vẫn có thể đạt được độ chính xác đào tạo giống như bạn có trên một tập dữ liệu được dán nhãn đúng cách",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "Về cơ bản, hàng triệu trọng số cho mạng cụ thể này là đủ để nó chỉ ghi nhớ dữ liệu ngẫu nhiên, điều này đặt ra câu hỏi liệu việc giảm thiểu hàm chi phí này có thực sự tương ứng với bất kỳ loại cấu trúc nào trong hình ảnh hay chỉ là ghi nhớ?",
  "model": "google_nmt",
  "from_community_srt": "Về cơ bản, hàng triệu trọng lượng cho mạng cụ thể này là đủ để nó chỉ ghi nhớ dữ liệu ngẫu nhiên Loại câu hỏi đặt ra cho việc giảm thiểu chức năng chi phí này có thực sự tương ứng với bất kỳ loại cấu trúc nào trong hình ảnh không? Hay chỉ là bạn biết? ghi nhớ toàn bộ",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "Nếu bạn nhìn vào đường cong chính xác đó, nếu bạn chỉ đang đào tạo trên một tập dữ liệu ngẫu nhiên, thì đường cong đó sẽ đi xuống rất chậm theo kiểu gần như tuyến tính, vì vậy bạn thực sự đang gặp khó khăn để tìm ra mức tối thiểu cục bộ có thể có, bạn biết đấy , trọng lượng phù hợp sẽ giúp bạn có được độ chính xác đó.",
  "model": "google_nmt",
  "from_community_srt": "Tập dữ liệu về phân loại chính xác là gì và vì vậy một vài bạn biết nửa năm sau tại ICML năm nay Không có giấy giấy chính xác nào đề cập đến một số người được hỏi như này Trên thực tế, các mạng này đang hoạt động thông minh hơn một chút nếu bạn nhìn vào đường cong chính xác đó nếu bạn chỉ tập luyện Bộ dữ liệu ngẫu nhiên mà đường cong sắp xếp đi xuống rất bạn biết rất chậm trong hầu hết các loại thời trang tuyến tính Vì vậy, bạn đang thực sự đấu tranh để tìm thấy rằng minima địa phương có thể bạn biết đúng trọng lượng sẽ giúp bạn có được độ chính xác đó trong khi nếu bạn đang thực sự đào tạo trên bộ dữ liệu có cấu trúc có",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "Trong khi đó, nếu bạn thực sự đang đào tạo trên một tập dữ liệu có cấu trúc, một tập dữ liệu có nhãn phù hợp, bạn sẽ loay hoay một chút lúc đầu, nhưng sau đó bạn đã giảm rất nhanh để đạt được mức độ chính xác đó, và theo một nghĩa nào đó, nó việc tìm cực đại địa phương đó dễ dàng hơn.",
  "model": "google_nmt",
  "from_community_srt": "Nhãn phải. Bạn biết bạn fiddle xung quanh một chút trong đầu, nhưng sau đó bạn loại bỏ rất nhanh để có được điều đó Mức độ chính xác và vì vậy theo một nghĩa nào đó, việc tìm ra Địa phương tối đa và do đó,",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "Và điều thú vị ở đây là nó đưa ra ánh sáng một bài báo khác từ vài năm trước, trong đó có nhiều sự đơn giản hóa hơn về các lớp mạng, nhưng một trong những kết quả là nói rằng nếu bạn nhìn vào bối cảnh tối ưu hóa, mức tối thiểu cục bộ mà các mạng này có xu hướng tìm hiểu thực sự có chất lượng như nhau, vì vậy, theo một nghĩa nào đó, nếu tập dữ liệu của bạn có cấu trúc, bạn sẽ có thể tìm thấy dữ liệu đó dễ dàng hơn nhiều.",
  "model": "google_nmt",
  "from_community_srt": "nó cũng thú vị về điều đó là nó bị bắt đưa vào ánh sáng giấy khác từ thực tế một vài năm trước đây Trong đó có nhiều hơn đơn giản hóa về các lớp mạng Nhưng một trong những kết quả đã nói rằng nếu bạn nhìn vào cảnh quan tối ưu hóa, minima địa phương mà các mạng này có xu hướng tìm hiểu là Trên thực tế có chất lượng như nhau vì vậy trong một số ý nghĩa nếu tập dữ liệu của bạn là cấu trúc và bạn sẽ có thể tìm thấy dễ dàng hơn nhiều",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "Như mọi khi, tôi xin gửi lời cảm ơn tới những người đã ủng hộ trên Patreon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "Tôi đã từng nói Patreon là công cụ thay đổi cuộc chơi nhưng những video này thực sự sẽ không thể thực hiện được nếu không có bạn.",
  "model": "google_nmt",
  "from_community_srt": "Lời cảm ơn của tôi như mọi khi trong số các bạn ủng hộ trên patreon Tôi đã nói trước đó chỉ là những gì một cuộc đổi tên game là nhưng những đoạn video này thực sự sẽ không thể nếu không có bạn tôi",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "Tôi cũng muốn gửi lời cảm ơn đặc biệt đến công ty Amplify Partners của VC vì đã hỗ trợ những video đầu tiên trong chuỗi này.",
  "model": "google_nmt",
  "from_community_srt": "Cũng muốn đưa ra một đặc biệt. Cảm ơn các đối tác amplifi của VC trong việc hỗ trợ những video ban đầu này trong bộ phim",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]