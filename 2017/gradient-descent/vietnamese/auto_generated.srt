1
00:00:04,180 --> 00:00:07,280
Video cuối cùng tôi đã trình bày cấu trúc của mạng lưới thần kinh.

2
00:00:07,680 --> 00:00:10,140
Tôi sẽ tóm tắt nhanh ở đây để chúng ta dễ nhớ hơn 

3
00:00:10,140 --> 00:00:12,600
và sau đó tôi có hai mục tiêu chính cho video này.

4
00:00:13,100 --> 00:00:15,600
Đầu tiên là giới thiệu ý tưởng về gradient giảm dần, 

5
00:00:15,600 --> 00:00:19,232
không chỉ làm nền tảng cho cách mạng lưới thần kinh học mà còn làm cơ sở cho 

6
00:00:19,232 --> 00:00:20,600
nhiều hoạt động học máy khác.

7
00:00:21,120 --> 00:00:24,435
Sau đó, chúng ta sẽ tìm hiểu thêm một chút về cách thức hoạt động của 

8
00:00:24,435 --> 00:00:27,940
mạng cụ thể này và những lớp tế bào thần kinh ẩn đó đang tìm kiếm điều gì.

9
00:00:28,980 --> 00:00:33,649
Xin nhắc lại, mục tiêu của chúng ta ở đây là ví dụ cổ điển về nhận dạng chữ số viết tay, 

10
00:00:33,649 --> 00:00:36,220
chương trình Hello world của mạng lưới thần kinh.

11
00:00:37,020 --> 00:00:39,864
Các chữ số này được hiển thị trên lưới 28x28 pixel, 

12
00:00:39,864 --> 00:00:43,420
mỗi pixel có một số giá trị thang độ xám trong khoảng từ 0 đến 1.

13
00:00:43,820 --> 00:00:50,040
Đó là những gì quyết định sự kích hoạt của 784 nơ-ron trong lớp đầu vào của mạng.

14
00:00:51,180 --> 00:00:55,879
Và sau đó, việc kích hoạt mỗi nơ-ron trong các lớp sau dựa trên tổng trọng số 

15
00:00:55,879 --> 00:01:00,820
của tất cả các lần kích hoạt ở lớp trước, cộng với một số đặc biệt gọi là độ lệch.

16
00:01:02,160 --> 00:01:04,520
Sau đó, bạn tính tổng đó bằng một số hàm khác, 

17
00:01:04,520 --> 00:01:08,940
chẳng hạn như tính năng chia nhỏ sigmoid hoặc relu, như cách tôi đã xem qua video trước.

18
00:01:09,480 --> 00:01:14,815
Tổng cộng, với sự lựa chọn có phần tùy tiện của hai lớp ẩn với 16 nơ-ron mỗi lớp, 

19
00:01:14,815 --> 00:01:19,695
mạng có khoảng 13.000 trọng số và độ lệch mà chúng ta có thể điều chỉnh và 

20
00:01:19,695 --> 00:01:24,380
chính những giá trị này sẽ xác định chính xác những gì mạng thực sự làm.

21
00:01:24,880 --> 00:01:29,011
Vậy thì điều chúng ta muốn nói khi nói rằng mạng này phân loại một chữ số nhất 

22
00:01:29,011 --> 00:01:33,300
định là điểm sáng nhất trong số 10 nơ-ron ở lớp cuối cùng tương ứng với chữ số đó.

23
00:01:34,100 --> 00:01:38,869
Và hãy nhớ rằng, động lực mà chúng ta nghĩ đến ở đây đối với cấu trúc phân lớp là có 

24
00:01:38,869 --> 00:01:43,694
thể lớp thứ hai có thể xử lý các cạnh và lớp thứ ba có thể xử lý các mẫu như vòng lặp 

25
00:01:43,694 --> 00:01:48,631
và đường thẳng, và lớp cuối cùng có thể ghép những mẫu đó lại với nhau để nhận biết chữ 

26
00:01:48,631 --> 00:01:48,800
số.

27
00:01:49,800 --> 00:01:52,240
Vậy ở đây chúng ta tìm hiểu cách thức mạng thần kinh học.

28
00:01:52,640 --> 00:01:56,941
Điều chúng ta muốn là một thuật toán mà bạn có thể hiển thị cho mạng này toàn 

29
00:01:56,941 --> 00:02:01,903
bộ dữ liệu huấn luyện, dưới dạng một loạt các hình ảnh khác nhau của các chữ số viết tay, 

30
00:02:01,903 --> 00:02:06,204
cùng với các nhãn cho biết chúng phải là gì, và nó sẽ điều chỉnh 13.000 trọng 

31
00:02:06,204 --> 00:02:10,120
số và độ lệch đó để cải thiện hiệu suất của nó trên dữ liệu huấn luyện.

32
00:02:10,720 --> 00:02:13,721
Hy vọng rằng cấu trúc phân lớp này sẽ có nghĩa là những gì nó học 

33
00:02:13,721 --> 00:02:16,860
được sẽ khái quát hóa thành các hình ảnh ngoài dữ liệu huấn luyện đó.

34
00:02:17,640 --> 00:02:20,047
Cách chúng ta kiểm tra là sau khi bạn huấn luyện mạng, 

35
00:02:20,047 --> 00:02:23,023
bạn hiển thị cho nó nhiều dữ liệu được gắn nhãn hơn mà nó chưa từng 

36
00:02:23,023 --> 00:02:26,700
thấy trước đây và bạn thấy nó phân loại những hình ảnh mới đó chính xác như thế nào.

37
00:02:31,120 --> 00:02:34,770
May mắn cho chúng ta và điều tạo nên ví dụ phổ biến để bắt đầu như vậy, 

38
00:02:34,770 --> 00:02:39,130
đó là những người tốt bụng đằng sau cơ sở dữ liệu MNIST đã tập hợp một bộ sưu tập gồm 

39
00:02:39,130 --> 00:02:43,388
hàng chục nghìn hình ảnh chữ số viết tay, mỗi hình ảnh được dán nhãn bằng các số mà 

40
00:02:43,388 --> 00:02:44,200
chúng được nhận.

41
00:02:44,900 --> 00:02:47,545
Và thật kích thích khi mô tả một cỗ máy đang học hỏi, 

42
00:02:47,545 --> 00:02:50,973
một khi bạn thấy nó hoạt động như thế nào, nó có cảm giác không giống 

43
00:02:50,973 --> 00:02:54,549
một tiền đề khoa học viễn tưởng điên rồ nào đó mà giống một bài tập tính 

44
00:02:54,549 --> 00:02:55,480
toán hơn rất nhiều.

45
00:02:56,200 --> 00:02:59,960
Ý tôi là, về cơ bản, vấn đề là tìm giá trị nhỏ nhất của một hàm nào đó.

46
00:03:01,940 --> 00:03:06,249
Hãy nhớ rằng, về mặt khái niệm, chúng ta đang nghĩ mỗi nơ-ron được kết nối với 

47
00:03:06,249 --> 00:03:10,504
tất cả các nơ-ron ở lớp trước và các trọng số trong tổng trọng số xác định sự 

48
00:03:10,504 --> 00:03:14,814
kích hoạt của nó giống như độ mạnh của các kết nối đó và độ lệch là một số dấu 

49
00:03:14,814 --> 00:03:18,960
hiệu cho thấy tế bào thần kinh đó có xu hướng hoạt động hay không hoạt động.

50
00:03:19,720 --> 00:03:22,060
Và để bắt đầu mọi thứ, chúng ta sẽ khởi tạo tất 

51
00:03:22,060 --> 00:03:24,400
cả các trọng số và độ lệch hoàn toàn ngẫu nhiên.

52
00:03:24,940 --> 00:03:28,823
Không cần phải nói, mạng này sẽ hoạt động khá tệ trên một ví dụ huấn luyện nhất định, 

53
00:03:28,823 --> 00:03:30,720
vì nó chỉ thực hiện điều gì đó ngẫu nhiên.

54
00:03:31,040 --> 00:03:36,020
Ví dụ: bạn nạp hình ảnh số 3 này và lớp đầu ra trông giống như một mớ hỗn độn.

55
00:03:36,600 --> 00:03:41,300
Vì vậy, những gì bạn làm là xác định hàm chi phí, một cách để nói với máy tính, 

56
00:03:41,300 --> 00:03:46,470
không, máy tính tệ, đầu ra đó phải có số lần kích hoạt là 0 đối với hầu hết các nơ-ron, 

57
00:03:46,470 --> 00:03:50,760
nhưng là 1 đối với nơ-ron này, những gì bạn đưa cho tôi hoàn toàn là rác.

58
00:03:51,720 --> 00:03:56,133
Nói một cách toán học hơn một chút, bạn cộng các bình phương của sự khác 

59
00:03:56,133 --> 00:04:00,727
biệt giữa mỗi lần kích hoạt đầu ra rác đó với giá trị mà bạn muốn chúng có, 

60
00:04:00,727 --> 00:04:05,020
và đây là cái mà chúng ta gọi là một ví dụ đơn giản về chi phí đào tạo.

61
00:04:05,960 --> 00:04:11,893
Lưu ý rằng tổng này nhỏ khi mạng tự tin phân loại hình ảnh một cách chính xác, 

62
00:04:11,893 --> 00:04:16,399
nhưng nó lớn khi mạng có vẻ như không biết mình đang làm gì.

63
00:04:18,640 --> 00:04:21,983
Vì vậy, điều bạn làm là xem xét chi phí trung bình của tất 

64
00:04:21,983 --> 00:04:25,440
cả hàng chục nghìn ví dụ đào tạo mà bạn có thể tùy ý sử dụng.

65
00:04:27,040 --> 00:04:29,864
Chi phí trung bình này là thước đo của chúng ta để đánh 

66
00:04:29,864 --> 00:04:32,740
giá mức độ tệ hại của mạng và mức độ tồi tệ của máy tính.

67
00:04:33,420 --> 00:04:34,600
Và đó là một điều phức tạp.

68
00:04:35,040 --> 00:04:40,424
Hãy nhớ rằng bản thân mạng về cơ bản là một hàm, một hàm lấy 784 số làm đầu vào, 

69
00:04:40,424 --> 00:04:44,878
giá trị pixel và đưa ra 10 số làm đầu ra và theo một nghĩa nào đó, 

70
00:04:44,878 --> 00:04:48,800
nó được tham số hóa bởi tất cả các trọng số và độ lệch này?

71
00:04:49,500 --> 00:04:52,820
Vâng, hàm chi phí là một lớp phức tạp trên đó.

72
00:04:53,100 --> 00:04:56,445
Nó lấy khoảng 13.000 trọng số và độ lệch làm đầu vào, 

73
00:04:56,445 --> 00:05:01,774
đồng thời đưa ra một con số duy nhất mô tả mức độ nghiêm trọng của các trọng số và độ 

74
00:05:01,774 --> 00:05:07,041
lệch đó, đồng thời cách xác định nó phụ thuộc vào hành vi của mạng đối với hàng chục 

75
00:05:07,041 --> 00:05:08,900
nghìn phần dữ liệu huấn luyện.

76
00:05:09,520 --> 00:05:11,000
Thật nhiều điều phải nghĩ.

77
00:05:12,400 --> 00:05:14,164
Nhưng chỉ nói cho máy tính biết nó đang làm công 

78
00:05:14,164 --> 00:05:15,820
việc tồi tệ như thế nào thì không hữu ích lắm.

79
00:05:16,220 --> 00:05:20,060
Bạn muốn nói với nó cách thay đổi những trọng số và độ lệch đó để nó trở nên tốt hơn.

80
00:05:20,780 --> 00:05:25,465
Để dễ dàng hơn, thay vì cố gắng tưởng tượng một hàm có 13.000 đầu vào, 

81
00:05:25,465 --> 00:05:30,480
hãy tưởng tượng một hàm đơn giản có một số làm đầu vào và một số làm đầu ra.

82
00:05:31,480 --> 00:05:35,300
Làm thế nào để bạn tìm thấy đầu vào giảm thiểu giá trị của hàm này?

83
00:05:36,460 --> 00:05:40,091
Sinh viên giải tích sẽ biết rằng đôi khi bạn có thể tính ra mức tối thiểu đó 

84
00:05:40,091 --> 00:05:43,675
một cách rõ ràng, nhưng điều đó không phải lúc nào cũng khả thi đối với các 

85
00:05:43,675 --> 00:05:47,354
hàm thực sự phức tạp, chắc chắn không phải trong phiên bản 13.000 đầu vào của 

86
00:05:47,354 --> 00:05:51,080
tình huống này đối với hàm chi phí mạng thần kinh cực kỳ phức tạp của chúng ta.

87
00:05:51,580 --> 00:05:55,332
Một chiến thuật linh hoạt hơn là bắt đầu ở bất kỳ đầu vào nào và 

88
00:05:55,332 --> 00:05:59,200
tìm ra hướng của bước tiếp theo bạn nên làm cho đầu ra đó thấp hơn.

89
00:06:00,080 --> 00:06:03,778
Cụ thể, nếu bạn có thể tìm ra độ dốc của hàm số hiện tại, 

90
00:06:03,778 --> 00:06:08,815
thì hãy dịch sang trái nếu độ dốc đó là dương và dịch chuyển đầu vào sang phải 

91
00:06:08,815 --> 00:06:09,900
nếu độ dốc đó âm.

92
00:06:11,960 --> 00:06:15,979
Nếu bạn làm điều này nhiều lần, tại mỗi điểm kiểm tra hệ số góc mới và thực 

93
00:06:15,979 --> 00:06:19,840
hiện bước thích hợp, bạn sẽ tiến tới điểm cực tiểu cục bộ nào đó của hàm.

94
00:06:20,640 --> 00:06:23,800
Hình ảnh mà bạn có thể nghĩ đến ở đây là một quả bóng lăn xuống một ngọn đồi.

95
00:06:24,620 --> 00:06:28,250
Lưu ý, ngay cả đối với hàm đầu vào đơn thực sự được đơn giản hóa này, 

96
00:06:28,250 --> 00:06:31,102
có rất nhiều điểm có thể xảy ra mà bạn có thể rơi vào, 

97
00:06:31,102 --> 00:06:34,836
tùy thuộc vào đầu vào ngẫu nhiên nào bạn bắt đầu và không có gì đảm bảo 

98
00:06:34,836 --> 00:06:39,400
rằng mức tối thiểu cục bộ mà bạn đạt được sẽ là giá trị nhỏ nhất có thể của hàm chi phí.

99
00:06:40,220 --> 00:06:42,620
Điều đó cũng sẽ được chuyển sang trường hợp mạng lưới thần kinh của chúng ta.

100
00:06:43,180 --> 00:06:46,950
Tôi cũng muốn bạn lưu ý rằng nếu bạn làm cho kích thước bước của mình 

101
00:06:46,950 --> 00:06:50,506
tỷ lệ thuận với độ dốc, thì khi độ dốc giảm dần về mức tối thiểu, 

102
00:06:50,506 --> 00:06:54,600
các bước của bạn sẽ ngày càng nhỏ hơn và điều đó giúp bạn không bị vượt quá.

103
00:06:55,940 --> 00:06:58,407
Tăng độ phức tạp lên một chút, thay vào đó hãy 

104
00:06:58,407 --> 00:07:00,980
tưởng tượng một hàm có hai đầu vào và một đầu ra.

105
00:07:01,500 --> 00:07:04,791
Bạn có thể coi không gian đầu vào là mặt phẳng xy và hàm 

106
00:07:04,791 --> 00:07:08,140
chi phí được đồ thị hóa dưới dạng một bề mặt phía trên nó.

107
00:07:08,760 --> 00:07:13,599
Thay vì hỏi về độ dốc của hàm số, bạn phải hỏi mình nên bước vào 

108
00:07:13,599 --> 00:07:18,960
không gian đầu vào này theo hướng nào để giảm đầu ra của hàm nhanh nhất.

109
00:07:19,720 --> 00:07:21,760
Nói cách khác, hướng xuống dốc là gì?

110
00:07:22,380 --> 00:07:25,560
Một lần nữa, thật hữu ích khi nghĩ đến một quả bóng lăn xuống ngọn đồi đó.

111
00:07:26,660 --> 00:07:32,757
Những ai quen thuộc với giải tích nhiều biến sẽ biết rằng gradient của hàm số cho 

112
00:07:32,757 --> 00:07:38,780
bạn hướng đi lên dốc nhất, bạn nên bước theo hướng nào để tăng hàm số nhanh nhất.

113
00:07:39,560 --> 00:07:42,833
Đương nhiên, việc lấy giá trị âm của gradient đó 

114
00:07:42,833 --> 00:07:46,040
sẽ cho bạn hướng bước để giảm hàm số nhanh nhất.

115
00:07:47,240 --> 00:07:50,539
Hơn thế nữa, độ dài của vectơ gradient này là một 

116
00:07:50,539 --> 00:07:53,840
dấu hiệu cho biết độ dốc lớn nhất đó là bao nhiêu.

117
00:07:54,540 --> 00:07:57,304
Nếu bạn chưa quen với giải tích đa biến và muốn hiểu rõ hơn, 

118
00:07:57,304 --> 00:08:00,340
xem thêm một số bài tập ở Khan Academy mà tôi đã làm về chủ đề này.

119
00:08:00,860 --> 00:08:04,442
Thành thật mà nói, tất cả những gì quan trọng đối với bạn và 

120
00:08:04,442 --> 00:08:08,259
tôi lúc này là về nguyên tắc tồn tại một cách để tính vectơ này, 

121
00:08:08,259 --> 00:08:11,900
vectơ này cho bạn biết hướng xuống dốc là gì và độ dốc của nó.

122
00:08:12,400 --> 00:08:16,120
Sẽ ổn thôi nếu đó là tất cả những gì bạn biết và bạn không nắm chắc các chi tiết.

123
00:08:17,200 --> 00:08:21,999
Nếu bạn có thể hiểu được điều đó, thì thuật toán để thu nhỏ hàm số là tính hướng 

124
00:08:21,999 --> 00:08:26,740
gradient này, sau đó thực hiện một bước nhỏ xuống dốc và lặp đi lặp lại điều đó.

125
00:08:27,700 --> 00:08:32,820
Đó là ý tưởng cơ bản tương tự đối với một hàm có 13.000 đầu vào thay vì 2 đầu vào.

126
00:08:33,400 --> 00:08:36,346
Hãy tưởng tượng sắp xếp tất cả 13.000 trọng số và độ 

127
00:08:36,346 --> 00:08:39,460
lệch của mạng của chúng ta thành một cột vectơ khổng lồ.

128
00:08:40,140 --> 00:08:44,902
Độ dốc âm của hàm chi phí chỉ là một vectơ, đó là một số hướng 

129
00:08:44,902 --> 00:08:49,891
bên trong không gian đầu vào cực kỳ lớn này cho bạn biết tác động 

130
00:08:49,891 --> 00:08:54,880
nào đối với tất cả các số đó sẽ khiến hàm chi phí giảm nhanh nhất.

131
00:08:55,640 --> 00:08:59,099
Và tất nhiên, với hàm chi phí được thiết kế đặc biệt của chúng ta, 

132
00:08:59,099 --> 00:09:02,868
việc thay đổi trọng số và độ lệch để giảm có nghĩa là làm cho đầu ra của 

133
00:09:02,868 --> 00:09:06,482
mạng trên mỗi phần dữ liệu huấn luyện trông không giống một mảng ngẫu 

134
00:09:06,482 --> 00:09:10,820
nhiên gồm 10 giá trị mà giống một quyết định thực tế mà chúng ta muốn hơn nó để làm.

135
00:09:11,440 --> 00:09:14,621
Điều quan trọng cần nhớ là hàm chi phí này bao gồm giá trị trung 

136
00:09:14,621 --> 00:09:17,900
bình trên tất cả dữ liệu huấn luyện, vì vậy nếu bạn giảm thiểu nó, 

137
00:09:17,900 --> 00:09:21,180
điều đó có nghĩa là nó có hiệu suất tốt hơn trên tất cả các mẫu đó.

138
00:09:23,820 --> 00:09:26,919
Thuật toán để tính toán độ dốc này một cách hiệu quả, 

139
00:09:26,919 --> 00:09:29,617
vốn là trung tâm của cách mạng nơ-ron học hỏi, 

140
00:09:29,617 --> 00:09:33,980
được gọi là lan truyền ngược và đó là điều tôi sẽ nói trong video tiếp theo.

141
00:09:34,660 --> 00:09:38,806
Ở đó, tôi thực sự muốn dành thời gian để tìm hiểu chính xác điều gì sẽ xảy ra với từng 

142
00:09:38,806 --> 00:09:42,000
trọng số và độ lệch đối với một phần dữ liệu huấn luyện nhất định, 

143
00:09:42,000 --> 00:09:46,242
cố gắng mang lại cảm giác trực quan về những gì đang xảy ra ngoài đống giải tích và công 

144
00:09:46,242 --> 00:09:47,100
thức có liên quan.

145
00:09:47,780 --> 00:09:51,194
Ngay tại đây, ngay bây giờ, điều chính mà tôi muốn bạn biết, 

146
00:09:51,194 --> 00:09:54,609
không phụ thuộc vào chi tiết triển khai, đó là điều chúng ta 

147
00:09:54,609 --> 00:09:58,360
muốn nói khi nói về việc học mạng là nó chỉ giảm thiểu hàm chi phí.

148
00:09:59,300 --> 00:10:02,217
Và lưu ý, một hệ quả của điều đó là điều quan trọng là hàm chi 

149
00:10:02,217 --> 00:10:05,135
phí này phải có đầu ra mượt mà, sao cho ta có thể tìm được một 

150
00:10:05,135 --> 00:10:08,100
cực tiểu địa phương bằng cách thực hiện từng bước dốc xuống nhỏ.

151
00:10:09,260 --> 00:10:13,790
Nhân tiện, đây là lý do tại sao các nơ-ron nhân tạo có các mức kích hoạt liên tục, 

152
00:10:13,790 --> 00:10:17,065
thay vì chỉ đơn giản là hoạt động hoặc không hoạt động theo 

153
00:10:17,065 --> 00:10:19,140
cách nhị phân như các nơ-ron sinh học.

154
00:10:20,220 --> 00:10:23,490
Quá trình tác động liên tục tại một đầu vào của một hàm 

155
00:10:23,490 --> 00:10:26,760
theo bội số của gradient âm được gọi là độ dốc giảm dần.

156
00:10:27,300 --> 00:10:30,616
Đó là một cách để hội tụ về cực tiểu địa phương của một hàm chi phí nào đó, 

157
00:10:30,616 --> 00:10:32,580
về cơ bản là một thung lũng trong đồ thị này.

158
00:10:33,440 --> 00:10:36,882
Tất nhiên, tôi vẫn đang hiển thị hình ảnh của một hàm có hai đầu vào, 

159
00:10:36,882 --> 00:10:41,112
bởi vì những tác động trong không gian đầu vào 13.000 chiều hơi khó để bạn hiểu được, 

160
00:10:41,112 --> 00:10:44,260
nhưng có một cách hay không phải không gian để nghĩ về điều này.

161
00:10:45,080 --> 00:10:48,440
Mỗi thành phần của gradient âm cho chúng ta biết hai điều.

162
00:10:49,060 --> 00:10:52,156
Tất nhiên, dấu hiệu cho chúng ta biết thành phần tương 

163
00:10:52,156 --> 00:10:55,140
ứng của vectơ đầu vào nên được nâng lên hay hạ xuống.

164
00:10:55,800 --> 00:10:59,260
Nhưng quan trọng là, mức độ tương đối của tất cả các thành 

165
00:10:59,260 --> 00:11:02,720
phần này sẽ cho bạn biết những thay đổi nào quan trọng hơn.

166
00:11:05,220 --> 00:11:09,060
Bạn thấy đấy, trong mạng của chúng ta, sự điều chỉnh một trong các trọng số có thể 

167
00:11:09,060 --> 00:11:13,040
có tác động lớn hơn nhiều đến hàm chi phí so với việc điều chỉnh một số trọng số khác.

168
00:11:14,800 --> 00:11:18,200
Một số kết nối này quan trọng hơn đối với dữ liệu đào tạo của chúng ta.

169
00:11:19,320 --> 00:11:23,592
Vì vậy, bạn có thể nghĩ về vectơ gradient này của hàm chi phí khổng lồ đáng kinh 

170
00:11:23,592 --> 00:11:28,127
ngạc của chúng ta là nó mã hóa tầm quan trọng tương đối của từng trọng số và độ lệch, 

171
00:11:28,127 --> 00:11:32,400
nghĩa là, những thay đổi nào trong số này sẽ mang lại nhiều lợi ích nhất cho bạn.

172
00:11:33,620 --> 00:11:36,640
Đây thực sự chỉ là một cách nghĩ khác về hướng.

173
00:11:37,100 --> 00:11:42,047
Lấy một ví dụ đơn giản hơn, nếu bạn có một hàm nào đó với hai biến làm đầu vào và 

174
00:11:42,047 --> 00:11:46,331
bạn tính toán rằng độ dốc của nó tại một điểm cụ thể nào đó sẽ là 3,1, 

175
00:11:46,331 --> 00:11:51,158
thì một mặt bạn có thể hiểu điều đó là nói rằng khi bạn đang đứng ở đầu vào đó, 

176
00:11:51,158 --> 00:11:54,778
di chuyển dọc theo hướng này sẽ làm tăng hàm số nhanh nhất, 

177
00:11:54,778 --> 00:11:58,820
khi bạn vẽ đồ thị hàm số phía trên mặt phẳng của các điểm đầu vào, 

178
00:11:58,820 --> 00:12:02,260
vectơ đó là thứ mang lại cho bạn hướng đi thẳng lên trên.

179
00:12:02,860 --> 00:12:06,279
Nhưng một cách khác để đọc điều đó là nói rằng những thay đổi đối 

180
00:12:06,279 --> 00:12:09,802
với biến đầu tiên này có tầm quan trọng gấp 3 lần so với những thay 

181
00:12:09,802 --> 00:12:13,791
đổi đối với biến thứ hai, ít nhất là trong lân cận của đầu vào có liên quan, 

182
00:12:13,791 --> 00:12:16,900
việc tác động giá trị x mang lại nhiều lợi ích hơn cho bạn. 

183
00:12:19,880 --> 00:12:22,340
Hãy thu nhỏ và tóm tắt vị trí của chúng ta bây giờ.

184
00:12:22,840 --> 00:12:26,779
Bản thân mạng là hàm số này với 784 đầu vào và 10 đầu ra, 

185
00:12:26,779 --> 00:12:30,040
được xác định theo tất cả các tổng trọng số này.

186
00:12:30,640 --> 00:12:33,680
Trên hết, hàm chi phí là một lớp phức tạp.

187
00:12:33,980 --> 00:12:37,786
Nó lấy 13.000 trọng số và độ lệch làm đầu vào và đưa ra một 

188
00:12:37,786 --> 00:12:41,720
thước đo duy nhất về mức độ tệ hại dựa trên các ví dụ đào tạo.

189
00:12:42,440 --> 00:12:46,900
Và độ dốc của hàm chi phí vẫn còn một lớp phức tạp nữa.

190
00:12:47,360 --> 00:12:50,881
Nó cho chúng ta biết điều gì tác động đến tất cả các trọng số và độ lệch này 

191
00:12:50,881 --> 00:12:53,763
gây ra sự thay đổi nhanh nhất đối với giá trị của hàm chi phí, 

192
00:12:53,763 --> 00:12:57,880
mà bạn có thể hiểu là cho biết những thay đổi nào đối với trọng số nào là quan trọng nhất.

193
00:13:02,560 --> 00:13:05,784
Vì vậy, khi bạn khởi tạo mạng với các trọng số và độ lệch ngẫu nhiên, 

194
00:13:05,784 --> 00:13:09,146
đồng thời điều chỉnh chúng nhiều lần dựa trên quy trình giảm độ dốc này, 

195
00:13:09,146 --> 00:13:13,200
mạng thực sự hoạt động tốt như thế nào trên các hình ảnh mà nó chưa từng thấy trước đây?

196
00:13:14,100 --> 00:13:18,650
Cái mà tôi đã mô tả ở đây, với hai lớp ẩn, mỗi lớp gồm 16 nơ-ron, 

197
00:13:18,650 --> 00:13:24,580
được chọn chủ yếu vì lý do thẩm mỹ, không tệ, phân loại chính xác khoảng 96% hình ảnh 

198
00:13:24,580 --> 00:13:25,960
mới mà nó nhìn thấy.

199
00:13:26,680 --> 00:13:30,317
Và thành thật mà nói, nếu bạn nhìn vào một số ví dụ mà nó gây nhầm lẫn, 

200
00:13:30,317 --> 00:13:32,540
bạn cảm thấy buộc phải cắt giảm nó một chút.

201
00:13:36,220 --> 00:13:39,837
Bây giờ nếu bạn thử nghiệm với cấu trúc lớp ẩn và thực hiện một vài chỉnh sửa, 

202
00:13:39,837 --> 00:13:41,760
bạn có thể đạt được tỷ lệ này lên tới 98%.

203
00:13:41,760 --> 00:13:42,720
Và điều đó khá tốt!

204
00:13:43,020 --> 00:13:46,690
Nó không phải là tốt nhất, bạn chắc chắn có thể có được hiệu suất tốt hơn 

205
00:13:46,690 --> 00:13:49,169
bằng cách phức tạp hơn mạng vanilla đơn giản này, 

206
00:13:49,169 --> 00:13:51,600
nhưng với nhiệm vụ ban đầu khó khăn như thế nào, 

207
00:13:51,600 --> 00:13:55,319
tôi nghĩ có điều gì đó đáng kinh ngạc về bất kỳ mạng nào hoạt động tốt như 

208
00:13:55,319 --> 00:13:57,997
vậy trên các hình ảnh mà nó chưa từng thấy trước đây, 

209
00:13:57,997 --> 00:14:01,420
vì điều đó chúng ta chưa bao giờ nói cụ thể với nó những mẫu cần tìm.

210
00:14:02,560 --> 00:14:06,166
Ban đầu, cách tôi thúc đẩy cấu trúc này là bằng cách mô tả niềm hy vọng mà 

211
00:14:06,166 --> 00:14:09,389
chúng ta có thể có, rằng lớp thứ hai có thể thu được các cạnh nhỏ, 

212
00:14:09,389 --> 00:14:13,717
lớp thứ ba sẽ ghép các cạnh đó lại với nhau để nhận ra các vòng lặp và các đường dài hơn, 

213
00:14:13,717 --> 00:14:17,180
và rằng chúng có thể được ghép lại với nhau. cùng nhau nhận biết chữ số.

214
00:14:17,960 --> 00:14:20,400
Vậy đây có phải là điều mà mạng lưới của chúng ta thực sự đang làm?

215
00:14:21,080 --> 00:14:24,400
Vâng, ít nhất là đối với điều này thì không hề.

216
00:14:24,820 --> 00:14:28,984
Hãy nhớ video trước chúng ta đã xét trọng số của các kết nối từ tất cả các nơ-ron 

217
00:14:28,984 --> 00:14:33,047
ở lớp đầu tiên đến một nơ-ron nhất định ở lớp thứ hai có thể được hình dung như 

218
00:14:33,047 --> 00:14:37,060
thế nào dưới dạng một mẫu pixel nhất định mà nơ-ron lớp thứ hai đang tiếp nhận?

219
00:14:37,780 --> 00:14:42,940
Chà, khi chúng ta thực sự làm điều đó đối với các trọng số liên quan đến những chuyển 

220
00:14:42,940 --> 00:14:48,280
đổi này, từ lớp đầu tiên sang lớp tiếp theo, thay vì chọn các cạnh nhỏ biệt lập ở đây và 

221
00:14:48,280 --> 00:14:53,680
ở đó, chúng trông gần như ngẫu nhiên, chỉ với một số mẫu rất lỏng lẻo trong chính giữa đó.

222
00:14:53,760 --> 00:14:57,612
Có vẻ như trong không gian rộng lớn không thể đo lường được 13.000 chiều 

223
00:14:57,612 --> 00:15:01,465
với các trọng số và độ lệch có thể xảy ra, mạng của chúng ta đã tìm thấy 

224
00:15:01,465 --> 00:15:05,160
một cực tiểu địa phương nhỏ đáng mừng, mặc dù đã phân loại thành công 

225
00:15:05,160 --> 00:15:08,960
hầu hết các hình ảnh, nhưng không chọn chính xác các mẫu mà ta mong đợi.

226
00:15:09,780 --> 00:15:11,800
Và để thực sự hiểu được điểm này, hãy xem điều 

227
00:15:11,800 --> 00:15:13,820
gì xảy ra khi bạn nhập một hình ảnh ngẫu nhiên.

228
00:15:14,320 --> 00:15:18,307
Nếu hệ thống thông minh, bạn có thể cho rằng nó sẽ có cảm giác không chắc chắn, 

229
00:15:18,307 --> 00:15:22,246
có thể không thực sự kích hoạt bất kỳ nơ-ron đầu ra nào trong số 10 nơ-ron đầu 

230
00:15:22,246 --> 00:15:25,835
ra đó hoặc kích hoạt tất cả chúng một cách đồng đều, nhưng thay vào đó, 

231
00:15:25,835 --> 00:15:28,527
nó tự tin đưa ra cho bạn một số câu trả lời vô nghĩa, 

232
00:15:28,527 --> 00:15:32,365
như thể nó cảm thấy chắc chắn rằng tiếng ồn ngẫu nhiên này là số 5 vì nó cho 

233
00:15:32,365 --> 00:15:34,160
thấy hình ảnh thực của số 5 là số 5.

234
00:15:34,540 --> 00:15:38,868
Nói theo cách khác, ngay cả khi mạng này có thể nhận dạng các chữ số khá tốt, 

235
00:15:38,868 --> 00:15:40,700
nó cũng không biết cách vẽ chúng.

236
00:15:41,420 --> 00:15:45,240
Phần lớn điều này là do đây là một thiết lập đào tạo bị ràng buộc chặt chẽ.

237
00:15:45,880 --> 00:15:47,740
Ý tôi là, hãy đặt mình vào vị trí của mạng ở đây.

238
00:15:48,140 --> 00:15:51,353
Theo quan điểm của nó, toàn bộ vũ trụ không bao gồm gì ngoài những chữ số 

239
00:15:51,353 --> 00:15:54,132
bất động được xác định rõ ràng tập trung vào một mạng lưới nhỏ, 

240
00:15:54,132 --> 00:15:57,389
và hàm chi phí của nó không bao giờ mang lại cho nó bất kỳ động lực nào để 

241
00:15:57,389 --> 00:16:01,080
trở thành bất cứ thứ gì ngoại trừ sự hoàn toàn tin tưởng vào các quyết định của mình.

242
00:16:02,120 --> 00:16:05,480
Vì vậy, với đây là hình ảnh về những gì các nơ-ron lớp thứ hai thực sự đang làm, 

243
00:16:05,480 --> 00:16:08,011
bạn có thể thắc mắc tại sao tôi lại giới thiệu mạng lưới này 

244
00:16:08,011 --> 00:16:09,920
với động cơ là tìm hiểu các cạnh và khuôn mẫu.

245
00:16:09,920 --> 00:16:12,300
Ý tôi là, đó hoàn toàn không phải là điều nó sẽ làm.

246
00:16:13,380 --> 00:16:17,180
Chà, đây không phải là mục tiêu cuối cùng của chúng ta mà thay vào đó là điểm khởi đầu.

247
00:16:17,640 --> 00:16:21,902
Thành thật mà nói, đây là công nghệ cũ, loại được nghiên cứu vào những năm 80 và 90, 

248
00:16:21,902 --> 00:16:26,114
và bạn cần phải hiểu nó trước khi có thể hiểu các biến thể hiện đại chi tiết hơn và 

249
00:16:26,114 --> 00:16:28,922
rõ ràng nó có khả năng giải quyết một số vấn đề thú vị, 

250
00:16:28,922 --> 00:16:33,185
nhưng bạn càng đào sâu vào những gì những lớp ẩn đó thực sự đang hoạt động thì có vẻ 

251
00:16:33,185 --> 00:16:34,740
như nó càng kém thông minh hơn.

252
00:16:38,480 --> 00:16:41,955
Chuyển trọng tâm trong giây lát từ cách mạng học sang cách bạn học, 

253
00:16:41,955 --> 00:16:46,300
điều đó sẽ chỉ xảy ra nếu bạn tương tác tích cực với tài liệu ở đây bằng cách nào đó.

254
00:16:47,060 --> 00:16:51,575
Một điều khá đơn giản mà tôi muốn bạn làm là tạm dừng ngay bây giờ và suy nghĩ sâu 

255
00:16:51,575 --> 00:16:56,146
sắc một chút về những thay đổi bạn có thể thực hiện đối với hệ thống này và cách hệ 

256
00:16:56,146 --> 00:17:00,880
thống cảm nhận hình ảnh nếu bạn muốn nó tiếp thu tốt hơn những thứ như cạnh và các mẫu.

257
00:17:01,480 --> 00:17:04,377
Nhưng tốt hơn thế, để thực sự tương tác với tài liệu, 

258
00:17:04,377 --> 00:17:09,099
tôi đặc biệt giới thiệu cuốn sách của Michael Nielsen về học sâu và mạng lưới thần kinh.

259
00:17:09,680 --> 00:17:14,076
Trong đó, bạn có thể tìm thấy mã và dữ liệu để tải xuống và sử dụng cho ví dụ 

260
00:17:14,076 --> 00:17:18,359
chính xác này và cuốn sách sẽ hướng dẫn bạn từng bước về của mã đó đang làm.

261
00:17:19,300 --> 00:17:22,304
Điều tuyệt vời là cuốn sách này được cung cấp miễn phí và công khai, 

262
00:17:22,304 --> 00:17:24,481
vì vậy nếu bạn nhận được điều gì đó từ cuốn sách, 

263
00:17:24,481 --> 00:17:27,660
hãy cân nhắc tham gia cùng tôi để quyên góp cho những nỗ lực của Nielsen.

264
00:17:27,660 --> 00:17:32,024
Tôi cũng đã liên kết một số tài nguyên khác mà tôi rất thích trong phần mô tả, 

265
00:17:32,024 --> 00:17:36,500
bao gồm bài đăng blog hay và ấn tượng của Chris Ola và các bài viết trên Distill.

266
00:17:38,280 --> 00:17:40,511
Để kết thúc mọi chuyện ở đây trong vài phút vừa qua, 

267
00:17:40,511 --> 00:17:43,880
tôi muốn quay lại một đoạn trong cuộc phỏng vấn tôi đã thực hiện với Leisha Lee.

268
00:17:44,300 --> 00:17:47,720
Bạn có thể nhớ đến cô ấy từ video trước, cô ấy đã làm luận án tiến sĩ về học sâu.

269
00:17:48,300 --> 00:17:52,065
Trong đoạn trích này, cô ấy nói về hai bài báo gần đây thực sự đi sâu vào 

270
00:17:52,065 --> 00:17:55,780
cách một số mạng nhận dạng hình ảnh hiện đại hơn là đang thực sự học hỏi.

271
00:17:56,120 --> 00:17:58,410
Để xác định vị trí của chúng ta trong cuộc trò chuyện, 

272
00:17:58,410 --> 00:18:01,451
bài báo đầu tiên đã sử dụng một trong những mạng lưới thần kinh đặc biệt 

273
00:18:01,451 --> 00:18:04,658
sâu có khả năng nhận dạng hình ảnh thực sự tốt và thay vì huấn luyện nó trên 

274
00:18:04,658 --> 00:18:07,865
một tập dữ liệu được dán nhãn chính xác, xáo trộn tất cả các nhãn xung quanh 

275
00:18:07,865 --> 00:18:08,740
trước khi huấn luyện.

276
00:18:09,480 --> 00:18:12,925
Rõ ràng độ chính xác của thử nghiệm ở đây không tốt hơn ngẫu nhiên, 

277
00:18:12,925 --> 00:18:16,624
vì mọi thứ chỉ được gắn nhãn ngẫu nhiên, nhưng nó vẫn có thể đạt được độ 

278
00:18:16,624 --> 00:18:20,880
chính xác huấn luyện giống như bạn làm trên một tập dữ liệu được gắn nhãn chính xác.

279
00:18:21,600 --> 00:18:26,410
Về cơ bản, hàng triệu trọng số cho mạng cụ thể này là đủ để nó chỉ ghi nhớ dữ 

280
00:18:26,410 --> 00:18:31,405
liệu ngẫu nhiên, điều này đặt ra câu hỏi liệu việc giảm thiểu hàm chi phí này có 

281
00:18:31,405 --> 00:18:36,400
thực sự tương ứng với bất kỳ loại cấu trúc nào trong hình ảnh hay chỉ là ghi nhớ?

282
00:18:51,440 --> 00:18:56,450
Nếu bạn nhìn vào đường cong chính xác đó, nếu bạn chỉ đang đào tạo trên một 

283
00:18:56,450 --> 00:19:01,526
tập dữ liệu ngẫu nhiên, thì đường cong đó sẽ đi xuống rất chậm theo kiểu gần 

284
00:19:01,526 --> 00:19:06,536
như tuyến tính, vì vậy bạn thực sự đang gặp khó khăn để tìm ra cực tiểu địa 

285
00:19:06,536 --> 00:19:12,140
phương có thể có, bạn biết đấy, trọng số phù hợp sẽ giúp bạn có được độ chính xác đó.

286
00:19:12,240 --> 00:19:16,557
Trong khi đó, nếu bạn thực sự đang đào tạo trên một tập dữ liệu có cấu trúc, 

287
00:19:16,557 --> 00:19:20,370
một tập dữ liệu có nhãn phù hợp, bạn sẽ loay hoay một chút lúc đầu, 

288
00:19:20,370 --> 00:19:24,182
nhưng sau đó bạn đã giảm rất nhanh để đạt được mức độ chính xác đó, 

289
00:19:24,182 --> 00:19:28,220
và theo một nghĩa nào đó, nó việc tìm cực đại địa phương đó dễ dàng hơn.

290
00:19:28,540 --> 00:19:33,293
Và điều thú vị ở đây là nó đưa ra ánh sáng một bài báo khác từ vài năm trước, 

291
00:19:33,293 --> 00:19:36,645
trong đó có nhiều sự đơn giản hóa hơn về các lớp mạng, 

292
00:19:36,645 --> 00:19:41,521
nhưng một trong những kết quả là nói rằng nếu bạn nhìn vào bối cảnh tối ưu hóa, 

293
00:19:41,521 --> 00:19:47,006
cực tiểu địa phương  mà các mạng này có xu hướng tìm hiểu thực sự có chất lượng như nhau, 

294
00:19:47,006 --> 00:19:51,150
vì vậy, theo một nghĩa nào đó, nếu tập dữ liệu của bạn có cấu trúc, 

295
00:19:51,150 --> 00:19:54,320
bạn sẽ có thể tìm thấy dữ liệu đó dễ dàng hơn nhiều.

296
00:19:58,160 --> 00:20:01,180
Như mọi khi, tôi xin gửi lời cảm ơn tới những người đã ủng hộ trên Patreon.

297
00:20:01,520 --> 00:20:03,909
Tôi đã từng nói Patreon chỉ làm thay đổi một trò chơi nào đó, 

298
00:20:03,909 --> 00:20:06,800
nhưng những video này thực sự sẽ không thể thực hiện được nếu không có bạn.

299
00:20:07,460 --> 00:20:10,266
Tôi cũng muốn gửi lời cảm ơn đặc biệt đến công ty Amplify Partners 

300
00:20:10,266 --> 00:20:12,780
của VC vì đã hỗ trợ những video đầu tiên trong loạt bài này.

