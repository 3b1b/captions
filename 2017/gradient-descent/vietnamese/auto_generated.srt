1
00:00:04,180 --> 00:00:07,280
Video cuối cùng tôi đã trình bày cấu trúc của mạng lưới thần kinh.

2
00:00:07,680 --> 00:00:10,140
Tôi sẽ tóm tắt nhanh ở đây để chúng ta dễ nhớ hơn 

3
00:00:10,140 --> 00:00:12,600
và sau đó tôi có hai mục tiêu chính cho video này.

4
00:00:13,100 --> 00:00:15,536
Đầu tiên là giới thiệu ý tưởng về độ dốc giảm dần, 

5
00:00:15,536 --> 00:00:19,214
không chỉ làm nền tảng cho cách mạng lưới thần kinh học mà còn làm cơ sở cho 

6
00:00:19,214 --> 00:00:20,600
nhiều hoạt động học máy khác.

7
00:00:21,120 --> 00:00:24,435
Sau đó, chúng ta sẽ tìm hiểu thêm một chút về cách thức hoạt động của 

8
00:00:24,435 --> 00:00:27,940
mạng cụ thể này và những lớp tế bào thần kinh ẩn đó đang tìm kiếm điều gì.

9
00:00:28,980 --> 00:00:33,916
Xin nhắc lại, mục tiêu của chúng tôi ở đây là ví dụ cổ điển về nhận dạng chữ số viết tay, 

10
00:00:33,916 --> 00:00:36,220
thế giới xin chào của mạng lưới thần kinh.

11
00:00:37,020 --> 00:00:39,864
Các chữ số này được hiển thị trên lưới 28x28 pixel, 

12
00:00:39,864 --> 00:00:43,420
mỗi pixel có một số giá trị thang độ xám trong khoảng từ 0 đến 1.

13
00:00:43,820 --> 00:00:50,040
Đó là những gì quyết định sự kích hoạt của 784 nơ-ron trong lớp đầu vào của mạng.

14
00:00:51,180 --> 00:00:55,879
Và sau đó, việc kích hoạt mỗi nơ-ron trong các lớp sau dựa trên tổng trọng số 

15
00:00:55,879 --> 00:01:00,820
của tất cả các lần kích hoạt ở lớp trước, cộng với một số đặc biệt gọi là độ lệch.

16
00:01:02,160 --> 00:01:04,616
Sau đó, bạn tính số tiền đó bằng một số hàm khác, 

17
00:01:04,616 --> 00:01:08,940
chẳng hạn như tính năng chia nhỏ sigmoid hoặc relu, như cách tôi đã xem qua video trước.

18
00:01:09,480 --> 00:01:14,815
Tổng cộng, với sự lựa chọn có phần tùy tiện của hai lớp ẩn với 16 nơ-ron mỗi lớp, 

19
00:01:14,815 --> 00:01:19,695
mạng có khoảng 13.000 trọng số và độ lệch mà chúng ta có thể điều chỉnh và 

20
00:01:19,695 --> 00:01:24,380
chính những giá trị này sẽ xác định chính xác những gì mạng thực sự làm.

21
00:01:24,880 --> 00:01:29,038
Vậy thì điều chúng tôi muốn nói khi nói rằng mạng này phân loại một chữ số nhất 

22
00:01:29,038 --> 00:01:33,300
định là điểm sáng nhất trong số 10 nơ-ron ở lớp cuối cùng tương ứng với chữ số đó.

23
00:01:34,100 --> 00:01:39,000
Và hãy nhớ rằng, động lực mà chúng tôi nghĩ đến ở đây đối với cấu trúc phân lớp là có 

24
00:01:39,000 --> 00:01:43,843
thể lớp thứ hai có thể xử lý các cạnh và lớp thứ ba có thể xử lý các mẫu như vòng và 

25
00:01:43,843 --> 00:01:48,800
đường, và lớp cuối cùng có thể ghép những thứ đó lại với nhau. mẫu để nhận biết chữ số.

26
00:01:49,800 --> 00:01:52,240
Vì vậy, ở đây chúng ta tìm hiểu cách mạng học.

27
00:01:52,640 --> 00:01:56,982
Điều chúng tôi muốn là một thuật toán mà bạn có thể hiển thị cho mạng này toàn 

28
00:01:56,982 --> 00:02:01,929
bộ dữ liệu huấn luyện, dưới dạng một loạt các hình ảnh khác nhau của các chữ số viết tay, 

29
00:02:01,929 --> 00:02:06,217
cùng với các nhãn cho biết chúng phải là gì, và nó sẽ điều chỉnh 13.000 trọng 

30
00:02:06,217 --> 00:02:10,120
số và độ lệch đó để cải thiện hiệu suất của nó trên dữ liệu huấn luyện.

31
00:02:10,720 --> 00:02:13,721
Hy vọng rằng cấu trúc phân lớp này sẽ có nghĩa là những gì nó học 

32
00:02:13,721 --> 00:02:16,860
được sẽ khái quát hóa thành các hình ảnh ngoài dữ liệu huấn luyện đó.

33
00:02:17,640 --> 00:02:20,079
Cách chúng tôi kiểm tra là sau khi bạn huấn luyện mạng, 

34
00:02:20,079 --> 00:02:23,041
bạn hiển thị cho nó nhiều dữ liệu được gắn nhãn hơn mà nó chưa từng 

35
00:02:23,041 --> 00:02:26,700
thấy trước đây và bạn thấy nó phân loại những hình ảnh mới đó chính xác như thế nào.

36
00:02:31,120 --> 00:02:35,195
Thật may mắn cho chúng ta, và điều khiến đây trở thành một ví dụ phổ biến để bắt đầu, 

37
00:02:35,195 --> 00:02:38,465
đó là những người tốt đằng sau cơ sở dữ liệu MNIST đã tập hợp một bộ 

38
00:02:38,465 --> 00:02:41,024
sưu tập gồm hàng chục nghìn hình ảnh chữ số viết tay, 

39
00:02:41,024 --> 00:02:44,200
mỗi hình ảnh được dán nhãn bằng những con số mà chúng phải ghi. là.

40
00:02:44,900 --> 00:02:47,581
Và thật khiêu khích khi mô tả một cỗ máy đang học hỏi, 

41
00:02:47,581 --> 00:02:50,994
một khi bạn thấy nó hoạt động như thế nào, nó có cảm giác không giống 

42
00:02:50,994 --> 00:02:54,553
một tiền đề khoa học viễn tưởng điên rồ nào đó mà giống một bài tập tính 

43
00:02:54,553 --> 00:02:55,480
toán hơn rất nhiều.

44
00:02:56,200 --> 00:02:59,960
Ý tôi là, về cơ bản, vấn đề là tìm giá trị nhỏ nhất của một hàm nào đó.

45
00:03:01,940 --> 00:03:06,249
Hãy nhớ rằng, về mặt khái niệm, chúng ta đang nghĩ mỗi nơ-ron được kết nối với 

46
00:03:06,249 --> 00:03:10,504
tất cả các nơ-ron ở lớp trước và các trọng số trong tổng trọng số xác định sự 

47
00:03:10,504 --> 00:03:14,814
kích hoạt của nó giống như độ mạnh của các kết nối đó và độ lệch là một số dấu 

48
00:03:14,814 --> 00:03:18,960
hiệu cho thấy tế bào thần kinh đó có xu hướng hoạt động hay không hoạt động.

49
00:03:19,720 --> 00:03:22,060
Và để bắt đầu mọi thứ, chúng ta sẽ khởi tạo tất 

50
00:03:22,060 --> 00:03:24,400
cả các trọng số và độ lệch hoàn toàn ngẫu nhiên.

51
00:03:24,940 --> 00:03:28,823
Không cần phải nói, mạng này sẽ hoạt động khá tệ trên một ví dụ huấn luyện nhất định, 

52
00:03:28,823 --> 00:03:30,720
vì nó chỉ thực hiện điều gì đó ngẫu nhiên.

53
00:03:31,040 --> 00:03:36,020
Ví dụ: bạn nạp hình ảnh số 3 này và lớp đầu ra trông giống như một mớ hỗn độn.

54
00:03:36,600 --> 00:03:41,186
Vì vậy, những gì bạn làm là xác định hàm chi phí, một cách để nói với máy tính, 

55
00:03:41,186 --> 00:03:46,288
không, máy tính xấu, đầu ra đó phải có số lần kích hoạt là 0 đối với hầu hết các nơ-ron, 

56
00:03:46,288 --> 00:03:50,760
nhưng là 1 đối với nơ-ron này, những gì bạn đưa cho tôi hoàn toàn là rác rưởi.

57
00:03:51,720 --> 00:03:56,278
Nói một cách toán học hơn một chút, bạn cộng các bình phương của sự khác 

58
00:03:56,278 --> 00:04:01,023
biệt giữa mỗi lần kích hoạt đầu ra rác đó với giá trị mà bạn muốn chúng có, 

59
00:04:01,023 --> 00:04:05,020
và đây là cái mà chúng tôi gọi là chi phí của một ví dụ đào tạo.

60
00:04:05,960 --> 00:04:11,893
Lưu ý rằng tổng này nhỏ khi mạng tự tin phân loại hình ảnh một cách chính xác, 

61
00:04:11,893 --> 00:04:16,399
nhưng nó lớn khi mạng có vẻ như không biết mình đang làm gì.

62
00:04:18,640 --> 00:04:21,983
Vì vậy, điều bạn làm là xem xét chi phí trung bình của tất 

63
00:04:21,983 --> 00:04:25,440
cả hàng chục nghìn ví dụ đào tạo mà bạn có thể tùy ý sử dụng.

64
00:04:27,040 --> 00:04:29,890
Chi phí trung bình này là thước đo của chúng tôi để đánh 

65
00:04:29,890 --> 00:04:32,740
giá mức độ tệ hại của mạng và mức độ tồi tệ của máy tính.

66
00:04:33,420 --> 00:04:34,600
Và đó là một điều phức tạp.

67
00:04:35,040 --> 00:04:40,424
Hãy nhớ rằng bản thân mạng về cơ bản là một hàm, một hàm lấy 784 số làm đầu vào, 

68
00:04:40,424 --> 00:04:44,878
giá trị pixel và đưa ra 10 số làm đầu ra và theo một nghĩa nào đó, 

69
00:04:44,878 --> 00:04:48,800
nó được tham số hóa bởi tất cả các trọng số và độ lệch này?

70
00:04:49,500 --> 00:04:52,820
Vâng, hàm chi phí là một lớp phức tạp trên đó.

71
00:04:53,100 --> 00:04:56,445
Nó lấy khoảng 13.000 trọng số và độ lệch làm đầu vào, 

72
00:04:56,445 --> 00:05:01,774
đồng thời đưa ra một con số duy nhất mô tả mức độ nghiêm trọng của các trọng số và độ 

73
00:05:01,774 --> 00:05:07,041
lệch đó, đồng thời cách xác định nó phụ thuộc vào hành vi của mạng đối với hàng chục 

74
00:05:07,041 --> 00:05:08,900
nghìn phần dữ liệu huấn luyện.

75
00:05:09,520 --> 00:05:11,000
Đó là rất nhiều điều để suy nghĩ.

76
00:05:12,400 --> 00:05:14,164
Nhưng chỉ nói cho máy tính biết nó đang làm công 

77
00:05:14,164 --> 00:05:15,820
việc tồi tệ như thế nào thì không hữu ích lắm.

78
00:05:16,220 --> 00:05:20,060
Bạn muốn nói với nó cách thay đổi những trọng số và thành kiến đó để nó trở nên tốt hơn.

79
00:05:20,780 --> 00:05:25,465
Để dễ dàng hơn, thay vì cố gắng tưởng tượng một hàm có 13.000 đầu vào, 

80
00:05:25,465 --> 00:05:30,480
hãy tưởng tượng một hàm đơn giản có một số làm đầu vào và một số làm đầu ra.

81
00:05:31,480 --> 00:05:35,300
Làm thế nào để bạn tìm thấy đầu vào giảm thiểu giá trị của hàm này?

82
00:05:36,460 --> 00:05:40,091
Sinh viên giải tích sẽ biết rằng đôi khi bạn có thể tính ra mức tối thiểu đó 

83
00:05:40,091 --> 00:05:43,675
một cách rõ ràng, nhưng điều đó không phải lúc nào cũng khả thi đối với các 

84
00:05:43,675 --> 00:05:47,354
hàm thực sự phức tạp, chắc chắn không phải trong phiên bản 13.000 đầu vào của 

85
00:05:47,354 --> 00:05:51,080
tình huống này đối với hàm chi phí mạng thần kinh cực kỳ phức tạp của chúng ta.

86
00:05:51,580 --> 00:05:55,232
Một chiến thuật linh hoạt hơn là bắt đầu ở bất kỳ đầu vào 

87
00:05:55,232 --> 00:05:59,200
nào và tìm ra hướng bạn nên bước để làm cho đầu ra đó thấp hơn.

88
00:06:00,080 --> 00:06:03,707
Cụ thể, nếu bạn có thể tìm ra độ dốc của hàm số hiện tại, 

89
00:06:03,707 --> 00:06:08,649
thì hãy dịch sang trái nếu độ dốc đó là dương và dịch chuyển đầu vào sang phải 

90
00:06:08,649 --> 00:06:09,900
nếu độ dốc đó là âm.

91
00:06:11,960 --> 00:06:15,979
Nếu bạn làm điều này nhiều lần, tại mỗi điểm kiểm tra hệ số góc mới và thực 

92
00:06:15,979 --> 00:06:19,840
hiện bước thích hợp, bạn sẽ tiến tới điểm cực tiểu cục bộ nào đó của hàm.

93
00:06:20,640 --> 00:06:23,800
Hình ảnh mà bạn có thể nghĩ đến ở đây là một quả bóng lăn xuống một ngọn đồi.

94
00:06:24,620 --> 00:06:28,250
Lưu ý, ngay cả đối với hàm đầu vào đơn thực sự được đơn giản hóa này, 

95
00:06:28,250 --> 00:06:31,102
có rất nhiều điểm có thể xảy ra mà bạn có thể rơi vào, 

96
00:06:31,102 --> 00:06:34,836
tùy thuộc vào đầu vào ngẫu nhiên nào bạn bắt đầu và không có gì đảm bảo 

97
00:06:34,836 --> 00:06:39,400
rằng mức tối thiểu cục bộ mà bạn đạt được sẽ là giá trị nhỏ nhất có thể của hàm chi phí.

98
00:06:40,220 --> 00:06:42,620
Điều đó cũng sẽ được chuyển sang trường hợp mạng lưới thần kinh của chúng ta.

99
00:06:43,180 --> 00:06:46,950
Tôi cũng muốn bạn lưu ý rằng nếu bạn làm cho kích thước bước của mình 

100
00:06:46,950 --> 00:06:50,506
tỷ lệ thuận với độ dốc, thì khi độ dốc giảm dần về mức tối thiểu, 

101
00:06:50,506 --> 00:06:54,600
các bước của bạn sẽ ngày càng nhỏ hơn và điều đó giúp bạn không bị vượt quá.

102
00:06:55,940 --> 00:06:58,407
Tăng độ phức tạp lên một chút, thay vào đó hãy 

103
00:06:58,407 --> 00:07:00,980
tưởng tượng một hàm có hai đầu vào và một đầu ra.

104
00:07:01,500 --> 00:07:04,879
Bạn có thể coi không gian đầu vào là mặt phẳng xy và hàm 

105
00:07:04,879 --> 00:07:08,140
chi phí được biểu đồ dưới dạng một bề mặt phía trên nó.

106
00:07:08,760 --> 00:07:13,599
Thay vì hỏi về độ dốc của hàm số, bạn phải hỏi mình nên bước vào 

107
00:07:13,599 --> 00:07:18,960
không gian đầu vào này theo hướng nào để giảm đầu ra của hàm nhanh nhất.

108
00:07:19,720 --> 00:07:21,760
Nói cách khác, hướng xuống dốc là gì?

109
00:07:22,380 --> 00:07:25,560
Một lần nữa, thật hữu ích khi nghĩ đến một quả bóng lăn xuống ngọn đồi đó.

110
00:07:26,660 --> 00:07:32,757
Những ai quen thuộc với phép tính nhiều biến sẽ biết rằng gradient của hàm số cho 

111
00:07:32,757 --> 00:07:38,780
bạn hướng đi lên dốc nhất, bạn nên bước theo hướng nào để tăng hàm số nhanh nhất.

112
00:07:39,560 --> 00:07:42,833
Đương nhiên, việc lấy giá trị âm của gradient đó 

113
00:07:42,833 --> 00:07:46,040
sẽ cho bạn hướng bước để giảm hàm số nhanh nhất.

114
00:07:47,240 --> 00:07:50,539
Hơn thế nữa, độ dài của vectơ gradient này là một 

115
00:07:50,539 --> 00:07:53,840
dấu hiệu cho biết độ dốc lớn nhất đó là bao nhiêu.

116
00:07:54,540 --> 00:07:57,350
Nếu bạn chưa quen với phép tính đa biến và muốn tìm hiểu thêm, 

117
00:07:57,350 --> 00:08:00,340
hãy xem một số công việc tôi đã làm cho Khan Academy về chủ đề này.

118
00:08:00,860 --> 00:08:04,442
Thành thật mà nói, tất cả những gì quan trọng đối với bạn và 

119
00:08:04,442 --> 00:08:08,259
tôi lúc này là về nguyên tắc tồn tại một cách để tính vectơ này, 

120
00:08:08,259 --> 00:08:11,900
vectơ này cho bạn biết hướng xuống dốc là gì và độ dốc của nó.

121
00:08:12,400 --> 00:08:16,120
Sẽ ổn thôi nếu đó là tất cả những gì bạn biết và bạn không nắm chắc các chi tiết.

122
00:08:17,200 --> 00:08:21,999
Nếu bạn có thể hiểu được điều đó, thì thuật toán để thu nhỏ hàm số là tính hướng 

123
00:08:21,999 --> 00:08:26,740
gradient này, sau đó thực hiện một bước nhỏ xuống dốc và lặp đi lặp lại điều đó.

124
00:08:27,700 --> 00:08:32,820
Đó là ý tưởng cơ bản tương tự đối với một hàm có 13.000 đầu vào thay vì 2 đầu vào.

125
00:08:33,400 --> 00:08:36,346
Hãy tưởng tượng sắp xếp tất cả 13.000 trọng số và độ 

126
00:08:36,346 --> 00:08:39,460
lệch của mạng của chúng ta thành một vectơ cột khổng lồ.

127
00:08:40,140 --> 00:08:44,926
Độ dốc âm của hàm chi phí chỉ là một vectơ, đó là một số hướng 

128
00:08:44,926 --> 00:08:49,865
bên trong không gian đầu vào cực kỳ lớn này cho bạn biết lực đẩy 

129
00:08:49,865 --> 00:08:54,880
nào đối với tất cả các số đó sẽ khiến hàm chi phí giảm nhanh nhất.

130
00:08:55,640 --> 00:08:59,139
Và tất nhiên, với hàm chi phí được thiết kế đặc biệt của chúng tôi, 

131
00:08:59,139 --> 00:09:02,895
việc thay đổi trọng số và độ lệch để giảm có nghĩa là làm cho đầu ra của 

132
00:09:02,895 --> 00:09:06,497
mạng trên mỗi phần dữ liệu huấn luyện trông không giống một mảng ngẫu 

133
00:09:06,497 --> 00:09:10,820
nhiên gồm 10 giá trị mà giống một quyết định thực tế mà chúng ta muốn hơn nó để làm.

134
00:09:11,440 --> 00:09:14,621
Điều quan trọng cần nhớ là hàm chi phí này bao gồm giá trị trung 

135
00:09:14,621 --> 00:09:17,900
bình trên tất cả dữ liệu huấn luyện, vì vậy nếu bạn giảm thiểu nó, 

136
00:09:17,900 --> 00:09:21,180
điều đó có nghĩa là nó có hiệu suất tốt hơn trên tất cả các mẫu đó.

137
00:09:23,820 --> 00:09:26,919
Thuật toán để tính toán độ dốc này một cách hiệu quả, 

138
00:09:26,919 --> 00:09:29,617
vốn là trung tâm của cách mạng nơ-ron học hỏi, 

139
00:09:29,617 --> 00:09:33,980
được gọi là lan truyền ngược và đó là điều tôi sẽ nói trong video tiếp theo.

140
00:09:34,660 --> 00:09:38,759
Ở đó, tôi thực sự muốn dành thời gian để tìm hiểu chính xác điều gì sẽ xảy ra với từng 

141
00:09:38,759 --> 00:09:42,058
trọng lượng và độ lệch đối với một phần dữ liệu huấn luyện nhất định, 

142
00:09:42,058 --> 00:09:46,251
cố gắng mang lại cảm giác trực quan về những gì đang xảy ra ngoài đống phép tính và công 

143
00:09:46,251 --> 00:09:47,100
thức có liên quan.

144
00:09:47,780 --> 00:09:51,176
Ngay tại đây, ngay bây giờ, điều chính mà tôi muốn bạn biết, 

145
00:09:51,176 --> 00:09:54,629
không phụ thuộc vào chi tiết triển khai, đó là điều chúng tôi 

146
00:09:54,629 --> 00:09:58,360
muốn nói khi nói về việc học mạng là nó chỉ giảm thiểu hàm chi phí.

147
00:09:59,300 --> 00:10:02,172
Và lưu ý, một hệ quả của điều đó là điều quan trọng là hàm chi 

148
00:10:02,172 --> 00:10:05,136
phí này phải có đầu ra mượt mà, sao cho chúng ta có thể tìm được 

149
00:10:05,136 --> 00:10:08,100
mức tối thiểu cục bộ bằng cách thực hiện từng bước nhỏ xuống dốc.

150
00:10:09,260 --> 00:10:13,790
Nhân tiện, đây là lý do tại sao các nơ-ron nhân tạo có các mức kích hoạt liên tục, 

151
00:10:13,790 --> 00:10:17,065
thay vì chỉ đơn giản là hoạt động hoặc không hoạt động theo 

152
00:10:17,065 --> 00:10:19,140
cách nhị phân như các nơ-ron sinh học.

153
00:10:20,220 --> 00:10:23,396
Quá trình liên tục dịch chuyển đầu vào của một hàm 

154
00:10:23,396 --> 00:10:26,760
theo bội số của gradient âm được gọi là giảm gradient.

155
00:10:27,300 --> 00:10:30,468
Đó là một cách để hội tụ về một hàm chi phí tối thiểu cục bộ nào đó, 

156
00:10:30,468 --> 00:10:32,580
về cơ bản là một thung lũng trong biểu đồ này.

157
00:10:33,440 --> 00:10:36,882
Tất nhiên, tôi vẫn đang hiển thị hình ảnh của một hàm có hai đầu vào, 

158
00:10:36,882 --> 00:10:41,112
bởi vì những cú huých trong không gian đầu vào 13.000 chiều hơi khó để bạn hiểu được, 

159
00:10:41,112 --> 00:10:44,260
nhưng có một cách hay không phải không gian để nghĩ về điều này.

160
00:10:45,080 --> 00:10:48,440
Mỗi thành phần của gradient âm cho chúng ta biết hai điều.

161
00:10:49,060 --> 00:10:52,156
Tất nhiên, dấu hiệu cho chúng ta biết thành phần tương 

162
00:10:52,156 --> 00:10:55,140
ứng của vectơ đầu vào nên được nâng lên hay hạ xuống.

163
00:10:55,800 --> 00:10:59,260
Nhưng quan trọng là, mức độ tương đối của tất cả các thành 

164
00:10:59,260 --> 00:11:02,720
phần này sẽ cho bạn biết những thay đổi nào quan trọng hơn.

165
00:11:05,220 --> 00:11:09,063
Bạn thấy đấy, trong mạng lưới của chúng tôi, việc điều chỉnh một trong các trọng số có 

166
00:11:09,063 --> 00:11:13,040
thể có tác động lớn hơn nhiều đến hàm chi phí so với việc điều chỉnh một số trọng số khác.

167
00:11:14,800 --> 00:11:18,200
Một số kết nối này quan trọng hơn đối với dữ liệu đào tạo của chúng tôi.

168
00:11:19,320 --> 00:11:23,592
Vì vậy, bạn có thể nghĩ về vectơ gradient này của hàm chi phí khổng lồ đáng kinh 

169
00:11:23,592 --> 00:11:28,127
ngạc của chúng ta là nó mã hóa tầm quan trọng tương đối của từng trọng số và độ lệch, 

170
00:11:28,127 --> 00:11:32,400
nghĩa là, những thay đổi nào trong số này sẽ mang lại nhiều lợi ích nhất cho bạn.

171
00:11:33,620 --> 00:11:36,640
Đây thực sự chỉ là một cách suy nghĩ khác về phương hướng.

172
00:11:37,100 --> 00:11:41,977
Lấy một ví dụ đơn giản hơn, nếu bạn có một hàm nào đó với hai biến làm đầu vào và 

173
00:11:41,977 --> 00:11:46,200
bạn tính toán rằng độ dốc của nó tại một điểm cụ thể nào đó sẽ là 3,1, 

174
00:11:46,200 --> 00:11:51,315
thì một mặt bạn có thể hiểu điều đó là nói rằng khi bạn &#39; đang đứng ở đầu vào đó, 

175
00:11:51,315 --> 00:11:54,884
di chuyển dọc theo hướng này sẽ làm tăng hàm số nhanh nhất, 

176
00:11:54,884 --> 00:11:58,869
khi bạn vẽ đồ thị hàm số phía trên mặt phẳng của các điểm đầu vào, 

177
00:11:58,869 --> 00:12:02,260
vectơ đó là thứ mang lại cho bạn hướng đi thẳng lên trên.

178
00:12:02,860 --> 00:12:06,332
Nhưng một cách khác để đọc điều đó là nói rằng những thay đổi đối với 

179
00:12:06,332 --> 00:12:10,797
biến đầu tiên này có tầm quan trọng gấp 3 lần so với những thay đổi đối với biến thứ hai, 

180
00:12:10,797 --> 00:12:13,576
ít nhất là trong vùng lân cận của đầu vào có liên quan, 

181
00:12:13,576 --> 00:12:16,900
việc thúc đẩy giá trị x mang lại nhiều lợi ích hơn cho bạn. Cái xô.

182
00:12:19,880 --> 00:12:22,340
Hãy thu nhỏ và tóm tắt vị trí của chúng ta cho đến nay.

183
00:12:22,840 --> 00:12:26,869
Bản thân mạng là chức năng này với 784 đầu vào và 10 đầu ra, 

184
00:12:26,869 --> 00:12:30,040
được xác định theo tất cả các tổng trọng số này.

185
00:12:30,640 --> 00:12:33,680
Trên hết, hàm chi phí là một lớp phức tạp.

186
00:12:33,980 --> 00:12:37,786
Nó lấy 13.000 trọng số và độ lệch làm đầu vào và đưa ra một 

187
00:12:37,786 --> 00:12:41,720
thước đo duy nhất về mức độ tệ hại dựa trên các ví dụ đào tạo.

188
00:12:42,440 --> 00:12:46,900
Và độ dốc của hàm chi phí vẫn còn một lớp phức tạp nữa.

189
00:12:47,360 --> 00:12:50,881
Nó cho chúng ta biết điều gì tác động đến tất cả các trọng số và độ lệch này 

190
00:12:50,881 --> 00:12:53,763
gây ra sự thay đổi nhanh nhất đối với giá trị của hàm chi phí, 

191
00:12:53,763 --> 00:12:57,880
mà bạn có thể hiểu là cho biết những thay đổi nào đối với trọng số nào là quan trọng nhất.

192
00:13:02,560 --> 00:13:05,784
Vì vậy, khi bạn khởi tạo mạng với các trọng số và độ lệch ngẫu nhiên, 

193
00:13:05,784 --> 00:13:09,146
đồng thời điều chỉnh chúng nhiều lần dựa trên quy trình giảm độ dốc này, 

194
00:13:09,146 --> 00:13:13,200
mạng thực sự hoạt động tốt như thế nào trên các hình ảnh mà nó chưa từng thấy trước đây?

195
00:13:14,100 --> 00:13:18,650
Cái mà tôi đã mô tả ở đây, với hai lớp ẩn, mỗi lớp gồm 16 nơ-ron, 

196
00:13:18,650 --> 00:13:24,580
được chọn chủ yếu vì lý do thẩm mỹ, không tệ, phân loại chính xác khoảng 96% hình ảnh 

197
00:13:24,580 --> 00:13:25,960
mới mà nó nhìn thấy.

198
00:13:26,680 --> 00:13:30,317
Và thành thật mà nói, nếu bạn nhìn vào một số ví dụ mà nó gây nhầm lẫn, 

199
00:13:30,317 --> 00:13:32,540
bạn cảm thấy buộc phải cắt giảm nó một chút.

200
00:13:36,220 --> 00:13:39,837
Bây giờ nếu bạn thử nghiệm với cấu trúc lớp ẩn và thực hiện một vài chỉnh sửa, 

201
00:13:39,837 --> 00:13:41,760
bạn có thể đạt được tỷ lệ này lên tới 98%.

202
00:13:41,760 --> 00:13:42,720
Và điều đó khá tốt!

203
00:13:43,020 --> 00:13:46,680
Nó không phải là tốt nhất, bạn chắc chắn có thể có được hiệu suất tốt hơn 

204
00:13:46,680 --> 00:13:49,153
bằng cách phức tạp hơn mạng vanilla đơn giản này, 

205
00:13:49,153 --> 00:13:51,576
nhưng với nhiệm vụ ban đầu khó khăn như thế nào, 

206
00:13:51,576 --> 00:13:55,286
tôi nghĩ có điều gì đó đáng kinh ngạc về bất kỳ mạng nào hoạt động tốt như 

207
00:13:55,286 --> 00:13:57,957
vậy trên các hình ảnh mà nó chưa từng thấy trước đây, 

208
00:13:57,957 --> 00:14:01,420
vì điều đó chúng tôi chưa bao giờ nói cụ thể với nó những mẫu cần tìm.

209
00:14:02,560 --> 00:14:06,166
Ban đầu, cách tôi thúc đẩy cấu trúc này là bằng cách mô tả niềm hy vọng mà 

210
00:14:06,166 --> 00:14:09,389
chúng ta có thể có, rằng lớp thứ hai có thể thu được các cạnh nhỏ, 

211
00:14:09,389 --> 00:14:13,717
lớp thứ ba sẽ ghép các cạnh đó lại với nhau để nhận ra các vòng lặp và các đường dài hơn, 

212
00:14:13,717 --> 00:14:17,180
và rằng chúng có thể được ghép lại với nhau. cùng nhau nhận biết chữ số.

213
00:14:17,960 --> 00:14:20,400
Vậy đây có phải là điều mà mạng lưới của chúng ta thực sự đang làm?

214
00:14:21,080 --> 00:14:24,400
Vâng, ít nhất là đối với điều này thì không hề.

215
00:14:24,820 --> 00:14:28,766
Hãy nhớ video trước chúng ta đã xem xét trọng số của các kết nối từ tất cả các 

216
00:14:28,766 --> 00:14:32,913
nơ-ron ở lớp đầu tiên đến một nơ-ron nhất định ở lớp thứ hai có thể được hình dung 

217
00:14:32,913 --> 00:14:37,060
như thế nào dưới dạng một mẫu pixel nhất định mà nơ-ron lớp thứ hai đang tiếp nhận?

218
00:14:37,780 --> 00:14:42,980
Chà, khi chúng tôi thực sự làm điều đó đối với các trọng số liên quan đến những chuyển 

219
00:14:42,980 --> 00:14:48,300
đổi này, từ lớp đầu tiên sang lớp tiếp theo, thay vì chọn các cạnh nhỏ biệt lập ở đây và 

220
00:14:48,300 --> 00:14:53,680
ở đó, chúng trông gần như ngẫu nhiên, chỉ với một số mẫu rất lỏng lẻo trong chính giữa đó.

221
00:14:53,760 --> 00:14:57,610
Có vẻ như trong không gian rộng lớn không thể đo lường được 13.000 chiều với 

222
00:14:57,610 --> 00:15:01,310
các trọng số và độ lệch có thể xảy ra, mạng của chúng tôi đã tìm thấy một 

223
00:15:01,310 --> 00:15:05,760
mức tối thiểu cục bộ nhỏ đáng mừng, mặc dù đã phân loại thành công hầu hết các hình ảnh, 

224
00:15:05,760 --> 00:15:08,960
nhưng không chọn chính xác các mẫu mà chúng tôi có thể mong đợi.

225
00:15:09,780 --> 00:15:11,800
Và để thực sự hiểu được điểm này, hãy xem điều 

226
00:15:11,800 --> 00:15:13,820
gì xảy ra khi bạn nhập một hình ảnh ngẫu nhiên.

227
00:15:14,320 --> 00:15:18,307
Nếu hệ thống thông minh, bạn có thể cho rằng nó sẽ có cảm giác không chắc chắn, 

228
00:15:18,307 --> 00:15:22,246
có thể không thực sự kích hoạt bất kỳ nơ-ron đầu ra nào trong số 10 nơ-ron đầu 

229
00:15:22,246 --> 00:15:25,835
ra đó hoặc kích hoạt tất cả chúng một cách đồng đều, nhưng thay vào đó, 

230
00:15:25,835 --> 00:15:28,527
nó tự tin đưa ra cho bạn một số câu trả lời vô nghĩa, 

231
00:15:28,527 --> 00:15:32,365
như thể nó cảm thấy chắc chắn rằng tiếng ồn ngẫu nhiên này là số 5 vì nó cho 

232
00:15:32,365 --> 00:15:34,160
thấy hình ảnh thực của số 5 là số 5.

233
00:15:34,540 --> 00:15:38,868
Nói theo cách khác, ngay cả khi mạng này có thể nhận dạng các chữ số khá tốt, 

234
00:15:38,868 --> 00:15:40,700
nó cũng không biết cách vẽ chúng.

235
00:15:41,420 --> 00:15:45,240
Phần lớn điều này là do đây là một thiết lập đào tạo bị ràng buộc chặt chẽ.

236
00:15:45,880 --> 00:15:47,740
Ý tôi là, hãy đặt mình vào vị trí của mạng lưới ở đây.

237
00:15:48,140 --> 00:15:51,353
Theo quan điểm của nó, toàn bộ vũ trụ không bao gồm gì ngoài những chữ số 

238
00:15:51,353 --> 00:15:54,132
bất động được xác định rõ ràng tập trung vào một mạng lưới nhỏ, 

239
00:15:54,132 --> 00:15:57,389
và hàm chi phí của nó không bao giờ mang lại cho nó bất kỳ động lực nào để 

240
00:15:57,389 --> 00:16:01,080
trở thành bất cứ thứ gì ngoại trừ sự hoàn toàn tin tưởng vào các quyết định của mình.

241
00:16:02,120 --> 00:16:05,480
Vì vậy, với đây là hình ảnh về những gì các nơ-ron lớp thứ hai thực sự đang làm, 

242
00:16:05,480 --> 00:16:08,011
bạn có thể thắc mắc tại sao tôi lại giới thiệu mạng lưới này 

243
00:16:08,011 --> 00:16:09,920
với động cơ là tìm hiểu các cạnh và khuôn mẫu.

244
00:16:09,920 --> 00:16:12,300
Ý tôi là, đó hoàn toàn không phải là điều nó sẽ làm.

245
00:16:13,380 --> 00:16:17,180
Chà, đây không phải là mục tiêu cuối cùng của chúng tôi mà thay vào đó là điểm khởi đầu.

246
00:16:17,640 --> 00:16:21,902
Thành thật mà nói, đây là công nghệ cũ, loại được nghiên cứu vào những năm 80 và 90, 

247
00:16:21,902 --> 00:16:26,114
và bạn cần phải hiểu nó trước khi có thể hiểu các biến thể hiện đại chi tiết hơn và 

248
00:16:26,114 --> 00:16:28,922
rõ ràng nó có khả năng giải quyết một số vấn đề thú vị, 

249
00:16:28,922 --> 00:16:33,185
nhưng bạn càng đào sâu vào những gì những lớp ẩn đó thực sự đang hoạt động thì có vẻ 

250
00:16:33,185 --> 00:16:34,740
như nó càng kém thông minh hơn.

251
00:16:38,480 --> 00:16:41,955
Chuyển trọng tâm trong giây lát từ cách mạng học sang cách bạn học, 

252
00:16:41,955 --> 00:16:46,300
điều đó sẽ chỉ xảy ra nếu bạn tương tác tích cực với tài liệu ở đây bằng cách nào đó.

253
00:16:47,060 --> 00:16:51,575
Một điều khá đơn giản mà tôi muốn bạn làm là tạm dừng ngay bây giờ và suy nghĩ sâu 

254
00:16:51,575 --> 00:16:56,146
sắc một chút về những thay đổi bạn có thể thực hiện đối với hệ thống này và cách hệ 

255
00:16:56,146 --> 00:17:00,880
thống cảm nhận hình ảnh nếu bạn muốn nó tiếp thu tốt hơn những thứ như cạnh và hoa văn.

256
00:17:01,480 --> 00:17:04,377
Nhưng tốt hơn thế, để thực sự tương tác với tài liệu, 

257
00:17:04,377 --> 00:17:09,099
tôi đặc biệt giới thiệu cuốn sách của Michael Nielsen về học sâu và mạng lưới thần kinh.

258
00:17:09,680 --> 00:17:14,048
Trong đó, bạn có thể tìm thấy mã và dữ liệu để tải xuống và sử dụng cho ví dụ 

259
00:17:14,048 --> 00:17:18,359
chính xác này và cuốn sách sẽ hướng dẫn bạn từng bước về chức năng của mã đó.

260
00:17:19,300 --> 00:17:22,304
Điều tuyệt vời là cuốn sách này được cung cấp miễn phí và công khai, 

261
00:17:22,304 --> 00:17:24,481
vì vậy nếu bạn nhận được điều gì đó từ cuốn sách, 

262
00:17:24,481 --> 00:17:27,660
hãy cân nhắc tham gia cùng tôi để quyên góp cho những nỗ lực của Nielsen.

263
00:17:27,660 --> 00:17:32,024
Tôi cũng đã liên kết một số tài nguyên khác mà tôi rất thích trong phần mô tả, 

264
00:17:32,024 --> 00:17:36,500
bao gồm bài đăng blog hay và ấn tượng của Chris Ola và các bài viết trên Distill.

265
00:17:38,280 --> 00:17:40,511
Để kết thúc mọi chuyện ở đây trong vài phút vừa qua, 

266
00:17:40,511 --> 00:17:43,880
tôi muốn quay lại một đoạn trong cuộc phỏng vấn tôi đã thực hiện với Leisha Lee.

267
00:17:44,300 --> 00:17:47,720
Bạn có thể nhớ đến cô ấy từ video trước, cô ấy đã làm luận án tiến sĩ về học sâu.

268
00:17:48,300 --> 00:17:52,040
Trong đoạn trích nhỏ này, cô ấy nói về hai bài báo gần đây thực sự đi sâu 

269
00:17:52,040 --> 00:17:55,780
vào cách một số mạng nhận dạng hình ảnh hiện đại hơn đang thực sự học hỏi.

270
00:17:56,120 --> 00:17:58,414
Để xác định vị trí của chúng tôi trong cuộc trò chuyện, 

271
00:17:58,414 --> 00:18:01,569
bài báo đầu tiên đã sử dụng một trong những mạng lưới thần kinh đặc biệt sâu 

272
00:18:01,569 --> 00:18:04,724
có khả năng nhận dạng hình ảnh thực sự tốt và thay vì huấn luyện nó trên một 

273
00:18:04,724 --> 00:18:07,879
tập dữ liệu được dán nhãn chính xác, hãy xáo trộn tất cả các nhãn xung quanh 

274
00:18:07,879 --> 00:18:08,740
trước khi huấn luyện.

275
00:18:09,480 --> 00:18:12,925
Rõ ràng độ chính xác của thử nghiệm ở đây không tốt hơn ngẫu nhiên, 

276
00:18:12,925 --> 00:18:16,624
vì mọi thứ chỉ được gắn nhãn ngẫu nhiên, nhưng nó vẫn có thể đạt được độ 

277
00:18:16,624 --> 00:18:20,880
chính xác huấn luyện giống như bạn làm trên một tập dữ liệu được gắn nhãn chính xác.

278
00:18:21,600 --> 00:18:26,410
Về cơ bản, hàng triệu trọng số cho mạng cụ thể này là đủ để nó chỉ ghi nhớ dữ 

279
00:18:26,410 --> 00:18:31,405
liệu ngẫu nhiên, điều này đặt ra câu hỏi liệu việc giảm thiểu hàm chi phí này có 

280
00:18:31,405 --> 00:18:36,400
thực sự tương ứng với bất kỳ loại cấu trúc nào trong hình ảnh hay chỉ là ghi nhớ?

281
00:18:51,440 --> 00:18:56,631
Nếu bạn nhìn vào đường cong chính xác đó, nếu bạn chỉ đang đào tạo trên một tập 

282
00:18:56,631 --> 00:19:02,406
dữ liệu ngẫu nhiên, thì đường cong đó sẽ đi xuống rất chậm theo kiểu gần như tuyến tính, 

283
00:19:02,406 --> 00:19:07,532
vì vậy bạn thực sự đang gặp khó khăn để tìm ra mức tối thiểu cục bộ có thể có, 

284
00:19:07,532 --> 00:19:12,140
bạn biết đấy , trọng lượng phù hợp sẽ giúp bạn có được độ chính xác đó.

285
00:19:12,240 --> 00:19:16,557
Trong khi đó, nếu bạn thực sự đang đào tạo trên một tập dữ liệu có cấu trúc, 

286
00:19:16,557 --> 00:19:20,370
một tập dữ liệu có nhãn phù hợp, bạn sẽ loay hoay một chút lúc đầu, 

287
00:19:20,370 --> 00:19:24,182
nhưng sau đó bạn đã giảm rất nhanh để đạt được mức độ chính xác đó, 

288
00:19:24,182 --> 00:19:28,220
và theo một nghĩa nào đó, nó việc tìm cực đại địa phương đó dễ dàng hơn.

289
00:19:28,540 --> 00:19:33,293
Và điều thú vị ở đây là nó đưa ra ánh sáng một bài báo khác từ vài năm trước, 

290
00:19:33,293 --> 00:19:36,645
trong đó có nhiều sự đơn giản hóa hơn về các lớp mạng, 

291
00:19:36,645 --> 00:19:41,521
nhưng một trong những kết quả là nói rằng nếu bạn nhìn vào bối cảnh tối ưu hóa, 

292
00:19:41,521 --> 00:19:47,006
mức tối thiểu cục bộ mà các mạng này có xu hướng tìm hiểu thực sự có chất lượng như nhau, 

293
00:19:47,006 --> 00:19:51,150
vì vậy, theo một nghĩa nào đó, nếu tập dữ liệu của bạn có cấu trúc, 

294
00:19:51,150 --> 00:19:54,320
bạn sẽ có thể tìm thấy dữ liệu đó dễ dàng hơn nhiều.

295
00:19:58,160 --> 00:20:01,180
Như mọi khi, tôi xin gửi lời cảm ơn tới những người đã ủng hộ trên Patreon.

296
00:20:01,520 --> 00:20:04,221
Tôi đã từng nói Patreon là công cụ thay đổi cuộc chơi nhưng những 

297
00:20:04,221 --> 00:20:06,800
video này thực sự sẽ không thể thực hiện được nếu không có bạn.

298
00:20:07,460 --> 00:20:09,948
Tôi cũng muốn gửi lời cảm ơn đặc biệt đến công ty Amplify 

299
00:20:09,948 --> 00:20:12,780
Partners của VC vì đã hỗ trợ những video đầu tiên trong chuỗi này.

