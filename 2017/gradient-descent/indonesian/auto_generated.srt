1
00:00:00,000 --> 00:00:07,240
Video terakhir saya memaparkan struktur jaringan saraf.

2
00:00:07,240 --> 00:00:11,560
Saya akan memberikan rekap singkatnya di sini agar segar dalam ingatan

3
00:00:11,560 --> 00:00:13,160
kita, dan kemudian saya memiliki dua tujuan utama untuk video ini.

4
00:00:13,160 --> 00:00:17,960
Yang pertama adalah memperkenalkan gagasan penurunan gradien, yang tidak hanya mendasari cara

5
00:00:17,960 --> 00:00:20,800
jaringan saraf belajar, tetapi juga cara kerja banyak pembelajaran mesin lainnya.

6
00:00:20,800 --> 00:00:25,160
Kemudian setelah itu kita akan menggali lebih jauh tentang bagaimana kinerja jaringan

7
00:00:25,160 --> 00:00:29,560
ini, dan apa yang akhirnya dicari oleh lapisan neuron tersembunyi tersebut.

8
00:00:29,560 --> 00:00:34,680
Sebagai pengingat, tujuan kami di sini adalah contoh klasik

9
00:00:34,680 --> 00:00:37,080
pengenalan angka tulisan tangan, halo dunia jaringan saraf.

10
00:00:37,080 --> 00:00:42,160
Digit-digit ini dirender pada grid 28x28 piksel, masing-masing piksel

11
00:00:42,160 --> 00:00:44,260
memiliki nilai skala abu-abu antara 0 dan 1.

12
00:00:44,260 --> 00:00:51,400
Hal itulah yang menentukan aktivasi 784 neuron di lapisan input jaringan.

13
00:00:51,400 --> 00:00:56,880
Aktivasi setiap neuron pada lapisan berikutnya didasarkan pada jumlah tertimbang semua

14
00:00:56,880 --> 00:01:02,300
aktivasi pada lapisan sebelumnya, ditambah beberapa bilangan khusus yang disebut bias.

15
00:01:02,300 --> 00:01:07,480
Anda menyusun jumlah tersebut dengan beberapa fungsi lain, seperti squishification

16
00:01:07,480 --> 00:01:09,640
sigmoid, atau ReLU, seperti yang saya lihat di video sebelumnya.

17
00:01:09,640 --> 00:01:14,960
Secara total, mengingat pilihan dua lapisan tersembunyi dengan masing-masing 16 neuron,

18
00:01:14,960 --> 00:01:20,940
jaringan memiliki sekitar 13.000 bobot dan bias yang dapat kita sesuaikan,

19
00:01:20,940 --> 00:01:25,320
dan nilai inilah yang menentukan apa sebenarnya yang dilakukan jaringan.

20
00:01:25,320 --> 00:01:29,800
Dan yang kami maksud ketika kami mengatakan bahwa jaringan ini mengklasifikasikan digit tertentu adalah

21
00:01:29,800 --> 00:01:34,080
bahwa yang paling terang dari 10 neuron di lapisan terakhir berhubungan dengan digit tersebut.

22
00:01:34,080 --> 00:01:39,240
Dan ingat, motivasi yang ada dalam pikiran kita untuk struktur

23
00:01:39,240 --> 00:01:43,920
berlapis adalah mungkin lapisan kedua dapat mengambil bagian tepinya, lapisan

24
00:01:43,920 --> 00:01:48,640
ketiga mungkin mengambil pola seperti lingkaran dan garis, dan lapisan

25
00:01:48,640 --> 00:01:49,640
terakhir dapat menyatukan pola-pola tersebut menjadi satu. mengenali angka.

26
00:01:49,640 --> 00:01:52,880
Jadi di sini, kita mempelajari bagaimana jaringan belajar.

27
00:01:52,880 --> 00:01:56,880
Apa yang kami inginkan adalah sebuah algoritma dimana Anda dapat menunjukkan jaringan ini

28
00:01:56,880 --> 00:02:01,540
sejumlah besar data pelatihan, yang datang dalam bentuk sekumpulan gambar angka tulisan

29
00:02:01,540 --> 00:02:06,360
tangan yang berbeda, bersama dengan label untuk apa yang seharusnya, dan itu akan

30
00:02:06,360 --> 00:02:10,760
menyesuaikan 13.000 bobot dan bias tersebut untuk meningkatkan kinerjanya pada data pelatihan.

31
00:02:10,760 --> 00:02:15,540
Mudah-mudahan struktur berlapis ini berarti bahwa apa yang dipelajari

32
00:02:15,540 --> 00:02:17,840
dapat digeneralisasi menjadi gambar di luar data pelatihan tersebut.

33
00:02:17,840 --> 00:02:22,240
Cara kami mengujinya adalah setelah Anda melatih jaringan, Anda menampilkan lebih banyak

34
00:02:22,240 --> 00:02:31,160
data berlabel, dan Anda melihat seberapa akurat jaringan mengklasifikasikan gambar-gambar baru tersebut.

35
00:02:31,160 --> 00:02:34,760
Untungnya bagi kami, dan apa yang membuat ini menjadi contoh umum, adalah

36
00:02:34,760 --> 00:02:39,520
bahwa orang-orang baik di belakang database MNIST telah mengumpulkan puluhan ribu

37
00:02:39,520 --> 00:02:45,080
gambar digit tulisan tangan, masing-masing diberi label dengan angka yang seharusnya.

38
00:02:45,080 --> 00:02:49,920
Dan meskipun provokatif untuk mendeskripsikan mesin sebagai pembelajaran, begitu Anda melihat cara kerjanya,

39
00:02:49,920 --> 00:02:55,560
rasanya tidak seperti premis fiksi ilmiah yang gila, dan lebih seperti latihan kalkulus.

40
00:02:55,560 --> 00:03:01,040
Maksud saya, pada dasarnya ini adalah menemukan fungsi minimum tertentu.

41
00:03:01,040 --> 00:03:06,480
Ingat, secara konseptual kita menganggap setiap neuron terhubung ke semua

42
00:03:06,480 --> 00:03:11,440
neuron di lapisan sebelumnya, dan bobot dalam jumlah tertimbang yang

43
00:03:11,440 --> 00:03:16,400
menentukan aktivasinya mirip dengan kekuatan koneksi tersebut, dan bias adalah

44
00:03:16,400 --> 00:03:19,780
indikasi dari apakah neuron tersebut cenderung aktif atau tidak aktif.

45
00:03:19,780 --> 00:03:23,300
Dan sebagai permulaan, kita hanya akan menginisialisasi

46
00:03:23,300 --> 00:03:25,020
semua bobot dan bias tersebut secara acak.

47
00:03:25,020 --> 00:03:29,100
Tak perlu dikatakan lagi, jaringan ini akan berkinerja buruk pada contoh

48
00:03:29,100 --> 00:03:31,180
pelatihan yang diberikan, karena jaringan ini hanya melakukan sesuatu yang acak.

49
00:03:31,180 --> 00:03:36,820
Misalnya, Anda memasukkan gambar 3 ini, dan lapisan keluarannya terlihat berantakan.

50
00:03:36,820 --> 00:03:43,340
Jadi yang Anda lakukan adalah mendefinisikan fungsi biaya, cara untuk memberi tahu komputer, tidak, komputer buruk,

51
00:03:43,340 --> 00:03:48,940
bahwa output harus memiliki aktivasi sebesar 0 untuk sebagian besar neuron, tetapi 1 untuk neuron ini.

52
00:03:48,980 --> 00:03:51,740
Apa yang kamu berikan padaku benar-benar sampah.

53
00:03:51,740 --> 00:03:56,740
Untuk mengatakannya secara lebih matematis, Anda menjumlahkan kuadrat perbedaan antara

54
00:03:56,740 --> 00:04:01,980
masing-masing aktivasi keluaran sampah tersebut dan nilai yang Anda inginkan,

55
00:04:01,980 --> 00:04:06,020
dan inilah yang kami sebut sebagai biaya satu contoh pelatihan.

56
00:04:06,020 --> 00:04:12,660
Perhatikan bahwa jumlah ini kecil ketika jaringan dengan percaya diri mengklasifikasikan gambar dengan

57
00:04:12,660 --> 00:04:18,820
benar, namun menjadi besar ketika jaringan sepertinya tidak mengetahui apa yang dilakukannya.

58
00:04:18,820 --> 00:04:23,860
Jadi yang Anda lakukan adalah mempertimbangkan biaya rata-rata

59
00:04:23,860 --> 00:04:27,580
dari puluhan ribu contoh pelatihan yang Anda miliki.

60
00:04:27,580 --> 00:04:32,300
Biaya rata-rata ini adalah ukuran seberapa buruk

61
00:04:32,300 --> 00:04:33,300
jaringan tersebut, dan seberapa buruk kinerja komputer.

62
00:04:33,300 --> 00:04:35,300
Dan itu adalah hal yang rumit.

63
00:04:35,300 --> 00:04:40,380
Ingat bagaimana jaringan itu sendiri pada dasarnya adalah sebuah fungsi, yang mengambil 784

64
00:04:40,380 --> 00:04:46,100
angka sebagai masukan, nilai piksel, dan mengeluarkan 10 angka sebagai keluarannya, dan

65
00:04:46,100 --> 00:04:49,700
dalam arti tertentu jaringan tersebut diparameterisasi oleh semua bobot dan bias ini?

66
00:04:49,700 --> 00:04:53,340
Fungsi biaya juga merupakan lapisan kompleksitas.

67
00:04:53,340 --> 00:04:59,140
Dibutuhkan 13.000 atau lebih bobot dan bias sebagai masukan, dan mengeluarkan

68
00:04:59,140 --> 00:05:04,620
satu angka yang menggambarkan seberapa buruk bobot dan bias tersebut, dan

69
00:05:04,620 --> 00:05:09,140
cara mendefinisikannya bergantung pada perilaku jaringan pada puluhan ribu data pelatihan.

70
00:05:09,140 --> 00:05:12,460
Banyak hal yang perlu dipikirkan.

71
00:05:12,460 --> 00:05:16,380
Namun hanya memberi tahu komputer betapa buruknya pekerjaan yang dilakukannya tidaklah terlalu membantu.

72
00:05:16,380 --> 00:05:21,300
Anda ingin memberi tahu cara mengubah bobot dan bias tersebut agar menjadi lebih baik.

73
00:05:21,300 --> 00:05:25,580
Agar lebih mudah, daripada bersusah payah membayangkan suatu fungsi dengan 13.000 masukan, bayangkan saja

74
00:05:25,580 --> 00:05:31,440
sebuah fungsi sederhana yang memiliki satu bilangan sebagai masukan dan satu bilangan sebagai keluaran.

75
00:05:31,440 --> 00:05:36,420
Bagaimana cara menemukan masukan yang meminimalkan nilai fungsi ini?

76
00:05:36,420 --> 00:05:41,300
Siswa kalkulus akan mengetahui bahwa terkadang Anda dapat mengetahui jumlah minimum tersebut secara eksplisit, namun

77
00:05:41,340 --> 00:05:46,620
hal tersebut tidak selalu dapat dilakukan untuk fungsi yang sangat rumit, tentunya tidak dalam

78
00:05:46,620 --> 00:05:51,640
versi masukan 13.000 dari situasi ini untuk fungsi biaya jaringan saraf yang sangat rumit.

79
00:05:51,640 --> 00:05:56,820
Taktik yang lebih fleksibel adalah memulai dari masukan apa pun, dan mencari

80
00:05:56,820 --> 00:05:59,860
tahu arah mana yang harus Anda ambil untuk menurunkan keluaran tersebut.

81
00:05:59,860 --> 00:06:05,020
Khususnya, jika Anda dapat mengetahui kemiringan fungsi di tempat

82
00:06:05,020 --> 00:06:09,280
Anda berada, geser ke kiri jika kemiringan tersebut positif,

83
00:06:09,280 --> 00:06:12,720
dan geser input ke kanan jika kemiringan tersebut negatif.

84
00:06:12,720 --> 00:06:17,040
Jika Anda melakukan ini berulang kali, pada setiap titik memeriksa kemiringan baru

85
00:06:17,040 --> 00:06:20,680
dan mengambil langkah yang tepat, Anda akan mendekati fungsi minimum lokal.

86
00:06:20,680 --> 00:06:24,600
Dan gambaran yang mungkin Anda bayangkan di sini adalah sebuah bola yang menggelinding menuruni bukit.

87
00:06:24,600 --> 00:06:29,380
Dan perhatikan, bahkan untuk fungsi masukan tunggal yang sangat disederhanakan ini, ada

88
00:06:29,380 --> 00:06:34,220
banyak kemungkinan lembah yang mungkin Anda masuki, bergantung pada masukan acak mana

89
00:06:34,220 --> 00:06:38,460
yang Anda mulai, dan tidak ada jaminan bahwa nilai minimum lokal tempat

90
00:06:38,460 --> 00:06:39,460
Anda mendarat akan menjadi nilai terkecil yang mungkin. dari fungsi biaya.

91
00:06:39,460 --> 00:06:43,180
Hal ini juga akan terbawa ke kasus jaringan saraf kita.

92
00:06:43,180 --> 00:06:48,140
Dan saya juga ingin Anda memperhatikan bagaimana jika Anda membuat ukuran langkah

93
00:06:48,140 --> 00:06:52,920
Anda proporsional dengan kemiringan, maka ketika kemiringannya mendatar ke arah minimum, langkah

94
00:06:52,920 --> 00:06:56,020
Anda akan semakin kecil, dan hal ini membantu Anda menghindari overshooting.

95
00:06:56,020 --> 00:07:01,640
Untuk menambah kerumitannya, bayangkan sebuah fungsi dengan dua masukan dan satu keluaran.

96
00:07:01,640 --> 00:07:06,360
Anda mungkin menganggap ruang masukan sebagai bidang xy,

97
00:07:06,360 --> 00:07:09,020
dan fungsi biaya digambarkan sebagai permukaan di atasnya.

98
00:07:09,020 --> 00:07:13,600
Daripada bertanya tentang kemiringan suatu fungsi, Anda harus menanyakan ke arah mana Anda

99
00:07:13,600 --> 00:07:19,780
harus melangkah dalam ruang masukan ini agar keluaran fungsi dapat diturunkan paling cepat.

100
00:07:19,780 --> 00:07:22,340
Dengan kata lain, ke arah mana arah menurunnya?

101
00:07:22,340 --> 00:07:26,740
Dan sekali lagi, ada gunanya membayangkan sebuah bola menggelinding menuruni bukit itu.

102
00:07:26,740 --> 00:07:31,920
Bagi Anda yang familiar dengan kalkulus multivariabel pasti tahu bahwa

103
00:07:31,920 --> 00:07:37,460
gradien suatu fungsi memberi Anda arah kenaikan paling curam, arah

104
00:07:37,460 --> 00:07:39,420
mana yang harus Anda ambil untuk meningkatkan fungsi paling cepat.

105
00:07:39,420 --> 00:07:43,820
Tentu saja, mengambil nilai negatif dari gradien tersebut memberi

106
00:07:43,820 --> 00:07:47,460
Anda arah ke langkah yang menurunkan fungsi paling cepat.

107
00:07:47,460 --> 00:07:52,320
Lebih dari itu, panjang vektor gradien ini merupakan

108
00:07:52,320 --> 00:07:54,580
indikasi seberapa curam lereng paling curam tersebut.

109
00:07:54,580 --> 00:07:58,080
Sekarang jika Anda belum terbiasa dengan kalkulus multivariabel dan ingin mempelajari lebih lanjut,

110
00:07:58,080 --> 00:08:01,100
lihat beberapa pekerjaan yang saya lakukan untuk Khan Academy mengenai topik tersebut.

111
00:08:01,100 --> 00:08:05,680
Sejujurnya, yang penting bagi Anda dan saya saat ini adalah

112
00:08:05,680 --> 00:08:10,440
bahwa pada prinsipnya terdapat cara untuk menghitung vektor ini, vektor

113
00:08:10,440 --> 00:08:12,040
ini yang memberi tahu Anda arah menurun dan seberapa curamnya.

114
00:08:12,040 --> 00:08:17,280
Anda akan baik-baik saja jika hanya itu yang Anda ketahui dan Anda tidak terlalu yakin dengan detailnya.

115
00:08:17,280 --> 00:08:21,440
Karena kalau bisa, algoritma untuk meminimalkan fungsinya adalah dengan menghitung arah gradien

116
00:08:21,440 --> 00:08:27,400
ini, lalu mengambil langkah kecil menuruni bukit, dan mengulanginya berulang kali.

117
00:08:28,300 --> 00:08:33,700
Itu ide dasar yang sama untuk fungsi yang memiliki 13.000 masukan, bukan 2 masukan.

118
00:08:33,700 --> 00:08:38,980
Bayangkan mengatur 13.000 bobot dan bias jaringan

119
00:08:38,980 --> 00:08:40,180
kita ke dalam vektor kolom raksasa.

120
00:08:40,180 --> 00:08:46,140
Gradien negatif dari fungsi biaya hanyalah sebuah vektor, ini adalah suatu arah di

121
00:08:46,140 --> 00:08:51,660
dalam ruang masukan yang sangat besar ini yang memberi tahu Anda dorongan mana

122
00:08:51,660 --> 00:08:55,900
ke semua angka tersebut yang akan menyebabkan penurunan paling cepat pada fungsi biaya.

123
00:08:55,900 --> 00:09:00,000
Dan tentu saja, dengan fungsi biaya yang dirancang khusus, mengubah bobot dan

124
00:09:00,000 --> 00:09:05,520
bias menjadi lebih kecil berarti membuat output jaringan pada setiap bagian

125
00:09:05,520 --> 00:09:10,280
data pelatihan tidak terlihat seperti array acak yang terdiri dari 10 nilai,

126
00:09:10,280 --> 00:09:11,280
dan lebih seperti keputusan sebenarnya yang kita inginkan. itu untuk dibuat.

127
00:09:11,280 --> 00:09:15,940
Penting untuk diingat, fungsi biaya ini melibatkan rata-rata seluruh data pelatihan, jadi

128
00:09:15,940 --> 00:09:24,260
jika Anda meminimalkannya, artinya performanya lebih baik pada semua sampel tersebut.

129
00:09:24,260 --> 00:09:28,540
Algoritme untuk menghitung gradien ini secara efisien, yang secara efektif

130
00:09:28,540 --> 00:09:32,520
merupakan inti dari cara jaringan saraf belajar, disebut propagasi mundur,

131
00:09:32,520 --> 00:09:34,040
dan itulah yang akan saya bicarakan di video berikutnya.

132
00:09:34,040 --> 00:09:39,100
Di sana, saya benar-benar ingin meluangkan waktu untuk menelusuri apa yang sebenarnya terjadi

133
00:09:39,100 --> 00:09:44,100
pada setiap bobot dan bias untuk data pelatihan tertentu, mencoba memberikan gambaran intuitif

134
00:09:44,100 --> 00:09:47,980
tentang apa yang terjadi di luar tumpukan kalkulus dan rumus yang relevan.

135
00:09:47,980 --> 00:09:51,780
Di sini, saat ini, hal utama yang saya ingin Anda ketahui, terlepas

136
00:09:51,780 --> 00:09:56,820
dari detail implementasinya, adalah bahwa yang kita maksud ketika kita berbicara

137
00:09:56,820 --> 00:09:59,320
tentang pembelajaran jaringan adalah bahwa pembelajaran jaringan hanya meminimalkan fungsi biaya.

138
00:09:59,320 --> 00:10:02,760
Dan perhatikan, salah satu konsekuensi dari hal ini adalah penting

139
00:10:02,760 --> 00:10:07,820
agar fungsi biaya ini memiliki keluaran yang lancar, sehingga kita

140
00:10:07,820 --> 00:10:09,340
dapat menemukan nilai minimum lokal dengan mengambil sedikit langkah menurun.

141
00:10:09,340 --> 00:10:14,140
Inilah sebabnya mengapa neuron buatan memiliki aktivasi yang

142
00:10:14,140 --> 00:10:18,580
terus menerus, bukan hanya aktif atau tidak

143
00:10:18,580 --> 00:10:20,440
aktif secara biner, seperti halnya neuron biologis.

144
00:10:20,440 --> 00:10:24,600
Proses berulang kali mendorong input suatu fungsi dengan

145
00:10:24,600 --> 00:10:26,960
beberapa kelipatan gradien negatif disebut penurunan gradien.

146
00:10:26,960 --> 00:10:31,760
Ini adalah cara untuk menyatu menuju fungsi biaya minimum

147
00:10:31,760 --> 00:10:33,000
lokal, yang pada dasarnya merupakan lembah dalam grafik ini.

148
00:10:33,000 --> 00:10:37,040
Saya masih menampilkan gambar fungsi dengan dua masukan, tentu saja, karena

149
00:10:37,040 --> 00:10:41,480
dorongan dalam ruang masukan 13.000 dimensi agak sulit untuk dipahami, namun

150
00:10:41,480 --> 00:10:45,220
sebenarnya ada cara non-spasial yang bagus untuk memikirkan hal ini.

151
00:10:45,220 --> 00:10:49,100
Setiap komponen gradien negatif memberi tahu kita dua hal.

152
00:10:49,100 --> 00:10:53,600
Tandanya, tentu saja, memberi tahu kita apakah komponen

153
00:10:53,600 --> 00:10:55,860
vektor masukan yang bersesuaian harus dinaikkan atau diturunkan.

154
00:10:55,860 --> 00:11:01,340
Namun yang terpenting, besaran relatif dari semua komponen ini

155
00:11:01,340 --> 00:11:05,620
memberi tahu Anda perubahan mana yang lebih penting.

156
00:11:05,620 --> 00:11:09,780
Anda lihat, dalam jaringan kami, penyesuaian pada salah satu bobot mungkin memiliki dampak

157
00:11:09,780 --> 00:11:14,980
yang jauh lebih besar pada fungsi biaya dibandingkan penyesuaian pada bobot lainnya.

158
00:11:14,980 --> 00:11:19,440
Beberapa dari koneksi ini lebih penting untuk data pelatihan kami.

159
00:11:19,440 --> 00:11:23,520
Jadi, cara Anda memikirkan vektor gradien dari fungsi biaya yang sangat

160
00:11:23,520 --> 00:11:29,740
besar ini adalah dengan mengkodekan kepentingan relatif dari setiap bobot dan

161
00:11:29,740 --> 00:11:34,100
bias, yaitu, perubahan mana yang akan menghasilkan keuntungan paling besar.

162
00:11:34,100 --> 00:11:37,360
Ini sebenarnya hanyalah cara berpikir lain tentang arah.

163
00:11:37,360 --> 00:11:41,740
Untuk mengambil contoh yang lebih sederhana, jika Anda memiliki suatu fungsi dengan

164
00:11:41,740 --> 00:11:48,720
dua variabel sebagai masukan, dan menghitung bahwa gradiennya pada suatu titik tertentu

165
00:11:48,720 --> 00:11:52,880
adalah 3,1, maka di satu sisi Anda dapat menafsirkannya sebagai pernyataan bahwa

166
00:11:52,880 --> 00:11:57,400
ketika Anda berdiri di masukan tersebut, bergerak sepanjang arah ini akan meningkatkan

167
00:11:57,400 --> 00:12:02,200
fungsi paling cepat, sehingga ketika Anda membuat grafik fungsi di atas bidang

168
00:12:02,200 --> 00:12:03,200
titik masukan, vektor itulah yang memberi Anda arah lurus ke atas.

169
00:12:03,200 --> 00:12:07,600
Namun cara lain untuk membacanya adalah dengan mengatakan bahwa perubahan pada variabel pertama ini

170
00:12:07,600 --> 00:12:12,400
memiliki kepentingan tiga kali lipat dibandingkan perubahan pada variabel kedua, bahwa setidaknya di sekitar

171
00:12:12,400 --> 00:12:17,740
masukan yang relevan, mendorong nilai x akan membawa lebih banyak keuntungan bagi Anda. uang.

172
00:12:17,740 --> 00:12:22,880
Baiklah, mari kita perkecil dan simpulkan posisi kita sejauh ini.

173
00:12:22,880 --> 00:12:28,660
Jaringan itu sendiri adalah fungsi ini dengan 784 masukan dan

174
00:12:28,660 --> 00:12:30,860
10 keluaran, yang didefinisikan dalam bentuk semua jumlah tertimbang ini.

175
00:12:30,860 --> 00:12:34,160
Fungsi biaya juga merupakan lapisan kompleksitas.

176
00:12:34,160 --> 00:12:39,300
Dibutuhkan 13.000 bobot dan bias sebagai masukan, dan

177
00:12:39,300 --> 00:12:42,640
mengeluarkan satu ukuran keburukan berdasarkan contoh pelatihan.

178
00:12:42,640 --> 00:12:47,520
Gradien fungsi biaya masih merupakan satu lapisan kompleksitas lagi.

179
00:12:47,520 --> 00:12:52,860
Hal ini memberi tahu kita dorongan apa pada semua bobot dan bias

180
00:12:52,860 --> 00:12:56,640
ini yang menyebabkan perubahan tercepat pada nilai fungsi biaya, yang mungkin

181
00:12:56,640 --> 00:13:03,040
Anda tafsirkan sebagai perubahan mana pada bobot mana yang paling penting.

182
00:13:03,040 --> 00:13:07,620
Jadi, ketika Anda menginisialisasi jaringan dengan bobot dan bias acak,

183
00:13:07,620 --> 00:13:12,420
dan menyesuaikannya berkali-kali berdasarkan proses penurunan gradien ini, seberapa

184
00:13:12,420 --> 00:13:14,240
baik kinerjanya pada gambar yang belum pernah terlihat sebelumnya?

185
00:13:14,240 --> 00:13:19,000
Yang telah saya jelaskan di sini, dengan dua lapisan tersembunyi yang masing-masing terdiri dari 16 neuron,

186
00:13:19,000 --> 00:13:26,920
sebagian besar dipilih karena alasan estetika, lumayan, mengklasifikasikan sekitar 96% gambar baru yang dilihatnya dengan benar.

187
00:13:26,920 --> 00:13:31,580
Dan sejujurnya, jika Anda melihat beberapa contoh yang

188
00:13:31,580 --> 00:13:36,300
membuat kesalahan, Anda merasa harus menguranginya sedikit.

189
00:13:36,300 --> 00:13:40,220
Jika Anda bermain-main dengan struktur lapisan tersembunyi dan membuat

190
00:13:40,220 --> 00:13:41,220
beberapa penyesuaian, Anda bisa mendapatkan ini hingga 98%.

191
00:13:41,220 --> 00:13:42,900
Dan itu cukup bagus!

192
00:13:42,900 --> 00:13:47,020
Ini bukan yang terbaik, Anda pasti bisa mendapatkan kinerja yang lebih baik dengan menjadi lebih canggih

193
00:13:47,020 --> 00:13:52,460
daripada jaringan vanilla biasa ini, namun mengingat betapa beratnya tugas awalnya, menurut saya ada sesuatu yang

194
00:13:52,460 --> 00:13:56,800
luar biasa tentang jaringan mana pun yang melakukan hal ini dengan baik pada gambar yang belum

195
00:13:56,800 --> 00:14:02,000
pernah terlihat sebelumnya mengingat kami tidak pernah secara spesifik memberi tahu pola apa yang harus dicari.

196
00:14:02,000 --> 00:14:07,840
Awalnya, cara saya memotivasi struktur ini adalah dengan mendeskripsikan harapan yang mungkin

197
00:14:07,840 --> 00:14:11,880
kita miliki, bahwa lapisan kedua dapat menangkap tepi-tepi kecil, bahwa lapisan

198
00:14:11,880 --> 00:14:16,080
ketiga akan menyatukan tepi-tepi tersebut untuk mengenali loop dan garis-garis yang lebih

199
00:14:16,080 --> 00:14:18,220
panjang, dan agar tepi-tepi tersebut dapat disatukan. bersama-sama untuk mengenali angka.

200
00:14:18,220 --> 00:14:21,040
Jadi apakah ini yang sebenarnya dilakukan jaringan kita?

201
00:14:21,040 --> 00:14:24,880
Setidaknya untuk yang satu ini, tidak sama sekali.

202
00:14:24,960 --> 00:14:29,120
Ingat bagaimana video terakhir kita melihat bagaimana bobot koneksi dari semua

203
00:14:29,120 --> 00:14:33,900
neuron di lapisan pertama ke neuron tertentu di lapisan kedua dapat

204
00:14:33,900 --> 00:14:37,440
divisualisasikan sebagai pola piksel tertentu yang diambil oleh neuron lapisan kedua?

205
00:14:37,440 --> 00:14:44,600
Nah, ketika kita melakukan itu untuk bobot yang terkait dengan transisi ini,

206
00:14:44,600 --> 00:14:51,000
alih-alih mengambil tepi kecil yang terisolasi di sana-sini, bobot tersebut terlihat

207
00:14:51,000 --> 00:14:54,200
hampir acak, hanya dengan beberapa pola yang sangat longgar di tengahnya.

208
00:14:54,200 --> 00:14:59,020
Tampaknya dalam ruang 13.000 dimensi yang sangat besar dan berisi

209
00:14:59,020 --> 00:15:04,020
kemungkinan bobot dan bias, jaringan kami menemukan dirinya sebagai

210
00:15:04,020 --> 00:15:08,440
minimum lokal kecil yang menyenangkan, meskipun berhasil mengklasifikasikan sebagian

211
00:15:08,440 --> 00:15:09,840
besar gambar, tidak benar-benar menangkap pola yang kami harapkan.

212
00:15:09,840 --> 00:15:14,600
Dan untuk benar-benar memahami hal ini, perhatikan apa yang terjadi ketika Anda memasukkan gambar acak.

213
00:15:14,600 --> 00:15:19,240
Jika sistemnya cerdas, Anda mungkin mengira sistem tersebut akan terasa tidak pasti, mungkin tidak benar-benar mengaktifkan salah

214
00:15:19,240 --> 00:15:24,120
satu dari 10 neuron keluaran tersebut, atau mengaktifkan semuanya secara merata, namun sebaliknya sistem tersebut dengan

215
00:15:24,520 --> 00:15:29,800
percaya diri memberi Anda jawaban yang tidak masuk akal, seolah-olah sistem tersebut terasa yakin bahwa sistem ini

216
00:15:29,800 --> 00:15:34,560
bersifat acak. noise adalah angka 5 seperti halnya gambar sebenarnya dari angka 5 adalah angka 5.

217
00:15:34,560 --> 00:15:39,300
Dengan kata lain, meskipun jaringan ini dapat mengenali angka

218
00:15:39,300 --> 00:15:41,800
dengan cukup baik, ia tidak tahu cara menggambarnya.

219
00:15:41,800 --> 00:15:45,400
Hal ini sebagian besar disebabkan oleh pengaturan pelatihan yang sangat terbatas.

220
00:15:45,400 --> 00:15:48,220
Maksud saya, tempatkan diri Anda pada posisi jaringan di sini.

221
00:15:48,220 --> 00:15:53,280
Dari sudut pandangnya, seluruh alam semesta hanya terdiri dari angka-angka tak bergerak yang

222
00:15:53,280 --> 00:15:58,560
terdefinisi dengan jelas dan berpusat pada sebuah kotak kecil, dan fungsi biayanya

223
00:15:58,560 --> 00:16:02,160
tidak pernah memberinya insentif apa pun kecuali keyakinan penuh dalam mengambil keputusan.

224
00:16:02,160 --> 00:16:05,760
Jadi dengan gambaran tentang apa yang sebenarnya dilakukan oleh neuron

225
00:16:05,760 --> 00:16:09,320
lapisan kedua tersebut, Anda mungkin bertanya-tanya mengapa saya memperkenalkan

226
00:16:09,320 --> 00:16:10,320
jaringan ini dengan motivasi untuk memahami tepian dan pola.

227
00:16:10,320 --> 00:16:13,040
Maksudku, bukan itu yang akhirnya terjadi.

228
00:16:13,040 --> 00:16:17,480
Ya, ini bukan dimaksudkan sebagai tujuan akhir kita, melainkan sebuah titik awal.

229
00:16:17,480 --> 00:16:22,280
Sejujurnya, ini adalah teknologi lama, jenis yang diteliti pada tahun 80an dan 90an, dan

230
00:16:22,280 --> 00:16:26,920
Anda perlu memahaminya sebelum Anda dapat memahami varian modern yang lebih detail, dan

231
00:16:26,920 --> 00:16:31,380
teknologi ini jelas mampu memecahkan beberapa masalah menarik, tetapi semakin Anda menggali apa

232
00:16:31,380 --> 00:16:38,720
yang ada di dalamnya. lapisan-lapisan tersembunyi itu benar-benar berfungsi, semakin tidak cerdas kelihatannya.

233
00:16:38,720 --> 00:16:43,540
Mengalihkan fokus sejenak dari cara jaringan belajar ke cara Anda belajar, itu

234
00:16:43,540 --> 00:16:47,160
hanya akan terjadi jika Anda terlibat secara aktif dengan materi di sini.

235
00:16:47,160 --> 00:16:51,920
Satu hal yang cukup sederhana yang saya ingin Anda lakukan adalah berhenti sejenak dan berpikir sejenak

236
00:16:51,920 --> 00:16:57,560
tentang perubahan apa yang mungkin Anda lakukan pada sistem ini dan bagaimana sistem ini memandang

237
00:16:57,560 --> 00:17:01,880
gambar jika Anda ingin sistem ini menangkap hal-hal seperti tepian dan pola dengan lebih baik.

238
00:17:01,880 --> 00:17:06,360
Namun lebih baik dari itu, untuk benar-benar memahami materi, saya sangat

239
00:17:06,360 --> 00:17:09,720
merekomendasikan buku karya Michael Nielsen tentang pembelajaran mendalam dan jaringan saraf.

240
00:17:09,720 --> 00:17:15,200
Di dalamnya, Anda dapat menemukan kode dan data untuk diunduh dan dimainkan untuk contoh persis ini,

241
00:17:15,200 --> 00:17:19,360
dan buku ini akan memandu Anda langkah demi langkah tentang apa yang dilakukan kode tersebut.

242
00:17:19,360 --> 00:17:23,920
Yang luar biasa adalah buku ini gratis dan tersedia untuk umum, jadi jika Anda

243
00:17:23,920 --> 00:17:28,040
mendapatkan manfaat darinya, pertimbangkan untuk bergabung dengan saya dalam memberikan donasi untuk upaya Nielsen.

244
00:17:28,040 --> 00:17:32,060
Saya juga menautkan beberapa sumber lain yang sangat saya sukai dalam deskripsi, termasuk

245
00:17:32,060 --> 00:17:38,720
postingan blog yang fenomenal dan indah oleh Chris Ola dan artikel di Distill.

246
00:17:38,720 --> 00:17:41,960
Untuk menutup beberapa menit terakhir di sini, saya ingin

247
00:17:41,960 --> 00:17:44,440
kembali ke cuplikan wawancara saya dengan Leisha Lee.

248
00:17:44,440 --> 00:17:48,520
Anda mungkin ingat dia dari video terakhir, dia menyelesaikan pekerjaan PhD-nya dalam pembelajaran mendalam.

249
00:17:48,560 --> 00:17:52,240
Dalam cuplikan kecil ini, dia berbicara tentang dua makalah terbaru yang

250
00:17:52,240 --> 00:17:56,380
benar-benar menggali bagaimana beberapa jaringan pengenalan gambar modern sebenarnya belajar.

251
00:17:56,380 --> 00:18:00,320
Sekadar untuk mengatur posisi kita dalam percakapan, makalah pertama mengambil salah satu dari jaringan

252
00:18:00,320 --> 00:18:04,480
saraf dalam yang sangat bagus dalam pengenalan gambar, dan alih-alih melatihnya pada kumpulan

253
00:18:04,480 --> 00:18:09,400
data yang diberi label dengan benar, makalah ini mengacak semua label sebelum pelatihan.

254
00:18:09,400 --> 00:18:13,840
Tentu saja keakuratan pengujian di sini tidak akan lebih baik

255
00:18:13,840 --> 00:18:15,320
daripada pengujian acak, karena semuanya hanya diberi label secara acak.

256
00:18:15,320 --> 00:18:20,080
Namun ia masih dapat mencapai akurasi pelatihan yang sama seperti yang

257
00:18:20,080 --> 00:18:21,440
Anda lakukan pada kumpulan data yang diberi label dengan benar.

258
00:18:21,440 --> 00:18:26,120
Pada dasarnya, jutaan bobot untuk jaringan khusus ini cukup untuk sekadar menghafal

259
00:18:26,120 --> 00:18:31,040
data acak, sehingga menimbulkan pertanyaan apakah meminimalkan fungsi biaya ini benar-benar

260
00:18:31,040 --> 00:18:36,720
sesuai dengan struktur apa pun dalam gambar, atau hanya sekadar menghafal?

261
00:18:36,720 --> 00:18:40,120
. . . untuk menghafal seluruh dataset tentang klasifikasi yang benar.

262
00:18:40,120 --> 00:18:45,720
Dan beberapa dari, Anda tahu, setengah tahun kemudian di ICML tahun ini, tidak

263
00:18:45,720 --> 00:18:50,440
ada makalah yang benar-benar membantah, namun makalah yang membahas beberapa aspek seperti,

264
00:18:50,440 --> 00:18:52,220
hei, sebenarnya jaringan-jaringan ini melakukan sesuatu yang sedikit lebih pintar dari itu.

265
00:18:52,220 --> 00:18:59,600
Jika Anda melihat kurva akurasi tersebut, jika Anda hanya berlatih pada kumpulan data

266
00:18:59,600 --> 00:19:05,240
acak, kurva tersebut akan turun dengan sangat lambat, hampir seperti gaya linier.

267
00:19:05,280 --> 00:19:10,840
Jadi Anda benar-benar kesulitan menemukan kemungkinan minimum lokal, Anda tahu,

268
00:19:10,840 --> 00:19:12,320
bobot yang tepat yang akan memberi Anda akurasi tersebut.

269
00:19:12,320 --> 00:19:16,720
Sedangkan jika Anda benar-benar berlatih pada kumpulan data terstruktur, yang memiliki label

270
00:19:16,720 --> 00:19:20,240
yang tepat, Anda tahu, Anda melakukan sedikit penyesuaian pada awalnya, namun

271
00:19:20,240 --> 00:19:23,360
kemudian Anda terjatuh dengan sangat cepat untuk mencapai tingkat akurasi tersebut.

272
00:19:23,360 --> 00:19:28,580
Jadi, dalam beberapa hal, lebih mudah untuk menemukan nilai maksimal lokal tersebut.

273
00:19:28,580 --> 00:19:32,900
Dan yang juga menarik dari hal ini adalah ia

274
00:19:32,900 --> 00:19:39,140
mengungkap makalah lain dari beberapa tahun yang lalu,

275
00:19:39,140 --> 00:19:40,140
yang memiliki lebih banyak penyederhanaan tentang lapisan jaringan.

276
00:19:40,140 --> 00:19:43,880
Namun salah satu hasilnya menunjukkan bahwa, jika Anda melihat lanskap pengoptimalan, nilai minimum

277
00:19:43,880 --> 00:19:49,400
lokal yang cenderung dipelajari oleh jaringan ini sebenarnya memiliki kualitas yang sama.

278
00:19:49,400 --> 00:19:54,300
Jadi dalam beberapa hal, jika kumpulan data Anda terstruktur, Anda akan dapat menemukannya dengan lebih mudah.

279
00:19:58,580 --> 00:20:01,140
Terima kasih saya seperti biasa kepada Anda yang mendukung Patreon.

280
00:20:01,480 --> 00:20:05,440
Saya telah mengatakan sebelumnya apa itu game changer di Patreon,

281
00:20:05,440 --> 00:20:07,160
tetapi video ini tidak akan mungkin terjadi tanpa Anda.

282
00:20:07,160 --> 00:20:11,540
Saya juga ingin mengucapkan terima kasih khusus kepada perusahaan VC

283
00:20:11,540 --> 00:20:13,240
Amplify Partners dan dukungan mereka terhadap video awal seri ini.

284
00:20:31,140 --> 00:20:33,140
Terima kasih.

