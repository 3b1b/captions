[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "Video terakhir saya memaparkan struktur jaringan saraf.",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "Saya akan memberikan rekap singkatnya di sini agar segar dalam ingatan kita, dan kemudian saya memiliki dua tujuan utama untuk video ini.",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "Yang pertama adalah memperkenalkan gagasan penurunan gradien, yang tidak hanya mendasari cara jaringan saraf belajar, tetapi juga cara kerja banyak pembelajaran mesin lainnya.",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "Kemudian setelah itu kita akan menggali lebih jauh tentang bagaimana kinerja jaringan ini, dan apa yang akhirnya dicari oleh lapisan neuron tersembunyi tersebut.",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "Sebagai pengingat, tujuan kami di sini adalah contoh klasik pengenalan angka tulisan tangan, halo dunia jaringan saraf.",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "Digit-digit ini dirender pada grid 28x28 piksel, masing-masing piksel memiliki nilai skala abu-abu antara 0 dan 1.",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "Hal itulah yang menentukan aktivasi 784 neuron di lapisan input jaringan.",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "Dan kemudian aktivasi untuk setiap neuron di lapisan berikutnya didasarkan pada jumlah tertimbang dari semua aktivasi di lapisan sebelumnya, ditambah beberapa bilangan khusus yang disebut bias.",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "Kemudian Anda menyusun jumlah tersebut dengan beberapa fungsi lain, seperti squishifikasi sigmoid, atau relu, seperti yang saya lihat di video sebelumnya.",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "Secara total, mengingat pilihan dua lapisan tersembunyi dengan masing-masing 16 neuron, jaringan memiliki sekitar 13.000 bobot dan bias yang dapat kita sesuaikan, dan nilai inilah yang menentukan apa sebenarnya yang dilakukan jaringan.",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "Lalu yang kami maksud ketika kami mengatakan bahwa jaringan ini mengklasifikasikan suatu digit adalah bahwa yang paling terang dari 10 neuron di lapisan terakhir berhubungan dengan digit tersebut.",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "Dan ingat, motivasi yang kita pikirkan di sini untuk struktur berlapis adalah mungkin lapisan kedua dapat mengambil bagian tepinya, dan lapisan ketiga mungkin mengambil pola seperti lingkaran dan garis, dan lapisan terakhir dapat menyatukannya. pola untuk mengenali angka.",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "Jadi di sini, kita mempelajari bagaimana jaringan belajar.",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "Apa yang kami inginkan adalah sebuah algoritma dimana Anda dapat menunjukkan jaringan ini sejumlah besar data pelatihan, yang datang dalam bentuk sekumpulan gambar angka tulisan tangan yang berbeda, bersama dengan label untuk apa yang seharusnya, dan itu akan menyesuaikan 13.000 bobot dan bias tersebut untuk meningkatkan kinerjanya pada data pelatihan.",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "Mudah-mudahan, struktur berlapis ini berarti bahwa apa yang dipelajari dapat digeneralisasi menjadi gambar di luar data pelatihan tersebut.",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "Cara kami mengujinya adalah setelah Anda melatih jaringan, Anda menampilkan lebih banyak data berlabel yang belum pernah dilihat sebelumnya, dan Anda melihat seberapa akurat jaringan mengklasifikasikan gambar-gambar baru tersebut.",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "Untungnya bagi kami, dan apa yang menjadikan hal ini sebagai contoh umum, adalah bahwa orang-orang baik di balik database MNIST telah mengumpulkan puluhan ribu gambar digit tulisan tangan, masing-masing diberi label dengan angka yang seharusnya. menjadi.",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "Dan meskipun provokatif untuk mendeskripsikan mesin sebagai pembelajaran, begitu Anda melihat cara kerjanya, rasanya tidak seperti premis fiksi ilmiah yang gila, dan lebih seperti latihan kalkulus.",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "Maksud saya, pada dasarnya ini adalah menemukan fungsi minimum tertentu.",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "Ingat, secara konseptual, kita menganggap setiap neuron terhubung ke semua neuron di lapisan sebelumnya, dan bobot dalam jumlah tertimbang yang menentukan aktivasinya mirip dengan kekuatan koneksi tersebut, dan bias adalah indikasinya. apakah neuron tersebut cenderung aktif atau tidak aktif.",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "Dan sebagai permulaan, kita hanya akan menginisialisasi semua bobot dan bias tersebut secara acak.",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "Tak perlu dikatakan lagi, jaringan ini akan berkinerja sangat buruk pada contoh pelatihan yang diberikan, karena jaringan ini hanya melakukan sesuatu yang acak.",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "Misalnya, Anda memasukkan gambar 3 ini, dan lapisan keluarannya terlihat berantakan.",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "Jadi yang Anda lakukan adalah mendefinisikan fungsi biaya, cara untuk memberi tahu komputer, tidak, komputer buruk, bahwa output harus memiliki aktivasi yang bernilai 0 untuk sebagian besar neuron, tetapi 1 untuk neuron ini, yang Anda berikan kepada saya adalah sampah total.",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "Untuk mengatakannya secara lebih matematis, Anda menjumlahkan kuadrat perbedaan antara masing-masing aktivasi keluaran sampah tersebut dan nilai yang Anda inginkan, dan inilah yang kami sebut sebagai biaya satu contoh pelatihan.",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "Perhatikan bahwa jumlah ini kecil ketika jaringan dengan yakin mengklasifikasikan gambar dengan benar, namun menjadi besar ketika jaringan sepertinya tidak mengetahui apa yang dilakukannya.",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "Jadi yang Anda lakukan adalah mempertimbangkan biaya rata-rata dari puluhan ribu contoh pelatihan yang Anda miliki.",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "Biaya rata-rata ini adalah ukuran seberapa buruk jaringan tersebut, dan seberapa buruk kinerja komputer.",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "Dan itu adalah hal yang rumit.",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "Ingat bagaimana jaringan itu sendiri pada dasarnya adalah sebuah fungsi, yang mengambil 784 angka sebagai masukan, nilai piksel, dan mengeluarkan 10 angka sebagai keluarannya, dan dalam arti tertentu jaringan tersebut diparameterisasi oleh semua bobot dan bias ini?",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "Fungsi biaya juga merupakan lapisan kompleksitas.",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "Dibutuhkan 13.000 atau lebih bobot dan bias sebagai masukan, dan mengeluarkan satu angka yang menggambarkan seberapa buruk bobot dan bias tersebut, dan cara mendefinisikannya bergantung pada perilaku jaringan pada puluhan ribu data pelatihan.",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "Banyak hal yang perlu dipikirkan.",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "Namun hanya memberi tahu komputer betapa buruknya pekerjaan yang dilakukannya tidaklah terlalu membantu.",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "Anda ingin memberi tahu cara mengubah bobot dan bias tersebut agar menjadi lebih baik.",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "Agar lebih mudah, daripada bersusah payah membayangkan suatu fungsi dengan 13.000 masukan, bayangkan saja sebuah fungsi sederhana yang memiliki satu bilangan sebagai masukan dan satu bilangan sebagai keluaran.",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "Bagaimana cara menemukan masukan yang meminimalkan nilai fungsi ini?",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "Siswa kalkulus akan mengetahui bahwa terkadang Anda dapat mengetahui jumlah minimum tersebut secara eksplisit, namun hal tersebut tidak selalu dapat dilakukan untuk fungsi yang sangat rumit, tentunya tidak dalam versi masukan 13.000 dari situasi ini untuk fungsi biaya jaringan saraf yang sangat rumit.",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "Taktik yang lebih fleksibel adalah memulai dari masukan apa pun, dan mencari tahu arah mana yang harus Anda ambil untuk menurunkan keluaran tersebut.",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "Khususnya, jika Anda dapat mengetahui kemiringan fungsi di tempat Anda berada, geser ke kiri jika kemiringan tersebut positif, dan geser input ke kanan jika kemiringan tersebut negatif.",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "Jika Anda melakukan ini berulang kali, pada setiap titik memeriksa kemiringan baru dan mengambil langkah yang tepat, Anda akan mendekati fungsi minimum lokal.",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "Gambaran yang mungkin Anda bayangkan di sini adalah sebuah bola yang menggelinding menuruni bukit.",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "Perhatikan, bahkan untuk fungsi masukan tunggal yang sangat disederhanakan ini, ada banyak kemungkinan lembah yang mungkin Anda masuki, bergantung pada masukan acak mana yang Anda mulai, dan tidak ada jaminan bahwa nilai minimum lokal tempat Anda mendarat akan menjadi nilai terkecil yang mungkin. dari fungsi biaya.",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "Itu juga akan terbawa ke kasus jaringan saraf kita.",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "Saya juga ingin Anda memperhatikan bagaimana jika Anda membuat ukuran langkah Anda proporsional dengan kemiringan, maka ketika kemiringannya mendatar ke arah minimum, langkah Anda akan semakin kecil, dan itu membantu Anda menghindari overshooting.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "Untuk menambah kerumitannya, bayangkan sebuah fungsi dengan dua masukan dan satu keluaran.",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "Anda mungkin menganggap ruang masukan sebagai bidang xy, dan fungsi biaya digambarkan sebagai permukaan di atasnya.",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "Daripada bertanya tentang kemiringan suatu fungsi, Anda harus menanyakan ke arah mana Anda harus melangkah dalam ruang masukan ini agar keluaran fungsi dapat diturunkan paling cepat.",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "Dengan kata lain, ke arah mana arah menurunnya?",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "Sekali lagi, ada gunanya membayangkan sebuah bola menggelinding menuruni bukit itu.",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "Bagi Anda yang familiar dengan kalkulus multivariabel pasti tahu bahwa gradien suatu fungsi memberi Anda arah kenaikan paling curam, arah mana yang harus Anda ambil untuk meningkatkan fungsi paling cepat.",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "Tentu saja, mengambil nilai negatif dari gradien tersebut memberi Anda arah ke langkah yang menurunkan fungsi paling cepat.",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "Lebih dari itu, panjang vektor gradien ini merupakan indikasi seberapa curam lereng paling curam tersebut.",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "Jika Anda belum terbiasa dengan kalkulus multivariabel dan ingin mempelajari lebih lanjut, lihat beberapa pekerjaan yang saya lakukan untuk Khan Academy mengenai topik tersebut.",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "Sejujurnya, yang penting bagi Anda dan saya saat ini adalah bahwa pada prinsipnya terdapat cara untuk menghitung vektor ini, vektor ini yang memberi tahu Anda arah menurun dan seberapa curamnya.",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "Anda akan baik-baik saja jika hanya itu yang Anda ketahui dan Anda tidak terlalu yakin dengan detailnya.",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "Jika Anda bisa mendapatkannya, algoritme untuk meminimalkan fungsi tersebut adalah dengan menghitung arah gradien ini, lalu mengambil langkah kecil menuruni bukit, dan mengulanginya berulang kali.",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "Itu ide dasar yang sama untuk fungsi yang memiliki 13.000 masukan, bukan 2 masukan.",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "Bayangkan mengatur 13.000 bobot dan bias jaringan kita ke dalam vektor kolom raksasa.",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "Gradien negatif dari fungsi biaya hanyalah sebuah vektor, ini adalah suatu arah di dalam ruang masukan yang sangat besar ini yang memberi tahu Anda dorongan mana ke semua angka tersebut yang akan menyebabkan penurunan paling cepat pada fungsi biaya.",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "Dan tentu saja, dengan fungsi biaya yang dirancang khusus, mengubah bobot dan bias menjadi lebih kecil berarti membuat output jaringan pada setiap bagian data pelatihan tidak terlihat seperti array acak yang terdiri dari 10 nilai, dan lebih seperti keputusan sebenarnya yang kita inginkan. itu untuk membuat.",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "Penting untuk diingat, fungsi biaya ini melibatkan rata-rata seluruh data pelatihan, jadi jika Anda meminimalkannya, artinya performanya lebih baik pada semua sampel tersebut.",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "Algoritme untuk menghitung gradien ini secara efisien, yang secara efektif merupakan inti dari cara jaringan saraf belajar, disebut propagasi mundur, dan itulah yang akan saya bicarakan di video berikutnya.",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "Di sana, saya benar-benar ingin meluangkan waktu untuk menelusuri apa yang sebenarnya terjadi pada setiap bobot dan bias untuk data pelatihan tertentu, mencoba memberikan gambaran intuitif tentang apa yang terjadi di luar tumpukan kalkulus dan rumus yang relevan.",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "Di sini, saat ini, hal utama yang saya ingin Anda ketahui, terlepas dari detail implementasinya, adalah bahwa yang kita maksud ketika kita berbicara tentang pembelajaran jaringan adalah bahwa pembelajaran jaringan hanya meminimalkan fungsi biaya.",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "Dan perhatikan, salah satu konsekuensi dari hal ini adalah penting agar fungsi biaya ini memiliki keluaran yang lancar, sehingga kita dapat menemukan nilai minimum lokal dengan mengambil sedikit langkah menurun.",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "Inilah sebabnya mengapa neuron buatan memiliki aktivasi yang terus menerus, bukan hanya aktif atau tidak aktif secara biner, seperti halnya neuron biologis.",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "Proses berulang kali mendorong input suatu fungsi dengan beberapa kelipatan gradien negatif disebut penurunan gradien.",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "Ini adalah cara untuk menyatu menuju fungsi biaya minimum lokal, yang pada dasarnya merupakan lembah dalam grafik ini.",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "Saya masih menampilkan gambar fungsi dengan dua masukan, tentu saja, karena dorongan dalam ruang masukan 13.000 dimensi agak sulit untuk dipahami, tetapi ada cara non-spasial yang bagus untuk memikirkan hal ini.",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "Setiap komponen gradien negatif memberi tahu kita dua hal.",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "Tandanya, tentu saja, memberi tahu kita apakah komponen vektor masukan yang bersesuaian harus dinaikkan atau diturunkan.",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "Namun yang terpenting, besaran relatif dari semua komponen ini memberi tahu Anda perubahan mana yang lebih penting.",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "Anda lihat, dalam jaringan kami, penyesuaian pada salah satu bobot mungkin memiliki dampak yang jauh lebih besar pada fungsi biaya dibandingkan penyesuaian pada bobot lainnya.",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "Beberapa dari koneksi ini lebih penting untuk data pelatihan kami.",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "Jadi, cara Anda memikirkan vektor gradien dari fungsi biaya yang sangat besar ini adalah dengan mengkodekan kepentingan relatif dari setiap bobot dan bias, yaitu, perubahan mana yang akan menghasilkan keuntungan paling besar.",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "Ini sebenarnya hanyalah cara berpikir lain tentang arah.",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "Untuk mengambil contoh yang lebih sederhana, jika Anda memiliki suatu fungsi dengan dua variabel sebagai masukan, dan Anda menghitung bahwa gradiennya pada suatu titik tertentu adalah 3,1, maka di satu sisi Anda dapat menafsirkannya sebagai pernyataan bahwa ketika Anda' Saat Anda berdiri di masukan tersebut, bergerak sepanjang arah ini akan meningkatkan fungsi paling cepat, sehingga saat Anda membuat grafik fungsi di atas bidang titik masukan, vektor itulah yang memberi Anda arah lurus ke atas.",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "Namun cara lain untuk membacanya adalah dengan mengatakan bahwa perubahan pada variabel pertama ini memiliki kepentingan 3 kali lipat dibandingkan perubahan pada variabel kedua, sehingga setidaknya di sekitar masukan yang relevan, mendorong nilai x membawa lebih banyak keuntungan bagi Anda. uang.",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "Mari kita perkecil dan simpulkan posisi kita sejauh ini.",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "Jaringan itu sendiri adalah fungsi ini dengan 784 masukan dan 10 keluaran, yang didefinisikan dalam bentuk semua jumlah tertimbang ini.",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "Fungsi biaya juga merupakan lapisan kompleksitas.",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "Dibutuhkan 13.000 bobot dan bias sebagai masukan dan mengeluarkan satu ukuran keburukan berdasarkan contoh pelatihan.",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "Dan gradien fungsi biaya masih merupakan satu lapisan kompleksitas lagi.",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "Hal ini memberi tahu kita dorongan apa pada semua bobot dan bias ini yang menyebabkan perubahan tercepat pada nilai fungsi biaya, yang mungkin Anda tafsirkan sebagai perubahan mana pada bobot mana yang paling penting.",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "Jadi, ketika Anda menginisialisasi jaringan dengan bobot dan bias acak, dan menyesuaikannya berkali-kali berdasarkan proses penurunan gradien ini, seberapa baik kinerjanya pada gambar yang belum pernah terlihat sebelumnya?",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "Yang telah saya jelaskan di sini, dengan dua lapisan tersembunyi yang masing-masing terdiri dari 16 neuron, sebagian besar dipilih karena alasan estetika, lumayan, mengklasifikasikan sekitar 96% gambar baru yang dilihatnya dengan benar.",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "Dan sejujurnya, jika Anda melihat beberapa contoh yang membuat kesalahan, Anda merasa harus menguranginya sedikit.",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "Sekarang jika Anda bermain-main dengan struktur lapisan tersembunyi dan membuat beberapa penyesuaian, Anda bisa mendapatkan ini hingga 98%.",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "Dan itu cukup bagus!",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "Ini bukan yang terbaik, Anda pasti bisa mendapatkan kinerja yang lebih baik dengan menjadi lebih canggih daripada jaringan vanilla biasa ini, namun mengingat betapa beratnya tugas awalnya, menurut saya ada sesuatu yang luar biasa tentang jaringan mana pun yang melakukan hal ini dengan baik pada gambar yang belum pernah dilihat sebelumnya, mengingat bahwa kami tidak pernah secara spesifik memberi tahu pola apa yang harus dicari.",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "Awalnya, cara saya memotivasi struktur ini adalah dengan mendeskripsikan harapan yang mungkin kita miliki, bahwa lapisan kedua dapat menangkap tepi-tepi kecil, bahwa lapisan ketiga akan menyatukan tepi-tepi tersebut untuk mengenali loop dan garis-garis yang lebih panjang, dan agar tepi-tepi tersebut dapat disatukan. bersama-sama untuk mengenali angka.",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "Jadi apakah ini yang sebenarnya dilakukan jaringan kita?",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "Setidaknya untuk yang satu ini, tidak sama sekali.",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "Ingat bagaimana video terakhir kita melihat bagaimana bobot koneksi dari semua neuron di lapisan pertama ke neuron tertentu di lapisan kedua dapat divisualisasikan sebagai pola piksel tertentu yang diambil oleh neuron lapisan kedua?",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "Nah, ketika kita benar-benar melakukan itu untuk bobot yang terkait dengan transisi ini, dari lapisan pertama ke lapisan berikutnya, alih-alih mengambil tepi kecil yang terisolasi di sana-sini, mereka terlihat hampir acak, hanya dengan beberapa pola yang sangat longgar di dalamnya. tengah di sana.",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "Tampaknya dalam ruang 13.000 dimensi yang sangat besar dan berisi kemungkinan bobot dan bias, jaringan kami menemukan dirinya sebagai minimum lokal kecil yang menyenangkan, meskipun berhasil mengklasifikasikan sebagian besar gambar, tidak benar-benar menangkap pola yang kami harapkan.",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "Dan untuk benar-benar memahami hal ini, perhatikan apa yang terjadi ketika Anda memasukkan gambar acak.",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "Jika sistemnya cerdas, Anda mungkin mengira sistemnya akan terasa tidak pasti, mungkin tidak benar-benar mengaktifkan salah satu dari 10 neuron keluaran tersebut atau mengaktifkan semuanya secara merata, namun sebaliknya sistem tersebut dengan percaya diri memberi Anda jawaban yang tidak masuk akal, seolah-olah sistem tersebut terasa yakin bahwa suara acak ini adalah angka 5 sebagaimana gambar sebenarnya dari angka 5 adalah angka 5.",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "Dengan kata lain, meskipun jaringan ini dapat mengenali angka dengan cukup baik, ia tidak tahu cara menggambarnya.",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "Hal ini sebagian besar disebabkan oleh pengaturan pelatihan yang sangat terbatas.",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "Maksud saya, tempatkan diri Anda pada posisi jaringan di sini.",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "Dari sudut pandangnya, seluruh alam semesta hanya terdiri dari angka-angka tak bergerak yang terdefinisi dengan jelas dan berpusat pada sebuah kotak kecil, dan fungsi biayanya tidak pernah memberinya insentif apa pun kecuali keyakinan penuh dalam mengambil keputusan.",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "Jadi dengan gambaran tentang apa yang sebenarnya dilakukan oleh neuron lapisan kedua tersebut, Anda mungkin bertanya-tanya mengapa saya memperkenalkan jaringan ini dengan motivasi untuk memahami tepian dan pola.",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "Maksudku, bukan itu yang akhirnya terjadi.",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "Ya, ini bukan dimaksudkan sebagai tujuan akhir kita, melainkan sebuah titik awal.",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "Sejujurnya, ini adalah teknologi lama, jenis yang diteliti pada tahun 80an dan 90an, dan Anda perlu memahaminya sebelum Anda dapat memahami varian modern yang lebih detail, dan teknologi ini jelas mampu memecahkan beberapa masalah menarik, tetapi semakin Anda menggali apa yang ada di dalamnya. lapisan-lapisan tersembunyi itu benar-benar berfungsi, semakin tidak cerdas kelihatannya.",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "Mengalihkan fokus sejenak dari cara jaringan belajar ke cara Anda belajar, itu hanya akan terjadi jika Anda terlibat secara aktif dengan materi di sini.",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "Satu hal yang cukup sederhana yang saya ingin Anda lakukan adalah berhenti sejenak dan berpikir sejenak tentang perubahan apa yang mungkin Anda lakukan pada sistem ini dan bagaimana sistem ini memandang gambar jika Anda ingin sistem ini menangkap hal-hal seperti tepian dan pola dengan lebih baik.",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "Namun lebih baik dari itu, untuk benar-benar memahami materi, saya sangat merekomendasikan buku karya Michael Nielsen tentang pembelajaran mendalam dan jaringan saraf.",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "Di dalamnya, Anda dapat menemukan kode dan data untuk diunduh dan dimainkan untuk contoh persis ini, dan buku ini akan memandu Anda langkah demi langkah tentang apa yang dilakukan kode tersebut.",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "Yang luar biasa adalah buku ini gratis dan tersedia untuk umum, jadi jika Anda mendapatkan manfaat darinya, pertimbangkan untuk bergabung dengan saya dalam memberikan donasi untuk upaya Nielsen.",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "Saya juga menautkan beberapa sumber lain yang sangat saya sukai dalam deskripsi, termasuk postingan blog yang fenomenal dan indah oleh Chris Ola dan artikel di Distill.",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "Untuk menutup beberapa menit terakhir di sini, saya ingin kembali ke cuplikan wawancara saya dengan Leisha Lee.",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "Anda mungkin ingat dia dari video terakhir, dia menyelesaikan pekerjaan PhD-nya dalam pembelajaran mendalam.",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "Dalam cuplikan kecil ini dia berbicara tentang dua makalah terbaru yang benar-benar menggali bagaimana beberapa jaringan pengenalan gambar modern sebenarnya belajar.",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "Sekadar untuk mengatur posisi kita dalam percakapan, makalah pertama mengambil salah satu dari jaringan saraf dalam yang sangat bagus dalam pengenalan gambar, dan alih-alih melatihnya pada kumpulan data yang diberi label dengan benar, mereka mengacak semua label sebelum pelatihan.",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "Tentu saja akurasi pengujian di sini tidak lebih baik daripada pengujian acak, karena semuanya hanya diberi label secara acak, namun akurasi pelatihan masih dapat dicapai seperti yang Anda lakukan pada kumpulan data yang diberi label dengan benar.",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "Pada dasarnya, jutaan bobot untuk jaringan khusus ini cukup untuk sekadar menghafal data acak, sehingga menimbulkan pertanyaan apakah meminimalkan fungsi biaya ini benar-benar sesuai dengan struktur apa pun dalam gambar, atau hanya sekadar menghafal?",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "Jika Anda melihat kurva akurasi tersebut, jika Anda hanya berlatih pada kumpulan data acak, kurva tersebut turun sangat lambat hampir seperti gaya linier, jadi Anda benar-benar kesulitan menemukan kemungkinan minimum lokal tersebut, Anda tahu , bobot yang tepat yang akan memberi Anda akurasi tersebut.",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "Sedangkan jika Anda benar-benar berlatih pada kumpulan data terstruktur, yang memiliki label yang tepat, Anda mengutak-atiknya sedikit di awal, namun kemudian Anda terjatuh dengan sangat cepat untuk mencapai tingkat akurasi tersebut, dan dalam beberapa hal itu lebih mudah untuk menemukan maxima lokal itu.",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "Jadi yang menarik dari hal ini adalah makalah ini menyoroti makalah lain dari beberapa tahun yang lalu, yang memiliki lebih banyak penyederhanaan tentang lapisan jaringan, tetapi salah satu hasilnya menunjukkan bagaimana jika Anda melihat lanskap pengoptimalan, nilai minimum lokal yang cenderung dipelajari oleh jaringan ini sebenarnya memiliki kualitas yang sama, jadi jika kumpulan data Anda terstruktur, Anda akan dapat menemukannya dengan lebih mudah.",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "Terima kasih saya, seperti biasa, kepada Anda yang mendukung Patreon.",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "Saya telah mengatakan sebelumnya apa itu Patreon yang mengubah permainan, tetapi video ini tidak akan mungkin terjadi tanpa Anda.",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "Saya juga ingin mengucapkan terima kasih khusus kepada perusahaan VC Amplify Partners, atas dukungan mereka terhadap video awal dalam seri ini.",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]