1
00:00:04,180 --> 00:00:07,280
Nell'ultimo video ho presentato la struttura di una rete neurale.

2
00:00:07,680 --> 00:00:10,520
Farò un breve riepilogo qui in modo che sia fresco nelle nostre menti, 

3
00:00:10,520 --> 00:00:12,600
quindi ho due obiettivi principali per questo video.

4
00:00:13,100 --> 00:00:15,221
Il primo è introdurre l’idea della discesa del gradiente, 

5
00:00:15,221 --> 00:00:17,746
che è alla base non solo del modo in cui le reti neurali apprendono, 

6
00:00:17,746 --> 00:00:20,600
ma anche del funzionamento di molti altri sistemi di apprendimento automatico.

7
00:00:21,120 --> 00:00:24,442
Successivamente approfondiremo un po' di più il funzionamento di questa 

8
00:00:24,442 --> 00:00:27,940
particolare rete e cosa finiscono per cercare quegli strati nascosti di neuroni.

9
00:00:28,979 --> 00:00:32,345
Come promemoria, il nostro obiettivo qui è il classico esempio di 

10
00:00:32,345 --> 00:00:36,220
riconoscimento delle cifre scritte a mano, il ciao mondo delle reti neurali.

11
00:00:37,020 --> 00:00:40,195
Queste cifre vengono visualizzate su una griglia di 28x28 pixel, 

12
00:00:40,195 --> 00:00:43,420
ciascun pixel con un valore di scala di grigio compreso tra 0 e 1.

13
00:00:43,820 --> 00:00:46,930
Questi sono ciò che determina l'attivazione 

14
00:00:46,930 --> 00:00:50,040
di 784 neuroni nello strato di input della rete.

15
00:00:51,180 --> 00:00:54,113
E poi l'attivazione di ciascun neurone negli strati 

16
00:00:54,113 --> 00:00:57,571
successivi si basa su una somma ponderata di tutte le attivazioni 

17
00:00:57,571 --> 00:01:00,820
nello strato precedente, più un numero speciale chiamato bias.

18
00:01:02,160 --> 00:01:04,782
Poi componi quella somma con qualche altra funzione, 

19
00:01:04,782 --> 00:01:08,940
come lo schiacciamento del sigmoide, o un relu, come ho visto nell'ultimo video.

20
00:01:09,480 --> 00:01:14,425
In totale, data la scelta un po' arbitraria di due strati nascosti con 16 

21
00:01:14,425 --> 00:01:19,307
neuroni ciascuno, la rete ha circa 13.000 pesi e bias che possiamo regolare, 

22
00:01:19,307 --> 00:01:24,380
e sono questi valori che determinano esattamente cosa fa effettivamente la rete.

23
00:01:24,880 --> 00:01:29,137
Quindi ciò che intendiamo quando diciamo che questa rete classifica una determinata cifra 

24
00:01:29,137 --> 00:01:33,300
è che il più luminoso di quei 10 neuroni nello strato finale corrisponde a quella cifra.

25
00:01:34,100 --> 00:01:37,611
E ricorda, la motivazione che avevamo in mente qui per la struttura a 

26
00:01:37,611 --> 00:01:40,923
strati era che forse il secondo strato poteva riprendere i bordi, 

27
00:01:40,923 --> 00:01:44,184
e il terzo strato poteva riprendere modelli come anelli e linee, 

28
00:01:44,184 --> 00:01:47,746
e l'ultimo poteva semplicemente mettere insieme quelli modelli per 

29
00:01:47,746 --> 00:01:48,800
riconoscere le cifre.

30
00:01:49,800 --> 00:01:52,240
Quindi qui impariamo come apprende la rete.

31
00:01:52,640 --> 00:01:57,010
Quello che vogliamo è un algoritmo in cui puoi mostrare a questa rete un sacco di dati di 

32
00:01:57,010 --> 00:02:01,282
addestramento, che si presentano sotto forma di un mucchio di immagini diverse di cifre 

33
00:02:01,282 --> 00:02:04,827
scritte a mano, insieme alle etichette per quello che dovrebbero essere, 

34
00:02:04,827 --> 00:02:09,148
e sarà aggiustare questi 13.000 pesi e bias in modo da migliorare le sue prestazioni sui 

35
00:02:09,148 --> 00:02:10,120
dati di allenamento.

36
00:02:10,720 --> 00:02:13,694
Si spera che questa struttura a strati significhi che ciò che 

37
00:02:13,694 --> 00:02:16,860
apprende si generalizzi in immagini oltre i dati di addestramento.

38
00:02:17,640 --> 00:02:20,958
Il modo in cui lo testiamo è che dopo aver addestrato la rete, 

39
00:02:20,958 --> 00:02:25,541
le mostri più dati etichettati mai visti prima e vedi con quanta precisione classifica 

40
00:02:25,541 --> 00:02:26,700
quelle nuove immagini.

41
00:02:31,120 --> 00:02:35,039
Fortunatamente per noi, e ciò che rende questo esempio così comune per cominciare, 

42
00:02:35,039 --> 00:02:38,250
è che le brave persone dietro il database MNIST hanno messo insieme 

43
00:02:38,250 --> 00:02:41,650
una raccolta di decine di migliaia di immagini di cifre scritte a mano, 

44
00:02:41,650 --> 00:02:44,200
ognuna etichettata con i numeri che dovrebbero Essere.

45
00:02:44,900 --> 00:02:49,046
E per quanto provocatorio sia descrivere una macchina in grado di apprendere, 

46
00:02:49,046 --> 00:02:52,662
una volta visto come funziona, sembra molto meno una folle premessa 

47
00:02:52,662 --> 00:02:55,480
fantascientifica e molto più un esercizio di calcolo.

48
00:02:56,200 --> 00:02:59,960
Voglio dire, fondamentalmente si tratta di trovare il minimo di una certa funzione.

49
00:03:01,939 --> 00:03:06,024
Ricorda, concettualmente, pensiamo che ciascun neurone sia connesso a tutti i 

50
00:03:06,024 --> 00:03:10,266
neuroni dello strato precedente, e i pesi nella somma ponderata che definisce la 

51
00:03:10,266 --> 00:03:14,299
sua attivazione sono un po' come i punti di forza di quelle connessioni, 

52
00:03:14,299 --> 00:03:18,960
e il bias è una qualche indicazione di se quel neurone tende ad essere attivo o inattivo.

53
00:03:19,720 --> 00:03:24,400
E per iniziare, inizializzeremo tutti questi pesi e pregiudizi in modo totalmente casuale.

54
00:03:24,940 --> 00:03:27,867
Inutile dire che questa rete funzionerà in modo piuttosto orribile su un dato 

55
00:03:27,867 --> 00:03:30,720
esempio di formazione, poiché sta semplicemente facendo qualcosa di casuale.

56
00:03:31,040 --> 00:03:33,530
Ad esempio, inserisci questa immagine di un 3 e il 

57
00:03:33,530 --> 00:03:36,020
livello di output sembra semplicemente un disastro.

58
00:03:36,600 --> 00:03:41,195
Quindi quello che fai è definire una funzione di costo, un modo per dire al computer, 

59
00:03:41,195 --> 00:03:45,683
no, cattivo computer, che l'output dovrebbe avere attivazioni che sono 0 per la 

60
00:03:45,683 --> 00:03:48,462
maggior parte dei neuroni, ma 1 per questo neurone, 

61
00:03:48,462 --> 00:03:50,760
quello che mi hai dato è totale spazzatura.

62
00:03:51,720 --> 00:03:56,171
Per dirlo in modo un po' più matematico, sommi i quadrati delle differenze tra 

63
00:03:56,171 --> 00:04:00,836
ciascuna di quelle attivazioni di output dei rifiuti e il valore che vuoi che abbiano, 

64
00:04:00,836 --> 00:04:05,020
e questo è quello che chiameremo il costo di un singolo esempio di formazione.

65
00:04:05,960 --> 00:04:11,454
Si noti che questa somma è piccola quando la rete classifica correttamente l'immagine 

66
00:04:11,454 --> 00:04:16,399
con sicurezza, ma è grande quando sembra che la rete non sappia cosa sta facendo.

67
00:04:18,640 --> 00:04:22,067
Quindi quello che fai è considerare il costo medio di tutte le 

68
00:04:22,067 --> 00:04:25,440
decine di migliaia di esempi di formazione a tua disposizione.

69
00:04:27,040 --> 00:04:29,938
Questo costo medio è la nostra misura di quanto sia pessima 

70
00:04:29,938 --> 00:04:32,740
la rete e di quanto dovrebbe sentirsi pessimo il computer.

71
00:04:33,420 --> 00:04:34,600
E questa è una cosa complicata.

72
00:04:35,040 --> 00:04:38,831
Ricordi che la rete stessa era fondamentalmente una funzione, 

73
00:04:38,831 --> 00:04:42,195
che accetta 784 numeri come input, i valori dei pixel, 

74
00:04:42,195 --> 00:04:46,720
e restituisce 10 numeri come output, e in un certo senso è parametrizzata 

75
00:04:46,720 --> 00:04:48,800
da tutti questi pesi e pregiudizi?

76
00:04:49,500 --> 00:04:52,820
Ebbene, la funzione di costo è uno strato di complessità in più.

77
00:04:53,100 --> 00:04:58,185
Prende come input quei circa 13.000 pesi e pregiudizi e sputa un singolo numero che 

78
00:04:58,185 --> 00:05:01,332
descrive quanto siano gravi tali pesi e pregiudizi, 

79
00:05:01,332 --> 00:05:06,660
e il modo in cui viene definito dipende dal comportamento della rete su tutte le decine 

80
00:05:06,660 --> 00:05:08,900
di migliaia di dati di addestramento.

81
00:05:09,520 --> 00:05:11,000
C'è molto a cui pensare.

82
00:05:12,400 --> 00:05:15,820
Ma limitarsi a dire al computer che lavoro schifoso sta facendo non è molto utile.

83
00:05:16,220 --> 00:05:20,060
Vuoi dirgli come cambiare quei pesi e pregiudizi in modo che migliori.

84
00:05:20,780 --> 00:05:25,658
Per rendere più semplice, anziché faticare a immaginare una funzione con 13.000 input, 

85
00:05:25,658 --> 00:05:30,480
immagina una semplice funzione che abbia un numero come input e un numero come output.

86
00:05:31,480 --> 00:05:35,300
Come trovi un input che minimizzi il valore di questa funzione?

87
00:05:36,460 --> 00:05:39,758
Gli studenti di calcolo sapranno che a volte è possibile calcolare 

88
00:05:39,758 --> 00:05:43,253
esplicitamente quel minimo, ma ciò non è sempre fattibile per funzioni 

89
00:05:43,253 --> 00:05:46,748
veramente complicate, certamente non nella versione da 13.000 input di 

90
00:05:46,748 --> 00:05:51,080
questa situazione per la nostra folle e complicata funzione di costo della rete neurale.

91
00:05:51,580 --> 00:05:55,390
Una tattica più flessibile è quella di iniziare da qualsiasi input e 

92
00:05:55,390 --> 00:05:59,200
capire in quale direzione dovresti procedere per ridurre tale output.

93
00:06:00,080 --> 00:06:03,403
Nello specifico, se riesci a calcolare la pendenza della funzione 

94
00:06:03,403 --> 00:06:06,626
nel punto in cui ti trovi, spostati a sinistra se la pendenza è 

95
00:06:06,626 --> 00:06:09,900
positiva e sposta l'input a destra se la pendenza è negativa.

96
00:06:11,960 --> 00:06:15,692
Se lo fai ripetutamente, controllando in ogni punto la nuova pendenza e 

97
00:06:15,692 --> 00:06:19,840
facendo il passo appropriato, ti avvicinerai ad un minimo locale della funzione.

98
00:06:20,640 --> 00:06:23,800
L'immagine che potresti avere in mente qui è una palla che rotola giù da una collina.

99
00:06:24,620 --> 00:06:28,094
Nota, anche per questa funzione di input singolo davvero semplificata, 

100
00:06:28,094 --> 00:06:30,884
ci sono molte possibili valli in cui potresti atterrare, 

101
00:06:30,884 --> 00:06:33,184
a seconda dell'input casuale da cui inizi, 

102
00:06:33,184 --> 00:06:36,806
e non c'è alcuna garanzia che il minimo locale in cui atterri sarà il 

103
00:06:36,806 --> 00:06:39,400
valore più piccolo possibile della funzione di costo.

104
00:06:40,220 --> 00:06:42,620
Ciò si ripercuoterà anche sul caso della nostra rete neurale.

105
00:06:43,180 --> 00:06:47,199
Voglio anche che tu noti come se rendi le dimensioni dei tuoi passi proporzionali 

106
00:06:47,199 --> 00:06:50,433
alla pendenza, quando la pendenza si appiattisce verso il minimo, 

107
00:06:50,433 --> 00:06:54,600
i tuoi passi diventano sempre più piccoli e questo ti aiuta a non superare il limite.

108
00:06:55,940 --> 00:06:58,487
Aumentando un po' la complessità, immagina 

109
00:06:58,487 --> 00:07:00,980
invece una funzione con due input e un output.

110
00:07:01,500 --> 00:07:04,865
Potresti pensare allo spazio di input come al piano xy e alla funzione di 

111
00:07:04,865 --> 00:07:08,140
costo come rappresentata graficamente come una superficie sopra di esso.

112
00:07:08,760 --> 00:07:11,879
Invece di chiedere informazioni sulla pendenza della funzione, 

113
00:07:11,879 --> 00:07:15,295
devi chiederti in quale direzione dovresti muoverti in questo spazio 

114
00:07:15,295 --> 00:07:18,960
di input in modo da diminuire più rapidamente l'output della funzione.

115
00:07:19,720 --> 00:07:21,760
In altre parole, qual è la direzione in discesa?

116
00:07:22,380 --> 00:07:25,560
Ancora una volta, è utile pensare a una palla che rotola giù da quella collina.

117
00:07:26,660 --> 00:07:30,664
Quelli di voi che hanno familiarità con il calcolo multivariabile sapranno 

118
00:07:30,664 --> 00:07:34,935
che il gradiente di una funzione ti dà la direzione dell'ascesa più ripida, 

119
00:07:34,935 --> 00:07:38,780
quale direzione dovresti fare per aumentare la funzione più rapidamente.

120
00:07:39,560 --> 00:07:42,775
Naturalmente, prendendo il negativo di quel gradiente si ottiene 

121
00:07:42,775 --> 00:07:46,040
la direzione del passo che diminuisce la funzione più rapidamente.

122
00:07:47,240 --> 00:07:50,428
Ancor di più, la lunghezza di questo vettore gradiente è 

123
00:07:50,428 --> 00:07:53,840
un'indicazione di quanto sia ripido il pendio più ripido.

124
00:07:54,540 --> 00:07:57,253
Se non hai familiarità con il calcolo multivariabile e desideri saperne di più, 

125
00:07:57,253 --> 00:07:59,695
dai un'occhiata ad alcuni dei lavori che ho svolto per Khan Academy 

126
00:07:59,695 --> 00:08:00,340
sull'argomento.

127
00:08:00,860 --> 00:08:04,455
Onestamente, però, tutto ciò che conta per me e te in questo momento è 

128
00:08:04,455 --> 00:08:08,051
che in linea di principio esiste un modo per calcolare questo vettore, 

129
00:08:08,051 --> 00:08:11,900
questo vettore che ti dice qual è la direzione in discesa e quanto è ripida.

130
00:08:12,400 --> 00:08:14,321
Starai bene se questo è tutto quello che sai e 

131
00:08:14,321 --> 00:08:16,120
non sei solido come una roccia sui dettagli.

132
00:08:17,200 --> 00:08:20,498
Se riesci a capirlo, l'algoritmo per minimizzare la funzione 

133
00:08:20,498 --> 00:08:23,289
consiste nel calcolare questa direzione del gradiente, 

134
00:08:23,289 --> 00:08:26,740
quindi fare un piccolo passo in discesa e ripeterlo ancora e ancora.

135
00:08:27,700 --> 00:08:32,820
È la stessa idea di base per una funzione che ha 13.000 input invece di 2 input.

136
00:08:33,400 --> 00:08:36,511
Immagina di organizzare tutti i 13.000 pesi e pregiudizi 

137
00:08:36,511 --> 00:08:39,460
della nostra rete in un gigantesco vettore di colonne.

138
00:08:40,140 --> 00:08:44,018
Il gradiente negativo della funzione di costo è solo un vettore, 

139
00:08:44,018 --> 00:08:48,972
è una direzione all'interno di questo spazio di input follemente enorme che ti 

140
00:08:48,972 --> 00:08:53,805
dice quali spinte a tutti quei numeri causeranno la diminuzione più rapida della 

141
00:08:53,805 --> 00:08:54,880
funzione di costo.

142
00:08:55,640 --> 00:08:59,435
E ovviamente, con la nostra funzione di costo appositamente progettata, 

143
00:08:59,435 --> 00:09:03,282
modificare i pesi e i bias per ridurli significa far sì che l'output 

144
00:09:03,282 --> 00:09:06,972
della rete su ciascun dato di addestramento assomigli meno a un array 

145
00:09:06,972 --> 00:09:10,820
casuale di 10 valori e più a una decisione effettiva che vogliamo. farlo.

146
00:09:11,440 --> 00:09:14,510
È importante ricordare che questa funzione di costo implica una 

147
00:09:14,510 --> 00:09:17,917
media su tutti i dati di addestramento, quindi se la riduci al minimo, 

148
00:09:17,917 --> 00:09:21,180
significa che ci sono prestazioni migliori su tutti questi campioni.

149
00:09:23,820 --> 00:09:27,046
L'algoritmo per calcolare questo gradiente in modo efficiente, 

150
00:09:27,046 --> 00:09:30,609
che è effettivamente il cuore dell'apprendimento di una rete neurale, 

151
00:09:30,609 --> 00:09:33,980
si chiama backpropagation, ed è ciò di cui parlerò nel prossimo video.

152
00:09:34,660 --> 00:09:38,687
Lì, voglio davvero prendermi il tempo per esaminare cosa succede esattamente a 

153
00:09:38,687 --> 00:09:41,491
ciascun peso e bias per un dato dato di addestramento, 

154
00:09:41,491 --> 00:09:45,621
cercando di dare un'idea intuitiva di ciò che sta accadendo oltre la pila di 

155
00:09:45,621 --> 00:09:47,100
calcoli e formule pertinenti.

156
00:09:47,780 --> 00:09:50,821
Proprio qui, proprio ora, la cosa principale che voglio che tu sappia, 

157
00:09:50,821 --> 00:09:53,005
indipendentemente dai dettagli di implementazione, 

158
00:09:53,005 --> 00:09:56,261
è che ciò che intendiamo quando parliamo di apprendimento in rete è che sta 

159
00:09:56,261 --> 00:09:58,360
semplicemente minimizzando una funzione di costo.

160
00:09:59,300 --> 00:10:02,114
E notate, una conseguenza di ciò è che è importante che questa 

161
00:10:02,114 --> 00:10:04,571
funzione di costo abbia un output abbastanza regolare, 

162
00:10:04,571 --> 00:10:08,100
in modo da poter trovare un minimo locale facendo piccoli passi verso il basso.

163
00:10:09,260 --> 00:10:12,585
Questo è il motivo per cui, tra l’altro, i neuroni artificiali hanno 

164
00:10:12,585 --> 00:10:15,862
attivazioni che variano continuamente, anziché essere semplicemente 

165
00:10:15,862 --> 00:10:19,140
attivi o inattivi in modo binario, come lo sono i neuroni biologici.

166
00:10:20,220 --> 00:10:23,490
Questo processo di spostamento ripetuto dell'input di una funzione 

167
00:10:23,490 --> 00:10:26,760
di un multiplo del gradiente negativo è chiamato discesa del gradiente.

168
00:10:27,300 --> 00:10:30,611
È un modo per convergere verso un minimo locale di una funzione di costo, 

169
00:10:30,611 --> 00:10:32,580
sostanzialmente una valle in questo grafico.

170
00:10:33,440 --> 00:10:37,222
Sto ancora mostrando l'immagine di una funzione con due input, ovviamente, 

171
00:10:37,222 --> 00:10:40,765
perché i nudge in uno spazio di input a 13.000 dimensioni sono un po' 

172
00:10:40,765 --> 00:10:44,260
difficili da comprendere, ma esiste un bel modo non spaziale di pensarci.

173
00:10:45,080 --> 00:10:48,440
Ogni componente del gradiente negativo ci dice due cose.

174
00:10:49,060 --> 00:10:51,967
Il segno, ovviamente, ci dice se la componente corrispondente del 

175
00:10:51,967 --> 00:10:55,140
vettore di input deve essere spostata verso l'alto o verso il basso.

176
00:10:55,800 --> 00:10:59,289
Ma, cosa ancora più importante, l’entità relativa di tutti 

177
00:10:59,289 --> 00:11:02,720
questi componenti indica quali cambiamenti contano di più.

178
00:11:05,220 --> 00:11:08,996
Vedete, nella nostra rete, un aggiustamento a uno dei pesi potrebbe avere un impatto 

179
00:11:08,996 --> 00:11:12,817
molto maggiore sulla funzione di costo rispetto all'aggiustamento a qualche altro 

180
00:11:12,817 --> 00:11:13,040
peso.

181
00:11:14,800 --> 00:11:18,200
Alcune di queste connessioni contano di più per i nostri dati di addestramento.

182
00:11:19,320 --> 00:11:23,697
Quindi un modo in cui puoi pensare a questo vettore gradiente della nostra enorme 

183
00:11:23,697 --> 00:11:28,342
funzione di costo è che codifica l'importanza relativa di ogni peso e pregiudizio, 

184
00:11:28,342 --> 00:11:32,400
cioè quale di questi cambiamenti porterà il maggior rapporto qualità-prezzo.

185
00:11:33,620 --> 00:11:36,640
Questo è davvero solo un altro modo di pensare alla direzione.

186
00:11:37,100 --> 00:11:41,204
Per fare un esempio più semplice, se hai una funzione con due variabili come 

187
00:11:41,204 --> 00:11:45,415
input e calcoli che il suo gradiente in un punto particolare risulta come 3,1, 

188
00:11:45,415 --> 00:11:49,253
allora da un lato puoi interpretarlo come se dicessi che quando tu' 

189
00:11:49,253 --> 00:11:53,571
Stando su quell'input, muovendoti lungo questa direzione la funzione aumenta 

190
00:11:53,571 --> 00:11:57,675
più rapidamente, ovvero quando rappresenti graficamente la funzione sopra il 

191
00:11:57,675 --> 00:12:02,260
piano dei punti di input, quel vettore è ciò che ti dà la direzione diritta in salita.

192
00:12:02,860 --> 00:12:07,399
Ma un altro modo di leggerlo è dire che le modifiche a questa prima variabile hanno 3 

193
00:12:07,399 --> 00:12:10,724
volte l'importanza delle modifiche alla seconda variabile, 

194
00:12:10,724 --> 00:12:13,521
che almeno nelle vicinanze dell'input rilevante, 

195
00:12:13,521 --> 00:12:16,900
spostare il valore x porta molto più effetto per il tuo secchio.

196
00:12:19,880 --> 00:12:22,340
Riduciamo lo zoom e riassumiamo dove siamo finora.

197
00:12:22,840 --> 00:12:26,784
La rete stessa è questa funzione con 784 ingressi e 10 uscite, 

198
00:12:26,784 --> 00:12:30,040
definite in termini di tutte queste somme ponderate.

199
00:12:30,640 --> 00:12:33,680
La funzione di costo è uno strato di complessità in più.

200
00:12:33,980 --> 00:12:37,440
Prende i 13.000 pesi e pregiudizi come input e produce 

201
00:12:37,440 --> 00:12:41,720
un'unica misura di pessimazza basata sugli esempi di formazione.

202
00:12:42,440 --> 00:12:44,859
E il gradiente della funzione di costo rappresenta 

203
00:12:44,859 --> 00:12:46,900
ancora un ulteriore livello di complessità.

204
00:12:47,360 --> 00:12:50,679
Ci dice quali spinte a tutti questi pesi e pregiudizi causano il 

205
00:12:50,679 --> 00:12:53,692
cambiamento più rapido nel valore della funzione di costo, 

206
00:12:53,692 --> 00:12:57,880
che potresti interpretare come dire quali cambiamenti a quali pesi contano di più.

207
00:13:02,560 --> 00:13:05,931
Quindi, quando inizializzi la rete con pesi e bias casuali e li 

208
00:13:05,931 --> 00:13:09,670
regoli molte volte in base a questo processo di discesa del gradiente, 

209
00:13:09,670 --> 00:13:13,200
quanto bene si comporta effettivamente su immagini mai viste prima?

210
00:13:14,100 --> 00:13:18,692
Quello che ho descritto qui, con i due strati nascosti di 16 neuroni ciascuno, 

211
00:13:18,692 --> 00:13:21,832
scelti soprattutto per ragioni estetiche, non è male, 

212
00:13:21,832 --> 00:13:25,960
classificando correttamente circa il 96% delle nuove immagini che vede.

213
00:13:26,680 --> 00:13:30,414
E onestamente, se guardi alcuni degli esempi in cui si incasina, 

214
00:13:30,414 --> 00:13:32,540
ti senti obbligato a darci un taglio.

215
00:13:36,220 --> 00:13:40,210
Ora, se giochi con la struttura dei livelli nascosti e apporti un paio di modifiche, 

216
00:13:40,210 --> 00:13:41,760
puoi ottenere questo fino al 98%.

217
00:13:41,760 --> 00:13:42,720
E questo è abbastanza buono!

218
00:13:43,020 --> 00:13:46,819
Non è il massimo, puoi sicuramente ottenere prestazioni migliori diventando 

219
00:13:46,819 --> 00:13:49,270
più sofisticato di questa semplice rete vanilla, 

220
00:13:49,270 --> 00:13:51,919
ma dato quanto sia scoraggiante il compito iniziale, 

221
00:13:51,919 --> 00:13:55,420
penso che ci sia qualcosa di incredibile nel fatto che qualsiasi rete 

222
00:13:55,420 --> 00:13:58,969
riesca così bene su immagini mai viste prima, dato che non gli abbiamo 

223
00:13:58,969 --> 00:14:01,420
mai detto specificatamente quali modelli cercare.

224
00:14:02,560 --> 00:14:06,170
Originariamente, il modo in cui ho motivato questa struttura era descrivendo una 

225
00:14:06,170 --> 00:14:09,825
speranza che potremmo avere, che il secondo strato potesse captare piccoli bordi, 

226
00:14:09,825 --> 00:14:13,525
che il terzo strato mettesse insieme quei bordi per riconoscere anelli e linee più 

227
00:14:13,525 --> 00:14:17,180
lunghe, e che questi potessero essere ricomposti insieme per riconoscere le cifre.

228
00:14:17,960 --> 00:14:20,400
Quindi è questo ciò che sta effettivamente facendo la nostra rete?

229
00:14:21,079 --> 00:14:24,400
Beh, almeno per questo, per niente.

230
00:14:24,820 --> 00:14:28,994
Ricordi come nell'ultimo video abbiamo visto come i pesi delle connessioni da tutti 

231
00:14:28,994 --> 00:14:32,742
i neuroni del primo strato a un dato neurone del secondo strato possono essere 

232
00:14:32,742 --> 00:14:36,585
visualizzati come un dato modello di pixel che il neurone del secondo strato sta 

233
00:14:36,585 --> 00:14:37,060
rilevando?

234
00:14:37,780 --> 00:14:43,080
Bene, quando lo facciamo effettivamente per i pesi associati a queste transizioni, 

235
00:14:43,080 --> 00:14:48,571
dal primo strato al successivo, invece di raccogliere piccoli bordi isolati qua e là, 

236
00:14:48,571 --> 00:14:53,680
sembrano, beh, quasi casuali, solo con alcuni schemi molto vaghi in il mezzo lì.

237
00:14:53,760 --> 00:14:57,560
Sembrerebbe che nell'insondabilmente ampio spazio di 13.000 dimensioni dei 

238
00:14:57,560 --> 00:15:01,360
possibili pesi e pregiudizi, la nostra rete si sia trovata un piccolo e felice 

239
00:15:01,360 --> 00:15:05,160
minimo locale che, nonostante abbia classificato con successo la maggior parte 

240
00:15:05,160 --> 00:15:08,960
delle immagini, non riprende esattamente gli schemi che avremmo potuto sperare.

241
00:15:09,780 --> 00:15:11,800
E per chiarire davvero questo punto, guarda cosa 

242
00:15:11,800 --> 00:15:13,820
succede quando inserisci un'immagine casuale.

243
00:15:14,320 --> 00:15:18,728
Se il sistema fosse intelligente, potresti aspettarti che si senta incerto, 

244
00:15:18,728 --> 00:15:23,485
magari non attivando realmente nessuno di quei 10 neuroni in uscita o attivandoli 

245
00:15:23,485 --> 00:15:28,358
tutti in modo uniforme, ma invece ti dà con sicurezza qualche risposta senza senso, 

246
00:15:28,358 --> 00:15:33,289
come se fosse sicuro che questo rumore casuale è un 5 così come l'immagine reale 

247
00:15:33,289 --> 00:15:34,160
di un 5 è un 5.

248
00:15:34,540 --> 00:15:39,121
In altre parole, anche se questa rete è in grado di riconoscere le cifre abbastanza bene, 

249
00:15:39,121 --> 00:15:40,700
non ha idea di come disegnarle.

250
00:15:41,420 --> 00:15:43,165
In gran parte ciò è dovuto al fatto che si tratta di 

251
00:15:43,165 --> 00:15:45,240
un'impostazione di allenamento così strettamente vincolata.

252
00:15:45,880 --> 00:15:47,740
Voglio dire, mettiti nei panni della rete qui.

253
00:15:48,140 --> 00:15:52,177
Dal suo punto di vista, l’intero universo non consiste altro che di cifre immobili 

254
00:15:52,177 --> 00:15:54,901
chiaramente definite centrate in una minuscola griglia, 

255
00:15:54,901 --> 00:15:58,939
e la sua funzione di costo non gli ha mai dato alcun incentivo ad essere altro che 

256
00:15:58,939 --> 00:16:01,080
completamente fiduciosi nelle sue decisioni.

257
00:16:02,120 --> 00:16:04,560
Quindi, con questa come immagine di ciò che stanno realmente 

258
00:16:04,560 --> 00:16:07,119
facendo i neuroni del secondo strato, potreste chiedervi perché 

259
00:16:07,119 --> 00:16:09,920
introdurrei questa rete con la motivazione di cogliere bordi e schemi.

260
00:16:09,920 --> 00:16:12,300
Voglio dire, non è affatto quello che finisce per fare.

261
00:16:13,380 --> 00:16:15,805
Ebbene, questo non vuole essere il nostro obiettivo finale, 

262
00:16:15,805 --> 00:16:17,180
ma piuttosto un punto di partenza.

263
00:16:17,640 --> 00:16:21,853
Francamente, questa è una tecnologia vecchia, del tipo studiato negli anni '80 e 

264
00:16:21,853 --> 00:16:26,066
'90, e devi capirla prima di poter comprendere varianti moderne più dettagliate, 

265
00:16:26,066 --> 00:16:29,486
ed è chiaramente in grado di risolvere alcuni problemi interessanti, 

266
00:16:29,486 --> 00:16:33,104
ma più approfondisci cosa quegli strati nascosti stanno davvero facendo, 

267
00:16:33,104 --> 00:16:34,740
tanto meno intelligenti sembrano.

268
00:16:38,480 --> 00:16:42,482
Spostando per un momento l'attenzione da come le reti apprendono a come impari tu, 

269
00:16:42,482 --> 00:16:46,300
ciò accadrà solo se ti impegnerai attivamente con il materiale qui in qualche modo.

270
00:16:47,060 --> 00:16:50,527
Una cosa piuttosto semplice che voglio che tu faccia è semplicemente 

271
00:16:50,527 --> 00:16:53,693
fermarti adesso e pensare profondamente per un momento a quali 

272
00:16:53,693 --> 00:16:56,859
modifiche potresti apportare a questo sistema e al modo in cui 

273
00:16:56,859 --> 00:17:00,880
percepisce le immagini se volessi che rilevasse meglio cose come bordi e motivi.

274
00:17:01,479 --> 00:17:04,373
Ma meglio di così, per interagire davvero con il materiale, 

275
00:17:04,373 --> 00:17:08,231
consiglio vivamente il libro di Michael Nielsen sull'apprendimento profondo 

276
00:17:08,231 --> 00:17:09,099
e le reti neurali.

277
00:17:09,680 --> 00:17:13,891
In esso puoi trovare il codice e i dati da scaricare e con cui giocare per questo 

278
00:17:13,891 --> 00:17:18,359
esatto esempio, e il libro ti guiderà passo dopo passo su cosa sta facendo quel codice.

279
00:17:19,300 --> 00:17:22,311
La cosa fantastica è che questo libro è gratuito e disponibile al pubblico, 

280
00:17:22,311 --> 00:17:24,965
quindi se ne trai qualcosa, prendi in considerazione l'idea di 

281
00:17:24,965 --> 00:17:27,660
unirti a me per fare una donazione a favore degli sforzi di Nielsen.

282
00:17:27,660 --> 00:17:32,029
Ho anche collegato un paio di altre risorse che mi piacciono molto nella descrizione, 

283
00:17:32,029 --> 00:17:36,500
incluso il fenomenale e bellissimo post sul blog di Chris Ola e gli articoli in Distill.

284
00:17:38,280 --> 00:17:41,127
Per chiudere qui per gli ultimi minuti, voglio tornare a un 

285
00:17:41,127 --> 00:17:43,880
frammento dell'intervista che ho avuto con Leisha Lee.

286
00:17:44,300 --> 00:17:45,905
Potresti ricordarla dall'ultimo video, ha 

287
00:17:45,905 --> 00:17:47,720
svolto il suo dottorato di ricerca in deep learning.

288
00:17:48,300 --> 00:17:50,666
In questo piccolo frammento parla di due articoli recenti che 

289
00:17:50,666 --> 00:17:53,108
approfondiscono davvero il modo in cui alcune delle più moderne 

290
00:17:53,108 --> 00:17:55,780
reti di riconoscimento delle immagini stanno effettivamente imparando.

291
00:17:56,120 --> 00:17:58,636
Giusto per stabilire il punto in cui eravamo nella conversazione, 

292
00:17:58,636 --> 00:18:01,800
il primo articolo ha preso una di queste reti neurali particolarmente profonde che 

293
00:18:01,800 --> 00:18:03,745
è davvero brava nel riconoscimento delle immagini, 

294
00:18:03,745 --> 00:18:06,414
e invece di addestrarla su un set di dati opportunamente etichettato, 

295
00:18:06,414 --> 00:18:08,740
ha mescolato tutte le etichette prima dell'addestramento.

296
00:18:09,480 --> 00:18:12,737
Ovviamente la precisione del test qui non era migliore di quella casuale, 

297
00:18:12,737 --> 00:18:16,478
poiché tutto è etichettato in modo casuale, ma è stato comunque in grado di ottenere 

298
00:18:16,478 --> 00:18:20,351
la stessa precisione di addestramento che si otterrebbe su un set di dati correttamente 

299
00:18:20,351 --> 00:18:20,880
etichettato.

300
00:18:21,600 --> 00:18:24,974
Fondamentalmente, i milioni di pesi per questa particolare rete erano 

301
00:18:24,974 --> 00:18:27,770
sufficienti per memorizzare semplicemente i dati casuali, 

302
00:18:27,770 --> 00:18:31,627
il che solleva la questione se minimizzare questa funzione di costo corrisponda 

303
00:18:31,627 --> 00:18:34,712
effettivamente a qualsiasi tipo di struttura nell'immagine, 

304
00:18:34,712 --> 00:18:36,400
o si tratta solo di memorizzazione?

305
00:18:51,440 --> 00:18:58,088
Se guardi quella curva di precisione, se ti stessi allenando su un set di dati casuale, 

306
00:18:58,088 --> 00:19:02,772
quella curva scendeva molto lentamente in modo quasi lineare, 

307
00:19:02,772 --> 00:19:07,984
quindi fai davvero fatica a trovare quel minimo locale di possibile, 

308
00:19:07,984 --> 00:19:12,140
sai , i pesi giusti che ti darebbero quella precisione.

309
00:19:12,240 --> 00:19:16,374
Mentre se ti stai effettivamente allenando su un set di dati strutturato, 

310
00:19:16,374 --> 00:19:20,174
uno che ha le etichette giuste, all'inizio giocheri un po', 

311
00:19:20,174 --> 00:19:24,364
ma poi scendi molto velocemente per arrivare a quel livello di precisione, 

312
00:19:24,364 --> 00:19:28,220
e quindi in un certo senso è era più facile trovare i massimi locali.

313
00:19:28,540 --> 00:19:33,789
E quindi la cosa interessante è che porta alla luce un altro documento di un paio di anni 

314
00:19:33,789 --> 00:19:37,522
fa, che presenta molte più semplificazioni sui livelli di rete, 

315
00:19:37,522 --> 00:19:42,304
ma uno dei risultati diceva che se si guarda al panorama dell'ottimizzazione, 

316
00:19:42,304 --> 00:19:47,320
i minimi locali che queste reti tendono ad apprendere sono in realtà di pari qualità, 

317
00:19:47,320 --> 00:19:50,937
quindi in un certo senso se il tuo set di dati è strutturato, 

318
00:19:50,937 --> 00:19:54,320
dovresti essere in grado di trovarlo molto più facilmente.

319
00:19:58,160 --> 00:20:01,180
I miei ringraziamenti, come sempre, a quelli di voi che sostengono su Patreon.

320
00:20:01,520 --> 00:20:04,224
Ho già detto in precedenza che Patreon rappresenta una svolta, 

321
00:20:04,224 --> 00:20:06,800
ma questi video non sarebbero davvero possibili senza di te.

322
00:20:07,460 --> 00:20:10,540
Voglio anche ringraziare in modo speciale la società di VC Amplify Partners, 

323
00:20:10,540 --> 00:20:12,780
per il suo supporto a questi video iniziali della serie.

