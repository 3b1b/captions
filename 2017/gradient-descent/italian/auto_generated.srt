1
00:00:00,000 --> 00:00:07,240
Nell&#39;ultimo video ho presentato la struttura di una rete neurale.

2
00:00:07,240 --> 00:00:11,560
Farò un breve riepilogo qui in modo che sia fresco nelle

3
00:00:11,560 --> 00:00:13,160
nostre menti, quindi ho due obiettivi principali per questo video.

4
00:00:13,160 --> 00:00:17,960
Il primo è introdurre l’idea della discesa del gradiente, che è alla base non solo del modo

5
00:00:17,960 --> 00:00:20,800
in cui le reti neurali apprendono, ma anche del funzionamento di molti altri sistemi di apprendimento automatico.

6
00:00:20,800 --> 00:00:25,160
Successivamente approfondiremo un po&#39; di più il funzionamento di questa particolare

7
00:00:25,160 --> 00:00:29,560
rete e cosa finiscono per cercare quegli strati nascosti di neuroni.

8
00:00:29,560 --> 00:00:34,680
Come promemoria, il nostro obiettivo qui è il classico esempio di riconoscimento

9
00:00:34,680 --> 00:00:37,080
delle cifre scritte a mano, il ciao mondo delle reti neurali.

10
00:00:37,080 --> 00:00:42,160
Queste cifre vengono visualizzate su una griglia di 28x28 pixel, ciascun pixel

11
00:00:42,160 --> 00:00:44,260
con un valore di scala di grigio compreso tra 0 e 1.

12
00:00:44,260 --> 00:00:51,400
Questi sono ciò che determina l&#39;attivazione di 784 neuroni nello strato di input della rete.

13
00:00:51,400 --> 00:00:56,880
L&#39;attivazione di ciascun neurone negli strati successivi si basa su una somma ponderata

14
00:00:56,880 --> 00:01:02,300
di tutte le attivazioni dello strato precedente, più un numero speciale chiamato bias.

15
00:01:02,300 --> 00:01:07,480
Componi quella somma con qualche altra funzione, come lo schiacciamento

16
00:01:07,480 --> 00:01:09,640
del sigmoide o un ReLU, come ho visto nell&#39;ultimo video.

17
00:01:09,640 --> 00:01:14,960
In totale, data la scelta un po&#39; arbitraria di due strati nascosti con

18
00:01:14,960 --> 00:01:20,940
16 neuroni ciascuno, la rete ha circa 13.000 pesi e bias che possiamo

19
00:01:20,940 --> 00:01:25,320
regolare, e sono questi valori che determinano esattamente cosa fa effettivamente la rete.

20
00:01:25,320 --> 00:01:29,800
E ciò che intendiamo quando diciamo che questa rete classifica una determinata cifra è che

21
00:01:29,800 --> 00:01:34,080
il più luminoso di quei 10 neuroni nello strato finale corrisponde a quella cifra.

22
00:01:34,080 --> 00:01:39,240
E ricorda, la motivazione che avevamo in mente per la struttura a

23
00:01:39,240 --> 00:01:43,920
strati era che forse il secondo strato poteva riprendere i bordi,

24
00:01:43,920 --> 00:01:48,640
il terzo strato poteva riprendere schemi come anelli e linee, e

25
00:01:48,640 --> 00:01:49,640
l&#39;ultimo poteva semplicemente mettere insieme quegli schemi per riconoscere le cifre.

26
00:01:49,640 --> 00:01:52,880
Quindi qui impariamo come apprende la rete.

27
00:01:52,880 --> 00:01:56,880
Quello che vogliamo è un algoritmo in cui puoi mostrare a questa rete un sacco di

28
00:01:56,880 --> 00:02:01,540
dati di addestramento, che si presentano sotto forma di un mucchio di immagini diverse di

29
00:02:01,540 --> 00:02:06,360
cifre scritte a mano, insieme alle etichette per quello che dovrebbero essere, e sarà aggiustare questi

30
00:02:06,360 --> 00:02:10,760
13.000 pesi e bias in modo da migliorare le sue prestazioni sui dati di allenamento.

31
00:02:10,760 --> 00:02:15,540
Si spera che questa struttura a strati significhi che ciò che

32
00:02:15,540 --> 00:02:17,840
apprende si generalizzi in immagini oltre i dati di addestramento.

33
00:02:17,840 --> 00:02:22,240
Il modo in cui lo testiamo è che dopo aver addestrato la rete, le

34
00:02:22,240 --> 00:02:31,160
mostri più dati etichettati e vedi con quanta precisione classifica quelle nuove immagini.

35
00:02:31,160 --> 00:02:34,760
Fortunatamente per noi, e ciò che rende questo un esempio comune con cui cominciare, è che

36
00:02:34,760 --> 00:02:39,520
le brave persone dietro il database MNIST hanno messo insieme una raccolta di decine di migliaia

37
00:02:39,520 --> 00:02:45,080
di immagini di cifre scritte a mano, ciascuna etichettata con i numeri che dovrebbero essere.

38
00:02:45,080 --> 00:02:49,920
E per quanto provocatorio sia descrivere una macchina in grado di apprendere, una volta visto come

39
00:02:49,920 --> 00:02:55,560
funziona, sembra molto meno una folle premessa fantascientifica e molto più un esercizio di calcolo.

40
00:02:55,560 --> 00:03:01,040
Voglio dire, fondamentalmente si tratta di trovare il minimo di una certa funzione.

41
00:03:01,040 --> 00:03:06,480
Ricorda, concettualmente pensiamo che ciascun neurone sia connesso a tutti i neuroni dello strato

42
00:03:06,480 --> 00:03:11,440
precedente, e i pesi nella somma ponderata che definisce la sua attivazione sono

43
00:03:11,440 --> 00:03:16,400
un po&#39; come i punti di forza di quelle connessioni, e il bias è

44
00:03:16,400 --> 00:03:19,780
una qualche indicazione di se quel neurone tende ad essere attivo o inattivo.

45
00:03:19,780 --> 00:03:23,300
E per iniziare, inizializzeremo tutti questi pesi

46
00:03:23,300 --> 00:03:25,020
e pregiudizi in modo totalmente casuale.

47
00:03:25,020 --> 00:03:29,100
Inutile dire che questa rete funzionerà in modo pessimo con un

48
00:03:29,100 --> 00:03:31,180
dato esempio di formazione, poiché sta semplicemente facendo qualcosa di casuale.

49
00:03:31,180 --> 00:03:36,820
Ad esempio, inserisci questa immagine di un 3 e il livello di output sembra semplicemente un disastro.

50
00:03:36,820 --> 00:03:43,340
Quindi quello che fai è definire una funzione di costo, un modo per dire al computer, no, cattivo computer,

51
00:03:43,340 --> 00:03:48,940
che l&#39;output dovrebbe avere attivazioni che sono 0 per la maggior parte dei neuroni, ma 1 per questo neurone.

52
00:03:48,980 --> 00:03:51,740
Ciò che mi hai dato è pura spazzatura.

53
00:03:51,740 --> 00:03:56,740
Per dirlo in modo un po&#39; più matematico, sommi i quadrati delle differenze tra ciascuna

54
00:03:56,740 --> 00:04:01,980
di quelle attivazioni di output dei rifiuti e il valore che vuoi che abbiano,

55
00:04:01,980 --> 00:04:06,020
e questo è quello che chiameremo il costo di un singolo esempio di formazione.

56
00:04:06,020 --> 00:04:12,660
Si noti che questa somma è piccola quando la rete classifica correttamente l&#39;immagine con

57
00:04:12,660 --> 00:04:18,820
sicurezza, ma è grande quando sembra che la rete non sappia cosa sta facendo.

58
00:04:18,820 --> 00:04:23,860
Quindi quello che fai è considerare il costo medio di tutte

59
00:04:23,860 --> 00:04:27,580
le decine di migliaia di esempi di formazione a tua disposizione.

60
00:04:27,580 --> 00:04:32,300
Questo costo medio è la nostra misura di quanto sia scadente

61
00:04:32,300 --> 00:04:33,300
la rete e di quanto pessimo dovrebbe sentirsi il computer.

62
00:04:33,300 --> 00:04:35,300
E questa è una cosa complicata.

63
00:04:35,300 --> 00:04:40,380
Ricordi che la rete stessa era fondamentalmente una funzione, che accetta 784 numeri

64
00:04:40,380 --> 00:04:46,100
come input, i valori dei pixel, e restituisce 10 numeri come output, e

65
00:04:46,100 --> 00:04:49,700
in un certo senso è parametrizzata da tutti questi pesi e pregiudizi?

66
00:04:49,700 --> 00:04:53,340
La funzione di costo è uno strato di complessità in più.

67
00:04:53,340 --> 00:04:59,140
Prende come input quei circa 13.000 pesi e pregiudizi e sputa un singolo numero che

68
00:04:59,140 --> 00:05:04,620
descrive quanto siano gravi tali pesi e pregiudizi, e il modo in cui viene definito

69
00:05:04,620 --> 00:05:09,140
dipende dal comportamento della rete su tutte le decine di migliaia di dati di addestramento.

70
00:05:09,140 --> 00:05:12,460
C&#39;è molto a cui pensare.

71
00:05:12,460 --> 00:05:16,380
Ma limitarsi a dire al computer che lavoro schifoso sta facendo non è molto utile.

72
00:05:16,380 --> 00:05:21,300
Vuoi dirgli come cambiare quei pesi e pregiudizi in modo che migliori.

73
00:05:21,300 --> 00:05:25,580
Per rendere più semplice, anziché faticare a immaginare una funzione con 13.000 input, immagina

74
00:05:25,580 --> 00:05:31,440
una semplice funzione che abbia un numero come input e un numero come output.

75
00:05:31,440 --> 00:05:36,420
Come trovi un input che minimizzi il valore di questa funzione?

76
00:05:36,420 --> 00:05:41,300
Gli studenti di calcolo sapranno che a volte è possibile calcolare esplicitamente quel minimo, ma ciò

77
00:05:41,340 --> 00:05:46,620
non è sempre fattibile per funzioni veramente complicate, certamente non nella versione da 13.000 input

78
00:05:46,620 --> 00:05:51,640
di questa situazione per la nostra folle e complicata funzione di costo della rete neurale.

79
00:05:51,640 --> 00:05:56,820
Una tattica più flessibile è quella di iniziare da qualsiasi input

80
00:05:56,820 --> 00:05:59,860
e capire in quale direzione dovresti procedere per ridurre tale output.

81
00:05:59,860 --> 00:06:05,020
Nello specifico, se riesci a calcolare la pendenza della funzione nel punto

82
00:06:05,020 --> 00:06:09,280
in cui ti trovi, spostati a sinistra se la pendenza è

83
00:06:09,280 --> 00:06:12,720
positiva e sposta l&#39;input a destra se la pendenza è negativa.

84
00:06:12,720 --> 00:06:17,040
Se lo fai ripetutamente, controllando in ogni punto la nuova pendenza e

85
00:06:17,040 --> 00:06:20,680
facendo il passo appropriato, ti avvicinerai ad un minimo locale della funzione.

86
00:06:20,680 --> 00:06:24,600
E l&#39;immagine che potresti avere in mente qui è una palla che rotola giù da una collina.

87
00:06:24,600 --> 00:06:29,380
E nota, anche per questa funzione di input singolo davvero semplificata, ci sono

88
00:06:29,380 --> 00:06:34,220
molte possibili valli in cui potresti atterrare, a seconda dell&#39;input casuale da

89
00:06:34,220 --> 00:06:38,460
cui inizi, e non c&#39;è alcuna garanzia che il minimo locale in

90
00:06:38,460 --> 00:06:39,460
cui atterri sarà il valore più piccolo possibile della funzione di costo.

91
00:06:39,460 --> 00:06:43,180
Ciò si ripercuoterà anche sul caso della nostra rete neurale.

92
00:06:43,180 --> 00:06:48,140
E voglio anche che tu noti come se rendi le dimensioni dei tuoi passi

93
00:06:48,140 --> 00:06:52,920
proporzionali alla pendenza, quando la pendenza si appiattisce verso il minimo, i tuoi passi

94
00:06:52,920 --> 00:06:56,020
diventano sempre più piccoli, e questo ti aiuta a non superare il limite.

95
00:06:56,020 --> 00:07:01,640
Aumentando un po&#39; la complessità, immagina invece una funzione con due input e un output.

96
00:07:01,640 --> 00:07:06,360
Potresti pensare allo spazio di input come al piano xy e alla

97
00:07:06,360 --> 00:07:09,020
funzione di costo come rappresentata graficamente come una superficie sopra di esso.

98
00:07:09,020 --> 00:07:13,600
Invece di chiedere informazioni sulla pendenza della funzione, devi chiederti in quale direzione dovresti muoverti

99
00:07:13,600 --> 00:07:19,780
in questo spazio di input in modo da diminuire più rapidamente l&#39;output della funzione.

100
00:07:19,780 --> 00:07:22,340
In altre parole, qual è la direzione in discesa?

101
00:07:22,340 --> 00:07:26,740
E ancora, è utile pensare a una palla che rotola giù da quella collina.

102
00:07:26,740 --> 00:07:31,920
Quelli di voi che hanno familiarità con il calcolo multivariabile sapranno che

103
00:07:31,920 --> 00:07:37,460
il gradiente di una funzione ti dà la direzione dell&#39;ascesa più

104
00:07:37,460 --> 00:07:39,420
ripida, quale direzione dovresti fare per aumentare la funzione più rapidamente.

105
00:07:39,420 --> 00:07:43,820
Naturalmente, prendendo il negativo di quel gradiente si ottiene la

106
00:07:43,820 --> 00:07:47,460
direzione del passo che diminuisce la funzione più rapidamente.

107
00:07:47,460 --> 00:07:52,320
Ancor di più, la lunghezza di questo vettore gradiente è

108
00:07:52,320 --> 00:07:54,580
un&#39;indicazione di quanto sia ripido il pendio più ripido.

109
00:07:54,580 --> 00:07:58,080
Ora, se non hai familiarità con il calcolo multivariabile e desideri saperne di più,

110
00:07:58,080 --> 00:08:01,100
dai un&#39;occhiata ad alcuni dei lavori che ho svolto per Khan Academy sull&#39;argomento.

111
00:08:01,100 --> 00:08:05,680
Onestamente, però, tutto ciò che conta per me e te in questo momento è

112
00:08:05,680 --> 00:08:10,440
che in linea di principio esiste un modo per calcolare questo vettore, questo vettore

113
00:08:10,440 --> 00:08:12,040
che ti dice qual è la direzione in discesa e quanto è ripida.

114
00:08:12,040 --> 00:08:17,280
Starai bene se questo è tutto quello che sai e non sei solido come una roccia sui dettagli.

115
00:08:17,280 --> 00:08:21,440
Perché se riesci a capirlo, l&#39;algoritmo per minimizzare la funzione è calcolare questa direzione

116
00:08:21,440 --> 00:08:27,400
del gradiente, quindi fare un piccolo passo in discesa e ripeterlo ancora e ancora.

117
00:08:28,300 --> 00:08:33,700
È la stessa idea di base per una funzione che ha 13.000 input invece di 2 input.

118
00:08:33,700 --> 00:08:38,980
Immagina di organizzare tutti i 13.000 pesi e pregiudizi

119
00:08:38,980 --> 00:08:40,180
della nostra rete in un gigantesco vettore di colonne.

120
00:08:40,180 --> 00:08:46,140
Il gradiente negativo della funzione di costo è solo un vettore, è una direzione

121
00:08:46,140 --> 00:08:51,660
all&#39;interno di questo spazio di input follemente enorme che ti dice quali spinte

122
00:08:51,660 --> 00:08:55,900
a tutti quei numeri causeranno la diminuzione più rapida della funzione di costo.

123
00:08:55,900 --> 00:09:00,000
E ovviamente, con la nostra funzione di costo appositamente progettata, modificare i

124
00:09:00,000 --> 00:09:05,520
pesi e i bias per ridurli significa far sì che l&#39;output della

125
00:09:05,520 --> 00:09:10,280
rete su ciascun dato di addestramento assomigli meno a un array casuale

126
00:09:10,280 --> 00:09:11,280
di 10 valori e più a una decisione effettiva che vogliamo. farlo.

127
00:09:11,280 --> 00:09:15,940
È importante ricordare che questa funzione di costo implica una media su tutti i dati di addestramento,

128
00:09:15,940 --> 00:09:24,260
quindi se la riduci al minimo, significa che ci sono prestazioni migliori su tutti questi campioni.

129
00:09:24,260 --> 00:09:28,540
L&#39;algoritmo per calcolare questo gradiente in modo efficiente, che è

130
00:09:28,540 --> 00:09:32,520
effettivamente il cuore dell&#39;apprendimento di una rete neurale, si chiama

131
00:09:32,520 --> 00:09:34,040
backpropagation, ed è ciò di cui parlerò nel prossimo video.

132
00:09:34,040 --> 00:09:39,100
Lì, voglio davvero prendermi il tempo per esaminare cosa succede esattamente a ciascun peso

133
00:09:39,100 --> 00:09:44,100
e pregiudizio per un dato dato di addestramento, cercando di dare un&#39;idea intuitiva

134
00:09:44,100 --> 00:09:47,980
di ciò che sta accadendo oltre la pila di calcoli e formule pertinenti.

135
00:09:47,980 --> 00:09:51,780
Proprio qui, proprio ora, la cosa principale che voglio che tu sappia, indipendentemente

136
00:09:51,780 --> 00:09:56,820
dai dettagli di implementazione, è che ciò che intendiamo quando parliamo di

137
00:09:56,820 --> 00:09:59,320
apprendimento in rete è che sta semplicemente minimizzando una funzione di costo.

138
00:09:59,320 --> 00:10:02,760
E notate, una conseguenza di ciò è che è importante che questa

139
00:10:02,760 --> 00:10:07,820
funzione di costo abbia un output abbastanza regolare, in modo da

140
00:10:07,820 --> 00:10:09,340
poter trovare un minimo locale facendo piccoli passi verso il basso.

141
00:10:09,340 --> 00:10:14,140
Questo è il motivo per cui, tra l&#39;altro, i neuroni artificiali

142
00:10:14,140 --> 00:10:18,580
hanno attivazioni che variano continuamente, piuttosto che essere semplicemente attivi o

143
00:10:18,580 --> 00:10:20,440
inattivi in modo binario, come lo sono i neuroni biologici.

144
00:10:20,440 --> 00:10:24,600
Questo processo di spostamento ripetuto dell&#39;input di una funzione di

145
00:10:24,600 --> 00:10:26,960
un multiplo del gradiente negativo è chiamato discesa del gradiente.

146
00:10:26,960 --> 00:10:31,760
È un modo per convergere verso un minimo locale di

147
00:10:31,760 --> 00:10:33,000
una funzione di costo, sostanzialmente una valle in questo grafico.

148
00:10:33,000 --> 00:10:37,040
Sto ancora mostrando l&#39;immagine di una funzione con due input, ovviamente, perché i

149
00:10:37,040 --> 00:10:41,480
nudge in uno spazio di input a 13.000 dimensioni sono un po&#39; difficili

150
00:10:41,480 --> 00:10:45,220
da comprendere, ma in realtà esiste un bel modo non spaziale di pensarci.

151
00:10:45,220 --> 00:10:49,100
Ogni componente del gradiente negativo ci dice due cose.

152
00:10:49,100 --> 00:10:53,600
Il segno, ovviamente, ci dice se la componente corrispondente del vettore

153
00:10:53,600 --> 00:10:55,860
di input deve essere spostata verso l&#39;alto o verso il basso.

154
00:10:55,860 --> 00:11:01,340
Ma, cosa ancora più importante, l’entità relativa di tutti

155
00:11:01,340 --> 00:11:05,620
questi componenti indica quali cambiamenti contano di più.

156
00:11:05,620 --> 00:11:09,780
Vedete, nella nostra rete, un aggiustamento a uno dei pesi potrebbe avere un

157
00:11:09,780 --> 00:11:14,980
impatto molto maggiore sulla funzione di costo rispetto all&#39;aggiustamento a qualche altro peso.

158
00:11:14,980 --> 00:11:19,440
Alcune di queste connessioni contano di più per i nostri dati di addestramento.

159
00:11:19,440 --> 00:11:23,520
Quindi un modo in cui puoi pensare a questo vettore gradiente della nostra

160
00:11:23,520 --> 00:11:29,740
enorme funzione di costo è che codifica l&#39;importanza relativa di ogni peso

161
00:11:29,740 --> 00:11:34,100
e pregiudizio, cioè quale di questi cambiamenti porterà il maggior rapporto qualità-prezzo.

162
00:11:34,100 --> 00:11:37,360
Questo è davvero solo un altro modo di pensare alla direzione.

163
00:11:37,360 --> 00:11:41,740
Per fare un esempio più semplice, se hai una funzione con due variabili

164
00:11:41,740 --> 00:11:48,720
come input e calcoli che il suo gradiente in un punto particolare risulta

165
00:11:48,720 --> 00:11:52,880
come 3,1, allora da un lato puoi interpretarlo come se dicessi che quando

166
00:11:52,880 --> 00:11:57,400
sei stando su quell&#39;input, muovendosi lungo questa direzione la funzione aumenta più rapidamente,

167
00:11:57,400 --> 00:12:02,200
quindi quando rappresenti graficamente la funzione sopra il piano dei punti di input,

168
00:12:02,200 --> 00:12:03,200
quel vettore è ciò che ti dà la direzione diritta in salita.

169
00:12:03,200 --> 00:12:07,600
Ma un altro modo di leggerlo è dire che le modifiche a questa prima

170
00:12:07,600 --> 00:12:12,400
variabile hanno tre volte l&#39;importanza delle modifiche alla seconda variabile, che almeno nelle vicinanze

171
00:12:12,400 --> 00:12:17,740
dell&#39;input rilevante, spostare il valore x porta molto più effetto per il tuo secchio.

172
00:12:17,740 --> 00:12:22,880
Va bene, rimpiccioliamo e riassumiamo dove siamo finora.

173
00:12:22,880 --> 00:12:28,660
La rete stessa è questa funzione con 784 ingressi e

174
00:12:28,660 --> 00:12:30,860
10 uscite, definite in termini di tutte queste somme ponderate.

175
00:12:30,860 --> 00:12:34,160
La funzione di costo è uno strato di complessità in più.

176
00:12:34,160 --> 00:12:39,300
Prende i 13.000 pesi e pregiudizi come input e produce

177
00:12:39,300 --> 00:12:42,640
un&#39;unica misura di pessimazza basata sugli esempi di formazione.

178
00:12:42,640 --> 00:12:47,520
Il gradiente della funzione di costo rappresenta ancora un ulteriore livello di complessità.

179
00:12:47,520 --> 00:12:52,860
Ci dice quali spinte a tutti questi pesi e pregiudizi causano il

180
00:12:52,860 --> 00:12:56,640
cambiamento più rapido nel valore della funzione di costo, che potresti

181
00:12:56,640 --> 00:13:03,040
interpretare come dire quali cambiamenti a quali pesi contano di più.

182
00:13:03,040 --> 00:13:07,620
Quindi, quando inizializzi la rete con pesi e bias casuali e li

183
00:13:07,620 --> 00:13:12,420
regoli molte volte in base a questo processo di discesa del

184
00:13:12,420 --> 00:13:14,240
gradiente, quanto bene si comporta effettivamente su immagini mai viste prima?

185
00:13:14,240 --> 00:13:19,000
Quello che ho descritto qui, con i due strati nascosti di 16 neuroni ciascuno, scelti soprattutto

186
00:13:19,000 --> 00:13:26,920
per ragioni estetiche, non è male, classificando correttamente circa il 96% delle nuove immagini che vede.

187
00:13:26,920 --> 00:13:31,580
E onestamente, se guardi alcuni degli esempi in cui

188
00:13:31,580 --> 00:13:36,300
si incasina, ti senti obbligato a darci un taglio.

189
00:13:36,300 --> 00:13:40,220
Se giochi con la struttura dei livelli nascosti e apporti

190
00:13:40,220 --> 00:13:41,220
un paio di modifiche, puoi ottenere questo fino al 98%.

191
00:13:41,220 --> 00:13:42,900
E questo è abbastanza buono!

192
00:13:42,900 --> 00:13:47,020
Non è il massimo, puoi sicuramente ottenere prestazioni migliori diventando più sofisticato di questa semplice

193
00:13:47,020 --> 00:13:52,460
rete vanilla, ma considerando quanto sia scoraggiante il compito iniziale, penso che ci sia

194
00:13:52,460 --> 00:13:56,800
qualcosa di incredibile nel fatto che qualsiasi rete riesca così bene su immagini mai

195
00:13:56,800 --> 00:14:02,000
viste prima, dato che noi non gli ho mai detto specificamente quali modelli cercare.

196
00:14:02,000 --> 00:14:07,840
Originariamente, il modo in cui ho motivato questa struttura era descrivendo una speranza

197
00:14:07,840 --> 00:14:11,880
che potremmo avere, che il secondo strato potesse captare piccoli bordi, che

198
00:14:11,880 --> 00:14:16,080
il terzo strato mettesse insieme quei bordi per riconoscere anelli e linee più

199
00:14:16,080 --> 00:14:18,220
lunghe, e che questi potessero essere ricomposti insieme per riconoscere le cifre.

200
00:14:18,220 --> 00:14:21,040
Quindi è questo ciò che sta effettivamente facendo la nostra rete?

201
00:14:21,040 --> 00:14:24,880
Beh, almeno per questo, per niente.

202
00:14:24,960 --> 00:14:29,120
Ricordi come nell&#39;ultimo video abbiamo visto come i pesi delle connessioni da tutti i

203
00:14:29,120 --> 00:14:33,900
neuroni del primo strato a un dato neurone del secondo strato possono essere visualizzati

204
00:14:33,900 --> 00:14:37,440
come un dato modello di pixel che il neurone del secondo strato sta rilevando?

205
00:14:37,440 --> 00:14:44,600
Bene, quando lo facciamo per i pesi associati a queste transizioni,

206
00:14:44,600 --> 00:14:51,000
invece di raccogliere piccoli bordi isolati qua e là, sembrano, beh,

207
00:14:51,000 --> 00:14:54,200
quasi casuali, solo con alcuni schemi molto vaghi nel mezzo.

208
00:14:54,200 --> 00:14:59,020
Sembrerebbe che nell&#39;insondabilmente ampio spazio di 13.000 dimensioni dei possibili pesi e

209
00:14:59,020 --> 00:15:04,020
pregiudizi, la nostra rete si sia trovata un piccolo e felice

210
00:15:04,020 --> 00:15:08,440
minimo locale che, nonostante abbia classificato con successo la maggior parte

211
00:15:08,440 --> 00:15:09,840
delle immagini, non riprende esattamente gli schemi che avremmo potuto sperare.

212
00:15:09,840 --> 00:15:14,600
E per chiarire davvero questo punto, guarda cosa succede quando inserisci un&#39;immagine casuale.

213
00:15:14,600 --> 00:15:19,240
Se il sistema fosse intelligente, potresti aspettarti che si senta incerto, magari non attivando realmente

214
00:15:19,240 --> 00:15:24,120
nessuno di quei 10 neuroni in uscita o attivandoli tutti in modo uniforme, ma invece

215
00:15:24,520 --> 00:15:29,800
ti dà con sicurezza qualche risposta senza senso, come se fosse sicuro che questo casuale

216
00:15:29,800 --> 00:15:34,560
il rumore è un 5 così come un&#39;immagine reale di un 5 è un 5.

217
00:15:34,560 --> 00:15:39,300
In altre parole, anche se questa rete è in grado di

218
00:15:39,300 --> 00:15:41,800
riconoscere le cifre abbastanza bene, non ha idea di come disegnarle.

219
00:15:41,800 --> 00:15:45,400
In gran parte ciò è dovuto al fatto che si tratta di un&#39;impostazione di allenamento così strettamente vincolata.

220
00:15:45,400 --> 00:15:48,220
Voglio dire, mettiti nei panni della rete qui.

221
00:15:48,220 --> 00:15:53,280
Dal suo punto di vista, l’intero universo non consiste altro che di cifre immobili chiaramente

222
00:15:53,280 --> 00:15:58,560
definite centrate in una minuscola griglia, e la sua funzione di costo non gli

223
00:15:58,560 --> 00:16:02,160
ha mai dato alcun incentivo ad essere altro che completamente fiduciosi nelle sue decisioni.

224
00:16:02,160 --> 00:16:05,760
Quindi, con questa come immagine di ciò che stanno realmente

225
00:16:05,760 --> 00:16:09,320
facendo i neuroni del secondo strato, potreste chiedervi perché introdurrei

226
00:16:09,320 --> 00:16:10,320
questa rete con la motivazione di cogliere bordi e schemi.

227
00:16:10,320 --> 00:16:13,040
Voglio dire, non è affatto quello che finisce per fare.

228
00:16:13,040 --> 00:16:17,480
Ebbene, questo non vuole essere il nostro obiettivo finale, ma piuttosto un punto di partenza.

229
00:16:17,480 --> 00:16:22,280
Francamente, questa è una tecnologia vecchia, del tipo studiato negli anni &#39;80 e

230
00:16:22,280 --> 00:16:26,920
&#39;90, e devi capirla prima di poter comprendere varianti moderne più dettagliate,

231
00:16:26,920 --> 00:16:31,380
ed è chiaramente in grado di risolvere alcuni problemi interessanti, ma più

232
00:16:31,380 --> 00:16:38,720
approfondisci cosa quegli strati nascosti stanno davvero facendo, tanto meno intelligenti sembrano.

233
00:16:38,720 --> 00:16:43,540
Spostando per un momento l&#39;attenzione da come le reti apprendono a come impari tu,

234
00:16:43,540 --> 00:16:47,160
ciò accadrà solo se ti impegnerai attivamente con il materiale qui in qualche modo.

235
00:16:47,160 --> 00:16:51,920
Una cosa piuttosto semplice che voglio che tu faccia è semplicemente fermarti adesso e pensare

236
00:16:51,920 --> 00:16:57,560
profondamente per un momento a quali modifiche potresti apportare a questo sistema e al modo

237
00:16:57,560 --> 00:17:01,880
in cui percepisce le immagini se volessi che rilevasse meglio cose come bordi e motivi.

238
00:17:01,880 --> 00:17:06,360
Ma meglio di così, per interagire davvero con il materiale, consiglio vivamente

239
00:17:06,360 --> 00:17:09,720
il libro di Michael Nielsen sull&#39;apprendimento profondo e le reti neurali.

240
00:17:09,720 --> 00:17:15,200
In esso puoi trovare il codice e i dati da scaricare e con cui giocare per questo

241
00:17:15,200 --> 00:17:19,360
esatto esempio, e il libro ti guiderà passo dopo passo su cosa sta facendo quel codice.

242
00:17:19,360 --> 00:17:23,920
La cosa fantastica è che questo libro è gratuito e disponibile al pubblico, quindi se ne trai qualcosa,

243
00:17:23,920 --> 00:17:28,040
prendi in considerazione l&#39;idea di unirti a me per fare una donazione a favore degli sforzi di Nielsen.

244
00:17:28,040 --> 00:17:32,060
Ho anche collegato un paio di altre risorse che mi piacciono molto nella descrizione, incluso

245
00:17:32,060 --> 00:17:38,720
il fenomenale e bellissimo post sul blog di Chris Ola e gli articoli in Distill.

246
00:17:38,720 --> 00:17:41,960
Per chiudere qui per gli ultimi minuti, voglio tornare a

247
00:17:41,960 --> 00:17:44,440
un frammento dell&#39;intervista che ho avuto con Leisha Lee.

248
00:17:44,440 --> 00:17:48,520
Potresti ricordarla dall&#39;ultimo video, ha svolto il suo dottorato di ricerca in deep learning.

249
00:17:48,560 --> 00:17:52,240
In questo piccolo frammento, parla di due articoli recenti che approfondiscono davvero il modo

250
00:17:52,240 --> 00:17:56,380
in cui alcune delle più moderne reti di riconoscimento delle immagini stanno effettivamente imparando.

251
00:17:56,380 --> 00:18:00,320
Giusto per stabilire a che punto eravamo nella conversazione, il primo articolo ha preso una di

252
00:18:00,320 --> 00:18:04,480
queste reti neurali particolarmente profonde che è davvero brava nel riconoscimento delle immagini, e invece di

253
00:18:04,480 --> 00:18:09,400
addestrarla su un set di dati opportunamente etichettato, ha mescolato tutte le etichette prima dell&#39;addestramento.

254
00:18:09,400 --> 00:18:13,840
Ovviamente la precisione del test qui non sarebbe stata migliore di

255
00:18:13,840 --> 00:18:15,320
quella casuale, dal momento che tutto è etichettato in modo casuale.

256
00:18:15,320 --> 00:18:20,080
Ma è stato comunque in grado di ottenere la stessa precisione di

257
00:18:20,080 --> 00:18:21,440
addestramento che si otterrebbe su un set di dati correttamente etichettato.

258
00:18:21,440 --> 00:18:26,120
Fondamentalmente, i milioni di pesi per questa particolare rete erano sufficienti per memorizzare semplicemente

259
00:18:26,120 --> 00:18:31,040
i dati casuali, il che solleva la questione se minimizzare questa funzione di costo

260
00:18:31,040 --> 00:18:36,720
corrisponda effettivamente a qualsiasi tipo di struttura nell&#39;immagine, o si tratti solo di memorizzazione?

261
00:18:36,720 --> 00:18:40,120
. . . per memorizzare l&#39;intero set di dati di quale sia la classificazione corretta.

262
00:18:40,120 --> 00:18:45,720
E così un paio di, sei mesi dopo, all&#39;ICML di quest&#39;anno, non c&#39;era esattamente

263
00:18:45,720 --> 00:18:50,440
un documento di confutazione, ma un documento che affrontava alcuni aspetti del tipo, ehi,

264
00:18:50,440 --> 00:18:52,220
in realtà queste reti stanno facendo qualcosa di un po&#39; più intelligente di così.

265
00:18:52,220 --> 00:18:59,600
Se guardi quella curva di precisione, se ti stessi allenando su un set di

266
00:18:59,600 --> 00:19:05,240
dati casuale, quella curva è scesa molto, sai, molto lentamente in modo quasi lineare.

267
00:19:05,280 --> 00:19:10,840
Quindi stai davvero lottando per trovare i minimi locali possibili,

268
00:19:10,840 --> 00:19:12,320
sai, i pesi giusti che ti darebbero quella precisione.

269
00:19:12,320 --> 00:19:16,720
Mentre se ti stai effettivamente allenando su un set di dati strutturato,

270
00:19:16,720 --> 00:19:20,240
uno che ha le etichette giuste, sai, all&#39;inizio giocheri un po&#39;, ma

271
00:19:20,240 --> 00:19:23,360
poi scendi molto velocemente per arrivare a quel livello di precisione.

272
00:19:23,360 --> 00:19:28,580
E quindi in un certo senso è stato più facile trovare i massimi locali.

273
00:19:28,580 --> 00:19:32,900
E quindi la cosa interessante è che porta alla luce

274
00:19:32,900 --> 00:19:39,140
un altro documento di un paio di anni fa,

275
00:19:39,140 --> 00:19:40,140
che contiene molte più semplificazioni sui livelli di rete.

276
00:19:40,140 --> 00:19:43,880
Ma uno dei risultati è stato che, se si guarda al panorama dell’ottimizzazione, i

277
00:19:43,880 --> 00:19:49,400
minimi locali che queste reti tendono ad apprendere sono in realtà di pari qualità.

278
00:19:49,400 --> 00:19:54,300
Quindi, in un certo senso, se il tuo set di dati è strutturato, dovresti essere in grado di trovarlo molto più facilmente.

279
00:19:58,580 --> 00:20:01,140
I miei ringraziamenti come sempre a quelli di voi che sostengono su Patreon.

280
00:20:01,480 --> 00:20:05,440
Ho già detto in precedenza quale sia la svolta su Patreon,

281
00:20:05,440 --> 00:20:07,160
ma questi video non sarebbero davvero possibili senza di te.

282
00:20:07,160 --> 00:20:11,540
Voglio anche ringraziare in modo speciale la società di VC Amplify

283
00:20:11,540 --> 00:20:13,240
Partners e il loro supporto per questi video iniziali della serie.

284
00:20:31,140 --> 00:20:33,140
Grazie.

