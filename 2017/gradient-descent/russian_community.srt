1
00:00:04,070 --> 00:00:07,059
В последнем видео я изложил структуру нейронной сети

2
00:00:07,160 --> 00:00:10,089
Здесь я дам краткое описание структуры, чтобы освежить память

3
00:00:10,089 --> 00:00:15,368
И тогда у меня есть две основные цели для этого видео. Первый - представить идею градиентного спуска,

4
00:00:15,650 --> 00:00:18,219
которая лежит в основе не только того, как учатся нейронные сети,

5
00:00:18,220 --> 00:00:20,439
но как работает еще много других машин.

6
00:00:20,660 --> 00:00:24,609
Затем, после этого, мы собираемся еще немного разобраться в том, как эта конкретная сеть работает

7
00:00:24,609 --> 00:00:27,758
И то, что скрывают эти скрытые слои нейронов, фактически ищет

8
00:00:28,999 --> 00:00:33,489
Напоминаем, что наша цель - классический пример распознавания рукописного знака

9
00:00:34,120 --> 00:00:36,120
привет мир нейронных сетей

10
00:00:36,500 --> 00:00:43,100
эти цифры отображаются на сетке размером 28 х 28 пикселей каждый пиксель с некоторым значением оттенков серого между 0 и 1

11
00:00:43,600 --> 00:00:46,080
это то, что определяют активации

12
00:00:46,850 --> 00:00:50,199
784 нейронов во входном слое сети и

13
00:00:50,840 --> 00:00:55,719
Тогда активация для каждого нейрона в следующих слоях основана на взвешенной сумме

14
00:00:56,000 --> 00:01:00,639
Всех активаций в предыдущем слое плюс некоторое специальное значение, называемое смещением

15
00:01:01,699 --> 00:01:06,338
затем вы составляете эту сумму с помощью какой-либо другой функции, такой как сигмовидное уплотнение или

16
00:01:06,400 --> 00:01:08,769
ReLu, описание которого я предоставил в последнем видео

17
00:01:09,110 --> 00:01:15,729
В целом, учитывая мой произвольный выбор двух скрытых слоев с 16 нейронами, каждая из которых имеет около

18
00:01:16,579 --> 00:01:24,159
13 000 весов и смещений, которые мы можем настроить, и именно эти значения определяют, что именно сеть, которую вы видите, действительно делает

19
00:01:24,799 --> 00:01:28,328
Тогда что мы имеем в виду, когда говорим, что эта сеть классифицирует данную цифру

20
00:01:28,329 --> 00:01:33,429
Самый яркий из этих 10 нейронов в конечном слое соответствует этой цифре

21
00:01:33,950 --> 00:01:38,589
И запомните, что мотивация, которую мы имели в виду здесь для многоуровневой структуры, заключалась в том, что, возможно,

22
00:01:38,780 --> 00:01:44,680
Второй слой может распознать кусочки, а третий слой может набирать рисунки, такие как петли и линии

23
00:01:44,930 --> 00:01:48,729
И последний мог просто объединить эти шаблоны, чтобы распознавать цифры

24
00:01:49,369 --> 00:01:52,029
Итак, здесь мы узнаем, как сеть обучается

25
00:01:52,399 --> 00:01:57,099
Нам нужен алгоритм, в котором вы cможете показать этой сети целую кучу обучающих данных,

26
00:01:57,229 --> 00:02:03,669
которая поставляется в виде кучи разных изображений рукописных цифр вместе с пометками, что это за цифры

27
00:02:03,890 --> 00:02:05,659
Он будет корректировать эти

28
00:02:05,659 --> 00:02:09,789
13000 весов и смещений для того, чтобы улучшить его производительность по данным обучения

29
00:02:10,730 --> 00:02:13,569
Надеемся, что эта слоистая структура усвоит то,чему ее обучают

30
00:02:14,269 --> 00:02:16,719
и обобщит изображения, выходящие за рамки обучающих данных

31
00:02:16,720 --> 00:02:20,289
И мы это протестируем после обучения сети

32
00:02:20,290 --> 00:02:26,560
Вы показываете алгоритму дополнительные изображения, которые он раньше не видел, и увидите, насколько точно он классифицирует эти новые изображения

33
00:02:31,040 --> 00:02:37,000
К счастью для нас и что делает этот такой распространенный пример, чтобы начать с того, что хорошие люди, стоящие за базой MNIST,

34
00:02:37,000 --> 00:02:44,289
собрал коллекцию из десятков тысяч рукописных цифровых изображений, каждый из которых помечен цифрами, которые они должны были и

35
00:02:44,720 --> 00:02:49,539
Это провокационно, так как описывать машину как обучение, когда вы действительно видите, как это работает

36
00:02:49,540 --> 00:02:55,359
Он чувствует себя намного меньше, чем какая-то сумасшедшая научно-фантастическая предпосылка, а также гораздо больше,

37
00:02:55,390 --> 00:02:59,589
Я имею в виду, что в основном это сводится к поиску минимума определенной функции

38
00:03:01,519 --> 00:03:05,199
Помните концептуально, что мы думаем о каждом нейроне как о соединении

39
00:03:05,390 --> 00:03:12,309
для всех нейронов в предыдущем слое, а веса в взвешенной сумме, определяющие ее активацию, выглядят как

40
00:03:12,440 --> 00:03:14,060
сильные стороны этих связей

41
00:03:14,060 --> 00:03:20,440
И предвзятость - это некоторый признак того, что этот нейрон имеет тенденцию быть активным или неактивным и начинать

42
00:03:20,440 --> 00:03:26,919
Мы просто собираемся инициализировать все эти веса и предубеждения, совершенно бесполезно сказать, что эта сеть будет выполнять

43
00:03:26,919 --> 00:03:33,759
довольно ужасно на данном примере обучения, поскольку он просто делает что-то случайное, например, вы кормите этим изображением 3 и

44
00:03:33,760 --> 00:03:35,799
Выходной уровень - это просто беспорядок

45
00:03:36,349 --> 00:03:42,518
Итак, что вы делаете, вы определяете функцию стоимости как способ сообщить компьютеру: «Нет плохого компьютера!

46
00:03:42,739 --> 00:03:50,529
Этот вывод должен иметь активацию, которая равна нулю для большинства нейронов, но для этого нейрона то, что вы дали мне, - это полный мусор "

47
00:03:51,260 --> 00:03:56,530
Сказать, что немного математически то, что вы делаете, это добавить квадраты различий между

48
00:03:56,720 --> 00:04:01,419
каждой из этих активированных выходов мусора и значения, которое вы хотите, чтобы они имели и

49
00:04:01,489 --> 00:04:04,599
Это то, что мы будем называть стоимостью одного примера обучения

50
00:04:05,599 --> 00:04:10,749
Обратите внимание: эта сумма невелика, когда сеть уверенно классифицирует изображение правильно

51
00:04:12,199 --> 00:04:15,639
Но это большое, когда сеть кажется, что она не знает, что она делает

52
00:04:18,330 --> 00:04:25,249
Итак, что вы делаете, считайте среднюю стоимость по всем десяткам тысяч примеров обучения в вашем распоряжении

53
00:04:27,060 --> 00:04:34,310
Эта средняя стоимость - это наша мера за то, насколько паршивая сеть и насколько плохо компьютер должен чувствовать, и это сложная вещь

54
00:04:34,830 --> 00:04:38,960
Помните, как сама сеть была в основном функцией, которая

55
00:04:39,540 --> 00:04:45,890
784 числа вводит значения пикселей и выплескивает десять чисел в качестве вывода и в некотором смысле

56
00:04:45,890 --> 00:04:48,770
Он параметризуется всеми этими весами и смещениями

57
00:04:49,140 --> 00:04:54,020
В то время как функция стоимости представляет собой слой сложности, поверх которого он принимает в качестве своего входного

58
00:04:54,450 --> 00:05:02,059
те тринадцать тысяч или около того весов и предубеждений, и он выплескивает один номер, описывающий, насколько плохи эти веса и предубеждения и

59
00:05:02,340 --> 00:05:08,749
Способ его определения зависит от поведения сети по всем десяткам тысяч учебных данных

60
00:05:09,150 --> 00:05:11,150
Об этом много думать

61
00:05:12,000 --> 00:05:15,619
Но просто рассказывая компьютеру, какая дрянная работа, это не очень полезно

62
00:05:15,900 --> 00:05:19,819
Вы хотите сказать, как изменить эти веса и предубеждения, чтобы он стал лучше?

63
00:05:20,820 --> 00:05:25,129
Сделать это проще, чем пытаться представить себе функцию с 13 000 входов

64
00:05:25,130 --> 00:05:30,409
Представьте себе простую функцию, которая имеет один номер в качестве входного и одно число в качестве вывода

65
00:05:30,960 --> 00:05:34,999
Как вы находите вход, который минимизирует значение этой функции?

66
00:05:36,270 --> 00:05:40,039
Студенты исчисления будут знать, что иногда вы можете понять, что минимум явно

67
00:05:40,260 --> 00:05:43,879
Но это не всегда возможно для действительно сложных функций

68
00:05:44,310 --> 00:05:52,160
Конечно, не в тринадцати тысячной входной версии этой ситуации для нашей сумасшедшей сложной функции стоимости нейронной сети

69
00:05:52,350 --> 00:05:59,029
Более гибкая тактика заключается в том, чтобы начать с любого старого ввода и выяснить, в каком направлении вы должны шагнуть, чтобы сделать этот вывод ниже

70
00:06:00,120 --> 00:06:03,710
В частности, если вы можете определить наклон функции, в которой вы находитесь

71
00:06:04,020 --> 00:06:09,619
Затем сдвиньтесь влево, если этот наклон положителен и сдвиньте вход вправо, если этот наклон отрицательный

72
00:06:12,130 --> 00:06:16,799
Если вы делаете это повторно в каждой точке, проверяя новый наклон и делая соответствующий шаг

73
00:06:16,800 --> 00:06:20,039
вы подходите к локальному минимуму функции и

74
00:06:20,280 --> 00:06:24,080
изображение, которое вы, возможно, имеете в виду, это мяч, спускающийся с холма и

75
00:06:24,400 --> 00:06:30,900
Обратите внимание даже на эту очень упрощенную единую функцию ввода, есть много возможных долин, которые вы можете приземлиться

76
00:06:31,540 --> 00:06:36,220
В зависимости от того, какой случайный ввод вы начинаете, и нет гарантии, что местный минимум

77
00:06:36,580 --> 00:06:39,040
Вы приземляетесь, это будет наименьшее возможное значение функции стоимости

78
00:06:39,610 --> 00:06:44,009
Это также будет перенесено на наш случай с нейронной сетью, и я также хочу, чтобы вы заметили

79
00:06:44,010 --> 00:06:47,190
Как сделать размеры шага пропорциональными наклону

80
00:06:47,620 --> 00:06:54,540
Затем, когда наклон сглаживается к минимуму, ваши шаги становятся все меньше и меньше, и этот вид помогает вам избежать перерегулирования

81
00:06:55,720 --> 00:07:00,449
Вместо того, чтобы усложнять сложность, вместо этого вы можете использовать функцию с двумя входами и одним выходом

82
00:07:01,120 --> 00:07:07,739
Вы можете представить входное пространство как плоскость XY, а функцию стоимости, как графику, как поверхность над ней

83
00:07:08,230 --> 00:07:15,060
Теперь вместо того, чтобы спрашивать о наклоне функции, вы должны спросить, в каком направлении вы должны входить в это пространство ввода?

84
00:07:15,310 --> 00:07:22,440
Чтобы быстрее уменьшить выход функции, другими словами. Каково направление спуска?

85
00:07:22,440 --> 00:07:25,379
И снова полезно подумать о том, как мяч катится вниз по холму

86
00:07:26,260 --> 00:07:34,080
Те из вас, кто знаком с многовариантным исчислением, будут знать, что градиент функции дает вам направление самого крутого подъема

87
00:07:34,750 --> 00:07:38,459
В основном, в каком направлении вы должны быстро активировать функцию

88
00:07:39,100 --> 00:07:46,439
естественно, принимая отрицательное значение этого градиента, вы получаете направление на шаг, который быстрее уменьшает функцию и

89
00:07:47,020 --> 00:07:53,400
Даже более того, длина этого вектора градиента на самом деле является показателем того, насколько крутым является самый крутой наклон

90
00:07:54,130 --> 00:07:56,280
Теперь, если вы не знакомы с многовариантным исчислением

91
00:07:56,280 --> 00:08:00,239
И вы хотите узнать больше о том, какую часть работы я сделал для Академии Хан по этой теме

92
00:08:00,910 --> 00:08:03,779
Честно говоря, хотя все, что имеет значение для вас и меня прямо сейчас

93
00:08:03,780 --> 00:08:09,419
Это в принципе существует способ вычислить этот вектор. Этот вектор, который сообщает вам, что

94
00:08:09,520 --> 00:08:15,900
Наклон вниз, и как круто это будет, вы будете в порядке, если это все, что вы знаете, и вы не прочны на деталях

95
00:08:16,790 --> 00:08:24,580
потому что, если вы можете получить, что алгоритм минимизации функции состоит в том, чтобы вычислить это направление градиента, тогда сделайте небольшой шаг вниз и

96
00:08:24,740 --> 00:08:26,740
Просто повторите это снова и снова

97
00:08:27,800 --> 00:08:34,600
Это та же основная идея для функции, которая имеет 13 000 входов вместо двух входов, представляющих собой организацию всего

98
00:08:35,330 --> 00:08:39,400
13 000 весов и смещения нашей сети в гигантский вектор столбца

99
00:08:39,680 --> 00:08:43,870
Отрицательным градиентом функции стоимости является только вектор

100
00:08:43,880 --> 00:08:49,299
Это какое-то направление в этом безумно огромном пространстве ввода, которое говорит вам, какие

101
00:08:49,400 --> 00:08:55,030
подталкивание ко всем этим числам приведет к самому быстрому снижению функции затрат и

102
00:08:55,460 --> 00:08:58,150
конечно, с нашей специально разработанной функцией стоимости

103
00:08:58,580 --> 00:09:04,900
Изменение весов и смещений для его уменьшения означает создание выходных данных сети на каждой части данных обучения

104
00:09:05,180 --> 00:09:10,599
Похоже, что это случайный массив из десяти значений и больше похоже на фактическое решение, которое мы хотим сделать

105
00:09:11,030 --> 00:09:16,030
Важно помнить, что эта функция стоимости включает в себя среднее значение по всем данным обучения

106
00:09:16,370 --> 00:09:20,590
Поэтому, если вы минимизируете это, это означает, что это лучшая производительность для всех этих образцов

107
00:09:23,780 --> 00:09:30,849
Алгоритм для эффективного вычисления этого градиента, который эффективно является сердцем того, как учится нейронная сеть, называется обратным распространением

108
00:09:31,190 --> 00:09:34,690
И это то, о чем я буду говорить о следующем видео

109
00:09:34,690 --> 00:09:36,690
Там я действительно хочу потратить время, чтобы пройти

110
00:09:36,830 --> 00:09:41,439
Что именно происходит с каждым весом и каждым смещением для данной части данных обучения?

111
00:09:41,810 --> 00:09:46,960
Пытаясь дать интуитивное чувство того, что происходит за кучей соответствующих исчислений и формул

112
00:09:47,510 --> 00:09:52,179
Прямо здесь прямо сейчас главное. Я хочу, чтобы вы знали независимо от деталей реализации

113
00:09:52,180 --> 00:09:58,479
это то, что мы имеем в виду, когда говорим о сетевом обучении, состоит в том, что это просто минимизирует функцию затрат и

114
00:09:58,940 --> 00:10:04,479
Обратите внимание, что одним из следствий этого является то, что для этой функции затрат важно иметь приятный плавный выход

115
00:10:04,480 --> 00:10:07,810
Чтобы мы могли найти локальный минимум, выполняя небольшие шаги вниз

116
00:10:08,810 --> 00:10:10,520
Вот почему, кстати

117
00:10:10,520 --> 00:10:16,749
Искусственные нейроны имеют непрерывную активацию, а не просто активную или неактивную в двоичном режиме

118
00:10:16,750 --> 00:10:18,750
если способ биологических нейронов

119
00:10:19,940 --> 00:10:26,770
Этот процесс многократного подталкивания ввода функции некоторым кратным отрицательного градиента называется градиентным спусканием

120
00:10:26,930 --> 00:10:32,380
Это способ сходиться к некоторому локальному минимуму функции стоимости, в основном долины на этом графике

121
00:10:32,930 --> 00:10:38,890
Я все еще показываю изображение функции с двумя входами, конечно, потому что подталкивает тринадцать тысяч входных данных

122
00:10:38,890 --> 00:10:44,049
Пространство немного сложно окутать вокруг, но на самом деле есть хороший не пространственный способ подумать об этом

123
00:10:44,630 --> 00:10:51,340
Каждая компонента отрицательного градиента говорит нам две вещи: знак, конечно, говорит нам, соответствует ли соответствующая

124
00:10:51,830 --> 00:10:59,139
Компонент входного вектора должен подталкиваться вверх или вниз, но важно относительные величины всех этих компонентов

125
00:10:59,840 --> 00:11:02,530
Какие из вас рассказывают, какие изменения важны

126
00:11:05,150 --> 00:11:09,340
Вы видите, что в нашей сети настройка на один из весов может быть намного больше

127
00:11:09,710 --> 00:11:12,939
влияние на функцию стоимости, чем корректировка на какой-либо другой вес

128
00:11:14,450 --> 00:11:17,950
Некоторые из этих соединений имеют большее значение для наших данных обучения

129
00:11:18,920 --> 00:11:22,690
Таким образом, вы можете думать об этом градиентном векторе нашего разума

130
00:11:22,690 --> 00:11:27,999
массивная функция затрат заключается в том, что она кодирует относительную важность каждого веса и смещения

131
00:11:28,250 --> 00:11:32,200
То есть, какие из этих изменений будут нести наибольший удар для вашего доллара

132
00:11:33,560 --> 00:11:36,460
Это действительно еще один способ задуматься о направлении

133
00:11:36,860 --> 00:11:41,290
Чтобы взять более простой пример, если у вас есть функция с двумя переменными в качестве ввода, и вы

134
00:11:41,690 --> 00:11:46,540
Вычислите, что его градиент в какой-то конкретной точке выражается как (3,1)

135
00:11:47,420 --> 00:11:51,670
Затем, с одной стороны, вы можете интерпретировать это, говоря, что, когда вы стоите на этом входе

136
00:11:52,070 --> 00:11:55,150
движение по этому направлению быстрее увеличивает функцию

137
00:11:55,460 --> 00:12:02,229
То, что, когда вы рисуете функцию над плоскостью входных точек, этот вектор является тем, что дает вам прямое направление в гору

138
00:12:02,600 --> 00:12:06,580
Но еще один способ прочитать это - сказать, что изменения этой первой переменной

139
00:12:06,740 --> 00:12:13,390
В три раза важны изменения во второй переменной, по крайней мере, в окрестности соответствующего ввода

140
00:12:13,520 --> 00:12:16,689
Подталкивание значения x несет намного больший удар для вашего доллара

141
00:12:19,310 --> 00:12:19,930
Отлично

142
00:12:19,930 --> 00:12:24,940
Давайте уменьшим масштаб и подведем итог, где мы находимся до сих пор, самой сетью является эта функция с

143
00:12:25,400 --> 00:12:29,859
784 входа и 10 выходов, определенных в терминах всех этих взвешенных сумм

144
00:12:30,350 --> 00:12:34,780
функция стоимости представляет собой слой сложности, поверх которого он принимает

145
00:12:35,120 --> 00:12:41,870
13 000 весов и предубеждений в качестве исходных данных и выплескивают одну меру паршивости на основе примеров обучения и

146
00:12:42,180 --> 00:12:47,930
Градиент функции стоимости - еще один уровень сложности, который все еще говорит нам

147
00:12:47,930 --> 00:12:53,839
То, что подталкивает все эти веса и предубеждения, вызывают самое быстрое изменение стоимости функции затрат

148
00:12:53,970 --> 00:12:57,680
Который вы могли бы интерпретировать, говорит, какие изменения, вес которых имеет наибольшее значение

149
00:13:02,550 --> 00:13:09,289
Поэтому, когда вы инициализируете сеть со случайными весами и смещениями и настраиваете их много раз на основе этого процесса спуска градиента

150
00:13:09,420 --> 00:13:12,949
Насколько хорошо он работает на изображениях, которых он никогда не видел?

151
00:13:13,680 --> 00:13:19,609
Ну, тот, который я описал здесь, с двумя скрытыми слоями из шестнадцати нейронов, каждый из которых выбран в основном по эстетическим соображениям

152
00:13:20,579 --> 00:13:26,089
ну, неплохо, что он классифицирует около 96 процентов новых изображений, которые он видит правильно, и

153
00:13:26,759 --> 00:13:32,239
Честно говоря, если вы посмотрите на некоторые примеры, из-за которых вы чувствуете себя вынужденным немного порезать

154
00:13:35,759 --> 00:13:39,079
Теперь, если вы играете со скрытой структурой слоя и делаете пару настроек

155
00:13:39,079 --> 00:13:43,698
Вы можете получить это до 98%, и это очень хорошо. Это не лучший

156
00:13:43,740 --> 00:13:48,409
Вы, безусловно, можете получить лучшую производительность, получив более сложную, чем эта простая ванильная сеть

157
00:13:48,569 --> 00:13:52,669
Но учитывая, насколько сложной является первоначальная задача, я просто думаю, что есть что-то?

158
00:13:52,889 --> 00:13:56,929
Невероятно, что любая сеть делает это хорошо на изображениях, которые никогда не видели раньше

159
00:13:57,389 --> 00:14:00,919
Учитывая, что мы никогда конкретно не говорили, какие шаблоны искать

160
00:14:02,579 --> 00:14:07,068
Первоначально я мотивировал эту структуру, описывая надежду на то, что у нас могут быть

161
00:14:07,259 --> 00:14:09,739
То, что второй слой может зацепиться за небольшие края

162
00:14:09,809 --> 00:14:17,089
Чтобы третий слой скомпоновал эти края для распознавания петель и более длинных строк и чтобы их можно было собрать вместе, чтобы распознавать цифры

163
00:14:17,699 --> 00:14:22,729
Так это то, что наша сеть на самом деле делает? Хорошо для этого, по крайней мере,

164
00:14:23,339 --> 00:14:24,449
Не за что

165
00:14:24,449 --> 00:14:27,409
помните, как в последнем видео мы смотрели, как весы

166
00:14:27,480 --> 00:14:31,849
Соединения от всех нейронов в первом слое к данному нейрону во втором слое

167
00:14:31,980 --> 00:14:36,829
Может быть визуализирован как заданный шаблон пикселя, который нейрон второго слоя набирает

168
00:14:37,350 --> 00:14:43,309
Хорошо, когда мы на самом деле делаем это для весов, связанных с этими переходами от первого уровня к следующему

169
00:14:43,709 --> 00:14:50,209
Вместо того, чтобы собираться на изолированных маленьких краях здесь и там. Они хорошо выглядят почти случайными

170
00:14:50,370 --> 00:14:56,399
Просто положите некоторые очень свободные шаблоны в середине, казалось бы, что в необъяснимо больших

171
00:14:56,920 --> 00:15:02,580
13 000-мерное пространство возможных весов и уклонов, наша сеть оказалась очень маленьким локальным минимумом,

172
00:15:02,860 --> 00:15:08,940
несмотря на успешную классификацию большинства изображений, точно не подбирают шаблоны, на которые мы могли надеяться, и

173
00:15:09,430 --> 00:15:13,709
Чтобы действительно управлять этой точкой дома, смотрите, что происходит, когда вы вводите случайное изображение

174
00:15:14,019 --> 00:15:21,449
если бы система была умной, вы могли бы ожидать, что она либо почувствует неопределенность, может быть, не активирует ни один из этих 10 выходных нейронов или

175
00:15:21,579 --> 00:15:23,200
Активация их всех равномерно

176
00:15:23,200 --> 00:15:24,820
Но вместо этого

177
00:15:24,820 --> 00:15:32,010
Уверенно дает вам какой-то бессмысленный ответ, как будто он чувствует себя уверенным, что этот случайный шум равен 5, так как он действительно

178
00:15:32,010 --> 00:15:34,010
образ 5 - это 5

179
00:15:34,180 --> 00:15:40,829
фразу по-разному, даже если эта сеть может распознавать цифры довольно хорошо, она не знает, как их привлечь

180
00:15:41,500 --> 00:15:45,149
Это связано с тем, что это такая строго ограниченная тренировочная установка

181
00:15:45,149 --> 00:15:51,479
Я имею в виду поставить себя в туфли сети здесь, с его точки зрения, вся вселенная состоит из ничего

182
00:15:51,480 --> 00:15:57,539
Но четко определенные неподвижные цифры, сосредоточенные в крошечной сетке, и ее функция стоимости просто никогда не давали ей

183
00:15:57,700 --> 00:16:00,959
Стимулирование быть чем угодно, но совершенно уверенно в своих решениях

184
00:16:01,690 --> 00:16:05,070
Итак, если это изображение того, что действительно делают эти нейроны второго слоя

185
00:16:05,140 --> 00:16:09,839
Вы можете задаться вопросом, почему я бы представил эту сеть с мотивацией собирать по краям и узорам

186
00:16:09,839 --> 00:16:11,969
Я имею в виду, это совсем не то, что заканчивается

187
00:16:13,029 --> 00:16:17,909
Ну, это не значит быть нашей конечной целью, но вместо этого отправной точкой откровенно

188
00:16:17,910 --> 00:16:19,120
Это старая технология

189
00:16:19,120 --> 00:16:21,510
вид, исследованный в 80-х и 90-х годах и

190
00:16:21,640 --> 00:16:29,129
Вам нужно понять это, прежде чем вы сможете понять более подробные современные варианты, и это, безусловно, способно решить некоторые интересные проблемы

191
00:16:29,410 --> 00:16:34,110
Но чем больше вы вникаете в то, что эти скрытые слои действительно делают менее умным,

192
00:16:38,530 --> 00:16:42,359
Переход на какой-то момент с того, как сети узнают, как вы учитесь

193
00:16:42,580 --> 00:16:46,139
Это произойдет только в том случае, если вы будете активно участвовать в этом материале здесь

194
00:16:46,660 --> 00:16:53,100
Одна довольно простая вещь, которую я хочу, чтобы вы сделали, это просто приостановить прямо сейчас и задуматься на мгновение о том, что

195
00:16:53,440 --> 00:16:55,230
Изменения, которые вы могли бы внести в эту систему

196
00:16:55,230 --> 00:17:00,719
И как он воспринимает образы, если вы хотите, чтобы он лучше воспринимал такие вещи, как ребра и узоры?

197
00:17:01,360 --> 00:17:04,410
Но лучше, чем фактически заниматься материалом

198
00:17:04,410 --> 00:17:05,079
я

199
00:17:05,079 --> 00:17:08,969
Очень рекомендую книгу Майкла Нильсена по глубокому обучению и нейронным сетям

200
00:17:09,190 --> 00:17:14,369
В нем вы можете найти код и данные для загрузки и воспроизведения для этого точного примера

201
00:17:14,410 --> 00:17:18,089
И книга проведет вас шаг за шагом, что делает этот код

202
00:17:18,910 --> 00:17:21,749
Что удивительно в том, что эта книга является бесплатной и общедоступной

203
00:17:22,360 --> 00:17:27,540
Поэтому, если вы что-то извлечете из этого, подумайте о том, чтобы присоединиться ко мне, чтобы пожертвовать на усилия Нильсена

204
00:17:27,910 --> 00:17:32,219
Я также связал пару других ресурсов, которые мне очень нравятся в описании, включая

205
00:17:32,470 --> 00:17:36,390
феноменальный и красивый пост в блоге Криса Олы и статьи в дистилляции

206
00:17:38,230 --> 00:17:40,200
Чтобы закрыть все здесь в течение последних нескольких минут

207
00:17:40,200 --> 00:17:43,740
Я хочу вернуться к фрагменту интервью, которое у меня было с Лейшей Ли

208
00:17:43,930 --> 00:17:49,079
Вы можете вспомнить ее из последнего видео. Она занималась своей докторской работой в глубоком обучении и в этом маленьком фрагменте

209
00:17:49,080 --> 00:17:55,530
Она рассказывает о двух последних документах, которые действительно копаются в том, как некоторые из более современных сетей распознавания образов фактически учатся

210
00:17:55,810 --> 00:18:01,349
Просто чтобы настроить, где мы были в разговоре, первая работа взяла одну из этих особенно глубоких нейронных сетей

211
00:18:01,350 --> 00:18:05,910
Это действительно хорошо для распознавания образов и вместо того, чтобы обучать его соответствующим образом помеченным данным

212
00:18:05,910 --> 00:18:08,579
Установите его, чтобы перетасовать все ярлыки перед тренировкой

213
00:18:08,800 --> 00:18:14,669
Очевидно, что точность тестирования здесь не лучше, чем случайная, поскольку все просто случайно помечено

214
00:18:14,800 --> 00:18:20,879
Но он все еще мог достичь такой же точности обучения, как и для правильно маркированного набора данных

215
00:18:21,490 --> 00:18:27,540
В основном миллионы весов для этой конкретной сети были достаточными для того, чтобы просто запомнить случайные данные

216
00:18:27,820 --> 00:18:34,379
Какой вопрос поднимает вопрос о том, действительно ли минимизация этой функции стоимости соответствует какой-либо структуре изображения?

217
00:18:34,380 --> 00:18:36,380
Или это просто ты знаешь?

218
00:18:36,520 --> 00:18:37,420
запомнить все

219
00:18:37,420 --> 00:18:43,859
Набор данных о том, какова правильная классификация, и поэтому пара из вас знает полгода спустя в ICML в этом году

220
00:18:44,470 --> 00:18:49,039
Не было точно отформатированной бумажной бумаги, которая обращалась к некоторым из них:

221
00:18:49,470 --> 00:18:55,279
Фактически, эти сети делают что-то немного умнее, если вы посмотрите на эту кривую точности

222
00:18:55,279 --> 00:18:57,499
если вы просто тренировались на

223
00:18:58,259 --> 00:19:05,179
Случайные данные показывают, что тип кривой спустился очень точно, вы знаете очень медленно почти линейным способом

224
00:19:05,179 --> 00:19:09,589
Таким образом, вы действительно пытаетесь найти, что местные минимумы возможных

225
00:19:09,590 --> 00:19:15,289
вы знаете правильные веса, которые доставят вам такую ​​точность, тогда как если вы действительно тренируетесь по структурированному набору данных, который имеет

226
00:19:15,289 --> 00:19:21,439
Правые этикетки. Вы знаете, что вы немного заиграли в начале, но потом вы очень быстро упали, чтобы добраться до этого

227
00:19:22,200 --> 00:19:26,149
Уровень точности и, следовательно, в каком-то смысле было легче найти, что

228
00:19:26,759 --> 00:19:33,949
Местные максимумы, и поэтому было интересно также, что это поймано, приносит в свет другую бумагу от фактически пару лет назад

229
00:19:34,080 --> 00:19:36,080
У чего намного больше

230
00:19:36,990 --> 00:19:39,169
упрощения сетевых уровней

231
00:19:39,169 --> 00:19:46,788
Но один из результатов заключался в том, что, если вы посмотрите на оптимизационный ландшафт, то локальные минимумы, которые эти сети имеют тенденцию изучать,

232
00:19:47,340 --> 00:19:54,079
Фактически с равным качеством, поэтому в некотором смысле, если ваш набор данных является структурой, и вы должны быть в состоянии найти это намного проще

233
00:19:58,139 --> 00:20:01,189
Моя благодарность, как всегда, тем, кто поддерживает вас на patreon

234
00:20:01,190 --> 00:20:06,950
Я уже говорил, что это только что созданный игровой чейнджер, но эти видео действительно не будут возможны без тебя.

235
00:20:07,230 --> 00:20:12,889
Также хочу дать специальный. Благодаря партнерам VC фирмы amplifi в поддержке этих первых видеороликов в серии

