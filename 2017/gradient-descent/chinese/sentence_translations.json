[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "上一个视频我展示了神经网络的结构。",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "我将在这里快速回顾一下，以便我们记住它，然后我对这个视频有两个主要目标。",
  "from_community_srt": "在上一節影片裡我講解了神經網路的結構 首先我們來快速回顧一下 在本節影片裡，",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "首先是介绍梯度下降的概念，它不仅是神经网络学习方式的基础，也是许多其他机器学习工作方式的基础。",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "然后，我们将进一步深入研究这个特定网络的执行方式，以及神经元的隐藏层最终要寻找什么。",
  "from_community_srt": "我們有兩個目標 首介紹梯度下降的概念 它不僅是神經網路工作的基礎 也是很多其他機器學習方法的基礎 然後我們會研究一下這個特別的網路是如何工作的 以及這些隱藏的神經元層究竟在尋找什麽 作為覆習 這裡我們引用一個經典的例子——手寫數字識別",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "提醒一下，我们的目标是手写数字识别的经典示例，即神经网络的“你好世界”。",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "这些数字在 28x28 像素网格上呈现，每个像素都有 0 到 1 之间的某个灰度值。",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "这些决定了网络输入层 784 个神经元的激活。",
  "from_community_srt": "神經網路領域的\"Hello World\" 這些數字書寫在28乘28像素的網格上 每個網格對應一個0到1之間的灰度值 這些灰度值 決定了神經網路輸入層的784個神經元的激活 隨後每一層的各個神經元的激活值 都基於前一層的加權和",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "然后，后续层中每个神经元的激活基于前一层中所有激活的加权和，加上一些称为偏差的特殊数字。",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "然后你用一些其他函数来组合这个总和，比如 sigmoid 压缩或 relu，就像我在上一个视频中演示的那样。",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "总的来说，考虑到两个隐藏层（每个隐藏层有 16 个神经元）的任意选择，网络有大约 13,000 个我们可以调整的权重和偏差，正是这些值决定了网络到底做什么。",
  "from_community_srt": "與一個被叫做偏差的常數 相加 來獲得 然後你把它和一些其他函數相加 比如sigmoid 或者我上節影片提到的ReLu 總之我們隨意給出兩個具有16個神經元的層 每一個神經網路有 13000個可以調整的權重值和偏差 正是這些值決定了這個神經網路如何工作",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "那么当我们说这个网络对给定数字进行分类时，我们的意思是最后一层中 10 个神经元中最亮的神经元对应于该数字。",
  "from_community_srt": "那麽“這個網路可以將給定數字分類”是什麽意思呢 即最後一層10個數字中被點亮的那個數字就是輸入的數字 請記住，",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "请记住，我们在这里想到分层结构的动机是，也许第二层可以拾取边缘，第三层可能拾取诸如循环和线条之类的图案，最后一层可以将这些拼凑在一起识别数字的模式。",
  "from_community_srt": "我們使用這個分層的結構， 目的是 或許， 第二層可以辨別出數字中的特徵線段 第三層或許可以辨別出組成數字的圈和線 而最後一層可以把所有特徵結合在一起 從而辨別出這個輸入的數字 因此，",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "所以在这里，我们学习网络是如何学习的。",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "我们想要的是一种算法，您可以向该网络显示一大堆训练数据，这些数据以一堆不同的手写数字图像的形式出现，以及它们应该是什么的标签，它会调整这 13,000 个权重和偏差，以提高其在训练数据上的性能。",
  "from_community_srt": "在這裡 我們將學習神經網路是如何學習的 我們想要的是一種可以向這個神經網路展示大量訓練數據的算法 這裡所說的大量訓練數據是指 很多手寫數字的圖像以及 標明了這個圖像上的數字到底是幾的標籤",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "希望这种分层结构意味着它所学到的东西可以推广到训练数据之外的图像。",
  "from_community_srt": "它能夠通過這些訓練數據 來調整13000個權重值和偏差以達到 改善神經網路表現的目的 我們所期望的是 這個分層的結構可以學習 超出訓練數據範圍的圖像的識別 我們測試的方法是 當你完成對這個網路的訓練後",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "我们测试的方式是，在训练网络后，向其显示更多以前从未见过的标记数据，然后您会看到它对这些新图像进行分类的准确性。",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "对我们来说幸运的是，MNIST 数据库背后的优秀人员已经收集了数以万计的手写数字图像，每张都标有它们应该标记的数字，这使得这个例子成为一个常见的例子。是。",
  "from_community_srt": "當你向它展示它從未見過的圖像時 觀察它判斷的精確度 幸運的是 通常我們可以用來自MNIST base的數據來開始訓練 MNIST base的好人們收集了數以萬計帶有標籤的手寫數字圖像",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "尽管将机器描述为学习是一种挑衅性的说法，但一旦你看到它是如何工作的，你就会感觉它不再像一些疯狂的科幻小说前提，而更像是一次微积分练习。",
  "from_community_srt": "當你一但真正了解它的工作原理， 你會發現向機器解釋學習的過程非常有挑戰的一件事 它並不像一些瘋狂的科幻 反倒是更像微積分練習 也就是說 基本上是找到某一個特定函數的最小值 請記住，",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "我的意思是，基本上归结为找到某个函数的最小值。",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "请记住，从概念上讲，我们认为每个神经元都连接到前一层中的所有神经元，定义其激活的加权和中的权重有点像这些连接的强度，而偏差是该神经元是否倾向于活跃或不活跃。",
  "from_community_srt": "從概念上講， 我們認為每一個神經元都與前一層的所有神經元相連 加權求和計算中的加權值 在定義中像是一種 神經元間連接強度的參考值 而偏差值則代表了某個神經元是傾向於激活 還是不激活並關閉 如果我們將所有的權重值和偏差值初始化為隨機數 毫無疑問，",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "首先，我们将完全随机地初始化所有这些权重和偏差。",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "不用说，这个网络在给定的训练示例上的表现将非常糟糕，因为它只是做一些随机的事情。",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "例如，您输入 3 的图像，输出层看起来一团糟。",
  "from_community_srt": "這個神經網路會表現地一塌糊塗 用一個例子來說明 當你輸入一個3的圖像 輸出層看起來一片混亂 所以，",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "所以你要做的就是定义一个成本函数，一种告诉计算机的方式，不，糟糕的计算机，输出应该具有对于大多数神经元来说是 0 的激活，但是对于这个神经元来说是 1，你给我的完全是垃圾。",
  "from_community_srt": "你要做的是， 定義一個成本函數 來告訴電腦， 不！ 你是錯的！ 正確的輸出應該是， 多數神經元激活值為0 但是對於這個神經元來說，",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "从数学角度来说，您可以将每个垃圾输出激活之间的差异的平方与您希望它们具有的值相加，这就是我们所说的单个训练示例的成本。",
  "from_community_srt": "你給我的是垃圾 用數學語言來描述， 就是你需要把每個【垃圾輸出】與【你想要的正確輸出】的【差的平方】相加 這就是在單個訓練例子中的成本 注意，",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "请注意，当网络自信地正确分类图像时，这个总和很小，但当网络似乎不知道自己在做什么时，这个总和很大。",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "因此，您要做的就是考虑您可以使用的所有数以万计的训练示例的平均成本。",
  "from_community_srt": "如果網路能很正確地辨別出圖像 這個和會非常小 但如果這個值很大 說明這個神經網路根本不知道它在幹嘛 所以你要做的就是 考慮在你所能處理的上萬個訓練案例中的平均成本 這個平均成本就是我們對該神經網路 表現好壞的衡量值",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "这个平均成本是我们衡量网络有多糟糕以及计算机应该感觉有多糟糕的标准。",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "这是一件复杂的事情。",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "还记得网络本身基本上是一个函数吗？它接受 784 个数字作为输入、像素值，并输出 10 个数字作为输出，从某种意义上说，它是由所有这些权重和偏差参数化的？",
  "from_community_srt": "記住這個神經網路本質上是一個函數 它將784個像素值數字作為輸入 10個數字作為輸出 從某種意義上來說 是通過這些權重和偏差來參數化 然而成本函數的覆雜性表現在 最重要的是它將一萬三千左右的權重和偏差值作為輸入",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "成本函数是其之上的一层复杂性。",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "它将大约 13,000 个权重和偏差作为输入，并输出一个数字来描述这些权重和偏差的严重程度，其定义方式取决于网络在数以万计的训练数据上的行为。",
  "from_community_srt": "並輸出一個數字來反應這些權重和偏差質量的好壞 它的定義， 由神經網路經過上萬次訓練後的表現來決定 這裡面有很多要思考的 不過直接告訴電腦，",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "有很多值得思考的地方。",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "但仅仅告诉计算机它正在做一件多么糟糕的工作并没有多大帮助。",
  "from_community_srt": "它的工作有多爛 它一點幫助也沒有 你想要知道的是，",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "你想告诉它如何改变这些权重和偏差，以便它变得更好。",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "为了让它变得更容易，不要费力想象一个具有 13,000 个输入的函数，只需想象一个简单的函数，其中一个数字作为输入，一个数字作为输出。",
  "from_community_srt": "如何可以調整這些權重和偏差 從而讓它表現的好一點 我們用一個簡單的例子來說明 （而不是費力思考一個有著13000個輸入輸出的函數） 我們想象這樣一個簡單的函數 它只有一個輸入和一個輸出",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "如何找到使该函数的值最小化的输入？",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "学微积分的学生会知道，有时您可以明确地算出最小值，但这对于真正复杂的函数并不总是可行，对于我们疯狂复杂的神经网络成本函数来说，在这种情况的 13,000 个输入版本中当然不行。",
  "from_community_srt": "如何找到一個輸入值使函數值最小 學過微積分的學生知道 有時你可以非常容易地指出一個函數的最小值 但對於一些非常覆雜的函數來說， 就不一定可行了 當然包括我們那個超級複雜的有著13000個自變量的成本函數 一個更靈活的辦法是，",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "一种更灵活的策略是从任何输入开始，然后找出应该采取哪个方向来降低输出。",
  "from_community_srt": "從任意一個輸入量開始， 找出讓函數值變小的方向 尤其是，",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "具体来说，如果您可以计算出所在函数的斜率，则如果斜率为正，则将输入移至左侧；如果斜率为负，则将输入移至右侧。",
  "from_community_srt": "如果你知道函數在某一點的斜率 那麽， 當斜率為正時， 向左；當斜率為負時，",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "如果您重复执行此操作，在每个点检查新的斜率并采取适当的步骤，您将接近函数的某些局部最小值。",
  "from_community_srt": "向右 就可以找到函數輸出變小的方向 如果你用合適的步驟不斷地重覆檢查每一點的斜率 你就可以找到函式的局部最小值 你可以在大腦裡想象這樣一幅圖 一個球向山下滾落 值得注意的是，",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "您可能想到的图像是一个从山上滚下来的球。",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "请注意，即使对于这个真正简化的单输入函数，您也可能会遇到许多可能的山谷，具体取决于您从哪个随机输入开始，并且不能保证您遇到的局部最小值将是最小的可能值的成本函数。",
  "from_community_srt": "即使在這樣一個簡單的單一輸入函式中， 依然有可能出現很多可以滾入的山谷 從你隨機選取的輸入值開始， 找到的局部最小值 根本不能保證，",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "这也将延续到我们的神经网络案例中。",
  "from_community_srt": "它就是整個函數的最小值 對於我們的神經網路的函式來說，",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "我还希望您注意，如果您使步长与斜率成正比，那么当斜率趋于最小值时，您的步数会变得越来越小，这有助于您避免过度调整。",
  "from_community_srt": "也是一樣的情況 另外需要注意的是， 如果你的步長和斜率成比例 那麽當越接近最小值時， 你的步長就越小，",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "稍微提高一下复杂性，想象一个具有两个输入和一个输出的函数。",
  "from_community_srt": "這會幫助你避免找過頭 擴展一下想象力， 如果一個函數有兩個自變量和一個因變量 你可以想象，",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "您可能会将输入空间视为 xy 平面，并将成本函数视为其上方的曲面。",
  "from_community_srt": "輸入自變量空間是一個XY平面 而成本函數則是飄浮在上面的一個曲面 現在，",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "您不必询问函数的斜率，而必须询问在该输入空间中应该朝哪个方向迈进，以便最快地减少函数的输出。",
  "from_community_srt": "需要考慮的不是函數的斜率 而是在輸入空間的尋找前進方向 換句話說，",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "换句话说，下坡的方向是什么？",
  "from_community_srt": "就是讓函數輸出減小得最快 下山的方向是什麽？",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "再次，想象一个球从山上滚下来是有帮助的。",
  "from_community_srt": "同樣的，",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "熟悉多变量微积分的人都会知道，函数的梯度为您提供了最陡上升的方向，您应该朝哪个方向迈进以最快地增加函数。",
  "from_community_srt": "我們想像一個球向山下滾落 熟悉多變量微積分的人會知道函數的梯度會給你最陡峭的上升方向 也就等同於哪個方向是函數增加最快的方向 很自然的，",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "很自然，取该梯度的负值可以为您提供以最快的速度减小函数的方向。",
  "from_community_srt": "用負梯度就可以找到函數下降最快的方向 而且，",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "更重要的是，该梯度向量的长度指示了最陡坡度的陡度。",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "如果您不熟悉多变量微积分并想了解更多信息，请查看我为可汗学院所做的有关该主题的一些工作。",
  "from_community_srt": "這個梯度向量的長度實際上是這個最陡斜坡有多陡的指標 如果你並不熟悉多變量微積分 並且想學習更多關於這方面的內容 你可以看一下可汗學院關於這一章節的內容 事實上，",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "但老实说，现在对你我来说最重要的是，原则上存在一种计算这个向量的方法，这个向量告诉你下坡方向是什么以及它有多陡。",
  "from_community_srt": "對於我們來說， 最重要的是 原則上這個向量是可以計算出來的 它會告訴你下山的方向以及會有多陡 知道這些知識就夠了，",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "如果您只知道这些并且对细节不太了解，那也没关系。",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "如果你能得到这个，最小化函数的算法就是计算这个梯度方向，然后向下走一小步，然后一遍又一遍地重复这个过程。",
  "from_community_srt": "具體的細節並不重要 因為如果你知道可以通過計算梯度方向來找到函數值變小的方向並向山下走出第一步 那麽你就可以重覆這個過程 這個原理在擁有13000個自變量的函式中同樣適用 想像一下，",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "对于具有 13,000 个输入而不是 2 个输入的函数，其基本思想相同。",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "想象一下将我们网络的所有 13,000 个权重和偏差组织成一个巨大的列向量。",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "成本函数的负梯度只是一个向量，它是这个极其巨大的输入空间内的某个方向，它告诉您对所有这些数字的哪些推动将导致成本函数最快速的下降。",
  "from_community_srt": "把有13000個權重和偏差的神經網路放入一個超大的向量中 成本函數的負梯度只是一個簡單的向量 它是一個超級大的輸入變量空間中的一個方向 告訴你哪個方向會讓成本函數最快地變小 當然，",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "当然，通过我们专门设计的成本函数，改变权重和偏差来减少它意味着使网络在每条训练数据上的输出看起来不像一个由 10 个值组成的随机数组，而更像是我们想要的实际决策它使.",
  "from_community_srt": "對於我們專門設計的成本函數而言 改變權重和偏差意味著 讓神經網路對每一組訓練數據的輸出 看起來不像是十個數字中隨機的一個 而是實際上我們想讓它輸出的那一個 要知道，",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "重要的是要记住，这个成本函数涉及所有训练数据的平均值，因此如果将其最小化，则意味着它在所有这些样本上都有更好的性能。",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "有效计算梯度的算法被称为反向传播，它实际上是神经网络学习的核心，这也是我将在下一个视频中讨论的内容。",
  "from_community_srt": "這個成本函式是每一組訓練數據效果的平均 所以如果你減小這個函數值 意味著改善了所以樣本的表現 讓這個梯度計算更有效率的算法是神經網路學習的核心 它叫做傳播 這是我下個影片重點要講的 其中我非常想花時間講一講",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "在那里，我真的想花时间来了解给定训练数据的每个权重和偏差到底发生了什么，试图对除了一堆相关微积分和公式之外发生的事情给出直观的感受。",
  "from_community_srt": "對於一組特定的訓練數據 每一個權重和偏差到底發生了什麽 試圖給出除了相關微積分和公式以外的直觀地感受 而現在，",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "就在这里，现在，我想让你知道的最重要的事情，与实现细节无关，是当我们谈论网络学习时，我们的意思是它只是最小化成本函数。",
  "from_community_srt": "我想讓你知道的實現細節是 當我們說“神經網路學習就是減小成本函式”到底是什麽意思 注意它的結果，",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "请注意，这样做的一个结果是，对于该成本函数来说，具有良好的平滑输出非常重要，这样我们就可以通过向下走一小步来找到局部最小值。",
  "from_community_srt": "就是讓成本函式有一個平滑的輸出是非常重要的 所以我們可以通過小的步長來找出局部最小值 順便說一下，",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "顺便说一句，这就是为什么人工神经元具有连续范围的激活，而不是像生物神经元那样简单地以二元方式激活或不激活。",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "这种通过负梯度的倍数反复推动函数输入的过程称为梯度下降。",
  "from_community_srt": "這就是為什麽 人工神經元擁有連續激活行為 而不是如同自然神經元那樣的 簡單的激活或不激活這樣的二元狀態 這個反覆地將一個函數的輸入按照負梯度的倍數來輸入的過程被稱為梯度下降 它是讓成本函數向局部最小值收斂的方法 也就是圖中沿山谷下降的過程",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "这是一种收敛到成本函数的局部最小值的方法，基本上是该图中的一个山谷。",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "当然，我仍然展示具有两个输入的函数的图片，因为 13,000 维输入空间中的微移有点难以理解，但有一种很好的非空间方式来思考这个问题。",
  "from_community_srt": "這裡我依然用二維函數是因為， 如果用13000維函數，",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "负梯度的每个分量告诉我们两件事。",
  "from_community_srt": "對於我們的大腦來說是很難想象的 但是事實上還有一個非圖形的方法來思考這個問題 梯度中的每個部分告訴我們兩個東西 梯度的符號當然是告訴我們輸入向量的對應部分是向上還是向下，",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "当然，符号告诉我们输入向量的相应分量是否应该向上或向下微移。",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "但重要的是，所有这些组成部分的相对大小可以告诉您哪些变化更重要。",
  "from_community_srt": "但重要的是， 所有這些部分相關的幅度大小 會告訴你哪些改變更重要 你會發現，",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "您会看到，在我们的网络中，对其中一个权重的调整可能比对其他权重的调整对成本函数产生更大的影响。",
  "from_community_srt": "在神經網路中，",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "其中一些联系对于我们的训练数据来说更重要。",
  "from_community_srt": "一些權重值的改變對於成本函數來說影響很大 而另一些權重值的改變對成本函數的影響則很小 某些關係只與我們的訓練數據有關 所以，",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "因此，您可以考虑我们的令人难以置信的巨大成本函数的梯度向量，它编码了每个权重和偏差的相对重要性，也就是说，这些变化中的哪一个将为您带来最大的收益。",
  "from_community_srt": "你可以認為這個巨大的成本函數的梯度向量 編碼每個權重和偏差的相對重要性 那就是這些變化中的哪一個，",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "这实际上只是思考方向的另一种方式。",
  "from_community_srt": "將會為你帶來最大的影響 這的確是思考方向問題的另一個途徑 舉個簡單的例子，",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "举一个更简单的例子，如果你有一个带有两个变量作为输入的函数，并且你计算出它在某个特定点的梯度为 3,1，那么一方面你可以将其解释为当你'站在该输入处，沿着这个方向移动会最快地增加函数，当您在输入点平面上方绘制函数时，该向量就是给您直线上坡方向的方向。",
  "from_community_srt": "有一個二維函數 你來計算它在(3,1)點的梯度 那麽， 一方面你可以把這個過程翻譯為 當你站在這一點時 你可以沿梯度方向最快地增加 在函數的圖像中， 這個向量就是最快地直接上山的方向 但另一方面，",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "但另一种解读方式是，对第一个变量的更改的重要性是对第二个变量的更改的 3 倍，至少在相关输入的附近，轻推 x 值会给您带来更大的影响。巴克。",
  "from_community_srt": "你也可以說 第一個變量的變化對函數的影響三倍於第二個變量 至少在輸入量的鄰域內是這樣的 改變x的值的影響大的多 現在我們來總結一下，",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "让我们缩小范围并总结一下目前为止的情况。",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "网络本身就是一个具有 784 个输入和 10 个输出的函数，根据所有这些加权和进行定义。",
  "from_community_srt": "神經網路本身一是個具有784個輸入量和10個輸出量的函數 定義為所有權重的和的形式 成本函數反應複雜性 它有13000個權重與偏差作為輸入，",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "成本函数是其之上的一层复杂性。",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "它以 13,000 个权重和偏差作为输入，并根据训练示例输出单一的糟糕程度度量。",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "而成本函数的梯度又增加了一层复杂性。",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "它告诉我们对所有这些权重和偏差的推动会导致成本函数值发生最快的变化，您可以将其解释为哪些权重的变化最重要。",
  "from_community_srt": "輸出一個基於訓練案例的好壞程度的值 成本函數的梯度還有一層覆雜性 它告訴我們如何改變所以這些權重值和偏差值可以讓成本函數最快的變小 也可以翻譯為，",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "那么，当您使用随机权重和偏差初始化网络，并根据梯度下降过程多次调整它们时，它在以前从未见过的图像上实际表现如何？",
  "from_community_srt": "哪些改變權重更大 所以當你用隨機值來初始化權重和偏差值， 並且基於梯度下降過程多次調整它們 它會對一張從來沒有見過的圖像表現如何？",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "我在这里描述的那个有两个隐藏层，每个隐藏层有 16 个神经元，主要是出于美观原因而选择的，这还不错，它对它看到的大约 96% 的新图像进行了正确分类。",
  "from_community_srt": "我在這裡描述的是16個神經元的兩個隱藏層， 至於每一層為什麽是16個...只是這個數字看著順眼罷了 對於新圖像， 它有96%的正確識別率，",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "老实说，如果你看一下它搞砸的一些例子，你就会觉得有必要稍微放松一下。",
  "from_community_srt": "這已經很不錯了 老實說，",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "现在，如果您尝试一下隐藏层结构并进行一些调整，您可以将其提高到 98%。",
  "from_community_srt": "如果你看一些它搞砸的例子 你會覺得真的是讓人無奈 如果你用隱藏的層結構並做一些調整 你可以獲得98%的正確率 相對棒！",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "这非常好！",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "这不是最好的，你当然可以通过比这个普通网络更复杂来获得更好的性能，但考虑到最初的任务是多么艰巨，我认为任何网络在以前从未见过的图像上做得这么好都是令人难以置信的，因为我们从未具体告诉它要寻找什么模式。",
  "from_community_srt": "你當然可以用更覆雜的網路來獲得比現在這個網路更好的表現 考慮到一最初的任務有多艱巨， 我真的覺得 神經網路對於那些之前沒有見過的圖像 的表現好到讓人驚訝 假如我們從來沒有專門告訴它我們是在尋找什麽樣式 對於這個結構的目標，",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "最初，我激发这种结构的方式是通过描述我们可能拥有的希望，即第二层可能会拾取小边缘，第三层会将这些边缘拼凑在一起以识别循环和较长的线，并且这些可能会被拼凑起来一起识别数字。",
  "from_community_srt": "最初我們只是給出了一個期望 第二層可能取出了小的片段組分 第三層可能是將片段識別成圈和長點的線段， 然後通過這些元素最終識別出了數字 那麽，",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "那么这就是我们的网络实际上正在做的事情吗？",
  "from_community_srt": "我們的網路真的是這樣工作的嗎？",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "好吧，至少对于这一点来说，根本不是。",
  "from_community_srt": "至少對於當前這個例子 根本不是！",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "还记得我们在上一个视频中如何将第一层中的所有神经元到第二层中的给定神经元的连接权重可视化为第二层神经元正在拾取的给定像素模式吗？",
  "from_community_srt": "請記住在上一節影片中我們是如何看待：如何連接從第一層中的所有神經元 到第二層中的給定神經元的的權重 可以被可視化為該第二層神經元正在拾取的給定像素模式 當我們真的計算與從前一層向後一層轉換相關的權重值",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "好吧，当我们实际上对与这些过渡相关的权重执行此操作时，从第一层到下一层，而不是在这里或那里拾取孤立的小边缘，它们看起来几乎是随机的，只是有一些非常松散的模式中间那里。",
  "from_community_srt": "而不是選取相互獨立的這兒一塊那兒一塊的小片段時 它看起來是完全隨機的 只是一些非常鬆散的樣式 對於大的不可思議的13000維空間的可能的權重和偏差 我們的網路發現自己是一個完美的局部最小值",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "看起来，在可能的权重和偏差的深不可测的 13,000 维空间中，我们的网络发现自己有一个令人愉快的局部最小值，尽管成功地对大多数图像进行了分类，但并没有完全拾取我们可能希望的模式。",
  "from_community_srt": "盡管可以將大多數圖像正確識別 但是並不是我們所希望的樣式 當你真正運行它，",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "为了真正理解这一点，请观察输入随机图像时会发生什么。",
  "from_community_srt": "觀察如果你輸入一個隨機圖像，",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "如果系统很聪明，你可能会认为它会感到不确定，也许并没有真正激活这 10 个输出神经元中的任何一个或均匀地激活它们，但它却自信地给你一些无意义的答案，就好像它感觉确定这个随机噪声是 5，就像 5 的实际图像是 5 一样。",
  "from_community_srt": "它會作何反應 如果系統足夠聰明， 它會發現結果是不確定的， 可能不會激活10個輸出神經元中的任何一個 也可能會平均地激活它們 然而 它很自信地給你一個毫無意義的答案， 仿佛它很肯定這個隨機信號就是5 就好像它真的識別了一張寫著5的圖像一樣 換句話說，",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "换句话说，即使这个网络可以很好地识别数字，它也不知道如何绘制它们。",
  "from_community_srt": "不管它識別數字的正確率有多高，",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "这很大程度上是因为它的训练设置受到严格限制。",
  "from_community_srt": "它還是不會寫出數字 很大程度上是因為這是一個限制非常嚴格的訓練設置 我的意思是說，",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "我的意思是，请站在网络的立场上思考。",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "从它的角度来看，整个宇宙只是由以微小网格为中心的明确定义的不动数字组成，而它的成本函数从来没有给它任何激励，除了对自己的决定完全有信心。",
  "from_community_srt": "如果你站在神經網路的角度， 你會發現整個宇宙只有 小網格中心不變的數字及其成本函數 並且完全有信心做出自己的判斷 所以，",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "因此，以此作为第二层神经元真正在做什么的图像，您可能想知道为什么我会出于拾取边缘和模式的动机而引入这个网络。",
  "from_community_srt": "如果這個圖像就是第二層真的在做的事情 你會很好奇， 為什麽我會介紹神經網路可能提取一些片段和形狀 也就是說，",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "我的意思是，这根本不是它最终要做的事情。",
  "from_community_srt": "根本不是它最終要做的事情 是的，",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "嗯，这并不是我们的最终目标，而是一个起点。",
  "from_community_srt": "這不意味著它是我們的最終目標，",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "坦率地说，这是旧技术，是 80 年代和 90 年代研究的那种技术，你确实需要先了解它，然后才能了解更详细的现代变体，而且它显然能够解决一些有趣的问题，但你越深入了解什么那些隐藏层确实在做事，但看起来却不太智能。",
  "from_community_srt": "而是起點 坦率的講， 這是一個老的技術了 是80年代和90年代研究的東西 但在你理解當代的一些變體之前， 你確實有必要先理解它 很顯然， 它可以解決一些有趣的問題 但是你越深挖隱蔽層到底在幹什麽，",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "将焦点暂时从网络如何学习转移到你如何学习，只有当你以某种方式积极参与这里的材料时才会发生这种情况。",
  "from_community_srt": "它的智能程度看起來就越低 暫時轉移開關於神經網路如何學習與你是如何學習這一焦點 只有當你非常積極地處理相關材料時才會發生 建議你暫時停下來，",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "我希望您做的一件非常简单的事情就是现在暂停并深入思考一下您可以对该系统进行哪些更改以及如果您希望它更好地识别边缘和图案等内容，它会如何感知图像。",
  "from_community_srt": "深入思考一下 你可能對這個系統做出怎樣的改變 如果你想讓它更好的提取諸如線段、形狀之類的元素，",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "但更好的是，为了真正理解这些材料，我强烈推荐迈克尔·尼尔森（Michael Nielsen）撰写的关于深度学习和神经网络的书。",
  "from_community_srt": "應該讓它怎麽感知圖像 但真正更好的處理這樣材料的辦法是 我 強烈建議你去讀Michael Nielsen關於深度學習和神經網路的書 在這本書裡面，",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "在其中，您可以找到要下载和使用该示例的代码和数据，并且本书将逐步引导您完成该代码的作用。",
  "from_community_srt": "你可以找到相關代碼和數據， 下載並運行相關實例 這本書會一步一步地給你介紹程式碼的含意 最爽的是，",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "很棒的是，这本书是免费且公开的，所以如果您确实从中有所收获，请考虑与我一起为尼尔森的努力捐款。",
  "from_community_srt": "這本書是免費並且公開發行的 所以如果你真的從中得到了一些東西，",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "我还在描述中链接了一些我非常喜欢的其他资源，包括 Chris Ola 的精彩博客文章和 Distill 中的文章。",
  "from_community_srt": "可以考慮和我一起為Nielsen的努力捐款 我也提供了一些我非常喜歡的資源連結 包括Chris Ola的讓人震撼又非常漂亮的部落格和提交的文章 最後幾分鐘時間 我回到我和Leisha Lee的一段採訪",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "为了结束最后几分钟的讨论，我想跳回到我与 Leisha Lee 的采访片段。",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "您可能还记得上一个视频中的她，她在深度学习方面取得了博士学位。",
  "from_community_srt": "你可能還記得在上節影片中的她 她在進行關於深度學習的博士研究工作 在那個採訪片段中，",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "在这个小片段中，她谈到了最近的两篇论文，这些论文真正深入探讨了一些更现代的图像识别网络实际上是如何学习的。",
  "from_community_srt": "她談論了最近的兩篇論文， 其中深入研究了當前圖像識別領域神經網路到底是如何工作這一問題 第一篇論文就確定了我們的討論主題，",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "为了确定我们在对话中的位置，第一篇论文采用了一个非常擅长图像识别的特别深层的神经网络，并且不是在正确标记的数据集上对其进行训练，而是在训练之前对所有标签进行了洗牌。",
  "from_community_srt": "它介紹了最為深入的神經網路之一 它可以非常準確地識別圖像 但並非用了正確標識過的數據集 而是用打亂了所有標籤的訓練數據 很顯然，",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "显然，这里的测试准确性并不比随机测试更好，因为所有内容都是随机标记的，但它仍然能够达到与在正确标记的数据集上相同的训练准确性。",
  "from_community_srt": "測試準確度不會比隨機結果好到哪去， 因為標籤本身就是混亂的 但是一但你使用了正確標記的數據集， 依然可以達到相同的識別精度 基本上，",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "基本上，这个特定网络的数百万个权重足以让它记住随机数据，这就提出了一个问题：最小化这个成本函数是否实际上对应于图像中的任何类型的结构，或者只是记忆？",
  "from_community_srt": "這個特別的神經網路中數以百萬計的權重值足以記住那些隨機數據 最小化這個成本函數是否真的對應圖像中任意類型的結構 這一問題是怎樣提出來的？ 或許， 只是， 你知道的...",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "如果你看一下准确率曲线，如果你只是在随机数据集上进行训练，那么该曲线几乎以线性方式缓慢下降，所以你真的很难找到可能的局部最小值，你知道，正确的权重可以让您获得准确度。",
  "from_community_srt": "記憶整個正確分類的數據集 今年在ICML 沒有反駁的論文， 只有一些簡單提及的論文 事實上這些神經網路做的更聰明一些 如果你看這個準確度曲線 如果你只是訓練一個隨機數據集 這個曲線會非常非常慢的下降， 近似於線性 所以你可能真的很難找到局部最小值 只要你用正確標記過的結構化的數據集 正確的權重會讓你得到一定的準確度 一開始你可能會反覆折騰，",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "然而，如果您实际上是在结构化数据集（具有正确标签的数据集）上进行训练，那么一开始您会稍微调整一下，但随后您会很快下降到达到该准确度水平，因此从某种意义上来说更容易找到局部最大值。",
  "from_community_srt": "但是很快就會快速下降到這個準確程度 所以， 一定程度上來說，",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "因此，有趣的是，它揭示了几年前的另一篇论文，该论文对网络层进行了更多简化，但其中一个结果是说，如果你看看优化情况，这些网络倾向于学习的局部最小值实际上具有相同的质量，因此从某种意义上来说，如果您的数据集是结构化的，您应该能够更容易地找到它。",
  "from_community_srt": "還是很容易找到局部最大值 幾年前另外一篇論文也引起了人們的興趣 它大大地簡化了 神經網路層 其中一個結論講的是， 為何如果你觀察優化的情景， 神經網路傾向學習的局部最小值， 事實上效果是相同的 所以從某種程度來講， 如果你的數據集是結構化的，",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "我一如既往地感谢那些支持 Patreon 的人。",
  "from_community_srt": "你應該會發現找到它是很容易的 我一如既往地感謝那些支持Patreon的人 我之前已經說過Patreon是一個怎樣的遊戲規則改變者，",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "我之前已经说过 Patreon 是一个游戏规则的改变者，但如果没有您，这些视频真的不可能实现。",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "我还要特别感谢风险投资公司 Amplify Partners 对本系列初始视频的支持。",
  "from_community_srt": "但是如果沒有你是不可能的做出這些影片的 同時也要特別感謝VC公司的合作夥伴對這些系列影片的支持",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]