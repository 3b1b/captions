1
00:00:04,180 --> 00:00:07,280
上一个视频我展示了神经网络的结构。

2
00:00:07,680 --> 00:00:10,413
我将在这里快速回顾一下，以便我们记住它，

3
00:00:10,413 --> 00:00:12,600
然后我对这个视频有两个主要目标。

4
00:00:13,100 --> 00:00:17,727
首先是介绍梯度下降的概念，它不仅是神经网络学习方式的基础，

5
00:00:17,727 --> 00:00:20,600
也是许多其他机器学习工作方式的基础。

6
00:00:21,120 --> 00:00:25,179
然后，我们将进一步深入研究这个特定网络的执行方式，

7
00:00:25,179 --> 00:00:27,940
以及神经元的隐藏层最终要寻找什么。

8
00:00:28,979 --> 00:00:33,605
提醒一下，我们的目标是手写数字识别的经典示例，

9
00:00:33,605 --> 00:00:36,220
即神经网络的“你好世界”。

10
00:00:37,020 --> 00:00:40,067
这些数字在 28x28 像素网格上呈现，

11
00:00:40,067 --> 00:00:43,420
每个像素都有 0 到 1 之间的某个灰度值。

12
00:00:43,820 --> 00:00:50,040
这些决定了网络输入层 784 个神经元的激活。

13
00:00:51,180 --> 00:00:57,752
然后，后续层中每个神经元的激活基于前一层中所有激活的加权和，

14
00:00:57,752 --> 00:01:00,820
加上一些称为偏差的特殊数字。

15
00:01:02,160 --> 00:01:04,420
然后你用一些其他函数来组合这个总和，

16
00:01:04,420 --> 00:01:06,931
比如 sigmoid 压缩或 relu，

17
00:01:06,931 --> 00:01:08,940
就像我在上一个视频中演示的那样。

18
00:01:09,480 --> 00:01:14,064
总的来说，考虑到两个隐藏层（每个隐藏层有 16 

19
00:01:14,064 --> 00:01:18,649
个神经元）的任意选择，网络有大约 13,000 

20
00:01:18,649 --> 00:01:24,380
个我们可以调整的权重和偏差，正是这些值决定了网络到底做什么。

21
00:01:24,880 --> 00:01:28,154
那么当我们说这个网络对给定数字进行分类时，

22
00:01:28,154 --> 00:01:32,364
我们的意思是最后一层中 10 个神经元中最亮的神经元对

23
00:01:32,364 --> 00:01:33,300
应于该数字。

24
00:01:34,100 --> 00:01:38,072
请记住，我们在这里想到分层结构的动机是，

25
00:01:38,072 --> 00:01:42,840
也许第二层可以拾取边缘，第三层可能拾取诸如循环和

26
00:01:42,840 --> 00:01:48,800
线条之类的图案，最后一层可以将这些拼凑在一起识别数字的模式。

27
00:01:49,800 --> 00:01:52,240
所以在这里，我们学习网络是如何学习的。

28
00:01:52,640 --> 00:01:57,685
我们想要的是一种算法，您可以向该网络显示一大堆训练数据，

29
00:01:57,685 --> 00:02:01,650
这些数据以一堆不同的手写数字图像的形式出现，

30
00:02:01,650 --> 00:02:06,335
以及它们应该是什么的标签，它会调整这 13,000 

31
00:02:06,335 --> 00:02:10,120
个权重和偏差，以提高其在训练数据上的性能。

32
00:02:10,720 --> 00:02:13,696
希望这种分层结构意味着它所学到的

33
00:02:13,696 --> 00:02:16,860
东西可以推广到训练数据之外的图像。

34
00:02:17,640 --> 00:02:20,228
我们测试的方式是，在训练网络后，

35
00:02:20,228 --> 00:02:23,140
向其显示更多以前从未见过的标记数据，

36
00:02:23,140 --> 00:02:26,700
然后您会看到它对这些新图像进行分类的准确性。

37
00:02:31,120 --> 00:02:33,837
对我们来说幸运的是，MNIST 

38
00:02:33,837 --> 00:02:38,424
数据库背后的优秀人员已经收集了数以万计的手写数字图像，

39
00:02:38,424 --> 00:02:42,671
每张都标有它们应该标记的数字，这使得这个例子成为一

40
00:02:42,671 --> 00:02:44,200
个常见的例子。是。

41
00:02:44,900 --> 00:02:48,058
尽管将机器描述为学习是一种挑衅性的说法，

42
00:02:48,058 --> 00:02:51,532
但一旦你看到它是如何工作的，你就会感觉它不再

43
00:02:51,532 --> 00:02:55,480
像一些疯狂的科幻小说前提，而更像是一次微积分练习。

44
00:02:56,200 --> 00:02:59,960
我的意思是，基本上归结为找到某个函数的最小值。

45
00:03:01,939 --> 00:03:07,613
请记住，从概念上讲，我们认为每个神经元都连接到前一层

46
00:03:07,613 --> 00:03:13,286
中的所有神经元，定义其激活的加权和中的权重有点像这些

47
00:03:13,286 --> 00:03:18,960
连接的强度，而偏差是该神经元是否倾向于活跃或不活跃。

48
00:03:19,720 --> 00:03:24,400
首先，我们将完全随机地初始化所有这些权重和偏差。

49
00:03:24,940 --> 00:03:28,697
不用说，这个网络在给定的训练示例上的表现将非常糟糕，

50
00:03:28,697 --> 00:03:30,720
因为它只是做一些随机的事情。

51
00:03:31,040 --> 00:03:36,020
例如，您输入 3 的图像，输出层看起来一团糟。

52
00:03:36,600 --> 00:03:41,210
所以你要做的就是定义一个成本函数，一种告诉计算机的方式，

53
00:03:41,210 --> 00:03:45,655
不，糟糕的计算机，输出应该具有对于大多数神经元来说是 

54
00:03:45,655 --> 00:03:49,113
0 的激活，但是对于这个神经元来说是 1，

55
00:03:49,113 --> 00:03:50,760
你给我的完全是垃圾。

56
00:03:51,720 --> 00:03:58,257
从数学角度来说，您可以将每个垃圾输出激活之间的差异的平方与

57
00:03:58,257 --> 00:04:05,020
您希望它们具有的值相加，这就是我们所说的单个训练示例的成本。

58
00:04:05,960 --> 00:04:11,286
请注意，当网络自信地正确分类图像时，这个总和很小，

59
00:04:11,286 --> 00:04:16,399
但当网络似乎不知道自己在做什么时，这个总和很大。

60
00:04:18,640 --> 00:04:22,039
因此，您要做的就是考虑您可以使用的

61
00:04:22,039 --> 00:04:25,440
所有数以万计的训练示例的平均成本。

62
00:04:27,040 --> 00:04:29,890
这个平均成本是我们衡量网络有多糟糕

63
00:04:29,890 --> 00:04:32,740
以及计算机应该感觉有多糟糕的标准。

64
00:04:33,420 --> 00:04:34,600
这是一件复杂的事情。

65
00:04:35,040 --> 00:04:38,077
还记得网络本身基本上是一个函数吗？

66
00:04:38,077 --> 00:04:41,651
它接受 784 个数字作为输入、像素值，

67
00:04:41,651 --> 00:04:45,762
并输出 10 个数字作为输出，从某种意义上说，

68
00:04:45,762 --> 00:04:48,800
它是由所有这些权重和偏差参数化的？

69
00:04:49,500 --> 00:04:52,820
成本函数是其之上的一层复杂性。

70
00:04:53,100 --> 00:04:58,218
它将大约 13,000 个权重和偏差作为输入，

71
00:04:58,218 --> 00:05:03,336
并输出一个数字来描述这些权重和偏差的严重程度，

72
00:05:03,336 --> 00:05:08,900
其定义方式取决于网络在数以万计的训练数据上的行为。

73
00:05:09,520 --> 00:05:11,000
有很多值得思考的地方。

74
00:05:12,400 --> 00:05:15,820
但仅仅告诉计算机它正在做一件多么糟糕的工作并没有多大帮助。

75
00:05:16,220 --> 00:05:20,060
你想告诉它如何改变这些权重和偏差，以便它变得更好。

76
00:05:20,780 --> 00:05:23,820
为了让它变得更容易，不要费力想象一个具有 

77
00:05:23,820 --> 00:05:27,584
13,000 个输入的函数，只需想象一个简单的函数，

78
00:05:27,584 --> 00:05:30,480
其中一个数字作为输入，一个数字作为输出。

79
00:05:31,480 --> 00:05:35,300
如何找到使该函数的值最小化的输入？

80
00:05:36,460 --> 00:05:40,613
学微积分的学生会知道，有时您可以明确地算出最小值，

81
00:05:40,613 --> 00:05:43,603
但这对于真正复杂的函数并不总是可行，

82
00:05:43,603 --> 00:05:46,926
对于我们疯狂复杂的神经网络成本函数来说，

83
00:05:46,926 --> 00:05:51,080
在这种情况的 13,000 个输入版本中当然不行。

84
00:05:51,580 --> 00:05:55,281
一种更灵活的策略是从任何输入开始，

85
00:05:55,281 --> 00:05:59,200
然后找出应该采取哪个方向来降低输出。

86
00:06:00,080 --> 00:06:05,353
具体来说，如果您可以计算出所在函数的斜率，则如果斜率为正，

87
00:06:05,353 --> 00:06:09,900
则将输入移至左侧；如果斜率为负，则将输入移至右侧。

88
00:06:11,960 --> 00:06:17,213
如果您重复执行此操作，在每个点检查新的斜率并采取适当的步骤，

89
00:06:17,213 --> 00:06:19,840
您将接近函数的某些局部最小值。

90
00:06:20,640 --> 00:06:23,800
您可能想到的图像是一个从山上滚下来的球。

91
00:06:24,620 --> 00:06:28,451
请注意，即使对于这个真正简化的单输入函数，

92
00:06:28,451 --> 00:06:33,378
您也可能会遇到许多可能的山谷，具体取决于您从哪个随机输

93
00:06:33,378 --> 00:06:38,305
入开始，并且不能保证您遇到的局部最小值将是最小的可能值

94
00:06:38,305 --> 00:06:39,400
的成本函数。

95
00:06:40,220 --> 00:06:42,620
这也将延续到我们的神经网络案例中。

96
00:06:43,180 --> 00:06:47,387
我还希望您注意，如果您使步长与斜率成正比，

97
00:06:47,387 --> 00:06:52,195
那么当斜率趋于最小值时，您的步数会变得越来越小，

98
00:06:52,195 --> 00:06:54,600
这有助于您避免过度调整。

99
00:06:55,940 --> 00:07:00,980
稍微提高一下复杂性，想象一个具有两个输入和一个输出的函数。

100
00:07:01,500 --> 00:07:05,121
您可能会将输入空间视为 xy 平面，

101
00:07:05,121 --> 00:07:08,140
并将成本函数视为其上方的曲面。

102
00:07:08,760 --> 00:07:13,860
您不必询问函数的斜率，而必须询问在该输入空间中

103
00:07:13,860 --> 00:07:18,960
应该朝哪个方向迈进，以便最快地减少函数的输出。

104
00:07:19,720 --> 00:07:21,760
换句话说，下坡的方向是什么？

105
00:07:22,380 --> 00:07:25,560
再次，想象一个球从山上滚下来是有帮助的。

106
00:07:26,660 --> 00:07:32,720
熟悉多变量微积分的人都会知道，函数的梯度为您提供了最

107
00:07:32,720 --> 00:07:38,780
陡上升的方向，您应该朝哪个方向迈进以最快地增加函数。

108
00:07:39,560 --> 00:07:42,695
很自然，取该梯度的负值可以为您

109
00:07:42,695 --> 00:07:46,040
提供以最快的速度减小函数的方向。

110
00:07:47,240 --> 00:07:53,840
更重要的是，该梯度向量的长度指示了最陡坡度的陡度。

111
00:07:54,540 --> 00:07:57,308
如果您不熟悉多变量微积分并想了解更多信息，

112
00:07:57,308 --> 00:08:00,340
请查看我为可汗学院所做的有关该主题的一些工作。

113
00:08:00,860 --> 00:08:04,408
但老实说，现在对你我来说最重要的是，

114
00:08:04,408 --> 00:08:07,760
原则上存在一种计算这个向量的方法，

115
00:08:07,760 --> 00:08:11,900
这个向量告诉你下坡方向是什么以及它有多陡。

116
00:08:12,400 --> 00:08:16,120
如果您只知道这些并且对细节不太了解，那也没关系。

117
00:08:17,200 --> 00:08:22,336
如果你能得到这个，最小化函数的算法就是计算这个梯度方向，

118
00:08:22,336 --> 00:08:26,740
然后向下走一小步，然后一遍又一遍地重复这个过程。

119
00:08:27,700 --> 00:08:31,682
对于具有 13,000 个输入而不是 2 个输入的函数，

120
00:08:31,682 --> 00:08:32,820
其基本思想相同。

121
00:08:33,400 --> 00:08:36,589
想象一下将我们网络的所有 13,000 

122
00:08:36,589 --> 00:08:39,460
个权重和偏差组织成一个巨大的列向量。

123
00:08:40,140 --> 00:08:44,902
成本函数的负梯度只是一个向量，它是这个极其

124
00:08:44,902 --> 00:08:49,664
巨大的输入空间内的某个方向，它告诉您对所有

125
00:08:49,664 --> 00:08:54,880
这些数字的哪些推动将导致成本函数最快速的下降。

126
00:08:55,640 --> 00:08:58,749
当然，通过我们专门设计的成本函数，

127
00:08:58,749 --> 00:09:03,687
改变权重和偏差来减少它意味着使网络在每条训练数据上的输

128
00:09:03,687 --> 00:09:07,893
出看起来不像一个由 10 个值组成的随机数组，

129
00:09:07,893 --> 00:09:10,820
而更像是我们想要的实际决策它使.

130
00:09:11,440 --> 00:09:15,974
重要的是要记住，这个成本函数涉及所有训练数据的平均值，

131
00:09:15,974 --> 00:09:20,844
因此如果将其最小化，则意味着它在所有这些样本上都有更好的性

132
00:09:20,844 --> 00:09:21,180
能。

133
00:09:23,820 --> 00:09:27,274
有效计算梯度的算法被称为反向传播，

134
00:09:27,274 --> 00:09:32,354
它实际上是神经网络学习的核心，这也是我将在下一个视

135
00:09:32,354 --> 00:09:33,980
频中讨论的内容。

136
00:09:34,660 --> 00:09:38,806
在那里，我真的想花时间来了解给定训练数据的每

137
00:09:38,806 --> 00:09:42,953
个权重和偏差到底发生了什么，试图对除了一堆相

138
00:09:42,953 --> 00:09:47,100
关微积分和公式之外发生的事情给出直观的感受。

139
00:09:47,780 --> 00:09:53,159
就在这里，现在，我想让你知道的最重要的事情，与实现细节无关，

140
00:09:53,159 --> 00:09:58,360
是当我们谈论网络学习时，我们的意思是它只是最小化成本函数。

141
00:09:59,300 --> 00:10:02,706
请注意，这样做的一个结果是，对于该成本函数来说，

142
00:10:02,706 --> 00:10:05,545
具有良好的平滑输出非常重要，这样我们就可

143
00:10:05,545 --> 00:10:08,100
以通过向下走一小步来找到局部最小值。

144
00:10:09,260 --> 00:10:14,293
顺便说一句，这就是为什么人工神经元具有连续范围的激活，

145
00:10:14,293 --> 00:10:19,140
而不是像生物神经元那样简单地以二元方式激活或不激活。

146
00:10:20,220 --> 00:10:26,760
这种通过负梯度的倍数反复推动函数输入的过程称为梯度下降。

147
00:10:27,300 --> 00:10:30,561
这是一种收敛到成本函数的局部最小值的方法，

148
00:10:30,561 --> 00:10:32,580
基本上是该图中的一个山谷。

149
00:10:33,440 --> 00:10:36,831
当然，我仍然展示具有两个输入的函数的图片，

150
00:10:36,831 --> 00:10:41,030
因为 13,000 维输入空间中的微移有点难以理解，

151
00:10:41,030 --> 00:10:44,260
但有一种很好的非空间方式来思考这个问题。

152
00:10:45,080 --> 00:10:48,440
负梯度的每个分量告诉我们两件事。

153
00:10:49,060 --> 00:10:55,140
当然，符号告诉我们输入向量的相应分量是否应该向上或向下微移。

154
00:10:55,800 --> 00:10:59,260
但重要的是，所有这些组成部分的相

155
00:10:59,260 --> 00:11:02,720
对大小可以告诉您哪些变化更重要。

156
00:11:05,220 --> 00:11:09,046
您会看到，在我们的网络中，对其中一个权重的调整

157
00:11:09,046 --> 00:11:13,040
可能比对其他权重的调整对成本函数产生更大的影响。

158
00:11:14,800 --> 00:11:18,200
其中一些联系对于我们的训练数据来说更重要。

159
00:11:19,320 --> 00:11:24,695
因此，您可以考虑我们的令人难以置信的巨大成本函数的梯度向量，

160
00:11:24,695 --> 00:11:28,816
它编码了每个权重和偏差的相对重要性，也就是说，

161
00:11:28,816 --> 00:11:32,400
这些变化中的哪一个将为您带来最大的收益。

162
00:11:33,620 --> 00:11:36,640
这实际上只是思考方向的另一种方式。

163
00:11:37,100 --> 00:11:42,951
举一个更简单的例子，如果你有一个带有两个变量作为输入的函数，

164
00:11:42,951 --> 00:11:47,242
并且你计算出它在某个特定点的梯度为 3,1，

165
00:11:47,242 --> 00:11:52,508
那么一方面你可以将其解释为当你'站在该输入处，

166
00:11:52,508 --> 00:11:55,823
沿着这个方向移动会最快地增加函数，

167
00:11:55,823 --> 00:11:58,944
当您在输入点平面上方绘制函数时，

168
00:11:58,944 --> 00:12:02,260
该向量就是给您直线上坡方向的方向。

169
00:12:02,860 --> 00:12:07,473
但另一种解读方式是，对第一个变量的更改的重要性

170
00:12:07,473 --> 00:12:12,888
是对第二个变量的更改的 3 倍，至少在相关输入的附近，

171
00:12:12,888 --> 00:12:16,900
轻推 x 值会给您带来更大的影响。巴克。

172
00:12:19,880 --> 00:12:22,340
让我们缩小范围并总结一下目前为止的情况。

173
00:12:22,840 --> 00:12:27,749
网络本身就是一个具有 784 个输入和 10 个输出的函数，

174
00:12:27,749 --> 00:12:30,040
根据所有这些加权和进行定义。

175
00:12:30,640 --> 00:12:33,680
成本函数是其之上的一层复杂性。

176
00:12:33,980 --> 00:12:38,043
它以 13,000 个权重和偏差作为输入，

177
00:12:38,043 --> 00:12:41,720
并根据训练示例输出单一的糟糕程度度量。

178
00:12:42,440 --> 00:12:46,900
而成本函数的梯度又增加了一层复杂性。

179
00:12:47,360 --> 00:12:52,520
它告诉我们对所有这些权重和偏差的推动会导致成本函数值

180
00:12:52,520 --> 00:12:57,880
发生最快的变化，您可以将其解释为哪些权重的变化最重要。

181
00:13:02,560 --> 00:13:06,359
那么，当您使用随机权重和偏差初始化网络，

182
00:13:06,359 --> 00:13:09,589
并根据梯度下降过程多次调整它们时，

183
00:13:09,589 --> 00:13:13,200
它在以前从未见过的图像上实际表现如何？

184
00:13:14,100 --> 00:13:16,630
我在这里描述的那个有两个隐藏层，

185
00:13:16,630 --> 00:13:21,216
每个隐藏层有 16 个神经元，主要是出于美观原因而选择的，

186
00:13:21,216 --> 00:13:25,960
这还不错，它对它看到的大约 96% 的新图像进行了正确分类。

187
00:13:26,680 --> 00:13:29,954
老实说，如果你看一下它搞砸的一些例子，

188
00:13:29,954 --> 00:13:32,540
你就会觉得有必要稍微放松一下。

189
00:13:36,220 --> 00:13:39,759
现在，如果您尝试一下隐藏层结构并进行一些调整，

190
00:13:39,759 --> 00:13:41,760
您可以将其提高到 98%。

191
00:13:41,760 --> 00:13:42,720
这非常好！

192
00:13:43,020 --> 00:13:47,620
这不是最好的，你当然可以通过比这个普通网络更复杂来

193
00:13:47,620 --> 00:13:51,852
获得更好的性能，但考虑到最初的任务是多么艰巨，

194
00:13:51,852 --> 00:13:56,452
我认为任何网络在以前从未见过的图像上做得这么好都是

195
00:13:56,452 --> 00:14:01,420
令人难以置信的，因为我们从未具体告诉它要寻找什么模式。

196
00:14:02,560 --> 00:14:07,376
最初，我激发这种结构的方式是通过描述我们可能拥有的希望，

197
00:14:07,376 --> 00:14:12,192
即第二层可能会拾取小边缘，第三层会将这些边缘拼凑在一起以

198
00:14:12,192 --> 00:14:17,180
识别循环和较长的线，并且这些可能会被拼凑起来一起识别数字。

199
00:14:17,960 --> 00:14:20,400
那么这就是我们的网络实际上正在做的事情吗？

200
00:14:21,079 --> 00:14:24,400
好吧，至少对于这一点来说，根本不是。

201
00:14:24,820 --> 00:14:28,836
还记得我们在上一个视频中如何将第一层中的所

202
00:14:28,836 --> 00:14:32,852
有神经元到第二层中的给定神经元的连接权重可

203
00:14:32,852 --> 00:14:37,060
视化为第二层神经元正在拾取的给定像素模式吗？

204
00:14:37,780 --> 00:14:42,952
好吧，当我们实际上对与这些过渡相关的权重执行此操作时，

205
00:14:42,952 --> 00:14:48,124
从第一层到下一层，而不是在这里或那里拾取孤立的小边缘，

206
00:14:48,124 --> 00:14:53,680
它们看起来几乎是随机的，只是有一些非常松散的模式中间那里。

207
00:14:53,760 --> 00:14:58,319
看起来，在可能的权重和偏差的深不可测的 13,000 

208
00:14:58,319 --> 00:15:03,048
维空间中，我们的网络发现自己有一个令人愉快的局部最小值，

209
00:15:03,048 --> 00:15:05,920
尽管成功地对大多数图像进行了分类，

210
00:15:05,920 --> 00:15:08,960
但并没有完全拾取我们可能希望的模式。

211
00:15:09,780 --> 00:15:13,820
为了真正理解这一点，请观察输入随机图像时会发生什么。

212
00:15:14,320 --> 00:15:18,324
如果系统很聪明，你可能会认为它会感到不确定，

213
00:15:18,324 --> 00:15:23,238
也许并没有真正激活这 10 个输出神经元中的任何一个或

214
00:15:23,238 --> 00:15:27,789
均匀地激活它们，但它却自信地给你一些无意义的答案，

215
00:15:27,789 --> 00:15:31,065
就好像它感觉确定这个随机噪声是 5，

216
00:15:31,065 --> 00:15:34,160
就像 5 的实际图像是 5 一样。

217
00:15:34,540 --> 00:15:38,460
换句话说，即使这个网络可以很好地识别数字，

218
00:15:38,460 --> 00:15:40,700
它也不知道如何绘制它们。

219
00:15:41,420 --> 00:15:45,240
这很大程度上是因为它的训练设置受到严格限制。

220
00:15:45,880 --> 00:15:47,740
我的意思是，请站在网络的立场上思考。

221
00:15:48,140 --> 00:15:52,326
从它的角度来看，整个宇宙只是由以微小网格为中

222
00:15:52,326 --> 00:15:56,512
心的明确定义的不动数字组成，而它的成本函数从

223
00:15:56,512 --> 00:16:01,080
来没有给它任何激励，除了对自己的决定完全有信心。

224
00:16:02,120 --> 00:16:05,442
因此，以此作为第二层神经元真正在做什么的图像，

225
00:16:05,442 --> 00:16:09,342
您可能想知道为什么我会出于拾取边缘和模式的动机而引入这

226
00:16:09,342 --> 00:16:09,920
个网络。

227
00:16:09,920 --> 00:16:12,300
我的意思是，这根本不是它最终要做的事情。

228
00:16:13,380 --> 00:16:17,180
嗯，这并不是我们的最终目标，而是一个起点。

229
00:16:17,640 --> 00:16:21,385
坦率地说，这是旧技术，是 80 年代和 90 

230
00:16:21,385 --> 00:16:24,642
年代研究的那种技术，你确实需要先了解它，

231
00:16:24,642 --> 00:16:28,877
然后才能了解更详细的现代变体，而且它显然能够解决一些

232
00:16:28,877 --> 00:16:33,111
有趣的问题，但你越深入了解什么那些隐藏层确实在做事，

233
00:16:33,111 --> 00:16:34,740
但看起来却不太智能。

234
00:16:38,480 --> 00:16:41,831
将焦点暂时从网络如何学习转移到你如何学习，

235
00:16:41,831 --> 00:16:46,300
只有当你以某种方式积极参与这里的材料时才会发生这种情况。

236
00:16:47,060 --> 00:16:51,666
我希望您做的一件非常简单的事情就是现在暂停并深入

237
00:16:51,666 --> 00:16:56,273
思考一下您可以对该系统进行哪些更改以及如果您希望

238
00:16:56,273 --> 00:17:00,880
它更好地识别边缘和图案等内容，它会如何感知图像。

239
00:17:01,479 --> 00:17:03,536
但更好的是，为了真正理解这些材料，

240
00:17:03,536 --> 00:17:06,076
我强烈推荐迈克尔·尼尔森（Michael 

241
00:17:06,076 --> 00:17:09,099
Nielsen）撰写的关于深度学习和神经网络的书。

242
00:17:09,680 --> 00:17:14,611
在其中，您可以找到要下载和使用该示例的代码和数据，

243
00:17:14,611 --> 00:17:18,359
并且本书将逐步引导您完成该代码的作用。

244
00:17:19,300 --> 00:17:22,145
很棒的是，这本书是免费且公开的，

245
00:17:22,145 --> 00:17:26,237
所以如果您确实从中有所收获，请考虑与我一起为尼

246
00:17:26,237 --> 00:17:27,660
尔森的努力捐款。

247
00:17:27,660 --> 00:17:31,071
我还在描述中链接了一些我非常喜欢的其他资源，

248
00:17:31,071 --> 00:17:35,724
包括 Chris Ola 的精彩博客文章和 Distill 

249
00:17:35,724 --> 00:17:36,500
中的文章。

250
00:17:38,280 --> 00:17:41,374
为了结束最后几分钟的讨论，我想跳回到我与 

251
00:17:41,374 --> 00:17:43,880
Leisha Lee 的采访片段。

252
00:17:44,300 --> 00:17:45,954
您可能还记得上一个视频中的她，

253
00:17:45,954 --> 00:17:47,720
她在深度学习方面取得了博士学位。

254
00:17:48,300 --> 00:17:51,122
在这个小片段中，她谈到了最近的两篇论文，

255
00:17:51,122 --> 00:17:54,792
这些论文真正深入探讨了一些更现代的图像识别网络实际上

256
00:17:54,792 --> 00:17:55,780
是如何学习的。

257
00:17:56,120 --> 00:18:00,275
为了确定我们在对话中的位置，第一篇论文采用了一个非常擅

258
00:18:00,275 --> 00:18:02,737
长图像识别的特别深层的神经网络，

259
00:18:02,737 --> 00:18:05,969
并且不是在正确标记的数据集上对其进行训练，

260
00:18:05,969 --> 00:18:08,740
而是在训练之前对所有标签进行了洗牌。

261
00:18:09,480 --> 00:18:13,280
显然，这里的测试准确性并不比随机测试更好，

262
00:18:13,280 --> 00:18:17,080
因为所有内容都是随机标记的，但它仍然能够达

263
00:18:17,080 --> 00:18:20,880
到与在正确标记的数据集上相同的训练准确性。

264
00:18:21,600 --> 00:18:27,200
基本上，这个特定网络的数百万个权重足以让它记住随机数据，

265
00:18:27,200 --> 00:18:32,000
这就提出了一个问题：最小化这个成本函数是否实际上

266
00:18:32,000 --> 00:18:36,400
对应于图像中的任何类型的结构，或者只是记忆？

267
00:18:51,440 --> 00:18:58,672
如果你看一下准确率曲线，如果你只是在随机数据集上进行训练，

268
00:18:58,672 --> 00:19:02,912
那么该曲线几乎以线性方式缓慢下降，

269
00:19:02,912 --> 00:19:08,399
所以你真的很难找到可能的局部最小值，你知道，

270
00:19:08,399 --> 00:19:12,140
正确的权重可以让您获得准确度。

271
00:19:12,240 --> 00:19:17,446
然而，如果您实际上是在结构化数据集（具有正确标签的数据集）

272
00:19:17,446 --> 00:19:21,037
上进行训练，那么一开始您会稍微调整一下，

273
00:19:21,037 --> 00:19:24,449
但随后您会很快下降到达到该准确度水平，

274
00:19:24,449 --> 00:19:28,220
因此从某种意义上来说更容易找到局部最大值。

275
00:19:28,540 --> 00:19:33,266
因此，有趣的是，它揭示了几年前的另一篇论文，

276
00:19:33,266 --> 00:19:38,637
该论文对网络层进行了更多简化，但其中一个结果是说，

277
00:19:38,637 --> 00:19:45,082
如果你看看优化情况，这些网络倾向于学习的局部最小值实际上具有

278
00:19:45,082 --> 00:19:51,527
相同的质量，因此从某种意义上来说，如果您的数据集是结构化的，

279
00:19:51,527 --> 00:19:54,320
您应该能够更容易地找到它。

280
00:19:58,160 --> 00:20:01,180
我一如既往地感谢那些支持 Patreon 的人。

281
00:20:01,520 --> 00:20:04,665
我之前已经说过 Patreon 是一个游戏规则的改变者，

282
00:20:04,665 --> 00:20:06,800
但如果没有您，这些视频真的不可能实现。

283
00:20:07,460 --> 00:20:10,181
我还要特别感谢风险投资公司 Amplify 

284
00:20:10,181 --> 00:20:12,780
Partners 对本系列初始视频的支持。

