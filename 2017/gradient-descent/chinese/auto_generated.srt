1
00:00:00,000 --> 00:00:07,240
上一个视频我展示了神经网络的结构。

2
00:00:07,240 --> 00:00:10,280
我将在这里快速回顾一下，以便我们记住

3
00:00:10,280 --> 00:00:13,160
它，然后我对这个视频有两个主要目标。

4
00:00:13,160 --> 00:00:17,139
首先是介绍梯度下降的概念，它不仅是神经网络学习方

5
00:00:17,139 --> 00:00:20,800
式的基础，也是许多其他机器学习工作方式的基础。

6
00:00:20,800 --> 00:00:25,281
然后，我们将进一步深入研究这个特定网络的执

7
00:00:25,281 --> 00:00:29,560
行方式，以及神经元的隐藏层最终要寻找什么。

8
00:00:29,560 --> 00:00:33,421
提醒一下，我们的目标是手写数字识别的

9
00:00:33,421 --> 00:00:37,080
经典示例，即神经网络的“你好世界”。

10
00:00:37,080 --> 00:00:40,753
这些数字在 28x28 像素网格上呈现，每

11
00:00:40,753 --> 00:00:44,260
个像素都有 0 到 1 之间的某个灰度值。

12
00:00:44,260 --> 00:00:51,400
这些决定了网络输入层 784 个神经元的激活。

13
00:00:51,400 --> 00:00:57,109
以下层中每个神经元的激活基于前一层中所有激

14
00:00:57,109 --> 00:01:02,300
活的加权和，加上一些称为偏差的特殊数字。

15
00:01:02,300 --> 00:01:06,036
你可以用一些其他函数来组合这个总和，比如 sigmoi

16
00:01:06,036 --> 00:01:09,640
d 压缩或 ReLU，就像我在上一个视频中演示的那样。

17
00:01:09,640 --> 00:01:14,932
总的来说，考虑到两个隐藏层（每个隐藏层有 16 个神

18
00:01:14,932 --> 00:01:20,224
经元）的任意选择，网络有大约 13,000 个我们可

19
00:01:20,224 --> 00:01:25,320
以调整的权重和偏差，正是这些值决定了网络到底做什么。

20
00:01:25,320 --> 00:01:29,782
当我们说这个网络对给定数字进行分类时，我们的意思是最

21
00:01:29,782 --> 00:01:34,080
后一层中 10 个神经元中最亮的神经元对应于该数字。

22
00:01:34,080 --> 00:01:38,186
请记住，我们对分层结构的想法是，也许

23
00:01:38,186 --> 00:01:42,076
第二层可以拾取边缘，第三层可能拾取

24
00:01:42,076 --> 00:01:45,966
诸如循环和线条之类的图案，最后一层

25
00:01:45,966 --> 00:01:49,640
可以将这些图案拼凑在一起识别数字。

26
00:01:49,640 --> 00:01:52,880
所以在这里，我们学习网络是如何学习的。

27
00:01:52,880 --> 00:01:57,528
我们想要的是一种算法，您可以向该网络显示一大堆训练

28
00:01:57,528 --> 00:02:01,998
数据，这些数据以一堆不同的手写数字图像的形式出现

29
00:02:01,998 --> 00:02:06,468
，以及它们应该是什么的标签，它会调整这 13,0

30
00:02:06,468 --> 00:02:10,760
00 个权重和偏差，以提高其在训练数据上的性能。

31
00:02:10,760 --> 00:02:14,508
希望这种分层结构意味着它所学到的内

32
00:02:14,508 --> 00:02:17,840
容可以推广到训练数据之外的图像。

33
00:02:17,840 --> 00:02:24,766
我们测试的方式是，在训练网络后，向其显示更多标记数

34
00:02:24,766 --> 00:02:31,160
据，然后您会看到它对这些新图像进行分类的准确性。

35
00:02:31,160 --> 00:02:35,978
对我们来说幸运的是，MNIST 数据库背后的优秀人员

36
00:02:35,978 --> 00:02:40,618
已经收集了数以万计的手写数字图像，每个图像都标有它

37
00:02:40,618 --> 00:02:45,080
们应该是的数字，这使得这个例子成为一个常见的例子。

38
00:02:45,080 --> 00:02:48,470
尽管将机器描述为学习是一种挑衅性的说法，但一

39
00:02:48,470 --> 00:02:51,861
旦你看到它是如何工作的， 你就会感觉它不再像

40
00:02:51,861 --> 00:02:55,560
一些疯狂的科幻小说前提，而更像是一次微积分练习。

41
00:02:55,560 --> 00:03:01,040
我的意思是，基本上归结为找到某个函数的最小值。

42
00:03:01,040 --> 00:03:05,898
请记住，从概念上讲，我们认为每个神经元都

43
00:03:05,898 --> 00:03:10,525
连接到前一层中的所有神经元，定义其激活

44
00:03:10,525 --> 00:03:15,384
的加权和中的权重有点像这些连接的强度，而

45
00:03:15,384 --> 00:03:19,780
偏差是该神经元是否倾向于活跃或不活跃。

46
00:03:19,780 --> 00:03:25,020
首先，我们将完全随机地初 始化所有这些权重和偏差。

47
00:03:25,020 --> 00:03:28,254
不用说，这个网络在给定的训练示例上将表现

48
00:03:28,254 --> 00:03:31,180
得很糟糕，因为它只是做一些随机的事情。

49
00:03:31,180 --> 00:03:36,820
例如，您输入 3 的图像，输出层看起来一团糟。

50
00:03:36,820 --> 00:03:40,762
所以你要做的就是定义一个成本函数，这是一种告诉计算机，

51
00:03:40,762 --> 00:03:44,851
不，糟糕的计算机，输出应该具 有对于大多数神经元来说为

52
00:03:44,851 --> 00:03:48,940
0 的激活值，但对于这个神经元来说应该为 1 的激活值。

53
00:03:48,940 --> 00:03:51,740
你给我的东西根本就是垃圾。

54
00:03:51,740 --> 00:03:56,656
从数学角度来说，您可以将每个垃圾输出激活

55
00:03:56,656 --> 00:04:01,572
之间的差异的平方与您希望它们具有的值相加

56
00:04:01,572 --> 00:04:06,020
，这就是我们所说的单个训练示例的成本。

57
00:04:06,020 --> 00:04:12,676
请注意，当网络自信地正确分类图像时，这个总和很小，

58
00:04:12,676 --> 00:04:18,820
但当网络似乎不知道自己在做什么时，这个总和很大。

59
00:04:18,820 --> 00:04:23,457
那么您要做的就是考虑您可以使用的所

60
00:04:23,457 --> 00:04:27,580
有数以万计的训练示例的平均成本。

61
00:04:27,580 --> 00:04:30,521
这个平均成本是我们衡量网络有多糟糕

62
00:04:30,521 --> 00:04:33,300
以及计算机应该感觉有多糟糕的标准。

63
00:04:33,300 --> 00:04:35,300
这是一件复杂的事情。

64
00:04:35,300 --> 00:04:38,398
还记得网络本身基本上是一个函数吗？

65
00:04:38,398 --> 00:04:42,955
它接受 784 个 数字作为输入、像素值，并输出

66
00:04:42,955 --> 00:04:47,694
10 个数字作为输出， 从某种意义上说，它是由所有这

67
00:04:47,694 --> 00:04:49,700
些权重和偏差参数化的？

68
00:04:49,700 --> 00:04:53,340
成本函数是其之上的一层复杂性。

69
00:04:53,340 --> 00:04:58,750
它以大约 13,000 个权重和偏差作为输入，并

70
00:04:58,750 --> 00:05:04,161
输出一个数字来描述这些权重和偏差的严重程度，其定

71
00:05:04,161 --> 00:05:09,140
义方式取决于网络在数以万计的训练数据上的行为。

72
00:05:09,140 --> 00:05:12,460
有很多值得思考的地方。

73
00:05:12,460 --> 00:05:16,380
但仅仅告诉计算机它正在做一件多么糟糕的工作并没有多大帮助。

74
00:05:16,380 --> 00:05:21,300
你想告诉它如何改变这些权重和偏差，以便它变得更好。

75
00:05:21,300 --> 00:05:24,431
为了让它变得更容易，不要费力想象一个具有

76
00:05:24,431 --> 00:05:27,712
13,000 个输入的函数 ，只需想象一个简

77
00:05:27,712 --> 00:05:31,440
单的函数，其中一个数字作为输入，一个数字作为输出。

78
00:05:31,440 --> 00:05:36,420
如何找到使该函数的值最小化的输入？

79
00:05:36,420 --> 00:05:41,493
学微积分的学生会知道，有时您可以明确地算出最小值，但这对于真

80
00:05:41,493 --> 00:05:46,566
正复杂的函数并不总是可行，对于我们疯狂复杂的神经网络成本函

81
00:05:46,566 --> 00:05:51,640
数来说，在这种情况的 13,000 个输入版本中当然不行。

82
00:05:51,640 --> 00:05:55,991
更灵活的策略是从任何输入开始，然后

83
00:05:55,991 --> 00:05:59,860
找出应该采取哪个方向来降低输出。

84
00:05:59,860 --> 00:06:04,223
具体来说，如果您可以计算出所在函数的

85
00:06:04,223 --> 00:06:08,586
斜率，则如果斜率为正，则将输入移至左

86
00:06:08,586 --> 00:06:12,720
侧；如果斜率为负，则将输入移至右侧。

87
00:06:12,720 --> 00:06:16,873
如果您重复执行此操作，在每个点检查新的斜率并采

88
00:06:16,873 --> 00:06:20,680
取适当的步骤，您将接近函数的某些局部最小值。

89
00:06:20,680 --> 00:06:24,600
您可能想到的图像是一个从山上滚下来的球。

90
00:06:24,600 --> 00:06:28,491
请注意，即使对于这个真正简化的单输入函数，

91
00:06:28,491 --> 00:06:32,206
您也可能会遇到许多可能的山谷，具体取决于

92
00:06:32,206 --> 00:06:35,921
您从哪个随机输入开始，并且不能保证您遇到

93
00:06:35,921 --> 00:06:39,460
的局部最小值将是最小的可能值的成本函数。

94
00:06:39,460 --> 00:06:43,180
这也将延续到我们的神经网络案例中。

95
00:06:43,180 --> 00:06:47,663
我还想让你注意到，如果你让你的步长与坡度成

96
00:06:47,663 --> 00:06:51,943
正比，那么当坡度向最小值变平时，你的步幅

97
00:06:51,943 --> 00:06:56,020
会变得越来越小，这有助于你避免过度调整。

98
00:06:56,020 --> 00:07:01,640
稍微提高一下复杂性，想象一个具有两个输入和一个输出的函数。

99
00:07:01,640 --> 00:07:05,547
您可能会将输入空间视为 xy 平面

100
00:07:05,547 --> 00:07:09,020
，并将成本函数视为其上方的曲面。

101
00:07:09,020 --> 00:07:14,514
您不必询问函数的斜率，而必须询问在该输入空间中

102
00:07:14,514 --> 00:07:19,780
应该朝哪个方向迈进，以便最快地减少函数的输出。

103
00:07:19,780 --> 00:07:22,340
换句话说，下坡的方向是什么？

104
00:07:22,340 --> 00:07:26,740
再说一遍，想象一个球从山上滚下来是有帮助的。

105
00:07:26,740 --> 00:07:31,201
熟悉多变量微积分的人都会知道，函数的

106
00:07:31,201 --> 00:07:35,428
梯度为您提供了最陡上升的方向，您应

107
00:07:35,428 --> 00:07:39,420
该朝哪个方向迈进以最快地增加函数。

108
00:07:39,420 --> 00:07:43,691
很自然，取该梯度的负值可以为您提

109
00:07:43,691 --> 00:07:47,460
供以最快的速度减小函数的方向。

110
00:07:47,460 --> 00:07:54,580
更重要的是，该梯度向量的长 度指示了最陡坡度的陡度。

111
00:07:54,580 --> 00:07:57,975
现在，如果您不熟悉多变量微积分并想了解更多信息，

112
00:07:57,975 --> 00:08:01,100
请查看我为可汗学院所做的有关该主题的一些工作。

113
00:08:01,100 --> 00:08:04,872
但老实说，现在对你我来说最重要的是，原

114
00:08:04,872 --> 00:08:08,644
则上存在一种计算这个向量的方法，这个向

115
00:08:08,644 --> 00:08:12,040
量告诉你下坡方向是什么以及它有多陡。

116
00:08:12,040 --> 00:08:17,280
如果您只知道这些并且对细节不太了解，那也没关系。

117
00:08:17,280 --> 00:08:22,527
因为如果你能得到这个，最小化函数的算法就是计算这个梯度

118
00:08:22,527 --> 00:08:27,400
方向，然后向下走一小步，并一遍又一遍地重复这个过程。

119
00:08:27,400 --> 00:08:30,725
对于具有 13,000 个输入而不是

120
00:08:30,725 --> 00:08:33,700
2 个输入的函数，其基本思想相同。

121
00:08:33,700 --> 00:08:37,110
想象一下将我们网络的所有 13,000

122
00:08:37,110 --> 00:08:40,180
个权重和偏差组织成一个巨大的列向量。

123
00:08:40,180 --> 00:08:45,576
成本函数的负梯度只是一个向量，它是这个极其巨

124
00:08:45,576 --> 00:08:50,972
大的输入空间内的某个方向，它告诉您对所有这些

125
00:08:50,972 --> 00:08:55,900
数字的哪些推动将导致成本函数最快速的下降。

126
00:08:55,900 --> 00:08:59,789
当然，通过我们专门设计的成本函数，改变权重

127
00:08:59,789 --> 00:09:03,678
和偏差来减少它意味着使网络在每条训练数据上

128
00:09:03,678 --> 00:09:07,567
的输出看起来不像一个由 10 个值组成的随

129
00:09:07,567 --> 00:09:11,280
机数组，而更像是我们想要的实际决策它使.

130
00:09:11,280 --> 00:09:17,880
重要的是要记住，这个成本函数涉及所有训练数据的平均值，因此

131
00:09:17,880 --> 00:09:24,260
如果将其最小化，则意味着它在所有这些样本上都有更好的性能。

132
00:09:24,260 --> 00:09:27,645
有效计算梯度的算法被称为反向传播，

133
00:09:27,645 --> 00:09:31,030
它实际上是神经网络学习的核心，这也

134
00:09:31,030 --> 00:09:34,040
是我将在下一个视频中讨论的内容。

135
00:09:34,040 --> 00:09:38,755
在那里，我真的想花时间来了解给定训练数据的每

136
00:09:38,755 --> 00:09:43,470
个权重和偏差到底发生了什么，试图对除了一堆相

137
00:09:43,470 --> 00:09:47,980
关微积分和公式之外发生的事情给出直观的感受。

138
00:09:47,980 --> 00:09:51,883
就在这里，现在，我想让你知道的最重要的事

139
00:09:51,883 --> 00:09:55,787
情，与实现细节无关，是当我们谈论网络学习

140
00:09:55,787 --> 00:09:59,320
时，我们的意思是它只是最小化成本函数。

141
00:09:59,320 --> 00:10:02,764
请注意，这样做的一个结果是，对于该成本函数

142
00:10:02,764 --> 00:10:06,208
来说，具有良好的平滑输出非常重要，这样我们

143
00:10:06,208 --> 00:10:09,340
就可以通过向下走一小步来找到局部最小值。

144
00:10:09,340 --> 00:10:13,174
顺便说一句，这就是为什么人工神经元具

145
00:10:13,174 --> 00:10:17,009
有连续范围的激活，而不是像生物神经元

146
00:10:17,009 --> 00:10:20,440
那样简单地以二元方式激活或不激活。

147
00:10:20,440 --> 00:10:26,960
这种通过负梯度的倍数反复推动 函数输入的过程称为梯度下降。

148
00:10:26,960 --> 00:10:30,066
这是一种收敛到成本函数的局部最小值

149
00:10:30,066 --> 00:10:33,000
的方法，基本上是该图中的一个山谷。

150
00:10:33,000 --> 00:10:37,130
当然，我仍然展示具有两个输入的函数的图片，因为

151
00:10:37,130 --> 00:10:41,261
13,000 维输入空间中的微移有点难以理解，

152
00:10:41,261 --> 00:10:45,220
但实际上有一种很好的非空间方式来思考这个问题。

153
00:10:45,220 --> 00:10:49,100
负梯度的每个分量告诉我们两件事。

154
00:10:49,100 --> 00:10:52,589
当然，符号告诉我们输入向量的相

155
00:10:52,589 --> 00:10:55,860
应分量是否应该向上或向下微移。

156
00:10:55,860 --> 00:11:00,887
但重要的是，所有这些组成部分的相

157
00:11:00,887 --> 00:11:05,620
对大小可以告诉您哪些变化更重要。

158
00:11:05,620 --> 00:11:10,495
您会看到，在我们的网络中，对其中一个权重的调整可

159
00:11:10,495 --> 00:11:14,980
能比对其他权重的调整对成本函数产生更大的影响。

160
00:11:14,980 --> 00:11:19,440
其中一些联系对于我们的训练数据来说更重要。

161
00:11:19,440 --> 00:11:24,522
因此，您可以考虑我们的令人难以置信的巨大成本函数的

162
00:11:24,522 --> 00:11:29,408
梯度向量，它编码了每个权重和偏差的相对重要性，也

163
00:11:29,408 --> 00:11:34,100
就是说，这些变化中的哪一个将为您带来最大的收益。

164
00:11:34,100 --> 00:11:37,360
这实际上只是思考方向的另一种方式。

165
00:11:37,360 --> 00:11:41,732
举一个更简单的例子，如果你有一个带有两个变

166
00:11:41,732 --> 00:11:46,900
量作为输入的函数，并计算出它在某个特定点的 梯度为

167
00:11:46,900 --> 00:11:50,478
3,1，那么一方面你可以将其解释为

168
00:11:50,478 --> 00:11:54,851
当你站在该输入处，沿着这个方向移动，函数的

169
00:11:54,851 --> 00:11:59,224
增加速度最快，当您在输入点平面上方绘制函数

170
00:11:59,224 --> 00:12:03,200
图时，该向量就是给您直线上坡方向的方向。

171
00:12:03,200 --> 00:12:08,185
但另一种解读方式是，对第一个变量的更改的重要性

172
00:12:08,185 --> 00:12:13,170
是对第二个变量的更改的三倍，至少在相关输入的附

173
00:12:13,170 --> 00:12:17,740
近，推动 x 值会给您带来更大的影响。巴克。

174
00:12:17,740 --> 00:12:22,880
好吧，让我们缩小范围并总结一下到目前为止我们所处的位置。

175
00:12:22,880 --> 00:12:27,051
网络本身就是一个具有 784 个输入和 10

176
00:12:27,051 --> 00:12:30,860
个输出的函数，根据所有这些加权和进行定义。

177
00:12:30,860 --> 00:12:34,160
成本函数是其之上的一层复杂性。

178
00:12:34,160 --> 00:12:38,503
它以 13,000 个权重和偏差作为输入

179
00:12:38,503 --> 00:12:42,640
，并根据训练示例给出单一的糟糕程度度量。

180
00:12:42,640 --> 00:12:47,520
成本函数的梯度仍然是一层复杂性。

181
00:12:47,520 --> 00:12:52,881
它告诉我们对所有这些权重和偏差的推动

182
00:12:52,881 --> 00:12:58,242
会导致成本函数值发生最快的变化，您可

183
00:12:58,242 --> 00:13:03,040
以将其解释为哪些权重的变化最重要。

184
00:13:03,040 --> 00:13:06,902
因此，当您使用随机权重和偏差初始化网络

185
00:13:06,902 --> 00:13:10,764
，并根据梯度下降过程多次调整它们时，它

186
00:13:10,764 --> 00:13:14,240
在以前从未见过的图像上实际表现如何？

187
00:13:14,240 --> 00:13:18,577
我在这里描述的那个有两个隐藏层，每个隐藏层有 16

188
00:13:18,577 --> 00:13:22,748
个神经元，主要是出于美观 原因而选择的，这还不错，

189
00:13:22,748 --> 00:13:26,920
它对它看到的大约 96% 的新图像进行了正确分类。

190
00:13:26,920 --> 00:13:31,744
老实说，如果你看一下它搞砸的一些例

191
00:13:31,744 --> 00:13:36,300
子，你就会觉得有必要稍微放松一下。

192
00:13:36,300 --> 00:13:38,904
如果您尝试一下隐藏层结构并进行一些

193
00:13:38,904 --> 00:13:41,220
调整，您可以将其提高到 98%。

194
00:13:41,220 --> 00:13:42,900
这非常好！

195
00:13:42,900 --> 00:13:47,858
这不是最好的，你当然可以通过比这个简单的普通网络更复

196
00:13:47,858 --> 00:13:52,633
杂来获得更好的性能，但考虑到最初的任务是多么艰巨，

197
00:13:52,633 --> 00:13:57,408
我认为任何网络在图像上做得这么好都是令人难以置信的

198
00:13:57,408 --> 00:14:02,000
，这是以前从未见过的从未具体告诉它要寻找什么模式。

199
00:14:02,000 --> 00:14:06,239
最初，我激发这种结构的方式是通过描述我们可能

200
00:14:06,239 --> 00:14:10,294
拥有的希望，即第二层可能会拾取小边缘，第三

201
00:14:10,294 --> 00:14:14,349
层会将这些边缘拼凑在一起以识别循环和较长的

202
00:14:14,349 --> 00:14:18,220
线，并且这些可能会被拼凑起来一起识别数字。

203
00:14:18,220 --> 00:14:21,040
那么这就是我们的网络实际上正在做的事情吗？

204
00:14:21,040 --> 00:14:24,880
好吧，至少对于这一点来说，根本不是。

205
00:14:24,880 --> 00:14:29,256
还记得我们在上一个视频中如何将第一层中的所有

206
00:14:29,256 --> 00:14:33,443
神经元到第二层中的给定神经元的连接权重可视

207
00:14:33,443 --> 00:14:37,440
化为第二层神经元正在拾取的给定像素模式吗？

208
00:14:37,440 --> 00:14:43,193
好吧，当我们对与这些过渡相关的权重执行此操作

209
00:14:43,193 --> 00:14:48,946
时，它们看起来几乎是随机的，而不是到处拾取孤

210
00:14:48,946 --> 00:14:54,200
立的小边缘，只是中间有一些非常松散的图案。

211
00:14:54,200 --> 00:14:58,236
看起来，在可能的权重和偏差的深不可测的 13,

212
00:14:58,236 --> 00:15:02,104
000 维空间中，我们的网络发现自己有一个令

213
00:15:02,104 --> 00:15:06,140
人愉快的局部最小值，尽管成功地对大多数图像进行

214
00:15:06,140 --> 00:15:09,840
了分类，但并没有完全拾取我们可能希望的模式。

215
00:15:09,840 --> 00:15:14,600
为了真正理解这一点，请观察输入随机图像时会发生什么。

216
00:15:14,600 --> 00:15:19,717
如果系统很聪明，你可能会认为它要么感觉不确定，要么没有真正

217
00:15:19,717 --> 00:15:24,665
激活这 10 个输出神经元中的任何一个，或者均匀地激活它

218
00:15:24,665 --> 00:15:29,783
们，但相反，它会自信地给你一些无意义的答案，就好像它感觉确

219
00:15:29,783 --> 00:15:34,560
定这个随机噪声是 5，就像 5 的实际图像是 5 一样。

220
00:15:34,560 --> 00:15:38,392
换句话说，即使这个网络可以很好地识

221
00:15:38,392 --> 00:15:41,800
别数字，它也不知道如何绘制它们。

222
00:15:41,800 --> 00:15:45,400
这很大程度上是因为它的训练设置受到严格限制。

223
00:15:45,400 --> 00:15:48,220
我的意思是，请站在网络的立场上思考。

224
00:15:48,220 --> 00:15:52,999
从它的角度来看，整个宇宙只是由以微小网格为中心

225
00:15:52,999 --> 00:15:57,778
的明确定义的不动数字组成，而它的成本函数从来没

226
00:15:57,778 --> 00:16:02,160
有给它任何激励，除了对自己的决定完全有信心。

227
00:16:02,160 --> 00:16:04,928
因此，以此作为第二层神经元真正在做什

228
00:16:04,928 --> 00:16:07,697
么的图像，您可能想知道为什么我会出于

229
00:16:07,697 --> 00:16:10,320
拾取边缘和模式的动机而引入这个网络。

230
00:16:10,320 --> 00:16:13,040
我的意思是，这根本不是它最终要做的事情。

231
00:16:13,040 --> 00:16:17,480
嗯，这并不是我们的最终目标，而是一个起点。

232
00:16:17,480 --> 00:16:22,986
坦率地说，这是旧技术，是 80 年代和 90 年代研究

233
00:16:22,986 --> 00:16:28,296
的那种技术，你确实需要先了解它，然后才能了解更详细的

234
00:16:28,296 --> 00:16:33,606
现代变体，而且它显然能够解决一些有趣的问题，但你越深

235
00:16:33,606 --> 00:16:38,720
入了解什么那些隐藏层确实在做事，但看起来却不太智能。

236
00:16:38,720 --> 00:16:43,108
将焦点暂时从网络如何学习转移到你如何学习，只有当你

237
00:16:43,108 --> 00:16:47,160
以某种方式积极参与这里的材料时才会发生这种情况。

238
00:16:47,160 --> 00:16:52,132
我希望您做的一件非常简单的事情就是现在暂停并深入

239
00:16:52,132 --> 00:16:57,105
思考一下您可以对该系统进行哪些更改以及如果您希望

240
00:16:57,105 --> 00:17:01,880
它更好地识别边缘和图案等内容，它会如何感知图像。

241
00:17:01,880 --> 00:17:04,452
但更好的是，为了真正理解这些材料，我强烈推

242
00:17:04,452 --> 00:17:06,657
荐迈克尔·尼尔森（Mi chael

243
00:17:06,657 --> 00:17:09,720
Nielsen）撰写的关于深度学习和神经网络的书。

244
00:17:09,720 --> 00:17:14,647
在其中，您可以找到要下载和使用该示例的代码和

245
00:17:14,647 --> 00:17:19,360
数据，并且本书将逐步引导您完成该代码的作用。

246
00:17:19,360 --> 00:17:23,880
很棒的是，这本书是免费且公开的，所以如果您确实从

247
00:17:23,880 --> 00:17:28,040
中有所收获，请考虑与我一起为尼尔森的努力捐款。

248
00:17:28,040 --> 00:17:33,564
我还在描述中链接了一些我非常喜欢的其他资源，包括 Chri

249
00:17:33,564 --> 00:17:38,720
s Ola 的精彩博客文章和 Distill 中的文章。

250
00:17:38,720 --> 00:17:41,653
为了结束最后几分钟的讨论，我想跳回到我

251
00:17:41,653 --> 00:17:44,440
与 Leisha Lee 的采访片段。

252
00:17:44,440 --> 00:17:46,414
您可能还记得上一个视频中的她，

253
00:17:46,414 --> 00:17:48,520
她在深度学习方面取得了博士学位。

254
00:17:48,520 --> 00:17:52,595
在这个小片段中，她谈到了最近的两篇论文，这些论文真正深

255
00:17:52,595 --> 00:17:56,380
入探讨了一些更现代的图像识别网络实际上是如何学习的。

256
00:17:56,380 --> 00:18:00,772
为了确定我们在对话中的位置，第一篇论文采用了一个非常擅

257
00:18:00,772 --> 00:18:05,164
长图像识别的特别深层的神经网络，它不是在正确标记的数据

258
00:18:05,164 --> 00:18:09,400
集上对其进行训练，而是在训练之前对所有标签进行了洗牌。

259
00:18:09,400 --> 00:18:12,524
显然，这里的测试准确性不会比随机测试

260
00:18:12,524 --> 00:18:15,320
更好，因为所有内容都是随机标记的。

261
00:18:15,320 --> 00:18:21,440
但它仍然能够达到与在正确标记 的数据集上相同的训练精度。

262
00:18:21,440 --> 00:18:26,599
基本上，这个特定网络的数百万个权重足以让它记住随机

263
00:18:26,599 --> 00:18:31,758
数据，这就提出了一个问题：最小化这个成本函数是否实

264
00:18:31,758 --> 00:18:36,720
际上对应于图像中的任何类型的结构，或者只是记忆？。

265
00:18:36,720 --> 00:18:40,120
。。记住正确分类的整个数据集。

266
00:18:40,120 --> 00:18:44,268
所以，半年后的今年，在 ICML 上，并没有出

267
00:18:44,268 --> 00:18:48,417
现反驳论文，而是讨论了某些方面的论文，例如，嘿

268
00:18:48,417 --> 00:18:52,220
，实际上这些网络正在做一些比这更聪明的事情。

269
00:18:52,220 --> 00:18:58,980
如果你看一下准确度曲线，如果你只是在随机数据集上进行

270
00:18:58,980 --> 00:19:05,240
训练，那么这条曲线会以几乎线性的方式下降得非常慢。

271
00:19:05,240 --> 00:19:08,957
所以你真的很难找到可能的局部最小值，你知

272
00:19:08,957 --> 00:19:12,320
道，正确的权重可以让你获得这种准确性。

273
00:19:12,320 --> 00:19:16,153
然而，如果您实际上是在结构化数据集（具有正确标签

274
00:19:16,153 --> 00:19:19,833
的数据集）上进行训练，您知道，一开始您会进行一

275
00:19:19,833 --> 00:19:23,360
些调整，但随后您会很快下降以达到该准确度水平。

276
00:19:23,360 --> 00:19:28,580
因此从某种意义上说，找到局部最大值更容易。

277
00:19:28,580 --> 00:19:36,506
有趣的是，它揭示了几年 前的另一篇论文，其中对

278
00:19:36,506 --> 00:19:40,140
网络层进行了更多简化。

279
00:19:40,140 --> 00:19:44,877
但结果之一是，如果你看看优化情况，这些网络

280
00:19:44,877 --> 00:19:49,400
倾向于学习的局部最小值实际上是相同质量的。

281
00:19:49,400 --> 00:19:51,783
因此，从某种意义上说，如果您的数据集

282
00:19:51,783 --> 00:19:54,300
是结构化的，您应该能够更轻松地找到它。

283
00:19:54,300 --> 00:20:01,140
我一如既往地感谢你们对 Patreon 的支持。

284
00:20:01,140 --> 00:20:04,211
我之前已经说过 Patreon 上的游戏规则改变

285
00:20:04,211 --> 00:20:07,160
者是什么，但如果没有您，这些视频真的不可能实现。

286
00:20:07,160 --> 00:20:10,326
我还要特别感谢风险投资公司 Amplify Pa

287
00:20:10,326 --> 00:20:13,240
rtners 以及他们对本系列初始视频的支持。

288
00:20:13,240 --> 00:20:33,140
谢谢。

