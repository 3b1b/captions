1
00:00:04,180 --> 00:00:07,280
上一个视频我展示了神经网络的结构。

2
00:00:07,680 --> 00:00:10,140
我将在这里快速回顾一下，以便我们记住

3
00:00:10,140 --> 00:00:12,600
它，然后我对这个视频有两个主要目标。

4
00:00:13,100 --> 00:00:16,770
首先是介绍梯度下降的概念，它不仅是神经网络学习

5
00:00:16,770 --> 00:00:20,600
方式的基础，也是许多其他机器学习工作方式的基础。

6
00:00:21,120 --> 00:00:24,530
然后，我们将进一步深入研究这个特定网络的执

7
00:00:24,530 --> 00:00:27,940
行方式，以及神经元的隐藏层最终要寻找什么。

8
00:00:28,979 --> 00:00:32,599
提醒一下，我们的目标是手写数字识别的

9
00:00:32,599 --> 00:00:36,220
经典示例，即神经网络的“你好世界”。

10
00:00:37,020 --> 00:00:41,134
这些数字在 28x28 像素网格上呈现，每个像素都有 

11
00:00:41,134 --> 00:00:43,420
0 到 1 之间的某个灰度值。

12
00:00:43,820 --> 00:00:50,040
这些决定了网络输入层 784 个神经元的激活。

13
00:00:51,180 --> 00:00:56,000
然后，后续层中每个神经元的激活基于前一层中所

14
00:00:56,000 --> 00:01:00,820
有激活的加权和，加上一些称为偏差的特殊数字。

15
00:01:02,160 --> 00:01:05,801
然后你用一些其他函数来组合这个总和，比如 sigmoid 

16
00:01:05,801 --> 00:01:08,940
压缩或 relu，就像我在上一个视频中演示的那样。

17
00:01:09,480 --> 00:01:14,064
总的来说，考虑到两个隐藏层（每个隐藏层有 16 

18
00:01:14,064 --> 00:01:18,649
个神经元）的任意选择，网络有大约 13,000 

19
00:01:18,649 --> 00:01:24,380
个我们可以调整的权重和偏差，正是这些值决定了网络到底做什么。

20
00:01:24,880 --> 00:01:29,090
那么当我们说这个网络对给定数字进行分类时，我们的意思是

21
00:01:29,090 --> 00:01:33,300
最后一层中 10 个神经元中最亮的神经元对应于该数字。

22
00:01:34,100 --> 00:01:38,867
请记住，我们在这里想到分层结构的动机是，也许第二

23
00:01:38,867 --> 00:01:43,635
层可以拾取边缘，第三层可能拾取诸如循环和线条之类

24
00:01:43,635 --> 00:01:48,800
的图案，最后一层可以将这些拼凑在一起识别数字的模式。

25
00:01:49,800 --> 00:01:52,240
所以在这里，我们学习网络是如何学习的。

26
00:01:52,640 --> 00:01:56,964
我们想要的是一种算法，您可以向该网络显示一大堆训

27
00:01:56,964 --> 00:02:01,289
练数据，这些数据以一堆不同的手写数字图像的形式出

28
00:02:01,289 --> 00:02:05,074
现，以及它们应该是什么的标签，它会调整这 

29
00:02:05,074 --> 00:02:10,120
13,000 个权重和偏差，以提高其在训练数据上的性能。

30
00:02:10,720 --> 00:02:13,696
希望这种分层结构意味着它所学到的

31
00:02:13,696 --> 00:02:16,860
东西可以推广到训练数据之外的图像。

32
00:02:17,640 --> 00:02:22,170
我们测试的方式是，在训练网络后，向其显示更多以前从未见过

33
00:02:22,170 --> 00:02:26,700
的标记数据，然后您会看到它对这些新图像进行分类的准确性。

34
00:02:31,120 --> 00:02:33,837
对我们来说幸运的是，MNIST 

35
00:02:33,837 --> 00:02:38,084
数据库背后的优秀人员已经收集了数以万计的手写数字图

36
00:02:38,084 --> 00:02:42,331
像，每张都标有它们应该标记的数字，这使得这个例子成

37
00:02:42,331 --> 00:02:44,200
为一个常见的例子。是。

38
00:02:44,900 --> 00:02:48,374
尽管将机器描述为学习是一种挑衅性的说法，但一

39
00:02:48,374 --> 00:02:51,848
旦你看到它是如何工作的，你就会感觉它不再像一

40
00:02:51,848 --> 00:02:55,480
些疯狂的科幻小说前提，而更像是一次微积分练习。

41
00:02:56,200 --> 00:02:59,960
我的意思是，基本上归结为找到某个函数的最小值。

42
00:03:01,939 --> 00:03:07,613
请记住，从概念上讲，我们认为每个神经元都连接到前一层

43
00:03:07,613 --> 00:03:13,286
中的所有神经元，定义其激活的加权和中的权重有点像这些

44
00:03:13,286 --> 00:03:18,960
连接的强度，而偏差是该神经元是否倾向于活跃或不活跃。

45
00:03:19,720 --> 00:03:24,400
首先，我们将完全随机地初始化所有这些权重和偏差。

46
00:03:24,940 --> 00:03:27,829
不用说，这个网络在给定的训练示例上的表现

47
00:03:27,829 --> 00:03:30,720
将非常糟糕，因为它只是做一些随机的事情。

48
00:03:31,040 --> 00:03:36,020
例如，您输入 3 的图像，输出层看起来一团糟。

49
00:03:36,600 --> 00:03:41,210
所以你要做的就是定义一个成本函数，一种告诉计算机的方式，

50
00:03:41,210 --> 00:03:45,655
不，糟糕的计算机，输出应该具有对于大多数神经元来说是 

51
00:03:45,655 --> 00:03:48,784
0 的激活，但是对于这个神经元来说是 

52
00:03:48,784 --> 00:03:50,760
1，你给我的完全是垃圾。

53
00:03:51,720 --> 00:03:58,257
从数学角度来说，您可以将每个垃圾输出激活之间的差异的平方与

54
00:03:58,257 --> 00:04:05,020
您希望它们具有的值相加，这就是我们所说的单个训练示例的成本。

55
00:04:05,960 --> 00:04:11,073
请注意，当网络自信地正确分类图像时，这个总和很小

56
00:04:11,073 --> 00:04:16,399
，但当网络似乎不知道自己在做什么时，这个总和很大。

57
00:04:18,640 --> 00:04:22,039
因此，您要做的就是考虑您可以使用的

58
00:04:22,039 --> 00:04:25,440
所有数以万计的训练示例的平均成本。

59
00:04:27,040 --> 00:04:29,890
这个平均成本是我们衡量网络有多糟糕

60
00:04:29,890 --> 00:04:32,740
以及计算机应该感觉有多糟糕的标准。

61
00:04:33,420 --> 00:04:34,600
这是一件复杂的事情。

62
00:04:35,040 --> 00:04:38,077
还记得网络本身基本上是一个函数吗？

63
00:04:38,077 --> 00:04:42,366
它接受 784 个数字作为输入、像素值，并输出 

64
00:04:42,366 --> 00:04:46,834
10 个数字作为输出，从某种意义上说，它是由所有这

65
00:04:46,834 --> 00:04:48,800
些权重和偏差参数化的？

66
00:04:49,500 --> 00:04:52,820
成本函数是其之上的一层复杂性。

67
00:04:53,100 --> 00:04:58,218
它将大约 13,000 个权重和偏差作为输入，

68
00:04:58,218 --> 00:05:03,336
并输出一个数字来描述这些权重和偏差的严重程度，

69
00:05:03,336 --> 00:05:08,900
其定义方式取决于网络在数以万计的训练数据上的行为。

70
00:05:09,520 --> 00:05:11,000
有很多值得思考的地方。

71
00:05:12,400 --> 00:05:15,820
但仅仅告诉计算机它正在做一件多么糟糕的工作并没有多大帮助。

72
00:05:16,220 --> 00:05:20,060
你想告诉它如何改变这些权重和偏差，以便它变得更好。

73
00:05:20,780 --> 00:05:23,820
为了让它变得更容易，不要费力想象一个具有 

74
00:05:23,820 --> 00:05:27,005
13,000 个输入的函数，只需想象一个简单

75
00:05:27,005 --> 00:05:30,480
的函数，其中一个数字作为输入，一个数字作为输出。

76
00:05:31,480 --> 00:05:35,300
如何找到使该函数的值最小化的输入？

77
00:05:36,460 --> 00:05:41,277
学微积分的学生会知道，有时您可以明确地算出最小值，但这对于

78
00:05:41,277 --> 00:05:46,095
真正复杂的函数并不总是可行，对于我们疯狂复杂的神经网络成本

79
00:05:46,095 --> 00:05:51,080
函数来说，在这种情况的 13,000 个输入版本中当然不行。

80
00:05:51,580 --> 00:05:55,281
一种更灵活的策略是从任何输入开始，

81
00:05:55,281 --> 00:05:59,200
然后找出应该采取哪个方向来降低输出。

82
00:06:00,080 --> 00:06:04,990
具体来说，如果您可以计算出所在函数的斜率，则如果斜率为

83
00:06:04,990 --> 00:06:09,900
正，则将输入移至左侧；如果斜率为负，则将输入移至右侧。

84
00:06:11,960 --> 00:06:15,812
如果您重复执行此操作，在每个点检查新的斜率并

85
00:06:15,812 --> 00:06:19,840
采取适当的步骤，您将接近函数的某些局部最小值。

86
00:06:20,640 --> 00:06:23,800
您可能想到的图像是一个从山上滚下来的球。

87
00:06:24,620 --> 00:06:29,546
请注意，即使对于这个真正简化的单输入函数，您也可能会遇

88
00:06:29,546 --> 00:06:34,473
到许多可能的山谷，具体取决于您从哪个随机输入开始，并且

89
00:06:34,473 --> 00:06:39,400
不能保证您遇到的局部最小值将是最小的可能值的成本函数。

90
00:06:40,220 --> 00:06:42,620
这也将延续到我们的神经网络案例中。

91
00:06:43,180 --> 00:06:48,789
我还希望您注意，如果您使步长与斜率成正比，那么当斜率趋于

92
00:06:48,789 --> 00:06:54,600
最小值时，您的步数会变得越来越小，这有助于您避免过度调整。

93
00:06:55,940 --> 00:07:00,980
稍微提高一下复杂性，想象一个具有两个输入和一个输出的函数。

94
00:07:01,500 --> 00:07:04,719
您可能会将输入空间视为 xy 平

95
00:07:04,719 --> 00:07:08,140
面，并将成本函数视为其上方的曲面。

96
00:07:08,760 --> 00:07:13,860
您不必询问函数的斜率，而必须询问在该输入空间中

97
00:07:13,860 --> 00:07:18,960
应该朝哪个方向迈进，以便最快地减少函数的输出。

98
00:07:19,720 --> 00:07:21,760
换句话说，下坡的方向是什么？

99
00:07:22,380 --> 00:07:25,560
再次，想象一个球从山上滚下来是有帮助的。

100
00:07:26,660 --> 00:07:32,720
熟悉多变量微积分的人都会知道，函数的梯度为您提供了最

101
00:07:32,720 --> 00:07:38,780
陡上升的方向，您应该朝哪个方向迈进以最快地增加函数。

102
00:07:39,560 --> 00:07:42,695
很自然，取该梯度的负值可以为您

103
00:07:42,695 --> 00:07:46,040
提供以最快的速度减小函数的方向。

104
00:07:47,240 --> 00:07:53,840
更重要的是，该梯度向量的长度指示了最陡坡度的陡度。

105
00:07:54,540 --> 00:07:57,440
如果您不熟悉多变量微积分并想了解更多信息，请

106
00:07:57,440 --> 00:08:00,340
查看我为可汗学院所做的有关该主题的一些工作。

107
00:08:00,860 --> 00:08:06,380
但老实说，现在对你我来说最重要的是，原则上存在一种计算这

108
00:08:06,380 --> 00:08:11,900
个向量的方法，这个向量告诉你下坡方向是什么以及它有多陡。

109
00:08:12,400 --> 00:08:16,120
如果您只知道这些并且对细节不太了解，那也没关系。

110
00:08:17,200 --> 00:08:21,970
如果你能得到这个，最小化函数的算法就是计算这个梯度方

111
00:08:21,970 --> 00:08:26,740
向，然后向下走一小步，然后一遍又一遍地重复这个过程。

112
00:08:27,700 --> 00:08:30,402
对于具有 13,000 个输入而不是 

113
00:08:30,402 --> 00:08:32,820
2 个输入的函数，其基本思想相同。

114
00:08:33,400 --> 00:08:36,589
想象一下将我们网络的所有 13,000 

115
00:08:36,589 --> 00:08:39,460
个权重和偏差组织成一个巨大的列向量。

116
00:08:40,140 --> 00:08:44,902
成本函数的负梯度只是一个向量，它是这个极其

117
00:08:44,902 --> 00:08:49,664
巨大的输入空间内的某个方向，它告诉您对所有

118
00:08:49,664 --> 00:08:54,880
这些数字的哪些推动将导致成本函数最快速的下降。

119
00:08:55,640 --> 00:09:00,578
当然，通过我们专门设计的成本函数，改变权重和偏差来减少

120
00:09:00,578 --> 00:09:05,516
它意味着使网络在每条训练数据上的输出看起来不像一个由 

121
00:09:05,516 --> 00:09:10,820
10 个值组成的随机数组，而更像是我们想要的实际决策它使.

122
00:09:11,440 --> 00:09:16,310
重要的是要记住，这个成本函数涉及所有训练数据的平均值，因此

123
00:09:16,310 --> 00:09:21,180
如果将其最小化，则意味着它在所有这些样本上都有更好的性能。

124
00:09:23,820 --> 00:09:28,900
有效计算梯度的算法被称为反向传播，它实际上是神经网

125
00:09:28,900 --> 00:09:33,980
络学习的核心，这也是我将在下一个视频中讨论的内容。

126
00:09:34,660 --> 00:09:38,806
在那里，我真的想花时间来了解给定训练数据的每

127
00:09:38,806 --> 00:09:42,953
个权重和偏差到底发生了什么，试图对除了一堆相

128
00:09:42,953 --> 00:09:47,100
关微积分和公式之外发生的事情给出直观的感受。

129
00:09:47,780 --> 00:09:52,980
就在这里，现在，我想让你知道的最重要的事情，与实现细节无关

130
00:09:52,980 --> 00:09:58,360
，是当我们谈论网络学习时，我们的意思是它只是最小化成本函数。

131
00:09:59,300 --> 00:10:02,138
请注意，这样做的一个结果是，对于该成本函

132
00:10:02,138 --> 00:10:04,977
数来说，具有良好的平滑输出非常重要，这样

133
00:10:04,977 --> 00:10:08,100
我们就可以通过向下走一小步来找到局部最小值。

134
00:10:09,260 --> 00:10:14,106
顺便说一句，这就是为什么人工神经元具有连续范围的激活

135
00:10:14,106 --> 00:10:19,140
，而不是像生物神经元那样简单地以二元方式激活或不激活。

136
00:10:20,220 --> 00:10:26,760
这种通过负梯度的倍数反复推动函数输入的过程称为梯度下降。

137
00:10:27,300 --> 00:10:29,940
这是一种收敛到成本函数的局部最小值

138
00:10:29,940 --> 00:10:32,580
的方法，基本上是该图中的一个山谷。

139
00:10:33,440 --> 00:10:37,315
当然，我仍然展示具有两个输入的函数的图片，因为 

140
00:10:37,315 --> 00:10:40,868
13,000 维输入空间中的微移有点难以理解

141
00:10:40,868 --> 00:10:44,260
，但有一种很好的非空间方式来思考这个问题。

142
00:10:45,080 --> 00:10:48,440
负梯度的每个分量告诉我们两件事。

143
00:10:49,060 --> 00:10:55,140
当然，符号告诉我们输入向量的相应分量是否应该向上或向下微移。

144
00:10:55,800 --> 00:10:59,260
但重要的是，所有这些组成部分的相

145
00:10:59,260 --> 00:11:02,720
对大小可以告诉您哪些变化更重要。

146
00:11:05,220 --> 00:11:09,046
您会看到，在我们的网络中，对其中一个权重的调整

147
00:11:09,046 --> 00:11:13,040
可能比对其他权重的调整对成本函数产生更大的影响。

148
00:11:14,800 --> 00:11:18,200
其中一些联系对于我们的训练数据来说更重要。

149
00:11:19,320 --> 00:11:23,620
因此，您可以考虑我们的令人难以置信的巨大成本函数

150
00:11:23,620 --> 00:11:27,920
的梯度向量，它编码了每个权重和偏差的相对重要性，

151
00:11:27,920 --> 00:11:32,400
也就是说，这些变化中的哪一个将为您带来最大的收益。

152
00:11:33,620 --> 00:11:36,640
这实际上只是思考方向的另一种方式。

153
00:11:37,100 --> 00:11:41,975
举一个更简单的例子，如果你有一个带有两个变量作为输

154
00:11:41,975 --> 00:11:46,461
入的函数，并且你计算出它在某个特定点的梯度为 

155
00:11:46,461 --> 00:11:51,337
3,1，那么一方面你可以将其解释为当你&#39;站

156
00:11:51,337 --> 00:11:56,213
在该输入处，沿着这个方向移动会最快地增加函数，当您

157
00:11:56,213 --> 00:12:01,089
在输入点平面上方绘制函数时，该向量就是给您直线上坡

158
00:12:01,089 --> 00:12:02,260
方向的方向。

159
00:12:02,860 --> 00:12:07,473
但另一种解读方式是，对第一个变量的更改的重要性

160
00:12:07,473 --> 00:12:13,490
是对第二个变量的更改的 3 倍，至少在相关输入的附近，轻推 

161
00:12:13,490 --> 00:12:16,900
x 值会给您带来更大的影响。巴克。

162
00:12:19,880 --> 00:12:22,340
让我们缩小范围并总结一下目前为止的情况。

163
00:12:22,840 --> 00:12:26,603
网络本身就是一个具有 784 个输入和 10 

164
00:12:26,603 --> 00:12:30,040
个输出的函数，根据所有这些加权和进行定义。

165
00:12:30,640 --> 00:12:33,680
成本函数是其之上的一层复杂性。

166
00:12:33,980 --> 00:12:37,850
它以 13,000 个权重和偏差作为输入

167
00:12:37,850 --> 00:12:41,720
，并根据训练示例输出单一的糟糕程度度量。

168
00:12:42,440 --> 00:12:46,900
而成本函数的梯度又增加了一层复杂性。

169
00:12:47,360 --> 00:12:52,520
它告诉我们对所有这些权重和偏差的推动会导致成本函数值

170
00:12:52,520 --> 00:12:57,880
发生最快的变化，您可以将其解释为哪些权重的变化最重要。

171
00:13:02,560 --> 00:13:07,880
那么，当您使用随机权重和偏差初始化网络，并根据梯度下降过

172
00:13:07,880 --> 00:13:13,200
程多次调整它们时，它在以前从未见过的图像上实际表现如何？

173
00:13:14,100 --> 00:13:18,211
我在这里描述的那个有两个隐藏层，每个隐藏层有 16 

174
00:13:18,211 --> 00:13:22,164
个神经元，主要是出于美观原因而选择的，这还不错，它

175
00:13:22,164 --> 00:13:25,960
对它看到的大约 96% 的新图像进行了正确分类。

176
00:13:26,680 --> 00:13:29,609
老实说，如果你看一下它搞砸的一些例

177
00:13:29,609 --> 00:13:32,540
子，你就会觉得有必要稍微放松一下。

178
00:13:36,220 --> 00:13:38,990
现在，如果您尝试一下隐藏层结构并进行

179
00:13:38,990 --> 00:13:41,760
一些调整，您可以将其提高到 98%。

180
00:13:41,760 --> 00:13:42,720
这非常好！

181
00:13:43,020 --> 00:13:47,620
这不是最好的，你当然可以通过比这个普通网络更复杂来

182
00:13:47,620 --> 00:13:52,220
获得更好的性能，但考虑到最初的任务是多么艰巨，我认

183
00:13:52,220 --> 00:13:56,819
为任何网络在以前从未见过的图像上做得这么好都是令人

184
00:13:56,819 --> 00:14:01,420
难以置信的，因为我们从未具体告诉它要寻找什么模式。

185
00:14:02,560 --> 00:14:07,376
最初，我激发这种结构的方式是通过描述我们可能拥有的希望，

186
00:14:07,376 --> 00:14:12,192
即第二层可能会拾取小边缘，第三层会将这些边缘拼凑在一起以

187
00:14:12,192 --> 00:14:17,180
识别循环和较长的线，并且这些可能会被拼凑起来一起识别数字。

188
00:14:17,960 --> 00:14:20,400
那么这就是我们的网络实际上正在做的事情吗？

189
00:14:21,079 --> 00:14:24,400
好吧，至少对于这一点来说，根本不是。

190
00:14:24,820 --> 00:14:28,836
还记得我们在上一个视频中如何将第一层中的所

191
00:14:28,836 --> 00:14:32,852
有神经元到第二层中的给定神经元的连接权重可

192
00:14:32,852 --> 00:14:37,060
视化为第二层神经元正在拾取的给定像素模式吗？

193
00:14:37,780 --> 00:14:42,952
好吧，当我们实际上对与这些过渡相关的权重执行此操作时，

194
00:14:42,952 --> 00:14:48,124
从第一层到下一层，而不是在这里或那里拾取孤立的小边缘，

195
00:14:48,124 --> 00:14:53,680
它们看起来几乎是随机的，只是有一些非常松散的模式中间那里。

196
00:14:53,760 --> 00:14:58,319
看起来，在可能的权重和偏差的深不可测的 13,000 

197
00:14:58,319 --> 00:15:03,386
维空间中，我们的网络发现自己有一个令人愉快的局部最小值，尽管

198
00:15:03,386 --> 00:15:08,453
成功地对大多数图像进行了分类，但并没有完全拾取我们可能希望的

199
00:15:08,453 --> 00:15:08,960
模式。

200
00:15:09,780 --> 00:15:13,820
为了真正理解这一点，请观察输入随机图像时会发生什么。

201
00:15:14,320 --> 00:15:19,234
如果系统很聪明，你可能会认为它会感到不确定，也许并没有

202
00:15:19,234 --> 00:15:24,148
真正激活这 10 个输出神经元中的任何一个或均匀地激活

203
00:15:24,148 --> 00:15:29,063
它们，但它却自信地给你一些无意义的答案，就好像它感觉确

204
00:15:29,063 --> 00:15:34,160
定这个随机噪声是 5，就像 5 的实际图像是 5 一样。

205
00:15:34,540 --> 00:15:37,526
换句话说，即使这个网络可以很好地

206
00:15:37,526 --> 00:15:40,700
识别数字，它也不知道如何绘制它们。

207
00:15:41,420 --> 00:15:45,240
这很大程度上是因为它的训练设置受到严格限制。

208
00:15:45,880 --> 00:15:47,740
我的意思是，请站在网络的立场上思考。

209
00:15:48,140 --> 00:15:52,326
从它的角度来看，整个宇宙只是由以微小网格为中

210
00:15:52,326 --> 00:15:56,512
心的明确定义的不动数字组成，而它的成本函数从

211
00:15:56,512 --> 00:16:01,080
来没有给它任何激励，除了对自己的决定完全有信心。

212
00:16:02,120 --> 00:16:06,020
因此，以此作为第二层神经元真正在做什么的图像，您可能想

213
00:16:06,020 --> 00:16:09,920
知道为什么我会出于拾取边缘和模式的动机而引入这个网络。

214
00:16:09,920 --> 00:16:12,300
我的意思是，这根本不是它最终要做的事情。

215
00:16:13,380 --> 00:16:17,180
嗯，这并不是我们的最终目标，而是一个起点。

216
00:16:17,640 --> 00:16:21,385
坦率地说，这是旧技术，是 80 年代和 90 

217
00:16:21,385 --> 00:16:25,620
年代研究的那种技术，你确实需要先了解它，然后才能了解

218
00:16:25,620 --> 00:16:29,854
更详细的现代变体，而且它显然能够解决一些有趣的问题，

219
00:16:29,854 --> 00:16:34,740
但你越深入了解什么那些隐藏层确实在做事，但看起来却不太智能。

220
00:16:38,480 --> 00:16:42,310
将焦点暂时从网络如何学习转移到你如何学习，只有当

221
00:16:42,310 --> 00:16:46,300
你以某种方式积极参与这里的材料时才会发生这种情况。

222
00:16:47,060 --> 00:16:51,666
我希望您做的一件非常简单的事情就是现在暂停并深入

223
00:16:51,666 --> 00:16:56,273
思考一下您可以对该系统进行哪些更改以及如果您希望

224
00:16:56,273 --> 00:17:00,880
它更好地识别边缘和图案等内容，它会如何感知图像。

225
00:17:01,479 --> 00:17:04,020
但更好的是，为了真正理解这些材料，我强烈推

226
00:17:04,020 --> 00:17:06,076
荐迈克尔·尼尔森（Michael 

227
00:17:06,076 --> 00:17:09,099
Nielsen）撰写的关于深度学习和神经网络的书。

228
00:17:09,680 --> 00:17:14,020
在其中，您可以找到要下载和使用该示例的代码和

229
00:17:14,020 --> 00:17:18,359
数据，并且本书将逐步引导您完成该代码的作用。

230
00:17:19,300 --> 00:17:23,391
很棒的是，这本书是免费且公开的，所以如果您确实

231
00:17:23,391 --> 00:17:27,660
从中有所收获，请考虑与我一起为尼尔森的努力捐款。

232
00:17:27,660 --> 00:17:31,537
我还在描述中链接了一些我非常喜欢的其他资源，包括 

233
00:17:31,537 --> 00:17:35,724
Chris Ola 的精彩博客文章和 Distill 

234
00:17:35,724 --> 00:17:36,500
中的文章。

235
00:17:38,280 --> 00:17:41,374
为了结束最后几分钟的讨论，我想跳回到我与 

236
00:17:41,374 --> 00:17:43,880
Leisha Lee 的采访片段。

237
00:17:44,300 --> 00:17:45,954
您可能还记得上一个视频中的她，

238
00:17:45,954 --> 00:17:47,720
她在深度学习方面取得了博士学位。

239
00:17:48,300 --> 00:17:51,969
在这个小片段中，她谈到了最近的两篇论文，这些论文真正

240
00:17:51,969 --> 00:17:55,780
深入探讨了一些更现代的图像识别网络实际上是如何学习的。

241
00:17:56,120 --> 00:18:00,275
为了确定我们在对话中的位置，第一篇论文采用了一个非常擅

242
00:18:00,275 --> 00:18:04,430
长图像识别的特别深层的神经网络，并且不是在正确标记的数

243
00:18:04,430 --> 00:18:08,740
据集上对其进行训练，而是在训练之前对所有标签进行了洗牌。

244
00:18:09,480 --> 00:18:13,280
显然，这里的测试准确性并不比随机测试更好，

245
00:18:13,280 --> 00:18:17,080
因为所有内容都是随机标记的，但它仍然能够达

246
00:18:17,080 --> 00:18:20,880
到与在正确标记的数据集上相同的训练准确性。

247
00:18:21,600 --> 00:18:26,400
基本上，这个特定网络的数百万个权重足以让它记住随

248
00:18:26,400 --> 00:18:31,200
机数据，这就提出了一个问题：最小化这个成本函数是

249
00:18:31,200 --> 00:18:36,400
否实际上对应于图像中的任何类型的结构，或者只是记忆？

250
00:18:51,440 --> 00:18:58,173
如果你看一下准确率曲线，如果你只是在随机数据集上进行训

251
00:18:58,173 --> 00:19:04,907
练，那么该曲线几乎以线性方式缓慢下降，所以你真的很难找

252
00:19:04,907 --> 00:19:12,140
到可能的局部最小值，你知道，正确的权重可以让您获得准确度。

253
00:19:12,240 --> 00:19:17,446
然而，如果您实际上是在结构化数据集（具有正确标签的数据集）

254
00:19:17,446 --> 00:19:22,653
上进行训练，那么一开始您会稍微调整一下，但随后您会很快下降

255
00:19:22,653 --> 00:19:27,860
到达到该准确度水平，因此从某种意义上来说更容易找到局部最大

256
00:19:27,860 --> 00:19:28,220
值。

257
00:19:28,540 --> 00:19:34,985
因此，有趣的是，它揭示了几年前的另一篇论文，该论文对网络层进

258
00:19:34,985 --> 00:19:41,429
行了更多简化，但其中一个结果是说，如果你看看优化情况，这些网

259
00:19:41,429 --> 00:19:47,875
络倾向于学习的局部最小值实际上具有相同的质量，因此从某种意义

260
00:19:47,875 --> 00:19:54,320
上来说，如果您的数据集是结构化的，您应该能够更容易地找到它。

261
00:19:58,160 --> 00:20:01,180
我一如既往地感谢那些支持 Patreon 的人。

262
00:20:01,520 --> 00:20:03,317
我之前已经说过 Patreon 

263
00:20:03,317 --> 00:20:05,901
是一个游戏规则的改变者，但如果没有您，这些视频

264
00:20:05,901 --> 00:20:06,800
真的不可能实现。

265
00:20:07,460 --> 00:20:10,181
我还要特别感谢风险投资公司 Amplify 

266
00:20:10,181 --> 00:20:12,780
Partners 对本系列初始视频的支持。

