1
00:00:04,180 --> 00:00:07,280
วิดีโอล่าสุด ฉันอธิบายโครงสร้างของโครงข่ายประสาทเทียม

2
00:00:07,680 --> 00:00:10,063
ฉันจะสรุปสั้นๆ ที่นี่เพื่อให้ความคิดของเราสดใส 

3
00:00:10,063 --> 00:00:12,600
จากนั้นฉันก็มีเป้าหมายหลักสองประการสำหรับวิดีโอนี้

4
00:00:13,100 --> 00:00:15,094
ประการแรกคือการแนะนำแนวคิดเรื่องการไล่ระดับสี 

5
00:00:15,094 --> 00:00:18,128
ซึ่งไม่เพียงแต่เป็นรากฐานของการเรียนรู้ของโครงข่ายประสาทเทียมเท่านั้น 

6
00:00:18,128 --> 00:00:20,600
แต่ยังรวมถึงการทำงานของการเรียนรู้ของเครื่องอื่นๆ อีกด้วย

7
00:00:21,120 --> 00:00:24,956
หลังจากนั้นเราจะเจาะลึกลงไปอีกหน่อยว่าเครือข่ายนี้ทำงานอย่างไร 

8
00:00:24,956 --> 00:00:27,940
และเซลล์ประสาทในชั้นที่ซ่อนอยู่เหล่านั้นมองหาอะไร

9
00:00:28,979 --> 00:00:32,599
เพื่อเป็นการเตือนความจำ เป้าหมายของเราที่นี่คือตัวอย่างคลาสสิกของ

10
00:00:32,599 --> 00:00:36,220
การรู้จำตัวเลขที่เขียนด้วยลายมือ สวัสดีโลกแห่งโครงข่ายประสาทเทียม

11
00:00:37,020 --> 00:00:40,289
ตัวเลขเหล่านี้แสดงผลบนตารางพิกเซล 28x28 พิกเซล 

12
00:00:40,289 --> 00:00:43,420
แต่ละพิกเซลมีค่าระดับสีเทาอยู่ระหว่าง 0 ถึง 1

13
00:00:43,820 --> 00:00:50,040
สิ่งเหล่านี้คือสิ่งที่กำหนดการเปิดใช้งานของเซลล์ประสาท 784 ตัวในเลเยอร์อินพุตของเครือข่าย

14
00:00:51,180 --> 00:00:56,000
จากนั้นการเปิดใช้งานของเซลล์ประสาทแต่ละอันในเลเยอร์ต่อไปนี้จะขึ้นอยู่กับผลรวมถ่วงน

15
00:00:56,000 --> 00:01:00,820
้ำหนักของการกระตุ้นทั้งหมดในเลเยอร์ก่อนหน้า บวกกับจำนวนพิเศษบางตัวที่เรียกว่าไบแอส

16
00:01:02,160 --> 00:01:05,110
จากนั้นคุณเขียนผลบวกนั้นด้วยฟังก์ชันอื่นๆ เช่น 

17
00:01:05,110 --> 00:01:08,940
ซิกมอยด์สควิชิฟิเคชั่น หรือเรลู แบบที่ผมอธิบายในวิดีโอที่แล้ว

18
00:01:09,480 --> 00:01:14,428
โดยรวมแล้ว เมื่อพิจารณาถึงการเลือกเลเยอร์ที่ซ่อนอยู่สองชั้น โดยแต่ละเลเยอร์มีเซลล์ประสาท 

19
00:01:14,428 --> 00:01:19,320
16 เซลล์ ซึ่งแต่ละชั้นมีเซลล์ประสาทให้เลือก 16 เซลล์ เครือข่ายจึงมีน้ำหนักและอคติประมาณ 

20
00:01:19,320 --> 00:01:22,934
13,000 ค่าที่เราปรับเปลี่ยนได้ และค่าเหล่านี้เองที่กำหนดว่าจริงๆ 

21
00:01:22,934 --> 00:01:24,380
แล้วเครือข่ายทำหน้าที่อะไร

22
00:01:24,880 --> 00:01:29,062
สิ่งที่เราหมายถึงเมื่อเราบอกว่าเครือข่ายนี้จำแนกตัวเลขที่กำหนดก็คือเซลล์ประ

23
00:01:29,062 --> 00:01:33,300
สาทที่สว่างที่สุดจาก 10 เซลล์ประสาทในเลเยอร์สุดท้ายนั้นสอดคล้องกับตัวเลขนั้น

24
00:01:34,100 --> 00:01:38,715
และจำไว้ว่า แรงจูงใจที่เรามีอยู่ในใจสำหรับโครงสร้างแบบเลเยอร์นี้ 

25
00:01:38,715 --> 00:01:43,686
คือบางทีชั้นที่สองอาจหยิบขึ้นมาที่ขอบ และชั้นที่สามอาจหยิบรูปแบบ เช่น 

26
00:01:43,686 --> 00:01:48,800
ลูปหรือเส้น และอันสุดท้ายก็สามารถต่อเข้าด้วยกันได้ รูปแบบในการจดจำตัวเลข

27
00:01:49,800 --> 00:01:52,240
ตรงนี้ เราจะเรียนรู้ว่าเครือข่ายเรียนรู้อย่างไร

28
00:01:52,640 --> 00:01:58,133
สิ่งที่เราต้องการคืออัลกอริธึมที่คุณสามารถแสดงข้อมูลการฝึกอบรมทั้งหมดให้กับเครือข่ายนี้ 

29
00:01:58,133 --> 00:02:01,941
ซึ่งมาในรูปแบบของรูปภาพต่างๆ ที่เป็นตัวเลขที่เขียนด้วยลายมือ 

30
00:02:01,941 --> 00:02:06,624
พร้อมด้วยป้ายกำกับสำหรับสิ่งที่พวกเขาควรจะเป็น และมันจะ ปรับน้ำหนักและอคติ 

31
00:02:06,624 --> 00:02:10,120
13,000 รายการเพื่อปรับปรุงประสิทธิภาพของข้อมูลการฝึกอบรม

32
00:02:10,720 --> 00:02:13,790
หวังว่าโครงสร้างแบบเลเยอร์นี้จะหมายความว่าสิ่งที่เรีย

33
00:02:13,790 --> 00:02:16,860
นรู้จะสรุปกับรูปภาพที่นอกเหนือจากข้อมูลการฝึกอบรมนั้น

34
00:02:17,640 --> 00:02:20,174
วิธีที่เราทดสอบก็คือ หลังจากที่คุณฝึกเครือข่าย 

35
00:02:20,174 --> 00:02:23,248
คุณจะแสดงข้อมูลที่มีป้ายกำกับมากขึ้นซึ่งไม่เคยเห็นมาก่อน 

36
00:02:23,248 --> 00:02:26,700
และคุณจะเห็นว่ามันจำแนกประเภทรูปภาพใหม่เหล่านั้นได้แม่นยำเพียงใด

37
00:02:31,120 --> 00:02:35,153
โชคดีสำหรับเรา และสิ่งที่ทำให้ตัวอย่างทั่วไปนี้เริ่มต้นได้ ก็คือคนดีๆ 

38
00:02:35,153 --> 00:02:39,475
ที่อยู่เบื้องหลังฐานข้อมูล MNIST ได้รวบรวมคอลเลกชันรูปภาพตัวเลขที่เขียนด้วย

39
00:02:39,475 --> 00:02:44,200
ลายมือจำนวนนับหมื่น ภาพ โดยแต่ละภาพจะมีป้ายกำกับด้วยตัวเลขที่พวกเขาควรจะเป็น เป็น.

40
00:02:44,900 --> 00:02:48,524
และการอธิบายเครื่องจักรว่าเป็นการเรียนรู้นั้นช่างยั่วยวนซะอีก 

41
00:02:48,524 --> 00:02:52,908
เมื่อคุณเห็นวิธีการทำงานแล้ว มันก็จะรู้สึกเหมือนกับเป็นนิยายไซไฟที่บ้าๆบอๆ 

42
00:02:52,908 --> 00:02:55,480
น้อยลงมาก และเหมือนเป็นการฝึกแคลคูลัสมากกว่า

43
00:02:56,200 --> 00:02:59,960
ฉันหมายถึง โดยพื้นฐานแล้วมันขึ้นอยู่กับการหาค่าต่ำสุดของฟังก์ชันบางอย่าง

44
00:03:01,939 --> 00:03:06,195
โปรดจำไว้ว่า ตามแนวคิดแล้ว เรากำลังคิดว่าเซลล์ประสาทแต่ละเซลล์เชื่อมต่อกับเซลล์ปร

45
00:03:06,195 --> 00:03:10,450
ะสาททั้งหมดในเลเยอร์ก่อนหน้า และน้ำหนักในผลรวมถ่วงน้ำหนักที่กำหนดการเปิดใช้งานของ

46
00:03:10,450 --> 00:03:13,496
เซลล์ประสาทนั้นก็เหมือนกับจุดแข็งของการเชื่อมต่อเหล่านั้น 

47
00:03:13,496 --> 00:03:17,751
และความลำเอียงเป็นข้อบ่งชี้บางประการของ ไม่ว่าเซลล์ประสาทนั้นมีแนวโน้มที่จะมีการใ

48
00:03:17,751 --> 00:03:18,960
ช้งานหรือไม่ใช้งานก็ตาม

49
00:03:19,720 --> 00:03:24,400
และเพื่อเริ่มต้นสิ่งต่างๆ เราจะเริ่มต้นน้ำหนักและอคติเหล่านั้นทั้งหมดโดยการสุ่มโดยสิ้นเชิง

50
00:03:24,940 --> 00:03:28,981
ไม่จำเป็นต้องพูดว่า เครือข่ายนี้จะทำงานได้ค่อนข้างน่ากลัวในตัวอย่างการฝึกอบรมที่กำหนด 

51
00:03:28,981 --> 00:03:30,720
เนื่องจากเป็นเพียงการทำบางสิ่งแบบสุ่ม

52
00:03:31,040 --> 00:03:36,020
ตัวอย่างเช่น คุณป้อนรูปภาพ 3 นี้ และเลเยอร์เอาต์พุตดูเหมือนยุ่งเหยิง

53
00:03:36,600 --> 00:03:41,179
ดังนั้นสิ่งที่คุณทำคือกำหนดฟังก์ชันต้นทุน วิธีบอกคอมพิวเตอร์ ไม่ 

54
00:03:41,179 --> 00:03:45,053
คอมพิวเตอร์เสีย ผลลัพธ์นั้นควรมีการเปิดใช้งานที่เป็น 0 

55
00:03:45,053 --> 00:03:50,760
สำหรับเซลล์ประสาทส่วนใหญ่ แต่ 1 สำหรับเซลล์ประสาทนี้ สิ่งที่คุณให้ฉันมาคือขยะสุดๆ

56
00:03:51,720 --> 00:03:54,600
หากจะกล่าวให้มากกว่านี้อีกหน่อยในทางคณิตศาสตร์ 

57
00:03:54,600 --> 00:03:59,013
คุณจะต้องบวกกำลังสองของความแตกต่างระหว่างการเปิดใช้งานเอาท์พุตขยะแต่ละรา

58
00:03:59,013 --> 00:04:03,426
ยการกับค่าที่คุณต้องการให้มี และนี่คือสิ่งที่เราจะเรียกว่าต้นทุนของตัวอย

59
00:04:03,426 --> 00:04:05,020
่างการฝึกอบรมตัวอย่างเดียว

60
00:04:05,960 --> 00:04:11,487
โปรดสังเกตว่าผลรวมนี้จะน้อยเมื่อเครือข่ายจัดประเภทรูปภาพได้อย่างถูกต้อง 

61
00:04:11,487 --> 00:04:16,399
แต่จะมีขนาดใหญ่เมื่อเครือข่ายดูเหมือนว่าไม่รู้ว่ากำลังทำอะไรอยู่

62
00:04:18,640 --> 00:04:22,006
ดังนั้นสิ่งที่คุณทำคือพิจารณาต้นทุนเฉลี่ยของตัวอย่

63
00:04:22,006 --> 00:04:25,440
างการฝึกอบรมนับหมื่นตัวอย่างทั้งหมดตามที่คุณต้องการ

64
00:04:27,040 --> 00:04:32,740
ต้นทุนเฉลี่ยนี้เป็นการวัดของเราว่าเครือข่ายแย่เพียงใด และคอมพิวเตอร์ควรรู้สึกแย่เพียงใด

65
00:04:33,420 --> 00:04:34,600
และนั่นเป็นสิ่งที่ซับซ้อน

66
00:04:35,040 --> 00:04:40,113
จำได้ไหมว่าเครือข่ายนั้นเป็นฟังก์ชันโดยพื้นฐานแล้วเครือข่ายนั้นรับตัวเลข 

67
00:04:40,113 --> 00:04:44,074
784 เป็นอินพุต ค่าพิกเซล และแยกตัวเลข 10 ตัวเป็นเอาต์พุต 

68
00:04:44,074 --> 00:04:48,800
และในแง่หนึ่งมันถูกกำหนดพารามิเตอร์ด้วยน้ำหนักและอคติเหล่านี้ทั้งหมด

69
00:04:49,500 --> 00:04:52,820
ฟังก์ชันต้นทุนนั้นเป็นชั้นของความซับซ้อนที่นอกเหนือไปจากนั้น

70
00:04:53,100 --> 00:04:57,521
โดยจะใช้น้ำหนักและอคติประมาณ 13,000 รายการเป็นข้อมูลป้อนเข้า 

71
00:04:57,521 --> 00:05:03,101
และแยกตัวเลขออกมาเพียงตัวเดียวเพื่ออธิบายว่าน้ำหนักและอคติเหล่านั้นแย่แค่ไหน 

72
00:05:03,101 --> 00:05:08,900
และวิธีการกำหนดนั้นขึ้นอยู่กับพฤติกรรมของเครือข่ายในข้อมูลการฝึกอบรมนับหมื่นชิ้น

73
00:05:09,520 --> 00:05:11,000
นั่นเป็นเรื่องที่ต้องคิดมาก

74
00:05:12,400 --> 00:05:15,820
แต่การบอกคอมพิวเตอร์ว่างานเส็งเคร็งกำลังทำอยู่นั้นไม่ได้ช่วยอะไรมากนัก

75
00:05:16,220 --> 00:05:20,060
คุณต้องการจะบอกวิธีเปลี่ยนน้ำหนักและอคติเหล่านั้นเพื่อให้ดีขึ้น

76
00:05:20,780 --> 00:05:25,310
เพื่อให้ง่ายขึ้น แทนที่จะจินตนาการถึงฟังก์ชันที่มีอินพุต 13,000 รายการ 

77
00:05:25,310 --> 00:05:30,480
ลองจินตนาการถึงฟังก์ชันง่ายๆ ที่มีตัวเลขหนึ่งเป็นอินพุตและตัวเลขหนึ่งเป็นเอาต์พุต

78
00:05:31,480 --> 00:05:35,300
คุณจะค้นหาอินพุตที่ลดค่าของฟังก์ชันนี้ให้เหลือน้อยที่สุดได้อย่างไร?

79
00:05:36,460 --> 00:05:40,596
นักเรียนแคลคูลัสจะรู้ว่าบางครั้งคุณสามารถคิดค่าขั้นต่ำนั้นได้อย่างชัดเจน 

80
00:05:40,596 --> 00:05:45,640
แต่นั่นก็เป็นไปไม่ได้เสมอไปสำหรับฟังก์ชันที่ซับซ้อนจริงๆ แน่นอนว่าไม่ใช่ในเวอร์ชันอินพุต 

81
00:05:45,640 --> 00:05:50,513
13,000 รายการของสถานการณ์นี้สำหรับฟังก์ชันต้นทุนโครงข่ายประสาทเทียมที่ซับซ้อนอย่างบ้าค

82
00:05:50,513 --> 00:05:51,080
ลั่งของเรา

83
00:05:51,580 --> 00:05:55,390
กลยุทธ์ที่ยืดหยุ่นกว่าคือเริ่มจากอินพุตใดๆ และห

84
00:05:55,390 --> 00:05:59,200
าทิศทางที่คุณควรก้าวเพื่อทำให้เอาต์พุตนั้นต่ำลง

85
00:06:00,080 --> 00:06:04,588
โดยเฉพาะอย่างยิ่ง หากคุณสามารถหาความชันของฟังก์ชันในตำแหน่งที่คุณอยู่ได้ 

86
00:06:04,588 --> 00:06:09,900
ให้เลื่อนไปทางซ้ายหากความชันนั้นเป็นบวก และเลื่อนค่าอินพุตไปทางขวาหากความชันนั้นเป็นลบ

87
00:06:11,960 --> 00:06:17,409
หากคุณทำเช่นนี้ซ้ำๆ ในแต่ละจุดตรวจสอบความชันใหม่และทำตามขั้นตอนที่เหมาะสม 

88
00:06:17,409 --> 00:06:19,840
คุณจะเข้าใกล้ค่าต่ำสุดของฟังก์ชัน

89
00:06:20,640 --> 00:06:23,800
ภาพที่คุณอาจนึกถึงที่นี่คือลูกบอลกลิ้งลงมาจากเนินเขา

90
00:06:24,620 --> 00:06:27,891
โปรดสังเกตว่า แม้ว่าฟังก์ชันอินพุตเดี่ยวที่เรียบง่ายนี้ 

91
00:06:27,891 --> 00:06:32,973
ยังมีหุบเขาที่เป็นไปได้มากมายที่คุณอาจเข้าไปได้ ขึ้นอยู่กับอินพุตแบบสุ่มที่คุณเริ่มต้น 

92
00:06:32,973 --> 00:06:37,881
และไม่มีการรับประกันว่าค่าต่ำสุดในพื้นที่ที่คุณไปถึงจะเป็นค่าที่เล็กที่สุดเท่าที่จะเ

93
00:06:37,881 --> 00:06:39,400
ป็นไปได้ ของฟังก์ชันต้นทุน

94
00:06:40,220 --> 00:06:42,620
นั่นจะส่งต่อไปยังกรณีโครงข่ายประสาทเทียมของเราเช่นกัน

95
00:06:43,180 --> 00:06:48,046
ฉันยังอยากให้คุณสังเกตด้วยว่าหากคุณทำให้ขนาดขั้นบันไดเป็นสัดส่วนกับความชัน 

96
00:06:48,046 --> 00:06:52,199
แล้วเมื่อความชันแบนราบไปทางขั้นต่ำ ขั้นตอนของคุณจะเล็กลงเรื่อยๆ 

97
00:06:52,199 --> 00:06:54,600
และนั่นจะช่วยคุณจากการถ่ายภาพเกินขนาด

98
00:06:55,940 --> 00:07:00,980
เพิ่มความซับซ้อนขึ้นเล็กน้อย ลองจินตนาการถึงฟังก์ชันที่มีสองอินพุตและหนึ่งเอาต์พุตแทน

99
00:07:01,500 --> 00:07:08,140
คุณอาจคิดว่าพื้นที่อินพุตเป็นระนาบ xy และฟังก์ชันต้นทุนถูกวาดเป็นกราฟเป็นพื้นผิวด้านบน

100
00:07:08,760 --> 00:07:13,860
แทนที่จะถามเกี่ยวกับความชันของฟังก์ชัน คุณต้องถามว่าคุณควรก้าวไปใน

101
00:07:13,860 --> 00:07:18,960
ทิศทางใดในพื้นที่อินพุตนี้ เพื่อลดเอาต์พุตของฟังก์ชันให้เร็วที่สุด

102
00:07:19,720 --> 00:07:21,760
กล่าวอีกนัยหนึ่ง ทิศทางลงเขาคืออะไร?

103
00:07:22,380 --> 00:07:25,560
ขอย้ำอีกครั้งว่าการนึกถึงลูกบอลกลิ้งลงมาตามเนินเขานั้นก็เป็นประโยชน์

104
00:07:26,660 --> 00:07:32,720
พวกคุณที่คุ้นเคยกับแคลคูลัสหลายตัวแปรจะรู้ว่าความชันของฟังก์ชันช่วยให้คุณกำหนดทิศทา

105
00:07:32,720 --> 00:07:38,780
งของการขึ้นที่สูงที่สุดได้ และคุณควรก้าวไปในทิศทางใดเพื่อเพิ่มฟังก์ชันให้เร็วที่สุด

106
00:07:39,560 --> 00:07:42,767
โดยธรรมชาติแล้ว การหาค่าลบของการไล่ระดับสีนั้นจะทำ

107
00:07:42,767 --> 00:07:46,040
ให้คุณมีทิศทางในการก้าวที่ลดฟังก์ชันลงได้เร็วที่สุด

108
00:07:47,240 --> 00:07:50,539
ยิ่งไปกว่านั้น ความยาวของเวกเตอร์เกรเดียนต์นี้

109
00:07:50,539 --> 00:07:53,840
ยังเป็นตัวบ่งชี้ความชันของความชันสูงสุดอีกด้วย

110
00:07:54,540 --> 00:07:57,708
หากคุณไม่คุ้นเคยกับแคลคูลัสหลายตัวแปรและต้องการเรียนรู้เพิ่มเติม 

111
00:07:57,708 --> 00:08:00,340
ลองดูงานบางส่วนที่ฉันทำให้กับ Khan Academy ในหัวข้อนี้

112
00:08:00,860 --> 00:08:05,792
จริงๆ แล้ว สิ่งที่สำคัญสำหรับคุณและฉันตอนนี้คือ โดยหลักการแล้ว 

113
00:08:05,792 --> 00:08:11,900
มีวิธีคำนวณเวกเตอร์นี้ เวกเตอร์นี้ที่บอกคุณว่าทิศทางลงเนินคืออะไร และชันแค่ไหน

114
00:08:12,400 --> 00:08:16,120
คุณจะไม่เป็นไรถ้านั่นคือทั้งหมดที่คุณรู้และคุณไม่ใส่ใจรายละเอียดมากนัก

115
00:08:17,200 --> 00:08:22,972
หากคุณเข้าใจได้ อัลกอริธึมในการลดฟังก์ชันคือการคำนวณทิศทางการไล่ระดับสี 

116
00:08:22,972 --> 00:08:26,740
จากนั้นก้าวลงเนินเล็กน้อย และทำซ้ำซ้ำแล้วซ้ำอีก

117
00:08:27,700 --> 00:08:32,820
เป็นแนวคิดพื้นฐานเดียวกันสำหรับฟังก์ชันที่มีอินพุต 13,000 อินพุตแทนที่จะเป็น 2 อินพุต

118
00:08:33,400 --> 00:08:36,399
ลองนึกภาพการจัดน้ำหนักและอคติทั้งหมด 13,000 รายกา

119
00:08:36,399 --> 00:08:39,460
รของเครือข่ายของเราให้เป็นเวกเตอร์คอลัมน์ขนาดยักษ์

120
00:08:40,140 --> 00:08:44,435
เกรเดียนต์เชิงลบของฟังก์ชันต้นทุนเป็นเพียงเวกเตอร์ 

121
00:08:44,435 --> 00:08:48,731
มันเป็นทิศทางบางอย่างภายในพื้นที่อินพุตขนาดใหญ่มาก 

122
00:08:48,731 --> 00:08:54,880
ที่บอกคุณว่าการขยับตัวเลขใดที่จะทำให้ฟังก์ชันต้นทุนลดลงอย่างรวดเร็วที่สุด

123
00:08:55,640 --> 00:08:59,046
และแน่นอนว่าด้วยฟังก์ชันต้นทุนที่ออกแบบมาเป็นพิเศษของเรา 

124
00:08:59,046 --> 00:09:04,066
การเปลี่ยนน้ำหนักและความลำเอียงเพื่อลด หมายถึงการทำให้เอาต์พุตของเครือข่ายในข้อมูลกา

125
00:09:04,066 --> 00:09:07,353
รฝึกแต่ละชิ้นดูเหมือนอาร์เรย์สุ่มที่มีค่า 10 ค่าน้อยลง 

126
00:09:07,353 --> 00:09:10,820
และเหมือนกับการตัดสินใจจริงที่เราต้องการมากกว่า มันจะทำให้

127
00:09:11,440 --> 00:09:16,486
สิ่งสำคัญที่ต้องจำก็คือ ฟังก์ชันต้นทุนนี้เกี่ยวข้องกับค่าเฉลี่ยของข้อมูลการฝึกทั้งหมด 

128
00:09:16,486 --> 00:09:21,180
ดังนั้น หากคุณย่อให้เล็กสุด ก็หมายความว่าประสิทธิภาพดีขึ้นในกลุ่มตัวอย่างทั้งหมด

129
00:09:23,820 --> 00:09:27,150
อัลกอริธึมสำหรับการคำนวณการไล่ระดับสีนี้อย่างมีประสิทธิภาพ 

130
00:09:27,150 --> 00:09:31,722
ซึ่งเป็นหัวใจสำคัญของการเรียนรู้ของโครงข่ายประสาทเทียม เรียกว่าการแพร่กระจายกลับ 

131
00:09:31,722 --> 00:09:33,980
และนั่นคือสิ่งที่ฉันจะพูดถึงในวิดีโอหน้า

132
00:09:34,660 --> 00:09:38,785
ที่นั่น ฉันอยากจะใช้เวลาศึกษาสิ่งที่เกิดขึ้นกับน้ำหนักและอคติแต่

133
00:09:38,785 --> 00:09:42,910
ละอย่างสำหรับข้อมูลการฝึกแต่ละชิ้น โดยพยายามให้ความรู้สึกตามสัญช

134
00:09:42,910 --> 00:09:47,100
าตญาณว่าเกิดอะไรขึ้นนอกเหนือจากแคลคูลัสและสูตรที่เกี่ยวข้องมากมาย

135
00:09:47,780 --> 00:09:52,482
ที่นี่ ตอนนี้ สิ่งสำคัญที่ฉันอยากให้คุณรู้ โดยไม่ขึ้นอยู่กับรายละเอียดการใช้งาน 

136
00:09:52,482 --> 00:09:56,008
คือสิ่งที่เราหมายถึงเมื่อเราพูดถึงการเรียนรู้เครือข่ายก็คือ 

137
00:09:56,008 --> 00:09:58,360
มันแค่ลดฟังก์ชันต้นทุนให้เหลือน้อยที่สุด

138
00:09:59,300 --> 00:10:03,700
และสังเกตว่า ผลที่ตามมาประการหนึ่งก็คือ สิ่งสำคัญคือฟังก์ชันต้นทุนนี้จะต้องมีผล

139
00:10:03,700 --> 00:10:08,100
ลัพธ์ที่ราบรื่น เพื่อที่เราจะได้หาค่าต่ำสุดในพื้นที่ได้โดยการก้าวลงเนินเล็กน้อย

140
00:10:09,260 --> 00:10:13,332
ด้วยเหตุนี้เอง เซลล์ประสาทเทียมจึงมีการกระตุ้นอย่างต่อเนื่อง 

141
00:10:13,332 --> 00:10:19,140
แทนที่จะเป็นเพียงการทำงานหรือไม่ใช้งานในลักษณะไบนารี เหมือนที่เซลล์ประสาทชีวภาพเป็นอยู่

142
00:10:20,220 --> 00:10:23,490
กระบวนการดันอินพุตของฟังก์ชันซ้ำๆ โดยการทวีคูณข

143
00:10:23,490 --> 00:10:26,760
องการไล่ระดับสีเชิงลบนี้เรียกว่าการไล่ระดับสีลง

144
00:10:27,300 --> 00:10:30,694
เป็นวิธีหนึ่งที่จะมาบรรจบกันกับฟังก์ชันต้นทุนขั้นต่ำในท้องถิ่น 

145
00:10:30,694 --> 00:10:32,580
โดยพื้นฐานแล้วจะเป็นหุบเขาในกราฟนี้

146
00:10:33,440 --> 00:10:36,670
แน่นอนว่าฉันยังคงแสดงรูปภาพของฟังก์ชันที่มีอินพุตสองอินพุตอยู่ 

147
00:10:36,670 --> 00:10:41,131
เนื่องจากการขยับในพื้นที่อินพุตขนาด 13,000 มิตินั้นอาจเป็นเรื่องยากเล็กน้อยที่จะเข้าใจ 

148
00:10:41,131 --> 00:10:44,260
แต่มีวิธีคิดที่ไม่เกี่ยวกับเชิงพื้นที่ที่ดีเกี่ยวกับเรื่องนี้

149
00:10:45,080 --> 00:10:48,440
แต่ละองค์ประกอบของเกรเดียนต์เชิงลบบอกเราสองอย่าง

150
00:10:49,060 --> 00:10:55,140
แน่นอนว่าเครื่องหมายบอกเราว่าองค์ประกอบที่สอดคล้องกันของเวกเตอร์อินพุตควรถูกดันขึ้นหรือลง

151
00:10:55,800 --> 00:11:02,720
แต่ที่สำคัญ ขนาดสัมพัทธ์ของส่วนประกอบทั้งหมดนี้ จะบอกคุณได้ว่าการเปลี่ยนแปลงใดสำคัญกว่ากัน

152
00:11:05,220 --> 00:11:09,130
คุณจะเห็นว่าในเครือข่ายของเรา การปรับเปลี่ยนน้ำหนักอย่างใดอย่างหนึ่

153
00:11:09,130 --> 00:11:13,040
งอาจมีผลกระทบต่อฟังก์ชันต้นทุนมากกว่าการปรับเปลี่ยนน้ำหนักอื่นๆ มาก

154
00:11:14,800 --> 00:11:18,200
การเชื่อมต่อบางส่วนเหล่านี้มีความสำคัญต่อข้อมูลการฝึกอบรมของเรามากกว่า

155
00:11:19,320 --> 00:11:25,176
วิธีหนึ่งที่คุณสามารถคิดถึงเวกเตอร์เกรเดียนต์ของฟังก์ชันต้นทุนมหาศาลที่บิดเบือนความคิดได้ 

156
00:11:25,176 --> 00:11:29,406
ก็คือมันเข้ารหัสความสำคัญสัมพัทธ์ ของแต่ละน้ำหนักและอคติ นั่นคือ 

157
00:11:29,406 --> 00:11:32,400
การเปลี่ยนแปลงใดที่จะทำให้คุณเสียเงินมากที่สุด

158
00:11:33,620 --> 00:11:36,640
นี่เป็นเพียงวิธีคิดเกี่ยวกับทิศทางอีกวิธีหนึ่ง

159
00:11:37,100 --> 00:11:42,572
เพื่อยกตัวอย่างที่ง่ายกว่านี้ หากคุณมีฟังก์ชันบางอย่างที่มีตัวแปรสองตัวเป็นอินพุต 

160
00:11:42,572 --> 00:11:47,177
และคุณคำนวณว่าการไล่ระดับสีที่จุดใดจุดหนึ่งออกมาเป็น 3,1 ในด้านหนึ่ง 

161
00:11:47,177 --> 00:11:51,982
คุณสามารถตีความสิ่งนั้นได้ว่าเป็นการบอกว่าเมื่อคุณ ยืนอยู่ที่อินพุตนั้น 

162
00:11:51,982 --> 00:11:55,519
การเคลื่อนไปตามทิศทางนี้จะเพิ่มฟังก์ชันให้เร็วที่สุด 

163
00:11:55,519 --> 00:11:58,923
คือเมื่อคุณสร้างกราฟฟังก์ชันเหนือระนาบของจุดอินพุต 

164
00:11:58,923 --> 00:12:02,260
เวกเตอร์นั้นคือสิ่งที่ให้ทิศทางขึ้นเนินเป็นเส้นตรง

165
00:12:02,860 --> 00:12:07,928
แต่วิธีอ่านอีกวิธีหนึ่งคือบอกว่าการเปลี่ยนแปลงตัวแปรแรกนี้มีความสำคัญเป็น 

166
00:12:07,928 --> 00:12:14,092
3 เท่าของการเปลี่ยนแปลงตัวแปรตัวที่สอง อย่างน้อยก็ในบริเวณใกล้เคียงกับอินพุตที่เกี่ยวข้อง 

167
00:12:14,092 --> 00:12:16,900
การดันค่า x จะทำให้คุณปังมากขึ้น เจ้าชู้.

168
00:12:19,880 --> 00:12:22,340
ลองซูมออกและสรุปว่าเราอยู่ไกลถึงไหนแล้ว

169
00:12:22,840 --> 00:12:26,806
ตัวเครือข่ายเองเป็นฟังก์ชันนี้ซึ่งมีอินพุต 784 รายการและเอาต์พุต 

170
00:12:26,806 --> 00:12:30,040
10 รายการ ซึ่งกำหนดในรูปของผลรวมถ่วงน้ำหนักทั้งหมดนี้

171
00:12:30,640 --> 00:12:33,680
ฟังก์ชันต้นทุนยังเป็นชั้นของความซับซ้อนที่นอกเหนือไปจากนั้น

172
00:12:33,980 --> 00:12:37,695
ใช้น้ำหนักและอคติ 13,000 รายการเป็นข้อมูลนำเข้า 

173
00:12:37,695 --> 00:12:41,720
และแยกความเลวออกมาเพียงค่าเดียวตามตัวอย่างการฝึกอบรม

174
00:12:42,440 --> 00:12:46,900
และการไล่ระดับสีของฟังก์ชันต้นทุนก็ยังมีความซับซ้อนอีกชั้นหนึ่ง

175
00:12:47,360 --> 00:12:52,620
โดยบอกเราว่าการเปลี่ยนแปลงน้ำหนักและอคติใดที่ทำให้เกิดการเปลี่ยนแปลงค่าของฟังก์ชันต้น

176
00:12:52,620 --> 00:12:57,880
ทุนได้เร็วที่สุด ซึ่งคุณอาจตีความได้ว่าการเปลี่ยนแปลงใดที่น้ำหนักมีความสำคัญมากที่สุด

177
00:13:02,560 --> 00:13:06,717
ดังนั้น เมื่อคุณเริ่มต้นเครือข่ายด้วยน้ำหนักและอคติแบบสุ่ม 

178
00:13:06,717 --> 00:13:12,002
และปรับมันหลายครั้งตามกระบวนการไล่ระดับสีนี้ มันจะทำงานได้ดีเพียงใดกับภาพที

179
00:13:12,002 --> 00:13:13,200
่ไม่เคยเห็นมาก่อน

180
00:13:14,100 --> 00:13:17,715
สิ่งที่ฉันได้อธิบายไว้ที่นี่ ซึ่งมีสองชั้นที่ซ่อนอยู่ 16 

181
00:13:17,715 --> 00:13:21,900
เซลล์ประสาทแต่ละชั้น ซึ่งส่วนใหญ่เลือกด้วยเหตุผลด้านสุนทรียศาสตร์ 

182
00:13:21,900 --> 00:13:25,960
ถือว่าไม่แย่ โดยจำแนกประมาณ 96% ของภาพใหม่ที่เห็นได้อย่างถูกต้อง

183
00:13:26,680 --> 00:13:30,010
และโดยสุจริต หากคุณดูตัวอย่างบางส่วนที่ทำให้เกิดปัญหา 

184
00:13:30,010 --> 00:13:32,540
คุณจะรู้สึกว่าจำเป็นต้องลดหย่อนลงเล็กน้อย

185
00:13:36,220 --> 00:13:40,092
ตอนนี้ถ้าคุณลองใช้โครงสร้างเลเยอร์ที่ซ่อนอยู่และปรับแต่งเล็กน้อย 

186
00:13:40,092 --> 00:13:41,760
คุณจะได้รับสิ่งนี้มากถึง 98%

187
00:13:41,760 --> 00:13:42,720
และนั่นก็ค่อนข้างดี!

188
00:13:43,020 --> 00:13:47,590
ไม่ใช่สิ่งที่ดีที่สุด คุณสามารถได้รับประสิทธิภาพที่ดีขึ้นอย่างแน่นอนโดยมีความ

189
00:13:47,590 --> 00:13:52,160
ซับซ้อนมากขึ้นกว่าเครือข่ายวานิลลาธรรมดานี้ แต่เมื่อพิจารณาว่างานเริ่มแรกนั้น

190
00:13:52,160 --> 00:13:56,137
น่ากลัวเพียงใด ฉันคิดว่ามีบางอย่างที่น่าทึ่งเกี่ยวกับเครือข่ายใด ๆ 

191
00:13:56,137 --> 00:14:01,420
ที่ทำได้ดีกับภาพที่ไม่เคยเห็นมาก่อน เนื่องจาก เราไม่เคยบอกเจาะจงเจาะจงว่าควรมองหารูปแบบใด

192
00:14:02,560 --> 00:14:06,617
เดิมที วิธีที่ฉันกระตุ้นโครงสร้างนี้คือการอธิบายความหวังที่เราอาจมี 

193
00:14:06,617 --> 00:14:11,451
ว่าชั้นที่สองอาจหยิบยกขึ้นมาจากขอบเล็กๆ ว่าชั้นที่สามจะปะติดปะต่อขอบเหล่านั้นเข้า

194
00:14:11,451 --> 00:14:15,867
ด้วยกันเพื่อรับรู้ถึงลูปและเส้นที่ยาวกว่า และเหล่านั้นอาจถูกปะติดปะต่อกัน 

195
00:14:15,867 --> 00:14:17,180
ร่วมกันเพื่อจดจำตัวเลข

196
00:14:17,960 --> 00:14:20,400
นี่คือสิ่งที่เครือข่ายของเรากำลังทำอยู่จริงหรือ?

197
00:14:21,079 --> 00:14:24,400
อย่างน้อยที่สุดก็ไม่ใช่เลยสำหรับอันนี้

198
00:14:24,820 --> 00:14:28,880
จำได้ไหมว่าวิดีโอล่าสุดที่เราดูว่าน้ำหนักของการเชื่อมต่อจากเซลล์ประสา

199
00:14:28,880 --> 00:14:32,940
ททั้งหมดในชั้นแรกไปยังเซลล์ประสาทที่กำหนดในชั้นที่สองนั้นสามารถมองเห็

200
00:14:32,940 --> 00:14:37,060
นเป็นรูปแบบพิกเซลที่กำหนดซึ่งเซลล์ประสาทชั้นที่สองหยิบขึ้นมาได้อย่างไร

201
00:14:37,780 --> 00:14:42,781
เมื่อเราทำเช่นนั้นกับน้ำหนักที่เกี่ยวข้องกับการเปลี่ยนผ่านเหล่านี้ 

202
00:14:42,781 --> 00:14:48,529
จากชั้นแรกไปยังชั้นถัดไป แทนที่จะเก็บบนขอบเล็กๆ ที่แยกจากกันตรงนี้และตรงนั้น 

203
00:14:48,529 --> 00:14:53,680
พวกมันดูเกือบจะสุ่มเลย มีเพียงรูปแบบที่หลวมๆ บางส่วนใน ตรงกลางตรงนั้น

204
00:14:53,760 --> 00:14:58,803
ดูเหมือนว่าในพื้นที่ขนาดใหญ่ถึง 13,000 มิติของน้ำหนักและอคติที่เป็นไปได้ 

205
00:14:58,803 --> 00:15:03,225
เครือข่ายของเราพบว่าตัวเองมีเกณฑ์ขั้นต่ำในท้องถิ่นที่น่าพึงพอใจ 

206
00:15:03,225 --> 00:15:08,960
ซึ่งแม้จะจำแนกภาพส่วนใหญ่ได้สำเร็จ แต่ก็ยังไม่สามารถเข้าใจรูปแบบที่เราคาดหวังไว้ได้

207
00:15:09,780 --> 00:15:13,820
และเพื่อขับเคลื่อนจุดนี้กลับบ้านจริงๆ ให้ดูสิ่งที่เกิดขึ้นเมื่อคุณป้อนภาพแบบสุ่ม

208
00:15:14,320 --> 00:15:17,862
หากระบบฉลาด คุณอาจคาดหวังว่าระบบจะรู้สึกไม่แน่นอน 

209
00:15:17,862 --> 00:15:22,468
อาจจะไม่ได้เปิดใช้งานเซลล์ประสาทเอาท์พุตใด ๆ จาก 10 ตัวนั้นจริงๆ 

210
00:15:22,468 --> 00:15:28,208
หรือเปิดใช้งานพวกมันทั้งหมดเท่า ๆ กัน แต่กลับให้คำตอบที่ไร้สาระแก่คุณอย่างมั่นใจ 

211
00:15:28,208 --> 00:15:34,160
ราวกับว่ามันรู้สึกแน่ใจว่าสัญญาณรบกวนแบบสุ่มนี้ คือ 5 เหมือนกับที่ภาพจริงของ 5 คือ 5

212
00:15:34,540 --> 00:15:38,895
ใช้ถ้อยคำแตกต่างออกไป แม้ว่าเครือข่ายนี้สามารถจดจำตัวเลขได้ค่อนข้างดี 

213
00:15:38,895 --> 00:15:40,700
แต่ก็ไม่รู้ว่าจะวาดมันอย่างไร

214
00:15:41,420 --> 00:15:45,240
สาเหตุส่วนใหญ่มาจากการจัดเตรียมการฝึกอบรมที่มีข้อจำกัดอย่างเข้มงวด

215
00:15:45,880 --> 00:15:47,740
ฉันหมายถึง ใส่ตัวเองเข้าไปอยู่ในบทบาทของเครือข่ายที่นี่

216
00:15:48,140 --> 00:15:52,453
จากมุมมองของมัน จักรวาลทั้งหมดไม่มีอะไรนอกจากตัวเลขที่ไม่เคลื่อนไหวที่กำห

217
00:15:52,453 --> 00:15:55,348
นดไว้อย่างชัดเจนซึ่งมีศูนย์กลางอยู่ในตารางเล็ก ๆ 

218
00:15:55,348 --> 00:15:58,125
และฟังก์ชันต้นทุนของมันก็ไม่เคยให้แรงจูงใจใด ๆ 

219
00:15:58,125 --> 00:16:01,080
ที่จะเป็นอะไรนอกจากมั่นใจอย่างเต็มที่ในการตัดสินใจ

220
00:16:02,120 --> 00:16:05,803
ดังนั้น ด้วยภาพนี้ว่าจริงๆ แล้วเซลล์ประสาทชั้นที่สองกำลังทำอะไรอยู่ 

221
00:16:05,803 --> 00:16:09,920
คุณอาจสงสัยว่าทำไมฉันถึงแนะนำเครือข่ายนี้ ด้วยแรงจูงใจในการเลือกขอบและรูปแบบ

222
00:16:09,920 --> 00:16:12,300
ฉันหมายความว่านั่นไม่ใช่สิ่งที่ท้ายที่สุดแล้ว

223
00:16:13,380 --> 00:16:17,180
นี่ไม่ใช่เป้าหมายสุดท้ายของเรา แต่เป็นจุดเริ่มต้นแทน

224
00:16:17,640 --> 00:16:21,780
จริงๆ แล้ว นี่เป็นเทคโนโลยีเก่า ซึ่งเป็นเทคโนโลยีที่ได้รับการค้นคว้าในยุค 80 

225
00:16:21,780 --> 00:16:26,620
และ 90 และคุณจำเป็นต้องเข้าใจก่อนจึงจะสามารถเข้าใจตัวแปรสมัยใหม่ที่มีรายละเอียดมากขึ้นได้ 

226
00:16:26,620 --> 00:16:31,083
และเห็นได้ชัดว่าสามารถแก้ไขปัญหาที่น่าสนใจบางอย่างได้ แต่ยิ่งคุณเจาะลึกลงไปถึงอะไร 

227
00:16:31,083 --> 00:16:34,740
เลเยอร์ที่ซ่อนอยู่เหล่านั้นกำลังทำอยู่จริงๆ ยิ่งดูฉลาดน้อยลงเท่านั้น

228
00:16:38,480 --> 00:16:42,293
การเปลี่ยนโฟกัสไปชั่วขณะจากวิธีที่เครือข่ายเรียนรู้ไปเป็นวิธีการเรียนรู้ของคุณ 

229
00:16:42,293 --> 00:16:46,300
ซึ่งจะเกิดขึ้นก็ต่อเมื่อคุณมีส่วนร่วมอย่างแข็งขันกับเนื้อหาที่นี่ไม่ทางใดก็ทางหนึ่ง

230
00:16:47,060 --> 00:16:51,643
สิ่งง่ายๆ อย่างหนึ่งที่ฉันอยากให้คุณทำคือหยุดตอนนี้และคิดให้ลึกซึ้ง

231
00:16:51,643 --> 00:16:56,843
สักครู่เกี่ยวกับการเปลี่ยนแปลงที่คุณอาจทำกับระบบนี้ และวิธีที่ระบบรับรู้ภาพ 

232
00:16:56,843 --> 00:17:00,880
หากคุณต้องการให้ระบบรับสิ่งต่างๆ เช่น ขอบและลวดลายได้ดีขึ้น

233
00:17:01,479 --> 00:17:05,189
แต่ที่ดีกว่านั้น หากต้องการมีส่วนร่วมกับเนื้อหาจริงๆ ฉันขอแนะนำหนังสือของ 

234
00:17:05,189 --> 00:17:09,099
Michael Nielsen เกี่ยวกับการเรียนรู้เชิงลึกและโครงข่ายประสาทเทียมเป็นอย่างยิ่ง

235
00:17:09,680 --> 00:17:14,611
ในนั้น คุณจะพบโค้ดและข้อมูลที่จะดาวน์โหลดและเล่นสำหรับตัวอย่างที่ชัดเจนนี้ 

236
00:17:14,611 --> 00:17:18,359
และหนังสือจะแนะนำคุณทีละขั้นตอนว่าโค้ดนั้นกำลังทำอะไรอยู่

237
00:17:19,300 --> 00:17:22,814
สิ่งที่ยอดเยี่ยมคือหนังสือเล่มนี้ให้บริการฟรีและเผยแพร่ต่อสาธารณะ 

238
00:17:22,814 --> 00:17:26,914
ดังนั้นหากคุณได้ประโยชน์จากหนังสือเล่มนี้ ลองมาร่วมบริจาคให้กับความพยายามของ 

239
00:17:26,914 --> 00:17:27,660
Nielsen กับฉัน

240
00:17:27,660 --> 00:17:31,985
ฉันยังได้เชื่อมโยงแหล่งข้อมูลอื่นๆ สองสามอย่างที่ฉันชอบมากในคำอธิบาย 

241
00:17:31,985 --> 00:17:36,500
รวมถึงบล็อกโพสต์ที่สวยงามและน่าอัศจรรย์โดย Chris Ola และบทความใน Distill

242
00:17:38,280 --> 00:17:41,080
เพื่อปิดประเด็นในช่วงไม่กี่นาทีที่ผ่านมา ฉันอยากจะ

243
00:17:41,080 --> 00:17:43,880
ย้อนกลับไปดูตัวอย่างบทสัมภาษณ์ที่ฉันมีกับเลอิชา ลี

244
00:17:44,300 --> 00:17:47,720
คุณอาจจำเธอได้จากวิดีโอที่แล้ว เธอทำงานระดับปริญญาเอกด้านการเรียนรู้เชิงลึก

245
00:17:48,300 --> 00:17:52,012
ในตัวอย่างเล็กๆ น้อยๆ นี้ เธอพูดถึงเอกสารสองฉบับล่าสุดที่เจาะลึกว่าเ

246
00:17:52,012 --> 00:17:55,780
ครือข่ายการจดจำรูปภาพที่ทันสมัยกว่าบางส่วนกำลังเรียนรู้จริง ๆ อย่างไร

247
00:17:56,120 --> 00:18:00,308
เพื่อกำหนดจุดที่เราอยู่ในการสนทนา รายงานฉบับแรกได้นำหนึ่งในโครงข่ายประสาทเท

248
00:18:00,308 --> 00:18:03,714
ียมระดับลึกพิเศษเหล่านี้ซึ่งมีความสามารถในการจดจำภาพได้ดีมาก 

249
00:18:03,714 --> 00:18:08,740
และแทนที่จะฝึกกับชุดข้อมูลที่มีป้ายกำกับอย่างถูกต้อง กลับสับป้ายกำกับทั้งหมดก่อนการฝึกอบรม

250
00:18:09,480 --> 00:18:12,778
แน่นอนว่าความแม่นยำในการทดสอบที่นี่ไม่ได้ดีไปกว่าการสุ่ม 

251
00:18:12,778 --> 00:18:16,539
เนื่องจากทุกอย่างเป็นเพียงป้ายกำกับแบบสุ่ม แต่ก็ยังสามารถบรรลุควา

252
00:18:16,539 --> 00:18:20,880
มแม่นยำในการฝึกอบรมแบบเดียวกับที่คุณทำในชุดข้อมูลที่มีป้ายกำกับอย่างถูกต้อง

253
00:18:21,600 --> 00:18:26,875
โดยพื้นฐานแล้ว น้ำหนักนับล้านสำหรับเครือข่ายนี้เพียงพอที่จะจดจำข้อมูลแบบสุ่ม 

254
00:18:26,875 --> 00:18:31,809
ซึ่งทำให้เกิดคำถามว่าการลดฟังก์ชันต้นทุนนี้ให้เหลือน้อยที่สุดนั้นสอดคล้อ

255
00:18:31,809 --> 00:18:36,400
งกับโครงสร้างประเภทใด ๆ ในภาพหรือไม่ หรือเป็นเพียงการท่องจำเท่านั้น

256
00:18:51,440 --> 00:18:59,452
หากคุณดูกราฟความแม่นยำนั้น หากคุณแค่ฝึกกับชุดข้อมูลสุ่ม เส้นโค้งนั้นจะลดลงอย่างช้าๆ 

257
00:18:59,452 --> 00:19:06,893
ในลักษณะเชิงเส้นตรง ดังนั้นคุณจึงลำบากมากที่จะหาค่าต่ำสุดเฉพาะจุดที่เป็นไปได้ 

258
00:19:06,893 --> 00:19:12,140
ตุ้มน้ำหนักที่เหมาะสมซึ่งจะทำให้คุณได้รับความแม่นยำนั้น

259
00:19:12,240 --> 00:19:15,693
ในขณะที่หากคุณกำลังฝึกชุดข้อมูลที่มีโครงสร้างจริงๆ 

260
00:19:15,693 --> 00:19:20,771
ซึ่งเป็นชุดข้อมูลที่มีป้ายกำกับที่ถูกต้อง คุณจะลังเลเล็กน้อยในช่วงเริ่มต้น 

261
00:19:20,771 --> 00:19:25,646
แต่แล้วคุณก็ลดลงอย่างรวดเร็วเพื่อไปถึงระดับความแม่นยำนั้น และในแง่หนึ่ง 

262
00:19:25,646 --> 00:19:28,220
จะหาจุดสูงสุดในท้องถิ่นนั้นได้ง่ายกว่า

263
00:19:28,540 --> 00:19:33,657
ดังนั้นสิ่งที่น่าสนใจเกี่ยวกับเรื่องนี้ก็คือ ได้มีการนำเสนอรายงานอีกฉบับหนึ่งจา

264
00:19:33,657 --> 00:19:38,515
กสองสามปีที่แล้ว ซึ่งมีการลดความซับซ้อนมากขึ้นมากเกี่ยวกับเลเยอร์เครือข่าย 

265
00:19:38,515 --> 00:19:42,660
แต่ผลลัพธ์ประการหนึ่งก็คือ ถ้าคุณดูภาพรวมของการเพิ่มประสิทธิภาพ 

266
00:19:42,660 --> 00:19:47,777
ค่าต่ำสุดเฉพาะที่เครือข่ายเหล่านี้มีแนวโน้มที่จะเรียนรู้นั้นแท้จริงแล้วมีคุณภาพ

267
00:19:47,777 --> 00:19:51,729
เท่าเทียมกัน ดังนั้น ในแง่หนึ่งหากชุดข้อมูลของคุณมีโครงสร้าง 

268
00:19:51,729 --> 00:19:54,320
คุณก็จะสามารถค้นหาสิ่งนั้นได้ง่ายกว่ามาก

269
00:19:58,160 --> 00:20:01,180
ฉันขอขอบคุณผู้ที่สนับสนุน Patreon เช่นเคย

270
00:20:01,520 --> 00:20:06,800
ฉันเคยพูดไปแล้วว่า Patreon ผู้เปลี่ยนเกมคืออะไร แต่วิดีโอเหล่านี้คงเป็นไปไม่ได้หากไม่มีคุณ

271
00:20:07,460 --> 00:20:10,143
ฉันยังอยากจะแสดงความขอบคุณเป็นพิเศษต่อบริษัท VC Amplify 

272
00:20:10,143 --> 00:20:12,780
Partners ในการสนับสนุนวิดีโอเริ่มต้นเหล่านี้ในซีรีส์นี้

