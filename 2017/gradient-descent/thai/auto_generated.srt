1
00:00:00,000 --> 00:00:07,240
วิดีโอล่าสุด ฉันอธิบายโครงสร้างของโครงข่ายประสาทเทียม

2
00:00:07,240 --> 00:00:11,560
ฉันจะสรุปสั้นๆ ที่นี่เพื่อให้ความคิดของเราสดใส

3
00:00:11,560 --> 00:00:13,160
จากนั้นฉันก็มีเป้าหมายหลักสองประการสำหรับวิดีโอนี้

4
00:00:13,160 --> 00:00:17,960
ประการแรกคือการแนะนำแนวคิดเรื่องการไล่ระดับสี ซึ่งไม่เพียงแต่เป็นรากฐานของการเรียนรู้ของโครงข่ายประสาทเทียมเท่านั้น

5
00:00:17,960 --> 00:00:20,800
แต่ยังรวมถึงการทำงานของการเรียนรู้ของเครื่องอื่นๆ อีกด้วย

6
00:00:20,800 --> 00:00:25,160
หลังจากนั้นเราจะเจาะลึกลงไปอีกหน่อยว่าเครือข่ายนี้ทำงานอย่างไร

7
00:00:25,160 --> 00:00:29,560
และเซลล์ประสาทในชั้นที่ซ่อนอยู่เหล่านั้นมองหาอะไร

8
00:00:29,560 --> 00:00:34,680
เพื่อเป็นการเตือนความจำ เป้าหมายของเราที่นี่คือตัวอย่างคลาสสิกของการรู้จำตัวเลขที่เขียนด้วยลายมือ

9
00:00:34,680 --> 00:00:37,080
สวัสดีโลกแห่งโครงข่ายประสาทเทียม

10
00:00:37,080 --> 00:00:42,160
ตัวเลขเหล่านี้แสดงผลบนตารางพิกเซล 28x28 พิกเซล แต่ละพิกเซลมีค่าระดับสีเทาอยู่ระหว่าง

11
00:00:42,160 --> 00:00:44,260
0 ถึง 1

12
00:00:44,260 --> 00:00:51,400
สิ่งเหล่านี้คือสิ่งที่กำหนดการเปิดใช้งานของเซลล์ประสาท 784 ตัวในเลเยอร์อินพุตของเครือข่าย

13
00:00:51,400 --> 00:00:56,880
การเปิดใช้งานของเซลล์ประสาทแต่ละอันในเลเยอร์ต่อไปนี้จะขึ้นอยู่กับผลรวมถ่วงน้ำหนักของการกระตุ้นทั้งหมดในเลเยอร์ก่อนหน้า

14
00:00:56,880 --> 00:01:02,300
บวกกับจำนวนพิเศษบางตัวที่เรียกว่าไบแอส

15
00:01:02,300 --> 00:01:07,480
คุณเขียนผลรวมนั้นด้วยฟังก์ชันอื่นๆ เช่น ซิกมอยด์สควิชิฟิเคชัน

16
00:01:07,480 --> 00:01:09,640
หรือ ReLU แบบที่ผมอธิบายในวิดีโอที่แล้ว

17
00:01:09,640 --> 00:01:14,960
โดยรวมแล้ว เมื่อพิจารณาถึงการเลือกเลเยอร์ที่ซ่อนอยู่สองชั้น โดยแต่ละเลเยอร์มีเซลล์ประสาท 16 เซลล์

18
00:01:14,960 --> 00:01:20,940
ซึ่งแต่ละชั้นมีเซลล์ประสาทให้เลือก 16 เซลล์ เครือข่ายจึงมีน้ำหนักและอคติประมาณ

19
00:01:20,940 --> 00:01:25,320
13,000 ค่าที่เราปรับเปลี่ยนได้ และค่าเหล่านี้เองที่กำหนดว่าจริงๆ แล้วเครือข่ายทำหน้าที่อะไร

20
00:01:25,320 --> 00:01:29,800
และสิ่งที่เราหมายถึงเมื่อเราบอกว่าเครือข่ายนี้แยกประเภทตัวเลขที่กำหนดก็คือเซลล์ประสาทที่สว่างที่สุดจาก 10

21
00:01:29,800 --> 00:01:34,080
เซลล์ประสาทในเลเยอร์สุดท้ายนั้นสอดคล้องกับตัวเลขนั้น

22
00:01:34,080 --> 00:01:39,240
และจำไว้ว่า แรงจูงใจที่เรามีอยู่ในใจสำหรับโครงสร้างแบบเลเยอร์ก็คือ

23
00:01:39,240 --> 00:01:43,920
บางทีชั้นที่สองอาจหยิบจับที่ขอบ ชั้นที่สามอาจจับรูปแบบต่างๆ

24
00:01:43,920 --> 00:01:48,640
เช่น ลูปหรือเส้น

25
00:01:48,640 --> 00:01:49,640
และอันสุดท้ายก็แค่ปะติดปะต่อรูปแบบเหล่านั้นเข้าด้วยกัน จดจำตัวเลข

26
00:01:49,640 --> 00:01:52,880
ตรงนี้ เราจะเรียนรู้ว่าเครือข่ายเรียนรู้อย่างไร

27
00:01:52,880 --> 00:01:56,880
สิ่งที่เราต้องการคืออัลกอริธึมที่คุณสามารถแสดงข้อมูลการฝึกอบรมทั้งหมดให้กับเครือข่ายนี้ ซึ่งมาในรูปแบบของรูปภาพต่างๆ

28
00:01:56,880 --> 00:02:01,540
ที่เป็นตัวเลขที่เขียนด้วยลายมือ พร้อมด้วยป้ายกำกับสำหรับสิ่งที่พวกเขาควรจะเป็น

29
00:02:01,540 --> 00:02:06,360
และมันจะ ปรับน้ำหนักและอคติ

30
00:02:06,360 --> 00:02:10,760
13,000 รายการเพื่อปรับปรุงประสิทธิภาพของข้อมูลการฝึกอบรม

31
00:02:10,760 --> 00:02:15,540
หวังว่าโครงสร้างแบบเลเยอร์นี้จะหมายความว่าสิ่งที่เรียนรู้จะสรุปกับรูปภาพที่นอกเหนือจากข้อมูลการฝึกอบรมนั้น

32
00:02:15,540 --> 00:02:17,840


33
00:02:17,840 --> 00:02:22,240
วิธีที่เราทดสอบก็คือ หลังจากที่คุณฝึกเครือข่าย

34
00:02:22,240 --> 00:02:31,160
คุณจะแสดงข้อมูลที่ติดป้ายกำกับมากขึ้น และคุณจะเห็นว่ามันจำแนกประเภทรูปภาพใหม่เหล่านั้นได้แม่นยำเพียงใด

35
00:02:31,160 --> 00:02:34,760
โชคดีสำหรับเรา และสิ่งที่ทำให้ตัวอย่างทั่วไปนี้เริ่มต้นได้ ก็คือคนดีๆ

36
00:02:34,760 --> 00:02:39,520
ที่อยู่เบื้องหลังฐานข้อมูล MNIST ได้รวบรวมคอลเลกชันรูปภาพตัวเลขที่เขียนด้วยลายมือจำนวนนับหมื่น

37
00:02:39,520 --> 00:02:45,080
ภาพ โดยแต่ละภาพจะมีป้ายกำกับด้วยตัวเลขที่ควรจะเป็น

38
00:02:45,080 --> 00:02:49,920
และการอธิบายเครื่องจักรว่าเป็นการเรียนรู้นั้นช่างยั่วยวนซะอีก เมื่อคุณได้เห็นว่ามันทำงานอย่างไร มันก็จะรู้สึกเหมือนกับนิยายไซไฟบ้าๆ

39
00:02:49,920 --> 00:02:55,560
บอๆ น้อยลงมาก และเหมือนเป็นการฝึกแคลคูลัสมากกว่า

40
00:02:55,560 --> 00:03:01,040
ฉันหมายถึง โดยพื้นฐานแล้วมันขึ้นอยู่กับการหาค่าต่ำสุดของฟังก์ชันบางอย่าง

41
00:03:01,040 --> 00:03:06,480
โปรดจำไว้ว่า ตามแนวคิดแล้ว

42
00:03:06,480 --> 00:03:11,440
เรากำลังคิดว่าเซลล์ประสาทแต่ละเซลล์เชื่อมต่อกับเซลล์ประสาททั้งหมดในเลเยอร์ก่อนหน้า

43
00:03:11,440 --> 00:03:16,400
และน้ำหนักในผลรวมถ่วงน้ำหนักที่กำหนดการเปิดใช้งานของเซลล์ประสาทนั้นก็เหมือนกับจุดแข็งของการเชื่อมต่อเหล่านั้น และความลำเอียงเป็นข้อบ่งชี้บางประการของ

44
00:03:16,400 --> 00:03:19,780
ไม่ว่าเซลล์ประสาทนั้นมีแนวโน้มที่จะมีการใช้งานหรือไม่ใช้งานก็ตาม

45
00:03:19,780 --> 00:03:23,300
และเพื่อเริ่มต้นสิ่งต่างๆ

46
00:03:23,300 --> 00:03:25,020
เราจะเริ่มต้นน้ำหนักและอคติเหล่านั้นทั้งหมดโดยการสุ่มโดยสิ้นเชิง

47
00:03:25,020 --> 00:03:29,100
ไม่จำเป็นต้องพูดว่า เครือข่ายนี้จะทำงานได้แย่มากในตัวอย่างการฝึกอบรมที่กำหนด

48
00:03:29,100 --> 00:03:31,180
เนื่องจากเป็นเพียงการทำบางสิ่งแบบสุ่ม

49
00:03:31,180 --> 00:03:36,820
ตัวอย่างเช่น คุณป้อนรูปภาพ 3 นี้ และเลเยอร์เอาต์พุตดูเหมือนยุ่งเหยิง

50
00:03:36,820 --> 00:03:43,340
ดังนั้นสิ่งที่คุณทำคือกำหนดฟังก์ชันต้นทุน วิธีบอกคอมพิวเตอร์ ไม่ คอมพิวเตอร์เสีย ผลลัพธ์นั้นควรมีการเปิดใช้งาน ซึ่งเป็น

51
00:03:43,340 --> 00:03:48,940
0 สำหรับเซลล์ประสาทส่วนใหญ่ แต่เป็น 1 สำหรับเซลล์ประสาทนี้

52
00:03:48,980 --> 00:03:51,740
สิ่งที่คุณให้ฉันคือขยะทั้งหมด

53
00:03:51,740 --> 00:03:56,740
หากจะกล่าวให้มากกว่านี้อีกหน่อยในทางคณิตศาสตร์

54
00:03:56,740 --> 00:04:01,980
คุณจะต้องบวกกำลังสองของความแตกต่างระหว่างการเปิดใช้งานเอาท์พุตขยะแต่ละรายการกับค่าที่คุณต้องการให้มี

55
00:04:01,980 --> 00:04:06,020
และนี่คือสิ่งที่เราจะเรียกว่าต้นทุนของตัวอย่างการฝึกอบรมตัวอย่างเดียว

56
00:04:06,020 --> 00:04:12,660
โปรดสังเกตว่าผลรวมนี้จะน้อยเมื่อเครือข่ายจัดประเภทรูปภาพได้อย่างถูกต้อง

57
00:04:12,660 --> 00:04:18,820
แต่จะมีขนาดใหญ่เมื่อเครือข่ายดูเหมือนว่าไม่รู้ว่ากำลังทำอะไรอยู่

58
00:04:18,820 --> 00:04:23,860
ดังนั้นสิ่งที่คุณทำคือพิจารณาต้นทุนเฉลี่ยของตัวอย่างการฝึกอบรมนับหมื่นตัวอย่างทั้งหมดตามที่คุณต้องการ

59
00:04:23,860 --> 00:04:27,580


60
00:04:27,580 --> 00:04:32,300
ต้นทุนเฉลี่ยนี้เป็นการวัดของเราว่าเครือข่ายแย่เพียงใด

61
00:04:32,300 --> 00:04:33,300
และคอมพิวเตอร์ควรรู้สึกแย่เพียงใด

62
00:04:33,300 --> 00:04:35,300
และนั่นเป็นสิ่งที่ซับซ้อน

63
00:04:35,300 --> 00:04:40,380
จำได้ไหมว่าเครือข่ายนั้นเป็นฟังก์ชันโดยพื้นฐานแล้วเครือข่ายนั้นรับตัวเลข 784 เป็นอินพุต

64
00:04:40,380 --> 00:04:46,100
ค่าพิกเซล และแยกตัวเลข 10

65
00:04:46,100 --> 00:04:49,700
ตัวเป็นเอาต์พุต และในแง่หนึ่งมันถูกกำหนดพารามิเตอร์ด้วยน้ำหนักและอคติเหล่านี้ทั้งหมด

66
00:04:49,700 --> 00:04:53,340
ฟังก์ชันต้นทุนยังเป็นชั้นของความซับซ้อนที่นอกเหนือไปจากนั้น

67
00:04:53,340 --> 00:04:59,140
โดยจะใช้น้ำหนักและอคติประมาณ 13,000

68
00:04:59,140 --> 00:05:04,620
รายการเป็นข้อมูลป้อนเข้า และแยกตัวเลขออกมาเพียงตัวเดียวเพื่ออธิบายว่าน้ำหนักและอคติเหล่านั้นแย่แค่ไหน

69
00:05:04,620 --> 00:05:09,140
และวิธีการกำหนดนั้นขึ้นอยู่กับพฤติกรรมของเครือข่ายในข้อมูลการฝึกอบรมนับหมื่นชิ้น

70
00:05:09,140 --> 00:05:12,460
นั่นเป็นเรื่องที่ต้องคิดมาก

71
00:05:12,460 --> 00:05:16,380
แต่การบอกคอมพิวเตอร์ว่างานเส็งเคร็งกำลังทำอยู่นั้นไม่ได้ช่วยอะไรมากนัก

72
00:05:16,380 --> 00:05:21,300
คุณต้องการจะบอกวิธีเปลี่ยนน้ำหนักและอคติเหล่านั้นเพื่อให้ดีขึ้น

73
00:05:21,300 --> 00:05:25,580
เพื่อให้ง่ายขึ้น แทนที่จะจินตนาการถึงฟังก์ชันที่มีอินพุต 13,000

74
00:05:25,580 --> 00:05:31,440
รายการ ลองจินตนาการถึงฟังก์ชันง่ายๆ ที่มีตัวเลขหนึ่งเป็นอินพุตและตัวเลขหนึ่งเป็นเอาต์พุต

75
00:05:31,440 --> 00:05:36,420
คุณจะค้นหาอินพุตที่ลดค่าของฟังก์ชันนี้ให้เหลือน้อยที่สุดได้อย่างไร?

76
00:05:36,420 --> 00:05:41,300
นักเรียนแคลคูลัสจะรู้ว่าบางครั้งคุณสามารถคิดค่าขั้นต่ำนั้นได้อย่างชัดเจน แต่นั่นก็เป็นไปไม่ได้เสมอไปสำหรับฟังก์ชันที่ซับซ้อนจริงๆ

77
00:05:41,340 --> 00:05:46,620
แน่นอนว่าไม่ใช่ในเวอร์ชันอินพุต 13,000

78
00:05:46,620 --> 00:05:51,640
รายการของสถานการณ์นี้สำหรับฟังก์ชันต้นทุนโครงข่ายประสาทเทียมที่ซับซ้อนอย่างบ้าคลั่งของเรา

79
00:05:51,640 --> 00:05:56,820
กลยุทธ์ที่ยืดหยุ่นกว่าคือเริ่มจากอินพุตใดๆ

80
00:05:56,820 --> 00:05:59,860
และหาทิศทางที่คุณควรก้าวเพื่อทำให้เอาต์พุตนั้นต่ำลง

81
00:05:59,860 --> 00:06:05,020
โดยเฉพาะอย่างยิ่ง หากคุณสามารถหาความชันของฟังก์ชันในตำแหน่งที่คุณอยู่ได้

82
00:06:05,020 --> 00:06:09,280
ให้เลื่อนไปทางซ้ายหากความชันนั้นเป็นบวก

83
00:06:09,280 --> 00:06:12,720
และเลื่อนค่าอินพุตไปทางขวาหากความชันนั้นเป็นลบ

84
00:06:12,720 --> 00:06:17,040
หากคุณทำเช่นนี้ซ้ำๆ ในแต่ละจุดตรวจสอบความชันใหม่และทำตามขั้นตอนที่เหมาะสม

85
00:06:17,040 --> 00:06:20,680
คุณจะเข้าใกล้ค่าต่ำสุดของฟังก์ชัน

86
00:06:20,680 --> 00:06:24,600
และภาพที่คุณอาจนึกได้คือลูกบอลกลิ้งลงมาจากเนินเขา

87
00:06:24,600 --> 00:06:29,380
และสังเกตว่า แม้ว่าฟังก์ชันอินพุตเดี่ยวที่เรียบง่ายนี้

88
00:06:29,380 --> 00:06:34,220
ยังมีหุบเขาที่เป็นไปได้มากมายที่คุณอาจเข้าไปได้

89
00:06:34,220 --> 00:06:38,460
ขึ้นอยู่กับอินพุตแบบสุ่มที่คุณเริ่มต้น และไม่มีการรับประกันว่าค่าต่ำสุดในพื้นที่ที่คุณไปถึงจะเป็นค่าที่เล็กที่สุดเท่าที่จะเป็นไปได้

90
00:06:38,460 --> 00:06:39,460
ของฟังก์ชันต้นทุน

91
00:06:39,460 --> 00:06:43,180
นั่นจะส่งต่อไปยังโครงข่ายประสาทเทียมของเราเช่นกัน

92
00:06:43,180 --> 00:06:48,140
และฉันอยากให้คุณสังเกตด้วยว่า ถ้าคุณทำให้ขนาดขั้นบันไดของคุณเป็นสัดส่วนกับความชัน

93
00:06:48,140 --> 00:06:52,920
แล้วเมื่อความชันแบนไปทางขั้นต่ำ ขั้นของคุณจะเล็กลงเรื่อยๆ

94
00:06:52,920 --> 00:06:56,020
และแบบนั้นจะช่วยคุณจากการถ่ายภาพเกินเหตุ

95
00:06:56,020 --> 00:07:01,640
เพิ่มความซับซ้อนขึ้นเล็กน้อย ลองจินตนาการถึงฟังก์ชันที่มีสองอินพุตและหนึ่งเอาต์พุตแทน

96
00:07:01,640 --> 00:07:06,360
คุณอาจคิดว่าพื้นที่อินพุตเป็นระนาบ xy

97
00:07:06,360 --> 00:07:09,020
และฟังก์ชันต้นทุนถูกวาดเป็นกราฟเป็นพื้นผิวด้านบน

98
00:07:09,020 --> 00:07:13,600
แทนที่จะถามเกี่ยวกับความชันของฟังก์ชัน คุณต้องถามว่าคุณควรก้าวไปในทิศทางใดในพื้นที่อินพุตนี้

99
00:07:13,600 --> 00:07:19,780
เพื่อลดเอาต์พุตของฟังก์ชันให้เร็วที่สุด

100
00:07:19,780 --> 00:07:22,340
กล่าวอีกนัยหนึ่ง ทิศทางลงเขาคืออะไร?

101
00:07:22,340 --> 00:07:26,740
และขอย้ำอีกครั้งว่า การนึกถึงลูกบอลกลิ้งลงมาตามเนินเขานั้นก็เป็นประโยชน์

102
00:07:26,740 --> 00:07:31,920
พวกคุณที่คุ้นเคยกับแคลคูลัสหลายตัวแปรจะรู้ว่าความชันของฟังก์ชันช่วยให้คุณกำหนดทิศทางของการขึ้นที่สูงที่สุดได้

103
00:07:31,920 --> 00:07:37,460
และคุณควรก้าวไปในทิศทางใดเพื่อเพิ่มฟังก์ชันให้เร็วที่สุด

104
00:07:37,460 --> 00:07:39,420


105
00:07:39,420 --> 00:07:43,820
โดยธรรมชาติแล้ว

106
00:07:43,820 --> 00:07:47,460
การหาค่าลบของการไล่ระดับสีนั้นจะทำให้คุณมีทิศทางในการก้าวที่ลดฟังก์ชันลงได้เร็วที่สุด

107
00:07:47,460 --> 00:07:52,320
ยิ่งไปกว่านั้น

108
00:07:52,320 --> 00:07:54,580
ความยาวของเวกเตอร์เกรเดียนต์นี้ยังเป็นตัวบ่งชี้ความชันของความชันสูงสุดอีกด้วย

109
00:07:54,580 --> 00:07:58,080
ตอนนี้ถ้าคุณไม่คุ้นเคยกับแคลคูลัสหลายตัวแปรและต้องการเรียนรู้เพิ่มเติม ลองดูงานบางส่วนที่ฉันทำให้กับ Khan

110
00:07:58,080 --> 00:08:01,100
Academy ในหัวข้อนี้

111
00:08:01,100 --> 00:08:05,680
จริงๆ แล้ว สิ่งที่สำคัญสำหรับคุณและฉันตอนนี้คือ

112
00:08:05,680 --> 00:08:10,440
โดยหลักการแล้ว มีวิธีคำนวณเวกเตอร์นี้

113
00:08:10,440 --> 00:08:12,040
เวกเตอร์นี้ที่บอกคุณว่าทิศทางลงเนินคืออะไร และชันแค่ไหน

114
00:08:12,040 --> 00:08:17,280
คุณจะไม่เป็นไรถ้านั่นคือทั้งหมดที่คุณรู้และคุณไม่ใส่ใจรายละเอียดมากนัก

115
00:08:17,280 --> 00:08:21,440
เพราะถ้าคุณทำได้ อัลกอริธึมในการลดฟังก์ชันคือคำนวณทิศทางการไล่ระดับสี

116
00:08:21,440 --> 00:08:27,400
จากนั้นก้าวลงเนินเล็กน้อย แล้วทำซ้ำซ้ำแล้วซ้ำอีก

117
00:08:28,300 --> 00:08:33,700
เป็นแนวคิดพื้นฐานเดียวกันสำหรับฟังก์ชันที่มีอินพุต 13,000 อินพุตแทนที่จะเป็น 2 อินพุต

118
00:08:33,700 --> 00:08:38,980
ลองนึกภาพการจัดน้ำหนักและอคติทั้งหมด 13,000

119
00:08:38,980 --> 00:08:40,180
รายการของเครือข่ายของเราให้เป็นเวกเตอร์คอลัมน์ขนาดยักษ์

120
00:08:40,180 --> 00:08:46,140
เกรเดียนต์เชิงลบของฟังก์ชันต้นทุนเป็นเพียงเวกเตอร์

121
00:08:46,140 --> 00:08:51,660
มันเป็นทิศทางบางอย่างภายในพื้นที่อินพุตขนาดใหญ่มาก

122
00:08:51,660 --> 00:08:55,900
ที่บอกคุณว่าการขยับตัวเลขใดที่จะทำให้ฟังก์ชันต้นทุนลดลงอย่างรวดเร็วที่สุด

123
00:08:55,900 --> 00:09:00,000
และแน่นอนว่าด้วยฟังก์ชันต้นทุนที่ออกแบบมาเป็นพิเศษของเรา การเปลี่ยนน้ำหนักและความลำเอียงเพื่อลด

124
00:09:00,000 --> 00:09:05,520
หมายถึงการทำให้เอาต์พุตของเครือข่ายในข้อมูลการฝึกแต่ละชิ้นดูเหมือนอาร์เรย์สุ่มที่มีค่า 10

125
00:09:05,520 --> 00:09:10,280
ค่าน้อยลง และเหมือนกับการตัดสินใจจริงที่เราต้องการมากกว่า

126
00:09:10,280 --> 00:09:11,280
มันจะทำให้

127
00:09:11,280 --> 00:09:15,940
สิ่งสำคัญที่ต้องจำก็คือ ฟังก์ชันต้นทุนนี้เกี่ยวข้องกับค่าเฉลี่ยของข้อมูลการฝึกทั้งหมด ดังนั้น

128
00:09:15,940 --> 00:09:24,260
หากคุณย่อให้เล็กสุด ก็หมายความว่าประสิทธิภาพดีขึ้นในกลุ่มตัวอย่างทั้งหมด

129
00:09:24,260 --> 00:09:28,540
อัลกอริธึมสำหรับการคำนวณการไล่ระดับสีนี้อย่างมีประสิทธิภาพ ซึ่งเป็นหัวใจสำคัญของการเรียนรู้ของโครงข่ายประสาทเทียม

130
00:09:28,540 --> 00:09:32,520
เรียกว่าการแพร่กระจายกลับ

131
00:09:32,520 --> 00:09:34,040
และนั่นคือสิ่งที่ฉันจะพูดถึงในวิดีโอหน้า

132
00:09:34,040 --> 00:09:39,100
ที่นั่น

133
00:09:39,100 --> 00:09:44,100
ฉันอยากจะใช้เวลาศึกษาสิ่งที่เกิดขึ้นกับน้ำหนักและอคติแต่ละอย่างสำหรับข้อมูลการฝึกแต่ละชิ้น

134
00:09:44,100 --> 00:09:47,980
โดยพยายามให้ความรู้สึกตามสัญชาตญาณว่าเกิดอะไรขึ้นนอกเหนือจากแคลคูลัสและสูตรที่เกี่ยวข้องมากมาย

135
00:09:47,980 --> 00:09:51,780
ที่นี่ ตอนนี้

136
00:09:51,780 --> 00:09:56,820
สิ่งสำคัญที่ฉันอยากให้คุณรู้ โดยไม่ขึ้นอยู่กับรายละเอียดการใช้งาน

137
00:09:56,820 --> 00:09:59,320
คือสิ่งที่เราหมายถึงเมื่อเราพูดถึงการเรียนรู้เครือข่ายก็คือ มันแค่ลดฟังก์ชันต้นทุนให้เหลือน้อยที่สุด

138
00:09:59,320 --> 00:10:02,760
และสังเกตว่า ผลที่ตามมาประการหนึ่งก็คือ

139
00:10:02,760 --> 00:10:07,820
สิ่งสำคัญคือฟังก์ชันต้นทุนนี้จะต้องมีผลลัพธ์ที่ราบรื่น

140
00:10:07,820 --> 00:10:09,340
เพื่อที่เราจะได้หาค่าต่ำสุดในพื้นที่ได้โดยการก้าวลงเนินเล็กน้อย

141
00:10:09,340 --> 00:10:14,140
ด้วยเหตุนี้เอง เซลล์ประสาทเทียมจึงมีการกระตุ้นอย่างต่อเนื่อง

142
00:10:14,140 --> 00:10:18,580
แทนที่จะเป็นเพียงการทำงานหรือไม่ใช้งานในลักษณะไบนารี

143
00:10:18,580 --> 00:10:20,440
เหมือนกับที่เซลล์ประสาทชีวภาพเป็นอยู่

144
00:10:20,440 --> 00:10:24,600
กระบวนการดันอินพุตของฟังก์ชันซ้ำๆ

145
00:10:24,600 --> 00:10:26,960
โดยการทวีคูณของการไล่ระดับสีเชิงลบนี้เรียกว่าการไล่ระดับสีลง

146
00:10:26,960 --> 00:10:31,760
เป็นวิธีหนึ่งที่จะมาบรรจบกันกับฟังก์ชันต้นทุนขั้นต่ำในท้องถิ่น

147
00:10:31,760 --> 00:10:33,000
โดยพื้นฐานแล้วจะเป็นหุบเขาในกราฟนี้

148
00:10:33,000 --> 00:10:37,040
แน่นอนว่าฉันยังคงแสดงรูปภาพของฟังก์ชันที่มีอินพุตสองตัวอยู่ เนื่องจากการขยับในพื้นที่อินพุตขนาด

149
00:10:37,040 --> 00:10:41,480
13,000 มิตินั้นยากสักหน่อยที่จะเข้าใจ

150
00:10:41,480 --> 00:10:45,220
แต่จริงๆ แล้วมีวิธีคิดที่ไม่เกี่ยวกับเชิงพื้นที่ที่ดีจริงๆ

151
00:10:45,220 --> 00:10:49,100
แต่ละองค์ประกอบของเกรเดียนต์เชิงลบบอกเราสองอย่าง

152
00:10:49,100 --> 00:10:53,600
แน่นอนว่าเครื่องหมายบอกเราว่าองค์ประกอบที่สอดคล้องกันของเวกเตอร์อินพุตควรถูกดันขึ้นหรือลง

153
00:10:53,600 --> 00:10:55,860


154
00:10:55,860 --> 00:11:01,340
แต่ที่สำคัญ ขนาดสัมพัทธ์ของส่วนประกอบทั้งหมดนี้

155
00:11:01,340 --> 00:11:05,620
จะบอกคุณได้ว่าการเปลี่ยนแปลงใดสำคัญกว่ากัน

156
00:11:05,620 --> 00:11:09,780
คุณจะเห็นว่าในเครือข่ายของเรา การปรับน้ำหนักอย่างใดอย่างหนึ่งอาจมีผลกระทบต่อฟังก์ชันต้นทุนมากกว่าการปรับน้ำหนักอื่นๆ

157
00:11:09,780 --> 00:11:14,980
มาก

158
00:11:14,980 --> 00:11:19,440
การเชื่อมต่อบางส่วนเหล่านี้มีความสำคัญต่อข้อมูลการฝึกอบรมของเรามากกว่า

159
00:11:19,440 --> 00:11:23,520
วิธีหนึ่งที่คุณสามารถคิดถึงเวกเตอร์เกรเดียนต์ของฟังก์ชันต้นทุนมหาศาลที่บิดเบือนความคิดได้ ก็คือมันเข้ารหัสความสำคัญสัมพัทธ์

160
00:11:23,520 --> 00:11:29,740
ของแต่ละน้ำหนักและอคติ นั่นคือ

161
00:11:29,740 --> 00:11:34,100
การเปลี่ยนแปลงใดที่จะทำให้คุณเสียเงินมากที่สุด

162
00:11:34,100 --> 00:11:37,360
นี่เป็นเพียงวิธีคิดเกี่ยวกับทิศทางอีกวิธีหนึ่งเท่านั้น

163
00:11:37,360 --> 00:11:41,740
เพื่อยกตัวอย่างที่ง่ายกว่านี้ หากคุณมีฟังก์ชันบางอย่างที่มีตัวแปรสองตัวเป็นอินพุต

164
00:11:41,740 --> 00:11:48,720
และคำนวณว่าการไล่ระดับสีที่จุดใดจุดหนึ่งออกมาเป็น 3,1

165
00:11:48,720 --> 00:11:52,880
ในด้านหนึ่ง

166
00:11:52,880 --> 00:11:57,400
คุณสามารถตีความสิ่งนั้นได้ว่าเป็นการบอกว่าเมื่อคุณ การยืนอยู่ที่อินพุตนั้น

167
00:11:57,400 --> 00:12:02,200
การเคลื่อนไปตามทิศทางนี้จะเพิ่มฟังก์ชันให้เร็วที่สุด คือเมื่อคุณสร้างกราฟฟังก์ชันเหนือระนาบของจุดอินพุต

168
00:12:02,200 --> 00:12:03,200
เวกเตอร์นั้นคือสิ่งที่ให้ทิศทางขึ้นเนินเป็นเส้นตรง

169
00:12:03,200 --> 00:12:07,600
แต่วิธีอ่านอีกวิธีหนึ่งก็คือบอกว่าการเปลี่ยนแปลงในตัวแปรแรกนี้มีความสำคัญมากกว่าการเปลี่ยนแปลงตัวแปรตัวที่สองถึง 3 เท่า

170
00:12:07,600 --> 00:12:12,400
อย่างน้อยก็ในบริเวณใกล้เคียงกับอินพุตที่เกี่ยวข้อง การดันค่า x

171
00:12:12,400 --> 00:12:17,740
จะทำให้คุณปังมากขึ้น เจ้าชู้.

172
00:12:17,740 --> 00:12:22,880
เอาล่ะ ซูมออกและสรุปว่าเราอยู่ไกลถึงไหนแล้ว

173
00:12:22,880 --> 00:12:28,660
ตัวเครือข่ายเองเป็นฟังก์ชันนี้ซึ่งมีอินพุต 784 รายการและเอาต์พุต

174
00:12:28,660 --> 00:12:30,860
10 รายการ ซึ่งกำหนดในรูปของผลรวมถ่วงน้ำหนักทั้งหมดนี้

175
00:12:30,860 --> 00:12:34,160
ฟังก์ชันต้นทุนยังเป็นชั้นของความซับซ้อนที่นอกเหนือไปจากนั้น

176
00:12:34,160 --> 00:12:39,300
ใช้น้ำหนักและอคติ 13,000

177
00:12:39,300 --> 00:12:42,640
รายการเป็นข้อมูล และแยกความเลวออกมาเพียงค่าเดียวตามตัวอย่างการฝึกอบรม

178
00:12:42,640 --> 00:12:47,520
การไล่ระดับสีของฟังก์ชันต้นทุนยังคงเป็นความซับซ้อนอีกชั้นหนึ่ง

179
00:12:47,520 --> 00:12:52,860
โดยบอกเราว่าการเปลี่ยนแปลงน้ำหนักและความลำเอียงใดที่ทำให้เกิดการเปลี่ยนแปลงค่าของฟังก์ชันต้นทุนได้เร็วที่สุด

180
00:12:52,860 --> 00:12:56,640
ซึ่งคุณอาจตีความได้ว่าการเปลี่ยนแปลงใดที่น้ำหนักมีความสำคัญมากที่สุด

181
00:12:56,640 --> 00:13:03,040


182
00:13:03,040 --> 00:13:07,620
ดังนั้นเมื่อคุณเริ่มต้นเครือข่ายด้วยน้ำหนักและอคติแบบสุ่ม

183
00:13:07,620 --> 00:13:12,420
และปรับหลายครั้งตามกระบวนการไล่ระดับสีนี้

184
00:13:12,420 --> 00:13:14,240
มันจะทำงานได้ดีแค่ไหนกับภาพที่ไม่เคยเห็นมาก่อน

185
00:13:14,240 --> 00:13:19,000
สิ่งที่ฉันได้อธิบายไว้ที่นี่ ซึ่งมีสองชั้นที่ซ่อนอยู่ 16 เซลล์ประสาทแต่ละชั้น ซึ่งส่วนใหญ่เลือกด้วยเหตุผลด้านสุนทรียศาสตร์

186
00:13:19,000 --> 00:13:26,920
ถือว่าไม่แย่ โดยจำแนกประมาณ 96% ของภาพใหม่ที่เห็นได้อย่างถูกต้อง

187
00:13:26,920 --> 00:13:31,580
และโดยสุจริต หากคุณดูตัวอย่างบางส่วนที่ทำให้เกิดปัญหา

188
00:13:31,580 --> 00:13:36,300
คุณจะรู้สึกว่าจำเป็นต้องลดหย่อนลงเล็กน้อย

189
00:13:36,300 --> 00:13:40,220
หากคุณลองใช้โครงสร้างเลเยอร์ที่ซ่อนอยู่และปรับแต่งเล็กน้อย คุณจะได้รับสิ่งนี้มากถึง

190
00:13:40,220 --> 00:13:41,220
98%

191
00:13:41,220 --> 00:13:42,900
และนั่นก็ค่อนข้างดี!

192
00:13:42,900 --> 00:13:47,020
ไม่ใช่สิ่งที่ดีที่สุด คุณสามารถได้รับประสิทธิภาพที่ดีขึ้นอย่างแน่นอนโดยมีความซับซ้อนมากกว่าเครือข่ายวานิลลาธรรมดานี้

193
00:13:47,020 --> 00:13:52,460
แต่เมื่อพิจารณาว่างานเริ่มแรกนั้นน่ากลัวเพียงใด

194
00:13:52,460 --> 00:13:56,800
ฉันคิดว่ามีบางอย่างที่น่าทึ่งเกี่ยวกับเครือข่ายใดๆ ที่ทำได้ดีกับภาพที่ไม่เคยเห็นมาก่อน

195
00:13:56,800 --> 00:14:02,000
ไม่เคยบอกเจาะจงว่าควรมองหารูปแบบอะไร

196
00:14:02,000 --> 00:14:07,840
เดิมที วิธีที่ฉันกระตุ้นโครงสร้างนี้คือการอธิบายความหวังที่เราอาจมี

197
00:14:07,840 --> 00:14:11,880
ว่าชั้นที่สองอาจหยิบขึ้นมาจากขอบเล็กๆ

198
00:14:11,880 --> 00:14:16,080
ว่าชั้นที่สามจะปะติดปะต่อขอบเหล่านั้นเข้าด้วยกันเพื่อรับรู้ถึงลูปและเส้นที่ยาวกว่า และเหล่านั้นอาจถูกปะติดปะต่อกัน

199
00:14:16,080 --> 00:14:18,220
ร่วมกันเพื่อจดจำตัวเลข

200
00:14:18,220 --> 00:14:21,040
นี่คือสิ่งที่เครือข่ายของเรากำลังทำอยู่จริงหรือ?

201
00:14:21,040 --> 00:14:24,880
อย่างน้อยที่สุดก็ไม่ใช่เลยสำหรับอันนี้

202
00:14:24,960 --> 00:14:29,120
จำได้ไหมว่าวิดีโอล่าสุดที่เราดูว่าน้ำหนักของการเชื่อมต่อจากเซลล์ประสาททั้งหมดในชั้นแรกไปยังเซลล์ประสาทที่กำหนดในชั้นที่สองนั้นสามารถมองเห็นเป็นรูปแบบพิกเซลที่กำหนดซึ่งเซลล์ประสาทชั้นที่สองหยิบขึ้นมาได้อย่างไร

203
00:14:29,120 --> 00:14:33,900


204
00:14:33,900 --> 00:14:37,440


205
00:14:37,440 --> 00:14:44,600
เมื่อเราทำเช่นนั้นกับน้ำหนักที่เกี่ยวข้องกับการเปลี่ยนภาพเหล่านี้ แทนที่จะหยิบขึ้นมาจากขอบเล็กๆ

206
00:14:44,600 --> 00:14:51,000
ที่แยกจากกันตรงนี้และตรงนั้น พวกมันดูเกือบจะสุ่มเลย

207
00:14:51,000 --> 00:14:54,200
โดยมีรูปแบบที่หลวมมากอยู่ตรงกลาง

208
00:14:54,200 --> 00:14:59,020
ดูเหมือนว่าในพื้นที่ขนาดใหญ่ถึง 13,000

209
00:14:59,020 --> 00:15:04,020
มิติของน้ำหนักและอคติที่เป็นไปได้

210
00:15:04,020 --> 00:15:08,440
เครือข่ายของเราพบว่าตัวเองมีเกณฑ์ขั้นต่ำในท้องถิ่นที่น่าพึงพอใจ ซึ่งแม้จะจำแนกภาพส่วนใหญ่ได้สำเร็จ

211
00:15:08,440 --> 00:15:09,840
แต่ก็ยังไม่สามารถเข้าใจรูปแบบที่เราคาดหวังไว้ได้

212
00:15:09,840 --> 00:15:14,600
และเพื่อขับเคลื่อนจุดนี้กลับบ้านจริงๆ ให้ดูสิ่งที่เกิดขึ้นเมื่อคุณป้อนภาพแบบสุ่ม

213
00:15:14,600 --> 00:15:19,240
หากระบบฉลาด คุณอาจคาดหวังว่าระบบจะรู้สึกไม่แน่นอน อาจจะไม่เปิดใช้งานเซลล์ประสาทเอาท์พุตใด ๆ ใน

214
00:15:19,240 --> 00:15:24,120
10 เซลล์นั้นหรือเปิดใช้งานพวกมันทั้งหมดเท่า ๆ กัน

215
00:15:24,520 --> 00:15:29,800
แต่กลับให้คำตอบไร้สาระแก่คุณอย่างมั่นใจ ราวกับว่ามันรู้สึกมั่นใจว่าการสุ่มนี้ สัญญาณรบกวนคือ 5

216
00:15:29,800 --> 00:15:34,560
เนื่องจากภาพจริงของ 5 คือ 5

217
00:15:34,560 --> 00:15:39,300
ใช้ถ้อยคำแตกต่างออกไป แม้ว่าเครือข่ายนี้สามารถจดจำตัวเลขได้ค่อนข้างดี

218
00:15:39,300 --> 00:15:41,800
แต่ก็ไม่รู้ว่าจะวาดมันอย่างไร

219
00:15:41,800 --> 00:15:45,400
สาเหตุส่วนใหญ่มาจากการจัดเตรียมการฝึกอบรมที่มีข้อจำกัดอย่างเข้มงวด

220
00:15:45,400 --> 00:15:48,220
ฉันหมายถึง ใส่ตัวเองเข้าไปอยู่ในบทบาทของเครือข่ายที่นี่

221
00:15:48,220 --> 00:15:53,280
จากมุมมองของมัน จักรวาลทั้งหมดไม่มีอะไรนอกจากตัวเลขที่ไม่เคลื่อนไหวที่กำหนดไว้อย่างชัดเจนซึ่งมีศูนย์กลางอยู่ในตารางเล็ก

222
00:15:53,280 --> 00:15:58,560
ๆ และฟังก์ชันต้นทุนของมันก็ไม่เคยให้แรงจูงใจใด

223
00:15:58,560 --> 00:16:02,160
ๆ ที่จะเป็นอะไรนอกจากมั่นใจอย่างเต็มที่ในการตัดสินใจ

224
00:16:02,160 --> 00:16:05,760
ดังนั้น ด้วยภาพนี้ว่าจริงๆ

225
00:16:05,760 --> 00:16:09,320
แล้วเซลล์ประสาทชั้นที่สองกำลังทำอะไรอยู่ คุณอาจสงสัยว่าทำไมฉันถึงแนะนำเครือข่ายนี้

226
00:16:09,320 --> 00:16:10,320
ด้วยแรงจูงใจในการเลือกขอบและรูปแบบ

227
00:16:10,320 --> 00:16:13,040
ฉันหมายความว่านั่นไม่ใช่สิ่งที่ท้ายที่สุดแล้ว

228
00:16:13,040 --> 00:16:17,480
นี่ไม่ใช่เป้าหมายสุดท้ายของเรา แต่เป็นจุดเริ่มต้นแทน

229
00:16:17,480 --> 00:16:22,280
จริงๆ แล้ว นี่เป็นเทคโนโลยีเก่า

230
00:16:22,280 --> 00:16:26,920
ซึ่งเป็นเทคโนโลยีที่ได้รับการค้นคว้าในยุค 80 และ

231
00:16:26,920 --> 00:16:31,380
90 และคุณจำเป็นต้องเข้าใจก่อนจึงจะสามารถเข้าใจตัวแปรสมัยใหม่ที่มีรายละเอียดมากขึ้นได้ และเห็นได้ชัดว่าสามารถแก้ไขปัญหาที่น่าสนใจบางอย่างได้

232
00:16:31,380 --> 00:16:38,720
แต่ยิ่งคุณเจาะลึกลงไปถึงอะไร เลเยอร์ที่ซ่อนอยู่เหล่านั้นกำลังทำอยู่จริงๆ ยิ่งดูฉลาดน้อยลงเท่านั้น

233
00:16:38,720 --> 00:16:43,540
การเปลี่ยนโฟกัสไปชั่วขณะจากวิธีที่เครือข่ายเรียนรู้ไปเป็นวิธีการเรียนรู้ของคุณ

234
00:16:43,540 --> 00:16:47,160
ซึ่งจะเกิดขึ้นก็ต่อเมื่อคุณมีส่วนร่วมอย่างแข็งขันกับเนื้อหาที่นี่ไม่ทางใดก็ทางหนึ่ง

235
00:16:47,160 --> 00:16:51,920
สิ่งง่ายๆ อย่างหนึ่งที่ฉันอยากให้คุณทำคือหยุดตอนนี้และคิดให้ลึกซึ้งสักครู่เกี่ยวกับการเปลี่ยนแปลงที่คุณอาจทำกับระบบนี้

236
00:16:51,920 --> 00:16:57,560
และวิธีที่ระบบรับรู้ภาพ หากคุณต้องการให้ระบบรับสิ่งต่างๆ

237
00:16:57,560 --> 00:17:01,880
เช่น ขอบและลวดลายได้ดีขึ้น

238
00:17:01,880 --> 00:17:06,360
แต่ที่ดีกว่านั้น หากต้องการมีส่วนร่วมกับเนื้อหาจริงๆ ฉันขอแนะนำหนังสือของ

239
00:17:06,360 --> 00:17:09,720
Michael Nielsen เกี่ยวกับการเรียนรู้เชิงลึกและโครงข่ายประสาทเทียมเป็นอย่างยิ่ง

240
00:17:09,720 --> 00:17:15,200
ในนั้น คุณจะพบโค้ดและข้อมูลที่จะดาวน์โหลดและเล่นสำหรับตัวอย่างที่ชัดเจนนี้

241
00:17:15,200 --> 00:17:19,360
และหนังสือจะแนะนำคุณทีละขั้นตอนว่าโค้ดนั้นกำลังทำอะไรอยู่

242
00:17:19,360 --> 00:17:23,920
สิ่งที่ยอดเยี่ยมคือหนังสือเล่มนี้ให้บริการฟรีและเผยแพร่ต่อสาธารณะ ดังนั้นหากคุณได้ประโยชน์จากหนังสือเล่มนี้ ลองมาร่วมบริจาคให้กับความพยายามของ

243
00:17:23,920 --> 00:17:28,040
Nielsen กับฉัน

244
00:17:28,040 --> 00:17:32,060
ฉันยังได้เชื่อมโยงแหล่งข้อมูลอื่นๆ สองสามอย่างที่ฉันชอบมากในคำอธิบาย รวมถึงบล็อกโพสต์ที่สวยงามและน่าอัศจรรย์โดย Chris

245
00:17:32,060 --> 00:17:38,720
Ola และบทความใน Distill

246
00:17:38,720 --> 00:17:41,960
เพื่อปิดประเด็นในช่วงไม่กี่นาทีที่ผ่านมา ฉันอยากจะย้อนกลับไปดูตัวอย่างบทสัมภาษณ์ที่ฉันมีกับเลอิชา

247
00:17:41,960 --> 00:17:44,440
ลี

248
00:17:44,440 --> 00:17:48,520
คุณอาจจำเธอได้จากวิดีโอที่แล้ว เธอทำงานระดับปริญญาเอกด้านการเรียนรู้เชิงลึก

249
00:17:48,560 --> 00:17:52,240
ในตัวอย่างเล็กๆ น้อยๆ นี้

250
00:17:52,240 --> 00:17:56,380
เธอพูดถึงเอกสารสองฉบับล่าสุดที่เจาะลึกว่าเครือข่ายการจดจำรูปภาพที่ทันสมัยกว่าบางฉบับกำลังเรียนรู้จริง ๆ อย่างไร

251
00:17:56,380 --> 00:18:00,320
เพื่อกำหนดจุดที่เราอยู่ในการสนทนา บทความฉบับแรกได้นำหนึ่งในโครงข่ายประสาทเทียมเชิงลึกพิเศษเหล่านี้

252
00:18:00,320 --> 00:18:04,480
ซึ่งสามารถจดจำภาพได้ดีมาก และแทนที่จะฝึกมันกับชุดข้อมูลที่มีป้ายกำกับอย่างถูกต้อง

253
00:18:04,480 --> 00:18:09,400
กลับสับเปลี่ยนป้ายกำกับทั้งหมดก่อนการฝึก

254
00:18:09,400 --> 00:18:13,840
แน่นอนว่าความแม่นยำในการทดสอบที่นี่จะไม่ดีไปกว่าการสุ่ม

255
00:18:13,840 --> 00:18:15,320
เนื่องจากทุกอย่างเป็นเพียงป้ายกำกับแบบสุ่ม

256
00:18:15,320 --> 00:18:20,080
แต่ยังคงสามารถบรรลุความแม่นยำในการฝึกอบรมได้เช่นเดียวกับที่คุณทำกับชุดข้อมูลที่มีป้ายกำกับอย่างถูกต้อง

257
00:18:20,080 --> 00:18:21,440


258
00:18:21,440 --> 00:18:26,120
โดยพื้นฐานแล้ว น้ำหนักนับล้านสำหรับเครือข่ายนี้เพียงพอที่จะจดจำข้อมูลแบบสุ่ม

259
00:18:26,120 --> 00:18:31,040
ซึ่งทำให้เกิดคำถามว่าการลดฟังก์ชันต้นทุนนี้ให้เหลือน้อยที่สุดนั้นสอดคล้องกับโครงสร้างประเภทใด ๆ

260
00:18:31,040 --> 00:18:36,720
ในภาพหรือไม่ หรือเป็นเพียงการท่องจำเท่านั้น

261
00:18:36,720 --> 00:18:40,120
. . . เพื่อจดจำชุดข้อมูลทั้งหมดว่าการจำแนกประเภทที่ถูกต้องคืออะไร

262
00:18:40,120 --> 00:18:45,720
ครึ่งปีให้หลังที่ ICML ในปีนี้ สองสามคนใน ICML

263
00:18:45,720 --> 00:18:50,440
ปีนี้ ไม่ได้มีรายงานโต้แย้งแต่อย่างใด มีแต่รายงานที่กล่าวถึงบางแง่มุม เช่น

264
00:18:50,440 --> 00:18:52,220
จริงๆ แล้ว เครือข่ายเหล่านี้ กำลังทำอะไรที่ฉลาดกว่านั้นนิดหน่อย

265
00:18:52,220 --> 00:18:59,600
ถ้าคุณดูกราฟความแม่นยำนั้น หากคุณแค่ฝึกกับชุดข้อมูลสุ่ม เส้นโค้งนั้นจะลดลงอย่างมาก

266
00:18:59,600 --> 00:19:05,240
คุณก็รู้ ช้ามากในลักษณะเกือบเป็นเส้นตรง

267
00:19:05,280 --> 00:19:10,840
คุณกำลังลำบากมากที่จะหาค่าตุ้มน้ำหนักที่เหมาะสมที่จะทำให้คุณได้รับความแม่นยำนั้น

268
00:19:10,840 --> 00:19:12,320


269
00:19:12,320 --> 00:19:16,720
ในขณะที่หากคุณกำลังฝึกชุดข้อมูลที่มีโครงสร้างจริงๆ ซึ่งเป็นชุดข้อมูลที่มีป้ายกำกับที่ถูกต้อง

270
00:19:16,720 --> 00:19:20,240
คุณก็รู้ไหมว่าในช่วงแรกๆ คุณจะเล่นซอนิดหน่อย

271
00:19:20,240 --> 00:19:23,360
แต่แล้วคุณกลับลดลงอย่างรวดเร็วเพื่อไปถึงระดับความแม่นยำนั้น

272
00:19:23,360 --> 00:19:28,580
ดังนั้นในแง่หนึ่ง มันง่ายกว่าที่จะค้นหาหลักคำสอนท้องถิ่นนั้น

273
00:19:28,580 --> 00:19:32,900
และสิ่งที่น่าสนใจเกี่ยวกับเรื่องนี้ก็คือ

274
00:19:32,900 --> 00:19:39,140
ได้มีการเปิดเผยรายงานอีกฉบับหนึ่งจากสองสามปีที่แล้ว

275
00:19:39,140 --> 00:19:40,140
ซึ่งมีการลดความซับซ้อนมากขึ้นเกี่ยวกับเลเยอร์เครือข่าย

276
00:19:40,140 --> 00:19:43,880
แต่ผลลัพธ์ประการหนึ่งก็คือว่า ถ้าคุณดูภาพรวมของการเพิ่มประสิทธิภาพ

277
00:19:43,880 --> 00:19:49,400
ค่าขั้นต่ำในท้องถิ่นที่เครือข่ายเหล่านี้มีแนวโน้มที่จะเรียนรู้นั้นมีคุณภาพเท่าเทียมกันได้อย่างไร

278
00:19:49,400 --> 00:19:54,300
ดังนั้น ในแง่หนึ่ง หากชุดข้อมูลของคุณมีโครงสร้าง คุณก็จะสามารถค้นหาข้อมูลนั้นได้ง่ายขึ้นมาก

279
00:19:58,580 --> 00:20:01,140
ฉันขอขอบคุณผู้ที่สนับสนุน Patreon เช่นเคย

280
00:20:01,480 --> 00:20:05,440
ฉันเคยบอกไปแล้วว่าตัวเปลี่ยนเกมของ Patreon

281
00:20:05,440 --> 00:20:07,160
คืออะไร แต่วิดีโอเหล่านี้จะเป็นไปไม่ได้เลยหากไม่มีคุณ

282
00:20:07,160 --> 00:20:11,540
ฉันยังอยากจะแสดงความขอบคุณเป็นพิเศษต่อบริษัท VC Amplify

283
00:20:11,540 --> 00:20:13,240
Partners และการสนับสนุนวิดีโอเริ่มต้นเหล่านี้ในซีรีส์นี้

284
00:20:31,140 --> 00:20:33,140
ขอบคุณ

