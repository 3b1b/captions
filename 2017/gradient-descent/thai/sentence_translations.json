[
 {
  "translatedText": "วิดีโอล่าสุด ฉันอธิบายโครงสร้างของโครงข่ายประสาทเทียม",
  "input": "Last video I laid out the structure of a neural network.",
  "time_range": [
   4.180000000000002,
   7.28
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ฉันจะสรุปสั้นๆ ที่นี่เพื่อให้ความคิดของเราสดใส จากนั้นฉันก็มีเป้าหมายหลักสองประการสำหรับวิดีโอนี้",
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "time_range": [
   7.68,
   12.6
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ประการแรกคือการแนะนำแนวคิดเรื่องการไล่ระดับสี ซึ่งไม่เพียงแต่เป็นรากฐานของการเรียนรู้ของโครงข่ายประสาทเทียมเท่านั้น แต่ยังรวมถึงการทำงานของการเรียนรู้ของเครื่องอื่นๆ อีกด้วย",
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "time_range": [
   13.1,
   20.6
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "หลังจากนั้นเราจะเจาะลึกลงไปอีกหน่อยว่าเครือข่ายนี้ทำงานอย่างไร และเซลล์ประสาทในชั้นที่ซ่อนอยู่เหล่านั้นมองหาอะไร",
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "time_range": [
   21.12,
   27.94
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "เพื่อเป็นการเตือนความจำ เป้าหมายของเราที่นี่คือตัวอย่างคลาสสิกของการรู้จำตัวเลขที่เขียนด้วยลายมือ สวัสดีโลกแห่งโครงข่ายประสาทเทียม",
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "time_range": [
   28.979999999999997,
   36.22
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ตัวเลขเหล่านี้แสดงผลบนตารางพิกเซล 28x28 พิกเซล แต่ละพิกเซลมีค่าระดับสีเทาอยู่ระหว่าง 0 ถึง 1",
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "time_range": [
   37.02,
   43.42
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "สิ่งเหล่านี้คือสิ่งที่กำหนดการเปิดใช้งานของเซลล์ประสาท 784 ตัวในเลเยอร์อินพุตของเครือข่าย",
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "time_range": [
   43.82,
   50.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "จากนั้นการเปิดใช้งานของเซลล์ประสาทแต่ละอันในเลเยอร์ต่อไปนี้จะขึ้นอยู่กับผลรวมถ่วงน้ำหนักของการกระตุ้นทั้งหมดในเลเยอร์ก่อนหน้า บวกกับจำนวนพิเศษบางตัวที่เรียกว่าไบแอส",
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "time_range": [
   51.18,
   60.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "จากนั้นคุณเขียนผลบวกนั้นด้วยฟังก์ชันอื่นๆ เช่น ซิกมอยด์สควิชิฟิเคชั่น หรือเรลู แบบที่ผมอธิบายในวิดีโอที่แล้ว",
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "time_range": [
   62.16,
   68.94
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "โดยรวมแล้ว เมื่อพิจารณาถึงการเลือกเลเยอร์ที่ซ่อนอยู่สองชั้น โดยแต่ละเลเยอร์มีเซลล์ประสาท 16 เซลล์ ซึ่งแต่ละชั้นมีเซลล์ประสาทให้เลือก 16 เซลล์ เครือข่ายจึงมีน้ำหนักและอคติประมาณ 13,000 ค่าที่เราปรับเปลี่ยนได้ และค่าเหล่านี้เองที่กำหนดว่าจริงๆ แล้วเครือข่ายทำหน้าที่อะไร",
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "time_range": [
   69.48,
   84.38
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "สิ่งที่เราหมายถึงเมื่อเราบอกว่าเครือข่ายนี้จำแนกตัวเลขที่กำหนดก็คือเซลล์ประสาทที่สว่างที่สุดจาก 10 เซลล์ประสาทในเลเยอร์สุดท้ายนั้นสอดคล้องกับตัวเลขนั้น",
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "time_range": [
   84.88,
   93.3
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "และจำไว้ว่า แรงจูงใจที่เรามีอยู่ในใจสำหรับโครงสร้างแบบเลเยอร์นี้ คือบางทีชั้นที่สองอาจหยิบขึ้นมาที่ขอบ และชั้นที่สามอาจหยิบรูปแบบ เช่น ลูปหรือเส้น และอันสุดท้ายก็สามารถต่อเข้าด้วยกันได้ รูปแบบในการจดจำตัวเลข",
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "time_range": [
   94.1,
   108.8
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ตรงนี้ เราจะเรียนรู้ว่าเครือข่ายเรียนรู้อย่างไร",
  "input": "So here, we learn how the network learns.",
  "time_range": [
   109.8,
   112.24
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "สิ่งที่เราต้องการคืออัลกอริธึมที่คุณสามารถแสดงข้อมูลการฝึกอบรมทั้งหมดให้กับเครือข่ายนี้ ซึ่งมาในรูปแบบของรูปภาพต่างๆ ที่เป็นตัวเลขที่เขียนด้วยลายมือ พร้อมด้วยป้ายกำกับสำหรับสิ่งที่พวกเขาควรจะเป็น และมันจะ ปรับน้ำหนักและอคติ 13,000 รายการเพื่อปรับปรุงประสิทธิภาพของข้อมูลการฝึกอบรม",
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "time_range": [
   112.64,
   130.12
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "หวังว่าโครงสร้างแบบเลเยอร์นี้จะหมายความว่าสิ่งที่เรียนรู้จะสรุปกับรูปภาพที่นอกเหนือจากข้อมูลการฝึกอบรมนั้น",
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "time_range": [
   130.72,
   136.86
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "วิธีที่เราทดสอบก็คือ หลังจากที่คุณฝึกเครือข่าย คุณจะแสดงข้อมูลที่มีป้ายกำกับมากขึ้นซึ่งไม่เคยเห็นมาก่อน และคุณจะเห็นว่ามันจำแนกประเภทรูปภาพใหม่เหล่านั้นได้แม่นยำเพียงใด",
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "time_range": [
   137.64000000000001,
   146.7
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "โชคดีสำหรับเรา และสิ่งที่ทำให้ตัวอย่างทั่วไปนี้เริ่มต้นได้ ก็คือคนดีๆ ที่อยู่เบื้องหลังฐานข้อมูล MNIST ได้รวบรวมคอลเลกชันรูปภาพตัวเลขที่เขียนด้วยลายมือจำนวนนับหมื่น ภาพ โดยแต่ละภาพจะมีป้ายกำกับด้วยตัวเลขที่พวกเขาควรจะเป็น เป็น.",
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "time_range": [
   151.12,
   164.2
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "และการอธิบายเครื่องจักรว่าเป็นการเรียนรู้นั้นช่างยั่วยวนซะอีก เมื่อคุณเห็นวิธีการทำงานแล้ว มันก็จะรู้สึกเหมือนกับเป็นนิยายไซไฟที่บ้าๆบอๆ น้อยลงมาก และเหมือนเป็นการฝึกแคลคูลัสมากกว่า",
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "time_range": [
   164.9,
   175.48
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ฉันหมายถึง โดยพื้นฐานแล้วมันขึ้นอยู่กับการหาค่าต่ำสุดของฟังก์ชันบางอย่าง",
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "time_range": [
   176.2,
   179.96
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "โปรดจำไว้ว่า ตามแนวคิดแล้ว เรากำลังคิดว่าเซลล์ประสาทแต่ละเซลล์เชื่อมต่อกับเซลล์ประสาททั้งหมดในเลเยอร์ก่อนหน้า และน้ำหนักในผลรวมถ่วงน้ำหนักที่กำหนดการเปิดใช้งานของเซลล์ประสาทนั้นก็เหมือนกับจุดแข็งของการเชื่อมต่อเหล่านั้น และความลำเอียงเป็นข้อบ่งชี้บางประการของ ไม่ว่าเซลล์ประสาทนั้นมีแนวโน้มที่จะมีการใช้งานหรือไม่ใช้งานก็ตาม",
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "time_range": [
   181.93999999999997,
   198.96
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "และเพื่อเริ่มต้นสิ่งต่างๆ เราจะเริ่มต้นน้ำหนักและอคติเหล่านั้นทั้งหมดโดยการสุ่มโดยสิ้นเชิง",
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "time_range": [
   199.72,
   204.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ไม่จำเป็นต้องพูดว่า เครือข่ายนี้จะทำงานได้ค่อนข้างน่ากลัวในตัวอย่างการฝึกอบรมที่กำหนด เนื่องจากเป็นเพียงการทำบางสิ่งแบบสุ่ม",
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "time_range": [
   204.94,
   210.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ตัวอย่างเช่น คุณป้อนรูปภาพ 3 นี้ และเลเยอร์เอาต์พุตดูเหมือนยุ่งเหยิง",
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "time_range": [
   211.04,
   216.02
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ดังนั้นสิ่งที่คุณทำคือกำหนดฟังก์ชันต้นทุน วิธีบอกคอมพิวเตอร์ ไม่ คอมพิวเตอร์เสีย ผลลัพธ์นั้นควรมีการเปิดใช้งานที่เป็น 0 สำหรับเซลล์ประสาทส่วนใหญ่ แต่ 1 สำหรับเซลล์ประสาทนี้ สิ่งที่คุณให้ฉันมาคือขยะสุดๆ",
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "time_range": [
   216.6,
   230.76
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "หากจะกล่าวให้มากกว่านี้อีกหน่อยในทางคณิตศาสตร์ คุณจะต้องบวกกำลังสองของความแตกต่างระหว่างการเปิดใช้งานเอาท์พุตขยะแต่ละรายการกับค่าที่คุณต้องการให้มี และนี่คือสิ่งที่เราจะเรียกว่าต้นทุนของตัวอย่างการฝึกอบรมตัวอย่างเดียว",
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "time_range": [
   231.72,
   245.02
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "โปรดสังเกตว่าผลรวมนี้จะน้อยเมื่อเครือข่ายจัดประเภทรูปภาพได้อย่างถูกต้อง แต่จะมีขนาดใหญ่เมื่อเครือข่ายดูเหมือนว่าไม่รู้ว่ากำลังทำอะไรอยู่",
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "time_range": [
   245.96,
   256.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ดังนั้นสิ่งที่คุณทำคือพิจารณาต้นทุนเฉลี่ยของตัวอย่างการฝึกอบรมนับหมื่นตัวอย่างทั้งหมดตามที่คุณต้องการ",
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "time_range": [
   258.64,
   265.44
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ต้นทุนเฉลี่ยนี้เป็นการวัดของเราว่าเครือข่ายแย่เพียงใด และคอมพิวเตอร์ควรรู้สึกแย่เพียงใด",
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "time_range": [
   267.04,
   272.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "และนั่นเป็นสิ่งที่ซับซ้อน",
  "input": "And that's a complicated thing.",
  "time_range": [
   273.42,
   274.6
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "จำได้ไหมว่าเครือข่ายนั้นเป็นฟังก์ชันโดยพื้นฐานแล้วเครือข่ายนั้นรับตัวเลข 784 เป็นอินพุต ค่าพิกเซล และแยกตัวเลข 10 ตัวเป็นเอาต์พุต และในแง่หนึ่งมันถูกกำหนดพารามิเตอร์ด้วยน้ำหนักและอคติเหล่านี้ทั้งหมด",
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "time_range": [
   275.04,
   288.8
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ฟังก์ชันต้นทุนนั้นเป็นชั้นของความซับซ้อนที่นอกเหนือไปจากนั้น",
  "input": "Well the cost function is a layer of complexity on top of that.",
  "time_range": [
   289.5,
   292.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "โดยจะใช้น้ำหนักและอคติประมาณ 13,000 รายการเป็นข้อมูลป้อนเข้า และแยกตัวเลขออกมาเพียงตัวเดียวเพื่ออธิบายว่าน้ำหนักและอคติเหล่านั้นแย่แค่ไหน และวิธีการกำหนดนั้นขึ้นอยู่กับพฤติกรรมของเครือข่ายในข้อมูลการฝึกอบรมนับหมื่นชิ้น",
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "time_range": [
   293.1,
   308.9
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "นั่นเป็นเรื่องที่ต้องคิดมาก",
  "input": "That's a lot to think about.",
  "time_range": [
   309.52,
   311.0
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "แต่การบอกคอมพิวเตอร์ว่างานเส็งเคร็งกำลังทำอยู่นั้นไม่ได้ช่วยอะไรมากนัก",
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "time_range": [
   312.4,
   315.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "คุณต้องการจะบอกวิธีเปลี่ยนน้ำหนักและอคติเหล่านั้นเพื่อให้ดีขึ้น",
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "time_range": [
   316.22,
   320.06
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "เพื่อให้ง่ายขึ้น แทนที่จะจินตนาการถึงฟังก์ชันที่มีอินพุต 13,000 รายการ ลองจินตนาการถึงฟังก์ชันง่ายๆ ที่มีตัวเลขหนึ่งเป็นอินพุตและตัวเลขหนึ่งเป็นเอาต์พุต",
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "time_range": [
   320.78,
   330.48
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "คุณจะค้นหาอินพุตที่ลดค่าของฟังก์ชันนี้ให้เหลือน้อยที่สุดได้อย่างไร?",
  "input": "How do you find an input that minimizes the value of this function?",
  "time_range": [
   331.48,
   335.3
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "นักเรียนแคลคูลัสจะรู้ว่าบางครั้งคุณสามารถคิดค่าขั้นต่ำนั้นได้อย่างชัดเจน แต่นั่นก็เป็นไปไม่ได้เสมอไปสำหรับฟังก์ชันที่ซับซ้อนจริงๆ แน่นอนว่าไม่ใช่ในเวอร์ชันอินพุต 13,000 รายการของสถานการณ์นี้สำหรับฟังก์ชันต้นทุนโครงข่ายประสาทเทียมที่ซับซ้อนอย่างบ้าคลั่งของเรา",
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "time_range": [
   336.46,
   351.08
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "กลยุทธ์ที่ยืดหยุ่นกว่าคือเริ่มจากอินพุตใดๆ และหาทิศทางที่คุณควรก้าวเพื่อทำให้เอาต์พุตนั้นต่ำลง",
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "time_range": [
   351.58,
   359.2
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "โดยเฉพาะอย่างยิ่ง หากคุณสามารถหาความชันของฟังก์ชันในตำแหน่งที่คุณอยู่ได้ ให้เลื่อนไปทางซ้ายหากความชันนั้นเป็นบวก และเลื่อนค่าอินพุตไปทางขวาหากความชันนั้นเป็นลบ",
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "time_range": [
   360.08,
   369.9
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "หากคุณทำเช่นนี้ซ้ำๆ ในแต่ละจุดตรวจสอบความชันใหม่และทำตามขั้นตอนที่เหมาะสม คุณจะเข้าใกล้ค่าต่ำสุดของฟังก์ชัน",
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "time_range": [
   371.96,
   379.84
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ภาพที่คุณอาจนึกถึงที่นี่คือลูกบอลกลิ้งลงมาจากเนินเขา",
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "time_range": [
   380.64,
   383.8
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "โปรดสังเกตว่า แม้ว่าฟังก์ชันอินพุตเดี่ยวที่เรียบง่ายนี้ ยังมีหุบเขาที่เป็นไปได้มากมายที่คุณอาจเข้าไปได้ ขึ้นอยู่กับอินพุตแบบสุ่มที่คุณเริ่มต้น และไม่มีการรับประกันว่าค่าต่ำสุดในพื้นที่ที่คุณไปถึงจะเป็นค่าที่เล็กที่สุดเท่าที่จะเป็นไปได้ ของฟังก์ชันต้นทุน",
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "time_range": [
   384.62,
   399.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "นั่นจะส่งต่อไปยังกรณีโครงข่ายประสาทเทียมของเราเช่นกัน",
  "input": "That will carry over to our neural network case as well.",
  "time_range": [
   400.22,
   402.62
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ฉันยังอยากให้คุณสังเกตด้วยว่าหากคุณทำให้ขนาดขั้นบันไดเป็นสัดส่วนกับความชัน แล้วเมื่อความชันแบนราบไปทางขั้นต่ำ ขั้นตอนของคุณจะเล็กลงเรื่อยๆ และนั่นจะช่วยคุณจากการถ่ายภาพเกินขนาด",
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "time_range": [
   403.18,
   414.6
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "เพิ่มความซับซ้อนขึ้นเล็กน้อย ลองจินตนาการถึงฟังก์ชันที่มีสองอินพุตและหนึ่งเอาต์พุตแทน",
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "time_range": [
   415.94,
   420.98
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "คุณอาจคิดว่าพื้นที่อินพุตเป็นระนาบ xy และฟังก์ชันต้นทุนถูกวาดเป็นกราฟเป็นพื้นผิวด้านบน",
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "time_range": [
   421.5,
   428.14
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "แทนที่จะถามเกี่ยวกับความชันของฟังก์ชัน คุณต้องถามว่าคุณควรก้าวไปในทิศทางใดในพื้นที่อินพุตนี้ เพื่อลดเอาต์พุตของฟังก์ชันให้เร็วที่สุด",
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "time_range": [
   428.76,
   438.96
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "กล่าวอีกนัยหนึ่ง ทิศทางลงเขาคืออะไร?",
  "input": "In other words, what's the downhill direction?",
  "time_range": [
   439.72,
   441.76
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ขอย้ำอีกครั้งว่าการนึกถึงลูกบอลกลิ้งลงมาตามเนินเขานั้นก็เป็นประโยชน์",
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "time_range": [
   442.38,
   445.56
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "พวกคุณที่คุ้นเคยกับแคลคูลัสหลายตัวแปรจะรู้ว่าความชันของฟังก์ชันช่วยให้คุณกำหนดทิศทางของการขึ้นที่สูงที่สุดได้ และคุณควรก้าวไปในทิศทางใดเพื่อเพิ่มฟังก์ชันให้เร็วที่สุด",
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "time_range": [
   446.66,
   458.78
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "โดยธรรมชาติแล้ว การหาค่าลบของการไล่ระดับสีนั้นจะทำให้คุณมีทิศทางในการก้าวที่ลดฟังก์ชันลงได้เร็วที่สุด",
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "time_range": [
   459.56,
   466.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ยิ่งไปกว่านั้น ความยาวของเวกเตอร์เกรเดียนต์นี้ยังเป็นตัวบ่งชี้ความชันของความชันสูงสุดอีกด้วย",
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "time_range": [
   467.24,
   473.84
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "หากคุณไม่คุ้นเคยกับแคลคูลัสหลายตัวแปรและต้องการเรียนรู้เพิ่มเติม ลองดูงานบางส่วนที่ฉันทำให้กับ Khan Academy ในหัวข้อนี้",
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "time_range": [
   474.54,
   480.34
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "จริงๆ แล้ว สิ่งที่สำคัญสำหรับคุณและฉันตอนนี้คือ โดยหลักการแล้ว มีวิธีคำนวณเวกเตอร์นี้ เวกเตอร์นี้ที่บอกคุณว่าทิศทางลงเนินคืออะไร และชันแค่ไหน",
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "time_range": [
   480.86,
   491.9
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "คุณจะไม่เป็นไรถ้านั่นคือทั้งหมดที่คุณรู้และคุณไม่ใส่ใจรายละเอียดมากนัก",
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "time_range": [
   492.4,
   496.12
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "หากคุณเข้าใจได้ อัลกอริธึมในการลดฟังก์ชันคือการคำนวณทิศทางการไล่ระดับสี จากนั้นก้าวลงเนินเล็กน้อย และทำซ้ำซ้ำแล้วซ้ำอีก",
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "time_range": [
   497.2,
   506.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "เป็นแนวคิดพื้นฐานเดียวกันสำหรับฟังก์ชันที่มีอินพุต 13,000 อินพุตแทนที่จะเป็น 2 อินพุต",
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "time_range": [
   507.7,
   512.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ลองนึกภาพการจัดน้ำหนักและอคติทั้งหมด 13,000 รายการของเครือข่ายของเราให้เป็นเวกเตอร์คอลัมน์ขนาดยักษ์",
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "time_range": [
   513.4,
   519.46
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "เกรเดียนต์เชิงลบของฟังก์ชันต้นทุนเป็นเพียงเวกเตอร์ มันเป็นทิศทางบางอย่างภายในพื้นที่อินพุตขนาดใหญ่มาก ที่บอกคุณว่าการขยับตัวเลขใดที่จะทำให้ฟังก์ชันต้นทุนลดลงอย่างรวดเร็วที่สุด",
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "time_range": [
   520.14,
   534.88
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "และแน่นอนว่าด้วยฟังก์ชันต้นทุนที่ออกแบบมาเป็นพิเศษของเรา การเปลี่ยนน้ำหนักและความลำเอียงเพื่อลด หมายถึงการทำให้เอาต์พุตของเครือข่ายในข้อมูลการฝึกแต่ละชิ้นดูเหมือนอาร์เรย์สุ่มที่มีค่า 10 ค่าน้อยลง และเหมือนกับการตัดสินใจจริงที่เราต้องการมากกว่า มันจะทำให้",
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "time_range": [
   535.64,
   550.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "สิ่งสำคัญที่ต้องจำก็คือ ฟังก์ชันต้นทุนนี้เกี่ยวข้องกับค่าเฉลี่ยของข้อมูลการฝึกทั้งหมด ดังนั้น หากคุณย่อให้เล็กสุด ก็หมายความว่าประสิทธิภาพดีขึ้นในกลุ่มตัวอย่างทั้งหมด",
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "time_range": [
   551.44,
   561.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "อัลกอริธึมสำหรับการคำนวณการไล่ระดับสีนี้อย่างมีประสิทธิภาพ ซึ่งเป็นหัวใจสำคัญของการเรียนรู้ของโครงข่ายประสาทเทียม เรียกว่าการแพร่กระจายกลับ และนั่นคือสิ่งที่ฉันจะพูดถึงในวิดีโอหน้า",
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "time_range": [
   563.82,
   573.98
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ที่นั่น ฉันอยากจะใช้เวลาศึกษาสิ่งที่เกิดขึ้นกับน้ำหนักและอคติแต่ละอย่างสำหรับข้อมูลการฝึกแต่ละชิ้น โดยพยายามให้ความรู้สึกตามสัญชาตญาณว่าเกิดอะไรขึ้นนอกเหนือจากแคลคูลัสและสูตรที่เกี่ยวข้องมากมาย",
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "time_range": [
   574.66,
   587.1
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ที่นี่ ตอนนี้ สิ่งสำคัญที่ฉันอยากให้คุณรู้ โดยไม่ขึ้นอยู่กับรายละเอียดการใช้งาน คือสิ่งที่เราหมายถึงเมื่อเราพูดถึงการเรียนรู้เครือข่ายก็คือ มันแค่ลดฟังก์ชันต้นทุนให้เหลือน้อยที่สุด",
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "time_range": [
   587.78,
   598.36
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "และสังเกตว่า ผลที่ตามมาประการหนึ่งก็คือ สิ่งสำคัญคือฟังก์ชันต้นทุนนี้จะต้องมีผลลัพธ์ที่ราบรื่น เพื่อที่เราจะได้หาค่าต่ำสุดในพื้นที่ได้โดยการก้าวลงเนินเล็กน้อย",
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "time_range": [
   599.3,
   608.1
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ด้วยเหตุนี้เอง เซลล์ประสาทเทียมจึงมีการกระตุ้นอย่างต่อเนื่อง แทนที่จะเป็นเพียงการทำงานหรือไม่ใช้งานในลักษณะไบนารี เหมือนที่เซลล์ประสาทชีวภาพเป็นอยู่",
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "time_range": [
   609.26,
   619.14
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "กระบวนการดันอินพุตของฟังก์ชันซ้ำๆ โดยการทวีคูณของการไล่ระดับสีเชิงลบนี้เรียกว่าการไล่ระดับสีลง",
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "time_range": [
   620.22,
   626.76
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "เป็นวิธีหนึ่งที่จะมาบรรจบกันกับฟังก์ชันต้นทุนขั้นต่ำในท้องถิ่น โดยพื้นฐานแล้วจะเป็นหุบเขาในกราฟนี้",
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "time_range": [
   627.3,
   632.58
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "แน่นอนว่าฉันยังคงแสดงรูปภาพของฟังก์ชันที่มีอินพุตสองอินพุตอยู่ เนื่องจากการขยับในพื้นที่อินพุตขนาด 13,000 มิตินั้นอาจเป็นเรื่องยากเล็กน้อยที่จะเข้าใจ แต่มีวิธีคิดที่ไม่เกี่ยวกับเชิงพื้นที่ที่ดีเกี่ยวกับเรื่องนี้",
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "time_range": [
   633.44,
   644.26
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "แต่ละองค์ประกอบของเกรเดียนต์เชิงลบบอกเราสองอย่าง",
  "input": "Each component of the negative gradient tells us two things.",
  "time_range": [
   645.08,
   648.44
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "แน่นอนว่าเครื่องหมายบอกเราว่าองค์ประกอบที่สอดคล้องกันของเวกเตอร์อินพุตควรถูกดันขึ้นหรือลง",
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "time_range": [
   649.06,
   655.14
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "แต่ที่สำคัญ ขนาดสัมพัทธ์ของส่วนประกอบทั้งหมดนี้ จะบอกคุณได้ว่าการเปลี่ยนแปลงใดสำคัญกว่ากัน",
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "time_range": [
   655.8,
   662.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "คุณจะเห็นว่าในเครือข่ายของเรา การปรับเปลี่ยนน้ำหนักอย่างใดอย่างหนึ่งอาจมีผลกระทบต่อฟังก์ชันต้นทุนมากกว่าการปรับเปลี่ยนน้ำหนักอื่นๆ มาก",
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "time_range": [
   665.22,
   673.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "การเชื่อมต่อบางส่วนเหล่านี้มีความสำคัญต่อข้อมูลการฝึกอบรมของเรามากกว่า",
  "input": "Some of these connections just matter more for our training data.",
  "time_range": [
   674.8,
   678.2
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "วิธีหนึ่งที่คุณสามารถคิดถึงเวกเตอร์เกรเดียนต์ของฟังก์ชันต้นทุนมหาศาลที่บิดเบือนความคิดได้ ก็คือมันเข้ารหัสความสำคัญสัมพัทธ์ ของแต่ละน้ำหนักและอคติ นั่นคือ การเปลี่ยนแปลงใดที่จะทำให้คุณเสียเงินมากที่สุด",
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "time_range": [
   679.32,
   692.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "นี่เป็นเพียงวิธีคิดเกี่ยวกับทิศทางอีกวิธีหนึ่ง",
  "input": "This really is just another way of thinking about direction.",
  "time_range": [
   693.62,
   696.64
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "เพื่อยกตัวอย่างที่ง่ายกว่านี้ หากคุณมีฟังก์ชันบางอย่างที่มีตัวแปรสองตัวเป็นอินพุต และคุณคำนวณว่าการไล่ระดับสีที่จุดใดจุดหนึ่งออกมาเป็น 3,1 ในด้านหนึ่ง คุณสามารถตีความสิ่งนั้นได้ว่าเป็นการบอกว่าเมื่อคุณ ยืนอยู่ที่อินพุตนั้น การเคลื่อนไปตามทิศทางนี้จะเพิ่มฟังก์ชันให้เร็วที่สุด คือเมื่อคุณสร้างกราฟฟังก์ชันเหนือระนาบของจุดอินพุต เวกเตอร์นั้นคือสิ่งที่ให้ทิศทางขึ้นเนินเป็นเส้นตรง",
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "time_range": [
   697.1,
   722.26
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "แต่วิธีอ่านอีกวิธีหนึ่งคือบอกว่าการเปลี่ยนแปลงตัวแปรแรกนี้มีความสำคัญเป็น 3 เท่าของการเปลี่ยนแปลงตัวแปรตัวที่สอง อย่างน้อยก็ในบริเวณใกล้เคียงกับอินพุตที่เกี่ยวข้อง การดันค่า x จะทำให้คุณปังมากขึ้น เจ้าชู้.",
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "time_range": [
   722.86,
   736.9
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ลองซูมออกและสรุปว่าเราอยู่ไกลถึงไหนแล้ว",
  "input": "Let's zoom out and sum up where we are so far.",
  "time_range": [
   739.88,
   742.34
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ตัวเครือข่ายเองเป็นฟังก์ชันนี้ซึ่งมีอินพุต 784 รายการและเอาต์พุต 10 รายการ ซึ่งกำหนดในรูปของผลรวมถ่วงน้ำหนักทั้งหมดนี้",
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "time_range": [
   742.84,
   750.04
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ฟังก์ชันต้นทุนยังเป็นชั้นของความซับซ้อนที่นอกเหนือไปจากนั้น",
  "input": "The cost function is a layer of complexity on top of that.",
  "time_range": [
   750.64,
   753.68
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ใช้น้ำหนักและอคติ 13,000 รายการเป็นข้อมูลนำเข้า และแยกความเลวออกมาเพียงค่าเดียวตามตัวอย่างการฝึกอบรม",
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "time_range": [
   753.98,
   761.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "และการไล่ระดับสีของฟังก์ชันต้นทุนก็ยังมีความซับซ้อนอีกชั้นหนึ่ง",
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "time_range": [
   762.44,
   766.9
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "โดยบอกเราว่าการเปลี่ยนแปลงน้ำหนักและอคติใดที่ทำให้เกิดการเปลี่ยนแปลงค่าของฟังก์ชันต้นทุนได้เร็วที่สุด ซึ่งคุณอาจตีความได้ว่าการเปลี่ยนแปลงใดที่น้ำหนักมีความสำคัญมากที่สุด",
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "time_range": [
   767.36,
   777.88
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ดังนั้น เมื่อคุณเริ่มต้นเครือข่ายด้วยน้ำหนักและอคติแบบสุ่ม และปรับมันหลายครั้งตามกระบวนการไล่ระดับสีนี้ มันจะทำงานได้ดีเพียงใดกับภาพที่ไม่เคยเห็นมาก่อน",
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "time_range": [
   782.56,
   793.2
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "สิ่งที่ฉันได้อธิบายไว้ที่นี่ ซึ่งมีสองชั้นที่ซ่อนอยู่ 16 เซลล์ประสาทแต่ละชั้น ซึ่งส่วนใหญ่เลือกด้วยเหตุผลด้านสุนทรียศาสตร์ ถือว่าไม่แย่ โดยจำแนกประมาณ 96% ของภาพใหม่ที่เห็นได้อย่างถูกต้อง",
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "time_range": [
   794.1,
   805.96
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "และโดยสุจริต หากคุณดูตัวอย่างบางส่วนที่ทำให้เกิดปัญหา คุณจะรู้สึกว่าจำเป็นต้องลดหย่อนลงเล็กน้อย",
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "time_range": [
   806.68,
   812.54
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ตอนนี้ถ้าคุณลองใช้โครงสร้างเลเยอร์ที่ซ่อนอยู่และปรับแต่งเล็กน้อย คุณจะได้รับสิ่งนี้มากถึง 98%",
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "time_range": [
   816.22,
   821.76
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "และนั่นก็ค่อนข้างดี!",
  "input": "And that's pretty good!",
  "time_range": [
   821.76,
   822.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ไม่ใช่สิ่งที่ดีที่สุด คุณสามารถได้รับประสิทธิภาพที่ดีขึ้นอย่างแน่นอนโดยมีความซับซ้อนมากขึ้นกว่าเครือข่ายวานิลลาธรรมดานี้ แต่เมื่อพิจารณาว่างานเริ่มแรกนั้นน่ากลัวเพียงใด ฉันคิดว่ามีบางอย่างที่น่าทึ่งเกี่ยวกับเครือข่ายใด ๆ ที่ทำได้ดีกับภาพที่ไม่เคยเห็นมาก่อน เนื่องจาก เราไม่เคยบอกเจาะจงเจาะจงว่าควรมองหารูปแบบใด",
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "time_range": [
   823.02,
   841.42
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "เดิมที วิธีที่ฉันกระตุ้นโครงสร้างนี้คือการอธิบายความหวังที่เราอาจมี ว่าชั้นที่สองอาจหยิบยกขึ้นมาจากขอบเล็กๆ ว่าชั้นที่สามจะปะติดปะต่อขอบเหล่านั้นเข้าด้วยกันเพื่อรับรู้ถึงลูปและเส้นที่ยาวกว่า และเหล่านั้นอาจถูกปะติดปะต่อกัน ร่วมกันเพื่อจดจำตัวเลข",
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "time_range": [
   842.5600000000001,
   857.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "นี่คือสิ่งที่เครือข่ายของเรากำลังทำอยู่จริงหรือ?",
  "input": "So is this what our network is actually doing?",
  "time_range": [
   857.96,
   860.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "อย่างน้อยที่สุดก็ไม่ใช่เลยสำหรับอันนี้",
  "input": "Well, for this one at least, not at all.",
  "time_range": [
   861.0799999999999,
   864.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "จำได้ไหมว่าวิดีโอล่าสุดที่เราดูว่าน้ำหนักของการเชื่อมต่อจากเซลล์ประสาททั้งหมดในชั้นแรกไปยังเซลล์ประสาทที่กำหนดในชั้นที่สองนั้นสามารถมองเห็นเป็นรูปแบบพิกเซลที่กำหนดซึ่งเซลล์ประสาทชั้นที่สองหยิบขึ้นมาได้อย่างไร",
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "time_range": [
   864.82,
   877.06
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "เมื่อเราทำเช่นนั้นกับน้ำหนักที่เกี่ยวข้องกับการเปลี่ยนผ่านเหล่านี้ จากชั้นแรกไปยังชั้นถัดไป แทนที่จะเก็บบนขอบเล็กๆ ที่แยกจากกันตรงนี้และตรงนั้น พวกมันดูเกือบจะสุ่มเลย มีเพียงรูปแบบที่หลวมๆ บางส่วนใน ตรงกลางตรงนั้น",
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "time_range": [
   877.78,
   893.68
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ดูเหมือนว่าในพื้นที่ขนาดใหญ่ถึง 13,000 มิติของน้ำหนักและอคติที่เป็นไปได้ เครือข่ายของเราพบว่าตัวเองมีเกณฑ์ขั้นต่ำในท้องถิ่นที่น่าพึงพอใจ ซึ่งแม้จะจำแนกภาพส่วนใหญ่ได้สำเร็จ แต่ก็ยังไม่สามารถเข้าใจรูปแบบที่เราคาดหวังไว้ได้",
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "time_range": [
   893.76,
   908.96
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "และเพื่อขับเคลื่อนจุดนี้กลับบ้านจริงๆ ให้ดูสิ่งที่เกิดขึ้นเมื่อคุณป้อนภาพแบบสุ่ม",
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "time_range": [
   909.78,
   913.82
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "หากระบบฉลาด คุณอาจคาดหวังว่าระบบจะรู้สึกไม่แน่นอน อาจจะไม่ได้เปิดใช้งานเซลล์ประสาทเอาท์พุตใด ๆ จาก 10 ตัวนั้นจริงๆ หรือเปิดใช้งานพวกมันทั้งหมดเท่า ๆ กัน แต่กลับให้คำตอบที่ไร้สาระแก่คุณอย่างมั่นใจ ราวกับว่ามันรู้สึกแน่ใจว่าสัญญาณรบกวนแบบสุ่มนี้ คือ 5 เหมือนกับที่ภาพจริงของ 5 คือ 5",
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "time_range": [
   914.32,
   934.16
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ใช้ถ้อยคำแตกต่างออกไป แม้ว่าเครือข่ายนี้สามารถจดจำตัวเลขได้ค่อนข้างดี แต่ก็ไม่รู้ว่าจะวาดมันอย่างไร",
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "time_range": [
   934.54,
   940.7
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "สาเหตุส่วนใหญ่มาจากการจัดเตรียมการฝึกอบรมที่มีข้อจำกัดอย่างเข้มงวด",
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "time_range": [
   941.42,
   945.24
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ฉันหมายถึง ใส่ตัวเองเข้าไปอยู่ในบทบาทของเครือข่ายที่นี่",
  "input": "I mean, put yourself in the network's shoes here.",
  "time_range": [
   945.88,
   947.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "จากมุมมองของมัน จักรวาลทั้งหมดไม่มีอะไรนอกจากตัวเลขที่ไม่เคลื่อนไหวที่กำหนดไว้อย่างชัดเจนซึ่งมีศูนย์กลางอยู่ในตารางเล็ก ๆ และฟังก์ชันต้นทุนของมันก็ไม่เคยให้แรงจูงใจใด ๆ ที่จะเป็นอะไรนอกจากมั่นใจอย่างเต็มที่ในการตัดสินใจ",
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "time_range": [
   948.14,
   961.08
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ดังนั้น ด้วยภาพนี้ว่าจริงๆ แล้วเซลล์ประสาทชั้นที่สองกำลังทำอะไรอยู่ คุณอาจสงสัยว่าทำไมฉันถึงแนะนำเครือข่ายนี้ ด้วยแรงจูงใจในการเลือกขอบและรูปแบบ",
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "time_range": [
   962.12,
   969.92
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ฉันหมายความว่านั่นไม่ใช่สิ่งที่ท้ายที่สุดแล้ว",
  "input": "I mean, that's just not at all what it ends up doing.",
  "time_range": [
   969.92,
   972.3
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "นี่ไม่ใช่เป้าหมายสุดท้ายของเรา แต่เป็นจุดเริ่มต้นแทน",
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "time_range": [
   973.38,
   977.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "จริงๆ แล้ว นี่เป็นเทคโนโลยีเก่า ซึ่งเป็นเทคโนโลยีที่ได้รับการค้นคว้าในยุค 80 และ 90 และคุณจำเป็นต้องเข้าใจก่อนจึงจะสามารถเข้าใจตัวแปรสมัยใหม่ที่มีรายละเอียดมากขึ้นได้ และเห็นได้ชัดว่าสามารถแก้ไขปัญหาที่น่าสนใจบางอย่างได้ แต่ยิ่งคุณเจาะลึกลงไปถึงอะไร เลเยอร์ที่ซ่อนอยู่เหล่านั้นกำลังทำอยู่จริงๆ ยิ่งดูฉลาดน้อยลงเท่านั้น",
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "time_range": [
   977.64,
   994.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "การเปลี่ยนโฟกัสไปชั่วขณะจากวิธีที่เครือข่ายเรียนรู้ไปเป็นวิธีการเรียนรู้ของคุณ ซึ่งจะเกิดขึ้นก็ต่อเมื่อคุณมีส่วนร่วมอย่างแข็งขันกับเนื้อหาที่นี่ไม่ทางใดก็ทางหนึ่ง",
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "time_range": [
   998.48,
   1006.3
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "สิ่งง่ายๆ อย่างหนึ่งที่ฉันอยากให้คุณทำคือหยุดตอนนี้และคิดให้ลึกซึ้งสักครู่เกี่ยวกับการเปลี่ยนแปลงที่คุณอาจทำกับระบบนี้ และวิธีที่ระบบรับรู้ภาพ หากคุณต้องการให้ระบบรับสิ่งต่างๆ เช่น ขอบและลวดลายได้ดีขึ้น",
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "time_range": [
   1007.06,
   1020.88
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "แต่ที่ดีกว่านั้น หากต้องการมีส่วนร่วมกับเนื้อหาจริงๆ ฉันขอแนะนำหนังสือของ Michael Nielsen เกี่ยวกับการเรียนรู้เชิงลึกและโครงข่ายประสาทเทียมเป็นอย่างยิ่ง",
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "time_range": [
   1021.4799999999999,
   1029.1
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ในนั้น คุณจะพบโค้ดและข้อมูลที่จะดาวน์โหลดและเล่นสำหรับตัวอย่างที่ชัดเจนนี้ และหนังสือจะแนะนำคุณทีละขั้นตอนว่าโค้ดนั้นกำลังทำอะไรอยู่",
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "time_range": [
   1029.68,
   1038.36
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "สิ่งที่ยอดเยี่ยมคือหนังสือเล่มนี้ให้บริการฟรีและเผยแพร่ต่อสาธารณะ ดังนั้นหากคุณได้ประโยชน์จากหนังสือเล่มนี้ ลองมาร่วมบริจาคให้กับความพยายามของ Nielsen กับฉัน",
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "time_range": [
   1039.3,
   1047.66
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ฉันยังได้เชื่อมโยงแหล่งข้อมูลอื่นๆ สองสามอย่างที่ฉันชอบมากในคำอธิบาย รวมถึงบล็อกโพสต์ที่สวยงามและน่าอัศจรรย์โดย Chris Ola และบทความใน Distill",
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "time_range": [
   1047.66,
   1056.5
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "เพื่อปิดประเด็นในช่วงไม่กี่นาทีที่ผ่านมา ฉันอยากจะย้อนกลับไปดูตัวอย่างบทสัมภาษณ์ที่ฉันมีกับเลอิชา ลี",
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "time_range": [
   1058.28,
   1063.88
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "คุณอาจจำเธอได้จากวิดีโอที่แล้ว เธอทำงานระดับปริญญาเอกด้านการเรียนรู้เชิงลึก",
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "time_range": [
   1064.3,
   1067.72
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ในตัวอย่างเล็กๆ น้อยๆ นี้ เธอพูดถึงเอกสารสองฉบับล่าสุดที่เจาะลึกว่าเครือข่ายการจดจำรูปภาพที่ทันสมัยกว่าบางส่วนกำลังเรียนรู้จริง ๆ อย่างไร",
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "time_range": [
   1068.3,
   1075.78
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "เพื่อกำหนดจุดที่เราอยู่ในการสนทนา รายงานฉบับแรกได้นำหนึ่งในโครงข่ายประสาทเทียมระดับลึกพิเศษเหล่านี้ซึ่งมีความสามารถในการจดจำภาพได้ดีมาก และแทนที่จะฝึกกับชุดข้อมูลที่มีป้ายกำกับอย่างถูกต้อง กลับสับป้ายกำกับทั้งหมดก่อนการฝึกอบรม",
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "time_range": [
   1076.1200000000001,
   1088.74
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "แน่นอนว่าความแม่นยำในการทดสอบที่นี่ไม่ได้ดีไปกว่าการสุ่ม เนื่องจากทุกอย่างเป็นเพียงป้ายกำกับแบบสุ่ม แต่ก็ยังสามารถบรรลุความแม่นยำในการฝึกอบรมแบบเดียวกับที่คุณทำในชุดข้อมูลที่มีป้ายกำกับอย่างถูกต้อง",
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "time_range": [
   1089.48,
   1100.88
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "โดยพื้นฐานแล้ว น้ำหนักนับล้านสำหรับเครือข่ายนี้เพียงพอที่จะจดจำข้อมูลแบบสุ่ม ซึ่งทำให้เกิดคำถามว่าการลดฟังก์ชันต้นทุนนี้ให้เหลือน้อยที่สุดนั้นสอดคล้องกับโครงสร้างประเภทใด ๆ ในภาพหรือไม่ หรือเป็นเพียงการท่องจำเท่านั้น",
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "time_range": [
   1101.6000000000001,
   1116.4
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "หากคุณดูกราฟความแม่นยำนั้น หากคุณแค่ฝึกกับชุดข้อมูลสุ่ม เส้นโค้งนั้นจะลดลงอย่างช้าๆ ในลักษณะเชิงเส้นตรง ดังนั้นคุณจึงลำบากมากที่จะหาค่าต่ำสุดเฉพาะจุดที่เป็นไปได้ ตุ้มน้ำหนักที่เหมาะสมซึ่งจะทำให้คุณได้รับความแม่นยำนั้น",
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "time_range": [
   1131.44,
   1152.14
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ในขณะที่หากคุณกำลังฝึกชุดข้อมูลที่มีโครงสร้างจริงๆ ซึ่งเป็นชุดข้อมูลที่มีป้ายกำกับที่ถูกต้อง คุณจะลังเลเล็กน้อยในช่วงเริ่มต้น แต่แล้วคุณก็ลดลงอย่างรวดเร็วเพื่อไปถึงระดับความแม่นยำนั้น และในแง่หนึ่ง จะหาจุดสูงสุดในท้องถิ่นนั้นได้ง่ายกว่า",
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "time_range": [
   1152.24,
   1168.22
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ดังนั้นสิ่งที่น่าสนใจเกี่ยวกับเรื่องนี้ก็คือ ได้มีการนำเสนอรายงานอีกฉบับหนึ่งจากสองสามปีที่แล้ว ซึ่งมีการลดความซับซ้อนมากขึ้นมากเกี่ยวกับเลเยอร์เครือข่าย แต่ผลลัพธ์ประการหนึ่งก็คือ ถ้าคุณดูภาพรวมของการเพิ่มประสิทธิภาพ ค่าต่ำสุดเฉพาะที่เครือข่ายเหล่านี้มีแนวโน้มที่จะเรียนรู้นั้นแท้จริงแล้วมีคุณภาพเท่าเทียมกัน ดังนั้น ในแง่หนึ่งหากชุดข้อมูลของคุณมีโครงสร้าง คุณก็จะสามารถค้นหาสิ่งนั้นได้ง่ายกว่ามาก",
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "time_range": [
   1168.54,
   1194.32
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ฉันขอขอบคุณผู้ที่สนับสนุน Patreon เช่นเคย",
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "time_range": [
   1198.16,
   1201.18
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ฉันเคยพูดไปแล้วว่า Patreon ผู้เปลี่ยนเกมคืออะไร แต่วิดีโอเหล่านี้คงเป็นไปไม่ได้หากไม่มีคุณ",
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "time_range": [
   1201.52,
   1206.8
  ],
  "n_reviews": 0
 },
 {
  "translatedText": "ฉันยังอยากจะแสดงความขอบคุณเป็นพิเศษต่อบริษัท VC Amplify Partners ในการสนับสนุนวิดีโอเริ่มต้นเหล่านี้ในซีรีส์นี้",
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "time_range": [
   1207.46,
   1212.78
  ],
  "n_reviews": 0
 }
]