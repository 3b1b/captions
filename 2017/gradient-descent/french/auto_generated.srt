1
00:00:04,180 --> 00:00:07,280
Dans la dernière vidéo, j'ai présenté la structure d'un réseau de neurones.

2
00:00:07,680 --> 00:00:10,668
Je vais donner un bref récapitulatif ici pour que ce soit frais dans nos esprits, 

3
00:00:10,668 --> 00:00:12,600
puis j'ai deux objectifs principaux pour cette vidéo.

4
00:00:13,100 --> 00:00:15,309
La première consiste à introduire l’idée de descente de gradient, 

5
00:00:15,309 --> 00:00:17,921
qui sous-tend non seulement la façon dont les réseaux de neurones apprennent, 

6
00:00:17,921 --> 00:00:20,600
mais également le fonctionnement de nombreux autres apprentissages automatiques.

7
00:00:21,120 --> 00:00:24,371
Ensuite, nous approfondirons un peu plus le fonctionnement de ce réseau 

8
00:00:24,371 --> 00:00:27,940
particulier et ce que ces couches cachées de neurones finissent par rechercher.

9
00:00:28,980 --> 00:00:32,232
Pour rappel, notre objectif ici est l'exemple classique de la 

10
00:00:32,232 --> 00:00:36,220
reconnaissance de chiffres manuscrits, le bon monde des réseaux de neurones.

11
00:00:37,020 --> 00:00:39,924
Ces chiffres sont rendus sur une grille de 28 x 28 pixels, 

12
00:00:39,924 --> 00:00:43,420
chaque pixel ayant une valeur en niveaux de gris comprise entre 0 et 1.

13
00:00:43,820 --> 00:00:50,040
C’est ce qui détermine l’activation de 784 neurones dans la couche d’entrée du réseau.

14
00:00:51,180 --> 00:00:54,600
Et puis l’activation de chaque neurone dans les couches suivantes 

15
00:00:54,600 --> 00:00:58,954
est basée sur une somme pondérée de toutes les activations de la couche précédente, 

16
00:00:58,954 --> 00:01:00,820
plus un nombre spécial appelé biais.

17
00:01:02,160 --> 00:01:04,927
Ensuite, vous composez cette somme avec une autre fonction, 

18
00:01:04,927 --> 00:01:08,940
comme l'écrasement sigmoïde, ou un relu, comme je l'ai parcouru dans la dernière vidéo.

19
00:01:09,480 --> 00:01:14,427
Au total, étant donné le choix quelque peu arbitraire de deux couches cachées de 16 

20
00:01:14,427 --> 00:01:19,491
neurones chacune, le réseau a environ 13 000 poids et biais que nous pouvons ajuster, 

21
00:01:19,491 --> 00:01:24,380
et ce sont ces valeurs qui déterminent exactement ce que fait réellement le réseau.

22
00:01:24,880 --> 00:01:29,090
Alors ce que nous voulons dire lorsque nous disons que ce réseau classe un chiffre donné, 

23
00:01:29,090 --> 00:01:33,300
c'est que le plus brillant de ces 10 neurones de la couche finale correspond à ce chiffre.

24
00:01:34,100 --> 00:01:37,903
Et rappelez-vous, la motivation que nous avions en tête ici pour la structure en couches 

25
00:01:37,903 --> 00:01:40,851
était que peut-être la deuxième couche pourrait reprendre les bords, 

26
00:01:40,851 --> 00:01:44,526
et la troisième couche pourrait reprendre des motifs comme des boucles et des lignes, 

27
00:01:44,526 --> 00:01:47,176
et la dernière pourrait simplement reconstituer ces éléments. 

28
00:01:47,176 --> 00:01:48,800
modèles pour reconnaître les chiffres.

29
00:01:49,800 --> 00:01:52,240
Nous apprenons donc ici comment le réseau apprend.

30
00:01:52,640 --> 00:01:56,062
Ce que nous voulons, c'est un algorithme dans lequel vous pouvez montrer à 

31
00:01:56,062 --> 00:01:58,299
ce réseau tout un tas de données d'entraînement, 

32
00:01:58,299 --> 00:02:02,224
qui se présentent sous la forme d'un tas d'images différentes de chiffres manuscrits, 

33
00:02:02,224 --> 00:02:05,099
ainsi que des étiquettes indiquant ce qu'ils sont censés être, 

34
00:02:05,099 --> 00:02:08,705
et cela va ajuster ces 13 000 poids et biais afin d'améliorer ses performances 

35
00:02:08,705 --> 00:02:10,120
sur les données d'entraînement.

36
00:02:10,720 --> 00:02:13,582
Espérons que cette structure en couches signifie que ce qu’il 

37
00:02:13,582 --> 00:02:16,860
apprend se généralise aux images au-delà de ces données d’entraînement.

38
00:02:17,640 --> 00:02:20,550
La façon dont nous testons cela est qu'après avoir entraîné le réseau, 

39
00:02:20,550 --> 00:02:23,953
vous lui montrez davantage de données étiquetées qu'il n'a jamais vues auparavant, 

40
00:02:23,953 --> 00:02:26,700
et vous voyez avec quelle précision il classe ces nouvelles images.

41
00:02:31,120 --> 00:02:34,051
Heureusement pour nous, et ce qui en fait un exemple si courant, 

42
00:02:34,051 --> 00:02:37,163
c'est que les bonnes personnes derrière la base de données MNIST ont 

43
00:02:37,163 --> 00:02:40,907
rassemblé une collection de dizaines de milliers d'images de chiffres manuscrites, 

44
00:02:40,907 --> 00:02:44,200
chacune étiquetée avec les chiffres qu'elles sont censées indiquer. être.

45
00:02:44,900 --> 00:02:48,906
Et aussi provocateur que cela puisse être de décrire une machine comme un apprentissage, 

46
00:02:48,906 --> 00:02:51,112
une fois que vous voyez comment elle fonctionne, 

47
00:02:51,112 --> 00:02:54,624
cela ressemble beaucoup moins à une prémisse folle de science-fiction qu'à un 

48
00:02:54,624 --> 00:02:55,480
exercice de calcul.

49
00:02:56,200 --> 00:02:59,960
Je veux dire, en gros, cela revient à trouver le minimum d'une certaine fonction.

50
00:03:01,940 --> 00:03:05,972
Rappelez-vous, conceptuellement, nous considérons chaque neurone comme étant 

51
00:03:05,972 --> 00:03:08,800
connecté à tous les neurones de la couche précédente, 

52
00:03:08,800 --> 00:03:13,042
et les poids dans la somme pondérée définissant son activation sont un peu comme 

53
00:03:13,042 --> 00:03:17,231
les forces de ces connexions, et le biais est une indication de si ce neurone a 

54
00:03:17,231 --> 00:03:18,960
tendance à être actif ou inactif.

55
00:03:19,720 --> 00:03:22,017
Et pour commencer, nous allons simplement initialiser 

56
00:03:22,017 --> 00:03:24,400
tous ces poids et biais de manière totalement aléatoire.

57
00:03:24,940 --> 00:03:27,653
Inutile de dire que ce réseau fonctionnera assez horriblement sur un 

58
00:03:27,653 --> 00:03:30,720
exemple de formation donné, car il fait simplement quelque chose de aléatoire.

59
00:03:31,040 --> 00:03:33,478
Par exemple, vous introduisez cette image d'un 

60
00:03:33,478 --> 00:03:36,020
3 et la couche de sortie ressemble à un désordre.

61
00:03:36,600 --> 00:03:39,769
Donc, ce que vous faites, c'est définir une fonction de coût, 

62
00:03:39,769 --> 00:03:42,785
une façon de dire à l'ordinateur, non, mauvais ordinateur, 

63
00:03:42,785 --> 00:03:47,130
que la sortie devrait avoir des activations qui sont 0 pour la plupart des neurones, 

64
00:03:47,130 --> 00:03:50,760
mais 1 pour ce neurone, ce que vous m'avez donné est une pure poubelle.

65
00:03:51,720 --> 00:03:56,186
Pour dire cela un peu plus mathématiquement, vous additionnez les carrés des différences 

66
00:03:56,186 --> 00:04:00,603
entre chacune de ces activations de sortie de corbeille et la valeur que vous souhaitez 

67
00:04:00,603 --> 00:04:05,020
qu'elles aient, et c'est ce que nous appellerons le coût d'un seul exemple de formation.

68
00:04:05,960 --> 00:04:11,150
Notez que cette somme est petite lorsque le réseau classe correctement l'image en toute 

69
00:04:11,150 --> 00:04:16,399
confiance, mais elle est importante lorsque le réseau semble ne pas savoir ce qu'il fait.

70
00:04:18,640 --> 00:04:21,904
Vous devez donc considérer le coût moyen sur l’ensemble des 

71
00:04:21,904 --> 00:04:25,440
dizaines de milliers d’exemples de formation à votre disposition.

72
00:04:27,040 --> 00:04:29,890
Ce coût moyen est notre mesure de la mauvaise qualité 

73
00:04:29,890 --> 00:04:32,740
du réseau et de la mauvaise sensation de l'ordinateur.

74
00:04:33,420 --> 00:04:34,600
Et c'est une chose compliquée.

75
00:04:35,040 --> 00:04:39,318
Rappelez-vous que le réseau lui-même était fondamentalement une fonction, 

76
00:04:39,318 --> 00:04:43,307
une fonction qui prend en entrée 784 nombres, les valeurs de pixels, 

77
00:04:43,307 --> 00:04:46,140
et crache 10 nombres en sortie, et dans un sens, 

78
00:04:46,140 --> 00:04:48,800
il est paramétré par tous ces poids et biais ?

79
00:04:49,500 --> 00:04:52,820
Eh bien, la fonction de coût est en plus une couche de complexité.

80
00:04:53,100 --> 00:04:56,611
Il prend en entrée ces quelque 13 000 poids et biais, 

81
00:04:56,611 --> 00:05:01,162
et crache un seul chiffre décrivant la gravité de ces poids et biais, 

82
00:05:01,162 --> 00:05:06,429
et la façon dont il est défini dépend du comportement du réseau sur les dizaines 

83
00:05:06,429 --> 00:05:08,900
de milliers de données d'entraînement.

84
00:05:09,520 --> 00:05:11,000
Cela fait beaucoup de choses à penser.

85
00:05:12,400 --> 00:05:15,820
Mais il ne suffit pas de dire à l'ordinateur à quel point il fait un travail merdique.

86
00:05:16,220 --> 00:05:20,060
Vous voulez lui dire comment modifier ces pondérations et ces biais pour qu’il s’améliore.

87
00:05:20,780 --> 00:05:24,030
Pour faciliter les choses, plutôt que de lutter pour imaginer 

88
00:05:24,030 --> 00:05:27,071
une fonction avec 13 000 entrées, imaginez simplement une 

89
00:05:27,071 --> 00:05:30,480
fonction simple qui a un nombre en entrée et un nombre en sortie.

90
00:05:31,480 --> 00:05:35,300
Comment trouver une entrée qui minimise la valeur de cette fonction ?

91
00:05:36,460 --> 00:05:40,162
Les étudiants en calcul sauront que vous pouvez parfois déterminer ce minimum 

92
00:05:40,162 --> 00:05:44,007
explicitement, mais ce n'est pas toujours réalisable pour des fonctions vraiment 

93
00:05:44,007 --> 00:05:47,425
compliquées, certainement pas dans la version à 13 000 entrées de cette 

94
00:05:47,425 --> 00:05:51,080
situation pour notre fonction de coût de réseau neuronal compliquée et folle.

95
00:05:51,580 --> 00:05:55,414
Une tactique plus flexible consiste à commencer par n'importe quelle entrée et 

96
00:05:55,414 --> 00:05:59,200
à déterminer dans quelle direction vous devez aller pour réduire cette sortie.

97
00:06:00,080 --> 00:06:03,338
Plus précisément, si vous pouvez déterminer la pente de la fonction là 

98
00:06:03,338 --> 00:06:06,550
où vous vous trouvez, déplacez-vous vers la gauche si cette pente est 

99
00:06:06,550 --> 00:06:09,900
positive et déplacez l'entrée vers la droite si cette pente est négative.

100
00:06:11,960 --> 00:06:15,855
Si vous faites cela à plusieurs reprises, en vérifiant à chaque point la nouvelle pente 

101
00:06:15,855 --> 00:06:19,840
et en prenant l'étape appropriée, vous vous approcherez d'un minimum local de la fonction.

102
00:06:20,640 --> 00:06:23,800
L’image que vous avez peut-être en tête ici est celle d’une balle qui dévale une colline.

103
00:06:24,620 --> 00:06:27,686
Remarquez que même pour cette fonction à entrée unique très simplifiée, 

104
00:06:27,686 --> 00:06:31,179
il existe de nombreuses vallées possibles dans lesquelles vous pourriez atterrir, 

105
00:06:31,179 --> 00:06:33,777
en fonction de l'entrée aléatoire à laquelle vous commencez, 

106
00:06:33,777 --> 00:06:37,483
et rien ne garantit que le minimum local dans lequel vous atterrirez sera la valeur la 

107
00:06:37,483 --> 00:06:39,400
plus petite possible. de la fonction de coût.

108
00:06:40,220 --> 00:06:42,620
Cela se répercutera également sur notre cas de réseau neuronal.

109
00:06:43,180 --> 00:06:46,886
Je veux également que vous remarquiez que si vous rendez la taille de vos 

110
00:06:46,886 --> 00:06:50,693
pas proportionnelle à la pente, lorsque la pente s'aplatit vers le minimum, 

111
00:06:50,693 --> 00:06:54,600
vos pas deviennent de plus en plus petits, ce qui vous aide à ne pas dépasser.

112
00:06:55,940 --> 00:06:58,641
En augmentant un peu la complexité, imaginez plutôt 

113
00:06:58,641 --> 00:07:00,980
une fonction avec deux entrées et une sortie.

114
00:07:01,500 --> 00:07:04,820
Vous pourriez considérer l’espace d’entrée comme le plan xy et la fonction de 

115
00:07:04,820 --> 00:07:08,140
coût comme étant représentée graphiquement comme une surface au-dessus de lui.

116
00:07:08,760 --> 00:07:11,646
Au lieu de poser des questions sur la pente de la fonction, 

117
00:07:11,646 --> 00:07:15,062
vous devez vous demander dans quelle direction vous devez marcher dans 

118
00:07:15,062 --> 00:07:18,960
cet espace d'entrée afin de diminuer le plus rapidement la sortie de la fonction.

119
00:07:19,720 --> 00:07:21,760
En d’autres termes, quelle est la direction de la descente ?

120
00:07:22,380 --> 00:07:25,560
Encore une fois, il est utile de penser à une balle qui dévale cette colline.

121
00:07:26,660 --> 00:07:30,632
Ceux d'entre vous qui sont familiers avec le calcul multivarié sauront que la 

122
00:07:30,632 --> 00:07:34,349
pente d'une fonction vous donne la direction de la montée la plus raide, 

123
00:07:34,349 --> 00:07:38,780
dans quelle direction devez-vous avancer pour augmenter la fonction le plus rapidement.

124
00:07:39,560 --> 00:07:42,720
Naturellement, prendre le négatif de ce gradient vous donne 

125
00:07:42,720 --> 00:07:46,040
la direction du pas qui diminue la fonction le plus rapidement.

126
00:07:47,240 --> 00:07:50,345
Plus encore, la longueur de ce vecteur gradient 

127
00:07:50,345 --> 00:07:53,840
indique à quel point la pente la plus raide est raide.

128
00:07:54,540 --> 00:07:57,440
Si vous n'êtes pas familier avec le calcul multivarié et souhaitez en savoir plus, 

129
00:07:57,440 --> 00:08:00,340
consultez certains des travaux que j'ai réalisés pour la Khan Academy sur le sujet.

130
00:08:00,860 --> 00:08:04,050
Honnêtement, tout ce qui compte pour vous et moi en ce moment, 

131
00:08:04,050 --> 00:08:07,342
c'est qu'en principe, il existe un moyen de calculer ce vecteur, 

132
00:08:07,342 --> 00:08:11,900
ce vecteur qui vous indique quelle est la direction de la descente et quelle est sa pente.

133
00:08:12,400 --> 00:08:14,260
Tout ira bien si c'est tout ce que vous savez 

134
00:08:14,260 --> 00:08:16,120
et que vous n'êtes pas solide sur les détails.

135
00:08:17,200 --> 00:08:20,348
Si vous pouvez obtenir cela, l'algorithme permettant de minimiser 

136
00:08:20,348 --> 00:08:23,257
la fonction consiste à calculer cette direction de gradient, 

137
00:08:23,257 --> 00:08:26,740
puis à faire un petit pas en descente et à répéter cela encore et encore.

138
00:08:27,700 --> 00:08:30,397
C'est la même idée de base pour une fonction qui 

139
00:08:30,397 --> 00:08:32,820
possède 13 000 entrées au lieu de 2 entrées.

140
00:08:33,400 --> 00:08:36,596
Imaginez organiser les 13 000 poids et biais de 

141
00:08:36,596 --> 00:08:39,460
notre réseau dans un vecteur colonne géant.

142
00:08:40,140 --> 00:08:43,673
Le gradient négatif de la fonction de coût n'est qu'un vecteur, 

143
00:08:43,673 --> 00:08:48,531
c'est une direction à l'intérieur de cet espace d'entrée incroyablement énorme qui vous 

144
00:08:48,531 --> 00:08:53,223
indique quels coups de pouce à tous ces nombres vont provoquer la diminution la plus 

145
00:08:53,223 --> 00:08:54,880
rapide de la fonction de coût.

146
00:08:55,640 --> 00:08:58,694
Et bien sûr, grâce à notre fonction de coût spécialement conçue, 

147
00:08:58,694 --> 00:09:02,454
modifier les pondérations et les biais pour les diminuer signifie que la sortie 

148
00:09:02,454 --> 00:09:06,026
du réseau sur chaque élément de données d'entraînement ressemble moins à un 

149
00:09:06,026 --> 00:09:10,162
tableau aléatoire de 10 valeurs, mais plutôt à une décision réelle que nous souhaitons. 

150
00:09:10,162 --> 00:09:10,820
c'est à faire.

151
00:09:11,440 --> 00:09:14,584
Il est important de se rappeler que cette fonction de coût implique une 

152
00:09:14,584 --> 00:09:17,816
moyenne sur toutes les données d'entraînement, donc si vous la minimisez, 

153
00:09:17,816 --> 00:09:21,180
cela signifie que les performances sont meilleures sur tous ces échantillons.

154
00:09:23,820 --> 00:09:26,709
L'algorithme permettant de calculer efficacement ce gradient, 

155
00:09:26,709 --> 00:09:29,925
qui est en fait au cœur de la façon dont un réseau neuronal apprend, 

156
00:09:29,925 --> 00:09:33,980
s'appelle la rétropropagation, et c'est ce dont je vais parler dans la prochaine vidéo.

157
00:09:34,660 --> 00:09:38,741
Là, je veux vraiment prendre le temps d'examiner ce qui arrive exactement à chaque 

158
00:09:38,741 --> 00:09:41,396
poids et biais pour une donnée d'entraînement donnée, 

159
00:09:41,396 --> 00:09:45,428
en essayant de donner une idée intuitive de ce qui se passe au-delà de la pile de 

160
00:09:45,428 --> 00:09:47,100
calculs et de formules pertinents.

161
00:09:47,780 --> 00:09:50,548
Ici, maintenant, la principale chose que je veux que vous sachiez, 

162
00:09:50,548 --> 00:09:54,061
indépendamment des détails de mise en œuvre, c'est que ce que nous entendons lorsque 

163
00:09:54,061 --> 00:09:57,492
nous parlons d'apprentissage en réseau, c'est qu'il s'agit simplement de minimiser 

164
00:09:57,492 --> 00:09:58,360
une fonction de coût.

165
00:09:59,300 --> 00:10:02,134
Et remarquez, une des conséquences de cela est qu'il est important 

166
00:10:02,134 --> 00:10:04,800
que cette fonction de coût ait un résultat agréable et fluide, 

167
00:10:04,800 --> 00:10:08,100
afin que nous puissions trouver un minimum local en descendant par petits pas.

168
00:10:09,260 --> 00:10:13,546
C’est d’ailleurs pourquoi les neurones artificiels ont des activations continues, 

169
00:10:13,546 --> 00:10:17,101
plutôt que d’être simplement actifs ou inactifs de manière binaire, 

170
00:10:17,101 --> 00:10:19,140
comme le sont les neurones biologiques.

171
00:10:20,220 --> 00:10:23,377
Ce processus consistant à pousser à plusieurs reprises l'entrée d'une 

172
00:10:23,377 --> 00:10:26,760
fonction d'un multiple du gradient négatif est appelé descente de gradient.

173
00:10:27,300 --> 00:10:30,583
C'est un moyen de converger vers un minimum local d'une fonction de coût, 

174
00:10:30,583 --> 00:10:32,580
essentiellement une vallée dans ce graphique.

175
00:10:33,440 --> 00:10:36,780
Je montre toujours l'image d'une fonction avec deux entrées, bien sûr, 

176
00:10:36,780 --> 00:10:40,308
car les coups de pouce dans un espace d'entrée à 13 000 dimensions sont un 

177
00:10:40,308 --> 00:10:44,260
peu difficiles à comprendre, mais il existe une belle façon non spatiale d'y penser.

178
00:10:45,080 --> 00:10:48,440
Chaque composante du gradient négatif nous dit deux choses.

179
00:10:49,060 --> 00:10:52,076
Le signe, bien sûr, nous indique si la composante correspondante 

180
00:10:52,076 --> 00:10:55,140
du vecteur d’entrée doit être poussée vers le haut ou vers le bas.

181
00:10:55,800 --> 00:10:59,318
Mais surtout, les ampleurs relatives de tous ces composants 

182
00:10:59,318 --> 00:11:02,720
vous indiquent quels changements sont les plus importants.

183
00:11:05,220 --> 00:11:09,153
Vous voyez, dans notre réseau, un ajustement à l’un des poids peut avoir un impact 

184
00:11:09,153 --> 00:11:13,040
beaucoup plus important sur la fonction de coût que l’ajustement à un autre poids.

185
00:11:14,800 --> 00:11:16,535
Certaines de ces connexions sont tout simplement 

186
00:11:16,535 --> 00:11:18,200
plus importantes pour nos données de formation.

187
00:11:19,320 --> 00:11:23,386
Donc, une façon de penser à ce vecteur gradient de notre fonction de coût 

188
00:11:23,386 --> 00:11:28,113
incroyablement massive est qu'il code l'importance relative de chaque poids et biais, 

189
00:11:28,113 --> 00:11:32,400
c'est-à-dire lequel de ces changements va rapporter le plus pour votre argent.

190
00:11:33,620 --> 00:11:36,640
Il s’agit en réalité d’une autre façon de penser la direction.

191
00:11:37,100 --> 00:11:41,309
Pour prendre un exemple plus simple, si vous avez une fonction avec deux variables en 

192
00:11:41,309 --> 00:11:45,225
entrée et que vous calculez que son gradient à un point particulier est de 3,1, 

193
00:11:45,225 --> 00:11:49,337
alors d'une part vous pouvez interpréter cela comme disant que lorsque vous Si vous 

194
00:11:49,337 --> 00:11:53,498
vous trouvez à cette entrée, vous déplacer dans cette direction augmente la fonction 

195
00:11:53,498 --> 00:11:57,609
le plus rapidement, et lorsque vous représentez graphiquement la fonction au-dessus 

196
00:11:57,609 --> 00:12:01,868
du plan des points d'entrée, ce vecteur est ce qui vous donne la direction droite vers 

197
00:12:01,868 --> 00:12:02,260
le haut.

198
00:12:02,860 --> 00:12:06,534
Mais une autre façon de lire cela est de dire que les modifications apportées 

199
00:12:06,534 --> 00:12:10,162
à cette première variable ont 3 fois plus d'importance que les modifications 

200
00:12:10,162 --> 00:12:14,026
apportées à la deuxième variable, qu'au moins au voisinage de l'entrée concernée, 

201
00:12:14,026 --> 00:12:16,900
pousser la valeur x a beaucoup plus d'impact pour votre mâle.

202
00:12:19,880 --> 00:12:22,340
Faisons un zoom arrière et résumons où nous en sommes jusqu'à présent.

203
00:12:22,840 --> 00:12:26,971
Le réseau lui-même est cette fonction avec 784 entrées et 10 sorties, 

204
00:12:26,971 --> 00:12:30,040
définies en fonction de toutes ces sommes pondérées.

205
00:12:30,640 --> 00:12:33,680
La fonction de coût est en outre une couche de complexité supplémentaire.

206
00:12:33,980 --> 00:12:37,754
Il prend les 13 000 poids et biais comme entrées et génère 

207
00:12:37,754 --> 00:12:41,720
une seule mesure de moche basée sur les exemples de formation.

208
00:12:42,440 --> 00:12:44,717
Et le gradient de la fonction de coût constitue 

209
00:12:44,717 --> 00:12:46,900
encore un niveau de complexité supplémentaire.

210
00:12:47,360 --> 00:12:50,749
Il nous indique quels coups de pouce à tous ces poids et biais provoquent le 

211
00:12:50,749 --> 00:12:53,522
changement le plus rapide de la valeur de la fonction de coût, 

212
00:12:53,522 --> 00:12:56,867
ce que vous pourriez interpréter comme indiquant quels changements et quels 

213
00:12:56,867 --> 00:12:57,880
poids comptent le plus.

214
00:13:02,560 --> 00:13:06,024
Ainsi, lorsque vous initialisez le réseau avec des poids et des biais aléatoires et 

215
00:13:06,024 --> 00:13:09,694
que vous les ajustez plusieurs fois en fonction de ce processus de descente de gradient, 

216
00:13:09,694 --> 00:13:13,200
dans quelle mesure fonctionne-t-il réellement sur des images jamais vues auparavant ?

217
00:13:14,100 --> 00:13:18,527
Celui que j'ai décrit ici, avec les deux couches cachées de 16 neurones chacune, 

218
00:13:18,527 --> 00:13:22,298
choisies principalement pour des raisons esthétiques, n'est pas mal, 

219
00:13:22,298 --> 00:13:25,960
classant correctement environ 96 % des nouvelles images qu'il voit.

220
00:13:26,680 --> 00:13:30,239
Et honnêtement, si vous regardez certains des exemples sur lesquels il se trompe, 

221
00:13:30,239 --> 00:13:32,540
vous vous sentez obligé de prendre un peu de relâche.

222
00:13:36,220 --> 00:13:39,031
Maintenant, si vous jouez avec la structure des couches cachées et 

223
00:13:39,031 --> 00:13:41,760
effectuez quelques ajustements, vous pouvez obtenir jusqu'à 98 %.

224
00:13:41,760 --> 00:13:42,720
Et c'est plutôt bien !

225
00:13:43,020 --> 00:13:46,870
Ce n'est pas le meilleur, vous pouvez certainement obtenir de meilleures performances 

226
00:13:46,870 --> 00:13:49,153
en devenant plus sophistiqué que ce réseau simple, 

227
00:13:49,153 --> 00:13:51,794
mais étant donné à quel point la tâche initiale est ardue, 

228
00:13:51,794 --> 00:13:55,420
je pense qu'il y a quelque chose d'incroyable à ce qu'un réseau fasse aussi bien 

229
00:13:55,420 --> 00:13:57,614
sur des images qu'il n'a jamais vues auparavant, 

230
00:13:57,614 --> 00:14:01,420
étant donné que nous ne lui avons jamais dit spécifiquement quels modèles rechercher.

231
00:14:02,560 --> 00:14:06,163
À l'origine, la façon dont j'ai motivé cette structure était en décrivant un espoir que 

232
00:14:06,163 --> 00:14:09,317
nous pourrions avoir, que la deuxième couche puisse capter les petits bords, 

233
00:14:09,317 --> 00:14:12,757
que la troisième couche rassemblerait ces bords pour reconnaître les boucles et les 

234
00:14:12,757 --> 00:14:15,582
lignes plus longues, et que celles-ci pourraient être reconstituées. 

235
00:14:15,582 --> 00:14:17,180
ensemble pour reconnaître les chiffres.

236
00:14:17,960 --> 00:14:20,400
Alors, est-ce réellement ce que fait notre réseau ?

237
00:14:21,080 --> 00:14:24,400
Eh bien, pour celui-ci au moins, pas du tout.

238
00:14:24,820 --> 00:14:26,790
Rappelez-vous comment, dans la dernière vidéo, 

239
00:14:26,790 --> 00:14:29,850
nous avons regardé comment les poids des connexions de tous les neurones 

240
00:14:29,850 --> 00:14:32,826
de la première couche à un neurone donné de la deuxième couche peuvent 

241
00:14:32,826 --> 00:14:35,844
être visualisés sous la forme d'un motif de pixels donné que le neurone 

242
00:14:35,844 --> 00:14:37,060
de la deuxième couche capte ?

243
00:14:37,780 --> 00:14:42,662
Eh bien, lorsque nous faisons cela pour les poids associés à ces transitions, 

244
00:14:42,662 --> 00:14:47,920
de la première couche à la suivante, au lieu de détecter de petits bords isolés ici 

245
00:14:47,920 --> 00:14:53,054
et là, ils semblent presque aléatoires, avec juste des motifs très lâches dans le 

246
00:14:53,054 --> 00:14:53,680
milieu là.

247
00:14:53,760 --> 00:14:57,271
Il semblerait que dans l'espace insondable de 13 000 dimensions de 

248
00:14:57,271 --> 00:15:01,097
pondérations et de biais possibles, notre réseau s'est trouvé un heureux 

249
00:15:01,097 --> 00:15:05,500
petit minimum local qui, malgré la classification réussie de la plupart des images, 

250
00:15:05,500 --> 00:15:08,960
ne reprend pas exactement les modèles que nous aurions pu espérer.

251
00:15:09,780 --> 00:15:11,760
Et pour bien comprendre ce point, regardez ce qui 

252
00:15:11,760 --> 00:15:13,820
se passe lorsque vous saisissez une image aléatoire.

253
00:15:14,320 --> 00:15:19,107
Si le système était intelligent, vous pourriez vous attendre à ce qu'il semble incertain, 

254
00:15:19,107 --> 00:15:23,096
n'activant peut-être pas vraiment l'un de ces 10 neurones de sortie ou les 

255
00:15:23,096 --> 00:15:26,128
activant tous de manière uniforme, mais au lieu de cela, 

256
00:15:26,128 --> 00:15:29,000
il vous donne en toute confiance une réponse absurde, 

257
00:15:29,000 --> 00:15:32,191
comme s'il était aussi sûr que ce bruit aléatoire est un 5, 

258
00:15:32,191 --> 00:15:34,160
car une image réelle d'un 5 est un 5.

259
00:15:34,540 --> 00:15:38,567
Autrement dit, même si ce réseau reconnaît assez bien les chiffres, 

260
00:15:38,567 --> 00:15:40,700
il ne sait pas comment les dessiner.

261
00:15:41,420 --> 00:15:43,409
Cela est dû en grande partie au fait qu’il s’agit 

262
00:15:43,409 --> 00:15:45,240
d’une configuration de formation très limitée.

263
00:15:45,880 --> 00:15:47,740
Je veux dire, mettez-vous à la place du réseau ici.

264
00:15:48,140 --> 00:15:52,417
De son point de vue, l’univers entier n’est constitué que de chiffres immobiles 

265
00:15:52,417 --> 00:15:55,465
clairement définis et centrés dans une minuscule grille, 

266
00:15:55,465 --> 00:15:59,582
et sa fonction de coût ne l’a jamais incité à être autre chose qu’absolument 

267
00:15:59,582 --> 00:16:01,080
confiant dans ses décisions.

268
00:16:02,120 --> 00:16:05,446
Donc, avec cela comme image de ce que font réellement ces neurones de deuxième couche, 

269
00:16:05,446 --> 00:16:08,046
vous pourriez vous demander pourquoi j'introduireais ce réseau avec 

270
00:16:08,046 --> 00:16:09,920
la motivation de capter les bords et les modèles.

271
00:16:09,920 --> 00:16:12,300
Je veux dire, ce n’est tout simplement pas du tout ce que cela finit par faire.

272
00:16:13,380 --> 00:16:17,180
Eh bien, ce n’est pas notre objectif final, mais plutôt un point de départ.

273
00:16:17,640 --> 00:16:19,959
Franchement, il s’agit d’une technologie ancienne, 

274
00:16:19,959 --> 00:16:23,370
du type étudié dans les années 80 et 90, et vous devez la comprendre avant 

275
00:16:23,370 --> 00:16:26,190
de pouvoir comprendre des variantes modernes plus détaillées, 

276
00:16:26,190 --> 00:16:29,646
et elle est clairement capable de résoudre certains problèmes intéressants, 

277
00:16:29,646 --> 00:16:33,375
mais plus vous approfondissez ce que ces couches cachées font vraiment l'affaire, 

278
00:16:33,375 --> 00:16:34,740
moins cela semble intelligent.

279
00:16:38,480 --> 00:16:41,109
En déplaçant un instant l'attention de la façon dont les réseaux apprennent 

280
00:16:41,109 --> 00:16:43,566
vers la façon dont vous apprenez, cela ne se produira que si vous vous 

281
00:16:43,566 --> 00:16:46,300
engagez activement avec le matériel présenté ici, d'une manière ou d'une autre.

282
00:16:47,060 --> 00:16:50,458
Une chose assez simple que je veux que vous fassiez est de faire une pause 

283
00:16:50,458 --> 00:16:53,902
maintenant et de réfléchir profondément un instant aux changements que vous 

284
00:16:53,902 --> 00:16:57,391
pourriez apporter à ce système et à la manière dont il perçoit les images si 

285
00:16:57,391 --> 00:17:00,880
vous vouliez qu'il capte mieux des éléments tels que les bords et les motifs.

286
00:17:01,480 --> 00:17:04,204
Mais mieux que cela, pour réellement approfondir le sujet, 

287
00:17:04,204 --> 00:17:07,991
je recommande vivement le livre de Michael Nielsen sur l'apprentissage profond et 

288
00:17:07,991 --> 00:17:09,099
les réseaux de neurones.

289
00:17:09,680 --> 00:17:14,098
Vous y trouverez le code et les données à télécharger et avec lesquelles jouer pour 

290
00:17:14,098 --> 00:17:18,359
cet exemple précis, et le livre vous guidera étape par étape ce que fait ce code.

291
00:17:19,300 --> 00:17:22,548
Ce qui est génial, c'est que ce livre est gratuit et accessible au public, 

292
00:17:22,548 --> 00:17:25,277
donc si vous en retirez quelque chose, pensez à vous joindre à 

293
00:17:25,277 --> 00:17:27,660
moi pour faire un don en faveur des efforts de Nielsen.

294
00:17:27,660 --> 00:17:31,909
J'ai également lié quelques autres ressources que j'aime beaucoup dans la description, 

295
00:17:31,909 --> 00:17:34,692
y compris le magnifique et phénoménal article de blog de 

296
00:17:34,692 --> 00:17:36,500
Chris Ola et les articles de Distill.

297
00:17:38,280 --> 00:17:41,150
Pour conclure ici ces dernières minutes, je voudrais revenir 

298
00:17:41,150 --> 00:17:43,880
sur un extrait de l'entretien que j'ai eu avec Leisha Lee.

299
00:17:44,300 --> 00:17:46,165
Vous vous souvenez peut-être d'elle dans la dernière vidéo, 

300
00:17:46,165 --> 00:17:47,720
elle a fait son doctorat en apprentissage profond.

301
00:17:48,300 --> 00:17:50,729
Dans ce petit extrait, elle parle de deux articles récents qui 

302
00:17:50,729 --> 00:17:53,196
approfondissent réellement la manière dont certains des réseaux 

303
00:17:53,196 --> 00:17:55,780
de reconnaissance d’images les plus modernes apprennent réellement.

304
00:17:56,120 --> 00:17:58,400
Juste pour situer où nous en étions dans la conversation, 

305
00:17:58,400 --> 00:18:01,624
le premier article a pris l'un de ces réseaux neuronaux particulièrement profonds 

306
00:18:01,624 --> 00:18:03,550
qui est vraiment bon en reconnaissance d'images, 

307
00:18:03,550 --> 00:18:06,538
et au lieu de l'entraîner sur un ensemble de données correctement étiqueté, 

308
00:18:06,538 --> 00:18:08,740
il a mélangé toutes les étiquettes avant l'entraînement.

309
00:18:09,480 --> 00:18:12,978
De toute évidence, la précision des tests ici n'était pas meilleure que celle du hasard, 

310
00:18:12,978 --> 00:18:14,865
puisque tout est étiqueté de manière aléatoire, 

311
00:18:14,865 --> 00:18:17,735
mais il était toujours possible d'obtenir la même précision de formation 

312
00:18:17,735 --> 00:18:20,880
que celle que vous obtiendriez sur un ensemble de données correctement étiqueté.

313
00:18:21,600 --> 00:18:25,480
Fondamentalement, les millions de poids pour ce réseau particulier étaient suffisants 

314
00:18:25,480 --> 00:18:27,962
pour qu'il mémorise simplement les données aléatoires, 

315
00:18:27,962 --> 00:18:31,662
ce qui soulève la question de savoir si la minimisation de cette fonction de coût 

316
00:18:31,662 --> 00:18:34,414
correspond réellement à une sorte de structure dans l'image, 

317
00:18:34,414 --> 00:18:36,400
ou s'agit-il simplement d'une mémorisation ?

318
00:18:51,440 --> 00:18:56,456
Si vous regardez cette courbe de précision, si vous vous entraîniez simplement 

319
00:18:56,456 --> 00:19:01,472
sur un ensemble de données aléatoires, cette courbe descendait très lentement, 

320
00:19:01,472 --> 00:19:06,615
de manière presque linéaire, donc vous avez vraiment du mal à trouver ce minimum 

321
00:19:06,615 --> 00:19:12,140
local de possible, vous savez. , les bons poids qui vous apporteraient cette précision.

322
00:19:12,240 --> 00:19:16,466
Alors que si vous vous entraînez réellement sur un ensemble de données structuré, 

323
00:19:16,466 --> 00:19:19,662
qui a les bonnes étiquettes, vous bidouillez un peu au début, 

324
00:19:19,662 --> 00:19:23,993
mais vous avez ensuite chuté très rapidement pour atteindre ce niveau de précision, 

325
00:19:23,993 --> 00:19:28,220
et donc dans un certain sens, cela était plus facile de trouver ces maxima locaux.

326
00:19:28,540 --> 00:19:32,602
Et ce qui était également intéressant, c'est que cela met en lumière un autre 

327
00:19:32,602 --> 00:19:37,133
article datant d'il y a quelques années, qui contient beaucoup plus de simplifications 

328
00:19:37,133 --> 00:19:41,247
sur les couches réseau, mais l'un des résultats disait que si vous regardez le 

329
00:19:41,247 --> 00:19:45,310
paysage de l'optimisation, les minimums locaux que ces réseaux ont tendance à 

330
00:19:45,310 --> 00:19:49,007
apprendre sont en réalité de qualité égale, donc dans un certain sens, 

331
00:19:49,007 --> 00:19:53,486
si votre ensemble de données est structuré, vous devriez pouvoir les trouver beaucoup 

332
00:19:53,486 --> 00:19:54,320
plus facilement.

333
00:19:58,160 --> 00:20:01,180
Mes remerciements, comme toujours, à ceux d'entre vous qui soutiennent Patreon.

334
00:20:01,520 --> 00:20:03,949
J'ai déjà dit à quel point Patreon change la donne, 

335
00:20:03,949 --> 00:20:06,800
mais ces vidéos ne seraient vraiment pas possibles sans vous.

336
00:20:07,460 --> 00:20:10,330
Je tiens également à remercier tout particulièrement la société de capital-risque 

337
00:20:10,330 --> 00:20:12,780
Amplify Partners, pour son soutien à ces premières vidéos de la série.

