[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "Dans la dernière vidéo, j'ai présenté la structure d'un réseau de neurones.",
  "n_reviews": 0
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "Je vais donner un bref récapitulatif ici pour que ce soit frais dans nos esprits, puis j'ai deux objectifs principaux pour cette vidéo.",
  "n_reviews": 0
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "La première consiste à introduire l’idée de descente de gradient, qui sous-tend non seulement la façon dont les réseaux de neurones apprennent, mais également le fonctionnement de nombreux autres apprentissages automatiques.",
  "n_reviews": 0
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "Ensuite, nous approfondirons un peu plus le fonctionnement de ce réseau particulier et ce que ces couches cachées de neurones finissent par rechercher.",
  "n_reviews": 0
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "Pour rappel, notre objectif ici est l'exemple classique de la reconnaissance de chiffres manuscrits, le bon monde des réseaux de neurones.",
  "n_reviews": 0
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "Ces chiffres sont rendus sur une grille de 28 x 28 pixels, chaque pixel ayant une valeur en niveaux de gris comprise entre 0 et 1.",
  "n_reviews": 0
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "C’est ce qui détermine l’activation de 784 neurones dans la couche d’entrée du réseau.",
  "n_reviews": 0
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "Et puis l’activation de chaque neurone dans les couches suivantes est basée sur une somme pondérée de toutes les activations de la couche précédente, plus un nombre spécial appelé biais.",
  "n_reviews": 0
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "Ensuite, vous composez cette somme avec une autre fonction, comme l'écrasement sigmoïde, ou un relu, comme je l'ai parcouru dans la dernière vidéo.",
  "n_reviews": 0
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "Au total, étant donné le choix quelque peu arbitraire de deux couches cachées de 16 neurones chacune, le réseau a environ 13 000 poids et biais que nous pouvons ajuster, et ce sont ces valeurs qui déterminent exactement ce que fait réellement le réseau.",
  "n_reviews": 0
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "Alors ce que nous voulons dire lorsque nous disons que ce réseau classe un chiffre donné, c'est que le plus brillant de ces 10 neurones de la couche finale correspond à ce chiffre.",
  "n_reviews": 0
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "Et rappelez-vous, la motivation que nous avions en tête ici pour la structure en couches était que peut-être la deuxième couche pourrait reprendre les bords, et la troisième couche pourrait reprendre des motifs comme des boucles et des lignes, et la dernière pourrait simplement reconstituer ces éléments. modèles pour reconnaître les chiffres.",
  "n_reviews": 0
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "Nous apprenons donc ici comment le réseau apprend.",
  "n_reviews": 0
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "Ce que nous voulons, c'est un algorithme dans lequel vous pouvez montrer à ce réseau tout un tas de données d'entraînement, qui se présentent sous la forme d'un tas d'images différentes de chiffres manuscrits, ainsi que des étiquettes indiquant ce qu'ils sont censés être, et cela va ajuster ces 13 000 poids et biais afin d'améliorer ses performances sur les données d'entraînement.",
  "n_reviews": 0
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "Espérons que cette structure en couches signifie que ce qu’il apprend se généralise aux images au-delà de ces données d’entraînement.",
  "n_reviews": 0
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "La façon dont nous testons cela est qu'après avoir entraîné le réseau, vous lui montrez davantage de données étiquetées qu'il n'a jamais vues auparavant, et vous voyez avec quelle précision il classe ces nouvelles images.",
  "n_reviews": 0
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "Heureusement pour nous, et ce qui en fait un exemple si courant, c'est que les bonnes personnes derrière la base de données MNIST ont rassemblé une collection de dizaines de milliers d'images de chiffres manuscrites, chacune étiquetée avec les chiffres qu'elles sont censées indiquer. être.",
  "n_reviews": 0
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "Et aussi provocateur que cela puisse être de décrire une machine comme un apprentissage, une fois que vous voyez comment elle fonctionne, cela ressemble beaucoup moins à une prémisse folle de science-fiction qu'à un exercice de calcul.",
  "n_reviews": 0
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "Je veux dire, en gros, cela revient à trouver le minimum d'une certaine fonction.",
  "n_reviews": 0
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "Rappelez-vous, conceptuellement, nous considérons chaque neurone comme étant connecté à tous les neurones de la couche précédente, et les poids dans la somme pondérée définissant son activation sont un peu comme les forces de ces connexions, et le biais est une indication de si ce neurone a tendance à être actif ou inactif.",
  "n_reviews": 0
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "Et pour commencer, nous allons simplement initialiser tous ces poids et biais de manière totalement aléatoire.",
  "n_reviews": 0
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "Inutile de dire que ce réseau fonctionnera assez horriblement sur un exemple de formation donné, car il fait simplement quelque chose de aléatoire.",
  "n_reviews": 0
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "Par exemple, vous introduisez cette image d'un 3 et la couche de sortie ressemble à un désordre.",
  "n_reviews": 0
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "Donc, ce que vous faites, c'est définir une fonction de coût, une façon de dire à l'ordinateur, non, mauvais ordinateur, que la sortie devrait avoir des activations qui sont 0 pour la plupart des neurones, mais 1 pour ce neurone, ce que vous m'avez donné est une pure poubelle.",
  "n_reviews": 0
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "Pour dire cela un peu plus mathématiquement, vous additionnez les carrés des différences entre chacune de ces activations de sortie de corbeille et la valeur que vous souhaitez qu'elles aient, et c'est ce que nous appellerons le coût d'un seul exemple de formation.",
  "n_reviews": 0
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "Notez que cette somme est petite lorsque le réseau classe correctement l'image en toute confiance, mais elle est importante lorsque le réseau semble ne pas savoir ce qu'il fait.",
  "n_reviews": 0
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "Vous devez donc considérer le coût moyen sur l’ensemble des dizaines de milliers d’exemples de formation à votre disposition.",
  "n_reviews": 0
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "Ce coût moyen est notre mesure de la mauvaise qualité du réseau et de la mauvaise sensation de l'ordinateur.",
  "n_reviews": 0
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "Et c'est une chose compliquée.",
  "n_reviews": 0
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "Rappelez-vous que le réseau lui-même était fondamentalement une fonction, une fonction qui prend en entrée 784 nombres, les valeurs de pixels, et crache 10 nombres en sortie, et dans un sens, il est paramétré par tous ces poids et biais ?",
  "n_reviews": 0
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "Eh bien, la fonction de coût est en plus une couche de complexité.",
  "n_reviews": 0
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "Il prend en entrée ces quelque 13 000 poids et biais, et crache un seul chiffre décrivant la gravité de ces poids et biais, et la façon dont il est défini dépend du comportement du réseau sur les dizaines de milliers de données d'entraînement.",
  "n_reviews": 0
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "Cela fait beaucoup de choses à penser.",
  "n_reviews": 0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "Mais il ne suffit pas de dire à l'ordinateur à quel point il fait un travail merdique.",
  "n_reviews": 0
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "Vous voulez lui dire comment modifier ces pondérations et ces biais pour qu’il s’améliore.",
  "n_reviews": 0
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "Pour faciliter les choses, plutôt que de lutter pour imaginer une fonction avec 13 000 entrées, imaginez simplement une fonction simple qui a un nombre en entrée et un nombre en sortie.",
  "n_reviews": 0
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "Comment trouver une entrée qui minimise la valeur de cette fonction ?",
  "n_reviews": 0
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "Les étudiants en calcul sauront que vous pouvez parfois déterminer ce minimum explicitement, mais ce n'est pas toujours réalisable pour des fonctions vraiment compliquées, certainement pas dans la version à 13 000 entrées de cette situation pour notre fonction de coût de réseau neuronal compliquée et folle.",
  "n_reviews": 0
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "Une tactique plus flexible consiste à commencer par n'importe quelle entrée et à déterminer dans quelle direction vous devez aller pour réduire cette sortie.",
  "n_reviews": 0
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "Plus précisément, si vous pouvez déterminer la pente de la fonction là où vous vous trouvez, déplacez-vous vers la gauche si cette pente est positive et déplacez l'entrée vers la droite si cette pente est négative.",
  "n_reviews": 0
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "Si vous faites cela à plusieurs reprises, en vérifiant à chaque point la nouvelle pente et en prenant l'étape appropriée, vous vous approcherez d'un minimum local de la fonction.",
  "n_reviews": 0
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "L’image que vous avez peut-être en tête ici est celle d’une balle qui dévale une colline.",
  "n_reviews": 0
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "Remarquez que même pour cette fonction à entrée unique très simplifiée, il existe de nombreuses vallées possibles dans lesquelles vous pourriez atterrir, en fonction de l'entrée aléatoire à laquelle vous commencez, et rien ne garantit que le minimum local dans lequel vous atterrirez sera la valeur la plus petite possible. de la fonction de coût.",
  "n_reviews": 0
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "Cela se répercutera également sur notre cas de réseau neuronal.",
  "n_reviews": 0
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "Je veux également que vous remarquiez que si vous rendez la taille de vos pas proportionnelle à la pente, lorsque la pente s'aplatit vers le minimum, vos pas deviennent de plus en plus petits, ce qui vous aide à ne pas dépasser.",
  "n_reviews": 0
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "En augmentant un peu la complexité, imaginez plutôt une fonction avec deux entrées et une sortie.",
  "n_reviews": 0
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "Vous pourriez considérer l’espace d’entrée comme le plan xy et la fonction de coût comme étant représentée graphiquement comme une surface au-dessus de lui.",
  "n_reviews": 0
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "Au lieu de poser des questions sur la pente de la fonction, vous devez vous demander dans quelle direction vous devez marcher dans cet espace d'entrée afin de diminuer le plus rapidement la sortie de la fonction.",
  "n_reviews": 0
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "En d’autres termes, quelle est la direction de la descente ?",
  "n_reviews": 0
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "Encore une fois, il est utile de penser à une balle qui dévale cette colline.",
  "n_reviews": 0
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "Ceux d'entre vous qui sont familiers avec le calcul multivarié sauront que la pente d'une fonction vous donne la direction de la montée la plus raide, dans quelle direction devez-vous avancer pour augmenter la fonction le plus rapidement.",
  "n_reviews": 0
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "Naturellement, prendre le négatif de ce gradient vous donne la direction du pas qui diminue la fonction le plus rapidement.",
  "n_reviews": 0
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "Plus encore, la longueur de ce vecteur gradient indique à quel point la pente la plus raide est raide.",
  "n_reviews": 0
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "Si vous n'êtes pas familier avec le calcul multivarié et souhaitez en savoir plus, consultez certains des travaux que j'ai réalisés pour la Khan Academy sur le sujet.",
  "n_reviews": 0
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "Honnêtement, tout ce qui compte pour vous et moi en ce moment, c'est qu'en principe, il existe un moyen de calculer ce vecteur, ce vecteur qui vous indique quelle est la direction de la descente et quelle est sa pente.",
  "n_reviews": 0
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "Tout ira bien si c'est tout ce que vous savez et que vous n'êtes pas solide sur les détails.",
  "n_reviews": 0
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "Si vous pouvez obtenir cela, l'algorithme permettant de minimiser la fonction consiste à calculer cette direction de gradient, puis à faire un petit pas en descente et à répéter cela encore et encore.",
  "n_reviews": 0
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "C'est la même idée de base pour une fonction qui possède 13 000 entrées au lieu de 2 entrées.",
  "n_reviews": 0
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "Imaginez organiser les 13 000 poids et biais de notre réseau dans un vecteur colonne géant.",
  "n_reviews": 0
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "Le gradient négatif de la fonction de coût n'est qu'un vecteur, c'est une direction à l'intérieur de cet espace d'entrée incroyablement énorme qui vous indique quels coups de pouce à tous ces nombres vont provoquer la diminution la plus rapide de la fonction de coût.",
  "n_reviews": 0
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "Et bien sûr, grâce à notre fonction de coût spécialement conçue, modifier les pondérations et les biais pour les diminuer signifie que la sortie du réseau sur chaque élément de données d'entraînement ressemble moins à un tableau aléatoire de 10 valeurs, mais plutôt à une décision réelle que nous souhaitons. c'est à faire.",
  "n_reviews": 0
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "Il est important de se rappeler que cette fonction de coût implique une moyenne sur toutes les données d'entraînement, donc si vous la minimisez, cela signifie que les performances sont meilleures sur tous ces échantillons.",
  "n_reviews": 0
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "L'algorithme permettant de calculer efficacement ce gradient, qui est en fait au cœur de la façon dont un réseau neuronal apprend, s'appelle la rétropropagation, et c'est ce dont je vais parler dans la prochaine vidéo.",
  "n_reviews": 0
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "Là, je veux vraiment prendre le temps d'examiner ce qui arrive exactement à chaque poids et biais pour une donnée d'entraînement donnée, en essayant de donner une idée intuitive de ce qui se passe au-delà de la pile de calculs et de formules pertinents.",
  "n_reviews": 0
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "Ici, maintenant, la principale chose que je veux que vous sachiez, indépendamment des détails de mise en œuvre, c'est que ce que nous entendons lorsque nous parlons d'apprentissage en réseau, c'est qu'il s'agit simplement de minimiser une fonction de coût.",
  "n_reviews": 0
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "Et remarquez, une des conséquences de cela est qu'il est important que cette fonction de coût ait un résultat agréable et fluide, afin que nous puissions trouver un minimum local en descendant par petits pas.",
  "n_reviews": 0
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "C’est d’ailleurs pourquoi les neurones artificiels ont des activations continues, plutôt que d’être simplement actifs ou inactifs de manière binaire, comme le sont les neurones biologiques.",
  "n_reviews": 0
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "Ce processus consistant à pousser à plusieurs reprises l'entrée d'une fonction d'un multiple du gradient négatif est appelé descente de gradient.",
  "n_reviews": 0
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "C'est un moyen de converger vers un minimum local d'une fonction de coût, essentiellement une vallée dans ce graphique.",
  "n_reviews": 0
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "Je montre toujours l'image d'une fonction avec deux entrées, bien sûr, car les coups de pouce dans un espace d'entrée à 13 000 dimensions sont un peu difficiles à comprendre, mais il existe une belle façon non spatiale d'y penser.",
  "n_reviews": 0
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "Chaque composante du gradient négatif nous dit deux choses.",
  "n_reviews": 0
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "Le signe, bien sûr, nous indique si la composante correspondante du vecteur d’entrée doit être poussée vers le haut ou vers le bas.",
  "n_reviews": 0
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "Mais surtout, les ampleurs relatives de tous ces composants vous indiquent quels changements sont les plus importants.",
  "n_reviews": 0
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "Vous voyez, dans notre réseau, un ajustement à l’un des poids peut avoir un impact beaucoup plus important sur la fonction de coût que l’ajustement à un autre poids.",
  "n_reviews": 0
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "Certaines de ces connexions sont tout simplement plus importantes pour nos données de formation.",
  "n_reviews": 0
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "Donc, une façon de penser à ce vecteur gradient de notre fonction de coût incroyablement massive est qu'il code l'importance relative de chaque poids et biais, c'est-à-dire lequel de ces changements va rapporter le plus pour votre argent.",
  "n_reviews": 0
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "Il s’agit en réalité d’une autre façon de penser la direction.",
  "n_reviews": 0
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "Pour prendre un exemple plus simple, si vous avez une fonction avec deux variables en entrée et que vous calculez que son gradient à un point particulier est de 3,1, alors d'une part vous pouvez interpréter cela comme disant que lorsque vous Si vous vous trouvez à cette entrée, vous déplacer dans cette direction augmente la fonction le plus rapidement, et lorsque vous représentez graphiquement la fonction au-dessus du plan des points d'entrée, ce vecteur est ce qui vous donne la direction droite vers le haut.",
  "n_reviews": 0
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "Mais une autre façon de lire cela est de dire que les modifications apportées à cette première variable ont 3 fois plus d'importance que les modifications apportées à la deuxième variable, qu'au moins au voisinage de l'entrée concernée, pousser la valeur x a beaucoup plus d'impact pour votre mâle.",
  "n_reviews": 0
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "Faisons un zoom arrière et résumons où nous en sommes jusqu'à présent.",
  "n_reviews": 0
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "Le réseau lui-même est cette fonction avec 784 entrées et 10 sorties, définies en fonction de toutes ces sommes pondérées.",
  "n_reviews": 0
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "La fonction de coût est en outre une couche de complexité supplémentaire.",
  "n_reviews": 0
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "Il prend les 13 000 poids et biais comme entrées et génère une seule mesure de moche basée sur les exemples de formation.",
  "n_reviews": 0
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "Et le gradient de la fonction de coût constitue encore un niveau de complexité supplémentaire.",
  "n_reviews": 0
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "Il nous indique quels coups de pouce à tous ces poids et biais provoquent le changement le plus rapide de la valeur de la fonction de coût, ce que vous pourriez interpréter comme indiquant quels changements et quels poids comptent le plus.",
  "n_reviews": 0
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "Ainsi, lorsque vous initialisez le réseau avec des poids et des biais aléatoires et que vous les ajustez plusieurs fois en fonction de ce processus de descente de gradient, dans quelle mesure fonctionne-t-il réellement sur des images jamais vues auparavant ?",
  "n_reviews": 0
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "Celui que j'ai décrit ici, avec les deux couches cachées de 16 neurones chacune, choisies principalement pour des raisons esthétiques, n'est pas mal, classant correctement environ 96 % des nouvelles images qu'il voit.",
  "n_reviews": 0
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "Et honnêtement, si vous regardez certains des exemples sur lesquels il se trompe, vous vous sentez obligé de prendre un peu de relâche.",
  "n_reviews": 0
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "Maintenant, si vous jouez avec la structure des couches cachées et effectuez quelques ajustements, vous pouvez obtenir jusqu'à 98 %.",
  "n_reviews": 0
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "Et c'est plutôt bien !",
  "n_reviews": 0
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "Ce n'est pas le meilleur, vous pouvez certainement obtenir de meilleures performances en devenant plus sophistiqué que ce réseau simple, mais étant donné à quel point la tâche initiale est ardue, je pense qu'il y a quelque chose d'incroyable à ce qu'un réseau fasse aussi bien sur des images qu'il n'a jamais vues auparavant, étant donné que nous ne lui avons jamais dit spécifiquement quels modèles rechercher.",
  "n_reviews": 0
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "À l'origine, la façon dont j'ai motivé cette structure était en décrivant un espoir que nous pourrions avoir, que la deuxième couche puisse capter les petits bords, que la troisième couche rassemblerait ces bords pour reconnaître les boucles et les lignes plus longues, et que celles-ci pourraient être reconstituées. ensemble pour reconnaître les chiffres.",
  "n_reviews": 0
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "Alors, est-ce réellement ce que fait notre réseau ?",
  "n_reviews": 0
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "Eh bien, pour celui-ci au moins, pas du tout.",
  "n_reviews": 0
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "Rappelez-vous comment, dans la dernière vidéo, nous avons regardé comment les poids des connexions de tous les neurones de la première couche à un neurone donné de la deuxième couche peuvent être visualisés sous la forme d'un motif de pixels donné que le neurone de la deuxième couche capte ?",
  "n_reviews": 0
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "Eh bien, lorsque nous faisons cela pour les poids associés à ces transitions, de la première couche à la suivante, au lieu de détecter de petits bords isolés ici et là, ils semblent presque aléatoires, avec juste des motifs très lâches dans le milieu là.",
  "n_reviews": 0
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "Il semblerait que dans l'espace insondable de 13 000 dimensions de pondérations et de biais possibles, notre réseau s'est trouvé un heureux petit minimum local qui, malgré la classification réussie de la plupart des images, ne reprend pas exactement les modèles que nous aurions pu espérer.",
  "n_reviews": 0
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "Et pour bien comprendre ce point, regardez ce qui se passe lorsque vous saisissez une image aléatoire.",
  "n_reviews": 0
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "Si le système était intelligent, vous pourriez vous attendre à ce qu'il semble incertain, n'activant peut-être pas vraiment l'un de ces 10 neurones de sortie ou les activant tous de manière uniforme, mais au lieu de cela, il vous donne en toute confiance une réponse absurde, comme s'il était aussi sûr que ce bruit aléatoire est un 5, car une image réelle d'un 5 est un 5.",
  "n_reviews": 0
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "Autrement dit, même si ce réseau reconnaît assez bien les chiffres, il ne sait pas comment les dessiner.",
  "n_reviews": 0
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "Cela est dû en grande partie au fait qu’il s’agit d’une configuration de formation très limitée.",
  "n_reviews": 0
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "Je veux dire, mettez-vous à la place du réseau ici.",
  "n_reviews": 0
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "De son point de vue, l’univers entier n’est constitué que de chiffres immobiles clairement définis et centrés dans une minuscule grille, et sa fonction de coût ne l’a jamais incité à être autre chose qu’absolument confiant dans ses décisions.",
  "n_reviews": 0
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "Donc, avec cela comme image de ce que font réellement ces neurones de deuxième couche, vous pourriez vous demander pourquoi j'introduireais ce réseau avec la motivation de capter les bords et les modèles.",
  "n_reviews": 0
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "Je veux dire, ce n’est tout simplement pas du tout ce que cela finit par faire.",
  "n_reviews": 0
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "Eh bien, ce n’est pas notre objectif final, mais plutôt un point de départ.",
  "n_reviews": 0
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "Franchement, il s’agit d’une technologie ancienne, du type étudié dans les années 80 et 90, et vous devez la comprendre avant de pouvoir comprendre des variantes modernes plus détaillées, et elle est clairement capable de résoudre certains problèmes intéressants, mais plus vous approfondissez ce que ces couches cachées font vraiment l'affaire, moins cela semble intelligent.",
  "n_reviews": 0
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "En déplaçant un instant l'attention de la façon dont les réseaux apprennent vers la façon dont vous apprenez, cela ne se produira que si vous vous engagez activement avec le matériel présenté ici, d'une manière ou d'une autre.",
  "n_reviews": 0
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "Une chose assez simple que je veux que vous fassiez est de faire une pause maintenant et de réfléchir profondément un instant aux changements que vous pourriez apporter à ce système et à la manière dont il perçoit les images si vous vouliez qu'il capte mieux des éléments tels que les bords et les motifs.",
  "n_reviews": 0
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "Mais mieux que cela, pour réellement approfondir le sujet, je recommande vivement le livre de Michael Nielsen sur l'apprentissage profond et les réseaux de neurones.",
  "n_reviews": 0
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "Vous y trouverez le code et les données à télécharger et avec lesquelles jouer pour cet exemple précis, et le livre vous guidera étape par étape ce que fait ce code.",
  "n_reviews": 0
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "Ce qui est génial, c'est que ce livre est gratuit et accessible au public, donc si vous en retirez quelque chose, pensez à vous joindre à moi pour faire un don en faveur des efforts de Nielsen.",
  "n_reviews": 0
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "J'ai également lié quelques autres ressources que j'aime beaucoup dans la description, y compris le magnifique et phénoménal article de blog de Chris Ola et les articles de Distill.",
  "n_reviews": 0
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "Pour conclure ici ces dernières minutes, je voudrais revenir sur un extrait de l'entretien que j'ai eu avec Leisha Lee.",
  "n_reviews": 0
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "Vous vous souvenez peut-être d'elle dans la dernière vidéo, elle a fait son doctorat en apprentissage profond.",
  "n_reviews": 0
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "Dans ce petit extrait, elle parle de deux articles récents qui approfondissent réellement la manière dont certains des réseaux de reconnaissance d’images les plus modernes apprennent réellement.",
  "n_reviews": 0
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "Juste pour situer où nous en étions dans la conversation, le premier article a pris l'un de ces réseaux neuronaux particulièrement profonds qui est vraiment bon en reconnaissance d'images, et au lieu de l'entraîner sur un ensemble de données correctement étiqueté, il a mélangé toutes les étiquettes avant l'entraînement.",
  "n_reviews": 0
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "De toute évidence, la précision des tests ici n'était pas meilleure que celle du hasard, puisque tout est étiqueté de manière aléatoire, mais il était toujours possible d'obtenir la même précision de formation que celle que vous obtiendriez sur un ensemble de données correctement étiqueté.",
  "n_reviews": 0
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "Fondamentalement, les millions de poids pour ce réseau particulier étaient suffisants pour qu'il mémorise simplement les données aléatoires, ce qui soulève la question de savoir si la minimisation de cette fonction de coût correspond réellement à une sorte de structure dans l'image, ou s'agit-il simplement d'une mémorisation ?",
  "n_reviews": 0
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "Si vous regardez cette courbe de précision, si vous vous entraîniez simplement sur un ensemble de données aléatoires, cette courbe descendait très lentement, de manière presque linéaire, donc vous avez vraiment du mal à trouver ce minimum local de possible, vous savez. , les bons poids qui vous apporteraient cette précision.",
  "n_reviews": 0
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "Alors que si vous vous entraînez réellement sur un ensemble de données structuré, qui a les bonnes étiquettes, vous bidouillez un peu au début, mais vous avez ensuite chuté très rapidement pour atteindre ce niveau de précision, et donc dans un certain sens, cela était plus facile de trouver ces maxima locaux.",
  "n_reviews": 0
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "Et ce qui était également intéressant, c'est que cela met en lumière un autre article datant d'il y a quelques années, qui contient beaucoup plus de simplifications sur les couches réseau, mais l'un des résultats disait que si vous regardez le paysage de l'optimisation, les minimums locaux que ces réseaux ont tendance à apprendre sont en réalité de qualité égale, donc dans un certain sens, si votre ensemble de données est structuré, vous devriez pouvoir les trouver beaucoup plus facilement.",
  "n_reviews": 0
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "Mes remerciements, comme toujours, à ceux d'entre vous qui soutiennent Patreon.",
  "n_reviews": 0
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "J'ai déjà dit à quel point Patreon change la donne, mais ces vidéos ne seraient vraiment pas possibles sans vous.",
  "n_reviews": 0
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "Je tiens également à remercier tout particulièrement la société de capital-risque Amplify Partners, pour son soutien à ces premières vidéos de la série.",
  "n_reviews": 0
 }
]