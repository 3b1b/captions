1
00:00:04,070 --> 00:00:07,059
Utolsó videó Megmutattam egy neurális hálózat szerkezetét

2
00:00:07,160 --> 00:00:10,089
Gyorsan beszámolok itt, csak azért, hogy friss legyen a fejünkben

3
00:00:10,089 --> 00:00:15,368
És akkor két fő célom van erre a videóra. Az első az, hogy bemutassuk a gradiens süllyedés eszméjét,

4
00:00:15,650 --> 00:00:18,219
amely nem csak az neurális hálózatok megtanulása,

5
00:00:18,220 --> 00:00:20,439
de hogyan működik sok más gépi tanulás is

6
00:00:20,660 --> 00:00:24,609
Ezután aztán egy kicsit többet is meg fogunk ásni, hogy hogyan működik ez a hálózat

7
00:00:24,609 --> 00:00:27,758
És mi rejlik azok a rejtett rétegek a neuronok, amelyek ténylegesen keresnek

8
00:00:28,999 --> 00:00:33,489
Emlékeztetőül a célunk a klasszikus példa a kézírásos számjegyek felismerésére

9
00:00:34,129 --> 00:00:36,129
a neurális hálózatok hello világa

10
00:00:36,500 --> 00:00:43,090
ezek a számok 28 és 28 képpontos rácson jelenik meg minden pixelben, néhány szürkeárnyalatos érték között 0 és 1 között

11
00:00:43,610 --> 00:00:46,089
ezek határozzák meg a

12
00:00:46,850 --> 00:00:50,199
784 neuron a hálózat bemeneti rétegében és

13
00:00:50,840 --> 00:00:55,719
Ezután az egyes neuronok aktiválása az alábbi rétegekben súlyozott összeg alapján történik

14
00:00:56,000 --> 00:01:00,639
Az előző réteg összes aktiválása és egy speciális szám, amelyet előítéletnek neveznek

15
00:01:01,699 --> 00:01:06,338
akkor ezt az összeget más funkcióval írja, mint például a sigmoid squishification vagy

16
00:01:06,400 --> 00:01:08,769
egy ReLu-t, ahogyan átmentem az utolsó videón

17
00:01:09,110 --> 00:01:15,729
Összességében, mivel két rejtett réteg kissé önkényes választása van, itt 16 neuron van, mindegyikük kb

18
00:01:16,579 --> 00:01:24,159
13.000 súly és előítélet, amit beállíthatunk, és ezek az értékek határozzák meg, hogy pontosan mi a tényleges hálózata

19
00:01:24,799 --> 00:01:28,328
Akkor mit értünk, amikor azt mondjuk, hogy ez a hálózat egy adott számjegyet sorol fel

20
00:01:28,329 --> 00:01:33,429
Ez a legfrissebb a 10 neuron közül a végső rétegnek felel meg

21
00:01:33,950 --> 00:01:38,589
És ne feledje, hogy a motiváció, amelyet a réteges struktúrára gondolunk, talán ez volt

22
00:01:38,780 --> 00:01:44,680
A második réteg felveheti az éleket, és a harmadik réteg felveheti a mintákat, mint hurkot és vonalat

23
00:01:44,930 --> 00:01:48,729
És az utolsó csak összegyűjti ezeket a mintákat, hogy felismerje a számjegyeket

24
00:01:49,369 --> 00:01:52,029
Tehát itt megtanuljuk, hogyan tanul a hálózat

25
00:01:52,399 --> 00:01:57,099
Amit csak akarunk, egy algoritmus, ahol ez a hálózat egy csomó képzési adatot jeleníthet meg

26
00:01:57,229 --> 00:02:03,669
amely egy kézzel írott számjegyből álló különböző képparkok formájában jelenik meg, valamint címkékkel, amelyekről azt állítják,

27
00:02:03,890 --> 00:02:05,659
Ez beállítja azokat

28
00:02:05,659 --> 00:02:09,789
13000 súlyt és előítéletet, hogy javítsa teljesítményét a képzési adatokon

29
00:02:10,730 --> 00:02:13,569
Remélhetőleg ez a réteges szerkezet azt jelenti, hogy mit tanul

30
00:02:14,269 --> 00:02:16,719
a képzési adatokon túlmutató képekre általánosítható

31
00:02:16,720 --> 00:02:20,289
És ahogy teszteljük, az az, hogy a hálózat felkészítése után

32
00:02:20,290 --> 00:02:26,560
Megmutatja, hogy a theta többet jelölt, hogy soha nem látott korábban, és látod, hogy pontosan osztályozza ezeket az új képeket

33
00:02:31,040 --> 00:02:37,000
Szerencsére számunkra, és mi teszi ezt a közös példát az, hogy a MNIST bázis mögött álló jó emberek vannak

34
00:02:37,000 --> 00:02:44,289
összeállítottak egy gyűjteményt több tízezer kézzel írott digitális képen, mindegyiküket a számokkal jelölték, amiket feltételeznek.

35
00:02:44,720 --> 00:02:49,539
Ez provokatív, mivel leírni egy gépet, mint tanulást, ha tényleg látja, hogyan működik

36
00:02:49,540 --> 00:02:55,359
Sokkal kevésbé érzi magát, mint egy őrült sci-fi helyszín, és sokkal jobban hasonlít egy kalkulus gyakorlatra

37
00:02:55,390 --> 00:02:59,589
Úgy értem, alapvetően lefelé fordul, hogy megtalálja a minimálisan egy bizonyos funkciót

38
00:03:01,519 --> 00:03:05,199
Ne feledkezzünk meg arról, hogy mindegyik neuront összekapcsoljuk

39
00:03:05,390 --> 00:03:12,309
az előző réteg összes neuronja számára, és a súlyozott összegben az aktiválást meghatározó súlyok olyanok, mint a

40
00:03:12,440 --> 00:03:14,060
erősségeit

41
00:03:14,060 --> 00:03:20,440
És az elfogultság azt jelzi, hogy az adott neuron aktív vagy inaktív-e, és elkezdi-e a dolgokat

42
00:03:20,440 --> 00:03:26,919
Csak inicializáljuk az összes súlyt, és teljesen véletlenszerűen szükségtelen, hogy ezt a hálózatot fogjuk végrehajtani

43
00:03:26,919 --> 00:03:33,759
meglehetősen rettenetesen egy adott képzési példánál, mert csak csinál valami véletlenszerűen, például betölti ezt a képet a 3-as és a

44
00:03:33,760 --> 00:03:35,799
Kimeneti réteg csak úgy néz ki, mint egy rendetlenség

45
00:03:36,349 --> 00:03:42,518
Tehát, amit csinálsz, a költségfüggvényt úgy definiálod, hogy megmondja a számítógépet: "Nincs rossz számítógép!

46
00:03:42,739 --> 00:03:50,529
A kimenetnek aktivitásokkal kell rendelkeznie, amelyek nulla a legtöbb neurontól, de egy ilyen neuron, amit adtál neked, teljes kuka "

47
00:03:51,260 --> 00:03:56,530
Azt mondani, hogy egy kicsit matematikailag, amit csinálsz, feloldod a különbségek négyzetét

48
00:03:56,720 --> 00:04:01,419
mindegyik trágyakimenet aktiválását és azt az értéket, amelyet szeretnél, és

49
00:04:01,489 --> 00:04:04,599
Ezt hívjuk egy képzési példa költségének

50
00:04:05,599 --> 00:04:10,749
Figyeljük meg, hogy ez az összeg kicsi, ha a hálózat magabiztosan osztályozza a képet helyesen

51
00:04:12,199 --> 00:04:15,639
De nagy, amikor a hálózat úgy tűnik, hogy nem igazán tudja, mit csinál

52
00:04:18,330 --> 00:04:25,249
Így aztán mindent megtesz az átlagos költséget az Ön rendelkezésére álló több tízezer képzési példán

53
00:04:27,060 --> 00:04:34,310
Ez az átlagos költség az intézkedésünk, hogy milyen rossz a hálózat, és milyen rossz a számítógép érzi, és ez bonyolult dolog

54
00:04:34,830 --> 00:04:38,960
Ne feledje, hogy maga a hálózat alapvetően egy függvény, amelyik befogadja

55
00:04:39,540 --> 00:04:45,890
A 784-es szám bemeneti értékként adja meg a pixel értékét, és tíz számot tár fel kimenetként és bizonyos értelemben

56
00:04:45,890 --> 00:04:48,770
Mindezeket a súlyokat és előítéleteket paraméterezi

57
00:04:49,140 --> 00:04:54,020
Míg a költségfüggvény egy komplexitási réteg, amely túllépi a bevitelt

58
00:04:54,450 --> 00:05:02,059
ezek a tizenháromezer nagyságú súlyok és előítéletek, és egyetlen számot tár fel, amely leírja, milyen rosszak ezek a súlyok és előítéletek és

59
00:05:02,340 --> 00:05:08,749
A meghatározás módja attól függ, hogy a hálózat hogyan viselkedik a több tízezer képzési adat fölött

60
00:05:09,150 --> 00:05:11,150
Sokat kell gondolni

61
00:05:12,000 --> 00:05:15,619
De csak azt mondja a számítógépnek, hogy egy csúnya feladat, amit csinál, nem nagyon segít

62
00:05:15,900 --> 00:05:19,819
Szeretné elmondani, hogyan kell ezeket a súlyokat és előítéleteket megváltoztatni, hogy jobb lesz?

63
00:05:20,820 --> 00:05:25,129
Annak érdekében, hogy egyszerűbbé tegye a 13 000 bemenettel rendelkező funkció helyett

64
00:05:25,130 --> 00:05:30,409
Képzeljünk csak el egy egyszerű funkciót, amelynek egy száma bemenetként és egy kimeneti számként van

65
00:05:30,960 --> 00:05:34,999
Hogyan találja meg azt a bemenetet, amely minimalizálja a funkció értékét?

66
00:05:36,270 --> 00:05:40,039
A kalkulus hallgatók tudni fogják, hogy néha kifejezetten kimutathatjátok ezt a minimumot

67
00:05:40,260 --> 00:05:43,879
De ez nem mindig megvalósítható az igazán bonyolult feladatokhoz

68
00:05:44,310 --> 00:05:52,160
Természetesen nem a tizenhárom ezer bemeneti változata ennek a helyzetnek a mi őrült bonyolult neurális hálózat költsége funkció

69
00:05:52,350 --> 00:05:59,029
A rugalmasabb taktika minden régi bemeneten elindul, és kitalálja, melyik irányba kell lépnie ahhoz, hogy a kimenet alacsonyabb legyen

70
00:06:00,120 --> 00:06:03,710
Pontosabban, ha kitalálhatja a funkció meredekségét

71
00:06:04,020 --> 00:06:09,619
Ezután tolja balra, ha ez a lejtés pozitív, és jobbra tolja a bemenetet, ha a meredekség negatív

72
00:06:12,130 --> 00:06:16,799
Ha ezt többször is megismételjük minden ponton, ellenõrizzük az új lejtõt és megtesszük a megfelelõ lépést

73
00:06:16,800 --> 00:06:20,039
akkor hozzá fog járulni a helyi minimumhoz a funkcióhoz és

74
00:06:20,280 --> 00:06:24,080
a kép, amelyre gondolhatsz itt, egy golyó, amely lefelé halad egy dombon

75
00:06:24,400 --> 00:06:30,900
A ténylegesen leegyszerűsített bemeneti funkciókért is észrevehető, hogy számos lehetséges völgy van, amelybe beléphet

76
00:06:31,540 --> 00:06:36,220
Attól függően, hogy melyik véletlenszerű bemenet kezdődik, és nincs garancia arra, hogy a helyi minimum

77
00:06:36,580 --> 00:06:39,040
Bejutsz a költségfunkció legkisebb lehetséges értéke

78
00:06:39,610 --> 00:06:44,009
Ez át fog terjedni a neurális hálózathoz is, és azt is szeretném, ha észreveszed

79
00:06:44,010 --> 00:06:47,190
Hogyan csinálhatja lépcsőméreteit a lejtővel arányosnak?

80
00:06:47,620 --> 00:06:54,540
Akkor, amikor a lejtés a minimálisra süllyed, lépései kisebbek és kisebbek lesznek, és ez a fajta segít a túllövésnek

81
00:06:55,720 --> 00:07:00,449
A bonyolultság felhúzásával elképzelhető egy két bemenet és egy kimenet funkciója

82
00:07:01,120 --> 00:07:07,739
Gondolhat arra, hogy a bemeneti tér XY sík és a költségfüggvény, mint a felett levő felület

83
00:07:08,230 --> 00:07:15,060
Ahelyett, hogy megkérdeznéd a függvény meredekségét, meg kell kérdezned, melyik irányba kell lépnie ebben a bemeneti területen?

84
00:07:15,310 --> 00:07:22,440
Annak érdekében, hogy a funkció leggyorsabban csökkentse a kimenetét. Mi a lejtő iránya?

85
00:07:22,440 --> 00:07:25,379
És újra hasznos lehet gondolni egy labdát, amely lecsúszik a dombon

86
00:07:26,260 --> 00:07:34,080
Azok, akik ismernek a többváltozós kalkulusokkal, tudni fogják, hogy egy függvény gradiense adja a legmeredekebb emelkedést

87
00:07:34,750 --> 00:07:38,459
Alapvetően, melyik irányba kell lépnie a funkció leggyorsabb növeléséhez

88
00:07:39,100 --> 00:07:46,439
Természetesen elegendő az adott gradiens negatívja, megadja azt a lépést, amely a leggyorsabban csökkenti a funkciót

89
00:07:47,020 --> 00:07:53,400
Még ennél is több, hogy ennek a gradiensvektornak a hossza valójában jelzi, hogy milyen meredek, hogy a legmeredekebb lejtő

90
00:07:54,130 --> 00:07:56,280
Most, ha nem ismeri a többváltozós kalkulust

91
00:07:56,280 --> 00:08:00,239
És szeretnél többet megtudni, nézd meg a munkámat, amit a Khan Akadémián tettem a témában

92
00:08:00,910 --> 00:08:03,779
Őszintén szólva, bár minden ami számodra fontos neked és nekem

93
00:08:03,780 --> 00:08:09,419
Ez elvben létezik mód arra, hogy kiszámítsuk ezt a vektort. Ez a vektor, amely megmondja, hogy mi a

94
00:08:09,520 --> 00:08:15,900
A lejtő iránya és mennyire meredek, hogy rendben leszel, ha ez minden, amit tudsz, és nem vagy sziklaszilárd a részletekben

95
00:08:16,790 --> 00:08:24,580
mert ha tudod, hogy az algoritmus minimalizálja a függvényt, akkor ezt a gradiens irányt kell kiszámítani, majd egy kis lépést lefelé, és

96
00:08:24,740 --> 00:08:26,740
Csak ismételje meg újra és újra

97
00:08:27,800 --> 00:08:34,600
Ugyanaz az alapötlet egy olyan funkcióhoz, amelynek 13 000 bemenete van a két bemenet helyett, képzeld el mindent

98
00:08:35,330 --> 00:08:39,400
13 ezer súly és a hálózatunk elfogultsága egy óriási oszlopvektorba

99
00:08:39,680 --> 00:08:43,870
A költségfüggvény negatív gradiense csak egy vektor

100
00:08:43,880 --> 00:08:49,299
Ez valami irány az õrületesen hatalmas bemeneti helyén belül, amely megmondja, hogy melyik

101
00:08:49,400 --> 00:08:55,030
hogy az összes szám megmutatja a költségcsökkenés leggyorsabb csökkenését és

102
00:08:55,460 --> 00:08:58,150
természetesen a speciálisan tervezett költségfunkcióval

103
00:08:58,580 --> 00:09:04,900
A súlyok és az előítéletek csökkentése azt jelenti, hogy a hálózat kimenetét minden egyes képzési adatra ki kell alakítani

104
00:09:05,180 --> 00:09:10,599
Kevésbé néz ki, mint egy tíz érték véletlenszerű tömbje, és inkább egy tényleges döntés, amit akarunk

105
00:09:11,030 --> 00:09:16,030
Fontos megjegyezni, hogy ez a költségfüggvény átlagosan az összes képzési adatot tartalmazza

106
00:09:16,370 --> 00:09:20,590
Tehát ha minimalizáljuk azt, azt jelenti, hogy ez a jobb teljesítmény minden ilyen mintán

107
00:09:23,780 --> 00:09:30,849
Az algoritmust, amely hatékonyan képes a gradiens kiszámítására, ami a neurális hálózat megtanulása révén valójában a szív,

108
00:09:31,190 --> 00:09:34,690
És ez az, amit a következő videóról fogok beszélni

109
00:09:34,690 --> 00:09:36,690
Itt nagyon szeretnék időt kivenni

110
00:09:36,830 --> 00:09:41,439
Mi történik pontosan az egyes súlyokkal és minden egyes torzítással az adott képzési adatokkal kapcsolatban?

111
00:09:41,810 --> 00:09:46,960
Megpróbálja intuitív érzést adni arra vonatkozóan, hogy mi történik a megfelelő kalkulusok és képletek halmán kívül

112
00:09:47,510 --> 00:09:52,179
Itt van most a lényeg. Szeretném, ha a végrehajtás részleteitől függetlenül tudnátok

113
00:09:52,180 --> 00:09:58,479
hogy mit értünk, amikor egy hálózati tanulásról beszélünk, hogy csak minimalizálja a költségfüggvényt és

114
00:09:58,940 --> 00:10:04,479
Vegyük észre, hogy ennek egyik következménye, hogy ez a költségfüggvénynek jó sima kimenetre van szüksége

115
00:10:04,480 --> 00:10:07,810
Annak érdekében, hogy a helyi minimális szintet lecsökkentsük a lejtőn

116
00:10:08,810 --> 00:10:10,520
Ezért az úton

117
00:10:10,520 --> 00:10:16,749
A mesterséges neuronok folyamatosan aktiválódnak, nem pedig egyszerűen aktívak vagy inaktívak bináris módon

118
00:10:16,750 --> 00:10:18,750
ha a biológiai neuronok így vannak

119
00:10:19,940 --> 00:10:26,770
Ezt a folyamatot többszörös negatív gradiensnek egy függvény bemenetének átbillentésével nevezzük gradiens süllyesztésnek

120
00:10:26,930 --> 00:10:32,380
Ez a módja annak, hogy egy helyi költség minimálisan egy minimális költségfüggvényhez közelítsenek egy völgyet ebben a grafikonban

121
00:10:32,930 --> 00:10:38,890
Természetesen két bejáratnál is megmutatom a függvény képét, mivel tizenhárom ezer dimenziós bemeneten mozog

122
00:10:38,890 --> 00:10:44,049
A tér egy kicsit nehéz elrejteni az elmédet, de tényleg van egy jó, nem térbeli módszer erre gondolni

123
00:10:44,630 --> 00:10:51,340
A negatív gradiens minden komponense két dolgot mond el nekünk, természetesen a jel arra utal, hogy a megfelelő

124
00:10:51,830 --> 00:10:59,139
A bemeneti vektor komponensét felfelé vagy lefelé kell felhúzni, de fontos, hogy ezeknek az összetevőknek relatív nagysága

125
00:10:59,840 --> 00:11:02,530
A fajta megmondja, hogy mely változások jelentenek többet

126
00:11:05,150 --> 00:11:09,340
Úgy látja, hogy a hálózatunkon a súlyok valamelyikének beállítása sokkal nagyobb lehet

127
00:11:09,710 --> 00:11:12,939
hatással van a költségfüggvényre, mint egy másik súlyra való beállítás

128
00:11:14,450 --> 00:11:17,950
Néhány ilyen kapcsolat csak számunkra többet jelent a képzési adatokhoz

129
00:11:18,920 --> 00:11:22,690
Tehát olyan módon, hogy gondolkodni tud ezen a gradiens vektorán a tudatunkon

130
00:11:22,690 --> 00:11:27,999
A masszív költségfüggvény az, hogy kódolja az egyes súlyok és torzítások relatív fontosságát

131
00:11:28,250 --> 00:11:32,200
Ez az, ami ezek közül a változások közül a leginkább a bummért jár

132
00:11:33,560 --> 00:11:36,460
Ez csak egy újabb gondolkodási mód

133
00:11:36,860 --> 00:11:41,290
Egyszerűbb példa, ha van egy funkciója két változóval, mint bemenet, és te

134
00:11:41,690 --> 00:11:46,540
Számítsd ki, hogy a gradiens bizonyos pontokon jön ki, mint (3,1)

135
00:11:47,420 --> 00:11:51,670
Aztán egyrészt úgy értelmezheted azt, mint amikor azt mondod, hogy amikor az adott bemeneten állsz

136
00:11:52,070 --> 00:11:55,150
az ilyen irányú mozgás gyorsabban növeli a funkciót

137
00:11:55,460 --> 00:12:02,229
Ha a függvényt a bemeneti pontok síkja felett ábrázolja, akkor a vektor az, ami az egyenes felfelé irányuló irányt adja

138
00:12:02,600 --> 00:12:06,580
De egy másik módja annak, hogy ezt olvassuk, azt jelenti, hogy megváltozik ez az első változó

139
00:12:06,740 --> 00:12:13,390
Háromszor nagyobb a fontossága, mint a második változóban bekövetkezett változások, amelyek legalább a releváns bemenet szomszédságában vannak

140
00:12:13,520 --> 00:12:16,689
Az x érték elcsúszása sokkal nagyobb bummért jár a pénzedért

141
00:12:19,310 --> 00:12:19,930
Rendben

142
00:12:19,930 --> 00:12:24,940
Növeljük ki, és összegezzük, ahol eddig vagyunk, és maga a hálózat maga is ezzel a funkcióval

143
00:12:25,400 --> 00:12:29,859
784 bemenetet és 10 kimenetet határoz meg mindegyik súlyozott összeg tekintetében

144
00:12:30,350 --> 00:12:34,780
a költségfüggvény egy olyan komplexitási réteg, amely tetején van

145
00:12:35,120 --> 00:12:41,870
13.000 súly és előítélet, mint bemenet, és kiegyenlít egy mércét a hülyeség alapján a képzési példák és

146
00:12:42,180 --> 00:12:47,930
A költségfüggvény gradiense még egy további komplexitási réteg, mégis azt mondja nekünk

147
00:12:47,930 --> 00:12:53,839
Ami ezeknek a súlyoknak és előítéleteknek köszönhető, a leggyorsabb változást okozzák a költségfüggvény értékében

148
00:12:53,970 --> 00:12:57,680
Amit értelmezhetsz, azt mondod, melyik változás milyen súlyú a súlyok szempontjából

149
00:13:02,550 --> 00:13:09,289
Így, ha a hálózatot véletlenszerű súlyokkal és előítéletekkel kezded, és sokszor beállítod ezeket a gradiens süllyedési folyamat alapján

150
00:13:09,420 --> 00:13:12,949
Mennyire jól működik olyan képeken, amelyeket soha nem látott?

151
00:13:13,680 --> 00:13:19,609
Nos, amit itt leírtál a tizenhat neuronból álló két rejtett réteggel, melyek mindegyikét esztétikai okokból választották

152
00:13:20,579 --> 00:13:26,089
Nos, nem rossz, hogy az új képek 96 százaléka helyesen osztályozza

153
00:13:26,759 --> 00:13:32,239
Őszintén szólva, ha megnézed azokat a példákat, amelyekkel elrontotta magát, olyan érzést keltett, hogy egy kicsit lassan

154
00:13:35,759 --> 00:13:39,079
Most, ha játszol a rejtett rétegszerkezettel és csinálsz pár csípést

155
00:13:39,079 --> 00:13:43,698
Tudod ezt akár 98%, és ez nagyon jó. Nem a legjobb

156
00:13:43,740 --> 00:13:48,409
Biztosan jobb teljesítményt érhet el, ha kifinomultabbá válik, mint ez a sima vaníliahálózat

157
00:13:48,569 --> 00:13:52,669
De mivel a kezdeti feladat megrémisztult, azt gondolom, hogy van valami?

158
00:13:52,889 --> 00:13:56,929
Hihetetlen bármilyen hálózatra, ami jól csinálja ezeket a képeket, amelyeket soha nem látott

159
00:13:57,389 --> 00:14:00,919
Tekintettel arra, hogy soha nem mondtuk el kifejezetten, hogy milyen mintákat keresni

160
00:14:02,579 --> 00:14:07,068
Eredetileg az a motiváció, hogy motiváltam ezt a struktúrát azáltal, hogy leírtam a reményt, hogy mi lehet

161
00:14:07,259 --> 00:14:09,739
Hogy a második réteg kis széleken felveszi

162
00:14:09,809 --> 00:14:17,089
Hogy a harmadik réteg összezárja azokat az éleket, hogy felismerjék a hurkot és a hosszabb vonalakat, és hogy ezeket a számokat felismerhessék

163
00:14:17,699 --> 00:14:22,729
Tehát ez a mi hálózatunk valójában? Legalább ezt

164
00:14:23,339 --> 00:14:24,449
Egyáltalán nem

165
00:14:24,449 --> 00:14:27,409
ne feledje, hogy az utolsó videó hogyan nézett ki a súlyok a

166
00:14:27,480 --> 00:14:31,849
Az első réteg összes neuronja és az adott neuron közötti kapcsolat a második rétegben

167
00:14:31,980 --> 00:14:36,829
Úgy lehet megjeleníteni, mint egy adott képpont mintát, amelyet a második réteg neuronja felszed

168
00:14:37,350 --> 00:14:43,309
Nos, amikor ténylegesen ezt tesszük az ilyen átmenetekhez kapcsolódó súlyoknál az első rétegtől a másikig

169
00:14:43,709 --> 00:14:50,209
Ahelyett, hogy itt-ott elszigetelt kis széleket felvenne. Szinte véletlenszerűnek tűnnek

170
00:14:50,370 --> 00:14:56,399
Csak tedd fel néhány nagyon laza mintát a közepén ott, úgy tűnik, hogy a feltétlenül nagy

171
00:14:56,920 --> 00:15:02,580
A potenciális súlyok 13 000 dimenziós teret és elfogultságát a hálózatunkban boldog kis helyi minimumnak találta

172
00:15:02,860 --> 00:15:08,940
annak ellenére, hogy sikeresen osztályozta a legtöbb képet, nem pontosan veszi fel azokat a mintákat, amelyekre reménykedtünk

173
00:15:09,430 --> 00:15:13,709
Ahhoz, hogy valóban meghajtja ezt a pontot, otthoni figyeld, mi történik, ha véletlenszerű képet ad meg

174
00:15:14,019 --> 00:15:21,449
ha a rendszer okos volt, azt valószínűleg bizonytalanná válna, talán nem igazán aktiválja a 10 kimeneti neuronok egyikét sem

175
00:15:21,579 --> 00:15:23,200
Mindezt egyenletesen aktiválja

176
00:15:23,200 --> 00:15:24,820
De inkább

177
00:15:24,820 --> 00:15:32,010
Magabiztosan ad neked valami értelmetlen választ, mintha biztos lenne benne, hogy ez a véletlenszerű zaj 5, ugyanúgy, mint egy tényleges

178
00:15:32,010 --> 00:15:34,010
az 5-ös kép 5-ös

179
00:15:34,180 --> 00:15:40,829
másképp, ha ez a hálózat nagyon jól tudja felismerni a számjegyeket, fogalma sincs, hogyan kell őket rajzolni

180
00:15:41,500 --> 00:15:45,149
Sok ilyen, mert ilyen szigorúan korlátozott képzés

181
00:15:45,149 --> 00:15:51,479
Úgy értem, hogy itt a hálózat cipőjébe helyezem, az egész világmindenség a semmiből áll

182
00:15:51,480 --> 00:15:57,539
De az egyértelműen definiált, mozgatható számjegyek egy apró rácsban és költségfüggvényében soha nem adták meg

183
00:15:57,700 --> 00:16:00,959
Ösztönözni kell, hogy legyen bármi, de határozottan magabiztos a döntéseiben

184
00:16:01,690 --> 00:16:05,070
Tehát, ha ez a kép, amit a második réteg neuronjai valóban csinálnak

185
00:16:05,140 --> 00:16:09,839
Talán azon tűnődnél, hogy miért vezetném be ezt a hálózatot azzal a motivációval, hogy a széleken és a mintákon felszedjem

186
00:16:09,839 --> 00:16:11,969
Úgy értem, ez egyáltalán nem egyezik meg azzal, amit csinál

187
00:16:13,029 --> 00:16:17,909
Nos, ez nem a végső célunk, hanem nyíltan kiindulópont

188
00:16:17,910 --> 00:16:19,120
Ez a régi technológia

189
00:16:19,120 --> 00:16:21,510
a fajta kutatott a 80-as és 90-es években és

190
00:16:21,640 --> 00:16:29,129
Ezt meg kell értenie, mielőtt megértené a részletesebb modern változatokat, és egyértelműen képes néhány érdekes problémát megoldani

191
00:16:29,410 --> 00:16:34,110
De minél többet ássz be, amit ezek a rejtett rétegek tényleg a kevésbé intelligensek, úgy tűnik

192
00:16:38,530 --> 00:16:42,359
A fókusz egy pillanatra történő elmozdítása arról, hogy a hálózatok megtanulják a tanulás módját

193
00:16:42,580 --> 00:16:46,139
Ez csak akkor történik, ha valamilyen módon aktívan részt vesz az anyaggal

194
00:16:46,660 --> 00:16:53,100
Egy nagyon egyszerű dolog, hogy azt akarom, hogy csinálj, most szünetelsz, és egy pillanatig mélyen gondolkodj azon, hogy mit

195
00:16:53,440 --> 00:16:55,230
A rendszerhez esetleg végrehajtott módosítások

196
00:16:55,230 --> 00:17:00,719
És hogyan érzékeli a képeket, ha jobban szeretné, ha olyan dolgokra venné fel a dolgokat, mint az élek és a minták?

197
00:17:01,360 --> 00:17:04,410
De annál jobb, hogy valóban vegyen részt az anyaggal

198
00:17:04,410 --> 00:17:05,079
én

199
00:17:05,079 --> 00:17:08,969
Nagyon ajánljuk Michael Nielsen könyvét a mély tanulásról és a neurális hálózatokról

200
00:17:09,190 --> 00:17:14,369
Ebben megtalálja a kódot és az adatokat, amelyeket letölteni és játszani ezzel a pontos példával

201
00:17:14,410 --> 00:17:18,089
És a könyv lépésről-lépésre meglátogatja Önt, mi a kód

202
00:17:18,910 --> 00:17:21,749
Ami félelmetes, hogy ez a könyv ingyenes és nyilvánosan elérhető

203
00:17:22,360 --> 00:17:27,540
Tehát ha valamit kapsz ki, fontold meg, hogy csatlakozol hozzám, hogy adományt csinálj Nielsen erőfeszítései ellen

204
00:17:27,910 --> 00:17:32,219
Én is összekapcsoltam egy pár más erőforrást, amelyeket sokat szeretek a leírásban, beleértve a

205
00:17:32,470 --> 00:17:36,390
Chris Ola fenomenális és szép blogbejegyzése és a desztillált cikkek

206
00:17:38,230 --> 00:17:40,200
A dolgok lezárása az utóbbi néhány percben

207
00:17:40,200 --> 00:17:43,740
Szeretnék visszalépni egy olyan részletre, amellyel Leisha Lee-nal találkoztam

208
00:17:43,930 --> 00:17:49,079
Talán emlékszik rá az utolsó videóról. PhD munkáját mély tanulással és ebben a kis részletben végezte

209
00:17:49,080 --> 00:17:55,530
Két új tanulmányról beszél, amelyek valóban azonosítják, hogy a modern képfelismerő hálózatok valóban tanulnak

210
00:17:55,810 --> 00:18:01,349
Csak azért, hogy felállítsuk, hol vagyunk a beszélgetésben, az első papír az egyik különösen mély neurális hálózatot vette

211
00:18:01,350 --> 00:18:05,910
Ez valóban jó a képfelismerésnél, és nem helyesen felcímkézett adatokon

212
00:18:05,910 --> 00:18:08,579
Állítsa be, hogy az összes címkét megcsinálta a képzés előtt

213
00:18:08,800 --> 00:18:14,669
Nyilvánvaló, hogy a tesztelési pontosság itt nem jobb, mint véletlenszerű, mivel mindent véletlenszerűen címkézett

214
00:18:14,800 --> 00:18:20,879
De még mindig képes volt ugyanazt a képzési pontosságot elérni, mint egy megfelelően címkézett adatkészleten

215
00:18:21,490 --> 00:18:27,540
Alapvetően ez a hálózat több millió súlya elég volt ahhoz, hogy csak a véletlenszerű adatokat tárolja

216
00:18:27,820 --> 00:18:34,379
Milyen kérdés merül fel arra vonatkozóan, hogy a költségfunkció minimalizálása ténylegesen megfelel-e a kép bármely szerkezetének?

217
00:18:34,380 --> 00:18:36,380
Vagy csak tudod?

218
00:18:36,520 --> 00:18:37,420
memorizálni az egészet

219
00:18:37,420 --> 00:18:43,859
Adja meg, hogy mi a helyes besorolás, tehát néhányan fél évvel később az ICML-ben ismerkedtek meg idén

220
00:18:44,470 --> 00:18:49,039
Nem volt pontosan a fellebbező papíralap, amely a megkérdezetteknek szólt, mint a hé

221
00:18:49,470 --> 00:18:55,279
Valójában ezek a hálózatok valami okosabbat csinálnak, mint ha megnézzük azt a pontossági görbét

222
00:18:55,279 --> 00:18:57,499
ha csak egy a

223
00:18:58,259 --> 00:19:05,179
Véletlen adatkészlet, hogy a görbe fajta lement nagyon tudod nagyon lassan szinte egyfajta lineáris módon

224
00:19:05,179 --> 00:19:09,589
Tehát tényleg igyekszel megtalálni a lehetséges helyi minimumokat

225
00:19:09,590 --> 00:19:15,289
tudod a megfelelő súlyokat, amelyek megkapnák a pontosságot, pedig ha egy olyan strukturált adathalmazon tanulsz, amelyik a

226
00:19:15,289 --> 00:19:21,439
Jobb címkék. Tudja, hogy eleinte egy kicsit hegedülsz, de aztán nagyon gyorsan leesett, hogy elérje

227
00:19:22,200 --> 00:19:26,149
Pontossági szint, és így bizonyos értelemben könnyebb volt megtalálni

228
00:19:26,759 --> 00:19:33,949
A helyi maximák, és ezért érdekes volt, hogy az elkapta, egy másik, valójában néhány évvel ezelőtti papírt vet fel

229
00:19:34,080 --> 00:19:36,080
Ami sokkal több

230
00:19:36,990 --> 00:19:39,169
egyszerűsítéseket a hálózati rétegekről

231
00:19:39,169 --> 00:19:46,788
De az egyik eredmény azt mondta, hogy ha megnézzük az optimalizálási tájat, akkor a helyi minimumok, amelyeket ezek a hálózatok általában tanulnak

232
00:19:47,340 --> 00:19:54,079
Valójában ugyanolyan minőségű, így bizonyos értelemben, ha az adatkészleted szerkezet, és könnyebben meg tudod találni

233
00:19:58,139 --> 00:20:01,189
Köszönetemet, mint mindig azoknak, akik támogatják a patrónust

234
00:20:01,190 --> 00:20:06,950
Már korábban azt mondtam, hogy egy játékváltó patreon van, de ezek a videók valóban nem lennének lehetségesek nélküled

235
00:20:07,230 --> 00:20:12,889
Azt is szeretnék, hogy egy különleges. A VC cégnek köszönhetően erősíti a partnereit a sorozat kezdeti videóinak támogatásában

