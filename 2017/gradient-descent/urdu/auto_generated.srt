1
00:00:04,180 --> 00:00:07,280
آخری ویڈیو میں نے نیورل نیٹ ورک کا ڈھانچہ پیش کیا۔

2
00:00:07,680 --> 00:00:10,140
میں یہاں ایک فوری ریکیپ دوں گا تاکہ یہ ہمارے ذہنوں میں 

3
00:00:10,140 --> 00:00:12,600
تازہ ہو، اور پھر اس ویڈیو کے لیے میرے دو اہم مقاصد ہیں۔

4
00:00:13,100 --> 00:00:16,785
سب سے پہلے تدریجی نزول کے آئیڈیا کو متعارف کرانا ہے، جو نہ صرف اس بات کی نشاندہی کرتا 

5
00:00:16,785 --> 00:00:20,600
ہے کہ نیورل نیٹ ورک کس طرح سیکھتے ہیں، بلکہ بہت سی دوسری مشین لرننگ بھی کیسے کام کرتی ہے۔

6
00:00:21,120 --> 00:00:24,583
پھر اس کے بعد ہم تھوڑا اور کھوج لگائیں گے کہ یہ مخصوص نیٹ ورک کس 

7
00:00:24,583 --> 00:00:27,940
طرح کام کرتا ہے، اور نیوران کی وہ پوشیدہ پرتیں کیا ڈھونڈتی ہیں۔

8
00:00:28,979 --> 00:00:32,481
ایک یاد دہانی کے طور پر، ہمارا مقصد یہاں ہاتھ سے لکھے ہوئے 

9
00:00:32,481 --> 00:00:36,220
ہندسوں کی شناخت کی بہترین مثال ہے، نیورل نیٹ ورکس کی ہیلو ورلڈ۔

10
00:00:37,020 --> 00:00:43,420
یہ ہندسے 28x28 پکسل گرڈ پر پیش کیے گئے ہیں، ہر پکسل کی قدر 0 اور 1 کے درمیان ہوتی ہے۔

11
00:00:43,820 --> 00:00:50,040
یہ وہی ہیں جو نیٹ ورک کی ان پٹ پرت میں 784 نیوران کی فعالیت کا تعین کرتے ہیں۔

12
00:00:51,180 --> 00:00:55,873
اور پھر مندرجہ ذیل تہوں میں ہر نیوران کے لیے ایکٹیویشن پچھلی پرت میں تمام 

13
00:00:55,873 --> 00:01:00,820
ایکٹیویشنز کے وزنی مجموعہ پر مبنی ہے، نیز کچھ خاص نمبر جنہیں تعصب کہا جاتا ہے۔

14
00:01:02,160 --> 00:01:05,716
پھر آپ اس رقم کو کسی اور فنکشن کے ساتھ تحریر کریں، جیسے سگمائیڈ 

15
00:01:05,716 --> 00:01:08,940
اسکویشیفیکیشن، یا ریلو، جس طرح میں نے پچھلی ویڈیو سے گزرا۔

16
00:01:09,480 --> 00:01:14,446
مجموعی طور پر، 16 نیورونز کے ساتھ دو چھپی ہوئی تہوں کے کسی حد تک من مانی انتخاب 

17
00:01:14,446 --> 00:01:19,475
کو دیکھتے ہوئے، نیٹ ورک میں تقریباً 13,000 وزن اور تعصبات ہیں جنہیں ہم ایڈجسٹ کر 

18
00:01:19,475 --> 00:01:24,380
سکتے ہیں، اور یہی قدریں اس بات کا تعین کرتی ہیں کہ نیٹ ورک اصل میں کیا کرتا ہے۔

19
00:01:24,880 --> 00:01:28,963
پھر جب ہم کہتے ہیں کہ یہ نیٹ ورک کسی دیے گئے ہندسے کی درجہ بندی کرتا ہے تو ہمارا 

20
00:01:28,963 --> 00:01:33,300
مطلب یہ ہے کہ آخری تہہ میں موجود ان 10 نیورونز میں سے سب سے روشن اس ہندسے کے مساوی ہے۔

21
00:01:34,100 --> 00:01:39,124
اور یاد رکھیں، تہہ دار ڈھانچے کے لیے ہمارے ذہن میں جو محرک تھا وہ یہ تھا کہ شاید 

22
00:01:39,124 --> 00:01:44,024
دوسری تہہ کناروں پر اُٹھ سکتی ہے، اور تیسری تہہ لوپس اور لکیروں جیسے نمونوں پر 

23
00:01:44,024 --> 00:01:48,800
اُٹھ سکتی ہے، اور آخری تہہ ان کو جوڑ سکتی ہے۔ ہندسوں کو پہچاننے کے لیے پیٹرن۔

24
00:01:49,800 --> 00:01:52,240
تو یہاں، ہم سیکھتے ہیں کہ نیٹ ورک کیسے سیکھتا ہے۔

25
00:01:52,640 --> 00:01:56,933
ہم جو چاہتے ہیں وہ ایک الگورتھم ہے جہاں آپ اس نیٹ ورک کو تربیتی اعداد و شمار کا ایک 

26
00:01:56,933 --> 00:02:01,175
پورا گروپ دکھا سکتے ہیں، جو ہاتھ سے لکھے ہندسوں کی مختلف تصاویر کے ایک گروپ کی شکل 

27
00:02:01,175 --> 00:02:05,622
میں آتا ہے، اس کے ساتھ ساتھ ان کے لیبلز کے ساتھ کہ وہ کیا ہونا چاہیے، اور یہ ان 13,000 

28
00:02:05,622 --> 00:02:10,120
وزنوں اور تعصبات کو ایڈجسٹ کریں تاکہ تربیتی ڈیٹا پر اس کی کارکردگی کو بہتر بنایا جا سکے۔

29
00:02:10,720 --> 00:02:13,712
امید ہے کہ اس تہہ دار ڈھانچے کا مطلب یہ ہوگا کہ جو کچھ یہ 

30
00:02:13,712 --> 00:02:16,860
سیکھتا ہے وہ اس تربیتی ڈیٹا سے باہر کی تصاویر کو عام کرتا ہے۔

31
00:02:17,640 --> 00:02:20,605
جس طرح سے ہم جانچتے ہیں وہ یہ ہے کہ آپ نیٹ ورک کو تربیت دینے کے بعد، آپ 

32
00:02:20,605 --> 00:02:23,611
اسے مزید لیبل والا ڈیٹا دکھاتے ہیں جو اس نے پہلے کبھی نہیں دیکھا، اور آپ 

33
00:02:23,611 --> 00:02:26,700
دیکھتے ہیں کہ یہ ان نئی تصاویر کو کس حد تک درست طریقے سے درجہ بندی کرتا ہے۔

34
00:02:31,120 --> 00:02:35,374
خوش قسمتی سے ہمارے لیے، اور جس چیز سے یہ ایک عام مثال بنتی ہے، وہ یہ ہے کہ MNIST 

35
00:02:35,374 --> 00:02:39,892
ڈیٹابیس کے پیچھے اچھے لوگوں نے دسیوں ہزار ہاتھ سے لکھی ہندسوں کی تصاویر کا ایک مجموعہ 

36
00:02:39,892 --> 00:02:44,200
جمع کیا ہے، ہر ایک پر ان نمبروں کے ساتھ لیبل لگا ہوا ہے جن پر وہ سوچ رہے ہیں۔ ہونا

37
00:02:44,900 --> 00:02:48,392
اور کسی مشین کو سیکھنے کے طور پر بیان کرنا جتنا اشتعال انگیز ہے، ایک 

38
00:02:48,392 --> 00:02:51,987
بار جب آپ دیکھتے ہیں کہ یہ کیسے کام کرتی ہے، تو یہ کچھ پاگل سائنس فائی 

39
00:02:51,987 --> 00:02:55,480
بنیاد کی طرح بہت کم محسوس ہوتی ہے، اور بہت زیادہ کیلکولس ورزش کی طرح۔

40
00:02:56,200 --> 00:02:59,960
میرا مطلب ہے، بنیادی طور پر یہ کسی خاص فنکشن کی کم از کم تلاش کرنے پر آتا ہے۔

41
00:03:01,939 --> 00:03:07,634
یاد رکھیں، تصوراتی طور پر، ہم ہر نیوران کے بارے میں سوچ رہے ہیں کہ وہ پچھلی پرت کے تمام 

42
00:03:07,634 --> 00:03:13,459
نیورانوں سے جڑا ہوا ہے، اور اس کی ایکٹیویشن کی وضاحت کرنے والے وزنی رقم میں وزن ان رابطوں 

43
00:03:13,459 --> 00:03:18,960
کی طاقتوں کی طرح ہے، اور تعصب اس کا کچھ اشارہ ہے۔ چاہے وہ نیوران فعال ہو یا غیر فعال۔

44
00:03:19,720 --> 00:03:22,039
اور چیزوں کو شروع کرنے کے لئے، ہم صرف ان تمام وزنوں اور 

45
00:03:22,039 --> 00:03:24,400
تعصبات کو مکمل طور پر تصادفی طور پر شروع کرنے جا رہے ہیں۔

46
00:03:24,940 --> 00:03:27,910
یہ کہنے کی ضرورت نہیں کہ یہ نیٹ ورک ایک دی گئی تربیتی مثال پر کافی خوفناک 

47
00:03:27,910 --> 00:03:30,720
کارکردگی کا مظاہرہ کرنے والا ہے، کیونکہ یہ صرف بے ترتیب کچھ کر رہا ہے۔

48
00:03:31,040 --> 00:03:33,605
مثال کے طور پر، آپ 3 کی اس تصویر میں فیڈ کرتے ہیں، 

49
00:03:33,605 --> 00:03:36,020
اور آؤٹ پٹ پرت بالکل گڑبڑ کی طرح دکھائی دیتی ہے۔

50
00:03:36,600 --> 00:03:41,261
لہذا آپ جو کرتے ہیں وہ ایک لاگت کے فنکشن کی وضاحت کرتا ہے، کمپیوٹر کو بتانے کا 

51
00:03:41,261 --> 00:03:45,863
ایک طریقہ، نہیں، خراب کمپیوٹر، اس آؤٹ پٹ میں ایکٹیویشن ہونا چاہیے جو زیادہ تر 

52
00:03:45,863 --> 00:03:50,760
نیوران کے لیے 0 ہیں، لیکن اس نیوران کے لیے 1، جو آپ نے مجھے دیا ہے وہ سراسر ردی ہے۔

53
00:03:51,720 --> 00:03:56,093
یہ کہنے کے لیے کہ تھوڑا سا اور ریاضیاتی طور پر، آپ ان میں سے ہر ایک کوڑے 

54
00:03:56,093 --> 00:04:00,586
دان کے آؤٹ پٹ ایکٹیویشن اور اس قدر کے درمیان فرق کے مربعوں کو جوڑتے ہیں جو 

55
00:04:00,586 --> 00:04:05,020
آپ چاہتے ہیں کہ ان کے پاس ہوں، اور اسے ہم ایک تربیتی مثال کی قیمت کہیں گے۔

56
00:04:05,960 --> 00:04:11,061
نوٹ کریں کہ جب نیٹ ورک اعتماد کے ساتھ تصویر کی صحیح درجہ بندی کرتا ہے تو یہ رقم چھوٹی 

57
00:04:11,061 --> 00:04:16,399
ہوتی ہے، لیکن یہ بڑی ہوتی ہے جب نیٹ ورک ایسا لگتا ہے کہ یہ نہیں جانتا کہ وہ کیا کر رہا ہے۔

58
00:04:18,640 --> 00:04:22,131
تو پھر آپ جو کرتے ہیں وہ یہ ہے کہ آپ کے اختیار میں موجود 

59
00:04:22,131 --> 00:04:25,440
دسیوں ہزار تربیتی مثالوں میں سے اوسط لاگت پر غور کریں۔

60
00:04:27,040 --> 00:04:30,068
یہ اوسط لاگت ہمارا پیمانہ ہے کہ نیٹ ورک کتنا گھٹیا 

61
00:04:30,068 --> 00:04:32,740
ہے، اور کمپیوٹر کو کتنا برا محسوس ہونا چاہیے۔

62
00:04:33,420 --> 00:04:34,600
اور یہ ایک پیچیدہ چیز ہے۔

63
00:04:35,040 --> 00:04:39,646
یاد رکھیں کہ نیٹ ورک بنیادی طور پر کس طرح ایک فنکشن تھا، جو 784 نمبروں کو ان 

64
00:04:39,646 --> 00:04:44,133
پٹ، پکسل ویلیوز کے طور پر لیتا ہے، اور اس کے آؤٹ پٹ کے طور پر 10 نمبروں کو 

65
00:04:44,133 --> 00:04:48,800
نکالتا ہے، اور ایک لحاظ سے یہ ان تمام وزنوں اور تعصبات سے پیرامیٹرائز ہوتا ہے؟

66
00:04:49,500 --> 00:04:52,820
ٹھیک ہے لاگت کا فنکشن اس کے اوپر پیچیدگی کی ایک پرت ہے۔

67
00:04:53,100 --> 00:04:58,388
یہ ان 13,000 یا اس سے زیادہ وزن اور تعصبات کو اپنے ان پٹ کے طور پر لیتا ہے، اور 

68
00:04:58,388 --> 00:05:03,479
یہ بتاتا ہے کہ وہ وزن اور تعصبات کتنے خراب ہیں، اور جس طرح سے اس کی وضاحت کی 

69
00:05:03,479 --> 00:05:08,900
گئی ہے اس کا انحصار تربیتی ڈیٹا کے دسیوں ہزار ٹکڑوں پر نیٹ ورک کے رویے پر ہوتا ہے۔

70
00:05:09,520 --> 00:05:11,000
یہ بہت سوچنے کی بات ہے۔

71
00:05:12,400 --> 00:05:15,820
لیکن کمپیوٹر کو صرف یہ بتانا کہ وہ کیا گھٹیا کام کر رہا ہے زیادہ مددگار نہیں ہے۔

72
00:05:16,220 --> 00:05:20,060
آپ اسے بتانا چاہتے ہیں کہ ان وزنوں اور تعصبات کو کیسے تبدیل کیا جائے تاکہ یہ بہتر ہو جائے۔

73
00:05:20,780 --> 00:05:23,978
اسے آسان بنانے کے لیے، 13,000 ان پٹ کے ساتھ کسی فنکشن کا تصور 

74
00:05:23,978 --> 00:05:27,126
کرنے کے لیے جدوجہد کرنے کے بجائے، صرف ایک سادہ فنکشن کا تصور 

75
00:05:27,126 --> 00:05:30,480
کریں جس میں ایک نمبر بطور ان پٹ اور ایک نمبر آؤٹ پٹ کے طور پر ہو۔

76
00:05:31,480 --> 00:05:35,300
آپ کو ایک ان پٹ کیسے ملتا ہے جو اس فنکشن کی قدر کو کم کرتا ہے؟

77
00:05:36,460 --> 00:05:41,198
کیلکولس کے طلباء کو معلوم ہوگا کہ آپ بعض اوقات اس کم از کم واضح طور پر اندازہ لگا 

78
00:05:41,198 --> 00:05:46,052
سکتے ہیں، لیکن یہ واقعی پیچیدہ افعال کے لیے ہمیشہ ممکن نہیں ہوتا، یقیناً ہمارے پاگل 

79
00:05:46,052 --> 00:05:51,080
پیچیدہ نیورل نیٹ ورک کی لاگت کے فنکشن کے لیے اس صورت حال کے 13,000 ان پٹ ورژن میں نہیں۔

80
00:05:51,580 --> 00:05:55,498
ایک زیادہ لچکدار حربہ یہ ہے کہ کسی بھی ان پٹ سے آغاز کریں، اور یہ معلوم 

81
00:05:55,498 --> 00:05:59,200
کریں کہ اس آؤٹ پٹ کو کم کرنے کے لیے آپ کو کس سمت کو قدم رکھنا چاہیے۔

82
00:06:00,080 --> 00:06:04,848
خاص طور پر، اگر آپ اس فنکشن کی ڈھلوان کا پتہ لگا سکتے ہیں جہاں آپ ہیں، تو بائیں طرف 

83
00:06:04,848 --> 00:06:09,900
شفٹ کریں اگر وہ ڈھلوان مثبت ہے، اور اگر وہ ڈھلوان منفی ہے تو ان پٹ کو دائیں طرف شفٹ کریں۔

84
00:06:11,960 --> 00:06:15,848
اگر آپ یہ بار بار کرتے ہیں، ہر ایک نقطہ پر نئی ڈھلوان کی جانچ کرتے ہوئے اور 

85
00:06:15,848 --> 00:06:19,840
مناسب قدم اٹھاتے ہوئے، آپ فنکشن کے کچھ مقامی کم از کم سے رجوع کرنے جا رہے ہیں۔

86
00:06:20,640 --> 00:06:23,800
یہاں آپ کے ذہن میں جو تصویر ہو سکتی ہے وہ ایک گیند ہے جو ایک پہاڑی سے نیچے گر رہی ہے۔

87
00:06:24,620 --> 00:06:28,175
غور کریں، یہاں تک کہ اس واقعی آسان بنائے گئے واحد ان پٹ فنکشن کے لیے، 

88
00:06:28,175 --> 00:06:31,832
بہت سی ممکنہ وادییں ہیں جن میں آپ اتر سکتے ہیں، اس پر منحصر ہے کہ آپ کس 

89
00:06:31,832 --> 00:06:35,539
بے ترتیب ان پٹ سے شروع کرتے ہیں، اور اس بات کی کوئی گارنٹی نہیں ہے کہ آپ 

90
00:06:35,539 --> 00:06:39,400
جس مقامی کم از کم میں اتریں گے وہ سب سے چھوٹی ممکنہ قدر ہوگی۔ لاگت کی تقریب.

91
00:06:40,220 --> 00:06:42,620
یہ ہمارے نیورل نیٹ ورک کیس کو بھی لے جائے گا۔

92
00:06:43,180 --> 00:06:46,970
میں آپ کو یہ بھی دیکھنا چاہتا ہوں کہ اگر آپ اپنے قدموں کے سائز کو ڈھلوان کے 

93
00:06:46,970 --> 00:06:50,660
متناسب بناتے ہیں، تو جب ڈھلوان کم سے کم کی طرف چپٹا ہوتا ہے، تو آپ کے قدم 

94
00:06:50,660 --> 00:06:54,600
چھوٹے سے چھوٹے ہوتے جاتے ہیں، اور یہ آپ کو اوور شوٹنگ سے بچانے میں مدد کرتا ہے۔

95
00:06:55,940 --> 00:07:00,980
پیچیدگی کو تھوڑا سا بڑھاتے ہوئے، دو ان پٹ اور ایک آؤٹ پٹ کے ساتھ ایک فنکشن کا تصور کریں۔

96
00:07:01,500 --> 00:07:04,875
آپ ان پٹ کی جگہ کو xy-plane کے طور پر سوچ سکتے ہیں، اور لاگت 

97
00:07:04,875 --> 00:07:08,140
کے فنکشن کو اس کے اوپر کی سطح کے طور پر گراف کیا جا رہا ہے۔

98
00:07:08,760 --> 00:07:13,827
فنکشن کی ڈھلوان کے بارے میں پوچھنے کے بجائے، آپ کو یہ پوچھنا ہوگا کہ آپ کو اس 

99
00:07:13,827 --> 00:07:18,960
ان پٹ اسپیس میں کس سمت جانا چاہئے تاکہ فنکشن کے آؤٹ پٹ کو تیزی سے کم کیا جاسکے۔

100
00:07:19,720 --> 00:07:21,760
دوسرے الفاظ میں، نیچے کی سمت کیا ہے؟

101
00:07:22,380 --> 00:07:25,560
ایک بار پھر، اس پہاڑی سے نیچے گرنے والی گیند کے بارے میں سوچنا مددگار ہے۔

102
00:07:26,660 --> 00:07:30,592
آپ میں سے جو لوگ ملٹی ویری ایبل کیلکولس سے واقف ہیں وہ جانتے 

103
00:07:30,592 --> 00:07:34,654
ہوں گے کہ فنکشن کا میلان آپ کو تیز ترین چڑھائی کی سمت دیتا ہے، 

104
00:07:34,654 --> 00:07:38,780
فنکشن کو تیزی سے بڑھانے کے لیے آپ کو کس سمت میں قدم رکھنا چاہیے۔

105
00:07:39,560 --> 00:07:42,771
قدرتی طور پر کافی ہے، اس میلان کے منفی کو لینے سے آپ کو 

106
00:07:42,771 --> 00:07:46,040
قدم اٹھانے کی سمت ملتی ہے جو فنکشن کو تیزی سے کم کرتا ہے۔

107
00:07:47,240 --> 00:07:50,606
اس سے بھی زیادہ، اس گریڈینٹ ویکٹر کی لمبائی اس بات 

108
00:07:50,606 --> 00:07:53,840
کا اشارہ ہے کہ وہ سب سے کھڑی ڈھلوان کتنی کھڑی ہے۔

109
00:07:54,540 --> 00:07:57,461
اگر آپ ملٹی ویری ایبل کیلکولس سے ناواقف ہیں اور مزید جاننا چاہتے ہیں 

110
00:07:57,461 --> 00:08:00,340
تو اس موضوع پر خان اکیڈمی کے لیے میں نے کیے گئے کچھ کاموں کو دیکھیں۔

111
00:08:00,860 --> 00:08:04,442
سچ کہوں تو، اس وقت آپ اور میرے لیے جو چیز اہم ہے وہ یہ ہے کہ 

112
00:08:04,442 --> 00:08:08,024
اصولی طور پر اس ویکٹر کی گنتی کرنے کا ایک طریقہ موجود ہے، یہ 

113
00:08:08,024 --> 00:08:11,900
ویکٹر جو آپ کو بتاتا ہے کہ نیچے کی سمت کیا ہے اور یہ کتنی کھڑی ہے۔

114
00:08:12,400 --> 00:08:16,120
آپ ٹھیک ہو جائیں گے اگر آپ صرف اتنا جانتے ہیں اور آپ تفصیلات پر ٹھوس نہیں ہیں۔

115
00:08:17,200 --> 00:08:22,026
اگر آپ اسے حاصل کر سکتے ہیں تو، فنکشن کو کم سے کم کرنے کے لیے الگورتھم اس تدریجی سمت 

116
00:08:22,026 --> 00:08:26,740
کی گنتی کرنا ہے، پھر نیچے کی طرف ایک چھوٹا سا قدم اٹھائیں، اور اسے بار بار دہرائیں۔

117
00:08:27,700 --> 00:08:32,820
یہ ایک فنکشن کے لیے وہی بنیادی خیال ہے جس میں 2 ان پٹ کے بجائے 13,000 ان پٹ ہوتے ہیں۔

118
00:08:33,400 --> 00:08:36,493
ہمارے نیٹ ورک کے تمام 13,000 وزنوں اور تعصبات کو 

119
00:08:36,493 --> 00:08:39,460
ایک بڑے کالم ویکٹر میں ترتیب دینے کا تصور کریں۔

120
00:08:40,140 --> 00:08:44,978
لاگت کے فنکشن کا منفی میلان صرف ایک ویکٹر ہے، یہ اس بے حد بڑی ان 

121
00:08:44,978 --> 00:08:49,966
پٹ اسپیس کے اندر کچھ سمت ہے جو آپ کو بتاتی ہے کہ ان تمام نمبروں کو 

122
00:08:49,966 --> 00:08:54,880
کون سا جھٹکا لگانا لاگت کے فنکشن میں تیزی سے کمی کا سبب بن رہا ہے۔

123
00:08:55,640 --> 00:09:00,700
اور یقیناً، ہمارے خاص طور پر ڈیزائن کردہ لاگت کے فنکشن کے ساتھ، وزن اور تعصبات کو کم کرنے 

124
00:09:00,700 --> 00:09:05,647
کے لیے تبدیل کرنے کا مطلب ہے کہ تربیتی ڈیٹا کے ہر ٹکڑے پر نیٹ ورک کا آؤٹ پٹ 10 اقدار کی 

125
00:09:05,647 --> 00:09:09,864
بے ترتیب صف کی طرح کم نظر آتا ہے، اور ایک حقیقی فیصلہ جیسا کہ ہم چاہتے ہیں۔

126
00:09:09,864 --> 00:09:10,820
 یہ بنانے کے لئے.

127
00:09:11,440 --> 00:09:16,119
یہ یاد رکھنا ضروری ہے، اس لاگت کے فنکشن میں ٹریننگ کے تمام ڈیٹا پر اوسط شامل ہوتا ہے، 

128
00:09:16,119 --> 00:09:21,016
لہذا اگر آپ اسے کم سے کم کرتے ہیں، تو اس کا مطلب ہے کہ یہ ان تمام نمونوں پر بہتر کارکردگی 

129
00:09:21,016 --> 00:09:21,180
ہے۔

130
00:09:23,820 --> 00:09:27,192
اس میلان کو مؤثر طریقے سے کمپیوٹنگ کرنے کے لیے الگورتھم، جو مؤثر طریقے سے اس 

131
00:09:27,192 --> 00:09:30,476
بات کا دل ہے کہ نیورل نیٹ ورک کس طرح سیکھتا ہے، اسے بیک پروپیگیشن کہا جاتا 

132
00:09:30,476 --> 00:09:33,980
ہے، اور یہ وہی ہے جس کے بارے میں میں اگلی ویڈیو کے بارے میں بات کرنے جا رہا ہوں۔

133
00:09:34,660 --> 00:09:38,772
وہاں، میں واقعتاً یہ جاننے کے لیے وقت نکالنا چاہتا ہوں کہ تربیتی اعداد و شمار کے 

134
00:09:38,772 --> 00:09:43,088
دیئے گئے حصے کے لیے ہر وزن اور تعصب کے ساتھ کیا ہوتا ہے، متعلقہ کیلکولس اور فارمولوں 

135
00:09:43,088 --> 00:09:47,100
کے ڈھیر سے باہر کیا ہو رہا ہے اس کے لیے ایک بدیہی احساس دلانے کی کوشش کرتا ہوں۔

136
00:09:47,780 --> 00:09:51,141
یہیں، ابھی، بنیادی چیز جو میں آپ کو جاننا چاہتا ہوں، نفاذ کی 

137
00:09:51,141 --> 00:09:54,557
تفصیلات سے آزاد، یہ ہے کہ جب ہم نیٹ ورک لرننگ کے بارے میں بات 

138
00:09:54,557 --> 00:09:58,360
کرتے ہیں تو ہمارا مطلب یہ ہے کہ یہ صرف لاگت کے فنکشن کو کم کر رہا ہے۔

139
00:09:59,300 --> 00:10:03,513
اور نوٹس کریں، اس کا ایک نتیجہ یہ ہے کہ لاگت کے اس فنکشن کے لیے یہ ضروری ہے کہ 

140
00:10:03,513 --> 00:10:08,100
ہموار آؤٹ پٹ ہو، تاکہ ہم نیچے کی طرف تھوڑا سا قدم اٹھا کر مقامی کم از کم تلاش کر سکیں۔

141
00:10:09,260 --> 00:10:14,137
یہی وجہ ہے کہ، ویسے، مصنوعی نیوران میں مسلسل ایکٹیویشنز ہوتی ہیں، بجائے اس کے 

142
00:10:14,137 --> 00:10:19,140
کہ بائنری طریقے سے فعال یا غیر فعال ہونے کے، جس طرح سے حیاتیاتی نیوران ہوتے ہیں۔

143
00:10:20,220 --> 00:10:23,364
کسی فنکشن کے ان پٹ کو منفی میلان کے کچھ ملٹیپل سے 

144
00:10:23,364 --> 00:10:26,760
بار بار جھکانے کے اس عمل کو گریڈینٹ ڈیسنٹ کہا جاتا ہے۔

145
00:10:27,300 --> 00:10:29,989
یہ کچھ مقامی کم از کم لاگت کے فنکشن کی طرف اکٹھا ہونے 

146
00:10:29,989 --> 00:10:32,580
کا ایک طریقہ ہے، بنیادی طور پر اس گراف میں ایک وادی۔

147
00:10:33,440 --> 00:10:36,956
میں اب بھی دو ان پٹ کے ساتھ فنکشن کی تصویر دکھا رہا ہوں، یقیناً، 

148
00:10:36,956 --> 00:10:40,364
کیونکہ 13,000 جہتی ان پٹ اسپیس میں nudges آپ کے دماغ کو سمیٹنا 

149
00:10:40,364 --> 00:10:44,260
تھوڑا مشکل ہے، لیکن اس کے بارے میں سوچنے کا ایک اچھا غیر مقامی طریقہ ہے۔

150
00:10:45,080 --> 00:10:48,440
منفی میلان کا ہر جزو ہمیں دو چیزیں بتاتا ہے۔

151
00:10:49,060 --> 00:10:52,040
نشان، یقیناً، ہمیں بتاتا ہے کہ آیا ان پٹ ویکٹر کے 

152
00:10:52,040 --> 00:10:55,140
متعلقہ جزو کو اوپر یا نیچے کی طرف جھکایا جانا چاہیے۔

153
00:10:55,800 --> 00:10:59,159
لیکن اہم بات یہ ہے کہ ان تمام اجزاء کی نسبتی وسعت 

154
00:10:59,159 --> 00:11:02,720
آپ کو بتاتی ہے کہ کون سی تبدیلی زیادہ اہمیت رکھتی ہے۔

155
00:11:05,220 --> 00:11:08,997
آپ دیکھتے ہیں، ہمارے نیٹ ورک میں، کسی ایک وزن میں ایڈجسٹمنٹ کا لاگت کے 

156
00:11:08,997 --> 00:11:13,040
فنکشن پر کسی دوسرے وزن میں ایڈجسٹمنٹ کے مقابلے میں بہت زیادہ اثر پڑ سکتا ہے۔

157
00:11:14,800 --> 00:11:18,200
ان میں سے کچھ کنکشن ہمارے تربیتی ڈیٹا کے لیے زیادہ اہمیت رکھتے ہیں۔

158
00:11:19,320 --> 00:11:23,526
تو ایک طریقہ جس سے آپ ہمارے دماغی طور پر بڑے پیمانے پر لاگت کے فنکشن کے اس تدریجی 

159
00:11:23,526 --> 00:11:27,937
ویکٹر کے بارے میں سوچ سکتے ہیں وہ یہ ہے کہ یہ ہر وزن اور تعصب کی نسبتہ اہمیت کو انکوڈ 

160
00:11:27,937 --> 00:11:32,400
کرتا ہے، یعنی، ان میں سے کون سی تبدیلی آپ کے پیسے کے لیے سب سے زیادہ اثر ڈالنے والی ہے۔

161
00:11:33,620 --> 00:11:36,640
یہ واقعی سمت کے بارے میں سوچنے کا ایک اور طریقہ ہے۔

162
00:11:37,100 --> 00:11:42,155
ایک آسان مثال کے طور پر، اگر آپ کے پاس دو متغیرات کے ساتھ کچھ فنکشن ایک ان پٹ کے طور 

163
00:11:42,155 --> 00:11:47,092
پر ہے، اور آپ شمار کرتے ہیں کہ کسی خاص نقطہ پر اس کا میلان 3,1 کے طور پر نکلتا ہے، 

164
00:11:47,092 --> 00:11:50,482
تو ایک طرف آپ اس کی تشریح یہ کہہ سکتے ہیں کہ جب آپ ' 

165
00:11:50,482 --> 00:11:55,598
اس ان پٹ پر دوبارہ کھڑے ہونے سے، اس سمت میں حرکت کرنے سے فنکشن میں تیزی سے اضافہ ہوتا 

166
00:11:55,598 --> 00:12:00,535
ہے، کہ جب آپ فنکشن کو ان پٹ پوائنٹس کے جہاز کے اوپر گراف کرتے ہیں، تو وہی ویکٹر آپ 

167
00:12:00,535 --> 00:12:02,260
کو سیدھی اوپر کی سمت دیتا ہے۔

168
00:12:02,860 --> 00:12:07,402
لیکن پڑھنے کا ایک اور طریقہ یہ ہے کہ اس پہلے متغیر میں ہونے والی تبدیلیوں کی 

169
00:12:07,402 --> 00:12:12,121
اہمیت دوسرے متغیر میں ہونے والی تبدیلیوں کے مقابلے میں 3 گنا زیادہ ہے، کہ کم از 

170
00:12:12,121 --> 00:12:16,900
کم متعلقہ ان پٹ کے پڑوس میں، x-value کو جھکانا آپ کے لیے بہت زیادہ دھڑکتا ہے۔ ہرن

171
00:12:19,880 --> 00:12:22,340
آئیے زوم آؤٹ کریں اور خلاصہ کریں کہ ہم اب تک کہاں ہیں۔

172
00:12:22,840 --> 00:12:26,408
نیٹ ورک بذات خود یہ فنکشن ہے جس میں 784 ان پٹ اور 10 آؤٹ 

173
00:12:26,408 --> 00:12:30,040
پٹس ہیں، جو ان تمام وزنی رقوم کے لحاظ سے بیان کیے گئے ہیں۔

174
00:12:30,640 --> 00:12:33,680
لاگت کا فنکشن اس کے اوپر پیچیدگی کی ایک پرت ہے۔

175
00:12:33,980 --> 00:12:37,653
یہ 13,000 وزن اور تعصبات کو ان پٹ کے طور پر لیتا ہے اور 

176
00:12:37,653 --> 00:12:41,720
تربیتی مثالوں کی بنیاد پر گھٹیا پن کا ایک پیمانہ نکال دیتا ہے۔

177
00:12:42,440 --> 00:12:46,900
اور لاگت کے فنکشن کا میلان اب بھی پیچیدگی کی ایک اور تہہ ہے۔

178
00:12:47,360 --> 00:12:50,819
یہ ہمیں بتاتا ہے کہ ان تمام وزنوں اور تعصبات کو کون سے جھٹکے لگتے ہیں جو 

179
00:12:50,819 --> 00:12:54,325
لاگت کے فنکشن کی قدر میں تیز ترین تبدیلی کا باعث بنتے ہیں، جس کی تشریح آپ 

180
00:12:54,325 --> 00:12:57,880
یہ کہہ سکتے ہیں کہ کون سے وزن میں کون سی تبدیلی سب سے زیادہ اہمیت رکھتی ہے۔

181
00:13:02,560 --> 00:13:06,075
لہذا، جب آپ نیٹ ورک کو بے ترتیب وزن اور تعصبات کے ساتھ شروع کرتے ہیں، اور 

182
00:13:06,075 --> 00:13:09,542
اس گریڈینٹ ڈیسنٹ عمل کی بنیاد پر انہیں کئی بار ایڈجسٹ کرتے ہیں، تو یہ ان 

183
00:13:09,542 --> 00:13:13,200
تصاویر پر کتنی اچھی کارکردگی کا مظاہرہ کرتا ہے جو اس نے پہلے کبھی نہیں دیکھا؟

184
00:13:14,100 --> 00:13:17,980
جو میں نے یہاں بیان کیا ہے، ہر ایک میں 16 نیورونز کی دو چھپی ہوئی تہوں 

185
00:13:17,980 --> 00:13:21,915
کے ساتھ، زیادہ تر جمالیاتی وجوہات کی بناء پر منتخب کیا گیا ہے، برا نہیں 

186
00:13:21,915 --> 00:13:25,960
ہے، جو کہ اس کی نظر آنے والی تقریباً 96% نئی تصویروں کی درجہ بندی کرتا ہے۔

187
00:13:26,680 --> 00:13:29,587
اور ایمانداری سے، اگر آپ ان میں سے کچھ مثالوں کو دیکھیں جن پر یہ 

188
00:13:29,587 --> 00:13:32,540
گڑبڑ کرتا ہے، تو آپ اسے تھوڑا سا سست کرنے پر مجبور محسوس کرتے ہیں۔

189
00:13:36,220 --> 00:13:38,891
اب اگر آپ پوشیدہ پرت کے ڈھانچے کے ساتھ کھیلتے ہیں اور 

190
00:13:38,891 --> 00:13:41,760
ایک دو ٹویکس بناتے ہیں، تو آپ اسے 98% تک حاصل کر سکتے ہیں۔

191
00:13:41,760 --> 00:13:42,720
اور یہ بہت اچھا ہے!

192
00:13:43,020 --> 00:13:46,700
یہ سب سے بہتر نہیں ہے، آپ یقینی طور پر اس سادہ وینیلا نیٹ ورک سے زیادہ نفیس 

193
00:13:46,700 --> 00:13:50,283
حاصل کر کے بہتر کارکردگی حاصل کر سکتے ہیں، لیکن یہ دیکھتے ہوئے کہ ابتدائی 

194
00:13:50,283 --> 00:13:54,011
کام کتنا مشکل ہے، میرے خیال میں کسی بھی نیٹ ورک کے بارے میں ایسی ناقابل یقین 

195
00:13:54,011 --> 00:13:58,030
چیز ہے جو ایسی تصاویر پر اچھی طرح سے کر رہی ہے جو اس نے پہلے کبھی نہیں دیکھی ہو گی۔

196
00:13:58,030 --> 00:14:01,420
 ہم نے کبھی خاص طور پر یہ نہیں بتایا کہ کون سے نمونوں کی تلاش کرنی ہے۔

197
00:14:02,560 --> 00:14:06,202
اصل میں، میں نے جس طرح سے اس ڈھانچے کی حوصلہ افزائی کی وہ اس امید کو بیان 

198
00:14:06,202 --> 00:14:09,796
کرتے ہوئے تھا جو ہمیں ہو سکتی ہے، کہ دوسری تہہ چھوٹے کناروں پر اُٹھ سکتی 

199
00:14:09,796 --> 00:14:13,389
ہے، کہ تیسری تہہ ان کناروں کو لوپس اور لمبی لکیروں کو پہچاننے کے لیے جوڑ 

200
00:14:13,389 --> 00:14:17,180
دے گی، اور یہ کہ وہ ٹکڑے ٹکڑے ہو سکتے ہیں۔ ہندسوں کو پہچاننے کے لیے ایک ساتھ۔

201
00:14:17,960 --> 00:14:20,400
تو کیا ہمارا نیٹ ورک دراصل یہی کر رہا ہے؟

202
00:14:21,079 --> 00:14:24,400
ٹھیک ہے، اس کے لیے کم از کم، بالکل نہیں۔

203
00:14:24,820 --> 00:14:29,050
یاد رکھیں کہ پچھلی ویڈیو میں ہم نے کس طرح دیکھا کہ پہلی پرت کے تمام نیوران 

204
00:14:29,050 --> 00:14:32,998
سے دوسری پرت میں دیئے گئے نیوران تک کنکشن کے وزن کو ایک دیئے گئے پکسل 

205
00:14:32,998 --> 00:14:37,060
پیٹرن کے طور پر تصور کیا جا سکتا ہے جسے دوسری تہہ کا نیوران اٹھا رہا ہے؟

206
00:14:37,780 --> 00:14:42,964
ٹھیک ہے، جب ہم اصل میں ان ٹرانزیشنز سے وابستہ وزن کے لیے، پہلی پرت سے اگلی 

207
00:14:42,964 --> 00:14:48,218
پرت تک، الگ تھلگ چھوٹے کناروں کو یہاں اور وہاں اٹھانے کے بجائے کرتے ہیں، تو 

208
00:14:48,218 --> 00:14:53,680
وہ بالکل بے ترتیب نظر آتے ہیں، بس کچھ بہت ہی ڈھیلے نمونوں کے ساتھ۔ درمیان وہاں.

209
00:14:53,760 --> 00:14:58,807
ایسا لگتا ہے کہ ممکنہ وزن اور تعصبات کی ناقابل یقین حد تک بڑی 13,000 جہتی جگہ میں، ہمارے 

210
00:14:58,807 --> 00:15:03,571
نیٹ ورک نے اپنے آپ کو ایک خوش کن مقامی کم از کم پایا جو کہ کامیابی کے ساتھ زیادہ تر 

211
00:15:03,571 --> 00:15:08,506
تصاویر کی درجہ بندی کرنے کے باوجود، ان نمونوں پر بالکل درست نہیں ہوتا جس کی ہم امید کر 

212
00:15:08,506 --> 00:15:08,960
رہے تھے۔

213
00:15:09,780 --> 00:15:11,779
اور واقعی اس مقام کو گھر پہنچانے کے لیے، دیکھیں 

214
00:15:11,779 --> 00:15:13,820
کہ جب آپ بے ترتیب تصویر ڈالتے ہیں تو کیا ہوتا ہے۔

215
00:15:14,320 --> 00:15:19,322
اگر سسٹم سمارٹ تھا، تو آپ اس سے غیر یقینی محسوس کرنے کی توقع کر سکتے ہیں، شاید ان 10 آؤٹ 

216
00:15:19,322 --> 00:15:24,268
پٹ نیورونز میں سے کسی کو بھی چالو نہ کر رہے ہوں یا ان سب کو یکساں طور پر فعال نہ کر رہے 

217
00:15:24,268 --> 00:15:29,214
ہوں، لیکن اس کے بجائے یہ آپ کو اعتماد کے ساتھ کچھ بکواس جواب دیتا ہے، گویا یہ یقینی طور 

218
00:15:29,214 --> 00:15:34,160
پر محسوس ہوتا ہے کہ یہ بے ترتیب شور۔ ایک 5 ہے جیسا کہ یہ کرتا ہے کہ 5 کی اصل تصویر 5 ہے۔

219
00:15:34,540 --> 00:15:37,577
مختلف طریقے سے بیان کیا جائے، یہاں تک کہ اگر یہ نیٹ ورک ہندسوں کو اچھی 

220
00:15:37,577 --> 00:15:40,700
طرح پہچان سکتا ہے، تو اسے کوئی اندازہ نہیں ہے کہ انہیں کس طرح کھینچنا ہے۔

221
00:15:41,420 --> 00:15:45,240
اس میں سے بہت کچھ اس لیے ہے کہ یہ اس قدر سختی سے مجبور تربیتی سیٹ اپ ہے۔

222
00:15:45,880 --> 00:15:47,740
میرا مطلب ہے، اپنے آپ کو یہاں نیٹ ورک کے جوتوں میں ڈالیں۔

223
00:15:48,140 --> 00:15:52,513
اس کے نقطہ نظر سے، پوری کائنات ایک چھوٹے سے گرڈ میں مرکز میں واضح طور پر 

224
00:15:52,513 --> 00:15:56,706
متعین غیر متحرک ہندسوں کے سوا کچھ پر مشتمل نہیں ہے، اور اس کی لاگت کی 

225
00:15:56,706 --> 00:16:01,080
تقریب نے اسے اپنے فیصلوں پر مکمل اعتماد کے سوا کچھ بننے کی ترغیب نہیں دی۔

226
00:16:02,120 --> 00:16:04,607
تو اس کے ساتھ اس تصویر کے طور پر کہ وہ دوسری پرت کے نیوران 

227
00:16:04,607 --> 00:16:07,095
واقعی کیا کر رہے ہیں، آپ حیران ہوں گے کہ میں اس نیٹ ورک کو 

228
00:16:07,095 --> 00:16:09,920
کناروں اور نمونوں کو اٹھانے کی ترغیب کے ساتھ کیوں متعارف کرواؤں گا۔

229
00:16:09,920 --> 00:16:12,300
میرا مطلب ہے، یہ بالکل بھی ایسا نہیں ہے جو یہ کر رہا ہے۔

230
00:16:13,380 --> 00:16:17,180
ٹھیک ہے، اس کا مقصد ہمارا آخری مقصد نہیں ہے، بلکہ ایک نقطہ آغاز ہے۔

231
00:16:17,640 --> 00:16:21,966
سچ کہوں تو، یہ پرانی ٹیکنالوجی ہے، جس کی 80 اور 90 کی دہائیوں میں تحقیق کی گئی تھی، 

232
00:16:21,966 --> 00:16:26,293
اور اس سے پہلے کہ آپ مزید تفصیلی جدید قسموں کو سمجھ سکیں، آپ کو اسے سمجھنے کی ضرورت 

233
00:16:26,293 --> 00:16:30,568
ہے، اور یہ واضح طور پر کچھ دلچسپ مسائل کو حل کرنے کی صلاحیت رکھتی ہے، لیکن آپ جتنا 

234
00:16:30,568 --> 00:16:34,740
زیادہ اس میں کھودیں گے وہ پوشیدہ پرتیں واقعی کر رہی ہیں، یہ جتنا کم ذہین لگتا ہے۔

235
00:16:38,480 --> 00:16:42,487
نیٹ ورکس آپ کے سیکھنے کے طریقہ سے ایک لمحے کے لیے توجہ مرکوز کرنا، یہ صرف اس صورت 

236
00:16:42,487 --> 00:16:46,300
میں ہوگا جب آپ کسی نہ کسی طرح یہاں موجود مواد کے ساتھ سرگرمی سے مشغول ہوجائیں۔

237
00:16:47,060 --> 00:16:50,577
ایک بہت ہی آسان چیز جو میں آپ سے کرنا چاہتا ہوں وہ یہ ہے کہ ابھی توقف 

238
00:16:50,577 --> 00:16:54,196
کریں اور ایک لمحے کے لیے گہرائی سے سوچیں کہ آپ اس سسٹم میں کیا تبدیلیاں 

239
00:16:54,196 --> 00:16:57,613
لا سکتے ہیں اور اگر آپ چاہتے ہیں کہ یہ کناروں اور نمونوں جیسی چیزوں 

240
00:16:57,613 --> 00:17:00,880
کو بہتر طریقے سے اٹھانا چاہتے ہیں تو یہ تصاویر کو کیسے سمجھتا ہے۔

241
00:17:01,479 --> 00:17:05,290
لیکن اس سے بہتر، حقیقت میں مواد کے ساتھ مشغول ہونے کے لیے، میں گہری 

242
00:17:05,290 --> 00:17:09,099
تعلیم اور اعصابی نیٹ ورکس پر مائیکل نیلسن کی کتاب کی سفارش کرتا ہوں۔

243
00:17:09,680 --> 00:17:14,137
اس میں، آپ اس درست مثال کے لیے ڈاؤن لوڈ اور کھیلنے کے لیے کوڈ اور ڈیٹا تلاش 

244
00:17:14,137 --> 00:17:18,359
کر سکتے ہیں، اور کتاب آپ کو قدم بہ قدم چلائے گی کہ وہ کوڈ کیا کر رہا ہے۔

245
00:17:19,300 --> 00:17:23,455
حیرت انگیز بات یہ ہے کہ یہ کتاب مفت اور عوامی طور پر دستیاب ہے، لہذا اگر آپ اس سے کچھ 

246
00:17:23,455 --> 00:17:27,660
حاصل کرتے ہیں، تو نیلسن کی کوششوں کے لیے عطیہ کرنے میں میرے ساتھ شامل ہونے پر غور کریں۔

247
00:17:27,660 --> 00:17:32,017
میں نے تفصیل میں کچھ دوسرے وسائل کو بھی جوڑا ہے جو مجھے بہت پسند ہیں، 

248
00:17:32,017 --> 00:17:36,500
بشمول Chris Ola کی غیر معمولی اور خوبصورت بلاگ پوسٹ اور ڈسٹل میں مضامین۔

249
00:17:38,280 --> 00:17:41,148
گزشتہ چند منٹوں کے لیے چیزوں کو یہاں بند کرنے کے لیے، میں لیشا 

250
00:17:41,148 --> 00:17:43,880
لی کے ساتھ کیے گئے انٹرویو کے ٹکڑوں میں واپس جانا چاہتا ہوں۔

251
00:17:44,300 --> 00:17:47,720
آپ اسے آخری ویڈیو سے یاد کر سکتے ہیں، اس نے اپنا پی ایچ ڈی کام ڈیپ لرننگ میں کیا۔

252
00:17:48,300 --> 00:17:51,994
اس چھوٹے سے ٹکڑوں میں وہ دو حالیہ کاغذات کے بارے میں بات کرتی ہے جو واقعی اس بات 

253
00:17:51,994 --> 00:17:55,780
کی کھوج کرتے ہیں کہ تصویر کی شناخت کے کچھ جدید نیٹ ورک حقیقت میں کیسے سیکھ رہے ہیں۔

254
00:17:56,120 --> 00:17:59,355
صرف اس بات کو ترتیب دینے کے لیے کہ ہم بات چیت میں کہاں تھے، پہلے پیپر 

255
00:17:59,355 --> 00:18:02,453
نے ان خاص طور پر گہرے نیورل نیٹ ورکس میں سے ایک لیا جو کہ تصویر کی 

256
00:18:02,453 --> 00:18:05,642
شناخت میں واقعی اچھا ہے، اور اسے مناسب طریقے سے لیبل والے ڈیٹاسیٹ پر 

257
00:18:05,642 --> 00:18:08,740
تربیت دینے کے بجائے، تربیت سے پہلے اردگرد کے تمام لیبلز کو بدل دیا۔

258
00:18:09,480 --> 00:18:13,246
ظاہر ہے کہ یہاں جانچ کی درستگی بے ترتیب سے بہتر نہیں تھی، کیونکہ ہر چیز پر 

259
00:18:13,246 --> 00:18:17,063
صرف تصادفی طور پر لیبل لگا ہوا ہے، لیکن یہ پھر بھی وہی تربیت کی درستگی حاصل 

260
00:18:17,063 --> 00:18:20,880
کرنے میں کامیاب رہا جیسا کہ آپ مناسب طریقے سے لیبل والے ڈیٹاسیٹ پر کرتے ہیں۔

261
00:18:21,600 --> 00:18:26,376
بنیادی طور پر، اس مخصوص نیٹ ورک کے لیے لاکھوں وزن اس کے لیے صرف بے ترتیب ڈیٹا کو 

262
00:18:26,376 --> 00:18:31,270
حفظ کرنے کے لیے کافی تھے، جس سے یہ سوال پیدا ہوتا ہے کہ کیا اس لاگت کے فنکشن کو کم 

263
00:18:31,270 --> 00:18:36,400
سے کم کرنا دراصل تصویر میں موجود کسی بھی ساخت سے مطابقت رکھتا ہے، یا کیا یہ محض حفظ ہے؟

264
00:18:51,440 --> 00:18:56,378
اگر آپ اس درستگی کے منحنی خطوط پر نظر ڈالتے ہیں، اگر آپ صرف ایک بے ترتیب 

265
00:18:56,378 --> 00:19:01,384
ڈیٹاسیٹ پر تربیت لے رہے تھے، تو وہ وکر کی قسم تقریباً ایک لکیری انداز میں 

266
00:19:01,384 --> 00:19:06,525
بہت آہستہ آہستہ نیچے جاتی ہے، لہذا آپ واقعی اس مقامی کم سے کم ممکنہ کو تلاش 

267
00:19:06,525 --> 00:19:12,140
کرنے کے لیے جدوجہد کر رہے ہیں، آپ جانتے ہیں ، صحیح وزن جو آپ کو درستگی حاصل کرے گا۔

268
00:19:12,240 --> 00:19:17,627
جب کہ اگر آپ واقعتاً ایک سٹرکچرڈ ڈیٹاسیٹ کی تربیت کر رہے ہیں، جس میں صحیح لیبلز ہیں، تو 

269
00:19:17,627 --> 00:19:22,893
آپ شروع میں تھوڑا سا چکر لگاتے ہیں، لیکن پھر آپ اس درستگی کی سطح تک پہنچنے کے لیے بہت 

270
00:19:22,893 --> 00:19:28,220
تیزی سے گر جاتے ہیں، اور اس طرح کچھ معنوں میں یہ اس مقامی میکسما کو تلاش کرنا آسان تھا۔

271
00:19:28,540 --> 00:19:33,577
اور اس کے بارے میں جو بات بھی دلچسپ تھی وہ یہ ہے کہ یہ حقیقت میں چند سال پہلے کا ایک 

272
00:19:33,577 --> 00:19:38,792
اور مقالہ سامنے لاتا ہے، جس میں نیٹ ورک کی تہوں کے بارے میں بہت زیادہ آسانیاں ہیں، لیکن 

273
00:19:38,792 --> 00:19:44,008
ایک نتیجہ یہ کہہ رہا تھا کہ اگر آپ اصلاح کے منظر نامے کو دیکھیں تو کیسے، مقامی منیما جو 

274
00:19:44,008 --> 00:19:49,282
یہ نیٹ ورک سیکھنے کا رجحان رکھتے ہیں وہ درحقیقت مساوی معیار کے ہوتے ہیں، اس لیے کسی لحاظ 

275
00:19:49,282 --> 00:19:54,320
سے اگر آپ کا ڈیٹاسیٹ تشکیل شدہ ہے، تو آپ اسے زیادہ آسانی سے تلاش کرنے کے قابل ہوں گے۔

276
00:19:58,160 --> 00:20:01,180
میرا شکریہ، ہمیشہ کی طرح، آپ میں سے جو پیٹریون کی حمایت کر رہے ہیں۔

277
00:20:01,520 --> 00:20:04,083
میں پہلے بھی کہہ چکا ہوں کہ گیم چینجر پیٹریون کیا 

278
00:20:04,083 --> 00:20:06,800
ہے، لیکن یہ ویڈیوز واقعی آپ کے بغیر ممکن نہیں ہوں گی۔

279
00:20:07,460 --> 00:20:10,045
میں سیریز میں ان ابتدائی ویڈیوز کی حمایت میں VC فرم 

280
00:20:10,045 --> 00:20:12,780
Amplify Partners کا بھی خصوصی شکریہ ادا کرنا چاہتا ہوں۔

