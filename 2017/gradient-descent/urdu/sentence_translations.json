[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "آخری ویڈیو میں نے نیورل نیٹ ورک کا ڈھانچہ پیش کیا۔",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "میں یہاں ایک فوری ریکیپ دوں گا تاکہ یہ ہمارے ذہنوں میں تازہ ہو، اور پھر اس ویڈیو کے لیے میرے دو اہم مقاصد ہیں۔",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "سب سے پہلے تدریجی نزول کے آئیڈیا کو متعارف کرانا ہے، جو نہ صرف اس بات کی نشاندہی کرتا ہے کہ نیورل نیٹ ورک کس طرح سیکھتے ہیں، بلکہ بہت سی دوسری مشین لرننگ بھی کیسے کام کرتی ہے۔",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "پھر اس کے بعد ہم تھوڑا اور کھوج لگائیں گے کہ یہ مخصوص نیٹ ورک کس طرح کام کرتا ہے، اور نیوران کی وہ پوشیدہ پرتیں کیا ڈھونڈتی ہیں۔",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "ایک یاد دہانی کے طور پر، ہمارا مقصد یہاں ہاتھ سے لکھے ہوئے ہندسوں کی شناخت کی بہترین مثال ہے، نیورل نیٹ ورکس کی ہیلو ورلڈ۔",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "یہ ہندسے 28x28 پکسل گرڈ پر پیش کیے گئے ہیں، ہر پکسل کی قدر 0 اور 1 کے درمیان ہوتی ہے۔",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "یہ وہی ہیں جو نیٹ ورک کی ان پٹ پرت میں 784 نیوران کی فعالیت کا تعین کرتے ہیں۔",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "اور پھر مندرجہ ذیل تہوں میں ہر نیوران کے لیے ایکٹیویشن پچھلی پرت میں تمام ایکٹیویشنز کے وزنی مجموعہ پر مبنی ہے، نیز کچھ خاص نمبر جنہیں تعصب کہا جاتا ہے۔",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "پھر آپ اس رقم کو کسی اور فنکشن کے ساتھ تحریر کریں، جیسے سگمائیڈ اسکویشیفیکیشن، یا ریلو، جس طرح میں نے پچھلی ویڈیو سے گزرا۔",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "مجموعی طور پر، 16 نیورونز کے ساتھ دو چھپی ہوئی تہوں کے کسی حد تک من مانی انتخاب کو دیکھتے ہوئے، نیٹ ورک میں تقریباً 13,000 وزن اور تعصبات ہیں جنہیں ہم ایڈجسٹ کر سکتے ہیں، اور یہی قدریں اس بات کا تعین کرتی ہیں کہ نیٹ ورک اصل میں کیا کرتا ہے۔",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "پھر جب ہم کہتے ہیں کہ یہ نیٹ ورک کسی دیے گئے ہندسے کی درجہ بندی کرتا ہے تو ہمارا مطلب یہ ہے کہ آخری تہہ میں موجود ان 10 نیورونز میں سے سب سے روشن اس ہندسے کے مساوی ہے۔",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "اور یاد رکھیں، تہہ دار ڈھانچے کے لیے ہمارے ذہن میں جو محرک تھا وہ یہ تھا کہ شاید دوسری تہہ کناروں پر اُٹھ سکتی ہے، اور تیسری تہہ لوپس اور لکیروں جیسے نمونوں پر اُٹھ سکتی ہے، اور آخری تہہ ان کو جوڑ سکتی ہے۔ ہندسوں کو پہچاننے کے لیے پیٹرن۔",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "تو یہاں، ہم سیکھتے ہیں کہ نیٹ ورک کیسے سیکھتا ہے۔",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "ہم جو چاہتے ہیں وہ ایک الگورتھم ہے جہاں آپ اس نیٹ ورک کو تربیتی اعداد و شمار کا ایک پورا گروپ دکھا سکتے ہیں، جو ہاتھ سے لکھے ہندسوں کی مختلف تصاویر کے ایک گروپ کی شکل میں آتا ہے، اس کے ساتھ ساتھ ان کے لیبلز کے ساتھ کہ وہ کیا ہونا چاہیے، اور یہ ان 13,000 وزنوں اور تعصبات کو ایڈجسٹ کریں تاکہ تربیتی ڈیٹا پر اس کی کارکردگی کو بہتر بنایا جا سکے۔",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "امید ہے کہ اس تہہ دار ڈھانچے کا مطلب یہ ہوگا کہ جو کچھ یہ سیکھتا ہے وہ اس تربیتی ڈیٹا سے باہر کی تصاویر کو عام کرتا ہے۔",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "جس طرح سے ہم جانچتے ہیں وہ یہ ہے کہ آپ نیٹ ورک کو تربیت دینے کے بعد، آپ اسے مزید لیبل والا ڈیٹا دکھاتے ہیں جو اس نے پہلے کبھی نہیں دیکھا، اور آپ دیکھتے ہیں کہ یہ ان نئی تصاویر کو کس حد تک درست طریقے سے درجہ بندی کرتا ہے۔",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "خوش قسمتی سے ہمارے لیے، اور جس چیز سے یہ ایک عام مثال بنتی ہے، وہ یہ ہے کہ MNIST ڈیٹابیس کے پیچھے اچھے لوگوں نے دسیوں ہزار ہاتھ سے لکھی ہندسوں کی تصاویر کا ایک مجموعہ جمع کیا ہے، ہر ایک پر ان نمبروں کے ساتھ لیبل لگا ہوا ہے جن پر وہ سوچ رہے ہیں۔ ہونا",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "اور کسی مشین کو سیکھنے کے طور پر بیان کرنا جتنا اشتعال انگیز ہے، ایک بار جب آپ دیکھتے ہیں کہ یہ کیسے کام کرتی ہے، تو یہ کچھ پاگل سائنس فائی بنیاد کی طرح بہت کم محسوس ہوتی ہے، اور بہت زیادہ کیلکولس ورزش کی طرح۔",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "میرا مطلب ہے، بنیادی طور پر یہ کسی خاص فنکشن کی کم از کم تلاش کرنے پر آتا ہے۔",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "یاد رکھیں، تصوراتی طور پر، ہم ہر نیوران کے بارے میں سوچ رہے ہیں کہ وہ پچھلی پرت کے تمام نیورانوں سے جڑا ہوا ہے، اور اس کی ایکٹیویشن کی وضاحت کرنے والے وزنی رقم میں وزن ان رابطوں کی طاقتوں کی طرح ہے، اور تعصب اس کا کچھ اشارہ ہے۔ چاہے وہ نیوران فعال ہو یا غیر فعال۔",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "اور چیزوں کو شروع کرنے کے لئے، ہم صرف ان تمام وزنوں اور تعصبات کو مکمل طور پر تصادفی طور پر شروع کرنے جا رہے ہیں۔",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "یہ کہنے کی ضرورت نہیں کہ یہ نیٹ ورک ایک دی گئی تربیتی مثال پر کافی خوفناک کارکردگی کا مظاہرہ کرنے والا ہے، کیونکہ یہ صرف بے ترتیب کچھ کر رہا ہے۔",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "مثال کے طور پر، آپ 3 کی اس تصویر میں فیڈ کرتے ہیں، اور آؤٹ پٹ پرت بالکل گڑبڑ کی طرح دکھائی دیتی ہے۔",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "لہذا آپ جو کرتے ہیں وہ ایک لاگت کے فنکشن کی وضاحت کرتا ہے، کمپیوٹر کو بتانے کا ایک طریقہ، نہیں، خراب کمپیوٹر، اس آؤٹ پٹ میں ایکٹیویشن ہونا چاہیے جو زیادہ تر نیوران کے لیے 0 ہیں، لیکن اس نیوران کے لیے 1، جو آپ نے مجھے دیا ہے وہ سراسر ردی ہے۔",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "یہ کہنے کے لیے کہ تھوڑا سا اور ریاضیاتی طور پر، آپ ان میں سے ہر ایک کوڑے دان کے آؤٹ پٹ ایکٹیویشن اور اس قدر کے درمیان فرق کے مربعوں کو جوڑتے ہیں جو آپ چاہتے ہیں کہ ان کے پاس ہوں، اور اسے ہم ایک تربیتی مثال کی قیمت کہیں گے۔",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "نوٹ کریں کہ جب نیٹ ورک اعتماد کے ساتھ تصویر کی صحیح درجہ بندی کرتا ہے تو یہ رقم چھوٹی ہوتی ہے، لیکن یہ بڑی ہوتی ہے جب نیٹ ورک ایسا لگتا ہے کہ یہ نہیں جانتا کہ وہ کیا کر رہا ہے۔",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "تو پھر آپ جو کرتے ہیں وہ یہ ہے کہ آپ کے اختیار میں موجود دسیوں ہزار تربیتی مثالوں میں سے اوسط لاگت پر غور کریں۔",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "یہ اوسط لاگت ہمارا پیمانہ ہے کہ نیٹ ورک کتنا گھٹیا ہے، اور کمپیوٹر کو کتنا برا محسوس ہونا چاہیے۔",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "اور یہ ایک پیچیدہ چیز ہے۔",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "یاد رکھیں کہ نیٹ ورک بنیادی طور پر کس طرح ایک فنکشن تھا، جو 784 نمبروں کو ان پٹ، پکسل ویلیوز کے طور پر لیتا ہے، اور اس کے آؤٹ پٹ کے طور پر 10 نمبروں کو نکالتا ہے، اور ایک لحاظ سے یہ ان تمام وزنوں اور تعصبات سے پیرامیٹرائز ہوتا ہے؟",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "ٹھیک ہے لاگت کا فنکشن اس کے اوپر پیچیدگی کی ایک پرت ہے۔",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "یہ ان 13,000 یا اس سے زیادہ وزن اور تعصبات کو اپنے ان پٹ کے طور پر لیتا ہے، اور یہ بتاتا ہے کہ وہ وزن اور تعصبات کتنے خراب ہیں، اور جس طرح سے اس کی وضاحت کی گئی ہے اس کا انحصار تربیتی ڈیٹا کے دسیوں ہزار ٹکڑوں پر نیٹ ورک کے رویے پر ہوتا ہے۔",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "یہ بہت سوچنے کی بات ہے۔",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "لیکن کمپیوٹر کو صرف یہ بتانا کہ وہ کیا گھٹیا کام کر رہا ہے زیادہ مددگار نہیں ہے۔",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "آپ اسے بتانا چاہتے ہیں کہ ان وزنوں اور تعصبات کو کیسے تبدیل کیا جائے تاکہ یہ بہتر ہو جائے۔",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "اسے آسان بنانے کے لیے، 13,000 ان پٹ کے ساتھ کسی فنکشن کا تصور کرنے کے لیے جدوجہد کرنے کے بجائے، صرف ایک سادہ فنکشن کا تصور کریں جس میں ایک نمبر بطور ان پٹ اور ایک نمبر آؤٹ پٹ کے طور پر ہو۔",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "آپ کو ایک ان پٹ کیسے ملتا ہے جو اس فنکشن کی قدر کو کم کرتا ہے؟",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "کیلکولس کے طلباء کو معلوم ہوگا کہ آپ بعض اوقات اس کم از کم واضح طور پر اندازہ لگا سکتے ہیں، لیکن یہ واقعی پیچیدہ افعال کے لیے ہمیشہ ممکن نہیں ہوتا، یقیناً ہمارے پاگل پیچیدہ نیورل نیٹ ورک کی لاگت کے فنکشن کے لیے اس صورت حال کے 13,000 ان پٹ ورژن میں نہیں۔",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "ایک زیادہ لچکدار حربہ یہ ہے کہ کسی بھی ان پٹ سے آغاز کریں، اور یہ معلوم کریں کہ اس آؤٹ پٹ کو کم کرنے کے لیے آپ کو کس سمت کو قدم رکھنا چاہیے۔",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "خاص طور پر، اگر آپ اس فنکشن کی ڈھلوان کا پتہ لگا سکتے ہیں جہاں آپ ہیں، تو بائیں طرف شفٹ کریں اگر وہ ڈھلوان مثبت ہے، اور اگر وہ ڈھلوان منفی ہے تو ان پٹ کو دائیں طرف شفٹ کریں۔",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "اگر آپ یہ بار بار کرتے ہیں، ہر ایک نقطہ پر نئی ڈھلوان کی جانچ کرتے ہوئے اور مناسب قدم اٹھاتے ہوئے، آپ فنکشن کے کچھ مقامی کم از کم سے رجوع کرنے جا رہے ہیں۔",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "یہاں آپ کے ذہن میں جو تصویر ہو سکتی ہے وہ ایک گیند ہے جو ایک پہاڑی سے نیچے گر رہی ہے۔",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "غور کریں، یہاں تک کہ اس واقعی آسان بنائے گئے واحد ان پٹ فنکشن کے لیے، بہت سی ممکنہ وادییں ہیں جن میں آپ اتر سکتے ہیں، اس پر منحصر ہے کہ آپ کس بے ترتیب ان پٹ سے شروع کرتے ہیں، اور اس بات کی کوئی گارنٹی نہیں ہے کہ آپ جس مقامی کم از کم میں اتریں گے وہ سب سے چھوٹی ممکنہ قدر ہوگی۔ لاگت کی تقریب.",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "یہ ہمارے نیورل نیٹ ورک کیس کو بھی لے جائے گا۔",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "میں آپ کو یہ بھی دیکھنا چاہتا ہوں کہ اگر آپ اپنے قدموں کے سائز کو ڈھلوان کے متناسب بناتے ہیں، تو جب ڈھلوان کم سے کم کی طرف چپٹا ہوتا ہے، تو آپ کے قدم چھوٹے سے چھوٹے ہوتے جاتے ہیں، اور یہ آپ کو اوور شوٹنگ سے بچانے میں مدد کرتا ہے۔",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "پیچیدگی کو تھوڑا سا بڑھاتے ہوئے، دو ان پٹ اور ایک آؤٹ پٹ کے ساتھ ایک فنکشن کا تصور کریں۔",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "آپ ان پٹ کی جگہ کو xy-plane کے طور پر سوچ سکتے ہیں، اور لاگت کے فنکشن کو اس کے اوپر کی سطح کے طور پر گراف کیا جا رہا ہے۔",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "فنکشن کی ڈھلوان کے بارے میں پوچھنے کے بجائے، آپ کو یہ پوچھنا ہوگا کہ آپ کو اس ان پٹ اسپیس میں کس سمت جانا چاہئے تاکہ فنکشن کے آؤٹ پٹ کو تیزی سے کم کیا جاسکے۔",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "دوسرے الفاظ میں، نیچے کی سمت کیا ہے؟",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "ایک بار پھر، اس پہاڑی سے نیچے گرنے والی گیند کے بارے میں سوچنا مددگار ہے۔",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "آپ میں سے جو لوگ ملٹی ویری ایبل کیلکولس سے واقف ہیں وہ جانتے ہوں گے کہ فنکشن کا میلان آپ کو تیز ترین چڑھائی کی سمت دیتا ہے، فنکشن کو تیزی سے بڑھانے کے لیے آپ کو کس سمت میں قدم رکھنا چاہیے۔",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "قدرتی طور پر کافی ہے، اس میلان کے منفی کو لینے سے آپ کو قدم اٹھانے کی سمت ملتی ہے جو فنکشن کو تیزی سے کم کرتا ہے۔",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "اس سے بھی زیادہ، اس گریڈینٹ ویکٹر کی لمبائی اس بات کا اشارہ ہے کہ وہ سب سے کھڑی ڈھلوان کتنی کھڑی ہے۔",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "اگر آپ ملٹی ویری ایبل کیلکولس سے ناواقف ہیں اور مزید جاننا چاہتے ہیں تو اس موضوع پر خان اکیڈمی کے لیے میں نے کیے گئے کچھ کاموں کو دیکھیں۔",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "سچ کہوں تو، اس وقت آپ اور میرے لیے جو چیز اہم ہے وہ یہ ہے کہ اصولی طور پر اس ویکٹر کی گنتی کرنے کا ایک طریقہ موجود ہے، یہ ویکٹر جو آپ کو بتاتا ہے کہ نیچے کی سمت کیا ہے اور یہ کتنی کھڑی ہے۔",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "آپ ٹھیک ہو جائیں گے اگر آپ صرف اتنا جانتے ہیں اور آپ تفصیلات پر ٹھوس نہیں ہیں۔",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "اگر آپ اسے حاصل کر سکتے ہیں تو، فنکشن کو کم سے کم کرنے کے لیے الگورتھم اس تدریجی سمت کی گنتی کرنا ہے، پھر نیچے کی طرف ایک چھوٹا سا قدم اٹھائیں، اور اسے بار بار دہرائیں۔",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "یہ ایک فنکشن کے لیے وہی بنیادی خیال ہے جس میں 2 ان پٹ کے بجائے 13,000 ان پٹ ہوتے ہیں۔",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "ہمارے نیٹ ورک کے تمام 13,000 وزنوں اور تعصبات کو ایک بڑے کالم ویکٹر میں ترتیب دینے کا تصور کریں۔",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "لاگت کے فنکشن کا منفی میلان صرف ایک ویکٹر ہے، یہ اس بے حد بڑی ان پٹ اسپیس کے اندر کچھ سمت ہے جو آپ کو بتاتی ہے کہ ان تمام نمبروں کو کون سا جھٹکا لگانا لاگت کے فنکشن میں تیزی سے کمی کا سبب بن رہا ہے۔",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "اور یقیناً، ہمارے خاص طور پر ڈیزائن کردہ لاگت کے فنکشن کے ساتھ، وزن اور تعصبات کو کم کرنے کے لیے تبدیل کرنے کا مطلب ہے کہ تربیتی ڈیٹا کے ہر ٹکڑے پر نیٹ ورک کا آؤٹ پٹ 10 اقدار کی بے ترتیب صف کی طرح کم نظر آتا ہے، اور ایک حقیقی فیصلہ جیسا کہ ہم چاہتے ہیں۔ یہ بنانے کے لئے.",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "یہ یاد رکھنا ضروری ہے، اس لاگت کے فنکشن میں ٹریننگ کے تمام ڈیٹا پر اوسط شامل ہوتا ہے، لہذا اگر آپ اسے کم سے کم کرتے ہیں، تو اس کا مطلب ہے کہ یہ ان تمام نمونوں پر بہتر کارکردگی ہے۔",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "اس میلان کو مؤثر طریقے سے کمپیوٹنگ کرنے کے لیے الگورتھم، جو مؤثر طریقے سے اس بات کا دل ہے کہ نیورل نیٹ ورک کس طرح سیکھتا ہے، اسے بیک پروپیگیشن کہا جاتا ہے، اور یہ وہی ہے جس کے بارے میں میں اگلی ویڈیو کے بارے میں بات کرنے جا رہا ہوں۔",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "وہاں، میں واقعتاً یہ جاننے کے لیے وقت نکالنا چاہتا ہوں کہ تربیتی اعداد و شمار کے دیئے گئے حصے کے لیے ہر وزن اور تعصب کے ساتھ کیا ہوتا ہے، متعلقہ کیلکولس اور فارمولوں کے ڈھیر سے باہر کیا ہو رہا ہے اس کے لیے ایک بدیہی احساس دلانے کی کوشش کرتا ہوں۔",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "یہیں، ابھی، بنیادی چیز جو میں آپ کو جاننا چاہتا ہوں، نفاذ کی تفصیلات سے آزاد، یہ ہے کہ جب ہم نیٹ ورک لرننگ کے بارے میں بات کرتے ہیں تو ہمارا مطلب یہ ہے کہ یہ صرف لاگت کے فنکشن کو کم کر رہا ہے۔",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "اور نوٹس کریں، اس کا ایک نتیجہ یہ ہے کہ لاگت کے اس فنکشن کے لیے یہ ضروری ہے کہ ہموار آؤٹ پٹ ہو، تاکہ ہم نیچے کی طرف تھوڑا سا قدم اٹھا کر مقامی کم از کم تلاش کر سکیں۔",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "یہی وجہ ہے کہ، ویسے، مصنوعی نیوران میں مسلسل ایکٹیویشنز ہوتی ہیں، بجائے اس کے کہ بائنری طریقے سے فعال یا غیر فعال ہونے کے، جس طرح سے حیاتیاتی نیوران ہوتے ہیں۔",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "کسی فنکشن کے ان پٹ کو منفی میلان کے کچھ ملٹیپل سے بار بار جھکانے کے اس عمل کو گریڈینٹ ڈیسنٹ کہا جاتا ہے۔",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "یہ کچھ مقامی کم از کم لاگت کے فنکشن کی طرف اکٹھا ہونے کا ایک طریقہ ہے، بنیادی طور پر اس گراف میں ایک وادی۔",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "میں اب بھی دو ان پٹ کے ساتھ فنکشن کی تصویر دکھا رہا ہوں، یقیناً، کیونکہ 13,000 جہتی ان پٹ اسپیس میں nudges آپ کے دماغ کو سمیٹنا تھوڑا مشکل ہے، لیکن اس کے بارے میں سوچنے کا ایک اچھا غیر مقامی طریقہ ہے۔",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "منفی میلان کا ہر جزو ہمیں دو چیزیں بتاتا ہے۔",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "نشان، یقیناً، ہمیں بتاتا ہے کہ آیا ان پٹ ویکٹر کے متعلقہ جزو کو اوپر یا نیچے کی طرف جھکایا جانا چاہیے۔",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "لیکن اہم بات یہ ہے کہ ان تمام اجزاء کی نسبتی وسعت آپ کو بتاتی ہے کہ کون سی تبدیلی زیادہ اہمیت رکھتی ہے۔",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "آپ دیکھتے ہیں، ہمارے نیٹ ورک میں، کسی ایک وزن میں ایڈجسٹمنٹ کا لاگت کے فنکشن پر کسی دوسرے وزن میں ایڈجسٹمنٹ کے مقابلے میں بہت زیادہ اثر پڑ سکتا ہے۔",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "ان میں سے کچھ کنکشن ہمارے تربیتی ڈیٹا کے لیے زیادہ اہمیت رکھتے ہیں۔",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "تو ایک طریقہ جس سے آپ ہمارے دماغی طور پر بڑے پیمانے پر لاگت کے فنکشن کے اس تدریجی ویکٹر کے بارے میں سوچ سکتے ہیں وہ یہ ہے کہ یہ ہر وزن اور تعصب کی نسبتہ اہمیت کو انکوڈ کرتا ہے، یعنی، ان میں سے کون سی تبدیلی آپ کے پیسے کے لیے سب سے زیادہ اثر ڈالنے والی ہے۔",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "یہ واقعی سمت کے بارے میں سوچنے کا ایک اور طریقہ ہے۔",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "ایک آسان مثال کے طور پر، اگر آپ کے پاس دو متغیرات کے ساتھ کچھ فنکشن ایک ان پٹ کے طور پر ہے، اور آپ شمار کرتے ہیں کہ کسی خاص نقطہ پر اس کا میلان 3,1 کے طور پر نکلتا ہے، تو ایک طرف آپ اس کی تشریح یہ کہہ سکتے ہیں کہ جب آپ ' اس ان پٹ پر دوبارہ کھڑے ہونے سے، اس سمت میں حرکت کرنے سے فنکشن میں تیزی سے اضافہ ہوتا ہے، کہ جب آپ فنکشن کو ان پٹ پوائنٹس کے جہاز کے اوپر گراف کرتے ہیں، تو وہی ویکٹر آپ کو سیدھی اوپر کی سمت دیتا ہے۔",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "لیکن پڑھنے کا ایک اور طریقہ یہ ہے کہ اس پہلے متغیر میں ہونے والی تبدیلیوں کی اہمیت دوسرے متغیر میں ہونے والی تبدیلیوں کے مقابلے میں 3 گنا زیادہ ہے، کہ کم از کم متعلقہ ان پٹ کے پڑوس میں، x-value کو جھکانا آپ کے لیے بہت زیادہ دھڑکتا ہے۔ ہرن",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "آئیے زوم آؤٹ کریں اور خلاصہ کریں کہ ہم اب تک کہاں ہیں۔",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "نیٹ ورک بذات خود یہ فنکشن ہے جس میں 784 ان پٹ اور 10 آؤٹ پٹس ہیں، جو ان تمام وزنی رقوم کے لحاظ سے بیان کیے گئے ہیں۔",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "لاگت کا فنکشن اس کے اوپر پیچیدگی کی ایک پرت ہے۔",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "یہ 13,000 وزن اور تعصبات کو ان پٹ کے طور پر لیتا ہے اور تربیتی مثالوں کی بنیاد پر گھٹیا پن کا ایک پیمانہ نکال دیتا ہے۔",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "اور لاگت کے فنکشن کا میلان اب بھی پیچیدگی کی ایک اور تہہ ہے۔",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "یہ ہمیں بتاتا ہے کہ ان تمام وزنوں اور تعصبات کو کون سے جھٹکے لگتے ہیں جو لاگت کے فنکشن کی قدر میں تیز ترین تبدیلی کا باعث بنتے ہیں، جس کی تشریح آپ یہ کہہ سکتے ہیں کہ کون سے وزن میں کون سی تبدیلی سب سے زیادہ اہمیت رکھتی ہے۔",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "لہذا، جب آپ نیٹ ورک کو بے ترتیب وزن اور تعصبات کے ساتھ شروع کرتے ہیں، اور اس گریڈینٹ ڈیسنٹ عمل کی بنیاد پر انہیں کئی بار ایڈجسٹ کرتے ہیں، تو یہ ان تصاویر پر کتنی اچھی کارکردگی کا مظاہرہ کرتا ہے جو اس نے پہلے کبھی نہیں دیکھا؟",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "جو میں نے یہاں بیان کیا ہے، ہر ایک میں 16 نیورونز کی دو چھپی ہوئی تہوں کے ساتھ، زیادہ تر جمالیاتی وجوہات کی بناء پر منتخب کیا گیا ہے، برا نہیں ہے، جو کہ اس کی نظر آنے والی تقریباً 96% نئی تصویروں کی درجہ بندی کرتا ہے۔",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "اور ایمانداری سے، اگر آپ ان میں سے کچھ مثالوں کو دیکھیں جن پر یہ گڑبڑ کرتا ہے، تو آپ اسے تھوڑا سا سست کرنے پر مجبور محسوس کرتے ہیں۔",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "اب اگر آپ پوشیدہ پرت کے ڈھانچے کے ساتھ کھیلتے ہیں اور ایک دو ٹویکس بناتے ہیں، تو آپ اسے 98% تک حاصل کر سکتے ہیں۔",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "اور یہ بہت اچھا ہے!",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "یہ سب سے بہتر نہیں ہے، آپ یقینی طور پر اس سادہ وینیلا نیٹ ورک سے زیادہ نفیس حاصل کر کے بہتر کارکردگی حاصل کر سکتے ہیں، لیکن یہ دیکھتے ہوئے کہ ابتدائی کام کتنا مشکل ہے، میرے خیال میں کسی بھی نیٹ ورک کے بارے میں ایسی ناقابل یقین چیز ہے جو ایسی تصاویر پر اچھی طرح سے کر رہی ہے جو اس نے پہلے کبھی نہیں دیکھی ہو گی۔ ہم نے کبھی خاص طور پر یہ نہیں بتایا کہ کون سے نمونوں کی تلاش کرنی ہے۔",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "اصل میں، میں نے جس طرح سے اس ڈھانچے کی حوصلہ افزائی کی وہ اس امید کو بیان کرتے ہوئے تھا جو ہمیں ہو سکتی ہے، کہ دوسری تہہ چھوٹے کناروں پر اُٹھ سکتی ہے، کہ تیسری تہہ ان کناروں کو لوپس اور لمبی لکیروں کو پہچاننے کے لیے جوڑ دے گی، اور یہ کہ وہ ٹکڑے ٹکڑے ہو سکتے ہیں۔ ہندسوں کو پہچاننے کے لیے ایک ساتھ۔",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "تو کیا ہمارا نیٹ ورک دراصل یہی کر رہا ہے؟",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "ٹھیک ہے، اس کے لیے کم از کم، بالکل نہیں۔",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "یاد رکھیں کہ پچھلی ویڈیو میں ہم نے کس طرح دیکھا کہ پہلی پرت کے تمام نیوران سے دوسری پرت میں دیئے گئے نیوران تک کنکشن کے وزن کو ایک دیئے گئے پکسل پیٹرن کے طور پر تصور کیا جا سکتا ہے جسے دوسری تہہ کا نیوران اٹھا رہا ہے؟",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "ٹھیک ہے، جب ہم اصل میں ان ٹرانزیشنز سے وابستہ وزن کے لیے، پہلی پرت سے اگلی پرت تک، الگ تھلگ چھوٹے کناروں کو یہاں اور وہاں اٹھانے کے بجائے کرتے ہیں، تو وہ بالکل بے ترتیب نظر آتے ہیں، بس کچھ بہت ہی ڈھیلے نمونوں کے ساتھ۔ درمیان وہاں.",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "ایسا لگتا ہے کہ ممکنہ وزن اور تعصبات کی ناقابل یقین حد تک بڑی 13,000 جہتی جگہ میں، ہمارے نیٹ ورک نے اپنے آپ کو ایک خوش کن مقامی کم از کم پایا جو کہ کامیابی کے ساتھ زیادہ تر تصاویر کی درجہ بندی کرنے کے باوجود، ان نمونوں پر بالکل درست نہیں ہوتا جس کی ہم امید کر رہے تھے۔",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "اور واقعی اس مقام کو گھر پہنچانے کے لیے، دیکھیں کہ جب آپ بے ترتیب تصویر ڈالتے ہیں تو کیا ہوتا ہے۔",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "اگر سسٹم سمارٹ تھا، تو آپ اس سے غیر یقینی محسوس کرنے کی توقع کر سکتے ہیں، شاید ان 10 آؤٹ پٹ نیورونز میں سے کسی کو بھی چالو نہ کر رہے ہوں یا ان سب کو یکساں طور پر فعال نہ کر رہے ہوں، لیکن اس کے بجائے یہ آپ کو اعتماد کے ساتھ کچھ بکواس جواب دیتا ہے، گویا یہ یقینی طور پر محسوس ہوتا ہے کہ یہ بے ترتیب شور۔ ایک 5 ہے جیسا کہ یہ کرتا ہے کہ 5 کی اصل تصویر 5 ہے۔",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "مختلف طریقے سے بیان کیا جائے، یہاں تک کہ اگر یہ نیٹ ورک ہندسوں کو اچھی طرح پہچان سکتا ہے، تو اسے کوئی اندازہ نہیں ہے کہ انہیں کس طرح کھینچنا ہے۔",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "اس میں سے بہت کچھ اس لیے ہے کہ یہ اس قدر سختی سے مجبور تربیتی سیٹ اپ ہے۔",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "میرا مطلب ہے، اپنے آپ کو یہاں نیٹ ورک کے جوتوں میں ڈالیں۔",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "اس کے نقطہ نظر سے، پوری کائنات ایک چھوٹے سے گرڈ میں مرکز میں واضح طور پر متعین غیر متحرک ہندسوں کے سوا کچھ پر مشتمل نہیں ہے، اور اس کی لاگت کی تقریب نے اسے اپنے فیصلوں پر مکمل اعتماد کے سوا کچھ بننے کی ترغیب نہیں دی۔",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "تو اس کے ساتھ اس تصویر کے طور پر کہ وہ دوسری پرت کے نیوران واقعی کیا کر رہے ہیں، آپ حیران ہوں گے کہ میں اس نیٹ ورک کو کناروں اور نمونوں کو اٹھانے کی ترغیب کے ساتھ کیوں متعارف کرواؤں گا۔",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "میرا مطلب ہے، یہ بالکل بھی ایسا نہیں ہے جو یہ کر رہا ہے۔",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "ٹھیک ہے، اس کا مقصد ہمارا آخری مقصد نہیں ہے، بلکہ ایک نقطہ آغاز ہے۔",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "سچ کہوں تو، یہ پرانی ٹیکنالوجی ہے، جس کی 80 اور 90 کی دہائیوں میں تحقیق کی گئی تھی، اور اس سے پہلے کہ آپ مزید تفصیلی جدید قسموں کو سمجھ سکیں، آپ کو اسے سمجھنے کی ضرورت ہے، اور یہ واضح طور پر کچھ دلچسپ مسائل کو حل کرنے کی صلاحیت رکھتی ہے، لیکن آپ جتنا زیادہ اس میں کھودیں گے وہ پوشیدہ پرتیں واقعی کر رہی ہیں، یہ جتنا کم ذہین لگتا ہے۔",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "نیٹ ورکس آپ کے سیکھنے کے طریقہ سے ایک لمحے کے لیے توجہ مرکوز کرنا، یہ صرف اس صورت میں ہوگا جب آپ کسی نہ کسی طرح یہاں موجود مواد کے ساتھ سرگرمی سے مشغول ہوجائیں۔",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "ایک بہت ہی آسان چیز جو میں آپ سے کرنا چاہتا ہوں وہ یہ ہے کہ ابھی توقف کریں اور ایک لمحے کے لیے گہرائی سے سوچیں کہ آپ اس سسٹم میں کیا تبدیلیاں لا سکتے ہیں اور اگر آپ چاہتے ہیں کہ یہ کناروں اور نمونوں جیسی چیزوں کو بہتر طریقے سے اٹھانا چاہتے ہیں تو یہ تصاویر کو کیسے سمجھتا ہے۔",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "لیکن اس سے بہتر، حقیقت میں مواد کے ساتھ مشغول ہونے کے لیے، میں گہری تعلیم اور اعصابی نیٹ ورکس پر مائیکل نیلسن کی کتاب کی سفارش کرتا ہوں۔",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "اس میں، آپ اس درست مثال کے لیے ڈاؤن لوڈ اور کھیلنے کے لیے کوڈ اور ڈیٹا تلاش کر سکتے ہیں، اور کتاب آپ کو قدم بہ قدم چلائے گی کہ وہ کوڈ کیا کر رہا ہے۔",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "حیرت انگیز بات یہ ہے کہ یہ کتاب مفت اور عوامی طور پر دستیاب ہے، لہذا اگر آپ اس سے کچھ حاصل کرتے ہیں، تو نیلسن کی کوششوں کے لیے عطیہ کرنے میں میرے ساتھ شامل ہونے پر غور کریں۔",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "میں نے تفصیل میں کچھ دوسرے وسائل کو بھی جوڑا ہے جو مجھے بہت پسند ہیں، بشمول Chris Ola کی غیر معمولی اور خوبصورت بلاگ پوسٹ اور ڈسٹل میں مضامین۔",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "گزشتہ چند منٹوں کے لیے چیزوں کو یہاں بند کرنے کے لیے، میں لیشا لی کے ساتھ کیے گئے انٹرویو کے ٹکڑوں میں واپس جانا چاہتا ہوں۔",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "آپ اسے آخری ویڈیو سے یاد کر سکتے ہیں، اس نے اپنا پی ایچ ڈی کام ڈیپ لرننگ میں کیا۔",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "اس چھوٹے سے ٹکڑوں میں وہ دو حالیہ کاغذات کے بارے میں بات کرتی ہے جو واقعی اس بات کی کھوج کرتے ہیں کہ تصویر کی شناخت کے کچھ جدید نیٹ ورک حقیقت میں کیسے سیکھ رہے ہیں۔",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "صرف اس بات کو ترتیب دینے کے لیے کہ ہم بات چیت میں کہاں تھے، پہلے پیپر نے ان خاص طور پر گہرے نیورل نیٹ ورکس میں سے ایک لیا جو کہ تصویر کی شناخت میں واقعی اچھا ہے، اور اسے مناسب طریقے سے لیبل والے ڈیٹاسیٹ پر تربیت دینے کے بجائے، تربیت سے پہلے اردگرد کے تمام لیبلز کو بدل دیا۔",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "ظاہر ہے کہ یہاں جانچ کی درستگی بے ترتیب سے بہتر نہیں تھی، کیونکہ ہر چیز پر صرف تصادفی طور پر لیبل لگا ہوا ہے، لیکن یہ پھر بھی وہی تربیت کی درستگی حاصل کرنے میں کامیاب رہا جیسا کہ آپ مناسب طریقے سے لیبل والے ڈیٹاسیٹ پر کرتے ہیں۔",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "بنیادی طور پر، اس مخصوص نیٹ ورک کے لیے لاکھوں وزن اس کے لیے صرف بے ترتیب ڈیٹا کو حفظ کرنے کے لیے کافی تھے، جس سے یہ سوال پیدا ہوتا ہے کہ کیا اس لاگت کے فنکشن کو کم سے کم کرنا دراصل تصویر میں موجود کسی بھی ساخت سے مطابقت رکھتا ہے، یا کیا یہ محض حفظ ہے؟",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "اگر آپ اس درستگی کے منحنی خطوط پر نظر ڈالتے ہیں، اگر آپ صرف ایک بے ترتیب ڈیٹاسیٹ پر تربیت لے رہے تھے، تو وہ وکر کی قسم تقریباً ایک لکیری انداز میں بہت آہستہ آہستہ نیچے جاتی ہے، لہذا آپ واقعی اس مقامی کم سے کم ممکنہ کو تلاش کرنے کے لیے جدوجہد کر رہے ہیں، آپ جانتے ہیں ، صحیح وزن جو آپ کو درستگی حاصل کرے گا۔",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "جب کہ اگر آپ واقعتاً ایک سٹرکچرڈ ڈیٹاسیٹ کی تربیت کر رہے ہیں، جس میں صحیح لیبلز ہیں، تو آپ شروع میں تھوڑا سا چکر لگاتے ہیں، لیکن پھر آپ اس درستگی کی سطح تک پہنچنے کے لیے بہت تیزی سے گر جاتے ہیں، اور اس طرح کچھ معنوں میں یہ اس مقامی میکسما کو تلاش کرنا آسان تھا۔",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "اور اس کے بارے میں جو بات بھی دلچسپ تھی وہ یہ ہے کہ یہ حقیقت میں چند سال پہلے کا ایک اور مقالہ سامنے لاتا ہے، جس میں نیٹ ورک کی تہوں کے بارے میں بہت زیادہ آسانیاں ہیں، لیکن ایک نتیجہ یہ کہہ رہا تھا کہ اگر آپ اصلاح کے منظر نامے کو دیکھیں تو کیسے، مقامی منیما جو یہ نیٹ ورک سیکھنے کا رجحان رکھتے ہیں وہ درحقیقت مساوی معیار کے ہوتے ہیں، اس لیے کسی لحاظ سے اگر آپ کا ڈیٹاسیٹ تشکیل شدہ ہے، تو آپ اسے زیادہ آسانی سے تلاش کرنے کے قابل ہوں گے۔",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "میرا شکریہ، ہمیشہ کی طرح، آپ میں سے جو پیٹریون کی حمایت کر رہے ہیں۔",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "میں پہلے بھی کہہ چکا ہوں کہ گیم چینجر پیٹریون کیا ہے، لیکن یہ ویڈیوز واقعی آپ کے بغیر ممکن نہیں ہوں گی۔",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "میں سیریز میں ان ابتدائی ویڈیوز کی حمایت میں VC فرم Amplify Partners کا بھی خصوصی شکریہ ادا کرنا چاہتا ہوں۔",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]