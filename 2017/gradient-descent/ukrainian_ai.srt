1
00:00:00,000 --> 00:00:07,240
У минулому відео я описав структуру нейронної мережі.

2
00:00:07,240 --> 00:00:11,560
Я дам тут короткий підсумок, щоб це було свіжим у нашій пам’яті,

3
00:00:11,560 --> 00:00:13,160
а потім у мене є дві головні цілі для цього відео.

4
00:00:13,160 --> 00:00:17,960
Перший полягає в тому, щоб представити ідею градієнтного спуску, яка лежить в основі не тільки

5
00:00:17,960 --> 00:00:20,800
того, як навчаються нейронні мережі, але й того, як працює багато інших машинного навчання.

6
00:00:20,800 --> 00:00:25,160
Після цього ми детальніше розглянемо, як працює ця конкретна

7
00:00:25,160 --> 00:00:29,560
мережа, і що шукають ці приховані шари нейронів.

8
00:00:29,560 --> 00:00:34,680
Нагадуємо, що нашою метою тут є класичний приклад

9
00:00:34,680 --> 00:00:37,080
розпізнавання рукописних цифр, привіт, світ нейронних мереж.

10
00:00:37,080 --> 00:00:42,160
Ці цифри відображаються на сітці 28x28 пікселів, кожен піксель

11
00:00:42,160 --> 00:00:44,260
має значення відтінків сірого від 0 до 1.

12
00:00:44,260 --> 00:00:51,400
Саме вони визначають активацію 784 нейронів на вхідному рівні мережі.

13
00:00:51,400 --> 00:00:56,880
Активація для кожного нейрона в наступних шарах базується на зваженій сумі всіх

14
00:00:56,880 --> 00:01:02,300
активацій у попередньому шарі, плюс деяке спеціальне число, яке називається зміщенням.

15
00:01:02,300 --> 00:01:07,480
Ви складаєте цю суму за допомогою якоїсь іншої функції, як-от

16
00:01:07,480 --> 00:01:09,640
сигмоїда, або ReLU, як я пройшов у минулому відео.

17
00:01:09,640 --> 00:01:14,960
Загалом, враховуючи дещо довільний вибір двох прихованих шарів із 16 нейронами кожен,

18
00:01:14,960 --> 00:01:20,940
мережа має близько 13 000 ваг і зміщень, які ми можемо

19
00:01:20,940 --> 00:01:25,320
налаштувати, і саме ці значення визначають, що саме мережа насправді робить.

20
00:01:25,320 --> 00:01:29,800
Коли ми кажемо, що ця мережа класифікує певну цифру, ми маємо на

21
00:01:29,800 --> 00:01:34,080
увазі те, що найяскравіший із 10 нейронів останнього шару відповідає цій цифрі.

22
00:01:34,080 --> 00:01:39,240
І пам’ятайте, мотивація, яку ми мали на увазі для багатошарової структури,

23
00:01:39,240 --> 00:01:43,920
полягала в тому, що, можливо, другий шар міг би підхопити краї,

24
00:01:43,920 --> 00:01:48,640
третій шар міг би підхопити візерунки, як-от петлі та лінії, а

25
00:01:48,640 --> 00:01:49,640
останній міг би просто з’єднати ці візерунки, щоб розпізнавати цифри.

26
00:01:49,640 --> 00:01:52,880
Отже, ми дізнаємося, як навчається мережа.

27
00:01:52,880 --> 00:01:56,880
Те, що ми хочемо, — це алгоритм, за допомогою якого ви можете показати

28
00:01:56,880 --> 00:02:01,540
цій мережі цілий набір навчальних даних, які надходять у формі набору різних

29
00:02:01,540 --> 00:02:06,360
зображень рукописних цифр разом із мітками, якими вони мають бути, і це буде

30
00:02:06,360 --> 00:02:10,760
відкоригувати ці 13 000 ваг і зміщень, щоб покращити ефективність тренувальних даних.

31
00:02:10,760 --> 00:02:15,540
Сподіваюся, ця багаторівнева структура означатиме, що те, що

32
00:02:15,540 --> 00:02:17,840
вона вивчає, узагальнює зображення за межами навчальних даних.

33
00:02:17,840 --> 00:02:22,240
Ми перевіряємо це так: після навчання мережі ви показуєте їй більше

34
00:02:22,240 --> 00:02:31,160
позначених даних і бачите, наскільки точно вона класифікує ці нові зображення.

35
00:02:31,160 --> 00:02:34,760
На щастя для нас, і те, що робить цей приклад типовим для початку, полягає

36
00:02:34,760 --> 00:02:39,520
в тому, що хороші люди, що стоять за базою даних MNIST, зібрали колекцію з

37
00:02:39,520 --> 00:02:45,080
десятків тисяч рукописних зображень цифр, кожне з яких позначено номерами, якими вони повинні бути.

38
00:02:45,080 --> 00:02:49,920
І як би не було провокаційно описувати машину як навчальну, коли ви бачите, як вона працює, це

39
00:02:49,920 --> 00:02:55,560
стає набагато менше схожим на якусь божевільну науково-фантастичну передумову, а набагато більше схоже на вправу з обчислення.

40
00:02:55,560 --> 00:03:01,040
Я маю на увазі, що в основному це зводиться до пошуку мінімуму певної функції.

41
00:03:01,040 --> 00:03:06,480
Пам’ятайте, концептуально ми думаємо про те, що кожен нейрон пов’язаний з

42
00:03:06,480 --> 00:03:11,440
усіма нейронами попереднього шару, і ваги у зваженій сумі, що визначає

43
00:03:11,440 --> 00:03:16,400
його активацію, схожі на силу цих зв’язків, а зміщення є певним

44
00:03:16,400 --> 00:03:19,780
показником чи цей нейрон має тенденцію бути активним чи неактивним.

45
00:03:19,780 --> 00:03:23,300
І для початку ми просто ініціалізуємо всі

46
00:03:23,300 --> 00:03:25,020
ці ваги та зміщення абсолютно випадковим чином.

47
00:03:25,020 --> 00:03:29,100
Зайве говорити, що ця мережа працюватиме жахливо на даному

48
00:03:29,100 --> 00:03:31,180
навчальному прикладі, оскільки вона просто робить щось випадкове.

49
00:03:31,180 --> 00:03:36,820
Наприклад, ви подаєте на це зображення 3, а вихідний шар виглядає просто безладом.

50
00:03:36,820 --> 00:03:43,340
Отже, що ви робите, це визначаєте функцію вартості, спосіб сказати комп’ютеру, ні, поганий комп’ютер, що

51
00:03:43,340 --> 00:03:48,940
вихід має мати активації, які дорівнюють 0 для більшості нейронів, але 1 для цього нейрона.

52
00:03:48,980 --> 00:03:51,740
Те, що ти мені дав, це чисте сміття.

53
00:03:51,740 --> 00:03:56,740
Якщо говорити трохи математичніше, ви складаєте квадрати різниць між кожною з

54
00:03:56,740 --> 00:04:01,980
цих активацій виведення сміття та значенням, яке ви хочете, щоб вони

55
00:04:01,980 --> 00:04:06,020
мали, і це те, що ми називатимемо вартістю одного навчального прикладу.

56
00:04:06,020 --> 00:04:12,660
Зауважте, що ця сума невелика, коли мережа впевнено правильно класифікує зображення,

57
00:04:12,660 --> 00:04:18,820
але велика, коли здається, що мережа не знає, що вона робить.

58
00:04:18,820 --> 00:04:23,860
Отже, що ви робите, це розглядаєте середню вартість усіх

59
00:04:23,860 --> 00:04:27,580
десятків тисяч навчальних прикладів, які є у вашому розпорядженні.

60
00:04:27,580 --> 00:04:32,300
Ця середня вартість є нашим показником того, наскільки поганою

61
00:04:32,300 --> 00:04:33,300
є мережа та наскільки погано має працювати комп’ютер.

62
00:04:33,300 --> 00:04:35,300
І це складна річ.

63
00:04:35,300 --> 00:04:40,380
Пам’ятаєте, як сама мережа була в основному функцією, яка приймає 784 числа

64
00:04:40,380 --> 00:04:46,100
як вхідні дані, значення пікселів, і викидає 10 чисел як свій вихід,

65
00:04:46,100 --> 00:04:49,700
і в певному сенсі вона параметризована всіма цими вагами та зміщеннями?

66
00:04:49,700 --> 00:04:53,340
Функція витрат є ще одним шаром складності.

67
00:04:53,340 --> 00:04:59,140
Він бере на вхід приблизно 13 000 ваг і упереджень і видає одне

68
00:04:59,140 --> 00:05:04,620
число, яке описує, наскільки погані ці ваги та упередження, і спосіб його

69
00:05:04,620 --> 00:05:09,140
визначення залежить від поведінки мережі над усіма десятками тисяч фрагментів навчальних даних.

70
00:05:09,140 --> 00:05:12,460
Це багато про що думати.

71
00:05:12,460 --> 00:05:16,380
Але просто говорити комп’ютеру, яку погану роботу він робить, не дуже корисно.

72
00:05:16,380 --> 00:05:21,300
Ви хочете сказати йому, як змінити ці ваги та упередження, щоб воно стало кращим.

73
00:05:21,300 --> 00:05:25,580
Щоб зробити це легше, замість того, щоб намагатися уявити функцію з 13 000 входами, просто

74
00:05:25,580 --> 00:05:31,440
уявіть просту функцію, яка має одне число як вхід і одне число як вихід.

75
00:05:31,440 --> 00:05:36,420
Як знайти вхідні дані, які мінімізують значення цієї функції?

76
00:05:36,420 --> 00:05:41,300
Студенти, які вивчають обчислення, знатимуть, що іноді можна чітко визначити цей мінімум, але це

77
00:05:41,340 --> 00:05:46,620
не завжди можливо для справді складних функцій, а особливо не у версії цієї ситуації

78
00:05:46,620 --> 00:05:51,640
з 13 000 вхідних даних для нашої божевільно складної функції вартості нейронної мережі.

79
00:05:51,640 --> 00:05:56,820
Більш гнучка тактика полягає в тому, щоб почати з будь-якого входу

80
00:05:56,820 --> 00:05:59,860
та визначити, у якому напрямку слід рухатися, щоб знизити цей вихід.

81
00:05:59,860 --> 00:06:05,020
Зокрема, якщо ви можете визначити нахил функції, де ви

82
00:06:05,020 --> 00:06:09,280
знаходитесь, тоді перемістіть ліворуч, якщо цей нахил додатний, і

83
00:06:09,280 --> 00:06:12,720
перемістіть вхідні дані праворуч, якщо цей нахил від’ємний.

84
00:06:12,720 --> 00:06:17,040
Якщо ви робите це неодноразово, у кожній точці перевіряючи новий нахил

85
00:06:17,040 --> 00:06:20,680
і роблячи відповідний крок, ви наблизитесь до деякого локального мінімуму функції.

86
00:06:20,680 --> 00:06:24,600
І зображення, яке ви можете мати тут на увазі, це м’яч, що котиться з пагорба.

87
00:06:24,600 --> 00:06:29,380
І зауважте, що навіть для цієї справді спрощеної функції єдиного введення існує

88
00:06:29,380 --> 00:06:34,220
багато можливих долин, у які ви можете потрапити, залежно від того,

89
00:06:34,220 --> 00:06:38,460
з якого випадкового введення ви почнете, і немає гарантії, що локальний

90
00:06:38,460 --> 00:06:39,460
мінімум, у який ви потрапите, буде найменшим можливим значенням функції витрат.

91
00:06:39,460 --> 00:06:43,180
Це також перенесеться на нашу нейронну мережу.

92
00:06:43,180 --> 00:06:48,140
І я також хочу, щоб ви помітили, що якщо ви робите розмір

93
00:06:48,140 --> 00:06:52,920
кроку пропорційним схилу, тоді, коли схил вирівнюється до мінімуму, ваші кроки

94
00:06:52,920 --> 00:06:56,020
стають все меншими і меншими, і це допомагає вам уникнути перевищення.

95
00:06:56,020 --> 00:07:01,640
Трохи збільшуючи складність, уявіть натомість функцію з двома входами та одним виходом.

96
00:07:01,640 --> 00:07:06,360
Ви можете подумати про вхідний простір як про площину

97
00:07:06,360 --> 00:07:09,020
xy, а функцію вартості як поверхню над нею.

98
00:07:09,020 --> 00:07:13,600
Замість того, щоб запитувати про нахил функції, ви повинні запитати, у якому напрямку

99
00:07:13,600 --> 00:07:19,780
вам слід зробити крок у цьому вхідному просторі, щоб найшвидше зменшити вихід функції.

100
00:07:19,780 --> 00:07:22,340
Іншими словами, який напрямок спуску?

101
00:07:22,340 --> 00:07:26,740
І знову ж таки, корисно подумати про м’яч, що котиться з того пагорба.

102
00:07:26,740 --> 00:07:31,920
Ті з вас, хто знайомий із численням багатьох змінних, знають,

103
00:07:31,920 --> 00:07:37,460
що градієнт функції дає вам напрямок найкрутішого підйому, у

104
00:07:37,460 --> 00:07:39,420
якому напрямку слід зробити крок, щоб збільшити функцію найшвидше.

105
00:07:39,420 --> 00:07:43,820
Цілком природно, що негативний градієнт дає вам

106
00:07:43,820 --> 00:07:47,460
напрямок кроку, який найшвидше зменшує функцію.

107
00:07:47,460 --> 00:07:52,320
Навіть більше того, довжина цього вектора градієнта є

108
00:07:52,320 --> 00:07:54,580
показником того, наскільки крутим є найкрутіший схил.

109
00:07:54,580 --> 00:07:58,080
Тепер, якщо ви не знайомі з численням багатьох змінних і хочете дізнатися більше,

110
00:07:58,080 --> 00:08:01,100
ознайомтеся з деякою роботою, яку я виконав для Академії Хана на цю тему.

111
00:08:01,100 --> 00:08:05,680
Але, чесно кажучи, для нас з вами зараз важливо лише те,

112
00:08:05,680 --> 00:08:10,440
що в принципі існує спосіб обчислити цей вектор, цей вектор, який

113
00:08:10,440 --> 00:08:12,040
повідомляє вам, яким є напрямок спуску та наскільки він крутий.

114
00:08:12,040 --> 00:08:17,280
З тобою все буде добре, якщо це все, що ти знаєш, і ти не розбираєшся в деталях.

115
00:08:17,280 --> 00:08:21,440
Тому що, якщо ви можете отримати це, алгоритм для мінімізації функції полягає в тому, щоб

116
00:08:21,440 --> 00:08:27,400
обчислити цей напрямок градієнта, потім зробити невеликий крок вниз і повторити це знову і знову.

117
00:08:28,300 --> 00:08:33,700
Це та сама основна ідея для функції, яка має 13 000 входів замість 2 входів.

118
00:08:33,700 --> 00:08:38,980
Уявіть собі організацію всіх 13 000 ваг

119
00:08:38,980 --> 00:08:40,180
і зміщень нашої мережі у гігантський вектор-стовпець.

120
00:08:40,180 --> 00:08:46,140
Від’ємний градієнт функції витрат — це просто вектор, це певний напрямок

121
00:08:46,140 --> 00:08:51,660
у цьому шалено величезному просторі введення, який говорить вам, які підштовхи

122
00:08:51,660 --> 00:08:55,900
до всіх цих чисел призведуть до найшвидшого зменшення функції витрат.

123
00:08:55,900 --> 00:09:00,000
І, звісно, завдяки нашій спеціально розробленій функції вартості зміна вагових коефіцієнтів і

124
00:09:00,000 --> 00:09:05,520
зміщень для їх зменшення означає, що вихідні дані мережі для кожної

125
00:09:05,520 --> 00:09:10,280
частини навчальних даних виглядатимуть не так як випадковий масив із 10

126
00:09:10,280 --> 00:09:11,280
значень, а більше як фактичне рішення, яке ми хочемо це зробити.

127
00:09:11,280 --> 00:09:15,940
Важливо пам’ятати, що ця функція вартості передбачає середнє значення за всіма навчальними даними, тож

128
00:09:15,940 --> 00:09:24,260
якщо ви мінімізуєте її, це означає, що для всіх цих зразків буде краща продуктивність.

129
00:09:24,260 --> 00:09:28,540
Алгоритм для ефективного обчислення цього градієнта, який фактично є основою

130
00:09:28,540 --> 00:09:32,520
того, як нейронна мережа навчається, називається зворотним поширенням, і

131
00:09:32,520 --> 00:09:34,040
саме про нього я буду говорити в наступному відео.

132
00:09:34,040 --> 00:09:39,100
Там я справді хочу витратити час, щоб пройти через те, що саме відбувається

133
00:09:39,100 --> 00:09:44,100
з кожною вагою та зміщенням для певної частини тренувальних даних, намагаючись дати

134
00:09:44,100 --> 00:09:47,980
інтуїтивне відчуття того, що відбувається за межами купи відповідних обчислень і формул.

135
00:09:47,980 --> 00:09:51,780
Прямо тут, прямо зараз, головне, що я хочу, щоб ви знали, незалежно

136
00:09:51,780 --> 00:09:56,820
від деталей реалізації, це те, що ми маємо на увазі, коли говоримо

137
00:09:56,820 --> 00:09:59,320
про мережеве навчання, це те, що це просто мінімізація функції витрат.

138
00:09:59,320 --> 00:10:02,760
І зауважте, одним із наслідків цього є те, що для

139
00:10:02,760 --> 00:10:07,820
цієї функції витрат важливо мати гарний плавний результат, щоб

140
00:10:07,820 --> 00:10:09,340
ми могли знайти локальний мінімум, роблячи невеликі кроки вниз.

141
00:10:09,340 --> 00:10:14,140
Ось чому, до речі, штучні нейрони мають безперервний

142
00:10:14,140 --> 00:10:18,580
діапазон активацій, а не просто активні чи

143
00:10:18,580 --> 00:10:20,440
неактивні у бінарному порядку, як біологічні нейрони.

144
00:10:20,440 --> 00:10:24,600
Цей процес багаторазового підштовхування вхідних даних функції

145
00:10:24,600 --> 00:10:26,960
деяким кратним від’ємним градієнтом називається градієнтним спуском.

146
00:10:26,960 --> 00:10:31,760
Це спосіб сходитися до деякого локального мінімуму функції

147
00:10:31,760 --> 00:10:33,000
вартості, по суті, долини на цьому графіку.

148
00:10:33,000 --> 00:10:37,040
Звичайно, я все ще показую зображення функції з двома входами,

149
00:10:37,040 --> 00:10:41,480
тому що підштовхування у 13 000-вимірному просторі введення трохи важко

150
00:10:41,480 --> 00:10:45,220
уявити, але насправді є гарний непросторовий спосіб подумати про це.

151
00:10:45,220 --> 00:10:49,100
Кожен компонент негативного градієнта говорить нам про дві речі.

152
00:10:49,100 --> 00:10:53,600
Знак, звичайно, говорить нам про те, чи потрібно

153
00:10:53,600 --> 00:10:55,860
підштовхнути відповідний компонент вхідного вектора вгору чи вниз.

154
00:10:55,860 --> 00:11:01,340
Але важливо те, що відносні величини всіх цих

155
00:11:01,340 --> 00:11:05,620
компонентів ніби підказують вам, які зміни важливіші.

156
00:11:05,620 --> 00:11:09,780
Розумієте, у нашій мережі коригування однієї з ваг може мати

157
00:11:09,780 --> 00:11:14,980
набагато більший вплив на функцію витрат, ніж коригування іншої ваги.

158
00:11:14,980 --> 00:11:19,440
Деякі з цих зв’язків просто важливіші для наших навчальних даних.

159
00:11:19,440 --> 00:11:23,520
Таким чином, ви можете подумати про цей вектор градієнта нашої величезної функції витрат, що

160
00:11:23,520 --> 00:11:29,740
спотворює розум, так це те, що він кодує відносну важливість кожної ваги та

161
00:11:29,740 --> 00:11:34,100
зміщення, тобто те, яка з цих змін принесе найбільшу віддачу від ваших грошей.

162
00:11:34,100 --> 00:11:37,360
Це просто інший спосіб думати про напрямок.

163
00:11:37,360 --> 00:11:41,740
Щоб взяти простіший приклад, якщо у вас є деяка функція з двома

164
00:11:41,740 --> 00:11:48,720
змінними як вхідні дані, і ви обчислюєте, що її градієнт у

165
00:11:48,720 --> 00:11:52,880
певній точці виходить як 3,1, тоді, з одного боку, ви можете інтерпретувати

166
00:11:52,880 --> 00:11:57,400
це як те, що коли ви стоячи на цьому вході, рух

167
00:11:57,400 --> 00:12:02,200
уздовж цього напрямку збільшує функцію найшвидше, тому, коли ви будуєте графік функції

168
00:12:02,200 --> 00:12:03,200
над площиною вхідних точок, цей вектор дає вам прямий напрямок угору.

169
00:12:03,200 --> 00:12:07,600
Але інший спосіб прочитати це – сказати, що зміни цієї першої змінної мають

170
00:12:07,600 --> 00:12:12,400
утричі більшу важливість, ніж зміни другої змінної, що принаймні в околицях відповідного

171
00:12:12,400 --> 00:12:17,740
вхідного значення, підштовхування значення x несе набагато більше удару для вашого бакс.

172
00:12:17,740 --> 00:12:22,880
Гаразд, давайте зменшимо масштаб і підведемо підсумок, де ми зараз.

173
00:12:22,880 --> 00:12:28,660
Сама мережа є цією функцією з 784 входами та

174
00:12:28,660 --> 00:12:30,860
10 виходами, визначеними в термінах усіх цих зважених сум.

175
00:12:30,860 --> 00:12:34,160
Функція витрат є ще одним шаром складності.

176
00:12:34,160 --> 00:12:39,300
Він приймає 13 000 ваг і упереджень як вхідні дані

177
00:12:39,300 --> 00:12:42,640
та викидає єдину міру паршивості на основі навчальних прикладів.

178
00:12:42,640 --> 00:12:47,520
Градієнт функції витрат — це ще один рівень складності.

179
00:12:47,520 --> 00:12:52,860
Він говорить нам, які підштовхи до всіх цих ваг і

180
00:12:52,860 --> 00:12:56,640
упереджень спричиняють найшвидшу зміну значення функції вартості, що можна інтерпретувати

181
00:12:56,640 --> 00:13:03,040
як те, які зміни до яких ваг мають найбільше значення.

182
00:13:03,040 --> 00:13:07,620
Отже, коли ви ініціалізуєте мережу випадковими вагами та зміщеннями та багато

183
00:13:07,620 --> 00:13:12,420
разів налаштовуєте їх на основі цього процесу градієнтного спуску, наскільки

184
00:13:12,420 --> 00:13:14,240
добре вона справді працює на зображеннях, яких раніше не бачив?

185
00:13:14,240 --> 00:13:19,000
Той, який я описав тут, із двома прихованими шарами по 16 нейронів кожен, обраними здебільшого

186
00:13:19,000 --> 00:13:26,920
з естетичних міркувань, непоганий, оскільки він класифікує приблизно 96% нових зображень, які він бачить правильно.

187
00:13:26,920 --> 00:13:31,580
І, чесно кажучи, якщо ви подивіться на деякі приклади,

188
00:13:31,580 --> 00:13:36,300
з якими він зіпсувався, ви відчуєте потребу трохи послабити.

189
00:13:36,300 --> 00:13:40,220
Якщо ви пограєте зі структурою прихованого шару та зробите

190
00:13:40,220 --> 00:13:41,220
кілька налаштувань, ви можете отримати це до 98%.

191
00:13:41,220 --> 00:13:42,900
І це дуже добре!

192
00:13:42,900 --> 00:13:47,020
Це не найкраще, ви, звичайно, можете отримати кращу продуктивність, ставши більш складною, ніж ця

193
00:13:47,020 --> 00:13:52,460
звичайна ванільна мережа, але враховуючи, наскільки складним є початкове завдання, я вважаю, що є

194
00:13:52,460 --> 00:13:56,800
щось неймовірне в тому, щоб будь-яка мережа так добре справлялася із зображеннями, яких вона

195
00:13:56,800 --> 00:14:02,000
ніколи раніше не бачила, враховуючи, що ми ніколи конкретно не говорив, які шаблони шукати.

196
00:14:02,000 --> 00:14:07,840
Спочатку я мотивував цю структуру, описуючи надію, яку ми могли б

197
00:14:07,840 --> 00:14:11,880
мати, що другий шар може підхопити маленькі краї, що третій

198
00:14:11,880 --> 00:14:16,080
шар з’єднає ці краї разом, щоб розпізнати петлі та довші лінії,

199
00:14:16,080 --> 00:14:18,220
і що вони можуть бути складені разом, щоб розпізнавати цифри.

200
00:14:18,220 --> 00:14:21,040
Отже, це те, що насправді робить наша мережа?

201
00:14:21,040 --> 00:14:24,880
Ну, принаймні для цього, зовсім ні.

202
00:14:24,960 --> 00:14:29,120
Пам’ятаєте, як у минулому відео ми дивилися на те, як ваги

203
00:14:29,120 --> 00:14:33,900
зв’язків від усіх нейронів першого шару до даного нейрона другого шару

204
00:14:33,900 --> 00:14:37,440
можна візуалізувати як даний піксельний шаблон, який нейрон другого шару вловлює?

205
00:14:37,440 --> 00:14:44,600
Що ж, коли ми робимо це для ваг, пов’язаних із цими переходами,

206
00:14:44,600 --> 00:14:51,000
замість того, щоб шукати окремі маленькі краї тут і там, вони

207
00:14:51,000 --> 00:14:54,200
виглядають, ну, майже випадковими, просто з деякими дуже вільними візерунками посередині.

208
00:14:54,200 --> 00:14:59,020
Здавалося б, що в незбагненно великому 13 000-вимірному просторі

209
00:14:59,020 --> 00:15:04,020
можливих ваг і упереджень наша мережа знайшла щасливий маленький

210
00:15:04,020 --> 00:15:08,440
локальний мінімум, який, незважаючи на успішну класифікацію більшості зображень,

211
00:15:08,440 --> 00:15:09,840
не точно вловлює шаблони, на які ми могли сподіватися.

212
00:15:09,840 --> 00:15:14,600
Щоб переконатися в цьому, подивіться, що відбувається, коли ви вводите випадкове зображення.

213
00:15:14,600 --> 00:15:19,240
Якби система була розумною, ви могли б очікувати, що вона буде відчувати себе невизначеною, можливо,

214
00:15:19,240 --> 00:15:24,120
насправді не активуючи жоден із цих 10 вихідних нейронів або активуючи їх усі рівномірно,

215
00:15:24,520 --> 00:15:29,800
але натомість вона впевнено дає вам якусь безглузду відповідь, ніби вона відчуває себе впевненою, що

216
00:15:29,800 --> 00:15:34,560
цей випадковий Шум – це 5, тому що фактичне зображення 5 – це 5.

217
00:15:34,560 --> 00:15:39,300
Іншими словами, навіть якщо ця мережа досить добре

218
00:15:39,300 --> 00:15:41,800
розпізнає цифри, вона не знає, як їх намалювати.

219
00:15:41,800 --> 00:15:45,400
Багато в чому це тому, що це жорстко обмежена система навчання.

220
00:15:45,400 --> 00:15:48,220
Я маю на увазі, поставте себе на місце мережі.

221
00:15:48,220 --> 00:15:53,280
З його точки зору, весь Всесвіт складається лише з чітко визначених нерухомих

222
00:15:53,280 --> 00:15:58,560
цифр, зосереджених у крихітній сітці, і його функція вартості ніколи не давала

223
00:15:58,560 --> 00:16:02,160
жодних стимулів бути чимось іншим, крім як абсолютно впевненим у своїх рішеннях.

224
00:16:02,160 --> 00:16:05,760
Отже, маючи це як образ того, що насправді роблять

225
00:16:05,760 --> 00:16:09,320
нейрони другого шару, ви можете задатися питанням, чому я

226
00:16:09,320 --> 00:16:10,320
представив цю мережу з мотивацією підхоплення країв і шаблонів.

227
00:16:10,320 --> 00:16:13,040
Я маю на увазі, що це зовсім не те, що це закінчується.

228
00:16:13,040 --> 00:16:17,480
Що ж, це не наша кінцева мета, а натомість відправна точка.

229
00:16:17,480 --> 00:16:22,280
Відверто кажучи, це стара технологія, яку досліджували у 80-х і 90-х роках, і

230
00:16:22,280 --> 00:16:26,920
вам потрібно її зрозуміти, перш ніж ви зможете зрозуміти більш детальні сучасні варіанти,

231
00:16:26,920 --> 00:16:31,380
і вона явно здатна вирішити деякі цікаві проблеми, але чим більше ви заглиблюєтесь

232
00:16:31,380 --> 00:16:38,720
у те, що ті приховані шари дійсно роблять, тим менш розумним це здається.

233
00:16:38,720 --> 00:16:43,540
Якщо на мить перенести фокус із того, як мережі навчаються, на те,

234
00:16:43,540 --> 00:16:47,160
як навчаєтеся ви, це станеться лише за умови активного вивчення матеріалу тут.

235
00:16:47,160 --> 00:16:51,920
Я хочу, щоб ви зробили одну досить просту річ: просто зупиніться зараз і на мить

236
00:16:51,920 --> 00:16:57,560
глибоко подумайте про те, які зміни ви можете внести в цю систему та як вона

237
00:16:57,560 --> 00:17:01,880
сприймає зображення, якщо ви хочете, щоб вона краще вловлювала такі речі, як краї та візерунки.

238
00:17:01,880 --> 00:17:06,360
Але краще за все, щоб справді ознайомитися з матеріалом, я настійно

239
00:17:06,360 --> 00:17:09,720
рекомендую книгу Майкла Нільсена про глибоке навчання та нейронні мережі.

240
00:17:09,720 --> 00:17:15,200
У ньому ви можете знайти код і дані для завантаження та використання для цього

241
00:17:15,200 --> 00:17:19,360
точного прикладу, і книга проведе вас крок за кроком, що цей код робить.

242
00:17:19,360 --> 00:17:23,920
Що чудово, так це те, що ця книга є безкоштовною та загальнодоступною, тому, якщо ви щось

243
00:17:23,920 --> 00:17:28,040
отримаєте від неї, подумайте про те, щоб приєднатися до мене та зробити пожертву на користь Nielsen.

244
00:17:28,040 --> 00:17:32,060
Я також пов’язав кілька інших ресурсів, які мені дуже подобаються, в описі, включно

245
00:17:32,060 --> 00:17:38,720
з феноменальним і красивим дописом у блозі Кріса Оли та статті в Distill.

246
00:17:38,720 --> 00:17:41,960
Щоб завершити це на останні кілька хвилин, я хочу повернутися

247
00:17:41,960 --> 00:17:44,440
до фрагменту інтерв’ю, яке я мав із Лейшею Лі.

248
00:17:44,440 --> 00:17:48,520
Можливо, ви пам’ятаєте її з останнього відео, вона захистила докторську роботу з глибокого навчання.

249
00:17:48,560 --> 00:17:52,240
У цьому невеличкому фрагменті вона розповідає про дві нещодавні статті, які дійсно

250
00:17:52,240 --> 00:17:56,380
досліджують, як деякі з більш сучасних мереж розпізнавання зображень насправді навчаються.

251
00:17:56,380 --> 00:18:00,320
Просто щоб визначити, де ми знаходимося в розмові, перша стаття взяла одну з цих

252
00:18:00,320 --> 00:18:04,480
особливо глибоких нейронних мереж, яка справді добре розпізнає зображення, і замість того, щоб

253
00:18:04,480 --> 00:18:09,400
навчати її на правильно позначеному наборі даних, вона перетасувала всі мітки перед навчанням.

254
00:18:09,400 --> 00:18:13,840
Очевидно, що точність тестування тут мала бути не

255
00:18:13,840 --> 00:18:15,320
кращою, ніж випадкова, оскільки все просто випадково позначено.

256
00:18:15,320 --> 00:18:20,080
Але він все одно зміг досягти такої ж точності

257
00:18:20,080 --> 00:18:21,440
навчання, як і на правильно позначеному наборі даних.

258
00:18:21,440 --> 00:18:26,120
По суті, мільйонів ваг для цієї конкретної мережі було достатньо, щоб вона

259
00:18:26,120 --> 00:18:31,040
просто запам’ятовувала випадкові дані, що піднімає питання, чи насправді мінімізація цієї

260
00:18:31,040 --> 00:18:36,720
функції вартості відповідає будь-якій структурі в зображенні, чи це просто запам’ятовування?

261
00:18:36,720 --> 00:18:40,120
. . . щоб запам’ятати весь набір даних, що таке правильна класифікація.

262
00:18:40,120 --> 00:18:45,720
І ось кілька, ви знаєте, півроку пізніше на ICML цього року, була

263
00:18:45,720 --> 00:18:50,440
не зовсім спростувальна стаття, а стаття, в якій розглядалися деякі аспекти,

264
00:18:50,440 --> 00:18:52,220
наприклад, ага, насправді ці мережі роблять щось трохи розумніше, ніж це.

265
00:18:52,220 --> 00:18:59,600
Якщо ви подивитеся на цю криву точності, якби ви просто тренувалися на випадковому

266
00:18:59,600 --> 00:19:05,240
наборі даних, ця крива начебто опускалася дуже, знаєте, дуже повільно майже лінійним способом.

267
00:19:05,280 --> 00:19:10,840
Тож вам справді важко знайти локальні мінімуми можливих,

268
00:19:10,840 --> 00:19:12,320
знаєте, правильних ваг, які дадуть вам таку точність.

269
00:19:12,320 --> 00:19:16,720
У той час як якщо ви насправді тренуєтеся на структурованому наборі даних,

270
00:19:16,720 --> 00:19:20,240
який має правильні мітки, ви знаєте, ви знаєте, ви трохи возитеся на

271
00:19:20,240 --> 00:19:23,360
початку, але потім ви дуже швидко впали, щоб досягти такого рівня точності.

272
00:19:23,360 --> 00:19:28,580
І тому в якомусь сенсі було легше знайти ці локальні максимуми.

273
00:19:28,580 --> 00:19:32,900
І що також було цікаво в цьому, це

274
00:19:32,900 --> 00:19:39,140
висвітлює іншу статтю, фактично пару років тому, яка

275
00:19:39,140 --> 00:19:40,140
містить набагато більше спрощень щодо мережевих рівнів.

276
00:19:40,140 --> 00:19:43,880
Але один із результатів показав, що, якщо ви подивитеся на ландшафт оптимізації,

277
00:19:43,880 --> 00:19:49,400
локальні мінімуми, які ці мережі, як правило, вивчають, насправді мають однакову якість.

278
00:19:49,400 --> 00:19:54,300
Тож у певному сенсі, якщо ваш набір даних структурований, ви зможете знайти це набагато легше.

279
00:19:58,580 --> 00:20:01,140
Я, як завжди, дякую тим із вас, хто підтримує на Patreon.

280
00:20:01,480 --> 00:20:05,440
Раніше я вже говорив про те, що змінює гру на

281
00:20:05,440 --> 00:20:07,160
Patreon, але ці відео справді були б неможливими без вас.

282
00:20:07,160 --> 00:20:11,540
Я також хочу висловити особливу подяку фірмі венчурного капіталу Amplify

283
00:20:11,540 --> 00:20:13,240
Partners та її підтримці цих перших відео в серії.

284
00:20:31,140 --> 00:20:33,140
Дякую тобі.

