1
00:00:04,070 --> 00:00:07,059
En último vídeo presenté la estructura de una red neuronal

2
00:00:07,160 --> 00:00:10,089
Voy a dar un resumen rápido aquí ,sólo para que este fresca en nuestra mente

3
00:00:10,089 --> 00:00:15,368
Y luego tengo dos objetivos principales para este vídeo. La primera consiste en introducir la idea de "descenso de gradiente",

4
00:00:15,650 --> 00:00:18,219
que es la base no sólo  de cómo las redes neuronales aprenden,

5
00:00:18,220 --> 00:00:20,439
pero  también cómo otro monto de  máquinas de aprendizaje  funciona  .

6
00:00:20,660 --> 00:00:24,609
Luego de eso,  vamos a cavar un poco más en cómo funciona esta red en particular

7
00:00:24,609 --> 00:00:27,758
Y lo que esas capas ocultas de  neuronas terminan buscando en realidad.

8
00:00:28,999 --> 00:00:33,489
Como recordatorio nuestro objetivo aquí es el clásico ejemplo de reconocimiento de escritura dígitos a mano.

9
00:00:34,129 --> 00:00:36,129
el "Hola Mundo" de las redes neuronales

10
00:00:36,500 --> 00:00:43,090
estos dígitos se representan en una cuadrícula de 28 por 28 píxeles , cada píxel con un valor de escala de grises entre 0 y 1

11
00:00:43,600 --> 00:00:46,080
esos son los que determinan las activaciones de

12
00:00:46,260 --> 00:00:50,200
784 neuronas en la capa de entrada de la red y

13
00:00:50,840 --> 00:00:55,719
Entonces la activación para cada neurona en las siguientes capas se basa en una suma ponderada de

14
00:00:56,000 --> 00:01:00,639
Todas las activaciones en la capa anterior más un número especial llamado "Bias"

15
00:01:01,699 --> 00:01:06,338
luego,  al componer esa suma con alguna otra función como, la sigmoide  "exprimidora"

16
00:01:06,400 --> 00:01:08,769
o una RELU ,la forma que recorrí a través  del último vídeo.

17
00:01:09,100 --> 00:01:15,720
En total ,dada una  elección arbitraria de algo  de las  dos capas ocultas ,cada una  con 16 neuronas. La red tiene alrededor de

18
00:01:16,240 --> 00:01:24,160
13.000 pesos y BIASES que podemos ajustar y son estos valores los que determinan qué  exactamente hace la red .

19
00:01:24,799 --> 00:01:28,328
Entonces lo que queremos decir cuando decimos que esta red clasifica a un dígito dado

20
00:01:28,329 --> 00:01:33,429
Es que las 10 neuronas  más brillantes  en la capa final corresponden a ese dígito

21
00:01:33,950 --> 00:01:38,589
Y recuerda que la motivación que teníamos en mente aquí para obtener la estructura en capas era que tal vez

22
00:01:38,780 --> 00:01:44,680
La segunda capa podría elegir los  bordes y la tercera capa podría escoger los  patrones como bucles y líneas

23
00:01:44,930 --> 00:01:48,729
Y el último sólo pudiera juntar esos patrones para reconocer dígitos

24
00:01:49,369 --> 00:01:52,029
Así que aquí aprendemos cómo aprende la red

25
00:01:52,399 --> 00:01:57,099
Lo que queremos es un algoritmo donde tú puedas mostrarle a esta red un montón de datos para entrenarla.

26
00:01:57,229 --> 00:02:03,669
que vienen en la forma de un montón de diferentes imágenes de dígitos escritos a mano, junto con las etiquetas de lo que se supone que son, y la red los

27
00:02:03,890 --> 00:02:05,659
ajustará.

28
00:02:05,659 --> 00:02:09,789
13000 pesos y BIASES a fin de mejorar su desempeño en los datos de entrenamiento

29
00:02:10,730 --> 00:02:13,569
Esperemos que esta estructura de capas diga   lo que la red aprende.

30
00:02:14,269 --> 00:02:16,719
generalizar imágenes más allá de  lo que  los datos de entrenamiento

31
00:02:16,720 --> 00:02:20,289
Y la forma en que  se prueba  es que después de entrenar a la red.

32
00:02:20,290 --> 00:02:26,560
es que Tú le muestras mas datos etiquetados que nunca han sido vistos antes, y luego mira como clasifica esas imágenes precisamente.

33
00:02:31,040 --> 00:02:37,000
Afortunadamente para nosotros, y lo que hace de este un ejemplo tan común para empezar es que las buenas personas detrás de la base de MNIST han

34
00:02:37,000 --> 00:02:44,289
reunido una colección de decenas de miles de imágenes de dígitos escritos a mano ,cada una marcada con los números que se supone que deben ser y

35
00:02:44,720 --> 00:02:49,539
Es provocador como para describir una máquina como aprendizaje ,  una vez que realmente veas  cómo funciona

36
00:02:49,540 --> 00:02:55,359
,Se siente mucho menos como una loca premisa de ciencia ficción y mucho más como un ejercicio de cálculo.

37
00:02:55,390 --> 00:02:59,589
Lo que digo es que básicamente, se reduce a encontrar el mínimo de una función determinada.

38
00:03:01,519 --> 00:03:05,199
Recuerda,  conceptualmente estamos pensando en cada neurona siendo conectada

39
00:03:05,390 --> 00:03:12,309
a todas las neuronas de la capa anterior, y los pesos de la suma ponderada que definen su activación son algo así como la

40
00:03:12,440 --> 00:03:14,060
fortalezas de esas conexiones.

41
00:03:14,060 --> 00:03:20,440
Y el BIAS es alguna indicación de si esa neurona tiende a ser activa o inactiva y para empezar las cosas

42
00:03:20,440 --> 00:03:26,919
Sólo vamos a inicializar todos esos pesos y sesgos totalmente al azar.Es innecesario  decir que esta red se va a comportar

43
00:03:26,919 --> 00:03:33,759
bastante horrible en un ejemplo de entrenamiento dado, ya que sólo está haciendo algo al azar, por ejemplo,  se le da   esta imagen de un 3 y  la

44
00:03:33,760 --> 00:03:35,799
capa de salida sólo loe  ve como un desastre.

45
00:03:36,349 --> 00:03:42,518
Así que lo que hace es definir una función de "costes", una manera de decirle a la computadora: "No !!, computadora mala!

46
00:03:42,739 --> 00:03:50,529
"Esa  salida  debería  tener activaciones que son cero para la mayoría de las neuronas, pero uno para esta neurona, lo  que me diste es pura basura""

47
00:03:51,260 --> 00:03:56,530
Para decir   más matemáticamente lo que se hace es sumar los cuadrados de las diferencias entre

48
00:03:56,720 --> 00:04:01,419
cada una de esas activaciones de salida de basura ,y el valor que queremos que tengan y

49
00:04:01,489 --> 00:04:04,599
Esto es lo que llamaremos el costo de un solo ejemplo de formación

50
00:04:05,599 --> 00:04:10,749
Nota que  esta suma es pequeña cuando la red clasifica con confianza la imagen correctamente

51
00:04:12,199 --> 00:04:15,639
Pero es grande cuando la red parece que no sabe realmente lo que está haciendo

52
00:04:18,330 --> 00:04:25,249
De manera que lo que se hace es considerar el costo promedio de todas las decenas de miles de ejemplos de entrenamiento a su disposición

53
00:04:27,060 --> 00:04:34,310
Este costo promedio es nuestra medida de lo mal que la red es y lo mal que la computadora debe sentirse, y eso es una cosa complicada.

54
00:04:34,830 --> 00:04:38,960
Recuerda cómo la red en sí era, básicamente una función que toma

55
00:04:39,540 --> 00:04:45,890
784 números como entradas, los valores de los píxeles, y escupe diez números como su salida y en un sentido

56
00:04:45,890 --> 00:04:48,770
Se parametriza por todos estos pesos y BIASES

57
00:04:49,140 --> 00:04:54,020
Mientras que la función de coste es una capa de complejidad en la parte superior de eso,  que toma como su entrada

58
00:04:54,450 --> 00:05:02,059
esos trece mil o más pesos y BIASES   escupe un solo número describiendo lo mal que esos pesos y BIASES son, y

59
00:05:02,340 --> 00:05:08,749
la forma en que se define depende del comportamiento de la red a través de todas las decenas de miles de piezas de datos de entrenamiento

60
00:05:09,150 --> 00:05:11,150
Eso es mucho en que  pensar!!

61
00:05:12,000 --> 00:05:15,619
Pero con simplemente decirle a la computadora qué es un trabajo, que lo que está haciendo no es muy útil,

62
00:05:15,900 --> 00:05:19,819
¿Quieres decirle cómo cambiar esos pesos y BIASES de manera que se  mejore?

63
00:05:20,820 --> 00:05:25,129
Para hacerlo más fácil ,en lugar de luchar imaginando  una función con 13.000 entradas

64
00:05:25,130 --> 00:05:30,409
solo Imagínate  una función simple que tiene un número como una entrada y un número como una salida

65
00:05:30,960 --> 00:05:34,999
¿Cómo encontrar una entrada que minimiza el valor de esta función?

66
00:05:36,270 --> 00:05:40,039
Los estudiantes de cálculo sabrán que a veces se puede averiguar ese mínimo de forma explícita

67
00:05:40,260 --> 00:05:43,879
Pero eso no siempre es factible para funciones muy complicadas

68
00:05:44,310 --> 00:05:52,160
Ciertamente no en la trece mil versión de entrada de esta situación para nuestra función de costos de redes neuronales complicada

69
00:05:52,350 --> 00:05:59,029
Una táctica más flexible es comenzar en cualquier  de entrada  vieja y averiguar en qué dirección  se debe ir  para hacer el output  más bajo.

70
00:06:00,120 --> 00:06:03,710
En concreto, si se puede averiguar la pendiente de la función  dónde  te encuentras.

71
00:06:04,020 --> 00:06:09,619
Entonces  se desplaza hacia la izquierda si esa pendiente es positiva y cambia la entrada hacia la derecha si esa pendiente es negativa

72
00:06:12,130 --> 00:06:16,799
Si haces esto repetidamente , en cada punto de revisando la nueva pendiente y tomando medida apropiada

73
00:06:16,800 --> 00:06:20,039
vas a aproximar algún mínimo local de la función y

74
00:06:20,280 --> 00:06:24,080
la imagen que podrías tener en cuenta aquí es una bola rodando por una colina y

75
00:06:24,400 --> 00:06:30,900
Observa que  incluso para esta función de entrada realmente simplificada  hay muchos valles posibles  que podrías recorrer.

76
00:06:31,300 --> 00:06:36,380
Dependiendo de la entrada aleatoria que en la que inicias   y no hay garantía de que el mínimo local que estas recorriendo

77
00:06:36,580 --> 00:06:39,040
va a ser el menor valor posible de la función de coste

78
00:06:39,610 --> 00:06:44,009
Eso va a llevar a nuestro caso de redes neuronales, así, y también quiero que se den cuenta

79
00:06:44,010 --> 00:06:47,190
Cómo ,si haces el tamaño de los pasos proporcional a la pendiente.

80
00:06:47,620 --> 00:06:54,540
Luego, cuando la pendiente se aplana hacia el mínimo , Tus pasos se hacen más pequeños y más pequeños y ese tipo de ayuda que te rebalsa.

81
00:06:55,720 --> 00:07:00,449
Topandonos  un poco con la complejidad ,imagina  en lugar de una función con dos entradas y una salida

82
00:07:01,120 --> 00:07:07,739
Se puede pensar en el espacio de entrada como el plano XY y la función de costo como la graficada de una superficie por encima de ella

83
00:07:08,230 --> 00:07:15,060
Ahora, en lugar de preguntar por la pendiente de la función , tienes que preguntar en qué dirección debes tomar  en este espacio de entrada?

84
00:07:15,310 --> 00:07:22,440
Con el fin de disminuir la salida de la función más rápidamente en otras palabras. ¿Cuál es la dirección de descenso?

85
00:07:22,440 --> 00:07:25,379
Y de nuevo, es útil pensar en una bola rodando por la colina

86
00:07:26,260 --> 00:07:34,080
Aquellos que estén familiarizados con el cálculo multivariable sabrán que el gradiente de una función te da la dirección de ascenso más rápido

87
00:07:34,750 --> 00:07:38,459
Básicamente, en qué dirección debe ir para aumentar la función más rápidamente.

88
00:07:39,100 --> 00:07:46,439
como es natural tomando el negativo de ese gradiente le da la dirección al paso que disminuye la función más rápidamente y

89
00:07:47,020 --> 00:07:53,400
Incluso más de eso,  que la longitud de este vector gradiente es en realidad una indicación de cuán empinada la pendiente más pronunciada  es

90
00:07:54,130 --> 00:07:56,280
Ahora bien, si no estás familiarizado con el cálculo multivariable

91
00:07:56,280 --> 00:08:00,239
Y deseas aprender  más , echa un vistazo a algunos de los trabajos que hice para la KhanAcademy sobre el tema

92
00:08:00,910 --> 00:08:03,779
Honestamente, sin embargo todo lo que importa para ti y para mí en este momento

93
00:08:03,780 --> 00:08:09,419
Es que, en principio, existe una forma de calcular este vector. Este vector que lo que te  dice

94
00:08:09,520 --> 00:08:15,900
es la dirección de descenso y lo  empinada que es , estarás  bien si eso es todo lo que sabes y no eres  como una roca sólida en los detalles.

95
00:08:16,790 --> 00:08:24,580
porque si tu  puedes conseguir eso, el algoritmo de minimización de la función es calcular esta  gradiente de dirección, luego tomar un pequeño paso de descenso y

96
00:08:24,740 --> 00:08:26,740
sólo tiene que repetirlo una y otra vez.

97
00:08:27,800 --> 00:08:34,600
Es la misma idea básica de una función que tiene 13.000 entradas en lugar de dos entradas ,imagínate   organizando  todos los

98
00:08:35,330 --> 00:08:39,400
13.000 pesos y BIASES de nuestra red en un vector-columna gigante

99
00:08:39,680 --> 00:08:43,870
El gradiente negativo de la función de coste es sólo un vector

100
00:08:43,880 --> 00:08:49,299
Es cierta dirección  dentro de este increíblemente enorme espacio de entrada que te dice qué

101
00:08:49,400 --> 00:08:55,030
empujones  a todos esos números va a provocar la disminución más rápida de la función de costos y

102
00:08:55,460 --> 00:08:58,150
por supuesto con nuestra función de coste especialmente diseñada

103
00:08:58,580 --> 00:09:04,900
cambiando  los pesos y BIASES para disminuir, significa hacer la salida de la red en cada pieza de datos de entrenamiento

104
00:09:05,180 --> 00:09:10,599
Parecerse menos a una matriz aleatoria de diez valores y más como una decisión real que queremos que se haga

105
00:09:11,030 --> 00:09:16,030
Es importante recordar esta función de costos implica un promedio sobre todos los datos de entrenamiento

106
00:09:16,370 --> 00:09:20,590
Así que si la minimizas , significa que es un mejor rendimiento en todas esas muestras

107
00:09:23,780 --> 00:09:30,849
El algoritmo para el cálculo de este gradiente de manera eficiente ,que es efectivamente el corazón de cómo aprende una red neuronal, se propagación hacia atras.

108
00:09:31,190 --> 00:09:34,690
Y es lo que voy a estar hablando en el  siguiente video.

109
00:09:34,690 --> 00:09:36,690
Allí  realmente quiero tomar el tiempo para caminar a través de

110
00:09:36,830 --> 00:09:41,439
¿Qué ocurre exactamente a cada peso y cada de las BIAS para una determinada pieza de datos de entrenamiento?

111
00:09:41,810 --> 00:09:46,960
Tratando de dar una sensación intuitiva de lo que está sucediendo más allá de la pila de cálculos  y fórmulas relevantes

112
00:09:47,510 --> 00:09:52,179
Aquí mismo, en este momento lo más importante.  es que quiero que sepas , independiente de los detalles de implementación,

113
00:09:52,180 --> 00:09:58,479
es que lo que queremos decir cuando hablamos de una red de aprendizaje es que es sólo esta minimizando una función de coste y

114
00:09:58,940 --> 00:10:04,479
nota una consecuencia de esto: es que es importante para esta función de costos  tener una buena salida suave

115
00:10:04,480 --> 00:10:08,200
De modo que podamos encontrar un mínimo local, tomando pequeños pasos de descenso

116
00:10:08,800 --> 00:10:10,520
Esta es la razón , ya que

117
00:10:10,520 --> 00:10:16,749
Las  neuronas artificiales se van continuamente activaciones en lugar de simplemente ser activada o inactivada  de forma binaria

118
00:10:16,750 --> 00:10:18,750
si es la  forma en que las neuronas biológicas son.

119
00:10:19,940 --> 00:10:26,770
Este proceso de empujar 
 repetidamente una entrada de una función por algún múltiplo del gradiente negativo se denomina "descenso de gradiente"

120
00:10:26,930 --> 00:10:32,380
Es una manera de converger hacia algún mínimo local de una función de coste ,básicamente un valle en este gráfico

121
00:10:32,930 --> 00:10:38,890
Todavía estoy mostrando la imagen de una función con dos entradas, por supuesto, debido a que los empujones  en una entrada de  trece mil dimensiones

122
00:10:38,890 --> 00:10:44,049
en el espacio es un poco difícil de envolver  alrededor de tu mente, pero no es en realidad una buena manera de no-espacial para pensar en esto

123
00:10:44,630 --> 00:10:51,340
Cada componente del gradiente negativo nos dice dos cosas: el signo por supuesto nos dice si el correspondiente

124
00:10:51,830 --> 00:10:59,139
Componente del vector de entrada debe ser empujado  hacia arriba o hacia abajo, pero importantemente , las magnitudes relativas de todos estos componentes

125
00:10:59,840 --> 00:11:02,530
es un Tipo de indicio que te dice que cambios importan más.

126
00:11:05,150 --> 00:11:09,340
Vemos  en nuestra red que  un ajuste a uno de los pesos podría tener un mayor

127
00:11:09,710 --> 00:11:12,939
impacto en la función de coste que el ajuste a algún otro peso

128
00:11:14,450 --> 00:11:17,950
Algunas de estas conexiones importan  más para nuestros datos de entrenamiento

129
00:11:18,920 --> 00:11:22,690
Así que una manera que se puede pensar en este vector gradiente de nuestra

130
00:11:22,690 --> 00:11:27,999
función masiva costo es que codifica la importancia relativa de cada peso y BIAS

131
00:11:28,250 --> 00:11:32,200
Es decir ,cuál de estos cambios va a sacar el máximo partido de tu inversión.

132
00:11:33,560 --> 00:11:36,460
Esto realmente es sólo otra manera de pensar acerca de la dirección

133
00:11:36,860 --> 00:11:41,290
Para tomar un ejemplo más simple, si tiene alguna función de dos variables como una entrada y se

134
00:11:41,690 --> 00:11:46,540
Calcula que su gradiente en algún punto en particular sale como (3,1)

135
00:11:47,420 --> 00:11:51,670
Entonces, por un lado se puede interpretar  como : cuando se está  en esa entrada

136
00:11:52,070 --> 00:11:55,150
moviéndose a lo  largo de esta dirección aumenta la función más rápidamente

137
00:11:55,460 --> 00:12:02,229
,Que cuando se hace un gráfico de la función por encima del plano de entrada, ese  vector  señala la dirección  en subida.

138
00:12:02,600 --> 00:12:06,580
Sin embargo, otra manera de leerlo, es decir que los cambios en esta primera variable

139
00:12:06,740 --> 00:12:13,390
Es Tener tres veces la importancia como cambio a la segunda variable que, al menos en las proximidades de la entrada correspondiente

140
00:12:13,520 --> 00:12:16,689
Empujando el valor de x lleva saca más partido de tu inversión.

141
00:12:19,310 --> 00:12:19,930
Todo bien

142
00:12:19,930 --> 00:12:24,940
Vamos a alejarnos y resumir hasta donde estamos,   la red en sí en esta función con

143
00:12:25,400 --> 00:12:29,859
784 entradas y 10 salidas, Esta definida en términos de todas estas sumas ponderadas.

144
00:12:30,350 --> 00:12:34,780
la función de costos es una capa de complejidad en la parte superior  que toma los

145
00:12:35,120 --> 00:12:41,870
13.000 pesos y biases  como entradas y escupe una sola medida de pérdida  basada en los ejemplos de entrenamiento y

146
00:12:42,180 --> 00:12:47,930
El gradiente de la función de costes es una capa más de complejidad todavía, esta  nos dice

147
00:12:47,930 --> 00:12:53,839
Lo que empuja a todos estos pesos y biases  que causan el cambio más rápido en el valor de la función de coste

148
00:12:53,970 --> 00:12:57,680
Que podrías interpretar  que te está diciendo  que  cambios de que pesos importan más.

149
00:13:02,550 --> 00:13:09,289
Así que al inicializar la red con pesos y biases  aleatorios y los ajustas muchas veces en base a este proceso de descenso de gradiente

150
00:13:09,420 --> 00:13:12,949
¿Realmente , qué tan bien se desenvuelve con las imágenes que nunca se han visto antes?

151
00:13:13,680 --> 00:13:19,609
Pues la que he descrito aquí con las dos capas ocultas de dieciséis neuronas cada una ,elegida sobre todo por razones estéticas

152
00:13:20,579 --> 00:13:26,089
bueno, no es mala,  esta clasifica el 96 por ciento de las nuevas imágenes que ve correctamente y

153
00:13:26,759 --> 00:13:32,239
Honestamente, si nos fijamos en algunos de los ejemplos que se estropea ,te sientes obligado a cortar un poco de holgura

154
00:13:35,759 --> 00:13:39,079
Ahora bien, si  juegas un poco con la estructura de capas ocultas y haces un par de ajustes

155
00:13:39,079 --> 00:13:43,698
Tu puede llevarla  hasta el 98% y eso es bastante bueno. No es el mejor

156
00:13:43,740 --> 00:13:48,409
Por supuesto que puede obtener un mejor rendimiento al conseguir una más sofisticado que esta Red " sabor de vainilla"

157
00:13:48,569 --> 00:13:52,669
Pero teniendo en cuenta lo desalentadora que es la tarea inicial , yo simplemente creo que hay algo?

158
00:13:52,889 --> 00:13:56,929
Increíble sobre cualquier red haciendo esto bien con las imágenes  que nunca se han visto antes.

159
00:13:57,389 --> 00:14:00,919
Dado que nunca específicamente esta indicado qué patrones buscar

160
00:14:02,579 --> 00:14:07,068
Originalmente, la forma en que me motivó  esta estructura fue por describir una esperanza de que podríamos tener

161
00:14:07,259 --> 00:14:09,739
Que la segunda capa podría recoger  pequeños bordes,

162
00:14:09,809 --> 00:14:17,089
que la tercera capa podría reconstruir los bordes para reconocer bucles y líneas más largas y que podrían  ser reconstruido juntos para  reconocer dígitos.

163
00:14:17,699 --> 00:14:22,729
Así que es esto lo que nuestra red está haciendo en realidad? Bueno para ésta, al menos,

164
00:14:23,339 --> 00:14:24,449
De ningún modo

165
00:14:24,449 --> 00:14:27,409
Recordemos cómo el vídeo pasada vimos cómo el peso de la

166
00:14:27,480 --> 00:14:31,849
Las conexiones de todas las neuronas en la primera capa a una neurona dada en la segunda capa

167
00:14:31,980 --> 00:14:36,829
Se puede visualizar como un patrón de píxeles ya que esa neurona de la segunda capa  lo está recogiendo.

168
00:14:37,350 --> 00:14:43,309
Bien cuando en realidad lo hacemos para los pesos asociados a estas transiciones desde la primera capa a la siguiente

169
00:14:43,709 --> 00:14:50,209
En lugar de recoger pequeños bordes aislados aquí y allá. Se ven bien casi al azar

170
00:14:50,370 --> 00:14:56,399
Sólo hay que poner algunos patrones muy flojos en el medio , ahí parecerían que en el insondable espacio de

171
00:14:56,920 --> 00:15:02,580
13.000 dimensiones  de posibles pesos y BIASES  nuestra red encontró  
 en si misma pequeño y feliz mínimo local que

172
00:15:02,860 --> 00:15:08,940
a pesar de la clasificación con éxito, la mayoría de las imágenes no se toman  exactamente arriba en los patrones que podríamos haber esperado y

173
00:15:09,430 --> 00:15:13,709
Para conducir realmente este punto a casa, mira  lo que sucede cuando se introduce una imagen aleatoria

174
00:15:14,019 --> 00:15:21,449
si el sistema era inteligente, tu  podría esperar que se sienta inseguro, tal vez no es realmente la activación de cualquiera de esas 10 neuronas de salida o

175
00:15:21,579 --> 00:15:23,200
La activación de todas ellas uniformemente

176
00:15:23,200 --> 00:15:24,820
Pero en cambio,

177
00:15:24,820 --> 00:15:32,010
Con confianza te da alguna respuesta sin sentido como si se sintiese  tan seguro de que este ruido aleatorio es un 5, y como si lo hiciese con una.

178
00:15:32,010 --> 00:15:34,010
imagen  real de un 5.

179
00:15:34,180 --> 00:15:40,829
frase diferente, incluso si esta red puede reconocer dígitos bastante bien que no tiene idea de la forma de dibujar una

180
00:15:41,500 --> 00:15:45,149
Mucho de esto se debe a que se trata de un sistema de formación tales fuertes limitaciones

181
00:15:45,149 --> 00:15:51,479
Me refiero a ponerse en el lugar de la red aquí desde su punto de vista, el universo entero consiste en nada

182
00:15:51,480 --> 00:15:57,539
Pero dígitos inmóviles claramente definidos centrados en una pequeña rejilla y su función de costes nunca se dio ningún

183
00:15:57,700 --> 00:16:00,959
Incentivo para ser cualquier cosa, pero completamente seguro en sus decisiones

184
00:16:01,690 --> 00:16:05,070
Así que si esta es la imagen de lo que esas segundas neuronas de la capa están haciendo realidad

185
00:16:05,140 --> 00:16:09,839
Usted podría preguntarse por qué habría que introducir esta red con la motivación de recoger en los bordes y patrones

186
00:16:09,839 --> 00:16:11,969
Quiero decir, eso no es en absoluto lo que termina haciendo

187
00:16:13,029 --> 00:16:17,909
Bueno, esto no está destinado a ser nuestro objetivo final, sino un punto de partida ,francamente

188
00:16:17,910 --> 00:16:19,120
Esta es una tecnología antigua

189
00:16:19,120 --> 00:16:21,510
del tipo investigado en los años 80 y 90 y

190
00:16:21,640 --> 00:16:29,129
Tu tienes que entender que antes de poder comprender  las variantes modernas más detalladas , esta claramente es capaz de resolver algunos problemas interesantes.

191
00:16:29,410 --> 00:16:34,110
Pero cuanto indagas  en lo que esas capas ocultas están hacen realmente, menos inteligentes que parece

192
00:16:38,530 --> 00:16:42,359
Toma en consideración por un momento como aprenden las redes a cómo aprendes tu,

193
00:16:42,580 --> 00:16:46,139
eso sólo podrá suceder si te involucras activamente aqui con el material  de alguna manera

194
00:16:46,660 --> 00:16:53,100
Una cosa muy simple que quiero que hagas es en este momento es pausar y pensar profundamente por un momento en los

195
00:16:53,440 --> 00:16:55,230
cambios que podrías hacerle a este sistema

196
00:16:55,230 --> 00:17:00,719
Y la forma en que percibe las imágenes, si quieres que recoja de mejor manera  las  cosas como bordes y patrones?

197
00:17:01,360 --> 00:17:04,420
Pero mejor aun que  involucrarse en realidad con el material

198
00:17:04,680 --> 00:17:08,980
Recomiendo encarecidamente el libro de Michael Nielsen  sobre aprendizaje profundo y redes neuronales.

199
00:17:09,190 --> 00:17:14,369
En el se puede encontrar el código y los datos para descargar y jugar  con este ejemplo.

200
00:17:14,410 --> 00:17:18,089
El libro te guiará a través  de lo que hace el código paso a paso.

201
00:17:18,910 --> 00:17:21,749
Lo que es increíble es que este libro es gratuito y está disponible públicamente

202
00:17:22,360 --> 00:17:27,540
Así que ,si consigues  algo de él, considera unirte a mí y hacer una donación a los esfuerzos de Nielsen.

203
00:17:27,910 --> 00:17:32,219
También he enlazado un par de otros recursos  en la descripción que me gustan mucho , incluido el

204
00:17:32,470 --> 00:17:36,390
fenomenal y hermoso  POST del blog de Chris Ola y los artículos de distill

205
00:17:38,230 --> 00:17:40,200
Para cerrar las cosas  en los últimos minutos.

206
00:17:40,200 --> 00:17:43,740
Quiero saltar de nuevo en un fragmento de la entrevista que tuve con Leisha Lee

207
00:17:43,930 --> 00:17:49,079
Puede que la recuerdes  del último vídeo. Ella hizo su trabajo de doctorado sobre aprendizaje profundo y en este pequeño fragmento

208
00:17:49,080 --> 00:17:55,530
Ella habla de dos trabajos recientes que realmente indagan en cómo algunas de las redes de reconocimiento de imágenes más modernas están aprendiendo en realidad.

209
00:17:55,810 --> 00:18:01,349
Sólo para configurar donde estábamos en la conversación, el primer documento tomó una de estas redes neuronales particularmente profundas

210
00:18:01,350 --> 00:18:05,910
Que  es muy buena en el reconocimiento de imágenes y en lugar de entrenarla  con   un conjunto dedatos debidamente etiquetados

211
00:18:05,910 --> 00:18:08,579
Esta barajan todas las etiquetas alrededor  antes del  entrenamiento.

212
00:18:08,800 --> 00:18:14,669
Obviamente, la exactitud de las pruebas  aquí no iban  a ser mejor que el azar ,ya que todo acaba de ser etiquetado al azar

213
00:18:14,800 --> 00:18:20,879
Pero todavía era capaz de conseguir la misma precisión  de entrenamiento  como la que obtendría con un conjunto de datos debidamente etiquetados.

214
00:18:21,490 --> 00:18:27,540
Básicamente los millones de pesos para esta red en particular fueron suficientes para que simplemente memorice  los datos aleatorios

215
00:18:27,820 --> 00:18:34,379
¿Qué tipo de pregunta se plantea  si la  función coste realmente corresponde realmente  a cualquier tipo de estructura en la imagen?

216
00:18:34,380 --> 00:18:36,380
O si es  sólo , ya sabes?, memorizar

217
00:18:36,520 --> 00:18:37,420
Leisa // Memorizar el

218
00:18:37,420 --> 00:18:43,859
conjunto de datos entero de lo que es la correcta clasificación , y así un par de ustedes saben , en la medio  año después ,en ICML este año

219
00:18:44,470 --> 00:18:49,039
No hubo  exactamente   alguna refutación por papel ,que preguntara por algo como:

220
00:18:49,470 --> 00:18:55,279
Oye!, En realidad, estas redes están haciendo algo un poco más inteligente  de lo vemos  en que la curva de precisión

221
00:18:55,279 --> 00:18:57,499
si tu solo estuviste  entrando con

222
00:18:58,259 --> 00:19:05,179
conjunto de datos aleatorios que llevaron a la curva en descenso,  ya sabes,muy lentamente en casi una especie de forma lineal

223
00:19:05,179 --> 00:19:09,589
Por lo que tu  realmente estas luchando para encontrar es valor mínimo  posible

224
00:19:09,590 --> 00:19:15,289
tu conoces  los pesos adecuados que te conseguirían es precisión mientras que si en realidad estás entrenando con un conjunto estructurado de datos que tiene el

225
00:19:15,289 --> 00:19:21,439
las etiquetas correctas. Tu sabes que tecas el  violín alrededor, un poco al principio, pero luego  se tu lo soltaste  muy rápido para llegar a ese

226
00:19:22,200 --> 00:19:26,149
nivel de precisión y así en cierto sentido era más fácil  encontrar el

227
00:19:26,759 --> 00:19:33,949
máximo local y  lo que también fue interesante de esto es que  trajo a la luz otro papel, de hecho de hace un par de años

228
00:19:34,080 --> 00:19:36,080
Que tiene mucha más

229
00:19:36,990 --> 00:19:39,169
simplificaciones sobre las capas de red

230
00:19:39,169 --> 00:19:46,788
Pero uno de los resultados estaba diciendo ; "" si nos fijamos en el paisaje de optimización de los mínimos locales que estas redes tienden a aprender son

231
00:19:47,340 --> 00:19:54,079
En realidad de la misma calidad", así que en cierto sentido, si  tu  conjunto de datos esta  estructurado ,  deberías ser capaz de encontrar mucho más fácilmente.

232
00:19:58,139 --> 00:20:01,189
Mis gracias como siempre a aquellos de ustedes apoyándome  en patreon

233
00:20:01,190 --> 00:20:06,950
Ya he dicho antes que  patreon es un juego de cambio, pero estos videos realmente no sería posibles sin ti

234
00:20:07,230 --> 00:20:12,889
También quiero dar un especial. Gracias a la frima de VC AMPLIFI PARTNERS,  por su apoyo de estos videos iniciales de la serie

