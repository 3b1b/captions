[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "في الفيديو الأخير قمت بوضع هيكل الشبكة العصبية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "سأقدم ملخصًا سريعًا هنا حتى يظل جديدًا في أذهاننا، وبعد ذلك لدي هدفان رئيسيان لهذا الفيديو.",
  "model": "google_nmt",
  "from_community_srt": "آخر فيديو وضعت هيكل الشبكة العصبية سأعطي ملخصًا سريعًا هنا فقط لكي يكون جديدًا في أذهاننا ثم لدي هدفين رئيسيين لهذا الفيديو.",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "الأول هو تقديم فكرة النسب المتدرج، والتي لا تكمن وراء كيفية تعلم الشبكات العصبية فحسب، بل أيضًا كيفية عمل الكثير من أنظمة التعلم الآلي الأخرى.",
  "model": "google_nmt",
  "from_community_srt": "الأول هو تقديم فكرة الانحدار التدرج ، الذي يكمن وراء ليس فقط كيفية تعلم الشبكات العصبية ، ولكن كيف يعمل الكثير من التعلم الآلي أيضًا",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "ثم بعد ذلك سنتعمق أكثر في كيفية أداء هذه الشبكة تحديدًا، وما الذي تبحث عنه تلك الطبقات المخفية من الخلايا العصبية في نهاية المطاف.",
  "model": "google_nmt",
  "from_community_srt": "ثم بعد ذلك سوف نحفر أكثر قليلاً حول كيفية أداء هذه الشبكة بالتحديد وماذا هذه الطبقات الخفية من الخلايا العصبية ينتهي في الواقع تبحث عنه",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "للتذكير، هدفنا هنا هو المثال الكلاسيكي للتعرف على الأرقام المكتوبة بخط اليد، عالم الشبكات العصبية المرحب.",
  "model": "google_nmt",
  "from_community_srt": "كتذكير هدفنا هنا هو المثال الكلاسيكي للتعرف على الأرقام المكتوبة بخط اليد مرحبا بعالم الشبكات العصبية",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "يتم عرض هذه الأرقام على شبكة بحجم 28 × 28 بكسل، ولكل بكسل قيمة تدرج رمادي تتراوح بين 0 و1.",
  "model": "google_nmt",
  "from_community_srt": "يتم تقديم هذه الأرقام على شبكة 28 × 28 بكسل لكل بكسل مع بعض القيمة الرمادية بين 0 و 1 تلك هي التي تحدد عمليات التنشيط",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "هذه هي التي تحدد تنشيط 784 خلية عصبية في طبقة الإدخال للشبكة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "ومن ثم يعتمد تنشيط كل خلية عصبية في الطبقات التالية على مجموع مرجح لجميع عمليات التنشيط في الطبقة السابقة، بالإضافة إلى رقم خاص يسمى التحيز.",
  "model": "google_nmt",
  "from_community_srt": "784 من الخلايا العصبية في طبقة المدخلات من الشبكة و ثم يقوم التنشيط لكل عصبون في الطبقات التالية على أساس مجموع وزن",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "ثم تقوم بتكوين هذا المجموع باستخدام وظيفة أخرى، مثل السحق السيني، أو الريلو، بالطريقة التي مشيت بها خلال الفيديو الأخير.",
  "model": "google_nmt",
  "from_community_srt": "جميع عمليات التنشيط في الطبقة السابقة بالإضافة إلى بعض الأرقام الخاصة تسمى التحيز ثم تقوم بتكوين هذا المبلغ مع بعض الوظائف الأخرى مثل sishmoid squishification أو",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "في المجمل، بالنظر إلى الاختيار التعسفي إلى حد ما لطبقتين مخفيتين تحتوي كل منهما على 16 خلية عصبية، فإن الشبكة لديها حوالي 13000 من الأوزان والتحيزات التي يمكننا تعديلها، وهذه القيم هي التي تحدد بالضبط ما تفعله الشبكة بالفعل.",
  "model": "google_nmt",
  "from_community_srt": "لعبه بالطريقة التي مشيت من خلال الفيديو الماضي في المجموع ، بالنظر إلى الاختيار التعسفي نوعًا ما لطبقتين مخفيتين هنا مع 16 خلية عصبية لكل منها الشبكة 13000 من الأوزان والتحيزات التي يمكننا ضبطها وهذه القيم هي التي تحدد بالضبط ما تعرفه الشبكة بالفعل",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "ثم ما نعنيه عندما نقول أن هذه الشبكة تصنف رقمًا معينًا هو أن ألمع تلك الخلايا العصبية العشرة في الطبقة النهائية يتوافق مع هذا الرقم.",
  "model": "google_nmt",
  "from_community_srt": "ثم ما نعنيه عندما نقول أن هذه الشبكة تصنف رقمًا معينًا هل هذا ألمع من تلك الخلايا العصبية العشرة في الطبقة النهائية يتوافق مع هذا الرقم",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "وتذكر أن الدافع الذي وضعناه في أذهاننا هنا للبنية الطبقية هو أنه ربما يمكن للطبقة الثانية أن تلتقط الحواف، والطبقة الثالثة قد تلتقط أنماطًا مثل الحلقات والخطوط، ويمكن للطبقة الأخيرة أن تجمع تلك الأنماط معًا أنماط للتعرف على الأرقام.",
  "model": "google_nmt",
  "from_community_srt": "وتذكر أن الدافع الذي كان يدور في ذهننا هنا حول بنية الطبقات هو ربما يمكن أن تلتقط الطبقة الثانية على الحواف وقد تلتقط الطبقة الثالثة أنماطًا مثل الحلقات والخطوط",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "إذن هنا، نتعلم كيف تتعلم الشبكة.",
  "model": "google_nmt",
  "from_community_srt": "ويمكن للمرء الأخير فقط تجميع تلك الأنماط للتعرف على الأرقام إذن هنا نتعلم كيف تتعلم الشبكة ما نريده هو خوارزمية حيث يمكنك إظهار هذه الشبكة مجموعة كاملة من بيانات التدريب",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "ما نريده هو خوارزمية حيث يمكنك أن تعرض لهذه الشبكة مجموعة كاملة من بيانات التدريب، والتي تأتي في شكل مجموعة من الصور المختلفة للأرقام المكتوبة بخط اليد، بالإضافة إلى تسميات لما يفترض أن تكون عليه، وسوف قم بتعديل تلك الأوزان والتحيزات البالغ عددها 13000 لتحسين أدائها في بيانات التدريب.",
  "model": "google_nmt",
  "from_community_srt": "الذي يأتي على شكل مجموعة من الصور المختلفة للأرقام المكتوبة بخط اليد مع ملصقات لما يفترض أن يكون انها سوف ضبط تلك 13000 أوزان وانحياز لتحسين أدائه في بيانات التدريب نأمل أن يعني هذا الهيكل الطبقات ما يتعلم",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "نأمل أن يعني هذا الهيكل متعدد الطبقات أن ما يتعلمه يعمم على الصور بما يتجاوز بيانات التدريب تلك.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "الطريقة التي نختبر بها ذلك هي أنه بعد تدريب الشبكة، تظهر لها المزيد من البيانات المصنفة التي لم يسبق لها رؤيتها من قبل، وترى مدى دقة تصنيف تلك الصور الجديدة.",
  "model": "google_nmt",
  "from_community_srt": "يعمم على الصور بعد تلك البيانات التدريبية والطريقة التي نختبرها هي أنه بعد تدريب الشبكة كنت تظهر أكثر ثيتا المسمى أنه لم يسبق له مثيل من قبل ، وترى مدى دقة تصنيف تلك الصور الجديدة",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "لحسن الحظ بالنسبة لنا، وما يجعل هذا مثالًا شائعًا للبدء به، هو أن الأشخاص الجيدين الذين يقفون وراء قاعدة بيانات MNIST قاموا بتجميع مجموعة من عشرات الآلاف من الصور الرقمية المكتوبة بخط اليد، كل واحدة منها تحمل الأرقام التي من المفترض أن تحملها يكون.",
  "model": "google_nmt",
  "from_community_srt": "ولحسن الحظ بالنسبة لنا وما يجعل هذا المثال المشترك هو أن الأشخاص الجيدين وراء قاعدة MNIST لديهم تجميع مجموعة من عشرات الآلاف من صور الأرقام المكتوبة بخط اليد كل واحد منهم المسمى بالأرقام التي من المفترض أن تكون",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "وبقدر ما يكون وصف الآلة بأنها تتعلم أمرًا مثيرًا، إلا أنه بمجرد أن ترى كيف تعمل، فإن الأمر يبدو أقل شبهًا ببعض فرضيات الخيال العلمي المجنونة، وأكثر شبهًا بتمرين حساب التفاضل والتكامل.",
  "model": "google_nmt",
  "from_community_srt": "إنه أمر مثير للاستفزاز لأنه لوصف آلة ما بأنها تعلم بمجرد أن ترى كيف تعمل إنه شعور أقل بكثير مثل فرضية الخيال العلمي المجنونة وأكثر بكثير مثل ممارسة حساب التفاضل والتكامل",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "أعني أن الأمر يتعلق في الأساس بإيجاد الحد الأدنى من وظيفة معينة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "تذكر، من الناحية النظرية، نحن نفكر في كل خلية عصبية على أنها متصلة بجميع الخلايا العصبية في الطبقة السابقة، والأوزان في المجموع المرجح الذي يحدد تنشيطها تشبه إلى حد ما نقاط قوة تلك الاتصالات، والتحيز هو بعض المؤشرات على ما إذا كانت تلك الخلية العصبية تميل إلى أن تكون نشطة أو غير نشطة.",
  "model": "google_nmt",
  "from_community_srt": "أعني أساسًا ، هو العثور على الحد الأدنى من وظيفة معينة تذكر من الناحية النظرية أننا نفكر في كل خلية عصبية بأنها متصلة لجميع الخلايا العصبية في الطبقة السابقة ، والأوزان في المجموع المرجح تحديد التنشيط لها هي مثل نقاط القوة في تلك الاتصالات والتحيز هو مؤشر على ما إذا كان ذلك العصبون يميل إلى أن يكون نشطًا أو غير نشط ويبدأ الأمور",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "ولبدء الأمور، سنقوم بتهيئة كل هذه الأوزان والتحيزات بشكل عشوائي تمامًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "وغني عن القول أن أداء هذه الشبكة سيكون سيئًا جدًا في مثال تدريب معين، نظرًا لأنها تفعل شيئًا عشوائيًا.",
  "model": "google_nmt",
  "from_community_srt": "سنقوم فقط بتهيئة كل تلك الأوزان والتحيزات بشكل عشوائي بدون داع لتقول أن هذه الشبكة ستؤديها رهيبة جدا على مثال تدريب معين لأنه مجرد القيام بشيء عشوائي على سبيل المثال تقوم بتغذية هذه الصورة من 3 و",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "على سبيل المثال، قمت بتغذية هذه الصورة بالرقم 3، وستبدو طبقة الإخراج وكأنها في حالة من الفوضى.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "إذن ما تفعله هو تحديد دالة التكلفة، وهي طريقة لإخبار الكمبيوتر، لا، كمبيوتر سيء، يجب أن يحتوي هذا الإخراج على عمليات تنشيط تكون 0 لمعظم الخلايا العصبية، ولكن 1 لهذه الخلية العصبية، ما قدمته لي هو قمامة مطلقة.",
  "model": "google_nmt",
  "from_community_srt": "طبقة الإخراج يبدو فقط مثل الفوضى إذن ما تفعله هو تحديد وظيفة التكلفة كطريقة لإخبار الكمبيوتر: \"لا يوجد كمبيوتر سيء! يجب أن يكون لهذا الناتج عمليات تنشيط تكون صفراً لمعظم العصبونات ، لكن واحد لهذا العصبون هو ما أعطيتني هو القمامة المطلقة \"",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "لنقول ذلك من الناحية الرياضية، يمكنك جمع مربعات الاختلافات بين كل من عمليات تنشيط مخرجات المهملات هذه والقيمة التي تريدها لها، وهذا ما سنسميه تكلفة مثال تدريبي واحد.",
  "model": "google_nmt",
  "from_community_srt": "لقول ما هو أكثر من ذلك بقليل رياضيا ما تفعله هو إضافة مربعات الاختلافات بين كل من هذه التنبيهات إخراج القمامة والقيمة التي تريدها أن يكون لها و",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "لاحظ أن هذا المجموع يكون صغيرًا عندما تقوم الشبكة بتصنيف الصورة بشكل صحيح بثقة، ولكنه يكون كبيرًا عندما تبدو الشبكة وكأنها لا تعرف ما تفعله.",
  "model": "google_nmt",
  "from_community_srt": "هذا هو ما سنسميه تكلفة مثال تدريب واحد لاحظ أن هذا المبلغ صغير عندما تقوم الشبكة بتصنيف الصورة بشكل صحيح",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "إذن ما عليك فعله هو أن تأخذ في الاعتبار متوسط التكلفة لجميع عشرات الآلاف من أمثلة التدريب المتاحة لك.",
  "model": "google_nmt",
  "from_community_srt": "ولكنها كبيرة عندما تبدو الشبكة وكأنها لا تعرف حقًا ما تفعله إذن ، ما عليك فعله هو النظر في متوسط ​​التكلفة على عشرات الآلاف من الأمثلة التدريبية المتاحة لك",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "إن متوسط التكلفة هذا هو مقياسنا لمدى رديئة الشبكة، ومدى السوء الذي يجب أن يشعر به الكمبيوتر.",
  "model": "google_nmt",
  "from_community_srt": "هذه التكلفة المتوسطة هي مقياسنا لمدى روعة الشبكة ومدى الضرر الذي يشعر به الكمبيوتر ، وهذا أمر معقد",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "وهذا شيء معقد.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "هل تتذكر كيف كانت الشبكة نفسها في الأساس وظيفة، تأخذ 784 رقمًا كمدخلات، وقيم البكسل، وتخرج 10 أرقام كمخرجات لها، وبمعنى ما يتم تحديد معلماتها بكل هذه الأوزان والتحيزات؟",
  "model": "google_nmt",
  "from_community_srt": "تذكر كيف كانت الشبكة نفسها في الأساس وظيفة تستلزمها أرقام 784 كمدخلات قيم البكسل وتبصق بها عشرة أرقام كمخرج لها وبمعنى ما",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "حسنًا، دالة التكلفة هي طبقة من التعقيد فوق ذلك.",
  "model": "google_nmt",
  "from_community_srt": "إنها معلمة بكل هذه الأوزان والتحيزات في حين أن وظيفة التكلفة هي طبقة من التعقيد فوق ذلك تأخذها كمدخل لها",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "إنها تأخذ كمدخلاتها تلك الأوزان والتحيزات التي يبلغ عددها 13000 أو نحو ذلك، وتطلق رقمًا واحدًا يصف مدى سوء تلك الأوزان والتحيزات، وتعتمد طريقة تعريفها على سلوك الشبكة على كل عشرات الآلاف من أجزاء بيانات التدريب.",
  "model": "google_nmt",
  "from_community_srt": "تلك ثلاثة عشر ألف أو ما يقاربها من الأوزان والتحيزات وتبث رقمًا واحدًا يصف مدى سوء تلك الأوزان والتحيزات وتعتمد الطريقة التي يتم تحديدها على سلوك الشبكة على جميع عشرات الآلاف من بيانات التدريب هذا الكثير للتفكير",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "هذا كثير للتفكير فيه.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "لكن مجرد إخبار الكمبيوتر بالمهمة السيئة التي يقوم بها ليس مفيدًا جدًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "تريد أن تخبره بكيفية تغيير تلك الأوزان والتحيزات حتى يتحسن.",
  "model": "google_nmt",
  "from_community_srt": "ولكن مجرد إخبار الكمبيوتر عن الوظيفة المزعجة ، فهو ليس مفيدًا للغاية هل تريد أن تعرف كيف تغير هذه الأوزان والتحيزات حتى تتحسن؟",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "لتسهيل الأمر، بدلًا من صعوبة تخيل دالة تحتوي على 13000 مدخل، تخيل فقط دالة بسيطة تحتوي على رقم واحد كمدخل ورقم واحد كمخرج.",
  "model": "google_nmt",
  "from_community_srt": "لجعل الأمر أسهل بدلاً من تكافح تخيل وظيفة مع 13000 مدخلات فقط تخيل وظيفة بسيطة تحتوي على رقم واحد كمدخل ورقم واحد كمخرج",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "كيف يمكنك العثور على مدخلات تقلل من قيمة هذه الوظيفة؟",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "سيعرف طلاب حساب التفاضل والتكامل أنه يمكنك في بعض الأحيان معرفة هذا الحد الأدنى بشكل صريح، ولكن هذا ليس ممكنًا دائمًا للوظائف المعقدة حقًا، وبالتأكيد ليس في إصدار الإدخال 13000 من هذا الموقف لوظيفة تكلفة الشبكة العصبية المعقدة والمجنونة.",
  "model": "google_nmt",
  "from_community_srt": "كيف يمكنك العثور على مدخلات تقلل من قيمة هذه الوظيفة؟ سيعلم الطلاب في حساب التفاضل والتكامل أنه يمكنك أحيانًا معرفة ذلك الحد الأدنى بشكل صريح ولكن هذا ليس ممكنا دائما لوظائف معقدة حقا بالتأكيد ليس في النسخة ثلاثة عشر ألف مدخلات من هذا الوضع لدينا وظيفة تكلفة الشبكة العصبية معقدة مجنون",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "التكتيك الأكثر مرونة هو البدء عند أي مدخلات، ومعرفة الاتجاه الذي يجب أن تسلكه لتقليل هذا الناتج.",
  "model": "google_nmt",
  "from_community_srt": "التكتيك الأكثر مرونة هو أن تبدأ من أي مدخلات قديمة وتعرف على الاتجاه الذي يجب أن تتخذه لجعل هذا الإنتاج أقل",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "على وجه التحديد، إذا كان بإمكانك معرفة ميل الدالة التي تتواجد فيها، فانتقل إلى اليسار إذا كان هذا الميل موجبًا، وقم بتحويل الإدخال إلى اليمين إذا كان هذا الميل سالبًا.",
  "model": "google_nmt",
  "from_community_srt": "على وجه التحديد إذا كنت تستطيع معرفة منحدر الوظيفة التي أنت فيها ثم انتقل إلى اليسار إذا كان هذا المنحدر موجبًا وقم بتحويل الإدخال إلى اليمين إذا كان هذا المنحدر سلبيًا",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "إذا قمت بذلك بشكل متكرر، عند كل نقطة تتحقق من الميل الجديد وتتخذ الخطوة المناسبة، فسوف تقترب من الحد الأدنى المحلي للدالة.",
  "model": "google_nmt",
  "from_community_srt": "إذا قمت بذلك مرارًا وتكرارًا في كل نقطة فحص المنحدر الجديد واتخاذ الخطوة المناسبة كنت ستعمل الاقتراب من الحد الأدنى المحلي للوظيفة و",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "الصورة التي قد تكون في ذهنك هنا هي كرة تتدحرج أسفل التل.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "لاحظ، حتى بالنسبة لوظيفة الإدخال الفردي المبسطة حقًا، هناك العديد من الانحدارات المحتملة التي قد تصل إليها، اعتمادًا على الإدخال العشوائي الذي تبدأ منه، وليس هناك ما يضمن أن الحد الأدنى المحلي الذي تهبط فيه سيكون أصغر قيمة ممكنة من دالة التكلفة.",
  "model": "google_nmt",
  "from_community_srt": "الصورة التي قد تدور في ذهنك هنا هي الكرة المتدحرجة أسفل التل و لاحظ حتى بالنسبة إلى وظيفة الإدخال المفرد هذه المبسطة حقًا ، هناك العديد من الوديان المحتملة التي قد تصل إليها اعتمادا على أي إدخال عشوائي تبدأ في وليس هناك ما يضمن أن الحد الأدنى المحلي سوف تهبط ستكون أصغر قيمة ممكنة لوظيفة التكلفة",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "سيتم نقل ذلك إلى حالة الشبكة العصبية لدينا أيضًا.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "And I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that kind of helps you from overshooting.",
  "translatedText": "أريدك أيضًا أن تلاحظ كيف أنه إذا جعلت أحجام خطواتك متناسبة مع المنحدر، فعندما يصبح المنحدر مسطحًا نحو الحد الأدنى، تصبح خطواتك أصغر فأصغر، وهذا يساعدك على عدم التجاوز.",
  "model": "google_nmt",
  "from_community_srt": "هذا سوف ينتقل إلى حالة الشبكة العصبية أيضًا ، وأريد أيضًا أن تلاحظ ذلك كيف إذا كنت تجعل أحجام الخطوة الخاصة بك متناسبة مع المنحدر ثم عندما يميل المنحدر نحو الحد الأدنى ، تصبح خطواتك أصغر وأصغر وهذا النوع يساعدك على تجاوز الحدود",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "ولزيادة التعقيد قليلاً، تخيل بدلاً من ذلك دالة ذات مدخلين ومخرج واحد.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "قد تفكر في مساحة الإدخال على أنها مستوى xy، ودالة التكلفة على أنها مرسوم بيانيًا كسطح فوقها.",
  "model": "google_nmt",
  "from_community_srt": "إن الارتقاء بالتعقيد قد يتخيل بدلاً من ذلك دالة ذات مدخلين ومخرج واحد قد تفكر في مساحة الإدخال كمستوى XY ووظيفة التكلفة على أنها رسوم بيانية على هيئة سطح فوقها",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "بدلًا من السؤال عن ميل الدالة، عليك أن تسأل عن الاتجاه الذي يجب أن تخطو فيه في مساحة الإدخال هذه لتقليل مخرجات الدالة بسرعة أكبر.",
  "model": "google_nmt",
  "from_community_srt": "الآن بدلاً من السؤال عن منحدر الدالة ، عليك أن تسأل عن أي اتجاه يجب أن تخطو في مساحة الإدخال هذه؟ وذلك لتقليل انتاج الوظيفة بسرعة أكبر وبعبارة أخرى.",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "وبعبارة أخرى، ما هو الاتجاه الهبوطي؟",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "مرة أخرى، من المفيد أن نفكر في كرة تتدحرج إلى أسفل ذلك التل.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "أولئك الذين هم على دراية بحساب التفاضل والتكامل متعدد المتغيرات سيعرفون أن تدرج الدالة يمنحك الاتجاه الأكثر انحدارًا للصعود، وهو الاتجاه الذي يجب أن تخطو إليه لزيادة الدالة بسرعة أكبر.",
  "model": "google_nmt",
  "from_community_srt": "ما هو اتجاه الإنحدار؟ ومرة أخرى من المفيد أن نفكر في كرة تتدحرج إلى أسفل هذا التل سيعرف أولئك الذين على دراية بحساب التفاضل والتكامل متعدد المتغيرات أن التدرج في وظيفة يمنحك اتجاه صعود حاد",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "وبطبيعة الحال، فإن أخذ سالب هذا التدرج يمنحك الاتجاه للخطوة التي تقلل الدالة بسرعة أكبر.",
  "model": "google_nmt",
  "from_community_srt": "بشكل أساسي ، أي اتجاه يجب أن تخطوه لزيادة الوظيفة بسرعة أكبر بطبيعة الحال ما يكفي من اتخاذ السلبية من هذا الانحدار يعطيك الاتجاه إلى الخطوة التي تقلل من وظيفة بسرعة أكبر و",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "والأكثر من ذلك، فإن طول متجه التدرج هذا يعد مؤشرًا على مدى انحدار هذا المنحدر الأكثر انحدارًا.",
  "model": "google_nmt",
  "from_community_srt": "أكثر من ذلك أن طول هذا الناقل المتدرج هو في الواقع مؤشر على مدى الانحدار الحاد لهذا المنحدر",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "إذا لم تكن على دراية بحساب التفاضل والتكامل متعدد المتغيرات وترغب في معرفة المزيد، فاطلع على بعض الأعمال التي قمت بها لصالح أكاديمية خان حول هذا الموضوع.",
  "model": "google_nmt",
  "from_community_srt": "الآن إذا كنت غير معتاد على حساب التفاضل والتكامل متعدد المتغيرات وتريد أن تتعرف أكثر على بعض الأعمال التي قمت بها في أكاديمية خان حول هذا الموضوع",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "لكن بصراحة، كل ما يهمني ولك الآن هو أنه من حيث المبدأ توجد طريقة لحساب هذا المتجه، هذا المتجه الذي يخبرك ما هو اتجاه الهبوط ومدى انحداره.",
  "model": "google_nmt",
  "from_community_srt": "بصراحة ، على الرغم من كل ما يهم بالنسبة لي وأنا الآن هل هذا من حيث المبدأ هناك طريقة لحساب هذا الناقل. هذا المتجه الذي يخبرك ما اتجاه انحدار هو وكيف حاد هو أنك ستكون على ما يرام إذا كان هذا هو كل ما تعرفه وأنت لست صخرة الصلبة على التفاصيل",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "ستكون على ما يرام إذا كان هذا هو كل ما تعرفه ولم تكن ملتزمًا بالتفاصيل.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "إذا تمكنت من الحصول على ذلك، فإن خوارزمية تقليل الوظيفة هي حساب اتجاه التدرج هذا، ثم اتخاذ خطوة صغيرة إلى أسفل، وتكرار ذلك مرارًا وتكرارًا.",
  "model": "google_nmt",
  "from_community_srt": "لأنه إذا كان يمكنك الحصول على أن الخوارزمية من التقليل من الوظيفة هي حساب هذا الاتجاه التدرج ثم اتخاذ خطوة صغيرة إلى أسفل و",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "إنها نفس الفكرة الأساسية للدالة التي تحتوي على 13000 مدخلًا بدلاً من مدخلين.",
  "model": "google_nmt",
  "from_community_srt": "فقط أكرر ذلك مرارا وتكرارا إنها نفس الفكرة الأساسية لوظيفة لديها 13000 مدخلات بدلاً من مدخلين تخيل تنظيم الكل",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "تخيل تنظيم جميع الأوزان والتحيزات البالغ عددها 13000 لشبكتنا في ناقل عمود عملاق.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "التدرج السلبي لدالة التكلفة هو مجرد متجه، إنه اتجاه ما داخل مساحة الإدخال الضخمة بجنون والتي تخبرك بأي دفعات لكل هذه الأرقام ستسبب أسرع انخفاض في دالة التكلفة.",
  "model": "google_nmt",
  "from_community_srt": "13000 أوزان وانحياز شبكتنا إلى ناقل عمودية عملاقة التدرج السلبي لوظيفة التكلفة هو مجرد متجه انها بعض الاتجاه داخل هذا الفضاء المدخلات الضخمة بجنون تخبرك دفعات لجميع هذه الأرقام سيؤدي إلى انخفاض أسرع إلى وظيفة التكلفة و",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "وبطبيعة الحال، مع دالة التكلفة المصممة خصيصًا لدينا، فإن تغيير الأوزان والتحيزات لتقليلها يعني جعل مخرجات الشبكة على كل جزء من بيانات التدريب تبدو أقل كمصفوفة عشوائية من 10 قيم، وأكثر مثل القرار الفعلي الذي نريده لجعل.",
  "model": "google_nmt",
  "from_community_srt": "بالطبع مع وظيفة التكلفة المصممة خصيصا لدينا تغيير الأوزان والتحيزات لتقليل ذلك يعني جعل ناتج الشبكة على كل جزء من بيانات التدريب تبدو أقل مثل مجموعة عشوائية من عشر قيم وأكثر مثل القرار الفعلي الذي نريده من المهم أن تتذكر أن وظيفة التكلفة هذه تشتمل على متوسط ​​لكل بيانات التدريب",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "من المهم أن تتذكر أن دالة التكلفة هذه تتضمن متوسطًا لجميع بيانات التدريب، لذلك إذا قمت بتصغيره، فهذا يعني أنه أداء أفضل في كل تلك العينات.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "الخوارزمية لحساب هذا التدرج بكفاءة، والتي هي في الواقع جوهر كيفية تعلم الشبكة العصبية، تسمى الانتشار العكسي، وهذا ما سأتحدث عنه في الفيديو التالي.",
  "model": "google_nmt",
  "from_community_srt": "لذا إذا قمت بتصغيره ، فهذا يعني أنه أداء أفضل على جميع تلك العينات إن خوارزمية حوسبة هذا التدرج بكفاءة والتي هي في الواقع قلب كيفية معرفة الشبكة العصبية تسمى التكرار",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "هناك، أريد حقًا أن أخصص وقتًا للتجول في ما يحدث بالضبط لكل وزن وتحيز لجزء معين من بيانات التدريب، في محاولة لإعطاء إحساس بديهي بما يحدث خارج كومة حسابات التفاضل والتكامل والصيغ ذات الصلة.",
  "model": "google_nmt",
  "from_community_srt": "وهذا ما سأتحدث عنه بشأن الفيديو التالي هناك أنا حقا أريد أن تأخذ من الوقت للمشي من خلال ما الذي يحدث بالضبط لكل وزن وكل تحيز لجزء معين من بيانات التدريب؟ في محاولة لإعطاء إحساس بديهية لما يحدث خارج كومة حساب التفاضل والتكامل والصيغ ذات الصلة هنا الآن الشيء الرئيسي.",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "هنا، الآن، الشيء الرئيسي الذي أريدك أن تعرفه، بغض النظر عن تفاصيل التنفيذ، هو أن ما نعنيه عندما نتحدث عن التعلم الشبكي هو أنه مجرد تقليل دالة التكلفة.",
  "model": "google_nmt",
  "from_community_srt": "أريدك أن تعرف مستقلًا عن تفاصيل التنفيذ هو أن ما نعنيه عندما نتحدث عن تعلم الشبكة هو أنه يقلل من وظيفة التكلفة و",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "ولاحظ أن إحدى نتائج ذلك هي أنه من المهم لدالة التكلفة هذه أن يكون لها ناتج سلس ولطيف، حتى نتمكن من إيجاد الحد الأدنى المحلي عن طريق اتخاذ خطوات صغيرة نحو الأسفل.",
  "model": "google_nmt",
  "from_community_srt": "لاحظ إحدى نتائج ذلك أنه من المهم أن تكون وظيفة التكلفة هذه ذات ناتج سلس جميل حتى نتمكن من العثور على الحد الأدنى المحلي عن طريق اتخاذ خطوات قليلة إلى أسفل",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "ولهذا السبب، بالمناسبة، تتمتع الخلايا العصبية الاصطناعية بتنشيطات متفاوتة باستمرار، بدلاً من أن تكون ببساطة نشطة أو غير نشطة بطريقة ثنائية، كما هي الحال مع الخلايا العصبية البيولوجية.",
  "model": "google_nmt",
  "from_community_srt": "هذا هو السبب في ذلك تقوم العصبونات الاصطناعية بشكل مستمر بتدفق عمليات التنشيط بدلاً من كونها نشطة أو غير نشطة بطريقة ثنائية",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "تسمى هذه العملية المتمثلة في دفع مدخلات دالة بشكل متكرر عن طريق عدة مضاعفات التدرج السلبي نزول التدرج.",
  "model": "google_nmt",
  "from_community_srt": "إذا كانت الطريقة التي تكون بها الخلايا العصبية البيولوجية ويطلق على هذه العملية من تكرار إدخال إحدى الدوال من قبل بعض مضاعفات التدرج السلبي تدعى تنازلي التدرج",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "إنها طريقة للتقارب نحو حد أدنى محلي لدالة التكلفة، وهو في الأساس واد في هذا الرسم البياني.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "ما زلت أعرض صورة الدالة ذات المدخلين، بالطبع، لأن الدفعات في مساحة إدخال ذات 13000 بُعد يصعب قليلاً استيعابها، ولكن هناك طريقة لطيفة غير مكانية للتفكير في هذا.",
  "model": "google_nmt",
  "from_community_srt": "إنها طريقة للتوافق مع بعض الحد الأدنى المحلي لوظيفة التكلفة أساسًا في هذا الرسم البياني لا أزال أقوم بعرض صورة دالة مع اثنين من المدخلات بالطبع لأن الإغراءات في ثلاثة عشر ألف مدخلات الأبعاد",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "يخبرنا كل مكون من مكونات التدرج السلبي بأمرين.",
  "model": "google_nmt",
  "from_community_srt": "الفضاء صعب قليلاً لفك عقلك ، لكن هناك في الواقع طريقة غير مكانية لطيفة للتفكير في هذا يخبرنا كل عنصر من عناصر التدرج السلبي عن أمرين ، تخبرنا الإشارة بالطبع عما إذا كانت المقابلة",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "تخبرنا العلامة بالطبع ما إذا كان يجب دفع المركبة المقابلة لمتجه الإدخال لأعلى أم لأسفل.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "ولكن الأهم من ذلك، أن الأحجام النسبية لجميع هذه المكونات تخبرك بالتغييرات الأكثر أهمية.",
  "model": "google_nmt",
  "from_community_srt": "يجب أن يتم دفع عنصر ناقل الدخل لأعلى أو لأسفل ، ولكن الأهم من ذلك هو المقاييس النسبية لكل هذه المكونات",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "كما ترى، في شبكتنا، قد يكون لتعديل أحد الأوزان تأثير أكبر بكثير على دالة التكلفة من التعديل على بعض الأوزان الأخرى.",
  "model": "google_nmt",
  "from_community_srt": "نوع من يخبرك التغييرات التي تهم أكثر كما ترى في شبكتنا ، قد يكون التعديل على أحد الأوزان أكبر بكثير",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "بعض هذه الاتصالات مهمة أكثر بالنسبة لبيانات التدريب لدينا.",
  "model": "google_nmt",
  "from_community_srt": "تأثير على وظيفة التكلفة من التكيف مع بعض الوزن الآخر بعض هذه الاتصالات لها أهمية أكبر لبيانات التدريب الخاصة بنا",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "لذا، إحدى الطرق التي يمكنك من خلالها التفكير في متجه التدرج هذا لدالة التكلفة الضخمة المذهلة لدينا هي أنه يشفر الأهمية النسبية لكل وزن وتحيز، أي أي من هذه التغييرات سيحقق أكبر قدر من المال.",
  "model": "google_nmt",
  "from_community_srt": "لذا ، يمكنك التفكير في هذا المتجه المتدرج لعقلنا وظيفة التكلفة الهائلة هي أنه يشفر الأهمية النسبية لكل الوزن والتحيز هذا هو أي من هذه التغييرات سوف تحمل أكثر ضجة لباك الخاصة بك هذا حقا هو مجرد طريقة أخرى للتفكير في الاتجاه",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "هذه في الواقع مجرد طريقة أخرى للتفكير في الاتجاه.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "لنأخذ مثالًا أبسط، إذا كان لديك دالة تحتوي على متغيرين كمدخل، وقمت بحساب أن تدرجها عند نقطة معينة يظهر كـ 3,1، فمن ناحية يمكنك تفسير ذلك على أنه قول ذلك عندما عندما نقف عند هذا المدخل، فإن التحرك على طول هذا الاتجاه يزيد من الدالة بسرعة أكبر، وعندما ترسم الدالة رسمًا بيانيًا فوق مستوى نقاط الإدخال، فإن هذا المتجه هو ما يمنحك الاتجاه الصعودي المستقيم.",
  "model": "google_nmt",
  "from_community_srt": "لأخذ مثال أبسط إذا كان لديك بعض الوظائف مع متغيرين كمدخل وأنت احسب أن التدرج في بعض النقاط يخرج كما هو (3،1) ثم من ناحية ، يمكنك تفسير ذلك كقولك عندما تقف عند هذا الإدخال التحرك على طول هذا الاتجاه يزيد من الوظيفة بسرعة أكبر عندما تقوم بعمل رسم بياني للدالة أعلى مستوى نقاط الإدخال التي يتم توجيهها ، فهذا هو ما يعطيك اتجاه صاعد مستقيم",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "ولكن هناك طريقة أخرى لقراءة ذلك وهي أن نقول إن التغييرات في هذا المتغير الأول لها أهمية 3 أضعاف أهمية التغييرات في المتغير الثاني، وذلك على الأقل في جوار المدخلات ذات الصلة، فإن دفع قيمة x يحمل تأثيرًا أكبر بكثير بالنسبة لك دولار.",
  "model": "google_nmt",
  "from_community_srt": "لكن هناك طريقة أخرى لقراءة ذلك وهي أن التغييرات على هذا المتغير الأول أن يكون لديك ثلاثة أضعاف الأهمية كتغييرات للمتغير الثاني على الأقل في الجوار للمدخلات ذات الصلة",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "دعونا نصغر الصورة ونلخص ما وصلنا إليه حتى الآن.",
  "model": "google_nmt",
  "from_community_srt": "إن دفع قيمة x يحمل الكثير من الدوي لجهودكم حسنا لنقوم بتصغير ونلخص حيث أننا حتى الآن الشبكة نفسها هي هذه الوظيفة",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "الشبكة نفسها هي هذه الوظيفة التي تحتوي على 784 مدخلاً و10 مخرجات، محددة من حيث كل هذه المبالغ الموزونة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "دالة التكلفة هي طبقة من التعقيد فوق ذلك.",
  "model": "google_nmt",
  "from_community_srt": "784 مدخلات و 10 مخرجات محددة من حيث كل هذه المبالغ المرجحة وظيفة التكلفة هي طبقة من التعقيد على رأس أنه يأخذ",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "فهو يأخذ 13000 من الأوزان والتحيزات كمدخلات ويطلق مقياسًا واحدًا للرداءة بناءً على أمثلة التدريب.",
  "model": "google_nmt",
  "from_community_srt": "13،000 أوزان وانحياز كمدخلات وبصق خارجا تدبير واحد من lousyness إستنادا على أمثلة التدريب و",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "ولا يزال تدرج دالة التكلفة يمثل طبقة أخرى من التعقيد.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "فهو يخبرنا ما هي الحوافز لكل هذه الأوزان والتحيزات التي تسبب التغيير الأسرع في قيمة دالة التكلفة، والتي قد تفسرها على أنها تحدد التغييرات التي تطرأ على الأوزان الأكثر أهمية.",
  "model": "google_nmt",
  "from_community_srt": "إن التدرج في دالة التكلفة هو طبقة أخرى من التعقيد ما زالت تخبرنا ما يدفع إلى كل هذه الأوزان والتحيزات يسبب التغيير الأسرع في قيمة دالة التكلفة",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "لذلك، عند تهيئة الشبكة بأوزان وتحيزات عشوائية، وضبطها عدة مرات بناءً على عملية الهبوط المتدرج هذه، ما مدى جودة أدائها فعليًا على الصور التي لم يتم رؤيتها من قبل؟",
  "model": "google_nmt",
  "from_community_srt": "ما يمكنك تفسيره هو تحديد التغييرات التي تهم الأوزان أكثر من غيرها لذلك عند تهيئة الشبكة باستخدام الأوزان والتحيزات العشوائية وضبطها عدة مرات بناءً على عملية هبوط التدرج",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "تلك التي وصفتها هنا، مع الطبقتين المخفيتين المكونتين من 16 خلية عصبية، والتي تم اختيارها في الغالب لأسباب جمالية، ليست سيئة، حيث تصنف حوالي 96٪ من الصور الجديدة التي تراها بشكل صحيح.",
  "model": "google_nmt",
  "from_community_srt": "ما مدى جودة الأداء الفعلي للصور التي لم يسبق مشاهدتها من قبل؟ حسناً تلك التي وصفتها هنا مع الطبقتين المختبئين من 16 خلية عصبية تم اختيار كل منها لأسباب جمالية",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "وبصراحة، إذا نظرت إلى بعض الأمثلة التي أخطأت فيها، ستشعر أنك مجبر على التقليل من الأمر قليلاً.",
  "model": "google_nmt",
  "from_community_srt": "حسنا ، انها ليست سيئة يصنف حوالي 96 في المئة من الصور الجديدة التي يراها بشكل صحيح و بصراحة ، إذا نظرت إلى بعض الأمثلة التي تعتصر عليك نوعًا ما تشعر بأنها مضطرة إلى قطعها قليلاً من الركود",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "الآن، إذا تلاعبت ببنية الطبقة المخفية وقمت بإجراء بعض التعديلات، يمكنك الحصول على ما يصل إلى 98%.",
  "model": "google_nmt",
  "from_community_srt": "الآن إذا كنت تلعب مع بنية الطبقة الخفية وجعل اثنين من القرص يمكنك الحصول على هذا بنسبة تصل إلى 98٪ وهذا جيد جدًا.",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "وهذا جيد جدًا!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "إنها ليست الأفضل، يمكنك بالتأكيد الحصول على أداء أفضل من خلال الحصول على شبكة أكثر تعقيدًا من شبكة الفانيليا البسيطة هذه، ولكن نظرًا لمدى صعوبة المهمة الأولية، أعتقد أن هناك شيئًا لا يصدق في أي شبكة تفعل هذا جيدًا على صور لم يسبق لها مثيل من قبل، نظرًا لذلك لم نخبره مطلقًا بالأنماط التي يجب البحث عنها.",
  "model": "google_nmt",
  "from_community_srt": "ليس هذا هو الأفضل يمكنك بالتأكيد الحصول على أداء أفضل من خلال الحصول على أكثر تطوراً من شبكة الفانيلا هذه ولكن بالنظر إلى مدى صعوبة المهمة الأولية ، فأنا أعتقد أن هناك شيئًا ما؟ لا يصدق أي شبكة تفعل ذلك بشكل جيد على الصور التي لم يسبق له مثيل من قبل",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "في الأصل، كانت الطريقة التي حفزت بها هذا الهيكل هي وصف الأمل الذي قد يكون لدينا، وهو أن الطبقة الثانية قد تلتقط حوافًا صغيرة، وأن الطبقة الثالثة ستجمع تلك الحواف معًا للتعرف على الحلقات والخطوط الأطول، وأنه يمكن تجميعها معا للتعرف على الأرقام.",
  "model": "google_nmt",
  "from_community_srt": "وبالنظر إلى أننا لم نخبره تحديدًا عن الأنماط التي يجب البحث عنها في الأصل كانت الطريقة التي حفزت بها هذا الهيكل هي وصف الأمل الذي قد نتمتع به أن الطبقة الثانية قد تلتقط على حواف صغيرة أن الطبقة الثالثة ستجمع هذه الحواف لتمييز الحلقات وخطوط أطول ، وقد يتم تجميعها معًا للتعرف على الأرقام",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "فهل هذا ما تفعله شبكتنا بالفعل؟",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "حسنًا، بالنسبة لهذا على الأقل، ليس على الإطلاق.",
  "model": "google_nmt",
  "from_community_srt": "فهل هذا ما تفعله شبكتنا بالفعل؟ حسنا لهذا واحد على الأقل على الاطلاق تذكر كيف الفيديو الماضي نظرنا في كيفية أوزان",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "هل تتذكر كيف نظرنا في الفيديو الأخير إلى كيفية تصور أوزان الاتصالات من جميع الخلايا العصبية في الطبقة الأولى إلى خلية عصبية معينة في الطبقة الثانية كنمط بكسل معين تلتقطه الخلية العصبية في الطبقة الثانية؟",
  "model": "google_nmt",
  "from_community_srt": "اتصالات من جميع الخلايا العصبية في الطبقة الأولى إلى خلية عصبية معينة في الطبقة الثانية يمكن تصورها كنمط بكسل محدد يتم التقاطه في الخلية العصبية من الطبقة الثانية",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "حسنًا، عندما نفعل ذلك بالفعل بالنسبة للأوزان المرتبطة بهذه التحولات، من الطبقة الأولى إلى الطبقة التالية، بدلاً من التقاط حواف صغيرة معزولة هنا وهناك، فإنها تبدو عشوائية تقريبًا، فقط مع بعض الأنماط الفضفاضة جدًا في الوسط هناك.",
  "model": "google_nmt",
  "from_community_srt": "حسنا عندما نقوم بذلك فعلا للأوزان المرتبطة بهذه التحولات من الطبقة الأولى إلى التالية بدلا من التقاط على حواف صغيرة معزولة هنا وهناك. أنها تبدو بشكل جيد تقريبا عشوائي مجرد وضع بعض أنماط فضفاضة جدا في الوسط هناك يبدو أنه في كبير لا يسبر غوره",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "يبدو أنه في الفضاء البُعدي الكبير الذي لا يسبر غوره والذي يبلغ 13000 بُعدًا من الأوزان والتحيزات المحتملة، وجدت شبكتنا نفسها حدًا أدنى محليًا صغيرًا سعيدًا، والذي، على الرغم من نجاحه في تصنيف معظم الصور، لا يلتقط تمامًا الأنماط التي كنا نأملها.",
  "model": "google_nmt",
  "from_community_srt": "13،000 مساحة الأبعاد من الأوزان الممكنة والتحيزات وجدت شبكتنا نفسها الدنيا المحلية الصغيرة السعيدة ذلك على الرغم من أن تصنيف معظم الصور بنجاح لا يرقى بالضبط إلى الأنماط التي كنا نأمل في الحصول عليها و",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "ولتوضيح هذه النقطة حقًا، شاهد ما يحدث عند إدخال صورة عشوائية.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "إذا كان النظام ذكيًا، فقد تتوقع أن يشعر بعدم اليقين، وربما لا ينشط أيًا من تلك الخلايا العصبية العشرة أو ينشطها جميعًا بشكل متساوٍ، ولكنه بدلاً من ذلك يعطيك بثقة بعض الإجابات الهراء، كما لو كان متأكدًا من أن هذا الضجيج العشوائي هي 5 كما هو الحال مع الصورة الفعلية للرقم 5 هي 5.",
  "model": "google_nmt",
  "from_community_srt": "حقا لدفع هذه النقطة المنزل مشاهدة ما يحدث عند إدخال صورة عشوائية إذا كان النظام ذكيًا ، فقد تتوقع أن يشعر إما بعدم اليقين أو عدم تنشيط أي من تلك الخلايا العصبية الخرجية 10 أو تفعيلها كلها بالتساوي لكن بدلا من ذلك يعطيك بعض الجواب بكل معنى الكلمة وكأنه يشعر أنه متأكد من أن هذا الضجيج العشوائي هو 5 كما يفعل",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "وبصياغة مختلفة، حتى لو كانت هذه الشبكة قادرة على التعرف على الأرقام بشكل جيد، فليس لديها أي فكرة عن كيفية رسمها.",
  "model": "google_nmt",
  "from_community_srt": "صورة من 5 هي 5 العبارة بشكل مختلف حتى لو كانت هذه الشبكة يمكن التعرف على أرقام بشكل جيد ليس لديها فكرة كيفية استخلاصها",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "يرجع الكثير من هذا إلى أنه إعداد تدريب مقيد بشدة.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "أعني، ضع نفسك مكان الشبكة هنا.",
  "model": "google_nmt",
  "from_community_srt": "الكثير من هذا هو لأنه إعداد التدريب ضيق للغاية أعني وضع نفسك في حذاء الشبكة هنا من وجهة نظرها الكون كله يتكون من لا شيء",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "ومن وجهة نظره، فإن الكون بأكمله لا يتكون من شيء سوى أرقام ثابتة محددة بوضوح ومتمركزة في شبكة صغيرة، ولم تمنحه دالة التكلفة أي حافز أبدًا ليكون واثقًا تمامًا في قراراته.",
  "model": "google_nmt",
  "from_community_srt": "لكن الأرقام التي لم يتم تحديدها بشكل واضح تركزت في شبكة صغيرة ولم تعمل وظيفة التكلفة الخاصة بها على الإطلاق",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "إذن مع هذه الصورة لما تفعله بالفعل الخلايا العصبية في الطبقة الثانية، قد تتساءل لماذا أقدم هذه الشبكة بدافع التقاط الحواف والأنماط.",
  "model": "google_nmt",
  "from_community_srt": "حافز ليكون أي شيء ، ولكن ثقة كاملة في قراراتها إذا كانت هذه هي صورة ما تفعله تلك العصبونات من الطبقة الثانية قد تتساءل عن سبب تقديم هذه الشبكة بدافع الالتقاط على الحواف والأنماط أعني ، هذا ليس على الإطلاق ما ينتهي به الأمر",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "أعني أن هذا ليس ما ينتهي به الأمر على الإطلاق.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "حسنًا، ليس المقصود من هذا أن يكون هدفنا النهائي، بل نقطة البداية.",
  "model": "google_nmt",
  "from_community_srt": "حسنا ، هذا لا يعني أن يكون هدفنا النهائي ، ولكن بدلا من نقطة البداية بصراحة هذه هي التكنولوجيا القديمة",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "بصراحة، هذه تقنية قديمة، من النوع الذي تم بحثه في الثمانينيات والتسعينيات، وتحتاج إلى فهمها قبل أن تتمكن من فهم المتغيرات الحديثة الأكثر تفصيلاً، ومن الواضح أنها قادرة على حل بعض المشكلات المثيرة للاهتمام، ولكن كلما بحثت أكثر في ما ما تفعله تلك الطبقات المخفية حقًا هو أنها تبدو أقل ذكاءً.",
  "model": "google_nmt",
  "from_community_srt": "النوع الذي بحث في 80s و 90 s و تحتاج إلى فهمه قبل أن تتمكن من فهم المتغيرات الحديثة الأكثر تفصيلاً ومن الواضح أنها قادرة على حل بعض المشاكل المثيرة للاهتمام",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "تحويل التركيز للحظة من كيفية تعلم الشبكات إلى كيفية تعلمك، لن يحدث ذلك إلا إذا انخرطت بنشاط في المادة هنا بطريقة أو بأخرى.",
  "model": "google_nmt",
  "from_community_srt": "ولكن كلما تحققت أكثر في ما تفعله هذه الطبقات المخفية أقل ذكاءً تحويل التركيز للحظة من كيفية تعلم الشبكات لكيفية تعلّمك",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "أحد الأشياء البسيطة جدًا التي أريدك أن تفعلها هو التوقف الآن والتفكير بعمق للحظة حول التغييرات التي قد تجريها على هذا النظام وكيف يتصور الصور إذا أردت أن يلتقط أشياء مثل الحواف والأنماط بشكل أفضل.",
  "model": "google_nmt",
  "from_community_srt": "لن يحدث ذلك إلا إذا شاركت بنشاط مع المادة هنا بطريقة أو بأخرى شيء واحد بسيط جدا أريدك أن تفعله هو مجرد التوقف الآن والتفكير بعمق للحظة حول ماذا التغييرات التي قد تجريها على هذا النظام وكيف يدرك الصور إذا كنت تريد أن تلتقط أشياء أفضل مثل الحواف والأنماط؟",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "ولكن الأفضل من ذلك، للتفاعل فعليًا مع المادة، أوصي بشدة بكتاب مايكل نيلسن حول التعلم العميق والشبكات العصبية.",
  "model": "google_nmt",
  "from_community_srt": "لكن أفضل من ذلك أن تتفاعل مع المادة فعليًا أنا نوصي بشدة الكتاب مايكل نيلسن على التعلم العميق والشبكات العصبية",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "يمكنك العثور فيه على الكود والبيانات التي يمكنك تنزيلها واللعب بها لهذا المثال بالتحديد، وسيرشدك الكتاب خطوة بخطوة إلى ما يفعله هذا الكود.",
  "model": "google_nmt",
  "from_community_srt": "في ذلك يمكنك العثور على الكود والبيانات للتنزيل واللعب مع هذا المثال الدقيق وسيرشدك الكتاب خطوة بخطوة ما يفعله هذا الرمز",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "الأمر الرائع هو أن هذا الكتاب مجاني ومتاح للعامة، لذا إذا حصلت على شيء منه، فكر في الانضمام إلي في التبرع لصالح جهود Nielsen.",
  "model": "google_nmt",
  "from_community_srt": "ما هو رائع أن هذا الكتاب مجاني ومتوفر للجمهور لذا إذا كنت تفكر في شيء ما ، ففكر في الانضمام إليّ في تقديم تبرع لجهود نيلسن",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "لقد قمت أيضًا بربط بعض الموارد الأخرى التي أعجبتني كثيرًا في الوصف، بما في ذلك مشاركة المدونة الرائعة والجميلة التي كتبها كريس أولا والمقالات الموجودة في Distill.",
  "model": "google_nmt",
  "from_community_srt": "لقد قمت أيضًا بربط بعض الموارد الأخرى التي أحبها كثيرًا في الوصف بما في ذلك هاجم بلوق وظيفة رائعة وجميلة من قبل كريس علا والمقالات في التقطير",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "لإغلاق الأمور هنا خلال الدقائق القليلة الماضية، أريد العودة إلى مقتطف من المقابلة التي أجريتها مع ليشا لي.",
  "model": "google_nmt",
  "from_community_srt": "لإغلاق الأشياء هنا في الدقائق القليلة الماضية أريد العودة إلى مقتطف من المقابلة التي أجريتها مع ليشا لي",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "ربما تتذكرها من الفيديو الأخير، حيث أنها حصلت على درجة الدكتوراه في التعلم العميق.",
  "model": "google_nmt",
  "from_community_srt": "قد تتذكرها من الفيديو الأخير.",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "تتحدث في هذا المقتطف الصغير عن ورقتين بحثيتين حديثتين تبحثان حقًا في كيفية التعلم الفعلي لبعض شبكات التعرف على الصور الأكثر حداثة.",
  "model": "google_nmt",
  "from_community_srt": "لقد قامت بعمل الدكتوراه في التعلم العميق وفي هذا المقتطف الصغير وهي تتحدث عن ورقتين حديثتين حقًا يكتشفان كيف تتعلم بعض شبكات التعرف على الصور الحديثة",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "فقط لتحديد ما وصلنا إليه في المحادثة، تناولت الورقة الأولى واحدة من هذه الشبكات العصبية العميقة بشكل خاص والتي تعتبر جيدة حقًا في التعرف على الصور، وبدلاً من تدريبها على مجموعة بيانات مصنفة بشكل صحيح، قامت بخلط جميع التصنيفات قبل التدريب.",
  "model": "google_nmt",
  "from_community_srt": "فقط لإعداد حيث كنا في المحادثة ، أخذت الورقة الأولى واحدة من هذه الشبكات العصبية العميقة بشكل خاص هذا جيد حقًا في التعرف على الصور وبدلاً من تدريبها على بيانات مصنفة بشكل صحيح اضبطه على جميع التصنيفات قبل التدريب",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was going to be no better than random, since everything's just randomly labeled. But it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "من الواضح أن دقة الاختبار هنا لم تكن أفضل من العشوائية، نظرًا لأن كل شيء تم تصنيفه بشكل عشوائي، لكنه كان لا يزال قادرًا على تحقيق نفس دقة التدريب كما تفعل في مجموعة بيانات مصنفة بشكل صحيح.",
  "model": "google_nmt",
  "from_community_srt": "من الواضح أن دقة الاختبار هنا لن تكون أفضل من العشوائية حيث أن كل شيء مكتوب بشكل عشوائي ولكنها كانت لا تزال قادرة على تحقيق نفس دقة التدريب كما تفعل في مجموعة بيانات مصنفة بشكل صحيح",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "في الأساس، كانت ملايين الأوزان لهذه الشبكة بالذات كافية لحفظ البيانات العشوائية فقط، مما يثير السؤال حول ما إذا كان تقليل دالة التكلفة هذه يتوافق بالفعل مع أي نوع من البنية في الصورة، أم أنه مجرد حفظ؟",
  "model": "google_nmt",
  "from_community_srt": "بشكل أساسي ، كانت ملايين الأوزان لهذه الشبكة بعينها كافية لحفظ البيانات العشوائية ما هو نوع السؤال الذي يطرح نفسه حول ما إذا كان تقليل دالة التكلفة هذه يتوافق فعليًا مع أي نوع من البنية في الصورة؟",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "...to memorize the entire dataset of what the correct classification is. And so a couple of, you know, half a year later at ICML this year, there was not exactly rebuttal paper, but paper that addressed some aspects of like, hey, actually these networks are doing something a little bit smarter than that. If you look at that accuracy c",
  "translatedText": "إذا نظرت إلى منحنى الدقة هذا، إذا كنت تتدرب فقط على مجموعة بيانات عشوائية، فقد انخفض هذا المنحنى ببطء شديد بطريقة خطية تقريبًا، لذا فأنت تكافح حقًا للعثور على الحد الأدنى المحلي الممكن، كما تعلم ، الأوزان المناسبة التي من شأنها أن تمنحك تلك الدقة.",
  "model": "google_nmt",
  "from_community_srt": "أم هل أنت فقط تعرف؟ احفظ كامل مجموعة البيانات الخاصة بالتصنيف الصحيح ، ومن ثم نعرف بعضًا منكم بعد مرور نصف عام على ICML هذا العام لم يكن هناك بالضبط ورقة ورقة الطعن التي عالجت بعض طلب مثل يا في الواقع ، تقوم هذه الشبكات بعمل شيء أكثر ذكاءً من ذلك إذا نظرت إلى منحنى الدقة هذا إذا كنت مجرد تدريب على مجموعة بيانات عشوائية منحنى نوع من انحدار جدا جدا تعرف ببطء شديد تقريبا في شكل خطي لذلك أنت حقا تكافح من أجل العثور على الحد الأدنى المحلي ممكن أنت تعرف الأوزان الصحيحة التي من شأنها أن تحصل على هذه الدقة في حين إذا كنت في الواقع التدريب على مجموعة البيانات المنظمة التي لديها",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "بينما إذا كنت تتدرب فعليًا على مجموعة بيانات منظمة، مجموعة تحتوي على التصنيفات الصحيحة، فإنك تعبث قليلاً في البداية، ولكن بعد ذلك تنخفض بسرعة كبيرة للوصول إلى مستوى الدقة هذا، وهكذا إلى حد ما كان من الأسهل العثور على الحد الأقصى المحلي.",
  "model": "google_nmt",
  "from_community_srt": "التسميات المناسبة. أنت تعلم أنك تعزف قليلاً في البداية ، لكنك تراجعت بسرعة كبيرة للوصول إلى ذلك مستوى الدقة ، ومن هذا المنطلق ، كان من الأسهل العثور عليه",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "ولذا فإن ما كان مثيرًا للاهتمام أيضًا في ذلك هو أنه يسلط الضوء على بحث آخر منذ بضع سنوات مضت، والذي يحتوي على الكثير من التبسيطات حول طبقات الشبكة، ولكن إحدى النتائج كانت تقول كيف إذا نظرت إلى مشهد التحسين، الحد الأدنى المحلي الذي تميل هذه الشبكات إلى تعلمه هو في الواقع متساوٍ في الجودة، لذا، إلى حد ما، إذا كانت مجموعة البيانات الخاصة بك منظمة، فيجب أن تكون قادرًا على العثور على ذلك بسهولة أكبر.",
  "model": "google_nmt",
  "from_community_srt": "فالحد الأقصى المحلي ، وكذلك الأمر المثير للاهتمام أيضًا ، هو أنه تم التقاطه في ضوء ورقة أخرى من الواقع قبل عامين الذي لديه الكثير تبسيط حول طبقات الشبكة ولكن إحدى النتائج كانت تقول كيف إذا نظرت إلى مشهد التحسين ، فإن الحدود المحلية التي تميل هذه الشبكات إلى تعلمها هي في الواقع ، من نوعية متساوية ، بمعنى ما إذا كانت مجموعة البيانات الخاصة بك هي بنية ، ويجب أن تكون قادرًا على العثور عليها بسهولة أكبر",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "شكري، كما هو الحال دائمًا، لأولئك الذين يدعمونكم على Patreon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "لقد قلت من قبل ما الذي سيغيره Patreon من قواعد اللعبة، لكن مقاطع الفيديو هذه لن تكون ممكنة بدونك.",
  "model": "google_nmt",
  "from_community_srt": "شكري كما هو الحال دائما لمن دعمكم على patreon لقد قلت من قبل فقط ما هو patreon المغير لعبة ولكن هذه الفيديوهات حقا لن يكون من الممكن بدونك أنا",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners and their support of these initial videos in the series. Thank you.",
  "translatedText": "أريد أيضًا أن أتقدم بشكر خاص لشركة VC Amplify Partners، لدعمها لمقاطع الفيديو الأولية هذه في السلسلة.",
  "model": "google_nmt",
  "from_community_srt": "تريد أيضا أن تعطي خاص. شكرًا لشركاء VC firmifi في دعمهم لمقاطع الفيديو الأولية هذه في السلسلة",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]