1
00:00:04,070 --> 00:00:07,059
آخر فيديو وضعت هيكل الشبكة العصبية

2
00:00:07,160 --> 00:00:10,089
سأعطي ملخصًا سريعًا هنا فقط لكي يكون جديدًا في أذهاننا

3
00:00:10,089 --> 00:00:15,368
ثم لدي هدفين رئيسيين لهذا الفيديو. الأول هو تقديم فكرة الانحدار التدرج ،

4
00:00:15,650 --> 00:00:18,219
الذي يكمن وراء ليس فقط كيفية تعلم الشبكات العصبية ،

5
00:00:18,220 --> 00:00:20,439
ولكن كيف يعمل الكثير من التعلم الآلي أيضًا

6
00:00:20,660 --> 00:00:24,609
ثم بعد ذلك سوف نحفر أكثر قليلاً حول كيفية أداء هذه الشبكة بالتحديد

7
00:00:24,609 --> 00:00:27,758
وماذا هذه الطبقات الخفية من الخلايا العصبية ينتهي في الواقع تبحث عنه

8
00:00:28,999 --> 00:00:33,489
كتذكير هدفنا هنا هو المثال الكلاسيكي للتعرف على الأرقام المكتوبة بخط اليد

9
00:00:34,129 --> 00:00:36,129
مرحبا بعالم الشبكات العصبية

10
00:00:36,500 --> 00:00:43,090
يتم تقديم هذه الأرقام على شبكة 28 × 28 بكسل لكل بكسل مع بعض القيمة الرمادية بين 0 و 1

11
00:00:43,610 --> 00:00:46,089
تلك هي التي تحدد عمليات التنشيط

12
00:00:46,850 --> 00:00:50,199
784 من الخلايا العصبية في طبقة المدخلات من الشبكة و

13
00:00:50,840 --> 00:00:55,719
ثم يقوم التنشيط لكل عصبون في الطبقات التالية على أساس مجموع وزن

14
00:00:56,000 --> 00:01:00,639
جميع عمليات التنشيط في الطبقة السابقة بالإضافة إلى بعض الأرقام الخاصة تسمى التحيز

15
00:01:01,699 --> 00:01:06,338
ثم تقوم بتكوين هذا المبلغ مع بعض الوظائف الأخرى مثل sishmoid squishification أو

16
00:01:06,400 --> 00:01:08,769
لعبه بالطريقة التي مشيت من خلال الفيديو الماضي

17
00:01:09,110 --> 00:01:15,729
في المجموع ، بالنظر إلى الاختيار التعسفي نوعًا ما لطبقتين مخفيتين هنا مع 16 خلية عصبية لكل منها الشبكة

18
00:01:16,579 --> 00:01:24,159
13000 من الأوزان والتحيزات التي يمكننا ضبطها وهذه القيم هي التي تحدد بالضبط ما تعرفه الشبكة بالفعل

19
00:01:24,799 --> 00:01:28,328
ثم ما نعنيه عندما نقول أن هذه الشبكة تصنف رقمًا معينًا

20
00:01:28,329 --> 00:01:33,429
هل هذا ألمع من تلك الخلايا العصبية العشرة في الطبقة النهائية يتوافق مع هذا الرقم

21
00:01:33,950 --> 00:01:38,589
وتذكر أن الدافع الذي كان يدور في ذهننا هنا حول بنية الطبقات هو ربما

22
00:01:38,780 --> 00:01:44,680
يمكن أن تلتقط الطبقة الثانية على الحواف وقد تلتقط الطبقة الثالثة أنماطًا مثل الحلقات والخطوط

23
00:01:44,930 --> 00:01:48,729
ويمكن للمرء الأخير فقط تجميع تلك الأنماط للتعرف على الأرقام

24
00:01:49,369 --> 00:01:52,029
إذن هنا نتعلم كيف تتعلم الشبكة

25
00:01:52,399 --> 00:01:57,099
ما نريده هو خوارزمية حيث يمكنك إظهار هذه الشبكة مجموعة كاملة من بيانات التدريب

26
00:01:57,229 --> 00:02:03,669
الذي يأتي على شكل مجموعة من الصور المختلفة للأرقام المكتوبة بخط اليد مع ملصقات لما يفترض أن يكون

27
00:02:03,890 --> 00:02:05,659
انها سوف ضبط تلك

28
00:02:05,659 --> 00:02:09,789
13000 أوزان وانحياز لتحسين أدائه في بيانات التدريب

29
00:02:10,730 --> 00:02:13,569
نأمل أن يعني هذا الهيكل الطبقات ما يتعلم

30
00:02:14,269 --> 00:02:16,719
يعمم على الصور بعد تلك البيانات التدريبية

31
00:02:16,720 --> 00:02:20,289
والطريقة التي نختبرها هي أنه بعد تدريب الشبكة

32
00:02:20,290 --> 00:02:26,560
كنت تظهر أكثر ثيتا المسمى أنه لم يسبق له مثيل من قبل ، وترى مدى دقة تصنيف تلك الصور الجديدة

33
00:02:31,040 --> 00:02:37,000
ولحسن الحظ بالنسبة لنا وما يجعل هذا المثال المشترك هو أن الأشخاص الجيدين وراء قاعدة MNIST لديهم

34
00:02:37,000 --> 00:02:44,289
تجميع مجموعة من عشرات الآلاف من صور الأرقام المكتوبة بخط اليد كل واحد منهم المسمى بالأرقام التي من المفترض أن تكون

35
00:02:44,720 --> 00:02:49,539
إنه أمر مثير للاستفزاز لأنه لوصف آلة ما بأنها تعلم بمجرد أن ترى كيف تعمل

36
00:02:49,540 --> 00:02:55,359
إنه شعور أقل بكثير مثل فرضية الخيال العلمي المجنونة وأكثر بكثير مثل ممارسة حساب التفاضل والتكامل

37
00:02:55,390 --> 00:02:59,589
أعني أساسًا ، هو العثور على الحد الأدنى من وظيفة معينة

38
00:03:01,519 --> 00:03:05,199
تذكر من الناحية النظرية أننا نفكر في كل خلية عصبية بأنها متصلة

39
00:03:05,390 --> 00:03:12,309
لجميع الخلايا العصبية في الطبقة السابقة ، والأوزان في المجموع المرجح تحديد التنشيط لها هي مثل

40
00:03:12,440 --> 00:03:14,060
نقاط القوة في تلك الاتصالات

41
00:03:14,060 --> 00:03:20,440
والتحيز هو مؤشر على ما إذا كان ذلك العصبون يميل إلى أن يكون نشطًا أو غير نشط ويبدأ الأمور

42
00:03:20,440 --> 00:03:26,919
سنقوم فقط بتهيئة كل تلك الأوزان والتحيزات بشكل عشوائي بدون داع لتقول أن هذه الشبكة ستؤديها

43
00:03:26,919 --> 00:03:33,759
رهيبة جدا على مثال تدريب معين لأنه مجرد القيام بشيء عشوائي على سبيل المثال تقوم بتغذية هذه الصورة من 3 و

44
00:03:33,760 --> 00:03:35,799
طبقة الإخراج يبدو فقط مثل الفوضى

45
00:03:36,349 --> 00:03:42,518
إذن ما تفعله هو تحديد وظيفة التكلفة كطريقة لإخبار الكمبيوتر: "لا يوجد كمبيوتر سيء!

46
00:03:42,739 --> 00:03:50,529
يجب أن يكون لهذا الناتج عمليات تنشيط تكون صفراً لمعظم العصبونات ، لكن واحد لهذا العصبون هو ما أعطيتني هو القمامة المطلقة "

47
00:03:51,260 --> 00:03:56,530
لقول ما هو أكثر من ذلك بقليل رياضيا ما تفعله هو إضافة مربعات الاختلافات بين

48
00:03:56,720 --> 00:04:01,419
كل من هذه التنبيهات إخراج القمامة والقيمة التي تريدها أن يكون لها و

49
00:04:01,489 --> 00:04:04,599
هذا هو ما سنسميه تكلفة مثال تدريب واحد

50
00:04:05,599 --> 00:04:10,749
لاحظ أن هذا المبلغ صغير عندما تقوم الشبكة بتصنيف الصورة بشكل صحيح

51
00:04:12,199 --> 00:04:15,639
ولكنها كبيرة عندما تبدو الشبكة وكأنها لا تعرف حقًا ما تفعله

52
00:04:18,330 --> 00:04:25,249
إذن ، ما عليك فعله هو النظر في متوسط ​​التكلفة على عشرات الآلاف من الأمثلة التدريبية المتاحة لك

53
00:04:27,060 --> 00:04:34,310
هذه التكلفة المتوسطة هي مقياسنا لمدى روعة الشبكة ومدى الضرر الذي يشعر به الكمبيوتر ، وهذا أمر معقد

54
00:04:34,830 --> 00:04:38,960
تذكر كيف كانت الشبكة نفسها في الأساس وظيفة تستلزمها

55
00:04:39,540 --> 00:04:45,890
أرقام 784 كمدخلات قيم البكسل وتبصق بها عشرة أرقام كمخرج لها وبمعنى ما

56
00:04:45,890 --> 00:04:48,770
إنها معلمة بكل هذه الأوزان والتحيزات

57
00:04:49,140 --> 00:04:54,020
في حين أن وظيفة التكلفة هي طبقة من التعقيد فوق ذلك تأخذها كمدخل لها

58
00:04:54,450 --> 00:05:02,059
تلك ثلاثة عشر ألف أو ما يقاربها من الأوزان والتحيزات وتبث رقمًا واحدًا يصف مدى سوء تلك الأوزان والتحيزات

59
00:05:02,340 --> 00:05:08,749
وتعتمد الطريقة التي يتم تحديدها على سلوك الشبكة على جميع عشرات الآلاف من بيانات التدريب

60
00:05:09,150 --> 00:05:11,150
هذا الكثير للتفكير

61
00:05:12,000 --> 00:05:15,619
ولكن مجرد إخبار الكمبيوتر عن الوظيفة المزعجة ، فهو ليس مفيدًا للغاية

62
00:05:15,900 --> 00:05:19,819
هل تريد أن تعرف كيف تغير هذه الأوزان والتحيزات حتى تتحسن؟

63
00:05:20,820 --> 00:05:25,129
لجعل الأمر أسهل بدلاً من تكافح تخيل وظيفة مع 13000 مدخلات

64
00:05:25,130 --> 00:05:30,409
فقط تخيل وظيفة بسيطة تحتوي على رقم واحد كمدخل ورقم واحد كمخرج

65
00:05:30,960 --> 00:05:34,999
كيف يمكنك العثور على مدخلات تقلل من قيمة هذه الوظيفة؟

66
00:05:36,270 --> 00:05:40,039
سيعلم الطلاب في حساب التفاضل والتكامل أنه يمكنك أحيانًا معرفة ذلك الحد الأدنى بشكل صريح

67
00:05:40,260 --> 00:05:43,879
ولكن هذا ليس ممكنا دائما لوظائف معقدة حقا

68
00:05:44,310 --> 00:05:52,160
بالتأكيد ليس في النسخة ثلاثة عشر ألف مدخلات من هذا الوضع لدينا وظيفة تكلفة الشبكة العصبية معقدة مجنون

69
00:05:52,350 --> 00:05:59,029
التكتيك الأكثر مرونة هو أن تبدأ من أي مدخلات قديمة وتعرف على الاتجاه الذي يجب أن تتخذه لجعل هذا الإنتاج أقل

70
00:06:00,120 --> 00:06:03,710
على وجه التحديد إذا كنت تستطيع معرفة منحدر الوظيفة التي أنت فيها

71
00:06:04,020 --> 00:06:09,619
ثم انتقل إلى اليسار إذا كان هذا المنحدر موجبًا وقم بتحويل الإدخال إلى اليمين إذا كان هذا المنحدر سلبيًا

72
00:06:12,130 --> 00:06:16,799
إذا قمت بذلك مرارًا وتكرارًا في كل نقطة فحص المنحدر الجديد واتخاذ الخطوة المناسبة

73
00:06:16,800 --> 00:06:20,039
كنت ستعمل الاقتراب من الحد الأدنى المحلي للوظيفة و

74
00:06:20,280 --> 00:06:24,080
الصورة التي قد تدور في ذهنك هنا هي الكرة المتدحرجة أسفل التل و

75
00:06:24,400 --> 00:06:30,900
لاحظ حتى بالنسبة إلى وظيفة الإدخال المفرد هذه المبسطة حقًا ، هناك العديد من الوديان المحتملة التي قد تصل إليها

76
00:06:31,540 --> 00:06:36,220
اعتمادا على أي إدخال عشوائي تبدأ في وليس هناك ما يضمن أن الحد الأدنى المحلي

77
00:06:36,580 --> 00:06:39,040
سوف تهبط ستكون أصغر قيمة ممكنة لوظيفة التكلفة

78
00:06:39,610 --> 00:06:44,009
هذا سوف ينتقل إلى حالة الشبكة العصبية أيضًا ، وأريد أيضًا أن تلاحظ ذلك

79
00:06:44,010 --> 00:06:47,190
كيف إذا كنت تجعل أحجام الخطوة الخاصة بك متناسبة مع المنحدر

80
00:06:47,620 --> 00:06:54,540
ثم عندما يميل المنحدر نحو الحد الأدنى ، تصبح خطواتك أصغر وأصغر وهذا النوع يساعدك على تجاوز الحدود

81
00:06:55,720 --> 00:07:00,449
إن الارتقاء بالتعقيد قد يتخيل بدلاً من ذلك دالة ذات مدخلين ومخرج واحد

82
00:07:01,120 --> 00:07:07,739
قد تفكر في مساحة الإدخال كمستوى XY ووظيفة التكلفة على أنها رسوم بيانية على هيئة سطح فوقها

83
00:07:08,230 --> 00:07:15,060
الآن بدلاً من السؤال عن منحدر الدالة ، عليك أن تسأل عن أي اتجاه يجب أن تخطو في مساحة الإدخال هذه؟

84
00:07:15,310 --> 00:07:22,440
وذلك لتقليل انتاج الوظيفة بسرعة أكبر وبعبارة أخرى. ما هو اتجاه الإنحدار؟

85
00:07:22,440 --> 00:07:25,379
ومرة أخرى من المفيد أن نفكر في كرة تتدحرج إلى أسفل هذا التل

86
00:07:26,260 --> 00:07:34,080
سيعرف أولئك الذين على دراية بحساب التفاضل والتكامل متعدد المتغيرات أن التدرج في وظيفة يمنحك اتجاه صعود حاد

87
00:07:34,750 --> 00:07:38,459
بشكل أساسي ، أي اتجاه يجب أن تخطوه لزيادة الوظيفة بسرعة أكبر

88
00:07:39,100 --> 00:07:46,439
بطبيعة الحال ما يكفي من اتخاذ السلبية من هذا الانحدار يعطيك الاتجاه إلى الخطوة التي تقلل من وظيفة بسرعة أكبر و

89
00:07:47,020 --> 00:07:53,400
أكثر من ذلك أن طول هذا الناقل المتدرج هو في الواقع مؤشر على مدى الانحدار الحاد لهذا المنحدر

90
00:07:54,130 --> 00:07:56,280
الآن إذا كنت غير معتاد على حساب التفاضل والتكامل متعدد المتغيرات

91
00:07:56,280 --> 00:08:00,239
وتريد أن تتعرف أكثر على بعض الأعمال التي قمت بها في أكاديمية خان حول هذا الموضوع

92
00:08:00,910 --> 00:08:03,779
بصراحة ، على الرغم من كل ما يهم بالنسبة لي وأنا الآن

93
00:08:03,780 --> 00:08:09,419
هل هذا من حيث المبدأ هناك طريقة لحساب هذا الناقل. هذا المتجه الذي يخبرك ما

94
00:08:09,520 --> 00:08:15,900
اتجاه انحدار هو وكيف حاد هو أنك ستكون على ما يرام إذا كان هذا هو كل ما تعرفه وأنت لست صخرة الصلبة على التفاصيل

95
00:08:16,790 --> 00:08:24,580
لأنه إذا كان يمكنك الحصول على أن الخوارزمية من التقليل من الوظيفة هي حساب هذا الاتجاه التدرج ثم اتخاذ خطوة صغيرة إلى أسفل و

96
00:08:24,740 --> 00:08:26,740
فقط أكرر ذلك مرارا وتكرارا

97
00:08:27,800 --> 00:08:34,600
إنها نفس الفكرة الأساسية لوظيفة لديها 13000 مدخلات بدلاً من مدخلين تخيل تنظيم الكل

98
00:08:35,330 --> 00:08:39,400
13000 أوزان وانحياز شبكتنا إلى ناقل عمودية عملاقة

99
00:08:39,680 --> 00:08:43,870
التدرج السلبي لوظيفة التكلفة هو مجرد متجه

100
00:08:43,880 --> 00:08:49,299
انها بعض الاتجاه داخل هذا الفضاء المدخلات الضخمة بجنون تخبرك

101
00:08:49,400 --> 00:08:55,030
دفعات لجميع هذه الأرقام سيؤدي إلى انخفاض أسرع إلى وظيفة التكلفة و

102
00:08:55,460 --> 00:08:58,150
بالطبع مع وظيفة التكلفة المصممة خصيصا لدينا

103
00:08:58,580 --> 00:09:04,900
تغيير الأوزان والتحيزات لتقليل ذلك يعني جعل ناتج الشبكة على كل جزء من بيانات التدريب

104
00:09:05,180 --> 00:09:10,599
تبدو أقل مثل مجموعة عشوائية من عشر قيم وأكثر مثل القرار الفعلي الذي نريده

105
00:09:11,030 --> 00:09:16,030
من المهم أن تتذكر أن وظيفة التكلفة هذه تشتمل على متوسط ​​لكل بيانات التدريب

106
00:09:16,370 --> 00:09:20,590
لذا إذا قمت بتصغيره ، فهذا يعني أنه أداء أفضل على جميع تلك العينات

107
00:09:23,780 --> 00:09:30,849
إن خوارزمية حوسبة هذا التدرج بكفاءة والتي هي في الواقع قلب كيفية معرفة الشبكة العصبية تسمى التكرار

108
00:09:31,190 --> 00:09:34,690
وهذا ما سأتحدث عنه بشأن الفيديو التالي

109
00:09:34,690 --> 00:09:36,690
هناك أنا حقا أريد أن تأخذ من الوقت للمشي من خلال

110
00:09:36,830 --> 00:09:41,439
ما الذي يحدث بالضبط لكل وزن وكل تحيز لجزء معين من بيانات التدريب؟

111
00:09:41,810 --> 00:09:46,960
في محاولة لإعطاء إحساس بديهية لما يحدث خارج كومة حساب التفاضل والتكامل والصيغ ذات الصلة

112
00:09:47,510 --> 00:09:52,179
هنا الآن الشيء الرئيسي. أريدك أن تعرف مستقلًا عن تفاصيل التنفيذ

113
00:09:52,180 --> 00:09:58,479
هو أن ما نعنيه عندما نتحدث عن تعلم الشبكة هو أنه يقلل من وظيفة التكلفة و

114
00:09:58,940 --> 00:10:04,479
لاحظ إحدى نتائج ذلك أنه من المهم أن تكون وظيفة التكلفة هذه ذات ناتج سلس جميل

115
00:10:04,480 --> 00:10:07,810
حتى نتمكن من العثور على الحد الأدنى المحلي عن طريق اتخاذ خطوات قليلة إلى أسفل

116
00:10:08,810 --> 00:10:10,520
هذا هو السبب في ذلك

117
00:10:10,520 --> 00:10:16,749
تقوم العصبونات الاصطناعية بشكل مستمر بتدفق عمليات التنشيط بدلاً من كونها نشطة أو غير نشطة بطريقة ثنائية

118
00:10:16,750 --> 00:10:18,750
إذا كانت الطريقة التي تكون بها الخلايا العصبية البيولوجية

119
00:10:19,940 --> 00:10:26,770
ويطلق على هذه العملية من تكرار إدخال إحدى الدوال من قبل بعض مضاعفات التدرج السلبي تدعى تنازلي التدرج

120
00:10:26,930 --> 00:10:32,380
إنها طريقة للتوافق مع بعض الحد الأدنى المحلي لوظيفة التكلفة أساسًا في هذا الرسم البياني

121
00:10:32,930 --> 00:10:38,890
لا أزال أقوم بعرض صورة دالة مع اثنين من المدخلات بالطبع لأن الإغراءات في ثلاثة عشر ألف مدخلات الأبعاد

122
00:10:38,890 --> 00:10:44,049
الفضاء صعب قليلاً لفك عقلك ، لكن هناك في الواقع طريقة غير مكانية لطيفة للتفكير في هذا

123
00:10:44,630 --> 00:10:51,340
يخبرنا كل عنصر من عناصر التدرج السلبي عن أمرين ، تخبرنا الإشارة بالطبع عما إذا كانت المقابلة

124
00:10:51,830 --> 00:10:59,139
يجب أن يتم دفع عنصر ناقل الدخل لأعلى أو لأسفل ، ولكن الأهم من ذلك هو المقاييس النسبية لكل هذه المكونات

125
00:10:59,840 --> 00:11:02,530
نوع من يخبرك التغييرات التي تهم أكثر

126
00:11:05,150 --> 00:11:09,340
كما ترى في شبكتنا ، قد يكون التعديل على أحد الأوزان أكبر بكثير

127
00:11:09,710 --> 00:11:12,939
تأثير على وظيفة التكلفة من التكيف مع بعض الوزن الآخر

128
00:11:14,450 --> 00:11:17,950
بعض هذه الاتصالات لها أهمية أكبر لبيانات التدريب الخاصة بنا

129
00:11:18,920 --> 00:11:22,690
لذا ، يمكنك التفكير في هذا المتجه المتدرج لعقلنا

130
00:11:22,690 --> 00:11:27,999
وظيفة التكلفة الهائلة هي أنه يشفر الأهمية النسبية لكل الوزن والتحيز

131
00:11:28,250 --> 00:11:32,200
هذا هو أي من هذه التغييرات سوف تحمل أكثر ضجة لباك الخاصة بك

132
00:11:33,560 --> 00:11:36,460
هذا حقا هو مجرد طريقة أخرى للتفكير في الاتجاه

133
00:11:36,860 --> 00:11:41,290
لأخذ مثال أبسط إذا كان لديك بعض الوظائف مع متغيرين كمدخل وأنت

134
00:11:41,690 --> 00:11:46,540
احسب أن التدرج في بعض النقاط يخرج كما هو (3،1)

135
00:11:47,420 --> 00:11:51,670
ثم من ناحية ، يمكنك تفسير ذلك كقولك عندما تقف عند هذا الإدخال

136
00:11:52,070 --> 00:11:55,150
التحرك على طول هذا الاتجاه يزيد من الوظيفة بسرعة أكبر

137
00:11:55,460 --> 00:12:02,229
عندما تقوم بعمل رسم بياني للدالة أعلى مستوى نقاط الإدخال التي يتم توجيهها ، فهذا هو ما يعطيك اتجاه صاعد مستقيم

138
00:12:02,600 --> 00:12:06,580
لكن هناك طريقة أخرى لقراءة ذلك وهي أن التغييرات على هذا المتغير الأول

139
00:12:06,740 --> 00:12:13,390
أن يكون لديك ثلاثة أضعاف الأهمية كتغييرات للمتغير الثاني على الأقل في الجوار للمدخلات ذات الصلة

140
00:12:13,520 --> 00:12:16,689
إن دفع قيمة x يحمل الكثير من الدوي لجهودكم

141
00:12:19,310 --> 00:12:19,930
حسنا

142
00:12:19,930 --> 00:12:24,940
لنقوم بتصغير ونلخص حيث أننا حتى الآن الشبكة نفسها هي هذه الوظيفة

143
00:12:25,400 --> 00:12:29,859
784 مدخلات و 10 مخرجات محددة من حيث كل هذه المبالغ المرجحة

144
00:12:30,350 --> 00:12:34,780
وظيفة التكلفة هي طبقة من التعقيد على رأس أنه يأخذ

145
00:12:35,120 --> 00:12:41,870
13،000 أوزان وانحياز كمدخلات وبصق خارجا تدبير واحد من lousyness إستنادا على أمثلة التدريب و

146
00:12:42,180 --> 00:12:47,930
إن التدرج في دالة التكلفة هو طبقة أخرى من التعقيد ما زالت تخبرنا

147
00:12:47,930 --> 00:12:53,839
ما يدفع إلى كل هذه الأوزان والتحيزات يسبب التغيير الأسرع في قيمة دالة التكلفة

148
00:12:53,970 --> 00:12:57,680
ما يمكنك تفسيره هو تحديد التغييرات التي تهم الأوزان أكثر من غيرها

149
00:13:02,550 --> 00:13:09,289
لذلك عند تهيئة الشبكة باستخدام الأوزان والتحيزات العشوائية وضبطها عدة مرات بناءً على عملية هبوط التدرج

150
00:13:09,420 --> 00:13:12,949
ما مدى جودة الأداء الفعلي للصور التي لم يسبق مشاهدتها من قبل؟

151
00:13:13,680 --> 00:13:19,609
حسناً تلك التي وصفتها هنا مع الطبقتين المختبئين من 16 خلية عصبية تم اختيار كل منها لأسباب جمالية

152
00:13:20,579 --> 00:13:26,089
حسنا ، انها ليست سيئة يصنف حوالي 96 في المئة من الصور الجديدة التي يراها بشكل صحيح و

153
00:13:26,759 --> 00:13:32,239
بصراحة ، إذا نظرت إلى بعض الأمثلة التي تعتصر عليك نوعًا ما تشعر بأنها مضطرة إلى قطعها قليلاً من الركود

154
00:13:35,759 --> 00:13:39,079
الآن إذا كنت تلعب مع بنية الطبقة الخفية وجعل اثنين من القرص

155
00:13:39,079 --> 00:13:43,698
يمكنك الحصول على هذا بنسبة تصل إلى 98٪ وهذا جيد جدًا. ليس هذا هو الأفضل

156
00:13:43,740 --> 00:13:48,409
يمكنك بالتأكيد الحصول على أداء أفضل من خلال الحصول على أكثر تطوراً من شبكة الفانيلا هذه

157
00:13:48,569 --> 00:13:52,669
ولكن بالنظر إلى مدى صعوبة المهمة الأولية ، فأنا أعتقد أن هناك شيئًا ما؟

158
00:13:52,889 --> 00:13:56,929
لا يصدق أي شبكة تفعل ذلك بشكل جيد على الصور التي لم يسبق له مثيل من قبل

159
00:13:57,389 --> 00:14:00,919
وبالنظر إلى أننا لم نخبره تحديدًا عن الأنماط التي يجب البحث عنها

160
00:14:02,579 --> 00:14:07,068
في الأصل كانت الطريقة التي حفزت بها هذا الهيكل هي وصف الأمل الذي قد نتمتع به

161
00:14:07,259 --> 00:14:09,739
أن الطبقة الثانية قد تلتقط على حواف صغيرة

162
00:14:09,809 --> 00:14:17,089
أن الطبقة الثالثة ستجمع هذه الحواف لتمييز الحلقات وخطوط أطول ، وقد يتم تجميعها معًا للتعرف على الأرقام

163
00:14:17,699 --> 00:14:22,729
فهل هذا ما تفعله شبكتنا بالفعل؟ حسنا لهذا واحد على الأقل

164
00:14:23,339 --> 00:14:24,449
على الاطلاق

165
00:14:24,449 --> 00:14:27,409
تذكر كيف الفيديو الماضي نظرنا في كيفية أوزان

166
00:14:27,480 --> 00:14:31,849
اتصالات من جميع الخلايا العصبية في الطبقة الأولى إلى خلية عصبية معينة في الطبقة الثانية

167
00:14:31,980 --> 00:14:36,829
يمكن تصورها كنمط بكسل محدد يتم التقاطه في الخلية العصبية من الطبقة الثانية

168
00:14:37,350 --> 00:14:43,309
حسنا عندما نقوم بذلك فعلا للأوزان المرتبطة بهذه التحولات من الطبقة الأولى إلى التالية

169
00:14:43,709 --> 00:14:50,209
بدلا من التقاط على حواف صغيرة معزولة هنا وهناك. أنها تبدو بشكل جيد تقريبا عشوائي

170
00:14:50,370 --> 00:14:56,399
مجرد وضع بعض أنماط فضفاضة جدا في الوسط هناك يبدو أنه في كبير لا يسبر غوره

171
00:14:56,920 --> 00:15:02,580
13،000 مساحة الأبعاد من الأوزان الممكنة والتحيزات وجدت شبكتنا نفسها الدنيا المحلية الصغيرة السعيدة ذلك

172
00:15:02,860 --> 00:15:08,940
على الرغم من أن تصنيف معظم الصور بنجاح لا يرقى بالضبط إلى الأنماط التي كنا نأمل في الحصول عليها و

173
00:15:09,430 --> 00:15:13,709
حقا لدفع هذه النقطة المنزل مشاهدة ما يحدث عند إدخال صورة عشوائية

174
00:15:14,019 --> 00:15:21,449
إذا كان النظام ذكيًا ، فقد تتوقع أن يشعر إما بعدم اليقين أو عدم تنشيط أي من تلك الخلايا العصبية الخرجية 10 أو

175
00:15:21,579 --> 00:15:23,200
تفعيلها كلها بالتساوي

176
00:15:23,200 --> 00:15:24,820
لكن بدلا من ذلك

177
00:15:24,820 --> 00:15:32,010
يعطيك بعض الجواب بكل معنى الكلمة وكأنه يشعر أنه متأكد من أن هذا الضجيج العشوائي هو 5 كما يفعل

178
00:15:32,010 --> 00:15:34,010
صورة من 5 هي 5

179
00:15:34,180 --> 00:15:40,829
العبارة بشكل مختلف حتى لو كانت هذه الشبكة يمكن التعرف على أرقام بشكل جيد ليس لديها فكرة كيفية استخلاصها

180
00:15:41,500 --> 00:15:45,149
الكثير من هذا هو لأنه إعداد التدريب ضيق للغاية

181
00:15:45,149 --> 00:15:51,479
أعني وضع نفسك في حذاء الشبكة هنا من وجهة نظرها الكون كله يتكون من لا شيء

182
00:15:51,480 --> 00:15:57,539
لكن الأرقام التي لم يتم تحديدها بشكل واضح تركزت في شبكة صغيرة ولم تعمل وظيفة التكلفة الخاصة بها على الإطلاق

183
00:15:57,700 --> 00:16:00,959
حافز ليكون أي شيء ، ولكن ثقة كاملة في قراراتها

184
00:16:01,690 --> 00:16:05,070
إذا كانت هذه هي صورة ما تفعله تلك العصبونات من الطبقة الثانية

185
00:16:05,140 --> 00:16:09,839
قد تتساءل عن سبب تقديم هذه الشبكة بدافع الالتقاط على الحواف والأنماط

186
00:16:09,839 --> 00:16:11,969
أعني ، هذا ليس على الإطلاق ما ينتهي به الأمر

187
00:16:13,029 --> 00:16:17,909
حسنا ، هذا لا يعني أن يكون هدفنا النهائي ، ولكن بدلا من نقطة البداية بصراحة

188
00:16:17,910 --> 00:16:19,120
هذه هي التكنولوجيا القديمة

189
00:16:19,120 --> 00:16:21,510
النوع الذي بحث في 80s و 90 s و

190
00:16:21,640 --> 00:16:29,129
تحتاج إلى فهمه قبل أن تتمكن من فهم المتغيرات الحديثة الأكثر تفصيلاً ومن الواضح أنها قادرة على حل بعض المشاكل المثيرة للاهتمام

191
00:16:29,410 --> 00:16:34,110
ولكن كلما تحققت أكثر في ما تفعله هذه الطبقات المخفية أقل ذكاءً

192
00:16:38,530 --> 00:16:42,359
تحويل التركيز للحظة من كيفية تعلم الشبكات لكيفية تعلّمك

193
00:16:42,580 --> 00:16:46,139
لن يحدث ذلك إلا إذا شاركت بنشاط مع المادة هنا بطريقة أو بأخرى

194
00:16:46,660 --> 00:16:53,100
شيء واحد بسيط جدا أريدك أن تفعله هو مجرد التوقف الآن والتفكير بعمق للحظة حول ماذا

195
00:16:53,440 --> 00:16:55,230
التغييرات التي قد تجريها على هذا النظام

196
00:16:55,230 --> 00:17:00,719
وكيف يدرك الصور إذا كنت تريد أن تلتقط أشياء أفضل مثل الحواف والأنماط؟

197
00:17:01,360 --> 00:17:04,410
لكن أفضل من ذلك أن تتفاعل مع المادة فعليًا

198
00:17:04,410 --> 00:17:05,079
أنا

199
00:17:05,079 --> 00:17:08,969
نوصي بشدة الكتاب مايكل نيلسن على التعلم العميق والشبكات العصبية

200
00:17:09,190 --> 00:17:14,369
في ذلك يمكنك العثور على الكود والبيانات للتنزيل واللعب مع هذا المثال الدقيق

201
00:17:14,410 --> 00:17:18,089
وسيرشدك الكتاب خطوة بخطوة ما يفعله هذا الرمز

202
00:17:18,910 --> 00:17:21,749
ما هو رائع أن هذا الكتاب مجاني ومتوفر للجمهور

203
00:17:22,360 --> 00:17:27,540
لذا إذا كنت تفكر في شيء ما ، ففكر في الانضمام إليّ في تقديم تبرع لجهود نيلسن

204
00:17:27,910 --> 00:17:32,219
لقد قمت أيضًا بربط بعض الموارد الأخرى التي أحبها كثيرًا في الوصف بما في ذلك

205
00:17:32,470 --> 00:17:36,390
هاجم بلوق وظيفة رائعة وجميلة من قبل كريس علا والمقالات في التقطير

206
00:17:38,230 --> 00:17:40,200
لإغلاق الأشياء هنا في الدقائق القليلة الماضية

207
00:17:40,200 --> 00:17:43,740
أريد العودة إلى مقتطف من المقابلة التي أجريتها مع ليشا لي

208
00:17:43,930 --> 00:17:49,079
قد تتذكرها من الفيديو الأخير. لقد قامت بعمل الدكتوراه في التعلم العميق وفي هذا المقتطف الصغير

209
00:17:49,080 --> 00:17:55,530
وهي تتحدث عن ورقتين حديثتين حقًا يكتشفان كيف تتعلم بعض شبكات التعرف على الصور الحديثة

210
00:17:55,810 --> 00:18:01,349
فقط لإعداد حيث كنا في المحادثة ، أخذت الورقة الأولى واحدة من هذه الشبكات العصبية العميقة بشكل خاص

211
00:18:01,350 --> 00:18:05,910
هذا جيد حقًا في التعرف على الصور وبدلاً من تدريبها على بيانات مصنفة بشكل صحيح

212
00:18:05,910 --> 00:18:08,579
اضبطه على جميع التصنيفات قبل التدريب

213
00:18:08,800 --> 00:18:14,669
من الواضح أن دقة الاختبار هنا لن تكون أفضل من العشوائية حيث أن كل شيء مكتوب بشكل عشوائي

214
00:18:14,800 --> 00:18:20,879
ولكنها كانت لا تزال قادرة على تحقيق نفس دقة التدريب كما تفعل في مجموعة بيانات مصنفة بشكل صحيح

215
00:18:21,490 --> 00:18:27,540
بشكل أساسي ، كانت ملايين الأوزان لهذه الشبكة بعينها كافية لحفظ البيانات العشوائية

216
00:18:27,820 --> 00:18:34,379
ما هو نوع السؤال الذي يطرح نفسه حول ما إذا كان تقليل دالة التكلفة هذه يتوافق فعليًا مع أي نوع من البنية في الصورة؟

217
00:18:34,380 --> 00:18:36,380
أم هل أنت فقط تعرف؟

218
00:18:36,520 --> 00:18:37,420
احفظ كامل

219
00:18:37,420 --> 00:18:43,859
مجموعة البيانات الخاصة بالتصنيف الصحيح ، ومن ثم نعرف بعضًا منكم بعد مرور نصف عام على ICML هذا العام

220
00:18:44,470 --> 00:18:49,039
لم يكن هناك بالضبط ورقة ورقة الطعن التي عالجت بعض طلب مثل يا

221
00:18:49,470 --> 00:18:55,279
في الواقع ، تقوم هذه الشبكات بعمل شيء أكثر ذكاءً من ذلك إذا نظرت إلى منحنى الدقة هذا

222
00:18:55,279 --> 00:18:57,499
إذا كنت مجرد تدريب على

223
00:18:58,259 --> 00:19:05,179
مجموعة بيانات عشوائية منحنى نوع من انحدار جدا جدا تعرف ببطء شديد تقريبا في شكل خطي

224
00:19:05,179 --> 00:19:09,589
لذلك أنت حقا تكافح من أجل العثور على الحد الأدنى المحلي ممكن

225
00:19:09,590 --> 00:19:15,289
أنت تعرف الأوزان الصحيحة التي من شأنها أن تحصل على هذه الدقة في حين إذا كنت في الواقع التدريب على مجموعة البيانات المنظمة التي لديها

226
00:19:15,289 --> 00:19:21,439
التسميات المناسبة. أنت تعلم أنك تعزف قليلاً في البداية ، لكنك تراجعت بسرعة كبيرة للوصول إلى ذلك

227
00:19:22,200 --> 00:19:26,149
مستوى الدقة ، ومن هذا المنطلق ، كان من الأسهل العثور عليه

228
00:19:26,759 --> 00:19:33,949
فالحد الأقصى المحلي ، وكذلك الأمر المثير للاهتمام أيضًا ، هو أنه تم التقاطه في ضوء ورقة أخرى من الواقع قبل عامين

229
00:19:34,080 --> 00:19:36,080
الذي لديه الكثير

230
00:19:36,990 --> 00:19:39,169
تبسيط حول طبقات الشبكة

231
00:19:39,169 --> 00:19:46,788
ولكن إحدى النتائج كانت تقول كيف إذا نظرت إلى مشهد التحسين ، فإن الحدود المحلية التي تميل هذه الشبكات إلى تعلمها هي

232
00:19:47,340 --> 00:19:54,079
في الواقع ، من نوعية متساوية ، بمعنى ما إذا كانت مجموعة البيانات الخاصة بك هي بنية ، ويجب أن تكون قادرًا على العثور عليها بسهولة أكبر

233
00:19:58,139 --> 00:20:01,189
شكري كما هو الحال دائما لمن دعمكم على patreon

234
00:20:01,190 --> 00:20:06,950
لقد قلت من قبل فقط ما هو patreon المغير لعبة ولكن هذه الفيديوهات حقا لن يكون من الممكن بدونك أنا

235
00:20:07,230 --> 00:20:12,889
تريد أيضا أن تعطي خاص. شكرًا لشركاء VC firmifi في دعمهم لمقاطع الفيديو الأولية هذه في السلسلة

