1
00:00:04,180 --> 00:00:07,280
في الفيديو الأخير قمت بوضع هيكل الشبكة العصبية.

2
00:00:07,680 --> 00:00:10,437
سأقدم ملخصًا سريعًا هنا حتى يظل جديدًا في أذهاننا، 

3
00:00:10,437 --> 00:00:12,600
وبعد ذلك لدي هدفان رئيسيان لهذا الفيديو.

4
00:00:13,100 --> 00:00:16,955
الأول هو تقديم فكرة النسب المتدرج، والتي لا تكمن وراء كيفية تعلم الشبكات 

5
00:00:16,955 --> 00:00:20,600
العصبية فحسب، بل أيضًا كيفية عمل الكثير من أنظمة التعلم الآلي الأخرى.

6
00:00:21,120 --> 00:00:24,582
ثم بعد ذلك سنتعمق أكثر في كيفية أداء هذه الشبكة تحديدًا، وما الذي 

7
00:00:24,582 --> 00:00:27,940
تبحث عنه تلك الطبقات المخفية من الخلايا العصبية في نهاية المطاف.

8
00:00:28,980 --> 00:00:32,427
للتذكير، هدفنا هنا هو المثال الكلاسيكي للتعرف على 

9
00:00:32,427 --> 00:00:36,220
الأرقام المكتوبة بخط اليد، عالم الشبكات العصبية المرحب.

10
00:00:37,020 --> 00:00:43,420
يتم عرض هذه الأرقام على شبكة بحجم 28 × 28 بكسل، ولكل بكسل قيمة تدرج رمادي تتراوح بين 0 و1.

11
00:00:43,820 --> 00:00:50,040
هذه هي التي تحدد تنشيط 784 خلية عصبية في طبقة الإدخال للشبكة.

12
00:00:51,180 --> 00:00:55,793
ومن ثم يعتمد تنشيط كل خلية عصبية في الطبقات التالية على مجموع مرجح 

13
00:00:55,793 --> 00:01:00,820
لجميع عمليات التنشيط في الطبقة السابقة، بالإضافة إلى رقم خاص يسمى التحيز.

14
00:01:02,160 --> 00:01:05,436
ثم تقوم بتكوين هذا المجموع باستخدام وظيفة أخرى، مثل السحق 

15
00:01:05,436 --> 00:01:08,940
السيني، أو الريلو، بالطريقة التي مشيت بها خلال الفيديو الأخير.

16
00:01:09,480 --> 00:01:14,402
في المجمل، بالنظر إلى الاختيار التعسفي إلى حد ما لطبقتين مخفيتين تحتوي كل 

17
00:01:14,402 --> 00:01:19,391
منهما على 16 خلية عصبية، فإن الشبكة لديها حوالي 13000 من الأوزان والتحيزات 

18
00:01:19,391 --> 00:01:24,380
التي يمكننا تعديلها، وهذه القيم هي التي تحدد بالضبط ما تفعله الشبكة بالفعل.

19
00:01:24,880 --> 00:01:29,090
ثم ما نعنيه عندما نقول أن هذه الشبكة تصنف رقمًا معينًا هو أن ألمع 

20
00:01:29,090 --> 00:01:33,300
تلك الخلايا العصبية العشرة في الطبقة النهائية يتوافق مع هذا الرقم.

21
00:01:34,100 --> 00:01:38,852
وتذكر أن الدافع الذي وضعناه في أذهاننا هنا للبنية الطبقية هو أنه ربما يمكن 

22
00:01:38,852 --> 00:01:43,731
للطبقة الثانية أن تلتقط الحواف، والطبقة الثالثة قد تلتقط أنماطًا مثل الحلقات 

23
00:01:43,731 --> 00:01:48,800
والخطوط، ويمكن للطبقة الأخيرة أن تجمع تلك الأنماط معًا أنماط للتعرف على الأرقام.

24
00:01:49,800 --> 00:01:52,240
إذن هنا، نتعلم كيف تتعلم الشبكة.

25
00:01:52,640 --> 00:01:57,178
ما نريده هو خوارزمية حيث يمكنك أن تعرض لهذه الشبكة مجموعة كاملة من بيانات 

26
00:01:57,178 --> 00:02:01,410
التدريب، والتي تأتي في شكل مجموعة من الصور المختلفة للأرقام المكتوبة 

27
00:02:01,410 --> 00:02:05,642
بخط اليد، بالإضافة إلى تسميات لما يفترض أن تكون عليه، وسوف قم بتعديل 

28
00:02:05,642 --> 00:02:10,120
تلك الأوزان والتحيزات البالغ عددها 13000 لتحسين أدائها في بيانات التدريب.

29
00:02:10,720 --> 00:02:13,981
نأمل أن يعني هذا الهيكل متعدد الطبقات أن ما يتعلمه 

30
00:02:13,981 --> 00:02:16,860
يعمم على الصور بما يتجاوز بيانات التدريب تلك.

31
00:02:17,640 --> 00:02:22,256
الطريقة التي نختبر بها ذلك هي أنه بعد تدريب الشبكة، تظهر لها المزيد من البيانات 

32
00:02:22,256 --> 00:02:26,700
المصنفة التي لم يسبق لها رؤيتها من قبل، وترى مدى دقة تصنيف تلك الصور الجديدة.

33
00:02:31,120 --> 00:02:35,444
لحسن الحظ بالنسبة لنا، وما يجعل هذا مثالًا شائعًا للبدء به، هو أن الأشخاص الجيدين 

34
00:02:35,444 --> 00:02:39,716
الذين يقفون وراء قاعدة بيانات MNIST قاموا بتجميع مجموعة من عشرات الآلاف من الصور 

35
00:02:39,716 --> 00:02:44,200
الرقمية المكتوبة بخط اليد، كل واحدة منها تحمل الأرقام التي من المفترض أن تحملها يكون.

36
00:02:44,900 --> 00:02:48,253
وبقدر ما يكون وصف الآلة بأنها تتعلم أمرًا مثيرًا، إلا أنه 

37
00:02:48,253 --> 00:02:51,722
بمجرد أن ترى كيف تعمل، فإن الأمر يبدو أقل شبهًا ببعض فرضيات 

38
00:02:51,722 --> 00:02:55,480
الخيال العلمي المجنونة، وأكثر شبهًا بتمرين حساب التفاضل والتكامل.

39
00:02:56,200 --> 00:02:59,960
أعني أن الأمر يتعلق في الأساس بإيجاد الحد الأدنى من وظيفة معينة.

40
00:03:01,940 --> 00:03:06,299
تذكر، من الناحية النظرية، نحن نفكر في كل خلية عصبية على أنها متصلة بجميع 

41
00:03:06,299 --> 00:03:10,599
الخلايا العصبية في الطبقة السابقة، والأوزان في المجموع المرجح الذي يحدد 

42
00:03:10,599 --> 00:03:14,839
تنشيطها تشبه إلى حد ما نقاط قوة تلك الاتصالات، والتحيز هو بعض المؤشرات 

43
00:03:14,839 --> 00:03:18,960
على ما إذا كانت تلك الخلية العصبية تميل إلى أن تكون نشطة أو غير نشطة.

44
00:03:19,720 --> 00:03:24,400
ولبدء الأمور، سنقوم بتهيئة كل هذه الأوزان والتحيزات بشكل عشوائي تمامًا.

45
00:03:24,940 --> 00:03:27,773
وغني عن القول أن أداء هذه الشبكة سيكون سيئًا جدًا 

46
00:03:27,773 --> 00:03:30,720
في مثال تدريب معين، نظرًا لأنها تفعل شيئًا عشوائيًا.

47
00:03:31,040 --> 00:03:33,477
على سبيل المثال، قمت بتغذية هذه الصورة بالرقم 

48
00:03:33,477 --> 00:03:36,020
3، وستبدو طبقة الإخراج وكأنها في حالة من الفوضى.

49
00:03:36,600 --> 00:03:41,229
إذن ما تفعله هو تحديد دالة التكلفة، وهي طريقة لإخبار الكمبيوتر، لا، 

50
00:03:41,229 --> 00:03:45,858
كمبيوتر سيء، يجب أن يحتوي هذا الإخراج على عمليات تنشيط تكون 0 لمعظم 

51
00:03:45,858 --> 00:03:50,760
الخلايا العصبية، ولكن 1 لهذه الخلية العصبية، ما قدمته لي هو قمامة مطلقة.

52
00:03:51,720 --> 00:03:58,329
لنقول ذلك من الناحية الرياضية، يمكنك جمع مربعات الاختلافات بين كل من عمليات تنشيط 

53
00:03:58,329 --> 00:04:05,020
مخرجات المهملات هذه والقيمة التي تريدها لها، وهذا ما سنسميه تكلفة مثال تدريبي واحد.

54
00:04:05,960 --> 00:04:11,105
لاحظ أن هذا المجموع يكون صغيرًا عندما تقوم الشبكة بتصنيف الصورة بشكل 

55
00:04:11,105 --> 00:04:16,399
صحيح بثقة، ولكنه يكون كبيرًا عندما تبدو الشبكة وكأنها لا تعرف ما تفعله.

56
00:04:18,640 --> 00:04:21,737
إذن ما عليك فعله هو أن تأخذ في الاعتبار متوسط 

57
00:04:21,737 --> 00:04:25,440
التكلفة لجميع عشرات الآلاف من أمثلة التدريب المتاحة لك.

58
00:04:27,040 --> 00:04:30,199
إن متوسط التكلفة هذا هو مقياسنا لمدى رديئة الشبكة، 

59
00:04:30,199 --> 00:04:32,740
ومدى السوء الذي يجب أن يشعر به الكمبيوتر.

60
00:04:33,420 --> 00:04:34,600
وهذا شيء معقد.

61
00:04:35,040 --> 00:04:42,001
هل تتذكر كيف كانت الشبكة نفسها في الأساس وظيفة، تأخذ 784 رقمًا كمدخلات، وقيم البكسل، 

62
00:04:42,001 --> 00:04:48,800
وتخرج 10 أرقام كمخرجات لها، وبمعنى ما يتم تحديد معلماتها بكل هذه الأوزان والتحيزات؟

63
00:04:49,500 --> 00:04:52,820
حسنًا، دالة التكلفة هي طبقة من التعقيد فوق ذلك.

64
00:04:53,100 --> 00:04:58,416
إنها تأخذ كمدخلاتها تلك الأوزان والتحيزات التي يبلغ عددها 13000 أو نحو 

65
00:04:58,416 --> 00:05:03,808
ذلك، وتطلق رقمًا واحدًا يصف مدى سوء تلك الأوزان والتحيزات، وتعتمد طريقة 

66
00:05:03,808 --> 00:05:08,900
تعريفها على سلوك الشبكة على كل عشرات الآلاف من أجزاء بيانات التدريب.

67
00:05:09,520 --> 00:05:11,000
هذا كثير للتفكير فيه.

68
00:05:12,400 --> 00:05:15,820
لكن مجرد إخبار الكمبيوتر بالمهمة السيئة التي يقوم بها ليس مفيدًا جدًا.

69
00:05:16,220 --> 00:05:20,060
تريد أن تخبره بكيفية تغيير تلك الأوزان والتحيزات حتى يتحسن.

70
00:05:20,780 --> 00:05:25,630
لتسهيل الأمر، بدلًا من صعوبة تخيل دالة تحتوي على 13000 مدخل، 

71
00:05:25,630 --> 00:05:30,480
تخيل فقط دالة بسيطة تحتوي على رقم واحد كمدخل ورقم واحد كمخرج.

72
00:05:31,480 --> 00:05:35,300
كيف يمكنك العثور على مدخلات تقلل من قيمة هذه الوظيفة؟

73
00:05:36,460 --> 00:05:41,395
سيعرف طلاب حساب التفاضل والتكامل أنه يمكنك في بعض الأحيان معرفة هذا الحد الأدنى 

74
00:05:41,395 --> 00:05:46,144
بشكل صريح، ولكن هذا ليس ممكنًا دائمًا للوظائف المعقدة حقًا، وبالتأكيد ليس في 

75
00:05:46,144 --> 00:05:51,080
إصدار الإدخال 13000 من هذا الموقف لوظيفة تكلفة الشبكة العصبية المعقدة والمجنونة.

76
00:05:51,580 --> 00:05:55,707
التكتيك الأكثر مرونة هو البدء عند أي مدخلات، ومعرفة 

77
00:05:55,707 --> 00:05:59,200
الاتجاه الذي يجب أن تسلكه لتقليل هذا الناتج.

78
00:06:00,080 --> 00:06:05,137
على وجه التحديد، إذا كان بإمكانك معرفة ميل الدالة التي تتواجد فيها، فانتقل إلى اليسار 

79
00:06:05,137 --> 00:06:09,900
إذا كان هذا الميل موجبًا، وقم بتحويل الإدخال إلى اليمين إذا كان هذا الميل سالبًا.

80
00:06:11,960 --> 00:06:15,770
إذا قمت بذلك بشكل متكرر، عند كل نقطة تتحقق من الميل الجديد 

81
00:06:15,770 --> 00:06:19,840
وتتخذ الخطوة المناسبة، فسوف تقترب من الحد الأدنى المحلي للدالة.

82
00:06:20,640 --> 00:06:23,800
الصورة التي قد تكون في ذهنك هنا هي كرة تتدحرج أسفل التل.

83
00:06:24,620 --> 00:06:29,446
لاحظ، حتى بالنسبة لوظيفة الإدخال الفردي المبسطة حقًا، هناك العديد من الانحدارات 

84
00:06:29,446 --> 00:06:34,453
المحتملة التي قد تصل إليها، اعتمادًا على الإدخال العشوائي الذي تبدأ منه، وليس هناك 

85
00:06:34,453 --> 00:06:39,400
ما يضمن أن الحد الأدنى المحلي الذي تهبط فيه سيكون أصغر قيمة ممكنة من دالة التكلفة.

86
00:06:40,220 --> 00:06:42,620
سيتم نقل ذلك إلى حالة الشبكة العصبية لدينا أيضًا.

87
00:06:43,180 --> 00:06:48,855
أريدك أيضًا أن تلاحظ كيف أنه إذا جعلت أحجام خطواتك متناسبة مع المنحدر، فعندما يصبح 

88
00:06:48,855 --> 00:06:54,600
المنحدر مسطحًا نحو الحد الأدنى، تصبح خطواتك أصغر فأصغر، وهذا يساعدك على عدم التجاوز.

89
00:06:55,940 --> 00:07:00,980
ولزيادة التعقيد قليلاً، تخيل بدلاً من ذلك دالة ذات مدخلين ومخرج واحد.

90
00:07:01,500 --> 00:07:05,108
قد تفكر في مساحة الإدخال على أنها مستوى xy، ودالة 

91
00:07:05,108 --> 00:07:08,140
التكلفة على أنها مرسوم بيانيًا كسطح فوقها.

92
00:07:08,760 --> 00:07:13,820
بدلًا من السؤال عن ميل الدالة، عليك أن تسأل عن الاتجاه الذي يجب 

93
00:07:13,820 --> 00:07:18,960
أن تخطو فيه في مساحة الإدخال هذه لتقليل مخرجات الدالة بسرعة أكبر.

94
00:07:19,720 --> 00:07:21,760
وبعبارة أخرى، ما هو الاتجاه الهبوطي؟

95
00:07:22,380 --> 00:07:25,560
مرة أخرى، من المفيد أن نفكر في كرة تتدحرج إلى أسفل ذلك التل.

96
00:07:26,660 --> 00:07:30,875
أولئك الذين هم على دراية بحساب التفاضل والتكامل متعدد المتغيرات 

97
00:07:30,875 --> 00:07:34,893
سيعرفون أن تدرج الدالة يمنحك الاتجاه الأكثر انحدارًا للصعود، 

98
00:07:34,893 --> 00:07:38,780
وهو الاتجاه الذي يجب أن تخطو إليه لزيادة الدالة بسرعة أكبر.

99
00:07:39,560 --> 00:07:46,040
وبطبيعة الحال، فإن أخذ سالب هذا التدرج يمنحك الاتجاه للخطوة التي تقلل الدالة بسرعة أكبر.

100
00:07:47,240 --> 00:07:50,820
والأكثر من ذلك، فإن طول متجه التدرج هذا يعد مؤشرًا 

101
00:07:50,820 --> 00:07:53,840
على مدى انحدار هذا المنحدر الأكثر انحدارًا.

102
00:07:54,540 --> 00:07:57,383
إذا لم تكن على دراية بحساب التفاضل والتكامل متعدد المتغيرات وترغب في معرفة 

103
00:07:57,383 --> 00:08:00,340
المزيد، فاطلع على بعض الأعمال التي قمت بها لصالح أكاديمية خان حول هذا الموضوع.

104
00:08:00,860 --> 00:08:06,097
لكن بصراحة، كل ما يهمني ولك الآن هو أنه من حيث المبدأ توجد طريقة 

105
00:08:06,097 --> 00:08:11,900
لحساب هذا المتجه، هذا المتجه الذي يخبرك ما هو اتجاه الهبوط ومدى انحداره.

106
00:08:12,400 --> 00:08:16,120
ستكون على ما يرام إذا كان هذا هو كل ما تعرفه ولم تكن ملتزمًا بالتفاصيل.

107
00:08:17,200 --> 00:08:22,004
إذا تمكنت من الحصول على ذلك، فإن خوارزمية تقليل الوظيفة هي حساب اتجاه 

108
00:08:22,004 --> 00:08:26,740
التدرج هذا، ثم اتخاذ خطوة صغيرة إلى أسفل، وتكرار ذلك مرارًا وتكرارًا.

109
00:08:27,700 --> 00:08:32,820
إنها نفس الفكرة الأساسية للدالة التي تحتوي على 13000 مدخلًا بدلاً من مدخلين.

110
00:08:33,400 --> 00:08:39,460
تخيل تنظيم جميع الأوزان والتحيزات البالغ عددها 13000 لشبكتنا في ناقل عمود عملاق.

111
00:08:40,140 --> 00:08:47,649
التدرج السلبي لدالة التكلفة هو مجرد متجه، إنه اتجاه ما داخل مساحة الإدخال الضخمة 

112
00:08:47,649 --> 00:08:54,880
بجنون والتي تخبرك بأي دفعات لكل هذه الأرقام ستسبب أسرع انخفاض في دالة التكلفة.

113
00:08:55,640 --> 00:09:00,583
وبطبيعة الحال، مع دالة التكلفة المصممة خصيصًا لدينا، فإن تغيير الأوزان 

114
00:09:00,583 --> 00:09:05,527
والتحيزات لتقليلها يعني جعل مخرجات الشبكة على كل جزء من بيانات التدريب 

115
00:09:05,527 --> 00:09:10,820
تبدو أقل كمصفوفة عشوائية من 10 قيم، وأكثر مثل القرار الفعلي الذي نريده لجعل.

116
00:09:11,440 --> 00:09:16,027
من المهم أن تتذكر أن دالة التكلفة هذه تتضمن متوسطًا لجميع بيانات 

117
00:09:16,027 --> 00:09:21,180
التدريب، لذلك إذا قمت بتصغيره، فهذا يعني أنه أداء أفضل في كل تلك العينات.

118
00:09:23,820 --> 00:09:28,760
الخوارزمية لحساب هذا التدرج بكفاءة، والتي هي في الواقع جوهر كيفية تعلم 

119
00:09:28,760 --> 00:09:33,980
الشبكة العصبية، تسمى الانتشار العكسي، وهذا ما سأتحدث عنه في الفيديو التالي.

120
00:09:34,660 --> 00:09:38,851
هناك، أريد حقًا أن أخصص وقتًا للتجول في ما يحدث بالضبط لكل وزن 

121
00:09:38,851 --> 00:09:43,108
وتحيز لجزء معين من بيانات التدريب، في محاولة لإعطاء إحساس بديهي 

122
00:09:43,108 --> 00:09:47,100
بما يحدث خارج كومة حسابات التفاضل والتكامل والصيغ ذات الصلة.

123
00:09:47,780 --> 00:09:53,069
هنا، الآن، الشيء الرئيسي الذي أريدك أن تعرفه، بغض النظر عن تفاصيل التنفيذ، 

124
00:09:53,069 --> 00:09:58,360
هو أن ما نعنيه عندما نتحدث عن التعلم الشبكي هو أنه مجرد تقليل دالة التكلفة.

125
00:09:59,300 --> 00:10:03,618
ولاحظ أن إحدى نتائج ذلك هي أنه من المهم لدالة التكلفة هذه أن يكون لها ناتج سلس 

126
00:10:03,618 --> 00:10:08,100
ولطيف، حتى نتمكن من إيجاد الحد الأدنى المحلي عن طريق اتخاذ خطوات صغيرة نحو الأسفل.

127
00:10:09,260 --> 00:10:12,337
ولهذا السبب، بالمناسبة، تتمتع الخلايا العصبية الاصطناعية 

128
00:10:12,337 --> 00:10:15,522
بتنشيطات متفاوتة باستمرار، بدلاً من أن تكون ببساطة نشطة أو 

129
00:10:15,522 --> 00:10:19,140
غير نشطة بطريقة ثنائية، كما هي الحال مع الخلايا العصبية البيولوجية.

130
00:10:20,220 --> 00:10:23,425
تسمى هذه العملية المتمثلة في دفع مدخلات دالة بشكل 

131
00:10:23,425 --> 00:10:26,760
متكرر عن طريق عدة مضاعفات التدرج السلبي نزول التدرج.

132
00:10:27,300 --> 00:10:32,580
إنها طريقة للتقارب نحو حد أدنى محلي لدالة التكلفة، وهو في الأساس واد في هذا الرسم البياني.

133
00:10:33,440 --> 00:10:38,679
ما زلت أعرض صورة الدالة ذات المدخلين، بالطبع، لأن الدفعات في مساحة إدخال ذات 

134
00:10:38,679 --> 00:10:44,260
13000 بُعد يصعب قليلاً استيعابها، ولكن هناك طريقة لطيفة غير مكانية للتفكير في هذا.

135
00:10:45,080 --> 00:10:48,440
يخبرنا كل مكون من مكونات التدرج السلبي بأمرين.

136
00:10:49,060 --> 00:10:55,140
تخبرنا العلامة بالطبع ما إذا كان يجب دفع المركبة المقابلة لمتجه الإدخال لأعلى أم لأسفل.

137
00:10:55,800 --> 00:11:02,720
ولكن الأهم من ذلك، أن الأحجام النسبية لجميع هذه المكونات تخبرك بالتغييرات الأكثر أهمية.

138
00:11:05,220 --> 00:11:09,164
كما ترى، في شبكتنا، قد يكون لتعديل أحد الأوزان تأثير أكبر 

139
00:11:09,164 --> 00:11:13,040
بكثير على دالة التكلفة من التعديل على بعض الأوزان الأخرى.

140
00:11:14,800 --> 00:11:18,200
بعض هذه الاتصالات مهمة أكثر بالنسبة لبيانات التدريب لدينا.

141
00:11:19,320 --> 00:11:23,749
لذا، إحدى الطرق التي يمكنك من خلالها التفكير في متجه التدرج هذا 

142
00:11:23,749 --> 00:11:28,109
لدالة التكلفة الضخمة المذهلة لدينا هي أنه يشفر الأهمية النسبية 

143
00:11:28,109 --> 00:11:32,400
لكل وزن وتحيز، أي أي من هذه التغييرات سيحقق أكبر قدر من المال.

144
00:11:33,620 --> 00:11:36,640
هذه في الواقع مجرد طريقة أخرى للتفكير في الاتجاه.

145
00:11:37,100 --> 00:11:43,318
لنأخذ مثالًا أبسط، إذا كان لديك دالة تحتوي على متغيرين كمدخل، وقمت بحساب أن تدرجها عند 

146
00:11:43,318 --> 00:11:49,465
نقطة معينة يظهر كـ 3,1، فمن ناحية يمكنك تفسير ذلك على أنه قول ذلك عندما عندما نقف عند 

147
00:11:49,465 --> 00:11:55,827
هذا المدخل، فإن التحرك على طول هذا الاتجاه يزيد من الدالة بسرعة أكبر، وعندما ترسم الدالة 

148
00:11:55,827 --> 00:12:02,260
رسمًا بيانيًا فوق مستوى نقاط الإدخال، فإن هذا المتجه هو ما يمنحك الاتجاه الصعودي المستقيم.

149
00:12:02,860 --> 00:12:07,621
ولكن هناك طريقة أخرى لقراءة ذلك وهي أن نقول إن التغييرات في هذا المتغير الأول 

150
00:12:07,621 --> 00:12:12,260
لها أهمية 3 أضعاف أهمية التغييرات في المتغير الثاني، وذلك على الأقل في جوار 

151
00:12:12,260 --> 00:12:16,900
المدخلات ذات الصلة، فإن دفع قيمة x يحمل تأثيرًا أكبر بكثير بالنسبة لك دولار.

152
00:12:19,880 --> 00:12:22,340
دعونا نصغر الصورة ونلخص ما وصلنا إليه حتى الآن.

153
00:12:22,840 --> 00:12:26,614
الشبكة نفسها هي هذه الوظيفة التي تحتوي على 784 مدخلاً 

154
00:12:26,614 --> 00:12:30,040
و10 مخرجات، محددة من حيث كل هذه المبالغ الموزونة.

155
00:12:30,640 --> 00:12:33,680
دالة التكلفة هي طبقة من التعقيد فوق ذلك.

156
00:12:33,980 --> 00:12:37,969
فهو يأخذ 13000 من الأوزان والتحيزات كمدخلات ويطلق 

157
00:12:37,969 --> 00:12:41,720
مقياسًا واحدًا للرداءة بناءً على أمثلة التدريب.

158
00:12:42,440 --> 00:12:46,900
ولا يزال تدرج دالة التكلفة يمثل طبقة أخرى من التعقيد.

159
00:12:47,360 --> 00:12:52,467
فهو يخبرنا ما هي الحوافز لكل هذه الأوزان والتحيزات التي تسبب التغيير الأسرع في قيمة 

160
00:12:52,467 --> 00:12:57,880
دالة التكلفة، والتي قد تفسرها على أنها تحدد التغييرات التي تطرأ على الأوزان الأكثر أهمية.

161
00:13:02,560 --> 00:13:07,780
لذلك، عند تهيئة الشبكة بأوزان وتحيزات عشوائية، وضبطها عدة مرات بناءً على عملية 

162
00:13:07,780 --> 00:13:13,200
الهبوط المتدرج هذه، ما مدى جودة أدائها فعليًا على الصور التي لم يتم رؤيتها من قبل؟

163
00:13:14,100 --> 00:13:17,944
تلك التي وصفتها هنا، مع الطبقتين المخفيتين المكونتين من 16 

164
00:13:17,944 --> 00:13:21,854
خلية عصبية، والتي تم اختيارها في الغالب لأسباب جمالية، ليست 

165
00:13:21,854 --> 00:13:25,960
سيئة، حيث تصنف حوالي 96٪ من الصور الجديدة التي تراها بشكل صحيح.

166
00:13:26,680 --> 00:13:29,859
وبصراحة، إذا نظرت إلى بعض الأمثلة التي أخطأت فيها، 

167
00:13:29,859 --> 00:13:32,540
ستشعر أنك مجبر على التقليل من الأمر قليلاً.

168
00:13:36,220 --> 00:13:39,075
الآن، إذا تلاعبت ببنية الطبقة المخفية وقمت بإجراء 

169
00:13:39,075 --> 00:13:41,760
بعض التعديلات، يمكنك الحصول على ما يصل إلى 98%.

170
00:13:41,760 --> 00:13:42,720
وهذا جيد جدًا!

171
00:13:43,020 --> 00:13:47,453
إنها ليست الأفضل، يمكنك بالتأكيد الحصول على أداء أفضل من خلال الحصول على 

172
00:13:47,453 --> 00:13:52,189
شبكة أكثر تعقيدًا من شبكة الفانيليا البسيطة هذه، ولكن نظرًا لمدى صعوبة المهمة 

173
00:13:52,189 --> 00:13:56,683
الأولية، أعتقد أن هناك شيئًا لا يصدق في أي شبكة تفعل هذا جيدًا على صور لم 

174
00:13:56,683 --> 00:14:01,420
يسبق لها مثيل من قبل، نظرًا لذلك لم نخبره مطلقًا بالأنماط التي يجب البحث عنها.

175
00:14:02,560 --> 00:14:07,433
في الأصل، كانت الطريقة التي حفزت بها هذا الهيكل هي وصف الأمل الذي قد يكون لدينا، 

176
00:14:07,433 --> 00:14:12,306
وهو أن الطبقة الثانية قد تلتقط حوافًا صغيرة، وأن الطبقة الثالثة ستجمع تلك الحواف 

177
00:14:12,306 --> 00:14:17,180
معًا للتعرف على الحلقات والخطوط الأطول، وأنه يمكن تجميعها معا للتعرف على الأرقام.

178
00:14:17,960 --> 00:14:20,400
فهل هذا ما تفعله شبكتنا بالفعل؟

179
00:14:21,080 --> 00:14:24,400
حسنًا، بالنسبة لهذا على الأقل، ليس على الإطلاق.

180
00:14:24,820 --> 00:14:28,920
هل تتذكر كيف نظرنا في الفيديو الأخير إلى كيفية تصور أوزان الاتصالات 

181
00:14:28,920 --> 00:14:32,839
من جميع الخلايا العصبية في الطبقة الأولى إلى خلية عصبية معينة في 

182
00:14:32,839 --> 00:14:37,060
الطبقة الثانية كنمط بكسل معين تلتقطه الخلية العصبية في الطبقة الثانية؟

183
00:14:37,780 --> 00:14:42,845
حسنًا، عندما نفعل ذلك بالفعل بالنسبة للأوزان المرتبطة بهذه التحولات، من 

184
00:14:42,845 --> 00:14:47,910
الطبقة الأولى إلى الطبقة التالية، بدلاً من التقاط حواف صغيرة معزولة هنا 

185
00:14:47,910 --> 00:14:53,680
وهناك، فإنها تبدو عشوائية تقريبًا، فقط مع بعض الأنماط الفضفاضة جدًا في الوسط هناك.

186
00:14:53,760 --> 00:14:58,681
يبدو أنه في الفضاء البُعدي الكبير الذي لا يسبر غوره والذي يبلغ 13000 بُعدًا من 

187
00:14:58,681 --> 00:15:03,540
الأوزان والتحيزات المحتملة، وجدت شبكتنا نفسها حدًا أدنى محليًا صغيرًا سعيدًا، 

188
00:15:03,540 --> 00:15:08,960
والذي، على الرغم من نجاحه في تصنيف معظم الصور، لا يلتقط تمامًا الأنماط التي كنا نأملها.

189
00:15:09,780 --> 00:15:13,820
ولتوضيح هذه النقطة حقًا، شاهد ما يحدث عند إدخال صورة عشوائية.

190
00:15:14,320 --> 00:15:19,118
إذا كان النظام ذكيًا، فقد تتوقع أن يشعر بعدم اليقين، وربما لا ينشط 

191
00:15:19,118 --> 00:15:24,204
أيًا من تلك الخلايا العصبية العشرة أو ينشطها جميعًا بشكل متساوٍ، ولكنه 

192
00:15:24,204 --> 00:15:29,217
بدلاً من ذلك يعطيك بثقة بعض الإجابات الهراء، كما لو كان متأكدًا من أن 

193
00:15:29,217 --> 00:15:34,160
هذا الضجيج العشوائي هي 5 كما هو الحال مع الصورة الفعلية للرقم 5 هي 5.

194
00:15:34,540 --> 00:15:37,620
وبصياغة مختلفة، حتى لو كانت هذه الشبكة قادرة على التعرف 

195
00:15:37,620 --> 00:15:40,700
على الأرقام بشكل جيد، فليس لديها أي فكرة عن كيفية رسمها.

196
00:15:41,420 --> 00:15:45,240
يرجع الكثير من هذا إلى أنه إعداد تدريب مقيد بشدة.

197
00:15:45,880 --> 00:15:47,740
أعني، ضع نفسك مكان الشبكة هنا.

198
00:15:48,140 --> 00:15:54,687
ومن وجهة نظره، فإن الكون بأكمله لا يتكون من شيء سوى أرقام ثابتة محددة بوضوح ومتمركزة 

199
00:15:54,687 --> 00:16:01,080
في شبكة صغيرة، ولم تمنحه دالة التكلفة أي حافز أبدًا ليكون واثقًا تمامًا في قراراته.

200
00:16:02,120 --> 00:16:05,752
إذن مع هذه الصورة لما تفعله بالفعل الخلايا العصبية في الطبقة 

201
00:16:05,752 --> 00:16:09,920
الثانية، قد تتساءل لماذا أقدم هذه الشبكة بدافع التقاط الحواف والأنماط.

202
00:16:09,920 --> 00:16:12,300
أعني أن هذا ليس ما ينتهي به الأمر على الإطلاق.

203
00:16:13,380 --> 00:16:17,180
حسنًا، ليس المقصود من هذا أن يكون هدفنا النهائي، بل نقطة البداية.

204
00:16:17,640 --> 00:16:22,077
بصراحة، هذه تقنية قديمة، من النوع الذي تم بحثه في الثمانينيات والتسعينيات، 

205
00:16:22,077 --> 00:16:26,278
وتحتاج إلى فهمها قبل أن تتمكن من فهم المتغيرات الحديثة الأكثر تفصيلاً، 

206
00:16:26,278 --> 00:16:30,420
ومن الواضح أنها قادرة على حل بعض المشكلات المثيرة للاهتمام، ولكن كلما 

207
00:16:30,420 --> 00:16:34,740
بحثت أكثر في ما ما تفعله تلك الطبقات المخفية حقًا هو أنها تبدو أقل ذكاءً.

208
00:16:38,480 --> 00:16:42,454
تحويل التركيز للحظة من كيفية تعلم الشبكات إلى كيفية تعلمك، لن 

209
00:16:42,454 --> 00:16:46,300
يحدث ذلك إلا إذا انخرطت بنشاط في المادة هنا بطريقة أو بأخرى.

210
00:16:47,060 --> 00:16:51,361
أحد الأشياء البسيطة جدًا التي أريدك أن تفعلها هو التوقف الآن 

211
00:16:51,361 --> 00:16:55,873
والتفكير بعمق للحظة حول التغييرات التي قد تجريها على هذا النظام 

212
00:16:55,873 --> 00:17:00,880
وكيف يتصور الصور إذا أردت أن يلتقط أشياء مثل الحواف والأنماط بشكل أفضل.

213
00:17:01,480 --> 00:17:05,394
ولكن الأفضل من ذلك، للتفاعل فعليًا مع المادة، أوصي بشدة 

214
00:17:05,394 --> 00:17:09,099
بكتاب مايكل نيلسن حول التعلم العميق والشبكات العصبية.

215
00:17:09,680 --> 00:17:13,894
يمكنك العثور فيه على الكود والبيانات التي يمكنك تنزيلها واللعب بها 

216
00:17:13,894 --> 00:17:18,359
لهذا المثال بالتحديد، وسيرشدك الكتاب خطوة بخطوة إلى ما يفعله هذا الكود.

217
00:17:19,300 --> 00:17:23,513
الأمر الرائع هو أن هذا الكتاب مجاني ومتاح للعامة، لذا إذا حصلت 

218
00:17:23,513 --> 00:17:27,660
على شيء منه، فكر في الانضمام إلي في التبرع لصالح جهود Nielsen.

219
00:17:27,660 --> 00:17:31,970
لقد قمت أيضًا بربط بعض الموارد الأخرى التي أعجبتني كثيرًا في الوصف، بما في ذلك 

220
00:17:31,970 --> 00:17:36,500
مشاركة المدونة الرائعة والجميلة التي كتبها كريس أولا والمقالات الموجودة في Distill.

221
00:17:38,280 --> 00:17:41,080
لإغلاق الأمور هنا خلال الدقائق القليلة الماضية، أريد 

222
00:17:41,080 --> 00:17:43,880
العودة إلى مقتطف من المقابلة التي أجريتها مع ليشا لي.

223
00:17:44,300 --> 00:17:47,720
ربما تتذكرها من الفيديو الأخير، حيث أنها حصلت على درجة الدكتوراه في التعلم العميق.

224
00:17:48,300 --> 00:17:52,097
تتحدث في هذا المقتطف الصغير عن ورقتين بحثيتين حديثتين تبحثان حقًا 

225
00:17:52,097 --> 00:17:55,780
في كيفية التعلم الفعلي لبعض شبكات التعرف على الصور الأكثر حداثة.

226
00:17:56,120 --> 00:18:00,362
فقط لتحديد ما وصلنا إليه في المحادثة، تناولت الورقة الأولى واحدة من هذه الشبكات 

227
00:18:00,362 --> 00:18:04,497
العصبية العميقة بشكل خاص والتي تعتبر جيدة حقًا في التعرف على الصور، وبدلاً من 

228
00:18:04,497 --> 00:18:08,740
تدريبها على مجموعة بيانات مصنفة بشكل صحيح، قامت بخلط جميع التصنيفات قبل التدريب.

229
00:18:09,480 --> 00:18:13,239
من الواضح أن دقة الاختبار هنا لم تكن أفضل من العشوائية، نظرًا 

230
00:18:13,239 --> 00:18:16,999
لأن كل شيء تم تصنيفه بشكل عشوائي، لكنه كان لا يزال قادرًا على 

231
00:18:16,999 --> 00:18:20,880
تحقيق نفس دقة التدريب كما تفعل في مجموعة بيانات مصنفة بشكل صحيح.

232
00:18:21,600 --> 00:18:26,780
في الأساس، كانت ملايين الأوزان لهذه الشبكة بالذات كافية لحفظ البيانات 

233
00:18:26,780 --> 00:18:31,590
العشوائية فقط، مما يثير السؤال حول ما إذا كان تقليل دالة التكلفة 

234
00:18:31,590 --> 00:18:36,400
هذه يتوافق بالفعل مع أي نوع من البنية في الصورة، أم أنه مجرد حفظ؟

235
00:18:51,440 --> 00:18:58,060
إذا نظرت إلى منحنى الدقة هذا، إذا كنت تتدرب فقط على مجموعة بيانات عشوائية، فقد 

236
00:18:58,060 --> 00:19:04,765
انخفض هذا المنحنى ببطء شديد بطريقة خطية تقريبًا، لذا فأنت تكافح حقًا للعثور على 

237
00:19:04,765 --> 00:19:12,140
الحد الأدنى المحلي الممكن، كما تعلم ، الأوزان المناسبة التي من شأنها أن تمنحك تلك الدقة.

238
00:19:12,240 --> 00:19:17,589
بينما إذا كنت تتدرب فعليًا على مجموعة بيانات منظمة، مجموعة تحتوي على التصنيفات 

239
00:19:17,589 --> 00:19:22,735
الصحيحة، فإنك تعبث قليلاً في البداية، ولكن بعد ذلك تنخفض بسرعة كبيرة للوصول 

240
00:19:22,735 --> 00:19:28,220
إلى مستوى الدقة هذا، وهكذا إلى حد ما كان من الأسهل العثور على الحد الأقصى المحلي.

241
00:19:28,540 --> 00:19:33,657
ولذا فإن ما كان مثيرًا للاهتمام أيضًا في ذلك هو أنه يسلط الضوء على بحث آخر منذ 

242
00:19:33,657 --> 00:19:38,774
بضع سنوات مضت، والذي يحتوي على الكثير من التبسيطات حول طبقات الشبكة، ولكن إحدى 

243
00:19:38,774 --> 00:19:43,826
النتائج كانت تقول كيف إذا نظرت إلى مشهد التحسين، الحد الأدنى المحلي الذي تميل 

244
00:19:43,826 --> 00:19:48,878
هذه الشبكات إلى تعلمه هو في الواقع متساوٍ في الجودة، لذا، إلى حد ما، إذا كانت 

245
00:19:48,878 --> 00:19:54,320
مجموعة البيانات الخاصة بك منظمة، فيجب أن تكون قادرًا على العثور على ذلك بسهولة أكبر.

246
00:19:58,160 --> 00:20:01,180
شكري، كما هو الحال دائمًا، لأولئك الذين يدعمونكم على Patreon.

247
00:20:01,520 --> 00:20:04,078
لقد قلت من قبل ما الذي سيغيره Patreon من قواعد 

248
00:20:04,078 --> 00:20:06,800
اللعبة، لكن مقاطع الفيديو هذه لن تكون ممكنة بدونك.

249
00:20:07,460 --> 00:20:09,882
أريد أيضًا أن أتقدم بشكر خاص لشركة VC Amplify 

250
00:20:09,882 --> 00:20:12,780
Partners، لدعمها لمقاطع الفيديو الأولية هذه في السلسلة.

