1
00:00:00,000 --> 00:00:07,240
पिछले वीडियो में मैंने एक तंत्रिका नेटवर्क की संरचना बताई थी।

2
00:00:07,240 --> 00:00:10,249
मैं यहां एक संक्षिप्त पुनर्कथन दूंगा ताकि यह हमारे दिमाग में

3
00:00:10,249 --> 00:00:13,160
ताजा रहे, और फिर इस वीडियो के लिए मेरे दो मुख्य लक्ष्य हैं।

4
00:00:13,160 --> 00:00:17,119
सबसे पहले ग्रेडिएंट डिसेंट के विचार को पेश करना है, जो न केवल तंत्रिका

5
00:00:17,119 --> 00:00:20,800
नेटवर्क कैसे सीखते हैं, बल्कि कई अन्य मशीन लर्निंग भी काम करता है।

6
00:00:20,800 --> 00:00:25,084
फिर उसके बाद हम थोड़ा और गहराई से देखेंगे कि यह विशेष नेटवर्क कैसा

7
00:00:25,084 --> 00:00:29,560
प्रदर्शन करता है, और न्यूरॉन्स की छिपी हुई परतें आखिर क्या ढूंढती हैं।

8
00:00:29,560 --> 00:00:33,192
एक अनुस्मारक के रूप में, हमारा लक्ष्य यहां हस्तलिखित अंक

9
00:00:33,192 --> 00:00:37,080
पहचान का उत्कृष्ट उदाहरण, तंत्रिका नेटवर्क की हैलो दुनिया है।

10
00:00:37,080 --> 00:00:40,389
ये अंक 28x28 पिक्सेल ग्रिड पर प्रस्तुत किए जाते हैं,

11
00:00:40,389 --> 00:00:44,260
प्रत्येक पिक्सेल में 0 और 1 के बीच कुछ ग्रेस्केल मान होते हैं।

12
00:00:44,260 --> 00:00:51,400
वे ही नेटवर्क की इनपुट परत में 784 न्यूरॉन्स की सक्रियता निर्धारित करते हैं।

13
00:00:51,400 --> 00:00:56,882
निम्नलिखित परतों में प्रत्येक न्यूरॉन के लिए सक्रियता पिछली परत में सभी सक्रियणों के

14
00:00:56,882 --> 00:01:02,300
भारित योग के साथ-साथ कुछ विशेष संख्या पर आधारित होती है जिसे पूर्वाग्रह कहा जाता है।

15
00:01:02,300 --> 00:01:05,832
आप उस राशि को किसी अन्य फ़ंक्शन के साथ बनाते हैं, जैसे सिग्मॉइड

16
00:01:05,832 --> 00:01:09,640
स्क्विशिफिकेशन, या एक ReLU, जिस तरह से मैंने पिछले वीडियो को देखा था।

17
00:01:09,640 --> 00:01:14,824
कुल मिलाकर, प्रत्येक 16 न्यूरॉन्स वाली दो छिपी हुई परतों की कुछ हद तक मनमानी पसंद

18
00:01:14,824 --> 00:01:20,072
को देखते हुए, नेटवर्क में लगभग 13,000 वजन और पूर्वाग्रह हैं जिन्हें हम समायोजित कर

19
00:01:20,072 --> 00:01:25,320
सकते हैं, और ये मूल्य हैं जो निर्धारित करते हैं कि नेटवर्क वास्तव में क्या करता है।

20
00:01:25,320 --> 00:01:29,754
और जब हम कहते हैं कि यह नेटवर्क किसी दिए गए अंक को वर्गीकृत करता है तो हमारा मतलब

21
00:01:29,754 --> 00:01:34,080
यह है कि अंतिम परत में उन 10 न्यूरॉन्स में से सबसे चमकीला उस अंक से मेल खाता है।

22
00:01:34,080 --> 00:01:39,408
और याद रखें, स्तरित संरचना के लिए हमारे मन में प्रेरणा यह थी कि शायद दूसरी

23
00:01:39,408 --> 00:01:44,524
परत किनारों को पकड़ सकती है, तीसरी परत लूप और रेखाओं जैसे पैटर्न को चुन

24
00:01:44,524 --> 00:01:49,640
सकती है, और आखिरी परत उन पैटर्न को एक साथ जोड़ सकती है अंकों को पहचानें.

25
00:01:49,640 --> 00:01:52,880
तो यहां, हम सीखते हैं कि नेटवर्क कैसे सीखता है।

26
00:01:52,880 --> 00:01:57,321
हम जो चाहते हैं वह एक एल्गोरिदम है जहां आप इस नेटवर्क को प्रशिक्षण डेटा का एक

27
00:01:57,321 --> 00:02:01,706
पूरा समूह दिखा सकते हैं, जो हस्तलिखित अंकों की विभिन्न छवियों के समूह के रूप

28
00:02:01,706 --> 00:02:05,976
में आता है, साथ ही उनके लिए लेबल भी होते हैं, और यह होगा उन 13,000 वज़न और

29
00:02:05,976 --> 00:02:10,760
पूर्वाग्रहों को समायोजित करें ताकि प्रशिक्षण डेटा पर इसके प्रदर्शन में सुधार हो सके।

30
00:02:10,760 --> 00:02:14,356
उम्मीद है कि इस स्तरित संरचना का मतलब यह होगा कि यह जो सीखता है

31
00:02:14,356 --> 00:02:17,840
वह उस प्रशिक्षण डेटा से परे छवियों के लिए सामान्यीकृत होता है।

32
00:02:17,840 --> 00:02:21,911
जिस तरह से हम इसका परीक्षण करते हैं वह यह है कि नेटवर्क को

33
00:02:21,911 --> 00:02:26,259
प्रशिक्षित करने के बाद, आप इसे अधिक लेबल वाला डेटा दिखाते हैं,

34
00:02:26,259 --> 00:02:31,160
और आप देखते हैं कि यह उन नई छवियों को कितनी सटीकता से वर्गीकृत करता है।

35
00:02:31,160 --> 00:02:35,800
सौभाग्य से हमारे लिए, और जो बात इसे शुरू करने के लिए एक सामान्य उदाहरण बनाती है, वह यह

36
00:02:35,800 --> 00:02:40,386
है कि एमएनआईएसटी डेटाबेस के पीछे अच्छे लोगों ने हजारों हस्तलिखित अंकों वाली छवियों का

37
00:02:40,386 --> 00:02:45,080
एक संग्रह रखा है, प्रत्येक को उन संख्याओं के साथ लेबल किया गया है जिन्हें वे होना चाहिए।

38
00:02:45,080 --> 00:02:48,537
और एक मशीन को सीखने के रूप में वर्णित करना जितना उत्तेजक है, एक

39
00:02:48,537 --> 00:02:51,778
बार जब आप देखते हैं कि यह कैसे काम करता है, तो यह किसी पागल

40
00:02:51,778 --> 00:02:55,560
विज्ञान-फाई आधार की तरह कम और कैलकुलस अभ्यास की तरह बहुत अधिक लगता है।

41
00:02:55,560 --> 00:03:01,040
मेरा मतलब है, मूल रूप से यह एक निश्चित फ़ंक्शन का न्यूनतम पता लगाने पर निर्भर करता है।

42
00:03:01,040 --> 00:03:07,263
याद रखें, वैचारिक रूप से हम प्रत्येक न्यूरॉन को पिछली परत के सभी न्यूरॉन्स से जुड़े होने

43
00:03:07,263 --> 00:03:13,137
के रूप में सोच रहे हैं, और इसकी सक्रियता को परिभाषित करने वाले भारित योग में वजन उन

44
00:03:13,137 --> 00:03:19,080
कनेक्शनों की ताकत की तरह है, और पूर्वाग्रह कुछ संकेत है चाहे वह न्यूरॉन सक्रिय हो या

45
00:03:19,080 --> 00:03:19,780
निष्क्रिय।

46
00:03:19,780 --> 00:03:22,533
और चीजों को शुरू करने के लिए, हम उन सभी भारों और पूर्वाग्रहों

47
00:03:22,533 --> 00:03:25,020
को पूरी तरह से यादृच्छिक रूप से प्रारंभ करने जा रहे हैं।

48
00:03:25,020 --> 00:03:27,966
कहने की जरूरत नहीं है, यह नेटवर्क किसी दिए गए प्रशिक्षण उदाहरण पर

49
00:03:27,966 --> 00:03:31,180
भयानक प्रदर्शन करने जा रहा है, क्योंकि यह सिर्फ कुछ यादृच्छिक कर रहा है।

50
00:03:31,180 --> 00:03:34,154
उदाहरण के लिए, आप 3 की इस छवि को फ़ीड करते हैं,

51
00:03:34,154 --> 00:03:36,820
और आउटपुट परत बस एक गड़बड़ की तरह दिखती है।

52
00:03:36,820 --> 00:03:40,835
तो आप जो करते हैं वह एक लागत फ़ंक्शन को परिभाषित करना है, कंप्यूटर को

53
00:03:40,835 --> 00:03:44,735
यह बताने का एक तरीका है, नहीं, खराब कंप्यूटर, उस आउटपुट में सक्रियण

54
00:03:44,735 --> 00:03:48,980
होना चाहिए जो अधिकांश न्यूरॉन्स के लिए 0 है, लेकिन इस न्यूरॉन के लिए 1 है।

55
00:03:48,980 --> 00:03:51,740
तुमने मुझे जो दिया वह बिल्कुल कूड़ा है।

56
00:03:51,740 --> 00:03:56,719
इसे थोड़ा और गणितीय रूप से कहने के लिए, आप उन कचरा आउटपुट सक्रियणों

57
00:03:56,719 --> 00:04:01,406
में से प्रत्येक के बीच अंतर के वर्गों को जोड़ते हैं और वह मूल्य

58
00:04:01,406 --> 00:04:06,020
जो आप चाहते हैं, और इसे हम एकल प्रशिक्षण उदाहरण की लागत कहेंगे।

59
00:04:06,020 --> 00:04:10,264
ध्यान दें कि यह राशि तब छोटी होती है जब नेटवर्क आत्मविश्वास से

60
00:04:10,264 --> 00:04:14,575
छवि को सही ढंग से वर्गीकृत करता है, लेकिन यह तब बड़ी होती है जब

61
00:04:14,575 --> 00:04:18,820
नेटवर्क को ऐसा लगता है कि उसे पता नहीं है कि वह क्या कर रहा है।

62
00:04:18,820 --> 00:04:27,580
तो फिर आप अपने पास मौजूद हजारों प्रशिक्षण उदाहरणों की औसत लागत पर विचार करें।

63
00:04:27,580 --> 00:04:30,327
यह औसत लागत इस बात का पैमाना है कि नेटवर्क कितना

64
00:04:30,327 --> 00:04:33,300
ख़राब है और कंप्यूटर को कितना ख़राब महसूस होना चाहिए।

65
00:04:33,300 --> 00:04:35,300
और यह एक जटिल बात है.

66
00:04:35,300 --> 00:04:40,077
याद रखें कि नेटवर्क मूल रूप से एक फ़ंक्शन कैसे था, जो इनपुट के रूप में

67
00:04:40,077 --> 00:04:44,922
784 नंबर लेता है, पिक्सेल मान, और आउटपुट के रूप में 10 नंबर निकालता है,

68
00:04:44,922 --> 00:04:49,700
और एक अर्थ में यह इन सभी भारों और पूर्वाग्रहों द्वारा पैरामीटरयुक्त है?

69
00:04:49,700 --> 00:04:53,340
लागत फ़ंक्शन उसके ऊपर जटिलता की एक परत है।

70
00:04:53,340 --> 00:04:57,275
यह अपने इनपुट के रूप में उन 13,000 या उससे अधिक वजन और पूर्वाग्रहों

71
00:04:57,275 --> 00:05:00,921
को लेता है, और एक एकल संख्या बताता है जो बताता है कि वे वजन और

72
00:05:00,921 --> 00:05:04,915
पूर्वाग्रह कितने खराब हैं, और जिस तरह से इसे परिभाषित किया गया है वह

73
00:05:04,915 --> 00:05:09,140
प्रशिक्षण डेटा के हजारों टुकड़ों पर नेटवर्क के व्यवहार पर निर्भर करता है।

74
00:05:09,140 --> 00:05:12,460
यह बहुत सोचने वाली बात है.

75
00:05:12,460 --> 00:05:16,380
लेकिन केवल कंप्यूटर को यह बताना कि वह कितना घटिया काम कर रहा है, बहुत मददगार नहीं है।

76
00:05:16,380 --> 00:05:21,300
आप इसे बताना चाहते हैं कि उन वज़न और पूर्वाग्रहों को कैसे बदला जाए ताकि यह बेहतर हो जाए।

77
00:05:21,300 --> 00:05:24,611
इसे आसान बनाने के लिए, 13,000 इनपुट वाले फ़ंक्शन की कल्पना करने

78
00:05:24,611 --> 00:05:27,922
के लिए संघर्ष करने के बजाय, बस एक साधारण फ़ंक्शन की कल्पना करें

79
00:05:27,922 --> 00:05:31,440
जिसमें इनपुट के रूप में एक संख्या और आउटपुट के रूप में एक संख्या हो।

80
00:05:31,440 --> 00:05:36,420
आप ऐसा इनपुट कैसे ढूंढते हैं जो इस फ़ंक्शन के मान को न्यूनतम करता है?

81
00:05:36,420 --> 00:05:41,454
कैलकुलस के छात्रों को पता होगा कि आप कभी-कभी उस न्यूनतम को स्पष्ट रूप से समझ सकते हैं,

82
00:05:41,454 --> 00:05:46,431
लेकिन वास्तव में जटिल कार्यों के लिए यह हमेशा संभव नहीं होता है, निश्चित रूप से हमारे

83
00:05:46,431 --> 00:05:51,640
जटिल जटिल तंत्रिका नेटवर्क लागत फ़ंक्शन के लिए इस स्थिति के 13,000 इनपुट संस्करण में नहीं।

84
00:05:51,640 --> 00:05:55,720
एक अधिक लचीली रणनीति किसी भी इनपुट से शुरू करना है, और यह पता लगाना

85
00:05:55,720 --> 00:05:59,860
है कि उस आउटपुट को कम करने के लिए आपको किस दिशा में कदम बढ़ाना चाहिए।

86
00:05:59,860 --> 00:06:06,035
विशेष रूप से, यदि आप उस फ़ंक्शन के ढलान का पता लगा सकते हैं जहां आप हैं, तो यदि ढलान

87
00:06:06,035 --> 00:06:12,356
सकारात्मक है तो बाईं ओर शिफ्ट करें, और यदि ढलान नकारात्मक है तो इनपुट को दाईं ओर शिफ्ट

88
00:06:12,356 --> 00:06:12,720
करें।

89
00:06:12,720 --> 00:06:16,596
यदि आप ऐसा बार-बार करते हैं, प्रत्येक बिंदु पर नए ढलान की जाँच करते हैं और

90
00:06:16,596 --> 00:06:20,680
उचित कदम उठाते हैं, तो आप फ़ंक्शन के कुछ स्थानीय न्यूनतम तक पहुँचने जा रहे हैं।

91
00:06:20,680 --> 00:06:24,600
और यहां आपके मन में जो छवि होगी वह एक पहाड़ी से लुढ़कती हुई गेंद की होगी।

92
00:06:24,600 --> 00:06:28,302
और ध्यान दें, यहां तक कि इस वास्तव में सरलीकृत एकल इनपुट फ़ंक्शन के लिए भी,

93
00:06:28,302 --> 00:06:32,054
कई संभावित घाटियां हैं जिनमें आप उतर सकते हैं, यह इस पर निर्भर करता है कि आप

94
00:06:32,054 --> 00:06:35,757
किस यादृच्छिक इनपुट से शुरू करते हैं, और इस बात की कोई गारंटी नहीं है कि आप

95
00:06:35,757 --> 00:06:39,460
जिस स्थानीय न्यूनतम पर उतरेंगे वह सबसे छोटा संभव मूल्य होगा लागत फ़ंक्शन का.

96
00:06:39,460 --> 00:06:43,180
यह हमारे तंत्रिका नेटवर्क मामले पर भी लागू होगा।

97
00:06:43,180 --> 00:06:47,688
और मैं यह भी देखना चाहता हूं कि यदि आप अपने कदमों के आकार को ढलान के समानुपाती

98
00:06:47,688 --> 00:06:51,854
बनाते हैं, तो जब ढलान न्यूनतम की ओर समतल हो जाती है, तो आपके कदम छोटे और

99
00:06:51,854 --> 00:06:56,020
छोटे होते जाते हैं, और इस तरह से आपको ओवरशूटिंग से बचने में मदद मिलती है।

100
00:06:56,020 --> 00:07:01,640
जटिलता को थोड़ा बढ़ाते हुए, दो इनपुट और एक आउटपुट वाले फ़ंक्शन की कल्पना करें।

101
00:07:01,640 --> 00:07:05,299
आप इनपुट स्पेस को xy-प्लेन के रूप में सोच सकते हैं, और लागत

102
00:07:05,299 --> 00:07:09,020
फ़ंक्शन को इसके ऊपर की सतह के रूप में ग्राफ़ किया जा सकता है।

103
00:07:09,020 --> 00:07:14,337
फ़ंक्शन के ढलान के बारे में पूछने के बजाय, आपको यह पूछना होगा कि आपको इस इनपुट स्पेस

104
00:07:14,337 --> 00:07:19,780
में किस दिशा में कदम रखना चाहिए ताकि फ़ंक्शन के आउटपुट को सबसे तेज़ी से कम किया जा सके।

105
00:07:19,780 --> 00:07:22,340
दूसरे शब्दों में, ढलान की दिशा क्या है?

106
00:07:22,340 --> 00:07:26,740
और फिर, उस पहाड़ी से लुढ़कती हुई गेंद के बारे में सोचना उपयोगी है।

107
00:07:26,740 --> 00:07:30,885
आपमें से जो लोग मल्टीवेरिएबल कैलकुलस से परिचित हैं, उन्हें पता होगा

108
00:07:30,885 --> 00:07:35,213
कि किसी फ़ंक्शन का ग्रेडिएंट आपको सबसे तेज चढ़ाई की दिशा देता है, आपको

109
00:07:35,213 --> 00:07:39,420
फ़ंक्शन को सबसे तेज़ी से बढ़ाने के लिए किस दिशा में कदम बढ़ाना चाहिए।

110
00:07:39,420 --> 00:07:43,341
स्वाभाविक रूप से, उस ग्रेडिएंट के नकारात्मक को लेने से आपको

111
00:07:43,341 --> 00:07:47,460
उस कदम की दिशा मिलती है जो फ़ंक्शन को सबसे तेज़ी से कम करता है।

112
00:07:47,460 --> 00:07:51,055
इससे भी अधिक, इस ग्रेडिएंट वेक्टर की लंबाई इस बात

113
00:07:51,055 --> 00:07:54,580
का संकेत है कि वह सबसे तीव्र ढलान कितनी तीव्र है।

114
00:07:54,580 --> 00:07:57,817
अब यदि आप मल्टीवेरिएबल कैलकुलस से अपरिचित हैं और अधिक सीखना चाहते हैं,

115
00:07:57,817 --> 00:08:01,100
तो इस विषय पर खान अकादमी के लिए मेरे द्वारा किए गए कुछ कार्यों को देखें।

116
00:08:01,100 --> 00:08:04,746
हालाँकि, ईमानदारी से कहें तो, अभी आपके और मेरे लिए यह सब मायने रखता

117
00:08:04,746 --> 00:08:08,393
है कि सिद्धांत रूप में इस वेक्टर की गणना करने का एक तरीका मौजूद है,

118
00:08:08,393 --> 00:08:12,040
यह वेक्टर आपको बताता है कि ढलान की दिशा क्या है और यह कितनी खड़ी है।

119
00:08:12,040 --> 00:08:14,688
यदि आप इतना ही जानते हैं और विवरण के मामले में

120
00:08:14,688 --> 00:08:17,280
आप ठोस नहीं हैं तो आपको कोई परेशानी नहीं होगी।

121
00:08:17,280 --> 00:08:22,722
क्योंकि यदि आप इसे प्राप्त कर सकते हैं, तो फ़ंक्शन को छोटा करने के लिए एल्गोरिदम

122
00:08:22,722 --> 00:08:28,300
इस ढाल दिशा की गणना करना है, फिर ढलान पर एक छोटा कदम उठाएं, और इसे बार-बार दोहराएं।

123
00:08:28,300 --> 00:08:33,700
यह किसी फ़ंक्शन के लिए वही मूल विचार है जिसमें 2 इनपुट के बजाय 13,000 इनपुट हैं।

124
00:08:33,700 --> 00:08:36,850
हमारे नेटवर्क के सभी 13,000 भारों और पूर्वाग्रहों को

125
00:08:36,850 --> 00:08:40,180
एक विशाल स्तंभ वेक्टर में व्यवस्थित करने की कल्पना करें।

126
00:08:40,180 --> 00:08:45,204
लागत फ़ंक्शन का नकारात्मक ग्रेडिएंट सिर्फ एक वेक्टर है, यह इस अत्यधिक

127
00:08:45,204 --> 00:08:50,516
विशाल इनपुट स्थान के अंदर कुछ दिशा है जो आपको बताता है कि उन सभी संख्याओं

128
00:08:50,516 --> 00:08:55,900
में से कौन सा संकेत लागत फ़ंक्शन में सबसे तेजी से कमी का कारण बनने वाला है।

129
00:08:55,900 --> 00:08:59,607
और निश्चित रूप से, हमारे विशेष रूप से डिज़ाइन किए गए लागत फ़ंक्शन के साथ,

130
00:08:59,607 --> 00:09:03,464
इसे कम करने के लिए वजन और पूर्वाग्रहों को बदलने का मतलब है कि प्रशिक्षण डेटा

131
00:09:03,464 --> 00:09:07,322
के प्रत्येक टुकड़े पर नेटवर्क का आउटपुट 10 मानों की यादृच्छिक सरणी की तरह कम

132
00:09:07,322 --> 00:09:11,280
दिखता है, और वास्तविक निर्णय की तरह अधिक दिखता है जो हम चाहते हैं इसे बनाना है.

133
00:09:11,280 --> 00:09:17,807
यह याद रखना महत्वपूर्ण है, इस लागत फ़ंक्शन में सभी प्रशिक्षण डेटा का औसत शामिल होता है,

134
00:09:17,807 --> 00:09:24,260
इसलिए यदि आप इसे कम करते हैं, तो इसका मतलब है कि यह उन सभी नमूनों पर बेहतर प्रदर्शन है।

135
00:09:24,260 --> 00:09:27,422
इस ग्रेडिएंट की कुशलता से गणना करने के लिए एल्गोरिदम, जो प्रभावी

136
00:09:27,422 --> 00:09:30,925
रूप से एक तंत्रिका नेटवर्क कैसे सीखता है, का मूल है, जिसे बैकप्रॉपैगेशन

137
00:09:30,925 --> 00:09:34,040
कहा जाता है, और मैं अगले वीडियो के बारे में बात करने जा रहा हूं।

138
00:09:34,040 --> 00:09:38,651
वहां, मैं वास्तव में प्रशिक्षण डेटा के दिए गए टुकड़े के लिए प्रत्येक वजन और पूर्वाग्रह

139
00:09:38,651 --> 00:09:43,315
के साथ वास्तव में क्या होता है, इस पर चलने के लिए समय लेना चाहता हूं, प्रासंगिक कैलकुलस

140
00:09:43,315 --> 00:09:47,980
और सूत्रों के ढेर से परे क्या हो रहा है, इसके लिए एक सहज अनुभव देने की कोशिश कर रहा हूं।

141
00:09:47,980 --> 00:09:51,835
यहीं, अभी, मुख्य बात जो मैं आपको जानना चाहता हूं, कार्यान्वयन विवरण

142
00:09:51,835 --> 00:09:55,577
से स्वतंत्र, वह यह है कि जब हम नेटवर्क सीखने के बारे में बात करते

143
00:09:55,577 --> 00:09:59,320
हैं तो हमारा मतलब यह है कि यह सिर्फ एक लागत फ़ंक्शन को कम करना है।

144
00:09:59,320 --> 00:10:04,172
और ध्यान दें, इसका एक परिणाम यह है कि इस लागत फ़ंक्शन के लिए एक अच्छा सुचारू

145
00:10:04,172 --> 00:10:09,340
आउटपुट होना महत्वपूर्ण है, ताकि हम ढलान पर छोटे कदम उठाकर स्थानीय न्यूनतम पा सकें।

146
00:10:09,340 --> 00:10:14,686
यही कारण है कि, वैसे, कृत्रिम न्यूरॉन्स में लगातार सक्रियण होते हैं, न कि केवल

147
00:10:14,686 --> 00:10:20,440
द्विआधारी तरीके से सक्रिय या निष्क्रिय होते हैं, जिस तरह से जैविक न्यूरॉन्स होते हैं।

148
00:10:20,440 --> 00:10:23,775
किसी फ़ंक्शन के इनपुट को नकारात्मक ग्रेडिएंट के कुछ गुणकों द्वारा

149
00:10:23,775 --> 00:10:26,960
बार-बार धकेलने की इस प्रक्रिया को ग्रेडिएंट डिसेंट कहा जाता है।

150
00:10:26,960 --> 00:10:29,868
यह लागत फ़ंक्शन के कुछ स्थानीय न्यूनतम की ओर अभिसरण

151
00:10:29,868 --> 00:10:33,000
करने का एक तरीका है, मूल रूप से इस ग्राफ में एक घाटी है।

152
00:10:33,000 --> 00:10:36,877
बेशक, मैं अभी भी दो इनपुट वाले एक फ़ंक्शन की तस्वीर दिखा रहा हूं,

153
00:10:36,877 --> 00:10:40,872
क्योंकि 13,000 आयामी इनपुट स्पेस में आपके दिमाग को घेरना थोड़ा कठिन

154
00:10:40,872 --> 00:10:45,220
है, लेकिन वास्तव में इसके बारे में सोचने का एक अच्छा गैर-स्थानिक तरीका है।

155
00:10:45,220 --> 00:10:49,100
नकारात्मक प्रवणता का प्रत्येक घटक हमें दो बातें बताता है।

156
00:10:49,100 --> 00:10:52,386
संकेत, निश्चित रूप से, हमें बताता है कि इनपुट वेक्टर

157
00:10:52,386 --> 00:10:55,860
के संबंधित घटक को ऊपर या नीचे झुकाया जाना चाहिए या नहीं।

158
00:10:55,860 --> 00:11:00,524
लेकिन महत्वपूर्ण बात यह है कि इन सभी घटकों का सापेक्ष

159
00:11:00,524 --> 00:11:05,620
परिमाण आपको बताता है कि कौन सा परिवर्तन अधिक मायने रखता है।

160
00:11:05,620 --> 00:11:10,365
आप देखते हैं, हमारे नेटवर्क में, किसी एक वज़न के समायोजन का लागत फ़ंक्शन

161
00:11:10,365 --> 00:11:14,980
पर किसी अन्य वज़न के समायोजन की तुलना में बहुत अधिक प्रभाव पड़ सकता है।

162
00:11:14,980 --> 00:11:19,440
इनमें से कुछ कनेक्शन हमारे प्रशिक्षण डेटा के लिए अधिक मायने रखते हैं।

163
00:11:19,440 --> 00:11:23,010
तो जिस तरह से आप हमारे मन-मस्तिष्क के बड़े पैमाने पर लागत फ़ंक्शन

164
00:11:23,010 --> 00:11:26,472
के इस ग्रेडिएंट वेक्टर के बारे में सोच सकते हैं, वह यह है कि यह

165
00:11:26,472 --> 00:11:30,259
प्रत्येक वजन और पूर्वाग्रह के सापेक्ष महत्व को एनकोड करता है, अर्थात,

166
00:11:30,259 --> 00:11:34,100
इनमें से कौन सा परिवर्तन आपके पैसे के लिए सबसे अधिक धमाका करने वाला है।

167
00:11:34,100 --> 00:11:37,360
यह वास्तव में दिशा के बारे में सोचने का एक और तरीका है।

168
00:11:37,360 --> 00:11:42,553
एक सरल उदाहरण लेने के लिए, यदि आपके पास इनपुट के रूप में दो चर के साथ कुछ फ़ंक्शन

169
00:11:42,553 --> 00:11:47,683
है, और गणना करें कि किसी विशेष बिंदु पर इसका ग्रेडिएंट 3,1 के रूप में आता है, तो

170
00:11:47,683 --> 00:11:52,876
एक तरफ आप इसे यह कहकर व्याख्या कर सकते हैं कि जब आप हैं उस इनपुट पर खड़े होकर, इस

171
00:11:52,876 --> 00:11:58,133
दिशा में आगे बढ़ने से फ़ंक्शन सबसे तेज़ी से बढ़ता है, जब आप इनपुट बिंदुओं के विमान

172
00:11:58,133 --> 00:12:03,200
के ऊपर फ़ंक्शन को ग्राफ़ करते हैं, तो वह वेक्टर आपको सीधे ऊपर की दिशा दे रहा है।

173
00:12:03,200 --> 00:12:07,875
लेकिन इसे पढ़ने का एक और तरीका यह है कि इस पहले चर में परिवर्तन दूसरे चर

174
00:12:07,875 --> 00:12:12,615
में परिवर्तन के रूप में तीन गुना महत्व रखते हैं, कम से कम प्रासंगिक इनपुट

175
00:12:12,615 --> 00:12:17,740
के पड़ोस में, एक्स-वैल्यू को कम करने से आपके लिए बहुत अधिक प्रभाव पड़ता है हिरन.

176
00:12:17,740 --> 00:12:22,880
ठीक है, आइए ज़ूम आउट करें और संक्षेप में बताएं कि हम अब तक कहां हैं।

177
00:12:22,880 --> 00:12:26,908
नेटवर्क स्वयं 784 इनपुट और 10 आउटपुट वाला यह फ़ंक्शन

178
00:12:26,908 --> 00:12:30,860
है, जो इन सभी भारित योगों के संदर्भ में परिभाषित है।

179
00:12:30,860 --> 00:12:34,160
लागत फ़ंक्शन उसके ऊपर जटिलता की एक परत है।

180
00:12:34,160 --> 00:12:38,400
यह 13,000 वज़न और पूर्वाग्रहों को इनपुट के रूप में लेता है,

181
00:12:38,400 --> 00:12:42,640
और प्रशिक्षण उदाहरणों के आधार पर घटियापन का एक माप उगलता है।

182
00:12:42,640 --> 00:12:47,520
लागत फ़ंक्शन का ग्रेडिएंट अभी भी जटिलता की एक और परत है।

183
00:12:47,520 --> 00:12:52,871
यह हमें बताता है कि इन सभी भारों और पूर्वाग्रहों के कारण लागत फ़ंक्शन

184
00:12:52,871 --> 00:12:57,917
के मूल्य में सबसे तेज़ परिवर्तन होता है, जिसे आप यह कहकर व्याख्या

185
00:12:57,917 --> 00:13:03,040
कर सकते हैं कि किस भार में कौन सा परिवर्तन सबसे अधिक मायने रखता है।

186
00:13:03,040 --> 00:13:06,726
तो जब आप नेटवर्क को यादृच्छिक भार और पूर्वाग्रहों के साथ आरंभ करते हैं, और इस

187
00:13:06,726 --> 00:13:10,364
ग्रेडिएंट डिसेंट प्रक्रिया के आधार पर उन्हें कई बार समायोजित करते हैं, तो यह

188
00:13:10,364 --> 00:13:14,240
वास्तव में उन छवियों पर कितना अच्छा प्रदर्शन करता है जो पहले कभी नहीं देखी गई हैं?

189
00:13:14,240 --> 00:13:18,361
जिसका मैंने यहां वर्णन किया है, प्रत्येक 16 न्यूरॉन्स की दो छिपी

190
00:13:18,361 --> 00:13:22,482
हुई परतों के साथ, ज्यादातर सौंदर्य संबंधी कारणों से चुना गया है,

191
00:13:22,482 --> 00:13:26,920
वह बुरा नहीं है, यह लगभग 96% नई छवियों को सही ढंग से वर्गीकृत करता है।

192
00:13:26,920 --> 00:13:31,507
और ईमानदारी से कहूं तो, यदि आप ऐसे कुछ उदाहरणों को देखें जिन पर यह

193
00:13:31,507 --> 00:13:36,300
गड़बड़ करता है, तो आप इसे थोड़ा ढीला करने के लिए बाध्य महसूस करते हैं।

194
00:13:36,300 --> 00:13:38,712
यदि आप छिपी हुई परत संरचना के साथ खेलते हैं और कुछ

195
00:13:38,712 --> 00:13:41,220
बदलाव करते हैं, तो आप इसे 98% तक प्राप्त कर सकते हैं।

196
00:13:41,220 --> 00:13:42,900
और यह बहुत अच्छा है!

197
00:13:42,900 --> 00:13:46,638
यह सबसे अच्छा नहीं है, आप निश्चित रूप से इस सादे वेनिला नेटवर्क की तुलना

198
00:13:46,638 --> 00:13:50,427
में अधिक परिष्कृत होकर बेहतर प्रदर्शन प्राप्त कर सकते हैं, लेकिन यह देखते

199
00:13:50,427 --> 00:13:54,370
हुए कि प्रारंभिक कार्य कितना कठिन है, मुझे लगता है कि किसी भी नेटवर्क द्वारा

200
00:13:54,370 --> 00:13:58,057
छवियों पर इतना अच्छा प्रदर्शन करना अविश्वसनीय है, जैसा कि हमने पहले कभी

201
00:13:58,057 --> 00:14:02,000
नहीं देखा है। इसे विशेष रूप से कभी नहीं बताया गया कि कौन से पैटर्न देखने हैं।

202
00:14:02,000 --> 00:14:06,040
मूल रूप से, जिस तरह से मैंने इस संरचना को प्रेरित किया वह हमारी आशा

203
00:14:06,040 --> 00:14:10,199
का वर्णन करके था, कि दूसरी परत छोटे किनारों को पकड़ सकती है, कि तीसरी

204
00:14:10,199 --> 00:14:14,417
परत लूप और लंबी रेखाओं को पहचानने के लिए उन किनारों को एक साथ जोड़ेगी,

205
00:14:14,417 --> 00:14:18,220
और उन्हें टुकड़े किया जा सकता है अंकों को पहचानने के लिए एक साथ।

206
00:14:18,220 --> 00:14:21,040
तो क्या हमारा नेटवर्क वास्तव में यही कर रहा है?

207
00:14:21,040 --> 00:14:24,960
ख़ैर, कम से कम इस मामले में तो बिल्कुल नहीं।

208
00:14:24,960 --> 00:14:29,232
याद रखें कि पिछले वीडियो में हमने देखा था कि कैसे पहली परत के सभी न्यूरॉन्स

209
00:14:29,232 --> 00:14:33,448
से दूसरी परत के किसी दिए गए न्यूरॉन के कनेक्शन के वजन को एक दिए गए पिक्सेल

210
00:14:33,448 --> 00:14:37,440
पैटर्न के रूप में देखा जा सकता है जिसे दूसरी परत का न्यूरॉन उठा रहा है?

211
00:14:37,440 --> 00:14:45,349
खैर, जब हम इन बदलावों से जुड़े वजनों के लिए ऐसा करते हैं, तो यहां-वहां अलग-अलग छोटे

212
00:14:45,349 --> 00:14:53,823
किनारों को उठाने के बजाय, वे लगभग यादृच्छिक दिखते हैं, बस बीच में कुछ बहुत ढीले पैटर्न के

213
00:14:53,823 --> 00:14:54,200
साथ।

214
00:14:54,200 --> 00:14:58,152
ऐसा प्रतीत होता है कि संभावित भार और पूर्वाग्रहों के अथाह रूप से बड़े

215
00:14:58,152 --> 00:15:01,935
13,000 आयामी स्थान में, हमारे नेटवर्क ने खुद को एक छोटा सा स्थानीय

216
00:15:01,935 --> 00:15:05,718
न्यूनतम पाया, जो कि अधिकांश छवियों को सफलतापूर्वक वर्गीकृत करने के

217
00:15:05,718 --> 00:15:09,840
बावजूद, उन पैटर्नों को बिल्कुल नहीं पकड़ता है जिनकी हम उम्मीद कर सकते थे।

218
00:15:09,840 --> 00:15:12,241
और वास्तव में इस बिंदु को स्पष्ट करने के लिए, देखें कि

219
00:15:12,241 --> 00:15:14,600
जब आप एक यादृच्छिक छवि इनपुट करते हैं तो क्या होता है।

220
00:15:14,600 --> 00:15:18,602
यदि सिस्टम स्मार्ट था, तो आप उम्मीद कर सकते हैं कि यह या तो अनिश्चित महसूस

221
00:15:18,602 --> 00:15:22,712
करेगा, हो सकता है कि वास्तव में उन 10 आउटपुट न्यूरॉन्स में से किसी को सक्रिय

222
00:15:22,712 --> 00:15:26,608
न कर रहा हो या उन सभी को समान रूप से सक्रिय न कर रहा हो, लेकिन इसके बजाय

223
00:15:26,608 --> 00:15:30,610
यह आत्मविश्वास से आपको कुछ बकवास उत्तर देता है, जैसे कि यह निश्चित लगता है

224
00:15:30,610 --> 00:15:34,560
कि यह यादृच्छिक है शोर 5 है क्योंकि ऐसा होता है कि 5 की वास्तविक छवि 5 है।

225
00:15:34,560 --> 00:15:38,091
अलग-अलग शब्दों में, भले ही यह नेटवर्क अंकों को अच्छी तरह से

226
00:15:38,091 --> 00:15:41,800
पहचान सकता है, लेकिन उसे पता नहीं है कि उन्हें कैसे निकालना है।

227
00:15:41,800 --> 00:15:45,400
इसका बहुत कुछ कारण यह है कि यह बहुत सख्ती से प्रतिबंधित प्रशिक्षण व्यवस्था है।

228
00:15:45,400 --> 00:15:48,220
मेरा मतलब है, यहां अपने आप को नेटवर्क की जगह पर रखें।

229
00:15:48,220 --> 00:15:53,049
इसके दृष्टिकोण से, पूरे ब्रह्मांड में एक छोटे ग्रिड में केंद्रित स्पष्ट रूप से परिभाषित

230
00:15:53,049 --> 00:15:57,714
गतिहीन अंकों के अलावा और कुछ नहीं है, और इसके लागत फ़ंक्शन ने इसे कभी भी कुछ भी करने

231
00:15:57,714 --> 00:16:02,160
के लिए कोई प्रोत्साहन नहीं दिया, लेकिन अपने निर्णयों में पूरी तरह से आश्वस्त रहा।

232
00:16:02,160 --> 00:16:04,955
तो इस छवि के साथ कि वे दूसरी परत के न्यूरॉन्स वास्तव में क्या

233
00:16:04,955 --> 00:16:07,524
कर रहे हैं, आपको आश्चर्य हो सकता है कि मैं इस नेटवर्क को

234
00:16:07,524 --> 00:16:10,320
किनारों और पैटर्न को समझने की प्रेरणा के साथ क्यों पेश करूंगा।

235
00:16:10,320 --> 00:16:13,040
मेरा मतलब है, यह बिलकुल भी नहीं है कि यह क्या कर रहा है।

236
00:16:13,040 --> 00:16:17,480
खैर, यह हमारा अंतिम लक्ष्य नहीं है, बल्कि एक शुरुआती बिंदु है।

237
00:16:17,480 --> 00:16:22,712
सच कहूँ तो, यह पुरानी तकनीक है, जिस पर 80 और 90 के दशक में शोध किया गया था, और अधिक

238
00:16:22,712 --> 00:16:27,944
विस्तृत आधुनिक वेरिएंट को समझने से पहले आपको इसे समझने की आवश्यकता है, और यह स्पष्ट

239
00:16:27,944 --> 00:16:33,238
रूप से कुछ दिलचस्प समस्याओं को हल करने में सक्षम है, लेकिन जितना अधिक आप इसमें गहराई

240
00:16:33,238 --> 00:16:38,720
से उतरेंगे वे छिपी हुई परतें वास्तव में काम कर रही हैं, यह उतना ही कम बुद्धिमान लगता है।

241
00:16:38,720 --> 00:16:43,049
एक पल के लिए फोकस इस बात से हटा दें कि नेटवर्क कैसे सीखते हैं कि आप कैसे सीखते

242
00:16:43,049 --> 00:16:47,160
हैं, यह तभी होगा जब आप किसी तरह यहां सामग्री के साथ सक्रिय रूप से जुड़ेंगे।

243
00:16:47,160 --> 00:16:51,966
एक बहुत ही सरल चीज जो मैं आपसे करना चाहता हूं वह यह है कि अभी रुकें और एक पल के

244
00:16:51,966 --> 00:16:56,953
लिए गहराई से सोचें कि आप इस सिस्टम में क्या बदलाव कर सकते हैं और यह छवियों को कैसे

245
00:16:56,953 --> 00:17:01,880
देखता है यदि आप चाहते हैं कि यह किनारों और पैटर्न जैसी चीजों को बेहतर ढंग से उठाए।

246
00:17:01,880 --> 00:17:05,717
लेकिन इससे बेहतर, वास्तव में सामग्री से जुड़ने के लिए, मैं गहन शिक्षण

247
00:17:05,717 --> 00:17:09,720
और तंत्रिका नेटवर्क पर माइकल नीलसन की पुस्तक की अत्यधिक अनुशंसा करता हूं।

248
00:17:09,720 --> 00:17:14,606
इसमें, आप इस सटीक उदाहरण के लिए डाउनलोड करने और खेलने के लिए कोड और डेटा

249
00:17:14,606 --> 00:17:19,360
पा सकते हैं, और पुस्तक आपको चरण दर चरण बताएगी कि वह कोड क्या कर रहा है।

250
00:17:19,360 --> 00:17:22,285
कमाल की बात यह है कि यह पुस्तक मुफ़्त है और सार्वजनिक रूप से

251
00:17:22,285 --> 00:17:25,018
उपलब्ध है, इसलिए यदि आपको इससे कुछ मिलता है, तो नीलसन के

252
00:17:25,018 --> 00:17:28,040
प्रयासों के लिए दान देने में मेरे साथ शामिल होने पर विचार करें।

253
00:17:28,040 --> 00:17:33,593
मैंने विवरण में कुछ अन्य संसाधन भी लिंक किए हैं जो मुझे बहुत पसंद हैं, जिनमें

254
00:17:33,593 --> 00:17:38,720
क्रिस ओला का अभूतपूर्व और सुंदर ब्लॉग पोस्ट और डिस्टिल के लेख शामिल हैं।

255
00:17:38,720 --> 00:17:41,673
अंतिम कुछ मिनटों की बातों को यहीं समाप्त करने के लिए, मैं लीशा

256
00:17:41,673 --> 00:17:44,440
ली के साथ हुए साक्षात्कार के एक अंश पर वापस जाना चाहता हूं।

257
00:17:44,440 --> 00:17:48,560
आपको शायद वह पिछले वीडियो से याद होगी, उसने गहन शिक्षण में पीएचडी की थी।

258
00:17:48,560 --> 00:17:52,402
इस छोटे से स्निपेट में, वह दो हालिया पेपरों के बारे में बात करती है जो वास्तव में इस

259
00:17:52,402 --> 00:17:56,380
बात की पड़ताल करते हैं कि कुछ अधिक आधुनिक छवि पहचान नेटवर्क वास्तव में कैसे सीख रहे हैं।

260
00:17:56,380 --> 00:17:59,553
बस यह स्थापित करने के लिए कि हम बातचीत में कहाँ थे, पहले पेपर ने इन

261
00:17:59,553 --> 00:18:02,820
विशेष रूप से गहरे तंत्रिका नेटवर्क में से एक को लिया जो छवि पहचान में

262
00:18:02,820 --> 00:18:06,273
वास्तव में अच्छा है, और इसे उचित रूप से लेबल किए गए डेटासेट पर प्रशिक्षित

263
00:18:06,273 --> 00:18:09,400
करने के बजाय, इसने प्रशिक्षण से पहले सभी लेबलों को इधर-उधर कर दिया।

264
00:18:09,400 --> 00:18:12,410
जाहिर तौर पर यहां परीक्षण की सटीकता यादृच्छिक से बेहतर नहीं

265
00:18:12,410 --> 00:18:15,320
होगी, क्योंकि हर चीज को यादृच्छिक रूप से लेबल किया गया है।

266
00:18:15,320 --> 00:18:18,308
लेकिन यह अभी भी उसी प्रशिक्षण सटीकता को प्राप्त करने में सक्षम

267
00:18:18,308 --> 00:18:21,440
था जैसा कि आप उचित रूप से लेबल किए गए डेटासेट पर प्राप्त करते हैं।

268
00:18:21,440 --> 00:18:26,511
मूल रूप से, इस विशेष नेटवर्क के लिए लाखों वज़न केवल यादृच्छिक डेटा को याद रखने

269
00:18:26,511 --> 00:18:31,776
के लिए पर्याप्त थे, जो यह सवाल उठाता है कि क्या इस लागत फ़ंक्शन को कम करना वास्तव

270
00:18:31,776 --> 00:18:36,720
में छवि में किसी भी प्रकार की संरचना से मेल खाता है, या यह सिर्फ याद रखना है?

271
00:18:36,720 --> 00:18:36,720
.

272
00:18:36,720 --> 00:18:36,720
.

273
00:18:36,720 --> 00:18:36,720
.

274
00:18:36,720 --> 00:18:40,120
सही वर्गीकरण क्या है, इसके संपूर्ण डेटासेट को याद रखना।

275
00:18:40,120 --> 00:18:44,038
और इसलिए कुछ, आप जानते हैं, आधे साल बाद इस साल आईसीएमएल में, वास्तव

276
00:18:44,038 --> 00:18:47,956
में खंडन पत्र नहीं था, लेकिन पेपर जिसमें कुछ पहलुओं को संबोधित किया

277
00:18:47,956 --> 00:18:52,220
गया था, अरे, वास्तव में ये नेटवर्क उससे थोड़ा अधिक स्मार्ट काम कर रहे हैं।

278
00:18:52,220 --> 00:18:58,787
यदि आप उस सटीकता वक्र को देखते हैं, यदि आप बस एक यादृच्छिक डेटासेट पर प्रशिक्षण ले रहे

279
00:18:58,787 --> 00:19:05,280
थे, तो वह वक्र बहुत नीचे चला गया, आप जानते हैं, लगभग एक रैखिक फैशन में बहुत धीरे-धीरे।

280
00:19:05,280 --> 00:19:08,605
तो आप वास्तव में संभव के उस स्थानीय न्यूनतम को खोजने के लिए

281
00:19:08,605 --> 00:19:12,320
संघर्ष कर रहे हैं, आप जानते हैं, सही वजन जो आपको वह सटीकता दिलाएगा।

282
00:19:12,320 --> 00:19:16,085
जबकि यदि आप वास्तव में एक संरचित डेटासेट पर प्रशिक्षण ले रहे हैं, जिसमें

283
00:19:16,085 --> 00:19:19,645
सही लेबल हैं, तो आप जानते हैं, आप शुरुआत में थोड़ा इधर-उधर करते हैं,

284
00:19:19,645 --> 00:19:23,360
लेकिन फिर आप उस सटीकता स्तर तक पहुंचने के लिए बहुत तेजी से गिर जाते हैं।

285
00:19:23,360 --> 00:19:28,580
और इसलिए कुछ अर्थों में उस स्थानीय मैक्सिमा को खोजना आसान था।

286
00:19:28,580 --> 00:19:34,288
और इसलिए इसके बारे में दिलचस्प बात यह है कि यह वास्तव में कुछ साल पहले के एक और

287
00:19:34,288 --> 00:19:40,140
पेपर को प्रकाश में लाता है, जिसमें नेटवर्क परतों के बारे में बहुत अधिक सरलीकरण है।

288
00:19:40,140 --> 00:19:44,740
लेकिन परिणामों में से एक यह बता रहा था कि, यदि आप अनुकूलन परिदृश्य को देखें,

289
00:19:44,740 --> 00:19:49,400
तो ये नेटवर्क जो स्थानीय मिनीमा सीखते हैं, वे वास्तव में समान गुणवत्ता के हैं।

290
00:19:49,400 --> 00:19:53,858
तो कुछ अर्थों में, यदि आपका डेटा सेट संरचित है, तो

291
00:19:53,858 --> 00:19:58,580
आपको इसे और अधिक आसानी से ढूंढने में सक्षम होना चाहिए।

292
00:19:58,580 --> 00:20:01,480
आपमें से पैट्रियन का समर्थन करने वालों को हमेशा की तरह मेरा धन्यवाद।

293
00:20:01,480 --> 00:20:04,372
मैंने पहले ही कहा है कि पैट्रियन पर गेम चेंजर क्या है,

294
00:20:04,372 --> 00:20:07,160
लेकिन ये वीडियो वास्तव में आपके बिना संभव नहीं होंगे।

295
00:20:07,160 --> 00:20:19,150
मैं वीसी फर्म एम्प्लीफाई पार्टनर्स और श्रृंखला के इन शुरुआती

296
00:20:19,150 --> 00:20:31,140
वीडियो के लिए उनके समर्थन को भी विशेष धन्यवाद देना चाहता हूं।

297
00:20:31,140 --> 00:20:31,140
धन्यवाद।

