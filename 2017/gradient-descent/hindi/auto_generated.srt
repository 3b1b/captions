1
00:00:00,000 --> 00:00:07,240
पिछले वीडियो में मैंने एक तंत्रिका नेटवर्क की संरचना बताई थी।

2
00:00:07,240 --> 00:00:10,742
मैं यहां एक संक्षिप्त पुनर्कथन दूंगा ताकि यह हमारे दिमाग में ताजा रहे,

3
00:00:10,742 --> 00:00:13,160
और फिर इस वीडियो के लिए मेरे दो मुख्य लक्ष्य हैं।

4
00:00:13,160 --> 00:00:16,059
सबसे पहले ग्रेडिएंट डिसेंट के विचार को पेश करना है,

5
00:00:16,059 --> 00:00:20,800
जो न केवल तंत्रिका नेटवर्क कैसे सीखते हैं, बल्कि कई अन्य मशीन लर्निंग भी काम करता है।

6
00:00:20,800 --> 00:00:26,235
फिर उसके बाद हम थोड़ा और गहराई से देखेंगे कि यह विशेष नेटवर्क कैसा प्रदर्शन करता है,

7
00:00:26,235 --> 00:00:29,560
और न्यूरॉन्स की छिपी हुई परतें आखिर क्या ढूंढती हैं।

8
00:00:29,560 --> 00:00:34,849
एक अनुस्मारक के रूप में, हमारा लक्ष्य यहां हस्तलिखित अंक पहचान का उत्कृष्ट उदाहरण,

9
00:00:34,849 --> 00:00:37,080
तंत्रिका नेटवर्क की हैलो दुनिया है।

10
00:00:37,080 --> 00:00:40,389
ये अंक 28x28 पिक्सेल ग्रिड पर प्रस्तुत किए जाते हैं,

11
00:00:40,389 --> 00:00:44,260
प्रत्येक पिक्सेल में 0 और 1 के बीच कुछ ग्रेस्केल मान होते हैं।

12
00:00:44,260 --> 00:00:51,400
वे ही नेटवर्क की इनपुट परत में 784 न्यूरॉन्स की सक्रियता निर्धारित करते हैं।

13
00:00:51,400 --> 00:00:56,882
निम्नलिखित परतों में प्रत्येक न्यूरॉन के लिए सक्रियता पिछली परत में सभी सक्रियणों के

14
00:00:56,882 --> 00:01:02,300
भारित योग के साथ-साथ कुछ विशेष संख्या पर आधारित होती है जिसे पूर्वाग्रह कहा जाता है।

15
00:01:02,300 --> 00:01:06,715
आप उस राशि को किसी अन्य फ़ंक्शन के साथ बनाते हैं, जैसे सिग्मॉइड स्क्विशिफिकेशन,

16
00:01:06,715 --> 00:01:09,640
या एक ReLU, जिस तरह से मैंने पिछले वीडियो को देखा था।

17
00:01:09,640 --> 00:01:14,824
कुल मिलाकर, प्रत्येक 16 न्यूरॉन्स वाली दो छिपी हुई परतों की कुछ हद तक मनमानी पसंद

18
00:01:14,824 --> 00:01:20,072
को देखते हुए, नेटवर्क में लगभग 13,000 वजन और पूर्वाग्रह हैं जिन्हें हम समायोजित कर

19
00:01:20,072 --> 00:01:25,320
सकते हैं, और ये मूल्य हैं जो निर्धारित करते हैं कि नेटवर्क वास्तव में क्या करता है।

20
00:01:25,320 --> 00:01:29,754
और जब हम कहते हैं कि यह नेटवर्क किसी दिए गए अंक को वर्गीकृत करता है तो हमारा मतलब

21
00:01:29,754 --> 00:01:34,080
यह है कि अंतिम परत में उन 10 न्यूरॉन्स में से सबसे चमकीला उस अंक से मेल खाता है।

22
00:01:34,080 --> 00:01:39,408
और याद रखें, स्तरित संरचना के लिए हमारे मन में प्रेरणा यह थी कि शायद दूसरी

23
00:01:39,408 --> 00:01:45,163
परत किनारों को पकड़ सकती है, तीसरी परत लूप और रेखाओं जैसे पैटर्न को चुन सकती है,

24
00:01:45,163 --> 00:01:49,640
और आखिरी परत उन पैटर्न को एक साथ जोड़ सकती है अंकों को पहचानें.

25
00:01:49,640 --> 00:01:52,880
तो यहां, हम सीखते हैं कि नेटवर्क कैसे सीखता है।

26
00:01:52,880 --> 00:01:57,321
हम जो चाहते हैं वह एक एल्गोरिदम है जहां आप इस नेटवर्क को प्रशिक्षण डेटा का एक

27
00:01:57,321 --> 00:02:02,389
पूरा समूह दिखा सकते हैं, जो हस्तलिखित अंकों की विभिन्न छवियों के समूह के रूप में आता है,

28
00:02:02,389 --> 00:02:06,887
साथ ही उनके लिए लेबल भी होते हैं, और यह होगा उन 13,000 वज़न और पूर्वाग्रहों को

29
00:02:06,887 --> 00:02:10,760
समायोजित करें ताकि प्रशिक्षण डेटा पर इसके प्रदर्शन में सुधार हो सके।

30
00:02:10,760 --> 00:02:14,356
उम्मीद है कि इस स्तरित संरचना का मतलब यह होगा कि यह जो सीखता है

31
00:02:14,356 --> 00:02:17,840
वह उस प्रशिक्षण डेटा से परे छवियों के लिए सामान्यीकृत होता है।

32
00:02:17,840 --> 00:02:23,568
जिस तरह से हम इसका परीक्षण करते हैं वह यह है कि नेटवर्क को प्रशिक्षित करने के बाद,

33
00:02:23,568 --> 00:02:27,985
आप इसे अधिक लेबल वाला डेटा दिखाते हैं, और आप देखते हैं कि यह उन

34
00:02:27,985 --> 00:02:31,160
नई छवियों को कितनी सटीकता से वर्गीकृत करता है।

35
00:02:31,160 --> 00:02:35,480
सौभाग्य से हमारे लिए, और जो बात इसे शुरू करने के लिए एक सामान्य उदाहरण बनाती है,

36
00:02:35,480 --> 00:02:40,226
वह यह है कि एमएनआईएसटी डेटाबेस के पीछे अच्छे लोगों ने हजारों हस्तलिखित अंकों वाली छवियों

37
00:02:40,226 --> 00:02:44,760
का एक संग्रह रखा है, प्रत्येक को उन संख्याओं के साथ लेबल किया गया है जिन्हें वे होना

38
00:02:44,760 --> 00:02:45,080
चाहिए।

39
00:02:45,080 --> 00:02:48,375
और एक मशीन को सीखने के रूप में वर्णित करना जितना उत्तेजक है,

40
00:02:48,375 --> 00:02:50,914
एक बार जब आप देखते हैं कि यह कैसे काम करता है,

41
00:02:50,914 --> 00:02:55,560
तो यह किसी पागल विज्ञान-फाई आधार की तरह कम और कैलकुलस अभ्यास की तरह बहुत अधिक लगता है।

42
00:02:55,560 --> 00:03:01,040
मेरा मतलब है, मूल रूप से यह एक निश्चित फ़ंक्शन का न्यूनतम पता लगाने पर निर्भर करता है।

43
00:03:01,040 --> 00:03:07,263
याद रखें, वैचारिक रूप से हम प्रत्येक न्यूरॉन को पिछली परत के सभी न्यूरॉन्स से जुड़े होने

44
00:03:07,263 --> 00:03:13,137
के रूप में सोच रहे हैं, और इसकी सक्रियता को परिभाषित करने वाले भारित योग में वजन उन

45
00:03:13,137 --> 00:03:19,080
कनेक्शनों की ताकत की तरह है, और पूर्वाग्रह कुछ संकेत है चाहे वह न्यूरॉन सक्रिय हो या

46
00:03:19,080 --> 00:03:19,780
निष्क्रिय।

47
00:03:19,780 --> 00:03:22,533
और चीजों को शुरू करने के लिए, हम उन सभी भारों और पूर्वाग्रहों

48
00:03:22,533 --> 00:03:25,020
को पूरी तरह से यादृच्छिक रूप से प्रारंभ करने जा रहे हैं।

49
00:03:25,020 --> 00:03:27,966
कहने की जरूरत नहीं है, यह नेटवर्क किसी दिए गए प्रशिक्षण उदाहरण पर

50
00:03:27,966 --> 00:03:31,180
भयानक प्रदर्शन करने जा रहा है, क्योंकि यह सिर्फ कुछ यादृच्छिक कर रहा है।

51
00:03:31,180 --> 00:03:34,154
उदाहरण के लिए, आप 3 की इस छवि को फ़ीड करते हैं,

52
00:03:34,154 --> 00:03:36,820
और आउटपुट परत बस एक गड़बड़ की तरह दिखती है।

53
00:03:36,820 --> 00:03:40,146
तो आप जो करते हैं वह एक लागत फ़ंक्शन को परिभाषित करना है,

54
00:03:40,146 --> 00:03:43,473
कंप्यूटर को यह बताने का एक तरीका है, नहीं, खराब कंप्यूटर,

55
00:03:43,473 --> 00:03:47,316
उस आउटपुट में सक्रियण होना चाहिए जो अधिकांश न्यूरॉन्स के लिए 0 है,

56
00:03:47,316 --> 00:03:48,980
लेकिन इस न्यूरॉन के लिए 1 है।

57
00:03:48,980 --> 00:03:51,740
तुमने मुझे जो दिया वह बिल्कुल कूड़ा है।

58
00:03:51,740 --> 00:03:56,719
इसे थोड़ा और गणितीय रूप से कहने के लिए, आप उन कचरा आउटपुट सक्रियणों

59
00:03:56,719 --> 00:04:02,651
में से प्रत्येक के बीच अंतर के वर्गों को जोड़ते हैं और वह मूल्य जो आप चाहते हैं,

60
00:04:02,651 --> 00:04:06,020
और इसे हम एकल प्रशिक्षण उदाहरण की लागत कहेंगे।

61
00:04:06,020 --> 00:04:10,264
ध्यान दें कि यह राशि तब छोटी होती है जब नेटवर्क आत्मविश्वास से

62
00:04:10,264 --> 00:04:14,575
छवि को सही ढंग से वर्गीकृत करता है, लेकिन यह तब बड़ी होती है जब

63
00:04:14,575 --> 00:04:18,820
नेटवर्क को ऐसा लगता है कि उसे पता नहीं है कि वह क्या कर रहा है।

64
00:04:18,820 --> 00:04:27,580
तो फिर आप अपने पास मौजूद हजारों प्रशिक्षण उदाहरणों की औसत लागत पर विचार करें।

65
00:04:27,580 --> 00:04:30,327
यह औसत लागत इस बात का पैमाना है कि नेटवर्क कितना

66
00:04:30,327 --> 00:04:33,300
ख़राब है और कंप्यूटर को कितना ख़राब महसूस होना चाहिए।

67
00:04:33,300 --> 00:04:35,300
और यह एक जटिल बात है.

68
00:04:35,300 --> 00:04:41,288
याद रखें कि नेटवर्क मूल रूप से एक फ़ंक्शन कैसे था, जो इनपुट के रूप में 784 नंबर लेता है,

69
00:04:41,288 --> 00:04:44,922
पिक्सेल मान, और आउटपुट के रूप में 10 नंबर निकालता है,

70
00:04:44,922 --> 00:04:49,700
और एक अर्थ में यह इन सभी भारों और पूर्वाग्रहों द्वारा पैरामीटरयुक्त है?

71
00:04:49,700 --> 00:04:53,340
लागत फ़ंक्शन उसके ऊपर जटिलता की एक परत है।

72
00:04:53,340 --> 00:04:57,970
यह अपने इनपुट के रूप में उन 13,000 या उससे अधिक वजन और पूर्वाग्रहों को लेता है,

73
00:04:57,970 --> 00:05:02,484
और एक एकल संख्या बताता है जो बताता है कि वे वजन और पूर्वाग्रह कितने खराब हैं,

74
00:05:02,484 --> 00:05:06,361
और जिस तरह से इसे परिभाषित किया गया है वह प्रशिक्षण डेटा के हजारों

75
00:05:06,361 --> 00:05:09,140
टुकड़ों पर नेटवर्क के व्यवहार पर निर्भर करता है।

76
00:05:09,140 --> 00:05:12,460
यह बहुत सोचने वाली बात है.

77
00:05:12,460 --> 00:05:16,380
लेकिन केवल कंप्यूटर को यह बताना कि वह कितना घटिया काम कर रहा है, बहुत मददगार नहीं है।

78
00:05:16,380 --> 00:05:21,300
आप इसे बताना चाहते हैं कि उन वज़न और पूर्वाग्रहों को कैसे बदला जाए ताकि यह बेहतर हो जाए।

79
00:05:21,300 --> 00:05:24,611
इसे आसान बनाने के लिए, 13,000 इनपुट वाले फ़ंक्शन की कल्पना करने

80
00:05:24,611 --> 00:05:27,922
के लिए संघर्ष करने के बजाय, बस एक साधारण फ़ंक्शन की कल्पना करें

81
00:05:27,922 --> 00:05:31,440
जिसमें इनपुट के रूप में एक संख्या और आउटपुट के रूप में एक संख्या हो।

82
00:05:31,440 --> 00:05:36,420
आप ऐसा इनपुट कैसे ढूंढते हैं जो इस फ़ंक्शन के मान को न्यूनतम करता है?

83
00:05:36,420 --> 00:05:41,454
कैलकुलस के छात्रों को पता होगा कि आप कभी-कभी उस न्यूनतम को स्पष्ट रूप से समझ सकते हैं,

84
00:05:41,454 --> 00:05:45,216
लेकिन वास्तव में जटिल कार्यों के लिए यह हमेशा संभव नहीं होता है,

85
00:05:45,216 --> 00:05:50,308
निश्चित रूप से हमारे जटिल जटिल तंत्रिका नेटवर्क लागत फ़ंक्शन के लिए इस स्थिति के 13,000

86
00:05:50,308 --> 00:05:51,640
इनपुट संस्करण में नहीं।

87
00:05:51,640 --> 00:05:54,760
एक अधिक लचीली रणनीति किसी भी इनपुट से शुरू करना है,

88
00:05:54,760 --> 00:05:59,860
और यह पता लगाना है कि उस आउटपुट को कम करने के लिए आपको किस दिशा में कदम बढ़ाना चाहिए।

89
00:05:59,860 --> 00:06:05,163
विशेष रूप से, यदि आप उस फ़ंक्शन के ढलान का पता लगा सकते हैं जहां आप हैं,

90
00:06:05,163 --> 00:06:08,651
तो यदि ढलान सकारात्मक है तो बाईं ओर शिफ्ट करें,

91
00:06:08,651 --> 00:06:12,720
और यदि ढलान नकारात्मक है तो इनपुट को दाईं ओर शिफ्ट करें।

92
00:06:12,720 --> 00:06:16,596
यदि आप ऐसा बार-बार करते हैं, प्रत्येक बिंदु पर नए ढलान की जाँच करते हैं और

93
00:06:16,596 --> 00:06:20,680
उचित कदम उठाते हैं, तो आप फ़ंक्शन के कुछ स्थानीय न्यूनतम तक पहुँचने जा रहे हैं।

94
00:06:20,680 --> 00:06:24,600
और यहां आपके मन में जो छवि होगी वह एक पहाड़ी से लुढ़कती हुई गेंद की होगी।

95
00:06:24,600 --> 00:06:28,302
और ध्यान दें, यहां तक कि इस वास्तव में सरलीकृत एकल इनपुट फ़ंक्शन के लिए भी,

96
00:06:28,302 --> 00:06:30,592
कई संभावित घाटियां हैं जिनमें आप उतर सकते हैं,

97
00:06:30,592 --> 00:06:33,905
यह इस पर निर्भर करता है कि आप किस यादृच्छिक इनपुट से शुरू करते हैं,

98
00:06:33,905 --> 00:06:37,657
और इस बात की कोई गारंटी नहीं है कि आप जिस स्थानीय न्यूनतम पर उतरेंगे वह सबसे

99
00:06:37,657 --> 00:06:39,460
छोटा संभव मूल्य होगा लागत फ़ंक्शन का.

100
00:06:39,460 --> 00:06:43,180
यह हमारे तंत्रिका नेटवर्क मामले पर भी लागू होगा।

101
00:06:43,180 --> 00:06:48,316
और मैं यह भी देखना चाहता हूं कि यदि आप अपने कदमों के आकार को ढलान के समानुपाती बनाते हैं,

102
00:06:48,316 --> 00:06:52,995
तो जब ढलान न्यूनतम की ओर समतल हो जाती है, तो आपके कदम छोटे और छोटे होते जाते हैं,

103
00:06:52,995 --> 00:06:56,020
और इस तरह से आपको ओवरशूटिंग से बचने में मदद मिलती है।

104
00:06:56,020 --> 00:07:01,640
जटिलता को थोड़ा बढ़ाते हुए, दो इनपुट और एक आउटपुट वाले फ़ंक्शन की कल्पना करें।

105
00:07:01,640 --> 00:07:04,811
आप इनपुट स्पेस को xy-प्लेन के रूप में सोच सकते हैं,

106
00:07:04,811 --> 00:07:09,020
और लागत फ़ंक्शन को इसके ऊपर की सतह के रूप में ग्राफ़ किया जा सकता है।

107
00:07:09,020 --> 00:07:14,337
फ़ंक्शन के ढलान के बारे में पूछने के बजाय, आपको यह पूछना होगा कि आपको इस इनपुट स्पेस

108
00:07:14,337 --> 00:07:19,780
में किस दिशा में कदम रखना चाहिए ताकि फ़ंक्शन के आउटपुट को सबसे तेज़ी से कम किया जा सके।

109
00:07:19,780 --> 00:07:22,340
दूसरे शब्दों में, ढलान की दिशा क्या है?

110
00:07:22,340 --> 00:07:26,740
और फिर, उस पहाड़ी से लुढ़कती हुई गेंद के बारे में सोचना उपयोगी है।

111
00:07:26,740 --> 00:07:29,910
आपमें से जो लोग मल्टीवेरिएबल कैलकुलस से परिचित हैं,

112
00:07:29,910 --> 00:07:34,908
उन्हें पता होगा कि किसी फ़ंक्शन का ग्रेडिएंट आपको सबसे तेज चढ़ाई की दिशा देता है,

113
00:07:34,908 --> 00:07:39,420
आपको फ़ंक्शन को सबसे तेज़ी से बढ़ाने के लिए किस दिशा में कदम बढ़ाना चाहिए।

114
00:07:39,420 --> 00:07:43,341
स्वाभाविक रूप से, उस ग्रेडिएंट के नकारात्मक को लेने से आपको

115
00:07:43,341 --> 00:07:47,460
उस कदम की दिशा मिलती है जो फ़ंक्शन को सबसे तेज़ी से कम करता है।

116
00:07:47,460 --> 00:07:51,055
इससे भी अधिक, इस ग्रेडिएंट वेक्टर की लंबाई इस बात

117
00:07:51,055 --> 00:07:54,580
का संकेत है कि वह सबसे तीव्र ढलान कितनी तीव्र है।

118
00:07:54,580 --> 00:07:57,817
अब यदि आप मल्टीवेरिएबल कैलकुलस से अपरिचित हैं और अधिक सीखना चाहते हैं,

119
00:07:57,817 --> 00:08:01,100
तो इस विषय पर खान अकादमी के लिए मेरे द्वारा किए गए कुछ कार्यों को देखें।

120
00:08:01,100 --> 00:08:04,746
हालाँकि, ईमानदारी से कहें तो, अभी आपके और मेरे लिए यह सब मायने रखता

121
00:08:04,746 --> 00:08:08,393
है कि सिद्धांत रूप में इस वेक्टर की गणना करने का एक तरीका मौजूद है,

122
00:08:08,393 --> 00:08:12,040
यह वेक्टर आपको बताता है कि ढलान की दिशा क्या है और यह कितनी खड़ी है।

123
00:08:12,040 --> 00:08:14,688
यदि आप इतना ही जानते हैं और विवरण के मामले में

124
00:08:14,688 --> 00:08:17,280
आप ठोस नहीं हैं तो आपको कोई परेशानी नहीं होगी।

125
00:08:17,280 --> 00:08:22,722
क्योंकि यदि आप इसे प्राप्त कर सकते हैं, तो फ़ंक्शन को छोटा करने के लिए एल्गोरिदम

126
00:08:22,722 --> 00:08:28,300
इस ढाल दिशा की गणना करना है, फिर ढलान पर एक छोटा कदम उठाएं, और इसे बार-बार दोहराएं।

127
00:08:28,300 --> 00:08:33,700
यह किसी फ़ंक्शन के लिए वही मूल विचार है जिसमें 2 इनपुट के बजाय 13,000 इनपुट हैं।

128
00:08:33,700 --> 00:08:36,850
हमारे नेटवर्क के सभी 13,000 भारों और पूर्वाग्रहों को

129
00:08:36,850 --> 00:08:40,180
एक विशाल स्तंभ वेक्टर में व्यवस्थित करने की कल्पना करें।

130
00:08:40,180 --> 00:08:44,199
लागत फ़ंक्शन का नकारात्मक ग्रेडिएंट सिर्फ एक वेक्टर है,

131
00:08:44,199 --> 00:08:49,367
यह इस अत्यधिक विशाल इनपुट स्थान के अंदर कुछ दिशा है जो आपको बताता है कि

132
00:08:49,367 --> 00:08:54,607
उन सभी संख्याओं में से कौन सा संकेत लागत फ़ंक्शन में सबसे तेजी से कमी का

133
00:08:54,607 --> 00:08:55,900
कारण बनने वाला है।

134
00:08:55,900 --> 00:08:59,607
और निश्चित रूप से, हमारे विशेष रूप से डिज़ाइन किए गए लागत फ़ंक्शन के साथ,

135
00:08:59,607 --> 00:09:03,464
इसे कम करने के लिए वजन और पूर्वाग्रहों को बदलने का मतलब है कि प्रशिक्षण डेटा

136
00:09:03,464 --> 00:09:07,823
के प्रत्येक टुकड़े पर नेटवर्क का आउटपुट 10 मानों की यादृच्छिक सरणी की तरह कम दिखता है,

137
00:09:07,823 --> 00:09:11,280
और वास्तविक निर्णय की तरह अधिक दिखता है जो हम चाहते हैं इसे बनाना है.

138
00:09:11,280 --> 00:09:17,807
यह याद रखना महत्वपूर्ण है, इस लागत फ़ंक्शन में सभी प्रशिक्षण डेटा का औसत शामिल होता है,

139
00:09:17,807 --> 00:09:24,260
इसलिए यदि आप इसे कम करते हैं, तो इसका मतलब है कि यह उन सभी नमूनों पर बेहतर प्रदर्शन है।

140
00:09:24,260 --> 00:09:26,887
इस ग्रेडिएंट की कुशलता से गणना करने के लिए एल्गोरिदम,

141
00:09:26,887 --> 00:09:30,001
जो प्रभावी रूप से एक तंत्रिका नेटवर्क कैसे सीखता है, का मूल है,

142
00:09:30,001 --> 00:09:34,040
जिसे बैकप्रॉपैगेशन कहा जाता है, और मैं अगले वीडियो के बारे में बात करने जा रहा हूं।

143
00:09:34,040 --> 00:09:38,651
वहां, मैं वास्तव में प्रशिक्षण डेटा के दिए गए टुकड़े के लिए प्रत्येक वजन और पूर्वाग्रह

144
00:09:38,651 --> 00:09:42,361
के साथ वास्तव में क्या होता है, इस पर चलने के लिए समय लेना चाहता हूं,

145
00:09:42,361 --> 00:09:45,488
प्रासंगिक कैलकुलस और सूत्रों के ढेर से परे क्या हो रहा है,

146
00:09:45,488 --> 00:09:47,980
इसके लिए एक सहज अनुभव देने की कोशिश कर रहा हूं।

147
00:09:47,980 --> 00:09:52,572
यहीं, अभी, मुख्य बात जो मैं आपको जानना चाहता हूं, कार्यान्वयन विवरण से स्वतंत्र,

148
00:09:52,572 --> 00:09:56,314
वह यह है कि जब हम नेटवर्क सीखने के बारे में बात करते हैं तो हमारा

149
00:09:56,314 --> 00:09:59,320
मतलब यह है कि यह सिर्फ एक लागत फ़ंक्शन को कम करना है।

150
00:09:59,320 --> 00:10:04,172
और ध्यान दें, इसका एक परिणाम यह है कि इस लागत फ़ंक्शन के लिए एक अच्छा सुचारू

151
00:10:04,172 --> 00:10:09,340
आउटपुट होना महत्वपूर्ण है, ताकि हम ढलान पर छोटे कदम उठाकर स्थानीय न्यूनतम पा सकें।

152
00:10:09,340 --> 00:10:14,010
यही कारण है कि, वैसे, कृत्रिम न्यूरॉन्स में लगातार सक्रियण होते हैं,

153
00:10:14,010 --> 00:10:18,003
न कि केवल द्विआधारी तरीके से सक्रिय या निष्क्रिय होते हैं,

154
00:10:18,003 --> 00:10:20,440
जिस तरह से जैविक न्यूरॉन्स होते हैं।

155
00:10:20,440 --> 00:10:23,775
किसी फ़ंक्शन के इनपुट को नकारात्मक ग्रेडिएंट के कुछ गुणकों द्वारा

156
00:10:23,775 --> 00:10:26,960
बार-बार धकेलने की इस प्रक्रिया को ग्रेडिएंट डिसेंट कहा जाता है।

157
00:10:26,960 --> 00:10:31,042
यह लागत फ़ंक्शन के कुछ स्थानीय न्यूनतम की ओर अभिसरण करने का एक तरीका है,

158
00:10:31,042 --> 00:10:33,000
मूल रूप से इस ग्राफ में एक घाटी है।

159
00:10:33,000 --> 00:10:36,877
बेशक, मैं अभी भी दो इनपुट वाले एक फ़ंक्शन की तस्वीर दिखा रहा हूं,

160
00:10:36,877 --> 00:10:41,107
क्योंकि 13,000 आयामी इनपुट स्पेस में आपके दिमाग को घेरना थोड़ा कठिन है,

161
00:10:41,107 --> 00:10:45,220
लेकिन वास्तव में इसके बारे में सोचने का एक अच्छा गैर-स्थानिक तरीका है।

162
00:10:45,220 --> 00:10:49,100
नकारात्मक प्रवणता का प्रत्येक घटक हमें दो बातें बताता है।

163
00:10:49,100 --> 00:10:52,386
संकेत, निश्चित रूप से, हमें बताता है कि इनपुट वेक्टर

164
00:10:52,386 --> 00:10:55,860
के संबंधित घटक को ऊपर या नीचे झुकाया जाना चाहिए या नहीं।

165
00:10:55,860 --> 00:11:00,524
लेकिन महत्वपूर्ण बात यह है कि इन सभी घटकों का सापेक्ष

166
00:11:00,524 --> 00:11:05,620
परिमाण आपको बताता है कि कौन सा परिवर्तन अधिक मायने रखता है।

167
00:11:05,620 --> 00:11:10,365
आप देखते हैं, हमारे नेटवर्क में, किसी एक वज़न के समायोजन का लागत फ़ंक्शन

168
00:11:10,365 --> 00:11:14,980
पर किसी अन्य वज़न के समायोजन की तुलना में बहुत अधिक प्रभाव पड़ सकता है।

169
00:11:14,980 --> 00:11:19,440
इनमें से कुछ कनेक्शन हमारे प्रशिक्षण डेटा के लिए अधिक मायने रखते हैं।

170
00:11:19,440 --> 00:11:23,010
तो जिस तरह से आप हमारे मन-मस्तिष्क के बड़े पैमाने पर लागत फ़ंक्शन

171
00:11:23,010 --> 00:11:25,661
के इस ग्रेडिएंट वेक्टर के बारे में सोच सकते हैं,

172
00:11:25,661 --> 00:11:29,826
वह यह है कि यह प्रत्येक वजन और पूर्वाग्रह के सापेक्ष महत्व को एनकोड करता है,

173
00:11:29,826 --> 00:11:34,100
अर्थात, इनमें से कौन सा परिवर्तन आपके पैसे के लिए सबसे अधिक धमाका करने वाला है।

174
00:11:34,100 --> 00:11:37,360
यह वास्तव में दिशा के बारे में सोचने का एक और तरीका है।

175
00:11:37,360 --> 00:11:42,806
एक सरल उदाहरण लेने के लिए, यदि आपके पास इनपुट के रूप में दो चर के साथ कुछ फ़ंक्शन है,

176
00:11:42,806 --> 00:11:47,493
और गणना करें कि किसी विशेष बिंदु पर इसका ग्रेडिएंट 3,1 के रूप में आता है,

177
00:11:47,493 --> 00:11:52,686
तो एक तरफ आप इसे यह कहकर व्याख्या कर सकते हैं कि जब आप हैं उस इनपुट पर खड़े होकर,

178
00:11:52,686 --> 00:11:56,296
इस दिशा में आगे बढ़ने से फ़ंक्शन सबसे तेज़ी से बढ़ता है,

179
00:11:56,296 --> 00:12:00,350
जब आप इनपुट बिंदुओं के विमान के ऊपर फ़ंक्शन को ग्राफ़ करते हैं,

180
00:12:00,350 --> 00:12:03,200
तो वह वेक्टर आपको सीधे ऊपर की दिशा दे रहा है।

181
00:12:03,200 --> 00:12:07,875
लेकिन इसे पढ़ने का एक और तरीका यह है कि इस पहले चर में परिवर्तन दूसरे चर

182
00:12:07,875 --> 00:12:13,512
में परिवर्तन के रूप में तीन गुना महत्व रखते हैं, कम से कम प्रासंगिक इनपुट के पड़ोस में,

183
00:12:13,512 --> 00:12:17,740
एक्स-वैल्यू को कम करने से आपके लिए बहुत अधिक प्रभाव पड़ता है हिरन.

184
00:12:17,740 --> 00:12:22,880
ठीक है, आइए ज़ूम आउट करें और संक्षेप में बताएं कि हम अब तक कहां हैं।

185
00:12:22,880 --> 00:12:27,212
नेटवर्क स्वयं 784 इनपुट और 10 आउटपुट वाला यह फ़ंक्शन है,

186
00:12:27,212 --> 00:12:30,860
जो इन सभी भारित योगों के संदर्भ में परिभाषित है।

187
00:12:30,860 --> 00:12:34,160
लागत फ़ंक्शन उसके ऊपर जटिलता की एक परत है।

188
00:12:34,160 --> 00:12:38,400
यह 13,000 वज़न और पूर्वाग्रहों को इनपुट के रूप में लेता है,

189
00:12:38,400 --> 00:12:42,640
और प्रशिक्षण उदाहरणों के आधार पर घटियापन का एक माप उगलता है।

190
00:12:42,640 --> 00:12:47,520
लागत फ़ंक्शन का ग्रेडिएंट अभी भी जटिलता की एक और परत है।

191
00:12:47,520 --> 00:12:52,871
यह हमें बताता है कि इन सभी भारों और पूर्वाग्रहों के कारण लागत फ़ंक्शन

192
00:12:52,871 --> 00:12:57,917
के मूल्य में सबसे तेज़ परिवर्तन होता है, जिसे आप यह कहकर व्याख्या

193
00:12:57,917 --> 00:13:03,040
कर सकते हैं कि किस भार में कौन सा परिवर्तन सबसे अधिक मायने रखता है।

194
00:13:03,040 --> 00:13:06,442
तो जब आप नेटवर्क को यादृच्छिक भार और पूर्वाग्रहों के साथ आरंभ करते हैं,

195
00:13:06,442 --> 00:13:10,081
और इस ग्रेडिएंट डिसेंट प्रक्रिया के आधार पर उन्हें कई बार समायोजित करते हैं,

196
00:13:10,081 --> 00:13:14,240
तो यह वास्तव में उन छवियों पर कितना अच्छा प्रदर्शन करता है जो पहले कभी नहीं देखी गई हैं?

197
00:13:14,240 --> 00:13:19,502
जिसका मैंने यहां वर्णन किया है, प्रत्येक 16 न्यूरॉन्स की दो छिपी हुई परतों के साथ,

198
00:13:19,502 --> 00:13:23,559
ज्यादातर सौंदर्य संबंधी कारणों से चुना गया है, वह बुरा नहीं है,

199
00:13:23,559 --> 00:13:26,920
यह लगभग 96% नई छवियों को सही ढंग से वर्गीकृत करता है।

200
00:13:26,920 --> 00:13:32,602
और ईमानदारी से कहूं तो, यदि आप ऐसे कुछ उदाहरणों को देखें जिन पर यह गड़बड़ करता है,

201
00:13:32,602 --> 00:13:36,300
तो आप इसे थोड़ा ढीला करने के लिए बाध्य महसूस करते हैं।

202
00:13:36,300 --> 00:13:39,469
यदि आप छिपी हुई परत संरचना के साथ खेलते हैं और कुछ बदलाव करते हैं,

203
00:13:39,469 --> 00:13:41,220
तो आप इसे 98% तक प्राप्त कर सकते हैं।

204
00:13:41,220 --> 00:13:42,900
और यह बहुत अच्छा है!

205
00:13:42,900 --> 00:13:46,638
यह सबसे अच्छा नहीं है, आप निश्चित रूप से इस सादे वेनिला नेटवर्क की तुलना

206
00:13:46,638 --> 00:13:49,659
में अधिक परिष्कृत होकर बेहतर प्रदर्शन प्राप्त कर सकते हैं,

207
00:13:49,659 --> 00:13:52,373
लेकिन यह देखते हुए कि प्रारंभिक कार्य कितना कठिन है,

208
00:13:52,373 --> 00:13:56,930
मुझे लगता है कि किसी भी नेटवर्क द्वारा छवियों पर इतना अच्छा प्रदर्शन करना अविश्वसनीय है,

209
00:13:56,930 --> 00:14:00,771
जैसा कि हमने पहले कभी नहीं देखा है। इसे विशेष रूप से कभी नहीं बताया गया कि

210
00:14:00,771 --> 00:14:02,000
कौन से पैटर्न देखने हैं।

211
00:14:02,000 --> 00:14:07,109
मूल रूप से, जिस तरह से मैंने इस संरचना को प्रेरित किया वह हमारी आशा का वर्णन करके था,

212
00:14:07,109 --> 00:14:11,149
कि दूसरी परत छोटे किनारों को पकड़ सकती है, कि तीसरी परत लूप और लंबी

213
00:14:11,149 --> 00:14:14,417
रेखाओं को पहचानने के लिए उन किनारों को एक साथ जोड़ेगी,

214
00:14:14,417 --> 00:14:18,220
और उन्हें टुकड़े किया जा सकता है अंकों को पहचानने के लिए एक साथ।

215
00:14:18,220 --> 00:14:21,040
तो क्या हमारा नेटवर्क वास्तव में यही कर रहा है?

216
00:14:21,040 --> 00:14:24,960
ख़ैर, कम से कम इस मामले में तो बिल्कुल नहीं।

217
00:14:24,960 --> 00:14:29,232
याद रखें कि पिछले वीडियो में हमने देखा था कि कैसे पहली परत के सभी न्यूरॉन्स

218
00:14:29,232 --> 00:14:33,448
से दूसरी परत के किसी दिए गए न्यूरॉन के कनेक्शन के वजन को एक दिए गए पिक्सेल

219
00:14:33,448 --> 00:14:37,440
पैटर्न के रूप में देखा जा सकता है जिसे दूसरी परत का न्यूरॉन उठा रहा है?

220
00:14:37,440 --> 00:14:42,901
खैर, जब हम इन बदलावों से जुड़े वजनों के लिए ऐसा करते हैं,

221
00:14:42,901 --> 00:14:50,527
तो यहां-वहां अलग-अलग छोटे किनारों को उठाने के बजाय, वे लगभग यादृच्छिक दिखते हैं,

222
00:14:50,527 --> 00:14:54,200
बस बीच में कुछ बहुत ढीले पैटर्न के साथ।

223
00:14:54,200 --> 00:14:58,152
ऐसा प्रतीत होता है कि संभावित भार और पूर्वाग्रहों के अथाह रूप से बड़े

224
00:14:58,152 --> 00:15:02,725
13,000 आयामी स्थान में, हमारे नेटवर्क ने खुद को एक छोटा सा स्थानीय न्यूनतम पाया,

225
00:15:02,725 --> 00:15:06,169
जो कि अधिकांश छवियों को सफलतापूर्वक वर्गीकृत करने के बावजूद,

226
00:15:06,169 --> 00:15:09,840
उन पैटर्नों को बिल्कुल नहीं पकड़ता है जिनकी हम उम्मीद कर सकते थे।

227
00:15:09,840 --> 00:15:12,241
और वास्तव में इस बिंदु को स्पष्ट करने के लिए, देखें कि

228
00:15:12,241 --> 00:15:14,600
जब आप एक यादृच्छिक छवि इनपुट करते हैं तो क्या होता है।

229
00:15:14,600 --> 00:15:18,976
यदि सिस्टम स्मार्ट था, तो आप उम्मीद कर सकते हैं कि यह या तो अनिश्चित महसूस करेगा,

230
00:15:18,976 --> 00:15:22,978
हो सकता है कि वास्तव में उन 10 आउटपुट न्यूरॉन्स में से किसी को सक्रिय न कर

231
00:15:22,978 --> 00:15:25,754
रहा हो या उन सभी को समान रूप से सक्रिय न कर रहा हो,

232
00:15:25,754 --> 00:15:29,169
लेकिन इसके बजाय यह आत्मविश्वास से आपको कुछ बकवास उत्तर देता है,

233
00:15:29,169 --> 00:15:33,172
जैसे कि यह निश्चित लगता है कि यह यादृच्छिक है शोर 5 है क्योंकि ऐसा होता है

234
00:15:33,172 --> 00:15:34,560
कि 5 की वास्तविक छवि 5 है।

235
00:15:34,560 --> 00:15:38,974
अलग-अलग शब्दों में, भले ही यह नेटवर्क अंकों को अच्छी तरह से पहचान सकता है,

236
00:15:38,974 --> 00:15:41,800
लेकिन उसे पता नहीं है कि उन्हें कैसे निकालना है।

237
00:15:41,800 --> 00:15:45,400
इसका बहुत कुछ कारण यह है कि यह बहुत सख्ती से प्रतिबंधित प्रशिक्षण व्यवस्था है।

238
00:15:45,400 --> 00:15:48,220
मेरा मतलब है, यहां अपने आप को नेटवर्क की जगह पर रखें।

239
00:15:48,220 --> 00:15:53,049
इसके दृष्टिकोण से, पूरे ब्रह्मांड में एक छोटे ग्रिड में केंद्रित स्पष्ट रूप से परिभाषित

240
00:15:53,049 --> 00:15:57,714
गतिहीन अंकों के अलावा और कुछ नहीं है, और इसके लागत फ़ंक्शन ने इसे कभी भी कुछ भी करने

241
00:15:57,714 --> 00:16:02,160
के लिए कोई प्रोत्साहन नहीं दिया, लेकिन अपने निर्णयों में पूरी तरह से आश्वस्त रहा।

242
00:16:02,160 --> 00:16:05,496
तो इस छवि के साथ कि वे दूसरी परत के न्यूरॉन्स वास्तव में क्या कर रहे हैं,

243
00:16:05,496 --> 00:16:08,336
आपको आश्चर्य हो सकता है कि मैं इस नेटवर्क को किनारों और पैटर्न

244
00:16:08,336 --> 00:16:10,320
को समझने की प्रेरणा के साथ क्यों पेश करूंगा।

245
00:16:10,320 --> 00:16:13,040
मेरा मतलब है, यह बिलकुल भी नहीं है कि यह क्या कर रहा है।

246
00:16:13,040 --> 00:16:17,480
खैर, यह हमारा अंतिम लक्ष्य नहीं है, बल्कि एक शुरुआती बिंदु है।

247
00:16:17,480 --> 00:16:22,213
सच कहूँ तो, यह पुरानी तकनीक है, जिस पर 80 और 90 के दशक में शोध किया गया था,

248
00:16:22,213 --> 00:16:27,134
और अधिक विस्तृत आधुनिक वेरिएंट को समझने से पहले आपको इसे समझने की आवश्यकता है,

249
00:16:27,134 --> 00:16:31,245
और यह स्पष्ट रूप से कुछ दिलचस्प समस्याओं को हल करने में सक्षम है,

250
00:16:31,245 --> 00:16:36,726
लेकिन जितना अधिक आप इसमें गहराई से उतरेंगे वे छिपी हुई परतें वास्तव में काम कर रही हैं,

251
00:16:36,726 --> 00:16:38,720
यह उतना ही कम बुद्धिमान लगता है।

252
00:16:38,720 --> 00:16:43,323
एक पल के लिए फोकस इस बात से हटा दें कि नेटवर्क कैसे सीखते हैं कि आप कैसे सीखते हैं,

253
00:16:43,323 --> 00:16:47,160
यह तभी होगा जब आप किसी तरह यहां सामग्री के साथ सक्रिय रूप से जुड़ेंगे।

254
00:16:47,160 --> 00:16:51,966
एक बहुत ही सरल चीज जो मैं आपसे करना चाहता हूं वह यह है कि अभी रुकें और एक पल के

255
00:16:51,966 --> 00:16:56,953
लिए गहराई से सोचें कि आप इस सिस्टम में क्या बदलाव कर सकते हैं और यह छवियों को कैसे

256
00:16:56,953 --> 00:17:01,880
देखता है यदि आप चाहते हैं कि यह किनारों और पैटर्न जैसी चीजों को बेहतर ढंग से उठाए।

257
00:17:01,880 --> 00:17:04,895
लेकिन इससे बेहतर, वास्तव में सामग्री से जुड़ने के लिए,

258
00:17:04,895 --> 00:17:09,720
मैं गहन शिक्षण और तंत्रिका नेटवर्क पर माइकल नीलसन की पुस्तक की अत्यधिक अनुशंसा करता हूं।

259
00:17:09,720 --> 00:17:15,477
इसमें, आप इस सटीक उदाहरण के लिए डाउनलोड करने और खेलने के लिए कोड और डेटा पा सकते हैं,

260
00:17:15,477 --> 00:17:19,360
और पुस्तक आपको चरण दर चरण बताएगी कि वह कोड क्या कर रहा है।

261
00:17:19,360 --> 00:17:22,812
कमाल की बात यह है कि यह पुस्तक मुफ़्त है और सार्वजनिक रूप से उपलब्ध है,

262
00:17:22,812 --> 00:17:25,594
इसलिए यदि आपको इससे कुछ मिलता है, तो नीलसन के प्रयासों के

263
00:17:25,594 --> 00:17:28,040
लिए दान देने में मेरे साथ शामिल होने पर विचार करें।

264
00:17:28,040 --> 00:17:33,095
मैंने विवरण में कुछ अन्य संसाधन भी लिंक किए हैं जो मुझे बहुत पसंद हैं,

265
00:17:33,095 --> 00:17:38,720
जिनमें क्रिस ओला का अभूतपूर्व और सुंदर ब्लॉग पोस्ट और डिस्टिल के लेख शामिल हैं।

266
00:17:38,720 --> 00:17:41,251
अंतिम कुछ मिनटों की बातों को यहीं समाप्त करने के लिए,

267
00:17:41,251 --> 00:17:44,440
मैं लीशा ली के साथ हुए साक्षात्कार के एक अंश पर वापस जाना चाहता हूं।

268
00:17:44,440 --> 00:17:48,560
आपको शायद वह पिछले वीडियो से याद होगी, उसने गहन शिक्षण में पीएचडी की थी।

269
00:17:48,560 --> 00:17:52,402
इस छोटे से स्निपेट में, वह दो हालिया पेपरों के बारे में बात करती है जो वास्तव में इस

270
00:17:52,402 --> 00:17:56,380
बात की पड़ताल करते हैं कि कुछ अधिक आधुनिक छवि पहचान नेटवर्क वास्तव में कैसे सीख रहे हैं।

271
00:17:56,380 --> 00:17:58,806
बस यह स्थापित करने के लिए कि हम बातचीत में कहाँ थे,

272
00:17:58,806 --> 00:18:02,026
पहले पेपर ने इन विशेष रूप से गहरे तंत्रिका नेटवर्क में से एक को लिया

273
00:18:02,026 --> 00:18:05,246
जो छवि पहचान में वास्तव में अच्छा है, और इसे उचित रूप से लेबल किए गए

274
00:18:05,246 --> 00:18:09,400
डेटासेट पर प्रशिक्षित करने के बजाय, इसने प्रशिक्षण से पहले सभी लेबलों को इधर-उधर कर दिया।

275
00:18:09,400 --> 00:18:12,711
जाहिर तौर पर यहां परीक्षण की सटीकता यादृच्छिक से बेहतर नहीं होगी,

276
00:18:12,711 --> 00:18:15,320
क्योंकि हर चीज को यादृच्छिक रूप से लेबल किया गया है।

277
00:18:15,320 --> 00:18:18,308
लेकिन यह अभी भी उसी प्रशिक्षण सटीकता को प्राप्त करने में सक्षम

278
00:18:18,308 --> 00:18:21,440
था जैसा कि आप उचित रूप से लेबल किए गए डेटासेट पर प्राप्त करते हैं।

279
00:18:21,440 --> 00:18:26,511
मूल रूप से, इस विशेष नेटवर्क के लिए लाखों वज़न केवल यादृच्छिक डेटा को याद रखने

280
00:18:26,511 --> 00:18:31,776
के लिए पर्याप्त थे, जो यह सवाल उठाता है कि क्या इस लागत फ़ंक्शन को कम करना वास्तव

281
00:18:31,776 --> 00:18:36,720
में छवि में किसी भी प्रकार की संरचना से मेल खाता है, या यह सिर्फ याद रखना है?

282
00:18:36,720 --> 00:18:36,720
.

283
00:18:36,720 --> 00:18:36,720
.

284
00:18:36,720 --> 00:18:36,720
.

285
00:18:36,720 --> 00:18:40,120
सही वर्गीकरण क्या है, इसके संपूर्ण डेटासेट को याद रखना।

286
00:18:40,120 --> 00:18:43,634
और इसलिए कुछ, आप जानते हैं, आधे साल बाद इस साल आईसीएमएल में,

287
00:18:43,634 --> 00:18:48,417
वास्तव में खंडन पत्र नहीं था, लेकिन पेपर जिसमें कुछ पहलुओं को संबोधित किया गया था,

288
00:18:48,417 --> 00:18:52,220
अरे, वास्तव में ये नेटवर्क उससे थोड़ा अधिक स्मार्ट काम कर रहे हैं।

289
00:18:52,220 --> 00:18:58,787
यदि आप उस सटीकता वक्र को देखते हैं, यदि आप बस एक यादृच्छिक डेटासेट पर प्रशिक्षण ले रहे

290
00:18:58,787 --> 00:19:05,280
थे, तो वह वक्र बहुत नीचे चला गया, आप जानते हैं, लगभग एक रैखिक फैशन में बहुत धीरे-धीरे।

291
00:19:05,280 --> 00:19:09,659
तो आप वास्तव में संभव के उस स्थानीय न्यूनतम को खोजने के लिए संघर्ष कर रहे हैं,

292
00:19:09,659 --> 00:19:12,320
आप जानते हैं, सही वजन जो आपको वह सटीकता दिलाएगा।

293
00:19:12,320 --> 00:19:15,724
जबकि यदि आप वास्तव में एक संरचित डेटासेट पर प्रशिक्षण ले रहे हैं,

294
00:19:15,724 --> 00:19:19,645
जिसमें सही लेबल हैं, तो आप जानते हैं, आप शुरुआत में थोड़ा इधर-उधर करते हैं,

295
00:19:19,645 --> 00:19:23,360
लेकिन फिर आप उस सटीकता स्तर तक पहुंचने के लिए बहुत तेजी से गिर जाते हैं।

296
00:19:23,360 --> 00:19:28,580
और इसलिए कुछ अर्थों में उस स्थानीय मैक्सिमा को खोजना आसान था।

297
00:19:28,580 --> 00:19:34,288
और इसलिए इसके बारे में दिलचस्प बात यह है कि यह वास्तव में कुछ साल पहले के एक और

298
00:19:34,288 --> 00:19:40,140
पेपर को प्रकाश में लाता है, जिसमें नेटवर्क परतों के बारे में बहुत अधिक सरलीकरण है।

299
00:19:40,140 --> 00:19:44,740
लेकिन परिणामों में से एक यह बता रहा था कि, यदि आप अनुकूलन परिदृश्य को देखें,

300
00:19:44,740 --> 00:19:49,400
तो ये नेटवर्क जो स्थानीय मिनीमा सीखते हैं, वे वास्तव में समान गुणवत्ता के हैं।

301
00:19:49,400 --> 00:19:53,596
तो कुछ अर्थों में, यदि आपका डेटा सेट संरचित है,

302
00:19:53,596 --> 00:19:58,580
तो आपको इसे और अधिक आसानी से ढूंढने में सक्षम होना चाहिए।

303
00:19:58,580 --> 00:20:01,480
आपमें से पैट्रियन का समर्थन करने वालों को हमेशा की तरह मेरा धन्यवाद।

304
00:20:01,480 --> 00:20:04,372
मैंने पहले ही कहा है कि पैट्रियन पर गेम चेंजर क्या है,

305
00:20:04,372 --> 00:20:07,160
लेकिन ये वीडियो वास्तव में आपके बिना संभव नहीं होंगे।

306
00:20:07,160 --> 00:20:19,150
मैं वीसी फर्म एम्प्लीफाई पार्टनर्स और श्रृंखला के इन शुरुआती

307
00:20:19,150 --> 00:20:31,140
वीडियो के लिए उनके समर्थन को भी विशेष धन्यवाद देना चाहता हूं।

308
00:20:31,140 --> 00:20:31,140
धन्यवाद।

