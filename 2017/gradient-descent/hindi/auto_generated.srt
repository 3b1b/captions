1
00:00:04,180 --> 00:00:07,280
पिछले वीडियो में मैंने एक तंत्रिका नेटवर्क की संरचना बताई थी।

2
00:00:07,680 --> 00:00:10,591
मैं यहां एक संक्षिप्त पुनर्कथन दूंगा ताकि यह हमारे दिमाग में ताजा रहे, 

3
00:00:10,591 --> 00:00:12,600
और फिर इस वीडियो के लिए मेरे दो मुख्य लक्ष्य हैं।

4
00:00:13,100 --> 00:00:15,946
सबसे पहले ग्रेडिएंट डिसेंट के विचार को पेश करना है, 

5
00:00:15,946 --> 00:00:20,600
जो न केवल तंत्रिका नेटवर्क कैसे सीखते हैं, बल्कि कई अन्य मशीन लर्निंग भी काम करता है।

6
00:00:21,120 --> 00:00:25,351
फिर उसके बाद हम थोड़ा और गहराई से देखेंगे कि यह विशेष नेटवर्क कैसा प्रदर्शन करता है, 

7
00:00:25,351 --> 00:00:27,940
और न्यूरॉन्स की छिपी हुई परतें आखिर क्या ढूंढती हैं।

8
00:00:28,979 --> 00:00:34,072
एक अनुस्मारक के रूप में, हमारा लक्ष्य यहां हस्तलिखित अंक पहचान का उत्कृष्ट उदाहरण, 

9
00:00:34,072 --> 00:00:36,220
तंत्रिका नेटवर्क की हैलो दुनिया है।

10
00:00:37,020 --> 00:00:39,969
ये अंक 28x28 पिक्सेल ग्रिड पर प्रस्तुत किए जाते हैं, 

11
00:00:39,969 --> 00:00:43,420
प्रत्येक पिक्सेल में 0 और 1 के बीच कुछ ग्रेस्केल मान होते हैं।

12
00:00:43,820 --> 00:00:50,040
वे ही नेटवर्क की इनपुट परत में 784 न्यूरॉन्स की सक्रियता निर्धारित करते हैं।

13
00:00:51,180 --> 00:00:56,054
और फिर निम्नलिखित परतों में प्रत्येक न्यूरॉन के लिए सक्रियता पिछली परत में सभी सक्रियणों 

14
00:00:56,054 --> 00:01:00,820
के भारित योग के साथ-साथ कुछ विशेष संख्या पर आधारित होती है जिसे पूर्वाग्रह कहा जाता है।

15
00:01:02,160 --> 00:01:04,802
फिर आप उस योग को किसी अन्य फ़ंक्शन के साथ बनाते हैं, 

16
00:01:04,802 --> 00:01:08,940
जैसे सिग्मॉइड स्क्विशिफिकेशन, या एक रिले, जिस तरह से मैंने पिछले वीडियो को देखा था।

17
00:01:09,480 --> 00:01:14,406
कुल मिलाकर, प्रत्येक 16 न्यूरॉन्स वाली दो छिपी हुई परतों की कुछ हद तक मनमानी पसंद 

18
00:01:14,406 --> 00:01:19,393
को देखते हुए, नेटवर्क में लगभग 13,000 वजन और पूर्वाग्रह हैं जिन्हें हम समायोजित कर 

19
00:01:19,393 --> 00:01:24,380
सकते हैं, और ये मूल्य हैं जो निर्धारित करते हैं कि नेटवर्क वास्तव में क्या करता है।

20
00:01:24,880 --> 00:01:29,167
फिर जब हम कहते हैं कि यह नेटवर्क किसी दिए गए अंक को वर्गीकृत करता है तो हमारा मतलब 

21
00:01:29,167 --> 00:01:33,300
यह है कि अंतिम परत में उन 10 न्यूरॉन्स में से सबसे चमकीला उस अंक से मेल खाता है।

22
00:01:34,100 --> 00:01:38,979
और याद रखें, स्तरित संरचना के लिए हमारे मन में जो प्रेरणा थी वह यह थी कि शायद 

23
00:01:38,979 --> 00:01:44,608
दूसरी परत किनारों को पकड़ सकती है, और तीसरी परत लूप और रेखाओं जैसे पैटर्न को चुन सकती है, 

24
00:01:44,608 --> 00:01:48,800
और आखिरी परत उन्हें एक साथ जोड़ सकती है अंकों को पहचानने के पैटर्न.

25
00:01:49,800 --> 00:01:52,240
तो यहां, हम सीखते हैं कि नेटवर्क कैसे सीखता है।

26
00:01:52,640 --> 00:01:56,982
हम जो चाहते हैं वह एक एल्गोरिदम है जहां आप इस नेटवर्क को प्रशिक्षण डेटा का एक 

27
00:01:56,982 --> 00:02:01,936
पूरा समूह दिखा सकते हैं, जो हस्तलिखित अंकों की विभिन्न छवियों के समूह के रूप में आता है, 

28
00:02:01,936 --> 00:02:06,334
साथ ही उनके लिए लेबल भी होते हैं, और यह होगा उन 13,000 वज़न और पूर्वाग्रहों को 

29
00:02:06,334 --> 00:02:10,120
समायोजित करें ताकि प्रशिक्षण डेटा पर इसके प्रदर्शन में सुधार हो सके।

30
00:02:10,720 --> 00:02:13,790
उम्मीद है, इस स्तरित संरचना का मतलब यह होगा कि यह जो सीखता है 

31
00:02:13,790 --> 00:02:16,860
वह उस प्रशिक्षण डेटा से परे छवियों के लिए सामान्यीकृत होता है।

32
00:02:17,640 --> 00:02:20,881
जिस तरह से हम परीक्षण करते हैं वह यह है कि नेटवर्क को प्रशिक्षित करने के बाद, 

33
00:02:20,881 --> 00:02:23,749
आप उसे अधिक लेबल वाला डेटा दिखाते हैं जो उसने पहले कभी नहीं देखा है, 

34
00:02:23,749 --> 00:02:26,700
और आप देखते हैं कि यह उन नई छवियों को कितनी सटीकता से वर्गीकृत करता है।

35
00:02:31,120 --> 00:02:35,613
हमारे लिए सौभाग्य की बात है, और जो बात इसे शुरू करने के लिए इतना सामान्य उदाहरण बनाती है, 

36
00:02:35,613 --> 00:02:40,006
वह यह है कि एमएनआईएसटी डेटाबेस के पीछे अच्छे लोगों ने हजारों हस्तलिखित अंक छवियों का एक 

37
00:02:40,006 --> 00:02:44,200
संग्रह रखा है, प्रत्येक को उन संख्याओं के साथ लेबल किया गया है जो उन्हें चाहिए होना।

38
00:02:44,900 --> 00:02:48,226
और एक मशीन को सीखने के रूप में वर्णित करना जितना उत्तेजक है, 

39
00:02:48,226 --> 00:02:50,789
एक बार जब आप देखते हैं कि यह कैसे काम करता है, 

40
00:02:50,789 --> 00:02:55,480
तो यह किसी पागल विज्ञान-फाई आधार की तरह कम और कैलकुलस अभ्यास की तरह बहुत अधिक लगता है।

41
00:02:56,200 --> 00:02:59,960
मेरा मतलब है, मूल रूप से यह एक निश्चित फ़ंक्शन का न्यूनतम पता लगाने पर निर्भर करता है।

42
00:03:01,939 --> 00:03:07,634
याद रखें, वैचारिक रूप से, हम प्रत्येक न्यूरॉन को पिछली परत के सभी न्यूरॉन्स से जुड़े होने 

43
00:03:07,634 --> 00:03:12,949
के रूप में सोच रहे हैं, और इसकी सक्रियता को परिभाषित करने वाले भारित योग में वजन उन 

44
00:03:12,949 --> 00:03:18,327
कनेक्शनों की ताकत की तरह है, और पूर्वाग्रह कुछ संकेत है चाहे वह न्यूरॉन सक्रिय हो या 

45
00:03:18,327 --> 00:03:18,960
निष्क्रिय।

46
00:03:19,720 --> 00:03:22,243
और चीजों को शुरू करने के लिए, हम उन सभी भारों और पूर्वाग्रहों 

47
00:03:22,243 --> 00:03:24,400
को पूरी तरह से यादृच्छिक रूप से आरंभ करने जा रहे हैं।

48
00:03:24,940 --> 00:03:27,892
कहने की जरूरत नहीं है, यह नेटवर्क किसी दिए गए प्रशिक्षण उदाहरण पर बहुत 

49
00:03:27,892 --> 00:03:30,720
खराब प्रदर्शन करने जा रहा है, क्योंकि यह बस कुछ यादृच्छिक कर रहा है।

50
00:03:31,040 --> 00:03:33,666
उदाहरण के लिए, आप 3 की इस छवि को फ़ीड करते हैं, 

51
00:03:33,666 --> 00:03:36,020
और आउटपुट परत बस एक गड़बड़ की तरह दिखती है।

52
00:03:36,600 --> 00:03:39,833
तो आप जो करते हैं वह एक लागत फ़ंक्शन को परिभाषित करना है, 

53
00:03:39,833 --> 00:03:43,066
कंप्यूटर को यह बताने का एक तरीका है, नहीं, खराब कंप्यूटर, 

54
00:03:43,066 --> 00:03:46,801
आउटपुट में सक्रियण होना चाहिए जो कि अधिकांश न्यूरॉन्स के लिए 0 है, 

55
00:03:46,801 --> 00:03:50,760
लेकिन इस न्यूरॉन के लिए 1 है, आपने मुझे जो दिया वह पूरी तरह से कचरा है।

56
00:03:51,720 --> 00:03:56,357
इसे थोड़ा और गणितीय रूप से कहने के लिए, आप उन कचरा आउटपुट सक्रियणों 

57
00:03:56,357 --> 00:04:01,882
में से प्रत्येक के बीच अंतर के वर्गों को जोड़ते हैं और वह मूल्य जो आप चाहते हैं, 

58
00:04:01,882 --> 00:04:05,020
और इसे हम एकल प्रशिक्षण उदाहरण की लागत कहेंगे।

59
00:04:05,960 --> 00:04:09,421
ध्यान दें कि यह राशि तब छोटी होती है जब नेटवर्क आत्मविश्वास से 

60
00:04:09,421 --> 00:04:12,938
छवि को सही ढंग से वर्गीकृत करता है, लेकिन यह तब बड़ी होती है जब 

61
00:04:12,938 --> 00:04:16,399
नेटवर्क को ऐसा लगता है कि उसे पता नहीं है कि वह क्या कर रहा है।

62
00:04:18,640 --> 00:04:25,440
तो फिर आप अपने पास मौजूद हजारों प्रशिक्षण उदाहरणों की औसत लागत पर विचार करें।

63
00:04:27,040 --> 00:04:29,778
यह औसत लागत इस बात का पैमाना है कि नेटवर्क कितना 

64
00:04:29,778 --> 00:04:32,740
ख़राब है और कंप्यूटर को कितना ख़राब महसूस होना चाहिए।

65
00:04:33,420 --> 00:04:34,600
और यह एक जटिल बात है.

66
00:04:35,040 --> 00:04:40,762
याद रखें कि कैसे नेटवर्क मूल रूप से एक फ़ंक्शन था, जो इनपुट के रूप में 784 नंबर लेता है, 

67
00:04:40,762 --> 00:04:44,234
पिक्सेल मान, और आउटपुट के रूप में 10 नंबर निकालता है, 

68
00:04:44,234 --> 00:04:48,800
और एक अर्थ में यह इन सभी भारों और पूर्वाग्रहों द्वारा पैरामीटरयुक्त है?

69
00:04:49,500 --> 00:04:52,820
खैर, लागत फ़ंक्शन उसके ऊपर जटिलता की एक परत है।

70
00:04:53,100 --> 00:04:57,730
यह अपने इनपुट के रूप में उन 13,000 या उससे अधिक वजन और पूर्वाग्रहों को लेता है, 

71
00:04:57,730 --> 00:05:02,244
और एक एकल संख्या बताता है जो बताता है कि वे वजन और पूर्वाग्रह कितने खराब हैं, 

72
00:05:02,244 --> 00:05:06,121
और जिस तरह से इसे परिभाषित किया गया है वह प्रशिक्षण डेटा के हजारों 

73
00:05:06,121 --> 00:05:08,900
टुकड़ों पर नेटवर्क के व्यवहार पर निर्भर करता है।

74
00:05:09,520 --> 00:05:11,000
यह बहुत सोचने वाली बात है.

75
00:05:12,400 --> 00:05:15,820
लेकिन केवल कंप्यूटर को यह बताना कि वह कितना घटिया काम कर रहा है, बहुत मददगार नहीं है।

76
00:05:16,220 --> 00:05:20,060
आप इसे बताना चाहते हैं कि उन वज़न और पूर्वाग्रहों को कैसे बदला जाए ताकि यह बेहतर हो जाए।

77
00:05:20,780 --> 00:05:23,947
इसे आसान बनाने के लिए, 13,000 इनपुट वाले फ़ंक्शन की कल्पना करने 

78
00:05:23,947 --> 00:05:27,114
के लिए संघर्ष करने के बजाय, बस एक साधारण फ़ंक्शन की कल्पना करें 

79
00:05:27,114 --> 00:05:30,480
जिसमें इनपुट के रूप में एक संख्या और आउटपुट के रूप में एक संख्या हो।

80
00:05:31,480 --> 00:05:35,300
आप ऐसा इनपुट कैसे ढूंढते हैं जो इस फ़ंक्शन के मान को न्यूनतम करता है?

81
00:05:36,460 --> 00:05:41,296
कैलकुलस के छात्रों को पता होगा कि आप कभी-कभी उस न्यूनतम को स्पष्ट रूप से समझ सकते हैं, 

82
00:05:41,296 --> 00:05:44,909
लेकिन वास्तव में जटिल कार्यों के लिए यह हमेशा संभव नहीं होता है, 

83
00:05:44,909 --> 00:05:49,801
निश्चित रूप से हमारे जटिल जटिल तंत्रिका नेटवर्क लागत फ़ंक्शन के लिए इस स्थिति के 13,000 

84
00:05:49,801 --> 00:05:51,080
इनपुट संस्करण में नहीं।

85
00:05:51,580 --> 00:05:54,472
एक अधिक लचीली रणनीति किसी भी इनपुट से शुरू करना है, 

86
00:05:54,472 --> 00:05:59,200
और यह पता लगाना है कि उस आउटपुट को कम करने के लिए आपको किस दिशा में कदम बढ़ाना चाहिए।

87
00:06:00,080 --> 00:06:04,130
विशेष रूप से, यदि आप उस फ़ंक्शन के ढलान का पता लगा सकते हैं जहां आप हैं, 

88
00:06:04,130 --> 00:06:06,793
तो यदि ढलान सकारात्मक है तो बाईं ओर शिफ्ट करें, 

89
00:06:06,793 --> 00:06:09,900
और यदि ढलान नकारात्मक है तो इनपुट को दाईं ओर शिफ्ट करें।

90
00:06:11,960 --> 00:06:15,797
यदि आप ऐसा बार-बार करते हैं, प्रत्येक बिंदु पर नई ढलान की जाँच करते हैं और 

91
00:06:15,797 --> 00:06:19,840
उचित कदम उठाते हैं, तो आप फ़ंक्शन के कुछ स्थानीय न्यूनतम तक पहुँचने जा रहे हैं।

92
00:06:20,640 --> 00:06:23,800
यहां आपके मन में जो छवि होगी वह एक पहाड़ी से लुढ़कती हुई गेंद की होगी।

93
00:06:24,620 --> 00:06:28,192
ध्यान दें, यहां तक कि इस वास्तव में सरलीकृत एकल इनपुट फ़ंक्शन के लिए भी, 

94
00:06:28,192 --> 00:06:30,492
कई संभावित घाटियां हैं जिनमें आप उतर सकते हैं, 

95
00:06:30,492 --> 00:06:33,820
यह इस पर निर्भर करता है कि आप किस यादृच्छिक इनपुट से शुरू करते हैं, 

96
00:06:33,820 --> 00:06:37,589
और इस बात की कोई गारंटी नहीं है कि आप जिस स्थानीय न्यूनतम पर उतरेंगे वह सबसे 

97
00:06:37,589 --> 00:06:39,400
छोटा संभव मूल्य होगा लागत फ़ंक्शन का.

98
00:06:40,220 --> 00:06:42,620
यह हमारे तंत्रिका नेटवर्क मामले को भी आगे बढ़ाएगा।

99
00:06:43,180 --> 00:06:47,662
मैं यह भी देखना चाहता हूं कि यदि आप अपने कदमों का आकार ढलान के समानुपाती बनाते हैं, 

100
00:06:47,662 --> 00:06:52,038
तो जब ढलान न्यूनतम की ओर समतल हो जाता है, तो आपके कदम छोटे और छोटे होते जाते हैं, 

101
00:06:52,038 --> 00:06:54,600
और इससे आपको ओवरशूटिंग से बचने में मदद मिलती है।

102
00:06:55,940 --> 00:07:00,980
जटिलता को थोड़ा बढ़ाते हुए, दो इनपुट और एक आउटपुट वाले फ़ंक्शन की कल्पना करें।

103
00:07:01,500 --> 00:07:04,353
आप इनपुट स्पेस को xy-प्लेन के रूप में सोच सकते हैं, 

104
00:07:04,353 --> 00:07:08,140
और लागत फ़ंक्शन को इसके ऊपर की सतह के रूप में ग्राफ़ किया जा सकता है।

105
00:07:08,760 --> 00:07:13,800
फ़ंक्शन के ढलान के बारे में पूछने के बजाय, आपको यह पूछना होगा कि आपको इस इनपुट स्पेस 

106
00:07:13,800 --> 00:07:18,960
में किस दिशा में कदम रखना चाहिए ताकि फ़ंक्शन के आउटपुट को सबसे तेज़ी से कम किया जा सके।

107
00:07:19,720 --> 00:07:21,760
दूसरे शब्दों में, ढलान की दिशा क्या है?

108
00:07:22,380 --> 00:07:25,560
फिर, उस पहाड़ी से लुढ़कती हुई गेंद के बारे में सोचना उपयोगी है।

109
00:07:26,660 --> 00:07:29,690
आपमें से जो लोग मल्टीवेरिएबल कैलकुलस से परिचित हैं, 

110
00:07:29,690 --> 00:07:34,468
उन्हें पता होगा कि किसी फ़ंक्शन का ग्रेडिएंट आपको सबसे तेज चढ़ाई की दिशा देता है, 

111
00:07:34,468 --> 00:07:38,780
आपको फ़ंक्शन को सबसे तेज़ी से बढ़ाने के लिए किस दिशा में कदम बढ़ाना चाहिए।

112
00:07:39,560 --> 00:07:42,720
स्वाभाविक रूप से, उस ग्रेडिएंट के नकारात्मक को लेने से आपको 

113
00:07:42,720 --> 00:07:46,040
उस कदम की दिशा मिलती है जो फ़ंक्शन को सबसे तेज़ी से कम करता है।

114
00:07:47,240 --> 00:07:50,573
इससे भी अधिक, इस ग्रेडिएंट वेक्टर की लंबाई इस बात 

115
00:07:50,573 --> 00:07:53,840
का संकेत है कि वह सबसे तीव्र ढलान कितनी तीव्र है।

116
00:07:54,540 --> 00:07:57,357
यदि आप मल्टीवेरिएबल कैलकुलस से अपरिचित हैं और अधिक सीखना चाहते हैं, 

117
00:07:57,357 --> 00:08:00,340
तो इस विषय पर खान अकादमी के लिए मेरे द्वारा किए गए कुछ कार्यों को देखें।

118
00:08:00,860 --> 00:08:04,540
हालाँकि, ईमानदारी से कहें तो, अभी आपके और मेरे लिए यह सब मायने रखता 

119
00:08:04,540 --> 00:08:08,219
है कि सिद्धांत रूप में इस वेक्टर की गणना करने का एक तरीका मौजूद है, 

120
00:08:08,219 --> 00:08:11,900
यह वेक्टर आपको बताता है कि ढलान की दिशा क्या है और यह कितनी खड़ी है।

121
00:08:12,400 --> 00:08:14,280
यदि आप इतना ही जानते हैं और विवरण के मामले में 

122
00:08:14,280 --> 00:08:16,120
आप ठोस नहीं हैं तो आपको कोई परेशानी नहीं होगी।

123
00:08:17,200 --> 00:08:21,847
यदि आप इसे प्राप्त कर सकते हैं, तो फ़ंक्शन को छोटा करने के लिए एल्गोरिदम इस 

124
00:08:21,847 --> 00:08:26,740
ढाल दिशा की गणना करना है, फिर ढलान पर एक छोटा कदम उठाएं, और इसे बार-बार दोहराएं।

125
00:08:27,700 --> 00:08:32,820
यह किसी फ़ंक्शन के लिए वही मूल विचार है जिसमें 2 इनपुट के बजाय 13,000 इनपुट हैं।

126
00:08:33,400 --> 00:08:36,346
हमारे नेटवर्क के सभी 13,000 भारों और पूर्वाग्रहों को 

127
00:08:36,346 --> 00:08:39,460
एक विशाल स्तंभ वेक्टर में व्यवस्थित करने की कल्पना करें।

128
00:08:40,140 --> 00:08:43,909
लागत फ़ंक्शन का नकारात्मक ग्रेडिएंट सिर्फ एक वेक्टर है, 

129
00:08:43,909 --> 00:08:48,755
यह इस अत्यधिक विशाल इनपुट स्थान के अंदर कुछ दिशा है जो आपको बताता है कि 

130
00:08:48,755 --> 00:08:53,668
उन सभी संख्याओं में से कौन सा संकेत लागत फ़ंक्शन में सबसे तेजी से कमी का 

131
00:08:53,668 --> 00:08:54,880
कारण बनने वाला है।

132
00:08:55,640 --> 00:08:59,299
और निश्चित रूप से, हमारे विशेष रूप से डिज़ाइन किए गए लागत फ़ंक्शन के साथ, 

133
00:08:59,299 --> 00:09:03,106
इसे कम करने के लिए वजन और पूर्वाग्रहों को बदलने का मतलब है कि प्रशिक्षण डेटा 

134
00:09:03,106 --> 00:09:07,408
के प्रत्येक टुकड़े पर नेटवर्क का आउटपुट 10 मानों की यादृच्छिक सरणी की तरह कम दिखता है, 

135
00:09:07,408 --> 00:09:10,820
और वास्तविक निर्णय की तरह अधिक दिखता है जो हम चाहते हैं इसे बनाना है.

136
00:09:11,440 --> 00:09:16,337
यह याद रखना महत्वपूर्ण है, इस लागत फ़ंक्शन में सभी प्रशिक्षण डेटा का औसत शामिल होता है, 

137
00:09:16,337 --> 00:09:21,180
इसलिए यदि आप इसे कम करते हैं, तो इसका मतलब है कि यह उन सभी नमूनों पर बेहतर प्रदर्शन है।

138
00:09:23,820 --> 00:09:26,549
इस ग्रेडिएंट की कुशलता से गणना करने के लिए एल्गोरिदम, 

139
00:09:26,549 --> 00:09:29,784
जो प्रभावी रूप से एक तंत्रिका नेटवर्क कैसे सीखता है, का मूल है, 

140
00:09:29,784 --> 00:09:33,980
जिसे बैकप्रॉपैगेशन कहा जाता है, और मैं अगले वीडियो के बारे में बात करने जा रहा हूं।

141
00:09:34,660 --> 00:09:38,775
वहां, मैं वास्तव में प्रशिक्षण डेटा के दिए गए टुकड़े के लिए प्रत्येक वजन और पूर्वाग्रह 

142
00:09:38,775 --> 00:09:42,086
के साथ वास्तव में क्या होता है, इस पर चलने के लिए समय लेना चाहता हूं, 

143
00:09:42,086 --> 00:09:44,876
प्रासंगिक कैलकुलस और सूत्रों के ढेर से परे क्या हो रहा है, 

144
00:09:44,876 --> 00:09:47,100
इसके लिए एक सहज अनुभव देने की कोशिश कर रहा हूं।

145
00:09:47,780 --> 00:09:52,064
यहीं, अभी, मुख्य बात जो मैं आपको जानना चाहता हूं, कार्यान्वयन विवरण से स्वतंत्र, 

146
00:09:52,064 --> 00:09:55,556
वह यह है कि जब हम नेटवर्क सीखने के बारे में बात करते हैं तो हमारा 

147
00:09:55,556 --> 00:09:58,360
मतलब यह है कि यह सिर्फ एक लागत फ़ंक्शन को कम करना है।

148
00:09:59,300 --> 00:10:03,561
और ध्यान दें, इसका एक परिणाम यह है कि इस लागत फ़ंक्शन के लिए एक अच्छा सुचारू 

149
00:10:03,561 --> 00:10:08,100
आउटपुट होना महत्वपूर्ण है, ताकि हम ढलान पर छोटे कदम उठाकर स्थानीय न्यूनतम पा सकें।

150
00:10:09,260 --> 00:10:13,416
यही कारण है कि, वैसे, कृत्रिम न्यूरॉन्स में लगातार सक्रियण होते हैं, 

151
00:10:13,416 --> 00:10:16,971
न कि केवल द्विआधारी तरीके से सक्रिय या निष्क्रिय होते हैं, 

152
00:10:16,971 --> 00:10:19,140
जिस तरह से जैविक न्यूरॉन्स होते हैं।

153
00:10:20,220 --> 00:10:23,566
किसी फ़ंक्शन के इनपुट को नकारात्मक ग्रेडिएंट के कुछ गुणकों द्वारा 

154
00:10:23,566 --> 00:10:26,760
बार-बार धकेलने की इस प्रक्रिया को ग्रेडिएंट डिसेंट कहा जाता है।

155
00:10:27,300 --> 00:10:30,868
यह लागत फ़ंक्शन के कुछ स्थानीय न्यूनतम की ओर अभिसरण करने का एक तरीका है, 

156
00:10:30,868 --> 00:10:32,580
मूल रूप से इस ग्राफ में एक घाटी है।

157
00:10:33,440 --> 00:10:36,915
मैं अभी भी दो इनपुट के साथ एक फ़ंक्शन की तस्वीर दिखा रहा हूं, 

158
00:10:36,915 --> 00:10:40,952
क्योंकि 13,000 आयामी इनपुट स्पेस में आपके दिमाग को घेरना थोड़ा कठिन है, 

159
00:10:40,952 --> 00:10:44,260
लेकिन इसके बारे में सोचने का एक अच्छा गैर-स्थानिक तरीका है।

160
00:10:45,080 --> 00:10:48,440
नकारात्मक प्रवणता का प्रत्येक घटक हमें दो बातें बताता है।

161
00:10:49,060 --> 00:10:52,016
संकेत, निश्चित रूप से, हमें बताता है कि इनपुट वेक्टर 

162
00:10:52,016 --> 00:10:55,140
के संबंधित घटक को ऊपर या नीचे झुकाया जाना चाहिए या नहीं।

163
00:10:55,800 --> 00:10:59,106
लेकिन महत्वपूर्ण बात यह है कि इन सभी घटकों का सापेक्ष 

164
00:10:59,106 --> 00:11:02,720
परिमाण आपको बताता है कि कौन सा परिवर्तन अधिक मायने रखता है।

165
00:11:05,220 --> 00:11:09,184
आप देखते हैं, हमारे नेटवर्क में, किसी एक वज़न के समायोजन का लागत फ़ंक्शन 

166
00:11:09,184 --> 00:11:13,040
पर किसी अन्य वज़न के समायोजन की तुलना में बहुत अधिक प्रभाव पड़ सकता है।

167
00:11:14,800 --> 00:11:18,200
इनमें से कुछ कनेक्शन हमारे प्रशिक्षण डेटा के लिए अधिक मायने रखते हैं।

168
00:11:19,320 --> 00:11:22,505
तो जिस तरह से आप हमारे मन-मस्तिष्क के बड़े पैमाने पर लागत फ़ंक्शन 

169
00:11:22,505 --> 00:11:24,870
के इस ग्रेडिएंट वेक्टर के बारे में सोच सकते हैं, 

170
00:11:24,870 --> 00:11:28,587
वह यह है कि यह प्रत्येक वजन और पूर्वाग्रह के सापेक्ष महत्व को एनकोड करता है, 

171
00:11:28,587 --> 00:11:32,400
अर्थात, इनमें से कौन सा परिवर्तन आपके पैसे के लिए सबसे अधिक धमाका करने वाला है।

172
00:11:33,620 --> 00:11:36,640
यह वास्तव में दिशा के बारे में सोचने का एक और तरीका है।

173
00:11:37,100 --> 00:11:42,227
एक सरल उदाहरण लेने के लिए, यदि आपके पास इनपुट के रूप में दो चर के साथ कुछ फ़ंक्शन है, 

174
00:11:42,227 --> 00:11:47,235
और आप गणना करते हैं कि किसी विशेष बिंदु पर इसका ग्रेडिएंट 3,1 के रूप में निकलता है, 

175
00:11:47,235 --> 00:11:52,362
तो एक तरफ आप इसे यह कहकर व्याख्या कर सकते हैं कि जब आप ' आप उस इनपुट पर खड़े हैं, 

176
00:11:52,362 --> 00:11:55,761
इस दिशा में आगे बढ़ने से फ़ंक्शन सबसे तेज़ी से बढ़ता है, 

177
00:11:55,761 --> 00:11:59,577
जब आप इनपुट बिंदुओं के विमान के ऊपर फ़ंक्शन को ग्राफ़ करते हैं, 

178
00:11:59,577 --> 00:12:02,260
तो वह वेक्टर आपको सीधे ऊपर की दिशा दे रहा है।

179
00:12:02,860 --> 00:12:07,415
लेकिन इसे पढ़ने का एक और तरीका यह है कि इस पहले चर में परिवर्तन दूसरे चर 

180
00:12:07,415 --> 00:12:12,781
में परिवर्तन के रूप में 3 गुना महत्व रखते हैं, कम से कम प्रासंगिक इनपुट के पड़ोस में, 

181
00:12:12,781 --> 00:12:16,900
एक्स-वैल्यू को कम करने से आपके लिए बहुत अधिक प्रभाव पड़ता है हिरन.

182
00:12:19,880 --> 00:12:22,340
आइए ज़ूम आउट करें और सारांशित करें कि हम अब तक कहां हैं।

183
00:12:22,840 --> 00:12:26,377
नेटवर्क स्वयं 784 इनपुट और 10 आउटपुट वाला यह फ़ंक्शन है, 

184
00:12:26,377 --> 00:12:30,040
जिसे इन सभी भारित योगों के संदर्भ में परिभाषित किया गया है।

185
00:12:30,640 --> 00:12:33,680
लागत फ़ंक्शन उसके ऊपर जटिलता की एक परत है।

186
00:12:33,980 --> 00:12:37,817
यह 13,000 वज़न और पूर्वाग्रहों को इनपुट के रूप में लेता है 

187
00:12:37,817 --> 00:12:41,720
और प्रशिक्षण उदाहरणों के आधार पर घटियापन का एक माप उगलता है।

188
00:12:42,440 --> 00:12:46,900
और लागत फ़ंक्शन का ग्रेडिएंट अभी भी जटिलता की एक और परत है।

189
00:12:47,360 --> 00:12:50,987
यह हमें बताता है कि इन सभी भारों और पूर्वाग्रहों के कारण लागत फ़ंक्शन 

190
00:12:50,987 --> 00:12:54,407
के मूल्य में सबसे तेज़ परिवर्तन होता है, जिसे आप यह कहकर व्याख्या 

191
00:12:54,407 --> 00:12:57,880
कर सकते हैं कि किस भार में कौन सा परिवर्तन सबसे अधिक मायने रखता है।

192
00:13:02,560 --> 00:13:05,915
इसलिए, जब आप नेटवर्क को यादृच्छिक भार और पूर्वाग्रहों के साथ आरंभ करते हैं, 

193
00:13:05,915 --> 00:13:09,314
और इस ग्रेडिएंट डिसेंट प्रक्रिया के आधार पर उन्हें कई बार समायोजित करते हैं, 

194
00:13:09,314 --> 00:13:13,200
तो यह वास्तव में उन छवियों पर कितना अच्छा प्रदर्शन करता है जो पहले कभी नहीं देखी गई हैं?

195
00:13:14,100 --> 00:13:19,021
जिसका मैंने यहां वर्णन किया है, प्रत्येक 16 न्यूरॉन्स की दो छिपी हुई परतों के साथ, 

196
00:13:19,021 --> 00:13:22,817
ज्यादातर सौंदर्य संबंधी कारणों से चुना गया है, वह बुरा नहीं है, 

197
00:13:22,817 --> 00:13:25,960
यह लगभग 96% नई छवियों को सही ढंग से वर्गीकृत करता है।

198
00:13:26,680 --> 00:13:30,230
और ईमानदारी से कहूं तो, यदि आप ऐसे कुछ उदाहरणों को देखें जिन पर यह गड़बड़ करता है, 

199
00:13:30,230 --> 00:13:32,540
तो आप इसे थोड़ा ढीला करने के लिए बाध्य महसूस करते हैं।

200
00:13:36,220 --> 00:13:39,844
अब यदि आप छिपी हुई परत संरचना के साथ खेलते हैं और कुछ बदलाव करते हैं, 

201
00:13:39,844 --> 00:13:41,760
तो आप इसे 98% तक प्राप्त कर सकते हैं।

202
00:13:41,760 --> 00:13:42,720
और यह बहुत अच्छा है!

203
00:13:43,020 --> 00:13:46,650
यह सबसे अच्छा नहीं है, आप निश्चित रूप से इस सादे वेनिला नेटवर्क की तुलना 

204
00:13:46,650 --> 00:13:49,584
में अधिक परिष्कृत होकर बेहतर प्रदर्शन प्राप्त कर सकते हैं, 

205
00:13:49,584 --> 00:13:52,220
लेकिन यह देखते हुए कि प्रारंभिक कार्य कितना कठिन है, 

206
00:13:52,220 --> 00:13:56,645
मुझे लगता है कि किसी भी नेटवर्क द्वारा छवियों पर इतना अच्छा प्रदर्शन करना अविश्वसनीय है, 

207
00:13:56,645 --> 00:14:00,226
यह पहले कभी नहीं देखा गया है। हमने कभी भी विशेष रूप से यह नहीं बताया कि 

208
00:14:00,226 --> 00:14:01,420
कौन से पैटर्न देखने हैं।

209
00:14:02,560 --> 00:14:07,165
मूल रूप से, जिस तरह से मैंने इस संरचना को प्रेरित किया वह हमारी आशा का वर्णन करके था, 

210
00:14:07,165 --> 00:14:10,807
कि दूसरी परत छोटे किनारों को पकड़ सकती है, कि तीसरी परत लूप और लंबी 

211
00:14:10,807 --> 00:14:13,752
रेखाओं को पहचानने के लिए उन किनारों को एक साथ जोड़ेगी, 

212
00:14:13,752 --> 00:14:17,180
और उन्हें टुकड़े किया जा सकता है अंकों को पहचानने के लिए एक साथ।

213
00:14:17,960 --> 00:14:20,400
तो क्या हमारा नेटवर्क वास्तव में यही कर रहा है?

214
00:14:21,079 --> 00:14:24,400
ख़ैर, कम से कम इस मामले में तो बिल्कुल नहीं।

215
00:14:24,820 --> 00:14:29,010
याद रखें कि पिछले वीडियो में हमने देखा था कि कैसे पहली परत के सभी न्यूरॉन्स 

216
00:14:29,010 --> 00:14:33,145
से दूसरी परत के किसी दिए गए न्यूरॉन के कनेक्शन के वजन को एक दिए गए पिक्सेल 

217
00:14:33,145 --> 00:14:37,060
पैटर्न के रूप में देखा जा सकता है जिसे दूसरी परत का न्यूरॉन उठा रहा है?

218
00:14:37,780 --> 00:14:42,472
खैर, जब हम वास्तव में इन बदलावों से जुड़े वजन के लिए ऐसा करते हैं, 

219
00:14:42,472 --> 00:14:47,936
पहली परत से दूसरी परत तक, यहां और वहां अलग-अलग छोटे किनारों को चुनने के बजाय, 

220
00:14:47,936 --> 00:14:53,680
वे, ठीक है, लगभग यादृच्छिक दिखते हैं, बस कुछ बहुत ही ढीले पैटर्न के साथ वहाँ मध्य.

221
00:14:53,760 --> 00:14:57,643
ऐसा प्रतीत होता है कि संभावित भार और पूर्वाग्रहों के अथाह रूप से बड़े 

222
00:14:57,643 --> 00:15:02,136
13,000 आयामी स्थान में, हमारे नेटवर्क ने खुद को एक छोटा सा स्थानीय न्यूनतम पाया, 

223
00:15:02,136 --> 00:15:05,520
जो कि अधिकांश छवियों को सफलतापूर्वक वर्गीकृत करने के बावजूद, 

224
00:15:05,520 --> 00:15:08,960
उन पैटर्नों को बिल्कुल नहीं पकड़ता जिनकी हम उम्मीद कर सकते थे।

225
00:15:09,780 --> 00:15:11,818
और वास्तव में इस बिंदु को स्पष्ट करने के लिए, देखें कि 

226
00:15:11,818 --> 00:15:13,820
जब आप एक यादृच्छिक छवि इनपुट करते हैं तो क्या होता है।

227
00:15:14,320 --> 00:15:18,385
यदि सिस्टम स्मार्ट था, तो आप उम्मीद कर सकते हैं कि यह अनिश्चित महसूस होगा, 

228
00:15:18,385 --> 00:15:22,342
हो सकता है कि वास्तव में उन 10 आउटपुट न्यूरॉन्स में से किसी को भी सक्रिय 

229
00:15:22,342 --> 00:15:25,215
न किया हो या उन सभी को समान रूप से सक्रिय न किया हो, 

230
00:15:25,215 --> 00:15:28,685
लेकिन इसके बजाय यह आत्मविश्वास से आपको कुछ बकवास उत्तर देता है, 

231
00:15:28,685 --> 00:15:32,587
जैसे कि यह निश्चित लगता है कि यह यादृच्छिक शोर है 5 है क्योंकि ऐसा होता 

232
00:15:32,587 --> 00:15:34,160
है कि 5 की वास्तविक छवि 5 है।

233
00:15:34,540 --> 00:15:38,296
अलग-अलग शब्दों में, भले ही यह नेटवर्क अंकों को अच्छी तरह से पहचान सकता है, 

234
00:15:38,296 --> 00:15:40,700
लेकिन उसे पता नहीं है कि उन्हें कैसे निकालना है।

235
00:15:41,420 --> 00:15:45,240
इसका बहुत कुछ कारण यह है कि यह बहुत सख्ती से प्रतिबंधित प्रशिक्षण व्यवस्था है।

236
00:15:45,880 --> 00:15:47,740
मेरा मतलब है, यहां अपने आप को नेटवर्क की जगह पर रखें।

237
00:15:48,140 --> 00:15:52,623
इसके दृष्टिकोण से, पूरे ब्रह्मांड में एक छोटे ग्रिड में केंद्रित स्पष्ट रूप से परिभाषित 

238
00:15:52,623 --> 00:15:56,953
गतिहीन अंकों के अलावा और कुछ नहीं है, और इसके लागत फ़ंक्शन ने इसे कभी भी कुछ भी करने 

239
00:15:56,953 --> 00:16:01,080
के लिए कोई प्रोत्साहन नहीं दिया, लेकिन अपने निर्णयों में पूरी तरह से आश्वस्त रहा।

240
00:16:02,120 --> 00:16:05,308
तो इस छवि के साथ कि वे दूसरी परत के न्यूरॉन्स वास्तव में क्या कर रहे हैं, 

241
00:16:05,308 --> 00:16:08,023
आपको आश्चर्य हो सकता है कि मैं इस नेटवर्क को किनारों और पैटर्न 

242
00:16:08,023 --> 00:16:09,920
को समझने की प्रेरणा के साथ क्यों पेश करूंगा।

243
00:16:09,920 --> 00:16:12,300
मेरा मतलब है, यह बिलकुल भी नहीं है कि यह क्या कर रहा है।

244
00:16:13,380 --> 00:16:17,180
खैर, यह हमारा अंतिम लक्ष्य नहीं है, बल्कि एक शुरुआती बिंदु है।

245
00:16:17,640 --> 00:16:21,451
सच कहूँ तो, यह पुरानी तकनीक है, जिस पर 80 और 90 के दशक में शोध किया गया था, 

246
00:16:21,451 --> 00:16:25,412
और अधिक विस्तृत आधुनिक वेरिएंट को समझने से पहले आपको इसे समझने की आवश्यकता है, 

247
00:16:25,412 --> 00:16:28,722
और यह स्पष्ट रूप से कुछ दिलचस्प समस्याओं को हल करने में सक्षम है, 

248
00:16:28,722 --> 00:16:33,135
लेकिन जितना अधिक आप इसमें गहराई से उतरेंगे वे छिपी हुई परतें वास्तव में काम कर रही हैं, 

249
00:16:33,135 --> 00:16:34,740
यह उतना ही कम बुद्धिमान लगता है।

250
00:16:38,480 --> 00:16:42,745
एक पल के लिए फोकस इस बात से हटा दें कि नेटवर्क कैसे सीखते हैं कि आप कैसे सीखते हैं, 

251
00:16:42,745 --> 00:16:46,300
यह तभी होगा जब आप किसी तरह यहां सामग्री के साथ सक्रिय रूप से जुड़ेंगे।

252
00:16:47,060 --> 00:16:51,572
एक बहुत ही सरल चीज जो मैं आपसे करना चाहता हूं वह यह है कि अभी रुकें और एक पल के 

253
00:16:51,572 --> 00:16:56,254
लिए गहराई से सोचें कि आप इस सिस्टम में क्या बदलाव कर सकते हैं और यह छवियों को कैसे 

254
00:16:56,254 --> 00:17:00,880
देखता है यदि आप चाहते हैं कि यह किनारों और पैटर्न जैसी चीजों को बेहतर ढंग से उठाए।

255
00:17:01,479 --> 00:17:04,410
लेकिन इससे बेहतर, वास्तव में सामग्री से जुड़ने के लिए, 

256
00:17:04,410 --> 00:17:09,099
मैं गहन शिक्षण और तंत्रिका नेटवर्क पर माइकल नीलसन की पुस्तक की अत्यधिक अनुशंसा करता हूं।

257
00:17:09,680 --> 00:17:14,863
इसमें, आप इस सटीक उदाहरण के लिए डाउनलोड करने और खेलने के लिए कोड और डेटा पा सकते हैं, 

258
00:17:14,863 --> 00:17:18,359
और पुस्तक आपको चरण दर चरण बताएगी कि वह कोड क्या कर रहा है।

259
00:17:19,300 --> 00:17:22,625
कमाल की बात यह है कि यह पुस्तक मुफ़्त है और सार्वजनिक रूप से उपलब्ध है, 

260
00:17:22,625 --> 00:17:25,304
इसलिए यदि आपको इससे कुछ मिलता है, तो नीलसन के प्रयासों के 

261
00:17:25,304 --> 00:17:27,660
लिए दान देने में मेरे साथ शामिल होने पर विचार करें।

262
00:17:27,660 --> 00:17:31,844
मैंने विवरण में कुछ अन्य संसाधन भी लिंक किए हैं जो मुझे बहुत पसंद हैं, 

263
00:17:31,844 --> 00:17:36,500
जिनमें क्रिस ओला का अभूतपूर्व और सुंदर ब्लॉग पोस्ट और डिस्टिल के लेख शामिल हैं।

264
00:17:38,280 --> 00:17:40,758
अंतिम कुछ मिनटों की बातों को यहीं समाप्त करने के लिए, 

265
00:17:40,758 --> 00:17:43,880
मैं लीशा ली के साथ हुए साक्षात्कार के एक अंश पर वापस जाना चाहता हूं।

266
00:17:44,300 --> 00:17:47,720
आपको शायद वह पिछले वीडियो से याद होगी, उसने गहन शिक्षण में पीएचडी की थी।

267
00:17:48,300 --> 00:17:52,040
इस छोटे से अंश में वह दो हालिया पेपरों के बारे में बात करती है जो वास्तव में इस बात 

268
00:17:52,040 --> 00:17:55,780
की पड़ताल करते हैं कि कुछ अधिक आधुनिक छवि पहचान नेटवर्क वास्तव में कैसे सीख रहे हैं।

269
00:17:56,120 --> 00:17:58,515
बस यह स्थापित करने के लिए कि हम बातचीत में कहाँ थे, 

270
00:17:58,515 --> 00:18:01,693
पहले पेपर ने इन विशेष रूप से गहरे तंत्रिका नेटवर्क में से एक को लिया 

271
00:18:01,693 --> 00:18:04,871
जो छवि पहचान में वास्तव में अच्छा है, और इसे उचित रूप से लेबल किए गए 

272
00:18:04,871 --> 00:18:08,740
डेटासेट पर प्रशिक्षित करने के बजाय, प्रशिक्षण से पहले सभी लेबलों को इधर-उधर कर दिया।

273
00:18:09,480 --> 00:18:12,500
स्पष्ट रूप से यहां परीक्षण सटीकता यादृच्छिक से बेहतर नहीं थी, 

274
00:18:12,500 --> 00:18:15,082
क्योंकि सब कुछ बस यादृच्छिक रूप से लेबल किया गया है, 

275
00:18:15,082 --> 00:18:18,833
लेकिन यह अभी भी उसी प्रशिक्षण सटीकता को प्राप्त करने में सक्षम था जैसा कि आप 

276
00:18:18,833 --> 00:18:20,880
उचित रूप से लेबल किए गए डेटासेट पर करेंगे।

277
00:18:21,600 --> 00:18:26,512
मूल रूप से, इस विशेष नेटवर्क के लिए लाखों वज़न केवल यादृच्छिक डेटा को याद रखने 

278
00:18:26,512 --> 00:18:31,611
के लिए पर्याप्त थे, जो यह सवाल उठाता है कि क्या इस लागत फ़ंक्शन को कम करना वास्तव 

279
00:18:31,611 --> 00:18:36,400
में छवि में किसी भी प्रकार की संरचना से मेल खाता है, या यह सिर्फ याद रखना है?

280
00:18:51,440 --> 00:18:56,506
यदि आप उस सटीकता वक्र को देखते हैं, यदि आप बस एक यादृच्छिक डेटासेट पर 

281
00:18:56,506 --> 00:19:02,586
प्रशिक्षण ले रहे थे, तो वह वक्र लगभग एक रैखिक फैशन में बहुत धीरे-धीरे नीचे चला गया, 

282
00:19:02,586 --> 00:19:08,521
इसलिए आप वास्तव में संभव के उस स्थानीय न्यूनतम को खोजने के लिए संघर्ष कर रहे हैं, 

283
00:19:08,521 --> 00:19:12,140
आप जानते हैं , सही वज़न जो आपको वह सटीकता दिलाएगा।

284
00:19:12,240 --> 00:19:16,117
जबकि यदि आप वास्तव में एक संरचित डेटासेट पर प्रशिक्षण ले रहे हैं, 

285
00:19:16,117 --> 00:19:19,936
जिसमें सही लेबल हैं, तो आप शुरुआत में थोड़ा इधर-उधर हो जाते हैं, 

286
00:19:19,936 --> 00:19:24,225
लेकिन फिर आप उस सटीकता स्तर तक पहुंचने के लिए बहुत तेजी से गिर जाते हैं, 

287
00:19:24,225 --> 00:19:28,220
और कुछ अर्थों में ऐसा होता है उस स्थानीय मैक्सिमा को ढूंढना आसान था।

288
00:19:28,540 --> 00:19:33,743
और इसलिए इसके बारे में दिलचस्प बात यह है कि यह वास्तव में कुछ साल पहले के एक और पेपर को 

289
00:19:33,743 --> 00:19:38,177
प्रकाश में लाता है, जिसमें नेटवर्क परतों के बारे में बहुत अधिक सरलीकरण है, 

290
00:19:38,177 --> 00:19:43,144
लेकिन परिणामों में से एक यह कह रहा था कि यदि आप अनुकूलन परिदृश्य को देखें, तो कैसे, 

291
00:19:43,144 --> 00:19:48,052
ये नेटवर्क जो स्थानीय मिनीमा सीखते हैं, वे वास्तव में समान गुणवत्ता वाले होते हैं, 

292
00:19:48,052 --> 00:19:50,949
इसलिए कुछ अर्थों में यदि आपका डेटासेट संरचित है, 

293
00:19:50,949 --> 00:19:54,320
तो आपको इसे और अधिक आसानी से ढूंढने में सक्षम होना चाहिए।

294
00:19:58,160 --> 00:20:01,180
पैट्रियन पर समर्थन करने वाले आप में से उन लोगों को, हमेशा की तरह, मेरा धन्यवाद।

295
00:20:01,520 --> 00:20:04,083
मैंने पहले ही कहा है कि पैट्रियन एक गेम चेंजर है, 

296
00:20:04,083 --> 00:20:06,800
लेकिन ये वीडियो वास्तव में आपके बिना संभव नहीं होंगे।

297
00:20:07,460 --> 00:20:10,006
मैं श्रृंखला के इन शुरुआती वीडियो के समर्थन के लिए वीसी 

298
00:20:10,006 --> 00:20:12,780
फर्म एम्प्लीफाई पार्टनर्स को भी विशेष धन्यवाद देना चाहता हूं।

