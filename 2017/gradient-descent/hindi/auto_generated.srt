1
00:00:00,000 --> 00:00:07,240
पिछले वीडियो में मैंने एक तंत्रिका नेटवर्क की संरचना बताई थी।

2
00:00:07,240 --> 00:00:11,560
मैं यहां एक संक्षिप्त पुनर्कथन दूंगा ताकि यह हमारे दिमाग में ताजा

3
00:00:11,560 --> 00:00:13,160
रहे, और फिर इस वीडियो के लिए मेरे दो मुख्य लक्ष्य हैं।

4
00:00:13,160 --> 00:00:17,960
सबसे पहले ग्रेडिएंट डिसेंट के विचार को पेश करना है, जो न केवल तंत्रिका

5
00:00:17,960 --> 00:00:20,800
नेटवर्क कैसे सीखते हैं, बल्कि कई अन्य मशीन लर्निंग भी काम करता है।

6
00:00:20,800 --> 00:00:25,160
फिर उसके बाद हम थोड़ा और गहराई से देखेंगे कि यह विशेष नेटवर्क कैसा

7
00:00:25,160 --> 00:00:29,560
प्रदर्शन करता है, और न्यूरॉन्स की छिपी हुई परतें आखिर क्या ढूंढती हैं।

8
00:00:29,560 --> 00:00:34,680
एक अनुस्मारक के रूप में, हमारा लक्ष्य यहां हस्तलिखित अंक

9
00:00:34,680 --> 00:00:37,080
पहचान का उत्कृष्ट उदाहरण, तंत्रिका नेटवर्क की हैलो दुनिया है।

10
00:00:37,080 --> 00:00:42,160
ये अंक 28x28 पिक्सेल ग्रिड पर प्रस्तुत किए जाते हैं, प्रत्येक पिक्सेल

11
00:00:42,160 --> 00:00:44,260
में 0 और 1 के बीच कुछ ग्रेस्केल मान होते हैं।

12
00:00:44,260 --> 00:00:51,400
वे ही नेटवर्क की इनपुट परत में 784 न्यूरॉन्स की सक्रियता निर्धारित करते हैं।

13
00:00:51,400 --> 00:00:56,880
निम्नलिखित परतों में प्रत्येक न्यूरॉन के लिए सक्रियता पिछली परत में सभी सक्रियणों के भारित

14
00:00:56,880 --> 00:01:02,300
योग के साथ-साथ कुछ विशेष संख्या पर आधारित होती है जिसे पूर्वाग्रह कहा जाता है।

15
00:01:02,300 --> 00:01:07,480
आप उस राशि को किसी अन्य फ़ंक्शन के साथ बनाते हैं, जैसे सिग्मॉइड

16
00:01:07,480 --> 00:01:09,640
स्क्विशिफिकेशन, या एक ReLU, जिस तरह से मैंने पिछले वीडियो को देखा था।

17
00:01:09,640 --> 00:01:14,960
कुल मिलाकर, प्रत्येक 16 न्यूरॉन्स वाली दो छिपी हुई परतों की कुछ हद तक मनमानी पसंद

18
00:01:14,960 --> 00:01:20,940
को देखते हुए, नेटवर्क में लगभग 13,000 वजन और पूर्वाग्रह हैं जिन्हें हम समायोजित कर सकते

19
00:01:20,940 --> 00:01:25,320
हैं, और ये मूल्य हैं जो निर्धारित करते हैं कि नेटवर्क वास्तव में क्या करता है।

20
00:01:25,320 --> 00:01:29,800
और जब हम कहते हैं कि यह नेटवर्क किसी दिए गए अंक को वर्गीकृत करता है तो हमारा मतलब

21
00:01:29,800 --> 00:01:34,080
यह है कि अंतिम परत में उन 10 न्यूरॉन्स में से सबसे चमकीला उस अंक से मेल खाता है।

22
00:01:34,080 --> 00:01:39,240
और याद रखें, स्तरित संरचना के लिए हमारे मन में जो प्रेरणा थी

23
00:01:39,240 --> 00:01:43,920
वह यह थी कि शायद दूसरी परत किनारों को पकड़ सकती है,

24
00:01:43,920 --> 00:01:48,640
तीसरी परत लूप और लाइनों जैसे पैटर्न को चुन सकती है, और आखिरी

25
00:01:48,640 --> 00:01:49,640
परत उन पैटर्न को एक साथ जोड़ सकती है अंकों को पहचानें.

26
00:01:49,640 --> 00:01:52,880
तो यहां, हम सीखते हैं कि नेटवर्क कैसे सीखता है।

27
00:01:52,880 --> 00:01:56,880
हम जो चाहते हैं वह एक एल्गोरिदम है जहां आप इस नेटवर्क को प्रशिक्षण डेटा का

28
00:01:56,880 --> 00:02:01,540
एक पूरा समूह दिखा सकते हैं, जो हस्तलिखित अंकों की विभिन्न छवियों के समूह के रूप

29
00:02:01,540 --> 00:02:06,360
में आता है, साथ ही उनके लिए लेबल भी होते हैं, और यह होगा उन 13,000

30
00:02:06,360 --> 00:02:10,760
वज़न और पूर्वाग्रहों को समायोजित करें ताकि प्रशिक्षण डेटा पर इसके प्रदर्शन में सुधार हो सके।

31
00:02:10,760 --> 00:02:15,540
उम्मीद है कि इस स्तरित संरचना का मतलब यह होगा कि यह जो सीखता

32
00:02:15,540 --> 00:02:17,840
है वह उस प्रशिक्षण डेटा से परे छवियों के लिए सामान्यीकृत होता है।

33
00:02:17,840 --> 00:02:22,240
जिस तरह से हम इसका परीक्षण करते हैं वह यह है कि नेटवर्क को प्रशिक्षित करने के बाद, आप इसे अधिक

34
00:02:22,240 --> 00:02:31,160
लेबल वाला डेटा दिखाते हैं, और आप देखते हैं कि यह उन नई छवियों को कितनी सटीकता से वर्गीकृत करता है।

35
00:02:31,160 --> 00:02:34,760
सौभाग्य से हमारे लिए, और जो बात इसे शुरू करने के लिए एक सामान्य उदाहरण बनाती है, वह

36
00:02:34,760 --> 00:02:39,520
यह है कि एमएनआईएसटी डेटाबेस के पीछे अच्छे लोगों ने हजारों हस्तलिखित अंकों वाली छवियों का एक

37
00:02:39,520 --> 00:02:45,080
संग्रह रखा है, प्रत्येक को उन संख्याओं के साथ लेबल किया गया है जिन्हें वे होना चाहिए।

38
00:02:45,080 --> 00:02:49,920
और एक मशीन को सीखने के रूप में वर्णित करना जितना उत्तेजक है, एक बार जब आप देखते हैं कि यह कैसे

39
00:02:49,920 --> 00:02:55,560
काम करता है, तो यह किसी पागल विज्ञान-फाई आधार की तरह कम और कैलकुलस अभ्यास की तरह बहुत अधिक लगता है।

40
00:02:55,560 --> 00:03:01,040
मेरा मतलब है, मूल रूप से यह एक निश्चित फ़ंक्शन का न्यूनतम पता लगाने पर निर्भर करता है।

41
00:03:01,040 --> 00:03:06,480
याद रखें, वैचारिक रूप से हम प्रत्येक न्यूरॉन को पिछली परत के सभी न्यूरॉन्स

42
00:03:06,480 --> 00:03:11,440
से जुड़े होने के रूप में सोच रहे हैं, और इसकी सक्रियता को

43
00:03:11,440 --> 00:03:16,400
परिभाषित करने वाले भारित योग में वजन उन कनेक्शनों की ताकत की तरह

44
00:03:16,400 --> 00:03:19,780
है, और पूर्वाग्रह कुछ संकेत है चाहे वह न्यूरॉन सक्रिय हो या निष्क्रिय।

45
00:03:19,780 --> 00:03:23,300
और चीजों को शुरू करने के लिए, हम उन सभी भारों और पूर्वाग्रहों

46
00:03:23,300 --> 00:03:25,020
को पूरी तरह से यादृच्छिक रूप से आरंभ करने जा रहे हैं।

47
00:03:25,020 --> 00:03:29,100
कहने की जरूरत नहीं है, यह नेटवर्क किसी दिए गए प्रशिक्षण उदाहरण पर भयानक

48
00:03:29,100 --> 00:03:31,180
प्रदर्शन करने जा रहा है, क्योंकि यह बस कुछ यादृच्छिक कर रहा है।

49
00:03:31,180 --> 00:03:36,820
उदाहरण के लिए, आप 3 की इस छवि को फ़ीड करते हैं, और आउटपुट परत बस एक गड़बड़ की तरह दिखती है।

50
00:03:36,820 --> 00:03:43,340
तो आप जो करते हैं वह एक लागत फ़ंक्शन को परिभाषित करना है, कंप्यूटर को यह बताने का एक तरीका है, नहीं,

51
00:03:43,340 --> 00:03:48,940
खराब कंप्यूटर, उस आउटपुट में सक्रियण होना चाहिए जो अधिकांश न्यूरॉन्स के लिए 0 है, लेकिन इस न्यूरॉन के लिए 1 है।

52
00:03:48,980 --> 00:03:51,740
तुमने मुझे जो दिया वह बिल्कुल कूड़ा है।

53
00:03:51,740 --> 00:03:56,740
इसे थोड़ा और गणितीय रूप से कहने के लिए, आप उन कचरा आउटपुट सक्रियणों

54
00:03:56,740 --> 00:04:01,980
में से प्रत्येक के बीच अंतर के वर्गों को जोड़ते हैं और वह मूल्य

55
00:04:01,980 --> 00:04:06,020
जो आप चाहते हैं, और इसे हम एकल प्रशिक्षण उदाहरण की लागत कहेंगे।

56
00:04:06,020 --> 00:04:12,660
ध्यान दें कि यह राशि तब छोटी होती है जब नेटवर्क आत्मविश्वास से छवि को सही ढंग से वर्गीकृत करता है, लेकिन

57
00:04:12,660 --> 00:04:18,820
यह तब बड़ी होती है जब नेटवर्क को ऐसा लगता है कि उसे पता नहीं है कि वह क्या कर रहा है।

58
00:04:18,820 --> 00:04:23,860
तो फिर आप अपने पास मौजूद हजारों प्रशिक्षण

59
00:04:23,860 --> 00:04:27,580
उदाहरणों की औसत लागत पर विचार करें।

60
00:04:27,580 --> 00:04:32,300
यह औसत लागत इस बात का पैमाना है कि नेटवर्क कितना

61
00:04:32,300 --> 00:04:33,300
ख़राब है और कंप्यूटर को कितना ख़राब महसूस होना चाहिए।

62
00:04:33,300 --> 00:04:35,300
और यह एक जटिल बात है.

63
00:04:35,300 --> 00:04:40,380
याद रखें कि नेटवर्क मूल रूप से एक फ़ंक्शन कैसे था, जो इनपुट के रूप

64
00:04:40,380 --> 00:04:46,100
में 784 नंबर लेता है, पिक्सेल मान, और आउटपुट के रूप में 10 नंबर निकालता

65
00:04:46,100 --> 00:04:49,700
है, और एक अर्थ में यह इन सभी भारों और पूर्वाग्रहों द्वारा पैरामीटरयुक्त है?

66
00:04:49,700 --> 00:04:53,340
लागत फ़ंक्शन उसके ऊपर जटिलता की एक परत है।

67
00:04:53,340 --> 00:04:59,140
यह अपने इनपुट के रूप में उन 13,000 या उससे अधिक वजन और पूर्वाग्रहों को लेता है, और एक

68
00:04:59,140 --> 00:05:04,620
एकल संख्या बताता है जो बताता है कि वे वजन और पूर्वाग्रह कितने खराब हैं, और जिस तरह से

69
00:05:04,620 --> 00:05:09,140
इसे परिभाषित किया गया है वह प्रशिक्षण डेटा के हजारों टुकड़ों पर नेटवर्क के व्यवहार पर निर्भर करता है।

70
00:05:09,140 --> 00:05:12,460
यह बहुत सोचने वाली बात है.

71
00:05:12,460 --> 00:05:16,380
लेकिन केवल कंप्यूटर को यह बताना कि वह कितना घटिया काम कर रहा है, बहुत मददगार नहीं है।

72
00:05:16,380 --> 00:05:21,300
आप इसे बताना चाहते हैं कि उन वज़न और पूर्वाग्रहों को कैसे बदला जाए ताकि यह बेहतर हो जाए।

73
00:05:21,300 --> 00:05:25,580
इसे आसान बनाने के लिए, 13,000 इनपुट वाले फ़ंक्शन की कल्पना करने के लिए संघर्ष करने के बजाय, बस एक

74
00:05:25,580 --> 00:05:31,440
साधारण फ़ंक्शन की कल्पना करें जिसमें इनपुट के रूप में एक संख्या और आउटपुट के रूप में एक संख्या हो।

75
00:05:31,440 --> 00:05:36,420
आप ऐसा इनपुट कैसे ढूंढते हैं जो इस फ़ंक्शन के मान को न्यूनतम करता है?

76
00:05:36,420 --> 00:05:41,300
कैलकुलस के छात्रों को पता होगा कि आप कभी-कभी उस न्यूनतम को स्पष्ट रूप से समझ सकते

77
00:05:41,340 --> 00:05:46,620
हैं, लेकिन वास्तव में जटिल कार्यों के लिए यह हमेशा संभव नहीं होता है, निश्चित रूप से

78
00:05:46,620 --> 00:05:51,640
हमारे जटिल जटिल तंत्रिका नेटवर्क लागत फ़ंक्शन के लिए इस स्थिति के 13,000 इनपुट संस्करण में नहीं।

79
00:05:51,640 --> 00:05:56,820
एक अधिक लचीली रणनीति किसी भी इनपुट से शुरू करना है, और यह पता लगाना है

80
00:05:56,820 --> 00:05:59,860
कि उस आउटपुट को कम करने के लिए आपको किस दिशा में कदम बढ़ाना चाहिए।

81
00:05:59,860 --> 00:06:05,020
विशेष रूप से, यदि आप उस फ़ंक्शन के ढलान का पता लगा सकते

82
00:06:05,020 --> 00:06:09,280
हैं जहां आप हैं, तो यदि ढलान सकारात्मक है तो बाईं ओर शिफ्ट

83
00:06:09,280 --> 00:06:12,720
करें, और यदि ढलान नकारात्मक है तो इनपुट को दाईं ओर शिफ्ट करें।

84
00:06:12,720 --> 00:06:17,040
यदि आप ऐसा बार-बार करते हैं, प्रत्येक बिंदु पर नए ढलान की जाँच करते हैं और

85
00:06:17,040 --> 00:06:20,680
उचित कदम उठाते हैं, तो आप फ़ंक्शन के कुछ स्थानीय न्यूनतम तक पहुँचने जा रहे हैं।

86
00:06:20,680 --> 00:06:24,600
और यहां आपके मन में जो छवि होगी वह एक पहाड़ी से लुढ़कती हुई गेंद की होगी।

87
00:06:24,600 --> 00:06:29,380
और ध्यान दें, यहां तक कि इस वास्तव में सरलीकृत एकल इनपुट फ़ंक्शन के लिए भी,

88
00:06:29,380 --> 00:06:34,220
कई संभावित घाटियां हैं जिनमें आप उतर सकते हैं, यह इस पर निर्भर करता है कि

89
00:06:34,220 --> 00:06:38,460
आप किस यादृच्छिक इनपुट से शुरू करते हैं, और इस बात की कोई गारंटी नहीं है

90
00:06:38,460 --> 00:06:39,460
कि आप जिस स्थानीय न्यूनतम पर उतरेंगे वह सबसे छोटा संभव मूल्य होगा लागत फ़ंक्शन का.

91
00:06:39,460 --> 00:06:43,180
यह हमारे तंत्रिका नेटवर्क मामले पर भी लागू होगा।

92
00:06:43,180 --> 00:06:48,140
और मैं यह भी देखना चाहता हूं कि यदि आप अपने कदमों के आकार को ढलान के

93
00:06:48,140 --> 00:06:52,920
समानुपाती बनाते हैं, तो जब ढलान न्यूनतम की ओर समतल हो जाती है, तो आपके कदम छोटे

94
00:06:52,920 --> 00:06:56,020
और छोटे होते जाते हैं, और इस तरह से आपको ओवरशूटिंग से बचने में मदद मिलती है।

95
00:06:56,020 --> 00:07:01,640
जटिलता को थोड़ा बढ़ाते हुए, दो इनपुट और एक आउटपुट वाले फ़ंक्शन की कल्पना करें।

96
00:07:01,640 --> 00:07:06,360
आप इनपुट स्पेस को xy-प्लेन के रूप में सोच सकते हैं, और लागत फ़ंक्शन

97
00:07:06,360 --> 00:07:09,020
को इसके ऊपर की सतह के रूप में ग्राफ़ किया जा सकता है।

98
00:07:09,020 --> 00:07:13,600
फ़ंक्शन के ढलान के बारे में पूछने के बजाय, आपको यह पूछना होगा कि आपको इस इनपुट स्पेस में

99
00:07:13,600 --> 00:07:19,780
किस दिशा में कदम रखना चाहिए ताकि फ़ंक्शन के आउटपुट को सबसे तेज़ी से कम किया जा सके।

100
00:07:19,780 --> 00:07:22,340
दूसरे शब्दों में, ढलान की दिशा क्या है?

101
00:07:22,340 --> 00:07:26,740
और फिर, उस पहाड़ी से लुढ़कती हुई गेंद के बारे में सोचना उपयोगी है।

102
00:07:26,740 --> 00:07:31,920
आपमें से जो लोग मल्टीवेरिएबल कैलकुलस से परिचित हैं, उन्हें पता होगा कि किसी

103
00:07:31,920 --> 00:07:37,460
फ़ंक्शन का ग्रेडिएंट आपको सबसे तेज चढ़ाई की दिशा देता है, आपको फ़ंक्शन

104
00:07:37,460 --> 00:07:39,420
को सबसे तेज़ी से बढ़ाने के लिए किस दिशा में कदम बढ़ाना चाहिए।

105
00:07:39,420 --> 00:07:43,820
स्वाभाविक रूप से, उस ग्रेडिएंट के नकारात्मक को लेने से आपको उस कदम

106
00:07:43,820 --> 00:07:47,460
की दिशा मिलती है जो फ़ंक्शन को सबसे तेज़ी से कम करता है।

107
00:07:47,460 --> 00:07:52,320
इससे भी अधिक, इस ग्रेडिएंट वेक्टर की लंबाई इस बात का

108
00:07:52,320 --> 00:07:54,580
संकेत है कि वह सबसे तीव्र ढलान कितनी तीव्र है।

109
00:07:54,580 --> 00:07:58,080
अब यदि आप मल्टीवेरिएबल कैलकुलस से अपरिचित हैं और अधिक सीखना चाहते हैं, तो इस

110
00:07:58,080 --> 00:08:01,100
विषय पर खान अकादमी के लिए मेरे द्वारा किए गए कुछ कार्यों को देखें।

111
00:08:01,100 --> 00:08:05,680
हालाँकि, ईमानदारी से कहें तो, अभी आपके और मेरे लिए यह सब मायने रखता है

112
00:08:05,680 --> 00:08:10,440
कि सिद्धांत रूप में इस वेक्टर की गणना करने का एक तरीका मौजूद है, यह

113
00:08:10,440 --> 00:08:12,040
वेक्टर आपको बताता है कि ढलान की दिशा क्या है और यह कितनी खड़ी है।

114
00:08:12,040 --> 00:08:17,280
यदि आप इतना ही जानते हैं और विवरण के मामले में आप ठोस नहीं हैं तो आपको कोई परेशानी नहीं होगी।

115
00:08:17,280 --> 00:08:21,440
क्योंकि यदि आप इसे प्राप्त कर सकते हैं, तो फ़ंक्शन को छोटा करने के लिए एल्गोरिदम इस

116
00:08:21,440 --> 00:08:27,400
ढाल दिशा की गणना करना है, फिर ढलान पर एक छोटा कदम उठाएं, और इसे बार-बार दोहराएं।

117
00:08:28,300 --> 00:08:33,700
यह किसी फ़ंक्शन के लिए वही मूल विचार है जिसमें 2 इनपुट के बजाय 13,000 इनपुट हैं।

118
00:08:33,700 --> 00:08:38,980
हमारे नेटवर्क के सभी 13,000 भारों और पूर्वाग्रहों को एक

119
00:08:38,980 --> 00:08:40,180
विशाल स्तंभ वेक्टर में व्यवस्थित करने की कल्पना करें।

120
00:08:40,180 --> 00:08:46,140
लागत फ़ंक्शन का नकारात्मक ग्रेडिएंट सिर्फ एक वेक्टर है, यह इस अत्यधिक विशाल इनपुट स्थान

121
00:08:46,140 --> 00:08:51,660
के अंदर कुछ दिशा है जो आपको बताता है कि उन सभी संख्याओं में से

122
00:08:51,660 --> 00:08:55,900
कौन सा संकेत लागत फ़ंक्शन में सबसे तेजी से कमी का कारण बनने वाला है।

123
00:08:55,900 --> 00:09:00,000
और निश्चित रूप से, हमारे विशेष रूप से डिज़ाइन किए गए लागत फ़ंक्शन के साथ, इसे

124
00:09:00,000 --> 00:09:05,520
कम करने के लिए वजन और पूर्वाग्रहों को बदलने का मतलब है कि प्रशिक्षण डेटा के

125
00:09:05,520 --> 00:09:10,280
प्रत्येक टुकड़े पर नेटवर्क का आउटपुट 10 मानों की यादृच्छिक सरणी की तरह कम दिखता है,

126
00:09:10,280 --> 00:09:11,280
और वास्तविक निर्णय की तरह अधिक दिखता है जो हम चाहते हैं इसे बनाना है.

127
00:09:11,280 --> 00:09:15,940
यह याद रखना महत्वपूर्ण है, इस लागत फ़ंक्शन में सभी प्रशिक्षण डेटा का औसत शामिल होता है, इसलिए यदि

128
00:09:15,940 --> 00:09:24,260
आप इसे कम करते हैं, तो इसका मतलब है कि यह उन सभी नमूनों पर बेहतर प्रदर्शन है।

129
00:09:24,260 --> 00:09:28,540
इस ग्रेडिएंट की कुशलता से गणना करने के लिए एल्गोरिदम, जो प्रभावी रूप से

130
00:09:28,540 --> 00:09:32,520
एक तंत्रिका नेटवर्क कैसे सीखता है, का मूल है, जिसे बैकप्रॉपैगेशन कहा जाता

131
00:09:32,520 --> 00:09:34,040
है, और मैं अगले वीडियो के बारे में बात करने जा रहा हूं।

132
00:09:34,040 --> 00:09:39,100
वहां, मैं वास्तव में प्रशिक्षण डेटा के दिए गए टुकड़े के लिए प्रत्येक वजन और पूर्वाग्रह के साथ वास्तव

133
00:09:39,100 --> 00:09:44,100
में क्या होता है, इस पर चलने के लिए समय लेना चाहता हूं, प्रासंगिक कैलकुलस और सूत्रों के

134
00:09:44,100 --> 00:09:47,980
ढेर से परे क्या हो रहा है, इसके लिए एक सहज अनुभव देने की कोशिश कर रहा हूं।

135
00:09:47,980 --> 00:09:51,780
यहीं, अभी, मुख्य बात जो मैं आपको जानना चाहता हूं, कार्यान्वयन विवरण से स्वतंत्र, वह

136
00:09:51,780 --> 00:09:56,820
यह है कि जब हम नेटवर्क सीखने के बारे में बात करते हैं तो

137
00:09:56,820 --> 00:09:59,320
हमारा मतलब यह है कि यह सिर्फ एक लागत फ़ंक्शन को कम करना है।

138
00:09:59,320 --> 00:10:02,760
और ध्यान दें, इसका एक परिणाम यह है कि इस लागत

139
00:10:02,760 --> 00:10:07,820
फ़ंक्शन के लिए एक अच्छा सुचारू आउटपुट होना महत्वपूर्ण है, ताकि

140
00:10:07,820 --> 00:10:09,340
हम ढलान पर छोटे कदम उठाकर एक स्थानीय न्यूनतम पा सकें।

141
00:10:09,340 --> 00:10:14,140
यही कारण है कि, वैसे, कृत्रिम न्यूरॉन्स में लगातार सक्रियण

142
00:10:14,140 --> 00:10:18,580
होते हैं, न कि केवल द्विआधारी तरीके से सक्रिय या

143
00:10:18,580 --> 00:10:20,440
निष्क्रिय होते हैं, जिस तरह से जैविक न्यूरॉन्स होते हैं।

144
00:10:20,440 --> 00:10:24,600
किसी फ़ंक्शन के इनपुट को नकारात्मक ग्रेडिएंट के कुछ गुणकों द्वारा

145
00:10:24,600 --> 00:10:26,960
बार-बार धकेलने की इस प्रक्रिया को ग्रेडिएंट डिसेंट कहा जाता है।

146
00:10:26,960 --> 00:10:31,760
यह लागत फ़ंक्शन के कुछ स्थानीय न्यूनतम की ओर अभिसरण करने का

147
00:10:31,760 --> 00:10:33,000
एक तरीका है, मूल रूप से इस ग्राफ में एक घाटी है।

148
00:10:33,000 --> 00:10:37,040
बेशक, मैं अभी भी दो इनपुट वाले एक फ़ंक्शन की तस्वीर दिखा रहा हूं,

149
00:10:37,040 --> 00:10:41,480
क्योंकि 13,000 आयामी इनपुट स्पेस में आपके दिमाग को घेरना थोड़ा कठिन है,

150
00:10:41,480 --> 00:10:45,220
लेकिन वास्तव में इसके बारे में सोचने का एक अच्छा गैर-स्थानिक तरीका है।

151
00:10:45,220 --> 00:10:49,100
नकारात्मक प्रवणता का प्रत्येक घटक हमें दो बातें बताता है।

152
00:10:49,100 --> 00:10:53,600
संकेत, निश्चित रूप से, हमें बताता है कि इनपुट वेक्टर के

153
00:10:53,600 --> 00:10:55,860
संबंधित घटक को ऊपर या नीचे झुकाया जाना चाहिए या नहीं।

154
00:10:55,860 --> 00:11:01,340
लेकिन महत्वपूर्ण बात यह है कि इन सभी घटकों का सापेक्ष परिमाण

155
00:11:01,340 --> 00:11:05,620
आपको बताता है कि कौन सा परिवर्तन अधिक मायने रखता है।

156
00:11:05,620 --> 00:11:09,780
आप देखते हैं, हमारे नेटवर्क में, वजन में से किसी एक के समायोजन का लागत फ़ंक्शन

157
00:11:09,780 --> 00:11:14,980
पर किसी अन्य वजन के समायोजन की तुलना में बहुत अधिक प्रभाव पड़ सकता है।

158
00:11:14,980 --> 00:11:19,440
इनमें से कुछ कनेक्शन हमारे प्रशिक्षण डेटा के लिए अधिक मायने रखते हैं।

159
00:11:19,440 --> 00:11:23,520
तो जिस तरह से आप हमारे मन-मस्तिष्क के बड़े पैमाने पर लागत फ़ंक्शन के इस ग्रेडिएंट वेक्टर के बारे

160
00:11:23,520 --> 00:11:29,740
में सोच सकते हैं, वह यह है कि यह प्रत्येक वजन और पूर्वाग्रह के सापेक्ष महत्व को एनकोड

161
00:11:29,740 --> 00:11:34,100
करता है, अर्थात, इनमें से कौन सा परिवर्तन आपके पैसे के लिए सबसे अधिक धमाका करने वाला है।

162
00:11:34,100 --> 00:11:37,360
यह वास्तव में दिशा के बारे में सोचने का एक और तरीका है।

163
00:11:37,360 --> 00:11:41,740
एक सरल उदाहरण लेने के लिए, यदि आपके पास इनपुट के रूप में दो चर के

164
00:11:41,740 --> 00:11:48,720
साथ कुछ फ़ंक्शन है, और गणना करें कि किसी विशेष बिंदु पर इसका ग्रेडिएंट 3,1

165
00:11:48,720 --> 00:11:52,880
के रूप में आता है, तो एक तरफ आप इसे यह कहकर व्याख्या कर सकते

166
00:11:52,880 --> 00:11:57,400
हैं कि जब आप हैं उस इनपुट पर खड़े होकर, इस दिशा में आगे बढ़ने से

167
00:11:57,400 --> 00:12:02,200
फ़ंक्शन सबसे तेज़ी से बढ़ता है, जब आप इनपुट बिंदुओं के विमान के ऊपर फ़ंक्शन

168
00:12:02,200 --> 00:12:03,200
को ग्राफ़ करते हैं, तो वह वेक्टर आपको सीधे ऊपर की दिशा दे रहा है।

169
00:12:03,200 --> 00:12:07,600
लेकिन इसे पढ़ने का एक और तरीका यह है कि इस पहले चर में परिवर्तन दूसरे

170
00:12:07,600 --> 00:12:12,400
चर में परिवर्तन के रूप में तीन गुना महत्व रखते हैं, कम से कम प्रासंगिक इनपुट

171
00:12:12,400 --> 00:12:17,740
के पड़ोस में, एक्स-वैल्यू को कम करने से आपके लिए बहुत अधिक प्रभाव पड़ता है हिरन.

172
00:12:17,740 --> 00:12:22,880
ठीक है, आइए ज़ूम आउट करें और संक्षेप में बताएं कि हम अब तक कहां हैं।

173
00:12:22,880 --> 00:12:28,660
नेटवर्क स्वयं 784 इनपुट और 10 आउटपुट वाला यह फ़ंक्शन है,

174
00:12:28,660 --> 00:12:30,860
जो इन सभी भारित योगों के संदर्भ में परिभाषित है।

175
00:12:30,860 --> 00:12:34,160
लागत फ़ंक्शन उसके ऊपर जटिलता की एक परत है।

176
00:12:34,160 --> 00:12:39,300
यह 13,000 वज़न और पूर्वाग्रहों को इनपुट के रूप में लेता है,

177
00:12:39,300 --> 00:12:42,640
और प्रशिक्षण उदाहरणों के आधार पर घटियापन का एक माप उगलता है।

178
00:12:42,640 --> 00:12:47,520
लागत फ़ंक्शन का ग्रेडिएंट अभी भी जटिलता की एक और परत है।

179
00:12:47,520 --> 00:12:52,860
यह हमें बताता है कि इन सभी भारों और पूर्वाग्रहों के कारण लागत फ़ंक्शन

180
00:12:52,860 --> 00:12:56,640
के मूल्य में सबसे तेज़ परिवर्तन होता है, जिसे आप यह कहकर व्याख्या कर

181
00:12:56,640 --> 00:13:03,040
सकते हैं कि किस भार में कौन सा परिवर्तन सबसे अधिक मायने रखता है।

182
00:13:03,040 --> 00:13:07,620
तो जब आप नेटवर्क को यादृच्छिक भार और पूर्वाग्रहों के साथ आरंभ करते हैं, और इस

183
00:13:07,620 --> 00:13:12,420
ग्रेडिएंट डिसेंट प्रक्रिया के आधार पर उन्हें कई बार समायोजित करते हैं, तो यह वास्तव में

184
00:13:12,420 --> 00:13:14,240
उन छवियों पर कितना अच्छा प्रदर्शन करता है जो पहले कभी नहीं देखी गई हैं?

185
00:13:14,240 --> 00:13:19,000
जिसका मैंने यहां वर्णन किया है, प्रत्येक 16 न्यूरॉन्स की दो छिपी हुई परतों के साथ, ज्यादातर सौंदर्य संबंधी कारणों

186
00:13:19,000 --> 00:13:26,920
से चुना गया है, वह बुरा नहीं है, यह लगभग 96% नई छवियों को सही ढंग से वर्गीकृत करता है।

187
00:13:26,920 --> 00:13:31,580
और ईमानदारी से कहूं तो, यदि आप ऐसे कुछ उदाहरणों को देखें जिन पर यह

188
00:13:31,580 --> 00:13:36,300
गड़बड़ करता है, तो आप इसे थोड़ा ढीला करने के लिए बाध्य महसूस करते हैं।

189
00:13:36,300 --> 00:13:40,220
यदि आप छिपी हुई परत संरचना के साथ खेलते हैं और कुछ

190
00:13:40,220 --> 00:13:41,220
बदलाव करते हैं, तो आप इसे 98% तक प्राप्त कर सकते हैं।

191
00:13:41,220 --> 00:13:42,900
और यह बहुत अच्छा है!

192
00:13:42,900 --> 00:13:47,020
यह सबसे अच्छा नहीं है, आप निश्चित रूप से इस सादे वेनिला नेटवर्क की तुलना में अधिक परिष्कृत होकर

193
00:13:47,020 --> 00:13:52,460
बेहतर प्रदर्शन प्राप्त कर सकते हैं, लेकिन यह देखते हुए कि प्रारंभिक कार्य कितना कठिन है, मुझे लगता

194
00:13:52,460 --> 00:13:56,800
है कि किसी भी नेटवर्क द्वारा छवियों पर इतना अच्छा प्रदर्शन करना अविश्वसनीय है, जैसा कि हमने पहले

195
00:13:56,800 --> 00:14:02,000
कभी नहीं देखा है। इसे विशेष रूप से कभी नहीं बताया गया कि कौन से पैटर्न देखने हैं।

196
00:14:02,000 --> 00:14:07,840
मूल रूप से, जिस तरह से मैंने इस संरचना को प्रेरित किया वह हमारी आशा

197
00:14:07,840 --> 00:14:11,880
का वर्णन करके था, कि दूसरी परत छोटे किनारों को पकड़ सकती है, कि तीसरी

198
00:14:11,880 --> 00:14:16,080
परत लूप और लंबी रेखाओं को पहचानने के लिए उन किनारों को एक साथ जोड़ेगी,

199
00:14:16,080 --> 00:14:18,220
और उन्हें टुकड़े किया जा सकता है अंकों को पहचानने के लिए एक साथ।

200
00:14:18,220 --> 00:14:21,040
तो क्या हमारा नेटवर्क वास्तव में यही कर रहा है?

201
00:14:21,040 --> 00:14:24,880
ख़ैर, कम से कम इस मामले में तो बिल्कुल नहीं।

202
00:14:24,960 --> 00:14:29,120
याद रखें कि पिछले वीडियो में हमने देखा था कि कैसे पहली परत के सभी न्यूरॉन्स से

203
00:14:29,120 --> 00:14:33,900
दूसरी परत के किसी दिए गए न्यूरॉन के कनेक्शन के वजन को एक दिए गए पिक्सेल

204
00:14:33,900 --> 00:14:37,440
पैटर्न के रूप में देखा जा सकता है जिसे दूसरी परत का न्यूरॉन उठा रहा है?

205
00:14:37,440 --> 00:14:44,600
खैर, जब हम इन बदलावों से जुड़े वजनों के लिए ऐसा करते

206
00:14:44,600 --> 00:14:51,000
हैं, तो यहां-वहां अलग-अलग छोटे किनारों को उठाने के बजाय, वे लगभग

207
00:14:51,000 --> 00:14:54,200
यादृच्छिक दिखते हैं, बस बीच में कुछ बहुत ढीले पैटर्न के साथ।

208
00:14:54,200 --> 00:14:59,020
ऐसा प्रतीत होता है कि संभावित भार और पूर्वाग्रहों के अथाह रूप से

209
00:14:59,020 --> 00:15:04,020
बड़े 13,000 आयामी स्थान में, हमारे नेटवर्क ने खुद को एक छोटा सा

210
00:15:04,020 --> 00:15:08,440
स्थानीय न्यूनतम पाया, जो कि अधिकांश छवियों को सफलतापूर्वक वर्गीकृत करने के बावजूद,

211
00:15:08,440 --> 00:15:09,840
उन पैटर्नों को बिल्कुल नहीं पकड़ता है जिनकी हम उम्मीद कर सकते थे।

212
00:15:09,840 --> 00:15:14,600
और वास्तव में इस बिंदु को स्पष्ट करने के लिए, देखें कि जब आप एक यादृच्छिक छवि इनपुट करते हैं तो क्या होता है।

213
00:15:14,600 --> 00:15:19,240
यदि सिस्टम स्मार्ट था, तो आप उम्मीद कर सकते हैं कि यह या तो अनिश्चित महसूस करेगा, हो सकता है कि

214
00:15:19,240 --> 00:15:24,120
वास्तव में उन 10 आउटपुट न्यूरॉन्स में से किसी को सक्रिय न कर रहा हो या उन सभी को समान रूप

215
00:15:24,520 --> 00:15:29,800
से सक्रिय न कर रहा हो, लेकिन इसके बजाय यह आत्मविश्वास से आपको कुछ बकवास उत्तर देता है, जैसे कि यह

216
00:15:29,800 --> 00:15:34,560
निश्चित लगता है कि यह यादृच्छिक है शोर 5 है क्योंकि ऐसा होता है कि 5 की वास्तविक छवि 5 है।

217
00:15:34,560 --> 00:15:39,300
अलग-अलग शब्दों में, भले ही यह नेटवर्क अंकों को अच्छी तरह से पहचान

218
00:15:39,300 --> 00:15:41,800
सकता है, लेकिन उसे पता नहीं है कि उन्हें कैसे निकालना है।

219
00:15:41,800 --> 00:15:45,400
इसका बहुत कुछ कारण यह है कि यह बहुत सख्ती से प्रतिबंधित प्रशिक्षण व्यवस्था है।

220
00:15:45,400 --> 00:15:48,220
मेरा मतलब है, यहां अपने आप को नेटवर्क की जगह पर रखें।

221
00:15:48,220 --> 00:15:53,280
इसके दृष्टिकोण से, पूरे ब्रह्मांड में एक छोटे ग्रिड में केंद्रित स्पष्ट रूप से परिभाषित गतिहीन अंकों

222
00:15:53,280 --> 00:15:58,560
के अलावा और कुछ नहीं है, और इसके लागत फ़ंक्शन ने इसे कभी भी कुछ भी

223
00:15:58,560 --> 00:16:02,160
करने के लिए कोई प्रोत्साहन नहीं दिया, लेकिन अपने निर्णयों में पूरी तरह से आश्वस्त रहा।

224
00:16:02,160 --> 00:16:05,760
तो इस छवि के साथ कि वे दूसरी परत के न्यूरॉन्स वास्तव में

225
00:16:05,760 --> 00:16:09,320
क्या कर रहे हैं, आपको आश्चर्य हो सकता है कि मैं इस नेटवर्क

226
00:16:09,320 --> 00:16:10,320
को किनारों और पैटर्न को समझने की प्रेरणा के साथ क्यों पेश करूंगा।

227
00:16:10,320 --> 00:16:13,040
मेरा मतलब है, यह बिलकुल भी नहीं है कि यह क्या कर रहा है।

228
00:16:13,040 --> 00:16:17,480
खैर, यह हमारा अंतिम लक्ष्य नहीं है, बल्कि एक शुरुआती बिंदु है।

229
00:16:17,480 --> 00:16:22,280
सच कहूँ तो, यह पुरानी तकनीक है, जिस पर 80 और 90 के दशक में शोध किया गया था,

230
00:16:22,280 --> 00:16:26,920
और अधिक विस्तृत आधुनिक वेरिएंट को समझने से पहले आपको इसे समझने की आवश्यकता है, और यह स्पष्ट

231
00:16:26,920 --> 00:16:31,380
रूप से कुछ दिलचस्प समस्याओं को हल करने में सक्षम है, लेकिन जितना अधिक आप इसमें गहराई से

232
00:16:31,380 --> 00:16:38,720
उतरेंगे वे छिपी हुई परतें वास्तव में काम कर रही हैं, यह उतना ही कम बुद्धिमान लगता है।

233
00:16:38,720 --> 00:16:43,540
एक पल के लिए फोकस इस बात से हटा दें कि नेटवर्क कैसे सीखते हैं कि आप कैसे

234
00:16:43,540 --> 00:16:47,160
सीखते हैं, यह तभी होगा जब आप किसी तरह यहां सामग्री के साथ सक्रिय रूप से जुड़ेंगे।

235
00:16:47,160 --> 00:16:51,920
एक बहुत ही सरल चीज जो मैं आपसे करना चाहता हूं वह यह है कि अभी रुकें और एक पल

236
00:16:51,920 --> 00:16:57,560
के लिए गहराई से सोचें कि आप इस सिस्टम में क्या बदलाव कर सकते हैं और यह छवियों को

237
00:16:57,560 --> 00:17:01,880
कैसे देखता है यदि आप चाहते हैं कि यह किनारों और पैटर्न जैसी चीजों को बेहतर ढंग से उठाए।

238
00:17:01,880 --> 00:17:06,360
लेकिन इससे बेहतर, वास्तव में सामग्री से जुड़ने के लिए, मैं गहन शिक्षण

239
00:17:06,360 --> 00:17:09,720
और तंत्रिका नेटवर्क पर माइकल नीलसन की पुस्तक की अत्यधिक अनुशंसा करता हूं।

240
00:17:09,720 --> 00:17:15,200
इसमें, आप इस सटीक उदाहरण के लिए डाउनलोड करने और खेलने के लिए कोड और डेटा पा

241
00:17:15,200 --> 00:17:19,360
सकते हैं, और पुस्तक आपको चरण दर चरण बताएगी कि वह कोड क्या कर रहा है।

242
00:17:19,360 --> 00:17:23,920
कमाल की बात यह है कि यह पुस्तक मुफ़्त है और सार्वजनिक रूप से उपलब्ध है, इसलिए यदि आपको इससे

243
00:17:23,920 --> 00:17:28,040
कुछ मिलता है, तो नीलसन के प्रयासों के लिए दान देने में मेरे साथ शामिल होने पर विचार करें।

244
00:17:28,040 --> 00:17:32,060
मैंने विवरण में कुछ अन्य संसाधन भी लिंक किए हैं जो मुझे बहुत पसंद हैं,

245
00:17:32,060 --> 00:17:38,720
जिनमें क्रिस ओला का अभूतपूर्व और सुंदर ब्लॉग पोस्ट और डिस्टिल के लेख शामिल हैं।

246
00:17:38,720 --> 00:17:41,960
अंतिम कुछ मिनटों की बातों को यहीं समाप्त करने के लिए, मैं लीशा

247
00:17:41,960 --> 00:17:44,440
ली के साथ हुए साक्षात्कार के एक अंश पर वापस जाना चाहता हूं।

248
00:17:44,440 --> 00:17:48,520
आपको शायद वह पिछले वीडियो से याद होगी, उसने गहन शिक्षण में पीएचडी की थी।

249
00:17:48,560 --> 00:17:52,240
इस छोटे से स्निपेट में, वह दो हालिया पेपरों के बारे में बात करती है जो वास्तव में इस

250
00:17:52,240 --> 00:17:56,380
बात की पड़ताल करते हैं कि कुछ अधिक आधुनिक छवि पहचान नेटवर्क वास्तव में कैसे सीख रहे हैं।

251
00:17:56,380 --> 00:18:00,320
बस यह स्थापित करने के लिए कि हम बातचीत में कहाँ थे, पहले पेपर ने इन विशेष रूप से गहरे

252
00:18:00,320 --> 00:18:04,480
तंत्रिका नेटवर्क में से एक को लिया जो छवि पहचान में वास्तव में अच्छा है, और इसे उचित रूप से

253
00:18:04,480 --> 00:18:09,400
लेबल किए गए डेटासेट पर प्रशिक्षित करने के बजाय, इसने प्रशिक्षण से पहले सभी लेबलों को इधर-उधर कर दिया।

254
00:18:09,400 --> 00:18:13,840
जाहिर तौर पर यहां परीक्षण की सटीकता यादृच्छिक से बेहतर नहीं होगी,

255
00:18:13,840 --> 00:18:15,320
क्योंकि हर चीज को यादृच्छिक रूप से लेबल किया गया है।

256
00:18:15,320 --> 00:18:20,080
लेकिन यह अभी भी उसी प्रशिक्षण सटीकता को प्राप्त करने में सक्षम था जैसा

257
00:18:20,080 --> 00:18:21,440
कि आप उचित रूप से लेबल किए गए डेटासेट पर प्राप्त करते हैं।

258
00:18:21,440 --> 00:18:26,120
मूल रूप से, इस विशेष नेटवर्क के लिए लाखों वज़न केवल यादृच्छिक डेटा को याद रखने के लिए

259
00:18:26,120 --> 00:18:31,040
पर्याप्त थे, जो यह सवाल उठाता है कि क्या इस लागत फ़ंक्शन को कम करना वास्तव में

260
00:18:31,040 --> 00:18:36,720
छवि में किसी भी प्रकार की संरचना से मेल खाता है, या यह सिर्फ याद रखना है?

261
00:18:36,720 --> 00:18:40,120
. . . सही वर्गीकरण क्या है, इसके संपूर्ण डेटासेट को याद रखना।

262
00:18:40,120 --> 00:18:45,720
और इसलिए कुछ, आप जानते हैं, आधे साल बाद इस साल आईसीएमएल में, वास्तव

263
00:18:45,720 --> 00:18:50,440
में खंडन पत्र नहीं था, लेकिन पेपर जिसमें कुछ पहलुओं को संबोधित किया गया

264
00:18:50,440 --> 00:18:52,220
था, अरे, वास्तव में ये नेटवर्क उससे थोड़ा अधिक स्मार्ट काम कर रहे हैं।

265
00:18:52,220 --> 00:18:59,600
यदि आप उस सटीकता वक्र को देखते हैं, यदि आप बस एक यादृच्छिक डेटासेट पर प्रशिक्षण ले रहे

266
00:18:59,600 --> 00:19:05,240
थे, तो वह वक्र बहुत नीचे चला गया, आप जानते हैं, लगभग एक रैखिक फैशन में बहुत धीरे-धीरे।

267
00:19:05,280 --> 00:19:10,840
तो आप वास्तव में संभव के उस स्थानीय न्यूनतम को खोजने के लिए संघर्ष

268
00:19:10,840 --> 00:19:12,320
कर रहे हैं, आप जानते हैं, सही वजन जो आपको वह सटीकता दिलाएगा।

269
00:19:12,320 --> 00:19:16,720
जबकि यदि आप वास्तव में एक संरचित डेटासेट पर प्रशिक्षण ले रहे हैं, जिसमें सही

270
00:19:16,720 --> 00:19:20,240
लेबल हैं, तो आप जानते हैं, आप शुरुआत में थोड़ा इधर-उधर करते हैं, लेकिन फिर

271
00:19:20,240 --> 00:19:23,360
आप उस सटीकता स्तर तक पहुंचने के लिए बहुत तेजी से गिर जाते हैं।

272
00:19:23,360 --> 00:19:28,580
और इसलिए कुछ अर्थों में उस स्थानीय मैक्सिमा को खोजना आसान था।

273
00:19:28,580 --> 00:19:32,900
और इसलिए इसके बारे में दिलचस्प बात यह है कि यह वास्तव

274
00:19:32,900 --> 00:19:39,140
में कुछ साल पहले के एक और पेपर को प्रकाश में लाता

275
00:19:39,140 --> 00:19:40,140
है, जिसमें नेटवर्क परतों के बारे में बहुत अधिक सरलीकरण है।

276
00:19:40,140 --> 00:19:43,880
लेकिन परिणामों में से एक यह बता रहा था कि, यदि आप अनुकूलन परिदृश्य को देखें,

277
00:19:43,880 --> 00:19:49,400
तो ये नेटवर्क जो स्थानीय मिनीमा सीखते हैं, वे वास्तव में समान गुणवत्ता के हैं।

278
00:19:49,400 --> 00:19:54,300
तो कुछ अर्थों में, यदि आपका डेटा सेट संरचित है, तो आपको इसे और अधिक आसानी से ढूंढने में सक्षम होना चाहिए।

279
00:19:58,580 --> 00:20:01,140
आपमें से पैट्रियन का समर्थन करने वालों को हमेशा की तरह मेरा धन्यवाद।

280
00:20:01,480 --> 00:20:05,440
मैंने पहले ही कहा है कि पैट्रियन पर गेम चेंजर क्या

281
00:20:05,440 --> 00:20:07,160
है, लेकिन ये वीडियो वास्तव में आपके बिना संभव नहीं होंगे।

282
00:20:07,160 --> 00:20:11,540
मैं वीसी फर्म एम्प्लीफाई पार्टनर्स और श्रृंखला के इन शुरुआती वीडियो

283
00:20:11,540 --> 00:20:13,240
के लिए उनके समर्थन को भी विशेष धन्यवाद देना चाहता हूं।

284
00:20:31,140 --> 00:20:33,140
धन्यवाद।

