[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "En el último vídeo expuse la estructura de una red neuronal.",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "Daré un resumen rápido aquí para que esté fresco en nuestras mentes y luego tengo dos objetivos principales para este video.",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "La primera es introducir la idea del descenso de gradiente, que subyace no sólo a cómo aprenden las redes neuronales, sino también a cómo funcionan muchos otros tipos de aprendizaje automático.",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "Luego, profundizaremos un poco más en cómo funciona esta red en particular y qué terminan buscando esas capas ocultas de neuronas.",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "Como recordatorio, nuestro objetivo aquí es el ejemplo clásico de reconocimiento de dígitos escritos a mano, el hola mundo de las redes neuronales.",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "Estos dígitos se representan en una cuadrícula de 28x28 píxeles, cada píxel con un valor de escala de grises entre 0 y 1.",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "Esos son los que determinan las activaciones de 784 neuronas en la capa de entrada de la red.",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "Y luego, la activación de cada neurona en las siguientes capas se basa en una suma ponderada de todas las activaciones en la capa anterior, más un número especial llamado sesgo.",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "Luego compones esa suma con alguna otra función, como la compresión sigmoidea, o un relu, como vi en el último video.",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "En total, dada la elección algo arbitraria de dos capas ocultas con 16 neuronas cada una, la red tiene alrededor de 13.000 pesos y sesgos que podemos ajustar, y son estos valores los que determinan qué hace exactamente la red.",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "Entonces lo que queremos decir cuando decimos que esta red clasifica un dígito determinado es que la más brillante de esas 10 neuronas en la capa final corresponde a ese dígito.",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "Y recuerde, la motivación que teníamos en mente aquí para la estructura en capas era que tal vez la segunda capa podría captar los bordes, y la tercera capa podría captar patrones como bucles y líneas, y la última podría simplemente unir esos Patrones para reconocer dígitos.",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "Entonces aquí aprendemos cómo aprende la red.",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "Lo que queremos es un algoritmo donde puedas mostrarle a esta red una gran cantidad de datos de entrenamiento, que vienen en forma de un montón de imágenes diferentes de dígitos escritos a mano, junto con etiquetas de lo que se supone que son, y ajuste esos 13.000 pesos y sesgos para mejorar su rendimiento en los datos de entrenamiento.",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "Con suerte, esta estructura en capas significará que lo que aprende se generalizará a imágenes más allá de los datos de entrenamiento.",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "La forma en que lo probamos es que después de entrenar la red, le muestra más datos etiquetados que nunca antes se había visto y ve con qué precisión clasifica esas nuevas imágenes.",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "Afortunadamente para nosotros, y lo que hace que este sea un ejemplo tan común para empezar, es que las buenas personas detrás de la base de datos MNIST han reunido una colección de decenas de miles de imágenes de dígitos escritas a mano, cada una etiquetada con los números que se supone que deben tener. ser.",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "Y por muy provocativo que sea describir una máquina como aprendizaje, una vez que ves cómo funciona, se siente mucho menos como una loca premisa de ciencia ficción y mucho más como un ejercicio de cálculo.",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "Quiero decir, básicamente todo se reduce a encontrar el mínimo de una determinada función.",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "Recuerde, conceptualmente, estamos pensando en cada neurona como si estuviera conectada a todas las neuronas de la capa anterior, y los pesos en la suma ponderada que define su activación son como las fortalezas de esas conexiones, y el sesgo es una indicación de si esa neurona tiende a estar activa o inactiva.",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "Y para empezar, vamos a inicializar todos esos pesos y sesgos de forma totalmente aleatoria.",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "No hace falta decir que esta red tendrá un rendimiento bastante terrible en un ejemplo de entrenamiento determinado, ya que simplemente está haciendo algo aleatorio.",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "Por ejemplo, introduce esta imagen de un 3 y la capa de salida parece un desastre.",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "Entonces lo que haces es definir una función de costo, una forma de decirle a la computadora, no, computadora mala, que la salida debe tener activaciones que son 0 para la mayoría de las neuronas, pero 1 para esta neurona, lo que me diste es una completa basura.",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "Para decirlo un poco más matemáticamente, sumas los cuadrados de las diferencias entre cada una de esas activaciones de salida de basura y el valor que quieres que tengan, y esto es lo que llamaremos el costo de un solo ejemplo de entrenamiento.",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "Observe que esta suma es pequeña cuando la red clasifica correctamente la imagen con confianza, pero es grande cuando la red parece no saber lo que está haciendo.",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "Entonces, lo que debe hacer es considerar el costo promedio de las decenas de miles de ejemplos de capacitación a su disposición.",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "Este costo promedio es nuestra medida de qué tan mala es la red y qué tan mal debería funcionar la computadora.",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "Y eso es algo complicado.",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "¿Recuerda que la red en sí era básicamente una función, una que toma 784 números como entradas, los valores de píxeles, y escupe 10 números como salida, y en cierto sentido está parametrizada por todos estos pesos y sesgos?",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "Bueno, la función de costos es una capa de complejidad además de eso.",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "Toma como entrada esos aproximadamente 13.000 pesos y sesgos, y escupe un solo número que describe qué tan malos son esos pesos y sesgos, y la forma en que se define depende del comportamiento de la red sobre las decenas de miles de datos de entrenamiento.",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "Hay mucho en qué pensar.",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "Pero simplemente decirle a la computadora el mal trabajo que está haciendo no es de mucha ayuda.",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "Quiere decirle cómo cambiar esos pesos y sesgos para que mejore.",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "Para hacerlo más fácil, en lugar de esforzarse por imaginar una función con 13.000 entradas, imagine una función simple que tenga un número como entrada y un número como salida.",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "¿Cómo se encuentra una entrada que minimice el valor de esta función?",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "Los estudiantes de cálculo sabrán que a veces se puede calcular ese mínimo explícitamente, pero eso no siempre es factible para funciones realmente complicadas, ciertamente no en la versión de 13,000 entradas de esta situación para nuestra loca y complicada función de costo de red neuronal.",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "Una táctica más flexible es comenzar en cualquier entrada y determinar en qué dirección debe avanzar para reducir esa salida.",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "Específicamente, si puedes calcular la pendiente de la función en la que te encuentras, entonces cambia hacia la izquierda si esa pendiente es positiva y cambia la entrada hacia la derecha si esa pendiente es negativa.",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "Si hace esto repetidamente, verificando en cada punto la nueva pendiente y dando el paso apropiado, se acercará a algún mínimo local de la función.",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "La imagen que quizás tengas en mente aquí es la de una pelota rodando colina abajo.",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "Tenga en cuenta que, incluso para esta función de entrada única realmente simplificada, hay muchos valles posibles en los que podría aterrizar, dependiendo de en qué entrada aleatoria comience, y no hay garantía de que el mínimo local en el que aterrice será el valor más pequeño posible. de la función de costos.",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "Esto también se trasladará a nuestro caso de redes neuronales.",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "También quiero que notes cómo si haces que el tamaño de tus pasos sea proporcional a la pendiente, cuando la pendiente se aplana hacia el mínimo, tus pasos se vuelven cada vez más pequeños, y eso te ayuda a no sobrepasarte.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "Para aumentar un poco la complejidad, imaginemos una función con dos entradas y una salida.",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "Se podría pensar en el espacio de entrada como el plano xy y en la función de costo graficada como una superficie encima de él.",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "En lugar de preguntar sobre la pendiente de la función, debe preguntar en qué dirección debe avanzar en este espacio de entrada para disminuir la salida de la función más rápidamente.",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "En otras palabras, ¿cuál es la dirección cuesta abajo?",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "Nuevamente, es útil pensar en una pelota rodando colina abajo.",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "Aquellos de ustedes que estén familiarizados con el cálculo multivariable sabrán que el gradiente de una función les da la dirección del ascenso más pronunciado, en qué dirección deben avanzar para aumentar la función más rápidamente.",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "Naturalmente, tomar el negativo de ese gradiente le da la dirección del paso que disminuye la función más rápidamente.",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "Aún más que eso, la longitud de este vector de gradiente es una indicación de cuán pronunciada es la pendiente más pronunciada.",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "Si no está familiarizado con el cálculo multivariable y desea obtener más información, consulte algunos de los trabajos que hice para Khan Academy sobre el tema.",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "Honestamente, lo único que nos importa a ti y a mí en este momento es que, en principio, existe una manera de calcular este vector, este vector que te dice cuál es la dirección cuesta abajo y qué tan empinada es.",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "Estarás bien si eso es todo lo que sabes y no eres muy sólido en los detalles.",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "Si puede conseguirlo, el algoritmo para minimizar la función es calcular esta dirección del gradiente, luego dar un pequeño paso cuesta abajo y repetirlo una y otra vez.",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "Es la misma idea básica para una función que tiene 13.000 entradas en lugar de 2 entradas.",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "Imagine organizar los 13.000 pesos y sesgos de nuestra red en un vector de columna gigante.",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "El gradiente negativo de la función de costos es solo un vector, es una dirección dentro de este espacio de entrada increíblemente enorme que le indica qué empujones a todos esos números causarán la disminución más rápida de la función de costos.",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "Y, por supuesto, con nuestra función de costos especialmente diseñada, cambiar los pesos y sesgos para disminuirlos significa hacer que la salida de la red en cada pieza de datos de entrenamiento se parezca menos a una matriz aleatoria de 10 valores y más a una decisión real que queremos. que hacer.",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "Es importante recordar que esta función de costo implica un promedio de todos los datos de entrenamiento, por lo que si la minimiza, significa que hay un mejor rendimiento en todas esas muestras.",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "El algoritmo para calcular este gradiente de manera eficiente, que es efectivamente el corazón de cómo aprende una red neuronal, se llama retropropagación y es de lo que hablaré en el próximo video.",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "Allí, realmente quiero tomarme el tiempo para explicar qué sucede exactamente con cada peso y sesgo para un determinado dato de entrenamiento, tratando de dar una idea intuitiva de lo que sucede más allá del montón de cálculos y fórmulas relevantes.",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "Aquí y ahora, lo principal que quiero que sepan, independientemente de los detalles de implementación, es que lo que queremos decir cuando hablamos de aprendizaje en red es que simplemente minimiza una función de costo.",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "Y observen, una consecuencia de esto es que es importante que esta función de costos tenga un resultado agradable y fluido, de modo que podamos encontrar un mínimo local dando pequeños pasos cuesta abajo.",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "Esta es la razón por la que, dicho sea de paso, las neuronas artificiales tienen activaciones que varían continuamente, en lugar de simplemente estar activas o inactivas de forma binaria, como lo son las neuronas biológicas.",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "Este proceso de empujar repetidamente una entrada de una función por algún múltiplo del gradiente negativo se llama descenso de gradiente.",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "Es una forma de converger hacia algún mínimo local de una función de costo, básicamente un valle en este gráfico.",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "Todavía estoy mostrando la imagen de una función con dos entradas, por supuesto, porque los empujones en un espacio de entrada de 13.000 dimensiones son un poco difíciles de entender, pero hay una buena manera no espacial de pensar en esto.",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "Cada componente del gradiente negativo nos dice dos cosas.",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "El signo, por supuesto, nos dice si el componente correspondiente del vector de entrada debe desplazarse hacia arriba o hacia abajo.",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "Pero lo más importante es que las magnitudes relativas de todos estos componentes indican qué cambios importan más.",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "Verá, en nuestra red, un ajuste a uno de los pesos podría tener un impacto mucho mayor en la función de costos que el ajuste a algún otro peso.",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "Algunas de estas conexiones simplemente importan más para nuestros datos de entrenamiento.",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "Entonces, una forma de pensar en este vector de gradiente de nuestra función de costos alucinantemente masiva es que codifica la importancia relativa de cada peso y sesgo, es decir, cuál de estos cambios generará el mayor beneficio por su inversión.",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "En realidad, ésta es sólo otra forma de pensar acerca de la dirección.",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "Para tomar un ejemplo más simple, si tienes alguna función con dos variables como entrada y calculas que su gradiente en algún punto particular resulta como 3,1, entonces, por un lado, puedes interpretar que eso dice que cuando Si estás parado en esa entrada, moverte en esta dirección aumenta la función más rápidamente, cuando graficas la función sobre el plano de los puntos de entrada, ese vector es lo que te da la dirección recta cuesta arriba.",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "Pero otra forma de leer esto es decir que los cambios en esta primera variable tienen 3 veces más importancia que los cambios en la segunda variable, que al menos en la vecindad de la entrada relevante, empujar el valor de x conlleva mucho más impacto para su dólar.",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "Alejémonos y resumamos dónde estamos hasta ahora.",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "La red en sí es esta función con 784 entradas y 10 salidas, definida en términos de todas estas sumas ponderadas.",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "La función de costos es una capa de complejidad además de eso.",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "Toma los 13.000 pesos y sesgos como entradas y escupe una única medida de pésima calidad basada en los ejemplos de entrenamiento.",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "Y el gradiente de la función de costos es aún una capa más de complejidad.",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "Nos dice qué empujones a todas estas ponderaciones y sesgos causan el cambio más rápido en el valor de la función de costo, lo que podría interpretarse como que indica qué cambios en qué ponderaciones son más importantes.",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "Entonces, cuando inicializas la red con pesos y sesgos aleatorios y los ajustas muchas veces según este proceso de descenso de gradiente, ¿qué tan bien funciona realmente en imágenes que nunca antes se habían visto?",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "La que he descrito aquí, con las dos capas ocultas de 16 neuronas cada una, elegidas principalmente por razones estéticas, no está mal, ya que clasifica correctamente alrededor del 96% de las nuevas imágenes que ve.",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "Y, sinceramente, si nos fijamos en algunos de los ejemplos en los que se equivoca, se sentirá obligado a dejarlo un poco de lado.",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "Ahora, si juegas con la estructura de capas ocultas y haces un par de ajustes, puedes conseguir esto hasta un 98%.",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "¡Y eso es bastante bueno!",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "No es lo mejor, ciertamente se puede obtener un mejor rendimiento si se vuelve más sofisticado que esta simple red básica, pero dado lo desalentadora que es la tarea inicial, creo que hay algo increíble en que cualquier red funcione tan bien en imágenes que nunca antes se había visto, dado que nunca le dijimos específicamente qué patrones buscar.",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "Originalmente, la forma en que motivé esta estructura fue describiendo una esperanza que podríamos tener: que la segunda capa pudiera recoger pequeños bordes, que la tercera capa uniera esos bordes para reconocer bucles y líneas más largas, y que esos pudieran reconstruirse. juntos para reconocer dígitos.",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "Entonces, ¿es esto lo que realmente está haciendo nuestra red?",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "Bueno, al menos para este, en absoluto.",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "¿Recuerda cómo en el último video vimos cómo los pesos de las conexiones de todas las neuronas en la primera capa a una neurona determinada en la segunda capa se pueden visualizar como un patrón de píxeles determinado que la neurona de la segunda capa está captando?",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "Bueno, cuando realmente hacemos eso para los pesos asociados con estas transiciones, de la primera capa a la siguiente, en lugar de seleccionar pequeños bordes aislados aquí y allá, se ven, bueno, casi aleatorios, sólo que con algunos patrones muy sueltos en el medio ahí.",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "Parecería que en el insondable espacio de 13.000 dimensiones de posibles pesos y sesgos, nuestra red se encontró como un pequeño y feliz mínimo local que, a pesar de clasificar con éxito la mayoría de las imágenes, no capta exactamente los patrones que podríamos haber esperado.",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "Y para aclarar este punto, observe lo que sucede cuando ingresa una imagen aleatoria.",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "Si el sistema fuera inteligente, se podría esperar que se sintiera inseguro, tal vez sin activar realmente ninguna de esas 10 neuronas de salida o activarlas todas de manera uniforme, pero en lugar de eso, con confianza, le da alguna respuesta sin sentido, como si se sintiera tan seguro de que este ruido aleatorio. es un 5 ya que una imagen real de un 5 es un 5.",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "Dicho de otra manera, incluso si esta red puede reconocer dígitos bastante bien, no tiene idea de cómo dibujarlos.",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "Mucho de esto se debe a que es una configuración de entrenamiento muy restringida.",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "Quiero decir, ponte en el lugar de la red aquí.",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "Desde su punto de vista, el universo entero no consiste más que en dígitos inmóviles claramente definidos y centrados en una pequeña cuadrícula, y su función de costos nunca le dio ningún incentivo para tener otra cosa que no sea una confianza absoluta en sus decisiones.",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "Entonces, con esto como imagen de lo que realmente están haciendo esas neuronas de la segunda capa, uno podría preguntarse por qué introduciría esta red con la motivación de detectar bordes y patrones.",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "Quiero decir, eso no es en absoluto lo que termina haciendo.",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "Bueno, este no pretende ser nuestro objetivo final, sino un punto de partida.",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "Francamente, esta es una tecnología antigua, del tipo que se investigó en los años 80 y 90, y es necesario comprenderla antes de poder comprender variantes modernas más detalladas, y claramente es capaz de resolver algunos problemas interesantes, pero cuanto más se profundiza en lo que Cuanto más funcionan esas capas ocultas, menos inteligente parece.",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "Cambiando el enfoque por un momento de cómo aprenden las redes a cómo aprendes tú, eso sólo sucederá si de alguna manera te involucras activamente con el material aquí.",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "Una cosa bastante simple que quiero que hagas es hacer una pausa ahora mismo y pensar profundamente por un momento sobre los cambios que podrías hacer en este sistema y cómo percibe las imágenes si quisieras que captara mejor cosas como bordes y patrones.",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "Pero mejor que eso, para realmente involucrarme con el material, recomiendo ampliamente el libro de Michael Nielsen sobre aprendizaje profundo y redes neuronales.",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "En él, puede encontrar el código y los datos para descargar y jugar con este ejemplo exacto, y el libro le explicará paso a paso lo que hace ese código.",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "Lo sorprendente es que este libro es gratuito y está disponible públicamente, así que si obtienes algo de él, considera unirte a mí para hacer una donación a los esfuerzos de Nielsen.",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "También vinculé un par de recursos más que me gustan mucho en la descripción, incluida la hermosa y fenomenal publicación de blog de Chris Ola y los artículos en Distill.",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "Para cerrar los últimos minutos, quiero volver a un fragmento de la entrevista que tuve con Leisha Lee.",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "Quizás la recuerdes del último video, hizo su trabajo de doctorado en aprendizaje profundo.",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "En este pequeño fragmento, habla de dos artículos recientes que realmente profundizan en cómo algunas de las redes de reconocimiento de imágenes más modernas están realmente aprendiendo.",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "Sólo para establecer dónde estábamos en la conversación, el primer artículo tomó una de estas redes neuronales particularmente profundas que es realmente buena en el reconocimiento de imágenes y, en lugar de entrenarla en un conjunto de datos correctamente etiquetado, barajó todas las etiquetas antes del entrenamiento.",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "Obviamente, la precisión de las pruebas aquí no fue mejor que la aleatoria, ya que todo está etiquetado simplemente al azar, pero aun así fue capaz de lograr la misma precisión de entrenamiento que lo haría en un conjunto de datos correctamente etiquetado.",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "Básicamente, los millones de pesos para esta red en particular fueron suficientes para memorizar los datos aleatorios, lo que plantea la pregunta de si minimizar esta función de costo realmente corresponde a algún tipo de estructura en la imagen, o es solo memorización.",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "Si observas esa curva de precisión, si solo estuvieras entrenando en un conjunto de datos aleatorio, esa curva descendió muy lentamente de manera casi lineal, por lo que realmente estás luchando por encontrar esos mínimos locales posibles, ya sabes. , los pesos correctos que le brindarían esa precisión.",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "Mientras que si en realidad estás entrenando en un conjunto de datos estructurado, uno que tiene las etiquetas correctas, jugueteas un poco al principio, pero luego bajas muy rápido para llegar a ese nivel de precisión, por lo que en cierto sentido Fue más fácil encontrar esos máximos locales.",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "Y lo que también fue interesante es que saca a la luz otro artículo de hace un par de años, que tiene muchas más simplificaciones sobre las capas de red, pero uno de los resultados decía que si nos fijamos en el panorama de optimización, los mínimos locales que estas redes tienden a aprender son en realidad de igual calidad, por lo que, en cierto sentido, si su conjunto de datos está estructurado, debería poder encontrarlo mucho más fácilmente.",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "Mi agradecimiento, como siempre, a aquellos que apoyan en Patreon.",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "He dicho antes lo revolucionario que es Patreon, pero estos videos realmente no serían posibles sin ti.",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "También quiero agradecer especialmente a la firma de capital riesgo Amplify Partners, por su apoyo a estos videos iniciales de la serie.",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]