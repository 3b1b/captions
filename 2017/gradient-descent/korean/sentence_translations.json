[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "",
  "from_community_srt": "지난 영상에서는 신경망의 구조에 대해 살펴봤습니다 지난 영상을 다 잊어버렸을게 분명하니 간단하게 복습하도록 하죠 이 영상엔 두 가지 목표가 있습니다.",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "",
  "from_community_srt": "첫 번째는 '경사 하강법'에 대해 소개하는 것입니다. 이것은 신경망이 어떻게 학습하는지 뿐만 아니라 다른 많은 기계학습이 어떻게 작동하는지도 포함합니다.",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "",
  "from_community_srt": "그러고 나서 우리는 이 특별한 신경망이 어떻게 작동하는지와 숨겨진 층의 뉴런들이 결국 무엇을 의미하는지도 파고들어 볼 겁니다 다시 상기시키자면 우리의 목표는 손글씨로 적은 숫자를 인식시키는 것입니다",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "",
  "from_community_srt": "신경망의 'hello world'에 해당하죠 이 숫자들은 28x28 픽셀로 이루어져 있으며 각 픽셀은 0부터 1 사이의 밝기를 갖습니다 이것들은 입력 층에 있는 784개 뉴런의 활성치를 결정합니다",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "",
  "from_community_srt": "다음 층에 있는 뉴런 각각의 활성치는 가중치와 이전 층의 활성치를 곱한 것들의 총합과 bias라고 하는 특별한 숫자 합에 의해 결정됩니다 그리고 그 합계에 지난 영상에서 보여드린 시그모이드 함수나",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "",
  "from_community_srt": "ReLU 함수를 취합니다 결론적으로 임의로 설정한 각각 16개의 뉴런들로 구성된 두 개의 숨겨진 층은 13,000여개의 조정 가능한 가중치와 bias들을 가지고 있으며",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "",
  "from_community_srt": "이 값들은 신경망이 실제로 어떻게 작동할지 결정합니다 우리는 신경망이 주어진 숫자를 분류할 때 마지막 층에서 가장 밝은 열 개의 뉴런 중 하나를 숫자와 대응시킵니다.",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "",
  "from_community_srt": "그리고 우리가 층 구조의 의의를 기억한다면 아마도 두 번째 층은 숫자의 테두리를 찾아내며 세 번째 층은 아마 고리와 직선 패턴을 찾아낼 것입니다 그리고 마지막은 저런 패턴을 조합하여 숫자를 인식합니다",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "",
  "from_community_srt": "여기까지 우리는 신경망이 어떻게 학습하는지 배워봤습니다 우리가 원하는 것은 이 신경망에 학습 데이터 전체를 집어넣는 알고리즘입니다 데이터엔 손으로 쓰여진 수많은 다른 숫자들과, 그 숫자들이 원래 무엇이었는지를 알려주는 라벨이 포함되어 있고 신경망은 학습 데이터를 통해 13,000개의 가중치 및 bias를 조정함으로써 성능이 개선될겁니다.",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "",
  "from_community_srt": "바라건대, 이 층 구조는 그 훈련 데이터를 넘어서서 일반적인 실제 이미지도 인식할 것입니다.",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "",
  "from_community_srt": "우리가 테스트하는 방식은 신경망을 훈련시킨 후 이전에 보여주지 않았던 데이터를 더 많이 보여줍니다. 그러면 당신은 신경망이 얼마나 새로운 이미지를 잘 분류하는지 볼 수 있습니다.",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "",
  "from_community_srt": "다행스럽게도 MNIST 소속 사람들이 손으로 쓴 수십만개의 숫자 이미지를 모아서 각각 하나의 숫자로 표시(라벨링)해주었기 때문에 우리는 이 자료를 사용할 수 있습니다.",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "",
  "from_community_srt": "신경망이 실제로 어떻게 작동하는지를 알게 되면 \"기계가 학습한다\"는 말이 이상하다는걸 알게 될겁니다. 기계학습은 SF소설의 이상한 설정보단 사실 미적분 예제에 더 가깝습니다.",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "",
  "from_community_srt": "제 말은, 기계학습은 근본적으로 특정한 함수의 최솟값을 찾는 일로 요약될 수 있습니다.",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "",
  "from_community_srt": "기억하세요. 개념상 우리는 각 뉴런이 이전 층의 모든 뉴런과 연결되어 있고, 각각의 활성화를 결정하는 가중 합계의 가중치는 그 연결의 세기 같은 거라고 생각합시다. 그리고 bias는 뉴런이 활동적인지 또는 비활동적인지를 나타내는 지표입니다.",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "",
  "from_community_srt": "자, 우린 이제 가중치와 bias를 완전히 무작위로 설정할 것입니다.",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "",
  "from_community_srt": "끔찍하게 작동할거라는 건 말할 필요도 없겠네요 무작위로 무언가를 하고 있으니 말이죠 예를 들어 3이라는 이미지를 입력하면 출력 층은 엉망으로 보입니다.",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "",
  "from_community_srt": "그래서 당신이 할 일은 'cost(비용) 함수'를 컴퓨터에게 정의해주는 것입니다. 이렇게 말이죠 \"그게 아니야! 바보같은 컴퓨터!\" 제대로 된 출력은 대부분의 뉴런이 0의 활성치를 가져야 합니다. 이 하나의 뉴런만 빼고요.",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "",
  "from_community_srt": "넌 나에게 쓰레기를 줬어 여러분이 할 일을 조금 더 수학적으로 말하자면, 잘못된 출력과 원하는 출력의 차의 제곱을 모두 더하는 것입니다. 이것이 우리가 하나의 훈련 예제에서 cost(비용)라고 부르는 것입니다.",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "",
  "from_community_srt": "신경망이 이미지를 올바르게 분류 할 때 이 합계가 작음을 기억하세요. 하지만 신경망이 자기가 뭘 하고 있는지 모를 때는 커집니다.",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "",
  "from_community_srt": "그렇다면 당신이 할 일은, 수만 가지의 학습 예시 전체에 대한 평균 cost를 검토하는 것입니다.",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "",
  "from_community_srt": "이 평균 cost는 신경망이 얼마나 엉망인지를 측정하는 수단이고, 컴퓨터가 스스로 뭔가 잘못됐다는걸 느끼게 해주는",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "",
  "from_community_srt": "복잡한 함수입니다.",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "",
  "from_community_srt": "신경망 자체가 기본적으로 함수임을 기억하세요. 784개의 픽셀 값을 입력받아서 10개의 숫자를 출력하는 함수요. 이것은 이 모든 가중치와 bias에 의해 매개변수화 되어있습니다.",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "",
  "from_community_srt": "cost 함수는 문제를 한 층 더 복잡하게 만드는데, 13,000여개의 가중치와 bias를 입력받아서 이 가중치와 bias가 적절한지 또는 그렇지 않은지를 나타내는 단 하나의 숫자를 출력하고, 이것은 수만 개의 훈련 데이터에 대한 신경망의 행동에 따라 결정됩니다.",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "",
  "from_community_srt": "생각해볼 만한 점이 많습니다.",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "",
  "from_community_srt": "그러나 단지 컴퓨터에 어떤 진절머리 나는 직업이 있다고 말하는 것은 별로 도움이되지 않습니다.",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "",
  "from_community_srt": "그보단 가중치와 bias를 어떻게 바꿔야 더 나아질지 알려주는 편이 좋겠지요.",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "",
  "from_community_srt": "쉽게 생각해 봅시다. 입력값이 13,000개나 되는 함수가 아니라 입력값 하나에 출력값 하나인 함수를 상상해보세요.",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "",
  "from_community_srt": "이 함수의 출력값을 최소화하는 입력값을 어떻게 찾을 수 있을까요? 미적분을 배운 학생들은 때로는 그 최솟값을 정확히 알아낼 수 있음을 알고 있을겁니다. 하지만 아주 복잡한 함수에선 항상 최솟값을 알아 낼 순 없죠. 입력값을 13,000개나 가진 신경망 cost 함수는 확실히 못 알아내겠네요.",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "",
  "from_community_srt": "좀 더 유연한 전략은, 한 입력값이 주어졌을 때, 어떤 방향으로 이동해야 출력값을 낮출 수 있을지를 파악하는 겁니다.",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "",
  "from_community_srt": "특히 지금 위치에서 함수의 기울기를 파악할 수 있다면 기울기가 양수면 왼쪽으로 이동하고 기울기가 음수면 오른쪽으로 이동하면 됩니다.",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "",
  "from_community_srt": "계속 기울기를 확인하며 적절한 방향으로 이동한다면 당신은 함수의 지역 최소값(local minimum)에 접근할 것입니다.",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "",
  "from_community_srt": "언덕에서 굴러떨어지는 공을 떠올려보면 됩니다.",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "",
  "from_community_srt": "이 단순화 된 단일 입력 함수에 대해서도 도착할 수 있는 많은 골짜기가 있습니다. 출발지점을 어디로 했느냐에 따라서 각기 다른 골짜기에 도착하게 되지만, 당신이 도착한 골짜기가 이 cost 함수에서 가장 작은 출력값이라는 보장은 없습니다.",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "",
  "from_community_srt": "이건 우리의 신경망에도 똑같이 적용될 겁니다.",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "",
  "from_community_srt": "또 여러분에게 알려드릴 게 있습니다. 만약 당신이 한번에 이동할 거리를 기울기에 비례해서 결정한다면 기울기가 줄어들수록 한번에 이동하는 거리가 점점 작아지고 이는 오버 슈팅을 방지하는 데 도움이됩니다.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "",
  "from_community_srt": "조금 더 복잡하게, 이번엔 두 개의 입력값에 하나의 출력값을 갖는 함수를 상상해 보세요.",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "",
  "from_community_srt": "입력 공간을 xy 평면으로 생각할 수 있으며 cost 함수 그래프는 그 평면 위에 떠있는 곡면으로 나타납니다.",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "",
  "from_community_srt": "이제 함수의 기울기 대신에 이 입력 공간에서 어느 방향으로 움직일지를 결정해야 합니다. 함수의 출력을 가장 빨리 줄일 수 있는 방향이요.",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "",
  "from_community_srt": "쉽게 말해 내리막은 어떤 방향일까요? 다시 언덕을 굴러 내려가는 공을 떠올리는게 도움이 됩니다.",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "",
  "from_community_srt": "다변수 미적분에 익숙한 사람들은 함수의 그래디언트가 가장 가파른 상승 방향을 알려줌을 압니다. 근본적으로 함숫값을 가장 빠르게 늘려줄 방향이 어디인지를 알려줍니다.",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "",
  "from_community_srt": "그렇다면 자연스럽게 그래디언트의 음의 방향이 가장 빠르게 함숫값을 낮추는 방향임을 알 수 있습니다.",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "",
  "from_community_srt": "게다가 이 그래디언트 벡터의 길이는 가장 가파른 경사가 얼마나 가파른지에 대한 지표이기도 합니다.",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "",
  "from_community_srt": "당신이 다변수 미적분학에 미숙해서 좀 더 배우고 싶다면, 제가 Khan Academy를 위해 만든 영상 몇가지를 시청해보세요.",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "",
  "from_community_srt": "솔직히 뭐가 어찌됐든, 지금 당신과 나에게 중요한 것은 원론적으로 이 벡터를 컴퓨터로 계산하는 방법이 존재하는가 입니다. 이 벡터는 내리막이 어떤 방향이고 얼마나 가파른지를 알려줄겁니다.",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "",
  "from_community_srt": "이외의 세부적인 내용에 대해서는 자세히 몰라도 괜찮습니다.",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "",
  "from_community_srt": "왜냐면 그래디언트를 계산해낼 줄 만 안다면 알고리즘이 내리막 방향과 한 번에 이동할 거리를 알아낼거고 이 과정을 계속 반복하기만 하면 함숫값을 줄일 수 있을테니까요.",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "",
  "from_community_srt": "이건 13,000개의 입력을 가진 함수까지 확장해서 적용할 수 있는 기본적인 아이디어입니다.",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "",
  "from_community_srt": "우리 신경망의 13,000여개의 가중치와 bias를 거대한 열 벡터로 조직했다고 생각해보세요.",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "",
  "from_community_srt": "cost 함수의 그래디언트의 음의 방향은 그냥 벡터일 뿐입니다. 이 벡터는 이 광활한 입력 공간에서 어떤 방향이 cost 함수를 가장 빠르게 감소시켜 주는지를 알려줍니다.",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "",
  "from_community_srt": "물론 우리의 특별 제작된 cost 함수는 가중치와 bias들을 cost 함수가 줄어드는 방향으로 조정할 것이고 그 뜻은 우리의 신경망이 훈련 데이터를 근거로 해서 무작위로 10개의 숫자를 출력하지 않고, 우리가 원하는 정확한 결과를 출력해준다는 뜻입니다.",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "",
  "from_community_srt": "기억하세요. 이 cost 함수는 모든 훈련 데이터의 평균 cost를 수반합니다. 따라서 cost 함수를 최소화한다는 것은 이 모든 샘플들에서 더 나은 성능을 보여준다는 것입니다.",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "",
  "from_community_srt": "이 그래디언트 계산을 효과적으로 만드는 알고리즘이 신경망이 얼마나 효과적으로 배울 수 있을 지를 결정하는 핵심적인 요소입니다. 그리고 그 알고리즘을 \"back propagation\"(오차역전파법)이라고 부릅니다. 이게 바로 다음 영상에서 다룰 내용이고요.",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "",
  "from_community_srt": "이제 많은 시간을 들여서 설명 드리고 싶은 내용들입니다. 주어진 훈련 데이터마다 각각의 가중치와 bias들에는 정확히 어떤 일이 일어나는 걸까요? 최대한 직관적으로 체험시켜드리기 위해, 관련된 미적분이나 수식들은 제껴놓겠습니다.",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "",
  "from_community_srt": "지금 가장 중요한 것은, \"구현 세부사항의 독립성\"(Independent of Implementation Details)이 무엇인지를 알아야 한다는 겁니다. 이게 무슨 뜻이냐면, 신경망 학습은 그저 cost 함수를 최소화하는 것 뿐이라는 겁니다.",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "",
  "from_community_srt": "그로부터 우리는 cost 함수가 매끄러운 출력을 갖는 것이 중요하다는 결론을 얻을 수 있습니다. 그래야지 우리가 내리막을 한발짝씩 내려가면서 지역 최솟값을 찾을 수 있을테니까요.",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "",
  "from_community_srt": "이런 이유로 인공 뉴런들은 단순히 활성화/비활성화로 결정되지 않고 연속적인 활성화 값을 갖습니다. 실제 생물의 뉴런처럼 말이죠.",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "",
  "from_community_srt": "이렇게 반복적으로 그래디언트의 음의 방향으로 함수의 입력값을 이동하는 방식을 \"경사 하강법\"(Gradient Descent)이라고 부릅니다.",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "",
  "from_community_srt": "지금 보이는 그래프의 골짜기, 즉, cost 함수의 지역 최소값으로  수렴하는 방법입니다.",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "",
  "from_community_srt": "저는 아직 두 개의 입력을 가진 함수만 보여드렸습니다. 왜냐면, 13,002차원 입력 공간에서 굴러다니는 공을 이해시켜드리긴 좀 어려우니까요. 하지만 이 상황을 비(非)공간적으로 생각할 수 있는 좋은 방법이 있습니다.",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "",
  "from_community_srt": "음의 그래디언트의 각각의 요소들은 우리에게 두 가지를 알려줍니다.",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "",
  "from_community_srt": "그래디언트의 부호는 상응하는 입력 벡터의 요소가 커져야 할지 작아져야 할지를 알려줍니다.",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "",
  "from_community_srt": "하지만 더 중요한 것은 이 요소들의 상대적인 크기입니다. 이 크기는 어떤 요소를 조정하는 것이 더 큰 영향을 미칠지를 알려줍니다.",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "",
  "from_community_srt": "어떤 한 가중치를 조정하는 것이 다른 가중치를 조정하는 것보다 cost 함수에 더 큰 영향을 미칠 수 있습니다.",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "",
  "from_community_srt": "이런 몇몇 연결선들은 훈련 데이터에 더 민감하게 반응합니다.",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "",
  "from_community_srt": "따라서 이 무지막지한 cost 함수의 그래디언트 벡터를 각각의 가중치와 bias의 중요도를 표현하는 것이라고 생각할 수 있습니다. 그러니까, 이것들 중에 몇몇은 조정하면 대박 치는 것들이라는 겁니다.",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "",
  "from_community_srt": "이것은 방향이라는 개념을 다르게 이해해보는 방법입니다.",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "",
  "from_community_srt": "쉬운 예시로는, 입력 변수가 두 개인 함수가 특정한 점에서의 그래디언트가 (3,1)임을 계산해냈다는 것은, 그 점에서 함숫값을 가장 빠르게 증가시키는 방향이 어디인지도 안다는 말입니다. 그 점에서 함숫값을 가장 빠르게 증가시키는 방향이 어디인지도 안다는 말입니다. 입력 평면 공간 위의 곡면에서는 가장 가파른 오르막이 그 방향인지 안다는 말이구요.",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "",
  "from_community_srt": "그 방향이라는 개념을 다르게 이해하는 방법은 첫 번째 변수를 조정하는 것이 두 번째 변수를 조정하는 것보다 3배 중요하다고 이해하는 것입니다. 그 점 근처에선 말이죠.",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "",
  "from_community_srt": "x값을 조정하는게 대박 치는 방법이겠죠 좋아요. 이제 우리가 어디까지 왔는지 잠깐 돌아봅시다.",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "",
  "from_community_srt": "신경망은 784개의 입력을 받아서 가중치와 bias를 통해 10개의 출력을 내놓는 함수입니다.",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "",
  "from_community_srt": "복잡함을 더하는 cost 함수는 13,000여개의 가중치와 bias를 입력값으로 받아서 신경망이 얼마나 엉망으로 작동하고 있는지를 나타내는 단 하나의 숫자를 내놓습니다.",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "",
  "from_community_srt": "또 어려운 부분인 cost 함수의 그래디언트는 이 가중치와 bias들을 어떻게 조정하는 것이 cost 함숫값을 가장 빠르게 줄여주는지 알려줍니다. 다른 표현으론, 어떤 가중치를 조정하는 것이 더 큰 영향을 끼치느냐,",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "",
  "from_community_srt": "라는 것이죠. 그렇다면, 당신이 처음 신경망을 무작위 가중치와 bias로 만들고 경사 하강법으로 그것들을 여러번에 걸쳐 조정한다면 이 신경망이 한 번도 보지 못한 이미지에 대해 실제로 얼마나 잘 작동할까요?",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "",
  "from_community_srt": "앞서 16개의 뉴런으로 된 두 개의 숨겨진 층을 쓰기로 사용한건 그냥 보기 좋아서 그랬다고 설명했었죠. 하지만, 썩 괜찮네요! 새로운 이미지를 정확도 96%로 구별해내고 있어요.",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "",
  "from_community_srt": "사실 신경망이 구별 못 한 이미지들을 보면 솔직히 이정도 틀리는건 봐줄만 하다는 생각이 들겁니다.",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "",
  "from_community_srt": "당신이 숨겨진 층을 좀 가지고 놀다보면 정확도를 98%까지 끌어올릴 수 있습니다.",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "",
  "from_community_srt": "아주 좋아요! 최고는 아니지만. 이 평범한 신경망보다 더 정교하고 복잡한 신경망을 쓴다면 더 좋은 정확도를 얻을 수 있습니다. 하지만 초기 작업이 아주 막막했던걸 감안하면 어떤 신경망이든 전혀 보지 못했던 이미지에 대해서 이렇게 잘 작동한다는건 아주 놀랍다고 생각해요. 어떤 패턴을 찾아야 하는지 구체적으로 알려주지도 않았는데 말이죠.",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "",
  "from_community_srt": "원래 제가 이 구조에서 원했던 것은 설명 드렸던 것처럼 두 번째 층이 숫자의 테두리들을 인식하고 세 번째 층은 테두리들을 합쳐서 동그라미나 직선을 인식해서 그것들을 합쳐 하나의 숫자를 인식하는 것이었습니다.",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "",
  "from_community_srt": "우리의 신경망에서 실제로 이런 과정들이 일어나고 있을까요? 음...",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "",
  "from_community_srt": "이런 경우엔 절대 아니죠.",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "",
  "from_community_srt": "이전 영상에서 본 내용을 기억하신다면 첫 번째 층의 모든 뉴런과 두 번째 층의 한 뉴런 사이의 연결은 지금 보시는 픽셀 패턴으로 시각화 될 수 있습니다. 두 번째 층의 모든 뉴런에 대해서요.",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "",
  "from_community_srt": "실제로 가중치들을 이렇게 시각화 해보면 독립된 테두리를 인식한다기 보단 가운데 희미하게 보이는 패턴을 제외하곤 아무렇게나 생긴 것처럼 보입니다.",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "",
  "from_community_srt": "우리의 신경망은 불가해하게 거대한 13,000차원 가중치와 bias 입력 공간에서 자신만의 작고 소중한 지역 최소값을 찾았고 그 덕에 우리가 원했던 방식으로 패턴을 인식하진 않지만 대부분의 이미지를 성공적으로 인식합니다.",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "",
  "from_community_srt": "이게 어떤 상황인지를 정확히 이해하기 위해, 무작위 이미지를 입력하면 어떤 일이 일어나는지 한 번 봅시다.",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "",
  "from_community_srt": "시스템이 아주 똑똑하다면, 아마 시스템이 불확실함을 느끼고 10개의 출력중 그 어떤 것도 활성화 시키지 않거나 10개 모두를 고르게 활성화 시킬 것이라고 예상하실 겁니다. 하지만 신경망은 이 무작위 이미지가 5라고 아주 자신있게 대답합니다. 실제 5의 이미지를 봤을 때 처럼요.",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "",
  "from_community_srt": "그 뜻은 신경망이 숫자 이미지를 아주 잘 인식하더라도, 숫자를 그릴줄은 모른다는 겁니다.",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "",
  "from_community_srt": "이는 너무 엄격하게 제한된 훈련 환경 때문입니다.",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "",
  "from_community_srt": "내 말은 당신이 신경망의 입장에서 생각해 본다면 세상에는 격자 안에서 움직이지 않는 명백히 숫자라고 생각되는 것밖에 없고 cost 함수는 오직 신경망이 자신의 결정에 확신을 가지게만 장려했습니다.",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "",
  "from_community_srt": "그래서, 이 이미지가 두 번째 층이 실제로 하는 일이라면 왜 제가 테두리와 패턴을 인식하는 신경망을 소개했는지 이상하다고 생각하실 겁니다.",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "",
  "from_community_srt": "제 대답은, 이게 신경망이 하는 일의 전부가 아니라는 겁니다.",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "",
  "from_community_srt": "그러니까, 이게 우리의 목표가 아니라 사실은 출발점에 불과합니다.",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "",
  "from_community_srt": "까놓고 말해서, 신경망은 오래된 기술입니다. 80~90년대에 연구됐던 기술이죠. 여러가지 흥미로운 문제의 해결을 위해 만들어진 현대의 다양하고 상세해진 신경망을 이해하기 위해 당신은 이 신경망부터 이해해야할 필요가 있습니다. 하지만 현대의 신경망의 숨겨진 층이 하는 일을 들여다 보면, '생각'이라는 과정과는 더욱 동떨어져 있다는걸 알게 될겁니다.",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "",
  "from_community_srt": "잠시 신경망이 학습하는 방법이 아니라 당신이 학습하는 방법에 대해서 생각해봅시다. 어찌됐든 당신이 이 영상을 보고 능동적으로 사유할 때  학습이 이루어집니다.",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "",
  "from_community_srt": "아주 간단한 부탁을 드리고 싶습니다. 지금 영상을 잠깐 멈추고 한 번 깊게 생각해 보세요. 당신은 이 시스템을 어떻게 바꿀 수 있을까요? 그리고 당신의 시스템이 테두리나 패턴 따위를 인식하기를 바란다면, 시스템은 이미지를 어떻게 인식하게 될까요? 하지만,",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "",
  "from_community_srt": "그보단 실제로 사용하고 있는 것들에 대해 공부하는 편이 좋을 것 같습니다. 저는 Michael Nielsen이 딥러닝과 신경망에 대해 쓴 책들을 적극 추천드립니다.",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "",
  "from_community_srt": "책에서는 영상에서 보신 예제에 대한 코드와 데이터를 제공하고 있습니다. 그리고 그 코드가 어떤 일을 하는지 차근차근 가르쳐드릴 겁니다.",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "",
  "from_community_srt": "게다가 이 책은 무료로 공개되어 있습니다. 멋지죠? 그러니 이 책으로 도움을 받았다면 Nielsen의 노고에 후원해보세요.",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "",
  "from_community_srt": "또 저는 제가 아주 좋아하는 몇 가지 다른 링크들도 달아놓았습니다. Chris Ola의 경이롭고 아름다운 포스팅이나 distill.pub의 게시물 같은 것들이요.",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "",
  "from_community_srt": "마치며 남은 몇 분 동안 지난번에 Lisha Li와 가졌던 인터뷰의 한 부분에 대해 이야기 해보고 싶습니다.",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "",
  "from_community_srt": "이전 영상에 그녀가 나왔던걸 기억하시겠죠. 그녀는 딥러닝 연구로 박사학위를 받았습니다.",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "",
  "from_community_srt": "그녀는 더 현대적인 이미지 인식 신경망이 실제로 어떻게 학습하는지 최근 깊게 분석한 두 논문에 대해 이야기해줬습니다.",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "",
  "from_community_srt": "첫 번째 논문은 이미지 인식에 아주 탁월한 심층적인 신경망을 다뤘습니다. 이 신경망은 제대로 라벨링된 데이터셋으로 훈련하는 대신 훈련 전에 모든 레이블을 섞어놓았습니다.",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "",
  "from_community_srt": "테스트 정확도는 그냥 무작위로 나올 것처럼 보입니다. 모두 무작위로 라벨링 돼있으니까요. 하지만 제대로 라벨링된 데이터셋으로 학습한 것과 동일한 테스트 정확도를 달성했습니다.",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "",
  "from_community_srt": "기본적으로 이 신경망에 있는 수백만개의 가중치들은 무작위 데이터를 기억하기에 충분합니다. 그렇다면 이런 의문점이 생깁니다. cost 함수를 최소화 한다는 것은 이미지에 나타나는 구조를 분류하는 건가요? 아니면 그냥 기억하는 것에 불과한 건가요?",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "",
  "from_community_srt": "올바른 분류가 무엇인지에 대한 데이터셋을 기억하는 거예요. 올해 ICML엔 반 년 동안 이를 반박하는 논문이 없었어요. 이렇게요. \"야! 신경망은 그것보다 더 똑똑해.\" 이 정확도 곡선을 보면 만약 당신이 무작위 데이터셋으로 훈련을 시작했다면 곡선이 거의 직선에 가깝게 아주 천천히 하강합니다. 당신이 구조화된 데이터셋,",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "",
  "from_community_srt": "그러니까 제대로 라벨링된 데이터셋으로 올바른 가중치를 찾아 높은 정확도를 얻기 위해 가능한 지역 최소값을 찾는다면 처음엔 조금 헤매다가, 어느순간 좋은 정확도 수준으로 갑자기 뚝 떨어질 겁니다. 따라서 어떤 면에선 저 지역 최대값을 찾기 더 쉽습니다.",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "",
  "from_community_srt": "그리고 또 흥미로운 사실은 몇 년 지난 다른 논문을 보면 층이 더 단순화된 신경망을 다룬 논문이요 층이 더 단순화된 신경망을 다룬 논문이요 그러나 결과는 이 최적화된 환경을 보면 신경망이 배우려고 하는 지역 최소값들은 사실 동등한 가치를 갖는다는 것을 말해줍니다. 그러므로 어떤 면에선 데이터셋이 구조화 돼있다면, 지역 최소값을 더 쉽게 찾을 수 있습니다.",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "",
  "from_community_srt": "patreon으로 후원해주시는 모든 분들께 감사드립니다.",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "",
  "from_community_srt": "patreon에 대해선 이미 말씀 드렸었지만, 이 영상들은 당신이 없었다면 만들수 없었을 겁니다.",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]