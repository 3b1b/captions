[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "지난 동영상에서 신경망의 구조를 설명했습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "기억에 남을 수 있도록 간단히 요약한 다음, 이 비디오의 두 가지 주요 목표를 말씀드리겠습니다.",
  "model": "DeepL",
  "from_community_srt": "지난 영상에서는 신경망의 구조에 대해 살펴봤습니다 지난 영상을 다 잊어버렸을게 분명하니 간단하게 복습하도록 하죠 이 영상엔 두 가지 목표가 있습니다.",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "첫 번째는 신경망의 학습 방식뿐만 아니라 다른 많은 머신러닝의 작동 방식에 기초가 되는 경사 하강이라는 개념을 소개하는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "첫 번째는 '경사 하강법'에 대해 소개하는 것입니다. 이것은 신경망이 어떻게 학습하는지 뿐만 아니라 다른 많은 기계학습이 어떻게 작동하는지도 포함합니다.",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "그런 다음 이 특정 네트워크의 작동 방식과 숨겨진 뉴런 층이 결국 무엇을 찾는지 조금 더 자세히 살펴보겠습니다.",
  "model": "DeepL",
  "from_community_srt": "그러고 나서 우리는 이 특별한 신경망이 어떻게 작동하는지와 숨겨진 층의 뉴런들이 결국 무엇을 의미하는지도 파고들어 볼 겁니다 다시 상기시키자면 우리의 목표는 손글씨로 적은 숫자를 인식시키는 것입니다",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "다시 한 번 말씀드리지만, 여기서는 손으로 쓴 숫자 인식의 대표적인 예인 신경망의 헬로 월드가 목표입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "이 숫자는 28x28 픽셀 그리드에 렌더링되며, 각 픽셀은 0과 1 사이의 회색조 값을 갖습니다.",
  "model": "DeepL",
  "from_community_srt": "신경망의 'hello world'에 해당하죠 이 숫자들은 28x28 픽셀로 이루어져 있으며 각 픽셀은 0부터 1 사이의 밝기를 갖습니다 이것들은 입력 층에 있는 784개 뉴런의 활성치를 결정합니다",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "이것이 네트워크의 입력 레이어에 있는 784개의 뉴런의 활성화를 결정하는 요소입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "그리고 다음 레이어의 각 뉴런에 대한 활성화는 이전 레이어의 모든 활성화의 가중치 합계에 바이어스라는 특수 숫자를 더한 값을 기반으로 합니다.",
  "model": "DeepL",
  "from_community_srt": "다음 층에 있는 뉴런 각각의 활성치는 가중치와 이전 층의 활성치를 곱한 것들의 총합과 bias라고 하는 특별한 숫자 합에 의해 결정됩니다 그리고 그 합계에 지난 영상에서 보여드린 시그모이드 함수나",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "그런 다음 지난 동영상에서 설명한 것처럼 시그모이드 스퀴시화 또는 릴루와 같은 다른 함수를 사용하여 합계를 구성합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "각각 16개의 뉴런이 있는 두 개의 숨겨진 레이어를 다소 임의적으로 선택하면 네트워크에는 약 13,000개의 가중치와 편향이 있으며, 이 값에 따라 네트워크가 실제로 수행하는 작업이 결정됩니다.",
  "model": "DeepL",
  "from_community_srt": "ReLU 함수를 취합니다 결론적으로 임의로 설정한 각각 16개의 뉴런들로 구성된 두 개의 숨겨진 층은 13,000여개의 조정 가능한 가중치와 bias들을 가지고 있으며",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "이 네트워크가 특정 숫자를 분류한다는 것은 최종 레이어에 있는 10개 뉴런 중 가장 밝은 뉴런이 해당 숫자에 해당한다는 뜻입니다.",
  "model": "DeepL",
  "from_community_srt": "이 값들은 신경망이 실제로 어떻게 작동할지 결정합니다 우리는 신경망이 주어진 숫자를 분류할 때 마지막 층에서 가장 밝은 열 개의 뉴런 중 하나를 숫자와 대응시킵니다.",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "여기서 레이어 구조를 염두에 둔 동기는 두 번째 레이어가 가장자리를 인식하고 세 번째 레이어가 루프나 선과 같은 패턴을 인식할 수 있으며, 마지막 레이어가 이러한 패턴을 조합하여 숫자를 인식할 수 있다는 점을 기억하세요.",
  "model": "DeepL",
  "from_community_srt": "그리고 우리가 층 구조의 의의를 기억한다면 아마도 두 번째 층은 숫자의 테두리를 찾아내며 세 번째 층은 아마 고리와 직선 패턴을 찾아낼 것입니다 그리고 마지막은 저런 패턴을 조합하여 숫자를 인식합니다",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "그래서 여기서는 네트워크가 어떻게 학습하는지 알아봅시다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "우리가 원하는 것은 이 네트워크에 손으로 쓴 숫자의 다양한 이미지와 그 숫자가 무엇인지에 대한 레이블의 형태로 제공되는 전체 학습 데이터를 보여주고, 학습 데이터에 대한 성능을 향상시키기 위해 13,000개의 가중치와 편향을 조정할 수 있는 알고리즘입니다.",
  "model": "DeepL",
  "from_community_srt": "여기까지 우리는 신경망이 어떻게 학습하는지 배워봤습니다 우리가 원하는 것은 이 신경망에 학습 데이터 전체를 집어넣는 알고리즘입니다 데이터엔 손으로 쓰여진 수많은 다른 숫자들과, 그 숫자들이 원래 무엇이었는지를 알려주는 라벨이 포함되어 있고 신경망은 학습 데이터를 통해 13,000개의 가중치 및 bias를 조정함으로써 성능이 개선될겁니다.",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "이러한 계층화된 구조를 통해 학습한 내용을 해당 학습 데이터 이외의 이미지에 일반화할 수 있기를 바랍니다.",
  "model": "DeepL",
  "from_community_srt": "바라건대, 이 층 구조는 그 훈련 데이터를 넘어서서 일반적인 실제 이미지도 인식할 것입니다.",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "이를 테스트하는 방법은 네트워크를 학습시킨 후 이전에 본 적이 없는 레이블이 지정된 데이터를 더 많이 보여주고 새로운 이미지가 얼마나 정확하게 분류되는지 확인하는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "우리가 테스트하는 방식은 신경망을 훈련시킨 후 이전에 보여주지 않았던 데이터를 더 많이 보여줍니다. 그러면 당신은 신경망이 얼마나 새로운 이미지를 잘 분류하는지 볼 수 있습니다.",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "다행히도, 그리고 이러한 일반적인 예로 시작하기 좋은 이유는 MNIST 데이터베이스의 훌륭한 사람들이 수만 개의 손으로 쓴 숫자 이미지를 모아 각 이미지에 해당 번호가 표시된 라벨을 붙였기 때문입니다.",
  "model": "DeepL",
  "from_community_srt": "다행스럽게도 MNIST 소속 사람들이 손으로 쓴 수십만개의 숫자 이미지를 모아서 각각 하나의 숫자로 표시(라벨링)해주었기 때문에 우리는 이 자료를 사용할 수 있습니다.",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "머신을 학습이라고 설명하는 것은 도발적이지만, 머신이 어떻게 작동하는지 보면 공상 과학의 전제라기보다는 미적분학 연습에 가깝게 느껴집니다.",
  "model": "DeepL",
  "from_community_srt": "신경망이 실제로 어떻게 작동하는지를 알게 되면 \"기계가 학습한다\"는 말이 이상하다는걸 알게 될겁니다. 기계학습은 SF소설의 이상한 설정보단 사실 미적분 예제에 더 가깝습니다.",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "기본적으로 특정 함수의 최소값을 찾는 것이 핵심입니다.",
  "model": "DeepL",
  "from_community_srt": "제 말은, 기계학습은 근본적으로 특정한 함수의 최솟값을 찾는 일로 요약될 수 있습니다.",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "개념적으로 각 뉴런은 이전 계층의 모든 뉴런에 연결되어 있다고 생각하며, 활성화를 정의하는 가중치 합계의 가중치는 이러한 연결의 강도와 같고 편향은 해당 뉴런이 활성화 또는 비활성화되는 경향을 나타내는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "기억하세요. 개념상 우리는 각 뉴런이 이전 층의 모든 뉴런과 연결되어 있고, 각각의 활성화를 결정하는 가중 합계의 가중치는 그 연결의 세기 같은 거라고 생각합시다. 그리고 bias는 뉴런이 활동적인지 또는 비활동적인지를 나타내는 지표입니다.",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "우선 모든 가중치와 편향성을 완전히 무작위로 초기화하겠습니다.",
  "model": "DeepL",
  "from_community_srt": "자, 우린 이제 가중치와 bias를 완전히 무작위로 설정할 것입니다.",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "말할 필요도 없이, 이 네트워크는 무작위적인 작업을 수행하기 때문에 주어진 훈련 예제에서 꽤 끔찍한 성능을 발휘할 것입니다.",
  "model": "DeepL",
  "from_community_srt": "끔찍하게 작동할거라는 건 말할 필요도 없겠네요 무작위로 무언가를 하고 있으니 말이죠 예를 들어 3이라는 이미지를 입력하면 출력 층은 엉망으로 보입니다.",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "예를 들어, 이 3 이미지를 입력하면 출력 레이어가 엉망진창처럼 보입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "따라서 여러분이 하는 일은 컴퓨터에게 '아니, 나쁜 컴퓨터, 대부분의 뉴런은 활성화가 0이지만 이 뉴런은 1이어야 한다'고 말하는 방법, 즉 비용 함수를 정의하는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "그래서 당신이 할 일은 'cost(비용) 함수'를 컴퓨터에게 정의해주는 것입니다. 이렇게 말이죠 \"그게 아니야! 바보같은 컴퓨터!\" 제대로 된 출력은 대부분의 뉴런이 0의 활성치를 가져야 합니다. 이 하나의 뉴런만 빼고요.",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "조금 더 수학적으로 말하자면, 각 쓰레기 출력 활성화와 원하는 값 사이의 차이의 제곱을 더하면 되며, 이를 단일 훈련 예제의 비용이라고 부를 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "넌 나에게 쓰레기를 줬어 여러분이 할 일을 조금 더 수학적으로 말하자면, 잘못된 출력과 원하는 출력의 차의 제곱을 모두 더하는 것입니다. 이것이 우리가 하나의 훈련 예제에서 cost(비용)라고 부르는 것입니다.",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "네트워크가 이미지를 정확하게 분류한다고 확신할 때는 이 합이 작지만, 네트워크가 무엇을 하는지 모르는 것처럼 보일 때는 이 합이 큽니다.",
  "model": "DeepL",
  "from_community_srt": "신경망이 이미지를 올바르게 분류 할 때 이 합계가 작음을 기억하세요. 하지만 신경망이 자기가 뭘 하고 있는지 모를 때는 커집니다.",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "따라서 수만 개의 교육 예시 모두에 대한 평균 비용을 고려해야 합니다.",
  "model": "DeepL",
  "from_community_srt": "그렇다면 당신이 할 일은, 수만 가지의 학습 예시 전체에 대한 평균 cost를 검토하는 것입니다.",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "이 평균 비용은 네트워크가 얼마나 형편없는지, 컴퓨터의 상태가 얼마나 나쁠지를 가늠하는 척도입니다.",
  "model": "DeepL",
  "from_community_srt": "이 평균 cost는 신경망이 얼마나 엉망인지를 측정하는 수단이고, 컴퓨터가 스스로 뭔가 잘못됐다는걸 느끼게 해주는",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "그리고 그것은 복잡한 문제입니다.",
  "model": "DeepL",
  "from_community_srt": "복잡한 함수입니다.",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "네트워크 자체가 기본적으로 784개의 숫자, 즉 픽셀 값을 입력으로 받아 10개의 숫자를 출력으로 뱉어내는 함수이며, 어떤 의미에서는 이 모든 가중치와 편향에 의해 매개변수화되어 있다는 점을 기억하시나요?",
  "model": "DeepL",
  "from_community_srt": "신경망 자체가 기본적으로 함수임을 기억하세요. 784개의 픽셀 값을 입력받아서 10개의 숫자를 출력하는 함수요. 이것은 이 모든 가중치와 bias에 의해 매개변수화 되어있습니다.",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "비용 함수는 그 위에 복잡성을 더합니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "13,000개 정도의 가중치와 편향을 입력으로 받아 그 가중치와 편향이 얼마나 나쁜지를 설명하는 하나의 숫자를 뱉어내는데, 그 정의 방식은 수만 개의 학습 데이터에 대한 네트워크의 행동에 따라 달라집니다.",
  "model": "DeepL",
  "from_community_srt": "cost 함수는 문제를 한 층 더 복잡하게 만드는데, 13,000여개의 가중치와 bias를 입력받아서 이 가중치와 bias가 적절한지 또는 그렇지 않은지를 나타내는 단 하나의 숫자를 출력하고, 이것은 수만 개의 훈련 데이터에 대한 신경망의 행동에 따라 결정됩니다.",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "생각해야 할 것이 많습니다.",
  "model": "DeepL",
  "from_community_srt": "생각해볼 만한 점이 많습니다.",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "하지만 컴퓨터가 얼마나 형편없는 일을 하고 있는지 알려주는 것만으로는 큰 도움이 되지 않습니다.",
  "model": "DeepL",
  "from_community_srt": "그러나 단지 컴퓨터에 어떤 진절머리 나는 직업이 있다고 말하는 것은 별로 도움이되지 않습니다.",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "이러한 가중치와 편향성을 어떻게 바꾸면 더 나아질 수 있는지 알려주고 싶을 것입니다.",
  "model": "DeepL",
  "from_community_srt": "그보단 가중치와 bias를 어떻게 바꿔야 더 나아질지 알려주는 편이 좋겠지요.",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "13,000개의 입력이 있는 함수를 어렵게 상상하기보다는 하나의 숫자를 입력으로 하고 하나의 숫자를 출력으로 하는 간단한 함수를 상상해 보세요.",
  "model": "DeepL",
  "from_community_srt": "쉽게 생각해 봅시다. 입력값이 13,000개나 되는 함수가 아니라 입력값 하나에 출력값 하나인 함수를 상상해보세요.",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "이 함수의 값을 최소화하는 입력을 어떻게 찾을 수 있을까요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "미적분학 학생이라면 최소값을 명시적으로 알아낼 수 있다는 것을 알겠지만, 정말 복잡한 함수에서는 그것이 항상 가능한 것은 아니며, 특히 이 상황과 같이 입력값이 13,000개에 달하는 복잡한 신경망 비용 함수에서는 더욱 그러합니다.",
  "model": "DeepL",
  "from_community_srt": "이 함수의 출력값을 최소화하는 입력값을 어떻게 찾을 수 있을까요? 미적분을 배운 학생들은 때로는 그 최솟값을 정확히 알아낼 수 있음을 알고 있을겁니다. 하지만 아주 복잡한 함수에선 항상 최솟값을 알아 낼 순 없죠. 입력값을 13,000개나 가진 신경망 cost 함수는 확실히 못 알아내겠네요.",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "보다 유연한 전략은 어떤 입력값에서 시작하여 어느 방향으로 나아가야 출력을 낮출 수 있는지 파악하는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "좀 더 유연한 전략은, 한 입력값이 주어졌을 때, 어떤 방향으로 이동해야 출력값을 낮출 수 있을지를 파악하는 겁니다.",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "구체적으로, 현재 위치에서 함수의 기울기를 파악할 수 있다면 그 기울기가 양수이면 입력을 왼쪽으로 이동하고, 음수이면 입력을 오른쪽으로 이동합니다.",
  "model": "DeepL",
  "from_community_srt": "특히 지금 위치에서 함수의 기울기를 파악할 수 있다면 기울기가 양수면 왼쪽으로 이동하고 기울기가 음수면 오른쪽으로 이동하면 됩니다.",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "이 작업을 반복하여 각 지점에서 새로운 기울기를 확인하고 적절한 단계를 수행하면 함수의 국부적 최소값에 접근하게 됩니다.",
  "model": "DeepL",
  "from_community_srt": "계속 기울기를 확인하며 적절한 방향으로 이동한다면 당신은 함수의 지역 최소값(local minimum)에 접근할 것입니다.",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "여기서 떠올릴 수 있는 이미지는 언덕을 굴러 내려가는 공입니다.",
  "model": "DeepL",
  "from_community_srt": "언덕에서 굴러떨어지는 공을 떠올려보면 됩니다.",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "이 매우 단순한 단일 입력 함수의 경우에도 어떤 임의의 입력에서 시작하느냐에 따라 다양한 계곡에 도달할 수 있으며, 도달하는 지역 최소값이 비용 함수의 가능한 가장 작은 값이 될 것이라는 보장은 없습니다.",
  "model": "DeepL",
  "from_community_srt": "이 단순화 된 단일 입력 함수에 대해서도 도착할 수 있는 많은 골짜기가 있습니다. 출발지점을 어디로 했느냐에 따라서 각기 다른 골짜기에 도착하게 되지만, 당신이 도착한 골짜기가 이 cost 함수에서 가장 작은 출력값이라는 보장은 없습니다.",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "이는 신경망 사례에도 적용됩니다.",
  "model": "DeepL",
  "from_community_srt": "이건 우리의 신경망에도 똑같이 적용될 겁니다.",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "또한 스텝 크기를 경사에 비례하게 만들면 경사가 최소가 될수록 스텝이 점점 작아져 오버슈팅을 방지하는 데 도움이 된다는 점도 알아두세요.",
  "model": "DeepL",
  "from_community_srt": "또 여러분에게 알려드릴 게 있습니다. 만약 당신이 한번에 이동할 거리를 기울기에 비례해서 결정한다면 기울기가 줄어들수록 한번에 이동하는 거리가 점점 작아지고 이는 오버 슈팅을 방지하는 데 도움이됩니다.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "복잡성을 조금 더 높여서 두 개의 입력과 하나의 출력이 있는 함수를 상상해 보세요.",
  "model": "DeepL",
  "from_community_srt": "조금 더 복잡하게, 이번엔 두 개의 입력값에 하나의 출력값을 갖는 함수를 상상해 보세요.",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "입력 공간을 xy-평면으로, 비용 함수는 그 위에 그래프로 표시되는 표면으로 생각할 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "입력 공간을 xy 평면으로 생각할 수 있으며 cost 함수 그래프는 그 평면 위에 떠있는 곡면으로 나타납니다.",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "함수의 기울기를 묻는 대신, 함수의 출력을 가장 빨리 줄이려면 이 입력 공간에서 어느 방향으로 스텝을 밟아야 하는지 물어봐야 합니다.",
  "model": "DeepL",
  "from_community_srt": "이제 함수의 기울기 대신에 이 입력 공간에서 어느 방향으로 움직일지를 결정해야 합니다. 함수의 출력을 가장 빨리 줄일 수 있는 방향이요.",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "다시 말해, 내리막길의 방향은 무엇인가요?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "다시 말하지만, 언덕을 굴러 내려가는 공을 생각하면 도움이 됩니다.",
  "model": "DeepL",
  "from_community_srt": "쉽게 말해 내리막은 어떤 방향일까요? 다시 언덕을 굴러 내려가는 공을 떠올리는게 도움이 됩니다.",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "다변수 미적분에 익숙하신 분들은 함수의 기울기가 가장 가파른 상승 방향을 알려주며, 함수를 가장 빠르게 증가시키려면 어느 방향으로 발걸음을 옮겨야 하는지 알고 계실 것입니다.",
  "model": "DeepL",
  "from_community_srt": "다변수 미적분에 익숙한 사람들은 함수의 그래디언트가 가장 가파른 상승 방향을 알려줌을 압니다. 근본적으로 함숫값을 가장 빠르게 늘려줄 방향이 어디인지를 알려줍니다.",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "당연히 그 기울기의 음수를 취하면 함수를 가장 빠르게 감소시키는 단계의 방향을 알 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "그렇다면 자연스럽게 그래디언트의 음의 방향이 가장 빠르게 함숫값을 낮추는 방향임을 알 수 있습니다.",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "이 그라데이션 벡터의 길이를 통해 가장 가파른 경사가 얼마나 가파른지 알 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "게다가 이 그래디언트 벡터의 길이는 가장 가파른 경사가 얼마나 가파른지에 대한 지표이기도 합니다.",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "다변수 미적분학이 익숙하지 않고 더 자세히 알고 싶다면 제가 칸 아카데미에서 이 주제에 대해 강의한 내용을 확인해 보세요.",
  "model": "DeepL",
  "from_community_srt": "당신이 다변수 미적분학에 미숙해서 좀 더 배우고 싶다면, 제가 Khan Academy를 위해 만든 영상 몇가지를 시청해보세요.",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "하지만 솔직히 지금 여러분과 저에게 중요한 것은 원칙적으로 내리막길의 방향과 경사를 알려주는 이 벡터를 계산하는 방법이 존재한다는 사실입니다.",
  "model": "DeepL",
  "from_community_srt": "솔직히 뭐가 어찌됐든, 지금 당신과 나에게 중요한 것은 원론적으로 이 벡터를 컴퓨터로 계산하는 방법이 존재하는가 입니다. 이 벡터는 내리막이 어떤 방향이고 얼마나 가파른지를 알려줄겁니다.",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "이 정도만 알고 있고 세부 사항에 대해 잘 모르더라도 괜찮을 것입니다.",
  "model": "DeepL",
  "from_community_srt": "이외의 세부적인 내용에 대해서는 자세히 몰라도 괜찮습니다.",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "이 함수를 최소화하는 알고리즘은 이 경사 방향을 계산한 다음 내리막길에서 작은 발걸음을 내딛고 이를 계속 반복하는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "왜냐면 그래디언트를 계산해낼 줄 만 안다면 알고리즘이 내리막 방향과 한 번에 이동할 거리를 알아낼거고 이 과정을 계속 반복하기만 하면 함숫값을 줄일 수 있을테니까요.",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "입력이 2개가 아닌 13,000개의 입력이 있는 함수에 대한 기본 아이디어는 동일합니다.",
  "model": "DeepL",
  "from_community_srt": "이건 13,000개의 입력을 가진 함수까지 확장해서 적용할 수 있는 기본적인 아이디어입니다.",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "네트워크의 13,000개의 가중치와 편향성을 모두 거대한 컬럼 벡터로 구성한다고 상상해 보세요.",
  "model": "DeepL",
  "from_community_srt": "우리 신경망의 13,000여개의 가중치와 bias를 거대한 열 벡터로 조직했다고 생각해보세요.",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "비용 함수의 음의 기울기는 벡터일 뿐이며, 이 엄청나게 큰 입력 공간 안에서 어떤 숫자를 넛지하면 비용 함수가 가장 빠르게 감소할지 알려주는 방향입니다.",
  "model": "DeepL",
  "from_community_srt": "cost 함수의 그래디언트의 음의 방향은 그냥 벡터일 뿐입니다. 이 벡터는 이 광활한 입력 공간에서 어떤 방향이 cost 함수를 가장 빠르게 감소시켜 주는지를 알려줍니다.",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "물론, 특별히 설계된 비용 함수를 사용하면 가중치와 편향을 변경하여 이를 낮추면 각 학습 데이터에 대한 네트워크의 출력이 10개의 값으로 이루어진 무작위 배열이 아니라 우리가 원하는 실제 의사 결정처럼 보이도록 만들 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "물론 우리의 특별 제작된 cost 함수는 가중치와 bias들을 cost 함수가 줄어드는 방향으로 조정할 것이고 그 뜻은 우리의 신경망이 훈련 데이터를 근거로 해서 무작위로 10개의 숫자를 출력하지 않고, 우리가 원하는 정확한 결과를 출력해준다는 뜻입니다.",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "이 비용 함수는 모든 학습 데이터에 대한 평균을 포함하므로 이를 최소화하면 모든 샘플에서 더 나은 성능을 얻을 수 있다는 것을 의미합니다.",
  "model": "DeepL",
  "from_community_srt": "기억하세요. 이 cost 함수는 모든 훈련 데이터의 평균 cost를 수반합니다. 따라서 cost 함수를 최소화한다는 것은 이 모든 샘플들에서 더 나은 성능을 보여준다는 것입니다.",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "신경망 학습의 핵심인 이 기울기를 효율적으로 계산하는 알고리즘을 역전파라고 하며, 다음 동영상에서 설명할 내용은 바로 이 역전파에 관한 것입니다.",
  "model": "DeepL",
  "from_community_srt": "이 그래디언트 계산을 효과적으로 만드는 알고리즘이 신경망이 얼마나 효과적으로 배울 수 있을 지를 결정하는 핵심적인 요소입니다. 그리고 그 알고리즘을 \"back propagation\"(오차역전파법)이라고 부릅니다. 이게 바로 다음 영상에서 다룰 내용이고요.",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "여기서 저는 주어진 학습 데이터의 각 가중치와 편향에 정확히 어떤 일이 일어나는지 시간을 들여 살펴보고, 관련 수식과 공식의 더미 너머에서 어떤 일이 일어나는지 직관적으로 느끼도록 노력했습니다.",
  "model": "DeepL",
  "from_community_srt": "이제 많은 시간을 들여서 설명 드리고 싶은 내용들입니다. 주어진 훈련 데이터마다 각각의 가중치와 bias들에는 정확히 어떤 일이 일어나는 걸까요? 최대한 직관적으로 체험시켜드리기 위해, 관련된 미적분이나 수식들은 제껴놓겠습니다.",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "지금 이 자리에서 구현 세부 사항과는 별개로, 네트워크 학습에 대해 이야기할 때 가장 중요한 것은 비용 함수를 최소화하는 것이라는 점입니다.",
  "model": "DeepL",
  "from_community_srt": "지금 가장 중요한 것은, \"구현 세부사항의 독립성\"(Independent of Implementation Details)이 무엇인지를 알아야 한다는 겁니다. 이게 무슨 뜻이냐면, 신경망 학습은 그저 cost 함수를 최소화하는 것 뿐이라는 겁니다.",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "그리고 그 결과 중 하나는 이 비용 함수가 매끄럽게 출력되는 것이 중요하므로, 내리막길을 조금씩 내려가면서 국부적 최소값을 찾을 수 있다는 점입니다.",
  "model": "DeepL",
  "from_community_srt": "그로부터 우리는 cost 함수가 매끄러운 출력을 갖는 것이 중요하다는 결론을 얻을 수 있습니다. 그래야지 우리가 내리막을 한발짝씩 내려가면서 지역 최솟값을 찾을 수 있을테니까요.",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "그런데 인공 뉴런은 생물학적 뉴런처럼 단순히 활성화되거나 비활성화되는 이분법적인 방식이 아니라 지속적으로 다양한 활성화 상태를 유지합니다.",
  "model": "DeepL",
  "from_community_srt": "이런 이유로 인공 뉴런들은 단순히 활성화/비활성화로 결정되지 않고 연속적인 활성화 값을 갖습니다. 실제 생물의 뉴런처럼 말이죠.",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "함수의 입력값을 음의 기울기의 배수만큼 반복적으로 넛지하는 이 과정을 기울기 하강이라고 합니다.",
  "model": "DeepL",
  "from_community_srt": "이렇게 반복적으로 그래디언트의 음의 방향으로 함수의 입력값을 이동하는 방식을 \"경사 하강법\"(Gradient Descent)이라고 부릅니다.",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "이는 비용 함수의 국부적 최소값을 향해 수렴하는 방법으로, 기본적으로 이 그래프에서 계곡을 그리는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "지금 보이는 그래프의 골짜기, 즉, cost 함수의 지역 최소값으로  수렴하는 방법입니다.",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "물론 13,000차원 입력 공간에서의 넛지는 이해하기 어렵기 때문에 여전히 두 개의 입력이 있는 함수의 그림을 보여주고 있지만, 비공간적으로 생각할 수 있는 좋은 방법이 있습니다.",
  "model": "DeepL",
  "from_community_srt": "저는 아직 두 개의 입력을 가진 함수만 보여드렸습니다. 왜냐면, 13,002차원 입력 공간에서 굴러다니는 공을 이해시켜드리긴 좀 어려우니까요. 하지만 이 상황을 비(非)공간적으로 생각할 수 있는 좋은 방법이 있습니다.",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "음수 그라데이션의 각 구성 요소는 두 가지를 알려줍니다.",
  "model": "DeepL",
  "from_community_srt": "음의 그래디언트의 각각의 요소들은 우리에게 두 가지를 알려줍니다.",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "물론 부호는 입력 벡터의 해당 컴포넌트를 위 또는 아래로 넛지해야 하는지 여부를 알려줍니다.",
  "model": "DeepL",
  "from_community_srt": "그래디언트의 부호는 상응하는 입력 벡터의 요소가 커져야 할지 작아져야 할지를 알려줍니다.",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "하지만 중요한 것은 이러한 모든 구성 요소의 상대적인 크기를 통해 어떤 변화가 더 중요한지 알 수 있다는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "하지만 더 중요한 것은 이 요소들의 상대적인 크기입니다. 이 크기는 어떤 요소를 조정하는 것이 더 큰 영향을 미칠지를 알려줍니다.",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "네트워크에서 가중치 중 하나를 조정하는 것이 다른 가중치를 조정하는 것보다 비용 함수에 훨씬 더 큰 영향을 미칠 수 있다는 것을 알 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "어떤 한 가중치를 조정하는 것이 다른 가중치를 조정하는 것보다 cost 함수에 더 큰 영향을 미칠 수 있습니다.",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "이러한 연결 중 일부는 학습 데이터에 더 중요합니다.",
  "model": "DeepL",
  "from_community_srt": "이런 몇몇 연결선들은 훈련 데이터에 더 민감하게 반응합니다.",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "따라서 이 거대한 비용 함수의 그래디언트 벡터에 대해 생각할 수 있는 방법은 각 가중치와 편향의 상대적 중요도, 즉 어떤 변화가 가장 큰 효과를 가져올 것인지를 암호화하는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "따라서 이 무지막지한 cost 함수의 그래디언트 벡터를 각각의 가중치와 bias의 중요도를 표현하는 것이라고 생각할 수 있습니다. 그러니까, 이것들 중에 몇몇은 조정하면 대박 치는 것들이라는 겁니다.",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "이것은 방향성에 대한 또 다른 사고 방식일 뿐입니다.",
  "model": "DeepL",
  "from_community_srt": "이것은 방향이라는 개념을 다르게 이해해보는 방법입니다.",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "더 간단한 예를 들자면, 두 개의 변수를 입력으로 하는 함수가 있고 특정 지점에서의 기울기가 3,1로 나온다고 계산하면, 한편으로는 그 입력에 서 있을 때 이 방향을 따라 이동하면 함수가 가장 빠르게 증가한다는 의미로 해석할 수 있고, 입력 점의 평면 위에 함수를 그래프로 그릴 때 그 벡터가 곧은 상승 방향을 제공한다는 의미로 해석할 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "쉬운 예시로는, 입력 변수가 두 개인 함수가 특정한 점에서의 그래디언트가 (3,1)임을 계산해냈다는 것은, 그 점에서 함숫값을 가장 빠르게 증가시키는 방향이 어디인지도 안다는 말입니다. 그 점에서 함숫값을 가장 빠르게 증가시키는 방향이 어디인지도 안다는 말입니다. 입력 평면 공간 위의 곡면에서는 가장 가파른 오르막이 그 방향인지 안다는 말이구요.",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "그러나 이를 읽는 또 다른 방법은 첫 번째 변수의 변경이 두 번째 변수의 변경보다 3배 더 중요하다는 것, 즉 적어도 관련 입력 근처에서는 X값을 넛지하는 것이 훨씬 더 큰 효과를 가져온다는 것을 의미합니다.",
  "model": "DeepL",
  "from_community_srt": "그 방향이라는 개념을 다르게 이해하는 방법은 첫 번째 변수를 조정하는 것이 두 번째 변수를 조정하는 것보다 3배 중요하다고 이해하는 것입니다. 그 점 근처에선 말이죠.",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "지금까지의 상황을 축소하여 요약해 보겠습니다.",
  "model": "DeepL",
  "from_community_srt": "x값을 조정하는게 대박 치는 방법이겠죠 좋아요. 이제 우리가 어디까지 왔는지 잠깐 돌아봅시다.",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "네트워크 자체는 784개의 입력과 10개의 출력으로 이루어진 함수이며, 이 모든 가중치 합으로 정의됩니다.",
  "model": "DeepL",
  "from_community_srt": "신경망은 784개의 입력을 받아서 가중치와 bias를 통해 10개의 출력을 내놓는 함수입니다.",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "비용 함수는 그 위에 복잡성을 더하는 계층입니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "13,000개의 가중치와 편향을 입력으로 받아 훈련 예시를 기반으로 형편없는 단일 측정값을 뱉어냅니다.",
  "model": "DeepL",
  "from_community_srt": "복잡함을 더하는 cost 함수는 13,000여개의 가중치와 bias를 입력값으로 받아서 신경망이 얼마나 엉망으로 작동하고 있는지를 나타내는 단 하나의 숫자를 내놓습니다.",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "그리고 비용 함수의 그라데이션은 여전히 복잡성이 한 층 더 높습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "이 모든 가중치와 편향에 대한 어떤 넛지가 비용 함수의 값을 가장 빠르게 변화시키는지 알려주며, 이는 어떤 가중치의 변화가 가장 중요한지 말해주는 것으로 해석할 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "또 어려운 부분인 cost 함수의 그래디언트는 이 가중치와 bias들을 어떻게 조정하는 것이 cost 함숫값을 가장 빠르게 줄여주는지 알려줍니다. 다른 표현으론, 어떤 가중치를 조정하는 것이 더 큰 영향을 끼치느냐,",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "그렇다면 무작위 가중치와 편향으로 네트워크를 초기화하고 이 그라데이션 하강 프로세스에 따라 여러 번 조정하면 이전에 본 적이 없는 이미지에서 실제로 얼마나 잘 작동할까요?",
  "model": "DeepL",
  "from_community_srt": "라는 것이죠. 그렇다면, 당신이 처음 신경망을 무작위 가중치와 bias로 만들고 경사 하강법으로 그것들을 여러번에 걸쳐 조정한다면 이 신경망이 한 번도 보지 못한 이미지에 대해 실제로 얼마나 잘 작동할까요?",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "제가 설명한 방식은 주로 미적인 이유로 선택한 16개의 뉴런으로 구성된 두 개의 숨겨진 레이어가 있으며, 새로 보는 이미지의 약 96%를 정확하게 분류하는 나쁘지 않은 수준입니다.",
  "model": "DeepL",
  "from_community_srt": "앞서 16개의 뉴런으로 된 두 개의 숨겨진 층을 쓰기로 사용한건 그냥 보기 좋아서 그랬다고 설명했었죠. 하지만, 썩 괜찮네요! 새로운 이미지를 정확도 96%로 구별해내고 있어요.",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "솔직히 말해서, 엉망이 된 몇 가지 사례를 보면 조금만 더 여유를 가져야겠다는 생각이 듭니다.",
  "model": "DeepL",
  "from_community_srt": "사실 신경망이 구별 못 한 이미지들을 보면 솔직히 이정도 틀리는건 봐줄만 하다는 생각이 들겁니다.",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "이제 숨겨진 레이어 구조를 가지고 놀면서 몇 가지 조정을 하면 98%까지 얻을 수 있습니다.",
  "model": "DeepL",
  "from_community_srt": "당신이 숨겨진 층을 좀 가지고 놀다보면 정확도를 98%까지 끌어올릴 수 있습니다.",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "그리고 그것은 꽤 좋습니다!",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "이 평범한 바닐라 네트워크보다 더 정교하게 만들면 더 나은 성능을 얻을 수 있지만, 초기 작업이 얼마나 어려운지를 고려할 때, 어떤 네트워크도 이전에 본 적 없는 이미지에서 이 정도로 잘 작동한다는 것은 놀라운 일이라고 생각합니다(어떤 패턴을 찾아야 하는지 구체적으로 알려주지 않았기 때문에).",
  "model": "DeepL",
  "from_community_srt": "아주 좋아요! 최고는 아니지만. 이 평범한 신경망보다 더 정교하고 복잡한 신경망을 쓴다면 더 좋은 정확도를 얻을 수 있습니다. 하지만 초기 작업이 아주 막막했던걸 감안하면 어떤 신경망이든 전혀 보지 못했던 이미지에 대해서 이렇게 잘 작동한다는건 아주 놀랍다고 생각해요. 어떤 패턴을 찾아야 하는지 구체적으로 알려주지도 않았는데 말이죠.",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "원래 이 구조의 동기는 두 번째 레이어가 작은 가장자리를 포착하고, 세 번째 레이어가 그 가장자리를 조합하여 루프와 긴 선을 인식하고, 이를 조합하여 숫자를 인식할 수 있을 것이라는 희망을 설명하는 것이었습니다.",
  "model": "DeepL",
  "from_community_srt": "원래 제가 이 구조에서 원했던 것은 설명 드렸던 것처럼 두 번째 층이 숫자의 테두리들을 인식하고 세 번째 층은 테두리들을 합쳐서 동그라미나 직선을 인식해서 그것들을 합쳐 하나의 숫자를 인식하는 것이었습니다.",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "그렇다면 우리 네트워크는 실제로 이런 일을 하고 있을까요?",
  "model": "DeepL",
  "from_community_srt": "우리의 신경망에서 실제로 이런 과정들이 일어나고 있을까요? 음...",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "적어도 이 경우에는 전혀 그렇지 않습니다.",
  "model": "DeepL",
  "from_community_srt": "이런 경우엔 절대 아니죠.",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "지난 동영상에서 첫 번째 레이어의 모든 뉴런에서 두 번째 레이어의 특정 뉴런까지의 연결 가중치를 두 번째 레이어 뉴런이 포착하는 주어진 픽셀 패턴으로 시각화하는 방법을 살펴본 것을 기억하시나요?",
  "model": "DeepL",
  "from_community_srt": "이전 영상에서 본 내용을 기억하신다면 첫 번째 층의 모든 뉴런과 두 번째 층의 한 뉴런 사이의 연결은 지금 보시는 픽셀 패턴으로 시각화 될 수 있습니다. 두 번째 층의 모든 뉴런에 대해서요.",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "실제로 이러한 전환과 관련된 가중치에 대해 첫 번째 레이어에서 다음 레이어로 전환할 때 여기저기서 고립된 작은 가장자리를 포착하는 대신, 중간에 매우 느슨한 패턴이 있는 거의 무작위적인 모양이 됩니다.",
  "model": "DeepL",
  "from_community_srt": "실제로 가중치들을 이렇게 시각화 해보면 독립된 테두리를 인식한다기 보단 가운데 희미하게 보이는 패턴을 제외하곤 아무렇게나 생긴 것처럼 보입니다.",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "헤아릴 수 없을 정도로 큰 13,000차원의 공간에서 가능한 가중치와 편향이 존재하는 네트워크는 대부분의 이미지를 성공적으로 분류했지만, 우리가 기대했던 패턴을 정확히 포착하지 못하는 작은 국소 최소값을 발견했습니다.",
  "model": "DeepL",
  "from_community_srt": "우리의 신경망은 불가해하게 거대한 13,000차원 가중치와 bias 입력 공간에서 자신만의 작고 소중한 지역 최소값을 찾았고 그 덕에 우리가 원했던 방식으로 패턴을 인식하진 않지만 대부분의 이미지를 성공적으로 인식합니다.",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "이 점을 확실히 이해하려면 임의의 이미지를 입력했을 때 어떤 일이 일어나는지 살펴보세요.",
  "model": "DeepL",
  "from_community_srt": "이게 어떤 상황인지를 정확히 이해하기 위해, 무작위 이미지를 입력하면 어떤 일이 일어나는지 한 번 봅시다.",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "시스템이 똑똑하다면 10개의 출력 뉴런 중 어느 하나도 실제로 활성화하지 않거나 모두 고르게 활성화하지 않는 등 불확실하게 느껴질 수도 있지만, 대신 이 무작위 노이즈가 5라는 것을 실제 5의 이미지가 5라는 것처럼 확신하는 것처럼 자신 있게 말도 안 되는 답을 내립니다.",
  "model": "DeepL",
  "from_community_srt": "시스템이 아주 똑똑하다면, 아마 시스템이 불확실함을 느끼고 10개의 출력중 그 어떤 것도 활성화 시키지 않거나 10개 모두를 고르게 활성화 시킬 것이라고 예상하실 겁니다. 하지만 신경망은 이 무작위 이미지가 5라고 아주 자신있게 대답합니다. 실제 5의 이미지를 봤을 때 처럼요.",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "다르게 표현하면, 이 네트워크는 숫자를 꽤 잘 인식할 수 있어도 숫자를 그리는 방법을 모른다는 뜻입니다.",
  "model": "DeepL",
  "from_community_srt": "그 뜻은 신경망이 숫자 이미지를 아주 잘 인식하더라도, 숫자를 그릴줄은 모른다는 겁니다.",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "이 중 많은 부분이 매우 제한적인 교육 환경이기 때문입니다.",
  "model": "DeepL",
  "from_community_srt": "이는 너무 엄격하게 제한된 훈련 환경 때문입니다.",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "네트워크의 입장에서 생각해 보세요.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "이 관점에서 보면, 우주 전체는 작은 격자를 중심으로 명확하게 정의된 움직이지 않는 숫자로만 구성되어 있으며, 비용 함수는 자신의 결정에 완전히 확신할 수밖에 없는 인센티브를 제공하지 않습니다.",
  "model": "DeepL",
  "from_community_srt": "내 말은 당신이 신경망의 입장에서 생각해 본다면 세상에는 격자 안에서 움직이지 않는 명백히 숫자라고 생각되는 것밖에 없고 cost 함수는 오직 신경망이 자신의 결정에 확신을 가지게만 장려했습니다.",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "두 번째 레이어 뉴런이 실제로 어떤 일을 하는지에 대한 이미지로, 가장자리와 패턴을 포착하는 동기를 가진 이 네트워크를 소개하는 이유가 궁금하실 것입니다.",
  "model": "DeepL",
  "from_community_srt": "그래서, 이 이미지가 두 번째 층이 실제로 하는 일이라면 왜 제가 테두리와 패턴을 인식하는 신경망을 소개했는지 이상하다고 생각하실 겁니다.",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "결국에는 전혀 그렇지 않습니다.",
  "model": "DeepL",
  "from_community_srt": "제 대답은, 이게 신경망이 하는 일의 전부가 아니라는 겁니다.",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "하지만 이는 최종 목표가 아니라 시작점일 뿐입니다.",
  "model": "DeepL",
  "from_community_srt": "그러니까, 이게 우리의 목표가 아니라 사실은 출발점에 불과합니다.",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "솔직히 이것은 80년대와 90년대에 연구된 오래된 기술이며, 더 자세한 최신 변형을 이해하기 전에 이해해야 하며, 몇 가지 흥미로운 문제를 해결할 수 있는 것은 분명하지만 숨겨진 계층이 실제로 무엇을 하는지 파헤칠수록 지능이 떨어지는 것 같습니다.",
  "model": "DeepL",
  "from_community_srt": "까놓고 말해서, 신경망은 오래된 기술입니다. 80~90년대에 연구됐던 기술이죠. 여러가지 흥미로운 문제의 해결을 위해 만들어진 현대의 다양하고 상세해진 신경망을 이해하기 위해 당신은 이 신경망부터 이해해야할 필요가 있습니다. 하지만 현대의 신경망의 숨겨진 층이 하는 일을 들여다 보면, '생각'이라는 과정과는 더욱 동떨어져 있다는걸 알게 될겁니다.",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "네트워크가 학습하는 방식에서 사용자가 학습하는 방식으로 잠시 초점을 옮기면, 어떤 식으로든 이 자료에 적극적으로 참여해야만 가능합니다.",
  "model": "DeepL",
  "from_community_srt": "잠시 신경망이 학습하는 방법이 아니라 당신이 학습하는 방법에 대해서 생각해봅시다. 어찌됐든 당신이 이 영상을 보고 능동적으로 사유할 때  학습이 이루어집니다.",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "가장자리와 패턴 같은 것을 더 잘 포착하기 위해 이 시스템에 어떤 변화를 줄 수 있는지, 이미지를 어떻게 인식하는지 잠시 멈춰서 깊이 생각해 보세요.",
  "model": "DeepL",
  "from_community_srt": "아주 간단한 부탁을 드리고 싶습니다. 지금 영상을 잠깐 멈추고 한 번 깊게 생각해 보세요. 당신은 이 시스템을 어떻게 바꿀 수 있을까요? 그리고 당신의 시스템이 테두리나 패턴 따위를 인식하기를 바란다면, 시스템은 이미지를 어떻게 인식하게 될까요? 하지만,",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "하지만 그보다 더 좋은 방법은 딥러닝과 신경망에 관한 마이클 닐슨의 책을 추천하는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "그보단 실제로 사용하고 있는 것들에 대해 공부하는 편이 좋을 것 같습니다. 저는 Michael Nielsen이 딥러닝과 신경망에 대해 쓴 책들을 적극 추천드립니다.",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "이 책에서는 이 예제를 위해 다운로드하여 사용할 코드와 데이터를 찾을 수 있으며, 해당 코드가 수행하는 작업을 단계별로 안내합니다.",
  "model": "DeepL",
  "from_community_srt": "책에서는 영상에서 보신 예제에 대한 코드와 데이터를 제공하고 있습니다. 그리고 그 코드가 어떤 일을 하는지 차근차근 가르쳐드릴 겁니다.",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "이 책은 무료로 공개되어 있으므로, 이 책을 통해 무언가를 얻으셨다면 닐슨의 노력에 기부하는 데 동참해 보세요.",
  "model": "DeepL",
  "from_community_srt": "게다가 이 책은 무료로 공개되어 있습니다. 멋지죠? 그러니 이 책으로 도움을 받았다면 Nielsen의 노고에 후원해보세요.",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "설명에 크리스 올라의 경이롭고 아름다운 블로그 게시물과 디스틸의 기사 등 제가 좋아하는 다른 리소스도 몇 개 링크해 두었습니다.",
  "model": "DeepL",
  "from_community_srt": "또 저는 제가 아주 좋아하는 몇 가지 다른 링크들도 달아놓았습니다. Chris Ola의 경이롭고 아름다운 포스팅이나 distill.pub의 게시물 같은 것들이요.",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "마지막 몇 분 동안의 이야기를 마무리하기 위해 제가 레이샤 리와 나눈 인터뷰의 일부를 다시 소개해드리겠습니다.",
  "model": "DeepL",
  "from_community_srt": "마치며 남은 몇 분 동안 지난번에 Lisha Li와 가졌던 인터뷰의 한 부분에 대해 이야기 해보고 싶습니다.",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "지난 영상에서 딥러닝으로 박사 학위를 취득한 그녀를 기억하실 겁니다.",
  "model": "DeepL",
  "from_community_srt": "이전 영상에 그녀가 나왔던걸 기억하시겠죠. 그녀는 딥러닝 연구로 박사학위를 받았습니다.",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "이 짧은 글에서는 최신 이미지 인식 네트워크가 실제로 어떻게 학습하는지에 대해 자세히 설명하는 두 편의 최근 논문을 소개합니다.",
  "model": "DeepL",
  "from_community_srt": "그녀는 더 현대적인 이미지 인식 신경망이 실제로 어떻게 학습하는지 최근 깊게 분석한 두 논문에 대해 이야기해줬습니다.",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "첫 번째 논문에서는 이미지 인식에 매우 능숙한 심층 신경망 중 하나를 가져와 라벨이 제대로 지정된 데이터 세트에서 훈련하는 대신 모든 라벨을 뒤섞어 훈련했습니다.",
  "model": "DeepL",
  "from_community_srt": "첫 번째 논문은 이미지 인식에 아주 탁월한 심층적인 신경망을 다뤘습니다. 이 신경망은 제대로 라벨링된 데이터셋으로 훈련하는 대신 훈련 전에 모든 레이블을 섞어놓았습니다.",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "모든 것이 무작위로 레이블이 지정되었기 때문에 테스트 정확도는 무작위보다 떨어지지만, 그래도 제대로 레이블이 지정된 데이터 세트에서와 동일한 학습 정확도를 달성할 수 있었습니다.",
  "model": "DeepL",
  "from_community_srt": "테스트 정확도는 그냥 무작위로 나올 것처럼 보입니다. 모두 무작위로 라벨링 돼있으니까요. 하지만 제대로 라벨링된 데이터셋으로 학습한 것과 동일한 테스트 정확도를 달성했습니다.",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "기본적으로 이 특정 네트워크의 수백만 개의 가중치는 무작위 데이터를 암기하는 데 충분했기 때문에 이 비용 함수를 최소화하는 것이 실제로 이미지의 어떤 구조에 해당하는지, 아니면 그냥 암기하는 것인지에 대한 의문이 생깁니다.",
  "model": "DeepL",
  "from_community_srt": "기본적으로 이 신경망에 있는 수백만개의 가중치들은 무작위 데이터를 기억하기에 충분합니다. 그렇다면 이런 의문점이 생깁니다. cost 함수를 최소화 한다는 것은 이미지에 나타나는 구조를 분류하는 건가요? 아니면 그냥 기억하는 것에 불과한 건가요?",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "정확도 곡선을 보면, 무작위 데이터 세트로 훈련하는 경우 이 곡선은 거의 선형적인 방식으로 매우 느리게 내려가므로 정확도를 얻을 수 있는 적절한 가중치를 찾기 위해 정말 고군분투하고 있습니다.",
  "model": "DeepL",
  "from_community_srt": "올바른 분류가 무엇인지에 대한 데이터셋을 기억하는 거예요. 올해 ICML엔 반 년 동안 이를 반박하는 논문이 없었어요. 이렇게요. \"야! 신경망은 그것보다 더 똑똑해.\" 이 정확도 곡선을 보면 만약 당신이 무작위 데이터셋으로 훈련을 시작했다면 곡선이 거의 직선에 가깝게 아주 천천히 하강합니다. 당신이 구조화된 데이터셋,",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "반면에 실제로 올바른 레이블이 있는 구조화된 데이터 집합으로 학습하는 경우 처음에는 조금 더듬거리다가 정확도 수준에 도달하기 위해 매우 빠르게 떨어지므로 어떤 의미에서는 로컬 최대값을 찾는 것이 더 쉬웠습니다.",
  "model": "DeepL",
  "from_community_srt": "그러니까 제대로 라벨링된 데이터셋으로 올바른 가중치를 찾아 높은 정확도를 얻기 위해 가능한 지역 최소값을 찾는다면 처음엔 조금 헤매다가, 어느순간 좋은 정확도 수준으로 갑자기 뚝 떨어질 겁니다. 따라서 어떤 면에선 저 지역 최대값을 찾기 더 쉽습니다.",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "그래서 그것에 대해 흥미로운 점은 실제로 몇 년 전에 나온 또 다른 논문이 네트워크 계층에 대해 훨씬 더 단순화되어 있지만 결과 중 하나는 최적화 환경을 보면 이러한 네트워크가 학습하는 경향이있는 로컬 최소값이 실제로 동일한 품질이므로 어떤 의미에서 데이터 세트가 구조화되어 있으면 훨씬 더 쉽게 찾을 수 있어야한다는 것입니다.",
  "model": "DeepL",
  "from_community_srt": "그리고 또 흥미로운 사실은 몇 년 지난 다른 논문을 보면 층이 더 단순화된 신경망을 다룬 논문이요 층이 더 단순화된 신경망을 다룬 논문이요 그러나 결과는 이 최적화된 환경을 보면 신경망이 배우려고 하는 지역 최소값들은 사실 동등한 가치를 갖는다는 것을 말해줍니다. 그러므로 어떤 면에선 데이터셋이 구조화 돼있다면, 지역 최소값을 더 쉽게 찾을 수 있습니다.",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "언제나 그렇듯이 Patreon을 후원해 주시는 분들께 감사드립니다.",
  "model": "DeepL",
  "from_community_srt": "patreon으로 후원해주시는 모든 분들께 감사드립니다.",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "이전에도 Patreon이 얼마나 획기적인지 말씀드렸지만, 이 동영상은 여러분 없이는 불가능했을 것입니다.",
  "model": "DeepL",
  "from_community_srt": "patreon에 대해선 이미 말씀 드렸었지만, 이 영상들은 당신이 없었다면 만들수 없었을 겁니다.",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "또한 이 시리즈의 첫 번째 동영상을 지원해 준 VC 회사인 Amplify Partners에 특별히 감사의 말씀을 전하고 싶습니다.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]