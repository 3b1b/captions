1
00:00:04,180 --> 00:00:07,280
지난 동영상에서 신경망의 구조를 설명했습니다.

2
00:00:07,680 --> 00:00:10,000
기억에 남을 수 있도록 간단히 요약한 다음, 

3
00:00:10,000 --> 00:00:12,600
이 비디오의 두 가지 주요 목표를 말씀드리겠습니다.

4
00:00:13,100 --> 00:00:15,495
첫 번째는 신경망의 학습 방식뿐만 아니라 

5
00:00:15,495 --> 00:00:17,891
다른 많은 머신러닝의 작동 방식에 기초가 

6
00:00:17,891 --> 00:00:20,600
되는 경사 하강이라는 개념을 소개하는 것입니다.

7
00:00:21,120 --> 00:00:23,320
그런 다음 이 특정 네트워크의 작동 

8
00:00:23,320 --> 00:00:25,630
방식과 숨겨진 뉴런 층이 결국 무엇을 

9
00:00:25,630 --> 00:00:27,940
찾는지 조금 더 자세히 살펴보겠습니다.

10
00:00:28,980 --> 00:00:32,455
이 동영상에서는 신경망의 헬로 월드라고 할 

11
00:00:32,455 --> 00:00:36,220
수 있는 손으로 쓴 숫자 인식을 다룰 것입니다.

12
00:00:37,020 --> 00:00:40,278
이 숫자들은 28x28 픽셀 그리드에 렌더링되며, 

13
00:00:40,278 --> 00:00:43,420
각 픽셀은 0과 1 사이의 회색조 값을 갖습니다.

14
00:00:43,820 --> 00:00:47,054
이 픽셀 값들이 네트워크의 입력 레이어에 있는 

15
00:00:47,054 --> 00:00:50,040
784개의 뉴런의 활성화를 결정할 것입니다.

16
00:00:51,180 --> 00:00:54,031
그리고 다음 레이어의 각 뉴런에 대한 

17
00:00:54,031 --> 00:00:56,882
활성화는 이전 레이어의 모든 활성화의 

18
00:00:56,882 --> 00:01:00,820
가중합에 편항이라는 특수 숫자를 더한 값을 구한 다음

19
00:01:02,160 --> 00:01:05,550
그런 다음 지난 동영상에서 설명한 것처럼 시그모이드 

20
00:01:05,550 --> 00:01:08,940
또는 렐루와 같은 다른 함수를 이 합계에 합성합니다.

21
00:01:09,480 --> 00:01:13,040
전체적으로 각각16개의 뉴런으로 구성된 두 개의 

22
00:01:13,040 --> 00:01:15,809
숨겨진 층을 구성한 결과, 네트워크는 

23
00:01:15,809 --> 00:01:19,237
13,000여개의 조정 가능한 가중치와 편항을 

24
00:01:19,237 --> 00:01:22,929
가지고 있으며 이 값들이 네트워크가 실제로 무엇을 

25
00:01:22,929 --> 00:01:24,380
하는지를 결정합니다.

26
00:01:24,880 --> 00:01:28,300
이 네트워크가 주어진 숫자를 분류한다는 것은, 

27
00:01:28,300 --> 00:01:30,800
최종 층의 10개의 뉴런 중 가장 

28
00:01:30,800 --> 00:01:33,300
활성화된 뉴런을 찾는다는 것입니다.

29
00:01:34,100 --> 00:01:36,968
여기서 레이어 구조를 염두에 둔 동기는 두 

30
00:01:36,968 --> 00:01:39,836
번째 레이어가 가장자리를 인식하고 세 번째 

31
00:01:39,836 --> 00:01:43,421
레이어가 루프나 선과 같은 패턴을 인식할 수 있으며, 

32
00:01:43,421 --> 00:01:46,051
마지막 레이어가 이러한 패턴을 조합하여 

33
00:01:46,051 --> 00:01:48,800
숫자를 인식할 수 있다는 점을 기억하세요.

34
00:01:49,800 --> 00:01:51,295
그래서 여기서는 네트워크가 어떻게 

35
00:01:51,295 --> 00:01:52,240
학습하는지 알아봅시다.

36
00:01:52,640 --> 00:01:56,184
우리가 원하는 것은 이 네트워크에 손으로 쓴 숫자의 

37
00:01:56,184 --> 00:01:59,729
다양한 이미지와 그 숫자가 무엇인지에 대한 레이블의 

38
00:01:59,729 --> 00:02:02,907
형태로 제공되는 전체 학습 데이터를 보여주고, 

39
00:02:02,907 --> 00:02:05,841
학습 데이터에 대한 성능을 향상시키기 위해 

40
00:02:05,841 --> 00:02:09,142
13,000개의 가중치와 편향을 조정할 수 있는 

41
00:02:09,142 --> 00:02:10,120
알고리즘입니다.

42
00:02:10,720 --> 00:02:13,842
이러한 계층화된 구조를 통해 학습한 내용을 해당 학습 

43
00:02:13,842 --> 00:02:16,860
데이터 이외의 이미지에 일반화할 수 있기를 바랍니다.

44
00:02:17,640 --> 00:02:19,904
이를 테스트하는 방법은 네트워크를 학습시킨 

45
00:02:19,904 --> 00:02:22,075
후 이전에 본 적이 없는 레이블이 지정된 

46
00:02:22,075 --> 00:02:24,340
데이터를 더 많이 보여주고 새로운 이미지가 

47
00:02:24,340 --> 00:02:26,700
얼마나 정확하게 분류되는지 확인하는 것입니다.

48
00:02:31,120 --> 00:02:34,273
다행히도, 그리고 이러한 일반적인 예로 시작하기 

49
00:02:34,273 --> 00:02:37,776
좋은 이유는 MNIST 데이터베이스의 훌륭한 사람들이 

50
00:02:37,776 --> 00:02:41,280
수만 개의 손으로 쓴 숫자 이미지를 모아 각 이미지에 

51
00:02:41,280 --> 00:02:44,200
해당 번호가 표시된 라벨을 붙였기 때문입니다.

52
00:02:44,900 --> 00:02:48,472
머신을 학습이라고 설명하는 것은 도발적이지만, 

53
00:02:48,472 --> 00:02:51,770
머신이 어떻게 작동하는지 보면 공상 과학의 

54
00:02:51,770 --> 00:02:55,480
전제라기보다는 미적분학 연습에 가깝게 느껴집니다.

55
00:02:56,200 --> 00:02:59,960
기본적으로 특정 함수의 최소값을 찾는 것이 핵심입니다.

56
00:03:01,940 --> 00:03:05,690
개념적으로 각 뉴런은 이전 계층의 모든 뉴런에 

57
00:03:05,690 --> 00:03:09,873
연결되어 있다고 생각하며, 활성화를 정의하는 가중치 

58
00:03:09,873 --> 00:03:13,911
합계의 가중치는 이러한 연결의 강도와 같고 편향은 

59
00:03:13,911 --> 00:03:18,238
해당 뉴런이 활성화 또는 비활성화되는 경향을 나타내는 

60
00:03:18,238 --> 00:03:18,960
것입니다.

61
00:03:19,720 --> 00:03:21,922
우선 모든 가중치와 편향성을 

62
00:03:21,922 --> 00:03:24,400
완전히 무작위로 초기화하겠습니다.

63
00:03:24,940 --> 00:03:27,034
말할 필요도 없이, 이 네트워크는 무작위적인 

64
00:03:27,034 --> 00:03:29,128
작업을 수행하기 때문에 주어진 훈련 예제에서 

65
00:03:29,128 --> 00:03:30,720
꽤 끔찍한 성능을 발휘할 것입니다.

66
00:03:31,040 --> 00:03:33,590
예를 들어, 이 3 이미지를 입력하면 

67
00:03:33,590 --> 00:03:36,020
출력 레이어가 엉망진창처럼 보입니다.

68
00:03:36,600 --> 00:03:40,435
따라서 여러분이 하는 일은 컴퓨터에게 '아니, 

69
00:03:40,435 --> 00:03:43,680
나쁜 컴퓨터, 대부분의 뉴런은 활성화가 

70
00:03:43,680 --> 00:03:47,957
0이지만 이 뉴런은 1이어야 한다'고 말하는 방법, 

71
00:03:47,957 --> 00:03:50,760
즉 비용 함수를 정의하는 것입니다.

72
00:03:51,720 --> 00:03:54,349
조금 더 수학적으로 말하자면, 

73
00:03:54,349 --> 00:03:58,679
각 쓰레기 출력 활성화와 원하는 값 사이의 차이의 

74
00:03:58,679 --> 00:04:02,545
제곱을 더하면 되며, 이를 단일 훈련 예제의 

75
00:04:02,545 --> 00:04:05,020
비용이라고 부를 수 있습니다.

76
00:04:05,960 --> 00:04:09,531
네트워크가 이미지를 정확하게 분류한다고 확신할 

77
00:04:09,531 --> 00:04:12,691
때는 이 합이 작지만, 네트워크가 무엇을 

78
00:04:12,691 --> 00:04:16,399
하는지 모르는 것처럼 보일 때는 이 합이 큽니다.

79
00:04:18,640 --> 00:04:22,127
따라서 수만 개의 교육 예시 모두에 

80
00:04:22,127 --> 00:04:25,440
대한 평균 비용을 고려해야 합니다.

81
00:04:27,040 --> 00:04:29,734
이 평균 비용은 네트워크가 얼마나 형편없는지, 

82
00:04:29,734 --> 00:04:32,740
컴퓨터의 상태가 얼마나 나쁠지를 가늠하는 척도입니다.

83
00:04:33,420 --> 00:04:34,600
그리고 그것은 복잡한 문제입니다.

84
00:04:35,040 --> 00:04:38,057
네트워크 자체가 기본적으로 784개의 숫자, 

85
00:04:38,057 --> 00:04:41,678
즉 픽셀 값을 입력으로 받아 10개의 숫자를 출력으로 

86
00:04:41,678 --> 00:04:45,299
뱉어내는 함수이며, 어떤 의미에서는 이 모든 가중치와 

87
00:04:45,299 --> 00:04:48,800
편향에 의해 매개변수화되어 있다는 점을 기억하시나요?

88
00:04:49,500 --> 00:04:52,820
비용 함수는 그 위에 복잡성을 더합니다.

89
00:04:53,100 --> 00:04:57,119
13,000개 정도의 가중치와 편향을 입력으로 받아 

90
00:04:57,119 --> 00:05:01,138
그 가중치와 편향이 얼마나 나쁜지를 설명하는 하나의 

91
00:05:01,138 --> 00:05:05,157
숫자를 뱉어내는데, 그 정의 방식은 수만 개의 학습 

92
00:05:05,157 --> 00:05:08,900
데이터에 대한 네트워크의 행동에 따라 달라집니다.

93
00:05:09,520 --> 00:05:11,000
생각해야 할 것이 많습니다.

94
00:05:12,400 --> 00:05:13,948
하지만 컴퓨터가 얼마나 형편없는 일을 하고 

95
00:05:13,948 --> 00:05:15,820
있는지 알려주는 것만으로는 큰 도움이 되지 않습니다.

96
00:05:16,220 --> 00:05:18,017
이러한 가중치와 편향성을 어떻게 바꾸면 

97
00:05:18,017 --> 00:05:20,060
더 나아질 수 있는지 알려주고 싶을 것입니다.

98
00:05:20,780 --> 00:05:23,689
13,000개의 입력이 있는 함수를 어렵게 

99
00:05:23,689 --> 00:05:26,600
상상하기보다는 하나의 숫자를 입력으로 하고 

100
00:05:26,600 --> 00:05:29,510
하나의 숫자를 출력으로 하는 간단한 함수를 

101
00:05:29,510 --> 00:05:30,480
상상해 보세요.

102
00:05:31,480 --> 00:05:33,679
이 함수의 값을 최소화하는 입력을 

103
00:05:33,679 --> 00:05:35,300
어떻게 찾을 수 있을까요?

104
00:05:36,460 --> 00:05:39,406
미적분학 학생이라면 최소값을 명시적으로 알아낼 

105
00:05:39,406 --> 00:05:41,900
수 있다는 것을 알겠지만, 정말 복잡한 

106
00:05:41,900 --> 00:05:44,733
함수에서는 그것이 항상 가능한 것은 아니며, 

107
00:05:44,733 --> 00:05:47,679
특히 이 상황과 같이 입력값이 13,000개에 

108
00:05:47,679 --> 00:05:51,080
달하는 복잡한 신경망 비용 함수에서는 더욱 그러합니다.

109
00:05:51,580 --> 00:05:54,078
보다 유연한 전략은 어떤 입력값에서 

110
00:05:54,078 --> 00:05:56,326
시작하여 어느 방향으로 나아가야 

111
00:05:56,326 --> 00:05:59,200
출력을 낮출 수 있는지 파악하는 것입니다.

112
00:06:00,080 --> 00:06:03,433
구체적으로, 현재 위치에서 함수의 기울기를 파악할 

113
00:06:03,433 --> 00:06:06,666
수 있다면 그 기울기가 양수이면 입력을 왼쪽으로 

114
00:06:06,666 --> 00:06:09,900
이동하고, 음수이면 입력을 오른쪽으로 이동합니다.

115
00:06:11,960 --> 00:06:14,547
이 작업을 반복하여 각 지점에서 새로운 

116
00:06:14,547 --> 00:06:17,252
기울기를 확인하고 적절한 단계를 수행하면 

117
00:06:17,252 --> 00:06:19,840
함수의 국부적 최소값에 접근하게 됩니다.

118
00:06:20,640 --> 00:06:22,265
여기서 떠올릴 수 있는 이미지는 

119
00:06:22,265 --> 00:06:23,800
언덕을 굴러 내려가는 공입니다.

120
00:06:24,620 --> 00:06:28,120
이 매우 단순한 단일 입력 함수의 경우에도 어떤 

121
00:06:28,120 --> 00:06:31,621
임의의 입력에서 시작하느냐에 따라 다양한 계곡에 

122
00:06:31,621 --> 00:06:35,121
도달할 수 있으며, 도달하는 지역 최소값이 비용 

123
00:06:35,121 --> 00:06:38,751
함수의 가능한 가장 작은 값이 될 것이라는 보장은 

124
00:06:38,751 --> 00:06:39,400
없습니다.

125
00:06:40,220 --> 00:06:42,620
이는 신경망 사례에도 적용됩니다.

126
00:06:43,180 --> 00:06:46,636
또한 스텝 크기를 경사에 비례하게 만들면 

127
00:06:46,636 --> 00:06:50,092
경사가 최소가 될수록 스텝이 점점 작아져 

128
00:06:50,092 --> 00:06:54,600
오버슈팅을 방지하는 데 도움이 된다는 점도 알아두세요.

129
00:06:55,940 --> 00:06:58,460
복잡성을 조금 더 높여서 두 개의 입력과 

130
00:06:58,460 --> 00:07:00,980
하나의 출력이 있는 함수를 상상해 보세요.

131
00:07:01,500 --> 00:07:04,942
입력 공간을 xy-평면으로, 비용 함수는 그 위에 

132
00:07:04,942 --> 00:07:08,140
그래프로 표시되는 표면으로 생각할 수 있습니다.

133
00:07:08,760 --> 00:07:12,068
함수의 기울기를 묻는 대신, 함수의 출력을 

134
00:07:12,068 --> 00:07:15,376
가장 빨리 줄이려면 이 입력 공간에서 어느 

135
00:07:15,376 --> 00:07:18,960
방향으로 스텝을 밟아야 하는지 물어봐야 합니다.

136
00:07:19,720 --> 00:07:21,760
다시 말해, 내리막길의 방향은 무엇인가요?

137
00:07:22,380 --> 00:07:23,755
다시 말하지만, 언덕을 굴러 

138
00:07:23,755 --> 00:07:25,560
내려가는 공을 생각하면 도움이 됩니다.

139
00:07:26,660 --> 00:07:29,408
다변수 미적분에 익숙하신 분들은 함수의 

140
00:07:29,408 --> 00:07:32,532
기울기가 가장 가파른 상승 방향을 알려주며, 

141
00:07:32,532 --> 00:07:35,781
함수를 가장 빠르게 증가시키려면 어느 방향으로 

142
00:07:35,781 --> 00:07:38,780
발걸음을 옮겨야 하는지 알고 계실 것입니다.

143
00:07:39,560 --> 00:07:42,738
당연히 그 기울기의 음수를 취하면 함수를 가장 

144
00:07:42,738 --> 00:07:46,040
빠르게 감소시키는 단계의 방향을 알 수 있습니다.

145
00:07:47,240 --> 00:07:50,265
이 그라데이션 벡터의 길이를 통해 가장 

146
00:07:50,265 --> 00:07:53,840
가파른 경사가 얼마나 가파른지 알 수 있습니다.

147
00:07:54,540 --> 00:07:56,245
다변수 미적분학이 익숙하지 않고 더 

148
00:07:56,245 --> 00:07:58,207
자세히 알고 싶다면 제가 칸 아카데미에서 

149
00:07:58,207 --> 00:08:00,340
이 주제에 대해 강의한 내용을 확인해 보세요.

150
00:08:00,860 --> 00:08:04,633
하지만 솔직히 지금 여러분과 저에게 중요한 것은 

151
00:08:04,633 --> 00:08:08,126
원칙적으로 내리막길의 방향과 경사를 알려주는 

152
00:08:08,126 --> 00:08:11,900
이 벡터를 계산하는 방법이 존재한다는 사실입니다.

153
00:08:12,400 --> 00:08:14,212
이 정도만 알고 있고 세부 사항에 

154
00:08:14,212 --> 00:08:16,120
대해 잘 모르더라도 괜찮을 것입니다.

155
00:08:17,200 --> 00:08:20,380
이 함수를 최소화하는 알고리즘은 이 경사 

156
00:08:20,380 --> 00:08:23,283
방향을 계산한 다음 내리막길에서 작은 

157
00:08:23,283 --> 00:08:26,740
발걸음을 내딛고 이를 계속 반복하는 것입니다.

158
00:08:27,700 --> 00:08:30,207
입력이 2개가 아닌 13,000개의 입력이 

159
00:08:30,207 --> 00:08:32,820
있는 함수에 대한 기본 아이디어는 동일합니다.

160
00:08:33,400 --> 00:08:36,258
네트워크의 13,000개의 가중치와 편향성을 

161
00:08:36,258 --> 00:08:39,460
모두 거대한 컬럼 벡터로 구성한다고 상상해 보세요.

162
00:08:40,140 --> 00:08:44,301
비용 함수의 음의 기울기는 벡터일 뿐이며, 

163
00:08:44,301 --> 00:08:48,810
이 엄청나게 큰 입력 공간 안에서 어떤 숫자를 

164
00:08:48,810 --> 00:08:53,839
넛지하면 비용 함수가 가장 빠르게 감소할지 알려주는 

165
00:08:53,839 --> 00:08:54,880
방향입니다.

166
00:08:55,640 --> 00:08:58,486
물론, 특별히 설계된 비용 함수를 사용하면 

167
00:08:58,486 --> 00:09:01,569
가중치와 편향을 변경하여 이를 낮추면 각 학습 

168
00:09:01,569 --> 00:09:04,297
데이터에 대한 네트워크의 출력이 10개의 

169
00:09:04,297 --> 00:09:07,262
값으로 이루어진 무작위 배열이 아니라 우리가 

170
00:09:07,262 --> 00:09:10,820
원하는 실제 의사 결정처럼 보이도록 만들 수 있습니다.

171
00:09:11,440 --> 00:09:14,349
이 비용 함수는 모든 학습 데이터에 대한 

172
00:09:14,349 --> 00:09:17,764
평균을 포함하므로 이를 최소화하면 모든 샘플에서 

173
00:09:17,764 --> 00:09:21,180
더 나은 성능을 얻을 수 있다는 것을 의미합니다.

174
00:09:23,820 --> 00:09:26,955
신경망 학습의 핵심인 이 기울기를 효율적으로 

175
00:09:26,955 --> 00:09:29,589
계산하는 알고리즘을 역전파라고 하며, 

176
00:09:29,589 --> 00:09:32,976
다음 동영상에서 설명할 내용은 바로 이 역전파에 

177
00:09:32,976 --> 00:09:33,980
관한 것입니다.

178
00:09:34,660 --> 00:09:37,654
여기서 저는 주어진 학습 데이터의 각 가중치와 

179
00:09:37,654 --> 00:09:40,764
편향에 정확히 어떤 일이 일어나는지 시간을 들여 

180
00:09:40,764 --> 00:09:43,990
살펴보고, 관련 수식과 공식의 더미 너머에서 어떤 

181
00:09:43,990 --> 00:09:47,100
일이 일어나는지 직관적으로 느끼도록 노력했습니다.

182
00:09:47,780 --> 00:09:51,306
지금 이 자리에서 구현 세부 사항과는 별개로, 

183
00:09:51,306 --> 00:09:54,833
네트워크 학습에 대해 이야기할 때 가장 중요한 

184
00:09:54,833 --> 00:09:58,360
것은 비용 함수를 최소화하는 것이라는 점입니다.

185
00:09:59,300 --> 00:10:02,374
그리고 그 결과 중 하나는 이 비용 함수가 매끄럽게 

186
00:10:02,374 --> 00:10:05,025
출력되는 것이 중요하므로, 내리막길을 조금씩 

187
00:10:05,025 --> 00:10:08,100
내려가면서 국부적 최소값을 찾을 수 있다는 점입니다.

188
00:10:09,260 --> 00:10:12,467
그런데 인공 뉴런은 생물학적 뉴런처럼 단순히 

189
00:10:12,467 --> 00:10:15,547
활성화되거나 비활성화되는 이분법적인 방식이 

190
00:10:15,547 --> 00:10:19,140
아니라 지속적으로 다양한 활성화 상태를 유지합니다.

191
00:10:20,220 --> 00:10:23,675
함수의 입력값을 음의 기울기의 배수만큼 반복적으로 

192
00:10:23,675 --> 00:10:26,760
넛지하는 이 과정을 기울기 하강이라고 합니다.

193
00:10:27,300 --> 00:10:29,675
이는 비용 함수의 국부적 최소값을 향해 수렴하는 

194
00:10:29,675 --> 00:10:32,140
방법으로, 기본적으로 이 그래프에서 계곡을 그리는 

195
00:10:32,140 --> 00:10:32,580
것입니다.

196
00:10:33,440 --> 00:10:36,145
물론 13,000차원 입력 공간에서의 넛지는 

197
00:10:36,145 --> 00:10:38,958
이해하기 어렵기 때문에 여전히 두 개의 입력이 

198
00:10:38,958 --> 00:10:41,230
있는 함수의 그림을 보여주고 있지만, 

199
00:10:41,230 --> 00:10:44,260
비공간적으로 생각할 수 있는 좋은 방법이 있습니다.

200
00:10:45,080 --> 00:10:47,139
음수 그라데이션의 각 구성 요소는 

201
00:10:47,139 --> 00:10:48,440
두 가지를 알려줍니다.

202
00:10:49,060 --> 00:10:52,040
물론 부호는 입력 벡터의 해당 컴포넌트를 위 

203
00:10:52,040 --> 00:10:55,140
또는 아래로 넛지해야 하는지 여부를 알려줍니다.

204
00:10:55,800 --> 00:10:58,070
하지만 중요한 것은 이러한 모든 구성 

205
00:10:58,070 --> 00:11:00,125
요소의 상대적인 크기를 통해 어떤 

206
00:11:00,125 --> 00:11:02,720
변화가 더 중요한지 알 수 있다는 것입니다.

207
00:11:05,220 --> 00:11:07,923
네트워크에서 가중치 중 하나를 조정하는 것이 다른 

208
00:11:07,923 --> 00:11:10,433
가중치를 조정하는 것보다 비용 함수에 훨씬 더 

209
00:11:10,433 --> 00:11:13,040
큰 영향을 미칠 수 있다는 것을 알 수 있습니다.

210
00:11:14,800 --> 00:11:18,200
이러한 연결 중 일부는 학습 데이터에 더 중요합니다.

211
00:11:19,320 --> 00:11:22,298
따라서 이 거대한 비용 함수의 그래디언트 

212
00:11:22,298 --> 00:11:25,795
벡터에 대해 생각할 수 있는 방법은 각 가중치와 

213
00:11:25,795 --> 00:11:29,032
편향의 상대적 중요도, 즉 어떤 변화가 가장 

214
00:11:29,032 --> 00:11:32,400
큰 효과를 가져올 것인지를 암호화하는 것입니다.

215
00:11:33,620 --> 00:11:36,640
이것은 방향성에 대한 또 다른 사고 방식일 뿐입니다.

216
00:11:37,100 --> 00:11:40,712
더 간단한 예를 들자면, 두 개의 변수를 입력으로 

217
00:11:40,712 --> 00:11:44,454
하는 함수가 있고 특정 지점에서의 기울기가 3,1로 

218
00:11:44,454 --> 00:11:48,067
나온다고 계산하면, 한편으로는 그 입력에 서 있을 

219
00:11:48,067 --> 00:11:51,550
때 이 방향을 따라 이동하면 함수가 가장 빠르게 

220
00:11:51,550 --> 00:11:54,131
증가한다는 의미로 해석할 수 있고, 

221
00:11:54,131 --> 00:11:57,486
입력 점의 평면 위에 함수를 그래프로 그릴 때 

222
00:11:57,486 --> 00:12:00,840
그 벡터가 곧은 상승 방향을 제공한다는 의미로 

223
00:12:00,840 --> 00:12:02,260
해석할 수 있습니다.

224
00:12:02,860 --> 00:12:06,248
그러나 이를 읽는 또 다른 방법은 첫 번째 변수의 

225
00:12:06,248 --> 00:12:09,758
변경이 두 번째 변수의 변경보다 3배 더 중요하다는 

226
00:12:09,758 --> 00:12:13,390
것, 즉 적어도 관련 입력 근처에서는 X값을 넛지하는 

227
00:12:13,390 --> 00:12:16,900
것이 훨씬 더 큰 효과를 가져온다는 것을 의미합니다.

228
00:12:19,880 --> 00:12:22,340
지금까지의 상황을 축소하여 요약해 보겠습니다.

229
00:12:22,840 --> 00:12:26,378
네트워크 자체는 784개의 입력과 10개의 출력으로 

230
00:12:26,378 --> 00:12:30,040
이루어진 함수이며, 이 모든 가중치 합으로 정의됩니다.

231
00:12:30,640 --> 00:12:33,680
비용 함수는 그 위에 복잡성을 더하는 계층입니다.

232
00:12:33,980 --> 00:12:37,917
13,000개의 가중치와 편향을 입력으로 받아 훈련 

233
00:12:37,917 --> 00:12:41,720
예시를 기반으로 형편없는 단일 측정값을 뱉어냅니다.

234
00:12:42,440 --> 00:12:44,552
그리고 비용 함수의 그라데이션은 

235
00:12:44,552 --> 00:12:46,900
여전히 복잡성이 한 층 더 높습니다.

236
00:12:47,360 --> 00:12:49,962
이 모든 가중치와 편향에 대한 어떤 넛지가 

237
00:12:49,962 --> 00:12:53,216
비용 함수의 값을 가장 빠르게 변화시키는지 알려주며, 

238
00:12:53,216 --> 00:12:55,710
이는 어떤 가중치의 변화가 가장 중요한지 

239
00:12:55,710 --> 00:12:57,880
말해주는 것으로 해석할 수 있습니다.

240
00:13:02,560 --> 00:13:05,360
그렇다면 무작위 가중치와 편향으로 네트워크를 

241
00:13:05,360 --> 00:13:07,936
초기화하고 이 그라데이션 하강 프로세스에 

242
00:13:07,936 --> 00:13:10,400
따라 여러 번 조정하면 이전에 본 적이 

243
00:13:10,400 --> 00:13:13,200
없는 이미지에서 실제로 얼마나 잘 작동할까요?

244
00:13:14,100 --> 00:13:17,183
제가 설명한 방식은 주로 미적인 이유로 선택한 

245
00:13:17,183 --> 00:13:19,911
16개의 뉴런으로 구성된 두 개의 숨겨진 

246
00:13:19,911 --> 00:13:22,639
레이어가 있으며, 새로 보는 이미지의 약 

247
00:13:22,639 --> 00:13:25,960
96%를 정확하게 분류하는 나쁘지 않은 수준입니다.

248
00:13:26,680 --> 00:13:29,722
솔직히 말해서, 엉망이 된 몇 가지 사례를 보면 

249
00:13:29,722 --> 00:13:32,540
조금만 더 여유를 가져야겠다는 생각이 듭니다.

250
00:13:36,220 --> 00:13:38,935
이제 숨겨진 레이어 구조를 가지고 놀면서 몇 

251
00:13:38,935 --> 00:13:41,760
가지 조정을 하면 98%까지 얻을 수 있습니다.

252
00:13:41,760 --> 00:13:42,720
그리고 그것은 꽤 좋습니다!

253
00:13:43,020 --> 00:13:46,161
이 평범한 바닐라 네트워크보다 더 정교하게 만들면 

254
00:13:46,161 --> 00:13:48,293
더 나은 성능을 얻을 수 있지만, 

255
00:13:48,293 --> 00:13:50,985
초기 작업이 얼마나 어려운지를 고려할 때, 

256
00:13:50,985 --> 00:13:53,902
어떤 네트워크도 이전에 본 적 없는 이미지에서 

257
00:13:53,902 --> 00:13:56,819
이 정도로 잘 작동한다는 것은 놀라운 일이라고 

258
00:13:56,819 --> 00:13:59,849
생각합니다(어떤 패턴을 찾아야 하는지 구체적으로 

259
00:13:59,849 --> 00:14:01,420
알려주지 않았기 때문에).

260
00:14:02,560 --> 00:14:05,781
원래 이 구조의 동기는 두 번째 레이어가 작은 

261
00:14:05,781 --> 00:14:09,498
가장자리를 포착하고, 세 번째 레이어가 그 가장자리를 

262
00:14:09,498 --> 00:14:11,976
조합하여 루프와 긴 선을 인식하고, 

263
00:14:11,976 --> 00:14:15,693
이를 조합하여 숫자를 인식할 수 있을 것이라는 희망을 

264
00:14:15,693 --> 00:14:17,180
설명하는 것이었습니다.

265
00:14:17,960 --> 00:14:19,332
그렇다면 우리 네트워크는 실제로 

266
00:14:19,332 --> 00:14:20,400
이런 일을 하고 있을까요?

267
00:14:21,080 --> 00:14:24,400
적어도 이 경우에는 전혀 그렇지 않습니다.

268
00:14:24,820 --> 00:14:27,880
지난 동영상에서 첫 번째 레이어의 모든 뉴런에서 

269
00:14:27,880 --> 00:14:30,940
두 번째 레이어의 특정 뉴런까지의 연결 가중치를 

270
00:14:30,940 --> 00:14:33,773
두 번째 레이어 뉴런이 포착하는 주어진 픽셀 

271
00:14:33,773 --> 00:14:37,060
패턴으로 시각화하는 방법을 살펴본 것을 기억하시나요?

272
00:14:37,780 --> 00:14:41,896
실제로 이러한 전환과 관련된 가중치에 대해 첫 번째 

273
00:14:41,896 --> 00:14:45,588
레이어에서 다음 레이어로 전환할 때 여기저기서 

274
00:14:45,588 --> 00:14:48,711
고립된 작은 가장자리를 포착하는 대신, 

275
00:14:48,711 --> 00:14:52,544
중간에 매우 느슨한 패턴이 있는 거의 무작위적인 

276
00:14:52,544 --> 00:14:53,680
모양이 됩니다.

277
00:14:53,760 --> 00:14:56,900
헤아릴 수 없을 정도로 큰 13,000차원의 

278
00:14:56,900 --> 00:14:59,789
공간에서 가능한 가중치와 편향이 존재하는 

279
00:14:59,789 --> 00:15:03,432
네트워크는 대부분의 이미지를 성공적으로 분류했지만, 

280
00:15:03,432 --> 00:15:06,196
우리가 기대했던 패턴을 정확히 포착하지 

281
00:15:06,196 --> 00:15:08,960
못하는 작은 국소 최소값을 발견했습니다.

282
00:15:09,780 --> 00:15:11,758
이 점을 확실히 이해하려면 임의의 이미지를 

283
00:15:11,758 --> 00:15:13,820
입력했을 때 어떤 일이 일어나는지 살펴보세요.

284
00:15:14,320 --> 00:15:17,452
시스템이 똑똑하다면 10개의 출력 뉴런 중 

285
00:15:17,452 --> 00:15:20,585
어느 하나도 실제로 활성화하지 않거나 모두 

286
00:15:20,585 --> 00:15:23,848
고르게 활성화하지 않는 등 불확실하게 느껴질 

287
00:15:23,848 --> 00:15:27,242
수도 있지만, 대신 이 무작위 노이즈가 5라는 

288
00:15:27,242 --> 00:15:30,766
것을 실제 5의 이미지가 5라는 것처럼 확신하는 

289
00:15:30,766 --> 00:15:34,160
것처럼 자신 있게 말도 안 되는 답을 내립니다.

290
00:15:34,540 --> 00:15:37,726
다르게 표현하면, 이 네트워크는 숫자를 꽤 잘 인식할 

291
00:15:37,726 --> 00:15:40,700
수 있어도 숫자를 그리는 방법을 모른다는 뜻입니다.

292
00:15:41,420 --> 00:15:43,619
이 중 많은 부분이 매우 제한적인 

293
00:15:43,619 --> 00:15:45,240
교육 환경이기 때문입니다.

294
00:15:45,880 --> 00:15:47,740
네트워크의 입장에서 생각해 보세요.

295
00:15:48,140 --> 00:15:51,135
이 관점에서 보면, 우주 전체는 작은 격자를 

296
00:15:51,135 --> 00:15:54,370
중심으로 명확하게 정의된 움직이지 않는 숫자로만 

297
00:15:54,370 --> 00:15:57,365
구성되어 있으며, 비용 함수는 자신의 결정에 

298
00:15:57,365 --> 00:16:00,480
완전히 확신할 수밖에 없는 인센티브를 제공하지 

299
00:16:00,480 --> 00:16:01,080
않습니다.

300
00:16:02,120 --> 00:16:04,659
두 번째 레이어 뉴런이 실제로 어떤 일을 하는지에 

301
00:16:04,659 --> 00:16:07,199
대한 이미지로, 가장자리와 패턴을 포착하는 동기를 

302
00:16:07,199 --> 00:16:09,920
가진 이 네트워크를 소개하는 이유가 궁금하실 것입니다.

303
00:16:09,920 --> 00:16:12,300
결국에는 전혀 그렇지 않습니다.

304
00:16:13,380 --> 00:16:17,180
하지만 이는 최종 목표가 아니라 시작점일 뿐입니다.

305
00:16:17,640 --> 00:16:21,109
솔직히 이것은 80년대와 90년대에 연구된 오래된 

306
00:16:21,109 --> 00:16:24,455
기술이며, 더 자세한 최신 변형을 이해하기 전에 

307
00:16:24,455 --> 00:16:27,800
이해해야 하며, 몇 가지 흥미로운 문제를 해결할 

308
00:16:27,800 --> 00:16:31,022
수 있는 것은 분명하지만 숨겨진 계층이 실제로 

309
00:16:31,022 --> 00:16:34,740
무엇을 하는지 파헤칠수록 지능이 떨어지는 것 같습니다.

310
00:16:38,480 --> 00:16:41,190
네트워크가 학습하는 방식에서 사용자가 학습하는 

311
00:16:41,190 --> 00:16:42,963
방식으로 잠시 초점을 옮기면, 

312
00:16:42,963 --> 00:16:45,674
어떤 식으로든 이 자료에 적극적으로 참여해야만 

313
00:16:45,674 --> 00:16:46,300
가능합니다.

314
00:16:47,060 --> 00:16:51,555
가장자리와 패턴 같은 것을 더 잘 포착하기 위해 

315
00:16:51,555 --> 00:16:55,385
이 시스템에 어떤 변화를 줄 수 있는지, 

316
00:16:55,385 --> 00:16:59,547
이미지를 어떻게 인식하는지 잠시 멈춰서 깊이 

317
00:16:59,547 --> 00:17:00,880
생각해 보세요.

318
00:17:01,480 --> 00:17:05,514
하지만 그보다 더 좋은 방법은 딥러닝과 신경망에 

319
00:17:05,514 --> 00:17:09,099
관한 마이클 닐슨의 책을 추천하는 것입니다.

320
00:17:09,680 --> 00:17:12,414
이 책에서는 이 예제를 위해 다운로드하여 

321
00:17:12,414 --> 00:17:15,149
사용할 코드와 데이터를 찾을 수 있으며, 

322
00:17:15,149 --> 00:17:18,359
해당 코드가 수행하는 작업을 단계별로 안내합니다.

323
00:17:19,300 --> 00:17:21,996
이 책은 무료로 공개되어 있으므로, 

324
00:17:21,996 --> 00:17:24,558
이 책을 통해 무언가를 얻으셨다면 

325
00:17:24,558 --> 00:17:27,660
닐슨의 노력에 기부하는 데 동참해 보세요.

326
00:17:27,660 --> 00:17:30,399
설명에 크리스 올라의 경이롭고 아름다운 

327
00:17:30,399 --> 00:17:33,138
블로그 게시물과 디스틸의 기사 등 제가 

328
00:17:33,138 --> 00:17:36,500
좋아하는 다른 리소스도 몇 개 링크해 두었습니다.

329
00:17:38,280 --> 00:17:39,840
마지막 몇 분 동안의 이야기를 

330
00:17:39,840 --> 00:17:41,584
마무리하기 위해 제가 레이샤 리와 

331
00:17:41,584 --> 00:17:43,880
나눈 인터뷰의 일부를 다시 소개해드리겠습니다.

332
00:17:44,300 --> 00:17:45,830
지난 영상에서 딥러닝으로 박사 

333
00:17:45,830 --> 00:17:47,720
학위를 취득한 그녀를 기억하실 겁니다.

334
00:17:48,300 --> 00:17:50,407
이 짧은 글에서는 최신 이미지 인식 

335
00:17:50,407 --> 00:17:52,935
네트워크가 실제로 어떻게 학습하는지에 대해 

336
00:17:52,935 --> 00:17:55,780
자세히 설명하는 두 편의 최근 논문을 소개합니다.

337
00:17:56,120 --> 00:18:00,186
첫 번째 논문에서는 이미지 인식에 매우 능숙한 심층 

338
00:18:00,186 --> 00:18:04,393
신경망 중 하나를 가져와 라벨이 제대로 지정된 데이터 

339
00:18:04,393 --> 00:18:07,758
세트에서 훈련하는 대신 모든 라벨을 뒤섞어 

340
00:18:07,758 --> 00:18:08,740
훈련했습니다.

341
00:18:09,480 --> 00:18:12,013
모든 것이 무작위로 레이블이 지정되었기 

342
00:18:12,013 --> 00:18:15,007
때문에 테스트 정확도는 무작위보다 떨어지지만, 

343
00:18:15,007 --> 00:18:17,425
그래도 제대로 레이블이 지정된 데이터 

344
00:18:17,425 --> 00:18:20,880
세트에서와 동일한 학습 정확도를 달성할 수 있었습니다.

345
00:18:21,600 --> 00:18:24,464
기본적으로 이 특정 네트워크의 수백만 개의 

346
00:18:24,464 --> 00:18:27,567
가중치는 무작위 데이터를 암기하는 데 충분했기 

347
00:18:27,567 --> 00:18:30,193
때문에 이 비용 함수를 최소화하는 것이 

348
00:18:30,193 --> 00:18:32,938
실제로 이미지의 어떤 구조에 해당하는지, 

349
00:18:32,938 --> 00:18:36,400
아니면 그냥 암기하는 것인지에 대한 의문이 생깁니다.

350
00:18:51,440 --> 00:18:56,082
정확도 곡선을 보면, 무작위 데이터 세트로 

351
00:18:56,082 --> 00:19:01,306
훈련하는 경우 이 곡선은 거의 선형적인 방식으로 

352
00:19:01,306 --> 00:19:06,336
매우 느리게 내려가므로 정확도를 얻을 수 있는 

353
00:19:06,336 --> 00:19:12,140
적절한 가중치를 찾기 위해 정말 고군분투하고 있습니다.

354
00:19:12,240 --> 00:19:16,200
반면에 실제로 올바른 레이블이 있는 구조화된 데이터 

355
00:19:16,200 --> 00:19:20,025
집합으로 학습하는 경우 처음에는 조금 더듬거리다가 

356
00:19:20,025 --> 00:19:23,985
정확도 수준에 도달하기 위해 매우 빠르게 떨어지므로 

357
00:19:23,985 --> 00:19:27,400
어떤 의미에서는 로컬 최대값을 찾는 것이 더 

358
00:19:27,400 --> 00:19:28,220
쉬웠습니다.

359
00:19:28,540 --> 00:19:32,061
그래서 그것에 대해 흥미로운 점은 실제로 몇 

360
00:19:32,061 --> 00:19:35,724
년 전에 나온 또 다른 논문이 네트워크 계층에 

361
00:19:35,724 --> 00:19:39,528
대해 훨씬 더 단순화되어 있지만 결과 중 하나는 

362
00:19:39,528 --> 00:19:43,190
최적화 환경을 보면 이러한 네트워크가 학습하는 

363
00:19:43,190 --> 00:19:47,135
경향이있는 로컬 최소값이 실제로 동일한 품질이므로 

364
00:19:47,135 --> 00:19:50,798
어떤 의미에서 데이터 세트가 구조화되어 있으면 

365
00:19:50,798 --> 00:19:54,320
훨씬 더 쉽게 찾을 수 있어야한다는 것입니다.

366
00:19:58,160 --> 00:19:59,629
언제나 그렇듯이 Patreon을 

367
00:19:59,629 --> 00:20:01,180
후원해 주시는 분들께 감사드립니다.

368
00:20:01,520 --> 00:20:03,704
이전에도 Patreon이 얼마나 획기적인지 

369
00:20:03,704 --> 00:20:06,344
말씀드렸지만, 이 동영상은 여러분 없이는 불가능했을 

370
00:20:06,344 --> 00:20:06,800
것입니다.

371
00:20:07,460 --> 00:20:09,233
또한 이 시리즈의 첫 번째 동영상을 지원해 

372
00:20:09,233 --> 00:20:11,228
준 VC 회사인 Amplify Partners에 

373
00:20:11,228 --> 00:20:12,780
특별히 감사의 말씀을 전하고 싶습니다.

