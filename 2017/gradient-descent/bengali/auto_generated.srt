1
00:00:04,180 --> 00:00:07,280
শেষ ভিডিওতে আমি একটি নিউরাল নেটওয়ার্কের গঠন তুলে ধরেছি।

2
00:00:07,680 --> 00:00:10,367
আমি এখানে একটি দ্রুত সংক্ষিপ্ত বিবরণ দেব যাতে এটি আমাদের মনে তাজা হয়, 

3
00:00:10,367 --> 00:00:12,600
এবং তারপরে এই ভিডিওটির জন্য আমার দুটি প্রধান লক্ষ্য রয়েছে৷

4
00:00:13,100 --> 00:00:15,809
প্রথমটি হল গ্রেডিয়েন্ট ডিসেন্টের ধারণাটি প্রবর্তন করা, 

5
00:00:15,809 --> 00:00:18,470
যা শুধুমাত্র নিউরাল নেটওয়ার্কগুলি কীভাবে শেখে তা নয়, 

6
00:00:18,470 --> 00:00:20,600
অন্যান্য অনেক মেশিন লার্নিংও কীভাবে কাজ করে।

7
00:00:21,120 --> 00:00:24,652
তারপরে এর পরে আমরা এই নির্দিষ্ট নেটওয়ার্কটি কীভাবে কাজ করে এবং নিউরনের 

8
00:00:24,652 --> 00:00:27,940
সেই লুকানো স্তরগুলি কী সন্ধান করে তা নিয়ে আমরা আরও কিছুটা খনন করব।

9
00:00:28,979 --> 00:00:32,738
একটি অনুস্মারক হিসাবে, এখানে আমাদের লক্ষ্য হ'ল হস্তলিখিত অঙ্কের 

10
00:00:32,738 --> 00:00:36,220
স্বীকৃতির ক্লাসিক উদাহরণ, নিউরাল নেটওয়ার্কের হ্যালো ওয়ার্ল্ড।

11
00:00:37,020 --> 00:00:40,277
এই সংখ্যাগুলি একটি 28x28 পিক্সেল গ্রিডে রেন্ডার করা হয়, 

12
00:00:40,277 --> 00:00:43,420
প্রতিটি পিক্সেল 0 এবং 1 এর মধ্যে কিছু গ্রেস্কেল মান সহ।

13
00:00:43,820 --> 00:00:50,040
এগুলিই নেটওয়ার্কের ইনপুট স্তরে 784 নিউরনের সক্রিয়তা নির্ধারণ করে।

14
00:00:51,180 --> 00:00:56,087
এবং তারপরে নিম্নলিখিত স্তরগুলিতে প্রতিটি নিউরনের সক্রিয়করণ পূর্ববর্তী স্তরের সমস্ত 

15
00:00:56,087 --> 00:01:00,820
সক্রিয়করণের ওজনযুক্ত যোগফলের উপর ভিত্তি করে, এবং কিছু বিশেষ সংখ্যাকে বায়াস বলে।

16
00:01:02,160 --> 00:01:05,051
তারপরে আপনি সেই যোগফলটি অন্য কিছু ফাংশন দিয়ে রচনা করবেন, 

17
00:01:05,051 --> 00:01:08,940
যেমন সিগমায়েড স্কুইশিফিকেশন, বা একটি রেলু, যেভাবে আমি শেষ ভিডিওটি দিয়েছিলাম।

18
00:01:09,480 --> 00:01:14,796
মোট, 16টি নিউরন সহ দুটি লুকানো স্তরের কিছুটা স্বেচ্ছাচারী পছন্দ দেওয়া হলে, 

19
00:01:14,796 --> 00:01:19,693
নেটওয়ার্কটিতে প্রায় 13,000 ওজন এবং পক্ষপাত রয়েছে যা আমরা সামঞ্জস্য 

20
00:01:19,693 --> 00:01:24,380
করতে পারি এবং এই মানগুলিই নির্ধারণ করে যে নেটওয়ার্কটি আসলে কী করে।

21
00:01:24,880 --> 00:01:27,671
তারপরে আমরা যখন বলি যে এই নেটওয়ার্কটি একটি প্রদত্ত সংখ্যাকে 

22
00:01:27,671 --> 00:01:30,417
শ্রেণিবদ্ধ করে তখন আমরা যা বোঝায় তা হল চূড়ান্ত স্তরের সেই 

23
00:01:30,417 --> 00:01:33,300
10টি নিউরনের মধ্যে সবচেয়ে উজ্জ্বলটি সেই অঙ্কের সাথে মিলে যায়।

24
00:01:34,100 --> 00:01:39,059
এবং মনে রাখবেন, স্তরযুক্ত কাঠামোর জন্য এখানে আমাদের মনে যে অনুপ্রেরণা ছিল তা হল যে 

25
00:01:39,059 --> 00:01:43,840
সম্ভবত দ্বিতীয় স্তরটি প্রান্তে উঠতে পারে, এবং তৃতীয় স্তরটি লুপ এবং লাইনের মতো 

26
00:01:43,840 --> 00:01:48,800
প্যাটার্নে উঠতে পারে এবং শেষটি কেবল সেগুলিকে একত্রিত করতে পারে। অঙ্ক চিনতে নিদর্শন।

27
00:01:49,800 --> 00:01:52,240
তাই এখানে, আমরা জানবো কিভাবে নেটওয়ার্ক শেখে।

28
00:01:52,640 --> 00:01:56,870
আমরা যা চাই তা হল একটি অ্যালগরিদম যেখানে আপনি এই নেটওয়ার্কটিকে প্রশিক্ষণের 

29
00:01:56,870 --> 00:02:01,101
ডেটার একটি সম্পূর্ণ গুচ্ছ দেখাতে পারেন, যা হাতে লেখা অঙ্কের বিভিন্ন চিত্রের 

30
00:02:01,101 --> 00:02:03,996
একটি গুচ্ছ আকারে আসে, সাথে লেবেলগুলি যা হওয়ার কথা, 

31
00:02:03,996 --> 00:02:08,449
এবং এটি হবে সেই 13,000 ওজন এবং পক্ষপাতগুলি সামঞ্জস্য করুন যাতে প্রশিক্ষণ ডেটাতে 

32
00:02:08,449 --> 00:02:10,120
এর কার্যকারিতা উন্নত করা যায়।

33
00:02:10,720 --> 00:02:13,707
আশা করি, এই স্তরযুক্ত কাঠামোর অর্থ হবে যে এটি যা শেখে 

34
00:02:13,707 --> 00:02:16,860
তা সেই প্রশিক্ষণ ডেটার বাইরের চিত্রগুলিতে সাধারণীকরণ করে।

35
00:02:17,640 --> 00:02:20,758
আমরা যেভাবে পরীক্ষা করি তা হল যে আপনি নেটওয়ার্ককে প্রশিক্ষণ দেওয়ার পরে, 

36
00:02:20,758 --> 00:02:23,665
আপনি এটিকে আরও লেবেলযুক্ত ডেটা দেখান যা এটি আগে কখনও দেখা যায়নি এবং 

37
00:02:23,665 --> 00:02:26,700
আপনি দেখতে পান যে এটি সেই নতুন চিত্রগুলিকে কতটা সঠিকভাবে শ্রেণীবদ্ধ করে৷

38
00:02:31,120 --> 00:02:35,736
সৌভাগ্যবশত আমাদের জন্য, এবং এটির সাথে শুরু করার মতো একটি সাধারণ উদাহরণ যা এটিকে তৈরি করে, 

39
00:02:35,736 --> 00:02:40,045
তা হল MNIST ডাটাবেসের পিছনে থাকা ভাল লোকেরা হাজার হাজার হাতে লেখা অঙ্কের চিত্রগুলির 

40
00:02:40,045 --> 00:02:44,200
একটি সংগ্রহ করেছে, প্রতিটিতে তাদের অনুমিত সংখ্যাগুলি দিয়ে লেবেল করা হয়েছে থাকা.

41
00:02:44,900 --> 00:02:47,735
এবং একটি মেশিনকে শেখার মতো বর্ণনা করা যতটা উত্তেজক, 

42
00:02:47,735 --> 00:02:51,117
একবার আপনি এটি কীভাবে কাজ করে তা একবার দেখলে, এটি কিছু উন্মাদ 

43
00:02:51,117 --> 00:02:55,480
সাই-ফাই প্রিমাইজের মতো অনেক কম এবং ক্যালকুলাস অনুশীলনের মতো অনেক বেশি অনুভব করে।

44
00:02:56,200 --> 00:02:59,960
আমি বলতে চাচ্ছি, মূলত এটি একটি নির্দিষ্ট ফাংশনের সর্বনিম্ন খুঁজে বের করার জন্য নেমে আসে।

45
00:03:01,939 --> 00:03:07,290
মনে রাখবেন, ধারণাগতভাবে, আমরা প্রতিটি নিউরনকে পূর্ববর্তী স্তরের সমস্ত নিউরনের সাথে 

46
00:03:07,290 --> 00:03:12,835
সংযুক্ত হিসাবে ভাবছি, এবং এর সক্রিয়করণকে সংজ্ঞায়িত করে ওজনযুক্ত সমষ্টির ওজনগুলি সেই 

47
00:03:12,835 --> 00:03:18,637
সংযোগগুলির শক্তির মতো, এবং পক্ষপাত হল কিছু ইঙ্গিত যে নিউরন সক্রিয় বা নিষ্ক্রিয় হতে থাকে 

48
00:03:18,637 --> 00:03:18,960
কিনা।

49
00:03:19,720 --> 00:03:22,285
এবং জিনিসগুলি শুরু করার জন্য, আমরা কেবলমাত্র এলোমেলোভাবে 

50
00:03:22,285 --> 00:03:24,400
সেই সমস্ত ওজন এবং পক্ষপাতগুলি শুরু করতে যাচ্ছি।

51
00:03:24,940 --> 00:03:27,716
বলা বাহুল্য, এই নেটওয়ার্কটি প্রদত্ত প্রশিক্ষণের উদাহরণে বেশ 

52
00:03:27,716 --> 00:03:30,720
ভয়ঙ্করভাবে পারফর্ম করতে চলেছে, যেহেতু এটি কেবল এলোমেলো কিছু করছে।

53
00:03:31,040 --> 00:03:33,530
উদাহরণস্বরূপ, আপনি একটি 3 এর এই চিত্রটিতে ফিড করেন 

54
00:03:33,530 --> 00:03:36,020
এবং আউটপুট স্তরটি কেবল একটি জগাখিচুড়ির মতো দেখায়।

55
00:03:36,600 --> 00:03:41,781
সুতরাং আপনি যা করেন তা হল একটি খরচ ফাংশন সংজ্ঞায়িত করা, কম্পিউটারকে বলার একটি উপায়, 

56
00:03:41,781 --> 00:03:46,602
না, খারাপ কম্পিউটার, সেই আউটপুটে সক্রিয় হওয়া উচিত যা বেশিরভাগ নিউরনের জন্য 0, 

57
00:03:46,602 --> 00:03:50,760
কিন্তু এই নিউরনের জন্য 1, আপনি আমাকে যা দিয়েছেন তা সম্পূর্ণ আবর্জনা।

58
00:03:51,720 --> 00:03:56,630
আরেকটু গাণিতিকভাবে বলতে গেলে, আপনি সেই ট্র্যাশ আউটপুট অ্যাক্টিভেশনগুলির 

59
00:03:56,630 --> 00:04:01,541
প্রতিটির মধ্যে পার্থক্যের স্কোয়ার যোগ করুন এবং আপনি যে মানটি পেতে চান, 

60
00:04:01,541 --> 00:04:05,020
এবং এটিকে আমরা একটি একক প্রশিক্ষণ উদাহরণের খরচ বলব।

61
00:04:05,960 --> 00:04:11,118
লক্ষ্য করুন যখন নেটওয়ার্ক আত্মবিশ্বাসের সাথে চিত্রটিকে সঠিকভাবে শ্রেণীবদ্ধ করে তখন 

62
00:04:11,118 --> 00:04:16,399
এই যোগফলটি ছোট, কিন্তু যখন নেটওয়ার্কটি মনে হয় যে এটি কী করছে তা জানে না তখন এটি বড়।

63
00:04:18,640 --> 00:04:22,137
তাহলে আপনি যা করবেন তা হল আপনার হাতে থাকা হাজার হাজার 

64
00:04:22,137 --> 00:04:25,440
প্রশিক্ষণের উদাহরণগুলির সমস্ত গড় খরচ বিবেচনা করুন।

65
00:04:27,040 --> 00:04:30,006
এই গড় খরচ হল নেটওয়ার্ক কতটা খারাপ, এবং কম্পিউটার 

66
00:04:30,006 --> 00:04:32,740
কতটা খারাপ বোধ করা উচিত তার জন্য আমাদের পরিমাপ।

67
00:04:33,420 --> 00:04:34,600
এবং এটি একটি জটিল জিনিস।

68
00:04:35,040 --> 00:04:38,607
মনে রাখবেন কিভাবে নেটওয়ার্ক নিজেই মূলত একটি ফাংশন ছিল, 

69
00:04:38,607 --> 00:04:41,665
যেটি ইনপুট হিসাবে 784 সংখ্যা নেয়, পিক্সেল মান, 

70
00:04:41,665 --> 00:04:46,315
এবং তার আউটপুট হিসাবে 10টি সংখ্যা বের করে দেয় এবং এক অর্থে এটি এই সমস্ত 

71
00:04:46,315 --> 00:04:48,800
ওজন এবং পক্ষপাত দ্বারা প্যারামিটারাইজড?

72
00:04:49,500 --> 00:04:52,820
ভাল খরচ ফাংশন যে উপরে জটিলতা একটি স্তর.

73
00:04:53,100 --> 00:04:58,105
এটি সেই 13,000 বা তার বেশি ওজন এবং পক্ষপাতগুলিকে ইনপুট হিসাবে নেয় এবং সেই ওজন এবং 

74
00:04:58,105 --> 00:05:02,990
পক্ষপাতগুলি কতটা খারাপ তা বর্ণনা করে একটি একক সংখ্যা বের করে দেয় এবং এটি যেভাবে 

75
00:05:02,990 --> 00:05:08,236
সংজ্ঞায়িত করা হয়েছে তা নির্ভর করে সমস্ত হাজার হাজার প্রশিক্ষণ ডেটার উপর নেটওয়ার্কের 

76
00:05:08,236 --> 00:05:08,900
আচরণের উপর৷

77
00:05:09,520 --> 00:05:11,000
সেটা অনেক ভাবার বিষয়।

78
00:05:12,400 --> 00:05:15,820
কিন্তু শুধু কম্পিউটারকে বলা যে এটি কী একটি বাজে কাজ করছে তা খুব সহায়ক নয়।

79
00:05:16,220 --> 00:05:20,060
আপনি এটিকে বলতে চান কীভাবে সেই ওজন এবং পক্ষপাতগুলি পরিবর্তন করতে হয় যাতে এটি আরও ভাল হয়।

80
00:05:20,780 --> 00:05:24,964
এটিকে সহজ করার জন্য, 13,000 ইনপুট সহ একটি ফাংশন কল্পনা করার জন্য সংগ্রাম করার পরিবর্তে, 

81
00:05:24,964 --> 00:05:28,150
শুধুমাত্র একটি সাধারণ ফাংশন কল্পনা করুন যার একটি ইনপুট হিসাবে একটি 

82
00:05:28,150 --> 00:05:30,480
সংখ্যা এবং একটি আউটপুট হিসাবে একটি সংখ্যা রয়েছে৷

83
00:05:31,480 --> 00:05:35,300
আপনি কিভাবে একটি ইনপুট খুঁজে পাবেন যা এই ফাংশনের মানকে ছোট করে?

84
00:05:36,460 --> 00:05:41,252
ক্যালকুলাসের শিক্ষার্থীরা জানবে যে আপনি কখনও কখনও সেই ন্যূনতমটি স্পষ্টভাবে বের 

85
00:05:41,252 --> 00:05:45,074
করতে পারেন, তবে এটি সত্যিই জটিল ফাংশনের জন্য সবসময় সম্ভব নয়, 

86
00:05:45,074 --> 00:05:49,927
অবশ্যই আমাদের পাগল জটিল নিউরাল নেটওয়ার্ক খরচ ফাংশনের জন্য এই পরিস্থিতির 13,000 

87
00:05:49,927 --> 00:05:51,080
ইনপুট সংস্করণে নয়।

88
00:05:51,580 --> 00:05:55,117
একটি আরও নমনীয় কৌশল হ'ল যে কোনও ইনপুট থেকে শুরু করা এবং সেই 

89
00:05:55,117 --> 00:05:59,200
আউটপুটটিকে কম করার জন্য আপনার কোন দিকে পদক্ষেপ নেওয়া উচিত তা নির্ধারণ করা।

90
00:06:00,080 --> 00:06:04,261
বিশেষ করে, আপনি যেখানে আছেন সেই ফাংশনের ঢালটি যদি বের করতে পারেন, 

91
00:06:04,261 --> 00:06:09,900
তাহলে সেই ঢালটি ধনাত্মক হলে বাম দিকে সরান এবং সেই ঢালটি ঋণাত্মক হলে ইনপুটটি ডানদিকে সরান৷

92
00:06:11,960 --> 00:06:16,856
আপনি যদি এটি বারবার করেন, প্রতিটি বিন্দুতে নতুন ঢাল পরীক্ষা করে যথাযথ পদক্ষেপ নিচ্ছেন, 

93
00:06:16,856 --> 00:06:19,840
আপনি ফাংশনের কিছু স্থানীয় ন্যূনতম কাছে যেতে যাচ্ছেন।

94
00:06:20,640 --> 00:06:23,800
এখানে আপনার মনে হতে পারে একটি বল একটি পাহাড়ের নিচে গড়িয়ে যাচ্ছে।

95
00:06:24,620 --> 00:06:27,718
লক্ষ্য করুন, এমনকি এই সত্যিই সরলীকৃত একক ইনপুট ফাংশনের জন্য, 

96
00:06:27,718 --> 00:06:30,918
এমন অনেক সম্ভাব্য উপত্যকা রয়েছে যেখানে আপনি অবতরণ করতে পারেন, 

97
00:06:30,918 --> 00:06:33,914
আপনি কোন এলোমেলো ইনপুট থেকে শুরু করবেন তার উপর নির্ভর করে, 

98
00:06:33,914 --> 00:06:37,520
এবং আপনি যে স্থানীয় ন্যূনতম ভূমিতে অবতরণ করবেন তার কোনো নিশ্চয়তা নেই 

99
00:06:37,520 --> 00:06:39,400
সম্ভাব্য সর্বনিম্ন মান হবে খরচ ফাংশন.

100
00:06:40,220 --> 00:06:42,620
এটি আমাদের নিউরাল নেটওয়ার্ক কেসেও বহন করবে।

101
00:06:43,180 --> 00:06:48,026
আমি এও চাই যে আপনি লক্ষ্য করুন কিভাবে আপনি যদি আপনার ধাপের আকার ঢালের সমানুপাতিক করেন, 

102
00:06:48,026 --> 00:06:51,814
তাহলে ঢাল যখন ন্যূনতম দিকে সমতল হয়, তখন আপনার পদক্ষেপগুলি ছোট থেকে 

103
00:06:51,814 --> 00:06:54,600
ছোট হয় এবং এটি আপনাকে ওভারশুটিং থেকে সাহায্য করে।

104
00:06:55,940 --> 00:06:58,653
জটিলতাকে কিছুটা বাম্পিং করে, পরিবর্তে দুটি ইনপুট 

105
00:06:58,653 --> 00:07:00,980
এবং একটি আউটপুট সহ একটি ফাংশন কল্পনা করুন।

106
00:07:01,500 --> 00:07:04,694
আপনি ভাবতে পারেন ইনপুট স্থানটিকে xy-প্লেন, এবং খরচ 

107
00:07:04,694 --> 00:07:08,140
ফাংশনটিকে এটির উপরে একটি পৃষ্ঠ হিসাবে গ্রাফ করা হয়েছে।

108
00:07:08,760 --> 00:07:13,927
ফাংশনের ঢাল সম্পর্কে জিজ্ঞাসা করার পরিবর্তে, আপনাকে এই ইনপুট স্পেসে কোন দিকে 

109
00:07:13,927 --> 00:07:18,960
যেতে হবে তা জিজ্ঞাসা করতে হবে যাতে ফাংশনের আউটপুট সবচেয়ে দ্রুত হ্রাস পায়।

110
00:07:19,720 --> 00:07:21,760
অন্য কথায়, উতরাই দিক কি?

111
00:07:22,380 --> 00:07:25,560
আবার, সেই পাহাড়ের নিচে একটি বল গড়িয়ে পড়ার কথা ভাবা সহায়ক।

112
00:07:26,660 --> 00:07:30,720
আপনারা যারা মাল্টিভেরিয়েবল ক্যালকুলাসের সাথে পরিচিত তারা জানেন যে 

113
00:07:30,720 --> 00:07:35,083
একটি ফাংশনের গ্রেডিয়েন্ট আপনাকে সবচেয়ে খাড়া আরোহনের দিক নির্দেশ করে, 

114
00:07:35,083 --> 00:07:38,780
ফাংশনটি সবচেয়ে দ্রুত বাড়ানোর জন্য আপনাকে কোন দিকে যেতে হবে।

115
00:07:39,560 --> 00:07:42,877
স্বাভাবিকভাবেই যথেষ্ট, সেই গ্রেডিয়েন্টের নেতিবাচক গ্রহণ আপনাকে 

116
00:07:42,877 --> 00:07:46,040
পদক্ষেপের দিকনির্দেশ দেয় যা ফাংশনটি সবচেয়ে দ্রুত হ্রাস করে।

117
00:07:47,240 --> 00:07:50,755
তার থেকেও বেশি, এই গ্রেডিয়েন্ট ভেক্টরের দৈর্ঘ্য 

118
00:07:50,755 --> 00:07:53,840
সেই খাড়া ঢালটি কতটা খাড়া তার ইঙ্গিত দেয়।

119
00:07:54,540 --> 00:07:57,527
আপনি যদি মাল্টিভেরিয়েবল ক্যালকুলাসের সাথে অপরিচিত হন এবং আরও জানতে 

120
00:07:57,527 --> 00:08:00,340
চান তবে খান একাডেমির জন্য আমি এই বিষয়ে কিছু কাজ করেছি তা দেখুন।

121
00:08:00,860 --> 00:08:04,681
সত্যি বলতে কি, এই মুহূর্তে আপনার এবং আমার জন্য যা গুরুত্বপূর্ণ 

122
00:08:04,681 --> 00:08:08,199
তা হল নীতিগতভাবে এই ভেক্টরটি গণনা করার একটি উপায় রয়েছে, 

123
00:08:08,199 --> 00:08:11,900
এই ভেক্টর যা আপনাকে বলে যে উতরাই দিকটি কী এবং এটি কতটা খাড়া।

124
00:08:12,400 --> 00:08:16,120
আপনি ঠিক থাকবেন যদি আপনি শুধু জানেন এবং আপনি বিশদ বিবরণে শক্ত না হন।

125
00:08:17,200 --> 00:08:21,828
যদি আপনি এটি পেতে পারেন, ফাংশনটি মিনিমাইজ করার জন্য অ্যালগরিদম হল এই গ্রেডিয়েন্ট 

126
00:08:21,828 --> 00:08:26,740
দিকটি গণনা করা, তারপরে নিচের দিকে একটি ছোট পদক্ষেপ নিন এবং এটি বারবার পুনরাবৃত্তি করুন।

127
00:08:27,700 --> 00:08:32,820
এটি একটি ফাংশনের জন্য একই মৌলিক ধারণা যাতে 2টি ইনপুটের পরিবর্তে 13,000 ইনপুট রয়েছে।

128
00:08:33,400 --> 00:08:36,635
আমাদের নেটওয়ার্কের সমস্ত 13,000 ওজন এবং পক্ষপাতগুলিকে 

129
00:08:36,635 --> 00:08:39,460
একটি বিশাল কলাম ভেক্টরে সংগঠিত করার কল্পনা করুন।

130
00:08:40,140 --> 00:08:43,938
খরচ ফাংশনের নেতিবাচক গ্রেডিয়েন্ট হল একটি ভেক্টর, 

131
00:08:43,938 --> 00:08:48,801
এটি এই অত্যন্ত বিশাল ইনপুট স্পেসের ভিতরে কিছু দিক যা আপনাকে বলে 

132
00:08:48,801 --> 00:08:54,880
যে এই সমস্ত সংখ্যার সাথে কোনটি নাজেস খরচ ফাংশনে সবচেয়ে দ্রুত হ্রাস ঘটাতে চলেছে৷

133
00:08:55,640 --> 00:08:58,721
এবং অবশ্যই, আমাদের বিশেষভাবে ডিজাইন করা খরচ ফাংশন সহ, 

134
00:08:58,721 --> 00:09:03,800
ওজন এবং পক্ষপাতগুলিকে হ্রাস করার জন্য পরিবর্তন করার অর্থ হল প্রশিক্ষণ ডেটার প্রতিটি অংশে 

135
00:09:03,800 --> 00:09:08,366
নেটওয়ার্কের আউটপুটকে 10টি মানের এলোমেলো অ্যারের মতো দেখায় এবং আরও একটি বাস্তব 

136
00:09:08,366 --> 00:09:10,820
সিদ্ধান্তের মতো যা আমরা চাই৷ এটা তৈরি করতে।

137
00:09:11,440 --> 00:09:16,093
এটি মনে রাখা গুরুত্বপূর্ণ, এই খরচ ফাংশনটি প্রশিক্ষণের সমস্ত ডেটার উপর একটি গড় জড়িত, 

138
00:09:16,093 --> 00:09:20,476
তাই আপনি যদি এটিকে ছোট করেন তবে এর অর্থ হল এই সমস্ত নমুনার ক্ষেত্রে এটি একটি ভাল 

139
00:09:20,476 --> 00:09:21,180
পারফরম্যান্স।

140
00:09:23,820 --> 00:09:26,679
এই গ্রেডিয়েন্টটি দক্ষতার সাথে কম্পিউট করার অ্যালগরিদম, 

141
00:09:26,679 --> 00:09:29,946
যেটি কার্যকরভাবে একটি নিউরাল নেটওয়ার্ক কীভাবে শেখে তার হৃদয়কে 

142
00:09:29,946 --> 00:09:33,980
ব্যাকপ্রোপ্যাগেশন বলা হয়, এবং এটিই আমি পরবর্তী ভিডিও সম্পর্কে কথা বলতে যাচ্ছি।

143
00:09:34,660 --> 00:09:38,859
সেখানে, আমি প্রদত্ত প্রশিক্ষণ ডেটার জন্য প্রতিটি ওজন এবং পক্ষপাতের সাথে ঠিক কী 

144
00:09:38,859 --> 00:09:41,411
ঘটে তার মধ্য দিয়ে হাঁটতে সত্যিই সময় নিতে চাই, 

145
00:09:41,411 --> 00:09:45,611
প্রাসঙ্গিক ক্যালকুলাস এবং সূত্রের স্তূপের বাইরে কী ঘটছে তার জন্য একটি স্বজ্ঞাত 

146
00:09:45,611 --> 00:09:47,100
অনুভূতি দেওয়ার চেষ্টা করছি।

147
00:09:47,780 --> 00:09:50,824
এই মুহুর্তে, এই মুহূর্তে, আমি মূল জিনিসটি যা আপনি জানতে চাই, 

148
00:09:50,824 --> 00:09:54,317
বাস্তবায়নের বিশদ বিবরণ থেকে স্বাধীন, তা হল আমরা যখন নেটওয়ার্ক শেখার 

149
00:09:54,317 --> 00:09:58,360
বিষয়ে কথা বলি তখন আমরা যা বোঝায় তা হল এটি কেবলমাত্র একটি খরচ ফাংশন কমিয়ে দেয়।

150
00:09:59,300 --> 00:10:02,126
এবং লক্ষ্য করুন, এর একটি ফলাফল হল যে এই খরচ ফাংশনের জন্য একটি 

151
00:10:02,126 --> 00:10:05,181
সুন্দর মসৃণ আউটপুট থাকা গুরুত্বপূর্ণ, যাতে আমরা নীচের দিকে সামান্য 

152
00:10:05,181 --> 00:10:08,100
পদক্ষেপ নেওয়ার মাধ্যমে একটি স্থানীয় সর্বনিম্ন খুঁজে পেতে পারি।

153
00:10:09,260 --> 00:10:14,054
এই কারণেই, যাইহোক, কৃত্রিম নিউরনগুলি জৈবিক নিউরনগুলির মতো বাইনারি 

154
00:10:14,054 --> 00:10:19,140
উপায়ে সক্রিয় বা নিষ্ক্রিয় হওয়ার পরিবর্তে ক্রমাগত সক্রিয়তা রয়েছে।

155
00:10:20,220 --> 00:10:23,336
নেতিবাচক গ্রেডিয়েন্টের কিছু একাধিক দ্বারা একটি ফাংশনের একটি 

156
00:10:23,336 --> 00:10:26,760
ইনপুটকে বারবার নাজ করার এই প্রক্রিয়াটিকে গ্রেডিয়েন্ট ডিসেন্ট বলে।

157
00:10:27,300 --> 00:10:31,033
এটি কিছু স্থানীয় ন্যূনতম খরচ ফাংশনের দিকে একত্রিত হওয়ার একটি উপায়, 

158
00:10:31,033 --> 00:10:32,580
মূলত এই গ্রাফের একটি উপত্যকা।

159
00:10:33,440 --> 00:10:36,531
আমি এখনও দুটি ইনপুট সহ একটি ফাংশনের ছবি দেখাচ্ছি, অবশ্যই, 

160
00:10:36,531 --> 00:10:40,848
কারণ একটি 13,000 ডাইমেনশনাল ইনপুট স্পেসে নাজগুলি আপনার মনকে ঘিরে রাখা একটু কঠিন, 

161
00:10:40,848 --> 00:10:44,260
তবে এটি সম্পর্কে চিন্তা করার একটি চমৎকার অ-স্থানিক উপায় রয়েছে।

162
00:10:45,080 --> 00:10:48,440
ঋণাত্মক গ্রেডিয়েন্টের প্রতিটি উপাদান আমাদের দুটি জিনিস বলে।

163
00:10:49,060 --> 00:10:52,290
চিহ্নটি, অবশ্যই, ইনপুট ভেক্টরের সংশ্লিষ্ট উপাদানটি 

164
00:10:52,290 --> 00:10:55,140
উপরে বা নিচে নাজ করা উচিত কিনা তা আমাদের বলে।

165
00:10:55,800 --> 00:10:59,109
কিন্তু গুরুত্বপূর্ণভাবে, এই সমস্ত উপাদানগুলির আপেক্ষিক 

166
00:10:59,109 --> 00:11:02,720
মাত্রাগুলি আপনাকে বলে যে কোন পরিবর্তনগুলি বেশি গুরুত্বপূর্ণ।

167
00:11:05,220 --> 00:11:09,003
আপনি দেখতে পাচ্ছেন, আমাদের নেটওয়ার্কে, ওজনগুলির একটির সাথে সামঞ্জস্য অন্য 

168
00:11:09,003 --> 00:11:13,040
কিছু ওজনের সাথে সামঞ্জস্যের চেয়ে ব্যয় ফাংশনের উপর অনেক বেশি প্রভাব ফেলতে পারে।

169
00:11:14,800 --> 00:11:18,200
এই সংযোগগুলির মধ্যে কিছু আমাদের প্রশিক্ষণ ডেটার জন্য আরও গুরুত্বপূর্ণ।

170
00:11:19,320 --> 00:11:23,747
সুতরাং আপনি আমাদের মন-warpingly বিশাল খরচ ফাংশন এই গ্রেডিয়েন্ট ভেক্টর সম্পর্কে চিন্তা 

171
00:11:23,747 --> 00:11:28,124
করতে পারেন একটি উপায় হল যে এটি প্রতিটি ওজন এবং পক্ষপাতের আপেক্ষিক গুরুত্ব এনকোড করে, 

172
00:11:28,124 --> 00:11:32,400
অর্থাৎ, এই পরিবর্তনগুলির মধ্যে কোনটি আপনার অর্থের জন্য সবচেয়ে বেশি ধাক্কা বহন করবে।

173
00:11:33,620 --> 00:11:36,640
এটি সত্যিই দিক সম্পর্কে চিন্তা করার অন্য উপায়।

174
00:11:37,100 --> 00:11:42,023
একটি সহজ উদাহরণ নেওয়ার জন্য, আপনার যদি ইনপুট হিসাবে দুটি ভেরিয়েবল সহ কিছু ফাংশন 

175
00:11:42,023 --> 00:11:47,188
থাকে এবং আপনি গণনা করেন যে কোনও নির্দিষ্ট বিন্দুতে এর গ্রেডিয়েন্ট 3,1 হিসাবে বেরিয়ে 

176
00:11:47,188 --> 00:11:51,031
আসে, তবে একদিকে আপনি এটিকে ব্যাখ্যা করতে পারেন যে যখন আপনি' 

177
00:11:51,031 --> 00:11:55,594
আবার সেই ইনপুটে দাঁড়িয়ে, এই দিক বরাবর চললে ফাংশনটি খুব দ্রুত বৃদ্ধি পায়, 

178
00:11:55,594 --> 00:11:59,197
যে আপনি যখন ইনপুট পয়েন্টের সমতলের উপরে ফাংশনটি গ্রাফ করেন, 

179
00:11:59,197 --> 00:12:02,260
তখন সেই ভেক্টরটি আপনাকে সোজা চড়াই দিক নির্দেশ করে।

180
00:12:02,860 --> 00:12:07,581
কিন্তু পড়ার আরেকটি উপায় হল যে এই প্রথম ভেরিয়েবলের পরিবর্তনগুলি দ্বিতীয় 

181
00:12:07,581 --> 00:12:11,170
ভেরিয়েবলের পরিবর্তনের চেয়ে 3 গুণ বেশি গুরুত্ব বহন করে, 

182
00:12:11,170 --> 00:12:16,711
যে অন্তত প্রাসঙ্গিক ইনপুটের আশেপাশে, x-মানকে নজ করা আপনার জন্য অনেক বেশি ধাক্কা বহন করে।

183
00:12:16,711 --> 00:12:16,900
 বক

184
00:12:19,880 --> 00:12:22,340
আসুন জুম আউট করুন এবং আমরা এখন পর্যন্ত কোথায় আছি তা যোগ করি।

185
00:12:22,840 --> 00:12:26,614
নেটওয়ার্ক নিজেই 784 ইনপুট এবং 10 আউটপুট সহ এই ফাংশন, 

186
00:12:26,614 --> 00:12:30,040
এই সমস্ত ওজনযুক্ত রাশির পরিপ্রেক্ষিতে সংজ্ঞায়িত।

187
00:12:30,640 --> 00:12:33,680
খরচ ফাংশন যে উপরে জটিলতা একটি স্তর.

188
00:12:33,980 --> 00:12:38,057
এটি ইনপুট হিসাবে 13,000 ওজন এবং পক্ষপাত নেয় এবং প্রশিক্ষণ 

189
00:12:38,057 --> 00:12:41,720
উদাহরণগুলির উপর ভিত্তি করে একটি একক পরিমাপ থুতু দেয়।

190
00:12:42,440 --> 00:12:46,900
এবং খরচ ফাংশনের গ্রেডিয়েন্ট এখনও জটিলতার আরও একটি স্তর।

191
00:12:47,360 --> 00:12:52,770
এটি আমাদের বলে যে এই সমস্ত ওজন এবং পক্ষপাতের সাথে কোন বিষয়গুলি খরচ ফাংশনের মানতে দ্রুততম 

192
00:12:52,770 --> 00:12:57,880
পরিবর্তন ঘটায়, যা আপনি ব্যাখ্যা করতে পারেন যে কোন পরিবর্তনগুলি সবচেয়ে গুরুত্বপূর্ণ।

193
00:13:02,560 --> 00:13:06,041
সুতরাং, যখন আপনি এলোমেলো ওজন এবং পক্ষপাত সহ নেটওয়ার্কটি শুরু করেন এবং 

194
00:13:06,041 --> 00:13:10,160
এই গ্রেডিয়েন্ট ডিসেন্ট প্রক্রিয়ার উপর ভিত্তি করে সেগুলিকে অনেকবার সামঞ্জস্য করেন, 

195
00:13:10,160 --> 00:13:13,200
এটি আগে কখনও দেখা যায়নি এমন চিত্রগুলিতে এটি কতটা ভাল কাজ করে?

196
00:13:14,100 --> 00:13:18,508
আমি এখানে যেটি বর্ণনা করেছি, প্রতিটি 16টি নিউরনের দুটি লুকানো স্তর সহ, 

197
00:13:18,508 --> 00:13:22,234
বেশিরভাগই নান্দনিক কারণে বেছে নেওয়া হয়েছে, এটি খারাপ নয়, 

198
00:13:22,234 --> 00:13:25,960
এটি সঠিকভাবে দেখা নতুন চিত্রগুলির প্রায় 96% শ্রেণীবদ্ধ করে৷

199
00:13:26,680 --> 00:13:30,258
এবং সত্যই, আপনি যদি কিছু উদাহরণের দিকে তাকান যা এটিকে বিভ্রান্ত করে, 

200
00:13:30,258 --> 00:13:32,540
আপনি এটিকে কিছুটা শিথিল করতে বাধ্য বোধ করেন।

201
00:13:36,220 --> 00:13:40,038
এখন আপনি যদি লুকানো স্তর কাঠামোর সাথে খেলা করেন এবং কয়েকটি টুইক করেন, 

202
00:13:40,038 --> 00:13:41,760
আপনি এটি 98% পর্যন্ত পেতে পারেন।

203
00:13:41,760 --> 00:13:42,720
এবং যে বেশ ভাল!

204
00:13:43,020 --> 00:13:47,542
এটি সর্বোত্তম নয়, আপনি অবশ্যই এই প্লেইন ভ্যানিলা নেটওয়ার্কের চেয়ে আরও পরিশীলিত হয়ে 

205
00:13:47,542 --> 00:13:51,804
আরও ভাল পারফরম্যান্স পেতে পারেন, তবে প্রাথমিক কাজটি কতটা দুঃসাধ্য তা বিবেচনা করে, 

206
00:13:51,804 --> 00:13:56,378
আমি মনে করি যে কোনও নেটওয়ার্ক ইমেজগুলিতে এটি ভালভাবে কাজ করে তার মধ্যে অবিশ্বাস্য কিছু 

207
00:13:56,378 --> 00:14:00,952
আছে যা আগে কখনও দেখা যায়নি। আমরা কখনই এটি নির্দিষ্টভাবে বলিনি যে কী নিদর্শনগুলি সন্ধান 

208
00:14:00,952 --> 00:14:01,420
করতে হবে।

209
00:14:02,560 --> 00:14:07,414
মূলত, আমি যেভাবে এই কাঠামোটিকে অনুপ্রাণিত করেছি তা হল আমাদের একটি আশার বর্ণনা দিয়ে, 

210
00:14:07,414 --> 00:14:12,325
যে দ্বিতীয় স্তরটি ছোট প্রান্তে উঠতে পারে, যে তৃতীয় স্তরটি সেই প্রান্তগুলিকে একত্রিত 

211
00:14:12,325 --> 00:14:17,180
করে লুপ এবং লম্বা লাইন চিনতে পারে এবং সেগুলি টুকরো টুকরো হতে পারে। একসাথে অঙ্ক চিনতে.

212
00:14:17,960 --> 00:14:20,400
তাহলে কি এই আমাদের নেটওয়ার্ক আসলে কি করছে?

213
00:14:21,079 --> 00:14:24,400
ওয়েল, এই এক জন্য অন্তত, সব না.

214
00:14:24,820 --> 00:14:28,882
মনে রাখবেন কিভাবে আমরা শেষ ভিডিওতে দেখেছিলাম যে প্রথম স্তরের সমস্ত নিউরন থেকে 

215
00:14:28,882 --> 00:14:33,101
দ্বিতীয় স্তরের একটি প্রদত্ত নিউরনের সাথে সংযোগের ওজনগুলিকে একটি প্রদত্ত পিক্সেল 

216
00:14:33,101 --> 00:14:37,060
প্যাটার্ন হিসাবে কল্পনা করা যেতে পারে যেটি দ্বিতীয় স্তরের নিউরন গ্রহণ করছে?

217
00:14:37,780 --> 00:14:42,382
ঠিক আছে, যখন আমরা এই রূপান্তরগুলির সাথে যুক্ত ওজনের জন্য এটি করি, 

218
00:14:42,382 --> 00:14:47,682
প্রথম স্তর থেকে পরের স্তরে, এখানে এবং সেখানে বিচ্ছিন্ন ছোট প্রান্তগুলি বেছে 

219
00:14:47,682 --> 00:14:53,680
নেওয়ার পরিবর্তে, তারা দেখতে, ভাল, প্রায় এলোমেলো, কিছু খুব আলগা প্যাটার্ন সহ মাঝখানে।

220
00:14:53,760 --> 00:14:58,503
এটা মনে হবে যে সম্ভাব্য ওজন এবং পক্ষপাতের অকল্পনীয়ভাবে বিশাল 13,000 ডাইমেনশনাল স্পেসে, 

221
00:14:58,503 --> 00:15:02,060
আমাদের নেটওয়ার্ক নিজেকে একটি সুখী সামান্য স্থানীয় ন্যূনতম খুঁজে 

222
00:15:02,060 --> 00:15:05,456
পেয়েছে যেটি সফলভাবে বেশিরভাগ চিত্রকে শ্রেণীবদ্ধ করা সত্ত্বেও, 

223
00:15:05,456 --> 00:15:08,960
আমরা যে প্যাটার্নগুলির জন্য আশা করেছিলাম তা ঠিকভাবে গ্রহণ করে না।

224
00:15:09,780 --> 00:15:13,820
এবং সত্যিই এই পয়েন্ট বাড়িতে চালাতে, আপনি একটি এলোমেলো চিত্র ইনপুট যখন কি ঘটতে দেখুন.

225
00:15:14,320 --> 00:15:18,691
যদি সিস্টেমটি স্মার্ট হয়, তাহলে আপনি এটিকে অনিশ্চিত বোধ করতে আশা করতে পারেন, 

226
00:15:18,691 --> 00:15:23,735
হয়তো সত্যিই সেই 10টি আউটপুট নিউরনগুলির একটিকে সক্রিয় করছে না বা তাদের সবগুলিকে সমানভাবে 

227
00:15:23,735 --> 00:15:28,499
সক্রিয় করছে না, কিন্তু পরিবর্তে এটি আত্মবিশ্বাসের সাথে আপনাকে কিছু বাজে উত্তর দেয়, 

228
00:15:28,499 --> 00:15:33,431
যেন এটি নিশ্চিত মনে হয় যে এই এলোমেলো শব্দ একটি 5 যেমন এটি করে যে একটি 5 এর একটি প্রকৃত 

229
00:15:33,431 --> 00:15:34,160
চিত্র একটি 5।

230
00:15:34,540 --> 00:15:38,438
ভিন্নভাবে বাক্যাংশ, এমনকি যদি এই নেটওয়ার্কটি অঙ্কগুলিকে খুব ভালভাবে চিনতে পারে, 

231
00:15:38,438 --> 00:15:40,700
তবে সেগুলি কীভাবে আঁকতে হয় তার কোনও ধারণা নেই।

232
00:15:41,420 --> 00:15:45,240
এর অনেক কিছু কারণ এটি একটি শক্তভাবে সীমাবদ্ধ প্রশিক্ষণ সেটআপ।

233
00:15:45,880 --> 00:15:47,740
আমি বলতে চাচ্ছি, এখানে নেটওয়ার্কের জুতা মধ্যে নিজেকে রাখা.

234
00:15:48,140 --> 00:15:52,240
এর দৃষ্টিকোণ থেকে, সমগ্র মহাবিশ্ব একটি ক্ষুদ্র গ্রিডে কেন্দ্রীভূত স্পষ্টভাবে 

235
00:15:52,240 --> 00:15:55,541
সংজ্ঞায়িত অপরিবর্তনীয় সংখ্যা ছাড়া আর কিছুই নিয়ে গঠিত নয়, 

236
00:15:55,541 --> 00:15:59,908
এবং এর খরচ ফাংশন এটিকে তার সিদ্ধান্তে সম্পূর্ণ আত্মবিশ্বাস ছাড়া অন্য কিছু হওয়ার 

237
00:15:59,908 --> 00:16:01,080
জন্য কোন উৎসাহ দেয়নি।

238
00:16:02,120 --> 00:16:05,230
সুতরাং এই দ্বিতীয় স্তরের নিউরনগুলি আসলে কী করছে তার চিত্র হিসাবে, 

239
00:16:05,230 --> 00:16:08,898
আপনি ভাবতে পারেন কেন আমি প্রান্ত এবং নিদর্শনগুলি বেছে নেওয়ার প্রেরণা দিয়ে এই 

240
00:16:08,898 --> 00:16:09,920
নেটওয়ার্কটি চালু করব।

241
00:16:09,920 --> 00:16:12,300
আমি বলতে চাচ্ছি, যে এটা শেষ পর্যন্ত কি সব ঠিক না.

242
00:16:13,380 --> 00:16:17,180
ঠিক আছে, এটি আমাদের শেষ লক্ষ্য নয়, বরং একটি সূচনা বিন্দু।

243
00:16:17,640 --> 00:16:22,610
সত্যি বলতে কি, এটি পুরানো প্রযুক্তি, 80 এবং 90 এর দশকে যে ধরনের গবেষণা করা হয়েছিল, 

244
00:16:22,610 --> 00:16:26,870
এবং আপনি আরও বিশদ আধুনিক রূপগুলি বোঝার আগে আপনাকে এটি বুঝতে হবে এবং এটি 

245
00:16:26,870 --> 00:16:29,828
স্পষ্টতই কিছু আকর্ষণীয় সমস্যা সমাধান করতে সক্ষম, 

246
00:16:29,828 --> 00:16:34,740
তবে আপনি যত বেশি খনন করবেন যারা লুকানো স্তর সত্যিই করছেন, কম বুদ্ধিমান এটা মনে হয়.

247
00:16:38,480 --> 00:16:41,985
আপনি কীভাবে শেখেন নেটওয়ার্কগুলি কীভাবে শেখে তার থেকে এক মুহুর্তের জন্য ফোকাস 

248
00:16:41,985 --> 00:16:45,895
স্থানান্তরিত করা, এটি কেবল তখনই ঘটবে যদি আপনি কোনওভাবে এখানে উপাদানের সাথে সক্রিয়ভাবে 

249
00:16:45,895 --> 00:16:46,300
জড়িত হন।

250
00:16:47,060 --> 00:16:50,414
একটি খুব সহজ জিনিস যা আমি আপনাকে করতে চাই তা হল এখনই বিরতি দিন এবং 

251
00:16:50,414 --> 00:16:53,669
আপনি এই সিস্টেমে কী পরিবর্তন করতে পারেন এবং আপনি যদি প্রান্ত এবং 

252
00:16:53,669 --> 00:16:56,924
নিদর্শনগুলির মতো জিনিসগুলিকে আরও ভালভাবে নিতে চান তবে এটি কীভাবে 

253
00:16:56,924 --> 00:17:00,880
চিত্রগুলিকে উপলব্ধি করতে পারে সে সম্পর্কে কিছুক্ষণের জন্য গভীরভাবে চিন্তা করুন৷

254
00:17:01,479 --> 00:17:04,948
কিন্তু তার চেয়েও ভালো, আসলে উপাদানের সাথে জড়িত থাকার জন্য, 

255
00:17:04,948 --> 00:17:09,099
আমি গভীর শিক্ষা এবং নিউরাল নেটওয়ার্কে মাইকেল নিলসনের বইটির সুপারিশ করছি।

256
00:17:09,680 --> 00:17:14,911
এটিতে, আপনি এই সঠিক উদাহরণের জন্য ডাউনলোড এবং খেলার জন্য কোড এবং ডেটা খুঁজে পেতে পারেন, 

257
00:17:14,911 --> 00:17:18,359
এবং বইটি আপনাকে ধাপে ধাপে সেই কোডটি কী করছে তা নিয়ে যাবে।

258
00:17:19,300 --> 00:17:22,332
কি আশ্চর্যজনক বিষয় হল যে এই বইটি বিনামূল্যে এবং সর্বজনীনভাবে উপলব্ধ, 

259
00:17:22,332 --> 00:17:25,104
তাই আপনি যদি এটি থেকে কিছু পেতে পারেন, তাহলে নিলসনের প্রচেষ্টার 

260
00:17:25,104 --> 00:17:27,660
প্রতি দান করার জন্য আমার সাথে যোগদান করার কথা বিবেচনা করুন৷

261
00:17:27,660 --> 00:17:32,347
আমি ক্রিস ওলার অসাধারণ এবং সুন্দর ব্লগ পোস্ট এবং ডিস্টিলের নিবন্ধগুলি 

262
00:17:32,347 --> 00:17:36,500
সহ বর্ণনায় আমার অনেক পছন্দের আরও কয়েকটি সংস্থান লিঙ্ক করেছি।

263
00:17:38,280 --> 00:17:40,708
গত কয়েক মিনিটের জন্য জিনিসগুলি এখানে বন্ধ করতে, 

264
00:17:40,708 --> 00:17:43,880
আমি লিশা লির সাথে আমার সাক্ষাৎকারের একটি স্নিপেটে ফিরে যেতে চাই।

265
00:17:44,300 --> 00:17:47,720
আপনি শেষ ভিডিও থেকে তাকে মনে রাখতে পারেন, তিনি গভীর শিক্ষায় তার পিএইচডি কাজ করেছেন।

266
00:17:48,300 --> 00:17:51,896
এই ছোট্ট স্নিপেটে তিনি দুটি সাম্প্রতিক কাগজপত্র সম্পর্কে কথা বলেছেন যেগুলি 

267
00:17:51,896 --> 00:17:55,780
আসলেই আরও কিছু আধুনিক ইমেজ শনাক্তকরণ নেটওয়ার্কগুলি আসলে কীভাবে শিখছে তা খনন করে৷

268
00:17:56,120 --> 00:17:58,275
আমরা কথোপকথনে যেখানে ছিলাম তা সেট আপ করার জন্য, 

269
00:17:58,275 --> 00:18:01,599
প্রথম কাগজটি এই বিশেষভাবে গভীর নিউরাল নেটওয়ার্কগুলির মধ্যে একটি নিয়েছিল 

270
00:18:01,599 --> 00:18:04,787
যা চিত্র সনাক্তকরণে সত্যিই ভাল, এবং এটিকে সঠিকভাবে লেবেলযুক্ত ডেটাসেটে 

271
00:18:04,787 --> 00:18:08,740
প্রশিক্ষণ দেওয়ার পরিবর্তে, প্রশিক্ষণের আগে চারপাশের সমস্ত লেবেলগুলিকে এলোমেলো করে দেয়৷

272
00:18:09,480 --> 00:18:12,900
স্পষ্টতই এখানে পরীক্ষার নির্ভুলতা র্যান্ডম থেকে ভাল ছিল না, 

273
00:18:12,900 --> 00:18:16,662
যেহেতু সবকিছুই কেবল এলোমেলোভাবে লেবেলযুক্ত, তবে এটি এখনও সঠিকভাবে 

274
00:18:16,662 --> 00:18:20,880
লেবেলযুক্ত ডেটাসেটের মতো একই প্রশিক্ষণ নির্ভুলতা অর্জন করতে সক্ষম হয়েছিল।

275
00:18:21,600 --> 00:18:26,426
মূলত, এই নির্দিষ্ট নেটওয়ার্কের জন্য লক্ষ লক্ষ ওজন শুধুমাত্র র্যান্ডম ডেটা 

276
00:18:26,426 --> 00:18:31,445
মুখস্ত করার জন্য যথেষ্ট ছিল, যা এই প্রশ্ন উত্থাপন করে যে এই খরচ ফাংশনটি হ্রাস 

277
00:18:31,445 --> 00:18:36,400
করা আসলে চিত্রের কোনও ধরণের কাঠামোর সাথে মিলে যায়, নাকি এটি কেবল মুখস্ত করা?

278
00:18:51,440 --> 00:18:56,597
আপনি যদি সেই নির্ভুলতার বক্ররেখাটি দেখেন, যদি আপনি শুধুমাত্র একটি এলোমেলো 

279
00:18:56,597 --> 00:19:01,755
ডেটাসেটের প্রশিক্ষণ নিচ্ছেন, সেই বক্ররেখাটি প্রায় এক ধরনের রৈখিক ফ্যাশনে 

280
00:19:01,755 --> 00:19:06,773
খুব ধীরে ধীরে নিচে নেমে গেছে, তাই আপনি সত্যিই সম্ভাব্য স্থানীয় মিনিমাম 

281
00:19:06,773 --> 00:19:12,140
খুঁজে পেতে সংগ্রাম করছেন, আপনি জানেন , সঠিক ওজন যা আপনাকে সেই নির্ভুলতা পাবে।

282
00:19:12,240 --> 00:19:16,501
আপনি যদি প্রকৃতপক্ষে একটি স্ট্রাকচার্ড ডেটাসেটের উপর প্রশিক্ষণ নিচ্ছেন, 

283
00:19:16,501 --> 00:19:19,756
যার সঠিক লেবেল রয়েছে, আপনি শুরুতে একটু ঘোরাঘুরি করেন, 

284
00:19:19,756 --> 00:19:24,254
কিন্তু তারপরে আপনি সেই নির্ভুলতার স্তরে পৌঁছানোর জন্য খুব দ্রুত নেমে গেলেন, 

285
00:19:24,254 --> 00:19:28,220
এবং তাই কিছু অর্থে এটি যে স্থানীয় ম্যাক্সিমা খুঁজে পাওয়া সহজ ছিল.

286
00:19:28,540 --> 00:19:33,646
এবং তাই এটি সম্পর্কেও যেটি আকর্ষণীয় ছিল তা হল এটি আসলে কয়েক বছর আগে থেকে আরেকটি 

287
00:19:33,646 --> 00:19:38,814
কাগজ আলোতে নিয়ে আসে, যেটিতে নেটওয়ার্ক স্তরগুলি সম্পর্কে আরও অনেক সরলীকরণ রয়েছে, 

288
00:19:38,814 --> 00:19:44,169
তবে ফলাফলগুলির মধ্যে একটি বলছে আপনি যদি অপ্টিমাইজেশন ল্যান্ডস্কেপটি দেখেন তবে কীভাবে, 

289
00:19:44,169 --> 00:19:48,840
স্থানীয় মিনিমা যা এই নেটওয়ার্কগুলি শেখার প্রবণতা প্রকৃতপক্ষে সমান মানের, 

290
00:19:48,840 --> 00:19:54,320
তাই কিছু অর্থে যদি আপনার ডেটাসেট সুগঠিত হয় তবে আপনি এটি আরও সহজে খুঁজে পেতে সক্ষম হবেন।

291
00:19:58,160 --> 00:20:01,180
আমার ধন্যবাদ, বরাবরের মতো, আপনারা যারা প্যাট্রিয়নে সমর্থন করছেন তাদের।

292
00:20:01,520 --> 00:20:04,049
গেম চেঞ্জার প্যাট্রিয়ন কী তা আমি আগেও বলেছি, 

293
00:20:04,049 --> 00:20:06,800
তবে এই ভিডিওগুলি সত্যিই আপনাকে ছাড়া সম্ভব হবে না।

294
00:20:07,460 --> 00:20:09,931
সিরিজের এই প্রাথমিক ভিডিওগুলির সমর্থনে আমি VC 

295
00:20:09,931 --> 00:20:12,780
সংস্থা Amplify Partners-কেও বিশেষ ধন্যবাদ জানাতে চাই৷

