1
00:00:00,000 --> 00:00:07,240
סרטון אחרון פרסמתי את המבנה של רשת עצבית.

2
00:00:07,240 --> 00:00:11,560
אני אתן כאן סיכום קצר כדי שזה יהיה רענן

3
00:00:11,560 --> 00:00:13,160
במוחנו, ואז יש לי שתי מטרות עיקריות לסרטון הזה.

4
00:00:13,160 --> 00:00:17,960
הראשון הוא להציג את הרעיון של ירידה בשיפוע, שעומד בבסיס לא רק

5
00:00:17,960 --> 00:00:20,800
כיצד רשתות עצביות לומדות, אלא כיצד פועלת גם הרבה למידת מכונה אחרת.

6
00:00:20,800 --> 00:00:25,160
ואז אחר כך נחפור קצת יותר כיצד הרשת הספציפית הזו

7
00:00:25,160 --> 00:00:29,560
מתפקדת, ומה בסופו של דבר השכבות הנסתרות של נוירונים מחפשות.

8
00:00:29,560 --> 00:00:34,680
כזכור, המטרה שלנו כאן היא הדוגמה הקלאסית של זיהוי

9
00:00:34,680 --> 00:00:37,080
ספרות בכתב יד, עולם השלום של רשתות עצביות.

10
00:00:37,080 --> 00:00:42,160
ספרות אלו מוצגות על גבי רשת של 28x28 פיקסלים,

11
00:00:42,160 --> 00:00:44,260
לכל פיקסל יש ערך בגווני אפור בין 0 ל-1.

12
00:00:44,260 --> 00:00:51,400
אלו הם שקובעים את הפעלות של 784 נוירונים בשכבת הקלט של הרשת.

13
00:00:51,400 --> 00:00:56,880
ההפעלה של כל נוירון בשכבות הבאות מבוססת על סכום משוקלל

14
00:00:56,880 --> 00:01:02,300
של כל הפעלות בשכבה הקודמת, בתוספת מספר מיוחד שנקרא הטיה.

15
00:01:02,300 --> 00:01:07,480
אתה מחבר את הסכום הזה עם פונקציה אחרת, כמו

16
00:01:07,480 --> 00:01:09,640
ה-squishification של סיגמואיד, או ReLU, כמו שעברתי בסרטון האחרון.

17
00:01:09,640 --> 00:01:14,960
בסך הכל, בהינתן הבחירה המעט שרירותית של שתי שכבות נסתרות עם

18
00:01:14,960 --> 00:01:20,940
16 נוירונים כל אחת, לרשת יש כ-13,000 משקלים והטיות שאנחנו יכולים

19
00:01:20,940 --> 00:01:25,320
להתאים, והערכים האלה הם שקובעים מה בדיוק הרשת עושה בפועל.

20
00:01:25,320 --> 00:01:29,800
ולמה שאנחנו מתכוונים כשאנחנו אומרים שהרשת הזו מסווגת ספרה נתונה הוא שהנוירונים

21
00:01:29,800 --> 00:01:34,080
הבהירים ביותר מבין 10 הנוירונים האלה בשכבה הסופית מתאים לספרה הזו.

22
00:01:34,080 --> 00:01:39,240
וזכרו, המניע שחשבנו למבנה השכבתי היה שאולי השכבה

23
00:01:39,240 --> 00:01:43,920
השנייה יכולה לקלוט את הקצוות, השכבה השלישית עשויה

24
00:01:43,920 --> 00:01:48,640
לקלוט דפוסים כמו לולאות וקווים, והאחרונה יכולה פשוט

25
00:01:48,640 --> 00:01:49,640
לחבר את הדפוסים האלה כדי לזהות ספרות.

26
00:01:49,640 --> 00:01:52,880
אז הנה, אנו לומדים כיצד הרשת לומדת.

27
00:01:52,880 --> 00:01:56,880
מה שאנחנו רוצים זה אלגוריתם שבו אתה יכול להראות לרשת הזו חבורה שלמה

28
00:01:56,880 --> 00:02:01,540
של נתוני אימון, שמגיעים בצורה של חבורה של תמונות שונות של ספרות

29
00:02:01,540 --> 00:02:06,360
בכתב יד, יחד עם תוויות למה שהם אמורים להיות, וזה יהיה להתאים

30
00:02:06,360 --> 00:02:10,760
את 13,000 המשקולות וההטיות האלה כדי לשפר את הביצועים שלה בנתוני האימון.

31
00:02:10,760 --> 00:02:15,540
יש לקוות שהמבנה השכבתי הזה יגרום לכך שמה

32
00:02:15,540 --> 00:02:17,840
שהוא לומד מתכלל לתמונות מעבר לנתוני האימון האלה.

33
00:02:17,840 --> 00:02:22,240
הדרך שבה אנו בודקים זאת היא שאחרי שאתה מאמן את הרשת, אתה מראה

34
00:02:22,240 --> 00:02:31,160
לה יותר נתונים מתויגים, ותראה באיזו מידה היא מסווגת את התמונות החדשות האלה.

35
00:02:31,160 --> 00:02:34,760
למזלנו, ומה שהופך זאת לדוגמא נפוצה מלכתחילה, הוא שהאנשים הטובים מאחורי

36
00:02:34,760 --> 00:02:39,520
מסד הנתונים של MNIST הרכיבו אוסף של עשרות אלפי תמונות

37
00:02:39,520 --> 00:02:45,080
ספרתיות בכתב יד, כל אחת מסומנת במספרים שהם אמורים להיות.

38
00:02:45,080 --> 00:02:49,920
ועד כמה שזה פרובוקטיבי לתאר מכונה כלמידה, ברגע שאתה רואה איך זה עובד, זה

39
00:02:49,920 --> 00:02:55,560
מרגיש הרבה פחות כמו איזו הנחת מדע בדיוני מטורפת, והרבה יותר כמו תרגיל חשבון.

40
00:02:55,560 --> 00:03:01,040
כלומר, בעצם זה מסתכם במציאת המינימום של פונקציה מסוימת.

41
00:03:01,040 --> 00:03:06,480
זכור, מבחינה רעיונית אנו חושבים על כל נוירון כמקושר לכל

42
00:03:06,480 --> 00:03:11,440
הנוירונים בשכבה הקודמת, והמשקלים בסכום המשוקלל המגדירים את הפעלתו דומים

43
00:03:11,440 --> 00:03:16,400
לנקודות החוזק של הקשרים הללו, וההטיה היא אינדיקציה כלשהי של

44
00:03:16,400 --> 00:03:19,780
האם הנוירון הזה נוטה להיות פעיל או לא פעיל.

45
00:03:19,780 --> 00:03:23,300
וכדי להתחיל בדברים, אנחנו פשוט הולכים לאתחל את

46
00:03:23,300 --> 00:03:25,020
כל המשקולות וההטיות האלה באופן אקראי לחלוטין.

47
00:03:25,020 --> 00:03:29,100
מיותר לציין שהרשת הזו הולכת להופיע בצורה איומה בדוגמה

48
00:03:29,100 --> 00:03:31,180
לאימון נתון, מכיוון שהיא פשוט עושה משהו אקראי.

49
00:03:31,180 --> 00:03:36,820
לדוגמה, אתה מזין את התמונה הזו של 3, ושכבת הפלט פשוט נראית כמו בלגן.

50
00:03:36,820 --> 00:03:43,340
אז מה שאתה עושה זה להגדיר פונקציית עלות, דרך לומר למחשב, לא, מחשב גרוע,

51
00:03:43,340 --> 00:03:48,940
שלפלט צריך להיות הפעלות שהן 0 עבור רוב הנוירונים, אבל 1 עבור הנוירון הזה.

52
00:03:48,980 --> 00:03:51,740
מה שנתת לי זה זבל מוחלט.

53
00:03:51,740 --> 00:03:56,740
אם לומר את זה בצורה קצת יותר מתמטית, אתה מחבר את הריבועים

54
00:03:56,740 --> 00:04:01,980
של ההבדלים בין כל אחת מאותן הפעלת פלט אשפה לבין הערך שאתה

55
00:04:01,980 --> 00:04:06,020
רוצה שיהיה להם, וזה מה שנכנה את העלות של דוגמה אחת לאימון.

56
00:04:06,020 --> 00:04:12,660
שימו לב שהסכום הזה קטן כאשר הרשת מסווגת בבטחה את התמונה בצורה נכונה,

57
00:04:12,660 --> 00:04:18,820
אך הוא גדול כאשר הרשת נראית כאילו היא לא יודעת מה היא עושה.

58
00:04:18,820 --> 00:04:23,860
אז מה שאתה עושה זה לשקול את העלות הממוצעת

59
00:04:23,860 --> 00:04:27,580
על פני כל עשרות אלפי דוגמאות ההדרכה העומדות לרשותך.

60
00:04:27,580 --> 00:04:32,300
העלות הממוצעת הזו היא המדד שלנו לכמה הרשת

61
00:04:32,300 --> 00:04:33,300
גרועה ועד כמה המחשב אמור להרגיש רע.

62
00:04:33,300 --> 00:04:35,300
וזה דבר מסובך.

63
00:04:35,300 --> 00:04:40,380
זוכרים איך הרשת עצמה הייתה בעצם פונקציה, כזו שמקבלת 784

64
00:04:40,380 --> 00:04:46,100
מספרים כקלט, את ערכי הפיקסלים, ויורקת 10 מספרים כפלט שלה,

65
00:04:46,100 --> 00:04:49,700
ובמובן מסוים היא מוגדרת על ידי כל המשקלים וההטיות האלה?

66
00:04:49,700 --> 00:04:53,340
פונקציית העלות היא שכבה של מורכבות נוסף על כך.

67
00:04:53,340 --> 00:04:59,140
הוא לוקח כקלט את כ-13,000 המשקלים וההטיות האלה, ויורק מספר בודד

68
00:04:59,140 --> 00:05:04,620
שמתאר כמה רעים המשקלים וההטיות האלה, והאופן שבו הם מוגדרים

69
00:05:04,620 --> 00:05:09,140
תלוי בהתנהגות הרשת על פני כל עשרות אלפי נתוני האימון.

70
00:05:09,140 --> 00:05:12,460
זה הרבה לחשוב על זה.

71
00:05:12,460 --> 00:05:16,380
אבל רק להגיד למחשב איזו עבודה מחורבן הוא עושה זה לא מאוד מועיל.

72
00:05:16,380 --> 00:05:21,300
אתה רוצה להגיד לו איך לשנות את המשקולות וההטיות האלה כדי שזה ישתפר.

73
00:05:21,300 --> 00:05:25,580
כדי להקל, במקום להתאמץ לדמיין פונקציה עם 13,000 כניסות, פשוט דמיינו

74
00:05:25,580 --> 00:05:31,440
פונקציה פשוטה שיש לה מספר אחד כקלט ומספר אחד כפלט.

75
00:05:31,440 --> 00:05:36,420
איך מוצאים קלט שממזער את הערך של הפונקציה הזו?

76
00:05:36,420 --> 00:05:41,300
תלמידי החשבון יידעו שלפעמים אתה יכול להבין את המינימום הזה במפורש, אבל

77
00:05:41,340 --> 00:05:46,620
זה לא תמיד אפשרי עבור פונקציות מסובכות באמת, בטח לא בגרסת 13,000

78
00:05:46,620 --> 00:05:51,640
הקלט של המצב הזה עבור פונקציית עלות הרשת העצבית המסובכת והמטורפת שלנו.

79
00:05:51,640 --> 00:05:56,820
טקטיקה גמישה יותר היא להתחיל בכל קלט, ולהבין באיזה

80
00:05:56,820 --> 00:05:59,860
כיוון עליך לצעוד כדי להוריד את הפלט הזה.

81
00:05:59,860 --> 00:06:05,020
באופן ספציפי, אם אתה יכול להבין את השיפוע של הפונקציה

82
00:06:05,020 --> 00:06:09,280
במקום שבו אתה נמצא, אז הזז שמאלה אם השיפוע הזה

83
00:06:09,280 --> 00:06:12,720
חיובי, והסט את הקלט ימינה אם השיפוע הזה שלילי.

84
00:06:12,720 --> 00:06:17,040
אם תעשה זאת שוב ושוב, בכל נקודה שתבדוק את השיפוע החדש ותנקוט

85
00:06:17,040 --> 00:06:20,680
את הצעד המתאים, אתה הולך להתקרב למינימום מקומי כלשהו של הפונקציה.

86
00:06:20,680 --> 00:06:24,600
והתמונה שאולי יש לך בראש כאן היא כדור שמתגלגל במורד גבעה.

87
00:06:24,600 --> 00:06:29,380
ושימו לב, אפילו עבור פונקציית הקלט הבודדת המפושטת הזו, ישנם

88
00:06:29,380 --> 00:06:34,220
עמקים אפשריים רבים שאתם עשויים לנחות בהם, תלוי באיזה

89
00:06:34,220 --> 00:06:38,460
קלט אקראי אתם מתחילים, ואין ערובה שהמינימום המקומי בו

90
00:06:38,460 --> 00:06:39,460
תנחתו יהיה הערך הקטן ביותר האפשרי של פונקציית העלות.

91
00:06:39,460 --> 00:06:43,180
זה יעבור גם למקרה של הרשת העצבית שלנו.

92
00:06:43,180 --> 00:06:48,140
ואני גם רוצה שתשים לב איך אם אתה הופך את

93
00:06:48,140 --> 00:06:52,920
גודל הצעדים שלך לפרופורציונלי למדרון, אז כשהשיפוע משתטח לכיוון המינימום,

94
00:06:52,920 --> 00:06:56,020
הצעדים שלך הולכים וקטנים, וזה סוג של עוזר לך לחרוג.

95
00:06:56,020 --> 00:07:01,640
להגביר מעט את המורכבות, דמיינו במקום זאת פונקציה עם שתי כניסות ופלט אחד.

96
00:07:01,640 --> 00:07:06,360
אתה יכול לחשוב על מרחב הקלט כמישור ה-xy,

97
00:07:06,360 --> 00:07:09,020
ועל פונקציית העלות כמתוארת בגרף כמשטח מעליו.

98
00:07:09,020 --> 00:07:13,600
במקום לשאול לגבי שיפוע הפונקציה, עליך לשאול באיזה כיוון עליך לצעוד במרחב

99
00:07:13,600 --> 00:07:19,780
הקלט הזה כדי להקטין את הפלט של הפונקציה במהירות הגבוהה ביותר.

100
00:07:19,780 --> 00:07:22,340
במילים אחרות, מה כיוון הירידה?

101
00:07:22,340 --> 00:07:26,740
ושוב, זה מועיל לחשוב על כדור שמתגלגל במורד הגבעה.

102
00:07:26,740 --> 00:07:31,920
מי מכם שמכיר את החשבון הרב-משתני יודע שהשיפוע של

103
00:07:31,920 --> 00:07:37,460
פונקציה נותן לכם את כיוון העלייה התלולה ביותר, לאיזה

104
00:07:37,460 --> 00:07:39,420
כיוון כדאי לצעוד כדי להגדיל את הפונקציה המהירה ביותר.

105
00:07:39,420 --> 00:07:43,820
באופן טבעי, לקיחת השלילי של השיפוע הזה נותן לך

106
00:07:43,820 --> 00:07:47,460
את הכיוון לצעד שמקטין את הפונקציה הכי מהר.

107
00:07:47,460 --> 00:07:52,320
אפילו יותר מזה, אורכו של וקטור השיפוע הזה

108
00:07:52,320 --> 00:07:54,580
הוא אינדיקציה למידת התלול של המדרון התלול ביותר.

109
00:07:54,580 --> 00:07:58,080
עכשיו אם אינך מכיר חשבון רב-משתני ומעוניין ללמוד עוד,

110
00:07:58,080 --> 00:08:01,100
בדוק חלק מהעבודות שעשיתי עבור אקדמיית חאן בנושא.

111
00:08:01,100 --> 00:08:05,680
אבל בכנות, כל מה שחשוב לך ולי כרגע זה

112
00:08:05,680 --> 00:08:10,440
שבאופן עקרוני קיימת דרך לחשב את הווקטור הזה, הווקטור

113
00:08:10,440 --> 00:08:12,040
הזה שאומר לך מה כיוון הירידה וכמה הוא תלול.

114
00:08:12,040 --> 00:08:17,280
אתה תהיה בסדר אם זה כל מה שאתה יודע ואתה לא איתן בפרטים.

115
00:08:17,280 --> 00:08:21,440
כי אם אתה יכול לקבל את זה, האלגוריתם למזער את הפונקציה הוא לחשב את

116
00:08:21,440 --> 00:08:27,400
כיוון השיפוע הזה, ואז לקחת צעד קטן במורד, ולחזור על זה שוב ושוב.

117
00:08:28,300 --> 00:08:33,700
זה אותו רעיון בסיסי לפונקציה שיש לה 13,000 כניסות במקום 2 כניסות.

118
00:08:33,700 --> 00:08:38,980
תאר לעצמך לארגן את כל 13,000 המשקלים וההטיות

119
00:08:38,980 --> 00:08:40,180
של הרשת שלנו לתוך וקטור עמודות ענק.

120
00:08:40,180 --> 00:08:46,140
השיפוע השלילי של פונקציית העלות הוא רק וקטור, זה כיוון

121
00:08:46,140 --> 00:08:51,660
כלשהו בתוך מרחב הקלט העצום בטירוף הזה שאומר לך אילו

122
00:08:51,660 --> 00:08:55,900
דחיפות לכל המספרים האלה יגרמו לירידה המהירה ביותר לפונקציית העלות.

123
00:08:55,900 --> 00:09:00,000
וכמובן, עם פונקציית העלות שתוכננה במיוחד שלנו, שינוי המשקלים וההטיות

124
00:09:00,000 --> 00:09:05,520
כדי להקטין את זה אומר לגרום לתפוקת הרשת על כל

125
00:09:05,520 --> 00:09:10,280
נתוני אימון להיראות פחות כמו מערך אקראי של 10 ערכים,

126
00:09:10,280 --> 00:09:11,280
ויותר כמו החלטה אמיתית שאנחנו רוצים את זה לעשות.

127
00:09:11,280 --> 00:09:15,940
חשוב לזכור, פונקציית עלות זו כוללת ממוצע של כל נתוני האימון, כך שאם

128
00:09:15,940 --> 00:09:24,260
אתה ממזער אותו, זה אומר שזה ביצועים טובים יותר בכל הדגימות הללו.

129
00:09:24,260 --> 00:09:28,540
האלגוריתם לחישוב שיפוע זה ביעילות, שהוא למעשה הלב

130
00:09:28,540 --> 00:09:32,520
של האופן שבו רשת עצבית לומדת, נקרא התפשטות

131
00:09:32,520 --> 00:09:34,040
לאחור, ועל זה אני הולך לדבר בסרטון הבא.

132
00:09:34,040 --> 00:09:39,100
שם, אני באמת רוצה להקדיש זמן כדי לעבור על מה בדיוק

133
00:09:39,100 --> 00:09:44,100
קורה לכל משקל והטיה עבור נתוני אימון נתון, מנסה לתת

134
00:09:44,100 --> 00:09:47,980
תחושה אינטואיטיבית של מה שקורה מעבר לערימת החישובים והנוסחאות הרלוונטיות.

135
00:09:47,980 --> 00:09:51,780
בדיוק כאן, כרגע, הדבר העיקרי שאני רוצה שתדע, ללא

136
00:09:51,780 --> 00:09:56,820
תלות בפרטי הטמעה, הוא שמה שאנחנו מתכוונים כשאנחנו מדברים

137
00:09:56,820 --> 00:09:59,320
על למידה ברשת הוא שזה רק מזעור פונקציית עלות.

138
00:09:59,320 --> 00:10:02,760
ושימו לב, תוצאה אחת של זה היא שחשוב שלפונקציית

139
00:10:02,760 --> 00:10:07,820
העלות הזו תהיה תפוקה חלקה ונחמדה, כדי שנוכל

140
00:10:07,820 --> 00:10:09,340
למצוא מינימום מקומי על ידי צעדים קטנים בירידה.

141
00:10:09,340 --> 00:10:14,140
זו הסיבה, אגב, לנוירונים מלאכותיים יש הפעלה

142
00:10:14,140 --> 00:10:18,580
מתמשכת, במקום פשוט להיות פעילים או לא

143
00:10:18,580 --> 00:10:20,440
פעילים בצורה בינארית, כמו הנוירונים הביולוגיים.

144
00:10:20,440 --> 00:10:24,600
תהליך זה של דחיפה חוזרת ונשנית של קלט של

145
00:10:24,600 --> 00:10:26,960
פונקציה בכפולה כלשהי של השיפוע השלילי נקרא ירידה בדרגה.

146
00:10:26,960 --> 00:10:31,760
זו דרך להתכנס למינימום מקומי כלשהו של

147
00:10:31,760 --> 00:10:33,000
פונקציית עלות, בעצם עמק בגרף הזה.

148
00:10:33,000 --> 00:10:37,040
אני עדיין מראה את התמונה של פונקציה עם שתי כניסות, כמובן,

149
00:10:37,040 --> 00:10:41,480
כי דחיפות בחלל קלט של 13,000 מימדים קצת קשה לעטוף את

150
00:10:41,480 --> 00:10:45,220
דעתך, אבל למעשה יש דרך נחמדה לא מרחבית לחשוב על זה.

151
00:10:45,220 --> 00:10:49,100
כל רכיב של הגרדיאנט השלילי אומר לנו שני דברים.

152
00:10:49,100 --> 00:10:53,600
הסימן, כמובן, אומר לנו אם יש להזיז את

153
00:10:53,600 --> 00:10:55,860
הרכיב המתאים של וקטור הקלט למעלה או למטה.

154
00:10:55,860 --> 00:11:01,340
אבל חשוב מכך, הגדלים היחסיים של כל הרכיבים

155
00:11:01,340 --> 00:11:05,620
האלה מעידים לך אילו שינויים חשובים יותר.

156
00:11:05,620 --> 00:11:09,780
אתה מבין, ברשת שלנו, התאמה לאחד המשקולות עשויה להשפיע

157
00:11:09,780 --> 00:11:14,980
הרבה יותר על פונקציית העלות מאשר התאמה למשקל אחר.

158
00:11:14,980 --> 00:11:19,440
חלק מהחיבורים האלה פשוט חשובים יותר לנתוני האימונים שלנו.

159
00:11:19,440 --> 00:11:23,520
אז הדרך שבה אתה יכול לחשוב על וקטור השיפוע הזה של פונקציית העלות

160
00:11:23,520 --> 00:11:29,740
המאסיבית שלנו, המעוותת את המוח, היא שהוא מקודד את החשיבות היחסית של

161
00:11:29,740 --> 00:11:34,100
כל משקל והטיה, כלומר, איזה מהשינויים האלה יביא הכי הרבה כסף עבורך.

162
00:11:34,100 --> 00:11:37,360
זו באמת רק עוד דרך לחשוב על כיוון.

163
00:11:37,360 --> 00:11:41,740
אם לקחת דוגמה פשוטה יותר, אם יש לך איזושהי פונקציה עם

164
00:11:41,740 --> 00:11:48,720
שני משתנים כקלט, ומחשבת שהשיפוע שלה בנקודה מסוימת יוצאת כ-3,1, אז

165
00:11:48,720 --> 00:11:52,880
מצד אחד אתה יכול לפרש את זה כאילו אתה אומר שכאשר

166
00:11:52,880 --> 00:11:57,400
אתה עמידה בקלט הזה, נע לאורך הכיוון הזה מגדיל את הפונקציה

167
00:11:57,400 --> 00:12:02,200
הכי מהר, שכאשר אתה משרטט את הפונקציה מעל מישור נקודות הקלט,

168
00:12:02,200 --> 00:12:03,200
הווקטור הזה הוא מה שנותן לך את כיוון העלייה הישר.

169
00:12:03,200 --> 00:12:07,600
אבל דרך נוספת לקרוא היא לומר שלשינויים במשתנה הראשון הזה יש

170
00:12:07,600 --> 00:12:12,400
חשיבות פי שלושה מאשר לשינויים במשתנה השני, שלפחות בסביבה של הקלט

171
00:12:12,400 --> 00:12:17,740
הרלוונטי, דחיפה של ערך ה-x גוררת הרבה יותר מרץ עבורך דוֹלָר.

172
00:12:17,740 --> 00:12:22,880
בסדר, בוא נתרחק ונסכם איפה אנחנו עד עכשיו.

173
00:12:22,880 --> 00:12:28,660
הרשת עצמה היא הפונקציה הזו עם 784 כניסות ו-10

174
00:12:28,660 --> 00:12:30,860
יציאות, המוגדרות במונחים של כל הסכומים המשוקללים הללו.

175
00:12:30,860 --> 00:12:34,160
פונקציית העלות היא שכבה של מורכבות נוסף על כך.

176
00:12:34,160 --> 00:12:39,300
הוא לוקח את 13,000 המשקולות וההטיות בתור תשומות, ויורק

177
00:12:39,300 --> 00:12:42,640
מידה אחת של עלוב בהתבסס על דוגמאות האימון.

178
00:12:42,640 --> 00:12:47,520
השיפוע של פונקציית העלות הוא עוד שכבה אחת של מורכבות.

179
00:12:47,520 --> 00:12:52,860
זה אומר לנו אילו דחיפות לכל המשקולות וההטיות הללו גורמות

180
00:12:52,860 --> 00:12:56,640
לשינוי המהיר ביותר בערך של פונקציית העלות, שאותו אתה

181
00:12:56,640 --> 00:13:03,040
עשוי לפרש כאומר אילו שינויים לאיזה משקלים חשובים ביותר.

182
00:13:03,040 --> 00:13:07,620
אז כשאתה מאתחל את הרשת עם משקלים והטיות אקראיות, ומתאים

183
00:13:07,620 --> 00:13:12,420
אותם פעמים רבות בהתבסס על תהליך הירידה בשיפוע הזה, עד

184
00:13:12,420 --> 00:13:14,240
כמה היא באמת מתפקדת בתמונות שלא נראו קודם לכן?

185
00:13:14,240 --> 00:13:19,000
זה שתיארתי כאן, עם שתי השכבות הנסתרות של 16 נוירונים כל אחת, שנבחרו

186
00:13:19,000 --> 00:13:26,920
בעיקר מסיבות אסתטיות, לא רע, ומסווג כ-96% מהתמונות החדשות שהוא רואה נכון.

187
00:13:26,920 --> 00:13:31,580
ובכנות, אם אתה מסתכל על כמה מהדוגמאות שהוא מבלגן

188
00:13:31,580 --> 00:13:36,300
בהן, אתה מרגיש נאלץ לחתוך את זה קצת.

189
00:13:36,300 --> 00:13:40,220
אם אתה משחק עם מבנה השכבה הנסתרת ותעשה כמה

190
00:13:40,220 --> 00:13:41,220
שינויים, אתה יכול להשיג את זה עד 98%.

191
00:13:41,220 --> 00:13:42,900
וזה די טוב!

192
00:13:42,900 --> 00:13:47,020
זה לא הכי טוב, אתה בהחלט יכול להשיג ביצועים טובים יותר על ידי ביצוע

193
00:13:47,020 --> 00:13:52,460
מתוחכם יותר מרשת הוניל הפשוטה הזו, אבל בהתחשב בכמה מרתיעה המשימה הראשונית, אני

194
00:13:52,460 --> 00:13:56,800
חושב שיש משהו מדהים בכל רשת שעושה את זה טוב בתמונות שהיא מעולם לא

195
00:13:56,800 --> 00:14:02,000
נראתה בעבר בהתחשב בכך שאנחנו מעולם לא אמר לו במפורש אילו דפוסים לחפש.

196
00:14:02,000 --> 00:14:07,840
במקור, הדרך שבה הנעתי את המבנה הזה הייתה על ידי תיאור

197
00:14:07,840 --> 00:14:11,880
תקווה שאולי תהיה לנו, שהשכבה השנייה עשויה לקלוט קצוות קטנים,

198
00:14:11,880 --> 00:14:16,080
שהשכבה השלישית תחבר את הקצוות האלה כדי לזהות לולאות וקווים

199
00:14:16,080 --> 00:14:18,220
ארוכים יותר, ושהם עשויים להיות חתוכים. יחד כדי לזהות ספרות.

200
00:14:18,220 --> 00:14:21,040
אז זה מה שהרשת שלנו עושה בעצם?

201
00:14:21,040 --> 00:14:24,880
ובכן, עבור זה לפחות, בכלל לא.

202
00:14:24,960 --> 00:14:29,120
זוכרים איך בסרטון האחרון הסתכלנו כיצד ניתן להמחיש את

203
00:14:29,120 --> 00:14:33,900
משקלי החיבורים מכל הנוירונים בשכבה הראשונה לנוירון נתון בשכבה

204
00:14:33,900 --> 00:14:37,440
השנייה כתבנית פיקסלים נתונה שנוירון השכבה השנייה קולט?

205
00:14:37,440 --> 00:14:44,600
ובכן, כשאנחנו עושים את זה עבור המשקולות הקשורות למעברים האלה,

206
00:14:44,600 --> 00:14:51,000
במקום לקלוט קצוות קטנים מבודדים פה ושם, הם נראים, ובכן,

207
00:14:51,000 --> 00:14:54,200
כמעט אקראיים, רק עם כמה דפוסים רופפים מאוד באמצע.

208
00:14:54,200 --> 00:14:59,020
נראה שבמרחב הבלתי נתפס של 13,000 ממדים של משקלים

209
00:14:59,020 --> 00:15:04,020
והטיות אפשריות, הרשת שלנו מצאה את עצמה מינימום

210
00:15:04,020 --> 00:15:08,440
מקומי קטן ומשמח, שלמרות סיווג מוצלח של רוב התמונות,

211
00:15:08,440 --> 00:15:09,840
לא בדיוק קולט את הדפוסים שאולי קיווינו להם.

212
00:15:09,840 --> 00:15:14,600
וכדי באמת להסיע את הנקודה הזו הביתה, צפה במה שקורה כשאתה מזין תמונה אקראית.

213
00:15:14,600 --> 00:15:19,240
אם המערכת הייתה חכמה, אולי הייתם מצפים שהיא תרגיש לא בטוחה, אולי לא באמת

214
00:15:19,240 --> 00:15:24,120
תפעיל אף אחד מ-10 נוירוני הפלט האלה או תפעיל את כולם באופן שווה, אבל

215
00:15:24,520 --> 00:15:29,800
במקום זאת היא נותנת לכם בביטחון איזו תשובה שטות, כאילו היא מרגישה בטוחה שזה

216
00:15:29,800 --> 00:15:34,560
אקראי רעש הוא 5 כפי שהוא עושה שתמונה בפועל של 5 היא 5.

217
00:15:34,560 --> 00:15:39,300
בניסוח שונה, גם אם הרשת הזו יכולה לזהות ספרות

218
00:15:39,300 --> 00:15:41,800
די טוב, אין לה מושג איך לצייר אותן.

219
00:15:41,800 --> 00:15:45,400
הרבה מזה נובע מכך שזה מערך אימון כל כך מוגבל.

220
00:15:45,400 --> 00:15:48,220
כלומר, שימו את עצמכם בנעלי הרשת כאן.

221
00:15:48,220 --> 00:15:53,280
מנקודת המבט שלו, היקום כולו אינו מורכב מכלום מלבד ספרות לא

222
00:15:53,280 --> 00:15:58,560
זזות מוגדרות בבירור ובמרכזן רשת זעירה, ותפקוד העלות שלו מעולם

223
00:15:58,560 --> 00:16:02,160
לא נתן לו שום תמריץ להיות אלא בטוח לחלוטין בהחלטותיו.

224
00:16:02,160 --> 00:16:05,760
אז עם זה בתור הדימוי של מה שהנוירונים בשכבה

225
00:16:05,760 --> 00:16:09,320
השנייה באמת עושים, אתם עשויים לתהות מדוע אציג את

226
00:16:09,320 --> 00:16:10,320
הרשת הזו עם מוטיבציה של לקלוט קצוות ודפוסים.

227
00:16:10,320 --> 00:16:13,040
כלומר, זה פשוט בכלל לא מה שזה בסופו של דבר עושה.

228
00:16:13,040 --> 00:16:17,480
ובכן, זו לא אמורה להיות המטרה הסופית שלנו, אלא נקודת התחלה.

229
00:16:17,480 --> 00:16:22,280
למען האמת, זו טכנולוגיה ישנה, מהסוג שנחקר בשנות ה-80 וה-90, ואתה

230
00:16:22,280 --> 00:16:26,920
צריך להבין אותה לפני שתוכל להבין גרסאות מודרניות מפורטות יותר, וברור

231
00:16:26,920 --> 00:16:31,380
שהיא מסוגלת לפתור כמה בעיות מעניינות, אבל ככל שתחפור יותר במה

232
00:16:31,380 --> 00:16:38,720
השכבות הנסתרות האלה באמת עושות, ככל שזה נראה פחות אינטליגנטי.

233
00:16:38,720 --> 00:16:43,540
העברת הפוקוס לרגע מהאופן שבו רשתות לומדות לאופן שבו אתה לומד,

234
00:16:43,540 --> 00:16:47,160
זה יקרה רק אם תעסוק באופן פעיל בחומר כאן איכשהו.

235
00:16:47,160 --> 00:16:51,920
דבר אחד די פשוט שאני רוצה שתעשה הוא פשוט לעצור ברגע זה ולחשוב

236
00:16:51,920 --> 00:16:57,560
לרגע לעומק אילו שינויים אתה עשוי לעשות במערכת הזו וכיצד היא תופסת

237
00:16:57,560 --> 00:17:01,880
תמונות אם אתה רוצה שהיא תקלוט טוב יותר דברים כמו קצוות ודפוסים.

238
00:17:01,880 --> 00:17:06,360
אבל יותר מזה, כדי לעסוק באמת בחומר, אני ממליץ בחום

239
00:17:06,360 --> 00:17:09,720
על ספרו של מייקל נילסן על למידה עמוקה ורשתות עצביות.

240
00:17:09,720 --> 00:17:15,200
בו, אתה יכול למצוא את הקוד ואת הנתונים להורדה ולשחק איתם עבור הדוגמה

241
00:17:15,200 --> 00:17:19,360
המדויקת הזו, והספר ידריך אותך צעד אחר צעד מה הקוד הזה עושה.

242
00:17:19,360 --> 00:17:23,920
מה שמדהים הוא שהספר הזה הוא חינמי וזמין לציבור, אז אם

243
00:17:23,920 --> 00:17:28,040
אתה מפיק ממנו משהו, שקול להצטרף אלי לתרום למאמצים של נילסן.

244
00:17:28,040 --> 00:17:32,060
קישרתי גם כמה משאבים אחרים שאני מאוד אוהב בתיאור, כולל

245
00:17:32,060 --> 00:17:38,720
פוסט הבלוג הפנומנלי והיפה של כריס אולה והמאמרים ב-Distill.

246
00:17:38,720 --> 00:17:41,960
כדי לסגור את העניינים כאן לדקות האחרונות, אני רוצה

247
00:17:41,960 --> 00:17:44,440
לחזור לקטע מהראיון שהיה לי עם ליישה לי.

248
00:17:44,440 --> 00:17:48,520
אתה אולי זוכר אותה מהסרטון האחרון, היא עשתה את עבודת הדוקטורט שלה בלמידה עמוקה.

249
00:17:48,560 --> 00:17:52,240
בקטע הקטן הזה, היא מדברת על שני מאמרים אחרונים שבאמת

250
00:17:52,240 --> 00:17:56,380
חופרים כיצד חלק מרשתות זיהוי התמונות המודרניות יותר לומדות למעשה.

251
00:17:56,380 --> 00:18:00,320
רק כדי להגדיר היכן היינו בשיחה, המאמר הראשון לקח את אחת מהרשתות

252
00:18:00,320 --> 00:18:04,480
הנוירוניות העמוקות במיוחד האלה שמאוד טובות בזיהוי תמונות, ובמקום לאמן אותה על

253
00:18:04,480 --> 00:18:09,400
מערך נתונים מסומן כהלכה, הוא עירבב את כל התוויות לפני האימון.

254
00:18:09,400 --> 00:18:13,840
ברור שדיוק הבדיקה כאן לא היה טוב

255
00:18:13,840 --> 00:18:15,320
יותר מאקראי, מכיוון שהכל פשוט מסומן באקראי.

256
00:18:15,320 --> 00:18:20,080
אבל זה עדיין היה מסוגל להשיג את אותו דיוק

257
00:18:20,080 --> 00:18:21,440
אימון כפי שאתה משיג על מערך נתונים מסומן כהלכה.

258
00:18:21,440 --> 00:18:26,120
בעיקרון, מיליוני המשקלים של הרשת הספציפית הזו הספיקו לה רק לשנן את

259
00:18:26,120 --> 00:18:31,040
הנתונים האקראיים, מה שמעלה את השאלה האם מזעור פונקציית העלות הזו

260
00:18:31,040 --> 00:18:36,720
אכן מתאים לכל סוג של מבנה בתמונה, או שזה רק שינון?

261
00:18:36,720 --> 00:18:40,120
. . . לשנן את כל מערך הנתונים של מהו הסיווג הנכון.

262
00:18:40,120 --> 00:18:45,720
אז כמה, אתם יודעים, חצי שנה מאוחר יותר ב-ICML השנה, לא

263
00:18:45,720 --> 00:18:50,440
היה בדיוק נייר הפרכה, אלא נייר שהתייחס לכמה היבטים כמו,

264
00:18:50,440 --> 00:18:52,220
היי, למעשה הרשתות האלה עושות משהו קצת יותר חכם מזה.

265
00:18:52,220 --> 00:18:59,600
אם אתה מסתכל על עקומת הדיוק הזו, אם רק היית מתאמן על מערך

266
00:18:59,600 --> 00:19:05,240
נתונים אקראי, העקומה הזו ירדה מאוד, אתה יודע, לאט מאוד בצורה כמעט ליניארית.

267
00:19:05,280 --> 00:19:10,840
אז אתה באמת מתקשה למצוא את המינימום המקומי הזה של

268
00:19:10,840 --> 00:19:12,320
המשקולות האפשריות, אתה יודע, שיביאו לך את הדיוק הזה.

269
00:19:12,320 --> 00:19:16,720
בעוד שאם אתה מתאמן על מערך נתונים מובנה, כזה שיש

270
00:19:16,720 --> 00:19:20,240
לו את התוויות הנכונות, אתה יודע, אתה מתעסק קצת בהתחלה,

271
00:19:20,240 --> 00:19:23,360
אבל אז ירדת מהר מאוד כדי להגיע לרמת הדיוק הזו.

272
00:19:23,360 --> 00:19:28,580
ובמובן מסוים היה קל יותר למצוא את המקסימום המקומי הזה.

273
00:19:28,580 --> 00:19:32,900
אז מה שהיה מעניין בזה הוא שהוא מביא

274
00:19:32,900 --> 00:19:39,140
לאור עוד מאמר מלפני כמה שנים, שיש

275
00:19:39,140 --> 00:19:40,140
בו הרבה יותר הפשטות לגבי שכבות הרשת.

276
00:19:40,140 --> 00:19:43,880
אבל אחת התוצאות אמרה כיצד, אם מסתכלים על נוף האופטימיזציה,

277
00:19:43,880 --> 00:19:49,400
המינימום המקומי שרשתות אלו נוטות ללמוד הם למעשה באיכות שווה.

278
00:19:49,400 --> 00:19:54,300
אז במובן מסוים, אם מערך הנתונים שלך מובנה, אתה אמור להיות מסוגל למצוא את זה הרבה יותר בקלות.

279
00:19:58,580 --> 00:20:01,140
תודתי כמו תמיד לאלו מכם שתומכים בפטראון.

280
00:20:01,480 --> 00:20:05,440
כבר אמרתי בעבר מה זה מחליף משחק בפטראון,

281
00:20:05,440 --> 00:20:07,160
אבל הסרטונים האלה באמת לא היו אפשריים בלעדיכם.

282
00:20:07,160 --> 00:20:11,540
אני גם רוצה להודות במיוחד לחברת ה-VC Amplify

283
00:20:11,540 --> 00:20:13,240
Partners ולתמיכה שלהם בסרטוני הווידאו הראשוניים בסדרה.

284
00:20:31,140 --> 00:20:33,140
תודה.

