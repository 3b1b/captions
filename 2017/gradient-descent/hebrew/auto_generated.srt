1
00:00:00,000 --> 00:00:07,240
סרטון אחרון פרסמתי את המבנה של רשת עצבית.

2
00:00:07,240 --> 00:00:13,160
אני אתן כאן סיכום קצר כדי שזה יהיה רענן במוחנו, ואז יש לי שתי מטרות עיקריות לסרטון הזה.

3
00:00:13,160 --> 00:00:16,831
הראשון הוא להציג את הרעיון של ירידה בשיפוע, שעומד בבסיס לא רק

4
00:00:16,831 --> 00:00:20,800
כיצד רשתות עצביות לומדות, אלא כיצד פועלת גם הרבה למידת מכונה אחרת.

5
00:00:20,800 --> 00:00:25,380
ואז אחר כך נחפור קצת יותר כיצד הרשת הספציפית הזו מתפקדת,

6
00:00:25,380 --> 00:00:29,560
ומה בסופו של דבר השכבות הנסתרות של נוירונים מחפשות.

7
00:00:29,560 --> 00:00:33,603
כזכור, המטרה שלנו כאן היא הדוגמה הקלאסית של זיהוי

8
00:00:33,603 --> 00:00:37,080
ספרות בכתב יד, עולם השלום של רשתות עצביות.

9
00:00:37,080 --> 00:00:44,260
ספרות אלו מוצגות על גבי רשת של 28x28 פיקסלים, לכל פיקסל יש ערך בגווני אפור בין 0 ל-1.

10
00:00:44,260 --> 00:00:51,400
אלו הם שקובעים את הפעלות של 784 נוירונים בשכבת הקלט של הרשת.

11
00:00:51,400 --> 00:00:56,752
ההפעלה של כל נוירון בשכבות הבאות מבוססת על סכום משוקלל

12
00:00:56,752 --> 00:01:02,300
של כל הפעלות בשכבה הקודמת, בתוספת מספר מיוחד שנקרא הטיה.

13
00:01:02,300 --> 00:01:06,407
אתה מחבר את הסכום הזה עם פונקציה אחרת, כמו ה-squishification

14
00:01:06,407 --> 00:01:09,640
של סיגמואיד, או ReLU, כמו שעברתי בסרטון האחרון.

15
00:01:09,640 --> 00:01:14,780
בסך הכל, בהינתן הבחירה המעט שרירותית של שתי שכבות נסתרות עם

16
00:01:14,780 --> 00:01:19,750
16 נוירונים כל אחת, לרשת יש כ-13,000 משקלים והטיות שאנחנו

17
00:01:19,750 --> 00:01:25,320
יכולים להתאים, והערכים האלה הם שקובעים מה בדיוק הרשת עושה בפועל.

18
00:01:25,320 --> 00:01:29,400
ולמה שאנחנו מתכוונים כשאנחנו אומרים שהרשת הזו מסווגת ספרה נתונה הוא

19
00:01:29,400 --> 00:01:34,080
שהנוירונים הבהירים ביותר מבין 10 הנוירונים האלה בשכבה הסופית מתאים לספרה הזו.

20
00:01:34,080 --> 00:01:39,184
וזכרו, המניע שחשבנו למבנה השכבתי היה שאולי השכבה השנייה יכולה

21
00:01:39,184 --> 00:01:44,206
לקלוט את הקצוות, השכבה השלישית עשויה לקלוט דפוסים כמו לולאות

22
00:01:44,206 --> 00:01:49,640
וקווים, והאחרונה יכולה פשוט לחבר את הדפוסים האלה כדי לזהות ספרות.

23
00:01:49,640 --> 00:01:52,880
אז הנה, אנו לומדים כיצד הרשת לומדת.

24
00:01:52,880 --> 00:01:58,547
מה שאנחנו רוצים זה אלגוריתם שבו אתה יכול להראות לרשת הזו חבורה שלמה של נתוני אימון,

25
00:01:58,547 --> 00:02:04,350
שמגיעים בצורה של חבורה של תמונות שונות של ספרות בכתב יד, יחד עם תוויות למה שהם אמורים

26
00:02:04,350 --> 00:02:10,220
להיות, וזה יהיה להתאים את 13,000 המשקולות וההטיות האלה כדי לשפר את הביצועים שלה בנתוני

27
00:02:10,220 --> 00:02:10,760
האימון.

28
00:02:10,760 --> 00:02:17,840
יש לקוות שהמבנה השכבתי הזה יגרום לכך שמה שהוא לומד מתכלל לתמונות מעבר לנתוני האימון האלה.

29
00:02:17,840 --> 00:02:24,596
הדרך שבה אנו בודקים זאת היא שאחרי שאתה מאמן את הרשת, אתה מראה לה יותר

30
00:02:24,596 --> 00:02:31,160
נתונים מתויגים, ותראה באיזו מידה היא מסווגת את התמונות החדשות האלה.

31
00:02:31,160 --> 00:02:35,519
למזלנו, ומה שהופך זאת לדוגמא נפוצה מלכתחילה, הוא שהאנשים

32
00:02:35,519 --> 00:02:40,185
הטובים מאחורי מסד הנתונים של MNIST הרכיבו אוסף של עשרות אלפי

33
00:02:40,185 --> 00:02:45,080
תמונות ספרתיות בכתב יד, כל אחת מסומנת במספרים שהם אמורים להיות.

34
00:02:45,080 --> 00:02:50,180
ועד כמה שזה פרובוקטיבי לתאר מכונה כלמידה, ברגע שאתה רואה איך זה עובד, זה

35
00:02:50,180 --> 00:02:55,560
מרגיש הרבה פחות כמו איזו הנחת מדע בדיוני מטורפת, והרבה יותר כמו תרגיל חשבון.

36
00:02:55,560 --> 00:03:01,040
כלומר, בעצם זה מסתכם במציאת המינימום של פונקציה מסוימת.

37
00:03:01,040 --> 00:03:07,502
זכור, מבחינה רעיונית אנו חושבים על כל נוירון כמקושר לכל הנוירונים בשכבה הקודמת,

38
00:03:07,502 --> 00:03:13,802
והמשקלים בסכום המשוקלל המגדירים את הפעלתו דומים לנקודות החוזק של הקשרים הללו,

39
00:03:13,802 --> 00:03:19,780
וההטיה היא אינדיקציה כלשהי של האם הנוירון הזה נוטה להיות פעיל או לא פעיל.

40
00:03:19,780 --> 00:03:22,428
וכדי להתחיל בדברים, אנחנו פשוט הולכים לאתחל את

41
00:03:22,428 --> 00:03:25,020
כל המשקולות וההטיות האלה באופן אקראי לחלוטין.

42
00:03:25,020 --> 00:03:27,886
מיותר לציין שהרשת הזו הולכת להופיע בצורה איומה

43
00:03:27,886 --> 00:03:31,180
בדוגמה לאימון נתון, מכיוון שהיא פשוט עושה משהו אקראי.

44
00:03:31,180 --> 00:03:36,820
לדוגמה, אתה מזין את התמונה הזו של 3, ושכבת הפלט פשוט נראית כמו בלגן.

45
00:03:36,820 --> 00:03:42,796
אז מה שאתה עושה זה להגדיר פונקציית עלות, דרך לומר למחשב, לא, מחשב גרוע,

46
00:03:42,796 --> 00:03:48,940
שלפלט צריך להיות הפעלות שהן 0 עבור רוב הנוירונים, אבל 1 עבור הנוירון הזה.

47
00:03:48,940 --> 00:03:51,740
מה שנתת לי זה זבל מוחלט.

48
00:03:51,740 --> 00:03:58,757
אם לומר את זה בצורה קצת יותר מתמטית, אתה מחבר את הריבועים של ההבדלים בין כל אחת מאותן

49
00:03:58,757 --> 00:04:06,020
הפעלת פלט אשפה לבין הערך שאתה רוצה שיהיה להם, וזה מה שנכנה את העלות של דוגמה אחת לאימון.

50
00:04:06,020 --> 00:04:12,171
שימו לב שהסכום הזה קטן כאשר הרשת מסווגת בבטחה את התמונה בצורה

51
00:04:12,171 --> 00:04:18,820
נכונה, אך הוא גדול כאשר הרשת נראית כאילו היא לא יודעת מה היא עושה.

52
00:04:18,820 --> 00:04:23,386
אז מה שאתה עושה זה לשקול את העלות הממוצעת על פני

53
00:04:23,386 --> 00:04:27,580
כל עשרות אלפי דוגמאות ההדרכה העומדות לרשותך.

54
00:04:27,580 --> 00:04:33,300
העלות הממוצעת הזו היא המדד שלנו לכמה הרשת גרועה ועד כמה המחשב אמור להרגיש רע.

55
00:04:33,300 --> 00:04:35,300
וזה דבר מסובך.

56
00:04:35,300 --> 00:04:42,669
זוכרים איך הרשת עצמה הייתה בעצם פונקציה, כזו שמקבלת 784 מספרים כקלט, את ערכי הפיקסלים,

57
00:04:42,669 --> 00:04:49,700
ויורקת 10 מספרים כפלט שלה, ובמובן מסוים היא מוגדרת על ידי כל המשקלים וההטיות האלה?

58
00:04:49,700 --> 00:04:53,340
פונקציית העלות היא שכבה של מורכבות נוסף על כך.

59
00:04:53,340 --> 00:05:01,106
הוא לוקח כקלט את כ-13,000 המשקלים וההטיות האלה, ויורק מספר בודד שמתאר כמה רעים המשקלים

60
00:05:01,106 --> 00:05:09,140
וההטיות האלה, והאופן שבו הם מוגדרים תלוי בהתנהגות הרשת על פני כל עשרות אלפי נתוני האימון.

61
00:05:09,140 --> 00:05:12,460
זה הרבה לחשוב על זה.

62
00:05:12,460 --> 00:05:16,380
אבל רק להגיד למחשב איזו עבודה מחורבן הוא עושה זה לא מאוד מועיל.

63
00:05:16,380 --> 00:05:21,300
אתה רוצה להגיד לו איך לשנות את המשקולות וההטיות האלה כדי שזה ישתפר.

64
00:05:21,300 --> 00:05:26,497
כדי להקל, במקום להתאמץ לדמיין פונקציה עם 13,000 כניסות, פשוט

65
00:05:26,497 --> 00:05:31,440
דמיינו פונקציה פשוטה שיש לה מספר אחד כקלט ומספר אחד כפלט.

66
00:05:31,440 --> 00:05:36,420
איך מוצאים קלט שממזער את הערך של הפונקציה הזו?

67
00:05:36,420 --> 00:05:41,346
תלמידי החשבון יידעו שלפעמים אתה יכול להבין את המינימום הזה במפורש,

68
00:05:41,346 --> 00:05:46,419
אבל זה לא תמיד אפשרי עבור פונקציות מסובכות באמת, בטח לא בגרסת 13,000

69
00:05:46,419 --> 00:05:51,640
הקלט של המצב הזה עבור פונקציית עלות הרשת העצבית המסובכת והמטורפת שלנו.

70
00:05:51,640 --> 00:05:56,196
טקטיקה גמישה יותר היא להתחיל בכל קלט, ולהבין באיזה

71
00:05:56,196 --> 00:05:59,860
כיוון עליך לצעוד כדי להוריד את הפלט הזה.

72
00:05:59,860 --> 00:06:06,290
באופן ספציפי, אם אתה יכול להבין את השיפוע של הפונקציה במקום שבו אתה נמצא,

73
00:06:06,290 --> 00:06:12,720
אז הזז שמאלה אם השיפוע הזה חיובי, והסט את הקלט ימינה אם השיפוע הזה שלילי.

74
00:06:12,720 --> 00:06:16,731
אם תעשה זאת שוב ושוב, בכל נקודה שתבדוק את השיפוע החדש ותנקוט את

75
00:06:16,731 --> 00:06:20,680
הצעד המתאים, אתה הולך להתקרב למינימום מקומי כלשהו של הפונקציה.

76
00:06:20,680 --> 00:06:24,600
והתמונה שאולי יש לך בראש כאן היא כדור שמתגלגל במורד גבעה.

77
00:06:24,600 --> 00:06:29,598
ושימו לב, אפילו עבור פונקציית הקלט הבודדת המפושטת הזו, ישנם עמקים אפשריים

78
00:06:29,598 --> 00:06:34,529
רבים שאתם עשויים לנחות בהם, תלוי באיזה קלט אקראי אתם מתחילים, ואין ערובה

79
00:06:34,529 --> 00:06:39,460
שהמינימום המקומי בו תנחתו יהיה הערך הקטן ביותר האפשרי של פונקציית העלות.

80
00:06:39,460 --> 00:06:43,180
זה יעבור גם למקרה של הרשת העצבית שלנו.

81
00:06:43,180 --> 00:06:49,522
ואני גם רוצה שתשים לב איך אם אתה הופך את גודל הצעדים שלך לפרופורציונלי למדרון, אז

82
00:06:49,522 --> 00:06:56,020
כשהשיפוע משתטח לכיוון המינימום, הצעדים שלך הולכים וקטנים, וזה סוג של עוזר לך לחרוג.

83
00:06:56,020 --> 00:07:01,640
להגביר מעט את המורכבות, דמיינו במקום זאת פונקציה עם שתי כניסות ופלט אחד.

84
00:07:01,640 --> 00:07:09,020
אתה יכול לחשוב על מרחב הקלט כמישור ה-xy, ועל פונקציית העלות כמתוארת בגרף כמשטח מעליו.

85
00:07:09,020 --> 00:07:14,360
במקום לשאול לגבי שיפוע הפונקציה, עליך לשאול באיזה כיוון עליך לצעוד

86
00:07:14,360 --> 00:07:19,780
במרחב הקלט הזה כדי להקטין את הפלט של הפונקציה במהירות הגבוהה ביותר.

87
00:07:19,780 --> 00:07:22,340
במילים אחרות, מה כיוון הירידה?

88
00:07:22,340 --> 00:07:26,740
ושוב, זה מועיל לחשוב על כדור שמתגלגל במורד הגבעה.

89
00:07:26,740 --> 00:07:32,836
מי מכם שמכיר את החשבון הרב-משתני יודע שהשיפוע של פונקציה נותן לכם את כיוון

90
00:07:32,836 --> 00:07:39,420
העלייה התלולה ביותר, לאיזה כיוון כדאי לצעוד כדי להגדיל את הפונקציה המהירה ביותר.

91
00:07:39,420 --> 00:07:47,460
באופן טבעי, לקיחת השלילי של השיפוע הזה נותן לך את הכיוון לצעד שמקטין את הפונקציה הכי מהר.

92
00:07:47,460 --> 00:07:51,059
אפילו יותר מזה, אורכו של וקטור השיפוע הזה הוא

93
00:07:51,059 --> 00:07:54,580
אינדיקציה למידת התלול של המדרון התלול ביותר.

94
00:07:54,580 --> 00:07:57,681
עכשיו אם אינך מכיר חשבון רב-משתני ומעוניין ללמוד

95
00:07:57,681 --> 00:08:01,100
עוד, בדוק חלק מהעבודות שעשיתי עבור אקדמיית חאן בנושא.

96
00:08:01,100 --> 00:08:06,529
אבל בכנות, כל מה שחשוב לך ולי כרגע זה שבאופן עקרוני קיימת דרך לחשב

97
00:08:06,529 --> 00:08:12,040
את הווקטור הזה, הווקטור הזה שאומר לך מה כיוון הירידה וכמה הוא תלול.

98
00:08:12,040 --> 00:08:17,280
אתה תהיה בסדר אם זה כל מה שאתה יודע ואתה לא איתן בפרטים.

99
00:08:17,280 --> 00:08:22,416
כי אם אתה יכול לקבל את זה, האלגוריתם למזער את הפונקציה הוא לחשב את

100
00:08:22,416 --> 00:08:27,400
כיוון השיפוע הזה, ואז לקחת צעד קטן במורד, ולחזור על זה שוב ושוב.

101
00:08:27,400 --> 00:08:33,700
זה אותו רעיון בסיסי לפונקציה שיש לה 13,000 כניסות במקום 2 כניסות.

102
00:08:33,700 --> 00:08:40,180
תאר לעצמך לארגן את כל 13,000 המשקלים וההטיות של הרשת שלנו לתוך וקטור עמודות ענק.

103
00:08:40,180 --> 00:08:48,220
השיפוע השלילי של פונקציית העלות הוא רק וקטור, זה כיוון כלשהו בתוך מרחב הקלט העצום בטירוף

104
00:08:48,220 --> 00:08:55,900
הזה שאומר לך אילו דחיפות לכל המספרים האלה יגרמו לירידה המהירה ביותר לפונקציית העלות.

105
00:08:55,900 --> 00:09:01,073
וכמובן, עם פונקציית העלות שתוכננה במיוחד שלנו, שינוי המשקלים וההטיות כדי

106
00:09:01,073 --> 00:09:06,106
להקטין את זה אומר לגרום לתפוקת הרשת על כל נתוני אימון להיראות פחות כמו

107
00:09:06,106 --> 00:09:11,280
מערך אקראי של 10 ערכים, ויותר כמו החלטה אמיתית שאנחנו רוצים את זה לעשות.

108
00:09:11,280 --> 00:09:17,526
חשוב לזכור, פונקציית עלות זו כוללת ממוצע של כל נתוני האימון, כך

109
00:09:17,526 --> 00:09:24,260
שאם אתה ממזער אותו, זה אומר שזה ביצועים טובים יותר בכל הדגימות הללו.

110
00:09:24,260 --> 00:09:29,186
האלגוריתם לחישוב שיפוע זה ביעילות, שהוא למעשה הלב של האופן שבו רשת

111
00:09:29,186 --> 00:09:34,040
עצבית לומדת, נקרא התפשטות לאחור, ועל זה אני הולך לדבר בסרטון הבא.

112
00:09:34,040 --> 00:09:40,970
שם, אני באמת רוצה להקדיש זמן כדי לעבור על מה בדיוק קורה לכל משקל והטיה עבור נתוני אימון

113
00:09:40,970 --> 00:09:47,980
נתון, מנסה לתת תחושה אינטואיטיבית של מה שקורה מעבר לערימת החישובים והנוסחאות הרלוונטיות.

114
00:09:47,980 --> 00:09:53,575
בדיוק כאן, כרגע, הדבר העיקרי שאני רוצה שתדע, ללא תלות בפרטי הטמעה, הוא שמה

115
00:09:53,575 --> 00:09:59,320
שאנחנו מתכוונים כשאנחנו מדברים על למידה ברשת הוא שזה רק מזעור פונקציית עלות.

116
00:09:59,320 --> 00:10:04,257
ושימו לב, תוצאה אחת של זה היא שחשוב שלפונקציית העלות הזו תהיה תפוקה

117
00:10:04,257 --> 00:10:09,340
חלקה ונחמדה, כדי שנוכל למצוא מינימום מקומי על ידי צעדים קטנים בירידה.

118
00:10:09,340 --> 00:10:14,719
זו הסיבה, אגב, לנוירונים מלאכותיים יש הפעלה מתמשכת, במקום פשוט

119
00:10:14,719 --> 00:10:20,440
להיות פעילים או לא פעילים בצורה בינארית, כמו הנוירונים הביולוגיים.

120
00:10:20,440 --> 00:10:23,733
תהליך זה של דחיפה חוזרת ונשנית של קלט של פונקציה

121
00:10:23,733 --> 00:10:26,960
בכפולה כלשהי של השיפוע השלילי נקרא ירידה בדרגה.

122
00:10:26,960 --> 00:10:33,000
זו דרך להתכנס למינימום מקומי כלשהו של פונקציית עלות, בעצם עמק בגרף הזה.

123
00:10:33,000 --> 00:10:38,997
אני עדיין מראה את התמונה של פונקציה עם שתי כניסות, כמובן, כי דחיפות בחלל קלט של

124
00:10:38,997 --> 00:10:45,220
13,000 מימדים קצת קשה לעטוף את דעתך, אבל למעשה יש דרך נחמדה לא מרחבית לחשוב על זה.

125
00:10:45,220 --> 00:10:49,100
כל רכיב של הגרדיאנט השלילי אומר לנו שני דברים.

126
00:10:49,100 --> 00:10:55,860
הסימן, כמובן, אומר לנו אם יש להזיז את הרכיב המתאים של וקטור הקלט למעלה או למטה.

127
00:10:55,860 --> 00:11:05,620
אבל חשוב מכך, הגדלים היחסיים של כל הרכיבים האלה מעידים לך אילו שינויים חשובים יותר.

128
00:11:05,620 --> 00:11:10,480
אתה מבין, ברשת שלנו, התאמה לאחד המשקולות עשויה להשפיע

129
00:11:10,480 --> 00:11:14,980
הרבה יותר על פונקציית העלות מאשר התאמה למשקל אחר.

130
00:11:14,980 --> 00:11:19,440
חלק מהחיבורים האלה פשוט חשובים יותר לנתוני האימונים שלנו.

131
00:11:19,440 --> 00:11:24,204
אז הדרך שבה אתה יכול לחשוב על וקטור השיפוע הזה של פונקציית העלות

132
00:11:24,204 --> 00:11:28,969
המאסיבית שלנו, המעוותת את המוח, היא שהוא מקודד את החשיבות היחסית

133
00:11:28,969 --> 00:11:34,100
של כל משקל והטיה, כלומר, איזה מהשינויים האלה יביא הכי הרבה כסף עבורך.

134
00:11:34,100 --> 00:11:37,360
זו באמת רק עוד דרך לחשוב על כיוון.

135
00:11:37,360 --> 00:11:43,954
אם לקחת דוגמה פשוטה יותר, אם יש לך איזושהי פונקציה עם שני משתנים כקלט, ומחשבת שהשיפוע

136
00:11:43,954 --> 00:11:50,318
שלה בנקודה מסוימת יוצאת כ-3,1, אז מצד אחד אתה יכול לפרש את זה כאילו אתה אומר שכאשר

137
00:11:50,318 --> 00:11:56,682
אתה עמידה בקלט הזה, נע לאורך הכיוון הזה מגדיל את הפונקציה הכי מהר, שכאשר אתה משרטט

138
00:11:56,682 --> 00:12:03,200
את הפונקציה מעל מישור נקודות הקלט, הווקטור הזה הוא מה שנותן לך את כיוון העלייה הישר.

139
00:12:03,200 --> 00:12:07,890
אבל דרך נוספת לקרוא היא לומר שלשינויים במשתנה הראשון הזה יש

140
00:12:07,890 --> 00:12:12,580
חשיבות פי שלושה מאשר לשינויים במשתנה השני, שלפחות בסביבה של

141
00:12:12,580 --> 00:12:17,740
הקלט הרלוונטי, דחיפה של ערך ה-x גוררת הרבה יותר מרץ עבורך דוֹלָר.

142
00:12:17,740 --> 00:12:22,880
בסדר, בוא נתרחק ונסכם איפה אנחנו עד עכשיו.

143
00:12:22,880 --> 00:12:26,514
הרשת עצמה היא הפונקציה הזו עם 784 כניסות ו-10

144
00:12:26,514 --> 00:12:30,860
יציאות, המוגדרות במונחים של כל הסכומים המשוקללים הללו.

145
00:12:30,860 --> 00:12:34,160
פונקציית העלות היא שכבה של מורכבות נוסף על כך.

146
00:12:34,160 --> 00:12:38,400
הוא לוקח את 13,000 המשקולות וההטיות בתור תשומות,

147
00:12:38,400 --> 00:12:42,640
ויורק מידה אחת של עלוב בהתבסס על דוגמאות האימון.

148
00:12:42,640 --> 00:12:47,520
השיפוע של פונקציית העלות הוא עוד שכבה אחת של מורכבות.

149
00:12:47,520 --> 00:12:55,373
זה אומר לנו אילו דחיפות לכל המשקולות וההטיות הללו גורמות לשינוי המהיר ביותר בערך של

150
00:12:55,373 --> 00:13:03,040
פונקציית העלות, שאותו אתה עשוי לפרש כאומר אילו שינויים לאיזה משקלים חשובים ביותר.

151
00:13:03,040 --> 00:13:08,675
אז כשאתה מאתחל את הרשת עם משקלים והטיות אקראיות, ומתאים אותם פעמים רבות בהתבסס

152
00:13:08,675 --> 00:13:14,240
על תהליך הירידה בשיפוע הזה, עד כמה היא באמת מתפקדת בתמונות שלא נראו קודם לכן?

153
00:13:14,240 --> 00:13:20,312
זה שתיארתי כאן, עם שתי השכבות הנסתרות של 16 נוירונים כל אחת, שנבחרו

154
00:13:20,312 --> 00:13:26,920
בעיקר מסיבות אסתטיות, לא רע, ומסווג כ-96% מהתמונות החדשות שהוא רואה נכון.

155
00:13:26,920 --> 00:13:36,300
ובכנות, אם אתה מסתכל על כמה מהדוגמאות שהוא מבלגן בהן, אתה מרגיש נאלץ לחתוך את זה קצת.

156
00:13:36,300 --> 00:13:41,220
אם אתה משחק עם מבנה השכבה הנסתרת ותעשה כמה שינויים, אתה יכול להשיג את זה עד 98%.

157
00:13:41,220 --> 00:13:42,900
וזה די טוב!

158
00:13:42,900 --> 00:13:47,489
זה לא הכי טוב, אתה בהחלט יכול להשיג ביצועים טובים יותר על ידי ביצוע

159
00:13:47,489 --> 00:13:51,876
מתוחכם יותר מרשת הוניל הפשוטה הזו, אבל בהתחשב בכמה מרתיעה המשימה

160
00:13:51,876 --> 00:13:56,668
הראשונית, אני חושב שיש משהו מדהים בכל רשת שעושה את זה טוב בתמונות שהיא

161
00:13:56,668 --> 00:14:02,000
מעולם לא נראתה בעבר בהתחשב בכך שאנחנו מעולם לא אמר לו במפורש אילו דפוסים לחפש.

162
00:14:02,000 --> 00:14:07,268
במקור, הדרך שבה הנעתי את המבנה הזה הייתה על ידי תיאור תקווה שאולי תהיה לנו,

163
00:14:07,268 --> 00:14:12,674
שהשכבה השנייה עשויה לקלוט קצוות קטנים, שהשכבה השלישית תחבר את הקצוות האלה כדי

164
00:14:12,674 --> 00:14:18,220
לזהות לולאות וקווים ארוכים יותר, ושהם עשויים להיות חתוכים. יחד כדי לזהות ספרות.

165
00:14:18,220 --> 00:14:21,040
אז זה מה שהרשת שלנו עושה בעצם?

166
00:14:21,040 --> 00:14:24,880
ובכן, עבור זה לפחות, בכלל לא.

167
00:14:24,880 --> 00:14:30,938
זוכרים איך בסרטון האחרון הסתכלנו כיצד ניתן להמחיש את משקלי החיבורים מכל הנוירונים

168
00:14:30,938 --> 00:14:37,440
בשכבה הראשונה לנוירון נתון בשכבה השנייה כתבנית פיקסלים נתונה שנוירון השכבה השנייה קולט?

169
00:14:37,440 --> 00:14:46,019
ובכן, כשאנחנו עושים את זה עבור המשקולות הקשורות למעברים האלה, במקום לקלוט קצוות קטנים

170
00:14:46,019 --> 00:14:54,200
מבודדים פה ושם, הם נראים, ובכן, כמעט אקראיים, רק עם כמה דפוסים רופפים מאוד באמצע.

171
00:14:54,200 --> 00:14:59,494
נראה שבמרחב הבלתי נתפס של 13,000 ממדים של משקלים והטיות אפשריות,

172
00:14:59,494 --> 00:15:04,463
הרשת שלנו מצאה את עצמה מינימום מקומי קטן ומשמח, שלמרות סיווג

173
00:15:04,463 --> 00:15:09,840
מוצלח של רוב התמונות, לא בדיוק קולט את הדפוסים שאולי קיווינו להם.

174
00:15:09,840 --> 00:15:14,600
וכדי באמת להסיע את הנקודה הזו הביתה, צפה במה שקורה כשאתה מזין תמונה אקראית.

175
00:15:14,600 --> 00:15:19,571
אם המערכת הייתה חכמה, אולי הייתם מצפים שהיא תרגיש לא בטוחה, אולי לא

176
00:15:19,571 --> 00:15:24,689
באמת תפעיל אף אחד מ-10 נוירוני הפלט האלה או תפעיל את כולם באופן שווה,

177
00:15:24,689 --> 00:15:29,807
אבל במקום זאת היא נותנת לכם בביטחון איזו תשובה שטות, כאילו היא מרגישה

178
00:15:29,807 --> 00:15:34,560
בטוחה שזה אקראי רעש הוא 5 כפי שהוא עושה שתמונה בפועל של 5 היא 5.

179
00:15:34,560 --> 00:15:41,800
בניסוח שונה, גם אם הרשת הזו יכולה לזהות ספרות די טוב, אין לה מושג איך לצייר אותן.

180
00:15:41,800 --> 00:15:45,400
הרבה מזה נובע מכך שזה מערך אימון כל כך מוגבל.

181
00:15:45,400 --> 00:15:48,220
כלומר, שימו את עצמכם בנעלי הרשת כאן.

182
00:15:48,220 --> 00:15:55,150
מנקודת המבט שלו, היקום כולו אינו מורכב מכלום מלבד ספרות לא זזות מוגדרות בבירור ובמרכזן

183
00:15:55,150 --> 00:16:02,160
רשת זעירה, ותפקוד העלות שלו מעולם לא נתן לו שום תמריץ להיות אלא בטוח לחלוטין בהחלטותיו.

184
00:16:02,160 --> 00:16:06,121
אז עם זה בתור הדימוי של מה שהנוירונים בשכבה השנייה באמת עושים, אתם

185
00:16:06,121 --> 00:16:10,320
עשויים לתהות מדוע אציג את הרשת הזו עם מוטיבציה של לקלוט קצוות ודפוסים.

186
00:16:10,320 --> 00:16:13,040
כלומר, זה פשוט בכלל לא מה שזה בסופו של דבר עושה.

187
00:16:13,040 --> 00:16:17,480
ובכן, זו לא אמורה להיות המטרה הסופית שלנו, אלא נקודת התחלה.

188
00:16:17,480 --> 00:16:24,560
למען האמת, זו טכנולוגיה ישנה, מהסוג שנחקר בשנות ה-80 וה-90, ואתה צריך להבין אותה לפני

189
00:16:24,560 --> 00:16:31,640
שתוכל להבין גרסאות מודרניות מפורטות יותר, וברור שהיא מסוגלת לפתור כמה בעיות מעניינות,

190
00:16:31,640 --> 00:16:38,720
אבל ככל שתחפור יותר במה השכבות הנסתרות האלה באמת עושות, ככל שזה נראה פחות אינטליגנטי.

191
00:16:38,720 --> 00:16:42,978
העברת הפוקוס לרגע מהאופן שבו רשתות לומדות לאופן שבו אתה

192
00:16:42,978 --> 00:16:47,160
לומד, זה יקרה רק אם תעסוק באופן פעיל בחומר כאן איכשהו.

193
00:16:47,160 --> 00:16:51,913
דבר אחד די פשוט שאני רוצה שתעשה הוא פשוט לעצור ברגע זה ולחשוב

194
00:16:51,913 --> 00:16:56,973
לרגע לעומק אילו שינויים אתה עשוי לעשות במערכת הזו וכיצד היא תופסת

195
00:16:56,973 --> 00:17:01,880
תמונות אם אתה רוצה שהיא תקלוט טוב יותר דברים כמו קצוות ודפוסים.

196
00:17:01,880 --> 00:17:05,724
אבל יותר מזה, כדי לעסוק באמת בחומר, אני ממליץ בחום

197
00:17:05,724 --> 00:17:09,720
על ספרו של מייקל נילסן על למידה עמוקה ורשתות עצביות.

198
00:17:09,720 --> 00:17:14,353
בו, אתה יכול למצוא את הקוד ואת הנתונים להורדה ולשחק איתם עבור

199
00:17:14,353 --> 00:17:19,360
הדוגמה המדויקת הזו, והספר ידריך אותך צעד אחר צעד מה הקוד הזה עושה.

200
00:17:19,360 --> 00:17:23,776
מה שמדהים הוא שהספר הזה הוא חינמי וזמין לציבור, אז אם אתה

201
00:17:23,776 --> 00:17:28,040
מפיק ממנו משהו, שקול להצטרף אלי לתרום למאמצים של נילסן.

202
00:17:28,040 --> 00:17:33,192
קישרתי גם כמה משאבים אחרים שאני מאוד אוהב בתיאור, כולל

203
00:17:33,192 --> 00:17:38,720
פוסט הבלוג הפנומנלי והיפה של כריס אולה והמאמרים ב-Distill.

204
00:17:38,720 --> 00:17:41,611
כדי לסגור את העניינים כאן לדקות האחרונות, אני

205
00:17:41,611 --> 00:17:44,440
רוצה לחזור לקטע מהראיון שהיה לי עם ליישה לי.

206
00:17:44,440 --> 00:17:48,520
אתה אולי זוכר אותה מהסרטון האחרון, היא עשתה את עבודת הדוקטורט שלה בלמידה עמוקה.

207
00:17:48,520 --> 00:17:52,483
בקטע הקטן הזה, היא מדברת על שני מאמרים אחרונים שבאמת חופרים

208
00:17:52,483 --> 00:17:56,380
כיצד חלק מרשתות זיהוי התמונות המודרניות יותר לומדות למעשה.

209
00:17:56,380 --> 00:18:00,464
רק כדי להגדיר היכן היינו בשיחה, המאמר הראשון לקח את אחת מהרשתות

210
00:18:00,464 --> 00:18:04,932
הנוירוניות העמוקות במיוחד האלה שמאוד טובות בזיהוי תמונות, ובמקום לאמן

211
00:18:04,932 --> 00:18:09,400
אותה על מערך נתונים מסומן כהלכה, הוא עירבב את כל התוויות לפני האימון.

212
00:18:09,400 --> 00:18:15,320
ברור שדיוק הבדיקה כאן לא היה טוב יותר מאקראי, מכיוון שהכל פשוט מסומן באקראי.

213
00:18:15,320 --> 00:18:21,440
אבל זה עדיין היה מסוגל להשיג את אותו דיוק אימון כפי שאתה משיג על מערך נתונים מסומן כהלכה.

214
00:18:21,440 --> 00:18:26,366
בעיקרון, מיליוני המשקלים של הרשת הספציפית הזו הספיקו לה רק

215
00:18:26,366 --> 00:18:31,626
לשנן את הנתונים האקראיים, מה שמעלה את השאלה האם מזעור פונקציית

216
00:18:31,626 --> 00:18:36,720
העלות הזו אכן מתאים לכל סוג של מבנה בתמונה, או שזה רק שינון?

217
00:18:36,720 --> 00:18:40,120
. . . לשנן את כל מערך הנתונים של מהו הסיווג הנכון.

218
00:18:40,120 --> 00:18:46,132
אז כמה, אתם יודעים, חצי שנה מאוחר יותר ב-ICML השנה, לא היה בדיוק נייר הפרכה, אלא

219
00:18:46,132 --> 00:18:52,220
נייר שהתייחס לכמה היבטים כמו, היי, למעשה הרשתות האלה עושות משהו קצת יותר חכם מזה.

220
00:18:52,220 --> 00:18:58,535
אם אתה מסתכל על עקומת הדיוק הזו, אם רק היית מתאמן על מערך נתונים

221
00:18:58,535 --> 00:19:05,240
אקראי, העקומה הזו ירדה מאוד, אתה יודע, לאט מאוד בצורה כמעט ליניארית.

222
00:19:05,240 --> 00:19:08,676
אז אתה באמת מתקשה למצוא את המינימום המקומי הזה של

223
00:19:08,676 --> 00:19:12,320
המשקולות האפשריות, אתה יודע, שיביאו לך את הדיוק הזה.

224
00:19:12,320 --> 00:19:17,876
בעוד שאם אתה מתאמן על מערך נתונים מובנה, כזה שיש לו את התוויות הנכונות, אתה

225
00:19:17,876 --> 00:19:23,360
יודע, אתה מתעסק קצת בהתחלה, אבל אז ירדת מהר מאוד כדי להגיע לרמת הדיוק הזו.

226
00:19:23,360 --> 00:19:28,580
ובמובן מסוים היה קל יותר למצוא את המקסימום המקומי הזה.

227
00:19:28,580 --> 00:19:33,981
אז מה שהיה מעניין בזה הוא שהוא מביא לאור עוד מאמר

228
00:19:33,981 --> 00:19:40,140
מלפני כמה שנים, שיש בו הרבה יותר הפשטות לגבי שכבות הרשת.

229
00:19:40,140 --> 00:19:44,692
אבל אחת התוצאות אמרה כיצד, אם מסתכלים על נוף האופטימיזציה,

230
00:19:44,692 --> 00:19:49,400
המינימום המקומי שרשתות אלו נוטות ללמוד הם למעשה באיכות שווה.

231
00:19:49,400 --> 00:19:51,876
אז במובן מסוים, אם מערך הנתונים שלך מובנה, אתה

232
00:19:51,876 --> 00:19:54,300
אמור להיות מסוגל למצוא את זה הרבה יותר בקלות.

233
00:19:54,300 --> 00:20:01,140
תודתי כמו תמיד לאלו מכם שתומכים בפטראון.

234
00:20:01,140 --> 00:20:07,160
כבר אמרתי בעבר מה זה מחליף משחק בפטראון, אבל הסרטונים האלה באמת לא היו אפשריים בלעדיכם.

235
00:20:07,160 --> 00:20:10,443
אני גם רוצה להודות במיוחד לחברת ה-VC Amplify Partners

236
00:20:10,443 --> 00:20:13,240
ולתמיכה שלהם בסרטוני הווידאו הראשוניים בסדרה.

237
00:20:13,240 --> 00:20:33,140
תודה.

