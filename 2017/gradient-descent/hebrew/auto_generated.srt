1
00:00:00,000 --> 00:00:07,240
סרטון אחרון פרסמתי את המבנה של רשת עצבית.

2
00:00:07,240 --> 00:00:13,160
אני אתן כאן סיכום קצר כדי שזה יהיה רענן במוחנו, ואז יש לי שתי מטרות עיקריות לסרטון הזה.

3
00:00:13,160 --> 00:00:18,371
הראשון הוא להציג את הרעיון של ירידה בשיפוע, שעומד בבסיס לא רק כיצד רשתות עצביות לומדות,

4
00:00:18,371 --> 00:00:20,800
אלא כיצד פועלת גם הרבה למידת מכונה אחרת.

5
00:00:20,800 --> 00:00:25,380
ואז אחר כך נחפור קצת יותר כיצד הרשת הספציפית הזו מתפקדת,

6
00:00:25,380 --> 00:00:29,560
ומה בסופו של דבר השכבות הנסתרות של נוירונים מחפשות.

7
00:00:29,560 --> 00:00:34,815
כזכור, המטרה שלנו כאן היא הדוגמה הקלאסית של זיהוי ספרות בכתב יד,

8
00:00:34,815 --> 00:00:37,080
עולם השלום של רשתות עצביות.

9
00:00:37,080 --> 00:00:44,260
ספרות אלו מוצגות על גבי רשת של 28x28 פיקסלים, לכל פיקסל יש ערך בגווני אפור בין 0 ל-1.

10
00:00:44,260 --> 00:00:51,400
אלו הם שקובעים את הפעלות של 784 נוירונים בשכבת הקלט של הרשת.

11
00:00:51,400 --> 00:00:59,380
ההפעלה של כל נוירון בשכבות הבאות מבוססת על סכום משוקלל של כל הפעלות בשכבה הקודמת,

12
00:00:59,380 --> 00:01:02,300
בתוספת מספר מיוחד שנקרא הטיה.

13
00:01:02,300 --> 00:01:07,283
אתה מחבר את הסכום הזה עם פונקציה אחרת, כמו ה-squishification של סיגמואיד,

14
00:01:07,283 --> 00:01:09,640
או ReLU, כמו שעברתי בסרטון האחרון.

15
00:01:09,640 --> 00:01:16,494
בסך הכל, בהינתן הבחירה המעט שרירותית של שתי שכבות נסתרות עם 16 נוירונים כל אחת,

16
00:01:16,494 --> 00:01:21,035
לרשת יש כ-13,000 משקלים והטיות שאנחנו יכולים להתאים,

17
00:01:21,035 --> 00:01:25,320
והערכים האלה הם שקובעים מה בדיוק הרשת עושה בפועל.

18
00:01:25,320 --> 00:01:29,400
ולמה שאנחנו מתכוונים כשאנחנו אומרים שהרשת הזו מסווגת ספרה נתונה הוא

19
00:01:29,400 --> 00:01:34,080
שהנוירונים הבהירים ביותר מבין 10 הנוירונים האלה בשכבה הסופית מתאים לספרה הזו.

20
00:01:34,080 --> 00:01:40,583
וזכרו, המניע שחשבנו למבנה השכבתי היה שאולי השכבה השנייה יכולה לקלוט את הקצוות,

21
00:01:40,583 --> 00:01:44,864
השכבה השלישית עשויה לקלוט דפוסים כמו לולאות וקווים,

22
00:01:44,864 --> 00:01:49,640
והאחרונה יכולה פשוט לחבר את הדפוסים האלה כדי לזהות ספרות.

23
00:01:49,640 --> 00:01:52,880
אז הנה, אנו לומדים כיצד הרשת לומדת.

24
00:01:52,880 --> 00:01:58,547
מה שאנחנו רוצים זה אלגוריתם שבו אתה יכול להראות לרשת הזו חבורה שלמה של נתוני אימון,

25
00:01:58,547 --> 00:02:02,393
שמגיעים בצורה של חבורה של תמונות שונות של ספרות בכתב יד,

26
00:02:02,393 --> 00:02:08,331
יחד עם תוויות למה שהם אמורים להיות, וזה יהיה להתאים את 13,000 המשקולות וההטיות האלה כדי

27
00:02:08,331 --> 00:02:10,760
לשפר את הביצועים שלה בנתוני האימון.

28
00:02:10,760 --> 00:02:17,840
יש לקוות שהמבנה השכבתי הזה יגרום לכך שמה שהוא לומד מתכלל לתמונות מעבר לנתוני האימון האלה.

29
00:02:17,840 --> 00:02:22,955
הדרך שבה אנו בודקים זאת היא שאחרי שאתה מאמן את הרשת,

30
00:02:22,955 --> 00:02:31,160
אתה מראה לה יותר נתונים מתויגים, ותראה באיזו מידה היא מסווגת את התמונות החדשות האלה.

31
00:02:31,160 --> 00:02:35,519
למזלנו, ומה שהופך זאת לדוגמא נפוצה מלכתחילה, הוא שהאנשים

32
00:02:35,519 --> 00:02:42,020
הטובים מאחורי מסד הנתונים של MNIST הרכיבו אוסף של עשרות אלפי תמונות ספרתיות בכתב יד,

33
00:02:42,020 --> 00:02:45,080
כל אחת מסומנת במספרים שהם אמורים להיות.

34
00:02:45,080 --> 00:02:49,970
ועד כמה שזה פרובוקטיבי לתאר מכונה כלמידה, ברגע שאתה רואה איך זה עובד,

35
00:02:49,970 --> 00:02:55,560
זה מרגיש הרבה פחות כמו איזו הנחת מדע בדיוני מטורפת, והרבה יותר כמו תרגיל חשבון.

36
00:02:55,560 --> 00:03:01,040
כלומר, בעצם זה מסתכם במציאת המינימום של פונקציה מסוימת.

37
00:03:01,040 --> 00:03:07,502
זכור, מבחינה רעיונית אנו חושבים על כל נוירון כמקושר לכל הנוירונים בשכבה הקודמת,

38
00:03:07,502 --> 00:03:13,802
והמשקלים בסכום המשוקלל המגדירים את הפעלתו דומים לנקודות החוזק של הקשרים הללו,

39
00:03:13,802 --> 00:03:19,780
וההטיה היא אינדיקציה כלשהי של האם הנוירון הזה נוטה להיות פעיל או לא פעיל.

40
00:03:19,780 --> 00:03:22,428
וכדי להתחיל בדברים, אנחנו פשוט הולכים לאתחל את

41
00:03:22,428 --> 00:03:25,020
כל המשקולות וההטיות האלה באופן אקראי לחלוטין.

42
00:03:25,020 --> 00:03:29,106
מיותר לציין שהרשת הזו הולכת להופיע בצורה איומה בדוגמה לאימון נתון,

43
00:03:29,106 --> 00:03:31,180
מכיוון שהיא פשוט עושה משהו אקראי.

44
00:03:31,180 --> 00:03:36,820
לדוגמה, אתה מזין את התמונה הזו של 3, ושכבת הפלט פשוט נראית כמו בלגן.

45
00:03:36,820 --> 00:03:42,796
אז מה שאתה עושה זה להגדיר פונקציית עלות, דרך לומר למחשב, לא, מחשב גרוע,

46
00:03:42,796 --> 00:03:48,940
שלפלט צריך להיות הפעלות שהן 0 עבור רוב הנוירונים, אבל 1 עבור הנוירון הזה.

47
00:03:48,940 --> 00:03:51,740
מה שנתת לי זה זבל מוחלט.

48
00:03:51,740 --> 00:03:58,757
אם לומר את זה בצורה קצת יותר מתמטית, אתה מחבר את הריבועים של ההבדלים בין כל אחת מאותן

49
00:03:58,757 --> 00:04:06,020
הפעלת פלט אשפה לבין הערך שאתה רוצה שיהיה להם, וזה מה שנכנה את העלות של דוגמה אחת לאימון.

50
00:04:06,020 --> 00:04:12,866
שימו לב שהסכום הזה קטן כאשר הרשת מסווגת בבטחה את התמונה בצורה נכונה,

51
00:04:12,866 --> 00:04:18,820
אך הוא גדול כאשר הרשת נראית כאילו היא לא יודעת מה היא עושה.

52
00:04:18,820 --> 00:04:23,386
אז מה שאתה עושה זה לשקול את העלות הממוצעת על פני

53
00:04:23,386 --> 00:04:27,580
כל עשרות אלפי דוגמאות ההדרכה העומדות לרשותך.

54
00:04:27,580 --> 00:04:33,300
העלות הממוצעת הזו היא המדד שלנו לכמה הרשת גרועה ועד כמה המחשב אמור להרגיש רע.

55
00:04:33,300 --> 00:04:35,300
וזה דבר מסובך.

56
00:04:35,300 --> 00:04:42,669
זוכרים איך הרשת עצמה הייתה בעצם פונקציה, כזו שמקבלת 784 מספרים כקלט, את ערכי הפיקסלים,

57
00:04:42,669 --> 00:04:49,700
ויורקת 10 מספרים כפלט שלה, ובמובן מסוים היא מוגדרת על ידי כל המשקלים וההטיות האלה?

58
00:04:49,700 --> 00:04:53,340
פונקציית העלות היא שכבה של מורכבות נוסף על כך.

59
00:04:53,340 --> 00:04:57,624
הוא לוקח כקלט את כ-13,000 המשקלים וההטיות האלה,

60
00:04:57,624 --> 00:05:02,355
ויורק מספר בודד שמתאר כמה רעים המשקלים וההטיות האלה,

61
00:05:02,355 --> 00:05:09,140
והאופן שבו הם מוגדרים תלוי בהתנהגות הרשת על פני כל עשרות אלפי נתוני האימון.

62
00:05:09,140 --> 00:05:12,460
זה הרבה לחשוב על זה.

63
00:05:12,460 --> 00:05:16,380
אבל רק להגיד למחשב איזו עבודה מחורבן הוא עושה זה לא מאוד מועיל.

64
00:05:16,380 --> 00:05:21,300
אתה רוצה להגיד לו איך לשנות את המשקולות וההטיות האלה כדי שזה ישתפר.

65
00:05:21,300 --> 00:05:26,071
כדי להקל, במקום להתאמץ לדמיין פונקציה עם 13,000 כניסות,

66
00:05:26,071 --> 00:05:31,440
פשוט דמיינו פונקציה פשוטה שיש לה מספר אחד כקלט ומספר אחד כפלט.

67
00:05:31,440 --> 00:05:36,420
איך מוצאים קלט שממזער את הערך של הפונקציה הזו?

68
00:05:36,420 --> 00:05:41,346
תלמידי החשבון יידעו שלפעמים אתה יכול להבין את המינימום הזה במפורש,

69
00:05:41,346 --> 00:05:44,949
אבל זה לא תמיד אפשרי עבור פונקציות מסובכות באמת,

70
00:05:44,949 --> 00:05:51,566
בטח לא בגרסת 13,000 הקלט של המצב הזה עבור פונקציית עלות הרשת העצבית המסובכת והמטורפת שלנו.

71
00:05:51,566 --> 00:05:51,640


72
00:05:51,640 --> 00:05:56,196
טקטיקה גמישה יותר היא להתחיל בכל קלט, ולהבין באיזה

73
00:05:56,196 --> 00:05:59,860
כיוון עליך לצעוד כדי להוריד את הפלט הזה.

74
00:05:59,860 --> 00:06:06,290
באופן ספציפי, אם אתה יכול להבין את השיפוע של הפונקציה במקום שבו אתה נמצא,

75
00:06:06,290 --> 00:06:12,720
אז הזז שמאלה אם השיפוע הזה חיובי, והסט את הקלט ימינה אם השיפוע הזה שלילי.

76
00:06:12,720 --> 00:06:17,546
אם תעשה זאת שוב ושוב, בכל נקודה שתבדוק את השיפוע החדש ותנקוט את הצעד המתאים,

77
00:06:17,546 --> 00:06:20,680
אתה הולך להתקרב למינימום מקומי כלשהו של הפונקציה.

78
00:06:20,680 --> 00:06:24,600
והתמונה שאולי יש לך בראש כאן היא כדור שמתגלגל במורד גבעה.

79
00:06:24,600 --> 00:06:28,315
ושימו לב, אפילו עבור פונקציית הקלט הבודדת המפושטת הזו,

80
00:06:28,315 --> 00:06:33,786
ישנם עמקים אפשריים רבים שאתם עשויים לנחות בהם, תלוי באיזה קלט אקראי אתם מתחילים,

81
00:06:33,786 --> 00:06:39,460
ואין ערובה שהמינימום המקומי בו תנחתו יהיה הערך הקטן ביותר האפשרי של פונקציית העלות.

82
00:06:39,460 --> 00:06:43,180
זה יעבור גם למקרה של הרשת העצבית שלנו.

83
00:06:43,180 --> 00:06:49,290
ואני גם רוצה שתשים לב איך אם אתה הופך את גודל הצעדים שלך לפרופורציונלי למדרון,

84
00:06:49,290 --> 00:06:56,020
אז כשהשיפוע משתטח לכיוון המינימום, הצעדים שלך הולכים וקטנים, וזה סוג של עוזר לך לחרוג.

85
00:06:56,020 --> 00:07:01,640
להגביר מעט את המורכבות, דמיינו במקום זאת פונקציה עם שתי כניסות ופלט אחד.

86
00:07:01,640 --> 00:07:09,020
אתה יכול לחשוב על מרחב הקלט כמישור ה-xy, ועל פונקציית העלות כמתוארת בגרף כמשטח מעליו.

87
00:07:09,020 --> 00:07:14,360
במקום לשאול לגבי שיפוע הפונקציה, עליך לשאול באיזה כיוון עליך לצעוד

88
00:07:14,360 --> 00:07:19,780
במרחב הקלט הזה כדי להקטין את הפלט של הפונקציה במהירות הגבוהה ביותר.

89
00:07:19,780 --> 00:07:22,340
במילים אחרות, מה כיוון הירידה?

90
00:07:22,340 --> 00:07:26,740
ושוב, זה מועיל לחשוב על כדור שמתגלגל במורד הגבעה.

91
00:07:26,740 --> 00:07:32,836
מי מכם שמכיר את החשבון הרב-משתני יודע שהשיפוע של פונקציה נותן לכם את כיוון

92
00:07:32,836 --> 00:07:39,420
העלייה התלולה ביותר, לאיזה כיוון כדאי לצעוד כדי להגדיל את הפונקציה המהירה ביותר.

93
00:07:39,420 --> 00:07:47,460
באופן טבעי, לקיחת השלילי של השיפוע הזה נותן לך את הכיוון לצעד שמקטין את הפונקציה הכי מהר.

94
00:07:47,460 --> 00:07:54,501
אפילו יותר מזה, אורכו של וקטור השיפוע הזה הוא אינדיקציה למידת התלול של המדרון התלול ביותר.

95
00:07:54,501 --> 00:07:54,580


96
00:07:54,580 --> 00:07:57,998
עכשיו אם אינך מכיר חשבון רב-משתני ומעוניין ללמוד עוד,

97
00:07:57,998 --> 00:08:01,100
בדוק חלק מהעבודות שעשיתי עבור אקדמיית חאן בנושא.

98
00:08:01,100 --> 00:08:07,826
אבל בכנות, כל מה שחשוב לך ולי כרגע זה שבאופן עקרוני קיימת דרך לחשב את הווקטור הזה,

99
00:08:07,826 --> 00:08:12,040
הווקטור הזה שאומר לך מה כיוון הירידה וכמה הוא תלול.

100
00:08:12,040 --> 00:08:17,280
אתה תהיה בסדר אם זה כל מה שאתה יודע ואתה לא איתן בפרטים.

101
00:08:17,280 --> 00:08:23,796
כי אם אתה יכול לקבל את זה, האלגוריתם למזער את הפונקציה הוא לחשב את כיוון השיפוע הזה,

102
00:08:23,796 --> 00:08:27,400
ואז לקחת צעד קטן במורד, ולחזור על זה שוב ושוב.

103
00:08:27,400 --> 00:08:33,700
זה אותו רעיון בסיסי לפונקציה שיש לה 13,000 כניסות במקום 2 כניסות.

104
00:08:33,700 --> 00:08:40,180
תאר לעצמך לארגן את כל 13,000 המשקלים וההטיות של הרשת שלנו לתוך וקטור עמודות ענק.

105
00:08:40,180 --> 00:08:48,220
השיפוע השלילי של פונקציית העלות הוא רק וקטור, זה כיוון כלשהו בתוך מרחב הקלט העצום בטירוף

106
00:08:48,220 --> 00:08:55,900
הזה שאומר לך אילו דחיפות לכל המספרים האלה יגרמו לירידה המהירה ביותר לפונקציית העלות.

107
00:08:55,900 --> 00:08:59,231
וכמובן, עם פונקציית העלות שתוכננה במיוחד שלנו,

108
00:08:59,231 --> 00:09:04,475
שינוי המשקלים וההטיות כדי להקטין את זה אומר לגרום לתפוקת הרשת על כל נתוני

109
00:09:04,475 --> 00:09:07,807
אימון להיראות פחות כמו מערך אקראי של 10 ערכים,

110
00:09:07,807 --> 00:09:11,280
ויותר כמו החלטה אמיתית שאנחנו רוצים את זה לעשות.

111
00:09:11,280 --> 00:09:17,233
חשוב לזכור, פונקציית עלות זו כוללת ממוצע של כל נתוני האימון,

112
00:09:17,233 --> 00:09:24,260
כך שאם אתה ממזער אותו, זה אומר שזה ביצועים טובים יותר בכל הדגימות הללו.

113
00:09:24,260 --> 00:09:30,142
האלגוריתם לחישוב שיפוע זה ביעילות, שהוא למעשה הלב של האופן שבו רשת עצבית לומדת,

114
00:09:30,142 --> 00:09:34,040
נקרא התפשטות לאחור, ועל זה אני הולך לדבר בסרטון הבא.

115
00:09:34,040 --> 00:09:40,970
שם, אני באמת רוצה להקדיש זמן כדי לעבור על מה בדיוק קורה לכל משקל והטיה עבור נתוני אימון

116
00:09:40,970 --> 00:09:47,980
נתון, מנסה לתת תחושה אינטואיטיבית של מה שקורה מעבר לערימת החישובים והנוסחאות הרלוונטיות.

117
00:09:47,980 --> 00:09:52,978
בדיוק כאן, כרגע, הדבר העיקרי שאני רוצה שתדע, ללא תלות בפרטי הטמעה,

118
00:09:52,978 --> 00:09:59,320
הוא שמה שאנחנו מתכוונים כשאנחנו מדברים על למידה ברשת הוא שזה רק מזעור פונקציית עלות.

119
00:09:59,320 --> 00:10:05,201
ושימו לב, תוצאה אחת של זה היא שחשוב שלפונקציית העלות הזו תהיה תפוקה חלקה ונחמדה,

120
00:10:05,201 --> 00:10:09,340
כדי שנוכל למצוא מינימום מקומי על ידי צעדים קטנים בירידה.

121
00:10:09,340 --> 00:10:13,780
זו הסיבה, אגב, לנוירונים מלאכותיים יש הפעלה מתמשכת,

122
00:10:13,780 --> 00:10:20,440
במקום פשוט להיות פעילים או לא פעילים בצורה בינארית, כמו הנוירונים הביולוגיים.

123
00:10:20,440 --> 00:10:23,733
תהליך זה של דחיפה חוזרת ונשנית של קלט של פונקציה

124
00:10:23,733 --> 00:10:26,960
בכפולה כלשהי של השיפוע השלילי נקרא ירידה בדרגה.

125
00:10:26,960 --> 00:10:33,000
זו דרך להתכנס למינימום מקומי כלשהו של פונקציית עלות, בעצם עמק בגרף הזה.

126
00:10:33,000 --> 00:10:37,348
אני עדיין מראה את התמונה של פונקציה עם שתי כניסות, כמובן,

127
00:10:37,348 --> 00:10:41,771
כי דחיפות בחלל קלט של 13,000 מימדים קצת קשה לעטוף את דעתך,

128
00:10:41,771 --> 00:10:45,220
אבל למעשה יש דרך נחמדה לא מרחבית לחשוב על זה.

129
00:10:45,220 --> 00:10:49,100
כל רכיב של הגרדיאנט השלילי אומר לנו שני דברים.

130
00:10:49,100 --> 00:10:55,860
הסימן, כמובן, אומר לנו אם יש להזיז את הרכיב המתאים של וקטור הקלט למעלה או למטה.

131
00:10:55,860 --> 00:11:05,620
אבל חשוב מכך, הגדלים היחסיים של כל הרכיבים האלה מעידים לך אילו שינויים חשובים יותר.

132
00:11:05,620 --> 00:11:10,480
אתה מבין, ברשת שלנו, התאמה לאחד המשקולות עשויה להשפיע

133
00:11:10,480 --> 00:11:14,980
הרבה יותר על פונקציית העלות מאשר התאמה למשקל אחר.

134
00:11:14,980 --> 00:11:19,440
חלק מהחיבורים האלה פשוט חשובים יותר לנתוני האימונים שלנו.

135
00:11:19,440 --> 00:11:25,304
אז הדרך שבה אתה יכול לחשוב על וקטור השיפוע הזה של פונקציית העלות המאסיבית שלנו,

136
00:11:25,304 --> 00:11:30,288
המעוותת את המוח, היא שהוא מקודד את החשיבות היחסית של כל משקל והטיה,

137
00:11:30,288 --> 00:11:34,100
כלומר, איזה מהשינויים האלה יביא הכי הרבה כסף עבורך.

138
00:11:34,100 --> 00:11:37,360
זו באמת רק עוד דרך לחשוב על כיוון.

139
00:11:37,360 --> 00:11:42,804
אם לקחת דוגמה פשוטה יותר, אם יש לך איזושהי פונקציה עם שני משתנים כקלט,

140
00:11:42,804 --> 00:11:49,168
ומחשבת שהשיפוע שלה בנקודה מסוימת יוצאת כ-3,1, אז מצד אחד אתה יכול לפרש את זה כאילו

141
00:11:49,168 --> 00:11:55,455
אתה אומר שכאשר אתה עמידה בקלט הזה, נע לאורך הכיוון הזה מגדיל את הפונקציה הכי מהר,

142
00:11:55,455 --> 00:11:59,366
שכאשר אתה משרטט את הפונקציה מעל מישור נקודות הקלט,

143
00:11:59,366 --> 00:12:03,200
הווקטור הזה הוא מה שנותן לך את כיוון העלייה הישר.

144
00:12:03,200 --> 00:12:07,890
אבל דרך נוספת לקרוא היא לומר שלשינויים במשתנה הראשון הזה יש

145
00:12:07,890 --> 00:12:13,753
חשיבות פי שלושה מאשר לשינויים במשתנה השני, שלפחות בסביבה של הקלט הרלוונטי,

146
00:12:13,753 --> 00:12:17,740
דחיפה של ערך ה-x גוררת הרבה יותר מרץ עבורך דוֹלָר.

147
00:12:17,740 --> 00:12:22,880
בסדר, בוא נתרחק ונסכם איפה אנחנו עד עכשיו.

148
00:12:22,880 --> 00:12:27,146
הרשת עצמה היא הפונקציה הזו עם 784 כניסות ו-10 יציאות,

149
00:12:27,146 --> 00:12:30,860
המוגדרות במונחים של כל הסכומים המשוקללים הללו.

150
00:12:30,860 --> 00:12:34,160
פונקציית העלות היא שכבה של מורכבות נוסף על כך.

151
00:12:34,160 --> 00:12:38,400
הוא לוקח את 13,000 המשקולות וההטיות בתור תשומות,

152
00:12:38,400 --> 00:12:42,640
ויורק מידה אחת של עלוב בהתבסס על דוגמאות האימון.

153
00:12:42,640 --> 00:12:47,520
השיפוע של פונקציית העלות הוא עוד שכבה אחת של מורכבות.

154
00:12:47,520 --> 00:12:55,373
זה אומר לנו אילו דחיפות לכל המשקולות וההטיות הללו גורמות לשינוי המהיר ביותר בערך של

155
00:12:55,373 --> 00:13:03,040
פונקציית העלות, שאותו אתה עשוי לפרש כאומר אילו שינויים לאיזה משקלים חשובים ביותר.

156
00:13:03,040 --> 00:13:06,535
אז כשאתה מאתחל את הרשת עם משקלים והטיות אקראיות,

157
00:13:06,535 --> 00:13:10,673
ומתאים אותם פעמים רבות בהתבסס על תהליך הירידה בשיפוע הזה,

158
00:13:10,673 --> 00:13:14,240
עד כמה היא באמת מתפקדת בתמונות שלא נראו קודם לכן?

159
00:13:14,240 --> 00:13:19,687
זה שתיארתי כאן, עם שתי השכבות הנסתרות של 16 נוירונים כל אחת,

160
00:13:19,687 --> 00:13:26,920
שנבחרו בעיקר מסיבות אסתטיות, לא רע, ומסווג כ-96% מהתמונות החדשות שהוא רואה נכון.

161
00:13:26,920 --> 00:13:36,300
ובכנות, אם אתה מסתכל על כמה מהדוגמאות שהוא מבלגן בהן, אתה מרגיש נאלץ לחתוך את זה קצת.

162
00:13:36,300 --> 00:13:41,220
אם אתה משחק עם מבנה השכבה הנסתרת ותעשה כמה שינויים, אתה יכול להשיג את זה עד 98%.

163
00:13:41,220 --> 00:13:42,900
וזה די טוב!

164
00:13:42,900 --> 00:13:47,489
זה לא הכי טוב, אתה בהחלט יכול להשיג ביצועים טובים יותר על ידי ביצוע

165
00:13:47,489 --> 00:13:52,551
מתוחכם יותר מרשת הוניל הפשוטה הזו, אבל בהתחשב בכמה מרתיעה המשימה הראשונית,

166
00:13:52,551 --> 00:13:57,275
אני חושב שיש משהו מדהים בכל רשת שעושה את זה טוב בתמונות שהיא מעולם לא

167
00:13:57,275 --> 00:14:02,000
נראתה בעבר בהתחשב בכך שאנחנו מעולם לא אמר לו במפורש אילו דפוסים לחפש.

168
00:14:02,000 --> 00:14:07,268
במקור, הדרך שבה הנעתי את המבנה הזה הייתה על ידי תיאור תקווה שאולי תהיה לנו,

169
00:14:07,268 --> 00:14:12,674
שהשכבה השנייה עשויה לקלוט קצוות קטנים, שהשכבה השלישית תחבר את הקצוות האלה כדי

170
00:14:12,674 --> 00:14:18,220
לזהות לולאות וקווים ארוכים יותר, ושהם עשויים להיות חתוכים. יחד כדי לזהות ספרות.

171
00:14:18,220 --> 00:14:21,040
אז זה מה שהרשת שלנו עושה בעצם?

172
00:14:21,040 --> 00:14:24,880
ובכן, עבור זה לפחות, בכלל לא.

173
00:14:24,880 --> 00:14:30,938
זוכרים איך בסרטון האחרון הסתכלנו כיצד ניתן להמחיש את משקלי החיבורים מכל הנוירונים

174
00:14:30,938 --> 00:14:37,440
בשכבה הראשונה לנוירון נתון בשכבה השנייה כתבנית פיקסלים נתונה שנוירון השכבה השנייה קולט?

175
00:14:37,440 --> 00:14:43,625
ובכן, כשאנחנו עושים את זה עבור המשקולות הקשורות למעברים האלה,

176
00:14:43,625 --> 00:14:50,608
במקום לקלוט קצוות קטנים מבודדים פה ושם, הם נראים, ובכן, כמעט אקראיים,

177
00:14:50,608 --> 00:14:54,200
רק עם כמה דפוסים רופפים מאוד באמצע.

178
00:14:54,200 --> 00:14:59,494
נראה שבמרחב הבלתי נתפס של 13,000 ממדים של משקלים והטיות אפשריות,

179
00:14:59,494 --> 00:15:03,404
הרשת שלנו מצאה את עצמה מינימום מקומי קטן ומשמח,

180
00:15:03,404 --> 00:15:09,840
שלמרות סיווג מוצלח של רוב התמונות, לא בדיוק קולט את הדפוסים שאולי קיווינו להם.

181
00:15:09,840 --> 00:15:14,600
וכדי באמת להסיע את הנקודה הזו הביתה, צפה במה שקורה כשאתה מזין תמונה אקראית.

182
00:15:14,600 --> 00:15:18,986
אם המערכת הייתה חכמה, אולי הייתם מצפים שהיא תרגיש לא בטוחה,

183
00:15:18,986 --> 00:15:24,689
אולי לא באמת תפעיל אף אחד מ-10 נוירוני הפלט האלה או תפעיל את כולם באופן שווה,

184
00:15:24,689 --> 00:15:28,564
אבל במקום זאת היא נותנת לכם בביטחון איזו תשובה שטות,

185
00:15:28,564 --> 00:15:34,560
כאילו היא מרגישה בטוחה שזה אקראי רעש הוא 5 כפי שהוא עושה שתמונה בפועל של 5 היא 5.

186
00:15:34,560 --> 00:15:41,800
בניסוח שונה, גם אם הרשת הזו יכולה לזהות ספרות די טוב, אין לה מושג איך לצייר אותן.

187
00:15:41,800 --> 00:15:45,400
הרבה מזה נובע מכך שזה מערך אימון כל כך מוגבל.

188
00:15:45,400 --> 00:15:48,220
כלומר, שימו את עצמכם בנעלי הרשת כאן.

189
00:15:48,220 --> 00:15:55,150
מנקודת המבט שלו, היקום כולו אינו מורכב מכלום מלבד ספרות לא זזות מוגדרות בבירור ובמרכזן

190
00:15:55,150 --> 00:16:02,160
רשת זעירה, ותפקוד העלות שלו מעולם לא נתן לו שום תמריץ להיות אלא בטוח לחלוטין בהחלטותיו.

191
00:16:02,160 --> 00:16:05,885
אז עם זה בתור הדימוי של מה שהנוירונים בשכבה השנייה באמת עושים,

192
00:16:05,885 --> 00:16:10,320
אתם עשויים לתהות מדוע אציג את הרשת הזו עם מוטיבציה של לקלוט קצוות ודפוסים.

193
00:16:10,320 --> 00:16:13,040
כלומר, זה פשוט בכלל לא מה שזה בסופו של דבר עושה.

194
00:16:13,040 --> 00:16:17,480
ובכן, זו לא אמורה להיות המטרה הסופית שלנו, אלא נקודת התחלה.

195
00:16:17,480 --> 00:16:22,419
למען האמת, זו טכנולוגיה ישנה, מהסוג שנחקר בשנות ה-80 וה-90,

196
00:16:22,419 --> 00:16:28,017
ואתה צריך להבין אותה לפני שתוכל להבין גרסאות מודרניות מפורטות יותר,

197
00:16:28,017 --> 00:16:35,262
וברור שהיא מסוגלת לפתור כמה בעיות מעניינות, אבל ככל שתחפור יותר במה השכבות הנסתרות האלה

198
00:16:35,262 --> 00:16:38,720
באמת עושות, ככל שזה נראה פחות אינטליגנטי.

199
00:16:38,720 --> 00:16:43,434
העברת הפוקוס לרגע מהאופן שבו רשתות לומדות לאופן שבו אתה לומד,

200
00:16:43,434 --> 00:16:47,160
זה יקרה רק אם תעסוק באופן פעיל בחומר כאן איכשהו.

201
00:16:47,160 --> 00:16:51,913
דבר אחד די פשוט שאני רוצה שתעשה הוא פשוט לעצור ברגע זה ולחשוב

202
00:16:51,913 --> 00:16:56,973
לרגע לעומק אילו שינויים אתה עשוי לעשות במערכת הזו וכיצד היא תופסת

203
00:16:56,973 --> 00:17:01,880
תמונות אם אתה רוצה שהיא תקלוט טוב יותר דברים כמו קצוות ודפוסים.

204
00:17:01,880 --> 00:17:05,724
אבל יותר מזה, כדי לעסוק באמת בחומר, אני ממליץ בחום

205
00:17:05,724 --> 00:17:09,720
על ספרו של מייקל נילסן על למידה עמוקה ורשתות עצביות.

206
00:17:09,720 --> 00:17:15,847
בו, אתה יכול למצוא את הקוד ואת הנתונים להורדה ולשחק איתם עבור הדוגמה המדויקת הזו,

207
00:17:15,847 --> 00:17:19,360
והספר ידריך אותך צעד אחר צעד מה הקוד הזה עושה.

208
00:17:19,360 --> 00:17:23,014
מה שמדהים הוא שהספר הזה הוא חינמי וזמין לציבור,

209
00:17:23,014 --> 00:17:28,040
אז אם אתה מפיק ממנו משהו, שקול להצטרף אלי לתרום למאמצים של נילסן.

210
00:17:28,040 --> 00:17:32,724
קישרתי גם כמה משאבים אחרים שאני מאוד אוהב בתיאור,

211
00:17:32,724 --> 00:17:38,720
כולל פוסט הבלוג הפנומנלי והיפה של כריס אולה והמאמרים ב-Distill.

212
00:17:38,720 --> 00:17:44,377
כדי לסגור את העניינים כאן לדקות האחרונות, אני רוצה לחזור לקטע מהראיון שהיה לי עם ליישה לי.

213
00:17:44,377 --> 00:17:44,440


214
00:17:44,440 --> 00:17:48,520
אתה אולי זוכר אותה מהסרטון האחרון, היא עשתה את עבודת הדוקטורט שלה בלמידה עמוקה.

215
00:17:48,520 --> 00:17:52,483
בקטע הקטן הזה, היא מדברת על שני מאמרים אחרונים שבאמת חופרים

216
00:17:52,483 --> 00:17:56,380
כיצד חלק מרשתות זיהוי התמונות המודרניות יותר לומדות למעשה.

217
00:17:56,380 --> 00:18:00,464
רק כדי להגדיר היכן היינו בשיחה, המאמר הראשון לקח את אחת מהרשתות

218
00:18:00,464 --> 00:18:04,166
הנוירוניות העמוקות במיוחד האלה שמאוד טובות בזיהוי תמונות,

219
00:18:04,166 --> 00:18:09,400
ובמקום לאמן אותה על מערך נתונים מסומן כהלכה, הוא עירבב את כל התוויות לפני האימון.

220
00:18:09,400 --> 00:18:15,320
ברור שדיוק הבדיקה כאן לא היה טוב יותר מאקראי, מכיוון שהכל פשוט מסומן באקראי.

221
00:18:15,320 --> 00:18:21,440
אבל זה עדיין היה מסוגל להשיג את אותו דיוק אימון כפי שאתה משיג על מערך נתונים מסומן כהלכה.

222
00:18:21,440 --> 00:18:28,537
בעיקרון, מיליוני המשקלים של הרשת הספציפית הזו הספיקו לה רק לשנן את הנתונים האקראיים,

223
00:18:28,537 --> 00:18:35,300
מה שמעלה את השאלה האם מזעור פונקציית העלות הזו אכן מתאים לכל סוג של מבנה בתמונה,

224
00:18:35,300 --> 00:18:36,720
או שזה רק שינון?

225
00:18:36,720 --> 00:18:40,120
. . . לשנן את כל מערך הנתונים של מהו הסיווג הנכון.

226
00:18:40,120 --> 00:18:45,835
אז כמה, אתם יודעים, חצי שנה מאוחר יותר ב-ICML השנה, לא היה בדיוק נייר הפרכה,

227
00:18:45,835 --> 00:18:52,220
אלא נייר שהתייחס לכמה היבטים כמו, היי, למעשה הרשתות האלה עושות משהו קצת יותר חכם מזה.

228
00:18:52,220 --> 00:18:59,215
אם אתה מסתכל על עקומת הדיוק הזו, אם רק היית מתאמן על מערך נתונים אקראי,

229
00:18:59,215 --> 00:19:05,240
העקומה הזו ירדה מאוד, אתה יודע, לאט מאוד בצורה כמעט ליניארית.

230
00:19:05,240 --> 00:19:09,982
אז אתה באמת מתקשה למצוא את המינימום המקומי הזה של המשקולות האפשריות,

231
00:19:09,982 --> 00:19:12,320
אתה יודע, שיביאו לך את הדיוק הזה.

232
00:19:12,320 --> 00:19:17,584
בעוד שאם אתה מתאמן על מערך נתונים מובנה, כזה שיש לו את התוויות הנכונות,

233
00:19:17,584 --> 00:19:23,360
אתה יודע, אתה מתעסק קצת בהתחלה, אבל אז ירדת מהר מאוד כדי להגיע לרמת הדיוק הזו.

234
00:19:23,360 --> 00:19:28,580
ובמובן מסוים היה קל יותר למצוא את המקסימום המקומי הזה.

235
00:19:28,580 --> 00:19:35,710
אז מה שהיה מעניין בזה הוא שהוא מביא לאור עוד מאמר מלפני כמה שנים,

236
00:19:35,710 --> 00:19:40,140
שיש בו הרבה יותר הפשטות לגבי שכבות הרשת.

237
00:19:40,140 --> 00:19:44,692
אבל אחת התוצאות אמרה כיצד, אם מסתכלים על נוף האופטימיזציה,

238
00:19:44,692 --> 00:19:49,400
המינימום המקומי שרשתות אלו נוטות ללמוד הם למעשה באיכות שווה.

239
00:19:49,400 --> 00:19:51,876
אז במובן מסוים, אם מערך הנתונים שלך מובנה, אתה

240
00:19:51,876 --> 00:19:54,300
אמור להיות מסוגל למצוא את זה הרבה יותר בקלות.

241
00:19:54,300 --> 00:20:01,140
תודתי כמו תמיד לאלו מכם שתומכים בפטראון.

242
00:20:01,140 --> 00:20:07,160
כבר אמרתי בעבר מה זה מחליף משחק בפטראון, אבל הסרטונים האלה באמת לא היו אפשריים בלעדיכם.

243
00:20:07,160 --> 00:20:10,443
אני גם רוצה להודות במיוחד לחברת ה-VC Amplify Partners

244
00:20:10,443 --> 00:20:13,240
ולתמיכה שלהם בסרטוני הווידאו הראשוניים בסדרה.

245
00:20:13,240 --> 00:20:33,140
תודה.

