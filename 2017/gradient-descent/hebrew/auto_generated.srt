1
00:00:04,180 --> 00:00:07,280
בסרטון האחרון פרסמתי את המבנה של רשת עצבית.

2
00:00:07,680 --> 00:00:12,600
אני אתן כאן סיכום קצר שלו כדי לרענן את זכרוננו, ואז יש לי שתי מטרות עיקריות לסרטון הזה.

3
00:00:13,100 --> 00:00:15,873
הראשונה היא להציג את הרעיון של ירידה בשיפוע הגרדיאנט, 

4
00:00:15,873 --> 00:00:18,288
שעומד בבסיס של לא רק כיצד רשתות עצביות לומדות, 

5
00:00:18,288 --> 00:00:20,600
אלא גם כיצד פועלות הרבה מלמידת המכונה האחרות.

6
00:00:21,120 --> 00:00:24,595
אחר כך נעמיק קצת יותר כיצד הרשת הספציפית הזו מתפקדת, 

7
00:00:24,595 --> 00:00:27,940
ומה בסופו של דבר השכבות הנסתרות של נוירונים מחפשות.

8
00:00:28,980 --> 00:00:33,933
כזכור, המטרה שלנו כאן היא הדוגמה הקלאסית של זיהוי ספרות בכתב יד, 

9
00:00:33,933 --> 00:00:36,220
ה-Hello World של רשתות עצביות.

10
00:00:37,020 --> 00:00:43,420
ספרות אלו מוצגות על גבי רשת של 28x28 פיקסלים, כאשר לכל פיקסל יש ערך בגווני אפור בין 0 ל-1.

11
00:00:43,820 --> 00:00:50,040
ערכים אלו הם שקובעים את הפעלת 784 הנוירונים בשכבת הקלט של הרשת.

12
00:00:51,180 --> 00:00:58,410
ואז ההפעלה של כל נוירון בשכבות הבאות מבוססת על סכום משוקלל של כל ההפעלות בשכבה הקודמת, 

13
00:00:58,410 --> 00:01:00,820
בתוספת מספר מיוחד שנקרא הטיה.

14
00:01:02,160 --> 00:01:05,550
אחר כך אתם מחברים את הסכום הזה עם פונקציה אחרת, 

15
00:01:05,550 --> 00:01:08,940
כמו סיגמואיד, או Relu, כמו שהראתי בסרטון האחרון.

16
00:01:09,480 --> 00:01:16,029
בסך הכל, בהינתן הבחירה המעט שרירותית של שתי שכבות נסתרות עם 16 נוירונים כל אחת, 

17
00:01:16,029 --> 00:01:20,368
לרשת יש כ-13,000 משקלים והטיות שאנחנו יכולים להתאים, 

18
00:01:20,368 --> 00:01:24,380
והערכים האלה הם שקובעים מה בדיוק הרשת עושה בפועל.

19
00:01:24,880 --> 00:01:28,861
אז כשאנחנו אומרים שהרשת מסווגת ספרה נתונה אנחנו מתכוונים לכך 

20
00:01:28,861 --> 00:01:33,300
שהנוירון הבהיר ביותר מבין 10 הנוירונים בשכבה הסופית מתאים לספרה הזו.

21
00:01:34,100 --> 00:01:40,012
וזכרו, המניע למבנה השכבתי היה שאולי השכבה השנייה יכולה לקלוט את הקצוות, 

22
00:01:40,012 --> 00:01:44,365
והשכבה השלישית עשויה לקלוט דפוסים כמו לולאות וקווים, 

23
00:01:44,365 --> 00:01:48,800
והאחרונה יכולה פשוט לחבר את הדפוסים האלו לזיהוי ספרות.

24
00:01:49,800 --> 00:01:52,240
אז כאן, אנו לומדים כיצד הרשת לומדת.

25
00:01:52,640 --> 00:01:58,236
מה שאנחנו רוצים זה אלגוריתם שבו אתם יכולים להראות לרשת הזו קבוצה של נתוני אימון, 

26
00:01:58,236 --> 00:02:01,967
שמגיעים בצורה של קבוצת תמונות שונות של ספרות בכתב יד, 

27
00:02:01,967 --> 00:02:07,770
יחד עם תוויות של מה שהם אמורים להיות, וזה יתאים את 13,000 המשקולות וההטיות האלה כדי 

28
00:02:07,770 --> 00:02:10,120
לשפר את הביצועים שלה נתוני האימון.

29
00:02:10,720 --> 00:02:16,860
יש לקוות שמבנה השכבות הזה יגרום לכך שמה שהיא לומדת מתכלל לתמונות מעבר לנתוני האימון הללו.

30
00:02:17,640 --> 00:02:20,539
הדרך שבה אנחנו בודקים זאת היא שאחרי אימון הרשת, 

31
00:02:20,539 --> 00:02:24,887
אתם מראים לה נתונים מתויגים שלא ראתה מעולם, ורואים באיזו מידת הצלחה היא 

32
00:02:24,887 --> 00:02:26,700
מסווגת את התמונות החדשות האלה.

33
00:02:31,120 --> 00:02:34,828
למזלנו, ומה שהופך את הדוגמה הזו לכל כך נפוצה מלכתחילה, 

34
00:02:34,828 --> 00:02:39,075
האנשים הטובים מאחורי מסד הנתונים של MNIST הרכיבו אוסף של עשרות 

35
00:02:39,075 --> 00:02:44,200
אלפי תמונות של ספרות בכתב יד, שכל אחת מהן מסומנת במספרים שהם אמורים לִהיוֹת.

36
00:02:44,900 --> 00:02:49,907
ועד כמה שזה פרובוקטיבי לתאר מכונה כלומדת, ברגע שאתם רואים איך זה עובד, 

37
00:02:49,907 --> 00:02:55,480
זה מרגיש הרבה פחות כמו איזו הנחת מדע בדיוני מטורפת, והרבה יותר כמו תרגיל חשבון.

38
00:02:56,200 --> 00:02:59,960
כלומר, בעצם זה מסתכם במציאת המינימום של פונקציה מסוימת.

39
00:03:01,940 --> 00:03:07,929
זכרו, מבחינה רעיונית אנחנו חושבים על כל נוירון כמקושר לכל הנוירונים בשכבה הקודמת, 

40
00:03:07,929 --> 00:03:13,627
והמשקלים בסכום המשוקלל שמגדירים את הפעלתו דומים לנקודות החוזק של הקשרים הללו, 

41
00:03:13,627 --> 00:03:18,960
וההטיה היא אינדיקציה כלשהי של האם הנוירון הזה נוטה להיות פעיל או לא פעיל.

42
00:03:19,720 --> 00:03:24,400
וכדי להתחיל, אנחנו פשוט הולכים לאתחל את כל המשקולות וההטיות האלה באופן אקראי לחלוטין.

43
00:03:24,940 --> 00:03:28,885
מיותר לציין שהרשת הזו הולכת לפעול בצורה די איומה על דוגמת אימון נתונה, 

44
00:03:28,885 --> 00:03:30,720
מכיוון שהיא פשוט עושה משהו אקראי.

45
00:03:31,040 --> 00:03:36,020
לדוגמה, אתם מזינים את התמונה הזו של 3, ושכבת הפלט פשוט נראית כמו בלגן.

46
00:03:36,600 --> 00:03:42,479
אז מה שאתם עושים זה להגדיר פונקציית עלות, דרך לומר למחשב, לא, מחשב רע, 

47
00:03:42,479 --> 00:03:48,689
לפלט צריכות להיות הפעלות שהן 0 עבור רוב הנוירונים, אבל 1 עבור הנוירון הזה, 

48
00:03:48,689 --> 00:03:50,760
מה שנתת לי הוא זבל מוחלט.

49
00:03:51,720 --> 00:03:58,370
כדי לומר את זה בצורה קצת יותר מתמטית, אתם מחברים את הריבועים של ההבדלים בין כל 

50
00:03:58,370 --> 00:04:05,020
אחת מאותן הפעלת פלט זבל לבין הערך הרצוי, וזה מה שנכנה העלות של דוגמת אימון אחת.

51
00:04:05,960 --> 00:04:11,351
שימו לב שהסכום הזה קטן כאשר הרשת מסווגת את התמונה בצורה נכונה, 

52
00:04:11,351 --> 00:04:16,399
אך הוא גדול כאשר הרשת נראית כאילו היא לא יודעת מה היא עושה.

53
00:04:18,640 --> 00:04:21,932
אז מה שאתם עושים זה לשקול את העלות הממוצעת על 

54
00:04:21,932 --> 00:04:25,440
פני כל עשרות אלפי דוגמאות ההדרכה העומדות לרשותכם.

55
00:04:27,040 --> 00:04:32,740
העלות הממוצעת הזו היא המדד שלנו לכמה הרשת גרועה ועד כמה המחשב אמור להרגיש רע.

56
00:04:33,420 --> 00:04:34,600
וזה דבר מסובך.

57
00:04:35,040 --> 00:04:42,123
זוכרים איך הרשת עצמה הייתה בעצם פונקציה, כזו שמקבלת 784 מספרים כקלט, את ערכי הפיקסלים, 

58
00:04:42,123 --> 00:04:48,800
ופולטת 10 מספרים כפלט שלה, ובמובן מסוים היא מוגדרת על ידי כל המשקלים וההטיות האלה?

59
00:04:49,500 --> 00:04:52,820
ובכן, פונקציית העלות היא שכבת מורכבות נוסף על כך.

60
00:04:53,100 --> 00:04:57,513
הרשת לוקחת כקלט את כ-13,000 המשקלים וההטיות האלה, 

61
00:04:57,513 --> 00:05:02,279
ופולטת מספר בודד שמתאר כמה רעים המשקלים וההטיות האלה, 

62
00:05:02,279 --> 00:05:08,900
והאופן שבו הם מוגדרים תלוי בהתנהגות הרשת על פני כל עשרות אלפי נתוני האימון.

63
00:05:09,520 --> 00:05:11,000
זה הרבה לחשוב עליו.

64
00:05:12,400 --> 00:05:15,820
אבל רק להגיד למחשב איזו עבודה גרועה הוא עושה זה לא מאוד מועיל.

65
00:05:16,220 --> 00:05:20,060
אתם רוצים להגיד לו איך לשנות את המשקולות וההטיות האלו כדי שהוא ישתפר.

66
00:05:20,780 --> 00:05:25,339
כדי להקל, במקום להתאמץ לדמיין פונקציה עם 13,000 קלטים, 

67
00:05:25,339 --> 00:05:30,480
פשוט דמיינו פונקציה פשוטה שיש לה מספר אחד כקלט ומספר אחד כפלט.

68
00:05:31,480 --> 00:05:35,300
איך מוצאים קלט שממזער את ערך הפלט של הפונקציה הזו?

69
00:05:36,460 --> 00:05:41,240
תלמידי החשבון יידעו שלפעמים אתם יכולים להבין את המינימום הזה במפורש, 

70
00:05:41,240 --> 00:05:44,636
אבל זה לא תמיד אפשרי עבור פונקציות מסובכות באמת, 

71
00:05:44,636 --> 00:05:49,555
בטח לא בגרסת ה-13,000 קלטים של המצב הזה עבור פונקציית עלות הרשת העצבית 

72
00:05:49,555 --> 00:05:51,080
המסובכת והמטורפת שלנו.

73
00:05:51,580 --> 00:05:55,804
טקטיקה גמישה יותר היא להתחיל בכל קלט, ולהבין באיזה 

74
00:05:55,804 --> 00:05:59,200
כיוון עליכם לצעוד כדי להקטין את הפלט הזה.

75
00:06:00,080 --> 00:06:05,808
באופן ספציפי, אם אתם יכולים לדעת את השיפוע של הפונקציה במקום שבו אתם נמצאים, 

76
00:06:05,808 --> 00:06:09,900
אז תזוזו שמאלה אם השיפוע חיובי, וימינה אם השיפוע שלילי.

77
00:06:11,960 --> 00:06:17,122
אם תעשו זאת שוב ושוב, בכל נקודה תבדקו את השיפוע החדש ותבצעו את הצעד המתאים, 

78
00:06:17,122 --> 00:06:19,840
תתקרבו למינימום מקומי כלשהו של הפונקציה.

79
00:06:20,640 --> 00:06:23,800
התמונה שאולי יש לכם בראש היא כדור שמתגלגל במורד גבעה.

80
00:06:24,620 --> 00:06:29,454
שימו לב, אפילו עבור הפונקצייה הממש פשוטה הזו, ישנם עמקים אפשריים רבים 

81
00:06:29,454 --> 00:06:33,045
שאתם עשויים לנחות בהם, תלוי באיזה קלט אקראי התחלתם, 

82
00:06:33,045 --> 00:06:38,156
ואין ערובה לכך שהמינימום המקומי בו אתם נוחתים יהיה הערך הקטן ביותר האפשרי 

83
00:06:38,156 --> 00:06:39,400
של פונקציית העלות.

84
00:06:40,220 --> 00:06:42,620
זה נכון גם למקרה של רשת הנוירונים שלנו.

85
00:06:43,180 --> 00:06:48,505
אני גם רוצה שתשימו לב איך אם אתם הופכים את גודל הצעדים שלכם לפרופורציונליים לשפוע, 

86
00:06:48,505 --> 00:06:52,482
אז כשהשיפוע משתטח לכיוון המינימום, הצעדים שלכם הולכים וקטנים, 

87
00:06:52,482 --> 00:06:54,600
וזה עוזר לכם לא לדלג על המינימום.

88
00:06:55,940 --> 00:07:00,980
להגדיל מעט את המורכבות, דמיינו במקום זאת פונקציה עם שני קלטים ופלט אחד.

89
00:07:01,500 --> 00:07:08,140
אתם יכולים לחשוב על מרחב הקלט כמישור ה-xy, ועל פונקציית העלות כמתוארת בגרף כמשטח מעליו.

90
00:07:08,760 --> 00:07:13,821
במקום לשאול על שיפוע הפונקציה, עליכם לשאול באיזה כיוון צריך לצעוד 

91
00:07:13,821 --> 00:07:18,960
במרחב הקלט הזה כדי להקטין את הפלט של הפונקציה במהירות הגבוהה ביותר.

92
00:07:19,720 --> 00:07:21,760
במילים אחרות, מהו כיוון הירידה?

93
00:07:22,380 --> 00:07:25,560
שוב, מועיל לחשוב על כדור שמתגלגל במורד הגבעה.

94
00:07:26,660 --> 00:07:32,944
מי מכם שמכיר את החשבון הרב-משתני יודע שהגרדיאנט של פונקציה נותן לכם את כיוון העלייה 

95
00:07:32,944 --> 00:07:38,780
התלולה ביותר, לאיזה כיוון כדאי לצעוד כדי להגדיל את הפונקציה באופן המהיר ביותר.

96
00:07:39,560 --> 00:07:42,800
באופן טבעי, לקיחת השלילי של הגרדיאנט הזה נותן 

97
00:07:42,800 --> 00:07:46,040
לכם את הכיוון לצעד שמקטין את הפונקציה הכי מהר.

98
00:07:47,240 --> 00:07:50,610
אפילו יותר מזה, אורכו של וקטור הגרדיאנט הזה הוא 

99
00:07:50,610 --> 00:07:53,840
אינדיקציה למידת התלילות של השיפוע התלול ביותר.

100
00:07:54,540 --> 00:07:57,553
אם אינכם מכירים חשבון רב-משתני ומעוניינים ללמוד עוד, 

101
00:07:57,553 --> 00:08:00,340
בדקו חלק מהעבודות שעשיתי עבור Khan Academy בנושא.

102
00:08:00,860 --> 00:08:06,469
אבל בכנות, כל מה שחשוב לכם ולי כרגע זה שבאופן עקרוני קיימת דרך 

103
00:08:06,469 --> 00:08:11,900
לחשב את הווקטור הזה שאומר לכם מה כיוון הירידה וכמה היא תלולה.

104
00:08:12,400 --> 00:08:16,120
אתם תהיו בסדר אם זה כל מה שאתם יודעים ואתם לא מתמצאים בפרטים.

105
00:08:17,200 --> 00:08:23,364
אם אתם יכולים לקבל את זה, האלגוריתם למזעור הפונקציה הוא לחשב את כיוון הגרדיאנט הזה, 

106
00:08:23,364 --> 00:08:26,740
ואז לקחת צעד קטן במורד, ולחזור על זה שוב ושוב.

107
00:08:27,700 --> 00:08:32,820
זה אותו רעיון בסיסי לפונקציה שיש לה 13,000 קלטים במקום 2 קלטים.

108
00:08:33,400 --> 00:08:39,460
תארו לעצמכם לארגן את כל 13,000 המשקלים וההטיות של הרשת שלנו לתוך וקטור עמודה ענק.

109
00:08:40,140 --> 00:08:44,092
הגרדיאנט השלילי של פונקציית העלות הוא רק וקטור, 

110
00:08:44,092 --> 00:08:51,256
זה כיוון כלשהו בתוך מרחב הקלט העצום בטירוף הזה שאומר לכם אילו שינויים לכל המספרים האלה 

111
00:08:51,256 --> 00:08:54,880
יגרמו לירידה המהירה ביותר של פונקציית העלות.

112
00:08:55,640 --> 00:08:58,943
וכמובן, עם פונקציית העלות שתוכננה במיוחד שלנו, 

113
00:08:58,943 --> 00:09:04,003
שינוי המשקלים וההטיות כדי להקטין את זה אומר לגרום לתפוקת הרשת על כל חלק 

114
00:09:04,003 --> 00:09:07,868
בנתוני האימון להיראות פחות כמו מערך אקראי של 10 ערכים, 

115
00:09:07,868 --> 00:09:10,820
ויותר כמו החלטה אמיתית שאנחנו רוצים שתעשה.

116
00:09:11,440 --> 00:09:15,841
חשוב לזכור, פונקציית עלות זו כוללת ממוצע של כל נתוני האימון, 

117
00:09:15,841 --> 00:09:21,180
כך שאם אתם ממזערים את הפלט, זה אומר שהביצועים טובים יותר בכל הדגימות האלו.

118
00:09:23,820 --> 00:09:29,331
האלגוריתם לחישוב שיפוע זה ביעילות, שהוא למעשה הלב של האופן שבו רשת נוירונים לומדת, 

119
00:09:29,331 --> 00:09:33,980
נקרא התפשטות לאחור (backpropagation), ועל זה אני הולך לדבר בסרטון הבא.

120
00:09:34,660 --> 00:09:40,880
שם, אני באמת רוצה להקדיש זמן כדי לעבור על מה בדיוק קורה לכל משקל והטיה עבור נתוני אימון 

121
00:09:40,880 --> 00:09:47,100
נתון, מנסה לתת תחושה אינטואיטיבית של מה שקורה מעבר לערימת החישובים והנוסחאות הרלוונטיות.

122
00:09:47,780 --> 00:09:52,132
כרגע, הדבר העיקרי שאני רוצה שתדעו, ללא תלות בפרטי היישום, 

123
00:09:52,132 --> 00:09:58,360
הוא שמה שאנחנו מתכוונים כשאנחנו מדברים על למידת רשת הוא שזה רק מזעור פונקציית עלות.

124
00:09:59,300 --> 00:10:04,338
ושימו לב, תוצאה אחת של זה היא שחשוב שהפלט של פונקציית העלות הזו תהיה חלקה, 

125
00:10:04,338 --> 00:10:08,100
כדי שנוכל למצוא מינימום מקומי על ידי צעדים קטנים בירידה.

126
00:10:09,260 --> 00:10:13,607
זו הסיבה, אגב, שלנוירונים מלאכותיים יש הפעלות מתמשכות, 

127
00:10:13,607 --> 00:10:19,140
במקום פשוט להיות פעילים או לא בצורה בינארית, כמו הנוירונים הביולוגיים.

128
00:10:20,220 --> 00:10:23,324
תהליך זה של שינוי חוזר ונשנה של קלט של פונקציה 

129
00:10:23,324 --> 00:10:26,760
בכפולה כלשהי של הגרדיאנט השלילי נקרא ירידה בגרדיאנט.

130
00:10:27,300 --> 00:10:32,580
זו דרך להתכנס למינימום מקומי כלשהו של פונקציית עלות, בעצם עמק בגרף הזה.

131
00:10:33,440 --> 00:10:37,590
אני עדיין מראה את התמונה של פונקציה עם שני קלטים כמובן, 

132
00:10:37,590 --> 00:10:44,260
כי שינויים במרחב קלט של 13,000 מימדים קצת קשה לתפוס, אבל יש דרך יפה לא מרחבית לחשוב על זה.

133
00:10:45,080 --> 00:10:48,440
כל רכיב של הגרדיאנט השלילי אומר לנו שני דברים.

134
00:10:49,060 --> 00:10:55,140
הסימן, כמובן, אומר לנו אם יש להזיז את הרכיב המתאים של וקטור הקלט למעלה או למטה.

135
00:10:55,800 --> 00:11:02,720
אבל חשוב מכך, הגדלים היחסיים של כל הרכיבים האלה מספרים לכם אילו שינויים חשובים יותר.

136
00:11:05,220 --> 00:11:09,347
אתם מבינים, ברשת שלנו, שינוי של אחד המשקלים עשויה להשפיע 

137
00:11:09,347 --> 00:11:13,040
הרבה יותר על פונקציית העלות מאשר שינוי של משקל אחר.

138
00:11:14,800 --> 00:11:18,200
חלק מהחיבורים האלה פשוט חשובים יותר לנתוני האימון שלנו.

139
00:11:19,320 --> 00:11:25,259
אז הדרך שבה אתם יכולים לחשוב על וקטור הגרדיאנט הזה של פונקציית העלות המאסיבית שלנו, 

140
00:11:25,259 --> 00:11:29,359
היא שהוא מקודד את החשיבות היחסית של כל משקל והטיה, כלומר, 

141
00:11:29,359 --> 00:11:32,400
איזה מהשינויים האלה יוסיף לכם הכי הרבה ערך.

142
00:11:33,620 --> 00:11:36,640
זו באמת רק עוד דרך לחשוב על כיוון.

143
00:11:37,100 --> 00:11:41,924
כדוגמה פשוטה יותר, אם יש לכם איזושהי פונקציה עם שני משתנים כקלט, 

144
00:11:41,924 --> 00:11:45,635
ואתם מחשבים שהגרדיאנט שלה בנקודה מסוימת יוצא 3,1, 

145
00:11:45,635 --> 00:11:51,275
אז מצד אחד אתם יכולים לפרש אותו כאילו אתם אומרים שכאשר אתם נמצאים בקלט הזה, 

146
00:11:51,275 --> 00:11:54,986
תזוזה לאורך הכיוון הזה מגדיל את הפונקציה הכי מהר, 

147
00:11:54,986 --> 00:11:58,920
שכאשר אתם משרטטים את הפונקציה מעל מישור נקודות הקלט, 

148
00:11:58,920 --> 00:12:02,260
הווקטור הזה הוא מה שנותן לכם את כיוון העלייה.

149
00:12:02,860 --> 00:12:09,720
אבל דרך נוספת לקרוא את זה היא לומר שלשינויים במשתנה הראשון יש חשיבות של פי 3 משינויים 

150
00:12:09,720 --> 00:12:16,900
במשתנה השני, שלפחות בסביבה של הקלט הרלוונטי, שינוי של ערך ה-x גורמת להרבה יותר ערך עבורכם.

151
00:12:19,880 --> 00:12:22,340
בואו נעשה זום אאוט ונסכם איפה אנחנו עד עכשיו.

152
00:12:22,840 --> 00:12:26,699
הרשת עצמה היא הפונקציה הזו עם 784 קלטים ו-10 פלטים, 

153
00:12:26,699 --> 00:12:30,040
המוגדרת במונחים של כל הסכומים המשוקללים הללו.

154
00:12:30,640 --> 00:12:33,680
פונקציית העלות היא שכבה של מורכבות נוספת על כך.

155
00:12:33,980 --> 00:12:37,889
היא לוקחת את 13,000 המשקולות וההטיות בתור תשומות 

156
00:12:37,889 --> 00:12:41,720
ופולטת ערך אחד של עלות בהתבסס על דוגמאות האימון.

157
00:12:42,440 --> 00:12:46,900
והגרדיאנט של פונקציית העלות הוא שכבה נוספת של מורכבות.

158
00:12:47,360 --> 00:12:52,803
זה אומר לנו אילו שינויים לכל המשקולות וההטיות גורמים לשינוי המהיר ביותר בערך של פונקציית 

159
00:12:52,803 --> 00:12:57,880
העלות, שאותו אתם עשויים לפרש כאילו הוא אומר אילו שינויים לאיזה משקלים חשובים ביותר.

160
00:13:02,560 --> 00:13:06,129
אז, כשאתם מאתחלים את הרשת עם משקלים והטיות אקראיים, 

161
00:13:06,129 --> 00:13:09,905
ומשנים אותם פעמים רבות בהתבסס על תהליך הירידה בגדיאנט, 

162
00:13:09,905 --> 00:13:13,200
איך היא מתפקדת בפועל בתמונות שטרם ראתה קודם לכן?

163
00:13:14,100 --> 00:13:18,988
זו שתיארתי כאן, עם שתי השכבות הנסתרות של 16 נוירונים כל אחת, 

164
00:13:18,988 --> 00:13:25,960
שנבחרו בעיקר מסיבות אסתטיות, מתפקדת לא רע, ומסווג נכון כ-96% מהתמונות החדשות שהיא רואה.

165
00:13:26,680 --> 00:13:32,540
ובכנות, אם אתם מסתכלים על כמה מהדוגמאות שהיא לא מצליחה בהן, אתם מרגישים שנאלץ להרפות קצת.

166
00:13:36,220 --> 00:13:41,760
עכשיו, אם אתם מבצעים כמה שינויים במבנה השכבה הנסתרת, אתם יכולים להשיג דיוק של עד 98%.

167
00:13:41,760 --> 00:13:42,720
וזה די טוב!

168
00:13:43,020 --> 00:13:47,587
זה לא הכי טוב, אתם בהחלט יכולים להשיג ביצועים טובים יותר על ידי ביצוע 

169
00:13:47,587 --> 00:13:52,220
מתוחכם יותר מהרשת הבסיסית הזו, אבל בהתחשב בכמה מרתיעה המשימה הראשונית, 

170
00:13:52,220 --> 00:13:57,505
אני חושב שיש משהו מדהים בכל רשת שעושה את זה טוב בתמונות שהיא מעולם לא ראתה בעבר, 

171
00:13:57,505 --> 00:14:01,420
בהתחשב בכך שמעולם לא אמרנו לה באופן ספציפי אילו דפוסים לחפש.

172
00:14:02,560 --> 00:14:09,106
במקור, המוטיבציה שלי למבנה הזה הייתהבנויה על תקווה שהשכבה השנייה עשויה לקלוט קצוות קטנים, 

173
00:14:09,106 --> 00:14:14,343
שהשכבה השלישית תחבר את הקצוות האלה כדי לזהות לולאות וקווים ארוכים יותר, 

174
00:14:14,343 --> 00:14:17,180
ושהן עשויות להתאחד יחד כדי לזהות ספרות.

175
00:14:17,960 --> 00:14:20,400
אז זה מה שהרשת שלנו עושה בעצם?

176
00:14:21,080 --> 00:14:24,400
ובכן, עבור זו לפחות, בכלל לא.

177
00:14:24,820 --> 00:14:30,758
זוכרים איך בסרטון האחרון הסתכלנו כיצד ניתן להמחיש את משקלי החיבורים מכל הנוירונים 

178
00:14:30,758 --> 00:14:37,060
בשכבה הראשונה לנוירון נתון בשכבה השנייה כתבנית פיקסלים נתונה שנוירון השכבה השנייה קולט?

179
00:14:37,780 --> 00:14:43,053
ובכן, כשאנחנו באמת עושים את זה עבור המשקולות הקשורות למעברים האלה, 

180
00:14:43,053 --> 00:14:48,327
מהשכבה הראשונה לשכבה הבאה, במקום לקלוט קצוות קטנים מבודדים פה ושם, 

181
00:14:48,327 --> 00:14:53,680
הם נראים, ובכן, כמעט אקראיים, רק עם כמה דפוסים רופפים מאוד באמצע שם.

182
00:14:53,760 --> 00:14:58,987
נראה שבמרחב הבלתי נתפס של 13,000 ממדים של משקלים והטיות אפשריות, 

183
00:14:58,987 --> 00:15:05,501
הרשת שלנו מצאה בעצמה מינימום מקומי קטן ומשמח, שלמרות סיווג מוצלח של רוב התמונות, 

184
00:15:05,501 --> 00:15:08,960
לא בדיוק קולט את הדפוסים שאולי קיווינו להם.

185
00:15:09,780 --> 00:15:13,820
וכדי באמת לחדד את הנקודה הזו, צפו במה שקורה כשאתם מזינים תמונות אקראית.

186
00:15:14,320 --> 00:15:18,602
אם המערכת הייתה חכמה, אולי הייתם מצפים שהיא תרגיש לא בטוחה, 

187
00:15:18,602 --> 00:15:24,168
אולי לא באמת תפעיל אף אחד מ-10 נוירוני הפלט האלה או תפעיל את כולם בצורה שווה, 

188
00:15:24,168 --> 00:15:28,307
אבל במקום זאת היא נותנת לכם בביטחון איזושהי תשובה שטותית, 

189
00:15:28,307 --> 00:15:34,160
כאילו היא מרגישה בטוחה שהרעש האקראי הזה הוא 5 כפי שהיא עושה כאשר תמונה של 5 היא 5.

190
00:15:34,540 --> 00:15:40,700
בניסוח שונה, גם אם הרשת הזו יכולה לזהות ספרות די טוב, אין לה מושג איך לצייר אותן.

191
00:15:41,420 --> 00:15:45,240
הרבה מזה נובע מכך שזה מערך אימון כל כך מוגבל.

192
00:15:45,880 --> 00:15:47,740
כלומר, שימו את עצמכם בנעלי הרשת כאן.

193
00:15:48,140 --> 00:15:54,760
מנקודת המבט שלה, היקום כולו אינו מורכב מכלום מלבד ספרות לא זזות שמוגדרות בבירור ובמרכזן 

194
00:15:54,760 --> 00:16:01,080
רשת זעירה, ותפקוד העלות שלה מעולם לא נתן לה תמריץ להיות אלא בטוחה לחלוטין בהחלטותיה.

195
00:16:02,120 --> 00:16:05,706
אז עם זה בתור הדימוי של מה שהנוירונים בשכבה השנייה באמת עושים, 

196
00:16:05,706 --> 00:16:09,920
אתם עשויים לתהות למה הצגתי את הרשת הזו עם מוטיבציה של קליטת קצוות ודפוסים.

197
00:16:09,920 --> 00:16:12,300
כלומר, זה פשוט בכלל לא מה שהיא בסופו של דבר עושה.

198
00:16:13,380 --> 00:16:17,180
ובכן, זו לא אמורה להיות המטרה הסופית שלנו, אלא רק נקודת התחלה.

199
00:16:17,640 --> 00:16:21,632
למען האמת, זו טכנולוגיה ישנה, מהסוג שנחקר בשנות ה-80 וה-90, 

200
00:16:21,632 --> 00:16:26,356
ואתם צריכים להבין אותה לפני שתוכלו להבין גרסאות מודרניות מפורטות יותר, 

201
00:16:26,356 --> 00:16:32,011
וברור שהיא מסוגלת לפתור כמה בעיות מעניינות, אבל ככל שתעמיקו יותר במה שהשכבות הנסתרות 

202
00:16:32,011 --> 00:16:34,740
האלה באמת עושות, זה נראה פחות אינטליגנטי.

203
00:16:38,480 --> 00:16:42,948
העברת הפוקוס לרגע מהאופן שבו רשתות לומדות לאופן שבו אתם לומדים, 

204
00:16:42,948 --> 00:16:46,300
זה יקרה רק אם תעסקו באופן פעיל בחומר כאן איכשהו.

205
00:16:47,060 --> 00:16:51,617
דבר אחד די פשוט שאני רוצה שתעשו הוא פשוט לעצור לרגע זה ולחשוב 

206
00:16:51,617 --> 00:16:56,175
לעומק אילו שינויים אתם עשוים לעשות במערכת הזו וכיצד היא מבינה 

207
00:16:56,175 --> 00:17:00,880
תמונות אם אתם רוצים שהיא תקלוט טוב יותר דברים כמו קצוות ודפוסים.

208
00:17:01,480 --> 00:17:05,325
אבל יותר מזה, כדי לעסוק באמת בחומר, אני ממליץ בחום על 

209
00:17:05,325 --> 00:17:09,099
ספרו של Michael Nielsen על למידה עמוקה ורשתות עצביות.

210
00:17:09,680 --> 00:17:15,221
בו אתם יכולים למצוא את הקוד ואת הנתונים להורדה ולשחק איתם עבור הדוגמה המדויקת הזו, 

211
00:17:15,221 --> 00:17:18,359
והספר ידריך אותכם צעד אחר צעד מה הקוד הזה עושה.

212
00:17:19,300 --> 00:17:22,789
מה שמדהים הוא שהספר הזה הוא חינמי וזמין לציבור, 

213
00:17:22,789 --> 00:17:27,660
אז אם אתם מפיקים ממנו משהו, שקלו להצטרף אלי לתרום למאמצים של נילסן.

214
00:17:27,660 --> 00:17:31,752
בתיאור קישרתי גם כמה משאבים אחרים שאני מאוד אוהב, 

215
00:17:31,752 --> 00:17:36,500
כולל הבלוג הפנומנלי והיפה של Chris Ola והמאמרים ב-Distill.

216
00:17:38,280 --> 00:17:41,080
כדי לסגור את העניינים כאן לדקות האחרונות, אני 

217
00:17:41,080 --> 00:17:43,880
רוצה לחזור לקטע מהראיון שהיה לי עם Leisha Lee.

218
00:17:44,300 --> 00:17:47,720
אתם אולי זוכרים אותה מהסרטון האחרון, היא עשתה את עבודת הדוקטורט שלה בלמידה עמוקה.

219
00:17:48,300 --> 00:17:52,009
בקטע הקטן הזה היא מדברת על שני מאמרים שפורסמו לאחרונה שבאמת 

220
00:17:52,009 --> 00:17:55,780
מתעמקים בכיצד חלק מרשתות זיהוי התמונות המודרניות יותר לומדות.

221
00:17:56,120 --> 00:18:00,155
רק כדי להגדיר היכן היינו בשיחה, המאמר הראשון לקח את אחת מרשתות 

222
00:18:00,155 --> 00:18:03,871
הנוירונים העמוקות במיוחד האלה, שמאוד טובות בזיהוי תמונות, 

223
00:18:03,871 --> 00:18:08,740
ובמקום לאמן אותה על מערך נתונים מסומן כהלכה, ערבב את כל התוויות לפני האימון.

224
00:18:09,480 --> 00:18:14,832
ברור שדיוק הבדיקה כאן לא היה טוב יותר מאקראי, מכיוון שהכל פשוט מסומן באקראי, 

225
00:18:14,832 --> 00:18:20,880
אבל היא עדיין הצליחה להשיג את אותו דיוק אימון כפי שאתם משיגים במערך נתונים מסומן כהלכה.

226
00:18:21,600 --> 00:18:28,681
בעיקרון, מיליוני המשקלים של הרשת הספציפית הזו הספיקו לה רק כדי לשנן את הנתונים האקראיים, 

227
00:18:28,681 --> 00:18:35,126
מה שמעלה את השאלה האם מזעור פונקציית העלות הזו אכן מתאים לכל סוג של מבנה בתמונה, 

228
00:18:35,126 --> 00:18:36,400
או שזה רק שינון?

229
00:18:51,440 --> 00:18:58,060
אם אתם מסתכלים על עקומת הדיוק הזו, אם אתם מאמנים על מערך נתונים אקראי, 

230
00:18:58,060 --> 00:19:04,587
העקומה הזו תרד לאט מאוד בצורה כמעט ליניארית. אתם באמת מתקשים למצוא את 

231
00:19:04,587 --> 00:19:12,140
המינימום המקומי האפשרי הזה, אתם יודעים, המשקולות הנכונות שיביאו לכם את הדיוק הזה.

232
00:19:12,240 --> 00:19:17,958
בעוד שאם אתם מאמנים על מערך נתונים מובנה, כזה שיש לו את התוויות הנכונות, 

233
00:19:17,958 --> 00:19:23,911
אתם קצת תקועים בהתחלה, אבל אז אתם יורדים מהר מאוד כדי להגיע לרמת הדיוק הזו, 

234
00:19:23,911 --> 00:19:28,220
וכך, במובן מסוים, קל יותר למצוא את המקסימום המקומי הזה.

235
00:19:28,540 --> 00:19:33,343
אז מה שהיה מעניין בזה הוא שהואחושף עוד מאמר מלפני כמה שנים, 

236
00:19:33,343 --> 00:19:38,707
שיש בו הרבה יותר הפשטות לגבי שכבות הרשת, אבל אחת התוצאות אמרה איך, 

237
00:19:38,707 --> 00:19:45,032
אם מסתכלים על האופטימיזציה בכללולתה, המינימום המקומי שרשתות אלו נוטות ללמוד הם 

238
00:19:45,032 --> 00:19:50,076
למעשה באיכות שווה, כך שבמובן מסוים אם מערך הנתונים שלכם מובנה, 

239
00:19:50,076 --> 00:19:54,320
אתם אמורים להיות מסוגלים למצוא את זה הרבה יותר בקלות.

240
00:19:58,160 --> 00:20:01,180
תודה שלי, כמו תמיד, לאלו מכם שתומכים ב-Patreon.

241
00:20:01,520 --> 00:20:06,800
כבר הסברתי בעבר מה זה Patreon, אבל הסרטונים האלה באמת לא היו אפשריים בלעדיכם.

242
00:20:07,460 --> 00:20:12,780
אני גם רוצה להודות במיוחד לחברת ה-VC Amplify Partners, בתמיכתם בסרטונים הראשונים בסדרה.

