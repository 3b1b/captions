1
00:00:04,180 --> 00:00:07,280
סרטון אחרון פרסמתי את המבנה של רשת עצבית.

2
00:00:07,680 --> 00:00:12,600
אני אתן כאן סיכום קצר כדי שזה יהיה רענן במוחנו, ואז יש לי שתי מטרות עיקריות לסרטון הזה.

3
00:00:13,100 --> 00:00:18,256
הראשון הוא להציג את הרעיון של ירידה בשיפוע, שעומד בבסיס לא רק כיצד רשתות עצביות לומדות, 

4
00:00:18,256 --> 00:00:20,600
אלא כיצד פועלת גם הרבה למידת מכונה אחרת.

5
00:00:21,120 --> 00:00:24,719
ואז אחר כך נחפור קצת יותר כיצד הרשת הספציפית הזו מתפקדת, 

6
00:00:24,719 --> 00:00:27,940
ומה בסופו של דבר השכבות הנסתרות של נוירונים מחפשות.

7
00:00:28,979 --> 00:00:34,095
כזכור, המטרה שלנו כאן היא הדוגמה הקלאסית של זיהוי ספרות בכתב יד, 

8
00:00:34,095 --> 00:00:36,220
עולם השלום של רשתות עצביות.

9
00:00:37,020 --> 00:00:43,420
ספרות אלו מוצגות על גבי רשת של 28x28 פיקסלים, כאשר לכל פיקסל יש ערך בגווני אפור בין 0 ל-1.

10
00:00:43,820 --> 00:00:50,040
אלו הם שקובעים את הפעלות של 784 נוירונים בשכבת הקלט של הרשת.

11
00:00:51,180 --> 00:00:58,389
ואז ההפעלה של כל נוירון בשכבות הבאות מבוססת על סכום משוקלל של כל ההפעלה בשכבה הקודמת, 

12
00:00:58,389 --> 00:01:00,820
בתוספת מספר מיוחד שנקרא הטיה.

13
00:01:02,160 --> 00:01:06,609
אחר כך אתה מחבר את הסכום הזה עם פונקציה אחרת, כמו סיגמואידיות, 

14
00:01:06,609 --> 00:01:08,940
או רלו, כמו שעברתי בסרטון האחרון.

15
00:01:09,480 --> 00:01:16,029
בסך הכל, בהינתן הבחירה המעט שרירותית של שתי שכבות נסתרות עם 16 נוירונים כל אחת, 

16
00:01:16,029 --> 00:01:20,368
לרשת יש כ-13,000 משקלים והטיות שאנחנו יכולים להתאים, 

17
00:01:20,368 --> 00:01:24,380
והערכים האלה הם שקובעים מה בדיוק הרשת עושה בפועל.

18
00:01:24,880 --> 00:01:28,859
אז מה שאנחנו מתכוונים כשאנחנו אומרים שהרשת הזו מסווגת ספרה נתונה הוא 

19
00:01:28,859 --> 00:01:33,300
שהנוירונים הבהירים ביותר מבין 10 הנוירונים האלה בשכבה הסופית מתאים לספרה הזו.

20
00:01:34,100 --> 00:01:40,598
ותזכור, המניע שחשבנו כאן למבנה השכבתי היה שאולי השכבה השנייה יכולה לקלוט את הקצוות, 

21
00:01:40,598 --> 00:01:44,699
והשכבה השלישית עשויה לקלוט דפוסים כמו לולאות וקווים, 

22
00:01:44,699 --> 00:01:48,800
והאחרונה יכולה פשוט לחבר את אלה. דפוסים לזיהוי ספרות.

23
00:01:49,800 --> 00:01:52,240
אז הנה, אנו לומדים כיצד הרשת לומדת.

24
00:01:52,640 --> 00:01:58,201
מה שאנחנו רוצים זה אלגוריתם שבו אתה יכול להראות לרשת הזו חבורה שלמה של נתוני אימון, 

25
00:01:58,201 --> 00:02:01,975
שמגיעים בצורה של חבורה של תמונות שונות של ספרות בכתב יד, 

26
00:02:01,975 --> 00:02:07,802
יחד עם תוויות למה שהם אמורים להיות, וזה יהיה להתאים את 13,000 המשקולות וההטיות האלה כדי 

27
00:02:07,802 --> 00:02:10,120
לשפר את הביצועים שלה בנתוני האימון.

28
00:02:10,720 --> 00:02:16,860
יש לקוות שמבנה השכבות הזה יגרום לכך שמה שהוא לומד מתכלל לתמונות מעבר לנתוני האימון הללו.

29
00:02:17,640 --> 00:02:20,929
הדרך שבה אנחנו בודקים את זה היא שאחרי שאתה מאמן את הרשת, 

30
00:02:20,929 --> 00:02:23,756
אתה מראה לה יותר נתונים מתויגים שלא נראתה מעולם, 

31
00:02:23,756 --> 00:02:26,700
ותראה באיזו מידה היא מסווגת את התמונות החדשות האלה.

32
00:02:31,120 --> 00:02:34,663
למזלנו, ומה שהופך את הדוגמה הזו לכל כך נפוצה מלכתחילה, 

33
00:02:34,663 --> 00:02:39,045
הוא שהאנשים הטובים מאחורי מסד הנתונים של MNIST הרכיבו אוסף של עשרות 

34
00:02:39,045 --> 00:02:44,200
אלפי תמונות ספרות בכתב יד, שכל אחת מהן מסומנת במספרים שהם אמורים לכתוב. לִהיוֹת.

35
00:02:44,900 --> 00:02:49,870
ועד כמה שזה פרובוקטיבי לתאר מכונה כלמידה, ברגע שאתה רואה איך זה עובד, 

36
00:02:49,870 --> 00:02:55,480
זה מרגיש הרבה פחות כמו איזו הנחת מדע בדיוני מטורפת, והרבה יותר כמו תרגיל חשבון.

37
00:02:56,200 --> 00:02:59,960
כלומר, בעצם זה מסתכם במציאת המינימום של פונקציה מסוימת.

38
00:03:01,939 --> 00:03:07,882
זכור, מבחינה רעיונית, אנו חושבים על כל נוירון כמקושר לכל הנוירונים בשכבה הקודמת, 

39
00:03:07,882 --> 00:03:13,604
והמשקלים בסכום המשוקלל המגדירים את הפעלתו דומים לנקודות החוזק של הקשרים הללו, 

40
00:03:13,604 --> 00:03:18,960
וההטיה היא אינדיקציה כלשהי של האם הנוירון הזה נוטה להיות פעיל או לא פעיל.

41
00:03:19,720 --> 00:03:22,110
וכדי להתחיל בדברים, אנחנו פשוט הולכים לאתחל את 

42
00:03:22,110 --> 00:03:24,400
כל המשקולות וההטיות האלה באופן אקראי לחלוטין.

43
00:03:24,940 --> 00:03:28,868
מיותר לציין שהרשת הזו הולכת להופיע בצורה די איומה בדוגמה לאימון נתון, 

44
00:03:28,868 --> 00:03:30,720
מכיוון שהיא פשוט עושה משהו אקראי.

45
00:03:31,040 --> 00:03:36,020
לדוגמה, אתה מזין את התמונה הזו של 3, ושכבת הפלט פשוט נראית כמו בלגן.

46
00:03:36,600 --> 00:03:42,562
אז מה שאתה עושה זה להגדיר פונקציית עלות, דרך לומר למחשב, לא, מחשב גרוע, 

47
00:03:42,562 --> 00:03:48,689
שלפלט צריך להיות הפעלות שהן 0 עבור רוב הנוירונים, אבל 1 עבור הנוירון הזה, 

48
00:03:48,689 --> 00:03:50,760
מה שנתת לי הוא זבל מוחלט.

49
00:03:51,720 --> 00:03:58,332
כדי לומר את זה בצורה קצת יותר מתמטית, אתה מחבר את הריבועים של ההבדלים בין כל אחת מאותן 

50
00:03:58,332 --> 00:04:05,020
הפעלת פלט אשפה לבין הערך שאתה רוצה שיהיה להם, וזה מה שנכנה את העלות של דוגמה אחת לאימון.

51
00:04:05,960 --> 00:04:11,587
שימו לב שהסכום הזה קטן כאשר הרשת מסווגת בבטחה את התמונה בצורה נכונה, 

52
00:04:11,587 --> 00:04:16,399
אך הוא גדול כאשר הרשת נראית כאילו היא לא יודעת מה היא עושה.

53
00:04:18,640 --> 00:04:22,222
אז מה שאתה עושה זה לשקול את העלות הממוצעת על פני 

54
00:04:22,222 --> 00:04:25,440
כל עשרות אלפי דוגמאות ההדרכה העומדות לרשותך.

55
00:04:27,040 --> 00:04:32,740
העלות הממוצעת הזו היא המדד שלנו לכמה הרשת גרועה ועד כמה המחשב אמור להרגיש רע.

56
00:04:33,420 --> 00:04:34,600
וזה דבר מסובך.

57
00:04:35,040 --> 00:04:42,123
זוכרים איך הרשת עצמה הייתה בעצם פונקציה, כזו שמקבלת 784 מספרים כקלט, את ערכי הפיקסלים, 

58
00:04:42,123 --> 00:04:48,800
ויורקת 10 מספרים כפלט שלה, ובמובן מסוים היא מוגדרת על ידי כל המשקלים וההטיות האלה?

59
00:04:49,500 --> 00:04:52,820
ובכן, פונקציית העלות היא שכבה של מורכבות נוסף על כך.

60
00:04:53,100 --> 00:04:57,409
הוא לוקח כקלט את כ-13,000 המשקלים וההטיות האלה, 

61
00:04:57,409 --> 00:05:02,167
ויורק מספר בודד שמתאר כמה רעים המשקלים וההטיות האלה, 

62
00:05:02,167 --> 00:05:08,900
והאופן שבו הם מוגדרים תלוי בהתנהגות הרשת על פני כל עשרות אלפי נתוני האימון.

63
00:05:09,520 --> 00:05:11,000
זה הרבה לחשוב על זה.

64
00:05:12,400 --> 00:05:15,820
אבל רק להגיד למחשב איזו עבודה מחורבן הוא עושה זה לא מאוד מועיל.

65
00:05:16,220 --> 00:05:20,060
אתה רוצה להגיד לו איך לשנות את המשקולות וההטיות האלה כדי שזה ישתפר.

66
00:05:20,780 --> 00:05:25,383
כדי להקל, במקום להתאמץ לדמיין פונקציה עם 13,000 כניסות, 

67
00:05:25,383 --> 00:05:30,480
פשוט דמיינו פונקציה פשוטה שיש לה מספר אחד כקלט ומספר אחד כפלט.

68
00:05:31,480 --> 00:05:35,300
איך מוצאים קלט שממזער את הערך של הפונקציה הזו?

69
00:05:36,460 --> 00:05:41,215
תלמידי החשבון יידעו שלפעמים אתה יכול להבין את המינימום הזה במפורש, 

70
00:05:41,215 --> 00:05:44,692
אבל זה לא תמיד אפשרי עבור פונקציות מסובכות באמת, 

71
00:05:44,692 --> 00:05:51,080
בטח לא בגרסת 13,000 הקלט של המצב הזה עבור פונקציית עלות הרשת העצבית המסובכת והמטורפת שלנו.

72
00:05:51,580 --> 00:05:55,850
טקטיקה גמישה יותר היא להתחיל בכל קלט, ולהבין באיזה 

73
00:05:55,850 --> 00:05:59,200
כיוון עליך לצעוד כדי להוריד את הפלט הזה.

74
00:06:00,080 --> 00:06:05,023
באופן ספציפי, אם אתה יכול להבין את השיפוע של הפונקציה במקום שבו אתה נמצא, 

75
00:06:05,023 --> 00:06:09,900
אז הזז שמאלה אם השיפוע הזה חיובי, והסט את הקלט ימינה אם השיפוע הזה שלילי.

76
00:06:11,960 --> 00:06:16,700
אם תעשה זאת שוב ושוב, בכל נקודה שתבדוק את השיפוע החדש ונקיטת הצעד המתאים, 

77
00:06:16,700 --> 00:06:19,840
אתה הולך להתקרב למינימום מקומי כלשהו של הפונקציה.

78
00:06:20,640 --> 00:06:23,800
התמונה שאולי יש לך בראש כאן היא כדור שמתגלגל במורד גבעה.

79
00:06:24,620 --> 00:06:28,148
שימו לב, אפילו עבור פונקציית קלט בודדת ממש פשוטה זו, 

80
00:06:28,148 --> 00:06:33,541
ישנם עמקים אפשריים רבים שאתם עשויים לנחות בהם, תלוי באיזה קלט אקראי אתם מתחילים, 

81
00:06:33,541 --> 00:06:39,400
ואין ערובה שהמינימום המקומי בו אתם נוחתים יהיה הערך הקטן ביותר האפשרי של פונקציית העלות.

82
00:06:40,220 --> 00:06:42,620
זה יועבר גם למקרה של הרשת העצבית שלנו.

83
00:06:43,180 --> 00:06:48,925
אני גם רוצה שתשים לב איך אם אתה הופך את גדלי הצעדים שלך לפרופורציונליים למדרון, 

84
00:06:48,925 --> 00:06:54,600
אז כשהשיפוע משתטח לכיוון המינימום, הצעדים שלך הולכים וקטנים, וזה עוזר לך לחרוג.

85
00:06:55,940 --> 00:07:00,980
להגביר מעט את המורכבות, דמיינו במקום זאת פונקציה עם שתי כניסות ופלט אחד.

86
00:07:01,500 --> 00:07:08,140
אתה יכול לחשוב על מרחב הקלט כמישור ה-xy, ועל פונקציית העלות כמתוארת בגרף כמשטח מעליו.

87
00:07:08,760 --> 00:07:13,782
במקום לשאול על שיפוע הפונקציה, עליך לשאול באיזה כיוון עליך לצעוד 

88
00:07:13,782 --> 00:07:18,960
במרחב הקלט הזה כדי להקטין את הפלט של הפונקציה במהירות הגבוהה ביותר.

89
00:07:19,720 --> 00:07:21,760
במילים אחרות, מה כיוון הירידה?

90
00:07:22,380 --> 00:07:25,560
שוב, זה מועיל לחשוב על כדור שמתגלגל במורד הגבעה.

91
00:07:26,660 --> 00:07:32,524
מי מכם שמכיר את החשבון הרב-משתני יודע שהשיפוע של פונקציה נותן לכם את כיוון 

92
00:07:32,524 --> 00:07:38,780
העלייה התלולה ביותר, לאיזה כיוון כדאי לצעוד כדי להגדיל את הפונקציה המהירה ביותר.

93
00:07:39,560 --> 00:07:46,040
באופן טבעי, לקיחת השלילי של השיפוע הזה נותן לך את הכיוון לצעד שמקטין את הפונקציה הכי מהר.

94
00:07:47,240 --> 00:07:53,840
אפילו יותר מזה, אורכו של וקטור השיפוע הזה הוא אינדיקציה למידת התלול של המדרון התלול ביותר.

95
00:07:54,540 --> 00:07:57,440
אם אינך מכיר חשבון רב-משתני ומעוניין ללמוד עוד, 

96
00:07:57,440 --> 00:08:00,340
בדוק חלק מהעבודות שעשיתי עבור אקדמיית חאן בנושא.

97
00:08:00,860 --> 00:08:07,698
אבל בכנות, כל מה שחשוב לך ולי כרגע זה שבאופן עקרוני קיימת דרך לחשב את הווקטור הזה, 

98
00:08:07,698 --> 00:08:11,900
הווקטור הזה שאומר לך מה כיוון הירידה וכמה הוא תלול.

99
00:08:12,400 --> 00:08:16,120
אתה תהיה בסדר אם זה כל מה שאתה יודע ואתה לא איתן בפרטים.

100
00:08:17,200 --> 00:08:23,311
אם אתה יכול לקבל את זה, האלגוריתם למזער את הפונקציה הוא לחשב את כיוון השיפוע הזה, 

101
00:08:23,311 --> 00:08:26,740
ואז לקחת צעד קטן במורד, ולחזור על זה שוב ושוב.

102
00:08:27,700 --> 00:08:32,820
זה אותו רעיון בסיסי לפונקציה שיש לה 13,000 כניסות במקום 2 כניסות.

103
00:08:33,400 --> 00:08:39,460
תאר לעצמך לארגן את כל 13,000 המשקלים וההטיות של הרשת שלנו לתוך וקטור עמודות ענק.

104
00:08:40,140 --> 00:08:47,723
השיפוע השלילי של פונקציית העלות הוא רק וקטור, זה כיוון כלשהו בתוך מרחב הקלט העצום בטירוף 

105
00:08:47,723 --> 00:08:54,880
הזה שאומר לך אילו דחיפות לכל המספרים האלה יגרמו לירידה המהירה ביותר לפונקציית העלות.

106
00:08:55,640 --> 00:08:58,943
וכמובן, עם פונקציית העלות שתוכננה במיוחד שלנו, 

107
00:08:58,943 --> 00:09:04,143
שינוי המשקלים וההטיות כדי להקטין את זה אומר לגרום לתפוקת הרשת על כל נתוני 

108
00:09:04,143 --> 00:09:07,446
אימון להיראות פחות כמו מערך אקראי של 10 ערכים, 

109
00:09:07,446 --> 00:09:10,820
ויותר כמו החלטה אמיתית שאנחנו רוצים את זה לעשות.

110
00:09:11,440 --> 00:09:15,941
חשוב לזכור, פונקציית עלות זו כוללת ממוצע של כל נתוני האימון, 

111
00:09:15,941 --> 00:09:21,180
כך שאם אתה ממזער אותו, זה אומר שזה ביצועים טובים יותר בכל הדגימות הללו.

112
00:09:23,820 --> 00:09:29,977
האלגוריתם לחישוב שיפוע זה ביעילות, שהוא למעשה הלב של האופן שבו רשת עצבית לומדת, 

113
00:09:29,977 --> 00:09:33,980
נקרא התפשטות לאחור, ועל זה אני הולך לדבר בסרטון הבא.

114
00:09:34,660 --> 00:09:40,880
שם, אני באמת רוצה להקדיש זמן כדי לעבור על מה בדיוק קורה לכל משקל והטיה עבור נתוני אימון 

115
00:09:40,880 --> 00:09:47,100
נתון, מנסה לתת תחושה אינטואיטיבית של מה שקורה מעבר לערימת החישובים והנוסחאות הרלוונטיות.

116
00:09:47,780 --> 00:09:52,474
בדיוק כאן, כרגע, הדבר העיקרי שאני רוצה שתדע, ללא תלות בפרטי הטמעה, 

117
00:09:52,474 --> 00:09:58,360
הוא שמה שאנחנו מתכוונים כשאנחנו מדברים על למידה ברשת הוא שזה רק מזעור פונקציית עלות.

118
00:09:59,300 --> 00:10:04,502
ושימו לב, תוצאה אחת של זה היא שחשוב שלפונקציית העלות הזו תהיה תפוקה חלקה ונחמדה, 

119
00:10:04,502 --> 00:10:08,100
כדי שנוכל למצוא מינימום מקומי על ידי צעדים קטנים בירידה.

120
00:10:09,260 --> 00:10:13,242
זו הסיבה, אגב, לנוירונים מלאכותיים יש הפעלה מתמשכת, 

121
00:10:13,242 --> 00:10:19,140
במקום פשוט להיות פעילים או לא פעילים בצורה בינארית, כמו הנוירונים הביולוגיים.

122
00:10:20,220 --> 00:10:23,558
תהליך זה של דחיפה חוזרת ונשנית של קלט של פונקציה 

123
00:10:23,558 --> 00:10:26,760
בכפולה כלשהי של השיפוע השלילי נקרא ירידה בדרגה.

124
00:10:27,300 --> 00:10:32,580
זו דרך להתכנס למינימום מקומי כלשהו של פונקציית עלות, בעצם עמק בגרף הזה.

125
00:10:33,440 --> 00:10:37,462
אני עדיין מראה את התמונה של פונקציה עם שתי כניסות, כמובן, 

126
00:10:37,462 --> 00:10:41,555
כי דחיפות בחלל קלט של 13,000 מימדים קצת קשה לעטוף את דעתך, 

127
00:10:41,555 --> 00:10:44,260
אבל יש דרך נחמדה לא מרחבית לחשוב על זה.

128
00:10:45,080 --> 00:10:48,440
כל רכיב של הגרדיאנט השלילי אומר לנו שני דברים.

129
00:10:49,060 --> 00:10:55,140
הסימן, כמובן, אומר לנו אם יש להזיז את הרכיב המתאים של וקטור הקלט למעלה או למטה.

130
00:10:55,800 --> 00:11:02,720
אבל חשוב מכך, הגדלים היחסיים של כל הרכיבים האלה מעידים לך אילו שינויים חשובים יותר.

131
00:11:05,220 --> 00:11:09,319
אתה מבין, ברשת שלנו, התאמה לאחד המשקולות עשויה להשפיע 

132
00:11:09,319 --> 00:11:13,040
הרבה יותר על פונקציית העלות מאשר התאמה למשקל אחר.

133
00:11:14,800 --> 00:11:18,200
חלק מהחיבורים האלה פשוט חשובים יותר לנתוני האימונים שלנו.

134
00:11:19,320 --> 00:11:24,578
אז הדרך שבה אתה יכול לחשוב על וקטור השיפוע הזה של פונקציית העלות המאסיבית שלנו, 

135
00:11:24,578 --> 00:11:29,047
המעוותת את המוח, היא שהוא מקודד את החשיבות היחסית של כל משקל והטיה, 

136
00:11:29,047 --> 00:11:32,400
כלומר, איזה מהשינויים האלה יביא הכי הרבה כסף עבורך.

137
00:11:33,620 --> 00:11:36,640
זו באמת רק עוד דרך לחשוב על כיוון.

138
00:11:37,100 --> 00:11:42,218
אם לקחת דוגמה פשוטה יותר, אם יש לך איזושהי פונקציה עם שני משתנים כקלט, 

139
00:11:42,218 --> 00:11:45,751
ואתה מחשב שהשיפוע שלה בנקודה מסוימת יוצאת כ-3,1, 

140
00:11:45,751 --> 00:11:51,662
אז מצד אחד אתה יכול לפרש את זה כאילו אתה אומר שכאשר אתה' כשאתה עומד בקלט הזה, 

141
00:11:51,662 --> 00:11:55,050
נע לאורך הכיוון הזה מגדיל את הפונקציה הכי מהר, 

142
00:11:55,050 --> 00:11:58,727
שכאשר אתה משרטט את הפונקציה מעל מישור נקודות הקלט, 

143
00:11:58,727 --> 00:12:02,260
הווקטור הזה הוא מה שנותן לך את כיוון העלייה הישר.

144
00:12:02,860 --> 00:12:09,801
אבל דרך נוספת לקרוא היא לומר שלשינויים במשתנה הראשון הזה יש חשיבות פי 3 משינויים במשתנה 

145
00:12:09,801 --> 00:12:16,900
השני, שלפחות בסביבה של הקלט הרלוונטי, דחיפה של ערך ה-x גורמת להרבה יותר מרץ עבורך. דוֹלָר.

146
00:12:19,880 --> 00:12:22,340
בוא נעשה זום אאוט ונסכם איפה אנחנו עד כה.

147
00:12:22,840 --> 00:12:26,728
הרשת עצמה היא הפונקציה הזו עם 784 כניסות ו-10 יציאות, 

148
00:12:26,728 --> 00:12:30,040
המוגדרות במונחים של כל הסכומים המשוקללים הללו.

149
00:12:30,640 --> 00:12:33,680
פונקציית העלות היא שכבה של מורכבות נוסף על כך.

150
00:12:33,980 --> 00:12:37,809
זה לוקח את 13,000 המשקולות וההטיות בתור תשומות 

151
00:12:37,809 --> 00:12:41,720
ויורק מידה אחת של עלוב בהתבסס על דוגמאות האימון.

152
00:12:42,440 --> 00:12:46,900
והשיפוע של פונקציית העלות הוא עדיין שכבה אחת של מורכבות.

153
00:12:47,360 --> 00:12:52,438
זה אומר לנו אילו דחיפות לכל המשקולות וההטיות הללו גורמות לשינוי המהיר ביותר בערך של 

154
00:12:52,438 --> 00:12:57,880
פונקציית העלות, שאותו אתה עשוי לפרש כאילו הוא אומר אילו שינויים לאיזה משקלים חשובים ביותר.

155
00:13:02,560 --> 00:13:05,905
אז, כשאתה מאתחל את הרשת עם משקלים והטיות אקראיות, 

156
00:13:05,905 --> 00:13:09,787
ומתאים אותם פעמים רבות בהתבסס על תהליך הירידה בשיפוע הזה, 

157
00:13:09,787 --> 00:13:13,200
עד כמה היא מתפקדת בפועל בתמונות שטרם נראו קודם לכן?

158
00:13:14,100 --> 00:13:19,230
זה שתיארתי כאן, עם שתי השכבות הנסתרות של 16 נוירונים כל אחת, 

159
00:13:19,230 --> 00:13:25,960
שנבחרו בעיקר מסיבות אסתטיות, לא רע, ומסווג כ-96% מהתמונות החדשות שהוא רואה נכון.

160
00:13:26,680 --> 00:13:32,540
ובכנות, אם אתה מסתכל על כמה מהדוגמאות שהוא מבלגן בהן, אתה מרגיש נאלץ לחתוך את זה קצת.

161
00:13:36,220 --> 00:13:41,760
עכשיו אם אתה משחק עם מבנה השכבה הנסתרת ותעשה כמה שינויים, אתה יכול להשיג את זה עד 98%.

162
00:13:41,760 --> 00:13:42,720
וזה די טוב!

163
00:13:43,020 --> 00:13:47,410
זה לא הכי טוב, אתה בהחלט יכול להשיג ביצועים טובים יותר על ידי ביצוע 

164
00:13:47,410 --> 00:13:52,316
מתוחכם יותר מרשת הווניל הפשוטה הזו, אבל בהתחשב בכמה מרתיעה המשימה הראשונית, 

165
00:13:52,316 --> 00:13:57,610
אני חושב שיש משהו מדהים בכל רשת שעושה את זה טוב בתמונות שהיא מעולם לא נראתה בעבר, 

166
00:13:57,610 --> 00:14:01,420
בהתחשב בכך מעולם לא אמרנו לו באופן ספציפי אילו דפוסים לחפש.

167
00:14:02,560 --> 00:14:07,328
במקור, הדרך שבה הנעתי את המבנה הזה הייתה על ידי תיאור תקווה שאולי תהיה לנו, 

168
00:14:07,328 --> 00:14:12,223
שהשכבה השנייה עשויה לקלוט קצוות קטנים, שהשכבה השלישית תחבר את הקצוות האלה כדי 

169
00:14:12,223 --> 00:14:17,180
לזהות לולאות וקווים ארוכים יותר, ושהם עשויים להיות חתוכים. יחד כדי לזהות ספרות.

170
00:14:17,960 --> 00:14:20,400
אז זה מה שהרשת שלנו עושה בעצם?

171
00:14:21,079 --> 00:14:24,400
ובכן, עבור זה לפחות, בכלל לא.

172
00:14:24,820 --> 00:14:30,758
זוכרים איך בסרטון האחרון הסתכלנו כיצד ניתן להמחיש את משקלי החיבורים מכל הנוירונים 

173
00:14:30,758 --> 00:14:37,060
בשכבה הראשונה לנוירון נתון בשכבה השנייה כתבנית פיקסלים נתונה שנוירון השכבה השנייה קולט?

174
00:14:37,780 --> 00:14:43,027
ובכן, כשאנחנו באמת עושים את זה עבור המשקולות הקשורות למעברים האלה, 

175
00:14:43,027 --> 00:14:48,275
מהשכבה הראשונה לשכבה הבאה, במקום לקלוט קצוות קטנים מבודדים פה ושם, 

176
00:14:48,275 --> 00:14:53,680
הם נראים, ובכן, כמעט אקראיים, רק עם כמה דפוסים רופפים מאוד. האמצע שם.

177
00:14:53,760 --> 00:14:58,932
נראה שבמרחב הבלתי נתפס של 13,000 ממדים של משקלים והטיות אפשריות, 

178
00:14:58,932 --> 00:15:02,752
הרשת שלנו מצאה את עצמה מינימום מקומי קטן ומשמח, 

179
00:15:02,752 --> 00:15:08,960
שלמרות סיווג מוצלח של רוב התמונות, לא בדיוק קולט את הדפוסים שאולי קיווינו להם.

180
00:15:09,780 --> 00:15:13,820
וכדי באמת להסיע את הנקודה הזו הביתה, צפה במה שקורה כשאתה מזין תמונה אקראית.

181
00:15:14,320 --> 00:15:18,648
אם המערכת הייתה חכמה, אולי הייתם מצפים שהיא תרגיש לא בטוחה, 

182
00:15:18,648 --> 00:15:24,276
אולי לא באמת תפעיל אף אחד מ-10 נוירוני הפלט האלה או תפעיל את כולם בצורה שווה, 

183
00:15:24,276 --> 00:15:28,099
אבל במקום זאת היא נותנת לכם בביטחון איזו תשובה שטות, 

184
00:15:28,099 --> 00:15:34,160
כאילו היא מרגישה בטוחה שהרעש האקראי הזה הוא 5 כפי שהוא עושה שתמונה בפועל של 5 היא 5.

185
00:15:34,540 --> 00:15:40,700
בניסוח שונה, גם אם הרשת הזו יכולה לזהות ספרות די טוב, אין לה מושג איך לצייר אותן.

186
00:15:41,420 --> 00:15:45,240
הרבה מזה נובע מכך שזה מערך אימון כל כך מוגבל.

187
00:15:45,880 --> 00:15:47,740
כלומר, שימו את עצמכם בנעלי הרשת כאן.

188
00:15:48,140 --> 00:15:54,610
מנקודת המבט שלו, היקום כולו אינו מורכב מכלום מלבד ספרות לא זזות מוגדרות בבירור ובמרכזן 

189
00:15:54,610 --> 00:16:01,080
רשת זעירה, ותפקוד העלות שלו מעולם לא נתן לו שום תמריץ להיות אלא בטוח לחלוטין בהחלטותיו.

190
00:16:02,120 --> 00:16:05,706
אז עם זה בתור הדימוי של מה שהנוירונים בשכבה השנייה באמת עושים, 

191
00:16:05,706 --> 00:16:09,920
אתם עשויים לתהות מדוע אציג את הרשת הזו עם מוטיבציה של לקלוט קצוות ודפוסים.

192
00:16:09,920 --> 00:16:12,300
כלומר, זה פשוט בכלל לא מה שזה בסופו של דבר עושה.

193
00:16:13,380 --> 00:16:17,180
ובכן, זו לא אמורה להיות המטרה הסופית שלנו, אלא נקודת התחלה.

194
00:16:17,640 --> 00:16:21,632
למען האמת, זו טכנולוגיה ישנה, מהסוג שנחקר בשנות ה-80 וה-90, 

195
00:16:21,632 --> 00:16:26,156
ואתה צריך להבין אותה לפני שתוכל להבין גרסאות מודרניות מפורטות יותר, 

196
00:16:26,156 --> 00:16:31,679
וברור שהיא מסוגלת לפתור כמה בעיות מעניינות, אבל ככל שתחפור יותר במה השכבות הנסתרות 

197
00:16:31,679 --> 00:16:34,740
האלה באמת עושות, ככל שזה נראה פחות אינטליגנטי.

198
00:16:38,480 --> 00:16:42,887
העברת הפוקוס לרגע מהאופן שבו רשתות לומדות לאופן שבו אתה לומד, 

199
00:16:42,887 --> 00:16:46,300
זה יקרה רק אם תעסוק באופן פעיל בחומר כאן איכשהו.

200
00:16:47,060 --> 00:16:51,546
דבר אחד די פשוט שאני רוצה שתעשה הוא פשוט לעצור ברגע זה ולחשוב 

201
00:16:51,546 --> 00:16:55,887
לרגע לעומק אילו שינויים אתה עשוי לעשות במערכת הזו וכיצד היא 

202
00:16:55,887 --> 00:17:00,880
תופסת תמונות אם אתה רוצה שהיא תקלוט טוב יותר דברים כמו קצוות ודפוסים.

203
00:17:01,479 --> 00:17:05,253
אבל יותר מזה, כדי לעסוק באמת בחומר, אני ממליץ בחום 

204
00:17:05,253 --> 00:17:09,099
על ספרו של מייקל נילסן על למידה עמוקה ורשתות עצביות.

205
00:17:09,680 --> 00:17:15,240
בו, אתה יכול למצוא את הקוד ואת הנתונים להורדה ולשחק איתם עבור הדוגמה המדויקת הזו, 

206
00:17:15,240 --> 00:17:18,359
והספר ידריך אותך צעד אחר צעד מה הקוד הזה עושה.

207
00:17:19,300 --> 00:17:22,851
מה שמדהים הוא שהספר הזה הוא חינמי וזמין לציבור, 

208
00:17:22,851 --> 00:17:27,660
אז אם אתה מפיק ממנו משהו, שקול להצטרף אלי לתרום למאמצים של נילסן.

209
00:17:27,660 --> 00:17:31,571
קישרתי גם כמה משאבים אחרים שאני מאוד אוהב בתיאור, 

210
00:17:31,571 --> 00:17:36,500
כולל פוסט הבלוג הפנומנלי והיפה של כריס אולה והמאמרים ב-Distill.

211
00:17:38,280 --> 00:17:43,880
כדי לסגור את העניינים כאן לדקות האחרונות, אני רוצה לחזור לקטע מהראיון שהיה לי עם ליישה לי.

212
00:17:44,300 --> 00:17:47,720
אתה אולי זוכר אותה מהסרטון האחרון, היא עשתה את עבודת הדוקטורט שלה בלמידה עמוקה.

213
00:17:48,300 --> 00:17:52,071
בקטע הקטן הזה היא מדברת על שני מאמרים אחרונים שבאמת חופרים 

214
00:17:52,071 --> 00:17:55,780
כיצד חלק מרשתות זיהוי התמונות המודרניות יותר לומדות למעשה.

215
00:17:56,120 --> 00:18:00,178
רק כדי להגדיר היכן היינו בשיחה, המאמר הראשון לקח את אחת מהרשתות 

216
00:18:00,178 --> 00:18:03,920
הנוירוניות העמוקות במיוחד האלה, שמאוד טובות בזיהוי תמונות, 

217
00:18:03,920 --> 00:18:08,740
ובמקום לאמן אותה על מערך נתונים מסומן כהלכה, ערבב את כל התוויות לפני האימון.

218
00:18:09,480 --> 00:18:14,932
ברור שדיוק הבדיקה כאן לא היה טוב יותר מאקראי, מכיוון שהכל פשוט מסומן באקראי, 

219
00:18:14,932 --> 00:18:20,880
אבל הוא עדיין הצליח להשיג את אותו דיוק אימון כפי שאתה משיג במערך נתונים מסומן כהלכה.

220
00:18:21,600 --> 00:18:28,512
בעיקרון, מיליוני המשקלים של הרשת הספציפית הזו הספיקו לה רק לשנן את הנתונים האקראיים, 

221
00:18:28,512 --> 00:18:35,098
מה שמעלה את השאלה האם מזעור פונקציית העלות הזו אכן מתאים לכל סוג של מבנה בתמונה, 

222
00:18:35,098 --> 00:18:36,400
או שזה רק שינון?

223
00:18:51,440 --> 00:18:58,093
אם אתה מסתכל על עקומת הדיוק הזו, אם רק היית מתאמן על מערך נתונים אקראי, 

224
00:18:58,093 --> 00:19:04,839
העקומה הזו ירדה לאט מאוד בצורה כמעט ליניארית, אז אתה באמת מתקשה למצוא את 

225
00:19:04,839 --> 00:19:12,140
המינימום המקומי האפשרי הזה, אתה יודע , המשקולות הנכונות שיביאו לך את הדיוק הזה.

226
00:19:12,240 --> 00:19:18,021
בעוד שאם אתה מתאמן על מערך נתונים מובנה, כזה שיש לו את התוויות הנכונות, 

227
00:19:18,021 --> 00:19:23,562
אתה מתעסק קצת בהתחלה, אבל אז ירדת מהר מאוד כדי להגיע לרמת הדיוק הזו, 

228
00:19:23,562 --> 00:19:28,220
וכך במובן מסוים. היה קל יותר למצוא את המקסימום המקומי הזה.

229
00:19:28,540 --> 00:19:33,907
אז מה שהיה מעניין בזה הוא שהוא מביא לאור עוד מאמר מלפני כמה שנים, 

230
00:19:33,907 --> 00:19:40,413
שיש בו הרבה יותר הפשטות לגבי שכבות הרשת, אבל אחת התוצאות אמרה איך אם מסתכלים על 

231
00:19:40,413 --> 00:19:46,838
נוף האופטימיזציה, המינימום המקומי שרשתות אלו נוטות ללמוד הם למעשה באיכות שווה, 

232
00:19:46,838 --> 00:19:53,425
כך שבמובן מסוים אם מערך הנתונים שלך מובנה, אתה אמור להיות מסוגל למצוא את זה הרבה 

233
00:19:53,425 --> 00:19:54,320
יותר בקלות.

234
00:19:58,160 --> 00:20:01,180
תודה שלי, כמו תמיד, לאלו מכם שתומכים בפטראון.

235
00:20:01,520 --> 00:20:06,800
כבר אמרתי בעבר מה זה Patreon מחליף משחק, אבל הסרטונים האלה באמת לא היו אפשריים בלעדיכם.

236
00:20:07,460 --> 00:20:12,780
אני גם רוצה להודות במיוחד לחברת ה-VC Amplify Partners, בתמיכתם בסרטונים הראשונים בסדרה.

