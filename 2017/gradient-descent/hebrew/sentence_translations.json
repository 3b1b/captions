[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "סרטון אחרון פרסמתי את המבנה של רשת עצבית.",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "אני אתן כאן סיכום קצר כדי שזה יהיה רענן במוחנו, ואז יש לי שתי מטרות עיקריות לסרטון הזה.",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "הראשון הוא להציג את הרעיון של ירידה בשיפוע, שעומד בבסיס לא רק כיצד רשתות עצביות לומדות, אלא כיצד פועלת גם הרבה למידת מכונה אחרת.",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "ואז אחר כך נחפור קצת יותר כיצד הרשת הספציפית הזו מתפקדת, ומה בסופו של דבר השכבות הנסתרות של נוירונים מחפשות.",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "כזכור, המטרה שלנו כאן היא הדוגמה הקלאסית של זיהוי ספרות בכתב יד, עולם השלום של רשתות עצביות.",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "ספרות אלו מוצגות על גבי רשת של 28x28 פיקסלים, כאשר לכל פיקסל יש ערך בגווני אפור בין 0 ל-1.",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "אלו הם שקובעים את הפעלות של 784 נוירונים בשכבת הקלט של הרשת.",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "ואז ההפעלה של כל נוירון בשכבות הבאות מבוססת על סכום משוקלל של כל ההפעלה בשכבה הקודמת, בתוספת מספר מיוחד שנקרא הטיה.",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "אחר כך אתה מחבר את הסכום הזה עם פונקציה אחרת, כמו סיגמואידיות, או רלו, כמו שעברתי בסרטון האחרון.",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "בסך הכל, בהינתן הבחירה המעט שרירותית של שתי שכבות נסתרות עם 16 נוירונים כל אחת, לרשת יש כ-13,000 משקלים והטיות שאנחנו יכולים להתאים, והערכים האלה הם שקובעים מה בדיוק הרשת עושה בפועל.",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "אז מה שאנחנו מתכוונים כשאנחנו אומרים שהרשת הזו מסווגת ספרה נתונה הוא שהנוירונים הבהירים ביותר מבין 10 הנוירונים האלה בשכבה הסופית מתאים לספרה הזו.",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "ותזכור, המניע שחשבנו כאן למבנה השכבתי היה שאולי השכבה השנייה יכולה לקלוט את הקצוות, והשכבה השלישית עשויה לקלוט דפוסים כמו לולאות וקווים, והאחרונה יכולה פשוט לחבר את אלה. דפוסים לזיהוי ספרות.",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "אז הנה, אנו לומדים כיצד הרשת לומדת.",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "מה שאנחנו רוצים זה אלגוריתם שבו אתה יכול להראות לרשת הזו חבורה שלמה של נתוני אימון, שמגיעים בצורה של חבורה של תמונות שונות של ספרות בכתב יד, יחד עם תוויות למה שהם אמורים להיות, וזה יהיה להתאים את 13,000 המשקולות וההטיות האלה כדי לשפר את הביצועים שלה בנתוני האימון.",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "יש לקוות שמבנה השכבות הזה יגרום לכך שמה שהוא לומד מתכלל לתמונות מעבר לנתוני האימון הללו.",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "הדרך שבה אנחנו בודקים את זה היא שאחרי שאתה מאמן את הרשת, אתה מראה לה יותר נתונים מתויגים שלא נראתה מעולם, ותראה באיזו מידה היא מסווגת את התמונות החדשות האלה.",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "למזלנו, ומה שהופך את הדוגמה הזו לכל כך נפוצה מלכתחילה, הוא שהאנשים הטובים מאחורי מסד הנתונים של MNIST הרכיבו אוסף של עשרות אלפי תמונות ספרות בכתב יד, שכל אחת מהן מסומנת במספרים שהם אמורים לכתוב. לִהיוֹת.",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "ועד כמה שזה פרובוקטיבי לתאר מכונה כלמידה, ברגע שאתה רואה איך זה עובד, זה מרגיש הרבה פחות כמו איזו הנחת מדע בדיוני מטורפת, והרבה יותר כמו תרגיל חשבון.",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "כלומר, בעצם זה מסתכם במציאת המינימום של פונקציה מסוימת.",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "זכור, מבחינה רעיונית, אנו חושבים על כל נוירון כמקושר לכל הנוירונים בשכבה הקודמת, והמשקלים בסכום המשוקלל המגדירים את הפעלתו דומים לנקודות החוזק של הקשרים הללו, וההטיה היא אינדיקציה כלשהי של האם הנוירון הזה נוטה להיות פעיל או לא פעיל.",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "וכדי להתחיל בדברים, אנחנו פשוט הולכים לאתחל את כל המשקולות וההטיות האלה באופן אקראי לחלוטין.",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "מיותר לציין שהרשת הזו הולכת להופיע בצורה די איומה בדוגמה לאימון נתון, מכיוון שהיא פשוט עושה משהו אקראי.",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "לדוגמה, אתה מזין את התמונה הזו של 3, ושכבת הפלט פשוט נראית כמו בלגן.",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "אז מה שאתה עושה זה להגדיר פונקציית עלות, דרך לומר למחשב, לא, מחשב גרוע, שלפלט צריך להיות הפעלות שהן 0 עבור רוב הנוירונים, אבל 1 עבור הנוירון הזה, מה שנתת לי הוא זבל מוחלט.",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "כדי לומר את זה בצורה קצת יותר מתמטית, אתה מחבר את הריבועים של ההבדלים בין כל אחת מאותן הפעלת פלט אשפה לבין הערך שאתה רוצה שיהיה להם, וזה מה שנכנה את העלות של דוגמה אחת לאימון.",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "שימו לב שהסכום הזה קטן כאשר הרשת מסווגת בבטחה את התמונה בצורה נכונה, אך הוא גדול כאשר הרשת נראית כאילו היא לא יודעת מה היא עושה.",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "אז מה שאתה עושה זה לשקול את העלות הממוצעת על פני כל עשרות אלפי דוגמאות ההדרכה העומדות לרשותך.",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "העלות הממוצעת הזו היא המדד שלנו לכמה הרשת גרועה ועד כמה המחשב אמור להרגיש רע.",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "וזה דבר מסובך.",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "זוכרים איך הרשת עצמה הייתה בעצם פונקציה, כזו שמקבלת 784 מספרים כקלט, את ערכי הפיקסלים, ויורקת 10 מספרים כפלט שלה, ובמובן מסוים היא מוגדרת על ידי כל המשקלים וההטיות האלה?",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "ובכן, פונקציית העלות היא שכבה של מורכבות נוסף על כך.",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "הוא לוקח כקלט את כ-13,000 המשקלים וההטיות האלה, ויורק מספר בודד שמתאר כמה רעים המשקלים וההטיות האלה, והאופן שבו הם מוגדרים תלוי בהתנהגות הרשת על פני כל עשרות אלפי נתוני האימון.",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "זה הרבה לחשוב על זה.",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "אבל רק להגיד למחשב איזו עבודה מחורבן הוא עושה זה לא מאוד מועיל.",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "אתה רוצה להגיד לו איך לשנות את המשקולות וההטיות האלה כדי שזה ישתפר.",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "כדי להקל, במקום להתאמץ לדמיין פונקציה עם 13,000 כניסות, פשוט דמיינו פונקציה פשוטה שיש לה מספר אחד כקלט ומספר אחד כפלט.",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "איך מוצאים קלט שממזער את הערך של הפונקציה הזו?",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "תלמידי החשבון יידעו שלפעמים אתה יכול להבין את המינימום הזה במפורש, אבל זה לא תמיד אפשרי עבור פונקציות מסובכות באמת, בטח לא בגרסת 13,000 הקלט של המצב הזה עבור פונקציית עלות הרשת העצבית המסובכת והמטורפת שלנו.",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "טקטיקה גמישה יותר היא להתחיל בכל קלט, ולהבין באיזה כיוון עליך לצעוד כדי להוריד את הפלט הזה.",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "באופן ספציפי, אם אתה יכול להבין את השיפוע של הפונקציה במקום שבו אתה נמצא, אז הזז שמאלה אם השיפוע הזה חיובי, והסט את הקלט ימינה אם השיפוע הזה שלילי.",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "אם תעשה זאת שוב ושוב, בכל נקודה שתבדוק את השיפוע החדש ונקיטת הצעד המתאים, אתה הולך להתקרב למינימום מקומי כלשהו של הפונקציה.",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "התמונה שאולי יש לך בראש כאן היא כדור שמתגלגל במורד גבעה.",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "שימו לב, אפילו עבור פונקציית קלט בודדת ממש פשוטה זו, ישנם עמקים אפשריים רבים שאתם עשויים לנחות בהם, תלוי באיזה קלט אקראי אתם מתחילים, ואין ערובה שהמינימום המקומי בו אתם נוחתים יהיה הערך הקטן ביותר האפשרי של פונקציית העלות.",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "זה יועבר גם למקרה של הרשת העצבית שלנו.",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "אני גם רוצה שתשים לב איך אם אתה הופך את גדלי הצעדים שלך לפרופורציונליים למדרון, אז כשהשיפוע משתטח לכיוון המינימום, הצעדים שלך הולכים וקטנים, וזה עוזר לך לחרוג.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "להגביר מעט את המורכבות, דמיינו במקום זאת פונקציה עם שתי כניסות ופלט אחד.",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "אתה יכול לחשוב על מרחב הקלט כמישור ה-xy, ועל פונקציית העלות כמתוארת בגרף כמשטח מעליו.",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "במקום לשאול על שיפוע הפונקציה, עליך לשאול באיזה כיוון עליך לצעוד במרחב הקלט הזה כדי להקטין את הפלט של הפונקציה במהירות הגבוהה ביותר.",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "במילים אחרות, מה כיוון הירידה?",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "שוב, זה מועיל לחשוב על כדור שמתגלגל במורד הגבעה.",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "מי מכם שמכיר את החשבון הרב-משתני יודע שהשיפוע של פונקציה נותן לכם את כיוון העלייה התלולה ביותר, לאיזה כיוון כדאי לצעוד כדי להגדיל את הפונקציה המהירה ביותר.",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "באופן טבעי, לקיחת השלילי של השיפוע הזה נותן לך את הכיוון לצעד שמקטין את הפונקציה הכי מהר.",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "אפילו יותר מזה, אורכו של וקטור השיפוע הזה הוא אינדיקציה למידת התלול של המדרון התלול ביותר.",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "אם אינך מכיר חשבון רב-משתני ומעוניין ללמוד עוד, בדוק חלק מהעבודות שעשיתי עבור אקדמיית חאן בנושא.",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "אבל בכנות, כל מה שחשוב לך ולי כרגע זה שבאופן עקרוני קיימת דרך לחשב את הווקטור הזה, הווקטור הזה שאומר לך מה כיוון הירידה וכמה הוא תלול.",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "אתה תהיה בסדר אם זה כל מה שאתה יודע ואתה לא איתן בפרטים.",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "אם אתה יכול לקבל את זה, האלגוריתם למזער את הפונקציה הוא לחשב את כיוון השיפוע הזה, ואז לקחת צעד קטן במורד, ולחזור על זה שוב ושוב.",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "זה אותו רעיון בסיסי לפונקציה שיש לה 13,000 כניסות במקום 2 כניסות.",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "תאר לעצמך לארגן את כל 13,000 המשקלים וההטיות של הרשת שלנו לתוך וקטור עמודות ענק.",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "השיפוע השלילי של פונקציית העלות הוא רק וקטור, זה כיוון כלשהו בתוך מרחב הקלט העצום בטירוף הזה שאומר לך אילו דחיפות לכל המספרים האלה יגרמו לירידה המהירה ביותר לפונקציית העלות.",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "וכמובן, עם פונקציית העלות שתוכננה במיוחד שלנו, שינוי המשקלים וההטיות כדי להקטין את זה אומר לגרום לתפוקת הרשת על כל נתוני אימון להיראות פחות כמו מערך אקראי של 10 ערכים, ויותר כמו החלטה אמיתית שאנחנו רוצים את זה לעשות.",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "חשוב לזכור, פונקציית עלות זו כוללת ממוצע של כל נתוני האימון, כך שאם אתה ממזער אותו, זה אומר שזה ביצועים טובים יותר בכל הדגימות הללו.",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "האלגוריתם לחישוב שיפוע זה ביעילות, שהוא למעשה הלב של האופן שבו רשת עצבית לומדת, נקרא התפשטות לאחור, ועל זה אני הולך לדבר בסרטון הבא.",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "שם, אני באמת רוצה להקדיש זמן כדי לעבור על מה בדיוק קורה לכל משקל והטיה עבור נתוני אימון נתון, מנסה לתת תחושה אינטואיטיבית של מה שקורה מעבר לערימת החישובים והנוסחאות הרלוונטיות.",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "בדיוק כאן, כרגע, הדבר העיקרי שאני רוצה שתדע, ללא תלות בפרטי הטמעה, הוא שמה שאנחנו מתכוונים כשאנחנו מדברים על למידה ברשת הוא שזה רק מזעור פונקציית עלות.",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "ושימו לב, תוצאה אחת של זה היא שחשוב שלפונקציית העלות הזו תהיה תפוקה חלקה ונחמדה, כדי שנוכל למצוא מינימום מקומי על ידי צעדים קטנים בירידה.",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "זו הסיבה, אגב, לנוירונים מלאכותיים יש הפעלה מתמשכת, במקום פשוט להיות פעילים או לא פעילים בצורה בינארית, כמו הנוירונים הביולוגיים.",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "תהליך זה של דחיפה חוזרת ונשנית של קלט של פונקציה בכפולה כלשהי של השיפוע השלילי נקרא ירידה בדרגה.",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "זו דרך להתכנס למינימום מקומי כלשהו של פונקציית עלות, בעצם עמק בגרף הזה.",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "אני עדיין מראה את התמונה של פונקציה עם שתי כניסות, כמובן, כי דחיפות בחלל קלט של 13,000 מימדים קצת קשה לעטוף את דעתך, אבל יש דרך נחמדה לא מרחבית לחשוב על זה.",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "כל רכיב של הגרדיאנט השלילי אומר לנו שני דברים.",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "הסימן, כמובן, אומר לנו אם יש להזיז את הרכיב המתאים של וקטור הקלט למעלה או למטה.",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "אבל חשוב מכך, הגדלים היחסיים של כל הרכיבים האלה מעידים לך אילו שינויים חשובים יותר.",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "אתה מבין, ברשת שלנו, התאמה לאחד המשקולות עשויה להשפיע הרבה יותר על פונקציית העלות מאשר התאמה למשקל אחר.",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "חלק מהחיבורים האלה פשוט חשובים יותר לנתוני האימונים שלנו.",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "אז הדרך שבה אתה יכול לחשוב על וקטור השיפוע הזה של פונקציית העלות המאסיבית שלנו, המעוותת את המוח, היא שהוא מקודד את החשיבות היחסית של כל משקל והטיה, כלומר, איזה מהשינויים האלה יביא הכי הרבה כסף עבורך.",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "זו באמת רק עוד דרך לחשוב על כיוון.",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "אם לקחת דוגמה פשוטה יותר, אם יש לך איזושהי פונקציה עם שני משתנים כקלט, ואתה מחשב שהשיפוע שלה בנקודה מסוימת יוצאת כ-3,1, אז מצד אחד אתה יכול לפרש את זה כאילו אתה אומר שכאשר אתה' כשאתה עומד בקלט הזה, נע לאורך הכיוון הזה מגדיל את הפונקציה הכי מהר, שכאשר אתה משרטט את הפונקציה מעל מישור נקודות הקלט, הווקטור הזה הוא מה שנותן לך את כיוון העלייה הישר.",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "אבל דרך נוספת לקרוא היא לומר שלשינויים במשתנה הראשון הזה יש חשיבות פי 3 משינויים במשתנה השני, שלפחות בסביבה של הקלט הרלוונטי, דחיפה של ערך ה-x גורמת להרבה יותר מרץ עבורך. דוֹלָר.",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "בוא נעשה זום אאוט ונסכם איפה אנחנו עד כה.",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "הרשת עצמה היא הפונקציה הזו עם 784 כניסות ו-10 יציאות, המוגדרות במונחים של כל הסכומים המשוקללים הללו.",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "פונקציית העלות היא שכבה של מורכבות נוסף על כך.",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "זה לוקח את 13,000 המשקולות וההטיות בתור תשומות ויורק מידה אחת של עלוב בהתבסס על דוגמאות האימון.",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "והשיפוע של פונקציית העלות הוא עדיין שכבה אחת של מורכבות.",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "זה אומר לנו אילו דחיפות לכל המשקולות וההטיות הללו גורמות לשינוי המהיר ביותר בערך של פונקציית העלות, שאותו אתה עשוי לפרש כאילו הוא אומר אילו שינויים לאיזה משקלים חשובים ביותר.",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "אז, כשאתה מאתחל את הרשת עם משקלים והטיות אקראיות, ומתאים אותם פעמים רבות בהתבסס על תהליך הירידה בשיפוע הזה, עד כמה היא מתפקדת בפועל בתמונות שטרם נראו קודם לכן?",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "זה שתיארתי כאן, עם שתי השכבות הנסתרות של 16 נוירונים כל אחת, שנבחרו בעיקר מסיבות אסתטיות, לא רע, ומסווג כ-96% מהתמונות החדשות שהוא רואה נכון.",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "ובכנות, אם אתה מסתכל על כמה מהדוגמאות שהוא מבלגן בהן, אתה מרגיש נאלץ לחתוך את זה קצת.",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "עכשיו אם אתה משחק עם מבנה השכבה הנסתרת ותעשה כמה שינויים, אתה יכול להשיג את זה עד 98%.",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "וזה די טוב!",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "זה לא הכי טוב, אתה בהחלט יכול להשיג ביצועים טובים יותר על ידי ביצוע מתוחכם יותר מרשת הווניל הפשוטה הזו, אבל בהתחשב בכמה מרתיעה המשימה הראשונית, אני חושב שיש משהו מדהים בכל רשת שעושה את זה טוב בתמונות שהיא מעולם לא נראתה בעבר, בהתחשב בכך מעולם לא אמרנו לו באופן ספציפי אילו דפוסים לחפש.",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "במקור, הדרך שבה הנעתי את המבנה הזה הייתה על ידי תיאור תקווה שאולי תהיה לנו, שהשכבה השנייה עשויה לקלוט קצוות קטנים, שהשכבה השלישית תחבר את הקצוות האלה כדי לזהות לולאות וקווים ארוכים יותר, ושהם עשויים להיות חתוכים. יחד כדי לזהות ספרות.",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "אז זה מה שהרשת שלנו עושה בעצם?",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "ובכן, עבור זה לפחות, בכלל לא.",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "זוכרים איך בסרטון האחרון הסתכלנו כיצד ניתן להמחיש את משקלי החיבורים מכל הנוירונים בשכבה הראשונה לנוירון נתון בשכבה השנייה כתבנית פיקסלים נתונה שנוירון השכבה השנייה קולט?",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "ובכן, כשאנחנו באמת עושים את זה עבור המשקולות הקשורות למעברים האלה, מהשכבה הראשונה לשכבה הבאה, במקום לקלוט קצוות קטנים מבודדים פה ושם, הם נראים, ובכן, כמעט אקראיים, רק עם כמה דפוסים רופפים מאוד. האמצע שם.",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "נראה שבמרחב הבלתי נתפס של 13,000 ממדים של משקלים והטיות אפשריות, הרשת שלנו מצאה את עצמה מינימום מקומי קטן ומשמח, שלמרות סיווג מוצלח של רוב התמונות, לא בדיוק קולט את הדפוסים שאולי קיווינו להם.",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "וכדי באמת להסיע את הנקודה הזו הביתה, צפה במה שקורה כשאתה מזין תמונה אקראית.",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "אם המערכת הייתה חכמה, אולי הייתם מצפים שהיא תרגיש לא בטוחה, אולי לא באמת תפעיל אף אחד מ-10 נוירוני הפלט האלה או תפעיל את כולם בצורה שווה, אבל במקום זאת היא נותנת לכם בביטחון איזו תשובה שטות, כאילו היא מרגישה בטוחה שהרעש האקראי הזה הוא 5 כפי שהוא עושה שתמונה בפועל של 5 היא 5.",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "בניסוח שונה, גם אם הרשת הזו יכולה לזהות ספרות די טוב, אין לה מושג איך לצייר אותן.",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "הרבה מזה נובע מכך שזה מערך אימון כל כך מוגבל.",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "כלומר, שימו את עצמכם בנעלי הרשת כאן.",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "מנקודת המבט שלו, היקום כולו אינו מורכב מכלום מלבד ספרות לא זזות מוגדרות בבירור ובמרכזן רשת זעירה, ותפקוד העלות שלו מעולם לא נתן לו שום תמריץ להיות אלא בטוח לחלוטין בהחלטותיו.",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "אז עם זה בתור הדימוי של מה שהנוירונים בשכבה השנייה באמת עושים, אתם עשויים לתהות מדוע אציג את הרשת הזו עם מוטיבציה של לקלוט קצוות ודפוסים.",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "כלומר, זה פשוט בכלל לא מה שזה בסופו של דבר עושה.",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "ובכן, זו לא אמורה להיות המטרה הסופית שלנו, אלא נקודת התחלה.",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "למען האמת, זו טכנולוגיה ישנה, מהסוג שנחקר בשנות ה-80 וה-90, ואתה צריך להבין אותה לפני שתוכל להבין גרסאות מודרניות מפורטות יותר, וברור שהיא מסוגלת לפתור כמה בעיות מעניינות, אבל ככל שתחפור יותר במה השכבות הנסתרות האלה באמת עושות, ככל שזה נראה פחות אינטליגנטי.",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "העברת הפוקוס לרגע מהאופן שבו רשתות לומדות לאופן שבו אתה לומד, זה יקרה רק אם תעסוק באופן פעיל בחומר כאן איכשהו.",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "דבר אחד די פשוט שאני רוצה שתעשה הוא פשוט לעצור ברגע זה ולחשוב לרגע לעומק אילו שינויים אתה עשוי לעשות במערכת הזו וכיצד היא תופסת תמונות אם אתה רוצה שהיא תקלוט טוב יותר דברים כמו קצוות ודפוסים.",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "אבל יותר מזה, כדי לעסוק באמת בחומר, אני ממליץ בחום על ספרו של מייקל נילסן על למידה עמוקה ורשתות עצביות.",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "בו, אתה יכול למצוא את הקוד ואת הנתונים להורדה ולשחק איתם עבור הדוגמה המדויקת הזו, והספר ידריך אותך צעד אחר צעד מה הקוד הזה עושה.",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "מה שמדהים הוא שהספר הזה הוא חינמי וזמין לציבור, אז אם אתה מפיק ממנו משהו, שקול להצטרף אלי לתרום למאמצים של נילסן.",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "קישרתי גם כמה משאבים אחרים שאני מאוד אוהב בתיאור, כולל פוסט הבלוג הפנומנלי והיפה של כריס אולה והמאמרים ב-Distill.",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "כדי לסגור את העניינים כאן לדקות האחרונות, אני רוצה לחזור לקטע מהראיון שהיה לי עם ליישה לי.",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "אתה אולי זוכר אותה מהסרטון האחרון, היא עשתה את עבודת הדוקטורט שלה בלמידה עמוקה.",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "בקטע הקטן הזה היא מדברת על שני מאמרים אחרונים שבאמת חופרים כיצד חלק מרשתות זיהוי התמונות המודרניות יותר לומדות למעשה.",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "רק כדי להגדיר היכן היינו בשיחה, המאמר הראשון לקח את אחת מהרשתות הנוירוניות העמוקות במיוחד האלה, שמאוד טובות בזיהוי תמונות, ובמקום לאמן אותה על מערך נתונים מסומן כהלכה, ערבב את כל התוויות לפני האימון.",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "ברור שדיוק הבדיקה כאן לא היה טוב יותר מאקראי, מכיוון שהכל פשוט מסומן באקראי, אבל הוא עדיין הצליח להשיג את אותו דיוק אימון כפי שאתה משיג במערך נתונים מסומן כהלכה.",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "בעיקרון, מיליוני המשקלים של הרשת הספציפית הזו הספיקו לה רק לשנן את הנתונים האקראיים, מה שמעלה את השאלה האם מזעור פונקציית העלות הזו אכן מתאים לכל סוג של מבנה בתמונה, או שזה רק שינון?",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "אם אתה מסתכל על עקומת הדיוק הזו, אם רק היית מתאמן על מערך נתונים אקראי, העקומה הזו ירדה לאט מאוד בצורה כמעט ליניארית, אז אתה באמת מתקשה למצוא את המינימום המקומי האפשרי הזה, אתה יודע , המשקולות הנכונות שיביאו לך את הדיוק הזה.",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "בעוד שאם אתה מתאמן על מערך נתונים מובנה, כזה שיש לו את התוויות הנכונות, אתה מתעסק קצת בהתחלה, אבל אז ירדת מהר מאוד כדי להגיע לרמת הדיוק הזו, וכך במובן מסוים. היה קל יותר למצוא את המקסימום המקומי הזה.",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "אז מה שהיה מעניין בזה הוא שהוא מביא לאור עוד מאמר מלפני כמה שנים, שיש בו הרבה יותר הפשטות לגבי שכבות הרשת, אבל אחת התוצאות אמרה איך אם מסתכלים על נוף האופטימיזציה, המינימום המקומי שרשתות אלו נוטות ללמוד הם למעשה באיכות שווה, כך שבמובן מסוים אם מערך הנתונים שלך מובנה, אתה אמור להיות מסוגל למצוא את זה הרבה יותר בקלות.",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "תודה שלי, כמו תמיד, לאלו מכם שתומכים בפטראון.",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "כבר אמרתי בעבר מה זה Patreon מחליף משחק, אבל הסרטונים האלה באמת לא היו אפשריים בלעדיכם.",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "אני גם רוצה להודות במיוחד לחברת ה-VC Amplify Partners, בתמיכתם בסרטונים הראשונים בסדרה.",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]