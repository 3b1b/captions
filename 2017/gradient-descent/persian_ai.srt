1
00:00:00,000 --> 00:00:07,240
آخرین ویدیو که من ساختار یک شبکه عصبی را ارائه کردم.

2
00:00:07,240 --> 00:00:11,560
در اینجا خلاصه ای سریع می نویسم تا در ذهن ما

3
00:00:11,560 --> 00:00:13,160
تازه شود و سپس دو هدف اصلی برای این ویدیو دارم.

4
00:00:13,160 --> 00:00:17,960
اولین مورد، معرفی ایده نزول گرادیان است، که نه تنها زیربنای نحوه یادگیری

5
00:00:17,960 --> 00:00:20,800
شبکه های عصبی، بلکه نحوه عملکرد بسیاری از یادگیری ماشینی دیگر نیز است.

6
00:00:20,800 --> 00:00:25,160
سپس بعد از آن، کمی بیشتر در مورد نحوه عملکرد این شبکه خاص و

7
00:00:25,160 --> 00:00:29,560
اینکه آن لایه‌های پنهان نورون‌ها در نهایت به دنبال چه چیزی هستند، خواهیم پرداخت.

8
00:00:29,560 --> 00:00:34,680
به عنوان یادآوری، هدف ما در اینجا نمونه کلاسیک

9
00:00:34,680 --> 00:00:37,080
تشخیص رقم دست‌نویس، دنیای سلام شبکه‌های عصبی است.

10
00:00:37,080 --> 00:00:42,160
این ارقام بر روی یک شبکه پیکسلی 28x28 ارائه می‌شوند که

11
00:00:42,160 --> 00:00:44,260
هر پیکسل دارای مقداری مقیاس خاکستری بین 0 و 1 است.

12
00:00:44,260 --> 00:00:51,400
اینها هستند که تعیین کننده فعال شدن 784 نورون در لایه ورودی شبکه هستند.

13
00:00:51,400 --> 00:00:56,880
فعال‌سازی هر نورون در لایه‌های زیر بر اساس مجموع وزنی تمام فعال‌سازی‌های

14
00:00:56,880 --> 00:01:02,300
لایه قبلی، به اضافه تعدادی عدد خاص به نام بایاس است.

15
00:01:02,300 --> 00:01:07,480
شما این مجموع را با یک تابع دیگر، مانند انقباض سیگموئید، یا

16
00:01:07,480 --> 00:01:09,640
یک ReLU، به روشی که من در آخرین ویدیو طی کردم، می‌نویسید.

17
00:01:09,640 --> 00:01:14,960
در مجموع، با توجه به انتخاب تا حدودی دلخواه از دو لایه پنهان با

18
00:01:14,960 --> 00:01:20,940
16 نورون، شبکه حدود 13000 وزن و بایاس دارد که می‌توانیم آنها را تنظیم

19
00:01:20,940 --> 00:01:25,320
کنیم، و این مقادیر هستند که مشخص می‌کنند شبکه واقعاً چه کاری انجام می‌دهد.

20
00:01:25,320 --> 00:01:29,800
و منظور ما وقتی می گوییم این شبکه یک رقم معین را طبقه بندی می کند

21
00:01:29,800 --> 00:01:34,080
این است که روشن ترین آن 10 نورون در لایه نهایی با آن رقم مطابقت دارد.

22
00:01:34,080 --> 00:01:39,240
و به یاد داشته باشید، انگیزه ای که ما برای ساختار لایه ای در

23
00:01:39,240 --> 00:01:43,920
نظر داشتیم این بود که شاید لایه دوم بتواند از لبه ها استفاده

24
00:01:43,920 --> 00:01:48,640
کند، لایه سوم ممکن است الگوهایی مانند حلقه ها و خطوط را انتخاب کند،

25
00:01:48,640 --> 00:01:49,640
و آخرین لایه بتواند آن الگوها را کنار هم قرار دهد. تشخیص ارقام

26
00:01:49,640 --> 00:01:52,880
بنابراین در اینجا، نحوه یادگیری شبکه را می آموزیم.

27
00:01:52,880 --> 00:01:56,880
چیزی که ما می‌خواهیم الگوریتمی است که در آن می‌توانید مجموعه کاملی از داده‌های آموزشی

28
00:01:56,880 --> 00:02:01,540
را به این شبکه نشان دهید، که به شکل دسته‌ای از تصاویر مختلف از ارقام

29
00:02:01,540 --> 00:02:06,360
دست‌نویس، همراه با برچسب‌هایی برای آنچه قرار است باشند، ارائه می‌شود. آن 13000 وزن و

30
00:02:06,360 --> 00:02:10,760
سوگیری را طوری تنظیم کنید که عملکرد آن در داده های تمرینی بهبود یابد.

31
00:02:10,760 --> 00:02:15,540
امیدواریم این ساختار لایه ای به این معنی باشد که آنچه می

32
00:02:15,540 --> 00:02:17,840
آموزد به تصاویر فراتر از داده های آموزشی تعمیم می یابد.

33
00:02:17,840 --> 00:02:22,240
روشی که ما آن را آزمایش می‌کنیم این است که پس از آموزش شبکه، داده‌های برچسب‌گذاری‌شده بیشتری

34
00:02:22,240 --> 00:02:31,160
را به آن نشان می‌دهید و می‌بینید که با چه دقتی آن تصاویر جدید را طبقه‌بندی می‌کند.

35
00:02:31,160 --> 00:02:34,760
خوشبختانه برای ما، و آنچه این را به یک مثال معمول برای شروع تبدیل می کند، این است

36
00:02:34,760 --> 00:02:39,520
که افراد خوب پشت پایگاه داده MNIST مجموعه ای از ده ها هزار تصویر رقمی دست نویس

37
00:02:39,520 --> 00:02:45,080
را گردآوری کرده اند که هر کدام با اعدادی که قرار است باشند برچسب گذاری شده اند.

38
00:02:45,080 --> 00:02:49,920
و به همان اندازه که توصیف یک ماشین به عنوان یادگیری تحریک‌کننده است، وقتی می‌بینید چگونه کار می‌کند، بسیار

39
00:02:49,920 --> 00:02:55,560
کمتر شبیه یک فرضیه علمی تخیلی دیوانه‌کننده است، و بیشتر شبیه یک تمرین حساب دیفرانسیل و انتگرال است.

40
00:02:55,560 --> 00:03:01,040
منظورم این است که اساساً به یافتن حداقل یک تابع خاص برمی گردد.

41
00:03:01,040 --> 00:03:06,480
به یاد داشته باشید، از نظر مفهومی، ما فکر می‌کنیم که هر نورون به

42
00:03:06,480 --> 00:03:11,440
تمام نورون‌های لایه قبلی متصل است، و وزن‌های موجود در مجموع وزنی که

43
00:03:11,440 --> 00:03:16,400
فعال‌سازی آن را مشخص می‌کند، به نوعی مانند نقاط قوت آن اتصالات است، و

44
00:03:16,400 --> 00:03:19,780
تعصب نشانه‌ای از آیا آن نورون تمایل به فعال یا غیر فعال دارد.

45
00:03:19,780 --> 00:03:23,300
و برای شروع کار، ما فقط می‌خواهیم تمام آن

46
00:03:23,300 --> 00:03:25,020
وزن‌ها و سوگیری‌ها را کاملاً تصادفی مقداردهی کنیم.

47
00:03:25,020 --> 00:03:29,100
نیازی به گفتن نیست که این شبکه در یک نمونه آموزشی خاص

48
00:03:29,100 --> 00:03:31,180
عملکرد وحشتناکی خواهد داشت، زیرا فقط یک کار تصادفی انجام می دهد.

49
00:03:31,180 --> 00:03:36,820
به عنوان مثال، شما در این تصویر از 3 تغذیه می کنید، و لایه خروجی فقط به نظر می رسد درهم و برهم است.

50
00:03:36,820 --> 00:03:43,340
بنابراین کاری که شما انجام می دهید این است که یک تابع هزینه تعریف کنید، راهی برای گفتن به کامپیوتر، نه، کامپیوتر

51
00:03:43,340 --> 00:03:48,940
بد، که خروجی باید فعال سازی هایی داشته باشد که برای اکثر نورون ها 0، اما برای این نورون 1 است.

52
00:03:48,980 --> 00:03:51,740
آنچه به من دادی آشغال محض است.

53
00:03:51,740 --> 00:03:56,740
اگر بخواهیم کمی ریاضی تر بگوییم، مربع تفاوت بین هر یک از فعال سازی های

54
00:03:56,740 --> 00:04:01,980
خروجی سطل زباله و مقداری که می خواهید داشته باشند را جمع آوری کنید، و

55
00:04:01,980 --> 00:04:06,020
این همان چیزی است که ما آن را هزینه یک مثال آموزشی می نامیم.

56
00:04:06,020 --> 00:04:12,660
توجه داشته باشید که این مجموع زمانی که شبکه با اطمینان تصویر را به درستی طبقه بندی می کند ناچیز

57
00:04:12,660 --> 00:04:18,820
است، اما زمانی که به نظر می رسد شبکه نمی داند چه کاری انجام می دهد، این مقدار زیاد است.

58
00:04:18,820 --> 00:04:23,860
بنابراین کاری که شما انجام می دهید این است که میانگین هزینه را

59
00:04:23,860 --> 00:04:27,580
در بین ده ها هزار نمونه آموزشی در اختیار شما در نظر بگیرید.

60
00:04:27,580 --> 00:04:32,300
این هزینه متوسط معیار ما برای اینکه شبکه چقدر

61
00:04:32,300 --> 00:04:33,300
ضعیف است و کامپیوتر چقدر باید احساس کند است.

62
00:04:33,300 --> 00:04:35,300
و این یک چیز پیچیده است.

63
00:04:35,300 --> 00:04:40,380
به یاد داشته باشید که چگونه خود شبکه اساساً یک تابع بود، تابعی که 784 عدد را

64
00:04:40,380 --> 00:04:46,100
به عنوان ورودی، مقادیر پیکسل را می گیرد و 10 عدد را به عنوان خروجی خود می

65
00:04:46,100 --> 00:04:49,700
ریزد، و به یک معنا با تمام این وزن ها و بایاس ها پارامتر می شود؟

66
00:04:49,700 --> 00:04:53,340
تابع هزینه یک لایه از پیچیدگی در بالای آن است.

67
00:04:53,340 --> 00:04:59,140
این 13000 یا بیشتر وزن و بایاس را به عنوان ورودی خود می گیرد، و یک

68
00:04:59,140 --> 00:05:04,620
عدد را بیان می کند که این وزن ها و سوگیری ها چقدر بد هستند، و

69
00:05:04,620 --> 00:05:09,140
نحوه تعریف آن به رفتار شبکه در تمام ده ها هزار قطعه داده آموزشی بستگی دارد.

70
00:05:09,140 --> 00:05:12,460
این خیلی جای تامل دارد.

71
00:05:12,460 --> 00:05:16,380
اما فقط گفتن اینکه کامپیوتر چه کارهای زشتی انجام می دهد خیلی مفید نیست.

72
00:05:16,380 --> 00:05:21,300
شما می خواهید به او بگویید چگونه این وزن ها و سوگیری ها را تغییر دهد تا بهتر شود.

73
00:05:21,300 --> 00:05:25,580
برای سهولت در تصور کردن تابعی با 13000 ورودی، کافیست یک تابع ساده را تصور

74
00:05:25,580 --> 00:05:31,440
کنید که یک عدد به عنوان ورودی و یک عدد به عنوان خروجی دارد.

75
00:05:31,440 --> 00:05:36,420
چگونه ورودی ای پیدا می کنید که مقدار این تابع را به حداقل برساند؟

76
00:05:36,420 --> 00:05:41,300
دانش‌آموزان حساب دیفرانسیل و انتگرال می‌دانند که گاهی اوقات می‌توانید این حداقل را به

77
00:05:41,340 --> 00:05:46,620
صراحت دریابید، اما این همیشه برای توابع واقعاً پیچیده امکان‌پذیر نیست، قطعاً در نسخه

78
00:05:46,620 --> 00:05:51,640
۱۳۰۰۰ ورودی این وضعیت برای تابع هزینه پیچیده شبکه عصبی دیوانه‌وار ما امکان‌پذیر نیست.

79
00:05:51,640 --> 00:05:56,820
یک تاکتیک منعطف تر این است که از هر ورودی شروع کنید

80
00:05:56,820 --> 00:05:59,860
و بفهمید که در کدام جهت باید آن خروجی را کاهش دهید.

81
00:05:59,860 --> 00:06:05,020
به طور خاص، اگر می‌توانید شیب تابعی را که در آن قرار

82
00:06:05,020 --> 00:06:09,280
دارید، مشخص کنید، اگر شیب مثبت است، آن را به چپ و

83
00:06:09,280 --> 00:06:12,720
اگر شیب منفی است، ورودی را به سمت راست تغییر دهید.

84
00:06:12,720 --> 00:06:17,040
اگر این کار را به طور مکرر انجام دهید، در هر نقطه شیب جدید را

85
00:06:17,040 --> 00:06:20,680
بررسی کنید و گام مناسب را بردارید، به حداقل محلی تابع نزدیک خواهید شد.

86
00:06:20,680 --> 00:06:24,600
و تصویری که ممکن است در اینجا در ذهن داشته باشید، توپی است که از تپه در حال غلتیدن است.

87
00:06:24,600 --> 00:06:29,380
و توجه کنید، حتی برای این تابع ورودی منفرد واقعاً ساده شده، بسته

88
00:06:29,380 --> 00:06:34,220
به اینکه از کدام ورودی تصادفی شروع می‌کنید، دره‌های احتمالی زیادی وجود دارد

89
00:06:34,220 --> 00:06:38,460
که ممکن است در آن فرود بیایید، و هیچ تضمینی وجود ندارد که

90
00:06:38,460 --> 00:06:39,460
حداقل محلی که وارد آن می‌شوید، کوچک‌ترین مقدار ممکن باشد. تابع هزینه

91
00:06:39,460 --> 00:06:43,180
این به پرونده شبکه عصبی ما نیز منتقل می شود.

92
00:06:43,180 --> 00:06:48,140
و همچنین می‌خواهم توجه داشته باشید که اگر اندازه‌های گام‌هایتان را با شیب

93
00:06:48,140 --> 00:06:52,920
متناسب کنید، وقتی شیب به سمت حداقل صاف می‌شود، قدم‌هایتان کوچک‌تر و کوچک‌تر

94
00:06:52,920 --> 00:06:56,020
می‌شوند، و این به شما کمک می‌کند تا بیش از حد نشوید.

95
00:06:56,020 --> 00:07:01,640
با کمی افزایش پیچیدگی، در عوض یک تابع با دو ورودی و یک خروجی را تصور کنید.

96
00:07:01,640 --> 00:07:06,360
ممکن است فضای ورودی را صفحه xy تصور کنید و تابع

97
00:07:06,360 --> 00:07:09,020
هزینه را به عنوان یک سطح بالای آن نمودار کنید.

98
00:07:09,020 --> 00:07:13,600
به جای سوال در مورد شیب تابع، باید بپرسید که در چه جهتی باید

99
00:07:13,600 --> 00:07:19,780
در این فضای ورودی قدم بردارید تا خروجی تابع را سریعتر کاهش دهید.

100
00:07:19,780 --> 00:07:22,340
به عبارت دیگر، جهت سراشیبی چیست؟

101
00:07:22,340 --> 00:07:26,740
و دوباره، فکر کردن به توپی که از آن تپه می غلتد مفید است.

102
00:07:26,740 --> 00:07:31,920
کسانی از شما که با حساب دیفرانسیل و انتگرال چند متغیره آشنا

103
00:07:31,920 --> 00:07:37,460
هستند، می‌دانند که گرادیان یک تابع، جهت شیب‌دارترین صعود را به شما

104
00:07:37,460 --> 00:07:39,420
می‌دهد، که برای افزایش سریع‌ترین تابع باید در کدام جهت قدم بردارید.

105
00:07:39,420 --> 00:07:43,820
به طور طبیعی، گرفتن نگاتیو آن گرادیان به شما جهت گام

106
00:07:43,820 --> 00:07:47,460
را می دهد که سریع ترین عملکرد را کاهش می دهد.

107
00:07:47,460 --> 00:07:52,320
حتی بیشتر از آن، طول این بردار گرادیان نشان

108
00:07:52,320 --> 00:07:54,580
می دهد که تندترین شیب چقدر تند است.

109
00:07:54,580 --> 00:07:58,080
حال اگر با حساب دیفرانسیل و انتگرال چند متغیره آشنا نیستید و می خواهید بیشتر بدانید،

110
00:07:58,080 --> 00:08:01,100
برخی از کارهایی را که برای آکادمی خان در این زمینه انجام دادم، بررسی کنید.

111
00:08:01,100 --> 00:08:05,680
راستش را بخواهید، تنها چیزی که در حال حاضر برای من و شما مهم

112
00:08:05,680 --> 00:08:10,440
است این است که اصولا راهی برای محاسبه این بردار وجود دارد، این

113
00:08:10,440 --> 00:08:12,040
بردار که به شما می گوید جهت سراشیبی چیست و چقدر شیب دارد.

114
00:08:12,040 --> 00:08:17,280
اگر این تنها چیزی است که می دانید و روی جزئیات دقیق نیستید، مشکلی ندارید.

115
00:08:17,280 --> 00:08:21,440
زیرا اگر بتوانید آن را بدست آورید، الگوریتم کمینه کردن تابع این است که این جهت گرادیان را

116
00:08:21,440 --> 00:08:27,400
محاسبه کنید، سپس یک قدم کوچک به سمت پایین بردارید و آن را بارها و بارها تکرار کنید.

117
00:08:28,300 --> 00:08:33,700
این همان ایده اولیه برای تابعی است که به جای 2 ورودی، 13000 ورودی دارد.

118
00:08:33,700 --> 00:08:38,980
تصور کنید که تمام 13000 وزن و بایاس شبکه خود

119
00:08:38,980 --> 00:08:40,180
را در یک بردار ستون غول پیکر سازماندهی کنید.

120
00:08:40,180 --> 00:08:46,140
گرادیان منفی تابع هزینه فقط یک بردار است، این یک جهت در

121
00:08:46,140 --> 00:08:51,660
داخل این فضای ورودی دیوانه‌وار عظیم است که به شما می‌گوید کدام

122
00:08:51,660 --> 00:08:55,900
ضربه به همه آن اعداد باعث سریع‌ترین کاهش در تابع هزینه می‌شود.

123
00:08:55,900 --> 00:09:00,000
و البته، با تابع هزینه طراحی شده ویژه ما، تغییر وزن ها و بایاس

124
00:09:00,000 --> 00:09:05,520
ها برای کاهش آن به این معنی است که خروجی شبکه در هر

125
00:09:05,520 --> 00:09:10,280
قطعه از داده های آموزشی کمتر شبیه یک آرایه تصادفی از 10 مقدار

126
00:09:10,280 --> 00:09:11,280
باشد و بیشتر شبیه یک تصمیم واقعی باشد که می خواهیم. ساختن آن

127
00:09:11,280 --> 00:09:15,940
مهم است که به یاد داشته باشید، این تابع هزینه شامل میانگینی از تمام داده های آموزشی است، بنابراین

128
00:09:15,940 --> 00:09:24,260
اگر آن را به حداقل برسانید، به این معنی است که عملکرد بهتری در تمام آن نمونه ها دارد.

129
00:09:24,260 --> 00:09:28,540
الگوریتم محاسبه موثر این گرادیان، که در واقع قلب نحوه یادگیری

130
00:09:28,540 --> 00:09:32,520
یک شبکه عصبی است، پس انتشار نامیده می شود، و این

131
00:09:32,520 --> 00:09:34,040
چیزی است که در ویدیوی بعدی درباره آن صحبت خواهم کرد.

132
00:09:34,040 --> 00:09:39,100
در آنجا، من واقعاً می‌خواهم وقت بگذارم تا در مورد آنچه دقیقاً برای هر وزنه

133
00:09:39,100 --> 00:09:44,100
و سوگیری برای یک قطعه داده تمرینی اتفاق می‌افتد، بگذرم، و سعی می‌کنم احساسی

134
00:09:44,100 --> 00:09:47,980
شهودی برای آنچه فراتر از انبوهی از محاسبات و فرمول‌های مربوطه اتفاق می‌افتد، بدهم.

135
00:09:47,980 --> 00:09:51,780
در اینجا، در حال حاضر، اصلی‌ترین چیزی که می‌خواهم بدون جزئیات پیاده‌سازی

136
00:09:51,780 --> 00:09:56,820
بدانید، این است که وقتی در مورد یادگیری شبکه صحبت می‌کنیم، منظور

137
00:09:56,820 --> 00:09:59,320
ما این است که فقط یک تابع هزینه را به حداقل می‌رساند.

138
00:09:59,320 --> 00:10:02,760
و توجه کنید، یکی از پیامدهای آن این است که برای این تابع

139
00:10:02,760 --> 00:10:07,820
هزینه مهم است که خروجی همواری داشته باشد، به طوری که ما بتوانیم

140
00:10:07,820 --> 00:10:09,340
با برداشتن گام های کوچک در سراشیبی، حداقل محلی را پیدا کنیم.

141
00:10:09,340 --> 00:10:14,140
به همین دلیل است که، اتفاقاً، نورون‌های مصنوعی

142
00:10:14,140 --> 00:10:18,580
به‌جای اینکه صرفاً به‌صورت دوتایی فعال یا غیرفعال

143
00:10:18,580 --> 00:10:20,440
باشند، مانند نورون‌های بیولوژیکی، فعالیت‌های دامنه‌ای پیوسته دارند.

144
00:10:20,440 --> 00:10:24,600
به این فرآیند حرکت مکرر ورودی یک تابع توسط

145
00:10:24,600 --> 00:10:26,960
چند مضرب گرادیان منفی، گرادیان نزول می گویند.

146
00:10:26,960 --> 00:10:31,760
این راهی است برای همگرایی به سمت حداقل محلی

147
00:10:31,760 --> 00:10:33,000
یک تابع هزینه، اساساً یک دره در این نمودار.

148
00:10:33,000 --> 00:10:37,040
البته من هنوز هم تصویر یک تابع را با دو ورودی نشان می‌دهم، زیرا تلنگرها

149
00:10:37,040 --> 00:10:41,480
در یک فضای ورودی 13000 بعدی کمی سخت است که ذهن شما را درگیر کند،

150
00:10:41,480 --> 00:10:45,220
اما در واقع یک راه غیرمکانی خوب برای فکر کردن به این موضوع وجود دارد.

151
00:10:45,220 --> 00:10:49,100
هر جزء از گرادیان منفی دو چیز را به ما می گوید.

152
00:10:49,100 --> 00:10:53,600
علامت، البته، به ما می گوید که آیا جزء مربوط به

153
00:10:53,600 --> 00:10:55,860
بردار ورودی باید به سمت بالا یا پایین حرکت کند.

154
00:10:55,860 --> 00:11:01,340
اما مهمتر از همه، بزرگی نسبی همه این اجزا به نوعی

155
00:11:01,340 --> 00:11:05,620
به شما می گوید که کدام تغییرات اهمیت بیشتری دارند.

156
00:11:05,620 --> 00:11:09,780
ببینید، در شبکه ما، تعدیل یکی از وزن‌ها ممکن است تأثیر بسیار

157
00:11:09,780 --> 00:11:14,980
بیشتری بر تابع هزینه داشته باشد تا تعدیل با وزن دیگر.

158
00:11:14,980 --> 00:11:19,440
برخی از این اتصالات فقط برای داده های آموزشی ما اهمیت بیشتری دارند.

159
00:11:19,440 --> 00:11:23,520
بنابراین راهی که می‌توانید در مورد این بردار گرادیان تابع هزینه عظیم ما فکر کنید

160
00:11:23,520 --> 00:11:29,740
این است که اهمیت نسبی هر وزن و سوگیری را رمزگذاری می‌کند، یعنی اینکه

161
00:11:29,740 --> 00:11:34,100
کدام یک از این تغییرات بیشترین ضربه را برای شما به همراه خواهد داشت.

162
00:11:34,100 --> 00:11:37,360
این واقعاً فقط یک راه دیگر برای تفکر در مورد جهت است.

163
00:11:37,360 --> 00:11:41,740
برای مثال ساده‌تر، اگر تابعی با دو متغیر به‌عنوان ورودی داشته باشید، و

164
00:11:41,740 --> 00:11:48,720
محاسبه کنید که گرادیان آن در نقطه‌ای خاص به صورت 3،1 می‌آید، از

165
00:11:48,720 --> 00:11:52,880
یک طرف می‌توانید آن را به این صورت تفسیر کنید که وقتی

166
00:11:52,880 --> 00:11:57,400
ایستادن در آن ورودی، حرکت در امتداد این جهت، تابع را سریع‌تر افزایش

167
00:11:57,400 --> 00:12:02,200
می‌دهد، که وقتی تابع را در بالای صفحه نقاط ورودی نمودار می‌کنید، آن

168
00:12:02,200 --> 00:12:03,200
بردار همان چیزی است که جهت سربالایی مستقیم را به شما می‌دهد.

169
00:12:03,200 --> 00:12:07,600
اما راه دیگری برای خواندن این است که بگوییم تغییرات در این متغیر اول

170
00:12:07,600 --> 00:12:12,400
سه برابر تغییرات متغیر دوم اهمیت دارد، که حداقل در همسایگی ورودی مربوطه،

171
00:12:12,400 --> 00:12:17,740
هل دادن مقدار x برای شما ضربه بسیار بیشتری به همراه دارد. دلار

172
00:12:17,740 --> 00:12:22,880
بسیار خوب، بیایید بزرگنمایی کنیم و خلاصه کنیم که تا به حال کجا بوده ایم.

173
00:12:22,880 --> 00:12:28,660
شبکه خود این تابع با 784 ورودی و 10 خروجی است

174
00:12:28,660 --> 00:12:30,860
که بر حسب همه این مجموع وزنی تعریف شده است.

175
00:12:30,860 --> 00:12:34,160
تابع هزینه یک لایه از پیچیدگی در بالای آن است.

176
00:12:34,160 --> 00:12:39,300
این 13000 وزنه و سوگیری را به عنوان ورودی می گیرد و

177
00:12:39,300 --> 00:12:42,640
بر اساس نمونه های تمرینی، یک معیار تنبلی را نشان می دهد.

178
00:12:42,640 --> 00:12:47,520
گرادیان تابع هزینه یک لایه دیگر از پیچیدگی است.

179
00:12:47,520 --> 00:12:52,860
به ما می‌گوید چه ضربه‌هایی به همه این وزن‌ها و سوگیری‌ها باعث

180
00:12:52,860 --> 00:12:56,640
سریع‌ترین تغییر در مقدار تابع هزینه می‌شوند، که ممکن است به

181
00:12:56,640 --> 00:13:03,040
این صورت تعبیر کنید که تغییرات برای کدام وزن‌ها مهم‌تر است.

182
00:13:03,040 --> 00:13:07,620
بنابراین وقتی شبکه را با وزن‌ها و بایاس‌های تصادفی مقداردهی اولیه می‌کنید،

183
00:13:07,620 --> 00:13:12,420
و آن‌ها را چندین بار بر اساس این فرآیند نزول گرادیان تنظیم

184
00:13:12,420 --> 00:13:14,240
می‌کنید، واقعاً چقدر روی تصاویری که قبلاً دیده نشده‌اند، عملکرد خوبی دارد؟

185
00:13:14,240 --> 00:13:19,000
لایه ای که من در اینجا توضیح دادم، با دو لایه پنهان از 16 نورون که هر کدام عمدتاً به دلایل زیبایی

186
00:13:19,000 --> 00:13:26,920
شناختی انتخاب شده اند، بد نیست و حدود 96٪ از تصاویر جدیدی را که به درستی می بیند طبقه بندی می کند.

187
00:13:26,920 --> 00:13:31,580
و راستش را بخواهید، اگر به برخی از نمونه‌هایی که آن را خراب

188
00:13:31,580 --> 00:13:36,300
می‌کند نگاه کنید، احساس می‌کنید مجبور هستید کمی آن را کاهش دهید.

189
00:13:36,300 --> 00:13:40,220
اگر با ساختار لایه پنهان بازی کنید و چند ترفند

190
00:13:40,220 --> 00:13:41,220
ایجاد کنید، می توانید این را تا 98٪ دریافت کنید.

191
00:13:41,220 --> 00:13:42,900
و این خیلی خوب است!

192
00:13:42,900 --> 00:13:47,020
این بهترین نیست، مطمئناً می‌توانید با پیچیده‌تر شدن از این شبکه وانیلی ساده، عملکرد بهتری داشته باشید،

193
00:13:47,020 --> 00:13:52,460
اما با توجه به اینکه کار اولیه چقدر ترسناک است، فکر می‌کنم چیزی باورنکردنی در مورد

194
00:13:52,460 --> 00:13:56,800
هر شبکه‌ای وجود دارد که این کار را به خوبی روی تصاویر انجام می‌دهد که قبلاً هرگز

195
00:13:56,800 --> 00:14:02,000
دیده نشده بود. هرگز به طور خاص به آن نگفتیم که به دنبال چه الگوهایی باشد.

196
00:14:02,000 --> 00:14:07,840
در اصل، روشی که من به این ساختار انگیزه دادم، توصیف امیدی بود که

197
00:14:07,840 --> 00:14:11,880
ممکن بود داشته باشیم، که لایه دوم ممکن است روی لبه‌های کوچکی قرار بگیرد،

198
00:14:11,880 --> 00:14:16,080
لایه سوم آن لبه‌ها را برای تشخیص حلقه‌ها و خطوط طولانی‌تر کنار هم قرار

199
00:14:16,080 --> 00:14:18,220
دهد، و اینکه ممکن است آن‌ها تکه تکه شوند. با هم برای تشخیص ارقام

200
00:14:18,220 --> 00:14:21,040
بنابراین آیا این همان کاری است که شبکه ما در واقع انجام می دهد؟

201
00:14:21,040 --> 00:14:24,880
خوب، حداقل برای این یکی، اصلاً.

202
00:14:24,960 --> 00:14:29,120
به یاد داشته باشید که چگونه آخرین ویدیوی ما به این موضوع نگاه کردیم که چگونه وزن

203
00:14:29,120 --> 00:14:33,900
اتصالات از تمام نورون های لایه اول به یک نورون مشخص در لایه دوم را می توان

204
00:14:33,900 --> 00:14:37,440
به عنوان یک الگوی پیکسل معینی که نورون لایه دوم در حال برداشتن آن است تجسم کرد؟

205
00:14:37,440 --> 00:14:44,600
خوب، وقتی این کار را برای وزنه‌های مرتبط با این انتقال‌ها انجام می‌دهیم،

206
00:14:44,600 --> 00:14:51,000
به‌جای اینکه لبه‌های کوچک جدا شده را اینجا و آنجا انتخاب کنیم، تقریباً

207
00:14:51,000 --> 00:14:54,200
تصادفی به نظر می‌رسند، فقط با چند الگوی بسیار شل در وسط.

208
00:14:54,200 --> 00:14:59,020
به نظر می‌رسد که در فضای غیرقابل درک 13000 بعدی

209
00:14:59,020 --> 00:15:04,020
وزن‌ها و سوگیری‌های ممکن، شبکه ما خود را یک حداقل

210
00:15:04,020 --> 00:15:08,440
محلی خوشحال می‌یابد که، علی‌رغم طبقه‌بندی موفقیت‌آمیز اکثر تصاویر، دقیقاً

211
00:15:08,440 --> 00:15:09,840
الگوهایی را که ممکن بود انتظارش را داشتیم، انتخاب کند.

212
00:15:09,840 --> 00:15:14,600
و برای اینکه واقعاً این نقطه را به خانه هدایت کنید، ببینید وقتی یک تصویر تصادفی را وارد می کنید چه اتفاقی می افتد.

213
00:15:14,600 --> 00:15:19,240
اگر سیستم هوشمند بود، ممکن است انتظار داشته باشید که یا احساس نامطمئنی داشته باشد، شاید واقعاً

214
00:15:19,240 --> 00:15:24,120
هیچ یک از آن 10 نورون خروجی را فعال نکند یا همه آنها را به طور مساوی

215
00:15:24,520 --> 00:15:29,800
فعال کند، اما در عوض با اطمینان به شما پاسخ بیهوده ای می دهد، گویی مطمئن است

216
00:15:29,800 --> 00:15:34,560
که این تصادفی است. نویز 5 است، همانطور که یک تصویر واقعی از 5 یک 5 است.

217
00:15:34,560 --> 00:15:39,300
با بیان متفاوت، حتی اگر این شبکه بتواند ارقام را به

218
00:15:39,300 --> 00:15:41,800
خوبی تشخیص دهد، هیچ ایده ای برای ترسیم آنها ندارد.

219
00:15:41,800 --> 00:15:45,400
بسیاری از این به این دلیل است که این یک مجموعه آموزشی بسیار محدود است.

220
00:15:45,400 --> 00:15:48,220
منظورم این است که اینجا خود را به جای شبکه قرار دهید.

221
00:15:48,220 --> 00:15:53,280
از دیدگاه آن، کل جهان چیزی جز ارقام متحرک کاملاً مشخص که در

222
00:15:53,280 --> 00:15:58,560
یک شبکه کوچک متمرکز شده‌اند، تشکیل نمی‌شود، و تابع هزینه آن هرگز انگیزه‌ای

223
00:15:58,560 --> 00:16:02,160
به آن نداده است که چیزی جز اطمینان کامل در تصمیم‌هایش داشته باشد.

224
00:16:02,160 --> 00:16:05,760
بنابراین با این تصویری از کاری که آن نورون های لایه دوم

225
00:16:05,760 --> 00:16:09,320
واقعاً انجام می دهند، ممکن است تعجب کنید که چرا من این

226
00:16:09,320 --> 00:16:10,320
شبکه را با انگیزه برداشتن لبه ها و الگوها معرفی می کنم.

227
00:16:10,320 --> 00:16:13,040
منظورم این است که اصلاً این کاری نیست که در نهایت انجام شود.

228
00:16:13,040 --> 00:16:17,480
خوب، این هدف نهایی ما نیست، بلکه یک نقطه شروع است.

229
00:16:17,480 --> 00:16:22,280
صادقانه بگویم، این فناوری قدیمی است، نوعی که در دهه‌های 80 و 90 مورد تحقیق قرار

230
00:16:22,280 --> 00:16:26,920
گرفت، و قبل از اینکه بتوانید انواع مدرن با جزئیات بیشتری را درک کنید، باید آن

231
00:16:26,920 --> 00:16:31,380
را درک کنید، و به وضوح می‌تواند مشکلات جالبی را حل کند، اما هر چه بیشتر

232
00:16:31,380 --> 00:16:38,720
در مورد آن تحقیق کنید. این لایه‌های پنهان واقعاً کار می‌کنند، هرچه هوشمندتر به نظر می‌رسد.

233
00:16:38,720 --> 00:16:43,540
تغییر تمرکز برای لحظه ای از نحوه یادگیری شبکه ها به نحوه یادگیری شما، این

234
00:16:43,540 --> 00:16:47,160
تنها در صورتی اتفاق می افتد که به نحوی فعالانه با مطالب اینجا درگیر شوید.

235
00:16:47,160 --> 00:16:51,920
یک کار بسیار ساده که از شما می‌خواهم انجام دهید این است که همین الان مکث کنید

236
00:16:51,920 --> 00:16:57,560
و برای لحظه‌ای عمیقاً فکر کنید که اگر می‌خواهید چیزهایی مانند لبه‌ها و الگوها را بهتر

237
00:16:57,560 --> 00:17:01,880
ببیند، چه تغییراتی ممکن است در این سیستم ایجاد کنید و چگونه تصاویر را درک می‌کند.

238
00:17:01,880 --> 00:17:06,360
اما بهتر از آن، برای درگیر شدن با مطالب، کتاب مایکل نیلسن در

239
00:17:06,360 --> 00:17:09,720
مورد یادگیری عمیق و شبکه های عصبی را به شدت توصیه می کنم.

240
00:17:09,720 --> 00:17:15,200
در آن، می‌توانید کد و داده‌هایی را برای دانلود و بازی برای همین مثال پیدا کنید،

241
00:17:15,200 --> 00:17:19,360
و کتاب گام به گام شما را با آنچه که کد انجام می‌دهد، آشنا می‌کند.

242
00:17:19,360 --> 00:17:23,920
نکته جالب این است که این کتاب رایگان و در دسترس عموم است، بنابراین اگر چیزی

243
00:17:23,920 --> 00:17:28,040
از آن به دست آوردید، به من بپیوندید تا به تلاش‌های نیلسن کمک مالی کنید.

244
00:17:28,040 --> 00:17:32,060
من همچنین چند منبع دیگر را که بسیار دوستشان دارم را در توضیحات پیوند داده

245
00:17:32,060 --> 00:17:38,720
ام، از جمله پست فوق العاده و زیبای وبلاگ کریس اولا و مقالات در Distill.

246
00:17:38,720 --> 00:17:41,960
برای اینکه در چند دقیقه آخر همه چیز را در اینجا ببندم،

247
00:17:41,960 --> 00:17:44,440
می‌خواهم به قسمتی از مصاحبه‌ای که با لیشا لی داشتم برگردم.

248
00:17:44,440 --> 00:17:48,520
شاید او را از آخرین ویدیو به یاد بیاورید، او کار دکترای خود را در زمینه یادگیری عمیق انجام داد.

249
00:17:48,560 --> 00:17:52,240
در این قطعه کوچک، او در مورد دو مقاله اخیر صحبت می کند که

250
00:17:52,240 --> 00:17:56,380
واقعاً به چگونگی یادگیری برخی از شبکه های مدرن تشخیص تصویر می پردازد.

251
00:17:56,380 --> 00:18:00,320
فقط برای تعیین جایی که در مکالمه بودیم، مقاله اول یکی از این شبکه‌های عصبی عمیق

252
00:18:00,320 --> 00:18:04,480
را انتخاب کرد که واقعاً در تشخیص تصویر خوب است، و به جای آموزش آن بر

253
00:18:04,480 --> 00:18:09,400
روی یک مجموعه داده با برچسب مناسب، همه برچسب‌ها را قبل از آموزش به هم ریخت.

254
00:18:09,400 --> 00:18:13,840
بدیهی است که دقت تست در اینجا بهتر از تصادفی نخواهد

255
00:18:13,840 --> 00:18:15,320
بود، زیرا همه چیز به صورت تصادفی برچسب گذاری شده است.

256
00:18:15,320 --> 00:18:20,080
اما همچنان می‌توانست همان دقت آموزشی را که در مجموعه

257
00:18:20,080 --> 00:18:21,440
داده‌های دارای برچسب مناسب انجام می‌دهید، به دست آورد.

258
00:18:21,440 --> 00:18:26,120
اساساً، میلیون‌ها وزن برای این شبکه خاص کافی بود تا فقط داده‌های تصادفی را به خاطر

259
00:18:26,120 --> 00:18:31,040
بسپارد، که این سوال را ایجاد می‌کند که آیا به حداقل رساندن این تابع هزینه

260
00:18:31,040 --> 00:18:36,720
واقعاً با هر نوع ساختاری در تصویر مطابقت دارد یا فقط به خاطر سپردن است؟

261
00:18:36,720 --> 00:18:40,120
. . . برای حفظ کل مجموعه داده طبقه بندی صحیح.

262
00:18:40,120 --> 00:18:45,720
و بنابراین، می‌دانید، نیم سال بعد در ICML امسال،

263
00:18:45,720 --> 00:18:50,440
مقاله‌ای دقیقاً ابطال‌کننده نبود، بلکه مقاله‌ای وجود داشت

264
00:18:50,440 --> 00:18:52,220
که به برخی از جنبه‌های آن اشاره می‌کرد.

265
00:18:52,220 --> 00:18:59,600
اگر به آن منحنی دقت نگاه کنید، اگر فقط روی یک مجموعه داده تصادفی تمرین

266
00:18:59,600 --> 00:19:05,240
می‌کردید، می‌دانید که آن منحنی بسیار آهسته و تقریباً به شکلی خطی پایین می‌آید.

267
00:19:05,280 --> 00:19:10,840
بنابراین شما واقعاً در تلاش برای یافتن آن حداقل محلی از

268
00:19:10,840 --> 00:19:12,320
وزن‌های ممکن، می‌دانید، هستید که به شما این دقت را می‌دهد.

269
00:19:12,320 --> 00:19:16,720
در حالی که اگر شما واقعاً روی یک مجموعه داده ساختاریافته آموزش می‌دهید، مجموعه‌ای

270
00:19:16,720 --> 00:19:20,240
که دارای برچسب‌های مناسب است، می‌دانید که در ابتدا کمی کمانچه می‌چرخید، اما

271
00:19:20,240 --> 00:19:23,360
بعد از آن خیلی سریع افت کردید تا به آن سطح دقت برسید.

272
00:19:23,360 --> 00:19:28,580
و بنابراین به نوعی یافتن حداکثر محلی آسانتر بود.

273
00:19:28,580 --> 00:19:32,900
و بنابراین چیزی که در مورد آن جالب بود این است که مقاله

274
00:19:32,900 --> 00:19:39,140
دیگری مربوط به چند سال پیش را در معرض دید قرار می دهد

275
00:19:39,140 --> 00:19:40,140
که در مورد لایه های شبکه ساده سازی های بسیار بیشتری دارد.

276
00:19:40,140 --> 00:19:43,880
اما یکی از نتایج این بود که چگونه، اگر به چشم‌انداز بهینه‌سازی نگاه کنید، حداقل‌های

277
00:19:43,880 --> 00:19:49,400
محلی که این شبکه‌ها تمایل به یادگیری دارند، در واقع از کیفیت یکسانی برخوردار هستند.

278
00:19:49,400 --> 00:19:54,300
بنابراین به نوعی، اگر مجموعه داده های شما ساختار یافته باشد، باید بتوانید آن را خیلی راحت تر پیدا کنید.

279
00:19:58,580 --> 00:20:01,140
مثل همیشه از کسانی از شما که در Patreon حمایت می کنید تشکر می کنم.

280
00:20:01,480 --> 00:20:05,440
من قبلاً گفته بودم که تغییر دهنده بازی در Patreon

281
00:20:05,440 --> 00:20:07,160
چیست، اما این ویدیوها واقعاً بدون شما امکان پذیر نیستند.

282
00:20:07,160 --> 00:20:11,540
همچنین می‌خواهم تشکر ویژه‌ای از شرکت VC Amplify Partners و حمایت

283
00:20:11,540 --> 00:20:13,240
آن‌ها از این ویدیوهای اولیه در این مجموعه داشته باشم.

284
00:20:31,140 --> 00:20:33,140
متشکرم.

