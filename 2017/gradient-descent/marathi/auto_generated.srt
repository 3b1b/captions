1
00:00:00,000 --> 00:00:07,240
शेवटचा व्हिडिओ मी न्यूरल नेटवर्कची रचना मांडली.

2
00:00:07,240 --> 00:00:10,124
मी येथे एक द्रुत रीकॅप देईन जेणेकरुन ते आपल्या मनात ताजे

3
00:00:10,124 --> 00:00:13,160
असेल आणि नंतर या व्हिडिओसाठी माझी दोन मुख्य उद्दिष्टे आहेत.

4
00:00:13,160 --> 00:00:15,562
प्रथम म्हणजे ग्रेडियंट डिसेंटची कल्पना सादर करणे,

5
00:00:15,562 --> 00:00:19,406
ज्यामध्ये केवळ न्यूरल नेटवर्क कसे शिकतात असे नाही तर इतर मशीन लर्निंग कसे कार्य

6
00:00:19,406 --> 00:00:20,800
करते हे देखील अधोरेखित करते.

7
00:00:20,800 --> 00:00:25,429
त्यानंतर आम्ही हे विशिष्ट नेटवर्क कसे कार्य करते आणि न्यूरॉन्सचे

8
00:00:25,429 --> 00:00:29,560
ते लपलेले स्तर काय शोधत आहेत याबद्दल थोडे अधिक जाणून घेऊ.

9
00:00:29,560 --> 00:00:34,931
एक स्मरणपत्र म्हणून, आमचे लक्ष्य हस्तलिखित अंक ओळखीचे उत्कृष्ट उदाहरण आहे,

10
00:00:34,931 --> 00:00:37,080
न्यूरल नेटवर्कचे हॅलो वर्ल्ड.

11
00:00:37,080 --> 00:00:40,175
हे अंक 28x28 पिक्सेल ग्रिडवर रेंडर केले जातात,

12
00:00:40,175 --> 00:00:44,260
प्रत्येक पिक्सेलचे काही ग्रेस्केल मूल्य 0 आणि 1 दरम्यान असते.

13
00:00:44,260 --> 00:00:51,400
ते नेटवर्कच्या इनपुट लेयरमध्ये 784 न्यूरॉन्सची सक्रियता निर्धारित करतात.

14
00:00:51,400 --> 00:00:57,102
खालील स्तरांमधील प्रत्येक न्यूरॉनचे सक्रियकरण मागील लेयरमधील सर्व सक्रियतेच्या

15
00:00:57,102 --> 00:01:02,300
भारित बेरजेवर, तसेच काही विशेष संख्येवर आधारित आहे ज्याला बायस म्हणतात.

16
00:01:02,300 --> 00:01:06,692
तुम्ही ती बेरीज इतर काही फंक्शनसह तयार करा, जसे की सिग्मॉइड स्क्विशिफिकेशन,

17
00:01:06,692 --> 00:01:09,640
किंवा ReLU, ज्या प्रकारे मी शेवटचा व्हिडिओ पाहिला.

18
00:01:09,640 --> 00:01:15,812
एकूण, प्रत्येकी 16 न्यूरॉन्ससह दोन लपविलेल्या स्तरांची काहीशी अनियंत्रित निवड दिल्यास,

19
00:01:15,812 --> 00:01:20,921
नेटवर्कमध्ये सुमारे 13,000 वजने आणि पूर्वाग्रह आहेत जे आपण समायोजित करू

20
00:01:20,921 --> 00:01:25,320
शकतो आणि ही मूल्ये नेटवर्क नेमके काय करते हे निर्धारित करतात.

21
00:01:25,320 --> 00:01:29,599
आणि जेव्हा आपण म्हणतो की हे नेटवर्क दिलेल्या अंकाचे वर्गीकरण करते तेव्हा त्याचा अर्थ

22
00:01:29,599 --> 00:01:34,080
असा होतो की अंतिम स्तरातील त्या 10 न्यूरॉन्सपैकी सर्वात तेजस्वी त्या अंकाशी संबंधित आहे.

23
00:01:34,080 --> 00:01:39,106
आणि लक्षात ठेवा, स्तरित संरचनेसाठी आमच्या मनात असलेली प्रेरणा ही होती की

24
00:01:39,106 --> 00:01:43,994
कदाचित दुसरा स्तर कडांवर उचलू शकेल, तिसरा स्तर लूप आणि रेषा यांसारख्या

25
00:01:43,994 --> 00:01:49,640
नमुन्यांवर उचलू शकेल आणि शेवटचा थर फक्त त्या नमुन्यांना एकत्र करू शकेल. अंक ओळखा.

26
00:01:49,640 --> 00:01:52,880
तर इथे, नेटवर्क कसे शिकते ते आपण शिकतो.

27
00:01:52,880 --> 00:01:57,150
आम्हाला काय हवे आहे ते एक अल्गोरिदम आहे जिथे तुम्ही या नेटवर्कला प्रशिक्षण

28
00:01:57,150 --> 00:02:01,421
डेटाचा संपूर्ण समूह दर्शवू शकता, जे हस्तलिखित अंकांच्या विविध प्रतिमांच्या

29
00:02:01,421 --> 00:02:04,781
गुच्छाच्या स्वरूपात येते आणि ते काय असावे याच्या लेबलांसह,

30
00:02:04,781 --> 00:02:09,051
आणि ते ते 13,000 वजन आणि पूर्वाग्रह समायोजित करा जेणेकरून प्रशिक्षण डेटावर

31
00:02:09,051 --> 00:02:10,760
त्याचे कार्यप्रदर्शन सुधारेल.

32
00:02:10,760 --> 00:02:14,381
आशा आहे की या स्तरित संरचनेचा अर्थ असा होईल की ते जे शिकते ते त्या

33
00:02:14,381 --> 00:02:17,840
प्रशिक्षण डेटाच्या पलीकडे असलेल्या प्रतिमांना सामान्यीकृत करते.

34
00:02:17,840 --> 00:02:23,337
आम्ही ज्या प्रकारे चाचणी करतो ते म्हणजे तुम्ही नेटवर्क प्रशिक्षित केल्यानंतर,

35
00:02:23,337 --> 00:02:27,495
तुम्ही त्यास अधिक लेबल केलेला डेटा दाखवता आणि ते त्या नवीन

36
00:02:27,495 --> 00:02:31,160
प्रतिमांचे किती अचूक वर्गीकरण करते ते तुम्ही पाहता.

37
00:02:31,160 --> 00:02:35,039
सुदैवाने आमच्यासाठी, आणि हे एक सामान्य उदाहरण ज्याने सुरू केले आहे,

38
00:02:35,039 --> 00:02:39,888
ते म्हणजे MNIST डेटाबेसच्या मागे असलेल्या चांगल्या लोकांनी हजारो हस्तलिखीत अंकांच्या

39
00:02:39,888 --> 00:02:45,022
प्रतिमांचा संग्रह केला आहे, ज्या प्रत्येकावर ते असायला हवे त्या संख्येसह लेबल केलेले आहेत.

40
00:02:45,022 --> 00:02:45,080


41
00:02:45,080 --> 00:02:48,448
आणि एखाद्या मशीनचे शिक्षण म्हणून वर्णन करणे जितके उत्तेजक आहे,

42
00:02:48,448 --> 00:02:50,961
ते कसे कार्य करते हे एकदा तुम्ही पाहिल्यानंतर,

43
00:02:50,961 --> 00:02:55,560
ते काही वेडगळ विज्ञान-कल्पनाप्रमाणे कमी आणि कॅल्क्युलस व्यायामासारखे बरेच काही वाटते.

44
00:02:55,560 --> 00:03:01,040
मला असे म्हणायचे आहे की, मुळात हे विशिष्ट फंक्शनचे किमान शोधण्यासाठी खाली येते.

45
00:03:01,040 --> 00:03:05,694
लक्षात ठेवा, संकल्पनात्मकदृष्ट्या आम्ही प्रत्येक न्यूरॉनचा आधीच्या लेयरमधील

46
00:03:05,694 --> 00:03:10,471
सर्व न्यूरॉन्सशी जोडलेला आहे असा विचार करत आहोत आणि त्याचे सक्रियकरण परिभाषित

47
00:03:10,471 --> 00:03:15,003
करणार्‍या भारित बेरीजमधील वजन हे त्या कनेक्शनच्या सामर्थ्यासारखे आहेत आणि

48
00:03:15,003 --> 00:03:19,780
पूर्वाग्रह हे काही संकेत आहेत. तो न्यूरॉन सक्रिय किंवा निष्क्रिय आहे की नाही.

49
00:03:19,780 --> 00:03:22,400
आणि गोष्टी सुरू करण्यासाठी, आम्ही ते सर्व वजन आणि

50
00:03:22,400 --> 00:03:25,020
पूर्वाग्रह पूर्णपणे यादृच्छिकपणे सुरू करणार आहोत.

51
00:03:25,020 --> 00:03:29,441
हे सांगण्याची गरज नाही, हे नेटवर्क दिलेल्या प्रशिक्षण उदाहरणावर भयानक कामगिरी करणार आहे,

52
00:03:29,441 --> 00:03:31,180
कारण ते काहीतरी यादृच्छिक करत आहे.

53
00:03:31,180 --> 00:03:36,820
उदाहरणार्थ, तुम्ही 3 च्या या प्रतिमेमध्ये फीड करा आणि आउटपुट लेयर फक्त गोंधळासारखे दिसते.

54
00:03:36,820 --> 00:03:41,509
तर तुम्ही काय करता ते म्हणजे कॉस्ट फंक्शन, कॉम्प्युटरला सांगण्याचा एक मार्ग,

55
00:03:41,509 --> 00:03:45,407
नाही, खराब कॉम्प्युटर, त्या आउटपुटमध्ये सक्रियता असली पाहिजे जी

56
00:03:45,407 --> 00:03:48,940
बहुतेक न्यूरॉन्ससाठी 0 असते, परंतु या न्यूरॉनसाठी 1 असते.

57
00:03:48,940 --> 00:03:51,740
तू मला जे दिले ते पूर्णपणे कचरा आहे.

58
00:03:51,740 --> 00:03:56,549
थोडे अधिक गणितीय दृष्ट्या सांगायचे तर, तुम्ही त्या प्रत्येक कचरा

59
00:03:56,549 --> 00:04:01,062
आउटपुट सक्रियकरणातील फरकांचे वर्ग जोडता आणि तुम्हाला त्यांचे

60
00:04:01,062 --> 00:04:06,020
मूल्य हवे आहे आणि यालाच आम्ही एका प्रशिक्षण उदाहरणाची किंमत म्हणू.

61
00:04:06,020 --> 00:04:09,985
लक्षात घ्या जेव्हा नेटवर्क आत्मविश्वासाने प्रतिमेचे अचूक

62
00:04:09,985 --> 00:04:14,437
वर्गीकरण करते तेव्हा ही बेरीज लहान असते, परंतु जेव्हा नेटवर्कला

63
00:04:14,437 --> 00:04:18,820
ते काय करत आहे हे माहित नसल्यासारखे दिसते तेव्हा ती मोठी असते.

64
00:04:18,820 --> 00:04:23,061
तर मग तुम्ही जे कराल ते तुमच्या सर्व दहा हजार

65
00:04:23,061 --> 00:04:27,580
प्रशिक्षण उदाहरणांच्या सरासरी खर्चाचा विचार करा.

66
00:04:27,580 --> 00:04:33,300
नेटवर्क किती खराब आहे आणि संगणकाला किती वाईट वाटले पाहिजे यासाठी ही सरासरी किंमत आहे.

67
00:04:33,300 --> 00:04:35,300
आणि ही एक गुंतागुंतीची गोष्ट आहे.

68
00:04:35,300 --> 00:04:40,451
लक्षात ठेवा की नेटवर्क स्वतःच एक फंक्शन कसे होते, जे इनपुट म्हणून 784 संख्या घेते,

69
00:04:40,451 --> 00:04:45,293
पिक्सेल व्हॅल्यूज घेते आणि त्याचे आउटपुट म्हणून 10 संख्या बाहेर टाकते आणि एका

70
00:04:45,293 --> 00:04:49,700
अर्थाने ते या सर्व वजन आणि पूर्वाग्रहांद्वारे पॅरामीटराइज्ड केले जाते?

71
00:04:49,700 --> 00:04:53,340
कॉस्ट फंक्शन हा त्यावरील गुंतागुंतीचा एक थर आहे.

72
00:04:53,340 --> 00:04:58,606
हे 13,000 किंवा त्यापेक्षा जास्त वजने आणि बायसेस इनपुट म्हणून घेते आणि ते वजन आणि

73
00:04:58,606 --> 00:05:03,937
बायसेस किती वाईट आहेत याचे वर्णन करणारी एकच संख्या बाहेर टाकते आणि ते कसे परिभाषित

74
00:05:03,937 --> 00:05:09,140
केले जाते ते प्रशिक्षण डेटाच्या हजारो भागांवर नेटवर्कच्या वर्तनावर अवलंबून असते.

75
00:05:09,140 --> 00:05:12,460
खूप विचार करण्यासारखे आहे.

76
00:05:12,460 --> 00:05:16,380
पण कॉम्प्युटरला फक्त सांगणे हे काय वाईट काम आहे ते फारसे उपयुक्त नाही.

77
00:05:16,380 --> 00:05:21,300
ते वजन आणि पूर्वाग्रह कसे बदलायचे ते तुम्हाला सांगायचे आहे जेणेकरून ते चांगले होईल.

78
00:05:21,300 --> 00:05:25,849
ते सोपे करण्यासाठी, 13,000 इनपुट्ससह फंक्शनची कल्पना करण्यासाठी संघर्ष करण्याऐवजी,

79
00:05:25,849 --> 00:05:29,247
फक्त एका साध्या फंक्शनची कल्पना करा ज्यामध्ये एक संख्या इनपुट

80
00:05:29,247 --> 00:05:31,440
म्हणून आणि एक संख्या आउटपुट म्हणून आहे.

81
00:05:31,440 --> 00:05:36,420
या फंक्शनचे मूल्य कमी करणारे इनपुट कसे शोधायचे?

82
00:05:36,420 --> 00:05:41,591
कॅल्क्युलसच्या विद्यार्थ्यांना हे कळेल की तुम्ही काहीवेळा ते किमान स्पष्टपणे काढू शकता,

83
00:05:41,591 --> 00:05:44,764
परंतु ते खरोखर क्लिष्ट फंक्शन्ससाठी नेहमीच शक्य नसते,

84
00:05:44,764 --> 00:05:49,759
आमच्या वेड्या गुंतागुंतीच्या न्यूरल नेटवर्क कॉस्ट फंक्शनसाठी या परिस्थितीच्या 13,000

85
00:05:49,759 --> 00:05:51,640
इनपुट आवृत्तीमध्ये नक्कीच नाही.

86
00:05:51,640 --> 00:05:55,658
अधिक लवचिक युक्ती म्हणजे कोणत्याही इनपुटपासून प्रारंभ करणे आणि ते

87
00:05:55,658 --> 00:05:59,860
आउटपुट कमी करण्यासाठी आपण कोणत्या दिशेने पाऊल टाकले पाहिजे हे शोधणे.

88
00:05:59,860 --> 00:06:05,667
विशेषत:, तुम्ही जेथे आहात त्या फंक्शनचा उतार तुम्ही काढू शकत असल्यास,

89
00:06:05,667 --> 00:06:12,720
तो उतार सकारात्मक असल्यास डावीकडे सरकवा आणि उतार ऋणात्मक असल्यास इनपुट उजवीकडे वळवा.

90
00:06:12,720 --> 00:06:16,593
तुम्ही हे वारंवार करत असल्यास, प्रत्येक बिंदूवर नवीन उतार तपासत आहात आणि

91
00:06:16,593 --> 00:06:20,680
योग्य पाऊल उचलत आहात, तुम्ही काही स्थानिक किमान फंक्शनशी संपर्क साधणार आहात.

92
00:06:20,680 --> 00:06:24,600
आणि इथे तुमच्या मनात असलेली प्रतिमा टेकडीवरून खाली लोटणारा चेंडू आहे.

93
00:06:24,600 --> 00:06:27,803
आणि लक्षात घ्या, या खरोखरच सरलीकृत सिंगल इनपुट फंक्शनसाठीही,

94
00:06:27,803 --> 00:06:31,111
तुम्ही कोणत्या यादृच्छिक इनपुटपासून सुरुवात करता यावर अवलंबून,

95
00:06:31,111 --> 00:06:34,891
अनेक संभाव्य व्हॅली आहेत ज्यात तुम्ही उतरू शकता आणि तुम्ही ज्या स्थानिक

96
00:06:34,891 --> 00:06:39,460
किमान मध्ये उतरता ते सर्वात लहान संभाव्य मूल्य असेल याची कोणतीही हमी नाही. खर्च कार्य.

97
00:06:39,460 --> 00:06:43,180
ते आमच्या न्यूरल नेटवर्क केसमध्ये देखील जाईल.

98
00:06:43,180 --> 00:06:47,477
आणि मी तुम्हाला हे देखील लक्षात घ्यायचे आहे की जर तुम्ही तुमच्या पायऱ्यांचा आकार

99
00:06:47,477 --> 00:06:51,403
उताराच्या प्रमाणात कसा बनवलात, तर जेव्हा उतार किमान दिशेने सपाट होत असेल,

100
00:06:51,403 --> 00:06:56,020
तेव्हा तुमची पावले लहान होत जातात आणि अशा प्रकारची तुम्हाला ओव्हरशूटिंगपासून मदत होते.

101
00:06:56,020 --> 00:07:01,640
जटिलता थोडीशी वाढवून, त्याऐवजी दोन इनपुट आणि एक आउटपुट असलेल्या फंक्शनची कल्पना करा.

102
00:07:01,640 --> 00:07:05,055
तुम्ही इनपुट स्पेसचा xy-प्लेन म्हणून विचार करू शकता आणि

103
00:07:05,055 --> 00:07:09,020
खर्चाचे कार्य त्याच्या वरच्या पृष्ठभागाच्या रूपात आलेख केले आहे.

104
00:07:09,020 --> 00:07:14,400
फंक्शनच्या स्लोपबद्दल विचारण्याऐवजी, फंक्शनचे आउटपुट लवकर कमी करण्यासाठी

105
00:07:14,400 --> 00:07:19,780
या इनपुट स्पेसमध्ये तुम्ही कोणत्या दिशेने पाऊल टाकावे हे विचारावे लागेल.

106
00:07:19,780 --> 00:07:22,340
दुसऱ्या शब्दांत, उताराची दिशा काय आहे?

107
00:07:22,340 --> 00:07:26,740
आणि पुन्हा, त्या टेकडीवरून बॉल फिरवण्याचा विचार करणे उपयुक्त आहे.

108
00:07:26,740 --> 00:07:31,068
तुमच्यापैकी जे मल्टीव्हेरिएबल कॅल्क्युलसशी परिचित आहेत त्यांना हे कळेल

109
00:07:31,068 --> 00:07:34,847
की फंक्शनचा ग्रेडियंट तुम्हाला सर्वात जास्त चढाईची दिशा देतो,

110
00:07:34,847 --> 00:07:39,420
फंक्शन सर्वात वेगाने वाढवण्यासाठी तुम्ही कोणत्या दिशेने पाऊल टाकले पाहिजे.

111
00:07:39,420 --> 00:07:43,544
साहजिकच पुरेसे आहे, त्या ग्रेडियंटचे ऋण घेतल्याने तुम्हाला

112
00:07:43,544 --> 00:07:47,460
पायरीची दिशा मिळते ज्यामुळे कार्य सर्वात लवकर कमी होते.

113
00:07:47,460 --> 00:07:54,580
त्याहूनही अधिक, या ग्रेडियंट वेक्टरची लांबी ही सर्वात उंच उतार किती तीव्र आहे हे दर्शवते.

114
00:07:54,580 --> 00:07:57,732
आता जर तुम्हाला मल्टीव्हेरिएबल कॅल्क्युलसबद्दल अपरिचित असेल आणि तुम्हाला

115
00:07:57,732 --> 00:08:01,100
अधिक जाणून घ्यायचे असेल, तर मी खान अकादमीसाठी या विषयावर केलेले काही काम पहा.

116
00:08:01,100 --> 00:08:04,623
खरे सांगायचे तर, तुमच्यासाठी आणि माझ्यासाठी सध्या महत्त्वाची गोष्ट

117
00:08:04,623 --> 00:08:08,147
म्हणजे या वेक्टरची गणना करण्याचा एक मार्ग तत्त्वतः अस्तित्वात आहे,

118
00:08:08,147 --> 00:08:12,040
हा वेक्टर जो तुम्हाला उताराची दिशा काय आहे आणि ती किती उंच आहे हे सांगते.

119
00:08:12,040 --> 00:08:17,280
तुम्हाला एवढंच माहीत असेल आणि तुम्ही तपशिलांवर ठोस नसाल तर तुम्ही ठीक असाल.

120
00:08:17,280 --> 00:08:22,154
कारण जर तुम्ही ते मिळवू शकत असाल तर, फंक्शन कमी करण्यासाठी अल्गोरिदम म्हणजे या

121
00:08:22,154 --> 00:08:27,400
ग्रेडियंट दिशेची गणना करणे, नंतर उतारावर एक लहान पाऊल घ्या आणि ते पुन्हा पुन्हा करा.

122
00:08:27,400 --> 00:08:33,700
2 इनपुट ऐवजी 13,000 इनपुट्स असलेल्या फंक्शनची हीच मूळ कल्पना आहे.

123
00:08:33,700 --> 00:08:36,843
आमच्या नेटवर्कचे सर्व 13,000 वजन आणि पक्षपात एका

124
00:08:36,843 --> 00:08:40,180
विशाल स्तंभ वेक्टरमध्ये आयोजित करण्याची कल्पना करा.

125
00:08:40,180 --> 00:08:44,584
कॉस्ट फंक्शनचा नकारात्मक ग्रेडियंट हा फक्त एक वेक्टर आहे,

126
00:08:44,584 --> 00:08:50,052
या प्रचंड मोठ्या इनपुट स्पेसच्या आत ही काही दिशा आहे जी तुम्हाला सांगते

127
00:08:50,052 --> 00:08:55,900
की या सर्व आकड्यांकडे कोणते नज केल्याने खर्च फंक्शनमध्ये सर्वात जलद घट होईल.

128
00:08:55,900 --> 00:08:59,267
आणि अर्थातच, आमच्या खास डिझाइन केलेल्या किमतीच्या कार्यासह,

129
00:08:59,267 --> 00:09:03,309
वजन आणि पूर्वाग्रह बदलून ते कमी करणे म्हणजे प्रशिक्षण डेटाच्या प्रत्येक

130
00:09:03,309 --> 00:09:07,014
तुकड्यावर नेटवर्कचे आउटपुट 10 मूल्यांच्या यादृच्छिक अॅरेसारखे कमी

131
00:09:07,014 --> 00:09:11,280
दिसणे आणि आम्हाला हवे असलेल्या वास्तविक निर्णयासारखे दिसते. ते बनवण्यासाठी.

132
00:09:11,280 --> 00:09:15,477
हे लक्षात ठेवणे महत्त्वाचे आहे की, या खर्चाच्या कार्यामध्ये सर्व

133
00:09:15,477 --> 00:09:19,868
प्रशिक्षण डेटावर सरासरीचा समावेश होतो, म्हणून जर तुम्ही ते कमी केले

134
00:09:19,868 --> 00:09:24,260
तर याचा अर्थ त्या सर्व नमुन्यांवर ते अधिक चांगले कार्यप्रदर्शन आहे.

135
00:09:24,260 --> 00:09:27,336
या ग्रेडियंटची कार्यक्षमतेने गणना करण्यासाठी अल्गोरिदम,

136
00:09:27,336 --> 00:09:31,952
जे प्रभावीपणे न्यूरल नेटवर्क कसे शिकते याचे हृदय आहे, त्याला बॅकप्रोपॅगेशन म्हणतात,

137
00:09:31,952 --> 00:09:34,040
आणि मी पुढील व्हिडिओबद्दल बोलणार आहे.

138
00:09:34,040 --> 00:09:38,742
तेथे, प्रशिक्षण डेटाच्या दिलेल्या भागासाठी प्रत्येक वजन आणि पूर्वाग्रहाचे नेमके काय

139
00:09:38,742 --> 00:09:41,653
होते ते जाणून घेण्यासाठी मला खरोखर वेळ काढायचा आहे,

140
00:09:41,653 --> 00:09:46,356
संबंधित कॅल्क्युलस आणि सूत्रांच्या ढिगाऱ्याच्या पलीकडे काय घडत आहे याची अंतर्ज्ञानी

141
00:09:46,356 --> 00:09:47,980
भावना देण्याचा प्रयत्न करतो.

142
00:09:47,980 --> 00:09:50,744
आत्ता इथे, आत्ता, मी तुम्हाला जाणून घ्यायची आहे,

143
00:09:50,744 --> 00:09:54,185
अंमलबजावणीच्या तपशिलांपासून स्वतंत्र आहे, जेव्हा आपण नेटवर्क

144
00:09:54,185 --> 00:09:59,263
लर्निंगबद्दल बोलतो तेव्हा आपल्याला काय म्हणायचे आहे ते म्हणजे केवळ खर्चाचे कार्य कमी करणे.

145
00:09:59,263 --> 00:09:59,320


146
00:09:59,320 --> 00:10:04,214
आणि लक्षात घ्या, त्याचा एक परिणाम असा आहे की या किमतीच्या कार्यासाठी एक छान गुळगुळीत

147
00:10:04,214 --> 00:10:09,340
आउटपुट असणे महत्वाचे आहे, जेणेकरुन आपण उतारावर थोडेसे पाऊल टाकून स्थानिक किमान शोधू शकू.

148
00:10:09,340 --> 00:10:14,668
म्हणूनच, बायनरी पद्धतीने सक्रिय किंवा निष्क्रिय राहण्याऐवजी

149
00:10:14,668 --> 00:10:20,440
कृत्रिम न्यूरॉन्समध्ये जैविक न्यूरॉन्सप्रमाणे सतत सक्रियता असते.

150
00:10:20,440 --> 00:10:23,567
नकारात्मक ग्रेडियंटच्या काही गुणाकाराने फंक्शनच्या इनपुटला

151
00:10:23,567 --> 00:10:26,960
वारंवार नडज करण्याच्या या प्रक्रियेला ग्रेडियंट डिसेंट म्हणतात.

152
00:10:26,960 --> 00:10:31,308
काही स्थानिक किमान खर्चाच्या फंक्शनकडे अभिसरण करण्याचा हा एक मार्ग आहे,

153
00:10:31,308 --> 00:10:33,000
मुळात या आलेखामध्ये एक दरी.

154
00:10:33,000 --> 00:10:36,535
मी अजूनही दोन इनपुटसह फंक्शनचे चित्र दाखवत आहे, अर्थातच,

155
00:10:36,535 --> 00:10:41,374
कारण 13,000 डायमेन्शनल इनपुट स्पेसमधील नज हे तुमचे मन गुंडाळणे थोडे कठीण आहे,

156
00:10:41,374 --> 00:10:45,220
परंतु याविषयी विचार करण्याचा एक चांगला गैर-स्थानिक मार्ग आहे.

157
00:10:45,220 --> 00:10:49,100
ऋण ग्रेडियंटचा प्रत्येक घटक आपल्याला दोन गोष्टी सांगतो.

158
00:10:49,100 --> 00:10:52,847
इनपुट व्हेक्टरचा संबंधित घटक वर किंवा खाली ढकलायचा

159
00:10:52,847 --> 00:10:55,860
की नाही हे चिन्ह अर्थातच आम्हाला सांगते.

160
00:10:55,860 --> 00:11:00,585
पण महत्त्वाचे म्हणजे, या सर्व घटकांचे सापेक्ष

161
00:11:00,585 --> 00:11:05,620
परिमाण कोणते बदल अधिक महत्त्वाचे आहेत हे सांगते.

162
00:11:05,620 --> 00:11:10,191
तुम्ही पाहता, आमच्या नेटवर्कमध्ये, वजनांपैकी एकाचे समायोजन इतर

163
00:11:10,191 --> 00:11:14,980
वजनाच्या समायोजनापेक्षा किमतीच्या कार्यावर जास्त परिणाम करू शकते.

164
00:11:14,980 --> 00:11:19,440
यापैकी काही कनेक्शन आमच्या प्रशिक्षण डेटासाठी अधिक महत्त्वाचे आहेत.

165
00:11:19,440 --> 00:11:24,425
तर तुम्ही आमच्या मनाच्या प्रचंड खर्चाच्या कार्याच्या या ग्रेडियंट वेक्टरबद्दल विचार

166
00:11:24,425 --> 00:11:29,470
करू शकता हा एक मार्ग आहे की ते प्रत्येक वजन आणि पूर्वाग्रहाचे सापेक्ष महत्त्व एन्कोड

167
00:11:29,470 --> 00:11:34,100
करते, म्हणजेच, यापैकी कोणता बदल तुमच्या पैशासाठी सर्वात जास्त दणका देणार आहे.

168
00:11:34,100 --> 00:11:37,360
दिग्दर्शनाबद्दल विचार करण्याचा हा खरोखर दुसरा मार्ग आहे.

169
00:11:37,360 --> 00:11:42,465
सोप्या उदाहरणासाठी, जर तुमच्याकडे इनपुट म्हणून दोन व्हेरिएबल्ससह काही फंक्शन असेल

170
00:11:42,465 --> 00:11:47,322
आणि एखाद्या विशिष्ट बिंदूवर त्याचा ग्रेडियंट 3,1 म्हणून बाहेर येतो असे मोजले,

171
00:11:47,322 --> 00:11:52,739
तर एकीकडे तुम्ही याचा अर्थ असा लावू शकता की जेव्हा तुम्ही आहात त्या इनपुटवर उभे राहून,

172
00:11:52,739 --> 00:11:58,032
या दिशेला जाण्याने फंक्शन सर्वात लवकर वाढते, की जेव्हा तुम्ही इनपुट पॉइंट्सच्या समतल

173
00:11:58,032 --> 00:12:03,200
भागाच्या वरच्या फंक्शनचा आलेख बनवता तेव्हा तो वेक्टर तुम्हाला सरळ चढाची दिशा देतो.

174
00:12:03,200 --> 00:12:07,764
पण हे वाचण्याचा आणखी एक मार्ग म्हणजे या पहिल्या व्हेरिएबलमधील बदलांना

175
00:12:07,764 --> 00:12:11,285
दुसऱ्या व्हेरिएबलमधील बदलांपेक्षा तिप्पट महत्त्व आहे,

176
00:12:11,285 --> 00:12:16,175
म्हणजे किमान संबंधित इनपुटच्या शेजारी, x-व्हॅल्यूला धक्का लावणे आपल्यासाठी

177
00:12:16,175 --> 00:12:17,740
खूप मोठा धक्कादायक आहे.

178
00:12:17,740 --> 00:12:22,880
बोकड ठीक आहे, चला झूम कमी करू आणि आपण आतापर्यंत कुठे आहोत याची बेरीज करू.

179
00:12:22,880 --> 00:12:26,941
784 इनपुट आणि 10 आउटपुट असलेले हे नेटवर्क हे फंक्शन आहे,

180
00:12:26,941 --> 00:12:30,860
जे या सर्व भारित रकमांच्या संदर्भात परिभाषित केले आहे.

181
00:12:30,860 --> 00:12:34,160
कॉस्ट फंक्शन हा त्यावरील गुंतागुंतीचा एक थर आहे.

182
00:12:34,160 --> 00:12:38,483
हे इनपुट म्हणून 13,000 वजने आणि पूर्वाग्रह घेते आणि

183
00:12:38,483 --> 00:12:42,640
प्रशिक्षण उदाहरणांच्या आधारे एकच माप काढून टाकते.

184
00:12:42,640 --> 00:12:47,520
कॉस्ट फंक्शनचा ग्रेडियंट हा अजून एक गुंतागुंतीचा थर आहे.

185
00:12:47,520 --> 00:12:52,693
हे आम्हाला सांगते की या सर्व वजन आणि पूर्वाग्रहांना कोणकोणते धक्कादायक

186
00:12:52,693 --> 00:12:57,283
कारणे किंमत कार्याच्या मूल्यामध्ये सर्वात जलद बदल घडवून आणतात,

187
00:12:57,283 --> 00:13:03,040
ज्याचा तुम्ही अर्थ सांगू शकता की कोणत्या वजनांमध्ये कोणते बदल महत्त्वाचे आहेत.

188
00:13:03,040 --> 00:13:06,824
त्यामुळे जेव्हा तुम्ही यादृच्छिक वजन आणि पूर्वाग्रहांसह नेटवर्क सुरू करता

189
00:13:06,824 --> 00:13:10,506
आणि या ग्रेडियंट डिसेंट प्रक्रियेच्या आधारे ते अनेक वेळा समायोजित करता,

190
00:13:10,506 --> 00:13:14,240
तेव्हा ते यापूर्वी कधीही न पाहिलेल्या प्रतिमांवर किती चांगले कार्य करते?

191
00:13:14,240 --> 00:13:19,517
मी येथे वर्णन केलेले, प्रत्येकी 16 न्यूरॉन्सच्या दोन लपलेल्या स्तरांसह,

192
00:13:19,517 --> 00:13:23,768
मुख्यतः सौंदर्याच्या कारणांसाठी निवडले गेलेले, वाईट नाही,

193
00:13:23,768 --> 00:13:26,920
ते 96% नवीन प्रतिमा अचूकपणे वर्गीकृत करते.

194
00:13:26,920 --> 00:13:31,433
आणि प्रामाणिकपणे, जर तुम्ही त्यात काही गडबड केलेली

195
00:13:31,433 --> 00:13:36,300
उदाहरणे पाहिली तर तुम्हाला ते थोडे ढिले करणे भाग पडते.

196
00:13:36,300 --> 00:13:39,580
तुम्ही लपविलेल्या लेयर स्ट्रक्चरसह खेळल्यास आणि काही बदल केल्यास,

197
00:13:39,580 --> 00:13:41,220
तुम्ही हे 98% पर्यंत मिळवू शकता.

198
00:13:41,220 --> 00:13:42,900
आणि ते खूप चांगले आहे!

199
00:13:42,900 --> 00:13:47,704
हे सर्वोत्कृष्ट नाही, या प्लेन व्हॅनिला नेटवर्कपेक्षा अधिक अत्याधुनिक बनून तुम्ही

200
00:13:47,704 --> 00:13:52,332
नक्कीच चांगली कामगिरी मिळवू शकता, परंतु सुरुवातीचे काम किती कठीण आहे हे पाहता,

201
00:13:52,332 --> 00:13:57,078
मला वाटते की कोणत्याही नेटवर्कमध्ये प्रतिमांवर असे चांगले कार्य करत असताना याआधी

202
00:13:57,078 --> 00:14:02,000
कधीही न पाहिलेले काहीतरी अविश्वसनीय आहे. कोणते नमुने पहावेत असे कधीच सांगितले नाही.

203
00:14:02,000 --> 00:14:07,543
मूलतः, मी या संरचनेला प्रेरित करण्याचा मार्ग म्हणजे आमच्याकडे असलेल्या एका आशेचे

204
00:14:07,543 --> 00:14:12,744
वर्णन करून, की दुसरा थर लहान कडांवर उठू शकेल, की तिसरा थर लूप आणि लांब रेषा

205
00:14:12,744 --> 00:14:18,220
ओळखण्यासाठी त्या कडा एकत्र करेल आणि ते तुकडे केले जातील. अंक ओळखण्यासाठी एकत्र.

206
00:14:18,220 --> 00:14:21,040
मग आमचे नेटवर्क हेच करत आहे का?

207
00:14:21,040 --> 00:14:24,880
बरं, यासाठी किमान, अजिबात नाही.

208
00:14:24,880 --> 00:14:28,869
पहिल्या लेयरमधील सर्व न्यूरॉन्सपासून दुस-या लेयरमधील दिलेल्या न्यूरॉनच्या

209
00:14:28,869 --> 00:14:32,965
कनेक्शनचे वजन दुसऱ्या लेयरचे न्यूरॉन उचलत असलेल्या पिक्सेल पॅटर्नच्या रूपात

210
00:14:32,965 --> 00:14:37,440
कसे व्हिज्युअलाइज केले जाऊ शकते हे आम्ही शेवटचा व्हिडिओ कसा पाहिला ते लक्षात ठेवा?

211
00:14:37,440 --> 00:14:42,529
बरं, जेव्हा आपण या संक्रमणांशी संबंधित वजनांसाठी ते करतो,

212
00:14:42,529 --> 00:14:48,496
तेव्हा इकडे-तिकडे वेगळ्या छोट्या कडांवर उचलण्याऐवजी, ते अगदी बरोबर,

213
00:14:48,496 --> 00:14:54,200
जवळजवळ यादृच्छिक दिसतात, अगदी मध्यभागी काही अगदी सैल नमुन्यांसह.

214
00:14:54,200 --> 00:14:58,944
असे दिसते की संभाव्य वजन आणि पूर्वाग्रहांच्या अतुलनीय मोठ्या 13,000 मितीय जागेत,

215
00:14:58,944 --> 00:15:02,635
आमच्या नेटवर्कला स्वतःला एक आनंदी थोडेसे स्थानिक आढळले आहे की,

216
00:15:02,635 --> 00:15:05,446
बहुतेक प्रतिमांचे यशस्वीरित्या वर्गीकरण करूनही,

217
00:15:05,446 --> 00:15:09,840
आम्ही ज्या नमुन्यांची अपेक्षा केली होती त्या नमुन्यांवर अचूकपणे उचलत नाही.

218
00:15:09,840 --> 00:15:12,445
आणि हा बिंदू खरोखर घरी आणण्यासाठी, तुम्ही यादृच्छिक

219
00:15:12,445 --> 00:15:14,600
प्रतिमा इनपुट करता तेव्हा काय होते ते पहा.

220
00:15:14,600 --> 00:15:19,047
जर सिस्टीम स्मार्ट असेल, तर तुम्हाला ती एकतर अनिश्चित वाटेल अशी अपेक्षा असू शकते,

221
00:15:19,047 --> 00:15:23,061
कदाचित त्या 10 आउटपुट न्यूरॉन्सपैकी कोणतेही सक्रिय केले जात नाही किंवा ते

222
00:15:23,061 --> 00:15:26,966
सर्व समान रीतीने सक्रिय केले जात नाही, परंतु त्याऐवजी ते आत्मविश्वासाने

223
00:15:26,966 --> 00:15:31,142
तुम्हाला काही निरर्थक उत्तर देते, जसे की हे यादृच्छिक असल्याची खात्री वाटते.

224
00:15:31,142 --> 00:15:34,560
नॉइज हा 5 आहे कारण ते असे करते की 5 ची वास्तविक प्रतिमा 5 आहे.

225
00:15:34,560 --> 00:15:38,324
वेगळ्या पद्धतीने शब्दबद्ध केले, जरी हे नेटवर्क अंक अगदी चांगल्या

226
00:15:38,324 --> 00:15:41,800
प्रकारे ओळखू शकत असले तरी, ते कसे काढायचे याची कल्पना नाही.

227
00:15:41,800 --> 00:15:45,400
यापैकी बरेच काही कारण हे इतके घट्ट विवशित प्रशिक्षण सेटअप आहे.

228
00:15:45,400 --> 00:15:48,220
म्हणजे, इथे स्वतःला नेटवर्कच्या शूजमध्ये ठेवा.

229
00:15:48,220 --> 00:15:52,830
त्याच्या दृष्टीकोनातून, संपूर्ण विश्वामध्ये एका लहान ग्रिडमध्ये केंद्रस्थानी असलेल्या

230
00:15:52,830 --> 00:15:55,350
स्पष्टपणे परिभाषित अचल अंकांशिवाय काहीही नाही,

231
00:15:55,350 --> 00:16:00,015
आणि त्याच्या किमतीच्या कार्यामुळे त्याला त्याच्या निर्णयांवर पूर्ण विश्वास असल्याशिवाय

232
00:16:00,015 --> 00:16:02,160
काहीही होण्यासाठी प्रोत्साहन दिले नाही.

233
00:16:02,160 --> 00:16:05,684
त्यामुळे हे दुसरे लेयर न्यूरॉन्स खरोखर काय करत आहेत याची प्रतिमा म्हणून,

234
00:16:05,684 --> 00:16:09,692
तुम्हाला कदाचित आश्चर्य वाटेल की मी हे नेटवर्क कडा आणि नमुने उचलण्याच्या प्रेरणेने

235
00:16:09,692 --> 00:16:10,320
का सादर करू.

236
00:16:10,320 --> 00:16:13,040
मला म्हणायचे आहे की, ते असेच करत नाही.

237
00:16:13,040 --> 00:16:17,480
बरं, हे आमचे अंतिम उद्दिष्ट नसून त्याऐवजी प्रारंभ बिंदू आहे.

238
00:16:17,480 --> 00:16:22,873
खरे सांगायचे तर, हे जुने तंत्रज्ञान आहे, ज्याचे 80 आणि 90 च्या दशकात संशोधन केले

239
00:16:22,873 --> 00:16:28,066
गेले होते, आणि अधिक तपशीलवार आधुनिक रूपे समजून घेण्यापूर्वी तुम्हाला ते समजून

240
00:16:28,066 --> 00:16:33,127
घेणे आवश्यक आहे, आणि हे स्पष्टपणे काही मनोरंजक समस्या सोडविण्यास सक्षम आहे,

241
00:16:33,127 --> 00:16:38,720
परंतु तुम्ही जितके अधिक जाणून घ्याल ते लपलेले स्तर खरोखर करत आहेत, कमी हुशार दिसते.

242
00:16:38,720 --> 00:16:42,822
तुम्ही कसे शिकता याकडे नेटवर्क्स कसे शिकतात यावरून क्षणभर फोकस हलवणे,

243
00:16:42,822 --> 00:16:47,160
हे केवळ तेव्हाच होईल जेव्हा तुम्ही इथल्या सामग्रीमध्ये सक्रियपणे गुंतलात.

244
00:16:47,160 --> 00:16:52,046
एक अतिशय सोपी गोष्ट मला हवी आहे की तुम्ही आत्ताच थांबा आणि तुम्ही या प्रणालीमध्ये

245
00:16:52,046 --> 00:16:56,993
कोणते बदल करू शकता आणि किनारी आणि नमुने यांसारख्या गोष्टींना अधिक चांगल्या प्रकारे

246
00:16:56,993 --> 00:17:01,880
उचलण्याची तुमची इच्छा असेल तर ती प्रतिमा कशी पाहते याबद्दल क्षणभर सखोल विचार करा.

247
00:17:01,880 --> 00:17:05,216
परंतु त्याहूनही चांगले, सामग्रीशी प्रत्यक्षात गुंतण्यासाठी,

248
00:17:05,216 --> 00:17:09,720
मी सखोल शिक्षण आणि न्यूरल नेटवर्कवर मायकेल निल्सन यांच्या पुस्तकाची शिफारस करतो.

249
00:17:09,720 --> 00:17:14,662
त्यामध्ये, तुम्हाला या अचूक उदाहरणासाठी डाउनलोड आणि प्ले करण्यासाठी कोड आणि डेटा

250
00:17:14,662 --> 00:17:19,360
मिळू शकेल आणि तो कोड काय करत आहे हे पुस्तक तुम्हाला टप्प्याटप्प्याने सांगेल.

251
00:17:19,360 --> 00:17:22,718
आश्चर्यकारक गोष्ट म्हणजे हे पुस्तक विनामूल्य आणि सार्वजनिकरित्या उपलब्ध आहे,

252
00:17:22,718 --> 00:17:25,771
त्यामुळे जर तुम्हाला त्यातून काही मिळाले तर, निल्सनच्या प्रयत्नांसाठी

253
00:17:25,771 --> 00:17:28,040
देणगी देण्यासाठी माझ्याशी सामील होण्याचा विचार करा.

254
00:17:28,040 --> 00:17:33,056
ख्रिस ओला ची अभूतपूर्व आणि सुंदर ब्लॉग पोस्ट आणि डिस्टिल मधील

255
00:17:33,056 --> 00:17:38,720
लेखांसह मी वर्णनात मला खूप आवडणारी काही इतर संसाधने देखील जोडली आहेत.

256
00:17:38,720 --> 00:17:41,191
शेवटच्या काही मिनिटांसाठी येथे गोष्टी बंद करण्यासाठी,

257
00:17:41,191 --> 00:17:44,440
मी लीशा लीसोबत घेतलेल्या मुलाखतीच्या एका स्निपेटमध्ये परत येऊ इच्छितो.

258
00:17:44,440 --> 00:17:46,523
तुम्हाला कदाचित शेवटच्या व्हिडिओवरून आठवत असेल,

259
00:17:46,523 --> 00:17:48,520
तिने तिचे पीएचडीचे काम डीप लर्निंगमध्ये केले.

260
00:17:48,520 --> 00:17:52,450
या छोट्या झलकामध्ये, ती अलीकडील दोन पेपर्सबद्दल बोलते जे खरोखरच काही

261
00:17:52,450 --> 00:17:56,380
अधिक आधुनिक प्रतिमा ओळख नेटवर्क प्रत्यक्षात कसे शिकत आहेत हे शोधतात.

262
00:17:56,380 --> 00:18:00,342
आम्ही संभाषणात कुठे होतो ते सेट करण्यासाठी, पहिल्या पेपरने यापैकी एक विशेषत:

263
00:18:00,342 --> 00:18:04,819
खोल न्यूरल नेटवर्क घेतले जे इमेज रेकग्निशनमध्ये खरोखर चांगले आहे आणि त्यास योग्यरित्या

264
00:18:04,819 --> 00:18:09,400
लेबल केलेल्या डेटासेटवर प्रशिक्षण देण्याऐवजी, प्रशिक्षणापूर्वी आसपासची सर्व लेबले बदलली.

265
00:18:09,400 --> 00:18:12,706
साहजिकच येथे चाचणी अचूकता यादृच्छिक पेक्षा चांगली असणार नाही,

266
00:18:12,706 --> 00:18:15,320
कारण सर्वकाही फक्त यादृच्छिकपणे लेबल केलेले आहे.

267
00:18:15,320 --> 00:18:21,372
पण तरीही तुम्ही योग्यरित्या लेबल केलेल्या डेटासेटवर तीच प्रशिक्षण अचूकता प्राप्त करू शकले.

268
00:18:21,372 --> 00:18:21,440


269
00:18:21,440 --> 00:18:26,407
मुळात, या विशिष्ट नेटवर्कसाठी लाखो वजने केवळ यादृच्छिक डेटा लक्षात ठेवण्यासाठी

270
00:18:26,407 --> 00:18:31,438
पुरेसे होते, ज्यामुळे हा प्रश्न निर्माण होतो की हे खर्चाचे कार्य कमी करणे खरोखर

271
00:18:31,438 --> 00:18:36,720
प्रतिमेतील कोणत्याही प्रकारच्या संरचनेशी सुसंगत आहे किंवा ते फक्त लक्षात ठेवणे आहे?

272
00:18:36,720 --> 00:18:40,120
. . . योग्य वर्गीकरण काय आहे याचा संपूर्ण डेटासेट लक्षात ठेवण्यासाठी.

273
00:18:40,120 --> 00:18:44,381
आणि म्हणून काही, तुम्हाला माहिती आहे की, या वर्षी ICML मध्ये अर्ध्या वर्षांनंतर,

274
00:18:44,381 --> 00:18:48,853
तंतोतंत खंडन करणारा पेपर नव्हता, परंतु पेपर होता ज्याने काही पैलूंवर लक्ष दिले होते,

275
00:18:48,853 --> 00:18:52,220
अहो, खरं तर ही नेटवर्क्स त्यापेक्षा थोडे अधिक स्मार्ट करत आहेत.

276
00:18:52,220 --> 00:18:56,583
जर तुम्ही अचूकता वक्र पाहिल्यास, जर तुम्ही फक्त एका यादृच्छिक

277
00:18:56,583 --> 00:19:01,298
डेटासेटवर प्रशिक्षण घेत असाल, तर तो वक्र प्रकार खूपच कमी झाला आहे,

278
00:19:01,298 --> 00:19:05,240
तुम्हाला माहिती आहे, जवळजवळ एक रेखीय फॅशनमध्ये खूप हळू.

279
00:19:05,240 --> 00:19:08,803
त्यामुळे तुम्‍हाला त्‍याची अचूकता मिळवून देण्‍यासाठी, तुम्‍हाला माहीत आहे,

280
00:19:08,803 --> 00:19:12,320
तुम्‍हाला शक्य असलेल्‍या स्‍थानिक मिनिमा शोधण्‍यासाठी खरोखर धडपड होत आहे.

281
00:19:12,320 --> 00:19:15,381
जर तुम्ही खरोखर एखाद्या संरचित डेटासेटवर प्रशिक्षण घेत असाल,

282
00:19:15,381 --> 00:19:19,797
ज्यामध्ये योग्य लेबले असतील, तर तुम्हाला माहिती आहे की, सुरुवातीला तुम्ही थोडेसे फिरता,

283
00:19:19,797 --> 00:19:23,360
परंतु नंतर त्या अचूकतेच्या पातळीवर जाण्यासाठी तुम्ही खूप वेगाने घसरले.

284
00:19:23,360 --> 00:19:28,580
आणि म्हणून काही अर्थाने स्थानिक मॅक्सिमा शोधणे सोपे होते.

285
00:19:28,580 --> 00:19:34,279
आणि म्हणूनच त्याबद्दल मनोरंजक गोष्ट म्हणजे काही वर्षांपूर्वीचा आणखी एक

286
00:19:34,279 --> 00:19:40,140
पेपर प्रकाशात आणतो, ज्यामध्ये नेटवर्क स्तरांबद्दल बरेच अधिक सरलीकरण आहे.

287
00:19:40,140 --> 00:19:44,534
परंतु परिणामांपैकी एक असे सांगत होता की, जर तुम्ही ऑप्टिमायझेशन लँडस्केप पाहिल्यास,

288
00:19:44,534 --> 00:19:49,138
या नेटवर्क्सकडून शिकण्याची प्रवृत्ती असलेली स्थानिक मिनिमा प्रत्यक्षात समान दर्जाची कशी

289
00:19:49,138 --> 00:19:49,400
आहे.

290
00:19:49,400 --> 00:19:52,046
त्यामुळे काही अर्थाने, जर तुमचा डेटा संच संरचित असेल,

291
00:19:52,046 --> 00:19:54,300
तर तुम्ही ते अधिक सहजपणे शोधण्यास सक्षम असाल.

292
00:19:54,300 --> 00:20:01,140
तुमच्यापैकी ज्यांनी Patreon ला पाठिंबा दिला त्यांचे मी नेहमीप्रमाणे आभारी आहे.

293
00:20:01,140 --> 00:20:04,121
Patreon वर गेम चेंजर काय आहे हे मी आधी सांगितले आहे,

294
00:20:04,121 --> 00:20:07,160
परंतु हे व्हिडिओ खरोखर तुमच्याशिवाय शक्य होणार नाहीत.

295
00:20:07,160 --> 00:20:10,175
मी VC फर्म Amplify Partners चे आणि मालिकेतील या सुरुवातीच्या

296
00:20:10,175 --> 00:20:13,240
व्हिडिओंना त्यांच्या समर्थनाचे देखील विशेष आभार मानू इच्छितो.

297
00:20:13,240 --> 00:20:33,140
धन्यवाद.

