1
00:00:04,180 --> 00:00:07,280
शेवटचा व्हिडिओ मी न्यूरल नेटवर्कची रचना मांडली.

2
00:00:07,680 --> 00:00:10,097
मी येथे एक द्रुत रीकॅप देईन जेणेकरून ते आपल्या मनात ताजे 

3
00:00:10,097 --> 00:00:12,600
असेल आणि नंतर या व्हिडिओसाठी माझी दोन मुख्य उद्दिष्टे आहेत.

4
00:00:13,100 --> 00:00:15,473
प्रथम म्हणजे ग्रेडियंट डिसेंटची कल्पना सादर करणे, 

5
00:00:15,473 --> 00:00:19,270
ज्यामध्ये केवळ न्यूरल नेटवर्क कसे शिकतात असे नाही तर इतर मशीन लर्निंग कसे कार्य 

6
00:00:19,270 --> 00:00:20,600
करते हे देखील अधोरेखित करते.

7
00:00:21,120 --> 00:00:24,157
त्यानंतर आम्ही हे विशिष्ट नेटवर्क कसे कार्य करते आणि 

8
00:00:24,157 --> 00:00:27,940
न्यूरॉन्सचे ते लपलेले स्तर काय शोधतात याबद्दल थोडे अधिक जाणून घेऊ.

9
00:00:28,979 --> 00:00:34,201
एक स्मरणपत्र म्हणून, आमचे लक्ष्य हस्तलिखित अंक ओळखीचे उत्कृष्ट उदाहरण आहे, 

10
00:00:34,201 --> 00:00:36,220
न्यूरल नेटवर्कचे हॅलो वर्ल्ड.

11
00:00:37,020 --> 00:00:39,805
हे अंक 28x28 पिक्सेल ग्रिडवर रेंडर केले जातात, 

12
00:00:39,805 --> 00:00:43,420
प्रत्येक पिक्सेलचे काही ग्रेस्केल मूल्य 0 आणि 1 दरम्यान असते.

13
00:00:43,820 --> 00:00:50,040
ते नेटवर्कच्या इनपुट लेयरमध्ये 784 न्यूरॉन्सची सक्रियता निर्धारित करतात.

14
00:00:51,180 --> 00:00:55,727
आणि नंतर खालील स्तरांमधील प्रत्येक न्यूरॉनचे सक्रियकरण मागील लेयरमधील सर्व 

15
00:00:55,727 --> 00:01:00,820
सक्रियतेच्या भारित बेरजेवर, तसेच काही विशेष संख्येवर आधारित आहे ज्याला बायस म्हणतात.

16
00:01:02,160 --> 00:01:06,187
मग तुम्ही ती बेरीज इतर काही फंक्शनसह तयार करा, जसे की सिग्मॉइड स्क्विशिफिकेशन, 

17
00:01:06,187 --> 00:01:08,940
किंवा रेलू, मी शेवटचा व्हिडिओ ज्या प्रकारे चालला होता.

18
00:01:09,480 --> 00:01:15,372
एकूण, प्रत्येकी 16 न्यूरॉन्ससह दोन लपविलेल्या स्तरांची काहीशी अनियंत्रित निवड दिल्यास, 

19
00:01:15,372 --> 00:01:20,248
नेटवर्कमध्ये सुमारे 13,000 वजने आणि पूर्वाग्रह आहेत जे आपण समायोजित करू 

20
00:01:20,248 --> 00:01:24,380
शकतो आणि ही मूल्ये नेटवर्क नेमके काय करते हे निर्धारित करतात.

21
00:01:24,880 --> 00:01:28,921
मग जेव्हा आपण म्हणतो की हे नेटवर्क दिलेल्या अंकाचे वर्गीकरण करते तेव्हा 

22
00:01:28,921 --> 00:01:33,300
अंतिम स्तरातील त्या 10 न्यूरॉन्सपैकी सर्वात तेजस्वी त्या अंकाशी संबंधित असतात.

23
00:01:34,100 --> 00:01:39,190
आणि लक्षात ठेवा, स्तरित संरचनेसाठी आमच्या मनात असलेली प्रेरणा ही होती की कदाचित 

24
00:01:39,190 --> 00:01:44,090
दुसरा स्तर कडांवर उठू शकेल आणि तिसरा स्तर लूप आणि रेषा यांसारख्या नमुन्यांवर 

25
00:01:44,090 --> 00:01:48,800
उचलू शकेल आणि शेवटचा लेयर फक्त त्या एकत्र करू शकेल. अंक ओळखण्यासाठी नमुने.

26
00:01:49,800 --> 00:01:52,240
तर इथे, नेटवर्क कसे शिकते ते आपण शिकतो.

27
00:01:52,640 --> 00:01:56,828
आम्हाला काय हवे आहे ते एक अल्गोरिदम आहे जिथे तुम्ही या नेटवर्कला प्रशिक्षण 

28
00:01:56,828 --> 00:02:01,016
डेटाचा संपूर्ण समूह दर्शवू शकता, जे हस्तलिखित अंकांच्या विविध प्रतिमांच्या 

29
00:02:01,016 --> 00:02:04,311
गुच्छाच्या स्वरूपात येते आणि ते काय असावे याच्या लेबलांसह, 

30
00:02:04,311 --> 00:02:08,500
आणि ते ते 13,000 वजन आणि पूर्वाग्रह समायोजित करा जेणेकरून प्रशिक्षण डेटावर 

31
00:02:08,500 --> 00:02:10,120
त्याचे कार्यप्रदर्शन सुधारेल.

32
00:02:10,720 --> 00:02:13,790
आशेने, या स्तरित संरचनेचा अर्थ असा होईल की ते जे शिकते ते त्या 

33
00:02:13,790 --> 00:02:16,860
प्रशिक्षण डेटाच्या पलीकडे असलेल्या प्रतिमांना सामान्यीकृत करते.

34
00:02:17,640 --> 00:02:20,923
आम्ही ज्या प्रकारे चाचणी करतो ती अशी आहे की तुम्ही नेटवर्कला प्रशिक्षण दिल्यानंतर, 

35
00:02:20,923 --> 00:02:23,851
तुम्ही त्यास अधिक लेबल केलेला डेटा दाखवता जो तो यापूर्वी कधीही न पाहिलेला 

36
00:02:23,851 --> 00:02:26,700
आहे आणि ते त्या नवीन प्रतिमांचे किती अचूक वर्गीकरण करते ते तुम्ही पाहता.

37
00:02:31,120 --> 00:02:34,649
सुदैवाने आमच्यासाठी, आणि हे एक सामान्य उदाहरण ज्याने सुरू केले आहे, 

38
00:02:34,649 --> 00:02:39,061
ते म्हणजे MNIST डेटाबेसच्या मागे असलेल्या चांगल्या लोकांनी हजारो हस्तलिखीत अंकांच्या 

39
00:02:39,061 --> 00:02:43,317
प्रतिमांचा संग्रह एकत्र ठेवला आहे, प्रत्येकाला ते अपेक्षित असलेल्या संख्येसह लेबल 

40
00:02:43,317 --> 00:02:44,200
केलेले आहेत. असणे

41
00:02:44,900 --> 00:02:48,318
आणि एखाद्या मशीनचे शिक्षण म्हणून वर्णन करणे जितके उत्तेजक आहे, 

42
00:02:48,318 --> 00:02:50,868
ते कसे कार्य करते हे एकदा तुम्ही पाहिल्यानंतर, 

43
00:02:50,868 --> 00:02:55,480
ते काही वेड्या साय-फाय प्रिमिससारखे कमी आणि कॅल्क्युलस व्यायामासारखे बरेच काही वाटते.

44
00:02:56,200 --> 00:02:59,960
मला असे म्हणायचे आहे की, मुळात हे विशिष्ट फंक्शनचे किमान शोधण्यासाठी खाली येते.

45
00:03:01,939 --> 00:03:06,152
लक्षात ठेवा, संकल्पनात्मकदृष्ट्या, आम्ही प्रत्येक न्यूरॉनचा मागील लेयरमधील 

46
00:03:06,152 --> 00:03:10,534
सर्व न्यूरॉन्सशी जोडलेला आहे असा विचार करत आहोत आणि त्याचे सक्रियकरण परिभाषित 

47
00:03:10,534 --> 00:03:14,634
करणाऱ्या भारित बेरीजमधील वजन हे त्या कनेक्शनच्या सामर्थ्यासारखे आहेत आणि 

48
00:03:14,634 --> 00:03:18,960
पूर्वाग्रह हे काही संकेत आहेत. तो न्यूरॉन सक्रिय किंवा निष्क्रिय आहे की नाही.

49
00:03:19,720 --> 00:03:22,015
आणि गोष्टी सुरू करण्यासाठी, आम्ही फक्त ते सर्व वजन 

50
00:03:22,015 --> 00:03:24,400
आणि पूर्वाग्रह पूर्णपणे यादृच्छिकपणे सुरू करणार आहोत.

51
00:03:24,940 --> 00:03:27,721
हे सांगण्याची गरज नाही, हे नेटवर्क दिलेल्या प्रशिक्षण उदाहरणावर 

52
00:03:27,721 --> 00:03:30,720
खूपच भयानक कामगिरी करणार आहे, कारण ते फक्त काहीतरी यादृच्छिक करत आहे.

53
00:03:31,040 --> 00:03:36,020
उदाहरणार्थ, तुम्ही 3 च्या या प्रतिमेमध्ये फीड करा आणि आउटपुट लेयर फक्त गोंधळासारखे दिसते.

54
00:03:36,600 --> 00:03:41,218
तर तुम्ही जे करता ते म्हणजे कॉस्ट फंक्शन, कॉम्प्युटरला सांगण्याचा एक मार्ग, 

55
00:03:41,218 --> 00:03:45,533
नाही, खराब कॉम्प्युटर, त्या आउटपुटमध्ये सक्रियता असली पाहिजे जी बहुतेक 

56
00:03:45,533 --> 00:03:50,760
न्यूरॉन्ससाठी 0 असते, परंतु या न्यूरॉनसाठी 1, तुम्ही मला जे दिले ते पूर्णपणे कचरा आहे.

57
00:03:51,720 --> 00:03:55,942
थोडे अधिक गणिती म्हणायचे असेल तर, तुम्ही त्या प्रत्येक कचरा 

58
00:03:55,942 --> 00:04:00,375
आउटपुट ॲक्टिव्हेशनमधील फरकांचे वर्ग जोडता आणि तुम्हाला त्यांचे 

59
00:04:00,375 --> 00:04:05,020
मूल्य हवे आहे आणि यालाच आम्ही एका प्रशिक्षण उदाहरणाची किंमत म्हणू.

60
00:04:05,960 --> 00:04:09,211
लक्षात घ्या जेव्हा नेटवर्क आत्मविश्वासाने प्रतिमेचे अचूक 

61
00:04:09,211 --> 00:04:12,862
वर्गीकरण करते तेव्हा ही बेरीज लहान असते, परंतु जेव्हा नेटवर्कला 

62
00:04:12,862 --> 00:04:16,399
ते काय करत आहे हे माहित नसल्यासारखे दिसते तेव्हा ती मोठी असते.

63
00:04:18,640 --> 00:04:22,004
तर मग तुम्ही काय करता ते तुमच्या सर्व दहा हजार 

64
00:04:22,004 --> 00:04:25,440
प्रशिक्षण उदाहरणांच्या सरासरी खर्चाचा विचार करा.

65
00:04:27,040 --> 00:04:32,740
नेटवर्क किती खराब आहे आणि संगणकाला किती वाईट वाटले पाहिजे यासाठी ही सरासरी किंमत आहे.

66
00:04:33,420 --> 00:04:34,600
आणि ही एक गुंतागुंतीची गोष्ट आहे.

67
00:04:35,040 --> 00:04:39,984
लक्षात ठेवा की नेटवर्क स्वतःच एक फंक्शन कसे होते, जे इनपुट म्हणून 784 संख्या घेते, 

68
00:04:39,984 --> 00:04:44,630
पिक्सेल व्हॅल्यूज घेते आणि त्याचे आउटपुट म्हणून 10 संख्या बाहेर टाकते आणि एका 

69
00:04:44,630 --> 00:04:48,800
अर्थाने ते या सर्व वजन आणि पूर्वाग्रहांद्वारे पॅरामीटराइज्ड केले जाते?

70
00:04:49,500 --> 00:04:52,820
बरं, खर्चाचे कार्य हे त्यावरील गुंतागुंतीचा एक थर आहे.

71
00:04:53,100 --> 00:04:58,281
हे 13,000 किंवा त्यापेक्षा जास्त वजन आणि बायसेस इनपुट म्हणून घेते आणि ते वजन आणि 

72
00:04:58,281 --> 00:05:03,590
बायसेस किती वाईट आहेत याचे वर्णन करणारी एकच संख्या बाहेर टाकते आणि ते कसे परिभाषित 

73
00:05:03,590 --> 00:05:08,900
केले जाते ते प्रशिक्षण डेटाच्या हजारो तुकड्यांवर नेटवर्कच्या वर्तनावर अवलंबून असते.

74
00:05:09,520 --> 00:05:11,000
खूप विचार करण्यासारखे आहे.

75
00:05:12,400 --> 00:05:15,820
पण कॉम्प्युटरला फक्त सांगणे हे काय वाईट काम आहे ते फारसे उपयुक्त नाही.

76
00:05:16,220 --> 00:05:20,060
ते वजन आणि पूर्वाग्रह कसे बदलायचे ते तुम्हाला सांगायचे आहे जेणेकरून ते चांगले होईल.

77
00:05:20,780 --> 00:05:25,126
13,000 इनपुटसह फंक्शनची कल्पना करण्यासाठी संघर्ष करण्यापेक्षा ते सोपे करण्यासाठी, 

78
00:05:25,126 --> 00:05:28,412
फक्त एका साध्या फंक्शनची कल्पना करा ज्यामध्ये एक संख्या इनपुट 

79
00:05:28,412 --> 00:05:30,480
म्हणून आणि एक संख्या आउटपुट म्हणून आहे.

80
00:05:31,480 --> 00:05:35,300
या फंक्शनचे मूल्य कमी करणारे इनपुट कसे शोधायचे?

81
00:05:36,460 --> 00:05:41,446
कॅल्क्युलसच्या विद्यार्थ्यांना हे कळेल की तुम्ही काहीवेळा ते किमान स्पष्टपणे काढू शकता, 

82
00:05:41,446 --> 00:05:44,506
परंतु ते खरोखर क्लिष्ट फंक्शन्ससाठी नेहमीच शक्य नसते, 

83
00:05:44,506 --> 00:05:49,323
आमच्या वेड्या गुंतागुंतीच्या न्यूरल नेटवर्क कॉस्ट फंक्शनसाठी या परिस्थितीच्या 13,000 

84
00:05:49,323 --> 00:05:51,080
इनपुट आवृत्तीमध्ये नक्कीच नाही.

85
00:05:51,580 --> 00:05:55,333
अधिक लवचिक युक्ती म्हणजे कोणत्याही इनपुटपासून प्रारंभ करणे आणि ते 

86
00:05:55,333 --> 00:05:59,200
आउटपुट कमी करण्यासाठी आपण कोणत्या दिशेने पाऊल टाकले पाहिजे हे शोधणे.

87
00:06:00,080 --> 00:06:04,543
विशेषत:, तुम्ही जेथे आहात त्या फंक्शनचा उतार तुम्ही काढू शकत असल्यास, 

88
00:06:04,543 --> 00:06:09,900
तो उतार सकारात्मक असल्यास डावीकडे सरकवा आणि उतार ऋणात्मक असल्यास इनपुट उजवीकडे वळवा.

89
00:06:11,960 --> 00:06:15,820
तुम्ही हे वारंवार करत असल्यास, प्रत्येक बिंदूवर नवीन उतार तपासत आहात आणि 

90
00:06:15,820 --> 00:06:19,840
योग्य पाऊल उचलत आहात, तुम्ही काही स्थानिक किमान फंक्शनशी संपर्क साधणार आहात.

91
00:06:20,640 --> 00:06:23,800
येथे तुमच्या मनात असलेली प्रतिमा टेकडीवरून खाली लोटणारा चेंडू आहे.

92
00:06:24,620 --> 00:06:27,683
लक्षात घ्या, या खरोखरच सरलीकृत सिंगल इनपुट फंक्शनसाठीही, 

93
00:06:27,683 --> 00:06:31,069
तुम्ही कोणत्या यादृच्छिक इनपुटपासून सुरुवात करता यावर अवलंबून, 

94
00:06:31,069 --> 00:06:33,702
अनेक संभाव्य व्हॅली आहेत ज्यात तुम्ही उतरू शकता, 

95
00:06:33,702 --> 00:06:37,303
आणि तुम्ही ज्या स्थानिक पातळीवर उतरता ते सर्वात लहान संभाव्य मूल्य 

96
00:06:37,303 --> 00:06:39,400
असेल याची कोणतीही हमी नाही. खर्च कार्य.

97
00:06:40,220 --> 00:06:42,620
ते आमच्या न्यूरल नेटवर्क केसमध्ये देखील नेले जाईल.

98
00:06:43,180 --> 00:06:46,890
मी तुम्हाला हे देखील लक्षात घ्यायचे आहे की जर तुम्ही तुमच्या पायऱ्यांचा आकार 

99
00:06:46,890 --> 00:06:50,456
उताराच्या प्रमाणात कसा बनवलात, तर जेव्हा उतार किमान दिशेने सपाट होत असेल, 

100
00:06:50,456 --> 00:06:54,600
तेव्हा तुमच्या पायऱ्या लहान होत जातात आणि त्यामुळे तुम्हाला ओव्हरशूटिंगपासून मदत होते.

101
00:06:55,940 --> 00:07:00,980
जटिलता थोडीशी वाढवून, त्याऐवजी दोन इनपुट आणि एक आउटपुट असलेल्या फंक्शनची कल्पना करा.

102
00:07:01,500 --> 00:07:04,598
तुम्ही इनपुट स्पेसचा xy-प्लेन म्हणून विचार करू शकता आणि 

103
00:07:04,598 --> 00:07:08,140
खर्चाचे कार्य त्याच्या वरच्या पृष्ठभागाच्या रूपात आलेख केले आहे.

104
00:07:08,760 --> 00:07:13,895
फंक्शनच्या स्लोपबद्दल विचारण्याऐवजी, फंक्शनचे आउटपुट लवकर कमी करण्यासाठी 

105
00:07:13,895 --> 00:07:18,960
या इनपुट स्पेसमध्ये तुम्ही कोणत्या दिशेने पाऊल टाकावे हे विचारावे लागेल.

106
00:07:19,720 --> 00:07:21,760
दुसऱ्या शब्दांत, उताराची दिशा काय आहे?

107
00:07:22,380 --> 00:07:25,560
पुन्हा, त्या टेकडीवरून बॉल फिरवण्याचा विचार करणे उपयुक्त आहे.

108
00:07:26,660 --> 00:07:30,817
तुमच्यापैकी जे मल्टीव्हेरिएबल कॅल्क्युलसशी परिचित आहेत त्यांना हे कळेल 

109
00:07:30,817 --> 00:07:34,447
की फंक्शनचा ग्रेडियंट तुम्हाला सर्वात जास्त चढाईची दिशा देतो, 

110
00:07:34,447 --> 00:07:38,780
फंक्शन सर्वात वेगाने वाढवण्यासाठी तुम्ही कोणत्या दिशेने पाऊल टाकले पाहिजे.

111
00:07:39,560 --> 00:07:42,913
साहजिकच पुरेसे आहे, त्या ग्रेडियंटचे ऋण घेतल्याने तुम्हाला 

112
00:07:42,913 --> 00:07:46,040
पायरीची दिशा मिळते ज्यामुळे कार्य सर्वात लवकर कमी होते.

113
00:07:47,240 --> 00:07:53,840
त्याहूनही अधिक, या ग्रेडियंट वेक्टरची लांबी ही सर्वात उंच उतार किती तीव्र आहे हे दर्शवते.

114
00:07:54,540 --> 00:07:58,200
जर तुम्ही मल्टीव्हेरिएबल कॅल्क्युलसशी अपरिचित असाल आणि तुम्हाला अधिक जाणून घ्यायचे असेल, 

115
00:07:58,200 --> 00:08:00,340
तर मी खान अकादमीसाठी या विषयावर केलेले काही काम पहा.

116
00:08:00,860 --> 00:08:04,433
खरे सांगायचे तर, तुमच्यासाठी आणि माझ्यासाठी सध्या महत्त्वाची गोष्ट 

117
00:08:04,433 --> 00:08:08,006
म्हणजे या वेक्टरची गणना करण्याचा एक मार्ग तत्त्वतः अस्तित्वात आहे, 

118
00:08:08,006 --> 00:08:11,900
हा वेक्टर जो तुम्हाला उताराची दिशा काय आहे आणि ती किती उंच आहे हे सांगते.

119
00:08:12,400 --> 00:08:16,120
तुम्हाला एवढंच माहीत असेल आणि तुम्ही तपशिलांवर ठोस नसाल तर तुम्ही ठीक असाल.

120
00:08:17,200 --> 00:08:21,668
जर तुम्ही ते मिळवू शकत असाल तर, फंक्शन कमी करण्यासाठी अल्गोरिदम म्हणजे या 

121
00:08:21,668 --> 00:08:26,740
ग्रेडियंट दिशेची गणना करणे, नंतर उतारावर एक लहान पाऊल घ्या आणि ते पुन्हा पुन्हा करा.

122
00:08:27,700 --> 00:08:32,820
2 इनपुट ऐवजी 13,000 इनपुट्स असलेल्या फंक्शनची हीच मूळ कल्पना आहे.

123
00:08:33,400 --> 00:08:36,369
आमच्या नेटवर्कचे सर्व 13,000 वजन आणि पक्षपात एका 

124
00:08:36,369 --> 00:08:39,460
विशाल स्तंभ वेक्टरमध्ये आयोजित करण्याची कल्पना करा.

125
00:08:40,140 --> 00:08:44,290
कॉस्ट फंक्शनचा नकारात्मक ग्रेडियंट हा फक्त एक वेक्टर आहे, 

126
00:08:44,290 --> 00:08:48,941
या प्रचंड मोठ्या इनपुट स्पेसच्या आत ही काही दिशा आहे जी तुम्हाला 

127
00:08:48,941 --> 00:08:54,880
सांगते की या सर्व आकड्यांकडे कोणते नज केल्याने खर्च फंक्शनमध्ये सर्वात जलद घट होईल.

128
00:08:55,640 --> 00:08:58,988
आणि अर्थातच, आमच्या खास डिझाइन केलेल्या किमतीच्या कार्यासह, 

129
00:08:58,988 --> 00:09:03,006
वजन आणि पूर्वाग्रह बदलून ते कमी करणे म्हणजे प्रशिक्षण डेटाच्या प्रत्येक 

130
00:09:03,006 --> 00:09:06,634
तुकड्यावर नेटवर्कचे आउटपुट 10 मूल्यांच्या यादृच्छिक ॲरेसारखे कमी 

131
00:09:06,634 --> 00:09:10,820
दिसणे आणि आम्हाला हवे असलेल्या वास्तविक निर्णयासारखे दिसते. ते बनवण्यासाठी.

132
00:09:11,440 --> 00:09:14,605
हे लक्षात ठेवणे महत्त्वाचे आहे की, या खर्चाच्या कार्यामध्ये सर्व 

133
00:09:14,605 --> 00:09:17,917
प्रशिक्षण डेटावर सरासरीचा समावेश होतो, म्हणून जर तुम्ही ते कमी केले 

134
00:09:17,917 --> 00:09:21,180
तर याचा अर्थ त्या सर्व नमुन्यांवर ते अधिक चांगले कार्यप्रदर्शन आहे.

135
00:09:23,820 --> 00:09:27,034
या ग्रेडियंटची कार्यक्षमतेने गणना करण्यासाठी अल्गोरिदम, 

136
00:09:27,034 --> 00:09:31,856
जे प्रभावीपणे न्यूरल नेटवर्क कसे शिकते याचे हृदय आहे, त्याला बॅकप्रोपॅगेशन म्हणतात, 

137
00:09:31,856 --> 00:09:33,980
आणि मी पुढील व्हिडिओबद्दल बोलणार आहे.

138
00:09:34,660 --> 00:09:38,672
तेथे, प्रशिक्षण डेटाच्या दिलेल्या भागासाठी प्रत्येक वजन आणि पूर्वाग्रहाचे नेमके 

139
00:09:38,672 --> 00:09:41,481
काय होते ते जाणून घेण्यासाठी मला खरोखर वेळ काढायचा आहे, 

140
00:09:41,481 --> 00:09:45,695
संबंधित कॅल्क्युलस आणि सूत्रांच्या ढिगाऱ्याच्या पलीकडे काय घडत आहे याची अंतर्ज्ञानी 

141
00:09:45,695 --> 00:09:47,100
भावना देण्याचा प्रयत्न करतो.

142
00:09:47,780 --> 00:09:50,372
आत्ता इथे, आत्ता, मी तुम्हाला जाणून घ्यायची आहे, 

143
00:09:50,372 --> 00:09:53,598
अंमलबजावणीच्या तपशिलांपासून स्वतंत्र आहे, जेव्हा आपण नेटवर्क 

144
00:09:53,598 --> 00:09:58,360
लर्निंगबद्दल बोलतो तेव्हा आपल्याला काय म्हणायचे आहे ते म्हणजे केवळ खर्चाचे कार्य कमी करणे.

145
00:09:59,300 --> 00:10:03,623
आणि लक्षात घ्या, त्याचा एक परिणाम असा आहे की या किमतीच्या कार्यासाठी एक छान गुळगुळीत 

146
00:10:03,623 --> 00:10:08,100
आउटपुट असणे महत्वाचे आहे, जेणेकरून आम्ही उतारावर थोडे पाऊल टाकून स्थानिक किमान शोधू शकू.

147
00:10:09,260 --> 00:10:13,694
म्हणूनच, बायनरी पद्धतीने सक्रिय किंवा निष्क्रिय न राहता, 

148
00:10:13,694 --> 00:10:19,140
जैविक न्यूरॉन्सच्या मार्गाने कृत्रिम न्यूरॉन्समध्ये सतत सक्रियता असते.

149
00:10:20,220 --> 00:10:23,382
नकारात्मक ग्रेडियंटच्या काही गुणाकाराने फंक्शनच्या इनपुटला 

150
00:10:23,382 --> 00:10:26,760
वारंवार नडज करण्याच्या या प्रक्रियेला ग्रेडियंट डिसेंट म्हणतात.

151
00:10:27,300 --> 00:10:31,140
काही स्थानिक किमान खर्चाच्या फंक्शनकडे अभिसरण करण्याचा हा एक मार्ग आहे, 

152
00:10:31,140 --> 00:10:32,580
मुळात या आलेखामध्ये एक दरी.

153
00:10:33,440 --> 00:10:36,586
मी अजूनही दोन इनपुटसह फंक्शनचे चित्र दाखवत आहे, अर्थातच, 

154
00:10:36,586 --> 00:10:40,892
कारण 13,000 डायमेन्शनल इनपुट स्पेसमधील नज हे तुमचे मन गुंडाळणे थोडे कठीण आहे, 

155
00:10:40,892 --> 00:10:44,260
परंतु याबद्दल विचार करण्याचा एक चांगला गैर-स्थानिक मार्ग आहे.

156
00:10:45,080 --> 00:10:48,440
ऋण ग्रेडियंटचा प्रत्येक घटक आपल्याला दोन गोष्टी सांगतो.

157
00:10:49,060 --> 00:10:52,467
इनपुट व्हेक्टरचा संबंधित घटक वर किंवा खाली ढकलायचा 

158
00:10:52,467 --> 00:10:55,140
की नाही हे चिन्ह अर्थातच आम्हाला सांगते.

159
00:10:55,800 --> 00:10:59,295
परंतु महत्त्वाचे म्हणजे, या सर्व घटकांचे सापेक्ष 

160
00:10:59,295 --> 00:11:02,720
परिमाण कोणते बदल अधिक महत्त्वाचे आहेत हे सांगते.

161
00:11:05,220 --> 00:11:09,068
तुम्ही पाहता, आमच्या नेटवर्कमध्ये, वजनांपैकी एकाचे समायोजन इतर 

162
00:11:09,068 --> 00:11:13,040
वजनाच्या समायोजनापेक्षा किमतीच्या कार्यावर जास्त परिणाम करू शकते.

163
00:11:14,800 --> 00:11:18,200
यापैकी काही कनेक्शन आमच्या प्रशिक्षण डेटासाठी अधिक महत्त्वाचे आहेत.

164
00:11:19,320 --> 00:11:23,786
तर तुम्ही आमच्या मनाच्या प्रचंड खर्चाच्या कार्याच्या या ग्रेडियंट वेक्टरबद्दल विचार 

165
00:11:23,786 --> 00:11:28,305
करू शकता हा एक मार्ग आहे की ते प्रत्येक वजन आणि पूर्वाग्रहाचे सापेक्ष महत्त्व एन्कोड 

166
00:11:28,305 --> 00:11:32,400
करते, म्हणजेच, यापैकी कोणता बदल तुमच्या पैशासाठी सर्वात जास्त दणका देणार आहे.

167
00:11:33,620 --> 00:11:36,640
दिग्दर्शनाबद्दल विचार करण्याचा हा खरोखर दुसरा मार्ग आहे.

168
00:11:37,100 --> 00:11:41,977
सोप्या उदाहरणासाठी, जर तुमच्याकडे इनपुट म्हणून दोन व्हेरिएबल्ससह काही फंक्शन असेल 

169
00:11:41,977 --> 00:11:46,973
आणि तुम्ही मोजता की एखाद्या विशिष्ट बिंदूवर त्याचा ग्रेडियंट 3,1 म्हणून बाहेर येतो, 

170
00:11:46,973 --> 00:11:52,267
तर एकीकडे तुम्ही त्याचा अर्थ असा लावू शकता की जेव्हा तुम्ही' त्या इनपुटवर उभे राहून, 

171
00:11:52,267 --> 00:11:57,323
या दिशेला जाण्याने फंक्शन सर्वात लवकर वाढते, की जेव्हा तुम्ही इनपुट पॉइंट्सच्या समतल 

172
00:11:57,323 --> 00:12:02,260
भागाच्या वरच्या फंक्शनचा आलेख करता तेव्हा तो व्हेक्टर तुम्हाला सरळ चढाची दिशा देतो.

173
00:12:02,860 --> 00:12:07,710
पण हे वाचण्याचा दुसरा मार्ग म्हणजे या पहिल्या व्हेरिएबलमधील बदलांना दुसऱ्या 

174
00:12:07,710 --> 00:12:13,007
व्हेरिएबलमधील बदलांपेक्षा 3 पट महत्त्व आहे, म्हणजे किमान संबंधित इनपुटच्या शेजारी, 

175
00:12:13,007 --> 00:12:16,900
x-व्हॅल्यूला धक्का लावणे आपल्यासाठी खूप मोठा धक्का देते. बोकड

176
00:12:19,880 --> 00:12:22,340
चला झूम कमी करू आणि आपण आतापर्यंत कुठे आहोत याचा सारांश काढू.

177
00:12:22,840 --> 00:12:26,537
784 इनपुट आणि 10 आउटपुट असलेले हे नेटवर्क हे फंक्शन आहे, 

178
00:12:26,537 --> 00:12:30,040
जे या सर्व वेटेड बेरीजच्या संदर्भात परिभाषित केले आहे.

179
00:12:30,640 --> 00:12:33,680
कॉस्ट फंक्शन हा त्यावरील गुंतागुंतीचा एक थर आहे.

180
00:12:33,980 --> 00:12:37,658
हे इनपुट म्हणून 13,000 वजने आणि पूर्वाग्रह घेते 

181
00:12:37,658 --> 00:12:41,720
आणि प्रशिक्षण उदाहरणांच्या आधारे एकच माप काढून टाकते.

182
00:12:42,440 --> 00:12:46,900
आणि कॉस्ट फंक्शनचा ग्रेडियंट हा अजून एक गुंतागुंतीचा थर आहे.

183
00:12:47,360 --> 00:12:50,782
हे आम्हाला सांगते की या सर्व वजन आणि पूर्वाग्रहांना कोणते धक्कादायक 

184
00:12:50,782 --> 00:12:53,953
कारणे किंमत कार्याच्या मूल्यामध्ये सर्वात जलद बदल घडवून आणतात, 

185
00:12:53,953 --> 00:12:57,880
ज्याचा तुम्ही अर्थ सांगू शकता की कोणत्या वजनांमध्ये कोणते बदल महत्त्वाचे आहेत.

186
00:13:02,560 --> 00:13:06,122
तर, जेव्हा तुम्ही यादृच्छिक वजन आणि पूर्वाग्रहांसह नेटवर्क सुरू करता आणि 

187
00:13:06,122 --> 00:13:09,685
या ग्रेडियंट डिसेंट प्रक्रियेच्या आधारे त्यांना अनेक वेळा समायोजित करता, 

188
00:13:09,685 --> 00:13:13,200
तेव्हा ते यापूर्वी कधीही न पाहिलेल्या प्रतिमांवर किती चांगले कार्य करते?

189
00:13:14,100 --> 00:13:19,064
मी येथे वर्णन केलेले, प्रत्येकी 16 न्यूरॉन्सच्या दोन लपलेल्या स्तरांसह, 

190
00:13:19,064 --> 00:13:23,063
मुख्यतः सौंदर्याच्या कारणांसाठी निवडले गेलेले, वाईट नाही, 

191
00:13:23,063 --> 00:13:25,960
ते 96% नवीन प्रतिमा अचूकपणे वर्गीकृत करते.

192
00:13:26,680 --> 00:13:29,526
आणि प्रामाणिकपणे, जर तुम्ही त्यात काही गडबड केलेली 

193
00:13:29,526 --> 00:13:32,540
उदाहरणे पाहिली तर तुम्हाला ते थोडे ढिले करणे भाग पडते.

194
00:13:36,220 --> 00:13:38,932
आता तुम्ही हिडन लेयर स्ट्रक्चरसह खेळत असाल आणि 

195
00:13:38,932 --> 00:13:41,760
काही बदल केले तर तुम्ही हे 98% पर्यंत मिळवू शकता.

196
00:13:41,760 --> 00:13:42,720
आणि ते खूप चांगले आहे!

197
00:13:43,020 --> 00:13:47,647
हे सर्वोत्कृष्ट नाही, या प्लेन व्हॅनिला नेटवर्कपेक्षा अधिक अत्याधुनिक मिळवून तुम्ही 

198
00:13:47,647 --> 00:13:51,999
नक्कीच चांगली कामगिरी मिळवू शकता, परंतु सुरुवातीचे काम किती कठीण आहे हे पाहता, 

199
00:13:51,999 --> 00:13:56,572
मला असे वाटते की कोणत्याही नेटवर्कने याआधी कधीही न पाहिलेल्या प्रतिमांवर हे चांगले 

200
00:13:56,572 --> 00:14:01,420
करत असल्याबद्दल काहीतरी अविश्वसनीय आहे. कोणते नमुने शोधायचे हे आम्ही कधीच सांगितले नाही.

201
00:14:02,560 --> 00:14:07,206
मूलतः, मी या संरचनेला प्रेरित करण्याचा मार्ग म्हणजे आमच्याकडे असलेल्या एका 

202
00:14:07,206 --> 00:14:10,427
आशेचे वर्णन करून, की दुसरा थर लहान कडांवर उठू शकेल, 

203
00:14:10,427 --> 00:14:15,817
की तिसरा थर लूप आणि लांब रेषा ओळखण्यासाठी त्या कडा एकत्र करेल आणि ते तुकडे केले जातील. 

204
00:14:15,817 --> 00:14:17,180
अंक ओळखण्यासाठी एकत्र.

205
00:14:17,960 --> 00:14:20,400
मग आमचे नेटवर्क हेच करत आहे का?

206
00:14:21,079 --> 00:14:24,400
बरं, यासाठी किमान, अजिबात नाही.

207
00:14:24,820 --> 00:14:28,724
पहिल्या लेयरमधील सर्व न्यूरॉन्सपासून दुस-या लेयरमधील दिलेल्या न्यूरॉनच्या 

208
00:14:28,724 --> 00:14:32,733
कनेक्शनचे वजन दुसऱ्या लेयरचे न्यूरॉन उचलत असलेल्या पिक्सेल पॅटर्नच्या रूपात 

209
00:14:32,733 --> 00:14:37,060
कसे व्हिज्युअलाइज केले जाऊ शकते हे आम्ही शेवटचा व्हिडिओ कसा पाहिला ते लक्षात ठेवा?

210
00:14:37,780 --> 00:14:44,227
बरं, जेव्हा आपण या संक्रमणांशी संबंधित वजनांसाठी, पहिल्या लेयरपासून दुसऱ्या लेयरपर्यंत, 

211
00:14:44,227 --> 00:14:48,184
इकडे-तिकडे वेगळ्या छोट्या कडांवर उचलण्याऐवजी ते करतो, 

212
00:14:48,184 --> 00:14:53,680
तेव्हा ते काही अगदी सैल नमुन्यांसह, जवळजवळ यादृच्छिक दिसतात. तेथे मध्यभागी.

213
00:14:53,760 --> 00:14:58,388
असे दिसते की संभाव्य वजन आणि पूर्वाग्रहांच्या अतुलनीय मोठ्या 13,000 मितीय जागेत, 

214
00:14:58,388 --> 00:15:01,988
आमच्या नेटवर्कला स्वतःला एक आनंदी थोडेसे स्थानिक आढळले आहे की, 

215
00:15:01,988 --> 00:15:04,731
बहुतेक प्रतिमांचे यशस्वीरित्या वर्गीकरण करूनही, 

216
00:15:04,731 --> 00:15:08,960
आम्ही ज्या नमुन्यांची अपेक्षा केली होती त्या नमुन्यांवर अचूकपणे उचलत नाही.

217
00:15:09,780 --> 00:15:12,014
आणि हा बिंदू खरोखर घरी आणण्यासाठी, तुम्ही यादृच्छिक 

218
00:15:12,014 --> 00:15:13,820
प्रतिमा इनपुट करता तेव्हा काय होते ते पहा.

219
00:15:14,320 --> 00:15:18,680
जर सिस्टीम स्मार्ट असेल, तर तुम्हाला कदाचित ती अनिश्चित वाटेल अशी अपेक्षा असेल, 

220
00:15:18,680 --> 00:15:22,659
कदाचित त्या 10 आउटपुट न्यूरॉन्सपैकी कोणतेही सक्रिय केले जाणार नाही किंवा 

221
00:15:22,659 --> 00:15:26,910
ते सर्व समान रीतीने सक्रिय केले जाणार नाहीत, परंतु त्याऐवजी ते आत्मविश्वासाने 

222
00:15:26,910 --> 00:15:31,216
तुम्हाला काही मूर्खपणाचे उत्तर देते, जसे की हे यादृच्छिक आवाजाची खात्री वाटते. 

223
00:15:31,216 --> 00:15:34,160
5 आहे कारण ते असे करते की 5 ची वास्तविक प्रतिमा 5 आहे.

224
00:15:34,540 --> 00:15:37,769
वेगळ्या पद्धतीने शब्दबद्ध केले, जरी हे नेटवर्क अंक अगदी चांगल्या 

225
00:15:37,769 --> 00:15:40,700
प्रकारे ओळखू शकत असले तरी, ते कसे काढायचे याची कल्पना नाही.

226
00:15:41,420 --> 00:15:45,240
यापैकी बरेच काही कारण हे इतके घट्ट विवशित प्रशिक्षण सेटअप आहे.

227
00:15:45,880 --> 00:15:47,740
म्हणजे, इथे स्वतःला नेटवर्कच्या शूजमध्ये ठेवा.

228
00:15:48,140 --> 00:15:52,436
त्याच्या दृष्टीकोनातून, संपूर्ण विश्वामध्ये एका लहान ग्रिडमध्ये केंद्रस्थानी असलेल्या 

229
00:15:52,436 --> 00:15:54,784
स्पष्टपणे परिभाषित अचल अंकांशिवाय काहीही नाही, 

230
00:15:54,784 --> 00:15:59,131
आणि त्याच्या किमतीच्या कार्यामुळे त्याला त्याच्या निर्णयांवर पूर्ण विश्वास असल्याशिवाय 

231
00:15:59,131 --> 00:16:01,080
काहीही होण्यासाठी प्रोत्साहन दिले नाही.

232
00:16:02,120 --> 00:16:05,509
त्यामुळे हे दुसरे लेयर न्यूरॉन्स खरोखर काय करत आहेत याची प्रतिमा म्हणून, 

233
00:16:05,509 --> 00:16:09,362
तुम्हाला कदाचित आश्चर्य वाटेल की मी हे नेटवर्क कडा आणि नमुने उचलण्याच्या प्रेरणेने 

234
00:16:09,362 --> 00:16:09,920
का सादर करू.

235
00:16:09,920 --> 00:16:12,300
मला म्हणायचे आहे की, ते असेच करत नाही.

236
00:16:13,380 --> 00:16:17,180
बरं, हे आमचे अंतिम उद्दिष्ट नसून त्याऐवजी प्रारंभ बिंदू आहे.

237
00:16:17,640 --> 00:16:21,995
खरे सांगायचे तर, हे जुने तंत्रज्ञान आहे, ज्याचे 80 आणि 90 च्या दशकात संशोधन केले 

238
00:16:21,995 --> 00:16:26,190
गेले होते, आणि अधिक तपशीलवार आधुनिक रूपे समजून घेण्यापूर्वी तुम्हाला ते समजून 

239
00:16:26,190 --> 00:16:30,276
घेणे आवश्यक आहे, आणि हे स्पष्टपणे काही मनोरंजक समस्या सोडविण्यास सक्षम आहे, 

240
00:16:30,276 --> 00:16:34,740
परंतु तुम्ही जितके अधिक जाणून घ्याल ते लपलेले स्तर खरोखर करत आहेत, कमी हुशार दिसते.

241
00:16:38,480 --> 00:16:42,307
तुम्ही कसे शिकता याकडे नेटवर्क्स कसे शिकतात यावरून क्षणभर फोकस हलवणे, 

242
00:16:42,307 --> 00:16:46,300
हे केवळ तेव्हाच होईल जेव्हा तुम्ही इथल्या सामग्रीमध्ये सक्रियपणे गुंतलात.

243
00:16:47,060 --> 00:16:51,666
एक अतिशय सोपी गोष्ट मला हवी आहे की तुम्ही आत्ताच थांबा आणि तुम्ही या प्रणालीमध्ये 

244
00:16:51,666 --> 00:16:56,329
कोणते बदल करू शकता आणि किनारी आणि नमुने यांसारख्या गोष्टींना अधिक चांगल्या प्रकारे 

245
00:16:56,329 --> 00:17:00,880
उचलण्याची तुमची इच्छा असेल तर ती प्रतिमा कशी पाहते याबद्दल क्षणभर सखोल विचार करा.

246
00:17:01,479 --> 00:17:04,745
परंतु त्याहूनही चांगले, सामग्रीशी प्रत्यक्षात गुंतण्यासाठी, 

247
00:17:04,745 --> 00:17:09,099
मी सखोल शिक्षण आणि न्यूरल नेटवर्कवर मायकेल निल्सन यांच्या पुस्तकाची शिफारस करतो.

248
00:17:09,680 --> 00:17:14,046
त्यामध्ये, तुम्हाला या अचूक उदाहरणासाठी डाउनलोड करण्यासाठी आणि प्ले करण्यासाठी कोड 

249
00:17:14,046 --> 00:17:18,359
आणि डेटा सापडेल आणि तो कोड काय करत आहे हे पुस्तक तुम्हाला टप्प्याटप्प्याने सांगेल.

250
00:17:19,300 --> 00:17:22,551
आश्चर्यकारक गोष्ट म्हणजे हे पुस्तक विनामूल्य आणि सार्वजनिकरित्या उपलब्ध आहे, 

251
00:17:22,551 --> 00:17:25,506
त्यामुळे जर तुम्हाला त्यातून काही मिळाले तर, निल्सनच्या प्रयत्नांसाठी 

252
00:17:25,506 --> 00:17:27,660
देणगी देण्यासाठी माझ्याशी सामील होण्याचा विचार करा.

253
00:17:27,660 --> 00:17:31,843
ख्रिस ओला ची अभूतपूर्व आणि सुंदर ब्लॉग पोस्ट आणि डिस्टिल मधील 

254
00:17:31,843 --> 00:17:36,500
लेखांसह मी वर्णनात मला खूप आवडणारी काही इतर संसाधने देखील जोडली आहेत.

255
00:17:38,280 --> 00:17:40,718
शेवटच्या काही मिनिटांसाठी येथे गोष्टी बंद करण्यासाठी, 

256
00:17:40,718 --> 00:17:43,880
मी लीशा लीसोबत घेतलेल्या मुलाखतीच्या एका स्निपेटमध्ये परत येऊ इच्छितो.

257
00:17:44,300 --> 00:17:46,065
तुम्हाला कदाचित शेवटच्या व्हिडिओवरून आठवत असेल, 

258
00:17:46,065 --> 00:17:47,720
तिने तिचे पीएचडीचे काम डीप लर्निंगमध्ये केले.

259
00:17:48,300 --> 00:17:52,094
या छोट्या छोट्या भागामध्ये ती अलीकडील दोन पेपर्सबद्दल बोलते जे खरोखरच 

260
00:17:52,094 --> 00:17:55,780
काही आधुनिक प्रतिमा ओळख नेटवर्क प्रत्यक्षात कसे शिकत आहेत हे शोधतात.

261
00:17:56,120 --> 00:17:59,945
आम्ही संभाषणात कुठे होतो ते सेट करण्यासाठी, पहिल्या पेपरने यापैकी एक विशेषत: 

262
00:17:59,945 --> 00:18:04,268
खोल न्यूरल नेटवर्क घेतले जे इमेज रेकग्निशनमध्ये खरोखर चांगले आहे आणि त्यास योग्यरित्या 

263
00:18:04,268 --> 00:18:08,740
लेबल केलेल्या डेटासेटवर प्रशिक्षण देण्याऐवजी, प्रशिक्षणापूर्वी आजूबाजूची सर्व लेबले बदलली.

264
00:18:09,480 --> 00:18:12,555
साहजिकच येथे चाचणी अचूकता यादृच्छिक पेक्षा चांगली नव्हती, 

265
00:18:12,555 --> 00:18:15,206
कारण प्रत्येक गोष्ट यादृच्छिकपणे लेबल केलेली आहे, 

266
00:18:15,206 --> 00:18:19,077
परंतु तरीही ती योग्यरित्या लेबल केलेल्या डेटासेटवर तुम्ही समान प्रशिक्षण 

267
00:18:19,077 --> 00:18:20,880
अचूकता प्राप्त करण्यास सक्षम होती.

268
00:18:21,600 --> 00:18:26,431
मुळात, या विशिष्ट नेटवर्कसाठी लाखो वजने केवळ यादृच्छिक डेटा लक्षात ठेवण्यासाठी 

269
00:18:26,431 --> 00:18:31,323
पुरेसे होते, ज्यामुळे हा प्रश्न निर्माण होतो की हे खर्चाचे कार्य कमी करणे खरोखर 

270
00:18:31,323 --> 00:18:36,400
प्रतिमेतील कोणत्याही प्रकारच्या संरचनेशी सुसंगत आहे किंवा ते फक्त लक्षात ठेवणे आहे?

271
00:18:51,440 --> 00:18:57,382
तुम्ही अचूकता वक्र पाहिल्यास, जर तुम्ही फक्त एका यादृच्छिक डेटासेटवर प्रशिक्षण घेत असाल, 

272
00:18:57,382 --> 00:19:02,190
तर वक्र क्रमवारी जवळजवळ एक रेखीय फॅशनमध्ये अतिशय हळू हळू खाली गेली आहे, 

273
00:19:02,190 --> 00:19:07,866
त्यामुळे तुम्हाला शक्य तितके स्थानिक किमान शोधण्यासाठी खरोखरच संघर्ष करावा लागत आहे, 

274
00:19:07,866 --> 00:19:12,140
तुम्हाला माहिती आहे. , योग्य वजन ज्यामुळे तुम्हाला अचूकता मिळेल.

275
00:19:12,240 --> 00:19:17,733
जर तुम्ही खरच एखाद्या संरचित डेटासेटवर प्रशिक्षण घेत असाल, ज्यामध्ये योग्य लेबले असतील, 

276
00:19:17,733 --> 00:19:23,226
तर तुम्ही सुरुवातीला थोडेसे फिराल, परंतु नंतर त्या अचूकतेच्या पातळीवर जाण्यासाठी तुम्ही 

277
00:19:23,226 --> 00:19:28,220
खूप वेगाने घसरलात आणि त्यामुळे काही अर्थाने ते स्थानिक मॅक्सिमा शोधणे सोपे होते.

278
00:19:28,540 --> 00:19:34,136
आणि म्हणूनच त्यातही मनोरंजक गोष्ट म्हणजे काही वर्षांपूर्वीचा आणखी एक पेपर प्रकाशात आणतो, 

279
00:19:34,136 --> 00:19:37,468
ज्यामध्ये नेटवर्क स्तरांबद्दल बरेच अधिक सरलीकरण आहे, 

280
00:19:37,468 --> 00:19:42,184
परंतु निकालांपैकी एक असे सांगत होता की आपण ऑप्टिमायझेशन लँडस्केप कसे पहाल, 

281
00:19:42,184 --> 00:19:47,089
स्थानिक मिनिमा जी या नेटवर्क्सना शिकण्याची प्रवृत्ती असते ती प्रत्यक्षात समान 

282
00:19:47,089 --> 00:19:51,427
गुणवत्तेची असते, त्यामुळे काही अर्थाने तुमचा डेटासेट संरचित असल्यास, 

283
00:19:51,427 --> 00:19:54,320
तुम्हाला ते अधिक सहजतेने शोधण्यात सक्षम असावे.

284
00:19:58,160 --> 00:20:01,180
तुमच्यापैकी जे पॅट्रिऑनला पाठिंबा देत आहेत त्यांचे नेहमीप्रमाणेच माझे आभार.

285
00:20:01,520 --> 00:20:04,109
गेम चेंजर पॅट्रिऑन काय आहे हे मी आधी सांगितले आहे, 

286
00:20:04,109 --> 00:20:06,800
परंतु हे व्हिडिओ खरोखर तुमच्याशिवाय शक्य होणार नाहीत.

287
00:20:07,460 --> 00:20:10,208
VC फर्म Amplify Partners चे देखील मी विशेष आभार मानू इच्छितो, 

288
00:20:10,208 --> 00:20:12,780
त्यांनी मालिकेतील या सुरुवातीच्या व्हिडिओंच्या समर्थनार्थ.

