[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "Στο προηγούμενο βίντεο παρουσίασα τη δομή ενός νευρωνικού δικτύου.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "Θα κάνω μια σύντομη ανακεφαλαίωση εδώ, ώστε να είναι φρέσκο στο μυαλό μας, και στη συνέχεια έχω δύο κύριους στόχους για αυτό το βίντεο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "Η πρώτη είναι να εισαγάγουμε την ιδέα της βαθμωτής καθόδου, η οποία διέπει όχι μόνο τον τρόπο με τον οποίο τα νευρωνικά δίκτυα μαθαίνουν, αλλά και τον τρόπο με τον οποίο λειτουργούν πολλά άλλα συστήματα μηχανικής μάθησης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "Στη συνέχεια, θα εμβαθύνουμε λίγο περισσότερο στο πώς αποδίδει το συγκεκριμένο δίκτυο και τι τελικά αναζητούν αυτά τα κρυφά στρώματα νευρώνων.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "Υπενθυμίζεται ότι ο στόχος μας εδώ είναι το κλασικό παράδειγμα της αναγνώρισης χειρόγραφων ψηφίων, ο κόσμος των νευρωνικών δικτύων.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "Αυτά τα ψηφία απεικονίζονται σε ένα πλέγμα 28x28 pixel, με κάθε pixel να έχει κάποια τιμή κλίμακας του γκρι μεταξύ 0 και 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "Αυτά καθορίζουν τις ενεργοποιήσεις των 784 νευρώνων στο επίπεδο εισόδου του δικτύου.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "Και στη συνέχεια, η ενεργοποίηση για κάθε νευρώνα στα επόμενα στρώματα βασίζεται σε ένα σταθμισμένο άθροισμα όλων των ενεργοποιήσεων στο προηγούμενο στρώμα, συν κάποιον ειδικό αριθμό που ονομάζεται προκατάληψη.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "Στη συνέχεια, συνθέτετε αυτό το άθροισμα με κάποια άλλη συνάρτηση, όπως η σιγμοειδής squishification, ή μια relu, όπως σας έδειξα στο προηγούμενο βίντεο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "Συνολικά, δεδομένης της κάπως αυθαίρετης επιλογής δύο κρυφών στρωμάτων με 16 νευρώνες το καθένα, το δίκτυο έχει περίπου 13.000 βάρη και προκαταλήψεις που μπορούμε να ρυθμίσουμε, και είναι αυτές οι τιμές που καθορίζουν τι ακριβώς κάνει το δίκτυο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "Τότε αυτό που εννοούμε όταν λέμε ότι αυτό το δίκτυο ταξινομεί ένα δεδομένο ψηφίο είναι ότι ο φωτεινότερος από αυτούς τους 10 νευρώνες στο τελικό στρώμα αντιστοιχεί σε αυτό το ψηφίο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "Και θυμηθείτε, το κίνητρο που είχαμε στο μυαλό μας εδώ για τη δομή των επιπέδων ήταν ότι ίσως το δεύτερο επίπεδο θα μπορούσε να εντοπίσει τις άκρες, και το τρίτο επίπεδο θα μπορούσε να εντοπίσει μοτίβα όπως βρόχους και γραμμές, και το τελευταίο θα μπορούσε απλώς να συνθέσει αυτά τα μοτίβα για να αναγνωρίσει ψηφία.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "Εδώ λοιπόν μαθαίνουμε πώς μαθαίνει το δίκτυο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "Αυτό που θέλουμε είναι ένας αλγόριθμος όπου μπορείτε να δείξετε σε αυτό το δίκτυο ένα σωρό δεδομένα εκπαίδευσης, τα οποία έχουν τη μορφή μιας σειράς διαφορετικών εικόνων χειρόγραφων ψηφίων, μαζί με ετικέτες για το τι υποτίθεται ότι είναι, και αυτό θα προσαρμόσει αυτά τα 13.000 βάρη και τις προκαταλήψεις έτσι ώστε να βελτιώσει την απόδοσή του στα δεδομένα εκπαίδευσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "Ας ελπίσουμε ότι αυτή η πολυεπίπεδη δομή θα σημαίνει ότι αυτό που μαθαίνει γενικεύεται σε εικόνες πέρα από τα δεδομένα εκπαίδευσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "Ο τρόπος με τον οποίο το δοκιμάζουμε αυτό είναι ότι αφού εκπαιδεύσουμε το δίκτυο, του δείχνουμε περισσότερα επισημασμένα δεδομένα που δεν έχει δει ποτέ πριν, και βλέπουμε με πόση ακρίβεια ταξινομεί αυτές τις νέες εικόνες.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "Ευτυχώς για εμάς, και αυτό που κάνει αυτό το παράδειγμα τόσο συνηθισμένο για να ξεκινήσουμε, είναι ότι οι καλοί άνθρωποι πίσω από τη βάση δεδομένων MNIST έχουν συγκεντρώσει μια συλλογή δεκάδων χιλιάδων χειρόγραφων εικόνων ψηφίων, κάθε μία από τις οποίες είναι επισημασμένη με τους αριθμούς που υποτίθεται ότι είναι.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "Και όσο προκλητικό κι αν είναι να περιγράψεις μια μηχανή ως μαθησιακή, μόλις δεις πώς λειτουργεί, μοιάζει πολύ λιγότερο με κάποια τρελή υπόθεση επιστημονικής φαντασίας και πολύ περισσότερο με άσκηση υπολογισμού.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "Θέλω να πω, βασικά καταλήγει στην εύρεση του ελαχίστου μιας συγκεκριμένης συνάρτησης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "Θυμηθείτε, εννοιολογικά, σκεφτόμαστε ότι κάθε νευρώνας συνδέεται με όλους τους νευρώνες του προηγούμενου στρώματος, και τα βάρη στο σταθμισμένο άθροισμα που καθορίζουν την ενεργοποίησή του είναι κάτι σαν την ισχύ αυτών των συνδέσεων, και η μεροληψία είναι κάποια ένδειξη για το αν αυτός ο νευρώνας τείνει να είναι ενεργός ή ανενεργός.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "Και για να ξεκινήσουμε τα πράγματα, θα αρχικοποιήσουμε όλα αυτά τα βάρη και τις προκαταλήψεις εντελώς τυχαία.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "Περιττό να πούμε ότι αυτό το δίκτυο θα έχει πολύ κακές επιδόσεις σε ένα δεδομένο παράδειγμα εκπαίδευσης, αφού απλά κάνει κάτι τυχαίο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "Για παράδειγμα, εισάγετε αυτή την εικόνα ενός 3 και το επίπεδο εξόδου μοιάζει με ένα χάος.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "Έτσι, αυτό που κάνετε είναι να ορίσετε μια συνάρτηση κόστους, έναν τρόπο να πείτε στον υπολογιστή, όχι, κακέ υπολογιστή, ότι η έξοδος θα πρέπει να έχει ενεργοποιήσεις που είναι 0 για τους περισσότερους νευρώνες, αλλά 1 για αυτόν τον νευρώνα, αυτό που μου δώσατε είναι εντελώς σκουπίδι.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "Για να το πούμε λίγο πιο μαθηματικά, προσθέτετε τα τετράγωνα των διαφορών μεταξύ κάθε μιας από αυτές τις ενεργοποιήσεις εξόδου σκουπιδιών και της τιμής που θέλετε να έχουν, και αυτό είναι που θα ονομάσουμε κόστος ενός μεμονωμένου παραδείγματος εκπαίδευσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "Παρατηρήστε ότι αυτό το άθροισμα είναι μικρό όταν το δίκτυο ταξινομεί σωστά την εικόνα, αλλά είναι μεγάλο όταν το δίκτυο μοιάζει να μην ξέρει τι κάνει.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "Έτσι, αυτό που κάνετε είναι να εξετάσετε το μέσο κόστος για όλες τις δεκάδες χιλιάδες εκπαιδευτικά παραδείγματα που έχετε στη διάθεσή σας.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "Αυτό το μέσο κόστος είναι το μέτρο για το πόσο χάλια είναι το δίκτυο και πόσο άσχημα πρέπει να αισθάνεται ο υπολογιστής.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "Και αυτό είναι ένα περίπλοκο πράγμα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "Θυμάστε πώς το ίδιο το δίκτυο ήταν βασικά μια συνάρτηση, που δέχεται 784 αριθμούς ως είσοδο, τις τιμές των εικονοστοιχείων, και παράγει 10 αριθμούς ως έξοδο, και κατά μία έννοια παραμετροποιείται από όλα αυτά τα βάρη και τις προκαταλήψεις;",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "Λοιπόν, η συνάρτηση κόστους είναι ένα επίπεδο πολυπλοκότητας πάνω σε αυτό.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "Παίρνει ως είσοδο αυτά τα 13.000 περίπου βάρη και τις προκαταλήψεις και βγάζει έναν ενιαίο αριθμό που περιγράφει πόσο κακά είναι αυτά τα βάρη και οι προκαταλήψεις, και ο τρόπος με τον οποίο ορίζεται εξαρτάται από τη συμπεριφορά του δικτύου σε όλες τις δεκάδες χιλιάδες των δεδομένων εκπαίδευσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "Αυτά είναι πολλά που πρέπει να σκεφτούμε.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "Αλλά το να λέτε απλώς στον υπολογιστή τι χάλια δουλειά κάνει δεν είναι πολύ χρήσιμο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "Θέλετε να του πείτε πώς να αλλάξει αυτά τα βάρη και τις προκαταλήψεις, ώστε να βελτιωθεί.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "Για να το κάνετε πιο εύκολο, αντί να προσπαθείτε να φανταστείτε μια συνάρτηση με 13.000 εισόδους, φανταστείτε μια απλή συνάρτηση που έχει έναν αριθμό ως είσοδο και έναν αριθμό ως έξοδο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "Πώς μπορείτε να βρείτε μια είσοδο που ελαχιστοποιεί την τιμή αυτής της συνάρτησης;",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "Οι φοιτητές του μαθηματικού λογισμού θα γνωρίζουν ότι μερικές φορές μπορείτε να υπολογίσετε αυτό το ελάχιστο ρητά, αλλά αυτό δεν είναι πάντα εφικτό για πραγματικά περίπλοκες συναρτήσεις, σίγουρα όχι στην έκδοση 13.000 εισόδων αυτής της κατάστασης για την τρελά περίπλοκη συνάρτηση κόστους του νευρωνικού μας δικτύου.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "Μια πιο ευέλικτη τακτική είναι να ξεκινήσετε από οποιαδήποτε είσοδο και να υπολογίσετε προς ποια κατεύθυνση πρέπει να κινηθείτε για να κάνετε την έξοδο χαμηλότερη.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "Συγκεκριμένα, αν μπορείτε να υπολογίσετε την κλίση της συνάρτησης στο σημείο που βρίσκεστε, τότε μετατοπίστε προς τα αριστερά αν η κλίση είναι θετική και μετατοπίστε την είσοδο προς τα δεξιά αν η κλίση είναι αρνητική.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "Αν το κάνετε αυτό επανειλημμένα, ελέγχοντας σε κάθε σημείο τη νέα κλίση και κάνοντας το κατάλληλο βήμα, θα προσεγγίσετε κάποιο τοπικό ελάχιστο της συνάρτησης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "Η εικόνα που μπορεί να έχετε στο μυαλό σας εδώ είναι μια μπάλα που κυλάει σε έναν λόφο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "Παρατηρήστε, ακόμη και για αυτή την πραγματικά απλοποιημένη συνάρτηση μίας εισόδου, υπάρχουν πολλές πιθανές κοιλάδες στις οποίες μπορεί να προσγειωθείτε, ανάλογα με την τυχαία είσοδο από την οποία ξεκινάτε, και δεν υπάρχει καμία εγγύηση ότι το τοπικό ελάχιστο στο οποίο θα προσγειωθείτε θα είναι η μικρότερη δυνατή τιμή της συνάρτησης κόστους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "Αυτό θα μεταφερθεί και στην περίπτωση των νευρωνικών δικτύων.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "Θέλω επίσης να παρατηρήσετε ότι αν κάνετε τα μεγέθη των βημάτων σας ανάλογα με την κλίση, τότε όταν η κλίση εξομαλύνεται προς το ελάχιστο, τα βήματά σας γίνονται όλο και μικρότερα, και αυτό σας βοηθάει να μην υπερβείτε τα όρια.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "Ανεβάζοντας λίγο την πολυπλοκότητα, φανταστείτε μια συνάρτηση με δύο εισόδους και μία έξοδο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "Μπορείτε να φανταστείτε το χώρο εισόδου ως το επίπεδο xy και τη συνάρτηση κόστους ως μια επιφάνεια πάνω από αυτό.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "Αντί να ρωτάτε για την κλίση της συνάρτησης, θα πρέπει να ρωτάτε προς ποια κατεύθυνση θα πρέπει να κάνετε ένα βήμα σε αυτόν τον χώρο εισόδου ώστε να μειώσετε την έξοδο της συνάρτησης πιο γρήγορα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "Με άλλα λόγια, ποια είναι η κατεύθυνση της κατηφόρας;",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "Και πάλι, είναι χρήσιμο να σκεφτείτε μια μπάλα που κυλάει κάτω από το λόφο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "Όσοι από εσάς είστε εξοικειωμένοι με τον πολυμεταβλητό λογισμό θα γνωρίζετε ότι η κλίση μιας συνάρτησης σας δίνει την κατεύθυνση της πιο απότομης ανόδου, δηλαδή προς ποια κατεύθυνση πρέπει να βαδίσετε για να αυξήσετε τη συνάρτηση πιο γρήγορα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "Φυσικά, λαμβάνοντας το αρνητικό αυτής της κλίσης, θα βρείτε την κατεύθυνση προς την οποία πρέπει να κάνετε το βήμα που μειώνει τη συνάρτηση ταχύτερα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "Ακόμη περισσότερο, το μήκος αυτού του διανύσματος κλίσης είναι μια ένδειξη για το πόσο απότομη είναι αυτή η πιο απότομη κλίση.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "Αν δεν είστε εξοικειωμένοι με τον πολυμεταβλητό λογισμό και θέλετε να μάθετε περισσότερα, δείτε κάποια από τις εργασίες που έκανα για την Khan Academy σχετικά με το θέμα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "Ειλικρινά όμως, το μόνο που έχει σημασία για εσάς και για μένα αυτή τη στιγμή είναι ότι κατ' αρχήν υπάρχει ένας τρόπος να υπολογιστεί αυτό το διάνυσμα, αυτό το διάνυσμα που σας λέει ποια είναι η κατεύθυνση της κατηφόρας και πόσο απότομη είναι.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "Θα είσαι εντάξει αν αυτό είναι το μόνο που ξέρεις και δεν είσαι απόλυτα σίγουρος για τις λεπτομέρειες.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "Αν μπορείτε να το βρείτε αυτό, ο αλγόριθμος για την ελαχιστοποίηση της συνάρτησης είναι να υπολογίσετε αυτή την κατεύθυνση κλίσης, στη συνέχεια να κάνετε ένα μικρό βήμα προς τα κάτω και να το επαναλάβετε ξανά και ξανά.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "Είναι η ίδια βασική ιδέα για μια συνάρτηση που έχει 13.000 εισόδους αντί για 2 εισόδους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "Φανταστείτε να οργανώσετε και τα 13.000 βάρη και προκαταλήψεις του δικτύου μας σε ένα γιγαντιαίο διάνυσμα στήλης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "Η αρνητική κλίση της συνάρτησης κόστους είναι απλώς ένα διάνυσμα, είναι κάποια κατεύθυνση μέσα σε αυτόν τον εξωφρενικά τεράστιο χώρο εισόδου που σας λέει ποιες ωθήσεις σε όλους αυτούς τους αριθμούς θα προκαλέσουν την ταχύτερη μείωση της συνάρτησης κόστους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "Και φυσικά, με την ειδικά σχεδιασμένη συνάρτηση κόστους, η αλλαγή των βαρών και των προκαταλήψεων για τη μείωσή της σημαίνει ότι η έξοδος του δικτύου σε κάθε κομμάτι δεδομένων εκπαίδευσης μοιάζει λιγότερο με μια τυχαία σειρά 10 τιμών και περισσότερο με μια πραγματική απόφαση που θέλουμε να λάβει.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "Είναι σημαντικό να θυμάστε ότι αυτή η συνάρτηση κόστους περιλαμβάνει ένα μέσο όρο για όλα τα δεδομένα εκπαίδευσης, οπότε αν την ελαχιστοποιήσετε, σημαίνει ότι η απόδοση είναι καλύτερη σε όλα αυτά τα δείγματα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "Ο αλγόριθμος για τον αποτελεσματικό υπολογισμό αυτής της κλίσης, ο οποίος είναι ουσιαστικά η καρδιά του τρόπου με τον οποίο ένα νευρωνικό δίκτυο μαθαίνει, ονομάζεται backpropagation, και γι' αυτόν θα μιλήσω στο επόμενο βίντεο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "Εκεί, θέλω πραγματικά να αφιερώσω χρόνο για να εξετάσω τι ακριβώς συμβαίνει σε κάθε βάρος και προκατάληψη για ένα δεδομένο κομμάτι δεδομένων εκπαίδευσης, προσπαθώντας να δώσω μια διαισθητική αίσθηση για το τι συμβαίνει πέρα από το σωρό των σχετικών υπολογισμών και τύπων.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "Εδώ και τώρα, το κύριο πράγμα που θέλω να ξέρετε, ανεξάρτητα από τις λεπτομέρειες της υλοποίησης, είναι ότι αυτό που εννοούμε όταν μιλάμε για ένα δίκτυο που μαθαίνει είναι ότι απλώς ελαχιστοποιεί μια συνάρτηση κόστους.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "Και παρατηρήστε, μια συνέπεια αυτού είναι ότι είναι σημαντικό αυτή η συνάρτηση κόστους να έχει μια ωραία ομαλή έξοδο, ώστε να μπορούμε να βρούμε ένα τοπικό ελάχιστο κάνοντας μικρά βήματα προς τα κάτω.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "Αυτός είναι ο λόγος, παρεμπιπτόντως, για τον οποίο οι τεχνητοί νευρώνες έχουν συνεχώς μεταβαλλόμενες ενεργοποιήσεις, αντί να είναι απλώς ενεργοί ή ανενεργοί με δυαδικό τρόπο, όπως οι βιολογικοί νευρώνες.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "Αυτή η διαδικασία επανειλημμένης ώθησης της εισόδου μιας συνάρτησης κατά κάποιο πολλαπλάσιο της αρνητικής κλίσης ονομάζεται κάθοδος κλίσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "Είναι ένας τρόπος σύγκλισης προς κάποιο τοπικό ελάχιστο μιας συνάρτησης κόστους, ουσιαστικά μια κοιλάδα σε αυτό το γράφημα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "Εξακολουθώ να δείχνω την εικόνα μιας συνάρτησης με δύο εισόδους, φυσικά, επειδή τα σπρωξίματα σε έναν χώρο εισόδου 13.000 διαστάσεων είναι λίγο δύσκολο να τα καταλάβετε, αλλά υπάρχει ένας ωραίος μη-χωρικός τρόπος να το σκεφτείτε αυτό.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "Κάθε συνιστώσα της αρνητικής κλίσης μας λέει δύο πράγματα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "Το πρόσημο, φυσικά, μας λέει αν η αντίστοιχη συνιστώσα του διανύσματος εισόδου πρέπει να ωθηθεί προς τα πάνω ή προς τα κάτω.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "Όμως, το σημαντικότερο είναι ότι τα σχετικά μεγέθη όλων αυτών των συνιστωσών σας λένε ποιες αλλαγές έχουν μεγαλύτερη σημασία.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "Βλέπετε, στο δίκτυό μας, μια προσαρμογή σε ένα από τα βάρη μπορεί να έχει πολύ μεγαλύτερο αντίκτυπο στη συνάρτηση κόστους από ό,τι η προσαρμογή σε κάποιο άλλο βάρος.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "Ορισμένες από αυτές τις συνδέσεις έχουν μεγαλύτερη σημασία για τα δεδομένα εκπαίδευσής μας.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "Έτσι, ένας τρόπος με τον οποίο μπορείτε να σκεφτείτε αυτό το διάνυσμα κλίσης της τεράστιας συνάρτησης κόστους είναι ότι κωδικοποιεί τη σχετική σημασία κάθε βάρους και προκατάληψης, δηλαδή ποια από αυτές τις αλλαγές θα φέρει το μεγαλύτερο όφελος για το χρήμα σας.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "Αυτός είναι πραγματικά ένας άλλος τρόπος σκέψης για την κατεύθυνση.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "Για να πάρουμε ένα απλούστερο παράδειγμα, αν έχετε κάποια συνάρτηση με δύο μεταβλητές ως είσοδο, και υπολογίσετε ότι η κλίση της σε κάποιο συγκεκριμένο σημείο βγαίνει ως 3,1, τότε από τη μία πλευρά μπορείτε να το ερμηνεύσετε αυτό ως κάτι που λέει ότι όταν στέκεστε σε αυτή την είσοδο, η κίνηση προς αυτή την κατεύθυνση αυξάνει τη συνάρτηση πιο γρήγορα, ότι όταν κάνετε τη γραφική παράσταση της συνάρτησης πάνω από το επίπεδο των σημείων εισόδου, αυτό το διάνυσμα είναι αυτό που σας δίνει την ευθεία ανοδική κατεύθυνση.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "Αλλά ένας άλλος τρόπος για να το διαβάσετε αυτό είναι να πείτε ότι οι αλλαγές σε αυτή την πρώτη μεταβλητή έχουν 3 φορές μεγαλύτερη σημασία από τις αλλαγές στη δεύτερη μεταβλητή, ότι τουλάχιστον στη γειτονιά της σχετικής εισόδου, η μετατόπιση της τιμής x έχει πολύ μεγαλύτερη αξία για το χρήμα σας.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "Ας μεγεθύνουμε και ας συνοψίσουμε πού βρισκόμαστε μέχρι στιγμής.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "Το ίδιο το δίκτυο είναι αυτή η συνάρτηση με 784 εισόδους και 10 εξόδους, που ορίζεται με βάση όλα αυτά τα σταθμισμένα αθροίσματα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "Η συνάρτηση κόστους είναι ένα στρώμα πολυπλοκότητας πάνω από αυτό.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "Λαμβάνει τα 13.000 βάρη και τις προκαταλήψεις ως δεδομένα εισόδου και παράγει ένα ενιαίο μέτρο αθλιότητας με βάση τα παραδείγματα εκπαίδευσης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "Και η κλίση της συνάρτησης κόστους είναι ένα ακόμη επίπεδο πολυπλοκότητας.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "Μας λέει ποιες αλλαγές σε όλα αυτά τα βάρη και τις προκαταλήψεις προκαλούν την ταχύτερη αλλαγή στην τιμή της συνάρτησης κόστους, κάτι που μπορείτε να ερμηνεύσετε ως το ποιες αλλαγές σε ποια βάρη έχουν τη μεγαλύτερη σημασία.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "Έτσι, όταν αρχικοποιείτε το δίκτυο με τυχαία βάρη και προκαταλήψεις και τα προσαρμόζετε πολλές φορές με βάση αυτή τη διαδικασία βαθμωτής καθόδου, πόσο καλά αποδίδει στην πραγματικότητα σε εικόνες που δεν έχει δει ποτέ πριν;",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "Αυτό που περιέγραψα εδώ, με τα δύο κρυφά στρώματα των 16 νευρώνων το καθένα, που επιλέχθηκαν κυρίως για αισθητικούς λόγους, δεν είναι κακό, ταξινομώντας σωστά περίπου το 96% των νέων εικόνων που βλέπει.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "Και ειλικρινά, αν δείτε μερικά από τα παραδείγματα στα οποία τα κάνει θάλασσα, αισθάνεστε υποχρεωμένοι να το αφήσετε λίγο χαλαρό.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "Τώρα, αν παίξετε με τη δομή του κρυμμένου στρώματος και κάνετε μερικές βελτιώσεις, μπορείτε να το φτάσετε στο 98%.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "Και αυτό είναι πολύ καλό!",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "Δεν είναι το καλύτερο, σίγουρα μπορείτε να έχετε καλύτερες επιδόσεις αν γίνετε πιο εξελιγμένοι από αυτό το απλό δίκτυο, αλλά δεδομένου του πόσο τρομακτικό είναι το αρχικό έργο, νομίζω ότι υπάρχει κάτι απίστευτο στο ότι οποιοδήποτε δίκτυο τα καταφέρνει τόσο καλά σε εικόνες που δεν έχει ξαναδεί, δεδομένου ότι ποτέ δεν του είπαμε συγκεκριμένα για ποια μοτίβα να ψάξει.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "Αρχικά, ο τρόπος με τον οποίο υποκίνησα αυτή τη δομή ήταν περιγράφοντας μια ελπίδα που θα μπορούσαμε να έχουμε, ότι το δεύτερο στρώμα θα μπορούσε να εντοπίσει μικρές άκρες, ότι το τρίτο στρώμα θα συνέθετε αυτές τις άκρες για να αναγνωρίσει βρόχους και μεγαλύτερες γραμμές, και ότι αυτές θα μπορούσαν να συναρμολογηθούν για να αναγνωρίσουν ψηφία.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "Αυτό κάνει λοιπόν το δίκτυό μας στην πραγματικότητα;",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "Λοιπόν, για αυτό τουλάχιστον, καθόλου.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "Θυμάστε πώς στο προηγούμενο βίντεο εξετάσαμε πώς τα βάρη των συνδέσεων από όλους τους νευρώνες του πρώτου στρώματος σε έναν δεδομένο νευρώνα του δεύτερου στρώματος μπορούν να απεικονιστούν ως ένα δεδομένο μοτίβο εικονοστοιχείων που ο νευρώνας του δεύτερου στρώματος λαμβάνει;",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "Λοιπόν, όταν το κάνουμε αυτό για τα βάρη που σχετίζονται με αυτές τις μεταβάσεις, από το πρώτο στρώμα στο επόμενο, αντί να εντοπίσουμε απομονωμένες μικρές ακμές εδώ και εκεί, φαίνονται, λοιπόν, σχεδόν τυχαίες, με μερικά πολύ χαλαρά μοτίβα στη μέση.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "Φαίνεται ότι στον απίστευτα μεγάλο χώρο των 13.000 διαστάσεων των πιθανών βαρών και προκαταλήψεων, το δίκτυό μας βρήκε ένα ευτυχές μικρό τοπικό ελάχιστο που, παρά την επιτυχή ταξινόμηση των περισσότερων εικόνων, δεν εντοπίζει ακριβώς τα μοτίβα που ελπίζαμε.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "Και για να το καταλάβετε αυτό, παρακολουθήστε τι συμβαίνει όταν εισάγετε μια τυχαία εικόνα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "Αν το σύστημα ήταν έξυπνο, θα περιμένατε να αισθάνεται αβεβαιότητα, ίσως να μην ενεργοποιεί πραγματικά κανέναν από αυτούς τους 10 νευρώνες εξόδου ή να τους ενεργοποιεί όλους ομοιόμορφα, αλλά αντ' αυτού σας δίνει με αυτοπεποίθηση κάποια ανόητη απάντηση, σαν να αισθάνεται τόσο σίγουρο ότι αυτός ο τυχαίος θόρυβος είναι 5 όσο και ότι μια πραγματική εικόνα του 5 είναι 5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "Διατυπωμένο διαφορετικά, ακόμη και αν αυτό το δίκτυο μπορεί να αναγνωρίσει αρκετά καλά τα ψηφία, δεν έχει ιδέα πώς να τα σχεδιάσει.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "Πολλά από αυτά οφείλονται στο γεγονός ότι είναι μια τόσο αυστηρά περιορισμένη εκπαιδευτική ρύθμιση.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "Θέλω να πω, βάλτε τον εαυτό σας στη θέση του δικτύου εδώ.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "Από τη σκοπιά του, ολόκληρο το σύμπαν δεν αποτελείται από τίποτε άλλο παρά από σαφώς καθορισμένα ακίνητα ψηφία που βρίσκονται σε ένα μικροσκοπικό πλέγμα, και η συνάρτηση κόστους του δεν του έδωσε ποτέ κανένα κίνητρο να είναι οτιδήποτε άλλο εκτός από απόλυτα σίγουρος για τις αποφάσεις του.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "Έτσι, με αυτή την εικόνα για το τι πραγματικά κάνουν οι νευρώνες του δεύτερου στρώματος, ίσως αναρωτηθείτε γιατί θα εισήγαγα αυτό το δίκτυο με κίνητρο την ανίχνευση ακμών και μοτίβων.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "Θέλω να πω, αυτό δεν είναι καθόλου αυτό που καταλήγει να κάνει.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "Λοιπόν, αυτό δεν πρόκειται να είναι ο τελικός μας στόχος, αλλά ένα σημείο εκκίνησης.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "Ειλικρινά, πρόκειται για παλιά τεχνολογία, το είδος που ερευνήθηκε στις δεκαετίες του '80 και του '90, και πρέπει να την καταλάβετε προτού κατανοήσετε τις πιο λεπτομερείς σύγχρονες παραλλαγές, και είναι σαφώς ικανή να επιλύσει μερικά ενδιαφέροντα προβλήματα, αλλά όσο περισσότερο ψάχνετε τι πραγματικά κάνουν αυτά τα κρυφά στρώματα, τόσο λιγότερο έξυπνη φαίνεται.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "Μετατοπίζοντας για λίγο την εστίαση από το πώς μαθαίνουν τα δίκτυα στο πώς μαθαίνετε εσείς, αυτό θα συμβεί μόνο αν ασχοληθείτε ενεργά με το υλικό εδώ με κάποιο τρόπο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "Ένα πολύ απλό πράγμα που θέλω να κάνετε είναι να σταματήσετε τώρα και να σκεφτείτε βαθιά για μια στιγμή ποιες αλλαγές θα μπορούσατε να κάνετε σε αυτό το σύστημα και στον τρόπο με τον οποίο αντιλαμβάνεται τις εικόνες, αν θέλατε να εντοπίζει καλύτερα πράγματα όπως οι άκρες και τα μοτίβα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "Αλλά για να ασχοληθείτε πραγματικά με την ύλη, συνιστώ ανεπιφύλακτα το βιβλίο του Michael Nielsen για τη βαθιά μάθηση και τα νευρωνικά δίκτυα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "Σε αυτό, μπορείτε να βρείτε τον κώδικα και τα δεδομένα που μπορείτε να κατεβάσετε και να παίξετε με αυτό ακριβώς το παράδειγμα, και το βιβλίο θα σας καθοδηγήσει βήμα προς βήμα τι κάνει αυτός ο κώδικας.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "Το φοβερό είναι ότι αυτό το βιβλίο είναι δωρεάν και διαθέσιμο στο κοινό, οπότε, αν έχετε κάτι από αυτό, σκεφτείτε να κάνετε μαζί μου μια δωρεά για τις προσπάθειες της Nielsen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "Έχω επίσης συνδέσει μερικές άλλες πηγές που μου αρέσουν πολύ στην περιγραφή, συμπεριλαμβανομένης της εκπληκτικής και όμορφης ανάρτησης στο blog του Chris Ola και των άρθρων στο Distill.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "Για να κλείσουμε τα πράγματα εδώ για τα τελευταία λεπτά, θέλω να επιστρέψω σε ένα απόσπασμα της συνέντευξης που είχα με τη Leisha Lee.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "Ίσως τη θυμάστε από το τελευταίο βίντεο, έκανε το διδακτορικό της στη βαθιά μάθηση.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "Σε αυτό το μικρό απόσπασμα μιλάει για δύο πρόσφατες εργασίες που εμβαθύνουν πραγματικά στο πώς μαθαίνουν στην πραγματικότητα ορισμένα από τα πιο σύγχρονα δίκτυα αναγνώρισης εικόνας.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "Για να ξεκινήσουμε τη συζήτηση, η πρώτη εργασία πήρε ένα από αυτά τα ιδιαίτερα βαθιά νευρωνικά δίκτυα που είναι πολύ καλά στην αναγνώριση εικόνων και αντί να το εκπαιδεύσει σε ένα σύνολο δεδομένων με κατάλληλες ετικέτες, ανακάτεψε όλες τις ετικέτες πριν από την εκπαίδευση.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "Προφανώς, η ακρίβεια δοκιμής εδώ δεν ήταν καλύτερη από την τυχαία, αφού όλα είναι τυχαία επισημασμένα, αλλά ήταν ακόμα σε θέση να επιτύχει την ίδια ακρίβεια εκπαίδευσης με αυτή που θα είχατε σε ένα σωστά επισημασμένο σύνολο δεδομένων.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "Βασικά, τα εκατομμύρια βάρη για αυτό το συγκεκριμένο δίκτυο ήταν αρκετά για να απομνημονεύσει τα τυχαία δεδομένα, γεγονός που εγείρει το ερώτημα για το αν η ελαχιστοποίηση αυτής της συνάρτησης κόστους αντιστοιχεί πραγματικά σε οποιοδήποτε είδος δομής στην εικόνα, ή είναι απλά απομνημόνευση;",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "Αν κοιτάξετε αυτή την καμπύλη ακρίβειας, αν απλά εκπαιδεύατε σε ένα τυχαίο σύνολο δεδομένων, αυτή η καμπύλη θα κατέβαινε πολύ αργά με σχεδόν γραμμικό τρόπο, οπότε πραγματικά αγωνίζεστε να βρείτε τα τοπικά ελάχιστα των πιθανών, ξέρετε, των σωστών βαρών που θα σας έδιναν αυτή την ακρίβεια.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "Ενώ αν εκπαιδεύεστε πραγματικά σε ένα δομημένο σύνολο δεδομένων, που έχει τις σωστές ετικέτες, παίζετε λίγο στην αρχή, αλλά στη συνέχεια πέφτετε πολύ γρήγορα για να φτάσετε σε αυτό το επίπεδο ακρίβειας, και έτσι κατά κάποιο τρόπο ήταν ευκολότερο να βρείτε αυτό το τοπικό μέγιστο.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "Και αυτό που ήταν επίσης ενδιαφέρον σε αυτό είναι ότι φέρνει στο φως μια άλλη εργασία που έγινε πριν από μερικά χρόνια, η οποία έχει πολύ περισσότερες απλοποιήσεις σχετικά με τα στρώματα του δικτύου, αλλά ένα από τα αποτελέσματα έλεγε ότι αν κοιτάξετε το τοπίο βελτιστοποίησης, τα τοπικά ελάχιστα που τείνουν να μαθαίνουν αυτά τα δίκτυα είναι στην πραγματικότητα ίσης ποιότητας, οπότε κατά μία έννοια, αν το σύνολο δεδομένων σας είναι δομημένο, θα πρέπει να μπορείτε να το βρείτε πολύ πιο εύκολα.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "Ευχαριστώ, όπως πάντα, όσους από εσάς υποστηρίζετε στο Patreon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "Έχω ξαναπεί πόσο σημαντικό είναι το Patreon, αλλά αυτά τα βίντεο δεν θα ήταν εφικτά χωρίς εσάς.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "Θέλω επίσης να ευχαριστήσω ιδιαίτερα την εταιρεία VC Amplify Partners για την υποστήριξή της σε αυτά τα πρώτα βίντεο της σειράς.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]