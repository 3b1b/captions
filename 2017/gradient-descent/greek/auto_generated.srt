1
00:00:04,180 --> 00:00:07,280
Στο προηγούμενο βίντεο παρουσίασα τη δομή ενός νευρωνικού δικτύου.

2
00:00:07,680 --> 00:00:10,413
Θα κάνω μια σύντομη ανακεφαλαίωση εδώ, ώστε να είναι φρέσκο στο μυαλό μας, 

3
00:00:10,413 --> 00:00:12,600
και στη συνέχεια έχω δύο κύριους στόχους για αυτό το βίντεο.

4
00:00:13,100 --> 00:00:15,136
Η πρώτη είναι να εισαγάγουμε την ιδέα της βαθμωτής καθόδου, 

5
00:00:15,136 --> 00:00:17,783
η οποία διέπει όχι μόνο τον τρόπο με τον οποίο τα νευρωνικά δίκτυα μαθαίνουν, 

6
00:00:17,783 --> 00:00:20,600
αλλά και τον τρόπο με τον οποίο λειτουργούν πολλά άλλα συστήματα μηχανικής μάθησης.

7
00:00:21,120 --> 00:00:24,289
Στη συνέχεια, θα εμβαθύνουμε λίγο περισσότερο στο πώς αποδίδει το 

8
00:00:24,289 --> 00:00:27,940
συγκεκριμένο δίκτυο και τι τελικά αναζητούν αυτά τα κρυφά στρώματα νευρώνων.

9
00:00:28,980 --> 00:00:32,461
Υπενθυμίζεται ότι ο στόχος μας εδώ είναι το κλασικό παράδειγμα 

10
00:00:32,461 --> 00:00:36,220
της αναγνώρισης χειρόγραφων ψηφίων, ο κόσμος των νευρωνικών δικτύων.

11
00:00:37,020 --> 00:00:39,933
Αυτά τα ψηφία απεικονίζονται σε ένα πλέγμα 28x28 pixel, 

12
00:00:39,933 --> 00:00:43,420
με κάθε pixel να έχει κάποια τιμή κλίμακας του γκρι μεταξύ 0 και 1.

13
00:00:43,820 --> 00:00:50,040
Αυτά καθορίζουν τις ενεργοποιήσεις των 784 νευρώνων στο επίπεδο εισόδου του δικτύου.

14
00:00:51,180 --> 00:00:54,439
Και στη συνέχεια, η ενεργοποίηση για κάθε νευρώνα στα επόμενα στρώματα 

15
00:00:54,439 --> 00:00:58,387
βασίζεται σε ένα σταθμισμένο άθροισμα όλων των ενεργοποιήσεων στο προηγούμενο στρώμα, 

16
00:00:58,387 --> 00:01:00,820
συν κάποιον ειδικό αριθμό που ονομάζεται προκατάληψη.

17
00:01:02,160 --> 00:01:05,129
Στη συνέχεια, συνθέτετε αυτό το άθροισμα με κάποια άλλη συνάρτηση, 

18
00:01:05,129 --> 00:01:08,940
όπως η σιγμοειδής squishification, ή μια relu, όπως σας έδειξα στο προηγούμενο βίντεο.

19
00:01:09,480 --> 00:01:14,162
Συνολικά, δεδομένης της κάπως αυθαίρετης επιλογής δύο κρυφών στρωμάτων με 16 

20
00:01:14,162 --> 00:01:18,845
νευρώνες το καθένα, το δίκτυο έχει περίπου 13.000 βάρη και προκαταλήψεις που 

21
00:01:18,845 --> 00:01:23,771
μπορούμε να ρυθμίσουμε, και είναι αυτές οι τιμές που καθορίζουν τι ακριβώς κάνει 

22
00:01:23,771 --> 00:01:24,380
το δίκτυο.

23
00:01:24,880 --> 00:01:27,764
Τότε αυτό που εννοούμε όταν λέμε ότι αυτό το δίκτυο ταξινομεί 

24
00:01:27,764 --> 00:01:30,555
ένα δεδομένο ψηφίο είναι ότι ο φωτεινότερος από αυτούς τους 

25
00:01:30,555 --> 00:01:33,300
10 νευρώνες στο τελικό στρώμα αντιστοιχεί σε αυτό το ψηφίο.

26
00:01:34,100 --> 00:01:37,833
Και θυμηθείτε, το κίνητρο που είχαμε στο μυαλό μας εδώ για τη δομή των επιπέδων 

27
00:01:37,833 --> 00:01:41,053
ήταν ότι ίσως το δεύτερο επίπεδο θα μπορούσε να εντοπίσει τις άκρες, 

28
00:01:41,053 --> 00:01:44,740
και το τρίτο επίπεδο θα μπορούσε να εντοπίσει μοτίβα όπως βρόχους και γραμμές, 

29
00:01:44,740 --> 00:01:48,800
και το τελευταίο θα μπορούσε απλώς να συνθέσει αυτά τα μοτίβα για να αναγνωρίσει ψηφία.

30
00:01:49,800 --> 00:01:52,240
Εδώ λοιπόν μαθαίνουμε πώς μαθαίνει το δίκτυο.

31
00:01:52,640 --> 00:01:56,050
Αυτό που θέλουμε είναι ένας αλγόριθμος όπου μπορείτε να δείξετε σε αυτό 

32
00:01:56,050 --> 00:01:59,366
το δίκτυο ένα σωρό δεδομένα εκπαίδευσης, τα οποία έχουν τη μορφή μιας 

33
00:01:59,366 --> 00:02:01,640
σειράς διαφορετικών εικόνων χειρόγραφων ψηφίων, 

34
00:02:01,640 --> 00:02:03,961
μαζί με ετικέτες για το τι υποτίθεται ότι είναι, 

35
00:02:03,961 --> 00:02:07,325
και αυτό θα προσαρμόσει αυτά τα 13.000 βάρη και τις προκαταλήψεις έτσι 

36
00:02:07,325 --> 00:02:10,120
ώστε να βελτιώσει την απόδοσή του στα δεδομένα εκπαίδευσης.

37
00:02:10,720 --> 00:02:13,813
Ας ελπίσουμε ότι αυτή η πολυεπίπεδη δομή θα σημαίνει ότι αυτό που 

38
00:02:13,813 --> 00:02:16,860
μαθαίνει γενικεύεται σε εικόνες πέρα από τα δεδομένα εκπαίδευσης.

39
00:02:17,640 --> 00:02:21,001
Ο τρόπος με τον οποίο το δοκιμάζουμε αυτό είναι ότι αφού εκπαιδεύσουμε το δίκτυο, 

40
00:02:21,001 --> 00:02:24,117
του δείχνουμε περισσότερα επισημασμένα δεδομένα που δεν έχει δει ποτέ πριν, 

41
00:02:24,117 --> 00:02:26,700
και βλέπουμε με πόση ακρίβεια ταξινομεί αυτές τις νέες εικόνες.

42
00:02:31,120 --> 00:02:34,317
Ευτυχώς για εμάς, και αυτό που κάνει αυτό το παράδειγμα τόσο συνηθισμένο για 

43
00:02:34,317 --> 00:02:37,514
να ξεκινήσουμε, είναι ότι οι καλοί άνθρωποι πίσω από τη βάση δεδομένων MNIST 

44
00:02:37,514 --> 00:02:40,670
έχουν συγκεντρώσει μια συλλογή δεκάδων χιλιάδων χειρόγραφων εικόνων ψηφίων, 

45
00:02:40,670 --> 00:02:44,200
κάθε μία από τις οποίες είναι επισημασμένη με τους αριθμούς που υποτίθεται ότι είναι.

46
00:02:44,900 --> 00:02:48,460
Και όσο προκλητικό κι αν είναι να περιγράψεις μια μηχανή ως μαθησιακή, 

47
00:02:48,460 --> 00:02:52,120
μόλις δεις πώς λειτουργεί, μοιάζει πολύ λιγότερο με κάποια τρελή υπόθεση 

48
00:02:52,120 --> 00:02:55,480
επιστημονικής φαντασίας και πολύ περισσότερο με άσκηση υπολογισμού.

49
00:02:56,200 --> 00:02:59,960
Θέλω να πω, βασικά καταλήγει στην εύρεση του ελαχίστου μιας συγκεκριμένης συνάρτησης.

50
00:03:01,940 --> 00:03:05,965
Θυμηθείτε, εννοιολογικά, σκεφτόμαστε ότι κάθε νευρώνας συνδέεται με όλους τους 

51
00:03:05,965 --> 00:03:09,940
νευρώνες του προηγούμενου στρώματος, και τα βάρη στο σταθμισμένο άθροισμα που 

52
00:03:09,940 --> 00:03:13,864
καθορίζουν την ενεργοποίησή του είναι κάτι σαν την ισχύ αυτών των συνδέσεων, 

53
00:03:13,864 --> 00:03:17,940
και η μεροληψία είναι κάποια ένδειξη για το αν αυτός ο νευρώνας τείνει να είναι 

54
00:03:17,940 --> 00:03:18,960
ενεργός ή ανενεργός.

55
00:03:19,720 --> 00:03:22,081
Και για να ξεκινήσουμε τα πράγματα, θα αρχικοποιήσουμε 

56
00:03:22,081 --> 00:03:24,400
όλα αυτά τα βάρη και τις προκαταλήψεις εντελώς τυχαία.

57
00:03:24,940 --> 00:03:27,764
Περιττό να πούμε ότι αυτό το δίκτυο θα έχει πολύ κακές επιδόσεις 

58
00:03:27,764 --> 00:03:30,720
σε ένα δεδομένο παράδειγμα εκπαίδευσης, αφού απλά κάνει κάτι τυχαίο.

59
00:03:31,040 --> 00:03:36,020
Για παράδειγμα, εισάγετε αυτή την εικόνα ενός 3 και το επίπεδο εξόδου μοιάζει με ένα χάος.

60
00:03:36,600 --> 00:03:39,680
Έτσι, αυτό που κάνετε είναι να ορίσετε μια συνάρτηση κόστους, 

61
00:03:39,680 --> 00:03:42,611
έναν τρόπο να πείτε στον υπολογιστή, όχι, κακέ υπολογιστή, 

62
00:03:42,611 --> 00:03:45,940
ότι η έξοδος θα πρέπει να έχει ενεργοποιήσεις που είναι 0 για τους 

63
00:03:45,940 --> 00:03:48,623
περισσότερους νευρώνες, αλλά 1 για αυτόν τον νευρώνα, 

64
00:03:48,623 --> 00:03:50,760
αυτό που μου δώσατε είναι εντελώς σκουπίδι.

65
00:03:51,720 --> 00:03:56,187
Για να το πούμε λίγο πιο μαθηματικά, προσθέτετε τα τετράγωνα των διαφορών μεταξύ κάθε 

66
00:03:56,187 --> 00:04:00,707
μιας από αυτές τις ενεργοποιήσεις εξόδου σκουπιδιών και της τιμής που θέλετε να έχουν, 

67
00:04:00,707 --> 00:04:05,020
και αυτό είναι που θα ονομάσουμε κόστος ενός μεμονωμένου παραδείγματος εκπαίδευσης.

68
00:04:05,960 --> 00:04:12,044
Παρατηρήστε ότι αυτό το άθροισμα είναι μικρό όταν το δίκτυο ταξινομεί σωστά την εικόνα, 

69
00:04:12,044 --> 00:04:16,399
αλλά είναι μεγάλο όταν το δίκτυο μοιάζει να μην ξέρει τι κάνει.

70
00:04:18,640 --> 00:04:22,039
Έτσι, αυτό που κάνετε είναι να εξετάσετε το μέσο κόστος για όλες τις 

71
00:04:22,039 --> 00:04:25,440
δεκάδες χιλιάδες εκπαιδευτικά παραδείγματα που έχετε στη διάθεσή σας.

72
00:04:27,040 --> 00:04:29,842
Αυτό το μέσο κόστος είναι το μέτρο για το πόσο χάλια είναι 

73
00:04:29,842 --> 00:04:32,740
το δίκτυο και πόσο άσχημα πρέπει να αισθάνεται ο υπολογιστής.

74
00:04:33,420 --> 00:04:34,600
Και αυτό είναι ένα περίπλοκο πράγμα.

75
00:04:35,040 --> 00:04:38,321
Θυμάστε πώς το ίδιο το δίκτυο ήταν βασικά μια συνάρτηση, 

76
00:04:38,321 --> 00:04:42,179
που δέχεται 784 αριθμούς ως είσοδο, τις τιμές των εικονοστοιχείων, 

77
00:04:42,179 --> 00:04:46,784
και παράγει 10 αριθμούς ως έξοδο, και κατά μία έννοια παραμετροποιείται από όλα 

78
00:04:46,784 --> 00:04:48,800
αυτά τα βάρη και τις προκαταλήψεις;

79
00:04:49,500 --> 00:04:52,820
Λοιπόν, η συνάρτηση κόστους είναι ένα επίπεδο πολυπλοκότητας πάνω σε αυτό.

80
00:04:53,100 --> 00:04:56,956
Παίρνει ως είσοδο αυτά τα 13.000 περίπου βάρη και τις προκαταλήψεις και 

81
00:04:56,956 --> 00:05:00,919
βγάζει έναν ενιαίο αριθμό που περιγράφει πόσο κακά είναι αυτά τα βάρη και 

82
00:05:00,919 --> 00:05:04,668
οι προκαταλήψεις, και ο τρόπος με τον οποίο ορίζεται εξαρτάται από τη 

83
00:05:04,668 --> 00:05:08,900
συμπεριφορά του δικτύου σε όλες τις δεκάδες χιλιάδες των δεδομένων εκπαίδευσης.

84
00:05:09,520 --> 00:05:11,000
Αυτά είναι πολλά που πρέπει να σκεφτούμε.

85
00:05:12,400 --> 00:05:15,820
Αλλά το να λέτε απλώς στον υπολογιστή τι χάλια δουλειά κάνει δεν είναι πολύ χρήσιμο.

86
00:05:16,220 --> 00:05:20,060
Θέλετε να του πείτε πώς να αλλάξει αυτά τα βάρη και τις προκαταλήψεις, ώστε να βελτιωθεί.

87
00:05:20,780 --> 00:05:24,135
Για να το κάνετε πιο εύκολο, αντί να προσπαθείτε να φανταστείτε 

88
00:05:24,135 --> 00:05:27,543
μια συνάρτηση με 13.000 εισόδους, φανταστείτε μια απλή συνάρτηση 

89
00:05:27,543 --> 00:05:30,480
που έχει έναν αριθμό ως είσοδο και έναν αριθμό ως έξοδο.

90
00:05:31,480 --> 00:05:35,300
Πώς μπορείτε να βρείτε μια είσοδο που ελαχιστοποιεί την τιμή αυτής της συνάρτησης;

91
00:05:36,460 --> 00:05:40,161
Οι φοιτητές του μαθηματικού λογισμού θα γνωρίζουν ότι μερικές φορές μπορείτε να 

92
00:05:40,161 --> 00:05:44,001
υπολογίσετε αυτό το ελάχιστο ρητά, αλλά αυτό δεν είναι πάντα εφικτό για πραγματικά 

93
00:05:44,001 --> 00:05:47,887
περίπλοκες συναρτήσεις, σίγουρα όχι στην έκδοση 13.000 εισόδων αυτής της κατάστασης 

94
00:05:47,887 --> 00:05:51,080
για την τρελά περίπλοκη συνάρτηση κόστους του νευρωνικού μας δικτύου.

95
00:05:51,580 --> 00:05:55,086
Μια πιο ευέλικτη τακτική είναι να ξεκινήσετε από οποιαδήποτε είσοδο και να 

96
00:05:55,086 --> 00:05:59,200
υπολογίσετε προς ποια κατεύθυνση πρέπει να κινηθείτε για να κάνετε την έξοδο χαμηλότερη.

97
00:06:00,080 --> 00:06:03,247
Συγκεκριμένα, αν μπορείτε να υπολογίσετε την κλίση της συνάρτησης στο 

98
00:06:03,247 --> 00:06:06,551
σημείο που βρίσκεστε, τότε μετατοπίστε προς τα αριστερά αν η κλίση είναι 

99
00:06:06,551 --> 00:06:09,900
θετική και μετατοπίστε την είσοδο προς τα δεξιά αν η κλίση είναι αρνητική.

100
00:06:11,960 --> 00:06:15,750
Αν το κάνετε αυτό επανειλημμένα, ελέγχοντας σε κάθε σημείο τη νέα κλίση και 

101
00:06:15,750 --> 00:06:19,840
κάνοντας το κατάλληλο βήμα, θα προσεγγίσετε κάποιο τοπικό ελάχιστο της συνάρτησης.

102
00:06:20,640 --> 00:06:23,800
Η εικόνα που μπορεί να έχετε στο μυαλό σας εδώ είναι μια μπάλα που κυλάει σε έναν λόφο.

103
00:06:24,620 --> 00:06:28,239
Παρατηρήστε, ακόμη και για αυτή την πραγματικά απλοποιημένη συνάρτηση μίας εισόδου, 

104
00:06:28,239 --> 00:06:31,255
υπάρχουν πολλές πιθανές κοιλάδες στις οποίες μπορεί να προσγειωθείτε, 

105
00:06:31,255 --> 00:06:33,539
ανάλογα με την τυχαία είσοδο από την οποία ξεκινάτε, 

106
00:06:33,539 --> 00:06:37,116
και δεν υπάρχει καμία εγγύηση ότι το τοπικό ελάχιστο στο οποίο θα προσγειωθείτε θα 

107
00:06:37,116 --> 00:06:39,400
είναι η μικρότερη δυνατή τιμή της συνάρτησης κόστους.

108
00:06:40,220 --> 00:06:42,620
Αυτό θα μεταφερθεί και στην περίπτωση των νευρωνικών δικτύων.

109
00:06:43,180 --> 00:06:47,707
Θέλω επίσης να παρατηρήσετε ότι αν κάνετε τα μεγέθη των βημάτων σας ανάλογα με την κλίση, 

110
00:06:47,707 --> 00:06:50,172
τότε όταν η κλίση εξομαλύνεται προς το ελάχιστο, 

111
00:06:50,172 --> 00:06:54,600
τα βήματά σας γίνονται όλο και μικρότερα, και αυτό σας βοηθάει να μην υπερβείτε τα όρια.

112
00:06:55,940 --> 00:06:58,569
Ανεβάζοντας λίγο την πολυπλοκότητα, φανταστείτε 

113
00:06:58,569 --> 00:07:00,980
μια συνάρτηση με δύο εισόδους και μία έξοδο.

114
00:07:01,500 --> 00:07:04,849
Μπορείτε να φανταστείτε το χώρο εισόδου ως το επίπεδο xy 

115
00:07:04,849 --> 00:07:08,140
και τη συνάρτηση κόστους ως μια επιφάνεια πάνω από αυτό.

116
00:07:08,760 --> 00:07:12,160
Αντί να ρωτάτε για την κλίση της συνάρτησης, θα πρέπει να ρωτάτε 

117
00:07:12,160 --> 00:07:15,455
προς ποια κατεύθυνση θα πρέπει να κάνετε ένα βήμα σε αυτόν τον 

118
00:07:15,455 --> 00:07:18,960
χώρο εισόδου ώστε να μειώσετε την έξοδο της συνάρτησης πιο γρήγορα.

119
00:07:19,720 --> 00:07:21,760
Με άλλα λόγια, ποια είναι η κατεύθυνση της κατηφόρας;

120
00:07:22,380 --> 00:07:25,560
Και πάλι, είναι χρήσιμο να σκεφτείτε μια μπάλα που κυλάει κάτω από το λόφο.

121
00:07:26,660 --> 00:07:30,666
Όσοι από εσάς είστε εξοικειωμένοι με τον πολυμεταβλητό λογισμό θα γνωρίζετε ότι 

122
00:07:30,666 --> 00:07:34,372
η κλίση μιας συνάρτησης σας δίνει την κατεύθυνση της πιο απότομης ανόδου, 

123
00:07:34,372 --> 00:07:38,780
δηλαδή προς ποια κατεύθυνση πρέπει να βαδίσετε για να αυξήσετε τη συνάρτηση πιο γρήγορα.

124
00:07:39,560 --> 00:07:41,734
Φυσικά, λαμβάνοντας το αρνητικό αυτής της κλίσης, 

125
00:07:41,734 --> 00:07:45,083
θα βρείτε την κατεύθυνση προς την οποία πρέπει να κάνετε το βήμα που μειώνει 

126
00:07:45,083 --> 00:07:46,040
τη συνάρτηση ταχύτερα.

127
00:07:47,240 --> 00:07:50,539
Ακόμη περισσότερο, το μήκος αυτού του διανύσματος κλίσης είναι 

128
00:07:50,539 --> 00:07:53,840
μια ένδειξη για το πόσο απότομη είναι αυτή η πιο απότομη κλίση.

129
00:07:54,540 --> 00:07:57,610
Αν δεν είστε εξοικειωμένοι με τον πολυμεταβλητό λογισμό και θέλετε να μάθετε περισσότερα, 

130
00:07:57,610 --> 00:08:00,340
δείτε κάποια από τις εργασίες που έκανα για την Khan Academy σχετικά με το θέμα.

131
00:08:00,860 --> 00:08:04,403
Ειλικρινά όμως, το μόνο που έχει σημασία για εσάς και για μένα αυτή τη στιγμή 

132
00:08:04,403 --> 00:08:07,765
είναι ότι κατ' αρχήν υπάρχει ένας τρόπος να υπολογιστεί αυτό το διάνυσμα, 

133
00:08:07,765 --> 00:08:11,263
αυτό το διάνυσμα που σας λέει ποια είναι η κατεύθυνση της κατηφόρας και πόσο 

134
00:08:11,263 --> 00:08:11,900
απότομη είναι.

135
00:08:12,400 --> 00:08:14,223
Θα είσαι εντάξει αν αυτό είναι το μόνο που ξέρεις 

136
00:08:14,223 --> 00:08:16,120
και δεν είσαι απόλυτα σίγουρος για τις λεπτομέρειες.

137
00:08:17,200 --> 00:08:20,307
Αν μπορείτε να το βρείτε αυτό, ο αλγόριθμος για την ελαχιστοποίηση της 

138
00:08:20,307 --> 00:08:22,932
συνάρτησης είναι να υπολογίσετε αυτή την κατεύθυνση κλίσης, 

139
00:08:22,932 --> 00:08:26,740
στη συνέχεια να κάνετε ένα μικρό βήμα προς τα κάτω και να το επαναλάβετε ξανά και ξανά.

140
00:08:27,700 --> 00:08:32,820
Είναι η ίδια βασική ιδέα για μια συνάρτηση που έχει 13.000 εισόδους αντί για 2 εισόδους.

141
00:08:33,400 --> 00:08:36,027
Φανταστείτε να οργανώσετε και τα 13.000 βάρη και 

142
00:08:36,027 --> 00:08:39,460
προκαταλήψεις του δικτύου μας σε ένα γιγαντιαίο διάνυσμα στήλης.

143
00:08:40,140 --> 00:08:43,940
Η αρνητική κλίση της συνάρτησης κόστους είναι απλώς ένα διάνυσμα, 

144
00:08:43,940 --> 00:08:48,719
είναι κάποια κατεύθυνση μέσα σε αυτόν τον εξωφρενικά τεράστιο χώρο εισόδου που σας 

145
00:08:48,719 --> 00:08:53,555
λέει ποιες ωθήσεις σε όλους αυτούς τους αριθμούς θα προκαλέσουν την ταχύτερη μείωση 

146
00:08:53,555 --> 00:08:54,880
της συνάρτησης κόστους.

147
00:08:55,640 --> 00:08:58,553
Και φυσικά, με την ειδικά σχεδιασμένη συνάρτηση κόστους, 

148
00:08:58,553 --> 00:09:02,335
η αλλαγή των βαρών και των προκαταλήψεων για τη μείωσή της σημαίνει ότι η 

149
00:09:02,335 --> 00:09:06,117
έξοδος του δικτύου σε κάθε κομμάτι δεδομένων εκπαίδευσης μοιάζει λιγότερο 

150
00:09:06,117 --> 00:09:09,951
με μια τυχαία σειρά 10 τιμών και περισσότερο με μια πραγματική απόφαση που 

151
00:09:09,951 --> 00:09:10,820
θέλουμε να λάβει.

152
00:09:11,440 --> 00:09:14,671
Είναι σημαντικό να θυμάστε ότι αυτή η συνάρτηση κόστους περιλαμβάνει 

153
00:09:14,671 --> 00:09:18,276
ένα μέσο όρο για όλα τα δεδομένα εκπαίδευσης, οπότε αν την ελαχιστοποιήσετε, 

154
00:09:18,276 --> 00:09:21,180
σημαίνει ότι η απόδοση είναι καλύτερη σε όλα αυτά τα δείγματα.

155
00:09:23,820 --> 00:09:26,742
Ο αλγόριθμος για τον αποτελεσματικό υπολογισμό αυτής της κλίσης, 

156
00:09:26,742 --> 00:09:30,788
ο οποίος είναι ουσιαστικά η καρδιά του τρόπου με τον οποίο ένα νευρωνικό δίκτυο μαθαίνει, 

157
00:09:30,788 --> 00:09:33,980
ονομάζεται backpropagation, και γι' αυτόν θα μιλήσω στο επόμενο βίντεο.

158
00:09:34,660 --> 00:09:37,689
Εκεί, θέλω πραγματικά να αφιερώσω χρόνο για να εξετάσω τι ακριβώς 

159
00:09:37,689 --> 00:09:41,729
συμβαίνει σε κάθε βάρος και προκατάληψη για ένα δεδομένο κομμάτι δεδομένων εκπαίδευσης, 

160
00:09:41,729 --> 00:09:44,712
προσπαθώντας να δώσω μια διαισθητική αίσθηση για το τι συμβαίνει 

161
00:09:44,712 --> 00:09:47,100
πέρα από το σωρό των σχετικών υπολογισμών και τύπων.

162
00:09:47,780 --> 00:09:50,206
Εδώ και τώρα, το κύριο πράγμα που θέλω να ξέρετε, 

163
00:09:50,206 --> 00:09:52,536
ανεξάρτητα από τις λεπτομέρειες της υλοποίησης, 

164
00:09:52,536 --> 00:09:56,127
είναι ότι αυτό που εννοούμε όταν μιλάμε για ένα δίκτυο που μαθαίνει είναι 

165
00:09:56,127 --> 00:09:58,360
ότι απλώς ελαχιστοποιεί μια συνάρτηση κόστους.

166
00:09:59,300 --> 00:10:02,248
Και παρατηρήστε, μια συνέπεια αυτού είναι ότι είναι σημαντικό αυτή 

167
00:10:02,248 --> 00:10:04,492
η συνάρτηση κόστους να έχει μια ωραία ομαλή έξοδο, 

168
00:10:04,492 --> 00:10:08,100
ώστε να μπορούμε να βρούμε ένα τοπικό ελάχιστο κάνοντας μικρά βήματα προς τα κάτω.

169
00:10:09,260 --> 00:10:12,326
Αυτός είναι ο λόγος, παρεμπιπτόντως, για τον οποίο οι τεχνητοί 

170
00:10:12,326 --> 00:10:14,954
νευρώνες έχουν συνεχώς μεταβαλλόμενες ενεργοποιήσεις, 

171
00:10:14,954 --> 00:10:19,140
αντί να είναι απλώς ενεργοί ή ανενεργοί με δυαδικό τρόπο, όπως οι βιολογικοί νευρώνες.

172
00:10:20,220 --> 00:10:23,419
Αυτή η διαδικασία επανειλημμένης ώθησης της εισόδου μιας συνάρτησης 

173
00:10:23,419 --> 00:10:26,760
κατά κάποιο πολλαπλάσιο της αρνητικής κλίσης ονομάζεται κάθοδος κλίσης.

174
00:10:27,300 --> 00:10:30,777
Είναι ένας τρόπος σύγκλισης προς κάποιο τοπικό ελάχιστο μιας συνάρτησης κόστους, 

175
00:10:30,777 --> 00:10:32,580
ουσιαστικά μια κοιλάδα σε αυτό το γράφημα.

176
00:10:33,440 --> 00:10:36,815
Εξακολουθώ να δείχνω την εικόνα μιας συνάρτησης με δύο εισόδους, φυσικά, 

177
00:10:36,815 --> 00:10:40,468
επειδή τα σπρωξίματα σε έναν χώρο εισόδου 13.000 διαστάσεων είναι λίγο δύσκολο 

178
00:10:40,468 --> 00:10:44,260
να τα καταλάβετε, αλλά υπάρχει ένας ωραίος μη-χωρικός τρόπος να το σκεφτείτε αυτό.

179
00:10:45,080 --> 00:10:48,440
Κάθε συνιστώσα της αρνητικής κλίσης μας λέει δύο πράγματα.

180
00:10:49,060 --> 00:10:51,952
Το πρόσημο, φυσικά, μας λέει αν η αντίστοιχη συνιστώσα του 

181
00:10:51,952 --> 00:10:55,140
διανύσματος εισόδου πρέπει να ωθηθεί προς τα πάνω ή προς τα κάτω.

182
00:10:55,800 --> 00:10:59,232
Όμως, το σημαντικότερο είναι ότι τα σχετικά μεγέθη όλων αυτών 

183
00:10:59,232 --> 00:11:02,720
των συνιστωσών σας λένε ποιες αλλαγές έχουν μεγαλύτερη σημασία.

184
00:11:05,220 --> 00:11:08,964
Βλέπετε, στο δίκτυό μας, μια προσαρμογή σε ένα από τα βάρη μπορεί να έχει πολύ 

185
00:11:08,964 --> 00:11:13,040
μεγαλύτερο αντίκτυπο στη συνάρτηση κόστους από ό,τι η προσαρμογή σε κάποιο άλλο βάρος.

186
00:11:14,800 --> 00:11:16,705
Ορισμένες από αυτές τις συνδέσεις έχουν μεγαλύτερη 

187
00:11:16,705 --> 00:11:18,200
σημασία για τα δεδομένα εκπαίδευσής μας.

188
00:11:19,320 --> 00:11:23,363
Έτσι, ένας τρόπος με τον οποίο μπορείτε να σκεφτείτε αυτό το διάνυσμα κλίσης της 

189
00:11:23,363 --> 00:11:27,657
τεράστιας συνάρτησης κόστους είναι ότι κωδικοποιεί τη σχετική σημασία κάθε βάρους και 

190
00:11:27,657 --> 00:11:31,900
προκατάληψης, δηλαδή ποια από αυτές τις αλλαγές θα φέρει το μεγαλύτερο όφελος για το 

191
00:11:31,900 --> 00:11:32,400
χρήμα σας.

192
00:11:33,620 --> 00:11:36,640
Αυτός είναι πραγματικά ένας άλλος τρόπος σκέψης για την κατεύθυνση.

193
00:11:37,100 --> 00:11:41,277
Για να πάρουμε ένα απλούστερο παράδειγμα, αν έχετε κάποια συνάρτηση με δύο μεταβλητές 

194
00:11:41,277 --> 00:11:45,600
ως είσοδο, και υπολογίσετε ότι η κλίση της σε κάποιο συγκεκριμένο σημείο βγαίνει ως 3,1, 

195
00:11:45,600 --> 00:11:49,971
τότε από τη μία πλευρά μπορείτε να το ερμηνεύσετε αυτό ως κάτι που λέει ότι όταν στέκεστε 

196
00:11:49,971 --> 00:11:54,245
σε αυτή την είσοδο, η κίνηση προς αυτή την κατεύθυνση αυξάνει τη συνάρτηση πιο γρήγορα, 

197
00:11:54,245 --> 00:11:58,325
ότι όταν κάνετε τη γραφική παράσταση της συνάρτησης πάνω από το επίπεδο των σημείων 

198
00:11:58,325 --> 00:12:02,260
εισόδου, αυτό το διάνυσμα είναι αυτό που σας δίνει την ευθεία ανοδική κατεύθυνση.

199
00:12:02,860 --> 00:12:06,169
Αλλά ένας άλλος τρόπος για να το διαβάσετε αυτό είναι να πείτε ότι οι 

200
00:12:06,169 --> 00:12:09,620
αλλαγές σε αυτή την πρώτη μεταβλητή έχουν 3 φορές μεγαλύτερη σημασία από 

201
00:12:09,620 --> 00:12:13,685
τις αλλαγές στη δεύτερη μεταβλητή, ότι τουλάχιστον στη γειτονιά της σχετικής εισόδου, 

202
00:12:13,685 --> 00:12:16,900
η μετατόπιση της τιμής x έχει πολύ μεγαλύτερη αξία για το χρήμα σας.

203
00:12:19,880 --> 00:12:22,340
Ας μεγεθύνουμε και ας συνοψίσουμε πού βρισκόμαστε μέχρι στιγμής.

204
00:12:22,840 --> 00:12:26,914
Το ίδιο το δίκτυο είναι αυτή η συνάρτηση με 784 εισόδους και 10 εξόδους, 

205
00:12:26,914 --> 00:12:30,040
που ορίζεται με βάση όλα αυτά τα σταθμισμένα αθροίσματα.

206
00:12:30,640 --> 00:12:33,680
Η συνάρτηση κόστους είναι ένα στρώμα πολυπλοκότητας πάνω από αυτό.

207
00:12:33,980 --> 00:12:37,795
Λαμβάνει τα 13.000 βάρη και τις προκαταλήψεις ως δεδομένα εισόδου και 

208
00:12:37,795 --> 00:12:41,720
παράγει ένα ενιαίο μέτρο αθλιότητας με βάση τα παραδείγματα εκπαίδευσης.

209
00:12:42,440 --> 00:12:46,900
Και η κλίση της συνάρτησης κόστους είναι ένα ακόμη επίπεδο πολυπλοκότητας.

210
00:12:47,360 --> 00:12:50,898
Μας λέει ποιες αλλαγές σε όλα αυτά τα βάρη και τις προκαταλήψεις προκαλούν 

211
00:12:50,898 --> 00:12:53,445
την ταχύτερη αλλαγή στην τιμή της συνάρτησης κόστους, 

212
00:12:53,445 --> 00:12:56,983
κάτι που μπορείτε να ερμηνεύσετε ως το ποιες αλλαγές σε ποια βάρη έχουν τη 

213
00:12:56,983 --> 00:12:57,880
μεγαλύτερη σημασία.

214
00:13:02,560 --> 00:13:06,138
Έτσι, όταν αρχικοποιείτε το δίκτυο με τυχαία βάρη και προκαταλήψεις και τα 

215
00:13:06,138 --> 00:13:09,526
προσαρμόζετε πολλές φορές με βάση αυτή τη διαδικασία βαθμωτής καθόδου, 

216
00:13:09,526 --> 00:13:13,200
πόσο καλά αποδίδει στην πραγματικότητα σε εικόνες που δεν έχει δει ποτέ πριν;

217
00:13:14,100 --> 00:13:18,620
Αυτό που περιέγραψα εδώ, με τα δύο κρυφά στρώματα των 16 νευρώνων το καθένα, 

218
00:13:18,620 --> 00:13:22,319
που επιλέχθηκαν κυρίως για αισθητικούς λόγους, δεν είναι κακό, 

219
00:13:22,319 --> 00:13:25,960
ταξινομώντας σωστά περίπου το 96% των νέων εικόνων που βλέπει.

220
00:13:26,680 --> 00:13:30,268
Και ειλικρινά, αν δείτε μερικά από τα παραδείγματα στα οποία τα κάνει θάλασσα, 

221
00:13:30,268 --> 00:13:32,540
αισθάνεστε υποχρεωμένοι να το αφήσετε λίγο χαλαρό.

222
00:13:36,220 --> 00:13:40,253
Τώρα, αν παίξετε με τη δομή του κρυμμένου στρώματος και κάνετε μερικές βελτιώσεις, 

223
00:13:40,253 --> 00:13:41,760
μπορείτε να το φτάσετε στο 98%.

224
00:13:41,760 --> 00:13:42,720
Και αυτό είναι πολύ καλό!

225
00:13:43,020 --> 00:13:46,629
Δεν είναι το καλύτερο, σίγουρα μπορείτε να έχετε καλύτερες επιδόσεις αν 

226
00:13:46,629 --> 00:13:49,036
γίνετε πιο εξελιγμένοι από αυτό το απλό δίκτυο, 

227
00:13:49,036 --> 00:13:51,894
αλλά δεδομένου του πόσο τρομακτικό είναι το αρχικό έργο, 

228
00:13:51,894 --> 00:13:55,654
νομίζω ότι υπάρχει κάτι απίστευτο στο ότι οποιοδήποτε δίκτυο τα καταφέρνει 

229
00:13:55,654 --> 00:13:59,163
τόσο καλά σε εικόνες που δεν έχει ξαναδεί, δεδομένου ότι ποτέ δεν του 

230
00:13:59,163 --> 00:14:01,420
είπαμε συγκεκριμένα για ποια μοτίβα να ψάξει.

231
00:14:02,560 --> 00:14:06,246
Αρχικά, ο τρόπος με τον οποίο υποκίνησα αυτή τη δομή ήταν περιγράφοντας μια ελπίδα που 

232
00:14:06,246 --> 00:14:09,891
θα μπορούσαμε να έχουμε, ότι το δεύτερο στρώμα θα μπορούσε να εντοπίσει μικρές άκρες, 

233
00:14:09,891 --> 00:14:13,238
ότι το τρίτο στρώμα θα συνέθετε αυτές τις άκρες για να αναγνωρίσει βρόχους και 

234
00:14:13,238 --> 00:14:16,925
μεγαλύτερες γραμμές, και ότι αυτές θα μπορούσαν να συναρμολογηθούν για να αναγνωρίσουν 

235
00:14:16,925 --> 00:14:17,180
ψηφία.

236
00:14:17,960 --> 00:14:20,400
Αυτό κάνει λοιπόν το δίκτυό μας στην πραγματικότητα;

237
00:14:21,080 --> 00:14:24,400
Λοιπόν, για αυτό τουλάχιστον, καθόλου.

238
00:14:24,820 --> 00:14:27,980
Θυμάστε πώς στο προηγούμενο βίντεο εξετάσαμε πώς τα βάρη των συνδέσεων 

239
00:14:27,980 --> 00:14:31,095
από όλους τους νευρώνες του πρώτου στρώματος σε έναν δεδομένο νευρώνα 

240
00:14:31,095 --> 00:14:34,255
του δεύτερου στρώματος μπορούν να απεικονιστούν ως ένα δεδομένο μοτίβο 

241
00:14:34,255 --> 00:14:37,060
εικονοστοιχείων που ο νευρώνας του δεύτερου στρώματος λαμβάνει;

242
00:14:37,780 --> 00:14:43,037
Λοιπόν, όταν το κάνουμε αυτό για τα βάρη που σχετίζονται με αυτές τις μεταβάσεις, 

243
00:14:43,037 --> 00:14:48,358
από το πρώτο στρώμα στο επόμενο, αντί να εντοπίσουμε απομονωμένες μικρές ακμές εδώ 

244
00:14:48,358 --> 00:14:53,680
και εκεί, φαίνονται, λοιπόν, σχεδόν τυχαίες, με μερικά πολύ χαλαρά μοτίβα στη μέση.

245
00:14:53,760 --> 00:14:58,728
Φαίνεται ότι στον απίστευτα μεγάλο χώρο των 13.000 διαστάσεων των πιθανών βαρών και 

246
00:14:58,728 --> 00:15:03,104
προκαταλήψεων, το δίκτυό μας βρήκε ένα ευτυχές μικρό τοπικό ελάχιστο που, 

247
00:15:03,104 --> 00:15:06,298
παρά την επιτυχή ταξινόμηση των περισσότερων εικόνων, 

248
00:15:06,298 --> 00:15:08,960
δεν εντοπίζει ακριβώς τα μοτίβα που ελπίζαμε.

249
00:15:09,780 --> 00:15:11,822
Και για να το καταλάβετε αυτό, παρακολουθήστε 

250
00:15:11,822 --> 00:15:13,820
τι συμβαίνει όταν εισάγετε μια τυχαία εικόνα.

251
00:15:14,320 --> 00:15:17,986
Αν το σύστημα ήταν έξυπνο, θα περιμένατε να αισθάνεται αβεβαιότητα, 

252
00:15:17,986 --> 00:15:21,813
ίσως να μην ενεργοποιεί πραγματικά κανέναν από αυτούς τους 10 νευρώνες 

253
00:15:21,813 --> 00:15:24,347
εξόδου ή να τους ενεργοποιεί όλους ομοιόμορφα, 

254
00:15:24,347 --> 00:15:27,960
αλλά αντ' αυτού σας δίνει με αυτοπεποίθηση κάποια ανόητη απάντηση, 

255
00:15:27,960 --> 00:15:31,787
σαν να αισθάνεται τόσο σίγουρο ότι αυτός ο τυχαίος θόρυβος είναι 5 όσο 

256
00:15:31,787 --> 00:15:34,160
και ότι μια πραγματική εικόνα του 5 είναι 5.

257
00:15:34,540 --> 00:15:37,502
Διατυπωμένο διαφορετικά, ακόμη και αν αυτό το δίκτυο μπορεί να 

258
00:15:37,502 --> 00:15:40,700
αναγνωρίσει αρκετά καλά τα ψηφία, δεν έχει ιδέα πώς να τα σχεδιάσει.

259
00:15:41,420 --> 00:15:43,272
Πολλά από αυτά οφείλονται στο γεγονός ότι είναι 

260
00:15:43,272 --> 00:15:45,240
μια τόσο αυστηρά περιορισμένη εκπαιδευτική ρύθμιση.

261
00:15:45,880 --> 00:15:47,740
Θέλω να πω, βάλτε τον εαυτό σας στη θέση του δικτύου εδώ.

262
00:15:48,140 --> 00:15:51,274
Από τη σκοπιά του, ολόκληρο το σύμπαν δεν αποτελείται από τίποτε άλλο 

263
00:15:51,274 --> 00:15:55,035
παρά από σαφώς καθορισμένα ακίνητα ψηφία που βρίσκονται σε ένα μικροσκοπικό πλέγμα, 

264
00:15:55,035 --> 00:15:58,214
και η συνάρτηση κόστους του δεν του έδωσε ποτέ κανένα κίνητρο να είναι 

265
00:15:58,214 --> 00:16:01,080
οτιδήποτε άλλο εκτός από απόλυτα σίγουρος για τις αποφάσεις του.

266
00:16:02,120 --> 00:16:05,913
Έτσι, με αυτή την εικόνα για το τι πραγματικά κάνουν οι νευρώνες του δεύτερου στρώματος, 

267
00:16:05,913 --> 00:16:08,556
ίσως αναρωτηθείτε γιατί θα εισήγαγα αυτό το δίκτυο με κίνητρο 

268
00:16:08,556 --> 00:16:09,920
την ανίχνευση ακμών και μοτίβων.

269
00:16:09,920 --> 00:16:12,300
Θέλω να πω, αυτό δεν είναι καθόλου αυτό που καταλήγει να κάνει.

270
00:16:13,380 --> 00:16:17,180
Λοιπόν, αυτό δεν πρόκειται να είναι ο τελικός μας στόχος, αλλά ένα σημείο εκκίνησης.

271
00:16:17,640 --> 00:16:21,031
Ειλικρινά, πρόκειται για παλιά τεχνολογία, το είδος που ερευνήθηκε στις 

272
00:16:21,031 --> 00:16:24,187
δεκαετίες του '80 και του '90, και πρέπει να την καταλάβετε προτού 

273
00:16:24,187 --> 00:16:26,731
κατανοήσετε τις πιο λεπτομερείς σύγχρονες παραλλαγές, 

274
00:16:26,731 --> 00:16:29,840
και είναι σαφώς ικανή να επιλύσει μερικά ενδιαφέροντα προβλήματα, 

275
00:16:29,840 --> 00:16:33,326
αλλά όσο περισσότερο ψάχνετε τι πραγματικά κάνουν αυτά τα κρυφά στρώματα, 

276
00:16:33,326 --> 00:16:34,740
τόσο λιγότερο έξυπνη φαίνεται.

277
00:16:38,480 --> 00:16:42,484
Μετατοπίζοντας για λίγο την εστίαση από το πώς μαθαίνουν τα δίκτυα στο πώς μαθαίνετε 

278
00:16:42,484 --> 00:16:46,300
εσείς, αυτό θα συμβεί μόνο αν ασχοληθείτε ενεργά με το υλικό εδώ με κάποιο τρόπο.

279
00:16:47,060 --> 00:16:50,442
Ένα πολύ απλό πράγμα που θέλω να κάνετε είναι να σταματήσετε τώρα και 

280
00:16:50,442 --> 00:16:53,921
να σκεφτείτε βαθιά για μια στιγμή ποιες αλλαγές θα μπορούσατε να κάνετε 

281
00:16:53,921 --> 00:16:57,545
σε αυτό το σύστημα και στον τρόπο με τον οποίο αντιλαμβάνεται τις εικόνες, 

282
00:16:57,545 --> 00:17:00,880
αν θέλατε να εντοπίζει καλύτερα πράγματα όπως οι άκρες και τα μοτίβα.

283
00:17:01,480 --> 00:17:04,002
Αλλά για να ασχοληθείτε πραγματικά με την ύλη, 

284
00:17:04,002 --> 00:17:07,812
συνιστώ ανεπιφύλακτα το βιβλίο του Michael Nielsen για τη βαθιά μάθηση 

285
00:17:07,812 --> 00:17:09,099
και τα νευρωνικά δίκτυα.

286
00:17:09,680 --> 00:17:12,616
Σε αυτό, μπορείτε να βρείτε τον κώδικα και τα δεδομένα που μπορείτε 

287
00:17:12,616 --> 00:17:15,207
να κατεβάσετε και να παίξετε με αυτό ακριβώς το παράδειγμα, 

288
00:17:15,207 --> 00:17:18,359
και το βιβλίο θα σας καθοδηγήσει βήμα προς βήμα τι κάνει αυτός ο κώδικας.

289
00:17:19,300 --> 00:17:23,121
Το φοβερό είναι ότι αυτό το βιβλίο είναι δωρεάν και διαθέσιμο στο κοινό, οπότε, 

290
00:17:23,121 --> 00:17:27,277
αν έχετε κάτι από αυτό, σκεφτείτε να κάνετε μαζί μου μια δωρεά για τις προσπάθειες της 

291
00:17:27,277 --> 00:17:27,660
Nielsen.

292
00:17:27,660 --> 00:17:31,339
Έχω επίσης συνδέσει μερικές άλλες πηγές που μου αρέσουν πολύ στην περιγραφή, 

293
00:17:31,339 --> 00:17:34,301
συμπεριλαμβανομένης της εκπληκτικής και όμορφης ανάρτησης στο 

294
00:17:34,301 --> 00:17:36,500
blog του Chris Ola και των άρθρων στο Distill.

295
00:17:38,280 --> 00:17:40,662
Για να κλείσουμε τα πράγματα εδώ για τα τελευταία λεπτά, 

296
00:17:40,662 --> 00:17:43,880
θέλω να επιστρέψω σε ένα απόσπασμα της συνέντευξης που είχα με τη Leisha Lee.

297
00:17:44,300 --> 00:17:47,720
Ίσως τη θυμάστε από το τελευταίο βίντεο, έκανε το διδακτορικό της στη βαθιά μάθηση.

298
00:17:48,300 --> 00:17:50,820
Σε αυτό το μικρό απόσπασμα μιλάει για δύο πρόσφατες εργασίες 

299
00:17:50,820 --> 00:17:53,465
που εμβαθύνουν πραγματικά στο πώς μαθαίνουν στην πραγματικότητα 

300
00:17:53,465 --> 00:17:55,780
ορισμένα από τα πιο σύγχρονα δίκτυα αναγνώρισης εικόνας.

301
00:17:56,120 --> 00:17:59,309
Για να ξεκινήσουμε τη συζήτηση, η πρώτη εργασία πήρε ένα από αυτά τα 

302
00:17:59,309 --> 00:18:02,499
ιδιαίτερα βαθιά νευρωνικά δίκτυα που είναι πολύ καλά στην αναγνώριση 

303
00:18:02,499 --> 00:18:06,336
εικόνων και αντί να το εκπαιδεύσει σε ένα σύνολο δεδομένων με κατάλληλες ετικέτες, 

304
00:18:06,336 --> 00:18:08,740
ανακάτεψε όλες τις ετικέτες πριν από την εκπαίδευση.

305
00:18:09,480 --> 00:18:12,744
Προφανώς, η ακρίβεια δοκιμής εδώ δεν ήταν καλύτερη από την τυχαία, 

306
00:18:12,744 --> 00:18:16,446
αφού όλα είναι τυχαία επισημασμένα, αλλά ήταν ακόμα σε θέση να επιτύχει την 

307
00:18:16,446 --> 00:18:20,392
ίδια ακρίβεια εκπαίδευσης με αυτή που θα είχατε σε ένα σωστά επισημασμένο σύνολο 

308
00:18:20,392 --> 00:18:20,880
δεδομένων.

309
00:18:21,600 --> 00:18:25,261
Βασικά, τα εκατομμύρια βάρη για αυτό το συγκεκριμένο δίκτυο ήταν αρκετά 

310
00:18:25,261 --> 00:18:28,974
για να απομνημονεύσει τα τυχαία δεδομένα, γεγονός που εγείρει το ερώτημα 

311
00:18:28,974 --> 00:18:32,432
για το αν η ελαχιστοποίηση αυτής της συνάρτησης κόστους αντιστοιχεί 

312
00:18:32,432 --> 00:18:36,400
πραγματικά σε οποιοδήποτε είδος δομής στην εικόνα, ή είναι απλά απομνημόνευση;

313
00:18:51,440 --> 00:18:56,830
Αν κοιτάξετε αυτή την καμπύλη ακρίβειας, αν απλά εκπαιδεύατε σε ένα τυχαίο 

314
00:18:56,830 --> 00:19:02,724
σύνολο δεδομένων, αυτή η καμπύλη θα κατέβαινε πολύ αργά με σχεδόν γραμμικό τρόπο, 

315
00:19:02,724 --> 00:19:07,755
οπότε πραγματικά αγωνίζεστε να βρείτε τα τοπικά ελάχιστα των πιθανών, 

316
00:19:07,755 --> 00:19:12,140
ξέρετε, των σωστών βαρών που θα σας έδιναν αυτή την ακρίβεια.

317
00:19:12,240 --> 00:19:15,962
Ενώ αν εκπαιδεύεστε πραγματικά σε ένα δομημένο σύνολο δεδομένων, 

318
00:19:15,962 --> 00:19:19,055
που έχει τις σωστές ετικέτες, παίζετε λίγο στην αρχή, 

319
00:19:19,055 --> 00:19:23,867
αλλά στη συνέχεια πέφτετε πολύ γρήγορα για να φτάσετε σε αυτό το επίπεδο ακρίβειας, 

320
00:19:23,867 --> 00:19:28,220
και έτσι κατά κάποιο τρόπο ήταν ευκολότερο να βρείτε αυτό το τοπικό μέγιστο.

321
00:19:28,540 --> 00:19:32,594
Και αυτό που ήταν επίσης ενδιαφέρον σε αυτό είναι ότι φέρνει στο φως μια άλλη 

322
00:19:32,594 --> 00:19:37,064
εργασία που έγινε πριν από μερικά χρόνια, η οποία έχει πολύ περισσότερες απλοποιήσεις 

323
00:19:37,064 --> 00:19:41,118
σχετικά με τα στρώματα του δικτύου, αλλά ένα από τα αποτελέσματα έλεγε ότι αν 

324
00:19:41,118 --> 00:19:45,276
κοιτάξετε το τοπίο βελτιστοποίησης, τα τοπικά ελάχιστα που τείνουν να μαθαίνουν 

325
00:19:45,276 --> 00:19:49,434
αυτά τα δίκτυα είναι στην πραγματικότητα ίσης ποιότητας, οπότε κατά μία έννοια, 

326
00:19:49,434 --> 00:19:53,748
αν το σύνολο δεδομένων σας είναι δομημένο, θα πρέπει να μπορείτε να το βρείτε πολύ 

327
00:19:53,748 --> 00:19:54,320
πιο εύκολα.

328
00:19:58,160 --> 00:20:01,180
Ευχαριστώ, όπως πάντα, όσους από εσάς υποστηρίζετε στο Patreon.

329
00:20:01,520 --> 00:20:04,298
Έχω ξαναπεί πόσο σημαντικό είναι το Patreon, αλλά 

330
00:20:04,298 --> 00:20:06,800
αυτά τα βίντεο δεν θα ήταν εφικτά χωρίς εσάς.

331
00:20:07,460 --> 00:20:09,995
Θέλω επίσης να ευχαριστήσω ιδιαίτερα την εταιρεία VC Amplify 

332
00:20:09,995 --> 00:20:12,780
Partners για την υποστήριξή της σε αυτά τα πρώτα βίντεο της σειράς.

