1
00:00:00,000 --> 00:00:07,240
শেষ ভিডিওতে আমি একটি নিউরাল নেটওয়ার্কের গঠন তুলে ধরেছি।

2
00:00:07,240 --> 00:00:11,560
আমি এখানে একটি দ্রুত সংক্ষিপ্ত বিবরণ দেব যাতে এটি আমাদের মনে তাজা

3
00:00:11,560 --> 00:00:13,160
হয়, এবং তারপরে এই ভিডিওটির জন্য আমার দুটি প্রধান লক্ষ্য রয়েছে৷

4
00:00:13,160 --> 00:00:17,960
প্রথমটি হল গ্রেডিয়েন্ট ডিসেন্টের ধারণাটি প্রবর্তন করা, যা শুধুমাত্র নিউরাল নেটওয়ার্কগুলি

5
00:00:17,960 --> 00:00:20,800
কীভাবে শেখে তা নয়, অন্যান্য অনেক মেশিন লার্নিংও কীভাবে কাজ করে।

6
00:00:20,800 --> 00:00:25,160
তারপরে এর পরে আমরা এই নির্দিষ্ট নেটওয়ার্কটি কীভাবে কাজ করে এবং নিউরনের সেই

7
00:00:25,160 --> 00:00:29,560
লুকানো স্তরগুলি কী সন্ধান করে তা নিয়ে আমরা আরও কিছুটা খনন করব।

8
00:00:29,560 --> 00:00:34,680
একটি অনুস্মারক হিসাবে, এখানে আমাদের লক্ষ্য হ'ল হস্তলিখিত

9
00:00:34,680 --> 00:00:37,080
অঙ্কের স্বীকৃতির ক্লাসিক উদাহরণ, নিউরাল নেটওয়ার্কের হ্যালো ওয়ার্ল্ড।

10
00:00:37,080 --> 00:00:42,160
এই সংখ্যাগুলি একটি 28x28 পিক্সেল গ্রিডে রেন্ডার করা হয়, প্রতিটি

11
00:00:42,160 --> 00:00:44,260
পিক্সেল 0 এবং 1 এর মধ্যে কিছু গ্রেস্কেল মান সহ।

12
00:00:44,260 --> 00:00:51,400
এগুলিই নেটওয়ার্কের ইনপুট স্তরে 784 নিউরনের সক্রিয়তা নির্ধারণ করে।

13
00:00:51,400 --> 00:00:56,880
নিম্নলিখিত স্তরগুলিতে প্রতিটি নিউরনের জন্য সক্রিয়করণ পূর্ববর্তী স্তরের সমস্ত সক্রিয়করণের ওজনযুক্ত

14
00:00:56,880 --> 00:01:02,300
যোগফলের উপর ভিত্তি করে, সাথে কিছু বিশেষ সংখ্যাকে বায়াস বলে।

15
00:01:02,300 --> 00:01:07,480
আপনি সেই যোগফলটি অন্য কোনো ফাংশনের সাথে রচনা করেছেন, যেমন

16
00:01:07,480 --> 00:01:09,640
সিগমায়েড স্কুইশিফিকেশন, বা একটি ReLU, যেভাবে আমি শেষ ভিডিওটি দিয়েছিলাম।

17
00:01:09,640 --> 00:01:14,960
মোট, প্রতিটি 16টি নিউরন সহ দুটি লুকানো স্তরের কিছুটা স্বেচ্ছাচারী পছন্দ দেওয়া

18
00:01:14,960 --> 00:01:20,940
হলে, নেটওয়ার্কে প্রায় 13,000 ওজন এবং পক্ষপাত রয়েছে যা আমরা সামঞ্জস্য করতে

19
00:01:20,940 --> 00:01:25,320
পারি এবং এই মানগুলিই নির্ধারণ করে যে নেটওয়ার্কটি আসলে কী করে।

20
00:01:25,320 --> 00:01:29,800
এবং আমরা যখন বলি যে এই নেটওয়ার্কটি একটি প্রদত্ত সংখ্যাকে শ্রেণিবদ্ধ করে তখন আমরা যা বোঝায়

21
00:01:29,800 --> 00:01:34,080
তা হল চূড়ান্ত স্তরের সেই 10টি নিউরনের মধ্যে সবচেয়ে উজ্জ্বলটি সেই অঙ্কের সাথে মিলে যায়।

22
00:01:34,080 --> 00:01:39,240
এবং মনে রাখবেন, স্তরযুক্ত কাঠামোর জন্য আমাদের মনে যে অনুপ্রেরণা

23
00:01:39,240 --> 00:01:43,920
ছিল তা হল যে সম্ভবত দ্বিতীয় স্তরটি প্রান্তে উঠতে পারে,

24
00:01:43,920 --> 00:01:48,640
তৃতীয় স্তরটি লুপ এবং লাইনের মতো প্যাটার্নগুলিকে বেছে নিতে পারে

25
00:01:48,640 --> 00:01:49,640
এবং শেষটি কেবল সেই নিদর্শনগুলিকে একত্রিত করতে পারে। অঙ্ক চিনতে।

26
00:01:49,640 --> 00:01:52,880
তাই এখানে, আমরা জানবো কিভাবে নেটওয়ার্ক শেখে।

27
00:01:52,880 --> 00:01:56,880
আমরা যা চাই তা হল একটি অ্যালগরিদম যেখানে আপনি এই নেটওয়ার্কটিকে প্রশিক্ষণের ডেটার

28
00:01:56,880 --> 00:02:01,540
একটি সম্পূর্ণ গুচ্ছ দেখাতে পারেন, যা হাতে লেখা অঙ্কের বিভিন্ন চিত্রের একটি গুচ্ছ

29
00:02:01,540 --> 00:02:06,360
আকারে আসে, সেই সাথে লেবেলগুলি যা হওয়ার কথা, এবং এটি হবে সেই 13,000

30
00:02:06,360 --> 00:02:10,760
ওজন এবং পক্ষপাতগুলি সামঞ্জস্য করুন যাতে প্রশিক্ষণ ডেটাতে এর কার্যকারিতা উন্নত করা যায়।

31
00:02:10,760 --> 00:02:15,540
আশা করি এই স্তরযুক্ত কাঠামোর অর্থ হবে যে এটি যা

32
00:02:15,540 --> 00:02:17,840
শেখে তা সেই প্রশিক্ষণ ডেটার বাইরের চিত্রগুলিতে সাধারণীকরণ করে।

33
00:02:17,840 --> 00:02:22,240
আমরা যেভাবে পরীক্ষা করি তা হল যে আপনি নেটওয়ার্ককে প্রশিক্ষণ দেওয়ার পরে, আপনি এটিকে আরও লেবেলযুক্ত

34
00:02:22,240 --> 00:02:31,160
ডেটা দেখান এবং আপনি দেখতে পান যে এটি সেই নতুন চিত্রগুলিকে কতটা সঠিকভাবে শ্রেণীবদ্ধ করে।

35
00:02:31,160 --> 00:02:34,760
সৌভাগ্যবশত আমাদের জন্য, এবং যা এটিকে শুরু করার জন্য একটি সাধারণ উদাহরণ করে

36
00:02:34,760 --> 00:02:39,520
তোলে তা হল যে MNIST ডাটাবেসের পিছনে থাকা ভাল লোকেরা হাজার হাজার হাতে

37
00:02:39,520 --> 00:02:45,080
লেখা অঙ্কের চিত্রগুলির একটি সংগ্রহ করেছে, প্রতিটিতে তাদের সংখ্যার সাথে লেবেল করা হয়েছে।

38
00:02:45,080 --> 00:02:49,920
এবং একটি মেশিনকে শেখার মতো বর্ণনা করা যতটা উত্তেজক, একবার আপনি এটি কীভাবে কাজ করে তা একবার

39
00:02:49,920 --> 00:02:55,560
দেখলে, এটি কিছু উন্মাদ সাই-ফাই প্রিমাইজের মতো অনেক কম এবং ক্যালকুলাস অনুশীলনের মতো অনেক বেশি অনুভব করে।

40
00:02:55,560 --> 00:03:01,040
আমি বলতে চাচ্ছি, মূলত এটি একটি নির্দিষ্ট ফাংশনের সর্বনিম্ন খুঁজে বের করার জন্য নেমে আসে।

41
00:03:01,040 --> 00:03:06,480
মনে রাখবেন, ধারণাগতভাবে আমরা প্রতিটি নিউরনকে পূর্ববর্তী স্তরের সমস্ত নিউরনের

42
00:03:06,480 --> 00:03:11,440
সাথে সংযুক্ত হিসাবে ভাবছি, এবং এর সক্রিয়করণকে সংজ্ঞায়িত করে ওজনযুক্ত

43
00:03:11,440 --> 00:03:16,400
সমষ্টির ওজনগুলি সেই সংযোগগুলির শক্তির মতো, এবং পক্ষপাত হল কিছু

44
00:03:16,400 --> 00:03:19,780
ইঙ্গিত যে নিউরন সক্রিয় বা নিষ্ক্রিয় হতে থাকে কিনা।

45
00:03:19,780 --> 00:03:23,300
এবং জিনিসগুলি শুরু করার জন্য, আমরা কেবলমাত্র এলোমেলোভাবে

46
00:03:23,300 --> 00:03:25,020
সেই সমস্ত ওজন এবং পক্ষপাতগুলি শুরু করতে যাচ্ছি।

47
00:03:25,020 --> 00:03:29,100
বলা বাহুল্য, এই নেটওয়ার্কটি প্রদত্ত প্রশিক্ষণের উদাহরণে ভয়ঙ্করভাবে পারফর্ম

48
00:03:29,100 --> 00:03:31,180
করতে চলেছে, যেহেতু এটি কেবল এলোমেলো কিছু করছে।

49
00:03:31,180 --> 00:03:36,820
উদাহরণস্বরূপ, আপনি একটি 3 এর এই চিত্রটিতে ফিড করেন এবং আউটপুট স্তরটি কেবল একটি জগাখিচুড়ির মতো দেখায়।

50
00:03:36,820 --> 00:03:43,340
সুতরাং আপনি যা করেন তা হল একটি খরচ ফাংশন সংজ্ঞায়িত করা, কম্পিউটারকে বলার একটি উপায়, না, খারাপ

51
00:03:43,340 --> 00:03:48,940
কম্পিউটার, সেই আউটপুটে সক্রিয়করণ থাকা উচিত যা বেশিরভাগ নিউরনের জন্য 0, কিন্তু এই নিউরনের জন্য 1।

52
00:03:48,980 --> 00:03:51,740
আপনি আমাকে যা দিয়েছেন তা সম্পূর্ণ আবর্জনা।

53
00:03:51,740 --> 00:03:56,740
আরেকটু গাণিতিকভাবে বলতে গেলে, আপনি সেই ট্র্যাশ আউটপুট অ্যাক্টিভেশনগুলির প্রতিটির

54
00:03:56,740 --> 00:04:01,980
মধ্যে পার্থক্যের স্কোয়ার যোগ করুন এবং আপনি যে মানটি পেতে

55
00:04:01,980 --> 00:04:06,020
চান, এবং এটিকে আমরা একটি একক প্রশিক্ষণ উদাহরণের খরচ বলব।

56
00:04:06,020 --> 00:04:12,660
লক্ষ্য করুন যখন নেটওয়ার্ক আত্মবিশ্বাসের সাথে চিত্রটিকে সঠিকভাবে শ্রেণীবদ্ধ করে তখন এই যোগফলটি ছোট, কিন্তু

57
00:04:12,660 --> 00:04:18,820
যখন নেটওয়ার্কটি মনে হয় যে এটি কী করছে তা জানে না তখন এটি বড়।

58
00:04:18,820 --> 00:04:23,860
তাহলে আপনি যা করবেন তা হল আপনার হাতে থাকা

59
00:04:23,860 --> 00:04:27,580
হাজার হাজার প্রশিক্ষণের উদাহরণগুলির সমস্ত গড় খরচ বিবেচনা করুন।

60
00:04:27,580 --> 00:04:32,300
এই গড় খরচ হল নেটওয়ার্ক কতটা খারাপ, এবং কম্পিউটার

61
00:04:32,300 --> 00:04:33,300
কতটা খারাপ বোধ করা উচিত তার জন্য আমাদের পরিমাপ।

62
00:04:33,300 --> 00:04:35,300
এবং এটি একটি জটিল জিনিস।

63
00:04:35,300 --> 00:04:40,380
মনে রাখবেন কিভাবে নেটওয়ার্ক নিজেই মূলত একটি ফাংশন ছিল, যেটি ইনপুট হিসাবে 784

64
00:04:40,380 --> 00:04:46,100
সংখ্যা নেয়, পিক্সেল মান, এবং তার আউটপুট হিসাবে 10টি সংখ্যা বের করে

65
00:04:46,100 --> 00:04:49,700
দেয় এবং এক অর্থে এটি এই সমস্ত ওজন এবং পক্ষপাত দ্বারা প্যারামিটারাইজড?

66
00:04:49,700 --> 00:04:53,340
খরচ ফাংশন যে উপরে জটিলতা একটি স্তর.

67
00:04:53,340 --> 00:04:59,140
এটি সেই 13,000 বা তার বেশি ওজন এবং পক্ষপাতগুলিকে ইনপুট হিসাবে নেয় এবং সেই ওজন এবং

68
00:04:59,140 --> 00:05:04,620
পক্ষপাতগুলি কতটা খারাপ তা বর্ণনা করে একটি একক সংখ্যা বের করে দেয় এবং এটি যেভাবে

69
00:05:04,620 --> 00:05:09,140
সংজ্ঞায়িত করা হয়েছে তা নির্ভর করে সমস্ত হাজার হাজার প্রশিক্ষণ ডেটার উপর নেটওয়ার্কের আচরণের উপর৷

70
00:05:09,140 --> 00:05:12,460
সেটা অনেক ভাবার বিষয়।

71
00:05:12,460 --> 00:05:16,380
কিন্তু শুধু কম্পিউটারকে বলা যে এটি কী একটি বাজে কাজ করছে তা খুব সহায়ক নয়।

72
00:05:16,380 --> 00:05:21,300
আপনি এটিকে বলতে চান কীভাবে সেই ওজন এবং পক্ষপাতগুলি পরিবর্তন করতে হয় যাতে এটি আরও ভাল হয়।

73
00:05:21,300 --> 00:05:25,580
এটিকে সহজ করার জন্য, 13,000 ইনপুট সহ একটি ফাংশন কল্পনা করার জন্য সংগ্রাম করার পরিবর্তে, শুধু একটি

74
00:05:25,580 --> 00:05:31,440
সাধারণ ফাংশন কল্পনা করুন যার একটি ইনপুট হিসাবে একটি সংখ্যা এবং একটি আউটপুট হিসাবে একটি সংখ্যা রয়েছে৷

75
00:05:31,440 --> 00:05:36,420
আপনি কিভাবে একটি ইনপুট খুঁজে পাবেন যা এই ফাংশনের মানকে ছোট করে?

76
00:05:36,420 --> 00:05:41,300
ক্যালকুলাসের শিক্ষার্থীরা জানবে যে আপনি কখনও কখনও সেই ন্যূনতমটি স্পষ্টভাবে বের করতে পারেন,

77
00:05:41,340 --> 00:05:46,620
তবে এটি সত্যিই জটিল ফাংশনের জন্য সবসময় সম্ভব নয়, অবশ্যই আমাদের পাগল

78
00:05:46,620 --> 00:05:51,640
জটিল নিউরাল নেটওয়ার্ক খরচ ফাংশনের জন্য এই পরিস্থিতির 13,000 ইনপুট সংস্করণে নয়।

79
00:05:51,640 --> 00:05:56,820
একটি আরও নমনীয় কৌশল হল যে কোনও ইনপুট থেকে শুরু করা এবং সেই

80
00:05:56,820 --> 00:05:59,860
আউটপুটটিকে কম করার জন্য আপনার কোন দিকে পদক্ষেপ নেওয়া উচিত তা নির্ধারণ করা।

81
00:05:59,860 --> 00:06:05,020
বিশেষ করে, আপনি যেখানে আছেন সেই ফাংশনের ঢালটি যদি বের

82
00:06:05,020 --> 00:06:09,280
করতে পারেন, তাহলে সেই ঢালটি ধনাত্মক হলে বাম দিকে

83
00:06:09,280 --> 00:06:12,720
সরান এবং সেই ঢালটি ঋণাত্মক হলে ইনপুটটি ডানদিকে সরান৷

84
00:06:12,720 --> 00:06:17,040
আপনি যদি এটি বারবার করেন, প্রতিটি বিন্দুতে নতুন ঢাল পরীক্ষা করে

85
00:06:17,040 --> 00:06:20,680
যথাযথ পদক্ষেপ নিচ্ছেন, আপনি ফাংশনের কিছু স্থানীয় ন্যূনতম কাছে যেতে যাচ্ছেন।

86
00:06:20,680 --> 00:06:24,600
এবং এখানে আপনার মনে যে চিত্রটি থাকতে পারে তা হল একটি পাহাড়ের নিচে গড়িয়ে যাওয়া একটি বল।

87
00:06:24,600 --> 00:06:29,380
এবং লক্ষ্য করুন, এমনকি এই সত্যই সরলীকৃত একক ইনপুট ফাংশনের জন্য, আপনি কোন

88
00:06:29,380 --> 00:06:34,220
এলোমেলো ইনপুট থেকে শুরু করবেন তার উপর নির্ভর করে আপনি অনেক সম্ভাব্য

89
00:06:34,220 --> 00:06:38,460
উপত্যকায় অবতরণ করতে পারেন, এবং আপনি যে স্থানীয় ন্যূনতম ভূমিতে অবতরণ করেন

90
00:06:38,460 --> 00:06:39,460
তার কোনও গ্যারান্টি নেই যে সম্ভাব্য সর্বনিম্ন মান হতে চলেছে খরচ ফাংশন.

91
00:06:39,460 --> 00:06:43,180
এটি আমাদের নিউরাল নেটওয়ার্ক ক্ষেত্রেও বহন করবে।

92
00:06:43,180 --> 00:06:48,140
এবং আমি এও চাই যে আপনি লক্ষ্য করুন কিভাবে আপনি যদি আপনার ধাপের আকার

93
00:06:48,140 --> 00:06:52,920
ঢালের সমানুপাতিক করেন, তারপর যখন ঢালটি ন্যূনতম দিকে সমতল হয়, তখন আপনার পদক্ষেপগুলি

94
00:06:52,920 --> 00:06:56,020
ছোট থেকে ছোট হতে থাকে এবং এই ধরনের আপনাকে ওভারশুটিং থেকে সাহায্য করে।

95
00:06:56,020 --> 00:07:01,640
জটিলতা কিছুটা কমিয়ে, পরিবর্তে দুটি ইনপুট এবং একটি আউটপুট সহ একটি ফাংশন কল্পনা করুন।

96
00:07:01,640 --> 00:07:06,360
আপনি ইনপুট স্পেসকে xy-প্লেন এবং খরচ ফাংশনটিকে এটির উপরে

97
00:07:06,360 --> 00:07:09,020
একটি পৃষ্ঠ হিসাবে গ্রাফ করা হিসাবে ভাবতে পারেন।

98
00:07:09,020 --> 00:07:13,600
ফাংশনের ঢাল সম্পর্কে জিজ্ঞাসা করার পরিবর্তে, আপনাকে এই ইনপুট স্পেসে কোন দিকে যেতে

99
00:07:13,600 --> 00:07:19,780
হবে তা জিজ্ঞাসা করতে হবে যাতে ফাংশনের আউটপুট সবচেয়ে দ্রুত হ্রাস পায়।

100
00:07:19,780 --> 00:07:22,340
অন্য কথায়, উতরাই দিক কি?

101
00:07:22,340 --> 00:07:26,740
এবং আবার, সেই পাহাড়ের নিচে একটি বল গড়িয়ে পড়ার কথা ভাবতে সহায়ক।

102
00:07:26,740 --> 00:07:31,920
আপনারা যারা মাল্টিভেরিয়েবল ক্যালকুলাসের সাথে পরিচিত তারা জানেন যে একটি

103
00:07:31,920 --> 00:07:37,460
ফাংশনের গ্রেডিয়েন্ট আপনাকে সবচেয়ে খাড়া আরোহনের দিক নির্দেশ করে, ফাংশনটি

104
00:07:37,460 --> 00:07:39,420
সবচেয়ে দ্রুত বাড়ানোর জন্য আপনাকে কোন দিকে যেতে হবে।

105
00:07:39,420 --> 00:07:43,820
স্বাভাবিকভাবেই যথেষ্ট, সেই গ্রেডিয়েন্টের নেতিবাচক গ্রহণ আপনাকে পদক্ষেপের

106
00:07:43,820 --> 00:07:47,460
দিকনির্দেশ দেয় যা ফাংশনটি সবচেয়ে দ্রুত হ্রাস করে।

107
00:07:47,460 --> 00:07:52,320
তার থেকেও বেশি, এই গ্রেডিয়েন্ট ভেক্টরের দৈর্ঘ্য সেই

108
00:07:52,320 --> 00:07:54,580
খাড়া ঢালটি কতটা খাড়া তার ইঙ্গিত দেয়।

109
00:07:54,580 --> 00:07:58,080
এখন আপনি যদি মাল্টিভেরিয়েবল ক্যালকুলাসের সাথে অপরিচিত হন এবং আরও জানতে চান,

110
00:07:58,080 --> 00:08:01,100
তবে খান একাডেমির জন্য আমি এই বিষয়ে কিছু কাজ করেছি তা দেখুন।

111
00:08:01,100 --> 00:08:05,680
সত্যি বলতে কি, এই মুহূর্তে আপনার এবং আমার জন্য যা গুরুত্বপূর্ণ তা

112
00:08:05,680 --> 00:08:10,440
হল নীতিগতভাবে এই ভেক্টরটি গণনা করার একটি উপায় রয়েছে, এই ভেক্টর

113
00:08:10,440 --> 00:08:12,040
যা আপনাকে বলে যে উতরাই দিকটি কী এবং এটি কতটা খাড়া।

114
00:08:12,040 --> 00:08:17,280
আপনি ঠিক থাকবেন যদি আপনি শুধু জানেন এবং আপনি বিশদ বিবরণে শক্ত না হন।

115
00:08:17,280 --> 00:08:21,440
কারণ যদি আপনি এটি পেতে পারেন, ফাংশনটি মিনিমাইজ করার জন্য অ্যালগরিদম হল এই গ্রেডিয়েন্ট দিকটি

116
00:08:21,440 --> 00:08:27,400
গণনা করা, তারপরে নিচের দিকে একটি ছোট পদক্ষেপ নিন এবং এটি বারবার পুনরাবৃত্তি করুন।

117
00:08:28,300 --> 00:08:33,700
এটি একটি ফাংশনের জন্য একই মৌলিক ধারণা যাতে 2টি ইনপুটের পরিবর্তে 13,000 ইনপুট রয়েছে।

118
00:08:33,700 --> 00:08:38,980
আমাদের নেটওয়ার্কের সমস্ত 13,000 ওজন এবং পক্ষপাতগুলিকে একটি

119
00:08:38,980 --> 00:08:40,180
বিশাল কলাম ভেক্টরে সংগঠিত করার কল্পনা করুন।

120
00:08:40,180 --> 00:08:46,140
খরচ ফাংশনের নেতিবাচক গ্রেডিয়েন্ট হল একটি ভেক্টর, এটি এই অত্যন্ত বিশাল

121
00:08:46,140 --> 00:08:51,660
ইনপুট স্পেসের ভিতরে কিছু দিক যা আপনাকে বলে যে এই সমস্ত

122
00:08:51,660 --> 00:08:55,900
সংখ্যার সাথে কোনটি নাজেস খরচ ফাংশনে সবচেয়ে দ্রুত হ্রাস ঘটাতে চলেছে৷

123
00:08:55,900 --> 00:09:00,000
এবং অবশ্যই, আমাদের বিশেষভাবে ডিজাইন করা খরচ ফাংশন সহ, ওজন এবং

124
00:09:00,000 --> 00:09:05,520
পক্ষপাতগুলিকে হ্রাস করার জন্য পরিবর্তন করার অর্থ হল প্রতিটি প্রশিক্ষণ ডেটাতে

125
00:09:05,520 --> 00:09:10,280
নেটওয়ার্কের আউটপুটকে 10টি মানের একটি এলোমেলো অ্যারের মতো দেখায় এবং আরও

126
00:09:10,280 --> 00:09:11,280
একটি বাস্তব সিদ্ধান্তের মতো যা আমরা চাই৷ এটা তৈরি করতে।

127
00:09:11,280 --> 00:09:15,940
এটি মনে রাখা গুরুত্বপূর্ণ, এই খরচ ফাংশনটি প্রশিক্ষণের সমস্ত ডেটার উপর একটি গড় জড়িত, তাই আপনি

128
00:09:15,940 --> 00:09:24,260
যদি এটিকে ছোট করেন তবে এর অর্থ হল এই সমস্ত নমুনার ক্ষেত্রে এটি একটি ভাল পারফরম্যান্স।

129
00:09:24,260 --> 00:09:28,540
এই গ্রেডিয়েন্টটি দক্ষতার সাথে কম্পিউট করার অ্যালগরিদম, যেটি কার্যকরভাবে একটি

130
00:09:28,540 --> 00:09:32,520
নিউরাল নেটওয়ার্ক কীভাবে শেখে তার হৃদয়কে ব্যাকপ্রোপ্যাগেশন বলা হয়,

131
00:09:32,520 --> 00:09:34,040
এবং এটিই আমি পরবর্তী ভিডিও সম্পর্কে কথা বলতে যাচ্ছি।

132
00:09:34,040 --> 00:09:39,100
সেখানে, আমি প্রদত্ত প্রশিক্ষণ ডেটার জন্য প্রতিটি ওজন এবং পক্ষপাতের সাথে ঠিক কী

133
00:09:39,100 --> 00:09:44,100
ঘটে তার মধ্য দিয়ে হাঁটতে সত্যিই সময় নিতে চাই, প্রাসঙ্গিক ক্যালকুলাস এবং সূত্রের

134
00:09:44,100 --> 00:09:47,980
স্তূপের বাইরে কী ঘটছে তার জন্য একটি স্বজ্ঞাত অনুভূতি দেওয়ার চেষ্টা করছি।

135
00:09:47,980 --> 00:09:51,780
এই মুহুর্তে, এই মুহূর্তে, আমি মূল জিনিসটি যা আপনি জানতে চাই, বাস্তবায়নের বিশদ

136
00:09:51,780 --> 00:09:56,820
বিবরণ থেকে স্বাধীন, তা হল আমরা যখন নেটওয়ার্ক শেখার বিষয়ে কথা বলি তখন

137
00:09:56,820 --> 00:09:59,320
আমরা যা বোঝায় তা হল এটি কেবলমাত্র একটি খরচ ফাংশন কমিয়ে দেয়।

138
00:09:59,320 --> 00:10:02,760
এবং লক্ষ্য করুন, এর একটি ফলাফল হল যে এই খরচ ফাংশনের

139
00:10:02,760 --> 00:10:07,820
জন্য একটি সুন্দর মসৃণ আউটপুট থাকা গুরুত্বপূর্ণ, যাতে আমরা নীচের দিকে

140
00:10:07,820 --> 00:10:09,340
সামান্য পদক্ষেপ নেওয়ার মাধ্যমে একটি স্থানীয় সর্বনিম্ন খুঁজে পেতে পারি।

141
00:10:09,340 --> 00:10:14,140
এই কারণেই, যাইহোক, কৃত্রিম নিউরনগুলি ক্রমাগত সক্রিয়

142
00:10:14,140 --> 00:10:18,580
বা বাইনারি উপায়ে নিষ্ক্রিয় হওয়ার পরিবর্তে, জৈবিক

143
00:10:18,580 --> 00:10:20,440
নিউরনগুলি যেভাবে হয় সেভাবে ক্রমাগত সক্রিয়তা রয়েছে।

144
00:10:20,440 --> 00:10:24,600
নেতিবাচক গ্রেডিয়েন্টের কিছু একাধিক দ্বারা একটি ফাংশনের একটি ইনপুটকে

145
00:10:24,600 --> 00:10:26,960
বারবার নাজ করার এই প্রক্রিয়াটিকে গ্রেডিয়েন্ট ডিসেন্ট বলে।

146
00:10:26,960 --> 00:10:31,760
এটি কিছু স্থানীয় ন্যূনতম খরচ ফাংশনের দিকে একত্রিত

147
00:10:31,760 --> 00:10:33,000
হওয়ার একটি উপায়, মূলত এই গ্রাফের একটি উপত্যকা।

148
00:10:33,000 --> 00:10:37,040
আমি এখনও দুটি ইনপুট সহ একটি ফাংশনের ছবি দেখাচ্ছি, অবশ্যই, কারণ

149
00:10:37,040 --> 00:10:41,480
13,000 ডাইমেনশনাল ইনপুট স্পেসে নাজগুলি আপনার মনকে ঘিরে রাখা একটু কঠিন,

150
00:10:41,480 --> 00:10:45,220
কিন্তু আসলে এটি সম্পর্কে চিন্তা করার একটি চমৎকার অ-স্থানিক উপায় রয়েছে।

151
00:10:45,220 --> 00:10:49,100
নেতিবাচক গ্রেডিয়েন্টের প্রতিটি উপাদান আমাদের দুটি জিনিস বলে।

152
00:10:49,100 --> 00:10:53,600
চিহ্নটি, অবশ্যই, ইনপুট ভেক্টরের সংশ্লিষ্ট উপাদানটি উপরে বা

153
00:10:53,600 --> 00:10:55,860
নিচে নাজ করা উচিত কিনা তা আমাদের বলে।

154
00:10:55,860 --> 00:11:01,340
কিন্তু গুরুত্বপূর্ণভাবে, এই সমস্ত উপাদানগুলির আপেক্ষিক মাত্রাগুলি

155
00:11:01,340 --> 00:11:05,620
আপনাকে বলে যে কোন পরিবর্তনগুলি বেশি গুরুত্বপূর্ণ।

156
00:11:05,620 --> 00:11:09,780
আপনি দেখতে পাচ্ছেন, আমাদের নেটওয়ার্কে, ওজনগুলির একটির সাথে সামঞ্জস্য অন্য কিছু ওজনের

157
00:11:09,780 --> 00:11:14,980
সাথে সামঞ্জস্যের চেয়ে ব্যয় ফাংশনের উপর অনেক বেশি প্রভাব ফেলতে পারে।

158
00:11:14,980 --> 00:11:19,440
এই সংযোগগুলির মধ্যে কিছু আমাদের প্রশিক্ষণ ডেটার জন্য আরও গুরুত্বপূর্ণ।

159
00:11:19,440 --> 00:11:23,520
সুতরাং আপনি আমাদের মন-warpingly বিশাল খরচ ফাংশন এই গ্রেডিয়েন্ট ভেক্টর সম্পর্কে চিন্তা করতে পারেন

160
00:11:23,520 --> 00:11:29,740
একটি উপায় হল যে এটি প্রতিটি ওজন এবং পক্ষপাতের আপেক্ষিক গুরুত্ব এনকোড করে,

161
00:11:29,740 --> 00:11:34,100
অর্থাৎ, এই পরিবর্তনগুলির মধ্যে কোনটি আপনার অর্থের জন্য সবচেয়ে বেশি ধাক্কা বহন করবে।

162
00:11:34,100 --> 00:11:37,360
এটি সত্যিই দিক সম্পর্কে চিন্তা করার অন্য উপায়।

163
00:11:37,360 --> 00:11:41,740
একটি সহজ উদাহরণ নিতে, আপনার যদি ইনপুট হিসাবে দুটি ভেরিয়েবল সহ কিছু

164
00:11:41,740 --> 00:11:48,720
ফাংশন থাকে এবং গণনা করে যে কোনও নির্দিষ্ট বিন্দুতে এর গ্রেডিয়েন্ট

165
00:11:48,720 --> 00:11:52,880
3,1 হিসাবে বেরিয়ে আসে, তবে একদিকে আপনি এটিকে ব্যাখ্যা করতে পারেন যে

166
00:11:52,880 --> 00:11:57,400
আপনি যখন বলছেন সেই ইনপুটে দাঁড়িয়ে, এই দিক বরাবর চললে ফাংশনটি

167
00:11:57,400 --> 00:12:02,200
সবচেয়ে দ্রুত বৃদ্ধি পায়, যে আপনি যখন ইনপুট পয়েন্টের সমতলের উপরে ফাংশনটি

168
00:12:02,200 --> 00:12:03,200
গ্রাফ করেন, তখন সেই ভেক্টরটি আপনাকে সোজা চড়াই দিক নির্দেশ করে।

169
00:12:03,200 --> 00:12:07,600
কিন্তু পড়ার আরেকটি উপায় হল যে এই প্রথম ভেরিয়েবলের পরিবর্তনগুলি দ্বিতীয় ভেরিয়েবলের

170
00:12:07,600 --> 00:12:12,400
পরিবর্তনের চেয়ে তিনগুণ গুরুত্ব বহন করে, যে অন্তত প্রাসঙ্গিক ইনপুটের আশেপাশে,

171
00:12:12,400 --> 00:12:17,740
x-মানকে নজ করা আপনার জন্য অনেক বেশি ধাক্কা বহন করে। বক

172
00:12:17,740 --> 00:12:22,880
ঠিক আছে, আসুন জুম আউট করি এবং আমরা এখন পর্যন্ত কোথায় আছি তা যোগ করি।

173
00:12:22,880 --> 00:12:28,660
নেটওয়ার্ক নিজেই 784 ইনপুট এবং 10 আউটপুট সহ

174
00:12:28,660 --> 00:12:30,860
এই ফাংশন, এই সমস্ত ওজনযুক্ত যোগফলের পরিপ্রেক্ষিতে সংজ্ঞায়িত।

175
00:12:30,860 --> 00:12:34,160
খরচ ফাংশন যে উপরে জটিলতা একটি স্তর.

176
00:12:34,160 --> 00:12:39,300
এটি ইনপুট হিসাবে 13,000 ওজন এবং পক্ষপাতগুলি নেয় এবং প্রশিক্ষণের

177
00:12:39,300 --> 00:12:42,640
উদাহরণগুলির উপর ভিত্তি করে একটি একক পরিমাপ থুতু দেয়৷

178
00:12:42,640 --> 00:12:47,520
খরচ ফাংশনের গ্রেডিয়েন্ট এখনও জটিলতার আরও একটি স্তর।

179
00:12:47,520 --> 00:12:52,860
এটি আমাদের বলে যে এই সমস্ত ওজন এবং পক্ষপাতগুলির সাথে কী

180
00:12:52,860 --> 00:12:56,640
কী ধাক্কা লাগে তা খরচ ফাংশনের মানতে দ্রুততম পরিবর্তন ঘটায়,

181
00:12:56,640 --> 00:13:03,040
যা আপনি ব্যাখ্যা করতে পারেন যে কোন পরিবর্তনগুলি সবচেয়ে গুরুত্বপূর্ণ।

182
00:13:03,040 --> 00:13:07,620
সুতরাং আপনি যখন র্যান্ডম ওজন এবং পক্ষপাতের সাথে নেটওয়ার্কটি শুরু করেন এবং

183
00:13:07,620 --> 00:13:12,420
এই গ্রেডিয়েন্ট ডিসেন্ট প্রক্রিয়ার উপর ভিত্তি করে সেগুলিকে অনেকবার সামঞ্জস্য করেন, এটি

184
00:13:12,420 --> 00:13:14,240
আগে কখনও দেখা যায়নি এমন চিত্রগুলিতে এটি কতটা ভাল কাজ করে?

185
00:13:14,240 --> 00:13:19,000
আমি এখানে যেটি বর্ণনা করেছি, প্রতিটি 16টি নিউরনের দুটি লুকানো স্তর সহ, বেশিরভাগই নান্দনিক কারণে

186
00:13:19,000 --> 00:13:26,920
বেছে নেওয়া হয়েছে, এটি খারাপ নয়, এটি সঠিকভাবে দেখা নতুন চিত্রগুলির প্রায় 96% শ্রেণীবদ্ধ করে৷

187
00:13:26,920 --> 00:13:31,580
এবং সত্যই, আপনি যদি কিছু উদাহরণের দিকে তাকান যা এটিকে

188
00:13:31,580 --> 00:13:36,300
বিভ্রান্ত করে, আপনি এটিকে কিছুটা শিথিল করতে বাধ্য বোধ করেন।

189
00:13:36,300 --> 00:13:40,220
আপনি যদি লুকানো স্তরের কাঠামোর সাথে খেলা করেন এবং কয়েকটি

190
00:13:40,220 --> 00:13:41,220
পরিবর্তন করেন তবে আপনি এটি 98% পর্যন্ত পেতে পারেন।

191
00:13:41,220 --> 00:13:42,900
এবং যে বেশ ভাল!

192
00:13:42,900 --> 00:13:47,020
এটি সর্বোত্তম নয়, আপনি অবশ্যই এই প্লেইন ভ্যানিলা নেটওয়ার্কের চেয়ে আরও পরিশীলিত হয়ে আরও

193
00:13:47,020 --> 00:13:52,460
ভাল পারফরম্যান্স পেতে পারেন, তবে প্রাথমিক কাজটি কতটা দুঃসাধ্য তা বিবেচনা করে, আমি

194
00:13:52,460 --> 00:13:56,800
মনে করি যে কোনও নেটওয়ার্ক ইমেজগুলিতে এটি ভালভাবে করছে তার মধ্যে অবিশ্বাস্য কিছু আছে

195
00:13:56,800 --> 00:14:02,000
যা আগে কখনও দেখা যায়নি। কোন নিদর্শন খুঁজতে হবে তা কখনই নির্দিষ্টভাবে বলেনি।

196
00:14:02,000 --> 00:14:07,840
মূলত, আমি যেভাবে এই কাঠামোটিকে অনুপ্রাণিত করেছি তা হল আমাদের একটি

197
00:14:07,840 --> 00:14:11,880
আশার বর্ণনা দিয়ে, যে দ্বিতীয় স্তরটি ছোট প্রান্তে উঠতে পারে, যে

198
00:14:11,880 --> 00:14:16,080
তৃতীয় স্তরটি সেই প্রান্তগুলিকে একত্রিত করে লুপ এবং লম্বা লাইন চিনতে

199
00:14:16,080 --> 00:14:18,220
পারে এবং সেগুলি টুকরো টুকরো হতে পারে। একসাথে অঙ্ক চিনতে.

200
00:14:18,220 --> 00:14:21,040
তাহলে কি এই আমাদের নেটওয়ার্ক আসলে কি করছে?

201
00:14:21,040 --> 00:14:24,880
ওয়েল, এই এক জন্য অন্তত, সব না.

202
00:14:24,960 --> 00:14:29,120
মনে রাখবেন কিভাবে আমরা শেষ ভিডিওতে দেখেছিলাম যে প্রথম স্তরের সমস্ত নিউরন

203
00:14:29,120 --> 00:14:33,900
থেকে দ্বিতীয় স্তরের একটি প্রদত্ত নিউরনের সাথে সংযোগের ওজনগুলিকে একটি প্রদত্ত পিক্সেল

204
00:14:33,900 --> 00:14:37,440
প্যাটার্ন হিসাবে কল্পনা করা যেতে পারে যেটি দ্বিতীয় স্তরের নিউরন গ্রহণ করছে?

205
00:14:37,440 --> 00:14:44,600
ঠিক আছে, যখন আমরা এই ট্রানজিশনের সাথে যুক্ত ওজনের জন্য এটি

206
00:14:44,600 --> 00:14:51,000
করি, এখানে এবং সেখানে বিচ্ছিন্ন ছোট প্রান্তে তোলার পরিবর্তে, তারা দেখতে,

207
00:14:51,000 --> 00:14:54,200
ভাল, প্রায় এলোমেলো, ঠিক মাঝখানে কিছু খুব আলগা প্যাটার্ন সহ।

208
00:14:54,200 --> 00:14:59,020
এটা মনে হবে যে সম্ভাব্য ওজন এবং পক্ষপাতের অকল্পনীয়ভাবে বিশাল 13,000

209
00:14:59,020 --> 00:15:04,020
ডাইমেনশনাল স্পেসে, আমাদের নেটওয়ার্ক নিজেকে একটি সুখী সামান্য স্থানীয় ন্যূনতম

210
00:15:04,020 --> 00:15:08,440
খুঁজে পেয়েছে যেটি সফলভাবে বেশিরভাগ চিত্রকে শ্রেণীবদ্ধ করা সত্ত্বেও, আমরা

211
00:15:08,440 --> 00:15:09,840
যে প্যাটার্নগুলির জন্য আশা করেছিলাম সেগুলি ঠিকভাবে গ্রহণ করে না৷

212
00:15:09,840 --> 00:15:14,600
এবং সত্যিই এই পয়েন্ট বাড়িতে চালাতে, আপনি একটি এলোমেলো চিত্র ইনপুট যখন কি ঘটতে দেখুন.

213
00:15:14,600 --> 00:15:19,240
যদি সিস্টেমটি স্মার্ট হয়, আপনি হয়ত এটিকে অনিশ্চিত বোধ করতে আশা করতে পারেন, হয়ত সত্যিই সেই

214
00:15:19,240 --> 00:15:24,120
10টি আউটপুট নিউরনগুলির একটিকে সক্রিয় করতে পারে না বা সেগুলিকে সমানভাবে সক্রিয় করতে পারে না, কিন্তু

215
00:15:24,520 --> 00:15:29,800
পরিবর্তে এটি আত্মবিশ্বাসের সাথে আপনাকে কিছু বাজে উত্তর দেয়, যেন এটি নিশ্চিত বোধ করে যে এই

216
00:15:29,800 --> 00:15:34,560
র্যান্ডম noise হল একটি 5 কারণ এটি করে যে একটি 5 এর প্রকৃত চিত্র একটি 5।

217
00:15:34,560 --> 00:15:39,300
ভিন্নভাবে বাক্যাংশ, এমনকি যদি এই নেটওয়ার্কটি অঙ্কগুলিকে বেশ ভালভাবে চিনতে

218
00:15:39,300 --> 00:15:41,800
পারে, তবে সেগুলি কীভাবে আঁকতে হয় তার কোনও ধারণা নেই।

219
00:15:41,800 --> 00:15:45,400
এর অনেক কিছু কারণ এটি একটি শক্তভাবে সীমাবদ্ধ প্রশিক্ষণ সেটআপ।

220
00:15:45,400 --> 00:15:48,220
আমি বলতে চাচ্ছি, এখানে নেটওয়ার্কের জুতা মধ্যে নিজেকে রাখা.

221
00:15:48,220 --> 00:15:53,280
এর দৃষ্টিকোণ থেকে, সমগ্র মহাবিশ্ব একটি ক্ষুদ্র গ্রিডে কেন্দ্রীভূত স্পষ্টভাবে সংজ্ঞায়িত অপরিবর্তনীয়

222
00:15:53,280 --> 00:15:58,560
সংখ্যা ছাড়া আর কিছুই নিয়ে গঠিত নয়, এবং এর খরচ ফাংশন এটিকে

223
00:15:58,560 --> 00:16:02,160
তার সিদ্ধান্তে সম্পূর্ণ আত্মবিশ্বাস ছাড়া অন্য কিছু হওয়ার জন্য কোন উৎসাহ দেয়নি।

224
00:16:02,160 --> 00:16:05,760
সুতরাং এই দ্বিতীয় স্তরের নিউরনগুলি আসলে কী করছে তার

225
00:16:05,760 --> 00:16:09,320
চিত্র হিসাবে, আপনি ভাবতে পারেন কেন আমি প্রান্ত এবং

226
00:16:09,320 --> 00:16:10,320
নিদর্শনগুলি বেছে নেওয়ার প্রেরণা দিয়ে এই নেটওয়ার্কটি চালু করব।

227
00:16:10,320 --> 00:16:13,040
আমি বলতে চাচ্ছি, যে এটা শেষ পর্যন্ত কি সব ঠিক না.

228
00:16:13,040 --> 00:16:17,480
ঠিক আছে, এটি আমাদের শেষ লক্ষ্য নয়, বরং একটি সূচনা বিন্দু।

229
00:16:17,480 --> 00:16:22,280
সত্যি বলতে কি, এটি পুরানো প্রযুক্তি, 80 এবং 90 এর দশকে যে ধরনের গবেষণা

230
00:16:22,280 --> 00:16:26,920
করা হয়েছিল, এবং আপনি আরও বিশদ আধুনিক রূপগুলি বোঝার আগে আপনাকে এটি বুঝতে

231
00:16:26,920 --> 00:16:31,380
হবে এবং এটি স্পষ্টতই কিছু আকর্ষণীয় সমস্যা সমাধান করতে সক্ষম, তবে আপনি যত

232
00:16:31,380 --> 00:16:38,720
বেশি খনন করবেন যারা লুকানো স্তর সত্যিই করছেন, কম বুদ্ধিমান এটা মনে হয়.

233
00:16:38,720 --> 00:16:43,540
আপনি কীভাবে শেখেন নেটওয়ার্কগুলি কীভাবে শেখে তার থেকে এক মুহুর্তের জন্য ফোকাস স্থানান্তরিত করা,

234
00:16:43,540 --> 00:16:47,160
এটি কেবল তখনই ঘটবে যদি আপনি কোনওভাবে এখানে উপাদানের সাথে সক্রিয়ভাবে জড়িত হন।

235
00:16:47,160 --> 00:16:51,920
একটি খুব সহজ জিনিস যা আমি আপনাকে করতে চাই তা হল এখনই বিরতি দিন এবং আপনি

236
00:16:51,920 --> 00:16:57,560
এই সিস্টেমে কী পরিবর্তন করতে পারেন এবং আপনি যদি প্রান্ত এবং নিদর্শনগুলির মতো জিনিসগুলিকে আরও ভালভাবে

237
00:16:57,560 --> 00:17:01,880
নিতে চান তবে এটি কীভাবে চিত্রগুলিকে উপলব্ধি করতে পারে সে সম্পর্কে কিছুক্ষণের জন্য গভীরভাবে চিন্তা করুন৷

238
00:17:01,880 --> 00:17:06,360
কিন্তু তার চেয়েও ভালো, আসলে উপাদানের সাথে জড়িত থাকার জন্য, আমি

239
00:17:06,360 --> 00:17:09,720
গভীর শিক্ষা এবং নিউরাল নেটওয়ার্কে মাইকেল নিলসনের বইটির সুপারিশ করছি।

240
00:17:09,720 --> 00:17:15,200
এটিতে, আপনি এই সঠিক উদাহরণের জন্য ডাউনলোড এবং খেলার জন্য কোড এবং ডেটা খুঁজে

241
00:17:15,200 --> 00:17:19,360
পেতে পারেন, এবং বইটি আপনাকে ধাপে ধাপে সেই কোডটি কী করছে তা নিয়ে যাবে।

242
00:17:19,360 --> 00:17:23,920
কি আশ্চর্যজনক বিষয় হল যে এই বইটি বিনামূল্যে এবং সর্বজনীনভাবে উপলব্ধ, তাই আপনি যদি এটি থেকে কিছু

243
00:17:23,920 --> 00:17:28,040
পেতে পারেন, তাহলে নিলসনের প্রচেষ্টার প্রতি দান করার জন্য আমার সাথে যোগদান করার কথা বিবেচনা করুন৷

244
00:17:28,040 --> 00:17:32,060
আমি ক্রিস ওলার অসাধারণ এবং সুন্দর ব্লগ পোস্ট এবং ডিস্টিলের নিবন্ধগুলি

245
00:17:32,060 --> 00:17:38,720
সহ বর্ণনায় আমার অনেক পছন্দের আরও কয়েকটি সংস্থান লিঙ্ক করেছি।

246
00:17:38,720 --> 00:17:41,960
গত কয়েক মিনিটের জন্য জিনিসগুলি এখানে বন্ধ করতে, আমি লিশা

247
00:17:41,960 --> 00:17:44,440
লির সাথে আমার সাক্ষাৎকারের একটি স্নিপেটে ফিরে যেতে চাই।

248
00:17:44,440 --> 00:17:48,520
আপনি শেষ ভিডিও থেকে তাকে মনে রাখতে পারেন, তিনি গভীর শিক্ষায় তার পিএইচডি কাজ করেছেন।

249
00:17:48,560 --> 00:17:52,240
এই ছোট্ট স্নিপেটে, তিনি দুটি সাম্প্রতিক কাগজপত্র সম্পর্কে কথা বলেছেন যেগুলি আসলেই

250
00:17:52,240 --> 00:17:56,380
আরও কিছু আধুনিক ইমেজ শনাক্তকরণ নেটওয়ার্কগুলি আসলে কীভাবে শিখছে তা খনন করে৷

251
00:17:56,380 --> 00:18:00,320
আমরা কথোপকথনে যেখানে ছিলাম তা সেট আপ করার জন্য, প্রথম কাগজটি এই বিশেষভাবে গভীর

252
00:18:00,320 --> 00:18:04,480
নিউরাল নেটওয়ার্কগুলির মধ্যে একটি নিয়েছিল যা চিত্র সনাক্তকরণে সত্যিই ভাল, এবং এটিকে সঠিকভাবে লেবেলযুক্ত

253
00:18:04,480 --> 00:18:09,400
ডেটাসেটে প্রশিক্ষণ দেওয়ার পরিবর্তে, এটি প্রশিক্ষণের আগে চারপাশের সমস্ত লেবেলগুলিকে এলোমেলো করে দেয়৷

254
00:18:09,400 --> 00:18:13,840
স্পষ্টতই এখানে পরীক্ষার নির্ভুলতা র্যান্ডম থেকে ভাল

255
00:18:13,840 --> 00:18:15,320
হবে না, যেহেতু সবকিছুই এলোমেলোভাবে লেবেলযুক্ত।

256
00:18:15,320 --> 00:18:20,080
কিন্তু এটি এখনও একই প্রশিক্ষণ নির্ভুলতা অর্জন করতে সক্ষম

257
00:18:20,080 --> 00:18:21,440
ছিল যেমন আপনি একটি সঠিকভাবে লেবেল করা ডেটাসেটে করবেন।

258
00:18:21,440 --> 00:18:26,120
মূলত, এই নির্দিষ্ট নেটওয়ার্কের জন্য লক্ষ লক্ষ ওজন শুধুমাত্র র্যান্ডম ডেটা মুখস্ত করার জন্য

259
00:18:26,120 --> 00:18:31,040
যথেষ্ট ছিল, যা এই প্রশ্ন উত্থাপন করে যে এই খরচ ফাংশনটি হ্রাস করা

260
00:18:31,040 --> 00:18:36,720
আসলে চিত্রের কোনও ধরণের কাঠামোর সাথে মিলে যায়, নাকি এটি কেবল মুখস্ত করা?

261
00:18:36,720 --> 00:18:40,120
. . . সঠিক শ্রেণীবিভাগ কি তা সম্পূর্ণ ডেটাসেট মুখস্থ করতে।

262
00:18:40,120 --> 00:18:45,720
এবং তাই কয়েকটা, আপনি জানেন, অর্ধেক বছর পরে এই বছর ICML-এ, সেখানে

263
00:18:45,720 --> 00:18:50,440
ঠিক খণ্ডন কাগজ ছিল না, কিন্তু কাগজ যা কিছু দিক সম্বোধন করে

264
00:18:50,440 --> 00:18:52,220
যেমন, আরে, আসলে এই নেটওয়ার্কগুলি তার চেয়ে কিছুটা স্মার্ট কিছু করছে।

265
00:18:52,220 --> 00:18:59,600
আপনি যদি সেই নির্ভুলতা বক্ররেখার দিকে তাকান, যদি আপনি কেবল একটি এলোমেলো ডেটাসেটের প্রশিক্ষণ নিচ্ছেন, তবে

266
00:18:59,600 --> 00:19:05,240
সেই বক্ররেখাটি খুব নিচে নেমে গেছে, আপনি জানেন, খুব ধীরে ধীরে প্রায় এক রৈখিক ফ্যাশনে।

267
00:19:05,280 --> 00:19:10,840
তাই আপনি সত্যিই সম্ভাব্য স্থানীয় মিনিমাম খুঁজে পেতে সংগ্রাম করছেন,

268
00:19:10,840 --> 00:19:12,320
আপনি জানেন, সঠিক ওজন যা আপনাকে সেই নির্ভুলতা পেতে পারে।

269
00:19:12,320 --> 00:19:16,720
আপনি যদি আসলে একটি স্ট্রাকচার্ড ডেটাসেটের উপর প্রশিক্ষণ নিচ্ছেন, যার সঠিক

270
00:19:16,720 --> 00:19:20,240
লেবেল রয়েছে, আপনি জানেন, আপনি শুরুতে একটু ঘোরাঘুরি করেন, কিন্তু তারপরে

271
00:19:20,240 --> 00:19:23,360
আপনি সেই নির্ভুলতার স্তরে পৌঁছানোর জন্য খুব দ্রুত নেমে গেলেন।

272
00:19:23,360 --> 00:19:28,580
এবং তাই কিছু অর্থে স্থানীয় ম্যাক্সিমা খুঁজে পাওয়া সহজ ছিল।

273
00:19:28,580 --> 00:19:32,900
এবং তাই এটি সম্পর্কেও যেটি আকর্ষণীয় ছিল তা হ'ল

274
00:19:32,900 --> 00:19:39,140
এটি কয়েক বছর আগে থেকে আরও একটি কাগজ প্রকাশ

275
00:19:39,140 --> 00:19:40,140
করে, যার নেটওয়ার্ক স্তরগুলি সম্পর্কে আরও অনেক সরলীকরণ রয়েছে।

276
00:19:40,140 --> 00:19:43,880
কিন্তু ফলাফলগুলির মধ্যে একটি বলছে কিভাবে, আপনি যদি অপ্টিমাইজেশান ল্যান্ডস্কেপ দেখেন, তাহলে

277
00:19:43,880 --> 00:19:49,400
এই নেটওয়ার্কগুলি যে স্থানীয় মিনিমা শিখতে থাকে তা আসলে সমান মানের।

278
00:19:49,400 --> 00:19:54,300
তাই কিছু অর্থে, যদি আপনার ডেটা সেট কাঠামোগত হয়, তাহলে আপনি এটি আরও সহজে খুঁজে পেতে সক্ষম হবেন।

279
00:19:58,580 --> 00:20:01,140
আপনি যারা Patreon সমর্থন করছেন বরাবরের মতো আমার ধন্যবাদ।

280
00:20:01,480 --> 00:20:05,440
প্যাট্রিয়নের গেম চেঞ্জার কী তা আমি আগে বলেছি, তবে

281
00:20:05,440 --> 00:20:07,160
এই ভিডিওগুলি সত্যিই আপনাকে ছাড়া সম্ভব হবে না।

282
00:20:07,160 --> 00:20:11,540
আমি ভিসি ফার্ম Amplify Partners এবং সিরিজের এই প্রাথমিক

283
00:20:11,540 --> 00:20:13,240
ভিডিওগুলির জন্য তাদের সমর্থনকেও বিশেষ ধন্যবাদ জানাতে চাই৷

284
00:20:31,140 --> 00:20:33,140
ধন্যবাদ.

