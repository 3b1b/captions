1
00:00:00,000 --> 00:00:07,240
शेवटचा व्हिडिओ मी न्यूरल नेटवर्कची रचना मांडली.

2
00:00:07,240 --> 00:00:11,560
मी येथे एक द्रुत रीकॅप देईन जेणेकरुन ते आपल्या मनात ताजे

3
00:00:11,560 --> 00:00:13,160
असेल आणि नंतर या व्हिडिओसाठी माझी दोन मुख्य उद्दिष्टे आहेत.

4
00:00:13,160 --> 00:00:17,960
प्रथम म्हणजे ग्रेडियंट डिसेंटची कल्पना सादर करणे, ज्यामध्ये केवळ न्यूरल नेटवर्क कसे शिकतात

5
00:00:17,960 --> 00:00:20,800
असे नाही तर इतर मशीन लर्निंग कसे कार्य करते हे देखील अधोरेखित करते.

6
00:00:20,800 --> 00:00:25,160
त्यानंतर आम्ही हे विशिष्ट नेटवर्क कसे कार्य करते आणि न्यूरॉन्सचे ते

7
00:00:25,160 --> 00:00:29,560
लपलेले स्तर काय शोधत आहेत याबद्दल थोडे अधिक जाणून घेऊ.

8
00:00:29,560 --> 00:00:34,680
एक स्मरणपत्र म्हणून, आमचे लक्ष्य हस्तलिखित अंक ओळखीचे

9
00:00:34,680 --> 00:00:37,080
उत्कृष्ट उदाहरण आहे, न्यूरल नेटवर्कचे हॅलो वर्ल्ड.

10
00:00:37,080 --> 00:00:42,160
हे अंक 28x28 पिक्सेल ग्रिडवर रेंडर केले जातात, प्रत्येक

11
00:00:42,160 --> 00:00:44,260
पिक्सेलचे काही ग्रेस्केल मूल्य 0 आणि 1 दरम्यान असते.

12
00:00:44,260 --> 00:00:51,400
ते नेटवर्कच्या इनपुट लेयरमध्ये 784 न्यूरॉन्सची सक्रियता निर्धारित करतात.

13
00:00:51,400 --> 00:00:56,880
खालील स्तरांमधील प्रत्येक न्यूरॉनचे सक्रियकरण मागील लेयरमधील सर्व सक्रियतेच्या भारित

14
00:00:56,880 --> 00:01:02,300
बेरजेवर, तसेच काही विशेष संख्येवर आधारित आहे ज्याला बायस म्हणतात.

15
00:01:02,300 --> 00:01:07,480
तुम्ही ती बेरीज इतर काही फंक्शनसह तयार करा, जसे की

16
00:01:07,480 --> 00:01:09,640
सिग्मॉइड स्क्विशिफिकेशन, किंवा ReLU, ज्या प्रकारे मी शेवटचा व्हिडिओ पाहिला.

17
00:01:09,640 --> 00:01:14,960
एकूण, प्रत्येकी 16 न्यूरॉन्ससह दोन लपविलेल्या स्तरांची काहीशी अनियंत्रित निवड दिल्यास,

18
00:01:14,960 --> 00:01:20,940
नेटवर्कमध्ये सुमारे 13,000 वजने आणि पूर्वाग्रह आहेत जे आपण समायोजित करू

19
00:01:20,940 --> 00:01:25,320
शकतो आणि ही मूल्ये नेटवर्क नेमके काय करते हे निर्धारित करतात.

20
00:01:25,320 --> 00:01:29,800
आणि जेव्हा आपण म्हणतो की हे नेटवर्क दिलेल्या अंकाचे वर्गीकरण करते तेव्हा त्याचा अर्थ

21
00:01:29,800 --> 00:01:34,080
असा होतो की अंतिम स्तरातील त्या 10 न्यूरॉन्सपैकी सर्वात तेजस्वी त्या अंकाशी संबंधित आहे.

22
00:01:34,080 --> 00:01:39,240
आणि लक्षात ठेवा, स्तरित संरचनेसाठी आमच्या मनात असलेली प्रेरणा ही

23
00:01:39,240 --> 00:01:43,920
होती की कदाचित दुसरा स्तर कडांवर उचलू शकेल, तिसरा

24
00:01:43,920 --> 00:01:48,640
स्तर लूप आणि रेषा यांसारख्या नमुन्यांवर उचलू शकेल आणि शेवटचा

25
00:01:48,640 --> 00:01:49,640
थर फक्त त्या नमुन्यांना एकत्र करू शकेल. अंक ओळखा.

26
00:01:49,640 --> 00:01:52,880
तर इथे, नेटवर्क कसे शिकते ते आपण शिकतो.

27
00:01:52,880 --> 00:01:56,880
आम्हाला काय हवे आहे ते एक अल्गोरिदम आहे जिथे तुम्ही या नेटवर्कला

28
00:01:56,880 --> 00:02:01,540
प्रशिक्षण डेटाचा संपूर्ण समूह दर्शवू शकता, जे हस्तलिखित अंकांच्या विविध प्रतिमांच्या गुच्छाच्या

29
00:02:01,540 --> 00:02:06,360
स्वरूपात येते आणि ते काय असावे याच्या लेबलांसह, आणि ते ते 13,000

30
00:02:06,360 --> 00:02:10,760
वजन आणि पूर्वाग्रह समायोजित करा जेणेकरून प्रशिक्षण डेटावर त्याचे कार्यप्रदर्शन सुधारेल.

31
00:02:10,760 --> 00:02:15,540
आशा आहे की या स्तरित संरचनेचा अर्थ असा होईल की ते

32
00:02:15,540 --> 00:02:17,840
जे शिकते ते त्या प्रशिक्षण डेटाच्या पलीकडे असलेल्या प्रतिमांना सामान्यीकृत करते.

33
00:02:17,840 --> 00:02:22,240
आम्ही ज्या प्रकारे चाचणी करतो ते म्हणजे तुम्ही नेटवर्क प्रशिक्षित केल्यानंतर, तुम्ही त्यास अधिक लेबल

34
00:02:22,240 --> 00:02:31,160
केलेला डेटा दाखवता आणि ते त्या नवीन प्रतिमांचे किती अचूक वर्गीकरण करते ते तुम्ही पाहता.

35
00:02:31,160 --> 00:02:34,760
सुदैवाने आमच्यासाठी, आणि हे एक सामान्य उदाहरण ज्याने सुरू केले आहे, ते

36
00:02:34,760 --> 00:02:39,520
म्हणजे MNIST डेटाबेसच्या मागे असलेल्या चांगल्या लोकांनी हजारो हस्तलिखीत अंकांच्या प्रतिमांचा संग्रह

37
00:02:39,520 --> 00:02:45,080
केला आहे, ज्या प्रत्येकावर ते असायला हवे त्या संख्येसह लेबल केलेले आहेत.

38
00:02:45,080 --> 00:02:49,920
आणि एखाद्या मशीनचे शिक्षण म्हणून वर्णन करणे जितके उत्तेजक आहे, ते कसे कार्य करते हे

39
00:02:49,920 --> 00:02:55,560
एकदा तुम्ही पाहिल्यानंतर, ते काही वेडगळ विज्ञान-कल्पनाप्रमाणे कमी आणि कॅल्क्युलस व्यायामासारखे बरेच काही वाटते.

40
00:02:55,560 --> 00:03:01,040
मला असे म्हणायचे आहे की, मुळात हे विशिष्ट फंक्शनचे किमान शोधण्यासाठी खाली येते.

41
00:03:01,040 --> 00:03:06,480
लक्षात ठेवा, संकल्पनात्मकदृष्ट्या आम्ही प्रत्येक न्यूरॉनचा आधीच्या लेयरमधील सर्व न्यूरॉन्सशी जोडलेला

42
00:03:06,480 --> 00:03:11,440
आहे असा विचार करत आहोत आणि त्याचे सक्रियकरण परिभाषित करणार्‍या भारित

43
00:03:11,440 --> 00:03:16,400
बेरीजमधील वजन हे त्या कनेक्शनच्या सामर्थ्यासारखे आहेत आणि पूर्वाग्रह हे काही

44
00:03:16,400 --> 00:03:19,780
संकेत आहेत. तो न्यूरॉन सक्रिय किंवा निष्क्रिय आहे की नाही.

45
00:03:19,780 --> 00:03:23,300
आणि गोष्टी सुरू करण्यासाठी, आम्ही ते सर्व वजन

46
00:03:23,300 --> 00:03:25,020
आणि पूर्वाग्रह पूर्णपणे यादृच्छिकपणे सुरू करणार आहोत.

47
00:03:25,020 --> 00:03:29,100
हे सांगण्याची गरज नाही, हे नेटवर्क दिलेल्या प्रशिक्षण उदाहरणावर भयानक

48
00:03:29,100 --> 00:03:31,180
कामगिरी करणार आहे, कारण ते काहीतरी यादृच्छिक करत आहे.

49
00:03:31,180 --> 00:03:36,820
उदाहरणार्थ, तुम्ही 3 च्या या प्रतिमेमध्ये फीड करा आणि आउटपुट लेयर फक्त गोंधळासारखे दिसते.

50
00:03:36,820 --> 00:03:43,340
तर तुम्ही काय करता ते म्हणजे कॉस्ट फंक्शन, कॉम्प्युटरला सांगण्याचा एक मार्ग, नाही, खराब कॉम्प्युटर,

51
00:03:43,340 --> 00:03:48,940
त्या आउटपुटमध्ये सक्रियता असली पाहिजे जी बहुतेक न्यूरॉन्ससाठी 0 असते, परंतु या न्यूरॉनसाठी 1 असते.

52
00:03:48,980 --> 00:03:51,740
तू मला जे दिले ते पूर्णपणे कचरा आहे.

53
00:03:51,740 --> 00:03:56,740
थोडे अधिक गणितीय दृष्ट्या सांगायचे तर, तुम्ही त्या प्रत्येक कचरा

54
00:03:56,740 --> 00:04:01,980
आउटपुट सक्रियकरणातील फरकांचे वर्ग जोडता आणि तुम्हाला त्यांचे मूल्य हवे

55
00:04:01,980 --> 00:04:06,020
आहे आणि यालाच आम्ही एका प्रशिक्षण उदाहरणाची किंमत म्हणू.

56
00:04:06,020 --> 00:04:12,660
लक्षात घ्या जेव्हा नेटवर्क आत्मविश्वासाने प्रतिमेचे अचूक वर्गीकरण करते तेव्हा ही बेरीज लहान असते, परंतु

57
00:04:12,660 --> 00:04:18,820
जेव्हा नेटवर्कला ते काय करत आहे हे माहित नसल्यासारखे दिसते तेव्हा ती मोठी असते.

58
00:04:18,820 --> 00:04:23,860
तर मग तुम्ही जे कराल ते तुमच्या सर्व

59
00:04:23,860 --> 00:04:27,580
दहा हजार प्रशिक्षण उदाहरणांच्या सरासरी खर्चाचा विचार करा.

60
00:04:27,580 --> 00:04:32,300
नेटवर्क किती खराब आहे आणि संगणकाला किती वाईट

61
00:04:32,300 --> 00:04:33,300
वाटले पाहिजे यासाठी ही सरासरी किंमत आहे.

62
00:04:33,300 --> 00:04:35,300
आणि ही एक गुंतागुंतीची गोष्ट आहे.

63
00:04:35,300 --> 00:04:40,380
लक्षात ठेवा की नेटवर्क स्वतःच एक फंक्शन कसे होते, जे इनपुट म्हणून 784

64
00:04:40,380 --> 00:04:46,100
संख्या घेते, पिक्सेल व्हॅल्यूज घेते आणि त्याचे आउटपुट म्हणून 10 संख्या बाहेर टाकते

65
00:04:46,100 --> 00:04:49,700
आणि एका अर्थाने ते या सर्व वजन आणि पूर्वाग्रहांद्वारे पॅरामीटराइज्ड केले जाते?

66
00:04:49,700 --> 00:04:53,340
कॉस्ट फंक्शन हा त्यावरील गुंतागुंतीचा एक थर आहे.

67
00:04:53,340 --> 00:04:59,140
हे 13,000 किंवा त्यापेक्षा जास्त वजने आणि बायसेस इनपुट म्हणून घेते आणि ते वजन

68
00:04:59,140 --> 00:05:04,620
आणि बायसेस किती वाईट आहेत याचे वर्णन करणारी एकच संख्या बाहेर टाकते आणि ते

69
00:05:04,620 --> 00:05:09,140
कसे परिभाषित केले जाते ते प्रशिक्षण डेटाच्या हजारो भागांवर नेटवर्कच्या वर्तनावर अवलंबून असते.

70
00:05:09,140 --> 00:05:12,460
खूप विचार करण्यासारखे आहे.

71
00:05:12,460 --> 00:05:16,380
पण कॉम्प्युटरला फक्त सांगणे हे काय वाईट काम आहे ते फारसे उपयुक्त नाही.

72
00:05:16,380 --> 00:05:21,300
ते वजन आणि पूर्वाग्रह कसे बदलायचे ते तुम्हाला सांगायचे आहे जेणेकरून ते चांगले होईल.

73
00:05:21,300 --> 00:05:25,580
ते सोपे करण्यासाठी, 13,000 इनपुट्ससह फंक्शनची कल्पना करण्यासाठी संघर्ष करण्याऐवजी, फक्त एका साध्या फंक्शनची

74
00:05:25,580 --> 00:05:31,440
कल्पना करा ज्यामध्ये एक संख्या इनपुट म्हणून आणि एक संख्या आउटपुट म्हणून आहे.

75
00:05:31,440 --> 00:05:36,420
या फंक्शनचे मूल्य कमी करणारे इनपुट कसे शोधायचे?

76
00:05:36,420 --> 00:05:41,300
कॅल्क्युलसच्या विद्यार्थ्यांना हे कळेल की तुम्ही काहीवेळा ते किमान स्पष्टपणे काढू शकता,

77
00:05:41,340 --> 00:05:46,620
परंतु ते खरोखर क्लिष्ट फंक्शन्ससाठी नेहमीच शक्य नसते, आमच्या वेड्या गुंतागुंतीच्या

78
00:05:46,620 --> 00:05:51,640
न्यूरल नेटवर्क कॉस्ट फंक्शनसाठी या परिस्थितीच्या 13,000 इनपुट आवृत्तीमध्ये नक्कीच नाही.

79
00:05:51,640 --> 00:05:56,820
अधिक लवचिक युक्ती म्हणजे कोणत्याही इनपुटपासून प्रारंभ करणे आणि ते आउटपुट

80
00:05:56,820 --> 00:05:59,860
कमी करण्यासाठी आपण कोणत्या दिशेने पाऊल टाकले पाहिजे हे शोधणे.

81
00:05:59,860 --> 00:06:05,020
विशेषत:, तुम्ही जेथे आहात त्या फंक्शनचा उतार तुम्ही

82
00:06:05,020 --> 00:06:09,280
काढू शकत असल्यास, तो उतार सकारात्मक असल्यास डावीकडे

83
00:06:09,280 --> 00:06:12,720
सरकवा आणि उतार ऋणात्मक असल्यास इनपुट उजवीकडे वळवा.

84
00:06:12,720 --> 00:06:17,040
तुम्ही हे वारंवार करत असल्यास, प्रत्येक बिंदूवर नवीन उतार तपासत आहात आणि

85
00:06:17,040 --> 00:06:20,680
योग्य पाऊल उचलत आहात, तुम्ही काही स्थानिक किमान फंक्शनशी संपर्क साधणार आहात.

86
00:06:20,680 --> 00:06:24,600
आणि इथे तुमच्या मनात असलेली प्रतिमा टेकडीवरून खाली लोटणारा चेंडू आहे.

87
00:06:24,600 --> 00:06:29,380
आणि लक्षात घ्या, या खरोखरच सरलीकृत सिंगल इनपुट फंक्शनसाठीही, तुम्ही कोणत्या

88
00:06:29,380 --> 00:06:34,220
यादृच्छिक इनपुटपासून सुरुवात करता यावर अवलंबून, अनेक संभाव्य व्हॅली आहेत ज्यात

89
00:06:34,220 --> 00:06:38,460
तुम्ही उतरू शकता आणि तुम्ही ज्या स्थानिक किमान मध्ये उतरता ते

90
00:06:38,460 --> 00:06:39,460
सर्वात लहान संभाव्य मूल्य असेल याची कोणतीही हमी नाही. खर्च कार्य.

91
00:06:39,460 --> 00:06:43,180
ते आमच्या न्यूरल नेटवर्क केसमध्ये देखील जाईल.

92
00:06:43,180 --> 00:06:48,140
आणि मी तुम्हाला हे देखील लक्षात घ्यायचे आहे की जर तुम्ही तुमच्या पायऱ्यांचा

93
00:06:48,140 --> 00:06:52,920
आकार उताराच्या प्रमाणात कसा बनवलात, तर जेव्हा उतार किमान दिशेने सपाट होत असेल,

94
00:06:52,920 --> 00:06:56,020
तेव्हा तुमची पावले लहान होत जातात आणि अशा प्रकारची तुम्हाला ओव्हरशूटिंगपासून मदत होते.

95
00:06:56,020 --> 00:07:01,640
जटिलता थोडीशी वाढवून, त्याऐवजी दोन इनपुट आणि एक आउटपुट असलेल्या फंक्शनची कल्पना करा.

96
00:07:01,640 --> 00:07:06,360
तुम्ही इनपुट स्पेसचा xy-प्लेन म्हणून विचार करू शकता आणि

97
00:07:06,360 --> 00:07:09,020
खर्चाचे कार्य त्याच्या वरच्या पृष्ठभागाच्या रूपात आलेख केले आहे.

98
00:07:09,020 --> 00:07:13,600
फंक्शनच्या स्लोपबद्दल विचारण्याऐवजी, फंक्शनचे आउटपुट लवकर कमी करण्यासाठी या इनपुट

99
00:07:13,600 --> 00:07:19,780
स्पेसमध्ये तुम्ही कोणत्या दिशेने पाऊल टाकावे हे विचारावे लागेल.

100
00:07:19,780 --> 00:07:22,340
दुसऱ्या शब्दांत, उताराची दिशा काय आहे?

101
00:07:22,340 --> 00:07:26,740
आणि पुन्हा, त्या टेकडीवरून बॉल फिरवण्याचा विचार करणे उपयुक्त आहे.

102
00:07:26,740 --> 00:07:31,920
तुमच्यापैकी जे मल्टीव्हेरिएबल कॅल्क्युलसशी परिचित आहेत त्यांना हे कळेल की

103
00:07:31,920 --> 00:07:37,460
फंक्शनचा ग्रेडियंट तुम्हाला सर्वात जास्त चढाईची दिशा देतो, फंक्शन

104
00:07:37,460 --> 00:07:39,420
सर्वात वेगाने वाढवण्यासाठी तुम्ही कोणत्या दिशेने पाऊल टाकले पाहिजे.

105
00:07:39,420 --> 00:07:43,820
साहजिकच पुरेसे आहे, त्या ग्रेडियंटचे ऋण घेतल्याने तुम्हाला पायरीची

106
00:07:43,820 --> 00:07:47,460
दिशा मिळते ज्यामुळे कार्य सर्वात लवकर कमी होते.

107
00:07:47,460 --> 00:07:52,320
त्याहूनही अधिक, या ग्रेडियंट वेक्टरची लांबी ही सर्वात

108
00:07:52,320 --> 00:07:54,580
उंच उतार किती तीव्र आहे हे दर्शवते.

109
00:07:54,580 --> 00:07:58,080
आता जर तुम्हाला मल्टीव्हेरिएबल कॅल्क्युलसबद्दल अपरिचित असेल आणि तुम्हाला अधिक जाणून घ्यायचे

110
00:07:58,080 --> 00:08:01,100
असेल, तर मी खान अकादमीसाठी या विषयावर केलेले काही काम पहा.

111
00:08:01,100 --> 00:08:05,680
खरे सांगायचे तर, तुमच्यासाठी आणि माझ्यासाठी सध्या महत्त्वाची गोष्ट म्हणजे या वेक्टरची

112
00:08:05,680 --> 00:08:10,440
गणना करण्याचा एक मार्ग तत्त्वतः अस्तित्वात आहे, हा वेक्टर जो तुम्हाला

113
00:08:10,440 --> 00:08:12,040
उताराची दिशा काय आहे आणि ती किती उंच आहे हे सांगते.

114
00:08:12,040 --> 00:08:17,280
तुम्हाला एवढंच माहीत असेल आणि तुम्ही तपशिलांवर ठोस नसाल तर तुम्ही ठीक असाल.

115
00:08:17,280 --> 00:08:21,440
कारण जर तुम्ही ते मिळवू शकत असाल तर, फंक्शन कमी करण्यासाठी अल्गोरिदम म्हणजे या ग्रेडियंट

116
00:08:21,440 --> 00:08:27,400
दिशेची गणना करणे, नंतर उतारावर एक लहान पाऊल घ्या आणि ते पुन्हा पुन्हा करा.

117
00:08:28,300 --> 00:08:33,700
2 इनपुट ऐवजी 13,000 इनपुट्स असलेल्या फंक्शनची हीच मूळ कल्पना आहे.

118
00:08:33,700 --> 00:08:38,980
आमच्या नेटवर्कचे सर्व 13,000 वजन आणि पक्षपात एका

119
00:08:38,980 --> 00:08:40,180
विशाल स्तंभ वेक्टरमध्ये आयोजित करण्याची कल्पना करा.

120
00:08:40,180 --> 00:08:46,140
कॉस्ट फंक्शनचा नकारात्मक ग्रेडियंट हा फक्त एक वेक्टर आहे, या प्रचंड मोठ्या

121
00:08:46,140 --> 00:08:51,660
इनपुट स्पेसच्या आत ही काही दिशा आहे जी तुम्हाला सांगते की या

122
00:08:51,660 --> 00:08:55,900
सर्व आकड्यांकडे कोणते नज केल्याने खर्च फंक्शनमध्ये सर्वात जलद घट होईल.

123
00:08:55,900 --> 00:09:00,000
आणि अर्थातच, आमच्या खास डिझाइन केलेल्या किमतीच्या कार्यासह, वजन आणि

124
00:09:00,000 --> 00:09:05,520
पूर्वाग्रह बदलून ते कमी करणे म्हणजे प्रशिक्षण डेटाच्या प्रत्येक

125
00:09:05,520 --> 00:09:10,280
तुकड्यावर नेटवर्कचे आउटपुट 10 मूल्यांच्या यादृच्छिक अॅरेसारखे कमी दिसणे

126
00:09:10,280 --> 00:09:11,280
आणि आम्हाला हवे असलेल्या वास्तविक निर्णयासारखे दिसते. ते बनवण्यासाठी.

127
00:09:11,280 --> 00:09:15,940
हे लक्षात ठेवणे महत्त्वाचे आहे की, या खर्चाच्या कार्यामध्ये सर्व प्रशिक्षण डेटावर सरासरीचा समावेश होतो, म्हणून

128
00:09:15,940 --> 00:09:24,260
जर तुम्ही ते कमी केले तर याचा अर्थ त्या सर्व नमुन्यांवर ते अधिक चांगले कार्यप्रदर्शन आहे.

129
00:09:24,260 --> 00:09:28,540
या ग्रेडियंटची कार्यक्षमतेने गणना करण्यासाठी अल्गोरिदम, जे प्रभावीपणे

130
00:09:28,540 --> 00:09:32,520
न्यूरल नेटवर्क कसे शिकते याचे हृदय आहे, त्याला

131
00:09:32,520 --> 00:09:34,040
बॅकप्रोपॅगेशन म्हणतात, आणि मी पुढील व्हिडिओबद्दल बोलणार आहे.

132
00:09:34,040 --> 00:09:39,100
तेथे, प्रशिक्षण डेटाच्या दिलेल्या भागासाठी प्रत्येक वजन आणि पूर्वाग्रहाचे नेमके काय होते

133
00:09:39,100 --> 00:09:44,100
ते जाणून घेण्यासाठी मला खरोखर वेळ काढायचा आहे, संबंधित कॅल्क्युलस आणि सूत्रांच्या

134
00:09:44,100 --> 00:09:47,980
ढिगाऱ्याच्या पलीकडे काय घडत आहे याची अंतर्ज्ञानी भावना देण्याचा प्रयत्न करतो.

135
00:09:47,980 --> 00:09:51,780
आत्ता इथे, आत्ता, मी तुम्हाला जाणून घ्यायची आहे, अंमलबजावणीच्या तपशिलांपासून

136
00:09:51,780 --> 00:09:56,820
स्वतंत्र आहे, जेव्हा आपण नेटवर्क लर्निंगबद्दल बोलतो तेव्हा आपल्याला काय

137
00:09:56,820 --> 00:09:59,320
म्हणायचे आहे ते म्हणजे केवळ खर्चाचे कार्य कमी करणे.

138
00:09:59,320 --> 00:10:02,760
आणि लक्षात घ्या, त्याचा एक परिणाम असा आहे की या

139
00:10:02,760 --> 00:10:07,820
किमतीच्या कार्यासाठी एक छान गुळगुळीत आउटपुट असणे महत्वाचे आहे, जेणेकरुन

140
00:10:07,820 --> 00:10:09,340
आपण उतारावर थोडेसे पाऊल टाकून स्थानिक किमान शोधू शकू.

141
00:10:09,340 --> 00:10:14,140
म्हणूनच, बायनरी पद्धतीने सक्रिय किंवा

142
00:10:14,140 --> 00:10:18,580
निष्क्रिय राहण्याऐवजी कृत्रिम न्यूरॉन्समध्ये जैविक

143
00:10:18,580 --> 00:10:20,440
न्यूरॉन्सप्रमाणे सतत सक्रियता असते.

144
00:10:20,440 --> 00:10:24,600
नकारात्मक ग्रेडियंटच्या काही गुणाकाराने फंक्शनच्या इनपुटला वारंवार

145
00:10:24,600 --> 00:10:26,960
नडज करण्याच्या या प्रक्रियेला ग्रेडियंट डिसेंट म्हणतात.

146
00:10:26,960 --> 00:10:31,760
काही स्थानिक किमान खर्चाच्या फंक्शनकडे अभिसरण करण्याचा हा

147
00:10:31,760 --> 00:10:33,000
एक मार्ग आहे, मुळात या आलेखामध्ये एक दरी.

148
00:10:33,000 --> 00:10:37,040
मी अजूनही दोन इनपुटसह फंक्शनचे चित्र दाखवत आहे, अर्थातच, कारण 13,000

149
00:10:37,040 --> 00:10:41,480
डायमेन्शनल इनपुट स्पेसमधील नज हे तुमचे मन गुंडाळणे थोडे कठीण

150
00:10:41,480 --> 00:10:45,220
आहे, परंतु याविषयी विचार करण्याचा एक चांगला गैर-स्थानिक मार्ग आहे.

151
00:10:45,220 --> 00:10:49,100
ऋण ग्रेडियंटचा प्रत्येक घटक आपल्याला दोन गोष्टी सांगतो.

152
00:10:49,100 --> 00:10:53,600
इनपुट व्हेक्टरचा संबंधित घटक वर किंवा खाली ढकलायचा

153
00:10:53,600 --> 00:10:55,860
की नाही हे चिन्ह अर्थातच आम्हाला सांगते.

154
00:10:55,860 --> 00:11:01,340
पण महत्त्वाचे म्हणजे, या सर्व घटकांचे सापेक्ष परिमाण

155
00:11:01,340 --> 00:11:05,620
कोणते बदल अधिक महत्त्वाचे आहेत हे सांगते.

156
00:11:05,620 --> 00:11:09,780
तुम्ही पाहता, आमच्या नेटवर्कमध्ये, वजनांपैकी एकाचे समायोजन इतर

157
00:11:09,780 --> 00:11:14,980
वजनाच्या समायोजनापेक्षा किमतीच्या कार्यावर जास्त परिणाम करू शकते.

158
00:11:14,980 --> 00:11:19,440
यापैकी काही कनेक्शन आमच्या प्रशिक्षण डेटासाठी अधिक महत्त्वाचे आहेत.

159
00:11:19,440 --> 00:11:23,520
तर तुम्ही आमच्या मनाच्या प्रचंड खर्चाच्या कार्याच्या या ग्रेडियंट वेक्टरबद्दल विचार करू शकता

160
00:11:23,520 --> 00:11:29,740
हा एक मार्ग आहे की ते प्रत्येक वजन आणि पूर्वाग्रहाचे सापेक्ष महत्त्व एन्कोड

161
00:11:29,740 --> 00:11:34,100
करते, म्हणजेच, यापैकी कोणता बदल तुमच्या पैशासाठी सर्वात जास्त दणका देणार आहे.

162
00:11:34,100 --> 00:11:37,360
दिग्दर्शनाबद्दल विचार करण्याचा हा खरोखर दुसरा मार्ग आहे.

163
00:11:37,360 --> 00:11:41,740
सोप्या उदाहरणासाठी, जर तुमच्याकडे इनपुट म्हणून दोन व्हेरिएबल्ससह काही फंक्शन असेल

164
00:11:41,740 --> 00:11:48,720
आणि एखाद्या विशिष्ट बिंदूवर त्याचा ग्रेडियंट 3,1 म्हणून बाहेर येतो असे

165
00:11:48,720 --> 00:11:52,880
मोजले, तर एकीकडे तुम्ही याचा अर्थ असा लावू शकता की जेव्हा

166
00:11:52,880 --> 00:11:57,400
तुम्ही आहात त्या इनपुटवर उभे राहून, या दिशेला जाण्याने फंक्शन सर्वात

167
00:11:57,400 --> 00:12:02,200
लवकर वाढते, की जेव्हा तुम्ही इनपुट पॉइंट्सच्या समतल भागाच्या वरच्या फंक्शनचा

168
00:12:02,200 --> 00:12:03,200
आलेख बनवता तेव्हा तो वेक्टर तुम्हाला सरळ चढाची दिशा देतो.

169
00:12:03,200 --> 00:12:07,600
पण हे वाचण्याचा आणखी एक मार्ग म्हणजे या पहिल्या व्हेरिएबलमधील बदलांना

170
00:12:07,600 --> 00:12:12,400
दुसऱ्या व्हेरिएबलमधील बदलांपेक्षा तिप्पट महत्त्व आहे, म्हणजे किमान संबंधित इनपुटच्या

171
00:12:12,400 --> 00:12:17,740
शेजारी, x-व्हॅल्यूला धक्का लावणे आपल्यासाठी खूप मोठा धक्कादायक आहे. बोकड

172
00:12:17,740 --> 00:12:22,880
ठीक आहे, चला झूम कमी करू आणि आपण आतापर्यंत कुठे आहोत याची बेरीज करू.

173
00:12:22,880 --> 00:12:28,660
784 इनपुट आणि 10 आउटपुट असलेले हे नेटवर्क हे फंक्शन

174
00:12:28,660 --> 00:12:30,860
आहे, जे या सर्व भारित रकमांच्या संदर्भात परिभाषित केले आहे.

175
00:12:30,860 --> 00:12:34,160
कॉस्ट फंक्शन हा त्यावरील गुंतागुंतीचा एक थर आहे.

176
00:12:34,160 --> 00:12:39,300
हे इनपुट म्हणून 13,000 वजने आणि पूर्वाग्रह घेते

177
00:12:39,300 --> 00:12:42,640
आणि प्रशिक्षण उदाहरणांच्या आधारे एकच माप काढून टाकते.

178
00:12:42,640 --> 00:12:47,520
कॉस्ट फंक्शनचा ग्रेडियंट हा अजून एक गुंतागुंतीचा थर आहे.

179
00:12:47,520 --> 00:12:52,860
हे आम्हाला सांगते की या सर्व वजन आणि पूर्वाग्रहांना कोणकोणते धक्कादायक

180
00:12:52,860 --> 00:12:56,640
कारणे किंमत कार्याच्या मूल्यामध्ये सर्वात जलद बदल घडवून आणतात, ज्याचा तुम्ही

181
00:12:56,640 --> 00:13:03,040
अर्थ सांगू शकता की कोणत्या वजनांमध्ये कोणते बदल महत्त्वाचे आहेत.

182
00:13:03,040 --> 00:13:07,620
त्यामुळे जेव्हा तुम्ही यादृच्छिक वजन आणि पूर्वाग्रहांसह नेटवर्क सुरू करता आणि

183
00:13:07,620 --> 00:13:12,420
या ग्रेडियंट डिसेंट प्रक्रियेच्या आधारे ते अनेक वेळा समायोजित करता, तेव्हा

184
00:13:12,420 --> 00:13:14,240
ते यापूर्वी कधीही न पाहिलेल्या प्रतिमांवर किती चांगले कार्य करते?

185
00:13:14,240 --> 00:13:19,000
मी येथे वर्णन केलेले, प्रत्येकी 16 न्यूरॉन्सच्या दोन लपलेल्या स्तरांसह, मुख्यतः सौंदर्याच्या

186
00:13:19,000 --> 00:13:26,920
कारणांसाठी निवडले गेलेले, वाईट नाही, ते 96% नवीन प्रतिमा अचूकपणे वर्गीकृत करते.

187
00:13:26,920 --> 00:13:31,580
आणि प्रामाणिकपणे, जर तुम्ही त्यात काही गडबड केलेली उदाहरणे

188
00:13:31,580 --> 00:13:36,300
पाहिली तर तुम्हाला ते थोडे ढिले करणे भाग पडते.

189
00:13:36,300 --> 00:13:40,220
तुम्ही लपविलेल्या लेयर स्ट्रक्चरसह खेळल्यास आणि काही बदल

190
00:13:40,220 --> 00:13:41,220
केल्यास, तुम्ही हे 98% पर्यंत मिळवू शकता.

191
00:13:41,220 --> 00:13:42,900
आणि ते खूप चांगले आहे!

192
00:13:42,900 --> 00:13:47,020
हे सर्वोत्कृष्ट नाही, या प्लेन व्हॅनिला नेटवर्कपेक्षा अधिक अत्याधुनिक बनून तुम्ही नक्कीच चांगली

193
00:13:47,020 --> 00:13:52,460
कामगिरी मिळवू शकता, परंतु सुरुवातीचे काम किती कठीण आहे हे पाहता, मला

194
00:13:52,460 --> 00:13:56,800
वाटते की कोणत्याही नेटवर्कमध्ये प्रतिमांवर असे चांगले कार्य करत असताना याआधी कधीही

195
00:13:56,800 --> 00:14:02,000
न पाहिलेले काहीतरी अविश्वसनीय आहे. कोणते नमुने पहावेत असे कधीच सांगितले नाही.

196
00:14:02,000 --> 00:14:07,840
मूलतः, मी या संरचनेला प्रेरित करण्याचा मार्ग म्हणजे आमच्याकडे असलेल्या एका

197
00:14:07,840 --> 00:14:11,880
आशेचे वर्णन करून, की दुसरा थर लहान कडांवर उठू शकेल,

198
00:14:11,880 --> 00:14:16,080
की तिसरा थर लूप आणि लांब रेषा ओळखण्यासाठी त्या कडा

199
00:14:16,080 --> 00:14:18,220
एकत्र करेल आणि ते तुकडे केले जातील. अंक ओळखण्यासाठी एकत्र.

200
00:14:18,220 --> 00:14:21,040
मग आमचे नेटवर्क हेच करत आहे का?

201
00:14:21,040 --> 00:14:24,880
बरं, यासाठी किमान, अजिबात नाही.

202
00:14:24,960 --> 00:14:29,120
पहिल्या लेयरमधील सर्व न्यूरॉन्सपासून दुस-या लेयरमधील दिलेल्या न्यूरॉनच्या कनेक्शनचे वजन दुसऱ्या

203
00:14:29,120 --> 00:14:33,900
लेयरचे न्यूरॉन उचलत असलेल्या पिक्सेल पॅटर्नच्या रूपात कसे व्हिज्युअलाइज केले जाऊ

204
00:14:33,900 --> 00:14:37,440
शकते हे आम्ही शेवटचा व्हिडिओ कसा पाहिला ते लक्षात ठेवा?

205
00:14:37,440 --> 00:14:44,600
बरं, जेव्हा आपण या संक्रमणांशी संबंधित वजनांसाठी ते करतो,

206
00:14:44,600 --> 00:14:51,000
तेव्हा इकडे-तिकडे वेगळ्या छोट्या कडांवर उचलण्याऐवजी, ते अगदी बरोबर,

207
00:14:51,000 --> 00:14:54,200
जवळजवळ यादृच्छिक दिसतात, अगदी मध्यभागी काही अगदी सैल नमुन्यांसह.

208
00:14:54,200 --> 00:14:59,020
असे दिसते की संभाव्य वजन आणि पूर्वाग्रहांच्या अतुलनीय मोठ्या 13,000

209
00:14:59,020 --> 00:15:04,020
मितीय जागेत, आमच्या नेटवर्कला स्वतःला एक आनंदी थोडेसे स्थानिक

210
00:15:04,020 --> 00:15:08,440
आढळले आहे की, बहुतेक प्रतिमांचे यशस्वीरित्या वर्गीकरण करूनही, आम्ही ज्या

211
00:15:08,440 --> 00:15:09,840
नमुन्यांची अपेक्षा केली होती त्या नमुन्यांवर अचूकपणे उचलत नाही.

212
00:15:09,840 --> 00:15:14,600
आणि हा बिंदू खरोखर घरी आणण्यासाठी, तुम्ही यादृच्छिक प्रतिमा इनपुट करता तेव्हा काय होते ते पहा.

213
00:15:14,600 --> 00:15:19,240
जर सिस्टीम स्मार्ट असेल, तर तुम्हाला ती एकतर अनिश्चित वाटेल अशी अपेक्षा असू शकते, कदाचित त्या

214
00:15:19,240 --> 00:15:24,120
10 आउटपुट न्यूरॉन्सपैकी कोणतेही सक्रिय केले जात नाही किंवा ते सर्व समान रीतीने सक्रिय केले जात

215
00:15:24,520 --> 00:15:29,800
नाही, परंतु त्याऐवजी ते आत्मविश्वासाने तुम्हाला काही निरर्थक उत्तर देते, जसे की हे यादृच्छिक असल्याची खात्री

216
00:15:29,800 --> 00:15:34,560
वाटते. नॉइज हा 5 आहे कारण ते असे करते की 5 ची वास्तविक प्रतिमा 5 आहे.

217
00:15:34,560 --> 00:15:39,300
वेगळ्या पद्धतीने शब्दबद्ध केले, जरी हे नेटवर्क अंक अगदी चांगल्या प्रकारे

218
00:15:39,300 --> 00:15:41,800
ओळखू शकत असले तरी, ते कसे काढायचे याची कल्पना नाही.

219
00:15:41,800 --> 00:15:45,400
यापैकी बरेच काही कारण हे इतके घट्ट विवशित प्रशिक्षण सेटअप आहे.

220
00:15:45,400 --> 00:15:48,220
म्हणजे, इथे स्वतःला नेटवर्कच्या शूजमध्ये ठेवा.

221
00:15:48,220 --> 00:15:53,280
त्याच्या दृष्टीकोनातून, संपूर्ण विश्वामध्ये एका लहान ग्रिडमध्ये केंद्रस्थानी असलेल्या स्पष्टपणे

222
00:15:53,280 --> 00:15:58,560
परिभाषित अचल अंकांशिवाय काहीही नाही, आणि त्याच्या किमतीच्या कार्यामुळे त्याला

223
00:15:58,560 --> 00:16:02,160
त्याच्या निर्णयांवर पूर्ण विश्वास असल्याशिवाय काहीही होण्यासाठी प्रोत्साहन दिले नाही.

224
00:16:02,160 --> 00:16:05,760
त्यामुळे हे दुसरे लेयर न्यूरॉन्स खरोखर काय करत आहेत याची

225
00:16:05,760 --> 00:16:09,320
प्रतिमा म्हणून, तुम्हाला कदाचित आश्चर्य वाटेल की मी हे

226
00:16:09,320 --> 00:16:10,320
नेटवर्क कडा आणि नमुने उचलण्याच्या प्रेरणेने का सादर करू.

227
00:16:10,320 --> 00:16:13,040
मला म्हणायचे आहे की, ते असेच करत नाही.

228
00:16:13,040 --> 00:16:17,480
बरं, हे आमचे अंतिम उद्दिष्ट नसून त्याऐवजी प्रारंभ बिंदू आहे.

229
00:16:17,480 --> 00:16:22,280
खरे सांगायचे तर, हे जुने तंत्रज्ञान आहे, ज्याचे 80 आणि 90 च्या दशकात संशोधन

230
00:16:22,280 --> 00:16:26,920
केले गेले होते, आणि अधिक तपशीलवार आधुनिक रूपे समजून घेण्यापूर्वी तुम्हाला ते समजून

231
00:16:26,920 --> 00:16:31,380
घेणे आवश्यक आहे, आणि हे स्पष्टपणे काही मनोरंजक समस्या सोडविण्यास सक्षम आहे, परंतु तुम्ही

232
00:16:31,380 --> 00:16:38,720
जितके अधिक जाणून घ्याल ते लपलेले स्तर खरोखर करत आहेत, कमी हुशार दिसते.

233
00:16:38,720 --> 00:16:43,540
तुम्ही कसे शिकता याकडे नेटवर्क्स कसे शिकतात यावरून क्षणभर फोकस हलवणे,

234
00:16:43,540 --> 00:16:47,160
हे केवळ तेव्हाच होईल जेव्हा तुम्ही इथल्या सामग्रीमध्ये सक्रियपणे गुंतलात.

235
00:16:47,160 --> 00:16:51,920
एक अतिशय सोपी गोष्ट मला हवी आहे की तुम्ही आत्ताच थांबा आणि तुम्ही या

236
00:16:51,920 --> 00:16:57,560
प्रणालीमध्ये कोणते बदल करू शकता आणि किनारी आणि नमुने यांसारख्या गोष्टींना अधिक चांगल्या प्रकारे

237
00:16:57,560 --> 00:17:01,880
उचलण्याची तुमची इच्छा असेल तर ती प्रतिमा कशी पाहते याबद्दल क्षणभर सखोल विचार करा.

238
00:17:01,880 --> 00:17:06,360
परंतु त्याहूनही चांगले, सामग्रीशी प्रत्यक्षात गुंतण्यासाठी, मी सखोल शिक्षण

239
00:17:06,360 --> 00:17:09,720
आणि न्यूरल नेटवर्कवर मायकेल निल्सन यांच्या पुस्तकाची शिफारस करतो.

240
00:17:09,720 --> 00:17:15,200
त्यामध्ये, तुम्हाला या अचूक उदाहरणासाठी डाउनलोड आणि प्ले करण्यासाठी कोड आणि डेटा मिळू

241
00:17:15,200 --> 00:17:19,360
शकेल आणि तो कोड काय करत आहे हे पुस्तक तुम्हाला टप्प्याटप्प्याने सांगेल.

242
00:17:19,360 --> 00:17:23,920
आश्चर्यकारक गोष्ट म्हणजे हे पुस्तक विनामूल्य आणि सार्वजनिकरित्या उपलब्ध आहे, त्यामुळे जर तुम्हाला

243
00:17:23,920 --> 00:17:28,040
त्यातून काही मिळाले तर, निल्सनच्या प्रयत्नांसाठी देणगी देण्यासाठी माझ्याशी सामील होण्याचा विचार करा.

244
00:17:28,040 --> 00:17:32,060
ख्रिस ओला ची अभूतपूर्व आणि सुंदर ब्लॉग पोस्ट आणि डिस्टिल मधील लेखांसह

245
00:17:32,060 --> 00:17:38,720
मी वर्णनात मला खूप आवडणारी काही इतर संसाधने देखील जोडली आहेत.

246
00:17:38,720 --> 00:17:41,960
शेवटच्या काही मिनिटांसाठी येथे गोष्टी बंद करण्यासाठी, मी लीशा

247
00:17:41,960 --> 00:17:44,440
लीसोबत घेतलेल्या मुलाखतीच्या एका स्निपेटमध्ये परत येऊ इच्छितो.

248
00:17:44,440 --> 00:17:48,520
तुम्हाला कदाचित शेवटच्या व्हिडिओवरून आठवत असेल, तिने तिचे पीएचडीचे काम डीप लर्निंगमध्ये केले.

249
00:17:48,560 --> 00:17:52,240
या छोट्या झलकामध्ये, ती अलीकडील दोन पेपर्सबद्दल बोलते जे खरोखरच काही

250
00:17:52,240 --> 00:17:56,380
अधिक आधुनिक प्रतिमा ओळख नेटवर्क प्रत्यक्षात कसे शिकत आहेत हे शोधतात.

251
00:17:56,380 --> 00:18:00,320
आम्ही संभाषणात कुठे होतो ते सेट करण्यासाठी, पहिल्या पेपरने यापैकी एक विशेषत:

252
00:18:00,320 --> 00:18:04,480
खोल न्यूरल नेटवर्क घेतले जे इमेज रेकग्निशनमध्ये खरोखर चांगले आहे आणि त्यास

253
00:18:04,480 --> 00:18:09,400
योग्यरित्या लेबल केलेल्या डेटासेटवर प्रशिक्षण देण्याऐवजी, प्रशिक्षणापूर्वी आसपासची सर्व लेबले बदलली.

254
00:18:09,400 --> 00:18:13,840
साहजिकच येथे चाचणी अचूकता यादृच्छिक पेक्षा चांगली असणार

255
00:18:13,840 --> 00:18:15,320
नाही, कारण सर्वकाही फक्त यादृच्छिकपणे लेबल केलेले आहे.

256
00:18:15,320 --> 00:18:20,080
पण तरीही तुम्ही योग्यरित्या लेबल केलेल्या डेटासेटवर

257
00:18:20,080 --> 00:18:21,440
तीच प्रशिक्षण अचूकता प्राप्त करू शकले.

258
00:18:21,440 --> 00:18:26,120
मुळात, या विशिष्ट नेटवर्कसाठी लाखो वजने केवळ यादृच्छिक डेटा लक्षात ठेवण्यासाठी पुरेसे होते,

259
00:18:26,120 --> 00:18:31,040
ज्यामुळे हा प्रश्न निर्माण होतो की हे खर्चाचे कार्य कमी करणे खरोखर

260
00:18:31,040 --> 00:18:36,720
प्रतिमेतील कोणत्याही प्रकारच्या संरचनेशी सुसंगत आहे किंवा ते फक्त लक्षात ठेवणे आहे?

261
00:18:36,720 --> 00:18:40,120
. . . योग्य वर्गीकरण काय आहे याचा संपूर्ण डेटासेट लक्षात ठेवण्यासाठी.

262
00:18:40,120 --> 00:18:45,720
आणि म्हणून काही, तुम्हाला माहिती आहे की, या वर्षी ICML मध्ये अर्ध्या वर्षांनंतर,

263
00:18:45,720 --> 00:18:50,440
तंतोतंत खंडन करणारा पेपर नव्हता, परंतु पेपर होता ज्याने काही पैलूंवर लक्ष दिले

264
00:18:50,440 --> 00:18:52,220
होते, अहो, खरं तर ही नेटवर्क्स त्यापेक्षा थोडे अधिक स्मार्ट करत आहेत.

265
00:18:52,220 --> 00:18:59,600
जर तुम्ही अचूकता वक्र पाहिल्यास, जर तुम्ही फक्त एका यादृच्छिक डेटासेटवर प्रशिक्षण घेत असाल, तर तो

266
00:18:59,600 --> 00:19:05,240
वक्र प्रकार खूपच कमी झाला आहे, तुम्हाला माहिती आहे, जवळजवळ एक रेखीय फॅशनमध्ये खूप हळू.

267
00:19:05,280 --> 00:19:10,840
त्यामुळे तुम्‍हाला त्‍याची अचूकता मिळवून देण्‍यासाठी, तुम्‍हाला माहीत आहे, तुम्‍हाला

268
00:19:10,840 --> 00:19:12,320
शक्य असलेल्‍या स्‍थानिक मिनिमा शोधण्‍यासाठी खरोखर धडपड होत आहे.

269
00:19:12,320 --> 00:19:16,720
जर तुम्ही खरोखर एखाद्या संरचित डेटासेटवर प्रशिक्षण घेत असाल, ज्यामध्ये योग्य

270
00:19:16,720 --> 00:19:20,240
लेबले असतील, तर तुम्हाला माहिती आहे की, सुरुवातीला तुम्ही थोडेसे फिरता,

271
00:19:20,240 --> 00:19:23,360
परंतु नंतर त्या अचूकतेच्या पातळीवर जाण्यासाठी तुम्ही खूप वेगाने घसरले.

272
00:19:23,360 --> 00:19:28,580
आणि म्हणून काही अर्थाने स्थानिक मॅक्सिमा शोधणे सोपे होते.

273
00:19:28,580 --> 00:19:32,900
आणि म्हणूनच त्याबद्दल मनोरंजक गोष्ट म्हणजे काही

274
00:19:32,900 --> 00:19:39,140
वर्षांपूर्वीचा आणखी एक पेपर प्रकाशात आणतो, ज्यामध्ये

275
00:19:39,140 --> 00:19:40,140
नेटवर्क स्तरांबद्दल बरेच अधिक सरलीकरण आहे.

276
00:19:40,140 --> 00:19:43,880
परंतु परिणामांपैकी एक असे सांगत होता की, जर तुम्ही ऑप्टिमायझेशन लँडस्केप पाहिल्यास,

277
00:19:43,880 --> 00:19:49,400
या नेटवर्क्सकडून शिकण्याची प्रवृत्ती असलेली स्थानिक मिनिमा प्रत्यक्षात समान दर्जाची कशी आहे.

278
00:19:49,400 --> 00:19:54,300
त्यामुळे काही अर्थाने, जर तुमचा डेटा संच संरचित असेल, तर तुम्ही ते अधिक सहजपणे शोधण्यास सक्षम असाल.

279
00:19:58,580 --> 00:20:01,140
तुमच्यापैकी ज्यांनी Patreon ला पाठिंबा दिला त्यांचे मी नेहमीप्रमाणे आभारी आहे.

280
00:20:01,480 --> 00:20:05,440
Patreon वर गेम चेंजर काय आहे हे मी आधी सांगितले

281
00:20:05,440 --> 00:20:07,160
आहे, परंतु हे व्हिडिओ खरोखर तुमच्याशिवाय शक्य होणार नाहीत.

282
00:20:07,160 --> 00:20:11,540
मी VC फर्म Amplify Partners चे आणि मालिकेतील या

283
00:20:11,540 --> 00:20:13,240
सुरुवातीच्या व्हिडिओंना त्यांच्या समर्थनाचे देखील विशेष आभार मानू इच्छितो.

284
00:20:31,140 --> 00:20:33,140
धन्यवाद.

