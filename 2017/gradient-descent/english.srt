1
00:00:00,000 --> 00:00:07,240
Last video I laid out the structure of a neural network.

2
00:00:07,240 --> 00:00:11,560
I'll give a quick recap here so that it's fresh in our minds, and then I have two main

3
00:00:11,560 --> 00:00:13,160
goals for this video.

4
00:00:13,160 --> 00:00:17,960
The first is to introduce the idea of gradient descent, which underlies not only how neural

5
00:00:17,960 --> 00:00:20,800
networks learn, but how a lot of other machine learning works as well.

6
00:00:20,800 --> 00:00:25,160
Then after that we'll dig in a little more into how this particular network performs,

7
00:00:25,160 --> 00:00:29,560
and what those hidden layers of neurons end up looking for.

8
00:00:29,560 --> 00:00:34,680
As a reminder, our goal here is the classic example of handwritten digit recognition,

9
00:00:34,680 --> 00:00:37,080
the hello world of neural networks.

10
00:00:37,080 --> 00:00:42,160
These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value

11
00:00:42,160 --> 00:00:44,260
between 0 and 1.

12
00:00:44,260 --> 00:00:51,400
Those are what determine the activations of 784 neurons in the input layer of the network.

13
00:00:51,400 --> 00:00:56,880
The activation for each neuron in the following layers is based on a weighted sum of all the

14
00:00:56,880 --> 00:01:02,300
activations in the previous layer, plus some special number called a bias.

15
00:01:02,300 --> 00:01:07,480
You compose that sum with some other function, like the sigmoid squishification, or a ReLU,

16
00:01:07,480 --> 00:01:09,640
the way I walked through last video.

17
00:01:09,640 --> 00:01:14,960
In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each,

18
00:01:14,960 --> 00:01:20,940
the network has about 13,000 weights and biases that we can adjust, and it's these values

19
00:01:20,940 --> 00:01:25,320
that determine what exactly the network actually does.

20
00:01:25,320 --> 00:01:29,800
And what we mean when we say that this network classifies a given digit is that the brightest

21
00:01:29,800 --> 00:01:34,080
of those 10 neurons in the final layer corresponds to that digit.

22
00:01:34,080 --> 00:01:39,240
And remember, the motivation we had in mind for the layered structure was that maybe the

23
00:01:39,240 --> 00:01:43,920
second layer could pick up on the edges, the third layer might pick up on patterns like

24
00:01:43,920 --> 00:01:48,640
loops and lines, and the last one could just piece together those patterns to recognize

25
00:01:48,640 --> 00:01:49,640
digits.

26
00:01:49,640 --> 00:01:52,880
So here, we learn how the network learns.

27
00:01:52,880 --> 00:01:56,880
What we want is an algorithm where you can show this network a whole bunch of training

28
00:01:56,880 --> 00:02:01,540
data, which comes in the form of a bunch of different images of handwritten digits, along

29
00:02:01,540 --> 00:02:06,360
with labels for what they're supposed to be, and it'll adjust those 13,000 weights

30
00:02:06,360 --> 00:02:10,760
and biases so as to improve its performance on the training data.

31
00:02:10,760 --> 00:02:15,540
Hopefully this layered structure will mean that what it learns generalizes to images

32
00:02:15,540 --> 00:02:17,840
beyond that training data.

33
00:02:17,840 --> 00:02:22,240
The way we test that is that after you train the network, you show it more labeled data,

34
00:02:22,240 --> 00:02:31,160
and you see how accurately it classifies those new images.

35
00:02:31,160 --> 00:02:34,760
Fortunately for us, and what makes this a common example to start with, is that the

36
00:02:34,760 --> 00:02:39,520
good people behind the MNIST database have put together a collection of tens of thousands

37
00:02:39,520 --> 00:02:45,080
of handwritten digit images, each labeled with the numbers they're supposed to be.

38
00:02:45,080 --> 00:02:49,920
And as provocative as it is to describe a machine as learning, once you see how it works,

39
00:02:49,920 --> 00:02:55,560
it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.

40
00:02:55,560 --> 00:03:01,040
I mean, basically it comes down to finding the minimum of a certain function.

41
00:03:01,040 --> 00:03:06,480
Remember, conceptually we're thinking of each neuron as being connected to all of the

42
00:03:06,480 --> 00:03:11,440
neurons in the previous layer, and the weights in the weighted sum defining its activation

43
00:03:11,440 --> 00:03:16,400
are kind of like the strengths of those connections, and the bias is some indication of whether

44
00:03:16,400 --> 00:03:19,780
that neuron tends to be active or inactive.

45
00:03:19,780 --> 00:03:23,300
And to start things off, we're just going to initialize all of those weights and biases

46
00:03:23,300 --> 00:03:25,020
totally randomly.

47
00:03:25,020 --> 00:03:29,100
Needless to say, this network is going to perform horribly on a given training example,

48
00:03:29,100 --> 00:03:31,180
since it's just doing something random.

49
00:03:31,180 --> 00:03:36,820
For example, you feed in this image of a 3, and the output layer just looks like a mess.

50
00:03:36,820 --> 00:03:43,340
So what you do is define a cost function, a way of telling the computer, no, bad computer,

51
00:03:43,340 --> 00:03:48,940
that output should have activations which are 0 for most neurons, but 1 for this neuron.

52
00:03:48,980 --> 00:03:51,740
What you gave me is utter trash.

53
00:03:51,740 --> 00:03:56,740
To say that a little more mathematically, you add up the squares of the differences

54
00:03:56,740 --> 00:04:01,980
between each of those trash output activations and the value you want them to have, and this

55
00:04:01,980 --> 00:04:06,020
is what we'll call the cost of a single training example.

56
00:04:06,020 --> 00:04:12,660
Notice this sum is small when the network confidently classifies the image correctly,

57
00:04:12,660 --> 00:04:18,820
but it's large when the network seems like it doesn't know what it's doing.

58
00:04:18,820 --> 00:04:23,860
So then what you do is consider the average cost over all of the tens of thousands of

59
00:04:23,860 --> 00:04:27,580
training examples at your disposal.

60
00:04:27,580 --> 00:04:32,300
This average cost is our measure for how lousy the network is, and how bad the computer should

61
00:04:32,300 --> 00:04:33,300
feel.

62
00:04:33,300 --> 00:04:35,300
And that's a complicated thing.

63
00:04:35,300 --> 00:04:40,380
Remember how the network itself was basically a function, one that takes in 784 numbers

64
00:04:40,380 --> 00:04:46,100
as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's

65
00:04:46,100 --> 00:04:49,700
parameterized by all these weights and biases?

66
00:04:49,700 --> 00:04:53,340
The cost function is a layer of complexity on top of that.

67
00:04:53,340 --> 00:04:59,140
It takes as its input those 13,000 or so weights and biases, and spits out a single number

68
00:04:59,140 --> 00:05:04,620
describing how bad those weights and biases are, and the way it's defined depends on

69
00:05:04,620 --> 00:05:09,140
the network's behavior over all the tens of thousands of pieces of training data.

70
00:05:09,140 --> 00:05:12,460
That's a lot to think about.

71
00:05:12,460 --> 00:05:16,380
But just telling the computer what a crappy job it's doing isn't very helpful.

72
00:05:16,380 --> 00:05:21,300
You want to tell it how to change those weights and biases so that it gets better.

73
00:05:21,300 --> 00:05:25,580
To make it easier, rather than struggling to imagine a function with 13,000 inputs,

74
00:05:25,580 --> 00:05:31,440
just imagine a simple function that has one number as an input and one number as an output.

75
00:05:31,440 --> 00:05:36,420
How do you find an input that minimizes the value of this function?

76
00:05:36,420 --> 00:05:41,300
Calculus students will know that you can sometimes figure out that minimum explicitly, but that's

77
00:05:41,340 --> 00:05:46,620
not always feasible for really complicated functions, certainly not in the 13,000 input

78
00:05:46,620 --> 00:05:51,640
version of this situation for our crazy complicated neural network cost function.

79
00:05:51,640 --> 00:05:56,820
A more flexible tactic is to start at any input, and figure out which direction you

80
00:05:56,820 --> 00:05:59,860
should step to make that output lower.

81
00:05:59,860 --> 00:06:05,020
Specifically, if you can figure out the slope of the function where you are, then shift

82
00:06:05,020 --> 00:06:09,280
to the left if that slope is positive, and shift the input to the right if that slope

83
00:06:09,280 --> 00:06:12,720
is negative.

84
00:06:12,720 --> 00:06:17,040
If you do this repeatedly, at each point checking the new slope and taking the appropriate step,

85
00:06:17,040 --> 00:06:20,680
you're going to approach some local minimum of the function.

86
00:06:20,680 --> 00:06:24,600
And the image you might have in mind here is a ball rolling down a hill.

87
00:06:24,600 --> 00:06:29,380
And notice, even for this really simplified single input function, there are many possible

88
00:06:29,380 --> 00:06:34,220
valleys you might land in, depending on which random input you start at, and there's no

89
00:06:34,220 --> 00:06:38,460
guarantee that the local minimum you land in is going to be the smallest possible value

90
00:06:38,460 --> 00:06:39,460
of the cost function.

91
00:06:39,460 --> 00:06:43,180
That's going to carry over to our neural network case as well.

92
00:06:43,180 --> 00:06:48,140
And I also want you to notice how if you make your step sizes proportional to the slope,

93
00:06:48,140 --> 00:06:52,920
then when the slope is flattening out towards the minimum, your steps get smaller and smaller,

94
00:06:52,920 --> 00:06:56,020
and that kind of helps you from overshooting.

95
00:06:56,020 --> 00:07:01,640
Bumping up the complexity a bit, imagine instead a function with two inputs and one output.

96
00:07:01,640 --> 00:07:06,360
You might think of the input space as the xy-plane, and the cost function as being graphed

97
00:07:06,360 --> 00:07:09,020
as a surface above it.

98
00:07:09,020 --> 00:07:13,600
Instead of asking about the slope of the function, you have to ask which direction you should

99
00:07:13,600 --> 00:07:19,780
step in this input space so as to decrease the output of the function most quickly.

100
00:07:19,780 --> 00:07:22,340
In other words, what's the downhill direction?

101
00:07:22,340 --> 00:07:26,740
And again, it's helpful to think of a ball rolling down that hill.

102
00:07:26,740 --> 00:07:31,920
Those of you familiar with multivariable calculus will know that the gradient of a function

103
00:07:31,920 --> 00:07:37,460
gives you the direction of steepest ascent, which direction should you step to increase

104
00:07:37,460 --> 00:07:39,420
the function most quickly.

105
00:07:39,420 --> 00:07:43,820
Naturally enough, taking the negative of that gradient gives you the direction to step that

106
00:07:43,820 --> 00:07:47,460
decreases the function most quickly.

107
00:07:47,460 --> 00:07:52,320
Even more than that, the length of this gradient vector is an indication for just how steep

108
00:07:52,320 --> 00:07:54,580
that steepest slope is.

109
00:07:54,580 --> 00:07:58,080
Now if you're unfamiliar with multivariable calculus and want to learn more, check out

110
00:07:58,080 --> 00:08:01,100
some of the work I did for Khan Academy on the topic.

111
00:08:01,100 --> 00:08:05,680
Honestly though, all that matters for you and me right now is that in principle there

112
00:08:05,680 --> 00:08:10,440
exists a way to compute this vector, this vector that tells you what the downhill direction

113
00:08:10,440 --> 00:08:12,040
is and how steep it is.

114
00:08:12,040 --> 00:08:17,280
You'll be okay if that's all you know and you're not rock solid on the details.

115
00:08:17,280 --> 00:08:21,440
Because if you can get that, the algorithm for minimizing the function is to compute

116
00:08:21,440 --> 00:08:27,400
this gradient direction, then take a small step downhill, and repeat that over and over.

117
00:08:28,300 --> 00:08:33,700
It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.

118
00:08:33,700 --> 00:08:38,980
Imagine organizing all 13,000 weights and biases of our network into a giant column

119
00:08:38,980 --> 00:08:40,180
vector.

120
00:08:40,180 --> 00:08:46,140
The negative gradient of the cost function is just a vector, it's some direction inside

121
00:08:46,140 --> 00:08:51,660
this insanely huge input space that tells you which nudges to all of those numbers is

122
00:08:51,660 --> 00:08:55,900
going to cause the most rapid decrease to the cost function.

123
00:08:55,900 --> 00:09:00,000
And of course, with our specially designed cost function, changing the weights and biases

124
00:09:00,000 --> 00:09:05,520
to decrease it means making the output of the network on each piece of training data

125
00:09:05,520 --> 00:09:10,280
look less like a random array of 10 values, and more like an actual decision we want it

126
00:09:10,280 --> 00:09:11,280
to make.

127
00:09:11,280 --> 00:09:15,940
It's important to remember, this cost function involves an average over all of the training

128
00:09:15,940 --> 00:09:24,260
data, so if you minimize it, it means it's a better performance on all of those samples.

129
00:09:24,260 --> 00:09:28,540
The algorithm for computing this gradient efficiently, which is effectively the heart

130
00:09:28,540 --> 00:09:32,520
of how a neural network learns, is called backpropagation, and it's what I'm going

131
00:09:32,520 --> 00:09:34,040
to be talking about next video.

132
00:09:34,040 --> 00:09:39,100
There, I really want to take the time to walk through what exactly happens to each weight

133
00:09:39,100 --> 00:09:44,100
and bias for a given piece of training data, trying to give an intuitive feel for what's

134
00:09:44,100 --> 00:09:47,980
happening beyond the pile of relevant calculus and formulas.

135
00:09:47,980 --> 00:09:51,780
Right here, right now, the main thing I want you to know, independent of implementation

136
00:09:51,780 --> 00:09:56,820
details, is that what we mean when we talk about a network learning is that it's just

137
00:09:56,820 --> 00:09:59,320
minimizing a cost function.

138
00:09:59,320 --> 00:10:02,760
And notice, one consequence of that is that it's important for this cost function to

139
00:10:02,760 --> 00:10:07,820
have a nice smooth output, so that we can find a local minimum by taking little steps

140
00:10:07,820 --> 00:10:09,340
downhill.

141
00:10:09,340 --> 00:10:14,140
This is why, by the way, artificial neurons have continuously ranging activations, rather

142
00:10:14,140 --> 00:10:18,580
than simply being active or inactive in a binary way, the way that biological neurons

143
00:10:18,580 --> 00:10:20,440
are.

144
00:10:20,440 --> 00:10:24,600
This process of repeatedly nudging an input of a function by some multiple of the negative

145
00:10:24,600 --> 00:10:26,960
gradient is called gradient descent.

146
00:10:26,960 --> 00:10:31,760
It's a way to converge towards some local minimum of a cost function, basically a valley

147
00:10:31,760 --> 00:10:33,000
in this graph.

148
00:10:33,000 --> 00:10:37,040
I'm still showing the picture of a function with two inputs, of course, because nudges

149
00:10:37,040 --> 00:10:41,480
in a 13,000 dimensional input space are a little hard to wrap your mind around, but

150
00:10:41,480 --> 00:10:45,220
there is actually a nice non-spatial way to think about this.

151
00:10:45,220 --> 00:10:49,100
Each component of the negative gradient tells us two things.

152
00:10:49,100 --> 00:10:53,600
The sign, of course, tells us whether the corresponding component of the input vector

153
00:10:53,600 --> 00:10:55,860
should be nudged up or down.

154
00:10:55,860 --> 00:11:01,340
But importantly, the relative magnitudes of all these components kind of tells you which

155
00:11:01,340 --> 00:11:05,620
changes matter more.

156
00:11:05,620 --> 00:11:09,780
You see, in our network, an adjustment to one of the weights might have a much greater

157
00:11:09,780 --> 00:11:14,980
impact on the cost function than the adjustment to some other weight.

158
00:11:14,980 --> 00:11:19,440
Some of these connections just matter more for our training data.

159
00:11:19,440 --> 00:11:23,520
So a way you can think about this gradient vector of our mind-warpingly massive cost

160
00:11:23,520 --> 00:11:29,740
function is that it encodes the relative importance of each weight and bias, that is, which of

161
00:11:29,740 --> 00:11:34,100
these changes is going to carry the most bang for your buck.

162
00:11:34,100 --> 00:11:37,360
This really is just another way of thinking about direction.

163
00:11:37,360 --> 00:11:41,740
To take a simpler example, if you have some function with two variables as an input, and

164
00:11:41,740 --> 00:11:48,720
compute that its gradient at some particular point comes out as 3,1, then on the one hand

165
00:11:48,720 --> 00:11:52,880
you can interpret that as saying that when you're standing at that input, moving along

166
00:11:52,880 --> 00:11:57,400
this direction increases the function most quickly, that when you graph the function

167
00:11:57,400 --> 00:12:02,200
above the plane of input points, that vector is what's giving you the straight uphill

168
00:12:02,200 --> 00:12:03,200
direction.

169
00:12:03,200 --> 00:12:07,600
But another way to read that is to say that changes to this first variable have three

170
00:12:07,600 --> 00:12:12,400
times the importance as changes to the second variable, that at least in the neighborhood

171
00:12:12,400 --> 00:12:17,740
of the relevant input, nudging the x-value carries a lot more bang for your buck.

172
00:12:17,740 --> 00:12:22,880
Alright, let's zoom out and sum up where we are so far.

173
00:12:22,880 --> 00:12:28,660
The network itself is this function with 784 inputs and 10 outputs, defined in terms of

174
00:12:28,660 --> 00:12:30,860
all of these weighted sums.

175
00:12:30,860 --> 00:12:34,160
The cost function is a layer of complexity on top of that.

176
00:12:34,160 --> 00:12:39,300
It takes the 13,000 weights and biases as inputs, and spits out a single measure of

177
00:12:39,300 --> 00:12:42,640
lousiness based on the training examples.

178
00:12:42,640 --> 00:12:47,520
The gradient of the cost function is one more layer of complexity still.

179
00:12:47,520 --> 00:12:52,860
It tells us what nudges to all of these weights and biases cause the fastest change to the

180
00:12:52,860 --> 00:12:56,640
value of the cost function, which you might interpret as saying which changes to which

181
00:12:56,640 --> 00:13:03,040
weights matter the most.

182
00:13:03,040 --> 00:13:07,620
So when you initialize the network with random weights and biases, and adjust them many times

183
00:13:07,620 --> 00:13:12,420
based on this gradient descent process, how well does it actually perform on images it's

184
00:13:12,420 --> 00:13:14,240
never seen before?

185
00:13:14,240 --> 00:13:19,000
The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly

186
00:13:19,000 --> 00:13:26,920
for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.

187
00:13:26,920 --> 00:13:31,580
And honestly, if you look at some of the examples it messes up on, you feel compelled to cut

188
00:13:31,580 --> 00:13:36,300
it a little slack.

189
00:13:36,300 --> 00:13:40,220
If you play around with the hidden layer structure and make a couple tweaks, you can get this

190
00:13:40,220 --> 00:13:41,220
up to 98%.

191
00:13:41,220 --> 00:13:42,900
And that's pretty good!

192
00:13:42,900 --> 00:13:47,020
It's not the best, you can certainly get better performance by getting more sophisticated

193
00:13:47,020 --> 00:13:52,460
than this plain vanilla network, but given how daunting the initial task is, I think

194
00:13:52,460 --> 00:13:56,800
there's something incredible about any network doing this well on images it's never seen

195
00:13:56,800 --> 00:14:02,000
before given that we never specifically told it what patterns to look for.

196
00:14:02,000 --> 00:14:07,840
Originally, the way I motivated this structure was by describing a hope we might have, that

197
00:14:07,840 --> 00:14:11,880
the second layer might pick up on little edges, that the third layer would piece together

198
00:14:11,880 --> 00:14:16,080
those edges to recognize loops and longer lines, and that those might be pieced together

199
00:14:16,080 --> 00:14:18,220
to recognize digits.

200
00:14:18,220 --> 00:14:21,040
So is this what our network is actually doing?

201
00:14:21,040 --> 00:14:24,880
Well, for this one at least, not at all.

202
00:14:24,960 --> 00:14:29,120
Remember how last video we looked at how the weights of the connections from all the neurons

203
00:14:29,120 --> 00:14:33,900
in the first layer to a given neuron in the second layer can be visualized as a given

204
00:14:33,900 --> 00:14:37,440
pixel pattern that the second layer neuron is picking up on?

205
00:14:37,440 --> 00:14:44,600
Well, when we do that for the weights associated with these transitions, instead of picking

206
00:14:44,600 --> 00:14:51,000
up on isolated little edges here and there, they look, well, almost random, just with

207
00:14:51,000 --> 00:14:54,200
some very loose patterns in the middle.

208
00:14:54,200 --> 00:14:59,020
It would seem that in the unfathomably large 13,000 dimensional space of possible weights

209
00:14:59,020 --> 00:15:04,020
and biases, our network found itself a happy little local minimum that, despite successfully

210
00:15:04,020 --> 00:15:08,440
classifying most images, doesn't exactly pick up on the patterns we might have hoped

211
00:15:08,440 --> 00:15:09,840
for.

212
00:15:09,840 --> 00:15:14,600
And to really drive this point home, watch what happens when you input a random image.

213
00:15:14,600 --> 00:15:19,240
If the system was smart, you might expect it to either feel uncertain, maybe not really

214
00:15:19,240 --> 00:15:24,120
activating any of those 10 output neurons or activating them all evenly, but instead

215
00:15:24,520 --> 00:15:29,800
it confidently gives you some nonsense answer, as if it feels as sure that this random noise

216
00:15:29,800 --> 00:15:34,560
is a 5 as it does that an actual image of a 5 is a 5.

217
00:15:34,560 --> 00:15:39,300
Phrased differently, even if this network can recognize digits pretty well, it has no

218
00:15:39,300 --> 00:15:41,800
idea how to draw them.

219
00:15:41,800 --> 00:15:45,400
A lot of this is because it's such a tightly constrained training setup.

220
00:15:45,400 --> 00:15:48,220
I mean, put yourself in the network's shoes here.

221
00:15:48,220 --> 00:15:53,280
From its point of view, the entire universe consists of nothing but clearly defined unmoving

222
00:15:53,280 --> 00:15:58,560
digits centered in a tiny grid, and its cost function never gave it any incentive to be

223
00:15:58,560 --> 00:16:02,160
anything but utterly confident in its decisions.

224
00:16:02,160 --> 00:16:05,760
So with this as the image of what those second layer neurons are really doing, you might

225
00:16:05,760 --> 00:16:09,320
wonder why I would introduce this network with the motivation of picking up on edges

226
00:16:09,320 --> 00:16:10,320
and patterns.

227
00:16:10,320 --> 00:16:13,040
I mean, that's just not at all what it ends up doing.

228
00:16:13,040 --> 00:16:17,480
Well, this is not meant to be our end goal, but instead a starting point.

229
00:16:17,480 --> 00:16:22,280
Frankly, this is old technology, the kind researched in the 80s and 90s, and you do

230
00:16:22,280 --> 00:16:26,920
need to understand it before you can understand more detailed modern variants, and it clearly

231
00:16:26,920 --> 00:16:31,380
is capable of solving some interesting problems, but the more you dig into what those hidden

232
00:16:31,380 --> 00:16:38,720
layers are really doing, the less intelligent it seems.

233
00:16:38,720 --> 00:16:43,540
Shifting the focus for a moment from how networks learn to how you learn, that'll only happen

234
00:16:43,540 --> 00:16:47,160
if you engage actively with the material here somehow.

235
00:16:47,160 --> 00:16:51,920
One pretty simple thing I want you to do is just pause right now and think deeply for

236
00:16:51,920 --> 00:16:57,560
a moment about what changes you might make to this system and how it perceives images

237
00:16:57,560 --> 00:17:01,880
if you wanted it to better pick up on things like edges and patterns.

238
00:17:01,880 --> 00:17:06,360
But better than that, to actually engage with the material, I highly recommend the book

239
00:17:06,360 --> 00:17:09,720
by Michael Nielsen on deep learning and neural networks.

240
00:17:09,720 --> 00:17:15,200
In it, you can find the code and the data to download and play with for this exact example,

241
00:17:15,200 --> 00:17:19,360
and the book will walk you through step by step what that code is doing.

242
00:17:19,360 --> 00:17:23,920
What's awesome is that this book is free and publicly available, so if you do get something

243
00:17:23,920 --> 00:17:28,040
out of it, consider joining me in making a donation towards Nielsen's efforts.

244
00:17:28,040 --> 00:17:32,060
I've also linked a couple other resources I like a lot in the description, including

245
00:17:32,060 --> 00:17:38,720
the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.

246
00:17:38,720 --> 00:17:41,960
To close things off here for the last few minutes, I want to jump back into a snippet

247
00:17:41,960 --> 00:17:44,440
of the interview I had with Leisha Lee.

248
00:17:44,440 --> 00:17:48,520
You might remember her from the last video, she did her PhD work in deep learning.

249
00:17:48,560 --> 00:17:52,240
In this little snippet, she talks about two recent papers that really dig into how some

250
00:17:52,240 --> 00:17:56,380
of the more modern image recognition networks are actually learning.

251
00:17:56,380 --> 00:18:00,320
Just to set up where we were in the conversation, the first paper took one of these particularly

252
00:18:00,320 --> 00:18:04,480
deep neural networks that's really good at image recognition, and instead of training

253
00:18:04,480 --> 00:18:09,400
it on a properly labeled dataset, it shuffled all of the labels around before training.

254
00:18:09,400 --> 00:18:13,840
Obviously the testing accuracy here was going to be no better than random, since everything's

255
00:18:13,840 --> 00:18:15,320
just randomly labeled.

256
00:18:15,320 --> 00:18:20,080
But it was still able to achieve the same training accuracy as you would on a properly

257
00:18:20,080 --> 00:18:21,440
labeled dataset.

258
00:18:21,440 --> 00:18:26,120
Basically, the millions of weights for this particular network were enough for it to just

259
00:18:26,120 --> 00:18:31,040
memorize the random data, which raises the question for whether minimizing this cost

260
00:18:31,040 --> 00:18:36,720
function actually corresponds to any sort of structure in the image, or is it just memorization?

261
00:18:36,720 --> 00:18:40,120
...to memorize the entire dataset of what the correct classification is.

262
00:18:40,120 --> 00:18:45,720
And so a couple of, you know, half a year later at ICML this year, there was not exactly

263
00:18:45,720 --> 00:18:50,440
rebuttal paper, but paper that addressed some aspects of like, hey, actually these networks

264
00:18:50,440 --> 00:18:52,220
are doing something a little bit smarter than that.

265
00:18:52,220 --> 00:18:59,600
If you look at that accuracy curve, if you were just training on a random dataset, that

266
00:18:59,600 --> 00:19:05,240
curve sort of went down very, you know, very slowly in almost kind of a linear fashion.

267
00:19:05,280 --> 00:19:10,840
So you're really struggling to find that local minima of possible, you know, the right weights

268
00:19:10,840 --> 00:19:12,320
that would get you that accuracy.

269
00:19:12,320 --> 00:19:16,720
Whereas if you're actually training on a structured dataset, one that has the right labels, you

270
00:19:16,720 --> 00:19:20,240
know, you fiddle around a little bit in the beginning, but then you kind of dropped very

271
00:19:20,240 --> 00:19:23,360
fast to get to that accuracy level.

272
00:19:23,360 --> 00:19:28,580
And so in some sense it was easier to find that local maxima.

273
00:19:28,580 --> 00:19:32,900
And so what was also interesting about that is it brings into light another paper from

274
00:19:32,900 --> 00:19:39,140
actually a couple of years ago, which has a lot more simplifications about the network

275
00:19:39,140 --> 00:19:40,140
layers.

276
00:19:40,140 --> 00:19:43,880
But one of the results was saying how, if you look at the optimization landscape, the

277
00:19:43,880 --> 00:19:49,400
local minima that these networks tend to learn are actually of equal quality.

278
00:19:49,400 --> 00:19:54,300
So in some sense, if your data set is structured, you should be able to find that much more easily.

279
00:19:58,580 --> 00:20:01,140
My thanks as always to those of you supporting on Patreon.

280
00:20:01,480 --> 00:20:05,440
I've said before just what a game changer on Patreon is, but these videos really would

281
00:20:05,440 --> 00:20:07,160
not be possible without you.

282
00:20:07,160 --> 00:20:11,540
I also want to give a special thanks to the VC firm Amplify Partners and their support

283
00:20:11,540 --> 00:20:13,240
of these initial videos in the series.

284
00:20:31,140 --> 00:20:33,140
Thank you.

