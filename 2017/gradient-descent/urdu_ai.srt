1
00:00:00,000 --> 00:00:07,240
آخری ویڈیو میں نے نیورل نیٹ ورک کا ڈھانچہ پیش کیا۔

2
00:00:07,240 --> 00:00:11,560
میں یہاں ایک فوری ریکیپ دوں گا تاکہ یہ ہمارے ذہنوں میں تازہ

3
00:00:11,560 --> 00:00:13,160
ہو، اور پھر اس ویڈیو کے لیے میرے دو اہم مقاصد ہیں۔

4
00:00:13,160 --> 00:00:17,960
سب سے پہلے تدریجی نزول کے خیال کو متعارف کرانا ہے، جو نہ صرف اعصابی نیٹ ورکس کے

5
00:00:17,960 --> 00:00:20,800
سیکھنے کے طریقے پر مبنی ہے، بلکہ بہت سی دوسری مشین لرننگ بھی کیسے کام کرتی ہے۔

6
00:00:20,800 --> 00:00:25,160
پھر اس کے بعد ہم تھوڑا اور کھوج لگائیں گے کہ یہ مخصوص نیٹ ورک

7
00:00:25,160 --> 00:00:29,560
کس طرح کام کرتا ہے، اور نیوران کی وہ پوشیدہ پرتیں کیا ڈھونڈتی ہیں۔

8
00:00:29,560 --> 00:00:34,680
ایک یاد دہانی کے طور پر، ہمارا مقصد یہاں ہاتھ سے لکھے ہوئے

9
00:00:34,680 --> 00:00:37,080
ہندسوں کی شناخت کی بہترین مثال ہے، نیورل نیٹ ورکس کی ہیلو ورلڈ۔

10
00:00:37,080 --> 00:00:42,160
یہ ہندسے 28x28 پکسل گرڈ پر پیش کیے گئے ہیں،

11
00:00:42,160 --> 00:00:44,260
ہر پکسل کی قدر 0 اور 1 کے درمیان ہے۔

12
00:00:44,260 --> 00:00:51,400
یہ وہی ہیں جو نیٹ ورک کی ان پٹ پرت میں 784 نیوران کی فعالیت کا تعین کرتے ہیں۔

13
00:00:51,400 --> 00:00:56,880
مندرجہ ذیل تہوں میں ہر نیوران کے لیے ایکٹیویشن پچھلی پرت میں تمام ایکٹیویشنز کے

14
00:00:56,880 --> 00:01:02,300
وزنی مجموعہ پر مبنی ہے، نیز کچھ خاص نمبر جنہیں تعصب کہا جاتا ہے۔

15
00:01:02,300 --> 00:01:07,480
آپ اس رقم کو کسی اور فنکشن کے ساتھ تحریر کرتے ہیں، جیسے

16
00:01:07,480 --> 00:01:09,640
سگمائڈ اسکویشیفیکیشن، یا ایک ReLU، جس طرح میں نے پچھلی ویڈیو سے گزرا۔

17
00:01:09,640 --> 00:01:14,960
مجموعی طور پر، 16 نیورونز کے ساتھ دو چھپی ہوئی تہوں کے کسی حد تک من مانی انتخاب

18
00:01:14,960 --> 00:01:20,940
کو دیکھتے ہوئے، نیٹ ورک میں تقریباً 13,000 وزن اور تعصبات ہیں جنہیں ہم ایڈجسٹ کر سکتے ہیں،

19
00:01:20,940 --> 00:01:25,320
اور یہی قدریں اس بات کا تعین کرتی ہیں کہ نیٹ ورک اصل میں کیا کرتا ہے۔

20
00:01:25,320 --> 00:01:29,800
اور جب ہم کہتے ہیں کہ یہ نیٹ ورک کسی عدد کی درجہ بندی کرتا ہے تو ہمارا مطلب یہ

21
00:01:29,800 --> 00:01:34,080
ہے کہ آخری تہہ میں موجود ان 10 نیورونز میں سے سب سے روشن اس ہندسے کے مطابق ہے۔

22
00:01:34,080 --> 00:01:39,240
اور یاد رکھیں، پرتوں والے ڈھانچے کے لیے ہمارے ذہن میں جو محرک

23
00:01:39,240 --> 00:01:43,920
تھا وہ یہ تھا کہ شاید دوسری پرت کناروں پر اُٹھ سکتی

24
00:01:43,920 --> 00:01:48,640
ہے، تیسری پرت لوپس اور لائنوں جیسے نمونوں پر اُٹھ سکتی ہے،

25
00:01:48,640 --> 00:01:49,640
اور آخری پرت ان نمونوں کو جوڑ سکتی ہے۔ ہندسوں کو پہچانیں۔

26
00:01:49,640 --> 00:01:52,880
تو یہاں، ہم سیکھتے ہیں کہ نیٹ ورک کیسے سیکھتا ہے۔

27
00:01:52,880 --> 00:01:56,880
ہم جو چاہتے ہیں وہ ایک الگورتھم ہے جہاں آپ اس نیٹ ورک کو تربیتی اعداد و شمار کا ایک

28
00:01:56,880 --> 00:02:01,540
پورا گروپ دکھا سکتے ہیں، جو ہاتھ سے لکھے ہندسوں کی مختلف تصاویر کے ایک گروپ کی شکل میں

29
00:02:01,540 --> 00:02:06,360
آتا ہے، اس کے ساتھ ساتھ ان کے لیبلز کے ساتھ کہ وہ کیا ہونا چاہیے، اور یہ ان

30
00:02:06,360 --> 00:02:10,760
13,000 وزنوں اور تعصبات کو ایڈجسٹ کریں تاکہ تربیتی ڈیٹا پر اس کی کارکردگی کو بہتر بنایا جا سکے۔

31
00:02:10,760 --> 00:02:15,540
امید ہے کہ اس تہہ دار ڈھانچے کا مطلب یہ ہوگا کہ یہ جو کچھ

32
00:02:15,540 --> 00:02:17,840
سیکھتا ہے وہ اس تربیتی ڈیٹا سے باہر کی تصاویر کو عام کرتا ہے۔

33
00:02:17,840 --> 00:02:22,240
جس طرح سے ہم جانچتے ہیں کہ آپ نیٹ ورک کو تربیت دینے کے بعد، آپ اسے مزید لیبل والا ڈیٹا دکھاتے

34
00:02:22,240 --> 00:02:31,160
ہیں، اور آپ دیکھتے ہیں کہ یہ ان نئی تصاویر کو کس حد تک درست طریقے سے درجہ بندی کرتا ہے۔

35
00:02:31,160 --> 00:02:34,760
خوش قسمتی سے ہمارے لیے، اور جس چیز کے ساتھ یہ ایک عام مثال بنتی ہے، وہ یہ ہے

36
00:02:34,760 --> 00:02:39,520
کہ MNIST ڈیٹا بیس کے پیچھے اچھے لوگوں نے دسیوں ہزار ہاتھ سے لکھی ہندسوں کی تصاویر کا ایک

37
00:02:39,520 --> 00:02:45,080
مجموعہ جمع کیا ہے، ہر ایک پر ان نمبروں کے ساتھ لیبل لگا ہوا ہے جو وہ ہونے چاہئیں۔

38
00:02:45,080 --> 00:02:49,920
اور جتنا اشتعال انگیز ہے کسی مشین کو سیکھنے کے طور پر بیان کرنا، ایک بار جب آپ دیکھتے ہیں کہ یہ کیسے کام

39
00:02:49,920 --> 00:02:55,560
کرتی ہے، تو یہ کچھ پاگل سائنس فائی بنیاد کی طرح بہت کم محسوس ہوتی ہے، اور بہت زیادہ کیلکولس ورزش کی طرح۔

40
00:02:55,560 --> 00:03:01,040
میرا مطلب ہے، بنیادی طور پر یہ کسی خاص فنکشن کی کم از کم تلاش کرنے پر آتا ہے۔

41
00:03:01,040 --> 00:03:06,480
یاد رکھیں، تصوراتی طور پر ہم ہر نیوران کے بارے میں سوچ رہے ہیں کہ

42
00:03:06,480 --> 00:03:11,440
وہ پچھلی پرت کے تمام نیورانوں سے جڑا ہوا ہے، اور اس کے ایکٹیویشن کی

43
00:03:11,440 --> 00:03:16,400
وضاحت کرنے والے وزنی رقم میں وزن ان کنکشنز کی طاقتوں کی طرح ہے، اور

44
00:03:16,400 --> 00:03:19,780
تعصب اس کا کچھ اشارہ ہے۔ چاہے وہ نیوران فعال ہو یا غیر فعال۔

45
00:03:19,780 --> 00:03:23,300
اور چیزوں کو شروع کرنے کے لئے، ہم صرف ان تمام وزنوں اور

46
00:03:23,300 --> 00:03:25,020
تعصبات کو مکمل طور پر تصادفی طور پر شروع کرنے جا رہے ہیں۔

47
00:03:25,020 --> 00:03:29,100
یہ کہنے کی ضرورت نہیں ہے کہ یہ نیٹ ورک دی گئی تربیتی مثال پر خوفناک

48
00:03:29,100 --> 00:03:31,180
کارکردگی کا مظاہرہ کرنے والا ہے، کیونکہ یہ صرف بے ترتیب کچھ کر رہا ہے۔

49
00:03:31,180 --> 00:03:36,820
مثال کے طور پر، آپ 3 کی اس تصویر میں فیڈ کرتے ہیں، اور آؤٹ پٹ پرت بالکل گڑبڑ کی طرح دکھائی دیتی ہے۔

50
00:03:36,820 --> 00:03:43,340
لہذا آپ جو کرتے ہیں وہ ایک لاگت کے فنکشن کی وضاحت کرتا ہے، کمپیوٹر کو بتانے کا ایک طریقہ، نہیں، خراب

51
00:03:43,340 --> 00:03:48,940
کمپیوٹر، اس آؤٹ پٹ میں ایکٹیویشن ہونی چاہیے جو زیادہ تر نیوران کے لیے 0 ہیں، لیکن اس نیوران کے لیے 1۔

52
00:03:48,980 --> 00:03:51,740
جو تم نے مجھے دیا وہ بالکل ردی ہے۔

53
00:03:51,740 --> 00:03:56,740
یہ کہنے کے لیے کہ تھوڑا سا اور ریاضیاتی طور پر، آپ ان میں سے ہر ایک کوڑے

54
00:03:56,740 --> 00:04:01,980
دان کے آؤٹ پٹ ایکٹیویشن اور اس قدر کے درمیان فرق کے مربعوں کو جوڑتے ہیں جو آپ

55
00:04:01,980 --> 00:04:06,020
چاہتے ہیں کہ ان کے پاس ہوں، اور اسے ہم ایک تربیتی مثال کی قیمت کہیں گے۔

56
00:04:06,020 --> 00:04:12,660
نوٹ کریں کہ جب نیٹ ورک اعتماد کے ساتھ تصویر کی صحیح درجہ بندی کرتا ہے تو یہ رقم چھوٹی ہوتی ہے،

57
00:04:12,660 --> 00:04:18,820
لیکن یہ بڑی ہوتی ہے جب نیٹ ورک ایسا لگتا ہے کہ یہ نہیں جانتا کہ وہ کیا کر رہا ہے۔

58
00:04:18,820 --> 00:04:23,860
تو پھر آپ جو کرتے ہیں وہ یہ ہے کہ آپ کے اختیار میں

59
00:04:23,860 --> 00:04:27,580
موجود دسیوں ہزار تربیتی مثالوں میں سے تمام اوسط لاگت پر غور کریں۔

60
00:04:27,580 --> 00:04:32,300
یہ اوسط لاگت ہمارا پیمانہ ہے کہ نیٹ ورک کتنا

61
00:04:32,300 --> 00:04:33,300
گھٹیا ہے، اور کمپیوٹر کو کتنا برا محسوس ہونا چاہیے۔

62
00:04:33,300 --> 00:04:35,300
اور یہ ایک پیچیدہ چیز ہے۔

63
00:04:35,300 --> 00:04:40,380
یاد رکھیں کہ نیٹ ورک بنیادی طور پر کس طرح ایک فنکشن تھا، جو 784 نمبروں کو ان

64
00:04:40,380 --> 00:04:46,100
پٹ، پکسل ویلیوز کے طور پر لیتا ہے، اور اس کے آؤٹ پٹ کے طور پر 10 نمبروں

65
00:04:46,100 --> 00:04:49,700
کو نکالتا ہے، اور ایک لحاظ سے یہ ان تمام وزنوں اور تعصبات سے پیرامیٹرائز ہوتا ہے؟

66
00:04:49,700 --> 00:04:53,340
لاگت کا فنکشن اس کے اوپر پیچیدگی کی ایک پرت ہے۔

67
00:04:53,340 --> 00:04:59,140
یہ ان 13,000 یا اس سے زیادہ وزن اور تعصبات کو اپنے ان پٹ کے طور پر لیتا ہے، اور

68
00:04:59,140 --> 00:05:04,620
یہ بتاتا ہے کہ وہ وزن اور تعصبات کتنے خراب ہیں، اور جس طرح سے اس کی وضاحت کی

69
00:05:04,620 --> 00:05:09,140
گئی ہے اس کا انحصار تربیتی ڈیٹا کے دسیوں ہزار ٹکڑوں پر نیٹ ورک کے رویے پر ہوتا ہے۔

70
00:05:09,140 --> 00:05:12,460
یہ بہت سوچنے کی بات ہے۔

71
00:05:12,460 --> 00:05:16,380
لیکن کمپیوٹر کو صرف یہ بتانا کہ وہ کیا گھٹیا کام کر رہا ہے زیادہ مددگار نہیں ہے۔

72
00:05:16,380 --> 00:05:21,300
آپ اسے بتانا چاہتے ہیں کہ ان وزنوں اور تعصبات کو کیسے تبدیل کیا جائے تاکہ یہ بہتر ہو جائے۔

73
00:05:21,300 --> 00:05:25,580
اسے آسان بنانے کے لیے، 13,000 ان پٹ کے ساتھ کسی فنکشن کا تصور کرنے کے لیے جدوجہد کرنے کے بجائے، صرف

74
00:05:25,580 --> 00:05:31,440
ایک سادہ فنکشن کا تصور کریں جس میں ایک نمبر بطور ان پٹ اور ایک نمبر آؤٹ پٹ کے طور پر ہو۔

75
00:05:31,440 --> 00:05:36,420
آپ کو ایک ان پٹ کیسے ملتا ہے جو اس فنکشن کی قدر کو کم کرتا ہے؟

76
00:05:36,420 --> 00:05:41,300
کیلکولس کے طلباء کو معلوم ہوگا کہ آپ بعض اوقات اس کم از کم واضح طور پر اندازہ لگا

77
00:05:41,340 --> 00:05:46,620
سکتے ہیں، لیکن یہ واقعی پیچیدہ افعال کے لیے ہمیشہ ممکن نہیں ہوتا، یقیناً ہمارے پاگل پیچیدہ نیورل

78
00:05:46,620 --> 00:05:51,640
نیٹ ورک کی لاگت کے فنکشن کے لیے اس صورت حال کے 13,000 ان پٹ ورژن میں نہیں۔

79
00:05:51,640 --> 00:05:56,820
ایک زیادہ لچکدار حربہ یہ ہے کہ کسی بھی ان پٹ سے آغاز کریں، اور یہ معلوم کریں

80
00:05:56,820 --> 00:05:59,860
کہ اس آؤٹ پٹ کو کم کرنے کے لیے آپ کو کس سمت کو قدم رکھنا چاہیے۔

81
00:05:59,860 --> 00:06:05,020
خاص طور پر، اگر آپ اس فنکشن کی ڈھلوان کا پتہ لگا سکتے ہیں

82
00:06:05,020 --> 00:06:09,280
جہاں آپ ہیں، تو بائیں طرف شفٹ کریں اگر وہ ڈھلوان مثبت ہے، اور

83
00:06:09,280 --> 00:06:12,720
اگر وہ ڈھلوان منفی ہے تو ان پٹ کو دائیں طرف شفٹ کریں۔

84
00:06:12,720 --> 00:06:17,040
اگر آپ یہ بار بار کرتے ہیں، ہر ایک نقطہ پر نئی ڈھلوان کی جانچ کرتے ہوئے اور

85
00:06:17,040 --> 00:06:20,680
مناسب قدم اٹھاتے ہوئے، آپ فنکشن کے کچھ مقامی کم از کم سے رجوع کرنے جا رہے ہیں۔

86
00:06:20,680 --> 00:06:24,600
اور یہاں جو تصویر آپ کے ذہن میں ہو گی وہ ایک گیند ہے جو ایک پہاڑی سے نیچے گر رہی ہے۔

87
00:06:24,600 --> 00:06:29,380
اور نوٹس کریں، یہاں تک کہ اس واقعی آسان بنائے گئے واحد ان پٹ فنکشن کے لیے، بہت

88
00:06:29,380 --> 00:06:34,220
سی ممکنہ وادیوں میں آپ اتر سکتے ہیں، اس پر منحصر ہے کہ آپ کس بے ترتیب

89
00:06:34,220 --> 00:06:38,460
ان پٹ سے شروع کرتے ہیں، اور اس بات کی کوئی گارنٹی نہیں ہے کہ آپ جس

90
00:06:38,460 --> 00:06:39,460
مقامی کم از کم میں اتریں گے وہ سب سے چھوٹی ممکنہ قیمت ہوگی۔ لاگت کی تقریب.

91
00:06:39,460 --> 00:06:43,180
یہ ہمارے نیورل نیٹ ورک کیس میں بھی لے جانے والا ہے۔

92
00:06:43,180 --> 00:06:48,140
اور میں یہ بھی چاہتا ہوں کہ آپ یہ دیکھیں کہ اگر آپ اپنے قدموں کے سائز کو ڈھلوان

93
00:06:48,140 --> 00:06:52,920
کے متناسب بناتے ہیں، تو جب ڈھلوان کم سے کم کی طرف چپٹا ہوتا ہے، تو آپ کے قدم

94
00:06:52,920 --> 00:06:56,020
چھوٹے سے چھوٹے ہوتے جاتے ہیں، اور اس قسم سے آپ کو اوور شوٹنگ میں مدد ملتی ہے۔

95
00:06:56,020 --> 00:07:01,640
پیچیدگی کو تھوڑا سا بڑھاتے ہوئے، دو ان پٹ اور ایک آؤٹ پٹ کے ساتھ ایک فنکشن کا تصور کریں۔

96
00:07:01,640 --> 00:07:06,360
آپ ان پٹ کی جگہ کو xy-plane کے طور پر سوچ سکتے ہیں، اور لاگت کے

97
00:07:06,360 --> 00:07:09,020
فنکشن کو اس کے اوپر کی سطح کے طور پر گراف کیا جا رہا ہے۔

98
00:07:09,020 --> 00:07:13,600
فنکشن کی ڈھلوان کے بارے میں پوچھنے کے بجائے، آپ کو یہ پوچھنا ہوگا کہ آپ کو اس ان

99
00:07:13,600 --> 00:07:19,780
پٹ اسپیس میں کس سمت جانا چاہئے تاکہ فنکشن کے آؤٹ پٹ کو تیزی سے کم کیا جاسکے۔

100
00:07:19,780 --> 00:07:22,340
دوسرے الفاظ میں، نیچے کی سمت کیا ہے؟

101
00:07:22,340 --> 00:07:26,740
اور ایک بار پھر، اس پہاڑی سے نیچے گرنے والی گیند کے بارے میں سوچنا مددگار ہے۔

102
00:07:26,740 --> 00:07:31,920
آپ میں سے جو لوگ ملٹی ویری ایبل کیلکولس سے واقف ہیں وہ جانتے ہوں

103
00:07:31,920 --> 00:07:37,460
گے کہ فنکشن کا میلان آپ کو تیز ترین چڑھائی کی سمت دیتا ہے، فنکشن

104
00:07:37,460 --> 00:07:39,420
کو تیزی سے بڑھانے کے لیے آپ کو کس سمت میں قدم رکھنا چاہیے۔

105
00:07:39,420 --> 00:07:43,820
قدرتی طور پر کافی ہے، اس میلان کے منفی کو لینے سے آپ کو

106
00:07:43,820 --> 00:07:47,460
قدم اٹھانے کی سمت ملتی ہے جو فنکشن کو تیزی سے کم کرتا ہے۔

107
00:07:47,460 --> 00:07:52,320
اس سے بھی زیادہ، اس گریڈینٹ ویکٹر کی لمبائی اس بات کا

108
00:07:52,320 --> 00:07:54,580
اشارہ ہے کہ وہ سب سے کھڑی ڈھلوان کتنی کھڑی ہے۔

109
00:07:54,580 --> 00:07:58,080
اب اگر آپ ملٹی ویری ایبل کیلکولس سے ناواقف ہیں اور مزید جاننا چاہتے ہیں

110
00:07:58,080 --> 00:08:01,100
تو اس موضوع پر خان اکیڈمی کے لیے میں نے کیا کچھ کام دیکھیں۔

111
00:08:01,100 --> 00:08:05,680
سچ کہوں تو، اس وقت آپ اور میرے لیے جو چیز اہم ہے وہ یہ ہے

112
00:08:05,680 --> 00:08:10,440
کہ اصولی طور پر اس ویکٹر کی گنتی کرنے کا ایک طریقہ موجود ہے، یہ ویکٹر

113
00:08:10,440 --> 00:08:12,040
جو آپ کو بتاتا ہے کہ نیچے کی سمت کیا ہے اور یہ کتنی کھڑی ہے۔

114
00:08:12,040 --> 00:08:17,280
آپ ٹھیک ہو جائیں گے اگر آپ صرف اتنا جانتے ہیں اور آپ تفصیلات پر ٹھوس نہیں ہیں۔

115
00:08:17,280 --> 00:08:21,440
کیونکہ اگر آپ اسے حاصل کر سکتے ہیں تو، فنکشن کو کم سے کم کرنے کے لیے الگورتھم یہ ہے کہ

116
00:08:21,440 --> 00:08:27,400
اس گریڈینٹ سمت کی گنتی کی جائے، پھر نیچے کی طرف ایک چھوٹا سا قدم اٹھائیں، اور اسے بار بار دہرائیں۔

117
00:08:28,300 --> 00:08:33,700
یہ ایک فنکشن کے لیے وہی بنیادی خیال ہے جس میں 2 ان پٹ کے بجائے 13,000 ان پٹ ہوتے ہیں۔

118
00:08:33,700 --> 00:08:38,980
ہمارے نیٹ ورک کے تمام 13,000 وزنوں اور تعصبات کو

119
00:08:38,980 --> 00:08:40,180
ایک بڑے کالم ویکٹر میں ترتیب دینے کا تصور کریں۔

120
00:08:40,180 --> 00:08:46,140
لاگت کے فنکشن کا منفی میلان صرف ایک ویکٹر ہے، یہ اس بے حد بڑی ان پٹ

121
00:08:46,140 --> 00:08:51,660
اسپیس کے اندر کچھ سمت ہے جو آپ کو بتاتی ہے کہ ان تمام نمبروں کو

122
00:08:51,660 --> 00:08:55,900
کون سا جھٹکا لگانا لاگت کے فنکشن میں تیزی سے کمی کا سبب بن رہا ہے۔

123
00:08:55,900 --> 00:09:00,000
اور یقیناً، ہمارے خاص طور پر ڈیزائن کردہ لاگت کے فنکشن کے ساتھ، وزن اور تعصبات

124
00:09:00,000 --> 00:09:05,520
کو کم کرنے کے لیے تبدیل کرنے کا مطلب ہے کہ تربیتی ڈیٹا کے ہر ٹکڑے

125
00:09:05,520 --> 00:09:10,280
پر نیٹ ورک کا آؤٹ پٹ 10 اقدار کی بے ترتیب صف کی طرح کم نظر

126
00:09:10,280 --> 00:09:11,280
آتا ہے، اور ایک حقیقی فیصلہ جیسا کہ ہم چاہتے ہیں۔ یہ بنانے کے لئے.

127
00:09:11,280 --> 00:09:15,940
یہ یاد رکھنا ضروری ہے، اس لاگت کے فنکشن میں ٹریننگ کے تمام ڈیٹا پر اوسط شامل ہوتا ہے، لہذا اگر

128
00:09:15,940 --> 00:09:24,260
آپ اسے کم سے کم کرتے ہیں، تو اس کا مطلب ہے کہ یہ ان تمام نمونوں پر بہتر کارکردگی ہے۔

129
00:09:24,260 --> 00:09:28,540
اس میلان کو مؤثر طریقے سے کمپیوٹنگ کرنے کے الگورتھم، جو مؤثر طریقے سے اس بات

130
00:09:28,540 --> 00:09:32,520
کا دل ہے کہ نیورل نیٹ ورک کیسے سیکھتا ہے، اسے بیک پروپیگیشن کہا جاتا ہے،

131
00:09:32,520 --> 00:09:34,040
اور یہ وہی ہے جو میں اگلی ویڈیو کے بارے میں بات کرنے جا رہا ہوں۔

132
00:09:34,040 --> 00:09:39,100
وہاں، میں واقعی میں اس بات کو سمجھنے کے لیے وقت نکالنا چاہتا ہوں کہ تربیتی اعداد و شمار

133
00:09:39,100 --> 00:09:44,100
کے دیے گئے حصے کے لیے ہر وزن اور تعصب کے ساتھ کیا ہوتا ہے، متعلقہ کیلکولس اور فارمولوں

134
00:09:44,100 --> 00:09:47,980
کے ڈھیر سے باہر کیا ہو رہا ہے اس کے لیے ایک بدیہی احساس دلانے کی کوشش کرتا ہوں۔

135
00:09:47,980 --> 00:09:51,780
یہیں، ابھی، بنیادی چیز جو میں آپ کو جاننا چاہتا ہوں، نفاذ کی تفصیلات سے آزاد،

136
00:09:51,780 --> 00:09:56,820
یہ ہے کہ جب ہم نیٹ ورک لرننگ کے بارے میں بات کرتے ہیں تو

137
00:09:56,820 --> 00:09:59,320
ہمارا مطلب یہ ہے کہ یہ صرف لاگت کے فنکشن کو کم کر رہا ہے۔

138
00:09:59,320 --> 00:10:02,760
اور نوٹس کریں، اس کا ایک نتیجہ یہ ہے کہ لاگت کے اس فنکشن

139
00:10:02,760 --> 00:10:07,820
کے لیے یہ ضروری ہے کہ ہموار آؤٹ پٹ ہو، تاکہ ہم نیچے کی

140
00:10:07,820 --> 00:10:09,340
طرف تھوڑا سا قدم اٹھا کر مقامی کم از کم تلاش کر سکیں۔

141
00:10:09,340 --> 00:10:14,140
یہی وجہ ہے کہ، ویسے، مصنوعی نیوران میں مسلسل ایکٹیویشنز ہوتی

142
00:10:14,140 --> 00:10:18,580
ہیں، بجائے اس کے کہ بائنری طریقے سے فعال یا غیر

143
00:10:18,580 --> 00:10:20,440
فعال ہونے کے، جس طرح سے حیاتیاتی نیوران ہوتے ہیں۔

144
00:10:20,440 --> 00:10:24,600
کسی فنکشن کے ان پٹ کو منفی میلان کے کچھ ملٹیپل سے

145
00:10:24,600 --> 00:10:26,960
بار بار جھکانے کے اس عمل کو گریڈینٹ ڈیسنٹ کہا جاتا ہے۔

146
00:10:26,960 --> 00:10:31,760
یہ کچھ مقامی کم از کم لاگت کے فنکشن کی طرف اکٹھا ہونے

147
00:10:31,760 --> 00:10:33,000
کا ایک طریقہ ہے، بنیادی طور پر اس گراف میں ایک وادی۔

148
00:10:33,000 --> 00:10:37,040
میں اب بھی ایک فنکشن کی تصویر دو ان پٹ کے ساتھ دکھا رہا ہوں،

149
00:10:37,040 --> 00:10:41,480
یقیناً، کیونکہ 13,000 جہتی ان پٹ اسپیس میں nudges آپ کے دماغ کو سمیٹنا تھوڑا

150
00:10:41,480 --> 00:10:45,220
مشکل ہے، لیکن اس کے بارے میں سوچنے کا ایک اچھا غیر مقامی طریقہ ہے۔

151
00:10:45,220 --> 00:10:49,100
منفی میلان کا ہر جزو ہمیں دو چیزیں بتاتا ہے۔

152
00:10:49,100 --> 00:10:53,600
نشان، یقیناً، ہمیں بتاتا ہے کہ آیا ان پٹ ویکٹر کے

153
00:10:53,600 --> 00:10:55,860
متعلقہ جز کو اوپر یا نیچے کی طرف دھکیلنا چاہیے۔

154
00:10:55,860 --> 00:11:01,340
لیکن اہم بات یہ ہے کہ ان تمام اجزاء کی نسبتی وسعت

155
00:11:01,340 --> 00:11:05,620
آپ کو بتاتی ہے کہ کون سی تبدیلی زیادہ اہمیت رکھتی ہے۔

156
00:11:05,620 --> 00:11:09,780
آپ دیکھتے ہیں، ہمارے نیٹ ورک میں، کسی ایک وزن میں ایڈجسٹمنٹ کا لاگت کے فنکشن

157
00:11:09,780 --> 00:11:14,980
پر کسی دوسرے وزن میں ایڈجسٹمنٹ کے مقابلے میں بہت زیادہ اثر پڑ سکتا ہے۔

158
00:11:14,980 --> 00:11:19,440
ان میں سے کچھ کنکشن ہمارے تربیتی ڈیٹا کے لیے زیادہ اہمیت رکھتے ہیں۔

159
00:11:19,440 --> 00:11:23,520
تو ایک طریقہ جس سے آپ ہمارے دماغی طور پر بڑے پیمانے پر لاگت کے فنکشن کے اس تدریجی ویکٹر کے

160
00:11:23,520 --> 00:11:29,740
بارے میں سوچ سکتے ہیں وہ یہ ہے کہ یہ ہر وزن اور تعصب کی نسبتہ اہمیت کو انکوڈ کرتا

161
00:11:29,740 --> 00:11:34,100
ہے، یعنی، ان میں سے کون سی تبدیلی آپ کے پیسے کے لیے سب سے زیادہ اثر ڈالنے والی ہے۔

162
00:11:34,100 --> 00:11:37,360
یہ واقعی سمت کے بارے میں سوچنے کا ایک اور طریقہ ہے۔

163
00:11:37,360 --> 00:11:41,740
ایک آسان مثال لینے کے لیے، اگر آپ کے پاس ان پٹ کے طور پر دو متغیرات

164
00:11:41,740 --> 00:11:48,720
کے ساتھ کچھ فنکشن موجود ہے، اور یہ شمار کریں کہ کسی خاص نقطہ پر اس کا

165
00:11:48,720 --> 00:11:52,880
میلان 3,1 کے طور پر نکلتا ہے، تو ایک طرف آپ اس کی تشریح یہ کہہ سکتے

166
00:11:52,880 --> 00:11:57,400
ہیں کہ جب آپ اس ان پٹ پر کھڑے ہو کر، اس سمت میں حرکت کرنے سے

167
00:11:57,400 --> 00:12:02,200
فنکشن میں تیزی سے اضافہ ہوتا ہے، کہ جب آپ فنکشن کو ان پٹ پوائنٹس کے جہاز

168
00:12:02,200 --> 00:12:03,200
کے اوپر گراف کرتے ہیں، تو وہی ویکٹر آپ کو سیدھی اوپر کی سمت دیتا ہے۔

169
00:12:03,200 --> 00:12:07,600
لیکن اسے پڑھنے کا ایک اور طریقہ یہ ہے کہ اس پہلے متغیر میں ہونے والی تبدیلیاں دوسرے

170
00:12:07,600 --> 00:12:12,400
متغیر میں ہونے والی تبدیلیوں کے مقابلے میں تین گنا زیادہ اہمیت رکھتی ہیں، کہ کم از کم

171
00:12:12,400 --> 00:12:17,740
متعلقہ ان پٹ کے پڑوس میں، x-value کو جھنجوڑنا آپ کے لیے بہت زیادہ دھڑکتا ہے۔ ہرن

172
00:12:17,740 --> 00:12:22,880
ٹھیک ہے، آئیے زوم آؤٹ کریں اور خلاصہ کریں کہ ہم اب تک کہاں ہیں۔

173
00:12:22,880 --> 00:12:28,660
نیٹ ورک بذات خود یہ فنکشن ہے جس میں 784 ان پٹ اور 10 آؤٹ

174
00:12:28,660 --> 00:12:30,860
پٹس ہیں، جو ان تمام وزنی رقم کے لحاظ سے بیان کیے گئے ہیں۔

175
00:12:30,860 --> 00:12:34,160
لاگت کا فنکشن اس کے اوپر پیچیدگی کی ایک پرت ہے۔

176
00:12:34,160 --> 00:12:39,300
یہ 13,000 وزن اور تعصبات کو ان پٹ کے طور پر لیتا ہے، اور

177
00:12:39,300 --> 00:12:42,640
تربیتی مثالوں کی بنیاد پر گھٹیا پن کا ایک پیمانہ نکال دیتا ہے۔

178
00:12:42,640 --> 00:12:47,520
لاگت کے فنکشن کا میلان اب بھی پیچیدگی کی ایک اور تہہ ہے۔

179
00:12:47,520 --> 00:12:52,860
یہ ہمیں بتاتا ہے کہ ان تمام وزنوں اور تعصبات میں کون سے جھٹکے لاگت کے

180
00:12:52,860 --> 00:12:56,640
فنکشن کی قدر میں تیز ترین تبدیلی کا سبب بنتے ہیں، جس کی تشریح آپ یہ

181
00:12:56,640 --> 00:13:03,040
کہہ سکتے ہیں کہ کون سی تبدیلیاں جن میں وزن سب سے زیادہ اہمیت رکھتا ہے۔

182
00:13:03,040 --> 00:13:07,620
لہذا جب آپ نیٹ ورک کو بے ترتیب وزن اور تعصبات کے ساتھ شروع کرتے ہیں، اور

183
00:13:07,620 --> 00:13:12,420
اس میلان نزول کے عمل کی بنیاد پر انہیں کئی بار ایڈجسٹ کرتے ہیں، تو یہ ان

184
00:13:12,420 --> 00:13:14,240
تصاویر پر کتنی اچھی کارکردگی کا مظاہرہ کرتا ہے جو اس نے پہلے کبھی نہیں دیکھا؟

185
00:13:14,240 --> 00:13:19,000
جس کا میں نے یہاں بیان کیا ہے، ہر ایک میں 16 نیورونز کی دو چھپی ہوئی تہوں کے ساتھ، زیادہ تر جمالیاتی وجوہات کی

186
00:13:19,000 --> 00:13:26,920
بناء پر منتخب کیا گیا ہے، برا نہیں ہے، جو کہ اس کی نظر آنے والی تقریباً 96% نئی تصویروں کی درجہ بندی کرتا ہے۔

187
00:13:26,920 --> 00:13:31,580
اور ایمانداری سے، اگر آپ ان میں سے کچھ مثالوں کو دیکھیں جن پر یہ

188
00:13:31,580 --> 00:13:36,300
گڑبڑ کرتا ہے، تو آپ اسے تھوڑا سا سست کرنے پر مجبور محسوس کرتے ہیں۔

189
00:13:36,300 --> 00:13:40,220
اگر آپ پوشیدہ پرت کے ڈھانچے کے ساتھ کھیلتے ہیں اور کچھ

190
00:13:40,220 --> 00:13:41,220
موافقت کرتے ہیں، تو آپ اسے 98% تک حاصل کر سکتے ہیں۔

191
00:13:41,220 --> 00:13:42,900
اور یہ بہت اچھا ہے!

192
00:13:42,900 --> 00:13:47,020
یہ سب سے بہتر نہیں ہے، آپ یقینی طور پر اس سادہ ونیلا نیٹ ورک سے زیادہ نفیس حاصل کر کے بہتر

193
00:13:47,020 --> 00:13:52,460
کارکردگی حاصل کر سکتے ہیں، لیکن یہ دیکھتے ہوئے کہ ابتدائی کام کتنا مشکل ہے، میں سمجھتا ہوں کہ کسی بھی نیٹ

194
00:13:52,460 --> 00:13:56,800
ورک کے بارے میں ایسی ناقابل یقین چیز ہے جو تصاویر پر یہ اچھی طرح سے کر رہی ہے اس سے پہلے

195
00:13:56,800 --> 00:14:02,000
کبھی نہیں دیکھا گیا کہ ہم کبھی بھی خاص طور پر یہ نہیں بتایا کہ کون سے نمونوں کی تلاش کرنی ہے۔

196
00:14:02,000 --> 00:14:07,840
اصل میں، میں نے جس طرح سے اس ڈھانچے کی حوصلہ افزائی کی وہ اس امید کو

197
00:14:07,840 --> 00:14:11,880
بیان کرتے ہوئے تھا جو ہمیں ہو سکتی ہے، کہ دوسری تہہ چھوٹے کناروں پر اُٹھ سکتی

198
00:14:11,880 --> 00:14:16,080
ہے، کہ تیسری تہہ ان کناروں کو لوپس اور لمبی لکیروں کو پہچاننے کے لیے جوڑ دے

199
00:14:16,080 --> 00:14:18,220
گی، اور یہ کہ وہ ٹکڑے ٹکڑے ہو سکتے ہیں۔ ہندسوں کو پہچاننے کے لیے ایک ساتھ۔

200
00:14:18,220 --> 00:14:21,040
تو کیا ہمارا نیٹ ورک دراصل یہی کر رہا ہے؟

201
00:14:21,040 --> 00:14:24,880
ٹھیک ہے، اس کے لیے کم از کم، بالکل نہیں۔

202
00:14:24,960 --> 00:14:29,120
یاد رکھیں کہ پچھلی ویڈیو میں ہم نے کس طرح دیکھا تھا کہ پہلی پرت کے تمام

203
00:14:29,120 --> 00:14:33,900
نیوران سے دوسری پرت میں دیئے گئے نیوران تک کنکشن کے وزن کو ایک دیئے گئے پکسل

204
00:14:33,900 --> 00:14:37,440
پیٹرن کے طور پر تصور کیا جا سکتا ہے جسے دوسری تہہ نیوران اٹھا رہا ہے؟

205
00:14:37,440 --> 00:14:44,600
ٹھیک ہے، جب ہم ان ٹرانزیشنز سے وابستہ وزن کے لیے یہ کرتے ہیں،

206
00:14:44,600 --> 00:14:51,000
الگ تھلگ چھوٹے کناروں کو یہاں اور وہاں اٹھانے کے بجائے، وہ بالکل بے

207
00:14:51,000 --> 00:14:54,200
ترتیب نظر آتے ہیں، بس درمیان میں کچھ بہت ہی ڈھیلے نمونوں کے ساتھ۔

208
00:14:54,200 --> 00:14:59,020
ایسا لگتا ہے کہ ممکنہ وزن اور تعصبات کی ناقابل یقین حد تک بڑی 13,000 جہتی

209
00:14:59,020 --> 00:15:04,020
جگہ میں، ہمارے نیٹ ورک نے اپنے آپ کو ایک خوش کن مقامی کم از

210
00:15:04,020 --> 00:15:08,440
کم پایا جو کہ کامیابی کے ساتھ زیادہ تر تصاویر کی درجہ بندی کرنے کے

211
00:15:08,440 --> 00:15:09,840
باوجود، ان نمونوں پر بالکل درست نہیں ہوتا جن کی ہم امید کر رہے تھے۔

212
00:15:09,840 --> 00:15:14,600
اور واقعی اس مقام کو گھر پہنچانے کے لیے، دیکھیں کہ جب آپ بے ترتیب تصویر ڈالتے ہیں تو کیا ہوتا ہے۔

213
00:15:14,600 --> 00:15:19,240
اگر سسٹم سمارٹ تھا، تو آپ اس سے یا تو غیر یقینی محسوس کرنے کی توقع کر سکتے ہیں، شاید ان 10 آؤٹ

214
00:15:19,240 --> 00:15:24,120
پٹ نیورونز میں سے کسی کو بھی چالو نہ کر رہے ہوں یا ان سب کو یکساں طور پر چالو نہ کر

215
00:15:24,520 --> 00:15:29,800
رہے ہوں، لیکن اس کے بجائے یہ آپ کو اعتماد کے ساتھ کچھ بکواس جواب دیتا ہے، گویا یہ یقینی طور پر محسوس

216
00:15:29,800 --> 00:15:34,560
ہوتا ہے کہ یہ بے ترتیب شور ایک 5 ہے جیسا کہ یہ کرتا ہے کہ 5 کی اصل تصویر 5 ہے۔

217
00:15:34,560 --> 00:15:39,300
مختلف طریقے سے بیان کیا جائے، یہاں تک کہ اگر یہ نیٹ ورک ہندسوں کو اچھی

218
00:15:39,300 --> 00:15:41,800
طرح پہچان سکتا ہے، تو اسے اندازہ نہیں ہے کہ انہیں کس طرح کھینچنا ہے۔

219
00:15:41,800 --> 00:15:45,400
اس میں سے بہت کچھ اس لیے ہے کہ یہ اس قدر سختی سے مجبور تربیتی سیٹ اپ ہے۔

220
00:15:45,400 --> 00:15:48,220
میرا مطلب ہے، اپنے آپ کو یہاں نیٹ ورک کے جوتوں میں ڈالیں۔

221
00:15:48,220 --> 00:15:53,280
اس کے نقطہ نظر سے، پوری کائنات ایک چھوٹے سے گرڈ میں مرکز میں واضح طور پر

222
00:15:53,280 --> 00:15:58,560
متعین غیر متحرک ہندسوں کے سوا کچھ پر مشتمل نہیں ہے، اور اس کی لاگت کی

223
00:15:58,560 --> 00:16:02,160
تقریب نے اسے اپنے فیصلوں پر مکمل اعتماد کے سوا کچھ بننے کی ترغیب نہیں دی۔

224
00:16:02,160 --> 00:16:05,760
تو اس کے ساتھ اس تصویر کے طور پر کہ وہ دوسری پرت کے نیوران

225
00:16:05,760 --> 00:16:09,320
واقعی کیا کر رہے ہیں، آپ حیران ہوں گے کہ میں اس نیٹ ورک

226
00:16:09,320 --> 00:16:10,320
کو کناروں اور نمونوں کو اٹھانے کی ترغیب کے ساتھ کیوں متعارف کرواؤں گا۔

227
00:16:10,320 --> 00:16:13,040
میرا مطلب ہے، یہ بالکل بھی ایسا نہیں ہے جو یہ کر رہا ہے۔

228
00:16:13,040 --> 00:16:17,480
ٹھیک ہے، اس کا مقصد ہمارا آخری مقصد نہیں ہے، بلکہ ایک نقطہ آغاز ہے۔

229
00:16:17,480 --> 00:16:22,280
سچ کہوں تو، یہ پرانی ٹیکنالوجی ہے، جس کی 80 اور 90 کی دہائیوں میں تحقیق کی گئی تھی،

230
00:16:22,280 --> 00:16:26,920
اور اس سے پہلے کہ آپ مزید تفصیلی جدید قسموں کو سمجھ سکیں، آپ کو اسے سمجھنے کی ضرورت

231
00:16:26,920 --> 00:16:31,380
ہے، اور یہ واضح طور پر کچھ دلچسپ مسائل کو حل کرنے کی صلاحیت رکھتی ہے، لیکن آپ جتنا

232
00:16:31,380 --> 00:16:38,720
زیادہ اس میں کھودیں گے وہ پوشیدہ پرتیں واقعی کر رہی ہیں، یہ جتنا کم ذہین لگتا ہے۔

233
00:16:38,720 --> 00:16:43,540
نیٹ ورکس آپ کے سیکھنے کے طریقہ سے ایک لمحے کے لیے توجہ مرکوز کرنا، یہ صرف اس

234
00:16:43,540 --> 00:16:47,160
صورت میں ہوگا جب آپ کسی نہ کسی طرح یہاں موجود مواد کے ساتھ سرگرمی سے مشغول ہوجائیں۔

235
00:16:47,160 --> 00:16:51,920
ایک بہت ہی آسان چیز جو میں آپ سے کرنا چاہتا ہوں وہ یہ ہے کہ ابھی توقف کریں اور ایک

236
00:16:51,920 --> 00:16:57,560
لمحے کے لیے گہرائی سے سوچیں کہ آپ اس سسٹم میں کیا تبدیلیاں لا سکتے ہیں اور اگر آپ چاہتے ہیں

237
00:16:57,560 --> 00:17:01,880
کہ یہ کناروں اور نمونوں جیسی چیزوں کو بہتر طریقے سے اٹھانا چاہتے ہیں تو یہ تصاویر کو کیسے سمجھتا ہے۔

238
00:17:01,880 --> 00:17:06,360
لیکن اس سے بہتر، اصل میں مواد کے ساتھ مشغول ہونے کے لیے، میں گہرے

239
00:17:06,360 --> 00:17:09,720
سیکھنے اور اعصابی نیٹ ورکس پر مائیکل نیلسن کی کتاب کی سفارش کرتا ہوں۔

240
00:17:09,720 --> 00:17:15,200
اس میں، آپ اس درست مثال کے لیے ڈاؤن لوڈ اور کھیلنے کے لیے کوڈ اور ڈیٹا تلاش کر

241
00:17:15,200 --> 00:17:19,360
سکتے ہیں، اور کتاب آپ کو قدم بہ قدم چلائے گی کہ وہ کوڈ کیا کر رہا ہے۔

242
00:17:19,360 --> 00:17:23,920
حیرت انگیز بات یہ ہے کہ یہ کتاب مفت اور عوامی طور پر دستیاب ہے، لہذا اگر آپ اس سے

243
00:17:23,920 --> 00:17:28,040
کچھ حاصل کرتے ہیں، تو نیلسن کی کوششوں کے لیے عطیہ کرنے میں میرے ساتھ شامل ہونے پر غور کریں۔

244
00:17:28,040 --> 00:17:32,060
میں نے تفصیل میں کچھ دوسرے وسائل کو بھی جوڑا ہے جو مجھے بہت پسند

245
00:17:32,060 --> 00:17:38,720
ہیں، بشمول Chris Ola کی غیر معمولی اور خوبصورت بلاگ پوسٹ اور ڈسٹل میں مضامین۔

246
00:17:38,720 --> 00:17:41,960
گزشتہ چند منٹوں کے لیے چیزوں کو یہاں بند کرنے کے لیے، میں لیشا

247
00:17:41,960 --> 00:17:44,440
لی کے ساتھ کیے گئے انٹرویو کے ٹکڑوں میں واپس جانا چاہتا ہوں۔

248
00:17:44,440 --> 00:17:48,520
آپ اسے آخری ویڈیو سے یاد کر سکتے ہیں، اس نے اپنا پی ایچ ڈی کام ڈیپ لرننگ میں کیا۔

249
00:17:48,560 --> 00:17:52,240
اس چھوٹے سے ٹکڑوں میں، وہ دو حالیہ کاغذات کے بارے میں بات کرتی ہے جو واقعی اس بات

250
00:17:52,240 --> 00:17:56,380
کی کھوج لگاتے ہیں کہ تصویر کی شناخت کے کچھ جدید نیٹ ورک حقیقت میں کیسے سیکھ رہے ہیں۔

251
00:17:56,380 --> 00:18:00,320
صرف اس بات کو ترتیب دینے کے لیے کہ ہم بات چیت میں کہاں تھے، پہلے پیپر نے ان میں سے

252
00:18:00,320 --> 00:18:04,480
ایک خاص طور پر گہرے نیورل نیٹ ورکس کو لیا جو تصویر کی شناخت میں واقعی اچھا ہے، اور اسے مناسب

253
00:18:04,480 --> 00:18:09,400
طریقے سے لیبل والے ڈیٹاسیٹ پر تربیت دینے کے بجائے، اس نے تربیت سے پہلے تمام لیبلز کو بدل دیا۔

254
00:18:09,400 --> 00:18:13,840
ظاہر ہے کہ یہاں جانچ کی درستگی بے ترتیب سے بہتر نہیں ہوگی،

255
00:18:13,840 --> 00:18:15,320
کیوں کہ ہر چیز پر تصادفی طور پر لیبل لگا ہوا ہے۔

256
00:18:15,320 --> 00:18:20,080
لیکن یہ پھر بھی وہی تربیت کی درستگی حاصل کرنے میں کامیاب

257
00:18:20,080 --> 00:18:21,440
تھا جیسا کہ آپ ایک مناسب لیبل والے ڈیٹاسیٹ پر کریں گے۔

258
00:18:21,440 --> 00:18:26,120
بنیادی طور پر، اس مخصوص نیٹ ورک کے لیے لاکھوں وزن اس کے لیے صرف بے ترتیب ڈیٹا کو حفظ

259
00:18:26,120 --> 00:18:31,040
کرنے کے لیے کافی تھے، جس سے یہ سوال پیدا ہوتا ہے کہ کیا اس لاگت کے فنکشن کو کم

260
00:18:31,040 --> 00:18:36,720
سے کم کرنا دراصل تصویر میں موجود کسی بھی ساخت سے مطابقت رکھتا ہے، یا کیا یہ محض حفظ ہے؟

261
00:18:36,720 --> 00:18:40,120
. . . صحیح درجہ بندی کیا ہے اس کے پورے ڈیٹاسیٹ کو یاد کرنے کے لیے۔

262
00:18:40,120 --> 00:18:45,720
اور اسی طرح، آپ کو معلوم ہے، اس سال ICML میں آدھے سال بعد، بالکل

263
00:18:45,720 --> 00:18:50,440
ردی کاغذ نہیں تھا، لیکن کاغذ جس میں کچھ پہلوؤں پر توجہ دی گئی تھی،

264
00:18:50,440 --> 00:18:52,220
ارے، دراصل یہ نیٹ ورک اس سے کچھ زیادہ ہوشیار کام کر رہے ہیں۔

265
00:18:52,220 --> 00:18:59,600
اگر آپ اس درستگی کے منحنی خطوط پر نظر ڈالتے ہیں، اگر آپ صرف ایک بے ترتیب ڈیٹاسیٹ پر تربیت کر رہے

266
00:18:59,600 --> 00:19:05,240
تھے، تو وہ وکر کی طرح بہت نیچے چلا گیا، آپ جانتے ہیں، تقریباً ایک لکیری انداز میں بہت آہستہ آہستہ۔

267
00:19:05,280 --> 00:19:10,840
اس لیے آپ واقعی اس مقامی کم سے کم ممکنہ کو تلاش کرنے کے لیے جدوجہد

268
00:19:10,840 --> 00:19:12,320
کر رہے ہیں، آپ جانتے ہیں، صحیح وزن جو آپ کو درستگی حاصل کر سکے گا۔

269
00:19:12,320 --> 00:19:16,720
جب کہ اگر آپ واقعتاً ایک سٹرکچرڈ ڈیٹاسیٹ کی تربیت کر رہے ہیں، جس میں صحیح

270
00:19:16,720 --> 00:19:20,240
لیبلز ہیں، تو آپ جانتے ہیں، آپ شروع میں تھوڑا سا چکر لگاتے ہیں، لیکن پھر

271
00:19:20,240 --> 00:19:23,360
آپ اس درستگی کی سطح تک پہنچنے کے لیے بہت تیزی سے گر جاتے ہیں۔

272
00:19:23,360 --> 00:19:28,580
اور اس طرح کسی لحاظ سے اس مقامی میکسما کو تلاش کرنا آسان تھا۔

273
00:19:28,580 --> 00:19:32,900
اور اس طرح اس کے بارے میں جو چیز بھی دلچسپ تھی وہ یہ ہے

274
00:19:32,900 --> 00:19:39,140
کہ یہ حقیقت میں کچھ سال پہلے کا ایک اور مقالہ روشنی میں لاتا

275
00:19:39,140 --> 00:19:40,140
ہے، جس میں نیٹ ورک کی تہوں کے بارے میں بہت زیادہ آسانیاں ہیں۔

276
00:19:40,140 --> 00:19:43,880
لیکن نتائج میں سے ایک یہ کہہ رہا تھا کہ اگر آپ اصلاحی منظر نامے پر نظر ڈالیں

277
00:19:43,880 --> 00:19:49,400
تو مقامی منیما جو یہ نیٹ ورک سیکھنے کا رجحان رکھتے ہیں درحقیقت مساوی معیار کے ہوتے ہیں۔

278
00:19:49,400 --> 00:19:54,300
لہذا کسی لحاظ سے، اگر آپ کا ڈیٹا سیٹ ساختہ ہے، تو آپ کو اسے زیادہ آسانی سے تلاش کرنے کے قابل ہونا چاہیے۔

279
00:19:58,580 --> 00:20:01,140
میں ہمیشہ کی طرح آپ میں سے ان لوگوں کا شکریہ ادا کرتا ہوں جو پیٹریون کی حمایت کرتے ہیں۔

280
00:20:01,480 --> 00:20:05,440
میں پہلے بھی کہہ چکا ہوں کہ پیٹریون پر گیم چینجر کیا

281
00:20:05,440 --> 00:20:07,160
ہے، لیکن یہ ویڈیوز واقعی آپ کے بغیر ممکن نہیں ہوں گی۔

282
00:20:07,160 --> 00:20:11,540
میں VC فرم Amplify Partners اور سیریز میں ان ابتدائی ویڈیوز کے

283
00:20:11,540 --> 00:20:13,240
لیے ان کے تعاون کا بھی خصوصی شکریہ ادا کرنا چاہتا ہوں۔

284
00:20:31,140 --> 00:20:33,140
شکریہ

