[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "Geçen videoda bir sinir ağının yapısını anlatmıştım.",
  "model": "DeepL",
  "from_community_srt": "Son videoda bir sinir ağının yapısını ortaya koymuştum Zihnimizin tazelenmesi için burada kısa bir özet vereceğim",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "Aklımızda taze kalması için burada kısa bir özet geçeceğim ve ardından bu video için iki ana hedefim var.",
  "model": "DeepL",
  "from_community_srt": "Ve sonra bu video için iki ana hedefim var. Birincisi,",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "Birincisi, sadece sinir ağlarının nasıl öğrendiğinin değil, aynı zamanda diğer birçok makine öğreniminin de temelini oluşturan gradyan inişi fikrini tanıtmaktır.",
  "model": "DeepL",
  "from_community_srt": "\"Dereceli Azalma\" (Gradient Descent) fikrini tanıtmak, fikrin temeli sadece sinir ağlarının nasıl öğrendiği değil, ama diğer birçok makine öğrenimi nasıl iyi çalıştığı? Ardından,",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "Daha sonra bu özel ağın nasıl performans gösterdiğini ve nöronların gizli katmanlarının ne aradığını biraz daha inceleyeceğiz.",
  "model": "DeepL",
  "from_community_srt": "bu özel ağın nasıl performans gösterdiğini, Ve nöronların gizli katmanlarının sonunda aslında aradığımız şeye biraz daha bakacağız",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "Hatırlatmak gerekirse, buradaki hedefimiz, sinir ağlarının merhaba dünyası olan klasik el yazısı rakam tanıma örneğidir.",
  "model": "DeepL",
  "from_community_srt": "Bir hatırlatma olarak amacımızın klasik örneği, el yazısı rakam tanımadır. Sinir Ağlarının Merhaba Dünyası (The Hello World of Neural Networks) bu rakamlar,",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "Bu rakamlar 28x28 piksellik bir ızgarada işlenir ve her piksel 0 ile 1 arasında bir gri tonlama değerine sahiptir.",
  "model": "DeepL",
  "from_community_srt": "her bir pikselle 28 x 28 piksel ızgara üzerinde, 0 ve 1 arasında bazı gri tonlamalı değerlerle işlenir.",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "Bunlar, ağın giriş katmanındaki 784 nöronun aktivasyonlarını belirleyen şeylerdir.",
  "model": "DeepL",
  "from_community_srt": "aktivasyonları belirleyenler bunlar Ağın giriş katmanındaki 784 nöron Daha sonra,",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "Sonraki katmanlardaki her bir nöron için aktivasyon, bir önceki katmandaki tüm aktivasyonların ağırlıklı toplamına ve bias adı verilen bazı özel sayılara dayanır.",
  "model": "DeepL",
  "from_community_srt": "aşağıdaki katmanlarda her bir nöron için aktivasyon, ağırlıklı bir toplama dayanır. Önceki katmandaki tüm aktivasyonlar artı bir önyargı olarak adlandırılan bazı özel numaralar",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "Daha sonra bu toplamı sigmoid squishification veya relu gibi başka bir fonksiyonla, geçen videoda anlattığım şekilde oluşturursunuz.",
  "model": "DeepL",
  "from_community_srt": "sonra bu toplamı sigmoid yumuşatma gibi başka bir işlevle veya Son videoda yürüdüğüm şekilde bir ReLu",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "Toplamda, her biri 16 nöronlu iki gizli katmanın biraz keyfi seçimi göz önüne alındığında, ağın ayarlayabileceğimiz yaklaşık 13.000 ağırlığı ve önyargısı vardır ve ağın gerçekte tam olarak ne yaptığını belirleyen bu değerlerdir.",
  "model": "DeepL",
  "from_community_srt": "Ağın her birinde 16 nöron bulunan iki gizli katmanın bir şekilde keyfi seçimi göz önüne alındığında Ayarlayabileceğimiz 13.000 ağırlık ve önyargı ve tam olarak bildiğiniz ağın tam olarak ne olduğunu belirleyen bu değerler",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "O zaman bu ağın belirli bir rakamı sınıflandırdığını söylediğimizde kastettiğimiz şey, son katmandaki bu 10 nörondan en parlak olanının o rakama karşılık geldiğidir.",
  "model": "DeepL",
  "from_community_srt": "Öyleyse, bu ağın belirli bir rakamı sınıfladığımızda ne demek istediğimizi kastediyoruz Son kattaki bu 10 nöronun en parlakı bu rakamla aynı mı?",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "Ve hatırlayın, burada katmanlı yapı için aklımızdaki motivasyon, belki ikinci katmanın kenarları, üçüncü katmanın ilmekler ve çizgiler gibi desenleri algılayabileceği ve sonuncusunun da rakamları tanımak için bu desenleri bir araya getirebileceğiydi.",
  "model": "DeepL",
  "from_community_srt": "Katmanlı yapı için burada aklımızdaki motivasyonu hatırlayın belki de İkinci tabaka kenarları kapatabilir ve üçüncü tabaka ilmekler ve çizgiler gibi desenleri alabilir Ve sonuncusu rakamları tanımlamak için bu modelleri bir araya getirebilirdi.",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "Yani burada, ağın nasıl öğrendiğini öğreniyoruz.",
  "model": "DeepL",
  "from_community_srt": "İşte burada ağın nasıl öğrendiğini öğreniyoruz İstediğimiz şey,",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "İstediğimiz şey, bu ağa bir dizi eğitim verisi gösterebileceğiniz bir algoritmadır; bu veriler, el yazısıyla yazılmış rakamların bir dizi farklı görüntüsü ve bunların ne olması gerektiğine dair etiketler şeklinde gelir ve bu 13.000 ağırlık ve önyargıyı, eğitim verilerindeki performansını artırmak için ayarlayacaktır.",
  "model": "DeepL",
  "from_community_srt": "bu ağı bir sürü eğitim verisini gösterebileceğiniz bir algoritmadır. el yazısıyla yazılan rakamlardan oluşan bir dizi farklı imgenin yanı sıra olması gereken şey için etiketlerle birlikte geliyor. Bunları ayarlayacak 13000 ağırlık ve önyargı, eğitim verilerinin performansını arttırmak için Umarım bu katmanlı yapı,",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "Umarım bu katmanlı yapı, öğrendiklerinin bu eğitim verilerinin ötesindeki görüntülere de genellenebileceği anlamına gelir.",
  "model": "DeepL",
  "from_community_srt": "öğrendiği şey anlamına gelir bu eğitim verilerinin ötesindeki görüntülere genel bakış Ve test ettiğimiz yol ağ kurduktan sonra",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "Bunu test etmenin yolu, ağı eğittikten sonra ona daha önce hiç görmediği daha fazla etiketli veri göstermek ve bu yeni görüntüleri ne kadar doğru sınıflandırdığını görmektir.",
  "model": "DeepL",
  "from_community_srt": "Daha önce hiç görülmediği daha fazla etiketlenmiş olduğunu ve bu yeni görüntüleri ne kadar doğru bir şekilde sınıflandırdığını görüyorsunuz.",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "Neyse ki bizim için ve bunu başlamak için bu kadar yaygın bir örnek yapan şey, MNIST veritabanının arkasındaki iyi insanların, her biri olması gereken sayılarla etiketlenmiş on binlerce el yazısı rakam görüntüsünden oluşan bir koleksiyon oluşturmuş olmasıdır.",
  "model": "DeepL",
  "from_community_srt": "Neyse ki bizim için ve bu böyle ortak bir örneğini başlangıçta yapan şey, MNIST üssünün arkasındaki iyi insanların her biri olması gereken rakamlarla etiketlenmiş on binlerce el yazısı haneli imgenin bir araya getirilmesi ve",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "Ve bir makineyi öğrenen olarak tanımlamak ne kadar kışkırtıcı olsa da, nasıl çalıştığını gördüğünüzde, çılgın bir bilim kurgu önermesi gibi değil, daha çok bir hesap alıştırması gibi geliyor.",
  "model": "DeepL",
  "from_community_srt": "Nasıl çalıştığını bir kez gördüğünüzde bir makineyi öğrenme olarak tanımlamak gibi kışkırtıcıdır. Bazı çılgın bilimkurgu öncülüğüne ve daha iyi bir matematik egzersizine çok benziyor Temel olarak,",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "Yani, temelde belirli bir fonksiyonun minimumunu bulmaya dayanıyor.",
  "model": "DeepL",
  "from_community_srt": "belli bir fonksiyonun minimumunu bulmaya geliyor.",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "Unutmayın, kavramsal olarak, her nöronun bir önceki katmandaki tüm nöronlara bağlı olduğunu düşünüyoruz ve aktivasyonunu tanımlayan ağırlıklı toplamdaki ağırlıklar, bu bağlantıların gücü gibidir ve yanlılık, bu nöronun aktif veya inaktif olma eğiliminde olduğunun bir göstergesidir.",
  "model": "DeepL",
  "from_community_srt": "Kavramsal olarak her nöronun birbirine bağlı olduğunu düşünüyoruz. önceki katmandaki nöronların tümüne ve aktivasyonunu tanımlayan ağırlıklı toplamtaki ağırlıklara benzer bu bağlantıların güçlü yönleri Ve önyargı, nöronun aktif veya inaktif olma eğiliminde olup olmadığını ve bazı şeyleri başlatabileceğinin bir göstergesidir.",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "Başlangıç olarak, tüm bu ağırlıkları ve önyargıları tamamen rastgele bir şekilde başlatacağız.",
  "model": "DeepL",
  "from_community_srt": "Bu ağın gerçekleştirileceğini söylemek için tüm bu ağırlık ve önyargıları tamamen rastgele gereksiz hale getireceğiz.",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "Söylemeye gerek yok, bu ağ sadece rastgele bir şey yaptığı için belirli bir eğitim örneğinde oldukça kötü performans gösterecektir.",
  "model": "DeepL",
  "from_community_srt": "Belirli bir eğitim örneğinde oldukça korkunç bir şey var çünkü sadece 3'lük bir görüntüde besleniyorsunuz.",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "Örneğin, bu 3 resmini giriyorsunuz ve çıktı katmanı karmakarışık görünüyor.",
  "model": "DeepL",
  "from_community_srt": "Çıkış katmanı sadece karışıklık gibi görünüyor Yani yaptığınız şey,",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "Yani yaptığınız şey bir maliyet fonksiyonu tanımlamak, bilgisayara, hayır, kötü bilgisayar, bu çıktının çoğu nöron için 0, ancak bu nöron için 1 olan aktivasyonlara sahip olması gerektiğini söylemenin bir yolu, bana verdiğiniz şey tamamen çöp.",
  "model": "DeepL",
  "from_community_srt": "bir maliyet fonksiyonunu, bilgisayarı anlatmanın bir yolu olarak tanımlamanızdır: \"Kötü bir bilgisayar yok! Bu çıktı çoğu nöron için sıfır olan aktivasyonlara sahip olmalıydı,",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "Bunu biraz daha matematiksel olarak söylemek gerekirse, bu çöp çıktı aktivasyonlarının her biri ile sahip olmalarını istediğiniz değer arasındaki farkların karelerini toplarsınız ve buna tek bir eğitim örneğinin maliyeti diyeceğiz.",
  "model": "DeepL",
  "from_community_srt": "ama bu nöron için bana verdiğin şey tam bir çöplük \" Matematiksel olarak yaptığınız şeyden biraz daha fazlasını söylemek, aralarındaki farkların karelerini toparlamaktır. Bu çöp çıkışı aktivasyonlarının her birinin ve sahip olmasını istediğiniz değeri ve Bu, tek bir eğitim örneğinin bedeli olarak adlandıracağımız şeydir Ağ,",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "Bu toplamın, ağ görüntüyü kendinden emin bir şekilde doğru sınıflandırdığında küçük olduğuna, ancak ağ ne yaptığını bilmiyormuş gibi göründüğünde büyük olduğuna dikkat edin.",
  "model": "DeepL",
  "from_community_srt": "görüntüyü güvenle doğru şekilde sınıflandırdığında bu toplamın küçük olduğuna dikkat edin. Ancak ağın ne işe yaradığını gerçekten bilmediği zaman büyük oluyor.",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "O zaman yapacağınız şey, elinizdeki on binlerce eğitim örneğinin tümünün ortalama maliyetini göz önünde bulundurmaktır.",
  "model": "DeepL",
  "from_community_srt": "Öyleyse, yaptığınız şey, on binlerce eğitim örneğinin tamamında ortalama maliyeti göz önünde bulundurmanızdır.",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "Bu ortalama maliyet, ağın ne kadar kötü olduğuna ve bilgisayarın ne kadar kötü hissetmesi gerektiğine dair ölçümüzdür.",
  "model": "DeepL",
  "from_community_srt": "Bu ortalama maliyet, ağın ne kadar berbat olduğunun ve bilgisayarın ne kadar kötü hissetmesi gerektiğimizin ölçüsüdür ve bu karmaşık bir şeydir.",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "Ve bu karmaşık bir şey.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "Ağın kendisinin temelde bir fonksiyon olduğunu hatırlıyor musunuz; 784 sayıyı girdi olarak alan, piksel değerleri ve 10 sayıyı çıktı olarak veren ve bir anlamda tüm bu ağırlıklar ve yanlılıklar tarafından parametrelendirilen bir fonksiyon?",
  "model": "DeepL",
  "from_community_srt": "Ağın temel olarak içeride nasıl bir işlev olduğunu hatırlayın Piksel değerlerini girdi olarak 784 sayı ve çıkışı olarak bir sayı ve bir anlamda tükürür Tüm bu ağırlık ve önyargılarla parametrelendirildi Maliyet fonksiyonu,",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "Maliyet fonksiyonu bunun üzerine eklenen bir karmaşıklık katmanıdır.",
  "model": "DeepL",
  "from_community_srt": "girdi olarak aldığı kadarıyla karmaşık bir katmandır.",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "Girdi olarak bu 13.000 kadar ağırlık ve önyargıyı alır ve bu ağırlık ve önyargıların ne kadar kötü olduğunu açıklayan tek bir sayı verir ve bu sayının tanımlanma şekli ağın on binlerce eğitim verisi üzerindeki davranışına bağlıdır.",
  "model": "DeepL",
  "from_community_srt": "Bu on üç bin veya daha fazla ağırlık ve önyargı ve bu ağırlıklar ve önyargıların ne kadar kötü olduğunu açıklayan tek bir sayı çıkar Tanımlanma şekli ağın on binlerce eğitim verisi üzerindeki davranışına bağlıdır.",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "Düşünecek çok şey var.",
  "model": "DeepL",
  "from_community_srt": "Düşünecek çok şey var.",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "Ancak sadece bilgisayara ne kadar kötü bir iş yaptığını söylemek pek yardımcı olmuyor.",
  "model": "DeepL",
  "from_community_srt": "Ama sadece bilgisayara ne kadar berbat bir iş olduğunu söylüyorum, bu çok işe yaramıyor.",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "Daha iyi olması için bu ağırlıkları ve önyargıları nasıl değiştireceğini söylemek istersiniz.",
  "model": "DeepL",
  "from_community_srt": "Bu ağırlıkların ve önyargıların nasıl değiştirileceğini söylemek istersiniz,",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "Bunu kolaylaştırmak için, 13.000 girdisi olan bir fonksiyon hayal etmeye çalışmak yerine, girdi olarak bir sayı ve çıktı olarak bir sayı içeren basit bir fonksiyon hayal edin.",
  "model": "DeepL",
  "from_community_srt": "böylece daha iyi olur? 13.000 girdiyle bir işlevi hayal etmek yerine, daha kolay hale getirmek Sadece giriş olarak bir sayı ve çıktı olarak bir sayıya sahip basit bir işlev düşünün",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "Bu fonksiyonun değerini minimize eden bir girdiyi nasıl bulursunuz?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "Kalkülüs öğrencileri bazen bu minimum değeri açık bir şekilde bulabileceğinizi bilirler, ancak bu gerçekten karmaşık fonksiyonlar için her zaman mümkün değildir, kesinlikle bu durumun çılgın karmaşık sinir ağı maliyet fonksiyonumuz için 13.000 girdi versiyonunda mümkün değildir.",
  "model": "DeepL",
  "from_community_srt": "Bu işlevin değerini en aza indiren bir girişi nasıl buluyorsunuz? Matematik öğrencileri bazen en azından açıkça anlayabileceğinizi bilir Ama bu gerçekten karmaşık fonksiyonlar için her zaman mümkün değildir Çılgın karmaşık sinir ağı maliyet fonksiyonumuz için kesinlikle bu durumun on üç bin girdi sürümünde değil",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "Daha esnek bir taktik, herhangi bir girdiden başlamak ve bu çıktıyı düşürmek için hangi yönde adım atmanız gerektiğini bulmaktır.",
  "model": "DeepL",
  "from_community_srt": "Daha esnek bir taktik, herhangi bir eski girdide başlamak ve bu çıkışı daha düşük yapmak için hangi yöne doğru ilerlemeniz gerektiğine karar vermektir.",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "Özellikle, bulunduğunuz yerde fonksiyonun eğimini bulabilirseniz, bu eğim pozitifse sola kaydırın ve eğim negatifse girişi sağa kaydırın.",
  "model": "DeepL",
  "from_community_srt": "Özellikle, bulunduğunuz yerin fonksiyonunun eğimini belirleyebilirseniz Daha sonra bu eğim pozitifse sola kaydırın ve bu eğim negatifse girişi sağa kaydırın.",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "Bunu tekrar tekrar yaparsanız, her noktada yeni eğimi kontrol eder ve uygun adımı atarsanız, fonksiyonun bazı yerel minimumlarına yaklaşırsınız.",
  "model": "DeepL",
  "from_community_srt": "Bunu her noktada yeni eğimi kontrol ederek ve uygun adımı atıyorsanız bazı yerel işlevlere yaklaşacaksınız ve",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "Burada aklınıza gelebilecek görüntü, bir tepeden aşağı yuvarlanan bir top olabilir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "Dikkat edin, bu gerçekten basitleştirilmiş tek girdi fonksiyonu için bile, hangi rastgele girdiyle başladığınıza bağlı olarak inebileceğiniz birçok olası vadi vardır ve indiğiniz yerel minimumun maliyet fonksiyonunun mümkün olan en küçük değeri olacağının garantisi yoktur.",
  "model": "DeepL",
  "from_community_srt": "Burada aklınıza gelen görüntü bir tepe yuvarlanan bir toptur ve Bu gerçekten sadeleştirilmiş tek giriş fonksiyonu için bile dikkat edin, inebileceğiniz birçok olası vadi var. Hangi rastgele girdiye bağlı olduğunuza ve yerel minimumun garanti edilmediğine bağlı olarak. Arazi,",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "Bu bizim sinir ağı vakamıza da yansıyacaktır.",
  "model": "DeepL",
  "from_community_srt": "maliyet fonksiyonunun mümkün olan en küçük değeri olacak Bu da bizim sinir ağımızın davasına taşınacak ... ...",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "Ayrıca, adım boyutlarınızı eğimle orantılı hale getirirseniz, eğim minimuma doğru düzleşirken adımlarınızın nasıl küçüldüğünü ve bunun aşırıya kaçmanıza nasıl yardımcı olduğunu fark etmenizi istiyorum.",
  "model": "DeepL",
  "from_community_srt": "ve ben de seni fark etmeni istiyorum. Adım boyutlarınızı eğim ile orantılı olarak nasıl yaparsınız? Daha sonra, eğim minimum düzeye doğru düzleştiğinde, adımlarınız küçülür ve daha küçüktür ve bu tipler aşırı çekimden size yardımcı olur.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "Karmaşıklığı biraz artırarak, bunun yerine iki girdisi ve bir çıktısı olan bir fonksiyon hayal edin.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "Girdi uzayını xy düzlemi olarak ve maliyet fonksiyonunu da bunun üzerinde bir yüzey olarak düşünebilirsiniz.",
  "model": "DeepL",
  "from_community_srt": "Karmaşıklığı arttırmak biraz hayal etmek yerine iki giriş ve bir çıkış ile bir fonksiyon Giriş uzayını XY düzlemi ve bunun üzerinde bir yüzey olarak çizilen maliyet fonksiyonu olarak düşünebilirsiniz.",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "Fonksiyonun eğimini sormak yerine, fonksiyonun çıktısını en hızlı şekilde azaltmak için bu girdi uzayında hangi yönde adım atmanız gerektiğini sormanız gerekir.",
  "model": "DeepL",
  "from_community_srt": "Şimdi fonksiyonun eğimini sormak yerine, bu giriş alanına hangi yönde adım atmanız gerektiğini sormalısınız. Başka bir deyişle, fonksiyonun çıktısını en hızlı şekilde azaltmak için.",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "Başka bir deyişle, yokuş aşağı yön nedir?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "Yine, tepeden aşağı yuvarlanan bir topu düşünmek faydalı olacaktır.",
  "model": "DeepL",
  "from_community_srt": "Yokuş aşağı yön nedir? Ve yine bu tepeden aşağı doğru yuvarlanan bir top düşünmek yardımcı olur",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "Çok değişkenli hesaba aşina olanlar, bir fonksiyonun gradyanının size en dik yükseliş yönünü verdiğini, fonksiyonu en hızlı şekilde artırmak için hangi yöne adım atmanız gerektiğini bilecektir.",
  "model": "DeepL",
  "from_community_srt": "Çok değişkenli hesaplamaya aşina olanlar, bir fonksiyonun eğiminin size en dik çıkış yönü verdiğini bilir Temel olarak,",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "Doğal olarak, bu gradyanın negatifini almak size fonksiyonu en hızlı şekilde azaltan adım yönünü verir.",
  "model": "DeepL",
  "from_community_srt": "en hızlı şekilde işlevi artırmak için hangi yönde ilerlemelisiniz? doğal olarak bu eğimin negatif çekilmesi, size fonksiyonu en hızlı şekilde azaltan ve Daha da fazlası,",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "Bunun da ötesinde, bu eğim vektörünün uzunluğu, en dik eğimin ne kadar dik olduğunun bir göstergesidir.",
  "model": "DeepL",
  "from_community_srt": "bu eğim vektörünün uzunluğu aslında bu dik eğimin ne kadar dik olduğunu gösteren bir işarettir.",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "Çok değişkenli kalkülüs konusuna aşina değilseniz ve daha fazla bilgi edinmek istiyorsanız, Khan Academy için bu konuda yaptığım bazı çalışmalara göz atın.",
  "model": "DeepL",
  "from_community_srt": "Şimdi çok değişkenli hesaplara aşina değilseniz Ve konuyla ilgili Khan Academy için yaptığım çalışmaların bir kısmını daha fazla öğrenmek istiyorsun.",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "Dürüst olmak gerekirse, şu anda sizin ve benim için önemli olan tek şey, prensipte bu vektörü, yokuş aşağı yönün ne olduğunu ve ne kadar dik olduğunu söyleyen bu vektörü hesaplamanın bir yolunun olmasıdır.",
  "model": "DeepL",
  "from_community_srt": "Dürüst olmak gerekirse, şu anda sizin ve benim için önemli olan her şeye rağmen Bu prensipte, bu vektörü hesaplamanın bir yolu vardır. Ne olduğunu söyleyen bu vektör Yokuş aşağı yön ve ne kadar dik olursanız o kadar iyi olacaksınız.",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "Bildiğiniz tek şey buysa ve ayrıntılar konusunda sağlam değilseniz sorun yaşamazsınız.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "Bunu elde edebilirseniz, fonksiyonu minimize etme algoritması bu gradyan yönünü hesaplamak, ardından yokuş aşağı küçük bir adım atmak ve bunu tekrar tekrar yapmaktır.",
  "model": "DeepL",
  "from_community_srt": "Çünkü eğer algoritmanın işlevi en aza indirgemesini sağlayabiliyorsanız, bu eğim yönünü hesaplamak için aşağı doğru küçük bir adım atın ve Sadece tekrar tekrar et İki giriş yerine 13.000 girdisi olan bir işlev için aynı temel fikir,",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "Aynı temel fikir 2 giriş yerine 13.000 girişi olan bir fonksiyon için de geçerlidir.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "Ağımızın 13.000 ağırlığını ve önyargısını dev bir sütun vektöründe düzenlediğinizi düşünün.",
  "model": "DeepL",
  "from_community_srt": "her şeyi organize etmeyi hayal ediyor Ağımızın 13.000 ağırlık ve önyargıları devasa bir sütun vektörüne",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "Maliyet fonksiyonunun negatif gradyanı sadece bir vektördür, bu delicesine büyük girdi uzayı içinde, tüm bu sayılara hangi dürtmelerin maliyet fonksiyonunda en hızlı düşüşe neden olacağını söyleyen bir yöndür.",
  "model": "DeepL",
  "from_community_srt": "Maliyet fonksiyonunun negatif gradyanı sadece bir vektördür Bu müthiş kocaman giriş alanının içinde, hangi Bu sayıların hepsinin dürtülmesi, maliyet fonksiyonunda en hızlı azalmaya neden olacak ve tabii ki özel olarak tasarlanmış maliyet fonksiyonumuzla",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "Ve elbette, özel olarak tasarlanmış maliyet fonksiyonumuzla, bunu azaltmak için ağırlıkları ve önyargıları değiştirmek, ağın her bir eğitim verisi parçasındaki çıktısının 10 değerden oluşan rastgele bir dizi gibi görünmesini ve daha çok vermesini istediğimiz gerçek bir karar gibi görünmesini sağlamak anlamına gelir.",
  "model": "DeepL",
  "from_community_srt": "Ağırlık ve önyargıların azaltılması, ağın çıkışının her bir eğitim verisi parçası üzerinde yapılması anlamına gelir. Rastgele bir dizi on değere ve daha fazlasını yapmak istediğimiz gerçek bir karar gibi daha az görünün",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "Bu maliyet fonksiyonunun tüm eğitim verileri üzerinde bir ortalama içerdiğini hatırlamak önemlidir, bu nedenle bunu en aza indirirseniz, tüm bu örneklerde daha iyi bir performans elde edeceğiniz anlamına gelir.",
  "model": "DeepL",
  "from_community_srt": "Bu maliyet fonksiyonunun, tüm eğitim verilerinin üzerinde bir ortalamaya sahip olduğunu hatırlamak önemlidir Yani eğer onu en aza indirirseniz, tüm bu örneklerde daha iyi bir performans olduğu anlamına gelir.",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "Bir sinir ağının nasıl öğrendiğinin kalbi olan bu gradyanı verimli bir şekilde hesaplamak için kullanılan algoritmaya geri yayılım denir ve bir sonraki videoda bahsedeceğim şey budur.",
  "model": "DeepL",
  "from_community_srt": "Bir sinir ağının nasıl öğrenildiğinin kalbi olan etkin bir şekilde bu eğimi hesaplamak için kullanılan algoritma, geri yayılım olarak adlandırılır. Ve sıradaki video hakkında konuşacağım şey Orada gerçekten yürümek için zaman ayırmak istiyorum",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "Burada, belirli bir eğitim verisi parçası için her bir ağırlığa ve önyargıya tam olarak ne olduğunu gözden geçirmek için gerçekten zaman ayırmak istiyorum, ilgili hesap ve formül yığınının ötesinde neler olduğuna dair sezgisel bir his vermeye çalışıyorum.",
  "model": "DeepL",
  "from_community_srt": "Belirli bir eğitim verisi için her ağırlık ve her bir önyargı tam olarak ne olur? İlgili hesap ve formül yığınlarının ötesinde neler olup bittiğine dair sezgisel bir his vermeye çalışıyorum.",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "Tam burada, şu anda, uygulama detaylarından bağımsız olarak bilmenizi istediğim ana şey, bir ağın öğrenmesinden bahsettiğimizde kastettiğimiz şeyin sadece bir maliyet fonksiyonunu minimize etmek olduğudur.",
  "model": "DeepL",
  "from_community_srt": "Tam şimdi burada ana şey. Uygulama ayrıntılarından bağımsız olarak bilmeni istiyorum Bir ağ öğrenimi hakkında konuştuğumuzda demek istediğimiz şey, sadece bir maliyet fonksiyonunu en aza indirmektir.",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "Bunun bir sonucunun da, bu maliyet fonksiyonunun düzgün bir çıktıya sahip olmasının önemli olduğuna dikkat edin, böylece yokuş aşağı küçük adımlar atarak yerel bir minimum bulabiliriz.",
  "model": "DeepL",
  "from_community_srt": "Bunun bir sonucu, bu maliyet fonksiyonunun güzel bir pürüzsüz çıktıya sahip olmasının önemli olduğudur. Böylece yokuş aşağı küçük adımlar alarak yerel bir minimum bulabiliriz Bu arada bu yüzden Yapay nöronlar,",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "Bu arada, yapay nöronların biyolojik nöronlar gibi ikili bir şekilde aktif veya inaktif olmak yerine sürekli değişen aktivasyonlara sahip olmasının nedeni budur.",
  "model": "DeepL",
  "from_community_srt": "aktif veya aktif olmayan bir şekilde ikili aktivasyondan çok sürekli aktivasyonlara sahiptirler.",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "Bir fonksiyonun girdisini negatif gradyanın bir katı kadar tekrar tekrar dürtme işlemine gradyan inişi denir.",
  "model": "DeepL",
  "from_community_srt": "biyolojik nöronların yolu ise Negatif gradyanın bazı katları tarafından bir fonksiyonun bir girdisini tekrar tekrar nudging işlemi, gradyan alçalması olarak adlandırılır.",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "Bu, bir maliyet fonksiyonunun yerel minimumuna, temelde bu grafikteki bir vadiye doğru yakınsamanın bir yoludur.",
  "model": "DeepL",
  "from_community_srt": "Bu grafikte temelde bir vadi fonksiyonunun bazı yerel minimumlarına yaklaşmanın bir yolu.",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "Elbette hala iki girdili bir fonksiyonun resmini gösteriyorum, çünkü 13.000 boyutlu bir girdi uzayında dürtmelerin zihninizi sarması biraz zor, ancak bunu düşünmenin uzamsal olmayan güzel bir yolu var.",
  "model": "DeepL",
  "from_community_srt": "Hala iki girişli bir fonksiyonun resmini gösteriyorum, çünkü on üç bin boyutlu girişteki dalgalanmalar Uzay, zihninizi sarmak biraz zor, ama aslında bu konuda düşünmek için güzel bir mekansal yolu var.",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "Negatif gradyanın her bir bileşeni bize iki şey söyler.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "Elbette işaret bize girdi vektörünün ilgili bileşeninin yukarı mı yoksa aşağı mı itilmesi gerektiğini söyler.",
  "model": "DeepL",
  "from_community_srt": "Negatif gradyanın her bir bileşeni bize iki şey söyler; Giriş vektörünün bileşeni yukarı doğru veya aşağı doğru olmalıdır,",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "Ancak daha da önemlisi, tüm bu bileşenlerin göreceli büyüklükleri size hangi değişikliklerin daha önemli olduğunu söyler.",
  "model": "DeepL",
  "from_community_srt": "ancak önemli olan tüm bu bileşenlerin nispi büyüklükleri olmalıdır. Biraz daha önemli olan maddeyi söyler.",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "Gördüğünüz gibi, ağımızda ağırlıklardan birinde yapılacak bir ayarlama, maliyet fonksiyonu üzerinde başka bir ağırlıkta yapılacak ayarlamadan çok daha büyük bir etkiye sahip olabilir.",
  "model": "DeepL",
  "from_community_srt": "Ağımızda gördüğünüz ağırlıklardan birine göre ayarlamalar çok daha büyük olabilir. Maliyet fonksiyonu üzerindeki etki,",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "Bu bağlantılardan bazıları eğitim verilerimiz için daha önemlidir.",
  "model": "DeepL",
  "from_community_srt": "başka bir ağırlığa göre ayarlamadan daha fazla Bu bağlantılardan bazıları eğitim verilerimiz için daha önemlidir",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "Akıl almaz derecede büyük maliyet fonksiyonumuzun bu gradyan vektörünü düşünmenin bir yolu, her bir ağırlığın ve önyargının göreceli önemini, yani bu değişikliklerden hangisinin paranız için en büyük patlamayı taşıyacağını kodlamasıdır.",
  "model": "DeepL",
  "from_community_srt": "Aklımızın bu degrade vektörünü düşünebilmenin bir yolu. Masif maliyet fonksiyonu, her bir ağırlığın ve önyargının göreceli önemini kodlamasıdır. Bu değişikliklerden hangisinin paranız için en çok parayı taşıyacağı Bu gerçekten yönünü düşünmenin başka bir yoludur",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "Bu gerçekten de yön hakkında düşünmenin başka bir yoludur.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "Daha basit bir örnek vermek gerekirse, girdi olarak iki değişkenli bir fonksiyonunuz varsa ve belirli bir noktadaki gradyanının 3,1 olarak çıktığını hesaplarsanız, bunu bir yandan bu girdide durduğunuzda, bu yönde hareket etmenin fonksiyonu en hızlı şekilde artırdığı şeklinde yorumlayabilirsiniz; fonksiyonu girdi noktaları düzleminin üzerinde çizdiğinizde, bu vektör size düz yokuş yukarı yönü veren şeydir.",
  "model": "DeepL",
  "from_community_srt": "Bir giriş olarak iki değişkenli bir fonksiyonunuz varsa ve daha basit bir örnek almak için Belirli bir noktada onun gradyanı olarak hesaplayın (3,1) Sonra bir yandan, bu girdide durduğunuzu söyleyebilirsiniz. Bu yönde ilerlemek en hızlı işlevi artırır Fonksiyonu, vektörün giriş noktalarının düzleminin üzerinde çizdiğinizde, düz yokuş yukarı yön veren şeydir.",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "Ancak bunu okumanın bir başka yolu da, bu ilk değişkendeki değişikliklerin ikinci değişkendeki değişikliklerden 3 kat daha fazla öneme sahip olduğunu, en azından ilgili girdinin çevresinde, x değerini dürtmenin paranız için çok daha fazla patlama taşıdığını söylemektir.",
  "model": "DeepL",
  "from_community_srt": "Ama bunu okumak için başka bir yol, bu ilk değişkenin değiştiğini söylemek. En azından ilgili girdinin mahallinde ikinci değişkendeki değişiklikler olarak üç kat daha önemlidir. X değeri nudging,",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "Şimdi biraz uzaklaşalım ve şu ana kadar geldiğimiz noktayı özetleyelim.",
  "model": "DeepL",
  "from_community_srt": "paranız için daha fazla patlama taşır Tamam Şimdiye kadar uzaklaştık ve özetleyelim,",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "Ağın kendisi, tüm bu ağırlıklı toplamlar cinsinden tanımlanan 784 girdi ve 10 çıktıya sahip bu fonksiyondur.",
  "model": "DeepL",
  "from_community_srt": "ağın kendisi bu işlevin Bu ağırlıklı toplamların toplamı olarak tanımlanan 784 giriş ve 10 çıkış",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "Maliyet fonksiyonu bunun üzerine eklenen bir karmaşıklık katmanıdır.",
  "model": "DeepL",
  "from_community_srt": "maliyet fonksiyonu, bunun üstesinden geldiği karmaşıklık katmanıdır.",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "Girdi olarak 13.000 ağırlık ve önyargıyı alır ve eğitim örneklerine dayanarak tek bir kötülük ölçüsü ortaya çıkarır.",
  "model": "DeepL",
  "from_community_srt": "13.000 ağırlık ve önyargı, girdi olarak ve eğitim örneklerine dayanarak tek bir ölçüsüzlük çıkarır.",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "Maliyet fonksiyonunun gradyanı ise karmaşıklığın bir kat daha artmasına neden olur.",
  "model": "DeepL",
  "from_community_srt": "Maliyet fonksiyonunun gradyanı, bize daha fazla karmaşıklık katmanıdır.",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "Tüm bu ağırlıklara ve önyargılara yapılan hangi dürtmelerin maliyet fonksiyonunun değerinde en hızlı değişikliğe neden olduğunu bize söyler, bunu hangi ağırlıklarda hangi değişikliklerin en önemli olduğunu söylemek olarak yorumlayabilirsiniz.",
  "model": "DeepL",
  "from_community_srt": "Bu ağırlıkların ve önyargıların hepsinde neyin tetikleneceği, maliyet fonksiyonunun değerinde en hızlı değişime neden olur. Hangi yorumcular sizin için hangi ağırlıkların en çok hangi değişimlere dönüştüğünü söylüyor",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "Peki, ağı rastgele ağırlıklar ve önyargılarla başlattığınızda ve bunları bu gradyan iniş sürecine göre birçok kez ayarladığınızda, daha önce hiç görmediği görüntüler üzerinde gerçekte ne kadar iyi performans gösteriyor?",
  "model": "DeepL",
  "from_community_srt": "Bu nedenle, ağı rasgele ağırlıklarla ve önyargılarla başlattığınızda ve bu degrade iniş sürecine göre birçok kez ayarladığınızda Daha önce hiç görmediği görüntüler üzerinde ne kadar iyi performans gösteriyor? Burada,",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "Burada anlattığım, her biri 16 nörondan oluşan ve çoğunlukla estetik nedenlerle seçilen iki gizli katmanlı katman, gördüğü yeni görüntülerin yaklaşık %96'sını doğru sınıflandırarak hiç de fena değil.",
  "model": "DeepL",
  "from_community_srt": "her biri çoğunlukla estetik nedenlerle seçilen, on altı nöronun iki gizli katmanı ile anlattığım şey peki, kötü değil, doğru gördüğü yeni görüntülerin yüzde 96'sını sınıflandırıyor.",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "Ve dürüst olmak gerekirse, berbat ettiği bazı örneklere bakarsanız, biraz gevşemek zorunda hissediyorsunuz.",
  "model": "DeepL",
  "from_community_srt": "Dürüst olmak gerekirse, bazı şeyleri karıştırırsanız, biraz rahatsız olursunuz.",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "Şimdi gizli katman yapısıyla oynar ve birkaç ince ayar yaparsanız, bunu %98'e kadar çıkarabilirsiniz.",
  "model": "DeepL",
  "from_community_srt": "Şimdi gizli katman yapısıyla oynar ve bir çift tweaks yaparsanız Bunu% 98'e kadar alabilirsiniz ve bu oldukça iyi.",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "Ve bu oldukça iyi!",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "En iyisi değil, bu sade vanilya ağından daha sofistike hale gelerek kesinlikle daha iyi performans elde edebilirsiniz, ancak başlangıçtaki görevin ne kadar göz korkutucu olduğu göz önüne alındığında, daha önce hiç görmediği görüntülerde bu kadar iyi performans gösteren herhangi bir ağın inanılmaz bir şey olduğunu düşünüyorum, çünkü ona hangi kalıpları arayacağını özellikle söylemedik.",
  "model": "DeepL",
  "from_community_srt": "En iyi değil Bu sade vanilya ağından daha karmaşık hale getirerek daha iyi performans elde edebilirsiniz. Ama ilk görevin ne kadar korkutucu olduğu göz önüne alındığında sadece bir şey olduğunu düşünüyorum? Daha önce hiç görmediği resimlerde bunu iyi yapan herhangi bir ağ hakkında inanılmaz Asla bunun için hangi kalıpların aranacağını hiç söylemediğimizi",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "Başlangıçta, bu yapıyı motive etme şeklim, ikinci katmanın küçük kenarları algılayabileceği, üçüncü katmanın ilmekleri ve daha uzun çizgileri tanımak için bu kenarları bir araya getirebileceği ve bunların rakamları tanımak için bir araya getirilebileceği umudunu tanımlamaktı.",
  "model": "DeepL",
  "from_community_srt": "Aslında bu yapıyı motive ettiğim yol, sahip olabileceğimiz bir ümidi açıklamaktı. İkinci katın küçük kenarlarda kalması Üçüncü katmanın bu kenarları ilmekleri ve daha uzun çizgileri tanıması için bir araya getireceğini ve bunların rakamları tanımak için birbirine birleştirilebileceğini",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "Peki ağımızın gerçekte yaptığı şey bu mu?",
  "model": "DeepL",
  "from_community_srt": "Yani bizim ağımız aslında böyle mi yapıyor?",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "En azından bu seferki için, hiç de değil.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "Geçen videoda, birinci katmandaki tüm nöronlardan ikinci katmandaki belirli bir nörona giden bağlantıların ağırlıklarının, ikinci katman nöronunun algıladığı belirli bir piksel deseni olarak nasıl görselleştirilebileceğini incelediğimizi hatırlıyor musunuz?",
  "model": "DeepL",
  "from_community_srt": "En azından bunun için bir tane Bir şey değil Son videonun nasıl ağırlandıklarına baktık Birinci tabakadaki tüm nöronların ikinci tabakadaki belirli bir nörona bağlantıları",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "Aslında bunu ilk katmandan diğerine geçişlerle ilişkili ağırlıklar için yaptığımızda, burada ve orada izole edilmiş küçük kenarları seçmek yerine, neredeyse rastgele görünüyorlar, sadece ortada bazı çok gevşek desenler var.",
  "model": "DeepL",
  "from_community_srt": "Belirli bir piksel modeli olarak görselleştirilebilir ki bu ikinci katman nöronu toplar Aslında bu geçişlerle ilişkili ağırlıklar için bunu ilk katmandan diğerine yaptığımızda Burada ve burada küçük kenarları kaldırmak yerine. Neredeyse rastgele görünüyorlar Sadece ortada çok gevşek desenler koymak o kadar geniş olmayan görünüyor ki orada",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "Görünüşe göre, olası ağırlıkların ve önyargıların akıl almaz derecede büyük 13.000 boyutlu uzayında, ağımız, çoğu görüntüyü başarılı bir şekilde sınıflandırmasına rağmen, umduğumuz kalıpları tam olarak algılamayan mutlu küçük bir yerel minimum buldu.",
  "model": "DeepL",
  "from_community_srt": "Ağımızın olası ağırlık ve önyargılarının 13,000 boyutlu uzaması, kendimizi mutlu küçük yerel minimum buldu. Çoğu görüntüyü başarılı bir şekilde tasnif etmemize rağmen, umduğumuz şekilleri tam olarak almıyoruz ve Bu noktayı gerçekten sürmek için,",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "Bu noktayı gerçekten vurgulamak için rastgele bir görüntü girdiğinizde ne olduğunu izleyin.",
  "model": "DeepL",
  "from_community_srt": "rastgele bir görüntüyü girdiğinizde ne olacağını izleyin.",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "Sistem akıllı olsaydı, kararsız hissetmesini, belki de bu 10 çıkış nöronundan hiçbirini gerçekten aktive etmemesini veya hepsini eşit şekilde aktive etmesini bekleyebilirdiniz, ancak bunun yerine, sanki bu rastgele gürültünün 5 olduğundan, 5'in gerçek bir görüntüsünün 5 olduğundan emin olduğu gibi emin bir şekilde size saçma bir cevap verir.",
  "model": "DeepL",
  "from_community_srt": "Sistem akıllı olsaydı, belirsiz hissetmemeyi bekleyebilir, belki de bu 10 çıkış nöronundan herhangi birini aktive edemez veya Hepsini eşit olarak aktive etmek Ama bunun yerine Bu rastgele sesin gerçekte olduğu gibi 5 olduğundan emin olduğunu sanki emin bir şekilde biraz saçma cevap verir. 5 bir 5 görüntüdür Bu ağ,",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "Başka bir ifadeyle, bu ağ rakamları oldukça iyi tanıyabilse bile, onları nasıl çizeceği konusunda hiçbir fikri yoktur.",
  "model": "DeepL",
  "from_community_srt": "rakamları oldukça iyi tanımasına rağmen farklı bir şekilde ifade edip, bunları nasıl çizebileceğine dair hiçbir fikri yoktur.",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "Bunun büyük bir kısmı, çok sıkı bir şekilde kısıtlanmış bir eğitim düzeni olmasından kaynaklanıyor.",
  "model": "DeepL",
  "from_community_srt": "Bunun nedeni, çok sıkı bir şekilde eğitilmiş bir eğitim kurulumudur.",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "Yani, kendinizi kanalın yerine koyun.",
  "model": "DeepL",
  "from_community_srt": "Ağın ayakkabısına kendini buraya koyduğunu kastediyorum,",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "Onun bakış açısına göre, tüm evren küçük bir ızgarada ortalanmış net bir şekilde tanımlanmış hareketsiz rakamlardan başka bir şey değildir ve maliyet fonksiyonu ona kararlarında son derece emin olmaktan başka bir şey yapması için hiçbir teşvik sağlamamıştır.",
  "model": "DeepL",
  "from_community_srt": "tüm evren hiçbir şeyden ibaret değildir. Ancak, açıkça tanımlanmış, küçük bir ızgarada ortalanmış basamaksız rakamlar ve maliyet fonksiyonu, Bir şey olmak için teşvik,",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "İkinci katman nöronlarının gerçekte ne yaptığının görüntüsü bu olduğuna göre, bu ağı neden kenarları ve desenleri tespit etme motivasyonuyla tanıttığımı merak edebilirsiniz.",
  "model": "DeepL",
  "from_community_srt": "ama kararlarında tamamen kendine güvenen Yani eğer bu ikinci katmandaki nöronların gerçekte ne olduğuna dair bir görüntü varsa Bu ağı neden kenarları ve desenleri toplama motivasyonuyla sunacağımı merak edebilirsiniz.",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "Yani, sonuçta hiç de öyle olmuyor.",
  "model": "DeepL",
  "from_community_srt": "Demek istediğim, hiç bitmeyecek bir şey değil.",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "Bunun nihai hedefimiz değil, bir başlangıç noktası olması gerekiyor.",
  "model": "DeepL",
  "from_community_srt": "Bu, bizim nihai hedefimiz değil, açık bir başlangıç ​​noktası olmak demek değildir.",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "Açıkçası, bu eski bir teknoloji, 80'li ve 90'lı yıllarda araştırılan türden ve daha ayrıntılı modern varyantları anlayabilmeniz için önce onu anlamanız gerekiyor ve açıkça bazı ilginç sorunları çözebiliyor, ancak bu gizli katmanların gerçekte ne yaptığını ne kadar çok araştırırsanız, o kadar az akıllı görünüyor.",
  "model": "DeepL",
  "from_community_srt": "Bu eski teknoloji 80'li ve 90'lı yıllarda araştırılan türden ve Daha ayrıntılı modern varyantları anlayabilmeniz için bunu anlamanız gerekir ve açıkça bazı ilginç problemleri çözebilme yeteneğine sahiptir. Ama bu gizli katmanların ne kadar çok kazandığını daha az zeki yapıyor gibi görünüyor.",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "Odağı bir an için ağların nasıl öğrendiğinden sizin nasıl öğrendiğinize kaydırırsak, bu ancak buradaki materyalle bir şekilde aktif olarak ilgilenirseniz gerçekleşecektir.",
  "model": "DeepL",
  "from_community_srt": "Ağın nasıl öğrendiğini öğrenmek için odağı bir anlığına değiştirmek Bu sadece bir şekilde burada malzeme ile aktif olarak ilgilenirseniz gerçekleşir",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "Sizden yapmanızı istediğim oldukça basit bir şey, şu anda durup bir an için bu sistemde ne gibi değişiklikler yapabileceğinizi ve kenar ve desen gibi şeyleri daha iyi algılamasını istiyorsanız görüntüleri nasıl algıladığını derinlemesine düşünmeniz.",
  "model": "DeepL",
  "from_community_srt": "Yapmanı istediğim çok basit bir şey şu an sadece duraksamak ve bir an için derinden düşünmek. Bu sistemde yapabileceğiniz değişiklikler Kenarları ve desenleri gibi şeyleri daha iyi ele almak istiyorsan görüntüleri nasıl algılıyor?",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "Ancak bundan daha iyisi, materyalle gerçekten ilgilenmek için Michael Nielsen'in derin öğrenme ve sinir ağları üzerine yazdığı kitabı şiddetle tavsiye ederim.",
  "model": "DeepL",
  "from_community_srt": "Ama aslında bu malzemeyle meşgul olmaktan daha iyi ben Michael Nielsen’in kitabını derinlemesine ve sinir ağlarında tavsiye ederim.",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "Kitapta, tam olarak bu örnek için indirip oynayabileceğiniz kodu ve verileri bulabilirsiniz ve kitap size bu kodun ne yaptığını adım adım gösterecektir.",
  "model": "DeepL",
  "from_community_srt": "İçinde bu tam örnek için indirmek ve oynamak için kod ve verileri bulabilirsiniz Ve kitap, bu kodun ne yaptığını adım adım size gösterecektir.",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "Harika olan şey, bu kitabın ücretsiz ve herkese açık olması, bu yüzden eğer bir şeyler öğrenirseniz, Nielsen'in çabalarına bağış yapmak için bana katılmayı düşünün.",
  "model": "DeepL",
  "from_community_srt": "Harika olan, bu kitabın ücretsiz ve herkese açık olması. Öyleyse eğer bir şey çıkarırsanız, Nielsen'in çabalarına karşı bir bağışta bulunmam için bana katılmayı düşünün.",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "Ayrıca Chris Ola'nın olağanüstü ve güzel blog yazısı ve Distill'deki makaleler de dahil olmak üzere açıklamada çok beğendiğim birkaç kaynağa daha bağlantı verdim.",
  "model": "DeepL",
  "from_community_srt": "Ayrıca, açıklamada çok beğendiğim birkaç başka kaynak da ekledim. Chris Ola ve damıtılmış makaleleri tarafından olağanüstü ve güzel blog yazısı Son birkaç dakika için buradaki şeyleri kapatmak için",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "Son birkaç dakikayı burada tamamlamak için Leisha Lee ile yaptığım röportajın bir bölümüne geri dönmek istiyorum.",
  "model": "DeepL",
  "from_community_srt": "Leisha Lee ile yaptığım röportajın bir parçasına geri dönmek istiyorum Onu son videodan hatırlayabilirsin.",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "Kendisini son videodan hatırlayabilirsiniz, doktora çalışmasını derin öğrenme alanında yapmıştı.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "Bu küçük parçacıkta, daha modern görüntü tanıma ağlarından bazılarının gerçekte nasıl öğrendiğini gerçekten araştıran iki yeni makaleden bahsediyor.",
  "model": "DeepL",
  "from_community_srt": "Doktora çalışmasını derin öğrenmede ve bu küçük pasajda yaptı Daha modern görüntü tanıma ağlarının bazılarının aslında nasıl öğrendiğini gerçekten anlatan iki yeni makaleden bahsediyor",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "Sadece konuşmanın neresinde olduğumuzu ayarlamak için, ilk makale görüntü tanımada gerçekten iyi olan bu özellikle derin sinir ağlarından birini aldı ve düzgün bir şekilde etiketlenmiş bir veri kümesi üzerinde eğitmek yerine, eğitimden önce tüm etiketleri karıştırdı.",
  "model": "DeepL",
  "from_community_srt": "Sadece konuşmamızda nerede olduğumuzu belirlemek için ilk kağıt bu derin nöral ağlardan birini aldı. Görüntü tanımada gerçekten iyi ve uygun şekilde etiketlenmiş veriler üzerinde çalışmak yerine Antrenmandan önce tüm etiketlerin etrafını karıştırın Açıkçası test tespiti doğruluğu,",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "Açıkçası buradaki test doğruluğu rastgele olandan daha iyi değildi, çünkü her şey rastgele etiketlenmişti, ancak yine de uygun şekilde etiketlenmiş bir veri kümesinde elde edeceğiniz aynı eğitim doğruluğunu elde edebildi.",
  "model": "DeepL",
  "from_community_srt": "her şey rastgele etiketlendiğinden, rasgele olmaktan daha iyi olmayacaktı. Fakat yine de düzgün etiketlenmiş veri kümesinde yaptığınız gibi aynı eğitim doğruluğunu elde edebildi.",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "Temel olarak, bu özel ağ için milyonlarca ağırlık, sadece rastgele verileri ezberlemesi için yeterliydi, bu da bu maliyet fonksiyonunu en aza indirmenin gerçekten görüntüdeki herhangi bir yapıya mı karşılık geldiği yoksa sadece ezberleme mi olduğu sorusunu gündeme getiriyor.",
  "model": "DeepL",
  "from_community_srt": "Temel olarak bu özel ağ için milyonlarca ağırlık, sadece rastgele verileri ezberlemek için yeterliydi Bu maliyet işlevini en aza indirmenin, görüntüdeki herhangi bir yapıya karşılık gelip gelmediği sorusunu hangi türden sorgular? Yoksa sadece biliyor musun?",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "Bu doğruluk eğrisine bakarsanız, rastgele bir veri kümesi üzerinde eğitim yapıyor olsaydınız, bu eğri neredeyse doğrusal bir şekilde çok yavaş bir şekilde aşağı inerdi, bu nedenle size bu doğruluğu sağlayacak olası doğru ağırlıkların yerel minimumunu bulmak için gerçekten mücadele ediyorsunuz.",
  "model": "DeepL",
  "from_community_srt": "tüm ezberlemek Doğru sınıflandırmanın ne olduğuna dair veri seti ve bu yüzden birkaçınız bu yıl ICML'de yarım yıl sonra biliyorsunuz Tamamen çığlık atan bir kağıt kağıdı yoktu. Aslında bu ağlar, bu doğruluk eğrisine bakarsanız biraz daha akıllı bir şey yapıyorlar. eğer sadece bir antrenman yapıyor olsaydın Rasgele veri seti, eğri türünün çok azaldığını biliyorsunuz. Yani gerçekten mümkün olan yerel azamı bulmakta zorlanıyorsunuz Eğer gerçek bir yapıya sahip bir veri seti üzerinde eğitim alıyorsanız,",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "Oysa doğru etiketlere sahip yapılandırılmış bir veri kümesi üzerinde eğitim yapıyorsanız, başlangıçta biraz kurcalarsınız, ancak daha sonra bu doğruluk seviyesine ulaşmak için çok hızlı bir şekilde düşersiniz ve bu nedenle bir anlamda yerel maksimumları bulmak daha kolaydır.",
  "model": "DeepL",
  "from_community_srt": "doğruluk elde edebilecek doğru ağırlıkları bilirsiniz. Doğru etiketler. Başından beri birazcık kuklacağını biliyorsun, ama sonra buna çok hızlı düştün. Doğruluk seviyesi ve bir anlamda bunu bulmak daha kolaydı Yerel maxima ve bu yüzden de ilginçti,",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "Bunun ilginç bir yanı da aslında birkaç yıl önce yayınlanan ve ağ katmanları hakkında çok daha fazla basitleştirmeye sahip olan başka bir makaleyi gün ışığına çıkarmasıdır, ancak sonuçlardan biri, optimizasyon ortamına bakarsanız, bu ağların öğrenme eğiliminde olduğu yerel minimumların aslında eşit kalitede olduğunu söylüyordu, bu nedenle bir anlamda veri kümeniz yapılandırılmışsa, bunu çok daha kolay bulabilmeniz gerekir.",
  "model": "DeepL",
  "from_community_srt": "yakalanan şey aslında birkaç yıl önce başka bir kağıda ışık getirdi. Hangisi daha çok ağ katmanları hakkında basitleştirmeler Ancak sonuçlardan biri, bu ağların öğrenmeye eğilimli olduğu yerel optimizasyon optimizasyon ortamına bakıp bakmadığınızı gösteriyordu. Aslında eşit kalitede,",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "Her zaman olduğu gibi Patreon'da destek verenlere teşekkür ederim.",
  "model": "DeepL",
  "from_community_srt": "yani bir şekilde veri kümenizin yapısı varsa ve bunu çok daha kolay bulabilmeniz gerekir Patreon'u destekleyenlere her zamanki gibi teşekkürler",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "Patreon'un ne kadar büyük bir oyun değiştirici olduğunu daha önce de söylemiştim, ancak bu videolar siz olmadan gerçekten mümkün olmazdı.",
  "model": "DeepL",
  "from_community_srt": "Daha önce bir oyun değiştirici patreonun ne olduğunu söyledim ama bu videolar sensiz mümkün olmazdı.",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "Ayrıca, serinin bu ilk videolarına destek veren VC firması Amplify Partners'a da özel olarak teşekkür etmek istiyorum.",
  "model": "DeepL",
  "from_community_srt": "Ayrıca özel bir vermek istiyorum. Serideki bu ilk videoları desteklemelerinde VC firması amplifi ortakları sayesinde",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]