1
00:00:00,000 --> 00:00:07,240
前回のビデオでは、ニューラル ネットワークの構造を説明しました。

2
00:00:07,240 --> 00:00:11,560
記憶に新しいように、ここで簡単に要約します。それか

3
00:00:11,560 --> 00:00:13,160
ら、このビデオには 2 つの主な目標があります。

4
00:00:13,160 --> 00:00:17,960
1 つ目は、勾配降下の考え方を導入することです。これは、ニューラル ネットワ

5
00:00:17,960 --> 00:00:20,800
ークの学習方法だけでなく、他の多くの機械学習の仕組みの基礎にもなっています。

6
00:00:20,800 --> 00:00:25,160
その後、この特定のネットワークがどのように動作するか、そしてニューロンの

7
00:00:25,160 --> 00:00:29,560
隠れた層が最終的に何を探すのかについてもう少し詳しく掘り下げていきます。

8
00:00:29,560 --> 00:00:34,680
念のため言っておきますが、ここでの目標は手書き数字認識の古典的な例

9
00:00:34,680 --> 00:00:37,080
、つまりニューラル ネットワークの Hello World です。

10
00:00:37,080 --> 00:00:42,160
これらの数字は 28x28 ピクセル グリッド上にレンダリングされ

11
00:00:42,160 --> 00:00:44,260
、各ピクセルは 0 から 1 までのグレースケール値を持ちます。

12
00:00:44,260 --> 00:00:51,400
これらは、ネットワークの入力層の 784 個のニューロンの活性化を決定するものです。

13
00:00:51,400 --> 00:00:56,880
次の層の各ニューロンの活性化は、前の層のすべての活性化の重み付き合

14
00:00:56,880 --> 00:01:02,300
計に、バイアスと呼ばれる特別な数値を加えたものに基づいています。

15
00:01:02,300 --> 00:01:07,480
前回のビデオで説明した方法である、シグモイド圧縮や R

16
00:01:07,480 --> 00:01:09,640
eLU などの他の関数を使用してその合計を計算します。

17
00:01:09,640 --> 00:01:14,960
合計すると、それぞれ 16 個のニューロンを持つ 2 つの隠れ層というやや恣意

18
00:01:14,960 --> 00:01:20,940
的な選択を考慮すると、ネットワークには調整できる重みとバイアスが約 13,00

19
00:01:20,940 --> 00:01:25,320
0 あり、ネットワークが実際に何を行うかを正確に決定するのはこれらの値です。

20
00:01:25,320 --> 00:01:29,800
そして、このネットワークが特定の数字を分類すると言うときの意味は、最後の層にある

21
00:01:29,800 --> 00:01:34,080
10 個のニューロンの中で最も明るいものがその数字に対応するということです。

22
00:01:34,080 --> 00:01:39,240
レイヤー構造について私たちが念頭に置いていた動機は、おそ

23
00:01:39,240 --> 00:01:43,920
らく 2 番目のレイヤーでエッジを認識し、3 番目のレイ

24
00:01:43,920 --> 00:01:48,640
ヤーでループやラインなどのパターンを認識し、最後のレイヤ

25
00:01:48,640 --> 00:01:49,640
ーでそれらのパターンをつなぎ合わせて、数字を認識します。

26
00:01:49,640 --> 00:01:52,880
ここでは、ネットワークがどのように学習するかを学びます。

27
00:01:52,880 --> 00:01:56,880
私たちが望んでいるのは、このネットワークに大量のトレーニング データを表示でき

28
00:01:56,880 --> 00:02:01,540
るアルゴリズムです。このデータは、手書きの数字のさまざまな画像と、それらが本来

29
00:02:01,540 --> 00:02:06,360
あるべきものを示すラベルの形で提供されます。トレーニング データのパフォーマン

30
00:02:06,360 --> 00:02:10,760
スを向上させるために、これらの 13,000 の重みとバイアスを調整します。

31
00:02:10,760 --> 00:02:15,540
この階層構造により、学習した内容がトレーニング デ

32
00:02:15,540 --> 00:02:17,840
ータを超えた画像に一般化されることが期待されます。

33
00:02:17,840 --> 00:02:22,240
これをテストする方法は、ネットワークをトレーニングした後、さらにラベル付けされた

34
00:02:22,240 --> 00:02:31,160
データを表示し、新しい画像がどの程度正確に分類されているかを確認することです。

35
00:02:31,160 --> 00:02:34,760
私たちにとって幸運なことに、そしてまずこれが一般的な例となっているのは、MNI

36
00:02:34,760 --> 00:02:39,520
ST データベースの背後にいる善良な人々が、それぞれが本来あるべき数字でラベ

37
00:02:39,520 --> 00:02:45,080
ル付けされた何万もの手書きの数字画像のコレクションをまとめてくれたことです。

38
00:02:45,080 --> 00:02:49,920
機械を学習すると表現するのは挑発的ですが、それがどのように機能するかを一度理解すると、

39
00:02:49,920 --> 00:02:55,560
それはおかしな SF の前提というよりも、はるかに微積分の練習のように感じられます。

40
00:02:55,560 --> 00:03:01,040
つまり、基本的には、特定の機能の最小値を見つけることになります。

41
00:03:01,040 --> 00:03:06,480
概念的には、各ニューロンは前の層のすべてのニューロンに接続されていると考

42
00:03:06,480 --> 00:03:11,440
えており、その活性化を定義する加重合計の重みはそれらの接続の強さのよう

43
00:03:11,440 --> 00:03:16,400
なものであり、バイアスは何らかの指標であることを覚えておいてください。

44
00:03:16,400 --> 00:03:19,780
そのニューロンが活動的になる傾向があるか、不活動的である傾向があるか。

45
00:03:19,780 --> 00:03:23,300
まず、これらの重みとバイアスをすべ

46
00:03:23,300 --> 00:03:25,020
て完全にランダムに初期化します。

47
00:03:25,020 --> 00:03:29,100
言うまでもなく、このネットワークはランダムに何かを実行しているだけ

48
00:03:29,100 --> 00:03:31,180
なので、特定のトレーニング例ではひどいパフォーマンスになります。

49
00:03:31,180 --> 00:03:36,820
たとえば、この 3 の画像を入力すると、出力レイヤーは混乱したように見えます。

50
00:03:36,820 --> 00:03:43,340
そこで、あなたがすることは、コスト関数を定義することです。これは、出力がほとんどのニューロンでは 0 ですが、この

51
00:03:43,340 --> 00:03:48,940
ニューロンでは 1 である活性化を持つべきであることをコンピューター、いや、悪いコンピューターに伝える方法です。

52
00:03:48,980 --> 00:03:51,740
あなたが私にくれたものは全くのゴミです。

53
00:03:51,740 --> 00:03:56,740
これをもう少し数学的に言うと、これらのゴミ出力アクティベーショ

54
00:03:56,740 --> 00:04:01,980
ンのそれぞれと、それらに必要な値との差の二乗を合計します。これ

55
00:04:01,980 --> 00:04:06,020
が、単一のトレーニング サンプルのコストと呼ばれるものです。

56
00:04:06,020 --> 00:04:12,660
ネットワークが自信を持って画像を正しく分類している場合にはこの合計は小さくなりますが、ネットワーク

57
00:04:12,660 --> 00:04:18,820
が何をしているかを認識していないように見える場合にはこの合計が大きくなることに注意してください。

58
00:04:18,820 --> 00:04:23,860
したがって、自由に使える数万のトレーニング サン

59
00:04:23,860 --> 00:04:27,580
プルすべての平均コストを考慮することになります。

60
00:04:27,580 --> 00:04:32,300
この平均コストは、ネットワークがどの程度劣悪であるか、およ

61
00:04:32,300 --> 00:04:33,300
びコンピュータの動作がどの程度悪くなるかを示す尺度です。

62
00:04:33,300 --> 00:04:35,300
それは複雑なことです。

63
00:04:35,300 --> 00:04:40,380
ネットワーク自体が基本的に 784 個の数値、ピクセル値を入力として取り込み、1

64
00:04:40,380 --> 00:04:46,100
0 個の数値を出力として吐き出す関数であったことを覚えていますか? ある意味、

65
00:04:46,100 --> 00:04:49,700
ネットワークはこれらすべての重みとバイアスによってパラメーター化されています。

66
00:04:49,700 --> 00:04:53,340
コスト関数は、その上に複雑な層が重なっています。

67
00:04:53,340 --> 00:04:59,140
それらの 13,000 ほどの重みとバイアスを入力として受け取り、それらの重みと

68
00:04:59,140 --> 00:05:04,620
バイアスがどれほど悪いかを説明する 1 つの数値を吐き出します。その定義方法は、

69
00:05:04,620 --> 00:05:09,140
数万のトレーニング データすべてに対するネットワークの動作によって異なります。

70
00:05:09,140 --> 00:05:12,460
それは考えるべきことがたくさんあります。

71
00:05:12,460 --> 00:05:16,380
しかし、コンピューターがどのようなくだらない仕事をしているかを単にコンピューターに伝えるだけでは、あまり役に立ちません。

72
00:05:16,380 --> 00:05:21,300
これらの重みとバイアスを変更して改善する方法を教えたいと考えています。

73
00:05:21,300 --> 00:05:25,580
わかりやすくするために、13,000 個の入力を持つ関数を想像するのに苦労するのではなく

74
00:05:25,580 --> 00:05:31,440
、入力として 1 つの数値、出力として 1 つの数値を持つ単純な関数を想像してください。

75
00:05:31,440 --> 00:05:36,420
この関数の値を最小にする入力をどのように見つけますか?

76
00:05:36,420 --> 00:05:41,300
微積分の学生は、その最小値を明示的に計算できる場合があることを知っているでしょうが、本

77
00:05:41,340 --> 00:05:46,620
当に複雑な関数では常に実現可能であるとは限りません。もちろん、この状況の 13,000

78
00:05:46,620 --> 00:05:51,640
入力バージョンでは、非常に複雑なニューラル ネットワークのコスト関数では不可能です。

79
00:05:51,640 --> 00:05:56,820
より柔軟な戦術は、任意の入力から開始して、その出力を下げる

80
00:05:56,820 --> 00:05:59,860
ためにどの方向にステップを進めるべきかを判断することです。

81
00:05:59,860 --> 00:06:05,020
具体的には、現在の関数の傾きがわかる場合は

82
00:06:05,020 --> 00:06:09,280
、その傾きが正の場合は左にシフトし、その

83
00:06:09,280 --> 00:06:12,720
傾きが負の場合は入力を右にシフトします。

84
00:06:12,720 --> 00:06:17,040
これを繰り返し実行し、各ポイントで新しい傾きをチェックし、適

85
00:06:17,040 --> 00:06:20,680
切な手順を実行すると、関数の極小値に近づくことになります。

86
00:06:20,680 --> 00:06:24,600
ここで皆さんが思い浮かべるイメージは、丘を転がり落ちるボールです。

87
00:06:24,600 --> 00:06:29,380
そして、この非常に単純化された単一入力関数であっても、ど

88
00:06:29,380 --> 00:06:34,220
のランダム入力から開始するかに応じて、到達する可能性のあ

89
00:06:34,220 --> 00:06:38,460
る谷が多数あり、到達する極小値が可能な限り最小の値になる

90
00:06:38,460 --> 00:06:39,460
という保証はないことに注意してください。コスト関数の。

91
00:06:39,460 --> 00:06:43,180
これはニューラル ネットワークの場合にも当てはまります。

92
00:06:43,180 --> 00:06:48,140
また、ステップ サイズを勾配に比例させると、勾配が最小に

93
00:06:48,140 --> 00:06:52,920
向かって平坦になるとステップがどんどん小さくなり、オーバ

94
00:06:52,920 --> 00:06:56,020
ーシュートを防ぐことができることにも注目してください。

95
00:06:56,020 --> 00:07:01,640
もう少し複雑にして、代わりに 2 つの入力と 1 つの出力を持つ関数を想像してください。

96
00:07:01,640 --> 00:07:06,360
入力空間を xy 平面、コスト関数をその上の

97
00:07:06,360 --> 00:07:09,020
面としてグラフ化すると考えることができます。

98
00:07:09,020 --> 00:07:13,600
関数の傾きについて尋ねる代わりに、関数の出力を最も早く減少させるために

99
00:07:13,600 --> 00:07:19,780
は、この入力空間でどの方向にステップすべきかを尋ねる必要があります。

100
00:07:19,780 --> 00:07:22,340
言い換えれば、下り坂の方向は何ですか？

101
00:07:22,340 --> 00:07:26,740
繰り返しますが、その丘を転がり落ちるボールを想像すると分かります。

102
00:07:26,740 --> 00:07:31,920
多変数微積分に詳しい人は、関数の勾配によって最

103
00:07:31,920 --> 00:07:37,460
も急な上昇の方向がわかり、関数を最も早く増加さ

104
00:07:37,460 --> 00:07:39,420
せるにはどの方向に進むべきかがわかるでしょう。

105
00:07:39,420 --> 00:07:43,820
当然のことながら、その勾配をマイナスにすると、関数

106
00:07:43,820 --> 00:07:47,460
を最も早く減少させるステップの方向がわかります。

107
00:07:47,460 --> 00:07:52,320
さらに、この勾配ベクトルの長さは、その最も

108
00:07:52,320 --> 00:07:54,580
急な勾配がどれほど急であるかを示します。

109
00:07:54,580 --> 00:07:58,080
多変数微積分に詳しくなく、さらに詳しく知りたい場合は、このテーマに

110
00:07:58,080 --> 00:08:01,100
関して私がカーン アカデミーで行った作品の一部を確認してください。

111
00:08:01,100 --> 00:08:05,680
しかし正直に言って、あなたと私にとって現時点で重要なのは、

112
00:08:05,680 --> 00:08:10,440
原理的にはこのベクトル、つまり下り坂の方向とその急勾配を

113
00:08:10,440 --> 00:08:12,040
示すベクトルを計算する方法が存在するということだけです。

114
00:08:12,040 --> 00:08:17,280
知っていることがこれだけで、詳細がしっかりしていなくても大丈夫です。

115
00:08:17,280 --> 00:08:21,440
それができれば、関数を最小化するアルゴリズムは、この勾配の方向を計算し、下り

116
00:08:21,440 --> 00:08:27,400
坂に向かって小さな一歩を踏み出し、それを何度も繰り返すことになるからです。

117
00:08:28,300 --> 00:08:33,700
これは、2 つの入力ではなく 13,000 の入力を持つ関数の基本的な考え方と同じです。

118
00:08:33,700 --> 00:08:38,980
ネットワークの 13,000 の重みとバイアスをすべ

119
00:08:38,980 --> 00:08:40,180
て巨大な列ベクトルに編成することを想像してください。

120
00:08:40,180 --> 00:08:46,140
コスト関数の負の勾配は単なるベクトルであり、この非常に巨大な入

121
00:08:46,140 --> 00:08:51,660
力空間内の何らかの方向であり、これらすべての数値に対するどの

122
00:08:51,660 --> 00:08:55,900
微調整がコスト関数の最も急速な減少を引き起こすかを示します。

123
00:08:55,900 --> 00:09:00,000
そしてもちろん、特別に設計されたコスト関数を使用して、重みとバイアスを

124
00:09:00,000 --> 00:09:05,520
変更して値を下げることは、トレーニング データの各部分に対するネット

125
00:09:05,520 --> 00:09:10,280
ワークの出力を 10 個の値のランダムな配列のように見せることを意味

126
00:09:10,280 --> 00:09:11,280
し、より実際に望む決定に近づけることを意味します。それを作るのです。

127
00:09:11,280 --> 00:09:15,940
覚えておくことが重要です。このコスト関数にはすべてのトレーニング データの平均が含まれるた

128
00:09:15,940 --> 00:09:24,260
め、これを最小化すると、それらのサンプルすべてでパフォーマンスが向上することになります。

129
00:09:24,260 --> 00:09:28,540
この勾配を効率的に計算するためのアルゴリズムは、事実上、ニュ

130
00:09:28,540 --> 00:09:32,520
ーラル ネットワークの学習方法の中心であり、バックプロパゲー

131
00:09:32,520 --> 00:09:34,040
ションと呼ばれます。これについては、次のビデオで説明します。

132
00:09:34,040 --> 00:09:39,100
そこでは、時間をかけて、特定のトレーニング データの各重みとバイアスに正

133
00:09:39,100 --> 00:09:44,100
確に何が起こっているのかを詳しく見ていき、関連する計算や公式の山の向こう

134
00:09:44,100 --> 00:09:47,980
で何が起こっているのかを直感的に感じられるようにしたいと考えています。

135
00:09:47,980 --> 00:09:51,780
今ここで、実装の詳細とは関係なく、皆さんに知っておいて

136
00:09:51,780 --> 00:09:56,820
いただきたい主な点は、ネットワーク学習について話すとき

137
00:09:56,820 --> 00:09:59,320
の意味は、コスト関数を最小化するだけだということです。

138
00:09:59,320 --> 00:10:02,760
そして、その結果の 1 つとして、このコスト関数が良好な滑らか

139
00:10:02,760 --> 00:10:07,820
な出力を持つことが重要であることに注意してください。これにより

140
00:10:07,820 --> 00:10:09,340
、下り坂を少しずつ進むことで極小値を見つけることができます。

141
00:10:09,340 --> 00:10:14,140
ちなみに、生物学的ニューロンのように単に二値的に

142
00:10:14,140 --> 00:10:18,580
活性または不活性になるのではなく、人工ニューロン

143
00:10:18,580 --> 00:10:20,440
が継続的に範囲の活性化を行うのはこのためです。

144
00:10:20,440 --> 00:10:24,600
負の勾配の倍数によって関数の入力を繰り返し微調

145
00:10:24,600 --> 00:10:26,960
整するこのプロセスは、勾配降下法と呼ばれます。

146
00:10:26,960 --> 00:10:31,760
これは、コスト関数の局所最小値、つまりこ

147
00:10:31,760 --> 00:10:33,000
のグラフの谷に向かって収束する方法です。

148
00:10:33,000 --> 00:10:37,040
もちろん、13,000 次元の入力空間でのナッジは少し理解するのが

149
00:10:37,040 --> 00:10:41,480
難しいため、まだ 2 つの入力を持つ関数の図を示していますが、実

150
00:10:41,480 --> 00:10:45,220
際には、これについて考えるための優れた非空間的な方法があります。

151
00:10:45,220 --> 00:10:49,100
負の勾配の各成分から 2 つのことが分かります。

152
00:10:49,100 --> 00:10:53,600
もちろん、この符号は、入力ベクトルの対応するコンポーネン

153
00:10:53,600 --> 00:10:55,860
トを上または下に微調整する必要があるかどうかを示します。

154
00:10:55,860 --> 00:11:01,340
しかし重要なのは、これらすべての要素の相対的な大きさによ

155
00:11:01,340 --> 00:11:05,620
って、どの変更がより重要であるかがわかるということです。

156
00:11:05,620 --> 00:11:09,780
私たちのネットワークでは、重みの 1 つを調整すると、他の重みを調整

157
00:11:09,780 --> 00:11:14,980
するよりもコスト関数にはるかに大きな影響を与える可能性があります。

158
00:11:14,980 --> 00:11:19,440
これらの接続の中には、トレーニング データにとってより重要なものもあります。

159
00:11:19,440 --> 00:11:23,520
したがって、気が遠くなるような膨大なコスト関数のこの勾配ベクトルについ

160
00:11:23,520 --> 00:11:29,740
て考える方法は、各重みとバイアスの相対的な重要性、つまり、これらの変

161
00:11:29,740 --> 00:11:34,100
更のどれが最も費用対効果が高いかをエンコードしているということです。

162
00:11:34,100 --> 00:11:37,360
これは方向性についての単なる考え方です。

163
00:11:37,360 --> 00:11:41,740
より単純な例を挙げると、入力として 2 つの変数を持つ関

164
00:11:41,740 --> 00:11:48,720
数があり、ある特定の点での勾配が 3,1 になると計算

165
00:11:48,720 --> 00:11:52,880
した場合、一方では、次のように解釈できます。その入力に立

166
00:11:52,880 --> 00:11:57,400
って、この方向に沿って移動すると、関数が最も早く増加し

167
00:11:57,400 --> 00:12:02,200
ます。つまり、入力点の平面上で関数をグラフにすると、その

168
00:12:02,200 --> 00:12:03,200
ベクトルが真っ直ぐ上り坂の方向を与えることになります。

169
00:12:03,200 --> 00:12:07,600
しかし、これを別の読み方で読むと、この最初の変数への変更は 2 番目の変数

170
00:12:07,600 --> 00:12:12,400
への変更の 3 倍の重要性があり、少なくとも関連する入力の付近では、X 値

171
00:12:12,400 --> 00:12:17,740
を微調整する方がはるかに大きな影響を与えるということになります。バック。

172
00:12:17,740 --> 00:12:22,880
さて、ズームアウトしてこれまでの状況をまとめてみましょう。

173
00:12:22,880 --> 00:12:28,660
ネットワーク自体は、784 個の入力と 10 個の出力を備えた関

174
00:12:28,660 --> 00:12:30,860
数であり、これらすべての重み付けされた合計によって定義されます。

175
00:12:30,860 --> 00:12:34,160
コスト関数は、その上に複雑な層が重なっています。

176
00:12:34,160 --> 00:12:39,300
13,000 の重みとバイアスを入力として受け取り、ト

177
00:12:39,300 --> 00:12:42,640
レーニング例に基づいて粗さの単一の尺度を吐き出します。

178
00:12:42,640 --> 00:12:47,520
コスト関数の勾配はさらに複雑な層になります。

179
00:12:47,520 --> 00:12:52,860
これは、これらすべての重みとバイアスに対してどのような調整がコ

180
00:12:52,860 --> 00:12:56,640
スト関数の値に最も速い変化を引き起こすかを示します。これは、ど

181
00:12:56,640 --> 00:13:03,040
の重みに対するどの変更が最も重要かを示していると解釈できます。

182
00:13:03,040 --> 00:13:07,620
では、ランダムな重みとバイアスを使用してネットワークを初期化し、この

183
00:13:07,620 --> 00:13:12,420
勾配降下プロセスに基づいてそれらを何度も調整すると、これまでに見たこ

184
00:13:12,420 --> 00:13:14,240
とのない画像で実際にどの程度のパフォーマンスが得られるでしょうか?

185
00:13:14,240 --> 00:13:19,000
私がここで説明したものは、それぞれ 16 個のニューロンからなる 2 つの隠れ層を備えており、主に美

186
00:13:19,000 --> 00:13:26,920
的理由から選択されていますが、悪くはなく、表示される新しい画像の約 96% を正しく分類しています。

187
00:13:26,920 --> 00:13:31,580
そして正直に言うと、それが台無しにしているいくつか

188
00:13:31,580 --> 00:13:36,300
の例を見ると、少し緩めなければならないと感じます。

189
00:13:36,300 --> 00:13:40,220
隠れ層構造を試していくつかの調整を加えれば

190
00:13:40,220 --> 00:13:41,220
、これを最大 98% まで達成できます。

191
00:13:41,220 --> 00:13:42,900
それはとても良いことです！

192
00:13:42,900 --> 00:13:47,020
これは最高ではありません。この単純なバニラ ネットワークよりも洗練されれば、確かにパフォ

193
00:13:47,020 --> 00:13:52,460
ーマンスを向上させることはできますが、最初のタスクがどれほど困難であるかを考えると、こ

194
00:13:52,460 --> 00:13:56,800
れまで見たことのない画像でこれほどうまく動作するネットワークには、信じられないほどの何

195
00:13:56,800 --> 00:14:02,000
かがあると思います。どのようなパターンを探すべきかを具体的に指示したことはありません。

196
00:14:02,000 --> 00:14:07,840
もともと、私がこの構造を動機付けた方法は、2 番目の層が小さ

197
00:14:07,840 --> 00:14:11,880
なエッジを検出し、3 番目の層がそれらのエッジをつなぎ合わせ

198
00:14:11,880 --> 00:14:16,080
てループや長い線を認識し、それらがつなぎ合わされるかもしれな

199
00:14:16,080 --> 00:14:18,220
いという希望を説明することでした。一緒に数字を認識します。

200
00:14:18,220 --> 00:14:21,040
では、これは私たちのネットワークが実際に行っていることなのでしょうか?

201
00:14:21,040 --> 00:14:24,880
まあ、少なくともこれに関しては、まったくそうではありません。

202
00:14:24,960 --> 00:14:29,120
前回のビデオで、最初の層のすべてのニューロンから 2 番目の層の特定のニュ

203
00:14:29,120 --> 00:14:33,900
ーロンへの接続の重みが、2 番目の層のニューロンが認識している特定のピクセ

204
00:14:33,900 --> 00:14:37,440
ル パターンとしてどのように視覚化できるかを説明したことを覚えていますか?

205
00:14:37,440 --> 00:14:44,600
これらのトランジションに関連付けられたウェイトに対してこれを行う

206
00:14:44,600 --> 00:14:51,000
と、あちこちで孤立した小さなエッジが検出されるのではなく、ほぼラ

207
00:14:51,000 --> 00:14:54,200
ンダムに見え、中央に非常に緩いパターンがいくつかあるだけです。

208
00:14:54,200 --> 00:14:59,020
考えられる重みとバイアスの計り知れないほど広い 13,000

209
00:14:59,020 --> 00:15:04,020
次元空間において、私たちのネットワークは、ほとんどの画像を正

210
00:15:04,020 --> 00:15:08,440
常に分類したにもかかわらず、私たちが期待していたパターンを正

211
00:15:08,440 --> 00:15:09,840
確に検出できない、幸せな小さな局所最小値を見つけたようです。

212
00:15:09,840 --> 00:15:14,600
この点をよく理解するには、ランダムな画像を入力したときに何が起こるかを見てください。

213
00:15:14,600 --> 00:15:19,240
システムが賢いものであれば、10 個の出力ニューロンのどれも実際には活性化していないのか

214
00:15:19,240 --> 00:15:24,120
、あるいはそれらすべてを均等に活性化しているのか、不確かに感じられるか、あるいは、この

215
00:15:24,520 --> 00:15:29,800
ランダムなニューロンが確実であるかのように、自信を持ってナンセンスな答えを与えることを期

216
00:15:29,800 --> 00:15:34,560
待するかもしれません。 5 の実際の画像が 5 であるのと同様に、ノイズも 5 です。

217
00:15:34,560 --> 00:15:39,300
別の言い方をすると、このネットワークは数字をかなりう

218
00:15:39,300 --> 00:15:41,800
まく認識できても、それを描画する方法がわかりません。

219
00:15:41,800 --> 00:15:45,400
その多くは、トレーニングの設定が非常に厳しく制限されているためです。

220
00:15:45,400 --> 00:15:48,220
つまり、ここではネットワークの立場に立って考えてみましょう。

221
00:15:48,220 --> 00:15:53,280
その観点から見ると、宇宙全体は小さなグリッドの中心にある、明確に

222
00:15:53,280 --> 00:15:58,560
定義された不動の数字だけで構成されており、そのコスト関数は、その

223
00:15:58,560 --> 00:16:02,160
決定に完全な自信を持つこと以外には何の動機も与えませんでした。

224
00:16:02,160 --> 00:16:05,760
これが第 2 層のニューロンが実際に行っていることのイメー

225
00:16:05,760 --> 00:16:09,320
ジであるため、なぜエッジやパターンを検出するという動機でこ

226
00:16:09,320 --> 00:16:10,320
のネットワークを紹介するのか不思議に思うかもしれません。

227
00:16:10,320 --> 00:16:13,040
つまり、それは最終的にはまったくそうではありません。

228
00:16:13,040 --> 00:16:17,480
これは最終目標ではなく、出発点です。

229
00:16:17,480 --> 00:16:22,280
率直に言って、これは 80 年代から 90 年代に研究された種類の古いテクノロジ

230
00:16:22,280 --> 00:16:26,920
ーであり、より詳細な現代の亜種を理解する前に、それを理解する必要があります。ま

231
00:16:26,920 --> 00:16:31,380
た、明らかにいくつかの興味深い問題を解決できる可能性がありますが、何を深く掘り下

232
00:16:31,380 --> 00:16:38,720
げるほど、これらの隠れ層が実際に機能しているほど、その層は知性が低く見えます。

233
00:16:38,720 --> 00:16:43,540
ネットワークがどのように学習するかということから、あなたがどのように学習するかに少し焦点を移し

234
00:16:43,540 --> 00:16:47,160
ますが、それは、何らかの方法でここで取り上げた資料に積極的に取り組んだ場合にのみ起こります。

235
00:16:47,160 --> 00:16:51,920
皆さんにしていただきたいのは、非常に簡単なことの 1 つです。今ちょっと立ち止まって

236
00:16:51,920 --> 00:16:57,560
、このシステムにどのような変更を加えるか、エッジやパターンなどをよりよく認識できるよ

237
00:16:57,560 --> 00:17:01,880
うにしたい場合に画像をどのように認識するかについて、少しの間深く考えてみてください。

238
00:17:01,880 --> 00:17:06,360
しかしそれよりも、実際に内容に取り組むには、深層学習とニューラル ネット

239
00:17:06,360 --> 00:17:09,720
ワークに関する Michael Nielsen の本を強くお勧めします。

240
00:17:09,720 --> 00:17:15,200
この中には、まさにこの例をダウンロードして試すためのコードとデータ

241
00:17:15,200 --> 00:17:19,360
が含まれており、そのコードが何をしているのかを段階的に説明します。

242
00:17:19,360 --> 00:17:23,920
素晴らしいのは、この本が無料で一般公開されているということです。この本から何かを得る

243
00:17:23,920 --> 00:17:28,040
ことができましたら、私と一緒にニールセンの取り組みに寄付することを検討してください。

244
00:17:28,040 --> 00:17:32,060
また、Chris Ola による驚異的で美しいブログ投稿や Distill

245
00:17:32,060 --> 00:17:38,720
の記事など、私がとても気に入っている他のリソースも説明にリンクしました。

246
00:17:38,720 --> 00:17:41,960
最後の数分間をここで締めくくるために、リーシャ・

247
00:17:41,960 --> 00:17:44,440
リーとのインタビューの抜粋に戻りたいと思います。

248
00:17:44,440 --> 00:17:48,520
前回のビデオで彼女を覚えているかもしれません。彼女は深層学習で博士号の研究をしていました。

249
00:17:48,560 --> 00:17:52,240
この短い抜粋では、最新の画像認識ネットワークの一部が実際にどのように学習し

250
00:17:52,240 --> 00:17:56,380
ているかを深く掘り下げた 2 つの最近の論文について彼女が話しています。

251
00:17:56,380 --> 00:18:00,320
会話の状況を説明するために、最初の論文では、画像認識に優れた特にディープ ニュ

252
00:18:00,320 --> 00:18:04,480
ーラル ネットワークの 1 つを使用し、適切にラベル付けされたデータセットでト

253
00:18:04,480 --> 00:18:09,400
レーニングする代わりに、トレーニング前にすべてのラベルをシャッフルしました。

254
00:18:09,400 --> 00:18:13,840
すべてがランダムにラベル付けされているだけであるため

255
00:18:13,840 --> 00:18:15,320
、ここでのテストの精度は明らかにランダムと同等です。

256
00:18:15,320 --> 00:18:20,080
ただし、適切にラベル付けされたデータセットを使用した場

257
00:18:20,080 --> 00:18:21,440
合と同じトレーニング精度を達成することはできました。

258
00:18:21,440 --> 00:18:26,120
基本的に、この特定のネットワークの何百万もの重みは、ランダムなデータを記憶

259
00:18:26,120 --> 00:18:31,040
するだけで十分でした。このため、このコスト関数の最小化が実際に画像内の何

260
00:18:31,040 --> 00:18:36,720
らかの構造に対応するのか、それとも単なる記憶なのかという疑問が生じます。

261
00:18:36,720 --> 00:18:40,120
。。。正しい分類が何であるかをデータセット全体を記憶するためです。

262
00:18:40,120 --> 00:18:45,720
それで、半年後の今年の ICML では、正確には反論の論文はありませ

263
00:18:45,720 --> 00:18:50,440
んでしたが、実際には、これらのネットワークはそれよりもう少し賢いこ

264
00:18:50,440 --> 00:18:52,220
とをしている、といったいくつかの側面を取り上げた論文がありました。

265
00:18:52,220 --> 00:18:59,600
その精度曲線を見ると、ランダムなデータセットでトレーニングしているだけだ

266
00:18:59,600 --> 00:19:05,240
とすると、その曲線は、ほぼ直線的に、非常にゆっくりと下降していきます。

267
00:19:05,280 --> 00:19:10,840
したがって、その精度を実現する適切な重みの可能

268
00:19:10,840 --> 00:19:12,320
な極小値を見つけるのに非常に苦労しています。

269
00:19:12,320 --> 00:19:16,720
一方、適切なラベルを持つ構造化データセットで実際にトレー

270
00:19:16,720 --> 00:19:20,240
ニングしている場合、最初は少しいじってみましたが、その

271
00:19:20,240 --> 00:19:23,360
精度レベルに達するまでに非常に早く落ちてしまいました。

272
00:19:23,360 --> 00:19:28,580
したがって、ある意味、極大値を見つけるのが簡単でした。

273
00:19:28,580 --> 00:19:32,900
そして、これに関して興味深いのは、実際に数年前に発行さ

274
00:19:32,900 --> 00:19:39,140
れた別の論文が明らかになったことであり、この論文では

275
00:19:39,140 --> 00:19:40,140
ネットワーク層についてさらに単純化が行われています。

276
00:19:40,140 --> 00:19:43,880
しかし、その結果の 1 つは、最適化の状況を見ると、これらのネットワーク

277
00:19:43,880 --> 00:19:49,400
が学習する傾向にある極小値が実際には同じ品質であることを示しています。

278
00:19:49,400 --> 00:19:54,300
したがって、ある意味、データセットが構造化されていれば、それをより簡単に見つけることができるはずです。

279
00:19:58,580 --> 00:20:01,140
Patreon でサポートしてくださっている皆様にいつも感謝しています。

280
00:20:01,480 --> 00:20:05,440
Patreon におけるゲームチェンジャーが何であるかについては以

281
00:20:05,440 --> 00:20:07,160
前に述べましたが、これらのビデオは本当に皆さんなしでは不可能です。

282
00:20:07,160 --> 00:20:11,540
また、VC 会社 Amplify Partners と、シリーズの最

283
00:20:11,540 --> 00:20:13,240
初のビデオに対する彼らのサポートにも特別な感謝を表したいと思います。

284
00:20:31,140 --> 00:20:33,140
ありがとう。

