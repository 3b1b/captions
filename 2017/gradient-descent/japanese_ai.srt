1
00:00:00,000 --> 00:00:07,240
前回のビデオでは、ニューラル ネットワークの構造を説明しました。

2
00:00:07,240 --> 00:00:11,560
記憶に新しいように、ここで簡単に要約します。それから、このビデオには 2

3
00:00:11,560 --> 00:00:13,160
つの主な目標があります。

4
00:00:13,160 --> 00:00:17,960
1 つ目は、勾配降下の考え方を導入することです。これは、ニューラル

5
00:00:17,960 --> 00:00:20,800
ネットワークの学習方法だけでなく、他の多くの機械学習の仕組みの基礎にもなっています。

6
00:00:20,800 --> 00:00:25,160
その後、この特定のネットワークがどのように動作するか、そしてニューロンの隠れた層が最終的に何を探すのかについてもう少し詳しく掘り下げていきます。

7
00:00:25,160 --> 00:00:29,560


8
00:00:29,560 --> 00:00:34,680
念のために言っておきますが、ここでの目標は手書き数字認識の古典的な例、つまりニューラル ネットワークの Hello

9
00:00:34,680 --> 00:00:37,080
World です。

10
00:00:37,080 --> 00:00:42,160
これらの数字は 28x28 ピクセル グリッド上にレンダリングされ、各ピクセルは

11
00:00:42,160 --> 00:00:44,260
0 から 1 までのグレースケール値を持ちます。

12
00:00:44,260 --> 00:00:51,400
これらは、ネットワークの入力層の 784 個のニューロンの活性化を決定するものです。

13
00:00:51,400 --> 00:00:56,880
次の層の各ニューロンの活性化は、前の層のすべての活性化の重み付き合計に、バイアスと呼ばれる特別な数値を加えたものに基づいています。

14
00:00:56,880 --> 00:01:02,300


15
00:01:02,300 --> 00:01:07,480
前回のビデオで説明した方法である、シグモイド圧縮や ReLU

16
00:01:07,480 --> 00:01:09,640
などの他の関数を使用してその合計を計算します。

17
00:01:09,640 --> 00:01:14,960
合計すると、それぞれ 16 個のニューロンを持つ

18
00:01:14,960 --> 00:01:20,940
2 つの隠れ層というやや恣意的な選択を考慮すると、ネットワークには調整できる重みとバイアスが約

19
00:01:20,940 --> 00:01:25,320
13,000 あり、ネットワークが実際に何を行うかを正確に決定するのはこれらの値です。

20
00:01:25,320 --> 00:01:29,800
そして、このネットワークが特定の数字を分類すると言うときの意味は、最後の層にある 10

21
00:01:29,800 --> 00:01:34,080
個のニューロンの中で最も明るいものがその数字に対応するということです。

22
00:01:34,080 --> 00:01:39,240
レイヤー構造について私たちが念頭に置いていた動機は、おそらく

23
00:01:39,240 --> 00:01:43,920
2

24
00:01:43,920 --> 00:01:48,640
番目のレイヤーでエッジを認識し、3

25
00:01:48,640 --> 00:01:49,640
番目のレイヤーでループやラインなどのパターンを認識し、最後のレイヤーでそれらのパターンをつなぎ合わせて、数字を認識します。

26
00:01:49,640 --> 00:01:52,880
ここでは、ネットワークがどのように学習するかを学びます。

27
00:01:52,880 --> 00:01:56,880
私たちが望んでいるのは、このネットワークに大量のトレーニング データを表示できるアルゴリズムです。このデータは、手書きの数字のさまざまな画像と、それらが本来あるべきものを示すラベルの形で提供されます。トレーニング

28
00:01:56,880 --> 00:02:01,540
データのパフォーマンスを向上させるために、これらの

29
00:02:01,540 --> 00:02:06,360
13,000

30
00:02:06,360 --> 00:02:10,760
の重みとバイアスを調整します。

31
00:02:10,760 --> 00:02:15,540
この階層構造により、学習した内容がトレーニング

32
00:02:15,540 --> 00:02:17,840
データを超えた画像に一般化されることが期待されます。

33
00:02:17,840 --> 00:02:22,240
これをテストする方法は、ネットワークをトレーニングした後、さらにラベル付けされたデータを表示し、それらの新しい画像がどの程度正確に分類されているかを確認することです。

34
00:02:22,240 --> 00:02:31,160


35
00:02:31,160 --> 00:02:34,760
私たちにとって幸運なことに、そしてまずこれが一般的な例となっているのは、MNIST

36
00:02:34,760 --> 00:02:39,520
データベースの背後にいる善良な人々が、それぞれが本来あるべき数字でラベル付けされた何万もの手書きの数字画像のコレクションをまとめてくれたことです。

37
00:02:39,520 --> 00:02:45,080


38
00:02:45,080 --> 00:02:49,920
機械を学習すると表現するのは挑発的ですが、その仕組みを一度見てみると、それはおかしな SF

39
00:02:49,920 --> 00:02:55,560
の前提というよりも、むしろ微積分の練習のように感じられます。

40
00:02:55,560 --> 00:03:01,040
つまり、基本的には、特定の機能の最小値を見つけることになります。

41
00:03:01,040 --> 00:03:06,480
概念的には、各ニューロンは前の層のすべてのニューロンに接続されていると考えており、その活性化を定義する加重合計の重みはそれらの接続の強さのようなものであり、バイアスは何らかの指標であることを覚えておいてください。そのニューロンが活動的になる傾向があるか、不活動的である傾向があるか。

42
00:03:06,480 --> 00:03:11,440


43
00:03:11,440 --> 00:03:16,400


44
00:03:16,400 --> 00:03:19,780


45
00:03:19,780 --> 00:03:23,300
まず、これらの重みとバイアスをすべて完全にランダムに初期化します。

46
00:03:23,300 --> 00:03:25,020


47
00:03:25,020 --> 00:03:29,100
言うまでもなく、このネットワークはランダムに何かを実行しているだけなので、特定のトレーニング例ではひどいパフォーマンスになります。

48
00:03:29,100 --> 00:03:31,180


49
00:03:31,180 --> 00:03:36,820
たとえば、この 3 の画像を入力すると、出力レイヤーは混乱したように見えます。

50
00:03:36,820 --> 00:03:43,340
そこで、あなたがすることは、コスト関数を定義することです。これは、出力がほとんどのニューロンでは 0 ですが、このニューロンでは

51
00:03:43,340 --> 00:03:48,940
1 である活性化を持つべきであることをコンピューター、いや、悪いコンピューターに伝える方法です。

52
00:03:48,980 --> 00:03:51,740
あなたが私にくれたものは全くのゴミです。

53
00:03:51,740 --> 00:03:56,740
これをもう少し数学的に言うと、これらのゴミ出力アクティベーションのそれぞれと、それらに必要な値との差の二乗を合計します。これが、単一のトレーニング

54
00:03:56,740 --> 00:04:01,980
サンプルのコストと呼ばれるものです。

55
00:04:01,980 --> 00:04:06,020


56
00:04:06,020 --> 00:04:12,660
ネットワークが自信を持って画像を正しく分類している場合にはこの合計は小さくなりますが、ネットワークが何をしているかを認識していないように見える場合にはこの合計が大きくなることに注意してください。

57
00:04:12,660 --> 00:04:18,820


58
00:04:18,820 --> 00:04:23,860
したがって、自由に使える数万のトレーニング

59
00:04:23,860 --> 00:04:27,580
サンプルすべての平均コストを考慮することになります。

60
00:04:27,580 --> 00:04:32,300
この平均コストは、ネットワークがどの程度劣悪であるか、およびコンピュータの動作がどの程度悪くなるかを示す尺度です。

61
00:04:32,300 --> 00:04:33,300


62
00:04:33,300 --> 00:04:35,300
それは複雑なことです。

63
00:04:35,300 --> 00:04:40,380
ネットワーク自体が基本的に 784

64
00:04:40,380 --> 00:04:46,100
個の数値、ピクセル値を入力として取り込み、10 個の数値を出力として吐き出す関数であったことを覚えていますか?

65
00:04:46,100 --> 00:04:49,700
ある意味、ネットワークはこれらすべての重みとバイアスによってパラメーター化されています。

66
00:04:49,700 --> 00:04:53,340
コスト関数は、その上に複雑な層が重なっています。

67
00:04:53,340 --> 00:04:59,140
それらの 13,000

68
00:04:59,140 --> 00:05:04,620
ほどの重みとバイアスを入力として受け取り、それらの重みとバイアスがどれほど悪いかを説明する 1

69
00:05:04,620 --> 00:05:09,140
つの数値を吐き出します。その定義方法は、数万のトレーニング データすべてに対するネットワークの動作によって異なります。

70
00:05:09,140 --> 00:05:12,460
それは考えるべきことがたくさんあります。

71
00:05:12,460 --> 00:05:16,380
しかし、コンピューターがどのようなくだらない仕事をしているかを単にコンピューターに伝えるだけでは、あまり役に立ちません。

72
00:05:16,380 --> 00:05:21,300
これらの重みとバイアスを変更して改善する方法を教えたいと考えています。

73
00:05:21,300 --> 00:05:25,580
わかりやすくするために、13,000 個の入力を持つ関数を想像するのに苦労するのではなく、入力として 1

74
00:05:25,580 --> 00:05:31,440
つの数値、出力として 1 つの数値を持つ単純な関数を想像してください。

75
00:05:31,440 --> 00:05:36,420
この関数の値を最小にする入力をどのように見つけますか?

76
00:05:36,420 --> 00:05:41,300
微積分の学生は、その最小値を明示的に計算できる場合があることを知っているでしょうが、実際に複雑な関数ではそれが常に実現可能であるとは限りません。もちろん、この状況の 13,000

77
00:05:41,340 --> 00:05:46,620
入力バージョンでは、非常に複雑なニューラル

78
00:05:46,620 --> 00:05:51,640
ネットワークのコスト関数では不可能です。

79
00:05:51,640 --> 00:05:56,820
より柔軟な戦術は、任意の入力から開始して、その出力を下げるためにどの方向にステップを進めるべきかを判断することです。

80
00:05:56,820 --> 00:05:59,860


81
00:05:59,860 --> 00:06:05,020
具体的には、現在の関数の傾きがわかる場合は、その傾きが正の場合は左にシフトし、その傾きが負の場合は入力を右にシフトします。

82
00:06:05,020 --> 00:06:09,280


83
00:06:09,280 --> 00:06:12,720


84
00:06:12,720 --> 00:06:17,040
これを繰り返し実行し、各ポイントで新しい傾きをチェックし、適切な手順を実行すると、関数の極小値に近づくことになります。

85
00:06:17,040 --> 00:06:20,680


86
00:06:20,680 --> 00:06:24,600
ここで皆さんが思い浮かべるイメージは、丘を転がり落ちるボールです。

87
00:06:24,600 --> 00:06:29,380
そして、この非常に単純化された単一入力関数であっても、どのランダム入力から開始するかに応じて、到達する可能性のある谷が多数あり、到達する極小値が可能な限り最小の値になるという保証はないことに注意してください。コスト関数の。

88
00:06:29,380 --> 00:06:34,220


89
00:06:34,220 --> 00:06:38,460


90
00:06:38,460 --> 00:06:39,460


91
00:06:39,460 --> 00:06:43,180
これはニューラル ネットワークの場合にも当てはまります。

92
00:06:43,180 --> 00:06:48,140
また、ステップ

93
00:06:48,140 --> 00:06:52,920
サイズを勾配に比例させた場合、勾配が最小に向かって平坦になるとステップがどんどん小さくなり、オーバーシュートが防止されることにも注目してください。

94
00:06:52,920 --> 00:06:56,020


95
00:06:56,020 --> 00:07:01,640
もう少し複雑にして、代わりに 2 つの入力と 1 つの出力を持つ関数を想像してください。

96
00:07:01,640 --> 00:07:06,360
入力空間を xy

97
00:07:06,360 --> 00:07:09,020
平面、コスト関数をその上の面としてグラフ化すると考えることができます。

98
00:07:09,020 --> 00:07:13,600
関数の傾きについて尋ねる代わりに、関数の出力を最も早く減少させるためには、この入力空間をどの方向にステップすべきかを尋ねる必要があります。

99
00:07:13,600 --> 00:07:19,780


100
00:07:19,780 --> 00:07:22,340
言い換えれば、下り坂の方向は何ですか?

101
00:07:22,340 --> 00:07:26,740
繰り返しますが、その丘を転がり落ちるボールを想像すると分かります。

102
00:07:26,740 --> 00:07:31,920
多変数微積分に詳しい人は、関数の勾配によって最も急な上昇の方向がわかり、関数を最も早く増加させるにはどの方向に進むべきかがわかるでしょう。

103
00:07:31,920 --> 00:07:37,460


104
00:07:37,460 --> 00:07:39,420


105
00:07:39,420 --> 00:07:43,820
当然のことながら、その勾配をマイナスにすると、関数を最も早く減少させるステップの方向がわかります。

106
00:07:43,820 --> 00:07:47,460


107
00:07:47,460 --> 00:07:52,320
さらに、この勾配ベクトルの長さは、その最も急な勾配がどれほど急であるかを示します。

108
00:07:52,320 --> 00:07:54,580


109
00:07:54,580 --> 00:07:58,080
多変数微積分に詳しくなく、さらに詳しく知りたい場合は、このテーマに関して私がカーン

110
00:07:58,080 --> 00:08:01,100
アカデミーで行った作品の一部を確認してください。

111
00:08:01,100 --> 00:08:05,680
しかし正直に言って、あなたと私にとって現時点で重要なのは、原理的にはこのベクトル、つまり下り坂の方向とその急勾配を示すベクトルを計算する方法が存在するということだけです。

112
00:08:05,680 --> 00:08:10,440


113
00:08:10,440 --> 00:08:12,040


114
00:08:12,040 --> 00:08:17,280
知っていることがこれだけで、詳細がしっかりしていなくても大丈夫です。

115
00:08:17,280 --> 00:08:21,440
それができれば、関数を最小化するアルゴリズムは、この勾配の方向を計算し、下り坂に向かって小さな一歩を踏み出し、それを何度も繰り返すことになるからです。

116
00:08:21,440 --> 00:08:27,400


117
00:08:28,300 --> 00:08:33,700
これは、2 つの入力ではなく 13,000 の入力を持つ関数の基本的な考え方と同じです。

118
00:08:33,700 --> 00:08:38,980
ネットワークの 13,000

119
00:08:38,980 --> 00:08:40,180
の重みとバイアスをすべて巨大な列ベクトルに編成することを想像してください。

120
00:08:40,180 --> 00:08:46,140
コスト関数の負の勾配は単なるベクトルであり、この非常に巨大な入力空間内の何らかの方向であり、これらすべての数値に対するどの微調整がコスト関数の最も急速な減少を引き起こすかを示します。

121
00:08:46,140 --> 00:08:51,660


122
00:08:51,660 --> 00:08:55,900


123
00:08:55,900 --> 00:09:00,000
そしてもちろん、特別に設計されたコスト関数を使用して、重みとバイアスを変更して値を下げることは、トレーニング

124
00:09:00,000 --> 00:09:05,520
データの各部分に対するネットワークの出力を

125
00:09:05,520 --> 00:09:10,280
10

126
00:09:10,280 --> 00:09:11,280
個の値のランダムな配列のように見せることを意味し、より実際に望む決定に近づけることを意味します。それを作るのです。

127
00:09:11,280 --> 00:09:15,940
覚えておくことが重要です。このコスト関数にはすべてのトレーニング

128
00:09:15,940 --> 00:09:24,260
データの平均が含まれるため、これを最小化すると、それらのサンプルすべてでパフォーマンスが向上することになります。

129
00:09:24,260 --> 00:09:28,540
この勾配を効率的に計算するためのアルゴリズムは、事実上、ニューラル

130
00:09:28,540 --> 00:09:32,520
ネットワークの学習方法の中心であり、バックプロパゲーションと呼ばれます。これについては、次のビデオで説明します。

131
00:09:32,520 --> 00:09:34,040


132
00:09:34,040 --> 00:09:39,100
そこでは、時間をかけて、特定のトレーニング

133
00:09:39,100 --> 00:09:44,100
データの各重みとバイアスに正確に何が起こっているのかを詳しく見ていき、関連する計算や公式の山の向こうで何が起こっているのかを直感的に感じられるようにしたいと考えています。

134
00:09:44,100 --> 00:09:47,980


135
00:09:47,980 --> 00:09:51,780
今ここで、実装の詳細とは関係なく、皆さんに知っておいていただきたい主な点は、ネットワーク学習について話すときの意味は、コスト関数を最小化するだけだということです。

136
00:09:51,780 --> 00:09:56,820


137
00:09:56,820 --> 00:09:59,320


138
00:09:59,320 --> 00:10:02,760
そして、その結果の

139
00:10:02,760 --> 00:10:07,820
1

140
00:10:07,820 --> 00:10:09,340
つとして、このコスト関数が良好な滑らかな出力を持つことが重要であることに注意してください。これにより、下り坂を少しずつ進むことで極小値を見つけることができます。

141
00:10:09,340 --> 00:10:14,140
ちなみに、生物学的ニューロンのように単に二値的に活性または不活性になるのではなく、人工ニューロンが継続的に範囲の活性化を行うのはこのためです。

142
00:10:14,140 --> 00:10:18,580


143
00:10:18,580 --> 00:10:20,440


144
00:10:20,440 --> 00:10:24,600
負の勾配の倍数によって関数の入力を繰り返し微調整するこのプロセスは、勾配降下法と呼ばれます。

145
00:10:24,600 --> 00:10:26,960


146
00:10:26,960 --> 00:10:31,760
これは、コスト関数の局所最小値、つまりこのグラフの谷に向かって収束する方法です。

147
00:10:31,760 --> 00:10:33,000


148
00:10:33,000 --> 00:10:37,040
もちろん、13,000 次元の入力空間でのナッジは少し理解するのが難しいため、まだ

149
00:10:37,040 --> 00:10:41,480
2

150
00:10:41,480 --> 00:10:45,220
つの入力を持つ関数の図を示していますが、実際には、これについて考えるための優れた非空間的な方法があります。

151
00:10:45,220 --> 00:10:49,100
負の勾配の各成分から 2 つのことがわかります。

152
00:10:49,100 --> 00:10:53,600
もちろん、この符号は、入力ベクトルの対応するコンポーネントを上または下に微調整する必要があるかどうかを示します。

153
00:10:53,600 --> 00:10:55,860


154
00:10:55,860 --> 00:11:01,340
しかし重要なのは、これらすべての要素の相対的な大きさによって、どの変更がより重要であるかがわかるということです。

155
00:11:01,340 --> 00:11:05,620


156
00:11:05,620 --> 00:11:09,780
私たちのネットワークでは、重みの 1

157
00:11:09,780 --> 00:11:14,980
つを調整すると、他の重みを調整するよりもコスト関数にはるかに大きな影響を与える可能性があります。

158
00:11:14,980 --> 00:11:19,440
これらの接続の中には、トレーニング データにとってより重要なものもあります。

159
00:11:19,440 --> 00:11:23,520
したがって、気が遠くなるような膨大なコスト関数のこの勾配ベクトルについて考える方法は、各重みとバイアスの相対的な重要性、つまり、これらの変更のどれが最も費用対効果が高いかをエンコードしているということです。

160
00:11:23,520 --> 00:11:29,740


161
00:11:29,740 --> 00:11:34,100


162
00:11:34,100 --> 00:11:37,360
これは方向性についての単なる考え方です。

163
00:11:37,360 --> 00:11:41,740
より単純な例を挙げると、入力として

164
00:11:41,740 --> 00:11:48,720
2

165
00:11:48,720 --> 00:11:52,880
つの変数を持つ関数があり、ある特定の点での勾配が

166
00:11:52,880 --> 00:11:57,400
3,1

167
00:11:57,400 --> 00:12:02,200
になると計算した場合、一方では、次のように解釈できます。その入力に立って、この方向に沿って移動すると、関数が最も早く増加します。つまり、入力点の平面上で関数をグラフにすると、そのベクトルが真っ直ぐ上り坂の方向を与えることになります。

168
00:12:02,200 --> 00:12:03,200


169
00:12:03,200 --> 00:12:07,600
しかし、これを別の読み方で読むと、この最初の変数への変更は 2

170
00:12:07,600 --> 00:12:12,400
番目の変数への変更の 3

171
00:12:12,400 --> 00:12:17,740
倍の重要性があり、少なくとも関連する入力の付近では、X 値を微調整する方がはるかに大きな影響を与えるということになります。バック。

172
00:12:17,740 --> 00:12:22,880
さて、ズームアウトしてこれまでの状況をまとめてみましょう。

173
00:12:22,880 --> 00:12:28,660
ネットワーク自体は、784 個の入力と

174
00:12:28,660 --> 00:12:30,860
10 個の出力を備えた関数であり、これらすべての重み付けされた合計によって定義されます。

175
00:12:30,860 --> 00:12:34,160
コスト関数は、その上に複雑な層が重なっています。

176
00:12:34,160 --> 00:12:39,300
13,000

177
00:12:39,300 --> 00:12:42,640
の重みとバイアスを入力として受け取り、トレーニング例に基づいて粗さの単一の尺度を吐き出します。

178
00:12:42,640 --> 00:12:47,520
コスト関数の勾配はさらに複雑な層になります。

179
00:12:47,520 --> 00:12:52,860
これは、これらすべての重みとバイアスに対してどのような調整がコスト関数の値に最も速い変化を引き起こすかを示します。これは、どの重みに対するどの変更が最も重要かを示していると解釈できます。

180
00:12:52,860 --> 00:12:56,640


181
00:12:56,640 --> 00:13:03,040


182
00:13:03,040 --> 00:13:07,620
では、ランダムな重みとバイアスを使用してネットワークを初期化し、この勾配降下プロセスに基づいてそれらを何度も調整すると、これまでに見たことのない画像で実際にどの程度のパフォーマンスが得られるでしょうか?

183
00:13:07,620 --> 00:13:12,420


184
00:13:12,420 --> 00:13:14,240


185
00:13:14,240 --> 00:13:19,000
私がここで説明したものは、それぞれ 16 個のニューロンからなる 2

186
00:13:19,000 --> 00:13:26,920
つの隠れ層を備えており、主に美的理由から選択されていますが、悪くはなく、表示される新しい画像の約 96% を正しく分類しています。

187
00:13:26,920 --> 00:13:31,580
そして正直に言うと、それが台無しにしているいくつかの例を見ると、少し緩めなければならないと感じます。

188
00:13:31,580 --> 00:13:36,300


189
00:13:36,300 --> 00:13:40,220
隠れ層構造を試していくつかの調整を加えれば、これを最大 98%

190
00:13:40,220 --> 00:13:41,220
まで達成できます。

191
00:13:41,220 --> 00:13:42,900
それはとても良いことです！

192
00:13:42,900 --> 00:13:47,020
これは最高ではありません。この単純なバニラ

193
00:13:47,020 --> 00:13:52,460


194
00:13:52,460 --> 00:13:56,800
ネットワークよりも洗練されれば、確かにパフォーマンスを向上させることはできますが、最初のタスクがどれほど困難であるかを考えると、これまで見たことのない画像でこれほどうまく動作するネットワークには、信じられないほどの何かがあると思います。どのようなパターンを探すべきかを具体的に指示したことはありません。

195
00:13:56,800 --> 00:14:02,000


196
00:14:02,000 --> 00:14:07,840
もともと、私がこの構造を動機付けた方法は、2

197
00:14:07,840 --> 00:14:11,880
番目の層が小さなエッジを検出し、3

198
00:14:11,880 --> 00:14:16,080
番目の層がそれらのエッジをつなぎ合わせてループや長い線を認識し、それらがつなぎ合わされるかもしれないという希望を説明することでした。一緒に数字を認識します。

199
00:14:16,080 --> 00:14:18,220


200
00:14:18,220 --> 00:14:21,040
では、これは私たちのネットワークが実際に行っていることなのでしょうか?

201
00:14:21,040 --> 00:14:24,880
まあ、少なくともこれに関しては、まったくそうではありません。

202
00:14:24,960 --> 00:14:29,120
前回のビデオで、最初の層のすべてのニューロンから 2

203
00:14:29,120 --> 00:14:33,900
番目の層の特定のニューロンへの接続の重みが、2 番目の層のニューロンが認識している特定のピクセル

204
00:14:33,900 --> 00:14:37,440
パターンとしてどのように視覚化できるかを説明したことを覚えていますか?

205
00:14:37,440 --> 00:14:44,600
これらのトランジションに関連付けられたウェイトに対してこれを行うと、あちこちで孤立した小さなエッジが検出されるのではなく、ほぼランダムに見え、中央に非常に緩いパターンがいくつかあるだけです。

206
00:14:44,600 --> 00:14:51,000


207
00:14:51,000 --> 00:14:54,200


208
00:14:54,200 --> 00:14:59,020
考えられる重みとバイアスの計り知れないほど広い

209
00:14:59,020 --> 00:15:04,020
13,000

210
00:15:04,020 --> 00:15:08,440
次元空間において、私たちのネットワークは、ほとんどの画像を正常に分類したにもかかわらず、私たちが期待していたパターンを正確に検出できない、幸せな小さな局所最小値を見つけたようです。

211
00:15:08,440 --> 00:15:09,840


212
00:15:09,840 --> 00:15:14,600
この点をよく理解するには、ランダムな画像を入力したときに何が起こるかを見てください。

213
00:15:14,600 --> 00:15:19,240
システムが賢いものであれば、10 個の出力ニューロンのどれも実際には活性化していないのか、あるいはそれらすべてを均等に活性化しているのか、不確かに感じられるか、あるいは、このランダムなニューロンが確実であるかのように、自信を持ってナンセンスな答えを与えることを期待するかもしれません。

214
00:15:19,240 --> 00:15:24,120
5 の実際の画像が

215
00:15:24,520 --> 00:15:29,800
5 であるのと同様に、ノイズも

216
00:15:29,800 --> 00:15:34,560
5 です。

217
00:15:34,560 --> 00:15:39,300
別の言い方をすると、このネットワークは数字をかなりうまく認識できても、それを描画する方法がわかりません。

218
00:15:39,300 --> 00:15:41,800


219
00:15:41,800 --> 00:15:45,400
その多くは、トレーニングの設定が非常に厳しく制限されているためです。

220
00:15:45,400 --> 00:15:48,220
つまり、ここではネットワークの立場に立って考えてみましょう。

221
00:15:48,220 --> 00:15:53,280
その観点から見ると、宇宙全体は小さなグリッドの中心にある、明確に定義された不動の数字だけで構成されており、そのコスト関数は、その決定に完全な自信を持つこと以外には何の動機も与えませんでした。

222
00:15:53,280 --> 00:15:58,560


223
00:15:58,560 --> 00:16:02,160


224
00:16:02,160 --> 00:16:05,760
これが第

225
00:16:05,760 --> 00:16:09,320
2

226
00:16:09,320 --> 00:16:10,320
層のニューロンが実際に行っていることのイメージであるため、なぜエッジやパターンを検出するという動機でこのネットワークを紹介するのか不思議に思うかもしれません。

227
00:16:10,320 --> 00:16:13,040
つまり、それは最終的にはまったくそうではありません。

228
00:16:13,040 --> 00:16:17,480
これは最終目標ではなく、出発点です。

229
00:16:17,480 --> 00:16:22,280
率直に言って、これは 80

230
00:16:22,280 --> 00:16:26,920
年代から

231
00:16:26,920 --> 00:16:31,380
90

232
00:16:31,380 --> 00:16:38,720
年代に研究された種類の古いテクノロジーであり、より詳細な現代の亜種を理解する前に、それを理解する必要があります。また、明らかにいくつかの興味深い問題を解決できる可能性がありますが、何を深く掘り下げるほど、これらの隠れ層が実際に機能しているほど、その層は知性が低く見えます。

233
00:16:38,720 --> 00:16:43,540
ネットワークがどのように学習するかということから、あなたがどのように学習するかに少し焦点を移しますが、それは、何らかの方法でここで取り上げた資料に積極的に取り組んだ場合にのみ起こります。

234
00:16:43,540 --> 00:16:47,160


235
00:16:47,160 --> 00:16:51,920
皆さんにしていただきたいのは、非常に簡単なことの

236
00:16:51,920 --> 00:16:57,560
1

237
00:16:57,560 --> 00:17:01,880
つです。今ちょっと立ち止まって、このシステムにどのような変更を加えるか、エッジやパターンなどをよりよく認識できるようにしたい場合に画像をどのように認識するかについて、少しの間深く考えてみてください。

238
00:17:01,880 --> 00:17:06,360
しかしそれよりも、実際に内容に取り組むには、深層学習とニューラル ネットワークに関する Michael

239
00:17:06,360 --> 00:17:09,720
Nielsen の本を強くお勧めします。

240
00:17:09,720 --> 00:17:15,200
この中には、まさにこの例をダウンロードして試すためのコードとデータが含まれており、そのコードが何をしているのかを段階的に説明します。

241
00:17:15,200 --> 00:17:19,360


242
00:17:19,360 --> 00:17:23,920
素晴らしいのは、この本が無料で一般公開されているということです。この本から何かを得ることができましたら、私と一緒にニールセンの取り組みに寄付することを検討してください。

243
00:17:23,920 --> 00:17:28,040


244
00:17:28,040 --> 00:17:32,060
また、Chris Ola による驚異的で美しいブログ投稿や

245
00:17:32,060 --> 00:17:38,720
Distill の記事など、私がとても気に入っている他のリソースも説明にリンクしました。

246
00:17:38,720 --> 00:17:41,960
最後の数分間をここで締めくくるために、リーシャ・リーとのインタビューの抜粋に戻りたいと思います。

247
00:17:41,960 --> 00:17:44,440


248
00:17:44,440 --> 00:17:48,520
前回のビデオで彼女を覚えているかもしれません。彼女は深層学習で博士号の研究をしていました。

249
00:17:48,560 --> 00:17:52,240
この短い抜粋では、最新の画像認識ネットワークの一部が実際にどのように学習しているかを深く掘り下げた 2

250
00:17:52,240 --> 00:17:56,380
つの最近の論文について彼女が話しています。

251
00:17:56,380 --> 00:18:00,320
会話の状況を説明するために、最初の論文では、画像認識に優れた特にディープ ニューラル

252
00:18:00,320 --> 00:18:04,480
ネットワークの 1

253
00:18:04,480 --> 00:18:09,400
つを使用し、適切にラベル付けされたデータセットでトレーニングする代わりに、トレーニング前にすべてのラベルをシャッフルしました。

254
00:18:09,400 --> 00:18:13,840
すべてがランダムにラベル付けされているだけであるため、ここでのテストの精度は明らかにランダムと同等です。

255
00:18:13,840 --> 00:18:15,320


256
00:18:15,320 --> 00:18:20,080
ただし、適切にラベル付けされたデータセットを使用した場合と同じトレーニング精度を達成することはできました。

257
00:18:20,080 --> 00:18:21,440


258
00:18:21,440 --> 00:18:26,120
基本的に、この特定のネットワークの何百万もの重みは、ランダムなデータを記憶するだけで十分でした。このため、このコスト関数の最小化が実際に画像内の何らかの構造に対応するのか、それとも単なる記憶なのかという疑問が生じます。

259
00:18:26,120 --> 00:18:31,040


260
00:18:31,040 --> 00:18:36,720


261
00:18:36,720 --> 00:18:40,120
。 。 。 正しい分類が何であるかをデータセット全体を記憶するためです。

262
00:18:40,120 --> 00:18:45,720
それで、半年後の今年の

263
00:18:45,720 --> 00:18:50,440
ICML

264
00:18:50,440 --> 00:18:52,220
では、正確には反論の論文はありませんでしたが、実際には、これらのネットワークはそれよりもう少し賢いことをしている、といったいくつかの側面を取り上げた論文がありました。

265
00:18:52,220 --> 00:18:59,600
その精度曲線を見ると、ランダムなデータセットでトレーニングしているだけだとすると、その曲線は、ほぼ直線的に、非常にゆっくりと下降していきます。

266
00:18:59,600 --> 00:19:05,240


267
00:19:05,280 --> 00:19:10,840
したがって、その精度を実現する適切な重みの可能な極小値を見つけるのに非常に苦労しています。

268
00:19:10,840 --> 00:19:12,320


269
00:19:12,320 --> 00:19:16,720
一方、適切なラベルを持つ構造化データセットで実際にトレーニングしている場合、最初は少しいじってみましたが、その精度レベルに達するまでに非常に早く落ちてしまいました。

270
00:19:16,720 --> 00:19:20,240


271
00:19:20,240 --> 00:19:23,360


272
00:19:23,360 --> 00:19:28,580
したがって、ある意味、極大値を見つけるのが簡単でした。

273
00:19:28,580 --> 00:19:32,900
そして、これに関して興味深いのは、実際に数年前に発行された別の論文が明らかになったことであり、この論文ではネットワーク層についてさらに単純化が行われています。

274
00:19:32,900 --> 00:19:39,140


275
00:19:39,140 --> 00:19:40,140


276
00:19:40,140 --> 00:19:43,880
しかし、その結果の 1

277
00:19:43,880 --> 00:19:49,400
つは、最適化の状況を見ると、これらのネットワークが学習する傾向にある極小値が実際には同じ品質であることを示しています。

278
00:19:49,400 --> 00:19:54,300
したがって、ある意味、データセットが構造化されていれば、それをより簡単に見つけることができるはずです。

279
00:19:58,580 --> 00:20:01,140
Patreon でサポートしてくださっている皆様にいつも感謝しています。

280
00:20:01,480 --> 00:20:05,440
Patreon

281
00:20:05,440 --> 00:20:07,160
におけるゲームチェンジャーが何であるかについては以前に述べましたが、これらのビデオは本当に皆さんなしでは不可能です。

282
00:20:07,160 --> 00:20:11,540
また、VC 会社 Amplify

283
00:20:11,540 --> 00:20:13,240
Partners と、シリーズの最初のビデオに対する彼らのサポートにも特別な感謝を表したいと思います。

284
00:20:31,140 --> 00:20:33,140
ありがとう。

