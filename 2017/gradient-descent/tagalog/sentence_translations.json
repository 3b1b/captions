[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "Huling video inilatag ko ang istraktura ng isang neural network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "Magbibigay ako ng isang mabilis na recap dito para sariwa ito sa ating isipan, at pagkatapos ay mayroon akong dalawang pangunahing layunin para sa video na ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "Ang una ay upang ipakilala ang ideya ng gradient descent, na sumasailalim hindi lamang kung paano natututo ang mga neural network, ngunit kung paano gumagana rin ang maraming iba pang machine learning.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "Pagkatapos nito, maghuhukay tayo ng kaunti pa sa kung paano gumaganap ang partikular na network na ito, at kung ano ang hinahanap ng mga nakatagong layer ng neuron na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "Bilang paalala, ang layunin namin dito ay ang klasikong halimbawa ng pagkilala sa sulat-kamay na digit, ang hello world ng mga neural network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "Ang mga digit na ito ay na-render sa isang 28x28 pixel grid, ang bawat pixel ay may ilang grayscale na value sa pagitan ng 0 at 1.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "Iyon ang tumutukoy sa mga pag-activate ng 784 neuron sa input layer ng network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "At pagkatapos ay ang activation para sa bawat neuron sa mga sumusunod na layer ay batay sa isang timbang na kabuuan ng lahat ng mga activation sa nakaraang layer, kasama ang ilang espesyal na numero na tinatawag na bias.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "Pagkatapos ay bubuo ka ng kabuuan na iyon gamit ang ilang iba pang function, tulad ng sigmoid squishification, o isang relu, sa paraan ng paglakad ko sa huling video.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "Sa kabuuan, dahil sa medyo arbitraryong pagpili ng dalawang nakatagong layer na may 16 na neuron bawat isa, ang network ay may humigit-kumulang 13,000 timbang at bias na maaari nating ayusin, at ang mga halagang ito ang tumutukoy kung ano ang eksaktong ginagawa ng network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "Kung gayon ang ibig nating sabihin kapag sinabi nating ang network na ito ay nag-uuri ng isang naibigay na digit ay ang pinakamaliwanag sa 10 neuron na iyon sa huling layer ay tumutugma sa digit na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "At tandaan, ang pagganyak na nasa isip namin dito para sa layered na istraktura ay maaaring ang pangalawang layer ay maaaring kunin sa mga gilid, at ang ikatlong layer ay maaaring kunin ang mga pattern tulad ng mga loop at linya, at ang huling isa ay maaari lamang pagsama-samahin ang mga iyon. mga pattern upang makilala ang mga digit.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "Kaya dito, natutunan natin kung paano natututo ang network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "Ang gusto namin ay isang algorithm kung saan maipapakita mo sa network na ito ang isang buong bungkos ng data ng pagsasanay, na nasa anyo ng isang grupo ng iba't ibang mga larawan ng mga sulat-kamay na digit, kasama ang mga label para sa kung ano ang dapat na mga ito, at ito ay ayusin ang 13,000 timbang at bias na iyon upang mapabuti ang pagganap nito sa data ng pagsasanay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "Sana, ang layered na istrakturang ito ay nangangahulugan na ang natututuhan nito ay nagsa-generalize sa mga larawang lampas sa data ng pagsasanay na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "Ang paraan namin ng pagsubok ay na pagkatapos mong sanayin ang network, ipinapakita mo ito ng mas may label na data na hindi pa nito nakikita dati, at makikita mo kung gaano katumpak ang pag-uuri nito sa mga bagong larawang iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "Sa kabutihang palad para sa amin, at kung bakit ito ay isang karaniwang halimbawa sa pagsisimula, ay ang mabubuting tao sa likod ng database ng MNIST ay nagsama-sama ng isang koleksyon ng sampu-sampung libong mga sulat-kamay na digit na mga imahe, bawat isa ay may label na may mga numero na dapat nilang itala. maging.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "At kahit gaano kagalit-galit na ilarawan ang isang makina bilang pag-aaral, kapag nakita mo na kung paano ito gumagana, mas mababa ang pakiramdam nito tulad ng ilang nakatutuwang sci-fi premise, at higit na katulad ng isang ehersisyo sa calculus.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "Ibig kong sabihin, karaniwang bumababa ito sa paghahanap ng minimum ng isang tiyak na function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "Tandaan, sa konsepto, iniisip natin na ang bawat neuron ay konektado sa lahat ng mga neuron sa nakaraang layer, at ang mga timbang sa weighted sum na tumutukoy sa pag-activate nito ay katulad ng mga lakas ng mga koneksyong iyon, at ang bias ay ilang indikasyon ng kung ang neuron na iyon ay may posibilidad na maging aktibo o hindi aktibo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "At para simulan ang mga bagay-bagay, sisimulan na lang natin ang lahat ng mga timbang at bias na iyon nang random.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "Hindi na kailangang sabihin, ang network na ito ay gaganap ng medyo kakila-kilabot sa isang ibinigay na halimbawa ng pagsasanay, dahil ito ay gumagawa lamang ng isang bagay na random.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "Halimbawa, nagpapakain ka sa larawang ito ng isang 3, at ang output layer ay mukhang isang gulo.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "Kaya ang gagawin mo ay tukuyin ang isang function ng gastos, isang paraan ng pagsasabi sa computer, hindi, masamang computer, ang output na iyon ay dapat magkaroon ng mga pag-activate na 0 para sa karamihan ng mga neuron, ngunit 1 para sa neuron na ito, ang ibinigay mo sa akin ay lubos na basura.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "Upang sabihin na mas mathematically, idinagdag mo ang mga parisukat ng mga pagkakaiba sa pagitan ng bawat isa sa mga pag-activate ng output ng basura at ang halaga na gusto mong magkaroon ng mga ito, at ito ang tatawagin naming halaga ng isang halimbawa ng pagsasanay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "Pansinin na maliit ang kabuuan na ito kapag kumpiyansa na inuri ng network ang larawan, ngunit malaki ito kapag tila hindi alam ng network kung ano ang ginagawa nito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "Kaya ang gagawin mo ay isaalang-alang ang average na gastos sa lahat ng sampu-sampung libong mga halimbawa ng pagsasanay na iyong magagamit.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "Ang average na gastos na ito ay ang aming sukatan para sa kung gaano kasira ang network, at kung gaano masama ang pakiramdam ng computer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "At iyon ay isang kumplikadong bagay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "Tandaan kung paano ang network mismo ay karaniwang isang function, isa na kumukuha ng 784 na numero bilang mga input, ang mga halaga ng pixel, at naglalabas ng 10 numero bilang output nito, at sa isang kahulugan ito ay na-parameter ng lahat ng mga timbang at bias na ito?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "Well ang cost function ay isang layer ng pagiging kumplikado sa itaas ng na.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "Kinukuha nito bilang input ang 13,000 o higit pang mga timbang at bias, at naglalabas ng isang solong numero na naglalarawan kung gaano kalala ang mga timbang at bias na iyon, at ang paraan ng pagtukoy nito ay depende sa gawi ng network sa lahat ng sampu-sampung libong piraso ng data ng pagsasanay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "Napakaraming dapat isipin.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "Ngunit ang pagsasabi lamang sa computer kung ano ang isang masamang trabaho na ginagawa nito ay hindi nakakatulong.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "Gusto mong sabihin dito kung paano baguhin ang mga timbang at pagkiling na iyon upang ito ay maging mas mahusay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "Upang gawing mas madali, sa halip na hirap na isipin ang isang function na may 13,000 input, isipin lamang ang isang simpleng function na may isang numero bilang isang input at isang numero bilang isang output.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "Paano ka makakahanap ng input na nagpapaliit sa halaga ng function na ito?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "Malalaman ng mga mag-aaral ng Calculus na kung minsan ay malinaw mong malalaman ang pinakamaliit na iyon, ngunit hindi ito palaging magagawa para sa talagang kumplikadong mga function, tiyak na wala sa 13,000 input na bersyon ng sitwasyong ito para sa aming nakakabaliw na kumplikadong paggana ng gastos sa neural network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "Ang isang mas nababaluktot na taktika ay ang magsimula sa anumang input, at alamin kung aling direksyon ang dapat mong hakbang upang gawing mas mababa ang output na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "Sa partikular, kung maaari mong malaman ang slope ng function kung nasaan ka, pagkatapos ay lumipat sa kaliwa kung positibo ang slope na iyon, at ilipat ang input sa kanan kung negatibo ang slope na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "Kung gagawin mo ito nang paulit-ulit, sa bawat punto na sinusuri ang bagong slope at ginagawa ang naaangkop na hakbang, lalapit ka sa ilang lokal na minimum ng function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "Ang imahe na maaaring nasa isip mo dito ay isang bola na gumugulong pababa sa isang burol.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "Pansinin, kahit na para sa talagang pinasimpleng single input function na ito, maraming posibleng lambak na maaari mong mapunta, depende sa kung saang random na input ka magsisimula, at walang garantiya na ang lokal na minimum na iyong napuntahan ay magiging pinakamaliit na posibleng halaga. ng function ng gastos.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "Dadalhin din iyon sa aming kaso ng neural network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "Gusto ko ring mapansin mo kung paano kung gagawin mong proporsyonal ang mga laki ng iyong hakbang sa slope, at kapag ang slope ay unti-unting lumalapad, unti-unting lumiliit ang iyong mga hakbang, at nakakatulong iyon sa iyong mag-overshoot.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "Papataasin ang pagiging kumplikado ng kaunti, isipin sa halip ang isang function na may dalawang input at isang output.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "Maaari mong isipin ang puwang ng pag-input bilang xy-plane, at ang function ng gastos ay naka-graph bilang isang ibabaw sa itaas nito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "Sa halip na magtanong tungkol sa slope ng function, kailangan mong itanong kung aling direksyon ang dapat mong hakbang sa input space na ito upang mabawasan ang output ng function nang pinakamabilis.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "Sa madaling salita, ano ang direksyon ng pababa?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "Muli, kapaki-pakinabang na mag-isip ng isang bola na lumiligid sa burol na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "Malalaman ng mga pamilyar sa multivariable calculus na ang gradient ng isang function ay nagbibigay sa iyo ng direksyon ng pinakamatarik na pag-akyat, kung aling direksyon ang dapat mong hakbang upang mapataas ang function nang pinakamabilis.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "Natural lang, ang pagkuha ng negatibo sa gradient na iyon ay nagbibigay sa iyo ng direksyon sa hakbang na pinakamabilis na nagpapababa sa function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "Higit pa riyan, ang haba ng gradient vector na ito ay isang indikasyon kung gaano katarik ang pinakamatarik na dalisdis na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "Kung hindi ka pamilyar sa multivariable calculus at gusto mong matuto nang higit pa, tingnan ang ilan sa mga ginawa ko para sa Khan Academy sa paksa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "Sa totoo lang, ang mahalaga lang para sa iyo at sa akin ngayon ay sa prinsipyo mayroong isang paraan upang makalkula ang vector na ito, ang vector na ito na nagsasabi sa iyo kung ano ang pababang direksyon at kung gaano ito katarik.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "Magiging okay ka kung iyon lang ang alam mo at hindi ka rock solid sa mga detalye.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "Kung makukuha mo iyon, ang algorithm para sa pagliit ng function ay upang kalkulahin ang gradient na direksyon na ito, pagkatapos ay gumawa ng isang maliit na hakbang pababa, at ulitin iyon nang paulit-ulit.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "Ito ay ang parehong pangunahing ideya para sa isang function na may 13,000 input sa halip na 2 input.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "Isipin na ayusin ang lahat ng 13,000 timbang at bias ng aming network sa isang higanteng column vector.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "Ang negatibong gradient ng cost function ay isang vector lamang, ito ay ilang direksyon sa loob ng napakalaking input space na ito na nagsasabi sa iyo kung aling mga nudge sa lahat ng mga numerong iyon ang magdudulot ng pinakamabilis na pagbaba sa cost function.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "At siyempre, sa aming espesyal na idinisenyong pag-andar sa gastos, ang pagpapalit ng mga timbang at pagkiling upang mabawasan ang ibig sabihin nito ay ang paggawa ng output ng network sa bawat piraso ng data ng pagsasanay na hindi mukhang isang random na hanay ng 10 mga halaga, at higit na katulad ng isang aktwal na desisyon na gusto namin. gawin ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "Mahalagang tandaan, ang function ng gastos na ito ay nagsasangkot ng average sa lahat ng data ng pagsasanay, kaya kung bawasan mo ito, nangangahulugan ito na ito ay isang mas mahusay na pagganap sa lahat ng mga sample na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "Ang algorithm para sa mahusay na pag-compute ng gradient na ito, na kung saan ay epektibong sentro ng kung paano natututo ang isang neural network, ay tinatawag na backpropagation, at ito ang pag-uusapan ko tungkol sa susunod na video.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "Doon, gusto ko talagang maglaan ng oras upang suriin kung ano ang eksaktong nangyayari sa bawat timbang at bias para sa isang partikular na piraso ng data ng pagsasanay, sinusubukang magbigay ng intuitive na pakiramdam para sa kung ano ang nangyayari sa kabila ng tumpok ng mga nauugnay na calculus at formula.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "Dito, sa ngayon, ang pangunahing bagay na gusto kong malaman mo, independiyente sa mga detalye ng pagpapatupad, ay ang ibig nating sabihin kapag pinag-uusapan natin ang tungkol sa isang pag-aaral sa network ay ang pagliit lamang ng isang function ng gastos.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "At pansinin, ang isang kahihinatnan nito ay mahalaga para sa function ng gastos na ito na magkaroon ng magandang maayos na output, upang makahanap tayo ng lokal na minimum sa pamamagitan ng pagkuha ng maliliit na hakbang pababa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "Ito ang dahilan kung bakit, sa pamamagitan ng paraan, ang mga artipisyal na neuron ay may patuloy na mga pag-activate, sa halip na maging aktibo o hindi aktibo sa isang binary na paraan, ang paraan ng mga biological neuron.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "Ang prosesong ito ng paulit-ulit na pag-nudging ng input ng isang function ng ilang multiple ng negatibong gradient ay tinatawag na gradient descent.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "Ito ay isang paraan upang magsama-sama patungo sa ilang lokal na minimum ng isang function ng gastos, karaniwang isang lambak sa graph na ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "Nagpapakita pa rin ako ng larawan ng isang function na may dalawang input, siyempre, dahil ang mga nudge sa isang 13,000 dimensional na espasyo sa pag-input ay medyo mahirap i-wrap ang iyong isip, ngunit mayroong isang magandang non-spatial na paraan upang isipin ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "Ang bawat bahagi ng negatibong gradient ay nagsasabi sa amin ng dalawang bagay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "Ang tanda, siyempre, ay nagsasabi sa amin kung ang kaukulang bahagi ng input vector ay dapat na itulak pataas o pababa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "Ngunit ang mahalaga, ang mga relatibong magnitude ng lahat ng mga bahaging ito ay uri ng nagsasabi sa iyo kung aling mga pagbabago ang mas mahalaga.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "Nakikita mo, sa aming network, ang isang pagsasaayos sa isa sa mga timbang ay maaaring magkaroon ng mas malaking epekto sa paggana ng gastos kaysa sa pagsasaayos sa ibang timbang.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "Ang ilan sa mga koneksyong ito ay mas mahalaga lamang para sa aming data ng pagsasanay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "Kaya isang paraan na maiisip mo ang tungkol sa gradient vector na ito ng ating mind-warpingly massive cost function ay ang pag-encode nito sa relatibong kahalagahan ng bawat timbang at bias, ibig sabihin, alin sa mga pagbabagong ito ang magdadala ng pinakamaraming bang para sa iyong pera.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "Ito ay talagang isa pang paraan ng pag-iisip tungkol sa direksyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "Upang kumuha ng isang mas simpleng halimbawa, kung mayroon kang ilang function na may dalawang variable bilang isang input, at kino-compute mo na ang gradient nito sa ilang partikular na punto ay lalabas bilang 3,1, pagkatapos ay sa isang banda maaari mong bigyang-kahulugan iyon bilang pagsasabi na kapag ikaw' muling nakatayo sa input na iyon, ang paglipat sa direksyong ito ay pinapataas ang function nang pinakamabilis, na kapag na-graph mo ang function sa itaas ng plane ng mga input point, ang vector na iyon ang nagbibigay sa iyo ng tuwid na direksyong paakyat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "Ngunit ang isa pang paraan para basahin iyon ay ang pagsasabi na ang mga pagbabago sa unang variable na ito ay may 3 beses ang kahalagahan ng mga pagbabago sa pangalawang variable, na kahit man lang sa kapitbahayan ng nauugnay na input, ang pag-nudging ng x-value ay nagdadala ng mas maraming bang para sa iyong buck.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "Mag-zoom out tayo at buod kung nasaan tayo sa ngayon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "Ang network mismo ay ang function na ito na may 784 input at 10 output, na tinukoy sa mga tuntunin ng lahat ng mga weighted sum na ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "Ang function ng gastos ay isang layer ng pagiging kumplikado sa itaas nito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "Ito ay tumatagal ng 13,000 mga timbang at bias bilang mga input at naglalabas ng isang solong sukatan ng kahinaan batay sa mga halimbawa ng pagsasanay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "At ang gradient ng cost function ay isa pang layer ng pagiging kumplikado.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "Sinasabi nito sa amin kung ano ang nagdudulot sa lahat ng mga timbang at bias na ito na nagiging sanhi ng pinakamabilis na pagbabago sa halaga ng function ng gastos, na maaari mong bigyang-kahulugan bilang pagsasabi kung aling mga pagbabago kung aling mga timbang ang pinakamahalaga.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "Kaya, kapag sinimulan mo ang network na may mga random na timbang at bias, at inayos ang mga ito nang maraming beses batay sa prosesong ito ng gradient descent, gaano ba ito kahusay na gumaganap sa mga larawang hindi pa nakikita noon?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "Ang isa na inilarawan ko dito, na may dalawang nakatagong layer ng 16 na neuron bawat isa, pinili karamihan para sa aesthetic na mga kadahilanan, ay hindi masama, pag-uuri tungkol sa 96% ng mga bagong larawan na nakikita nito nang tama.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "At sa totoo lang, kung titingnan mo ang ilan sa mga halimbawang pinagkakaguluhan nito, napipilitan kang bawasan ito nang kaunti.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "Ngayon kung maglalaro ka sa nakatagong istraktura ng layer at gumawa ng ilang mga pag-aayos, maaari mong makuha ito ng hanggang 98%.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "At iyon ay medyo maganda!",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "Hindi ito ang pinakamahusay, tiyak na makakakuha ka ng mas mahusay na pagganap sa pamamagitan ng pagiging mas sopistikado kaysa sa plain vanilla network na ito, ngunit dahil sa kung gaano nakakatakot ang paunang gawain, sa palagay ko mayroong isang bagay na hindi kapani-paniwala tungkol sa anumang network na ginagawa ito nang maayos sa mga larawang hindi pa nakikita noon, dahil doon hindi namin partikular na sinabi dito kung anong mga pattern ang hahanapin.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "Sa orihinal, ang paraan ng pag-udyok ko sa istrukturang ito ay sa pamamagitan ng paglalarawan ng isang pag-asa na maaaring mayroon tayo, na ang pangalawang layer ay maaaring kunin sa maliliit na gilid, na ang ikatlong layer ay pagsasama-samahin ang mga gilid upang makilala ang mga loop at mas mahabang linya, at ang mga iyon ay maaaring magkapira-piraso. magkasama upang makilala ang mga digit.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "Kaya ito ba talaga ang ginagawa ng ating network?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "Well, para sa isang ito hindi bababa sa, hindi sa lahat.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "Tandaan kung paano natin tiningnan ang huling video kung paano makikita ang mga bigat ng mga koneksyon mula sa lahat ng mga neuron sa unang layer patungo sa isang partikular na neuron sa pangalawang layer bilang isang partikular na pattern ng pixel na kinukuha ng pangalawang layer na neuron?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "Well, kapag ginawa talaga namin iyon para sa mga timbang na nauugnay sa mga transition na ito, mula sa unang layer hanggang sa susunod, sa halip na kunin sa mga nakahiwalay na maliliit na gilid dito at doon, ang hitsura nila, well, halos random, na may ilang napakaluwag na pattern sa dun sa gitna.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "Mukhang sa hindi maarok na malaking 13,000 dimensional na espasyo ng mga posibleng timbang at bias, nakita ng aming network ang sarili nitong isang masayang maliit na lokal na minimum na, sa kabila ng matagumpay na pag-uuri sa karamihan ng mga larawan, ay hindi eksaktong nakakakuha sa mga pattern na maaaring inaasahan namin.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "At para talagang maihatid ang puntong ito pauwi, panoorin kung ano ang mangyayari kapag nag-input ka ng random na larawan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "Kung matalino ang system, maaari mong asahan na hindi ito sigurado, marahil ay hindi talaga ina-activate ang alinman sa 10 output neuron na iyon o i-activate ang lahat ng ito nang pantay-pantay, ngunit sa halip ay may kumpiyansa itong nagbibigay sa iyo ng ilang walang kapararakan na sagot, na parang sigurado na ang random na ingay na ito. ay isang 5 tulad ng ginagawa nito na ang isang aktwal na imahe ng isang 5 ay isang 5.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "Naiiba ang phrase, kahit na makilala ng network na ito ang mga digit, wala itong ideya kung paano iguhit ang mga ito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "Marami sa mga ito ay dahil ito ay isang mahigpit na napilitang setup ng pagsasanay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "I mean, ilagay mo ang sarili mo sa network's shoes dito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "Mula sa pananaw nito, ang buong sansinukob ay binubuo ng walang anuman kundi malinaw na tinukoy na hindi gumagalaw na mga digit na nakasentro sa isang maliit na grid, at ang gastos nito ay hindi nagbigay ng anumang insentibo upang maging anumang bagay ngunit lubos na tiwala sa mga desisyon nito.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "Kaya sa ito bilang larawan ng kung ano talaga ang ginagawa ng mga pangalawang layer na neuron na iyon, maaari kang magtaka kung bakit ko ipapakilala ang network na ito na may pagganyak na kunin ang mga gilid at pattern.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "Ibig kong sabihin, hindi lang iyon ang natatapos sa paggawa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "Well, ito ay hindi sinadya upang maging ang aming pangwakas na layunin, ngunit sa halip ay isang panimulang punto.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "Sa totoo lang, ito ay lumang teknolohiya, ang uri na sinaliksik noong 80s at 90s, at kailangan mo itong maunawaan bago mo maunawaan ang mas detalyadong modernong mga variant, at malinaw na kaya nitong lutasin ang ilang kawili-wiling mga problema, ngunit lalo kang naghuhukay sa kung ano ang mga nakatagong layer ay talagang ginagawa, ang hindi gaanong katalinuhan ay tila.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "Saglit na inilipat ang focus mula sa kung paano natututo ang mga network sa kung paano ka natututo, mangyayari lang iyon kung aktibo kang nakikipag-ugnayan sa materyal dito kahit papaano.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "Ang isang medyo simpleng bagay na gusto kong gawin mo ay mag-pause lang ngayon at mag-isip nang malalim tungkol sa kung anong mga pagbabago ang maaari mong gawin sa system na ito at kung paano nito nakikita ang mga larawan kung gusto mo itong mas mahusay na makuha sa mga bagay tulad ng mga gilid at pattern.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "Ngunit mas mabuti kaysa doon, upang aktwal na makisali sa materyal, lubos kong inirerekomenda ang aklat ni Michael Nielsen sa malalim na pag-aaral at mga neural network.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "Sa loob nito, mahahanap mo ang code at ang data na ida-download at laruin para sa eksaktong halimbawang ito, at ituturo sa iyo ng aklat ang hakbang-hakbang kung ano ang ginagawa ng code na iyon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "Ang kahanga-hanga ay ang aklat na ito ay libre at magagamit ng publiko, kaya kung mayroon kang makukuha mula rito, isaalang-alang ang pagsama sa akin sa pagbibigay ng donasyon para sa mga pagsisikap ni Nielsen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "Nag-link din ako ng ilang iba pang mapagkukunan na gusto ko sa paglalarawan, kasama ang kahanga-hanga at magandang post sa blog ni Chris Ola at ang mga artikulo sa Distill.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "Upang isara ang mga bagay dito sa mga huling minuto, gusto kong bumalik sa isang snippet ng panayam ko kay Leisha Lee.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "Maaari mong matandaan siya mula sa huling video, ginawa niya ang kanyang PhD na trabaho sa malalim na pag-aaral.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "Sa maliit na snippet na ito, pinag-uusapan niya ang tungkol sa dalawang kamakailang papel na talagang naghuhukay sa kung paano aktwal na natututo ang ilan sa mga mas modernong network ng pagkilala sa imahe.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "Para lang i-set up kung nasaan kami sa pag-uusap, kinuha ng unang papel ang isa sa mga partikular na malalalim na neural network na ito na talagang mahusay sa pagkilala ng imahe, at sa halip na sanayin ito sa isang dataset na may wastong label, binasa ang lahat ng mga label sa paligid bago ang pagsasanay.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "Malinaw na ang katumpakan ng pagsubok dito ay hindi mas mahusay kaysa sa random, dahil ang lahat ay random na may label, ngunit nagawa pa rin nitong makamit ang parehong katumpakan ng pagsasanay tulad ng gagawin mo sa isang maayos na naka-label na dataset.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "Sa pangkalahatan, ang milyun-milyong timbang para sa partikular na network na ito ay sapat na para maisaulo lamang nito ang random na data, na nagpapataas ng tanong kung ang pag-minimize sa function ng gastos na ito ay talagang tumutugma sa anumang uri ng istraktura sa imahe, o ito ba ay pagsasaulo lamang?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "Kung titingnan mo ang curve ng katumpakan na iyon, kung nagsasanay ka lang sa isang random na dataset, ang uri ng curve na iyon ay bumaba nang napakabagal sa halos uri ng isang linear na paraan, kaya talagang nahihirapan kang hanapin ang lokal na minimum na posible, alam mo , ang mga tamang timbang na magbibigay sa iyo ng ganoong katumpakan.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "Samantalang kung talagang nagsasanay ka sa isang structured na dataset, isa na may tamang mga label, nagbiliko ka nang kaunti sa simula, ngunit pagkatapos ay medyo mabilis kang bumaba upang makarating sa antas ng katumpakan na iyon, at sa ilang mga kahulugan ito ay mas madaling mahanap ang lokal na maxima.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "At kaya kung ano ang kawili-wili din tungkol doon ay nagdudulot ito ng maliwanag na isa pang papel mula sa aktwal na ilang taon na ang nakalilipas, na may higit pang mga pagpapasimple tungkol sa mga layer ng network, ngunit ang isa sa mga resulta ay nagsasabi kung paano kung titingnan mo ang landscape ng pag-optimize, ang lokal na minima na malamang na matutunan ng mga network na ito ay talagang may pantay na kalidad, kaya sa ilang kahulugan kung ang iyong dataset ay nakaayos, dapat mong mahanap iyon nang mas madali.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "Ang aking pasasalamat, gaya ng dati, sa inyong sumusuporta sa Patreon.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "Nasabi ko na noon kung ano ang game changer ng Patreon, ngunit ang mga video na ito ay talagang hindi magiging posible kung wala ka.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "Gusto ko ring magbigay ng espesyal na pasasalamat sa VC firm na Amplify Partners, sa kanilang suporta sa mga unang video na ito sa serye.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]