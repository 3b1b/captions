1
00:00:04,180 --> 00:00:07,280
Huling video inilatag ko ang istraktura ng isang neural network.

2
00:00:07,680 --> 00:00:10,109
Magbibigay ako ng isang mabilis na recap dito para sariwa ito sa ating isipan, 

3
00:00:10,109 --> 00:00:12,600
at pagkatapos ay mayroon akong dalawang pangunahing layunin para sa video na ito.

4
00:00:13,100 --> 00:00:15,264
Ang una ay upang ipakilala ang ideya ng gradient descent, 

5
00:00:15,264 --> 00:00:17,988
na sumasailalim hindi lamang kung paano natututo ang mga neural network, 

6
00:00:17,988 --> 00:00:20,600
ngunit kung paano gumagana rin ang maraming iba pang machine learning.

7
00:00:21,120 --> 00:00:24,490
Pagkatapos nito, maghuhukay tayo ng kaunti pa sa kung paano gumaganap ang partikular 

8
00:00:24,490 --> 00:00:27,940
na network na ito, at kung ano ang hinahanap ng mga nakatagong layer ng neuron na iyon.

9
00:00:28,980 --> 00:00:32,498
Bilang paalala, ang layunin namin dito ay ang klasikong halimbawa ng 

10
00:00:32,498 --> 00:00:36,220
pagkilala sa sulat-kamay na digit, ang hello world ng mga neural network.

11
00:00:37,020 --> 00:00:40,023
Ang mga digit na ito ay na-render sa isang 28x28 pixel grid, 

12
00:00:40,023 --> 00:00:43,420
ang bawat pixel ay may ilang grayscale na value sa pagitan ng 0 at 1.

13
00:00:43,820 --> 00:00:50,040
Iyon ang tumutukoy sa mga pag-activate ng 784 neuron sa input layer ng network.

14
00:00:51,180 --> 00:00:54,378
At pagkatapos ay ang activation para sa bawat neuron sa mga sumusunod na 

15
00:00:54,378 --> 00:00:58,322
layer ay batay sa isang timbang na kabuuan ng lahat ng mga activation sa nakaraang layer, 

16
00:00:58,322 --> 00:01:00,820
kasama ang ilang espesyal na numero na tinatawag na bias.

17
00:01:02,160 --> 00:01:05,304
Pagkatapos ay bubuo ka ng kabuuan na iyon gamit ang ilang iba pang function, 

18
00:01:05,304 --> 00:01:08,940
tulad ng sigmoid squishification, o isang relu, sa paraan ng paglakad ko sa huling video.

19
00:01:09,480 --> 00:01:13,355
Sa kabuuan, dahil sa medyo arbitraryong pagpili ng dalawang nakatagong 

20
00:01:13,355 --> 00:01:17,339
layer na may 16 na neuron bawat isa, ang network ay may humigit-kumulang 

21
00:01:17,339 --> 00:01:19,959
13,000 timbang at bias na maaari nating ayusin, 

22
00:01:19,959 --> 00:01:24,380
at ang mga halagang ito ang tumutukoy kung ano ang eksaktong ginagawa ng network.

23
00:01:24,880 --> 00:01:27,659
Kung gayon ang ibig nating sabihin kapag sinabi nating ang network 

24
00:01:27,659 --> 00:01:30,520
na ito ay nag-uuri ng isang naibigay na digit ay ang pinakamaliwanag 

25
00:01:30,520 --> 00:01:33,300
sa 10 neuron na iyon sa huling layer ay tumutugma sa digit na iyon.

26
00:01:34,100 --> 00:01:37,731
At tandaan, ang pagganyak na nasa isip namin dito para sa layered na istraktura ay 

27
00:01:37,731 --> 00:01:40,487
maaaring ang pangalawang layer ay maaaring kunin sa mga gilid, 

28
00:01:40,487 --> 00:01:44,162
at ang ikatlong layer ay maaaring kunin ang mga pattern tulad ng mga loop at linya, 

29
00:01:44,162 --> 00:01:47,006
at ang huling isa ay maaari lamang pagsama-samahin ang mga iyon. 

30
00:01:47,006 --> 00:01:48,800
mga pattern upang makilala ang mga digit.

31
00:01:49,800 --> 00:01:52,240
Kaya dito, natutunan natin kung paano natututo ang network.

32
00:01:52,640 --> 00:01:56,044
Ang gusto namin ay isang algorithm kung saan maipapakita mo sa network na 

33
00:01:56,044 --> 00:01:58,390
ito ang isang buong bungkos ng data ng pagsasanay, 

34
00:01:58,390 --> 00:02:02,438
na nasa anyo ng isang grupo ng iba't ibang mga larawan ng mga sulat-kamay na digit, 

35
00:02:02,438 --> 00:02:05,198
kasama ang mga label para sa kung ano ang dapat na mga ito, 

36
00:02:05,198 --> 00:02:08,878
at ito ay ayusin ang 13,000 timbang at bias na iyon upang mapabuti ang pagganap 

37
00:02:08,878 --> 00:02:10,120
nito sa data ng pagsasanay.

38
00:02:10,720 --> 00:02:13,750
Sana, ang layered na istrakturang ito ay nangangahulugan na ang natututuhan 

39
00:02:13,750 --> 00:02:16,860
nito ay nagsa-generalize sa mga larawang lampas sa data ng pagsasanay na iyon.

40
00:02:17,640 --> 00:02:20,488
Ang paraan namin ng pagsubok ay na pagkatapos mong sanayin ang network, 

41
00:02:20,488 --> 00:02:23,495
ipinapakita mo ito ng mas may label na data na hindi pa nito nakikita dati, 

42
00:02:23,495 --> 00:02:26,700
at makikita mo kung gaano katumpak ang pag-uuri nito sa mga bagong larawang iyon.

43
00:02:31,120 --> 00:02:34,482
Sa kabutihang palad para sa amin, at kung bakit ito ay isang karaniwang halimbawa 

44
00:02:34,482 --> 00:02:37,926
sa pagsisimula, ay ang mabubuting tao sa likod ng database ng MNIST ay nagsama-sama 

45
00:02:37,926 --> 00:02:41,288
ng isang koleksyon ng sampu-sampung libong mga sulat-kamay na digit na mga imahe, 

46
00:02:41,288 --> 00:02:44,200
bawat isa ay may label na may mga numero na dapat nilang itala. maging.

47
00:02:44,900 --> 00:02:48,168
At kahit gaano kagalit-galit na ilarawan ang isang makina bilang pag-aaral, 

48
00:02:48,168 --> 00:02:51,652
kapag nakita mo na kung paano ito gumagana, mas mababa ang pakiramdam nito tulad 

49
00:02:51,652 --> 00:02:55,480
ng ilang nakatutuwang sci-fi premise, at higit na katulad ng isang ehersisyo sa calculus.

50
00:02:56,200 --> 00:02:58,020
Ibig kong sabihin, karaniwang bumababa ito sa 

51
00:02:58,020 --> 00:02:59,960
paghahanap ng minimum ng isang tiyak na function.

52
00:03:01,940 --> 00:03:06,207
Tandaan, sa konsepto, iniisip natin na ang bawat neuron ay konektado sa lahat ng mga 

53
00:03:06,207 --> 00:03:10,123
neuron sa nakaraang layer, at ang mga timbang sa weighted sum na tumutukoy sa 

54
00:03:10,123 --> 00:03:13,437
pag-activate nito ay katulad ng mga lakas ng mga koneksyong iyon, 

55
00:03:13,437 --> 00:03:17,504
at ang bias ay ilang indikasyon ng kung ang neuron na iyon ay may posibilidad na 

56
00:03:17,504 --> 00:03:18,960
maging aktibo o hindi aktibo.

57
00:03:19,720 --> 00:03:21,977
At para simulan ang mga bagay-bagay, sisimulan na lang 

58
00:03:21,977 --> 00:03:24,400
natin ang lahat ng mga timbang at bias na iyon nang random.

59
00:03:24,940 --> 00:03:26,898
Hindi na kailangang sabihin, ang network na ito ay gaganap ng 

60
00:03:26,898 --> 00:03:29,014
medyo kakila-kilabot sa isang ibinigay na halimbawa ng pagsasanay, 

61
00:03:29,014 --> 00:03:30,720
dahil ito ay gumagawa lamang ng isang bagay na random.

62
00:03:31,040 --> 00:03:33,841
Halimbawa, nagpapakain ka sa larawang ito ng isang 3, 

63
00:03:33,841 --> 00:03:36,020
at ang output layer ay mukhang isang gulo.

64
00:03:36,600 --> 00:03:39,508
Kaya ang gagawin mo ay tukuyin ang isang function ng gastos, 

65
00:03:39,508 --> 00:03:42,607
isang paraan ng pagsasabi sa computer, hindi, masamang computer, 

66
00:03:42,607 --> 00:03:45,992
ang output na iyon ay dapat magkaroon ng mga pag-activate na 0 para sa 

67
00:03:45,992 --> 00:03:48,709
karamihan ng mga neuron, ngunit 1 para sa neuron na ito, 

68
00:03:48,709 --> 00:03:50,760
ang ibinigay mo sa akin ay lubos na basura.

69
00:03:51,720 --> 00:03:56,136
Upang sabihin na mas mathematically, idinagdag mo ang mga parisukat ng mga pagkakaiba sa 

70
00:03:56,136 --> 00:04:00,553
pagitan ng bawat isa sa mga pag-activate ng output ng basura at ang halaga na gusto mong 

71
00:04:00,553 --> 00:04:05,020
magkaroon ng mga ito, at ito ang tatawagin naming halaga ng isang halimbawa ng pagsasanay.

72
00:04:05,960 --> 00:04:11,494
Pansinin na maliit ang kabuuan na ito kapag kumpiyansa na inuri ng network ang larawan, 

73
00:04:11,494 --> 00:04:16,399
ngunit malaki ito kapag tila hindi alam ng network kung ano ang ginagawa nito.

74
00:04:18,640 --> 00:04:21,991
Kaya ang gagawin mo ay isaalang-alang ang average na gastos sa lahat 

75
00:04:21,991 --> 00:04:25,440
ng sampu-sampung libong mga halimbawa ng pagsasanay na iyong magagamit.

76
00:04:27,040 --> 00:04:30,742
Ang average na gastos na ito ay ang aming sukatan para sa kung gaano kasira ang network, 

77
00:04:30,742 --> 00:04:32,740
at kung gaano masama ang pakiramdam ng computer.

78
00:04:33,420 --> 00:04:34,600
At iyon ay isang kumplikadong bagay.

79
00:04:35,040 --> 00:04:38,441
Tandaan kung paano ang network mismo ay karaniwang isang function, 

80
00:04:38,441 --> 00:04:42,300
isa na kumukuha ng 784 na numero bilang mga input, ang mga halaga ng pixel, 

81
00:04:42,300 --> 00:04:44,687
at naglalabas ng 10 numero bilang output nito, 

82
00:04:44,687 --> 00:04:48,800
at sa isang kahulugan ito ay na-parameter ng lahat ng mga timbang at bias na ito?

83
00:04:49,500 --> 00:04:52,820
Well ang cost function ay isang layer ng pagiging kumplikado sa itaas ng na.

84
00:04:53,100 --> 00:04:56,904
Kinukuha nito bilang input ang 13,000 o higit pang mga timbang at bias, 

85
00:04:56,904 --> 00:05:00,867
at naglalabas ng isang solong numero na naglalarawan kung gaano kalala ang 

86
00:05:00,867 --> 00:05:04,778
mga timbang at bias na iyon, at ang paraan ng pagtukoy nito ay depende sa 

87
00:05:04,778 --> 00:05:08,900
gawi ng network sa lahat ng sampu-sampung libong piraso ng data ng pagsasanay.

88
00:05:09,520 --> 00:05:11,000
Napakaraming dapat isipin.

89
00:05:12,400 --> 00:05:14,154
Ngunit ang pagsasabi lamang sa computer kung ano ang isang 

90
00:05:14,154 --> 00:05:15,820
masamang trabaho na ginagawa nito ay hindi nakakatulong.

91
00:05:16,220 --> 00:05:18,242
Gusto mong sabihin dito kung paano baguhin ang mga timbang 

92
00:05:18,242 --> 00:05:20,060
at pagkiling na iyon upang ito ay maging mas mahusay.

93
00:05:20,780 --> 00:05:24,105
Upang gawing mas madali, sa halip na hirap na isipin ang isang function 

94
00:05:24,105 --> 00:05:27,339
na may 13,000 input, isipin lamang ang isang simpleng function na may 

95
00:05:27,339 --> 00:05:30,480
isang numero bilang isang input at isang numero bilang isang output.

96
00:05:31,480 --> 00:05:35,300
Paano ka makakahanap ng input na nagpapaliit sa halaga ng function na ito?

97
00:05:36,460 --> 00:05:40,092
Malalaman ng mga mag-aaral ng Calculus na kung minsan ay malinaw mong malalaman 

98
00:05:40,092 --> 00:05:43,633
ang pinakamaliit na iyon, ngunit hindi ito palaging magagawa para sa talagang 

99
00:05:43,633 --> 00:05:47,356
kumplikadong mga function, tiyak na wala sa 13,000 input na bersyon ng sitwasyong 

100
00:05:47,356 --> 00:05:51,080
ito para sa aming nakakabaliw na kumplikadong paggana ng gastos sa neural network.

101
00:05:51,580 --> 00:05:54,852
Ang isang mas nababaluktot na taktika ay ang magsimula sa anumang input, 

102
00:05:54,852 --> 00:05:58,527
at alamin kung aling direksyon ang dapat mong hakbang upang gawing mas mababa ang 

103
00:05:58,527 --> 00:05:59,200
output na iyon.

104
00:06:00,080 --> 00:06:03,834
Sa partikular, kung maaari mong malaman ang slope ng function kung nasaan ka, 

105
00:06:03,834 --> 00:06:06,963
pagkatapos ay lumipat sa kaliwa kung positibo ang slope na iyon, 

106
00:06:06,963 --> 00:06:09,900
at ilipat ang input sa kanan kung negatibo ang slope na iyon.

107
00:06:11,960 --> 00:06:15,923
Kung gagawin mo ito nang paulit-ulit, sa bawat punto na sinusuri ang bagong slope at 

108
00:06:15,923 --> 00:06:19,840
ginagawa ang naaangkop na hakbang, lalapit ka sa ilang lokal na minimum ng function.

109
00:06:20,640 --> 00:06:23,800
Ang imahe na maaaring nasa isip mo dito ay isang bola na gumugulong pababa sa isang burol.

110
00:06:24,620 --> 00:06:28,326
Pansinin, kahit na para sa talagang pinasimpleng single input function na ito, 

111
00:06:28,326 --> 00:06:30,672
maraming posibleng lambak na maaari mong mapunta, 

112
00:06:30,672 --> 00:06:33,206
depende sa kung saang random na input ka magsisimula, 

113
00:06:33,206 --> 00:06:36,772
at walang garantiya na ang lokal na minimum na iyong napuntahan ay magiging 

114
00:06:36,772 --> 00:06:39,400
pinakamaliit na posibleng halaga. ng function ng gastos.

115
00:06:40,220 --> 00:06:42,620
Dadalhin din iyon sa aming kaso ng neural network.

116
00:06:43,180 --> 00:06:47,018
Gusto ko ring mapansin mo kung paano kung gagawin mong proporsyonal ang mga laki 

117
00:06:47,018 --> 00:06:50,430
ng iyong hakbang sa slope, at kapag ang slope ay unti-unting lumalapad, 

118
00:06:50,430 --> 00:06:54,600
unti-unting lumiliit ang iyong mga hakbang, at nakakatulong iyon sa iyong mag-overshoot.

119
00:06:55,940 --> 00:06:58,311
Papataasin ang pagiging kumplikado ng kaunti, isipin sa 

120
00:06:58,311 --> 00:07:00,980
halip ang isang function na may dalawang input at isang output.

121
00:07:01,500 --> 00:07:04,473
Maaari mong isipin ang puwang ng pag-input bilang xy-plane, 

122
00:07:04,473 --> 00:07:08,140
at ang function ng gastos ay naka-graph bilang isang ibabaw sa itaas nito.

123
00:07:08,760 --> 00:07:11,425
Sa halip na magtanong tungkol sa slope ng function, 

124
00:07:11,425 --> 00:07:14,859
kailangan mong itanong kung aling direksyon ang dapat mong hakbang 

125
00:07:14,859 --> 00:07:18,960
sa input space na ito upang mabawasan ang output ng function nang pinakamabilis.

126
00:07:19,720 --> 00:07:21,760
Sa madaling salita, ano ang direksyon ng pababa?

127
00:07:22,380 --> 00:07:25,560
Muli, kapaki-pakinabang na mag-isip ng isang bola na lumiligid sa burol na iyon.

128
00:07:26,660 --> 00:07:30,516
Malalaman ng mga pamilyar sa multivariable calculus na ang gradient ng isang 

129
00:07:30,516 --> 00:07:34,222
function ay nagbibigay sa iyo ng direksyon ng pinakamatarik na pag-akyat, 

130
00:07:34,222 --> 00:07:38,078
kung aling direksyon ang dapat mong hakbang upang mapataas ang function nang 

131
00:07:38,078 --> 00:07:38,780
pinakamabilis.

132
00:07:39,560 --> 00:07:42,733
Natural lang, ang pagkuha ng negatibo sa gradient na iyon ay nagbibigay 

133
00:07:42,733 --> 00:07:46,040
sa iyo ng direksyon sa hakbang na pinakamabilis na nagpapababa sa function.

134
00:07:47,240 --> 00:07:50,333
Higit pa riyan, ang haba ng gradient vector na ito ay isang 

135
00:07:50,333 --> 00:07:53,840
indikasyon kung gaano katarik ang pinakamatarik na dalisdis na iyon.

136
00:07:54,540 --> 00:07:57,848
Kung hindi ka pamilyar sa multivariable calculus at gusto mong matuto nang higit pa, 

137
00:07:57,848 --> 00:08:00,340
tingnan ang ilan sa mga ginawa ko para sa Khan Academy sa paksa.

138
00:08:00,860 --> 00:08:04,619
Sa totoo lang, ang mahalaga lang para sa iyo at sa akin ngayon ay sa prinsipyo 

139
00:08:04,619 --> 00:08:07,331
mayroong isang paraan upang makalkula ang vector na ito, 

140
00:08:07,331 --> 00:08:11,043
ang vector na ito na nagsasabi sa iyo kung ano ang pababang direksyon at kung 

141
00:08:11,043 --> 00:08:11,900
gaano ito katarik.

142
00:08:12,400 --> 00:08:16,120
Magiging okay ka kung iyon lang ang alam mo at hindi ka rock solid sa mga detalye.

143
00:08:17,200 --> 00:08:20,333
Kung makukuha mo iyon, ang algorithm para sa pagliit ng function ay 

144
00:08:20,333 --> 00:08:22,684
upang kalkulahin ang gradient na direksyon na ito, 

145
00:08:22,684 --> 00:08:26,740
pagkatapos ay gumawa ng isang maliit na hakbang pababa, at ulitin iyon nang paulit-ulit.

146
00:08:27,700 --> 00:08:30,336
Ito ay ang parehong pangunahing ideya para sa isang 

147
00:08:30,336 --> 00:08:32,820
function na may 13,000 input sa halip na 2 input.

148
00:08:33,400 --> 00:08:36,518
Isipin na ayusin ang lahat ng 13,000 timbang at bias 

149
00:08:36,518 --> 00:08:39,460
ng aming network sa isang higanteng column vector.

150
00:08:40,140 --> 00:08:43,796
Ang negatibong gradient ng cost function ay isang vector lamang, 

151
00:08:43,796 --> 00:08:48,635
ito ay ilang direksyon sa loob ng napakalaking input space na ito na nagsasabi sa iyo 

152
00:08:48,635 --> 00:08:53,473
kung aling mga nudge sa lahat ng mga numerong iyon ang magdudulot ng pinakamabilis na 

153
00:08:53,473 --> 00:08:54,880
pagbaba sa cost function.

154
00:08:55,640 --> 00:08:58,555
At siyempre, sa aming espesyal na idinisenyong pag-andar sa gastos, 

155
00:08:58,555 --> 00:09:02,329
ang pagpapalit ng mga timbang at pagkiling upang mabawasan ang ibig sabihin nito ay ang 

156
00:09:02,329 --> 00:09:06,188
paggawa ng output ng network sa bawat piraso ng data ng pagsasanay na hindi mukhang isang 

157
00:09:06,188 --> 00:09:09,833
random na hanay ng 10 mga halaga, at higit na katulad ng isang aktwal na desisyon na 

158
00:09:09,833 --> 00:09:10,820
gusto namin. gawin ito.

159
00:09:11,440 --> 00:09:14,758
Mahalagang tandaan, ang function ng gastos na ito ay nagsasangkot ng average 

160
00:09:14,758 --> 00:09:17,258
sa lahat ng data ng pagsasanay, kaya kung bawasan mo ito, 

161
00:09:17,258 --> 00:09:20,533
nangangahulugan ito na ito ay isang mas mahusay na pagganap sa lahat ng mga 

162
00:09:20,533 --> 00:09:21,180
sample na iyon.

163
00:09:23,820 --> 00:09:26,630
Ang algorithm para sa mahusay na pag-compute ng gradient na ito, 

164
00:09:26,630 --> 00:09:30,218
na kung saan ay epektibong sentro ng kung paano natututo ang isang neural network, 

165
00:09:30,218 --> 00:09:33,980
ay tinatawag na backpropagation, at ito ang pag-uusapan ko tungkol sa susunod na video.

166
00:09:34,660 --> 00:09:37,709
Doon, gusto ko talagang maglaan ng oras upang suriin kung ano ang eksaktong 

167
00:09:37,709 --> 00:09:40,880
nangyayari sa bawat timbang at bias para sa isang partikular na piraso ng data 

168
00:09:40,880 --> 00:09:43,969
ng pagsasanay, sinusubukang magbigay ng intuitive na pakiramdam para sa kung 

169
00:09:43,969 --> 00:09:47,100
ano ang nangyayari sa kabila ng tumpok ng mga nauugnay na calculus at formula.

170
00:09:47,780 --> 00:09:50,455
Dito, sa ngayon, ang pangunahing bagay na gusto kong malaman mo, 

171
00:09:50,455 --> 00:09:52,390
independiyente sa mga detalye ng pagpapatupad, 

172
00:09:52,390 --> 00:09:55,848
ay ang ibig nating sabihin kapag pinag-uusapan natin ang tungkol sa isang pag-aaral 

173
00:09:55,848 --> 00:09:58,360
sa network ay ang pagliit lamang ng isang function ng gastos.

174
00:09:59,300 --> 00:10:02,117
At pansinin, ang isang kahihinatnan nito ay mahalaga para sa function ng 

175
00:10:02,117 --> 00:10:04,356
gastos na ito na magkaroon ng magandang maayos na output, 

176
00:10:04,356 --> 00:10:07,405
upang makahanap tayo ng lokal na minimum sa pamamagitan ng pagkuha ng maliliit 

177
00:10:07,405 --> 00:10:08,100
na hakbang pababa.

178
00:10:09,260 --> 00:10:11,652
Ito ang dahilan kung bakit, sa pamamagitan ng paraan, 

179
00:10:11,652 --> 00:10:14,532
ang mga artipisyal na neuron ay may patuloy na mga pag-activate, 

180
00:10:14,532 --> 00:10:17,545
sa halip na maging aktibo o hindi aktibo sa isang binary na paraan, 

181
00:10:17,545 --> 00:10:19,140
ang paraan ng mga biological neuron.

182
00:10:20,220 --> 00:10:23,511
Ang prosesong ito ng paulit-ulit na pag-nudging ng input ng isang function 

183
00:10:23,511 --> 00:10:26,760
ng ilang multiple ng negatibong gradient ay tinatawag na gradient descent.

184
00:10:27,300 --> 00:10:30,014
Ito ay isang paraan upang magsama-sama patungo sa ilang lokal na minimum 

185
00:10:30,014 --> 00:10:32,580
ng isang function ng gastos, karaniwang isang lambak sa graph na ito.

186
00:10:33,440 --> 00:10:36,871
Nagpapakita pa rin ako ng larawan ng isang function na may dalawang input, siyempre, 

187
00:10:36,871 --> 00:10:40,464
dahil ang mga nudge sa isang 13,000 dimensional na espasyo sa pag-input ay medyo mahirap 

188
00:10:40,464 --> 00:10:44,098
i-wrap ang iyong isip, ngunit mayroong isang magandang non-spatial na paraan upang isipin 

189
00:10:44,098 --> 00:10:44,260
ito.

190
00:10:45,080 --> 00:10:48,440
Ang bawat bahagi ng negatibong gradient ay nagsasabi sa amin ng dalawang bagay.

191
00:10:49,060 --> 00:10:52,176
Ang tanda, siyempre, ay nagsasabi sa amin kung ang kaukulang 

192
00:10:52,176 --> 00:10:55,140
bahagi ng input vector ay dapat na itulak pataas o pababa.

193
00:10:55,800 --> 00:10:59,306
Ngunit ang mahalaga, ang mga relatibong magnitude ng lahat ng mga bahaging 

194
00:10:59,306 --> 00:11:02,720
ito ay uri ng nagsasabi sa iyo kung aling mga pagbabago ang mas mahalaga.

195
00:11:05,220 --> 00:11:09,020
Nakikita mo, sa aming network, ang isang pagsasaayos sa isa sa mga timbang ay maaaring 

196
00:11:09,020 --> 00:11:12,690
magkaroon ng mas malaking epekto sa paggana ng gastos kaysa sa pagsasaayos sa ibang 

197
00:11:12,690 --> 00:11:13,040
timbang.

198
00:11:14,800 --> 00:11:18,200
Ang ilan sa mga koneksyong ito ay mas mahalaga lamang para sa aming data ng pagsasanay.

199
00:11:19,320 --> 00:11:22,623
Kaya isang paraan na maiisip mo ang tungkol sa gradient vector na ito ng 

200
00:11:22,623 --> 00:11:25,746
ating mind-warpingly massive cost function ay ang pag-encode nito sa 

201
00:11:25,746 --> 00:11:28,598
relatibong kahalagahan ng bawat timbang at bias, ibig sabihin, 

202
00:11:28,598 --> 00:11:32,400
alin sa mga pagbabagong ito ang magdadala ng pinakamaraming bang para sa iyong pera.

203
00:11:33,620 --> 00:11:36,640
Ito ay talagang isa pang paraan ng pag-iisip tungkol sa direksyon.

204
00:11:37,100 --> 00:11:40,706
Upang kumuha ng isang mas simpleng halimbawa, kung mayroon kang ilang function na 

205
00:11:40,706 --> 00:11:44,313
may dalawang variable bilang isang input, at kino-compute mo na ang gradient nito 

206
00:11:44,313 --> 00:11:46,600
sa ilang partikular na punto ay lalabas bilang 3,1, 

207
00:11:46,600 --> 00:11:50,163
pagkatapos ay sa isang banda maaari mong bigyang-kahulugan iyon bilang pagsasabi 

208
00:11:50,163 --> 00:11:52,495
na kapag ikaw' muling nakatayo sa input na iyon, 

209
00:11:52,495 --> 00:11:55,926
ang paglipat sa direksyong ito ay pinapataas ang function nang pinakamabilis, 

210
00:11:55,926 --> 00:11:59,093
na kapag na-graph mo ang function sa itaas ng plane ng mga input point, 

211
00:11:59,093 --> 00:12:02,260
ang vector na iyon ang nagbibigay sa iyo ng tuwid na direksyong paakyat.

212
00:12:02,860 --> 00:12:06,536
Ngunit ang isa pang paraan para basahin iyon ay ang pagsasabi na ang mga pagbabago 

213
00:12:06,536 --> 00:12:09,902
sa unang variable na ito ay may 3 beses ang kahalagahan ng mga pagbabago sa 

214
00:12:09,902 --> 00:12:13,356
pangalawang variable, na kahit man lang sa kapitbahayan ng nauugnay na input, 

215
00:12:13,356 --> 00:12:16,900
ang pag-nudging ng x-value ay nagdadala ng mas maraming bang para sa iyong buck.

216
00:12:19,880 --> 00:12:22,340
Mag-zoom out tayo at buod kung nasaan tayo sa ngayon.

217
00:12:22,840 --> 00:12:26,680
Ang network mismo ay ang function na ito na may 784 input at 10 output, 

218
00:12:26,680 --> 00:12:30,040
na tinukoy sa mga tuntunin ng lahat ng mga weighted sum na ito.

219
00:12:30,640 --> 00:12:33,680
Ang function ng gastos ay isang layer ng pagiging kumplikado sa itaas nito.

220
00:12:33,980 --> 00:12:37,978
Ito ay tumatagal ng 13,000 mga timbang at bias bilang mga input at naglalabas 

221
00:12:37,978 --> 00:12:41,720
ng isang solong sukatan ng kahinaan batay sa mga halimbawa ng pagsasanay.

222
00:12:42,440 --> 00:12:46,900
At ang gradient ng cost function ay isa pang layer ng pagiging kumplikado.

223
00:12:47,360 --> 00:12:49,962
Sinasabi nito sa amin kung ano ang nagdudulot sa lahat ng mga timbang 

224
00:12:49,962 --> 00:12:52,675
at bias na ito na nagiging sanhi ng pinakamabilis na pagbabago sa halaga 

225
00:12:52,675 --> 00:12:55,389
ng function ng gastos, na maaari mong bigyang-kahulugan bilang pagsasabi 

226
00:12:55,389 --> 00:12:57,880
kung aling mga pagbabago kung aling mga timbang ang pinakamahalaga.

227
00:13:02,560 --> 00:13:05,970
Kaya, kapag sinimulan mo ang network na may mga random na timbang at bias, 

228
00:13:05,970 --> 00:13:09,880
at inayos ang mga ito nang maraming beses batay sa prosesong ito ng gradient descent, 

229
00:13:09,880 --> 00:13:13,200
gaano ba ito kahusay na gumaganap sa mga larawang hindi pa nakikita noon?

230
00:13:14,100 --> 00:13:18,120
Ang isa na inilarawan ko dito, na may dalawang nakatagong layer ng 16 na neuron 

231
00:13:18,120 --> 00:13:22,291
bawat isa, pinili karamihan para sa aesthetic na mga kadahilanan, ay hindi masama, 

232
00:13:22,291 --> 00:13:25,960
pag-uuri tungkol sa 96% ng mga bagong larawan na nakikita nito nang tama.

233
00:13:26,680 --> 00:13:30,662
At sa totoo lang, kung titingnan mo ang ilan sa mga halimbawang pinagkakaguluhan nito, 

234
00:13:30,662 --> 00:13:32,540
napipilitan kang bawasan ito nang kaunti.

235
00:13:36,220 --> 00:13:39,115
Ngayon kung maglalaro ka sa nakatagong istraktura ng layer at gumawa 

236
00:13:39,115 --> 00:13:41,760
ng ilang mga pag-aayos, maaari mong makuha ito ng hanggang 98%.

237
00:13:41,760 --> 00:13:42,720
At iyon ay medyo maganda!

238
00:13:43,020 --> 00:13:46,173
Hindi ito ang pinakamahusay, tiyak na makakakuha ka ng mas mahusay na pagganap 

239
00:13:46,173 --> 00:13:49,446
sa pamamagitan ng pagiging mas sopistikado kaysa sa plain vanilla network na ito, 

240
00:13:49,446 --> 00:13:51,800
ngunit dahil sa kung gaano nakakatakot ang paunang gawain, 

241
00:13:51,800 --> 00:13:54,954
sa palagay ko mayroong isang bagay na hindi kapani-paniwala tungkol sa anumang 

242
00:13:54,954 --> 00:13:57,987
network na ginagawa ito nang maayos sa mga larawang hindi pa nakikita noon, 

243
00:13:57,987 --> 00:14:01,420
dahil doon hindi namin partikular na sinabi dito kung anong mga pattern ang hahanapin.

244
00:14:02,560 --> 00:14:05,446
Sa orihinal, ang paraan ng pag-udyok ko sa istrukturang ito ay sa pamamagitan 

245
00:14:05,446 --> 00:14:07,630
ng paglalarawan ng isang pag-asa na maaaring mayroon tayo, 

246
00:14:07,630 --> 00:14:10,036
na ang pangalawang layer ay maaaring kunin sa maliliit na gilid, 

247
00:14:10,036 --> 00:14:12,997
na ang ikatlong layer ay pagsasama-samahin ang mga gilid upang makilala ang mga 

248
00:14:12,997 --> 00:14:15,736
loop at mas mahabang linya, at ang mga iyon ay maaaring magkapira-piraso. 

249
00:14:15,736 --> 00:14:17,180
magkasama upang makilala ang mga digit.

250
00:14:17,960 --> 00:14:20,400
Kaya ito ba talaga ang ginagawa ng ating network?

251
00:14:21,080 --> 00:14:24,400
Well, para sa isang ito hindi bababa sa, hindi sa lahat.

252
00:14:24,820 --> 00:14:27,838
Tandaan kung paano natin tiningnan ang huling video kung paano makikita 

253
00:14:27,838 --> 00:14:30,940
ang mga bigat ng mga koneksyon mula sa lahat ng mga neuron sa unang layer 

254
00:14:30,940 --> 00:14:33,958
patungo sa isang partikular na neuron sa pangalawang layer bilang isang 

255
00:14:33,958 --> 00:14:37,060
partikular na pattern ng pixel na kinukuha ng pangalawang layer na neuron?

256
00:14:37,780 --> 00:14:41,621
Well, kapag ginawa talaga namin iyon para sa mga timbang na nauugnay sa 

257
00:14:41,621 --> 00:14:45,036
mga transition na ito, mula sa unang layer hanggang sa susunod, 

258
00:14:45,036 --> 00:14:48,877
sa halip na kunin sa mga nakahiwalay na maliliit na gilid dito at doon, 

259
00:14:48,877 --> 00:14:53,680
ang hitsura nila, well, halos random, na may ilang napakaluwag na pattern sa dun sa gitna.

260
00:14:53,760 --> 00:14:57,629
Mukhang sa hindi maarok na malaking 13,000 dimensional na espasyo ng mga posibleng 

261
00:14:57,629 --> 00:15:01,406
timbang at bias, nakita ng aming network ang sarili nitong isang masayang maliit 

262
00:15:01,406 --> 00:15:05,556
na lokal na minimum na, sa kabila ng matagumpay na pag-uuri sa karamihan ng mga larawan, 

263
00:15:05,556 --> 00:15:08,960
ay hindi eksaktong nakakakuha sa mga pattern na maaaring inaasahan namin.

264
00:15:09,780 --> 00:15:11,402
At para talagang maihatid ang puntong ito pauwi, 

265
00:15:11,402 --> 00:15:13,820
panoorin kung ano ang mangyayari kapag nag-input ka ng random na larawan.

266
00:15:14,320 --> 00:15:17,486
Kung matalino ang system, maaari mong asahan na hindi ito sigurado, 

267
00:15:17,486 --> 00:15:21,678
marahil ay hindi talaga ina-activate ang alinman sa 10 output neuron na iyon o i-activate 

268
00:15:21,678 --> 00:15:25,776
ang lahat ng ito nang pantay-pantay, ngunit sa halip ay may kumpiyansa itong nagbibigay 

269
00:15:25,776 --> 00:15:29,828
sa iyo ng ilang walang kapararakan na sagot, na parang sigurado na ang random na ingay 

270
00:15:29,828 --> 00:15:33,787
na ito. ay isang 5 tulad ng ginagawa nito na ang isang aktwal na imahe ng isang 5 ay 

271
00:15:33,787 --> 00:15:34,160
isang 5.

272
00:15:34,540 --> 00:15:38,225
Naiiba ang phrase, kahit na makilala ng network na ito ang mga digit, 

273
00:15:38,225 --> 00:15:40,700
wala itong ideya kung paano iguhit ang mga ito.

274
00:15:41,420 --> 00:15:45,240
Marami sa mga ito ay dahil ito ay isang mahigpit na napilitang setup ng pagsasanay.

275
00:15:45,880 --> 00:15:47,740
I mean, ilagay mo ang sarili mo sa network's shoes dito.

276
00:15:48,140 --> 00:15:51,243
Mula sa pananaw nito, ang buong sansinukob ay binubuo ng walang anuman 

277
00:15:51,243 --> 00:15:54,478
kundi malinaw na tinukoy na hindi gumagalaw na mga digit na nakasentro sa 

278
00:15:54,478 --> 00:15:57,538
isang maliit na grid, at ang gastos nito ay hindi nagbigay ng anumang 

279
00:15:57,538 --> 00:16:01,080
insentibo upang maging anumang bagay ngunit lubos na tiwala sa mga desisyon nito.

280
00:16:02,120 --> 00:16:04,898
Kaya sa ito bilang larawan ng kung ano talaga ang ginagawa ng mga pangalawang 

281
00:16:04,898 --> 00:16:07,426
layer na neuron na iyon, maaari kang magtaka kung bakit ko ipapakilala 

282
00:16:07,426 --> 00:16:09,920
ang network na ito na may pagganyak na kunin ang mga gilid at pattern.

283
00:16:09,920 --> 00:16:12,300
Ibig kong sabihin, hindi lang iyon ang natatapos sa paggawa.

284
00:16:13,380 --> 00:16:15,780
Well, ito ay hindi sinadya upang maging ang aming pangwakas na layunin, 

285
00:16:15,780 --> 00:16:17,180
ngunit sa halip ay isang panimulang punto.

286
00:16:17,640 --> 00:16:21,409
Sa totoo lang, ito ay lumang teknolohiya, ang uri na sinaliksik noong 80s at 90s, 

287
00:16:21,409 --> 00:16:24,627
at kailangan mo itong maunawaan bago mo maunawaan ang mas detalyadong 

288
00:16:24,627 --> 00:16:27,706
modernong mga variant, at malinaw na kaya nitong lutasin ang ilang 

289
00:16:27,706 --> 00:16:31,016
kawili-wiling mga problema, ngunit lalo kang naghuhukay sa kung ano ang 

290
00:16:31,016 --> 00:16:34,740
mga nakatagong layer ay talagang ginagawa, ang hindi gaanong katalinuhan ay tila.

291
00:16:38,480 --> 00:16:41,030
Saglit na inilipat ang focus mula sa kung paano natututo ang 

292
00:16:41,030 --> 00:16:43,540
mga network sa kung paano ka natututo, mangyayari lang iyon 

293
00:16:43,540 --> 00:16:46,300
kung aktibo kang nakikipag-ugnayan sa materyal dito kahit papaano.

294
00:16:47,060 --> 00:16:50,604
Ang isang medyo simpleng bagay na gusto kong gawin mo ay mag-pause lang ngayon 

295
00:16:50,604 --> 00:16:54,059
at mag-isip nang malalim tungkol sa kung anong mga pagbabago ang maaari mong 

296
00:16:54,059 --> 00:16:57,559
gawin sa system na ito at kung paano nito nakikita ang mga larawan kung gusto 

297
00:16:57,559 --> 00:17:00,880
mo itong mas mahusay na makuha sa mga bagay tulad ng mga gilid at pattern.

298
00:17:01,480 --> 00:17:04,546
Ngunit mas mabuti kaysa doon, upang aktwal na makisali sa materyal, 

299
00:17:04,546 --> 00:17:08,243
lubos kong inirerekomenda ang aklat ni Michael Nielsen sa malalim na pag-aaral at 

300
00:17:08,243 --> 00:17:09,099
mga neural network.

301
00:17:09,680 --> 00:17:12,543
Sa loob nito, mahahanap mo ang code at ang data na ida-download 

302
00:17:12,543 --> 00:17:15,362
at laruin para sa eksaktong halimbawang ito, at ituturo sa iyo 

303
00:17:15,362 --> 00:17:18,359
ng aklat ang hakbang-hakbang kung ano ang ginagawa ng code na iyon.

304
00:17:19,300 --> 00:17:22,166
Ang kahanga-hanga ay ang aklat na ito ay libre at magagamit ng publiko, 

305
00:17:22,166 --> 00:17:24,952
kaya kung mayroon kang makukuha mula rito, isaalang-alang ang pagsama 

306
00:17:24,952 --> 00:17:27,660
sa akin sa pagbibigay ng donasyon para sa mga pagsisikap ni Nielsen.

307
00:17:27,660 --> 00:17:31,617
Nag-link din ako ng ilang iba pang mapagkukunan na gusto ko sa paglalarawan, 

308
00:17:31,617 --> 00:17:36,088
kasama ang kahanga-hanga at magandang post sa blog ni Chris Ola at ang mga artikulo sa 

309
00:17:36,088 --> 00:17:36,500
Distill.

310
00:17:38,280 --> 00:17:40,795
Upang isara ang mga bagay dito sa mga huling minuto, 

311
00:17:40,795 --> 00:17:43,880
gusto kong bumalik sa isang snippet ng panayam ko kay Leisha Lee.

312
00:17:44,300 --> 00:17:45,796
Maaari mong matandaan siya mula sa huling video, 

313
00:17:45,796 --> 00:17:47,720
ginawa niya ang kanyang PhD na trabaho sa malalim na pag-aaral.

314
00:17:48,300 --> 00:17:50,914
Sa maliit na snippet na ito, pinag-uusapan niya ang tungkol sa dalawang 

315
00:17:50,914 --> 00:17:53,274
kamakailang papel na talagang naghuhukay sa kung paano aktwal na 

316
00:17:53,274 --> 00:17:55,780
natututo ang ilan sa mga mas modernong network ng pagkilala sa imahe.

317
00:17:56,120 --> 00:17:58,182
Para lang i-set up kung nasaan kami sa pag-uusap, 

318
00:17:58,182 --> 00:18:01,440
kinuha ng unang papel ang isa sa mga partikular na malalalim na neural network 

319
00:18:01,440 --> 00:18:03,502
na ito na talagang mahusay sa pagkilala ng imahe, 

320
00:18:03,502 --> 00:18:06,224
at sa halip na sanayin ito sa isang dataset na may wastong label, 

321
00:18:06,224 --> 00:18:08,740
binasa ang lahat ng mga label sa paligid bago ang pagsasanay.

322
00:18:09,480 --> 00:18:13,087
Malinaw na ang katumpakan ng pagsubok dito ay hindi mas mahusay kaysa sa random, 

323
00:18:13,087 --> 00:18:17,050
dahil ang lahat ay random na may label, ngunit nagawa pa rin nitong makamit ang parehong 

324
00:18:17,050 --> 00:18:20,880
katumpakan ng pagsasanay tulad ng gagawin mo sa isang maayos na naka-label na dataset.

325
00:18:21,600 --> 00:18:25,215
Sa pangkalahatan, ang milyun-milyong timbang para sa partikular na network 

326
00:18:25,215 --> 00:18:28,349
na ito ay sapat na para maisaulo lamang nito ang random na data, 

327
00:18:28,349 --> 00:18:32,013
na nagpapataas ng tanong kung ang pag-minimize sa function ng gastos na ito 

328
00:18:32,013 --> 00:18:34,953
ay talagang tumutugma sa anumang uri ng istraktura sa imahe, 

329
00:18:34,953 --> 00:18:36,400
o ito ba ay pagsasaulo lamang?

330
00:18:51,440 --> 00:18:54,517
Kung titingnan mo ang curve ng katumpakan na iyon, 

331
00:18:54,517 --> 00:18:57,656
kung nagsasanay ka lang sa isang random na dataset, 

332
00:18:57,656 --> 00:19:02,725
ang uri ng curve na iyon ay bumaba nang napakabagal sa halos uri ng isang linear na 

333
00:19:02,725 --> 00:19:07,553
paraan, kaya talagang nahihirapan kang hanapin ang lokal na minimum na posible, 

334
00:19:07,553 --> 00:19:12,140
alam mo , ang mga tamang timbang na magbibigay sa iyo ng ganoong katumpakan.

335
00:19:12,240 --> 00:19:15,987
Samantalang kung talagang nagsasanay ka sa isang structured na dataset, 

336
00:19:15,987 --> 00:19:19,371
isa na may tamang mga label, nagbiliko ka nang kaunti sa simula, 

337
00:19:19,371 --> 00:19:23,327
ngunit pagkatapos ay medyo mabilis kang bumaba upang makarating sa antas ng 

338
00:19:23,327 --> 00:19:27,178
katumpakan na iyon, at sa ilang mga kahulugan ito ay mas madaling mahanap 

339
00:19:27,178 --> 00:19:28,220
ang lokal na maxima.

340
00:19:28,540 --> 00:19:32,886
At kaya kung ano ang kawili-wili din tungkol doon ay nagdudulot ito ng maliwanag na isa 

341
00:19:32,886 --> 00:19:35,849
pang papel mula sa aktwal na ilang taon na ang nakalilipas, 

342
00:19:35,849 --> 00:19:39,256
na may higit pang mga pagpapasimple tungkol sa mga layer ng network, 

343
00:19:39,256 --> 00:19:43,504
ngunit ang isa sa mga resulta ay nagsasabi kung paano kung titingnan mo ang landscape 

344
00:19:43,504 --> 00:19:47,800
ng pag-optimize, ang lokal na minima na malamang na matutunan ng mga network na ito ay 

345
00:19:47,800 --> 00:19:51,850
talagang may pantay na kalidad, kaya sa ilang kahulugan kung ang iyong dataset ay 

346
00:19:51,850 --> 00:19:54,320
nakaayos, dapat mong mahanap iyon nang mas madali.

347
00:19:58,160 --> 00:20:01,180
Ang aking pasasalamat, gaya ng dati, sa inyong sumusuporta sa Patreon.

348
00:20:01,520 --> 00:20:03,759
Nasabi ko na noon kung ano ang game changer ng Patreon, 

349
00:20:03,759 --> 00:20:06,800
ngunit ang mga video na ito ay talagang hindi magiging posible kung wala ka.

350
00:20:07,460 --> 00:20:10,644
Gusto ko ring magbigay ng espesyal na pasasalamat sa VC firm na Amplify Partners, 

351
00:20:10,644 --> 00:20:12,780
sa kanilang suporta sa mga unang video na ito sa serye.

