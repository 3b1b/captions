1
00:00:04,060 --> 00:00:07,040
지난 영상에서는 신경망의 구조에 대해 살펴봤습니다

2
00:00:07,160 --> 00:00:10,080
지난 영상을 다 잊어버렸을게 분명하니 간단하게 복습하도록 하죠

3
00:00:10,080 --> 00:00:15,680
이 영상엔 두 가지 목표가 있습니다. 첫 번째는 '경사 하강법'에 대해 소개하는 것입니다.

4
00:00:15,680 --> 00:00:18,220
이것은 신경망이 어떻게 학습하는지 뿐만 아니라

5
00:00:18,220 --> 00:00:20,540
다른 많은 기계학습이 어떻게 작동하는지도 포함합니다.

6
00:00:20,660 --> 00:00:24,600
그러고 나서 우리는 이 특별한 신경망이 어떻게 작동하는지와

7
00:00:24,600 --> 00:00:27,940
숨겨진 층의 뉴런들이 결국 무엇을 의미하는지도 파고들어 볼 겁니다

8
00:00:29,000 --> 00:00:34,140
다시 상기시키자면 우리의 목표는 손글씨로 적은 숫자를 인식시키는 것입니다

9
00:00:34,140 --> 00:00:36,220
신경망의 'hello world'에 해당하죠

10
00:00:36,500 --> 00:00:43,640
이 숫자들은 28x28 픽셀로 이루어져 있으며 각 픽셀은 0부터 1 사이의 밝기를 갖습니다

11
00:00:43,640 --> 00:00:49,980
이것들은 입력 층에 있는 784개 뉴런의 활성치를 결정합니다

12
00:00:50,840 --> 00:00:58,780
다음 층에 있는 뉴런 각각의 활성치는 가중치와 이전 층의 활성치를 곱한 것들의 총합과

13
00:00:58,780 --> 00:01:01,640
bias라고 하는 특별한 숫자 합에 의해 결정됩니다

14
00:01:01,900 --> 00:01:06,320
그리고 그 합계에 지난 영상에서 보여드린 시그모이드 함수나

15
00:01:06,320 --> 00:01:08,920
ReLU 함수를 취합니다

16
00:01:09,100 --> 00:01:14,800
결론적으로 임의로 설정한 각각 16개의 뉴런들로 구성된 두 개의 숨겨진 층은

17
00:01:14,800 --> 00:01:19,340
13,000여개의 조정 가능한 가중치와 bias들을 가지고 있으며

18
00:01:19,340 --> 00:01:24,320
이 값들은 신경망이 실제로 어떻게 작동할지 결정합니다

19
00:01:24,800 --> 00:01:28,320
우리는 신경망이 주어진 숫자를 분류할 때

20
00:01:28,329 --> 00:01:33,429
마지막 층에서 가장 밝은 열 개의 뉴런 중 하나를 숫자와 대응시킵니다.

21
00:01:33,940 --> 00:01:38,220
그리고 우리가 층 구조의 의의를 기억한다면

22
00:01:38,220 --> 00:01:44,960
아마도 두 번째 층은 숫자의 테두리를 찾아내며 세 번째 층은 아마 고리와 직선 패턴을 찾아낼 것입니다

23
00:01:44,960 --> 00:01:48,720
그리고 마지막은 저런 패턴을 조합하여 숫자를 인식합니다

24
00:01:49,360 --> 00:01:52,440
여기까지 우리는 신경망이 어떻게 학습하는지 배워봤습니다

25
00:01:52,440 --> 00:01:57,260
우리가 원하는 것은 이 신경망에 학습 데이터 전체를 집어넣는 알고리즘입니다

26
00:01:57,260 --> 00:02:00,140
데이터엔 손으로 쓰여진 수많은 다른 숫자들과,

27
00:02:00,140 --> 00:02:03,740
그 숫자들이 원래 무엇이었는지를 알려주는 라벨이 포함되어 있고

28
00:02:03,880 --> 00:02:06,700
신경망은 학습 데이터를 통해

29
00:02:06,700 --> 00:02:10,160
13,000개의 가중치 및 bias를 조정함으로써 성능이 개선될겁니다.

30
00:02:10,720 --> 00:02:13,820
바라건대, 이 층 구조는

31
00:02:13,820 --> 00:02:17,220
그 훈련 데이터를 넘어서서 일반적인 실제 이미지도 인식할 것입니다.

32
00:02:17,220 --> 00:02:20,280
우리가 테스트하는 방식은 신경망을 훈련시킨 후

33
00:02:20,280 --> 00:02:23,780
이전에 보여주지 않았던 데이터를 더 많이 보여줍니다.

34
00:02:23,780 --> 00:02:27,220
그러면 당신은 신경망이 얼마나 새로운 이미지를 잘 분류하는지 볼 수 있습니다.

35
00:02:31,040 --> 00:02:37,380
다행스럽게도 MNIST 소속 사람들이 
손으로 쓴 수십만개의 숫자 이미지를 모아서

36
00:02:37,380 --> 00:02:44,280
각각 하나의 숫자로 표시(라벨링)해주었기 때문에
우리는 이 자료를 사용할 수 있습니다.

37
00:02:44,720 --> 00:02:47,960
신경망이 실제로 어떻게 작동하는지를 알게 되면

38
00:02:47,960 --> 00:02:52,600
"기계가 학습한다"는 말이 이상하다는걸 알게 될겁니다.

39
00:02:52,600 --> 00:02:55,960
기계학습은 SF소설의 이상한 설정보단 사실 미적분 예제에 더 가깝습니다.

40
00:02:55,960 --> 00:03:00,640
제 말은, 기계학습은 근본적으로 특정한 함수의 최솟값을 찾는 일로 요약될 수 있습니다.

41
00:03:01,520 --> 00:03:07,880
기억하세요. 개념상 우리는 각 뉴런이 이전 층의 모든 뉴런과 연결되어 있고,

42
00:03:07,880 --> 00:03:11,380
각각의 활성화를 결정하는 가중 합계의 가중치는

43
00:03:11,380 --> 00:03:14,400
그 연결의 세기 같은 거라고 생각합시다.

44
00:03:14,400 --> 00:03:19,600
그리고 bias는 뉴런이 활동적인지 또는 비활동적인지를 나타내는 지표입니다.

45
00:03:19,600 --> 00:03:24,840
자, 우린 이제 가중치와 bias를 완전히 무작위로 
설정할 것입니다.

46
00:03:24,840 --> 00:03:28,880
끔찍하게 작동할거라는 건 말할 필요도 없겠네요

47
00:03:28,880 --> 00:03:30,800
무작위로 무언가를 하고 있으니 말이죠

48
00:03:30,800 --> 00:03:33,520
예를 들어 3이라는 이미지를 입력하면

49
00:03:33,520 --> 00:03:36,180
출력 층은 엉망으로 보입니다.

50
00:03:36,340 --> 00:03:39,600
그래서 당신이 할 일은 'cost(비용) 함수'를 컴퓨터에게 정의해주는 것입니다.

51
00:03:39,600 --> 00:03:41,220
이렇게 말이죠

52
00:03:41,220 --> 00:03:42,840
"그게 아니야! 바보같은 컴퓨터!"

53
00:03:42,840 --> 00:03:47,260
제대로 된 출력은 대부분의 뉴런이 0의 활성치를 가져야 합니다.

54
00:03:47,260 --> 00:03:48,880
이 하나의 뉴런만 빼고요.

55
00:03:48,880 --> 00:03:51,260
넌 나에게 쓰레기를 줬어

56
00:03:51,260 --> 00:03:54,740
여러분이 할 일을 조금 더 수학적으로 말하자면,

57
00:03:54,740 --> 00:04:01,420
잘못된 출력과 원하는 출력의 차의 제곱을 모두 더하는 것입니다.

58
00:04:01,480 --> 00:04:05,320
이것이 우리가 하나의 훈련 예제에서 cost(비용)라고 부르는 것입니다.

59
00:04:05,600 --> 00:04:11,000
신경망이 이미지를 올바르게 분류 할 때 이 합계가 작음을 기억하세요.

60
00:04:12,200 --> 00:04:16,500
하지만 신경망이 자기가 뭘 하고 있는지 모를 때는 커집니다.

61
00:04:18,320 --> 00:04:25,580
그렇다면 당신이 할 일은, 수만 가지의 학습 예시 전체에 대한 평균 cost를 검토하는 것입니다.

62
00:04:27,060 --> 00:04:30,840
이 평균 cost는 신경망이 얼마나 엉망인지를 측정하는 수단이고,

63
00:04:30,840 --> 00:04:33,200
컴퓨터가 스스로 뭔가 잘못됐다는걸 느끼게 해주는

64
00:04:33,200 --> 00:04:34,900
복잡한 함수입니다.

65
00:04:34,900 --> 00:04:38,020
신경망 자체가 기본적으로 함수임을 기억하세요.

66
00:04:38,020 --> 00:04:45,200
784개의 픽셀 값을 입력받아서 10개의 숫자를 출력하는 함수요.

67
00:04:45,200 --> 00:04:49,200
이것은 이 모든 가중치와 bias에 의해 매개변수화 되어있습니다.

68
00:04:49,200 --> 00:04:53,180
cost 함수는 문제를 한 층 더 복잡하게 만드는데,

69
00:04:53,180 --> 00:04:57,480
13,000여개의 가중치와 bias를 입력받아서

70
00:04:57,480 --> 00:05:02,480
이 가중치와 bias가 적절한지 또는 그렇지 않은지를 나타내는 단 하나의 숫자를 출력하고,

71
00:05:02,480 --> 00:05:09,120
이것은 수만 개의 훈련 데이터에 대한 신경망의 행동에 따라 결정됩니다.

72
00:05:09,400 --> 00:05:11,140
생각해볼 만한 점이 많습니다.

73
00:05:12,000 --> 00:05:15,940
그러나 단지 컴퓨터에 어떤 진절머리 나는 직업이 있다고 말하는 것은 별로 도움이되지 않습니다.

74
00:05:15,940 --> 00:05:20,440
그보단 가중치와 bias를 어떻게 바꿔야 더 나아질지 알려주는 편이 좋겠지요.

75
00:05:21,100 --> 00:05:25,200
쉽게 생각해 봅시다. 입력값이 13,000개나 되는 함수가 아니라

76
00:05:25,200 --> 00:05:30,680
입력값 하나에 출력값 하나인 함수를 상상해보세요.

77
00:05:30,960 --> 00:05:35,440
이 함수의 출력값을 최소화하는 입력값을 어떻게 찾을 수 있을까요?

78
00:05:36,260 --> 00:05:40,340
미적분을 배운 학생들은 때로는 그 최솟값을 정확히 알아낼 수 있음을 알고 있을겁니다.

79
00:05:40,340 --> 00:05:44,240
하지만 아주 복잡한 함수에선 항상 최솟값을 알아 낼 순 없죠.

80
00:05:44,240 --> 00:05:51,560
입력값을 13,000개나 가진 신경망 cost 함수는 확실히 못 알아내겠네요.

81
00:05:51,560 --> 00:05:53,120
좀 더 유연한 전략은,

82
00:05:53,120 --> 00:05:59,760
한 입력값이 주어졌을 때, 어떤 방향으로 이동해야 출력값을 낮출 수 있을지를 파악하는 겁니다.

83
00:06:00,100 --> 00:06:04,120
특히 지금 위치에서 함수의 기울기를 파악할 수 있다면

84
00:06:04,120 --> 00:06:10,120
기울기가 양수면 왼쪽으로 이동하고 기울기가 음수면 오른쪽으로 이동하면 됩니다.

85
00:06:12,500 --> 00:06:16,900
계속 기울기를 확인하며 적절한 방향으로 이동한다면

86
00:06:16,900 --> 00:06:20,120
당신은 함수의 지역 최소값(local minimum)에 접근할 것입니다.

87
00:06:20,280 --> 00:06:24,080
언덕에서 굴러떨어지는 공을 떠올려보면 됩니다.

88
00:06:24,400 --> 00:06:31,040
이 단순화 된 단일 입력 함수에 대해서도 도착할 수 있는 많은 골짜기가 있습니다.

89
00:06:31,260 --> 00:06:34,700
출발지점을 어디로 했느냐에 따라서 각기 다른 골짜기에 도착하게 되지만,

90
00:06:34,700 --> 00:06:39,360
당신이 도착한 골짜기가 이 cost 함수에서 가장 작은 출력값이라는 보장은 없습니다.

91
00:06:39,980 --> 00:06:42,540
이건 우리의 신경망에도 똑같이 적용될 겁니다.

92
00:06:42,980 --> 00:06:44,620
또 여러분에게 알려드릴 게 있습니다.

93
00:06:44,620 --> 00:06:47,780
만약 당신이 한번에 이동할 거리를 기울기에 비례해서 결정한다면

94
00:06:47,780 --> 00:06:54,820
기울기가 줄어들수록 한번에 이동하는 거리가 점점 작아지고 이는 오버 슈팅을 방지하는 데 도움이됩니다.

95
00:06:55,720 --> 00:07:01,120
조금 더 복잡하게, 이번엔 두 개의 입력값에 하나의 출력값을 갖는 함수를 상상해 보세요.

96
00:07:01,120 --> 00:07:07,980
입력 공간을 xy 평면으로 생각할 수 있으며 cost 함수 그래프는 그 평면 위에 떠있는 곡면으로 나타납니다.

97
00:07:08,220 --> 00:07:15,380
이제 함수의 기울기 대신에 이 입력 공간에서 어느 방향으로 움직일지를 결정해야 합니다.

98
00:07:15,380 --> 00:07:19,640
함수의 출력을 가장 빨리 줄일 수 있는 방향이요.

99
00:07:19,640 --> 00:07:22,040
쉽게 말해 내리막은 어떤 방향일까요?

100
00:07:22,280 --> 00:07:25,820
다시 언덕을 굴러 내려가는 공을 떠올리는게 도움이 됩니다.

101
00:07:26,260 --> 00:07:34,520
다변수 미적분에 익숙한 사람들은 함수의 그래디언트가 가장 가파른 상승 방향을 알려줌을 압니다.

102
00:07:34,740 --> 00:07:39,180
근본적으로 함숫값을 가장 빠르게 늘려줄 방향이 어디인지를 알려줍니다.

103
00:07:39,180 --> 00:07:46,440
그렇다면 자연스럽게 그래디언트의 음의 방향이 가장 빠르게 함숫값을 낮추는 방향임을 알 수 있습니다.

104
00:07:47,020 --> 00:07:53,880
게다가 이 그래디언트 벡터의 길이는 가장 가파른 경사가 얼마나 가파른지에 대한 지표이기도 합니다.

105
00:07:54,420 --> 00:07:57,620
당신이 다변수 미적분학에 미숙해서 좀 더 배우고 싶다면,

106
00:07:57,620 --> 00:08:00,480
제가 Khan Academy를 위해 만든 영상 몇가지를 시청해보세요.

107
00:08:00,900 --> 00:08:03,760
솔직히 뭐가 어찌됐든, 지금 당신과 나에게 중요한 것은

108
00:08:03,780 --> 00:08:08,240
원론적으로 이 벡터를 컴퓨터로 계산하는 방법이 존재하는가 입니다.

109
00:08:08,240 --> 00:08:12,340
이 벡터는 내리막이 어떤 방향이고 얼마나 가파른지를 알려줄겁니다.

110
00:08:12,340 --> 00:08:16,340
이외의 세부적인 내용에 대해서는 자세히 몰라도 괜찮습니다.

111
00:08:16,780 --> 00:08:18,780
왜냐면 그래디언트를 계산해낼 줄 만 안다면

112
00:08:18,780 --> 00:08:23,000
알고리즘이 내리막 방향과 한 번에 이동할 거리를 알아낼거고

113
00:08:23,000 --> 00:08:26,900
이 과정을 계속 반복하기만 하면 함숫값을 줄일 수 있을테니까요.

114
00:08:27,800 --> 00:08:32,940
이건 13,000개의 입력을 가진 함수까지 확장해서 적용할 수 있는 기본적인 아이디어입니다.

115
00:08:33,300 --> 00:08:39,400
우리 신경망의 13,000여개의 가중치와 bias를 거대한 열 벡터로 조직했다고 생각해보세요.

116
00:08:39,680 --> 00:08:43,870
cost 함수의 그래디언트의 음의 방향은 그냥 벡터일 뿐입니다.

117
00:08:43,880 --> 00:08:48,480
이 벡터는 이 광활한 입력 공간에서

118
00:08:48,480 --> 00:08:55,020
어떤 방향이 cost 함수를 가장 빠르게 감소시켜 주는지를 알려줍니다.

119
00:08:55,460 --> 00:08:58,480
물론 우리의 특별 제작된 cost 함수는

120
00:08:58,480 --> 00:09:01,540
가중치와 bias들을 cost 함수가 줄어드는 방향으로 조정할 것이고

121
00:09:01,540 --> 00:09:05,200
그 뜻은 우리의 신경망이 훈련 데이터를 근거로 해서

122
00:09:05,200 --> 00:09:10,880
무작위로 10개의 숫자를 출력하지 않고, 우리가 원하는 정확한 결과를 출력해준다는 뜻입니다.

123
00:09:11,020 --> 00:09:16,400
기억하세요. 이 cost 함수는 모든 훈련 데이터의 평균 cost를 수반합니다.

124
00:09:16,400 --> 00:09:21,000
따라서 cost 함수를 최소화한다는 것은 이 모든 샘플들에서 더 나은 성능을 보여준다는 것입니다.

125
00:09:23,780 --> 00:09:27,080
이 그래디언트 계산을 효과적으로 만드는 알고리즘이

126
00:09:27,080 --> 00:09:30,000
신경망이 얼마나 효과적으로 배울 수 있을 지를 결정하는 핵심적인 요소입니다.

127
00:09:30,000 --> 00:09:32,340
그리고 그 알고리즘을 "back propagation"(오차역전파법)이라고 부릅니다.

128
00:09:32,340 --> 00:09:34,240
이게 바로 다음 영상에서 다룰 내용이고요.

129
00:09:34,400 --> 00:09:36,880
이제 많은 시간을 들여서 설명 드리고 싶은 내용들입니다.

130
00:09:36,880 --> 00:09:41,640
주어진 훈련 데이터마다 각각의 가중치와 bias들에는 정확히 어떤 일이 일어나는 걸까요?

131
00:09:41,800 --> 00:09:47,260
최대한 직관적으로 체험시켜드리기 위해, 관련된 미적분이나 수식들은 제껴놓겠습니다.

132
00:09:47,780 --> 00:09:48,800
지금 가장 중요한 것은,

133
00:09:48,820 --> 00:09:52,500
"구현 세부사항의 독립성"(Independent of Implementation Details)이 무엇인지를 알아야 한다는 겁니다.

134
00:09:52,500 --> 00:09:58,480
이게 무슨 뜻이냐면, 신경망 학습은 그저 cost 함수를 최소화하는 것 뿐이라는 겁니다.

135
00:09:58,940 --> 00:10:04,479
그로부터 우리는 cost 함수가 매끄러운 출력을 갖는 것이 중요하다는 결론을 얻을 수 있습니다.

136
00:10:04,480 --> 00:10:08,460
그래야지 우리가 내리막을 한발짝씩 내려가면서 지역 최솟값을 찾을 수 있을테니까요.

137
00:10:08,800 --> 00:10:10,520
이런 이유로

138
00:10:10,520 --> 00:10:16,880
인공 뉴런들은 단순히 활성화/비활성화로 결정되지 않고 연속적인 활성화 값을 갖습니다.

139
00:10:16,880 --> 00:10:18,880
실제 생물의 뉴런처럼 말이죠.

140
00:10:19,940 --> 00:10:25,020
이렇게 반복적으로 그래디언트의 음의 방향으로 함수의 입력값을 이동하는 방식을

141
00:10:25,020 --> 00:10:26,920
"경사 하강법"(Gradient Descent)이라고 부릅니다.

142
00:10:26,920 --> 00:10:32,660
지금 보이는 그래프의 골짜기, 즉, cost 함수의 지역 최소값으로  수렴하는 방법입니다.

143
00:10:32,920 --> 00:10:36,040
저는 아직 두 개의 입력을 가진 함수만 보여드렸습니다.

144
00:10:36,040 --> 00:10:41,080
왜냐면,  13,002차원 입력 공간에서 굴러다니는 공을 이해시켜드리긴 좀 어려우니까요.

145
00:10:41,080 --> 00:10:44,740
하지만 이 상황을 비(非)공간적으로 생각할 수 있는 좋은 방법이 있습니다.

146
00:10:44,740 --> 00:10:49,140
음의 그래디언트의 각각의 요소들은 우리에게 두 가지를 알려줍니다.

147
00:10:49,140 --> 00:10:55,380
그래디언트의 부호는 상응하는 입력 벡터의 요소가 커져야 할지 작아져야 할지를 알려줍니다.

148
00:10:55,380 --> 00:10:59,900
하지만 더 중요한 것은 이 요소들의 상대적인 크기입니다.

149
00:10:59,900 --> 00:11:03,720
이 크기는 어떤 요소를 조정하는 것이 더 큰 영향을 미칠지를 알려줍니다.

150
00:11:05,140 --> 00:11:08,560
어떤 한 가중치를 조정하는 것이

151
00:11:08,560 --> 00:11:13,140
다른 가중치를 조정하는 것보다 cost 함수에 더 큰 영향을 미칠 수 있습니다.

152
00:11:14,440 --> 00:11:18,340
이런 몇몇 연결선들은 훈련 데이터에 더 민감하게 반응합니다.

153
00:11:18,920 --> 00:11:24,180
따라서 이 무지막지한 cost 함수의 그래디언트 벡터를

154
00:11:24,180 --> 00:11:28,400
각각의 가중치와 bias의 중요도를 표현하는 것이라고 생각할 수 있습니다.

155
00:11:28,400 --> 00:11:32,620
그러니까,  이것들 중에 몇몇은 조정하면 대박 치는 것들이라는 겁니다.

156
00:11:33,560 --> 00:11:36,940
이것은 방향이라는 개념을 다르게 이해해보는 방법입니다.

157
00:11:36,940 --> 00:11:41,520
쉬운 예시로는, 입력 변수가 두 개인 함수가

158
00:11:41,520 --> 00:11:47,460
특정한 점에서의 그래디언트가 (3,1)임을 계산해냈다는 것은,

159
00:11:47,460 --> 00:11:51,460
그 점에서 함숫값을 가장 빠르게 증가시키는 방향이 어디인지도 안다는 말입니다.

160
00:11:51,460 --> 00:11:55,280
그 점에서 함숫값을 가장 빠르게 증가시키는 방향이 어디인지도 안다는 말입니다.

161
00:11:55,460 --> 00:11:59,420
입력 평면 공간 위의 곡면에서는

162
00:11:59,420 --> 00:12:02,340
가장 가파른 오르막이 그 방향인지 안다는 말이구요.

163
00:12:02,600 --> 00:12:05,140
그 방향이라는 개념을 다르게 이해하는 방법은

164
00:12:05,140 --> 00:12:10,680
첫 번째 변수를 조정하는 것이 두 번째 변수를 조정하는 것보다 3배 중요하다고 이해하는 것입니다.

165
00:12:10,680 --> 00:12:13,620
그 점 근처에선 말이죠.

166
00:12:13,620 --> 00:12:16,880
x값을 조정하는게 대박 치는 방법이겠죠

167
00:12:19,680 --> 00:12:22,680
좋아요. 이제 우리가 어디까지 왔는지 잠깐 돌아봅시다.

168
00:12:22,680 --> 00:12:30,420
신경망은 784개의 입력을 받아서 가중치와 bias를 통해 10개의 출력을 내놓는 함수입니다.

169
00:12:30,420 --> 00:12:33,960
복잡함을 더하는 cost 함수는

170
00:12:33,960 --> 00:12:37,540
13,000여개의 가중치와 bias를 입력값으로 받아서

171
00:12:37,540 --> 00:12:41,920
신경망이 얼마나 엉망으로 작동하고 있는지를 나타내는 단 하나의 숫자를 내놓습니다.

172
00:12:42,180 --> 00:12:47,160
또 어려운 부분인 cost 함수의 그래디언트는

173
00:12:47,160 --> 00:12:53,660
이 가중치와 bias들을 어떻게 조정하는 것이 cost 함숫값을 가장 빠르게 줄여주는지 알려줍니다.

174
00:12:53,660 --> 00:12:58,720
다른 표현으론, 어떤 가중치를 조정하는 것이 더 큰 영향을 끼치느냐, 라는 것이죠.

175
00:13:02,540 --> 00:13:06,340
그렇다면, 당신이 처음 신경망을 무작위 가중치와 bias로 만들고

176
00:13:06,340 --> 00:13:09,520
경사 하강법으로 그것들을 여러번에 걸쳐 조정한다면

177
00:13:09,520 --> 00:13:13,400
이 신경망이 한 번도 보지 못한 이미지에 대해 실제로 얼마나 잘 작동할까요?

178
00:13:13,680 --> 00:13:18,140
앞서 16개의 뉴런으로 된 두 개의 숨겨진 층을 쓰기로 사용한건

179
00:13:18,140 --> 00:13:20,640
그냥 보기 좋아서 그랬다고 설명했었죠.

180
00:13:20,640 --> 00:13:22,080
하지만, 썩 괜찮네요!

181
00:13:22,080 --> 00:13:26,100
새로운 이미지를 정확도 96%로 구별해내고 있어요.

182
00:13:26,720 --> 00:13:30,160
사실 신경망이 구별 못 한 이미지들을 보면

183
00:13:30,160 --> 00:13:33,320
솔직히 이정도 틀리는건 봐줄만 하다는 생각이 들겁니다.

184
00:13:35,760 --> 00:13:39,080
당신이 숨겨진 층을 좀 가지고 놀다보면

185
00:13:39,080 --> 00:13:43,000
정확도를 98%까지 끌어올릴 수 있습니다. 아주 좋아요!

186
00:13:43,000 --> 00:13:43,740
최고는 아니지만.

187
00:13:43,740 --> 00:13:48,540
이 평범한 신경망보다 더 정교하고 복잡한 신경망을 쓴다면 더 좋은 정확도를 얻을 수 있습니다.

188
00:13:48,560 --> 00:13:51,980
하지만 초기 작업이 아주 막막했던걸 감안하면

189
00:13:51,980 --> 00:13:57,420
어떤 신경망이든 전혀 보지 못했던 이미지에 대해서 이렇게 잘 작동한다는건 아주 놀랍다고 생각해요.

190
00:13:57,420 --> 00:14:01,340
어떤 패턴을 찾아야 하는지 구체적으로 알려주지도 않았는데 말이죠.

191
00:14:02,480 --> 00:14:07,320
원래 제가 이 구조에서 원했던 것은 설명 드렸던 것처럼

192
00:14:07,320 --> 00:14:09,840
두 번째 층이 숫자의 테두리들을 인식하고

193
00:14:09,840 --> 00:14:14,360
세 번째 층은 테두리들을 합쳐서 동그라미나 직선을 인식해서

194
00:14:14,360 --> 00:14:17,400
그것들을 합쳐 하나의 숫자를 인식하는 것이었습니다.

195
00:14:17,700 --> 00:14:20,940
우리의 신경망에서 실제로 이런 과정들이 일어나고 있을까요?

196
00:14:20,940 --> 00:14:23,280
음... 이런 경우엔

197
00:14:23,280 --> 00:14:24,440
절대 아니죠.

198
00:14:24,440 --> 00:14:27,180
이전 영상에서 본 내용을 기억하신다면

199
00:14:27,180 --> 00:14:32,020
첫 번째 층의 모든 뉴런과 두 번째 층의 한 뉴런 사이의 연결은

200
00:14:32,020 --> 00:14:37,140
지금 보시는 픽셀 패턴으로 시각화 될 수 있습니다. 두 번째 층의 모든 뉴런에 대해서요.

201
00:14:37,340 --> 00:14:43,300
실제로 가중치들을 이렇게 시각화 해보면

202
00:14:43,700 --> 00:14:46,980
독립된 테두리를 인식한다기 보단

203
00:14:46,980 --> 00:14:53,700
가운데 희미하게 보이는 패턴을 제외하곤 아무렇게나 생긴 것처럼 보입니다.

204
00:14:54,080 --> 00:14:59,880
우리의 신경망은 불가해하게 거대한 13,000차원 가중치와 bias 입력 공간에서

205
00:14:59,880 --> 00:15:02,900
자신만의 작고 소중한 지역 최소값을 찾았고

206
00:15:02,900 --> 00:15:08,940
그 덕에 우리가 원했던 방식으로 패턴을 인식하진 않지만 대부분의 이미지를 성공적으로 인식합니다.

207
00:15:09,420 --> 00:15:14,060
이게 어떤 상황인지를 정확히 이해하기 위해, 무작위 이미지를 입력하면 어떤 일이 일어나는지 한 번 봅시다.

208
00:15:14,060 --> 00:15:21,360
시스템이 아주 똑똑하다면, 아마 시스템이 불확실함을 느끼고 10개의 출력중 그 어떤 것도 활성화 시키지 않거나

209
00:15:21,360 --> 00:15:23,200
10개 모두를 고르게 활성화 시킬 것이라고 예상하실 겁니다.

210
00:15:23,200 --> 00:15:24,820
하지만 신경망은

211
00:15:24,820 --> 00:15:30,960
이 무작위 이미지가 5라고 아주 자신있게 대답합니다.

212
00:15:30,960 --> 00:15:34,000
실제 5의 이미지를 봤을 때 처럼요.

213
00:15:34,180 --> 00:15:38,440
그 뜻은 신경망이 숫자 이미지를 아주 잘 인식하더라도,

214
00:15:38,440 --> 00:15:40,980
숫자를 그릴줄은 모른다는 겁니다.

215
00:15:41,500 --> 00:15:45,340
이는 너무 엄격하게 제한된 훈련 환경 때문입니다.

216
00:15:45,340 --> 00:15:48,020
내 말은 당신이 신경망의 입장에서 생각해 본다면

217
00:15:48,020 --> 00:15:55,400
세상에는 격자 안에서 움직이지 않는 명백히 숫자라고 생각되는 것밖에 없고

218
00:15:55,400 --> 00:16:01,360
cost 함수는 오직 신경망이 자신의 결정에 확신을 가지게만 장려했습니다.

219
00:16:01,680 --> 00:16:05,220
그래서, 이 이미지가 두 번째 층이 실제로 하는 일이라면

220
00:16:05,220 --> 00:16:09,839
왜 제가 테두리와 패턴을 인식하는 신경망을 소개했는지 이상하다고 생각하실 겁니다.

221
00:16:09,840 --> 00:16:12,440
제 대답은, 이게 신경망이 하는 일의 전부가 아니라는 겁니다.

222
00:16:13,020 --> 00:16:17,540
그러니까, 이게 우리의 목표가 아니라 사실은 출발점에 불과합니다.

223
00:16:17,540 --> 00:16:19,340
까놓고 말해서, 신경망은 오래된 기술입니다.

224
00:16:19,340 --> 00:16:21,520
80~90년대에 연구됐던 기술이죠.

225
00:16:21,640 --> 00:16:26,320
여러가지 흥미로운 문제의 해결을 위해 만들어진 현대의 다양하고 상세해진 신경망을 이해하기 위해

226
00:16:26,320 --> 00:16:29,400
당신은 이 신경망부터 이해해야할 필요가 있습니다.

227
00:16:29,400 --> 00:16:32,740
하지만 현대의 신경망의 숨겨진 층이 하는 일을 들여다 보면,

228
00:16:32,740 --> 00:16:35,080
'생각'이라는 과정과는 더욱 동떨어져 있다는걸 알게 될겁니다.

229
00:16:38,520 --> 00:16:42,660
잠시 신경망이 학습하는 방법이 아니라 당신이 학습하는 방법에 대해서 생각해봅시다.

230
00:16:42,660 --> 00:16:46,320
어찌됐든 당신이 이 영상을 보고 능동적으로 사유할 때  학습이 이루어집니다.

231
00:16:46,660 --> 00:16:49,600
아주 간단한 부탁을 드리고 싶습니다.

232
00:16:49,600 --> 00:16:52,660
지금 영상을 잠깐 멈추고 한 번 깊게 생각해 보세요.

233
00:16:52,660 --> 00:16:55,440
당신은 이 시스템을 어떻게 바꿀 수 있을까요?

234
00:16:55,440 --> 00:17:01,060
그리고 당신의 시스템이 테두리나 패턴 따위를 인식하기를 바란다면, 시스템은 이미지를 어떻게 인식하게 될까요?

235
00:17:01,360 --> 00:17:05,100
하지만, 그보단 실제로 사용하고 있는 것들에 대해 공부하는 편이 좋을 것 같습니다.

236
00:17:05,100 --> 00:17:09,200
저는 Michael Nielsen이 딥러닝과 신경망에 대해 쓴 책들을 적극 추천드립니다.

237
00:17:09,200 --> 00:17:14,880
책에서는 영상에서 보신 예제에 대한 코드와 데이터를 제공하고 있습니다.

238
00:17:14,880 --> 00:17:18,600
그리고 그 코드가 어떤 일을 하는지 차근차근 가르쳐드릴 겁니다.

239
00:17:19,180 --> 00:17:22,560
게다가 이 책은 무료로 공개되어 있습니다. 멋지죠?

240
00:17:22,560 --> 00:17:27,800
그러니 이 책으로 도움을 받았다면 Nielsen의 노고에 후원해보세요.

241
00:17:28,200 --> 00:17:32,360
또 저는 제가 아주 좋아하는 몇 가지 다른 링크들도 달아놓았습니다.

242
00:17:32,360 --> 00:17:36,880
Chris Ola의 경이롭고 아름다운 포스팅이나 distill.pub의 게시물 같은 것들이요.

243
00:17:38,220 --> 00:17:40,200
마치며 남은 몇 분 동안

244
00:17:40,200 --> 00:17:43,960
지난번에 Lisha Li와 가졌던 인터뷰의 한 부분에 대해 이야기 해보고 싶습니다.

245
00:17:43,960 --> 00:17:48,220
이전 영상에 그녀가 나왔던걸 기억하시겠죠. 그녀는 딥러닝 연구로 박사학위를 받았습니다.

246
00:17:48,220 --> 00:17:55,880
그녀는 더 현대적인 이미지 인식 신경망이 실제로 어떻게 학습하는지 최근 깊게 분석한 두 논문에 대해 이야기해줬습니다.

247
00:17:55,880 --> 00:18:03,420
첫 번째 논문은 이미지 인식에 아주 탁월한 심층적인 신경망을 다뤘습니다.

248
00:18:03,420 --> 00:18:06,440
이 신경망은 제대로 라벨링된 데이터셋으로 훈련하는 대신

249
00:18:06,440 --> 00:18:08,980
훈련 전에 모든 레이블을 섞어놓았습니다.

250
00:18:08,980 --> 00:18:14,840
테스트 정확도는 그냥 무작위로 나올 것처럼 보입니다. 모두 무작위로 라벨링 돼있으니까요.

251
00:18:14,840 --> 00:18:21,080
하지만 제대로 라벨링된 데이터셋으로 학습한 것과 동일한 테스트 정확도를 달성했습니다.

252
00:18:21,480 --> 00:18:28,100
기본적으로 이 신경망에 있는 수백만개의 가중치들은 무작위 데이터를 기억하기에 충분합니다.

253
00:18:28,100 --> 00:18:29,820
그렇다면 이런 의문점이 생깁니다.

254
00:18:29,820 --> 00:18:34,380
cost 함수를 최소화 한다는 것은 이미지에 나타나는 구조를 분류하는 건가요?

255
00:18:34,380 --> 00:18:36,580
아니면 그냥 기억하는 것에 불과한 건가요?

256
00:18:36,580 --> 00:18:39,880
올바른 분류가 무엇인지에 대한 데이터셋을 기억하는 거예요.

257
00:18:39,880 --> 00:18:43,960
올해 ICML엔 반 년 동안

258
00:18:44,380 --> 00:18:48,920
이를 반박하는 논문이 없었어요. 이렇게요.

259
00:18:48,920 --> 00:18:52,300
"야! 신경망은 그것보다 더 똑똑해."

260
00:18:52,300 --> 00:18:59,380
이 정확도 곡선을 보면 만약 당신이 무작위 데이터셋으로 훈련을 시작했다면

261
00:18:59,380 --> 00:19:05,000
곡선이 거의 직선에 가깝게 아주 천천히 하강합니다.

262
00:19:05,000 --> 00:19:09,580
당신이 구조화된 데이터셋, 그러니까 제대로 라벨링된 데이터셋으로

263
00:19:09,589 --> 00:19:16,340
올바른 가중치를 찾아 높은 정확도를 얻기 위해 가능한 지역 최소값을 찾는다면

264
00:19:16,340 --> 00:19:23,020
처음엔 조금 헤매다가, 어느순간 좋은 정확도 수준으로 갑자기 뚝 떨어질 겁니다.

265
00:19:23,020 --> 00:19:28,400
따라서 어떤 면에선 저 지역 최대값을 찾기 더 쉽습니다.

266
00:19:28,400 --> 00:19:34,200
그리고 또 흥미로운 사실은 몇 년 지난 다른 논문을 보면

267
00:19:34,200 --> 00:19:35,920
층이 더 단순화된 신경망을 다룬 논문이요

268
00:19:35,920 --> 00:19:39,000
층이 더 단순화된 신경망을 다룬 논문이요

269
00:19:39,000 --> 00:19:41,940
그러나 결과는 이 최적화된 환경을 보면

270
00:19:41,940 --> 00:19:49,460
신경망이 배우려고 하는 지역 최소값들은 사실 동등한 가치를 갖는다는 것을 말해줍니다.

271
00:19:49,460 --> 00:19:54,660
그러므로 어떤 면에선 데이터셋이 구조화 돼있다면, 지역 최소값을 더 쉽게 찾을 수 있습니다.

272
00:19:58,140 --> 00:20:01,480
patreon으로 후원해주시는 모든 분들께 감사드립니다.

273
00:20:01,480 --> 00:20:06,940
patreon에 대해선 이미 말씀 드렸었지만, 이 영상들은 당신이 없었다면 만들수 없었을 겁니다.

