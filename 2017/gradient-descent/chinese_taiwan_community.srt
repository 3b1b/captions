1
00:00:04,070 --> 00:00:07,160
在上一節影片裡我講解了神經網路的結構

2
00:00:07,160 --> 00:00:10,089
首先我們來快速回顧一下

3
00:00:10,089 --> 00:00:15,650
在本節影片裡，我們有兩個目標
首介紹梯度下降的概念

4
00:00:15,650 --> 00:00:18,220
它不僅是神經網路工作的基礎

5
00:00:18,220 --> 00:00:20,660
也是很多其他機器學習方法的基礎

6
00:00:20,660 --> 00:00:24,609
然後我們會研究一下這個特別的網路是如何工作的

7
00:00:24,609 --> 00:00:27,999
以及這些隱藏的神經元層究竟在尋找什麽

8
00:00:28,999 --> 00:00:33,129
作為覆習
這裡我們引用一個經典的例子——手寫數字識別

9
00:00:34,129 --> 00:00:36,500
神經網路領域的"Hello World"

10
00:00:36,500 --> 00:00:43,610
這些數字書寫在28乘28像素的網格上
每個網格對應一個0到1之間的灰度值

11
00:00:43,610 --> 00:00:46,850
這些灰度值

12
00:00:46,850 --> 00:00:50,840
決定了神經網路輸入層的784個神經元的激活

13
00:00:50,840 --> 00:00:55,000
隨後每一層的各個神經元的激活值
都基於前一層的加權和

14
00:00:56,000 --> 00:01:00,699
與一個被叫做偏差的常數 相加 來獲得

15
00:01:01,699 --> 00:01:06,400
然後你把它和一些其他函數相加
比如sigmoid

16
00:01:06,400 --> 00:01:08,110
或者我上節影片提到的ReLu

17
00:01:09,110 --> 00:01:15,578
總之我們隨意給出兩個具有16個神經元的層
每一個神經網路有

18
00:01:16,578 --> 00:01:24,799
13000個可以調整的權重值和偏差
正是這些值決定了這個神經網路如何工作

19
00:01:24,799 --> 00:01:28,328
那麽“這個網路可以將給定數字分類”是什麽意思呢

20
00:01:28,328 --> 00:01:33,950
即最後一層10個數字中被點亮的那個數字就是輸入的數字

21
00:01:33,950 --> 00:01:38,780
請記住，我們使用這個分層的結構，目的是

22
00:01:38,780 --> 00:01:44,930
或許，第二層可以辨別出數字中的特徵線段
第三層或許可以辨別出組成數字的圈和線

23
00:01:44,930 --> 00:01:48,369
而最後一層可以把所有特徵結合在一起
從而辨別出這個輸入的數字

24
00:01:49,369 --> 00:01:52,399
因此，在這裡
我們將學習神經網路是如何學習的

25
00:01:52,399 --> 00:01:57,229
我們想要的是一種可以向這個神經網路展示大量訓練數據的算法

26
00:01:57,229 --> 00:02:03,890
這裡所說的大量訓練數據是指
很多手寫數字的圖像以及
標明了這個圖像上的數字到底是幾的標籤

27
00:02:03,890 --> 00:02:05,659
它能夠通過這些訓練數據

28
00:02:05,659 --> 00:02:09,729
來調整13000個權重值和偏差以達到
改善神經網路表現的目的

29
00:02:10,729 --> 00:02:13,269
我們所期望的是
這個分層的結構可以學習

30
00:02:14,269 --> 00:02:16,720
超出訓練數據範圍的圖像的識別

31
00:02:16,720 --> 00:02:20,289
我們測試的方法是
當你完成對這個網路的訓練後

32
00:02:20,289 --> 00:02:26,039
當你向它展示它從未見過的圖像時
觀察它判斷的精確度

33
00:02:31,039 --> 00:02:37,000
幸運的是 通常我們可以用來自MNIST base的數據來開始訓練

34
00:02:37,000 --> 00:02:44,720
MNIST base的好人們收集了數以萬計帶有標籤的手寫數字圖像

35
00:02:44,720 --> 00:02:49,539
當你一但真正了解它的工作原理，你會發現向機器解釋學習的過程非常有挑戰的一件事

36
00:02:49,539 --> 00:02:55,389
它並不像一些瘋狂的科幻
反倒是更像微積分練習

37
00:02:55,389 --> 00:02:59,519
也就是說
基本上是找到某一個特定函數的最小值

38
00:03:01,519 --> 00:03:05,389
請記住，從概念上講，
我們認為每一個神經元都與前一層的所有神經元相連

39
00:03:05,389 --> 00:03:12,440
加權求和計算中的加權值
在定義中像是一種

40
00:03:12,440 --> 00:03:14,060
神經元間連接強度的參考值

41
00:03:14,060 --> 00:03:20,440
而偏差值則代表了某個神經元是傾向於激活
還是不激活並關閉

42
00:03:20,440 --> 00:03:26,919
如果我們將所有的權重值和偏差值初始化為隨機數
毫無疑問，這個神經網路會表現地一塌糊塗

43
00:03:26,919 --> 00:03:33,759
用一個例子來說明
當你輸入一個3的圖像

44
00:03:33,759 --> 00:03:35,348
輸出層看起來一片混亂

45
00:03:36,348 --> 00:03:42,739
所以，你要做的是，定義一個成本函數
來告訴電腦，不！你是錯的！

46
00:03:42,739 --> 00:03:50,259
正確的輸出應該是，多數神經元激活值為0
但是對於這個神經元來說，你給我的是垃圾

47
00:03:51,259 --> 00:03:56,720
用數學語言來描述，

48
00:03:56,720 --> 00:04:01,489
就是你需要把每個【垃圾輸出】與【你想要的正確輸出】的【差的平方】相加

49
00:04:01,489 --> 00:04:04,598
這就是在單個訓練例子中的成本

50
00:04:05,598 --> 00:04:10,199
注意，如果網路能很正確地辨別出圖像
這個和會非常小

51
00:04:12,199 --> 00:04:15,329
但如果這個值很大
說明這個神經網路根本不知道它在幹嘛

52
00:04:18,329 --> 00:04:25,060
所以你要做的就是
考慮在你所能處理的上萬個訓練案例中的平均成本

53
00:04:27,060 --> 00:04:34,829
這個平均成本就是我們對該神經網路 表現好壞的衡量值

54
00:04:34,829 --> 00:04:38,540
記住這個神經網路本質上是一個函數

55
00:04:39,540 --> 00:04:45,889
它將784個像素值數字作為輸入
10個數字作為輸出

56
00:04:45,889 --> 00:04:48,139
從某種意義上來說
是通過這些權重和偏差來參數化

57
00:04:49,139 --> 00:04:54,449
然而成本函數的覆雜性表現在
最重要的是它將一萬三千左右的權重和偏差值作為輸入

58
00:04:54,449 --> 00:05:02,339
並輸出一個數字來反應這些權重和偏差質量的好壞

59
00:05:02,339 --> 00:05:08,149
它的定義，由神經網路經過上萬次訓練後的表現來決定

60
00:05:09,149 --> 00:05:11,000
這裡面有很多要思考的

61
00:05:12,000 --> 00:05:15,899
不過直接告訴電腦，它的工作有多爛
它一點幫助也沒有

62
00:05:15,899 --> 00:05:19,819
你想要知道的是，如何可以調整這些權重和偏差
從而讓它表現的好一點

63
00:05:20,819 --> 00:05:25,130
我們用一個簡單的例子來說明
（而不是費力思考一個有著13000個輸入輸出的函數）

64
00:05:25,130 --> 00:05:30,959
我們想象這樣一個簡單的函數
它只有一個輸入和一個輸出

65
00:05:30,959 --> 00:05:34,269
如何找到一個輸入值使函數值最小

66
00:05:36,269 --> 00:05:40,259
學過微積分的學生知道
有時你可以非常容易地指出一個函數的最小值

67
00:05:40,259 --> 00:05:43,310
但對於一些非常覆雜的函數來說，就不一定可行了

68
00:05:44,310 --> 00:05:52,350
當然包括我們那個超級複雜的有著13000個自變量的成本函數

69
00:05:52,350 --> 00:05:59,120
一個更靈活的辦法是，從任意一個輸入量開始，找出讓函數值變小的方向

70
00:06:00,120 --> 00:06:03,019
尤其是，如果你知道函數在某一點的斜率

71
00:06:04,019 --> 00:06:09,130
那麽，當斜率為正時，向左；當斜率為負時，向右
就可以找到函數輸出變小的方向

72
00:06:12,130 --> 00:06:16,800
如果你用合適的步驟不斷地重覆檢查每一點的斜率

73
00:06:16,800 --> 00:06:20,279
你就可以找到函式的局部最小值

74
00:06:20,279 --> 00:06:24,399
你可以在大腦裡想象這樣一幅圖
一個球向山下滾落

75
00:06:24,399 --> 00:06:30,540
值得注意的是，即使在這樣一個簡單的單一輸入函式中，依然有可能出現很多可以滾入的山谷

76
00:06:31,540 --> 00:06:36,579
從你隨機選取的輸入值開始，找到的局部最小值

77
00:06:36,579 --> 00:06:39,610
根本不能保證，它就是整個函數的最小值

78
00:06:39,610 --> 00:06:44,009
對於我們的神經網路的函式來說，也是一樣的情況

79
00:06:44,009 --> 00:06:47,620
另外需要注意的是，如果你的步長和斜率成比例

80
00:06:47,620 --> 00:06:54,720
那麽當越接近最小值時，你的步長就越小，這會幫助你避免找過頭

81
00:06:55,720 --> 00:07:00,120
擴展一下想象力，如果一個函數有兩個自變量和一個因變量

82
00:07:01,120 --> 00:07:07,230
你可以想象，輸入自變量空間是一個XY平面
而成本函數則是飄浮在上面的一個曲面

83
00:07:08,230 --> 00:07:15,310
現在，需要考慮的不是函數的斜率
而是在輸入空間的尋找前進方向

84
00:07:15,310 --> 00:07:22,440
換句話說，就是讓函數輸出減小得最快
下山的方向是什麽？

85
00:07:22,440 --> 00:07:25,259
同樣的，我們想像一個球向山下滾落

86
00:07:26,259 --> 00:07:34,750
熟悉多變量微積分的人會知道函數的梯度會給你最陡峭的上升方向

87
00:07:34,750 --> 00:07:38,100
也就等同於哪個方向是函數增加最快的方向

88
00:07:39,100 --> 00:07:46,019
很自然的，用負梯度就可以找到函數下降最快的方向

89
00:07:47,019 --> 00:07:53,130
而且，這個梯度向量的長度實際上是這個最陡斜坡有多陡的指標

90
00:07:54,130 --> 00:07:56,279
如果你並不熟悉多變量微積分

91
00:07:56,279 --> 00:08:00,910
並且想學習更多關於這方面的內容
你可以看一下可汗學院關於這一章節的內容

92
00:08:00,910 --> 00:08:03,779
事實上，對於我們來說，最重要的是

93
00:08:03,779 --> 00:08:09,519
原則上這個向量是可以計算出來的

94
00:08:09,519 --> 00:08:15,790
它會告訴你下山的方向以及會有多陡
知道這些知識就夠了，具體的細節並不重要

95
00:08:16,790 --> 00:08:24,740
因為如果你知道可以通過計算梯度方向來找到函數值變小的方向並向山下走出第一步

96
00:08:24,740 --> 00:08:26,800
那麽你就可以重覆這個過程

97
00:08:27,800 --> 00:08:34,330
這個原理在擁有13000個自變量的函式中同樣適用

98
00:08:35,330 --> 00:08:39,679
想像一下，把有13000個權重和偏差的神經網路放入一個超大的向量中

99
00:08:39,679 --> 00:08:43,880
成本函數的負梯度只是一個簡單的向量

100
00:08:43,880 --> 00:08:49,399
它是一個超級大的輸入變量空間中的一個方向

101
00:08:49,399 --> 00:08:55,460
告訴你哪個方向會讓成本函數最快地變小

102
00:08:55,460 --> 00:08:58,580
當然，對於我們專門設計的成本函數而言

103
00:08:58,580 --> 00:09:04,179
改變權重和偏差意味著
讓神經網路對每一組訓練數據的輸出

104
00:09:05,179 --> 00:09:10,029
看起來不像是十個數字中隨機的一個
而是實際上我們想讓它輸出的那一個

105
00:09:11,029 --> 00:09:16,370
要知道，這個成本函式是每一組訓練數據效果的平均

106
00:09:16,370 --> 00:09:20,779
所以如果你減小這個函數值
意味著改善了所以樣本的表現

107
00:09:23,779 --> 00:09:30,190
讓這個梯度計算更有效率的算法是神經網路學習的核心
它叫做傳播

108
00:09:31,190 --> 00:09:34,690
這是我下個影片重點要講的

109
00:09:34,690 --> 00:09:36,830
其中我非常想花時間講一講

110
00:09:36,830 --> 00:09:41,809
對於一組特定的訓練數據
每一個權重和偏差到底發生了什麽

111
00:09:41,809 --> 00:09:46,509
試圖給出除了相關微積分和公式以外的直觀地感受

112
00:09:47,509 --> 00:09:52,179
而現在，我想讓你知道的實現細節是

113
00:09:52,179 --> 00:09:58,940
當我們說“神經網路學習就是減小成本函式”到底是什麽意思

114
00:09:58,940 --> 00:10:04,480
注意它的結果，就是讓成本函式有一個平滑的輸出是非常重要的

115
00:10:04,480 --> 00:10:07,809
所以我們可以通過小的步長來找出局部最小值

116
00:10:08,809 --> 00:10:10,519
順便說一下，這就是為什麽

117
00:10:10,519 --> 00:10:16,750
人工神經元擁有連續激活行為
而不是如同自然神經元那樣的

118
00:10:16,750 --> 00:10:18,940
簡單的激活或不激活這樣的二元狀態

119
00:10:19,940 --> 00:10:26,929
這個反覆地將一個函數的輸入按照負梯度的倍數來輸入的過程被稱為梯度下降

120
00:10:26,929 --> 00:10:32,929
它是讓成本函數向局部最小值收斂的方法
也就是圖中沿山谷下降的過程

121
00:10:32,929 --> 00:10:38,889
這裡我依然用二維函數是因為，
如果用13000維函數，對於我們的大腦來說是很難想象的

122
00:10:38,889 --> 00:10:44,630
但是事實上還有一個非圖形的方法來思考這個問題

123
00:10:44,630 --> 00:10:51,830
梯度中的每個部分告訴我們兩個東西

124
00:10:51,830 --> 00:10:59,840
梯度的符號當然是告訴我們輸入向量的對應部分是向上還是向下，但重要的是，所有這些部分相關的幅度大小

125
00:10:59,840 --> 00:11:02,149
會告訴你哪些改變更重要

126
00:11:05,149 --> 00:11:09,710
你會發現，在神經網路中，一些權重值的改變對於成本函數來說影響很大

127
00:11:09,710 --> 00:11:12,450
而另一些權重值的改變對成本函數的影響則很小

128
00:11:14,450 --> 00:11:17,919
某些關係只與我們的訓練數據有關

129
00:11:18,919 --> 00:11:22,690
所以，你可以認為這個巨大的成本函數的梯度向量

130
00:11:22,690 --> 00:11:27,250
編碼每個權重和偏差的相對重要性

131
00:11:28,250 --> 00:11:32,559
那就是這些變化中的哪一個，將會為你帶來最大的影響

132
00:11:33,559 --> 00:11:36,860
這的確是思考方向問題的另一個途徑

133
00:11:36,860 --> 00:11:41,690
舉個簡單的例子，有一個二維函數

134
00:11:41,690 --> 00:11:46,419
你來計算它在(3,1)點的梯度

135
00:11:47,419 --> 00:11:51,070
那麽，一方面你可以把這個過程翻譯為
當你站在這一點時

136
00:11:52,070 --> 00:11:55,460
你可以沿梯度方向最快地增加

137
00:11:55,460 --> 00:12:02,600
在函數的圖像中，這個向量就是最快地直接上山的方向

138
00:12:02,600 --> 00:12:06,740
但另一方面，你也可以說

139
00:12:06,740 --> 00:12:13,519
第一個變量的變化對函數的影響三倍於第二個變量
至少在輸入量的鄰域內是這樣的

140
00:12:13,519 --> 00:12:16,309
改變x的值的影響大的多

141
00:12:19,309 --> 00:12:25,309
現在我們來總結一下，神經網路本身一是個具有784個輸入量和10個輸出量的函數

142
00:12:25,399 --> 00:12:29,350
定義為所有權重的和的形式

143
00:12:30,350 --> 00:12:34,120
成本函數反應複雜性

144
00:12:35,120 --> 00:12:41,179
它有13000個權重與偏差作為輸入，輸出一個基於訓練案例的好壞程度的值

145
00:12:42,179 --> 00:12:47,929
成本函數的梯度還有一層覆雜性

146
00:12:47,929 --> 00:12:53,970
它告訴我們如何改變所以這些權重值和偏差值可以讓成本函數最快的變小

147
00:12:53,970 --> 00:12:57,549
也可以翻譯為，哪些改變權重更大

148
00:13:02,549 --> 00:13:09,419
所以當你用隨機值來初始化權重和偏差值，並且基於梯度下降過程多次調整它們

149
00:13:09,419 --> 00:13:12,679
它會對一張從來沒有見過的圖像表現如何？

150
00:13:13,679 --> 00:13:19,578
我在這裡描述的是16個神經元的兩個隱藏層，至於每一層為什麽是16個...只是這個數字看著順眼罷了

151
00:13:20,578 --> 00:13:26,759
對於新圖像，它有96%的正確識別率，這已經很不錯了

152
00:13:26,759 --> 00:13:32,759
老實說，如果你看一些它搞砸的例子
你會覺得真的是讓人無奈

153
00:13:35,759 --> 00:13:39,078
如果你用隱藏的層結構並做一些調整

154
00:13:39,078 --> 00:13:43,740
你可以獲得98%的正確率
相對棒！

155
00:13:43,740 --> 00:13:48,568
你當然可以用更覆雜的網路來獲得比現在這個網路更好的表現

156
00:13:48,568 --> 00:13:52,889
考慮到一最初的任務有多艱巨，我真的覺得

157
00:13:52,889 --> 00:13:56,389
神經網路對於那些之前沒有見過的圖像
的表現好到讓人驚訝

158
00:13:57,389 --> 00:14:00,578
假如我們從來沒有專門告訴它我們是在尋找什麽樣式

159
00:14:02,578 --> 00:14:07,259
對於這個結構的目標，最初我們只是給出了一個期望

160
00:14:07,259 --> 00:14:09,808
第二層可能取出了小的片段組分

161
00:14:09,808 --> 00:14:17,698
第三層可能是將片段識別成圈和長點的線段，然後通過這些元素最終識別出了數字

162
00:14:17,698 --> 00:14:22,339
那麽，我們的網路真的是這樣工作的嗎？
至少對於當前這個例子

163
00:14:23,339 --> 00:14:24,448
根本不是！

164
00:14:24,448 --> 00:14:27,480
請記住在上一節影片中我們是如何看待：如何連接從第一層中的所有神經元

165
00:14:27,480 --> 00:14:31,980
到第二層中的給定神經元的的權重

166
00:14:31,980 --> 00:14:36,350
可以被可視化為該第二層神經元正在拾取的給定像素模式

167
00:14:37,350 --> 00:14:43,708
當我們真的計算與從前一層向後一層轉換相關的權重值

168
00:14:43,708 --> 00:14:50,370
而不是選取相互獨立的這兒一塊那兒一塊的小片段時
它看起來是完全隨機的

169
00:14:50,370 --> 00:14:56,919
只是一些非常鬆散的樣式

170
00:14:56,919 --> 00:15:02,860
對於大的不可思議的13000維空間的可能的權重和偏差
我們的網路發現自己是一個完美的局部最小值

171
00:15:02,860 --> 00:15:08,429
盡管可以將大多數圖像正確識別
但是並不是我們所希望的樣式

172
00:15:09,429 --> 00:15:13,019
當你真正運行它，觀察如果你輸入一個隨機圖像，它會作何反應

173
00:15:14,019 --> 00:15:21,578
如果系統足夠聰明，它會發現結果是不確定的，可能不會激活10個輸出神經元中的任何一個

174
00:15:21,578 --> 00:15:23,200
也可能會平均地激活它們

175
00:15:23,200 --> 00:15:24,820
然而

176
00:15:24,820 --> 00:15:32,009
它很自信地給你一個毫無意義的答案，仿佛它很肯定這個隨機信號就是5

177
00:15:32,009 --> 00:15:34,179
就好像它真的識別了一張寫著5的圖像一樣

178
00:15:34,179 --> 00:15:40,500
換句話說，不管它識別數字的正確率有多高，它還是不會寫出數字

179
00:15:41,500 --> 00:15:45,149
很大程度上是因為這是一個限制非常嚴格的訓練設置

180
00:15:45,149 --> 00:15:51,480
我的意思是說，如果你站在神經網路的角度，你會發現整個宇宙只有

181
00:15:51,480 --> 00:15:57,700
小網格中心不變的數字及其成本函數

182
00:15:57,700 --> 00:16:00,690
並且完全有信心做出自己的判斷

183
00:16:01,690 --> 00:16:05,139
所以，如果這個圖像就是第二層真的在做的事情

184
00:16:05,139 --> 00:16:09,839
你會很好奇，為什麽我會介紹神經網路可能提取一些片段和形狀

185
00:16:09,839 --> 00:16:11,028
也就是說，根本不是它最終要做的事情

186
00:16:13,028 --> 00:16:17,909
是的，這不意味著它是我們的最終目標，而是起點

187
00:16:17,909 --> 00:16:19,120
坦率的講，這是一個老的技術了

188
00:16:19,120 --> 00:16:21,639
是80年代和90年代研究的東西

189
00:16:21,639 --> 00:16:29,409
但在你理解當代的一些變體之前，你確實有必要先理解它
很顯然，它可以解決一些有趣的問題

190
00:16:29,409 --> 00:16:34,529
但是你越深挖隱蔽層到底在幹什麽，它的智能程度看起來就越低

191
00:16:38,529 --> 00:16:42,580
暫時轉移開關於神經網路如何學習與你是如何學習這一焦點

192
00:16:42,580 --> 00:16:46,659
只有當你非常積極地處理相關材料時才會發生

193
00:16:46,659 --> 00:16:53,440
建議你暫時停下來，深入思考一下

194
00:16:53,440 --> 00:16:55,230
你可能對這個系統做出怎樣的改變

195
00:16:55,230 --> 00:17:00,360
如果你想讓它更好的提取諸如線段、形狀之類的元素，
應該讓它怎麽感知圖像

196
00:17:01,360 --> 00:17:04,410
但真正更好的處理這樣材料的辦法是

197
00:17:04,410 --> 00:17:05,078
我

198
00:17:05,078 --> 00:17:08,190
強烈建議你去讀Michael Nielsen關於深度學習和神經網路的書

199
00:17:09,190 --> 00:17:14,410
在這本書裡面，你可以找到相關代碼和數據，下載並運行相關實例

200
00:17:14,410 --> 00:17:18,910
這本書會一步一步地給你介紹程式碼的含意

201
00:17:18,910 --> 00:17:21,359
最爽的是，這本書是免費並且公開發行的

202
00:17:22,359 --> 00:17:27,910
所以如果你真的從中得到了一些東西，可以考慮和我一起為Nielsen的努力捐款

203
00:17:27,910 --> 00:17:32,470
我也提供了一些我非常喜歡的資源連結

204
00:17:32,470 --> 00:17:36,230
包括Chris Ola的讓人震撼又非常漂亮的部落格和提交的文章

205
00:17:38,230 --> 00:17:40,200
最後幾分鐘時間

206
00:17:40,200 --> 00:17:43,930
我回到我和Leisha Lee的一段採訪

207
00:17:43,930 --> 00:17:49,079
你可能還記得在上節影片中的她
她在進行關於深度學習的博士研究工作

208
00:17:49,079 --> 00:17:55,809
在那個採訪片段中，她談論了最近的兩篇論文，
其中深入研究了當前圖像識別領域神經網路到底是如何工作這一問題

209
00:17:55,809 --> 00:18:01,349
第一篇論文就確定了我們的討論主題，它介紹了最為深入的神經網路之一

210
00:18:01,349 --> 00:18:05,910
它可以非常準確地識別圖像
但並非用了正確標識過的數據集

211
00:18:05,910 --> 00:18:08,799
而是用打亂了所有標籤的訓練數據

212
00:18:08,799 --> 00:18:14,799
很顯然，測試準確度不會比隨機結果好到哪去，因為標籤本身就是混亂的

213
00:18:14,799 --> 00:18:20,490
但是一但你使用了正確標記的數據集，依然可以達到相同的識別精度

214
00:18:21,490 --> 00:18:27,819
基本上，這個特別的神經網路中數以百萬計的權重值足以記住那些隨機數據

215
00:18:27,819 --> 00:18:34,380
最小化這個成本函數是否真的對應圖像中任意類型的結構
這一問題是怎樣提出來的？

216
00:18:34,380 --> 00:18:36,519
或許，只是，你知道的...

217
00:18:36,519 --> 00:18:37,420
記憶整個正確分類的數據集

218
00:18:37,420 --> 00:18:43,470
今年在ICML

219
00:18:44,470 --> 00:18:49,470
沒有反駁的論文，只有一些簡單提及的論文

220
00:18:49,470 --> 00:18:55,279
事實上這些神經網路做的更聰明一些
如果你看這個準確度曲線

221
00:18:55,279 --> 00:18:57,259
如果你只是訓練一個隨機數據集

222
00:18:58,259 --> 00:19:05,179
這個曲線會非常非常慢的下降，近似於線性

223
00:19:05,179 --> 00:19:09,589
所以你可能真的很難找到局部最小值

224
00:19:09,589 --> 00:19:15,288
只要你用正確標記過的結構化的數據集
正確的權重會讓你得到一定的準確度

225
00:19:15,288 --> 00:19:21,200
一開始你可能會反覆折騰，但是很快就會快速下降到這個準確程度

226
00:19:22,200 --> 00:19:26,759
所以，一定程度上來說，還是很容易找到局部最大值

227
00:19:26,759 --> 00:19:33,079
幾年前另外一篇論文也引起了人們的興趣

228
00:19:34,079 --> 00:19:36,990
它大大地簡化了

229
00:19:36,990 --> 00:19:39,169
神經網路層

230
00:19:39,169 --> 00:19:46,339
其中一個結論講的是，為何如果你觀察優化的情景，神經網路傾向學習的局部最小值，事實上效果是相同的

231
00:19:47,339 --> 00:19:54,138
所以從某種程度來講，如果你的數據集是結構化的，你應該會發現找到它是很容易的

232
00:19:58,138 --> 00:20:01,190
我一如既往地感謝那些支持Patreon的人

233
00:20:01,190 --> 00:20:06,230
我之前已經說過Patreon是一個怎樣的遊戲規則改變者，但是如果沒有你是不可能的做出這些影片的

234
00:20:07,230 --> 00:20:12,470
同時也要特別感謝VC公司的合作夥伴對這些系列影片的支持

