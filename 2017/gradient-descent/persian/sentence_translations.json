[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "آخرین ویدیو که من ساختار یک شبکه عصبی را ارائه کردم.",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "در اینجا خلاصه ای سریع می نویسم تا در ذهن ما تازه شود و سپس دو هدف اصلی برای این ویدیو دارم.",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "اولین مورد، معرفی ایده نزول گرادیان است، که نه تنها زیربنای نحوه یادگیری شبکه های عصبی، بلکه نحوه عملکرد بسیاری از یادگیری ماشینی دیگر نیز است.",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "سپس بعد از آن، کمی بیشتر در مورد نحوه عملکرد این شبکه خاص و اینکه آن لایه‌های پنهان نورون‌ها در نهایت به دنبال چه چیزی هستند، خواهیم پرداخت.",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "به عنوان یادآوری، هدف ما در اینجا نمونه کلاسیک تشخیص رقم دست‌نویس، دنیای سلام شبکه‌های عصبی است.",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "این ارقام بر روی یک شبکه پیکسلی 28x28 ارائه می‌شوند که هر پیکسل دارای مقداری مقیاس خاکستری بین 0 و 1 است.",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "اینها هستند که تعیین کننده فعال شدن 784 نورون در لایه ورودی شبکه هستند.",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "و سپس فعال‌سازی برای هر نورون در لایه‌های زیر بر اساس مجموع وزنی تمام فعال‌سازی‌های لایه قبلی، به‌علاوه یک عدد خاص به نام بایاس است.",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "سپس آن مجموع را با یک تابع دیگر، مانند انقباض سیگموئید، یا یک relu، به روشی که من در آخرین ویدیو طی کردم، بنویسید.",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "در مجموع، با توجه به انتخاب تا حدودی دلخواه از دو لایه پنهان با 16 نورون، شبکه حدود 13000 وزن و بایاس دارد که می‌توانیم آنها را تنظیم کنیم، و این مقادیر هستند که مشخص می‌کنند شبکه واقعاً چه کاری انجام می‌دهد.",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "وقتی می گوییم این شبکه یک رقم معین را طبقه بندی می کند، منظور ما این است که روشن ترین آن 10 نورون در لایه نهایی با آن رقم مطابقت دارد.",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "و به یاد داشته باشید، انگیزه ای که ما در اینجا برای ساختار لایه ای در نظر داشتیم این بود که شاید لایه دوم بتواند روی لبه ها بچسبد، و لایه سوم ممکن است الگوهایی مانند حلقه ها و خطوط را انتخاب کند، و آخرین لایه بتواند آن ها را کنار هم قرار دهد. الگوهایی برای تشخیص ارقام",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "بنابراین در اینجا، نحوه یادگیری شبکه را می آموزیم.",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "چیزی که ما می‌خواهیم الگوریتمی است که در آن می‌توانید مجموعه کاملی از داده‌های آموزشی را به این شبکه نشان دهید، که به شکل دسته‌ای از تصاویر مختلف از ارقام دست‌نویس، همراه با برچسب‌هایی برای آنچه که قرار است باشند، ارائه می‌شود. آن 13000 وزن و سوگیری را طوری تنظیم کنید که عملکرد آن در داده های تمرینی بهبود یابد.",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "امیدواریم این ساختار لایه ای به این معنی باشد که آنچه می آموزد به تصاویر فراتر از داده های آموزشی تعمیم می یابد.",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "روشی که ما آن را آزمایش می‌کنیم این است که پس از آموزش شبکه، داده‌های برچسب‌گذاری‌شده بیشتری را به آن نشان می‌دهید که قبلاً هرگز دیده نشده‌اند، و می‌بینید که چقدر دقیق آن تصاویر جدید را طبقه‌بندی می‌کند.",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "خوشبختانه برای ما، و چیزی که این را به یک مثال معمولی برای شروع تبدیل می‌کند، این است که افراد خوب پشت پایگاه داده MNIST مجموعه‌ای از ده‌ها هزار تصویر رقمی دست‌نویس را گردآوری کرده‌اند که هر کدام با اعدادی که قرار است برچسب‌گذاری شده باشند. بودن.",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "و به همان اندازه که توصیف یک ماشین به عنوان یادگیری تحریک‌کننده است، وقتی می‌بینید چگونه کار می‌کند، بسیار کمتر شبیه یک فرضیه علمی تخیلی دیوانه‌کننده است، و بیشتر شبیه یک تمرین حساب دیفرانسیل و انتگرال است.",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "منظورم این است که اساساً به یافتن حداقل یک تابع خاص برمی گردد.",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "به یاد داشته باشید، از نظر مفهومی، ما فکر می کنیم که هر نورون به تمام نورون های لایه قبلی متصل است، و وزن های موجود در مجموع وزنی که فعال شدن آن را مشخص می کند، به نوعی مانند نقاط قوت آن اتصالات است، و تعصب نشانه ای از آیا آن نورون تمایل به فعال یا غیر فعال دارد.",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "و برای شروع کار، ما فقط می‌خواهیم تمام آن وزن‌ها و سوگیری‌ها را کاملاً تصادفی مقداردهی کنیم.",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "نیازی به گفتن نیست که این شبکه در یک مثال آموزشی خاص عملکرد بسیار وحشتناکی خواهد داشت، زیرا فقط یک کار تصادفی انجام می دهد.",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "به عنوان مثال، شما در این تصویر از 3 تغذیه می کنید، و لایه خروجی فقط به نظر می رسد درهم و برهم است.",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "بنابراین کاری که شما انجام می دهید این است که یک تابع هزینه تعریف کنید، راهی برای گفتن به رایانه، نه، رایانه بد، که خروجی باید فعال سازی هایی داشته باشد که برای اکثر نورون ها 0 است، اما 1 برای این نورون، چیزی که به من دادید زباله مطلق است.",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "اگر بخواهیم کمی ریاضی تر بگوییم، مربع تفاوت بین هر یک از فعال سازی های خروجی سطل زباله و مقداری که می خواهید داشته باشند را جمع آوری کنید، و این همان چیزی است که ما آن را هزینه یک مثال آموزشی می نامیم.",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "توجه داشته باشید که این مجموع زمانی که شبکه با اطمینان تصویر را به درستی طبقه بندی می کند ناچیز است، اما زمانی که به نظر می رسد شبکه نمی داند چه کاری انجام می دهد، این مقدار زیاد است.",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "بنابراین کاری که شما انجام می دهید این است که میانگین هزینه را در بین ده ها هزار نمونه آموزشی در اختیار شما در نظر بگیرید.",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "این هزینه متوسط معیار ما برای اینکه شبکه چقدر ضعیف است و کامپیوتر چقدر باید احساس کند است.",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "و این یک چیز پیچیده است.",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "به یاد داشته باشید که چگونه خود شبکه اساساً یک تابع بود، تابعی که 784 عدد را به عنوان ورودی، مقادیر پیکسل را می گیرد و 10 عدد را به عنوان خروجی خود می ریزد، و به یک معنا با تمام این وزن ها و بایاس ها پارامتر می شود؟",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "خب تابع هزینه لایه‌ای از پیچیدگی است.",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "این 13000 یا بیشتر وزن و بایاس را به عنوان ورودی خود می گیرد، و یک عدد را بیان می کند که این وزن ها و سوگیری ها چقدر بد هستند، و نحوه تعریف آن به رفتار شبکه در تمام ده ها هزار قطعه داده آموزشی بستگی دارد.",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "این خیلی جای تامل دارد.",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "اما فقط گفتن اینکه کامپیوتر چه کارهای زشتی انجام می دهد خیلی مفید نیست.",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "شما می خواهید به او بگویید چگونه این وزن ها و سوگیری ها را تغییر دهد تا بهتر شود.",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "برای سهولت در تصور کردن تابعی با 13000 ورودی، کافیست یک تابع ساده را تصور کنید که یک عدد به عنوان ورودی و یک عدد به عنوان خروجی دارد.",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "چگونه ورودی ای پیدا می کنید که مقدار این تابع را به حداقل برساند؟",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "دانش‌آموزان حساب دیفرانسیل و انتگرال می‌دانند که شما گاهی اوقات می‌توانید این حداقل را به صراحت دریابید، اما این همیشه برای توابع واقعاً پیچیده امکان‌پذیر نیست، قطعاً در نسخه 13000 ورودی این وضعیت برای تابع هزینه پیچیده شبکه عصبی دیوانه‌وار ما امکان‌پذیر نیست.",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "یک تاکتیک منعطف تر این است که از هر ورودی شروع کنید و بفهمید که در کدام جهت باید آن خروجی را کاهش دهید.",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "به طور خاص، اگر می‌توانید شیب تابعی را که در آن قرار دارید، مشخص کنید، اگر شیب مثبت است، آن را به چپ و اگر شیب منفی است، ورودی را به سمت راست تغییر دهید.",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "اگر این کار را به طور مکرر انجام دهید، در هر نقطه شیب جدید را بررسی کنید و گام مناسب را بردارید، به حداقل محلی تابع نزدیک خواهید شد.",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "تصویری که ممکن است در اینجا در ذهن داشته باشید، توپی است که از تپه در حال غلتیدن است.",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "توجه داشته باشید، حتی برای این تابع ورودی منفرد واقعاً ساده شده، بسته به اینکه از کدام ورودی تصادفی شروع می‌کنید، دره‌های احتمالی زیادی وجود دارد که ممکن است در آنها فرود بیایید، و هیچ تضمینی وجود ندارد که حداقل محلی که وارد می‌شوید کوچک‌ترین مقدار ممکن باشد. تابع هزینه",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "این به پرونده شبکه عصبی ما نیز منتقل می شود.",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "همچنین می‌خواهم توجه داشته باشید که اگر اندازه‌های گام‌هایتان را با شیب متناسب کنید، وقتی شیب به سمت حداقل می‌رود، قدم‌هایتان کوچک‌تر و کوچک‌تر می‌شوند و این به شما کمک می‌کند که بیش از حد شیب نکنید.",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "با کمی افزایش پیچیدگی، در عوض یک تابع با دو ورودی و یک خروجی را تصور کنید.",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "ممکن است فضای ورودی را صفحه xy تصور کنید و تابع هزینه را به عنوان یک سطح بالای آن نمودار کنید.",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "به جای سوال در مورد شیب تابع، باید بپرسید که در چه جهتی باید در این فضای ورودی قدم بردارید تا خروجی تابع را سریعتر کاهش دهید.",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "به عبارت دیگر، جهت سراشیبی چیست؟",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "باز هم، فکر کردن به توپی که از آن تپه می غلتد مفید است.",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "کسانی از شما که با حساب دیفرانسیل و انتگرال چند متغیره آشنا هستند، می‌دانند که گرادیان یک تابع، جهت شیب‌دارترین صعود را به شما می‌دهد، که برای افزایش سریع‌ترین تابع باید در کدام جهت قدم بردارید.",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "به طور طبیعی، گرفتن نگاتیو آن گرادیان به شما جهت گام را می دهد که سریع ترین عملکرد را کاهش می دهد.",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "حتی بیشتر از آن، طول این بردار گرادیان نشان می دهد که تندترین شیب چقدر تند است.",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "اگر با حساب دیفرانسیل و انتگرال چند متغیره آشنا نیستید و می خواهید بیشتر بدانید، برخی از کارهایی را که برای آکادمی خان در این موضوع انجام دادم، بررسی کنید.",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "راستش را بخواهید، تنها چیزی که در حال حاضر برای من و شما مهم است این است که اصولا راهی برای محاسبه این بردار وجود دارد، این بردار که به شما می گوید جهت سراشیبی چیست و چقدر شیب دارد.",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "اگر این تنها چیزی است که می دانید و روی جزئیات دقیق نیستید، مشکلی ندارید.",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "اگر بتوانید آن را بدست آورید، الگوریتم کمینه کردن تابع این است که این جهت گرادیان را محاسبه کنید، سپس یک قدم کوچک به سمت پایین بردارید و آن را بارها و بارها تکرار کنید.",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "این همان ایده اولیه برای تابعی است که به جای 2 ورودی، 13000 ورودی دارد.",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "تصور کنید که تمام 13000 وزن و بایاس شبکه خود را در یک بردار ستون غول پیکر سازماندهی کنید.",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "گرادیان منفی تابع هزینه فقط یک بردار است، این یک جهت در داخل این فضای ورودی دیوانه‌وار عظیم است که به شما می‌گوید کدام ضربه به همه آن اعداد باعث سریع‌ترین کاهش در تابع هزینه می‌شود.",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "و البته، با تابع هزینه طراحی شده ویژه ما، تغییر وزن ها و بایاس ها برای کاهش آن به این معنی است که خروجی شبکه در هر قطعه از داده های آموزشی کمتر شبیه یک آرایه تصادفی از 10 مقدار باشد و بیشتر شبیه یک تصمیم واقعی باشد که می خواهیم. ساختن آن",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "مهم است که به یاد داشته باشید، این تابع هزینه شامل میانگینی از تمام داده های آموزشی است، بنابراین اگر آن را به حداقل برسانید، به این معنی است که عملکرد بهتری در تمام آن نمونه ها دارد.",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "الگوریتم محاسبه موثر این گرادیان، که در واقع قلب نحوه یادگیری یک شبکه عصبی است، پس انتشار نامیده می شود، و این چیزی است که در ویدیوی بعدی درباره آن صحبت خواهم کرد.",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "در آنجا، من واقعاً می‌خواهم وقت بگذارم تا در مورد آنچه دقیقاً برای هر وزنه و سوگیری برای یک قطعه داده تمرینی اتفاق می‌افتد، بگذرم، و سعی می‌کنم احساسی شهودی برای آنچه فراتر از انبوهی از محاسبات و فرمول‌های مربوطه اتفاق می‌افتد، بدهم.",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "در اینجا، در حال حاضر، اصلی‌ترین چیزی که می‌خواهم بدون جزئیات پیاده‌سازی بدانید، این است که وقتی در مورد یادگیری شبکه صحبت می‌کنیم، منظور ما این است که فقط یک تابع هزینه را به حداقل می‌رساند.",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "و توجه کنید، یکی از پیامدهای آن این است که برای این تابع هزینه مهم است که خروجی همواری داشته باشد، به طوری که ما بتوانیم با برداشتن گام های کوچک در سراشیبی، حداقل محلی را پیدا کنیم.",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "به همین دلیل است که، اتفاقاً، نورون‌های مصنوعی به‌جای فعال یا غیرفعال بودن به‌صورت دوتایی، مانند نورون‌های بیولوژیکی، فعالیت‌های دامنه‌ای پیوسته دارند.",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "این فرآیند تلنگر زدن مکرر ورودی یک تابع توسط چند مضرب گرادیان منفی را گرادیان نزول می نامند.",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "این راهی برای همگرایی به سمت حداقل محلی تابع هزینه است، که اساساً یک دره در این نمودار است.",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "البته هنوز هم تصویر یک تابع را با دو ورودی نشان می‌دهم، زیرا تلنگرها در یک فضای ورودی 13000 بعدی کمی سخت است که ذهن شما را درگیر کند، اما یک راه غیرمکانی خوب برای فکر کردن به این موضوع وجود دارد.",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "هر جزء از گرادیان منفی دو چیز را به ما می گوید.",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "علامت، البته، به ما می گوید که آیا جزء مربوط به بردار ورودی باید به سمت بالا یا پایین حرکت کند.",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "اما مهمتر از همه، بزرگی نسبی همه این اجزا به نوعی به شما می گوید که کدام تغییرات اهمیت بیشتری دارند.",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "ببینید، در شبکه ما، تعدیل یکی از وزن‌ها ممکن است تأثیر بسیار بیشتری بر تابع هزینه داشته باشد تا تعدیل با وزن دیگر.",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "برخی از این اتصالات فقط برای داده های آموزشی ما اهمیت بیشتری دارند.",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "بنابراین راهی که می‌توانید در مورد این بردار گرادیان تابع هزینه عظیم ما فکر کنید این است که اهمیت نسبی هر وزن و سوگیری را رمزگذاری می‌کند، یعنی اینکه کدام یک از این تغییرات بیشترین ضربه را برای شما به همراه خواهد داشت.",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "این واقعاً فقط یک راه دیگر برای تفکر در مورد جهت است.",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "برای مثال ساده‌تر، اگر تابعی با دو متغیر به‌عنوان ورودی داشته باشید، و محاسبه کنید که گرادیان آن در نقطه‌ای خاص به صورت 3،1 می‌آید، از یک سو می‌توانید آن را به این صورت تفسیر کنید که وقتی با ایستادن در آن ورودی، حرکت در امتداد این جهت، تابع را سریع‌تر افزایش می‌دهد، که وقتی تابع را در بالای صفحه نقاط ورودی نمودار می‌کنید، آن بردار همان چیزی است که جهت سربالایی مستقیم را به شما می‌دهد.",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "اما راه دیگری برای خواندن این است که بگوییم تغییرات این متغیر اول 3 برابر تغییرات متغیر دوم اهمیت دارد، که حداقل در همسایگی ورودی مربوطه، هل دادن مقدار x برای شما ضربه بسیار بیشتری به همراه دارد. دلار",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "بیایید بزرگنمایی کنیم و خلاصه کنیم که تا اینجای کار کجا بوده ایم.",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "خود شبکه این تابع با 784 ورودی و 10 خروجی است که بر حسب همه این مجموع وزنی تعریف شده است.",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "تابع هزینه یک لایه از پیچیدگی در بالای آن است.",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "این 13000 وزنه و سوگیری را به عنوان ورودی می گیرد و بر اساس نمونه های تمرینی، یک معیار تنبلی را نشان می دهد.",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "و گرادیان تابع هزینه همچنان یک لایه دیگر از پیچیدگی است.",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "به ما می‌گوید چه ضربه‌هایی به همه این وزن‌ها و سوگیری‌ها باعث سریع‌ترین تغییر در مقدار تابع هزینه می‌شود، که ممکن است به این صورت تعبیر کنید که تغییرات برای کدام وزن‌ها مهم‌تر است.",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "بنابراین، وقتی شبکه را با وزن‌ها و بایاس‌های تصادفی مقداردهی اولیه می‌کنید، و آن‌ها را چندین بار بر اساس این فرآیند نزول گرادیان تنظیم می‌کنید، واقعاً چقدر روی تصاویری که قبلاً دیده نشده‌اند، عملکرد خوبی دارد؟",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "لایه ای که من در اینجا توضیح دادم، با دو لایه پنهان از 16 نورون که هر کدام عمدتاً به دلایل زیبایی شناختی انتخاب شده اند، بد نیست و حدود 96٪ از تصاویر جدیدی را که به درستی می بیند طبقه بندی می کند.",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "و راستش را بخواهید، اگر به برخی از نمونه‌هایی که آن را خراب می‌کند نگاه کنید، احساس می‌کنید مجبور هستید کمی آن را کاهش دهید.",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "حالا اگر با ساختار لایه پنهان بازی کنید و چند ترفند انجام دهید، می توانید این را تا 98٪ دریافت کنید.",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "و این خیلی خوب است!",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "این بهترین نیست، مطمئناً می‌توانید با پیچیده‌تر شدن از این شبکه وانیلی ساده، عملکرد بهتری داشته باشید، اما با توجه به اینکه کار اولیه چقدر ترسناک است، فکر می‌کنم در مورد هر شبکه‌ای که این کار را به خوبی روی تصاویری که قبلاً هرگز دیده نشده است، چیزی باورنکردنی وجود دارد. ما هرگز به طور خاص به آن نگفتیم که به دنبال چه الگوهایی باشیم.",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "در اصل، روشی که من به این ساختار انگیزه دادم، توصیف امیدی بود که ممکن بود داشته باشیم، که لایه دوم ممکن است روی لبه‌های کوچکی قرار بگیرد، لایه سوم آن لبه‌ها را برای تشخیص حلقه‌ها و خطوط طولانی‌تر کنار هم قرار دهد، و اینکه ممکن است آن‌ها تکه تکه شوند. با هم برای تشخیص ارقام",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "پس آیا این همان کاری است که شبکه ما در واقع انجام می دهد؟",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "خوب، حداقل برای این یکی، اصلا.",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "به یاد داشته باشید که چگونه آخرین ویدیوی ما به این موضوع نگاه کردیم که چگونه وزن اتصالات از تمام نورون های لایه اول به یک نورون مشخص در لایه دوم را می توان به عنوان یک الگوی پیکسل معینی که نورون لایه دوم در حال برداشتن آن است تجسم کرد؟",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "خوب، وقتی واقعاً این کار را برای وزن‌های مرتبط با این انتقال‌ها انجام می‌دهیم، از لایه اول به لایه بعدی، به جای اینکه لبه‌های کوچک جدا شده را اینجا و آنجا انتخاب کنیم، تقریباً تصادفی به نظر می‌رسند، فقط با الگوهای بسیار شل در وسط اونجا",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "به نظر می‌رسد که در فضای غیرقابل درک 13000 بعدی وزن‌ها و سوگیری‌های ممکن، شبکه ما خود را یک حداقل محلی خوشحال می‌یابد که، علی‌رغم طبقه‌بندی موفقیت‌آمیز اکثر تصاویر، دقیقاً الگوهایی را که ممکن بود انتظارش را داشتیم، انتخاب کند.",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "و برای اینکه واقعاً این نقطه را به خانه هدایت کنید، ببینید وقتی یک تصویر تصادفی را وارد می کنید چه اتفاقی می افتد.",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "اگر سیستم هوشمند بود، ممکن است انتظار داشته باشید که احساس نامطمئنی داشته باشید، شاید واقعاً هیچ یک از آن 10 نورون خروجی را فعال نمی کند یا همه آنها را به طور مساوی فعال نمی کند، اما در عوض با اطمینان به شما پاسخ های بیهوده ای می دهد، گویی مطمئن است که این نویز تصادفی 5 است همانطور که یک تصویر واقعی از 5 یک 5 است.",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "با بیان متفاوت، حتی اگر این شبکه بتواند ارقام را به خوبی تشخیص دهد، هیچ ایده ای برای ترسیم آنها ندارد.",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "بسیاری از این به این دلیل است که این یک مجموعه آموزشی بسیار محدود است.",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "منظورم این است که اینجا خود را به جای شبکه قرار دهید.",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "از دیدگاه آن، کل جهان چیزی جز ارقام متحرک کاملاً مشخص که در یک شبکه کوچک متمرکز شده‌اند، تشکیل نمی‌شود، و تابع هزینه آن هرگز انگیزه‌ای به آن نداده است که چیزی جز اطمینان کامل در تصمیم‌هایش داشته باشد.",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "بنابراین با این تصویری از کاری که آن نورون های لایه دوم واقعاً انجام می دهند، ممکن است تعجب کنید که چرا من این شبکه را با انگیزه برداشتن لبه ها و الگوها معرفی می کنم.",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "منظورم این است که اصلاً این کاری نیست که در نهایت انجام شود.",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "خوب، این هدف نهایی ما نیست، بلکه یک نقطه شروع است.",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "صادقانه بگویم، این فناوری قدیمی است، نوعی که در دهه‌های 80 و 90 مورد تحقیق قرار گرفت، و قبل از اینکه بتوانید انواع مدرن با جزئیات بیشتری را درک کنید، باید آن را درک کنید، و به وضوح می‌تواند مشکلات جالبی را حل کند، اما هر چه بیشتر در مورد آن تحقیق کنید. این لایه‌های پنهان واقعاً کار می‌کنند، هرچه هوشمندتر به نظر می‌رسد.",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "تغییر تمرکز برای لحظه ای از نحوه یادگیری شبکه ها به نحوه یادگیری شما، این تنها در صورتی اتفاق می افتد که به نحوی فعالانه با مطالب اینجا درگیر شوید.",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "یک کار بسیار ساده که از شما می‌خواهم انجام دهید این است که همین الان مکث کنید و برای لحظه‌ای عمیقاً فکر کنید که اگر می‌خواهید چیزهایی مانند لبه‌ها و الگوها را بهتر ببیند، چه تغییراتی ممکن است در این سیستم ایجاد کنید و چگونه تصاویر را درک می‌کند.",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "اما بهتر از آن، برای درگیر شدن با مطالب، کتاب مایکل نیلسن در مورد یادگیری عمیق و شبکه های عصبی را به شدت توصیه می کنم.",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "در آن، می‌توانید کد و داده‌هایی را برای دانلود و بازی برای همین مثال پیدا کنید، و کتاب گام به گام شما را با آنچه که کد انجام می‌دهد، آشنا می‌کند.",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "نکته جالب این است که این کتاب رایگان و در دسترس عموم است، بنابراین اگر چیزی از آن به دست آوردید، به من بپیوندید تا به تلاش‌های نیلسن کمک مالی کنید.",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "من همچنین چند منبع دیگر را که بسیار دوستشان دارم را در توضیحات پیوند داده ام، از جمله پست فوق العاده و زیبای وبلاگ کریس اولا و مقالات در Distill.",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "برای اینکه در چند دقیقه آخر همه چیز را در اینجا ببندم، می‌خواهم به قسمتی از مصاحبه‌ای که با لیشا لی داشتم برگردم.",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "شاید او را از آخرین ویدیو به یاد بیاورید، او کار دکترای خود را در زمینه یادگیری عمیق انجام داد.",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "در این قطعه کوچک، او در مورد دو مقاله اخیر صحبت می کند که واقعاً به چگونگی یادگیری برخی از شبکه های مدرن تشخیص تصویر می پردازد.",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "فقط برای تنظیم جایی که در مکالمه بودیم، مقاله اول یکی از این شبکه‌های عصبی عمیق را انتخاب کرد که در تشخیص تصویر واقعاً خوب است، و به جای آموزش آن بر روی یک مجموعه داده با برچسب مناسب، همه برچسب‌ها را قبل از آموزش به هم ریخت.",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "بدیهی است که دقت آزمایش در اینجا بهتر از تصادفی نبود، زیرا همه چیز فقط به صورت تصادفی برچسب‌گذاری شده است، اما همچنان می‌توانست به همان دقت آموزشی که در مجموعه داده‌های دارای برچسب مناسب می‌رسید، دست یابد.",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "اساساً، میلیون‌ها وزن برای این شبکه خاص کافی بود تا فقط داده‌های تصادفی را به خاطر بسپارد، که این سوال را ایجاد می‌کند که آیا به حداقل رساندن این تابع هزینه واقعاً با هر نوع ساختاری در تصویر مطابقت دارد یا فقط به خاطر سپردن است؟",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "اگر به آن منحنی دقت نگاه کنید، اگر فقط روی یک مجموعه داده تصادفی تمرین می‌کردید، آن منحنی به آرامی به شکلی تقریباً خطی پایین می‌آید، بنابراین شما واقعاً در تلاش برای یافتن آن حداقل ممکن محلی هستید، می‌دانید ، وزنه های مناسبی که به شما این دقت را می دهد.",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "در حالی که اگر شما واقعاً روی یک مجموعه داده ساختاریافته آموزش می‌دهید، مجموعه‌ای که دارای برچسب‌های مناسب است، در ابتدا کمی کمانچه می‌چرخید، اما پس از آن خیلی سریع افت کرده‌اید تا به آن سطح دقت برسید، و به نوعی آن را یافتن حداکثر محلی آسان تر بود.",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "و بنابراین چیزی که در مورد آن جالب بود این است که مقاله دیگری را از چند سال پیش به نمایش می گذارد که در مورد لایه های شبکه ساده سازی های بسیار بیشتری دارد، اما یکی از نتایج این بود که چگونه اگر به چشم انداز بهینه سازی نگاه کنید، حداقل های محلی که این شبکه ها تمایل به یادگیری دارند در واقع از کیفیت یکسانی برخوردار هستند، بنابراین به نوعی اگر مجموعه داده شما ساختار یافته باشد، باید بتوانید آن را خیلی راحت تر پیدا کنید.",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "مثل همیشه از کسانی که از Patreon حمایت می کنند تشکر می کنم.",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "قبلاً گفته‌ام که Patreon چه تغییری در بازی دارد، اما این ویدیوها واقعاً بدون شما امکان‌پذیر نیستند.",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "همچنین می‌خواهم تشکر ویژه‌ای از شرکت VC Amplify Partners داشته باشم که از این ویدیوهای اولیه در این سری حمایت می‌کند.",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]