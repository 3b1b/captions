1
00:00:04,180 --> 00:00:07,280
آخرین ویدیو که من ساختار یک شبکه عصبی را ارائه کردم.

2
00:00:07,680 --> 00:00:10,353
در اینجا خلاصه ای سریع می نویسم تا در ذهن ما تازه 

3
00:00:10,353 --> 00:00:12,600
شود و سپس دو هدف اصلی برای این ویدیو دارم.

4
00:00:13,100 --> 00:00:16,928
اولین مورد، معرفی ایده نزول گرادیان است، که نه تنها زیربنای نحوه یادگیری 

5
00:00:16,928 --> 00:00:20,600
شبکه های عصبی، بلکه نحوه عملکرد بسیاری از یادگیری ماشینی دیگر نیز است.

6
00:00:21,120 --> 00:00:24,481
سپس بعد از آن، کمی بیشتر در مورد نحوه عملکرد این شبکه خاص و اینکه آن 

7
00:00:24,481 --> 00:00:27,940
لایه‌های پنهان نورون‌ها در نهایت به دنبال چه چیزی هستند، خواهیم پرداخت.

8
00:00:28,979 --> 00:00:32,524
به عنوان یادآوری، هدف ما در اینجا نمونه کلاسیک 

9
00:00:32,524 --> 00:00:36,220
تشخیص رقم دست‌نویس، دنیای سلام شبکه‌های عصبی است.

10
00:00:37,020 --> 00:00:40,189
این ارقام بر روی یک شبکه پیکسلی 28x28 ارائه می‌شوند 

11
00:00:40,189 --> 00:00:43,420
که هر پیکسل دارای مقداری مقیاس خاکستری بین 0 و 1 است.

12
00:00:43,820 --> 00:00:50,040
اینها هستند که تعیین کننده فعال شدن 784 نورون در لایه ورودی شبکه هستند.

13
00:00:51,180 --> 00:00:55,926
و سپس فعال‌سازی برای هر نورون در لایه‌های زیر بر اساس مجموع وزنی 

14
00:00:55,926 --> 00:01:00,820
تمام فعال‌سازی‌های لایه قبلی، به‌علاوه یک عدد خاص به نام بایاس است.

15
00:01:02,160 --> 00:01:05,490
سپس آن مجموع را با یک تابع دیگر، مانند انقباض سیگموئید، 

16
00:01:05,490 --> 00:01:08,940
یا یک relu، به روشی که من در آخرین ویدیو طی کردم، بنویسید.

17
00:01:09,480 --> 00:01:14,279
در مجموع، با توجه به انتخاب تا حدودی دلخواه از دو لایه پنهان با 16 

18
00:01:14,279 --> 00:01:19,079
نورون، شبکه حدود 13000 وزن و بایاس دارد که می‌توانیم آنها را تنظیم 

19
00:01:19,079 --> 00:01:24,380
کنیم، و این مقادیر هستند که مشخص می‌کنند شبکه واقعاً چه کاری انجام می‌دهد.

20
00:01:24,880 --> 00:01:28,964
وقتی می گوییم این شبکه یک رقم معین را طبقه بندی می کند، منظور ما 

21
00:01:28,964 --> 00:01:33,300
این است که روشن ترین آن 10 نورون در لایه نهایی با آن رقم مطابقت دارد.

22
00:01:34,100 --> 00:01:38,981
و به یاد داشته باشید، انگیزه ای که ما در اینجا برای ساختار لایه ای در نظر داشتیم این بود 

23
00:01:38,981 --> 00:01:43,753
که شاید لایه دوم بتواند روی لبه ها بچسبد، و لایه سوم ممکن است الگوهایی مانند حلقه ها و 

24
00:01:43,753 --> 00:01:47,428
خطوط را انتخاب کند، و آخرین لایه بتواند آن ها را کنار هم قرار دهد. 

25
00:01:47,428 --> 00:01:48,800
الگوهایی برای تشخیص ارقام

26
00:01:49,800 --> 00:01:52,240
بنابراین در اینجا، نحوه یادگیری شبکه را می آموزیم.

27
00:01:52,640 --> 00:01:57,066
چیزی که ما می‌خواهیم الگوریتمی است که در آن می‌توانید مجموعه کاملی از داده‌های 

28
00:01:57,066 --> 00:02:01,323
آموزشی را به این شبکه نشان دهید، که به شکل دسته‌ای از تصاویر مختلف از ارقام 

29
00:02:01,323 --> 00:02:05,413
دست‌نویس، همراه با برچسب‌هایی برای آنچه که قرار است باشند، ارائه می‌شود. 

30
00:02:05,413 --> 00:02:10,120
آن 13000 وزن و سوگیری را طوری تنظیم کنید که عملکرد آن در داده های تمرینی بهبود یابد.

31
00:02:10,720 --> 00:02:13,844
امیدواریم این ساختار لایه ای به این معنی باشد که آنچه می 

32
00:02:13,844 --> 00:02:16,860
آموزد به تصاویر فراتر از داده های آموزشی تعمیم می یابد.

33
00:02:17,640 --> 00:02:20,764
روشی که ما آن را آزمایش می‌کنیم این است که پس از آموزش شبکه، داده‌های 

34
00:02:20,764 --> 00:02:23,620
برچسب‌گذاری‌شده بیشتری را به آن نشان می‌دهید که قبلاً هرگز دیده 

35
00:02:23,620 --> 00:02:26,700
نشده‌اند، و می‌بینید که چقدر دقیق آن تصاویر جدید را طبقه‌بندی می‌کند.

36
00:02:31,120 --> 00:02:35,480
خوشبختانه برای ما، و چیزی که این را به یک مثال معمولی برای شروع تبدیل می‌کند، این 

37
00:02:35,480 --> 00:02:39,893
است که افراد خوب پشت پایگاه داده MNIST مجموعه‌ای از ده‌ها هزار تصویر رقمی دست‌نویس 

38
00:02:39,893 --> 00:02:44,200
را گردآوری کرده‌اند که هر کدام با اعدادی که قرار است برچسب‌گذاری شده باشند. بودن.

39
00:02:44,900 --> 00:02:48,443
و به همان اندازه که توصیف یک ماشین به عنوان یادگیری تحریک‌کننده است، 

40
00:02:48,443 --> 00:02:51,936
وقتی می‌بینید چگونه کار می‌کند، بسیار کمتر شبیه یک فرضیه علمی تخیلی 

41
00:02:51,936 --> 00:02:55,480
دیوانه‌کننده است، و بیشتر شبیه یک تمرین حساب دیفرانسیل و انتگرال است.

42
00:02:56,200 --> 00:02:59,960
منظورم این است که اساساً به یافتن حداقل یک تابع خاص برمی گردد.

43
00:03:01,939 --> 00:03:07,440
به یاد داشته باشید، از نظر مفهومی، ما فکر می کنیم که هر نورون به تمام نورون های لایه 

44
00:03:07,440 --> 00:03:12,876
قبلی متصل است، و وزن های موجود در مجموع وزنی که فعال شدن آن را مشخص می کند، به نوعی 

45
00:03:12,876 --> 00:03:18,636
مانند نقاط قوت آن اتصالات است، و تعصب نشانه ای از آیا آن نورون تمایل به فعال یا غیر فعال 

46
00:03:18,636 --> 00:03:18,960
دارد.

47
00:03:19,720 --> 00:03:22,212
و برای شروع کار، ما فقط می‌خواهیم تمام آن وزن‌ها 

48
00:03:22,212 --> 00:03:24,400
و سوگیری‌ها را کاملاً تصادفی مقداردهی کنیم.

49
00:03:24,940 --> 00:03:27,759
نیازی به گفتن نیست که این شبکه در یک مثال آموزشی خاص عملکرد 

50
00:03:27,759 --> 00:03:30,720
بسیار وحشتناکی خواهد داشت، زیرا فقط یک کار تصادفی انجام می دهد.

51
00:03:31,040 --> 00:03:33,353
به عنوان مثال، شما در این تصویر از 3 تغذیه می 

52
00:03:33,353 --> 00:03:36,020
کنید، و لایه خروجی فقط به نظر می رسد درهم و برهم است.

53
00:03:36,600 --> 00:03:41,162
بنابراین کاری که شما انجام می دهید این است که یک تابع هزینه تعریف کنید، راهی 

54
00:03:41,162 --> 00:03:45,842
برای گفتن به رایانه، نه، رایانه بد، که خروجی باید فعال سازی هایی داشته باشد که 

55
00:03:45,842 --> 00:03:50,760
برای اکثر نورون ها 0 است، اما 1 برای این نورون، چیزی که به من دادید زباله مطلق است.

56
00:03:51,720 --> 00:03:56,153
اگر بخواهیم کمی ریاضی تر بگوییم، مربع تفاوت بین هر یک از فعال سازی 

57
00:03:56,153 --> 00:04:00,520
های خروجی سطل زباله و مقداری که می خواهید داشته باشند را جمع آوری 

58
00:04:00,520 --> 00:04:05,020
کنید، و این همان چیزی است که ما آن را هزینه یک مثال آموزشی می نامیم.

59
00:04:05,960 --> 00:04:09,440
توجه داشته باشید که این مجموع زمانی که شبکه با اطمینان تصویر 

60
00:04:09,440 --> 00:04:12,862
را به درستی طبقه بندی می کند ناچیز است، اما زمانی که به نظر 

61
00:04:12,862 --> 00:04:16,399
می رسد شبکه نمی داند چه کاری انجام می دهد، این مقدار زیاد است.

62
00:04:18,640 --> 00:04:21,984
بنابراین کاری که شما انجام می دهید این است که میانگین هزینه 

63
00:04:21,984 --> 00:04:25,440
را در بین ده ها هزار نمونه آموزشی در اختیار شما در نظر بگیرید.

64
00:04:27,040 --> 00:04:32,740
این هزینه متوسط معیار ما برای اینکه شبکه چقدر ضعیف است و کامپیوتر چقدر باید احساس کند است.

65
00:04:33,420 --> 00:04:34,600
و این یک چیز پیچیده است.

66
00:04:35,040 --> 00:04:39,520
به یاد داشته باشید که چگونه خود شبکه اساساً یک تابع بود، تابعی که 784 

67
00:04:39,520 --> 00:04:43,872
عدد را به عنوان ورودی، مقادیر پیکسل را می گیرد و 10 عدد را به عنوان 

68
00:04:43,872 --> 00:04:48,800
خروجی خود می ریزد، و به یک معنا با تمام این وزن ها و بایاس ها پارامتر می شود؟

69
00:04:49,500 --> 00:04:52,820
خب تابع هزینه لایه‌ای از پیچیدگی است.

70
00:04:53,100 --> 00:04:58,289
این 13000 یا بیشتر وزن و بایاس را به عنوان ورودی خود می گیرد، و یک 

71
00:04:58,289 --> 00:05:03,478
عدد را بیان می کند که این وزن ها و سوگیری ها چقدر بد هستند، و نحوه 

72
00:05:03,478 --> 00:05:08,900
تعریف آن به رفتار شبکه در تمام ده ها هزار قطعه داده آموزشی بستگی دارد.

73
00:05:09,520 --> 00:05:11,000
این خیلی جای تامل دارد.

74
00:05:12,400 --> 00:05:15,820
اما فقط گفتن اینکه کامپیوتر چه کارهای زشتی انجام می دهد خیلی مفید نیست.

75
00:05:16,220 --> 00:05:20,060
شما می خواهید به او بگویید چگونه این وزن ها و سوگیری ها را تغییر دهد تا بهتر شود.

76
00:05:20,780 --> 00:05:25,593
برای سهولت در تصور کردن تابعی با 13000 ورودی، کافیست یک تابع ساده 

77
00:05:25,593 --> 00:05:30,480
را تصور کنید که یک عدد به عنوان ورودی و یک عدد به عنوان خروجی دارد.

78
00:05:31,480 --> 00:05:35,300
چگونه ورودی ای پیدا می کنید که مقدار این تابع را به حداقل برساند؟

79
00:05:36,460 --> 00:05:41,352
دانش‌آموزان حساب دیفرانسیل و انتگرال می‌دانند که شما گاهی اوقات می‌توانید این حداقل را 

80
00:05:41,352 --> 00:05:46,300
به صراحت دریابید، اما این همیشه برای توابع واقعاً پیچیده امکان‌پذیر نیست، قطعاً در نسخه 

81
00:05:46,300 --> 00:05:51,080
13000 ورودی این وضعیت برای تابع هزینه پیچیده شبکه عصبی دیوانه‌وار ما امکان‌پذیر نیست.

82
00:05:51,580 --> 00:05:55,426
یک تاکتیک منعطف تر این است که از هر ورودی شروع کنید 

83
00:05:55,426 --> 00:05:59,200
و بفهمید که در کدام جهت باید آن خروجی را کاهش دهید.

84
00:06:00,080 --> 00:06:04,893
به طور خاص، اگر می‌توانید شیب تابعی را که در آن قرار دارید، مشخص کنید، اگر 

85
00:06:04,893 --> 00:06:09,900
شیب مثبت است، آن را به چپ و اگر شیب منفی است، ورودی را به سمت راست تغییر دهید.

86
00:06:11,960 --> 00:06:16,019
اگر این کار را به طور مکرر انجام دهید، در هر نقطه شیب جدید را بررسی 

87
00:06:16,019 --> 00:06:19,840
کنید و گام مناسب را بردارید، به حداقل محلی تابع نزدیک خواهید شد.

88
00:06:20,640 --> 00:06:23,800
تصویری که ممکن است در اینجا در ذهن داشته باشید، توپی است که از تپه در حال غلتیدن است.

89
00:06:24,620 --> 00:06:29,382
توجه داشته باشید، حتی برای این تابع ورودی منفرد واقعاً ساده شده، بسته به اینکه از کدام 

90
00:06:29,382 --> 00:06:33,980
ورودی تصادفی شروع می‌کنید، دره‌های احتمالی زیادی وجود دارد که ممکن است در آنها فرود 

91
00:06:33,980 --> 00:06:38,852
بیایید، و هیچ تضمینی وجود ندارد که حداقل محلی که وارد می‌شوید کوچک‌ترین مقدار ممکن باشد. 

92
00:06:38,852 --> 00:06:39,400
تابع هزینه

93
00:06:40,220 --> 00:06:42,620
این به پرونده شبکه عصبی ما نیز منتقل می شود.

94
00:06:43,180 --> 00:06:46,910
همچنین می‌خواهم توجه داشته باشید که اگر اندازه‌های گام‌هایتان را 

95
00:06:46,910 --> 00:06:50,869
با شیب متناسب کنید، وقتی شیب به سمت حداقل می‌رود، قدم‌هایتان کوچک‌تر 

96
00:06:50,869 --> 00:06:54,600
و کوچک‌تر می‌شوند و این به شما کمک می‌کند که بیش از حد شیب نکنید.

97
00:06:55,940 --> 00:07:00,980
با کمی افزایش پیچیدگی، در عوض یک تابع با دو ورودی و یک خروجی را تصور کنید.

98
00:07:01,500 --> 00:07:04,890
ممکن است فضای ورودی را صفحه xy تصور کنید و تابع 

99
00:07:04,890 --> 00:07:08,140
هزینه را به عنوان یک سطح بالای آن نمودار کنید.

100
00:07:08,760 --> 00:07:13,737
به جای سوال در مورد شیب تابع، باید بپرسید که در چه جهتی باید 

101
00:07:13,737 --> 00:07:18,960
در این فضای ورودی قدم بردارید تا خروجی تابع را سریعتر کاهش دهید.

102
00:07:19,720 --> 00:07:21,760
به عبارت دیگر، جهت سراشیبی چیست؟

103
00:07:22,380 --> 00:07:25,560
باز هم، فکر کردن به توپی که از آن تپه می غلتد مفید است.

104
00:07:26,660 --> 00:07:30,845
کسانی از شما که با حساب دیفرانسیل و انتگرال چند متغیره آشنا هستند، 

105
00:07:30,845 --> 00:07:34,594
می‌دانند که گرادیان یک تابع، جهت شیب‌دارترین صعود را به شما 

106
00:07:34,594 --> 00:07:38,780
می‌دهد، که برای افزایش سریع‌ترین تابع باید در کدام جهت قدم بردارید.

107
00:07:39,560 --> 00:07:42,800
به طور طبیعی، گرفتن نگاتیو آن گرادیان به شما جهت 

108
00:07:42,800 --> 00:07:46,040
گام را می دهد که سریع ترین عملکرد را کاهش می دهد.

109
00:07:47,240 --> 00:07:53,840
حتی بیشتر از آن، طول این بردار گرادیان نشان می دهد که تندترین شیب چقدر تند است.

110
00:07:54,540 --> 00:07:57,271
اگر با حساب دیفرانسیل و انتگرال چند متغیره آشنا نیستید و می خواهید بیشتر 

111
00:07:57,271 --> 00:08:00,340
بدانید، برخی از کارهایی را که برای آکادمی خان در این موضوع انجام دادم، بررسی کنید.

112
00:08:00,860 --> 00:08:04,580
راستش را بخواهید، تنها چیزی که در حال حاضر برای من و شما مهم 

113
00:08:04,580 --> 00:08:08,179
است این است که اصولا راهی برای محاسبه این بردار وجود دارد، 

114
00:08:08,179 --> 00:08:11,900
این بردار که به شما می گوید جهت سراشیبی چیست و چقدر شیب دارد.

115
00:08:12,400 --> 00:08:16,120
اگر این تنها چیزی است که می دانید و روی جزئیات دقیق نیستید، مشکلی ندارید.

116
00:08:17,200 --> 00:08:22,026
اگر بتوانید آن را بدست آورید، الگوریتم کمینه کردن تابع این است که این جهت گرادیان را 

117
00:08:22,026 --> 00:08:26,740
محاسبه کنید، سپس یک قدم کوچک به سمت پایین بردارید و آن را بارها و بارها تکرار کنید.

118
00:08:27,700 --> 00:08:32,820
این همان ایده اولیه برای تابعی است که به جای 2 ورودی، 13000 ورودی دارد.

119
00:08:33,400 --> 00:08:39,460
تصور کنید که تمام 13000 وزن و بایاس شبکه خود را در یک بردار ستون غول پیکر سازماندهی کنید.

120
00:08:40,140 --> 00:08:45,107
گرادیان منفی تابع هزینه فقط یک بردار است، این یک جهت در داخل 

121
00:08:45,107 --> 00:08:49,830
این فضای ورودی دیوانه‌وار عظیم است که به شما می‌گوید کدام 

122
00:08:49,830 --> 00:08:54,880
ضربه به همه آن اعداد باعث سریع‌ترین کاهش در تابع هزینه می‌شود.

123
00:08:55,640 --> 00:09:00,700
و البته، با تابع هزینه طراحی شده ویژه ما، تغییر وزن ها و بایاس ها برای کاهش آن 

124
00:09:00,700 --> 00:09:05,760
به این معنی است که خروجی شبکه در هر قطعه از داده های آموزشی کمتر شبیه یک آرایه 

125
00:09:05,760 --> 00:09:10,820
تصادفی از 10 مقدار باشد و بیشتر شبیه یک تصمیم واقعی باشد که می خواهیم. ساختن آن

126
00:09:11,440 --> 00:09:14,633
مهم است که به یاد داشته باشید، این تابع هزینه شامل میانگینی 

127
00:09:14,633 --> 00:09:17,667
از تمام داده های آموزشی است، بنابراین اگر آن را به حداقل 

128
00:09:17,667 --> 00:09:21,180
برسانید، به این معنی است که عملکرد بهتری در تمام آن نمونه ها دارد.

129
00:09:23,820 --> 00:09:28,806
الگوریتم محاسبه موثر این گرادیان، که در واقع قلب نحوه یادگیری یک شبکه عصبی است، 

130
00:09:28,806 --> 00:09:33,980
پس انتشار نامیده می شود، و این چیزی است که در ویدیوی بعدی درباره آن صحبت خواهم کرد.

131
00:09:34,660 --> 00:09:38,771
در آنجا، من واقعاً می‌خواهم وقت بگذارم تا در مورد آنچه دقیقاً برای هر وزنه و 

132
00:09:38,771 --> 00:09:42,721
سوگیری برای یک قطعه داده تمرینی اتفاق می‌افتد، بگذرم، و سعی می‌کنم احساسی 

133
00:09:42,721 --> 00:09:47,100
شهودی برای آنچه فراتر از انبوهی از محاسبات و فرمول‌های مربوطه اتفاق می‌افتد، بدهم.

134
00:09:47,780 --> 00:09:51,214
در اینجا، در حال حاضر، اصلی‌ترین چیزی که می‌خواهم بدون جزئیات 

135
00:09:51,214 --> 00:09:54,593
پیاده‌سازی بدانید، این است که وقتی در مورد یادگیری شبکه صحبت 

136
00:09:54,593 --> 00:09:58,360
می‌کنیم، منظور ما این است که فقط یک تابع هزینه را به حداقل می‌رساند.

137
00:09:59,300 --> 00:10:02,071
و توجه کنید، یکی از پیامدهای آن این است که برای این تابع 

138
00:10:02,071 --> 00:10:04,793
هزینه مهم است که خروجی همواری داشته باشد، به طوری که ما 

139
00:10:04,793 --> 00:10:08,100
بتوانیم با برداشتن گام های کوچک در سراشیبی، حداقل محلی را پیدا کنیم.

140
00:10:09,260 --> 00:10:14,167
به همین دلیل است که، اتفاقاً، نورون‌های مصنوعی به‌جای فعال یا غیرفعال بودن 

141
00:10:14,167 --> 00:10:19,140
به‌صورت دوتایی، مانند نورون‌های بیولوژیکی، فعالیت‌های دامنه‌ای پیوسته دارند.

142
00:10:20,220 --> 00:10:23,703
این فرآیند تلنگر زدن مکرر ورودی یک تابع توسط چند 

143
00:10:23,703 --> 00:10:26,760
مضرب گرادیان منفی را گرادیان نزول می نامند.

144
00:10:27,300 --> 00:10:30,259
این راهی برای همگرایی به سمت حداقل محلی تابع هزینه 

145
00:10:30,259 --> 00:10:32,580
است، که اساساً یک دره در این نمودار است.

146
00:10:33,440 --> 00:10:36,824
البته هنوز هم تصویر یک تابع را با دو ورودی نشان می‌دهم، زیرا 

147
00:10:36,824 --> 00:10:40,264
تلنگرها در یک فضای ورودی 13000 بعدی کمی سخت است که ذهن شما را 

148
00:10:40,264 --> 00:10:44,260
درگیر کند، اما یک راه غیرمکانی خوب برای فکر کردن به این موضوع وجود دارد.

149
00:10:45,080 --> 00:10:48,440
هر جزء از گرادیان منفی دو چیز را به ما می گوید.

150
00:10:49,060 --> 00:10:52,132
علامت، البته، به ما می گوید که آیا جزء مربوط به 

151
00:10:52,132 --> 00:10:55,140
بردار ورودی باید به سمت بالا یا پایین حرکت کند.

152
00:10:55,800 --> 00:10:59,260
اما مهمتر از همه، بزرگی نسبی همه این اجزا به نوعی 

153
00:10:59,260 --> 00:11:02,720
به شما می گوید که کدام تغییرات اهمیت بیشتری دارند.

154
00:11:05,220 --> 00:11:08,992
ببینید، در شبکه ما، تعدیل یکی از وزن‌ها ممکن است تأثیر 

155
00:11:08,992 --> 00:11:13,040
بسیار بیشتری بر تابع هزینه داشته باشد تا تعدیل با وزن دیگر.

156
00:11:14,800 --> 00:11:18,200
برخی از این اتصالات فقط برای داده های آموزشی ما اهمیت بیشتری دارند.

157
00:11:19,320 --> 00:11:23,640
بنابراین راهی که می‌توانید در مورد این بردار گرادیان تابع هزینه عظیم ما 

158
00:11:23,640 --> 00:11:27,960
فکر کنید این است که اهمیت نسبی هر وزن و سوگیری را رمزگذاری می‌کند، یعنی 

159
00:11:27,960 --> 00:11:32,400
اینکه کدام یک از این تغییرات بیشترین ضربه را برای شما به همراه خواهد داشت.

160
00:11:33,620 --> 00:11:36,640
این واقعاً فقط یک راه دیگر برای تفکر در مورد جهت است.

161
00:11:37,100 --> 00:11:42,171
برای مثال ساده‌تر، اگر تابعی با دو متغیر به‌عنوان ورودی داشته باشید، و محاسبه 

162
00:11:42,171 --> 00:11:47,111
کنید که گرادیان آن در نقطه‌ای خاص به صورت 3،1 می‌آید، از یک سو می‌توانید آن 

163
00:11:47,111 --> 00:11:52,117
را به این صورت تفسیر کنید که وقتی با ایستادن در آن ورودی، حرکت در امتداد این 

164
00:11:52,117 --> 00:11:57,123
جهت، تابع را سریع‌تر افزایش می‌دهد، که وقتی تابع را در بالای صفحه نقاط ورودی 

165
00:11:57,123 --> 00:12:02,260
نمودار می‌کنید، آن بردار همان چیزی است که جهت سربالایی مستقیم را به شما می‌دهد.

166
00:12:02,860 --> 00:12:07,493
اما راه دیگری برای خواندن این است که بگوییم تغییرات این متغیر اول 

167
00:12:07,493 --> 00:12:11,986
3 برابر تغییرات متغیر دوم اهمیت دارد، که حداقل در همسایگی ورودی 

168
00:12:11,986 --> 00:12:16,900
مربوطه، هل دادن مقدار x برای شما ضربه بسیار بیشتری به همراه دارد. دلار

169
00:12:19,880 --> 00:12:22,340
بیایید بزرگنمایی کنیم و خلاصه کنیم که تا اینجای کار کجا بوده ایم.

170
00:12:22,840 --> 00:12:30,040
خود شبکه این تابع با 784 ورودی و 10 خروجی است که بر حسب همه این مجموع وزنی تعریف شده است.

171
00:12:30,640 --> 00:12:33,680
تابع هزینه یک لایه از پیچیدگی در بالای آن است.

172
00:12:33,980 --> 00:12:37,921
این 13000 وزنه و سوگیری را به عنوان ورودی می گیرد و بر 

173
00:12:37,921 --> 00:12:41,720
اساس نمونه های تمرینی، یک معیار تنبلی را نشان می دهد.

174
00:12:42,440 --> 00:12:46,900
و گرادیان تابع هزینه همچنان یک لایه دیگر از پیچیدگی است.

175
00:12:47,360 --> 00:12:52,444
به ما می‌گوید چه ضربه‌هایی به همه این وزن‌ها و سوگیری‌ها باعث سریع‌ترین تغییر در مقدار 

176
00:12:52,444 --> 00:12:57,646
تابع هزینه می‌شود، که ممکن است به این صورت تعبیر کنید که تغییرات برای کدام وزن‌ها مهم‌تر 

177
00:12:57,646 --> 00:12:57,880
است.

178
00:13:02,560 --> 00:13:05,970
بنابراین، وقتی شبکه را با وزن‌ها و بایاس‌های تصادفی مقداردهی اولیه 

179
00:13:05,970 --> 00:13:09,432
می‌کنید، و آن‌ها را چندین بار بر اساس این فرآیند نزول گرادیان تنظیم 

180
00:13:09,432 --> 00:13:13,200
می‌کنید، واقعاً چقدر روی تصاویری که قبلاً دیده نشده‌اند، عملکرد خوبی دارد؟

181
00:13:14,100 --> 00:13:17,972
لایه ای که من در اینجا توضیح دادم، با دو لایه پنهان از 16 نورون 

182
00:13:17,972 --> 00:13:21,905
که هر کدام عمدتاً به دلایل زیبایی شناختی انتخاب شده اند، بد نیست 

183
00:13:21,905 --> 00:13:25,960
و حدود 96٪ از تصاویر جدیدی را که به درستی می بیند طبقه بندی می کند.

184
00:13:26,680 --> 00:13:29,515
و راستش را بخواهید، اگر به برخی از نمونه‌هایی که آن را خراب 

185
00:13:29,515 --> 00:13:32,540
می‌کند نگاه کنید، احساس می‌کنید مجبور هستید کمی آن را کاهش دهید.

186
00:13:36,220 --> 00:13:39,100
حالا اگر با ساختار لایه پنهان بازی کنید و چند ترفند 

187
00:13:39,100 --> 00:13:41,760
انجام دهید، می توانید این را تا 98٪ دریافت کنید.

188
00:13:41,760 --> 00:13:42,720
و این خیلی خوب است!

189
00:13:43,020 --> 00:13:47,661
این بهترین نیست، مطمئناً می‌توانید با پیچیده‌تر شدن از این شبکه وانیلی ساده، عملکرد 

190
00:13:47,661 --> 00:13:52,137
بهتری داشته باشید، اما با توجه به اینکه کار اولیه چقدر ترسناک است، فکر می‌کنم در 

191
00:13:52,137 --> 00:13:56,778
مورد هر شبکه‌ای که این کار را به خوبی روی تصاویری که قبلاً هرگز دیده نشده است، چیزی 

192
00:13:56,778 --> 00:14:01,420
باورنکردنی وجود دارد. ما هرگز به طور خاص به آن نگفتیم که به دنبال چه الگوهایی باشیم.

193
00:14:02,560 --> 00:14:06,308
در اصل، روشی که من به این ساختار انگیزه دادم، توصیف امیدی بود که ممکن 

194
00:14:06,308 --> 00:14:09,950
بود داشته باشیم، که لایه دوم ممکن است روی لبه‌های کوچکی قرار بگیرد، 

195
00:14:09,950 --> 00:14:13,484
لایه سوم آن لبه‌ها را برای تشخیص حلقه‌ها و خطوط طولانی‌تر کنار هم 

196
00:14:13,484 --> 00:14:17,180
قرار دهد، و اینکه ممکن است آن‌ها تکه تکه شوند. با هم برای تشخیص ارقام

197
00:14:17,960 --> 00:14:20,400
پس آیا این همان کاری است که شبکه ما در واقع انجام می دهد؟

198
00:14:21,079 --> 00:14:24,400
خوب، حداقل برای این یکی، اصلا.

199
00:14:24,820 --> 00:14:28,830
به یاد داشته باشید که چگونه آخرین ویدیوی ما به این موضوع نگاه کردیم که چگونه 

200
00:14:28,830 --> 00:14:32,945
وزن اتصالات از تمام نورون های لایه اول به یک نورون مشخص در لایه دوم را می توان 

201
00:14:32,945 --> 00:14:37,060
به عنوان یک الگوی پیکسل معینی که نورون لایه دوم در حال برداشتن آن است تجسم کرد؟

202
00:14:37,780 --> 00:14:43,125
خوب، وقتی واقعاً این کار را برای وزن‌های مرتبط با این انتقال‌ها انجام می‌دهیم، 

203
00:14:43,125 --> 00:14:48,267
از لایه اول به لایه بعدی، به جای اینکه لبه‌های کوچک جدا شده را اینجا و آنجا 

204
00:14:48,267 --> 00:14:53,680
انتخاب کنیم، تقریباً تصادفی به نظر می‌رسند، فقط با الگوهای بسیار شل در وسط اونجا

205
00:14:53,760 --> 00:14:58,736
به نظر می‌رسد که در فضای غیرقابل درک 13000 بعدی وزن‌ها و سوگیری‌های ممکن، 

206
00:14:58,736 --> 00:15:03,983
شبکه ما خود را یک حداقل محلی خوشحال می‌یابد که، علی‌رغم طبقه‌بندی موفقیت‌آمیز 

207
00:15:03,983 --> 00:15:08,960
اکثر تصاویر، دقیقاً الگوهایی را که ممکن بود انتظارش را داشتیم، انتخاب کند.

208
00:15:09,780 --> 00:15:11,870
و برای اینکه واقعاً این نقطه را به خانه هدایت کنید، ببینید 

209
00:15:11,870 --> 00:15:13,820
وقتی یک تصویر تصادفی را وارد می کنید چه اتفاقی می افتد.

210
00:15:14,320 --> 00:15:19,421
اگر سیستم هوشمند بود، ممکن است انتظار داشته باشید که احساس نامطمئنی داشته باشید، 

211
00:15:19,421 --> 00:15:24,334
شاید واقعاً هیچ یک از آن 10 نورون خروجی را فعال نمی کند یا همه آنها را به طور 

212
00:15:24,334 --> 00:15:29,121
مساوی فعال نمی کند، اما در عوض با اطمینان به شما پاسخ های بیهوده ای می دهد، 

213
00:15:29,121 --> 00:15:34,160
گویی مطمئن است که این نویز تصادفی 5 است همانطور که یک تصویر واقعی از 5 یک 5 است.

214
00:15:34,540 --> 00:15:37,680
با بیان متفاوت، حتی اگر این شبکه بتواند ارقام را به 

215
00:15:37,680 --> 00:15:40,700
خوبی تشخیص دهد، هیچ ایده ای برای ترسیم آنها ندارد.

216
00:15:41,420 --> 00:15:45,240
بسیاری از این به این دلیل است که این یک مجموعه آموزشی بسیار محدود است.

217
00:15:45,880 --> 00:15:47,740
منظورم این است که اینجا خود را به جای شبکه قرار دهید.

218
00:15:48,140 --> 00:15:52,539
از دیدگاه آن، کل جهان چیزی جز ارقام متحرک کاملاً مشخص که در یک شبکه 

219
00:15:52,539 --> 00:15:56,874
کوچک متمرکز شده‌اند، تشکیل نمی‌شود، و تابع هزینه آن هرگز انگیزه‌ای 

220
00:15:56,874 --> 00:16:01,080
به آن نداده است که چیزی جز اطمینان کامل در تصمیم‌هایش داشته باشد.

221
00:16:02,120 --> 00:16:06,020
بنابراین با این تصویری از کاری که آن نورون های لایه دوم واقعاً انجام می دهند، ممکن 

222
00:16:06,020 --> 00:16:09,920
است تعجب کنید که چرا من این شبکه را با انگیزه برداشتن لبه ها و الگوها معرفی می کنم.

223
00:16:09,920 --> 00:16:12,300
منظورم این است که اصلاً این کاری نیست که در نهایت انجام شود.

224
00:16:13,380 --> 00:16:17,180
خوب، این هدف نهایی ما نیست، بلکه یک نقطه شروع است.

225
00:16:17,640 --> 00:16:21,915
صادقانه بگویم، این فناوری قدیمی است، نوعی که در دهه‌های 80 و 90 مورد تحقیق قرار 

226
00:16:21,915 --> 00:16:26,243
گرفت، و قبل از اینکه بتوانید انواع مدرن با جزئیات بیشتری را درک کنید، باید آن را 

227
00:16:26,243 --> 00:16:30,518
درک کنید، و به وضوح می‌تواند مشکلات جالبی را حل کند، اما هر چه بیشتر در مورد آن 

228
00:16:30,518 --> 00:16:34,740
تحقیق کنید. این لایه‌های پنهان واقعاً کار می‌کنند، هرچه هوشمندتر به نظر می‌رسد.

229
00:16:38,480 --> 00:16:42,416
تغییر تمرکز برای لحظه ای از نحوه یادگیری شبکه ها به نحوه یادگیری شما، این 

230
00:16:42,416 --> 00:16:46,300
تنها در صورتی اتفاق می افتد که به نحوی فعالانه با مطالب اینجا درگیر شوید.

231
00:16:47,060 --> 00:16:51,572
یک کار بسیار ساده که از شما می‌خواهم انجام دهید این است که همین الان مکث کنید و 

232
00:16:51,572 --> 00:16:56,028
برای لحظه‌ای عمیقاً فکر کنید که اگر می‌خواهید چیزهایی مانند لبه‌ها و الگوها را 

233
00:16:56,028 --> 00:17:00,880
بهتر ببیند، چه تغییراتی ممکن است در این سیستم ایجاد کنید و چگونه تصاویر را درک می‌کند.

234
00:17:01,479 --> 00:17:05,225
اما بهتر از آن، برای درگیر شدن با مطالب، کتاب مایکل نیلسن 

235
00:17:05,225 --> 00:17:09,099
در مورد یادگیری عمیق و شبکه های عصبی را به شدت توصیه می کنم.

236
00:17:09,680 --> 00:17:14,109
در آن، می‌توانید کد و داده‌هایی را برای دانلود و بازی برای همین مثال پیدا 

237
00:17:14,109 --> 00:17:18,359
کنید، و کتاب گام به گام شما را با آنچه که کد انجام می‌دهد، آشنا می‌کند.

238
00:17:19,300 --> 00:17:23,337
نکته جالب این است که این کتاب رایگان و در دسترس عموم است، بنابراین اگر 

239
00:17:23,337 --> 00:17:27,660
چیزی از آن به دست آوردید، به من بپیوندید تا به تلاش‌های نیلسن کمک مالی کنید.

240
00:17:27,660 --> 00:17:32,171
من همچنین چند منبع دیگر را که بسیار دوستشان دارم را در توضیحات پیوند داده 

241
00:17:32,171 --> 00:17:36,500
ام، از جمله پست فوق العاده و زیبای وبلاگ کریس اولا و مقالات در Distill.

242
00:17:38,280 --> 00:17:41,005
برای اینکه در چند دقیقه آخر همه چیز را در اینجا ببندم، 

243
00:17:41,005 --> 00:17:43,880
می‌خواهم به قسمتی از مصاحبه‌ای که با لیشا لی داشتم برگردم.

244
00:17:44,300 --> 00:17:46,064
شاید او را از آخرین ویدیو به یاد بیاورید، او کار 

245
00:17:46,064 --> 00:17:47,720
دکترای خود را در زمینه یادگیری عمیق انجام داد.

246
00:17:48,300 --> 00:17:52,128
در این قطعه کوچک، او در مورد دو مقاله اخیر صحبت می کند که واقعاً 

247
00:17:52,128 --> 00:17:55,780
به چگونگی یادگیری برخی از شبکه های مدرن تشخیص تصویر می پردازد.

248
00:17:56,120 --> 00:18:00,345
فقط برای تنظیم جایی که در مکالمه بودیم، مقاله اول یکی از این شبکه‌های عصبی 

249
00:18:00,345 --> 00:18:04,514
عمیق را انتخاب کرد که در تشخیص تصویر واقعاً خوب است، و به جای آموزش آن بر 

250
00:18:04,514 --> 00:18:08,740
روی یک مجموعه داده با برچسب مناسب، همه برچسب‌ها را قبل از آموزش به هم ریخت.

251
00:18:09,480 --> 00:18:13,205
بدیهی است که دقت آزمایش در اینجا بهتر از تصادفی نبود، زیرا همه چیز 

252
00:18:13,205 --> 00:18:17,042
فقط به صورت تصادفی برچسب‌گذاری شده است، اما همچنان می‌توانست به همان 

253
00:18:17,042 --> 00:18:20,880
دقت آموزشی که در مجموعه داده‌های دارای برچسب مناسب می‌رسید، دست یابد.

254
00:18:21,600 --> 00:18:26,468
اساساً، میلیون‌ها وزن برای این شبکه خاص کافی بود تا فقط داده‌های تصادفی را 

255
00:18:26,468 --> 00:18:31,401
به خاطر بسپارد، که این سوال را ایجاد می‌کند که آیا به حداقل رساندن این تابع 

256
00:18:31,401 --> 00:18:36,400
هزینه واقعاً با هر نوع ساختاری در تصویر مطابقت دارد یا فقط به خاطر سپردن است؟

257
00:18:51,440 --> 00:18:58,312
اگر به آن منحنی دقت نگاه کنید، اگر فقط روی یک مجموعه داده تصادفی تمرین می‌کردید، آن 

258
00:18:58,312 --> 00:19:05,021
منحنی به آرامی به شکلی تقریباً خطی پایین می‌آید، بنابراین شما واقعاً در تلاش برای 

259
00:19:05,021 --> 00:19:12,140
یافتن آن حداقل ممکن محلی هستید، می‌دانید ، وزنه های مناسبی که به شما این دقت را می دهد.

260
00:19:12,240 --> 00:19:17,523
در حالی که اگر شما واقعاً روی یک مجموعه داده ساختاریافته آموزش می‌دهید، مجموعه‌ای 

261
00:19:17,523 --> 00:19:22,871
که دارای برچسب‌های مناسب است، در ابتدا کمی کمانچه می‌چرخید، اما پس از آن خیلی سریع 

262
00:19:22,871 --> 00:19:28,220
افت کرده‌اید تا به آن سطح دقت برسید، و به نوعی آن را یافتن حداکثر محلی آسان تر بود.

263
00:19:28,540 --> 00:19:33,634
و بنابراین چیزی که در مورد آن جالب بود این است که مقاله دیگری را از چند سال پیش به 

264
00:19:33,634 --> 00:19:38,790
نمایش می گذارد که در مورد لایه های شبکه ساده سازی های بسیار بیشتری دارد، اما یکی از 

265
00:19:38,790 --> 00:19:43,946
نتایج این بود که چگونه اگر به چشم انداز بهینه سازی نگاه کنید، حداقل های محلی که این 

266
00:19:43,946 --> 00:19:49,041
شبکه ها تمایل به یادگیری دارند در واقع از کیفیت یکسانی برخوردار هستند، بنابراین به 

267
00:19:49,041 --> 00:19:54,320
نوعی اگر مجموعه داده شما ساختار یافته باشد، باید بتوانید آن را خیلی راحت تر پیدا کنید.

268
00:19:58,160 --> 00:20:01,180
مثل همیشه از کسانی که از Patreon حمایت می کنند تشکر می کنم.

269
00:20:01,520 --> 00:20:04,133
قبلاً گفته‌ام که Patreon چه تغییری در بازی دارد، 

270
00:20:04,133 --> 00:20:06,800
اما این ویدیوها واقعاً بدون شما امکان‌پذیر نیستند.

271
00:20:07,460 --> 00:20:10,051
همچنین می‌خواهم تشکر ویژه‌ای از شرکت VC Amplify Partners 

272
00:20:10,051 --> 00:20:12,780
داشته باشم که از این ویدیوهای اولیه در این سری حمایت می‌کند.

