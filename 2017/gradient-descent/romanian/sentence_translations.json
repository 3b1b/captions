[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "În ultimul videoclip am prezentat structura unei rețele neuronale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "Voi face o scurtă recapitulare aici, pentru a ne rămâne în minte, iar apoi am două obiective principale pentru acest videoclip.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "Prima este de a introduce ideea de coborâre a gradientului, care stă la baza nu numai a modului în care învață rețelele neuronale, ci și a modului în care funcționează multe alte tipuri de învățare automată.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "După aceea, vom cerceta puțin mai amănunțit modul în care funcționează această rețea și ce caută straturile ascunse de neuroni.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "Vă reamintim că obiectivul nostru este exemplul clasic de recunoaștere a cifrelor scrise de mână, lumea bună a rețelelor neuronale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "Aceste cifre sunt redate pe o grilă de 28x28 pixeli, fiecare pixel având o valoare de nivel de gri între 0 și 1.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "Acestea sunt cele care determină activările celor 784 de neuroni din stratul de intrare al rețelei.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "Apoi, activarea fiecărui neuron din straturile următoare se bazează pe o sumă ponderată a tuturor activărilor din stratul anterior, plus un număr special numit bias.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "Apoi compuneți această sumă cu o altă funcție, cum ar fi squishificarea sigmoidă sau o relu, așa cum am arătat în videoclipul trecut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "În total, având în vedere alegerea oarecum arbitrară a două straturi ascunse cu 16 neuroni fiecare, rețeaua are aproximativ 13 000 de ponderi și polarizări pe care le putem ajusta, iar aceste valori sunt cele care determină ce anume face rețeaua.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "Atunci, când spunem că această rețea clasifică o anumită cifră, ne referim la faptul că cel mai luminos dintre cei 10 neuroni din stratul final corespunde acelei cifre.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "Și nu uitați, motivația pe care am avut-o în minte aici pentru structura stratificată a fost că poate al doilea strat ar putea să detecteze marginile, iar al treilea strat ar putea să detecteze modele precum buclele și liniile, iar ultimul ar putea să pună cap la cap aceste modele pentru a recunoaște cifrele.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "Deci, aici învățăm cum învață rețeaua.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "Ceea ce ne dorim este un algoritm prin care să putem arăta acestei rețele o mulțime de date de antrenament, care se prezintă sub forma unor imagini diferite de cifre scrise de mână, împreună cu etichete pentru ceea ce ar trebui să fie, iar aceasta va ajusta cele 13.000 de ponderi și polarizări pentru a-și îmbunătăți performanța pe datele de antrenament.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "Să sperăm că această structură stratificată va însemna că ceea ce se învață se va generaliza la imagini dincolo de acele date de instruire.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "Modul în care testăm acest lucru este ca, după ce ați antrenat rețeaua, să îi arătați mai multe date etichetate pe care nu le-a mai văzut până acum și să vedeți cât de precis clasifică aceste noi imagini.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "Din fericire pentru noi, și ceea ce face ca acesta să fie un exemplu atât de comun pentru început, este faptul că oamenii buni din spatele bazei de date MNIST au adunat o colecție de zeci de mii de imagini cu cifre scrise de mână, fiecare dintre ele fiind etichetată cu numerele pe care ar trebui să le reprezinte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "Și oricât de provocator ar fi să descrii o mașină ca fiind de învățare, odată ce vezi cum funcționează, pare mai puțin ca o premisă științifico-fantastică nebună și mai mult ca un exercițiu de calcul.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "Adică, practic, se reduce la găsirea minimului unei anumite funcții.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "Amintiți-vă că, din punct de vedere conceptual, ne gândim la fiecare neuron ca fiind conectat la toți neuronii din stratul anterior, iar ponderile din suma ponderată care definește activarea sa sunt ca și cum ar fi puterea acestor conexiuni, iar tendința este o indicație a faptului că acel neuron tinde să fie activ sau inactiv.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "Pentru a începe, vom inițializa toate aceste ponderi și polarizări în mod total aleatoriu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "Nu mai este nevoie să spunem că această rețea va avea performanțe destul de slabe pe un anumit exemplu de antrenament, deoarece face ceva aleatoriu.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "De exemplu, introduceți această imagine a unui 3, iar stratul de ieșire arată ca un dezastru.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "Deci, ceea ce trebuie să faci este să definești o funcție de cost, un mod de a-i spune computerului, nu, computer rău, că ieșirea ar trebui să aibă activări care sunt 0 pentru majoritatea neuronilor, dar 1 pentru acest neuron, ceea ce mi-ai dat este un gunoi total.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "Pentru a spune acest lucru într-un mod mai matematic, se adună pătratele diferențelor dintre fiecare dintre aceste activări de ieșire a gunoiului și valoarea pe care doriți să o aibă, iar acesta este ceea ce vom numi costul unui singur exemplu de instruire.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "Observați că această sumă este mică atunci când rețeaua clasifică cu încredere imaginea în mod corect, dar este mare atunci când rețeaua pare că nu știe ce face.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "Deci, ceea ce trebuie să faceți este să luați în considerare costul mediu pentru toate zecile de mii de exemple de formare pe care le aveți la dispoziție.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "Acest cost mediu este măsura noastră pentru cât de proastă este rețeaua și cât de rău ar trebui să se simtă calculatorul.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "Iar acesta este un lucru complicat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "Vă amintiți cum rețeaua în sine era practic o funcție, una care primește 784 de numere ca intrări, valorile pixelilor, și emite 10 numere ca ieșire și, într-un fel, este parametrizată de toate aceste ponderi și polarizări?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "Ei bine, funcția de cost este un strat de complexitate în plus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "Aceasta ia ca intrare cele aproximativ 13 000 de ponderi și polarizări și emite un singur număr care descrie cât de rele sunt aceste ponderi și polarizări, iar modul în care este definit depinde de comportamentul rețelei pe toate zecile de mii de date de antrenament.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "Sunt multe lucruri la care trebuie să te gândești.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "Dar nu este de prea mare ajutor să-i spui calculatorului ce treabă proastă face.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "Vreți să-i spuneți cum să schimbe aceste ponderi și prejudecăți astfel încât să se îmbunătățească.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "Pentru a ușura lucrurile, în loc să vă străduiți să vă imaginați o funcție cu 13 000 de intrări, imaginați-vă o funcție simplă care are un număr ca intrare și un număr ca ieșire.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "Cum se găsește o intrare care minimizează valoarea acestei funcții?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "Studenții de la calcul știu că, uneori, se poate afla acest minim în mod explicit, dar acest lucru nu este întotdeauna fezabil pentru funcții foarte complicate, cu siguranță nu în versiunea cu 13.000 de intrări a acestei situații pentru funcția de cost a rețelei noastre neuronale nebunește de complicate.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "O tactică mai flexibilă este să începeți de la orice intrare și să vă dați seama în ce direcție ar trebui să pășiți pentru a face ca acea ieșire să fie mai mică.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "Mai exact, dacă vă puteți da seama de panta funcției în locul în care vă aflați, atunci deplasați la stânga dacă această pantă este pozitivă și deplasați intrarea la dreapta dacă această pantă este negativă.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "Dacă faceți acest lucru în mod repetat, verificând în fiecare punct noua pantă și făcând pasul corespunzător, vă veți apropia de un minim local al funcției.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "Imaginea pe care o aveți în minte aici este o minge care se rostogolește pe un deal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "Observați că, chiar și în cazul acestei funcții simplificate cu o singură intrare, există multe văi posibile în care ați putea ateriza, în funcție de intrarea aleatorie de la care începeți, și nu există nicio garanție că minimul local în care aterizați va fi cea mai mică valoare posibilă a funcției de cost.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "Acest lucru se va aplica și în cazul rețelei neuronale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "De asemenea, vreau să observați cum, dacă mărim pașii proporțional cu panta, atunci când panta se aplatizează spre minim, pașii devin din ce în ce mai mici, ceea ce vă ajută să nu depășiți limita.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "Pentru a mări puțin complexitatea, imaginați-vă în schimb o funcție cu două intrări și o ieșire.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "Vă puteți gândi la spațiul de intrare ca fiind planul xy, iar funcția de cost ca fiind reprezentată grafic ca o suprafață deasupra acestuia.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "În loc să vă întrebați despre panta funcției, trebuie să vă întrebați în ce direcție ar trebui să pășiți în acest spațiu de intrare pentru a diminua cât mai repede valoarea de ieșire a funcției.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "Cu alte cuvinte, care este direcția de coborâre?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "Din nou, este util să ne gândim la o minge care se rostogolește pe acel deal.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "Cei dintre voi care sunteți familiarizați cu calculul multivariabil știu că gradientul unei funcții vă oferă direcția de ascensiune cea mai abruptă, care este direcția în care ar trebui să pășiți pentru a crește funcția cel mai rapid.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "În mod firesc, dacă se ia negativul acestui gradient, se obține direcția în care trebuie să pășești pentru ca funcția să scadă cel mai rapid.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "Mai mult decât atât, lungimea acestui vector de gradient este o indicație pentru cât de abruptă este cea mai abruptă pantă.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "Dacă nu sunteți familiarizați cu calculul multivariabil și doriți să aflați mai multe, consultați o parte din lucrarea pe care am realizat-o pentru Khan Academy pe această temă.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "Sincer să fiu, tot ce contează pentru noi doi în acest moment este că, în principiu, există o modalitate de a calcula acest vector, acest vector care vă spune care este direcția de coborâre și cât de abruptă este.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "Vei fi în regulă dacă asta e tot ce știi și nu ești solid ca o stâncă în ceea ce privește detaliile.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "Dacă puteți obține acest lucru, algoritmul de minimizare a funcției este de a calcula această direcție de gradient, apoi de a face un mic pas în jos și de a repeta această operațiune la nesfârșit.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "Este aceeași idee de bază pentru o funcție care are 13.000 de intrări în loc de 2 intrări.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "Imaginați-vă organizarea tuturor celor 13.000 de ponderi și prejudecăți ale rețelei noastre într-un vector coloană uriaș.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "Gradientul negativ al funcției de cost este doar un vector, este o direcție în interiorul acestui spațiu de intrare incredibil de mare, care vă spune care dintre aceste numere va cauza cea mai rapidă scădere a funcției de cost.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "Și, bineînțeles, cu funcția noastră de cost special concepută, modificarea ponderilor și a polarizărilor pentru a o diminua înseamnă că ieșirea rețelei pentru fiecare bucată de date de instruire nu mai seamănă atât de mult cu o matrice aleatorie de 10 valori, cât mai mult cu o decizie reală pe care dorim să o ia.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "Este important de reținut că această funcție de cost implică o medie pe toate datele de instruire, astfel încât, dacă o minimizați, înseamnă că este o performanță mai bună pe toate aceste eșantioane.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "Algoritmul pentru calcularea eficientă a acestui gradient, care este, de fapt, inima modului în care o rețea neuronală învață, se numește backpropagation și despre el voi vorbi în următorul videoclip.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "Acolo, vreau să îmi fac timp pentru a explica ce se întâmplă exact cu fiecare greutate și părtinire pentru o anumită bucată de date de instruire, încercând să dau o senzație intuitivă a ceea ce se întâmplă dincolo de calculele și formulele relevante.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "Chiar aici, chiar acum, principalul lucru pe care vreau să îl știți, indiferent de detaliile de implementare, este că atunci când vorbim despre o rețea care învață, ne referim la minimizarea unei funcții de cost.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "Observați că o consecință a acestui fapt este că este important ca această funcție de cost să aibă o ieșire netedă, astfel încât să putem găsi un minim local prin pași mici în jos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "Acesta este motivul pentru care, apropo, neuronii artificiali au activări care variază continuu, în loc să fie pur și simplu activi sau inactivi într-un mod binar, așa cum sunt neuronii biologici.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "Acest proces care constă în stimularea repetată a unei intrări a unei funcții cu un multiplu al gradientului negativ se numește coborâre a gradientului.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "Este o modalitate de a converge către un minim local al unei funcții de cost, practic o vale în acest grafic.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "Vă arăt în continuare imaginea unei funcții cu două intrări, desigur, pentru că nuanțele într-un spațiu de intrare cu 13 000 de dimensiuni sunt puțin cam greu de înțeles, dar există o modalitate non-spațială de a vă gândi la acest lucru.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "Fiecare componentă a gradientului negativ ne spune două lucruri.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "Semnul, desigur, ne spune dacă componenta corespunzătoare a vectorului de intrare trebuie să fie împinsă în sus sau în jos.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "Dar, ceea ce este important, magnitudinea relativă a tuturor acestor componente ne spune ce schimbări sunt mai importante.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "Vedeți, în rețeaua noastră, o ajustare a uneia dintre ponderi ar putea avea un impact mult mai mare asupra funcției de cost decât ajustarea unei alte ponderi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "Unele dintre aceste conexiuni sunt mai importante pentru datele noastre de instruire.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "Deci, un mod în care vă puteți gândi la acest vector de gradient al funcției noastre de cost extrem de masive este că acesta codifică importanța relativă a fiecărei ponderi și prejudecăți, adică care dintre aceste modificări va aduce cel mai mult profit pentru banii dumneavoastră.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "Acesta este doar un alt mod de a gândi despre direcție.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "Pentru a lua un exemplu mai simplu, dacă aveți o funcție cu două variabile ca intrare și calculați că gradientul său într-un anumit punct este 3,1, atunci, pe de o parte, puteți interpreta acest lucru ca spunând că, atunci când vă aflați la acea intrare, deplasarea de-a lungul acestei direcții mărește funcția cel mai rapid, și că, atunci când reprezentați grafic funcția deasupra planului punctelor de intrare, acel vector este cel care vă oferă direcția de urcare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "Dar un alt mod de a citi această afirmație este să spunem că modificările acestei prime variabile au o importanță de 3 ori mai mare decât modificările celei de-a doua variabile, că, cel puțin în vecinătatea datelor de intrare relevante, modificarea valorii x are un impact mult mai mare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "Să facem un zoom out și să rezumăm unde ne aflăm până acum.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "Rețeaua în sine este această funcție cu 784 de intrări și 10 ieșiri, definite în funcție de toate aceste sume ponderate.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "Funcția de cost este un strat de complexitate în plus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "Acesta ia cele 13.000 de ponderi și prejudecăți ca intrări și emite o singură măsură a prostiei pe baza exemplelor de instruire.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "Iar gradientul funcției de cost este încă un strat de complexitate.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "Aceasta ne spune ce modificări ale tuturor acestor ponderi și prejudecăți determină cea mai rapidă schimbare a valorii funcției de cost, ceea ce poate fi interpretat ca și cum ați spune ce modificări ale căror ponderi contează cel mai mult.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "Așadar, atunci când inițializați rețeaua cu ponderi și polarizări aleatorii și le ajustați de mai multe ori pe baza acestui proces de coborâre a gradientului, cât de bine se comportă cu imagini pe care nu le-a văzut niciodată?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "Cea pe care am descris-o aici, cu cele două straturi ascunse de 16 neuroni fiecare, alese mai ales din motive estetice, nu este rea, clasificând corect aproximativ 96% din noile imagini pe care le vede.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "Și, sincer, dacă te uiți la unele dintre exemplele în care greșește, te simți obligat să o lași mai moale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "Acum, dacă vă jucați cu structura stratului ascuns și faceți câteva modificări, puteți ajunge la 98%.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "Și asta e destul de bine!",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "Nu este cea mai bună, cu siguranță puteți obține performanțe mai bune dacă deveniți mai sofisticați decât această rețea simplă, dar având în vedere cât de dificilă este sarcina inițială, cred că este ceva incredibil ca o rețea să se descurce atât de bine cu imagini pe care nu le-a mai văzut niciodată, având în vedere că nu i-am spus niciodată în mod specific ce tipare să caute.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "Inițial, modul în care am motivat această structură a fost prin descrierea unei speranțe pe care am putea să o avem, că al doilea strat ar putea să detecteze marginile mici, că al treilea strat ar pune cap la cap aceste margini pentru a recunoaște bucle și linii mai lungi, iar acestea ar putea fi puse cap la cap pentru a recunoaște cifrele.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "Deci, asta face de fapt rețeaua noastră?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "Ei bine, cel puțin în cazul acesta, deloc.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "Vă amintiți că în ultimul video am văzut cum ponderile conexiunilor de la toți neuronii din primul strat la un anumit neuron din al doilea strat pot fi vizualizate ca un anumit model de pixeli pe care neuronul din al doilea strat îl captează?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "Ei bine, atunci când facem acest lucru pentru greutățile asociate cu aceste tranziții, de la primul strat la următorul, în loc de a detecta mici margini izolate aici și acolo, ele arată, ei bine, aproape la întâmplare, doar cu câteva modele foarte libere în mijloc.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "S-ar părea că, în spațiul imens de 13 000 de dimensiuni al ponderilor și prejudecăților posibile, rețeaua noastră a găsit un mic minim local fericit care, deși clasifică cu succes majoritatea imaginilor, nu detectează exact modelele pe care le-am fi sperat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "Și pentru ca acest lucru să fie clar, urmăriți ce se întâmplă atunci când introduceți o imagine aleatorie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "Dacă sistemul ar fi fost inteligent, v-ați fi așteptat să se simtă nesigur, poate că nu ar fi activat niciunul dintre cei 10 neuroni de ieșire sau că i-ar fi activat pe toți în mod egal, dar în schimb vă oferă cu încredere un răspuns fără sens, ca și cum ar fi la fel de sigur că acest zgomot aleatoriu este un 5 ca și cum ar fi la fel de sigur că o imagine reală a unui 5 este un 5.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "Altfel spus, chiar dacă această rețea poate recunoaște destul de bine cifrele, nu are nicio idee despre cum să le deseneze.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "O mare parte din acest lucru se datorează faptului că este o configurație de antrenament atât de restrânsă.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "Adică, puneți-vă în locul rețelei.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "Din punctul său de vedere, întregul univers nu constă decât în cifre nemișcate și clar definite, centrate într-o grilă minusculă, iar funcția sa de cost nu i-a oferit niciodată vreun stimulent pentru a fi altfel decât extrem de încrezător în deciziile sale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "Astfel, având această imagine a ceea ce fac cu adevărat neuronii din al doilea strat, vă puteți întreba de ce aș introduce această rețea cu motivația de a detecta marginile și modelele.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "Vreau să spun că nu este deloc ceea ce ajunge să facă.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "Ei bine, acesta nu este menit să fie obiectivul nostru final, ci un punct de plecare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "Sincer, aceasta este o tehnologie veche, de genul celei cercetate în anii '80 și '90, pe care trebuie să o înțelegeți înainte de a înțelege variantele moderne mai detaliate, și este evident că este capabilă să rezolve unele probleme interesante, dar cu cât vă adânciți mai mult în ceea ce fac cu adevărat acele straturi ascunse, cu atât mai puțin inteligentă pare.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "Dacă ne mutăm pentru o clipă atenția de la modul în care rețelele învață la modul în care înveți tu, acest lucru se va întâmpla doar dacă te implici activ în materialul de aici, într-un fel sau altul.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "Un lucru destul de simplu pe care aș vrea să îl faceți este să vă opriți acum și să vă gândiți profund pentru un moment la ce schimbări ați putea face în acest sistem și la modul în care percepe imaginile dacă ați dori ca acesta să sesizeze mai bine lucruri precum marginile și modelele.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "Dar, mai mult decât atât, pentru a vă familiariza cu acest material, vă recomand cu căldură cartea lui Michael Nielsen despre învățarea profundă și rețelele neuronale.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "În ea, puteți găsi codul și datele pe care să le descărcați și cu care să vă jucați pentru acest exemplu exact, iar cartea vă va explica pas cu pas ce face codul respectiv.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "Ceea ce este minunat este că această carte este gratuită și disponibilă publicului, așa că, dacă ați învățat ceva din ea, luați în considerare posibilitatea de a face o donație pentru eforturile lui Nielsen, alături de mine.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "De asemenea, în descriere am pus un link către alte câteva resurse care îmi plac foarte mult, inclusiv către fenomenalul și frumosul articol de blog al lui Chris Ola și către articolele din Distill.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "Pentru a încheia aici ultimele minute, vreau să revin la un fragment din interviul pe care l-am avut cu Leisha Lee.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "Poate că vă amintiți de ea din ultimul videoclip, ea și-a făcut doctoratul în învățare profundă.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "În acest mic fragment, ea vorbește despre două lucrări recente care analizează cu adevărat modul în care unele dintre cele mai moderne rețele de recunoaștere a imaginilor învață de fapt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "Doar pentru a stabili unde ne aflăm în conversație, prima lucrare a luat una dintre aceste rețele neuronale deosebit de profunde care este foarte bună la recunoașterea imaginilor și, în loc să o antreneze pe un set de date etichetate corespunzător, a amestecat toate etichetele înainte de antrenament.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "Evident, acuratețea testelor nu a fost mai bună decât cea aleatorie, deoarece totul este etichetat la întâmplare, dar a reușit să obțină aceeași acuratețe de instruire ca și în cazul unui set de date etichetat corespunzător.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "Practic, milioanele de ponderi pentru această rețea au fost suficiente pentru ca aceasta să memoreze datele aleatorii, ceea ce ridică întrebarea dacă minimizarea acestei funcții de cost corespunde de fapt vreunui tip de structură a imaginii sau este doar memorare?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "Dacă vă uitați la curba de acuratețe, dacă v-ați antrena pe un set de date aleatoriu, curba a coborât foarte încet, aproape liniar, astfel încât vă străduiți să găsiți minimul local al posibilelor ponderi corecte care să vă asigure această acuratețe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "În timp ce, dacă te antrenezi pe un set de date structurat, unul care are etichetele corecte, te joci puțin la început, dar apoi ajungi foarte repede la acel nivel de acuratețe și, într-un fel, a fost mai ușor să găsești acel maxim local.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "Ceea ce a fost interesant este că a scos la lumină o altă lucrare de acum câțiva ani, care are mult mai multe simplificări cu privire la straturile de rețea, dar unul dintre rezultate a fost că, dacă vă uitați la peisajul de optimizare, minimele locale pe care aceste rețele tind să le învețe sunt de fapt de aceeași calitate, astfel încât, într-un fel, dacă setul de date este structurat, ar trebui să puteți găsi mult mai ușor acest lucru.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "Mulțumirile mele, ca întotdeauna, celor care mă susțin pe Patreon.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "Am mai spus înainte ce schimbare de joc este Patreon, dar aceste videoclipuri nu ar fi fost posibile fără voi.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "De asemenea, doresc să mulțumesc în mod special firmei de capital de risc Amplify Partners, care a sprijinit aceste videoclipuri inițiale din serie.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]