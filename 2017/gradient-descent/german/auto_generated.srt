1
00:00:04,180 --> 00:00:07,280
Im letzten Video habe ich dir die Struktur eines neuronalen Netzes erklärt.

2
00:00:07,680 --> 00:00:10,597
Ich fasse das hier kurz zusammen, damit wir es noch im Kopf haben, 

3
00:00:10,597 --> 00:00:12,600
und dann habe ich zwei Ziele für dieses Video.

4
00:00:13,100 --> 00:00:15,542
Die erste besteht darin, die Idee des Gradientenabstiegs vorzustellen, 

5
00:00:15,542 --> 00:00:18,157
die nicht nur der Art und Weise zugrunde liegt, wie neuronale Netze lernen, 

6
00:00:18,157 --> 00:00:20,600
sondern auch, wie viele andere maschinelle Lernverfahren funktionieren.

7
00:00:21,120 --> 00:00:24,357
Danach werden wir uns etwas genauer ansehen, wie dieses spezielle Netzwerk 

8
00:00:24,357 --> 00:00:27,940
funktioniert und wonach die versteckten Schichten der Neuronen letztendlich suchen.

9
00:00:28,980 --> 00:00:32,240
Zur Erinnerung: Unser Ziel ist das klassische Beispiel der 

10
00:00:32,240 --> 00:00:36,220
handschriftlichen Ziffernerkennung, die Hallo-Welt der neuronalen Netze.

11
00:00:37,020 --> 00:00:40,466
Diese Ziffern werden auf einem 28x28 Pixel großen Raster dargestellt, 

12
00:00:40,466 --> 00:00:43,420
wobei jedes Pixel einen Graustufenwert zwischen 0 und 1 hat.

13
00:00:43,820 --> 00:00:50,040
Diese bestimmen die Aktivierungen von 784 Neuronen in der Eingabeschicht des Netzwerks.

14
00:00:51,180 --> 00:00:54,409
Die Aktivierung jedes Neurons in den folgenden Schichten basiert 

15
00:00:54,409 --> 00:00:57,391
dann auf einer gewichteten Summe aller Aktivierungen in der 

16
00:00:57,391 --> 00:01:00,820
vorherigen Schicht plus einer speziellen Zahl, die Bias genannt wird.

17
00:01:02,160 --> 00:01:05,139
Dann fügst du diese Summe mit einer anderen Funktion zusammen, z. B. 

18
00:01:05,139 --> 00:01:08,940
der sigmoiden Squishifikation oder einer Relu, wie ich es im letzten Video erklärt habe.

19
00:01:09,480 --> 00:01:14,637
Insgesamt hat das Netzwerk bei der etwas willkürlichen Wahl von zwei versteckten 

20
00:01:14,637 --> 00:01:18,967
Schichten mit je 16 Neuronen etwa 13.000 Gewichte und Verzerrungen, 

21
00:01:18,967 --> 00:01:24,380
die wir anpassen können, und diese Werte bestimmen, was das Netzwerk tatsächlich tut.

22
00:01:24,880 --> 00:01:28,638
Wenn wir also sagen, dass dieses Netzwerk eine bestimmte Ziffer klassifiziert, 

23
00:01:28,638 --> 00:01:32,776
bedeutet das, dass das hellste dieser 10 Neuronen in der letzten Schicht dieser Ziffer 

24
00:01:32,776 --> 00:01:33,300
entspricht.

25
00:01:34,100 --> 00:01:38,198
Und denk daran, dass die Motivation für die Schichtstruktur darin bestand, 

26
00:01:38,198 --> 00:01:41,204
dass die zweite Schicht vielleicht die Kanten erkennt, 

27
00:01:41,204 --> 00:01:45,958
die dritte Schicht Muster wie Schleifen und Linien und die letzte Schicht diese Muster 

28
00:01:45,958 --> 00:01:48,800
einfach zusammensetzen kann, um Ziffern zu erkennen.

29
00:01:49,800 --> 00:01:52,240
Hier lernen wir also, wie das Netzwerk lernt.

30
00:01:52,640 --> 00:01:57,064
Was wir wollen, ist ein Algorithmus, bei dem du diesem Netzwerk eine ganze Reihe 

31
00:01:57,064 --> 00:02:00,997
von Trainingsdaten zeigen kannst, die in Form von verschiedenen Bildern 

32
00:02:00,997 --> 00:02:04,056
handgeschriebener Ziffern und Beschriftungen vorliegen, 

33
00:02:04,056 --> 00:02:07,115
und es passt diese 13.000 Gewichte und Verzerrungen an, 

34
00:02:07,115 --> 00:02:10,120
um seine Leistung bei den Trainingsdaten zu verbessern.

35
00:02:10,720 --> 00:02:13,746
Es ist zu hoffen, dass sich das Gelernte dank dieser mehrschichtigen 

36
00:02:13,746 --> 00:02:16,860
Struktur auch auf Bilder außerhalb der Trainingsdaten übertragen lässt.

37
00:02:17,640 --> 00:02:21,813
Nachdem du das Netzwerk trainiert hast, zeigst du ihm weitere beschriftete Daten, 

38
00:02:21,813 --> 00:02:24,358
die es noch nie zuvor gesehen hat, und du siehst, 

39
00:02:24,358 --> 00:02:26,700
wie genau es diese neuen Bilder klassifiziert.

40
00:02:31,120 --> 00:02:34,209
Zum Glück für uns, und das macht dieses Beispiel so alltäglich, 

41
00:02:34,209 --> 00:02:37,442
haben die guten Leute hinter der MNIST-Datenbank eine Sammlung von 

42
00:02:37,442 --> 00:02:40,773
Zehntausenden von handgeschriebenen Ziffernbildern zusammengestellt, 

43
00:02:40,773 --> 00:02:44,200
die jeweils mit den Zahlen beschriftet sind, die sie darstellen sollen.

44
00:02:44,900 --> 00:02:48,314
Und so provokant es auch ist, eine Maschine als lernend zu bezeichnen, 

45
00:02:48,314 --> 00:02:51,728
sobald du siehst, wie sie funktioniert, fühlt es sich weniger wie eine 

46
00:02:51,728 --> 00:02:55,480
verrückte Science-Fiction-Prämisse an, sondern viel mehr wie eine Rechenübung.

47
00:02:56,200 --> 00:02:59,960
Ich meine, im Grunde geht es darum, das Minimum einer bestimmten Funktion zu finden.

48
00:03:01,940 --> 00:03:07,140
Denke daran, dass jedes Neuron mit allen Neuronen der vorherigen Schicht verbunden ist. 

49
00:03:07,140 --> 00:03:11,336
Die Gewichte in der gewichteten Summe, die die Aktivierung definieren, 

50
00:03:11,336 --> 00:03:16,477
sind so etwas wie die Stärke dieser Verbindungen, und der Bias ist ein Hinweis darauf, 

51
00:03:16,477 --> 00:03:18,960
ob das Neuron eher aktiv oder inaktiv ist.

52
00:03:19,720 --> 00:03:24,400
Für den Anfang werden wir alle Gewichte und Verzerrungen völlig zufällig initialisieren.

53
00:03:24,940 --> 00:03:27,484
Es ist unnötig zu erwähnen, dass dieses Netzwerk bei einem bestimmten 

54
00:03:27,484 --> 00:03:30,720
Trainingsbeispiel ziemlich schlecht abschneiden wird, da es einfach etwas Zufälliges tut.

55
00:03:31,040 --> 00:03:33,269
Du gibst zum Beispiel dieses Bild einer 3 ein, 

56
00:03:33,269 --> 00:03:36,020
und die Ausgabeschicht sieht einfach nur unordentlich aus.

57
00:03:36,600 --> 00:03:40,824
Du definierst also eine Kostenfunktion, mit der du dem Computer sagst, 

58
00:03:40,824 --> 00:03:45,405
dass die Ausgabe für die meisten Neuronen eine Aktivierung von 0 haben soll, 

59
00:03:45,405 --> 00:03:50,760
für dieses Neuron aber eine Aktivierung von 1. Was du mir gegeben hast, ist völliger Müll.

60
00:03:51,720 --> 00:03:56,172
Mathematisch ausgedrückt: Du addierst die Quadrate der Differenzen zwischen 

61
00:03:56,172 --> 00:04:00,918
den einzelnen Ausgangsaktivierungen des Mülls und dem Wert, den du haben willst, 

62
00:04:00,918 --> 00:04:05,020
und das nennen wir dann die Kosten für ein einziges Trainingsbeispiel.

63
00:04:05,960 --> 00:04:11,147
Beachte, dass diese Summe klein ist, wenn das Netzwerk das Bild sicher richtig 

64
00:04:11,147 --> 00:04:16,399
klassifiziert, aber groß, wenn das Netzwerk nicht zu wissen scheint, was es tut.

65
00:04:18,640 --> 00:04:22,015
Du musst also die durchschnittlichen Kosten aller zehntausenden von 

66
00:04:22,015 --> 00:04:25,440
Ausbildungsbeispielen, die dir zur Verfügung stehen, berücksichtigen.

67
00:04:27,040 --> 00:04:29,337
Diese Durchschnittskosten sind unser Maßstab dafür, 

68
00:04:29,337 --> 00:04:32,740
wie lausig das Netzwerk ist und wie schlecht sich der Computer fühlen sollte.

69
00:04:33,420 --> 00:04:34,600
Und das ist eine komplizierte Sache.

70
00:04:35,040 --> 00:04:39,237
Erinnerst du dich daran, dass das Netzwerk selbst im Grunde eine Funktion war, 

71
00:04:39,237 --> 00:04:43,965
die 784 Zahlen als Eingaben, die Pixelwerte, aufnimmt und 10 Zahlen als Ausgabe ausgibt, 

72
00:04:43,965 --> 00:04:48,587
und dass es in gewisser Weise durch all diese Gewichte und Verzerrungen parametrisiert 

73
00:04:48,587 --> 00:04:48,800
ist?

74
00:04:49,500 --> 00:04:52,820
Nun, die Kostenfunktion ist eine zusätzliche Komplexitätsschicht, die noch dazu kommt.

75
00:04:53,100 --> 00:04:57,198
Es nimmt diese etwa 13.000 Gewichte und Verzerrungen als Eingabe und spuckt 

76
00:04:57,198 --> 00:05:02,051
eine einzige Zahl aus, die beschreibt, wie schlecht diese Gewichte und Verzerrungen sind, 

77
00:05:02,051 --> 00:05:05,826
und die Art und Weise, wie sie definiert ist, hängt vom Verhalten des 

78
00:05:05,826 --> 00:05:08,900
Netzwerks in all den Zehntausenden von Trainingsdaten ab.

79
00:05:09,520 --> 00:05:11,000
Da gibt es viel zu bedenken.

80
00:05:12,400 --> 00:05:15,820
Aber dem Computer nur zu sagen, wie schlecht er arbeitet, ist nicht sehr hilfreich.

81
00:05:16,220 --> 00:05:19,183
Du willst ihm sagen, wie es diese Gewichte und Vorurteile ändern soll, 

82
00:05:19,183 --> 00:05:20,060
damit es besser wird.

83
00:05:20,780 --> 00:05:25,217
Um es einfacher zu machen, statt dir eine Funktion mit 13.000 Eingängen vorzustellen, 

84
00:05:25,217 --> 00:05:28,312
stell dir einfach eine einfache Funktion vor, die eine Zahl 

85
00:05:28,312 --> 00:05:30,480
als Eingang und eine Zahl als Ausgang hat.

86
00:05:31,480 --> 00:05:35,300
Wie kannst du eine Eingabe finden, die den Wert dieser Funktion minimiert?

87
00:05:36,460 --> 00:05:40,644
Kalkulationsschüler wissen, dass man das Minimum manchmal explizit berechnen kann, 

88
00:05:40,644 --> 00:05:44,274
aber das ist bei wirklich komplizierten Funktionen nicht immer möglich, 

89
00:05:44,274 --> 00:05:48,559
schon gar nicht in der 13.000-Eingabe-Version dieser Situation für unsere verrückte, 

90
00:05:48,559 --> 00:05:51,080
komplizierte Kostenfunktion des neuronalen Netzes.

91
00:05:51,580 --> 00:05:56,182
Eine flexiblere Taktik ist es, bei einem beliebigen Input zu beginnen und herauszufinden, 

92
00:05:56,182 --> 00:05:59,200
in welche Richtung du gehen musst, um den Output zu senken.

93
00:06:00,080 --> 00:06:03,031
Wenn du die Steigung der Funktion herausfinden kannst, 

94
00:06:03,031 --> 00:06:06,143
verschiebe sie nach links, wenn die Steigung positiv ist, 

95
00:06:06,143 --> 00:06:09,900
und verschiebe die Eingabe nach rechts, wenn die Steigung negativ ist.

96
00:06:11,960 --> 00:06:15,900
Wenn du dies wiederholt tust, indem du an jedem Punkt die neue Steigung prüfst und den 

97
00:06:15,900 --> 00:06:19,840
entsprechenden Schritt machst, wirst du dich einem lokalen Minimum der Funktion nähern.

98
00:06:20,640 --> 00:06:23,800
Das Bild, das du vielleicht im Kopf hast, ist eine Kugel, die einen Hügel hinunterrollt.

99
00:06:24,620 --> 00:06:28,315
Beachte, dass es selbst für diese stark vereinfachte Funktion mit nur einer Eingabe 

100
00:06:28,315 --> 00:06:31,218
viele mögliche Täler gibt, in denen du landen kannst, je nachdem, 

101
00:06:31,218 --> 00:06:34,869
mit welcher zufälligen Eingabe du beginnst, und dass es keine Garantie dafür gibt, 

102
00:06:34,869 --> 00:06:38,300
dass das lokale Minimum, in dem du landest, auch der kleinstmögliche Wert der 

103
00:06:38,300 --> 00:06:39,400
Kostenfunktion sein wird.

104
00:06:40,220 --> 00:06:42,620
Das wird sich auch auf unser neuronales Netzwerk übertragen.

105
00:06:43,180 --> 00:06:47,836
Außerdem solltest du beachten, dass die Schrittgröße proportional zur Steigung ist. 

106
00:06:47,836 --> 00:06:51,495
Wenn die Steigung zum Minimum hin abflacht, werden deine Schritte 

107
00:06:51,495 --> 00:06:54,600
immer kleiner und das hilft dir, nicht zu weit zu gehen.

108
00:06:55,940 --> 00:06:58,630
Um die Komplexität ein wenig zu erhöhen, stell dir stattdessen 

109
00:06:58,630 --> 00:07:00,980
eine Funktion mit zwei Eingängen und einem Ausgang vor.

110
00:07:01,500 --> 00:07:04,725
Du kannst dir den Eingaberaum als die xy-Ebene und 

111
00:07:04,725 --> 00:07:08,140
die Kostenfunktion als eine Fläche darüber vorstellen.

112
00:07:08,760 --> 00:07:12,535
Anstatt nach der Steigung der Funktion zu fragen, musst du fragen, 

113
00:07:12,535 --> 00:07:15,747
in welche Richtung du in diesem Eingaberaum gehen musst, 

114
00:07:15,747 --> 00:07:18,960
um die Ausgabe der Funktion am schnellsten zu verringern.

115
00:07:19,720 --> 00:07:21,760
Mit anderen Worten: In welche Richtung geht es bergab?

116
00:07:22,380 --> 00:07:25,560
Auch hier ist es hilfreich, sich eine Kugel vorzustellen, die den Hügel hinunterrollt.

117
00:07:26,660 --> 00:07:30,795
Diejenigen unter euch, die mit der multivariablen Infinitesimalrechnung vertraut sind, 

118
00:07:30,795 --> 00:07:34,835
wissen, dass die Steigung einer Funktion die Richtung des steilsten Anstiegs angibt, 

119
00:07:34,835 --> 00:07:38,780
d.h. in welche Richtung du gehen musst, um die Funktion am schnellsten zu steigern.

120
00:07:39,560 --> 00:07:42,102
Wenn du den negativen Wert dieser Steigung nimmst, 

121
00:07:42,102 --> 00:07:46,040
erhältst du natürlich die Richtung, in der die Funktion am schnellsten abnimmt.

122
00:07:47,240 --> 00:07:51,728
Mehr noch: Die Länge dieses Neigungsvektors ist ein Hinweis darauf, 

123
00:07:51,728 --> 00:07:53,840
wie steil der steilste Hang ist.

124
00:07:54,540 --> 00:07:57,196
Wenn du dich mit der Mehrgrößenrechnung nicht auskennst und mehr darüber erfahren 

125
00:07:57,196 --> 00:07:59,789
möchtest, schau dir die Arbeit an, die ich für die Khan Academy zu diesem Thema 

126
00:07:59,789 --> 00:08:00,340
geschrieben habe.

127
00:08:00,860 --> 00:08:04,227
Ehrlich gesagt, ist für dich und mich im Moment nur wichtig, 

128
00:08:04,227 --> 00:08:08,091
dass es im Prinzip eine Möglichkeit gibt, diesen Vektor zu berechnen, 

129
00:08:08,091 --> 00:08:11,900
der dir sagt, in welche Richtung es bergab geht und wie steil es ist.

130
00:08:12,400 --> 00:08:14,065
Du wirst schon klarkommen, wenn das alles ist, 

131
00:08:14,065 --> 00:08:16,120
was du weißt, und du dich nicht mit den Details auskennst.

132
00:08:17,200 --> 00:08:21,035
Wenn du das kannst, besteht der Algorithmus zur Minimierung der Funktion darin, 

133
00:08:21,035 --> 00:08:24,199
diese Gradientenrichtung zu berechnen, dann einen kleinen Schritt 

134
00:08:24,199 --> 00:08:26,740
bergab zu machen und das immer wieder zu wiederholen.

135
00:08:27,700 --> 00:08:32,820
Es ist die gleiche Grundidee für eine Funktion, die 13.000 Eingänge statt 2 Eingänge hat.

136
00:08:33,400 --> 00:08:36,251
Stell dir vor, du organisierst alle 13.000 Gewichte und 

137
00:08:36,251 --> 00:08:39,460
Verzerrungen unseres Netzwerks in einem riesigen Spaltenvektor.

138
00:08:40,140 --> 00:08:43,949
Die negative Steigung der Kostenfunktion ist nur ein Vektor, 

139
00:08:43,949 --> 00:08:48,259
eine Richtung in diesem wahnsinnig großen Eingaberaum, die dir sagt, 

140
00:08:48,259 --> 00:08:53,443
welche Änderungen an all diesen Zahlen den schnellsten Rückgang der Kostenfunktion 

141
00:08:53,443 --> 00:08:54,880
zur Folge haben werden.

142
00:08:55,640 --> 00:08:59,388
Und mit unserer speziell entwickelten Kostenfunktion bedeutet eine Änderung der 

143
00:08:59,388 --> 00:09:01,683
Gewichte und Verzerrungen, um sie zu verringern, 

144
00:09:01,683 --> 00:09:05,385
dass die Ausgabe des Netzes bei jedem Teil der Trainingsdaten weniger wie eine 

145
00:09:05,385 --> 00:09:09,086
zufällige Anordnung von 10 Werten aussieht, sondern eher wie eine tatsächliche 

146
00:09:09,086 --> 00:09:10,820
Entscheidung, die wir treffen wollen.

147
00:09:11,440 --> 00:09:14,728
Es ist wichtig, sich daran zu erinnern, dass es sich bei dieser Kostenfunktion 

148
00:09:14,728 --> 00:09:18,266
um einen Durchschnitt über alle Trainingsdaten handelt. Wenn du sie also minimierst, 

149
00:09:18,266 --> 00:09:21,180
bedeutet das, dass die Leistung bei all diesen Stichproben besser ist.

150
00:09:23,820 --> 00:09:27,001
Der Algorithmus zur effizienten Berechnung dieses Gradienten, 

151
00:09:27,001 --> 00:09:30,336
der das Herzstück des Lernprozesses eines neuronalen Netzes ist, 

152
00:09:30,336 --> 00:09:33,980
heißt Backpropagation und ich werde im nächsten Video darüber sprechen.

153
00:09:34,660 --> 00:09:37,207
Dort möchte ich mir wirklich die Zeit nehmen, durchzugehen, 

154
00:09:37,207 --> 00:09:40,349
was genau mit den einzelnen Gewichten und Verzerrungen für ein bestimmtes 

155
00:09:40,349 --> 00:09:44,127
Stück Trainingsdaten passiert, und versuchen, ein intuitives Gefühl dafür zu vermitteln, 

156
00:09:44,127 --> 00:09:47,100
was jenseits des Haufens relevanter Berechnungen und Formeln passiert.

157
00:09:47,780 --> 00:09:52,729
Unabhängig von den Implementierungsdetails möchte ich, dass du weißt, dass wir, 

158
00:09:52,729 --> 00:09:55,699
wenn wir von einem lernenden Netzwerk sprechen, 

159
00:09:55,699 --> 00:09:58,360
einfach nur eine Kostenfunktion minimieren.

160
00:09:59,300 --> 00:10:02,512
Eine Folge davon ist, dass es wichtig ist, dass diese Kostenfunktion 

161
00:10:02,512 --> 00:10:06,237
einen schönen glatten Ausgang hat, damit wir ein lokales Minimum finden können, 

162
00:10:06,237 --> 00:10:08,100
indem wir kleine Schritte bergab machen.

163
00:10:09,260 --> 00:10:14,319
Das ist übrigens auch der Grund, warum künstliche Neuronen kontinuierlich aktiv sind 

164
00:10:14,319 --> 00:10:19,140
und nicht einfach nur binär aktiv oder inaktiv, wie biologische Neuronen es sind.

165
00:10:20,220 --> 00:10:23,310
Dieser Prozess, bei dem die Eingabe einer Funktion wiederholt um ein 

166
00:10:23,310 --> 00:10:26,760
Vielfaches des negativen Gradienten verschoben wird, heißt Gradientenabstieg.

167
00:10:27,300 --> 00:10:30,969
Es ist ein Weg, um zu einem lokalen Minimum einer Kostenfunktion zu konvergieren, 

168
00:10:30,969 --> 00:10:32,580
im Grunde ein Tal in diesem Graphen.

169
00:10:33,440 --> 00:10:36,999
Ich zeige natürlich immer noch das Bild einer Funktion mit zwei Eingängen, 

170
00:10:36,999 --> 00:10:41,080
weil Nudges in einem 13.000-dimensionalen Eingaberaum etwas schwer zu verstehen sind, 

171
00:10:41,080 --> 00:10:44,260
aber es gibt eine schöne nicht-räumliche Art, darüber nachzudenken.

172
00:10:45,080 --> 00:10:48,440
Jede Komponente des negativen Gradienten sagt uns zwei Dinge.

173
00:10:49,060 --> 00:10:52,193
Das Vorzeichen sagt uns natürlich, ob die entsprechende Komponente 

174
00:10:52,193 --> 00:10:55,140
des Eingangsvektors nach oben oder unten geschoben werden soll.

175
00:10:55,800 --> 00:11:00,495
Wichtig ist aber, dass die relative Größe all dieser Komponenten dir zeigt, 

176
00:11:00,495 --> 00:11:02,720
welche Veränderungen wichtiger sind.

177
00:11:05,220 --> 00:11:09,243
Du siehst, dass in unserem Netzwerk die Anpassung eines der Gewichte einen viel größeren 

178
00:11:09,243 --> 00:11:13,040
Einfluss auf die Kostenfunktion haben kann als die Anpassung eines anderen Gewichts.

179
00:11:14,800 --> 00:11:18,200
Einige dieser Verbindungen sind für unsere Trainingsdaten einfach wichtiger.

180
00:11:19,320 --> 00:11:24,127
Du kannst dir den Gradientenvektor unserer gigantischen Kostenfunktion so vorstellen, 

181
00:11:24,127 --> 00:11:28,934
dass er die relative Wichtigkeit der einzelnen Gewichtungen und Verzerrungen kodiert, 

182
00:11:28,934 --> 00:11:32,400
d.h. welche dieser Änderungen den größten Nutzen für dich hat.

183
00:11:33,620 --> 00:11:36,640
Das ist wirklich nur eine andere Art, über die Richtung nachzudenken.

184
00:11:37,100 --> 00:11:42,096
Um ein einfacheres Beispiel zu nehmen: Wenn du eine Funktion mit zwei Variablen als 

185
00:11:42,096 --> 00:11:47,271
Eingabe hast und berechnest, dass ihre Steigung an einem bestimmten Punkt 3,1 beträgt, 

186
00:11:47,271 --> 00:11:52,326
kannst du das einerseits so interpretieren, dass die Funktion am schnellsten steigt, 

187
00:11:52,326 --> 00:11:57,144
wenn du an der Eingabe stehst, und dass, wenn du die Funktion über der Ebene der 

188
00:11:57,144 --> 00:12:02,260
Eingabepunkte grafisch darstellst, dieser Vektor dir die gerade Aufwärtsrichtung gibt.

189
00:12:02,860 --> 00:12:07,559
Man kann das aber auch so interpretieren, dass Änderungen an der ersten Variable 

190
00:12:07,559 --> 00:12:11,272
dreimal so wichtig sind wie Änderungen an der zweiten Variable, 

191
00:12:11,272 --> 00:12:15,913
dass also zumindest in der Nähe des relevanten Inputs die Änderung des x-Wertes 

192
00:12:15,913 --> 00:12:16,900
viel mehr bringt.

193
00:12:19,880 --> 00:12:22,340
Lasst uns herauszoomen und zusammenfassen, wo wir bisher stehen.

194
00:12:22,840 --> 00:12:27,026
Das Netzwerk selbst ist diese Funktion mit 784 Eingängen und 10 Ausgängen, 

195
00:12:27,026 --> 00:12:30,040
die durch all diese gewichteten Summen definiert sind.

196
00:12:30,640 --> 00:12:33,680
Die Kostenfunktion ist eine weitere Ebene der Komplexität obendrauf.

197
00:12:33,980 --> 00:12:37,798
Es nimmt die 13.000 Gewichte und Verzerrungen als Eingaben und spuckt auf 

198
00:12:37,798 --> 00:12:41,720
der Grundlage der Trainingsbeispiele ein einziges Maß für die Lousiness aus.

199
00:12:42,440 --> 00:12:46,900
Und der Gradient der Kostenfunktion ist noch eine weitere Ebene der Komplexität.

200
00:12:47,360 --> 00:12:50,851
Sie sagt uns, welche Änderungen an all diesen Gewichten und Verzerrungen die 

201
00:12:50,851 --> 00:12:53,708
schnellste Veränderung des Wertes der Kostenfunktion bewirken, 

202
00:12:53,708 --> 00:12:57,109
was du so interpretieren könntest, dass die Änderungen an den Gewichten am 

203
00:12:57,109 --> 00:12:57,880
wichtigsten sind.

204
00:13:02,560 --> 00:13:06,239
Wenn du also das Netzwerk mit zufälligen Gewichten und Verzerrungen initialisierst 

205
00:13:06,239 --> 00:13:09,830
und sie viele Male auf der Grundlage dieses Gradientenabstiegsprozesses anpasst, 

206
00:13:09,830 --> 00:13:13,200
wie gut schneidet es dann bei Bildern ab, die es noch nie zuvor gesehen hat?

207
00:13:14,100 --> 00:13:18,618
Die hier beschriebene Lösung mit zwei versteckten Schichten von je 16 Neuronen, 

208
00:13:18,618 --> 00:13:21,893
die hauptsächlich aus ästhetischen Gründen gewählt wurde, 

209
00:13:21,893 --> 00:13:25,960
ist nicht schlecht und klassifiziert etwa 96 % der neuen Bilder richtig.

210
00:13:26,680 --> 00:13:29,226
Und ehrlich gesagt, wenn du dir einige der Beispiele ansiehst, 

211
00:13:29,226 --> 00:13:32,540
bei denen er Mist gebaut hat, bist du gezwungen, ein bisschen nachsichtig zu sein.

212
00:13:36,220 --> 00:13:38,949
Wenn du jetzt mit der Struktur der verborgenen Schicht herumspielst 

213
00:13:38,949 --> 00:13:41,760
und ein paar Änderungen vornimmst, kannst du den Wert auf 98% erhöhen.

214
00:13:41,760 --> 00:13:42,720
Und das ist ziemlich gut!

215
00:13:43,020 --> 00:13:47,079
Es ist nicht das beste, du kannst sicherlich eine bessere Leistung erzielen, 

216
00:13:47,079 --> 00:13:49,873
wenn du dich mehr anstrengst, aber wenn man bedenkt, 

217
00:13:49,873 --> 00:13:53,458
wie gewaltig die anfängliche Aufgabe ist, finde ich es unglaublich, 

218
00:13:53,458 --> 00:13:57,993
dass ein Netzwerk bei Bildern, die es noch nie zuvor gesehen hat, so gut abschneidet, 

219
00:13:57,993 --> 00:14:01,420
da wir ihm nie gesagt haben, nach welchen Mustern es suchen soll.

220
00:14:02,560 --> 00:14:06,214
Ursprünglich habe ich diese Struktur damit begründet, dass wir hoffen, 

221
00:14:06,214 --> 00:14:08,634
dass die zweite Schicht kleine Kanten erkennt, 

222
00:14:08,634 --> 00:14:11,311
dass die dritte Schicht diese Kanten zusammensetzt, 

223
00:14:11,311 --> 00:14:15,223
um Schleifen und längere Linien zu erkennen, und dass diese zusammengesetzt 

224
00:14:15,223 --> 00:14:17,180
werden können, um Ziffern zu erkennen.

225
00:14:17,960 --> 00:14:20,400
Ist es das, was unser Netzwerk tatsächlich tut?

226
00:14:21,080 --> 00:14:24,400
Nun, zumindest in diesem Fall nicht.

227
00:14:24,820 --> 00:14:27,250
Weißt du noch, wie wir im letzten Video gesehen haben, 

228
00:14:27,250 --> 00:14:30,431
wie die Gewichte der Verbindungen von allen Neuronen der ersten Schicht 

229
00:14:30,431 --> 00:14:33,348
zu einem bestimmten Neuron der zweiten Schicht als ein bestimmtes 

230
00:14:33,348 --> 00:14:37,060
Pixelmuster visualisiert werden können, das das Neuron der zweiten Schicht aufnimmt?

231
00:14:37,780 --> 00:14:42,911
Wenn wir das für die Gewichte tun, die mit den Übergängen von der ersten Schicht 

232
00:14:42,911 --> 00:14:48,168
zur nächsten verbunden sind, sehen sie, anstatt vereinzelte kleine Kanten hier und 

233
00:14:48,168 --> 00:14:53,680
da aufzugreifen, fast zufällig aus, nur mit einigen sehr lockeren Mustern in der Mitte.

234
00:14:53,760 --> 00:14:57,891
Es scheint, dass unser Netzwerk in dem unvorstellbar großen 13.000-dimensionalen 

235
00:14:57,891 --> 00:15:01,819
Raum möglicher Gewichtungen und Verzerrungen ein glückliches kleines lokales 

236
00:15:01,819 --> 00:15:05,746
Minimum gefunden hat, das zwar die meisten Bilder erfolgreich klassifiziert, 

237
00:15:05,746 --> 00:15:08,960
aber nicht genau die Muster erkennt, auf die wir gehofft haben.

238
00:15:09,780 --> 00:15:11,980
Und um diesen Punkt wirklich zu verdeutlichen, schau dir an, 

239
00:15:11,980 --> 00:15:13,820
was passiert, wenn du ein zufälliges Bild eingibst.

240
00:15:14,320 --> 00:15:16,868
Wenn das System schlau wäre, könntest du erwarten, 

241
00:15:16,868 --> 00:15:21,016
dass es sich unsicher fühlt und vielleicht keines der 10 Ausgangsneuronen wirklich 

242
00:15:21,016 --> 00:15:23,365
aktiviert oder sie alle gleichmäßig aktiviert, 

243
00:15:23,365 --> 00:15:26,713
aber stattdessen gibt es dir selbstbewusst eine unsinnige Antwort, 

244
00:15:26,713 --> 00:15:30,611
als ob es sich genauso sicher ist, dass dieses zufällige Geräusch eine 5 ist, 

245
00:15:30,611 --> 00:15:34,160
wie es sich sicher ist, dass ein tatsächliches Bild einer 5 eine 5 ist.

246
00:15:34,540 --> 00:15:38,486
Anders ausgedrückt: Auch wenn dieses Netzwerk Ziffern ziemlich gut erkennen kann, 

247
00:15:38,486 --> 00:15:40,700
hat es keine Ahnung, wie es sie zeichnen soll.

248
00:15:41,420 --> 00:15:45,240
Das liegt zum großen Teil daran, dass die Ausbildung so stark eingeschränkt ist.

249
00:15:45,880 --> 00:15:47,740
Ich meine, versetz dich in die Lage des Netzwerks.

250
00:15:48,140 --> 00:15:52,080
Aus seiner Sicht besteht das gesamte Universum aus nichts anderem als klar definierten, 

251
00:15:52,080 --> 00:15:55,124
unbeweglichen Ziffern, die in einem winzigen Raster zentriert sind, 

252
00:15:55,124 --> 00:15:57,766
und seine Kostenfunktion hat ihm nie einen Anreiz gegeben, 

253
00:15:57,766 --> 00:16:01,080
etwas anderes als absolut zuversichtlich in seinen Entscheidungen zu sein.

254
00:16:02,120 --> 00:16:05,369
Wenn du dir also vorstellst, was die Neuronen der zweiten Schicht wirklich tun, 

255
00:16:05,369 --> 00:16:08,701
fragst du dich vielleicht, warum ich dieses Netzwerk mit der Motivation einführe, 

256
00:16:08,701 --> 00:16:09,920
Kanten und Muster aufzuspüren.

257
00:16:09,920 --> 00:16:12,300
Ich meine, das ist überhaupt nicht das, was es am Ende macht.

258
00:16:13,380 --> 00:16:17,180
Nun, das soll nicht unser Endziel sein, sondern ein Startpunkt.

259
00:16:17,640 --> 00:16:22,690
Ehrlich gesagt handelt es sich um eine alte Technologie, 

260
00:16:22,690 --> 00:16:29,512
die in den 80er und 90er Jahren erforscht wurde, und du musst sie verstehen, 

261
00:16:29,512 --> 00:16:34,740
bevor du detailliertere moderne Varianten verstehen kannst.

262
00:16:38,480 --> 00:16:40,687
Wenn wir den Fokus für einen Moment von der Art und Weise, 

263
00:16:40,687 --> 00:16:43,344
wie Netzwerke lernen, auf die Art und Weise, wie du lernst, verlagern, 

264
00:16:43,344 --> 00:16:46,300
dann wird das nur passieren, wenn du dich aktiv mit dem Stoff auseinandersetzt.

265
00:16:47,060 --> 00:16:50,080
Eine ganz einfache Sache, die ich dir ans Herz legen möchte, ist, 

266
00:16:50,080 --> 00:16:52,734
jetzt innezuhalten und einen Moment darüber nachzudenken, 

267
00:16:52,734 --> 00:16:55,663
welche Änderungen du an diesem System und an der Art und Weise, 

268
00:16:55,663 --> 00:16:58,546
wie es Bilder wahrnimmt, vornehmen könntest, wenn du möchtest, 

269
00:16:58,546 --> 00:17:00,880
dass es Dinge wie Kanten und Muster besser erkennt.

270
00:17:01,480 --> 00:17:04,473
Aber um sich wirklich mit der Materie zu beschäftigen, 

271
00:17:04,473 --> 00:17:09,099
empfehle ich dir das Buch von Michael Nielsen über Deep Learning und neuronale Netze.

272
00:17:09,680 --> 00:17:14,120
Darin findest du den Code und die Daten, die du für genau dieses Beispiel herunterladen 

273
00:17:14,120 --> 00:17:18,359
und ausprobieren kannst, und das Buch führt dich Schritt für Schritt durch den Code.

274
00:17:19,300 --> 00:17:22,690
Das Tolle ist, dass dieses Buch kostenlos und öffentlich zugänglich ist. 

275
00:17:22,690 --> 00:17:26,545
Wenn du also etwas damit anfangen kannst, solltest du dich mir anschließen und für 

276
00:17:26,545 --> 00:17:27,660
Nielsens Arbeit spenden.

277
00:17:27,660 --> 00:17:32,104
Ich habe in der Beschreibung auch ein paar andere Ressourcen verlinkt, die ich sehr mag, 

278
00:17:32,104 --> 00:17:36,500
darunter den phänomenalen und schönen Blogpost von Chris Ola und die Artikel in Distill.

279
00:17:38,280 --> 00:17:40,966
Um die letzten Minuten hier abzuschließen, möchte ich noch 

280
00:17:40,966 --> 00:17:43,880
einmal einen Ausschnitt aus dem Interview mit Leisha Lee zeigen.

281
00:17:44,300 --> 00:17:46,079
Du erinnerst dich vielleicht noch an sie aus dem letzten Video, 

282
00:17:46,079 --> 00:17:47,720
sie hat ihre Doktorarbeit im Bereich Deep Learning gemacht.

283
00:17:48,300 --> 00:17:51,790
In diesem kleinen Ausschnitt spricht sie über zwei aktuelle Arbeiten, 

284
00:17:51,790 --> 00:17:55,780
die sich damit befassen, wie moderne Bilderkennungsnetzwerke tatsächlich lernen.

285
00:17:56,120 --> 00:17:59,785
Die erste Arbeit nahm eines dieser besonders tiefen neuronalen Netze, 

286
00:17:59,785 --> 00:18:03,817
die wirklich gut in der Bilderkennung sind, und anstatt es mit einem richtig 

287
00:18:03,817 --> 00:18:08,163
beschrifteten Datensatz zu trainieren, wurden alle Beschriftungen vor dem Training 

288
00:18:08,163 --> 00:18:08,740
vertauscht.

289
00:18:09,480 --> 00:18:13,215
Natürlich war die Testgenauigkeit hier nicht besser als beim Zufallsprinzip, 

290
00:18:13,215 --> 00:18:16,853
da alles nur zufällig beschriftet ist, aber es konnte trotzdem die gleiche 

291
00:18:16,853 --> 00:18:20,880
Trainingsgenauigkeit wie bei einem richtig beschrifteten Datensatz erreicht werden.

292
00:18:21,600 --> 00:18:29,083
Das wirft die Frage auf, ob die Minimierung dieser Kostenfunktion tatsächlich irgendeiner 

293
00:18:29,083 --> 00:18:36,400
Art von Struktur im Bild entspricht, oder ob es sich nur um ein Auswendiglernen handelt?

294
00:18:51,440 --> 00:18:58,496
Wenn du dir die Genauigkeitskurve ansiehst und mit einem zufälligen Datensatz trainierst, 

295
00:18:58,496 --> 00:19:05,553
geht die Kurve sehr langsam und fast linear nach unten, sodass du wirklich darum kämpfst, 

296
00:19:05,553 --> 00:19:12,140
ein lokales Minimum möglicher Gewichte zu finden, die dir diese Genauigkeit bringen.

297
00:19:12,240 --> 00:19:15,841
Wenn du hingegen mit einem strukturierten Datensatz trainierst, 

298
00:19:15,841 --> 00:19:19,892
der die richtigen Labels hat, fummelst du am Anfang ein bisschen herum, 

299
00:19:19,892 --> 00:19:24,056
aber dann fällst du sehr schnell, um das Genauigkeitsniveau zu erreichen, 

300
00:19:24,056 --> 00:19:28,220
und so war es in gewisser Weise einfacher, diese lokalen Maxima zu finden.

301
00:19:28,540 --> 00:19:35,005
Das Interessante daran ist, dass es eine andere Arbeit von vor ein paar Jahren 

302
00:19:35,005 --> 00:19:40,243
ans Licht bringt, in der die Netzschichten viel einfacher sind, 

303
00:19:40,243 --> 00:19:47,609
aber eines der Ergebnisse besagt, dass, wenn man sich die Optimierungslandschaft ansieht, 

304
00:19:47,609 --> 00:19:54,320
die lokalen Minima, die diese Netze lernen, eigentlich von gleicher Qualität sind.

305
00:19:58,160 --> 00:20:01,180
Mein Dank gilt wie immer denjenigen von euch, die mich auf Patreon unterstützen.

306
00:20:01,520 --> 00:20:04,213
Ich habe bereits gesagt, wie wichtig Patreon ist, 

307
00:20:04,213 --> 00:20:06,800
aber diese Videos wären ohne dich nicht möglich.

308
00:20:07,460 --> 00:20:10,383
Ein besonderer Dank gilt auch der VC-Firma Amplify Partners, 

309
00:20:10,383 --> 00:20:12,780
die diese ersten Videos der Reihe unterstützt hat.

