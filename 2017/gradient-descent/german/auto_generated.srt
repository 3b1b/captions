1
00:00:00,000 --> 00:00:07,240
Im letzten Video habe ich die Struktur eines neuronalen Netzwerks dargelegt.

2
00:00:07,240 --> 00:00:09,200
Ich werde hier eine kurze Zusammenfassung geben,

3
00:00:09,200 --> 00:00:11,080
damit es uns noch frisch im Gedächtnis bleibt,

4
00:00:11,080 --> 00:00:13,160
und dann habe ich zwei Hauptziele für dieses Video.

5
00:00:13,160 --> 00:00:15,819
Die erste besteht darin, die Idee des Gradientenabstiegs vorzustellen,

6
00:00:15,819 --> 00:00:17,991
der nicht nur dem Lernen neuronaler Netze zugrunde liegt,

7
00:00:17,991 --> 00:00:20,800
sondern auch der Funktionsweise vieler anderer maschineller Lernverfahren.

8
00:00:20,800 --> 00:00:23,594
Danach werden wir uns etwas genauer damit befassen,

9
00:00:23,594 --> 00:00:27,464
wie dieses spezielle Netzwerk funktioniert und wonach diese verborgenen

10
00:00:27,464 --> 00:00:29,560
Neuronenschichten letztendlich suchen.

11
00:00:29,560 --> 00:00:33,072
Zur Erinnerung: Unser Ziel ist hier das klassische Beispiel der

12
00:00:33,072 --> 00:00:37,080
handschriftlichen Ziffernerkennung, die Hallo-Welt der neuronalen Netze.

13
00:00:37,080 --> 00:00:40,499
Diese Ziffern werden in einem 28x28-Pixel-Raster gerendert,

14
00:00:40,499 --> 00:00:44,260
wobei jedes Pixel einen Graustufenwert zwischen 0 und 1 aufweist.

15
00:00:44,260 --> 00:00:51,400
Diese bestimmen die Aktivierung von 784 Neuronen in der Eingabeschicht des Netzwerks.

16
00:00:51,400 --> 00:00:55,106
Die Aktivierung für jedes Neuron in den folgenden Schichten basiert

17
00:00:55,106 --> 00:00:58,703
auf einer gewichteten Summe aller Aktivierungen in der vorherigen

18
00:00:58,703 --> 00:01:02,300
Schicht plus einer speziellen Zahl, die als Bias bezeichnet wird.

19
00:01:02,300 --> 00:01:04,779
Sie bilden diese Summe mit einer anderen Funktion,

20
00:01:04,779 --> 00:01:07,209
wie der Sigmoid-Squishifizierung oder einer ReLU,

21
00:01:07,209 --> 00:01:09,640
so wie ich es im letzten Video durchgegangen bin.

22
00:01:09,640 --> 00:01:14,338
Insgesamt verfügt das Netzwerk angesichts der etwas willkürlichen Wahl von zwei

23
00:01:14,338 --> 00:01:19,153
versteckten Schichten mit jeweils 16 Neuronen über etwa 13.000 Gewichte und Bias,

24
00:01:19,153 --> 00:01:22,970
die wir anpassen können, und es sind diese Werte, die bestimmen,

25
00:01:22,970 --> 00:01:25,320
was genau das Netzwerk tatsächlich tut.

26
00:01:25,320 --> 00:01:28,271
Und was wir meinen, wenn wir sagen, dass dieses Netzwerk eine

27
00:01:28,271 --> 00:01:31,175
bestimmte Ziffer klassifiziert, ist, dass das hellste dieser

28
00:01:31,175 --> 00:01:34,080
10 Neuronen in der letzten Schicht dieser Ziffer entspricht.

29
00:01:34,080 --> 00:01:38,453
Und denken Sie daran, die Motivation, die wir für die Schichtstruktur im Sinn hatten,

30
00:01:38,453 --> 00:01:41,961
war, dass die zweite Schicht vielleicht die Kanten aufnehmen könnte,

31
00:01:41,961 --> 00:01:45,826
die dritte Schicht Muster wie Schleifen und Linien aufnehmen könnte und die

32
00:01:45,826 --> 00:01:49,640
letzte Schicht diese Muster einfach zusammenfügen könnte Ziffern erkennen.

33
00:01:49,640 --> 00:01:52,880
Hier erfahren wir also, wie das Netzwerk lernt.

34
00:01:52,880 --> 00:01:56,302
Was wir wollen, ist ein Algorithmus, mit dem Sie diesem Netzwerk eine ganze

35
00:01:56,302 --> 00:01:59,815
Reihe von Trainingsdaten zeigen können, die in Form einer Reihe verschiedener

36
00:01:59,815 --> 00:02:03,463
Bilder handgeschriebener Ziffern vorliegen, zusammen mit Beschriftungen für das,

37
00:02:03,463 --> 00:02:07,247
was sie sein sollen, und das wird auch so sein Passen Sie diese 13.000 Gewichtungen

38
00:02:07,247 --> 00:02:10,760
und Verzerrungen an, um die Leistung anhand der Trainingsdaten zu verbessern.

39
00:02:10,760 --> 00:02:14,349
Hoffentlich führt diese Schichtstruktur dazu, dass sich das Gelernte auf

40
00:02:14,349 --> 00:02:17,840
Bilder verallgemeinern lässt, die über die Trainingsdaten hinausgehen.

41
00:02:17,840 --> 00:02:24,677
Wir testen das so, dass Sie dem Netzwerk nach dem Training mehr beschriftete

42
00:02:24,677 --> 00:02:31,160
Daten anzeigen und sehen, wie genau es diese neuen Bilder klassifiziert.

43
00:02:31,160 --> 00:02:34,589
Zu unserem Glück, und was dies zu einem allgemeinen Beispiel macht,

44
00:02:34,589 --> 00:02:37,968
ist, dass die guten Leute hinter der MNIST-Datenbank eine Sammlung

45
00:02:37,968 --> 00:02:41,751
von Zehntausenden handgeschriebenen Ziffernbildern zusammengestellt haben,

46
00:02:41,751 --> 00:02:45,080
die jeweils mit den Zahlen beschriftet sind, die sie sein sollen.

47
00:02:45,080 --> 00:02:48,414
Und so provokativ es auch sein mag, eine Maschine als lernend zu bezeichnen,

48
00:02:48,414 --> 00:02:50,579
wenn man erst einmal sieht, wie sie funktioniert,

49
00:02:50,579 --> 00:02:53,827
fühlt es sich viel weniger wie eine verrückte Science-Fiction-Prämisse an,

50
00:02:53,827 --> 00:02:55,560
sondern viel mehr wie eine Rechenübung.

51
00:02:55,560 --> 00:03:01,040
Ich meine, im Grunde kommt es darauf an, das Minimum einer bestimmten Funktion zu finden.

52
00:03:01,040 --> 00:03:03,776
Bedenken Sie, dass wir konzeptionell davon ausgehen,

53
00:03:03,776 --> 00:03:07,802
dass jedes Neuron mit allen Neuronen in der vorherigen Schicht verbunden ist,

54
00:03:07,802 --> 00:03:12,036
und dass die Gewichte in der gewichteten Summe, die seine Aktivierung definieren,

55
00:03:12,036 --> 00:03:14,669
so etwas wie die Stärken dieser Verbindungen sind,

56
00:03:14,669 --> 00:03:18,334
und die Verzerrung ist ein Hinweis darauf ob dieses Neuron dazu neigt,

57
00:03:18,334 --> 00:03:19,780
aktiv oder inaktiv zu sein.

58
00:03:19,780 --> 00:03:22,511
Und zu Beginn werden wir alle diese Gewichte und

59
00:03:22,511 --> 00:03:25,020
Verzerrungen völlig zufällig initialisieren.

60
00:03:25,020 --> 00:03:28,138
Unnötig zu erwähnen, dass dieses Netzwerk bei einem bestimmten Trainingsbeispiel

61
00:03:28,138 --> 00:03:31,180
eine schreckliche Leistung erbringen wird, da es einfach etwas Zufälliges tut.

62
00:03:31,180 --> 00:03:33,950
Wenn Sie beispielsweise dieses Bild einer 3 einspeisen,

63
00:03:33,950 --> 00:03:36,820
sieht die Ausgabeebene einfach wie ein Durcheinander aus.

64
00:03:36,820 --> 00:03:40,658
Was Sie also tun, ist, eine Kostenfunktion zu definieren, eine Möglichkeit,

65
00:03:40,658 --> 00:03:43,385
dem Computer, nein, schlechter Computer, mitzuteilen,

66
00:03:43,385 --> 00:03:47,273
dass die Ausgabe Aktivierungen haben sollte, die für die meisten Neuronen 0,

67
00:03:47,273 --> 00:03:48,940
für dieses Neuron jedoch 1 sind.

68
00:03:48,940 --> 00:03:51,740
Was du mir gegeben hast, ist völliger Müll.

69
00:03:51,740 --> 00:03:56,617
Um es etwas mathematischer auszudrücken: Sie addieren die Quadrate der Differenzen

70
00:03:56,617 --> 00:04:01,612
zwischen jeder dieser Trash-Output-Aktivierungen und dem Wert, den sie haben sollen,

71
00:04:01,612 --> 00:04:06,020
und das ist, was wir die Kosten eines einzelnen Trainingsbeispiels nennen.

72
00:04:06,020 --> 00:04:12,026
Beachten Sie, dass diese Summe klein ist, wenn das Netzwerk das Bild sicher korrekt

73
00:04:12,026 --> 00:04:17,961
klassifiziert, aber groß ist, wenn das Netzwerk den Eindruck hat, nicht zu wissen,

74
00:04:17,961 --> 00:04:18,820
was es tut.

75
00:04:18,820 --> 00:04:23,170
Was Sie dann tun, ist, die durchschnittlichen Kosten für alle Zehntausende

76
00:04:23,170 --> 00:04:27,580
von Schulungsbeispielen zu berücksichtigen, die Ihnen zur Verfügung stehen.

77
00:04:27,580 --> 00:04:29,885
Diese durchschnittlichen Kosten sind unser Maß dafür,

78
00:04:29,885 --> 00:04:33,300
wie schlecht das Netzwerk ist und wie schlecht sich der Computer fühlen sollte.

79
00:04:33,300 --> 00:04:35,300
Und das ist eine komplizierte Sache.

80
00:04:35,300 --> 00:04:39,850
Erinnern Sie sich daran, dass das Netzwerk selbst im Grunde eine Funktion war,

81
00:04:39,850 --> 00:04:44,458
die 784 Zahlen als Eingaben, die Pixelwerte, aufnimmt und 10 Zahlen als Ausgabe

82
00:04:44,458 --> 00:04:49,354
ausgibt und in gewisser Weise durch all diese Gewichte und Vorurteile parametrisiert

83
00:04:49,354 --> 00:04:49,700
wird?

84
00:04:49,700 --> 00:04:53,340
Darüber hinaus ist die Kostenfunktion eine Ebene der Komplexität.

85
00:04:53,340 --> 00:04:58,398
Als Eingabe nimmt es diese rund 13.000 Gewichte und Bias und gibt eine einzige Zahl aus,

86
00:04:58,398 --> 00:05:01,751
die beschreibt, wie schlecht diese Gewichte und Bias sind,

87
00:05:01,751 --> 00:05:04,422
und die Art und Weise, wie sie definiert wird,

88
00:05:04,422 --> 00:05:09,140
hängt vom Verhalten des Netzwerks über all die Zehntausende von Trainingsdaten ab.

89
00:05:09,140 --> 00:05:12,460
Das gibt viel zu bedenken.

90
00:05:12,460 --> 00:05:15,360
Aber dem Computer nur zu sagen, was für eine beschissene Arbeit er macht,

91
00:05:15,360 --> 00:05:16,380
ist nicht sehr hilfreich.

92
00:05:16,380 --> 00:05:18,482
Sie möchten ihm sagen, wie diese Gewichtungen und

93
00:05:18,482 --> 00:05:21,300
Voreingenommenheiten geändert werden können, damit es besser wird.

94
00:05:21,300 --> 00:05:25,749
Um es einfacher zu machen, statt sich eine Funktion mit 13.000 Eingaben vorzustellen,

95
00:05:25,749 --> 00:05:28,491
stellen Sie sich einfach eine einfache Funktion vor,

96
00:05:28,491 --> 00:05:31,440
die eine Zahl als Eingabe und eine Zahl als Ausgabe hat.

97
00:05:31,440 --> 00:05:36,420
Wie findet man eine Eingabe, die den Wert dieser Funktion minimiert?

98
00:05:36,420 --> 00:05:38,735
Infinitesimalrechnungsstudenten werden wissen,

99
00:05:38,735 --> 00:05:41,591
dass man dieses Minimum manchmal explizit ermitteln kann,

100
00:05:41,591 --> 00:05:45,138
aber das ist bei wirklich komplizierten Funktionen nicht immer machbar,

101
00:05:45,138 --> 00:05:48,783
schon gar nicht in der 13.000-Eingabe-Version dieser Situation für unsere

102
00:05:48,783 --> 00:05:51,640
verrückt komplizierte Kostenfunktion für neuronale Netze.

103
00:05:51,640 --> 00:05:55,801
Eine flexiblere Taktik besteht darin, bei einem beliebigen Input zu beginnen und

104
00:05:55,801 --> 00:05:59,860
herauszufinden, in welche Richtung Sie gehen sollten, um den Output zu senken.

105
00:05:59,860 --> 00:06:03,849
Wenn Sie insbesondere die Steigung der Funktion an Ihrem aktuellen Standort

106
00:06:03,849 --> 00:06:06,893
ermitteln können, verschieben Sie die Eingabe nach links,

107
00:06:06,893 --> 00:06:10,987
wenn diese Steigung positiv ist, und verschieben Sie die Eingabe nach rechts,

108
00:06:10,987 --> 00:06:12,720
wenn diese Steigung negativ ist.

109
00:06:12,720 --> 00:06:16,559
Wenn Sie dies wiederholt tun, an jedem Punkt die neue Steigung überprüfen und den

110
00:06:16,559 --> 00:06:20,680
entsprechenden Schritt unternehmen, nähern Sie sich einem lokalen Minimum der Funktion.

111
00:06:20,680 --> 00:06:22,800
Und das Bild, das Sie hier vielleicht im Kopf haben,

112
00:06:22,800 --> 00:06:24,600
ist ein Ball, der einen Hügel hinunterrollt.

113
00:06:24,600 --> 00:06:28,602
Und beachten Sie, dass es selbst für diese wirklich vereinfachte Einzeleingabefunktion

114
00:06:28,602 --> 00:06:31,730
viele mögliche Täler gibt, in denen Sie landen könnten, je nachdem,

115
00:06:31,730 --> 00:06:35,181
bei welcher Zufallseingabe Sie beginnen, und es keine Garantie dafür gibt,

116
00:06:35,181 --> 00:06:38,723
dass das lokale Minimum, in dem Sie landen, der kleinstmögliche Wert ist der

117
00:06:38,723 --> 00:06:39,460
Kostenfunktion.

118
00:06:39,460 --> 00:06:43,180
Das wird sich auch auf unseren Fall des neuronalen Netzwerks übertragen lassen.

119
00:06:43,180 --> 00:06:47,477
Und ich möchte auch, dass Sie bemerken, dass Ihre Schritte immer kleiner werden,

120
00:06:47,477 --> 00:06:50,873
wenn Sie Ihre Schrittgrößen proportional zur Steigung anpassen,

121
00:06:50,873 --> 00:06:54,322
wenn die Steigung zum Minimum hin abflacht, und das hilft Ihnen,

122
00:06:54,322 --> 00:06:56,020
ein Überschießen zu verhindern.

123
00:06:56,020 --> 00:06:59,060
Um die Komplexität etwas zu erhöhen, stellen Sie sich stattdessen

124
00:06:59,060 --> 00:07:01,640
eine Funktion mit zwei Eingängen und einem Ausgang vor.

125
00:07:01,640 --> 00:07:05,303
Sie können sich den Eingaberaum als die xy-Ebene vorstellen und die

126
00:07:05,303 --> 00:07:09,020
Kostenfunktion als eine darüber liegende Fläche grafisch darstellen.

127
00:07:09,020 --> 00:07:12,990
Anstatt nach der Steigung der Funktion zu fragen, müssen Sie fragen,

128
00:07:12,990 --> 00:07:16,442
in welche Richtung Sie in diesem Eingaberaum gehen sollten,

129
00:07:16,442 --> 00:07:19,780
um die Ausgabe der Funktion am schnellsten zu verringern.

130
00:07:19,780 --> 00:07:22,340
Mit anderen Worten: Wie geht es bergab?

131
00:07:22,340 --> 00:07:25,192
Und wieder ist es hilfreich, sich einen Ball vorzustellen,

132
00:07:25,192 --> 00:07:26,740
der diesen Hügel hinunterrollt.

133
00:07:26,740 --> 00:07:31,250
Diejenigen unter Ihnen, die sich mit der Multivariablenrechnung auskennen, werden wissen,

134
00:07:31,250 --> 00:07:35,109
dass der Gradient einer Funktion die Richtung des steilsten Anstiegs angibt,

135
00:07:35,109 --> 00:07:39,420
also in welche Richtung Sie gehen sollten, um die Funktion am schnellsten zu erhöhen.

136
00:07:39,420 --> 00:07:42,219
Wenn Sie das Negativ dieses Gradienten nehmen,

137
00:07:42,219 --> 00:07:47,460
erhalten Sie natürlich die Schrittrichtung, die die Funktion am schnellsten verringert.

138
00:07:47,460 --> 00:07:52,384
Darüber hinaus ist die Länge dieses Gradientenvektors ein Hinweis darauf,

139
00:07:52,384 --> 00:07:54,580
wie steil der steilste Hang ist.

140
00:07:54,580 --> 00:07:57,956
Wenn Sie mit der Multivariablenrechnung nicht vertraut sind und mehr erfahren möchten,

141
00:07:57,956 --> 00:08:01,100
schauen Sie sich einige meiner Arbeiten zu diesem Thema für die Khan Academy an.

142
00:08:01,100 --> 00:08:04,111
Ehrlich gesagt, für Sie und mich ist im Moment nur wichtig,

143
00:08:04,111 --> 00:08:07,623
dass es im Prinzip eine Möglichkeit gibt, diesen Vektor zu berechnen,

144
00:08:07,623 --> 00:08:12,040
diesen Vektor, der Ihnen sagt, wie die Abfahrtsrichtung verläuft und wie steil sie ist.

145
00:08:12,040 --> 00:08:14,913
Wenn das alles ist, was Sie wissen, wird es Ihnen nichts ausmachen,

146
00:08:14,913 --> 00:08:17,280
und Sie sind nicht ganz sicher, was die Details angeht.

147
00:08:17,280 --> 00:08:21,561
Denn wenn Sie das bekommen, besteht der Algorithmus zur Minimierung der Funktion darin,

148
00:08:21,561 --> 00:08:24,772
diese Gradientenrichtung zu berechnen, dann einen kleinen Schritt

149
00:08:24,772 --> 00:08:27,400
bergab zu machen und das immer wieder zu wiederholen.

150
00:08:27,400 --> 00:08:30,485
Es ist die gleiche Grundidee für eine Funktion,

151
00:08:30,485 --> 00:08:33,700
die 13.000 Eingänge anstelle von 2 Eingängen hat.

152
00:08:33,700 --> 00:08:37,071
Stellen Sie sich vor, alle 13.000 Gewichtungen und Bias unseres

153
00:08:37,071 --> 00:08:40,180
Netzwerks in einem riesigen Spaltenvektor zu organisieren.

154
00:08:40,180 --> 00:08:44,110
Der negative Gradient der Kostenfunktion ist nur ein Vektor,

155
00:08:44,110 --> 00:08:49,650
es ist eine Richtung innerhalb dieses wahnsinnig großen Eingaberaums, die Ihnen sagt,

156
00:08:49,650 --> 00:08:54,933
welche Verschiebung all dieser Zahlen den schnellsten Rückgang der Kostenfunktion

157
00:08:54,933 --> 00:08:55,900
bewirken wird.

158
00:08:55,900 --> 00:08:59,421
Und mit unserer speziell entwickelten Kostenfunktion bedeutet die Änderung der

159
00:08:59,421 --> 00:09:02,274
Gewichtungen und Verzerrungen, um sie zu verringern, natürlich,

160
00:09:02,274 --> 00:09:06,287
dass die Ausgabe des Netzwerks für jedes Trainingsdatenelement weniger wie eine zufällige

161
00:09:06,287 --> 00:09:09,987
Anordnung von 10 Werten aussieht, sondern eher wie eine tatsächliche Entscheidung,

162
00:09:09,987 --> 00:09:11,280
die wir wollen es zu machen.

163
00:09:11,280 --> 00:09:15,625
Es ist wichtig zu bedenken, dass es sich bei dieser Kostenfunktion um einen

164
00:09:15,625 --> 00:09:19,799
Durchschnitt aller Trainingsdaten handelt. Wenn Sie sie also minimieren,

165
00:09:19,799 --> 00:09:24,260
bedeutet dies, dass bei allen Stichproben eine bessere Leistung erzielt wird.

166
00:09:24,260 --> 00:09:27,189
Der Algorithmus zur effizienten Berechnung dieses Gradienten,

167
00:09:27,189 --> 00:09:30,591
der praktisch das Herzstück des Lernens eines neuronalen Netzwerks ist,

168
00:09:30,591 --> 00:09:34,040
heißt Backpropagation, und darüber werde ich im nächsten Video sprechen.

169
00:09:34,040 --> 00:09:36,974
Dort möchte ich mir wirklich die Zeit nehmen, durchzugehen,

170
00:09:36,974 --> 00:09:40,447
was genau mit jeder Gewichtung und Verzerrung für ein bestimmtes Stück

171
00:09:40,447 --> 00:09:44,507
Trainingsdaten passiert, und versuchen, ein intuitives Gefühl dafür zu vermitteln,

172
00:09:44,507 --> 00:09:47,980
was jenseits des Stapels relevanter Berechnungen und Formeln passiert.

173
00:09:47,980 --> 00:09:51,007
Ich möchte Sie hier und jetzt vor allem wissen lassen,

174
00:09:51,007 --> 00:09:54,310
unabhängig von den Implementierungsdetails: Was wir meinen,

175
00:09:54,310 --> 00:09:59,264
wenn wir über Netzwerklernen sprechen, ist lediglich die Minimierung einer Kostenfunktion.

176
00:09:59,264 --> 00:09:59,320


177
00:09:59,320 --> 00:10:02,578
Und beachten Sie, eine Konsequenz daraus ist, dass es wichtig ist,

178
00:10:02,578 --> 00:10:05,740
dass diese Kostenfunktion eine schöne, gleichmäßige Ausgabe hat,

179
00:10:05,740 --> 00:10:09,340
damit wir durch kleine Schritte bergab ein lokales Minimum finden können.

180
00:10:09,340 --> 00:10:13,205
Dies ist übrigens der Grund, warum künstliche Neuronen kontinuierlich

181
00:10:13,205 --> 00:10:17,844
wechselnde Aktivierungen aufweisen und nicht einfach binär aktiv oder inaktiv sind,

182
00:10:17,844 --> 00:10:20,440
wie es bei biologischen Neuronen der Fall ist.

183
00:10:20,440 --> 00:10:23,679
Dieser Vorgang, bei dem die Eingabe einer Funktion wiederholt um ein Vielfaches

184
00:10:23,679 --> 00:10:26,960
des negativen Gradienten verschoben wird, wird als Gradientenabstieg bezeichnet.

185
00:10:26,960 --> 00:10:31,206
Dies ist eine Möglichkeit, zu einem lokalen Minimum einer Kostenfunktion zu konvergieren,

186
00:10:31,206 --> 00:10:33,000
im Grunde ein Tal in diesem Diagramm.

187
00:10:33,000 --> 00:10:36,646
Ich zeige natürlich immer noch das Bild einer Funktion mit zwei Eingaben,

188
00:10:36,646 --> 00:10:40,834
da Stupser in einem 13.000-dimensionalen Eingaberaum etwas schwer zu verstehen sind,

189
00:10:40,834 --> 00:10:45,220
aber es gibt tatsächlich eine schöne, nicht-räumliche Möglichkeit, darüber nachzudenken.

190
00:10:45,220 --> 00:10:49,100
Jede Komponente des negativen Gradienten sagt uns zwei Dinge.

191
00:10:49,100 --> 00:10:52,557
Das Vorzeichen sagt uns natürlich, ob die entsprechende Komponente

192
00:10:52,557 --> 00:10:55,860
des Eingabevektors nach oben oder unten verschoben werden soll.

193
00:10:55,860 --> 00:11:00,929
Wichtig ist jedoch, dass die relative Größe all dieser Komponenten

194
00:11:00,929 --> 00:11:05,620
Aufschluss darüber gibt, welche Veränderungen wichtiger sind.

195
00:11:05,620 --> 00:11:10,219
Sie sehen, in unserem Netzwerk könnte eine Anpassung an eines der Gewichte einen viel

196
00:11:10,219 --> 00:11:14,980
größeren Einfluss auf die Kostenfunktion haben als die Anpassung an ein anderes Gewicht.

197
00:11:14,980 --> 00:11:19,440
Einige dieser Verbindungen sind für unsere Trainingsdaten einfach wichtiger.

198
00:11:19,440 --> 00:11:24,462
Sie können sich diesen Gradientenvektor unserer überwältigend massiven Kostenfunktion

199
00:11:24,462 --> 00:11:29,544
also so vorstellen, dass er die relative Bedeutung jedes Gewichts und jeder Verzerrung

200
00:11:29,544 --> 00:11:34,100
kodiert, d. h. welche dieser Änderungen das meiste für Ihr Geld bringen wird.

201
00:11:34,100 --> 00:11:37,360
Das ist wirklich nur eine andere Art, über die Richtung nachzudenken.

202
00:11:37,360 --> 00:11:41,692
Um ein einfacheres Beispiel zu nennen: Wenn Sie eine Funktion mit zwei Variablen als

203
00:11:41,692 --> 00:11:46,177
Eingabe haben und berechnen, dass deren Gradient an einem bestimmten Punkt 3,1 beträgt,

204
00:11:46,177 --> 00:11:49,286
können Sie das einerseits so interpretieren, dass Sie sagen,

205
00:11:49,286 --> 00:11:53,414
dass dies der Fall ist Wenn Sie an dieser Eingabe stehen und sich entlang dieser

206
00:11:53,414 --> 00:11:56,421
Richtung bewegen, erhöht sich die Funktion am schnellsten.

207
00:11:56,421 --> 00:12:00,294
Wenn Sie die Funktion über der Ebene der Eingabepunkte grafisch darstellen,

208
00:12:00,294 --> 00:12:03,200
gibt Ihnen dieser Vektor die gerade Aufwärtsrichtung an.

209
00:12:03,200 --> 00:12:06,769
Man kann das aber auch so interpretieren, dass Änderungen an dieser

210
00:12:06,769 --> 00:12:11,073
ersten Variablen dreimal so wichtig sind wie Änderungen an der zweiten Variablen,

211
00:12:11,073 --> 00:12:14,748
sodass das Verändern des x-Werts zumindest in der Nähe der relevanten

212
00:12:14,748 --> 00:12:17,740
Eingabe viel mehr Vorteile für Sie mit sich bringt Bock.

213
00:12:17,740 --> 00:12:22,880
Okay, lasst uns herauszoomen und zusammenfassen, wo wir bisher stehen.

214
00:12:22,880 --> 00:12:27,785
Das Netzwerk selbst ist diese Funktion mit 784 Eingängen und 10 Ausgängen,

215
00:12:27,785 --> 00:12:30,860
definiert durch alle diese gewichteten Summen.

216
00:12:30,860 --> 00:12:34,160
Darüber hinaus ist die Kostenfunktion eine Ebene der Komplexität.

217
00:12:34,160 --> 00:12:38,117
Es nimmt die 13.000 Gewichte und Verzerrungen als Eingaben und spuckt

218
00:12:38,117 --> 00:12:42,640
basierend auf den Trainingsbeispielen ein einzelnes Maß für die Missstände aus.

219
00:12:42,640 --> 00:12:47,520
Der Gradient der Kostenfunktion ist noch eine weitere Ebene der Komplexität.

220
00:12:47,520 --> 00:12:52,397
Es sagt uns, welche Anstöße bei all diesen Gewichtungen und Verzerrungen die

221
00:12:52,397 --> 00:12:56,135
schnellste Änderung des Werts der Kostenfunktion bewirken,

222
00:12:56,135 --> 00:12:59,239
was man so interpretieren könnte, dass man sagt,

223
00:12:59,239 --> 00:13:03,040
welche Änderungen an welchen Gewichten am wichtigsten sind.

224
00:13:03,040 --> 00:13:06,876
Wenn Sie also das Netzwerk mit zufälligen Gewichtungen und Verzerrungen initialisieren

225
00:13:06,876 --> 00:13:10,315
und diese basierend auf diesem Gradientenabstiegsprozess viele Male anpassen,

226
00:13:10,315 --> 00:13:14,240
wie gut funktioniert es dann tatsächlich bei Bildern, die es noch nie zuvor gesehen hat?

227
00:13:14,240 --> 00:13:18,894
Das hier beschriebene Bild mit den zwei verborgenen Schichten von jeweils 16 Neuronen,

228
00:13:18,894 --> 00:13:22,211
die hauptsächlich aus ästhetischen Gründen ausgewählt wurden,

229
00:13:22,211 --> 00:13:26,920
ist nicht schlecht und klassifiziert etwa 96 % der neuen Bilder, die es sieht, korrekt.

230
00:13:26,920 --> 00:13:31,067
Und ehrlich gesagt, wenn man sich einige der Beispiele anschaut,

231
00:13:31,067 --> 00:13:36,300
die es vermasselt, fühlt man sich gezwungen, es etwas lockerer angehen zu lassen.

232
00:13:36,300 --> 00:13:38,670
Wenn Sie mit der Struktur der verborgenen Ebenen herumspielen und

233
00:13:38,670 --> 00:13:41,220
ein paar Änderungen vornehmen, können Sie diese bis zu 98 % erreichen.

234
00:13:41,220 --> 00:13:42,900
Und das ist ziemlich gut!

235
00:13:42,900 --> 00:13:46,232
Es ist nicht das Beste, Sie können sicherlich eine bessere Leistung erzielen,

236
00:13:46,232 --> 00:13:49,779
indem Sie ausgefeilter werden als dieses einfache Netzwerk, aber wenn man bedenkt,

237
00:13:49,779 --> 00:13:52,172
wie entmutigend die anfängliche Aufgabe ist, denke ich,

238
00:13:52,172 --> 00:13:55,248
dass es etwas Unglaubliches an sich hat, wenn ein Netzwerk bei Bildern,

239
00:13:55,248 --> 00:13:58,410
die es noch nie zuvor gesehen hat, so gut funktioniert, wenn man bedenkt,

240
00:13:58,410 --> 00:14:02,000
dass wir Ich habe ihm nie ausdrücklich gesagt, nach welchen Mustern er suchen soll.

241
00:14:02,000 --> 00:14:06,101
Ursprünglich habe ich diese Struktur dadurch motiviert, dass ich die Hoffnung beschrieb,

242
00:14:06,101 --> 00:14:09,787
die wir haben könnten, dass die zweite Schicht kleine Kanten aufgreifen könnte,

243
00:14:09,787 --> 00:14:12,460
dass die dritte Schicht diese Kanten zusammenfügen würde,

244
00:14:12,460 --> 00:14:16,284
um Schleifen und längere Linien zu erkennen, und dass diese zusammengesetzt werden

245
00:14:16,284 --> 00:14:18,220
könnten zusammen, um Ziffern zu erkennen.

246
00:14:18,220 --> 00:14:21,040
Ist es also genau das, was unser Netzwerk tut?

247
00:14:21,040 --> 00:14:24,880
Zumindest für dieses hier überhaupt nicht.

248
00:14:24,880 --> 00:14:27,894
Erinnern Sie sich daran, wie wir uns im letzten Video angeschaut haben,

249
00:14:27,894 --> 00:14:30,866
wie die Gewichtungen der Verbindungen von allen Neuronen in der ersten

250
00:14:30,866 --> 00:14:33,881
Schicht zu einem bestimmten Neuron in der zweiten Schicht als gegebenes

251
00:14:33,881 --> 00:14:37,440
Pixelmuster visualisiert werden können, das das Neuron der zweiten Schicht aufnimmt?

252
00:14:37,440 --> 00:14:43,262
Nun, wenn wir das für die mit diesen Übergängen verbundenen Gewichte tun,

253
00:14:43,262 --> 00:14:47,826
anstatt hier und da isolierte kleine Kanten aufzugreifen,

254
00:14:47,826 --> 00:14:54,200
sehen sie fast zufällig aus, nur mit einigen sehr lockeren Mustern in der Mitte.

255
00:14:54,200 --> 00:14:58,353
Es scheint, dass unser Netzwerk in dem unvorstellbar großen 13.000-dimensionalen

256
00:14:58,353 --> 00:15:02,302
Raum möglicher Gewichtungen und Verzerrungen ein glückliches kleines lokales

257
00:15:02,302 --> 00:15:06,301
Minimum gefunden hat, das trotz der erfolgreichen Klassifizierung der meisten

258
00:15:06,301 --> 00:15:09,840
Bilder nicht genau die Muster aufgreift, die wir uns erhofft hatten.

259
00:15:09,840 --> 00:15:12,425
Und um diesen Punkt wirklich zu verdeutlichen, beobachten Sie,

260
00:15:12,425 --> 00:15:14,600
was passiert, wenn Sie ein zufälliges Bild eingeben.

261
00:15:14,600 --> 00:15:17,330
Wenn das System intelligent wäre, könnte man erwarten,

262
00:15:17,330 --> 00:15:20,905
dass es sich entweder unsicher anfühlt, möglicherweise keines dieser 10

263
00:15:20,905 --> 00:15:24,480
Ausgabeneuronen wirklich aktiviert oder sie alle gleichmäßig aktiviert,

264
00:15:24,480 --> 00:15:28,055
sondern dass es Ihnen stattdessen souverän eine unsinnige Antwort gibt,

265
00:15:28,055 --> 00:15:32,077
als ob es sich so sicher anfühlt, als ob dies zufällig wäre Rauschen ist eine 5,

266
00:15:32,077 --> 00:15:34,560
so wie ein tatsächliches Bild einer 5 eine 5 ist.

267
00:15:34,560 --> 00:15:39,309
Anders ausgedrückt: Auch wenn dieses Netzwerk Ziffern ziemlich gut erkennen kann,

268
00:15:39,309 --> 00:15:41,800
hat es keine Ahnung, wie man sie zeichnet.

269
00:15:41,800 --> 00:15:43,600
Das liegt zum großen Teil daran, dass es sich um

270
00:15:43,600 --> 00:15:45,400
einen so eng begrenzten Trainingsaufbau handelt.

271
00:15:45,400 --> 00:15:48,220
Ich meine, versetzen Sie sich hier in die Lage des Netzwerks.

272
00:15:48,220 --> 00:15:52,060
Aus seiner Sicht besteht das gesamte Universum nur aus klar definierten,

273
00:15:52,060 --> 00:15:55,637
unbeweglichen Ziffern, die in einem winzigen Raster zentriert sind,

274
00:15:55,637 --> 00:15:58,319
und seine Kostenfunktion gab ihm nie einen Anreiz,

275
00:15:58,319 --> 00:16:02,160
bei seinen Entscheidungen etwas anderes als völliges Vertrauen zu haben.

276
00:16:02,160 --> 00:16:05,651
Da dies also ein Bild davon ist, was diese Neuronen der zweiten Schicht wirklich tun,

277
00:16:05,651 --> 00:16:09,020
fragen Sie sich vielleicht, warum ich dieses Netzwerk mit der Motivation einführe,

278
00:16:09,020 --> 00:16:10,320
Kanten und Muster aufzugreifen.

279
00:16:10,320 --> 00:16:13,040
Ich meine, das ist einfach überhaupt nicht das, was es letztendlich tut.

280
00:16:13,040 --> 00:16:17,480
Nun, dies soll nicht unser Endziel sein, sondern vielmehr ein Ausgangspunkt.

281
00:16:17,480 --> 00:16:20,720
Ehrlich gesagt handelt es sich hierbei um eine alte Technologie,

282
00:16:20,720 --> 00:16:24,759
wie sie in den 80er und 90er Jahren erforscht wurde, und man muss sie verstehen,

283
00:16:24,759 --> 00:16:27,701
bevor man detailliertere moderne Varianten verstehen kann,

284
00:16:27,701 --> 00:16:31,390
und sie ist eindeutig in der Lage, einige interessante Probleme zu lösen,

285
00:16:31,390 --> 00:16:35,529
aber je mehr man sich mit dem beschäftigt, was Je mehr diese verborgenen Schichten

286
00:16:35,529 --> 00:16:38,720
wirklich funktionieren, desto weniger intelligent erscheint es.

287
00:16:38,720 --> 00:16:41,855
Wenn Sie den Fokus für einen Moment von der Art und Weise, wie Netzwerke lernen,

288
00:16:41,855 --> 00:16:44,411
auf die Art und Weise verlagern, wie Sie lernen, gelingt das nur,

289
00:16:44,411 --> 00:16:47,160
wenn Sie sich irgendwie aktiv mit dem Material hier auseinandersetzen.

290
00:16:47,160 --> 00:16:51,948
Ich möchte, dass Sie ganz einfach jetzt innehalten und einen Moment tief darüber

291
00:16:51,948 --> 00:16:56,677
nachdenken, welche Änderungen Sie an diesem System vornehmen könnten und wie es

292
00:16:56,677 --> 00:17:01,880
Bilder wahrnimmt, wenn Sie möchten, dass es Dinge wie Kanten und Muster besser erkennt.

293
00:17:01,880 --> 00:17:05,426
Aber noch besser: Um sich tatsächlich mit dem Material auseinanderzusetzen,

294
00:17:05,426 --> 00:17:09,393
empfehle ich wärmstens das Buch von Michael Nielsen über Deep Learning und neuronale

295
00:17:09,393 --> 00:17:09,720
Netze.

296
00:17:09,720 --> 00:17:14,405
Darin finden Sie den Code und die Daten zum Herunterladen und Spielen für genau dieses

297
00:17:14,405 --> 00:17:18,983
Beispiel, und das Buch führt Sie Schritt für Schritt durch die Funktionsweise dieses

298
00:17:18,983 --> 00:17:19,360
Codes.

299
00:17:19,360 --> 00:17:22,711
Das Tolle daran ist, dass dieses Buch kostenlos und öffentlich verfügbar ist.

300
00:17:22,711 --> 00:17:25,203
Wenn Sie also etwas davon haben, denken Sie darüber nach,

301
00:17:25,203 --> 00:17:28,040
gemeinsam mit mir eine Spende für Nielsens Bemühungen zu leisten.

302
00:17:28,040 --> 00:17:32,116
Ich habe in der Beschreibung auch ein paar andere Ressourcen verlinkt,

303
00:17:32,116 --> 00:17:35,619
die mir sehr gefallen, darunter den phänomenalen und schönen

304
00:17:35,619 --> 00:17:38,720
Blogbeitrag von Chris Ola und die Artikel in Distill.

305
00:17:38,720 --> 00:17:41,506
Um die letzten paar Minuten abzuschließen, möchte ich noch einmal auf einen

306
00:17:41,506 --> 00:17:44,440
Ausschnitt aus dem Interview zurückkommen, das ich mit Leisha Lee geführt habe.

307
00:17:44,440 --> 00:17:46,462
Vielleicht erinnern Sie sich an sie aus dem letzten Video,

308
00:17:46,462 --> 00:17:48,520
sie hat ihre Doktorarbeit im Bereich Deep Learning gemacht.

309
00:17:48,520 --> 00:17:51,682
In diesem kleinen Ausschnitt spricht sie über zwei aktuelle Arbeiten,

310
00:17:51,682 --> 00:17:55,476
die sich intensiv damit befassen, wie einige der moderneren Bilderkennungsnetzwerke

311
00:17:55,476 --> 00:17:56,380
tatsächlich lernen.

312
00:17:56,380 --> 00:17:58,854
Nur um auf den Punkt zu bringen, an dem wir uns gerade befanden:

313
00:17:58,854 --> 00:18:02,242
In der ersten Arbeit wurde eines dieser besonders tiefen neuronalen Netzwerke verwendet,

314
00:18:02,242 --> 00:18:05,250
das wirklich gut in der Bilderkennung ist, und anstatt es anhand eines richtig

315
00:18:05,250 --> 00:18:08,486
beschrifteten Datensatzes zu trainieren, wurden alle Beschriftungen vor dem Training

316
00:18:08,486 --> 00:18:09,400
durcheinander gebracht.

317
00:18:09,400 --> 00:18:13,221
Offensichtlich war die Testgenauigkeit hier nicht besser als zufällig,

318
00:18:13,221 --> 00:18:15,320
da alles nur zufällig beschriftet ist.

319
00:18:15,320 --> 00:18:18,460
Es konnte jedoch immer noch die gleiche Trainingsgenauigkeit erreicht werden,

320
00:18:18,460 --> 00:18:21,440
die Sie mit einem ordnungsgemäß beschrifteten Datensatz erreichen würden.

321
00:18:21,440 --> 00:18:25,979
Im Grunde reichten die Millionen von Gewichten für dieses spezielle Netzwerk aus,

322
00:18:25,979 --> 00:18:29,855
um sich lediglich die Zufallsdaten zu merken, was die Frage aufwirft,

323
00:18:29,855 --> 00:18:33,453
ob die Minimierung dieser Kostenfunktion tatsächlich irgendeiner

324
00:18:33,453 --> 00:18:36,720
Struktur im Bild entspricht oder nur eine Speicherung ist.

325
00:18:36,720 --> 00:18:40,120
...um sich den gesamten Datensatz der korrekten Klassifizierung zu merken.

326
00:18:40,120 --> 00:18:43,189
Und so gab es ein paar, wissen Sie, ein halbes Jahr später beim ICML

327
00:18:43,189 --> 00:18:46,347
in diesem Jahr nicht gerade einen Gegenentwurf, sondern einen Artikel,

328
00:18:46,347 --> 00:18:49,372
der sich mit einigen Aspekten beschäftigte, wie zum Beispiel: „Hey,

329
00:18:49,372 --> 00:18:52,220
eigentlich machen diese Netzwerke etwas, das etwas klüger ist.“

330
00:18:52,220 --> 00:18:58,968
Wenn Sie sich diese Genauigkeitskurve ansehen und nur mit einem zufälligen Datensatz

331
00:18:58,968 --> 00:19:05,240
trainieren, sinkt die Kurve sehr, Sie wissen schon, sehr langsam, fast linear.

332
00:19:05,240 --> 00:19:08,845
Es fällt Ihnen also wirklich schwer, die lokalen Minima der möglichen, wissen Sie,

333
00:19:08,845 --> 00:19:12,320
richtigen Gewichte zu finden, mit denen Sie diese Genauigkeit erreichen würden.

334
00:19:12,320 --> 00:19:15,636
Wenn Sie dagegen tatsächlich mit einem strukturierten Datensatz trainieren,

335
00:19:15,636 --> 00:19:19,214
der die richtigen Bezeichnungen hat, müssen Sie am Anfang ein wenig herumfummeln,

336
00:19:19,214 --> 00:19:21,483
aber dann sind Sie ziemlich schnell zurückgefallen,

337
00:19:21,483 --> 00:19:23,360
um dieses Genauigkeitsniveau zu erreichen.

338
00:19:23,360 --> 00:19:28,580
Und so war es in gewisser Weise einfacher, diese lokalen Maxima zu finden.

339
00:19:28,580 --> 00:19:32,454
Und was daran auch interessant war, ist, dass es ein weiteres

340
00:19:32,454 --> 00:19:35,516
Papier von vor ein paar Jahren ans Licht bringt,

341
00:19:35,516 --> 00:19:40,140
das viel mehr Vereinfachungen in Bezug auf die Netzwerkschichten enthält.

342
00:19:40,140 --> 00:19:43,099
Eines der Ergebnisse besagte jedoch, dass die lokalen Minima,

343
00:19:43,099 --> 00:19:47,013
die diese Netzwerke normalerweise lernen, tatsächlich von gleicher Qualität sind,

344
00:19:47,013 --> 00:19:49,400
wenn man sich die Optimierungslandschaft ansieht.

345
00:19:49,400 --> 00:19:51,733
Wenn Ihr Datensatz also strukturiert ist, sollten

346
00:19:51,733 --> 00:19:54,300
Sie ihn in gewisser Weise viel leichter finden können.

347
00:19:54,300 --> 00:20:01,140
Mein Dank gilt wie immer allen, die Patreon unterstützen.

348
00:20:01,140 --> 00:20:04,392
Ich habe bereits gesagt, was für ein Game Changer auf Patreon ist,

349
00:20:04,392 --> 00:20:07,160
aber diese Videos wären ohne Sie wirklich nicht möglich.

350
00:20:07,160 --> 00:20:10,089
Ein besonderer Dank möchte ich auch der VC-Firma Amplify Partners

351
00:20:10,089 --> 00:20:13,240
und ihrer Unterstützung für diese ersten Videos der Serie aussprechen.

352
00:20:13,240 --> 00:20:33,140
Danke schön.

