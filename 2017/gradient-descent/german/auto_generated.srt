1
00:00:04,180 --> 00:00:07,280
Im letzten Video habe ich die Struktur eines neuronalen Netzwerks dargelegt.

2
00:00:07,680 --> 00:00:09,320
Ich werde hier eine kurze Zusammenfassung geben, 

3
00:00:09,320 --> 00:00:10,893
damit es uns noch frisch im Gedächtnis bleibt, 

4
00:00:10,893 --> 00:00:12,600
und dann habe ich zwei Hauptziele für dieses Video.

5
00:00:13,100 --> 00:00:15,723
Die erste besteht darin, die Idee des Gradientenabstiegs vorzustellen, 

6
00:00:15,723 --> 00:00:17,866
der nicht nur dem Lernen neuronaler Netze zugrunde liegt, 

7
00:00:17,866 --> 00:00:20,600
sondern auch der Funktionsweise vieler anderer maschineller Lernverfahren.

8
00:00:21,120 --> 00:00:23,309
Danach werden wir uns etwas genauer damit befassen, 

9
00:00:23,309 --> 00:00:26,340
wie dieses spezielle Netzwerk funktioniert und wonach diese verborgenen 

10
00:00:26,340 --> 00:00:27,940
Neuronenschichten letztendlich suchen.

11
00:00:28,979 --> 00:00:32,387
Zur Erinnerung: Unser Ziel ist hier das klassische Beispiel der 

12
00:00:32,387 --> 00:00:36,220
handschriftlichen Ziffernerkennung, die Hallo-Welt der neuronalen Netze.

13
00:00:37,020 --> 00:00:40,092
Diese Ziffern werden in einem 28x28-Pixel-Raster gerendert, 

14
00:00:40,092 --> 00:00:43,420
wobei jedes Pixel einen Graustufenwert zwischen 0 und 1 aufweist.

15
00:00:43,820 --> 00:00:50,040
Diese bestimmen die Aktivierung von 784 Neuronen in der Eingabeschicht des Netzwerks.

16
00:00:51,180 --> 00:00:54,285
Und dann basiert die Aktivierung für jedes Neuron in den folgenden 

17
00:00:54,285 --> 00:00:57,297
Schichten auf einer gewichteten Summe aller Aktivierungen in der 

18
00:00:57,297 --> 00:01:00,820
vorherigen Schicht plus einer speziellen Zahl, die als Bias bezeichnet wird.

19
00:01:02,160 --> 00:01:04,609
Dann bilden Sie diese Summe mit einer anderen Funktion, 

20
00:01:04,609 --> 00:01:06,796
wie der Sigmoid-Squishifizierung oder einem Relu, 

21
00:01:06,796 --> 00:01:08,940
so wie ich es im letzten Video durchgegangen bin.

22
00:01:09,480 --> 00:01:13,961
Insgesamt verfügt das Netzwerk angesichts der etwas willkürlichen Wahl von zwei 

23
00:01:13,961 --> 00:01:18,554
versteckten Schichten mit jeweils 16 Neuronen über etwa 13.000 Gewichte und Bias, 

24
00:01:18,554 --> 00:01:22,195
die wir anpassen können, und es sind diese Werte, die bestimmen, 

25
00:01:22,195 --> 00:01:24,380
was genau das Netzwerk tatsächlich tut.

26
00:01:24,880 --> 00:01:28,681
Wenn wir also sagen, dass dieses Netzwerk eine bestimmte Ziffer klassifiziert, 

27
00:01:28,681 --> 00:01:32,770
meinen wir, dass das hellste dieser 10 Neuronen in der letzten Schicht dieser Ziffer 

28
00:01:32,770 --> 00:01:33,300
entspricht.

29
00:01:34,100 --> 00:01:37,660
Und denken Sie daran, die Motivation, die wir hier für die Schichtstruktur im 

30
00:01:37,660 --> 00:01:41,358
Sinn hatten, war, dass die zweite Schicht vielleicht die Kanten aufnehmen könnte 

31
00:01:41,358 --> 00:01:45,056
und die dritte Schicht Muster wie Schleifen und Linien aufgreifen könnte und die 

32
00:01:45,056 --> 00:01:48,800
letzte Schicht diese einfach zusammenfügen könnte Muster zum Erkennen von Ziffern.

33
00:01:49,800 --> 00:01:52,240
Hier erfahren wir also, wie das Netzwerk lernt.

34
00:01:52,640 --> 00:01:55,994
Was wir wollen, ist ein Algorithmus, mit dem Sie diesem Netzwerk eine ganze 

35
00:01:55,994 --> 00:01:59,437
Reihe von Trainingsdaten zeigen können, die in Form einer Reihe verschiedener 

36
00:01:59,437 --> 00:02:03,013
Bilder handgeschriebener Ziffern vorliegen, zusammen mit Beschriftungen für das, 

37
00:02:03,013 --> 00:02:06,721
was sie sein sollen, und das wird auch so sein Passen Sie diese 13.000 Gewichtungen 

38
00:02:06,721 --> 00:02:10,120
und Verzerrungen an, um die Leistung anhand der Trainingsdaten zu verbessern.

39
00:02:10,720 --> 00:02:13,811
Hoffentlich führt diese Schichtstruktur dazu, dass sich das Gelernte auf 

40
00:02:13,811 --> 00:02:16,860
Bilder verallgemeinern lässt, die über diese Trainingsdaten hinausgehen.

41
00:02:17,640 --> 00:02:20,560
Wir testen das so, dass Sie dem Netzwerk nach dem Training 

42
00:02:20,560 --> 00:02:23,878
mehr beschriftete Daten zeigen, die es noch nie zuvor gesehen hat, 

43
00:02:23,878 --> 00:02:26,700
und sehen, wie genau es diese neuen Bilder klassifiziert.

44
00:02:31,120 --> 00:02:34,731
Zum Glück für uns, und was dies zu einem so häufigen Beispiel macht, ist, 

45
00:02:34,731 --> 00:02:38,636
dass die guten Leute hinter der MNIST-Datenbank eine Sammlung von Zehntausenden 

46
00:02:38,636 --> 00:02:41,418
handgeschriebenen Ziffernbildern zusammengestellt haben, 

47
00:02:41,418 --> 00:02:44,200
jedes mit den Zahlen beschriftet, die es tragen soll Sei.

48
00:02:44,900 --> 00:02:48,280
Und so provokativ es auch sein mag, eine Maschine als lernend zu bezeichnen, 

49
00:02:48,280 --> 00:02:50,475
wenn man erst einmal sieht, wie sie funktioniert, 

50
00:02:50,475 --> 00:02:53,767
fühlt es sich viel weniger wie eine verrückte Science-Fiction-Prämisse an, 

51
00:02:53,767 --> 00:02:55,480
sondern viel mehr wie eine Rechenübung.

52
00:02:56,200 --> 00:02:59,960
Ich meine, im Grunde kommt es darauf an, das Minimum einer bestimmten Funktion zu finden.

53
00:03:01,939 --> 00:03:04,371
Bedenken Sie, dass wir konzeptionell davon ausgehen, 

54
00:03:04,371 --> 00:03:07,949
dass jedes Neuron mit allen Neuronen in der vorherigen Schicht verbunden ist, 

55
00:03:07,949 --> 00:03:11,711
und dass die Gewichte in der gewichteten Summe, die seine Aktivierung definieren, 

56
00:03:11,711 --> 00:03:14,051
so etwas wie die Stärken dieser Verbindungen sind, 

57
00:03:14,051 --> 00:03:17,721
und die Verzerrung ist ein gewisser Hinweis darauf ob dieses Neuron dazu neigt, 

58
00:03:17,721 --> 00:03:18,960
aktiv oder inaktiv zu sein.

59
00:03:19,720 --> 00:03:22,185
Und zu Beginn werden wir alle diese Gewichte und 

60
00:03:22,185 --> 00:03:24,400
Verzerrungen völlig zufällig initialisieren.

61
00:03:24,940 --> 00:03:27,524
Es erübrigt sich zu erwähnen, dass dieses Netzwerk bei einem bestimmten 

62
00:03:27,524 --> 00:03:30,720
Trainingsbeispiel ziemlich schlecht abschneiden wird, da es einfach etwas Zufälliges tut.

63
00:03:31,040 --> 00:03:33,507
Wenn Sie beispielsweise dieses Bild einer 3 einspeisen, 

64
00:03:33,507 --> 00:03:36,020
sieht die Ausgabeebene einfach wie ein Durcheinander aus.

65
00:03:36,600 --> 00:03:40,416
Was Sie also tun, ist, eine Kostenfunktion zu definieren, eine Möglichkeit, 

66
00:03:40,416 --> 00:03:43,077
dem Computer mitzuteilen, nein, schlechter Computer, 

67
00:03:43,077 --> 00:03:47,194
dass die Ausgabe Aktivierungen haben sollte, die für die meisten Neuronen 0 sind, 

68
00:03:47,194 --> 00:03:50,760
aber 1 für dieses Neuron. Was Sie mir gegeben haben, ist völliger Müll.

69
00:03:51,720 --> 00:03:56,281
Um es etwas mathematischer auszudrücken: Sie addieren die Quadrate der Differenzen 

70
00:03:56,281 --> 00:04:00,953
zwischen jeder dieser Trash-Output-Aktivierungen und dem Wert, den sie haben sollen, 

71
00:04:00,953 --> 00:04:05,020
und das ist, was wir die Kosten eines einzelnen Trainingsbeispiels nennen.

72
00:04:05,960 --> 00:04:10,886
Beachten Sie, dass diese Summe klein ist, wenn das Netzwerk das Bild sicher korrekt 

73
00:04:10,886 --> 00:04:15,754
klassifiziert, aber groß ist, wenn das Netzwerk den Eindruck hat, nicht zu wissen, 

74
00:04:15,754 --> 00:04:16,399
was es tut.

75
00:04:18,640 --> 00:04:22,039
Was Sie dann tun, ist, die durchschnittlichen Kosten für alle Zehntausende 

76
00:04:22,039 --> 00:04:25,440
von Schulungsbeispielen zu berücksichtigen, die Ihnen zur Verfügung stehen.

77
00:04:27,040 --> 00:04:29,354
Diese durchschnittlichen Kosten sind unser Maß dafür, 

78
00:04:29,354 --> 00:04:32,740
wie schlecht das Netzwerk ist und wie schlecht sich der Computer fühlen sollte.

79
00:04:33,420 --> 00:04:34,600
Und das ist eine komplizierte Sache.

80
00:04:35,040 --> 00:04:39,405
Erinnern Sie sich daran, dass das Netzwerk selbst im Grunde eine Funktion war, 

81
00:04:39,405 --> 00:04:43,826
die 784 Zahlen als Eingaben, die Pixelwerte, aufnimmt und 10 Zahlen als Ausgabe 

82
00:04:43,826 --> 00:04:48,800
ausgibt und in gewisser Weise durch all diese Gewichte und Vorurteile parametrisiert wird?

83
00:04:49,500 --> 00:04:52,820
Nun, die Kostenfunktion ist darüber hinaus eine Ebene der Komplexität.

84
00:04:53,100 --> 00:04:58,176
Als Eingabe nimmt es diese rund 13.000 Gewichte und Bias und gibt eine einzige Zahl aus, 

85
00:04:58,176 --> 00:05:01,541
die beschreibt, wie schlecht diese Gewichte und Bias sind, 

86
00:05:01,541 --> 00:05:04,222
und die Art und Weise, wie sie definiert wird, 

87
00:05:04,222 --> 00:05:08,900
hängt vom Verhalten des Netzwerks über all die Zehntausende von Trainingsdaten ab.

88
00:05:09,520 --> 00:05:11,000
Das gibt viel zu bedenken.

89
00:05:12,400 --> 00:05:14,956
Aber dem Computer nur zu sagen, was für eine beschissene Arbeit er macht, 

90
00:05:14,956 --> 00:05:15,820
ist nicht sehr hilfreich.

91
00:05:16,220 --> 00:05:17,875
Sie möchten ihm sagen, wie diese Gewichtungen und 

92
00:05:17,875 --> 00:05:20,060
Voreingenommenheiten geändert werden können, damit es besser wird.

93
00:05:20,780 --> 00:05:25,057
Um es einfacher zu machen, statt sich eine Funktion mit 13.000 Eingaben vorzustellen, 

94
00:05:25,057 --> 00:05:27,694
stellen Sie sich einfach eine einfache Funktion vor, 

95
00:05:27,694 --> 00:05:30,480
die eine Zahl als Eingabe und eine Zahl als Ausgabe hat.

96
00:05:31,480 --> 00:05:35,300
Wie findet man eine Eingabe, die den Wert dieser Funktion minimiert?

97
00:05:36,460 --> 00:05:38,690
Infinitesimalrechnungsstudenten werden wissen, 

98
00:05:38,690 --> 00:05:41,444
dass man dieses Minimum manchmal explizit ermitteln kann, 

99
00:05:41,444 --> 00:05:44,861
aber das ist bei wirklich komplizierten Funktionen nicht immer machbar, 

100
00:05:44,861 --> 00:05:48,374
schon gar nicht in der 13.000-Eingabe-Version dieser Situation für unsere 

101
00:05:48,374 --> 00:05:51,080
verrückt komplizierte Kostenfunktion für neuronale Netze.

102
00:05:51,580 --> 00:05:55,270
Eine flexiblere Taktik besteht darin, bei einem beliebigen Input zu beginnen 

103
00:05:55,270 --> 00:05:59,200
und herauszufinden, in welche Richtung Sie gehen sollten, um den Output zu senken.

104
00:06:00,080 --> 00:06:03,495
Konkret: Wenn Sie die Steigung der Funktion an Ihrem Standort ermitteln können, 

105
00:06:03,495 --> 00:06:06,612
verschieben Sie die Eingabe nach links, wenn diese Steigung positiv ist, 

106
00:06:06,612 --> 00:06:09,900
und verschieben Sie die Eingabe nach rechts, wenn diese Steigung negativ ist.

107
00:06:11,960 --> 00:06:15,783
Wenn Sie dies wiederholt tun, an jedem Punkt die neue Steigung überprüfen und den 

108
00:06:15,783 --> 00:06:19,840
entsprechenden Schritt unternehmen, nähern Sie sich einem lokalen Minimum der Funktion.

109
00:06:20,640 --> 00:06:22,304
Das Bild, das Sie hier vielleicht im Kopf haben, 

110
00:06:22,304 --> 00:06:23,800
ist ein Ball, der einen Hügel hinunterrollt.

111
00:06:24,620 --> 00:06:28,477
Beachten Sie, dass es selbst für diese wirklich vereinfachte Einzeleingabefunktion 

112
00:06:28,477 --> 00:06:31,638
viele mögliche Täler gibt, in denen Sie landen könnten, je nachdem, 

113
00:06:31,638 --> 00:06:35,124
bei welcher Zufallseingabe Sie beginnen, und es keine Garantie dafür gibt, 

114
00:06:35,124 --> 00:06:38,702
dass das lokale Minimum, in dem Sie landen, der kleinstmögliche Wert ist der 

115
00:06:38,702 --> 00:06:39,400
Kostenfunktion.

116
00:06:40,220 --> 00:06:42,620
Das wird sich auch auf unseren Fall des neuronalen Netzwerks übertragen lassen.

117
00:06:43,180 --> 00:06:47,053
Ich möchte auch, dass Sie bemerken, dass Ihre Schritte immer kleiner werden, 

118
00:06:47,053 --> 00:06:50,273
wenn Sie Ihre Schrittgrößen proportional zur Steigung anpassen, 

119
00:06:50,273 --> 00:06:54,600
wenn die Steigung zum Minimum hin abflacht, und das verhindert, dass Sie überschießen.

120
00:06:55,940 --> 00:06:58,189
Um die Komplexität etwas zu erhöhen, stellen Sie sich 

121
00:06:58,189 --> 00:07:00,980
stattdessen eine Funktion mit zwei Eingängen und einem Ausgang vor.

122
00:07:01,500 --> 00:07:04,820
Sie können sich den Eingaberaum als die xy-Ebene vorstellen und die 

123
00:07:04,820 --> 00:07:08,140
Kostenfunktion als eine darüber liegende Fläche grafisch darstellen.

124
00:07:08,760 --> 00:07:12,543
Anstatt nach der Steigung der Funktion zu fragen, müssen Sie fragen, 

125
00:07:12,543 --> 00:07:15,834
in welche Richtung Sie in diesem Eingaberaum gehen sollten, 

126
00:07:15,834 --> 00:07:18,960
um die Ausgabe der Funktion am schnellsten zu verringern.

127
00:07:19,720 --> 00:07:21,760
Mit anderen Worten: Wie geht es bergab?

128
00:07:22,380 --> 00:07:25,560
Auch hier ist es hilfreich, sich einen Ball vorzustellen, der diesen Hügel hinunterrollt.

129
00:07:26,660 --> 00:07:30,988
Diejenigen unter Ihnen, die sich mit der Multivariablenrechnung auskennen, werden wissen, 

130
00:07:30,988 --> 00:07:34,691
dass der Gradient einer Funktion die Richtung des steilsten Anstiegs angibt, 

131
00:07:34,691 --> 00:07:38,780
also in welche Richtung Sie gehen sollten, um die Funktion am schnellsten zu erhöhen.

132
00:07:39,560 --> 00:07:41,832
Wenn Sie das Negativ dieses Gradienten nehmen, 

133
00:07:41,832 --> 00:07:46,040
erhalten Sie natürlich die Schrittrichtung, die die Funktion am schnellsten verringert.

134
00:07:47,240 --> 00:07:51,847
Darüber hinaus ist die Länge dieses Gradientenvektors ein Hinweis darauf, 

135
00:07:51,847 --> 00:07:53,840
wie steil der steilste Hang ist.

136
00:07:54,540 --> 00:07:57,561
Wenn Sie mit der Multivariablenrechnung nicht vertraut sind und mehr erfahren möchten, 

137
00:07:57,561 --> 00:08:00,340
schauen Sie sich einige meiner Arbeiten zu diesem Thema für die Khan Academy an.

138
00:08:00,860 --> 00:08:03,912
Ehrlich gesagt, für Sie und mich ist im Moment nur wichtig, 

139
00:08:03,912 --> 00:08:07,473
dass es im Prinzip eine Möglichkeit gibt, diesen Vektor zu berechnen, 

140
00:08:07,473 --> 00:08:11,900
diesen Vektor, der Ihnen sagt, wie die Abfahrtsrichtung verläuft und wie steil sie ist.

141
00:08:12,400 --> 00:08:14,456
Wenn das alles ist, was Sie wissen, wird es Ihnen nichts ausmachen, 

142
00:08:14,456 --> 00:08:16,120
und Sie sind nicht ganz sicher, was die Details angeht.

143
00:08:17,200 --> 00:08:21,106
Wenn Sie das hinbekommen, besteht der Algorithmus zur Minimierung der Funktion darin, 

144
00:08:21,106 --> 00:08:24,332
diese Gradientenrichtung zu berechnen, dann einen kleinen Schritt nach 

145
00:08:24,332 --> 00:08:26,740
unten zu machen und dies immer wieder zu wiederholen.

146
00:08:27,700 --> 00:08:30,233
Es ist die gleiche Grundidee für eine Funktion, 

147
00:08:30,233 --> 00:08:32,820
die 13.000 Eingänge anstelle von 2 Eingängen hat.

148
00:08:33,400 --> 00:08:36,579
Stellen Sie sich vor, alle 13.000 Gewichtungen und Bias unseres 

149
00:08:36,579 --> 00:08:39,460
Netzwerks in einem riesigen Spaltenvektor zu organisieren.

150
00:08:40,140 --> 00:08:43,840
Der negative Gradient der Kostenfunktion ist nur ein Vektor, 

151
00:08:43,840 --> 00:08:49,056
es ist eine Richtung innerhalb dieses wahnsinnig großen Eingaberaums, die Ihnen sagt, 

152
00:08:49,056 --> 00:08:54,030
welche Verschiebung all dieser Zahlen den schnellsten Rückgang der Kostenfunktion 

153
00:08:54,030 --> 00:08:54,880
bewirken wird.

154
00:08:55,640 --> 00:08:59,126
Und mit unserer speziell entwickelten Kostenfunktion bedeutet die Änderung der 

155
00:08:59,126 --> 00:09:01,950
Gewichtungen und Verzerrungen, um sie zu verringern, natürlich, 

156
00:09:01,950 --> 00:09:05,921
dass die Ausgabe des Netzwerks für jedes Trainingsdatenelement weniger wie eine zufällige 

157
00:09:05,921 --> 00:09:09,584
Anordnung von 10 Werten aussieht, sondern eher wie eine tatsächliche Entscheidung, 

158
00:09:09,584 --> 00:09:10,820
die wir wollen es zu machen.

159
00:09:11,440 --> 00:09:14,715
Es ist wichtig zu bedenken, dass es sich bei dieser Kostenfunktion um einen 

160
00:09:14,715 --> 00:09:17,861
Durchschnitt aller Trainingsdaten handelt. Wenn Sie sie also minimieren, 

161
00:09:17,861 --> 00:09:21,180
bedeutet dies, dass bei allen Stichproben eine bessere Leistung erzielt wird.

162
00:09:23,820 --> 00:09:26,877
Der Algorithmus zur effizienten Berechnung dieses Gradienten, 

163
00:09:26,877 --> 00:09:30,428
der praktisch das Herzstück des Lernens eines neuronalen Netzwerks ist, 

164
00:09:30,428 --> 00:09:33,980
heißt Backpropagation, und darüber werde ich im nächsten Video sprechen.

165
00:09:34,660 --> 00:09:37,288
Dort möchte ich mir wirklich die Zeit nehmen, durchzugehen, 

166
00:09:37,288 --> 00:09:40,398
was genau mit jeder Gewichtung und Verzerrung für ein bestimmtes Stück 

167
00:09:40,398 --> 00:09:44,033
Trainingsdaten passiert, und versuchen, ein intuitives Gefühl dafür zu vermitteln, 

168
00:09:44,033 --> 00:09:47,100
was jenseits des Stapels relevanter Berechnungen und Formeln passiert.

169
00:09:47,780 --> 00:09:50,618
Ich möchte Sie hier und jetzt vor allem wissen lassen, 

170
00:09:50,618 --> 00:09:53,715
unabhängig von den Implementierungsdetails: Was wir meinen, 

171
00:09:53,715 --> 00:09:58,360
wenn wir über Netzwerklernen sprechen, ist lediglich die Minimierung einer Kostenfunktion.

172
00:09:59,300 --> 00:10:02,176
Und beachten Sie, eine Konsequenz daraus ist, dass es wichtig ist, 

173
00:10:02,176 --> 00:10:04,966
dass diese Kostenfunktion eine schöne, gleichmäßige Ausgabe hat, 

174
00:10:04,966 --> 00:10:08,100
damit wir durch kleine Schritte bergab ein lokales Minimum finden können.

175
00:10:09,260 --> 00:10:12,687
Aus diesem Grund weisen künstliche Neuronen übrigens kontinuierlich 

176
00:10:12,687 --> 00:10:16,821
wechselnde Aktivierungen auf und sind nicht einfach nur binär aktiv oder inaktiv, 

177
00:10:16,821 --> 00:10:19,140
wie es bei biologischen Neuronen der Fall ist.

178
00:10:20,220 --> 00:10:23,490
Dieser Vorgang, bei dem die Eingabe einer Funktion wiederholt um ein Vielfaches 

179
00:10:23,490 --> 00:10:26,760
des negativen Gradienten verschoben wird, wird als Gradientenabstieg bezeichnet.

180
00:10:27,300 --> 00:10:31,041
Dies ist eine Möglichkeit, zu einem lokalen Minimum einer Kostenfunktion zu konvergieren, 

181
00:10:31,041 --> 00:10:32,580
im Grunde ein Tal in diesem Diagramm.

182
00:10:33,440 --> 00:10:36,847
Ich zeige natürlich immer noch das Bild einer Funktion mit zwei Eingaben, 

183
00:10:36,847 --> 00:10:40,760
da Stupser in einem 13.000-dimensionalen Eingaberaum etwas schwer zu verstehen sind, 

184
00:10:40,760 --> 00:10:44,260
aber es gibt eine schöne, nicht-räumliche Möglichkeit, darüber nachzudenken.

185
00:10:45,080 --> 00:10:48,440
Jede Komponente des negativen Gradienten sagt uns zwei Dinge.

186
00:10:49,060 --> 00:10:52,193
Das Vorzeichen sagt uns natürlich, ob die entsprechende Komponente 

187
00:10:52,193 --> 00:10:55,140
des Eingabevektors nach oben oder unten verschoben werden soll.

188
00:10:55,800 --> 00:10:59,422
Wichtig ist jedoch, dass die relative Größe all dieser Komponenten 

189
00:10:59,422 --> 00:11:02,720
Aufschluss darüber gibt, welche Veränderungen wichtiger sind.

190
00:11:05,220 --> 00:11:09,085
Sie sehen, in unserem Netzwerk könnte eine Anpassung an eines der Gewichte einen viel 

191
00:11:09,085 --> 00:11:13,040
größeren Einfluss auf die Kostenfunktion haben als die Anpassung an ein anderes Gewicht.

192
00:11:14,800 --> 00:11:18,200
Einige dieser Verbindungen sind für unsere Trainingsdaten einfach wichtiger.

193
00:11:19,320 --> 00:11:23,819
Sie können sich diesen Gradientenvektor unserer überwältigend massiven Kostenfunktion 

194
00:11:23,819 --> 00:11:28,371
also so vorstellen, dass er die relative Bedeutung jedes Gewichts und jeder Verzerrung 

195
00:11:28,371 --> 00:11:32,400
kodiert, d. h. welche dieser Änderungen das meiste für Ihr Geld bringen wird.

196
00:11:33,620 --> 00:11:36,640
Das ist wirklich nur eine andere Art, über die Richtung nachzudenken.

197
00:11:37,100 --> 00:11:41,233
Um ein einfacheres Beispiel zu nennen: Wenn Sie eine Funktion mit zwei Variablen 

198
00:11:41,233 --> 00:11:45,469
als Eingabe haben und berechnen, dass deren Gradient an einem bestimmten Punkt 3,1 

199
00:11:45,469 --> 00:11:49,552
beträgt, können Sie das einerseits so interpretieren, dass Sie Folgendes sagen: 

200
00:11:49,552 --> 00:11:53,431
Wenn Sie an dieser Eingabe stehen und sich entlang dieser Richtung bewegen, 

201
00:11:53,431 --> 00:11:57,615
erhöht sich die Funktion am schnellsten. Wenn Sie die Funktion über der Ebene der 

202
00:11:57,615 --> 00:12:02,106
Eingabepunkte grafisch darstellen, gibt Ihnen dieser Vektor die gerade Aufwärtsrichtung 

203
00:12:02,106 --> 00:12:02,260
an.

204
00:12:02,860 --> 00:12:07,329
Aber man kann das auch so interpretieren, dass Änderungen an dieser ersten Variablen 

205
00:12:07,329 --> 00:12:10,747
dreimal so wichtig sind wie Änderungen an der zweiten Variablen, 

206
00:12:10,747 --> 00:12:15,375
sodass zumindest in der Nähe der relevanten Eingabe das Verändern des x-Werts viel mehr 

207
00:12:15,375 --> 00:12:16,900
Vorteile für Sie bringt Bock.

208
00:12:19,880 --> 00:12:22,340
Lassen Sie uns herauszoomen und zusammenfassen, wo wir bisher stehen.

209
00:12:22,840 --> 00:12:27,302
Das Netzwerk selbst ist diese Funktion mit 784 Eingängen und 10 Ausgängen, 

210
00:12:27,302 --> 00:12:30,040
definiert durch alle diese gewichteten Summen.

211
00:12:30,640 --> 00:12:33,680
Darüber hinaus ist die Kostenfunktion eine Ebene der Komplexität.

212
00:12:33,980 --> 00:12:37,616
Es nimmt die 13.000 Gewichte und Verzerrungen als Eingaben und spuckt 

213
00:12:37,616 --> 00:12:41,720
basierend auf den Trainingsbeispielen ein einzelnes Maß für die Missstände aus.

214
00:12:42,440 --> 00:12:46,900
Und der Gradient der Kostenfunktion ist noch eine weitere Ebene der Komplexität.

215
00:12:47,360 --> 00:12:50,679
Es sagt uns, welche Anstöße bei all diesen Gewichtungen und Verzerrungen die 

216
00:12:50,679 --> 00:12:53,223
schnellste Änderung des Werts der Kostenfunktion bewirken, 

217
00:12:53,223 --> 00:12:55,336
was man so interpretieren könnte, dass man sagt, 

218
00:12:55,336 --> 00:12:57,880
welche Änderungen an welchen Gewichten am wichtigsten sind.

219
00:13:02,560 --> 00:13:06,218
Wenn Sie also das Netzwerk mit zufälligen Gewichtungen und Verzerrungen initialisieren 

220
00:13:06,218 --> 00:13:09,499
und diese basierend auf diesem Gradientenabstiegsprozess viele Male anpassen, 

221
00:13:09,499 --> 00:13:13,200
wie gut funktioniert es dann tatsächlich bei Bildern, die es noch nie zuvor gesehen hat?

222
00:13:14,100 --> 00:13:18,472
Das hier beschriebene Bild mit den zwei verborgenen Schichten von jeweils 16 Neuronen, 

223
00:13:18,472 --> 00:13:21,587
die hauptsächlich aus ästhetischen Gründen ausgewählt wurden, 

224
00:13:21,587 --> 00:13:25,960
ist nicht schlecht und klassifiziert etwa 96 % der neuen Bilder, die es sieht, korrekt.

225
00:13:26,680 --> 00:13:29,288
Und ehrlich gesagt, wenn man sich einige der Beispiele anschaut, 

226
00:13:29,288 --> 00:13:32,540
die es vermasselt, fühlt man sich gezwungen, es etwas lockerer angehen zu lassen.

227
00:13:36,220 --> 00:13:38,990
Wenn Sie nun mit der Struktur der verborgenen Ebenen herumspielen und 

228
00:13:38,990 --> 00:13:41,760
ein paar Änderungen vornehmen, können Sie diese bis zu 98 % erreichen.

229
00:13:41,760 --> 00:13:42,720
Und das ist ziemlich gut!

230
00:13:43,020 --> 00:13:46,453
Es ist nicht das Beste, Sie können sicherlich eine bessere Leistung erzielen, 

231
00:13:46,453 --> 00:13:50,063
wenn Sie ausgefeilter als dieses einfache Netzwerk werden, aber wenn man bedenkt, 

232
00:13:50,063 --> 00:13:52,528
wie entmutigend die anfängliche Aufgabe ist, denke ich, 

233
00:13:52,528 --> 00:13:55,697
dass es etwas Unglaubliches an sich hat, wenn ein Netzwerk bei Bildern, 

234
00:13:55,697 --> 00:13:59,483
die es noch nie zuvor gesehen hat, so gut funktioniert Wir haben ihm nie ausdrücklich 

235
00:13:59,483 --> 00:14:01,420
gesagt, nach welchen Mustern er suchen soll.

236
00:14:02,560 --> 00:14:06,267
Ursprünglich habe ich diese Struktur dadurch motiviert, dass ich die Hoffnung beschrieb, 

237
00:14:06,267 --> 00:14:09,599
die wir haben könnten, dass die zweite Schicht kleine Kanten aufgreifen könnte, 

238
00:14:09,599 --> 00:14:12,015
dass die dritte Schicht diese Kanten zusammenfügen würde, 

239
00:14:12,015 --> 00:14:15,472
um Schleifen und längere Linien zu erkennen, und dass diese zusammengesetzt werden 

240
00:14:15,472 --> 00:14:17,180
könnten zusammen, um Ziffern zu erkennen.

241
00:14:17,960 --> 00:14:20,400
Ist es also genau das, was unser Netzwerk tut?

242
00:14:21,079 --> 00:14:24,400
Zumindest für dieses hier überhaupt nicht.

243
00:14:24,820 --> 00:14:27,767
Erinnern Sie sich daran, wie wir uns im letzten Video angeschaut haben, 

244
00:14:27,767 --> 00:14:30,673
wie die Gewichtungen der Verbindungen von allen Neuronen in der ersten 

245
00:14:30,673 --> 00:14:33,621
Schicht zu einem bestimmten Neuron in der zweiten Schicht als gegebenes 

246
00:14:33,621 --> 00:14:37,060
Pixelmuster visualisiert werden können, das das Neuron der zweiten Schicht aufnimmt?

247
00:14:37,780 --> 00:14:41,083
Nun, wenn wir das tatsächlich für die Gewichtungen machen, 

248
00:14:41,083 --> 00:14:45,450
die mit diesen Übergängen von der ersten Schicht zur nächsten verbunden sind, 

249
00:14:45,450 --> 00:14:49,313
sehen sie, anstatt hier und da isolierte kleine Kanten aufzugreifen, 

250
00:14:49,313 --> 00:14:53,680
fast zufällig aus, nur mit einigen sehr lockeren Mustern darin die Mitte dort.

251
00:14:53,760 --> 00:14:57,810
Es scheint, dass unser Netzwerk in dem unvorstellbar großen 13.000-dimensionalen 

252
00:14:57,810 --> 00:15:01,660
Raum möglicher Gewichtungen und Verzerrungen ein glückliches kleines lokales 

253
00:15:01,660 --> 00:15:05,560
Minimum gefunden hat, das trotz der erfolgreichen Klassifizierung der meisten 

254
00:15:05,560 --> 00:15:08,960
Bilder nicht genau die Muster aufgreift, die wir uns erhofft hatten.

255
00:15:09,780 --> 00:15:11,993
Und um diesen Punkt wirklich zu verdeutlichen, beobachten Sie, 

256
00:15:11,993 --> 00:15:13,820
was passiert, wenn Sie ein zufälliges Bild eingeben.

257
00:15:14,320 --> 00:15:17,117
Wenn das System intelligent wäre, könnte man erwarten, 

258
00:15:17,117 --> 00:15:21,289
dass es sich unsicher anfühlt und möglicherweise keines dieser 10 Ausgabeneuronen 

259
00:15:21,289 --> 00:15:24,138
wirklich aktiviert oder sie alle gleichmäßig aktiviert, 

260
00:15:24,138 --> 00:15:27,801
sondern dass es Ihnen stattdessen souverän eine unsinnige Antwort gibt, 

261
00:15:27,801 --> 00:15:31,870
als ob es sich genauso sicher anfühlt wie dieses zufällige Geräusch ist eine 5, 

262
00:15:31,870 --> 00:15:34,160
da ein tatsächliches Bild einer 5 eine 5 ist.

263
00:15:34,540 --> 00:15:38,613
Anders ausgedrückt: Auch wenn dieses Netzwerk Ziffern ziemlich gut erkennen kann, 

264
00:15:38,613 --> 00:15:40,700
hat es keine Ahnung, wie man sie zeichnet.

265
00:15:41,420 --> 00:15:43,349
Das liegt zum großen Teil daran, dass es sich um 

266
00:15:43,349 --> 00:15:45,240
einen so eng begrenzten Trainingsaufbau handelt.

267
00:15:45,880 --> 00:15:47,740
Ich meine, versetzen Sie sich hier in die Lage des Netzwerks.

268
00:15:48,140 --> 00:15:51,718
Aus seiner Sicht besteht das gesamte Universum nur aus klar definierten, 

269
00:15:51,718 --> 00:15:55,051
unbeweglichen Ziffern, die in einem winzigen Raster zentriert sind, 

270
00:15:55,051 --> 00:15:57,550
und seine Kostenfunktion gab ihm nie einen Anreiz, 

271
00:15:57,550 --> 00:16:01,080
bei seinen Entscheidungen etwas anderes als völliges Vertrauen zu haben.

272
00:16:02,120 --> 00:16:05,473
Da dies also ein Bild davon ist, was diese Neuronen der zweiten Schicht wirklich tun, 

273
00:16:05,473 --> 00:16:08,710
fragen Sie sich vielleicht, warum ich dieses Netzwerk mit der Motivation einführe, 

274
00:16:08,710 --> 00:16:09,920
Kanten und Muster aufzugreifen.

275
00:16:09,920 --> 00:16:12,300
Ich meine, das ist einfach überhaupt nicht das, was es letztendlich tut.

276
00:16:13,380 --> 00:16:17,180
Nun, dies soll nicht unser Endziel sein, sondern vielmehr ein Ausgangspunkt.

277
00:16:17,640 --> 00:16:20,255
Ehrlich gesagt handelt es sich hierbei um eine alte Technologie, 

278
00:16:20,255 --> 00:16:23,514
wie sie in den 80er und 90er Jahren erforscht wurde, und man muss sie verstehen, 

279
00:16:23,514 --> 00:16:25,888
bevor man detailliertere moderne Varianten verstehen kann, 

280
00:16:25,888 --> 00:16:28,865
und sie ist eindeutig in der Lage, einige interessante Probleme zu lösen, 

281
00:16:28,865 --> 00:16:32,205
aber je mehr man sich mit dem beschäftigt, was Je mehr diese verborgenen Schichten 

282
00:16:32,205 --> 00:16:34,740
wirklich funktionieren, desto weniger intelligent erscheint es.

283
00:16:38,480 --> 00:16:41,398
Wenn Sie den Fokus für einen Moment von der Art und Weise, wie Netzwerke lernen, 

284
00:16:41,398 --> 00:16:43,777
auf die Art und Weise verlagern, wie Sie lernen, gelingt das nur, 

285
00:16:43,777 --> 00:16:46,300
wenn Sie sich irgendwie aktiv mit dem Material hier auseinandersetzen.

286
00:16:47,060 --> 00:16:51,573
Ich möchte, dass Sie ganz einfach jetzt innehalten und einen Moment tief darüber 

287
00:16:51,573 --> 00:16:56,031
nachdenken, welche Änderungen Sie an diesem System vornehmen könnten und wie es 

288
00:16:56,031 --> 00:17:00,880
Bilder wahrnimmt, wenn Sie möchten, dass es Dinge wie Kanten und Muster besser erkennt.

289
00:17:01,479 --> 00:17:04,947
Aber noch besser: Um sich tatsächlich mit dem Material auseinanderzusetzen, 

290
00:17:04,947 --> 00:17:08,826
empfehle ich wärmstens das Buch von Michael Nielsen über Deep Learning und neuronale 

291
00:17:08,826 --> 00:17:09,099
Netze.

292
00:17:09,680 --> 00:17:13,922
Darin finden Sie den Code und die Daten zum Herunterladen und Spielen für genau dieses 

293
00:17:13,922 --> 00:17:18,067
Beispiel, und das Buch führt Sie Schritt für Schritt durch die Funktionsweise dieses 

294
00:17:18,067 --> 00:17:18,359
Codes.

295
00:17:19,300 --> 00:17:22,544
Das Tolle daran ist, dass dieses Buch kostenlos und öffentlich verfügbar ist. 

296
00:17:22,544 --> 00:17:24,956
Wenn Sie also etwas davon haben, denken Sie darüber nach, 

297
00:17:24,956 --> 00:17:27,660
gemeinsam mit mir eine Spende für Nielsens Bemühungen zu leisten.

298
00:17:27,660 --> 00:17:31,052
Ich habe in der Beschreibung auch ein paar andere Ressourcen verlinkt, 

299
00:17:31,052 --> 00:17:33,967
die mir sehr gefallen, darunter den phänomenalen und schönen 

300
00:17:33,967 --> 00:17:36,500
Blogbeitrag von Chris Ola und die Artikel in Distill.

301
00:17:38,280 --> 00:17:41,025
Um die letzten paar Minuten abzuschließen, möchte ich noch einmal auf einen 

302
00:17:41,025 --> 00:17:43,880
Ausschnitt aus dem Interview zurückkommen, das ich mit Leisha Lee geführt habe.

303
00:17:44,300 --> 00:17:46,010
Vielleicht erinnern Sie sich an sie aus dem letzten Video, 

304
00:17:46,010 --> 00:17:47,720
sie hat ihre Doktorarbeit im Bereich Deep Learning gemacht.

305
00:17:48,300 --> 00:17:51,326
In diesem kleinen Ausschnitt spricht sie über zwei aktuelle Arbeiten, 

306
00:17:51,326 --> 00:17:54,958
die sich intensiv damit befassen, wie einige der moderneren Bilderkennungsnetzwerke 

307
00:17:54,958 --> 00:17:55,780
tatsächlich lernen.

308
00:17:56,120 --> 00:17:58,569
Nur um zu verdeutlichen, wo wir uns in der Konversation befinden: 

309
00:17:58,569 --> 00:18:01,873
In der ersten Arbeit wurde eines dieser besonders tiefen neuronalen Netzwerke verwendet, 

310
00:18:01,873 --> 00:18:04,731
das wirklich gut in der Bilderkennung ist, und statt es anhand eines richtig 

311
00:18:04,731 --> 00:18:07,886
beschrifteten Datensatzes zu trainieren, wurden alle Beschriftungen vor dem Training 

312
00:18:07,886 --> 00:18:08,740
durcheinander gebracht.

313
00:18:09,480 --> 00:18:12,605
Offensichtlich war die Testgenauigkeit hier nicht besser als zufällig, 

314
00:18:12,605 --> 00:18:15,994
da alles nur zufällig beschriftet ist, aber es konnte immer noch die gleiche 

315
00:18:15,994 --> 00:18:19,691
Trainingsgenauigkeit erreicht werden, die Sie mit einem ordnungsgemäß beschrifteten 

316
00:18:19,691 --> 00:18:20,880
Datensatz erreichen würden.

317
00:18:21,600 --> 00:18:26,013
Im Grunde reichten die Millionen von Gewichten für dieses spezielle Netzwerk aus, 

318
00:18:26,013 --> 00:18:29,780
um sich lediglich die Zufallsdaten zu merken, was die Frage aufwirft, 

319
00:18:29,780 --> 00:18:33,278
ob die Minimierung dieser Kostenfunktion tatsächlich irgendeiner 

320
00:18:33,278 --> 00:18:36,400
Struktur im Bild entspricht oder nur eine Speicherung ist.

321
00:18:51,440 --> 00:18:56,580
Wenn Sie sich diese Genauigkeitskurve ansehen und nur mit einem zufälligen 

322
00:18:56,580 --> 00:19:01,584
Datensatz trainieren, sinkt diese Kurve sehr langsam und fast linear ab, 

323
00:19:01,584 --> 00:19:07,616
sodass Sie wirklich Schwierigkeiten haben, die lokalen Minima des Möglichen zu finden , 

324
00:19:07,616 --> 00:19:12,140
die richtigen Gewichte, mit denen Sie diese Genauigkeit erreichen.

325
00:19:12,240 --> 00:19:15,780
Wenn Sie dagegen tatsächlich mit einem strukturierten Datensatz trainieren, 

326
00:19:15,780 --> 00:19:19,647
der die richtigen Beschriftungen hat, müssen Sie am Anfang ein wenig herumfummeln, 

327
00:19:19,647 --> 00:19:22,070
aber dann sind Sie ziemlich schnell zurückgefallen, 

328
00:19:22,070 --> 00:19:25,937
um dieses Genauigkeitsniveau zu erreichen, und so ist es in gewissem Sinne auch so 

329
00:19:25,937 --> 00:19:28,220
war es einfacher, diese lokalen Maxima zu finden.

330
00:19:28,540 --> 00:19:32,719
Und was daran auch interessant war, ist, dass es ein weiteres Papier von vor 

331
00:19:32,719 --> 00:19:36,572
ein paar Jahren ans Licht bringt, das viel mehr Vereinfachungen zu den 

332
00:19:36,572 --> 00:19:40,263
Netzwerkschichten enthält, aber eines der Ergebnisse war, dass man, 

333
00:19:40,263 --> 00:19:44,116
wenn man sich die Optimierungslandschaft anschaut, Die lokalen Minima, 

334
00:19:44,116 --> 00:19:48,566
die diese Netzwerke normalerweise lernen, sind tatsächlich von gleicher Qualität. 

335
00:19:48,566 --> 00:19:52,746
Wenn Ihr Datensatz also strukturiert ist, sollten Sie dies in gewisser Weise 

336
00:19:52,746 --> 00:19:54,320
viel einfacher finden können.

337
00:19:58,160 --> 00:20:01,180
Mein Dank gilt wie immer allen, die Patreon unterstützen.

338
00:20:01,520 --> 00:20:04,315
Ich habe bereits gesagt, was für ein Game-Changer Patreon ist, 

339
00:20:04,315 --> 00:20:06,800
aber diese Videos wären ohne Sie wirklich nicht möglich.

340
00:20:07,460 --> 00:20:10,182
Ich möchte mich auch besonders bei der VC-Firma Amplify Partners 

341
00:20:10,182 --> 00:20:12,780
für die Unterstützung dieser ersten Videos der Serie bedanken.

