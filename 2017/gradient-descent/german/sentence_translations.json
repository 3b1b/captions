[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "Im letzten Video habe ich dir die Struktur eines neuronalen Netzes erklärt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "Ich fasse das hier kurz zusammen, damit wir es noch im Kopf haben, und dann habe ich zwei Ziele für dieses Video.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "Die erste besteht darin, die Idee des Gradientenabstiegs vorzustellen, die nicht nur der Art und Weise zugrunde liegt, wie neuronale Netze lernen, sondern auch, wie viele andere maschinelle Lernverfahren funktionieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "Danach werden wir uns etwas genauer ansehen, wie dieses spezielle Netzwerk funktioniert und wonach die versteckten Schichten der Neuronen letztendlich suchen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "Zur Erinnerung: Unser Ziel ist das klassische Beispiel der handschriftlichen Ziffernerkennung, die Hallo-Welt der neuronalen Netze.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "Diese Ziffern werden auf einem 28x28 Pixel großen Raster dargestellt, wobei jedes Pixel einen Graustufenwert zwischen 0 und 1 hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "Diese bestimmen die Aktivierungen von 784 Neuronen in der Eingabeschicht des Netzwerks.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "Die Aktivierung jedes Neurons in den folgenden Schichten basiert dann auf einer gewichteten Summe aller Aktivierungen in der vorherigen Schicht plus einer speziellen Zahl, die Bias genannt wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "Dann fügst du diese Summe mit einer anderen Funktion zusammen, z. B. der sigmoiden Squishifikation oder einer Relu, wie ich es im letzten Video erklärt habe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "Insgesamt hat das Netzwerk bei der etwas willkürlichen Wahl von zwei versteckten Schichten mit je 16 Neuronen etwa 13.000 Gewichte und Verzerrungen, die wir anpassen können, und diese Werte bestimmen, was das Netzwerk tatsächlich tut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "Wenn wir also sagen, dass dieses Netzwerk eine bestimmte Ziffer klassifiziert, bedeutet das, dass das hellste dieser 10 Neuronen in der letzten Schicht dieser Ziffer entspricht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "Und denk daran, dass die Motivation für die Schichtstruktur darin bestand, dass die zweite Schicht vielleicht die Kanten erkennt, die dritte Schicht Muster wie Schleifen und Linien und die letzte Schicht diese Muster einfach zusammensetzen kann, um Ziffern zu erkennen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "Hier lernen wir also, wie das Netzwerk lernt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "Was wir wollen, ist ein Algorithmus, bei dem du diesem Netzwerk eine ganze Reihe von Trainingsdaten zeigen kannst, die in Form von verschiedenen Bildern handgeschriebener Ziffern und Beschriftungen vorliegen, und es passt diese 13.000 Gewichte und Verzerrungen an, um seine Leistung bei den Trainingsdaten zu verbessern.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "Es ist zu hoffen, dass sich das Gelernte dank dieser mehrschichtigen Struktur auch auf Bilder außerhalb der Trainingsdaten übertragen lässt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "Nachdem du das Netzwerk trainiert hast, zeigst du ihm weitere beschriftete Daten, die es noch nie zuvor gesehen hat, und du siehst, wie genau es diese neuen Bilder klassifiziert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "Zum Glück für uns, und das macht dieses Beispiel so alltäglich, haben die guten Leute hinter der MNIST-Datenbank eine Sammlung von Zehntausenden von handgeschriebenen Ziffernbildern zusammengestellt, die jeweils mit den Zahlen beschriftet sind, die sie darstellen sollen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "Und so provokant es auch ist, eine Maschine als lernend zu bezeichnen, sobald du siehst, wie sie funktioniert, fühlt es sich weniger wie eine verrückte Science-Fiction-Prämisse an, sondern viel mehr wie eine Rechenübung.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "Ich meine, im Grunde geht es darum, das Minimum einer bestimmten Funktion zu finden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "Denke daran, dass jedes Neuron mit allen Neuronen der vorherigen Schicht verbunden ist. Die Gewichte in der gewichteten Summe, die die Aktivierung definieren, sind so etwas wie die Stärke dieser Verbindungen, und der Bias ist ein Hinweis darauf, ob das Neuron eher aktiv oder inaktiv ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "Für den Anfang werden wir alle Gewichte und Verzerrungen völlig zufällig initialisieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "Es ist unnötig zu erwähnen, dass dieses Netzwerk bei einem bestimmten Trainingsbeispiel ziemlich schlecht abschneiden wird, da es einfach etwas Zufälliges tut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "Du gibst zum Beispiel dieses Bild einer 3 ein, und die Ausgabeschicht sieht einfach nur unordentlich aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "Du definierst also eine Kostenfunktion, mit der du dem Computer sagst, dass die Ausgabe für die meisten Neuronen eine Aktivierung von 0 haben soll, für dieses Neuron aber eine Aktivierung von 1. Was du mir gegeben hast, ist völliger Müll.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "Mathematisch ausgedrückt: Du addierst die Quadrate der Differenzen zwischen den einzelnen Ausgangsaktivierungen des Mülls und dem Wert, den du haben willst, und das nennen wir dann die Kosten für ein einziges Trainingsbeispiel.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "Beachte, dass diese Summe klein ist, wenn das Netzwerk das Bild sicher richtig klassifiziert, aber groß, wenn das Netzwerk nicht zu wissen scheint, was es tut.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "Du musst also die durchschnittlichen Kosten aller zehntausenden von Ausbildungsbeispielen, die dir zur Verfügung stehen, berücksichtigen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "Diese Durchschnittskosten sind unser Maßstab dafür, wie lausig das Netzwerk ist und wie schlecht sich der Computer fühlen sollte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "Und das ist eine komplizierte Sache.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "Erinnerst du dich daran, dass das Netzwerk selbst im Grunde eine Funktion war, die 784 Zahlen als Eingaben, die Pixelwerte, aufnimmt und 10 Zahlen als Ausgabe ausgibt, und dass es in gewisser Weise durch all diese Gewichte und Verzerrungen parametrisiert ist?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "Nun, die Kostenfunktion ist eine zusätzliche Komplexitätsschicht, die noch dazu kommt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "Es nimmt diese etwa 13.000 Gewichte und Verzerrungen als Eingabe und spuckt eine einzige Zahl aus, die beschreibt, wie schlecht diese Gewichte und Verzerrungen sind, und die Art und Weise, wie sie definiert ist, hängt vom Verhalten des Netzwerks in all den Zehntausenden von Trainingsdaten ab.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "Da gibt es viel zu bedenken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "Aber dem Computer nur zu sagen, wie schlecht er arbeitet, ist nicht sehr hilfreich.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "Du willst ihm sagen, wie es diese Gewichte und Vorurteile ändern soll, damit es besser wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "Um es einfacher zu machen, statt dir eine Funktion mit 13.000 Eingängen vorzustellen, stell dir einfach eine einfache Funktion vor, die eine Zahl als Eingang und eine Zahl als Ausgang hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "Wie kannst du eine Eingabe finden, die den Wert dieser Funktion minimiert?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "Kalkulationsschüler wissen, dass man das Minimum manchmal explizit berechnen kann, aber das ist bei wirklich komplizierten Funktionen nicht immer möglich, schon gar nicht in der 13.000-Eingabe-Version dieser Situation für unsere verrückte, komplizierte Kostenfunktion des neuronalen Netzes.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "Eine flexiblere Taktik ist es, bei einem beliebigen Input zu beginnen und herauszufinden, in welche Richtung du gehen musst, um den Output zu senken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "Wenn du die Steigung der Funktion herausfinden kannst, verschiebe sie nach links, wenn die Steigung positiv ist, und verschiebe die Eingabe nach rechts, wenn die Steigung negativ ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "Wenn du dies wiederholt tust, indem du an jedem Punkt die neue Steigung prüfst und den entsprechenden Schritt machst, wirst du dich einem lokalen Minimum der Funktion nähern.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "Das Bild, das du vielleicht im Kopf hast, ist eine Kugel, die einen Hügel hinunterrollt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "Beachte, dass es selbst für diese stark vereinfachte Funktion mit nur einer Eingabe viele mögliche Täler gibt, in denen du landen kannst, je nachdem, mit welcher zufälligen Eingabe du beginnst, und dass es keine Garantie dafür gibt, dass das lokale Minimum, in dem du landest, auch der kleinstmögliche Wert der Kostenfunktion sein wird.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "Das wird sich auch auf unser neuronales Netzwerk übertragen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "Außerdem solltest du beachten, dass die Schrittgröße proportional zur Steigung ist. Wenn die Steigung zum Minimum hin abflacht, werden deine Schritte immer kleiner und das hilft dir, nicht zu weit zu gehen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "Um die Komplexität ein wenig zu erhöhen, stell dir stattdessen eine Funktion mit zwei Eingängen und einem Ausgang vor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "Du kannst dir den Eingaberaum als die xy-Ebene und die Kostenfunktion als eine Fläche darüber vorstellen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "Anstatt nach der Steigung der Funktion zu fragen, musst du fragen, in welche Richtung du in diesem Eingaberaum gehen musst, um die Ausgabe der Funktion am schnellsten zu verringern.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "Mit anderen Worten: In welche Richtung geht es bergab?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "Auch hier ist es hilfreich, sich eine Kugel vorzustellen, die den Hügel hinunterrollt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "Diejenigen unter euch, die mit der multivariablen Infinitesimalrechnung vertraut sind, wissen, dass die Steigung einer Funktion die Richtung des steilsten Anstiegs angibt, d.h. in welche Richtung du gehen musst, um die Funktion am schnellsten zu steigern.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "Wenn du den negativen Wert dieser Steigung nimmst, erhältst du natürlich die Richtung, in der die Funktion am schnellsten abnimmt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "Mehr noch: Die Länge dieses Neigungsvektors ist ein Hinweis darauf, wie steil der steilste Hang ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "Wenn du dich mit der Mehrgrößenrechnung nicht auskennst und mehr darüber erfahren möchtest, schau dir die Arbeit an, die ich für die Khan Academy zu diesem Thema geschrieben habe.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "Ehrlich gesagt, ist für dich und mich im Moment nur wichtig, dass es im Prinzip eine Möglichkeit gibt, diesen Vektor zu berechnen, der dir sagt, in welche Richtung es bergab geht und wie steil es ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "Du wirst schon klarkommen, wenn das alles ist, was du weißt, und du dich nicht mit den Details auskennst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "Wenn du das kannst, besteht der Algorithmus zur Minimierung der Funktion darin, diese Gradientenrichtung zu berechnen, dann einen kleinen Schritt bergab zu machen und das immer wieder zu wiederholen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "Es ist die gleiche Grundidee für eine Funktion, die 13.000 Eingänge statt 2 Eingänge hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "Stell dir vor, du organisierst alle 13.000 Gewichte und Verzerrungen unseres Netzwerks in einem riesigen Spaltenvektor.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "Die negative Steigung der Kostenfunktion ist nur ein Vektor, eine Richtung in diesem wahnsinnig großen Eingaberaum, die dir sagt, welche Änderungen an all diesen Zahlen den schnellsten Rückgang der Kostenfunktion zur Folge haben werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "Und mit unserer speziell entwickelten Kostenfunktion bedeutet eine Änderung der Gewichte und Verzerrungen, um sie zu verringern, dass die Ausgabe des Netzes bei jedem Teil der Trainingsdaten weniger wie eine zufällige Anordnung von 10 Werten aussieht, sondern eher wie eine tatsächliche Entscheidung, die wir treffen wollen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "Es ist wichtig, sich daran zu erinnern, dass es sich bei dieser Kostenfunktion um einen Durchschnitt über alle Trainingsdaten handelt. Wenn du sie also minimierst, bedeutet das, dass die Leistung bei all diesen Stichproben besser ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "Der Algorithmus zur effizienten Berechnung dieses Gradienten, der das Herzstück des Lernprozesses eines neuronalen Netzes ist, heißt Backpropagation und ich werde im nächsten Video darüber sprechen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "Dort möchte ich mir wirklich die Zeit nehmen, durchzugehen, was genau mit den einzelnen Gewichten und Verzerrungen für ein bestimmtes Stück Trainingsdaten passiert, und versuchen, ein intuitives Gefühl dafür zu vermitteln, was jenseits des Haufens relevanter Berechnungen und Formeln passiert.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "Unabhängig von den Implementierungsdetails möchte ich, dass du weißt, dass wir, wenn wir von einem lernenden Netzwerk sprechen, einfach nur eine Kostenfunktion minimieren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "Eine Folge davon ist, dass es wichtig ist, dass diese Kostenfunktion einen schönen glatten Ausgang hat, damit wir ein lokales Minimum finden können, indem wir kleine Schritte bergab machen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "Das ist übrigens auch der Grund, warum künstliche Neuronen kontinuierlich aktiv sind und nicht einfach nur binär aktiv oder inaktiv, wie biologische Neuronen es sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "Dieser Prozess, bei dem die Eingabe einer Funktion wiederholt um ein Vielfaches des negativen Gradienten verschoben wird, heißt Gradientenabstieg.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "Es ist ein Weg, um zu einem lokalen Minimum einer Kostenfunktion zu konvergieren, im Grunde ein Tal in diesem Graphen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "Ich zeige natürlich immer noch das Bild einer Funktion mit zwei Eingängen, weil Nudges in einem 13.000-dimensionalen Eingaberaum etwas schwer zu verstehen sind, aber es gibt eine schöne nicht-räumliche Art, darüber nachzudenken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "Jede Komponente des negativen Gradienten sagt uns zwei Dinge.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "Das Vorzeichen sagt uns natürlich, ob die entsprechende Komponente des Eingangsvektors nach oben oder unten geschoben werden soll.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "Wichtig ist aber, dass die relative Größe all dieser Komponenten dir zeigt, welche Veränderungen wichtiger sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "Du siehst, dass in unserem Netzwerk die Anpassung eines der Gewichte einen viel größeren Einfluss auf die Kostenfunktion haben kann als die Anpassung eines anderen Gewichts.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "Einige dieser Verbindungen sind für unsere Trainingsdaten einfach wichtiger.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "Du kannst dir den Gradientenvektor unserer gigantischen Kostenfunktion so vorstellen, dass er die relative Wichtigkeit der einzelnen Gewichtungen und Verzerrungen kodiert, d.h. welche dieser Änderungen den größten Nutzen für dich hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "Das ist wirklich nur eine andere Art, über die Richtung nachzudenken.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "Um ein einfacheres Beispiel zu nehmen: Wenn du eine Funktion mit zwei Variablen als Eingabe hast und berechnest, dass ihre Steigung an einem bestimmten Punkt 3,1 beträgt, kannst du das einerseits so interpretieren, dass die Funktion am schnellsten steigt, wenn du an der Eingabe stehst, und dass, wenn du die Funktion über der Ebene der Eingabepunkte grafisch darstellst, dieser Vektor dir die gerade Aufwärtsrichtung gibt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "Man kann das aber auch so interpretieren, dass Änderungen an der ersten Variable dreimal so wichtig sind wie Änderungen an der zweiten Variable, dass also zumindest in der Nähe des relevanten Inputs die Änderung des x-Wertes viel mehr bringt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "Lasst uns herauszoomen und zusammenfassen, wo wir bisher stehen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "Das Netzwerk selbst ist diese Funktion mit 784 Eingängen und 10 Ausgängen, die durch all diese gewichteten Summen definiert sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "Die Kostenfunktion ist eine weitere Ebene der Komplexität obendrauf.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "Es nimmt die 13.000 Gewichte und Verzerrungen als Eingaben und spuckt auf der Grundlage der Trainingsbeispiele ein einziges Maß für die Lousiness aus.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "Und der Gradient der Kostenfunktion ist noch eine weitere Ebene der Komplexität.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "Sie sagt uns, welche Änderungen an all diesen Gewichten und Verzerrungen die schnellste Veränderung des Wertes der Kostenfunktion bewirken, was du so interpretieren könntest, dass die Änderungen an den Gewichten am wichtigsten sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "Wenn du also das Netzwerk mit zufälligen Gewichten und Verzerrungen initialisierst und sie viele Male auf der Grundlage dieses Gradientenabstiegsprozesses anpasst, wie gut schneidet es dann bei Bildern ab, die es noch nie zuvor gesehen hat?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "Die hier beschriebene Lösung mit zwei versteckten Schichten von je 16 Neuronen, die hauptsächlich aus ästhetischen Gründen gewählt wurde, ist nicht schlecht und klassifiziert etwa 96 % der neuen Bilder richtig.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "Und ehrlich gesagt, wenn du dir einige der Beispiele ansiehst, bei denen er Mist gebaut hat, bist du gezwungen, ein bisschen nachsichtig zu sein.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "Wenn du jetzt mit der Struktur der verborgenen Schicht herumspielst und ein paar Änderungen vornimmst, kannst du den Wert auf 98% erhöhen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "Und das ist ziemlich gut!",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "Es ist nicht das beste, du kannst sicherlich eine bessere Leistung erzielen, wenn du dich mehr anstrengst, aber wenn man bedenkt, wie gewaltig die anfängliche Aufgabe ist, finde ich es unglaublich, dass ein Netzwerk bei Bildern, die es noch nie zuvor gesehen hat, so gut abschneidet, da wir ihm nie gesagt haben, nach welchen Mustern es suchen soll.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "Ursprünglich habe ich diese Struktur damit begründet, dass wir hoffen, dass die zweite Schicht kleine Kanten erkennt, dass die dritte Schicht diese Kanten zusammensetzt, um Schleifen und längere Linien zu erkennen, und dass diese zusammengesetzt werden können, um Ziffern zu erkennen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "Ist es das, was unser Netzwerk tatsächlich tut?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "Nun, zumindest in diesem Fall nicht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "Weißt du noch, wie wir im letzten Video gesehen haben, wie die Gewichte der Verbindungen von allen Neuronen der ersten Schicht zu einem bestimmten Neuron der zweiten Schicht als ein bestimmtes Pixelmuster visualisiert werden können, das das Neuron der zweiten Schicht aufnimmt?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "Wenn wir das für die Gewichte tun, die mit den Übergängen von der ersten Schicht zur nächsten verbunden sind, sehen sie, anstatt vereinzelte kleine Kanten hier und da aufzugreifen, fast zufällig aus, nur mit einigen sehr lockeren Mustern in der Mitte.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "Es scheint, dass unser Netzwerk in dem unvorstellbar großen 13.000-dimensionalen Raum möglicher Gewichtungen und Verzerrungen ein glückliches kleines lokales Minimum gefunden hat, das zwar die meisten Bilder erfolgreich klassifiziert, aber nicht genau die Muster erkennt, auf die wir gehofft haben.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "Und um diesen Punkt wirklich zu verdeutlichen, schau dir an, was passiert, wenn du ein zufälliges Bild eingibst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "Wenn das System schlau wäre, könntest du erwarten, dass es sich unsicher fühlt und vielleicht keines der 10 Ausgangsneuronen wirklich aktiviert oder sie alle gleichmäßig aktiviert, aber stattdessen gibt es dir selbstbewusst eine unsinnige Antwort, als ob es sich genauso sicher ist, dass dieses zufällige Geräusch eine 5 ist, wie es sich sicher ist, dass ein tatsächliches Bild einer 5 eine 5 ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "Anders ausgedrückt: Auch wenn dieses Netzwerk Ziffern ziemlich gut erkennen kann, hat es keine Ahnung, wie es sie zeichnen soll.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "Das liegt zum großen Teil daran, dass die Ausbildung so stark eingeschränkt ist.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "Ich meine, versetz dich in die Lage des Netzwerks.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "Aus seiner Sicht besteht das gesamte Universum aus nichts anderem als klar definierten, unbeweglichen Ziffern, die in einem winzigen Raster zentriert sind, und seine Kostenfunktion hat ihm nie einen Anreiz gegeben, etwas anderes als absolut zuversichtlich in seinen Entscheidungen zu sein.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "Wenn du dir also vorstellst, was die Neuronen der zweiten Schicht wirklich tun, fragst du dich vielleicht, warum ich dieses Netzwerk mit der Motivation einführe, Kanten und Muster aufzuspüren.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "Ich meine, das ist überhaupt nicht das, was es am Ende macht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "Nun, das soll nicht unser Endziel sein, sondern ein Startpunkt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "Ehrlich gesagt handelt es sich um eine alte Technologie, die in den 80er und 90er Jahren erforscht wurde, und du musst sie verstehen, bevor du detailliertere moderne Varianten verstehen kannst.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "Wenn wir den Fokus für einen Moment von der Art und Weise, wie Netzwerke lernen, auf die Art und Weise, wie du lernst, verlagern, dann wird das nur passieren, wenn du dich aktiv mit dem Stoff auseinandersetzt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "Eine ganz einfache Sache, die ich dir ans Herz legen möchte, ist, jetzt innezuhalten und einen Moment darüber nachzudenken, welche Änderungen du an diesem System und an der Art und Weise, wie es Bilder wahrnimmt, vornehmen könntest, wenn du möchtest, dass es Dinge wie Kanten und Muster besser erkennt.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "Aber um sich wirklich mit der Materie zu beschäftigen, empfehle ich dir das Buch von Michael Nielsen über Deep Learning und neuronale Netze.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "Darin findest du den Code und die Daten, die du für genau dieses Beispiel herunterladen und ausprobieren kannst, und das Buch führt dich Schritt für Schritt durch den Code.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "Das Tolle ist, dass dieses Buch kostenlos und öffentlich zugänglich ist. Wenn du also etwas damit anfangen kannst, solltest du dich mir anschließen und für Nielsens Arbeit spenden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "Ich habe in der Beschreibung auch ein paar andere Ressourcen verlinkt, die ich sehr mag, darunter den phänomenalen und schönen Blogpost von Chris Ola und die Artikel in Distill.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "Um die letzten Minuten hier abzuschließen, möchte ich noch einmal einen Ausschnitt aus dem Interview mit Leisha Lee zeigen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "Du erinnerst dich vielleicht noch an sie aus dem letzten Video, sie hat ihre Doktorarbeit im Bereich Deep Learning gemacht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "In diesem kleinen Ausschnitt spricht sie über zwei aktuelle Arbeiten, die sich damit befassen, wie moderne Bilderkennungsnetzwerke tatsächlich lernen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "Die erste Arbeit nahm eines dieser besonders tiefen neuronalen Netze, die wirklich gut in der Bilderkennung sind, und anstatt es mit einem richtig beschrifteten Datensatz zu trainieren, wurden alle Beschriftungen vor dem Training vertauscht.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "Natürlich war die Testgenauigkeit hier nicht besser als beim Zufallsprinzip, da alles nur zufällig beschriftet ist, aber es konnte trotzdem die gleiche Trainingsgenauigkeit wie bei einem richtig beschrifteten Datensatz erreicht werden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "Das wirft die Frage auf, ob die Minimierung dieser Kostenfunktion tatsächlich irgendeiner Art von Struktur im Bild entspricht, oder ob es sich nur um ein Auswendiglernen handelt?",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "Wenn du dir die Genauigkeitskurve ansiehst und mit einem zufälligen Datensatz trainierst, geht die Kurve sehr langsam und fast linear nach unten, sodass du wirklich darum kämpfst, ein lokales Minimum möglicher Gewichte zu finden, die dir diese Genauigkeit bringen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "Wenn du hingegen mit einem strukturierten Datensatz trainierst, der die richtigen Labels hat, fummelst du am Anfang ein bisschen herum, aber dann fällst du sehr schnell, um das Genauigkeitsniveau zu erreichen, und so war es in gewisser Weise einfacher, diese lokalen Maxima zu finden.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "Das Interessante daran ist, dass es eine andere Arbeit von vor ein paar Jahren ans Licht bringt, in der die Netzschichten viel einfacher sind, aber eines der Ergebnisse besagt, dass, wenn man sich die Optimierungslandschaft ansieht, die lokalen Minima, die diese Netze lernen, eigentlich von gleicher Qualität sind.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "Mein Dank gilt wie immer denjenigen von euch, die mich auf Patreon unterstützen.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "Ich habe bereits gesagt, wie wichtig Patreon ist, aber diese Videos wären ohne dich nicht möglich.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "Ein besonderer Dank gilt auch der VC-Firma Amplify Partners, die diese ersten Videos der Reihe unterstützt hat.",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]