1
00:00:04,180 --> 00:00:07,280
A legutóbbi videóban bemutattam a neurális hálózat felépítését.

2
00:00:07,680 --> 00:00:10,975
Egy gyors összefoglalóval indítok, hogy felfrissítsem az emlékeiteket. 

3
00:00:10,975 --> 00:00:12,600
Ezután két fő célom van a videóval.

4
00:00:13,100 --> 00:00:15,860
Az első a gradiens ereszkedés fogalmának bemutatása, 

5
00:00:15,860 --> 00:00:18,360
amely nemcsak a neurális hálózatok tanulásának, 

6
00:00:18,360 --> 00:00:20,600
hanem sok más gépi tanulásnak is az alapja.

7
00:00:21,120 --> 00:00:23,817
Ezután egy kicsit mélyebben beleássuk magunkat abba, 

8
00:00:23,817 --> 00:00:27,940
hogyan működik ez a bizonyos hálózat, és mi a rejtett neuronrétegek pontos célja.

9
00:00:28,980 --> 00:00:34,083
Emlékeztetőül, a célunk itt a neurális hálózatok klasszikus példájának megvalósítása, 

10
00:00:34,083 --> 00:00:36,220
a kézzel írt számjegyek felismerése.

11
00:00:37,020 --> 00:00:39,897
Ezeket a számjegyeket egy 28x28 pixeles rácsba rendezzük, 

12
00:00:39,897 --> 00:00:43,420
minden egyes pixelhez 0 és 1 közötti szürkeárnyalatos értéket rendelve.

13
00:00:43,820 --> 00:00:50,040
Ezek határozzák meg a hálózat bemeneti rétegében lévő 784 neuron aktivációját.

14
00:00:51,180 --> 00:00:55,571
Ezután a következő rétegekben az egyes neuronok aktivációja az előző réteg összes 

15
00:00:55,571 --> 00:00:59,320
aktivációjának súlyozott összegén alapul, plusz egy speciális számon, 

16
00:00:59,320 --> 00:01:00,820
az úgynevezett eltolósúlyon.

17
00:01:02,160 --> 00:01:04,900
Utána ezt az összeget képzed valamilyen más függvénnyel, 

18
00:01:04,900 --> 00:01:08,940
például a szigmoid tömörítéssel vagy ReLu-val, ahogy a múltkori videóban bemutattam.

19
00:01:09,480 --> 00:01:14,427
A kissé önkényesen választott egyenként 16 neuronnal rendelkező két rejtett rétegből 

20
00:01:14,427 --> 00:01:18,385
álló hálózatnak összesen körülbelül 13000 súlya és eltolósúlya van, 

21
00:01:18,385 --> 00:01:22,168
amelyeket be tudunk állítani, és ezek az értékek határozzák meg, 

22
00:01:22,168 --> 00:01:24,380
hogy pontosan mit is csinál a hálózat.

23
00:01:24,880 --> 00:01:28,616
Amikor azt mondjuk, hogy ez a hálózat egy adott számjegyet beazonosít, 

24
00:01:28,616 --> 00:01:33,300
ez alatt az utolsó rétegben lévő 10 neuron közül a legfényesebbhez tartozó számot értjük.

25
00:01:34,100 --> 00:01:37,539
És ne feledjük, hogy a réteges struktúra motivációja az volt, 

26
00:01:37,539 --> 00:01:42,198
hogy talán a második réteg megtalálja az éleket, a harmadik réteg pedig a mintákat, 

27
00:01:42,198 --> 00:01:47,080
például a hurkokat és a vonalakat, az utolsó pedig össze tudja rakni ezeket a mintákat, 

28
00:01:47,080 --> 00:01:48,800
hogy felismerje a számjegyeket.

29
00:01:49,800 --> 00:01:52,240
Most viszont megtudjuk, hogyan tanul a hálózat.

30
00:01:52,640 --> 00:01:57,024
Mi egy olyan algoritmust szeretnénk, ahol megmutathatunk ennek a hálózatnak 

31
00:01:57,024 --> 00:02:01,120
egy csomó minta adatot, amely kézzel írt számjegyek különböző képeinek 

32
00:02:01,120 --> 00:02:04,985
formájában érkezik felcímkézve a rajtuk lévő számjegyekkel együtt, 

33
00:02:04,985 --> 00:02:10,120
és a hálózat beállítja ezt a 13000 súlyt, hogy javítsa a minta adatokon elért eredményét.

34
00:02:10,720 --> 00:02:13,305
Remélhetőleg ez a réteges struktúra képes arra, 

35
00:02:13,305 --> 00:02:16,860
hogy a tanultakat a képzési adatokon kívüli képekre is alkalmazza.

36
00:02:17,640 --> 00:02:20,559
Ezt úgy teszteljük, hogy miután betanítottuk a hálózatot, 

37
00:02:20,559 --> 00:02:24,988
több olyan felcímkézett képet mutatunk neki, amelyet még soha nem látott, és megnézzük, 

38
00:02:24,988 --> 00:02:26,700
milyen pontosan azonosítja ezeket.

39
00:02:31,120 --> 00:02:35,425
Szerencsénkre az MNIST adatbázist létrehozó kedves emberek összeállították ezt 

40
00:02:35,425 --> 00:02:39,077
a több tízezer kézzel írt számjegyes képet tartalmazó gyűjteményt, 

41
00:02:39,077 --> 00:02:42,565
amelyek mindegyike fel van címkézve a rajtuk látható számokkal. 

42
00:02:42,565 --> 00:02:44,200
Emiatt ilyen gyakori ez példa.

43
00:02:44,900 --> 00:02:48,480
És bármennyire is provokatív azt állítani egy gépről, hogy tanul, 

44
00:02:48,480 --> 00:02:53,092
ha egyszer látod hogyan működik, sokkal kevésbé tűnik valami őrült sci-fi regénynek, 

45
00:02:53,092 --> 00:02:55,480
és sokkal inkább egy számítási gyakorlatnak.

46
00:02:56,200 --> 00:02:59,960
Úgy értem, alapvetően egy bizonyos függvény minimumát kell megtalálni.

47
00:03:01,940 --> 00:03:05,747
Ne feledjük, hogy koncepcionálisan úgy gondolunk minden neuronra, 

48
00:03:05,747 --> 00:03:08,978
mint ami az előző réteg összes neuronjához kapcsolódik, 

49
00:03:08,978 --> 00:03:13,363
és az aktivitást meghatározó súlyozott összeg súlyai a kapcsolatok egyfajta 

50
00:03:13,363 --> 00:03:16,248
erősségét jelentik. A eltolósúly pedig azt jelzi, 

51
00:03:16,248 --> 00:03:18,960
hogy az adott neuron inkább aktív vagy inaktív.

52
00:03:19,720 --> 00:03:24,400
És hogy a dolgokat elkezdjük, inicializáljuk az összes súlyt teljesen véletlenszerűen.

53
00:03:24,940 --> 00:03:28,777
Mondanom sem kell, hogy ez a hálózat elég szörnyen fog teljesíteni egy példán, 

54
00:03:28,777 --> 00:03:30,720
mivel csak valami véletlenszerűt csinál.

55
00:03:31,040 --> 00:03:33,666
Például betáplálod ezt a 3-as képet, a kimeneti 

56
00:03:33,666 --> 00:03:36,020
rétegen meg teljes össze-visszaság látható.

57
00:03:36,600 --> 00:03:40,358
Tehát, azt teszed, hogy definiálsz egy költségfüggvényt, egy módot arra, 

58
00:03:40,358 --> 00:03:43,139
hogy megmondd a számítógépnek: Nem! Rossz számítógép! 

59
00:03:43,139 --> 00:03:45,816
A kimenetnek olyan aktivációkkal kell rendelkeznie, 

60
00:03:45,816 --> 00:03:48,957
amelyek a legtöbb neuron esetében 0, de ennél a neuronnál 1, 

61
00:03:48,957 --> 00:03:50,760
amit adtál nekem, az teljes szemét.

62
00:03:51,720 --> 00:03:58,482
Kicsit matekosabban kifejezve, összeadjuk az egyes szemét kimeneti aktivációk és a kívánt 

63
00:03:58,482 --> 00:04:05,020
érték közötti különbségek négyzetét, és ezt nevezzük az adott tréningpélda költségének.

64
00:04:05,960 --> 00:04:10,955
Vegyük észre, hogy ez az összeg kicsi, amikor a hálózat magabiztosan helyesen 

65
00:04:10,955 --> 00:04:16,399
osztályozza a képet, de nagy, amikor a hálózat úgy tűnik, hogy nem tudja, mit csinál.

66
00:04:18,640 --> 00:04:22,159
Így aztán azt kell tenned, hogy a rendelkezésére álló több 

67
00:04:22,159 --> 00:04:25,440
tízezer képzési példa átlagköltségét veszed figyelembe.

68
00:04:27,040 --> 00:04:30,497
Ez az átlagköltség a mi mérőszámunk arra, hogy mennyire pocsék a hálózat, 

69
00:04:30,497 --> 00:04:32,740
és mennyire kell rosszul éreznie magát a gépnek.

70
00:04:33,420 --> 00:04:34,600
És ez egy bonyolult dolog.

71
00:04:35,040 --> 00:04:38,820
Emlékszel, hogy maga a hálózat alapvetően egy függvény volt, 

72
00:04:38,820 --> 00:04:44,337
amely bemenetként 784 számot vesz fel, a pixelértékeket, és kimenetként 10 számot ad ki, 

73
00:04:44,337 --> 00:04:48,800
és bizonyos értelemben a súlyok és eltolósúlyok által van paraméterezve?

74
00:04:49,500 --> 00:04:52,820
Nos, a költségfüggvény egy újabb szinttel növeli a komplexitást.

75
00:04:53,100 --> 00:04:58,814
A rendszer bemenetként veszi ezt a körülbelül 13000 súlyt, és egyetlen számot ad ki, 

76
00:04:58,814 --> 00:05:02,243
amely leírja, hogy mennyire rosszak ezek a súlyok. 

77
00:05:02,243 --> 00:05:07,622
Ennek értéke mindig változik a hálózat viselkedésétől függően a több tízezernyi 

78
00:05:07,622 --> 00:05:08,900
képzési adat során.

79
00:05:09,520 --> 00:05:11,000
Ez sok minden, amit fejben kell tartani.

80
00:05:12,400 --> 00:05:15,820
De csak annyit mondani a számítógépnek, hogy szar munkát végez, nem túl hasznos.

81
00:05:16,220 --> 00:05:20,060
Azt is meg akarod mondani neki, hogyan változtassa meg a súlyokat, hogy jobbá váljon.

82
00:05:20,780 --> 00:05:23,960
A könnyebbség kedvéért, ahelyett, hogy egy 13000 bemenettel 

83
00:05:23,960 --> 00:05:28,253
rendelkező függvényt kellene elképzelnünk, képzeljünk el egy egyszerű függvényt, 

84
00:05:28,253 --> 00:05:30,480
amelynek bemenete és kimenete is egy szám.

85
00:05:31,480 --> 00:05:35,300
Hogyan találjuk meg azt a bemenetet, amely minimalizálja ennek a függvénynek az értékét?

86
00:05:36,460 --> 00:05:41,392
A kalkulust tanuló diákok tudják, hogy a minimumot néha explicit módon is ki lehet 

87
00:05:41,392 --> 00:05:45,374
számolni, de ez nem mindig lehetséges bonyolult függvények esetén, 

88
00:05:45,374 --> 00:05:50,307
pláne nem az őrülten bonyolult neurális hálózati költségfüggvényünk 13000 bemenetű 

89
00:05:50,307 --> 00:05:51,080
változatában.

90
00:05:51,580 --> 00:05:55,525
Rugalmasabb taktika, ha bármilyen bemenetről indulva ki tudjuk találjuk, 

91
00:05:55,525 --> 00:05:59,200
hogy melyik irányba kell lépnünk, hogy a kimenet alacsonyabb legyen.

92
00:06:00,080 --> 00:06:05,048
Pontosabban, ha ki tudod számolni a függvény meredekségét, akkor balra kell mozogni, 

93
00:06:05,048 --> 00:06:09,900
ha a meredekség pozitív, és jobbra kell tolni a bemenetet, ha a meredekség negatív.

94
00:06:11,960 --> 00:06:15,827
Ha ezt többször megismétled minden ponton ellenőrizve a meredekséget és megtéve 

95
00:06:15,827 --> 00:06:19,840
a megfelelő lépést, akkor a függvény valamelyik helyi minimumához fogsz közelíteni.

96
00:06:20,640 --> 00:06:23,800
Ilyenkor gondolj egy dombon lefelé guruló labdára.

97
00:06:24,620 --> 00:06:28,046
Vegyük észre, hogy még ennél a nagyon leegyszerűsített egyváltozós 

98
00:06:28,046 --> 00:06:31,524
függvénynél is sok lehetséges völgyben landolhatunk, attól függően, 

99
00:06:31,524 --> 00:06:34,746
hogy melyik véletlen helyről indulunk, és nincs garancia arra, 

100
00:06:34,746 --> 00:06:38,223
hogy a lokális minimum, ahol landolunk, a költségfüggvény legkisebb 

101
00:06:38,223 --> 00:06:39,400
lehetséges értéke lesz.

102
00:06:40,220 --> 00:06:42,620
Ez a neurális hálózatos esetünkre is igaz lesz.

103
00:06:43,180 --> 00:06:47,924
Azt is szeretném, hogy észrevegyed, ha a lépések méretét a lejtővel arányossá teszed, 

104
00:06:47,924 --> 00:06:52,558
akkor amikor a lejtő a minimum felé ellaposodik, a lépéseid egyre kisebbek lesznek, 

105
00:06:52,558 --> 00:06:54,600
és ez segít a túllövés elkerülésében.

106
00:06:55,940 --> 00:06:58,484
A bonyolultságot egy kicsit megnövelve, képzeljünk 

107
00:06:58,484 --> 00:07:00,980
el egy függvényt két bemenettel és egy kimenettel.

108
00:07:01,500 --> 00:07:04,994
A bemeneti teret az xy-síknak, a költségfüggvényt 

109
00:07:04,994 --> 00:07:08,140
pedig a felette lévő felületnek tekinthetjük.

110
00:07:08,760 --> 00:07:12,976
Most a függvény meredekségének vizsgálata helyett, azt kell megtalálnunk, 

111
00:07:12,976 --> 00:07:15,996
hogy milyen irányba lépjünk ebben a bemeneti térben, 

112
00:07:15,996 --> 00:07:18,960
hogy a függvény kimenete a leggyorsabban csökkenjen.

113
00:07:19,720 --> 00:07:21,760
Más szóval, mi a lejtő iránya?

114
00:07:22,380 --> 00:07:25,560
Ismét hasznos, ha egy golyóra gondolunk, amely legurul a dombon.

115
00:07:26,660 --> 00:07:29,804
Akik ismerik a többváltozós függvényeket, azok tudják, 

116
00:07:29,804 --> 00:07:33,863
hogy egy függvény gradiense megadja a legmeredekebb emelkedés irányát, 

117
00:07:33,863 --> 00:07:38,780
vagyis azt, hogy melyik irányba kell lépni, hogy a kimenetet a leggyorsabban növeljük.

118
00:07:39,560 --> 00:07:42,162
Természetesen, ha a gradiens negatívját vesszük, 

119
00:07:42,162 --> 00:07:46,040
megkapjuk azt a lépésirányt, amely a leggyorsabban csökkenti a függvényt.

120
00:07:47,240 --> 00:07:50,768
Sőt, a meredekségvektor hossza még azt is megmutatja, 

121
00:07:50,768 --> 00:07:53,840
hogy mennyire meredek az a legmeredekebb lejtő.

122
00:07:54,540 --> 00:07:58,029
Ha nem ismered a többváltozós kalkulust, de többet szeretnél megtudni, 

123
00:07:58,029 --> 00:08:00,340
nézd meg a Khan Academy-re készített videóimat.

124
00:08:00,860 --> 00:08:04,107
Őszintén szólva azonban most csak annyi számít nekünk, 

125
00:08:04,107 --> 00:08:07,590
hogy létezik egy mód arra, hogy kiszámítsuk ezt a vektort. 

126
00:08:07,590 --> 00:08:11,900
Azt a vektort, amely megmondja, hogy mi a lejtő iránya és milyen meredek.

127
00:08:12,400 --> 00:08:16,120
Nem gond, ha most csak ennyit tudsz, és nincs jártasságod a részletekben.

128
00:08:17,200 --> 00:08:21,339
Viszont ha ezeket megértetted, akkor a függvény minimalizálásának algoritmusa az, 

129
00:08:21,339 --> 00:08:25,074
hogy kiszámítod ezt a gradiens irányt, majd teszel egy kis lépést lefelé, 

130
00:08:25,074 --> 00:08:26,740
és ezt újra meg újra megismétled.

131
00:08:27,700 --> 00:08:30,366
Ugyanez az alapötlet egy olyan függvény esetében, 

132
00:08:30,366 --> 00:08:32,820
amelynek 2 bemenet helyett 13000 bemenete van.

133
00:08:33,400 --> 00:08:36,372
Képzeljük el, hogy a hálózatunk mind a 13000 súlyát 

134
00:08:36,372 --> 00:08:39,460
és eltolósúlyát egy hatalmas oszlopvektorba rendezzük.

135
00:08:40,140 --> 00:08:43,156
A költségfüggvény negatív gradiense csak egy vektor, 

136
00:08:43,156 --> 00:08:47,709
ami egy irányt mutat ebben az őrülten nagy bemeneti térben, és amely megmondja, 

137
00:08:47,709 --> 00:08:51,294
hogy mely kis változtatásokat kell az összes számon elvégezni, 

138
00:08:51,294 --> 00:08:54,880
ami a leggyorsabb csökkenést fogja okozni a költségfüggvényben.

139
00:08:55,640 --> 00:08:59,473
És természetesen ha a célunknak megfelelő költségfüggvény értékét sikerül 

140
00:08:59,473 --> 00:09:03,359
csökkenteni a súlyok megváltoztatásával, akkor a hálózat kimenete az egyes 

141
00:09:03,359 --> 00:09:07,607
minta adatokra kevésbé fog hasonlítani egy 10 értékből álló véletlenszerű tömbre, 

142
00:09:07,607 --> 00:09:10,820
hanem inkább egy tényleges döntésre, amelyet kapni szeretnénk.

143
00:09:11,440 --> 00:09:14,601
Fontos megjegyezni, hogy ez a költségfüggvény az összes minta 

144
00:09:14,601 --> 00:09:17,967
adatra kapott eredmény átlagát tartalmazza, így ha minimalizálod, 

145
00:09:17,967 --> 00:09:21,180
az azt jelenti, hogy az összes mintán jobb teljesítményt nyújt.

146
00:09:23,820 --> 00:09:26,862
A gradiens hatékony kiszámítására szolgáló algoritmust, 

147
00:09:26,862 --> 00:09:30,013
amely gyakorlatilag a neurális hálózat tanulásának lelke, 

148
00:09:30,013 --> 00:09:33,980
visszaterjesztésnek hívják, és erről fogok beszélni a következő videóban.

149
00:09:34,660 --> 00:09:38,660
Ott arra szeretnék időt szánni, hogy végigvegyük mi történik pontosan az 

150
00:09:38,660 --> 00:09:41,236
egyes súlyokkal egy adott minta adat esetében, 

151
00:09:41,236 --> 00:09:43,866
és megpróbálok intuitív magyarázatot adni arra, 

152
00:09:43,866 --> 00:09:47,100
hogy valójában mi történik a számítások és képletek mögött.

153
00:09:47,780 --> 00:09:50,437
Itt és most az a legfontosabb dolog, amit szeretném, 

154
00:09:50,437 --> 00:09:53,095
ha tudnátok a megvalósítás részleteitől függetlenül, 

155
00:09:53,095 --> 00:09:56,304
hogy amikor a hálózat tanulásáról beszélünk, azt értjük alatta, 

156
00:09:56,304 --> 00:09:58,360
hogy egy költségfüggvényt minimalizálunk.

157
00:09:59,300 --> 00:10:01,736
És vegyük észre, hogy ennek egyik következménye, 

158
00:10:01,736 --> 00:10:04,768
hogy a költségfüggvény kimenetének szép simának kell lennie, 

159
00:10:04,768 --> 00:10:08,100
hogy kis lépésekkel lefelé haladva megtaláljuk a lokális minimumot.

160
00:10:09,260 --> 00:10:14,170
Ez az oka egyébként annak, hogy a mesterséges neuronok aktivációja folytonos skálán 

161
00:10:14,170 --> 00:10:19,140
mozog, és nem egyszerűen binárisan aktívak vagy inaktívak, mint a biológiai neuronok.

162
00:10:20,220 --> 00:10:23,490
Ezt a folyamatot, amely során egy függvény bemenetét ismételten a 

163
00:10:23,490 --> 00:10:26,760
negatív gradienssel változtatják, gradiens ereszkedésnek nevezzük.

164
00:10:27,300 --> 00:10:31,190
Ez egy módja annak, hogy a költségfüggvény egy lokális minimuma felé konvergáljunk, 

165
00:10:31,190 --> 00:10:32,580
ami egy völgy ebben a gráfban.

166
00:10:33,440 --> 00:10:36,821
Természetesen még mindig egy kétváltozós függvény képét mutatom, 

167
00:10:36,821 --> 00:10:40,826
mert a 13000 dimenziós térben a változtatásokat egy kicsit nehéz elképzelni, 

168
00:10:40,826 --> 00:10:44,260
de van egy szép nem térbeli módja annak, hogy erről gondolkodjunk.

169
00:10:45,080 --> 00:10:48,440
A negatív gradiens minden egyes összetevője két dolgot mond el nekünk.

170
00:10:49,060 --> 00:10:52,180
Az előjel természetesen megmondja, hogy a bemeneti vektor 

171
00:10:52,180 --> 00:10:55,140
megfelelő komponensét felfelé vagy lefelé kell-e tolni.

172
00:10:55,800 --> 00:11:00,125
De ami fontos, hogy ezen összetevők relatív nagysága megmutatja, 

173
00:11:00,125 --> 00:11:02,720
hogy mely változások számítanak többet.

174
00:11:05,220 --> 00:11:09,101
Tudod, a mi hálózatunkban az egyik súly kiigazítása sokkal nagyobb 

175
00:11:09,101 --> 00:11:13,040
hatással lehet a költségfüggvényre, mint egy másik súly kiigazítása.

176
00:11:14,800 --> 00:11:18,200
Néhány kapcsolat egyszerűen csak többet számít a minta adataink szempontjából.

177
00:11:19,320 --> 00:11:23,642
Tehát az észbontóan masszív költségfüggvényünk gradiens vektora igazából az 

178
00:11:23,642 --> 00:11:27,622
egyes súlyok és eltolósúlyok relatív fontosságát kódolja, vagyis azt, 

179
00:11:27,622 --> 00:11:32,400
hogy ezek közül a változások közül melyik fogja a legtöbb vizet hajtani a malmunkra.

180
00:11:33,620 --> 00:11:36,640
Ez tényleg csak egy másik módja az irányról való gondolkodásnak.

181
00:11:37,100 --> 00:11:42,167
Egy egyszerűbb példával élve, ha van valamilyen függvényünk két bemeneti változóval, 

182
00:11:42,167 --> 00:11:46,341
és kiszámítjuk, hogy a gradiensére egy adott ponton három-egy jön ki, 

183
00:11:46,341 --> 00:11:50,395
akkor egyrészt ezt úgy értelmezhetjük, hogy ha a bemenetnél állunk, 

184
00:11:50,395 --> 00:11:54,867
akkor az ebben az irányban való mozgás növeli a leggyorsabban a függvényt. 

185
00:11:54,867 --> 00:11:58,801
Így amikor a függvényt a bemeneti pontok síkja felett ábrázoljuk, 

186
00:11:58,801 --> 00:12:02,260
akkor ez a vektor adja az egyenesen felfelé mutató irányt.

187
00:12:02,860 --> 00:12:05,644
De ezt másképp is értelmezhetjük, történetesen, 

188
00:12:05,644 --> 00:12:10,808
hogy az első bemenet változása háromszor olyan fontos, mint a második bemenet változása. 

189
00:12:10,808 --> 00:12:15,507
Tehát legalábbis a releváns bemenetek szomszédságában az x-érték megváltoztatása 

190
00:12:15,507 --> 00:12:16,900
sokkal több hasznot hoz.

191
00:12:19,880 --> 00:12:22,340
Lépjünk kicsit hátrébb és foglaljuk össze, hol tartunk.

192
00:12:22,840 --> 00:12:26,755
Maga a hálózat ez a függvény 784 bemenettel és 10 kimenettel, 

193
00:12:26,755 --> 00:12:30,040
amelyet e súlyozott összegek alapján határozunk meg.

194
00:12:30,640 --> 00:12:33,680
A költségfüggvény egy újabb komplexitási réteg a tetején.

195
00:12:33,980 --> 00:12:37,813
A program bemenete a 13000 súly és a képzési példák, 

196
00:12:37,813 --> 00:12:41,720
amelyek alapján egyetlen pocséksági mérőszámot dob ki.

197
00:12:42,440 --> 00:12:46,900
A költségfüggvény gradiense pedig egy újabb bonyolultsági réteg.

198
00:12:47,360 --> 00:12:50,952
Megmondja, hogy a súlyok és eltolósúlyok milyen változtatásai okozzák 

199
00:12:50,952 --> 00:12:53,723
a leggyorsabb csökkenést a költségfüggvény értékében, 

200
00:12:53,723 --> 00:12:57,880
amit úgy is értelmezhetünk, hogy melyik súlyok változásai számítanak a legtöbbet.

201
00:13:02,560 --> 00:13:05,793
Tehát, ha a hálózatot véletlenszerű súlyokkal inicializáljuk, 

202
00:13:05,793 --> 00:13:09,496
és ezeket a gradiens ereszkedés folyamat során többször is módosítjuk, 

203
00:13:09,496 --> 00:13:13,200
mennyire fog jól teljesít olyan képeken, amelyeket még soha nem látott?

204
00:13:14,100 --> 00:13:19,496
Az eddigiekben bemutatott hálózat, két, egyenként 16 neuronból álló rejtett réteggel, 

205
00:13:19,496 --> 00:13:23,449
amelyet főleg esztétikai okokból választottam: Hát, nem rossz, 

206
00:13:23,449 --> 00:13:25,960
az új képek 96%-át helyesen osztályozza.

207
00:13:26,680 --> 00:13:29,641
És őszintén szólva, ha megnézel néhány példát, 

208
00:13:29,641 --> 00:13:32,540
amit elront, nem igazán tudod érte hibáztatni.

209
00:13:36,220 --> 00:13:38,918
Ha kicsit eljátszadozol a rejtett rétegek felépítésével, 

210
00:13:38,918 --> 00:13:41,760
és végzel néhány finomítást, akkor ezt 98%-ra tudod növelni.

211
00:13:41,760 --> 00:13:42,720
És ez elég jó eredmény!

212
00:13:43,020 --> 00:13:47,474
Biztosan el lehet érni jobb teljesítményt, egy kifinomultabb hálózat használatával, 

213
00:13:47,474 --> 00:13:50,973
de tekintve, hogy mennyire ijesztő a kezdeti feladat, azt hiszem, 

214
00:13:50,973 --> 00:13:55,587
van valami hihetetlen abban, hogy egy hálózat ilyen jól tud teljesíteni olyan képeken, 

215
00:13:55,587 --> 00:13:59,829
amelyeket még soha nem látott, főleg, hogy soha nem mondtuk meg neki konkrétan, 

216
00:13:59,829 --> 00:14:01,420
hogy milyen mintákat keressen.

217
00:14:02,560 --> 00:14:06,281
Eredetileg abban reménykedtem, hogy arra tudom motiválni a hálózatot, 

218
00:14:06,281 --> 00:14:10,162
hogy a második réteg felismeri a kis éleket, a harmadik réteg összerakja 

219
00:14:10,162 --> 00:14:13,883
ezeket az éleket, hogy felismerje a hurkokat és a hosszabb vonalakat, 

220
00:14:13,883 --> 00:14:17,180
és ezeket összeállítva felismerhetőek legyenek a számjegyeket.

221
00:14:17,960 --> 00:14:20,400
De valóban ezt csinálja a hálózatunk?

222
00:14:21,080 --> 00:14:24,400
Nos, ebben az esetben legalábbis egyáltalán nem.

223
00:14:24,820 --> 00:14:27,453
Emlékeztek, hogy az előző videóban azt néztük meg, 

224
00:14:27,453 --> 00:14:31,637
hogy az első réteg összes neuronja és a második réteg egy adott neuronja közötti 

225
00:14:31,637 --> 00:14:34,994
kapcsolatok súlyai hogyan ábrázolhatók egy adott pixelmintaként, 

226
00:14:34,994 --> 00:14:37,060
amelyet a második réteg neuronja felfog?

227
00:14:37,780 --> 00:14:43,235
Nos, amikor ezt alkalmazzuk az első és második réteg közötti átmenetekhez tartozó 

228
00:14:43,235 --> 00:14:47,958
súlyokra, ahelyett, hogy itt-ott elszigetelt kis éleket vennénk észre, 

229
00:14:47,958 --> 00:14:53,680
szinte teljesen véletlenszerűnek tűnnek, csak néhány nagyon elvont mintával a közepén.

230
00:14:53,760 --> 00:14:58,102
Úgy tűnik, hogy a lehetséges súlyok és eltolósúlyok kifürkészhetetlenül nagy, 

231
00:14:58,102 --> 00:15:03,058
13000 dimenziós terében a hálózatunk egy kényelmes kis lokális minimumot talált magának, 

232
00:15:03,058 --> 00:15:06,732
amely annak ellenére, hogy a legtöbb képet sikeresen osztályozza, 

233
00:15:06,732 --> 00:15:08,960
nem éppen a remélt mintákat veszi észre.

234
00:15:09,780 --> 00:15:13,820
És hogy ezt az állításomat igazolni is tudjam, nézd meg, mi történik egy random képpel.

235
00:15:14,320 --> 00:15:19,127
Ha a rendszer okos lenne, akkor azt várnánk, hogy bizonytalannak érezze magát, 

236
00:15:19,127 --> 00:15:23,813
talán nem aktiválva a 10 kimeneti neuron egyikét sem, vagy nem egyenletesen. 

237
00:15:23,813 --> 00:15:29,047
Ehelyett magabiztosan ad valami ostoba választ, mintha ugyanolyan biztos lenne abban, 

238
00:15:29,047 --> 00:15:34,160
hogy ez a véletlenszerű zaj egy 5-ös, mint abban, hogy az 5-ös valódi képe egy 5-ös.

239
00:15:34,540 --> 00:15:38,496
Másképp fogalmazva, még ha ez a hálózat elég jól fel is ismeri a számjegyeket, 

240
00:15:38,496 --> 00:15:40,700
fogalma sincs, hogyan kell őket megrajzolni.

241
00:15:41,420 --> 00:15:45,240
Ez nagyrészt azért van, mert nem ez a tanítás célja.

242
00:15:45,880 --> 00:15:47,740
Úgy értem, képzeld magát a hálózat helyébe.

243
00:15:48,140 --> 00:15:51,130
Az ő szemszögéből nézve az egész világ nem áll másból, 

244
00:15:51,130 --> 00:15:54,446
mint egy apró rácsban elhelyezett mozdulatlan számjegyekből, 

245
00:15:54,446 --> 00:15:58,361
és a költségfüggvénye soha nem ösztönözte arra, hogy bármi mást tegyen, 

246
00:15:58,361 --> 00:16:01,080
minthogy teljesen magabiztos legyen a döntéseiben.

247
00:16:02,120 --> 00:16:04,818
Így felmerülhet benned a kérdés, hogy miért szeretném, 

248
00:16:04,818 --> 00:16:07,565
ha a hálózat az éleket és hasonló mintákat vegye észre, 

249
00:16:07,565 --> 00:16:09,920
ha a jelenlegi neuronok teljesen mást csinálnak.

250
00:16:09,920 --> 00:16:12,300
Akkor miért ezzel mutatom be a hálózat működését?

251
00:16:13,380 --> 00:16:17,180
Nos, mert a mostani hálózat nem a végcélunk, hanem egy kiindulópont.

252
00:16:17,640 --> 00:16:21,940
Őszintén szólva, ez elég régi technológia, amit a 80-as és 90-es években kutattak. 

253
00:16:21,940 --> 00:16:25,620
De ezt meg kell értened a részletesebb modern változatok megértéséhez. 

254
00:16:25,620 --> 00:16:30,180
És ez is képes megoldani néhány érdekes problémát, de minél jobban beleásod magad abba, 

255
00:16:30,180 --> 00:16:34,740
hogy mit csinálnak valójában ezek a rejtett rétegek, annál kevésbé tűnik intelligensnek.

256
00:16:38,480 --> 00:16:42,241
Egy pillanatra helyezzük át a hangsúlyt a hálózatok tanulási módjáról arra, 

257
00:16:42,241 --> 00:16:46,300
hogy te hogyan tanulsz. Ez csak akkor működik, ha aktívan foglalkozol az anyaggal.

258
00:16:47,060 --> 00:16:50,691
Egy nagyon egyszerű dolgot szeretnék. Állítsd meg kicsit a videót, 

259
00:16:50,691 --> 00:16:55,081
és és gondolkozz el azon, hogy mit tudnál változtatni ezen a rendszeren és azon, 

260
00:16:55,081 --> 00:16:59,416
ahogyan a képeket érzékeli, ha azt akarod, hogy jobban észrevegye az élekhez és 

261
00:16:59,416 --> 00:17:00,880
hurkokhoz hasonló mintákat.

262
00:17:01,480 --> 00:17:04,592
De ha még ennél is jobban akarsz foglalkozni az anyaggal, 

263
00:17:04,592 --> 00:17:09,099
nagyon ajánlom Michael Nielsen könyvét a gépi tanulásról és a neurális hálózatokról.

264
00:17:09,680 --> 00:17:13,965
Itt megtalálod a kódot és az adatokat, amelyeket letölthetsz és játszhatsz az 

265
00:17:13,965 --> 00:17:18,359
eddigiekben bemutatott példával, és a könyv lépésről lépésre végigvezet a kódon.

266
00:17:19,300 --> 00:17:23,059
A legjobb az egészben az, hogy ez a könyv nyilvános és ingyenesen elérhető, 

267
00:17:23,059 --> 00:17:26,868
így ha hasznosnak találod, fontold meg, hogy hozzám hasonlóan adományozol az 

268
00:17:26,868 --> 00:17:27,660
erőfeszítéséért.

269
00:17:27,660 --> 00:17:31,872
A leírásban néhány más, általam nagyon kedvelt forrást is belinkeltem, 

270
00:17:31,872 --> 00:17:36,500
köztük Chris Ola fenomenális és gyönyörű blogbejegyzését és a Distill cikkeit.

271
00:17:38,280 --> 00:17:40,996
Az utolsó percekben lezárásként szeretnék idézni 

272
00:17:40,996 --> 00:17:43,880
egy részletet a Leisha Lee-vel készített interjúból.

273
00:17:44,300 --> 00:17:47,720
Talán emlékeztek rá az előző videóból. A gépi tanulásból végezte a PhD-ját.

274
00:17:48,300 --> 00:17:51,491
Ebben a kis részletben két nemrégiben megjelent cikkről beszél, 

275
00:17:51,491 --> 00:17:55,780
amelyek igazán mélyre ásnak abban, hogyan tanulnak a modernebb képfelismerő hálózatok.

276
00:17:56,120 --> 00:18:00,214
Ott tartottunk az interjúban, hogy az első cikk egy különösen sok rétegű 

277
00:18:00,214 --> 00:18:03,860
neurális hálózatot vizsgált, amely nagyon jó a képfelismerésben, 

278
00:18:03,860 --> 00:18:08,740
de a betanítás előtt a megfelelően felcímkézett adathalmaz összes címkéjét megkeverték.

279
00:18:09,480 --> 00:18:12,744
Nyilvánvaló, hogy a tesztelési pontosság nem jobb a randomnál, 

280
00:18:12,744 --> 00:18:15,231
mivel minden csak véletlenszerűen van címkézve, 

281
00:18:15,231 --> 00:18:18,548
de még mindig képes volt ugyanazt a képzési pontosságot elérni, 

282
00:18:18,548 --> 00:18:20,880
mint egy megfelelően címkézett adathalmaznál.

283
00:18:21,600 --> 00:18:25,028
Lényegében a több millió súly elég volt a hálózatnak ahhoz, 

284
00:18:25,028 --> 00:18:29,314
hogy szimplán megjegyezze a véletlenszerű adatokat, ami felveti a kérdést, 

285
00:18:29,314 --> 00:18:34,457
hogy vajon a költségfüggvény minimalizálása valóban valamilyen alakzatokat fog észrevenni 

286
00:18:34,457 --> 00:18:36,400
a képen, vagy ez csak memorizálás?

287
00:18:51,440 --> 00:18:57,710
Ha megnézzük a pontossági görbét, ha csak egy véletlenszerű adathalmazon edzenénk, 

288
00:18:57,710 --> 00:19:02,318
akkor ez a görbe nagyon lassan, szinte lineárisan csökkenne, 

289
00:19:02,318 --> 00:19:07,758
tehát tényleg küzdünk, hogy megtaláljuk a lehetséges helyi minimumokat, 

290
00:19:07,758 --> 00:19:12,140
a megfelelő súlyokat, amelyekkel elérhetjük a pontosságot.

291
00:19:12,240 --> 00:19:17,279
Míg ha egy strukturált, megfelelő címkékkel rendelkező adathalmazon edzünk, 

292
00:19:17,279 --> 00:19:22,849
akkor az elején egy kicsit babrálunk, de aztán nagyon gyorsan eljutunk a pontossági 

293
00:19:22,849 --> 00:19:28,220
szintre, és így bizonyos értelemben könnyebb volt megtalálni a lokális maximumot.

294
00:19:28,540 --> 00:19:31,981
És ami szintén érdekes volt ebben, az az, hogy ez egy másik, 

295
00:19:31,981 --> 00:19:36,381
néhány évvel ezelőtti tanulmányt is felidéz, amely sokkal több egyszerűsítést 

296
00:19:36,381 --> 00:19:40,442
tartalmaz a hálózati rétegekkel kapcsolatban, de az egyik eredménye az, 

297
00:19:40,442 --> 00:19:44,391
hogy ha megnézzük az optimalizációs domborzatot, a lokális minimumok, 

298
00:19:44,391 --> 00:19:48,848
amelyeket ezek a hálózatok hajlamosak megtanulni, valójában azonos minőségűek, 

299
00:19:48,848 --> 00:19:52,232
így bizonyos értelemben, ha az adatállományunk strukturált, 

300
00:19:52,232 --> 00:19:54,320
sokkal könnyebben megtalálhatjuk azt.

301
00:19:58,160 --> 00:20:01,180
Mint mindig, köszönöm azoknak, akik támogatnak a Patreonon.

302
00:20:01,520 --> 00:20:04,081
A Patreon-os támogatás hatalmas segítség. Ezek a 

303
00:20:04,081 --> 00:20:06,800
videók tényleg nem lennének lehetségesek nélkületek.

304
00:20:07,460 --> 00:20:10,741
Külön köszönetet szeretnék mondani az Amplify Partners cégnek is, 

305
00:20:10,741 --> 00:20:12,780
amely támogatta e sorozat eddigi videóit.

