1
00:00:04,180 --> 00:00:07,280
В прошлом видео я изложил структуру нейронной сети.

2
00:00:07,680 --> 00:00:10,367
Я подведу здесь краткий обзор, чтобы он был свеж в нашей памяти, 

3
00:00:10,367 --> 00:00:12,600
а также у меня есть две основные цели для этого видео.

4
00:00:13,100 --> 00:00:16,683
Первый — представить идею градиентного спуска, которая лежит в основе не только того, 

5
00:00:16,683 --> 00:00:20,225
как обучаются нейронные сети, но и того, как работают многие другие методы машинного 

6
00:00:20,225 --> 00:00:20,600
обучения.

7
00:00:21,120 --> 00:00:24,586
Затем мы немного углубимся в то, как работает эта конкретная 

8
00:00:24,586 --> 00:00:27,940
сеть и что в конечном итоге ищут эти скрытые слои нейронов.

9
00:00:28,980 --> 00:00:34,325
Напоминаем, что наша цель — классический пример распознавания рукописных цифр, 

10
00:00:34,325 --> 00:00:36,220
привет, мир нейронных сетей.

11
00:00:37,020 --> 00:00:39,754
Эти цифры отображаются в сетке 28x28 пикселей, 

12
00:00:39,754 --> 00:00:43,420
каждый пиксель имеет некоторое значение шкалы серого от 0 до 1.

13
00:00:43,820 --> 00:00:50,040
Именно они определяют активацию 784 нейронов входного слоя сети.

14
00:00:51,180 --> 00:00:55,912
А затем активация каждого нейрона в следующих слоях основана на взвешенной сумме 

15
00:00:55,912 --> 00:01:00,820
всех активаций в предыдущем слое плюс некое специальное число, называемое смещением.

16
00:01:02,160 --> 00:01:05,550
Затем вы составляете эту сумму с помощью какой-либо другой функции, например, 

17
00:01:05,550 --> 00:01:08,940
сжимания сигмовидной кишки или повторения, как я рассматривал в прошлом видео.

18
00:01:09,480 --> 00:01:14,507
В общей сложности, учитывая несколько произвольный выбор двух скрытых слоев по 16 

19
00:01:14,507 --> 00:01:20,026
нейронов каждый, сеть имеет около 13 000 весов и смещений, которые мы можем регулировать, 

20
00:01:20,026 --> 00:01:24,380
и именно эти значения определяют, что именно делает сеть на самом деле.

21
00:01:24,880 --> 00:01:29,245
Тогда, когда мы говорим, что эта сеть классифицирует данную цифру, мы имеем в виду, 

22
00:01:29,245 --> 00:01:33,300
что самый яркий из этих 10 нейронов в последнем слое соответствует этой цифре.

23
00:01:34,100 --> 00:01:38,141
И помните, мотивация, которую мы имели в виду здесь для многослойной структуры, 

24
00:01:38,141 --> 00:01:41,677
заключалась в том, что, возможно, второй слой мог бы улавливать края, 

25
00:01:41,677 --> 00:01:44,910
а третий слой мог бы улавливать такие узоры, как петли и линии, 

26
00:01:44,910 --> 00:01:48,800
а последний мог бы просто собрать воедино эти шаблоны для распознавания цифр.

27
00:01:49,800 --> 00:01:52,240
Итак, здесь мы узнаем, как учится сеть.

28
00:01:52,640 --> 00:01:56,969
Нам нужен алгоритм, с помощью которого вы сможете показать этой сети целый набор 

29
00:01:56,969 --> 00:02:01,567
обучающих данных, которые представлены в виде набора различных изображений рукописных 

30
00:02:01,567 --> 00:02:04,346
цифр вместе с метками того, какими они должны быть, 

31
00:02:04,346 --> 00:02:07,233
и он будет отрегулируйте эти 13 000 весов и смещений, 

32
00:02:07,233 --> 00:02:10,120
чтобы улучшить производительность на обучающих данных.

33
00:02:10,720 --> 00:02:13,595
Будем надеяться, что эта многоуровневая структура будет означать, что то, 

34
00:02:13,595 --> 00:02:16,860
что он изучает, обобщается на изображения, выходящие за рамки этих обучающих данных.

35
00:02:17,640 --> 00:02:21,745
Мы тестируем это так: после обучения сети вы показываете ей больше размеченных данных, 

36
00:02:21,745 --> 00:02:24,010
которых она никогда раньше не видела, и видите, 

37
00:02:24,010 --> 00:02:26,700
насколько точно она классифицирует эти новые изображения.

38
00:02:31,120 --> 00:02:35,160
К счастью для нас, и что делает этот пример таким распространенным, так это то, 

39
00:02:35,160 --> 00:02:37,634
что хорошие люди, стоящие за базой данных MNIST, 

40
00:02:37,634 --> 00:02:40,917
собрали коллекцию из десятков тысяч рукописных изображений цифр, 

41
00:02:40,917 --> 00:02:44,200
каждое из которых помечено числами, которые они должны были быть.

42
00:02:44,900 --> 00:02:48,751
И как бы провокационно ни было описание машины как обучающейся, как только вы увидите, 

43
00:02:48,751 --> 00:02:52,691
как она работает, это станет уже не похоже на какую-то сумасшедшую научно-фантастическую 

44
00:02:52,691 --> 00:02:55,480
предпосылку, а скорее на упражнение по математическому анализу.

45
00:02:56,200 --> 00:02:59,960
Я имею в виду, что по сути все сводится к поиску минимума определенной функции.

46
00:03:01,940 --> 00:03:06,138
Помните, концептуально мы думаем о том, что каждый нейрон связан со всеми 

47
00:03:06,138 --> 00:03:10,903
нейронами предыдущего слоя, а веса во взвешенной сумме, определяющей его активацию, 

48
00:03:10,903 --> 00:03:14,875
являются чем-то вроде силы этих связей, а смещение является некоторым 

49
00:03:14,875 --> 00:03:18,960
показателем имеет ли этот нейрон тенденцию быть активным или неактивным.

50
00:03:19,720 --> 00:03:22,015
И для начала мы просто собираемся инициализировать 

51
00:03:22,015 --> 00:03:24,400
все эти веса и смещения совершенно случайным образом.

52
00:03:24,940 --> 00:03:28,671
Излишне говорить, что эта сеть будет работать ужасно на данном обучающем примере, 

53
00:03:28,671 --> 00:03:30,720
поскольку она просто делает что-то случайное.

54
00:03:31,040 --> 00:03:36,020
Например, вы вводите это изображение с цифрой 3, а выходной слой выглядит как беспорядок.

55
00:03:36,600 --> 00:03:39,884
Итак, что вы делаете, так это определяете функцию стоимости, 

56
00:03:39,884 --> 00:03:42,683
способ сообщить компьютеру (нет, плохой компьютер), 

57
00:03:42,683 --> 00:03:47,314
что выходные данные должны иметь активации, которые равны 0 для большинства нейронов, 

58
00:03:47,314 --> 00:03:50,760
но 1 для этого нейрона, то, что вы мне дали, — это полный мусор.

59
00:03:51,720 --> 00:03:54,161
Если выразить это немного более математически, 

60
00:03:54,161 --> 00:03:58,525
вы складываете квадраты различий между каждой из этих мусорных выходных активаций и 

61
00:03:58,525 --> 00:04:01,954
желаемым значением, которое вы хотите, чтобы они имели, и это то, 

62
00:04:01,954 --> 00:04:05,020
что мы будем называть стоимостью одного обучающего примера.

63
00:04:05,960 --> 00:04:10,880
Обратите внимание, что эта сумма мала, когда сеть уверенно классифицирует 

64
00:04:10,880 --> 00:04:16,399
изображение правильно, но она велика, когда кажется, что сеть не знает, что делает.

65
00:04:18,640 --> 00:04:21,915
Итак, что вам нужно сделать, так это оценить среднюю стоимость по 

66
00:04:21,915 --> 00:04:25,440
всем десяткам тысяч обучающих примеров, имеющихся в вашем распоряжении.

67
00:04:27,040 --> 00:04:29,310
Эта средняя стоимость является нашей мерой того, 

68
00:04:29,310 --> 00:04:32,740
насколько паршивой является сеть и насколько плохим должен быть компьютер.

69
00:04:33,420 --> 00:04:34,600
И это сложная вещь.

70
00:04:35,040 --> 00:04:38,816
Помните, что сама сеть по сути представляла собой функцию, 

71
00:04:38,816 --> 00:04:44,192
которая принимает на вход 784 числа, значения пикселей и выдает 10 чисел на выходе, 

72
00:04:44,192 --> 00:04:48,800
и в каком-то смысле она параметризуется всеми этими весами и смещениями?

73
00:04:49,500 --> 00:04:52,820
Ну, функция стоимости — это еще один уровень сложности.

74
00:04:53,100 --> 00:04:58,323
В качестве входных данных он принимает примерно 13 000 весов и смещений и выдает 

75
00:04:58,323 --> 00:05:02,322
одно число, описывающее, насколько плохи эти веса и смещения, 

76
00:05:02,322 --> 00:05:07,803
а способ его определения зависит от поведения сети на всех десятках тысяч фрагментов 

77
00:05:07,803 --> 00:05:08,900
обучающих данных.

78
00:05:09,520 --> 00:05:11,000
Об этом нужно много думать.

79
00:05:12,400 --> 00:05:15,820
Но просто сказать компьютеру, какую дрянную работу он делает, не очень-то поможет.

80
00:05:16,220 --> 00:05:20,060
Вы хотите рассказать ему, как изменить эти веса и предубеждения, чтобы он стал лучше.

81
00:05:20,780 --> 00:05:24,104
Чтобы упростить задачу, вместо того, чтобы изо всех сил пытаться представить функцию 

82
00:05:24,104 --> 00:05:26,920
с 13 000 входными параметрами, просто представьте себе простую функцию, 

83
00:05:26,920 --> 00:05:30,206
которая имеет одно число в качестве входных данных и одно число в качестве выходных 

84
00:05:30,206 --> 00:05:30,480
данных.

85
00:05:31,480 --> 00:05:35,300
Как найти входные данные, которые минимизируют значение этой функции?

86
00:05:36,460 --> 00:05:39,197
Студенты, изучающие математический анализ, знают, 

87
00:05:39,197 --> 00:05:43,523
что иногда можно вычислить этот минимум явно, но это не всегда осуществимо для 

88
00:05:43,523 --> 00:05:48,396
действительно сложных функций, особенно в версии этой ситуации с 13 000 входов для нашей 

89
00:05:48,396 --> 00:05:51,080
безумно сложной функции стоимости нейронной сети.

90
00:05:51,580 --> 00:05:55,057
Более гибкая тактика — начать с любого входного сигнала и выяснить, 

91
00:05:55,057 --> 00:05:59,200
в каком направлении вам следует двигаться, чтобы снизить этот выходной результат.

92
00:06:00,080 --> 00:06:03,321
В частности, если вы можете определить наклон функции в том месте, 

93
00:06:03,321 --> 00:06:06,755
где вы находитесь, затем сдвиньте влево, если этот наклон положителен, 

94
00:06:06,755 --> 00:06:09,900
и сдвиньте входные данные вправо, если этот наклон отрицательный.

95
00:06:11,960 --> 00:06:15,779
Если вы будете делать это неоднократно, в каждой точке проверяя новый наклон и 

96
00:06:15,779 --> 00:06:19,840
делая соответствующий шаг, вы приблизитесь к некоторому локальному минимуму функции.

97
00:06:20,640 --> 00:06:23,800
Вы, возможно, имеете в виду образ мяча, катящегося с холма.

98
00:06:24,620 --> 00:06:28,161
Обратите внимание: даже для этой действительно упрощенной функции с одним входом 

99
00:06:28,161 --> 00:06:31,135
существует множество возможных впадин, в которые вы можете попасть, 

100
00:06:31,135 --> 00:06:34,939
в зависимости от того, с какого случайного входа вы начинаете, и нет никакой гарантии, 

101
00:06:34,939 --> 00:06:38,612
что локальный минимум, в который вы попадете, будет наименьшим возможным значением. 

102
00:06:38,612 --> 00:06:39,400
функции стоимости.

103
00:06:40,220 --> 00:06:42,620
Это применимо и к нашему случаю с нейронной сетью.

104
00:06:43,180 --> 00:06:46,653
Я также хочу, чтобы вы заметили, что если вы сделаете размер шага 

105
00:06:46,653 --> 00:06:50,231
пропорциональным наклону, то, когда уклон выравнивается к минимуму, 

106
00:06:50,231 --> 00:06:54,600
ваши шаги будут становиться все меньше и меньше, и это поможет вам не промахнуться.

107
00:06:55,940 --> 00:07:00,980
Немного усложняя, представьте себе функцию с двумя входами и одним выходом.

108
00:07:01,500 --> 00:07:04,820
Вы можете думать о входном пространстве как о плоскости xy, 

109
00:07:04,820 --> 00:07:08,140
а о функции стоимости как о графической поверхности над ней.

110
00:07:08,760 --> 00:07:12,332
Вместо того, чтобы спрашивать о наклоне функции, вы должны спросить, 

111
00:07:12,332 --> 00:07:16,008
в каком направлении вам следует двигаться в этом входном пространстве, 

112
00:07:16,008 --> 00:07:18,960
чтобы быстрее всего уменьшить выходной результат функции.

113
00:07:19,720 --> 00:07:21,760
Другими словами, каково направление спуска?

114
00:07:22,380 --> 00:07:25,560
Опять же, полезно представить себе мяч, катящийся с холма.

115
00:07:26,660 --> 00:07:30,255
Те из вас, кто знаком с исчислением многих переменных, знают, 

116
00:07:30,255 --> 00:07:33,966
что градиент функции указывает направление наибольшего подъема, 

117
00:07:33,966 --> 00:07:38,780
в каком направлении вам следует сделать шаг, чтобы увеличить функцию быстрее всего.

118
00:07:39,560 --> 00:07:43,616
Вполне естественно, что отрицание этого градиента дает вам направление шага, 

119
00:07:43,616 --> 00:07:46,040
при котором функция уменьшается быстрее всего.

120
00:07:47,240 --> 00:07:51,234
Более того, длина этого вектора градиента является показателем того, 

121
00:07:51,234 --> 00:07:53,840
насколько крутым является самый крутой склон.

122
00:07:54,540 --> 00:07:57,155
Если вы не знакомы с многомерным исчислением и хотите узнать больше, 

123
00:07:57,155 --> 00:08:00,340
ознакомьтесь с некоторыми работами, которые я сделал для Академии Хана по этой теме.

124
00:08:00,860 --> 00:08:04,299
Однако, честно говоря, для нас с вами сейчас важно только то, 

125
00:08:04,299 --> 00:08:08,127
что в принципе существует способ вычислить этот вектор, этот вектор, 

126
00:08:08,127 --> 00:08:11,900
который скажет вам, каково направление спуска и насколько он крутой.

127
00:08:12,400 --> 00:08:16,120
Все будет в порядке, если это все, что вы знаете, и вы не очень уверены в деталях.

128
00:08:17,200 --> 00:08:21,026
Если вы можете это понять, алгоритм минимизации функции состоит в том, 

129
00:08:21,026 --> 00:08:25,769
чтобы вычислить это направление градиента, затем сделать небольшой шаг вниз и повторять 

130
00:08:25,769 --> 00:08:26,740
это снова и снова.

131
00:08:27,700 --> 00:08:32,820
Это та же основная идея для функции, которая имеет 13 000 входов вместо двух.

132
00:08:33,400 --> 00:08:36,336
Представьте себе организацию всех 13 000 весов 

133
00:08:36,336 --> 00:08:39,460
и смещений нашей сети в гигантский вектор-столбец.

134
00:08:40,140 --> 00:08:43,797
Отрицательный градиент функции стоимости — это всего лишь вектор, 

135
00:08:43,797 --> 00:08:48,230
это некоторое направление внутри этого безумно огромного входного пространства, 

136
00:08:48,230 --> 00:08:52,774
которое подсказывает вам, какое подталкивание всех этих чисел приведет к наиболее 

137
00:08:52,774 --> 00:08:54,880
быстрому уменьшению функции стоимости.

138
00:08:55,640 --> 00:08:59,275
И, конечно же, с помощью нашей специально разработанной функции стоимости 

139
00:08:59,275 --> 00:09:01,829
изменение весов и смещений для уменьшения означает, 

140
00:09:01,829 --> 00:09:05,760
что выходные данные сети для каждого фрагмента обучающих данных будут выглядеть 

141
00:09:05,760 --> 00:09:09,395
не как случайный массив из 10 значений, а больше как фактическое решение, 

142
00:09:09,395 --> 00:09:10,820
которое мы хотим это сделать.

143
00:09:11,440 --> 00:09:14,761
Важно помнить, что эта функция стоимости включает в себя среднее значение 

144
00:09:14,761 --> 00:09:18,127
по всем обучающим данным, поэтому, если вы ее минимизируете, это означает, 

145
00:09:18,127 --> 00:09:21,180
что она будет иметь лучшую производительность на всех этих выборках.

146
00:09:23,820 --> 00:09:26,964
Алгоритм эффективного расчета этого градиента, который, по сути, 

147
00:09:26,964 --> 00:09:31,173
является основой обучения нейронной сети, называется обратным распространением ошибки, 

148
00:09:31,173 --> 00:09:33,980
и именно об этом я собираюсь поговорить в следующем видео.

149
00:09:34,660 --> 00:09:37,781
Здесь я действительно хочу потратить время на то, чтобы разобраться, 

150
00:09:37,781 --> 00:09:40,993
что именно происходит с каждым весом и смещением для данного фрагмента 

151
00:09:40,993 --> 00:09:44,069
тренировочных данных, пытаясь дать интуитивное представление о том, 

152
00:09:44,069 --> 00:09:47,100
что происходит за пределами кучи соответствующих расчетов и формул.

153
00:09:47,780 --> 00:09:50,773
Прямо здесь и сейчас главное, что я хочу, чтобы вы знали, 

154
00:09:50,773 --> 00:09:55,108
независимо от деталей реализации, это то, что, когда мы говорим о сетевом обучении, 

155
00:09:55,108 --> 00:09:58,360
мы имеем в виду, что оно просто минимизирует функцию стоимости.

156
00:09:59,300 --> 00:10:01,944
И обратите внимание: одним из последствий этого является то, 

157
00:10:01,944 --> 00:10:05,238
что для этой функции стоимости важно иметь хороший плавный выходной сигнал, 

158
00:10:05,238 --> 00:10:08,100
чтобы мы могли найти локальный минимум, делая небольшие шаги вниз.

159
00:10:09,260 --> 00:10:14,564
Вот почему, кстати, искусственные нейроны имеют постоянно меняющуюся активацию, 

160
00:10:14,564 --> 00:10:19,140
а не просто бинарно активны или неактивны, как биологические нейроны.

161
00:10:20,220 --> 00:10:23,766
Этот процесс многократного подталкивания входных данных функции на величину, 

162
00:10:23,766 --> 00:10:26,760
кратную отрицательному градиенту, называется градиентным спуском.

163
00:10:27,300 --> 00:10:30,915
Это способ приблизиться к некоторому локальному минимуму функции стоимости, 

164
00:10:30,915 --> 00:10:32,580
по сути, к впадине на этом графике.

165
00:10:33,440 --> 00:10:36,824
Конечно, я все еще показываю изображение функции с двумя входами, 

166
00:10:36,824 --> 00:10:41,234
потому что подталкивания в 13 000-мерном входном пространстве немного сложно усвоить, 

167
00:10:41,234 --> 00:10:44,260
но есть хороший непространственный способ подумать об этом.

168
00:10:45,080 --> 00:10:48,440
Каждый компонент отрицательного градиента говорит нам о двух вещах.

169
00:10:49,060 --> 00:10:51,813
Знак, конечно, говорит нам, следует ли сдвинуть 

170
00:10:51,813 --> 00:10:55,140
соответствующий компонент входного вектора вверх или вниз.

171
00:10:55,800 --> 00:11:00,452
Но что немаловажно, относительные величины всех этих компонентов подскажут вам, 

172
00:11:00,452 --> 00:11:02,720
какие изменения имеют большее значение.

173
00:11:05,220 --> 00:11:09,005
Видите ли, в нашей сети корректировка одного из весов может оказать гораздо 

174
00:11:09,005 --> 00:11:13,040
большее влияние на функцию стоимости, чем корректировка какого-либо другого веса.

175
00:11:14,800 --> 00:11:18,200
Некоторые из этих связей имеют большее значение для наших тренировочных данных.

176
00:11:19,320 --> 00:11:23,887
Таким образом, вы можете думать об этом градиентном векторе нашей ошеломляюще массивной 

177
00:11:23,887 --> 00:11:28,247
функции затрат так, что он кодирует относительную важность каждого веса и смещения, 

178
00:11:28,247 --> 00:11:32,400
то есть какое из этих изменений принесет наибольшую отдачу от вложенных средств.

179
00:11:33,620 --> 00:11:36,640
На самом деле это просто еще один способ мышления о направлении.

180
00:11:37,100 --> 00:11:40,979
Возьмем более простой пример: если у вас есть некоторая функция с двумя 

181
00:11:40,979 --> 00:11:43,996
переменными в качестве входных данных, и вы вычисляете, 

182
00:11:43,996 --> 00:11:48,090
что ее градиент в какой-то конкретной точке равен 3,1, то, с одной стороны, 

183
00:11:48,090 --> 00:11:50,676
вы можете интерпретировать это как утверждение, 

184
00:11:50,676 --> 00:11:54,932
что когда вы Если вы стоите на этом входе, движение в этом направлении быстрее 

185
00:11:54,932 --> 00:11:59,727
всего увеличивает функцию: когда вы строите график функции над плоскостью входных точек, 

186
00:11:59,727 --> 00:12:02,260
этот вектор дает вам прямое направление в гору.

187
00:12:02,860 --> 00:12:06,195
Но другой способ понять это — сказать, что изменения в этой первой 

188
00:12:06,195 --> 00:12:10,178
переменной имеют в 3 раза большую важность, чем изменения во второй переменной, 

189
00:12:10,178 --> 00:12:13,414
что, по крайней мере, вблизи соответствующего входного значения, 

190
00:12:13,414 --> 00:12:16,900
подталкивание значения x несет гораздо большую пользу для вашего бакс.

191
00:12:19,880 --> 00:12:22,340
Давайте уменьшим масштаб и подведем итоги того, где мы находимся на данный момент.

192
00:12:22,840 --> 00:12:27,222
Сама сеть представляет собой эту функцию с 784 входами и 10 выходами, 

193
00:12:27,222 --> 00:12:30,040
определяемыми через все эти взвешенные суммы.

194
00:12:30,640 --> 00:12:33,680
Функция стоимости представляет собой еще один уровень сложности.

195
00:12:33,980 --> 00:12:37,819
Он принимает 13 000 весов и смещений в качестве входных данных 

196
00:12:37,819 --> 00:12:41,720
и выдает единственную меру неудачи на основе обучающих примеров.

197
00:12:42,440 --> 00:12:46,900
И градиент функции стоимости — это еще один уровень сложности.

198
00:12:47,360 --> 00:12:50,851
Он говорит нам, какие подталкивания ко всем этим весам и отклонениям вызывают 

199
00:12:50,851 --> 00:12:53,313
наиболее быстрое изменение значения функции стоимости, 

200
00:12:53,313 --> 00:12:56,716
что можно интерпретировать как указание того, какие изменения в каких весах 

201
00:12:56,716 --> 00:12:57,880
имеют наибольшее значение.

202
00:13:02,560 --> 00:13:05,989
Итак, когда вы инициализируете сеть со случайными весами и смещениями и много 

203
00:13:05,989 --> 00:13:08,891
раз настраиваете их на основе этого процесса градиентного спуска, 

204
00:13:08,891 --> 00:13:11,573
насколько хорошо она на самом деле работает с изображениями, 

205
00:13:11,573 --> 00:13:13,200
которых она никогда раньше не видела?

206
00:13:14,100 --> 00:13:18,526
Тот, который я описал здесь, с двумя скрытыми слоями по 16 нейронов в каждом, 

207
00:13:18,526 --> 00:13:21,987
выбранными в основном из эстетических соображений, неплохой, 

208
00:13:21,987 --> 00:13:25,960
правильно классифицируя около 96% новых изображений, которые он видит.

209
00:13:26,680 --> 00:13:29,260
И, честно говоря, если вы посмотрите на некоторые примеры, 

210
00:13:29,260 --> 00:13:32,540
в которых он дает сбой, вы почувствуете необходимость немного ослабить его.

211
00:13:36,220 --> 00:13:40,362
Теперь, если вы поиграете со структурой скрытых слоев и сделаете пару настроек, 

212
00:13:40,362 --> 00:13:41,760
вы сможете получить до 98%.

213
00:13:41,760 --> 00:13:42,720
И это очень хорошо!

214
00:13:43,020 --> 00:13:46,717
Это не самое лучшее решение, вы, конечно, можете добиться большей производительности, 

215
00:13:46,717 --> 00:13:49,898
если станете более сложной, чем эта простая ванильная сеть, но, учитывая, 

216
00:13:49,898 --> 00:13:52,434
насколько сложной является первоначальная задача, я думаю, 

217
00:13:52,434 --> 00:13:56,304
что есть что-то невероятное в том, что любая сеть так хорошо справляется с изображениями, 

218
00:13:56,304 --> 00:13:58,367
которые она никогда раньше не видела, учитывая, 

219
00:13:58,367 --> 00:14:01,420
что мы никогда специально не говорили ему, какие закономерности искать.

220
00:14:02,560 --> 00:14:06,332
Первоначально я мотивировал эту структуру, описывая нашу надежду на то, 

221
00:14:06,332 --> 00:14:09,005
что второй слой сможет улавливать маленькие ребра, 

222
00:14:09,005 --> 00:14:12,568
что третий слой будет соединять эти ребра, чтобы распознавать петли 

223
00:14:12,568 --> 00:14:17,180
и более длинные линии, и что их можно будет соединить. вместе, чтобы распознавать цифры.

224
00:14:17,960 --> 00:14:20,400
Так действительно ли этим занимается наша сеть?

225
00:14:21,080 --> 00:14:24,400
Ну, по крайней мере, для этого, совсем нет.

226
00:14:24,820 --> 00:14:27,533
Помните, как в прошлом видео мы рассматривали, 

227
00:14:27,533 --> 00:14:31,517
как веса связей всех нейронов первого слоя с данным нейроном второго 

228
00:14:31,517 --> 00:14:34,808
слоя можно визуализировать как заданный шаблон пикселей, 

229
00:14:34,808 --> 00:14:37,060
который улавливает нейрон второго слоя?

230
00:14:37,780 --> 00:14:42,573
Что ж, когда мы на самом деле делаем это для весов, связанных с этими переходами, 

231
00:14:42,573 --> 00:14:46,548
от первого слоя к следующему, вместо того, чтобы собирать отдельные 

232
00:14:46,548 --> 00:14:50,055
маленькие ребра здесь и там, они выглядят почти случайными, 

233
00:14:50,055 --> 00:14:53,680
просто с некоторыми очень свободными шаблонами в середина там.

234
00:14:53,760 --> 00:14:57,640
Казалось бы, в непостижимо большом 13 000-мерном пространстве возможных 

235
00:14:57,640 --> 00:15:01,845
весов и смещений наша сеть обнаружила счастливый маленький локальный минимум, 

236
00:15:01,845 --> 00:15:05,564
который, несмотря на успешную классификацию большинства изображений, 

237
00:15:05,564 --> 00:15:08,960
не совсем уловил закономерности, на которые мы могли надеяться.

238
00:15:09,780 --> 00:15:11,743
И чтобы по-настоящему понять эту мысль, посмотрите, 

239
00:15:11,743 --> 00:15:13,820
что происходит, когда вы вводите случайное изображение.

240
00:15:14,320 --> 00:15:16,714
Если бы система была умной, вы могли бы ожидать, 

241
00:15:16,714 --> 00:15:19,304
что она будет чувствовать себя неуверенно, возможно, 

242
00:15:19,304 --> 00:15:23,360
на самом деле не активирует ни один из этих 10 выходных нейронов или не активирует 

243
00:15:23,360 --> 00:15:27,611
их все равномерно, но вместо этого она уверенно дает вам какой-то бессмысленный ответ, 

244
00:15:27,611 --> 00:15:31,863
как если бы она чувствовала себя настолько же уверенным, что этот случайный шум это 5, 

245
00:15:31,863 --> 00:15:34,160
так же как и фактическое изображение 5 — это 5.

246
00:15:34,540 --> 00:15:38,532
Другими словами, даже если эта сеть довольно хорошо распознает цифры, 

247
00:15:38,532 --> 00:15:40,700
она понятия не имеет, как их рисовать.

248
00:15:41,420 --> 00:15:45,240
Во многом это связано с тем, что это очень жестко ограниченная система тренировок.

249
00:15:45,880 --> 00:15:47,740
Я имею в виду, поставьте себя на место сети.

250
00:15:48,140 --> 00:15:52,806
С ее точки зрения, вся Вселенная состоит только из четко определенных неподвижных цифр, 

251
00:15:52,806 --> 00:15:56,996
сосредоточенных в крошечной сетке, и ее функция стоимости никогда не давала ей 

252
00:15:56,996 --> 00:16:01,080
никаких стимулов быть чем-то иным, кроме полной уверенности в своих решениях.

253
00:16:02,120 --> 00:16:05,148
Итак, учитывая то, что на самом деле делают нейроны второго слоя, 

254
00:16:05,148 --> 00:16:09,140
вы можете задаться вопросом, почему я представил эту сеть с мотивацией улавливать края 

255
00:16:09,140 --> 00:16:09,920
и закономерности.

256
00:16:09,920 --> 00:16:12,300
Я имею в виду, что в конечном итоге это совсем не то.

257
00:16:13,380 --> 00:16:17,180
Что ж, это не наша конечная цель, а отправная точка.

258
00:16:17,640 --> 00:16:21,411
Честно говоря, это старая технология, исследованная в 80-х и 90-х годах, 

259
00:16:21,411 --> 00:16:26,060
и вам нужно понять ее, прежде чем вы сможете понять более подробные современные варианты, 

260
00:16:26,060 --> 00:16:29,057
и она явно способна решать некоторые интересные проблемы, 

261
00:16:29,057 --> 00:16:33,138
но чем больше вы копаетесь в том, что эти скрытые слои действительно работают, 

262
00:16:33,138 --> 00:16:34,740
тем менее разумным это кажется.

263
00:16:38,480 --> 00:16:41,999
Если на мгновение сместить фокус с того, как сети учатся, на то, как вы учитесь, 

264
00:16:41,999 --> 00:16:45,648
это произойдет только в том случае, если вы каким-то образом активно будете изучать 

265
00:16:45,648 --> 00:16:46,300
материал здесь.

266
00:16:47,060 --> 00:16:49,551
Я хочу, чтобы вы сделали одну довольно простую вещь: 

267
00:16:49,551 --> 00:16:53,170
просто сделайте паузу прямо сейчас и на мгновение глубоко задумайтесь о том, 

268
00:16:53,170 --> 00:16:55,897
какие изменения вы могли бы внести в эту систему и о том, 

269
00:16:55,897 --> 00:17:00,080
как она воспринимает изображения, если вы хотите, чтобы она лучше улавливала такие вещи, 

270
00:17:00,080 --> 00:17:00,880
как края и узоры.

271
00:17:01,480 --> 00:17:04,549
Но более того, чтобы по-настоящему углубиться в материал, 

272
00:17:04,549 --> 00:17:09,099
я настоятельно рекомендую книгу Майкла Нильсена о глубоком обучении и нейронных сетях.

273
00:17:09,680 --> 00:17:13,964
В ней вы можете найти код и данные для загрузки и воспроизведения именно для 

274
00:17:13,964 --> 00:17:18,359
этого примера, и книга шаг за шагом проведет вас через то, что делает этот код.

275
00:17:19,300 --> 00:17:22,213
Что удивительно, так это то, что эта книга бесплатна и общедоступна, 

276
00:17:22,213 --> 00:17:24,662
поэтому, если вы что-то из нее получите, подумайте о том, 

277
00:17:24,662 --> 00:17:27,660
чтобы присоединиться ко мне и сделать пожертвование на усилия Нильсена.

278
00:17:27,660 --> 00:17:31,994
Я также связал в описании пару других ресурсов, которые мне очень нравятся, 

279
00:17:31,994 --> 00:17:36,500
в том числе феноменальный и красивый пост в блоге Криса Олы и статьи в Distill.

280
00:17:38,280 --> 00:17:42,687
Чтобы закончить на последние несколько минут, я хочу вернуться к фрагменту интервью, 

281
00:17:42,687 --> 00:17:43,880
которое я дал Лейше Ли.

282
00:17:44,300 --> 00:17:46,103
Возможно, вы помните ее по последнему видео: она защитила 

283
00:17:46,103 --> 00:17:47,720
докторскую диссертацию в области глубокого обучения.

284
00:17:48,300 --> 00:17:50,896
В этом небольшом отрывке она рассказывает о двух недавних статьях, 

285
00:17:50,896 --> 00:17:53,338
в которых действительно изучается, как на самом деле обучаются 

286
00:17:53,338 --> 00:17:55,780
некоторые из более современных сетей распознавания изображений.

287
00:17:56,120 --> 00:17:58,466
Чтобы понять, на каком этапе разговора мы находимся, 

288
00:17:58,466 --> 00:18:01,787
в первой статье была взята одна из этих особенно глубоких нейронных сетей, 

289
00:18:01,787 --> 00:18:04,356
которая действительно хороша в распознавании изображений, 

290
00:18:04,356 --> 00:18:07,278
и вместо обучения ее на правильно размеченном наборе данных перед 

291
00:18:07,278 --> 00:18:08,740
обучением перетасованы все метки.

292
00:18:09,480 --> 00:18:13,280
Очевидно, что точность тестирования здесь была не лучше, чем случайная, 

293
00:18:13,280 --> 00:18:17,027
поскольку все помечено случайным образом, но все равно удалось достичь 

294
00:18:17,027 --> 00:18:20,880
той же точности обучения, что и на правильно маркированном наборе данных.

295
00:18:21,600 --> 00:18:25,413
По сути, миллионов весов для этой конкретной сети было достаточно, 

296
00:18:25,413 --> 00:18:29,227
чтобы она просто запомнила случайные данные, что поднимает вопрос, 

297
00:18:29,227 --> 00:18:34,180
действительно ли минимизация этой функции стоимости соответствует какой-либо структуре 

298
00:18:34,180 --> 00:18:36,400
изображения или это просто запоминание?

299
00:18:51,440 --> 00:18:56,549
Если вы посмотрите на эту кривую точности, если бы вы просто тренировались на 

300
00:18:56,549 --> 00:19:02,052
случайном наборе данных, эта кривая как бы снижалась очень медленно, почти линейно, 

301
00:19:02,052 --> 00:19:07,816
так что вы действительно изо всех сил пытаетесь найти этот локальный минимум возможных, 

302
00:19:07,816 --> 00:19:12,140
вы знаете , правильные веса, которые обеспечат вам такую точность.

303
00:19:12,240 --> 00:19:16,768
В то время как, если вы на самом деле тренируетесь на структурированном наборе данных, 

304
00:19:16,768 --> 00:19:20,047
который имеет правильные метки, вначале вы немного повозитесь, 

305
00:19:20,047 --> 00:19:24,368
но затем вы как бы очень быстро падаете, чтобы добраться до этого уровня точности, 

306
00:19:24,368 --> 00:19:28,220
и поэтому в некотором смысле это было легче найти эти локальные максимумы.

307
00:19:28,540 --> 00:19:33,471
И что еще было интересно в этом, так это то, что это проливает свет на еще одну статью, 

308
00:19:33,471 --> 00:19:37,619
написанную пару лет назад, в которой гораздо больше упрощений в отношении 

309
00:19:37,619 --> 00:19:40,981
сетевых уровней, но в одном из результатов говорилось, что, 

310
00:19:40,981 --> 00:19:44,568
если вы посмотрите на ландшафт оптимизации, локальные минимумы, 

311
00:19:44,568 --> 00:19:48,715
которые эти сети обычно изучают, на самом деле имеют одинаковое качество, 

312
00:19:48,715 --> 00:19:52,414
поэтому в некотором смысле, если ваш набор данных структурирован, 

313
00:19:52,414 --> 00:19:54,320
вам будет гораздо легче найти его.

314
00:19:58,160 --> 00:20:01,180
Моя благодарность, как всегда, тем из вас, кто поддерживает Patreon.

315
00:20:01,520 --> 00:20:04,004
Я уже говорил, что меняет правила игры Патреон, 

316
00:20:04,004 --> 00:20:06,800
но эти видео действительно были бы невозможны без вас.

317
00:20:07,460 --> 00:20:10,244
Я также хочу выразить особую благодарность венчурной фирме Amplify 

318
00:20:10,244 --> 00:20:12,780
Partners за поддержку этих первых видеороликов из этой серии.

