1
00:00:04,180 --> 00:00:07,280
Last video I laid out the structure of a neural network.

2
00:00:07,680 --> 00:00:11,600
I'll give a quick recap here so that it's fresh in our minds, and then I have two main

3
00:00:11,600 --> 00:00:12,600
goals for this video.

4
00:00:13,100 --> 00:00:17,700
The first is to introduce the idea of gradient descent, which underlies not only how neural

5
00:00:17,700 --> 00:00:20,600
networks learn, but how a lot of other machine learning works as well.

6
00:00:21,120 --> 00:00:24,660
Then after that we'll dig in a little more into how this particular network performs,

7
00:00:25,060 --> 00:00:27,940
and what those hidden layers of neurons end up looking for.

8
00:00:28,980 --> 00:00:34,040
As a reminder, our goal here is the classic example of handwritten digit recognition,

9
00:00:34,700 --> 00:00:36,220
the hello world of neural networks.

10
00:00:37,020 --> 00:00:42,180
These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value

11
00:00:42,180 --> 00:00:43,420
between 0 and 1.

12
00:00:43,820 --> 00:00:50,040
Those are what determine the activations of 784 neurons in the input layer of the network.

13
00:00:51,180 --> 00:00:55,420
And then the activation for each neuron in the following layers is based on a weighted

14
00:00:55,420 --> 00:01:00,820
sum of all the activations in the previous layer, plus some special number called a bias.

15
00:01:02,160 --> 00:01:06,120
Then you compose that sum with some other function, like the sigmoid squishification,

16
00:01:06,520 --> 00:01:08,940
or a relu, the way I walked through last video.

17
00:01:09,480 --> 00:01:14,420
In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each,

18
00:01:14,860 --> 00:01:20,820
the network has about 13,000 weights and biases that we can adjust, and it's these values

19
00:01:20,820 --> 00:01:24,380
that determine what exactly the network actually does.

20
00:01:24,880 --> 00:01:29,680
Then what we mean when we say that this network classifies a given digit is that the brightest

21
00:01:29,680 --> 00:01:33,300
of those 10 neurons in the final layer corresponds to that digit.

22
00:01:34,100 --> 00:01:38,760
And remember, the motivation we had in mind here for the layered structure was that maybe

23
00:01:38,760 --> 00:01:43,760
the second layer could pick up on the edges, and the third layer might pick up on patterns

24
00:01:43,760 --> 00:01:48,800
like loops and lines, and the last one could just piece together those patterns to recognize digits.

25
00:01:49,800 --> 00:01:52,240
So here, we learn how the network learns.

26
00:01:52,640 --> 00:01:56,920
What we want is an algorithm where you can show this network a whole bunch of training

27
00:01:56,920 --> 00:02:01,580
data, which comes in the form of a bunch of different images of handwritten digits, along

28
00:02:01,580 --> 00:02:06,380
with labels for what they're supposed to be, and it'll adjust those 13,000 weights

29
00:02:06,380 --> 00:02:10,120
and biases so as to improve its performance on the training data.

30
00:02:10,720 --> 00:02:15,380
Hopefully, this layered structure will mean that what it learns generalizes to images

31
00:02:15,380 --> 00:02:16,860
beyond that training data.

32
00:02:17,640 --> 00:02:22,140
The way we test that is that after you train the network, you show it more labeled data

33
00:02:22,140 --> 00:02:26,700
that it's never seen before, and you see how accurately it classifies those new images.

34
00:02:31,120 --> 00:02:34,860
Fortunately for us, and what makes this such a common example to start with, is that the

35
00:02:34,860 --> 00:02:39,460
good people behind the MNIST database have put together a collection of tens of thousands

36
00:02:39,460 --> 00:02:44,200
of handwritten digit images, each one labeled with the numbers they're supposed to be.

37
00:02:44,900 --> 00:02:49,660
And as provocative as it is to describe a machine as learning, once you see how it works,

38
00:02:49,860 --> 00:02:55,480
it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.

39
00:02:56,200 --> 00:02:59,960
I mean, basically it comes down to finding the minimum of a certain function.

40
00:03:01,940 --> 00:03:06,600
Remember, conceptually, we're thinking of each neuron as being connected to all the

41
00:03:06,600 --> 00:03:11,020
neurons in the previous layer, and the weights in the weighted sum defining its activation

42
00:03:11,360 --> 00:03:16,440
are kind of like the strengths of those connections, and the bias is some indication of whether

43
00:03:16,440 --> 00:03:18,960
that neuron tends to be active or inactive.

44
00:03:19,720 --> 00:03:23,120
And to start things off, we're just going to initialize all of those weights and biases

45
00:03:23,120 --> 00:03:24,400
totally randomly.

46
00:03:24,940 --> 00:03:28,420
Needless to say, this network is going to perform pretty horribly on a given training

47
00:03:28,420 --> 00:03:30,720
example, since it's just doing something random.

48
00:03:31,040 --> 00:03:36,020
For example, you feed in this image of a 3, and the output layer just looks like a mess.

49
00:03:36,600 --> 00:03:42,700
So what you do is define a cost function, a way of telling the computer, no, bad computer,

50
00:03:43,220 --> 00:03:48,440
that output should have activations which are 0 for most neurons, but 1 for this neuron,

51
00:03:48,800 --> 00:03:50,760
what you gave me is utter trash.

52
00:03:51,720 --> 00:03:56,500
To say that a little more mathematically, you add up the squares of the differences

53
00:03:56,500 --> 00:04:02,060
between each of those trash output activations and the value you want them to have, and this

54
00:04:02,060 --> 00:04:05,020
is what we'll call the cost of a single training example.

55
00:04:05,960 --> 00:04:11,040
Notice this sum is small when the network confidently classifies the image correctly,

56
00:04:12,200 --> 00:04:16,400
but it's large when the network seems like it doesn't know what it's doing.

57
00:04:18,640 --> 00:04:23,980
So then what you do is consider the average cost over all of the tens of thousands of

58
00:04:23,980 --> 00:04:25,440
training examples at your disposal.

59
00:04:27,040 --> 00:04:32,140
This average cost is our measure for how lousy the network is, and how bad the computer

60
00:04:32,140 --> 00:04:32,740
should feel.

61
00:04:33,420 --> 00:04:34,600
And that's a complicated thing.

62
00:04:35,040 --> 00:04:40,420
Remember how the network itself was basically a function, one that takes in 784 numbers

63
00:04:40,420 --> 00:04:46,300
as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's

64
00:04:46,300 --> 00:04:48,800
parameterized by all these weights and biases?

65
00:04:49,500 --> 00:04:52,820
Well the cost function is a layer of complexity on top of that.

66
00:04:53,100 --> 00:04:59,260
It takes as its input those 13,000 or so weights and biases, and spits out a single number

67
00:04:59,260 --> 00:05:04,640
describing how bad those weights and biases are, and the way it's defined depends on

68
00:05:04,640 --> 00:05:08,900
the network's behavior over all the tens of thousands of pieces of training data.

69
00:05:09,520 --> 00:05:11,000
That's a lot to think about.

70
00:05:12,400 --> 00:05:15,820
But just telling the computer what a crappy job it's doing isn't very helpful.

71
00:05:16,220 --> 00:05:20,060
You want to tell it how to change those weights and biases so that it gets better.

72
00:05:20,780 --> 00:05:25,200
To make it easier, rather than struggling to imagine a function with 13,000 inputs,

73
00:05:25,760 --> 00:05:30,480
just imagine a simple function that has one number as an input and one number as an output.

74
00:05:31,480 --> 00:05:35,300
How do you find an input that minimizes the value of this function?

75
00:05:36,460 --> 00:05:41,400
Calculus students will know that you can sometimes figure out that minimum explicitly, but that's

76
00:05:41,400 --> 00:05:46,640
not always feasible for really complicated functions, certainly not in the 13,000 input

77
00:05:46,640 --> 00:05:51,080
version of this situation for our crazy complicated neural network cost function.

78
00:05:51,580 --> 00:05:56,900
A more flexible tactic is to start at any input, and figure out which direction you

79
00:05:56,900 --> 00:05:59,200
should step to make that output lower.

80
00:06:00,080 --> 00:06:05,020
Specifically, if you can figure out the slope of the function where you are, then shift

81
00:06:05,020 --> 00:06:09,340
to the left if that slope is positive, and shift the input to the right if that slope

82
00:06:09,340 --> 00:06:09,900
is negative.

83
00:06:11,960 --> 00:06:16,900
If you do this repeatedly, at each point checking the new slope and taking the appropriate step,

84
00:06:17,200 --> 00:06:19,840
you're going to approach some local minimum of the function.

85
00:06:20,640 --> 00:06:23,800
The image you might have in mind here is a ball rolling down a hill.

86
00:06:24,620 --> 00:06:29,880
Notice, even for this really simplified single input function, there are many possible valleys

87
00:06:29,880 --> 00:06:34,360
that you might land in, depending on which random input you start at, and there's no

88
00:06:34,360 --> 00:06:38,480
guarantee that the local minimum you land in is going to be the smallest possible value

89
00:06:38,480 --> 00:06:39,400
of the cost function.

90
00:06:40,220 --> 00:06:42,620
That will carry over to our neural network case as well.

91
00:06:43,180 --> 00:06:47,500
I also want you to notice how if you make your step sizes proportional to the slope,

92
00:06:48,160 --> 00:06:52,460
then when the slope is flattening out towards the minimum, your steps get smaller and smaller,

93
00:06:52,800 --> 00:06:54,600
and that helps you from overshooting.

94
00:06:55,940 --> 00:07:00,980
Bumping up the complexity a bit, imagine instead a function with two inputs and one output.

95
00:07:01,500 --> 00:07:06,360
You might think of the input space as the xy-plane, and the cost function as being graphed

96
00:07:06,360 --> 00:07:08,140
as a surface above it.

97
00:07:08,760 --> 00:07:13,620
Instead of asking about the slope of the function, you have to ask which direction you should

98
00:07:13,620 --> 00:07:18,960
step in this input space so as to decrease the output of the function most quickly.

99
00:07:19,720 --> 00:07:21,760
In other words, what's the downhill direction?

100
00:07:22,380 --> 00:07:25,560
Again, it's helpful to think of a ball rolling down that hill.

101
00:07:26,660 --> 00:07:31,620
Those of you familiar with multivariable calculus will know that the gradient of a function

102
00:07:31,620 --> 00:07:37,500
gives you the direction of steepest ascent, which direction should you step to increase

103
00:07:37,500 --> 00:07:38,780
the function most quickly.

104
00:07:39,560 --> 00:07:44,080
Naturally enough, taking the negative of that gradient gives you the direction to step that

105
00:07:44,080 --> 00:07:46,040
decreases the function most quickly.

106
00:07:47,240 --> 00:07:52,260
Even more than that, the length of this gradient vector is an indication for just how steep

107
00:07:52,260 --> 00:07:53,840
that steepest slope is.

108
00:07:54,540 --> 00:07:58,480
If you're unfamiliar with multivariable calculus and want to learn more, check out some of

109
00:07:58,480 --> 00:08:00,340
the work I did for Khan Academy on the topic.

110
00:08:00,860 --> 00:08:05,780
Honestly though, all that matters for you and me right now is that in principle there

111
00:08:05,780 --> 00:08:10,420
exists a way to compute this vector, this vector that tells you what the downhill direction

112
00:08:10,420 --> 00:08:11,900
is and how steep it is.

113
00:08:12,400 --> 00:08:16,120
You'll be okay if that's all you know and you're not rock solid on the details.

114
00:08:17,200 --> 00:08:22,540
If you can get that, the algorithm for minimizing the function is to compute this gradient direction,

115
00:08:23,040 --> 00:08:26,740
then take a small step downhill, and repeat that over and over.

116
00:08:27,700 --> 00:08:32,820
It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.

117
00:08:33,400 --> 00:08:39,020
Imagine organizing all 13,000 weights and biases of our network into a giant column

118
00:08:39,020 --> 00:08:39,460
vector.

119
00:08:40,140 --> 00:08:46,180
The negative gradient of the cost function is just a vector, it's some direction inside

120
00:08:46,180 --> 00:08:51,740
this insanely huge input space that tells you which nudges to all of those numbers is

121
00:08:51,740 --> 00:08:54,880
going to cause the most rapid decrease to the cost function.

122
00:08:55,640 --> 00:09:00,000
And of course, with our specially designed cost function, changing the weights and biases

123
00:09:00,000 --> 00:09:05,120
to decrease it means making the output of the network on each piece of training data

124
00:09:05,120 --> 00:09:10,380
look less like a random array of 10 values, and more like an actual decision we want it

125
00:09:10,380 --> 00:09:10,820
to make.

126
00:09:11,440 --> 00:09:15,980
It's important to remember, this cost function involves an average over all of the training

127
00:09:15,980 --> 00:09:21,180
data, so if you minimize it, it means it's a better performance on all of those samples.

128
00:09:23,820 --> 00:09:28,500
The algorithm for computing this gradient efficiently, which is effectively the heart

129
00:09:28,500 --> 00:09:32,620
of how a neural network learns, is called backpropagation, and it's what I'm going

130
00:09:32,620 --> 00:09:33,980
to be talking about next video.

131
00:09:34,660 --> 00:09:39,080
There, I really want to take the time to walk through what exactly happens to each weight

132
00:09:39,080 --> 00:09:43,940
and bias for a given piece of training data, trying to give an intuitive feel for what's

133
00:09:43,940 --> 00:09:47,100
happening beyond the pile of relevant calculus and formulas.

134
00:09:47,780 --> 00:09:51,760
Right here, right now, the main thing I want you to know, independent of implementation

135
00:09:51,760 --> 00:09:56,820
details, is that what we mean when we talk about a network learning is that it's just

136
00:09:56,820 --> 00:09:58,360
minimizing a cost function.

137
00:09:59,300 --> 00:10:02,820
And notice, one consequence of that is that it's important for this cost function to

138
00:10:02,820 --> 00:10:07,640
have a nice smooth output, so that we can find a local minimum by taking little steps

139
00:10:07,640 --> 00:10:08,100
downhill.

140
00:10:09,260 --> 00:10:14,160
This is why, by the way, artificial neurons have continuously ranging activations, rather

141
00:10:14,160 --> 00:10:19,140
than simply being active or inactive in a binary way, the way biological neurons are.

142
00:10:20,220 --> 00:10:24,580
This process of repeatedly nudging an input of a function by some multiple of the negative

143
00:10:24,580 --> 00:10:26,760
gradient is called gradient descent.

144
00:10:27,300 --> 00:10:31,780
It's a way to converge towards some local minimum of a cost function, basically a valley

145
00:10:31,780 --> 00:10:32,580
in this graph.

146
00:10:33,440 --> 00:10:37,080
I'm still showing the picture of a function with two inputs, of course, because nudges

147
00:10:37,080 --> 00:10:41,460
in a 13,000 dimensional input space are a little hard to wrap your mind around, but

148
00:10:41,460 --> 00:10:44,260
there is a nice non-spatial way to think about this.

149
00:10:45,080 --> 00:10:48,440
Each component of the negative gradient tells us two things.

150
00:10:49,060 --> 00:10:53,340
The sign, of course, tells us whether the corresponding component of the input vector

151
00:10:53,340 --> 00:10:55,140
should be nudged up or down.

152
00:10:55,800 --> 00:11:01,400
But importantly, the relative magnitudes of all these components kind of tells you which

153
00:11:01,400 --> 00:11:02,720
changes matter more.

154
00:11:05,220 --> 00:11:09,860
You see, in our network, an adjustment to one of the weights might have a much greater

155
00:11:09,860 --> 00:11:13,040
impact on the cost function than the adjustment to some other weight.

156
00:11:14,800 --> 00:11:18,200
Some of these connections just matter more for our training data.

157
00:11:19,320 --> 00:11:23,540
So a way you can think about this gradient vector of our mind-warpingly massive cost

158
00:11:23,540 --> 00:11:29,680
function is that it encodes the relative importance of each weight and bias, that is, which of

159
00:11:29,680 --> 00:11:32,400
these changes is going to carry the most bang for your buck.

160
00:11:33,620 --> 00:11:36,640
This really is just another way of thinking about direction.

161
00:11:37,100 --> 00:11:41,680
To take a simpler example, if you have some function with two variables as an input, and

162
00:11:41,680 --> 00:11:48,620
you compute that its gradient at some particular point comes out as 3,1, then on the one hand

163
00:11:48,620 --> 00:11:52,900
you can interpret that as saying that when you're standing at that input, moving along

164
00:11:52,900 --> 00:11:57,380
this direction increases the function most quickly, that when you graph the function

165
00:11:57,380 --> 00:12:01,740
above the plane of input points, that vector is what's giving you the straight uphill

166
00:12:01,740 --> 00:12:02,260
direction.

167
00:12:02,860 --> 00:12:08,060
But another way to read that is to say that changes to this first variable have 3 times

168
00:12:08,060 --> 00:12:12,980
the importance as changes to the second variable, that at least in the neighborhood of the relevant

169
00:12:12,980 --> 00:12:16,900
input, nudging the x-value carries a lot more bang for your buck.

170
00:12:19,880 --> 00:12:22,340
Let's zoom out and sum up where we are so far.

171
00:12:22,840 --> 00:12:28,760
The network itself is this function with 784 inputs and 10 outputs, defined in terms of

172
00:12:28,760 --> 00:12:30,040
all these weighted sums.

173
00:12:30,640 --> 00:12:33,680
The cost function is a layer of complexity on top of that.

174
00:12:33,980 --> 00:12:39,960
It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness

175
00:12:39,960 --> 00:12:41,720
based on the training examples.

176
00:12:42,440 --> 00:12:46,900
And the gradient of the cost function is one more layer of complexity still.

177
00:12:47,360 --> 00:12:52,940
It tells us what nudges to all these weights and biases cause the fastest change to the

178
00:12:52,940 --> 00:12:56,700
value of the cost function, which you might interpret as saying which changes to which

179
00:12:56,700 --> 00:12:57,880
weights matter the most.

180
00:13:02,560 --> 00:13:06,960
So, when you initialize the network with random weights and biases, and adjust them

181
00:13:06,960 --> 00:13:11,240
many times based on this gradient descent process, how well does it actually perform

182
00:13:11,240 --> 00:13:13,200
on images it's never seen before?

183
00:13:14,100 --> 00:13:18,980
The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly

184
00:13:18,980 --> 00:13:25,960
for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.

185
00:13:26,680 --> 00:13:31,580
And honestly, if you look at some of the examples it messes up on, you feel compelled to cut

186
00:13:31,580 --> 00:13:32,540
it a little slack.

187
00:13:36,220 --> 00:13:39,840
Now if you play around with the hidden layer structure and make a couple tweaks, you can

188
00:13:39,840 --> 00:13:41,760
get this up to 98%.

189
00:13:41,760 --> 00:13:42,720
And that's pretty good!

190
00:13:43,020 --> 00:13:46,880
It's not the best, you can certainly get better performance by getting more sophisticated

191
00:13:46,880 --> 00:13:52,360
than this plain vanilla network, but given how daunting the initial task is, I think

192
00:13:52,360 --> 00:13:56,800
there's something incredible about any network doing this well on images it's never seen

193
00:13:56,800 --> 00:14:01,420
before, given that we never specifically told it what patterns to look for.

194
00:14:02,560 --> 00:14:07,820
Originally, the way I motivated this structure was by describing a hope we might have, that

195
00:14:07,820 --> 00:14:11,920
the second layer might pick up on little edges, that the third layer would piece together

196
00:14:11,920 --> 00:14:16,120
those edges to recognize loops and longer lines, and that those might be pieced together

197
00:14:16,120 --> 00:14:17,180
to recognize digits.

198
00:14:17,960 --> 00:14:20,400
So is this what our network is actually doing?

199
00:14:21,080 --> 00:14:24,400
Well, for this one at least, not at all.

200
00:14:24,820 --> 00:14:29,120
Remember how last video we looked at how the weights of the connections from all the neurons

201
00:14:29,120 --> 00:14:33,840
in the first layer to a given neuron in the second layer can be visualized as a given

202
00:14:33,840 --> 00:14:37,060
pixel pattern that the second layer neuron is picking up on?

203
00:14:37,780 --> 00:14:42,360
Well, when we actually do that for the weights associated with these transitions, from the

204
00:14:42,360 --> 00:14:46,980
first layer to the next, instead of picking up on isolated little edges here and there,

205
00:14:47,320 --> 00:14:53,680
they look, well, almost random, just with some very loose patterns in the middle there.

206
00:14:53,760 --> 00:14:59,000
It would seem that in the unfathomably large 13,000 dimensional space of possible weights

207
00:14:59,000 --> 00:15:04,040
and biases, our network found itself a happy little local minimum that, despite successfully

208
00:15:04,040 --> 00:15:08,400
classifying most images, doesn't exactly pick up on the patterns we might have hoped

209
00:15:08,400 --> 00:15:08,960
for.

210
00:15:09,780 --> 00:15:13,820
And to really drive this point home, watch what happens when you input a random image.

211
00:15:14,320 --> 00:15:19,820
If the system was smart, you might expect it to feel uncertain, maybe not really activating

212
00:15:19,820 --> 00:15:25,340
any of those 10 output neurons or activating them all evenly, but instead it confidently

213
00:15:25,340 --> 00:15:31,200
gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it

214
00:15:31,200 --> 00:15:34,160
does that an actual image of a 5 is a 5.

215
00:15:34,540 --> 00:15:39,440
Phrased differently, even if this network can recognize digits pretty well, it has no

216
00:15:39,440 --> 00:15:40,700
idea how to draw them.

217
00:15:41,420 --> 00:15:45,240
A lot of this is because it's such a tightly constrained training setup.

218
00:15:45,880 --> 00:15:47,740
I mean, put yourself in the network's shoes here.

219
00:15:48,140 --> 00:15:53,320
From its point of view, the entire universe consists of nothing but clearly defined unmoving

220
00:15:53,320 --> 00:15:58,660
digits centered in a tiny grid, and its cost function never gave it any incentive to be

221
00:15:58,660 --> 00:16:01,080
anything but utterly confident in its decisions.

222
00:16:02,120 --> 00:16:05,860
So with this as the image of what those second layer neurons are really doing, you might

223
00:16:05,860 --> 00:16:09,360
wonder why I would introduce this network with the motivation of picking up on edges

224
00:16:09,360 --> 00:16:09,920
and patterns.

225
00:16:09,920 --> 00:16:12,300
I mean, that's just not at all what it ends up doing.

226
00:16:13,380 --> 00:16:17,180
Well, this is not meant to be our end goal, but instead a starting point.

227
00:16:17,640 --> 00:16:22,440
Frankly, this is old technology, the kind researched in the 80s and 90s, and you do

228
00:16:22,440 --> 00:16:26,940
need to understand it before you can understand more detailed modern variants, and it clearly

229
00:16:26,940 --> 00:16:31,480
is capable of solving some interesting problems, but the more you dig into what those hidden

230
00:16:31,480 --> 00:16:34,740
layers are really doing, the less intelligent it seems.

231
00:16:38,480 --> 00:16:43,100
Shifting the focus for a moment from how networks learn to how you learn, that'll

232
00:16:43,100 --> 00:16:46,300
only happen if you engage actively with the material here somehow.

233
00:16:47,060 --> 00:16:52,020
One pretty simple thing I want you to do is just pause right now and think deeply for

234
00:16:52,020 --> 00:16:57,060
a moment about what changes you might make to this system and how it perceives images

235
00:16:57,460 --> 00:17:00,880
if you wanted it to better pick up on things like edges and patterns.

236
00:17:01,480 --> 00:17:06,440
But better than that, to actually engage with the material, I highly recommend the book

237
00:17:06,440 --> 00:17:09,100
by Michael Nielsen on deep learning and neural networks.

238
00:17:09,680 --> 00:17:14,740
In it, you can find the code and the data to download and play with for this exact example,

239
00:17:15,220 --> 00:17:18,360
and the book will walk you through step by step what that code is doing.

240
00:17:19,300 --> 00:17:23,920
What's awesome is that this book is free and publicly available, so if you do get something

241
00:17:23,920 --> 00:17:27,660
out of it, consider joining me in making a donation towards Nielsen's efforts.

242
00:17:27,660 --> 00:17:32,080
I've also linked a couple other resources I like a lot in the description, including

243
00:17:32,080 --> 00:17:36,500
the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.

244
00:17:38,280 --> 00:17:41,920
To close things off here for the last few minutes, I want to jump back into a snippet

245
00:17:41,920 --> 00:17:43,880
of the interview I had with Leisha Lee.

246
00:17:44,300 --> 00:17:47,720
You might remember her from the last video, she did her PhD work in deep learning.

247
00:17:48,300 --> 00:17:52,280
In this little snippet she talks about two recent papers that really dig into how some

248
00:17:52,280 --> 00:17:55,780
of the more modern image recognition networks are actually learning.

249
00:17:56,120 --> 00:18:00,260
Just to set up where we were in the conversation, the first paper took one of these particularly

250
00:18:00,260 --> 00:18:04,460
deep neural networks that's really good at image recognition, and instead of training

251
00:18:04,460 --> 00:18:08,740
it on a properly labeled dataset, shuffled all the labels around before training.

252
00:18:09,480 --> 00:18:13,880
Obviously the testing accuracy here was no better than random, since everything is just

253
00:18:13,880 --> 00:18:19,200
randomly labeled, but it was still able to achieve the same training accuracy as you

254
00:18:19,200 --> 00:18:20,880
would on a properly labeled dataset.

255
00:18:21,600 --> 00:18:26,140
Basically, the millions of weights for this particular network were enough for it to just

256
00:18:26,140 --> 00:18:31,040
memorize the random data, which raises the question for whether minimizing this cost

257
00:18:31,040 --> 00:18:36,400
function actually corresponds to any sort of structure in the image, or is it just memorization?

258
00:18:51,440 --> 00:18:59,620
If you look at that accuracy curve, if you were just training on a random dataset, that

259
00:18:59,620 --> 00:19:06,180
curve sort of went down very slowly in almost kind of a linear fashion, so you're really

260
00:19:06,180 --> 00:19:11,380
struggling to find that local minima of possible, you know, the right weights that would get

261
00:19:11,380 --> 00:19:12,140
you that accuracy.

262
00:19:12,240 --> 00:19:16,980
Whereas if you're actually training on a structured dataset, one that has the right labels, you

263
00:19:16,980 --> 00:19:21,000
fiddle around a little bit in the beginning, but then you kind of dropped very fast to

264
00:19:21,000 --> 00:19:28,220
get to that accuracy level, and so in some sense it was easier to find that local maxima.

265
00:19:28,540 --> 00:19:32,960
And so what was also interesting about that is it brings into light another paper from

266
00:19:32,960 --> 00:19:39,000
actually a couple of years ago, which has a lot more simplifications about the network

267
00:19:39,000 --> 00:19:43,020
layers, but one of the results was saying how if you look at the optimization landscape,

268
00:19:43,020 --> 00:19:49,480
the local minima that these networks tend to learn are actually of equal quality, so

269
00:19:49,480 --> 00:19:53,960
in some sense if your dataset is structured, you should be able to find that much more

270
00:19:53,960 --> 00:19:54,320
easily.

271
00:19:58,160 --> 00:20:01,180
My thanks, as always, to those of you supporting on Patreon.

272
00:20:01,520 --> 00:20:05,480
I've said before just what a game changer Patreon is, but these videos really would

273
00:20:05,480 --> 00:20:06,800
not be possible without you.

274
00:20:07,460 --> 00:20:11,480
I also want to give a special thanks to the VC firm Amplify Partners, in their support

275
00:20:11,480 --> 00:20:13,160
of these initial videos in the series.

