1
00:00:04,180 --> 00:00:07,280
Last video I laid out the structure of a neural network.

2
00:00:07,680 --> 00:00:10,504
I'll give a quick recap here so that it's fresh in our minds, 

3
00:00:10,504 --> 00:00:12,600
and then I have two main goals for this video.

4
00:00:13,100 --> 00:00:15,692
The first is to introduce the idea of gradient descent, 

5
00:00:15,692 --> 00:00:18,100
which underlies not only how neural networks learn, 

6
00:00:18,100 --> 00:00:20,600
but how a lot of other machine learning works as well.

7
00:00:21,120 --> 00:00:25,164
Then after that we'll dig in a little more into how this particular network performs, 

8
00:00:25,164 --> 00:00:27,940
and what those hidden layers of neurons end up looking for.

9
00:00:28,980 --> 00:00:34,125
As a reminder, our goal here is the classic example of handwritten digit recognition, 

10
00:00:34,125 --> 00:00:36,220
the hello world of neural networks.

11
00:00:37,020 --> 00:00:40,094
These digits are rendered on a 28x28 pixel grid, 

12
00:00:40,094 --> 00:00:43,420
each pixel with some grayscale value between 0 and 1.

13
00:00:43,820 --> 00:00:50,040
Those are what determine the activations of 784 neurons in the input layer of the network.

14
00:00:51,180 --> 00:00:55,945
And then the activation for each neuron in the following layers is based on a weighted 

15
00:00:55,945 --> 00:01:00,820
sum of all the activations in the previous layer, plus some special number called a bias.

16
00:01:02,160 --> 00:01:04,810
Then you compose that sum with some other function, 

17
00:01:04,810 --> 00:01:08,940
like the sigmoid squishification, or a relu, the way I walked through last video.

18
00:01:09,480 --> 00:01:15,171
In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, 

19
00:01:15,171 --> 00:01:19,519
the network has about 13,000 weights and biases that we can adjust, 

20
00:01:19,519 --> 00:01:24,380
and it's these values that determine what exactly the network actually does.

21
00:01:24,880 --> 00:01:29,142
Then what we mean when we say that this network classifies a given digit is that 

22
00:01:29,142 --> 00:01:33,300
the brightest of those 10 neurons in the final layer corresponds to that digit.

23
00:01:34,100 --> 00:01:37,500
And remember, the motivation we had in mind here for the layered 

24
00:01:37,500 --> 00:01:41,162
structure was that maybe the second layer could pick up on the edges, 

25
00:01:41,162 --> 00:01:44,719
and the third layer might pick up on patterns like loops and lines, 

26
00:01:44,719 --> 00:01:48,800
and the last one could just piece together those patterns to recognize digits.

27
00:01:49,800 --> 00:01:52,240
So here, we learn how the network learns.

28
00:01:52,640 --> 00:01:56,835
What we want is an algorithm where you can show this network a whole bunch of 

29
00:01:56,835 --> 00:02:01,406
training data, which comes in the form of a bunch of different images of handwritten 

30
00:02:01,406 --> 00:02:04,580
digits, along with labels for what they're supposed to be, 

31
00:02:04,580 --> 00:02:08,990
and it'll adjust those 13,000 weights and biases so as to improve its performance 

32
00:02:08,990 --> 00:02:10,120
on the training data.

33
00:02:10,720 --> 00:02:13,844
Hopefully, this layered structure will mean that what it 

34
00:02:13,844 --> 00:02:16,860
learns generalizes to images beyond that training data.

35
00:02:17,640 --> 00:02:20,642
The way we test that is that after you train the network, 

36
00:02:20,642 --> 00:02:23,697
you show it more labeled data that it's never seen before, 

37
00:02:23,697 --> 00:02:26,700
and you see how accurately it classifies those new images.

38
00:02:31,120 --> 00:02:34,920
Fortunately for us, and what makes this such a common example to start with, 

39
00:02:34,920 --> 00:02:39,313
is that the good people behind the MNIST database have put together a collection of tens 

40
00:02:39,313 --> 00:02:43,459
of thousands of handwritten digit images, each one labeled with the numbers they're 

41
00:02:43,459 --> 00:02:44,200
supposed to be.

42
00:02:44,900 --> 00:02:48,562
And as provocative as it is to describe a machine as learning, 

43
00:02:48,562 --> 00:02:53,154
once you see how it works, it feels a lot less like some crazy sci-fi premise, 

44
00:02:53,154 --> 00:02:55,480
and a lot more like a calculus exercise.

45
00:02:56,200 --> 00:02:59,960
I mean, basically it comes down to finding the minimum of a certain function.

46
00:03:01,940 --> 00:03:06,059
Remember, conceptually, we're thinking of each neuron as being connected to 

47
00:03:06,059 --> 00:03:10,124
all the neurons in the previous layer, and the weights in the weighted sum 

48
00:03:10,124 --> 00:03:14,298
defining its activation are kind of like the strengths of those connections, 

49
00:03:14,298 --> 00:03:18,960
and the bias is some indication of whether that neuron tends to be active or inactive.

50
00:03:19,720 --> 00:03:22,216
And to start things off, we're just going to initialize 

51
00:03:22,216 --> 00:03:24,400
all of those weights and biases totally randomly.

52
00:03:24,940 --> 00:03:27,786
Needless to say, this network is going to perform pretty horribly 

53
00:03:27,786 --> 00:03:30,720
on a given training example, since it's just doing something random.

54
00:03:31,040 --> 00:03:36,020
For example, you feed in this image of a 3, and the output layer just looks like a mess.

55
00:03:36,600 --> 00:03:41,475
So what you do is define a cost function, a way of telling the computer, 

56
00:03:41,475 --> 00:03:47,086
no, bad computer, that output should have activations which are 0 for most neurons, 

57
00:03:47,086 --> 00:03:50,760
but 1 for this neuron, what you gave me is utter trash.

58
00:03:51,720 --> 00:03:55,812
To say that a little more mathematically, you add up the squares of the 

59
00:03:55,812 --> 00:04:00,188
differences between each of those trash output activations and the value you 

60
00:04:00,188 --> 00:04:05,020
want them to have, and this is what we'll call the cost of a single training example.

61
00:04:05,960 --> 00:04:11,536
Notice this sum is small when the network confidently classifies the image correctly, 

62
00:04:11,536 --> 00:04:16,399
but it's large when the network seems like it doesn't know what it's doing.

63
00:04:18,640 --> 00:04:22,068
So then what you do is consider the average cost over all of 

64
00:04:22,068 --> 00:04:25,440
the tens of thousands of training examples at your disposal.

65
00:04:27,040 --> 00:04:30,631
This average cost is our measure for how lousy the network is, 

66
00:04:30,631 --> 00:04:32,740
and how bad the computer should feel.

67
00:04:33,420 --> 00:04:34,600
And that's a complicated thing.

68
00:04:35,040 --> 00:04:38,618
Remember how the network itself was basically a function, 

69
00:04:38,618 --> 00:04:42,259
one that takes in 784 numbers as inputs, the pixel values, 

70
00:04:42,259 --> 00:04:46,825
and spits out 10 numbers as its output, and in a sense it's parameterized 

71
00:04:46,825 --> 00:04:48,800
by all these weights and biases?

72
00:04:49,500 --> 00:04:52,820
Well the cost function is a layer of complexity on top of that.

73
00:04:53,100 --> 00:04:56,850
It takes as its input those 13,000 or so weights and biases, 

74
00:04:56,850 --> 00:05:01,707
and spits out a single number describing how bad those weights and biases are, 

75
00:05:01,707 --> 00:05:06,625
and the way it's defined depends on the network's behavior over all the tens of 

76
00:05:06,625 --> 00:05:08,900
thousands of pieces of training data.

77
00:05:09,520 --> 00:05:11,000
That's a lot to think about.

78
00:05:12,400 --> 00:05:15,820
But just telling the computer what a crappy job it's doing isn't very helpful.

79
00:05:16,220 --> 00:05:20,060
You want to tell it how to change those weights and biases so that it gets better.

80
00:05:20,780 --> 00:05:25,436
To make it easier, rather than struggling to imagine a function with 13,000 inputs, 

81
00:05:25,436 --> 00:05:30,092
just imagine a simple function that has one number as an input and one number as an 

82
00:05:30,092 --> 00:05:30,480
output.

83
00:05:31,480 --> 00:05:35,300
How do you find an input that minimizes the value of this function?

84
00:05:36,460 --> 00:05:41,223
Calculus students will know that you can sometimes figure out that minimum explicitly, 

85
00:05:41,223 --> 00:05:44,782
but that's not always feasible for really complicated functions, 

86
00:05:44,782 --> 00:05:49,492
certainly not in the 13,000 input version of this situation for our crazy complicated 

87
00:05:49,492 --> 00:05:51,080
neural network cost function.

88
00:05:51,580 --> 00:05:54,640
A more flexible tactic is to start at any input, 

89
00:05:54,640 --> 00:05:59,200
and figure out which direction you should step to make that output lower.

90
00:06:00,080 --> 00:06:04,145
Specifically, if you can figure out the slope of the function where you are, 

91
00:06:04,145 --> 00:06:06,785
then shift to the left if that slope is positive, 

92
00:06:06,785 --> 00:06:09,900
and shift the input to the right if that slope is negative.

93
00:06:11,960 --> 00:06:15,925
If you do this repeatedly, at each point checking the new slope and taking the 

94
00:06:15,925 --> 00:06:19,840
appropriate step, you're going to approach some local minimum of the function.

95
00:06:20,640 --> 00:06:23,800
The image you might have in mind here is a ball rolling down a hill.

96
00:06:24,620 --> 00:06:27,841
Notice, even for this really simplified single input function, 

97
00:06:27,841 --> 00:06:30,705
there are many possible valleys that you might land in, 

98
00:06:30,705 --> 00:06:34,336
depending on which random input you start at, and there's no guarantee 

99
00:06:34,336 --> 00:06:38,019
that the local minimum you land in is going to be the smallest possible 

100
00:06:38,019 --> 00:06:39,400
value of the cost function.

101
00:06:40,220 --> 00:06:42,620
That will carry over to our neural network case as well.

102
00:06:43,180 --> 00:06:47,653
I also want you to notice how if you make your step sizes proportional to the slope, 

103
00:06:47,653 --> 00:06:50,758
then when the slope is flattening out towards the minimum, 

104
00:06:50,758 --> 00:06:54,600
your steps get smaller and smaller, and that helps you from overshooting.

105
00:06:55,940 --> 00:06:58,653
Bumping up the complexity a bit, imagine instead 

106
00:06:58,653 --> 00:07:00,980
a function with two inputs and one output.

107
00:07:01,500 --> 00:07:04,555
You might think of the input space as the xy-plane, 

108
00:07:04,555 --> 00:07:08,140
and the cost function as being graphed as a surface above it.

109
00:07:08,760 --> 00:07:11,698
Instead of asking about the slope of the function, 

110
00:07:11,698 --> 00:07:16,654
you have to ask which direction you should step in this input space so as to decrease 

111
00:07:16,654 --> 00:07:18,960
the output of the function most quickly.

112
00:07:19,720 --> 00:07:21,760
In other words, what's the downhill direction?

113
00:07:22,380 --> 00:07:25,560
Again, it's helpful to think of a ball rolling down that hill.

114
00:07:26,660 --> 00:07:30,719
Those of you familiar with multivariable calculus will know that the 

115
00:07:30,719 --> 00:07:34,661
gradient of a function gives you the direction of steepest ascent, 

116
00:07:34,661 --> 00:07:38,780
which direction should you step to increase the function most quickly.

117
00:07:39,560 --> 00:07:42,850
Naturally enough, taking the negative of that gradient gives you 

118
00:07:42,850 --> 00:07:46,040
the direction to step that decreases the function most quickly.

119
00:07:47,240 --> 00:07:50,453
Even more than that, the length of this gradient vector 

120
00:07:50,453 --> 00:07:53,840
is an indication for just how steep that steepest slope is.

121
00:07:54,540 --> 00:07:57,653
If you're unfamiliar with multivariable calculus and want to learn more, 

122
00:07:57,653 --> 00:08:00,340
check out some of the work I did for Khan Academy on the topic.

123
00:08:00,860 --> 00:08:04,540
Honestly though, all that matters for you and me right now is that 

124
00:08:04,540 --> 00:08:07,615
in principle there exists a way to compute this vector, 

125
00:08:07,615 --> 00:08:11,900
this vector that tells you what the downhill direction is and how steep it is.

126
00:08:12,400 --> 00:08:16,120
You'll be okay if that's all you know and you're not rock solid on the details.

127
00:08:17,200 --> 00:08:21,941
If you can get that, the algorithm for minimizing the function is to compute this 

128
00:08:21,941 --> 00:08:26,740
gradient direction, then take a small step downhill, and repeat that over and over.

129
00:08:27,700 --> 00:08:32,820
It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.

130
00:08:33,400 --> 00:08:36,663
Imagine organizing all 13,000 weights and biases 

131
00:08:36,663 --> 00:08:39,460
of our network into a giant column vector.

132
00:08:40,140 --> 00:08:43,982
The negative gradient of the cost function is just a vector, 

133
00:08:43,982 --> 00:08:48,958
it's some direction inside this insanely huge input space that tells you which 

134
00:08:48,958 --> 00:08:53,746
nudges to all of those numbers is going to cause the most rapid decrease to 

135
00:08:53,746 --> 00:08:54,880
the cost function.

136
00:08:55,640 --> 00:08:58,888
And of course, with our specially designed cost function, 

137
00:08:58,888 --> 00:09:02,473
changing the weights and biases to decrease it means making the 

138
00:09:02,473 --> 00:09:06,282
output of the network on each piece of training data look less like 

139
00:09:06,282 --> 00:09:10,820
a random array of 10 values, and more like an actual decision we want it to make.

140
00:09:11,440 --> 00:09:15,931
It's important to remember, this cost function involves an average over all of the 

141
00:09:15,931 --> 00:09:20,747
training data, so if you minimize it, it means it's a better performance on all of those 

142
00:09:20,747 --> 00:09:21,180
samples.

143
00:09:23,820 --> 00:09:26,614
The algorithm for computing this gradient efficiently, 

144
00:09:26,614 --> 00:09:29,814
which is effectively the heart of how a neural network learns, 

145
00:09:29,814 --> 00:09:33,980
is called backpropagation, and it's what I'm going to be talking about next video.

146
00:09:34,660 --> 00:09:38,719
There, I really want to take the time to walk through what exactly happens to 

147
00:09:38,719 --> 00:09:41,686
each weight and bias for a given piece of training data, 

148
00:09:41,686 --> 00:09:45,954
trying to give an intuitive feel for what's happening beyond the pile of relevant 

149
00:09:45,954 --> 00:09:47,100
calculus and formulas.

150
00:09:47,780 --> 00:09:50,832
Right here, right now, the main thing I want you to know, 

151
00:09:50,832 --> 00:09:54,412
independent of implementation details, is that what we mean when we 

152
00:09:54,412 --> 00:09:58,360
talk about a network learning is that it's just minimizing a cost function.

153
00:09:59,300 --> 00:10:03,700
And notice, one consequence of that is that it's important for this cost function to have 

154
00:10:03,700 --> 00:10:08,100
a nice smooth output, so that we can find a local minimum by taking little steps downhill.

155
00:10:09,260 --> 00:10:13,945
This is why, by the way, artificial neurons have continuously ranging activations, 

156
00:10:13,945 --> 00:10:17,389
rather than simply being active or inactive in a binary way, 

157
00:10:17,389 --> 00:10:19,140
the way biological neurons are.

158
00:10:20,220 --> 00:10:23,361
This process of repeatedly nudging an input of a function by 

159
00:10:23,361 --> 00:10:26,760
some multiple of the negative gradient is called gradient descent.

160
00:10:27,300 --> 00:10:30,888
It's a way to converge towards some local minimum of a cost function, 

161
00:10:30,888 --> 00:10:32,580
basically a valley in this graph.

162
00:10:33,440 --> 00:10:36,933
I'm still showing the picture of a function with two inputs, of course, 

163
00:10:36,933 --> 00:10:40,426
because nudges in a 13,000 dimensional input space are a little hard to 

164
00:10:40,426 --> 00:10:44,260
wrap your mind around, but there is a nice non-spatial way to think about this.

165
00:10:45,080 --> 00:10:48,440
Each component of the negative gradient tells us two things.

166
00:10:49,060 --> 00:10:52,046
The sign, of course, tells us whether the corresponding 

167
00:10:52,046 --> 00:10:55,140
component of the input vector should be nudged up or down.

168
00:10:55,800 --> 00:10:59,228
But importantly, the relative magnitudes of all these 

169
00:10:59,228 --> 00:11:02,720
components kind of tells you which changes matter more.

170
00:11:05,220 --> 00:11:09,180
You see, in our network, an adjustment to one of the weights might have a much 

171
00:11:09,180 --> 00:11:13,040
greater impact on the cost function than the adjustment to some other weight.

172
00:11:14,800 --> 00:11:18,200
Some of these connections just matter more for our training data.

173
00:11:19,320 --> 00:11:23,680
So a way you can think about this gradient vector of our mind-warpingly massive 

174
00:11:23,680 --> 00:11:28,149
cost function is that it encodes the relative importance of each weight and bias, 

175
00:11:28,149 --> 00:11:32,400
that is, which of these changes is going to carry the most bang for your buck.

176
00:11:33,620 --> 00:11:36,640
This really is just another way of thinking about direction.

177
00:11:37,100 --> 00:11:41,841
To take a simpler example, if you have some function with two variables as an input, 

178
00:11:41,841 --> 00:11:46,137
and you compute that its gradient at some particular point comes out as 3,1, 

179
00:11:46,137 --> 00:11:50,098
then on the one hand you can interpret that as saying that when you're 

180
00:11:50,098 --> 00:11:55,063
standing at that input, moving along this direction increases the function most quickly, 

181
00:11:55,063 --> 00:11:58,745
that when you graph the function above the plane of input points, 

182
00:11:58,745 --> 00:12:02,260
that vector is what's giving you the straight uphill direction.

183
00:12:02,860 --> 00:12:07,410
But another way to read that is to say that changes to this first variable have 3 

184
00:12:07,410 --> 00:12:10,518
times the importance as changes to the second variable, 

185
00:12:10,518 --> 00:12:13,681
that at least in the neighborhood of the relevant input, 

186
00:12:13,681 --> 00:12:16,900
nudging the x-value carries a lot more bang for your buck.

187
00:12:19,880 --> 00:12:22,340
Let's zoom out and sum up where we are so far.

188
00:12:22,840 --> 00:12:27,211
The network itself is this function with 784 inputs and 10 outputs, 

189
00:12:27,211 --> 00:12:30,040
defined in terms of all these weighted sums.

190
00:12:30,640 --> 00:12:33,680
The cost function is a layer of complexity on top of that.

191
00:12:33,980 --> 00:12:37,912
It takes the 13,000 weights and biases as inputs and spits out 

192
00:12:37,912 --> 00:12:41,720
a single measure of lousiness based on the training examples.

193
00:12:42,440 --> 00:12:46,900
And the gradient of the cost function is one more layer of complexity still.

194
00:12:47,360 --> 00:12:50,849
It tells us what nudges to all these weights and biases cause the 

195
00:12:50,849 --> 00:12:53,492
fastest change to the value of the cost function, 

196
00:12:53,492 --> 00:12:57,880
which you might interpret as saying which changes to which weights matter the most.

197
00:13:02,560 --> 00:13:06,141
So, when you initialize the network with random weights and biases, 

198
00:13:06,141 --> 00:13:09,670
and adjust them many times based on this gradient descent process, 

199
00:13:09,670 --> 00:13:13,200
how well does it actually perform on images it's never seen before?

200
00:13:14,100 --> 00:13:19,025
The one I've described here, with the two hidden layers of 16 neurons each, 

201
00:13:19,025 --> 00:13:22,201
chosen mostly for aesthetic reasons, is not bad, 

202
00:13:22,201 --> 00:13:25,960
classifying about 96% of the new images it sees correctly.

203
00:13:26,680 --> 00:13:30,217
And honestly, if you look at some of the examples it messes up on, 

204
00:13:30,217 --> 00:13:32,540
you feel compelled to cut it a little slack.

205
00:13:36,220 --> 00:13:40,375
Now if you play around with the hidden layer structure and make a couple tweaks, 

206
00:13:40,375 --> 00:13:41,760
you can get this up to 98%.

207
00:13:41,760 --> 00:13:42,720
And that's pretty good!

208
00:13:43,020 --> 00:13:47,904
It's not the best, you can certainly get better performance by getting more sophisticated 

209
00:13:47,904 --> 00:13:52,084
than this plain vanilla network, but given how daunting the initial task is, 

210
00:13:52,084 --> 00:13:56,752
I think there's something incredible about any network doing this well on images it's 

211
00:13:56,752 --> 00:14:01,420
never seen before, given that we never specifically told it what patterns to look for.

212
00:14:02,560 --> 00:14:06,930
Originally, the way I motivated this structure was by describing a hope we might have, 

213
00:14:06,930 --> 00:14:09,593
that the second layer might pick up on little edges, 

214
00:14:09,593 --> 00:14:13,261
that the third layer would piece together those edges to recognize loops 

215
00:14:13,261 --> 00:14:17,180
and longer lines, and that those might be pieced together to recognize digits.

216
00:14:17,960 --> 00:14:20,400
So is this what our network is actually doing?

217
00:14:21,080 --> 00:14:24,400
Well, for this one at least, not at all.

218
00:14:24,820 --> 00:14:28,763
Remember how last video we looked at how the weights of the connections from 

219
00:14:28,763 --> 00:14:32,860
all the neurons in the first layer to a given neuron in the second layer can be 

220
00:14:32,860 --> 00:14:37,060
visualized as a given pixel pattern that the second layer neuron is picking up on?

221
00:14:37,780 --> 00:14:42,681
Well, when we actually do that for the weights associated with these transitions, 

222
00:14:42,681 --> 00:14:47,822
from the first layer to the next, instead of picking up on isolated little edges here 

223
00:14:47,822 --> 00:14:52,902
and there, they look, well, almost random, just with some very loose patterns in the 

224
00:14:52,902 --> 00:14:53,680
middle there.

225
00:14:53,760 --> 00:14:57,671
It would seem that in the unfathomably large 13,000 dimensional space 

226
00:14:57,671 --> 00:15:01,304
of possible weights and biases, our network found itself a happy 

227
00:15:01,304 --> 00:15:05,383
little local minimum that, despite successfully classifying most images, 

228
00:15:05,383 --> 00:15:08,960
doesn't exactly pick up on the patterns we might have hoped for.

229
00:15:09,780 --> 00:15:13,820
And to really drive this point home, watch what happens when you input a random image.

230
00:15:14,320 --> 00:15:18,389
If the system was smart, you might expect it to feel uncertain, 

231
00:15:18,389 --> 00:15:24,112
maybe not really activating any of those 10 output neurons or activating them all evenly, 

232
00:15:24,112 --> 00:15:27,864
but instead it confidently gives you some nonsense answer, 

233
00:15:27,864 --> 00:15:32,888
as if it feels as sure that this random noise is a 5 as it does that an actual 

234
00:15:32,888 --> 00:15:34,160
image of a 5 is a 5.

235
00:15:34,540 --> 00:15:38,874
Phrased differently, even if this network can recognize digits pretty well, 

236
00:15:38,874 --> 00:15:40,700
it has no idea how to draw them.

237
00:15:41,420 --> 00:15:45,240
A lot of this is because it's such a tightly constrained training setup.

238
00:15:45,880 --> 00:15:47,740
I mean, put yourself in the network's shoes here.

239
00:15:48,140 --> 00:15:52,434
From its point of view, the entire universe consists of nothing but clearly 

240
00:15:52,434 --> 00:15:55,203
defined unmoving digits centered in a tiny grid, 

241
00:15:55,203 --> 00:15:59,554
and its cost function never gave it any incentive to be anything but utterly 

242
00:15:59,554 --> 00:16:01,080
confident in its decisions.

243
00:16:02,120 --> 00:16:05,415
So with this as the image of what those second layer neurons are really doing, 

244
00:16:05,415 --> 00:16:07,959
you might wonder why I would introduce this network with the 

245
00:16:07,959 --> 00:16:09,920
motivation of picking up on edges and patterns.

246
00:16:09,920 --> 00:16:12,300
I mean, that's just not at all what it ends up doing.

247
00:16:13,380 --> 00:16:17,180
Well, this is not meant to be our end goal, but instead a starting point.

248
00:16:17,640 --> 00:16:21,469
Frankly, this is old technology, the kind researched in the 80s and 90s, 

249
00:16:21,469 --> 00:16:26,190
and you do need to understand it before you can understand more detailed modern variants, 

250
00:16:26,190 --> 00:16:29,547
and it clearly is capable of solving some interesting problems, 

251
00:16:29,547 --> 00:16:33,166
but the more you dig into what those hidden layers are really doing, 

252
00:16:33,166 --> 00:16:34,740
the less intelligent it seems.

253
00:16:38,480 --> 00:16:42,390
Shifting the focus for a moment from how networks learn to how you learn, 

254
00:16:42,390 --> 00:16:46,300
that'll only happen if you engage actively with the material here somehow.

255
00:16:47,060 --> 00:16:51,762
One pretty simple thing I want you to do is just pause right now and think deeply 

256
00:16:51,762 --> 00:16:56,521
for a moment about what changes you might make to this system and how it perceives 

257
00:16:56,521 --> 00:17:00,880
images if you wanted it to better pick up on things like edges and patterns.

258
00:17:01,480 --> 00:17:04,654
But better than that, to actually engage with the material, 

259
00:17:04,654 --> 00:17:09,099
I highly recommend the book by Michael Nielsen on deep learning and neural networks.

260
00:17:09,680 --> 00:17:14,072
In it, you can find the code and the data to download and play with for this exact 

261
00:17:14,072 --> 00:17:18,359
example, and the book will walk you through step by step what that code is doing.

262
00:17:19,300 --> 00:17:22,496
What's awesome is that this book is free and publicly available, 

263
00:17:22,496 --> 00:17:26,774
so if you do get something out of it, consider joining me in making a donation towards 

264
00:17:26,774 --> 00:17:27,660
Nielsen's efforts.

265
00:17:27,660 --> 00:17:31,678
I've also linked a couple other resources I like a lot in the description, 

266
00:17:31,678 --> 00:17:36,500
including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.

267
00:17:38,280 --> 00:17:40,564
To close things off here for the last few minutes, 

268
00:17:40,564 --> 00:17:43,880
I want to jump back into a snippet of the interview I had with Leisha Lee.

269
00:17:44,300 --> 00:17:47,720
You might remember her from the last video, she did her PhD work in deep learning.

270
00:17:48,300 --> 00:17:52,064
In this little snippet she talks about two recent papers that really dig into 

271
00:17:52,064 --> 00:17:55,780
how some of the more modern image recognition networks are actually learning.

272
00:17:56,120 --> 00:17:58,510
Just to set up where we were in the conversation, 

273
00:17:58,510 --> 00:18:02,669
the first paper took one of these particularly deep neural networks that's really good 

274
00:18:02,669 --> 00:18:06,493
at image recognition, and instead of training it on a properly labeled dataset, 

275
00:18:06,493 --> 00:18:08,740
shuffled all the labels around before training.

276
00:18:09,480 --> 00:18:12,916
Obviously the testing accuracy here was no better than random, 

277
00:18:12,916 --> 00:18:16,625
since everything is just randomly labeled, but it was still able to 

278
00:18:16,625 --> 00:18:20,880
achieve the same training accuracy as you would on a properly labeled dataset.

279
00:18:21,600 --> 00:18:25,313
Basically, the millions of weights for this particular network were 

280
00:18:25,313 --> 00:18:27,935
enough for it to just memorize the random data, 

281
00:18:27,935 --> 00:18:31,648
which raises the question for whether minimizing this cost function 

282
00:18:31,648 --> 00:18:36,400
actually corresponds to any sort of structure in the image, or is it just memorization?

283
00:18:51,440 --> 00:18:57,405
If you look at that accuracy curve, if you were just training on a random dataset, 

284
00:18:57,405 --> 00:19:02,940
that curve sort of went down very slowly in almost kind of a linear fashion, 

285
00:19:02,940 --> 00:19:07,755
so you're really struggling to find that local minima of possible, 

286
00:19:07,755 --> 00:19:12,140
you know, the right weights that would get you that accuracy.

287
00:19:12,240 --> 00:19:15,823
Whereas if you're actually training on a structured dataset, 

288
00:19:15,823 --> 00:19:20,523
one that has the right labels, you fiddle around a little bit in the beginning, 

289
00:19:20,523 --> 00:19:24,636
but then you kind of dropped very fast to get to that accuracy level, 

290
00:19:24,636 --> 00:19:28,220
and so in some sense it was easier to find that local maxima.

291
00:19:28,540 --> 00:19:33,614
And so what was also interesting about that is it brings into light another paper from 

292
00:19:33,614 --> 00:19:38,688
actually a couple of years ago, which has a lot more simplifications about the network 

293
00:19:38,688 --> 00:19:43,879
layers, but one of the results was saying how if you look at the optimization landscape, 

294
00:19:43,879 --> 00:19:48,662
the local minima that these networks tend to learn are actually of equal quality, 

295
00:19:48,662 --> 00:19:51,462
so in some sense if your dataset is structured, 

296
00:19:51,462 --> 00:19:54,320
you should be able to find that much more easily.

297
00:19:58,160 --> 00:20:01,180
My thanks, as always, to those of you supporting on Patreon.

298
00:20:01,520 --> 00:20:04,065
I've said before just what a game changer Patreon is, 

299
00:20:04,065 --> 00:20:06,800
but these videos really would not be possible without you.

300
00:20:07,460 --> 00:20:10,439
I also want to give a special thanks to the VC firm Amplify Partners, 

301
00:20:10,439 --> 00:20:12,780
in their support of these initial videos in the series.

