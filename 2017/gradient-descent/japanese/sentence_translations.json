[
 {
  "input": "Last video I laid out the structure of a neural network.",
  "translatedText": "前回のビデオでは、ニューラル ネットワークの構造を説明しました。",
  "n_reviews": 0,
  "start": 4.18,
  "end": 7.28
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video.",
  "translatedText": "記憶に新しいように、ここで簡単に要約します。その後、このビデオには 2 つの主な目標があります。",
  "n_reviews": 0,
  "start": 7.68,
  "end": 12.6
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well.",
  "translatedText": "1 つ目は、勾配降下の考え方を導入することです。これは、ニューラル ネットワークの学習方法だけでなく、他の多くの機械学習の仕組みの基礎にもなっています。",
  "n_reviews": 0,
  "start": 13.1,
  "end": 20.6
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for.",
  "translatedText": "その後、この特定のネットワークがどのように動作するか、そしてニューロンの隠れた層が最終的に何を探すのかについてもう少し詳しく掘り下げていきます。",
  "n_reviews": 0,
  "start": 21.12,
  "end": 27.94
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks.",
  "translatedText": "念のため言っておきますが、ここでの目標は手書き数字認識の古典的な例、つまりニューラル ネットワークの Hello World です。",
  "n_reviews": 0,
  "start": 28.98,
  "end": 36.22
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1.",
  "translatedText": "これらの数字は 28x28 ピクセル グリッド上にレンダリングされ、各ピクセルは 0 から 1 までのグレースケール値を持ちます。",
  "n_reviews": 0,
  "start": 37.02,
  "end": 43.42
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network.",
  "translatedText": "これらは、ネットワークの入力層の 784 個のニューロンの活性化を決定するものです。",
  "n_reviews": 0,
  "start": 43.82,
  "end": 50.04
 },
 {
  "input": "And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias.",
  "translatedText": "そして、後続の層の各ニューロンの活性化は、前の層のすべての活性化の加重合計に、バイアスと呼ばれる特別な数値を加えたものに基づいています。",
  "n_reviews": 0,
  "start": 51.18,
  "end": 60.82
 },
 {
  "input": "Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video.",
  "translatedText": "次に、前回のビデオで説明した方法である、シグモイド圧縮や Relu などの他の関数を使用してその合計を作成します。",
  "n_reviews": 0,
  "start": 62.16,
  "end": 68.94
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does.",
  "translatedText": "合計すると、それぞれ 16 個のニューロンを持つ 2 つの隠れ層というやや恣意的な選択を考慮すると、ネットワークには調整できる重みとバイアスが約 13,000 あり、ネットワークが実際に何を行うかを正確に決定するのはこれらの値です。",
  "n_reviews": 0,
  "start": 69.48,
  "end": 84.38
 },
 {
  "input": "Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit.",
  "translatedText": "このネットワークが特定の数字を分類すると言うときの意味は、最終層の 10 個のニューロンの中で最も明るいものがその数字に対応するということです。",
  "n_reviews": 0,
  "start": 84.88,
  "end": 93.3
 },
 {
  "input": "And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits.",
  "translatedText": "ここでレイヤー構造について念頭に置いていた動機は、おそらく 2 番目のレイヤーでエッジを認識し、3 番目のレイヤーでループやラインなどのパターンを認識し、最後のレイヤーでそれらをつなぎ合わせることができるということを思い出してください。数字を認識するパターン。",
  "n_reviews": 0,
  "start": 94.1,
  "end": 108.8
 },
 {
  "input": "So here, we learn how the network learns.",
  "translatedText": "ここでは、ネットワークがどのように学習するかを学びます。",
  "n_reviews": 0,
  "start": 109.8,
  "end": 112.24
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data.",
  "translatedText": "私たちが望んでいるのは、このネットワークに大量のトレーニング データを表示できるアルゴリズムです。このデータは、手書きの数字のさまざまな画像と、それらが本来あるべきものを示すラベルの形で提供されます。トレーニング データのパフォーマンスを向上させるために、これらの 13,000 の重みとバイアスを調整します。",
  "n_reviews": 0,
  "start": 112.64,
  "end": 130.12
 },
 {
  "input": "Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data.",
  "translatedText": "うまくいけば、この階層構造は、学習した内容がトレーニング データを超えた画像に一般化されることを意味します。",
  "n_reviews": 0,
  "start": 130.72,
  "end": 136.86
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images.",
  "translatedText": "これをテストする方法は、ネットワークをトレーニングした後、これまでに見たことのないラベル付きデータをさらに表示し、それらの新しい画像がどの程度正確に分類されているかを確認することです。",
  "n_reviews": 0,
  "start": 137.64,
  "end": 146.7
 },
 {
  "input": "Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be.",
  "translatedText": "私たちにとって幸運なことに、そしてそもそもこれが非常に一般的な例である理由は、MNIST データベースの背後にいる善良な人々が何万もの手書きの数字画像のコレクションをまとめ、それぞれの画像に本来あるべき数字がラベル付けされていることです。なれ。",
  "n_reviews": 0,
  "start": 151.12,
  "end": 164.2
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise.",
  "translatedText": "機械を学習すると表現するのは挑発的ですが、それがどのように機能するかを一度理解すると、それはおかしな SF の前提というよりも、はるかに微積分の練習のように感じられます。",
  "n_reviews": 0,
  "start": 164.9,
  "end": 175.48
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function.",
  "translatedText": "つまり、基本的には、特定の機能の最小値を見つけることになります。",
  "n_reviews": 0,
  "start": 176.2,
  "end": 179.96
 },
 {
  "input": "Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive.",
  "translatedText": "概念的には、各ニューロンは前の層のすべてのニューロンに接続されていると考えており、その活性化を定義する加重和の重みはそれらの接続の強さのようなものであり、バイアスは何らかの指標であることを思い出してください。そのニューロンが活動的になる傾向があるか、不活動的である傾向があるか。",
  "n_reviews": 0,
  "start": 181.94,
  "end": 198.96
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly.",
  "translatedText": "まず、これらの重みとバイアスをすべて完全にランダムに初期化します。",
  "n_reviews": 0,
  "start": 199.72,
  "end": 204.4
 },
 {
  "input": "Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random.",
  "translatedText": "言うまでもなく、このネットワークはランダムに何かを実行しているだけなので、特定のトレーニング例ではかなりひどいパフォーマンスを発揮することになります。",
  "n_reviews": 0,
  "start": 204.94,
  "end": 210.72
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess.",
  "translatedText": "たとえば、この 3 の画像を入力すると、出力レイヤーは混乱したように見えます。",
  "n_reviews": 0,
  "start": 211.04,
  "end": 216.02
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash.",
  "translatedText": "それで、あなたがすることは、コスト関数を定義することです。これは、コンピュータ、いや、悪いコンピュータに、出力の活性化がほとんどのニューロンでは 0 ですが、このニューロンでは 1 になるはずです、あなたが私にくれたものは全くのゴミです、と伝える方法です。",
  "n_reviews": 0,
  "start": 216.6,
  "end": 230.76
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example.",
  "translatedText": "これをもう少し数学的に言うと、これらのゴミ出力アクティベーションのそれぞれと、それらに必要な値との差の二乗を合計します。これが、単一のトレーニング サンプルのコストと呼ばれるものです。",
  "n_reviews": 0,
  "start": 231.72,
  "end": 245.02
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing.",
  "translatedText": "ネットワークが自信を持って画像を正しく分類している場合にはこの合計は小さくなりますが、ネットワークが何をしているかを認識していないように見える場合にはこの合計が大きくなることに注意してください。",
  "n_reviews": 0,
  "start": 245.96,
  "end": 256.4
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.",
  "translatedText": "したがって、自由に使える数万のトレーニング サンプルすべての平均コストを考慮することになります。",
  "n_reviews": 0,
  "start": 258.64,
  "end": 265.44
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel.",
  "translatedText": "この平均コストは、ネットワークがどの程度劣悪であるか、およびコンピュータの動作がどの程度悪くなるかを示す尺度です。",
  "n_reviews": 0,
  "start": 267.04,
  "end": 272.74
 },
 {
  "input": "And that's a complicated thing.",
  "translatedText": "それは複雑なことです。",
  "n_reviews": 0,
  "start": 273.42,
  "end": 274.6
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases?",
  "translatedText": "ネットワーク自体が基本的に 784 個の数値、ピクセル値を入力として取り込み、10 個の数値を出力として吐き出す関数であったことを覚えていますか? ある意味、ネットワークはこれらすべての重みとバイアスによってパラメーター化されています。",
  "n_reviews": 0,
  "start": 275.04,
  "end": 288.8
 },
 {
  "input": "Well the cost function is a layer of complexity on top of that.",
  "translatedText": "そうですね、コスト関数はその上に複雑な層が重なっています。",
  "n_reviews": 0,
  "start": 289.5,
  "end": 292.82
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data.",
  "translatedText": "それらの 13,000 ほどの重みとバイアスを入力として受け取り、それらの重みとバイアスがどれほど悪いかを説明する 1 つの数値を吐き出します。その定義方法は、数万のトレーニング データすべてに対するネットワークの動作によって異なります。",
  "n_reviews": 0,
  "start": 293.1,
  "end": 308.9
 },
 {
  "input": "That's a lot to think about.",
  "translatedText": "それは考えるべきことがたくさんあります。",
  "n_reviews": 0,
  "start": 309.52,
  "end": 311.0
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful.",
  "translatedText": "しかし、コンピューターがどのようなくだらない仕事をしているかを単にコンピューターに伝えるだけでは、あまり役に立ちません。",
  "n_reviews": 0,
  "start": 312.4,
  "end": 315.82
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better.",
  "translatedText": "これらの重みとバイアスを変更して改善する方法を教えたいと考えています。",
  "n_reviews": 0,
  "start": 316.22,
  "end": 320.06
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output.",
  "translatedText": "わかりやすくするために、13,000 個の入力を持つ関数を想像するのに苦労するのではなく、入力として 1 つの数値、出力として 1 つの数値を持つ単純な関数を想像してください。",
  "n_reviews": 0,
  "start": 320.78,
  "end": 330.48
 },
 {
  "input": "How do you find an input that minimizes the value of this function?",
  "translatedText": "この関数の値を最小にする入力をどのように見つけますか?",
  "n_reviews": 0,
  "start": 331.48,
  "end": 335.3
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function.",
  "translatedText": "微積分の学生は、その最小値を明示的に計算できる場合があることを知っているでしょうが、本当に複雑な関数では常に実現可能であるわけではありません。もちろん、この状況の 13,000 入力バージョンでは、非常に複雑なニューラル ネットワークのコスト関数では不可能です。",
  "n_reviews": 0,
  "start": 336.46,
  "end": 351.08
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower.",
  "translatedText": "より柔軟な戦術は、任意の入力から開始して、その出力を下げるためにどの方向にステップを進めるべきかを判断することです。",
  "n_reviews": 0,
  "start": 351.58,
  "end": 359.2
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative.",
  "translatedText": "具体的には、現在の関数の傾きがわかる場合は、その傾きが正の場合は左にシフトし、その傾きが負の場合は入力を右にシフトします。",
  "n_reviews": 0,
  "start": 360.08,
  "end": 369.9
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function.",
  "translatedText": "これを繰り返し実行し、各ポイントで新しい傾きをチェックし、適切な手順を実行すると、関数の極小値に近づくことになります。",
  "n_reviews": 0,
  "start": 371.96,
  "end": 379.84
 },
 {
  "input": "The image you might have in mind here is a ball rolling down a hill.",
  "translatedText": "ここで皆さんが思い浮かべるのは、丘を転がり落ちるボールです。",
  "n_reviews": 0,
  "start": 380.64,
  "end": 383.8
 },
 {
  "input": "Notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function.",
  "translatedText": "この非常に単純化された単一入力関数であっても、どのランダム入力から開始するかに応じて、到達する可能性のある谷が多数あり、到達する極小値が可能な限り最小の値になるという保証はないことに注意してください。コスト関数の。",
  "n_reviews": 0,
  "start": 384.62,
  "end": 399.4
 },
 {
  "input": "That will carry over to our neural network case as well.",
  "translatedText": "これはニューラル ネットワークのケースにも引き継がれます。",
  "n_reviews": 0,
  "start": 400.22,
  "end": 402.62
 },
 {
  "input": "I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that helps you from overshooting.",
  "translatedText": "また、ステップ サイズを勾配に比例させた場合、勾配が最小に向かって平坦になるとステップがどんどん小さくなり、オーバーシュートが防止されることにも注目してください。",
  "n_reviews": 0,
  "start": 403.18,
  "end": 414.6
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output.",
  "translatedText": "もう少し複雑にして、代わりに 2 つの入力と 1 つの出力を持つ関数を想像してください。",
  "n_reviews": 0,
  "start": 415.94,
  "end": 420.98
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it.",
  "translatedText": "入力空間を xy 平面、コスト関数をその上の面としてグラフ化すると考えることができます。",
  "n_reviews": 0,
  "start": 421.5,
  "end": 428.14
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly.",
  "translatedText": "関数の傾きについて尋ねる代わりに、関数の出力を最も早く減少させるためには、この入力空間をどの方向にステップすべきかを尋ねる必要があります。",
  "n_reviews": 0,
  "start": 428.76,
  "end": 438.96
 },
 {
  "input": "In other words, what's the downhill direction?",
  "translatedText": "言い換えれば、下り坂の方向は何ですか？",
  "n_reviews": 0,
  "start": 439.72,
  "end": 441.76
 },
 {
  "input": "Again, it's helpful to think of a ball rolling down that hill.",
  "translatedText": "繰り返しになりますが、ボールが丘を転がり落ちていくことを考えるとわかりやすいでしょう。",
  "n_reviews": 0,
  "start": 442.38,
  "end": 445.56
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly.",
  "translatedText": "多変数微積分に詳しい人は、関数の勾配によって最も急な上昇の方向がわかり、関数を最も早く増加させるにはどの方向に進むべきかがわかるでしょう。",
  "n_reviews": 0,
  "start": 446.66,
  "end": 458.78
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly.",
  "translatedText": "当然のことながら、その勾配をマイナスにすると、関数を最も早く減少させるステップの方向がわかります。",
  "n_reviews": 0,
  "start": 459.56,
  "end": 466.04
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is.",
  "translatedText": "さらに、この勾配ベクトルの長さは、その最も急な勾配がどれほど急であるかを示します。",
  "n_reviews": 0,
  "start": 467.24,
  "end": 473.84
 },
 {
  "input": "If you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic.",
  "translatedText": "多変数微積分に詳しくなく、さらに詳しく知りたい場合は、このテーマに関して私がカーン アカデミーで行った作品の一部を確認してください。",
  "n_reviews": 0,
  "start": 474.54,
  "end": 480.34
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is.",
  "translatedText": "しかし正直に言って、あなたと私にとって現時点で重要なのは、原理的にはこのベクトル、つまり下り坂の方向とその急勾配を示すベクトルを計算する方法が存在するということだけです。",
  "n_reviews": 0,
  "start": 480.86,
  "end": 491.9
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details.",
  "translatedText": "知っていることがこれだけで、詳細がしっかりしていなくても大丈夫です。",
  "n_reviews": 0,
  "start": 492.4,
  "end": 496.12
 },
 {
  "input": "If you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over.",
  "translatedText": "それが得られれば、関数を最小化するアルゴリズムは、この勾配の方向を計算し、下り坂に向かって小さな一歩を踏み出し、それを何度も繰り返すことになります。",
  "n_reviews": 0,
  "start": 497.2,
  "end": 506.74
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.",
  "translatedText": "これは、2 つの入力ではなく 13,000 の入力を持つ関数の基本的な考え方と同じです。",
  "n_reviews": 0,
  "start": 507.7,
  "end": 512.82
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
  "translatedText": "ネットワークの 13,000 の重みとバイアスをすべて巨大な列ベクトルに編成することを想像してください。",
  "n_reviews": 0,
  "start": 513.4,
  "end": 519.46
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function.",
  "translatedText": "コスト関数の負の勾配は単なるベクトルであり、この非常に巨大な入力空間内の何らかの方向であり、これらすべての数値に対するどの微調整がコスト関数の最も急速な減少を引き起こすかを示します。",
  "n_reviews": 0,
  "start": 520.14,
  "end": 534.88
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make.",
  "translatedText": "そしてもちろん、特別に設計されたコスト関数を使用して、重みとバイアスを変更して値を下げることは、トレーニング データの各部分に対するネットワークの出力を 10 個の値のランダムな配列のように見せることを意味し、より実際に望む決定に近づけることを意味します。それを作るのです。",
  "n_reviews": 0,
  "start": 535.64,
  "end": 550.82
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples.",
  "translatedText": "覚えておくことが重要です。このコスト関数にはすべてのトレーニング データの平均が含まれるため、これを最小化すると、それらのサンプルすべてでパフォーマンスが向上することになります。",
  "n_reviews": 0,
  "start": 551.44,
  "end": 561.18
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video.",
  "translatedText": "この勾配を効率的に計算するためのアルゴリズムは、事実上、ニューラル ネットワークの学習方法の中心であり、バックプロパゲーションと呼ばれます。これについては、次のビデオで説明します。",
  "n_reviews": 0,
  "start": 563.82,
  "end": 573.98
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas.",
  "translatedText": "そこでは、時間をかけて、特定のトレーニング データの各重みとバイアスに正確に何が起こっているのかを詳しく見ていき、関連する計算や式の山の向こうで何が起こっているのかを直感的に感じられるようにしたいと考えています。",
  "n_reviews": 0,
  "start": 574.66,
  "end": 587.1
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function.",
  "translatedText": "今ここで、実装の詳細とは関係なく、私が皆さんに知っておいていただきたい主なことは、ネットワーク学習について話すときの意味は、コスト関数を最小化するだけだということです。",
  "n_reviews": 0,
  "start": 587.78,
  "end": 598.36
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill.",
  "translatedText": "そして、その結果の 1 つとして、このコスト関数が良好な滑らかな出力を持つことが重要であることに注意してください。これにより、下り坂を少しずつ進むことで極小値を見つけることができます。",
  "n_reviews": 0,
  "start": 599.3,
  "end": 608.1
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way biological neurons are.",
  "translatedText": "ちなみに、生物学的ニューロンのように単に二値的に活性または不活性になるのではなく、人工ニューロンが継続的に範囲の活性化を行うのはこのためです。",
  "n_reviews": 0,
  "start": 609.26,
  "end": 619.14
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent.",
  "translatedText": "負の勾配の倍数によって関数の入力を繰り返し微調整するこのプロセスは、勾配降下法と呼ばれます。",
  "n_reviews": 0,
  "start": 620.22,
  "end": 626.76
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph.",
  "translatedText": "これは、コスト関数の局所最小値、つまりこのグラフの谷に向かって収束する方法です。",
  "n_reviews": 0,
  "start": 627.3,
  "end": 632.58
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is a nice non-spatial way to think about this.",
  "translatedText": "もちろん、13,000 次元の入力空間でのナッジは少し理解するのが難しいため、まだ 2 つの入力を持つ関数の図を示していますが、これについては空間を使わずに考える良い方法があります。",
  "n_reviews": 0,
  "start": 633.44,
  "end": 644.26
 },
 {
  "input": "Each component of the negative gradient tells us two things.",
  "translatedText": "負の勾配の各成分から 2 つのことが分かります。",
  "n_reviews": 0,
  "start": 645.08,
  "end": 648.44
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down.",
  "translatedText": "もちろん、この符号は、入力ベクトルの対応するコンポーネントを上または下に微調整する必要があるかどうかを示します。",
  "n_reviews": 0,
  "start": 649.06,
  "end": 655.14
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more.",
  "translatedText": "しかし重要なのは、これらすべての要素の相対的な大きさによって、どの変更がより重要であるかがわかるということです。",
  "n_reviews": 0,
  "start": 655.8,
  "end": 662.72
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.",
  "translatedText": "私たちのネットワークでは、重みの 1 つを調整すると、他の重みを調整するよりもコスト関数にはるかに大きな影響を与える可能性があります。",
  "n_reviews": 0,
  "start": 665.22,
  "end": 673.04
 },
 {
  "input": "Some of these connections just matter more for our training data.",
  "translatedText": "これらの接続の中には、トレーニング データにとってより重要なものもあります。",
  "n_reviews": 0,
  "start": 674.8,
  "end": 678.2
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck.",
  "translatedText": "したがって、気が遠くなるような膨大なコスト関数のこの勾配ベクトルについて考える方法は、各重みとバイアスの相対的な重要性、つまり、これらの変更のどれが最も費用対効果が高いかをエンコードしているということです。",
  "n_reviews": 0,
  "start": 679.32,
  "end": 692.4
 },
 {
  "input": "This really is just another way of thinking about direction.",
  "translatedText": "これは方向性についての単なる考え方です。",
  "n_reviews": 0,
  "start": 693.62,
  "end": 696.64
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and you compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction.",
  "translatedText": "より単純な例を挙げると、入力として 2 つの変数を持つ関数があり、ある特定の点での勾配が 3,1 になると計算した場合、一方では、次のような場合にそれを言っていると解釈できます。その入力に立って、この方向に沿って移動すると、関数が最も早く増加します。つまり、入力点の平面上で関数をグラフにすると、そのベクトルが真っ直ぐ上り坂の方向を与えることになります。",
  "n_reviews": 0,
  "start": 697.1,
  "end": 722.26
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have 3 times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck.",
  "translatedText": "しかし、これを別の読み方で読むと、この最初の変数への変更は 2 番目の変数への変更の 3 倍の重要性があり、少なくとも関連する入力の近傍では、X 値を微調整する方がはるかに大きな影響を与えるということになります。バック。",
  "n_reviews": 0,
  "start": 722.86,
  "end": 736.9
 },
 {
  "input": "Let's zoom out and sum up where we are so far.",
  "translatedText": "ズームアウトして、これまでの状況をまとめてみましょう。",
  "n_reviews": 0,
  "start": 739.88,
  "end": 742.34
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all these weighted sums.",
  "translatedText": "ネットワーク自体は、784 個の入力と 10 個の出力を備えた関数であり、これらすべての加重合計によって定義されます。",
  "n_reviews": 0,
  "start": 742.84,
  "end": 750.04
 },
 {
  "input": "The cost function is a layer of complexity on top of that.",
  "translatedText": "コスト関数はその上に複雑な層が重なっています。",
  "n_reviews": 0,
  "start": 750.64,
  "end": 753.68
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs and spits out a single measure of lousiness based on the training examples.",
  "translatedText": "13,000 の重みとバイアスを入力として受け取り、トレーニング例に基づいて粗さの単一の尺度を吐き出します。",
  "n_reviews": 0,
  "start": 753.98,
  "end": 761.72
 },
 {
  "input": "And the gradient of the cost function is one more layer of complexity still.",
  "translatedText": "そして、コスト関数の勾配はさらに複雑な層になります。",
  "n_reviews": 0,
  "start": 762.44,
  "end": 766.9
 },
 {
  "input": "It tells us what nudges to all these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most.",
  "translatedText": "これは、これらすべての重みとバイアスに対するどのような調整がコスト関数の値に最も速い変化を引き起こすかを示します。これは、どの重みに対するどの変更が最も重要かを示していると解釈できます。",
  "n_reviews": 0,
  "start": 767.36,
  "end": 777.88
 },
 {
  "input": "So, when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before?",
  "translatedText": "では、ランダムな重みとバイアスを使用してネットワークを初期化し、この勾配降下プロセスに基づいてそれらを何度も調整すると、これまでに見たことのない画像で実際にどの程度のパフォーマンスが得られるでしょうか?",
  "n_reviews": 0,
  "start": 782.56,
  "end": 793.2
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly.",
  "translatedText": "私がここで説明したものは、それぞれ 16 個のニューロンからなる 2 つの隠れ層を備えており、主に美的理由から選択されていますが、悪くはなく、表示される新しい画像の約 96% を正しく分類しています。",
  "n_reviews": 0,
  "start": 794.1,
  "end": 805.96
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack.",
  "translatedText": "そして正直に言うと、それが台無しにしているいくつかの例を見ると、少し緩めなければならないと感じます。",
  "n_reviews": 0,
  "start": 806.68,
  "end": 812.54
 },
 {
  "input": "Now if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%.",
  "translatedText": "ここで、隠れ層構造を試していくつかの調整を加えると、これを最大 98% まで達成できます。",
  "n_reviews": 0,
  "start": 816.22,
  "end": 821.76
 },
 {
  "input": "And that's pretty good!",
  "translatedText": "それはとても良いことです！",
  "n_reviews": 0,
  "start": 821.76,
  "end": 822.72
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before, given that we never specifically told it what patterns to look for.",
  "translatedText": "これは最高ではありません。この単純なバニラ ネットワークよりも洗練されれば、確かにパフォーマンスを向上させることはできますが、最初のタスクがどれほど困難であるかを考えると、これまでに見たことのない画像でこれほどうまく動作するネットワークには、信じられないほどの何かがあると思います。どのようなパターンを探すべきかを具体的に指示したことはありません。",
  "n_reviews": 0,
  "start": 823.02,
  "end": 841.42
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits.",
  "translatedText": "もともと、私がこの構造を動機づけた方法は、2 番目の層が小さなエッジを検出し、3 番目の層がそれらのエッジをつなぎ合わせてループや長い線を認識し、それらがつなぎ合わされるかもしれないという希望を説明することでした。一緒に数字を認識します。",
  "n_reviews": 0,
  "start": 842.56,
  "end": 857.18
 },
 {
  "input": "So is this what our network is actually doing?",
  "translatedText": "では、これは私たちのネットワークが実際に行っていることなのでしょうか?",
  "n_reviews": 0,
  "start": 857.96,
  "end": 860.4
 },
 {
  "input": "Well, for this one at least, not at all.",
  "translatedText": "まあ、少なくともこれに関しては、まったくそうではありません。",
  "n_reviews": 0,
  "start": 861.08,
  "end": 864.4
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on?",
  "translatedText": "前回のビデオで、最初の層のすべてのニューロンから 2 番目の層の特定のニューロンへの接続の重みが、2 番目の層のニューロンが認識している特定のピクセル パターンとしてどのように視覚化できるかを説明したことを覚えていますか?",
  "n_reviews": 0,
  "start": 864.82,
  "end": 877.06
 },
 {
  "input": "Well, when we actually do that for the weights associated with these transitions, from the first layer to the next, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle there.",
  "translatedText": "最初のレイヤーから次のレイヤーまで、これらのトランジションに関連付けられたウェイトに対して実際にそれを行うと、あちこちで孤立した小さなエッジを検出するのではなく、それらは、ほとんどランダムに見えますが、非常に緩やかなパターンがいくつかあるだけです。そこの真ん中。",
  "n_reviews": 0,
  "start": 877.78,
  "end": 893.68
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for.",
  "translatedText": "考えられる重みとバイアスの計り知れないほど広い 13,000 次元空間において、私たちのネットワークは、ほとんどの画像を正常に分類したにもかかわらず、私たちが期待していたパターンを正確に検出できない、幸せな小さな局所最小値を見つけたようです。",
  "n_reviews": 0,
  "start": 893.76,
  "end": 908.96
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image.",
  "translatedText": "この点をよく理解するには、ランダムな画像を入力したときに何が起こるかを見てください。",
  "n_reviews": 0,
  "start": 909.78,
  "end": 913.82
 },
 {
  "input": "If the system was smart, you might expect it to feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5.",
  "translatedText": "システムが賢いものであれば、10 個の出力ニューロンのどれも実際には活性化していない、あるいはすべてを均等に活性化していないなど、不確かに感じられることを期待するかもしれませんが、その代わりに、このランダムなノイズが確実であるかのように、自信を持ってナンセンスな答えを返します。 5 は、5 の実際の画像が 5 であるのと同様に、5 です。",
  "n_reviews": 0,
  "start": 914.32,
  "end": 934.16
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them.",
  "translatedText": "別の言い方をすると、このネットワークは数字をかなりうまく認識できても、それを描画する方法がわかりません。",
  "n_reviews": 0,
  "start": 934.54,
  "end": 940.7
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup.",
  "translatedText": "その多くは、トレーニングの設定が非常に厳しく制限されているためです。",
  "n_reviews": 0,
  "start": 941.42,
  "end": 945.24
 },
 {
  "input": "I mean, put yourself in the network's shoes here.",
  "translatedText": "つまり、ここではネットワークの立場に立って考えてみましょう。",
  "n_reviews": 0,
  "start": 945.88,
  "end": 947.74
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions.",
  "translatedText": "その観点から見ると、宇宙全体は小さなグリッドの中心にある、明確に定義された不動の数字だけで構成されており、そのコスト関数は、その決定に完全な自信を持つこと以外には何の動機も与えませんでした。",
  "n_reviews": 0,
  "start": 948.14,
  "end": 961.08
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns.",
  "translatedText": "これが第 2 層のニューロンが実際に行っていることのイメージであるため、なぜエッジやパターンを検出するという動機でこのネットワークを紹介するのか不思議に思うかもしれません。",
  "n_reviews": 0,
  "start": 962.12,
  "end": 969.92
 },
 {
  "input": "I mean, that's just not at all what it ends up doing.",
  "translatedText": "つまり、それは最終的にはまったくそうではありません。",
  "n_reviews": 0,
  "start": 969.92,
  "end": 972.3
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point.",
  "translatedText": "これは最終目標ではなく、出発点です。",
  "n_reviews": 0,
  "start": 973.38,
  "end": 977.18
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems.",
  "translatedText": "率直に言って、これは 80 年代から 90 年代に研究された種類の古いテクノロジーであり、より詳細な現代の亜種を理解する前に、それを理解する必要があります。また、明らかにいくつかの興味深い問題を解決できる可能性がありますが、何を深く掘り下げるほど、これらの隠れ層が実際に機能しているほど、その層は知性が低く見えます。",
  "n_reviews": 0,
  "start": 977.64,
  "end": 994.74
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow.",
  "translatedText": "ネットワークがどのように学習するかということから、あなたがどのように学習するかに少し焦点を移しますが、それは、何らかの方法でここで取り上げた資料に積極的に取り組んだ場合にのみ起こります。",
  "n_reviews": 0,
  "start": 998.48,
  "end": 1006.3
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns.",
  "translatedText": "皆さんにしていただきたいのは、非常に簡単なことの 1 つです。今ちょっと立ち止まって、このシステムにどのような変更を加えるか、エッジやパターンなどをよりよく認識できるようにしたい場合に画像をどのように認識するかについて、少しの間深く考えてみてください。",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1020.88
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks.",
  "translatedText": "しかしそれよりも、実際に教材に取り組むには、深層学習とニューラル ネットワークに関するマイケル ニールセンの本を強くお勧めします。",
  "n_reviews": 0,
  "start": 1021.48,
  "end": 1029.1
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing.",
  "translatedText": "この中には、まさにこの例をダウンロードして試すためのコードとデータが含まれており、そのコードが何をしているのかを段階的に説明します。",
  "n_reviews": 0,
  "start": 1029.68,
  "end": 1038.36
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts.",
  "translatedText": "素晴らしいのは、この本が無料で一般公開されているということです。この本から何かを得ることができましたら、私と一緒にニールセンの取り組みに寄付することを検討してください。",
  "n_reviews": 0,
  "start": 1039.3,
  "end": 1047.66
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.",
  "translatedText": "また、Chris Ola による驚異的で美しいブログ投稿や Distill の記事など、私がとても気に入っている他のリソースも説明にリンクしました。",
  "n_reviews": 0,
  "start": 1047.66,
  "end": 1056.5
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee.",
  "translatedText": "最後の数分間をここで締めくくるために、リーシャ・リーとのインタビューの抜粋に戻りたいと思います。",
  "n_reviews": 0,
  "start": 1058.28,
  "end": 1063.88
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning.",
  "translatedText": "前回のビデオで彼女を覚えているかもしれません。彼女は深層学習で博士号の研究をしていました。",
  "n_reviews": 0,
  "start": 1064.3,
  "end": 1067.72
 },
 {
  "input": "In this little snippet she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning.",
  "translatedText": "この短い抜粋で、彼女は、いくつかの最新の画像認識ネットワークが実際にどのように学習しているかを深く掘り下げた 2 つの最近の論文について話しています。",
  "n_reviews": 0,
  "start": 1068.3,
  "end": 1075.78
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, shuffled all the labels around before training.",
  "translatedText": "私たちが会話の中でどのような状況にあったかを説明するために、最初の論文では、画像認識に非常に優れた特にディープ ニューラル ネットワークの 1 つを取り上げ、適切にラベル付けされたデータセットでトレーニングする代わりに、トレーニング前にすべてのラベルをシャッフルしました。",
  "n_reviews": 0,
  "start": 1076.12,
  "end": 1088.74
 },
 {
  "input": "Obviously the testing accuracy here was no better than random, since everything is just randomly labeled, but it was still able to achieve the same training accuracy as you would on a properly labeled dataset.",
  "translatedText": "すべてがランダムにラベル付けされているだけであるため、ここでのテスト精度は明らかにランダムよりも優れていませんでしたが、それでも適切にラベル付けされたデータセットで行うのと同じトレーニング精度を達成することができました。",
  "n_reviews": 0,
  "start": 1089.48,
  "end": 1100.88
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization?",
  "translatedText": "基本的に、この特定のネットワークの何百万もの重みは、ランダムなデータを記憶するだけで十分でした。このため、このコスト関数の最小化が実際に画像内の何らかの構造に対応するのか、それとも単なる記憶なのかという疑問が生じます。",
  "n_reviews": 0,
  "start": 1101.6,
  "end": 1116.4
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very slowly in almost kind of a linear fashion, so you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy.",
  "translatedText": "その精度曲線を見ると、ランダムなデータセットでトレーニングしているだけだとすると、その曲線はほぼ直線的に非常にゆっくりと下がっていくので、可能な極小値を見つけるのに本当に苦労しています。 、その精度を実現する適切な重み。",
  "n_reviews": 0,
  "start": 1131.44,
  "end": 1152.14
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level, and so in some sense it was easier to find that local maxima.",
  "translatedText": "一方、適切なラベルを持つ構造化データセットで実際にトレーニングしている場合、最初は少しいじってみましたが、その後、その精度レベルに到達するのが非常に早くなったので、ある意味では極大値を見つけるのが簡単でした。",
  "n_reviews": 0,
  "start": 1152.24,
  "end": 1168.22
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers, but one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality, so in some sense if your dataset is structured, you should be able to find that much more easily.",
  "translatedText": "そして、これに関して興味深いのは、実際に数年前の別の論文が明らかになったということです。この論文では、ネットワーク層についてはさらに単純化されていますが、その結果の 1 つは、最適化の状況を見てみるとどうなるかを示していました。これらのネットワークが学習する傾向にある極小値は、実際には同じ品質であるため、ある意味、データセットが構造化されていれば、それをはるかに簡単に見つけることができるはずです。",
  "n_reviews": 0,
  "start": 1168.54,
  "end": 1194.32
 },
 {
  "input": "My thanks, as always, to those of you supporting on Patreon.",
  "translatedText": "いつものように、Patreon でサポートしてくださっている皆様に感謝します。",
  "n_reviews": 0,
  "start": 1198.16,
  "end": 1201.18
 },
 {
  "input": "I've said before just what a game changer Patreon is, but these videos really would not be possible without you.",
  "translatedText": "Patreon がいかにゲームチェンジャーであるかについては以前にも述べましたが、これらのビデオは本当に皆さんなしでは不可能です。",
  "n_reviews": 0,
  "start": 1201.52,
  "end": 1206.8
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners, in their support of these initial videos in the series.",
  "translatedText": "また、シリーズの最初のビデオをサポートしてくれたベンチャーキャピタル企業 Amplify Partners にも特別な感謝を表したいと思います。",
  "n_reviews": 0,
  "start": 1207.46,
  "end": 1212.78
 }
]