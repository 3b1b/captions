[
 {
  "input": "Last video I laid out the structure of a neural network. ",
  "translatedText": "前回のビデオでは、ニューラル ネットワークの構造を説明しました。",
  "model": "nmt",
  "time_range": [
   0.0,
   7.24
  ]
 },
 {
  "input": "I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video. ",
  "translatedText": "記憶に新しいように、ここで簡単に要約します。それか ら、このビデオには 2 つの主な目標があります。",
  "model": "nmt",
  "time_range": [
   7.24,
   13.16
  ]
 },
 {
  "input": "The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well. ",
  "translatedText": "1 つ目は、勾配降下の考え方を導入することです。これは、ニューラル ネットワ ークの学習方法だけでなく、他の多くの機械学習の仕組みの基礎にもなっています。",
  "model": "nmt",
  "time_range": [
   13.16,
   20.8
  ]
 },
 {
  "input": "Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for. ",
  "translatedText": "その後、この特定のネットワークがどのように動作するか、そしてニューロンの 隠れた層が最終的に何を探すのかについてもう少し詳しく掘り下げていきます。",
  "model": "nmt",
  "time_range": [
   20.8,
   29.56
  ]
 },
 {
  "input": "As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks. ",
  "translatedText": "念のため言っておきますが、ここでの目標は手書き数字認識の古典的な例 、つまりニューラル ネットワークの Hello World です。",
  "model": "nmt",
  "time_range": [
   29.56,
   37.08
  ]
 },
 {
  "input": "These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1. ",
  "translatedText": "これらの数字は 28x28 ピクセル グリッド上にレンダリングされ 、各ピクセルは 0 から 1 までのグレースケール値を持ちます。",
  "model": "nmt",
  "time_range": [
   37.08,
   44.26
  ]
 },
 {
  "input": "Those are what determine the activations of 784 neurons in the input layer of the network. ",
  "translatedText": "これらは、ネットワークの入力層の 784 個のニューロンの活性化を決定するものです。",
  "model": "nmt",
  "time_range": [
   44.26,
   51.4
  ]
 },
 {
  "input": "The activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias. ",
  "translatedText": "次の層の各ニューロンの活性化は、前の層のすべての活性化の重み付き合 計に、バイアスと呼ばれる特別な数値を加えたものに基づいています。",
  "model": "nmt",
  "time_range": [
   51.4,
   62.3
  ]
 },
 {
  "input": "You compose that sum with some other function, like the sigmoid squishification, or a ReLU, the way I walked through last video. ",
  "translatedText": "前回のビデオで説明した方法である、シグモイド圧縮や R eLU などの他の関数を使用してその合計を計算します。",
  "model": "nmt",
  "time_range": [
   62.3,
   69.64
  ]
 },
 {
  "input": "In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does. ",
  "translatedText": "合計すると、それぞれ 16 個のニューロンを持つ 2 つの隠れ層というやや恣意 的な選択を考慮すると、ネットワークには調整できる重みとバイアスが約 13,00 0 あり、ネットワークが実際に何を行うかを正確に決定するのはこれらの値です。",
  "model": "nmt",
  "time_range": [
   69.64,
   85.32
  ]
 },
 {
  "input": "And what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit. ",
  "translatedText": "そして、このネットワークが特定の数字を分類すると言うときの意味は、最後の層にある 10 個のニューロンの中で最も明るいものがその数字に対応するということです。",
  "model": "nmt",
  "time_range": [
   85.32,
   94.08
  ]
 },
 {
  "input": "And remember, the motivation we had in mind for the layered structure was that maybe the second layer could pick up on the edges, the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits. ",
  "translatedText": "レイヤー構造について私たちが念頭に置いていた動機は、おそ らく 2 番目のレイヤーでエッジを認識し、3 番目のレイ ヤーでループやラインなどのパターンを認識し、最後のレイヤ ーでそれらのパターンをつなぎ合わせて、数字を認識します。",
  "model": "nmt",
  "time_range": [
   94.08,
   109.64
  ]
 },
 {
  "input": "So here, we learn how the network learns. ",
  "translatedText": "ここでは、ネットワークがどのように学習するかを学びます。",
  "model": "nmt",
  "time_range": [
   109.64,
   112.88
  ]
 },
 {
  "input": "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data. ",
  "translatedText": "私たちが望んでいるのは、このネットワークに大量のトレーニング データを表示でき るアルゴリズムです。このデータは、手書きの数字のさまざまな画像と、それらが本来 あるべきものを示すラベルの形で提供されます。トレーニング データのパフォーマン スを向上させるために、これらの 13,000 の重みとバイアスを調整します。",
  "model": "nmt",
  "time_range": [
   112.88,
   130.76
  ]
 },
 {
  "input": "Hopefully this layered structure will mean that what it learns generalizes to images beyond that training data. ",
  "translatedText": "この階層構造により、学習した内容がトレーニング デ ータを超えた画像に一般化されることが期待されます。",
  "model": "nmt",
  "time_range": [
   130.76,
   137.84
  ]
 },
 {
  "input": "The way we test that is that after you train the network, you show it more labeled data, and you see how accurately it classifies those new images. ",
  "translatedText": "これをテストする方法は、ネットワークをトレーニングした後、さらにラベル付けされた データを表示し、新しい画像がどの程度正確に分類されているかを確認することです。",
  "model": "nmt",
  "time_range": [
   137.84,
   151.16
  ]
 },
 {
  "input": "Fortunately for us, and what makes this a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each labeled with the numbers they're supposed to be. ",
  "translatedText": "私たちにとって幸運なことに、そしてまずこれが一般的な例となっているのは、MNI ST データベースの背後にいる善良な人々が、それぞれが本来あるべき数字でラベ ル付けされた何万もの手書きの数字画像のコレクションをまとめてくれたことです。",
  "model": "nmt",
  "time_range": [
   151.16,
   165.08
  ]
 },
 {
  "input": "And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise. ",
  "translatedText": "機械を学習すると表現するのは挑発的ですが、それがどのように機能するかを一度理解すると、 それはおかしな SF の前提というよりも、はるかに微積分の練習のように感じられます。",
  "model": "nmt",
  "time_range": [
   165.08,
   175.56
  ]
 },
 {
  "input": "I mean, basically it comes down to finding the minimum of a certain function. ",
  "translatedText": "つまり、基本的には、特定の機能の最小値を見つけることになります。",
  "model": "nmt",
  "time_range": [
   175.56,
   181.04
  ]
 },
 {
  "input": "Remember, conceptually we're thinking of each neuron as being connected to all of the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive. ",
  "translatedText": "概念的には、各ニューロンは前の層のすべてのニューロンに接続されていると考 えており、その活性化を定義する加重合計の重みはそれらの接続の強さのよう なものであり、バイアスは何らかの指標であることを覚えておいてください。そのニューロンが活動的になる傾向があるか、不活動的である傾向があるか。",
  "model": "nmt",
  "time_range": [
   181.04,
   199.78
  ]
 },
 {
  "input": "And to start things off, we're just going to initialize all of those weights and biases totally randomly. ",
  "translatedText": "まず、これらの重みとバイアスをすべ て完全にランダムに初期化します。",
  "model": "nmt",
  "time_range": [
   199.78,
   205.02
  ]
 },
 {
  "input": "Needless to say, this network is going to perform horribly on a given training example, since it's just doing something random. ",
  "translatedText": "言うまでもなく、このネットワークはランダムに何かを実行しているだけ なので、特定のトレーニング例ではひどいパフォーマンスになります。",
  "model": "nmt",
  "time_range": [
   205.02,
   211.18
  ]
 },
 {
  "input": "For example, you feed in this image of a 3, and the output layer just looks like a mess. ",
  "translatedText": "たとえば、この 3 の画像を入力すると、出力レイヤーは混乱したように見えます。",
  "model": "nmt",
  "time_range": [
   211.18,
   216.82
  ]
 },
 {
  "input": "So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron. ",
  "translatedText": "そこで、あなたがすることは、コスト関数を定義することです。これは、出力がほとんどのニューロンでは 0 ですが、この ニューロンでは 1 である活性化を持つべきであることをコンピューター、いや、悪いコンピューターに伝える方法です。",
  "model": "nmt",
  "time_range": [
   216.82,
   228.94
  ]
 },
 {
  "input": "What you gave me is utter trash. ",
  "translatedText": "あなたが私にくれたものは全くのゴミです。",
  "model": "nmt",
  "time_range": [
   228.94,
   231.74
  ]
 },
 {
  "input": "To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example. ",
  "translatedText": "これをもう少し数学的に言うと、これらのゴミ出力アクティベーショ ンのそれぞれと、それらに必要な値との差の二乗を合計します。これ が、単一のトレーニング サンプルのコストと呼ばれるものです。",
  "model": "nmt",
  "time_range": [
   231.74,
   246.02
  ]
 },
 {
  "input": "Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing. ",
  "translatedText": "ネットワークが自信を持って画像を正しく分類している場合にはこの合計は小さくなりますが、ネットワーク が何をしているかを認識していないように見える場合にはこの合計が大きくなることに注意してください。",
  "model": "nmt",
  "time_range": [
   246.02,
   258.82
  ]
 },
 {
  "input": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal. ",
  "translatedText": "したがって、自由に使える数万のトレーニング サン プルすべての平均コストを考慮することになります。",
  "model": "nmt",
  "time_range": [
   258.82,
   267.58
  ]
 },
 {
  "input": "This average cost is our measure for how lousy the network is, and how bad the computer should feel. ",
  "translatedText": "この平均コストは、ネットワークがどの程度劣悪であるか、およ びコンピュータの動作がどの程度悪くなるかを示す尺度です。",
  "model": "nmt",
  "time_range": [
   267.58,
   273.3
  ]
 },
 {
  "input": "And that's a complicated thing. ",
  "translatedText": "それは複雑なことです。",
  "model": "nmt",
  "time_range": [
   273.3,
   275.3
  ]
 },
 {
  "input": "Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases? ",
  "translatedText": "ネットワーク自体が基本的に 784 個の数値、ピクセル値を入力として取り込み、1 0 個の数値を出力として吐き出す関数であったことを覚えていますか? ある意味、 ネットワークはこれらすべての重みとバイアスによってパラメーター化されています。",
  "model": "nmt",
  "time_range": [
   275.3,
   289.7
  ]
 },
 {
  "input": "The cost function is a layer of complexity on top of that. ",
  "translatedText": "コスト関数は、その上に複雑な層が重なっています。",
  "model": "nmt",
  "time_range": [
   289.7,
   293.34
  ]
 },
 {
  "input": "It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data. ",
  "translatedText": "それらの 13,000 ほどの重みとバイアスを入力として受け取り、それらの重みと バイアスがどれほど悪いかを説明する 1 つの数値を吐き出します。その定義方法は、 数万のトレーニング データすべてに対するネットワークの動作によって異なります。",
  "model": "nmt",
  "time_range": [
   293.34,
   309.14
  ]
 },
 {
  "input": "That's a lot to think about. ",
  "translatedText": "それは考えるべきことがたくさんあります。",
  "model": "nmt",
  "time_range": [
   309.14,
   312.46
  ]
 },
 {
  "input": "But just telling the computer what a crappy job it's doing isn't very helpful. ",
  "translatedText": "しかし、コンピューターがどのようなくだらない仕事をしているかを単にコンピューターに伝えるだけでは、あまり役に立ちません。",
  "model": "nmt",
  "time_range": [
   312.46,
   316.38
  ]
 },
 {
  "input": "You want to tell it how to change those weights and biases so that it gets better. ",
  "translatedText": "これらの重みとバイアスを変更して改善する方法を教えたいと考えています。",
  "model": "nmt",
  "time_range": [
   316.38,
   321.3
  ]
 },
 {
  "input": "To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output. ",
  "translatedText": "わかりやすくするために、13,000 個の入力を持つ関数を想像するのに苦労するのではなく 、入力として 1 つの数値、出力として 1 つの数値を持つ単純な関数を想像してください。",
  "model": "nmt",
  "time_range": [
   321.3,
   331.44
  ]
 },
 {
  "input": "How do you find an input that minimizes the value of this function? ",
  "translatedText": "この関数の値を最小にする入力をどのように見つけますか? ",
  "model": "nmt",
  "time_range": [
   331.44,
   336.42
  ]
 },
 {
  "input": "Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function. ",
  "translatedText": "微積分の学生は、その最小値を明示的に計算できる場合があることを知っているでしょうが、本 当に複雑な関数では常に実現可能であるとは限りません。もちろん、この状況の 13,000 入力バージョンでは、非常に複雑なニューラル ネットワークのコスト関数では不可能です。",
  "model": "nmt",
  "time_range": [
   336.42,
   351.64
  ]
 },
 {
  "input": "A more flexible tactic is to start at any input, and figure out which direction you should step to make that output lower. ",
  "translatedText": "より柔軟な戦術は、任意の入力から開始して、その出力を下げる ためにどの方向にステップを進めるべきかを判断することです。",
  "model": "nmt",
  "time_range": [
   351.64,
   359.86
  ]
 },
 {
  "input": "Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive, and shift the input to the right if that slope is negative. ",
  "translatedText": "具体的には、現在の関数の傾きがわかる場合は 、その傾きが正の場合は左にシフトし、その 傾きが負の場合は入力を右にシフトします。",
  "model": "nmt",
  "time_range": [
   359.86,
   372.72
  ]
 },
 {
  "input": "If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function. ",
  "translatedText": "これを繰り返し実行し、各ポイントで新しい傾きをチェックし、適 切な手順を実行すると、関数の極小値に近づくことになります。",
  "model": "nmt",
  "time_range": [
   372.72,
   380.68
  ]
 },
 {
  "input": "And the image you might have in mind here is a ball rolling down a hill. ",
  "translatedText": "ここで皆さんが思い浮かべるイメージは、丘を転がり落ちるボールです。",
  "model": "nmt",
  "time_range": [
   380.68,
   384.6
  ]
 },
 {
  "input": "And notice, even for this really simplified single input function, there are many possible valleys you might land in, depending on which random input you start at, and there's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function. ",
  "translatedText": "そして、この非常に単純化された単一入力関数であっても、ど のランダム入力から開始するかに応じて、到達する可能性のあ る谷が多数あり、到達する極小値が可能な限り最小の値になる という保証はないことに注意してください。コスト関数の。",
  "model": "nmt",
  "time_range": [
   384.6,
   399.46
  ]
 },
 {
  "input": "That's going to carry over to our neural network case as well. ",
  "translatedText": "これはニューラル ネットワークの場合にも当てはまります。",
  "model": "nmt",
  "time_range": [
   399.46,
   403.18
  ]
 },
 {
  "input": "And I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that kind of helps you from overshooting. ",
  "translatedText": "また、ステップ サイズを勾配に比例させると、勾配が最小に 向かって平坦になるとステップがどんどん小さくなり、オーバ ーシュートを防ぐことができることにも注目してください。",
  "model": "nmt",
  "time_range": [
   403.18,
   416.02
  ]
 },
 {
  "input": "Bumping up the complexity a bit, imagine instead a function with two inputs and one output. ",
  "translatedText": "もう少し複雑にして、代わりに 2 つの入力と 1 つの出力を持つ関数を想像してください。",
  "model": "nmt",
  "time_range": [
   416.02,
   421.64
  ]
 },
 {
  "input": "You might think of the input space as the xy-plane, and the cost function as being graphed as a surface above it. ",
  "translatedText": "入力空間を xy 平面、コスト関数をその上の 面としてグラフ化すると考えることができます。",
  "model": "nmt",
  "time_range": [
   421.64,
   429.02
  ]
 },
 {
  "input": "Instead of asking about the slope of the function, you have to ask which direction you should step in this input space so as to decrease the output of the function most quickly. ",
  "translatedText": "関数の傾きについて尋ねる代わりに、関数の出力を最も早く減少させるために は、この入力空間でどの方向にステップすべきかを尋ねる必要があります。",
  "model": "nmt",
  "time_range": [
   429.02,
   439.78
  ]
 },
 {
  "input": "In other words, what's the downhill direction? ",
  "translatedText": "言い換えれば、下り坂の方向は何ですか？",
  "model": "nmt",
  "time_range": [
   439.78,
   442.34
  ]
 },
 {
  "input": "And again, it's helpful to think of a ball rolling down that hill. ",
  "translatedText": "繰り返しますが、その丘を転がり落ちるボールを想像すると分かります。",
  "model": "nmt",
  "time_range": [
   442.34,
   446.74
  ]
 },
 {
  "input": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, which direction should you step to increase the function most quickly. ",
  "translatedText": "多変数微積分に詳しい人は、関数の勾配によって最 も急な上昇の方向がわかり、関数を最も早く増加さ せるにはどの方向に進むべきかがわかるでしょう。",
  "model": "nmt",
  "time_range": [
   446.74,
   459.42
  ]
 },
 {
  "input": "Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly. ",
  "translatedText": "当然のことながら、その勾配をマイナスにすると、関数 を最も早く減少させるステップの方向がわかります。",
  "model": "nmt",
  "time_range": [
   459.42,
   467.46
  ]
 },
 {
  "input": "Even more than that, the length of this gradient vector is an indication for just how steep that steepest slope is. ",
  "translatedText": "さらに、この勾配ベクトルの長さは、その最も 急な勾配がどれほど急であるかを示します。",
  "model": "nmt",
  "time_range": [
   467.46,
   474.58
  ]
 },
 {
  "input": "Now if you're unfamiliar with multivariable calculus and want to learn more, check out some of the work I did for Khan Academy on the topic. ",
  "translatedText": "多変数微積分に詳しくなく、さらに詳しく知りたい場合は、このテーマに 関して私がカーン アカデミーで行った作品の一部を確認してください。",
  "model": "nmt",
  "time_range": [
   474.58,
   481.1
  ]
 },
 {
  "input": "Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector, this vector that tells you what the downhill direction is and how steep it is. ",
  "translatedText": "しかし正直に言って、あなたと私にとって現時点で重要なのは、 原理的にはこのベクトル、つまり下り坂の方向とその急勾配を 示すベクトルを計算する方法が存在するということだけです。",
  "model": "nmt",
  "time_range": [
   481.1,
   492.04
  ]
 },
 {
  "input": "You'll be okay if that's all you know and you're not rock solid on the details. ",
  "translatedText": "知っていることがこれだけで、詳細がしっかりしていなくても大丈夫です。",
  "model": "nmt",
  "time_range": [
   492.04,
   497.28
  ]
 },
 {
  "input": "Because if you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill, and repeat that over and over. ",
  "translatedText": "それができれば、関数を最小化するアルゴリズムは、この勾配の方向を計算し、下り 坂に向かって小さな一歩を踏み出し、それを何度も繰り返すことになるからです。",
  "model": "nmt",
  "time_range": [
   497.28,
   507.4
  ]
 },
 {
  "input": "It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs. ",
  "translatedText": "これは、2 つの入力ではなく 13,000 の入力を持つ関数の基本的な考え方と同じです。",
  "model": "nmt",
  "time_range": [
   507.4,
   513.7
  ]
 },
 {
  "input": "Imagine organizing all 13,000 weights and biases of our network into a giant column vector. ",
  "translatedText": "ネットワークの 13,000 の重みとバイアスをすべ て巨大な列ベクトルに編成することを想像してください。",
  "model": "nmt",
  "time_range": [
   513.7,
   520.18
  ]
 },
 {
  "input": "The negative gradient of the cost function is just a vector, it's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function. ",
  "translatedText": "コスト関数の負の勾配は単なるベクトルであり、この非常に巨大な入 力空間内の何らかの方向であり、これらすべての数値に対するどの 微調整がコスト関数の最も急速な減少を引き起こすかを示します。",
  "model": "nmt",
  "time_range": [
   520.18,
   535.9
  ]
 },
 {
  "input": "And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values, and more like an actual decision we want it to make. ",
  "translatedText": "そしてもちろん、特別に設計されたコスト関数を使用して、重みとバイアスを 変更して値を下げることは、トレーニング データの各部分に対するネット ワークの出力を 10 個の値のランダムな配列のように見せることを意味 し、より実際に望む決定に近づけることを意味します。それを作るのです。",
  "model": "nmt",
  "time_range": [
   535.9,
   551.28
  ]
 },
 {
  "input": "It's important to remember, this cost function involves an average over all of the training data, so if you minimize it, it means it's a better performance on all of those samples. ",
  "translatedText": "覚えておくことが重要です。このコスト関数にはすべてのトレーニング データの平均が含まれるた め、これを最小化すると、それらのサンプルすべてでパフォーマンスが向上することになります。",
  "model": "nmt",
  "time_range": [
   551.28,
   564.26
  ]
 },
 {
  "input": "The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called backpropagation, and it's what I'm going to be talking about next video. ",
  "translatedText": "この勾配を効率的に計算するためのアルゴリズムは、事実上、ニュ ーラル ネットワークの学習方法の中心であり、バックプロパゲー ションと呼ばれます。これについては、次のビデオで説明します。",
  "model": "nmt",
  "time_range": [
   564.26,
   574.04
  ]
 },
 {
  "input": "There, I really want to take the time to walk through what exactly happens to each weight and bias for a given piece of training data, trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas. ",
  "translatedText": "そこでは、時間をかけて、特定のトレーニング データの各重みとバイアスに正 確に何が起こっているのかを詳しく見ていき、関連する計算や公式の山の向こう で何が起こっているのかを直感的に感じられるようにしたいと考えています。",
  "model": "nmt",
  "time_range": [
   574.04,
   587.98
  ]
 },
 {
  "input": "Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function. ",
  "translatedText": "今ここで、実装の詳細とは関係なく、皆さんに知っておいて いただきたい主な点は、ネットワーク学習について話すとき の意味は、コスト関数を最小化するだけだということです。",
  "model": "nmt",
  "time_range": [
   587.98,
   599.32
  ]
 },
 {
  "input": "And notice, one consequence of that is that it's important for this cost function to have a nice smooth output, so that we can find a local minimum by taking little steps downhill. ",
  "translatedText": "そして、その結果の 1 つとして、このコスト関数が良好な滑らか な出力を持つことが重要であることに注意してください。これにより 、下り坂を少しずつ進むことで極小値を見つけることができます。",
  "model": "nmt",
  "time_range": [
   599.32,
   609.34
  ]
 },
 {
  "input": "This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way that biological neurons are. ",
  "translatedText": "ちなみに、生物学的ニューロンのように単に二値的に 活性または不活性になるのではなく、人工ニューロン が継続的に範囲の活性化を行うのはこのためです。",
  "model": "nmt",
  "time_range": [
   609.34,
   620.44
  ]
 },
 {
  "input": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent. ",
  "translatedText": "負の勾配の倍数によって関数の入力を繰り返し微調 整するこのプロセスは、勾配降下法と呼ばれます。",
  "model": "nmt",
  "time_range": [
   620.44,
   626.96
  ]
 },
 {
  "input": "It's a way to converge towards some local minimum of a cost function, basically a valley in this graph. ",
  "translatedText": "これは、コスト関数の局所最小値、つまりこ のグラフの谷に向かって収束する方法です。",
  "model": "nmt",
  "time_range": [
   626.96,
   633.0
  ]
 },
 {
  "input": "I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000 dimensional input space are a little hard to wrap your mind around, but there is actually a nice non-spatial way to think about this. ",
  "translatedText": "もちろん、13,000 次元の入力空間でのナッジは少し理解するのが 難しいため、まだ 2 つの入力を持つ関数の図を示していますが、実 際には、これについて考えるための優れた非空間的な方法があります。",
  "model": "nmt",
  "time_range": [
   633.0,
   645.22
  ]
 },
 {
  "input": "Each component of the negative gradient tells us two things. ",
  "translatedText": "負の勾配の各成分から 2 つのことが分かります。",
  "model": "nmt",
  "time_range": [
   645.22,
   649.1
  ]
 },
 {
  "input": "The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down. ",
  "translatedText": "もちろん、この符号は、入力ベクトルの対応するコンポーネン トを上または下に微調整する必要があるかどうかを示します。",
  "model": "nmt",
  "time_range": [
   649.1,
   655.86
  ]
 },
 {
  "input": "But importantly, the relative magnitudes of all these components kind of tells you which changes matter more. ",
  "translatedText": "しかし重要なのは、これらすべての要素の相対的な大きさによ って、どの変更がより重要であるかがわかるということです。",
  "model": "nmt",
  "time_range": [
   655.86,
   665.62
  ]
 },
 {
  "input": "You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight. ",
  "translatedText": "私たちのネットワークでは、重みの 1 つを調整すると、他の重みを調整 するよりもコスト関数にはるかに大きな影響を与える可能性があります。",
  "model": "nmt",
  "time_range": [
   665.62,
   674.98
  ]
 },
 {
  "input": "Some of these connections just matter more for our training data. ",
  "translatedText": "これらの接続の中には、トレーニング データにとってより重要なものもあります。",
  "model": "nmt",
  "time_range": [
   674.98,
   679.44
  ]
 },
 {
  "input": "So a way you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck. ",
  "translatedText": "したがって、気が遠くなるような膨大なコスト関数のこの勾配ベクトルについ て考える方法は、各重みとバイアスの相対的な重要性、つまり、これらの変 更のどれが最も費用対効果が高いかをエンコードしているということです。",
  "model": "nmt",
  "time_range": [
   679.44,
   694.1
  ]
 },
 {
  "input": "This really is just another way of thinking about direction. ",
  "translatedText": "これは方向性についての単なる考え方です。",
  "model": "nmt",
  "time_range": [
   694.1,
   697.36
  ]
 },
 {
  "input": "To take a simpler example, if you have some function with two variables as an input, and compute that its gradient at some particular point comes out as 3,1, then on the one hand you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly, that when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction. ",
  "translatedText": "より単純な例を挙げると、入力として 2 つの変数を持つ関 数があり、ある特定の点での勾配が 3,1 になると計算 した場合、一方では、次のように解釈できます。その入力に立 って、この方向に沿って移動すると、関数が最も早く増加し ます。つまり、入力点の平面上で関数をグラフにすると、その ベクトルが真っ直ぐ上り坂の方向を与えることになります。",
  "model": "nmt",
  "time_range": [
   697.36,
   723.2
  ]
 },
 {
  "input": "But another way to read that is to say that changes to this first variable have three times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck. ",
  "translatedText": "しかし、これを別の読み方で読むと、この最初の変数への変更は 2 番目の変数 への変更の 3 倍の重要性があり、少なくとも関連する入力の付近では、X 値 を微調整する方がはるかに大きな影響を与えるということになります。バック。",
  "model": "nmt",
  "time_range": [
   723.2,
   737.74
  ]
 },
 {
  "input": "Alright, let's zoom out and sum up where we are so far. ",
  "translatedText": "さて、ズームアウトしてこれまでの状況をまとめてみましょう。",
  "model": "nmt",
  "time_range": [
   737.74,
   742.88
  ]
 },
 {
  "input": "The network itself is this function with 784 inputs and 10 outputs, defined in terms of all of these weighted sums. ",
  "translatedText": "ネットワーク自体は、784 個の入力と 10 個の出力を備えた関 数であり、これらすべての重み付けされた合計によって定義されます。",
  "model": "nmt",
  "time_range": [
   742.88,
   750.86
  ]
 },
 {
  "input": "The cost function is a layer of complexity on top of that. ",
  "translatedText": "コスト関数は、その上に複雑な層が重なっています。",
  "model": "nmt",
  "time_range": [
   750.86,
   754.16
  ]
 },
 {
  "input": "It takes the 13,000 weights and biases as inputs, and spits out a single measure of lousiness based on the training examples. ",
  "translatedText": "13,000 の重みとバイアスを入力として受け取り、ト レーニング例に基づいて粗さの単一の尺度を吐き出します。",
  "model": "nmt",
  "time_range": [
   754.16,
   762.64
  ]
 },
 {
  "input": "The gradient of the cost function is one more layer of complexity still. ",
  "translatedText": "コスト関数の勾配はさらに複雑な層になります。",
  "model": "nmt",
  "time_range": [
   762.64,
   767.52
  ]
 },
 {
  "input": "It tells us what nudges to all of these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most. ",
  "translatedText": "これは、これらすべての重みとバイアスに対してどのような調整がコ スト関数の値に最も速い変化を引き起こすかを示します。これは、ど の重みに対するどの変更が最も重要かを示していると解釈できます。",
  "model": "nmt",
  "time_range": [
   767.52,
   783.04
  ]
 },
 {
  "input": "So when you initialize the network with random weights and biases, and adjust them many times based on this gradient descent process, how well does it actually perform on images it's never seen before? ",
  "translatedText": "では、ランダムな重みとバイアスを使用してネットワークを初期化し、この 勾配降下プロセスに基づいてそれらを何度も調整すると、これまでに見たこ とのない画像で実際にどの程度のパフォーマンスが得られるでしょうか? ",
  "model": "nmt",
  "time_range": [
   783.04,
   794.24
  ]
 },
 {
  "input": "The one I've described here, with the two hidden layers of 16 neurons each, chosen mostly for aesthetic reasons, is not bad, classifying about 96% of the new images it sees correctly. ",
  "translatedText": "私がここで説明したものは、それぞれ 16 個のニューロンからなる 2 つの隠れ層を備えており、主に美 的理由から選択されていますが、悪くはなく、表示される新しい画像の約 96% を正しく分類しています。",
  "model": "nmt",
  "time_range": [
   794.24,
   806.92
  ]
 },
 {
  "input": "And honestly, if you look at some of the examples it messes up on, you feel compelled to cut it a little slack. ",
  "translatedText": "そして正直に言うと、それが台無しにしているいくつか の例を見ると、少し緩めなければならないと感じます。",
  "model": "nmt",
  "time_range": [
   806.92,
   816.3
  ]
 },
 {
  "input": "If you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%. ",
  "translatedText": "隠れ層構造を試していくつかの調整を加えれば 、これを最大 98% まで達成できます。",
  "model": "nmt",
  "time_range": [
   816.3,
   821.22
  ]
 },
 {
  "input": "And that's pretty good! ",
  "translatedText": "それはとても良いことです！",
  "model": "nmt",
  "time_range": [
   821.22,
   822.9
  ]
 },
 {
  "input": "It's not the best, you can certainly get better performance by getting more sophisticated than this plain vanilla network, but given how daunting the initial task is, I think there's something incredible about any network doing this well on images it's never seen before given that we never specifically told it what patterns to look for. ",
  "translatedText": "これは最高ではありません。この単純なバニラ ネットワークよりも洗練されれば、確かにパフォ ーマンスを向上させることはできますが、最初のタスクがどれほど困難であるかを考えると、こ れまで見たことのない画像でこれほどうまく動作するネットワークには、信じられないほどの何 かがあると思います。どのようなパターンを探すべきかを具体的に指示したことはありません。",
  "model": "nmt",
  "time_range": [
   822.9,
   842.0
  ]
 },
 {
  "input": "Originally, the way I motivated this structure was by describing a hope we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops and longer lines, and that those might be pieced together to recognize digits. ",
  "translatedText": "もともと、私がこの構造を動機付けた方法は、2 番目の層が小さ なエッジを検出し、3 番目の層がそれらのエッジをつなぎ合わせ てループや長い線を認識し、それらがつなぎ合わされるかもしれな いという希望を説明することでした。一緒に数字を認識します。",
  "model": "nmt",
  "time_range": [
   842.0,
   858.22
  ]
 },
 {
  "input": "So is this what our network is actually doing? ",
  "translatedText": "では、これは私たちのネットワークが実際に行っていることなのでしょうか? ",
  "model": "nmt",
  "time_range": [
   858.22,
   861.04
  ]
 },
 {
  "input": "Well, for this one at least, not at all. ",
  "translatedText": "まあ、少なくともこれに関しては、まったくそうではありません。",
  "model": "nmt",
  "time_range": [
   861.04,
   864.88
  ]
 },
 {
  "input": "Remember how last video we looked at how the weights of the connections from all the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that the second layer neuron is picking up on? ",
  "translatedText": "前回のビデオで、最初の層のすべてのニューロンから 2 番目の層の特定のニュ ーロンへの接続の重みが、2 番目の層のニューロンが認識している特定のピクセ ル パターンとしてどのように視覚化できるかを説明したことを覚えていますか? ",
  "model": "nmt",
  "time_range": [
   864.88,
   877.44
  ]
 },
 {
  "input": "Well, when we do that for the weights associated with these transitions, instead of picking up on isolated little edges here and there, they look, well, almost random, just with some very loose patterns in the middle. ",
  "translatedText": "これらのトランジションに関連付けられたウェイトに対してこれを行う と、あちこちで孤立した小さなエッジが検出されるのではなく、ほぼラ ンダムに見え、中央に非常に緩いパターンがいくつかあるだけです。",
  "model": "nmt",
  "time_range": [
   877.44,
   894.2
  ]
 },
 {
  "input": "It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns we might have hoped for. ",
  "translatedText": "考えられる重みとバイアスの計り知れないほど広い 13,000 次元空間において、私たちのネットワークは、ほとんどの画像を正 常に分類したにもかかわらず、私たちが期待していたパターンを正 確に検出できない、幸せな小さな局所最小値を見つけたようです。",
  "model": "nmt",
  "time_range": [
   894.2,
   909.84
  ]
 },
 {
  "input": "And to really drive this point home, watch what happens when you input a random image. ",
  "translatedText": "この点をよく理解するには、ランダムな画像を入力したときに何が起こるかを見てください。",
  "model": "nmt",
  "time_range": [
   909.84,
   914.6
  ]
 },
 {
  "input": "If the system was smart, you might expect it to either feel uncertain, maybe not really activating any of those 10 output neurons or activating them all evenly, but instead it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5 as it does that an actual image of a 5 is a 5. ",
  "translatedText": "システムが賢いものであれば、10 個の出力ニューロンのどれも実際には活性化していないのか 、あるいはそれらすべてを均等に活性化しているのか、不確かに感じられるか、あるいは、この ランダムなニューロンが確実であるかのように、自信を持ってナンセンスな答えを与えることを期 待するかもしれません。5 の実際の画像が 5 であるのと同様に、ノイズも 5 です。",
  "model": "nmt",
  "time_range": [
   914.6,
   934.56
  ]
 },
 {
  "input": "Phrased differently, even if this network can recognize digits pretty well, it has no idea how to draw them. ",
  "translatedText": "別の言い方をすると、このネットワークは数字をかなりう まく認識できても、それを描画する方法がわかりません。",
  "model": "nmt",
  "time_range": [
   934.56,
   941.8
  ]
 },
 {
  "input": "A lot of this is because it's such a tightly constrained training setup. ",
  "translatedText": "その多くは、トレーニングの設定が非常に厳しく制限されているためです。",
  "model": "nmt",
  "time_range": [
   941.8,
   945.4
  ]
 },
 {
  "input": "I mean, put yourself in the network's shoes here. ",
  "translatedText": "つまり、ここではネットワークの立場に立って考えてみましょう。",
  "model": "nmt",
  "time_range": [
   945.4,
   948.22
  ]
 },
 {
  "input": "From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid, and its cost function never gave it any incentive to be anything but utterly confident in its decisions. ",
  "translatedText": "その観点から見ると、宇宙全体は小さなグリッドの中心にある、明確に 定義された不動の数字だけで構成されており、そのコスト関数は、その 決定に完全な自信を持つこと以外には何の動機も与えませんでした。",
  "model": "nmt",
  "time_range": [
   948.22,
   962.16
  ]
 },
 {
  "input": "So with this as the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns. ",
  "translatedText": "これが第 2 層のニューロンが実際に行っていることのイメー ジであるため、なぜエッジやパターンを検出するという動機でこ のネットワークを紹介するのか不思議に思うかもしれません。",
  "model": "nmt",
  "time_range": [
   962.16,
   970.32
  ]
 },
 {
  "input": "I mean, that's just not at all what it ends up doing. ",
  "translatedText": "つまり、それは最終的にはまったくそうではありません。",
  "model": "nmt",
  "time_range": [
   970.32,
   973.04
  ]
 },
 {
  "input": "Well, this is not meant to be our end goal, but instead a starting point. ",
  "translatedText": "これは最終目標ではなく、出発点です。",
  "model": "nmt",
  "time_range": [
   973.04,
   977.48
  ]
 },
 {
  "input": "Frankly, this is old technology, the kind researched in the 80s and 90s, and you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems, but the more you dig into what those hidden layers are really doing, the less intelligent it seems. ",
  "translatedText": "率直に言って、これは 80 年代から 90 年代に研究された種類の古いテクノロジ ーであり、より詳細な現代の亜種を理解する前に、それを理解する必要があります。ま た、明らかにいくつかの興味深い問題を解決できる可能性がありますが、何を深く掘り下 げるほど、これらの隠れ層が実際に機能しているほど、その層は知性が低く見えます。",
  "model": "nmt",
  "time_range": [
   977.48,
   998.72
  ]
 },
 {
  "input": "Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow. ",
  "translatedText": "ネットワークがどのように学習するかということから、あなたがどのように学習するかに少し焦点を移し ますが、それは、何らかの方法でここで取り上げた資料に積極的に取り組んだ場合にのみ起こります。",
  "model": "nmt",
  "time_range": [
   998.72,
   1007.16
  ]
 },
 {
  "input": "One pretty simple thing I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns. ",
  "translatedText": "皆さんにしていただきたいのは、非常に簡単なことの 1 つです。今ちょっと立ち止まって 、このシステムにどのような変更を加えるか、エッジやパターンなどをよりよく認識できるよ うにしたい場合に画像をどのように認識するかについて、少しの間深く考えてみてください。",
  "model": "nmt",
  "time_range": [
   1007.16,
   1021.88
  ]
 },
 {
  "input": "But better than that, to actually engage with the material, I highly recommend the book by Michael Nielsen on deep learning and neural networks. ",
  "translatedText": "しかしそれよりも、実際に内容に取り組むには、深層学習とニューラル ネット ワークに関する Michael Nielsen の本を強くお勧めします。",
  "model": "nmt",
  "time_range": [
   1021.88,
   1029.72
  ]
 },
 {
  "input": "In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing. ",
  "translatedText": "この中には、まさにこの例をダウンロードして試すためのコードとデータ が含まれており、そのコードが何をしているのかを段階的に説明します。",
  "model": "nmt",
  "time_range": [
   1029.72,
   1039.36
  ]
 },
 {
  "input": "What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Nielsen's efforts. ",
  "translatedText": "素晴らしいのは、この本が無料で一般公開されているということです。この本から何かを得る ことができましたら、私と一緒にニールセンの取り組みに寄付することを検討してください。",
  "model": "nmt",
  "time_range": [
   1039.36,
   1048.04
  ]
 },
 {
  "input": "I've also linked a couple other resources I like a lot in the description, including the phenomenal and beautiful blog post by Chris Ola and the articles in Distill. ",
  "translatedText": "また、Chris Ola による驚異的で美しいブログ投稿や Distill の記事など、私がとても気に入っている他のリソースも説明にリンクしました。",
  "model": "nmt",
  "time_range": [
   1048.04,
   1058.72
  ]
 },
 {
  "input": "To close things off here for the last few minutes, I want to jump back into a snippet of the interview I had with Leisha Lee. ",
  "translatedText": "最後の数分間をここで締めくくるために、リーシャ・ リーとのインタビューの抜粋に戻りたいと思います。",
  "model": "nmt",
  "time_range": [
   1058.72,
   1064.44
  ]
 },
 {
  "input": "You might remember her from the last video, she did her PhD work in deep learning. ",
  "translatedText": "前回のビデオで彼女を覚えているかもしれません。彼女は深層学習で博士号の研究をしていました。",
  "model": "nmt",
  "time_range": [
   1064.44,
   1068.52
  ]
 },
 {
  "input": "In this little snippet, she talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning. ",
  "translatedText": "この短い抜粋では、最新の画像認識ネットワークの一部が実際にどのように学習し ているかを深く掘り下げた 2 つの最近の論文について彼女が話しています。",
  "model": "nmt",
  "time_range": [
   1068.52,
   1076.38
  ]
 },
 {
  "input": "Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled dataset, it shuffled all of the labels around before training. ",
  "translatedText": "会話の状況を説明するために、最初の論文では、画像認識に優れた特にディープ ニュ ーラル ネットワークの 1 つを使用し、適切にラベル付けされたデータセットでト レーニングする代わりに、トレーニング前にすべてのラベルをシャッフルしました。",
  "model": "nmt",
  "time_range": [
   1076.38,
   1089.4
  ]
 },
 {
  "input": "Obviously the testing accuracy here was going to be no better than random, since everything's just randomly labeled. ",
  "translatedText": "すべてがランダムにラベル付けされているだけであるため 、ここでのテストの精度は明らかにランダムと同等です。",
  "model": "nmt",
  "time_range": [
   1089.4,
   1095.32
  ]
 },
 {
  "input": "But it was still able to achieve the same training accuracy as you would on a properly labeled dataset. ",
  "translatedText": "ただし、適切にラベル付けされたデータセットを使用した場 合と同じトレーニング精度を達成することはできました。",
  "model": "nmt",
  "time_range": [
   1095.32,
   1101.44
  ]
 },
 {
  "input": "Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization? ",
  "translatedText": "基本的に、この特定のネットワークの何百万もの重みは、ランダムなデータを記憶 するだけで十分でした。このため、このコスト関数の最小化が実際に画像内の何 らかの構造に対応するのか、それとも単なる記憶なのかという疑問が生じます。。",
  "model": "nmt",
  "time_range": [
   1101.44,
   1116.72
  ]
 },
 {
  "input": "...to memorize the entire dataset of what the correct classification is. ",
  "translatedText": "。。正しい分類が何であるかをデータセット全体を記憶するためです。",
  "model": "nmt",
  "time_range": [
   1116.72,
   1120.12
  ]
 },
 {
  "input": "And so a couple of, you know, half a year later at ICML this year, there was not exactly rebuttal paper, but paper that addressed some aspects of like, hey, actually these networks are doing something a little bit smarter than that. ",
  "translatedText": "それで、半年後の今年の ICML では、正確には反論の論文はありませ んでしたが、実際には、これらのネットワークはそれよりもう少し賢いこ とをしている、といったいくつかの側面を取り上げた論文がありました。",
  "model": "nmt",
  "time_range": [
   1120.12,
   1132.22
  ]
 },
 {
  "input": "If you look at that accuracy curve, if you were just training on a random dataset, that curve sort of went down very, you know, very slowly in almost kind of a linear fashion. ",
  "translatedText": "その精度曲線を見ると、ランダムなデータセットでトレーニングしているだけだ とすると、その曲線は、ほぼ直線的に、非常にゆっくりと下降していきます。",
  "model": "nmt",
  "time_range": [
   1132.22,
   1145.24
  ]
 },
 {
  "input": "So you're really struggling to find that local minima of possible, you know, the right weights that would get you that accuracy. ",
  "translatedText": "したがって、その精度を実現する適切な重みの可能 な極小値を見つけるのに非常に苦労しています。",
  "model": "nmt",
  "time_range": [
   1145.24,
   1152.32
  ]
 },
 {
  "input": "Whereas if you're actually training on a structured dataset, one that has the right labels, you know, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level. ",
  "translatedText": "一方、適切なラベルを持つ構造化データセットで実際にトレー ニングしている場合、最初は少しいじってみましたが、その 精度レベルに達するまでに非常に早く落ちてしまいました。",
  "model": "nmt",
  "time_range": [
   1152.32,
   1163.36
  ]
 },
 {
  "input": "And so in some sense it was easier to find that local maxima. ",
  "translatedText": "したがって、ある意味、極大値を見つけるのが簡単でした。",
  "model": "nmt",
  "time_range": [
   1163.36,
   1168.58
  ]
 },
 {
  "input": "And so what was also interesting about that is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers. ",
  "translatedText": "そして、これに関して興味深いのは、実際に数年前に発行さ れた別の論文が明らかになったことであり、この論文では ネットワーク層についてさらに単純化が行われています。",
  "model": "nmt",
  "time_range": [
   1168.58,
   1180.14
  ]
 },
 {
  "input": "But one of the results was saying how, if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality. ",
  "translatedText": "しかし、その結果の 1 つは、最適化の状況を見ると、これらのネットワーク が学習する傾向にある極小値が実際には同じ品質であることを示しています。",
  "model": "nmt",
  "time_range": [
   1180.14,
   1189.4
  ]
 },
 {
  "input": "So in some sense, if your data set is structured, you should be able to find that much more easily. ",
  "translatedText": "したがって、ある意味、データセットが構造化されていれば、それをより簡単に見つけることができるはずです。",
  "model": "nmt",
  "time_range": [
   1189.4,
   1194.3
  ]
 },
 {
  "input": "My thanks as always to those of you supporting on Patreon. ",
  "translatedText": "Patreon でサポートしてくださっている皆様にいつも感謝しています。",
  "model": "nmt",
  "time_range": [
   1194.3,
   1201.14
  ]
 },
 {
  "input": "I've said before just what a game changer on Patreon is, but these videos really would not be possible without you. ",
  "translatedText": "Patreon におけるゲームチェンジャーが何であるかについては以 前に述べましたが、これらのビデオは本当に皆さんなしでは不可能です。",
  "model": "nmt",
  "time_range": [
   1201.14,
   1207.16
  ]
 },
 {
  "input": "I also want to give a special thanks to the VC firm Amplify Partners and their support of these initial videos in the series. ",
  "translatedText": "また、VC 会社 Amplify Partners と、シリーズの最 初のビデオに対する彼らのサポートにも特別な感謝を表したいと思います。",
  "model": "nmt",
  "time_range": [
   1207.16,
   1213.24
  ]
 },
 {
  "input": "Thank you. ",
  "translatedText": "ありがとう。",
  "model": "nmt",
  "time_range": [
   1213.24,
   1233.14
  ]
 }
]