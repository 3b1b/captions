1
00:00:04,180 --> 00:00:05,730
前回のビデオでは、ニューラル ネ

2
00:00:05,730 --> 00:00:07,280
ットワークの構造を説明しました。

3
00:00:07,680 --> 00:00:09,934
記憶に新しいように、ここで簡単に要約します。

4
00:00:09,934 --> 00:00:12,600
その後、このビデオには 2 つの主な目標があります。

5
00:00:13,100 --> 00:00:15,468
1 つ目は、勾配降下の考え方を導入することです。

6
00:00:15,468 --> 00:00:17,935
これは、ニューラル ネットワークの学習方法だけでな

7
00:00:17,935 --> 00:00:20,600
く、他の多くの機械学習の仕組みの基礎にもなっています。

8
00:00:21,120 --> 00:00:23,393
その後、この特定のネットワークがどのように動作す

9
00:00:23,393 --> 00:00:25,666
るか、そしてニューロンの隠れた層が最終的に何を探

10
00:00:25,666 --> 00:00:27,940
すのかについてもう少し詳しく掘り下げていきます。

11
00:00:28,979 --> 00:00:31,393
念のため言っておきますが、ここでの目標は手書

12
00:00:31,393 --> 00:00:33,696
き数字認識の古典的な例、つまりニューラル 

13
00:00:33,696 --> 00:00:36,220
ネットワークの Hello World です。

14
00:00:37,020 --> 00:00:38,890
これらの数字は 28x28 ピクセル 

15
00:00:38,890 --> 00:00:41,056
グリッド上にレンダリングされ、各ピクセルは 

16
00:00:41,056 --> 00:00:43,420
0 から 1 までのグレースケール値を持ちます。

17
00:00:43,820 --> 00:00:46,930
これらは、ネットワークの入力層の 784 

18
00:00:46,930 --> 00:00:50,040
個のニューロンの活性化を決定するものです。

19
00:00:51,180 --> 00:00:54,298
そして、後続の層の各ニューロンの活性化は、前

20
00:00:54,298 --> 00:00:57,417
の層のすべての活性化の加重合計に、バイアスと

21
00:00:57,417 --> 00:01:00,820
呼ばれる特別な数値を加えたものに基づいています。

22
00:01:02,160 --> 00:01:05,609
次に、前回のビデオで説明した方法である、シグモイド圧縮や 

23
00:01:05,609 --> 00:01:08,940
Relu などの他の関数を使用してその合計を作成します。

24
00:01:09,480 --> 00:01:12,948
合計すると、それぞれ 16 個のニューロンを持つ 2 

25
00:01:12,948 --> 00:01:16,673
つの隠れ層というやや恣意的な選択を考慮すると、ネットワーク

26
00:01:16,673 --> 00:01:19,755
には調整できる重みとバイアスが約 13,000 

27
00:01:19,755 --> 00:01:23,480
あり、ネットワークが実際に何を行うかを正確に決定するのはこ

28
00:01:23,480 --> 00:01:24,380
れらの値です。

29
00:01:24,880 --> 00:01:27,686
このネットワークが特定の数字を分類すると言うとき

30
00:01:27,686 --> 00:01:30,493
の意味は、最終層の 10 個のニューロンの中で最

31
00:01:30,493 --> 00:01:33,300
も明るいものがその数字に対応するということです。

32
00:01:34,100 --> 00:01:37,492
ここでレイヤー構造について念頭に置いていた動機は、おそらく 

33
00:01:37,492 --> 00:01:39,753
2 番目のレイヤーでエッジを認識し、3 

34
00:01:39,753 --> 00:01:42,693
番目のレイヤーでループやラインなどのパターンを認識し

35
00:01:42,693 --> 00:01:45,633
、最後のレイヤーでそれらをつなぎ合わせることができる

36
00:01:45,633 --> 00:01:48,800
ということを思い出してください。数字を認識するパターン。

37
00:01:49,800 --> 00:01:52,240
ここでは、ネットワークがどのように学習するかを学びます。

38
00:01:52,640 --> 00:01:55,459
私たちが望んでいるのは、このネットワークに大量のト

39
00:01:55,459 --> 00:01:58,165
レーニング データを表示できるアルゴリズムです。

40
00:01:58,165 --> 00:02:00,985
このデータは、手書きの数字のさまざまな画像と、それ

41
00:02:00,985 --> 00:02:03,917
らが本来あるべきものを示すラベルの形で提供されます。

42
00:02:03,917 --> 00:02:06,736
トレーニング データのパフォーマンスを向上させるた

43
00:02:06,736 --> 00:02:10,120
めに、これらの 13,000 の重みとバイアスを調整します。

44
00:02:10,720 --> 00:02:14,017
うまくいけば、この階層構造は、学習した内容がトレーニング 

45
00:02:14,017 --> 00:02:16,860
データを超えた画像に一般化されることを意味します。

46
00:02:17,640 --> 00:02:19,905
これをテストする方法は、ネットワークをトレーニ

47
00:02:19,905 --> 00:02:22,170
ングした後、これまでに見たことのないラベル付き

48
00:02:22,170 --> 00:02:24,435
データをさらに表示し、それらの新しい画像がどの

49
00:02:24,435 --> 00:02:26,700
程度正確に分類されているかを確認することです。

50
00:02:31,120 --> 00:02:33,693
私たちにとって幸運なことに、そしてそもそもこれが

51
00:02:33,693 --> 00:02:35,944
非常に一般的な例である理由は、MNIST 

52
00:02:35,944 --> 00:02:38,517
データベースの背後にいる善良な人々が何万もの手書

53
00:02:38,517 --> 00:02:41,090
きの数字画像のコレクションをまとめ、それぞれの画

54
00:02:41,090 --> 00:02:44,200
像に本来あるべき数字がラベル付けされていることです。なれ。

55
00:02:44,900 --> 00:02:48,385
機械を学習すると表現するのは挑発的ですが、それがどのよう

56
00:02:48,385 --> 00:02:51,621
に機能するかを一度理解すると、それはおかしな SF 

57
00:02:51,621 --> 00:02:55,106
の前提というよりも、はるかに微積分の練習のように感じられ

58
00:02:55,106 --> 00:02:55,480
ます。

59
00:02:56,200 --> 00:02:58,079
つまり、基本的には、特定の機能の

60
00:02:58,079 --> 00:02:59,960
最小値を見つけることになります。

61
00:03:01,939 --> 00:03:05,246
概念的には、各ニューロンは前の層のすべてのニューロンに

62
00:03:05,246 --> 00:03:08,552
接続されていると考えており、その活性化を定義する加重和

63
00:03:08,552 --> 00:03:11,858
の重みはそれらの接続の強さのようなものであり、バイアス

64
00:03:11,858 --> 00:03:14,674
は何らかの指標であることを思い出してください。

65
00:03:14,674 --> 00:03:17,980
そのニューロンが活動的になる傾向があるか、不活動的であ

66
00:03:17,980 --> 00:03:18,960
る傾向があるか。

67
00:03:19,720 --> 00:03:21,989
まず、これらの重みとバイアスをす

68
00:03:21,989 --> 00:03:24,400
べて完全にランダムに初期化します。

69
00:03:24,940 --> 00:03:26,866
言うまでもなく、このネットワークはランダムに何かを

70
00:03:26,866 --> 00:03:28,793
実行しているだけなので、特定のトレーニング例ではか

71
00:03:28,793 --> 00:03:30,720
なりひどいパフォーマンスを発揮することになります。

72
00:03:31,040 --> 00:03:33,466
たとえば、この 3 の画像を入力すると

73
00:03:33,466 --> 00:03:36,020
、出力レイヤーは混乱したように見えます。

74
00:03:36,600 --> 00:03:39,808
それで、あなたがすることは、コスト関数を定義することです。

75
00:03:39,808 --> 00:03:42,573
これは、コンピュータ、いや、悪いコンピュータに、出

76
00:03:42,573 --> 00:03:44,896
力の活性化がほとんどのニューロンでは 0 

77
00:03:44,896 --> 00:03:46,666
ですが、このニューロンでは 1 

78
00:03:46,666 --> 00:03:49,432
になるはずです、あなたが私にくれたものは全くのゴミ

79
00:03:49,432 --> 00:03:50,760
です、と伝える方法です。

80
00:03:51,720 --> 00:03:55,045
これをもう少し数学的に言うと、これらのゴミ出力

81
00:03:55,045 --> 00:03:58,370
アクティベーションのそれぞれと、それらに必要な

82
00:03:58,370 --> 00:04:02,417
値との差の二乗を合計します。これが、単一のトレーニング 

83
00:04:02,417 --> 00:04:05,020
サンプルのコストと呼ばれるものです。

84
00:04:05,960 --> 00:04:08,543
ネットワークが自信を持って画像を正しく分類してい

85
00:04:08,543 --> 00:04:11,126
る場合にはこの合計は小さくなりますが、ネットワー

86
00:04:11,126 --> 00:04:13,709
クが何をしているかを認識していないように見える場

87
00:04:13,709 --> 00:04:16,399
合にはこの合計が大きくなることに注意してください。

88
00:04:18,640 --> 00:04:21,756
したがって、自由に使える数万のトレーニング 

89
00:04:21,756 --> 00:04:25,440
サンプルすべての平均コストを考慮することになります。

90
00:04:27,040 --> 00:04:29,840
この平均コストは、ネットワークがどの程度劣悪であるか、お

91
00:04:29,840 --> 00:04:32,740
よびコンピュータの動作がどの程度悪くなるかを示す尺度です。

92
00:04:33,420 --> 00:04:34,600
それは複雑なことです。

93
00:04:35,040 --> 00:04:37,138
ネットワーク自体が基本的に 784 

94
00:04:37,138 --> 00:04:39,937
個の数値、ピクセル値を入力として取り込み、10 

95
00:04:39,937 --> 00:04:43,319
個の数値を出力として吐き出す関数であったことを覚えています

96
00:04:43,319 --> 00:04:46,701
か? ある意味、ネットワークはこれらすべての重みとバイアス

97
00:04:46,701 --> 00:04:48,800
によってパラメーター化されています。

98
00:04:49,500 --> 00:04:52,820
そうですね、コスト関数はその上に複雑な層が重なっています。

99
00:04:53,100 --> 00:04:56,950
それらの 13,000 ほどの重みとバイアスを入力として受

100
00:04:56,950 --> 00:05:00,800
け取り、それらの重みとバイアスがどれほど悪いかを説明する 

101
00:05:00,800 --> 00:05:04,651
1 つの数値を吐き出します。その定義方法は、数万のトレーニ

102
00:05:04,651 --> 00:05:08,501
ング データすべてに対するネットワークの動作によって異なり

103
00:05:08,501 --> 00:05:08,900
ます。

104
00:05:09,520 --> 00:05:11,000
それは考えるべきことがたくさんあります。

105
00:05:12,400 --> 00:05:14,110
しかし、コンピューターがどのようなくだらない仕事をしているか

106
00:05:14,110 --> 00:05:15,820
を単にコンピューターに伝えるだけでは、あまり役に立ちません。

107
00:05:16,220 --> 00:05:18,085
これらの重みとバイアスを変更して改

108
00:05:18,085 --> 00:05:20,060
善する方法を教えたいと考えています。

109
00:05:20,780 --> 00:05:22,874
わかりやすくするために、13,000 

110
00:05:22,874 --> 00:05:26,070
個の入力を持つ関数を想像するのに苦労するのではなく、入力と

111
00:05:26,070 --> 00:05:28,055
して 1 つの数値、出力として 1 

112
00:05:28,055 --> 00:05:30,480
つの数値を持つ単純な関数を想像してください。

113
00:05:31,480 --> 00:05:35,300
この関数の値を最小にする入力をどのように見つけますか?

114
00:05:36,460 --> 00:05:39,361
微積分の学生は、その最小値を明示的に計算できる場合が

115
00:05:39,361 --> 00:05:42,263
あることを知っているでしょうが、本当に複雑な関数では

116
00:05:42,263 --> 00:05:44,383
常に実現可能であるわけではありません。

117
00:05:44,383 --> 00:05:46,392
もちろん、この状況の 13,000 

118
00:05:46,392 --> 00:05:48,847
入力バージョンでは、非常に複雑なニューラル 

119
00:05:48,847 --> 00:05:51,080
ネットワークのコスト関数では不可能です。

120
00:05:51,580 --> 00:05:55,390
より柔軟な戦術は、任意の入力から開始して、その出力を下げる

121
00:05:55,390 --> 00:05:59,200
ためにどの方向にステップを進めるべきかを判断することです。

122
00:06:00,080 --> 00:06:03,299
具体的には、現在の関数の傾きがわかる場合

123
00:06:03,299 --> 00:06:06,519
は、その傾きが正の場合は左にシフトし、そ

124
00:06:06,519 --> 00:06:09,900
の傾きが負の場合は入力を右にシフトします。

125
00:06:11,960 --> 00:06:15,833
これを繰り返し実行し、各ポイントで新しい傾きをチェックし、

126
00:06:15,833 --> 00:06:19,840
適切な手順を実行すると、関数の極小値に近づくことになります。

127
00:06:20,640 --> 00:06:23,800
ここで皆さんが思い浮かべるのは、丘を転がり落ちるボールです。

128
00:06:24,620 --> 00:06:28,211
この非常に単純化された単一入力関数であっても、どのラ

129
00:06:28,211 --> 00:06:31,802
ンダム入力から開始するかに応じて、到達する可能性のあ

130
00:06:31,802 --> 00:06:35,394
る谷が多数あり、到達する極小値が可能な限り最小の値に

131
00:06:35,394 --> 00:06:39,400
なるという保証はないことに注意してください。コスト関数の。

132
00:06:40,220 --> 00:06:42,620
これはニューラル ネットワークのケースにも引き継がれます。

133
00:06:43,180 --> 00:06:46,986
また、ステップ サイズを勾配に比例させた場合、勾配が最

134
00:06:46,986 --> 00:06:50,793
小に向かって平坦になるとステップがどんどん小さくなり、

135
00:06:50,793 --> 00:06:54,600
オーバーシュートが防止されることにも注目してください。

136
00:06:55,940 --> 00:06:58,574
もう少し複雑にして、代わりに 2 つの入力と 

137
00:06:58,574 --> 00:07:00,980
1 つの出力を持つ関数を想像してください。

138
00:07:01,500 --> 00:07:04,820
入力空間を xy 平面、コスト関数をその上の

139
00:07:04,820 --> 00:07:08,140
面としてグラフ化すると考えることができます。

140
00:07:08,760 --> 00:07:12,160
関数の傾きについて尋ねる代わりに、関数の出力を

141
00:07:12,160 --> 00:07:15,560
最も早く減少させるためには、この入力空間をどの

142
00:07:15,560 --> 00:07:18,960
方向にステップすべきかを尋ねる必要があります。

143
00:07:19,720 --> 00:07:21,760
言い換えれば、下り坂の方向は何ですか？

144
00:07:22,380 --> 00:07:23,933
繰り返しになりますが、ボールが丘を転がり落

145
00:07:23,933 --> 00:07:25,560
ちていくことを考えるとわかりやすいでしょう。

146
00:07:26,660 --> 00:07:30,700
多変数微積分に詳しい人は、関数の勾配によって最

147
00:07:30,700 --> 00:07:34,740
も急な上昇の方向がわかり、関数を最も早く増加さ

148
00:07:34,740 --> 00:07:38,780
せるにはどの方向に進むべきかがわかるでしょう。

149
00:07:39,560 --> 00:07:42,733
当然のことながら、その勾配をマイナスにすると、関

150
00:07:42,733 --> 00:07:46,040
数を最も早く減少させるステップの方向がわかります。

151
00:07:47,240 --> 00:07:50,459
さらに、この勾配ベクトルの長さは、その最

152
00:07:50,459 --> 00:07:53,840
も急な勾配がどれほど急であるかを示します。

153
00:07:54,540 --> 00:07:56,473
多変数微積分に詳しくなく、さらに詳しく知りた

154
00:07:56,473 --> 00:07:58,230
い場合は、このテーマに関して私がカーン 

155
00:07:58,230 --> 00:08:00,340
アカデミーで行った作品の一部を確認してください。

156
00:08:00,860 --> 00:08:04,496
しかし正直に言って、あなたと私にとって現時点で重要なのは

157
00:08:04,496 --> 00:08:08,133
、原理的にはこのベクトル、つまり下り坂の方向とその急勾配

158
00:08:08,133 --> 00:08:11,900
を示すベクトルを計算する方法が存在するということだけです。

159
00:08:12,400 --> 00:08:14,260
知っていることがこれだけで、詳細が

160
00:08:14,260 --> 00:08:16,120
しっかりしていなくても大丈夫です。

161
00:08:17,200 --> 00:08:20,294
それが得られれば、関数を最小化するアルゴリズムは

162
00:08:20,294 --> 00:08:23,388
、この勾配の方向を計算し、下り坂に向かって小さな

163
00:08:23,388 --> 00:08:26,740
一歩を踏み出し、それを何度も繰り返すことになります。

164
00:08:27,700 --> 00:08:30,260
これは、2 つの入力ではなく 13,000 

165
00:08:30,260 --> 00:08:32,820
の入力を持つ関数の基本的な考え方と同じです。

166
00:08:33,400 --> 00:08:36,430
ネットワークの 13,000 の重みとバイアスをすべ

167
00:08:36,430 --> 00:08:39,460
て巨大な列ベクトルに編成することを想像してください。

168
00:08:40,140 --> 00:08:43,703
コスト関数の負の勾配は単なるベクトルであり、

169
00:08:43,703 --> 00:08:47,267
この非常に巨大な入力空間内の何らかの方向であ

170
00:08:47,267 --> 00:08:50,830
り、これらすべての数値に対するどの微調整がコ

171
00:08:50,830 --> 00:08:54,880
スト関数の最も急速な減少を引き起こすかを示します。

172
00:08:55,640 --> 00:08:58,631
そしてもちろん、特別に設計されたコスト関数を使用して、

173
00:08:58,631 --> 00:09:01,734
重みとバイアスを変更して値を下げることは、トレーニング 

174
00:09:01,734 --> 00:09:04,504
データの各部分に対するネットワークの出力を 10 

175
00:09:04,504 --> 00:09:07,495
個の値のランダムな配列のように見せることを意味し、より

176
00:09:07,495 --> 00:09:10,820
実際に望む決定に近づけることを意味します。それを作るのです。

177
00:09:11,440 --> 00:09:14,613
覚えておくことが重要です。このコスト関数にはすべてのトレー

178
00:09:14,613 --> 00:09:17,787
ニング データの平均が含まれるため、これを最小化すると、そ

179
00:09:17,787 --> 00:09:20,961
れらのサンプルすべてでパフォーマンスが向上することになりま

180
00:09:20,961 --> 00:09:21,180
す。

181
00:09:23,820 --> 00:09:27,206
この勾配を効率的に計算するためのアルゴリズムは、事実上、ニュ

182
00:09:27,206 --> 00:09:30,593
ーラル ネットワークの学習方法の中心であり、バックプロパゲー

183
00:09:30,593 --> 00:09:33,980
ションと呼ばれます。これについては、次のビデオで説明します。

184
00:09:34,660 --> 00:09:37,241
そこでは、時間をかけて、特定のトレーニング 

185
00:09:37,241 --> 00:09:40,293
データの各重みとバイアスに正確に何が起こっているのか

186
00:09:40,293 --> 00:09:43,344
を詳しく見ていき、関連する計算や式の山の向こうで何が

187
00:09:43,344 --> 00:09:46,395
起こっているのかを直感的に感じられるようにしたいと考

188
00:09:46,395 --> 00:09:47,100
えています。

189
00:09:47,780 --> 00:09:51,306
今ここで、実装の詳細とは関係なく、私が皆さんに知っておい

190
00:09:51,306 --> 00:09:54,833
ていただきたい主なことは、ネットワーク学習について話すと

191
00:09:54,833 --> 00:09:58,360
きの意味は、コスト関数を最小化するだけだということです。

192
00:09:59,300 --> 00:10:01,500
そして、その結果の 1 つとして、このコスト関

193
00:10:01,500 --> 00:10:03,700
数が良好な滑らかな出力を持つことが重要であるこ

194
00:10:03,700 --> 00:10:05,900
とに注意してください。これにより、下り坂を少し

195
00:10:05,900 --> 00:10:08,100
ずつ進むことで極小値を見つけることができます。

196
00:10:09,260 --> 00:10:12,460
ちなみに、生物学的ニューロンのように単に二値的

197
00:10:12,460 --> 00:10:15,661
に活性または不活性になるのではなく、人工ニュー

198
00:10:15,661 --> 00:10:19,140
ロンが継続的に範囲の活性化を行うのはこのためです。

199
00:10:20,220 --> 00:10:23,490
負の勾配の倍数によって関数の入力を繰り返し微調

200
00:10:23,490 --> 00:10:26,760
整するこのプロセスは、勾配降下法と呼ばれます。

201
00:10:27,300 --> 00:10:29,940
これは、コスト関数の局所最小値、つまりこ

202
00:10:29,940 --> 00:10:32,580
のグラフの谷に向かって収束する方法です。

203
00:10:33,440 --> 00:10:36,055
もちろん、13,000 次元の入力空間でのナ

204
00:10:36,055 --> 00:10:38,671
ッジは少し理解するのが難しいため、まだ 2 

205
00:10:38,671 --> 00:10:41,287
つの入力を持つ関数の図を示していますが、これ

206
00:10:41,287 --> 00:10:44,260
については空間を使わずに考える良い方法があります。

207
00:10:45,080 --> 00:10:48,440
負の勾配の各成分から 2 つのことが分かります。

208
00:10:49,060 --> 00:10:52,099
もちろん、この符号は、入力ベクトルの対応するコンポーネン

209
00:10:52,099 --> 00:10:55,140
トを上または下に微調整する必要があるかどうかを示します。

210
00:10:55,800 --> 00:10:59,260
しかし重要なのは、これらすべての要素の相対的な大きさによ

211
00:10:59,260 --> 00:11:02,720
って、どの変更がより重要であるかがわかるということです。

212
00:11:05,220 --> 00:11:07,437
私たちのネットワークでは、重みの 1 

213
00:11:07,437 --> 00:11:10,005
つを調整すると、他の重みを調整するよりもコス

214
00:11:10,005 --> 00:11:13,040
ト関数にはるかに大きな影響を与える可能性があります。

215
00:11:14,800 --> 00:11:16,410
これらの接続の中には、トレーニング 

216
00:11:16,410 --> 00:11:18,200
データにとってより重要なものもあります。

217
00:11:19,320 --> 00:11:22,494
したがって、気が遠くなるような膨大なコスト関数のこ

218
00:11:22,494 --> 00:11:25,669
の勾配ベクトルについて考える方法は、各重みとバイア

219
00:11:25,669 --> 00:11:28,844
スの相対的な重要性、つまり、これらの変更のどれが最

220
00:11:28,844 --> 00:11:32,400
も費用対効果が高いかをエンコードしているということです。

221
00:11:33,620 --> 00:11:36,640
これは方向性についての単なる考え方です。

222
00:11:37,100 --> 00:11:39,942
より単純な例を挙げると、入力として 2 

223
00:11:39,942 --> 00:11:44,065
つの変数を持つ関数があり、ある特定の点での勾配が 3,1 

224
00:11:44,065 --> 00:11:48,187
になると計算した場合、一方では、次のような場合にそれを言っ

225
00:11:48,187 --> 00:11:52,309
ていると解釈できます。その入力に立って、この方向に沿って移

226
00:11:52,309 --> 00:11:54,868
動すると、関数が最も早く増加します。

227
00:11:54,868 --> 00:11:58,990
つまり、入力点の平面上で関数をグラフにすると、そのベクトル

228
00:11:58,990 --> 00:12:02,260
が真っ直ぐ上り坂の方向を与えることになります。

229
00:12:02,860 --> 00:12:06,689
しかし、これを別の読み方で読むと、この最初の変数への変更は 

230
00:12:06,689 --> 00:12:10,135
2 番目の変数への変更の 3 倍の重要性があり、少なく

231
00:12:10,135 --> 00:12:12,177
とも関連する入力の近傍では、X 

232
00:12:12,177 --> 00:12:15,623
値を微調整する方がはるかに大きな影響を与えるということ

233
00:12:15,623 --> 00:12:16,900
になります。バック。

234
00:12:19,880 --> 00:12:22,340
ズームアウトして、これまでの状況をまとめてみましょう。

235
00:12:22,840 --> 00:12:25,646
ネットワーク自体は、784 個の入力と 10 

236
00:12:25,646 --> 00:12:29,185
個の出力を備えた関数であり、これらすべての加重合計によって

237
00:12:29,185 --> 00:12:30,040
定義されます。

238
00:12:30,640 --> 00:12:33,680
コスト関数はその上に複雑な層が重なっています。

239
00:12:33,980 --> 00:12:37,850
13,000 の重みとバイアスを入力として受け取り、ト

240
00:12:37,850 --> 00:12:41,720
レーニング例に基づいて粗さの単一の尺度を吐き出します。

241
00:12:42,440 --> 00:12:46,900
そして、コスト関数の勾配はさらに複雑な層になります。

242
00:12:47,360 --> 00:12:49,961
これは、これらすべての重みとバイアスに対するど

243
00:12:49,961 --> 00:12:52,563
のような調整がコスト関数の値に最も速い変化を引

244
00:12:52,563 --> 00:12:55,165
き起こすかを示します。これは、どの重みに対する

245
00:12:55,165 --> 00:12:57,880
どの変更が最も重要かを示していると解釈できます。

246
00:13:02,560 --> 00:13:05,193
では、ランダムな重みとバイアスを使用してネットワー

247
00:13:05,193 --> 00:13:07,827
クを初期化し、この勾配降下プロセスに基づいてそれら

248
00:13:07,827 --> 00:13:10,460
を何度も調整すると、これまでに見たことのない画像で

249
00:13:10,460 --> 00:13:13,200
実際にどの程度のパフォーマンスが得られるでしょうか?

250
00:13:14,100 --> 00:13:16,590
私がここで説明したものは、それぞれ 16 

251
00:13:16,590 --> 00:13:19,555
個のニューロンからなる 2 つの隠れ層を備えており

252
00:13:19,555 --> 00:13:22,520
、主に美的理由から選択されていますが、悪くはなく、

253
00:13:22,520 --> 00:13:25,960
表示される新しい画像の約 96% を正しく分類しています。

254
00:13:26,680 --> 00:13:29,609
そして正直に言うと、それが台無しにしているいくつか

255
00:13:29,609 --> 00:13:32,540
の例を見ると、少し緩めなければならないと感じます。

256
00:13:36,220 --> 00:13:38,928
ここで、隠れ層構造を試していくつかの調整を加

257
00:13:38,928 --> 00:13:41,760
えると、これを最大 98% まで達成できます。

258
00:13:41,760 --> 00:13:42,720
それはとても良いことです！

259
00:13:43,020 --> 00:13:45,346
これは最高ではありません。この単純なバニラ 

260
00:13:45,346 --> 00:13:48,413
ネットワークよりも洗練されれば、確かにパフォーマンスを向上

261
00:13:48,413 --> 00:13:51,479
させることはできますが、最初のタスクがどれほど困難であるか

262
00:13:51,479 --> 00:13:54,546
を考えると、これまでに見たことのない画像でこれほどうまく動

263
00:13:54,546 --> 00:13:57,613
作するネットワークには、信じられないほどの何かがあると思い

264
00:13:57,613 --> 00:14:00,679
ます。どのようなパターンを探すべきかを具体的に指示したこと

265
00:14:00,679 --> 00:14:01,420
はありません。

266
00:14:02,560 --> 00:14:05,385
もともと、私がこの構造を動機づけた方法は、2 

267
00:14:05,385 --> 00:14:07,597
番目の層が小さなエッジを検出し、3 

268
00:14:07,597 --> 00:14:11,160
番目の層がそれらのエッジをつなぎ合わせてループや長い線を認

269
00:14:11,160 --> 00:14:14,722
識し、それらがつなぎ合わされるかもしれないという希望を説明

270
00:14:14,722 --> 00:14:17,180
することでした。一緒に数字を認識します。

271
00:14:17,960 --> 00:14:19,145
では、これは私たちのネットワークが

272
00:14:19,145 --> 00:14:20,400
実際に行っていることなのでしょうか?

273
00:14:21,079 --> 00:14:24,400
まあ、少なくともこれに関しては、まったくそうではありません。

274
00:14:24,820 --> 00:14:27,797
前回のビデオで、最初の層のすべてのニューロンから 2 

275
00:14:27,797 --> 00:14:30,443
番目の層の特定のニューロンへの接続の重みが、2 

276
00:14:30,443 --> 00:14:33,200
番目の層のニューロンが認識している特定のピクセル 

277
00:14:33,200 --> 00:14:36,177
パターンとしてどのように視覚化できるかを説明したことを

278
00:14:36,177 --> 00:14:37,060
覚えていますか?

279
00:14:37,780 --> 00:14:40,935
最初のレイヤーから次のレイヤーまで、これらのトランジ

280
00:14:40,935 --> 00:14:44,091
ションに関連付けられたウェイトに対して実際にそれを行

281
00:14:44,091 --> 00:14:47,247
うと、あちこちで孤立した小さなエッジを検出するのでは

282
00:14:47,247 --> 00:14:50,402
なく、それらは、ほとんどランダムに見えますが、非常に

283
00:14:50,402 --> 00:14:53,680
緩やかなパターンがいくつかあるだけです。そこの真ん中。

284
00:14:53,760 --> 00:14:56,774
考えられる重みとバイアスの計り知れないほど広い 

285
00:14:56,774 --> 00:14:59,789
13,000 次元空間において、私たちのネットワ

286
00:14:59,789 --> 00:15:02,804
ークは、ほとんどの画像を正常に分類したにもかかわ

287
00:15:02,804 --> 00:15:05,819
らず、私たちが期待していたパターンを正確に検出で

288
00:15:05,819 --> 00:15:08,960
きない、幸せな小さな局所最小値を見つけたようです。

289
00:15:09,780 --> 00:15:11,800
この点をよく理解するには、ランダムな画像を

290
00:15:11,800 --> 00:15:13,820
入力したときに何が起こるかを見てください。

291
00:15:14,320 --> 00:15:16,304
システムが賢いものであれば、10 

292
00:15:16,304 --> 00:15:19,571
個の出力ニューロンのどれも実際には活性化していない、ある

293
00:15:19,571 --> 00:15:22,839
いはすべてを均等に活性化していないなど、不確かに感じられ

294
00:15:22,839 --> 00:15:26,107
ることを期待するかもしれませんが、その代わりに、このラン

295
00:15:26,107 --> 00:15:29,375
ダムなノイズが確実であるかのように、自信を持ってナンセン

296
00:15:29,375 --> 00:15:32,526
スな答えを返します。 5 は、5 の実際の画像が 5 

297
00:15:32,526 --> 00:15:34,160
であるのと同様に、5 です。

298
00:15:34,540 --> 00:15:37,620
別の言い方をすると、このネットワークは数字をかなりう

299
00:15:37,620 --> 00:15:40,700
まく認識できても、それを描画する方法がわかりません。

300
00:15:41,420 --> 00:15:43,329
その多くは、トレーニングの設定が非

301
00:15:43,329 --> 00:15:45,240
常に厳しく制限されているためです。

302
00:15:45,880 --> 00:15:47,740
つまり、ここではネットワークの立場に立って考えてみましょう。

303
00:15:48,140 --> 00:15:51,272
その観点から見ると、宇宙全体は小さなグリッドの

304
00:15:51,272 --> 00:15:54,405
中心にある、明確に定義された不動の数字だけで構

305
00:15:54,405 --> 00:15:57,538
成されており、そのコスト関数は、その決定に完全

306
00:15:57,538 --> 00:16:01,080
な自信を持つこと以外には何の動機も与えませんでした。

307
00:16:02,120 --> 00:16:04,659
これが第 2 層のニューロンが実際に行っていることのイメ

308
00:16:04,659 --> 00:16:07,199
ージであるため、なぜエッジやパターンを検出するという動機

309
00:16:07,199 --> 00:16:09,920
でこのネットワークを紹介するのか不思議に思うかもしれません。

310
00:16:09,920 --> 00:16:12,300
つまり、それは最終的にはまったくそうではありません。

311
00:16:13,380 --> 00:16:17,180
これは最終目標ではなく、出発点です。

312
00:16:17,640 --> 00:16:20,021
率直に言って、これは 80 年代から 90 

313
00:16:20,021 --> 00:16:22,834
年代に研究された種類の古いテクノロジーであり、より詳

314
00:16:22,834 --> 00:16:26,081
細な現代の亜種を理解する前に、それを理解する必要があります。

315
00:16:26,081 --> 00:16:28,895
また、明らかにいくつかの興味深い問題を解決できる可能

316
00:16:28,895 --> 00:16:31,709
性がありますが、何を深く掘り下げるほど、これらの隠れ

317
00:16:31,709 --> 00:16:34,740
層が実際に機能しているほど、その層は知性が低く見えます。

318
00:16:38,480 --> 00:16:40,413
ネットワークがどのように学習するかということか

319
00:16:40,413 --> 00:16:42,347
ら、あなたがどのように学習するかに少し焦点を移

320
00:16:42,347 --> 00:16:44,281
しますが、それは、何らかの方法でここで取り上げ

321
00:16:44,281 --> 00:16:46,300
た資料に積極的に取り組んだ場合にのみ起こります。

322
00:16:47,060 --> 00:16:49,802
皆さんにしていただきたいのは、非常に簡単なことの 

323
00:16:49,802 --> 00:16:52,544
1 つです。今ちょっと立ち止まって、このシステムに

324
00:16:52,544 --> 00:16:55,286
どのような変更を加えるか、エッジやパターンなどをよ

325
00:16:55,286 --> 00:16:58,028
りよく認識できるようにしたい場合に画像をどのように

326
00:16:58,028 --> 00:17:00,880
認識するかについて、少しの間深く考えてみてください。

327
00:17:01,479 --> 00:17:03,941
しかしそれよりも、実際に教材に取り組むには

328
00:17:03,941 --> 00:17:07,107
、深層学習とニューラル ネットワークに関するマイケル 

329
00:17:07,107 --> 00:17:09,099
ニールセンの本を強くお勧めします。

330
00:17:09,680 --> 00:17:12,573
この中には、まさにこの例をダウンロードして試

331
00:17:12,573 --> 00:17:15,466
すためのコードとデータが含まれており、そのコ

332
00:17:15,466 --> 00:17:18,359
ードが何をしているのかを段階的に説明します。

333
00:17:19,300 --> 00:17:22,086
素晴らしいのは、この本が無料で一般公開されているというこ

334
00:17:22,086 --> 00:17:24,873
とです。この本から何かを得ることができましたら、私と一緒

335
00:17:24,873 --> 00:17:27,660
にニールセンの取り組みに寄付することを検討してください。

336
00:17:27,660 --> 00:17:31,243
また、Chris Ola による驚異的で美しいブログ投稿や 

337
00:17:31,243 --> 00:17:34,110
Distill の記事など、私がとても気に入って

338
00:17:34,110 --> 00:17:36,500
いる他のリソースも説明にリンクしました。

339
00:17:38,280 --> 00:17:41,080
最後の数分間をここで締めくくるために、リーシャ・

340
00:17:41,080 --> 00:17:43,880
リーとのインタビューの抜粋に戻りたいと思います。

341
00:17:44,300 --> 00:17:46,048
前回のビデオで彼女を覚えているかもしれません。

342
00:17:46,048 --> 00:17:47,720
彼女は深層学習で博士号の研究をしていました。

343
00:17:48,300 --> 00:17:50,793
この短い抜粋で、彼女は、いくつかの最新の画像認識ネ

344
00:17:50,793 --> 00:17:53,785
ットワークが実際にどのように学習しているかを深く掘り下げた 

345
00:17:53,785 --> 00:17:55,780
2 つの最近の論文について話しています。

346
00:17:56,120 --> 00:17:58,625
私たちが会話の中でどのような状況にあったかを説明するた

347
00:17:58,625 --> 00:18:01,316
めに、最初の論文では、画像認識に非常に優れた特にディープ 

348
00:18:01,316 --> 00:18:02,801
ニューラル ネットワークの 1 

349
00:18:02,801 --> 00:18:05,306
つを取り上げ、適切にラベル付けされたデータセットでトレ

350
00:18:05,306 --> 00:18:07,812
ーニングする代わりに、トレーニング前にすべてのラベルを

351
00:18:07,812 --> 00:18:08,740
シャッフルしました。

352
00:18:09,480 --> 00:18:12,278
すべてがランダムにラベル付けされているだけであるため、

353
00:18:12,278 --> 00:18:15,076
ここでのテスト精度は明らかにランダムよりも優れていませ

354
00:18:15,076 --> 00:18:17,874
んでしたが、それでも適切にラベル付けされたデータセット

355
00:18:17,874 --> 00:18:20,880
で行うのと同じトレーニング精度を達成することができました。

356
00:18:21,600 --> 00:18:25,266
基本的に、この特定のネットワークの何百万もの重みは、ラ

357
00:18:25,266 --> 00:18:28,117
ンダムなデータを記憶するだけで十分でした。

358
00:18:28,117 --> 00:18:31,783
このため、このコスト関数の最小化が実際に画像内の何らか

359
00:18:31,783 --> 00:18:35,449
の構造に対応するのか、それとも単なる記憶なのかという疑

360
00:18:35,449 --> 00:18:36,400
問が生じます。

361
00:18:51,440 --> 00:18:56,520
その精度曲線を見ると、ランダムなデータセットでトレーニ

362
00:18:56,520 --> 00:19:01,601
ングしているだけだとすると、その曲線はほぼ直線的に非常

363
00:19:01,601 --> 00:19:06,682
にゆっくりと下がっていくので、可能な極小値を見つけるの

364
00:19:06,682 --> 00:19:12,140
に本当に苦労しています。 、その精度を実現する適切な重み。

365
00:19:12,240 --> 00:19:16,235
一方、適切なラベルを持つ構造化データセットで実際にト

366
00:19:16,235 --> 00:19:20,230
レーニングしている場合、最初は少しいじってみましたが

367
00:19:20,230 --> 00:19:24,225
、その後、その精度レベルに到達するのが非常に早くなっ

368
00:19:24,225 --> 00:19:28,220
たので、ある意味では極大値を見つけるのが簡単でした。

369
00:19:28,540 --> 00:19:32,149
そして、これに関して興味深いのは、実際に数年前の別の論文

370
00:19:32,149 --> 00:19:34,211
が明らかになったということです。

371
00:19:34,211 --> 00:19:37,820
この論文では、ネットワーク層についてはさらに単純化されて

372
00:19:37,820 --> 00:19:41,429
いますが、その結果の 1 つは、最適化の状況を見てみると

373
00:19:41,429 --> 00:19:45,039
どうなるかを示していました。これらのネットワークが学習す

374
00:19:45,039 --> 00:19:48,648
る傾向にある極小値は、実際には同じ品質であるため、ある意

375
00:19:48,648 --> 00:19:52,257
味、データセットが構造化されていれば、それをはるかに簡単

376
00:19:52,257 --> 00:19:54,320
に見つけることができるはずです。

377
00:19:58,160 --> 00:19:59,398
いつものように、Patreon 

378
00:19:59,398 --> 00:20:01,180
でサポートしてくださっている皆様に感謝します。

379
00:20:01,520 --> 00:20:03,225
Patreon がいかにゲームチェンジャー

380
00:20:03,225 --> 00:20:04,931
であるかについては以前にも述べましたが、こ

381
00:20:04,931 --> 00:20:06,800
れらのビデオは本当に皆さんなしでは不可能です。

382
00:20:07,460 --> 00:20:09,233
また、シリーズの最初のビデオをサポートしてくれた

383
00:20:09,233 --> 00:20:10,784
ベンチャーキャピタル企業 Amplify 

384
00:20:10,784 --> 00:20:12,780
Partners にも特別な感謝を表したいと思います。

