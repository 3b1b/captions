1
00:00:00,000 --> 00:00:03,620
前回のビデオでは、ニューラル ネ

2
00:00:03,620 --> 00:00:07,240
ットワークの構造を説明しました。

3
00:00:07,240 --> 00:00:09,844
記憶に新しいように、ここで簡単に要約します。

4
00:00:09,844 --> 00:00:13,160
それか ら、このビデオには 2 つの主な目標があります。

5
00:00:13,160 --> 00:00:15,541
1 つ目は、勾配降下の考え方を導入することです。

6
00:00:15,541 --> 00:00:18,021
これは、ニューラル ネットワ ークの学習方法だけで

7
00:00:18,021 --> 00:00:20,800
なく、他の多くの機械学習の仕組みの基礎にもなっています。

8
00:00:20,800 --> 00:00:23,680
その後、この特定のネットワークがどのように動作す

9
00:00:23,680 --> 00:00:26,560
るか、そしてニューロンの 隠れた層が最終的に何を

10
00:00:26,560 --> 00:00:29,560
探すのかについてもう少し詳しく掘り下げていきます。

11
00:00:29,560 --> 00:00:32,029
念のため言っておきますが、ここでの目標は手書

12
00:00:32,029 --> 00:00:34,498
き数字認識の古典的な例 、つまりニューラル

13
00:00:34,498 --> 00:00:37,080
ネットワークの Hello World です。

14
00:00:37,080 --> 00:00:39,146
これらの数字は 28x28 ピクセル

15
00:00:39,146 --> 00:00:41,649
グリッド上にレンダリングされ 、各ピクセルは

16
00:00:41,649 --> 00:00:44,260
0 から 1 までのグレースケール値を持ちます。

17
00:00:44,260 --> 00:00:47,830
これらは、ネットワークの入力層の 784

18
00:00:47,830 --> 00:00:51,400
個のニューロンの活性化を決定するものです。

19
00:00:51,400 --> 00:00:55,033
次の層の各ニューロンの活性化は、前の層のすべ

20
00:00:55,033 --> 00:00:58,666
ての活性化の重み付き合 計に、バイアスと呼ば

21
00:00:58,666 --> 00:01:02,300
れる特別な数値を加えたものに基づいています。

22
00:01:02,300 --> 00:01:05,769
前回のビデオで説明した方法である、シグモイド圧縮や

23
00:01:05,769 --> 00:01:09,640
R eLU などの他の関数を使用してその合計を計算します。

24
00:01:09,640 --> 00:01:13,227
合計すると、それぞれ 16 個のニューロンを持つ 2

25
00:01:13,227 --> 00:01:17,081
つの隠れ層というやや恣意 的な選択を考慮すると、ネットワー

26
00:01:17,081 --> 00:01:20,536
クには調整できる重みとバイアスが約 13,00 0

27
00:01:20,536 --> 00:01:24,389
あり、ネットワークが実際に何を行うかを正確に決定するのはこ

28
00:01:24,389 --> 00:01:25,320
れらの値です。

29
00:01:25,320 --> 00:01:28,203
そして、このネットワークが特定の数字を分類すると言う

30
00:01:28,203 --> 00:01:30,198
ときの意味は、最後の層にある 10

31
00:01:30,198 --> 00:01:33,082
個のニューロンの中で最も明るいものがその数字に対応す

32
00:01:33,082 --> 00:01:34,080
るということです。

33
00:01:34,080 --> 00:01:38,003
レイヤー構造について私たちが念頭に置いていた動機は、おそ

34
00:01:38,003 --> 00:01:41,927
らく 2 番目のレイヤーでエッジを認識し、3 番目のレイ

35
00:01:41,927 --> 00:01:45,851
ヤーでループやラインなどのパターンを認識し、最後のレイヤ

36
00:01:45,851 --> 00:01:49,640
ーでそれらのパターンをつなぎ合わせて、数字を認識します。

37
00:01:49,640 --> 00:01:52,880
ここでは、ネットワークがどのように学習するかを学びます。

38
00:01:52,880 --> 00:01:55,822
私たちが望んでいるのは、このネットワークに大量のトレ

39
00:01:55,822 --> 00:01:58,538
ーニング データを表示でき るアルゴリズムです。

40
00:01:58,538 --> 00:02:01,933
このデータは、手書きの数字のさまざまな画像と、それらが本来

41
00:02:01,933 --> 00:02:04,422
あるべきものを示すラベルの形で提供されます。

42
00:02:04,422 --> 00:02:06,459
トレーニング データのパフォーマン

43
00:02:06,459 --> 00:02:09,062
スを向上させるために、これらの 13,000

44
00:02:09,062 --> 00:02:10,760
の重みとバイアスを調整します。

45
00:02:10,760 --> 00:02:14,091
この階層構造により、学習した内容がトレーニング

46
00:02:14,091 --> 00:02:17,840
デ ータを超えた画像に一般化されることが期待されます。

47
00:02:17,840 --> 00:02:22,169
これをテストする方法は、ネットワークをトレーニングし

48
00:02:22,169 --> 00:02:26,498
た後、さらにラベル付けされた データを表示し、新しい

49
00:02:26,498 --> 00:02:31,160
画像がどの程度正確に分類されているかを確認することです。

50
00:02:31,160 --> 00:02:34,610
私たちにとって幸運なことに、そしてまずこれが一般的な例とな

51
00:02:34,610 --> 00:02:38,060
っているのは、MNI ST データベースの背後にいる善良な

52
00:02:38,060 --> 00:02:40,558
人々が、それぞれが本来あるべき数字でラベ

53
00:02:40,558 --> 00:02:44,009
ル付けされた何万もの手書きの数字画像のコレクションをまとめ

54
00:02:44,009 --> 00:02:45,080
てくれたことです。

55
00:02:45,080 --> 00:02:48,492
機械を学習すると表現するのは挑発的ですが、それがどのよう

56
00:02:48,492 --> 00:02:51,782
に機能するかを一度理解すると、 それはおかしな SF

57
00:02:51,782 --> 00:02:55,194
の前提というよりも、はるかに微積分の練習のように感じられ

58
00:02:55,194 --> 00:02:55,560
ます。

59
00:02:55,560 --> 00:02:58,300
つまり、基本的には、特定の機能の

60
00:02:58,300 --> 00:03:01,040
最小値を見つけることになります。

61
00:03:01,040 --> 00:03:04,709
概念的には、各ニューロンは前の層のすべてのニューロンに接

62
00:03:04,709 --> 00:03:08,378
続されていると考 えており、その活性化を定義する加重合計

63
00:03:08,378 --> 00:03:10,606
の重みはそれらの接続の強さのよう

64
00:03:10,606 --> 00:03:14,275
なものであり、バイアスは何らかの指標であることを覚えてお

65
00:03:14,275 --> 00:03:17,945
いてください。そのニューロンが活動的になる傾向があるか、

66
00:03:17,945 --> 00:03:19,780
不活動的である傾向があるか。

67
00:03:19,780 --> 00:03:22,554
まず、これらの重みとバイアスをすべ

68
00:03:22,554 --> 00:03:25,020
て完全にランダムに初期化します。

69
00:03:25,020 --> 00:03:27,073
言うまでもなく、このネットワークはランダムに

70
00:03:27,073 --> 00:03:29,126
何かを実行しているだけ なので、特定のトレー

71
00:03:29,126 --> 00:03:31,180
ニング例ではひどいパフォーマンスになります。

72
00:03:31,180 --> 00:03:33,927
たとえば、この 3 の画像を入力すると

73
00:03:33,927 --> 00:03:36,820
、出力レイヤーは混乱したように見えます。

74
00:03:36,820 --> 00:03:39,903
そこで、あなたがすることは、コスト関数を定義することです。

75
00:03:39,903 --> 00:03:42,986
これは、出力がほとんどのニューロンでは 0 ですが、この

76
00:03:42,986 --> 00:03:45,963
ニューロンでは 1 である活性化を持つべきであることをコ

77
00:03:45,963 --> 00:03:48,940
ンピューター、いや、悪いコンピューターに伝える方法です。

78
00:03:48,940 --> 00:03:51,740
あなたが私にくれたものは全くのゴミです。

79
00:03:51,740 --> 00:03:55,234
これをもう少し数学的に言うと、これらのゴミ出力

80
00:03:55,234 --> 00:03:58,728
アクティベーショ ンのそれぞれと、それらに必要

81
00:03:58,728 --> 00:04:01,462
な値との差の二乗を合計します。これ

82
00:04:01,462 --> 00:04:06,020
が、単一のトレーニング サンプルのコストと呼ばれるものです。

83
00:04:06,020 --> 00:04:09,154
ネットワークが自信を持って画像を正しく分類してい

84
00:04:09,154 --> 00:04:12,550
る場合にはこの合計は小さくなりますが、ネットワーク

85
00:04:12,550 --> 00:04:15,685
が何をしているかを認識していないように見える場合

86
00:04:15,685 --> 00:04:18,820
にはこの合計が大きくなることに注意してください。

87
00:04:18,820 --> 00:04:23,289
したがって、自由に使える数万のトレーニング サン

88
00:04:23,289 --> 00:04:27,580
プルすべての平均コストを考慮することになります。

89
00:04:27,580 --> 00:04:30,538
この平均コストは、ネットワークがどの程度劣悪であるか、およ

90
00:04:30,538 --> 00:04:33,300
びコンピュータの動作がどの程度悪くなるかを示す尺度です。

91
00:04:33,300 --> 00:04:35,300
それは複雑なことです。

92
00:04:35,300 --> 00:04:37,460
ネットワーク自体が基本的に 784

93
00:04:37,460 --> 00:04:40,460
個の数値、ピクセル値を入力として取り込み、1 0

94
00:04:40,460 --> 00:04:44,060
個の数値を出力として吐き出す関数であったことを覚えていますか

95
00:04:44,060 --> 00:04:47,660
? ある意味、 ネットワークはこれらすべての重みとバイアスに

96
00:04:47,660 --> 00:04:49,700
よってパラメーター化されています。

97
00:04:49,700 --> 00:04:53,340
コスト関数は、その上に複雑な層が重なっています。

98
00:04:53,340 --> 00:04:56,473
それらの 13,000 ほどの重みとバイアスを入

99
00:04:56,473 --> 00:04:58,693
力として受け取り、それらの重みと

100
00:04:58,693 --> 00:05:01,305
バイアスがどれほど悪いかを説明する 1

101
00:05:01,305 --> 00:05:04,047
つの数値を吐き出します。その定義方法は、

102
00:05:04,047 --> 00:05:07,181
数万のトレーニング データすべてに対するネットワ

103
00:05:07,181 --> 00:05:09,140
ークの動作によって異なります。

104
00:05:09,140 --> 00:05:12,460
それは考えるべきことがたくさんあります。

105
00:05:12,460 --> 00:05:14,420
しかし、コンピューターがどのようなくだらない仕事をしているか

106
00:05:14,420 --> 00:05:16,380
を単にコンピューターに伝えるだけでは、あまり役に立ちません。

107
00:05:16,380 --> 00:05:18,769
これらの重みとバイアスを変更して改

108
00:05:18,769 --> 00:05:21,300
善する方法を教えたいと考えています。

109
00:05:21,300 --> 00:05:23,464
わかりやすくするために、13,000

110
00:05:23,464 --> 00:05:26,426
個の入力を持つ関数を想像するのに苦労するのではなく

111
00:05:26,426 --> 00:05:28,933
、入力として 1 つの数値、出力として 1

112
00:05:28,933 --> 00:05:31,440
つの数値を持つ単純な関数を想像してください。

113
00:05:31,440 --> 00:05:36,420
この関数の値を最小にする入力をどのように見つけますか?

114
00:05:36,420 --> 00:05:39,464
微積分の学生は、その最小値を明示的に計算できる場合が

115
00:05:39,464 --> 00:05:41,571
あることを知っているでしょうが、本

116
00:05:41,571 --> 00:05:44,615
当に複雑な関数では常に実現可能であるとは限りません。

117
00:05:44,615 --> 00:05:46,722
もちろん、この状況の 13,000

118
00:05:46,722 --> 00:05:49,298
入力バージョンでは、非常に複雑なニューラル

119
00:05:49,298 --> 00:05:51,640
ネットワークのコスト関数では不可能です。

120
00:05:51,640 --> 00:05:55,819
より柔軟な戦術は、任意の入力から開始して、その出力を下げる

121
00:05:55,819 --> 00:05:59,860
ためにどの方向にステップを進めるべきかを判断することです。

122
00:05:59,860 --> 00:06:04,350
具体的には、現在の関数の傾きがわかる場合は

123
00:06:04,350 --> 00:06:08,637
、その傾きが正の場合は左にシフトし、その

124
00:06:08,637 --> 00:06:12,720
傾きが負の場合は入力を右にシフトします。

125
00:06:12,720 --> 00:06:16,700
これを繰り返し実行し、各ポイントで新しい傾きをチェックし、適

126
00:06:16,700 --> 00:06:20,680
切な手順を実行すると、関数の極小値に近づくことになります。

127
00:06:20,680 --> 00:06:22,580
ここで皆さんが思い浮かべるイメー

128
00:06:22,580 --> 00:06:24,600
ジは、丘を転がり落ちるボールです。

129
00:06:24,600 --> 00:06:28,380
そして、この非常に単純化された単一入力関数であっても、ど

130
00:06:28,380 --> 00:06:32,160
のランダム入力から開始するかに応じて、到達する可能性のあ

131
00:06:32,160 --> 00:06:35,940
る谷が多数あり、到達する極小値が可能な限り最小の値になる

132
00:06:35,940 --> 00:06:39,460
という保証はないことに注意してください。コスト関数の。

133
00:06:39,460 --> 00:06:43,180
これはニューラル ネットワークの場合にも当てはまります。

134
00:06:43,180 --> 00:06:47,560
また、ステップ サイズを勾配に比例させると、勾配が最小に

135
00:06:47,560 --> 00:06:51,941
向かって平坦になるとステップがどんどん小さくなり、オーバ

136
00:06:51,941 --> 00:06:56,020
ーシュートを防ぐことができることにも注目してください。

137
00:06:56,020 --> 00:06:58,957
もう少し複雑にして、代わりに 2 つの入力と

138
00:06:58,957 --> 00:07:01,640
1 つの出力を持つ関数を想像してください。

139
00:07:01,640 --> 00:07:05,412
入力空間を xy 平面、コスト関数をその上の

140
00:07:05,412 --> 00:07:09,020
面としてグラフ化すると考えることができます。

141
00:07:09,020 --> 00:07:12,555
関数の傾きについて尋ねる代わりに、関数の出力を

142
00:07:12,555 --> 00:07:16,090
最も早く減少させるために は、この入力空間でど

143
00:07:16,090 --> 00:07:19,780
の方向にステップすべきかを尋ねる必要があります。

144
00:07:19,780 --> 00:07:22,340
言い換えれば、下り坂の方向は何ですか？

145
00:07:22,340 --> 00:07:24,473
繰り返しますが、その丘を転がり落

146
00:07:24,473 --> 00:07:26,740
ちるボールを想像すると分かります。

147
00:07:26,740 --> 00:07:31,026
多変数微積分に詳しい人は、関数の勾配によって最

148
00:07:31,026 --> 00:07:35,312
も急な上昇の方向がわかり、関数を最も早く増加さ

149
00:07:35,312 --> 00:07:39,420
せるにはどの方向に進むべきかがわかるでしょう。

150
00:07:39,420 --> 00:07:43,600
当然のことながら、その勾配をマイナスにすると、関数

151
00:07:43,600 --> 00:07:47,460
を最も早く減少させるステップの方向がわかります。

152
00:07:47,460 --> 00:07:51,189
さらに、この勾配ベクトルの長さは、その最も

153
00:07:51,189 --> 00:07:54,580
急な勾配がどれほど急であるかを示します。

154
00:07:54,580 --> 00:07:56,720
多変数微積分に詳しくなく、さらに詳しく知りた

155
00:07:56,720 --> 00:07:58,764
い場合は、このテーマに 関して私がカーン

156
00:07:58,764 --> 00:08:01,100
アカデミーで行った作品の一部を確認してください。

157
00:08:01,100 --> 00:08:04,872
しかし正直に言って、あなたと私にとって現時点で重要なのは、

158
00:08:04,872 --> 00:08:08,519
原理的にはこのベクトル、つまり下り坂の方向とその急勾配を

159
00:08:08,519 --> 00:08:12,040
示すベクトルを計算する方法が存在するということだけです。

160
00:08:12,040 --> 00:08:14,660
知っていることがこれだけで、詳細が

161
00:08:14,660 --> 00:08:17,280
しっかりしていなくても大丈夫です。

162
00:08:17,280 --> 00:08:20,608
それができれば、関数を最小化するアルゴリズムは、こ

163
00:08:20,608 --> 00:08:23,937
の勾配の方向を計算し、下り 坂に向かって小さな一歩

164
00:08:23,937 --> 00:08:27,400
を踏み出し、それを何度も繰り返すことになるからです。

165
00:08:27,400 --> 00:08:30,550
これは、2 つの入力ではなく 13,000

166
00:08:30,550 --> 00:08:33,700
の入力を持つ関数の基本的な考え方と同じです。

167
00:08:33,700 --> 00:08:37,001
ネットワークの 13,000 の重みとバイアスをすべ

168
00:08:37,001 --> 00:08:40,180
て巨大な列ベクトルに編成することを想像してください。

169
00:08:40,180 --> 00:08:44,067
コスト関数の負の勾配は単なるベクトルであり、こ

170
00:08:44,067 --> 00:08:47,955
の非常に巨大な入 力空間内の何らかの方向であり

171
00:08:47,955 --> 00:08:50,829
、これらすべての数値に対するどの

172
00:08:50,829 --> 00:08:55,900
微調整がコスト関数の最も急速な減少を引き起こすかを示します。

173
00:08:55,900 --> 00:08:58,976
そしてもちろん、特別に設計されたコスト関数を使用して、重

174
00:08:58,976 --> 00:09:02,052
みとバイアスを 変更して値を下げることは、トレーニング

175
00:09:02,052 --> 00:09:04,908
データの各部分に対するネット ワークの出力を 10

176
00:09:04,908 --> 00:09:07,544
個の値のランダムな配列のように見せることを意味

177
00:09:07,544 --> 00:09:10,291
し、より実際に望む決定に近づけることを意味します。

178
00:09:10,291 --> 00:09:11,280
それを作るのです。

179
00:09:11,280 --> 00:09:15,606
覚えておくことが重要です。このコスト関数にはすべてのトレーニ

180
00:09:15,606 --> 00:09:17,914
ング データの平均が含まれるた

181
00:09:17,914 --> 00:09:22,240
め、これを最小化すると、それらのサンプルすべてでパフォーマン

182
00:09:22,240 --> 00:09:24,260
スが向上することになります。

183
00:09:24,260 --> 00:09:26,705
この勾配を効率的に計算するためのアルゴリズムは

184
00:09:26,705 --> 00:09:29,150
、事実上、ニュ ーラル ネットワークの学習方法

185
00:09:29,150 --> 00:09:31,913
の中心であり、バックプロパゲー ションと呼ばれます。

186
00:09:31,913 --> 00:09:34,040
これについては、次のビデオで説明します。

187
00:09:34,040 --> 00:09:36,853
そこでは、時間をかけて、特定のトレーニング

188
00:09:36,853 --> 00:09:40,306
データの各重みとバイアスに正 確に何が起こっているのか

189
00:09:40,306 --> 00:09:43,503
を詳しく見ていき、関連する計算や公式の山の向こう

190
00:09:43,503 --> 00:09:46,956
で何が起こっているのかを直感的に感じられるようにしたい

191
00:09:46,956 --> 00:09:47,980
と考えています。

192
00:09:47,980 --> 00:09:51,805
今ここで、実装の詳細とは関係なく、皆さんに知っておいて

193
00:09:51,805 --> 00:09:55,631
いただきたい主な点は、ネットワーク学習について話すとき

194
00:09:55,631 --> 00:09:59,320
の意味は、コスト関数を最小化するだけだということです。

195
00:09:59,320 --> 00:10:01,771
そして、その結果の 1 つとして、このコスト関

196
00:10:01,771 --> 00:10:04,223
数が良好な滑らか な出力を持つことが重要である

197
00:10:04,223 --> 00:10:06,142
ことに注意してください。これにより

198
00:10:06,142 --> 00:10:09,340
、下り坂を少しずつ進むことで極小値を見つけることができます。

199
00:10:09,340 --> 00:10:13,141
ちなみに、生物学的ニューロンのように単に二値的に

200
00:10:13,141 --> 00:10:16,942
活性または不活性になるのではなく、人工ニューロン

201
00:10:16,942 --> 00:10:20,440
が継続的に範囲の活性化を行うのはこのためです。

202
00:10:20,440 --> 00:10:23,769
負の勾配の倍数によって関数の入力を繰り返し微調

203
00:10:23,769 --> 00:10:26,960
整するこのプロセスは、勾配降下法と呼ばれます。

204
00:10:26,960 --> 00:10:30,053
これは、コスト関数の局所最小値、つまりこ

205
00:10:30,053 --> 00:10:33,000
のグラフの谷に向かって収束する方法です。

206
00:10:33,000 --> 00:10:35,962
もちろん、13,000 次元の入力空間でのナッジ

207
00:10:35,962 --> 00:10:38,554
は少し理解するのが 難しいため、まだ 2

208
00:10:38,554 --> 00:10:41,270
つの入力を持つ関数の図を示していますが、実

209
00:10:41,270 --> 00:10:44,232
際には、これについて考えるための優れた非空間的な

210
00:10:44,232 --> 00:10:45,220
方法があります。

211
00:10:45,220 --> 00:10:49,100
負の勾配の各成分から 2 つのことが分かります。

212
00:10:49,100 --> 00:10:52,539
もちろん、この符号は、入力ベクトルの対応するコンポーネン

213
00:10:52,539 --> 00:10:55,860
トを上または下に微調整する必要があるかどうかを示します。

214
00:10:55,860 --> 00:11:00,825
しかし重要なのは、これらすべての要素の相対的な大きさによ

215
00:11:00,825 --> 00:11:05,620
って、どの変更がより重要であるかがわかるということです。

216
00:11:05,620 --> 00:11:08,235
私たちのネットワークでは、重みの 1

217
00:11:08,235 --> 00:11:10,437
つを調整すると、他の重みを調整

218
00:11:10,437 --> 00:11:13,465
するよりもコスト関数にはるかに大きな影響を与

219
00:11:13,465 --> 00:11:14,980
える可能性があります。

220
00:11:14,980 --> 00:11:17,092
これらの接続の中には、トレーニング

221
00:11:17,092 --> 00:11:19,440
データにとってより重要なものもあります。

222
00:11:19,440 --> 00:11:23,070
したがって、気が遠くなるような膨大なコスト関数のこの

223
00:11:23,070 --> 00:11:26,700
勾配ベクトルについ て考える方法は、各重みとバイアス

224
00:11:26,700 --> 00:11:29,352
の相対的な重要性、つまり、これらの変

225
00:11:29,352 --> 00:11:32,983
更のどれが最も費用対効果が高いかをエンコードしている

226
00:11:32,983 --> 00:11:34,100
ということです。

227
00:11:34,100 --> 00:11:37,360
これは方向性についての単なる考え方です。

228
00:11:37,360 --> 00:11:41,768
より単純な例を挙げると、入力として 2 つの変数を持つ関

229
00:11:41,768 --> 00:11:46,024
数があり、ある特定の点での勾配が 3,1 になると計算

230
00:11:46,024 --> 00:11:49,368
した場合、一方では、次のように解釈できます。

231
00:11:49,368 --> 00:11:53,624
その入力に立 って、この方向に沿って移動すると、関数が最

232
00:11:53,624 --> 00:11:57,880
も早く増加し ます。つまり、入力点の平面上で関数をグラフ

233
00:11:57,880 --> 00:12:02,136
にすると、その ベクトルが真っ直ぐ上り坂の方向を与えるこ

234
00:12:02,136 --> 00:12:03,200
とになります。

235
00:12:03,200 --> 00:12:07,094
しかし、これを別の読み方で読むと、この最初の変数への変更は

236
00:12:07,094 --> 00:12:09,171
2 番目の変数 への変更の 3

237
00:12:09,171 --> 00:12:12,806
倍の重要性があり、少なくとも関連する入力の付近では、X

238
00:12:12,806 --> 00:12:16,441
値 を微調整する方がはるかに大きな影響を与えるということ

239
00:12:16,441 --> 00:12:17,740
になります。バック。

240
00:12:17,740 --> 00:12:22,880
さて、ズームアウトしてこれまでの状況をまとめてみましょう。

241
00:12:22,880 --> 00:12:25,335
ネットワーク自体は、784 個の入力と

242
00:12:25,335 --> 00:12:27,913
10 個の出力を備えた関 数であり、これら

243
00:12:27,913 --> 00:12:30,860
すべての重み付けされた合計によって定義されます。

244
00:12:30,860 --> 00:12:34,160
コスト関数は、その上に複雑な層が重なっています。

245
00:12:34,160 --> 00:12:38,477
13,000 の重みとバイアスを入力として受け取り、ト

246
00:12:38,477 --> 00:12:42,640
レーニング例に基づいて粗さの単一の尺度を吐き出します。

247
00:12:42,640 --> 00:12:47,520
コスト関数の勾配はさらに複雑な層になります。

248
00:12:47,520 --> 00:12:51,277
これは、これらすべての重みとバイアスに対してど

249
00:12:51,277 --> 00:12:55,034
のような調整がコ スト関数の値に最も速い変化を

250
00:12:55,034 --> 00:12:57,975
引き起こすかを示します。これは、ど

251
00:12:57,975 --> 00:13:01,733
の重みに対するどの変更が最も重要かを示している

252
00:13:01,733 --> 00:13:03,040
と解釈できます。

253
00:13:03,040 --> 00:13:05,840
では、ランダムな重みとバイアスを使用してネットワーク

254
00:13:05,840 --> 00:13:08,640
を初期化し、この 勾配降下プロセスに基づいてそれらを

255
00:13:08,640 --> 00:13:10,578
何度も調整すると、これまでに見たこ

256
00:13:10,578 --> 00:13:13,378
とのない画像で実際にどの程度のパフォーマンスが得られ

257
00:13:13,378 --> 00:13:14,240
るでしょうか?

258
00:13:14,240 --> 00:13:16,876
私がここで説明したものは、それぞれ 16

259
00:13:16,876 --> 00:13:20,642
個のニューロンからなる 2 つの隠れ層を備えており、主に美

260
00:13:20,642 --> 00:13:23,781
的理由から選択されていますが、悪くはなく、表示され

261
00:13:23,781 --> 00:13:26,920
る新しい画像の約 96% を正しく分類しています。

262
00:13:26,920 --> 00:13:31,701
そして正直に言うと、それが台無しにしているいくつか

263
00:13:31,701 --> 00:13:36,300
の例を見ると、少し緩めなければならないと感じます。

264
00:13:36,300 --> 00:13:38,877
隠れ層構造を試していくつかの調整を加えれば

265
00:13:38,877 --> 00:13:41,220
、これを最大 98% まで達成できます。

266
00:13:41,220 --> 00:13:42,900
それはとても良いことです！

267
00:13:42,900 --> 00:13:45,287
これは最高ではありません。この単純なバニラ

268
00:13:45,287 --> 00:13:47,783
ネットワークよりも洗練されれば、確かにパフォ

269
00:13:47,783 --> 00:13:50,930
ーマンスを向上させることはできますが、最初のタスクがどれほ

270
00:13:50,930 --> 00:13:54,077
ど困難であるかを考えると、こ れまで見たことのない画像でこ

271
00:13:54,077 --> 00:13:57,333
れほどうまく動作するネットワークには、信じられないほどの何

272
00:13:57,333 --> 00:14:00,480
かがあると思います。どのようなパターンを探すべきかを具体的

273
00:14:00,480 --> 00:14:02,000
に指示したことはありません。

274
00:14:02,000 --> 00:14:05,057
もともと、私がこの構造を動機付けた方法は、2

275
00:14:05,057 --> 00:14:07,583
番目の層が小さ なエッジを検出し、3

276
00:14:07,583 --> 00:14:10,242
番目の層がそれらのエッジをつなぎ合わせ

277
00:14:10,242 --> 00:14:13,433
てループや長い線を認識し、それらがつなぎ合わされ

278
00:14:13,433 --> 00:14:16,624
るかもしれな いという希望を説明することでした。

279
00:14:16,624 --> 00:14:18,220
一緒に数字を認識します。

280
00:14:18,220 --> 00:14:19,630
では、これは私たちのネットワークが実

281
00:14:19,630 --> 00:14:21,040
際に行っていることなのでしょうか?

282
00:14:21,040 --> 00:14:24,880
まあ、少なくともこれに関しては、まったくそうではありません。

283
00:14:24,880 --> 00:14:27,854
前回のビデオで、最初の層のすべてのニューロンから 2

284
00:14:27,854 --> 00:14:30,609
番目の層の特定のニュ ーロンへの接続の重みが、2

285
00:14:30,609 --> 00:14:33,473
番目の層のニューロンが認識している特定のピクセ ル

286
00:14:33,473 --> 00:14:36,558
パターンとしてどのように視覚化できるかを説明したことを覚

287
00:14:36,558 --> 00:14:37,440
えていますか?

288
00:14:37,440 --> 00:14:41,586
これらのトランジションに関連付けられたウェイトに

289
00:14:41,586 --> 00:14:45,733
対してこれを行う と、あちこちで孤立した小さなエ

290
00:14:45,733 --> 00:14:48,843
ッジが検出されるのではなく、ほぼラ

291
00:14:48,843 --> 00:14:52,990
ンダムに見え、中央に非常に緩いパターンがいくつか

292
00:14:52,990 --> 00:14:54,200
あるだけです。

293
00:14:54,200 --> 00:14:57,251
考えられる重みとバイアスの計り知れないほど広い

294
00:14:57,251 --> 00:15:00,303
13,000 次元空間において、私たちのネットワ

295
00:15:00,303 --> 00:15:03,355
ークは、ほとんどの画像を正 常に分類したにもかか

296
00:15:03,355 --> 00:15:06,025
わらず、私たちが期待していたパターンを正

297
00:15:06,025 --> 00:15:09,840
確に検出できない、幸せな小さな局所最小値を見つけたようです。

298
00:15:09,840 --> 00:15:12,220
この点をよく理解するには、ランダムな画像を

299
00:15:12,220 --> 00:15:14,600
入力したときに何が起こるかを見てください。

300
00:15:14,600 --> 00:15:16,527
システムが賢いものであれば、10

301
00:15:16,527 --> 00:15:19,703
個の出力ニューロンのどれも実際には活性化していないのか

302
00:15:19,703 --> 00:15:22,992
、あるいはそれらすべてを均等に活性化しているのか、不確かに

303
00:15:22,992 --> 00:15:26,281
感じられるか、あるいは、この ランダムなニューロンが確実で

304
00:15:26,281 --> 00:15:29,570
あるかのように、自信を持ってナンセンスな答えを与えることを

305
00:15:29,570 --> 00:15:32,405
期 待するかもしれません。5 の実際の画像が 5

306
00:15:32,405 --> 00:15:34,560
であるのと同様に、ノイズも 5 です。

307
00:15:34,560 --> 00:15:38,248
別の言い方をすると、このネットワークは数字をかなりう

308
00:15:38,248 --> 00:15:41,800
まく認識できても、それを描画する方法がわかりません。

309
00:15:41,800 --> 00:15:43,600
その多くは、トレーニングの設定が非

310
00:15:43,600 --> 00:15:45,400
常に厳しく制限されているためです。

311
00:15:45,400 --> 00:15:48,220
つまり、ここではネットワークの立場に立って考えてみましょう。

312
00:15:48,220 --> 00:15:51,669
その観点から見ると、宇宙全体は小さなグリッドの中

313
00:15:51,669 --> 00:15:55,118
心にある、明確に 定義された不動の数字だけで構成

314
00:15:55,118 --> 00:15:57,704
されており、そのコスト関数は、その

315
00:15:57,704 --> 00:16:01,154
決定に完全な自信を持つこと以外には何の動機も与え

316
00:16:01,154 --> 00:16:02,160
ませんでした。

317
00:16:02,160 --> 00:16:04,941
これが第 2 層のニューロンが実際に行っていることのイメー

318
00:16:04,941 --> 00:16:07,723
ジであるため、なぜエッジやパターンを検出するという動機でこ

319
00:16:07,723 --> 00:16:10,320
のネットワークを紹介するのか不思議に思うかもしれません。

320
00:16:10,320 --> 00:16:13,040
つまり、それは最終的にはまったくそうではありません。

321
00:16:13,040 --> 00:16:17,480
これは最終目標ではなく、出発点です。

322
00:16:17,480 --> 00:16:20,382
率直に言って、これは 80 年代から 90

323
00:16:20,382 --> 00:16:22,888
年代に研究された種類の古いテクノロジ

324
00:16:22,888 --> 00:16:26,319
ーであり、より詳細な現代の亜種を理解する前に、それを

325
00:16:26,319 --> 00:16:29,749
理解する必要があります。ま た、明らかにいくつかの興

326
00:16:29,749 --> 00:16:33,574
味深い問題を解決できる可能性がありますが、何を深く掘り下

327
00:16:33,574 --> 00:16:37,004
げるほど、これらの隠れ層が実際に機能しているほど、そ

328
00:16:37,004 --> 00:16:38,720
の層は知性が低く見えます。

329
00:16:38,720 --> 00:16:40,785
ネットワークがどのように学習するかということか

330
00:16:40,785 --> 00:16:43,029
ら、あなたがどのように学習するかに少し焦点を移し

331
00:16:43,029 --> 00:16:45,094
ますが、それは、何らかの方法でここで取り上げた

332
00:16:45,094 --> 00:16:47,160
資料に積極的に取り組んだ場合にのみ起こります。

333
00:16:47,160 --> 00:16:50,035
皆さんにしていただきたいのは、非常に簡単なことの

334
00:16:50,035 --> 00:16:52,105
1 つです。今ちょっと立ち止まって

335
00:16:52,105 --> 00:16:54,980
、このシステムにどのような変更を加えるか、エッジや

336
00:16:54,980 --> 00:16:57,050
パターンなどをよりよく認識できるよ

337
00:16:57,050 --> 00:16:59,925
うにしたい場合に画像をどのように認識するかについて

338
00:16:59,925 --> 00:17:01,880
、少しの間深く考えてみてください。

339
00:17:01,880 --> 00:17:04,457
しかしそれよりも、実際に内容に取り組むには、深層

340
00:17:04,457 --> 00:17:06,712
学習とニューラル ネット ワークに関する

341
00:17:06,712 --> 00:17:09,720
Michael Nielsen の本を強くお勧めします。

342
00:17:09,720 --> 00:17:12,885
この中には、まさにこの例をダウンロードして試

343
00:17:12,885 --> 00:17:16,050
すためのコードとデータ が含まれており、その

344
00:17:16,050 --> 00:17:19,360
コードが何をしているのかを段階的に説明します。

345
00:17:19,360 --> 00:17:22,219
素晴らしいのは、この本が無料で一般公開されているというこ

346
00:17:22,219 --> 00:17:25,078
とです。この本から何かを得る ことができましたら、私と一

347
00:17:25,078 --> 00:17:28,040
緒にニールセンの取り組みに寄付することを検討してください。

348
00:17:28,040 --> 00:17:32,369
また、Chris Ola による驚異的で美しいブログ投稿や

349
00:17:32,369 --> 00:17:35,833
Distill の記事など、私がとても気に入って

350
00:17:35,833 --> 00:17:38,720
いる他のリソースも説明にリンクしました。

351
00:17:38,720 --> 00:17:41,638
最後の数分間をここで締めくくるために、リーシャ・

352
00:17:41,638 --> 00:17:44,440
リーとのインタビューの抜粋に戻りたいと思います。

353
00:17:44,440 --> 00:17:46,525
前回のビデオで彼女を覚えているかもしれません。

354
00:17:46,525 --> 00:17:48,520
彼女は深層学習で博士号の研究をしていました。

355
00:17:48,520 --> 00:17:51,069
この短い抜粋では、最新の画像認識ネットワークの一

356
00:17:51,069 --> 00:17:53,937
部が実際にどのように学習し ているかを深く掘り下げた

357
00:17:53,937 --> 00:17:56,380
2 つの最近の論文について彼女が話しています。

358
00:17:56,380 --> 00:17:59,579
会話の状況を説明するために、最初の論文では、画像認識に優れ

359
00:17:59,579 --> 00:18:02,338
た特にディープ ニュ ーラル ネットワークの 1

360
00:18:02,338 --> 00:18:05,207
つを使用し、適切にラベル付けされたデータセットでト

361
00:18:05,207 --> 00:18:08,406
レーニングする代わりに、トレーニング前にすべてのラベルをシ

362
00:18:08,406 --> 00:18:09,400
ャッフルしました。

363
00:18:09,400 --> 00:18:12,415
すべてがランダムにラベル付けされているだけであるため

364
00:18:12,415 --> 00:18:15,320
、ここでのテストの精度は明らかにランダムと同等です。

365
00:18:15,320 --> 00:18:18,493
ただし、適切にラベル付けされたデータセットを使用した場

366
00:18:18,493 --> 00:18:21,440
合と同じトレーニング精度を達成することはできました。

367
00:18:21,440 --> 00:18:25,260
基本的に、この特定のネットワークの何百万もの重みは、ラン

368
00:18:25,260 --> 00:18:28,125
ダムなデータを記憶 するだけで十分でした。

369
00:18:28,125 --> 00:18:31,672
このため、このコスト関数の最小化が実際に画像内の何

370
00:18:31,672 --> 00:18:35,492
らかの構造に対応するのか、それとも単なる記憶なのかという

371
00:18:35,492 --> 00:18:36,720
疑問が生じます。。

372
00:18:36,720 --> 00:18:38,420
。。正しい分類が何であるかをデー

373
00:18:38,420 --> 00:18:40,120
タセット全体を記憶するためです。

374
00:18:40,120 --> 00:18:42,136
それで、半年後の今年の ICML

375
00:18:42,136 --> 00:18:44,271
では、正確には反論の論文はありませ

376
00:18:44,271 --> 00:18:47,237
んでしたが、実際には、これらのネットワークはそれよ

377
00:18:47,237 --> 00:18:50,203
りもう少し賢いこ とをしている、といったいくつかの

378
00:18:50,203 --> 00:18:52,220
側面を取り上げた論文がありました。

379
00:18:52,220 --> 00:18:56,560
その精度曲線を見ると、ランダムなデータセットでト

380
00:18:56,560 --> 00:19:00,900
レーニングしているだけだ とすると、その曲線は、

381
00:19:00,900 --> 00:19:05,240
ほぼ直線的に、非常にゆっくりと下降していきます。

382
00:19:05,240 --> 00:19:08,933
したがって、その精度を実現する適切な重みの可能

383
00:19:08,933 --> 00:19:12,320
な極小値を見つけるのに非常に苦労しています。

384
00:19:12,320 --> 00:19:16,131
一方、適切なラベルを持つ構造化データセットで実際にトレー

385
00:19:16,131 --> 00:19:19,811
ニングしている場合、最初は少しいじってみましたが、その

386
00:19:19,811 --> 00:19:23,360
精度レベルに達するまでに非常に早く落ちてしまいました。

387
00:19:23,360 --> 00:19:28,580
したがって、ある意味、極大値を見つけるのが簡単でした。

388
00:19:28,580 --> 00:19:32,576
そして、これに関して興味深いのは、実際に数年前に発行さ

389
00:19:32,576 --> 00:19:36,429
れた別の論文が明らかになったことであり、この論文では

390
00:19:36,429 --> 00:19:40,140
ネットワーク層についてさらに単純化が行われています。

391
00:19:40,140 --> 00:19:43,226
しかし、その結果の 1 つは、最適化の状況を見る

392
00:19:43,226 --> 00:19:46,313
と、これらのネットワーク が学習する傾向にある極

393
00:19:46,313 --> 00:19:49,400
小値が実際には同じ品質であることを示しています。

394
00:19:49,400 --> 00:19:51,801
したがって、ある意味、データセットが構造化されてい

395
00:19:51,801 --> 00:19:54,300
れば、それをより簡単に見つけることができるはずです。

396
00:19:54,300 --> 00:19:57,720
Patreon でサポートしてくださ

397
00:19:57,720 --> 00:20:01,140
っている皆様にいつも感謝しています。

398
00:20:01,140 --> 00:20:03,116
Patreon におけるゲームチェンジャーが

399
00:20:03,116 --> 00:20:05,093
何であるかについては以 前に述べましたが、こ

400
00:20:05,093 --> 00:20:07,160
れらのビデオは本当に皆さんなしでは不可能です。

401
00:20:07,160 --> 00:20:09,451
また、VC 会社 Amplify Partners

402
00:20:09,451 --> 00:20:11,477
と、シリーズの最 初のビデオに対する彼らのサポ

403
00:20:11,477 --> 00:20:13,240
ートにも特別な感謝を表したいと思います。

404
00:20:13,240 --> 00:20:33,140
ありがとう。

