1
00:00:00,000 --> 00:00:11,200
This is a 3. It's sloppily written and rendered at an extremely low resolution of 28x28 pixels,

2
00:00:11,200 --> 00:00:15,340
but your brain has no trouble recognizing it as a 3. And I want you to take a moment

3
00:00:15,340 --> 00:00:20,500
to appreciate how crazy it is that brains can do this so effortlessly. I mean, this,

4
00:00:20,500 --> 00:00:26,180
this and this are also recognizable as 3s, even though the specific values of each pixel

5
00:00:26,180 --> 00:00:31,260
is very different from one image to the next. The particular light-sensitive cells in your

6
00:00:31,260 --> 00:00:36,020
eye that are firing when you see this 3 are very different from the ones firing when you

7
00:00:36,020 --> 00:00:42,900
see this 3. But something in that crazy-smart visual cortex of yours resolves these as representing

8
00:00:42,900 --> 00:00:49,300
the same idea, while at the same time recognizing other images as their own distinct ideas.

9
00:00:49,300 --> 00:00:55,820
But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28

10
00:00:56,340 --> 00:01:01,780
and outputs a single number between 0 and 10, telling you what it thinks the digit is,

11
00:01:01,780 --> 00:01:07,860
well the task goes from comically trivial to dauntingly difficult. Unless you've been living

12
00:01:07,860 --> 00:01:12,020
under a rock, I think I hardly need to motivate the relevance and importance of machine learning

13
00:01:12,020 --> 00:01:16,460
and neural networks to the present and to the future. But what I want to do here is show you

14
00:01:16,460 --> 00:01:22,020
what a neural network actually is, assuming no background, and to help visualize what it's doing,

15
00:01:22,060 --> 00:01:26,860
not as a buzzword but as a piece of math. My hope is just that you come away feeling like

16
00:01:26,860 --> 00:01:31,460
the structure itself is motivated, and to feel like you know what it means when you read or

17
00:01:31,460 --> 00:01:36,780
you hear about a neural network quote-unquote learning. This video is just going to be devoted

18
00:01:36,780 --> 00:01:40,300
to the structure component of that, and the following one is going to tackle learning.

19
00:01:40,300 --> 00:01:45,580
What we're going to do is put together a neural network that can learn to recognize handwritten

20
00:01:45,580 --> 00:01:53,540
digits. This is a somewhat classic example for introducing the topic, and I'm happy to stick

21
00:01:53,540 --> 00:01:57,340
with the status quo here, because at the end of the two videos I want to point you to a couple

22
00:01:57,340 --> 00:02:01,420
good resources where you can learn more, and where you can download the code that does this

23
00:02:01,420 --> 00:02:07,820
and play with it on your own computer. There are many many variants of neural networks,

24
00:02:07,820 --> 00:02:12,900
and in recent years there's been sort of a boom in research towards these variants, but in these

25
00:02:12,940 --> 00:02:18,100
two introductory videos you and I are just going to look at the simplest plain vanilla form with

26
00:02:18,100 --> 00:02:23,020
no added frills. This is kind of a necessary prerequisite for understanding any of the more

27
00:02:23,020 --> 00:02:28,140
powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds

28
00:02:28,140 --> 00:02:33,440
around. But even in this simplest form it can learn to recognize handwritten digits, which is

29
00:02:33,440 --> 00:02:39,380
a pretty cool thing for a computer to be able to do. And at the same time you'll see how it does

30
00:02:39,460 --> 00:02:45,620
fall short of a couple hopes that we might have for it. As the name suggests, neural networks are

31
00:02:45,620 --> 00:02:50,820
inspired by the brain, but let's break that down. What are the neurons, and in what sense are they

32
00:02:50,820 --> 00:02:56,900
linked together? Right now when I say neuron, all I want you to think about is a thing that

33
00:02:56,900 --> 00:03:04,380
holds a number, specifically a number between 0 and 1. It's really not more than that. For example,

34
00:03:04,420 --> 00:03:10,060
the network starts with a bunch of neurons corresponding to each of the 28 times 28 pixels

35
00:03:10,060 --> 00:03:17,260
of the input image, which is 784 neurons in total. Each one of these holds a number that represents

36
00:03:17,260 --> 00:03:23,900
the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white

37
00:03:23,900 --> 00:03:30,060
pixels. This number inside the neuron is called its activation, and the image you might have in

38
00:03:30,060 --> 00:03:37,260
mind is that each neuron is lit up when its activation is a high number. So all of these

39
00:03:37,260 --> 00:03:47,820
784 neurons make up the first layer of our network. Now jumping over to the last layer,

40
00:03:47,820 --> 00:03:53,780
this has 10 neurons, each representing one of the digits. The activation in these neurons,

41
00:03:53,780 --> 00:03:59,460
again some number that's between 0 and 1, represents how much the system thinks that a

42
00:03:59,500 --> 00:04:05,180
given image corresponds with a given digit. There's also a couple layers in between called

43
00:04:05,180 --> 00:04:10,780
the hidden layers, which for the time being should just be a giant question mark for how on earth

44
00:04:10,780 --> 00:04:15,900
this process of recognizing digits is going to be handled. In this network I chose two hidden

45
00:04:15,900 --> 00:04:21,460
layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice. To be honest,

46
00:04:21,460 --> 00:04:26,620
I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that

47
00:04:26,620 --> 00:04:30,940
was just a nice number to fit on the screen. In practice there is a lot of room for experiment

48
00:04:30,940 --> 00:04:37,020
with a specific structure here. The way the network operates, activations in one layer determine the

49
00:04:37,020 --> 00:04:42,340
activations of the next layer. And of course the heart of the network as an information processing

50
00:04:42,340 --> 00:04:47,820
mechanism comes down to exactly how those activations from one layer bring about activations

51
00:04:47,820 --> 00:04:53,340
in the next layer. It's meant to be loosely analogous to how in biological networks of neurons

52
00:04:53,380 --> 00:04:59,380
some groups of neurons firing cause certain others to fire. Now the network I'm showing here has

53
00:04:59,380 --> 00:05:04,260
already been trained to recognize digits, and let me show you what I mean by that. It means if you

54
00:05:04,260 --> 00:05:10,900
feed in an image lighting up all 784 neurons of the input layer according to the brightness of

55
00:05:10,900 --> 00:05:16,860
each pixel in the image, that pattern of activations causes some very specific pattern in the next

56
00:05:16,860 --> 00:05:21,740
layer, which causes some pattern in the one after it, which finally gives some pattern in the output

57
00:05:21,780 --> 00:05:27,540
layer. And the brightest neuron of that output layer is the network's choice, so to speak,

58
00:05:27,540 --> 00:05:35,420
for what digit this image represents. And before jumping into the math for how one layer influences

59
00:05:35,420 --> 00:05:40,460
the next, or how training works, let's just talk about why it's even reasonable to expect a

60
00:05:40,460 --> 00:05:46,340
layered structure like this to behave intelligently. What are we expecting here? What is the best hope

61
00:05:46,420 --> 00:05:52,420
for what those middle layers might be doing? Well, when you or I recognize digits, we piece together

62
00:05:52,420 --> 00:05:58,980
various components. A 9 has a loop up top and a line on the right. An 8 also has a loop up top,

63
00:05:58,980 --> 00:06:05,420
but it's paired with another loop down low. A 4 basically breaks down into three specific lines,

64
00:06:05,420 --> 00:06:11,500
and things like that. Now in a perfect world, we might hope that each neuron in the second to last

65
00:06:11,740 --> 00:06:17,460
layer corresponds with one of these subcomponents, that anytime you feed in an image with, say,

66
00:06:17,460 --> 00:06:23,060
a loop up top, like a 9 or an 8, there's some specific neuron whose activation is going to be

67
00:06:23,060 --> 00:06:28,620
close to 1. And I don't mean this specific loop of pixels, the hope would be that any generally

68
00:06:28,620 --> 00:06:33,980
loopy pattern towards the top sets off this neuron. That way, going from the third layer

69
00:06:33,980 --> 00:06:39,380
to the last one just requires learning which combination of subcomponents corresponds to

70
00:06:39,380 --> 00:06:44,020
which digits. Of course, that just kicks the problem down the road, because how would you

71
00:06:44,020 --> 00:06:48,340
recognize these subcomponents, or even learn what the right subcomponents should be? And I still

72
00:06:48,340 --> 00:06:52,900
haven't even talked about how one layer influences the next, but run with me on this one for a moment.

73
00:06:52,900 --> 00:06:59,020
Recognizing a loop can also break down into subproblems. One reasonable way to do this would

74
00:06:59,020 --> 00:07:05,640
be to first recognize the various little edges that make it up. Similarly, a long line like the

75
00:07:05,640 --> 00:07:11,280
kind you might see in the digits 1 or 4 or 7, well that's really just a long edge, or maybe you

76
00:07:11,280 --> 00:07:18,440
think of it as a certain pattern of several smaller edges. So maybe our hope is that each neuron in the

77
00:07:18,440 --> 00:07:24,680
second layer of the network corresponds with the various relevant little edges. Maybe when an image

78
00:07:24,680 --> 00:07:30,760
like this one comes in, it lights up all of the neurons associated with around 8 to 10 specific

79
00:07:31,040 --> 00:07:36,480
little edges, which in turn lights up the neurons associated with the upper loop and a long vertical

80
00:07:36,480 --> 00:07:41,960
line, and those light up the neuron associated with a 9. Whether or not this is what our final

81
00:07:41,960 --> 00:07:46,560
network actually does is another question, one that I'll come back to once we see how to train

82
00:07:46,560 --> 00:07:51,800
the network. But this is a hope that we might have, a sort of goal with the layered structure

83
00:07:51,800 --> 00:07:57,440
like this. Moreover, you can imagine how being able to detect edges and patterns like this would

84
00:07:57,480 --> 00:08:02,440
be really useful for other image recognition tasks. And even beyond image recognition,

85
00:08:02,440 --> 00:08:06,640
there are all sorts of intelligent things you might want to do that break down into layers of

86
00:08:06,640 --> 00:08:12,640
abstraction. Parsing speech, for example, involves taking raw audio and picking out distinct sounds,

87
00:08:12,640 --> 00:08:17,760
which combine to make certain syllables, which combine to form words, which combine to make

88
00:08:17,760 --> 00:08:23,360
up phrases and more abstract thoughts, etc. But getting back to how any of this actually works,

89
00:08:23,400 --> 00:08:29,160
picture yourself right now designing how exactly the activations in one layer might determine the

90
00:08:29,160 --> 00:08:35,320
activations in the next. The goal is to have some mechanism that could conceivably combine pixels

91
00:08:35,320 --> 00:08:41,040
into edges, or edges into patterns, or patterns into digits. And to zoom in on one very specific

92
00:08:41,040 --> 00:08:47,440
example, let's say the hope is for one particular neuron in the second layer to pick up on whether

93
00:08:47,680 --> 00:08:54,440
the image has an edge in this region here. The question at hand is, what parameters should the

94
00:08:54,440 --> 00:09:00,440
network have? What dials and knobs should you be able to tweak so that it's expressive enough to

95
00:09:00,440 --> 00:09:05,880
potentially capture this pattern, or any other pixel pattern, or the pattern that several edges

96
00:09:05,880 --> 00:09:11,680
can make a loop, and other such things? Well, what we'll do is assign a weight to each one of the

97
00:09:11,680 --> 00:09:17,160
connections between our neuron and the neurons from the first layer. These weights are just

98
00:09:17,160 --> 00:09:23,960
numbers. Then take all of those activations from the first layer and compute their weighted sum

99
00:09:23,960 --> 00:09:30,400
according to these weights. I find it helpful to think of these weights as being organized into a

100
00:09:30,400 --> 00:09:35,200
little grid of their own, and I'm going to use green pixels to indicate positive weights, and

101
00:09:35,200 --> 00:09:40,760
red pixels to indicate negative weights, where the brightness of that pixel is some loose depiction

102
00:09:40,760 --> 00:09:45,880
of the weight's value. If we made the weights associated with almost all of the pixels zero,

103
00:09:45,880 --> 00:09:51,200
except for some positive weights in this region that we care about, then taking the weighted sum

104
00:09:51,200 --> 00:09:56,360
of all the pixel values really just amounts to adding up the values of the pixel just in the

105
00:09:56,360 --> 00:10:02,760
region that we care about. And if you really wanted to pick up on whether there's an edge here, what

106
00:10:02,760 --> 00:10:07,960
you might do is have some negative weights associated with the surrounding pixels. Then the

107
00:10:08,000 --> 00:10:12,680
sum is largest when those middle pixels are bright but the surrounding pixels are darker.

108
00:10:12,680 --> 00:10:19,200
When you compute a weighted sum like this, you might come out with any number, but for this

109
00:10:19,200 --> 00:10:25,200
network what we want is for activations to be some value between 0 and 1. So a common thing to do is

110
00:10:25,200 --> 00:10:30,560
to pump this weighted sum into some function that squishes the real number line into the range

111
00:10:30,560 --> 00:10:36,360
between 0 and 1. And a common function that does this is called the sigmoid function, also known as

112
00:10:36,360 --> 00:10:42,760
a logistic curve. Basically very negative inputs end up close to 0, very positive inputs end up

113
00:10:42,760 --> 00:10:51,400
close to 1, and it just steadily increases around the input 0. So the activation of the neuron here

114
00:10:51,400 --> 00:10:59,320
is basically a measure of how positive the relevant weighted sum is. But maybe it's not that you want

115
00:10:59,320 --> 00:11:04,080
the neuron to light up when the weighted sum is bigger than 0. Maybe you only want it to be active

116
00:11:04,120 --> 00:11:11,520
when the sum is bigger than say 10. That is, you want some bias for it to be inactive. What we'll

117
00:11:11,520 --> 00:11:17,560
do then is just add in some other number, like negative 10, to this weighted sum before plugging

118
00:11:17,560 --> 00:11:23,840
it through the sigmoid squishification function. That additional number is called the bias. So the

119
00:11:23,840 --> 00:11:29,080
weights tell you what pixel pattern this neuron in the second layer is picking up on, and the bias

120
00:11:29,120 --> 00:11:34,640
tells you how high the weighted sum needs to be before the neuron starts getting meaningfully

121
00:11:34,640 --> 00:11:41,760
active. And that is just one neuron. Every other neuron in this layer is going to be connected to

122
00:11:41,760 --> 00:11:49,080
all 784 pixel neurons from the first layer, and each one of those 784 connections has its own

123
00:11:49,080 --> 00:11:55,320
weight associated with it. Also, each one has some bias, some other number that you add on to the

124
00:11:55,320 --> 00:12:00,600
weighted sum before squishing it with the sigmoid. And that's a lot to think about! With this hidden

125
00:12:00,600 --> 00:12:09,280
layer of 16 neurons, that's a total of 784 times 16 weights, along with 16 biases. And all of that

126
00:12:09,280 --> 00:12:13,760
is just the connections from the first layer to the second. The connections between the other

127
00:12:13,760 --> 00:12:19,600
layers also have a bunch of weights and biases associated with them. All said and done, this

128
00:12:19,600 --> 00:12:26,680
network has almost exactly 13,000 total weights and biases. 13,000 knobs and dials that can be

129
00:12:26,680 --> 00:12:32,400
tweaked and turned to make this network behave in different ways. So when we talk about learning,

130
00:12:32,400 --> 00:12:38,440
what that's referring to is getting the computer to find a valid setting for all of these many many

131
00:12:38,440 --> 00:12:44,400
numbers so that it'll actually solve the problem at hand. One thought experiment that is at once

132
00:12:44,400 --> 00:12:49,440
fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases

133
00:12:49,440 --> 00:12:53,960
by hand, purposefully tweaking the numbers so that the second layer picks up on edges,

134
00:12:53,960 --> 00:12:59,680
the third layer picks up on patterns, etc. I personally find this satisfying rather than just

135
00:12:59,680 --> 00:13:04,400
treating the network as a total black box, because when the network doesn't perform the way you

136
00:13:04,400 --> 00:13:09,040
anticipate, if you've built up a little bit of a relationship with what those weights and biases

137
00:13:09,040 --> 00:13:13,440
actually mean, you have a starting place for experimenting with how to change the structure

138
00:13:13,440 --> 00:13:17,680
to improve. Or when the network does work, but not for the reasons you might expect,

139
00:13:17,680 --> 00:13:22,760
digging into what the weights and biases are doing is a good way to challenge your assumptions

140
00:13:22,760 --> 00:13:28,560
and really expose the full space of possible solutions. By the way, the actual function here

141
00:13:28,560 --> 00:13:34,840
is a little cumbersome to write down, don't you think? So let me show you a more notationally

142
00:13:34,840 --> 00:13:39,200
compact way that these connections are represented. This is how you'd see it if you choose to read

143
00:13:39,200 --> 00:13:45,360
up more about neural networks. Organize all of the activations from one layer into a column as

144
00:13:45,480 --> 00:13:53,400
a vector. Then organize all of the weights as a matrix, where each row of that matrix corresponds

145
00:13:53,400 --> 00:13:58,680
to the connections between one layer and a particular neuron in the next layer. What that

146
00:13:58,680 --> 00:14:03,360
means is that taking the weighted sum of the activations in the first layer according to these

147
00:14:03,360 --> 00:14:08,880
weights corresponds to one of the terms in the matrix vector product of everything we have on the

148
00:14:08,880 --> 00:14:17,840
left here. By the way, so much of machine learning just comes down to having a good grasp of linear

149
00:14:17,840 --> 00:14:23,000
algebra, so for any of you who want a nice visual understanding for matrices and what matrix vector

150
00:14:23,000 --> 00:14:29,320
multiplication means, take a look at the series I did on linear algebra, especially chapter 3. Back

151
00:14:29,320 --> 00:14:34,200
to our expression, instead of talking about adding the bias to each one of these values independently,

152
00:14:34,200 --> 00:14:40,440
we represent it by organizing all those biases into a vector, and adding the entire vector to

153
00:14:40,440 --> 00:14:47,240
the previous matrix vector product. Then as a final step, I'll wrap a sigmoid around the outside here,

154
00:14:47,240 --> 00:14:51,480
and what that's supposed to represent is that you're going to apply the sigmoid function to

155
00:14:51,480 --> 00:14:58,120
each specific component of the resulting vector inside. So once you write down this weight matrix

156
00:14:58,120 --> 00:15:03,320
and these vectors as their own symbols, you can communicate the full transition of activations

157
00:15:03,480 --> 00:15:08,840
from one layer to the next in an extremely tight and neat little expression, and this makes the

158
00:15:08,840 --> 00:15:14,600
relevant code both a lot simpler and a lot faster, since many libraries optimize the heck out of

159
00:15:14,600 --> 00:15:21,400
matrix multiplication. Remember how earlier I said these neurons are simply things that hold numbers?

160
00:15:22,120 --> 00:15:26,280
Well of course the specific numbers that they hold depends on the image you feed in,

161
00:15:28,120 --> 00:15:31,960
so it's actually more accurate to think of each neuron as a function,

162
00:15:31,960 --> 00:15:37,240
one that takes in the outputs of all the neurons in the previous layer, and spits out a number

163
00:15:37,240 --> 00:15:43,800
between 0 and 1. Really the entire network is just a function, one that takes in 784 numbers

164
00:15:43,800 --> 00:15:49,720
as an input and spits out 10 numbers as an output. It's an absurdly complicated function,

165
00:15:49,720 --> 00:15:54,520
one that involves 13,000 parameters in the forms of these weights and biases that pick up on

166
00:15:54,520 --> 00:15:59,000
certain patterns, and which involves iterating many matrix vector products and the sigmoid

167
00:15:59,000 --> 00:16:04,760
squishification function, but it's just a function nonetheless. And in a way it's kind

168
00:16:04,760 --> 00:16:09,720
of reassuring that it looks complicated. I mean if it were any simpler, what hope would we have

169
00:16:09,720 --> 00:16:14,920
that it could take on the challenge of recognizing digits? And how does it take on that challenge?

170
00:16:14,920 --> 00:16:19,320
How does this network learn the appropriate weights and biases just by looking at data?

171
00:16:19,880 --> 00:16:23,960
Well that's what I'll show in the next video, and I'll also dig a little more into what this

172
00:16:23,960 --> 00:16:29,880
particular network is really doing. Now is the point I suppose I should say subscribe to stay

173
00:16:29,880 --> 00:16:34,840
notified about when that video or any new videos come out, but realistically most of you don't

174
00:16:34,840 --> 00:16:39,880
actually receive notifications from YouTube, do you? Maybe more honestly I should say subscribe

175
00:16:39,880 --> 00:16:44,920
so that the neural networks that underlie YouTube's recommendation algorithm are primed to believe that

176
00:16:44,920 --> 00:16:49,800
you want to see content from this channel get recommended to you. Anyway, stay posted for more.

177
00:16:50,600 --> 00:16:54,840
Thank you very much to everyone supporting these videos on Patreon. I've been a little slow to

178
00:16:54,840 --> 00:16:59,160
progress in the probability series this summer, but I'm jumping back into it after this project,

179
00:16:59,160 --> 00:17:05,640
so patrons you can look out for updates there. To close things off here I have with me Leesha

180
00:17:05,640 --> 00:17:09,880
Lee who did her PhD work on the theoretical side of deep learning, and who currently works at a

181
00:17:09,880 --> 00:17:14,520
venture capital firm called Amplify Partners who kindly provided some of the funding for this video.

182
00:17:15,160 --> 00:17:19,480
So Leesha, one thing I think we should quickly bring up is this sigmoid function.

183
00:17:19,480 --> 00:17:23,400
As I understand it early networks use this to squish the relevant weighted sum into that

184
00:17:23,400 --> 00:17:28,200
interval between zero and one, you know kind of motivated by this biological analogy of neurons

185
00:17:28,200 --> 00:17:33,240
either being inactive or active. Exactly. But relatively few modern networks actually use

186
00:17:33,240 --> 00:17:37,800
sigmoid anymore. Yeah. It's kind of old school right? Yeah or rather relu seems to be much

187
00:17:37,800 --> 00:17:43,880
easier to train. And relu, relu stands for rectified linear unit? Yes it's this kind of

188
00:17:43,880 --> 00:17:50,280
function where you're just taking a max of zero and a where a is given by what you were explaining

189
00:17:50,280 --> 00:17:56,440
in the video. And what this was sort of motivated from I think was a partially by a biological

190
00:17:56,440 --> 00:18:03,640
analogy with how neurons would either be activated or not. And so if it passes a certain threshold

191
00:18:03,640 --> 00:18:09,080
it would be the identity function, but if it did not then it would just not be activated so it'd be

192
00:18:09,080 --> 00:18:13,640
zero. So it's kind of a simplification. Using sigmoids didn't help training or it was very

193
00:18:13,640 --> 00:18:21,320
difficult to train at some point and people just tried relu and it happened to work very well for

194
00:18:21,320 --> 00:18:26,120
these incredibly deep neural networks. All right, thank you Leesha.

195
00:18:39,080 --> 00:18:40,060
you

