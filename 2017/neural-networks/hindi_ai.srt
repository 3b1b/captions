1
00:00:00,000 --> 00:00:11,200
यह 3 है. इसे 28x28 पिक्सल के बेहद कम रिज़ॉल्यूशन पर लापरवाही से लिखा और प्रस्तुत किया गया है, लेकिन आपके मस्तिष्क

2
00:00:11,200 --> 00:00:15,340
को इसे 3 के रूप में पहचानने में कोई परेशानी नहीं होती है। और मैं चाहता हूं कि आप एक पल के

3
00:00:15,340 --> 00:00:20,500
लिए इसकी सराहना करें कि यह कितना अजीब है कि दिमाग इतनी आसानी से ऐसा कर सकता है। मेरा मतलब है, यह,

4
00:00:20,500 --> 00:00:26,180
यह और यह भी 3s के रूप में पहचाने जाने योग्य हैं, भले ही प्रत्येक पिक्सेल का विशिष्ट मान

5
00:00:26,180 --> 00:00:31,260
एक छवि से दूसरी छवि में बहुत भिन्न हो। जब आप इस 3 को देखते हैं तो आपकी आंख में

6
00:00:31,260 --> 00:00:36,020
विशेष प्रकाश-संवेदनशील कोशिकाएं सक्रिय हो जाती हैं, जब आप इस 3 को देखते हैं तो चमकती कोशिकाएं बहुत भिन्न

7
00:00:36,020 --> 00:00:42,900
होती हैं। लेकिन आपके उस पागल-स्मार्ट विज़ुअल कॉर्टेक्स में कुछ चीज़ इन्हें एक ही विचार का प्रतिनिधित्व करने के रूप

8
00:00:42,900 --> 00:00:49,300
में हल करती है, जबकि साथ ही अन्य छवियों को अपने स्वयं के विशिष्ट विचारों के रूप में पहचानती है।

9
00:00:49,300 --> 00:00:55,820
लेकिन अगर मैंने आपसे कहा, अरे, बैठ जाओ और मेरे लिए एक प्रोग्राम लिखो जो 28x28 की ग्रिड लेता है और

10
00:00:56,340 --> 00:01:01,780
0 और 10 के बीच एक एकल संख्या आउटपुट करता है, आपको बताता है कि वह अंक के बारे में क्या

11
00:01:01,780 --> 00:01:07,860
सोचता है, तो यह कार्य हास्यास्पद रूप से मामूली हो जाता है अत्यंत कठिन. जब तक आप किसी चट्टान के

12
00:01:07,860 --> 00:01:12,020
नीचे नहीं रह रहे हैं, मुझे लगता है कि मुझे वर्तमान और भविष्य के लिए मशीन लर्निंग और तंत्रिका नेटवर्क की

13
00:01:12,020 --> 00:01:16,460
प्रासंगिकता और महत्व को प्रेरित करने की शायद ही आवश्यकता है। लेकिन मैं यहां आपको यह दिखाना चाहता हूं कि तंत्रिका नेटवर्क

14
00:01:16,460 --> 00:01:22,020
वास्तव में क्या है, बिना किसी पृष्ठभूमि के, और यह कल्पना करने में मदद करना कि यह क्या कर रहा है, एक चर्चा शब्द के

15
00:01:22,060 --> 00:01:26,860
रूप में नहीं बल्कि गणित के एक टुकड़े के रूप में। मेरी आशा बस इतनी है कि आप यह महसूस करते हुए आएं

16
00:01:26,860 --> 00:01:31,460
कि संरचना स्वयं प्रेरित है, और जब आप तंत्रिका नेटवर्क उद्धरण-अनउद्धरण सीखने के बारे में पढ़ते हैं या सुनते हैं

17
00:01:31,460 --> 00:01:36,780
तो आपको ऐसा महसूस होता है कि इसका क्या मतलब है। यह वीडियो केवल इसके संरचना घटक

18
00:01:36,780 --> 00:01:40,300
के लिए समर्पित होगा, और अगला वीडियो सीखने से संबंधित होगा।

19
00:01:40,300 --> 00:01:45,580
हम जो करने जा रहे हैं वह एक तंत्रिका नेटवर्क को एक साथ रखना है जो हस्तलिखित अंकों को पहचानना सीख सकता

20
00:01:45,580 --> 00:01:53,540
है। विषय को प्रस्तुत करने के लिए यह कुछ हद तक उत्कृष्ट उदाहरण है, और मुझे यहां यथास्थिति पर बने रहने

21
00:01:53,540 --> 00:01:57,340
में खुशी हो रही है, क्योंकि दो वीडियो के अंत में मैं आपको कुछ अच्छे संसाधनों की ओर इंगित करना चाहता

22
00:01:57,340 --> 00:02:01,420
हूं जहां आप और अधिक सीख सकते हैं, और कहां आप ऐसा करने वाले कोड को डाउनलोड कर सकते हैं और

23
00:02:01,420 --> 00:02:07,820
अपने कंप्यूटर पर उसके साथ खेल सकते हैं। तंत्रिका नेटवर्क के कई प्रकार हैं, और हाल के वर्षों

24
00:02:07,820 --> 00:02:12,900
में इन प्रकारों के प्रति अनुसंधान में तेजी आई है, लेकिन इन दो परिचयात्मक वीडियो में

25
00:02:12,940 --> 00:02:18,100
आप और मैं बिना किसी अतिरिक्त तामझाम के सबसे सरल सादे वेनिला रूप को देखने जा

26
00:02:18,100 --> 00:02:23,020
रहे हैं। किसी भी अधिक शक्तिशाली आधुनिक संस्करण को समझने के लिए यह एक आवश्यक शर्त है,

27
00:02:23,020 --> 00:02:28,140
और मेरा विश्वास करें कि इसमें अभी भी हमारे दिमाग को समझने के लिए काफी जटिलताएं

28
00:02:28,140 --> 00:02:33,440
हैं। लेकिन इस सरलतम रूप में भी यह हस्तलिखित अंकों को पहचानना सीख सकता है, जो

29
00:02:33,440 --> 00:02:39,380
एक कंप्यूटर के लिए बहुत अच्छी बात है। और साथ ही आप देखेंगे कि कैसे यह कुछ उम्मीदों

30
00:02:39,460 --> 00:02:45,620
से कम हो जाता है जो हमारी इसके लिए हो सकती हैं। जैसा कि नाम से पता चलता है, तंत्रिका नेटवर्क

31
00:02:45,620 --> 00:02:50,820
मस्तिष्क से प्रेरित होते हैं, लेकिन आइए इसे तोड़ दें। न्यूरॉन्स क्या हैं, और वे किस अर्थ में एक साथ

32
00:02:50,820 --> 00:02:56,900
जुड़े हुए हैं? अभी जब मैं न्यूरॉन कहता हूं, तो मैं चाहता हूं कि आप एक ऐसी चीज के बारे में सोचें जो एक संख्या

33
00:02:56,900 --> 00:03:04,380
रखती है, विशेष रूप से 0 और 1 के बीच की एक संख्या। यह वास्तव में उससे अधिक नहीं है। उदाहरण के लिए,

34
00:03:04,420 --> 00:03:10,060
नेटवर्क इनपुट छवि के 28 गुना 28 पिक्सेल में से प्रत्येक के अनुरूप न्यूरॉन्स के एक समूह के साथ

35
00:03:10,060 --> 00:03:17,260
शुरू होता है, जो कुल मिलाकर 784 न्यूरॉन्स है। इनमें से प्रत्येक में एक संख्या होती है जो संबंधित पिक्सेल

36
00:03:17,260 --> 00:03:23,900
के ग्रेस्केल मान को दर्शाती है, जो काले पिक्सेल के लिए 0 से लेकर सफेद पिक्सेल के लिए 1 तक होती

37
00:03:23,900 --> 00:03:30,060
है। न्यूरॉन के अंदर की इस संख्या को इसकी सक्रियता कहा जाता है, और आपके मन में यह छवि हो सकती

38
00:03:30,060 --> 00:03:37,260
है कि प्रत्येक न्यूरॉन तब प्रकाशित होता है जब इसकी सक्रियता एक उच्च संख्या होती है। तो ये सभी

39
00:03:37,260 --> 00:03:47,820
784 न्यूरॉन्स हमारे नेटवर्क की पहली परत बनाते हैं। अब अंतिम परत पर जाएं, इसमें 10 न्यूरॉन

40
00:03:47,820 --> 00:03:53,780
हैं, जिनमें से प्रत्येक एक अंक का प्रतिनिधित्व करता है। इन न्यूरॉन्स में सक्रियता, फिर से कुछ संख्या

41
00:03:53,780 --> 00:03:59,460
जो 0 और 1 के बीच है, यह दर्शाती है कि सिस्टम कितना सोचता है कि दी गई छवि

42
00:03:59,500 --> 00:04:05,180
किसी दिए गए अंक से मेल खाती है। बीच में कुछ परतें भी होती हैं जिन्हें छिपी

43
00:04:05,180 --> 00:04:10,780
हुई परतें कहा जाता है, जो फिलहाल एक बड़ा प्रश्नचिह्न होना चाहिए कि पृथ्वी पर अंकों

44
00:04:10,780 --> 00:04:15,900
को पहचानने की इस प्रक्रिया को कैसे संभाला जाएगा। इस नेटवर्क में मैंने दो छिपी हुई परतों

45
00:04:15,900 --> 00:04:21,460
को चुना, प्रत्येक में 16 न्यूरॉन्स थे, और माना कि यह एक मनमाना विकल्प है। ईमानदारी से कहूं तो,

46
00:04:21,460 --> 00:04:26,620
मैंने इस आधार पर दो परतें चुनीं कि मैं संरचना को एक पल में कैसे प्रेरित करना चाहता हूं, और 16, खैर यह

47
00:04:26,620 --> 00:04:30,940
स्क्रीन पर फिट होने के लिए एक अच्छी संख्या थी। व्यवहार में यहां एक विशिष्ट संरचना के साथ प्रयोग

48
00:04:30,940 --> 00:04:37,020
के लिए बहुत जगह है। जिस तरह से नेटवर्क संचालित होता है, एक परत में सक्रियता अगली परत

49
00:04:37,020 --> 00:04:42,340
की सक्रियता निर्धारित करती है। और निश्चित रूप से एक सूचना प्रसंस्करण तंत्र के रूप में नेटवर्क का

50
00:04:42,340 --> 00:04:47,820
मूल वास्तव में इस बात पर निर्भर करता है कि कैसे एक परत से सक्रियता अगली परत में

51
00:04:47,820 --> 00:04:53,340
सक्रियता लाती है। इसका मतलब यह है कि न्यूरॉन्स के जैविक नेटवर्क में न्यूरॉन्स के कुछ

52
00:04:53,380 --> 00:04:59,380
समूह किस तरह से दूसरों को सक्रिय करते हैं। अब जो नेटवर्क मैं यहां दिखा रहा हूं उसे पहले से

53
00:04:59,380 --> 00:05:04,260
ही अंकों को पहचानने के लिए प्रशिक्षित किया गया है, और मैं आपको दिखाता हूं कि इससे मेरा क्या मतलब है। इसका मतलब है कि

54
00:05:04,260 --> 00:05:10,900
यदि आप छवि में प्रत्येक पिक्सेल की चमक के अनुसार इनपुट परत के सभी 784 न्यूरॉन्स को रोशन करते हुए एक

55
00:05:10,900 --> 00:05:16,860
छवि में फ़ीड करते हैं, तो सक्रियण का पैटर्न अगली परत में कुछ बहुत विशिष्ट पैटर्न का कारण बनता है, जो

56
00:05:16,860 --> 00:05:21,740
एक के बाद एक में कुछ पैटर्न का कारण बनता है यह, जो अंततः आउटपुट लेयर में कुछ पैटर्न देता

57
00:05:21,780 --> 00:05:27,540
है। और उस आउटपुट परत का सबसे चमकीला न्यूरॉन नेटवर्क की पसंद है, इसलिए बोलने के लिए, यह छवि

58
00:05:27,540 --> 00:05:35,420
किस अंक का प्रतिनिधित्व करती है। और गणित में कूदने से पहले कि एक परत अगली परत को कैसे प्रभावित

59
00:05:35,420 --> 00:05:40,460
करती है, या प्रशिक्षण कैसे काम करता है, आइए बस इस बारे में बात करें कि इस तरह की स्तरित संरचना से

60
00:05:40,460 --> 00:05:46,340
बुद्धिमानी से व्यवहार करने की अपेक्षा करना क्यों उचित है। हम यहाँ क्या उम्मीद कर रहे हैं? वे मध्य परतें क्या कर

61
00:05:46,420 --> 00:05:52,420
रही होंगी, इसके लिए सबसे अच्छी आशा क्या है? खैर, जब आप या मैं अंकों को पहचानते हैं, तो हम विभिन्न घटकों को

62
00:05:52,420 --> 00:05:58,980
एक साथ जोड़ते हैं। 9 में ऊपर एक लूप और दाहिनी ओर एक रेखा होती है। 8 में ऊपर की ओर एक लूप भी होता है,

63
00:05:58,980 --> 00:06:05,420
लेकिन इसे नीचे की ओर एक अन्य लूप के साथ जोड़ा जाता है। A 4 मूलतः तीन विशिष्ट रेखाओं और इसी तरह की चीज़ों

64
00:06:05,420 --> 00:06:11,500
में टूट जाता है। अब एक आदर्श दुनिया में, हम उम्मीद कर सकते हैं कि दूसरी से आखिरी परत में प्रत्येक

65
00:06:11,740 --> 00:06:17,460
न्यूरॉन इन उप-घटकों में से एक से मेल खाता है, जब भी आप एक छवि में फ़ीड करते हैं, उदाहरण के लिए,

66
00:06:17,460 --> 00:06:23,060
शीर्ष पर एक लूप, जैसे 9 या 8, तो वहां कुछ होता है विशिष्ट न्यूरॉन जिसकी सक्रियता 1 के करीब होने

67
00:06:23,060 --> 00:06:28,620
वाली है। और मेरा मतलब पिक्सल के इस विशिष्ट लूप से नहीं है, आशा यह होगी कि शीर्ष की ओर कोई भी

68
00:06:28,620 --> 00:06:33,980
आम तौर पर लूपी पैटर्न इस न्यूरॉन को बंद कर देता है। इस तरह, तीसरी परत से अंतिम परत तक

69
00:06:33,980 --> 00:06:39,380
जाने के लिए बस यह सीखने की आवश्यकता है कि उपघटकों का कौन सा संयोजन किन अंकों से मेल

70
00:06:39,380 --> 00:06:44,020
खाता है। बेशक, इससे समस्या ख़त्म हो जाती है, क्योंकि आप इन उप-घटकों को

71
00:06:44,020 --> 00:06:48,340
कैसे पहचानेंगे, या यह भी सीखेंगे कि सही उप-घटक क्या होने चाहिए? और मैंने अभी तक

72
00:06:48,340 --> 00:06:52,900
इस बारे में बात भी नहीं की है कि एक परत दूसरी परत को कैसे प्रभावित करती है, लेकिन एक पल के लिए इस पर मेरे साथ चलें।

73
00:06:52,900 --> 00:06:59,020
लूप को पहचानने से उप-समस्याएँ भी विभाजित हो सकती हैं। ऐसा करने का एक उचित तरीका यह होगा

74
00:06:59,020 --> 00:07:05,640
कि पहले इसे बनाने वाले विभिन्न छोटे किनारों को पहचान लिया जाए। इसी प्रकार, एक लंबी रेखा जैसी कि आप

75
00:07:05,640 --> 00:07:11,280
अंक 1 या 4 या 7 में देख सकते हैं, वास्तव में यह सिर्फ एक लंबा किनारा है, या शायद आप इसे

76
00:07:11,280 --> 00:07:18,440
कई छोटे किनारों के एक निश्चित पैटर्न के रूप में सोचते हैं। तो शायद हमारी आशा यह है कि नेटवर्क

77
00:07:18,440 --> 00:07:24,680
की दूसरी परत में प्रत्येक न्यूरॉन विभिन्न प्रासंगिक छोटे किनारों से मेल खाता है। हो सकता है कि जब

78
00:07:24,680 --> 00:07:30,760
इस तरह की कोई छवि आती है, तो यह लगभग 8 से 10 विशिष्ट छोटे किनारों से जुड़े सभी

79
00:07:31,040 --> 00:07:36,480
न्यूरॉन्स को रोशन करती है, जो बदले में ऊपरी लूप और एक लंबी ऊर्ध्वाधर रेखा से जुड़े न्यूरॉन्स को रोशन

80
00:07:36,480 --> 00:07:41,960
करती है, और वे प्रकाश डालते हैं 9 से संबद्ध न्यूरॉन। हमारा अंतिम नेटवर्क वास्तव में ऐसा करता है या

81
00:07:41,960 --> 00:07:46,560
नहीं, यह एक और सवाल है, जिस पर मैं एक बार फिर विचार करूंगा जब हम देखेंगे कि नेटवर्क को कैसे प्रशिक्षित

82
00:07:46,560 --> 00:07:51,800
किया जाए। लेकिन यह एक आशा है कि हमारे पास इस तरह की स्तरित संरचना के साथ एक प्रकार का लक्ष्य हो

83
00:07:51,800 --> 00:07:57,440
सकता है। इसके अलावा, आप कल्पना कर सकते हैं कि इस तरह किनारों और पैटर्न का पता लगाने में सक्षम होना

84
00:07:57,480 --> 00:08:02,440
अन्य छवि पहचान कार्यों के लिए वास्तव में कैसे उपयोगी होगा। और छवि पहचान से परे भी, सभी

85
00:08:02,440 --> 00:08:06,640
प्रकार की बुद्धिमान चीजें हैं जो आप करना चाहते हैं जो अमूर्तता की परतों में टूट जाती

86
00:08:06,640 --> 00:08:12,640
हैं। उदाहरण के लिए, भाषण को पार्स करने में कच्चा ऑडियो लेना और अलग-अलग ध्वनियों को

87
00:08:12,640 --> 00:08:17,760
चुनना शामिल है, जो मिलकर कुछ शब्दांश बनाते हैं, जो मिलकर शब्द बनाते हैं, जो मिलकर वाक्यांश

88
00:08:17,760 --> 00:08:23,360
और अधिक अमूर्त विचार बनाते हैं, आदि। लेकिन इनमें से कोई भी वास्तव में कैसे काम करता है, इस

89
00:08:23,400 --> 00:08:29,160
पर वापस लौटते हुए, अभी स्वयं कल्पना करें कि एक परत में सक्रियता वास्तव में अगली परत में सक्रियता कैसे

90
00:08:29,160 --> 00:08:35,320
निर्धारित कर सकती है। लक्ष्य कुछ ऐसा तंत्र बनाना है जो पिक्सेल को किनारों में, या किनारों

91
00:08:35,320 --> 00:08:41,040
को पैटर्न में, या पैटर्न को अंकों में जोड़ सके। और एक बहुत ही विशिष्ट उदाहरण पर ज़ूम

92
00:08:41,040 --> 00:08:47,440
करने के लिए, मान लें कि दूसरी परत में एक विशेष न्यूरॉन से यह पता लगाने की उम्मीद है कि छवि

93
00:08:47,680 --> 00:08:54,440
का इस क्षेत्र में कोई किनारा है या नहीं। सवाल यह है कि नेटवर्क में कौन से पैरामीटर

94
00:08:54,440 --> 00:09:00,440
होने चाहिए? आपको किस डायल और नॉब को बदलने में सक्षम होना चाहिए ताकि यह इस पैटर्न, या किसी

95
00:09:00,440 --> 00:09:05,880
अन्य पिक्सेल पैटर्न, या पैटर्न को कैप्चर करने के लिए पर्याप्त रूप से अभिव्यंजक हो, जिससे कई किनारे एक

96
00:09:05,880 --> 00:09:11,680
लूप बना सकें, और ऐसी अन्य चीजें? खैर, हम जो करेंगे वह हमारे न्यूरॉन और पहली परत

97
00:09:11,680 --> 00:09:17,160
के न्यूरॉन्स के बीच प्रत्येक कनेक्शन को एक भार प्रदान करेंगे। ये वज़न महज़ संख्याएँ

98
00:09:17,160 --> 00:09:23,960
हैं। फिर उन सभी सक्रियताओं को पहली परत से लें और इन भारों के अनुसार उनके भारित

99
00:09:23,960 --> 00:09:30,400
योग की गणना करें। मुझे इन वजनों को अपने स्वयं के एक छोटे ग्रिड में व्यवस्थित करने के बारे में सोचने

100
00:09:30,400 --> 00:09:35,200
में मदद मिलती है, और मैं सकारात्मक वजन को इंगित करने के लिए हरे पिक्सल का उपयोग करने जा रहा हूं, और नकारात्मक

101
00:09:35,200 --> 00:09:40,760
वजन को इंगित करने के लिए लाल पिक्सल का उपयोग करने जा रहा हूं, जहां उस पिक्सेल की चमक कुछ है वजन

102
00:09:40,760 --> 00:09:45,880
के मूल्य का ढीला चित्रण. यदि हमने इस क्षेत्र में कुछ सकारात्मक भारों को छोड़कर, जिनकी हम परवाह

103
00:09:45,880 --> 00:09:51,200
करते हैं, लगभग सभी पिक्सेल से जुड़े भार को शून्य कर दिया है, तो सभी पिक्सेल मानों

104
00:09:51,200 --> 00:09:56,360
का भारित योग लेना वास्तव में केवल पिक्सेल के मानों को जोड़ने के बराबर है वह क्षेत्र

105
00:09:56,360 --> 00:10:02,760
जिसकी हमें परवाह है। और यदि आप वास्तव में यह जानना चाहते हैं कि क्या यहां कोई बढ़त

106
00:10:02,760 --> 00:10:07,960
है, तो आप आसपास के पिक्सेल से जुड़े कुछ नकारात्मक भार को समझ सकते हैं। तब योग

107
00:10:08,000 --> 00:10:12,680
सबसे बड़ा होता है जब वे मध्य पिक्सेल चमकीले होते हैं लेकिन आसपास के पिक्सेल गहरे होते हैं।

108
00:10:12,680 --> 00:10:19,200
जब आप इस तरह एक भारित राशि की गणना करते हैं, तो आप किसी भी संख्या के साथ आ सकते हैं, लेकिन इस

109
00:10:19,200 --> 00:10:25,200
नेटवर्क के लिए हम चाहते हैं कि सक्रियण 0 और 1 के बीच कुछ मान हो। तो एक सामान्य बात यह है कि

110
00:10:25,200 --> 00:10:30,560
इस भारित योग को किसी फ़ंक्शन में पंप किया जाए जो वास्तविक संख्या रेखा को 0 और 1 के बीच की सीमा में

111
00:10:30,560 --> 00:10:36,360
दबा देता है। और ऐसा करने वाला एक सामान्य कार्य सिग्मॉइड फ़ंक्शन कहलाता है, जिसे लॉजिस्टिक वक्र के रूप में भी

112
00:10:36,360 --> 00:10:42,760
जाना जाता है। मूल रूप से बहुत नकारात्मक इनपुट 0 के करीब समाप्त होते हैं, बहुत सकारात्मक इनपुट 1 के करीब

113
00:10:42,760 --> 00:10:51,400
समाप्त होते हैं, और यह इनपुट 0 के आसपास लगातार बढ़ता जाता है। तो यहां न्यूरॉन की सक्रियता मूल रूप से

114
00:10:51,400 --> 00:10:59,320
इस बात का माप है कि प्रासंगिक भारित योग कितना सकारात्मक है। लेकिन शायद ऐसा नहीं है कि आप चाहते

115
00:10:59,320 --> 00:11:04,080
हैं कि जब भारित योग 0 से बड़ा हो तो न्यूरॉन प्रकाशमान हो। हो सकता है कि आप इसे केवल तभी सक्रिय करना

116
00:11:04,120 --> 00:11:11,520
चाहते हों जब योग 10 से बड़ा हो। यानी, आप इसके निष्क्रिय होने के लिए कुछ पूर्वाग्रह चाहते हैं। इसके बाद

117
00:11:11,520 --> 00:11:17,560
हम इस भारित योग को सिग्मॉइड स्क्विशिफिकेशन फ़ंक्शन के माध्यम से प्लग करने से पहले इसमें कोई

118
00:11:17,560 --> 00:11:23,840
अन्य संख्या, जैसे ऋणात्मक 10, जोड़ देंगे। उस अतिरिक्त संख्या को पूर्वाग्रह कहा जाता है। तो वज़न

119
00:11:23,840 --> 00:11:29,080
आपको बताता है कि दूसरी परत में यह न्यूरॉन किस पिक्सेल पैटर्न को उठा रहा है, और पूर्वाग्रह

120
00:11:29,120 --> 00:11:34,640
आपको बताता है कि न्यूरॉन के सार्थक रूप से सक्रिय होने से पहले भारित योग कितना अधिक होना

121
00:11:34,640 --> 00:11:41,760
चाहिए। और वह सिर्फ एक न्यूरॉन है. इस परत का हर दूसरा न्यूरॉन पहली परत के सभी

122
00:11:41,760 --> 00:11:49,080
784 पिक्सेल न्यूरॉन्स से जुड़ा होगा, और उन 784 कनेक्शनों में से प्रत्येक का अपना वजन

123
00:11:49,080 --> 00:11:55,320
जुड़ा हुआ है। इसके अलावा, प्रत्येक में कुछ पूर्वाग्रह होते हैं, कुछ अन्य संख्याएं जिन्हें आप सिग्मॉइड के

124
00:11:55,320 --> 00:12:00,600
साथ कुचलने से पहले भारित राशि में जोड़ते हैं। और यह बहुत सोचने वाली बात है! 16 न्यूरॉन्स की इस

125
00:12:00,600 --> 00:12:09,280
छिपी हुई परत के साथ, 16 पूर्वाग्रहों के साथ, यह कुल 784 गुना 16 भार है। और यह सब

126
00:12:09,280 --> 00:12:13,760
पहली परत से दूसरी परत तक का कनेक्शन मात्र है। अन्य परतों के बीच के संबंधों

127
00:12:13,760 --> 00:12:19,600
में बहुत सारे वजन और पूर्वाग्रह भी जुड़े हुए हैं। सब कुछ कहा और किया

128
00:12:19,600 --> 00:12:26,680
गया, इस नेटवर्क में लगभग 13,000 कुल भार और पूर्वाग्रह हैं। 13,000 नॉब और डायल जिन्हें इस नेटवर्क को

129
00:12:26,680 --> 00:12:32,400
अलग-अलग तरीकों से संचालित करने के लिए बदला और घुमाया जा सकता है। इसलिए जब हम सीखने के बारे

130
00:12:32,400 --> 00:12:38,440
में बात करते हैं, तो इसका मतलब कंप्यूटर को इन सभी संख्याओं के लिए एक वैध सेटिंग ढूंढना

131
00:12:38,440 --> 00:12:44,400
है ताकि वह वास्तव में समस्या का समाधान कर सके। एक विचार प्रयोग जो मजेदार भी है

132
00:12:44,400 --> 00:12:49,440
और भयावह भी, वह है बैठकर इन सभी वजनों और पूर्वाग्रहों को हाथ से सेट

133
00:12:49,440 --> 00:12:53,960
करने की कल्पना करना, जानबूझकर संख्याओं को बदलना ताकि दूसरी परत किनारों को पकड़ ले, तीसरी

134
00:12:53,960 --> 00:12:59,680
परत पैटर्न को पहचान ले, वगैरह। मैं व्यक्तिगत रूप से नेटवर्क को संपूर्ण ब्लैक बॉक्स के रूप

135
00:12:59,680 --> 00:13:04,400
में मानने के बजाय इसे संतोषजनक मानता हूं, क्योंकि जब नेटवर्क आपके अनुमान के अनुसार प्रदर्शन नहीं करता है, तो

136
00:13:04,400 --> 00:13:09,040
यदि आपने उन भारों और पूर्वाग्रहों का वास्तव में क्या मतलब है, इसके साथ थोड़ा सा संबंध बना लिया है

137
00:13:09,040 --> 00:13:13,440
, आपके पास यह प्रयोग करने के लिए एक प्रारंभिक स्थान है कि सुधार के लिए संरचना को कैसे

138
00:13:13,440 --> 00:13:17,680
बदला जाए। या जब नेटवर्क काम करता है, लेकिन उन कारणों के लिए नहीं जिनकी आप उम्मीद कर सकते हैं,

139
00:13:17,680 --> 00:13:22,760
तो वज़न और पूर्वाग्रह क्या कर रहे हैं, इसकी खोज करना आपकी धारणाओं को चुनौती देने और वास्तव में संभावित समाधानों

140
00:13:22,760 --> 00:13:28,560
की पूरी गुंजाइश को उजागर करने का एक अच्छा तरीका है। वैसे, यहाँ वास्तविक कार्य को लिखना

141
00:13:28,560 --> 00:13:34,840
थोड़ा बोझिल है, क्या आपको नहीं लगता? तो आइए मैं आपको एक अधिक सांकेतिक रूप से संक्षिप्त

142
00:13:34,840 --> 00:13:39,200
तरीका दिखाता हूं जिससे इन कनेक्शनों का प्रतिनिधित्व किया जाता है। यदि आप तंत्रिका नेटवर्क के बारे में अधिक पढ़ना चुनते हैं

143
00:13:39,200 --> 00:13:45,360
तो आप इसे इसी तरह देखेंगे। सभी सक्रियणों को एक परत से एक वेक्टर के रूप में एक कॉलम में व्यवस्थित

144
00:13:45,480 --> 00:13:53,400
करें। फिर सभी भारों को एक मैट्रिक्स के रूप में व्यवस्थित करें, जहां उस मैट्रिक्स की प्रत्येक पंक्ति

145
00:13:53,400 --> 00:13:58,680
एक परत और अगली परत में एक विशेष न्यूरॉन के बीच कनेक्शन से मेल खाती है। इसका मतलब

146
00:13:58,680 --> 00:14:03,360
यह है कि इन भारों के अनुसार पहली परत में सक्रियणों का भारित योग लेना हमारे यहां

147
00:14:03,360 --> 00:14:08,880
बाईं ओर मौजूद हर चीज के मैट्रिक्स वेक्टर उत्पाद में से एक शब्द से मेल खाता

148
00:14:08,880 --> 00:14:17,840
है। वैसे, मशीन सीखने का इतना सारा काम सिर्फ रैखिक बीजगणित की अच्छी समझ के कारण होता है, इसलिए

149
00:14:17,840 --> 00:14:23,000
आप में से जो लोग मैट्रिक्स के लिए एक अच्छी दृश्य समझ चाहते हैं और मैट्रिक्स वेक्टर गुणन का

150
00:14:23,000 --> 00:14:29,320
क्या मतलब है, मेरे द्वारा की गई श्रृंखला पर एक नज़र डालें रैखिक बीजगणित, विशेषकर अध्याय 3। अपनी अभिव्यक्ति

151
00:14:29,320 --> 00:14:34,200
पर वापस जाएं, इन मूल्यों में से प्रत्येक में पूर्वाग्रह को स्वतंत्र रूप से जोड़ने के बारे में बात करने

152
00:14:34,200 --> 00:14:40,440
के बजाय, हम उन सभी पूर्वाग्रहों को एक वेक्टर में व्यवस्थित करके और पूरे वेक्टर को पिछले मैट्रिक्स वेक्टर उत्पाद

153
00:14:40,440 --> 00:14:47,240
में जोड़कर इसका प्रतिनिधित्व करते हैं। फिर अंतिम चरण के रूप में, मैं यहां बाहर के चारों ओर एक

154
00:14:47,240 --> 00:14:51,480
सिग्मॉइड लपेटूंगा, और जो प्रतिनिधित्व करने वाला है वह यह है कि आप अंदर परिणामी वेक्टर के प्रत्येक

155
00:14:51,480 --> 00:14:58,120
विशिष्ट घटक पर सिग्मॉइड फ़ंक्शन लागू करने जा रहे हैं। इसलिए एक बार जब आप इस वेट मैट्रिक्स और

156
00:14:58,120 --> 00:15:03,320
इन वैक्टरों को अपने स्वयं के प्रतीकों के रूप में लिख लेते हैं, तो आप एक परत से दूसरी

157
00:15:03,480 --> 00:15:08,840
परत तक सक्रियताओं के पूर्ण संक्रमण को बेहद चुस्त और साफ-सुथरी छोटी अभिव्यक्ति में संप्रेषित कर सकते हैं, और

158
00:15:08,840 --> 00:15:14,600
यह प्रासंगिक कोड को बहुत सरल और सरल बना देता है। बहुत तेज़, क्योंकि कई लाइब्रेरी मैट्रिक्स गुणन

159
00:15:14,600 --> 00:15:21,400
को अनुकूलित करती हैं। याद रखें कि मैंने पहले कैसे कहा था कि ये न्यूरॉन केवल ऐसी चीज़ें हैं जिनमें संख्याएँ होती हैं?

160
00:15:22,120 --> 00:15:26,280
बेशक, उनके पास मौजूद विशिष्ट संख्याएं आपके द्वारा फीड की गई छवि पर निर्भर करती हैं, इसलिए प्रत्येक

161
00:15:28,120 --> 00:15:31,960
न्यूरॉन को एक फ़ंक्शन के रूप में सोचना वास्तव में अधिक सटीक है, एक जो पिछली परत

162
00:15:31,960 --> 00:15:37,240
के सभी न्यूरॉन्स के आउटपुट लेता है, और एक को बाहर निकालता है 0 और 1 के बीच

163
00:15:37,240 --> 00:15:43,800
की संख्या. वास्तव में पूरा नेटवर्क सिर्फ एक फ़ंक्शन है, जो इनपुट के रूप में 784 नंबर लेता

164
00:15:43,800 --> 00:15:49,720
है और आउटपुट के रूप में 10 नंबर निकालता है। यह एक बेतुका जटिल फ़ंक्शन है, जिसमें

165
00:15:49,720 --> 00:15:54,520
इन वज़न और पूर्वाग्रहों के रूप में 13,000 पैरामीटर शामिल हैं जो कुछ पैटर्न पर

166
00:15:54,520 --> 00:15:59,000
आधारित हैं, और जिसमें कई मैट्रिक्स वेक्टर उत्पादों और सिग्मॉइड स्क्विशिफिकेशन फ़ंक्शन को पुनरावृत्त करना

167
00:15:59,000 --> 00:16:04,760
शामिल है, लेकिन फिर भी यह केवल एक फ़ंक्शन है। और एक तरह से यह आश्वस्त

168
00:16:04,760 --> 00:16:09,720
करने वाला है कि यह जटिल दिखता है। मेरा मतलब है कि यदि यह और भी सरल होता, तो हमें क्या

169
00:16:09,720 --> 00:16:14,920
आशा होती कि यह अंकों को पहचानने की चुनौती का सामना कर सकता? और यह उस चुनौती को कैसे स्वीकार करता है?

170
00:16:14,920 --> 00:16:19,320
यह नेटवर्क केवल डेटा को देखकर उचित भार और पूर्वाग्रह कैसे सीखता है?

171
00:16:19,880 --> 00:16:23,960
खैर, यही मैं अगले वीडियो में दिखाऊंगा, और मैं यह भी जानूंगा कि यह विशेष नेटवर्क वास्तव

172
00:16:23,960 --> 00:16:29,880
में क्या कर रहा है। अब मुद्दा यह है कि मुझे लगता है कि मुझे यह कहना चाहिए कि जब वह

173
00:16:29,880 --> 00:16:34,840
वीडियो या कोई नया वीडियो आता है तो उसके बारे में सूचित रहने के लिए सदस्यता लें, लेकिन वास्तव में आप में से अधिकांश

174
00:16:34,840 --> 00:16:39,880
को वास्तव में यूट्यूब से सूचनाएं प्राप्त नहीं होती हैं, है ना? शायद अधिक ईमानदारी से मुझे यह कहना चाहिए कि

175
00:16:39,880 --> 00:16:44,920
सदस्यता लें ताकि YouTube की अनुशंसा एल्गोरिदम के अंतर्गत आने वाले तंत्रिका नेटवर्क को यह विश्वास हो सके कि

176
00:16:44,920 --> 00:16:49,800
आप इस चैनल की सामग्री देखना चाहते हैं जो आपके लिए अनुशंसित है। वैसे भी, अधिक जानकारी के लिए पोस्ट करते रहें।

177
00:16:50,600 --> 00:16:54,840
पैट्रियन पर इन वीडियो का समर्थन करने वाले सभी लोगों को बहुत-बहुत धन्यवाद। मैं इस गर्मी में संभाव्यता श्रृंखला

178
00:16:54,840 --> 00:16:59,160
में प्रगति करने में थोड़ा धीमा रहा हूं, लेकिन इस परियोजना के बाद मैं इसमें वापस कूद रहा

179
00:16:59,160 --> 00:17:05,640
हूं, ताकि संरक्षक आप वहां अपडेट देख सकें। यहां चीजों को बंद करने के लिए मेरे साथ लीशा

180
00:17:05,640 --> 00:17:09,880
ली हैं, जिन्होंने गहन शिक्षण के सैद्धांतिक पक्ष पर पीएचडी की है, और जो वर्तमान में एम्प्लीफाई पार्टनर्स नामक

181
00:17:09,880 --> 00:17:14,520
एक उद्यम पूंजी फर्म में काम करती हैं, जिन्होंने इस वीडियो के लिए कुछ फंडिंग प्रदान की है।

182
00:17:15,160 --> 00:17:19,480
तो लीशा, मुझे लगता है कि एक चीज़ जो हमें जल्दी से सामने लानी चाहिए वह है यह सिग्मॉइड फ़ंक्शन।

183
00:17:19,480 --> 00:17:23,400
जैसा कि मैं इसे समझता हूं, शुरुआती नेटवर्क शून्य और एक के बीच के अंतराल में प्रासंगिक भारित राशि को

184
00:17:23,400 --> 00:17:28,200
निचोड़ने के लिए इसका उपयोग करते हैं, आप जानते हैं कि न्यूरॉन्स के निष्क्रिय या सक्रिय होने के इस

185
00:17:28,200 --> 00:17:33,240
जैविक सादृश्य से प्रेरित होता है। बिल्कुल। लेकिन अपेक्षाकृत कुछ आधुनिक नेटवर्क वास्तव में अब सिग्मॉइड का

186
00:17:33,240 --> 00:17:37,800
उपयोग करते हैं। हाँ। यह एक तरह का पुराना स्कूल है ना? हाँ या यूँ कहें कि रेलू को प्रशिक्षित करना बहुत

187
00:17:37,800 --> 00:17:43,880
आसान लगता है। और रेलू, रेलू का मतलब रेक्टिफाइड लीनियर यूनिट है? हाँ, यह इस प्रकार का फ़ंक्शन

188
00:17:43,880 --> 00:17:50,280
है जहाँ आप अधिकतम शून्य ले रहे हैं और जहाँ a दिया गया है जो आप वीडियो में समझा

189
00:17:50,280 --> 00:17:56,440
रहे थे। और मुझे लगता है कि यह किस तरह से प्रेरित था, यह आंशिक रूप से एक जैविक

190
00:17:56,440 --> 00:18:03,640
सादृश्य द्वारा था कि न्यूरॉन्स कैसे सक्रिय होंगे या नहीं। और इसलिए यदि यह एक निश्चित सीमा पार कर

191
00:18:03,640 --> 00:18:09,080
जाता है तो यह पहचान कार्य होगा, लेकिन यदि ऐसा नहीं होता है तो यह सक्रिय नहीं होगा इसलिए यह शून्य होगा।

192
00:18:09,080 --> 00:18:13,640
तो यह एक तरह का सरलीकरण है। सिग्मोइड्स का उपयोग करने से प्रशिक्षण में मदद नहीं मिली या कुछ बिंदु

193
00:18:13,640 --> 00:18:21,320
पर प्रशिक्षित करना बहुत मुश्किल था और लोगों ने बस रिले की कोशिश की और यह इन अविश्वसनीय रूप से गहरे

194
00:18:21,320 --> 00:18:26,120
तंत्रिका नेटवर्क के लिए बहुत अच्छी तरह से काम करने लगा। ठीक है, धन्यवाद लीशा।

195
00:18:39,080 --> 00:18:40,060


