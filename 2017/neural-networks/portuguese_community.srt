1
00:00:04,020 --> 00:00:10,680
Isso é um três. É sloppily escrito e renderizado em uma resolução extremamente baixa de 28 por 28 pixels.

2
00:00:10,680 --> 00:00:15,660
Mas o seu cérebro não tem dificuldade em reconhecê-lo como um três e eu quero que você tire um momento para apreciar

3
00:00:15,900 --> 00:00:18,949
Como é que os cérebros conseguem fazer isso sem esforço?

4
00:00:18,949 --> 00:00:23,160
Quero dizer isso isso e isso também são reconhecíveis como três,

5
00:00:23,160 --> 00:00:28,060
mesmo que os valores específicos de cada pixel sejam muito diferentes de uma imagem para outra.

6
00:00:28,080 --> 00:00:33,780
Em particular as células sensíveis à luz  em seus olhos que estão disparando quando você vê este três

7
00:00:33,780 --> 00:00:36,800
são muito diferentes dos que disparam quando você vê este três.

8
00:00:37,140 --> 00:00:40,610
Mas algo louco nesse  córtex visual inteligente de vocês

9
00:00:41,129 --> 00:00:48,139
resolve estes como representando a mesma idéia, enquanto ao mesmo tempo reconhecendo outras imagens como suas próprias idéias distintas

10
00:00:48,840 --> 00:00:55,039
Mas se eu te dissesse, hey se sente e escreva para mim um programa que leva em uma grade de 28 por 28

11
00:00:55,379 --> 00:01:01,759
pixels como este e gera um único número entre 0 e 10, dizendo-lhe o que acha que o dígito é

12
00:01:02,250 --> 00:01:06,139
Bem, a tarefa vai de comicamente trivial para assustadoramente difícil

13
00:01:06,750 --> 00:01:08,270
A menos que você tenha vivido sob uma rocha

14
00:01:08,270 --> 00:01:14,599
Eu acho que dificilmente preciso motivar a relevância e a importância do aprendizado de máquina e das redes neurais para o presente no futuro.

15
00:01:14,640 --> 00:01:18,410
Mas o que eu quero fazer aqui é mostrar a você o que é uma rede neural

16
00:01:18,660 --> 00:01:24,229
Assumindo no fundo e para ajudar a visualizar o que está fazendo  não como um buzzword  mas como uma  parte  da matemática

17
00:01:24,570 --> 00:01:28,310
Minha esperança é que você saia sabendo o  que é essa estrutura em si

18
00:01:28,380 --> 00:01:34,399
Motivado e sentindo que sabe o que significa quando lê ou ouve sobre uma rede neural

19
00:01:34,950 --> 00:01:40,249
Este vídeo vai ser dedicado aos componente de estrutura e em seguida vai abordar a aprendizagem

20
00:01:40,530 --> 00:01:45,950
O que vamos fazer é montar uma rede neural que aprenda a reconhecer dígitos manuscritos

21
00:01:49,270 --> 00:01:51,329
Este é um exemplo um tanto clássico para

22
00:01:51,520 --> 00:01:56,759
Introduzir o tópico e estou feliz de ficar com o status aqui, porque no final dos dois vídeos eu quero apontar

23
00:01:56,760 --> 00:02:02,099
Você para alguns bons recursos onde você pode aprender mais e onde você pode baixar o código que faz isso e jogar com ele?

24
00:02:02,100 --> 00:02:04,100
no seu próprio computador

25
00:02:04,750 --> 00:02:08,970
Existem muitas variantes de redes neurais e nos últimos anos

26
00:02:08,970 --> 00:02:11,970
Houve uma espécie de boom na pesquisa para essas variantes

27
00:02:12,130 --> 00:02:19,019
Mas nesses dois vídeos introdutórios, você e eu vamos apenas olhar para os mais simples plain-vanilla  forma sem frescuras adicionais

28
00:02:19,300 --> 00:02:21,040
Isso é meio que necessário

29
00:02:21,040 --> 00:02:24,510
pré-requisito para a compreensão de qualquer das variantes modernas mais poderosas e

30
00:02:24,760 --> 00:02:28,199
Confie em mim ainda tem muita complexidade para nos envolvermos

31
00:02:28,690 --> 00:02:32,820
Mas mesmo desta forma mais simples, ele pode aprender a reconhecer dígitos manuscritos

32
00:02:32,820 --> 00:02:36,180
O que é uma coisa muito legal para um computador ser capaz de fazer.

33
00:02:37,120 --> 00:02:41,960
E, ao mesmo tempo, você verá como ele fica aquém de um
casal espera que possamos ter para isso

34
00:02:43,090 --> 00:02:48,179
Como o nome sugere, redes neurais são inspiradas no cérebro, mas vamos romper isso

35
00:02:48,520 --> 00:02:51,389
Quais são os neurônios e em que sentido eles estão interligados?

36
00:02:52,090 --> 00:02:57,750
Agora, quando eu digo neurônio tudo o que eu quero que você pense é uma coisa que contém um número

37
00:02:58,209 --> 00:03:02,129
Especificamente, um número entre 0 e 1 não é mais do que isso

38
00:03:03,430 --> 00:03:11,130
Por exemplo, a rede começa com um grupo de neurônios correspondentes a cada uma das 28 vezes 28 pixels da imagem de entrada

39
00:03:11,400 --> 00:03:12,460
que é

40
00:03:12,460 --> 00:03:20,240
784 neurônios no total, cada um deles contém um número que representa o valor da escala de cinza do pixel correspondente

41
00:03:20,769 --> 00:03:24,299
variando de 0 para pixels pretos até 1 para pixels brancos

42
00:03:24,910 --> 00:03:30,419
Esse número dentro do neurônio é chamado de ativação e a imagem que você pode ter em mente aqui

43
00:03:30,420 --> 00:03:33,959
É que cada neurônio é iluminado quando sua ativação é um número alto?

44
00:03:36,260 --> 00:03:41,559
Então, todos esses 784 neurônios formam a primeira camada da nossa rede

45
00:03:45,990 --> 00:03:51,289
Agora pulando para a última camada, esta tem dez neurônios cada um representando um dos dígitos

46
00:03:51,570 --> 00:03:56,239
a ativação nesses neurônios novamente algum número que está entre zero e um

47
00:03:56,880 --> 00:04:00,049
Representa quanto o sistema acha que uma determinada imagem?

48
00:04:00,720 --> 00:04:05,990
Corresponde a um determinado dígito. Há também algumas camadas entre chamadas as camadas ocultas

49
00:04:06,180 --> 00:04:07,770
Que por enquanto?

50
00:04:07,770 --> 00:04:13,549
Deve ser apenas um ponto de interrogação gigante para como esse processo de reconhecimento de dígitos será tratado

51
00:04:13,740 --> 00:04:20,209
Nesta rede eu escolhi duas camadas escondidas, cada uma com 16 neurônios e, reconhecidamente, isso é uma escolha arbitrária

52
00:04:20,609 --> 00:04:24,889
para ser honesto, escolhi duas camadas com base em como eu quero motivar a estrutura em apenas um momento e

53
00:04:25,350 --> 00:04:29,179
16 bem que foi apenas um bom número para caber na tela na prática

54
00:04:29,180 --> 00:04:32,209
Há muito espaço para experimentar com uma estrutura específica aqui

55
00:04:32,730 --> 00:04:38,329
A forma como a rede opera ativações em uma camada determina as ativações da próxima camada

56
00:04:38,760 --> 00:04:45,349
E, claro, o coração da rede como um mecanismo de processamento de informações se resume a exatamente como esses

57
00:04:45,570 --> 00:04:48,409
ativações de uma camada trazem ativações na próxima camada

58
00:04:48,900 --> 00:04:54,859
Significa ser vagamente análogo a como em redes biológicas de neurônios alguns grupos de neurônios disparando

59
00:04:55,410 --> 00:04:57,410
fazer com que certos outros disparem

60
00:04:57,570 --> 00:04:58,340
Agora a rede

61
00:04:58,340 --> 00:05:03,019
Eu estou mostrando aqui já foi treinado para reconhecer dígitos e deixe-me mostrar o que quero dizer com isso

62
00:05:03,140 --> 00:05:06,580
Isso significa que se você alimentar uma imagem iluminando tudo

63
00:05:06,640 --> 00:05:11,780
784 neurônios da camada de entrada de acordo com o brilho de cada pixel na imagem

64
00:05:12,330 --> 00:05:17,029
Esse padrão de ativações faz com que alguns padrões muito específicos na próxima camada

65
00:05:17,190 --> 00:05:19,309
O que causa algum padrão no outro depois dele?

66
00:05:19,440 --> 00:05:22,190
Que finalmente dá algum padrão na camada de saída e?

67
00:05:22,350 --> 00:05:29,359
O neurônio mais brilhante dessa camada de saída é a escolha da rede, por assim dizer, para qual dígito essa imagem representa?

68
00:05:32,070 --> 00:05:36,859
E antes de saltar para a matemática de como uma camada influencia a próxima ou como funciona o treinamento?

69
00:05:37,140 --> 00:05:43,069
Vamos apenas falar sobre por que é até razoável esperar que uma estrutura em camadas como essa se comporte de maneira inteligente

70
00:05:43,800 --> 00:05:48,260
O que estamos esperando aqui? Qual é a melhor esperança para o que essas camadas intermediárias poderiam estar fazendo?

71
00:05:48,860 --> 00:05:56,720
Bem, quando você ou eu reconhecemos dígitos, juntamos vários componentes, um nove tem um laço no topo e uma linha à direita

72
00:05:57,260 --> 00:06:01,280
um 8 também tem um loop no topo, mas ele está emparelhado com outro loop baixo

73
00:06:02,020 --> 00:06:06,599
A 4 basicamente se divide em três linhas específicas e coisas assim

74
00:06:07,180 --> 00:06:11,970
Agora, em um mundo perfeito, podemos esperar que cada neurônio na penúltima camada

75
00:06:12,640 --> 00:06:14,729
corresponde a um desses subcomponentes

76
00:06:14,890 --> 00:06:19,740
Que a qualquer momento você se alimenta em uma imagem com um loop no topo como um 9 ou um 8

77
00:06:19,870 --> 00:06:21,220
Há alguns específicos

78
00:06:21,220 --> 00:06:27,749
Neurônio cuja ativação vai ser perto de um e eu não quero dizer este loop específico de pixels a esperança seria que qualquer

79
00:06:28,090 --> 00:06:35,039
Geralmente padrão loopy em direção ao topo desencadeia este neurônio que vai da terceira camada para o último

80
00:06:35,380 --> 00:06:39,960
requer apenas aprender qual combinação de subcomponentes corresponde a quais dígitos

81
00:06:40,510 --> 00:06:42,810
Claro que apenas chuta o problema na estrada

82
00:06:42,910 --> 00:06:49,019
Porque como você reconheceria esses subcomponentes ou até mesmo aprenderia quais subcomponentes certos deveriam ser e eu ainda nem falei sobre

83
00:06:49,020 --> 00:06:52,829
Como uma camada influencia a próxima, mas corra comigo por um momento

84
00:06:53,650 --> 00:06:56,340
reconhecendo um loop também pode quebrar em subproblemas

85
00:06:56,860 --> 00:07:02,550
Uma maneira razoável de fazer isso seria primeiro reconhecer as várias pequenas arestas que compõem

86
00:07:03,520 --> 00:07:08,910
Da mesma forma, uma linha longa como o tipo que você pode ver nos dígitos 1 ou 4 ou 7

87
00:07:08,910 --> 00:07:14,279
Bem, isso é realmente apenas uma borda longa ou talvez você pense nisso como um certo padrão de várias bordas menores

88
00:07:14,740 --> 00:07:19,379
Então, talvez a nossa esperança é que cada neurônio na segunda camada da rede

89
00:07:20,290 --> 00:07:22,650
corresponde com as várias pequenas arestas relevantes

90
00:07:23,230 --> 00:07:28,259
Talvez quando uma imagem como esta venha iluminar todos os neurônios

91
00:07:28,720 --> 00:07:31,649
associado com cerca de oito a dez pequenas arestas específicas

92
00:07:31,930 --> 00:07:36,930
que, por sua vez, acende os neurônios associados ao laço superior e uma longa linha vertical e

93
00:07:37,300 --> 00:07:39,599
Aqueles acendem o neurônio associado a nove

94
00:07:40,300 --> 00:07:41,100
independente da resposta

95
00:07:41,100 --> 00:07:47,070
Isto é o que a nossa rede final realmente faz é outra questão, a qual eu voltarei depois de vermos como treinar a rede

96
00:07:47,350 --> 00:07:52,170
Mas esta é uma esperança que poderíamos ter. Uma espécie de objetivo com a estrutura em camadas como esta

97
00:07:53,020 --> 00:07:59,340
Além disso, você pode imaginar como ser capaz de detectar bordas e padrões como esses seria realmente útil para outras tarefas de reconhecimento de imagem.

98
00:07:59,740 --> 00:08:06,749
E, mesmo além do reconhecimento de imagens, existem todos os tipos de coisas inteligentes que você pode querer fazer e que se dividem em camadas de abstração.

99
00:08:07,690 --> 00:08:14,670
O discurso de análise, por exemplo, envolve a obtenção de áudio bruto e a seleção de sons distintos, que se combinam para produzir determinadas sílabas.

100
00:08:15,070 --> 00:08:19,829
Que se combinam para formar palavras que se combinam para formar frases e pensamentos mais abstratos, etc.

101
00:08:20,770 --> 00:08:25,710
Mas voltando a como tudo isso realmente funciona imagine-se agora projetando

102
00:08:25,710 --> 00:08:30,449
Como exatamente as ativações em uma camada podem determinar as ativações na próxima?

103
00:08:30,670 --> 00:08:35,879
O objetivo é ter algum mecanismo que possa combinar pixels em bordas

104
00:08:35,880 --> 00:08:41,430
Ou bordas em padrões ou padrões em dígitos e para ampliar em um exemplo muito específico

105
00:08:41,950 --> 00:08:44,189
Vamos dizer que a esperança é para um particular

106
00:08:44,380 --> 00:08:50,430
Neurônio na segunda camada para saber se a imagem tem ou não uma borda nessa região aqui

107
00:08:50,950 --> 00:08:54,960
A questão em questão é quais parâmetros a rede deve ter

108
00:08:55,270 --> 00:09:02,490
que dials e knobs você deve ser capaz de ajustar para que seja expressivo o suficiente para potencialmente capturar este padrão ou

109
00:09:02,590 --> 00:09:07,290
Qualquer outro padrão de pixel ou o padrão que várias arestas podem fazer um loop e outras coisas desse tipo?

110
00:09:08,290 --> 00:09:15,389
Bem, o que faremos é atribuir um peso a cada uma das conexões entre nosso neurônio e os neurônios da primeira camada

111
00:09:15,850 --> 00:09:17,850
Esses pesos são apenas números

112
00:09:18,190 --> 00:09:25,590
então pegue todas as ativações da primeira camada e calcule a soma ponderada de acordo com esses pesos.

113
00:09:27,370 --> 00:09:31,680
Ache útil pensar nesses pesos como sendo organizados em uma pequena grade própria

114
00:09:31,680 --> 00:09:37,079
E vou usar pixels verdes para indicar pesos positivos e pixels vermelhos para indicar pesos negativos

115
00:09:37,240 --> 00:09:41,670
Onde o brilho desse pixel é alguma representação solta do valor de pesos?

116
00:09:42,400 --> 00:09:45,840
Agora, se fizemos os pesos associados a quase todos os pixels zero

117
00:09:46,150 --> 00:09:49,079
exceto por alguns pesos positivos nesta região que nos preocupamos

118
00:09:49,480 --> 00:09:51,310
em seguida, tomando a soma ponderada de

119
00:09:51,310 --> 00:09:57,690
Todos os valores de pixel realmente significam somar os valores do pixel apenas na região com a qual nos preocupamos.

120
00:09:58,870 --> 00:10:04,440
E, se você realmente quer saber se há uma vantagem aqui, o que você pode fazer é ter alguns pesos negativos

121
00:10:04,900 --> 00:10:06,900
associado aos pixels adjacentes

122
00:10:07,030 --> 00:10:12,660
Em seguida, a soma é maior quando esses pixels médios são brilhantes, mas os pixels ao redor são mais escuros

123
00:10:14,279 --> 00:10:18,169
Quando você calcula uma soma ponderada como esta, você pode sair com qualquer número

124
00:10:18,240 --> 00:10:23,180
mas para essa rede o que queremos é que as ativações tenham algum valor entre 0 e 1

125
00:10:23,730 --> 00:10:26,599
então uma coisa comum a fazer é bombear essa soma ponderada

126
00:10:26,910 --> 00:10:32,000
Em alguma função que limita a linha numérica real no intervalo entre 0 e 1 e

127
00:10:32,190 --> 00:10:37,249
Uma função comum que faz isso é chamada de função sigmóide, também conhecida como uma curva logística

128
00:10:37,980 --> 00:10:43,339
basicamente entradas muito negativas acabam perto de zero entradas muito positivas acabam perto de 1

129
00:10:43,339 --> 00:10:46,398
e apenas aumenta de forma constante em torno da entrada 0

130
00:10:49,080 --> 00:10:56,029
Portanto, a ativação do neurônio aqui é basicamente uma medida de quão positiva é a soma ponderada relevante

131
00:10:57,450 --> 00:11:01,819
Mas talvez não seja que você queira que o neurônio acenda quando a soma ponderada for maior que 0

132
00:11:02,100 --> 00:11:06,260
Talvez você só queira que ele esteja ativo quando a soma for maior que 10

133
00:11:06,630 --> 00:11:10,279
Isso é que você quer algum preconceito para que seja inativo

134
00:11:10,860 --> 00:11:16,099
o que faremos então é adicionar outro número como 10 negativo a essa soma ponderada

135
00:11:16,529 --> 00:11:19,669
Antes de ligá-lo através da função de saturação de sigmóide

136
00:11:20,220 --> 00:11:22,730
Esse número adicional é chamado de viés

137
00:11:23,310 --> 00:11:29,060
Então os pesos dizem a você qual padrão de pixel este neurônio na segunda camada está captando e o viés

138
00:11:29,220 --> 00:11:35,450
diz-lhe quão alta a soma ponderada precisa ser antes que o neurônio comece a se tornar significativamente ativo

139
00:11:35,910 --> 00:11:37,910
E isso é apenas um neurônio

140
00:11:38,120 --> 00:11:41,940
Todos os outros neurônios nesta camada serão conectados a todos

141
00:11:42,320 --> 00:11:50,620
784 pixels de neurônios da primeira camada e cada uma dessas 784 conexões tem seu próprio peso associado a ela

142
00:11:51,330 --> 00:11:57,739
também cada um tem algum viés algum outro número que você adiciona à soma ponderada antes de esmagá-lo com o sigmóide e

143
00:11:58,020 --> 00:12:01,909
Isso é muito para se pensar com essa camada oculta de 16 neurônios

144
00:12:02,010 --> 00:12:08,270
isso é um total de 784 vezes 16 pesos, juntamente com 16 preconceitos

145
00:12:08,490 --> 00:12:14,029
E tudo isso são apenas as conexões da primeira camada até a segunda, as conexões entre as outras camadas

146
00:12:14,029 --> 00:12:17,208
Além disso, tem um monte de pesos e vieses associados a eles

147
00:12:17,760 --> 00:12:20,680
Tudo dito e feito esta rede tem quase exatamente

148
00:12:21,280 --> 00:12:23,920
13.000 pesos e vieses totais

149
00:12:24,280 --> 00:12:29,540
13.000 botões e mostradores que podem ser ajustados e tornados para fazer com que esta rede se comporte de diferentes maneiras

150
00:12:30,520 --> 00:12:32,520
Então, quando falamos de aprender?

151
00:12:32,530 --> 00:12:40,199
O que está se referindo é fazer com que o computador encontre uma configuração válida para todos esses muitos números, de modo que ele realmente resolva

152
00:12:40,200 --> 00:12:42,190
o problema na mão

153
00:12:42,190 --> 00:12:43,000
um pensamento

154
00:12:43,000 --> 00:12:49,979
Experimento que é ao mesmo tempo divertido e meio horrível é imaginar sentar e definir todos esses pesos e vieses à mão

155
00:12:50,380 --> 00:12:56,159
Propositalmente aprimorando os números para que a segunda camada pegue nas bordas, a terceira camada capta padrões etc.

156
00:12:56,350 --> 00:13:01,440
Eu pessoalmente acho isso satisfatório, em vez de apenas ler a rede como uma caixa preta total

157
00:13:01,870 --> 00:13:04,349
Porque quando a rede não faz o mesmo que você

158
00:13:04,600 --> 00:13:11,370
antecipar se você construiu um pouco de um relacionamento com o que esses pesos e preconceitos, na verdade, significa que você tem um ponto de partida para

159
00:13:11,680 --> 00:13:16,289
Experimentando como alterar a estrutura para melhorar ou quando a rede funciona?

160
00:13:16,290 --> 00:13:18,290
Mas não pelas razões que você poderia esperar

161
00:13:18,310 --> 00:13:25,169
Investigar o que os pesos e vieses estão fazendo é uma boa maneira de desafiar suas suposições e realmente expor o espaço total de possíveis

162
00:13:25,180 --> 00:13:26,350
soluções

163
00:13:26,350 --> 00:13:30,600
A propósito, a função real aqui é um pouco incômoda para anotar. Você não acha?

164
00:13:32,350 --> 00:13:38,460
Então deixe-me mostrar-lhe uma maneira mais notoriamente compacta que essas conexões são representadas. É assim que você veria

165
00:13:38,460 --> 00:13:40,460
Se você escolher ler mais sobre redes neurais

166
00:13:41,110 --> 00:13:45,810
Organize todas as ativações de uma camada em uma coluna como um vetor

167
00:13:47,470 --> 00:13:52,320
Em seguida, organize todos os pesos como uma matriz em que cada linha dessa matriz

168
00:13:52,900 --> 00:13:57,659
corresponde às conexões entre uma camada e um neurônio particular na próxima camada

169
00:13:58,060 --> 00:14:03,599
O que isso significa é que a soma ponderada das ativações na primeira camada de acordo com esses pesos?

170
00:14:04,000 --> 00:14:09,330
Corresponde a um dos termos do produto vetorial matriz de tudo o que temos à esquerda aqui

171
00:14:13,540 --> 00:14:18,380
Aliás, muito do aprendizado de máquina se resume a ter uma boa compreensão da álgebra linear

172
00:14:18,380 --> 00:14:26,940
Então, para qualquer um de vocês que quer um bom entendimento visual para matrizes e que multiplicação vetorial de matriz significa dar uma olhada na série que fiz sobre álgebra linear

173
00:14:27,250 --> 00:14:28,839
especialmente o capítulo três

174
00:14:28,839 --> 00:14:35,759
Voltando à nossa expressão, em vez de falar sobre adicionar o preconceito a cada um desses valores de forma independente, nós o representamos

175
00:14:36,010 --> 00:14:42,209
Organizando todos esses vieses em um vetor e adicionando o vetor inteiro ao produto vetor de matriz anterior

176
00:14:42,910 --> 00:14:44,040
Então, como um passo final

177
00:14:44,040 --> 00:14:47,250
Eu vou rap um sigmóide ao redor do lado de fora aqui

178
00:14:47,250 --> 00:14:51,899
E o que é suposto representar é que você vai aplicar a função sigmóide a cada

179
00:14:52,420 --> 00:14:54,570
componente do vetor resultante dentro

180
00:14:55,510 --> 00:15:00,749
Então, uma vez que você anote esta matriz de peso e esses vetores como seus próprios símbolos, você pode

181
00:15:01,000 --> 00:15:07,589
comunicar a transição completa de ativações de uma camada para a próxima em uma expressão pequena extremamente

182
00:15:07,930 --> 00:15:15,000
Isso torna o código relevante muito mais simples e muito mais rápido, já que muitas bibliotecas otimizam a multiplicação de matrizes

183
00:15:17,560 --> 00:15:21,359
Lembre-se de quão cedo eu disse que esses neurônios são simplesmente coisas que seguram números

184
00:15:21,790 --> 00:15:26,250
Bem, é claro que os números específicos que eles contêm dependem da imagem que você alimenta

185
00:15:27,790 --> 00:15:32,940
Então, é realmente mais preciso pensar em cada neurônio como uma função que absorve o

186
00:15:33,070 --> 00:15:38,070
saídas de todos os neurônios na camada anterior e cospe um número entre zero e um

187
00:15:38,800 --> 00:15:42,270
Realmente toda a rede é apenas uma função que leva em

188
00:15:42,760 --> 00:15:47,010
784 números como uma entrada e cospe dez números como uma saída

189
00:15:47,470 --> 00:15:48,700
É um absurdo

190
00:15:48,700 --> 00:15:56,249
Função complicada que envolve treze mil parâmetros nas formas desses pesos e vieses que captam certos padrões e que envolvem

191
00:15:56,250 --> 00:16:00,270
iterando muitos produtos vetoriais de matriz e a função de evocação de esmagamento sigmóide

192
00:16:00,610 --> 00:16:06,390
Mas é apenas uma função, e de certa forma é reconfortante parecer complicado

193
00:16:06,390 --> 00:16:12,239
Quero dizer, se fosse mais simples que esperança teríamos para enfrentar o desafio de reconhecer dígitos?

194
00:16:12,960 --> 00:16:19,559
E como isso leva esse desafio? Como esta rede aprende os pesos e vieses apropriados apenas olhando dados? Oh

195
00:16:20,080 --> 00:16:26,039
Isso é o que eu mostrarei no próximo vídeo, e eu também vou pesquisar um pouco mais sobre o que esta rede particular que estamos vendo está realmente fazendo

196
00:16:27,130 --> 00:16:32,640
Agora é o ponto que eu suponho que devo dizer se inscrever para ficar notificado sobre quando o vídeo ou qualquer novo vídeo sair

197
00:16:32,760 --> 00:16:37,560
Mas, na realidade, a maioria de vocês não recebe notificações do YouTube, não é?

198
00:16:37,560 --> 00:16:42,260
Talvez mais honestamente eu deva dizer se inscreva para que as redes neurais subjacentes ao YouTube

199
00:16:42,459 --> 00:16:47,639
O algoritmo de recomendação está preparado para acreditar que você deseja ver o conteúdo deste canal recomendado para você

200
00:16:48,250 --> 00:16:50,250
de qualquer maneira ficar postado por mais

201
00:16:50,410 --> 00:16:53,550
Muito obrigado a todos que apoiam esses vídeos no patreon

202
00:16:53,589 --> 00:16:56,759
Eu tenho sido um pouco lento para progredir na série de probabilidade neste verão

203
00:16:56,760 --> 00:17:01,379
Mas eu estou pulando de volta para ele depois deste projeto, então patronos, você pode olhar para atualizações lá

204
00:17:03,310 --> 00:17:05,550
Para fechar as coisas aqui eu tenho comigo Lisha Li

205
00:17:05,550 --> 00:17:12,029
Lee, que fez seu PhD trabalhar no lado teórico da aprendizagem profunda e que atualmente trabalha em uma empresa de capital de risco chamada amplificar parceiros

206
00:17:12,030 --> 00:17:16,530
Quem gentilmente forneceu alguns dos fundos para este vídeo, então Lisha uma coisa

207
00:17:16,530 --> 00:17:19,109
Eu acho que devemos rapidamente trazer é essa função sigmóide

208
00:17:19,180 --> 00:17:24,780
Pelo que entendi as primeiras redes usaram isso para esmagar a soma ponderada relevante para aquele intervalo entre zero e um

209
00:17:24,980 --> 00:17:30,340
Você sabe que tipo de motivado por esta analogia biológica de neurônios seja inativo ou ativo
(Lisha) - Exatamente

210
00:17:30,360 --> 00:17:36,320
(3B1B) - Mas relativamente poucas redes modernas realmente usam sigmoide. Isso é tipo de old school certo?
(Lisha) - Sim ou melhor

211
00:17:36,370 --> 00:17:42,780
ReLU parece ser muito mais fácil de treinar
(3B1B) - E ReLU realmente significa unidade linear retificada

212
00:17:42,780 --> 00:17:48,839
(Lisha) - Sim, é esse tipo de função onde você está tirando um máximo de 0 e um onde a é dado por

213
00:17:49,120 --> 00:17:53,670
o que você estava explicando no vídeo e o que isso foi meio que motivado por eu acho que foi um

214
00:17:54,610 --> 00:17:56,610
parcialmente por um biológico

215
00:17:56,620 --> 00:17:58,179
Analogia com como

216
00:17:58,179 --> 00:18:03,089
Os neurônios seriam ativados ou não e, portanto, se ultrapassasse um certo limite

217
00:18:03,250 --> 00:18:05,250
Seria a função de identidade

218
00:18:05,290 --> 00:18:10,439
Mas se isso não acontecesse, então não seria ativado, portanto, seja zero, então é uma espécie de simplificação

219
00:18:10,720 --> 00:18:14,429
O uso de sigmoides não ajudou no treinamento, ou foi muito difícil treinar

220
00:18:14,429 --> 00:18:19,589
É em algum momento e as pessoas apenas tentaram relu e aconteceu de funcionar

221
00:18:20,110 --> 00:18:22,140
Muito bem para estes incrivelmente

222
00:18:22,690 --> 00:18:25,090
Redes neurais profundas.
(3B1B) - Tudo bem

223
00:18:25,090 --> 00:18:26,060
Obrigado Lisha

