1
00:00:00,000 --> 00:00:01,516
这是一个3。

2
00:00:01,516 --> 00:00:09,042
它以 28x28 像素的极低分辨率编写和渲染，但你的

3
00:00:09,042 --> 00:00:13,781
大脑可以毫不费力地将其识别为 3。

4
00:00:13,781 --> 00:00:16,715
我希望你能花点时间体会 一下大脑可

5
00:00:16,715 --> 00:00:19,650
以如此轻松地做到这一点是多么疯狂。

6
00:00:19,650 --> 00:00:24,199
我的意思是 ，这个、这个和这个也可以被识别为 3

7
00:00:24,199 --> 00:00:28,747
秒，尽管每个像素的具 体值与下一个图像有很大不同。

8
00:00:28,747 --> 00:00:34,029
当您看到这 3 时，您眼睛中 发射的特定光敏细胞与您看到这

9
00:00:34,029 --> 00:00:36,845
3 时发射的光敏细胞非常不 同。

10
00:00:36,845 --> 00:00:42,959
但你那疯狂聪明的视觉皮层中的某些东西将这些图像解析为

11
00:00:42,959 --> 00:00:49,300
代表相同的想法，同时将其他图像识别为它们自己独特的想法。

12
00:00:49,300 --> 00:00:53,983
但如果我告诉你，嘿，坐下来为我写一个程序，它接受

13
00:00:53,983 --> 00:00:57,917
28x28 的网 格并输出 0 到 10

14
00:00:57,917 --> 00:01:01,851
之间的单个数字，告诉你它认为这个数字是什

15
00:01:01,851 --> 00:01:06,160
么，那么这个任务就会从可笑的琐碎变成极其困难。

16
00:01:06,160 --> 00:01:10,379
除非你一直生活在 岩石下，否则我认为我几乎不需要激发

17
00:01:10,379 --> 00:01:14,598
机器学习和神经网络对 于现在和未来的相关性和重要性。

18
00:01:14,598 --> 00:01:16,988
但我在这里想做的是在没有背景的情

19
00:01:16,988 --> 00:01:20,222
况下向您展示神经网络实际上是什么，并帮助您可视

20
00:01:20,222 --> 00:01:24,300
化它正在做什么，而不是作 为一个流行语，而是作为一段数学。

21
00:01:24,300 --> 00:01:27,140
我的希望只是，当你读完或听到神

22
00:01:27,140 --> 00:01:31,933
经网络引用-非引用学习时，你会感觉结构本身是有动机的

23
00:01:31,933 --> 00:01:34,596
，并且感觉你知道它意味着什么。

24
00:01:34,596 --> 00:01:40,300
本视频将专门讨论 其结构部分，下面的视频将讨论学习问题。

25
00:01:40,300 --> 00:01:46,264
我们要做的就是建立一个可以学习识别手写数字的神经网络 。

26
00:01:46,264 --> 00:01:51,302
这是介绍该主题的一个有点经典的示例，我很高兴在这里坚持

27
00:01:51,302 --> 00:01:56,699
现状，因为在两个视频的末尾，我想向您指出一些很好的资源，您

28
00:01:56,699 --> 00:02:02,097
可以在其中了解更多信息，以及您可以下载执行此操作的代码并在

29
00:02:02,097 --> 00:02:04,256
您自己的计算机上使用它。

30
00:02:04,256 --> 00:02:09,100
神经网络有很多变体，近年来， 对这些变体的研

31
00:02:09,100 --> 00:02:12,843
究蓬勃发展，但在这两个介绍性视频

32
00:02:12,843 --> 00:02:19,009
中，你和我将只看最简单的普通形式，没有任何额外的 装饰。

33
00:02:19,009 --> 00:02:23,267
这是理解任何更强大的现代变体的必要先决

34
00:02:23,267 --> 00:02:28,591
条件，相信我，它仍然有很多复杂性需要我们去思考 。

35
00:02:28,591 --> 00:02:33,807
但即使是这种最简单的形式，它也可以学习识别手写数字，这对

36
00:02:33,807 --> 00:02:36,685
于计算机来说是一件非常酷的事情。

37
00:02:36,685 --> 00:02:42,980
同时你会发现它确实没 有达到我们的一些希望。

38
00:02:42,980 --> 00:02:48,246
顾名思义，神经网络受到大脑 的启发，但让我们来分解一下。

39
00:02:48,246 --> 00:02:51,943
什么是神经元，它们以什么方式连接 在一起？

40
00:02:51,943 --> 00:02:57,661
现在，当我说神经元时，我想让你想到的是一个包含数字的东西

41
00:02:57,661 --> 00:03:01,014
，特别是 0 到 1 之间的数字。

42
00:03:01,014 --> 00:03:03,407
确实仅此而已。

43
00:03:03,407 --> 00:03:07,693
例如，网络 从对应于输入图像的 28×28

44
00:03:07,693 --> 00:03:12,758
像素中的每一个像素的一堆神经元 开始，总共 784

45
00:03:12,758 --> 00:03:13,732
个神经元。

46
00:03:13,732 --> 00:03:17,447
其中每一个都包含一个数字，表示

47
00:03:17,447 --> 00:03:24,413
相应像素的灰度值，范围从黑色像素的 0 到白色像素的 1 。

48
00:03:24,413 --> 00:03:30,882
神经元内部的这个数字称为它的激活值，您可能想到的图像是

49
00:03:30,882 --> 00:03:35,965
，当每个神经元的激活值很高时，它就会被点亮。

50
00:03:35,965 --> 00:03:43,500
因此，所有这 784 个神经元构成了我们网络的第一层。

51
00:03:43,500 --> 00:03:47,436
现在跳到最后一层，它有 1 0

52
00:03:47,436 --> 00:03:51,618
个神经元，每个神经元代表一个数字。

53
00:03:51,618 --> 00:03:55,869
这些神经元的激活（同 样是 0 到 1

54
00:03:55,869 --> 00:04:02,246
之间的某个数字）表示系统认为给定图 像与给定数字的对应程度。

55
00:04:02,246 --> 00:04:07,995
中间还有一些称为隐藏层的 层，目前这应该只是一

56
00:04:07,995 --> 00:04:13,993
个巨大的问号，不知道如 何处理这个数字识别过程。

57
00:04:13,993 --> 00:04:17,704
在这个网络中，我选择了两个隐 藏层，每个隐藏层有

58
00:04:17,704 --> 00:04:20,673
16 个神经元，诚然，这是一种任意选择。

59
00:04:20,673 --> 00:04:25,567
说实话，我 选择了两层，基于我想如何在短时间内激发结构，而

60
00:04:25,567 --> 00:04:28,666
16 层， 这只是一个适合屏幕的数字。

61
00:04:28,666 --> 00:04:32,866
实际上，这里有很大的空间可以对特 定结构进行实验。

62
00:04:32,866 --> 00:04:38,685
网络运行的方式，一层的激活决定下 一层的激活。

63
00:04:38,685 --> 00:04:42,781
当然，作为一种信息处理机制，网

64
00:04:42,781 --> 00:04:48,924
络的核心归结为一层的激活如何引起下一层的 激活。

65
00:04:48,924 --> 00:04:53,740
它大致类似于神经元生物网络中某些神经元群的放

66
00:04:53,740 --> 00:04:57,091
电导致某些其他神经元放电的方式。

67
00:04:57,091 --> 00:05:00,281
现在我在这里展示的网络已 经接受过识

68
00:05:00,281 --> 00:05:03,471
别数字的训练，让我向您展示我的意思。

69
00:05:03,471 --> 00:05:08,112
这意味着，如 果您输入一张图像，根据图像中每个像素

70
00:05:08,112 --> 00:05:11,083
的亮度照亮输入层的所有 784

71
00:05:11,083 --> 00:05:16,096
个神经元，则该激活模式会在下一层中导致一些非常特定的

72
00:05:16,096 --> 00:05:20,737
模式，从而在下一层中导致一些模式它最终在输出层中给

73
00:05:20,737 --> 00:05:22,223
出了一些模式 。

74
00:05:22,223 --> 00:05:28,389
输出层最亮的神经元是网络的选择，可以说，该图

75
00:05:28,389 --> 00:05:30,534
像代表什么数字。

76
00:05:30,534 --> 00:05:34,466
在深入了解一层如何影响下一层或训

77
00:05:34,466 --> 00:05:40,710
练如何进行之前，我们先来谈谈为什么期望这样的分层结构

78
00:05:40,710 --> 00:05:43,485
能够智能地运行是合理的。

79
00:05:43,485 --> 00:05:45,084
我们在这里期待什么？

80
00:05:45,084 --> 00:05:49,093
这些中间层可能 做的事情的最大希望是什么？

81
00:05:49,093 --> 00:05:53,786
好吧，当你或我识别数字时，我们会将各种组件 拼凑在一起。

82
00:05:53,786 --> 00:05:56,998
9 的顶部有一个环，右侧有一条线。

83
00:05:56,998 --> 00:06:01,901
8 也有一个顶部环， 但它与另一个底部环配对。

84
00:06:01,901 --> 00:06:06,771
4 基本上可以分为三行，诸 如此类。

85
00:06:06,771 --> 00:06:11,519
现在在一个完美的世界中，我们可能希望倒数第二层中的

86
00:06:11,519 --> 00:06:15,719
每个神经元都与这些子组件之一相对应，每当您输入

87
00:06:15,719 --> 00:06:19,919
带有顶部循环（例 如 9 或 8）的图像时，都

88
00:06:19,919 --> 00:06:23,755
会有一些其激活值接近 1 的特定神 经元。

89
00:06:23,755 --> 00:06:28,705
我并不是指这种特定的像素循环，而是希望任何朝向顶部

90
00:06:28,705 --> 00:06:31,751
的普遍循环模式都会引发该神经元。

91
00:06:31,751 --> 00:06:35,926
这样，从第三层到最 后一层只需要学

92
00:06:35,926 --> 00:06:40,101
习哪个子组件组合对应于哪个数 字。

93
00:06:40,101 --> 00:06:44,043
当然，这只是解决问题，因为您如何识别这些子

94
00:06:44,043 --> 00:06:47,805
组件，甚至如何了解正确的子组件应该是什么？

95
00:06:47,805 --> 00:06:50,275
我什至还 没有讨论一层如何影响下

96
00:06:50,275 --> 00:06:52,900
一层，但请跟我一起讨论一下这一层。

97
00:06:52,900 --> 00:06:56,650
识别循环也可以分解为子问题。

98
00:06:56,650 --> 00:07:03,433
一种合理的方法是首 先识别构成它的各种小边缘。

99
00:07:03,433 --> 00:07:07,340
类似地，一条长线，就像您 在数字 1、4

100
00:07:07,340 --> 00:07:11,992
或 7 中看到的那种，那实际上只是一条长边，或者

101
00:07:11,992 --> 00:07:15,341
您可能将其视为几个较小边的某种图案。

102
00:07:15,341 --> 00:07:19,271
因此，也许我们希望网络第二 层中的

103
00:07:19,271 --> 00:07:23,432
每个神经元都与各种相关的小边相对应。

104
00:07:23,432 --> 00:07:27,338
也许当像这样的 图像出现时，它会照亮与大约

105
00:07:27,338 --> 00:07:30,712
8 到 10 个特定小边缘相关的所有

106
00:07:30,712 --> 00:07:34,796
神经元，这些边缘又会照亮与上环和一条长垂直线相

107
00:07:34,796 --> 00:07:39,768
关的神经元，而这些 神经元又会照亮与 9 相关的神经元。

108
00:07:39,768 --> 00:07:43,526
这是否是我们最终网络实际上 所做的事情是另一个问题

109
00:07:43,526 --> 00:07:47,284
，一旦我们了解如何训练网络，我就会回到这个 问题。

110
00:07:47,284 --> 00:07:52,439
但这是我们可能拥有的一个希望，一种像这样的分层结构的目 标。

111
00:07:52,439 --> 00:07:57,415
此外，您可以想象能够检测这样的边缘和图案对于

112
00:07:57,415 --> 00:08:00,444
其他图像识别任务将非常有用。

113
00:08:00,444 --> 00:08:03,928
甚至除了图像识别之外， 您可能还想做各种各

114
00:08:03,928 --> 00:08:07,412
样的智能事情，这些事情可以分解为抽象 层。

115
00:08:07,412 --> 00:08:12,443
例如，解析语音涉及获取原始音频并挑选出不同的声音，这些声音

116
00:08:12,443 --> 00:08:16,636
结合起来形成某些音节，这些音节结合起来形成单词，这

117
00:08:16,636 --> 00:08:20,326
些声音结合起来 组成短语和更抽象的想法等等。

118
00:08:20,326 --> 00:08:23,794
但回到这些实际上是如何工作的，想

119
00:08:23,794 --> 00:08:29,507
象一下你自己现在正在设计一层中的激活如何准确地确定下一

120
00:08:29,507 --> 00:08:30,731
层中的激活。

121
00:08:30,731 --> 00:08:35,433
我们的目标是拥有某种机制，可以将像素组合成边缘，或者

122
00:08:35,433 --> 00:08:38,917
将边缘组合成图案，或者将图案组合成数字。

123
00:08:38,917 --> 00:08:44,702
放大一个非常具体的示 例，假设希望第二层中的一

124
00:08:44,702 --> 00:08:50,739
个特定神经元能够识别图像在 此区域中是否有边缘。

125
00:08:50,739 --> 00:08:55,305
当前的问题是，网络应该具有哪些 参数？

126
00:08:55,305 --> 00:09:00,447
您应该能够调整哪些转盘和旋钮，以便具有足够的表现力来潜在

127
00:09:00,447 --> 00:09:04,701
地捕捉这种图案，或任何其他像素图案，或多个边缘可

128
00:09:04,701 --> 00:09:08,247
以形成循环的图 案，以及其他类似的东西？

129
00:09:08,247 --> 00:09:11,921
好吧，我们要做的就是为我们的神经元和

130
00:09:11,921 --> 00:09:15,790
第一层神经元之间的每个连接分配一个权重。

131
00:09:15,790 --> 00:09:17,797
这些重量只是数 字。

132
00:09:17,797 --> 00:09:24,251
然后从第一层获取所有这些激活并根据这些权重计算它

133
00:09:24,251 --> 00:09:25,800
们的加权和。

134
00:09:25,800 --> 00:09:29,960
我发现将这些权重视为被组织成自己的小网

135
00:09:29,960 --> 00:09:35,369
格是有帮助的，我将使用绿色像素来表示正权重，使用红

136
00:09:35,369 --> 00:09:40,986
色像素来表示负权重，其中该像素的亮度是一些对重量值的

137
00:09:40,986 --> 00:09:42,026
宽松描述。

138
00:09:42,026 --> 00:09:46,279
如果我们将与几乎所有像素相关的权重设为零（

139
00:09:46,279 --> 00:09:51,498
除了我们关心的该区域中的一些正权重），那么对所有像素

140
00:09:51,498 --> 00:09:56,331
值进行加权和实际上相当于将刚刚在其中的像素值相加。

141
00:09:56,331 --> 00:09:58,070
我们 关心的地区。

142
00:09:58,070 --> 00:10:02,514
如果您确实想知道这里是否有边缘，您

143
00:10:02,514 --> 00:10:07,451
可能会做的是与周围像素相关的一些负权重。

144
00:10:07,451 --> 00:10:12,680
当中间 的像素较亮而周围的像素较暗时，总和最大。

145
00:10:12,680 --> 00:10:18,138
当你计算这样的加权和时，你可能会得到任何数字，但对于这个

146
00:10:18,138 --> 00:10:23,596
网络，我 们想要的是激活值是 0 到 1 之间的某个值。

147
00:10:23,596 --> 00:10:27,482
因此，常见的做法是 将这个加权和注入某个函数

148
00:10:27,482 --> 00:10:31,546
，将实数轴压缩到 0 到 1 之间的 范围内。

149
00:10:31,546 --> 00:10:35,373
执行此操作的常见函数称为 sigmoid

150
00:10:35,373 --> 00:10:37,560
函数，也称为 逻辑曲线。

151
00:10:37,560 --> 00:10:43,768
基本上，非常负的输入最终接近 0，非常正的输入最终接近

152
00:10:43,768 --> 00:10:48,203
1，并且它只是在输入 0 附近稳定增加。

153
00:10:48,203 --> 00:10:56,757
所以这里神经元的激活 基本上是衡量相关加权和的正值程度。

154
00:10:56,757 --> 00:11:02,349
但也许你并不希望 神经元在加权和大于0时亮起。

155
00:11:02,349 --> 00:11:06,838
也许您只希望当总和大于 1 0 时它才处于活动状态。

156
00:11:06,838 --> 00:11:10,689
也就是说，您希望对其不活动有一些偏差。

157
00:11:10,689 --> 00:11:16,233
然后我们 要做的就是在这个加权和中添加一些其他数字，比如负

158
00:11:16,233 --> 00:11:20,668
10，然后将其插 入 sigmoid 压缩函数。

159
00:11:20,668 --> 00:11:23,395
这个额外的数字称为偏差。

160
00:11:23,395 --> 00:11:27,273
因此 ，权重告诉您第二层中的该神经元正在

161
00:11:27,273 --> 00:11:31,150
接收什么像素模式，而偏 差则告诉您在神经

162
00:11:31,150 --> 00:11:35,221
元开始有意义地活动之前，加权和需要多高 。

163
00:11:35,221 --> 00:11:37,328
那只是一个神经元。

164
00:11:37,328 --> 00:11:41,958
该层中的每个其他神经元都将连接到第一层的所

165
00:11:41,958 --> 00:11:46,379
有 784 个像素神经元，并且这 784

166
00:11:46,379 --> 00:11:50,799
个连接中的每个连接都有其 自己的关联权重。

167
00:11:50,799 --> 00:11:54,791
此外，每个值都有一些偏差，即在用 sigmoid

168
00:11:54,791 --> 00:11:57,986
压缩 之前将其添加到加权和中的其他数字。

169
00:11:57,986 --> 00:11:59,711
这需要考虑很多！

170
00:11:59,711 --> 00:12:03,467
对于这个包含 16 个神经元的隐藏层，总共有

171
00:12:03,467 --> 00:12:07,877
784 个权重乘以 16 个权重，以及 16 个偏差。

172
00:12:07,877 --> 00:12:12,139
所有这些 只是从第一层到第二层的连接。

173
00:12:12,139 --> 00:12:18,092
其他层之间的连接 也有一堆与之相关的权重和偏差。

174
00:12:18,092 --> 00:12:24,071
总而言之，这个网络 几乎有 13,000 个总权重和偏差。

175
00:12:24,071 --> 00:12:27,301
13,000 个旋钮和转盘可 以进行调

176
00:12:27,301 --> 00:12:30,532
整和转动，以使该网络以不同的方式运行。

177
00:12:30,532 --> 00:12:36,267
因此，当我们谈论学 习时，指的是让计算机为所有这些

178
00:12:36,267 --> 00:12:42,003
许多数字找到有效的设 置，以便真正解决手头的问题。

179
00:12:42,003 --> 00:12:46,791
一个既有趣又有点可怕 的思想实验是想象坐下来手

180
00:12:46,791 --> 00:12:51,580
动设置所有这些权重和偏差 ，有目的地调整数字，

181
00:12:51,580 --> 00:12:56,576
以便第二层拾取边缘，第三层 拾取模式， ETC。

182
00:12:56,576 --> 00:12:59,398
我个人认为这令人满意，而不仅仅

183
00:12:59,398 --> 00:13:04,161
是将网络视为一个完全的黑匣子，因为当网络没有按照您预

184
00:13:04,161 --> 00:13:08,923
期的方式运行时，如果您已经与这些权重和偏差的实际含义

185
00:13:08,923 --> 00:13:14,038
建立了一些关系，您有一个开始尝试如何改变结构以进行 改进。

186
00:13:14,038 --> 00:13:18,528
或者，当网络确实工作，但不是出于您可能期望的原因

187
00:13:18,528 --> 00:13:23,558
时，深入研究权重和偏差的作用是挑战您的假设并真正揭示可

188
00:13:23,558 --> 00:13:26,252
能解决方案的全部空间的好方法。

189
00:13:26,252 --> 00:13:32,234
顺便说一句，这里的实际功 能写起来有点麻烦，你不觉得吗？

190
00:13:32,234 --> 00:13:37,130
因此，让我向您展示一种更 紧凑的表示这些连接的方式。

191
00:13:37,130 --> 00:13:41,210
如果您选择阅读更多有关神经网络的内容 ，您会看到这样的结果。

192
00:13:41,210 --> 00:13:46,288
将一层的所有激活组织为一列作为向量 。

193
00:13:46,288 --> 00:13:52,447
然后将所有权重组织为一个矩阵，其中该矩阵的每一

194
00:13:52,447 --> 00:13:58,093
行对应于一层与下一层中特定神经元之间的连接。

195
00:13:58,093 --> 00:14:04,235
这意味 着根据这些权重求第一层激活的加权和对应

196
00:14:04,235 --> 00:14:09,865
于我们左边所有内容的矩阵向量乘积中的一项 。

197
00:14:09,865 --> 00:14:16,172
顺便说一句，机器学习的大部分内容都归结为对线性代数的良好

198
00:14:16,172 --> 00:14:22,479
掌握，因此对于任何想要对矩阵以及矩阵向量乘法的含义有很好的

199
00:14:22,479 --> 00:14:29,004
视 觉理解的人，请看一下我所做的系列线性代数，特别是第三章。

200
00:14:29,004 --> 00:14:33,549
回到 我们的表达式，我们不是谈论将偏差独立地添加到这

201
00:14:33,549 --> 00:14:38,095
些值中的每一个，而 是通过将所有这些偏差组织到一个向

202
00:14:38,095 --> 00:14:42,816
量中，并将整个向量添加到前一个矩 阵向量乘积来表示它。

203
00:14:42,816 --> 00:14:46,969
然后，作为最后一步，我将在此处的外部包裹一 个

204
00:14:46,969 --> 00:14:50,429
sigmoid 函数，这应该表示您要将

205
00:14:50,429 --> 00:14:55,274
sigmoid 函数 应用于内部结果向量的每个特定组件。

206
00:14:55,274 --> 00:15:00,441
因此，一旦你写下这个权重矩 阵和这些向量作为它们自己的

207
00:15:00,441 --> 00:15:03,694
符号，你就可以用一个非常紧凑和整

208
00:15:03,694 --> 00:15:09,435
洁的小表达式来传达从一层到下一层的激活的完整转换，这使得相

209
00:15:09,435 --> 00:15:14,985
关代码变得更加简单和方便。速度要快得多，因为许多库都优化

210
00:15:14,985 --> 00:15:16,133
了矩阵乘法。

211
00:15:16,133 --> 00:15:21,400
还记得我之前说过这些神经元只是保存数字的东西吗？

212
00:15:21,400 --> 00:15:26,869
当然，它们保存的具体数字取决于您输入的图像，因此将

213
00:15:26,869 --> 00:15:32,338
每个神经元视为一个函数实际上更准确，该函数接收前一

214
00:15:32,338 --> 00:15:38,439
层中所有神经元的输出，并吐出一个0 到 1 之间 的数字。

215
00:15:38,439 --> 00:15:42,704
实际上，整个网络只是一个函数，它接受 784

216
00:15:42,704 --> 00:15:47,154
个数字作为 输入，并输出 10 个数字作为输出。

217
00:15:47,154 --> 00:15:50,765
这是一个极其复杂的函数，涉及 13,000

218
00:15:50,765 --> 00:15:54,539
个参数，这些参数以这些权重和偏差的形式出现，

219
00:15:54,539 --> 00:15:59,134
并根据某些模式进行选取，并且涉及迭代许多矩阵向量乘积和

220
00:15:59,134 --> 00:16:03,237
si gmoid 压缩函数，但它仍然只是一个函数。

221
00:16:03,237 --> 00:16:06,878
从某种程度上来说，它 看起来很复杂，这让人感到安心。

222
00:16:06,878 --> 00:16:09,924
我的意思是，如果它再简单一点，我们对它

223
00:16:09,924 --> 00:16:12,818
能够应对识别数字的挑战还有什么希望呢？

224
00:16:12,818 --> 00:16:14,920
它如何应对这一挑战？

225
00:16:14,920 --> 00:16:19,320
该网络如何仅通过查看数据来学习适当的权重和偏差？

226
00:16:19,320 --> 00:16:22,689
这就是我将在下一个视频中展示的内容，我还

227
00:16:22,689 --> 00:16:26,227
将进一步深入了解这个 特定网络的真正作用。

228
00:16:26,227 --> 00:16:29,709
现在我想我应该说订阅以便在该视频或任何新视

229
00:16:29,709 --> 00:16:33,507
频发布时随时收到通知，但实际上你们中的大多数人实

230
00:16:33,507 --> 00:16:37,622
际上并没有收到来 自 YouTube 的通知，是吗？

231
00:16:37,622 --> 00:16:42,581
也许更诚实地说，我应该说 订阅，这样 YouTube

232
00:16:42,581 --> 00:16:47,907
推荐算法的神经网络就会相信 您希望看到该频道的内容被推荐给

233
00:16:47,907 --> 00:16:48,275
您。

234
00:16:48,275 --> 00:16:49,800
无论如何，请继续关注更多信息。

235
00:16:49,800 --> 00:16:53,634
非常感谢 Patreon 上支持这些视频的所有人。

236
00:16:53,634 --> 00:16:58,017
今年夏天我在概率 系列上的进展有点慢，但在这个项目

237
00:16:58,017 --> 00:17:02,400
之后我会重新开始，所以 顾客们可以在那里留意更新。

238
00:17:02,400 --> 00:17:04,966
最后，我邀请 Leesha Lee

239
00:17:04,966 --> 00:17:08,388
进行深度学习理论方面的博士研究，目前在一家名为

240
00:17:08,388 --> 00:17:10,955
Amplify P artners

241
00:17:10,955 --> 00:17:14,520
的风险投资公司工作，该公司为该视频提供了部分资金。

242
00:17:14,520 --> 00:17:18,005
Leesha，我认为我们应该快速提出的一件事是这个

243
00:17:18,005 --> 00:17:19,480
sigmoid 函数。

244
00:17:19,480 --> 00:17:24,674
据我了解，早期网络使用它来将相关加权和压缩到零和一之间的区

245
00:17:24,674 --> 00:17:30,048
间，你知道这种生物类比的动机是神经元要么不活跃，要么活跃。

246
00:17:30,048 --> 00:17:30,552
确切地。

247
00:17:30,552 --> 00:17:34,091
但真正使用 sigmoid 的现代网络相对较少。

248
00:17:34,091 --> 00:17:34,392
是的。

249
00:17:34,392 --> 00:17:35,945
这有点老派吧？

250
00:17:35,945 --> 00:17:38,974
是的，或者更确切地说，relu 似 乎更容易训练。

251
00:17:38,974 --> 00:17:42,360
还有relu，relu代表整流线性单元？

252
00:17:42,360 --> 00:17:47,312
是的，在这种函数 中，您只需取最大值为零，而

253
00:17:47,312 --> 00:17:51,187
a 则由您在视频中解释的内容 给出。

254
00:17:51,187 --> 00:18:00,730
我认为这部分是出于对神经元如何激活 或不激活的生物学类比。

255
00:18:00,730 --> 00:18:04,970
因此，如果它通过了某个阈值， 它将成为恒等函数，但

256
00:18:04,970 --> 00:18:09,380
如果它没有通过，那么它就不会被激活，因此它会为零 。

257
00:18:09,380 --> 00:18:11,084
所以这是一种简化。

258
00:18:11,084 --> 00:18:14,770
使用 sigmoid 对训练没有帮助，或

259
00:18:14,770 --> 00:18:18,807
者在某些时候训练起来非常困难，人们只是尝试了

260
00:18:18,807 --> 00:18:24,072
relu，它恰好对这 些令人难以置信的深层神经网络非常有效。

261
00:18:24,072 --> 00:18:26,120
好的，谢谢莉莎。

