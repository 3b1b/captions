1
00:00:04,219 --> 00:00:05,400
这是一个3。

2
00:00:06,060 --> 00:00:10,064
它以 28x28 像素的极低分辨率编写和渲染，

3
00:00:10,064 --> 00:00:13,720
但你的 大脑可以毫不费力地将其识别为 3。

4
00:00:14,340 --> 00:00:16,650
我希望你能花点时间体会 一下大脑可

5
00:00:16,650 --> 00:00:18,960
以如此轻松地做到这一点是多么疯狂。

6
00:00:19,700 --> 00:00:24,354
我的意思是 ，这个、这个和这个也可以被识别为 3 秒，

7
00:00:24,354 --> 00:00:28,320
尽管每个像素的具 体值与下一个图像有很大不同。

8
00:00:28,900 --> 00:00:34,143
当您看到这 3 时，您眼睛中 发射的特定光敏细胞与您看到这 

9
00:00:34,143 --> 00:00:36,940
3 时发射的光敏细胞非常不 同。

10
00:00:37,520 --> 00:00:42,792
但你那疯狂聪明的视觉皮层中的某些东西将这些图像解析为 

11
00:00:42,792 --> 00:00:48,260
代表相同的想法，同时将其他图像识别为它们自己独特的想法。

12
00:00:49,220 --> 00:00:53,177
但如果我告诉你，嘿，坐下来为我写一个程序，

13
00:00:53,177 --> 00:00:57,888
它接受 28x28 的网 格并输出 0 到 10 

14
00:00:57,888 --> 00:01:02,222
之间的单个数字，告诉你它认为这个数字是什 么，

15
00:01:02,222 --> 00:01:06,180
那么这个任务就会从可笑的琐碎变成极其困难。

16
00:01:07,160 --> 00:01:10,900
除非你一直生活在 岩石下，否则我认为我几乎不需要激发

17
00:01:10,900 --> 00:01:14,640
机器学习和神经网络对 于现在和未来的相关性和重要性。

18
00:01:15,120 --> 00:01:17,342
但我在这里想做的是在没有背景的情 

19
00:01:17,342 --> 00:01:19,564
况下向您展示神经网络实际上是什么，

20
00:01:19,564 --> 00:01:22,963
并帮助您可视化它正在做什么，而不是作 为一个流行语，

21
00:01:22,963 --> 00:01:24,140
而是作为一段数学。

22
00:01:24,140 --> 00:01:29,240
我的希望只是，当你读完或听到神 经网络引用-非引用学习时，

23
00:01:29,240 --> 00:01:34,340
你会感觉结构本身是有动机的 ，并且感觉你知道它意味着什么。

24
00:01:35,360 --> 00:01:40,260
本视频将专门讨论 其结构部分，下面的视频将讨论学习问题。

25
00:01:40,960 --> 00:01:46,040
我们要做的就是建立一个可以学习识别手写数字的神经网络 。

26
00:01:49,360 --> 00:01:51,829
这是介绍该主题的一个有点经典的示例，

27
00:01:51,829 --> 00:01:55,122
我很高兴在这里坚持 现状，因为在两个视频的末尾，

28
00:01:55,122 --> 00:01:58,964
我想向您指出一些很好的资源，您 可以在其中了解更多信息，

29
00:01:58,964 --> 00:02:03,080
以及您可以下载执行此操作的代码并在 您自己的计算机上使用它。

30
00:02:05,040 --> 00:02:10,949
神经网络有很多变体，近年来， 对这些变体的研究蓬勃发展，

31
00:02:10,949 --> 00:02:16,858
但在这两个介绍性视频 中，你和我将只看最简单的普通形式，

32
00:02:16,858 --> 00:02:19,180
没有任何额外的 装饰。

33
00:02:19,860 --> 00:02:24,327
这是理解任何更强大的现代变体的必要先决 条件，

34
00:02:24,327 --> 00:02:28,600
相信我，它仍然有很多复杂性需要我们去思考 。

35
00:02:29,120 --> 00:02:33,395
但即使是这种最简单的形式，它也可以学习识别手写数字，

36
00:02:33,395 --> 00:02:36,520
这对 于计算机来说是一件非常酷的事情。

37
00:02:37,480 --> 00:02:42,280
同时你会发现它确实没 有达到我们的一些希望。

38
00:02:43,380 --> 00:02:48,500
顾名思义，神经网络受到大脑 的启发，但让我们来分解一下。

39
00:02:48,520 --> 00:02:51,660
什么是神经元，它们以什么方式连接 在一起？

40
00:02:52,500 --> 00:02:57,678
现在，当我说神经元时，我想让你想到的是一个包含数字的东西 ，

41
00:02:57,678 --> 00:03:00,440
特别是 0 到 1 之间的数字。

42
00:03:00,680 --> 00:03:02,000
确实仅此而已。

43
00:03:02,000 --> 00:03:07,072
例如，网络 从对应于输入图像的 28×28 

44
00:03:07,072 --> 00:03:11,453
像素中的每一个像素的一堆神经元 开始，

45
00:03:11,453 --> 00:03:14,220
总共 784 个神经元。

46
00:03:14,700 --> 00:03:19,960
其中每一个都包含一个数字，表示 相应像素的灰度值，

47
00:03:19,960 --> 00:03:24,380
范围从黑色像素的 0 到白色像素的 1 。

48
00:03:25,300 --> 00:03:30,438
神经元内部的这个数字称为它的激活值，您可能想到的图像是 ，

49
00:03:30,438 --> 00:03:34,160
当每个神经元的激活值很高时，它就会被点亮。

50
00:03:36,720 --> 00:03:41,860
因此，所有这 784 个神经元构成了我们网络的第一层。

51
00:03:46,500 --> 00:03:49,592
现在跳到最后一层，它有 1 0 个神经元，

52
00:03:49,592 --> 00:03:51,360
每个神经元代表一个数字。

53
00:03:52,040 --> 00:03:56,072
这些神经元的激活（同 样是 0 到 1 

54
00:03:56,072 --> 00:04:02,120
之间的某个数字）表示系统认为给定图 像与给定数字的对应程度。

55
00:04:03,040 --> 00:04:09,780
中间还有一些称为隐藏层的 层，目前这应该只是一个巨大的问号，

56
00:04:09,780 --> 00:04:13,600
不知道如 何处理这个数字识别过程。

57
00:04:14,260 --> 00:04:16,779
在这个网络中，我选择了两个隐 藏层，

58
00:04:16,779 --> 00:04:20,560
每个隐藏层有 16 个神经元，诚然，这是一种任意选择。

59
00:04:21,019 --> 00:04:25,122
说实话，我 选择了两层，基于我想如何在短时间内激发结构，

60
00:04:25,122 --> 00:04:28,200
而 16 层， 这只是一个适合屏幕的数字。

61
00:04:28,780 --> 00:04:32,340
实际上，这里有很大的空间可以对特 定结构进行实验。

62
00:04:33,020 --> 00:04:38,480
网络运行的方式，一层的激活决定下 一层的激活。

63
00:04:39,200 --> 00:04:42,952
当然，作为一种信息处理机制，网 

64
00:04:42,952 --> 00:04:48,580
络的核心归结为一层的激活如何引起下一层的 激活。

65
00:04:49,140 --> 00:04:53,881
它大致类似于神经元生物网络中某些神经元群的放 

66
00:04:53,881 --> 00:04:57,180
电导致某些其他神经元放电的方式。

67
00:04:58,120 --> 00:05:01,786
现在我在这里展示的网络已 经接受过识别数字的训练，

68
00:05:01,786 --> 00:05:03,400
让我向您展示我的意思。

69
00:05:03,640 --> 00:05:06,561
这意味着，如 果您输入一张图像，

70
00:05:06,561 --> 00:05:12,038
根据图像中每个像素的亮度照亮输入层的所有 784 个神经元，

71
00:05:12,038 --> 00:05:16,602
则该激活模式会在下一层中导致一些非常特定的 模式，

72
00:05:16,602 --> 00:05:22,080
从而在下一层中导致一些模式它最终在输出层中给出了一些模式 。

73
00:05:22,560 --> 00:05:26,090
输出层最亮的神经元是网络的选择，

74
00:05:26,090 --> 00:05:29,400
可以说，该图 像代表什么数字。

75
00:05:32,560 --> 00:05:37,452
在深入了解一层如何影响下一层或训 练如何进行之前，

76
00:05:37,452 --> 00:05:41,171
我们先来谈谈为什么期望这样的分层结构 

77
00:05:41,171 --> 00:05:43,520
能够智能地运行是合理的。

78
00:05:44,060 --> 00:05:45,220
我们在这里期待什么？

79
00:05:45,400 --> 00:05:47,600
这些中间层可能 做的事情的最大希望是什么？

80
00:05:48,920 --> 00:05:53,520
好吧，当你或我识别数字时，我们会将各种组件 拼凑在一起。

81
00:05:54,200 --> 00:05:56,820
9 的顶部有一个环，右侧有一条线。

82
00:05:57,380 --> 00:06:01,180
8 也有一个顶部环， 但它与另一个底部环配对。

83
00:06:01,980 --> 00:06:06,820
4 基本上可以分为三行，诸 如此类。

84
00:06:07,600 --> 00:06:12,123
现在在一个完美的世界中，我们可能希望倒数第二层中的 

85
00:06:12,123 --> 00:06:15,255
每个神经元都与这些子组件之一相对应，

86
00:06:15,255 --> 00:06:19,952
每当您输入带有顶部循环（例 如 9 或 8）的图像时，

87
00:06:19,952 --> 00:06:23,780
都会有一些其激活值接近 1 的特定神 经元。

88
00:06:24,500 --> 00:06:28,870
我并不是指这种特定的像素循环，而是希望任何朝向顶部 

89
00:06:28,870 --> 00:06:31,560
的普遍循环模式都会引发该神经元。

90
00:06:32,440 --> 00:06:36,240
这样，从第三层到最 后一层只需要学

91
00:06:36,240 --> 00:06:40,040
习哪个子组件组合对应于哪个数 字。

92
00:06:41,000 --> 00:06:44,860
当然，这只是解决问题，因为您如何识别这些子 组件，

93
00:06:44,860 --> 00:06:47,640
甚至如何了解正确的子组件应该是什么？

94
00:06:48,060 --> 00:06:50,938
我什至还 没有讨论一层如何影响下一层，

95
00:06:50,938 --> 00:06:53,060
但请跟我一起讨论一下这一层。

96
00:06:53,680 --> 00:06:56,680
识别循环也可以分解为子问题。

97
00:06:57,280 --> 00:07:02,280
一种合理的方法是首 先识别构成它的各种小边缘。

98
00:07:02,280 --> 00:07:06,230
类似地，一条长线，就像您 在数字 1、4 

99
00:07:06,230 --> 00:07:10,369
或 7 中看到的那种，那实际上只是一条长边，

100
00:07:10,369 --> 00:07:14,320
或者 您可能将其视为几个较小边的某种图案。

101
00:07:15,140 --> 00:07:18,821
因此，也许我们希望网络第二 层中的

102
00:07:18,821 --> 00:07:22,720
每个神经元都与各种相关的小边相对应。

103
00:07:23,540 --> 00:07:27,409
也许当像这样的 图像出现时，它会照亮与大约 

104
00:07:27,409 --> 00:07:31,454
8 到 10 个特定小边缘相关的所有 神经元，

105
00:07:31,454 --> 00:07:35,850
这些边缘又会照亮与上环和一条长垂直线相关的神经元，

106
00:07:35,850 --> 00:07:39,720
而这些 神经元又会照亮与 9 相关的神经元。

107
00:07:40,680 --> 00:07:44,060
这是否是我们最终网络实际上 所做的事情是另一个问题，

108
00:07:44,060 --> 00:07:47,180
一旦我们了解如何训练网络，我就会回到这个 问题。

109
00:07:47,500 --> 00:07:52,540
但这是我们可能拥有的一个希望，一种像这样的分层结构的目 标。

110
00:07:53,160 --> 00:07:57,598
此外，您可以想象能够检测这样的边缘和图案对于 

111
00:07:57,598 --> 00:08:00,300
其他图像识别任务将非常有用。

112
00:08:00,880 --> 00:08:05,146
甚至除了图像识别之外， 您可能还想做各种各样的智能事情，

113
00:08:05,146 --> 00:08:07,280
这些事情可以分解为抽象 层。

114
00:08:08,040 --> 00:08:11,942
例如，解析语音涉及获取原始音频并挑选出不同的声音，

115
00:08:11,942 --> 00:08:16,469
这些声音 结合起来形成某些音节，这些音节结合起来形成单词，

116
00:08:16,469 --> 00:08:20,060
这些声音结合起来 组成短语和更抽象的想法等等。

117
00:08:21,100 --> 00:08:24,040
但回到这些实际上是如何工作的，想 

118
00:08:24,040 --> 00:08:28,882
象一下你自己现在正在设计一层中的激活如何准确地确定下一 

119
00:08:28,882 --> 00:08:29,920
层中的激活。

120
00:08:30,860 --> 00:08:35,006
我们的目标是拥有某种机制，可以将像素组合成边缘，

121
00:08:35,006 --> 00:08:38,980
或者 将边缘组合成图案，或者将图案组合成数字。

122
00:08:39,440 --> 00:08:44,911
放大一个非常具体的示 例，假设希望第二层中的一

123
00:08:44,911 --> 00:08:50,620
个特定神经元能够识别图像在 此区域中是否有边缘。

124
00:08:51,440 --> 00:08:55,100
当前的问题是，网络应该具有哪些 参数？

125
00:08:55,640 --> 00:09:00,462
您应该能够调整哪些转盘和旋钮，以便具有足够的表现力来潜在 

126
00:09:00,462 --> 00:09:03,456
地捕捉这种图案，或任何其他像素图案，

127
00:09:03,456 --> 00:09:07,780
或多个边缘可以形成循环的图 案，以及其他类似的东西？

128
00:09:08,720 --> 00:09:12,052
好吧，我们要做的就是为我们的神经元和 

129
00:09:12,052 --> 00:09:15,560
第一层神经元之间的每个连接分配一个权重。

130
00:09:16,320 --> 00:09:17,700
这些重量只是数 字。

131
00:09:18,540 --> 00:09:24,152
然后从第一层获取所有这些激活并根据这些权重计算它 

132
00:09:24,152 --> 00:09:25,500
们的加权和。

133
00:09:27,700 --> 00:09:32,975
我发现将这些权重视为被组织成自己的小网 格是有帮助的，

134
00:09:32,975 --> 00:09:38,641
我将使用绿色像素来表示正权重，使用红 色像素来表示负权重，

135
00:09:38,641 --> 00:09:42,940
其中该像素的亮度是一些对重量值的 宽松描述。

136
00:09:42,940 --> 00:09:46,884
如果我们将与几乎所有像素相关的权重设为零（ 

137
00:09:46,884 --> 00:09:50,290
除了我们关心的该区域中的一些正权重），

138
00:09:50,290 --> 00:09:55,130
那么对所有像素 值进行加权和实际上相当于将刚刚在其中的

139
00:09:55,130 --> 00:09:57,820
像素值相加。我们 关心的地区。

140
00:09:59,140 --> 00:10:02,281
如果您确实想知道这里是否有边缘，

141
00:10:02,281 --> 00:10:06,600
您 可能会做的是与周围像素相关的一些负权重。

142
00:10:07,480 --> 00:10:12,700
当中间 的像素较亮而周围的像素较暗时，总和最大。

143
00:10:14,260 --> 00:10:18,071
当你计算这样的加权和时，你可能会得到任何数字，

144
00:10:18,071 --> 00:10:22,380
但对于这个网络，我 们想要的是激活值是 0 到 1 

145
00:10:22,380 --> 00:10:23,540
之间的某个值。

146
00:10:24,120 --> 00:10:28,219
因此，常见的做法是 将这个加权和注入某个函数，

147
00:10:28,219 --> 00:10:32,140
将实数轴压缩到 0 到 1 之间的 范围内。

148
00:10:32,460 --> 00:10:35,747
执行此操作的常见函数称为 sigmoid 函数，

149
00:10:35,747 --> 00:10:36,980
也称为 逻辑曲线。

150
00:10:36,980 --> 00:10:42,992
基本上，非常负的输入最终接近 0，非常正的输入最终接近 1，

151
00:10:42,992 --> 00:10:46,600
并且它只是在输入 0 附近稳定增加。

152
00:10:49,120 --> 00:10:56,360
所以这里神经元的激活 基本上是衡量相关加权和的正值程度。

153
00:10:57,540 --> 00:11:01,880
但也许你并不希望 神经元在加权和大于0时亮起。

154
00:11:02,280 --> 00:11:06,360
也许您只希望当总和大于 1 0 时它才处于活动状态。

155
00:11:06,840 --> 00:11:10,260
也就是说，您希望对其不活动有一些偏差。

156
00:11:11,380 --> 00:11:15,366
然后我们 要做的就是在这个加权和中添加一些其他数字，

157
00:11:15,366 --> 00:11:19,660
比如负 10，然后将其插 入 sigmoid 压缩函数。

158
00:11:20,580 --> 00:11:22,440
这个额外的数字称为偏差。

159
00:11:23,460 --> 00:11:29,031
因此 ，权重告诉您第二层中的该神经元正在接收什么像素模式，

160
00:11:29,031 --> 00:11:33,450
而偏 差则告诉您在神经元开始有意义地活动之前，

161
00:11:33,450 --> 00:11:35,180
加权和需要多高 。

162
00:11:36,120 --> 00:11:37,680
那只是一个神经元。

163
00:11:38,280 --> 00:11:42,631
该层中的每个其他神经元都将连接到第一层的所 

164
00:11:42,631 --> 00:11:46,785
有 784 个像素神经元，并且这 784 

165
00:11:46,785 --> 00:11:50,940
个连接中的每个连接都有其 自己的关联权重。

166
00:11:51,600 --> 00:11:54,933
此外，每个值都有一些偏差，即在用 sigmoid 

167
00:11:54,933 --> 00:11:57,600
压缩 之前将其添加到加权和中的其他数字。

168
00:11:58,110 --> 00:11:59,540
这需要考虑很多！

169
00:11:59,960 --> 00:12:03,007
对于这个包含 16 个神经元的隐藏层，

170
00:12:03,007 --> 00:12:06,376
总共有 784 个权重乘以 16 个权重，

171
00:12:06,376 --> 00:12:07,980
以及 16 个偏差。

172
00:12:08,840 --> 00:12:11,940
所有这些 只是从第一层到第二层的连接。

173
00:12:12,520 --> 00:12:17,340
其他层之间的连接 也有一堆与之相关的权重和偏差。

174
00:12:18,340 --> 00:12:23,800
总而言之，这个网络 几乎有 13,000 个总权重和偏差。

175
00:12:23,800 --> 00:12:27,690
13,000 个旋钮和转盘可 以进行调整和转动，

176
00:12:27,690 --> 00:12:29,960
以使该网络以不同的方式运行。

177
00:12:31,040 --> 00:12:36,200
因此，当我们谈论学 习时，指的是让计算机为所有这些

178
00:12:36,200 --> 00:12:41,360
许多数字找到有效的设 置，以便真正解决手头的问题。

179
00:12:42,620 --> 00:12:47,206
一个既有趣又有点可怕 的思想实验是想象坐下来手

180
00:12:47,206 --> 00:12:51,793
动设置所有这些权重和偏差 ，有目的地调整数字，

181
00:12:51,793 --> 00:12:56,580
以便第二层拾取边缘，第三层 拾取模式， ETC。

182
00:12:56,980 --> 00:12:59,759
我个人认为这令人满意，而不仅仅 

183
00:12:59,759 --> 00:13:04,450
是将网络视为一个完全的黑匣子，因为当网络没有按照您预 

184
00:13:04,450 --> 00:13:09,141
期的方式运行时，如果您已经与这些权重和偏差的实际含义 

185
00:13:09,141 --> 00:13:14,180
建立了一些关系，您有一个开始尝试如何改变结构以进行 改进。

186
00:13:14,960 --> 00:13:19,272
或者，当网络确实工作，但不是出于您可能期望的原因 时，

187
00:13:19,272 --> 00:13:23,424
深入研究权重和偏差的作用是挑战您的假设并真正揭示可 

188
00:13:23,424 --> 00:13:25,820
能解决方案的全部空间的好方法。

189
00:13:26,840 --> 00:13:30,680
顺便说一句，这里的实际功 能写起来有点麻烦，你不觉得吗？

190
00:13:32,500 --> 00:13:37,140
因此，让我向您展示一种更 紧凑的表示这些连接的方式。

191
00:13:37,660 --> 00:13:40,520
如果您选择阅读更多有关神经网络的内容 ，您会看到这样的结果。

192
00:13:41,380 --> 00:13:44,780
将一层的所有激活组织为一列作为向量 。

193
00:13:44,780 --> 00:13:52,710
然后将所有权重组织为一个矩阵，其中该矩阵的每一 

194
00:13:52,710 --> 00:13:59,980
行对应于一层与下一层中特定神经元之间的连接。

195
00:13:59,980 --> 00:14:07,701
这意味 着根据这些权重求第一层激活的加权和对应 

196
00:14:07,701 --> 00:14:14,780
于我们左边所有内容的矩阵向量乘积中的一项 。

197
00:14:14,780 --> 00:14:19,334
顺便说一句，机器学习的大部分内容都归结为对线性代数的良好 

198
00:14:19,334 --> 00:14:23,888
掌握，因此对于任何想要对矩阵以及矩阵向量乘法的含义有很好的

199
00:14:23,888 --> 00:14:28,600
视 觉理解的人，请看一下我所做的系列线性代数，特别是第三章。

200
00:14:29,240 --> 00:14:33,538
回到 我们的表达式，我们不是谈论将偏差独立地添加到这

201
00:14:33,538 --> 00:14:38,332
些值中的每一个，而 是通过将所有这些偏差组织到一个向量中，

202
00:14:38,332 --> 00:14:42,300
并将整个向量添加到前一个矩 阵向量乘积来表示它。

203
00:14:43,280 --> 00:14:47,100
然后，作为最后一步，我将在此处的外部包裹一 个 

204
00:14:47,100 --> 00:14:50,283
sigmoid 函数，这应该表示您要将 

205
00:14:50,283 --> 00:14:54,740
sigmoid 函数 应用于内部结果向量的每个特定组件。

206
00:14:55,940 --> 00:15:01,367
因此，一旦你写下这个权重矩 阵和这些向量作为它们自己的符号，

207
00:15:01,367 --> 00:15:06,252
你就可以用一个非常紧凑和整 洁的小表达式来传达从一层到

208
00:15:06,252 --> 00:15:11,679
下一层的激活的完整转换，这使得相 关代码变得更加简单和方便。

209
00:15:11,679 --> 00:15:15,660
速度要快得多，因为许多库都优化 了矩阵乘法。

210
00:15:17,820 --> 00:15:21,460
还记得我之前说过这些神经元只是保存数字的东西吗？

211
00:15:22,220 --> 00:15:26,598
当然，它们保存的具体数字取决于您输入的图像，

212
00:15:26,598 --> 00:15:30,976
因此将 每个神经元视为一个函数实际上更准确，

213
00:15:30,976 --> 00:15:34,757
该函数接收前一 层中所有神经元的输出，

214
00:15:34,757 --> 00:15:38,340
并吐出一个0 到 1 之间 的数字。

215
00:15:39,200 --> 00:15:43,046
实际上，整个网络只是一个函数，它接受 784 

216
00:15:43,046 --> 00:15:47,060
个数字作为 输入，并输出 10 个数字作为输出。

217
00:15:47,560 --> 00:15:51,560
这是一个极其复杂的函数，涉及 13,000 个参数，

218
00:15:51,560 --> 00:15:54,330
这些参数以这些权重和偏差的形式出现，

219
00:15:54,330 --> 00:15:58,793
 并根据某些模式进行选取，并且涉及迭代许多矩阵向量乘积和 

220
00:15:58,793 --> 00:16:02,640
si gmoid 压缩函数，但它仍然只是一个函数。

221
00:16:03,400 --> 00:16:06,660
从某种程度上来说，它 看起来很复杂，这让人感到安心。

222
00:16:07,340 --> 00:16:09,873
我的意思是，如果它再简单一点，我们对它 

223
00:16:09,873 --> 00:16:12,280
能够应对识别数字的挑战还有什么希望呢？

224
00:16:13,340 --> 00:16:14,700
它如何应对这一挑战？

225
00:16:15,080 --> 00:16:19,360
该网络如何仅通过查看数据来学习适当的权重和偏差？

226
00:16:20,140 --> 00:16:22,598
这就是我将在下一个视频中展示的内容，

227
00:16:22,598 --> 00:16:25,740
我还将进一步深入了解这个 特定网络的真正作用。

228
00:16:25,740 --> 00:16:29,308
现在我想我应该说订阅以便在该视频或任何新视 

229
00:16:29,308 --> 00:16:33,202
频发布时随时收到通知，但实际上你们中的大多数人实

230
00:16:33,202 --> 00:16:37,420
际上并没有收到来 自 YouTube 的通知，是吗？

231
00:16:38,020 --> 00:16:40,740
也许更诚实地说，我应该说 订阅，

232
00:16:40,740 --> 00:16:44,990
这样 YouTube 推荐算法的神经网络就会相信 

233
00:16:44,990 --> 00:16:47,880
您希望看到该频道的内容被推荐给您。

234
00:16:48,560 --> 00:16:49,940
无论如何，请继续关注更多信息。

235
00:16:50,760 --> 00:16:53,500
非常感谢 Patreon 上支持这些视频的所有人。

236
00:16:54,000 --> 00:16:57,002
今年夏天我在概率 系列上的进展有点慢，

237
00:16:57,002 --> 00:16:59,846
但在这个项目之后我会重新开始，所以 

238
00:16:59,846 --> 00:17:01,900
顾客们可以在那里留意更新。

239
00:17:03,600 --> 00:17:05,933
最后，我邀请 Leesha Lee 

240
00:17:05,933 --> 00:17:08,007
进行深度学习理论方面的博士研究，

241
00:17:08,007 --> 00:17:11,378
目前在一家名为 Amplify P artners 

242
00:17:11,378 --> 00:17:14,619
的风险投资公司工作，该公司为该视频提供了部分资金。

243
00:17:15,460 --> 00:17:18,031
Leesha，我认为我们应该快速提出的一件事是这个 

244
00:17:18,031 --> 00:17:19,119
sigmoid 函数。

245
00:17:19,700 --> 00:17:24,684
据我了解，早期网络使用它来将相关加权和压缩到零和一之间的区

246
00:17:24,684 --> 00:17:29,840
间，你知道这种生物类比的动机是神经元要么不活跃，要么活跃。 

247
00:17:30,280 --> 00:17:30,300
确切地。 

248
00:17:30,560 --> 00:17:34,040
但真正使用 sigmoid 的现代网络相对较少。 

249
00:17:34,320 --> 00:17:34,320
是的。

250
00:17:34,440 --> 00:17:35,540
这有点老派吧？

251
00:17:35,760 --> 00:17:38,580
是的，或者更确切地说，relu 似 乎更容易训练。

252
00:17:38,580 --> 00:17:42,340
还有relu，relu代表整流线性单元？

253
00:17:42,680 --> 00:17:46,900
是的，在这种函数 中，您只需取最大值为零，

254
00:17:46,900 --> 00:17:50,920
而 a 则由您在视频中解释的内容 给出。

255
00:17:50,920 --> 00:18:01,360
我认为这部分是出于对神经元如何激活 或不激活的生物学类比。

256
00:18:01,360 --> 00:18:05,171
因此，如果它通过了某个阈值， 它将成为恒等函数，

257
00:18:05,171 --> 00:18:09,460
但如果它没有通过，那么它就不会被激活，因此它会为零 。

258
00:18:09,460 --> 00:18:10,840
所以这是一种简化。

259
00:18:11,160 --> 00:18:14,615
使用 sigmoid 对训练没有帮助，

260
00:18:14,615 --> 00:18:20,072
或 者在某些时候训练起来非常困难，人们只是尝试了 relu，

261
00:18:20,072 --> 00:18:24,620
它恰好对这 些令人难以置信的深层神经网络非常有效。

262
00:18:25,100 --> 00:18:25,640
好的，谢谢莉莎。

