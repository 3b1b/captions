1
00:00:04,220 --> 00:00:05,400
这是一个 3.

2
00:00:06,060 --> 00:00:10,586
虽然文字潦草，分辨率也极低，仅为 28x28 像素，

3
00:00:10,586 --> 00:00:13,720
但你的大脑却能准确地将其识别为 3。

4
00:00:14,340 --> 00:00:16,337
我希望你们花一点时间来体会一下，

5
00:00:16,337 --> 00:00:18,960
大脑能如此毫不费力地做到这一点是多么疯狂。

6
00:00:19,700 --> 00:00:24,197
我的意思是，这个、这个和这个也可以被识别为 3，

7
00:00:24,197 --> 00:00:28,320
尽管每幅图像中每个像素的具体数值都大不相同。

8
00:00:28,900 --> 00:00:34,199
当你看到这个 3 时，你眼睛中特定的感光细胞与你看到这个 

9
00:00:34,199 --> 00:00:36,940
3 时的感光细胞是完全不同的。

10
00:00:37,520 --> 00:00:40,723
但是，在你那疯狂聪明的视觉皮层中，

11
00:00:40,723 --> 00:00:44,680
有一些东西将这些图像解析为代表同一个想法，

12
00:00:44,680 --> 00:00:48,260
同时又将其他图像识别为各自不同的想法。

13
00:00:49,220 --> 00:00:53,082
但是，如果我告诉你，嘿，坐下来给我写一个程序，

14
00:00:53,082 --> 00:00:56,440
像这样输入一个 28x28 像素的网格，

15
00:00:56,440 --> 00:00:59,799
然后输出一个 0 到 10 之间的数字，

16
00:00:59,799 --> 00:01:03,997
告诉你它认为这个数字是多少，那么这项任务就会从滑稽

17
00:01:03,997 --> 00:01:06,180
的琐碎变成令人生畏的困难。

18
00:01:07,160 --> 00:01:10,900
除非你一直生活在岩石之下，否则我想我几乎没有必要强

19
00:01:10,900 --> 00:01:14,640
调机器学习和神经网络对当前和未来的相关性和重要性。

20
00:01:15,120 --> 00:01:17,655
但我想做的是，在没有任何背景的情况下，

21
00:01:17,655 --> 00:01:21,524
向你展示神经网络到底是什么，并帮助你直观地了解它在做什么，

22
00:01:21,524 --> 00:01:24,460
而不是把它当作一个流行词，而是当作一门数学。

23
00:01:25,020 --> 00:01:29,197
我的希望是，当你读到或听到关于神经网络学习的引号时，

24
00:01:29,197 --> 00:01:31,929
你能感觉到这个结构本身是有动力的，

25
00:01:31,929 --> 00:01:34,340
并感觉到自己知道它意味着什么。

26
00:01:35,360 --> 00:01:40,260
本视频将专门介绍其中的结构部分，接下来的视频将讨论学习问题。

27
00:01:40,960 --> 00:01:46,040
我们要做的就是组建一个神经网络，让它学会识别手写数字。

28
00:01:49,360 --> 00:01:53,718
这是一个介绍该主题的经典例子，我很乐意在这里保持现状，

29
00:01:53,718 --> 00:01:57,753
因为在这两段视频的最后，我想给你指出几个好的资源，

30
00:01:57,753 --> 00:02:01,143
你可以在那里了解更多信息，也可以下载代码，

31
00:02:01,143 --> 00:02:03,080
在自己的电脑上进行操作。

32
00:02:05,040 --> 00:02:11,467
神经网络有很多很多变体，近年来对这些变体的研究可谓如火如荼，

33
00:02:11,467 --> 00:02:17,680
但在这两段介绍性视频中，你我将看到的只是最简单的普通形式，

34
00:02:17,680 --> 00:02:19,180
不加任何修饰。

35
00:02:19,860 --> 00:02:24,021
这是理解任何更强大的现代变体的必要前提，

36
00:02:24,021 --> 00:02:28,600
相信我，它仍然有很多复杂之处需要我们去思考。

37
00:02:29,120 --> 00:02:33,632
但即使是这种最简单的形式，它也能学会识别手写数字，

38
00:02:33,632 --> 00:02:36,520
这对计算机来说是一件很酷的事情。

39
00:02:37,480 --> 00:02:42,280
同时，你也会看到它是如何辜负了我们对它的一些希望。

40
00:02:43,380 --> 00:02:48,500
顾名思义，神经网络的灵感来源于大脑。

41
00:02:48,520 --> 00:02:51,660
神经元是什么，它们在什么意义上联系在一起？

42
00:02:52,500 --> 00:02:57,199
现在，当我说神经元时，我想让你想到的是一个保存数字的东西，

43
00:02:57,199 --> 00:03:00,440
特别是一个介于 0 和 1 之间的数字。

44
00:03:00,680 --> 00:03:02,560
其实也不过如此。

45
00:03:03,780 --> 00:03:09,453
例如，网络一开始有一堆神经元，分别对应输入图像的 

46
00:03:09,453 --> 00:03:14,220
28x28 像素，总共 784 个神经元。

47
00:03:14,700 --> 00:03:19,652
每个像素都有一个数字，代表相应像素的灰度值，

48
00:03:19,652 --> 00:03:24,380
从 0 代表黑色像素到 1 代表白色像素。

49
00:03:25,300 --> 00:03:30,132
神经元内部的这个数字被称为激活度，你可能会想到，

50
00:03:30,132 --> 00:03:34,160
当激活度为高数时，每个神经元都会被点亮。

51
00:03:36,720 --> 00:03:41,860
因此，所有这 784 个神经元构成了我们网络的第一层。

52
00:03:46,500 --> 00:03:49,693
现在跳转到最后一层，这一层有 10 个神经元，

53
00:03:49,693 --> 00:03:51,360
每个神经元代表一个数字。

54
00:03:52,040 --> 00:03:56,414
这些神经元的激活程度（同样是介于 0 和 1 

55
00:03:56,414 --> 00:04:02,120
之间的某个数字）代表了系统认为特定图像与特定数字对应的程度。

56
00:04:03,040 --> 00:04:09,760
中间还有几层被称为隐藏层，目前这应该只是一个巨大的问号，

57
00:04:09,760 --> 00:04:13,600
究竟如何处理这个识别数字的过程。

58
00:04:14,260 --> 00:04:17,606
在这个网络中，我选择了两个隐藏层，

59
00:04:17,606 --> 00:04:20,560
每个隐藏层有 16 个神经元。

60
00:04:21,019 --> 00:04:25,008
老实说，我之所以选择两层，是因为我想在稍后的时间里激发结构，

61
00:04:25,008 --> 00:04:28,200
而 16 层，只是一个适合在屏幕上显示的好数字。

62
00:04:28,780 --> 00:04:32,340
在实践中，这里的具体结构有很大的试验空间。

63
00:04:33,020 --> 00:04:38,480
网络的运行方式是，一层的激活决定下一层的激活。

64
00:04:39,200 --> 00:04:43,890
当然，作为一种信息处理机制，网络的核心在

65
00:04:43,890 --> 00:04:48,580
于这一层的激活是如何带动下一层的激活的。

66
00:04:49,140 --> 00:04:53,159
这就好比在生物神经元网络中，某些神经

67
00:04:53,159 --> 00:04:57,180
元群的发射会引起其他神经元群的发射。

68
00:04:58,120 --> 00:05:01,504
现在，我在这里展示的网络已经经过了识别数字的训练，

69
00:05:01,504 --> 00:05:03,400
让我向你们展示一下我的意思。

70
00:05:03,640 --> 00:05:08,159
这意味着，如果你输入一幅图像，根据图像中每个像素的

71
00:05:08,159 --> 00:05:11,775
亮度点亮输入层的所有 784 个神经元，

72
00:05:11,775 --> 00:05:16,294
这种激活模式就会在下一层中产生某种非常特殊的模式，

73
00:05:16,294 --> 00:05:19,549
这种模式又会在下一层中产生某种模式，

74
00:05:19,549 --> 00:05:22,080
最后在输出层中产生某种模式。

75
00:05:22,560 --> 00:05:29,400
可以说，输出层中最亮的神经元就是网络选择这个图像代表的数字。

76
00:05:32,560 --> 00:05:37,761
在深入探讨一层如何影响下一层，或者训练如何发挥作用之前，

77
00:05:37,761 --> 00:05:43,148
我们先来谈谈为什么期望这样一个分层结构能够智能地运行是合理

78
00:05:43,148 --> 00:05:43,520
的。

79
00:05:44,060 --> 00:05:45,220
我们在这里期待什么？

80
00:05:45,400 --> 00:05:47,600
这些中间层的最大希望是什么？

81
00:05:48,920 --> 00:05:53,520
好吧，当你我识别数字时，我们会拼凑出各种组成部分。

82
00:05:54,200 --> 00:05:56,820
9 上面有一个环，右边有一条线。

83
00:05:57,380 --> 00:06:01,180
8 号也有一个上环，但它与另一个下环搭配。

84
00:06:01,980 --> 00:06:06,820
一个 4 基本分为三条具体的线路，诸如此类。

85
00:06:07,600 --> 00:06:11,531
现在，在一个完美的世界里，我们可能希望第二层到最后一

86
00:06:11,531 --> 00:06:15,614
层的每个神经元都与这些子组件中的一个相对应，也就是说，

87
00:06:15,614 --> 00:06:19,243
无论何时你输入一个图像，比如说，上面有一个循环，

88
00:06:19,243 --> 00:06:23,780
比如说 9 或 8，都会有一些特定的神经元的激活度接近 1。

89
00:06:24,500 --> 00:06:27,255
我指的并不是这个特定的像素循环，

90
00:06:27,255 --> 00:06:31,560
而是希望任何朝向顶部的循环模式都能触发这个神经元。

91
00:06:32,440 --> 00:06:36,240
这样，从第三层到最后一层，只需学

92
00:06:36,240 --> 00:06:40,040
习哪个子组件的组合对应哪个数字。

93
00:06:41,000 --> 00:06:45,169
当然，这只是把问题往下推移，因为你如何识别这些子组件，

94
00:06:45,169 --> 00:06:47,640
甚至了解正确的子组件应该是什么？

95
00:06:48,060 --> 00:06:53,060
我还没讲到这一层是如何影响下一层的，先跟我讲讲这一层吧。

96
00:06:53,680 --> 00:06:56,680
识别循环也可以分解为多个子问题。

97
00:06:57,280 --> 00:07:02,780
要做到这一点，一个合理的方法是首先认识到构成它的各种小边缘。

98
00:07:03,780 --> 00:07:07,185
同样，一条长线，比如你可能在数字 1、4 

99
00:07:07,185 --> 00:07:10,590
或 7 中看到的那种，实际上只是一条长边，

100
00:07:10,590 --> 00:07:14,320
或者你可以把它看作是由几条小边组成的某种图案。

101
00:07:15,140 --> 00:07:18,930
因此，也许我们希望第二层网络中的每

102
00:07:18,930 --> 00:07:22,720
个神经元都能对应各种相关的小边缘。

103
00:07:23,540 --> 00:07:28,803
也许当像这样的图像出现时，它会点亮所有与大约 8 到 

104
00:07:28,803 --> 00:07:31,922
10 个特定小边缘相关的神经元，

105
00:07:31,922 --> 00:07:36,016
进而点亮与上环和一条长垂直线相关的神经元，

106
00:07:36,016 --> 00:07:39,720
这些神经元点亮了与 9 相关的神经元。

107
00:07:40,680 --> 00:07:42,955
至于我们的最终网络是否真的能做到这一点，这是另一个问题，

108
00:07:42,955 --> 00:07:44,823
等我们看到如何训练网络后，我再来讨论这个问题，

109
00:07:44,823 --> 00:07:47,180
但这是我们的一个希望，也是我们采用这种分层结构的一种目标。

110
00:07:47,500 --> 00:07:49,955
此外，你还可以想象，像这样检测边缘和图

111
00:07:49,955 --> 00:07:52,540
案的能力对于其他图像识别任务有多么有用。

112
00:07:53,160 --> 00:07:57,687
甚至在图像识别之外，你可能还想做各种各样的智能事情，

113
00:07:57,687 --> 00:08:00,300
这些事情可以分成不同的抽象层。

114
00:08:00,880 --> 00:08:03,873
例如，对语音进行解析，包括获取原始音频并挑选出不同的声音，

115
00:08:03,873 --> 00:08:05,834
这些声音组合成特定的音节，组合成单词，

116
00:08:05,834 --> 00:08:07,280
组合成短语和更抽象的思想等。

117
00:08:08,040 --> 00:08:13,569
但是，回到这一切究竟是如何运作的，请想象一下，

118
00:08:13,569 --> 00:08:20,060
你现在正在设计一层的激活究竟是如何决定下一层的激活的。

119
00:08:21,100 --> 00:08:25,910
我们的目标是建立一种机制，可以将像素组合成边缘，

120
00:08:25,910 --> 00:08:29,920
或将边缘组合成图案，或将图案组合成数字。

121
00:08:30,860 --> 00:08:33,458
放大一个非常具体的例子，比方说，

122
00:08:33,458 --> 00:08:37,518
我们希望第二层中的一个特定神经元能够捕捉到图像在这

123
00:08:37,518 --> 00:08:38,980
个区域是否有边缘。

124
00:08:39,440 --> 00:08:50,620
现在的问题是，网络应具备哪些参数？

125
00:08:51,440 --> 00:08:52,660
您应该调整哪些拨盘和旋钮，使其具有足够的表现

126
00:08:52,660 --> 00:08:53,658
力来捕捉这种图案或任何其他像素图案，

127
00:08:53,658 --> 00:08:55,100
或几个边可以构成一个循环的图案，以及其他类似的图案？

128
00:08:55,640 --> 00:09:01,554
那么，我们要做的就是为我们的神经元和第

129
00:09:01,554 --> 00:09:07,780
一层神经元之间的每一个连接分配一个权重。

130
00:09:08,720 --> 00:09:15,560
这些重量只是数字。

131
00:09:16,320 --> 00:09:17,700
然后从第一层提取所有激活值，并根据这些权重计算它们的加权和。

132
00:09:18,540 --> 00:09:20,976
我觉得把这些权重组织成一个小网格很有帮助，

133
00:09:20,976 --> 00:09:23,644
我将用绿色像素表示正权重，红色像素表示负权重，

134
00:09:23,644 --> 00:09:25,500
像素的亮度是对权重值的粗略描述。

135
00:09:27,700 --> 00:09:31,364
如果我们将几乎所有像素的权重都设为零，

136
00:09:31,364 --> 00:09:34,836
只保留我们所关注区域内的一些正权重，

137
00:09:34,836 --> 00:09:39,465
那么计算所有像素值的加权和实际上就相当于将我们所

138
00:09:39,465 --> 00:09:41,780
关注区域内的像素值相加。

139
00:09:42,780 --> 00:09:49,112
如果你真的想找出这里是否有边缘，

140
00:09:49,112 --> 00:09:57,820
你可能会做的是在周围的像素中加入一些负权重。

141
00:09:59,140 --> 00:10:06,600
当中间像素较亮而周围像素较暗时，总和最大。

142
00:10:07,480 --> 00:10:09,788
当你计算这样的加权和时，你可能会得出任何数字，

143
00:10:09,788 --> 00:10:12,700
但对于这个网络来说，我们希望激活值介于 0 和 1 之间。

144
00:10:14,260 --> 00:10:18,900
因此，常见的做法是将加权和输入某个函数，

145
00:10:18,900 --> 00:10:23,540
将实数线压缩到 0 和 1 之间的范围。

146
00:10:24,120 --> 00:10:29,155
实现这一功能的一个常见函数叫做 sigmoid 函数，

147
00:10:29,155 --> 00:10:32,140
也称为 logistic 曲线。

148
00:10:32,460 --> 00:10:34,444
基本上，非常负的输入最终接近于 0，

149
00:10:34,444 --> 00:10:37,420
正的输入最终接近于 1，然后在输入 0 附近稳步上升。

150
00:10:38,000 --> 00:10:46,600
因此，这里神经元的激活基本上就是衡量相关加权和的正向程度。

151
00:10:49,120 --> 00:10:56,360
但也许你并不是想让神经元在加权和大于 0 时亮起来。

152
00:10:57,540 --> 00:11:01,880
也许你只想让它在总和大于 10 时激活。

153
00:11:02,280 --> 00:11:06,360
也就是说，你希望它在不活动时有一些偏差。

154
00:11:06,840 --> 00:11:08,550
我们接下来要做的，就是在这个加权和中加入一些其他数字，

155
00:11:08,550 --> 00:11:10,260
比如负 10，然后将其插入 sigmoid 平方函数。

156
00:11:11,380 --> 00:11:19,660
这个额外的数字叫做偏差。

157
00:11:20,580 --> 00:11:21,433
因此，权重可以告诉你第二层的神经元正在捕捉什么像素模式，

158
00:11:21,433 --> 00:11:22,135
而偏置可以告诉你在神经元开始有意义地活跃之前，

159
00:11:22,135 --> 00:11:22,440
加权和需要达到多高。

160
00:11:23,460 --> 00:11:35,180
而这只是一个神经元。

161
00:11:36,120 --> 00:11:36,833
这一层中的每一个其他神经元都将与第一层的所有 784 

162
00:11:36,833 --> 00:11:37,256
个像素神经元相连，而这 784 

163
00:11:37,256 --> 00:11:37,680
个连接中的每一个都有自己的权重。

164
00:11:38,280 --> 00:11:45,255
此外，每个加权和都有一些偏差，即在用 sigmoid 

165
00:11:45,255 --> 00:11:50,940
函数压扁之前，在加权和上添加的一些其他数字。

166
00:11:51,600 --> 00:11:57,600
这需要考虑的事情太多了！

167
00:11:58,110 --> 00:11:58,857
这个隐藏层有 16 个神经元，总共是 784 

168
00:11:58,857 --> 00:11:59,540
乘以 16 个权重，再加上 16 个偏置。

169
00:11:59,960 --> 00:12:07,980
而所有这些都只是第一层与第二层之间的连接。

170
00:12:08,840 --> 00:12:11,940
其他层之间的连接也有一系列权重和偏差。

171
00:12:12,520 --> 00:12:16,096
综上所述，这个网络的权重和偏差总数几乎正好是 

172
00:12:16,096 --> 00:12:17,340
13000 个。

173
00:12:18,340 --> 00:12:21,872
1.3 万个旋钮和刻度盘可以进行调整和旋转，

174
00:12:21,872 --> 00:12:23,800
使网络以不同的方式运行。

175
00:12:23,800 --> 00:12:26,821
因此，当我们谈论学习时，我们指的是让计算机为所有这些

176
00:12:26,821 --> 00:12:29,960
众多的数字找到一个有效的设置，从而真正解决手头的问题。

177
00:12:31,040 --> 00:12:34,779
一个既有趣又有点可怕的思想实验是，想象自己坐下来，

178
00:12:34,779 --> 00:12:38,219
手工设置所有这些权重和偏差，有目的地调整数字，

179
00:12:38,219 --> 00:12:41,360
以便第二层拾取边缘，第三层拾取模式，等等。

180
00:12:42,620 --> 00:12:46,481
我个人认为这比把网络当作一个完全的黑盒子更令人满意，

181
00:12:46,481 --> 00:12:49,897
因为当网络的表现不尽如人意时，如果你已经对这些

182
00:12:49,897 --> 00:12:52,421
权重和偏差的实际含义有了一些了解，

183
00:12:52,421 --> 00:12:56,580
你就有了一个实验的起点，可以尝试如何改变结构来改进网络。

184
00:12:56,980 --> 00:13:03,595
或者，当网络确实起作用，但并不是你所期望的那样时，

185
00:13:03,595 --> 00:13:09,152
深入研究权重和偏差的作用是挑战你的假设并真

186
00:13:09,152 --> 00:13:14,180
正揭示可能解决方案的全部空间的好方法。

187
00:13:14,960 --> 00:13:25,820
顺便说一句，这里的实际功能写起来有点麻烦，你觉得呢？

188
00:13:26,840 --> 00:13:30,680
因此，让我向大家展示一种更简洁的方法来表示这些连接。

189
00:13:32,500 --> 00:13:37,140
如果你想了解更多关于神经网络的信息，你就会发现它是这样的。

190
00:13:37,660 --> 00:13:39,089
将某一层的所有激活值整理成一列，作为矩阵，

191
00:13:39,089 --> 00:13:40,520
对应于某一层与下一层特定神经元之间的连接。

192
00:13:40,941 --> 00:13:40,520
这意味着，根据这些权重对第一层的激活值进行加权求和，

193
00:13:41,380 --> 00:13:40,941
就相当于我们左侧所有内容的矩阵向量乘积中的一个项。

194
00:13:41,380 --> 00:13:47,503
顺便说一下，机器学习的很多内容都归结于对线性代数的掌握，

195
00:13:47,503 --> 00:13:52,970
所以如果你们想直观地了解矩阵和矩阵向量乘法的含义，

196
00:13:52,970 --> 00:13:58,000
可以看看我做的线性代数系列，尤其是第 3 章。

197
00:13:58,540 --> 00:14:03,220
回到我们的表达式，我们不再单独地将偏差加到每个值上，

198
00:14:03,220 --> 00:14:06,280
而是将所有这些偏差组织成一个向量，

199
00:14:06,280 --> 00:14:09,880
然后将整个向量加到前面的矩阵向量乘积中。

200
00:14:14,000 --> 00:14:19,959
最后一步，我会在外侧包上一个西格米函数，

201
00:14:19,959 --> 00:14:28,600
这代表你要将西格米函数应用到内侧结果向量的每个特定分量上。

202
00:14:29,240 --> 00:14:32,401
因此，一旦将权重矩阵和这些向量写成自己的符号，

203
00:14:32,401 --> 00:14:35,563
就可以用一个非常紧凑和整洁的小表达式来传达激活

204
00:14:35,563 --> 00:14:38,725
从一层到下一层的整个转换过程，这使得相关代码变

205
00:14:38,725 --> 00:14:42,300
得更加简单和快速，因为许多库都对矩阵乘法进行了优化。

206
00:14:43,280 --> 00:14:54,740
还记得我之前说过，这些神经元只是保存数字的东西吗？

207
00:14:55,940 --> 00:15:01,830
当然，神经元所保存的具体数字取决于输入的图像，

208
00:15:01,830 --> 00:15:06,696
因此将每个神经元视为一个函数更为准确，

209
00:15:06,696 --> 00:15:13,098
它接收上一层所有神经元的输出，并输出一个介于 0 

210
00:15:13,098 --> 00:15:15,660
和 1 之间的数字。

211
00:15:17,820 --> 00:15:20,305
实际上，整个网络只是一个函数，一个输入 784 个数字，

212
00:15:20,305 --> 00:15:21,460
输出 10 个数字的函数。

213
00:15:22,220 --> 00:15:25,877
这是一个复杂得离谱的函数，涉及 13,000 个参数，

214
00:15:25,877 --> 00:15:29,399
这些参数以权重和偏差的形式出现，可以捕捉到某些模式，

215
00:15:29,399 --> 00:15:33,327
还涉及许多矩阵向量乘积的迭代和 sigmoid 平方函数，

216
00:15:33,327 --> 00:15:37,391
但它仍然只是一个函数，而且从某种程度上来说，它看起来很复杂，

217
00:15:37,391 --> 00:15:38,340
这让人很放心。

218
00:15:39,200 --> 00:15:42,898
我的意思是说，如果它再简单一些，

219
00:15:42,898 --> 00:15:47,060
我们还能指望它接受识别数字的挑战吗？

220
00:15:47,560 --> 00:16:02,640
它又是如何应对这一挑战的呢？

221
00:16:03,400 --> 00:16:06,660
这个网络是如何通过观察数据来学习适当的权重和偏差的？

222
00:16:07,340 --> 00:16:09,360
这就是我将在下一个视频中展示的内容，

223
00:16:09,360 --> 00:16:12,280
同时我还将深入探讨我们看到的这个特殊网络的真正作用。

224
00:16:13,340 --> 00:16:13,793
现在，我想我应该说订阅，以便在视频或任何新视

225
00:16:13,793 --> 00:16:14,370
频发布时获得通知，但实际上，你们中的大多数人都不会收到 

226
00:16:14,370 --> 00:16:14,700
YouTube 的通知，不是吗？

227
00:16:15,080 --> 00:16:16,957
也许更确切地说，我应该订阅，这样 YouTube 

228
00:16:16,957 --> 00:16:19,059
推荐算法的神经网络就会相信，你希望看到这个频道的内容被推

229
00:16:19,059 --> 00:16:19,360
荐给你。

230
00:16:20,140 --> 00:16:26,120
无论如何，请继续关注我们的后续报道。

231
00:16:27,580 --> 00:16:37,420
非常感谢大家在 Patreon 上对这些视频的支持。

232
00:16:38,020 --> 00:16:42,100
今年夏天，我在 "概率 "系列中的进展有些缓慢，

233
00:16:42,100 --> 00:16:45,330
但在完成这个项目后，我将重新投入其中，

234
00:16:45,330 --> 00:16:47,880
所以各位顾客可以关注我的更新。

235
00:16:48,560 --> 00:16:49,014
最后，我请来了莱莎-李，她是深度学习理论方面的博士，

236
00:16:49,014 --> 00:16:49,450
目前在一家名为 Amplify Partners 

237
00:16:49,450 --> 00:16:49,940
的风险投资公司工作，该公司慷慨地为本视频提供了部分资金。

238
00:16:50,760 --> 00:16:53,500
所以，雷莎，我认为我们应该快速提出一件事，那就是乙状函数。

239
00:16:54,000 --> 00:16:56,004
据我所知，早期的网络会利用这一点，

240
00:16:56,004 --> 00:16:58,598
将相关的加权和压入 0 和 1 之间的区间，

241
00:16:58,598 --> 00:17:01,900
你知道这是由神经元要么不活跃要么活跃的生物类比所激发的。

242
00:17:03,600 --> 00:17:14,619
没错。

243
00:17:15,460 --> 00:17:19,119
但是，相对而言，现代网络已经很少再使用 sigmoid 了。

244
00:17:19,700 --> 00:17:29,840
是啊

245
00:17:30,280 --> 00:17:30,300
有点老派吧？

246
00:17:30,560 --> 00:17:34,040
是的，或者说 relu 似乎更容易训练。

247
00:17:34,320 --> 00:17:34,320
relu 代表整流线性单元？

248
00:17:34,440 --> 00:17:34,655
是的，它是这样一个函数，你只需取零和 a 的最大值，

249
00:17:34,655 --> 00:17:34,828
其中 a 是由你在视频中解释的内容给出的，

250
00:17:34,828 --> 00:17:35,052
我认为它的部分动机来自于生物类比，即神经元要么被激活，

251
00:17:35,052 --> 00:17:35,291
要么不被激活，所以如果它通过了某个阈值，它就会是身份函数，

252
00:17:35,291 --> 00:17:35,540
但如果没有通过，它就不会被激活，所以它是零，这算是一种简化。

253
00:17:35,760 --> 00:17:36,642
使用 sigmoids 对训练没有帮助，

254
00:17:36,642 --> 00:17:37,921
或者在某些时候训练非常困难，于是人们就尝试使用 relu，

255
00:17:37,921 --> 00:17:38,980
它碰巧对这些令人难以置信的深度神经网络非常有效。

256
00:17:39,400 --> 00:17:42,340
好的，谢谢你，艾丽西亚。

257
00:17:42,680 --> 00:17:46,900
是的，在这种函数 中，您只需取最大值为零，

258
00:17:46,900 --> 00:17:50,920
而 a 则由您在视频中解释的内容 给出。

259
00:17:50,920 --> 00:18:01,360
我认为这部分是出于对神经元如何激活 或不激活的生物学类比。

260
00:18:01,360 --> 00:18:05,171
因此，如果它通过了某个阈值， 它将成为恒等函数，

261
00:18:05,171 --> 00:18:09,460
但如果它没有通过，那么它就不会被激活，因此它会为零 。

262
00:18:09,460 --> 00:18:10,840
所以这是一种简化。

263
00:18:11,160 --> 00:18:14,615
使用 sigmoid 对训练没有帮助，

264
00:18:14,615 --> 00:18:20,072
或 者在某些时候训练起来非常困难，人们只是尝试了 relu，

265
00:18:20,072 --> 00:18:24,620
它恰好对这 些令人难以置信的深层神经网络非常有效。

266
00:18:25,100 --> 00:18:25,640
好的，谢谢莉莎。

