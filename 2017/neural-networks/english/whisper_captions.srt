1
00:00:04,220 --> 00:00:10,720
This is a 3. It's sloppily written and rendered at an extremely low resolution of 28x28 pixels,

2
00:00:11,200 --> 00:00:15,320
but your brain has no trouble recognizing it as a 3. And I want you to take a moment

3
00:00:15,320 --> 00:00:20,280
to appreciate how crazy it is that brains can do this so effortlessly. I mean, this,

4
00:00:20,460 --> 00:00:25,900
this and this are also recognizable as 3s, even though the specific values of each pixel

5
00:00:25,900 --> 00:00:31,360
is very different from one image to the next. The particular light-sensitive cells in your

6
00:00:31,360 --> 00:00:36,100
eye that are firing when you see this 3 are very different from the ones firing when you

7
00:00:36,100 --> 00:00:42,900
see this 3. But something in that crazy-smart visual cortex of yours resolves these as representing

8
00:00:42,900 --> 00:00:48,260
the same idea, while at the same time recognizing other images as their own distinct ideas.

9
00:00:49,220 --> 00:00:55,420
But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28

10
00:00:55,420 --> 00:01:01,120
pixels like this and outputs a single number between 0 and 10, telling you what it thinks

11
00:01:01,120 --> 00:01:07,560
the digit is, well the task goes from comically trivial to dauntingly difficult. Unless you've

12
00:01:07,560 --> 00:01:11,520
been living under a rock, I think I hardly need to motivate the relevance and importance of

13
00:01:11,520 --> 00:01:16,200
machine learning and neural networks to the present and to the future. But what I want to do here is

14
00:01:16,200 --> 00:01:21,200
show you what a neural network actually is, assuming no background, and to help visualize

15
00:01:21,200 --> 00:01:26,680
what it's doing, not as a buzzword but as a piece of math. My hope is that you come away feeling

16
00:01:26,680 --> 00:01:31,320
like the structure itself is motivated, and to feel like you know what it means when you read,

17
00:01:31,520 --> 00:01:36,800
or you hear about a neural network quote-unquote learning. This video is just going to be devoted

18
00:01:36,800 --> 00:01:40,260
to the structure component of that, and the following one is going to tackle learning.

19
00:01:40,960 --> 00:01:45,520
What we're going to do is put together a neural network that can learn to recognize handwritten

20
00:01:45,520 --> 00:01:53,560
digits. This is a somewhat classic example for introducing the topic, and I'm happy to stick

21
00:01:53,560 --> 00:01:57,400
with the status quo here, because at the end of the two videos I want to point you to a couple

22
00:01:57,400 --> 00:02:01,940
good resources where you can learn more, and where you can download the code that does this and play

23
00:02:01,940 --> 00:02:09,020
with it on your own computer. There are many many variants of neural networks, and in recent years

24
00:02:09,020 --> 00:02:13,860
there's been sort of a boom in research towards these variants, but in these two introductory

25
00:02:13,860 --> 00:02:19,180
videos you and I are just going to look at the simplest plain vanilla form with no added frills.

26
00:02:19,860 --> 00:02:23,920
This is kind of a necessary prerequisite for understanding any of the more powerful modern

27
00:02:23,920 --> 00:02:28,600
variants, and trust me it still has plenty of complexity for us to wrap our minds around.

28
00:02:29,120 --> 00:02:33,940
But even in this simplest form it can learn to recognize handwritten digits, which is a pretty

29
00:02:33,940 --> 00:02:40,080
cool thing for a computer to be able to do. And at the same time you'll see how it does fall short

30
00:02:40,080 --> 00:02:46,100
of a couple hopes that we might have for it. As the name suggests neural networks are inspired

31
00:02:46,100 --> 00:02:51,180
by the brain, but let's break that down. What are the neurons, and in what sense are they linked

32
00:02:51,180 --> 00:02:58,000
together? Right now when I say neuron all I want you to think about is a thing that holds a number,

33
00:02:58,320 --> 00:03:05,520
specifically a number between 0 and 1. It's really not more than that. For example the network starts

34
00:03:05,520 --> 00:03:13,080
with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784

35
00:03:13,080 --> 00:03:19,440
neurons in total. Each one of these holds a number that represents the grayscale value of the

36
00:03:19,440 --> 00:03:26,300
corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels. This number inside

37
00:03:26,300 --> 00:03:31,680
the neuron is called its activation, and the image you might have in mind here is that each neuron is

38
00:03:31,680 --> 00:03:40,200
lit up when its activation is a high number. So all of these 784 neurons make up the first

39
00:03:40,200 --> 00:03:50,760
layer of our network. Now jumping over to the last layer, this has 10 neurons, each representing one

40
00:03:50,760 --> 00:03:56,440
of the digits. The activation in these neurons, again some number that's between 0 and 1,

41
00:03:56,440 --> 00:04:03,440
represents how much the system thinks that a given image corresponds with a given digit. There's also

42
00:04:03,440 --> 00:04:08,720
a couple layers in between called the hidden layers, which for the time being should just be

43
00:04:08,720 --> 00:04:13,260
a giant question mark for how on earth this process of recognizing digits is going to be

44
00:04:13,260 --> 00:04:18,880
handled. In this network I chose two hidden layers, each one with 16 neurons, and admittedly

45
00:04:18,880 --> 00:04:23,540
that's kind of an arbitrary choice. To be honest I chose two layers based on how I want to motivate

46
00:04:23,540 --> 00:04:28,200
the structure in just a moment, and 16, well that was just a nice number to fit on the screen.

47
00:04:28,780 --> 00:04:32,340
In practice there is a lot of room for experiment with a specific structure here.

48
00:04:33,020 --> 00:04:38,480
The way the network operates, activations in one layer determine the activations of the next layer.

49
00:04:39,200 --> 00:04:44,080
And of course the heart of the network as an information processing mechanism comes down to

50
00:04:44,080 --> 00:04:48,580
exactly how those activations from one layer bring about activations in the next layer.

51
00:04:49,140 --> 00:04:53,420
It's meant to be loosely analogous to how in biological networks of neurons,

52
00:04:53,920 --> 00:04:59,440
some groups of neurons firing cause certain others to fire. Now the network I'm showing here has

53
00:04:59,440 --> 00:05:04,320
already been trained to recognize digits, and let me show you what I mean by that. It means if you

54
00:05:04,320 --> 00:05:10,980
feed in an image, lighting up all 784 neurons of the input layer according to the brightness of

55
00:05:10,980 --> 00:05:17,200
each pixel in the image, that pattern of activations causes some very specific pattern in the next layer

56
00:05:17,200 --> 00:05:21,740
which causes some pattern in the one after it, which finally gives some pattern in the output

57
00:05:21,740 --> 00:05:27,580
layer. And the brightest neuron of that output layer is the network's choice, so to speak,

58
00:05:27,580 --> 00:05:35,300
for what digit this image represents. And before jumping into the math for how one layer influences

59
00:05:35,300 --> 00:05:40,900
the next, or how training works, let's just talk about why it's even reasonable to expect a layered

60
00:05:40,900 --> 00:05:46,640
structure like this to behave intelligently. What are we expecting here? What is the best hope for

61
00:05:46,640 --> 00:05:53,520
those middle layers? Well, when you or I recognize digits, we piece together various components.

62
00:05:54,200 --> 00:06:00,080
A 9 has a loop up top and a line on the right. An 8 also has a loop up top, but it's paired with

63
00:06:00,080 --> 00:06:06,820
another loop down low. A 4 basically breaks down into three specific lines, and things like that.

64
00:06:07,600 --> 00:06:12,180
Now in a perfect world, we might hope that each neuron in the second to last layer

65
00:06:12,180 --> 00:06:18,260
corresponds with one of these subcomponents, that anytime you feed in an image with, say, a loop up

66
00:06:18,260 --> 00:06:23,780
top, like a 9 or an 8, there's some specific neuron whose activation is going to be close to 1.

67
00:06:24,500 --> 00:06:29,400
And I don't mean this specific loop of pixels, the hope would be that any generally loopy pattern

68
00:06:29,400 --> 00:06:35,580
towards the top sets off this neuron. That way, going from the third layer to the last one just

69
00:06:35,580 --> 00:06:41,840
requires learning which combination of subcomponents corresponds to which digits. Of course, that just

70
00:06:41,840 --> 00:06:46,080
kicks the problem down the road, because how would you recognize these subcomponents, or even learn

71
00:06:46,080 --> 00:06:49,820
what the right subcomponents should be? And I still haven't even talked about how one layer

72
00:06:49,820 --> 00:06:55,440
influences the next, but run with me on this one for a moment. Recognizing a loop can also break

73
00:06:55,440 --> 00:07:01,140
down into subproblems. One reasonable way to do this would be to first recognize the various little

74
00:07:01,140 --> 00:07:08,300
edges that make it up. Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7,

75
00:07:08,730 --> 00:07:14,320
is really just a long edge, or maybe you think of it as a certain pattern of several smaller edges.

76
00:07:15,140 --> 00:07:19,840
So maybe our hope is that each neuron in the second layer of the network

77
00:07:19,840 --> 00:07:26,360
corresponds with the various relevant little edges. Maybe when an image like this one comes in,

78
00:07:26,540 --> 00:07:31,740
it lights up all of the neurons associated with around 8 to 10 specific little edges,

79
00:07:32,280 --> 00:07:36,880
which in turn lights up the neurons associated with the upper loop and a long vertical line,

80
00:07:36,880 --> 00:07:42,280
and those light up the neuron associated with a 9. Whether or not this is what our final network

81
00:07:42,280 --> 00:07:47,180
actually does is another question, one that I'll come back to once we see how to train the network,

82
00:07:47,500 --> 00:07:52,540
but this is a hope that we might have, a sort of goal with the layered structure like this.

83
00:07:53,160 --> 00:07:56,980
Moreover, you can imagine how being able to detect edges and patterns like this

84
00:07:56,980 --> 00:08:02,400
would be really useful for other image recognition tasks. And even beyond image recognition,

85
00:08:02,400 --> 00:08:06,500
there are all sorts of intelligent things you might want to do that break down into layers

86
00:08:06,500 --> 00:08:12,140
of abstraction. Parsing speech, for example, involves taking raw audio and picking out distinct

87
00:08:12,140 --> 00:08:17,800
sounds, which combine to make certain syllables, which combine to form words, which combine to make

88
00:08:17,800 --> 00:08:23,540
up phrases and more abstract thoughts, etc. But getting back to how any of this actually works,

89
00:08:24,040 --> 00:08:29,280
picture yourself right now designing how exactly the activations in one layer might determine the

90
00:08:29,280 --> 00:08:35,960
next. The goal is to have some mechanism that could conceivably combine pixels into edges,

91
00:08:36,520 --> 00:08:41,840
or edges into patterns, or patterns into digits. And to zoom in on one very specific example,

92
00:08:42,200 --> 00:08:47,960
let's say the hope is for one particular neuron in the second layer to pick up on whether or not the

93
00:08:47,960 --> 00:08:54,720
image has an edge in this region here. The question at hand is what parameters should the network

94
00:08:54,720 --> 00:09:00,960
have? What dials and knobs should you be able to tweak so that it's expressive enough to potentially

95
00:09:00,960 --> 00:09:06,540
capture this pattern, or any other pixel pattern, or the pattern that several edges can make a loop,

96
00:09:06,780 --> 00:09:12,260
and other such things? Well, what we'll do is assign a weight to each one of the connections

97
00:09:12,260 --> 00:09:19,460
between our neuron and the neurons from the first layer. These weights are just numbers. Then take

98
00:09:19,460 --> 00:09:25,080
all of those activations from the first layer and compute their weighted sum according to these

99
00:09:25,080 --> 00:09:31,020
weights. I find it helpful to think of these weights as being organized into a little grid

100
00:09:31,020 --> 00:09:36,140
of their own, and I'm going to use green pixels to indicate positive weights, and red pixels to

101
00:09:36,140 --> 00:09:41,140
indicate negative weights, where the brightness of that pixel is some loose depiction of the

102
00:09:41,140 --> 00:09:45,980
weight's value. Now if we made the weights associated with almost all of the pixels zero

103
00:09:45,980 --> 00:09:49,500
except for some positive weights in this region that we care about,

104
00:09:50,020 --> 00:09:54,620
then taking the weighted sum of all the pixel values really just amounts to adding up the

105
00:09:54,620 --> 00:10:00,460
values of the pixel just in the region that we care about. And if you really wanted to pick up

106
00:10:00,460 --> 00:10:05,600
on whether there's an edge here, what you might do is have some negative weights associated with

107
00:10:05,600 --> 00:10:11,100
the surrounding pixels. Then the sum is largest when those middle pixels are bright but the

108
00:10:11,100 --> 00:10:17,480
surrounding pixels are darker. When you compute a weighted sum like this, you might come out with

109
00:10:17,480 --> 00:10:22,840
any number, but for this network what we want is for activations to be some value between 0

110
00:10:22,840 --> 00:10:29,020
and 1. So a common thing to do is to pump this weighted sum into some function that squishes the

111
00:10:29,020 --> 00:10:34,500
real number line into the range between 0 and 1. And a common function that does this is called the

112
00:10:34,500 --> 00:10:40,900
sigmoid function, also known as a logistic curve. Basically very negative inputs end up close to 0,

113
00:10:41,100 --> 00:10:46,600
positive inputs end up close to 1, and it just steadily increases around the input 0.

114
00:10:49,120 --> 00:10:55,440
So the activation of the neuron here is basically a measure of how positive the relevant weighted

115
00:10:55,440 --> 00:11:01,360
sum is. But maybe it's not that you want the neuron to light up when the weighted sum is bigger

116
00:11:01,360 --> 00:11:07,920
than 0. Maybe you only want it to be active when the sum is bigger than say 10. That is, you want

117
00:11:07,920 --> 00:11:14,580
some bias for it to be inactive. What we'll do then is just add in some other number like negative

118
00:11:14,580 --> 00:11:19,660
10 to this weighted sum before plugging it through the sigmoid squishification function.

119
00:11:20,580 --> 00:11:26,560
That additional number is called the bias. So the weights tell you what pixel pattern this neuron

120
00:11:26,560 --> 00:11:32,380
in the second layer is picking up on, and the bias tells you how high the weighted sum needs to be

121
00:11:32,380 --> 00:11:39,000
before the neuron starts getting meaningfully active. And that is just one neuron. Every other

122
00:11:39,000 --> 00:11:45,080
neuron in this layer is going to be connected to all 784 pixel neurons from the first layer,

123
00:11:45,440 --> 00:11:52,780
and each one of those 784 connections has its own weight associated with it. Also, each one has some

124
00:11:52,780 --> 00:11:57,600
bias, some other number that you add on to the weighted sum before squishing it with the sigmoid.

125
00:11:58,110 --> 00:12:05,580
And that's a lot to think about! With this hidden layer of 16 neurons, that's a total of 784 times 16

126
00:12:05,580 --> 00:12:11,660
weights, along with 16 biases. And all of that is just the connections from the first layer to the

127
00:12:11,660 --> 00:12:16,780
second. The connections between the other layers also have a bunch of weights and biases associated

128
00:12:16,780 --> 00:12:23,800
with them. All said and done, this network has almost exactly 13,000 total weights and biases.

129
00:12:23,800 --> 00:12:29,500
13,000 knobs and dials that can be tweaked and turned to make this network behave in different

130
00:12:29,500 --> 00:12:36,280
ways. So when we talk about learning, what that's referring to is getting the computer to find a

131
00:12:36,280 --> 00:12:41,360
valid setting for all of these many many numbers so that it'll actually solve the problem at hand.

132
00:12:42,620 --> 00:12:47,780
One thought experiment that is at once fun and kind of horrifying is to imagine sitting down and

133
00:12:47,780 --> 00:12:52,400
setting all of these weights and biases by hand, purposefully tweaking the numbers so that the

134
00:12:52,400 --> 00:12:58,260
second layer picks up on edges, the third layer picks up on patterns, etc. I personally find this

135
00:12:58,260 --> 00:13:03,180
satisfying rather than just treating the network as a total black box, because when the network

136
00:13:03,180 --> 00:13:08,040
doesn't perform the way you anticipate, if you've built up a little bit of a relationship with what

137
00:13:08,040 --> 00:13:12,680
those weights and biases actually mean, you have a starting place for experimenting with how to

138
00:13:12,680 --> 00:13:17,480
change the structure to improve. Or when the network does work but not for the reasons you

139
00:13:17,480 --> 00:13:22,360
might expect, digging into what the weights and biases are doing is a good way to challenge your

140
00:13:22,360 --> 00:13:28,280
assumptions and really expose the full space of possible solutions. By the way, the actual function

141
00:13:28,280 --> 00:13:34,780
here is a little cumbersome to write down, don't you think? So let me show you a more notationally

142
00:13:34,780 --> 00:13:39,380
compact way that these connections are represented. This is how you'd see it if you choose to read up

143
00:13:39,380 --> 00:13:52,340
more about neural networks. Organize all of the activations from one layer into a column as a

144
00:13:52,340 --> 00:13:58,000
matrix corresponds to the connections between one layer and a particular neuron in the next layer.

145
00:13:58,540 --> 00:14:03,240
What that means is that taking the weighted sum of the activations in the first layer according to

146
00:14:03,240 --> 00:14:09,880
these weights corresponds to one of the terms in the matrix vector product of everything we have on the left here.

147
00:14:14,000 --> 00:14:18,300
By the way, so much of machine learning just comes down to having a good grasp of linear algebra,

148
00:14:18,300 --> 00:14:22,740
so for any of you who want a nice visual understanding for matrices and what matrix

149
00:14:22,740 --> 00:14:28,600
vector multiplication means, take a look at the series I did on linear algebra, especially chapter 3.

150
00:14:29,240 --> 00:14:33,600
Back to our expression, instead of talking about adding the bias to each one of these values

151
00:14:33,600 --> 00:14:39,720
independently, we represent it by organizing all those biases into a vector, and adding the entire

152
00:14:39,720 --> 00:14:46,560
vector to the previous matrix vector product. Then as a final step, I'll wrap a sigmoid around the

153
00:14:46,560 --> 00:14:50,660
outside here, and what that's supposed to represent is that you're going to apply the

154
00:14:50,660 --> 00:14:57,320
sigmoid function to each specific component of the resulting vector inside. So once you write down

155
00:14:57,320 --> 00:15:02,540
this weight matrix and these vectors as their own symbols, you can communicate the full transition

156
00:15:02,540 --> 00:15:07,440
of activations from one layer to the next in an extremely tight and neat little expression,

157
00:15:08,140 --> 00:15:13,420
and this makes the relevant code both a lot simpler and a lot faster, since many libraries

158
00:15:13,420 --> 00:15:19,820
optimize the heck out of matrix multiplication. Remember how earlier I said these neurons are

159
00:15:19,820 --> 00:15:25,360
simply things that hold numbers? Well of course the specific numbers that they hold depends on

160
00:15:25,360 --> 00:15:31,600
the image you feed in, so it's actually more accurate to think of each neuron as a function,

161
00:15:32,120 --> 00:15:37,240
one that takes in the outputs of all the neurons in the previous layer and spits out a number

162
00:15:37,240 --> 00:15:43,600
between 0 and 1. Really the entire network is just a function, one that takes in 784 numbers

163
00:15:43,600 --> 00:15:49,580
as an input and spits out 10 numbers as an output. It's an absurdly complicated function,

164
00:15:49,860 --> 00:15:54,620
one that involves 13,000 parameters in the forms of these weights and biases that pick up on

165
00:15:54,620 --> 00:15:59,080
certain patterns, and which involves iterating many matrix vector products and the sigmoid

166
00:15:59,080 --> 00:16:05,500
squishification function, but it's just a function nonetheless, and in a way it's kind of reassuring

167
00:16:05,500 --> 00:16:10,280
that it looks complicated. I mean if it were any simpler, what hope would we have that it could

168
00:16:10,280 --> 00:16:15,760
take on the challenge of recognizing digits? And how does it take on that challenge? How does this

169
00:16:15,760 --> 00:16:20,900
network learn the appropriate weights and biases just by looking at data? Well that's what I'll

170
00:16:20,900 --> 00:16:25,160
show in the next video, and I'll also dig a little more into what this particular network we're

171
00:16:25,160 --> 00:16:30,820
seeing is really doing. Now is the point I suppose I should say subscribe to stay notified about when

172
00:16:30,820 --> 00:16:36,260
video or any new videos come out, but realistically most of you don't actually receive notifications

173
00:16:36,260 --> 00:16:41,480
from YouTube, do you? Maybe more honestly I should say subscribe so that the neural networks that

174
00:16:41,480 --> 00:16:45,700
underlie YouTube's recommendation algorithm are primed to believe that you want to see

175
00:16:45,700 --> 00:16:51,540
content from this channel get recommended to you. Anyway stay posted for more. Thank you very much

176
00:16:51,540 --> 00:16:55,600
to everyone supporting these videos on Patreon. I've been a little slow to progress in the

177
00:16:55,600 --> 00:17:00,280
probability series this summer, but I'm jumping back into it after this project, so patrons you

178
00:17:00,280 --> 00:17:06,540
can look out for updates there. To close things off here I have with me Leisha Lee who did her

179
00:17:06,540 --> 00:17:10,900
PhD work on the theoretical side of deep learning and who currently works at a venture capital firm

180
00:17:10,900 --> 00:17:16,560
called Amplify Partners who kindly provided some of the funding for this video. So Leisha one thing

181
00:17:16,560 --> 00:17:21,140
I think we should quickly bring up is this sigmoid function. As I understand it early networks use

182
00:17:21,140 --> 00:17:25,780
this to squish the relevant weighted sum into that interval between zero and one, you know kind of

183
00:17:25,780 --> 00:17:30,800
motivated by this biological analogy of neurons either being inactive or active. Exactly. But

184
00:17:30,800 --> 00:17:35,880
relatively few modern networks actually use sigmoid anymore. Yeah. It's kind of old school right? Yeah

185
00:17:35,880 --> 00:17:42,340
or rather relu seems to be much easier to train. And relu stands for rectified linear unit?

186
00:17:42,680 --> 00:17:49,060
Yes it's this kind of function where you're just taking a max of zero and a where a is given by

187
00:17:49,060 --> 00:17:53,920
what you were explaining in the video and what this was sort of motivated from I think was a

188
00:17:54,560 --> 00:18:01,980
partially by a biological analogy with how neurons would either be activated or not and so if it

189
00:18:01,980 --> 00:18:07,560
passes a certain threshold it would be the identity function but if it did not then it

190
00:18:07,560 --> 00:18:12,060
would just not be activated so it'd be zero so it's kind of a simplification. Using sigmoids

191
00:18:12,060 --> 00:18:18,140
didn't help training or it was very difficult to train at some point and people just tried relu and

192
00:18:18,140 --> 00:18:25,880
it happened to work very well for these incredibly deep neural networks. All right thank you Alicia.

