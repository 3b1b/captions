1
00:00:04,020 --> 00:00:10,680
این یک سه است. این عدد به صورت سریع نوشته شده و در رزولوشن بسیار پایین در ابعاد 28 تا 28 پیکسل  رندر شده است.

2
00:00:10,680 --> 00:00:15,660
اما مغز شما هیچ مشکلی در شناسایی آن به عنوان یک سه ندارد و من می خواهم یک لحظه به آن توجه کنید.

3
00:00:15,900 --> 00:00:18,949
کورتکس چگونه  است وقتی مغیز اینکار را بدون زحمت انجام می دهد.

4
00:00:18,949 --> 00:00:23,160
منظورم این است که این، این و این هم به عنوان سه، قابل تشخیص هستند.

5
00:00:23,160 --> 00:00:28,060
حتی اگر مقادیر خاص هر پیکسل از یک تصویر به تصویر بعد بسیار متفاوت باشد.

6
00:00:28,080 --> 00:00:33,780
وقتی این سه را می بینید، سلول های حساس به نور خاصی در چشم شما تصاویری ارسال می کنند

7
00:00:33,780 --> 00:00:36,800
که وقتی این سه را می بینید، با آن بسیار متفاوت هستند.

8
00:00:37,140 --> 00:00:40,610
اما چیزی  در آن وجود دارد که کورتکس بصری هوشمند شما

9
00:00:41,129 --> 00:00:48,139
اینها را به عنوان نمایشی از همان ایده حل می کند، در حالی که در عین حال تصاویر دیگر را به مثابه ایده های متمایز با آن تشخیص می دهد

10
00:00:48,840 --> 00:00:55,039
اما اگر من به شما بگویم که بنشینید و یک برنامه برایم بنویسید  که در شبکه 28 تا 28

11
00:00:55,379 --> 00:01:01,759
پیکسل هایی مثل این قرار داشته باشد و خروجی یک عدد بین 0 و 10 باشد، به شما می گوید که فکر می کند این رقم

12
00:01:02,250 --> 00:01:06,139
خوبی است، کاری به طرز ناخوشایند مشکل و بی اهمیت.

13
00:01:06,750 --> 00:01:08,270
مگر اینکه زیر سنگ زندگی کنید

14
00:01:08,270 --> 00:01:14,599
من فکر می کنم  ایجاد انگیزه ارتباط و اهمیت یادگیری ماشین و شبکه های عصبی را از حال به آینده به شدت مورد نیاز است

15
00:01:14,640 --> 00:01:18,410
اما آنچه که من می خواهم انجام دهم این است که به شما نشان می دهد که شبکه عصبی در واقع چیزی

16
00:01:18,660 --> 00:01:24,229
فرضی بدون هیچ پس زمینه است و برای کمک به تجسم کردن آنچه که آن را نه به عنوان یک عبارت مبهم بلکه به عنوان یک قطعه ریاضی انجام می دهد

17
00:01:24,570 --> 00:01:28,310
امید من فقط این است که شما احساس  کنید که این ساختار خودش

18
00:01:28,380 --> 00:01:34,399
انگیزه بخش است و احساس اینکه وقتی شما در مورد یادگیری مستقیم یا غیر مستقیم یک شبکه عصبی می خوانید یا می شنوید، می دانید معنای آن چیست

19
00:01:34,950 --> 00:01:40,249
این ویدئو فقط با ساختار جزئی از آن اختصاص پیدا کرده است و نوعی را دنبال می کند که منجر به یادگیری می شود

20
00:01:40,530 --> 00:01:45,950
آنچه که ما انجام می دهیم، یک شبکه عصبی است که می تواند یاد بگیرد که عدد دست نوشته را تشخیص دهد

21
00:01:49,270 --> 00:01:51,329
این یک مثال تقریبا قدیمی است

22
00:01:51,520 --> 00:01:56,759
موضوع را معرفی می کنم و  خوشحالم که وضعیت فعلی را در اینجا قرار می دهم زیرا در پایان دو فیلم است که می خواهم

23
00:01:56,760 --> 00:02:02,099
شما را به یک جفت منبع خوب هدایت کنم که می توانید بیشتر بیاموزید و اینکه  کدام کد را می توانید دانلود کنید که این کار را انجام دهد و با آن بازی  کند؟

24
00:02:02,100 --> 00:02:04,100
در کامپیوتر خودتان

25
00:02:04,750 --> 00:02:08,970
تعداد زیادی از انواع شبکه های عصبی وجود دارد و در سال های اخیر

26
00:02:08,970 --> 00:02:11,970
به نظر می رسد اینگونه تحقیقات رونق گرفته اند

27
00:02:12,130 --> 00:02:19,019
اما در این دو فیلم مقدماتی شما و من فقط می خواهیم به ساده ترین شکل وانیلی ساده نگاه کنیم و بدون هیچ زحمتی اضافه کنیم

28
00:02:19,300 --> 00:02:21,040
این نوعی ضرورت است

29
00:02:21,040 --> 00:02:24,510
پیش نیازی برای درک هر یک از انواع قدرتمند مدرن و

30
00:02:24,760 --> 00:02:28,199
به اعتقاد من هنوز هم پیچیدگی زیادی برای ما دارد تا ذهنمان بر آن احاطه یابد

31
00:02:28,690 --> 00:02:32,820
اما حتی در این ساده ترین شکل می تواند یاد بگیرد که عدد دست نویس را تشخیص دهد

32
00:02:32,820 --> 00:02:36,180
که چیز بسیار جالبی برای یک کامپیوتر است که می تواند انجام دهد.

33
00:02:37,120 --> 00:02:41,960
و در عین حال شما خواهید دید که چگونه از امیدهای اندکی  که ممکن است برای آن وجود داشته باشد، کاسته می شود

34
00:02:43,090 --> 00:02:48,179
همانطور که از نامش بر می آید شبکه های عصبی از مغز الهام گرفته اند، اما اجازه دهید آن را ریزتر بررسی کنیم

35
00:02:48,520 --> 00:02:51,389
نورون ها چه هستند وارتباط آنها با هم به چه معنا است؟

36
00:02:52,090 --> 00:02:57,750
در حال حاضر وقتی که من می گویم همه نورون ها من می خواهم شما را به فکر کردن در مورد چیزی وادار کنم که یک مقدار عددی را نگه می دارد

37
00:02:58,209 --> 00:03:02,129
به طور خاص عددی بین 0 و 1 و واقعا بیشتر از این نیست

38
00:03:03,430 --> 00:03:11,130
به عنوان مثال شبکه با یک دسته از نورون های متناظر به هم از 28 در 28 پیکسل تصویر ورودی شروع می شود

39
00:03:11,400 --> 00:03:12,460
که در آن

40
00:03:12,460 --> 00:03:20,240
784 نورون در کل، هر یک از این اعداد حفظ شده یک عدد را نشان می دهد که دارای ارزش سیاه و سفید پیکسل متناظر با آن است

41
00:03:20,769 --> 00:03:24,299
دامنه از 0 برای پیکسل های سیاه تا 1 برای پیکسل های سفید متغیر است

42
00:03:24,910 --> 00:03:30,419
این عدد در داخل نورون هایی به نام نورون فعال قرار دارد و تصویری است که شما ممکن است در ذهن داشته باشید

43
00:03:30,420 --> 00:03:33,959
آیا هر نورون زمانی روشن می شود که فعالساز آن یک عدد بزرگ  باشد؟

44
00:03:36,260 --> 00:03:41,559
بنابراین تمام این 784 نورون اولین لایه شبکه ما را تشکیل می دهند

45
00:03:45,990 --> 00:03:51,289
اکنون به آخرین لایه می رویم. این لایه ده نورون دارد که هر کدام از آنها  یک عدد را نشان می دهد

46
00:03:51,570 --> 00:03:56,239
فعالساز در این نورون ها دوباره عددی بین صفر و یک است

47
00:03:56,880 --> 00:04:00,049
که نشان می دهد سیستم  چقدر فکر می کند تا یک تصویر

48
00:04:00,720 --> 00:04:05,990
متناظر با یک رقم داده شده  ارائه دهد؟ همچنین لایه های چندگانه ای به نام لایه های مخفی وجود دارد

49
00:04:06,180 --> 00:04:07,770
این مربوط به چه زمانی است؟

50
00:04:07,770 --> 00:04:13,549
تنها باید یک علامت سوال عظیم برای چگونگی انجام این روند تشخیص عددی  وجود داشته باشد

51
00:04:13,740 --> 00:04:20,209
در این شبکه من دو لایه مخفی را انتخاب کردم که هر کدام با 16 نورون است و مسلما این نوع انتخاب دلخواه است

52
00:04:20,609 --> 00:04:24,889
صادقانه بگویم من دو لایه را بر اساس این که چگونه می خواهم فقط در یک لحظه ایجاد ساختار را انجام دهم انتخاب کردم

53
00:04:25,350 --> 00:04:29,179
16 عدد خوبی است به این دلیل که تنها عددی است که در عمل با صفحه متناسب است

54
00:04:29,180 --> 00:04:32,209
در اینجا اتاق زیادی برای آزمایش با یک ساختار خاص وجود دارد

55
00:04:32,730 --> 00:04:38,329
نحوه فعال سازی شبکه در یک لایه، فعال سازی لایه بعدی را تعیین می کند

56
00:04:38,760 --> 00:04:45,349
و البته قلب شبکه به عنوان یک مکانیسم پردازش اطلاعات، کاملا دقیق است

57
00:04:45,570 --> 00:04:48,409
فعال سازی از یک لایه باعث فعال شدن در لایه بعدی می شود

58
00:04:48,900 --> 00:04:54,859
این به معنای آن است که به طور مشابه با شبکه های بیولوژیکی نورونی، بعضی از گروه های نورون شلیک می کنند

59
00:04:55,410 --> 00:04:57,410
بعضی از نورون ها، دیگر نورون ها را روشن می کنند

60
00:04:57,570 --> 00:04:58,340
حالا شبکه

61
00:04:58,340 --> 00:05:03,019
من نشان می دهم در اینجا شبکه آموزش دیده است تا علامت ها را شناسایی کرده و اجازه دهید به شما نشان دهم که منظور من چیست

62
00:05:03,140 --> 00:05:06,580
این بدان معنی است که اگر شما در یک تصویرهمه را روشن کنید

63
00:05:06,640 --> 00:05:11,780
784 نورون لایه ورودی با توجه به روشنایی هر پیکسل در تصویر روشن می شوند

64
00:05:12,330 --> 00:05:17,029
این الگوی فعال سازی یک الگوی بسیار خاص در لایه بعدی ایجاد می کند

65
00:05:17,190 --> 00:05:19,309
کدام یک از الگوها را بعد از آن ایجاد می کند؟

66
00:05:19,440 --> 00:05:22,190
که در نهایت  برخی از الگوی در لایه خروجی را به دست می دهد و؟

67
00:05:22,350 --> 00:05:29,359
روشنترین نورون لایه خروجی، انتخاب شبکه است تا بگوید این تصویر چه عددی را نمایش می دهد؟

68
00:05:32,070 --> 00:05:36,859
و قبل از پریدن به ریاضی برای اینکه چطور یک لایه، لایه بعدی را تحت تاثیر قرار می دهد یا چگونه آموزش می بیند؟

69
00:05:37,140 --> 00:05:43,069
بیایید فقط درباره اینکه چرا حتی انتظار می رود یک ساختار لایه ای مانند این نیز انتظار داشته باشیم که به طور هوشمندانه رفتار کند، صحبت کنیم

70
00:05:43,800 --> 00:05:48,260
اینجا در انتظار چه چیزی هستیم؟ بهترین امید برای آنچه که لایه های میانی ​​می تواند انجام دهد چیست؟

71
00:05:48,860 --> 00:05:56,720
خوب وقتی که شما یا من علامت را تشخیص می دهیم، ما اجزای مختلف را با هم ترکیب می کنیم، نه یک حلقه بالا و یک خط در سمت راست است

72
00:05:57,260 --> 00:06:01,280
همچنین یک 8 هم یک حلقه در بالا و حلقه دیگری در پایین دارد

73
00:06:02,020 --> 00:06:06,599
یک 4 اساسا به سه خط خاص و چیزهایی مانند آن شکسته می شود

74
00:06:07,180 --> 00:06:11,970
در حال حاضر در دنیای کامل، ممکن است امیدوار باشیم که هر نورون در لایه دوم تا آخر

75
00:06:12,640 --> 00:06:14,729
با یکی از این زیرجزها ارتباط دارد

76
00:06:14,890 --> 00:06:19,740
هر بار که شما یک تصویر با یک حلقه در بالا مانند یک 9 یا 8 می بینید

77
00:06:19,870 --> 00:06:21,220
برخی از موارد خاص وجود دارد

78
00:06:21,220 --> 00:06:27,749
نورونی فعال می شود که مقدار آن نزدیک به یک است و من به این حلقه خاصی از پیکسل ها فکر نمی کنم این امید است که هر کدام

79
00:06:28,090 --> 00:06:35,039
به طور کلی الگوی حلقوی به سمت بالا مجموعه ای از این نورون در راه رفتن از لایه سوم به آخرین لایه را خاموش می کند

80
00:06:35,380 --> 00:06:39,960
فقط باید یاد بگیرد  که ترکیبی از اجزای زیر مربوط به کدام است

81
00:06:40,510 --> 00:06:42,810
البته این فقط مسئله را به پایین جاده می اندازد

82
00:06:42,910 --> 00:06:49,019
از آنجا که این شبکه اجزاء زیر را تشخیص می دهد یا حتی یاد می گیرد که اجزای زیر چه باید  باشند، من هنوز حتی درباره این صحبت نمی کنم که

83
00:06:49,020 --> 00:06:52,829
چگونه یک لایه، لایه بعدی را تحت تاثیر قرار می دهد، اما برای یک لحظه با من در این مورد همراه شوید

84
00:06:53,650 --> 00:06:56,340
شناخت یک حلقه نیز می تواند به سؤظن تبدیل شود

85
00:06:56,860 --> 00:07:02,550
یکی از راه های معقول برای انجام این کار این است که ابتدا لبه های مختلف کمی را تشخیص می دهد که باعث ایجاد آن می شود

86
00:07:03,520 --> 00:07:08,910
به طور مشابه یک خط طولانی مانند نوعی که ممکن است در رقم 1 یا 4 یا 7 مشاهده کنید

87
00:07:08,910 --> 00:07:14,279
خوب این واقعا یک لبه طولانی است یا شاید شما آن را به عنوان یک الگوی خاص از چند لبه کوچکتر تصور کنید

88
00:07:14,740 --> 00:07:19,379
بنابراین شاید امید ما این باشد که هر نورون در لایه دوم شبکه

89
00:07:20,290 --> 00:07:22,650
متناظر با لبه های کوچک مختلف مربوطه است

90
00:07:23,230 --> 00:07:28,259
شاید زمانی که یک تصویر مانند این یکی در می آید شبکه همه نورون ها را روشن می کند

91
00:07:28,720 --> 00:07:31,649
همراه با حدود هشت تا ده لبه خاص خاص

92
00:07:31,930 --> 00:07:36,930
که به نوبه خود نورون های مرتبط با حلقه بالا و یک خط عمودی طولانی را روشن می کنند و

93
00:07:37,300 --> 00:07:39,599
آنهایی که نورون را با یک 9 را روشن می کنند

94
00:07:40,300 --> 00:07:41,100
چرا که نه

95
00:07:41,100 --> 00:07:47,070
این همان چیزی است که شبکه نهایی ما در واقع انجام می دهد، یک سوال دیگر وجود دارد، که من می توانم ببینم چگونه می توانم شبکه را آموزش دهم

96
00:07:47,350 --> 00:07:52,170
اما این یک امیدواری است که ما ممکن است داشته باشیم. نوعی هدف با ساختار لایه ای مانند این

97
00:07:53,020 --> 00:07:59,340
علاوه بر این شما می توانید تصور کنید که چگونه قابلیت تشخیص لبه ها و الگوهای مانند این واقعا برای دیگر وظایف تشخیص تصویر مفید است

98
00:07:59,740 --> 00:08:06,749
و حتی فراتر از شناخت  تصویر، همه انواع چیزهای هوشمند وجود دارد . شما ممکن است بخواهید این کار را با تقسیم لایه های انتزاعی انجام دهید

99
00:08:07,690 --> 00:08:14,670
برای مثال تجزیه گفتار شامل گرفتن صدای خام و انتخاب صداهای متمایز است که ترکیبی از ساختن هجا های خاص

100
00:08:15,070 --> 00:08:19,829
کدام ترکیب را برای شکل دهی به کلمات تشکیل می دهند که ترکیب را به عبارات و افکار انتزاعی و غیره ترکیب کنید

101
00:08:20,770 --> 00:08:25,710
ما به عقب بر گردیم به این که چگونه هر یک از اینها در حال حاضر طراحی تصویر خود را انجام می دهد

102
00:08:25,710 --> 00:08:30,449
چگونه دقیقا فعال سازی در یک لایه ممکن است فعال سازی در لایه بعدی را تعیین کند؟

103
00:08:30,670 --> 00:08:35,879
هدف این است که یک مکانیسم داشته باشید که احتمالا پیکسل ها را به لبه ها تبدیل می کند

104
00:08:35,880 --> 00:08:41,430
یا لبه ها را به الگوها یا الگوها را به رقم تبدیل می کند و در یک مثال بسیار خاص برای بزرگنمایی است

105
00:08:41,950 --> 00:08:44,189
بگذارید بگوییم امید در مورد  یک نورون خاص

106
00:08:44,380 --> 00:08:50,430
در لایه دوم است برای انتخاب اینکه آیا تصویر دارای لبه در این منطقه در اینجا هست یا خیر؟

107
00:08:50,950 --> 00:08:54,960
سوال فعلی این است که شبکه باید چه  پارامترهایی داشته باشد

108
00:08:55,270 --> 00:09:02,490
و کدامیک از آنها باید بتوانید تحریک کردن را به گونه ای بیان کنند که به طور بالقوه بتواند این الگو را ضبط کند یا

109
00:09:02,590 --> 00:09:07,290
هر الگوی دیگری از پیکسل یا الگو که چند لبه می تواند یک حلقه و دیگر چیزهای دیگر را ایجاد کند؟

110
00:09:08,290 --> 00:09:15,389
خوب، آنچه ما انجام خواهیم داد این است که وزن هر یک از اتصالات بین نورون هایمان و نورون های لایه اول را حساب کنیم

111
00:09:15,850 --> 00:09:17,850
این وزن ها فقط عدد هستند

112
00:09:18,190 --> 00:09:25,590
سپس تمام این فعال سازی ها را از لایه اول انجام داده و مجموع وزن آنها را با توجه به این وزن ها محاسبه می کنیم

113
00:09:27,370 --> 00:09:31,680
به این فکر کنید که این وزنها به صورت یک شبکه کوچک از خودشان سازماندهی شده اند

114
00:09:31,680 --> 00:09:37,079
و من قصد دارم از پیکسل های سبز برای نشان دادن وزن مثبت و پیکسل های قرمز برای نشان دادن وزن های منفی استفاده کنم

115
00:09:37,240 --> 00:09:41,670
در کجا روشنایی آن پیکسل تصویر بیانگر ارزش وزن است؟

116
00:09:42,400 --> 00:09:45,840
اکنون ما وزن هایی را که تقریبا تمام پیکسل های صفر را تشکیل می دهند، ساخته ایم

117
00:09:46,150 --> 00:09:49,079
به جز برخی از وزنه های مثبت در این منطقه که ما به آن اهمیت می دهیم

118
00:09:49,480 --> 00:09:51,310
سپس وزن مجموع

119
00:09:51,310 --> 00:09:57,690
تمام مقادیر پیکسل واقعا فقط مقدار ارزش پیکسل را فقط در منطقه ای که مورد نظر ماست را بدست می آوریم

120
00:09:58,870 --> 00:10:04,440
و اگر شما واقعا می خواهید آن را انتخاب کنید که آیا وجود دارد یا خیر، لبه در اینجا چیزی است که شما ممکن است به برخی از وزن های منفی

121
00:10:04,900 --> 00:10:06,900
مرتبط با پیکسل های اطراف بیانجامد

122
00:10:07,030 --> 00:10:12,660
سپس وقتی که پیکسل های متوسط ​​روشن هستند، اما پیکسل های اطراف آن تیره تر هستند، جمع بیشترین مقدار است

123
00:10:14,279 --> 00:10:18,169
هنگامی که یک مقدار وزنی مانند این را محاسبه میکنید، ممکن است هر عددی بیرون بیاید

124
00:10:18,240 --> 00:10:23,180
اما  چیزی که ما برای این شبکه می خواهیم این است که برای فعال سازی یک مقدار بین 0 و 1 باشد

125
00:10:23,730 --> 00:10:26,599
بنابراین یک چیز مشترک برای انجام این کار این است که این مقدار وزنی را

126
00:10:26,910 --> 00:10:32,000
به برخی از توابعی که خط عدد حقیقی  را به محدوده بین 0 و 1 تبدیل می کنند، ارسال کنید

127
00:10:32,190 --> 00:10:37,249
یک تابع رایج که این کار را انجام می دهد، تابع سیگموئید نامیده می شود که به عنوان یک منحنی لجستیک نیز شناخته می شود

128
00:10:37,980 --> 00:10:43,339
در واقع ورودی های بسیار منفی در نهایت نزدیک به صفر هستند و ورودی های بسیار مثبتی به نزدیک به 1 می رسند

129
00:10:43,339 --> 00:10:46,398
و فقط به طور پیوسته در اطراف ورودی 0 افزایش می یابد

130
00:10:49,080 --> 00:10:56,029
بنابراین فعال سازی نورون در اینجا اساسا یک اندازه گیری از چگونگی مثبت بودن مجموع وزن است

131
00:10:57,450 --> 00:11:01,819
اما شاید وقتی که مجموع وزنی بزرگتر از 0 باشد آنطور که شما می خواهید نورون را روشن نکند

132
00:11:02,100 --> 00:11:06,260
شاید فقط بخواهید وقتی که جمع بزرگتر از 10 باشد  آن را فعال کنید

133
00:11:06,630 --> 00:11:10,279
به این دلیل که می خواهید برخی از بایاس برای آن غیر فعال باشد

134
00:11:10,860 --> 00:11:16,099
آنچه که ما انجام خواهیم داد این است که فقط عدد دیگری را مانند عدد منفی 10  به این مجموع وزن اضافه کنیم

135
00:11:16,529 --> 00:11:19,669
قبل از اتصال به تابع انقباضی سیگموئید

136
00:11:20,220 --> 00:11:22,730
عدد اضافه شده بایاس نامیده می شود

137
00:11:23,310 --> 00:11:29,060
بنابراین وزنها به شما میگویند که کدام یک از الگوی پیکسل این نورون در لایه دوم برداشته شده است و بایاس

138
00:11:29,220 --> 00:11:35,450
به شما می گوید که قبل از اینکه نورون شروع به فعال شدن معنی دار کند، باید مقدار وزنی بالاتری داشته باشد

139
00:11:35,910 --> 00:11:37,910
و این فقط یک نورون است

140
00:11:38,120 --> 00:11:41,940
هر نورون دیگر در این لایه، به همه

141
00:11:42,320 --> 00:11:50,620
نورون های 784 پیکسل از لایه اول متصل می شود و هر کدام از 784 اتصالات وزن خود را با آن مرتبط می کند

142
00:11:51,330 --> 00:11:57,739
همچنین هر یک از بایاس ها تعدادی عدد دیگری که شما را با مجموع وزنی اضافه می کند قبل از آن که با سیگموئید مخلوط شود و

143
00:11:58,020 --> 00:12:01,909
این مقدار زیادی برای فکر کردن درباره این لایه پنهان از 16 نورون است

144
00:12:02,010 --> 00:12:08,270
این یک مجموعه از 784 بار 16 وزن همراه با 16 بایاس است

145
00:12:08,490 --> 00:12:14,029
و همه اینها فقط اتصال از لایه اول به دوم ارتباطات بین لایه های دیگر است

146
00:12:14,029 --> 00:12:17,208
همچنین، دسته ای از وزن و بایاس مرتبط با آنها داریم

147
00:12:17,760 --> 00:12:20,680
همه خوانده شده اند و این تقریبا  شبکه دقیق است

148
00:12:21,280 --> 00:12:23,920
مجموع وزن ها و بایاس ها 13000 است

149
00:12:24,280 --> 00:12:29,540
13،000 گره وارتباطاتی است که می توان آن را تغییر داده و تبدیل کرد تا این شبکه به شیوه های مختلف رفتار کند

150
00:12:30,520 --> 00:12:32,520
پس وقتی ما در مورد یادگیری صحبت می کنیم؟

151
00:12:32,530 --> 00:12:40,199
آنچه که به آن اشاره شده است، دریافت کامپیوتر برای پیدا کردن یک تنظیم معتبر برای تمام این تعداد بسیار بسیار زیاد اعداد است به طوری که در واقع  آن را حل کند

152
00:12:40,200 --> 00:12:42,190
مشکل موجود

153
00:12:42,190 --> 00:12:43,000
این است که آزمایشی را تصور کنید

154
00:12:43,000 --> 00:12:49,979
که در آن زمان سرگرم کننده و به نوعی وحشتناک است. تصور کنید نشسته اید و تمام این وزن و بایاس ها را با دست تنظیم کرده اید

155
00:12:50,380 --> 00:12:56,159
به طور هدفمند، اعداد را تغییر داده اید، به طوری که لایه دوم بر روی لبه ها، لایه سوم بر الگوها و غیره قرار می گیرد

156
00:12:56,350 --> 00:13:01,440
من شخصا این را رضایت بخش تر از خواندن شبکه به عنوان یک جعبه سیاه کامل در یافته ام

157
00:13:01,870 --> 00:13:04,349
زیرا وقتی شبکه  کار شما را انجام نمی دهد

158
00:13:04,600 --> 00:13:11,370
پیش بینی کنید اگر شما با آنچه که این وزن ها و تعصب ها در واقع هستند کمی ارتباط داشته باشید، شما یک نقطه شروع برای

159
00:13:11,680 --> 00:13:16,289
بررسی چگونگی تغییر ساختار برای بهبود و یا زمانی که شبکه کار می کند، دارید؟

160
00:13:16,290 --> 00:13:18,290
اما نه به دلایلی که ممکن است انتظار داشته باشید

161
00:13:18,310 --> 00:13:25,169
کنکاش در وزن ها و بایاس ها، راه خوبی برای فائق آمدن به چالش های شما است و واقعا فضای کامل ممکن را افشا می کنید

162
00:13:25,180 --> 00:13:26,350
راه حل

163
00:13:26,350 --> 00:13:30,600
به هر حال، نوشتن تابع واقعی اینجا کمی کم دردسر است. آیا اینطور فکر نمی کنید؟

164
00:13:32,350 --> 00:13:38,460
بنابراین ببایید به شما یک روش جمع و جورتر نشان دهم که این ارتباطات را نشان می دهد. روش این است که چگونه می توانید آن را ببینید

165
00:13:38,460 --> 00:13:40,460
اگر بخواهید درباره شبکه های عصبی بیشتر بخوانید

166
00:13:41,110 --> 00:13:45,810
همه فعالسازها را از یک لایه به یک ستون به صورت یک بردار مرتب کنید

167
00:13:47,470 --> 00:13:52,320
سپس تمام وزن ها را به صورت ماتریس که هر وزن یک ردیف از آن ماتریس است، سازماندهی کنید

168
00:13:52,900 --> 00:13:57,659
بین اتصالات بین یک لایه و یک نورون خاص در لایه بعدی تناظر بر قرار است

169
00:13:58,060 --> 00:14:03,599
آیا این به معنی است که مجموع وزن های فعالسازها در لایه اول را بر اساس این وزن ها به دست می آوریم؟

170
00:14:04,000 --> 00:14:09,330
متناظر با هر یک از عبارات در ماتریس بردار محصول، ما  در اینجا در سمت چپ یک چیز داریم

171
00:14:13,540 --> 00:14:18,380
با روش های بسیاری که برای یادگیری ماشین وجود دارد، فقط به داشتن یک درک خوب از جبر خطی نیاز است

172
00:14:18,380 --> 00:14:26,940
بنابراین برای هر کدام از شما که می خواهید درک بصری خوبی از ماتریس و آنچه که ضرب ماتریس معکوس خوانده می شود داشته باشید  سری های آموزش من در جبر خطی را ببینید

173
00:14:27,250 --> 00:14:28,839
بخصوص فصل سوم را

174
00:14:28,839 --> 00:14:35,759
به بحث خود بر می گردیم  به جای صحبت کردن در مورد اضافه کردن بایاس به هر یک از این مقادیر به طور مستقل ما آن را  با

175
00:14:36,010 --> 00:14:42,209
سازماندهی تمام این بایاس ها در یک بردار و اضافه کردن کل بردار به محصول بردار ماتریس قبلی نشان می دهیم

176
00:14:42,910 --> 00:14:44,040
سپس به عنوان آخرین مرحله

177
00:14:44,040 --> 00:14:47,250
من یک سیگموئید در پیرامون آن قرار خواهم داد

178
00:14:47,250 --> 00:14:51,899
و آنچه که تصور می شود این است که شما قصد دارید تابع سیگموئید را به هر یک از موارد خاص اعمال کنید

179
00:14:52,420 --> 00:14:54,570
اجزای  بردار نتیجه داخلی

180
00:14:55,510 --> 00:15:00,749
پس وقتی این ماتریس وزن را  و این بردارها را به عنوان نمادهای خودشان  بنویسید، می توانید ببینید

181
00:15:01,000 --> 00:15:07,589
انتقال کامل فعال سازی از یک لایه به بعد در بیان مختصر و شفاف و ارتباط برقرار می کند

182
00:15:07,930 --> 00:15:15,000
این باعث می شود کد مربوطه هر دو بسیار ساده تر و بسیار سریع تر از بسیاری از کتابخانه های بهینه سازی شده از هک ضرب ماتریس است

183
00:15:17,560 --> 00:15:21,359
به یاد داشته باشید همانطور که قبلا گفتم  این نورون ها به سادگی چیزهایی هستند که اعداد را نگه می دارند

184
00:15:21,790 --> 00:15:26,250
خب، البته تعداد مشخصی که در اختیار دارند، وابسته به تصویری است که به آن داده اید

185
00:15:27,790 --> 00:15:32,940
بنابراین، در واقع دقیق تر است که هر نورون را به عنوان یک تابع در نظر بگیریم

186
00:15:33,070 --> 00:15:38,070
خروجی تمام نورون ها در لایه قبلی است و عدد صفر و یک را جدا می کند

187
00:15:38,800 --> 00:15:42,270
واقعا کل شبکه فقط یک تابع است که

188
00:15:42,760 --> 00:15:47,010
784 عدد به عنوان یک ورودی می گیرد و ده عدد را به عنوان خروجی جدا می کند

189
00:15:47,470 --> 00:15:48,700
این بی بدیل است

190
00:15:48,700 --> 00:15:56,249
تابع پیچیده ای که شامل سیزده هزار پارامتر در قالب این وزن ها و بایاس ها است که بر روی الگوهای مشخصی قرار می گیرند و شامل

191
00:15:56,250 --> 00:16:00,270
تکرار بسیاری از محصولات بردار ماتریکس و تابع انقباضی سیگموئید است

192
00:16:00,610 --> 00:16:06,390
با این وجود، این فقط یک تابع است و به نوعی اطمینان بخش است که به نظر پیچیده می آید

193
00:16:06,390 --> 00:16:12,239
منظورم این است که اگر  ساده تر بود، چه امیدی داشتیم که بتوانیم بر چالش شناخت رقم ها دست یابیم؟

194
00:16:12,960 --> 00:16:19,559
و این چالش چطور است؟ چطور این شبکه با توجه به داده ها وزن و بایاس مناسب آموزش می بیند؟ اوه؟

195
00:16:20,080 --> 00:16:26,039
این چیزی است که من در ویدیوی بعدی نشان می دهم و همچنین کمی بیشتر به آنچه که در این شبکه خاص می بینیم واقعا حیرت زده می شویم.

196
00:16:27,130 --> 00:16:32,640
در حال حاضر نقطه ای است که من فکر می کنم باید بگویم برای اطلاع از زمانی که این ویدئو و یا هر گونه ویدئو جدید بیرون می آیند، آنرا subscribe کنید

197
00:16:32,760 --> 00:16:37,560
اما واقعاً اکثر شما در حال دریافت اطلاعیه ها از یوتیوب نیستید؟

198
00:16:37,560 --> 00:16:42,260
شاید صادقانه تر به نظر برسد که شبکه های عصبی که YouTube را پایه گذاری کرده اند.

199
00:16:42,459 --> 00:16:47,639
الگوریتم پیشنهاد شده بر این باور است که می خواهید محتوایی از این کانال را ببینید تا به شما توصیه شود

200
00:16:48,250 --> 00:16:50,250
به هر حال پست برای زمان بیشتری باقی می ماند

201
00:16:50,410 --> 00:16:53,550
از همه شما متشکرم که این  فیلم ها در Patreon پشتیبانی می کنید

202
00:16:53,589 --> 00:16:56,759
در تابستان امسال کمی پیشرفت کرده ام

203
00:16:56,760 --> 00:17:01,379
اما من بعد از این پروژه به عقب برمیگردم تا مشتریان بتوانند بهروزرسانیهای خود را بیابند

204
00:17:03,310 --> 00:17:05,550
برای بستن چیزها اینجا من با لیشا لی هستم

205
00:17:05,550 --> 00:17:12,029
لی که کار دکترای خود را در زمینه نظری یادگیری عمیق انجام داد و در حال حاضر در یک شرکت سرمایه گذاری  به نام شرکای تقویت همکاری می کند

206
00:17:12,030 --> 00:17:16,530
او کسی دوستانه مقداری از بودجه این ویدئو را فراهم کرده است، لیشا منحصر بفرد است

207
00:17:16,530 --> 00:17:19,109
من فکر می کنم که ما باید سریعا این کار را انجام دهیم

208
00:17:19,180 --> 00:17:24,780
همانطور که می دانم شبکه های زودرس این کار را انجام می دهند تا مجموع وزن مربوطه را به این فاصله بین صفر و یک برساند

209
00:17:24,980 --> 00:17:30,340
شما نوع انگیزه ای از این سلسله بیولوژیکی نورون یا غیر فعال یا فعال (لیشا) را می شناسید- دقیقا

210
00:17:30,360 --> 00:17:36,320
(3B1B) - اما تعداد کمی از شبکه های مدرن در واقع از انواع سیگموئید دیگر  استفاده می کنند. این نوعی مدرسه قدیمی است؟ (لیشا) - آره یا نه

211
00:17:36,370 --> 00:17:42,780
ReLU  آموزش بسیار ساده تر به نظر می رسد (3B1B) - و ReLU واقعا برای واحد خطی اصلاح شده است

212
00:17:42,780 --> 00:17:48,839
(لیشا) - بله این نوعی تابع است که در آن فقط حداکثر 0 را می گیرید و در جایی مقدار یک می دهد

213
00:17:49,120 --> 00:17:53,670
آنچه را که در ویدیو توضیح دادید و از نظر من این انگیزه بود، من فکر میکنم یک

214
00:17:54,610 --> 00:17:56,610
بخشی از زندگی زیستی

215
00:17:56,620 --> 00:17:58,179
آنالوگ با چگونگی

216
00:17:58,179 --> 00:18:03,089
فعال شدن یا نشدن نورونهاست و اگر چنین باشد، آستانه مشخصی وجود دارد

217
00:18:03,250 --> 00:18:05,250
این می تواند هویت تابع باشد

218
00:18:05,290 --> 00:18:10,439
اما اگر آن را انجام نشود، فقط فعال نمی شود بنابراین صفر است، بنابراین این نوع از ساده سازی است

219
00:18:10,720 --> 00:18:14,429
استفاده از سیگموئید به آموزش کمک نمی کرد، یا آموزش بسیار دشوار بود

220
00:18:14,429 --> 00:18:19,589
این در بعضی نقاط است و مردم فقط تلاش می کنند که رله را تجربه کنند و کارش را انجام داد

221
00:18:20,110 --> 00:18:22,140
برای این منظور به طور فوق العاده ای بسیار خوب است

222
00:18:22,690 --> 00:18:25,090
شبکه های عصبی عمیق (3B1B) - بسیار خوب

223
00:18:25,090 --> 00:18:26,060
ممنون لیشا

