[
 {
  "input": "This is a 3.",
  "translatedText": "این یک 3 است.",
  "from_community_srt": "این یک سه است.",
  "n_reviews": 0,
  "start": 4.22,
  "end": 5.4
 },
 {
  "input": "It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3.",
  "translatedText": "این به صورت شلخته نوشته شده و با وضوح بسیار پایین 28x28 پیکسل ارائه شده است، اما مغز شما در تشخیص آن به عنوان 3 مشکلی ندارد.",
  "from_community_srt": "این عدد به صورت سریع نوشته شده و در رزولوشن بسیار پایین در ابعاد 28 تا 28 پیکسل  رندر شده است. اما مغز شما هیچ مشکلی در شناسایی آن به عنوان یک سه ندارد و من می خواهم یک لحظه به آن توجه کنید.",
  "n_reviews": 0,
  "start": 6.06,
  "end": 13.72
 },
 {
  "input": "And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly.",
  "translatedText": "و من از شما می خواهم که لحظه ای وقت بگذارید و قدردانی کنید که چقدر دیوانه کننده است که مغزها می توانند این کار را بدون زحمت انجام دهند.",
  "from_community_srt": "کورتکس چگونه  است وقتی مغیز اینکار را بدون زحمت انجام می دهد.",
  "n_reviews": 0,
  "start": 14.34,
  "end": 18.96
 },
 {
  "input": "I mean, this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next.",
  "translatedText": "منظورم این است که این، این و این نیز به عنوان 3s قابل تشخیص هستند، حتی اگر مقادیر خاص هر پیکسل از تصویری به تصویر دیگر بسیار متفاوت باشد.",
  "from_community_srt": "منظورم این است که این، این و این هم به عنوان سه، قابل تشخیص هستند. حتی اگر مقادیر خاص هر پیکسل از یک تصویر به تصویر بعد بسیار متفاوت باشد.",
  "n_reviews": 0,
  "start": 19.7,
  "end": 28.32
 },
 {
  "input": "The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3.",
  "translatedText": "سلول‌های حساس به نور خاص در چشم شما که با دیدن این 3 شلیک می‌کنند با سلول‌هایی که با دیدن این 3 شلیک می‌شوند بسیار متفاوت هستند.",
  "from_community_srt": "وقتی این سه را می بینید، سلول های حساس به نور خاصی در چشم شما تصاویری ارسال می کنند که وقتی این سه را می بینید، با آن بسیار متفاوت هستند.",
  "n_reviews": 0,
  "start": 28.9,
  "end": 36.94
 },
 {
  "input": "But something in that crazy-smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas.",
  "translatedText": "اما چیزی در آن قشر بصری دیوانه وار و هوشمند شما، اینها را به عنوان نمایانگر همان ایده تشخیص می دهد، در حالی که در همان زمان تصاویر دیگر را به عنوان ایده های متمایز خود تشخیص می دهد.",
  "from_community_srt": "اما چیزی  در آن وجود دارد که کورتکس بصری هوشمند شما اینها را به عنوان نمایشی از همان ایده حل می کند، در حالی که در عین حال تصاویر دیگر را به مثابه ایده های متمایز با آن تشخیص می دهد",
  "n_reviews": 0,
  "start": 37.52,
  "end": 48.26
 },
 {
  "input": "But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this and outputs a single number between 0 and 10, telling you what it thinks the digit is, well the task goes from comically trivial to dauntingly difficult.",
  "translatedText": "اما اگر به شما گفتم، هی، بنشینید و برنامه ای برای من بنویسید که یک شبکه 28x28 پیکسلی مانند این را می گیرد و یک عدد واحد بین 0 تا 10 را خروجی می دهد و به شما می گوید که آن رقم فکر می کند، خوب کار از آنجا می رود. کم اهمیت تا به شدت دشوار.",
  "from_community_srt": "اما اگر من به شما بگویم که بنشینید و یک برنامه برایم بنویسید  که در شبکه 28 تا 28 پیکسل هایی مثل این قرار داشته باشد و خروجی یک عدد بین 0 و 10 باشد، به شما می گوید که فکر می کند این رقم خوبی است، کاری به طرز ناخوشایند مشکل و بی اهمیت.",
  "n_reviews": 0,
  "start": 49.22,
  "end": 66.18
 },
 {
  "input": "Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future.",
  "translatedText": "من فکر می کنم به سختی نیازی به انگیزه دادن به ارتباط و اهمیت یادگیری ماشین و شبکه های عصبی برای حال و آینده دارم، مگر اینکه زیر سنگ زندگی کرده باشید.",
  "from_community_srt": "مگر اینکه زیر سنگ زندگی کنید من فکر می کنم  ایجاد انگیزه ارتباط و اهمیت یادگیری ماشین و شبکه های عصبی را از حال به آینده به شدت مورد نیاز است",
  "n_reviews": 0,
  "start": 67.16,
  "end": 74.64
 },
 {
  "input": "But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing, not as a buzzword but as a piece of math.",
  "translatedText": "اما کاری که من می‌خواهم در اینجا انجام دهم این است که به شما نشان دهم که یک شبکه عصبی در واقع چیست، با فرض اینکه هیچ پیش‌زمینه‌ای وجود ندارد، و به تجسم کاری که انجام می‌دهد، نه به عنوان یک کلمه کلیدی، بلکه به عنوان یک تکه ریاضی کمک کنم.",
  "from_community_srt": "اما آنچه که من می خواهم انجام دهم این است که به شما نشان می دهد که شبکه عصبی در واقع چیزی فرضی بدون هیچ پس زمینه است و برای کمک به تجسم کردن آنچه که آن را نه به عنوان یک عبارت مبهم بلکه به عنوان یک قطعه ریاضی انجام می دهد",
  "n_reviews": 0,
  "start": 75.12,
  "end": 84.46
 },
 {
  "input": "My hope is that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote-unquote learning.",
  "translatedText": "امید من این است که شما با احساس اینکه ساختار خود انگیزه دارد، بیرون بیایید، و احساس کنید که وقتی می‌خوانید معنی آن را می‌دانید، یا در مورد یادگیری نقل قول-بی نقل قول شبکه عصبی می‌شنوید.",
  "from_community_srt": "امید من فقط این است که شما احساس  کنید که این ساختار خودش انگیزه بخش است و احساس اینکه وقتی شما در مورد یادگیری مستقیم یا غیر مستقیم یک شبکه عصبی می خوانید یا می شنوید، می دانید معنای آن چیست",
  "n_reviews": 0,
  "start": 85.02,
  "end": 94.34
 },
 {
  "input": "This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning.",
  "translatedText": "این ویدیو فقط به مولفه ساختار آن اختصاص دارد و ویدیوی زیر به یادگیری می پردازد.",
  "from_community_srt": "این ویدئو فقط با ساختار جزئی از آن اختصاص پیدا کرده است و نوعی را دنبال می کند که منجر به یادگیری می شود",
  "n_reviews": 0,
  "start": 95.36,
  "end": 100.26
 },
 {
  "input": "What we're going to do is put together a neural network that can learn to recognize handwritten digits.",
  "translatedText": "کاری که می‌خواهیم انجام دهیم این است که یک شبکه عصبی را کنار هم قرار دهیم که می‌تواند تشخیص ارقام دست‌نویس را بیاموزد.",
  "from_community_srt": "آنچه که ما انجام می دهیم، یک شبکه عصبی است که می تواند یاد بگیرد که عدد دست نوشته را تشخیص دهد",
  "n_reviews": 0,
  "start": 100.96,
  "end": 106.04
 },
 {
  "input": "This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quo here, because at the end of the two videos I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer.",
  "translatedText": "این یک مثال تا حدی کلاسیک برای معرفی موضوع است و من خوشحالم که به وضعیت موجود در اینجا پایبندم، زیرا در پایان دو ویدیو می‌خواهم به چند منبع خوب اشاره کنم که در آن می‌توانید بیشتر بدانید و کجا می توانید کدی را که این کار را انجام می دهد دانلود کنید و با آن در رایانه شخصی خود بازی کنید.",
  "from_community_srt": "این یک مثال تقریبا قدیمی است موضوع را معرفی می کنم و  خوشحالم که وضعیت فعلی را در اینجا قرار می دهم زیرا در پایان دو فیلم است که می خواهم شما را به یک جفت منبع خوب هدایت کنم که می توانید بیشتر بیاموزید و اینکه  کدام کد را می توانید دانلود کنید که این کار را انجام دهد و با آن بازی  کند؟",
  "n_reviews": 0,
  "start": 109.36,
  "end": 123.08
 },
 {
  "input": "There are many many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos you and I are just going to look at the simplest plain vanilla form with no added frills.",
  "translatedText": "انواع زیادی از شبکه‌های عصبی وجود دارد، و در سال‌های اخیر به نوعی رونق تحقیقات در مورد این گونه‌ها وجود داشته است، اما در این دو ویدیوی مقدماتی، من و شما فقط می‌خواهیم ساده‌ترین شکل وانیل ساده را بدون هیچ زواید اضافه‌ای بررسی کنیم.",
  "from_community_srt": "در کامپیوتر خودتان تعداد زیادی از انواع شبکه های عصبی وجود دارد و در سال های اخیر به نظر می رسد اینگونه تحقیقات رونق گرفته اند اما در این دو فیلم مقدماتی شما و من فقط می خواهیم به ساده ترین شکل وانیلی ساده نگاه کنیم و بدون هیچ زحمتی اضافه کنیم",
  "n_reviews": 0,
  "start": 125.04,
  "end": 139.18
 },
 {
  "input": "This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds around.",
  "translatedText": "این یک نوع پیش نیاز ضروری برای درک هر یک از انواع مدرن قدرتمندتر است، و به من اعتماد کنید هنوز هم پیچیدگی زیادی برای ما دارد که بتوانیم ذهن خود را به اطراف بپیچیم.",
  "from_community_srt": "این نوعی ضرورت است پیش نیازی برای درک هر یک از انواع قدرتمند مدرن و به اعتقاد من هنوز هم پیچیدگی زیادی برای ما دارد تا ذهنمان بر آن احاطه یابد",
  "n_reviews": 0,
  "start": 139.86,
  "end": 148.6
 },
 {
  "input": "But even in this simplest form it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do.",
  "translatedText": "اما حتی در این ساده‌ترین شکل نیز می‌تواند یاد بگیرد که ارقام دست‌نویس را تشخیص دهد، که برای کامپیوتر کار بسیار جالبی است.",
  "from_community_srt": "اما حتی در این ساده ترین شکل می تواند یاد بگیرد که عدد دست نویس را تشخیص دهد که چیز بسیار جالبی برای یک کامپیوتر است که می تواند انجام دهد.",
  "n_reviews": 0,
  "start": 149.12,
  "end": 156.52
 },
 {
  "input": "And at the same time you'll see how it does fall short of a couple hopes that we might have for it.",
  "translatedText": "و در عین حال خواهید دید که چگونه از امیدهای زوجی که ممکن است برای آن داشته باشیم، کوتاهی می کند.",
  "from_community_srt": "و در عین حال شما خواهید دید که چگونه از امیدهای اندکی  که ممکن است برای آن وجود داشته باشد، کاسته می شود",
  "n_reviews": 0,
  "start": 157.48,
  "end": 162.28
 },
 {
  "input": "As the name suggests neural networks are inspired by the brain, but let's break that down.",
  "translatedText": "همانطور که از نام آن پیداست شبکه های عصبی از مغز الهام گرفته شده اند، اما بیایید آن را تجزیه کنیم.",
  "from_community_srt": "همانطور که از نامش بر می آید شبکه های عصبی از مغز الهام گرفته اند، اما اجازه دهید آن را ریزتر بررسی کنیم",
  "n_reviews": 0,
  "start": 163.38,
  "end": 168.5
 },
 {
  "input": "What are the neurons, and in what sense are they linked together?",
  "translatedText": "نورون ها چه هستند و از چه نظر به هم مرتبط هستند؟",
  "n_reviews": 0,
  "start": 168.52,
  "end": 171.66
 },
 {
  "input": "Right now when I say neuron all I want you to think about is a thing that holds a number, specifically a number between 0 and 1.",
  "translatedText": "در حال حاضر وقتی می‌گویم نورون تنها چیزی که می‌خواهم به آن فکر کنید چیزی است که عددی را در خود نگه می‌دارد، به‌ویژه عددی بین ۰ و ۱.",
  "from_community_srt": "نورون ها چه هستند وارتباط آنها با هم به چه معنا است؟ در حال حاضر وقتی که من می گویم همه نورون ها من می خواهم شما را به فکر کردن در مورد چیزی وادار کنم که یک مقدار عددی را نگه می دارد",
  "n_reviews": 0,
  "start": 172.5,
  "end": 180.44
 },
 {
  "input": "It's really not more than that.",
  "translatedText": "واقعا بیشتر از این نیست.",
  "n_reviews": 0,
  "start": 180.68,
  "end": 182.56
 },
 {
  "input": "For example the network starts with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784 neurons in total.",
  "translatedText": "به عنوان مثال، شبکه با یک دسته از نورون‌های مربوط به هر یک از پیکسل‌های ۲۸×۲۸ تصویر ورودی شروع می‌شود که در مجموع ۷۸۴ نورون است.",
  "from_community_srt": "به طور خاص عددی بین 0 و 1 و واقعا بیشتر از این نیست به عنوان مثال شبکه با یک دسته از نورون های متناظر به هم از 28 در 28 پیکسل تصویر ورودی شروع می شود",
  "n_reviews": 0,
  "start": 183.78,
  "end": 194.22
 },
 {
  "input": "Each one of these holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels.",
  "translatedText": "هر یک از اینها دارای یک عدد است که نشان دهنده مقدار مقیاس خاکستری پیکسل مربوطه است که از 0 برای پیکسل های سیاه تا 1 برای پیکسل های سفید متغیر است.",
  "from_community_srt": "که در آن 784 نورون در کل، هر یک از این اعداد حفظ شده یک عدد را نشان می دهد که دارای ارزش سیاه و سفید پیکسل متناظر با آن است",
  "n_reviews": 0,
  "start": 194.7,
  "end": 204.38
 },
 {
  "input": "This number inside the neuron is called its activation, and the image you might have in mind here is that each neuron is lit up when its activation is a high number.",
  "translatedText": "به این عدد در داخل نورون، فعال سازی آن می گویند، و تصویری که ممکن است در اینجا در ذهن داشته باشید این است که هر نورون زمانی روشن می شود که تعداد فعال آن زیاد باشد.",
  "from_community_srt": "دامنه از 0 برای پیکسل های سیاه تا 1 برای پیکسل های سفید متغیر است این عدد در داخل نورون هایی به نام نورون فعال قرار دارد و تصویری است که شما ممکن است در ذهن داشته باشید",
  "n_reviews": 0,
  "start": 205.3,
  "end": 214.16
 },
 {
  "input": "So all of these 784 neurons make up the first layer of our network.",
  "translatedText": "بنابراین همه این 784 نورون اولین لایه شبکه ما را تشکیل می دهند.",
  "from_community_srt": "آیا هر نورون زمانی روشن می شود که فعالساز آن یک عدد بزرگ  باشد؟ بنابراین تمام این 784 نورون اولین لایه شبکه ما را تشکیل می دهند",
  "n_reviews": 0,
  "start": 216.72,
  "end": 221.86
 },
 {
  "input": "Now jumping over to the last layer, this has 10 neurons, each representing one of the digits.",
  "translatedText": "اکنون با پرش به آخرین لایه، این لایه دارای 10 نورون است که هر یک نشان دهنده یکی از ارقام است.",
  "from_community_srt": "اکنون به آخرین لایه می رویم.",
  "n_reviews": 0,
  "start": 226.5,
  "end": 231.36
 },
 {
  "input": "The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit.",
  "translatedText": "فعال‌سازی در این نورون‌ها، باز هم عددی بین ۰ و ۱، نشان‌دهنده این است که سیستم چقدر فکر می‌کند که یک تصویر داده‌شده با یک رقم معین مطابقت دارد.",
  "from_community_srt": "این لایه ده نورون دارد که هر کدام از آنها  یک عدد را نشان می دهد فعالساز در این نورون ها دوباره عددی بین صفر و یک است که نشان می دهد سیستم  چقدر فکر می کند تا یک تصویر متناظر با یک رقم داده شده  ارائه دهد؟ همچنین لایه های چندگانه ای به نام لایه های مخفی وجود دارد",
  "n_reviews": 0,
  "start": 232.04,
  "end": 242.12
 },
 {
  "input": "There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark for how on earth this process of recognizing digits is going to be handled.",
  "translatedText": "همچنین چند لایه در این بین وجود دارد که لایه‌های پنهان نامیده می‌شوند، که فعلاً باید یک علامت سؤال غول‌پیکر برای این باشد که این فرآیند تشخیص ارقام چگونه انجام می‌شود.",
  "from_community_srt": "این مربوط به چه زمانی است؟ تنها باید یک علامت سوال عظیم برای چگونگی انجام این روند تشخیص عددی  وجود داشته باشد",
  "n_reviews": 0,
  "start": 243.04,
  "end": 253.6
 },
 {
  "input": "In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice.",
  "translatedText": "در این شبکه من دو لایه پنهان را انتخاب کردم که هر کدام دارای 16 نورون بودند، و مسلماً این یک انتخاب دلخواه است.",
  "from_community_srt": "در این شبکه من دو لایه مخفی را انتخاب کردم که هر کدام با 16 نورون است و مسلما این نوع انتخاب دلخواه است",
  "n_reviews": 0,
  "start": 254.26,
  "end": 260.56
 },
 {
  "input": "To be honest I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that was just a nice number to fit on the screen.",
  "translatedText": "صادقانه بگویم، من دو لایه را بر اساس اینکه چگونه می‌خواهم ساختار را در یک لحظه ایجاد کنم، و 16 را انتخاب کردم، خوب این فقط یک عدد خوب برای قرار دادن روی صفحه بود.",
  "from_community_srt": "صادقانه بگویم من دو لایه را بر اساس این که چگونه می خواهم فقط در یک لحظه ایجاد ساختار را انجام دهم انتخاب کردم",
  "n_reviews": 0,
  "start": 261.02,
  "end": 268.2
 },
 {
  "input": "In practice there is a lot of room for experiment with a specific structure here.",
  "translatedText": "در عمل فضای زیادی برای آزمایش با یک ساختار خاص در اینجا وجود دارد.",
  "from_community_srt": "16 عدد خوبی است به این دلیل که تنها عددی است که در عمل با صفحه متناسب است در اینجا اتاق زیادی برای آزمایش با یک ساختار خاص وجود دارد",
  "n_reviews": 0,
  "start": 268.78,
  "end": 272.34
 },
 {
  "input": "The way the network operates, activations in one layer determine the activations of the next layer.",
  "translatedText": "نحوه عملکرد شبکه، فعال‌سازی در یک لایه، فعال‌سازی لایه بعدی را تعیین می‌کند.",
  "n_reviews": 0,
  "start": 273.02,
  "end": 278.48
 },
 {
  "input": "And of course the heart of the network as an information processing mechanism comes down to exactly how those activations from one layer bring about activations in the next layer.",
  "translatedText": "و البته قلب شبکه به عنوان مکانیزم پردازش اطلاعات دقیقاً به این موضوع مربوط می شود که چگونه آن فعال سازی از یک لایه باعث فعال سازی در لایه بعدی می شود.",
  "from_community_srt": "نحوه فعال سازی شبکه در یک لایه، فعال سازی لایه بعدی را تعیین می کند و البته قلب شبکه به عنوان یک مکانیسم پردازش اطلاعات، کاملا دقیق است",
  "n_reviews": 0,
  "start": 279.2,
  "end": 288.58
 },
 {
  "input": "It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire.",
  "translatedText": "به این معناست که شباهت زیادی به این دارد که چگونه در شبکه‌های بیولوژیکی نورون‌ها، برخی از گروه‌های نورون که شلیک می‌کنند باعث شلیک برخی دیگر می‌شوند.",
  "from_community_srt": "فعال سازی از یک لایه باعث فعال شدن در لایه بعدی می شود این به معنای آن است که به طور مشابه با شبکه های بیولوژیکی نورونی، بعضی از گروه های نورون شلیک می کنند",
  "n_reviews": 0,
  "start": 289.14,
  "end": 297.18
 },
 {
  "input": "Now the network I'm showing here has already been trained to recognize digits, and let me show you what I mean by that.",
  "translatedText": "اکنون شبکه ای که در اینجا نشان می دهم قبلاً برای تشخیص ارقام آموزش دیده است، و اجازه دهید منظورم را از آن به شما نشان دهم.",
  "from_community_srt": "بعضی از نورون ها، دیگر نورون ها را روشن می کنند حالا شبکه من نشان می دهم در اینجا شبکه آموزش دیده است تا علامت ها را شناسایی کرده و اجازه دهید به شما نشان دهم که منظور من چیست",
  "n_reviews": 0,
  "start": 298.12,
  "end": 303.4
 },
 {
  "input": "It means if you feed in an image, lighting up all 784 neurons of the input layer according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer which causes some pattern in the one after it, which finally gives some pattern in the output layer.",
  "translatedText": "به این معنی که اگر در یک تصویر تغذیه کنید، و تمام 784 نورون لایه ورودی را با توجه به روشنایی هر پیکسل در تصویر روشن کنید، آن الگوی فعال‌سازی باعث ایجاد الگوی بسیار خاصی در لایه بعدی می‌شود که باعث ایجاد الگوی در لایه بعدی می‌شود. آن، که در نهایت مقداری الگو در لایه خروجی می دهد.",
  "from_community_srt": "این بدان معنی است که اگر شما در یک تصویرهمه را روشن کنید 784 نورون لایه ورودی با توجه به روشنایی هر پیکسل در تصویر روشن می شوند این الگوی فعال سازی یک الگوی بسیار خاص در لایه بعدی ایجاد می کند کدام یک از الگوها را بعد از آن ایجاد می کند؟",
  "n_reviews": 0,
  "start": 303.64,
  "end": 322.08
 },
 {
  "input": "And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents.",
  "translatedText": "و روشن‌ترین نورون آن لایه خروجی، انتخاب شبکه است، به‌طوری‌که بگوییم، این تصویر چه رقمی را نشان می‌دهد.",
  "from_community_srt": "که در نهایت  برخی از الگوی در لایه خروجی را به دست می دهد و؟ روشنترین نورون لایه خروجی، انتخاب شبکه است تا بگوید این تصویر چه عددی را نمایش می دهد؟",
  "n_reviews": 0,
  "start": 322.56,
  "end": 329.4
 },
 {
  "input": "And before jumping into the math for how one layer influences the next, or how training works, let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently.",
  "translatedText": "و قبل از پرداختن به ریاضیات در مورد اینکه چگونه یک لایه بر لایه بعدی تأثیر می‌گذارد، یا اینکه آموزش چگونه کار می‌کند، اجازه دهید در مورد اینکه چرا حتی منطقی است انتظار داشته باشیم که ساختار لایه‌ای مانند این رفتار هوشمندانه داشته باشد، صحبت کنیم.",
  "from_community_srt": "و قبل از پریدن به ریاضی برای اینکه چطور یک لایه، لایه بعدی را تحت تاثیر قرار می دهد یا چگونه آموزش می بیند؟ بیایید فقط درباره اینکه چرا حتی انتظار می رود یک ساختار لایه ای مانند این نیز انتظار داشته باشیم که به طور هوشمندانه رفتار کند، صحبت کنیم",
  "n_reviews": 0,
  "start": 332.56,
  "end": 343.52
 },
 {
  "input": "What are we expecting here?",
  "translatedText": "اینجا چه انتظاری داریم؟",
  "n_reviews": 0,
  "start": 344.06,
  "end": 345.22
 },
 {
  "input": "What is the best hope for what those middle layers might be doing?",
  "translatedText": "بهترین امید برای آن لایه های میانی چیست؟",
  "from_community_srt": "اینجا در انتظار چه چیزی هستیم؟ بهترین امید برای آنچه که لایه های میانی ​​می تواند انجام دهد چیست؟",
  "n_reviews": 0,
  "start": 345.4,
  "end": 347.6
 },
 {
  "input": "Well, when you or I recognize digits, we piece together various components.",
  "translatedText": "خوب، وقتی من یا شما ارقام را تشخیص می دهیم، اجزای مختلفی را کنار هم می گذاریم.",
  "from_community_srt": "خوب وقتی که شما یا من علامت را تشخیص می دهیم، ما اجزای مختلف را با هم ترکیب می کنیم، نه یک حلقه بالا و یک خط در سمت راست است",
  "n_reviews": 0,
  "start": 348.92,
  "end": 353.52
 },
 {
  "input": "A 9 has a loop up top and a line on the right.",
  "translatedText": "عدد 9 دارای یک حلقه بالا و یک خط در سمت راست است.",
  "n_reviews": 0,
  "start": 354.2,
  "end": 356.82
 },
 {
  "input": "An 8 also has a loop up top, but it's paired with another loop down low.",
  "translatedText": "8 همچنین دارای یک حلقه به بالا است، اما با یک حلقه دیگر به پایین جفت می شود.",
  "n_reviews": 0,
  "start": 357.38,
  "end": 361.18
 },
 {
  "input": "A 4 basically breaks down into three specific lines, and things like that.",
  "translatedText": "یک 4 اساساً به سه خط خاص و مواردی از این دست تقسیم می شود.",
  "from_community_srt": "همچنین یک 8 هم یک حلقه در بالا و حلقه دیگری در پایین دارد یک 4 اساسا به سه خط خاص و چیزهایی مانند آن شکسته می شود",
  "n_reviews": 0,
  "start": 361.98,
  "end": 366.82
 },
 {
  "input": "Now in a perfect world, we might hope that each neuron in the second to last layer corresponds with one of these subcomponents, that anytime you feed in an image with, say, a loop up top, like a 9 or an 8, there's some specific neuron whose activation is going to be close to 1.",
  "translatedText": "اکنون در یک دنیای کامل، ممکن است امیدوار باشیم که هر نورون در لایه دوم تا آخرین با یکی از این اجزای فرعی مطابقت داشته باشد، که هر زمان که در یک تصویر با مثلاً یک حلقه بالا، مانند 9 یا 8 تغذیه می‌کنید، مقداری وجود دارد. نورون خاصی که فعال شدن آن نزدیک به 1 خواهد بود.",
  "from_community_srt": "در حال حاضر در دنیای کامل، ممکن است امیدوار باشیم که هر نورون در لایه دوم تا آخر با یکی از این زیرجزها ارتباط دارد هر بار که شما یک تصویر با یک حلقه در بالا مانند یک 9 یا 8 می بینید برخی از موارد خاص وجود دارد",
  "n_reviews": 0,
  "start": 367.6,
  "end": 383.78
 },
 {
  "input": "And I don't mean this specific loop of pixels, the hope would be that any generally loopy pattern towards the top sets off this neuron.",
  "translatedText": "و منظور من این حلقه خاص از پیکسل ها نیست، امید این است که هر الگوی به طور کلی حلقه ای به سمت بالا، این نورون را تنظیم کند.",
  "from_community_srt": "نورونی فعال می شود که مقدار آن نزدیک به یک است و من به این حلقه خاصی از پیکسل ها فکر نمی کنم این امید است که هر کدام",
  "n_reviews": 0,
  "start": 384.5,
  "end": 391.56
 },
 {
  "input": "That way, going from the third layer to the last one just requires learning which combination of subcomponents corresponds to which digits.",
  "translatedText": "به این ترتیب، رفتن از لایه سوم به لایه آخر فقط مستلزم یادگیری ترکیبی از اجزای فرعی با کدام رقم است.",
  "from_community_srt": "به طور کلی الگوی حلقوی به سمت بالا مجموعه ای از این نورون در راه رفتن از لایه سوم به آخرین لایه را خاموش می کند فقط باید یاد بگیرد  که ترکیبی از اجزای زیر مربوط به کدام است البته این فقط مسئله را به پایین جاده می اندازد",
  "n_reviews": 0,
  "start": 392.44,
  "end": 400.04
 },
 {
  "input": "Of course, that just kicks the problem down the road, because how would you recognize these subcomponents, or even learn what the right subcomponents should be?",
  "translatedText": "البته، این فقط مشکل را به پایان می‌رساند، زیرا چگونه می‌توانید این اجزای فرعی را تشخیص دهید، یا حتی یاد بگیرید که اجزای فرعی مناسب باید چه باشند؟",
  "from_community_srt": "از آنجا که این شبکه اجزاء زیر را تشخیص می دهد یا حتی یاد می گیرد که اجزای زیر چه باید  باشند، من هنوز حتی درباره این صحبت نمی کنم که",
  "n_reviews": 0,
  "start": 401.0,
  "end": 407.64
 },
 {
  "input": "And I still haven't even talked about how one layer influences the next, but run with me on this one for a moment.",
  "translatedText": "و من هنوز حتی در مورد اینکه چگونه یک لایه بر لایه بعدی تأثیر می گذارد صحبت نکرده ام، اما برای یک لحظه با من روی این یکی بدوید.",
  "from_community_srt": "چگونه یک لایه، لایه بعدی را تحت تاثیر قرار می دهد، اما برای یک لحظه با من در این مورد همراه شوید",
  "n_reviews": 0,
  "start": 408.06,
  "end": 413.06
 },
 {
  "input": "Recognizing a loop can also break down into subproblems.",
  "translatedText": "تشخیص یک حلقه همچنین می تواند به مشکلات فرعی تقسیم شود.",
  "n_reviews": 0,
  "start": 413.68,
  "end": 416.68
 },
 {
  "input": "One reasonable way to do this would be to first recognize the various little edges that make it up.",
  "translatedText": "یک راه معقول برای انجام این کار این است که ابتدا لبه های کوچک مختلفی را که آن را تشکیل می دهند شناسایی کنید.",
  "from_community_srt": "شناخت یک حلقه نیز می تواند به سؤظن تبدیل شود یکی از راه های معقول برای انجام این کار این است که ابتدا لبه های مختلف کمی را تشخیص می دهد که باعث ایجاد آن می شود",
  "n_reviews": 0,
  "start": 417.28,
  "end": 422.78
 },
 {
  "input": "Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7, is really just a long edge, or maybe you think of it as a certain pattern of several smaller edges.",
  "translatedText": "به طور مشابه، یک خط بلند، مانند آن چیزی که ممکن است در ارقام 1 یا 4 یا 7 ببینید، در واقع فقط یک لبه بلند است، یا شاید شما آن را به عنوان الگوی خاصی از چندین لبه کوچکتر در نظر بگیرید.",
  "from_community_srt": "به طور مشابه یک خط طولانی مانند نوعی که ممکن است در رقم 1 یا 4 یا 7 مشاهده کنید خوب این واقعا یک لبه طولانی است یا شاید شما آن را به عنوان یک الگوی خاص از چند لبه کوچکتر تصور کنید",
  "n_reviews": 0,
  "start": 423.78,
  "end": 434.32
 },
 {
  "input": "So maybe our hope is that each neuron in the second layer of the network corresponds with the various relevant little edges.",
  "translatedText": "بنابراین شاید امید ما این باشد که هر نورون در لایه دوم شبکه با لبه های کوچک مرتبط مختلف مطابقت داشته باشد.",
  "from_community_srt": "بنابراین شاید امید ما این باشد که هر نورون در لایه دوم شبکه متناظر با لبه های کوچک مختلف مربوطه است",
  "n_reviews": 0,
  "start": 435.14,
  "end": 442.72
 },
 {
  "input": "Maybe when an image like this one comes in, it lights up all of the neurons associated with around 8 to 10 specific little edges, which in turn lights up the neurons associated with the upper loop and a long vertical line, and those light up the neuron associated with a 9.",
  "translatedText": "شاید وقتی تصویری مانند این می آید، تمام نورون های مرتبط با حدود 8 تا 10 لبه کوچک خاص را روشن می کند، که به نوبه خود نورون های مرتبط با حلقه بالایی و یک خط عمودی طولانی را روشن می کند و آن ها نور را روشن می کنند. نورون مرتبط با 9.",
  "from_community_srt": "شاید زمانی که یک تصویر مانند این یکی در می آید شبکه همه نورون ها را روشن می کند همراه با حدود هشت تا ده لبه خاص خاص که به نوبه خود نورون های مرتبط با حلقه بالا و یک خط عمودی طولانی را روشن می کنند و آنهایی که نورون را با یک 9 را روشن می کنند",
  "n_reviews": 0,
  "start": 443.54,
  "end": 459.72
 },
 {
  "input": "Whether or not this is what our final network actually does is another question, one that I'll come back to once we see how to train the network, but this is a hope that we might have, a sort of goal with the layered structure like this.",
  "translatedText": "اینکه آیا این همان کاری است که شبکه نهایی ما واقعاً انجام می دهد یا خیر، سؤال دیگری است که وقتی ببینیم چگونه شبکه را آموزش دهیم به آن باز خواهم گشت، اما این امیدی است که ممکن است داشته باشیم، نوعی هدف با ساختار لایه ای. مثل این.",
  "from_community_srt": "چرا که نه این همان چیزی است که شبکه نهایی ما در واقع انجام می دهد، یک سوال دیگر وجود دارد، که من می توانم ببینم چگونه می توانم شبکه را آموزش دهم اما این یک امیدواری است که ما ممکن است داشته باشیم.",
  "n_reviews": 0,
  "start": 460.68,
  "end": 472.54
 },
 {
  "input": "Moreover, you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks.",
  "translatedText": "علاوه بر این، می‌توانید تصور کنید که چگونه قادر به تشخیص لبه‌ها و الگوهای این چنینی برای سایر کارهای تشخیص تصویر واقعاً مفید است.",
  "from_community_srt": "نوعی هدف با ساختار لایه ای مانند این علاوه بر این شما می توانید تصور کنید که چگونه قابلیت تشخیص لبه ها و الگوهای مانند این واقعا برای دیگر وظایف تشخیص تصویر مفید است",
  "n_reviews": 0,
  "start": 473.16,
  "end": 480.3
 },
 {
  "input": "And even beyond image recognition, there are all sorts of intelligent things you might want to do that break down into layers of abstraction.",
  "translatedText": "و حتی فراتر از تشخیص تصویر، انواع کارهای هوشمندانه ای وجود دارد که ممکن است بخواهید انجام دهید که به لایه های انتزاعی تقسیم می شوند.",
  "from_community_srt": "و حتی فراتر از شناخت  تصویر، همه انواع چیزهای هوشمند وجود دارد .",
  "n_reviews": 0,
  "start": 480.88,
  "end": 487.28
 },
 {
  "input": "Parsing speech, for example, involves taking raw audio and picking out distinct sounds, which combine to make certain syllables, which combine to form words, which combine to make up phrases and more abstract thoughts, etc.",
  "translatedText": "به عنوان مثال، تجزیه گفتار شامل گرفتن صوت خام و انتخاب صداهای متمایز است که با هم ترکیب می شوند و هجاهای خاصی را می سازند، که ترکیب می شوند و کلمات را می سازند، که ترکیب می شوند تا عبارات و افکار انتزاعی تر و غیره را بسازند.",
  "from_community_srt": "شما ممکن است بخواهید این کار را با تقسیم لایه های انتزاعی انجام دهید برای مثال تجزیه گفتار شامل گرفتن صدای خام و انتخاب صداهای متمایز است که ترکیبی از ساختن هجا های خاص کدام ترکیب را برای شکل دهی به کلمات تشکیل می دهند که ترکیب را به عبارات و افکار انتزاعی و غیره ترکیب کنید",
  "n_reviews": 0,
  "start": 488.04,
  "end": 500.06
 },
 {
  "input": "But getting back to how any of this actually works, picture yourself right now designing how exactly the activations in one layer might determine the activations in the next.",
  "translatedText": "اما برای بازگشت به نحوه عملکرد هر یک از اینها، خود را در حال طراحی تصور کنید که دقیقاً چگونه فعال سازی در یک لایه ممکن است لایه بعدی را تعیین کند.",
  "from_community_srt": "ما به عقب بر گردیم به این که چگونه هر یک از اینها در حال حاضر طراحی تصویر خود را انجام می دهد",
  "n_reviews": 0,
  "start": 501.1,
  "end": 509.92
 },
 {
  "input": "The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits.",
  "translatedText": "هدف این است که مکانیزمی داشته باشیم که بتواند پیکسل ها را به لبه ها یا لبه ها را به الگوها یا الگوها را به ارقام ترکیب کند.",
  "from_community_srt": "چگونه دقیقا فعال سازی در یک لایه ممکن است فعال سازی در لایه بعدی را تعیین کند؟ هدف این است که یک مکانیسم داشته باشید که احتمالا پیکسل ها را به لبه ها تبدیل می کند یا لبه ها را به الگوها یا الگوها را به رقم تبدیل می کند و در یک مثال بسیار خاص برای بزرگنمایی است",
  "n_reviews": 0,
  "start": 510.86,
  "end": 518.98
 },
 {
  "input": "And to zoom in on one very specific example, let's say the hope is for one particular neuron in the second layer to pick up on whether or not the image has an edge in this region here.",
  "translatedText": "و برای بزرگنمایی روی یک مثال بسیار خاص، بیایید بگوییم امیدواریم که یک نورون خاص در لایه دوم تشخیص دهد که آیا تصویر در این ناحیه لبه دارد یا خیر.",
  "from_community_srt": "بگذارید بگوییم امید در مورد  یک نورون خاص در لایه دوم است برای انتخاب اینکه آیا تصویر دارای لبه در این منطقه در اینجا هست یا خیر؟",
  "n_reviews": 0,
  "start": 519.44,
  "end": 530.62
 },
 {
  "input": "The question at hand is what parameters should the network have?",
  "translatedText": "سوالی که مطرح است این است که شبکه چه پارامترهایی باید داشته باشد؟",
  "n_reviews": 0,
  "start": 531.44,
  "end": 535.1
 },
 {
  "input": "What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern, or any other pixel pattern, or the pattern that several edges can make a loop, and other such things?",
  "translatedText": "چه صفحه‌ها و دستگیره‌هایی را باید به گونه‌ای تنظیم کنید که به اندازه کافی رسا باشد که به طور بالقوه این الگو، یا هر الگوی پیکسلی دیگر، یا الگوی که چندین لبه می‌تواند یک حلقه ایجاد کند، و موارد دیگر از این قبیل را به تصویر بکشد؟",
  "from_community_srt": "سوال فعلی این است که شبکه باید چه  پارامترهایی داشته باشد و کدامیک از آنها باید بتوانید تحریک کردن را به گونه ای بیان کنند که به طور بالقوه بتواند این الگو را ضبط کند یا هر الگوی دیگری از پیکسل یا الگو که چند لبه می تواند یک حلقه و دیگر چیزهای دیگر را ایجاد کند؟",
  "n_reviews": 0,
  "start": 535.64,
  "end": 547.78
 },
 {
  "input": "Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer.",
  "translatedText": "خوب، کاری که ما انجام خواهیم داد این است که به هر یک از اتصالات بین نورون ما و نورون های لایه اول یک وزن اختصاص دهیم.",
  "from_community_srt": "خوب، آنچه ما انجام خواهیم داد این است که وزن هر یک از اتصالات بین نورون هایمان و نورون های لایه اول را حساب کنیم",
  "n_reviews": 0,
  "start": 548.72,
  "end": 555.56
 },
 {
  "input": "These weights are just numbers.",
  "translatedText": "این وزن ها فقط اعداد هستند.",
  "n_reviews": 0,
  "start": 556.32,
  "end": 557.7
 },
 {
  "input": "Then take all of those activations from the first layer and compute their weighted sum according to these weights.",
  "translatedText": "سپس تمام آن فعال‌سازی‌ها را از لایه اول بگیرید و مجموع وزنی آنها را با توجه به این وزن‌ها محاسبه کنید.",
  "from_community_srt": "این وزن ها فقط عدد هستند سپس تمام این فعال سازی ها را از لایه اول انجام داده و مجموع وزن آنها را با توجه به این وزن ها محاسبه می کنیم",
  "n_reviews": 0,
  "start": 558.54,
  "end": 565.5
 },
 {
  "input": "I find it helpful to think of these weights as being organized into a little grid of their own, and I'm going to use green pixels to indicate positive weights, and red pixels to indicate negative weights, where the brightness of that pixel is some loose depiction of the weight's value.",
  "translatedText": "من فکر می کنم که این وزن ها به عنوان یک شبکه کوچک سازماندهی شده اند مفید است، و من از پیکسل های سبز برای نشان دادن وزن های مثبت و از پیکسل های قرمز برای نشان دادن وزن های منفی استفاده می کنم، جایی که روشنایی آن پیکسل مقداری است. تصویری آزاد از ارزش وزن",
  "from_community_srt": "به این فکر کنید که این وزنها به صورت یک شبکه کوچک از خودشان سازماندهی شده اند و من قصد دارم از پیکسل های سبز برای نشان دادن وزن مثبت و پیکسل های قرمز برای نشان دادن وزن های منفی استفاده کنم در کجا روشنایی آن پیکسل تصویر بیانگر ارزش وزن است؟ اکنون ما وزن هایی را که تقریبا تمام پیکسل های صفر را تشکیل می دهند، ساخته ایم",
  "n_reviews": 0,
  "start": 567.7,
  "end": 581.78
 },
 {
  "input": "Now if we made the weights associated with almost all of the pixels zero except for some positive weights in this region that we care about, then taking the weighted sum of all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about.",
  "translatedText": "حال اگر وزن‌های مرتبط با تقریباً همه پیکسل‌ها را صفر کنیم، به جز برخی از وزن‌های مثبت در این ناحیه که به آن‌ها اهمیت می‌دهیم، پس گرفتن مجموع وزنی تمام مقادیر پیکسل واقعاً به معنای جمع کردن مقادیر پیکسل است. منطقه ای که ما به آن اهمیت می دهیم",
  "from_community_srt": "به جز برخی از وزنه های مثبت در این منطقه که ما به آن اهمیت می دهیم سپس وزن مجموع تمام مقادیر پیکسل واقعا فقط مقدار ارزش پیکسل را فقط در منطقه ای که مورد نظر ماست را بدست می آوریم",
  "n_reviews": 0,
  "start": 582.78,
  "end": 597.82
 },
 {
  "input": "And if you really wanted to pick up on whether there's an edge here, what you might do is have some negative weights associated with the surrounding pixels.",
  "translatedText": "و اگر واقعاً می‌خواهید بفهمید که آیا لبه‌ای در اینجا وجود دارد یا خیر، کاری که ممکن است انجام دهید این است که وزن‌های منفی مرتبط با پیکسل‌های اطراف داشته باشید.",
  "from_community_srt": "و اگر شما واقعا می خواهید آن را انتخاب کنید که آیا وجود دارد یا خیر، لبه در اینجا چیزی است که شما ممکن است به برخی از وزن های منفی",
  "n_reviews": 0,
  "start": 599.14,
  "end": 606.6
 },
 {
  "input": "Then the sum is largest when those middle pixels are bright but the surrounding pixels are darker.",
  "translatedText": "سپس مجموع زمانی که آن پیکسل‌های میانی روشن هستند اما پیکسل‌های اطراف تیره‌تر هستند، بزرگ‌تر است.",
  "from_community_srt": "مرتبط با پیکسل های اطراف بیانجامد سپس وقتی که پیکسل های متوسط ​​روشن هستند، اما پیکسل های اطراف آن تیره تر هستند، جمع بیشترین مقدار است",
  "n_reviews": 0,
  "start": 607.48,
  "end": 612.7
 },
 {
  "input": "When you compute a weighted sum like this, you might come out with any number, but for this network what we want is for activations to be some value between 0 and 1.",
  "translatedText": "وقتی یک مجموع وزنی مانند این را محاسبه می کنید، ممکن است با هر عددی بیرون بیایید، اما برای این شبکه چیزی که ما می خواهیم این است که مقدار فعال سازی ها بین 0 و 1 باشد.",
  "from_community_srt": "هنگامی که یک مقدار وزنی مانند این را محاسبه میکنید، ممکن است هر عددی بیرون بیاید اما  چیزی که ما برای این شبکه می خواهیم این است که برای فعال سازی یک مقدار بین 0 و 1 باشد",
  "n_reviews": 0,
  "start": 614.26,
  "end": 623.54
 },
 {
  "input": "So a common thing to do is to pump this weighted sum into some function that squishes the real number line into the range between 0 and 1.",
  "translatedText": "بنابراین یک کار معمول این است که این مجموع وزنی را به تابعی پمپ کنیم که خط اعداد واقعی را در محدوده بین 0 و 1 قرار می دهد.",
  "from_community_srt": "بنابراین یک چیز مشترک برای انجام این کار این است که این مقدار وزنی را به برخی از توابعی که خط عدد حقیقی  را به محدوده بین 0 و 1 تبدیل می کنند، ارسال کنید",
  "n_reviews": 0,
  "start": 624.12,
  "end": 632.14
 },
 {
  "input": "And a common function that does this is called the sigmoid function, also known as a logistic curve.",
  "translatedText": "و یک تابع رایج که این کار را انجام می دهد تابع سیگموئید نامیده می شود که به عنوان منحنی لجستیک نیز شناخته می شود.",
  "from_community_srt": "یک تابع رایج که این کار را انجام می دهد، تابع سیگموئید نامیده می شود که به عنوان یک منحنی لجستیک نیز شناخته می شود",
  "n_reviews": 0,
  "start": 632.46,
  "end": 637.42
 },
 {
  "input": "Basically very negative inputs end up close to 0, positive inputs end up close to 1, and it just steadily increases around the input 0.",
  "translatedText": "اساساً ورودی های بسیار منفی نزدیک به 0، ورودی های مثبت به 1 نزدیک می شوند و به طور پیوسته در اطراف ورودی 0 افزایش می یابد.",
  "from_community_srt": "در واقع ورودی های بسیار منفی در نهایت نزدیک به صفر هستند و ورودی های بسیار مثبتی به نزدیک به 1 می رسند",
  "n_reviews": 0,
  "start": 638.0,
  "end": 646.6
 },
 {
  "input": "So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is.",
  "translatedText": "بنابراین فعال شدن نورون در اینجا اساساً معیاری برای مثبت بودن مجموع وزنی مربوطه است.",
  "from_community_srt": "و فقط به طور پیوسته در اطراف ورودی 0 افزایش می یابد بنابراین فعال سازی نورون در اینجا اساسا یک اندازه گیری از چگونگی مثبت بودن مجموع وزن است",
  "n_reviews": 0,
  "start": 649.12,
  "end": 656.36
 },
 {
  "input": "But maybe it's not that you want the neuron to light up when the weighted sum is bigger than 0.",
  "translatedText": "اما شاید اینطور نباشد که بخواهید نورون زمانی که مجموع وزنی بزرگتر از 0 است روشن شود.",
  "from_community_srt": "اما شاید وقتی که مجموع وزنی بزرگتر از 0 باشد آنطور که شما می خواهید نورون را روشن نکند شاید فقط بخواهید وقتی که جمع بزرگتر از 10 باشد  آن را فعال کنید",
  "n_reviews": 0,
  "start": 657.54,
  "end": 661.88
 },
 {
  "input": "Maybe you only want it to be active when the sum is bigger than say 10.",
  "translatedText": "شاید بخواهید فقط زمانی فعال باشد که مجموع آن بزرگتر از مثلاً 10 باشد.",
  "n_reviews": 0,
  "start": 662.28,
  "end": 666.36
 },
 {
  "input": "That is, you want some bias for it to be inactive.",
  "translatedText": "یعنی شما می خواهید کمی سوگیری برای غیر فعال بودن آن داشته باشید.",
  "n_reviews": 0,
  "start": 666.84,
  "end": 670.26
 },
 {
  "input": "What we'll do then is just add in some other number like negative 10 to this weighted sum before plugging it through the sigmoid squishification function.",
  "translatedText": "کاری که ما انجام خواهیم داد این است که فقط یک عدد دیگر مانند منفی 10 را به این مجموع وزنی اضافه کنیم قبل از اینکه آن را از طریق تابع کوبیدن سیگموئید وصل کنیم.",
  "from_community_srt": "به این دلیل که می خواهید برخی از بایاس برای آن غیر فعال باشد آنچه که ما انجام خواهیم داد این است که فقط عدد دیگری را مانند عدد منفی 10  به این مجموع وزن اضافه کنیم",
  "n_reviews": 0,
  "start": 671.38,
  "end": 679.66
 },
 {
  "input": "That additional number is called the bias.",
  "translatedText": "به آن عدد اضافی سوگیری می گویند.",
  "n_reviews": 0,
  "start": 680.58,
  "end": 682.44
 },
 {
  "input": "So the weights tell you what pixel pattern this neuron in the second layer is picking up on, and the bias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active.",
  "translatedText": "بنابراین وزن‌ها به شما می‌گویند که این نورون در لایه دوم چه الگوی پیکسلی را انتخاب می‌کند، و سوگیری به شما می‌گوید که قبل از اینکه نورون شروع به فعال شدن معنی‌دار کند، مجموع وزنی چقدر باید باشد.",
  "from_community_srt": "قبل از اتصال به تابع انقباضی سیگموئید عدد اضافه شده بایاس نامیده می شود بنابراین وزنها به شما میگویند که کدام یک از الگوی پیکسل این نورون در لایه دوم برداشته شده است و بایاس به شما می گوید که قبل از اینکه نورون شروع به فعال شدن معنی دار کند، باید مقدار وزنی بالاتری داشته باشد",
  "n_reviews": 0,
  "start": 683.46,
  "end": 695.18
 },
 {
  "input": "And that is just one neuron.",
  "translatedText": "و این فقط یک نورون است.",
  "n_reviews": 0,
  "start": 696.12,
  "end": 697.68
 },
 {
  "input": "Every other neuron in this layer is going to be connected to all 784 pixel neurons from the first layer, and each one of those 784 connections has its own weight associated with it.",
  "translatedText": "هر نورون دیگر در این لایه قرار است به تمام نورون های 784 پیکسلی از لایه اول متصل شود و هر یک از آن 784 اتصال وزن خاص خود را دارد.",
  "from_community_srt": "و این فقط یک نورون است هر نورون دیگر در این لایه، به همه نورون های 784 پیکسل از لایه اول متصل می شود و هر کدام از 784 اتصالات وزن خود را با آن مرتبط می کند",
  "n_reviews": 0,
  "start": 698.28,
  "end": 710.94
 },
 {
  "input": "Also, each one has some bias, some other number that you add on to the weighted sum before squishing it with the sigmoid.",
  "translatedText": "همچنین، هر یک مقداری سوگیری دارد، تعدادی عدد دیگر که قبل از له کردن آن با سیگموئید به مجموع وزنی اضافه می‌کنید.",
  "from_community_srt": "همچنین هر یک از بایاس ها تعدادی عدد دیگری که شما را با مجموع وزنی اضافه می کند قبل از آن که با سیگموئید مخلوط شود و",
  "n_reviews": 0,
  "start": 711.6,
  "end": 717.6
 },
 {
  "input": "And that's a lot to think about!",
  "translatedText": "و این جای تامل زیادی دارد!",
  "n_reviews": 0,
  "start": 718.11,
  "end": 719.54
 },
 {
  "input": "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights, along with 16 biases.",
  "translatedText": "با این لایه پنهان از 16 نورون، در مجموع 784 ضربدر 16 وزن، همراه با 16 سوگیری است.",
  "from_community_srt": "این مقدار زیادی برای فکر کردن درباره این لایه پنهان از 16 نورون است این یک مجموعه از 784 بار 16 وزن همراه با 16 بایاس است",
  "n_reviews": 0,
  "start": 719.96,
  "end": 727.98
 },
 {
  "input": "And all of that is just the connections from the first layer to the second.",
  "translatedText": "و همه اینها فقط اتصالات از لایه اول به لایه دوم است.",
  "n_reviews": 0,
  "start": 728.84,
  "end": 731.94
 },
 {
  "input": "The connections between the other layers also have a bunch of weights and biases associated with them.",
  "translatedText": "اتصالات بین لایه های دیگر نیز دارای دسته ای از وزن ها و سوگیری های مرتبط با آنها است.",
  "from_community_srt": "و همه اینها فقط اتصال از لایه اول به دوم ارتباطات بین لایه های دیگر است همچنین، دسته ای از وزن و بایاس مرتبط با آنها داریم",
  "n_reviews": 0,
  "start": 732.52,
  "end": 737.34
 },
 {
  "input": "All said and done, this network has almost exactly 13,000 total weights and biases.",
  "translatedText": "همه گفته‌ها و انجام‌ها، این شبکه تقریباً دقیقاً 13000 وزن و بایاس کل دارد.",
  "from_community_srt": "همه خوانده شده اند و این تقریبا  شبکه دقیق است مجموع وزن ها و بایاس ها 13000 است 13،000 گره وارتباطاتی است که می توان آن را تغییر داده و تبدیل کرد تا این شبکه به شیوه های مختلف رفتار کند",
  "n_reviews": 0,
  "start": 738.34,
  "end": 743.8
 },
 {
  "input": "13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways.",
  "translatedText": "13000 دستگیره و شماره گیری که می توان آنها را تغییر داد و چرخاند تا این شبکه به روش های مختلف رفتار کند.",
  "n_reviews": 0,
  "start": 743.8,
  "end": 749.96
 },
 {
  "input": "So when we talk about learning, what that's referring to is getting the computer to find a valid setting for all of these many many numbers so that it'll actually solve the problem at hand.",
  "translatedText": "بنابراین وقتی در مورد یادگیری صحبت می کنیم، منظور این است که کامپیوتر را برای یافتن یک تنظیم معتبر برای همه این اعداد بسیار زیاد به طوری که در واقع مشکل موجود را حل کند.",
  "from_community_srt": "پس وقتی ما در مورد یادگیری صحبت می کنیم؟ آنچه که به آن اشاره شده است، دریافت کامپیوتر برای پیدا کردن یک تنظیم معتبر برای تمام این تعداد بسیار بسیار زیاد اعداد است به طوری که در واقع  آن را حل کند",
  "n_reviews": 0,
  "start": 751.04,
  "end": 761.36
 },
 {
  "input": "One thought experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand, purposefully tweaking the numbers so that the second layer picks up on edges, the third layer picks up on patterns, etc.",
  "translatedText": "یک آزمایش فکری که در عین حال سرگرم کننده و وحشتناک است این است که تصور کنید بنشینید و تمام این وزن ها و سوگیری ها را با دست تنظیم کنید، به طور هدفمند اعداد را تغییر دهید تا لایه دوم لبه ها را بگیرد، لایه سوم روی الگوها بنشیند. و غیره.",
  "from_community_srt": "مشکل موجود این است که آزمایشی را تصور کنید که در آن زمان سرگرم کننده و به نوعی وحشتناک است. تصور کنید نشسته اید و تمام این وزن و بایاس ها را با دست تنظیم کرده اید به طور هدفمند، اعداد را تغییر داده اید، به طوری که لایه دوم بر روی لبه ها، لایه سوم بر الگوها و غیره قرار می گیرد",
  "n_reviews": 0,
  "start": 762.62,
  "end": 776.58
 },
 {
  "input": "I personally find this satisfying rather than just treating the network as a total black box, because when the network doesn't perform the way you anticipate, if you've built up a little bit of a relationship with what those weights and biases actually mean, you have a starting place for experimenting with how to change the structure to improve.",
  "translatedText": "من شخصاً این را رضایت‌بخش می‌دانم تا اینکه شبکه را فقط به عنوان یک جعبه سیاه کامل نگاه کنم، زیرا وقتی شبکه آنطور که شما پیش‌بینی می‌کنید عمل نمی‌کند، اگر کمی با معنای واقعی آن وزن‌ها و سوگیری‌ها ارتباط برقرار کرده باشید. ، شما یک مکان شروع برای آزمایش نحوه تغییر ساختار برای بهبود دارید.",
  "from_community_srt": "من شخصا این را رضایت بخش تر از خواندن شبکه به عنوان یک جعبه سیاه کامل در یافته ام زیرا وقتی شبکه  کار شما را انجام نمی دهد پیش بینی کنید اگر شما با آنچه که این وزن ها و تعصب ها در واقع هستند کمی ارتباط داشته باشید، شما یک نقطه شروع برای",
  "n_reviews": 0,
  "start": 776.98,
  "end": 794.18
 },
 {
  "input": "Or when the network does work but not for the reasons you might expect, digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible solutions.",
  "translatedText": "یا زمانی که شبکه کار می کند، اما نه به دلایلی که ممکن است انتظارش را داشته باشید، کاوش در آنچه که وزن ها و سوگیری ها انجام می دهند، راه خوبی برای به چالش کشیدن مفروضات شما و افشای فضای کامل راه حل های ممکن است.",
  "from_community_srt": "بررسی چگونگی تغییر ساختار برای بهبود و یا زمانی که شبکه کار می کند، دارید؟ اما نه به دلایلی که ممکن است انتظار داشته باشید کنکاش در وزن ها و بایاس ها، راه خوبی برای فائق آمدن به چالش های شما است و واقعا فضای کامل ممکن را افشا می کنید",
  "n_reviews": 0,
  "start": 794.96,
  "end": 805.82
 },
 {
  "input": "By the way, the actual function here is a little cumbersome to write down, don't you think?",
  "translatedText": "به هر حال، نوشتن عملکرد واقعی در اینجا کمی دست و پا گیر است، فکر نمی کنید؟",
  "from_community_srt": "راه حل به هر حال، نوشتن تابع واقعی اینجا کمی کم دردسر است.",
  "n_reviews": 0,
  "start": 806.84,
  "end": 810.68
 },
 {
  "input": "So let me show you a more notationally compact way that these connections are represented.",
  "translatedText": "بنابراین اجازه دهید به شما روشی فشرده تر نشان دهم که این اتصالات نشان داده می شوند.",
  "from_community_srt": "آیا اینطور فکر نمی کنید؟ بنابراین ببایید به شما یک روش جمع و جورتر نشان دهم که این ارتباطات را نشان می دهد.",
  "n_reviews": 0,
  "start": 812.5,
  "end": 817.14
 },
 {
  "input": "This is how you'd see it if you choose to read up more about neural networks.",
  "translatedText": "اگر بخواهید در مورد شبکه های عصبی بیشتر بخوانید، اینگونه خواهید دید.",
  "n_reviews": 0,
  "start": 817.66,
  "end": 820.52
 },
 {
  "input": "Organize all of the activations from one layer into a column as a vector.",
  "translatedText": "همه فعال‌سازی‌ها را از یک لایه در یک ستون سازماندهی کنید، زیرا ماتریس مربوط به اتصالات بین یک لایه و یک نورون خاص در لایه بعدی است.",
  "n_reviews": 0,
  "start": 821.38,
  "end": 820.52
 },
 {
  "input": "Then organize all of the weights as a matrix, where each row of that matrix corresponds to the connections between one layer and a particular neuron in the next layer.",
  "translatedText": "منظور این است که گرفتن مجموع وزنی فعال‌سازی‌ها در لایه اول با توجه به این وزن‌ها، با یکی از عبارت‌های حاصلضرب بردار ماتریس هر چیزی که در سمت چپ اینجا داریم، مطابقت دارد.",
  "from_community_srt": "روش این است که چگونه می توانید آن را ببینید اگر بخواهید درباره شبکه های عصبی بیشتر بخوانید همه فعالسازها را از یک لایه به یک ستون به صورت یک بردار مرتب کنید سپس تمام وزن ها را به صورت ماتریس که هر وزن یک ردیف از آن ماتریس است، سازماندهی کنید بین اتصالات بین یک لایه و یک نورون خاص در لایه بعدی تناظر بر قرار است",
  "n_reviews": 0,
  "start": 821.38,
  "end": 838.0
 },
 {
  "input": "What that means is that taking the weighted sum of the activations in the first layer according to these weights corresponds to one of the terms in the matrix vector product of everything we have on the left here.",
  "translatedText": "به هر حال، بسیاری از یادگیری ماشین فقط به درک خوب جبر خطی بستگی دارد، بنابراین برای هر یک از شما که می‌خواهید درک بصری خوبی برای ماتریس‌ها و معنی ضرب بردار ماتریس داشته باشید، به سری‌هایی که من در آن انجام دادم نگاهی بیندازید. جبر خطی، به ویژه فصل 3.",
  "from_community_srt": "آیا این به معنی است که مجموع وزن های فعالسازها در لایه اول را بر اساس این وزن ها به دست می آوریم؟",
  "n_reviews": 0,
  "start": 838.54,
  "end": 849.88
 },
 {
  "input": "By the way, so much of machine learning just comes down to having a good grasp of linear algebra, so for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means, take a look at the series I did on linear algebra, especially chapter 3.",
  "translatedText": "به بیان خودمان برگردیم، به جای اینکه در مورد اضافه کردن بایاس به هر یک از این مقادیر به طور مستقل صحبت کنیم، آن را با سازماندهی تمام آن بایاس ها در یک بردار، و افزودن کل بردار به محصول بردار ماتریس قبلی، نشان می دهیم.",
  "from_community_srt": "متناظر با هر یک از عبارات در ماتریس بردار محصول، ما  در اینجا در سمت چپ یک چیز داریم با روش های بسیاری که برای یادگیری ماشین وجود دارد، فقط به داشتن یک درک خوب از جبر خطی نیاز است بنابراین برای هر کدام از شما که می خواهید درک بصری خوبی از ماتریس و آنچه که ضرب ماتریس معکوس خوانده می شود داشته باشید  سری های آموزش من در جبر خطی را ببینید",
  "n_reviews": 0,
  "start": 854.0,
  "end": 868.6
 },
 {
  "input": "Back to our expression, instead of talking about adding the bias to each one of these values independently, we represent it by organizing all those biases into a vector, and adding the entire vector to the previous matrix vector product.",
  "translatedText": "سپس به عنوان آخرین مرحله، من یک سیگموئید را در اطراف بیرون می‌پیچم، و چیزی که قرار است نشان‌دهنده آن باشد این است که شما می‌خواهید تابع sigmoid را برای هر جزء خاص از بردار حاصل در داخل اعمال کنید.",
  "from_community_srt": "بخصوص فصل سوم را به بحث خود بر می گردیم  به جای صحبت کردن در مورد اضافه کردن بایاس به هر یک از این مقادیر به طور مستقل ما آن را  با سازماندهی تمام این بایاس ها در یک بردار و اضافه کردن کل بردار به محصول بردار ماتریس قبلی نشان می دهیم",
  "n_reviews": 0,
  "start": 869.24,
  "end": 882.3
 },
 {
  "input": "Then as a final step, I'll wrap a sigmoid around the outside here, and what that's supposed to represent is that you're going to apply the sigmoid function to each specific component of the resulting vector inside.",
  "translatedText": "بنابراین هنگامی که این ماتریس وزن و این بردارها را به عنوان نمادهای خود یادداشت کردید، می‌توانید انتقال کامل فعال‌سازی‌ها را از یک لایه به لایه بعدی در یک بیان کوچک بسیار فشرده و منظم انتقال دهید، و این باعث می‌شود کد مربوطه هم بسیار ساده‌تر و هم ساده‌تر شود. بسیار سریعتر، زیرا بسیاری از کتابخانه ها ضرب ماتریس را بهینه می کنند.",
  "from_community_srt": "سپس به عنوان آخرین مرحله من یک سیگموئید در پیرامون آن قرار خواهم داد و آنچه که تصور می شود این است که شما قصد دارید تابع سیگموئید را به هر یک از موارد خاص اعمال کنید",
  "n_reviews": 0,
  "start": 883.28,
  "end": 894.74
 },
 {
  "input": "So once you write down this weight matrix and these vectors as their own symbols, you can communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression, and this makes the relevant code both a lot simpler and a lot faster, since many libraries optimize the heck out of matrix multiplication.",
  "translatedText": "به خاطر دارید که قبلاً گفتم این نورون ها چیزهایی هستند که اعداد را نگه می دارند؟",
  "from_community_srt": "اجزای  بردار نتیجه داخلی پس وقتی این ماتریس وزن را  و این بردارها را به عنوان نمادهای خودشان  بنویسید، می توانید ببینید انتقال کامل فعال سازی از یک لایه به بعد در بیان مختصر و شفاف و ارتباط برقرار می کند این باعث می شود کد مربوطه هر دو بسیار ساده تر و بسیار سریع تر از بسیاری از کتابخانه های بهینه سازی شده از هک ضرب ماتریس است",
  "n_reviews": 0,
  "start": 895.94,
  "end": 915.66
 },
 {
  "input": "Remember how earlier I said these neurons are simply things that hold numbers?",
  "translatedText": "البته اعداد خاصی که نگه می‌دارند بستگی به تصویری دارد که شما از آن تغذیه می‌کنید، بنابراین در واقع دقیق‌تر است که هر نورون را به عنوان یک تابع در نظر بگیرید، تابعی که خروجی‌های تمام نورون‌های لایه قبلی را می‌گیرد و عددی را بیرون می‌ریزد. بین 0 و 1",
  "from_community_srt": "به یاد داشته باشید همانطور که قبلا گفتم  این نورون ها به سادگی چیزهایی هستند که اعداد را نگه می دارند",
  "n_reviews": 0,
  "start": 917.82,
  "end": 921.46
 },
 {
  "input": "Well of course the specific numbers that they hold depends on the image you feed in, so it's actually more accurate to think of each neuron as a function, one that takes in the outputs of all the neurons in the previous layer and spits out a number between 0 and 1.",
  "translatedText": "در واقع کل شبکه فقط یک تابع است، تابعی که 784 عدد را به عنوان ورودی می گیرد و 10 عدد را به عنوان خروجی می ریزد.",
  "from_community_srt": "خب، البته تعداد مشخصی که در اختیار دارند، وابسته به تصویری است که به آن داده اید بنابراین، در واقع دقیق تر است که هر نورون را به عنوان یک تابع در نظر بگیریم خروجی تمام نورون ها در لایه قبلی است و عدد صفر و یک را جدا می کند واقعا کل شبکه فقط یک تابع است که",
  "n_reviews": 0,
  "start": 922.22,
  "end": 938.34
 },
 {
  "input": "Really the entire network is just a function, one that takes in 784 numbers as an input and spits out 10 numbers as an output.",
  "translatedText": "این یک تابع پیچیده است، تابعی که شامل 13000 پارامتر به شکل این وزن‌ها و سوگیری‌هایی است که الگوهای خاصی را نشان می‌دهند، و شامل تکرار بسیاری از محصولات بردار ماتریس و تابع انقباض سیگموئید است، اما با این وجود فقط یک تابع است و در یک این به نوعی اطمینان بخش است که پیچیده به نظر می رسد.",
  "n_reviews": 0,
  "start": 939.2,
  "end": 947.06
 },
 {
  "input": "It's an absurdly complicated function, one that involves 13,000 parameters in the forms of these weights and biases that pick up on certain patterns, and which involves iterating many matrix vector products and the sigmoid squishification function, but it's just a function nonetheless.",
  "translatedText": "منظورم این است که اگر ساده‌تر بود، چه امیدی داشتیم که بتواند چالش تشخیص ارقام را انجام دهد؟",
  "from_community_srt": "784 عدد به عنوان یک ورودی می گیرد و ده عدد را به عنوان خروجی جدا می کند این بی بدیل است تابع پیچیده ای که شامل سیزده هزار پارامتر در قالب این وزن ها و بایاس ها است که بر روی الگوهای مشخصی قرار می گیرند و شامل تکرار بسیاری از محصولات بردار ماتریکس و تابع انقباضی سیگموئید است با این وجود، این فقط یک تابع است و به نوعی اطمینان بخش است که به نظر پیچیده می آید",
  "n_reviews": 0,
  "start": 947.56,
  "end": 962.64
 },
 {
  "input": "And in a way it's kind of reassuring that it looks complicated.",
  "translatedText": "و چگونه آن چالش را انجام می دهد؟",
  "n_reviews": 0,
  "start": 963.4,
  "end": 966.66
 },
 {
  "input": "I mean if it were any simpler, what hope would we have that it could take on the challenge of recognizing digits?",
  "translatedText": "چگونه این شبکه تنها با مشاهده داده ها، وزن ها و سوگیری های مناسب را یاد می گیرد؟",
  "from_community_srt": "منظورم این است که اگر  ساده تر بود، چه امیدی داشتیم که بتوانیم بر چالش شناخت رقم ها دست یابیم؟",
  "n_reviews": 0,
  "start": 967.34,
  "end": 972.28
 },
 {
  "input": "And how does it take on that challenge?",
  "translatedText": "خب این چیزی است که در ویدیوی بعدی نشان خواهم داد، و همچنین کمی بیشتر در مورد آنچه که این شبکه خاصی که می بینیم واقعاً انجام می دهد، کاوش خواهم کرد.",
  "n_reviews": 0,
  "start": 973.34,
  "end": 974.7
 },
 {
  "input": "How does this network learn the appropriate weights and biases just by looking at data?",
  "translatedText": "حالا فکر می‌کنم باید بگویم مشترک شوید تا از زمانی که ویدیو یا هر ویدیوی جدیدی منتشر می‌شود مطلع شوید، اما در واقع اکثر شما واقعاً اعلان‌هایی را از YouTube دریافت نمی‌کنید، درست است؟",
  "from_community_srt": "و این چالش چطور است؟ چطور این شبکه با توجه به داده ها وزن و بایاس مناسب آموزش می بیند؟ اوه؟",
  "n_reviews": 0,
  "start": 975.08,
  "end": 979.36
 },
 {
  "input": "Well that's what I'll show in the next video, and I'll also dig a little more into what this particular network we're seeing is really doing.",
  "translatedText": "شاید صادقانه‌تر بگویم مشترک شوید تا شبکه‌های عصبی که زیربنای الگوریتم توصیه‌های YouTube هستند این باور را داشته باشند که می‌خواهید محتوای این کانال را ببینید به شما توصیه می‌شود.",
  "from_community_srt": "این چیزی است که من در ویدیوی بعدی نشان می دهم و همچنین کمی بیشتر به آنچه که در این شبکه خاص می بینیم واقعا حیرت زده می شویم.",
  "n_reviews": 0,
  "start": 980.14,
  "end": 986.12
 },
 {
  "input": "Now is the point I suppose I should say subscribe to stay notified about when that video or any new videos come out, but realistically most of you don't actually receive notifications from YouTube, do you?",
  "translatedText": "به هر حال برای اطلاعات بیشتر در جریان باشید.",
  "from_community_srt": "در حال حاضر نقطه ای است که من فکر می کنم باید بگویم برای اطلاع از زمانی که این ویدئو و یا هر گونه ویدئو جدید بیرون می آیند، آنرا subscribe کنید",
  "n_reviews": 0,
  "start": 987.58,
  "end": 997.42
 },
 {
  "input": "Maybe more honestly I should say subscribe so that the neural networks that underlie YouTube's recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you.",
  "translatedText": "از همه کسانی که از این ویدیوها در Patreon حمایت می کنند بسیار سپاسگزاریم.",
  "from_community_srt": "اما واقعاً اکثر شما در حال دریافت اطلاعیه ها از یوتیوب نیستید؟ شاید صادقانه تر به نظر برسد که شبکه های عصبی که YouTube را پایه گذاری کرده اند. الگوریتم پیشنهاد شده بر این باور است که می خواهید محتوایی از این کانال را ببینید تا به شما توصیه شود",
  "n_reviews": 0,
  "start": 998.02,
  "end": 1007.88
 },
 {
  "input": "Anyway, stay posted for more.",
  "translatedText": "من در تابستان امسال برای پیشرفت در سری احتمالات کمی کند بوده ام، اما بعد از این پروژه دوباره وارد آن می شوم، بنابراین کاربران می توانند منتظر به روز رسانی ها باشند.",
  "n_reviews": 0,
  "start": 1008.56,
  "end": 1009.94
 },
 {
  "input": "Thank you very much to everyone supporting these videos on Patreon.",
  "translatedText": "برای پایان دادن به این موضوع، لیشا لی را با خود دارم که کار دکترای خود را در جنبه نظری یادگیری عمیق انجام داد و در حال حاضر در یک شرکت سرمایه گذاری خطرپذیر به نام Amplify Partners کار می کند که با مهربانی مقداری از بودجه را برای این ویدیو فراهم کرد.",
  "from_community_srt": "به هر حال پست برای زمان بیشتری باقی می ماند از همه شما متشکرم که این  فیلم ها در Patreon پشتیبانی می کنید",
  "n_reviews": 0,
  "start": 1010.76,
  "end": 1013.5
 },
 {
  "input": "I've been a little slow to progress in the probability series this summer, but I'm jumping back into it after this project, so patrons you can look out for updates there.",
  "translatedText": "بنابراین لیشا یک چیزی که فکر می کنم باید سریعاً مطرح کنیم این تابع سیگموئید است.",
  "from_community_srt": "در تابستان امسال کمی پیشرفت کرده ام اما من بعد از این پروژه به عقب برمیگردم تا مشتریان بتوانند بهروزرسانیهای خود را بیابند",
  "n_reviews": 0,
  "start": 1014.0,
  "end": 1021.9
 },
 {
  "input": "To close things off here I have with me Lisha Li who did her PhD work on the theoretical side of deep learning and who currently works at a venture capital firm called Amplify Partners who kindly provided some of the funding for this video.",
  "translatedText": "همانطور که می‌دانم شبکه‌های اولیه از این استفاده می‌کنند تا مجموع وزنی مربوطه را در فاصله بین صفر و یک قرار دهند، شما می‌دانید که به نوعی انگیزه این تشبیه بیولوژیکی نورون‌ها، غیرفعال یا فعال است.",
  "from_community_srt": "برای بستن چیزها اینجا من با لیشا لی هستم لی که کار دکترای خود را در زمینه نظری یادگیری عمیق انجام داد و در حال حاضر در یک شرکت سرمایه گذاری  به نام شرکای تقویت همکاری می کند",
  "n_reviews": 0,
  "start": 1023.6,
  "end": 1034.62
 },
 {
  "input": "So Lisha one thing I think we should quickly bring up is this sigmoid function.",
  "translatedText": "دقیقا.",
  "from_community_srt": "او کسی دوستانه مقداری از بودجه این ویدئو را فراهم کرده است، لیشا منحصر بفرد است من فکر می کنم که ما باید سریعا این کار را انجام دهیم",
  "n_reviews": 0,
  "start": 1035.46,
  "end": 1039.12
 },
 {
  "input": "As I understand it early networks use this to squish the relevant weighted sum into that interval between zero and one, you know kind of motivated by this biological analogy of neurons either being inactive or active.",
  "translatedText": "اما تعداد نسبتا کمی از شبکه های مدرن دیگر از سیگموئید استفاده می کنند.",
  "from_community_srt": "همانطور که می دانم شبکه های زودرس این کار را انجام می دهند تا مجموع وزن مربوطه را به این فاصله بین صفر و یک برساند شما نوع انگیزه ای از این سلسله بیولوژیکی نورون یا غیر فعال یا فعال (لیشا) را می شناسید- دقیقا",
  "n_reviews": 0,
  "start": 1039.7,
  "end": 1049.84
 },
 {
  "input": "Exactly.",
  "translatedText": "آره",
  "n_reviews": 0,
  "start": 1050.28,
  "end": 1050.3
 },
 {
  "input": "But relatively few modern networks actually use sigmoid anymore.",
  "translatedText": "این یک نوع مدرسه قدیمی است درست است؟",
  "from_community_srt": "(3B1B) - اما تعداد کمی از شبکه های مدرن در واقع از انواع سیگموئید دیگر  استفاده می کنند.",
  "n_reviews": 0,
  "start": 1050.56,
  "end": 1054.04
 },
 {
  "input": "Yeah.",
  "translatedText": "بله یا بهتر بگوییم relu به نظر می رسد بسیار آسان تر برای آموزش.",
  "n_reviews": 0,
  "start": 1054.32,
  "end": 1054.32
 },
 {
  "input": "It's kind of old school right?",
  "translatedText": "و relu مخفف واحد خطی اصلاح شده است؟",
  "n_reviews": 0,
  "start": 1054.44,
  "end": 1055.54
 },
 {
  "input": "Yeah or rather ReLU seems to be much easier to train.",
  "translatedText": "بله، این نوع تابعی است که در آن شما فقط حداکثر صفر را می گیرید و a را با چیزی که در ویدیو توضیح می دهید به دست می آورید و این به نوعی انگیزه آن چیست، فکر می کنم تا حدی با یک قیاس بیولوژیکی با نورون ها بود. یا فعال می شود یا نه و بنابراین اگر از آستانه خاصی عبور کند، تابع هویت خواهد بود، اما اگر فعال نمی شد، فعال نمی شد، بنابراین صفر می شد، بنابراین یک نوع ساده سازی است.",
  "from_community_srt": "این نوعی مدرسه قدیمی است؟ (لیشا) - آره یا نه ReLU  آموزش بسیار ساده تر به نظر می رسد (3B1B) - و ReLU واقعا برای واحد خطی اصلاح شده است",
  "n_reviews": 0,
  "start": 1055.76,
  "end": 1058.98
 },
 {
  "input": "And ReLU, ReLU stands for rectified linear unit?",
  "translatedText": "استفاده از سیگموئیدها کمکی به آموزش نکرد یا آموزش در برخی موارد بسیار دشوار بود و مردم فقط relu را امتحان کردند و اتفاقاً برای این شبکه‌های عصبی فوق‌العاده عمیق بسیار خوب عمل کرد.",
  "n_reviews": 0,
  "start": 1059.4,
  "end": 1062.34
 },
 {
  "input": "Yes it's this kind of function where you're just taking a max of zero and a where a is given by what you were explaining in the video and what this was sort of motivated from I think was a partially by a biological analogy with how neurons would either be activated or not.",
  "translatedText": "باشه ممنون آلیشیا",
  "from_community_srt": "(لیشا) - بله این نوعی تابع است که در آن فقط حداکثر 0 را می گیرید و در جایی مقدار یک می دهد آنچه را که در ویدیو توضیح دادید و از نظر من این انگیزه بود، من فکر میکنم یک بخشی از زندگی زیستی آنالوگ با چگونگی فعال شدن یا نشدن نورونهاست و اگر چنین باشد، آستانه مشخصی وجود دارد",
  "n_reviews": 0,
  "start": 1062.68,
  "end": 1081.36
 },
 {
  "input": "And so if it passes a certain threshold it would be the identity function but if it did not then it would just not be activated so it'd be zero so it's kind of a simplification.",
  "translatedText": "",
  "from_community_srt": "این می تواند هویت تابع باشد اما اگر آن را انجام نشود، فقط فعال نمی شود بنابراین صفر است، بنابراین این نوع از ساده سازی است",
  "n_reviews": 0,
  "start": 1081.36,
  "end": 1090.84
 },
 {
  "input": "Using sigmoids didn't help training or it was very difficult to train at some point and people just tried ReLU and it happened to work very well for these incredibly deep neural networks.",
  "translatedText": "",
  "from_community_srt": "استفاده از سیگموئید به آموزش کمک نمی کرد، یا آموزش بسیار دشوار بود این در بعضی نقاط است و مردم فقط تلاش می کنند که رله را تجربه کنند و کارش را انجام داد برای این منظور به طور فوق العاده ای بسیار خوب است شبکه های عصبی عمیق (3B1B) - بسیار خوب ممنون لیشا",
  "n_reviews": 0,
  "start": 1091.16,
  "end": 1104.62
 },
 {
  "input": "All right thank you Lisha.",
  "translatedText": "",
  "from_community_srt": "ممنون لیشا",
  "n_reviews": 0,
  "start": 1105.1,
  "end": 1105.64
 }
]