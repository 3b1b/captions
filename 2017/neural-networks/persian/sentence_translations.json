[
 {
  "input": "This is a 3.",
  "translatedText": "",
  "from_community_srt": "این یک سه است.",
  "n_reviews": 0,
  "start": 4.22,
  "end": 5.4
 },
 {
  "input": "It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3.",
  "translatedText": "",
  "from_community_srt": "این عدد به صورت سریع نوشته شده و در رزولوشن بسیار پایین در ابعاد 28 تا 28 پیکسل  رندر شده است. اما مغز شما هیچ مشکلی در شناسایی آن به عنوان یک سه ندارد و من می خواهم یک لحظه به آن توجه کنید.",
  "n_reviews": 0,
  "start": 6.06,
  "end": 13.72
 },
 {
  "input": "And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly.",
  "translatedText": "",
  "from_community_srt": "کورتکس چگونه  است وقتی مغیز اینکار را بدون زحمت انجام می دهد.",
  "n_reviews": 0,
  "start": 14.34,
  "end": 18.96
 },
 {
  "input": "I mean, this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next.",
  "translatedText": "",
  "from_community_srt": "منظورم این است که این، این و این هم به عنوان سه، قابل تشخیص هستند. حتی اگر مقادیر خاص هر پیکسل از یک تصویر به تصویر بعد بسیار متفاوت باشد.",
  "n_reviews": 0,
  "start": 19.7,
  "end": 28.32
 },
 {
  "input": "The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3.",
  "translatedText": "",
  "from_community_srt": "وقتی این سه را می بینید، سلول های حساس به نور خاصی در چشم شما تصاویری ارسال می کنند که وقتی این سه را می بینید، با آن بسیار متفاوت هستند.",
  "n_reviews": 0,
  "start": 28.9,
  "end": 36.94
 },
 {
  "input": "But something in that crazy-smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas.",
  "translatedText": "",
  "from_community_srt": "اما چیزی  در آن وجود دارد که کورتکس بصری هوشمند شما اینها را به عنوان نمایشی از همان ایده حل می کند، در حالی که در عین حال تصاویر دیگر را به مثابه ایده های متمایز با آن تشخیص می دهد",
  "n_reviews": 0,
  "start": 37.52,
  "end": 48.26
 },
 {
  "input": "But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this and outputs a single number between 0 and 10, telling you what it thinks the digit is, well the task goes from comically trivial to dauntingly difficult.",
  "translatedText": "",
  "from_community_srt": "اما اگر من به شما بگویم که بنشینید و یک برنامه برایم بنویسید  که در شبکه 28 تا 28 پیکسل هایی مثل این قرار داشته باشد و خروجی یک عدد بین 0 و 10 باشد، به شما می گوید که فکر می کند این رقم خوبی است، کاری به طرز ناخوشایند مشکل و بی اهمیت.",
  "n_reviews": 0,
  "start": 49.22,
  "end": 66.18
 },
 {
  "input": "Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future.",
  "translatedText": "",
  "from_community_srt": "مگر اینکه زیر سنگ زندگی کنید من فکر می کنم  ایجاد انگیزه ارتباط و اهمیت یادگیری ماشین و شبکه های عصبی را از حال به آینده به شدت مورد نیاز است",
  "n_reviews": 0,
  "start": 67.16,
  "end": 74.64
 },
 {
  "input": "But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing, not as a buzzword but as a piece of math.",
  "translatedText": "",
  "from_community_srt": "اما آنچه که من می خواهم انجام دهم این است که به شما نشان می دهد که شبکه عصبی در واقع چیزی فرضی بدون هیچ پس زمینه است و برای کمک به تجسم کردن آنچه که آن را نه به عنوان یک عبارت مبهم بلکه به عنوان یک قطعه ریاضی انجام می دهد",
  "n_reviews": 0,
  "start": 75.12,
  "end": 84.46
 },
 {
  "input": "My hope is that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote-unquote learning.",
  "translatedText": "",
  "from_community_srt": "امید من فقط این است که شما احساس  کنید که این ساختار خودش انگیزه بخش است و احساس اینکه وقتی شما در مورد یادگیری مستقیم یا غیر مستقیم یک شبکه عصبی می خوانید یا می شنوید، می دانید معنای آن چیست",
  "n_reviews": 0,
  "start": 85.02,
  "end": 94.34
 },
 {
  "input": "This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning.",
  "translatedText": "",
  "from_community_srt": "این ویدئو فقط با ساختار جزئی از آن اختصاص پیدا کرده است و نوعی را دنبال می کند که منجر به یادگیری می شود",
  "n_reviews": 0,
  "start": 95.36,
  "end": 100.26
 },
 {
  "input": "What we're going to do is put together a neural network that can learn to recognize handwritten digits.",
  "translatedText": "",
  "from_community_srt": "آنچه که ما انجام می دهیم، یک شبکه عصبی است که می تواند یاد بگیرد که عدد دست نوشته را تشخیص دهد",
  "n_reviews": 0,
  "start": 100.96,
  "end": 106.04
 },
 {
  "input": "This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quo here, because at the end of the two videos I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer.",
  "translatedText": "",
  "from_community_srt": "این یک مثال تقریبا قدیمی است موضوع را معرفی می کنم و  خوشحالم که وضعیت فعلی را در اینجا قرار می دهم زیرا در پایان دو فیلم است که می خواهم شما را به یک جفت منبع خوب هدایت کنم که می توانید بیشتر بیاموزید و اینکه  کدام کد را می توانید دانلود کنید که این کار را انجام دهد و با آن بازی  کند؟",
  "n_reviews": 0,
  "start": 109.36,
  "end": 123.08
 },
 {
  "input": "There are many many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos you and I are just going to look at the simplest plain vanilla form with no added frills.",
  "translatedText": "",
  "from_community_srt": "در کامپیوتر خودتان تعداد زیادی از انواع شبکه های عصبی وجود دارد و در سال های اخیر به نظر می رسد اینگونه تحقیقات رونق گرفته اند اما در این دو فیلم مقدماتی شما و من فقط می خواهیم به ساده ترین شکل وانیلی ساده نگاه کنیم و بدون هیچ زحمتی اضافه کنیم",
  "n_reviews": 0,
  "start": 125.04,
  "end": 139.18
 },
 {
  "input": "This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds around.",
  "translatedText": "",
  "from_community_srt": "این نوعی ضرورت است پیش نیازی برای درک هر یک از انواع قدرتمند مدرن و به اعتقاد من هنوز هم پیچیدگی زیادی برای ما دارد تا ذهنمان بر آن احاطه یابد",
  "n_reviews": 0,
  "start": 139.86,
  "end": 148.6
 },
 {
  "input": "But even in this simplest form it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do.",
  "translatedText": "",
  "from_community_srt": "اما حتی در این ساده ترین شکل می تواند یاد بگیرد که عدد دست نویس را تشخیص دهد که چیز بسیار جالبی برای یک کامپیوتر است که می تواند انجام دهد.",
  "n_reviews": 0,
  "start": 149.12,
  "end": 156.52
 },
 {
  "input": "And at the same time you'll see how it does fall short of a couple hopes that we might have for it.",
  "translatedText": "",
  "from_community_srt": "و در عین حال شما خواهید دید که چگونه از امیدهای اندکی  که ممکن است برای آن وجود داشته باشد، کاسته می شود",
  "n_reviews": 0,
  "start": 157.48,
  "end": 162.28
 },
 {
  "input": "As the name suggests neural networks are inspired by the brain, but let's break that down.",
  "translatedText": "",
  "from_community_srt": "همانطور که از نامش بر می آید شبکه های عصبی از مغز الهام گرفته اند، اما اجازه دهید آن را ریزتر بررسی کنیم",
  "n_reviews": 0,
  "start": 163.38,
  "end": 168.5
 },
 {
  "input": "What are the neurons, and in what sense are they linked together?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 168.52,
  "end": 171.66
 },
 {
  "input": "Right now when I say neuron all I want you to think about is a thing that holds a number, specifically a number between 0 and 1.",
  "translatedText": "",
  "from_community_srt": "نورون ها چه هستند وارتباط آنها با هم به چه معنا است؟ در حال حاضر وقتی که من می گویم همه نورون ها من می خواهم شما را به فکر کردن در مورد چیزی وادار کنم که یک مقدار عددی را نگه می دارد",
  "n_reviews": 0,
  "start": 172.5,
  "end": 180.44
 },
 {
  "input": "It's really not more than that.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 180.68,
  "end": 182.56
 },
 {
  "input": "For example the network starts with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784 neurons in total.",
  "translatedText": "",
  "from_community_srt": "به طور خاص عددی بین 0 و 1 و واقعا بیشتر از این نیست به عنوان مثال شبکه با یک دسته از نورون های متناظر به هم از 28 در 28 پیکسل تصویر ورودی شروع می شود",
  "n_reviews": 0,
  "start": 183.78,
  "end": 194.22
 },
 {
  "input": "Each one of these holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels.",
  "translatedText": "",
  "from_community_srt": "که در آن 784 نورون در کل، هر یک از این اعداد حفظ شده یک عدد را نشان می دهد که دارای ارزش سیاه و سفید پیکسل متناظر با آن است",
  "n_reviews": 0,
  "start": 194.7,
  "end": 204.38
 },
 {
  "input": "This number inside the neuron is called its activation, and the image you might have in mind here is that each neuron is lit up when its activation is a high number.",
  "translatedText": "",
  "from_community_srt": "دامنه از 0 برای پیکسل های سیاه تا 1 برای پیکسل های سفید متغیر است این عدد در داخل نورون هایی به نام نورون فعال قرار دارد و تصویری است که شما ممکن است در ذهن داشته باشید",
  "n_reviews": 0,
  "start": 205.3,
  "end": 214.16
 },
 {
  "input": "So all of these 784 neurons make up the first layer of our network.",
  "translatedText": "",
  "from_community_srt": "آیا هر نورون زمانی روشن می شود که فعالساز آن یک عدد بزرگ  باشد؟ بنابراین تمام این 784 نورون اولین لایه شبکه ما را تشکیل می دهند",
  "n_reviews": 0,
  "start": 216.72,
  "end": 221.86
 },
 {
  "input": "Now jumping over to the last layer, this has 10 neurons, each representing one of the digits.",
  "translatedText": "",
  "from_community_srt": "اکنون به آخرین لایه می رویم.",
  "n_reviews": 0,
  "start": 226.5,
  "end": 231.36
 },
 {
  "input": "The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit.",
  "translatedText": "",
  "from_community_srt": "این لایه ده نورون دارد که هر کدام از آنها  یک عدد را نشان می دهد فعالساز در این نورون ها دوباره عددی بین صفر و یک است که نشان می دهد سیستم  چقدر فکر می کند تا یک تصویر متناظر با یک رقم داده شده  ارائه دهد؟ همچنین لایه های چندگانه ای به نام لایه های مخفی وجود دارد",
  "n_reviews": 0,
  "start": 232.04,
  "end": 242.12
 },
 {
  "input": "There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark for how on earth this process of recognizing digits is going to be handled.",
  "translatedText": "",
  "from_community_srt": "این مربوط به چه زمانی است؟ تنها باید یک علامت سوال عظیم برای چگونگی انجام این روند تشخیص عددی  وجود داشته باشد",
  "n_reviews": 0,
  "start": 243.04,
  "end": 253.6
 },
 {
  "input": "In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice.",
  "translatedText": "",
  "from_community_srt": "در این شبکه من دو لایه مخفی را انتخاب کردم که هر کدام با 16 نورون است و مسلما این نوع انتخاب دلخواه است",
  "n_reviews": 0,
  "start": 254.26,
  "end": 260.56
 },
 {
  "input": "To be honest I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that was just a nice number to fit on the screen.",
  "translatedText": "",
  "from_community_srt": "صادقانه بگویم من دو لایه را بر اساس این که چگونه می خواهم فقط در یک لحظه ایجاد ساختار را انجام دهم انتخاب کردم",
  "n_reviews": 0,
  "start": 261.02,
  "end": 268.2
 },
 {
  "input": "In practice there is a lot of room for experiment with a specific structure here.",
  "translatedText": "",
  "from_community_srt": "16 عدد خوبی است به این دلیل که تنها عددی است که در عمل با صفحه متناسب است در اینجا اتاق زیادی برای آزمایش با یک ساختار خاص وجود دارد",
  "n_reviews": 0,
  "start": 268.78,
  "end": 272.34
 },
 {
  "input": "The way the network operates, activations in one layer determine the activations of the next layer.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 273.02,
  "end": 278.48
 },
 {
  "input": "And of course the heart of the network as an information processing mechanism comes down to exactly how those activations from one layer bring about activations in the next layer.",
  "translatedText": "",
  "from_community_srt": "نحوه فعال سازی شبکه در یک لایه، فعال سازی لایه بعدی را تعیین می کند و البته قلب شبکه به عنوان یک مکانیسم پردازش اطلاعات، کاملا دقیق است",
  "n_reviews": 0,
  "start": 279.2,
  "end": 288.58
 },
 {
  "input": "It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire.",
  "translatedText": "",
  "from_community_srt": "فعال سازی از یک لایه باعث فعال شدن در لایه بعدی می شود این به معنای آن است که به طور مشابه با شبکه های بیولوژیکی نورونی، بعضی از گروه های نورون شلیک می کنند",
  "n_reviews": 0,
  "start": 289.14,
  "end": 297.18
 },
 {
  "input": "Now the network I'm showing here has already been trained to recognize digits, and let me show you what I mean by that.",
  "translatedText": "",
  "from_community_srt": "بعضی از نورون ها، دیگر نورون ها را روشن می کنند حالا شبکه من نشان می دهم در اینجا شبکه آموزش دیده است تا علامت ها را شناسایی کرده و اجازه دهید به شما نشان دهم که منظور من چیست",
  "n_reviews": 0,
  "start": 298.12,
  "end": 303.4
 },
 {
  "input": "It means if you feed in an image, lighting up all 784 neurons of the input layer according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer which causes some pattern in the one after it, which finally gives some pattern in the output layer.",
  "translatedText": "",
  "from_community_srt": "این بدان معنی است که اگر شما در یک تصویرهمه را روشن کنید 784 نورون لایه ورودی با توجه به روشنایی هر پیکسل در تصویر روشن می شوند این الگوی فعال سازی یک الگوی بسیار خاص در لایه بعدی ایجاد می کند کدام یک از الگوها را بعد از آن ایجاد می کند؟",
  "n_reviews": 0,
  "start": 303.64,
  "end": 322.08
 },
 {
  "input": "And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents.",
  "translatedText": "",
  "from_community_srt": "که در نهایت  برخی از الگوی در لایه خروجی را به دست می دهد و؟ روشنترین نورون لایه خروجی، انتخاب شبکه است تا بگوید این تصویر چه عددی را نمایش می دهد؟",
  "n_reviews": 0,
  "start": 322.56,
  "end": 329.4
 },
 {
  "input": "And before jumping into the math for how one layer influences the next, or how training works, let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently.",
  "translatedText": "",
  "from_community_srt": "و قبل از پریدن به ریاضی برای اینکه چطور یک لایه، لایه بعدی را تحت تاثیر قرار می دهد یا چگونه آموزش می بیند؟ بیایید فقط درباره اینکه چرا حتی انتظار می رود یک ساختار لایه ای مانند این نیز انتظار داشته باشیم که به طور هوشمندانه رفتار کند، صحبت کنیم",
  "n_reviews": 0,
  "start": 332.56,
  "end": 343.52
 },
 {
  "input": "What are we expecting here?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 344.06,
  "end": 345.22
 },
 {
  "input": "What is the best hope for what those middle layers might be doing?",
  "translatedText": "",
  "from_community_srt": "اینجا در انتظار چه چیزی هستیم؟ بهترین امید برای آنچه که لایه های میانی ​​می تواند انجام دهد چیست؟",
  "n_reviews": 0,
  "start": 345.4,
  "end": 347.6
 },
 {
  "input": "Well, when you or I recognize digits, we piece together various components.",
  "translatedText": "",
  "from_community_srt": "خوب وقتی که شما یا من علامت را تشخیص می دهیم، ما اجزای مختلف را با هم ترکیب می کنیم، نه یک حلقه بالا و یک خط در سمت راست است",
  "n_reviews": 0,
  "start": 348.92,
  "end": 353.52
 },
 {
  "input": "A 9 has a loop up top and a line on the right.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 354.2,
  "end": 356.82
 },
 {
  "input": "An 8 also has a loop up top, but it's paired with another loop down low.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 357.38,
  "end": 361.18
 },
 {
  "input": "A 4 basically breaks down into three specific lines, and things like that.",
  "translatedText": "",
  "from_community_srt": "همچنین یک 8 هم یک حلقه در بالا و حلقه دیگری در پایین دارد یک 4 اساسا به سه خط خاص و چیزهایی مانند آن شکسته می شود",
  "n_reviews": 0,
  "start": 361.98,
  "end": 366.82
 },
 {
  "input": "Now in a perfect world, we might hope that each neuron in the second to last layer corresponds with one of these subcomponents, that anytime you feed in an image with, say, a loop up top, like a 9 or an 8, there's some specific neuron whose activation is going to be close to 1.",
  "translatedText": "",
  "from_community_srt": "در حال حاضر در دنیای کامل، ممکن است امیدوار باشیم که هر نورون در لایه دوم تا آخر با یکی از این زیرجزها ارتباط دارد هر بار که شما یک تصویر با یک حلقه در بالا مانند یک 9 یا 8 می بینید برخی از موارد خاص وجود دارد",
  "n_reviews": 0,
  "start": 367.6,
  "end": 383.78
 },
 {
  "input": "And I don't mean this specific loop of pixels, the hope would be that any generally loopy pattern towards the top sets off this neuron.",
  "translatedText": "",
  "from_community_srt": "نورونی فعال می شود که مقدار آن نزدیک به یک است و من به این حلقه خاصی از پیکسل ها فکر نمی کنم این امید است که هر کدام",
  "n_reviews": 0,
  "start": 384.5,
  "end": 391.56
 },
 {
  "input": "That way, going from the third layer to the last one just requires learning which combination of subcomponents corresponds to which digits.",
  "translatedText": "",
  "from_community_srt": "به طور کلی الگوی حلقوی به سمت بالا مجموعه ای از این نورون در راه رفتن از لایه سوم به آخرین لایه را خاموش می کند فقط باید یاد بگیرد  که ترکیبی از اجزای زیر مربوط به کدام است البته این فقط مسئله را به پایین جاده می اندازد",
  "n_reviews": 0,
  "start": 392.44,
  "end": 400.04
 },
 {
  "input": "Of course, that just kicks the problem down the road, because how would you recognize these subcomponents, or even learn what the right subcomponents should be?",
  "translatedText": "",
  "from_community_srt": "از آنجا که این شبکه اجزاء زیر را تشخیص می دهد یا حتی یاد می گیرد که اجزای زیر چه باید  باشند، من هنوز حتی درباره این صحبت نمی کنم که",
  "n_reviews": 0,
  "start": 401.0,
  "end": 407.64
 },
 {
  "input": "And I still haven't even talked about how one layer influences the next, but run with me on this one for a moment.",
  "translatedText": "",
  "from_community_srt": "چگونه یک لایه، لایه بعدی را تحت تاثیر قرار می دهد، اما برای یک لحظه با من در این مورد همراه شوید",
  "n_reviews": 0,
  "start": 408.06,
  "end": 413.06
 },
 {
  "input": "Recognizing a loop can also break down into subproblems.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 413.68,
  "end": 416.68
 },
 {
  "input": "One reasonable way to do this would be to first recognize the various little edges that make it up.",
  "translatedText": "",
  "from_community_srt": "شناخت یک حلقه نیز می تواند به سؤظن تبدیل شود یکی از راه های معقول برای انجام این کار این است که ابتدا لبه های مختلف کمی را تشخیص می دهد که باعث ایجاد آن می شود",
  "n_reviews": 0,
  "start": 417.28,
  "end": 422.78
 },
 {
  "input": "Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7, is really just a long edge, or maybe you think of it as a certain pattern of several smaller edges.",
  "translatedText": "",
  "from_community_srt": "به طور مشابه یک خط طولانی مانند نوعی که ممکن است در رقم 1 یا 4 یا 7 مشاهده کنید خوب این واقعا یک لبه طولانی است یا شاید شما آن را به عنوان یک الگوی خاص از چند لبه کوچکتر تصور کنید",
  "n_reviews": 0,
  "start": 423.78,
  "end": 434.32
 },
 {
  "input": "So maybe our hope is that each neuron in the second layer of the network corresponds with the various relevant little edges.",
  "translatedText": "",
  "from_community_srt": "بنابراین شاید امید ما این باشد که هر نورون در لایه دوم شبکه متناظر با لبه های کوچک مختلف مربوطه است",
  "n_reviews": 0,
  "start": 435.14,
  "end": 442.72
 },
 {
  "input": "Maybe when an image like this one comes in, it lights up all of the neurons associated with around 8 to 10 specific little edges, which in turn lights up the neurons associated with the upper loop and a long vertical line, and those light up the neuron associated with a 9.",
  "translatedText": "",
  "from_community_srt": "شاید زمانی که یک تصویر مانند این یکی در می آید شبکه همه نورون ها را روشن می کند همراه با حدود هشت تا ده لبه خاص خاص که به نوبه خود نورون های مرتبط با حلقه بالا و یک خط عمودی طولانی را روشن می کنند و آنهایی که نورون را با یک 9 را روشن می کنند",
  "n_reviews": 0,
  "start": 443.54,
  "end": 459.72
 },
 {
  "input": "Whether or not this is what our final network actually does is another question, one that I'll come back to once we see how to train the network, but this is a hope that we might have, a sort of goal with the layered structure like this.",
  "translatedText": "",
  "from_community_srt": "چرا که نه این همان چیزی است که شبکه نهایی ما در واقع انجام می دهد، یک سوال دیگر وجود دارد، که من می توانم ببینم چگونه می توانم شبکه را آموزش دهم اما این یک امیدواری است که ما ممکن است داشته باشیم.",
  "n_reviews": 0,
  "start": 460.68,
  "end": 472.54
 },
 {
  "input": "Moreover, you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks.",
  "translatedText": "",
  "from_community_srt": "نوعی هدف با ساختار لایه ای مانند این علاوه بر این شما می توانید تصور کنید که چگونه قابلیت تشخیص لبه ها و الگوهای مانند این واقعا برای دیگر وظایف تشخیص تصویر مفید است",
  "n_reviews": 0,
  "start": 473.16,
  "end": 480.3
 },
 {
  "input": "And even beyond image recognition, there are all sorts of intelligent things you might want to do that break down into layers of abstraction.",
  "translatedText": "",
  "from_community_srt": "و حتی فراتر از شناخت  تصویر، همه انواع چیزهای هوشمند وجود دارد .",
  "n_reviews": 0,
  "start": 480.88,
  "end": 487.28
 },
 {
  "input": "Parsing speech, for example, involves taking raw audio and picking out distinct sounds, which combine to make certain syllables, which combine to form words, which combine to make up phrases and more abstract thoughts, etc.",
  "translatedText": "",
  "from_community_srt": "شما ممکن است بخواهید این کار را با تقسیم لایه های انتزاعی انجام دهید برای مثال تجزیه گفتار شامل گرفتن صدای خام و انتخاب صداهای متمایز است که ترکیبی از ساختن هجا های خاص کدام ترکیب را برای شکل دهی به کلمات تشکیل می دهند که ترکیب را به عبارات و افکار انتزاعی و غیره ترکیب کنید",
  "n_reviews": 0,
  "start": 488.04,
  "end": 500.06
 },
 {
  "input": "But getting back to how any of this actually works, picture yourself right now designing how exactly the activations in one layer might determine the next.",
  "translatedText": "",
  "from_community_srt": "ما به عقب بر گردیم به این که چگونه هر یک از اینها در حال حاضر طراحی تصویر خود را انجام می دهد",
  "n_reviews": 0,
  "start": 501.1,
  "end": 509.92
 },
 {
  "input": "The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits.",
  "translatedText": "",
  "from_community_srt": "چگونه دقیقا فعال سازی در یک لایه ممکن است فعال سازی در لایه بعدی را تعیین کند؟ هدف این است که یک مکانیسم داشته باشید که احتمالا پیکسل ها را به لبه ها تبدیل می کند یا لبه ها را به الگوها یا الگوها را به رقم تبدیل می کند و در یک مثال بسیار خاص برای بزرگنمایی است",
  "n_reviews": 0,
  "start": 510.86,
  "end": 518.98
 },
 {
  "input": "And to zoom in on one very specific example, let's say the hope is for one particular neuron in the second layer to pick up on whether or not the image has an edge in this region here.",
  "translatedText": "",
  "from_community_srt": "بگذارید بگوییم امید در مورد  یک نورون خاص در لایه دوم است برای انتخاب اینکه آیا تصویر دارای لبه در این منطقه در اینجا هست یا خیر؟",
  "n_reviews": 0,
  "start": 519.44,
  "end": 530.62
 },
 {
  "input": "The question at hand is what parameters should the network have?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 531.44,
  "end": 535.1
 },
 {
  "input": "What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern, or any other pixel pattern, or the pattern that several edges can make a loop, and other such things?",
  "translatedText": "",
  "from_community_srt": "سوال فعلی این است که شبکه باید چه  پارامترهایی داشته باشد و کدامیک از آنها باید بتوانید تحریک کردن را به گونه ای بیان کنند که به طور بالقوه بتواند این الگو را ضبط کند یا هر الگوی دیگری از پیکسل یا الگو که چند لبه می تواند یک حلقه و دیگر چیزهای دیگر را ایجاد کند؟",
  "n_reviews": 0,
  "start": 535.64,
  "end": 547.78
 },
 {
  "input": "Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer.",
  "translatedText": "",
  "from_community_srt": "خوب، آنچه ما انجام خواهیم داد این است که وزن هر یک از اتصالات بین نورون هایمان و نورون های لایه اول را حساب کنیم",
  "n_reviews": 0,
  "start": 548.72,
  "end": 555.56
 },
 {
  "input": "These weights are just numbers.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 556.32,
  "end": 557.7
 },
 {
  "input": "Then take all of those activations from the first layer and compute their weighted sum according to these weights.",
  "translatedText": "",
  "from_community_srt": "این وزن ها فقط عدد هستند سپس تمام این فعال سازی ها را از لایه اول انجام داده و مجموع وزن آنها را با توجه به این وزن ها محاسبه می کنیم",
  "n_reviews": 0,
  "start": 558.54,
  "end": 565.5
 },
 {
  "input": "I find it helpful to think of these weights as being organized into a little grid of their own, and I'm going to use green pixels to indicate positive weights, and red pixels to indicate negative weights, where the brightness of that pixel is some loose depiction of the weight's value.",
  "translatedText": "",
  "from_community_srt": "به این فکر کنید که این وزنها به صورت یک شبکه کوچک از خودشان سازماندهی شده اند و من قصد دارم از پیکسل های سبز برای نشان دادن وزن مثبت و پیکسل های قرمز برای نشان دادن وزن های منفی استفاده کنم در کجا روشنایی آن پیکسل تصویر بیانگر ارزش وزن است؟ اکنون ما وزن هایی را که تقریبا تمام پیکسل های صفر را تشکیل می دهند، ساخته ایم",
  "n_reviews": 0,
  "start": 567.7,
  "end": 581.78
 },
 {
  "input": "Now if we made the weights associated with almost all of the pixels zero except for some positive weights in this region that we care about, then taking the weighted sum of all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about.",
  "translatedText": "",
  "from_community_srt": "به جز برخی از وزنه های مثبت در این منطقه که ما به آن اهمیت می دهیم سپس وزن مجموع تمام مقادیر پیکسل واقعا فقط مقدار ارزش پیکسل را فقط در منطقه ای که مورد نظر ماست را بدست می آوریم",
  "n_reviews": 0,
  "start": 582.78,
  "end": 597.82
 },
 {
  "input": "And if you really wanted to pick up on whether there's an edge here, what you might do is have some negative weights associated with the surrounding pixels.",
  "translatedText": "",
  "from_community_srt": "و اگر شما واقعا می خواهید آن را انتخاب کنید که آیا وجود دارد یا خیر، لبه در اینجا چیزی است که شما ممکن است به برخی از وزن های منفی",
  "n_reviews": 0,
  "start": 599.14,
  "end": 606.6
 },
 {
  "input": "Then the sum is largest when those middle pixels are bright but the surrounding pixels are darker.",
  "translatedText": "",
  "from_community_srt": "مرتبط با پیکسل های اطراف بیانجامد سپس وقتی که پیکسل های متوسط ​​روشن هستند، اما پیکسل های اطراف آن تیره تر هستند، جمع بیشترین مقدار است",
  "n_reviews": 0,
  "start": 607.48,
  "end": 612.7
 },
 {
  "input": "When you compute a weighted sum like this, you might come out with any number, but for this network what we want is for activations to be some value between 0 and 1.",
  "translatedText": "",
  "from_community_srt": "هنگامی که یک مقدار وزنی مانند این را محاسبه میکنید، ممکن است هر عددی بیرون بیاید اما  چیزی که ما برای این شبکه می خواهیم این است که برای فعال سازی یک مقدار بین 0 و 1 باشد",
  "n_reviews": 0,
  "start": 614.26,
  "end": 623.54
 },
 {
  "input": "So a common thing to do is to pump this weighted sum into some function that squishes the real number line into the range between 0 and 1.",
  "translatedText": "",
  "from_community_srt": "بنابراین یک چیز مشترک برای انجام این کار این است که این مقدار وزنی را به برخی از توابعی که خط عدد حقیقی  را به محدوده بین 0 و 1 تبدیل می کنند، ارسال کنید",
  "n_reviews": 0,
  "start": 624.12,
  "end": 632.14
 },
 {
  "input": "And a common function that does this is called the sigmoid function, also known as a logistic curve.",
  "translatedText": "",
  "from_community_srt": "یک تابع رایج که این کار را انجام می دهد، تابع سیگموئید نامیده می شود که به عنوان یک منحنی لجستیک نیز شناخته می شود",
  "n_reviews": 0,
  "start": 632.46,
  "end": 637.42
 },
 {
  "input": "Basically very negative inputs end up close to 0, positive inputs end up close to 1, and it just steadily increases around the input 0.",
  "translatedText": "",
  "from_community_srt": "در واقع ورودی های بسیار منفی در نهایت نزدیک به صفر هستند و ورودی های بسیار مثبتی به نزدیک به 1 می رسند",
  "n_reviews": 0,
  "start": 638.0,
  "end": 646.6
 },
 {
  "input": "So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is.",
  "translatedText": "",
  "from_community_srt": "و فقط به طور پیوسته در اطراف ورودی 0 افزایش می یابد بنابراین فعال سازی نورون در اینجا اساسا یک اندازه گیری از چگونگی مثبت بودن مجموع وزن است",
  "n_reviews": 0,
  "start": 649.12,
  "end": 656.36
 },
 {
  "input": "But maybe it's not that you want the neuron to light up when the weighted sum is bigger than 0.",
  "translatedText": "",
  "from_community_srt": "اما شاید وقتی که مجموع وزنی بزرگتر از 0 باشد آنطور که شما می خواهید نورون را روشن نکند شاید فقط بخواهید وقتی که جمع بزرگتر از 10 باشد  آن را فعال کنید",
  "n_reviews": 0,
  "start": 657.54,
  "end": 661.88
 },
 {
  "input": "Maybe you only want it to be active when the sum is bigger than say 10.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 662.28,
  "end": 666.36
 },
 {
  "input": "That is, you want some bias for it to be inactive.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 666.84,
  "end": 670.26
 },
 {
  "input": "What we'll do then is just add in some other number like negative 10 to this weighted sum before plugging it through the sigmoid squishification function.",
  "translatedText": "",
  "from_community_srt": "به این دلیل که می خواهید برخی از بایاس برای آن غیر فعال باشد آنچه که ما انجام خواهیم داد این است که فقط عدد دیگری را مانند عدد منفی 10  به این مجموع وزن اضافه کنیم",
  "n_reviews": 0,
  "start": 671.38,
  "end": 679.66
 },
 {
  "input": "That additional number is called the bias.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 680.58,
  "end": 682.44
 },
 {
  "input": "So the weights tell you what pixel pattern this neuron in the second layer is picking up on, and the bias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active.",
  "translatedText": "",
  "from_community_srt": "قبل از اتصال به تابع انقباضی سیگموئید عدد اضافه شده بایاس نامیده می شود بنابراین وزنها به شما میگویند که کدام یک از الگوی پیکسل این نورون در لایه دوم برداشته شده است و بایاس به شما می گوید که قبل از اینکه نورون شروع به فعال شدن معنی دار کند، باید مقدار وزنی بالاتری داشته باشد",
  "n_reviews": 0,
  "start": 683.46,
  "end": 695.18
 },
 {
  "input": "And that is just one neuron.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 696.12,
  "end": 697.68
 },
 {
  "input": "Every other neuron in this layer is going to be connected to all 784 pixel neurons from the first layer, and each one of those 784 connections has its own weight associated with it.",
  "translatedText": "",
  "from_community_srt": "و این فقط یک نورون است هر نورون دیگر در این لایه، به همه نورون های 784 پیکسل از لایه اول متصل می شود و هر کدام از 784 اتصالات وزن خود را با آن مرتبط می کند",
  "n_reviews": 0,
  "start": 698.28,
  "end": 710.94
 },
 {
  "input": "Also, each one has some bias, some other number that you add on to the weighted sum before squishing it with the sigmoid.",
  "translatedText": "",
  "from_community_srt": "همچنین هر یک از بایاس ها تعدادی عدد دیگری که شما را با مجموع وزنی اضافه می کند قبل از آن که با سیگموئید مخلوط شود و",
  "n_reviews": 0,
  "start": 711.6,
  "end": 717.6
 },
 {
  "input": "And that's a lot to think about!",
  "translatedText": "",
  "n_reviews": 0,
  "start": 718.11,
  "end": 719.54
 },
 {
  "input": "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights, along with 16 biases.",
  "translatedText": "",
  "from_community_srt": "این مقدار زیادی برای فکر کردن درباره این لایه پنهان از 16 نورون است این یک مجموعه از 784 بار 16 وزن همراه با 16 بایاس است",
  "n_reviews": 0,
  "start": 719.96,
  "end": 727.98
 },
 {
  "input": "And all of that is just the connections from the first layer to the second.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 728.84,
  "end": 731.94
 },
 {
  "input": "The connections between the other layers also have a bunch of weights and biases associated with them.",
  "translatedText": "",
  "from_community_srt": "و همه اینها فقط اتصال از لایه اول به دوم ارتباطات بین لایه های دیگر است همچنین، دسته ای از وزن و بایاس مرتبط با آنها داریم",
  "n_reviews": 0,
  "start": 732.52,
  "end": 737.34
 },
 {
  "input": "All said and done, this network has almost exactly 13,000 total weights and biases.",
  "translatedText": "",
  "from_community_srt": "همه خوانده شده اند و این تقریبا  شبکه دقیق است مجموع وزن ها و بایاس ها 13000 است 13،000 گره وارتباطاتی است که می توان آن را تغییر داده و تبدیل کرد تا این شبکه به شیوه های مختلف رفتار کند",
  "n_reviews": 0,
  "start": 738.34,
  "end": 743.8
 },
 {
  "input": "13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 743.8,
  "end": 749.96
 },
 {
  "input": "So when we talk about learning, what that's referring to is getting the computer to find a valid setting for all of these many many numbers so that it'll actually solve the problem at hand.",
  "translatedText": "",
  "from_community_srt": "پس وقتی ما در مورد یادگیری صحبت می کنیم؟ آنچه که به آن اشاره شده است، دریافت کامپیوتر برای پیدا کردن یک تنظیم معتبر برای تمام این تعداد بسیار بسیار زیاد اعداد است به طوری که در واقع  آن را حل کند",
  "n_reviews": 0,
  "start": 751.04,
  "end": 761.36
 },
 {
  "input": "One thought experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand, purposefully tweaking the numbers so that the second layer picks up on edges, the third layer picks up on patterns, etc.",
  "translatedText": "",
  "from_community_srt": "مشکل موجود این است که آزمایشی را تصور کنید که در آن زمان سرگرم کننده و به نوعی وحشتناک است. تصور کنید نشسته اید و تمام این وزن و بایاس ها را با دست تنظیم کرده اید به طور هدفمند، اعداد را تغییر داده اید، به طوری که لایه دوم بر روی لبه ها، لایه سوم بر الگوها و غیره قرار می گیرد",
  "n_reviews": 0,
  "start": 762.62,
  "end": 776.58
 },
 {
  "input": "I personally find this satisfying rather than just treating the network as a total black box, because when the network doesn't perform the way you anticipate, if you've built up a little bit of a relationship with what those weights and biases actually mean, you have a starting place for experimenting with how to change the structure to improve.",
  "translatedText": "",
  "from_community_srt": "من شخصا این را رضایت بخش تر از خواندن شبکه به عنوان یک جعبه سیاه کامل در یافته ام زیرا وقتی شبکه  کار شما را انجام نمی دهد پیش بینی کنید اگر شما با آنچه که این وزن ها و تعصب ها در واقع هستند کمی ارتباط داشته باشید، شما یک نقطه شروع برای",
  "n_reviews": 0,
  "start": 776.98,
  "end": 794.18
 },
 {
  "input": "Or when the network does work but not for the reasons you might expect, digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible solutions.",
  "translatedText": "",
  "from_community_srt": "بررسی چگونگی تغییر ساختار برای بهبود و یا زمانی که شبکه کار می کند، دارید؟ اما نه به دلایلی که ممکن است انتظار داشته باشید کنکاش در وزن ها و بایاس ها، راه خوبی برای فائق آمدن به چالش های شما است و واقعا فضای کامل ممکن را افشا می کنید",
  "n_reviews": 0,
  "start": 794.96,
  "end": 805.82
 },
 {
  "input": "By the way, the actual function here is a little cumbersome to write down, don't you think?",
  "translatedText": "",
  "from_community_srt": "راه حل به هر حال، نوشتن تابع واقعی اینجا کمی کم دردسر است.",
  "n_reviews": 0,
  "start": 806.84,
  "end": 810.68
 },
 {
  "input": "So let me show you a more notationally compact way that these connections are represented.",
  "translatedText": "",
  "from_community_srt": "آیا اینطور فکر نمی کنید؟ بنابراین ببایید به شما یک روش جمع و جورتر نشان دهم که این ارتباطات را نشان می دهد.",
  "n_reviews": 0,
  "start": 812.5,
  "end": 817.14
 },
 {
  "input": "This is how you'd see it if you choose to read up more about neural networks.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 817.66,
  "end": 820.52
 },
 {
  "input": "Organize all of the activations from one layer into a column as a vector.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 821.38,
  "end": 820.52
 },
 {
  "input": "Then organize all of the weights as a matrix, where each row of that matrix corresponds to the connections between one layer and a particular neuron in the next layer.",
  "translatedText": "",
  "from_community_srt": "روش این است که چگونه می توانید آن را ببینید اگر بخواهید درباره شبکه های عصبی بیشتر بخوانید همه فعالسازها را از یک لایه به یک ستون به صورت یک بردار مرتب کنید سپس تمام وزن ها را به صورت ماتریس که هر وزن یک ردیف از آن ماتریس است، سازماندهی کنید بین اتصالات بین یک لایه و یک نورون خاص در لایه بعدی تناظر بر قرار است",
  "n_reviews": 0,
  "start": 821.38,
  "end": 838.0
 },
 {
  "input": "What that means is that taking the weighted sum of the activations in the first layer according to these weights corresponds to one of the terms in the matrix vector product of everything we have on the left here.",
  "translatedText": "",
  "from_community_srt": "آیا این به معنی است که مجموع وزن های فعالسازها در لایه اول را بر اساس این وزن ها به دست می آوریم؟",
  "n_reviews": 0,
  "start": 838.54,
  "end": 849.88
 },
 {
  "input": "By the way, so much of machine learning just comes down to having a good grasp of linear algebra, so for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means, take a look at the series I did on linear algebra, especially chapter 3.",
  "translatedText": "",
  "from_community_srt": "متناظر با هر یک از عبارات در ماتریس بردار محصول، ما  در اینجا در سمت چپ یک چیز داریم با روش های بسیاری که برای یادگیری ماشین وجود دارد، فقط به داشتن یک درک خوب از جبر خطی نیاز است بنابراین برای هر کدام از شما که می خواهید درک بصری خوبی از ماتریس و آنچه که ضرب ماتریس معکوس خوانده می شود داشته باشید  سری های آموزش من در جبر خطی را ببینید",
  "n_reviews": 0,
  "start": 854.0,
  "end": 868.6
 },
 {
  "input": "Back to our expression, instead of talking about adding the bias to each one of these values independently, we represent it by organizing all those biases into a vector, and adding the entire vector to the previous matrix vector product.",
  "translatedText": "",
  "from_community_srt": "بخصوص فصل سوم را به بحث خود بر می گردیم  به جای صحبت کردن در مورد اضافه کردن بایاس به هر یک از این مقادیر به طور مستقل ما آن را  با سازماندهی تمام این بایاس ها در یک بردار و اضافه کردن کل بردار به محصول بردار ماتریس قبلی نشان می دهیم",
  "n_reviews": 0,
  "start": 869.24,
  "end": 882.3
 },
 {
  "input": "Then as a final step, I'll wrap a sigmoid around the outside here, and what that's supposed to represent is that you're going to apply the sigmoid function to each specific component of the resulting vector inside.",
  "translatedText": "",
  "from_community_srt": "سپس به عنوان آخرین مرحله من یک سیگموئید در پیرامون آن قرار خواهم داد و آنچه که تصور می شود این است که شما قصد دارید تابع سیگموئید را به هر یک از موارد خاص اعمال کنید",
  "n_reviews": 0,
  "start": 883.28,
  "end": 894.74
 },
 {
  "input": "So once you write down this weight matrix and these vectors as their own symbols, you can communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression, and this makes the relevant code both a lot simpler and a lot faster, since many libraries optimize the heck out of matrix multiplication.",
  "translatedText": "",
  "from_community_srt": "اجزای  بردار نتیجه داخلی پس وقتی این ماتریس وزن را  و این بردارها را به عنوان نمادهای خودشان  بنویسید، می توانید ببینید انتقال کامل فعال سازی از یک لایه به بعد در بیان مختصر و شفاف و ارتباط برقرار می کند این باعث می شود کد مربوطه هر دو بسیار ساده تر و بسیار سریع تر از بسیاری از کتابخانه های بهینه سازی شده از هک ضرب ماتریس است",
  "n_reviews": 0,
  "start": 895.94,
  "end": 915.66
 },
 {
  "input": "Remember how earlier I said these neurons are simply things that hold numbers?",
  "translatedText": "",
  "from_community_srt": "به یاد داشته باشید همانطور که قبلا گفتم  این نورون ها به سادگی چیزهایی هستند که اعداد را نگه می دارند",
  "n_reviews": 0,
  "start": 917.82,
  "end": 921.46
 },
 {
  "input": "Well of course the specific numbers that they hold depends on the image you feed in, so it's actually more accurate to think of each neuron as a function, one that takes in the outputs of all the neurons in the previous layer and spits out a number between 0 and 1.",
  "translatedText": "",
  "from_community_srt": "خب، البته تعداد مشخصی که در اختیار دارند، وابسته به تصویری است که به آن داده اید بنابراین، در واقع دقیق تر است که هر نورون را به عنوان یک تابع در نظر بگیریم خروجی تمام نورون ها در لایه قبلی است و عدد صفر و یک را جدا می کند واقعا کل شبکه فقط یک تابع است که",
  "n_reviews": 0,
  "start": 922.22,
  "end": 938.34
 },
 {
  "input": "Really the entire network is just a function, one that takes in 784 numbers as an input and spits out 10 numbers as an output.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 939.2,
  "end": 947.06
 },
 {
  "input": "It's an absurdly complicated function, one that involves 13,000 parameters in the forms of these weights and biases that pick up on certain patterns, and which involves iterating many matrix vector products and the sigmoid squishification function, but it's just a function nonetheless.",
  "translatedText": "",
  "from_community_srt": "784 عدد به عنوان یک ورودی می گیرد و ده عدد را به عنوان خروجی جدا می کند این بی بدیل است تابع پیچیده ای که شامل سیزده هزار پارامتر در قالب این وزن ها و بایاس ها است که بر روی الگوهای مشخصی قرار می گیرند و شامل تکرار بسیاری از محصولات بردار ماتریکس و تابع انقباضی سیگموئید است با این وجود، این فقط یک تابع است و به نوعی اطمینان بخش است که به نظر پیچیده می آید",
  "n_reviews": 0,
  "start": 947.56,
  "end": 962.64
 },
 {
  "input": "And in a way it's kind of reassuring that it looks complicated.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 963.4,
  "end": 966.66
 },
 {
  "input": "I mean if it were any simpler, what hope would we have that it could take on the challenge of recognizing digits?",
  "translatedText": "",
  "from_community_srt": "منظورم این است که اگر  ساده تر بود، چه امیدی داشتیم که بتوانیم بر چالش شناخت رقم ها دست یابیم؟",
  "n_reviews": 0,
  "start": 967.34,
  "end": 972.28
 },
 {
  "input": "And how does it take on that challenge?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 973.34,
  "end": 974.7
 },
 {
  "input": "How does this network learn the appropriate weights and biases just by looking at data?",
  "translatedText": "",
  "from_community_srt": "و این چالش چطور است؟ چطور این شبکه با توجه به داده ها وزن و بایاس مناسب آموزش می بیند؟ اوه؟",
  "n_reviews": 0,
  "start": 975.08,
  "end": 979.36
 },
 {
  "input": "Well that's what I'll show in the next video, and I'll also dig a little more into what this particular network we're seeing is really doing.",
  "translatedText": "",
  "from_community_srt": "این چیزی است که من در ویدیوی بعدی نشان می دهم و همچنین کمی بیشتر به آنچه که در این شبکه خاص می بینیم واقعا حیرت زده می شویم.",
  "n_reviews": 0,
  "start": 980.14,
  "end": 986.12
 },
 {
  "input": "Now is the point I suppose I should say subscribe to stay notified about when that video or any new videos come out, but realistically most of you don't actually receive notifications from YouTube, do you?",
  "translatedText": "",
  "from_community_srt": "در حال حاضر نقطه ای است که من فکر می کنم باید بگویم برای اطلاع از زمانی که این ویدئو و یا هر گونه ویدئو جدید بیرون می آیند، آنرا subscribe کنید",
  "n_reviews": 0,
  "start": 987.58,
  "end": 997.42
 },
 {
  "input": "Maybe more honestly I should say subscribe so that the neural networks that underlie YouTube's recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you.",
  "translatedText": "",
  "from_community_srt": "اما واقعاً اکثر شما در حال دریافت اطلاعیه ها از یوتیوب نیستید؟ شاید صادقانه تر به نظر برسد که شبکه های عصبی که YouTube را پایه گذاری کرده اند. الگوریتم پیشنهاد شده بر این باور است که می خواهید محتوایی از این کانال را ببینید تا به شما توصیه شود",
  "n_reviews": 0,
  "start": 998.02,
  "end": 1007.88
 },
 {
  "input": "Anyway, stay posted for more.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1008.56,
  "end": 1009.94
 },
 {
  "input": "Thank you very much to everyone supporting these videos on Patreon.",
  "translatedText": "",
  "from_community_srt": "به هر حال پست برای زمان بیشتری باقی می ماند از همه شما متشکرم که این  فیلم ها در Patreon پشتیبانی می کنید",
  "n_reviews": 0,
  "start": 1010.76,
  "end": 1013.5
 },
 {
  "input": "I've been a little slow to progress in the probability series this summer, but I'm jumping back into it after this project, so patrons you can look out for updates there.",
  "translatedText": "",
  "from_community_srt": "در تابستان امسال کمی پیشرفت کرده ام اما من بعد از این پروژه به عقب برمیگردم تا مشتریان بتوانند بهروزرسانیهای خود را بیابند",
  "n_reviews": 0,
  "start": 1014.0,
  "end": 1021.9
 },
 {
  "input": "To close things off here I have with me Lisha Li who did her PhD work on the theoretical side of deep learning and who currently works at a venture capital firm called Amplify Partners who kindly provided some of the funding for this video.",
  "translatedText": "",
  "from_community_srt": "برای بستن چیزها اینجا من با لیشا لی هستم لی که کار دکترای خود را در زمینه نظری یادگیری عمیق انجام داد و در حال حاضر در یک شرکت سرمایه گذاری  به نام شرکای تقویت همکاری می کند",
  "n_reviews": 0,
  "start": 1023.6,
  "end": 1034.62
 },
 {
  "input": "So Lisha one thing I think we should quickly bring up is this sigmoid function.",
  "translatedText": "",
  "from_community_srt": "او کسی دوستانه مقداری از بودجه این ویدئو را فراهم کرده است، لیشا منحصر بفرد است من فکر می کنم که ما باید سریعا این کار را انجام دهیم",
  "n_reviews": 0,
  "start": 1035.46,
  "end": 1039.12
 },
 {
  "input": "As I understand it early networks use this to squish the relevant weighted sum into that interval between zero and one, you know kind of motivated by this biological analogy of neurons either being inactive or active.",
  "translatedText": "",
  "from_community_srt": "همانطور که می دانم شبکه های زودرس این کار را انجام می دهند تا مجموع وزن مربوطه را به این فاصله بین صفر و یک برساند شما نوع انگیزه ای از این سلسله بیولوژیکی نورون یا غیر فعال یا فعال (لیشا) را می شناسید- دقیقا",
  "n_reviews": 0,
  "start": 1039.7,
  "end": 1049.84
 },
 {
  "input": "Exactly.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1050.28,
  "end": 1050.3
 },
 {
  "input": "But relatively few modern networks actually use sigmoid anymore.",
  "translatedText": "",
  "from_community_srt": "(3B1B) - اما تعداد کمی از شبکه های مدرن در واقع از انواع سیگموئید دیگر  استفاده می کنند.",
  "n_reviews": 0,
  "start": 1050.56,
  "end": 1054.04
 },
 {
  "input": "Yeah.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1054.32,
  "end": 1054.32
 },
 {
  "input": "It's kind of old school right?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1054.44,
  "end": 1055.54
 },
 {
  "input": "Yeah or rather ReLU seems to be much easier to train.",
  "translatedText": "",
  "from_community_srt": "این نوعی مدرسه قدیمی است؟ (لیشا) - آره یا نه ReLU  آموزش بسیار ساده تر به نظر می رسد (3B1B) - و ReLU واقعا برای واحد خطی اصلاح شده است",
  "n_reviews": 0,
  "start": 1055.76,
  "end": 1058.98
 },
 {
  "input": "And ReLU, ReLU stands for rectified linear unit?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1059.4,
  "end": 1062.34
 },
 {
  "input": "Yes it's this kind of function where you're just taking a max of zero and a where a is given by what you were explaining in the video and what this was sort of motivated from I think was a partially by a biological analogy with how neurons would either be activated or not.",
  "translatedText": "",
  "from_community_srt": "(لیشا) - بله این نوعی تابع است که در آن فقط حداکثر 0 را می گیرید و در جایی مقدار یک می دهد آنچه را که در ویدیو توضیح دادید و از نظر من این انگیزه بود، من فکر میکنم یک بخشی از زندگی زیستی آنالوگ با چگونگی فعال شدن یا نشدن نورونهاست و اگر چنین باشد، آستانه مشخصی وجود دارد",
  "n_reviews": 0,
  "start": 1062.68,
  "end": 1081.36
 },
 {
  "input": "And so if it passes a certain threshold it would be the identity function but if it did not then it would just not be activated so it'd be zero so it's kind of a simplification.",
  "translatedText": "",
  "from_community_srt": "این می تواند هویت تابع باشد اما اگر آن را انجام نشود، فقط فعال نمی شود بنابراین صفر است، بنابراین این نوع از ساده سازی است",
  "n_reviews": 0,
  "start": 1081.36,
  "end": 1090.84
 },
 {
  "input": "Using sigmoids didn't help training or it was very difficult to train at some point and people just tried ReLU and it happened to work very well for these incredibly deep neural networks.",
  "translatedText": "",
  "from_community_srt": "استفاده از سیگموئید به آموزش کمک نمی کرد، یا آموزش بسیار دشوار بود این در بعضی نقاط است و مردم فقط تلاش می کنند که رله را تجربه کنند و کارش را انجام داد برای این منظور به طور فوق العاده ای بسیار خوب است شبکه های عصبی عمیق (3B1B) - بسیار خوب ممنون لیشا",
  "n_reviews": 0,
  "start": 1091.16,
  "end": 1104.62
 },
 {
  "input": "All right thank you Lisha.",
  "translatedText": "",
  "from_community_srt": "ممنون لیشا",
  "n_reviews": 0,
  "start": 1105.1,
  "end": 1105.64
 }
]