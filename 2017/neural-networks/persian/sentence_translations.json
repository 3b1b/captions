[
 {
  "translatedText": "این یک 3 است.",
  "input": "This is a 3.",
  "model": "google_nmt",
  "time_range": [
   4.22,
   5.4
  ]
 },
 {
  "translatedText": "این به صورت شلخته نوشته شده و با وضوح بسیار پایین 28x28 پیکسل ارائه شده است، اما مغز شما در تشخیص آن به عنوان 3 مشکلی ندارد.",
  "input": "It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3.",
  "model": "google_nmt",
  "time_range": [
   6.06,
   13.72
  ]
 },
 {
  "translatedText": "و من از شما می خواهم که لحظه ای وقت بگذارید و قدردانی کنید که چقدر دیوانه کننده است که مغزها می توانند این کار را بدون زحمت انجام دهند.",
  "input": "And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly.",
  "model": "google_nmt",
  "time_range": [
   14.34,
   18.96
  ]
 },
 {
  "translatedText": "منظورم این است که این، این و این نیز به عنوان 3s قابل تشخیص هستند، حتی اگر مقادیر خاص هر پیکسل از تصویری به تصویر دیگر بسیار متفاوت باشد.",
  "input": "I mean, this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next.",
  "model": "google_nmt",
  "time_range": [
   19.7,
   28.32
  ]
 },
 {
  "translatedText": "سلول‌های حساس به نور خاص در چشم شما که با دیدن این 3 شلیک می‌کنند با سلول‌هایی که با دیدن این 3 شلیک می‌شوند بسیار متفاوت هستند.",
  "input": "The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3.",
  "model": "google_nmt",
  "time_range": [
   28.9,
   36.94
  ]
 },
 {
  "translatedText": "اما چیزی در آن قشر بصری دیوانه وار و هوشمند شما، اینها را به عنوان نمایانگر همان ایده تشخیص می دهد، در حالی که در همان زمان تصاویر دیگر را به عنوان ایده های متمایز خود تشخیص می دهد.",
  "input": "But something in that crazy-smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas.",
  "model": "google_nmt",
  "time_range": [
   37.52,
   48.26
  ]
 },
 {
  "translatedText": "اما اگر به شما گفتم، هی، بنشینید و برنامه ای برای من بنویسید که یک شبکه 28x28 پیکسلی مانند این را می گیرد و یک عدد واحد بین 0 تا 10 را خروجی می دهد و به شما می گوید که آن رقم فکر می کند، خوب کار از آنجا می رود. کم اهمیت تا به شدت دشوار.",
  "input": "But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this and outputs a single number between 0 and 10, telling you what it thinks the digit is, well the task goes from comically trivial to dauntingly difficult.",
  "model": "google_nmt",
  "time_range": [
   49.22,
   66.18
  ]
 },
 {
  "translatedText": "من فکر می کنم به سختی نیازی به انگیزه دادن به ارتباط و اهمیت یادگیری ماشین و شبکه های عصبی برای حال و آینده دارم، مگر اینکه زیر سنگ زندگی کرده باشید.",
  "input": "Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future.",
  "model": "google_nmt",
  "time_range": [
   67.16,
   74.64
  ]
 },
 {
  "translatedText": "اما کاری که من می‌خواهم در اینجا انجام دهم این است که به شما نشان دهم که یک شبکه عصبی در واقع چیست، با فرض اینکه هیچ پیش‌زمینه‌ای وجود ندارد، و به تجسم کاری که انجام می‌دهد، نه به عنوان یک کلمه کلیدی، بلکه به عنوان یک تکه ریاضی کمک کنم.",
  "input": "But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing, not as a buzzword but as a piece of math.",
  "model": "google_nmt",
  "time_range": [
   75.12,
   84.46
  ]
 },
 {
  "translatedText": "امید من این است که شما با احساس اینکه ساختار خود انگیزه دارد، بیرون بیایید، و احساس کنید که وقتی می‌خوانید معنی آن را می‌دانید، یا در مورد یادگیری نقل قول-بی نقل قول شبکه عصبی می‌شنوید.",
  "input": "My hope is that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote-unquote learning.",
  "model": "google_nmt",
  "time_range": [
   85.02,
   94.34
  ]
 },
 {
  "translatedText": "این ویدیو فقط به مولفه ساختار آن اختصاص دارد و ویدیوی زیر به یادگیری می پردازد.",
  "input": "This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning.",
  "model": "google_nmt",
  "time_range": [
   95.36,
   100.26
  ]
 },
 {
  "translatedText": "کاری که می‌خواهیم انجام دهیم این است که یک شبکه عصبی را کنار هم قرار دهیم که می‌تواند تشخیص ارقام دست‌نویس را بیاموزد.",
  "input": "What we're going to do is put together a neural network that can learn to recognize handwritten digits.",
  "model": "google_nmt",
  "time_range": [
   100.96,
   106.04
  ]
 },
 {
  "translatedText": "این یک مثال تا حدی کلاسیک برای معرفی موضوع است و من خوشحالم که به وضعیت موجود در اینجا پایبندم، زیرا در پایان دو ویدیو می‌خواهم به چند منبع خوب اشاره کنم که در آن می‌توانید بیشتر بدانید و کجا می توانید کدی را که این کار را انجام می دهد دانلود کنید و با آن در رایانه شخصی خود بازی کنید.",
  "input": "This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quo here, because at the end of the two videos I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer.",
  "model": "google_nmt",
  "time_range": [
   109.36,
   123.08
  ]
 },
 {
  "translatedText": "انواع زیادی از شبکه‌های عصبی وجود دارد، و در سال‌های اخیر به نوعی رونق تحقیقات در مورد این گونه‌ها وجود داشته است، اما در این دو ویدیوی مقدماتی، من و شما فقط می‌خواهیم ساده‌ترین شکل وانیل ساده را بدون هیچ زواید اضافه‌ای بررسی کنیم.",
  "input": "There are many many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos you and I are just going to look at the simplest plain vanilla form with no added frills.",
  "model": "google_nmt",
  "time_range": [
   125.04,
   139.18
  ]
 },
 {
  "translatedText": "این یک نوع پیش نیاز ضروری برای درک هر یک از انواع مدرن قدرتمندتر است، و به من اعتماد کنید هنوز هم پیچیدگی زیادی برای ما دارد که بتوانیم ذهن خود را به اطراف بپیچیم.",
  "input": "This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds around.",
  "model": "google_nmt",
  "time_range": [
   139.86,
   148.6
  ]
 },
 {
  "translatedText": "اما حتی در این ساده‌ترین شکل نیز می‌تواند یاد بگیرد که ارقام دست‌نویس را تشخیص دهد، که برای کامپیوتر کار بسیار جالبی است.",
  "input": "But even in this simplest form it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do.",
  "model": "google_nmt",
  "time_range": [
   149.12,
   156.52
  ]
 },
 {
  "translatedText": "و در عین حال خواهید دید که چگونه از امیدهای زوجی که ممکن است برای آن داشته باشیم، کوتاهی می کند.",
  "input": "And at the same time you'll see how it does fall short of a couple hopes that we might have for it.",
  "model": "google_nmt",
  "time_range": [
   157.48,
   162.28
  ]
 },
 {
  "translatedText": "همانطور که از نام آن پیداست شبکه های عصبی از مغز الهام گرفته شده اند، اما بیایید آن را تجزیه کنیم.",
  "input": "As the name suggests neural networks are inspired by the brain, but let's break that down.",
  "model": "google_nmt",
  "time_range": [
   163.38,
   168.5
  ]
 },
 {
  "translatedText": "نورون ها چه هستند و از چه نظر به هم مرتبط هستند؟",
  "input": "What are the neurons, and in what sense are they linked together?",
  "model": "google_nmt",
  "time_range": [
   168.52,
   171.66
  ]
 },
 {
  "translatedText": "در حال حاضر وقتی می‌گویم نورون تنها چیزی که می‌خواهم به آن فکر کنید چیزی است که عددی را در خود نگه می‌دارد، به‌ویژه عددی بین ۰ و ۱.",
  "input": "Right now when I say neuron all I want you to think about is a thing that holds a number, specifically a number between 0 and 1.",
  "model": "google_nmt",
  "time_range": [
   172.5,
   180.44
  ]
 },
 {
  "translatedText": "واقعا بیشتر از این نیست.",
  "input": "It's really not more than that.",
  "model": "google_nmt",
  "time_range": [
   180.68,
   182.56
  ]
 },
 {
  "translatedText": "به عنوان مثال، شبکه با یک دسته از نورون‌های مربوط به هر یک از پیکسل‌های ۲۸×۲۸ تصویر ورودی شروع می‌شود که در مجموع ۷۸۴ نورون است.",
  "input": "For example the network starts with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784 neurons in total.",
  "model": "google_nmt",
  "time_range": [
   183.78,
   194.22
  ]
 },
 {
  "translatedText": "هر یک از اینها دارای یک عدد است که نشان دهنده مقدار مقیاس خاکستری پیکسل مربوطه است که از 0 برای پیکسل های سیاه تا 1 برای پیکسل های سفید متغیر است.",
  "input": "Each one of these holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels.",
  "model": "google_nmt",
  "time_range": [
   194.7,
   204.38
  ]
 },
 {
  "translatedText": "به این عدد در داخل نورون، فعال سازی آن می گویند، و تصویری که ممکن است در اینجا در ذهن داشته باشید این است که هر نورون زمانی روشن می شود که تعداد فعال آن زیاد باشد.",
  "input": "This number inside the neuron is called its activation, and the image you might have in mind here is that each neuron is lit up when its activation is a high number.",
  "model": "google_nmt",
  "time_range": [
   205.3,
   214.16
  ]
 },
 {
  "translatedText": "بنابراین همه این 784 نورون اولین لایه شبکه ما را تشکیل می دهند.",
  "input": "So all of these 784 neurons make up the first layer of our network.",
  "model": "google_nmt",
  "time_range": [
   216.72,
   221.86
  ]
 },
 {
  "translatedText": "اکنون با پرش به آخرین لایه، این لایه دارای 10 نورون است که هر یک نشان دهنده یکی از ارقام است.",
  "input": "Now jumping over to the last layer, this has 10 neurons, each representing one of the digits.",
  "model": "google_nmt",
  "time_range": [
   226.5,
   231.36
  ]
 },
 {
  "translatedText": "فعال‌سازی در این نورون‌ها، باز هم عددی بین ۰ و ۱، نشان‌دهنده این است که سیستم چقدر فکر می‌کند که یک تصویر داده‌شده با یک رقم معین مطابقت دارد.",
  "input": "The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit.",
  "model": "google_nmt",
  "time_range": [
   232.04,
   242.12
  ]
 },
 {
  "translatedText": "همچنین چند لایه در این بین وجود دارد که لایه‌های پنهان نامیده می‌شوند، که فعلاً باید یک علامت سؤال غول‌پیکر برای این باشد که این فرآیند تشخیص ارقام چگونه انجام می‌شود.",
  "input": "There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark for how on earth this process of recognizing digits is going to be handled.",
  "model": "google_nmt",
  "time_range": [
   243.04,
   253.6
  ]
 },
 {
  "translatedText": "در این شبکه من دو لایه پنهان را انتخاب کردم که هر کدام دارای 16 نورون بودند، و مسلماً این یک انتخاب دلخواه است.",
  "input": "In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice.",
  "model": "google_nmt",
  "time_range": [
   254.26,
   260.56
  ]
 },
 {
  "translatedText": "صادقانه بگویم، من دو لایه را بر اساس اینکه چگونه می‌خواهم ساختار را در یک لحظه ایجاد کنم، و 16 را انتخاب کردم، خوب این فقط یک عدد خوب برای قرار دادن روی صفحه بود.",
  "input": "To be honest I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that was just a nice number to fit on the screen.",
  "model": "google_nmt",
  "time_range": [
   261.02,
   268.2
  ]
 },
 {
  "translatedText": "در عمل فضای زیادی برای آزمایش با یک ساختار خاص در اینجا وجود دارد.",
  "input": "In practice there is a lot of room for experiment with a specific structure here.",
  "model": "google_nmt",
  "time_range": [
   268.78,
   272.34
  ]
 },
 {
  "translatedText": "نحوه عملکرد شبکه، فعال‌سازی در یک لایه، فعال‌سازی لایه بعدی را تعیین می‌کند.",
  "input": "The way the network operates, activations in one layer determine the activations of the next layer.",
  "model": "google_nmt",
  "time_range": [
   273.02,
   278.48
  ]
 },
 {
  "translatedText": "و البته قلب شبکه به عنوان مکانیزم پردازش اطلاعات دقیقاً به این موضوع مربوط می شود که چگونه آن فعال سازی از یک لایه باعث فعال سازی در لایه بعدی می شود.",
  "input": "And of course the heart of the network as an information processing mechanism comes down to exactly how those activations from one layer bring about activations in the next layer.",
  "model": "google_nmt",
  "time_range": [
   279.2,
   288.58
  ]
 },
 {
  "translatedText": "به این معناست که شباهت زیادی به این دارد که چگونه در شبکه‌های بیولوژیکی نورون‌ها، برخی از گروه‌های نورون که شلیک می‌کنند باعث شلیک برخی دیگر می‌شوند.",
  "input": "It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire.",
  "model": "google_nmt",
  "time_range": [
   289.14,
   297.18
  ]
 },
 {
  "translatedText": "اکنون شبکه ای که در اینجا نشان می دهم قبلاً برای تشخیص ارقام آموزش دیده است، و اجازه دهید منظورم را از آن به شما نشان دهم.",
  "input": "Now the network I'm showing here has already been trained to recognize digits, and let me show you what I mean by that.",
  "model": "google_nmt",
  "time_range": [
   298.12,
   303.4
  ]
 },
 {
  "translatedText": "به این معنی که اگر در یک تصویر تغذیه کنید، و تمام 784 نورون لایه ورودی را با توجه به روشنایی هر پیکسل در تصویر روشن کنید، آن الگوی فعال‌سازی باعث ایجاد الگوی بسیار خاصی در لایه بعدی می‌شود که باعث ایجاد الگوی در لایه بعدی می‌شود. آن، که در نهایت مقداری الگو در لایه خروجی می دهد.",
  "input": "It means if you feed in an image, lighting up all 784 neurons of the input layer according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer which causes some pattern in the one after it, which finally gives some pattern in the output layer.",
  "model": "google_nmt",
  "time_range": [
   303.64,
   322.08
  ]
 },
 {
  "translatedText": "و روشن‌ترین نورون آن لایه خروجی، انتخاب شبکه است، به‌طوری‌که بگوییم، این تصویر چه رقمی را نشان می‌دهد.",
  "input": "And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents.",
  "model": "google_nmt",
  "time_range": [
   322.56,
   329.4
  ]
 },
 {
  "translatedText": "و قبل از پرداختن به ریاضیات در مورد اینکه چگونه یک لایه بر لایه بعدی تأثیر می‌گذارد، یا اینکه آموزش چگونه کار می‌کند، اجازه دهید در مورد اینکه چرا حتی منطقی است انتظار داشته باشیم که ساختار لایه‌ای مانند این رفتار هوشمندانه داشته باشد، صحبت کنیم.",
  "input": "And before jumping into the math for how one layer influences the next, or how training works, let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently.",
  "model": "google_nmt",
  "time_range": [
   332.56,
   343.52
  ]
 },
 {
  "translatedText": "اینجا چه انتظاری داریم؟",
  "input": "What are we expecting here?",
  "model": "google_nmt",
  "time_range": [
   344.06,
   345.22
  ]
 },
 {
  "translatedText": "بهترین امید برای آن لایه های میانی چیست؟",
  "input": "What is the best hope for those middle layers?",
  "model": "google_nmt",
  "time_range": [
   345.4,
   347.6
  ]
 },
 {
  "translatedText": "خوب، وقتی من یا شما ارقام را تشخیص می دهیم، اجزای مختلفی را کنار هم می گذاریم.",
  "input": "Well, when you or I recognize digits, we piece together various components.",
  "model": "google_nmt",
  "time_range": [
   348.92,
   353.52
  ]
 },
 {
  "translatedText": "عدد 9 دارای یک حلقه بالا و یک خط در سمت راست است.",
  "input": "A 9 has a loop up top and a line on the right.",
  "model": "google_nmt",
  "time_range": [
   354.2,
   356.82
  ]
 },
 {
  "translatedText": "8 همچنین دارای یک حلقه به بالا است، اما با یک حلقه دیگر به پایین جفت می شود.",
  "input": "An 8 also has a loop up top, but it's paired with another loop down low.",
  "model": "google_nmt",
  "time_range": [
   357.38,
   361.18
  ]
 },
 {
  "translatedText": "یک 4 اساساً به سه خط خاص و مواردی از این دست تقسیم می شود.",
  "input": "A 4 basically breaks down into three specific lines, and things like that.",
  "model": "google_nmt",
  "time_range": [
   361.98,
   366.82
  ]
 },
 {
  "translatedText": "اکنون در یک دنیای کامل، ممکن است امیدوار باشیم که هر نورون در لایه دوم تا آخرین با یکی از این اجزای فرعی مطابقت داشته باشد، که هر زمان که در یک تصویر با مثلاً یک حلقه بالا، مانند 9 یا 8 تغذیه می‌کنید، مقداری وجود دارد. نورون خاصی که فعال شدن آن نزدیک به 1 خواهد بود.",
  "input": "Now in a perfect world, we might hope that each neuron in the second to last layer corresponds with one of these subcomponents, that anytime you feed in an image with, say, a loop up top, like a 9 or an 8, there's some specific neuron whose activation is going to be close to 1.",
  "model": "google_nmt",
  "time_range": [
   367.6,
   383.78
  ]
 },
 {
  "translatedText": "و منظور من این حلقه خاص از پیکسل ها نیست، امید این است که هر الگوی به طور کلی حلقه ای به سمت بالا، این نورون را تنظیم کند.",
  "input": "And I don't mean this specific loop of pixels, the hope would be that any generally loopy pattern towards the top sets off this neuron.",
  "model": "google_nmt",
  "time_range": [
   384.5,
   391.56
  ]
 },
 {
  "translatedText": "به این ترتیب، رفتن از لایه سوم به لایه آخر فقط مستلزم یادگیری ترکیبی از اجزای فرعی با کدام رقم است.",
  "input": "That way, going from the third layer to the last one just requires learning which combination of subcomponents corresponds to which digits.",
  "model": "google_nmt",
  "time_range": [
   392.44,
   400.04
  ]
 },
 {
  "translatedText": "البته، این فقط مشکل را به پایان می‌رساند، زیرا چگونه می‌توانید این اجزای فرعی را تشخیص دهید، یا حتی یاد بگیرید که اجزای فرعی مناسب باید چه باشند؟",
  "input": "Of course, that just kicks the problem down the road, because how would you recognize these subcomponents, or even learn what the right subcomponents should be?",
  "model": "google_nmt",
  "time_range": [
   401.0,
   407.64
  ]
 },
 {
  "translatedText": "و من هنوز حتی در مورد اینکه چگونه یک لایه بر لایه بعدی تأثیر می گذارد صحبت نکرده ام، اما برای یک لحظه با من روی این یکی بدوید.",
  "input": "And I still haven't even talked about how one layer influences the next, but run with me on this one for a moment.",
  "model": "google_nmt",
  "time_range": [
   408.06,
   413.06
  ]
 },
 {
  "translatedText": "تشخیص یک حلقه همچنین می تواند به مشکلات فرعی تقسیم شود.",
  "input": "Recognizing a loop can also break down into subproblems.",
  "model": "google_nmt",
  "time_range": [
   413.68,
   416.68
  ]
 },
 {
  "translatedText": "یک راه معقول برای انجام این کار این است که ابتدا لبه های کوچک مختلفی را که آن را تشکیل می دهند شناسایی کنید.",
  "input": "One reasonable way to do this would be to first recognize the various little edges that make it up.",
  "model": "google_nmt",
  "time_range": [
   417.28,
   422.78
  ]
 },
 {
  "translatedText": "به طور مشابه، یک خط بلند، مانند آن چیزی که ممکن است در ارقام 1 یا 4 یا 7 ببینید، در واقع فقط یک لبه بلند است، یا شاید شما آن را به عنوان الگوی خاصی از چندین لبه کوچکتر در نظر بگیرید.",
  "input": "Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7, is really just a long edge, or maybe you think of it as a certain pattern of several smaller edges.",
  "model": "google_nmt",
  "time_range": [
   423.78,
   434.32
  ]
 },
 {
  "translatedText": "بنابراین شاید امید ما این باشد که هر نورون در لایه دوم شبکه با لبه های کوچک مرتبط مختلف مطابقت داشته باشد.",
  "input": "So maybe our hope is that each neuron in the second layer of the network corresponds with the various relevant little edges.",
  "model": "google_nmt",
  "time_range": [
   435.14,
   442.72
  ]
 },
 {
  "translatedText": "شاید وقتی تصویری مانند این می آید، تمام نورون های مرتبط با حدود 8 تا 10 لبه کوچک خاص را روشن می کند، که به نوبه خود نورون های مرتبط با حلقه بالایی و یک خط عمودی طولانی را روشن می کند و آن ها نور را روشن می کنند. نورون مرتبط با 9.",
  "input": "Maybe when an image like this one comes in, it lights up all of the neurons associated with around 8 to 10 specific little edges, which in turn lights up the neurons associated with the upper loop and a long vertical line, and those light up the neuron associated with a 9.",
  "model": "google_nmt",
  "time_range": [
   443.54,
   459.72
  ]
 },
 {
  "translatedText": "اینکه آیا این همان کاری است که شبکه نهایی ما واقعاً انجام می دهد یا خیر، سؤال دیگری است که وقتی ببینیم چگونه شبکه را آموزش دهیم به آن باز خواهم گشت، اما این امیدی است که ممکن است داشته باشیم، نوعی هدف با ساختار لایه ای. مثل این.",
  "input": "Whether or not this is what our final network actually does is another question, one that I'll come back to once we see how to train the network, but this is a hope that we might have, a sort of goal with the layered structure like this.",
  "model": "google_nmt",
  "time_range": [
   460.68,
   472.54
  ]
 },
 {
  "translatedText": "علاوه بر این، می‌توانید تصور کنید که چگونه قادر به تشخیص لبه‌ها و الگوهای این چنینی برای سایر کارهای تشخیص تصویر واقعاً مفید است.",
  "input": "Moreover, you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks.",
  "model": "google_nmt",
  "time_range": [
   473.16,
   480.3
  ]
 },
 {
  "translatedText": "و حتی فراتر از تشخیص تصویر، انواع کارهای هوشمندانه ای وجود دارد که ممکن است بخواهید انجام دهید که به لایه های انتزاعی تقسیم می شوند.",
  "input": "And even beyond image recognition, there are all sorts of intelligent things you might want to do that break down into layers of abstraction.",
  "model": "google_nmt",
  "time_range": [
   480.88,
   487.28
  ]
 },
 {
  "translatedText": "به عنوان مثال، تجزیه گفتار شامل گرفتن صوت خام و انتخاب صداهای متمایز است که با هم ترکیب می شوند و هجاهای خاصی را می سازند، که ترکیب می شوند و کلمات را می سازند، که ترکیب می شوند تا عبارات و افکار انتزاعی تر و غیره را بسازند.",
  "input": "Parsing speech, for example, involves taking raw audio and picking out distinct sounds, which combine to make certain syllables, which combine to form words, which combine to make up phrases and more abstract thoughts, etc.",
  "model": "google_nmt",
  "time_range": [
   488.04,
   500.06
  ]
 },
 {
  "translatedText": "اما برای بازگشت به نحوه عملکرد هر یک از اینها، خود را در حال طراحی تصور کنید که دقیقاً چگونه فعال سازی در یک لایه ممکن است لایه بعدی را تعیین کند.",
  "input": "But getting back to how any of this actually works, picture yourself right now designing how exactly the activations in one layer might determine the next.",
  "model": "google_nmt",
  "time_range": [
   501.1,
   509.92
  ]
 },
 {
  "translatedText": "هدف این است که مکانیزمی داشته باشیم که بتواند پیکسل ها را به لبه ها یا لبه ها را به الگوها یا الگوها را به ارقام ترکیب کند.",
  "input": "The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits.",
  "model": "google_nmt",
  "time_range": [
   510.86,
   518.98
  ]
 },
 {
  "translatedText": "و برای بزرگنمایی روی یک مثال بسیار خاص، بیایید بگوییم امیدواریم که یک نورون خاص در لایه دوم تشخیص دهد که آیا تصویر در این ناحیه لبه دارد یا خیر.",
  "input": "And to zoom in on one very specific example, let's say the hope is for one particular neuron in the second layer to pick up on whether or not the image has an edge in this region here.",
  "model": "google_nmt",
  "time_range": [
   519.44,
   530.62
  ]
 },
 {
  "translatedText": "سوالی که مطرح است این است که شبکه چه پارامترهایی باید داشته باشد؟",
  "input": "The question at hand is what parameters should the network have?",
  "model": "google_nmt",
  "time_range": [
   531.44,
   535.1
  ]
 },
 {
  "translatedText": "چه صفحه‌ها و دستگیره‌هایی را باید به گونه‌ای تنظیم کنید که به اندازه کافی رسا باشد که به طور بالقوه این الگو، یا هر الگوی پیکسلی دیگر، یا الگوی که چندین لبه می‌تواند یک حلقه ایجاد کند، و موارد دیگر از این قبیل را به تصویر بکشد؟",
  "input": "What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern, or any other pixel pattern, or the pattern that several edges can make a loop, and other such things?",
  "model": "google_nmt",
  "time_range": [
   535.64,
   547.78
  ]
 },
 {
  "translatedText": "خوب، کاری که ما انجام خواهیم داد این است که به هر یک از اتصالات بین نورون ما و نورون های لایه اول یک وزن اختصاص دهیم.",
  "input": "Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer.",
  "model": "google_nmt",
  "time_range": [
   548.72,
   555.56
  ]
 },
 {
  "translatedText": "این وزن ها فقط اعداد هستند.",
  "input": "These weights are just numbers.",
  "model": "google_nmt",
  "time_range": [
   556.32,
   557.7
  ]
 },
 {
  "translatedText": "سپس تمام آن فعال‌سازی‌ها را از لایه اول بگیرید و مجموع وزنی آنها را با توجه به این وزن‌ها محاسبه کنید.",
  "input": "Then take all of those activations from the first layer and compute their weighted sum according to these weights.",
  "model": "google_nmt",
  "time_range": [
   558.54,
   565.5
  ]
 },
 {
  "translatedText": "من فکر می کنم که این وزن ها به عنوان یک شبکه کوچک سازماندهی شده اند مفید است، و من از پیکسل های سبز برای نشان دادن وزن های مثبت و از پیکسل های قرمز برای نشان دادن وزن های منفی استفاده می کنم، جایی که روشنایی آن پیکسل مقداری است. تصویری آزاد از ارزش وزن",
  "input": "I find it helpful to think of these weights as being organized into a little grid of their own, and I'm going to use green pixels to indicate positive weights, and red pixels to indicate negative weights, where the brightness of that pixel is some loose depiction of the weight's value.",
  "model": "google_nmt",
  "time_range": [
   567.7,
   581.78
  ]
 },
 {
  "translatedText": "حال اگر وزن‌های مرتبط با تقریباً همه پیکسل‌ها را صفر کنیم، به جز برخی از وزن‌های مثبت در این ناحیه که به آن‌ها اهمیت می‌دهیم، پس گرفتن مجموع وزنی تمام مقادیر پیکسل واقعاً به معنای جمع کردن مقادیر پیکسل است. منطقه ای که ما به آن اهمیت می دهیم",
  "input": "Now if we made the weights associated with almost all of the pixels zero except for some positive weights in this region that we care about, then taking the weighted sum of all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about.",
  "model": "google_nmt",
  "time_range": [
   582.78,
   597.82
  ]
 },
 {
  "translatedText": "و اگر واقعاً می‌خواهید بفهمید که آیا لبه‌ای در اینجا وجود دارد یا خیر، کاری که ممکن است انجام دهید این است که وزن‌های منفی مرتبط با پیکسل‌های اطراف داشته باشید.",
  "input": "And if you really wanted to pick up on whether there's an edge here, what you might do is have some negative weights associated with the surrounding pixels.",
  "model": "google_nmt",
  "time_range": [
   599.14,
   606.6
  ]
 },
 {
  "translatedText": "سپس مجموع زمانی که آن پیکسل‌های میانی روشن هستند اما پیکسل‌های اطراف تیره‌تر هستند، بزرگ‌تر است.",
  "input": "Then the sum is largest when those middle pixels are bright but the surrounding pixels are darker.",
  "model": "google_nmt",
  "time_range": [
   607.48,
   612.7
  ]
 },
 {
  "translatedText": "وقتی یک مجموع وزنی مانند این را محاسبه می کنید، ممکن است با هر عددی بیرون بیایید، اما برای این شبکه چیزی که ما می خواهیم این است که مقدار فعال سازی ها بین 0 و 1 باشد.",
  "input": "When you compute a weighted sum like this, you might come out with any number, but for this network what we want is for activations to be some value between 0 and 1.",
  "model": "google_nmt",
  "time_range": [
   614.26,
   623.54
  ]
 },
 {
  "translatedText": "بنابراین یک کار معمول این است که این مجموع وزنی را به تابعی پمپ کنیم که خط اعداد واقعی را در محدوده بین 0 و 1 قرار می دهد.",
  "input": "So a common thing to do is to pump this weighted sum into some function that squishes the real number line into the range between 0 and 1.",
  "model": "google_nmt",
  "time_range": [
   624.12,
   632.14
  ]
 },
 {
  "translatedText": "و یک تابع رایج که این کار را انجام می دهد تابع سیگموئید نامیده می شود که به عنوان منحنی لجستیک نیز شناخته می شود.",
  "input": "And a common function that does this is called the sigmoid function, also known as a logistic curve.",
  "model": "google_nmt",
  "time_range": [
   632.46,
   637.42
  ]
 },
 {
  "translatedText": "اساساً ورودی های بسیار منفی نزدیک به 0، ورودی های مثبت به 1 نزدیک می شوند و به طور پیوسته در اطراف ورودی 0 افزایش می یابد.",
  "input": "Basically very negative inputs end up close to 0, positive inputs end up close to 1, and it just steadily increases around the input 0.",
  "model": "google_nmt",
  "time_range": [
   638.0,
   646.6
  ]
 },
 {
  "translatedText": "بنابراین فعال شدن نورون در اینجا اساساً معیاری برای مثبت بودن مجموع وزنی مربوطه است.",
  "input": "So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is.",
  "model": "google_nmt",
  "time_range": [
   649.12,
   656.36
  ]
 },
 {
  "translatedText": "اما شاید اینطور نباشد که بخواهید نورون زمانی که مجموع وزنی بزرگتر از 0 است روشن شود.",
  "input": "But maybe it's not that you want the neuron to light up when the weighted sum is bigger than 0.",
  "model": "google_nmt",
  "time_range": [
   657.54,
   661.88
  ]
 },
 {
  "translatedText": "شاید بخواهید فقط زمانی فعال باشد که مجموع آن بزرگتر از مثلاً 10 باشد.",
  "input": "Maybe you only want it to be active when the sum is bigger than say 10.",
  "model": "google_nmt",
  "time_range": [
   662.28,
   666.36
  ]
 },
 {
  "translatedText": "یعنی شما می خواهید کمی سوگیری برای غیر فعال بودن آن داشته باشید.",
  "input": "That is, you want some bias for it to be inactive.",
  "model": "google_nmt",
  "time_range": [
   666.84,
   670.26
  ]
 },
 {
  "translatedText": "کاری که ما انجام خواهیم داد این است که فقط یک عدد دیگر مانند منفی 10 را به این مجموع وزنی اضافه کنیم قبل از اینکه آن را از طریق تابع کوبیدن سیگموئید وصل کنیم.",
  "input": "What we'll do then is just add in some other number like negative 10 to this weighted sum before plugging it through the sigmoid squishification function.",
  "model": "google_nmt",
  "time_range": [
   671.38,
   679.66
  ]
 },
 {
  "translatedText": "به آن عدد اضافی سوگیری می گویند.",
  "input": "That additional number is called the bias.",
  "model": "google_nmt",
  "time_range": [
   680.58,
   682.44
  ]
 },
 {
  "translatedText": "بنابراین وزن‌ها به شما می‌گویند که این نورون در لایه دوم چه الگوی پیکسلی را انتخاب می‌کند، و سوگیری به شما می‌گوید که قبل از اینکه نورون شروع به فعال شدن معنی‌دار کند، مجموع وزنی چقدر باید باشد.",
  "input": "So the weights tell you what pixel pattern this neuron in the second layer is picking up on, and the bias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active.",
  "model": "google_nmt",
  "time_range": [
   683.46,
   695.18
  ]
 },
 {
  "translatedText": "و این فقط یک نورون است.",
  "input": "And that is just one neuron.",
  "model": "google_nmt",
  "time_range": [
   696.12,
   697.68
  ]
 },
 {
  "translatedText": "هر نورون دیگر در این لایه قرار است به تمام نورون های 784 پیکسلی از لایه اول متصل شود و هر یک از آن 784 اتصال وزن خاص خود را دارد.",
  "input": "Every other neuron in this layer is going to be connected to all 784 pixel neurons from the first layer, and each one of those 784 connections has its own weight associated with it.",
  "model": "google_nmt",
  "time_range": [
   698.28,
   710.94
  ]
 },
 {
  "translatedText": "همچنین، هر یک مقداری سوگیری دارد، تعدادی عدد دیگر که قبل از له کردن آن با سیگموئید به مجموع وزنی اضافه می‌کنید.",
  "input": "Also, each one has some bias, some other number that you add on to the weighted sum before squishing it with the sigmoid.",
  "model": "google_nmt",
  "time_range": [
   711.6,
   717.6
  ]
 },
 {
  "translatedText": "و این جای تامل زیادی دارد!",
  "input": "And that's a lot to think about!",
  "model": "google_nmt",
  "time_range": [
   718.11,
   719.54
  ]
 },
 {
  "translatedText": "با این لایه پنهان از 16 نورون، در مجموع 784 ضربدر 16 وزن، همراه با 16 سوگیری است.",
  "input": "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights, along with 16 biases.",
  "model": "google_nmt",
  "time_range": [
   719.96,
   727.98
  ]
 },
 {
  "translatedText": "و همه اینها فقط اتصالات از لایه اول به لایه دوم است.",
  "input": "And all of that is just the connections from the first layer to the second.",
  "model": "google_nmt",
  "time_range": [
   728.84,
   731.94
  ]
 },
 {
  "translatedText": "اتصالات بین لایه های دیگر نیز دارای دسته ای از وزن ها و سوگیری های مرتبط با آنها است.",
  "input": "The connections between the other layers also have a bunch of weights and biases associated with them.",
  "model": "google_nmt",
  "time_range": [
   732.52,
   737.34
  ]
 },
 {
  "translatedText": "همه گفته‌ها و انجام‌ها، این شبکه تقریباً دقیقاً 13000 وزن و بایاس کل دارد.",
  "input": "All said and done, this network has almost exactly 13,000 total weights and biases.",
  "model": "google_nmt",
  "time_range": [
   738.34,
   743.8
  ]
 },
 {
  "translatedText": "13000 دستگیره و شماره گیری که می توان آنها را تغییر داد و چرخاند تا این شبکه به روش های مختلف رفتار کند.",
  "input": "13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways.",
  "model": "google_nmt",
  "time_range": [
   743.8,
   749.96
  ]
 },
 {
  "translatedText": "بنابراین وقتی در مورد یادگیری صحبت می کنیم، منظور این است که کامپیوتر را برای یافتن یک تنظیم معتبر برای همه این اعداد بسیار زیاد به طوری که در واقع مشکل موجود را حل کند.",
  "input": "So when we talk about learning, what that's referring to is getting the computer to find a valid setting for all of these many many numbers so that it'll actually solve the problem at hand.",
  "model": "google_nmt",
  "time_range": [
   751.04,
   761.36
  ]
 },
 {
  "translatedText": "یک آزمایش فکری که در عین حال سرگرم کننده و وحشتناک است این است که تصور کنید بنشینید و تمام این وزن ها و سوگیری ها را با دست تنظیم کنید، به طور هدفمند اعداد را تغییر دهید تا لایه دوم لبه ها را بگیرد، لایه سوم روی الگوها بنشیند. و غیره.",
  "input": "One thought experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand, purposefully tweaking the numbers so that the second layer picks up on edges, the third layer picks up on patterns, etc.",
  "model": "google_nmt",
  "time_range": [
   762.62,
   776.58
  ]
 },
 {
  "translatedText": "من شخصاً این را رضایت‌بخش می‌دانم تا اینکه شبکه را فقط به عنوان یک جعبه سیاه کامل نگاه کنم، زیرا وقتی شبکه آنطور که شما پیش‌بینی می‌کنید عمل نمی‌کند، اگر کمی با معنای واقعی آن وزن‌ها و سوگیری‌ها ارتباط برقرار کرده باشید. ، شما یک مکان شروع برای آزمایش نحوه تغییر ساختار برای بهبود دارید.",
  "input": "I personally find this satisfying rather than just treating the network as a total black box, because when the network doesn't perform the way you anticipate, if you've built up a little bit of a relationship with what those weights and biases actually mean, you have a starting place for experimenting with how to change the structure to improve.",
  "model": "google_nmt",
  "time_range": [
   776.98,
   794.18
  ]
 },
 {
  "translatedText": "یا زمانی که شبکه کار می کند، اما نه به دلایلی که ممکن است انتظارش را داشته باشید، کاوش در آنچه که وزن ها و سوگیری ها انجام می دهند، راه خوبی برای به چالش کشیدن مفروضات شما و افشای فضای کامل راه حل های ممکن است.",
  "input": "Or when the network does work but not for the reasons you might expect, digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible solutions.",
  "model": "google_nmt",
  "time_range": [
   794.96,
   805.82
  ]
 },
 {
  "translatedText": "به هر حال، نوشتن عملکرد واقعی در اینجا کمی دست و پا گیر است، فکر نمی کنید؟",
  "input": "By the way, the actual function here is a little cumbersome to write down, don't you think?",
  "model": "google_nmt",
  "time_range": [
   806.84,
   810.68
  ]
 },
 {
  "translatedText": "بنابراین اجازه دهید به شما روشی فشرده تر نشان دهم که این اتصالات نشان داده می شوند.",
  "input": "So let me show you a more notationally compact way that these connections are represented.",
  "model": "google_nmt",
  "time_range": [
   812.5,
   817.14
  ]
 },
 {
  "translatedText": "اگر بخواهید در مورد شبکه های عصبی بیشتر بخوانید، اینگونه خواهید دید.",
  "input": "This is how you'd see it if you choose to read up more about neural networks.",
  "model": "google_nmt",
  "time_range": [
   817.66,
   820.52
  ]
 },
 {
  "translatedText": "همه فعال‌سازی‌ها را از یک لایه در یک ستون سازماندهی کنید، زیرا ماتریس مربوط به اتصالات بین یک لایه و یک نورون خاص در لایه بعدی است.",
  "input": "Organize all of the activations from one layer into a column as a matrix corresponds to the connections between one layer and a particular neuron in the next layer.",
  "model": "google_nmt",
  "time_range": [
   821.38,
   838.0
  ]
 },
 {
  "translatedText": "منظور این است که گرفتن مجموع وزنی فعال‌سازی‌ها در لایه اول با توجه به این وزن‌ها، با یکی از عبارت‌های حاصلضرب بردار ماتریس هر چیزی که در سمت چپ اینجا داریم، مطابقت دارد.",
  "input": "What that means is that taking the weighted sum of the activations in the first layer according to these weights corresponds to one of the terms in the matrix vector product of everything we have on the left here.",
  "model": "google_nmt",
  "time_range": [
   838.54,
   849.88
  ]
 },
 {
  "translatedText": "به هر حال، بسیاری از یادگیری ماشین فقط به درک خوب جبر خطی بستگی دارد، بنابراین برای هر یک از شما که می‌خواهید درک بصری خوبی برای ماتریس‌ها و معنی ضرب بردار ماتریس داشته باشید، به سری‌هایی که من در آن انجام دادم نگاهی بیندازید. جبر خطی، به ویژه فصل 3.",
  "input": "By the way, so much of machine learning just comes down to having a good grasp of linear algebra, so for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means, take a look at the series I did on linear algebra, especially chapter 3.",
  "model": "google_nmt",
  "time_range": [
   854.0,
   868.6
  ]
 },
 {
  "translatedText": "به بیان خودمان برگردیم، به جای اینکه در مورد اضافه کردن بایاس به هر یک از این مقادیر به طور مستقل صحبت کنیم، آن را با سازماندهی تمام آن بایاس ها در یک بردار، و افزودن کل بردار به محصول بردار ماتریس قبلی، نشان می دهیم.",
  "input": "Back to our expression, instead of talking about adding the bias to each one of these values independently, we represent it by organizing all those biases into a vector, and adding the entire vector to the previous matrix vector product.",
  "model": "google_nmt",
  "time_range": [
   869.24,
   882.3
  ]
 },
 {
  "translatedText": "سپس به عنوان آخرین مرحله، من یک سیگموئید را در اطراف بیرون می‌پیچم، و چیزی که قرار است نشان‌دهنده آن باشد این است که شما می‌خواهید تابع sigmoid را برای هر جزء خاص از بردار حاصل در داخل اعمال کنید.",
  "input": "Then as a final step, I'll wrap a sigmoid around the outside here, and what that's supposed to represent is that you're going to apply the sigmoid function to each specific component of the resulting vector inside.",
  "model": "google_nmt",
  "time_range": [
   883.28,
   894.74
  ]
 },
 {
  "translatedText": "بنابراین هنگامی که این ماتریس وزن و این بردارها را به عنوان نمادهای خود یادداشت کردید، می‌توانید انتقال کامل فعال‌سازی‌ها را از یک لایه به لایه بعدی در یک بیان کوچک بسیار فشرده و منظم انتقال دهید، و این باعث می‌شود کد مربوطه هم بسیار ساده‌تر و هم ساده‌تر شود. بسیار سریعتر، زیرا بسیاری از کتابخانه ها ضرب ماتریس را بهینه می کنند.",
  "input": "So once you write down this weight matrix and these vectors as their own symbols, you can communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression, and this makes the relevant code both a lot simpler and a lot faster, since many libraries optimize the heck out of matrix multiplication.",
  "model": "google_nmt",
  "time_range": [
   895.94,
   915.66
  ]
 },
 {
  "translatedText": "به خاطر دارید که قبلاً گفتم این نورون ها چیزهایی هستند که اعداد را نگه می دارند؟",
  "input": "Remember how earlier I said these neurons are simply things that hold numbers?",
  "model": "google_nmt",
  "time_range": [
   917.82,
   921.46
  ]
 },
 {
  "translatedText": "البته اعداد خاصی که نگه می‌دارند بستگی به تصویری دارد که شما از آن تغذیه می‌کنید، بنابراین در واقع دقیق‌تر است که هر نورون را به عنوان یک تابع در نظر بگیرید، تابعی که خروجی‌های تمام نورون‌های لایه قبلی را می‌گیرد و عددی را بیرون می‌ریزد. بین 0 و 1",
  "input": "Well of course the specific numbers that they hold depends on the image you feed in, so it's actually more accurate to think of each neuron as a function, one that takes in the outputs of all the neurons in the previous layer and spits out a number between 0 and 1.",
  "model": "google_nmt",
  "time_range": [
   922.22,
   938.34
  ]
 },
 {
  "translatedText": "در واقع کل شبکه فقط یک تابع است، تابعی که 784 عدد را به عنوان ورودی می گیرد و 10 عدد را به عنوان خروجی می ریزد.",
  "input": "Really the entire network is just a function, one that takes in 784 numbers as an input and spits out 10 numbers as an output.",
  "model": "google_nmt",
  "time_range": [
   939.2,
   947.06
  ]
 },
 {
  "translatedText": "این یک تابع پیچیده است، تابعی که شامل 13000 پارامتر به شکل این وزن‌ها و سوگیری‌هایی است که الگوهای خاصی را نشان می‌دهند، و شامل تکرار بسیاری از محصولات بردار ماتریس و تابع انقباض سیگموئید است، اما با این وجود فقط یک تابع است و در یک این به نوعی اطمینان بخش است که پیچیده به نظر می رسد.",
  "input": "It's an absurdly complicated function, one that involves 13,000 parameters in the forms of these weights and biases that pick up on certain patterns, and which involves iterating many matrix vector products and the sigmoid squishification function, but it's just a function nonetheless, and in a way it's kind of reassuring that it looks complicated.",
  "model": "google_nmt",
  "time_range": [
   947.56,
   966.66
  ]
 },
 {
  "translatedText": "منظورم این است که اگر ساده‌تر بود، چه امیدی داشتیم که بتواند چالش تشخیص ارقام را انجام دهد؟",
  "input": "I mean if it were any simpler, what hope would we have that it could take on the challenge of recognizing digits?",
  "model": "google_nmt",
  "time_range": [
   967.34,
   972.28
  ]
 },
 {
  "translatedText": "و چگونه آن چالش را انجام می دهد؟",
  "input": "And how does it take on that challenge?",
  "model": "google_nmt",
  "time_range": [
   973.34,
   974.7
  ]
 },
 {
  "translatedText": "چگونه این شبکه تنها با مشاهده داده ها، وزن ها و سوگیری های مناسب را یاد می گیرد؟",
  "input": "How does this network learn the appropriate weights and biases just by looking at data?",
  "model": "google_nmt",
  "time_range": [
   975.08,
   979.36
  ]
 },
 {
  "translatedText": "خب این چیزی است که در ویدیوی بعدی نشان خواهم داد، و همچنین کمی بیشتر در مورد آنچه که این شبکه خاصی که می بینیم واقعاً انجام می دهد، کاوش خواهم کرد.",
  "input": "Well that's what I'll show in the next video, and I'll also dig a little more into what this particular network we're seeing is really doing.",
  "model": "google_nmt",
  "time_range": [
   980.14,
   986.12
  ]
 },
 {
  "translatedText": "حالا فکر می‌کنم باید بگویم مشترک شوید تا از زمانی که ویدیو یا هر ویدیوی جدیدی منتشر می‌شود مطلع شوید، اما در واقع اکثر شما واقعاً اعلان‌هایی را از YouTube دریافت نمی‌کنید، درست است؟",
  "input": "Now is the point I suppose I should say subscribe to stay notified about when video or any new videos come out, but realistically most of you don't actually receive notifications from YouTube, do you?",
  "model": "google_nmt",
  "time_range": [
   987.58,
   997.42
  ]
 },
 {
  "translatedText": "شاید صادقانه‌تر بگویم مشترک شوید تا شبکه‌های عصبی که زیربنای الگوریتم توصیه‌های YouTube هستند این باور را داشته باشند که می‌خواهید محتوای این کانال را ببینید به شما توصیه می‌شود.",
  "input": "Maybe more honestly I should say subscribe so that the neural networks that underlie YouTube's recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you.",
  "model": "google_nmt",
  "time_range": [
   998.02,
   1007.88
  ]
 },
 {
  "translatedText": "به هر حال برای اطلاعات بیشتر در جریان باشید.",
  "input": "Anyway stay posted for more.",
  "model": "google_nmt",
  "time_range": [
   1008.56,
   1009.94
  ]
 },
 {
  "translatedText": "از همه کسانی که از این ویدیوها در Patreon حمایت می کنند بسیار سپاسگزاریم.",
  "input": "Thank you very much to everyone supporting these videos on Patreon.",
  "model": "google_nmt",
  "time_range": [
   1010.76,
   1013.5
  ]
 },
 {
  "translatedText": "من در تابستان امسال برای پیشرفت در سری احتمالات کمی کند بوده ام، اما بعد از این پروژه دوباره وارد آن می شوم، بنابراین کاربران می توانند منتظر به روز رسانی ها باشند.",
  "input": "I've been a little slow to progress in the probability series this summer, but I'm jumping back into it after this project, so patrons you can look out for updates there.",
  "model": "google_nmt",
  "time_range": [
   1014.0,
   1021.9
  ]
 },
 {
  "translatedText": "برای پایان دادن به این موضوع، لیشا لی را با خود دارم که کار دکترای خود را در جنبه نظری یادگیری عمیق انجام داد و در حال حاضر در یک شرکت سرمایه گذاری خطرپذیر به نام Amplify Partners کار می کند که با مهربانی مقداری از بودجه را برای این ویدیو فراهم کرد.",
  "input": "To close things off here I have with me Leisha Lee who did her PhD work on the theoretical side of deep learning and who currently works at a venture capital firm called Amplify Partners who kindly provided some of the funding for this video.",
  "model": "google_nmt",
  "time_range": [
   1023.6,
   1034.62
  ]
 },
 {
  "translatedText": "بنابراین لیشا یک چیزی که فکر می کنم باید سریعاً مطرح کنیم این تابع سیگموئید است.",
  "input": "So Leisha one thing I think we should quickly bring up is this sigmoid function.",
  "model": "google_nmt",
  "time_range": [
   1035.46,
   1039.12
  ]
 },
 {
  "translatedText": "همانطور که می‌دانم شبکه‌های اولیه از این استفاده می‌کنند تا مجموع وزنی مربوطه را در فاصله بین صفر و یک قرار دهند، شما می‌دانید که به نوعی انگیزه این تشبیه بیولوژیکی نورون‌ها، غیرفعال یا فعال است.",
  "input": "As I understand it early networks use this to squish the relevant weighted sum into that interval between zero and one, you know kind of motivated by this biological analogy of neurons either being inactive or active.",
  "model": "google_nmt",
  "time_range": [
   1039.7,
   1049.84
  ]
 },
 {
  "translatedText": "دقیقا.",
  "input": "Exactly.",
  "model": "google_nmt",
  "time_range": [
   1050.28,
   1050.3
  ]
 },
 {
  "translatedText": "اما تعداد نسبتا کمی از شبکه های مدرن دیگر از سیگموئید استفاده می کنند.",
  "input": "But relatively few modern networks actually use sigmoid anymore.",
  "model": "google_nmt",
  "time_range": [
   1050.56,
   1054.04
  ]
 },
 {
  "translatedText": "آره",
  "input": "Yeah.",
  "model": "google_nmt",
  "time_range": [
   1054.32,
   1054.32
  ]
 },
 {
  "translatedText": "این یک نوع مدرسه قدیمی است درست است؟",
  "input": "It's kind of old school right?",
  "model": "google_nmt",
  "time_range": [
   1054.44,
   1055.54
  ]
 },
 {
  "translatedText": "بله یا بهتر بگوییم relu به نظر می رسد بسیار آسان تر برای آموزش.",
  "input": "Yeah or rather relu seems to be much easier to train.",
  "model": "google_nmt",
  "time_range": [
   1055.76,
   1058.98
  ]
 },
 {
  "translatedText": "و relu مخفف واحد خطی اصلاح شده است؟",
  "input": "And relu stands for rectified linear unit?",
  "model": "google_nmt",
  "time_range": [
   1059.4,
   1062.34
  ]
 },
 {
  "translatedText": "بله، این نوع تابعی است که در آن شما فقط حداکثر صفر را می گیرید و a را با چیزی که در ویدیو توضیح می دهید به دست می آورید و این به نوعی انگیزه آن چیست، فکر می کنم تا حدی با یک قیاس بیولوژیکی با نورون ها بود. یا فعال می شود یا نه و بنابراین اگر از آستانه خاصی عبور کند، تابع هویت خواهد بود، اما اگر فعال نمی شد، فعال نمی شد، بنابراین صفر می شد، بنابراین یک نوع ساده سازی است.",
  "input": "Yes it's this kind of function where you're just taking a max of zero and a where a is given by what you were explaining in the video and what this was sort of motivated from I think was a partially by a biological analogy with how neurons would either be activated or not and so if it passes a certain threshold it would be the identity function but if it did not then it would just not be activated so it'd be zero so it's kind of a simplification.",
  "model": "google_nmt",
  "time_range": [
   1062.68,
   1090.84
  ]
 },
 {
  "translatedText": "استفاده از سیگموئیدها کمکی به آموزش نکرد یا آموزش در برخی موارد بسیار دشوار بود و مردم فقط relu را امتحان کردند و اتفاقاً برای این شبکه‌های عصبی فوق‌العاده عمیق بسیار خوب عمل کرد.",
  "input": "Using sigmoids didn't help training or it was very difficult to train at some point and people just tried relu and it happened to work very well for these incredibly deep neural networks.",
  "model": "google_nmt",
  "time_range": [
   1091.16,
   1104.62
  ]
 },
 {
  "translatedText": "باشه ممنون آلیشیا",
  "input": "All right thank you Alicia.",
  "model": "google_nmt",
  "time_range": [
   1105.1,
   1105.64
  ]
 }
]