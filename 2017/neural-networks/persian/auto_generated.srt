1
00:00:04,220 --> 00:00:05,400
این یک 3 است.

2
00:00:06,060 --> 00:00:09,890
این به صورت شلخته نوشته شده و با وضوح بسیار پایین 28x28 پیکسل 

3
00:00:09,890 --> 00:00:13,720
ارائه شده است، اما مغز شما در تشخیص آن به عنوان 3 مشکلی ندارد.

4
00:00:14,340 --> 00:00:16,632
و من از شما می خواهم که لحظه ای وقت بگذارید و قدردانی کنید که چقدر 

5
00:00:16,632 --> 00:00:18,960
دیوانه کننده است که مغزها می توانند این کار را بدون زحمت انجام دهند.

6
00:00:19,700 --> 00:00:23,915
منظورم این است که این، این و این نیز به عنوان 3s قابل تشخیص هستند، 

7
00:00:23,915 --> 00:00:28,320
حتی اگر مقادیر خاص هر پیکسل از تصویری به تصویر دیگر بسیار متفاوت باشد.

8
00:00:28,900 --> 00:00:33,045
سلول‌های حساس به نور خاص در چشم شما که با دیدن این 3 شلیک می‌کنند 

9
00:00:33,045 --> 00:00:36,940
با سلول‌هایی که با دیدن این 3 شلیک می‌شوند بسیار متفاوت هستند.

10
00:00:37,520 --> 00:00:41,139
اما چیزی در آن قشر بصری دیوانه وار و هوشمند شما، اینها را به 

11
00:00:41,139 --> 00:00:44,581
عنوان نمایانگر همان ایده تشخیص می دهد، در حالی که در همان 

12
00:00:44,581 --> 00:00:48,260
زمان تصاویر دیگر را به عنوان ایده های متمایز خود تشخیص می دهد.

13
00:00:49,220 --> 00:00:54,825
اما اگر به شما گفتم، هی، بنشینید و برنامه ای برای من بنویسید که یک شبکه 28x28 

14
00:00:54,825 --> 00:01:00,502
پیکسلی مانند این را می گیرد و یک عدد واحد بین 0 تا 10 را خروجی می دهد و به شما 

15
00:01:00,502 --> 00:01:06,180
می گوید که آن رقم فکر می کند، خوب کار از آنجا می رود. کم اهمیت تا به شدت دشوار.

16
00:01:07,160 --> 00:01:10,925
من فکر می کنم به سختی نیازی به انگیزه دادن به ارتباط و اهمیت یادگیری ماشین 

17
00:01:10,925 --> 00:01:14,640
و شبکه های عصبی برای حال و آینده دارم، مگر اینکه زیر سنگ زندگی کرده باشید.

18
00:01:15,120 --> 00:01:18,127
اما کاری که من می‌خواهم در اینجا انجام دهم این است که به شما نشان دهم که یک 

19
00:01:18,127 --> 00:01:21,135
شبکه عصبی در واقع چیست، با فرض اینکه هیچ پیش‌زمینه‌ای وجود ندارد، و به تجسم 

20
00:01:21,135 --> 00:01:24,460
کاری که انجام می‌دهد، نه به عنوان یک کلمه کلیدی، بلکه به عنوان یک تکه ریاضی کمک کنم.

21
00:01:25,020 --> 00:01:28,143
امید من این است که شما با احساس اینکه ساختار خود انگیزه دارد، 

22
00:01:28,143 --> 00:01:30,964
بیرون بیایید، و احساس کنید که وقتی می‌خوانید معنی آن را 

23
00:01:30,964 --> 00:01:34,340
می‌دانید، یا در مورد یادگیری نقل قول-بی نقل قول شبکه عصبی می‌شنوید.

24
00:01:35,360 --> 00:01:40,260
این ویدیو فقط به مولفه ساختار آن اختصاص دارد و ویدیوی زیر به یادگیری می پردازد.

25
00:01:40,960 --> 00:01:43,586
کاری که می‌خواهیم انجام دهیم این است که یک شبکه عصبی را کنار 

26
00:01:43,586 --> 00:01:46,040
هم قرار دهیم که می‌تواند تشخیص ارقام دست‌نویس را بیاموزد.

27
00:01:49,360 --> 00:01:52,826
این یک مثال تا حدی کلاسیک برای معرفی موضوع است و من خوشحالم که به وضعیت 

28
00:01:52,826 --> 00:01:56,292
موجود در اینجا پایبندم، زیرا در پایان دو ویدیو می‌خواهم به چند منبع خوب 

29
00:01:56,292 --> 00:01:59,758
اشاره کنم که در آن می‌توانید بیشتر بدانید و کجا می توانید کدی را که این 

30
00:01:59,758 --> 00:02:03,080
کار را انجام می دهد دانلود کنید و با آن در رایانه شخصی خود بازی کنید.

31
00:02:05,040 --> 00:02:09,875
انواع زیادی از شبکه‌های عصبی وجود دارد، و در سال‌های اخیر به نوعی رونق تحقیقات 

32
00:02:09,875 --> 00:02:14,466
در مورد این گونه‌ها وجود داشته است، اما در این دو ویدیوی مقدماتی، من و شما 

33
00:02:14,466 --> 00:02:19,180
فقط می‌خواهیم ساده‌ترین شکل وانیل ساده را بدون هیچ زواید اضافه‌ای بررسی کنیم.

34
00:02:19,860 --> 00:02:24,042
این یک نوع پیش نیاز ضروری برای درک هر یک از انواع مدرن قدرتمندتر است، و به من 

35
00:02:24,042 --> 00:02:28,600
اعتماد کنید هنوز هم پیچیدگی زیادی برای ما دارد که بتوانیم ذهن خود را به اطراف بپیچیم.

36
00:02:29,120 --> 00:02:32,850
اما حتی در این ساده‌ترین شکل نیز می‌تواند یاد بگیرد که ارقام 

37
00:02:32,850 --> 00:02:36,520
دست‌نویس را تشخیص دهد، که برای کامپیوتر کار بسیار جالبی است.

38
00:02:37,480 --> 00:02:39,930
و در عین حال خواهید دید که چگونه از امیدهای زوجی 

39
00:02:39,930 --> 00:02:42,280
که ممکن است برای آن داشته باشیم، کوتاهی می کند.

40
00:02:43,380 --> 00:02:45,940
همانطور که از نام آن پیداست شبکه های عصبی از مغز 

41
00:02:45,940 --> 00:02:48,500
الهام گرفته شده اند، اما بیایید آن را تجزیه کنیم.

42
00:02:48,520 --> 00:02:51,660
نورون ها چه هستند و از چه نظر به هم مرتبط هستند؟

43
00:02:52,500 --> 00:02:56,318
در حال حاضر وقتی می‌گویم نورون تنها چیزی که می‌خواهم به آن فکر 

44
00:02:56,318 --> 00:03:00,440
کنید چیزی است که عددی را در خود نگه می‌دارد، به‌ویژه عددی بین ۰ و ۱.

45
00:03:00,680 --> 00:03:02,560
واقعا بیشتر از این نیست.

46
00:03:03,780 --> 00:03:08,836
به عنوان مثال، شبکه با یک دسته از نورون‌های مربوط به هر یک از 

47
00:03:08,836 --> 00:03:14,220
پیکسل‌های ۲۸×۲۸ تصویر ورودی شروع می‌شود که در مجموع ۷۸۴ نورون است.

48
00:03:14,700 --> 00:03:19,473
هر یک از اینها دارای یک عدد است که نشان دهنده مقدار مقیاس خاکستری پیکسل 

49
00:03:19,473 --> 00:03:24,380
مربوطه است که از 0 برای پیکسل های سیاه تا 1 برای پیکسل های سفید متغیر است.

50
00:03:25,300 --> 00:03:29,757
به این عدد در داخل نورون، فعال سازی آن می گویند، و تصویری که ممکن است در اینجا در 

51
00:03:29,757 --> 00:03:34,160
ذهن داشته باشید این است که هر نورون زمانی روشن می شود که تعداد فعال آن زیاد باشد.

52
00:03:36,720 --> 00:03:41,860
بنابراین همه این 784 نورون اولین لایه شبکه ما را تشکیل می دهند.

53
00:03:46,500 --> 00:03:48,903
اکنون با پرش به آخرین لایه، این لایه دارای 10 

54
00:03:48,903 --> 00:03:51,360
نورون است که هر یک نشان دهنده یکی از ارقام است.

55
00:03:52,040 --> 00:03:57,150
فعال‌سازی در این نورون‌ها، باز هم عددی بین ۰ و ۱، نشان‌دهنده این است که 

56
00:03:57,150 --> 00:04:02,120
سیستم چقدر فکر می‌کند که یک تصویر داده‌شده با یک رقم معین مطابقت دارد.

57
00:04:03,040 --> 00:04:08,414
همچنین چند لایه در این بین وجود دارد که لایه‌های پنهان نامیده می‌شوند، که فعلاً باید 

58
00:04:08,414 --> 00:04:13,600
یک علامت سؤال غول‌پیکر برای این باشد که این فرآیند تشخیص ارقام چگونه انجام می‌شود.

59
00:04:14,260 --> 00:04:17,381
در این شبکه من دو لایه پنهان را انتخاب کردم که هر کدام 

60
00:04:17,381 --> 00:04:20,560
دارای 16 نورون بودند، و مسلماً این یک انتخاب دلخواه است.

61
00:04:21,019 --> 00:04:24,521
صادقانه بگویم، من دو لایه را بر اساس اینکه چگونه می‌خواهم ساختار را در یک لحظه 

62
00:04:24,521 --> 00:04:28,200
ایجاد کنم، و 16 را انتخاب کردم، خوب این فقط یک عدد خوب برای قرار دادن روی صفحه بود.

63
00:04:28,780 --> 00:04:32,340
در عمل فضای زیادی برای آزمایش با یک ساختار خاص در اینجا وجود دارد.

64
00:04:33,020 --> 00:04:38,480
نحوه عملکرد شبکه، فعال‌سازی در یک لایه، فعال‌سازی لایه بعدی را تعیین می‌کند.

65
00:04:39,200 --> 00:04:43,890
و البته قلب شبکه به عنوان مکانیزم پردازش اطلاعات دقیقاً به این موضوع مربوط 

66
00:04:43,890 --> 00:04:48,580
می شود که چگونه آن فعال سازی از یک لایه باعث فعال سازی در لایه بعدی می شود.

67
00:04:49,140 --> 00:04:53,025
به این معناست که شباهت زیادی به این دارد که چگونه در شبکه‌های بیولوژیکی 

68
00:04:53,025 --> 00:04:57,180
نورون‌ها، برخی از گروه‌های نورون که شلیک می‌کنند باعث شلیک برخی دیگر می‌شوند.

69
00:04:58,120 --> 00:05:00,760
اکنون شبکه ای که در اینجا نشان می دهم قبلاً برای تشخیص ارقام 

70
00:05:00,760 --> 00:05:03,400
آموزش دیده است، و اجازه دهید منظورم را از آن به شما نشان دهم.

71
00:05:03,640 --> 00:05:08,332
به این معنی که اگر در یک تصویر تغذیه کنید، و تمام 784 نورون لایه ورودی 

72
00:05:08,332 --> 00:05:12,959
را با توجه به روشنایی هر پیکسل در تصویر روشن کنید، آن الگوی فعال‌سازی 

73
00:05:12,959 --> 00:05:18,841
باعث ایجاد الگوی بسیار خاصی در لایه بعدی می‌شود که باعث ایجاد الگوی در لایه بعدی می‌شود. 

74
00:05:18,841 --> 00:05:22,080
آن، که در نهایت مقداری الگو در لایه خروجی می دهد.

75
00:05:22,560 --> 00:05:25,912
و روشن‌ترین نورون آن لایه خروجی، انتخاب شبکه است، 

76
00:05:25,912 --> 00:05:29,400
به‌طوری‌که بگوییم، این تصویر چه رقمی را نشان می‌دهد.

77
00:05:32,560 --> 00:05:36,346
و قبل از پرداختن به ریاضیات در مورد اینکه چگونه یک لایه بر لایه بعدی تأثیر می‌گذارد، 

78
00:05:36,346 --> 00:05:40,044
یا اینکه آموزش چگونه کار می‌کند، اجازه دهید در مورد اینکه چرا حتی منطقی است انتظار 

79
00:05:40,044 --> 00:05:43,520
داشته باشیم که ساختار لایه‌ای مانند این رفتار هوشمندانه داشته باشد، صحبت کنیم.

80
00:05:44,060 --> 00:05:45,220
اینجا چه انتظاری داریم؟

81
00:05:45,400 --> 00:05:47,600
بهترین امید برای آن لایه های میانی چیست؟

82
00:05:48,920 --> 00:05:53,520
خوب، وقتی من یا شما ارقام را تشخیص می دهیم، اجزای مختلفی را کنار هم می گذاریم.

83
00:05:54,200 --> 00:05:56,820
عدد 9 دارای یک حلقه بالا و یک خط در سمت راست است.

84
00:05:57,380 --> 00:06:01,180
8 همچنین دارای یک حلقه به بالا است، اما با یک حلقه دیگر به پایین جفت می شود.

85
00:06:01,980 --> 00:06:06,820
یک 4 اساساً به سه خط خاص و مواردی از این دست تقسیم می شود.

86
00:06:07,600 --> 00:06:12,891
اکنون در یک دنیای کامل، ممکن است امیدوار باشیم که هر نورون در لایه دوم تا آخرین با یکی 

87
00:06:12,891 --> 00:06:18,123
از این اجزای فرعی مطابقت داشته باشد، که هر زمان که در یک تصویر با مثلاً یک حلقه بالا، 

88
00:06:18,123 --> 00:06:23,536
مانند 9 یا 8 تغذیه می‌کنید، مقداری وجود دارد. نورون خاصی که فعال شدن آن نزدیک به 1 خواهد 

89
00:06:23,536 --> 00:06:23,780
بود.

90
00:06:24,500 --> 00:06:28,030
و منظور من این حلقه خاص از پیکسل ها نیست، امید این است که هر 

91
00:06:28,030 --> 00:06:31,560
الگوی به طور کلی حلقه ای به سمت بالا، این نورون را تنظیم کند.

92
00:06:32,440 --> 00:06:36,048
به این ترتیب، رفتن از لایه سوم به لایه آخر فقط 

93
00:06:36,048 --> 00:06:40,040
مستلزم یادگیری ترکیبی از اجزای فرعی با کدام رقم است.

94
00:06:41,000 --> 00:06:44,342
البته، این فقط مشکل را به پایان می‌رساند، زیرا چگونه می‌توانید این اجزای 

95
00:06:44,342 --> 00:06:47,640
فرعی را تشخیص دهید، یا حتی یاد بگیرید که اجزای فرعی مناسب باید چه باشند؟

96
00:06:48,060 --> 00:06:50,599
و من هنوز حتی در مورد اینکه چگونه یک لایه بر لایه بعدی تأثیر می 

97
00:06:50,599 --> 00:06:53,060
گذارد صحبت نکرده ام، اما برای یک لحظه با من روی این یکی بدوید.

98
00:06:53,680 --> 00:06:56,680
تشخیص یک حلقه همچنین می تواند به مشکلات فرعی تقسیم شود.

99
00:06:57,280 --> 00:06:59,979
یک راه معقول برای انجام این کار این است که ابتدا لبه 

100
00:06:59,979 --> 00:07:02,780
های کوچک مختلفی را که آن را تشکیل می دهند شناسایی کنید.

101
00:07:03,780 --> 00:07:07,312
به طور مشابه، یک خط بلند، مانند آن چیزی که ممکن است در ارقام 

102
00:07:07,312 --> 00:07:10,845
1 یا 4 یا 7 ببینید، در واقع فقط یک لبه بلند است، یا شاید شما 

103
00:07:10,845 --> 00:07:14,320
آن را به عنوان الگوی خاصی از چندین لبه کوچکتر در نظر بگیرید.

104
00:07:15,140 --> 00:07:18,786
بنابراین شاید امید ما این باشد که هر نورون در لایه 

105
00:07:18,786 --> 00:07:22,720
دوم شبکه با لبه های کوچک مرتبط مختلف مطابقت داشته باشد.

106
00:07:23,540 --> 00:07:28,909
شاید وقتی تصویری مانند این می آید، تمام نورون های مرتبط با حدود 8 تا 10 لبه 

107
00:07:28,909 --> 00:07:34,279
کوچک خاص را روشن می کند، که به نوبه خود نورون های مرتبط با حلقه بالایی و یک 

108
00:07:34,279 --> 00:07:39,720
خط عمودی طولانی را روشن می کند و آن ها نور را روشن می کنند. نورون مرتبط با 9.

109
00:07:40,680 --> 00:07:44,737
اینکه آیا این همان کاری است که شبکه نهایی ما واقعاً انجام می دهد یا خیر، سؤال 

110
00:07:44,737 --> 00:07:48,638
دیگری است که وقتی ببینیم چگونه شبکه را آموزش دهیم به آن باز خواهم گشت، اما 

111
00:07:48,638 --> 00:07:52,540
این امیدی است که ممکن است داشته باشیم، نوعی هدف با ساختار لایه ای. مثل این.

112
00:07:53,160 --> 00:07:56,702
علاوه بر این، می‌توانید تصور کنید که چگونه قادر به تشخیص لبه‌ها 

113
00:07:56,702 --> 00:08:00,300
و الگوهای این چنینی برای سایر کارهای تشخیص تصویر واقعاً مفید است.

114
00:08:00,880 --> 00:08:04,128
و حتی فراتر از تشخیص تصویر، انواع کارهای هوشمندانه ای وجود دارد که 

115
00:08:04,128 --> 00:08:07,280
ممکن است بخواهید انجام دهید که به لایه های انتزاعی تقسیم می شوند.

116
00:08:08,040 --> 00:08:11,957
به عنوان مثال، تجزیه گفتار شامل گرفتن صوت خام و انتخاب صداهای متمایز است 

117
00:08:11,957 --> 00:08:15,981
که با هم ترکیب می شوند و هجاهای خاصی را می سازند، که ترکیب می شوند و کلمات 

118
00:08:15,981 --> 00:08:20,060
را می سازند، که ترکیب می شوند تا عبارات و افکار انتزاعی تر و غیره را بسازند.

119
00:08:21,100 --> 00:08:25,449
اما برای بازگشت به نحوه عملکرد هر یک از اینها، خود را در حال طراحی تصور 

120
00:08:25,449 --> 00:08:29,920
کنید که دقیقاً چگونه فعال سازی در یک لایه ممکن است لایه بعدی را تعیین کند.

121
00:08:30,860 --> 00:08:34,886
هدف این است که مکانیزمی داشته باشیم که بتواند پیکسل ها را به 

122
00:08:34,886 --> 00:08:38,980
لبه ها یا لبه ها را به الگوها یا الگوها را به ارقام ترکیب کند.

123
00:08:39,440 --> 00:08:44,874
و برای بزرگنمایی روی یک مثال بسیار خاص، بیایید بگوییم امیدواریم که یک 

124
00:08:44,874 --> 00:08:50,620
نورون خاص در لایه دوم تشخیص دهد که آیا تصویر در این ناحیه لبه دارد یا خیر.

125
00:08:51,440 --> 00:08:55,100
سوالی که مطرح است این است که شبکه چه پارامترهایی باید داشته باشد؟

126
00:08:55,640 --> 00:08:59,597
چه صفحه‌ها و دستگیره‌هایی را باید به گونه‌ای تنظیم کنید که به اندازه کافی 

127
00:08:59,597 --> 00:09:03,501
رسا باشد که به طور بالقوه این الگو، یا هر الگوی پیکسلی دیگر، یا الگوی که 

128
00:09:03,501 --> 00:09:07,780
چندین لبه می‌تواند یک حلقه ایجاد کند، و موارد دیگر از این قبیل را به تصویر بکشد؟

129
00:09:08,720 --> 00:09:11,993
خوب، کاری که ما انجام خواهیم داد این است که به هر یک از 

130
00:09:11,993 --> 00:09:15,560
اتصالات بین نورون ما و نورون های لایه اول یک وزن اختصاص دهیم.

131
00:09:16,320 --> 00:09:17,700
این وزن ها فقط اعداد هستند.

132
00:09:18,540 --> 00:09:21,883
سپس تمام آن فعال‌سازی‌ها را از لایه اول بگیرید و 

133
00:09:21,883 --> 00:09:25,500
مجموع وزنی آنها را با توجه به این وزن‌ها محاسبه کنید.

134
00:09:27,700 --> 00:09:32,337
من فکر می کنم که این وزن ها به عنوان یک شبکه کوچک سازماندهی شده اند مفید است، و من 

135
00:09:32,337 --> 00:09:37,030
از پیکسل های سبز برای نشان دادن وزن های مثبت و از پیکسل های قرمز برای نشان دادن وزن 

136
00:09:37,030 --> 00:09:41,780
های منفی استفاده می کنم، جایی که روشنایی آن پیکسل مقداری است. تصویری آزاد از ارزش وزن

137
00:09:42,780 --> 00:09:47,834
حال اگر وزن‌های مرتبط با تقریباً همه پیکسل‌ها را صفر کنیم، به جز برخی از وزن‌های 

138
00:09:47,834 --> 00:09:52,640
مثبت در این ناحیه که به آن‌ها اهمیت می‌دهیم، پس گرفتن مجموع وزنی تمام مقادیر 

139
00:09:52,640 --> 00:09:57,820
پیکسل واقعاً به معنای جمع کردن مقادیر پیکسل است. منطقه ای که ما به آن اهمیت می دهیم

140
00:09:59,140 --> 00:10:02,823
و اگر واقعاً می‌خواهید بفهمید که آیا لبه‌ای در اینجا وجود دارد یا خیر، کاری که 

141
00:10:02,823 --> 00:10:06,600
ممکن است انجام دهید این است که وزن‌های منفی مرتبط با پیکسل‌های اطراف داشته باشید.

142
00:10:07,480 --> 00:10:10,144
سپس مجموع زمانی که آن پیکسل‌های میانی روشن هستند 

143
00:10:10,144 --> 00:10:12,700
اما پیکسل‌های اطراف تیره‌تر هستند، بزرگ‌تر است.

144
00:10:14,260 --> 00:10:18,844
وقتی یک مجموع وزنی مانند این را محاسبه می کنید، ممکن است با هر عددی بیرون بیایید، 

145
00:10:18,844 --> 00:10:23,540
اما برای این شبکه چیزی که ما می خواهیم این است که مقدار فعال سازی ها بین 0 و 1 باشد.

146
00:10:24,120 --> 00:10:28,064
بنابراین یک کار معمول این است که این مجموع وزنی را به تابعی 

147
00:10:28,064 --> 00:10:32,140
پمپ کنیم که خط اعداد واقعی را در محدوده بین 0 و 1 قرار می دهد.

148
00:10:32,460 --> 00:10:34,918
و یک تابع رایج که این کار را انجام می دهد تابع سیگموئید 

149
00:10:34,918 --> 00:10:37,420
نامیده می شود که به عنوان منحنی لجستیک نیز شناخته می شود.

150
00:10:38,000 --> 00:10:42,229
اساساً ورودی های بسیار منفی نزدیک به 0، ورودی های مثبت به 1 

151
00:10:42,229 --> 00:10:46,600
نزدیک می شوند و به طور پیوسته در اطراف ورودی 0 افزایش می یابد.

152
00:10:49,120 --> 00:10:56,360
بنابراین فعال شدن نورون در اینجا اساساً معیاری برای مثبت بودن مجموع وزنی مربوطه است.

153
00:10:57,540 --> 00:11:01,880
اما شاید اینطور نباشد که بخواهید نورون زمانی که مجموع وزنی بزرگتر از 0 است روشن شود.

154
00:11:02,280 --> 00:11:06,360
شاید بخواهید فقط زمانی فعال باشد که مجموع آن بزرگتر از مثلاً 10 باشد.

155
00:11:06,840 --> 00:11:10,260
یعنی شما می خواهید کمی سوگیری برای غیر فعال بودن آن داشته باشید.

156
00:11:11,380 --> 00:11:15,520
کاری که ما انجام خواهیم داد این است که فقط یک عدد دیگر مانند منفی 10 را به این 

157
00:11:15,520 --> 00:11:19,660
مجموع وزنی اضافه کنیم قبل از اینکه آن را از طریق تابع کوبیدن سیگموئید وصل کنیم.

158
00:11:20,580 --> 00:11:22,440
به آن عدد اضافی سوگیری می گویند.

159
00:11:23,460 --> 00:11:27,447
بنابراین وزن‌ها به شما می‌گویند که این نورون در لایه دوم چه الگوی 

160
00:11:27,447 --> 00:11:31,374
پیکسلی را انتخاب می‌کند، و سوگیری به شما می‌گوید که قبل از اینکه 

161
00:11:31,374 --> 00:11:35,180
نورون شروع به فعال شدن معنی‌دار کند، مجموع وزنی چقدر باید باشد.

162
00:11:36,120 --> 00:11:37,680
و این فقط یک نورون است.

163
00:11:38,280 --> 00:11:44,560
هر نورون دیگر در این لایه قرار است به تمام نورون های 784 پیکسلی 

164
00:11:44,560 --> 00:11:50,940
از لایه اول متصل شود و هر یک از آن 784 اتصال وزن خاص خود را دارد.

165
00:11:51,600 --> 00:11:54,464
همچنین، هر یک مقداری سوگیری دارد، تعدادی عدد دیگر که 

166
00:11:54,464 --> 00:11:57,600
قبل از له کردن آن با سیگموئید به مجموع وزنی اضافه می‌کنید.

167
00:11:58,110 --> 00:11:59,540
و این جای تامل زیادی دارد!

168
00:11:59,960 --> 00:12:07,980
با این لایه پنهان از 16 نورون، در مجموع 784 ضربدر 16 وزن، همراه با 16 سوگیری است.

169
00:12:08,840 --> 00:12:11,940
و همه اینها فقط اتصالات از لایه اول به لایه دوم است.

170
00:12:12,520 --> 00:12:17,340
اتصالات بین لایه های دیگر نیز دارای دسته ای از وزن ها و سوگیری های مرتبط با آنها است.

171
00:12:18,340 --> 00:12:23,800
همه گفته‌ها و انجام‌ها، این شبکه تقریباً دقیقاً 13000 وزن و بایاس کل دارد.

172
00:12:23,800 --> 00:12:26,880
13000 دستگیره و شماره گیری که می توان آنها را تغییر 

173
00:12:26,880 --> 00:12:29,960
داد و چرخاند تا این شبکه به روش های مختلف رفتار کند.

174
00:12:31,040 --> 00:12:36,169
بنابراین وقتی در مورد یادگیری صحبت می کنیم، منظور این است که کامپیوتر را برای یافتن 

175
00:12:36,169 --> 00:12:41,360
یک تنظیم معتبر برای همه این اعداد بسیار زیاد به طوری که در واقع مشکل موجود را حل کند.

176
00:12:42,620 --> 00:12:47,154
یک آزمایش فکری که در عین حال سرگرم کننده و وحشتناک است این است که تصور کنید 

177
00:12:47,154 --> 00:12:51,926
بنشینید و تمام این وزن ها و سوگیری ها را با دست تنظیم کنید، به طور هدفمند اعداد 

178
00:12:51,926 --> 00:12:56,580
را تغییر دهید تا لایه دوم لبه ها را بگیرد، لایه سوم روی الگوها بنشیند. و غیره.

179
00:12:56,980 --> 00:13:01,294
من شخصاً این را رضایت‌بخش می‌دانم تا اینکه شبکه را فقط به عنوان یک جعبه 

180
00:13:01,294 --> 00:13:05,430
سیاه کامل نگاه کنم، زیرا وقتی شبکه آنطور که شما پیش‌بینی می‌کنید عمل 

181
00:13:05,430 --> 00:13:10,224
نمی‌کند، اگر کمی با معنای واقعی آن وزن‌ها و سوگیری‌ها ارتباط برقرار کرده باشید. 

182
00:13:10,224 --> 00:13:14,180
، شما یک مکان شروع برای آزمایش نحوه تغییر ساختار برای بهبود دارید.

183
00:13:14,960 --> 00:13:18,528
یا زمانی که شبکه کار می کند، اما نه به دلایلی که ممکن است انتظارش را 

184
00:13:18,528 --> 00:13:22,251
داشته باشید، کاوش در آنچه که وزن ها و سوگیری ها انجام می دهند، راه خوبی 

185
00:13:22,251 --> 00:13:25,820
برای به چالش کشیدن مفروضات شما و افشای فضای کامل راه حل های ممکن است.

186
00:13:26,840 --> 00:13:30,680
به هر حال، نوشتن عملکرد واقعی در اینجا کمی دست و پا گیر است، فکر نمی کنید؟

187
00:13:32,500 --> 00:13:37,140
بنابراین اجازه دهید به شما روشی فشرده تر نشان دهم که این اتصالات نشان داده می شوند.

188
00:13:37,660 --> 00:13:40,520
اگر بخواهید در مورد شبکه های عصبی بیشتر بخوانید، اینگونه خواهید دید.

189
00:13:41,380 --> 00:13:49,372
همه فعال‌سازی‌ها را از یک لایه در یک ستون سازماندهی کنید، زیرا 

190
00:13:49,372 --> 00:13:58,000
ماتریس مربوط به اتصالات بین یک لایه و یک نورون خاص در لایه بعدی است.

191
00:13:58,540 --> 00:14:04,243
منظور این است که گرفتن مجموع وزنی فعال‌سازی‌ها در لایه اول با توجه به این وزن‌ها، با 

192
00:14:04,243 --> 00:14:09,880
یکی از عبارت‌های حاصلضرب بردار ماتریس هر چیزی که در سمت چپ اینجا داریم، مطابقت دارد.

193
00:14:14,000 --> 00:14:18,905
به هر حال، بسیاری از یادگیری ماشین فقط به درک خوب جبر خطی بستگی دارد، بنابراین برای 

194
00:14:18,905 --> 00:14:23,869
هر یک از شما که می‌خواهید درک بصری خوبی برای ماتریس‌ها و معنی ضرب بردار ماتریس داشته 

195
00:14:23,869 --> 00:14:28,600
باشید، به سری‌هایی که من در آن انجام دادم نگاهی بیندازید. جبر خطی، به ویژه فصل 3.

196
00:14:29,240 --> 00:14:33,513
به بیان خودمان برگردیم، به جای اینکه در مورد اضافه کردن بایاس به هر یک 

197
00:14:33,513 --> 00:14:37,786
از این مقادیر به طور مستقل صحبت کنیم، آن را با سازماندهی تمام آن بایاس 

198
00:14:37,786 --> 00:14:42,300
ها در یک بردار، و افزودن کل بردار به محصول بردار ماتریس قبلی، نشان می دهیم.

199
00:14:43,280 --> 00:14:47,255
سپس به عنوان آخرین مرحله، من یک سیگموئید را در اطراف بیرون می‌پیچم، 

200
00:14:47,255 --> 00:14:50,939
و چیزی که قرار است نشان‌دهنده آن باشد این است که شما می‌خواهید 

201
00:14:50,939 --> 00:14:54,740
تابع sigmoid را برای هر جزء خاص از بردار حاصل در داخل اعمال کنید.

202
00:14:55,940 --> 00:15:00,735
بنابراین هنگامی که این ماتریس وزن و این بردارها را به عنوان نمادهای خود یادداشت 

203
00:15:00,735 --> 00:15:05,530
کردید، می‌توانید انتقال کامل فعال‌سازی‌ها را از یک لایه به لایه بعدی در یک بیان 

204
00:15:05,530 --> 00:15:10,445
کوچک بسیار فشرده و منظم انتقال دهید، و این باعث می‌شود کد مربوطه هم بسیار ساده‌تر 

205
00:15:10,445 --> 00:15:15,660
و هم ساده‌تر شود. بسیار سریعتر، زیرا بسیاری از کتابخانه ها ضرب ماتریس را بهینه می کنند.

206
00:15:17,820 --> 00:15:21,460
به خاطر دارید که قبلاً گفتم این نورون ها چیزهایی هستند که اعداد را نگه می دارند؟

207
00:15:22,220 --> 00:15:27,571
البته اعداد خاصی که نگه می‌دارند بستگی به تصویری دارد که شما از آن تغذیه می‌کنید، 

208
00:15:27,571 --> 00:15:32,923
بنابراین در واقع دقیق‌تر است که هر نورون را به عنوان یک تابع در نظر بگیرید، تابعی 

209
00:15:32,923 --> 00:15:38,340
که خروجی‌های تمام نورون‌های لایه قبلی را می‌گیرد و عددی را بیرون می‌ریزد. بین 0 و 1

210
00:15:39,200 --> 00:15:43,165
در واقع کل شبکه فقط یک تابع است، تابعی که 784 عدد را به 

211
00:15:43,165 --> 00:15:47,060
عنوان ورودی می گیرد و 10 عدد را به عنوان خروجی می ریزد.

212
00:15:47,560 --> 00:15:52,251
این یک تابع پیچیده است، تابعی که شامل 13000 پارامتر به شکل این وزن‌ها 

213
00:15:52,251 --> 00:15:57,009
و سوگیری‌هایی است که الگوهای خاصی را نشان می‌دهند، و شامل تکرار بسیاری 

214
00:15:57,009 --> 00:16:01,834
از محصولات بردار ماتریس و تابع انقباض سیگموئید است، اما با این وجود فقط 

215
00:16:01,834 --> 00:16:06,660
یک تابع است و در یک این به نوعی اطمینان بخش است که پیچیده به نظر می رسد.

216
00:16:07,340 --> 00:16:10,108
منظورم این است که اگر ساده‌تر بود، چه امیدی داشتیم 

217
00:16:10,108 --> 00:16:12,280
که بتواند چالش تشخیص ارقام را انجام دهد؟

218
00:16:13,340 --> 00:16:14,700
و چگونه آن چالش را انجام می دهد؟

219
00:16:15,080 --> 00:16:19,360
چگونه این شبکه تنها با مشاهده داده ها، وزن ها و سوگیری های مناسب را یاد می گیرد؟

220
00:16:20,140 --> 00:16:23,068
خب این چیزی است که در ویدیوی بعدی نشان خواهم داد، و همچنین کمی بیشتر در 

221
00:16:23,068 --> 00:16:26,120
مورد آنچه که این شبکه خاصی که می بینیم واقعاً انجام می دهد، کاوش خواهم کرد.

222
00:16:27,580 --> 00:16:30,787
حالا فکر می‌کنم باید بگویم مشترک شوید تا از زمانی که ویدیو 

223
00:16:30,787 --> 00:16:34,049
یا هر ویدیوی جدیدی منتشر می‌شود مطلع شوید، اما در واقع اکثر 

224
00:16:34,049 --> 00:16:37,420
شما واقعاً اعلان‌هایی را از YouTube دریافت نمی‌کنید، درست است؟

225
00:16:38,020 --> 00:16:42,894
شاید صادقانه‌تر بگویم مشترک شوید تا شبکه‌های عصبی که زیربنای الگوریتم توصیه‌های YouTube 

226
00:16:42,894 --> 00:16:47,880
هستند این باور را داشته باشند که می‌خواهید محتوای این کانال را ببینید به شما توصیه می‌شود.

227
00:16:48,560 --> 00:16:49,940
به هر حال برای اطلاعات بیشتر در جریان باشید.

228
00:16:50,760 --> 00:16:53,500
از همه کسانی که از این ویدیوها در Patreon حمایت می کنند بسیار سپاسگزاریم.

229
00:16:54,000 --> 00:16:57,853
من در تابستان امسال برای پیشرفت در سری احتمالات کمی کند بوده ام، اما بعد از این 

230
00:16:57,853 --> 00:17:01,900
پروژه دوباره وارد آن می شوم، بنابراین کاربران می توانند منتظر به روز رسانی ها باشند.

231
00:17:03,600 --> 00:17:07,229
برای پایان دادن به این موضوع، لیشا لی را با خود دارم که کار دکترای خود را در جنبه 

232
00:17:07,229 --> 00:17:10,813
نظری یادگیری عمیق انجام داد و در حال حاضر در یک شرکت سرمایه گذاری خطرپذیر به نام 

233
00:17:10,813 --> 00:17:14,619
Amplify Partners کار می کند که با مهربانی مقداری از بودجه را برای این ویدیو فراهم کرد.

234
00:17:15,460 --> 00:17:19,119
بنابراین لیشا یک چیزی که فکر می کنم باید سریعاً مطرح کنیم این تابع سیگموئید است.

235
00:17:19,700 --> 00:17:23,132
همانطور که می‌دانم شبکه‌های اولیه از این استفاده می‌کنند تا مجموع 

236
00:17:23,132 --> 00:17:26,460
وزنی مربوطه را در فاصله بین صفر و یک قرار دهند، شما می‌دانید که 

237
00:17:26,460 --> 00:17:29,840
به نوعی انگیزه این تشبیه بیولوژیکی نورون‌ها، غیرفعال یا فعال است.

238
00:17:30,280 --> 00:17:30,300
دقیقا.

239
00:17:30,560 --> 00:17:34,040
اما تعداد نسبتا کمی از شبکه های مدرن دیگر از سیگموئید استفاده می کنند.

240
00:17:34,320 --> 00:17:34,320
آره

241
00:17:34,440 --> 00:17:35,540
این یک نوع مدرسه قدیمی است درست است؟

242
00:17:35,760 --> 00:17:38,980
بله یا بهتر بگوییم relu به نظر می رسد بسیار آسان تر برای آموزش.

243
00:17:39,400 --> 00:17:42,340
و relu مخفف واحد خطی اصلاح شده است؟

244
00:17:42,680 --> 00:17:48,206
بله، این نوع تابعی است که در آن شما فقط حداکثر صفر را می گیرید و a را با 

245
00:17:48,206 --> 00:17:53,580
چیزی که در ویدیو توضیح می دهید به دست می آورید و این به نوعی انگیزه آن 

246
00:17:53,580 --> 00:17:58,273
چیست، فکر می کنم تا حدی با یک قیاس بیولوژیکی با نورون ها بود. 

247
00:17:58,273 --> 00:18:03,648
یا فعال می شود یا نه و بنابراین اگر از آستانه خاصی عبور کند، تابع هویت 

248
00:18:03,648 --> 00:18:09,250
خواهد بود، اما اگر فعال نمی شد، فعال نمی شد، بنابراین صفر می شد، بنابراین 

249
00:18:09,250 --> 00:18:10,840
یک نوع ساده سازی است.

250
00:18:11,160 --> 00:18:17,702
استفاده از سیگموئیدها کمکی به آموزش نکرد یا آموزش در برخی موارد بسیار دشوار بود و مردم 

251
00:18:17,702 --> 00:18:24,319
فقط relu را امتحان کردند و اتفاقاً برای این شبکه‌های عصبی فوق‌العاده عمیق بسیار خوب عمل 

252
00:18:24,319 --> 00:18:24,620
کرد.

253
00:18:25,100 --> 00:18:25,640
باشه ممنون آلیشیا

