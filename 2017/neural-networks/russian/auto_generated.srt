1
00:00:04,220 --> 00:00:05,400
Это 3.

2
00:00:06,060 --> 00:00:10,894
Она написана небрежно и выполнена в крайне низком разрешении 28х28 пикселей, 

3
00:00:10,894 --> 00:00:13,720
но твой мозг без проблем распознает ее как 3.

4
00:00:14,340 --> 00:00:16,506
И я хочу, чтобы ты уделил время тому, чтобы оценить, 

5
00:00:16,506 --> 00:00:18,960
насколько это безумно, что мозги могут делать это так легко.

6
00:00:19,700 --> 00:00:23,756
Я имею в виду, что это, это и это тоже можно распознать как 3s, несмотря на то, 

7
00:00:23,756 --> 00:00:28,320
что конкретные значения каждого пикселя сильно отличаются от одного изображения к другому.

8
00:00:28,900 --> 00:00:32,484
Конкретные светочувствительные клетки в твоем глазу, которые срабатывают, 

9
00:00:32,484 --> 00:00:35,874
когда ты видишь эту 3, сильно отличаются от тех, которые срабатывают, 

10
00:00:35,874 --> 00:00:36,940
когда ты видишь эту 3.

11
00:00:37,520 --> 00:00:40,620
Но что-то в твоей безумно умной зрительной коре решает, 

12
00:00:40,620 --> 00:00:44,052
что они представляют собой одну и ту же идею, и в то же время 

13
00:00:44,052 --> 00:00:48,260
распознает другие изображения как свои собственные, отличные от других идеи.

14
00:00:49,220 --> 00:00:53,080
Но если я скажу тебе: "Эй, сядь и напиши для меня программу, 

15
00:00:53,080 --> 00:00:58,585
которая принимает сетку из 28x28 пикселей, как здесь, и выводит одно число от 0 до 10, 

16
00:00:58,585 --> 00:01:02,129
сообщая тебе, какой, по ее мнению, является эта цифра", 

17
00:01:02,129 --> 00:01:06,180
- задача превратится из комически тривиальной в пугающе сложную.

18
00:01:07,160 --> 00:01:10,998
Если ты не жил под камнем, думаю, мне вряд ли нужно мотивировать актуальность 

19
00:01:10,998 --> 00:01:14,640
и важность машинного обучения и нейронных сетей для настоящего и будущего.

20
00:01:15,120 --> 00:01:18,824
Но здесь я хочу показать тебе, что такое нейронная сеть на самом деле, 

21
00:01:18,824 --> 00:01:23,259
без всяких предисловий, и помочь представить, что она делает, не как жужжащее слово, 

22
00:01:23,259 --> 00:01:24,460
а как кусок математики.

23
00:01:25,020 --> 00:01:28,788
Я надеюсь, что ты уйдешь с ощущением того, что сама структура мотивирует, 

24
00:01:28,788 --> 00:01:31,946
и почувствуешь, что знаешь, что это значит, когда читаешь или 

25
00:01:31,946 --> 00:01:34,340
слышишь о нейронной сети quot-unquote learning.

26
00:01:35,360 --> 00:01:40,260
Это видео будет посвящено только структурной составляющей, а следующее - обучению.

27
00:01:40,960 --> 00:01:43,676
Что мы собираемся сделать, так это собрать нейросеть, 

28
00:01:43,676 --> 00:01:46,040
которая научится распознавать рукописные цифры.

29
00:01:49,360 --> 00:01:51,993
Это в некотором роде классический пример для введения в тему, 

30
00:01:51,993 --> 00:01:54,202
и я с удовольствием придерживаюсь здесь статус-кво, 

31
00:01:54,202 --> 00:01:57,430
потому что в конце двух видео я хочу указать тебе на пару хороших ресурсов, 

32
00:01:57,430 --> 00:02:00,998
где ты можешь узнать больше, а также где ты можешь скачать код, который делает это, 

33
00:02:00,998 --> 00:02:03,080
и поиграть с ним на своем собственном компьютере.

34
00:02:05,040 --> 00:02:07,821
Существует множество вариантов нейронных сетей, 

35
00:02:07,821 --> 00:02:12,805
и в последние годы наблюдается своего рода бум исследований в области этих вариантов, 

36
00:02:12,805 --> 00:02:17,499
но в этих двух вводных видео мы с тобой рассмотрим самую простую ванильную форму 

37
00:02:17,499 --> 00:02:19,180
без дополнительных наворотов.

38
00:02:19,860 --> 00:02:24,356
Это своего рода необходимая предпосылка для понимания любого из более мощных современных 

39
00:02:24,356 --> 00:02:28,600
вариантов, и поверь мне, в них еще много сложностей, которые нам предстоит обдумать.

40
00:02:29,120 --> 00:02:34,013
Но даже в таком простейшем виде он может научиться распознавать рукописные цифры, 

41
00:02:34,013 --> 00:02:36,520
а это довольно крутая вещь для компьютера.

42
00:02:37,480 --> 00:02:40,486
И в то же время ты увидишь, как она не оправдала пару надежд, 

43
00:02:40,486 --> 00:02:42,280
которые мы могли бы на нее возлагать.

44
00:02:43,380 --> 00:02:46,648
Как следует из названия, нейронные сети вдохновлены мозгом, 

45
00:02:46,648 --> 00:02:48,500
но давай разложим это по полочкам.

46
00:02:48,520 --> 00:02:51,660
Что такое нейроны, и в каком смысле они связаны между собой?

47
00:02:52,500 --> 00:02:56,734
Сейчас, когда я говорю "нейрон", все, что я хочу, чтобы ты думал о нем, 

48
00:02:56,734 --> 00:03:00,440
- это штука, которая хранит число, в частности число от 0 до 1.

49
00:03:00,680 --> 00:03:02,560
На самом деле это не более того.

50
00:03:03,780 --> 00:03:08,963
Например, сеть начинается с пучка нейронов, соответствующих каждому из 

51
00:03:08,963 --> 00:03:14,220
28х28 пикселей входного изображения, что в сумме составляет 784 нейрона.

52
00:03:14,700 --> 00:03:19,481
Каждый из них содержит число, которое представляет собой значение градации серого 

53
00:03:19,481 --> 00:03:24,380
соответствующего пикселя, начиная от 0 для черных пикселей и заканчивая 1 для белых.

54
00:03:25,300 --> 00:03:28,285
Это число внутри нейрона называется его активацией, и образ, 

55
00:03:28,285 --> 00:03:30,782
который ты можешь иметь в виду, заключается в том, 

56
00:03:30,782 --> 00:03:34,160
что каждый нейрон светится, когда его активация равна высокому числу.

57
00:03:36,720 --> 00:03:41,860
Итак, все эти 784 нейрона составляют первый слой нашей сети.

58
00:03:46,500 --> 00:03:49,200
Теперь переходим к последнему слою, в нем 10 нейронов, 

59
00:03:49,200 --> 00:03:51,360
каждый из которых представляет одну из цифр.

60
00:03:52,040 --> 00:03:54,943
Активация в этих нейронах, опять же некоторое число, 

61
00:03:54,943 --> 00:03:57,901
которое находится между 0 и 1, представляет собой то, 

62
00:03:57,901 --> 00:04:02,120
насколько система считает, что данное изображение соответствует данной цифре.

63
00:04:03,040 --> 00:04:06,049
Между ними есть еще пара слоев, называемых скрытыми, 

64
00:04:06,049 --> 00:04:10,307
которые на данный момент должны быть просто огромным знаком вопроса о том, 

65
00:04:10,307 --> 00:04:13,600
как на земле будет происходить процесс распознавания цифр.

66
00:04:14,260 --> 00:04:18,246
В этой сети я выбрал два скрытых слоя, каждый из которых состоит из 16 нейронов, 

67
00:04:18,246 --> 00:04:20,560
и, признаться, это довольно произвольный выбор.

68
00:04:21,019 --> 00:04:23,036
Честно говоря, я выбрал два слоя, исходя из того, 

69
00:04:23,036 --> 00:04:25,820
как я хочу мотивировать структуру буквально через минуту, а 16 - ну, 

70
00:04:25,820 --> 00:04:28,200
это было просто красивое число, чтобы уместиться на экране.

71
00:04:28,780 --> 00:04:32,340
На практике здесь есть большой простор для экспериментов с конкретной структурой.

72
00:04:33,020 --> 00:04:38,480
По принципу работы сети активации в одном слое определяют активации следующего слоя.

73
00:04:39,200 --> 00:04:43,920
И, конечно же, суть сети как механизма обработки информации сводится к тому, 

74
00:04:43,920 --> 00:04:48,580
как именно эти активации одного слоя приводят к активациям в следующем слое.

75
00:04:49,140 --> 00:04:52,892
Это должно быть неким аналогом того, как в биологических сетях 

76
00:04:52,892 --> 00:04:57,180
нейронов одни группы нейронов, срабатывая, вызывают срабатывание других.

77
00:04:58,120 --> 00:05:01,484
Теперь сеть, которую я здесь показываю, уже обучена распознавать цифры, 

78
00:05:01,484 --> 00:05:03,400
и давай я покажу тебе, что я имею в виду.

79
00:05:03,640 --> 00:05:08,208
Это значит, что если ты подаешь изображение, освещая все 784 нейрона входного слоя 

80
00:05:08,208 --> 00:05:11,401
в соответствии с яркостью каждого пикселя на изображении, 

81
00:05:11,401 --> 00:05:16,355
то этот паттерн активаций вызывает какой-то очень специфический паттерн в следующем слое, 

82
00:05:16,355 --> 00:05:19,327
который вызывает какой-то паттерн в следующем за ним, 

83
00:05:19,327 --> 00:05:22,080
что в итоге дает какой-то паттерн в выходном слое.

84
00:05:22,560 --> 00:05:26,064
И самый яркий нейрон этого выходного слоя - это, так сказать, 

85
00:05:26,064 --> 00:05:29,400
выбор сети о том, какую цифру представляет это изображение.

86
00:05:32,560 --> 00:05:36,179
И прежде чем перейти к математике того, как один слой влияет на другой, 

87
00:05:36,179 --> 00:05:39,045
или как работает обучение, давай просто поговорим о том, 

88
00:05:39,045 --> 00:05:43,520
почему вообще разумно ожидать, что такая многослойная структура будет вести себя разумно.

89
00:05:44,060 --> 00:05:45,220
Чего мы здесь ожидаем?

90
00:05:45,400 --> 00:05:47,600
На что больше всего надеются эти средние слои?

91
00:05:48,920 --> 00:05:53,520
Ну, когда ты или я узнаем цифры, мы собираем воедино различные компоненты.

92
00:05:54,200 --> 00:05:56,820
У девятки петля наверху и линия справа.

93
00:05:57,380 --> 00:06:01,180
У восьмерки тоже есть петля вверху, но она соединена с другой петлей внизу.

94
00:06:01,980 --> 00:06:06,820
Четверка в основном разбивается на три конкретные линии и тому подобные вещи.

95
00:06:07,600 --> 00:06:11,891
В идеальном мире мы могли бы надеяться, что каждый нейрон в предпоследнем 

96
00:06:11,891 --> 00:06:15,429
слое соответствует одному из этих слагаемых, что каждый раз, 

97
00:06:15,429 --> 00:06:19,256
когда ты вводишь изображение, скажем, с петлей наверху, например, 

98
00:06:19,256 --> 00:06:23,780
9 или 8, есть какой-то конкретный нейрон, активация которого будет близка к 1.

99
00:06:24,500 --> 00:06:27,579
И я не имею в виду этот конкретный цикл пикселей, надежда на то, 

100
00:06:27,579 --> 00:06:31,560
что любой в целом закольцованный узор по направлению к вершине запустит этот нейрон.

101
00:06:32,440 --> 00:06:35,792
Таким образом, чтобы перейти от третьего слоя к последнему, 

102
00:06:35,792 --> 00:06:40,040
нужно просто выучить, какая комбинация слагаемых соответствует каким цифрам.

103
00:06:41,000 --> 00:06:43,213
Конечно, это лишь отбрасывает проблему на второй план, 

104
00:06:43,213 --> 00:06:45,869
ведь как ты сможешь распознать эти подкомпоненты или даже узнать, 

105
00:06:45,869 --> 00:06:47,640
какими должны быть правильные подкомпоненты?

106
00:06:48,060 --> 00:06:51,771
И я до сих пор даже не рассказал о том, как один слой влияет на другой, 

107
00:06:51,771 --> 00:06:53,060
но послушай меня немного.

108
00:06:53,680 --> 00:06:56,680
Распознавание цикла также может разбиваться на подпроблемы.

109
00:06:57,280 --> 00:06:59,887
Одним из разумных способов сделать это было бы сначала 

110
00:06:59,887 --> 00:07:02,780
распознать различные маленькие грани, из которых она состоит.

111
00:07:03,780 --> 00:07:07,591
Точно так же длинная линия, например, такая, какую ты можешь увидеть в цифрах 1, 

112
00:07:07,591 --> 00:07:10,979
4 или 7, на самом деле является просто длинным ребром, или, может быть, 

113
00:07:10,979 --> 00:07:14,320
ты думаешь о ней как об определенном узоре из нескольких меньших ребер.

114
00:07:15,140 --> 00:07:17,734
Так что, возможно, наша надежда заключается в том, 

115
00:07:17,734 --> 00:07:21,804
что каждый нейрон во втором слое сети соотносится с различными соответствующими 

116
00:07:21,804 --> 00:07:22,720
маленькими краями.

117
00:07:23,540 --> 00:07:28,264
Возможно, когда приходит изображение, подобное этому, оно зажигает все нейроны, 

118
00:07:28,264 --> 00:07:32,692
связанные с 8-10 определенными маленькими краями, которые, в свою очередь, 

119
00:07:32,692 --> 00:07:37,180
зажигают нейроны, связанные с верхней петлей и длинной вертикальной линией, 

120
00:07:37,180 --> 00:07:39,720
а те зажигают нейрон, связанный с цифрой 9.

121
00:07:40,680 --> 00:07:44,523
Так ли на самом деле работает наша конечная сеть - это другой вопрос, 

122
00:07:44,523 --> 00:07:48,531
к которому я вернусь, когда мы увидим, как обучать сеть, но это надежда, 

123
00:07:48,531 --> 00:07:52,540
которую мы можем иметь, своего рода цель с такой многослойной структурой.

124
00:07:53,160 --> 00:07:56,555
Более того, ты можешь представить, как способность обнаруживать края и узоры, 

125
00:07:56,555 --> 00:08:00,300
подобная этой, будет действительно полезна для других задач распознавания изображений.

126
00:08:00,880 --> 00:08:04,416
И даже помимо распознавания образов, существуют всевозможные интеллектуальные вещи, 

127
00:08:04,416 --> 00:08:07,280
которые ты можешь захотеть сделать, разбиваясь на уровни абстракции.

128
00:08:08,040 --> 00:08:11,251
Например, синтаксический разбор речи подразумевает взятие необработанного 

129
00:08:11,251 --> 00:08:14,418
звука и выделение из него отдельных звуков, которые в сочетании образуют 

130
00:08:14,418 --> 00:08:16,848
определенные слоги, которые в сочетании образуют слова, 

131
00:08:16,848 --> 00:08:20,060
которые в сочетании образуют фразы и более абстрактные мысли, и так далее.

132
00:08:21,100 --> 00:08:24,872
Но, возвращаясь к тому, как все это работает на самом деле, представь, 

133
00:08:24,872 --> 00:08:29,388
что ты прямо сейчас проектируешь, как именно активации в одном слое могут определять 

134
00:08:29,388 --> 00:08:29,920
следующий.

135
00:08:30,860 --> 00:08:33,812
Цель состоит в том, чтобы иметь некий механизм, 

136
00:08:33,812 --> 00:08:38,980
который мог бы мыслимо объединять пиксели в края, края в паттерны, паттерны в цифры.

137
00:08:39,440 --> 00:08:42,683
И чтобы увеличить один очень конкретный пример, скажем, 

138
00:08:42,683 --> 00:08:47,723
что надежда состоит в том, что один конкретный нейрон во втором слое будет определять, 

139
00:08:47,723 --> 00:08:50,620
есть ли у изображения край в этой области или нет.

140
00:08:51,440 --> 00:08:55,100
Вопрос в том, какими параметрами должна обладать сеть?

141
00:08:55,640 --> 00:08:58,113
Какие циферблаты и ручки ты должен уметь настраивать, 

142
00:08:58,113 --> 00:09:01,824
чтобы он был достаточно выразительным для потенциального захвата этого паттерна, 

143
00:09:01,824 --> 00:09:04,344
или любого другого пиксельного паттерна, или паттерна, 

144
00:09:04,344 --> 00:09:07,780
в котором несколько граней могут образовать петлю, и других подобных вещей?

145
00:09:08,720 --> 00:09:15,560
Что ж, мы назначим вес каждой связи между нашим нейроном и нейронами из первого слоя.

146
00:09:16,320 --> 00:09:17,700
Эти веса - всего лишь цифры.

147
00:09:18,540 --> 00:09:21,727
Затем возьми все эти активации из первого слоя и 

148
00:09:21,727 --> 00:09:25,500
вычисли их взвешенную сумму в соответствии с этими весами.

149
00:09:27,700 --> 00:09:32,197
Я считаю, что полезно думать об этих весах как об организованных в небольшую сетку, 

150
00:09:32,197 --> 00:09:36,479
и я собираюсь использовать зеленые пиксели для обозначения положительных весов, 

151
00:09:36,479 --> 00:09:41,030
а красные - для отрицательных, где яркость пикселя - это некое свободное отображение 

152
00:09:41,030 --> 00:09:41,780
значения веса.

153
00:09:42,780 --> 00:09:46,564
Теперь, если мы сделали веса, связанные почти со всеми пикселями, нулевыми, 

154
00:09:46,564 --> 00:09:50,648
за исключением некоторых положительных весов в этой области, которая нас волнует, 

155
00:09:50,648 --> 00:09:54,383
то взятие взвешенной суммы всех значений пикселей на самом деле сводится к 

156
00:09:54,383 --> 00:09:57,820
сложению значений пикселей только в той области, которая нас волнует.

157
00:09:59,140 --> 00:10:02,309
И если бы ты действительно хотел определить, есть ли здесь край, 

158
00:10:02,309 --> 00:10:06,600
то ты мог бы сделать так, чтобы с окружающими пикселями были связаны отрицательные веса.

159
00:10:07,480 --> 00:10:12,700
Тогда сумма будет наибольшей, когда эти средние пиксели яркие, а окружающие - темнее.

160
00:10:14,260 --> 00:10:18,958
Когда ты вычисляешь такую взвешенную сумму, у тебя может получиться любое число, 

161
00:10:18,958 --> 00:10:23,540
но для этой сети мы хотим, чтобы активации имели какое-то значение между 0 и 1.

162
00:10:24,120 --> 00:10:28,499
Поэтому обычное дело - закачать эту взвешенную сумму в какую-нибудь функцию, 

163
00:10:28,499 --> 00:10:32,140
которая сжимает линию вещественных чисел в диапазон между 0 и 1.

164
00:10:32,460 --> 00:10:34,427
И распространенная функция, которая это делает, 

165
00:10:34,427 --> 00:10:37,420
называется сигмоидной функцией, также известной как логистическая кривая.

166
00:10:38,000 --> 00:10:41,870
В основном очень отрицательные входы заканчиваются близко к 0, 

167
00:10:41,870 --> 00:10:46,600
положительные - близко к 1, и все это просто неуклонно растет вокруг входа 0.

168
00:10:49,120 --> 00:10:52,537
Так что активация нейрона здесь - это, по сути, мера того, 

169
00:10:52,537 --> 00:10:56,360
насколько положительной является соответствующая взвешенная сумма.

170
00:10:57,540 --> 00:11:00,491
Но, возможно, дело не в том, что ты хочешь, чтобы нейрон загорался, 

171
00:11:00,491 --> 00:11:01,880
когда взвешенная сумма больше 0.

172
00:11:02,280 --> 00:11:04,797
Может быть, ты хочешь, чтобы он был активен только тогда, 

173
00:11:04,797 --> 00:11:06,360
когда сумма больше, чем, скажем, 10.

174
00:11:06,840 --> 00:11:10,260
То есть тебе нужна некоторая предвзятость, чтобы она была неактивной.

175
00:11:11,380 --> 00:11:15,285
Затем мы просто добавим к этой взвешенной сумме какое-нибудь другое число, 

176
00:11:15,285 --> 00:11:19,660
например, отрицательные 10, а затем подключим ее к сигмоидной функции сквиширования.

177
00:11:20,580 --> 00:11:22,440
Это дополнительное число называется смещением.

178
00:11:23,460 --> 00:11:27,404
Таким образом, веса говорят тебе, какой пиксельный паттерн улавливает 

179
00:11:27,404 --> 00:11:32,419
этот нейрон во втором слое, а смещение - насколько высокой должна быть взвешенная сумма, 

180
00:11:32,419 --> 00:11:35,180
чтобы нейрон начал проявлять значимую активность.

181
00:11:36,120 --> 00:11:37,680
И это всего лишь один нейрон.

182
00:11:38,280 --> 00:11:44,610
Каждый другой нейрон в этом слое будет соединен со всеми 784 пиксельными нейронами из 

183
00:11:44,610 --> 00:11:50,940
первого слоя, и каждая из этих 784 связей имеет свой собственный вес, связанный с ней.

184
00:11:51,600 --> 00:11:54,492
Кроме того, у каждого из них есть смещение, какое-то другое число, 

185
00:11:54,492 --> 00:11:57,600
которое ты добавляешь к взвешенной сумме, прежде чем сжать ее сигмоидой.

186
00:11:58,110 --> 00:11:59,540
И об этом нужно много думать!

187
00:11:59,960 --> 00:12:06,407
При этом скрытый слой состоит из 16 нейронов, то есть всего 784 раза по 16 весов, 

188
00:12:06,407 --> 00:12:07,980
а также 16 смещений.

189
00:12:08,840 --> 00:12:11,940
И все это - лишь соединения от первого слоя ко второму.

190
00:12:12,520 --> 00:12:17,340
Связи между другими слоями также имеют кучу весов и смещений, связанных с ними.

191
00:12:18,340 --> 00:12:23,800
Все сказано и сделано, эта сеть имеет почти ровно 13 000 суммарных весов и смещений.

192
00:12:23,800 --> 00:12:27,485
13 000 ручек и циферблатов, которые можно настраивать и поворачивать, 

193
00:12:27,485 --> 00:12:29,960
чтобы заставить эту сеть вести себя по-разному.

194
00:12:31,040 --> 00:12:34,130
Поэтому, когда мы говорим об обучении, то имеем в виду, 

195
00:12:34,130 --> 00:12:38,655
что компьютер должен найти правильное значение для всех этих многих-многих чисел, 

196
00:12:38,655 --> 00:12:41,360
чтобы он действительно решил поставленную задачу.

197
00:12:42,620 --> 00:12:47,384
Один мысленный эксперимент, который одновременно забавен и ужасен, - это представить, 

198
00:12:47,384 --> 00:12:51,040
как ты садишься и устанавливаешь все эти веса и смещения вручную, 

199
00:12:51,040 --> 00:12:55,084
целенаправленно подстраивая числа так, чтобы второй слой улавливал края, 

200
00:12:55,084 --> 00:12:56,580
третий - узоры и так далее.

201
00:12:56,980 --> 00:13:01,757
Лично я нахожу это приятным, а не просто отношусь к сети как к абсолютному черному ящику, 

202
00:13:01,757 --> 00:13:04,783
потому что, когда сеть работает не так, как ты ожидаешь, 

203
00:13:04,783 --> 00:13:09,136
если ты немного разобрался в том, что на самом деле означают эти веса и смещения, 

204
00:13:09,136 --> 00:13:13,224
у тебя есть отправная точка для экспериментов с тем, как изменить структуру, 

205
00:13:13,224 --> 00:13:14,180
чтобы улучшить ее.

206
00:13:14,960 --> 00:13:18,237
Или когда сеть работает, но не по тем причинам, которые ты ожидал, 

207
00:13:18,237 --> 00:13:21,857
копание в том, что делают веса и смещения, - хороший способ бросить вызов 

208
00:13:21,857 --> 00:13:25,820
твоим предположениям и действительно раскрыть все пространство возможных решений.

209
00:13:26,840 --> 00:13:30,680
Кстати, фактическая функция здесь немного громоздкая, чтобы ее записать, тебе не кажется?

210
00:13:32,500 --> 00:13:34,798
Поэтому позволь мне показать тебе более компактный с 

211
00:13:34,798 --> 00:13:37,140
точки зрения нотации способ представления этих связей.

212
00:13:37,660 --> 00:13:40,520
Вот как ты это увидишь, если решишь почитать о нейронных сетях побольше.

213
00:13:41,380 --> 00:13:47,150
Организуй все активации из одного слоя в столбец, 

214
00:13:47,150 --> 00:13:54,999
так как матрица соответствует связям между одним слоем и конкретным 

215
00:13:54,999 --> 00:13:58,000
нейроном в следующем слое.

216
00:13:58,540 --> 00:14:04,021
Это значит, что взвешенная сумма активаций в первом слое в соответствии с этими весами 

217
00:14:04,021 --> 00:14:08,494
соответствует одному из членов в векторном произведении матрицы всего, 

218
00:14:08,494 --> 00:14:09,880
что у нас здесь слева.

219
00:14:14,000 --> 00:14:18,421
Кстати, очень многое в машинном обучении сводится к хорошему знанию линейной алгебры, 

220
00:14:18,421 --> 00:14:22,533
поэтому для тех, кто хочет получить наглядное представление о матрицах и о том, 

221
00:14:22,533 --> 00:14:25,772
что такое матрично-векторное умножение, посмотри серию статей, 

222
00:14:25,772 --> 00:14:28,600
которую я сделал по линейной алгебре, особенно главу 3.

223
00:14:29,240 --> 00:14:33,558
Возвращаясь к нашему выражению, вместо того чтобы говорить о добавлении смещения к 

224
00:14:33,558 --> 00:14:36,576
каждому из этих значений независимо, мы представляем это, 

225
00:14:36,576 --> 00:14:41,051
организуя все эти смещения в вектор, и добавляем весь вектор к предыдущему матричному 

226
00:14:41,051 --> 00:14:42,300
векторному произведению.

227
00:14:43,280 --> 00:14:47,623
Затем, в качестве последнего шага, я оберну сигмоиду вокруг внешней стороны здесь, 

228
00:14:47,623 --> 00:14:51,495
и это должно означать, что ты собираешься применить сигмоидальную функцию 

229
00:14:51,495 --> 00:14:54,740
к каждой конкретной компоненте результирующего вектора внутри.

230
00:14:55,940 --> 00:15:00,713
Таким образом, записав эту матрицу весов и эти векторы в виде собственных символов, 

231
00:15:00,713 --> 00:15:05,430
ты сможешь передать полный переход активаций от одного слоя к другому в виде очень 

232
00:15:05,430 --> 00:15:10,317
узкого и аккуратного маленького выражения, а это делает соответствующий код и намного 

233
00:15:10,317 --> 00:15:15,148
проще, и намного быстрее, так как многие библиотеки оптимизируют умножение матриц до 

234
00:15:15,148 --> 00:15:15,660
чертиков.

235
00:15:17,820 --> 00:15:21,460
Помнишь, ранее я говорил, что эти нейроны - просто вещи, в которых хранятся числа?

236
00:15:22,220 --> 00:15:26,779
Конечно, конкретные числа, которые они хранят, зависят от изображения, 

237
00:15:26,779 --> 00:15:32,110
которое ты вводишь, поэтому на самом деле правильнее думать о каждом нейроне как о 

238
00:15:32,110 --> 00:15:37,569
функции, которая принимает выходы всех нейронов предыдущего слоя и выплевывает число 

239
00:15:37,569 --> 00:15:38,340
между 0 и 1.

240
00:15:39,200 --> 00:15:43,021
На самом деле вся сеть - это просто функция, которая 

241
00:15:43,021 --> 00:15:47,060
принимает на вход 784 числа и выдает на выходе 10 чисел.

242
00:15:47,560 --> 00:15:52,402
Это абсурдно сложная функция, которая включает в себя 13 000 параметров в виде этих весов 

243
00:15:52,402 --> 00:15:55,469
и смещений, которые находят определенные закономерности, 

244
00:15:55,469 --> 00:15:59,611
и которая включает в себя итерации множества векторных произведений матриц и 

245
00:15:59,611 --> 00:16:03,431
сигмоидальной функции сглаживания, но тем не менее это просто функция, 

246
00:16:03,431 --> 00:16:06,660
и в некотором роде успокаивает то, что она выглядит сложной.

247
00:16:07,340 --> 00:16:09,664
Ведь если бы он был проще, какая надежда на то, 

248
00:16:09,664 --> 00:16:12,280
что он сможет справиться с задачей распознавания цифр?

249
00:16:13,340 --> 00:16:14,700
И как он справляется с этой задачей?

250
00:16:15,080 --> 00:16:19,360
Как эта сеть учится соответствующим весам и смещениям, просто глядя на данные?

251
00:16:20,140 --> 00:16:23,409
Именно это я покажу в следующем видео, а также немного подробнее разберусь, 

252
00:16:23,409 --> 00:16:26,120
что на самом деле делает эта конкретная сеть, которую мы видим.

253
00:16:27,580 --> 00:16:30,371
Сейчас я, наверное, должен сказать, что нужно подписаться, 

254
00:16:30,371 --> 00:16:33,540
чтобы получать уведомления о выходе видео или любых новых роликов, 

255
00:16:33,540 --> 00:16:37,420
но, если честно, большинство из вас не получают уведомлений от YouTube, не так ли?

256
00:16:38,020 --> 00:16:41,148
Может быть, честнее было бы сказать "подпишись", чтобы нейросети, 

257
00:16:41,148 --> 00:16:44,751
лежащие в основе рекомендательного алгоритма YouTube, были настроены на то, 

258
00:16:44,751 --> 00:16:47,880
что ты хочешь видеть контент с этого канала, рекомендованный тебе.

259
00:16:48,560 --> 00:16:49,940
В любом случае оставайся на связи, чтобы узнать больше.

260
00:16:50,760 --> 00:16:53,500
Большое спасибо всем, кто поддерживает эти видео на Patreon.

261
00:16:54,000 --> 00:16:57,131
Этим летом я немного замедлил продвижение в серии "Вероятность", 

262
00:16:57,131 --> 00:17:00,551
но после этого проекта я снова в нее погружаюсь, так что, покровители, 

263
00:17:00,551 --> 00:17:01,900
следите за обновлениями там.

264
00:17:03,600 --> 00:17:07,273
В заключение я приглашаю к себе Лейшу Ли, которая защитила докторскую диссертацию 

265
00:17:07,273 --> 00:17:10,857
по теоретической стороне глубокого обучения и сейчас работает в венчурной фирме 

266
00:17:10,857 --> 00:17:14,619
Amplify Partners, которая любезно предоставила часть финансирования для этого видео.

267
00:17:15,460 --> 00:17:18,185
Итак, Лейша, одна вещь, которую, я думаю, мы должны быстро упомянуть, 

268
00:17:18,185 --> 00:17:19,119
это сигмовидная функция.

269
00:17:19,700 --> 00:17:21,906
Как я понимаю, ранние сети используют это для того, 

270
00:17:21,906 --> 00:17:25,427
чтобы втиснуть соответствующую взвешенную сумму в интервал между нулем и единицей, 

271
00:17:25,427 --> 00:17:28,270
что, в общем-то, мотивировано биологической аналогией с нейронами, 

272
00:17:28,270 --> 00:17:29,840
которые либо неактивны, либо активны.

273
00:17:30,280 --> 00:17:30,300
Именно так.

274
00:17:30,560 --> 00:17:34,040
Но сравнительно немногие современные сети на самом деле используют сигмоид.

275
00:17:34,320 --> 00:17:34,320
Да.

276
00:17:34,440 --> 00:17:35,540
Это вроде как старая школа, верно?

277
00:17:35,760 --> 00:17:38,980
Да, или, скорее, Релу кажется намного проще в обучении.

278
00:17:39,400 --> 00:17:42,340
А relu расшифровывается как rectified linear unit?

279
00:17:42,680 --> 00:17:47,589
Да, это такая функция, в которой ты просто берешь максимум из нуля и a, 

280
00:17:47,589 --> 00:17:53,521
где a задается тем, что ты объяснял в видео, а мотивировано это было, как мне кажется, 

281
00:17:53,521 --> 00:17:59,044
частично биологической аналогией с тем, как нейроны либо активируются, либо нет, 

282
00:17:59,044 --> 00:18:04,771
и если они проходят определенный порог, то это будет функция тождества, а если нет, 

283
00:18:04,771 --> 00:18:10,840
то они просто не активируются, так что это будет ноль, так что это своего рода упрощение.

284
00:18:11,160 --> 00:18:16,891
Использование сигмоидов не помогало в обучении или в какой-то момент было очень сложным, 

285
00:18:16,891 --> 00:18:21,142
и люди просто попробовали relu, и оказалось, что это очень хорошо 

286
00:18:21,142 --> 00:18:24,620
работает для этих невероятно глубоких нейронных сетей.

287
00:18:25,100 --> 00:18:25,640
Хорошо, спасибо тебе, Алисия.

