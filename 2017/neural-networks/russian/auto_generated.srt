1
00:00:04,220 --> 00:00:05,400
Это 3.

2
00:00:06,060 --> 00:00:11,166
Он небрежно написан и визуализирован с чрезвычайно низким разрешением 28x28 пикселей, 

3
00:00:11,166 --> 00:00:13,720
но ваш мозг без труда распознает его как 3.

4
00:00:14,340 --> 00:00:17,245
И я хочу, чтобы вы на минутку оценили, насколько безумно то, 

5
00:00:17,245 --> 00:00:18,960
что мозг может делать это так легко.

6
00:00:19,700 --> 00:00:23,507
Я имею в виду, что это, это и это также можно распознать как 3 секунды, 

7
00:00:23,507 --> 00:00:27,791
хотя конкретные значения каждого пикселя сильно отличаются от одного изображения 

8
00:00:27,791 --> 00:00:28,320
к другому.

9
00:00:28,900 --> 00:00:32,215
Конкретные светочувствительные клетки вашего глаза, которые срабатывают, 

10
00:00:32,215 --> 00:00:35,668
когда вы видите эту цифру 3, сильно отличаются от тех, которые срабатывают, 

11
00:00:35,668 --> 00:00:36,940
когда вы видите эту цифру 3.

12
00:00:37,520 --> 00:00:42,380
Но что-то в вашей безумно умной зрительной коре воспринимает эти изображения как 

13
00:00:42,380 --> 00:00:47,360
представляющие одну и ту же идею, в то же время распознавая другие изображения как 

14
00:00:47,360 --> 00:00:48,260
отдельные идеи.

15
00:00:49,220 --> 00:00:53,085
Но если бы я сказал вам, эй, сядьте и напишите для меня программу, 

16
00:00:53,085 --> 00:00:58,219
которая принимает сетку размером 28x28 пикселей вот так и выводит одно число от 0 до 10, 

17
00:00:58,219 --> 00:01:02,026
сообщая вам, что, по ее мнению, представляет собой эта цифра, ну, 

18
00:01:02,026 --> 00:01:06,180
задача будет выглядеть так: от комично тривиального до пугающе сложного.

19
00:01:07,160 --> 00:01:10,998
Если вы не жили под камнем, я думаю, мне вряд ли нужно объяснять актуальность 

20
00:01:10,998 --> 00:01:14,640
и важность машинного обучения и нейронных сетей для настоящего и будущего.

21
00:01:15,120 --> 00:01:18,901
Но здесь я хочу показать вам, что на самом деле представляет собой нейронная сеть, 

22
00:01:18,901 --> 00:01:22,318
не предполагая никакого фона, и помочь визуализировать то, что она делает, 

23
00:01:22,318 --> 00:01:24,460
не как модное словечко, а как часть математики.

24
00:01:25,020 --> 00:01:28,635
Я надеюсь, что вы уйдете с ощущением, что сама структура мотивирована, 

25
00:01:28,635 --> 00:01:31,691
и почувствуете, что знаете, что она означает, когда читаете 

26
00:01:31,691 --> 00:01:34,340
или слышите об обучении цитированием нейронной сети.

27
00:01:35,360 --> 00:01:38,499
Это видео будет посвящено структурному компоненту этого процесса, 

28
00:01:38,499 --> 00:01:40,260
а следующее будет посвящено обучению.

29
00:01:40,960 --> 00:01:43,589
Что мы собираемся сделать, так это собрать нейронную сеть, 

30
00:01:43,589 --> 00:01:46,040
которая сможет научиться распознавать рукописные цифры.

31
00:01:49,360 --> 00:01:51,814
Это своего рода классический пример представления темы, 

32
00:01:51,814 --> 00:01:54,444
и я рад придерживаться здесь существующего положения вещей, 

33
00:01:54,444 --> 00:01:58,039
потому что в конце двух видеороликов я хочу указать вам на пару хороших ресурсов, 

34
00:01:58,039 --> 00:02:01,458
где вы можете узнать больше, и где вы можете скачать код, который делает это, 

35
00:02:01,458 --> 00:02:03,080
и поиграть с ним на своем компьютере.

36
00:02:05,040 --> 00:02:07,610
Существует множество вариантов нейронных сетей, 

37
00:02:07,610 --> 00:02:11,681
и в последние годы наблюдается своего рода бум исследований этих вариантов, 

38
00:02:11,681 --> 00:02:15,752
но в этих двух вводных видеороликах мы с вами просто рассмотрим простейшую, 

39
00:02:15,752 --> 00:02:19,180
простую ванильную форму без каких-либо дополнительных излишеств.

40
00:02:19,860 --> 00:02:23,914
Это своего рода необходимая предпосылка для понимания любого из более мощных 

41
00:02:23,914 --> 00:02:28,600
современных вариантов, и, поверьте мне, нам еще предстоит осмыслить множество сложностей.

42
00:02:29,120 --> 00:02:34,396
Но даже в этой простейшей форме он может научиться распознавать рукописные цифры, 

43
00:02:34,396 --> 00:02:36,520
что очень здорово для компьютера.

44
00:02:37,480 --> 00:02:40,474
И в то же время вы увидите, что он не оправдывает пары надежд, 

45
00:02:40,474 --> 00:02:42,280
которые мы могли бы на него возлагать.

46
00:02:43,380 --> 00:02:46,953
Как следует из названия, нейронные сети созданы по принципу мозга, 

47
00:02:46,953 --> 00:02:48,500
но давайте разберемся в этом.

48
00:02:48,520 --> 00:02:51,660
Что такое нейроны и в каком смысле они связаны между собой?

49
00:02:52,500 --> 00:02:56,867
Прямо сейчас, когда я говорю «нейрон», все, о чем я хочу, чтобы вы подумали, 

50
00:02:56,867 --> 00:03:00,440
— это о вещи, которая содержит число, особенно число от 0 до 1.

51
00:03:00,680 --> 00:03:02,560
На самом деле это не более того.

52
00:03:03,780 --> 00:03:08,967
Например, сеть начинается с группы нейронов, соответствующей каждому из пикселей 

53
00:03:08,967 --> 00:03:14,220
входного изображения размером 28x28, что в общей сложности составляет 784 нейрона.

54
00:03:14,700 --> 00:03:19,144
Каждый из них содержит число, которое представляет значение шкалы серого 

55
00:03:19,144 --> 00:03:24,380
соответствующего пикселя в диапазоне от 0 для черных пикселей до 1 для белых пикселей.

56
00:03:25,300 --> 00:03:29,289
Это число внутри нейрона называется его активацией, и вы, возможно, 

57
00:03:29,289 --> 00:03:34,160
имеете в виду, что каждый нейрон светится, когда его активация имеет большое число.

58
00:03:36,720 --> 00:03:41,860
Итак, все эти 784 нейрона составляют первый слой нашей сети.

59
00:03:46,500 --> 00:03:49,177
Теперь перейдем к последнему слою: здесь 10 нейронов, 

60
00:03:49,177 --> 00:03:51,360
каждый из которых представляет одну из цифр.

61
00:03:52,040 --> 00:03:56,945
Активация этих нейронов (опять же некоторое число от 0 до 1) показывает, 

62
00:03:56,945 --> 00:04:02,120
насколько система считает, что данное изображение соответствует данной цифре.

63
00:04:03,040 --> 00:04:06,262
Есть также пара промежуточных слоев, называемых скрытыми слоями, 

64
00:04:06,262 --> 00:04:10,079
которые на данный момент должны быть просто гигантским знаком вопроса о том, 

65
00:04:10,079 --> 00:04:13,600
как, черт возьми, будет осуществляться этот процесс распознавания цифр.

66
00:04:14,260 --> 00:04:18,118
В этой сети я выбрал два скрытых слоя, каждый из которых содержит 16 нейронов, 

67
00:04:18,118 --> 00:04:20,560
и, надо признать, это довольно произвольный выбор.

68
00:04:21,019 --> 00:04:23,131
Честно говоря, я выбрал два слоя, исходя из того, 

69
00:04:23,131 --> 00:04:25,708
как я хочу мотивировать структуру в данный момент, и 16, ну, 

70
00:04:25,708 --> 00:04:28,200
это было просто хорошее число, чтобы поместиться на экране.

71
00:04:28,780 --> 00:04:32,340
На практике здесь есть много возможностей для экспериментов с конкретной структурой.

72
00:04:33,020 --> 00:04:35,673
В зависимости от того, как работает сеть, активации 

73
00:04:35,673 --> 00:04:38,480
на одном уровне определяют активации следующего уровня.

74
00:04:39,200 --> 00:04:43,920
И, конечно же, суть сети как механизма обработки информации сводится к тому, 

75
00:04:43,920 --> 00:04:48,580
как именно активации на одном уровне вызывают активации на следующем уровне.

76
00:04:49,140 --> 00:04:53,249
Это должно быть во многом аналогично тому, как в биологических сетях 

77
00:04:53,249 --> 00:04:57,180
нейронов активация одних групп нейронов вызывает активацию других.

78
00:04:58,120 --> 00:05:01,157
Сеть, которую я здесь показываю, уже обучена распознавать цифры, 

79
00:05:01,157 --> 00:05:03,400
и позвольте мне показать вам, что я имею в виду.

80
00:05:03,640 --> 00:05:06,242
Это означает, что если вы подаете изображение, 

81
00:05:06,242 --> 00:05:10,672
освещая все 784 нейрона входного слоя в соответствии с яркостью каждого пикселя 

82
00:05:10,672 --> 00:05:15,268
изображения, этот шаблон активаций вызывает некоторый очень специфический шаблон в 

83
00:05:15,268 --> 00:05:20,086
следующем слое, который вызывает некоторый шаблон в следующем слое. это, что, наконец, 

84
00:05:20,086 --> 00:05:22,080
дает некоторый узор в выходном слое.

85
00:05:22,560 --> 00:05:26,247
И самый яркий нейрон этого выходного слоя — это, так сказать, 

86
00:05:26,247 --> 00:05:29,400
выбор сети, какую цифру представляет это изображение.

87
00:05:32,560 --> 00:05:35,117
И прежде чем приступить к математическим расчетам того, 

88
00:05:35,117 --> 00:05:37,902
как один слой влияет на следующий или как работает обучение, 

89
00:05:37,902 --> 00:05:40,780
давайте просто поговорим о том, почему вообще разумно ожидать, 

90
00:05:40,780 --> 00:05:43,520
что такая многоуровневая структура будет вести себя разумно.

91
00:05:44,060 --> 00:05:45,220
Чего мы здесь ожидаем?

92
00:05:45,400 --> 00:05:47,600
Какова наилучшая надежда для этих средних слоев?

93
00:05:48,920 --> 00:05:53,520
Что ж, когда мы с вами распознаем цифры, мы собираем воедино различные компоненты.

94
00:05:54,200 --> 00:05:56,820
У цифры 9 есть петля вверху и линия справа.

95
00:05:57,380 --> 00:06:01,180
У цифры 8 также есть верхняя петля, но она соединена с другой нижней петлей.

96
00:06:01,980 --> 00:06:06,820
4 по сути разбивается на три конкретные линии и тому подобное.

97
00:06:07,600 --> 00:06:10,225
Теперь, в идеальном мире, мы могли бы надеяться, 

98
00:06:10,225 --> 00:06:14,672
что каждый нейрон предпоследнего слоя соответствует одному из этих подкомпонентов, 

99
00:06:14,672 --> 00:06:18,583
и что каждый раз, когда вы подаете изображение, скажем, с петлей вверху, 

100
00:06:18,583 --> 00:06:21,851
например, с 9 или 8, происходит некоторая конкретный нейрон, 

101
00:06:21,851 --> 00:06:23,780
активация которого будет близка к 1.

102
00:06:24,500 --> 00:06:28,030
И я не имею в виду эту конкретную петлю пикселей, я надеюсь, 

103
00:06:28,030 --> 00:06:31,560
что любой вообще петлевой узор вверху активирует этот нейрон.

104
00:06:32,440 --> 00:06:36,947
Таким образом, для перехода от третьего уровня к последнему необходимо просто узнать, 

105
00:06:36,947 --> 00:06:40,040
какая комбинация подкомпонентов соответствует каким цифрам.

106
00:06:41,000 --> 00:06:43,131
Конечно, это только отбрасывает проблему в сторону, 

107
00:06:43,131 --> 00:06:45,836
потому что как вы распознаете эти подкомпоненты или даже узнаете, 

108
00:06:45,836 --> 00:06:47,640
какими должны быть правильные подкомпоненты?

109
00:06:48,060 --> 00:06:51,144
И я еще даже не говорил о том, как один слой влияет на следующий, 

110
00:06:51,144 --> 00:06:53,060
но пробежимся на минутку по этому поводу.

111
00:06:53,680 --> 00:06:56,680
Распознавание цикла также может быть разбито на подзадачи.

112
00:06:57,280 --> 00:07:01,659
Один из разумных способов сделать это — сначала распознать различные небольшие грани, 

113
00:07:01,659 --> 00:07:02,780
из которых он состоит.

114
00:07:03,780 --> 00:07:07,384
Точно так же длинная линия, подобная той, которую вы можете видеть в цифрах 1, 

115
00:07:07,384 --> 00:07:11,080
4 или 7, на самом деле представляет собой просто длинное ребро, или, может быть, 

116
00:07:11,080 --> 00:07:14,320
вы думаете о ней как об определенном узоре из нескольких меньших ребер.

117
00:07:15,140 --> 00:07:18,865
Так что, возможно, мы надеемся, что каждый нейрон второго 

118
00:07:18,865 --> 00:07:22,720
слоя сети соответствует различным значимым маленьким ребрам.

119
00:07:23,540 --> 00:07:27,952
Возможно, когда появляется такое изображение, оно освещает все нейроны, 

120
00:07:27,952 --> 00:07:32,794
связанные примерно с 8–10 конкретными маленькими ребрами, что, в свою очередь, 

121
00:07:32,794 --> 00:07:37,452
освещает нейроны, связанные с верхней петлей и длинной вертикальной линией, 

122
00:07:37,452 --> 00:07:39,720
а они освещают нейрон, связанный с 9.

123
00:07:40,680 --> 00:07:44,103
Является ли это тем, что на самом деле делает наша окончательная сеть, 

124
00:07:44,103 --> 00:07:48,104
— это другой вопрос, к которому я вернусь, как только мы увидим, как обучать сеть, 

125
00:07:48,104 --> 00:07:52,347
но это надежда, которая у нас может быть, своего рода цель с многоуровневой структурой. 

126
00:07:52,347 --> 00:07:52,540
так.

127
00:07:53,160 --> 00:07:56,626
Более того, вы можете себе представить, как возможность обнаружения краев и узоров, 

128
00:07:56,626 --> 00:08:00,300
подобная этой, может оказаться очень полезной для других задач распознавания изображений.

129
00:08:00,880 --> 00:08:03,274
И даже помимо распознавания изображений, вы, возможно, 

130
00:08:03,274 --> 00:08:05,494
захотите сделать множество интеллектуальных вещей, 

131
00:08:05,494 --> 00:08:07,280
которые разбиваются на уровни абстракции.

132
00:08:08,040 --> 00:08:12,234
Например, анализ речи включает в себя получение необработанного аудио и выделение 

133
00:08:12,234 --> 00:08:15,354
отдельных звуков, которые объединяются в определенные слоги, 

134
00:08:15,354 --> 00:08:19,957
которые объединяются в слова, которые объединяются в фразы и более абстрактные мысли и т. 

135
00:08:19,957 --> 00:08:20,060
д.

136
00:08:21,100 --> 00:08:24,936
Но возвращаясь к тому, как все это на самом деле работает, представьте себе, 

137
00:08:24,936 --> 00:08:29,421
как прямо сейчас вы разрабатываете, как именно активации на одном уровне могут определять 

138
00:08:29,421 --> 00:08:29,920
следующий.

139
00:08:30,860 --> 00:08:34,160
Цель состоит в том, чтобы создать некий механизм, 

140
00:08:34,160 --> 00:08:38,980
который мог бы объединять пиксели в края, края в узоры или узоры в цифры.

141
00:08:39,440 --> 00:08:44,527
И если приблизить один очень конкретный пример, скажем, есть надежда на то, 

142
00:08:44,527 --> 00:08:47,875
что один конкретный нейрон во втором слое поймет, 

143
00:08:47,875 --> 00:08:50,620
имеет ли изображение край в этой области.

144
00:08:51,440 --> 00:08:55,100
Возникает вопрос: какие параметры должна иметь сеть?

145
00:08:55,640 --> 00:08:59,965
Какие циферблаты и ручки вы должны настроить, чтобы они были достаточно выразительными, 

146
00:08:59,965 --> 00:09:04,093
чтобы потенциально захватить этот узор, или любой другой пиксельный узор, или узор, 

147
00:09:04,093 --> 00:09:07,780
в котором несколько краев могут образовывать петлю, и другие подобные вещи?

148
00:09:08,720 --> 00:09:15,560
Что ж, мы присвоим вес каждой связи между нашим нейроном и нейронами первого слоя.

149
00:09:16,320 --> 00:09:17,700
Эти веса являются просто числами.

150
00:09:18,540 --> 00:09:21,737
Затем возьмите все эти активации из первого слоя и 

151
00:09:21,737 --> 00:09:25,500
вычислите их взвешенную сумму в соответствии с этими весами.

152
00:09:27,700 --> 00:09:31,243
Я считаю полезным представить эти веса как организованные в небольшую сетку, 

153
00:09:31,243 --> 00:09:34,601
и я собираюсь использовать зеленые пиксели для обозначения положительных 

154
00:09:34,601 --> 00:09:37,408
весов и красные пиксели для обозначения отрицательных весов, 

155
00:09:37,408 --> 00:09:40,031
где яркость этого пикселя составляет некоторую величину. 

156
00:09:40,031 --> 00:09:41,780
свободное представление значения веса.

157
00:09:42,780 --> 00:09:46,750
Теперь, если мы сделали веса, связанные почти со всеми пикселями, равными нулю, 

158
00:09:46,750 --> 00:09:50,970
за исключением некоторых положительных весов в этой области, которые нас интересуют, 

159
00:09:50,970 --> 00:09:54,593
тогда взвешенная сумма всех значений пикселей на самом деле будет просто 

160
00:09:54,593 --> 00:09:57,820
складывать значения пикселя только в регион, который нас волнует.

161
00:09:59,140 --> 00:10:02,424
И если вы действительно хотите понять, есть ли здесь край, 

162
00:10:02,424 --> 00:10:06,600
вы можете установить отрицательные веса, связанные с окружающими пикселями.

163
00:10:07,480 --> 00:10:12,700
Тогда сумма будет наибольшей, если средние пиксели яркие, а окружающие пиксели темнее.

164
00:10:14,260 --> 00:10:19,099
Когда вы вычисляете взвешенную сумму, подобную этой, вы можете получить любое число, 

165
00:10:19,099 --> 00:10:23,540
но для этой сети мы хотим, чтобы активации имели некоторое значение от 0 до 1.

166
00:10:24,120 --> 00:10:28,550
Поэтому обычно приходится закачивать эту взвешенную сумму в некоторую функцию, 

167
00:10:28,550 --> 00:10:32,140
которая сжимает линию действительных чисел в диапазон от 0 до 1.

168
00:10:32,460 --> 00:10:35,620
И обычная функция, которая делает это, называется сигмовидной функцией, 

169
00:10:35,620 --> 00:10:37,420
также известной как логистическая кривая.

170
00:10:38,000 --> 00:10:41,440
По сути, очень отрицательные входные данные приближаются к 0, 

171
00:10:41,440 --> 00:10:45,490
положительные входные данные — к 1, и они постепенно увеличиваются около 

172
00:10:45,490 --> 00:10:46,600
входного значения 0.

173
00:10:49,120 --> 00:10:53,142
Таким образом, активация нейрона здесь, по сути, является мерой того, 

174
00:10:53,142 --> 00:10:56,360
насколько положительна соответствующая взвешенная сумма.

175
00:10:57,540 --> 00:11:00,491
Но, возможно, дело не в том, что вы хотите, чтобы нейрон загорался, 

176
00:11:00,491 --> 00:11:01,880
когда взвешенная сумма больше 0.

177
00:11:02,280 --> 00:11:06,360
Возможно, вы хотите, чтобы он был активен только тогда, когда сумма больше, скажем, 10.

178
00:11:06,840 --> 00:11:10,260
То есть вам нужна некоторая предвзятость, чтобы он был неактивным.

179
00:11:11,380 --> 00:11:15,025
Затем мы просто добавим какое-нибудь другое число, например минус 10, 

180
00:11:15,025 --> 00:11:19,660
к этой взвешенной сумме, прежде чем подключить ее через функцию сжатия сигмовидной кишки.

181
00:11:20,580 --> 00:11:22,440
Это дополнительное число называется смещением.

182
00:11:23,460 --> 00:11:27,242
Таким образом, веса говорят вам, какой шаблон пикселей улавливает этот 

183
00:11:27,242 --> 00:11:29,746
нейрон во втором слое, а смещение говорит вам, 

184
00:11:29,746 --> 00:11:32,303
насколько высокой должна быть взвешенная сумма, 

185
00:11:32,303 --> 00:11:35,180
прежде чем нейрон начнет становиться значимо активным.

186
00:11:36,120 --> 00:11:37,680
И это всего лишь один нейрон.

187
00:11:38,280 --> 00:11:44,532
Каждый второй нейрон в этом слое будет связан со всеми 784 пиксельными нейронами 

188
00:11:44,532 --> 00:11:50,940
первого слоя, и каждому из этих 784 соединений будет присвоен свой собственный вес.

189
00:11:51,600 --> 00:11:54,716
Кроме того, у каждого из них есть некоторая погрешность, какое-то другое число, 

190
00:11:54,716 --> 00:11:57,600
которое вы добавляете к взвешенной сумме, прежде чем сжимать ее сигмоидой.

191
00:11:58,110 --> 00:11:59,540
И это есть над чем подумать!

192
00:11:59,960 --> 00:12:06,376
С этим скрытым слоем из 16 нейронов это в общей сложности 784 раза по 16 весов, 

193
00:12:06,376 --> 00:12:07,980
а также 16 смещений.

194
00:12:08,840 --> 00:12:11,940
И все это — лишь связи первого слоя со вторым.

195
00:12:12,520 --> 00:12:17,340
Связи между другими слоями также имеют множество весов и смещений, связанных с ними.

196
00:12:18,340 --> 00:12:23,800
В целом эта сеть имеет почти ровно 13 000 весов и смещений.

197
00:12:23,800 --> 00:12:27,375
13 000 ручек и дисков, которые можно настраивать и поворачивать, 

198
00:12:27,375 --> 00:12:29,960
чтобы заставить эту сеть вести себя по-разному.

199
00:12:31,040 --> 00:12:36,229
Итак, когда мы говорим об обучении, имеется в виду заставить компьютер найти допустимую 

200
00:12:36,229 --> 00:12:41,360
настройку для всех этих многих чисел, чтобы он действительно решил поставленную задачу.

201
00:12:42,620 --> 00:12:46,205
Один мысленный эксперимент, который одновременно забавен и отчасти ужасен, 

202
00:12:46,205 --> 00:12:49,791
состоит в том, чтобы представить, что вы садитесь и вручную устанавливаете 

203
00:12:49,791 --> 00:12:52,803
все эти веса и смещения, целенаправленно настраивая числа так, 

204
00:12:52,803 --> 00:12:56,580
чтобы второй слой улавливал края, третий слой улавливал закономерности, и т. д.

205
00:12:56,980 --> 00:13:01,357
Лично меня это удовлетворяет, а не просто рассматривать сеть как полный черный ящик, 

206
00:13:01,357 --> 00:13:04,292
потому что, когда сеть не работает так, как вы ожидаете, 

207
00:13:04,292 --> 00:13:06,867
если вы выстроили хоть какое-то отношение к тому, 

208
00:13:06,867 --> 00:13:09,596
что на самом деле означают эти веса и предвзятости , 

209
00:13:09,596 --> 00:13:14,180
у вас есть отправная точка для экспериментов с тем, как изменить структуру для улучшения.

210
00:13:14,960 --> 00:13:18,440
Или, когда сеть работает, но не по тем причинам, которые вы могли ожидать, 

211
00:13:18,440 --> 00:13:22,060
изучение того, что делают веса и смещения, — это хороший способ бросить вызов 

212
00:13:22,060 --> 00:13:25,820
вашим предположениям и по-настоящему раскрыть все пространство возможных решений.

213
00:13:26,840 --> 00:13:30,680
Кстати, записывать эту функцию немного громоздко, вам не кажется?

214
00:13:32,500 --> 00:13:37,140
Итак, позвольте мне показать вам более компактный способ представления этих связей.

215
00:13:37,660 --> 00:13:40,520
Вот как вы это увидите, если решите узнать больше о нейронных сетях.

216
00:13:41,380 --> 00:13:46,996
Организуйте все активации одного слоя в столбце, 

217
00:13:46,996 --> 00:13:55,019
поскольку матрица соответствует связям между одним слоем и конкретным 

218
00:13:55,019 --> 00:13:58,000
нейроном в следующем слое.

219
00:13:58,540 --> 00:14:02,358
Это означает, что взятие взвешенной суммы активаций в первом слое 

220
00:14:02,358 --> 00:14:05,887
в соответствии с этими весами соответствует одному из членов 

221
00:14:05,887 --> 00:14:09,880
матричного векторного произведения всего, что у нас есть здесь слева.

222
00:14:14,000 --> 00:14:17,698
Между прочим, большая часть машинного обучения сводится к хорошему пониманию 

223
00:14:17,698 --> 00:14:21,540
линейной алгебры, поэтому для тех из вас, кто хочет получить хорошее визуальное 

224
00:14:21,540 --> 00:14:25,142
представление о матрицах и о том, что означает умножение матриц на вектор, 

225
00:14:25,142 --> 00:14:28,600
взгляните на серию, которую я делал. линейная алгебра, особенно глава 3.

226
00:14:29,240 --> 00:14:33,629
Возвращаясь к нашему выражению, вместо того, чтобы говорить о добавлении смещения 

227
00:14:33,629 --> 00:14:36,840
к каждому из этих значений независимо, мы представляем его, 

228
00:14:36,840 --> 00:14:41,443
организуя все эти смещения в вектор и добавляя весь вектор к предыдущему произведению 

229
00:14:41,443 --> 00:14:42,300
матрицы-вектора.

230
00:14:43,280 --> 00:14:47,100
Затем, в качестве последнего шага, я оберну здесь сигмоиду снаружи, 

231
00:14:47,100 --> 00:14:50,695
и это должно означать, что вы собираетесь применить сигмовидную 

232
00:14:50,695 --> 00:14:54,740
функцию к каждому конкретному компоненту результирующего вектора внутри.

233
00:14:55,940 --> 00:15:00,757
Итак, как только вы запишете эту весовую матрицу и эти векторы как отдельные символы, 

234
00:15:00,757 --> 00:15:05,463
вы сможете передать полный переход активаций от одного слоя к другому в чрезвычайно 

235
00:15:05,463 --> 00:15:10,393
сжатом и аккуратном маленьком выражении, и это делает соответствующий код намного проще 

236
00:15:10,393 --> 00:15:15,267
и проще. намного быстрее, поскольку многие библиотеки чертовски оптимизируют умножение 

237
00:15:15,267 --> 00:15:15,660
матриц.

238
00:15:17,820 --> 00:15:21,460
Помните, как ранее я говорил, что эти нейроны — это просто вещи, хранящие числа?

239
00:15:22,220 --> 00:15:26,957
Ну, конечно, конкретные числа, которые они содержат, зависят от изображения, 

240
00:15:26,957 --> 00:15:32,064
которое вы вводите, поэтому на самом деле правильнее думать о каждом нейроне как о 

241
00:15:32,064 --> 00:15:37,601
функции, которая принимает выходные данные всех нейронов предыдущего слоя и выдает число. 

242
00:15:37,601 --> 00:15:38,340
между 0 и 1.

243
00:15:39,200 --> 00:15:43,021
На самом деле вся сеть — это просто функция, которая 

244
00:15:43,021 --> 00:15:47,060
принимает 784 числа на вход и выдаёт 10 чисел на выходе.

245
00:15:47,560 --> 00:15:52,420
Это абсурдно сложная функция, которая включает в себя 13 000 параметров в виде весов 

246
00:15:52,420 --> 00:15:55,451
и смещений, которые улавливают определенные шаблоны, 

247
00:15:55,451 --> 00:16:00,026
и которая включает в себя итерацию множества матричных векторных произведений и 

248
00:16:00,026 --> 00:16:04,486
сигмовидной функции сжатия, но, тем не менее, это всего лишь функция, и в то, 

249
00:16:04,486 --> 00:16:06,660
что это выглядит сложным, успокаивает.

250
00:16:07,340 --> 00:16:10,093
Я имею в виду, если бы все было проще, какая у нас была бы надежда, 

251
00:16:10,093 --> 00:16:12,280
что он сможет справиться с задачей распознавания цифр?

252
00:16:13,340 --> 00:16:14,700
И как он справляется с этой задачей?

253
00:16:15,080 --> 00:16:19,360
Как эта сеть изучает соответствующие веса и смещения, просто просматривая данные?

254
00:16:20,140 --> 00:16:23,466
Что ж, это то, что я покажу в следующем видео, а также немного углублюсь в то, 

255
00:16:23,466 --> 00:16:26,120
что на самом деле делает эта конкретная сеть, которую мы видим.

256
00:16:27,580 --> 00:16:30,591
Теперь настал момент, я полагаю, я должен сказать, что подписывайтесь, 

257
00:16:30,591 --> 00:16:33,348
чтобы получать уведомления о выходе видео или любых новых видео, 

258
00:16:33,348 --> 00:16:36,995
но на самом деле большинство из вас на самом деле не получают уведомления от YouTube, 

259
00:16:36,995 --> 00:16:37,420
не так ли?

260
00:16:38,020 --> 00:16:41,664
Может быть, более честно я должен сказать, подпишитесь, чтобы нейронные сети, 

261
00:16:41,664 --> 00:16:44,982
лежащие в основе алгоритма рекомендаций YouTube, были готовы поверить, 

262
00:16:44,982 --> 00:16:47,880
что вы хотите, чтобы вам рекомендовали контент с этого канала.

263
00:16:48,560 --> 00:16:49,940
В любом случае оставайтесь в курсе, чтобы узнать больше.

264
00:16:50,760 --> 00:16:53,500
Большое спасибо всем, кто поддерживает эти видео на Patreon.

265
00:16:54,000 --> 00:16:57,027
Этим летом я немного медленно продвигался в серии вероятностей, 

266
00:16:57,027 --> 00:17:00,102
но я возвращаюсь к ней после этого проекта, так что, посетители, 

267
00:17:00,102 --> 00:17:01,900
вы можете следить за обновлениями там.

268
00:17:03,600 --> 00:17:07,128
В заключение, со мной здесь Лейша Ли, которая защитила докторскую диссертацию по 

269
00:17:07,128 --> 00:17:10,699
теоретической стороне глубокого обучения и в настоящее время работает в венчурной 

270
00:17:10,699 --> 00:17:14,619
фирме Amplify Partners, которая любезно предоставила часть финансирования для этого видео.

271
00:17:15,460 --> 00:17:19,119
Итак, Лейша, я думаю, нам следует быстро вспомнить об этой сигмовидной функции.

272
00:17:19,700 --> 00:17:21,787
Насколько я понимаю, ранние сети используют это, 

273
00:17:21,787 --> 00:17:25,196
чтобы сжать соответствующую взвешенную сумму в интервал между нулем и единицей, 

274
00:17:25,196 --> 00:17:28,007
вы знаете, это отчасти мотивировано этой биологической аналогией, 

275
00:17:28,007 --> 00:17:29,840
когда нейроны либо неактивны, либо активны.

276
00:17:30,280 --> 00:17:30,300
Точно.

277
00:17:30,560 --> 00:17:34,040
Но относительно немногие современные сети действительно используют сигмовидную форму.

278
00:17:34,320 --> 00:17:34,320
Ага.

279
00:17:34,440 --> 00:17:35,540
Это что-то вроде старой школы, да?

280
00:17:35,760 --> 00:17:38,980
Да, или, скорее, релу, кажется, гораздо легче тренировать.

281
00:17:39,400 --> 00:17:42,340
А relu означает выпрямленную линейную единицу?

282
00:17:42,680 --> 00:17:47,100
Да, это такая функция, в которой вы просто берете максимум, равный нулю, 

283
00:17:47,100 --> 00:17:52,187
а где a определяется тем, что вы объясняли в видео, и это было как бы мотивировано, 

284
00:17:52,187 --> 00:17:57,395
я думаю, частично биологической аналогией с тем, как нейроны будет либо активирована, 

285
00:17:57,395 --> 00:18:01,089
либо нет, и поэтому, если она преодолеет определенный порог, 

286
00:18:01,089 --> 00:18:04,481
это будет функция идентичности, но если бы это не было, 

287
00:18:04,481 --> 00:18:08,780
то она просто не была бы активирована, поэтому она была бы равна нулю, 

288
00:18:08,780 --> 00:18:10,840
так что это своего рода упрощение.

289
00:18:11,160 --> 00:18:15,624
Использование сигмоид не помогло в обучении, или в какой-то момент 

290
00:18:15,624 --> 00:18:19,689
обучение было очень трудным, и люди просто попробовали relu, 

291
00:18:19,689 --> 00:18:24,620
и оно очень хорошо сработало для этих невероятно глубоких нейронных сетей.

292
00:18:25,100 --> 00:18:25,640
Хорошо, спасибо, Алисия.

