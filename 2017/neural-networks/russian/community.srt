1
00:00:04,260 --> 00:00:05,720
Это тройка.

2
00:00:05,720 --> 00:00:09,220
Она небрежно написана и визуализирована в чрезвычайно маленьком разрешении

3
00:00:09,220 --> 00:00:10,920
28 на 28 пикселей.

4
00:00:10,920 --> 00:00:14,020
Но ваш мозг без труда узнаёт в ней тройку.

5
00:00:14,020 --> 00:00:19,520
И я хотел бы чтобы вы оценили тот изумительный факт, что мозг может делать это с лёгкостью.

6
00:00:19,520 --> 00:00:23,340
Я о том, что это, это и это тоже распознаётся как тройки,

7
00:00:23,340 --> 00:00:28,640
хотя конкретные значения пикселей очень разнятся от картинки к картинке.

8
00:00:28,640 --> 00:00:33,940
Набор светочувствительных клеток в вашем глазу, которые возбуждаются, когда вы видите эту тройку,

9
00:00:33,940 --> 00:00:37,300
сильно отличается от набора, когда вы видите эту тройку.

10
00:00:37,300 --> 00:00:44,020
Но что-то в вашей невероятно умной зрительной коре, решает что они представляют одну сущность.

11
00:00:44,020 --> 00:00:48,940
Одновременно понимая что другие картинки описывают другие сущности.

12
00:00:48,940 --> 00:00:54,400
Но если бы я сказал вам: Эй, напиши мне программу, которая принимает на вход сетку

13
00:00:54,400 --> 00:01:02,380
28 на 28 пикселей как показано, и выдаёт одно число от 0 до 9, угадывая что это за число,

14
00:01:02,380 --> 00:01:06,960
то задача оказалась бы от смешного простой до назойливо сложной.

15
00:01:06,960 --> 00:01:11,180
Если только вы не живёте в пещере, я думаю мне не придётся долго убеждать вас в

16
00:01:11,180 --> 00:01:14,900
актуальности и важности машинного обучения и нейросетей в настоящем и будущем.

17
00:01:14,900 --> 00:01:18,620
Но то чего я добиваюсь - это показать вам что такое нейросеть на самом деле,

18
00:01:18,620 --> 00:01:22,220
предполагая что вы понятия не имеете, и помочь визуализировать её работу.

19
00:01:22,220 --> 00:01:24,780
Не как модное словечко, а с точки зрения математики.

20
00:01:24,780 --> 00:01:29,000
Я бы хотел чтобы вы получили представление о том чем вообще обусловлена такая структура

21
00:01:29,000 --> 00:01:35,140
и чтобы вы понимали, когда читаете или слышите, что значит так называемое "обучение" нейросетей.

22
00:01:35,140 --> 00:01:40,700
Это видео будет посвящено только структуре её компонентов, а следующее затронет обучение.

23
00:01:40,700 --> 00:01:46,620
Мы собираемся построить нейросеть, которую можно будет обучить распознавать написанные вручную цифры.

24
00:01:49,420 --> 00:01:52,680
Это своего рода классический пример для введения в данную тему.

25
00:01:52,680 --> 00:01:56,360
И я хотел бы придерживаться здесь статуса-кво, потому-что после двух видеороликов

26
00:01:56,360 --> 00:01:59,280
я хочу указать вам на пару хороших ресурсов, где вы сможете узнать больше,

27
00:01:59,280 --> 00:02:03,820
и откуда вы сможете скачать код, решающий эту задачу, и самостоятельно поиграть с ним у себя.

28
00:02:04,920 --> 00:02:08,000
Существует огромное количество вариантов нейросетей

29
00:02:08,000 --> 00:02:12,380
и в последние годы наблюдается своего рода бум в исследованиях на эту тему,

30
00:02:12,380 --> 00:02:17,500
но в этих двух вводных видео вы и я будем рассматривать простейший вариант,

31
00:02:17,500 --> 00:02:19,560
без всяких наворотов.

32
00:02:19,560 --> 00:02:24,580
Это своего рода необходимый базис для понимая любых других более современных вариантов,

33
00:02:24,580 --> 00:02:28,820
и поверьте мне он тем не менее достаточно сложен для нашего понимания.

34
00:02:28,820 --> 00:02:33,040
Но даже в такой простейшей форме сеть может обучится распознавать рукописные цифры,

35
00:02:33,040 --> 00:02:36,760
что совсем не плохо для компьютера.

36
00:02:37,420 --> 00:02:42,440
Но так же вы увидите что она оправдает возможно не все наши ожидания.

37
00:02:43,440 --> 00:02:47,160
Как ясно из названия идея нейросетей была заимствованна у мозга.

38
00:02:47,160 --> 00:02:52,240
Но давайте разложим это по полочкам, что такое нейроны и как они связанны между собой?

39
00:02:52,240 --> 00:02:58,160
Сейчас, когда я говорю нейрон, я хочу чтобы вы представляли просто нечто, содержащее число.

40
00:02:58,160 --> 00:03:02,700
Конкретнее, число от 0 до 1. На самом деле это не далеко от истины.

41
00:03:03,720 --> 00:03:08,500
Например, нейросеть начинается с множества нейронов отвечающих за представление всех

42
00:03:08,500 --> 00:03:11,640
28 на 28 пикселей, входного изображения.

43
00:03:11,640 --> 00:03:14,460
То есть всего 784 нейрона.

44
00:03:14,460 --> 00:03:20,820
Каждый из них содержит число, выражающее градацию серого в соответствующем пикселе.

45
00:03:20,820 --> 00:03:24,880
В диапазоне от 0 для чёрных пикселей и до 1 для белых пикселей.

46
00:03:25,140 --> 00:03:28,840
Это число внутри нейрона называется его активацией.

47
00:03:28,840 --> 00:03:34,400
Вы можете представить это себе как-будто нейрон зажигается, когда содержит большее число.

48
00:03:36,480 --> 00:03:41,840
Итак, все эти 784 нейрона составляют первый слой нашей нейросети.

49
00:03:46,260 --> 00:03:51,840
Перепрыгнем на последний слой, он содержит десять нейронов, каждый представляет одно число.

50
00:03:51,840 --> 00:03:56,680
Активация в этих нейронах, опять же число от 0 до 1,

51
00:03:56,680 --> 00:04:02,700
выражает насколько система уверена что входное изображение содержит соответствующую цифру.

52
00:04:02,700 --> 00:04:06,500
Так же есть пара слоёв посередине, называемые скрытыми слоями,

53
00:04:06,500 --> 00:04:09,800
которые на данный момент будут просто большим вопросом -

54
00:04:09,800 --> 00:04:13,940
как, чёрт побери, будет работать этот механизм распознавания цифр.

55
00:04:13,940 --> 00:04:18,020
Для данной сети я выбрал два скрытых слоя, каждый по шестнадцать нейронов,

56
00:04:18,020 --> 00:04:20,900
и в принципе, это произвольный выбор.

57
00:04:20,900 --> 00:04:25,200
Но честно говоря, я выбрал два слоя представляя как я хочу чтобы она работала, об этом через секунду,

58
00:04:25,200 --> 00:04:28,600
а шестнадцать, ну это просто хорошо выглядит на экране.

59
00:04:28,600 --> 00:04:32,740
На практике тут есть большое поле для экспериментов над структурой.

60
00:04:33,100 --> 00:04:39,000
Принцип работы нейросети в том, что активация в одном слое определяет активацию в следующем слое.

61
00:04:39,000 --> 00:04:43,180
И разумеется суть нейросети как механизма обработки информации

62
00:04:43,180 --> 00:04:49,060
сводится к тому, как именно эти активации в одном слое приводят к активациям в следующем слое.

63
00:04:49,060 --> 00:04:53,600
Это грубо сравнивают с тем как работают биологические нейросети,

64
00:04:53,600 --> 00:04:57,780
некоторая группа нейронов возбуждается, вызывая возбуждение другой определённой группы.

65
00:04:57,780 --> 00:05:01,520
Итак, сеть, которую я привожу здесь, уже обучена распознаванию цифр.

66
00:05:01,520 --> 00:05:03,520
Давайте объясню что я имею ввиду.

67
00:05:03,520 --> 00:05:09,560
Это значит, что если дать (скормить) ей картинку, зажигание всех 784 нейронов входного уровня,

68
00:05:09,560 --> 00:05:14,380
согласно яркости каждого пикселя на картинке, такой шаблон активаций,

69
00:05:14,380 --> 00:05:17,440
приведёт к конкретно определённому шаблону в следующем слое,

70
00:05:17,440 --> 00:05:19,640
который приведёт к какому-то шаблону в следующем,

71
00:05:19,640 --> 00:05:22,520
который наконец выдаст какой-то шаблон в финальном слое.

72
00:05:22,520 --> 00:05:27,620
И самый яркий нейрон этого выходного слоя, это, так сказать, "выбор" нейросети

73
00:05:27,620 --> 00:05:30,040
по вопросу какую цифру представляет данная картинка.

74
00:05:32,400 --> 00:05:37,280
И прежде чем углубляться в математику того, как один слой влияет на следующий, или как происходит обучение,

75
00:05:37,280 --> 00:05:43,940
давайте просто поговорим почему вообще такая слоёная структура должна действовать разумно.

76
00:05:43,940 --> 00:05:49,040
Чего нам ожидать, что в лучшем случае смогут делать эти промежуточные слои?

77
00:05:49,040 --> 00:05:54,000
Ну, когда вы и я распознаём цифры, мы сводим воедино различные компоненты.

78
00:05:54,000 --> 00:05:57,200
Девятка содержит кружок вверху и линию справа.

79
00:05:57,200 --> 00:06:01,860
Восьмёрка так же содержит кружок вверху, но и парный ему внизу.

80
00:06:01,860 --> 00:06:07,100
Четвёрка по сути разбивается на три определённых линии, и тому подобное.

81
00:06:07,500 --> 00:06:12,340
И в идеальном мире мы можем ожидать что каждый нейрон из второго слоя и далее

82
00:06:12,340 --> 00:06:15,080
соотносится с одним из этих компонентов.

83
00:06:15,080 --> 00:06:20,020
Что каждый раз когда вы скармливаете картинку, скажем, с кружком наверху, как 9 или 8,

84
00:06:20,020 --> 00:06:24,200
существует определённый нейрон, чья активация станет близка к единице.

85
00:06:24,200 --> 00:06:26,640
И я не имею ввиду этот конкретный набор пикселей,

86
00:06:26,640 --> 00:06:32,160
предполагается что, что угодно, похожее на кружок в верхней части, возбудит этот нейрон.

87
00:06:32,160 --> 00:06:36,600
Таким образом переход от третьего слоя к последнему просто представляет знания о том,

88
00:06:36,600 --> 00:06:40,780
какая комбинация компонентов, какой цифре соответствует.

89
00:06:40,780 --> 00:06:45,400
Конечно это просто перевод стрелок, ведь как распознать эти компоненты,

90
00:06:45,400 --> 00:06:47,760
или даже понять какие компоненты должны присутствовать.

91
00:06:47,760 --> 00:06:53,420
И я всё ещё не сказал как один слой влияет на следующий, но давайте рассмотрим следующее.

92
00:06:53,420 --> 00:06:57,020
Распознавание кружка так же может быть разбито на подзадачи.

93
00:06:57,020 --> 00:06:59,640
Один из разумных способов сделать это заключается в предварительном

94
00:06:59,640 --> 00:07:03,540
распознавании различных маленьких граней из которых он образован.

95
00:07:03,540 --> 00:07:08,600
Аналогично длинная черта, подобная той что есть в цифрах 1, или 4, или 7,

96
00:07:08,600 --> 00:07:10,600
это всего лишь длинная грань,

97
00:07:10,600 --> 00:07:14,800
или её можно представить определённым шаблоном из нескольких меньших граней.

98
00:07:14,800 --> 00:07:20,040
Так что, ВОЗМОЖНО, мы надеемся что каждый нейрон из второго слоя сети

99
00:07:20,040 --> 00:07:23,280
сопоставляется с различными релевантными меленькими гранями.

100
00:07:23,280 --> 00:07:26,400
Возможно, когда на вход поступает картинка вроде этой,

101
00:07:26,400 --> 00:07:32,120
она зажигает все нейроны, ассоциированные с примерно 8 - 10 определёнными небольшими гранями,

102
00:07:32,120 --> 00:07:37,160
которые, в свою очередь, зажигают нейроны, ассоциированные с кружком вверху и длинной вертикальной чертой,

103
00:07:37,160 --> 00:07:40,360
а те зажигают нейрон ассоциированный с девяткой.

104
00:07:40,360 --> 00:07:44,480
Будет ли в конце концов так действовать наша сеть или нет, это другой вопрос.

105
00:07:44,480 --> 00:07:47,500
К которому я вернусь, когда мы увидим как обучается сеть.

106
00:07:47,500 --> 00:07:52,960
Но это может быть нашим ориентиром, своего рода целью для такой слоёной структуры.

107
00:07:52,960 --> 00:07:57,220
Более того, представьте как такое определение граней и шаблонов

108
00:07:57,220 --> 00:08:00,700
может быть весьма полезно в задачах по распознаванию других образов.

109
00:08:00,700 --> 00:08:05,120
И это не только распознавание образов, есть множество потенциальных интеллектуальных задач,

110
00:08:05,120 --> 00:08:07,660
которые разбиваются на слои абстракции.

111
00:08:07,660 --> 00:08:12,900
Разбор речи, например, требует получения сырого аудио и выделения отдельных звуков,

112
00:08:12,900 --> 00:08:15,340
которые комбинируются для образования слогов,

113
00:08:15,340 --> 00:08:20,960
которые комбинируются в слова, затем во фразы и более абстрактные мысли, и т.д.

114
00:08:20,960 --> 00:08:23,760
Но возвращаясь к тому как собственно что-либо из этого работает.

115
00:08:23,760 --> 00:08:25,880
Представьте себе сейчас идею того

116
00:08:25,880 --> 00:08:30,900
как активации в одном слое могут определять активации в следующем.

117
00:08:30,900 --> 00:08:36,300
Цель - получить механизм, который предположительно сможет комбинировать пиксели в грани,

118
00:08:36,300 --> 00:08:39,220
или грани в шаблоны, или шаблоны в цифры.

119
00:08:39,220 --> 00:08:42,100
И чтобы сконцентрироваться на конкретном примере,

120
00:08:42,100 --> 00:08:46,180
предположим цель одного конкретного нейрона во втором слое,

121
00:08:46,180 --> 00:08:51,160
определять содержит ли картинка грань в этой, указанной области.

122
00:08:51,160 --> 00:08:55,380
Первый вопрос - какие параметры должны быть у сети.

123
00:08:55,380 --> 00:08:58,540
Какие табло и ручки настройки должны быть доступны,

124
00:08:58,540 --> 00:09:02,400
достаточно выразительные, чтобы дать потенциальную возможность обнаружить этот шаблон

125
00:09:02,400 --> 00:09:04,420
или любой другой шаблон пикселей,

126
00:09:04,420 --> 00:09:08,000
или шаблон кружка из нескольких граней и тому подобное.

127
00:09:08,560 --> 00:09:13,760
Ну, вот что мы сделаем - мы назначим вес каждому соединению между нашим нейроном

128
00:09:13,760 --> 00:09:16,080
и нейронами из первого слоя.

129
00:09:16,080 --> 00:09:18,320
Эти веса - просто числа.

130
00:09:18,320 --> 00:09:22,300
Затем возьмём все активации из первого слоя

131
00:09:22,300 --> 00:09:26,040
и посчитаем их взвешенную сумму согласно этим весам.

132
00:09:27,480 --> 00:09:32,220
Я считаю сподручно представлять эти веса собранными в свою собственную небольшую сетку,

133
00:09:32,220 --> 00:09:35,020
и я буду использовать зелёные пиксели для отображения положительных весов

134
00:09:35,020 --> 00:09:37,460
и красные пиксели для отображения отрицательных весов,

135
00:09:37,460 --> 00:09:41,980
где яркость пикселя - примерное выражение значения веса.

136
00:09:42,700 --> 00:09:46,100
Итак, если мы установим веса, связанные с практически всеми пикселями в ноль,

137
00:09:46,100 --> 00:09:49,700
за исключением некоторого количества положительных весов в интересующей нас области,

138
00:09:49,700 --> 00:09:55,820
тогда получение взвешенной суммы всех пикселей сведётся к суммированию значений пикселей

139
00:09:55,820 --> 00:09:58,000
только в интересующей нас области.

140
00:09:58,920 --> 00:10:03,820
А если вы хотите определить есть ли там именно грань, вы можете добавить некоторое количество

141
00:10:03,820 --> 00:10:07,120
отрицательных весов, ассоциированных с окружающим пикселями.

142
00:10:07,120 --> 00:10:13,200
Тогда сумма будет наибольшей когда средние пиксели ярче, а окружающие их темнее.

143
00:10:14,580 --> 00:10:18,420
Когда вы вычислите такую взвешенную сумму у вас может получиться любое число,

144
00:10:18,420 --> 00:10:23,940
но для данной сети мы хотим чтобы активации представлялись значением от 0 до 1.

145
00:10:23,940 --> 00:10:26,980
Так что логично вогнать эту взвешенную сумму

146
00:10:26,980 --> 00:10:32,380
в какую-то функцию, которая втиснет реальный диапазон значений в диапазон от 0 до 1.

147
00:10:32,380 --> 00:10:37,900
Обычно функция, которая делает это,  называется сигмоидой или логистической кривой.

148
00:10:37,900 --> 00:10:41,220
По сути, сильно отрицательный ввод оказывается близким к нулю,

149
00:10:41,220 --> 00:10:43,700
сильно положительный ввод оказывается близким к единице,

150
00:10:43,700 --> 00:10:47,220
и она монотонно возрастает на входных значения вокруг нуля.

151
00:10:49,340 --> 00:10:51,700
Таким образом, активация нейрона

152
00:10:51,700 --> 00:10:56,840
это по сути мера того насколько положительна соответствующая взвешенная сумма.

153
00:10:57,740 --> 00:11:02,280
Но возможно не требуется чтобы нейрон зажигался когда взвешенная сумма больше нуля.

154
00:11:02,280 --> 00:11:06,800
Возможно вы хотите чтобы он активировался только когда сумма больше, например, 10.

155
00:11:06,800 --> 00:11:10,940
То есть вам требуется некий сдвиг его активности.

156
00:11:10,940 --> 00:11:16,420
Тогда надо просто добавить некое число, например -10, к этой взвешенной сумме,

157
00:11:16,420 --> 00:11:20,340
до передачи её в сигмоидную функцию сжатия.

158
00:11:20,340 --> 00:11:23,220
Это дополнительное число называется "сдвиг".

159
00:11:23,220 --> 00:11:28,540
Таким образом, веса отвечают за то, какой шаблон пикселей этот нейрон из второго слоя отбирает,

160
00:11:28,540 --> 00:11:32,280
а сдвиг определяет насколько большой должна быть взвешенная сумма,

161
00:11:32,280 --> 00:11:35,660
чтобы нейрон стал достаточно активным.

162
00:11:36,060 --> 00:11:38,040
Это только один нейрон.

163
00:11:38,040 --> 00:11:45,360
Каждый нейрон из этого слоя будет соединён со всеми 784 пиксельными нейронами из первого слоя.

164
00:11:45,360 --> 00:11:51,360
И каждое из этих 784 соединений будет иметь свой ассоциированный с ним вес.

165
00:11:51,360 --> 00:11:58,120
Так же у каждого есть сдвиг, какое-то число, добавляемое к взвешенной сумме до сжатия в сигноиде.

166
00:11:58,120 --> 00:12:02,260
И тут есть над чем задуматься. С этим скрытым слоем из 16 нейронов,

167
00:12:02,260 --> 00:12:08,500
получается всего 784х16 весов, плюс 16 сдвигов.

168
00:12:08,500 --> 00:12:12,260
И всё это просто соединения между первым и вторыми слоями.

169
00:12:12,260 --> 00:12:18,080
Соединения между другими слоями так же содержат кучу весов и сдвигов, связанным с ними.

170
00:12:18,080 --> 00:12:24,120
В результате, данная сеть всего содержит почти ровно 13 000 весов и сдвигов.

171
00:12:24,120 --> 00:12:30,700
13 000 ручек и табло, которые можно крутить и настраивать чтобы менять поведение этой сети.

172
00:12:30,700 --> 00:12:32,700
Так что когда мы говорим об обучении,

173
00:12:32,700 --> 00:12:39,000
имеется ввиду - заставить компьютер найти корректные значения для всех, всех этих чисел,

174
00:12:39,000 --> 00:12:42,000
так, чтобы это решило поставленную задачу.

175
00:12:42,460 --> 00:12:46,120
Одновременно забавный и ужасающий мысленный эксперимент, это

176
00:12:46,120 --> 00:12:50,140
представить себе настройку всех этих весов и сдвигов вручную,

177
00:12:50,140 --> 00:12:54,140
намеренный подбор чисел, чтобы второй слой выбирал грани,

178
00:12:54,140 --> 00:12:57,080
третий слой выбирал шаблоны и т.д.

179
00:12:57,080 --> 00:13:01,900
Лично я считаю это хорошим подходом, чем потом трактовать сеть как чёрный ящик.

180
00:13:01,900 --> 00:13:05,440
Потому-что когда сеть не работает так, как вы ожидаете,

181
00:13:05,440 --> 00:13:10,100
если вы немного представляете что на самом деле означают эти веса и сдвиги,

182
00:13:10,100 --> 00:13:14,680
у вас будет начальная точка для изменения структуры и улучшения результата.

183
00:13:14,680 --> 00:13:18,320
Или если сеть работает, но неожиданным способом,

184
00:13:18,320 --> 00:13:22,820
вникание в то, что означают веса и сдвиги - хороший вызов вашему мышлению,

185
00:13:22,820 --> 00:13:26,540
открывающий всю полноту возможных решений.

186
00:13:26,540 --> 00:13:31,380
Кстати, используемая здесь запись функции имеет немого громоздкий вид, согласны?

187
00:13:32,700 --> 00:13:37,440
Так что позвольте я покажу вам более компактный способ представления этих соединений,

188
00:13:37,440 --> 00:13:41,060
то как вы будите их видеть, если решите читать дальше про нейросети.

189
00:13:41,060 --> 00:13:46,340
Объединим все активации слоя в столбец - вектор.

190
00:13:47,700 --> 00:13:50,540
Затем объединим все веса в матрицу,

191
00:13:50,540 --> 00:13:55,300
каждая строка которой описывает соединения между нейронами одного слоя

192
00:13:55,300 --> 00:13:58,280
с конкретным нейроном следующего слоя.

193
00:13:58,280 --> 00:14:03,980
Выходит, получение взвешенной суммы активаций первого слоя в соответствии с этими весами

194
00:14:03,980 --> 00:14:09,800
соотносится с одним из членов матричного произведения всего того, что у нас слева.

195
00:14:13,720 --> 00:14:18,580
Кстати, большАя часть машинного обучения сводится к хорошему пониманию линейной алгебры,

196
00:14:18,580 --> 00:14:22,000
так что все, желающие увидеть простое и наглядное объяснение матриц

197
00:14:22,000 --> 00:14:24,260
а так же что означает матричное произведение,

198
00:14:24,260 --> 00:14:29,080
посмотрите серию моих видео про линейную алгебру, особенно третью главу.

199
00:14:29,080 --> 00:14:30,260
Возвращаясь к нашему выражению,

200
00:14:30,260 --> 00:14:34,820
вместо добавления сдвига к каждому из этих значений по отдельности

201
00:14:34,820 --> 00:14:38,520
мы представим это описанием всех наших сдвигов в виде вектора,

202
00:14:38,520 --> 00:14:43,040
и сложением этого вектора с полученным ранее матричным произведением.

203
00:14:43,040 --> 00:14:47,620
Тогда останется лишь обернуть всё это в сигмоиду,

204
00:14:47,620 --> 00:14:55,560
что означает применение сигмоидной функции к каждому члену получившегося внутри вектора.

205
00:14:55,560 --> 00:15:00,460
Таким образом, дав этой матрице весов и этим векторам собственные обозначения,

206
00:15:00,460 --> 00:15:04,900
вы сможете выражать весь переход активаций от одного слоя к следующему

207
00:15:04,900 --> 00:15:08,000
в виде чрезвычайно компактного и аккуратного выражения.

208
00:15:08,000 --> 00:15:12,300
И это делает соответствующий код и гораздо проще, и гораздо быстрее,

209
00:15:12,300 --> 00:15:16,480
так как многие библиотеки чертовски оптимизированы под матричные умножения.

210
00:15:17,660 --> 00:15:21,980
Помните как я ранее говорил что нейроны это просто сущности, содержащие числа?

211
00:15:21,980 --> 00:15:26,940
Ну и, содержащиеся в них числа конечно зависят от того какое изображение вы скормили.

212
00:15:28,120 --> 00:15:31,840
Так что правильнее представлять каждый нейрон в виде функции,

213
00:15:31,840 --> 00:15:39,040
которая принимает выходы со всех нейронов предыдущего слоя и выплёвывает число от 0 до 1.

214
00:15:39,040 --> 00:15:41,480
На самом деле вся сеть - это просто функция,

215
00:15:41,480 --> 00:15:47,660
которая берёт на вход 784 числа и выплёвывает 10 чисел на выходе.

216
00:15:47,660 --> 00:15:55,460
Это до абсурда сложная функция, которая включает 13 000 параметров в виде этих весов и сдвигов, которые отвечают за определённые шаблоны,

217
00:15:55,460 --> 00:16:00,760
и требующая выполнения множества матричных умножений и вычислений сигмоидных функций сжатия,

218
00:16:00,760 --> 00:16:03,500
но тем не менее, это просто функция.

219
00:16:03,500 --> 00:16:07,080
И в каком-то смысле это логично, что она выглядит сложной,

220
00:16:07,080 --> 00:16:13,080
я имею ввиду, если бы она была проще, с чего бы мы решили что она способна решить задачу распознавания цифр.

221
00:16:13,080 --> 00:16:15,080
А как она решает эту задачу?

222
00:16:15,080 --> 00:16:19,900
Как эта сеть находит подходящие веса и сдвиги просто смотря на данные?

223
00:16:19,900 --> 00:16:22,080
Хм, это я покажу в следующем видео,

224
00:16:22,080 --> 00:16:26,780
а так же я немного углублюсь в то, что именно эта, рассмотренная нами, сеть делает на самом деле.

225
00:16:27,380 --> 00:16:32,920
Теперь настало время, когда я по-видимому должен сказать: подписывайтесь чтобы быть в курсе когда выдут это или новые видео,

226
00:16:32,920 --> 00:16:37,840
но в реальности большинство из вас на самом деле не принимают оповещений с YouTube, не так ли?

227
00:16:37,840 --> 00:16:43,680
Может правильнее мне сказать: подписывайтесь чтобы нейросети, лежащие в основе алгоритмов рекомендации YouTube,

228
00:16:43,680 --> 00:16:48,200
уяснили что вы хотите видеть в рекомендациях контент с этого канала.

229
00:16:48,200 --> 00:16:50,560
В любом случае, оставайтесь на связи.

230
00:16:50,560 --> 00:16:53,860
Большое спасибо всем, кто спонсирует эти видео на "Patreon".

231
00:16:53,860 --> 00:16:57,120
Я немного затормозил с работой над серией о вероятности этим летом,

232
00:16:57,120 --> 00:16:59,540
но возьмусь за неё снова, после данного проекта,

233
00:16:59,540 --> 00:17:02,360
так что, Patreons (спонсоры), ожидайте там обновлений.

234
00:17:03,660 --> 00:17:06,040
Чтобы закрыть тему, тут со мной Лиша Ли (Lisha Li),

235
00:17:06,040 --> 00:17:08,920
чья докторская диссертация была посвящена теоретическим основам глубокого обучения,

236
00:17:08,920 --> 00:17:12,160
сейчас она работает в венчурной компании "Amplify Partners"

237
00:17:12,160 --> 00:17:15,160
и она добродушно внесла свой вклад в создание данного видео.

238
00:17:15,160 --> 00:17:19,460
Итак, Лиша, я думаю мы должны кратко затронуть тему этой сигмоидной функции,

239
00:17:19,460 --> 00:17:23,060
насколько я понял раньше сети использовали её чтобы сжать соответствующие взвешенные суммы

240
00:17:23,060 --> 00:17:27,820
в этот интервал от 0 до 1, как бы подражая биологическим нейронам,

241
00:17:27,820 --> 00:17:29,780
которые либо в пассивном, либо в активном состоянии.

242
00:17:29,780 --> 00:17:30,480
Точно.

243
00:17:30,480 --> 00:17:35,580
Но вообще-то уже относительно мало современных сетей используют сигмоиду, это, типа, старая школа, так?

244
00:17:35,580 --> 00:17:39,320
Да, но скорее "ReLU" выглядит более простым для обучения.

245
00:17:39,320 --> 00:17:42,560
И "ReLU" означает "Rectified Linear Unit" (выпрямленный линейный модуль).

246
00:17:42,560 --> 00:17:47,400
Да, это своего рода функция которая берёт максимум от нуля и "a",

247
00:17:47,400 --> 00:17:54,440
где "a" определяется тем о чём ты говорил в видео, и я думаю это было частично навеяно

248
00:17:54,440 --> 00:18:01,380
биологической аналогией того как нейроны могут находиться либо в активном состоянии, либо нет,

249
00:18:01,380 --> 00:18:05,660
и если пройден определённый порог, то отрабатывает функция,

250
00:18:05,660 --> 00:18:09,600
а если не пройден, тогда он просто остаётся неактивным - равным нулю.

251
00:18:09,600 --> 00:18:13,080
Это своего рода упрощение, использование сигмоид не способствовало обучению

252
00:18:13,080 --> 00:18:17,780
или в какой-то момент становилось слишком сложным, и люди попробовали "ReLU",

253
00:18:17,780 --> 00:18:24,860
и оказалось что это работает очень хорошо для таких невероятно глубоких (многослойных) нейросетей.

