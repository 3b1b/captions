1
00:00:04,357 --> 00:00:05,420
Isto é um três.

2
00:00:05,857 --> 00:00:09,166
Está escrito de forma desleixada
e renderizado a uma resolução extremamente baixa

3
00:00:09,199 --> 00:00:10,670
de 28 por 28 pixels.

4
00:00:11,079 --> 00:00:13,717
Mas o seu cérebro não tem dificuldade
para reconhecê-lo como um três.

5
00:00:14,285 --> 00:00:16,063
E queria que você parasse para pensar

6
00:00:16,309 --> 00:00:19,142
em como é louco que cérebros
conseguem fazer isso sem esforço?

7
00:00:19,610 --> 00:00:23,235
Assim, isto, isto e isto também são
reconhecidos como três,

8
00:00:23,394 --> 00:00:28,270
ainda que os valores específicos de cada pixel
sejam bem diferentes em cada imagem.

9
00:00:28,738 --> 00:00:33,780
As células fotossensíveis no seu olho que
estão sendo ativadas quando você vê este três

10
00:00:34,067 --> 00:00:36,874
são bem diferentes daquelas que são
ativadas quando você vê este três;

11
00:00:37,485 --> 00:00:40,813
mas algo nesse seu córtex visual
louco de inteligente

12
00:00:41,075 --> 00:00:43,972
decide que elas
representam a mesma ideia,

13
00:00:44,186 --> 00:00:48,355
enquanto reconhece outras imagens
como ideias distintas.

14
00:00:49,184 --> 00:00:52,830
Mas e se eu lhe dissesse para se sentar
e me escrever um programa

15
00:00:52,965 --> 00:00:56,563
que pega uma grade
de 28 por 28 pixels assim

16
00:00:56,888 --> 00:00:59,663
e emite um único número entre 0 e 10,

17
00:01:00,076 --> 00:01:02,076
que lhe diz qual ele acha
que é o dígito?

18
00:01:02,568 --> 00:01:06,274
Bom, a tarefa passa de comicamente fácil
para assustadoramente difícil.

19
00:01:07,120 --> 00:01:08,533
Se não tem vivido
embaixo de uma pedra,

20
00:01:08,628 --> 00:01:11,316
acho que mal preciso motivar
a relevância e importância

21
00:01:11,332 --> 00:01:14,699
do aprendizado de máquina
e redes neurais para o presente e o futuro.

22
00:01:15,080 --> 00:01:18,475
Mas aqui eu quero lhe mostrar
o que é uma rede neural de verdade,

23
00:01:18,745 --> 00:01:20,045
presumindo nenhum conhecimento,

24
00:01:20,253 --> 00:01:22,120
e ajudar a visualizar como ela funciona,

25
00:01:22,335 --> 00:01:24,429
não como uma palavra da moda,
mas como parte da matemática.

26
00:01:24,914 --> 00:01:29,007
Espero que você saia sentindo
que a própria estrutura faz sentido

27
00:01:29,099 --> 00:01:34,527
e que sabe o que significa quando lê ou ouve
que uma rede neural "aprende".

28
00:01:35,331 --> 00:01:38,470
Este vídeo só vai focar
nos componentes estruturais disso

29
00:01:38,629 --> 00:01:40,367
e o próximo vai lidar
com o aprendizado.

30
00:01:40,853 --> 00:01:46,117
Vamos montar uma rede neural
capaz de aprender a reconhecer dígitos manuscritos.

31
00:01:49,631 --> 00:01:52,508
Este é um exemplo clássico
para apresentar o tópico.

32
00:01:52,833 --> 00:01:56,361
E fico feliz em me ater ao status quo aqui,
porque no fim dos dois vídeos,

33
00:01:56,417 --> 00:01:59,253
quero lhe indicar umas boas fontes
onde você pode aprender mais,

34
00:01:59,387 --> 00:02:03,160
e baixar o código que faz isto,
e brincar com ele no seu computador.

35
00:02:05,071 --> 00:02:07,817
Há muitas variações de redes neurais,

36
00:02:08,009 --> 00:02:12,115
e nos últimos anos, houve uma explosão
nas pesquisas sobre essas variações.

37
00:02:12,520 --> 00:02:14,525
mas nestes dois
vídeos introdutórios

38
00:02:14,795 --> 00:02:19,210
só vamos ver a forma
mais simples sem penduricalhos.

39
00:02:19,702 --> 00:02:21,649
Isso é como um pré-requisito

40
00:02:21,736 --> 00:02:24,510
para entender qualquer uma
das variações modernas mais poderosas.

41
00:02:24,760 --> 00:02:28,470
E acredite em mim, ainda há muita complexidade
para tentarmos entender.

42
00:02:29,060 --> 00:02:30,756
Mas mesmo nesta forma simples,

43
00:02:30,766 --> 00:02:32,929
ela pode aprender a reconhecer
dígitos manuscritos,

44
00:02:33,159 --> 00:02:36,353
o que é uma capacidade muito legal
para um computador.

45
00:02:37,499 --> 00:02:41,960
E ao mesmo tempo, você verá como ela fica aquém
de algumas coisas que podemos esperar dela.

46
00:02:43,544 --> 00:02:46,992
Como o nome sugere, redes neurais
são inspiradas no cérebro.

47
00:02:47,373 --> 00:02:48,510
Mas vamos entender isso melhor.

48
00:02:48,799 --> 00:02:51,759
O que são os neurônios
e em que sentido eles estão conectados?

49
00:02:52,459 --> 00:02:54,421
Daqui em diante, quando eu digo neurônio,

50
00:02:54,612 --> 00:02:57,953
quero que só pense numa coisa
que contém um número,

51
00:02:58,273 --> 00:03:00,507
especificamente um número entre 0 e 1.

52
00:03:00,754 --> 00:03:02,237
Realmente não passa disso.

53
00:03:03,829 --> 00:03:06,847
Por exemplo, a rede começa
com um monte de neurônios

54
00:03:06,887 --> 00:03:11,406
correspondentes a cada um dos
28x28 pixels da imagem de entrada,

55
00:03:11,774 --> 00:03:14,247
que são 784 neurônios no total.

56
00:03:14,605 --> 00:03:20,472
Cada um deles contém um número que representa
o valor no nível de cinza do pixel correspondente,

57
00:03:20,903 --> 00:03:24,455
variando de zero para pixels pretos
a 1 para pixels brancos.

58
00:03:25,281 --> 00:03:28,574
Este número dentro do neurônio
se chama  sua "ativação".

59
00:03:29,090 --> 00:03:30,479
E a imagem que você
deve ter em mente aqui

60
00:03:30,495 --> 00:03:34,042
é que cada neurônio está "aceso"
quando sua ativação é um número alto.

61
00:03:36,649 --> 00:03:41,630
então, todos estes 784 neurônios
compõem a primeira camada da nossa rede.

62
00:03:46,361 --> 00:03:48,099
Agora, saltando para a última camada,

63
00:03:48,227 --> 00:03:51,394
esta tem dez neurônios,
cada um representando um dos dígitos.

64
00:03:51,968 --> 00:03:54,010
A ativação nestes neurônios

65
00:03:54,153 --> 00:03:56,480
(novamente um número entre 0 e1)

66
00:03:56,855 --> 00:04:02,129
representa quanto o sistema acha que uma dada imagem
corresponde com um dado dígito.

67
00:04:02,867 --> 00:04:06,317
Também há algumas camadas no meio,
chamadas camadas ocultas,

68
00:04:06,531 --> 00:04:09,796
que por enquanto devem ser
só um grande ponto de interrogação

69
00:04:09,923 --> 00:04:13,549
para como que será feito esse processo
de reconhecer dígitos.

70
00:04:14,159 --> 00:04:17,866
Nesta rede escolhi duas camadas ocultas,
cada qual com 16 neurônios,

71
00:04:18,186 --> 00:04:20,752
e admito que
é uma escolha arbitrária.

72
00:04:21,021 --> 00:04:24,896
Sendo honesto, escolhi duas camadas com base em
como quero motivar a estrutura daqui a pouco

73
00:04:25,350 --> 00:04:28,345
e 16, bem, foi um bom número
para encaixar na tela.

74
00:04:28,703 --> 00:04:32,277
Na prática, há muito espaço para experimentar
com esta estrutura específica.

75
00:04:33,152 --> 00:04:38,592
Como a rede funciona? Ativações em uma camada
determinam as ativações na camada seguinte.

76
00:04:39,152 --> 00:04:43,141
E, claro, o âmago da rede como um mecanismo
de processamento de informação,

77
00:04:43,315 --> 00:04:48,580
se resume a exatamente como as ativações
em uma camada provocam ativações na camada seguinte.

78
00:04:49,196 --> 00:04:53,539
É para ser, grosso modo, análogo a como,
em redes biológicas de neurônios,

79
00:04:53,700 --> 00:04:57,410
a ativação de alguns grupos de neurônios
causa a ativação de outros grupos.

80
00:04:57,987 --> 00:05:01,658
Ora, a rede que estou mostrando aqui
já foi treinada para reconhecer dígitos,

81
00:05:01,683 --> 00:05:03,414
e deixe-me mostrar
o que quero dizer com isso.

82
00:05:03,575 --> 00:05:05,404
Significa que, se você insere uma imagem

83
00:05:05,618 --> 00:05:09,446
que acende todos os 784 neurônios
da camada de entrada

84
00:05:09,621 --> 00:05:12,208
de acordo com o brilho
de cada pixel da imagem,

85
00:05:12,647 --> 00:05:17,192
esse padrão de ativação causa
um padrão bem específico na camada seguinte,

86
00:05:17,541 --> 00:05:19,432
que causa um padrão específico
na seguinte,

87
00:05:19,789 --> 00:05:22,085
que finalmente produz um padrão
na camada de saída.

88
00:05:22,633 --> 00:05:25,296
E o neurônio mais brilhante dessa camada de saída

89
00:05:25,614 --> 00:05:29,621
é a escolha da rede, por assim dizer,
sobre qual dígito a imagem representa.

90
00:05:32,511 --> 00:05:35,836
E antes de entrar na matemática
de como uma camada influencia a seguinte,

91
00:05:35,892 --> 00:05:37,130
ou de como o treinamento
funciona,

92
00:05:37,426 --> 00:05:41,984
vamos falar de por que faz sentido esperar
que uma estrutura com camadas assim

93
00:05:42,175 --> 00:05:43,711
se comporte de forma inteligente.

94
00:05:44,149 --> 00:05:45,252
O que esperamos aqui?

95
00:05:45,427 --> 00:05:48,323
O que é o melhor que podemos esperar
que as camadas ocultas estejam fazendo?

96
00:05:49,230 --> 00:05:53,710
Bem, quando você ou eu reconhecemos dígitos,
juntamos vários componentes.

97
00:05:54,139 --> 00:05:56,822
Um nove tem um círculo em cima
e uma linha na direita.

98
00:05:57,365 --> 00:06:01,218
Um oito também tem um circulo em cima,
mas é pareado com outro círculo embaixo.

99
00:06:01,980 --> 00:06:06,690
Um quatro basicamente se decompõe em 3 linhas,
etc.

100
00:06:07,588 --> 00:06:12,181
Ora, num mundo perfeito, podemos esperar que
cada neurônio na penúltima camada

101
00:06:12,572 --> 00:06:14,843
corresponda a
um desses subcomponentes,

102
00:06:15,168 --> 00:06:18,634
que sempre que você insira uma imagem,
digamos, com um círculo em cima,

103
00:06:18,697 --> 00:06:19,904
como um 9 ou um 8,

104
00:06:20,269 --> 00:06:23,803
haja algum neurônio específico
cuja ativação vá ser próxima a um.

105
00:06:24,422 --> 00:06:26,584
E não quero dizer este círculo específico de pixels.

106
00:06:26,796 --> 00:06:31,618
A esperança é que qualquer padrão
circular geral no topo ative este neurônio.

107
00:06:32,364 --> 00:06:35,039
Assim, para ir
da terceira camada até a última,

108
00:06:35,380 --> 00:06:40,126
só é preciso aprender qual combinação de
subcomponentes corresponde a quais dígitos.

109
00:06:40,963 --> 00:06:43,058
Claro, isso só adia o problema,

110
00:06:43,114 --> 00:06:45,426
pois como você reconheceria
esses subcomponentes

111
00:06:45,458 --> 00:06:47,643
ou sequer aprenderia quais devem ser
os  subcomponentes corretos?

112
00:06:47,968 --> 00:06:50,957
E eu ainda nem falei de como
uma camada influencia a seguinte,

113
00:06:51,148 --> 00:06:53,148
mas continue comigo nisso
por um momento.

114
00:06:53,572 --> 00:06:56,723
O reconhecimento de um círculo
também pode se decompor em subproblemas.

115
00:06:57,123 --> 00:07:02,698
Uma maneira razoável de fazer isso seria primeiro
reconhecer as várias pequenas bordas que o compõem.

116
00:07:03,605 --> 00:07:08,391
Da mesma forma, uma longa linha,
como você pode ver nos dígitos 1, 4 ou 7,

117
00:07:08,820 --> 00:07:10,639
isso é só uma longa borda.

118
00:07:10,806 --> 00:07:14,433
Ou você pode pensar nela como
certo padrão de várias bordas menores.

119
00:07:15,048 --> 00:07:19,836
Então, talvez esperemos que cada
neurônio na segunda camada da rede

120
00:07:20,195 --> 00:07:22,808
corresponda
às várias bordinhas relevantes.

121
00:07:23,526 --> 00:07:26,226
Talvez, quando uma imagem assim aparece,

122
00:07:26,586 --> 00:07:31,804
ela acenda todos os neurônios associados
com cerca de oito a dez bordinhas específicas,

123
00:07:32,304 --> 00:07:36,930
que, por sua vez, acendem os neurônios associados
com o círculo de cima e a longa linha vertical,

124
00:07:37,396 --> 00:07:39,801
e eles acendem
o neurônio associado com o nove.

125
00:07:40,590 --> 00:07:43,145
Se isso é ou não é
o que a nossa rede realmente faz

126
00:07:43,197 --> 00:07:44,300
é outra questão,

127
00:07:44,625 --> 00:07:47,054
e voltaremos a ela quando virmos
como treinar a rede.

128
00:07:47,684 --> 00:07:49,535
Mas isso é algo
que podemos esperar,

129
00:07:49,744 --> 00:07:52,386
um tipo de objetivo
com a camada estruturada assim.

130
00:07:53,036 --> 00:07:57,079
Além do mais, você pode imaginar
como poder detectar bordas e padrões assim

131
00:07:57,311 --> 00:08:00,327
seria muito útil para outras tarefas
de reconhecimento de imagem.

132
00:08:00,891 --> 00:08:02,555
E mesmo além do reconhecimento de imagem

133
00:08:02,619 --> 00:08:04,991
há vários tipos de coisa inteligentes
que você pode querer fazer

134
00:08:05,246 --> 00:08:07,431
que se decompõem
em camadas de abstração.

135
00:08:07,808 --> 00:08:12,777
A análise sintática, por exemplo, envolve
a captura de áudio bruto e a extração de sons distintos,

136
00:08:13,034 --> 00:08:15,146
que combinam para formar certas sílabas,

137
00:08:15,473 --> 00:08:17,001
que combinam
para formar palavras,

138
00:08:17,011 --> 00:08:20,338
que combinam para formar expressões
e pensamentos mais abstratos, etc.

139
00:08:21,123 --> 00:08:23,546
Mas voltando para como isso realmente funciona,

140
00:08:23,959 --> 00:08:28,249
imagine-se projetando o modo como
as ativações numa camada

141
00:08:28,353 --> 00:08:30,519
podem determinar
as ativações da seguinte.

142
00:08:31,061 --> 00:08:36,018
O objetivo é ter algum mecanismo
capaz de combinar pixels em bordas,

143
00:08:36,326 --> 00:08:39,068
ou bordas em padrões, ou padrões em dígitos.

144
00:08:39,354 --> 00:08:41,895
E focando num exemplo
bem específico,

145
00:08:42,252 --> 00:08:46,144
digamos que esperamos que um neurônio
em particular na segunda camada

146
00:08:46,295 --> 00:08:50,523
detecte se a imagem tem ou não
uma borda nesta região aqui.

147
00:08:51,392 --> 00:08:55,130
A questão em mãos é:
quais parâmetros a rede neural deve ter?

148
00:08:55,537 --> 00:08:58,461
Em que discadores e botões você deve poder mexer

149
00:08:58,636 --> 00:09:02,167
de modo que ela seja expressiva o bastante
para poder capturar este padrão?

150
00:09:02,590 --> 00:09:04,203
Ou qualquer outro padrão de pixels?

151
00:09:04,497 --> 00:09:07,698
Ou o padrão de que várias bordas
podem fazer um círculo, e coisas assim?

152
00:09:08,652 --> 00:09:15,548
Bem, atribuiremos um peso a cada uma das conexões
entre nosso neurônio e os da primeira camada.

153
00:09:16,248 --> 00:09:17,850
Esses pesos são só números.

154
00:09:18,497 --> 00:09:22,239
Então pegue todas aquelas ativações
da primeira camada

155
00:09:22,430 --> 00:09:25,588
e calcule a sua soma ponderada
segundo o valor desses pesos.

156
00:09:27,554 --> 00:09:31,961
Acho útil conceber esses pesos como
organizados numa pequena grade deles próprios.

157
00:09:32,381 --> 00:09:37,450
E vou usar pixels verdes para indicar pesos positivos
e vermelhos para indicar pesos negativos,

158
00:09:37,585 --> 00:09:41,925
sendo o brilho de cada pixel
uma representação aproximada do valor do peso.

159
00:09:42,809 --> 00:09:46,021
Ora, se definirmos como zero os pesos associados
com praticamente todos os pixels,

160
00:09:46,150 --> 00:09:49,420
exceto alguns pesos positivos
nesta região que nos importa,

161
00:09:49,864 --> 00:09:52,908
então, obter a soma ponderada
de todos os valores de pixels

162
00:09:53,042 --> 00:09:57,574
realmente se resume somar os valores
dos pixels só na região que nos importa.

163
00:09:58,870 --> 00:10:02,139
E, se você realmente deseja detectar
se existe uma borda aqui,

164
00:10:02,576 --> 00:10:06,730
você pode ter alguns pesos negativos
associados com os pixels ao redor.

165
00:10:07,030 --> 00:10:10,701
Então, a soma é maior
quando esses pixels do meio são brilhantes,

166
00:10:10,757 --> 00:10:12,757
mas os pixels ao redor são mais escuros.

167
00:10:14,673 --> 00:10:18,296
Quando se calcula uma soma ponderada assim,
pode dar qualquer número.

168
00:10:18,605 --> 00:10:23,557
Mas para esta rede, queremos que
as ativações sejam algum valor entre 0 e1.

169
00:10:24,133 --> 00:10:26,786
Então, algo comum é introduzir
esta soma ponderada

170
00:10:27,063 --> 00:10:32,000
em alguma função que comprime
a reta de números reais num intervalo entre 0 e 1.

171
00:10:32,527 --> 00:10:35,524
E uma função comum que faz isso
se chama função sigmoide,

172
00:10:35,826 --> 00:10:37,506
também conhecida
como curva logística.

173
00:10:38,036 --> 00:10:41,030
Basicamente, entradas muito negativas
terminam próximas de zero

174
00:10:41,395 --> 00:10:43,572
e entradas muito positivas
terminam próximas de 1,

175
00:10:43,898 --> 00:10:46,632
e aumentam de forma constante
perto da entrada 0.

176
00:10:49,473 --> 00:10:51,628
Então, a ativação do neurônio aqui é
basicamente

177
00:10:51,859 --> 00:10:56,628
uma medida de quão positiva
é a soma ponderada relevante.

178
00:10:57,807 --> 00:11:01,906
Mas talvez você não queira que o neurônio acenda
quando a soma ponderada passa de 0;

179
00:11:02,407 --> 00:11:06,394
talvez você só queira que ele se ative
quando a soma passa de, digamos, 10.

180
00:11:07,013 --> 00:11:10,279
Ou seja, você quer um viés para a inatividade.

181
00:11:11,242 --> 00:11:15,064
Então, só vamos acrescentar
outro número (como -10)

182
00:11:15,198 --> 00:11:16,302
a essa soma ponderada

183
00:11:16,566 --> 00:11:19,741
antes de inseri-la
na função sigmoide.

184
00:11:20,625 --> 00:11:22,730
Esse número adicional
se chama viés.

185
00:11:23,405 --> 00:11:28,392
Então, os pesos lhe dizem que padrão de pixel
este neurônio na segunda chamada está detectando,

186
00:11:28,640 --> 00:11:32,205
e o viés lhe diz quão alta
a soma ponderada precisa ser

187
00:11:32,428 --> 00:11:35,308
para o neurônio começar
a se ativar significativamente.

188
00:11:36,146 --> 00:11:37,805
E isso é só um neurônio!

189
00:11:38,197 --> 00:11:45,144
Cada neurônio nesta camada estará conectado
a todos os 784 neurônios de pixel da primeira camada,

190
00:11:45,454 --> 00:11:50,852
e cada uma dessas 784 conexões
tem o seu próprio peso associado consigo.

191
00:11:51,549 --> 00:11:53,318
Além disso, cada um tem um viés,

192
00:11:53,470 --> 00:11:57,636
outro número adicionado à soma ponderada
antes de ser comprimido com a simoide.

193
00:11:58,249 --> 00:11:59,578
E isso é muita coisa para pensar.

194
00:11:59,991 --> 00:12:08,375
Com esta camada oculta de 16 neurônios,
é um total de 784x16 pesos junto com 16 vieses.

195
00:12:08,791 --> 00:12:12,088
E isso são só as conexões
entre a primeira camada e a segunda.

196
00:12:12,452 --> 00:12:17,391
As conexões entre as outras camadas
também têm um monte de pesos e vieses associados.

197
00:12:18,289 --> 00:12:23,964
Ao todo, esta rede tem quase exatamente
13 mil pesos e vieses no total.

198
00:12:24,306 --> 00:12:29,935
13 mil botões em que podemos mexer
para fazer esta rede se comportar de formas diferentes.

199
00:12:30,903 --> 00:12:32,520
Então,
quando falamos de aprendizado,

200
00:12:32,770 --> 00:12:38,965
isso se refere a fazer o computador achar
uma definição válida para todos esses vários números

201
00:12:39,139 --> 00:12:41,468
para que ele efetivamente resolva
o problema em questão.

202
00:12:42,520 --> 00:12:46,084
Um experimento mental que é
tanto divertido quanto horrível

203
00:12:46,290 --> 00:12:50,093
é se imaginar definindo todos
esses pesos e vieses à mão,

204
00:12:50,326 --> 00:12:54,304
ajustando de propósito os números
para que a segunda camada detecte bordas,

205
00:12:54,399 --> 00:12:56,711
a terceira camada detecte
padrões, etc.

206
00:12:57,323 --> 00:12:58,967
Pessoalmente, acho
isso satisfatório,

207
00:12:59,007 --> 00:13:01,649
em vez de tratar a rede
como uma caixa-preta,

208
00:13:02,058 --> 00:13:05,232
porque quando a rede
não age como você espera,

209
00:13:05,494 --> 00:13:09,880
se você se familiarizou com
o significado desses pesos e vieses,

210
00:13:10,335 --> 00:13:14,291
você sabe onde começar
a mudar a estrutura para melhorá-la.

211
00:13:14,910 --> 00:13:18,089
Ou quando a rede funciona,
mas não pelas razões esperadas,

212
00:13:18,496 --> 00:13:22,721
captar o funcionamento dos pesos e vieses
é um bom modo de desafiar as suas suposições

213
00:13:22,822 --> 00:13:25,898
e expor o espaço completo
de soluções possíveis.

214
00:13:26,720 --> 00:13:30,104
Aliás, a função real aqui
é meio complicada de escrever,

215
00:13:30,152 --> 00:13:30,763
né?

216
00:13:32,742 --> 00:13:37,268
Então, vou mostrar um jeito de representar essas conexões
que é mais compacto em termos notacionais.

217
00:13:37,667 --> 00:13:40,540
Você vai ver assim,
se ler mais sobre redes neurais.

218
00:13:41,233 --> 00:13:46,045
Organize todas as ativações de uma camada
numa coluna como um vetor.

219
00:13:47,820 --> 00:13:50,396
Então, organize todos os pesos
como uma matriz,

220
00:13:50,753 --> 00:13:54,106
sendo que cada fileira da matriz
corresponde às conexões

221
00:13:54,131 --> 00:13:57,948
entre uma camada e um neurônio específico
na camada seguinte.

222
00:13:58,394 --> 00:14:03,835
Isso significa que pegar a soma ponderada
das ativações da primeira camada, segundo esses pesos,

223
00:14:03,974 --> 00:14:09,411
corresponde a um dos termos
no produto matriz-vetor de tudo na esquerda aqui.

224
00:14:13,922 --> 00:14:18,414
Aliás, muito do aprendizado de máquina se resume
a ter um bom entendimento de álgebra linear.

225
00:14:18,700 --> 00:14:22,026
Então, para os que desejam
um bom entendimento visual de matrizes

226
00:14:22,136 --> 00:14:24,153
e do que significa
a multiplicação de vetor e matriz,

227
00:14:24,465 --> 00:14:27,076
deem uma olhada na minha série
sobre álgebra linear,

228
00:14:27,295 --> 00:14:28,486
especialmente o capítulo 3.

229
00:14:29,152 --> 00:14:30,278
Voltando à nossa expressão,

230
00:14:30,446 --> 00:14:34,429
em vez de adicionar o viés
a cada um destes valores de modo independente,

231
00:14:34,970 --> 00:14:38,452
representamos organizando
todos esses vieses num vetor

232
00:14:38,714 --> 00:14:42,215
e adicionando o vetor inteiro
ao produtor vetor-matriz anterior.

233
00:14:43,281 --> 00:14:44,386
Então, como etapa final,

234
00:14:44,625 --> 00:14:47,657
vou envolver tudo com uma sigmoide aqui.

235
00:14:47,728 --> 00:14:54,775
E isso representa a aplicação da função sigmoide
a cada componente específico do vetor resultante ali dentro.

236
00:14:55,810 --> 00:15:00,319
Então, depois de escrever esta matriz de pesos
e estes vetores como seus próprios símbolos,

237
00:15:00,603 --> 00:15:04,843
você pode comunicar toda a transição
de ativações de uma camada para a outra

238
00:15:05,090 --> 00:15:07,486
numa expressãozinha
extremamente organizada.

239
00:15:08,220 --> 00:15:12,106
E isso torna o código relevante mais simples e rápido,

240
00:15:12,313 --> 00:15:15,773
já que muitas bibliotecas
otimizam bastante a multiplicação de matrizes.

241
00:15:17,666 --> 00:15:21,529
Lembra que disse que os neurônios
não passam de coisas que contêm números?

242
00:15:22,180 --> 00:15:26,437
Bem, claro que os números específicos
que eles contêm dependem da imagem inserida.

243
00:15:28,177 --> 00:15:31,704
Então, é mais preciso conceber
cada neurônio como uma função,

244
00:15:32,017 --> 00:15:35,964
que pega as saídas
de todos os neurônios da camada anterior

245
00:15:36,290 --> 00:15:38,393
e emite um número
entre 0 e 1.

246
00:15:39,167 --> 00:15:41,319
Na verdade, a rede inteira
não passa de uma função,

247
00:15:41,621 --> 00:15:47,078
que pega 784 números como entrada
e emite 10 números como saída.

248
00:15:47,830 --> 00:15:49,748
É uma função absurdamente complicada,

249
00:15:49,758 --> 00:15:55,433
que envolve 13 mil parâmetros na forma
desses pesos e vieses que captam certos padrões

250
00:15:55,616 --> 00:16:00,360
e que envolve a iteração de muitos produtos matriz-vetor
e a função sigmoide.

251
00:16:00,944 --> 00:16:02,944
No entanto, é só uma função.

252
00:16:03,626 --> 00:16:06,779
E de certa forma, é reconfortante
que pareça complicada.

253
00:16:07,227 --> 00:16:12,402
Se fosse mais simples, como poderíamos
esperar que ela fosse capaz de reconhecer dígitos?

254
00:16:13,288 --> 00:16:14,791
Mas como ela é capaz disso?

255
00:16:15,021 --> 00:16:19,457
Como é que esta rede aprende
os pesos e vieses adequados só olhando os dados?

256
00:16:20,076 --> 00:16:21,756
Bom,
vou mostrar isso no próximo vídeo.

257
00:16:22,163 --> 00:16:26,140
Também vou me aprofundar no que
esta rede específica aqui está fazendo.

258
00:16:27,517 --> 00:16:29,547
Acho que agora eu tenho que dizer
para vocês se inscreverem

259
00:16:29,564 --> 00:16:32,770
para serem notificados
quando esse ou qualquer novo vídeo sair.

260
00:16:33,127 --> 00:16:37,560
Mas, na realidade, a maioria de vocês
não recebe notificações do YouTube, né?

261
00:16:37,961 --> 00:16:39,382
Talvez seja mais honesto dizer:

262
00:16:39,414 --> 00:16:43,606
"Inscreva-se, para que as redes neurais
do algoritmo de recomendação do YouTube

263
00:16:43,802 --> 00:16:47,639
creiam que você deseja que
o conteúdo deste canal seja recomendado para você."

264
00:16:48,374 --> 00:16:50,146
Enfim, aguarde mais.

265
00:16:50,702 --> 00:16:53,686
Muito obrigado a todos que apoiam
estes vídeos no Patreon.

266
00:16:54,003 --> 00:16:56,978
Não progredi muito na série
sobre probabilidade este verão,

267
00:16:57,036 --> 00:16:59,186
mas vou voltar a ela depois deste projeto.

268
00:16:59,322 --> 00:17:01,457
Então, patrons,
podem procurar atualizações por lá.

269
00:17:03,661 --> 00:17:05,964
Para encerrar, aqui comigo
está Lisha Li,

270
00:17:06,067 --> 00:17:08,903
que fez doutorado sobre
o lado teórico do aprendizado profundo

271
00:17:09,008 --> 00:17:10,922
e trabalha atualmente
numa empresa de capital de risco

272
00:17:10,962 --> 00:17:12,262
chamada Amplify Partners,

273
00:17:12,295 --> 00:17:14,694
que gentilmente forneceu parte
do financiamento para este vídeo.

274
00:17:15,282 --> 00:17:19,237
Então, Lisha, temos que mencionar
essa função sigmoide.

275
00:17:19,618 --> 00:17:23,076
Se bem entendo, redes mais antigas usavam isso
para comprimir a soma ponderada relevante

276
00:17:23,092 --> 00:17:25,083
naquele intervalo entre 0 e 1,

277
00:17:25,233 --> 00:17:29,637
e isso era motivado por uma analogia biológica
de neurônios estarem ativos ou inativos.

278
00:17:29,694 --> 00:17:30,392
Exato.

279
00:17:30,448 --> 00:17:33,912
Mas poucas redes modernas
continuam usando a sigmoide.

280
00:17:33,953 --> 00:17:34,396
Sim.

281
00:17:34,453 --> 00:17:35,582
É meio antiquada,
né?

282
00:17:35,607 --> 00:17:39,195
Ou melhor, a ReLu parece
muito mais fácil de treinar.

283
00:17:39,276 --> 00:17:42,357
E ReLu significa
"unidade linear retificada"?

284
00:17:42,575 --> 00:17:47,399
Sim, é uma função em que
pegamos um max, um 0 e a,

285
00:17:47,423 --> 00:17:51,134
sendo a dado pelo que
você explicou no vídeo.

286
00:17:51,145 --> 00:17:57,237
E a motivação disso, em parte,
foi uma analogia biológica

287
00:17:57,257 --> 00:18:01,308
com a maneira como os neurônios
estão ativos ou não.

288
00:18:01,356 --> 00:18:05,630
Se passasse certo limiar,
seria a função de identidade.

289
00:18:05,671 --> 00:18:09,356
Se não, não seria ativado,
seria 0.

290
00:18:09,396 --> 00:18:10,920
Meio que uma simplificação.

291
00:18:11,015 --> 00:18:15,557
A sigmoide era muito difícil
de treinar em certo ponto,

292
00:18:15,613 --> 00:18:17,786
e experimentaram a ReLu

293
00:18:17,873 --> 00:18:24,709
e acabou funcionando muito bem
para redes neurais incrivelmente profundas.

294
00:18:24,907 --> 00:18:25,928
Certo!
Obrigado, Lisha.

295
00:18:28,009 --> 00:18:30,194
Legendas: Luan Marques
(rmo.luan@gmail.com)

