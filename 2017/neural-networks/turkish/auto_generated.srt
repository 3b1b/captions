1
00:00:04,220 --> 00:00:05,400
Bu bir 3.

2
00:00:06,060 --> 00:00:09,810
Özensiz bir şekilde yazılmış ve 28x28 piksel gibi son derece düşük bir 

3
00:00:09,810 --> 00:00:13,720
çözünürlükte işlenmiş, ancak beyniniz bunu 3 olarak tanımakta zorlanmıyor.

4
00:00:14,340 --> 00:00:16,363
Ve bir an durup beyinlerin bunu bu kadar zahmetsizce 

5
00:00:16,363 --> 00:00:18,960
yapabilmesinin ne kadar çılgınca olduğunu takdir etmenizi istiyorum.

6
00:00:19,700 --> 00:00:22,932
Yani, bu, bu ve bu da 3'ler olarak tanınabilir, 

7
00:00:22,932 --> 00:00:28,320
her bir pikselin belirli değerleri bir görüntüden diğerine çok farklı olsa bile.

8
00:00:28,900 --> 00:00:33,413
Gözünüzde bu 3'ü gördüğünüzde ateşlenen ışığa duyarlı hücreler, 

9
00:00:33,413 --> 00:00:36,940
bu 3'ü gördüğünüzde ateşlenenlerden çok farklıdır.

10
00:00:37,520 --> 00:00:40,853
Ancak o çılgın zekalı görsel korteksinizdeki bir şey, 

11
00:00:40,853 --> 00:00:44,124
bunları aynı fikri temsil ediyor olarak çözümlerken, 

12
00:00:44,124 --> 00:00:48,260
aynı zamanda diğer görüntüleri kendi farklı fikirleri olarak tanır.

13
00:00:49,220 --> 00:00:54,894
Ama size desem ki, hey, oturun ve benim için 28x28 piksellik bir ızgarayı bu şekilde alan 

14
00:00:54,894 --> 00:01:00,442
ve 0 ile 10 arasında tek bir sayı çıktısı veren ve size rakamın ne olduğunu düşündüğünü 

15
00:01:00,442 --> 00:01:05,675
söyleyen bir program yazın, görev komik derecede önemsizden ürkütücü derecede zora 

16
00:01:05,675 --> 00:01:06,180
dönüşür.

17
00:01:07,160 --> 00:01:10,900
Bir kayanın altında yaşamıyorsanız, makine öğrenimi ve sinir ağlarının günümüzle 

18
00:01:10,900 --> 00:01:14,640
ve gelecekle olan ilgisini ve önemini motive etmeme gerek olmadığını düşünüyorum.

19
00:01:15,120 --> 00:01:18,233
Ancak burada yapmak istediğim şey, hiçbir arka plan olmadığını varsayarak size 

20
00:01:18,233 --> 00:01:21,386
bir sinir ağının gerçekte ne olduğunu göstermek ve ne yaptığını bir moda sözcük 

21
00:01:21,386 --> 00:01:24,460
olarak değil, bir matematik parçası olarak görselleştirmeye yardımcı olmaktır.

22
00:01:25,020 --> 00:01:28,154
Umudum, yapının kendisinin motive edici olduğunu hissederek ayrılmanız ve 

23
00:01:28,154 --> 00:01:31,247
okuduğunuzda ya da bir sinir ağından alıntı yaparak öğrenme hakkında bir 

24
00:01:31,247 --> 00:01:34,340
şeyler duyduğunuzda bunun ne anlama geldiğini bildiğinizi hissetmenizdir.

25
00:01:35,360 --> 00:01:40,260
Bu video sadece bunun yapı bileşenine ayrılacak ve bir sonraki video öğrenmeyi ele alacak.

26
00:01:40,960 --> 00:01:43,472
Yapacağımız şey, el yazısı rakamları tanımayı 

27
00:01:43,472 --> 00:01:46,040
öğrenebilen bir sinir ağını bir araya getirmek.

28
00:01:49,360 --> 00:01:53,628
Bu, konuyu tanıtmak için biraz klasik bir örnek ve burada statükoya bağlı kalmaktan 

29
00:01:53,628 --> 00:01:58,201
mutluyum, çünkü iki videonun sonunda sizi daha fazla bilgi edinebileceğiniz ve bunu yapan 

30
00:01:58,201 --> 00:02:02,571
kodu indirip kendi bilgisayarınızda oynayabileceğiniz birkaç iyi kaynağa yönlendirmek 

31
00:02:02,571 --> 00:02:03,080
istiyorum.

32
00:02:05,040 --> 00:02:09,446
Sinir ağlarının pek çok çeşidi vardır ve son yıllarda bu çeşitlere 

33
00:02:09,446 --> 00:02:12,866
yönelik araştırmalarda bir tür patlama yaşanmıştır, 

34
00:02:12,866 --> 00:02:16,812
ancak bu iki tanıtım videosunda siz ve ben sadece en basit, 

35
00:02:16,812 --> 00:02:19,180
sade ve gösterişsiz forma bakacağız.

36
00:02:19,860 --> 00:02:24,097
Bu, daha güçlü modern varyantlardan herhangi birini anlamak için gerekli bir ön 

37
00:02:24,097 --> 00:02:28,600
koşuldur ve inanın bana hala zihnimizi sarmamız için çok fazla karmaşıklığa sahiptir.

38
00:02:29,120 --> 00:02:33,037
Ancak bu en basit haliyle bile el yazısıyla yazılmış rakamları tanımayı 

39
00:02:33,037 --> 00:02:36,520
öğrenebilir ki bu bir bilgisayar için oldukça harika bir şeydir.

40
00:02:37,480 --> 00:02:39,784
Ve aynı zamanda, onun için sahip olabileceğimiz 

41
00:02:39,784 --> 00:02:42,280
birkaç umudun nasıl yetersiz kaldığını göreceksiniz.

42
00:02:43,380 --> 00:02:48,500
Adından da anlaşılacağı gibi sinir ağları beyinden ilham alır, ancak bunu biraz açalım.

43
00:02:48,520 --> 00:02:51,660
Nöronlar nelerdir ve hangi anlamda birbirleriyle bağlantılıdırlar?

44
00:02:52,500 --> 00:02:56,830
Şu anda nöron dediğimde düşünmenizi istediğim tek şey bir sayıyı, 

45
00:02:56,830 --> 00:03:00,440
özellikle de 0 ile 1 arasında bir sayıyı tutan bir şey.

46
00:03:00,680 --> 00:03:02,560
Bundan daha fazlası değil.

47
00:03:03,780 --> 00:03:09,283
Örneğin ağ, giriş görüntüsünün 28x28 pikselinin her birine karşılık 

48
00:03:09,283 --> 00:03:14,220
gelen bir grup nöronla başlar, bu da toplamda 784 nöron eder.

49
00:03:14,700 --> 00:03:19,603
Bunların her biri, ilgili pikselin gri tonlama değerini temsil eden ve siyah 

50
00:03:19,603 --> 00:03:24,380
pikseller için 0'dan beyaz pikseller için 1'e kadar değişen bir sayı tutar.

51
00:03:25,300 --> 00:03:30,261
Nöronun içindeki bu sayıya aktivasyon denir ve burada aklınıza gelebilecek görüntü, 

52
00:03:30,261 --> 00:03:34,160
aktivasyonu yüksek bir sayı olduğunda her nöronun aydınlandığıdır.

53
00:03:36,720 --> 00:03:41,860
Yani bu 784 nöronun tamamı ağımızın ilk katmanını oluşturuyor.

54
00:03:46,500 --> 00:03:49,085
Şimdi son katmana geçiyoruz, bu katmanda her biri 

55
00:03:49,085 --> 00:03:51,360
rakamlardan birini temsil eden 10 nöron var.

56
00:03:52,040 --> 00:03:55,750
Bu nöronlardaki aktivasyon, yine 0 ile 1 arasında bir sayı, 

57
00:03:55,750 --> 00:04:00,635
sistemin belirli bir görüntünün belirli bir rakama ne kadar karşılık geldiğini 

58
00:04:00,635 --> 00:04:02,120
düşündüğünü temsil eder.

59
00:04:03,040 --> 00:04:08,116
Arada gizli katmanlar olarak adlandırılan birkaç katman daha var ki bunlar 

60
00:04:08,116 --> 00:04:13,600
şimdilik rakamları tanıma işleminin nasıl yapılacağına dair dev bir soru işareti.

61
00:04:14,260 --> 00:04:17,281
Bu ağda, her biri 16 nörondan oluşan iki gizli 

62
00:04:17,281 --> 00:04:20,560
katman seçtim ve kuşkusuz bu biraz keyfi bir seçim.

63
00:04:21,019 --> 00:04:24,707
Dürüst olmak gerekirse, birazdan yapıyı nasıl motive etmek istediğime bağlı 

64
00:04:24,707 --> 00:04:28,200
olarak iki katman seçtim ve 16, ekrana sığdırmak için güzel bir rakamdı.

65
00:04:28,780 --> 00:04:32,340
Pratikte, burada belirli bir yapı ile deney yapmak için çok fazla alan vardır.

66
00:04:33,020 --> 00:04:35,988
Ağın çalışma şekline göre, bir katmandaki aktivasyonlar 

67
00:04:35,988 --> 00:04:38,480
bir sonraki katmanın aktivasyonlarını belirler.

68
00:04:39,200 --> 00:04:42,494
Ve elbette bir bilgi işleme mekanizması olarak ağın kalbi, 

69
00:04:42,494 --> 00:04:47,351
bir katmandaki bu aktivasyonların bir sonraki katmanda tam olarak nasıl aktivasyonlara 

70
00:04:47,351 --> 00:04:48,580
yol açtığına bağlıdır.

71
00:04:49,140 --> 00:04:53,189
Bu, biyolojik nöron ağlarında bazı nöron gruplarının ateşlenmesinin 

72
00:04:53,189 --> 00:04:57,180
diğerlerinin de ateşlenmesine neden olmasıyla benzerlik gösteriyor.

73
00:04:58,120 --> 00:05:00,783
Şimdi burada gösterdiğim ağ zaten rakamları tanımak için 

74
00:05:00,783 --> 00:05:03,400
eğitildi ve bununla ne demek istediğimi size göstereyim.

75
00:05:03,640 --> 00:05:08,313
Bu, bir görüntüyü beslerseniz, görüntüdeki her pikselin parlaklığına göre 

76
00:05:08,313 --> 00:05:11,912
giriş katmanındaki 784 nöronun tamamını aydınlatırsanız, 

77
00:05:11,912 --> 00:05:16,585
bu aktivasyon modeli bir sonraki katmanda çok özel bir modele neden olur, 

78
00:05:16,585 --> 00:05:22,080
bu da bir sonrakinde bir modele neden olur ve sonunda çıktı katmanında bir model verir.

79
00:05:22,560 --> 00:05:25,778
Ve bu çıktı katmanının en parlak nöronu, tabiri caizse, 

80
00:05:25,778 --> 00:05:29,400
bu görüntünün hangi rakamı temsil ettiğine dair ağın seçimidir.

81
00:05:32,560 --> 00:05:36,466
Bir katmanın diğerini nasıl etkilediğine veya eğitimin nasıl işlediğine 

82
00:05:36,466 --> 00:05:39,884
dair matematiğe geçmeden önce, bunun gibi katmanlı bir yapının 

83
00:05:39,884 --> 00:05:43,520
akıllıca davranmasını beklemenin neden makul olduğundan bahsedelim.

84
00:05:44,060 --> 00:05:45,220
Burada ne bekliyoruz?

85
00:05:45,400 --> 00:05:47,600
Bu orta katmanlar için en iyi umut nedir?

86
00:05:48,920 --> 00:05:53,520
Siz veya ben rakamları fark ettiğimizde, çeşitli bileşenleri bir araya getiririz.

87
00:05:54,200 --> 00:05:56,820
9'un üstte bir halkası ve sağda bir çizgisi vardır.

88
00:05:57,380 --> 00:06:01,180
8'de de üstte bir halka vardır, ancak altta başka bir halka ile eşleştirilmiştir.

89
00:06:01,980 --> 00:06:06,820
Bir 4 temel olarak üç özel çizgiye ve bunun gibi şeylere ayrılır.

90
00:06:07,600 --> 00:06:12,850
Şimdi mükemmel bir dünyada, ikinci katmandan son katmana kadar her bir nöronun bu alt 

91
00:06:12,850 --> 00:06:16,025
bileşenlerden birine karşılık gelmesini umabiliriz, 

92
00:06:16,025 --> 00:06:20,482
örneğin 9 veya 8 gibi üstte bir döngü olan bir görüntüyü beslediğinizde, 

93
00:06:20,482 --> 00:06:23,780
aktivasyonu 1'e yakın olacak belirli bir nöron vardır.

94
00:06:24,500 --> 00:06:28,030
Bu özel piksel döngüsünü kastetmiyorum, umudum tepeye doğru olan 

95
00:06:28,030 --> 00:06:31,560
herhangi bir genel döngüsel desenin bu nöronu harekete geçirmesi.

96
00:06:32,440 --> 00:06:36,266
Bu şekilde, üçüncü katmandan son katmana geçmek sadece hangi alt bileşen 

97
00:06:36,266 --> 00:06:40,040
kombinasyonunun hangi rakamlara karşılık geldiğini öğrenmeyi gerektirir.

98
00:06:41,000 --> 00:06:42,880
Tabii ki bu da sorunu daha da derinleştiriyor, 

99
00:06:42,880 --> 00:06:44,760
çünkü bu alt bileşenleri nasıl tanıyacaksınız, 

100
00:06:44,760 --> 00:06:47,640
hatta doğru alt bileşenlerin ne olması gerektiğini nasıl öğreneceksiniz?

101
00:06:48,060 --> 00:06:51,225
Ve daha bir katmanın diğerini nasıl etkilediğinden bahsetmedim bile, 

102
00:06:51,225 --> 00:06:53,060
ama bir an için bu konuda benimle gelin.

103
00:06:53,680 --> 00:06:56,680
Bir döngünün tanınması da alt problemlere ayrılabilir.

104
00:06:57,280 --> 00:07:00,373
Bunu yapmanın makul bir yolu, öncelikle onu oluşturan 

105
00:07:00,373 --> 00:07:02,780
çeşitli küçük kenarları tanımak olacaktır.

106
00:07:03,780 --> 00:07:07,933
Benzer şekilde, 1, 4 veya 7 rakamlarında görebileceğiniz türden uzun bir çizgi, 

107
00:07:07,933 --> 00:07:11,308
aslında sadece uzun bir kenardır veya belki de bunu birkaç küçük 

108
00:07:11,308 --> 00:07:14,320
kenardan oluşan belirli bir desen olarak düşünebilirsiniz.

109
00:07:15,140 --> 00:07:18,690
Belki de umudumuz, ağın ikinci katmanındaki her bir 

110
00:07:18,690 --> 00:07:22,720
nöronun çeşitli ilgili küçük kenarlara karşılık gelmesidir.

111
00:07:23,540 --> 00:07:28,685
Belki de bunun gibi bir görüntü geldiğinde, yaklaşık 8 ila 10 belirli küçük 

112
00:07:28,685 --> 00:07:33,897
kenarla ilişkili tüm nöronları aydınlatır, bu da üst döngü ve uzun bir dikey 

113
00:07:33,897 --> 00:07:39,720
çizgi ile ilişkili nöronları aydınlatır ve bunlar da 9 ile ilişkili nöronu aydınlatır.

114
00:07:40,680 --> 00:07:43,726
Nihai ağımızın gerçekte yaptığı şeyin bu olup olmadığı, 

115
00:07:43,726 --> 00:07:47,589
ağı nasıl eğiteceğimizi gördüğümüzde geri döneceğim başka bir sorudur, 

116
00:07:47,589 --> 00:07:51,397
ancak bu, sahip olabileceğimiz bir umut, bunun gibi katmanlı bir yapı 

117
00:07:51,397 --> 00:07:52,540
ile bir tür hedeftir.

118
00:07:53,160 --> 00:07:56,706
Dahası, bunun gibi kenarları ve desenleri tespit edebilmenin diğer görüntü 

119
00:07:56,706 --> 00:08:00,300
tanıma görevleri için nasıl gerçekten yararlı olacağını hayal edebilirsiniz.

120
00:08:00,880 --> 00:08:04,053
Ve hatta görüntü tanımanın ötesinde, soyutlama katmanlarına 

121
00:08:04,053 --> 00:08:07,280
ayrılan yapmak isteyebileceğiniz her türlü akıllı şey vardır.

122
00:08:08,040 --> 00:08:11,896
Örneğin, konuşmanın ayrıştırılması, ham sesin alınmasını ve belirli heceleri 

123
00:08:11,896 --> 00:08:15,051
oluşturmak için birleşen, kelimeleri oluşturmak için birleşen, 

124
00:08:15,051 --> 00:08:18,307
cümleleri ve daha soyut düşünceleri oluşturmak için birleşen vb. 

125
00:08:18,307 --> 00:08:20,060
farklı seslerin seçilmesini içerir.

126
00:08:21,100 --> 00:08:23,893
Ancak bunların gerçekte nasıl çalıştığına geri dönersek, 

127
00:08:23,893 --> 00:08:27,813
şu anda kendinizi bir katmandaki aktivasyonların bir sonrakini tam olarak nasıl 

128
00:08:27,813 --> 00:08:29,920
belirleyebileceğini tasarlarken hayal edin.

129
00:08:30,860 --> 00:08:35,119
Amaç, pikselleri kenarlara, kenarları desenlere ya da desenleri 

130
00:08:35,119 --> 00:08:38,980
rakamlara dönüştürebilecek bir mekanizmaya sahip olmaktır.

131
00:08:39,440 --> 00:08:42,807
Çok spesifik bir örneği yakınlaştırmak gerekirse, 

132
00:08:42,807 --> 00:08:48,330
diyelim ki ikinci katmandaki belirli bir nöronun görüntünün bu bölgede bir kenarı 

133
00:08:48,330 --> 00:08:50,620
olup olmadığını anlaması umuluyor.

134
00:08:51,440 --> 00:08:55,100
Asıl soru, ağın hangi parametrelere sahip olması gerektiğidir?

135
00:08:55,640 --> 00:08:59,495
Bu deseni veya başka herhangi bir piksel desenini veya birkaç kenarın bir 

136
00:08:59,495 --> 00:09:03,611
döngü oluşturabileceği deseni ve benzeri şeyleri potansiyel olarak yakalayacak 

137
00:09:03,611 --> 00:09:07,780
kadar etkileyici olması için hangi kadranları ve düğmeleri ayarlayabilmelisiniz?

138
00:09:08,720 --> 00:09:12,049
Yapacağımız şey, nöronumuz ile ilk katmandaki nöronlar 

139
00:09:12,049 --> 00:09:15,560
arasındaki bağlantıların her birine bir ağırlık atamaktır.

140
00:09:16,320 --> 00:09:17,700
Bu ağırlıklar sadece rakamlardan ibarettir.

141
00:09:18,540 --> 00:09:22,051
Daha sonra ilk katmandan tüm bu aktivasyonları alın ve 

142
00:09:22,051 --> 00:09:25,500
bu ağırlıklara göre ağırlıklı toplamlarını hesaplayın.

143
00:09:27,700 --> 00:09:31,027
Bu ağırlıkları kendi başlarına küçük bir ızgarada düzenlenmiş olarak 

144
00:09:31,027 --> 00:09:34,498
düşünmeyi yararlı buluyorum ve pozitif ağırlıkları belirtmek için yeşil 

145
00:09:34,498 --> 00:09:38,452
pikselleri ve negatif ağırlıkları belirtmek için kırmızı pikselleri kullanacağım, 

146
00:09:38,452 --> 00:09:41,780
burada pikselin parlaklığı ağırlığın değerinin gevşek bir tasviridir.

147
00:09:42,780 --> 00:09:47,354
Şimdi, önemsediğimiz bu bölgedeki bazı pozitif ağırlıklar dışında neredeyse tüm 

148
00:09:47,354 --> 00:09:50,157
piksellerle ilişkili ağırlıkları sıfır yaparsak, 

149
00:09:50,157 --> 00:09:54,789
tüm piksel değerlerinin ağırlıklı toplamını almak gerçekten sadece önemsediğimiz 

150
00:09:54,789 --> 00:09:57,820
bölgedeki piksel değerlerini toplamak anlamına gelir.

151
00:09:59,140 --> 00:10:02,330
Ve burada gerçekten bir kenar olup olmadığını anlamak istiyorsanız, 

152
00:10:02,330 --> 00:10:05,896
yapabileceğiniz şey çevredeki piksellerle ilişkili bazı negatif ağırlıklara 

153
00:10:05,896 --> 00:10:06,600
sahip olmaktır.

154
00:10:07,480 --> 00:10:10,216
Bu durumda, ortadaki pikseller parlak ancak çevredeki 

155
00:10:10,216 --> 00:10:12,700
pikseller daha koyu olduğunda toplam en büyüktür.

156
00:10:14,260 --> 00:10:18,953
Bunun gibi ağırlıklı bir toplam hesapladığınızda, herhangi bir sayı elde edebilirsiniz, 

157
00:10:18,953 --> 00:10:23,540
ancak bu ağ için istediğimiz şey aktivasyonların 0 ile 1 arasında bir değer olmasıdır.

158
00:10:24,120 --> 00:10:28,051
Bu nedenle, bu ağırlıklı toplamı, gerçek sayı doğrusunu 0 ile 1 arasındaki 

159
00:10:28,051 --> 00:10:32,140
aralığa sıkıştıran bir fonksiyona pompalamak yaygın olarak yapılan bir şeydir.

160
00:10:32,460 --> 00:10:34,799
Ve bunu yapan yaygın bir fonksiyon, lojistik eğri 

161
00:10:34,799 --> 00:10:37,420
olarak da bilinen sigmoid fonksiyon olarak adlandırılır.

162
00:10:38,000 --> 00:10:42,443
Temel olarak çok negatif girişler 0'a yakın, pozitif girişler 

163
00:10:42,443 --> 00:10:46,600
1'e yakın olur ve 0 girişi etrafında sürekli olarak artar.

164
00:10:49,120 --> 00:10:52,374
Yani buradaki nöronun aktivasyonu temelde ilgili 

165
00:10:52,374 --> 00:10:56,360
ağırlıklı toplamın ne kadar pozitif olduğunun bir ölçüsüdür.

166
00:10:57,540 --> 00:11:01,880
Ancak belki de nöronun ağırlıklı toplam 0'dan büyük olduğunda yanmasını istemiyorsunuzdur.

167
00:11:02,280 --> 00:11:06,360
Belki de sadece toplam 10'dan büyük olduğunda aktif olmasını istiyorsunuz.

168
00:11:06,840 --> 00:11:10,260
Yani, etkin olmaması için bir miktar önyargı istiyorsunuz.

169
00:11:11,380 --> 00:11:15,520
O zaman yapacağımız şey, sigmoid squishification fonksiyonundan geçirmeden 

170
00:11:15,520 --> 00:11:19,660
önce bu ağırlıklı toplama negatif 10 gibi başka bir sayı eklemek olacaktır.

171
00:11:20,580 --> 00:11:22,440
Bu ek sayıya önyargı adı verilir.

172
00:11:23,460 --> 00:11:27,097
Yani ağırlıklar size ikinci katmandaki bu nöronun hangi piksel desenini 

173
00:11:27,097 --> 00:11:30,886
algıladığını söyler ve önyargı size nöronun anlamlı bir şekilde aktif hale 

174
00:11:30,886 --> 00:11:35,180
gelmeye başlamadan önce ağırlıklı toplamın ne kadar yüksek olması gerektiğini söyler.

175
00:11:36,120 --> 00:11:37,680
Ve bu sadece bir nöron.

176
00:11:38,280 --> 00:11:44,772
Bu katmandaki diğer her nöron, ilk katmandaki 784 piksel nöronuna bağlanacaktır 

177
00:11:44,772 --> 00:11:50,940
ve bu 784 bağlantının her birinin kendisiyle ilişkili kendi ağırlığı vardır.

178
00:11:51,600 --> 00:11:54,444
Ayrıca, her birinin sigmoid ile ezmeden önce ağırlıklı 

179
00:11:54,444 --> 00:11:57,600
toplama eklediğiniz başka bir sayı gibi bir önyargısı vardır.

180
00:11:58,110 --> 00:11:59,540
Ve düşünecek çok şey var!

181
00:11:59,960 --> 00:12:04,349
Bu 16 nöronlu gizli katman, 16 önyargı ile birlikte 

182
00:12:04,349 --> 00:12:07,980
toplam 784 çarpı 16 ağırlık anlamına gelir.

183
00:12:08,840 --> 00:12:11,940
Ve tüm bunlar sadece birinci katmandan ikinciye olan bağlantılardır.

184
00:12:12,520 --> 00:12:15,069
Diğer katmanlar arasındaki bağlantılar da kendileriyle 

185
00:12:15,069 --> 00:12:17,340
ilişkili bir dizi ağırlığa ve önyargıya sahiptir.

186
00:12:18,340 --> 00:12:20,940
Tüm söylenenler ve yapılanlar, bu ağın neredeyse tam olarak 

187
00:12:20,940 --> 00:12:23,800
13.000 toplam ağırlığa ve önyargıya sahip olduğunu göstermektedir.

188
00:12:23,800 --> 00:12:26,880
13.000 düğme ve kadran, bu ağın farklı şekillerde 

189
00:12:26,880 --> 00:12:29,960
davranmasını sağlamak için ayarlanıp çevrilebilir.

190
00:12:31,040 --> 00:12:36,050
Yani öğrenmeden bahsettiğimizde, bilgisayarın tüm bu çok sayıdaki sayı için geçerli 

191
00:12:36,050 --> 00:12:41,360
bir ayar bulmasını ve böylece eldeki sorunu gerçekten çözmesini sağlamaktan bahsediyoruz.

192
00:12:42,620 --> 00:12:45,779
Hem eğlenceli hem de dehşet verici bir düşünce deneyi, 

193
00:12:45,779 --> 00:12:50,892
oturup tüm bu ağırlıkları ve önyargıları elle ayarladığınızı, ikinci katmanın kenarları, 

194
00:12:50,892 --> 00:12:55,718
üçüncü katmanın desenleri vb. seçmesi için sayıları kasıtlı olarak değiştirdiğinizi 

195
00:12:55,718 --> 00:12:56,580
hayal etmektir.

196
00:12:56,980 --> 00:13:01,831
Ben şahsen bunu, ağa tamamen kara kutu muamelesi yapmaktan daha tatmin edici buluyorum, 

197
00:13:01,831 --> 00:13:06,131
çünkü ağ beklediğiniz şekilde çalışmadığında, bu ağırlıkların ve önyargıların 

198
00:13:06,131 --> 00:13:09,218
gerçekte ne anlama geldiğiyle biraz ilişki kurduysanız, 

199
00:13:09,218 --> 00:13:14,180
yapıyı iyileştirmek için nasıl değiştireceğinizi denemek için bir başlangıç noktanız olur.

200
00:13:14,960 --> 00:13:18,156
Ya da ağ çalıştığında ancak beklediğiniz nedenlerle çalışmadığında, 

201
00:13:18,156 --> 00:13:20,695
ağırlıkların ve önyargıların ne yaptığını araştırmak, 

202
00:13:20,695 --> 00:13:24,456
varsayımlarınıza meydan okumak ve olası çözümlerin tüm alanını gerçekten ortaya 

203
00:13:24,456 --> 00:13:25,820
çıkarmak için iyi bir yoldur.

204
00:13:26,840 --> 00:13:30,680
Bu arada, buradaki asıl işlevi yazmak biraz zahmetli, sizce de öyle değil mi?

205
00:13:32,500 --> 00:13:34,904
Şimdi size bu bağlantıların notasyonel olarak daha derli 

206
00:13:34,904 --> 00:13:37,140
toplu bir şekilde nasıl temsil edildiğini göstereyim.

207
00:13:37,660 --> 00:13:40,520
Sinir ağları hakkında daha fazla okumayı seçerseniz bunu bu şekilde görürsünüz.

208
00:13:41,380 --> 00:13:49,641
Bir katmandaki tüm aktivasyonları, bir katman ile bir sonraki katmandaki belirli bir 

209
00:13:49,641 --> 00:13:58,000
nöron arasındaki bağlantılara karşılık gelen bir matris olarak bir sütunda düzenleyin.

210
00:13:58,540 --> 00:14:02,300
Bunun anlamı, bu ağırlıklara göre ilk katmandaki aktivasyonların 

211
00:14:02,300 --> 00:14:06,003
ağırlıklı toplamının alınması, burada solda sahip olduğumuz her 

212
00:14:06,003 --> 00:14:09,880
şeyin matris vektör çarpımındaki terimlerden birine karşılık gelir.

213
00:14:14,000 --> 00:14:18,645
Bu arada, makine öğreniminin büyük bir kısmı lineer cebiri iyi kavramaktan geçiyor, 

214
00:14:18,645 --> 00:14:23,235
bu nedenle matrisler ve matris vektör çarpımının ne anlama geldiğini görsel olarak 

215
00:14:23,235 --> 00:14:27,272
anlamak isteyenler lineer cebir üzerine yaptığım seriye, özellikle de 3. 

216
00:14:27,272 --> 00:14:28,600
bölüme bir göz atabilir.

217
00:14:29,240 --> 00:14:33,516
İfademize geri dönersek, bu değerlerin her birine bağımsız olarak önyargı 

218
00:14:33,516 --> 00:14:38,139
eklemekten bahsetmek yerine, tüm bu önyargıları bir vektör halinde düzenleyerek 

219
00:14:38,139 --> 00:14:42,300
ve tüm vektörü önceki matris vektör çarpımına ekleyerek temsil ediyoruz.

220
00:14:43,280 --> 00:14:47,177
Son adım olarak, burada dış tarafın etrafına bir sigmoid saracağım 

221
00:14:47,177 --> 00:14:50,958
ve bunun temsil etmesi gereken şey, sigmoid fonksiyonunu içeride 

222
00:14:50,958 --> 00:14:54,740
ortaya çıkan vektörün her bir özel bileşenine uygulayacağınızdır.

223
00:14:55,940 --> 00:15:01,258
Dolayısıyla, bu ağırlık matrisini ve bu vektörleri kendi sembolleri olarak yazdığınızda, 

224
00:15:01,258 --> 00:15:06,278
aktivasyonların bir katmandan diğerine tam geçişini son derece sıkı ve düzgün küçük 

225
00:15:06,278 --> 00:15:11,237
bir ifadeyle iletebilirsiniz ve bu, ilgili kodu hem çok daha basit hem de çok daha 

226
00:15:11,237 --> 00:15:15,660
hızlı hale getirir, çünkü birçok kütüphane matris çarpımını optimize eder.

227
00:15:17,820 --> 00:15:19,620
Daha önce bu nöronların sadece sayıları tutan 

228
00:15:19,620 --> 00:15:21,460
şeyler olduğunu söylediğimi hatırlıyor musunuz?

229
00:15:22,220 --> 00:15:26,965
Elbette tuttukları belirli sayılar beslediğiniz görüntüye bağlıdır, 

230
00:15:26,965 --> 00:15:32,478
bu nedenle aslında her nöronu bir önceki katmandaki tüm nöronların çıktılarını 

231
00:15:32,478 --> 00:15:38,340
alan ve 0 ile 1 arasında bir sayı veren bir fonksiyon olarak düşünmek daha doğrudur.

232
00:15:39,200 --> 00:15:43,054
Aslında tüm ağ, 784 sayıyı girdi olarak alan ve 10 

233
00:15:43,054 --> 00:15:47,060
sayıyı çıktı olarak veren bir fonksiyondan ibarettir.

234
00:15:47,560 --> 00:15:52,274
Bu, belirli örüntüleri yakalayan bu ağırlıklar ve önyargılar biçiminde 13.000 

235
00:15:52,274 --> 00:15:56,989
parametre içeren ve birçok matris vektör çarpımını ve sigmoid squishification 

236
00:15:56,989 --> 00:16:01,099
işlevini yinelemeyi içeren saçma bir şekilde karmaşık bir işlevdir, 

237
00:16:01,099 --> 00:16:05,692
ancak yine de sadece bir işlevdir ve bir şekilde karmaşık görünmesi bir tür 

238
00:16:05,692 --> 00:16:06,660
güven vericidir.

239
00:16:07,340 --> 00:16:09,659
Demek istediğim, daha basit olsaydı, rakamları tanıma 

240
00:16:09,659 --> 00:16:12,280
zorluğunun üstesinden gelebileceğine dair ne umudumuz olurdu?

241
00:16:13,340 --> 00:16:14,700
Peki bu zorluğun üstesinden nasıl geliyor?

242
00:16:15,080 --> 00:16:19,360
Bu ağ sadece verilere bakarak uygun ağırlıkları ve önyargıları nasıl öğreniyor?

243
00:16:20,140 --> 00:16:23,104
Bir sonraki videoda bunu göstereceğim ve ayrıca gördüğümüz 

244
00:16:23,104 --> 00:16:26,120
bu özel ağın gerçekte ne yaptığını biraz daha inceleyeceğim.

245
00:16:27,580 --> 00:16:30,823
Şimdi, sanırım video veya yeni videolar çıktığında haberdar 

246
00:16:30,823 --> 00:16:34,067
olmak için abone olun demem gerekiyor, ancak gerçekçi olmak 

247
00:16:34,067 --> 00:16:37,420
gerekirse çoğunuz YouTube'dan bildirim almıyorsunuz, değil mi?

248
00:16:38,020 --> 00:16:41,306
Belki de daha dürüstçe abone olun demeliyim ki YouTube'un öneri 

249
00:16:41,306 --> 00:16:43,771
algoritmasının temelini oluşturan sinir ağları, 

250
00:16:43,771 --> 00:16:47,880
bu kanaldaki içeriğin size önerilmesini istediğinize inanmaya hazır hale gelsin.

251
00:16:48,560 --> 00:16:49,940
Her neyse, daha fazlası için haberdar olun.

252
00:16:50,760 --> 00:16:53,500
Bu videoları Patreon'da destekleyen herkese çok teşekkür ederim.

253
00:16:54,000 --> 00:16:56,805
Bu yaz olasılık serisinde biraz yavaş ilerledim, 

254
00:16:56,805 --> 00:17:01,900
ancak bu projeden sonra tekrar başlıyorum, bu yüzden güncellemeleri orada bulabilirsiniz.

255
00:17:03,600 --> 00:17:07,304
Kapanışı yapmak üzere, derin öğrenmenin teorik yönü üzerine doktora çalışması 

256
00:17:07,304 --> 00:17:11,152
yapan ve şu anda bu videonun finansmanının bir kısmını sağlayan Amplify Partners 

257
00:17:11,152 --> 00:17:14,619
adlı bir girişim sermayesi şirketinde çalışan Leisha Lee ile birlikteyim.

258
00:17:15,460 --> 00:17:19,119
Bu yüzden Leisha, bu sigmoid fonksiyonundan hızlıca bahsetmemiz gerektiğini düşünüyorum.

259
00:17:19,700 --> 00:17:22,972
Anladığım kadarıyla ilk ağlar bunu, ilgili ağırlıklı toplamı sıfır ile 

260
00:17:22,972 --> 00:17:25,922
bir arasındaki aralığa sıkıştırmak için kullanıyor, bilirsiniz, 

261
00:17:25,922 --> 00:17:29,840
nöronların inaktif ya da aktif olduğu bu biyolojik analojiyle motive edilmiş bir tür.

262
00:17:30,280 --> 00:17:30,300
Aynen öyle.

263
00:17:30,560 --> 00:17:34,040
Ancak nispeten az sayıda modern ağ artık sigmoid kullanmaktadır.

264
00:17:34,320 --> 00:17:34,320
Evet.

265
00:17:34,440 --> 00:17:35,540
Biraz eski moda değil mi?

266
00:17:35,760 --> 00:17:38,980
Evet, daha doğrusu Relu'yu eğitmek çok daha kolay görünüyor.

267
00:17:39,400 --> 00:17:42,340
Ve relu, rektifiye edilmiş doğrusal birim anlamına mı geliyor?

268
00:17:42,680 --> 00:17:48,285
Evet, bu sadece sıfır ve a'nın maksimumunu aldığınız ve a'nın videoda açıkladığınız 

269
00:17:48,285 --> 00:17:53,690
şey tarafından verildiği bir tür işlevdir ve bunun bir tür motivasyonunun kısmen 

270
00:17:53,690 --> 00:17:59,362
nöronların nasıl aktive edileceği veya edilmeyeceği ile ilgili biyolojik bir analoji 

271
00:17:59,362 --> 00:18:04,767
olduğunu düşünüyorum ve bu nedenle belirli bir eşiği geçerse kimlik işlevi olur, 

272
00:18:04,767 --> 00:18:08,704
ancak geçmezse o zaman aktive olmaz, bu yüzden sıfır olur, 

273
00:18:08,704 --> 00:18:10,840
bu yüzden bir tür basitleştirme.

274
00:18:11,160 --> 00:18:17,889
Sigmoid kullanmak eğitime yardımcı olmadı ya da bir noktada eğitmek çok zordu ve insanlar 

275
00:18:17,889 --> 00:18:24,620
sadece relu'yu denediler ve bu inanılmaz derecede derin sinir ağları için çok iyi çalıştı.

276
00:18:25,100 --> 00:18:25,640
Pekâlâ, teşekkürler Alicia.

