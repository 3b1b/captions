1
00:00:04,020 --> 00:00:10,680
これは3です。これは、28ピクセル×28ピクセルの極端に低い解像度で作成され、書かれています。

2
00:00:10,680 --> 00:00:15,660
あなたの脳は問題なくそれを3と認識するでしょう。ここで、私はあなたに理解してもらうために少し時間をかけてほしいことがある。

3
00:00:15,900 --> 00:00:18,949
どのようにして脳が楽々と3だと認識することができたのか？

4
00:00:18,949 --> 00:00:23,160
これも、これも、そしてこれもまた3として認識される事を意味します、

5
00:00:23,160 --> 00:00:28,060
ある画像から次の画像に対して、各々のピクセルの特定の値が非常に異なる場合であっても。

6
00:00:28,080 --> 00:00:33,780
この3を見た時にあなたの目の中で発火している特定の視細胞と

7
00:00:33,780 --> 00:00:36,800
この3を見たときに発火している視細胞は全く別物です。

8
00:00:37,140 --> 00:00:40,610
しかし、あなたの非常に高性能な視覚野の中の何かが

9
00:00:41,129 --> 00:00:48,139
これらを同じアイデアを表すものとして理解すると同時に、他のイメージを別個のアイデアとして認識します

10
00:00:48,840 --> 00:00:55,039
しかし、もし私があなたにちょっと座って私のためにプログラムを書いてくれと言ったら。プログラムは、28×28ピクセルのグリッドを使用し、

11
00:00:55,379 --> 00:01:01,759
0から10の間の単一の数値を出力し、数字が何であるかをあなたに伝えるものです

12
00:01:02,250 --> 00:01:06,139
まあ、仕事は馬鹿げて些細なことから、非常に難しいものに変わります

13
00:01:06,750 --> 00:01:08,270
あなたの知識が非常に限られていない限り、

14
00:01:08,270 --> 00:01:14,599
私は機械学習と神経ネットワークの現在と将来の関連性と重要性を動機付けする必要はほとんどないと思う

15
00:01:14,640 --> 00:01:18,410
しかし、私がここでやりたいことは、実際に神経ネットワークが何であるかを示すことです

16
00:01:18,660 --> 00:01:24,229
背景がないと仮定し、それが流行語としてではなく、数学的なものとして何をしているのかを視覚化するのを助ける

17
00:01:24,570 --> 00:01:28,310
私の希望は、あなたが次のように感じられる事です。この構造そのものによって動機づけられ、

18
00:01:28,380 --> 00:01:34,399
あなたが「神経ネットワーク」学習に関して読んだり、聞いたりする際に、それが何を意味するかを知っているかのように感じる事です

19
00:01:34,950 --> 00:01:40,249
このビデオはそれの構造要素に専念するつもりであり、次のものは学習に取り掛かる予定です

20
00:01:40,530 --> 00:01:45,950
私たちがやろうとしていることは、手書きの数字を認識することを学ぶことができる神経ネットワークをまとめることです

21
00:01:49,270 --> 00:01:51,329
これはやや古典的な例です

22
00:01:51,520 --> 00:01:56,759
トピックを紹介して、私はここで現状を守る事を嬉しく思います。なぜなら2つのビデオの最後に、あなたに指摘したいのです

23
00:01:56,760 --> 00:02:02,099
これを行い、それを使って遊ぶ事ができるコードをどこでダウンロードし、どこで更に学べるかがわかるいくつかの良い素材を

24
00:02:02,100 --> 00:02:04,100
自分のコンピュータで

25
00:02:04,750 --> 00:02:08,970
近年、神経ネットワークには多くの種類があります。

26
00:02:08,970 --> 00:02:11,970
これらの種類に対する研究では、一種のブームがありました

27
00:02:12,130 --> 00:02:19,019
しかし、これらの2つの紹介ビデオでは、あなたと私は単純なフォームを見ようとしています

28
00:02:19,300 --> 00:02:21,040
これは必要な前提条件です

29
00:02:21,040 --> 00:02:24,510
より強力な現代の種類を理解するために

30
00:02:24,760 --> 00:02:28,199
私の心を包み込むためにはまだまだ複雑です。

31
00:02:28,690 --> 00:02:32,820
しかし、この最も単純な形式でも、手書きの数字を認識することを学ぶことができます

32
00:02:32,820 --> 00:02:36,180
これは、コンピュータが行うことができる非常にクールなものです。

33
00:02:37,120 --> 00:02:41,960
それと同時に、あなたが抱いている可能性があるいくつかの要望にはかけている事がわかります

34
00:02:43,090 --> 00:02:48,179
名前が示唆するように、神経ネットワークは脳に触発されていますが、それを打破しましょう

35
00:02:48,520 --> 00:02:51,389
神経とは何ですか？どのような意味で、それらは一緒につながっていますか？

36
00:02:52,090 --> 00:02:57,750
今、私がニューロンと言うとき、あなたが考えて欲しいのは、数字を保持するものです

37
00:02:58,209 --> 00:03:02,129
具体的には、0と1の間の数字です

38
00:03:03,430 --> 00:03:11,130
例えば、ネットワークは、入力画像の28×28画素のそれぞれに対応する一群の神経から始まります

39
00:03:11,400 --> 00:03:12,460
それは

40
00:03:12,460 --> 00:03:20,240
合計784個の神経の各々は、対応するピクセルのグレースケール値を表す数を保持します

41
00:03:20,769 --> 00:03:24,299
黒ピクセルを表す0から始まり、白ピクセルを表す1までの

42
00:03:24,910 --> 00:03:30,419
ニューロン内部のこの数字は活性化と呼ばれ、ここで想像しているかもしれないイメージです

43
00:03:30,420 --> 00:03:33,959
その活性化が高い数字のときに各ニューロンが点灯していますか？

44
00:03:36,260 --> 00:03:41,559
したがって、これらの784個のニューロンのすべてが私たちのネットワークの第1層を構成します

45
00:03:45,990 --> 00:03:51,289
最後のレイヤーにジャンプすると、10個のニューロンがそれぞれ数字の1つを表します

46
00:03:51,570 --> 00:03:56,239
0と1の間の数に対するこれらの神経の活性化は、

47
00:03:56,880 --> 00:04:00,049
与えられたイメージがシステムにどのくらいあると考えるかを表します。

48
00:04:00,720 --> 00:04:05,990
指定された桁に対応します。間には隠れたレイヤーと呼ばれる2つのレイヤーがあります

49
00:04:06,180 --> 00:04:07,770
それは当分の間ですか？

50
00:04:07,770 --> 00:04:13,549
地球上でどのように数字を認識するかについての巨大な疑問符でなければなりません。

51
00:04:13,740 --> 00:04:20,209
このネットワークでは、それぞれ16個のニューロンを持つ2つの隠れたレイヤーを選択しましたが、これは任意の選択肢です

52
00:04:20,609 --> 00:04:24,889
正直言って私はちょうどその瞬間に構造に動機づけをしたい、

53
00:04:25,350 --> 00:04:29,179
16はうまく実際に画面に収まるような素敵な数字でした

54
00:04:29,180 --> 00:04:32,209
ここには特定の構造の実験のための余地がたくさんあります

55
00:04:32,730 --> 00:04:38,329
ネットワークが1つの層で活性化を実行する方法は、次の層の活性化を決定します

56
00:04:38,760 --> 00:04:45,349
もちろん、情報処理の仕組みとしてのネットワークの核心は、

57
00:04:45,570 --> 00:04:48,409
1つの層からの活性化が、どのように次の層の活性化を引き起こしますかによります

58
00:04:48,900 --> 00:04:54,859
それは、神経の生物学的ネットワークにおいて、どのようにあるグループの神経点火がある他のグループを点火させるのかをゆるやかに比較する事を意味しています

59
00:04:55,410 --> 00:04:57,410
 

60
00:04:57,570 --> 00:04:58,340
今やネットワーク

61
00:04:58,340 --> 00:05:03,019
私がここに示しているのは、既に数字を認識するように訓練されており、それにより私が意味するものをお見せしています

62
00:05:03,140 --> 00:05:06,580
それは、もしあなたが画像内の各画素の輝度に応じて入力層の全ての784個の神経を輝かせた画像を与えたなら

63
00:05:06,640 --> 00:05:11,780
 

64
00:05:12,330 --> 00:05:17,029
その活性化のパターンは、次の層において非常に特異的なパターンを生じる

65
00:05:17,190 --> 00:05:19,309
どれが、一つのパターンの後の一つにおいて、いくつかのパターンを引き起こすのでしょうか？

66
00:05:19,440 --> 00:05:22,190
最終的には、どれが出力層にいくつかのパターンを与えるのでしょうか？そして

67
00:05:22,350 --> 00:05:29,359
その出力層の最も明るい神経はネットワークの選択であり、言わば、この画像がどの桁を表しているかという事である

68
00:05:32,070 --> 00:05:36,859
どのようにして1つのレイヤーが次のレイヤーに影響するか、または、どのようにしてトレーニングが機能するのかについての数学に飛び込む前に、

69
00:05:37,140 --> 00:05:43,069
何故、このような階層化された構造が賢く動作すると期待する事が将に合理的であるかについて話しましょう

70
00:05:43,800 --> 00:05:48,260
ここで何を期待していますか？それらの中間層が何をしているのかについての最適な望みは何ですか？

71
00:05:48,860 --> 00:05:56,720
あなたや私が数字を認識する時、さまざまな構成要素を一つにします。9は丸が上にあり、右に縦線があります

72
00:05:57,260 --> 00:06:01,280
8も上に丸がありますが、下にある他の丸と対になっています

73
00:06:02,020 --> 00:06:06,599
4は基本的に3つの特定の行とそのようなものに分解されます

74
00:06:07,180 --> 00:06:11,970
完璧な世界では、2番目から最後の層の各神経は、これらの副構成品の一つとして対応する

75
00:06:12,640 --> 00:06:14,729
 

76
00:06:14,890 --> 00:06:19,740
それは、あなたが画像を入力する度に、9または8のような上の丸を言います

77
00:06:19,870 --> 00:06:21,220
活性化が１に近づいているいくつの特定の神経があります

78
00:06:21,220 --> 00:06:27,749
私はこの特定の画像の丸を意味するわけではありません。

79
00:06:28,090 --> 00:06:35,039
一般的には、上にある丸のパターンは第3層から最後の層に向かう神経を導きます

80
00:06:35,380 --> 00:06:39,960
サブコンポーネントのどの組み合わせがどの桁に対応するかを学習するだけでよい

81
00:06:40,510 --> 00:06:42,810
もちろん、それは道のりで問題を引き起こすだけです

82
00:06:42,910 --> 00:06:49,019
あなたがこれらのサブコンポーネントをどのように認識するか、あるいは正しいサブコンポーネントがどんなものであるべきかを学ぶことさえあります。私はまだ説明していませんが

83
00:06:49,020 --> 00:06:52,829
どの層が次の層にどのように影響しますか？

84
00:06:53,650 --> 00:06:56,340
丸を認識することは、付随する問題に分解することもできます

85
00:06:56,860 --> 00:07:02,550
これを行うための1つの合理的な方法は、最初にそれを構成する様々な小さな端を認識することです

86
00:07:03,520 --> 00:07:08,910
同様に、数字1または4または7において、そのような長い線をみる事でしょう

87
00:07:08,910 --> 00:07:14,279
まあ、それは実際には長いエッジか多分あなたはいくつかの小さなエッジの特定のパターンとしてそれを考える

88
00:07:14,740 --> 00:07:19,379
だから、私たちの希望は、ネットワークの第2層の各ニューロン

89
00:07:20,290 --> 00:07:22,650
様々な関連する小さなエッジに対応する

90
00:07:23,230 --> 00:07:28,259
たぶん、このような画像が入ってくると、すべてのニューロンが点灯します

91
00:07:28,720 --> 00:07:31,649
約8〜10個の特定の小さなエッジに関連する

92
00:07:31,930 --> 00:07:36,930
それは上のループに関連するニューロンと長い垂直線を点灯させ、

93
00:07:37,300 --> 00:07:39,599
それらは9つのニューロンに関連している

94
00:07:40,300 --> 00:07:41,100
それとも

95
00:07:41,100 --> 00:07:47,070
これが私たちの最終的なネットワークが実際にやっていることです。もう一つの質問です。ネットワークを鍛える方法を見たら、私は戻ってきます。

96
00:07:47,350 --> 00:07:52,170
しかし、これは私たちが持つ可能性のある希望です。このような階層構造の目標

97
00:07:53,020 --> 00:07:59,340
さらに、このようなエッジやパターンをどのように検出することが他の画像認識タスクにとって本当に役に立つか想像することができます

98
00:07:59,740 --> 00:08:06,749
さらに、画像認識以外にも、抽象化のレイヤーに分解することができる、あらゆる種類のインテリジェントなものがあります

99
00:08:07,690 --> 00:08:14,670
例えば、音声を解析するには、生の音声を取り出し、結合して特定の音節

100
00:08:15,070 --> 00:08:19,829
フレーズとより抽象的な思考を組み立てるために結合する単語を形成するために結合するもの

101
00:08:20,770 --> 00:08:25,710
しかし、これが実際にどのように実際に動作するかに戻ることは、現在自分自身を描いています

102
00:08:25,710 --> 00:08:30,449
1つの層の活性化が次の層の活性化をどの程度正確に決定するか？

103
00:08:30,670 --> 00:08:35,879
目標は、ピクセルをエッジに組み合わせる可能性のあるメカニズムを持つことです

104
00:08:35,880 --> 00:08:41,430
パターンやパターンの辺へのエッジや、非常に具体的な例をズームインする

105
00:08:41,950 --> 00:08:44,189
希望が1つの特定のものだとしましょう

106
00:08:44,380 --> 00:08:50,430
2番目の層のニューロンは、画像がこの領域にエッジを持っているかどうかをここでピックアップします

107
00:08:50,950 --> 00:08:54,960
手元にある質問は、ネットワークがどのようなパラメータを持っているべきかということです

108
00:08:55,270 --> 00:09:02,490
どのようなダイヤルやノブを微調整して、このパターンを潜在的に十分に表現できるようにするか、

109
00:09:02,590 --> 00:09:07,290
他のピクセルパターンや、いくつかのエッジがループなどのことができるパターン？

110
00:09:08,290 --> 00:09:15,389
さて、私たちがすることは、最初の層からのニューロンとニューロンとの間の接続のそれぞれに重みを割り当てることです

111
00:09:15,850 --> 00:09:17,850
これらの重みはちょうど数字です

112
00:09:18,190 --> 00:09:25,590
最初のレイヤーからすべてのアクティベーションを取り、これらの重みIに従って重み付けされた合計を計算します

113
00:09:27,370 --> 00:09:31,680
これらの重みを自分の小さなグリッドに編成していると考えると役立ちます

114
00:09:31,680 --> 00:09:37,079
また、正の重みを示す緑のピクセルと負の重みを示す赤のピクセルを使用します

115
00:09:37,240 --> 00:09:41,670
そのピクセルの明るさは、重み値の何らかの緩やかな描写であるか？

116
00:09:42,400 --> 00:09:45,840
今では、ほとんどすべてのピクセルに関連付けられた重みをゼロ

117
00:09:46,150 --> 00:09:49,079
この地域のいくつかの肯定的な重みを除いて

118
00:09:49,480 --> 00:09:51,310
次に、

119
00:09:51,310 --> 00:09:57,690
すべてのピクセル値は実際には気になる領域のピクセル値を加算するだけです

120
00:09:58,870 --> 00:10:04,440
そして、あなたが本当にそれがここにエッジがあるかどうかを拾うことを望むなら、あなたがするかもしれないことは、いくつかの負の重みを持つことです

121
00:10:04,900 --> 00:10:06,900
周辺画素に関連する

122
00:10:07,030 --> 00:10:12,660
その中間のピクセルが明るいときに合計が最大になりますが、周囲のピクセルはより暗くなります

123
00:10:14,279 --> 00:10:18,169
このような加重和を計算すると、任意の数

124
00:10:18,240 --> 00:10:23,180
しかし、このネットワークでは、活性化が0＆1の間の値になるようにしたい

125
00:10:23,730 --> 00:10:26,599
そのため、一般的なことは、この加重和をポンピングすることです

126
00:10:26,910 --> 00:10:32,000
実数の線を0と1の間の範囲に縮めるいくつかの関数に

127
00:10:32,190 --> 00:10:37,249
これを行う一般的な関数は、ロジスティック曲線とも呼ばれるシグモイド関数と呼ばれます

128
00:10:37,980 --> 00:10:43,339
基本的に非常に負の入力はゼロに近づく非常に正の入力は1に近づく

129
00:10:43,339 --> 00:10:46,398
それはちょうど入力0の周りで着実に増加します

130
00:10:49,080 --> 00:10:56,029
したがって、ここでのニューロンの活性化は、基本的に、関連する加重和がどのようにプラスであるかの尺度である

131
00:10:57,450 --> 00:11:01,819
しかし、重み付けされた合計が0より大きい場合にニューロンを点灯させたいというわけではないかもしれません

132
00:11:02,100 --> 00:11:06,260
たぶんあなたは、合計が10より大きいときにアクティブにしたいだけかもしれません

133
00:11:06,630 --> 00:11:10,279
それはあなたがそれが非アクティブであるためのいくつかのバイアスが欲しいです

134
00:11:10,860 --> 00:11:16,099
私たちがやることは、この加重和に負の10のような他の数を加えるだけです

135
00:11:16,529 --> 00:11:19,669
Sigmoid squishification機能を使ってプラグを差し込む前に

136
00:11:20,220 --> 00:11:22,730
その追加数はバイアスと呼ばれます

137
00:11:23,310 --> 00:11:29,060
したがって、ウェイトは、2番目のレイヤーのこのニューロンがどのピクセルパターンでピックアップしているかを示し、バイアス

138
00:11:29,220 --> 00:11:35,450
ニューロンが有意義にアクティブになる前に、加重合計がどれくらい高い必要があるかを示します

139
00:11:35,910 --> 00:11:37,910
それはただのニューロンです

140
00:11:38,120 --> 00:11:41,940
この層の他のすべてのニューロンは、すべての

141
00:11:42,320 --> 00:11:50,620
第1の層からの784ピクセルのニューロンおよびこれらの784個の接続の各々はそれに関連するそれ自体の重みを有する

142
00:11:51,330 --> 00:11:57,739
また、それぞれにはいくつかのバイアスがあります。これは、シグモイドでそれを潰す前に加重合計に加算します。

143
00:11:58,020 --> 00:12:01,909
この16個のニューロンの隠れたレイヤーについて考えておくと、

144
00:12:02,010 --> 00:12:08,270
それは合計で784回の16回の重みと16回の偏りです

145
00:12:08,490 --> 00:12:14,029
そして、それはすべて、第1層から第2層への接続だけであり、他の層

146
00:12:14,029 --> 00:12:17,208
また、それらに関連した重さと偏りの束

147
00:12:17,760 --> 00:12:20,680
すべてのことが言って、このネットワークを行ったほとんど正確に

148
00:12:21,280 --> 00:12:23,920
合計13,000の重みと偏り

149
00:12:24,280 --> 00:12:29,540
このネットワークをさまざまな方法で動作させるために調整可能な13,000個のノブとダイヤル

150
00:12:30,520 --> 00:12:32,520
だから私たちは学習について話すとき？

151
00:12:32,530 --> 00:12:40,199
それは、実際に解決されるように、これらの多くの数字のすべてのための有効な設定を見つけるためにコンピュータを取得させることです

152
00:12:40,200 --> 00:12:42,190
手元の問題

153
00:12:42,190 --> 00:12:43,000
思考

154
00:12:43,000 --> 00:12:49,979
すぐに楽しいと恐ろしいの実験は、座っていることを想像し、これらの重みと偏りのすべてを手で設定することです

155
00:12:50,380 --> 00:12:56,159
意図的に数字を微調整して、2番目のレイヤーがエッジでピックアップし、3番目のレイヤーがパターンなどでピックアップするようにします。

156
00:12:56,350 --> 00:13:01,440
私は個人的には、ネットワーク全体をブラックボックスとして読むのではなく、

157
00:13:01,870 --> 00:13:04,349
ネットワークがあなたの

158
00:13:04,600 --> 00:13:11,370
それらの重みや偏りが実際にあなたが出発点を持つことを意味するものと少しの関係を築き上げているかどうかを予測してください

159
00:13:11,680 --> 00:13:16,289
ネットワークを改善するために、あるいはネットワークが機能するように構造を変更する方法を試してみてください。

160
00:13:16,290 --> 00:13:18,290
しかし、あなたが期待する理由ではありません

161
00:13:18,310 --> 00:13:25,169
重みと偏りが何をしているのかを掘り下げることは、あなたの前提に挑戦し、可能な限り完全な空間を明らかにする良い方法です

162
00:13:25,180 --> 00:13:26,350
ソリューション

163
00:13:26,350 --> 00:13:30,600
ところで、ここの実際の機能は書き留めるのが少し面倒です。あなたは思いませんか？

164
00:13:32,350 --> 00:13:38,460
ですから、これらの接続が表現されるより表記的にコンパクトな方法を私に教えてください。これはあなたがそれを見る方法です

165
00:13:38,460 --> 00:13:40,460
あなたがニューラルネットワークの詳細を読むことを選択した場合

166
00:13:41,110 --> 00:13:45,810
1つのレイヤーからすべてのアクティベーションをベクトルとして列に整理する

167
00:13:47,470 --> 00:13:52,320
次に、すべての重みを行列として編成し、その行列の各行

168
00:13:52,900 --> 00:13:57,659
1つの層と次の層の特定のニューロンとの間の接続に対応する

169
00:13:58,060 --> 00:14:03,599
その意味は、これらの重みに従って第1層の活性化の加重和を取ることですか？

170
00:14:04,000 --> 00:14:09,330
ここにあるすべてのものの行列ベクトル積の項の1つに対応します

171
00:14:13,540 --> 00:14:18,380
ところで、機械学習の多くは、線形代数をよく理解することに帰着します

172
00:14:18,380 --> 00:14:26,940
ですから、マトリックスの視覚的な理解を望む人や、行列ベクトルの乗算が線形代数で行ったシリーズ

173
00:14:27,250 --> 00:14:28,839
特に第3章

174
00:14:28,839 --> 00:14:35,759
これらの値のそれぞれにバイアスを追加することについて話すのではなく、私たちの表現に戻ると、

175
00:14:36,010 --> 00:14:42,209
それらのバイアスを全てベクトル化し、ベクトル全体を前の行列ベクトル積に加える

176
00:14:42,910 --> 00:14:44,040
次に、最終ステップとして

177
00:14:44,040 --> 00:14:47,250
私はここの外側のSigmoidをラップします

178
00:14:47,250 --> 00:14:51,899
そして、それが表すはずのものは、あなたがシグモイド関数をそれぞれの固有の関数に適用することです

179
00:14:52,420 --> 00:14:54,570
結果として生じるベクトルの内部のコンポーネント

180
00:14:55,510 --> 00:15:00,749
したがって、この重み行列とこれらのベクトルを独自の記号として書き留めると、次のことができます。

181
00:15:01,000 --> 00:15:07,589
非常にタイトできちんとした小さな表情で、あるレイヤーから次のレイヤーへのアクティベーションの完全な移行を伝え、

182
00:15:07,930 --> 00:15:15,000
これにより、多くのライブラリが行列の乗算を最適化するので、関連するコードがずっと簡単で多く高速になります

183
00:15:17,560 --> 00:15:21,359
私がこれらのニューロンが単に数字を保持するもの

184
00:15:21,790 --> 00:15:26,250
もちろん、それらが保持する特定の数値は、あなたがフィードした画像に依存します

185
00:15:27,790 --> 00:15:32,940
したがって、実際には、各ニューロンを

186
00:15:33,070 --> 00:15:38,070
前の層のすべてのニューロンの出力と0と1の間の数を吐き出す

187
00:15:38,800 --> 00:15:42,270
実際には、ネットワーク全体が機能しているだけです

188
00:15:42,760 --> 00:15:47,010
784個の数字を入力とし、10個の数字を出力として吐き出す

189
00:15:47,470 --> 00:15:48,700
それはばかげている

190
00:15:48,700 --> 00:15:56,249
特定のパターンを拾い上げるこれらの重みおよび偏りの形で13,000のパラメータを含む複雑な関数1

191
00:15:56,250 --> 00:16:00,270
多くの行列ベクトル積とシグモイドスカッシュ喚起関数を反復する

192
00:16:00,610 --> 00:16:06,390
それにもかかわらず、それは単なる関数であり、ある意味では、それは複雑に見える

193
00:16:06,390 --> 00:16:12,239
私たちが数字を認識することの挑戦に就くことができると思ったのは、もっと簡単なことでしたか？

194
00:16:12,960 --> 00:16:19,559
そしてそれはどのようにその挑戦にかかりますか？このネットワークは、データを見るだけで適切な重みとバイアスをどのように学習しますか？ああ？

195
00:16:20,080 --> 00:16:26,039
それは私が次のビデオで見せてくれるものです。私たちが見ているこの特定のネットワークが本当にやっていることをもう少し詳しく見ていきます

196
00:16:27,130 --> 00:16:32,640
今は、そのビデオや新しいビデオがいつ出てくるかを知らせてくれるように申し込むといいでしょう。

197
00:16:32,760 --> 00:16:37,560
しかし、現実的にはほとんどの人が実際にYouTubeからの通知を受け取っていません。

198
00:16:37,560 --> 00:16:42,260
もっと正直なところ、私はサブスクリプションと言って、YouTubeの基盤となるニューラルネットワーク

199
00:16:42,459 --> 00:16:47,639
推奨アルゴリズムは、あなたがこのチャンネルからのコンテンツをあなたに推薦されて見たいと思っている

200
00:16:48,250 --> 00:16:50,250
とにかく続きを続ける

201
00:16:50,410 --> 00:16:53,550
これらのビデオをパトリオンでサポートしてくださったみなさん、本当にありがとう

202
00:16:53,589 --> 00:16:56,759
私はこの夏の確率シリーズでは少し遅れています

203
00:16:56,760 --> 00:17:01,379
しかし、私はこのプロジェクトの後にそれにジャンプしているので、あなたはそこにある更新を見極めることができます

204
00:17:03,310 --> 00:17:05,550
私はここにいるものを閉じるには私と一緒にいるLisha Li

205
00:17:05,550 --> 00:17:12,029
リー氏は、博士号を博士課程で学び、理論的な学習の面で仕事をしています。現在、ベンチャーキャピタルで働く

206
00:17:12,030 --> 00:17:16,530
誰がこのビデオのための資金の一部を親切に提供したのですか？Lishaのこと

207
00:17:16,530 --> 00:17:19,109
私はすぐにこのシグモイド関数を呼び出すべきだと思います

208
00:17:19,180 --> 00:17:24,780
私が理解しているように、初期のネットワークはこれを使って関連する加重和をゼロと1の間の区間

209
00:17:24,980 --> 00:17:30,340
あなたは、この生物学的なニューロンの類推によって動かされていることを知っています。
（Lisha） - まさに

210
00:17:30,360 --> 00:17:36,320
しかし、現代のネットワークでは実際にシグモイドを実際に使用している人はほとんどいません（3B1B）。それは古い学校の権利のようなものですか？
（Lisha） - そうか、むしろ

211
00:17:36,370 --> 00:17:42,780
ReLUは訓練がはるかに簡単だと思われる
（3B1B） - そしてReLUは実際に整流されたリニアユニット

212
00:17:42,780 --> 00:17:48,839
（Lisha） - はい、この種の関数です。ここでは、最大値0をとり、

213
00:17:49,120 --> 00:17:53,670
あなたがビデオで何を説明していたのか、これが何かの動機付けだったのは

214
00:17:54,610 --> 00:17:56,610
部分的には生物学的

215
00:17:56,620 --> 00:17:58,179
方法のアナロジー

216
00:17:58,179 --> 00:18:03,089
ニューロンは活性化されていてもいなくてもよいので、ある閾値を超えると

217
00:18:03,250 --> 00:18:05,250
アイデンティティ関数

218
00:18:05,290 --> 00:18:10,439
しかし、そうでなければ、それは単に活性化されないのでゼロになるので、それは単純化の一種です

219
00:18:10,720 --> 00:18:14,429
シグモイドを使用しても訓練には役に立たなかったか、訓練するのが非常に困難でした

220
00:18:14,429 --> 00:18:19,589
それはある時点であり、人々はただ気分を変えようとしたばかりで、うまくいった

221
00:18:20,110 --> 00:18:22,140
非常にこれらの信じられないほどのために

222
00:18:22,690 --> 00:18:25,090
深いニューラルネットワーク。
（3B1B） - すべての権利

223
00:18:25,090 --> 00:18:26,060
ありがとうございますLisha

