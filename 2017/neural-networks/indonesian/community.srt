1
00:00:04,020 --> 00:00:10,680
Ini adalah tulisan angka tiga (3). Angka tersebut ditulis secara sembarangan dan ditampilkan dengan resolusi rendah berukuran 28x28 piksel.

2
00:00:10,680 --> 00:00:15,660
Tetapi, otak Anda tidak kesulitan mengenalinya sebagai tulisan angka tiga dan saya ingin mengajak Anda untuk meluangkan waktu sejenak untuk menghargai

3
00:00:15,900 --> 00:00:18,949
Seberapa gokil otak manusia dapat mengenalinya dengan mudah.

4
00:00:18,949 --> 00:00:23,160
Tulisan gambar ini dan gambar ini juga dikenali sebagai angka tiga,

5
00:00:23,160 --> 00:00:28,060
meskipun setiap pixel memiliki nilai yang berbeda antara satu gambar dengan gambar lainnya.

6
00:00:28,080 --> 00:00:33,780
Sel-sel peka cahaya tertentu di mata Anda-yang bekerja ketika Anda melihat angka tiga yang ini

7
00:00:33,780 --> 00:00:36,800
sangat berbeda dari yang bekerja ketika Anda melihat angka tiga yang ini.

8
00:00:37,140 --> 00:00:40,610
Tetapi sesuatu dalam korteks visual pintar yang gokil itu

9
00:00:41,129 --> 00:00:48,139
menyelesaikan permasalahan ini sebagai mewakili ide yang sama sementara pada saat yang sama mengenali gambar lain sebagai ide mereka sendiri yang berbeda

10
00:00:48,840 --> 00:00:55,039
Tetapi jika saya mengatakan kepada Anda hei duduk dan menulis untuk saya sebuah program yang mengambil grid 28 oleh 28

11
00:00:55,379 --> 00:01:01,759
piksel seperti ini dan mengeluarkan satu angka antara 0 dan 10 memberi tahu Anda apa yang dianggapnya digit

12
00:01:02,250 --> 00:01:06,139
Nah tugasnya dari lucu menjadi sangat sulit

13
00:01:06,750 --> 00:01:08,270
Kecuali Anda telah hidup di bawah batu karang

14
00:01:08,270 --> 00:01:14,599
Saya pikir saya hampir tidak perlu memotivasi relevansi dan pentingnya pembelajaran mesin dan jaringan syaraf ke masa sekarang ke masa depan

15
00:01:14,640 --> 00:01:18,410
Tapi yang ingin saya lakukan di sini adalah menunjukkan kepada Anda apa sebenarnya jaringan saraf itu

16
00:01:18,660 --> 00:01:24,229
Dengan asumsi tidak ada latar belakang dan untuk membantu memvisualisasikan apa yang dilakukannya bukan sebagai kata kunci tetapi sebagai bagian dari matematika

17
00:01:24,570 --> 00:01:28,310
Harapan saya adalah bahwa Anda datang dengan perasaan seperti struktur ini

18
00:01:28,380 --> 00:01:34,399
Termotivasi dan merasa seperti Anda tahu apa artinya ketika Anda membaca atau Anda mendengar tentang pembelajaran kutipan-tanda kutip jaringan saraf

19
00:01:34,950 --> 00:01:40,249
Video ini hanya akan dikhususkan untuk komponen struktur itu dan yang berikut ini akan menangani pembelajaran

20
00:01:40,530 --> 00:01:45,950
Apa yang akan kita lakukan adalah menyatukan jaringan saraf yang dapat belajar mengenali digit tulisan tangan

21
00:01:49,270 --> 00:01:51,329
Ini adalah contoh yang agak klasik untuk

22
00:01:51,520 --> 00:01:56,759
Memperkenalkan topik dan saya senang mengikuti status quo di sini karena di akhir dua video saya ingin tunjukkan

23
00:01:56,760 --> 00:02:02,099
Anda ke beberapa sumber yang bagus di mana Anda dapat belajar lebih banyak dan di mana Anda dapat mengunduh kode yang melakukan ini dan bermain dengannya?

24
00:02:02,100 --> 00:02:04,100
di komputer Anda sendiri

25
00:02:04,750 --> 00:02:08,970
Ada banyak banyak varian jaringan saraf dan dalam beberapa tahun terakhir

26
00:02:08,970 --> 00:02:11,970
Ada semacam ledakan dalam penelitian terhadap varian ini

27
00:02:12,130 --> 00:02:19,019
Namun dalam dua video pengantar ini Anda dan saya hanya akan melihat bentuk polos-vanila yang paling sederhana tanpa embel-embel tambahan

28
00:02:19,300 --> 00:02:21,040
Ini adalah hal yang diperlukan

29
00:02:21,040 --> 00:02:24,510
prasyarat untuk memahami salah satu varian modern yang lebih kuat dan

30
00:02:24,760 --> 00:02:28,199
Percaya padaku masih memiliki banyak kerumitan bagi kita untuk menyelimuti pikiran kita

31
00:02:28,690 --> 00:02:32,820
Tetapi bahkan dalam bentuk yang paling sederhana ini dapat belajar mengenali angka-angka tulisan tangan

32
00:02:32,820 --> 00:02:36,180
Yang merupakan hal yang cukup keren untuk komputer untuk dapat dilakukan.

33
00:02:37,120 --> 00:02:41,960
Dan pada saat yang sama Anda akan melihat bagaimana ia gagal memenuhi harapan pasangan yang mungkin kita miliki untuk itu

34
00:02:43,090 --> 00:02:48,179
Seperti namanya jaringan saraf terinspirasi oleh otak, tetapi mari kita hancurkan itu

35
00:02:48,520 --> 00:02:51,389
Apa neuron dan dalam arti apakah mereka terhubung bersama?

36
00:02:52,090 --> 00:02:57,750
Saat ini ketika saya mengatakan neuron yang saya ingin Anda pikirkan adalah hal yang memegang angka

37
00:02:58,209 --> 00:03:02,129
Khususnya angka antara 0 & 1 itu sebenarnya tidak lebih dari itu

38
00:03:03,430 --> 00:03:11,130
Misalnya jaringan dimulai dengan sekelompok neuron yang sesuai dengan masing-masing 28 kali 28 piksel dari gambar input

39
00:03:11,400 --> 00:03:12,460
yang mana

40
00:03:12,460 --> 00:03:20,240
784 neuron secara total masing-masing memegang nomor yang mewakili nilai grayscale dari pixel yang sesuai

41
00:03:20,769 --> 00:03:24,299
mulai dari 0 untuk piksel hitam hingga 1 untuk piksel putih

42
00:03:24,910 --> 00:03:30,419
Angka ini di dalam neuron disebut aktivasi dan gambar yang mungkin ada dalam pikiran Anda di sini

43
00:03:30,420 --> 00:03:33,959
Apakah setiap neuron menyala ketika aktivasi adalah angka yang tinggi?

44
00:03:36,260 --> 00:03:41,559
Jadi semua 784 neuron ini membentuk lapisan pertama jaringan kami

45
00:03:45,990 --> 00:03:51,289
Sekarang melompat ke lapisan terakhir ini memiliki sepuluh neuron masing-masing mewakili salah satu digit

46
00:03:51,570 --> 00:03:56,239
aktivasi di neuron-neuron ini lagi sejumlah angka antara nol dan satu

47
00:03:56,880 --> 00:04:00,049
Mewakili seberapa banyak sistem berpikir bahwa gambar yang diberikan?

48
00:04:00,720 --> 00:04:05,990
Sesuai dengan digit yang diberikan. Ada juga beberapa lapisan di antara yang disebut lapisan tersembunyi

49
00:04:06,180 --> 00:04:07,770
Yang mana untuk saat ini?

50
00:04:07,770 --> 00:04:13,549
Seharusnya hanya menjadi tanda tanya besar untuk bagaimana proses mengenali digit ini akan ditangani

51
00:04:13,740 --> 00:04:20,209
Dalam jaringan ini saya memilih dua lapisan tersembunyi masing-masing dengan 16 neuron dan diakui itu semacam pilihan yang sewenang-wenang

52
00:04:20,609 --> 00:04:24,889
Sejujurnya saya memilih dua lapisan berdasarkan pada bagaimana saya ingin memotivasi struktur hanya sesaat dan

53
00:04:25,350 --> 00:04:29,179
16 baik itu hanya angka yang bagus untuk muat di layar dalam praktek

54
00:04:29,180 --> 00:04:32,209
Ada banyak ruang untuk bereksperimen dengan struktur khusus di sini

55
00:04:32,730 --> 00:04:38,329
Cara jaringan mengoperasikan aktivasi dalam satu lapisan menentukan aktivasi lapisan berikutnya

56
00:04:38,760 --> 00:04:45,349
Dan tentu saja jantung jaringan sebagai mekanisme pemrosesan informasi turun ke bagaimana persisnya itu

57
00:04:45,570 --> 00:04:48,409
aktivasi dari satu lapisan membawa aktivasi di lapisan berikutnya

58
00:04:48,900 --> 00:04:54,859
Ini dimaksudkan untuk menjadi analog secara longgar dengan bagaimana dalam jaringan biologis neuron beberapa kelompok neuron yang menembak

59
00:04:55,410 --> 00:04:57,410
menyebabkan orang lain menembak

60
00:04:57,570 --> 00:04:58,340
Sekarang jaringannya

61
00:04:58,340 --> 00:05:03,019
Saya tunjukkan di sini telah dilatih untuk mengenali angka dan biarkan saya menunjukkan apa yang saya maksud dengan itu

62
00:05:03,140 --> 00:05:06,580
Itu berarti jika Anda memberi makan pada gambar yang menerangi semua

63
00:05:06,640 --> 00:05:11,780
784 neuron dari lapisan input sesuai dengan kecerahan setiap piksel dalam gambar

64
00:05:12,330 --> 00:05:17,029
Pola aktivasi tersebut menyebabkan beberapa pola yang sangat spesifik di lapisan berikutnya

65
00:05:17,190 --> 00:05:19,309
Yang menyebabkan beberapa pola di satu setelahnya?

66
00:05:19,440 --> 00:05:22,190
Yang akhirnya memberikan beberapa pola di lapisan output dan?

67
00:05:22,350 --> 00:05:29,359
Neuron paling terang dari lapisan output itu adalah pilihan jaringan sehingga bisa berbicara untuk apa digit gambar ini mewakili?

68
00:05:32,070 --> 00:05:36,859
Dan sebelum melompat ke matematika untuk bagaimana pengaruh satu lapisan berikutnya atau bagaimana pelatihan bekerja?

69
00:05:37,140 --> 00:05:43,069
Mari kita bicara tentang mengapa bahkan masuk akal untuk mengharapkan struktur berlapis seperti ini untuk berperilaku cerdas

70
00:05:43,800 --> 00:05:48,260
Apa yang kita harapkan di sini? Apa harapan terbaik untuk apa yang mungkin dilakukan oleh lapisan menengah?

71
00:05:48,860 --> 00:05:56,720
Baik ketika Anda atau saya mengenali angka, kami menggabungkan berbagai komponen yang sembilan memiliki loop atas dan garis di sebelah kanan

72
00:05:57,260 --> 00:06:01,280
angka 8 juga memiliki lingkaran di bagian atas, tetapi dipasangkan dengan lingkaran lain ke bawah

73
00:06:02,020 --> 00:06:06,599
A 4 pada dasarnya terurai menjadi tiga garis spesifik dan hal-hal seperti itu

74
00:06:07,180 --> 00:06:11,970
Sekarang di dunia yang sempurna kita mungkin berharap setiap neuron di lapisan kedua hingga terakhir

75
00:06:12,640 --> 00:06:14,729
sesuai dengan salah satu dari sub komponen ini

76
00:06:14,890 --> 00:06:19,740
Itu setiap kali Anda memberi makan gambar dengan mengatakan lingkaran di bagian atas seperti 9 atau 8

77
00:06:19,870 --> 00:06:21,220
Ada beberapa yang spesifik

78
00:06:21,220 --> 00:06:27,749
Neuron yang aktivasi akan menjadi dekat dengan satu dan saya tidak bermaksud ini loop piksel spesifik harapan akan menjadi apa pun

79
00:06:28,090 --> 00:06:35,039
Umumnya pola berliku ke arah atas memicu neuron ini yang pergi dari lapisan ketiga ke yang terakhir

80
00:06:35,380 --> 00:06:39,960
hanya membutuhkan pembelajaran yang kombinasi dari komponen sub sesuai dengan digit

81
00:06:40,510 --> 00:06:42,810
Tentu saja itu hanya menendang masalah di jalan

82
00:06:42,910 --> 00:06:49,019
Karena bagaimana Anda akan mengenali sub komponen ini atau bahkan mempelajari sub komponen yang tepat dan saya masih belum membicarakannya

83
00:06:49,020 --> 00:06:52,829
Bagaimana satu lapisan mempengaruhi yang berikutnya tetapi jalankan dengan saya yang satu ini untuk sesaat

84
00:06:53,650 --> 00:06:56,340
mengenali suatu loop juga dapat terurai menjadi subproblem

85
00:06:56,860 --> 00:07:02,550
Satu cara yang masuk akal untuk melakukan ini adalah dengan mengenali berbagai sisi kecil yang membuatnya

86
00:07:03,520 --> 00:07:08,910
Demikian pula garis panjang seperti jenis yang mungkin Anda lihat di digit 1 atau 4 atau 7

87
00:07:08,910 --> 00:07:14,279
Nah itu benar-benar hanya tepi panjang atau mungkin Anda menganggapnya sebagai pola tertentu dari beberapa sisi yang lebih kecil

88
00:07:14,740 --> 00:07:19,379
Jadi mungkin harapan kami adalah setiap neuron di lapisan kedua jaringan

89
00:07:20,290 --> 00:07:22,650
sesuai dengan berbagai sisi kecil yang relevan

90
00:07:23,230 --> 00:07:28,259
Mungkin ketika gambar seperti ini muncul di dalamnya menyala semua neuron

91
00:07:28,720 --> 00:07:31,649
terkait dengan sekitar delapan hingga sepuluh tepian kecil tertentu

92
00:07:31,930 --> 00:07:36,930
yang pada gilirannya menyalakan neuron yang terkait dengan loop atas dan garis vertikal panjang dan

93
00:07:37,300 --> 00:07:39,599
Mereka menyalakan neuron yang terkait dengan sembilan

94
00:07:40,300 --> 00:07:41,100
apakah atau tidak

95
00:07:41,100 --> 00:07:47,070
Inilah yang sebenarnya dilakukan oleh jaringan kami yang terakhir adalah pertanyaan lain, yang akan saya kembalikan setelah kita melihat cara melatih jaringan

96
00:07:47,350 --> 00:07:52,170
Tetapi ini adalah harapan yang mungkin kita miliki. Semacam tujuan dengan struktur berlapis seperti ini

97
00:07:53,020 --> 00:07:59,340
Selain itu, Anda dapat membayangkan bagaimana kemampuan mendeteksi tepi dan pola seperti ini akan sangat berguna untuk tugas pengenalan gambar lainnya

98
00:07:59,740 --> 00:08:06,749
Dan bahkan di luar pengenalan gambar ada segala macam hal cerdas yang mungkin ingin Anda lakukan yang terurai menjadi lapisan abstraksi

99
00:08:07,690 --> 00:08:14,670
Mengurai pidato misalnya melibatkan pengambilan audio mentah dan memilih suara berbeda yang digabungkan untuk membuat suku kata tertentu

100
00:08:15,070 --> 00:08:19,829
Yang menggabungkan untuk membentuk kata-kata yang menggabungkan untuk membuat frase dan lebih banyak pemikiran abstrak dll

101
00:08:20,770 --> 00:08:25,710
Tetapi kembali ke bagaimana semua ini benar-benar bekerja menggambarkan diri Anda saat ini merancang

102
00:08:25,710 --> 00:08:30,449
Bagaimana sebenarnya aktivasi dalam satu lapisan dapat menentukan aktivasi di lapisan berikutnya?

103
00:08:30,670 --> 00:08:35,879
Tujuannya adalah memiliki mekanisme yang bisa menggabungkan piksel ke dalam beberapa sisi

104
00:08:35,880 --> 00:08:41,430
Atau tepi ke pola atau pola ke dalam digit dan memperbesar satu contoh yang sangat spesifik

105
00:08:41,950 --> 00:08:44,189
Katakan saja harapannya adalah untuk satu hal

106
00:08:44,380 --> 00:08:50,430
Neuron di lapisan kedua untuk mengetahui apakah gambar memiliki kelebihan di wilayah ini di sini

107
00:08:50,950 --> 00:08:54,960
Pertanyaannya adalah apa parameter yang harus dimiliki jaringan

108
00:08:55,270 --> 00:09:02,490
apa tombol dan tombol yang harus Anda dapat menyesuaikan sehingga cukup ekspresif untuk berpotensi menangkap pola ini atau

109
00:09:02,590 --> 00:09:07,290
Pola piksel lain atau pola yang beberapa tepinya dapat membuat lingkaran dan hal-hal lain seperti itu?

110
00:09:08,290 --> 00:09:15,389
Nah, apa yang akan kita lakukan adalah menetapkan bobot untuk setiap koneksi antara neuron kita dan neuron dari lapisan pertama

111
00:09:15,850 --> 00:09:17,850
Bobot ini hanyalah angka

112
00:09:18,190 --> 00:09:25,590
kemudian ambil semua aktivasi dari lapisan pertama dan hitung jumlah tertimbang mereka sesuai dengan bobot ini

113
00:09:27,370 --> 00:09:31,680
Temukan manfaat untuk memikirkan bobot ini sebagai diatur ke dalam kotak kecil mereka sendiri

114
00:09:31,680 --> 00:09:37,079
Dan saya akan menggunakan piksel hijau untuk menunjukkan bobot positif dan piksel merah untuk menunjukkan bobot negatif

115
00:09:37,240 --> 00:09:41,670
Di mana kecerahan pixel adalah penggambaran yang longgar dari nilai bobot?

116
00:09:42,400 --> 00:09:45,840
Sekarang jika kita membuat bobot yang terkait dengan hampir semua piksel nol

117
00:09:46,150 --> 00:09:49,079
kecuali untuk beberapa bobot positif di wilayah ini yang kita pedulikan

118
00:09:49,480 --> 00:09:51,310
kemudian mengambil jumlah tertimbang

119
00:09:51,310 --> 00:09:57,690
semua nilai piksel benar-benar hanya berjumlah menambahkan nilai-nilai piksel hanya di wilayah yang kita sayangi

120
00:09:58,870 --> 00:10:04,440
Dan, jika Anda benar-benar ingin mengetahui apakah ada kelebihan di sini apa yang mungkin Anda lakukan adalah memiliki bobot negatif

121
00:10:04,900 --> 00:10:06,900
terkait dengan piksel sekitarnya

122
00:10:07,030 --> 00:10:12,660
Maka jumlah terbesar ketika piksel tengah itu cerah, tetapi piksel sekitarnya lebih gelap

123
00:10:14,279 --> 00:10:18,169
Ketika Anda menghitung jumlah tertimbang seperti ini Anda mungkin keluar dengan nomor apa pun

124
00:10:18,240 --> 00:10:23,180
tetapi untuk jaringan ini yang kita inginkan adalah untuk aktivasi menjadi nilai antara 0 & 1

125
00:10:23,730 --> 00:10:26,599
jadi hal yang umum dilakukan adalah memompa penjumlahan tertimbang ini

126
00:10:26,910 --> 00:10:32,000
Ke beberapa fungsi yang squishes garis bilangan real ke dalam rentang antara 0 & 1 dan

127
00:10:32,190 --> 00:10:37,249
Fungsi umum yang melakukan ini disebut fungsi sigmoid juga dikenal sebagai kurva logistik

128
00:10:37,980 --> 00:10:43,339
pada dasarnya input yang sangat negatif berakhir dekat dengan nol input yang sangat positif berakhir mendekati 1

129
00:10:43,339 --> 00:10:46,398
dan itu terus meningkat di sekitar input 0

130
00:10:49,080 --> 00:10:56,029
Jadi aktivasi neuron di sini pada dasarnya adalah ukuran seberapa positif jumlah tertimbang yang relevan

131
00:10:57,450 --> 00:11:01,819
Tapi mungkin bukan karena Anda ingin neuron menyala ketika jumlah tertimbang lebih besar dari 0

132
00:11:02,100 --> 00:11:06,260
Mungkin Anda hanya ingin aktif ketika jumlahnya lebih besar daripada 10

133
00:11:06,630 --> 00:11:10,279
Itu adalah Anda ingin beberapa bias untuk itu menjadi tidak aktif

134
00:11:10,860 --> 00:11:16,099
apa yang akan kita lakukan adalah menambahkan angka lain seperti negatif 10 ke jumlah tertimbang ini

135
00:11:16,529 --> 00:11:19,669
Sebelum menghubungkannya melalui fungsi squishification sigmoid

136
00:11:20,220 --> 00:11:22,730
Nomor tambahan itu disebut bias

137
00:11:23,310 --> 00:11:29,060
Jadi bobot memberi tahu Anda apa pola piksel neuron ini di lapisan kedua adalah menangkap dan bias

138
00:11:29,220 --> 00:11:35,450
memberitahu Anda seberapa tinggi jumlah tertimbang harus sebelum neuron mulai menjadi aktif penuh makna

139
00:11:35,910 --> 00:11:37,910
Dan itu hanya satu neuron

140
00:11:38,120 --> 00:11:41,940
Setiap neuron lain di lapisan ini akan terhubung ke semua

141
00:11:42,320 --> 00:11:50,620
784 piksel neuron dari lapisan pertama dan masing-masing dari 784 koneksi memiliki beratnya sendiri yang terkait dengannya

142
00:11:51,330 --> 00:11:57,739
juga masing-masing memiliki beberapa bias beberapa nomor lain yang Anda tambahkan ke jumlah tertimbang sebelum meremasnya dengan sigmoid dan

143
00:11:58,020 --> 00:12:01,909
Itu banyak yang harus dipikirkan dengan lapisan tersembunyi dari 16 neuron ini

144
00:12:02,010 --> 00:12:08,270
itu total 784 kali 16 bobot bersama dengan 16 bias

145
00:12:08,490 --> 00:12:14,029
Dan semua itu hanyalah koneksi dari lapisan pertama ke kedua koneksi antara lapisan lain

146
00:12:14,029 --> 00:12:17,208
Juga, memiliki banyak bobot dan bias yang terkait dengannya

147
00:12:17,760 --> 00:12:20,680
Semua yang dikatakan dan dilakukan jaringan ini hampir persis

148
00:12:21,280 --> 00:12:23,920
13.000 bobot total dan bias

149
00:12:24,280 --> 00:12:29,540
13.000 kenop dan tombol yang dapat diubah dan diubah untuk membuat jaringan ini berperilaku dengan cara yang berbeda

150
00:12:30,520 --> 00:12:32,520
Jadi kapan kita berbicara tentang belajar?

151
00:12:32,530 --> 00:12:40,199
Apa yang dimaksud adalah mendapatkan komputer untuk menemukan pengaturan yang valid untuk semua banyak banyak angka ini sehingga benar-benar akan menyelesaikannya

152
00:12:40,200 --> 00:12:42,190
masalah yang dihadapi

153
00:12:42,190 --> 00:12:43,000
satu pikiran

154
00:12:43,000 --> 00:12:49,979
Eksperimen yang menyenangkan sekaligus menyenangkan dan mengerikan adalah membayangkan duduk dan mengatur semua bobot dan bias ini dengan tangan

155
00:12:50,380 --> 00:12:56,159
Dengan sengaja mengutak-atik angka sehingga layer kedua mengambil di tepi layer ketiga mengambil pada pola dll

156
00:12:56,350 --> 00:13:01,440
Saya pribadi menemukan ini memuaskan daripada hanya membaca jaringan sebagai kotak hitam total

157
00:13:01,870 --> 00:13:04,349
Karena ketika jaringan tidak melakukan cara Anda

158
00:13:04,600 --> 00:13:11,370
antisipasi jika Anda telah membangun sedikit hubungan dengan apa yang bobot dan bias itu benar-benar berarti Anda memiliki tempat awal

159
00:13:11,680 --> 00:13:16,289
Bereksperimen dengan cara mengubah struktur untuk meningkatkan atau saat jaringan berfungsi?

160
00:13:16,290 --> 00:13:18,290
Tapi bukan karena alasan yang Anda harapkan

161
00:13:18,310 --> 00:13:25,169
Menggali apa yang dilakukan bobot dan bias adalah cara yang baik untuk menantang asumsi Anda dan benar-benar mengekspos ruang penuh kemungkinan

162
00:13:25,180 --> 00:13:26,350
solusi

163
00:13:26,350 --> 00:13:30,600
By the way fungsi sebenarnya di sini adalah sedikit rumit untuk menulis. T Anda berpikir?

164
00:13:32,350 --> 00:13:38,460
Jadi biarkan saya menunjukkan kepada Anda cara yang lebih praktis dan kompak agar koneksi ini diwakili. Ini adalah bagaimana Anda akan melihatnya

165
00:13:38,460 --> 00:13:40,460
Jika Anda memilih untuk membaca lebih lanjut tentang jaringan saraf

166
00:13:41,110 --> 00:13:45,810
Atur semua aktivasi dari satu lapisan ke kolom sebagai vektor

167
00:13:47,470 --> 00:13:52,320
Kemudian mengatur semua bobot sebagai matriks di mana setiap baris dari matriks itu

168
00:13:52,900 --> 00:13:57,659
sesuai dengan koneksi antara satu lapisan dan neuron tertentu di lapisan berikutnya

169
00:13:58,060 --> 00:14:03,599
Apa itu artinya mengambil jumlah tertimbang dari aktivasi di lapisan pertama sesuai dengan bobot ini?

170
00:14:04,000 --> 00:14:09,330
Sesuai dengan salah satu istilah dalam produk vektor matriks dari semua yang kita miliki di sebelah kiri di sini

171
00:14:13,540 --> 00:14:18,380
Dengan begitu banyak pembelajaran mesin hanya datang untuk memiliki pemahaman yang baik dari aljabar linier

172
00:14:18,380 --> 00:14:26,940
Jadi bagi Anda yang menginginkan pemahaman visual yang bagus untuk matriks dan apa yang dimaksud dengan perkalian vektor matriks, lihatlah seri yang saya lakukan pada aljabar linier.

173
00:14:27,250 --> 00:14:28,839
terutama bab tiga

174
00:14:28,839 --> 00:14:35,759
Kembali ke ekspresi kami alih-alih berbicara tentang menambahkan bias ke masing-masing dari nilai-nilai ini secara mandiri kami mewakilinya

175
00:14:36,010 --> 00:14:42,209
Mengatur semua bias tersebut ke dalam vektor dan menambahkan seluruh vektor ke produk vektor matriks sebelumnya

176
00:14:42,910 --> 00:14:44,040
Kemudian sebagai langkah terakhir

177
00:14:44,040 --> 00:14:47,250
Saya akan rap sigmoid di sekitar luar sini

178
00:14:47,250 --> 00:14:51,899
Dan apa yang seharusnya diwakilinya adalah Anda akan menerapkan fungsi sigmoid untuk masing-masing spesifik

179
00:14:52,420 --> 00:14:54,570
komponen dari vektor yang dihasilkan di dalam

180
00:14:55,510 --> 00:15:00,749
Jadi, begitu Anda menuliskan matriks bobot ini dan vektor-vektor ini sebagai simbol mereka sendiri, Anda bisa

181
00:15:01,000 --> 00:15:07,589
mengomunikasikan transisi penuh aktivasi dari satu lapisan ke lapisan berikutnya dalam ekspresi kecil yang sangat ketat dan rapi

182
00:15:07,930 --> 00:15:15,000
Hal ini membuat kode yang relevan menjadi jauh lebih sederhana dan lebih cepat karena banyak pustaka yang mengoptimalkan keluar dari perkalian matriks

183
00:15:17,560 --> 00:15:21,359
Ingat bagaimana sebelumnya saya mengatakan neuron ini hanyalah hal-hal yang memiliki angka

184
00:15:21,790 --> 00:15:26,250
Yah tentu saja nomor tertentu yang mereka pegang tergantung pada gambar yang Anda masukkan

185
00:15:27,790 --> 00:15:32,940
Jadi sebenarnya lebih akurat untuk memikirkan setiap neuron sebagai fungsi yang mengambil alih

186
00:15:33,070 --> 00:15:38,070
output semua neuron pada lapisan sebelumnya dan mengeluarkan angka antara nol dan satu

187
00:15:38,800 --> 00:15:42,270
Benar-benar seluruh jaringan hanyalah fungsi yang mengambil alih

188
00:15:42,760 --> 00:15:47,010
784 angka sebagai input dan mengeluarkan sepuluh angka sebagai output

189
00:15:47,470 --> 00:15:48,700
Ini tidak masuk akal

190
00:15:48,700 --> 00:15:56,249
Fungsi rumit yang melibatkan tiga belas ribu parameter dalam bentuk bobot dan bias yang mengambil pola-pola tertentu dan yang melibatkan

191
00:15:56,250 --> 00:16:00,270
iterasi banyak produk vektor matriks dan fungsi penghamburan sigmoid squish

192
00:16:00,610 --> 00:16:06,390
Tapi itu hanya fungsi tetap dan dengan cara itu agak meyakinkan bahwa itu terlihat rumit

193
00:16:06,390 --> 00:16:12,239
Maksud saya jika ada yang lebih sederhana, harapan apa yang akan kita miliki, yang dapat mengambil tantangan mengenali digit?

194
00:16:12,960 --> 00:16:19,559
Dan bagaimana cara mengambil tantangan itu? Bagaimana jaringan ini mempelajari bobot dan bias yang tepat hanya dengan melihat data? Oh

195
00:16:20,080 --> 00:16:26,039
Itulah yang akan saya tunjukkan di video berikutnya, dan saya juga akan menggali sedikit lebih ke dalam apa yang jaringan kami lihat ini benar-benar lakukan

196
00:16:27,130 --> 00:16:32,640
Sekarang adalah titik saya kira saya harus mengatakan berlangganan untuk tetap diberitahu tentang kapan video itu atau video baru keluar

197
00:16:32,760 --> 00:16:37,560
Tetapi secara realistis sebagian besar dari Anda tidak benar-benar menerima pemberitahuan dari YouTube, bukan?

198
00:16:37,560 --> 00:16:42,260
Mungkin lebih jujur ​​saya harus mengatakan berlangganan agar jaringan syaraf yang mendasari YouTube

199
00:16:42,459 --> 00:16:47,639
Algoritme rekomendasi disiapkan untuk meyakini bahwa Anda ingin melihat konten dari saluran ini direkomendasikan bagi Anda

200
00:16:48,250 --> 00:16:50,250
tetap tinggal diposting untuk lebih

201
00:16:50,410 --> 00:16:53,550
Terima kasih banyak kepada semua orang yang mendukung video ini pada patreon

202
00:16:53,589 --> 00:16:56,759
Saya sudah agak lambat untuk maju dalam seri probabilitas musim panas ini

203
00:16:56,760 --> 00:17:01,379
Tapi aku melompat kembali ke dalamnya setelah proyek ini sehingga pelanggan Anda dapat melihat pembaruan di sana

204
00:17:03,310 --> 00:17:05,550
Untuk menutup hal-hal di sini saya miliki dengan saya Lisha Li

205
00:17:05,550 --> 00:17:12,029
Lee yang melakukan pekerjaan PhD-nya pada sisi teoritis dari pembelajaran yang mendalam dan yang saat ini bekerja di sebuah perusahaan modal ventura yang disebut memperkuat mitra

206
00:17:12,030 --> 00:17:16,530
Yang dengan senang hati memberikan sebagian dana untuk video ini jadi Lisha satu hal

207
00:17:16,530 --> 00:17:19,109
Saya pikir kita harus segera memunculkan fungsi sigmoid ini

208
00:17:19,180 --> 00:17:24,780
Seperti yang saya pahami, jaringan awal menggunakan ini untuk menekan jumlah tertimbang yang relevan ke interval antara nol dan satu

209
00:17:24,980 --> 00:17:30,340
Anda tahu jenis termotivasi oleh analogi biologis neuron ini entah tidak aktif atau aktif
(Lisha) - Tepat sekali

210
00:17:30,360 --> 00:17:36,320
(3B1B) - Tetapi relatif sedikit jaringan modern yang benar-benar menggunakan sigmoid lagi. Itu semacam sekolah lama kan?
(Lisha) - Ya atau lebih tepatnya

211
00:17:36,370 --> 00:17:42,780
ReLU tampaknya jauh lebih mudah untuk dilatih
(3B1B) - Dan ReLU benar-benar mewakili unit linear yang diperbaiki

212
00:17:42,780 --> 00:17:48,839
(Lisha) - Ya, ini semacam fungsi di mana Anda hanya mengambil maksimal 0 dan di mana a diberikan

213
00:17:49,120 --> 00:17:53,670
apa yang Anda jelaskan di dalam video dan apa yang dimotivasi oleh saya

214
00:17:54,610 --> 00:17:56,610
sebagian oleh biologis

215
00:17:56,620 --> 00:17:58,179
Analogi dengan caranya

216
00:17:58,179 --> 00:18:03,089
Neuron akan diaktifkan atau tidak dan jika melewati ambang tertentu

217
00:18:03,250 --> 00:18:05,250
Itu akan menjadi fungsi identitas

218
00:18:05,290 --> 00:18:10,439
Tetapi jika tidak maka itu tidak akan diaktifkan jadi nol jadi semacam penyederhanaan

219
00:18:10,720 --> 00:18:14,429
Menggunakan sigmoids tidak membantu pelatihan, atau sangat sulit untuk dilatih

220
00:18:14,429 --> 00:18:19,589
Itu pada titik tertentu dan orang-orang baru saja mencoba relu dan kebetulan berhasil

221
00:18:20,110 --> 00:18:22,140
Sangat baik untuk ini luar biasa

222
00:18:22,690 --> 00:18:25,090
Jaringan saraf dalam.
(3B1B) - Baiklah

223
00:18:25,090 --> 00:18:26,060
Terima kasih, Lisha

