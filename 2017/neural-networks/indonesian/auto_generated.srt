1
00:00:04,220 --> 00:00:05,400
Ini adalah nilai 3.

2
00:00:06,060 --> 00:00:09,753
Tulisan ini ditulis dengan ceroboh dan dirender pada resolusi yang sangat rendah 

3
00:00:09,753 --> 00:00:13,720
yaitu 28x28 piksel, tetapi otak Anda tidak akan kesulitan mengenalinya sebagai angka 3.

4
00:00:14,340 --> 00:00:16,767
Dan saya ingin Anda meluangkan waktu sejenak untuk menghargai 

5
00:00:16,767 --> 00:00:18,960
betapa gilanya otak bisa melakukan hal ini dengan mudah.

6
00:00:19,700 --> 00:00:23,029
Maksud saya, ini, ini dan ini juga dikenali sebagai 3s, 

7
00:00:23,029 --> 00:00:28,320
meskipun nilai spesifik tiap piksel sangat berbeda dari satu gambar ke gambar berikutnya.

8
00:00:28,900 --> 00:00:33,378
Sel-sel peka cahaya tertentu di mata Anda yang menembak ketika Anda melihat 3 ini, 

9
00:00:33,378 --> 00:00:36,940
sangat berbeda dengan sel yang menembak ketika Anda melihat 3 ini.

10
00:00:37,520 --> 00:00:41,006
Tetapi, sesuatu dalam korteks visual Anda yang sangat cerdas, 

11
00:00:41,006 --> 00:00:43,874
memutuskan bahwa semua ini mewakili ide yang sama, 

12
00:00:43,874 --> 00:00:48,260
sementara pada saat yang sama, mengenali gambar lain sebagai ide yang berbeda.

13
00:00:49,220 --> 00:00:53,542
Tetapi jika saya mengatakan kepada Anda, hei, duduklah dan tuliskan untuk saya 

14
00:00:53,542 --> 00:00:58,028
sebuah program yang mengambil kisi-kisi 28x28 piksel seperti ini dan menghasilkan 

15
00:00:58,028 --> 00:01:02,240
satu angka antara 0 dan 10, memberi tahu Anda apa yang dipikirkannya tentang 

16
00:01:02,240 --> 00:01:06,180
angka tersebut, nah, tugas ini berubah dari sepele menjadi sangat sulit.

17
00:01:07,160 --> 00:01:10,628
Kecuali jika Anda telah hidup di bawah batu, saya rasa saya tidak perlu memotivasi 

18
00:01:10,628 --> 00:01:14,389
relevansi dan pentingnya pembelajaran mesin dan jaringan saraf untuk saat ini dan di masa 

19
00:01:14,389 --> 00:01:14,640
depan.

20
00:01:15,120 --> 00:01:18,082
Namun, yang ingin saya lakukan di sini adalah menunjukkan kepada Anda apa sebenarnya 

21
00:01:18,082 --> 00:01:20,173
neural network itu, dengan asumsi tidak ada latar belakang, 

22
00:01:20,173 --> 00:01:22,264
dan untuk membantu memvisualisasikan apa yang dilakukannya, 

23
00:01:22,264 --> 00:01:24,460
bukan sebagai kata kunci tetapi sebagai bagian dari matematika.

24
00:01:25,020 --> 00:01:28,765
Harapan saya adalah bahwa Anda akan merasa termotivasi dengan struktur itu sendiri, 

25
00:01:28,765 --> 00:01:31,441
dan merasa bahwa Anda tahu apa artinya ketika Anda membaca, 

26
00:01:31,441 --> 00:01:34,340
atau mendengar tentang pembelajaran quote-unquote jaringan saraf.

27
00:01:35,360 --> 00:01:37,835
Video ini hanya akan membahas komponen struktur, 

28
00:01:37,835 --> 00:01:40,260
dan video berikutnya akan membahas pembelajaran.

29
00:01:40,960 --> 00:01:43,544
Apa yang akan kita lakukan adalah menyusun jaringan saraf 

30
00:01:43,544 --> 00:01:46,040
yang dapat belajar mengenali angka-angka tulisan tangan.

31
00:01:49,360 --> 00:01:51,892
Ini adalah contoh yang agak klasik untuk memperkenalkan topik ini, 

32
00:01:51,892 --> 00:01:53,971
dan saya senang untuk tetap dengan status quo di sini, 

33
00:01:53,971 --> 00:01:56,616
karena di akhir dua video saya ingin menunjukkan kepada Anda beberapa 

34
00:01:56,616 --> 00:01:59,186
sumber daya yang bagus di mana Anda dapat mempelajari lebih lanjut, 

35
00:01:59,186 --> 00:02:02,135
dan di mana Anda dapat mengunduh kode yang melakukan hal ini dan memainkannya 

36
00:02:02,135 --> 00:02:03,080
di komputer Anda sendiri.

37
00:02:05,040 --> 00:02:09,020
Ada banyak sekali varian neural network, dan dalam beberapa tahun terakhir, 

38
00:02:09,020 --> 00:02:12,110
ada semacam ledakan penelitian terhadap varian-varian ini, 

39
00:02:12,110 --> 00:02:16,613
tetapi dalam dua video perkenalan ini, Anda dan saya hanya akan melihat bentuk vanila 

40
00:02:16,613 --> 00:02:19,180
yang paling sederhana tanpa embel-embel tambahan.

41
00:02:19,860 --> 00:02:25,011
Ini merupakan prasyarat yang diperlukan untuk memahami varian modern yang lebih canggih, 

42
00:02:25,011 --> 00:02:28,600
dan percayalah, masih banyak kerumitan yang harus kita pahami.

43
00:02:29,120 --> 00:02:31,600
Tetapi bahkan dalam bentuk yang paling sederhana ini pun, 

44
00:02:31,600 --> 00:02:33,696
ia dapat belajar mengenali angka tulisan tangan, 

45
00:02:33,696 --> 00:02:36,520
yang merupakan hal yang cukup keren untuk dilakukan oleh komputer.

46
00:02:37,480 --> 00:02:39,785
Dan pada saat yang sama, Anda akan melihat bahwa 

47
00:02:39,785 --> 00:02:42,280
kamera ini tidak memenuhi harapan yang kami harapkan.

48
00:02:43,380 --> 00:02:46,430
Seperti namanya, jaringan saraf terinspirasi oleh otak, 

49
00:02:46,430 --> 00:02:48,500
tetapi mari kita uraikan lebih lanjut.

50
00:02:48,520 --> 00:02:51,660
Apa itu neuron, dan dalam arti apa mereka saling terhubung?

51
00:02:52,500 --> 00:02:56,555
Saat ini, ketika saya mengatakan neuron, yang saya ingin Anda pikirkan 

52
00:02:56,555 --> 00:03:00,440
adalah sesuatu yang menyimpan angka, khususnya angka antara 0 dan 1.

53
00:03:00,680 --> 00:03:02,560
Tidak lebih dari itu.

54
00:03:03,780 --> 00:03:08,565
Sebagai contoh, jaringan dimulai dengan sekelompok neuron yang sesuai dengan 

55
00:03:08,565 --> 00:03:13,785
masing-masing piksel 28x28 dari gambar input, yang secara keseluruhan berjumlah 784 

56
00:03:13,785 --> 00:03:14,220
neuron.

57
00:03:14,700 --> 00:03:19,626
Masing-masing menyimpan angka yang merepresentasikan nilai skala abu-abu dari piksel 

58
00:03:19,626 --> 00:03:24,380
yang bersangkutan, berkisar dari 0 untuk piksel hitam hingga 1 untuk piksel putih.

59
00:03:25,300 --> 00:03:29,755
Angka di dalam neuron ini disebut aktivasi, dan gambaran yang mungkin ada di benak Anda 

60
00:03:29,755 --> 00:03:34,160
di sini adalah bahwa setiap neuron menyala ketika aktivasinya adalah angka yang tinggi.

61
00:03:36,720 --> 00:03:41,860
Jadi, semua 784 neuron ini membentuk lapisan pertama jaringan kita.

62
00:03:46,500 --> 00:03:49,608
Sekarang melompat ke lapisan terakhir, lapisan ini memiliki 10 neuron, 

63
00:03:49,608 --> 00:03:51,360
masing-masing mewakili salah satu digit.

64
00:03:52,040 --> 00:03:56,183
Aktivasi dalam neuron-neuron ini, sekali lagi suatu angka antara 0 dan 1, 

65
00:03:56,183 --> 00:04:00,944
menunjukkan seberapa besar sistem berpikir bahwa gambar yang diberikan sesuai dengan 

66
00:04:00,944 --> 00:04:02,120
angka yang diberikan.

67
00:04:03,040 --> 00:04:07,184
Ada juga beberapa lapisan di antaranya yang disebut lapisan tersembunyi, 

68
00:04:07,184 --> 00:04:10,590
yang untuk saat ini hanya menjadi tanda tanya besar tentang 

69
00:04:10,590 --> 00:04:13,600
bagaimana proses pengenalan angka ini akan ditangani.

70
00:04:14,260 --> 00:04:18,048
Dalam jaringan ini saya memilih dua lapisan tersembunyi, masing-masing dengan 16 neuron, 

71
00:04:18,048 --> 00:04:20,560
dan harus diakui bahwa itu adalah pilihan yang sembarangan.

72
00:04:21,019 --> 00:04:24,587
Sejujurnya, saya memilih dua lapisan berdasarkan bagaimana saya ingin memotivasi 

73
00:04:24,587 --> 00:04:28,200
struktur dalam sekejap, dan 16, itu adalah angka yang bagus untuk dimuat di layar.

74
00:04:28,780 --> 00:04:30,540
Dalam praktiknya, terdapat banyak ruang untuk 

75
00:04:30,540 --> 00:04:32,340
bereksperimen dengan struktur tertentu di sini.

76
00:04:33,020 --> 00:04:38,480
Cara jaringan beroperasi, aktivasi di satu lapisan menentukan aktivasi lapisan berikutnya.

77
00:04:39,200 --> 00:04:43,860
Dan tentu saja inti dari jaringan sebagai mekanisme pemrosesan informasi adalah 

78
00:04:43,860 --> 00:04:48,580
bagaimana aktivasi dari satu lapisan menghasilkan aktivasi di lapisan berikutnya.

79
00:04:49,140 --> 00:04:51,600
Hal ini dimaksudkan untuk menganalogikan secara longgar 

80
00:04:51,600 --> 00:04:54,236
bagaimana dalam jaringan biologis neuron, beberapa kelompok 

81
00:04:54,236 --> 00:04:57,180
neuron yang menembak menyebabkan kelompok neuron tertentu menembak.

82
00:04:58,120 --> 00:05:01,081
Sekarang jaringan yang saya tunjukkan di sini telah dilatih untuk mengenali angka, 

83
00:05:01,081 --> 00:05:03,400
dan izinkan saya menunjukkan kepada Anda apa yang saya maksudkan.

84
00:05:03,640 --> 00:05:08,078
Artinya, jika Anda memasukkan gambar, menerangi semua 784 neuron pada lapisan input 

85
00:05:08,078 --> 00:05:10,772
sesuai dengan kecerahan setiap piksel pada gambar, 

86
00:05:10,772 --> 00:05:15,158
pola aktivasi tersebut menyebabkan beberapa pola yang sangat spesifik pada lapisan 

87
00:05:15,158 --> 00:05:18,698
berikutnya yang menyebabkan beberapa pola pada lapisan setelahnya, 

88
00:05:18,698 --> 00:05:22,080
yang pada akhirnya memberikan beberapa pola pada lapisan output.

89
00:05:22,560 --> 00:05:26,434
Dan neuron yang paling terang dari lapisan keluaran itu adalah pilihan jaringan, 

90
00:05:26,434 --> 00:05:29,400
bisa dikatakan, untuk digit apa yang diwakili oleh gambar ini.

91
00:05:32,560 --> 00:05:36,005
Dan sebelum masuk ke dalam perhitungan tentang bagaimana satu lapisan mempengaruhi 

92
00:05:36,005 --> 00:05:38,372
lapisan berikutnya, atau bagaimana cara kerja pelatihan, 

93
00:05:38,372 --> 00:05:42,025
mari kita bahas tentang mengapa masuk akal untuk mengharapkan struktur berlapis seperti 

94
00:05:42,025 --> 00:05:43,520
ini untuk berperilaku secara cerdas.

95
00:05:44,060 --> 00:05:45,220
Apa yang kita harapkan di sini?

96
00:05:45,400 --> 00:05:47,600
Apa harapan terbaik untuk lapisan menengah tersebut?

97
00:05:48,920 --> 00:05:53,520
Nah, ketika Anda atau saya mengenali angka, kita menyusun berbagai komponen.

98
00:05:54,200 --> 00:05:56,820
Angka 9 memiliki lingkaran di bagian atas dan garis di sebelah kanan.

99
00:05:57,380 --> 00:05:59,117
Angka 8 juga memiliki lingkaran di bagian atas, 

100
00:05:59,117 --> 00:06:01,180
tetapi dipasangkan dengan lingkaran lain di bagian bawah.

101
00:06:01,980 --> 00:06:06,820
Pada dasarnya, angka 4 terbagi menjadi tiga baris spesifik, dan hal-hal seperti itu.

102
00:06:07,600 --> 00:06:11,471
Sekarang di dunia yang sempurna, kita mungkin berharap bahwa setiap neuron di 

103
00:06:11,471 --> 00:06:15,044
lapisan kedua hingga terakhir sesuai dengan salah satu subkomponen ini, 

104
00:06:15,044 --> 00:06:18,121
sehingga kapan pun Anda memasukkan gambar dengan, katakanlah, 

105
00:06:18,121 --> 00:06:20,653
sebuah lingkaran di bagian atas, seperti 9 atau 8, 

106
00:06:20,653 --> 00:06:23,780
ada beberapa neuron tertentu yang aktivasinya akan mendekati 1.

107
00:06:24,500 --> 00:06:27,324
Dan yang saya maksudkan bukan lingkaran piksel yang spesifik ini, 

108
00:06:27,324 --> 00:06:30,789
harapannya adalah bahwa setiap pola yang secara umum melingkar ke arah atas akan 

109
00:06:30,789 --> 00:06:31,560
memicu neuron ini.

110
00:06:32,440 --> 00:06:35,682
Dengan begitu, beralih dari lapisan ketiga ke lapisan terakhir, 

111
00:06:35,682 --> 00:06:40,040
hanya perlu mempelajari kombinasi subkomponen mana yang sesuai dengan angka yang mana.

112
00:06:41,000 --> 00:06:44,251
Tentu saja, hal itu hanya menambah masalah, karena bagaimana Anda bisa 

113
00:06:44,251 --> 00:06:47,640
mengenali subkomponen ini, atau bahkan mempelajari subkomponen yang tepat?

114
00:06:48,060 --> 00:06:50,377
Dan saya masih belum membicarakan tentang bagaimana satu 

115
00:06:50,377 --> 00:06:53,060
lapisan memengaruhi lapisan berikutnya, tetapi ikuti saya sejenak.

116
00:06:53,680 --> 00:06:56,680
Mengenali sebuah loop juga dapat dipecah menjadi beberapa submasalah.

117
00:06:57,280 --> 00:06:59,862
Satu cara yang masuk akal untuk melakukan ini adalah, 

118
00:06:59,862 --> 00:07:02,780
pertama-tama mengenali berbagai tepi kecil yang membentuknya.

119
00:07:03,780 --> 00:07:07,940
Demikian pula, garis panjang, seperti yang mungkin Anda lihat pada angka 1 atau 4 atau 7, 

120
00:07:07,940 --> 00:07:10,159
sesungguhnya hanyalah sebuah tepi yang panjang, 

121
00:07:10,159 --> 00:07:14,320
atau mungkin Anda menganggapnya sebagai pola tertentu dari beberapa tepi yang lebih kecil.

122
00:07:15,140 --> 00:07:18,872
Jadi, mungkin harapan kami adalah bahwa setiap neuron di lapisan 

123
00:07:18,872 --> 00:07:22,720
kedua jaringan berhubungan dengan berbagai tepi kecil yang relevan.

124
00:07:23,540 --> 00:07:27,464
Mungkin ketika gambar seperti ini masuk, gambar tersebut menerangi semua 

125
00:07:27,464 --> 00:07:31,119
neuron yang terkait dengan sekitar 8 hingga 10 tepi kecil tertentu, 

126
00:07:31,119 --> 00:07:35,043
yang pada gilirannya menerangi neuron yang terkait dengan lingkaran atas 

127
00:07:35,043 --> 00:07:39,720
dan garis vertikal yang panjang, dan yang menerangi neuron yang terkait dengan angka 9.

128
00:07:40,680 --> 00:07:43,706
Apakah ini yang sebenarnya dilakukan oleh jaringan akhir kita atau tidak, 

129
00:07:43,706 --> 00:07:46,487
itu adalah pertanyaan lain, pertanyaan yang akan saya bahas kembali 

130
00:07:46,487 --> 00:07:49,350
setelah kita melihat cara melatih jaringan, tetapi ini adalah harapan 

131
00:07:49,350 --> 00:07:52,540
yang mungkin kita miliki, semacam tujuan dengan struktur berlapis seperti ini.

132
00:07:53,160 --> 00:07:56,681
Selain itu, Anda bisa membayangkan, betapa kemampuan mendeteksi tepi dan 

133
00:07:56,681 --> 00:08:00,300
pola seperti ini akan sangat berguna untuk tugas pengenalan gambar lainnya.

134
00:08:00,880 --> 00:08:03,988
Dan bahkan di luar pengenalan gambar, ada berbagai macam hal cerdas 

135
00:08:03,988 --> 00:08:07,280
yang mungkin ingin Anda lakukan yang terurai ke dalam lapisan abstraksi.

136
00:08:08,040 --> 00:08:10,968
Mengurai ucapan, misalnya, melibatkan pengambilan audio mentah dan 

137
00:08:10,968 --> 00:08:14,771
memilih suara-suara yang berbeda, yang digabungkan untuk membentuk suku kata tertentu, 

138
00:08:14,771 --> 00:08:17,699
yang digabungkan untuk membentuk kata-kata, yang digabungkan untuk 

139
00:08:17,699 --> 00:08:20,060
membentuk frasa dan pemikiran yang lebih abstrak, dll.

140
00:08:21,100 --> 00:08:25,669
Tetapi kembali ke cara kerja semua ini, bayangkan diri Anda sekarang sedang merancang 

141
00:08:25,669 --> 00:08:29,920
bagaimana tepatnya aktivasi di satu lapisan dapat menentukan lapisan berikutnya.

142
00:08:30,860 --> 00:08:35,034
Tujuannya adalah untuk memiliki suatu mekanisme yang dapat menggabungkan 

143
00:08:35,034 --> 00:08:38,980
piksel menjadi tepi, atau tepi menjadi pola, atau pola menjadi angka.

144
00:08:39,440 --> 00:08:42,586
Dan untuk memperbesar satu contoh yang sangat spesifik, 

145
00:08:42,586 --> 00:08:46,294
katakanlah harapannya adalah agar satu neuron tertentu di lapisan 

146
00:08:46,294 --> 00:08:50,620
kedua dapat mengetahui apakah gambar memiliki tepi di wilayah ini atau tidak.

147
00:08:51,440 --> 00:08:55,100
Pertanyaan yang ada adalah parameter apa yang harus dimiliki jaringan?

148
00:08:55,640 --> 00:08:59,494
Dial dan kenop apa yang bisa Anda sesuaikan supaya cukup ekspresif 

149
00:08:59,494 --> 00:09:02,486
untuk menangkap pola ini, atau pola piksel lainnya, 

150
00:09:02,486 --> 00:09:06,111
atau pola yang bisa membuat beberapa tepi membentuk lingkaran, 

151
00:09:06,111 --> 00:09:07,780
dan hal-hal lain semacam itu?

152
00:09:08,720 --> 00:09:12,279
Nah, yang akan kita lakukan adalah memberikan bobot pada setiap 

153
00:09:12,279 --> 00:09:15,560
koneksi antara neuron kita dan neuron dari lapisan pertama.

154
00:09:16,320 --> 00:09:17,700
Bobot ini hanyalah angka.

155
00:09:18,540 --> 00:09:22,140
Kemudian ambil semua aktivasi tersebut dari lapisan pertama 

156
00:09:22,140 --> 00:09:25,500
dan hitung jumlah tertimbangnya sesuai dengan bobot ini.

157
00:09:27,700 --> 00:09:31,592
Saya merasa akan sangat membantu jika bobot ini diatur ke dalam kisi-kisi kecil, 

158
00:09:31,592 --> 00:09:35,244
dan saya akan menggunakan piksel hijau untuk mengindikasikan bobot positif, 

159
00:09:35,244 --> 00:09:37,839
dan piksel merah untuk mengindikasikan bobot negatif, 

160
00:09:37,839 --> 00:09:41,780
di mana kecerahan piksel tersebut merupakan penggambaran longgar dari nilai bobot.

161
00:09:42,780 --> 00:09:47,141
Sekarang, jika kita membuat bobot yang terkait dengan hampir semua piksel menjadi nol, 

162
00:09:47,141 --> 00:09:50,350
kecuali beberapa bobot positif di wilayah yang kita perhatikan, 

163
00:09:50,350 --> 00:09:54,009
maka mengambil jumlah tertimbang dari semua nilai piksel sebenarnya sama 

164
00:09:54,009 --> 00:09:57,820
saja dengan menjumlahkan nilai piksel hanya di wilayah yang kita perhatikan.

165
00:09:59,140 --> 00:10:02,392
Dan jika Anda benar-benar ingin mengetahui, apakah ada keunggulan di sini, 

166
00:10:02,392 --> 00:10:05,992
yang bisa Anda lakukan yaitu, memiliki bobot negatif yang terkait dengan piksel di 

167
00:10:05,992 --> 00:10:06,600
sekelilingnya.

168
00:10:07,480 --> 00:10:10,621
Kemudian, jumlah terbesar apabila piksel di bagian tengah cerah, 

169
00:10:10,621 --> 00:10:12,700
tetapi piksel di sekelilingnya lebih gelap.

170
00:10:14,260 --> 00:10:16,939
Ketika Anda menghitung jumlah tertimbang seperti ini, 

171
00:10:16,939 --> 00:10:20,612
Anda mungkin akan mendapatkan angka berapapun, tetapi untuk jaringan ini, 

172
00:10:20,612 --> 00:10:23,540
yang kita inginkan adalah aktivasi bernilai antara 0 dan 1.

173
00:10:24,120 --> 00:10:28,002
Jadi, hal yang umum dilakukan adalah memompa jumlah tertimbang ini ke dalam 

174
00:10:28,002 --> 00:10:32,140
suatu fungsi yang memadatkan garis bilangan riil ke dalam kisaran antara 0 dan 1.

175
00:10:32,460 --> 00:10:35,616
Dan fungsi umum yang melakukan hal ini disebut fungsi sigmoid, 

176
00:10:35,616 --> 00:10:37,420
juga dikenal sebagai kurva logistik.

177
00:10:38,000 --> 00:10:41,866
Pada dasarnya input yang sangat negatif akan mendekati 0, 

178
00:10:41,866 --> 00:10:46,600
input positif akan mendekati 1, dan terus meningkat di sekitar input 0.

179
00:10:49,120 --> 00:10:52,603
Jadi, aktivasi neuron di sini pada dasarnya adalah 

180
00:10:52,603 --> 00:10:56,360
ukuran seberapa positif jumlah tertimbang yang relevan.

181
00:10:57,540 --> 00:10:59,781
Tetapi mungkin Anda tidak ingin neuron menyala 

182
00:10:59,781 --> 00:11:01,880
ketika jumlah tertimbang lebih besar dari 0.

183
00:11:02,280 --> 00:11:06,360
Mungkin Anda hanya ingin mengaktifkannya ketika jumlahnya lebih besar dari 10.

184
00:11:06,840 --> 00:11:10,260
Artinya, Anda ingin agar bias tidak aktif.

185
00:11:11,380 --> 00:11:15,544
Apa yang akan kita lakukan selanjutnya adalah menambahkan angka lain seperti negatif 

186
00:11:15,544 --> 00:11:19,660
10 ke jumlah tertimbang ini sebelum memasukkannya ke dalam fungsi pemusatan sigmoid.

187
00:11:20,580 --> 00:11:22,440
Angka tambahan itu disebut bias.

188
00:11:23,460 --> 00:11:27,514
Jadi, bobot memberi tahu Anda pola piksel apa yang ditangkap oleh neuron 

189
00:11:27,514 --> 00:11:31,291
di lapisan kedua, dan bias memberi tahu Anda seberapa tinggi jumlah 

190
00:11:31,291 --> 00:11:35,180
tertimbang yang dibutuhkan sebelum neuron mulai aktif secara bermakna.

191
00:11:36,120 --> 00:11:37,680
Dan itu hanya satu neuron.

192
00:11:38,280 --> 00:11:44,539
Setiap neuron lain di lapisan ini akan terhubung ke semua 784 neuron piksel dari lapisan 

193
00:11:44,539 --> 00:11:50,236
pertama, dan masing-masing dari 784 koneksi tersebut memiliki bobot yang terkait 

194
00:11:50,236 --> 00:11:50,940
dengannya.

195
00:11:51,600 --> 00:11:53,627
Selain itu, masing-masing memiliki beberapa bias, 

196
00:11:53,627 --> 00:11:56,424
beberapa angka lain yang Anda tambahkan ke jumlah tertimbang sebelum 

197
00:11:56,424 --> 00:11:57,600
mengalikannya dengan sigmoid.

198
00:11:58,110 --> 00:11:59,540
Dan itu banyak sekali yang harus dipikirkan!

199
00:11:59,960 --> 00:12:03,702
Dengan lapisan tersembunyi yang terdiri dari 16 neuron, 

200
00:12:03,702 --> 00:12:07,980
itu adalah total 784 dikalikan 16 bobot, bersama dengan 16 bias.

201
00:12:08,840 --> 00:12:11,940
Dan semua itu hanyalah koneksi dari lapisan pertama ke lapisan kedua.

202
00:12:12,520 --> 00:12:17,340
Koneksi antara lapisan lainnya juga memiliki banyak bobot dan bias yang terkait dengannya.

203
00:12:18,340 --> 00:12:23,800
Secara keseluruhan, jaringan ini memiliki hampir 13.000 total bobot dan bias.

204
00:12:23,800 --> 00:12:26,932
13.000 kenop dan tombol yang dapat diatur dan diputar untuk 

205
00:12:26,932 --> 00:12:29,960
membuat jaringan ini berperilaku dengan cara yang berbeda.

206
00:12:31,040 --> 00:12:33,254
Jadi, ketika kita berbicara tentang pembelajaran, 

207
00:12:33,254 --> 00:12:36,620
yang dimaksud adalah membuat komputer menemukan pengaturan yang valid untuk 

208
00:12:36,620 --> 00:12:39,765
semua angka yang sangat banyak ini sehingga komputer benar-benar dapat 

209
00:12:39,765 --> 00:12:41,360
menyelesaikan masalah yang dihadapi.

210
00:12:42,620 --> 00:12:46,839
Satu eksperimen pemikiran yang sekaligus menyenangkan dan agak mengerikan adalah 

211
00:12:46,839 --> 00:12:50,589
membayangkan duduk dan mengatur semua bobot dan bias ini dengan tangan, 

212
00:12:50,589 --> 00:12:54,756
dengan sengaja mengutak-atik angka-angka sehingga lapisan kedua mengambil tepi, 

213
00:12:54,756 --> 00:12:56,580
lapisan ketiga mengambil pola, dll.

214
00:12:56,980 --> 00:13:01,377
Saya pribadi merasa hal ini lebih memuaskan daripada memperlakukan jaringan sebagai kotak 

215
00:13:01,377 --> 00:13:05,286
hitam total, karena ketika jaringan tidak bekerja seperti yang Anda perkirakan, 

216
00:13:05,286 --> 00:13:09,391
jika Anda telah membangun sedikit hubungan dengan apa arti bobot dan bias tersebut, 

217
00:13:09,391 --> 00:13:13,398
Anda memiliki tempat awal untuk bereksperimen dengan cara mengubah struktur untuk 

218
00:13:13,398 --> 00:13:14,180
meningkatkannya.

219
00:13:14,960 --> 00:13:18,745
Atau ketika jaringan memang berfungsi tetapi tidak untuk alasan yang Anda harapkan, 

220
00:13:18,745 --> 00:13:22,169
menggali apa yang dilakukan oleh bobot dan bias adalah cara yang baik untuk 

221
00:13:22,169 --> 00:13:25,820
menantang asumsi Anda dan benar-benar mengekspos ruang penuh solusi yang mungkin.

222
00:13:26,840 --> 00:13:30,680
Ngomong-ngomong, fungsi sebenarnya di sini agak rumit untuk ditulis, bukan begitu?

223
00:13:32,500 --> 00:13:35,618
Jadi, izinkan saya menunjukkan kepada Anda cara yang lebih ringkas secara notasi, 

224
00:13:35,618 --> 00:13:37,140
bagaimana koneksi ini direpresentasikan.

225
00:13:37,660 --> 00:13:39,075
Beginilah yang akan Anda lihat jika Anda memilih 

226
00:13:39,075 --> 00:13:40,520
untuk membaca lebih lanjut tentang jaringan saraf.

227
00:13:41,380 --> 00:13:49,900
Atur semua aktivasi dari satu lapisan ke dalam kolom sebagai matriks yang sesuai 

228
00:13:49,900 --> 00:13:58,000
dengan koneksi antara satu lapisan dan neuron tertentu di lapisan berikutnya.

229
00:13:58,540 --> 00:14:02,374
Artinya, mengambil jumlah tertimbang dari aktivasi di lapisan pertama 

230
00:14:02,374 --> 00:14:06,209
sesuai dengan bobot ini sesuai dengan salah satu istilah dalam produk 

231
00:14:06,209 --> 00:14:09,880
vektor matriks dari semua yang kita miliki di sebelah kiri di sini.

232
00:14:14,000 --> 00:14:17,762
Ngomong-ngomong, sebagian besar pembelajaran mesin bermuara pada pemahaman 

233
00:14:17,762 --> 00:14:21,124
yang baik tentang aljabar linier, jadi bagi Anda yang menginginkan 

234
00:14:21,124 --> 00:14:25,188
pemahaman visual yang bagus untuk matriks dan apa arti perkalian vektor matriks, 

235
00:14:25,188 --> 00:14:28,600
lihatlah seri yang saya buat tentang aljabar linier, terutama bab 3.

236
00:14:29,240 --> 00:14:33,528
Kembali ke ekspresi kita, alih-alih berbicara tentang menambahkan bias ke masing-masing 

237
00:14:33,528 --> 00:14:37,865
nilai ini secara terpisah, kita merepresentasikannya dengan mengatur semua bias tersebut 

238
00:14:37,865 --> 00:14:41,763
ke dalam sebuah vektor, dan menambahkan seluruh vektor ke produk vektor matriks 

239
00:14:41,763 --> 00:14:42,300
sebelumnya.

240
00:14:43,280 --> 00:14:47,053
Kemudian sebagai langkah terakhir, saya akan membungkus sigmoid di sekitar bagian 

241
00:14:47,053 --> 00:14:50,920
luar di sini, dan apa yang seharusnya direpresentasikan adalah Anda akan menerapkan 

242
00:14:50,920 --> 00:14:54,740
fungsi sigmoid ke setiap komponen tertentu dari vektor yang dihasilkan di dalamnya.

243
00:14:55,940 --> 00:14:59,841
Jadi, setelah Anda menuliskan matriks bobot dan vektor-vektor ini sebagai 

244
00:14:59,841 --> 00:15:03,849
simbol mereka sendiri, Anda dapat mengkomunikasikan transisi penuh aktivasi 

245
00:15:03,849 --> 00:15:08,541
dari satu lapisan ke lapisan berikutnya dalam ekspresi kecil yang sangat ketat dan rapi, 

246
00:15:08,541 --> 00:15:12,760
dan ini membuat kode yang relevan menjadi lebih sederhana dan jauh lebih cepat, 

247
00:15:12,760 --> 00:15:15,660
karena banyak pustaka mengoptimalkan perkalian matriks.

248
00:15:17,820 --> 00:15:19,456
Ingat bagaimana sebelumnya saya mengatakan bahwa 

249
00:15:19,456 --> 00:15:21,460
neuron-neuron ini hanyalah benda-benda yang menyimpan angka?

250
00:15:22,220 --> 00:15:27,454
Tentu saja angka spesifik yang mereka pegang tergantung pada gambar yang Anda masukkan, 

251
00:15:27,454 --> 00:15:32,391
jadi sebenarnya lebih akurat untuk menganggap setiap neuron sebagai sebuah fungsi, 

252
00:15:32,391 --> 00:15:36,317
yang mengambil output dari semua neuron di lapisan sebelumnya dan 

253
00:15:36,317 --> 00:15:38,340
mengeluarkan angka antara 0 dan 1.

254
00:15:39,200 --> 00:15:42,298
Sesungguhnya seluruh jaringan hanyalah sebuah fungsi, 

255
00:15:42,298 --> 00:15:47,060
yang menerima 784 angka sebagai masukan dan mengeluarkan 10 angka sebagai keluaran.

256
00:15:47,560 --> 00:15:52,514
Ini adalah fungsi yang sangat rumit, yang melibatkan 13.000 parameter dalam bentuk 

257
00:15:52,514 --> 00:15:55,498
bobot dan bias yang menangkap pola-pola tertentu, 

258
00:15:55,498 --> 00:16:00,750
dan yang melibatkan iterasi banyak produk vektor matriks dan fungsi pemampatan sigmoid, 

259
00:16:00,750 --> 00:16:03,914
tetapi ini hanyalah sebuah fungsi, dan di satu sisi, 

260
00:16:03,914 --> 00:16:06,660
hal ini agak meyakinkan karena terlihat rumit.

261
00:16:07,340 --> 00:16:09,762
Maksud saya, jika ini lebih sederhana, apa harapan 

262
00:16:09,762 --> 00:16:12,280
kita untuk bisa menghadapi tantangan mengenali angka?

263
00:16:13,340 --> 00:16:14,700
Dan bagaimana cara menghadapi tantangan itu?

264
00:16:15,080 --> 00:16:19,360
Bagaimana jaringan ini mempelajari bobot dan bias yang sesuai hanya dengan melihat data?

265
00:16:20,140 --> 00:16:22,060
Itulah yang akan saya tunjukkan di video berikutnya, 

266
00:16:22,060 --> 00:16:25,068
dan saya juga akan menggali lebih dalam tentang apa yang sebenarnya dilakukan oleh 

267
00:16:25,068 --> 00:16:26,120
jaringan yang kita lihat ini.

268
00:16:27,580 --> 00:16:30,986
Sekarang adalah poin yang seharusnya saya katakan berlangganan untuk mendapatkan 

269
00:16:30,986 --> 00:16:33,383
notifikasi ketika ada video atau video baru yang keluar, 

270
00:16:33,383 --> 00:16:37,167
tetapi pada kenyataannya sebagian besar dari Anda tidak menerima notifikasi dari YouTube, 

271
00:16:37,167 --> 00:16:37,420
bukan?

272
00:16:38,020 --> 00:16:41,246
Mungkin lebih jujurnya saya harus mengatakan berlangganan agar jaringan 

273
00:16:41,246 --> 00:16:44,384
saraf yang mendasari algoritme rekomendasi YouTube siap untuk percaya 

274
00:16:44,384 --> 00:16:47,880
bahwa Anda ingin melihat konten dari saluran ini direkomendasikan kepada Anda.

275
00:16:48,560 --> 00:16:49,940
Pokoknya tetap ikuti perkembangannya.

276
00:16:50,760 --> 00:16:53,500
Terima kasih banyak kepada semua orang yang mendukung video ini di Patreon.

277
00:16:54,000 --> 00:16:57,520
Saya agak lambat dalam membuat kemajuan dalam seri probabilitas pada musim panas ini, 

278
00:16:57,520 --> 00:16:59,689
tetapi saya akan kembali ke sana setelah proyek ini, 

279
00:16:59,689 --> 00:17:01,900
jadi para pelanggan bisa menantikan pembaruan di sana.

280
00:17:03,600 --> 00:17:07,244
Sebagai penutup, saya akan memperkenalkan Leisha Lee yang mengambil gelar PhD-nya di 

281
00:17:07,244 --> 00:17:10,803
bidang teori deep learning dan saat ini bekerja di sebuah perusahaan modal ventura 

282
00:17:10,803 --> 00:17:14,619
bernama Amplify Partners yang dengan baik hati menyediakan sebagian dana untuk video ini.

283
00:17:15,460 --> 00:17:19,119
Jadi Leisha, satu hal yang menurut saya harus segera kita bahas adalah fungsi sigmoid ini.

284
00:17:19,700 --> 00:17:23,211
Seperti yang saya pahami, jaringan awal menggunakan ini untuk memasukkan jumlah 

285
00:17:23,211 --> 00:17:26,460
tertimbang yang relevan ke dalam interval antara nol dan satu, Anda tahu, 

286
00:17:26,460 --> 00:17:29,840
semacam termotivasi oleh analogi biologis neuron yang tidak aktif atau aktif.

287
00:17:30,280 --> 00:17:30,300
Tepat sekali.

288
00:17:30,560 --> 00:17:34,040
Tetapi relatif sedikit jaringan modern yang benar-benar menggunakan sigmoid lagi.

289
00:17:34,320 --> 00:17:34,320
Ya.

290
00:17:34,440 --> 00:17:35,540
Agak jadul, bukan?

291
00:17:35,760 --> 00:17:38,980
Ya, atau lebih tepatnya, relu tampaknya lebih mudah dilatih.

292
00:17:39,400 --> 00:17:42,340
Dan relu adalah singkatan dari unit linier yang diperbaiki?

293
00:17:42,680 --> 00:17:48,122
Ya, ini adalah jenis fungsi di mana Anda hanya mengambil maksimum nol dan a di mana a 

294
00:17:48,122 --> 00:17:53,184
diberikan oleh apa yang Anda jelaskan di video dan apa yang memotivasi hal ini, 

295
00:17:53,184 --> 00:17:58,626
menurut saya, sebagian dari analogi biologis tentang bagaimana neuron akan diaktifkan 

296
00:17:58,626 --> 00:18:04,258
atau tidak, jadi jika melewati ambang batas tertentu, itu akan menjadi fungsi identitas, 

297
00:18:04,258 --> 00:18:08,372
tetapi jika tidak, itu tidak akan diaktifkan, jadi nilainya nol, 

298
00:18:08,372 --> 00:18:10,840
jadi ini adalah semacam penyederhanaan.

299
00:18:11,160 --> 00:18:15,561
Menggunakan sigmoid tidak membantu pelatihan atau sangat sulit untuk 

300
00:18:15,561 --> 00:18:20,154
dilatih pada suatu saat dan orang-orang hanya mencoba relu dan ternyata 

301
00:18:20,154 --> 00:18:24,620
bekerja dengan sangat baik untuk jaringan saraf yang sangat dalam ini.

302
00:18:25,100 --> 00:18:25,640
Baiklah, terima kasih Alicia.

