[
 {
  "input": "This is a 3.",
  "translatedText": "これは3だ。",
  "model": "DeepL",
  "time_range": [
   4.22,
   5.4
  ]
 },
 {
  "input": "It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3.",
  "translatedText": "ぞんざいに書かれ、28x28ピクセルという極めて低い解像度でレンダリングされているが、あなたの脳はそれを3として認識するのに問題はない。",
  "model": "DeepL",
  "time_range": [
   6.06,
   13.72
  ]
 },
 {
  "input": "And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly.",
  "translatedText": "そして、頭脳がこのように難なくこなせることがいかにクレイジーなことなのか、少し考えてみてほしい。",
  "model": "DeepL",
  "time_range": [
   14.34,
   18.96
  ]
 },
 {
  "input": "I mean, this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next.",
  "translatedText": "つまり、各ピクセルの具体的な値は画像によって大きく異なるにもかかわらず、これとこれとこれも3Sとして認識できるのだ。",
  "model": "DeepL",
  "time_range": [
   19.7,
   28.32
  ]
 },
 {
  "input": "The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3.",
  "translatedText": "この3を見たときに発火している眼球内の特殊な光感受性細胞は、この3を見たときに発火しているものとは全く異なる。",
  "model": "DeepL",
  "time_range": [
   28.9,
   36.94
  ]
 },
 {
  "input": "But something in that crazy-smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas.",
  "translatedText": "しかし、あなたのそのクレイジーなほど賢い視覚野の何かが、これらを同じ考えを表していると解し、同時に他のイメージをそれ自身の異なる考えとして認識する。",
  "model": "DeepL",
  "time_range": [
   37.52,
   48.26
  ]
 },
 {
  "input": "But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this and outputs a single number between 0 and 10, telling you what it thinks the digit is, well the task goes from comically trivial to dauntingly difficult.",
  "translatedText": "しかし、このように28×28ピクセルのグリッドを取り込み、0から10までの数字を1つだけ出力し、その数字が何であるかを教えてくれるプログラムを書いてくれ、と言ったら、その仕事は滑稽なほど簡単なものから、気が遠くなるほど難しいものになるだろう。",
  "model": "DeepL",
  "time_range": [
   49.22,
   66.18
  ]
 },
 {
  "input": "Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future.",
  "translatedText": "あなたが岩の下で生きてきたのでなければ、機械学習とニューラルネットワークの現在と将来への関連性と重要性を説明する必要はないだろう。",
  "model": "DeepL",
  "time_range": [
   67.16,
   74.64
  ]
 },
 {
  "input": "But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing, not as a buzzword but as a piece of math.",
  "translatedText": "しかし、ここで私がしたいことは、バックグラウンドがないことを前提に、ニューラルネットワークが実際にどのようなものなのかをお見せし、それが何をやっているのかを、流行語ではなく、数学の一部として視覚化することだ。",
  "model": "DeepL",
  "time_range": [
   75.12,
   84.46
  ]
 },
 {
  "input": "My hope is that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote-unquote learning.",
  "translatedText": "私の望みは、構造そのものがやる気を起こさせるものだと感じてもらい、ニューラルネットワークの引用学習について読んだり聞いたりしたときに、それが何を意味するのかわかったような気分になってもらうことだ。",
  "model": "DeepL",
  "time_range": [
   85.02,
   94.34
  ]
 },
 {
  "input": "This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning.",
  "translatedText": "このビデオはその構造的な要素に焦点を当てたもので、次のビデオは学習に焦点を当てたものだ。",
  "model": "DeepL",
  "time_range": [
   95.36,
   100.26
  ]
 },
 {
  "input": "What we're going to do is put together a neural network that can learn to recognize handwritten digits.",
  "translatedText": "私たちがやろうとしているのは、手書きの数字を認識することを学習できるニューラルネットワークを構築することだ。",
  "model": "DeepL",
  "time_range": [
   100.96,
   106.04
  ]
 },
 {
  "input": "This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quo here, because at the end of the two videos I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer.",
  "translatedText": "これは、このトピックを紹介するためのやや古典的な例であり、ここでは現状維持で構わない。なぜなら、2本のビデオの最後に、もっと詳しく学べるいくつかの良いリソースと、これを実行するコードをダウンロードして自分のコンピューターで遊べる場所を紹介したいからだ。",
  "model": "DeepL",
  "time_range": [
   109.36,
   123.08
  ]
 },
 {
  "input": "There are many many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos you and I are just going to look at the simplest plain vanilla form with no added frills.",
  "translatedText": "ニューラルネットワークには様々な種類があり、近年はそのような種類の研究がある種のブームになっているが、この2つの入門ビデオでは、あなたと私は、飾り気のない最もシンプルなプレーン・バニラ形式を見るだけだ。",
  "model": "DeepL",
  "time_range": [
   125.04,
   139.18
  ]
 },
 {
  "input": "This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds around.",
  "translatedText": "これは、より強力な現代の変種を理解するために必要な前提条件のようなもので、信じてほしい。",
  "model": "DeepL",
  "time_range": [
   139.86,
   148.6
  ]
 },
 {
  "input": "But even in this simplest form it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do.",
  "translatedText": "しかし、この最も単純な形であっても、手書きの数字を認識することを学習することができる。",
  "model": "DeepL",
  "time_range": [
   149.12,
   156.52
  ]
 },
 {
  "input": "And at the same time you'll see how it does fall short of a couple hopes that we might have for it.",
  "translatedText": "そして同時に、私たちが期待していたような2、3の期待をいかに下回るものであったかもわかるだろう。",
  "model": "DeepL",
  "time_range": [
   157.48,
   162.28
  ]
 },
 {
  "input": "As the name suggests neural networks are inspired by the brain, but let's break that down.",
  "translatedText": "その名の通り、ニューラルネットワークは脳から着想を得ているが、それを分解してみよう。",
  "model": "DeepL",
  "time_range": [
   163.38,
   168.5
  ]
 },
 {
  "input": "What are the neurons, and in what sense are they linked together?",
  "translatedText": "神経細胞とは何なのか、どのような意味でつながっているのか。",
  "model": "DeepL",
  "time_range": [
   168.52,
   171.66
  ]
 },
 {
  "input": "Right now when I say neuron all I want you to think about is a thing that holds a number, specifically a number between 0 and 1.",
  "translatedText": "私が今ニューロンと言ったとき、皆さんに考えてほしいのは、数字、特に0から1の間の数字を保持するものだ。",
  "model": "DeepL",
  "time_range": [
   172.5,
   180.44
  ]
 },
 {
  "input": "It's really not more than that.",
  "translatedText": "それ以上のことはない。",
  "model": "DeepL",
  "time_range": [
   180.68,
   182.56
  ]
 },
 {
  "input": "For example the network starts with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784 neurons in total.",
  "translatedText": "例えば、このネットワークは、入力画像の28x28ピクセルそれぞれに対応するニューロンの束から始まり、合計で784ニューロンとなる。",
  "model": "DeepL",
  "time_range": [
   183.78,
   194.22
  ]
 },
 {
  "input": "Each one of these holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels.",
  "translatedText": "これらのピクセルはそれぞれ、対応するピクセルのグレースケール値を表す数値を保持しており、黒ピクセルの0から白ピクセルの1まである。",
  "model": "DeepL",
  "time_range": [
   194.7,
   204.38
  ]
 },
 {
  "input": "This number inside the neuron is called its activation, and the image you might have in mind here is that each neuron is lit up when its activation is a high number.",
  "translatedText": "ニューロン内部のこの数値は活性化と呼ばれ、ここで思い浮かべるイメージは、活性化が高い数値のときに各ニューロンが点灯するというものだろう。",
  "model": "DeepL",
  "time_range": [
   205.3,
   214.16
  ]
 },
 {
  "input": "So all of these 784 neurons make up the first layer of our network.",
  "translatedText": "つまり、これら784個のニューロンはすべて、ネットワークの第1層を構成しているのだ。",
  "model": "DeepL",
  "time_range": [
   216.72,
   221.86
  ]
 },
 {
  "input": "Now jumping over to the last layer, this has 10 neurons, each representing one of the digits.",
  "translatedText": "最後の層にジャンプすると、10個のニューロンがあり、それぞれが数字を1つずつ表す。",
  "model": "DeepL",
  "time_range": [
   226.5,
   231.36
  ]
 },
 {
  "input": "The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit.",
  "translatedText": "これらのニューロンの活性化は、やはり0から1の間の数値で、ある画像がある数字に対応するとシステムが考える度合いを表している。",
  "model": "DeepL",
  "time_range": [
   232.04,
   242.12
  ]
 },
 {
  "input": "There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark for how on earth this process of recognizing digits is going to be handled.",
  "translatedText": "また、その間に隠れ層と呼ばれる層があり、当面はこの数字を認識するプロセスが一体どのように処理されるのか、巨大な疑問符がつくだけだろう。",
  "model": "DeepL",
  "time_range": [
   243.04,
   253.6
  ]
 },
 {
  "input": "In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice.",
  "translatedText": "このネットワークでは2つの隠れ層を選び、それぞれ16のニューロンを持つ。",
  "model": "DeepL",
  "time_range": [
   254.26,
   260.56
  ]
 },
 {
  "input": "To be honest I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that was just a nice number to fit on the screen.",
  "translatedText": "正直なところ、2つのレイヤーを選んだのは、ほんの一瞬の間に構造をどのように動機づけたいかを考えてのことで、16は、まあ、画面に収まるちょうどいい数字だった。",
  "model": "DeepL",
  "time_range": [
   261.02,
   268.2
  ]
 },
 {
  "input": "In practice there is a lot of room for experiment with a specific structure here.",
  "translatedText": "実際には、ここで具体的な構造を実験する余地は大いにある。",
  "model": "DeepL",
  "time_range": [
   268.78,
   272.34
  ]
 },
 {
  "input": "The way the network operates, activations in one layer determine the activations of the next layer.",
  "translatedText": "ネットワークの動作は、ある層の活性化が次の層の活性化を決定する。",
  "model": "DeepL",
  "time_range": [
   273.02,
   278.48
  ]
 },
 {
  "input": "And of course the heart of the network as an information processing mechanism comes down to exactly how those activations from one layer bring about activations in the next layer.",
  "translatedText": "そしてもちろん、情報処理メカニズムとしてのネットワークの核心は、ある層の活性化が次の層の活性化をどのようにもたらすかということに尽きる。",
  "model": "DeepL",
  "time_range": [
   279.2,
   288.58
  ]
 },
 {
  "input": "It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire.",
  "translatedText": "これは、ニューロンの生物学的ネットワークにおいて、あるニューロン群の発火が、他の特定のニューロン群の発火を引き起こすことに似ている。",
  "model": "DeepL",
  "time_range": [
   289.14,
   297.18
  ]
 },
 {
  "input": "Now the network I'm showing here has already been trained to recognize digits, and let me show you what I mean by that.",
  "translatedText": "ここでお見せするネットワークは、数字を認識するようにすでに訓練されている。",
  "model": "DeepL",
  "time_range": [
   298.12,
   303.4
  ]
 },
 {
  "input": "It means if you feed in an image, lighting up all 784 neurons of the input layer according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer which causes some pattern in the one after it, which finally gives some pattern in the output layer.",
  "translatedText": "つまり、画像を入力し、画像の各ピクセルの明るさに応じて入力層の784個のニューロンをすべて点灯させると、その活性化パターンが次の層にある特定のパターンを引き起こし、それがさらに次の層にあるパターンを引き起こし、最終的に出力層にあるパターンを与えるということだ。",
  "model": "DeepL",
  "time_range": [
   303.64,
   322.08
  ]
 },
 {
  "input": "And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents.",
  "translatedText": "そして、その出力層の最も明るいニューロンは、いわば、この画像が何桁の数字を表しているかをネットワークが選択したことになる。",
  "model": "DeepL",
  "time_range": [
   322.56,
   329.4
  ]
 },
 {
  "input": "And before jumping into the math for how one layer influences the next, or how training works, let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently.",
  "translatedText": "そして、ある層が次の層にどのような影響を与えるのか、あるいはトレーニングがどのように機能するのか、その計算に飛びつく前に、なぜこのような層構造がインテリジェントに振る舞うことを期待するのが合理的なのかについてだけ話しておこう。",
  "model": "DeepL",
  "time_range": [
   332.56,
   343.52
  ]
 },
 {
  "input": "What are we expecting here?",
  "translatedText": "ここで何を期待しているのか？",
  "model": "DeepL",
  "time_range": [
   344.06,
   345.22
  ]
 },
 {
  "input": "What is the best hope for those middle layers?",
  "translatedText": "そうした中間層にとって、最善の希望は何だろうか？",
  "model": "DeepL",
  "time_range": [
   345.4,
   347.6
  ]
 },
 {
  "input": "Well, when you or I recognize digits, we piece together various components.",
  "translatedText": "私やあなたが数字を認識するとき、私たちはさまざまな要素を組み合わせる。",
  "model": "DeepL",
  "time_range": [
   348.92,
   353.52
  ]
 },
 {
  "input": "A 9 has a loop up top and a line on the right.",
  "translatedText": "9は上にループがあり、右にラインがある。",
  "model": "DeepL",
  "time_range": [
   354.2,
   356.82
  ]
 },
 {
  "input": "An 8 also has a loop up top, but it's paired with another loop down low.",
  "translatedText": "8もトップにループがあるが、低めのループと対になっている。",
  "model": "DeepL",
  "time_range": [
   357.38,
   361.18
  ]
 },
 {
  "input": "A 4 basically breaks down into three specific lines, and things like that.",
  "translatedText": "4は基本的に3つのラインに分かれている。",
  "model": "DeepL",
  "time_range": [
   361.98,
   366.82
  ]
 },
 {
  "input": "Now in a perfect world, we might hope that each neuron in the second to last layer corresponds with one of these subcomponents, that anytime you feed in an image with, say, a loop up top, like a 9 or an 8, there's some specific neuron whose activation is going to be close to 1.",
  "translatedText": "完璧な世界であれば、最後から2番目のレイヤーの各ニューロンが、これらのサブコンポーネントのいずれかに対応し、たとえば9や8のようなループを持つ画像を入力すると、その活性化が1に近くなる特定のニューロンが存在することを望むかもしれない。",
  "model": "DeepL",
  "time_range": [
   367.6,
   383.78
  ]
 },
 {
  "input": "And I don't mean this specific loop of pixels, the hope would be that any generally loopy pattern towards the top sets off this neuron.",
  "translatedText": "この特定のピクセルのループという意味ではなく、上部に向かう全体的にループしたパターンが、このニューロンを作動させることを期待しているのだ。",
  "model": "DeepL",
  "time_range": [
   384.5,
   391.56
  ]
 },
 {
  "input": "That way, going from the third layer to the last one just requires learning which combination of subcomponents corresponds to which digits.",
  "translatedText": "そうすれば、3番目のレイヤーから最後のレイヤーに行くには、どのサブコンポーネントの組み合わせがどの数字に対応するかを学習すればいい。",
  "model": "DeepL",
  "time_range": [
   392.44,
   400.04
  ]
 },
 {
  "input": "Of course, that just kicks the problem down the road, because how would you recognize these subcomponents, or even learn what the right subcomponents should be?",
  "translatedText": "というのも、これらのサブコンポーネントをどうやって認識するのか、あるいは正しいサブコンポーネントが何であるべきかを学ぶことさえできるのだろうか？",
  "model": "DeepL",
  "time_range": [
   401.0,
   407.64
  ]
 },
 {
  "input": "And I still haven't even talked about how one layer influences the next, but run with me on this one for a moment.",
  "translatedText": "そして、1つのレイヤーが次のレイヤーにどのような影響を与えるかについてはまだ話していない。",
  "model": "DeepL",
  "time_range": [
   408.06,
   413.06
  ]
 },
 {
  "input": "Recognizing a loop can also break down into subproblems.",
  "translatedText": "ループを認識することは、サブ問題に分解することもできる。",
  "model": "DeepL",
  "time_range": [
   413.68,
   416.68
  ]
 },
 {
  "input": "One reasonable way to do this would be to first recognize the various little edges that make it up.",
  "translatedText": "そのための合理的な方法のひとつは、まずそれを構成するさまざまな小さなエッジを認識することだろう。",
  "model": "DeepL",
  "time_range": [
   417.28,
   422.78
  ]
 },
 {
  "input": "Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7, is really just a long edge, or maybe you think of it as a certain pattern of several smaller edges.",
  "translatedText": "同様に、数字の1や4や7に見られるような長い線は、実際には単なる長いエッジである。",
  "model": "DeepL",
  "time_range": [
   423.78,
   434.32
  ]
 },
 {
  "input": "So maybe our hope is that each neuron in the second layer of the network corresponds with the various relevant little edges.",
  "translatedText": "つまり、ネットワークの第2層の各ニューロンが、関連するさまざまな小さなエッジに対応していることが望ましいのかもしれない。",
  "model": "DeepL",
  "time_range": [
   435.14,
   442.72
  ]
 },
 {
  "input": "Maybe when an image like this one comes in, it lights up all of the neurons associated with around 8 to 10 specific little edges, which in turn lights up the neurons associated with the upper loop and a long vertical line, and those light up the neuron associated with a 9.",
  "translatedText": "おそらく、このような画像が入ってくると、8～10個の特定の小さなエッジに関連するニューロンがすべて点灯し、それが上部のループと長い縦線に関連するニューロンを点灯させ、それらが9に関連するニューロンを点灯させるのだろう。",
  "model": "DeepL",
  "time_range": [
   443.54,
   459.72
  ]
 },
 {
  "input": "Whether or not this is what our final network actually does is another question, one that I'll come back to once we see how to train the network, but this is a hope that we might have, a sort of goal with the layered structure like this.",
  "translatedText": "これが最終的なネットワークが実際に行うことなのかどうかはまた別の問題で、ネットワークのトレーニング方法を確認したらまた話をするつもりだが、このようなレイヤー構造を持つことで、ある種の目標を持つことができるかもしれない。",
  "model": "DeepL",
  "time_range": [
   460.68,
   472.54
  ]
 },
 {
  "input": "Moreover, you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks.",
  "translatedText": "さらに、このようにエッジやパターンを検出できるようになれば、他の画像認識タスクにも大いに役立つことは想像できるだろう。",
  "model": "DeepL",
  "time_range": [
   473.16,
   480.3
  ]
 },
 {
  "input": "And even beyond image recognition, there are all sorts of intelligent things you might want to do that break down into layers of abstraction.",
  "translatedText": "また、画像認識だけでなく、抽象化されたレイヤーに分類される、あらゆる種類のインテリジェントなことができる。",
  "model": "DeepL",
  "time_range": [
   480.88,
   487.28
  ]
 },
 {
  "input": "Parsing speech, for example, involves taking raw audio and picking out distinct sounds, which combine to make certain syllables, which combine to form words, which combine to make up phrases and more abstract thoughts, etc.",
  "translatedText": "例えば、音声の解析では、生の音声を聞き取り、明瞭な音を選び出し、それが組み合わさって特定の音節を作り、それが組み合わさって単語を作り、それが組み合わさってフレーズやより抽象的な思考を作る、といった作業を行う。",
  "model": "DeepL",
  "time_range": [
   488.04,
   500.06
  ]
 },
 {
  "input": "But getting back to how any of this actually works, picture yourself right now designing how exactly the activations in one layer might determine the next.",
  "translatedText": "しかし、これが実際にどのように機能するかに話を戻すと、ある層の活性化が次の層の活性化をどのように決定するかを、今自分がデザインしているところを思い浮かべてほしい。",
  "model": "DeepL",
  "time_range": [
   501.1,
   509.92
  ]
 },
 {
  "input": "The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits.",
  "translatedText": "ピクセルをエッジに、エッジをパターンに、パターンを数字に結合するようなメカニズムが考えられる。",
  "model": "DeepL",
  "time_range": [
   510.86,
   518.98
  ]
 },
 {
  "input": "And to zoom in on one very specific example, let's say the hope is for one particular neuron in the second layer to pick up on whether or not the image has an edge in this region here.",
  "translatedText": "具体的な例を挙げると、第2層のあるニューロンが、画像にエッジがあるかどうかを検知する。",
  "model": "DeepL",
  "time_range": [
   519.44,
   530.62
  ]
 },
 {
  "input": "The question at hand is what parameters should the network have?",
  "translatedText": "目下の問題は、ネットワークはどのようなパラメータを持つべきか、ということだ。",
  "model": "DeepL",
  "time_range": [
   531.44,
   535.1
  ]
 },
 {
  "input": "What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern, or any other pixel pattern, or the pattern that several edges can make a loop, and other such things?",
  "translatedText": "このパターンや他のピクセルパターン、複数のエッジがループを作るパターンなどを捉える可能性を十分に表現できるように、どんなダイヤルやノブをいじればいいのか？",
  "model": "DeepL",
  "time_range": [
   535.64,
   547.78
  ]
 },
 {
  "input": "Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer.",
  "translatedText": "さて、これからやることは、ニューロンと第1層のニューロンとの間のそれぞれの接続に重みを割り当てることだ。",
  "model": "DeepL",
  "time_range": [
   548.72,
   555.56
  ]
 },
 {
  "input": "These weights are just numbers.",
  "translatedText": "これらのウェイトは単なる数字に過ぎない。",
  "model": "DeepL",
  "time_range": [
   556.32,
   557.7
  ]
 },
 {
  "input": "Then take all of those activations from the first layer and compute their weighted sum according to these weights.",
  "translatedText": "次に、第1層からすべての活性を取り出し、その重みに従って加重和を計算する。",
  "model": "DeepL",
  "time_range": [
   558.54,
   565.5
  ]
 },
 {
  "input": "I find it helpful to think of these weights as being organized into a little grid of their own, and I'm going to use green pixels to indicate positive weights, and red pixels to indicate negative weights, where the brightness of that pixel is some loose depiction of the weight's value.",
  "translatedText": "緑色のピクセルは正のウエイトを、赤色のピクセルは負のウエイトを示し、そのピクセルの明るさがウエイトの値をゆるやかに表している。",
  "model": "DeepL",
  "time_range": [
   567.7,
   581.78
  ]
 },
 {
  "input": "Now if we made the weights associated with almost all of the pixels zero except for some positive weights in this region that we care about, then taking the weighted sum of all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about.",
  "translatedText": "ここで、私たちが気にしているこの領域で正の重みをいくつか除いて、ほとんどすべてのピクセルに関連する重みをゼロにすると、すべてのピクセルの値の重み付き合計を取ることは、実際には、私たちが気にしている領域内のピクセルの値だけを合計することに等しくなる。",
  "model": "DeepL",
  "time_range": [
   582.78,
   597.82
  ]
 },
 {
  "input": "And if you really wanted to pick up on whether there's an edge here, what you might do is have some negative weights associated with the surrounding pixels.",
  "translatedText": "もし本当にエッジがあるかどうかを調べたいのであれば、周囲のピクセルに負のウェイトを設定すればいい。",
  "model": "DeepL",
  "time_range": [
   599.14,
   606.6
  ]
 },
 {
  "input": "Then the sum is largest when those middle pixels are bright but the surrounding pixels are darker.",
  "translatedText": "そうすると、中央のピクセルが明るく、周囲のピクセルが暗いとき、合計が最大になる。",
  "model": "DeepL",
  "time_range": [
   607.48,
   612.7
  ]
 },
 {
  "input": "When you compute a weighted sum like this, you might come out with any number, but for this network what we want is for activations to be some value between 0 and 1.",
  "translatedText": "このように加重和を計算すると、どのような数値でも出てくるかもしれないが、このネットワークに必要なのは、活性度が0と1の間の値になることだ。",
  "model": "DeepL",
  "time_range": [
   614.26,
   623.54
  ]
 },
 {
  "input": "So a common thing to do is to pump this weighted sum into some function that squishes the real number line into the range between 0 and 1.",
  "translatedText": "そこで一般的に行われるのは、この加重和を、実数線を0から1の範囲に押し込める関数に取り込むことである。",
  "model": "DeepL",
  "time_range": [
   624.12,
   632.14
  ]
 },
 {
  "input": "And a common function that does this is called the sigmoid function, also known as a logistic curve.",
  "translatedText": "これを行う一般的な関数はシグモイド関数と呼ばれ、ロジスティック曲線としても知られている。",
  "model": "DeepL",
  "time_range": [
   632.46,
   637.42
  ]
 },
 {
  "input": "Basically very negative inputs end up close to 0, positive inputs end up close to 1, and it just steadily increases around the input 0.",
  "translatedText": "基本的に、非常にネガティブな入力は0に近く、ポジティブな入力は1に近く、入力0を中心に着実に増加していく。",
  "model": "DeepL",
  "time_range": [
   638.0,
   646.6
  ]
 },
 {
  "input": "So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is.",
  "translatedText": "つまり、ここでのニューロンの活性化とは、基本的に、関連する加重和がどれだけ正であるかを示すものである。",
  "model": "DeepL",
  "time_range": [
   649.12,
   656.36
  ]
 },
 {
  "input": "But maybe it's not that you want the neuron to light up when the weighted sum is bigger than 0.",
  "translatedText": "しかし、加重和が0より大きいときにニューロンを点灯させたいわけではないのかもしれない。",
  "model": "DeepL",
  "time_range": [
   657.54,
   661.88
  ]
 },
 {
  "input": "Maybe you only want it to be active when the sum is bigger than say 10.",
  "translatedText": "例えば、合計が10より大きいときだけアクティブにしたい。",
  "model": "DeepL",
  "time_range": [
   662.28,
   666.36
  ]
 },
 {
  "input": "That is, you want some bias for it to be inactive.",
  "translatedText": "つまり、アクティブでないためには、何らかのバイアスが必要なのだ。",
  "model": "DeepL",
  "time_range": [
   666.84,
   670.26
  ]
 },
 {
  "input": "What we'll do then is just add in some other number like negative 10 to this weighted sum before plugging it through the sigmoid squishification function.",
  "translatedText": "この加重和にマイナス10などの数字を加えてから、シグモイド・スクイッシュ化関数にかけるのだ。",
  "model": "DeepL",
  "time_range": [
   671.38,
   679.66
  ]
 },
 {
  "input": "That additional number is called the bias.",
  "translatedText": "その追加の数値がバイアスと呼ばれる。",
  "model": "DeepL",
  "time_range": [
   680.58,
   682.44
  ]
 },
 {
  "input": "So the weights tell you what pixel pattern this neuron in the second layer is picking up on, and the bias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active.",
  "translatedText": "つまり重みは、第2層のニューロンがどのようなピクセルパターンを拾っているかを示し、バイアスは、ニューロンが意味のある活動を始める前に、重みの合計がどの程度高くなる必要があるかを示す。",
  "model": "DeepL",
  "time_range": [
   683.46,
   695.18
  ]
 },
 {
  "input": "And that is just one neuron.",
  "translatedText": "そして、それはたった1つのニューロンに過ぎない。",
  "model": "DeepL",
  "time_range": [
   696.12,
   697.68
  ]
 },
 {
  "input": "Every other neuron in this layer is going to be connected to all 784 pixel neurons from the first layer, and each one of those 784 connections has its own weight associated with it.",
  "translatedText": "この層の他のすべてのニューロンは、第1層の784個のピクセル・ニューロンすべてに接続され、その784個の接続のひとつひとつに、それぞれの重みが関連づけられている。",
  "model": "DeepL",
  "time_range": [
   698.28,
   710.94
  ]
 },
 {
  "input": "Also, each one has some bias, some other number that you add on to the weighted sum before squishing it with the sigmoid.",
  "translatedText": "また、シグモイドでつぶす前に加重和に加えるバイアスもある。",
  "model": "DeepL",
  "time_range": [
   711.6,
   717.6
  ]
 },
 {
  "input": "And that's a lot to think about!",
  "translatedText": "そして、それは考えることがたくさんある！",
  "model": "DeepL",
  "time_range": [
   718.11,
   719.54
  ]
 },
 {
  "input": "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights, along with 16 biases.",
  "translatedText": "この隠れ層は16ニューロンで、合計784×16の重みと16のバイアスを持つ。",
  "model": "DeepL",
  "time_range": [
   719.96,
   727.98
  ]
 },
 {
  "input": "And all of that is just the connections from the first layer to the second.",
  "translatedText": "そしてこれらはすべて、第1レイヤーから第2レイヤーへの接続にすぎない。",
  "model": "DeepL",
  "time_range": [
   728.84,
   731.94
  ]
 },
 {
  "input": "The connections between the other layers also have a bunch of weights and biases associated with them.",
  "translatedText": "他のレイヤー間のコネクションにも、たくさんの重みとバイアスが関連付けられている。",
  "model": "DeepL",
  "time_range": [
   732.52,
   737.34
  ]
 },
 {
  "input": "All said and done, this network has almost exactly 13,000 total weights and biases.",
  "translatedText": "このネットワークは、重みとバイアスの合計がほぼ正確に13,000である。",
  "model": "DeepL",
  "time_range": [
   738.34,
   743.8
  ]
 },
 {
  "input": "13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways.",
  "translatedText": "13,000ものノブやダイヤルがあり、それらを微調整したり回したりすることで、このネットワークをさまざまな方法で動作させることができる。",
  "model": "DeepL",
  "time_range": [
   743.8,
   749.96
  ]
 },
 {
  "input": "So when we talk about learning, what that's referring to is getting the computer to find a valid setting for all of these many many numbers so that it'll actually solve the problem at hand.",
  "translatedText": "つまり、学習について話すとき、それはコンピュータに、実際に目の前の問題を解決できるように、これら多くの数すべてについて有効な設定を見つけさせることを指している。",
  "model": "DeepL",
  "time_range": [
   751.04,
   761.36
  ]
 },
 {
  "input": "One thought experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand, purposefully tweaking the numbers so that the second layer picks up on edges, the third layer picks up on patterns, etc.",
  "translatedText": "楽しくもあり、恐ろしくもある思考実験のひとつは、座ってこれらの重みとバイアスをすべて手作業で設定し、第2レイヤーがエッジを拾い、第3レイヤーがパターンを拾うように、意図的に数字を微調整することを想像することだ。",
  "model": "DeepL",
  "time_range": [
   762.62,
   776.58
  ]
 },
 {
  "input": "I personally find this satisfying rather than just treating the network as a total black box, because when the network doesn't perform the way you anticipate, if you've built up a little bit of a relationship with what those weights and biases actually mean, you have a starting place for experimenting with how to change the structure to improve.",
  "translatedText": "私自身は、ネットワークを完全にブラックボックスとして扱うよりも、この方が満足できると思っている。なぜなら、ネットワークが予想通りに機能しないとき、その重みとバイアスが実際に何を意味するのか、少しでも関係が築けていれば、どのように構造を変えれば改善されるのか、実験するためのスタート地点に立てるからだ。",
  "model": "DeepL",
  "time_range": [
   776.98,
   794.18
  ]
 },
 {
  "input": "Or when the network does work but not for the reasons you might expect, digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible solutions.",
  "translatedText": "あるいは、ネットワークは機能するが、期待するような理由ではない場合、重みとバイアスが何をしているかを掘り下げることは、あなたの仮定に挑戦し、可能な解決策の全容を明らかにする良い方法である。",
  "model": "DeepL",
  "time_range": [
   794.96,
   805.82
  ]
 },
 {
  "input": "By the way, the actual function here is a little cumbersome to write down, don't you think?",
  "translatedText": "ところで、ここで実際の機能を書くのは少し面倒だと思わないか？",
  "model": "DeepL",
  "time_range": [
   806.84,
   810.68
  ]
 },
 {
  "input": "So let me show you a more notationally compact way that these connections are represented.",
  "translatedText": "そこで、これらのコネクションをよりコンパクトに表記する方法を紹介しよう。",
  "model": "DeepL",
  "time_range": [
   812.5,
   817.14
  ]
 },
 {
  "input": "This is how you'd see it if you choose to read up more about neural networks.",
  "translatedText": "ニューラル・ネットワークについてもっと詳しく読もうと思えば、こうなるだろう。",
  "model": "DeepL",
  "time_range": [
   817.66,
   820.52
  ]
 },
 {
  "input": "Organize all of the activations from one layer into a column as a matrix corresponds to the connections between one layer and a particular neuron in the next layer.",
  "translatedText": "ある層と次の層の特定のニューロンとの間の接続に対応する行列として、ある層からのすべての活性化を列に整理する。",
  "model": "DeepL",
  "time_range": [
   821.38,
   838.0
  ]
 },
 {
  "input": "What that means is that taking the weighted sum of the activations in the first layer according to these weights corresponds to one of the terms in the matrix vector product of everything we have on the left here.",
  "translatedText": "つまり、これらの重みに応じて第1層の活性度の加重和を取ると、左側のすべての行列のベクトル積の項のひとつに相当する、ということだ。",
  "model": "DeepL",
  "time_range": [
   838.54,
   849.88
  ]
 },
 {
  "input": "By the way, so much of machine learning just comes down to having a good grasp of linear algebra, so for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means, take a look at the series I did on linear algebra, especially chapter 3.",
  "translatedText": "ところで、機械学習の多くは線形代数をよく理解しているかどうかにかかっている。行列と行列のベクトル乗算の意味を視覚的に理解したい人は、私が線形代数について書いたシリーズ、特に第3章を見てほしい。",
  "model": "DeepL",
  "time_range": [
   854.0,
   868.6
  ]
 },
 {
  "input": "Back to our expression, instead of talking about adding the bias to each one of these values independently, we represent it by organizing all those biases into a vector, and adding the entire vector to the previous matrix vector product.",
  "translatedText": "式に戻ると、これらの値にそれぞれ独立にバイアスを加えるのではなく、これらのバイアスをすべてベクトルに整理し、そのベクトル全体を前の行列のベクトル積に加えることで表現する。",
  "model": "DeepL",
  "time_range": [
   869.24,
   882.3
  ]
 },
 {
  "input": "Then as a final step, I'll wrap a sigmoid around the outside here, and what that's supposed to represent is that you're going to apply the sigmoid function to each specific component of the resulting vector inside.",
  "translatedText": "そして最後の段階として、シグモイドを外側に巻いてみる。これは、シグモイド関数を内側のベクトルの各成分に適用することを表している。",
  "model": "DeepL",
  "time_range": [
   883.28,
   894.74
  ]
 },
 {
  "input": "So once you write down this weight matrix and these vectors as their own symbols, you can communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression, and this makes the relevant code both a lot simpler and a lot faster, since many libraries optimize the heck out of matrix multiplication.",
  "translatedText": "つまり、重み行列とベクトルをそれぞれのシンボルとして書き出せば、1つのレイヤーから次のレイヤーへのアクティベーションの完全な遷移を、非常にタイトで整然とした小さな式で伝えることができる。",
  "model": "DeepL",
  "time_range": [
   895.94,
   915.66
  ]
 },
 {
  "input": "Remember how earlier I said these neurons are simply things that hold numbers?",
  "translatedText": "先ほど、ニューロンは単に数字を保持するものだと言ったのを覚えているだろうか？",
  "model": "DeepL",
  "time_range": [
   917.82,
   921.46
  ]
 },
 {
  "input": "Well of course the specific numbers that they hold depends on the image you feed in, so it's actually more accurate to think of each neuron as a function, one that takes in the outputs of all the neurons in the previous layer and spits out a number between 0 and 1.",
  "translatedText": "各ニューロンは、前の層のすべてのニューロンの出力を取り込み、0から1の間の数値を出力する。",
  "model": "DeepL",
  "time_range": [
   922.22,
   938.34
  ]
 },
 {
  "input": "Really the entire network is just a function, one that takes in 784 numbers as an input and spits out 10 numbers as an output.",
  "translatedText": "ネットワーク全体は単なる関数で、784個の数字を入力として受け取り、10個の数字を出力として吐き出すものだ。",
  "model": "DeepL",
  "time_range": [
   939.2,
   947.06
  ]
 },
 {
  "input": "It's an absurdly complicated function, one that involves 13,000 parameters in the forms of these weights and biases that pick up on certain patterns, and which involves iterating many matrix vector products and the sigmoid squishification function, but it's just a function nonetheless, and in a way it's kind of reassuring that it looks complicated.",
  "translatedText": "この関数は、特定のパターンをピックアップする重みとバイアスの形で13,000ものパラメータを含み、多くの行列ベクトル積とシグモイド二乗化関数を反復する、とんでもなく複雑な関数だが、それでもただの関数であり、複雑に見えるのはある意味心強い。",
  "model": "DeepL",
  "time_range": [
   947.56,
   966.66
  ]
 },
 {
  "input": "I mean if it were any simpler, what hope would we have that it could take on the challenge of recognizing digits?",
  "translatedText": "つまり、もっと単純なものであれば、数字を認識することに挑戦できるという望みがあるだろうか？",
  "model": "DeepL",
  "time_range": [
   967.34,
   972.28
  ]
 },
 {
  "input": "And how does it take on that challenge?",
  "translatedText": "そして、その挑戦はどのように行われるのだろうか？",
  "model": "DeepL",
  "time_range": [
   973.34,
   974.7
  ]
 },
 {
  "input": "How does this network learn the appropriate weights and biases just by looking at data?",
  "translatedText": "このネットワークは、データを見るだけで、どのようにして適切な重みとバイアスを学習するのだろうか？",
  "model": "DeepL",
  "time_range": [
   975.08,
   979.36
  ]
 },
 {
  "input": "Well that's what I'll show in the next video, and I'll also dig a little more into what this particular network we're seeing is really doing.",
  "translatedText": "それは次のビデオでお見せするとして、私たちが見ているこの特別なネットワークが実際に何をしているのか、もう少し掘り下げてみよう。",
  "model": "DeepL",
  "time_range": [
   980.14,
   986.12
  ]
 },
 {
  "input": "Now is the point I suppose I should say subscribe to stay notified about when video or any new videos come out, but realistically most of you don't actually receive notifications from YouTube, do you?",
  "translatedText": "さて、ここでビデオや新しいビデオが公開されたときに通知を受け取れるように購読を申し込むべきだと思うが、現実的にはYouTubeからの通知を受け取っていない人がほとんどだろう。",
  "model": "DeepL",
  "time_range": [
   987.58,
   997.42
  ]
 },
 {
  "input": "Maybe more honestly I should say subscribe so that the neural networks that underlie YouTube's recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you.",
  "translatedText": "もっと正直に言うと、YouTubeの推薦アルゴリズムの根底にあるニューラルネットワークが、このチャンネルからのコンテンツがあなたに推薦されるのを見たいと思うようにするために、購読すると言うべきかもしれない。",
  "model": "DeepL",
  "time_range": [
   998.02,
   1007.88
  ]
 },
 {
  "input": "Anyway stay posted for more.",
  "translatedText": "とにかく、続報をお待ちいただきたい。",
  "model": "DeepL",
  "time_range": [
   1008.56,
   1009.94
  ]
 },
 {
  "input": "Thank you very much to everyone supporting these videos on Patreon.",
  "translatedText": "Patreonでこのビデオをサポートしてくれている皆さん、本当にありがとう。",
  "model": "DeepL",
  "time_range": [
   1010.76,
   1013.5
  ]
 },
 {
  "input": "I've been a little slow to progress in the probability series this summer, but I'm jumping back into it after this project, so patrons you can look out for updates there.",
  "translatedText": "この夏、私は確率のシリーズの進行が少し遅かったが、このプロジェクトの後、再びこのシリーズに飛び込んでいるので、パトロンの皆さんはそちらの更新を楽しみにしていてほしい。",
  "model": "DeepL",
  "time_range": [
   1014.0,
   1021.9
  ]
 },
 {
  "input": "To close things off here I have with me Leisha Lee who did her PhD work on the theoretical side of deep learning and who currently works at a venture capital firm called Amplify Partners who kindly provided some of the funding for this video.",
  "translatedText": "最後に、ディープラーニングの理論的側面について博士号を取得し、現在はアンプリファイ・パートナーズというベンチャーキャピタルで働いているリーシャ・リーを紹介しよう。",
  "model": "DeepL",
  "time_range": [
   1023.6,
   1034.62
  ]
 },
 {
  "input": "So Leisha one thing I think we should quickly bring up is this sigmoid function.",
  "translatedText": "そこでリーシャ、ひとつ手っ取り早くこのシグモイド関数について話しておこうと思う。",
  "model": "DeepL",
  "time_range": [
   1035.46,
   1039.12
  ]
 },
 {
  "input": "As I understand it early networks use this to squish the relevant weighted sum into that interval between zero and one, you know kind of motivated by this biological analogy of neurons either being inactive or active.",
  "translatedText": "私が理解するところでは、初期のネットワークは、この加重和をゼロと1の間の区間に押し込めるためにこれを使う。",
  "model": "DeepL",
  "time_range": [
   1039.7,
   1049.84
  ]
 },
 {
  "input": "Exactly.",
  "translatedText": "その通りだ。",
  "model": "DeepL",
  "time_range": [
   1050.28,
   1050.3
  ]
 },
 {
  "input": "But relatively few modern networks actually use sigmoid anymore.",
  "translatedText": "しかし、実際にシグモイドを使用している最新のネットワークはもう比較的少ない。",
  "model": "DeepL",
  "time_range": [
   1050.56,
   1054.04
  ]
 },
 {
  "input": "Yeah.",
  "translatedText": "そうだ。",
  "model": "DeepL",
  "time_range": [
   1054.32,
   1054.32
  ]
 },
 {
  "input": "It's kind of old school right?",
  "translatedText": "古臭いだろ？",
  "model": "DeepL",
  "time_range": [
   1054.44,
   1055.54
  ]
 },
 {
  "input": "Yeah or rather relu seems to be much easier to train.",
  "translatedText": "そう、いや、むしろレルーの方がトレーニングしやすいようだ。",
  "model": "DeepL",
  "time_range": [
   1055.76,
   1058.98
  ]
 },
 {
  "input": "And relu stands for rectified linear unit?",
  "translatedText": "reluはrectified linear unitの略か？",
  "model": "DeepL",
  "time_range": [
   1059.4,
   1062.34
  ]
 },
 {
  "input": "Yes it's this kind of function where you're just taking a max of zero and a where a is given by what you were explaining in the video and what this was sort of motivated from I think was a partially by a biological analogy with how neurons would either be activated or not and so if it passes a certain threshold it would be the identity function but if it did not then it would just not be activated so it'd be zero so it's kind of a simplification.",
  "translatedText": "そうだね、ゼロとaの最大値をとるだけの関数で、aはビデオで説明していたもので与えられる。これは、ニューロンが活性化されるかされないかという生物学的な類推から、ある閾値を超えると同一関数になるが、そうでなければ活性化されないのでゼロになる、というような単純化したものだと思う。",
  "model": "DeepL",
  "time_range": [
   1062.68,
   1090.84
  ]
 },
 {
  "input": "Using sigmoids didn't help training or it was very difficult to train at some point and people just tried relu and it happened to work very well for these incredibly deep neural networks.",
  "translatedText": "シグモイドを使ってもトレーニングがうまくいかなかったり、トレーニングが非常に難しかったりしたので、人々はreluを試してみた。",
  "model": "DeepL",
  "time_range": [
   1091.16,
   1104.62
  ]
 },
 {
  "input": "All right thank you Alicia.",
  "translatedText": "アリシア、ありがとう。",
  "model": "DeepL",
  "time_range": [
   1105.1,
   1105.64
  ]
 }
]