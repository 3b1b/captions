[
 {
  "input": "This is a 3.",
  "translatedText": "これは 3 です。",
  "model": "google_nmt",
  "from_community_srt": "これは3です。",
  "n_reviews": 0,
  "start": 4.22,
  "end": 5.4
 },
 {
  "input": "It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3.",
  "translatedText": "雑に書かれ、28x28 ピクセルという非常に低い解像度でレンダリングされ ていますが、脳は問題なく 3 として認識します。",
  "model": "google_nmt",
  "from_community_srt": "これは、28ピクセル×28ピクセルの極端に低い解像度で作成され、書かれています。 あなたの脳は問題なくそれを3と認識するでしょう。",
  "n_reviews": 0,
  "start": 6.06,
  "end": 13.72
 },
 {
  "input": "And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly.",
  "translatedText": "そして、脳がこれを簡単に実行でき ることがどれほどクレイジーであるかを少し理解してもらいたいのです。",
  "model": "google_nmt",
  "from_community_srt": "ここで、私はあなたに理解してもらうために少し時間をかけてほしいことがある。 どのようにして脳が楽々と3だと認識することができたのか？",
  "n_reviews": 0,
  "start": 14.34,
  "end": 18.96
 },
 {
  "input": "I mean, this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next.",
  "translatedText": "つまり、各ピ クセルの具体的な値は画像ごとに大きく異なりますが、これ、これ、こ れも 3 として認識できます。",
  "model": "google_nmt",
  "from_community_srt": "これも、これも、そしてこれもまた3として認識される事を意味します、 ある画像から次の画像に対して、各々のピクセルの特定の値が非常に異なる場合であっても。",
  "n_reviews": 0,
  "start": 19.7,
  "end": 28.32
 },
 {
  "input": "The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3.",
  "translatedText": "この 3 つを見たときに発火する目の特 定の光感受性細胞は、この 3 つを見たときに発火する細胞とは大きく異なり ます。",
  "model": "google_nmt",
  "from_community_srt": "この3を見た時にあなたの目の中で発火している特定の視細胞と この3を見たときに発火している視細胞は全く別物です。",
  "n_reviews": 0,
  "start": 28.9,
  "end": 36.94
 },
 {
  "input": "But something in that crazy-smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas.",
  "translatedText": "しかし、あなたのその非常に賢い視覚野の何かは、これらを同じアイデアを表す ものとして解決し、同時に他の画像をそれら自身の異なるアイデアとして認識します。",
  "model": "google_nmt",
  "from_community_srt": "しかし、あなたの非常に高性能な視覚野の中の何かが これらを同じアイデアを表すものとして理解すると同時に、他のイメージを別個のアイデアとして認識します しかし、もし私があなたにちょっと座って私のためにプログラムを書いてくれと言ったら。",
  "n_reviews": 0,
  "start": 37.52,
  "end": 48.26
 },
 {
  "input": "But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this and outputs a single number between 0 and 10, telling you what it thinks the digit is, well the task goes from comically trivial to dauntingly difficult.",
  "translatedText": "しかし、もし私が、「おい、座って、28x28 のグリッドを取り込んで 0 から 10 まで の 1 つの数値を出力し、その数字が何であるかを示すプログラムを書いてくれ」と言ったら、そ のタスクは滑稽なほど簡単なものから、気の遠くなるような難しい。",
  "model": "google_nmt",
  "from_community_srt": "プログラムは、28×28ピクセルのグリッドを使用し、 0から10の間の単一の数値を出力し、数字が何であるかをあなたに伝えるものです まあ、仕事は馬鹿げて些細なことから、非常に難しいものに変わります",
  "n_reviews": 0,
  "start": 49.22,
  "end": 66.18
 },
 {
  "input": "Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future.",
  "translatedText": "岩の下で生きている人で ない限り、機械学習とニューラル ネットワークの現在と将来に対する関連性と重 要性を説く必要はほとんどないと思います。",
  "model": "google_nmt",
  "from_community_srt": "あなたの知識が非常に限られていない限り、 私は機械学習と神経ネットワークの現在と将来の関連性と重要性を動機付けする必要はほとんどないと思う しかし、私がここでやりたいことは、実際に神経ネットワークが何であるかを示すことです",
  "n_reviews": 0,
  "start": 67.16,
  "end": 74.64
 },
 {
  "input": "But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing, not as a buzzword but as a piece of math.",
  "translatedText": "しかし、私がここでやりたいのは、背景が何もないことを前提 として、ニューラル ネットワークが実際にどのようなものであるかを示し、バズワードとしてではなく数学の一部として、ニューラル ネット ワークが何を行っているのかを視覚化するのに役立つことです。",
  "model": "google_nmt",
  "from_community_srt": "背景がないと仮定し、それが流行語としてではなく、数学的なものとして何をしているのかを視覚化するのを助ける 私の希望は、あなたが次のように感じられる事です。",
  "n_reviews": 0,
  "start": 75.12,
  "end": 84.46
 },
 {
  "input": "My hope is that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote-unquote learning.",
  "translatedText": "私の願いは、構造そのものが動機づけられていると感じて帰っ てきて、ニューラル ネットワークのクォートアンクォート学習について読んだり聞いたりしたときに、それが 何を意味するのかわかったように感じてもらえることだけです。",
  "model": "google_nmt",
  "from_community_srt": "この構造そのものによって動機づけられ、 あなたが「神経ネットワーク」学習に関して読んだり、聞いたりする際に、それが何を意味するかを知っているかのように感じる事です このビデオはそれの構造要素に専念するつもりであり、次のものは学習に取り掛かる予定です",
  "n_reviews": 0,
  "start": 85.02,
  "end": 94.34
 },
 {
  "input": "This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning.",
  "translatedText": "このビデオではその構造コンポ ーネントについてのみ説明し、次のビデオでは学習に取り組みます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.36,
  "end": 100.26
 },
 {
  "input": "What we're going to do is put together a neural network that can learn to recognize handwritten digits.",
  "translatedText": "私たちがやろうとしているのは、手書きの数字の認識を学習できるニューラル ネットワークを構築すること です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 100.96,
  "end": 106.04
 },
 {
  "input": "This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quo here, because at the end of the two videos I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer.",
  "translatedText": "これは、このトピックを紹介するためのやや古典的な例です。 ここでは現状のま まで構いません。 2 つのビデオの最後に、詳細を学ぶことができるいくつかの優れ たリソースを示したいからです。 これを行うコードをダウンロードして、自分のコ ンピュータで試すことができます。",
  "model": "google_nmt",
  "from_community_srt": "私たちがやろうとしていることは、手書きの数字を認識することを学ぶことができる神経ネットワークをまとめることです これはやや古典的な例です トピックを紹介して、私はここで現状を守る事を嬉しく思います。 なぜなら2つのビデオの最後に、あなたに指摘したいのです これを行い、それを使って遊ぶ事ができるコードをどこでダウンロードし、どこで更に学べるかがわかるいくつかの良い素材を 自分のコンピュータで",
  "n_reviews": 0,
  "start": 109.36,
  "end": 123.08
 },
 {
  "input": "There are many many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos you and I are just going to look at the simplest plain vanilla form with no added frills.",
  "translatedText": "ニューラル ネットワークには多くの亜種があり 、近年、これらの亜種に対する研究が一種のブームになっていますが、これら 2 つの紹介ビデオでは、余分な装飾のない、最も単純なプレーン バニラ形式を見 ていきます。",
  "model": "google_nmt",
  "from_community_srt": "近年、神経ネットワークには多くの種類があります。 これらの種類に対する研究では、一種のブームがありました しかし、これらの2つの紹介ビデオでは、あなたと私は単純なフォームを見ようとしています これは必要な前提条件です より強力な現代の種類を理解するために",
  "n_reviews": 0,
  "start": 125.04,
  "end": 139.18
 },
 {
  "input": "This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds around.",
  "translatedText": "これは、より強力な最新の亜種を理解するために必要な前提条件のよ うなものですが、私たちが理解するにはまだ複雑さがたくさんあると信じてくださ い。",
  "model": "google_nmt",
  "from_community_srt": "私の心を包み込むためにはまだまだ複雑です。",
  "n_reviews": 0,
  "start": 139.86,
  "end": 148.6
 },
 {
  "input": "But even in this simplest form it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do.",
  "translatedText": "しかし、この最も単純な形式であっても、手書きの数字を認識することを学習できます。 これは コンピューターにとって非常に素晴らしいことです。",
  "model": "google_nmt",
  "from_community_srt": "しかし、この最も単純な形式でも、手書きの数字を認識することを学ぶことができます これは、コンピュータが行うことができる非常にクールなものです。",
  "n_reviews": 0,
  "start": 149.12,
  "end": 156.52
 },
 {
  "input": "And at the same time you'll see how it does fall short of a couple hopes that we might have for it.",
  "translatedText": "そして同時に、それが私たちがそれに対して抱くかもしれ ないいくつかの希望をいかに満たしていないのかもわかるでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 157.48,
  "end": 162.28
 },
 {
  "input": "As the name suggests neural networks are inspired by the brain, but let's break that down.",
  "translatedText": "名前が示すように、ニューラル ネットワークは脳からイン スピレーションを得ていますが、それを詳しく見てみましょう。",
  "model": "google_nmt",
  "from_community_srt": "それと同時に、あなたが抱いている可能性があるいくつかの要望にはかけている事がわかります 名前が示唆するように、神経ネットワークは脳に触発されていますが、それを打破しましょう 神経とは何ですか？",
  "n_reviews": 0,
  "start": 163.38,
  "end": 168.5
 },
 {
  "input": "What are the neurons, and in what sense are they linked together?",
  "translatedText": "ニューロンとは何ですか? それらはどのような意味でつながって いるのでしょうか?",
  "model": "google_nmt",
  "from_community_srt": "どのような意味で、それらは一緒につながっていますか？",
  "n_reviews": 0,
  "start": 168.52,
  "end": 171.66
 },
 {
  "input": "Right now when I say neuron all I want you to think about is a thing that holds a number, specifically a number between 0 and 1.",
  "translatedText": "今、私がニューロンと言うときに考えていただきたいのは、数値、具体的には 0 と 1 の間の数値を保持するものだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 172.5,
  "end": 180.44
 },
 {
  "input": "It's really not more than that.",
  "translatedText": "本当にそれ以上ではありません。",
  "model": "google_nmt",
  "from_community_srt": "今、私がニューロンと言うとき、あなたが考えて欲しいのは、数字を保持するものです 具体的には、0と1の間の数字です 例えば、ネットワークは、入力画像の28×28画素のそれぞれに対応する一群の神経から始まります",
  "n_reviews": 0,
  "start": 180.68,
  "end": 182.56
 },
 {
  "input": "For example the network starts with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784 neurons in total.",
  "translatedText": "たとえば、ネッ トワークは、入力画像の 28 x 28 ピクセルのそれぞれに対応するニューロンの束から始ま り、合計 784 個のニューロンになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.78,
  "end": 194.22
 },
 {
  "input": "Each one of these holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels.",
  "translatedText": "これらのそれぞれには、黒ピクセルの 0 か ら白ピクセルの 1 までの、対応するピクセルのグレースケール値を表す数値が保持されま す。",
  "model": "google_nmt",
  "from_community_srt": "それは 合計784個の神経の各々は、対応するピクセルのグレースケール値を表す数を保持します 黒ピクセルを表す0から始まり、白ピクセルを表す1までの ニューロン内部のこの数字は活性化と呼ばれ、ここで想像しているかもしれないイメージです",
  "n_reviews": 0,
  "start": 194.7,
  "end": 204.38
 },
 {
  "input": "This number inside the neuron is called its activation, and the image you might have in mind here is that each neuron is lit up when its activation is a high number.",
  "translatedText": "ニューロン内のこの数値は活性化と呼ばれ、活性化の数値が高いと各ニュー ロンが点灯するというイメージを思い浮かべるかもしれません。",
  "model": "google_nmt",
  "from_community_srt": "その活性化が高い数字のときに各ニューロンが点灯していますか？",
  "n_reviews": 0,
  "start": 205.3,
  "end": 214.16
 },
 {
  "input": "So all of these 784 neurons make up the first layer of our network.",
  "translatedText": "したがって、これら 7 84 個のニューロンすべてがネットワークの最初の層を構成します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 216.72,
  "end": 221.86
 },
 {
  "input": "Now jumping over to the last layer, this has 10 neurons, each representing one of the digits.",
  "translatedText": "最後の層に飛びます。 これには 10 個 のニューロンがあり、それぞれが数字の 1 つを表します。",
  "model": "google_nmt",
  "from_community_srt": "したがって、これらの784個のニューロンのすべてが私たちのネットワークの第1層を構成します 最後のレイヤーにジャンプすると、10個のニューロンがそれぞれ数字の1つを表します 0と1の間の数に対するこれらの神経の活性化は、",
  "n_reviews": 0,
  "start": 226.5,
  "end": 231.36
 },
 {
  "input": "The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit.",
  "translatedText": "これらのニューロンの活性化は、 やはり 0 と 1 の間の数値であり、特定の画像が特定の数字に対応しているとシステ ムがどの程度考えているかを表します。",
  "model": "google_nmt",
  "from_community_srt": "与えられたイメージがシステムにどのくらいあると考えるかを表します。 指定された桁に対応します。",
  "n_reviews": 0,
  "start": 232.04,
  "end": 242.12
 },
 {
  "input": "There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark for how on earth this process of recognizing digits is going to be handled.",
  "translatedText": "間には隠れ層と呼ばれるいくつかの層も ありますが、当面はこの数字を認識するプロセスが一体どのように処理され るのかという大きな疑問符が付くはずです。",
  "model": "google_nmt",
  "from_community_srt": "間には隠れたレイヤーと呼ばれる2つのレイヤーがあります それは当分の間ですか？ 地球上でどのように数字を認識するかについての巨大な疑問符でなければなりません。",
  "n_reviews": 0,
  "start": 243.04,
  "end": 253.6
 },
 {
  "input": "In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice.",
  "translatedText": "このネットワークでは、それぞれ 16 個の ニューロンを持つ 2 つの隠れ層を選択しましたが、確かに、これは一種の恣意的な選択です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 254.26,
  "end": 260.56
 },
 {
  "input": "To be honest I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that was just a nice number to fit on the screen.",
  "translatedText": "正直に言うと、 一瞬のうちに構造をどのように動かしたいかに基づいて 2 つのレイヤーを選択しました。 16 は、画面に収まるちょうどいい数でした。",
  "model": "google_nmt",
  "from_community_srt": "このネットワークでは、それぞれ16個のニューロンを持つ2つの隠れたレイヤーを選択しましたが、これは任意の選択肢です 正直言って私はちょうどその瞬間に構造に動機づけをしたい、 16はうまく実際に画面に収まるような素敵な数字でした",
  "n_reviews": 0,
  "start": 261.02,
  "end": 268.2
 },
 {
  "input": "In practice there is a lot of room for experiment with a specific structure here.",
  "translatedText": "実際には、ここには特定の構造を実験する余 地がたくさんあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 268.78,
  "end": 272.34
 },
 {
  "input": "The way the network operates, activations in one layer determine the activations of the next layer.",
  "translatedText": "ネットワークの動作方法では、ある層でのアクティベーションが次の層のアク ティベーションを決定します。",
  "model": "google_nmt",
  "from_community_srt": "ここには特定の構造の実験のための余地がたくさんあります ネットワークが1つの層で活性化を実行する方法は、次の層の活性化を決定します もちろん、情報処理の仕組みとしてのネットワークの核心は、",
  "n_reviews": 0,
  "start": 273.02,
  "end": 278.48
 },
 {
  "input": "And of course the heart of the network as an information processing mechanism comes down to exactly how those activations from one layer bring about activations in the next layer.",
  "translatedText": "そしてもちろん、情報処理メカニズムとしてのネットワーク の中心は、ある層からの活性化が次の層の活性化をどのように引き起こすかということ になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 279.2,
  "end": 288.58
 },
 {
  "input": "It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire.",
  "translatedText": "これは、ニューロンの生物学的ネットワークにおいて、あるニューロンのグループの発火が他のニュー ロンの発火を引き起こす方法に大まかに似ていることを意図しています。",
  "model": "google_nmt",
  "from_community_srt": "1つの層からの活性化が、どのように次の層の活性化を引き起こしますかによります それは、神経の生物学的ネットワークにおいて、どのようにあるグループの神経点火がある他のグループを点火させるのかをゆるやかに比較する事を意味しています",
  "n_reviews": 0,
  "start": 289.14,
  "end": 297.18
 },
 {
  "input": "Now the network I'm showing here has already been trained to recognize digits, and let me show you what I mean by that.",
  "translatedText": "ここで示しているネットワークはすでに数 字を認識するように訓練されています。 これが何を意味するのかを説明しましょう。",
  "model": "google_nmt",
  "from_community_srt": "今やネットワーク 私がここに示しているのは、既に数字を認識するように訓練されており、それにより私が意味するものをお見せしています それは、もしあなたが画像内の各画素の輝度に応じて入力層の全ての784個の神経を輝かせた画像を与えたなら",
  "n_reviews": 0,
  "start": 298.12,
  "end": 303.4
 },
 {
  "input": "It means if you feed in an image, lighting up all 784 neurons of the input layer according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer which causes some pattern in the one after it, which finally gives some pattern in the output layer.",
  "translatedText": "これは、画像内の 各ピクセルの明るさに応じて入力層の 784 個のニューロンすべてを点灯する画像を入力す ると、その活性化パターンが次の層で非常に特殊なパターンを引き起こし、その次の層で何らか のパターンが発生することを意味します。 これにより、最終的に出力層にパターンが与えられま す。",
  "model": "google_nmt",
  "from_community_srt": "その活性化のパターンは、次の層において非常に特異的なパターンを生じる どれが、一つのパターンの後の一つにおいて、いくつかのパターンを引き起こすのでしょうか？ 最終的には、どれが出力層にいくつかのパターンを与えるのでしょうか？",
  "n_reviews": 0,
  "start": 303.64,
  "end": 322.08
 },
 {
  "input": "And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents.",
  "translatedText": "そして、その出力層の最も明るいニューロンは、いわば、この画像が表す桁をネットワ ークが選択することになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 322.56,
  "end": 329.4
 },
 {
  "input": "And before jumping into the math for how one layer influences the next, or how training works, let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently.",
  "translatedText": "ある層が次の層にどのような影響を与えるか、またはトレーニングがど のように機能するかについての数学に入る前に、このような層構造がインテリジェントに動作すると期待する のが合理的である理由について少しお話しましょう。",
  "model": "google_nmt",
  "from_community_srt": "そして その出力層の最も明るい神経はネットワークの選択であり、言わば、この画像がどの桁を表しているかという事である どのようにして1つのレイヤーが次のレイヤーに影響するか、または、どのようにしてトレーニングが機能するのかについての数学に飛び込む前に、 何故、このような階層化された構造が賢く動作すると期待する事が将に合理的であるかについて話しましょう ここで何を期待していますか？",
  "n_reviews": 0,
  "start": 332.56,
  "end": 343.52
 },
 {
  "input": "What are we expecting here?",
  "translatedText": "ここで私たちは何を期待しているのでしょうか？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 344.06,
  "end": 345.22
 },
 {
  "input": "What is the best hope for what those middle layers might be doing?",
  "translatedText": "これらの中間層がやっている 可能性のあることに対する最善の希望は何でしょうか?",
  "model": "google_nmt",
  "from_community_srt": "それらの中間層が何をしているのかについての最適な望みは何ですか？",
  "n_reviews": 0,
  "start": 345.4,
  "end": 347.6
 },
 {
  "input": "Well, when you or I recognize digits, we piece together various components.",
  "translatedText": "さて、あなたまたは私が数字を認識するとき、私たちはさまざまなコンポーネ ントを組み合わせます。",
  "model": "google_nmt",
  "from_community_srt": "あなたや私が数字を認識する時、さまざまな構成要素を一つにします。",
  "n_reviews": 0,
  "start": 348.92,
  "end": 353.52
 },
 {
  "input": "A 9 has a loop up top and a line on the right.",
  "translatedText": "A 9 には上部にループがあり、右側にラインがあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 354.2,
  "end": 356.82
 },
 {
  "input": "An 8 also has a loop up top, but it's paired with another loop down low.",
  "translatedText": "8 にもトップにループがあります が、ローに別のループが組み合わされています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 357.38,
  "end": 361.18
 },
 {
  "input": "A 4 basically breaks down into three specific lines, and things like that.",
  "translatedText": "A 4 は基本的に 3 つの特定の行に分 かれています。",
  "model": "google_nmt",
  "from_community_srt": "9は丸が上にあり、右に縦線があります 8も上に丸がありますが、下にある他の丸と対になっています 4は基本的に3つの特定の行とそのようなものに分解されます 完璧な世界では、2番目から最後の層の各神経は、これらの副構成品の一つとして対応する",
  "n_reviews": 0,
  "start": 361.98,
  "end": 366.82
 },
 {
  "input": "Now in a perfect world, we might hope that each neuron in the second to last layer corresponds with one of these subcomponents, that anytime you feed in an image with, say, a loop up top, like a 9 or an 8, there's some specific neuron whose activation is going to be close to 1.",
  "translatedText": "これで完璧な世界では、最後から 2 番目の層の各ニューロンがこれらのサブコン ポーネントの 1 つに対応し、たとえば 9 や 8 などの上部にループがある画像を入力するたび に、いくつかのサブコンポーネントが存在することを期待できます。 活性化が 1 に近づく特定のニュ ーロン。",
  "model": "google_nmt",
  "from_community_srt": "それは、あなたが画像を入力する度に、9または8のような上の丸を言います 活性化が１に近づいているいくつの特定の神経があります 私はこの特定の画像の丸を意味するわけではありません。",
  "n_reviews": 0,
  "start": 367.6,
  "end": 383.78
 },
 {
  "input": "And I don't mean this specific loop of pixels, the hope would be that any generally loopy pattern towards the top sets off this neuron.",
  "translatedText": "そして、私はこの特定のピクセルのループを意味するのではなく、上部に向かう一般的なループ状のパ ターンがこのニューロンを引き起こすことを期待しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 384.5,
  "end": 391.56
 },
 {
  "input": "That way, going from the third layer to the last one just requires learning which combination of subcomponents corresponds to which digits.",
  "translatedText": "そうすれば、3 番目の層から最後の層 に進むには、サブコンポーネントのどの組み合わせがどの数字に対応するかを学習するだけで 済みます。",
  "model": "google_nmt",
  "from_community_srt": "一般的には、上にある丸のパターンは第3層から最後の層に向かう神経を導きます サブコンポーネントのどの組み合わせがどの桁に対応するかを学習するだけでよい もちろん、それは道のりで問題を引き起こすだけです",
  "n_reviews": 0,
  "start": 392.44,
  "end": 400.04
 },
 {
  "input": "Of course, that just kicks the problem down the road, because how would you recognize these subcomponents, or even learn what the right subcomponents should be?",
  "translatedText": "もちろん、これは将来の問題を引き起こすだけです。 なぜなら、これらのサブコンポーネントをどうやって認識 するのでしょうか、あるいは、適切なサブコンポーネントが何であるべきかを知ることさえできるでしょうか?",
  "model": "google_nmt",
  "from_community_srt": "あなたがこれらのサブコンポーネントをどのように認識するか、あるいは正しいサブコンポーネントがどんなものであるべきかを学ぶことさえあります。",
  "n_reviews": 0,
  "start": 401.0,
  "end": 407.64
 },
 {
  "input": "And I still haven't even talked about how one layer influences the next, but run with me on this one for a moment.",
  "translatedText": "あるレイヤーが次 のレイヤーにどのような影響を与えるかについてはまだ話していませんが、この点について少し一緒に考えてみましょう。",
  "model": "google_nmt",
  "from_community_srt": "私はまだ説明していませんが どの層が次の層にどのように影響しますか？",
  "n_reviews": 0,
  "start": 408.06,
  "end": 413.06
 },
 {
  "input": "Recognizing a loop can also break down into subproblems.",
  "translatedText": "ループを認識すると、サブ問題に分解することもできます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 413.68,
  "end": 416.68
 },
 {
  "input": "One reasonable way to do this would be to first recognize the various little edges that make it up.",
  "translatedText": "これを行うための合理的な方法の 1 つは 、まずそれを構成するさまざまな小さなエッジを認識することです。",
  "model": "google_nmt",
  "from_community_srt": "丸を認識することは、付随する問題に分解することもできます これを行うための1つの合理的な方法は、最初にそれを構成する様々な小さな端を認識することです 同様に、数字1または4または7において、そのような長い線をみる事でしょう",
  "n_reviews": 0,
  "start": 417.28,
  "end": 422.78
 },
 {
  "input": "Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7, is really just a long edge, or maybe you think of it as a certain pattern of several smaller edges.",
  "translatedText": "同様に、数字の 1、4、または 7 に見られるような長い線も、実際には単なる長いエッジであるか、あるいはいくつかの小さなエ ッジからなる特定のパターンと考えることもできます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 423.78,
  "end": 434.32
 },
 {
  "input": "So maybe our hope is that each neuron in the second layer of the network corresponds with the various relevant little edges.",
  "translatedText": "したがって、おそらく私たちの希望は、ネットワーク の 2 番目の層の各ニューロンが、関連するさまざまな小さなエッジに対応することです。",
  "model": "google_nmt",
  "from_community_srt": "まあ、それは実際には長いエッジか多分あなたはいくつかの小さなエッジの特定のパターンとしてそれを考える だから、私たちの希望は、ネットワークの第2層の各ニューロン 様々な関連する小さなエッジに対応する",
  "n_reviews": 0,
  "start": 435.14,
  "end": 442.72
 },
 {
  "input": "Maybe when an image like this one comes in, it lights up all of the neurons associated with around 8 to 10 specific little edges, which in turn lights up the neurons associated with the upper loop and a long vertical line, and those light up the neuron associated with a 9.",
  "translatedText": "おそらく、このよう な画像が入力されると、約 8 ～ 10 個の特定の小さなエッジに関連付けられたすべてのニ ューロンが点灯し、次に上部のループと長い垂直線に関連付けられたニューロンが点灯し、それら のニューロンが点灯します。 9に関連するニューロン。",
  "model": "google_nmt",
  "from_community_srt": "たぶん、このような画像が入ってくると、すべてのニューロンが点灯します 約8〜10個の特定の小さなエッジに関連する それは上のループに関連するニューロンと長い垂直線を点灯させ、 それらは9つのニューロンに関連している",
  "n_reviews": 0,
  "start": 443.54,
  "end": 459.72
 },
 {
  "input": "Whether or not this is what our final network actually does is another question, one that I'll come back to once we see how to train the network,",
  "translatedText": "これが最終的なネットワークが実際に行うこ とであるかどうかは別の問題であり、ネットワークをトレーニングする方法がわかったら、またこの問題に 戻ります。",
  "model": "google_nmt",
  "from_community_srt": "それとも これが私たちの最終的なネットワークが実際にやっていることです。 もう一つの質問です。 ネットワークを鍛える方法を見たら、私は戻ってきます。",
  "n_reviews": 0,
  "start": 460.68,
  "end": 467.18
 },
 {
  "input": "but this is a hope that we might have, a sort of goal with the layered structure like this.",
  "translatedText": "しかし、これは私たちが持つかもしれない希望であり、このような階層構造を持つ一種の目 標です。",
  "model": "google_nmt",
  "from_community_srt": "しかし、これは私たちが持つ可能性のある希望です。",
  "n_reviews": 0,
  "start": 467.5,
  "end": 472.54
 },
 {
  "input": "Moreover, you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks.",
  "translatedText": "さらに、このようにエッジやパターンを検出できれば、他の画像認識タ スクにも非常に役立つことが想像できます。",
  "model": "google_nmt",
  "from_community_srt": "このような階層構造の目標 さらに、このようなエッジやパターンをどのように検出することが他の画像認識タスクにとって本当に役に立つか想像することができます さらに、画像認識以外にも、抽象化のレイヤーに分解することができる、あらゆる種類のインテリジェントなものがあります",
  "n_reviews": 0,
  "start": 473.16,
  "end": 480.3
 },
 {
  "input": "And even beyond image recognition, there are all sorts of intelligent things you might want to do that break down into layers of abstraction.",
  "translatedText": "画像認識を超えて、抽象化の層 に分割して実行したいあらゆる種類のインテリジェントな処理が存在し ます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 480.88,
  "end": 487.28
 },
 {
  "input": "Parsing speech, for example, involves taking raw audio and picking out distinct sounds, which combine to make certain syllables, which combine to form words, which combine to make up phrases and more abstract thoughts, etc.",
  "translatedText": "たとえば、音声の解析には、生の音声を取り込み、特定の音節を形成するために 結合したり、単語を形成したり結合してフレーズやより抽象的な思考を構成したりする個 別の音を抽出することが含まれます。",
  "model": "google_nmt",
  "from_community_srt": "例えば、音声を解析するには、生の音声を取り出し、結合して特定の音節 フレーズとより抽象的な思考を組み立てるために結合する単語を形成するために結合するもの しかし、これが実際にどのように実際に動作するかに戻ることは、現在自分自身を描いています",
  "n_reviews": 0,
  "start": 488.04,
  "end": 500.06
 },
 {
  "input": "But getting back to how any of this actually works, picture yourself right now designing how exactly the activations in one layer might determine the next.",
  "translatedText": "しかし、これが実際にどのように機能するかに戻って、ある層 のアクティベーションが次の層のアクティベーションをどのように正確に決定するかを今設計している自分を 想像してみてください。",
  "model": "google_nmt",
  "from_community_srt": "1つの層の活性化が次の層の活性化をどの程度正確に決定するか？",
  "n_reviews": 0,
  "start": 501.1,
  "end": 509.92
 },
 {
  "input": "The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits.",
  "translatedText": "目標は、ピクセルをエッジに結合したり、エッジをパターンに結合したり、パター ンを数字に結合したりできる何らかのメカニズムを持つことです。",
  "model": "google_nmt",
  "from_community_srt": "目標は、ピクセルをエッジに組み合わせる可能性のあるメカニズムを持つことです パターンやパターンの辺へのエッジや、非常に具体的な例をズームインする 希望が1つの特定のものだとしましょう",
  "n_reviews": 0,
  "start": 510.86,
  "end": 518.98
 },
 {
  "input": "And to zoom in on one very specific example, let's say the hope is for one particular neuron in the second layer to pick up on whether or not the image has an edge in this region here.",
  "translatedText": "そして、1 つの非常に具体的な例にズー ムインするために、2 番目の層の 1 つの特定のニューロンが、画像のこの領域にエッジがあるかどう かを検出することが期待されているとします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 519.44,
  "end": 530.62
 },
 {
  "input": "The question at hand is what parameters should the network have?",
  "translatedText": "ここでの問題は、ネットワークにどのようなパラメータが必要かとい うことです。",
  "model": "google_nmt",
  "from_community_srt": "2番目の層のニューロンは、画像がこの領域にエッジを持っているかどうかをここでピックアップします 手元にある質問は、ネットワークがどのようなパラメータを持っているべきかということです どのようなダイヤルやノブを微調整して、このパターンを潜在的に十分に表現できるようにするか、",
  "n_reviews": 0,
  "start": 531.44,
  "end": 535.1
 },
 {
  "input": "What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern, or any other pixel pattern, or the pattern that several edges can make a loop, and other such things?",
  "translatedText": "このパターンやその他のピクセル パターン、あるいは複数のエッジがループを 作るパターンなどを潜在的に捉えるのに十分な表現力を持たせるためには、どのダイヤルや ノブを調整すればよいでしょうか?",
  "model": "google_nmt",
  "from_community_srt": "他のピクセルパターンや、いくつかのエッジがループなどのことができるパターン？",
  "n_reviews": 0,
  "start": 535.64,
  "end": 547.78
 },
 {
  "input": "Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer.",
  "translatedText": "さて、これから行うことは、ニューロンと最初の層の ニューロンの間の接続のそれぞれに重みを割り当てることです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 548.72,
  "end": 555.56
 },
 {
  "input": "These weights are just numbers.",
  "translatedText": "これらの重みは単なる数値 です。",
  "model": "google_nmt",
  "from_community_srt": "さて、私たちがすることは、最初の層からのニューロンとニューロンとの間の接続のそれぞれに重みを割り当てることです これらの重みはちょうど数字です 最初のレイヤーからすべてのアクティベーションを取り、これらの重みIに従って重み付けされた合計を計算します",
  "n_reviews": 0,
  "start": 556.32,
  "end": 557.7
 },
 {
  "input": "Then take all of those activations from the first layer and compute their weighted sum according to these weights.",
  "translatedText": "次に、最初の層からこれらのアクティベーションをすべて取得し、これらの重みに従って重み付け された合計を計算します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 558.54,
  "end": 565.5
 },
 {
  "input": "I find it helpful to think of these weights as being organized into a little grid of their own, and I'm going to use green pixels to indicate positive weights, and red pixels to indicate negative weights, where the brightness of that pixel is some loose depiction of the weight's value.",
  "translatedText": "これらの重みが独自の小さなグリッドに編成されていると考える とわかりやすいと思います。 正の重みを示すために緑のピクセルを使用し、負の重みを示 すために赤のピクセルを使用します。 そのピクセルの明るさはある程度です。 重みの値 のゆるやかな描写。",
  "model": "google_nmt",
  "from_community_srt": "これらの重みを自分の小さなグリッドに編成していると考えると役立ちます また、正の重みを示す緑のピクセルと負の重みを示す赤のピクセルを使用します そのピクセルの明るさは、重み値の何らかの緩やかな描写であるか？",
  "n_reviews": 0,
  "start": 567.7,
  "end": 581.78
 },
 {
  "input": "Now if we made the weights associated with almost all of the pixels zero except for some positive weights in this region that we care about, then taking the weighted sum of all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about.",
  "translatedText": "関心のあるこの領域のいくつかの正の重みを除いて、ほぼすべて のピクセルに関連付けられた重みをゼロにすると、すべてのピクセル値の重み付き 合計を取ることは、実際にはピクセルの値を合計することになります。 私たちが大 切にしている地域。",
  "model": "google_nmt",
  "from_community_srt": "今では、ほとんどすべてのピクセルに関連付けられた重みをゼロ この地域のいくつかの肯定的な重みを除いて 次に、 すべてのピクセル値は実際には気になる領域のピクセル値を加算するだけです そして、あなたが本当にそれがここにエッジがあるかどうかを拾うことを望むなら、あなたがするかもしれないことは、いくつかの負の重みを持つことです",
  "n_reviews": 0,
  "start": 582.78,
  "end": 597.82
 },
 {
  "input": "And if you really wanted to pick up on whether there's an edge here, what you might do is have some negative weights associated with the surrounding pixels.",
  "translatedText": "ここにエッジがあるかどうかを本当に確認したい場合は 、周囲のピクセルに負の重みを関連付けることが考えられます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 599.14,
  "end": 606.6
 },
 {
  "input": "Then the sum is largest when those middle pixels are bright but the surrounding pixels are darker.",
  "translatedText": "中央のピ クセルが明るく、周囲のピクセルが暗い場合、合計は最大になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 607.48,
  "end": 612.7
 },
 {
  "input": "When you compute a weighted sum like this, you might come out with any number, but for this network what we want is for activations to be some value between 0 and 1.",
  "translatedText": "このように加重合計を計算すると、任意の数値が得られる可能性がありますが、このネットワークで必要 なのは、アクティベーションが 0 と 1 の間の値であることです。",
  "model": "google_nmt",
  "from_community_srt": "周辺画素に関連する その中間のピクセルが明るいときに合計が最大になりますが、周囲のピクセルはより暗くなります このような加重和を計算すると、任意の数 しかし、このネットワークでは、活性化が0＆1の間の値になるようにしたい",
  "n_reviews": 0,
  "start": 614.26,
  "end": 623.54
 },
 {
  "input": "So a common thing to do is to pump this weighted sum into some function that squishes the real number line into the range between 0 and 1.",
  "translatedText": "したがって、一般的に行うべき ことは、この重み付けされた合計を、実数直線を 0 と 1 の間の範囲に押し込む何らかの関数にポンプ することです。",
  "model": "google_nmt",
  "from_community_srt": "そのため、一般的なことは、この加重和をポンピングすることです 実数の線を0と1の間の範囲に縮めるいくつかの関数に これを行う一般的な関数は、ロジスティック曲線とも呼ばれるシグモイド関数と呼ばれます",
  "n_reviews": 0,
  "start": 624.12,
  "end": 632.14
 },
 {
  "input": "And a common function that does this is called the sigmoid function, also known as a logistic curve.",
  "translatedText": "これを行う一般的な関数はシグモイド関数と呼ばれ、ロジスティック曲線としても知 られています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 632.46,
  "end": 637.42
 },
 {
  "input": "Basically very negative inputs end up close to 0, positive inputs end up close to 1, and it just steadily increases around the input 0.",
  "translatedText": "基本的に、非常に負の入力は 0 に近くなり、非常に正の入力は 1 に近くなり、入力 0 の周りで着実に増加します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 638.0,
  "end": 646.6
 },
 {
  "input": "So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is.",
  "translatedText": "したがって、ここでのニューロンの活性化は 、基本的に、関連する加重合計がどれだけ正であるかの尺度になります。",
  "model": "google_nmt",
  "from_community_srt": "基本的に非常に負の入力はゼロに近づく非常に正の入力は1に近づく それはちょうど入力0の周りで着実に増加します したがって、ここでのニューロンの活性化は、基本的に、関連する加重和がどのようにプラスであるかの尺度である",
  "n_reviews": 0,
  "start": 649.12,
  "end": 656.36
 },
 {
  "input": "But maybe it's not that you want the neuron to light up when the weighted sum is bigger than 0.",
  "translatedText": "しかし、重み付けされた合計が 0 よ り大きいときにニューロンを点灯させたいわけではないかもしれません。",
  "model": "google_nmt",
  "from_community_srt": "しかし、重み付けされた合計が0より大きい場合にニューロンを点灯させたいというわけではないかもしれません たぶんあなたは、合計が10より大きいときにアクティブにしたいだけかもしれません",
  "n_reviews": 0,
  "start": 657.54,
  "end": 661.88
 },
 {
  "input": "Maybe you only want it to be active when the sum is bigger than say 10.",
  "translatedText": "おそらく、合計が 10 よりも大きい場合に のみアクティブにしたい場合があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 662.28,
  "end": 666.36
 },
 {
  "input": "That is, you want some bias for it to be inactive.",
  "translatedText": "つまり、非アクティブにするために何らかのバイアスが必要です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 666.84,
  "end": 670.26
 },
 {
  "input": "What we'll do then is just add in some other number like negative 10 to this weighted sum before plugging it through the sigmoid squishification function.",
  "translatedText": "次に行うこ とは、シグモイド圧縮関数に接続する前に、この加重合計にマイナス 10 などの他の数値を加算することです。",
  "model": "google_nmt",
  "from_community_srt": "それはあなたがそれが非アクティブであるためのいくつかのバイアスが欲しいです 私たちがやることは、この加重和に負の10のような他の数を加えるだけです Sigmoid squishification機能を使ってプラグを差し込む前に",
  "n_reviews": 0,
  "start": 671.38,
  "end": 679.66
 },
 {
  "input": "That additional number is called the bias.",
  "translatedText": "この追加の数はバイアスと呼ばれます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 680.58,
  "end": 682.44
 },
 {
  "input": "So the weights tell you what pixel pattern this neuron in the second layer is picking up on, and the bias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active.",
  "translatedText": "したがっ て、重みは、2 番目の層のこのニューロンがどのピクセル パターンを認識しているかを示し、バイアスは、ニュー ロンが意味のあるアクティブになり始める前に、重み付けされた合計がどのくらい大きくなければならないかを示し ます。",
  "model": "google_nmt",
  "from_community_srt": "その追加数はバイアスと呼ばれます したがって、ウェイトは、2番目のレイヤーのこのニューロンがどのピクセルパターンでピックアップしているかを示し、バイアス ニューロンが有意義にアクティブになる前に、加重合計がどれくらい高い必要があるかを示します",
  "n_reviews": 0,
  "start": 683.46,
  "end": 695.18
 },
 {
  "input": "And that is just one neuron.",
  "translatedText": "そしてそれは単なる 1 つのニューロンです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 696.12,
  "end": 697.68
 },
 {
  "input": "Every other neuron in this layer is going to be connected to all 784 pixel neurons from the first layer, and each one of those 784 connections has its own weight associated with it.",
  "translatedText": "この層の 1 つおきのニューロンは、最初の層の 784 個 のピクセル ニューロンすべてに接続され、これらの 784 個の接続のそれぞれに独自の重 みが関連付けられます。",
  "model": "google_nmt",
  "from_community_srt": "それはただのニューロンです この層の他のすべてのニューロンは、すべての 第1の層からの784ピクセルのニューロンおよびこれらの784個の接続の各々はそれに関連するそれ自体の重みを有する",
  "n_reviews": 0,
  "start": 698.28,
  "end": 710.94
 },
 {
  "input": "Also, each one has some bias, some other number that you add on to the weighted sum before squishing it with the sigmoid.",
  "translatedText": "また、それぞれには何らかのバイアスがあり、シグモイドで押しつぶす前 に加重和に加算する他の数値が含まれています。",
  "model": "google_nmt",
  "from_community_srt": "また、それぞれにはいくつかのバイアスがあります。 これは、シグモイドでそれを潰す前に加重合計に加算します。",
  "n_reviews": 0,
  "start": 711.6,
  "end": 717.6
 },
 {
  "input": "And that's a lot to think about!",
  "translatedText": "そして、それは考えるべきことがたくさんあります！",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 718.11,
  "end": 719.54
 },
 {
  "input": "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights, along with 16 biases.",
  "translatedText": "この 16 ニュー ロンの隠れ層では、合計 784 × 16 の重みと 16 のバイアスになります。",
  "model": "google_nmt",
  "from_community_srt": "この16個のニューロンの隠れたレイヤーについて考えておくと、 それは合計で784回の16回の重みと16回の偏りです そして、それはすべて、第1層から第2層への接続だけであり、他の層 また、それらに関連した重さと偏りの束",
  "n_reviews": 0,
  "start": 719.96,
  "end": 727.98
 },
 {
  "input": "And all of that is just the connections from the first layer to the second.",
  "translatedText": "これらはすべて 、第 1 層から第 2 層への接続にすぎません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 728.84,
  "end": 731.94
 },
 {
  "input": "The connections between the other layers also have a bunch of weights and biases associated with them.",
  "translatedText": "他の層間の接続にも、それ らに関連する一連の重みとバイアスがあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 732.52,
  "end": 737.34
 },
 {
  "input": "All said and done, this network has almost exactly 13,000 total weights and biases.",
  "translatedText": "結局のところ、このネットワーク には、合計でほぼ正確に 13,000 の重みとバイアスがあります。",
  "model": "google_nmt",
  "from_community_srt": "すべてのことが言って、このネットワークを行ったほとんど正確に 合計13,000の重みと偏り このネットワークをさまざまな方法で動作させるために調整可能な13,000個のノブとダイヤル",
  "n_reviews": 0,
  "start": 738.34,
  "end": 743.8
 },
 {
  "input": "13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways.",
  "translatedText": "13,000 個のノブとダイヤルを調整した り回して、このネットワークをさまざまな方法で動作させることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 743.8,
  "end": 749.96
 },
 {
  "input": "So when we talk about learning, what that's referring to is getting the computer to find a valid setting for all of these many many numbers so that it'll actually solve the problem at hand.",
  "translatedText": "したがって、学習について話すとき 、それが指しているのは、目前の問題を実際に解決できるように、これらの多くの数値すべてに対し て有効な設定をコンピューターに見つけさせることです。",
  "model": "google_nmt",
  "from_community_srt": "だから私たちは学習について話すとき？ それは、実際に解決されるように、これらの多くの数字のすべてのための有効な設定を見つけるためにコンピュータを取得させることです 手元の問題 思考 すぐに楽しいと恐ろしいの実験は、座っていることを想像し、これらの重みと偏りのすべてを手で設定することです",
  "n_reviews": 0,
  "start": 751.04,
  "end": 761.36
 },
 {
  "input": "One thought experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand, purposefully tweaking the numbers so that the second layer picks up on edges, the third layer picks up on patterns, etc.",
  "translatedText": "楽しくもあり、ある意味恐ろしくもある 思考実験の 1 つは、座ってこれらの重みとバイアスをすべて手動で設定し、2 番目の レイヤーがエッジを認識し、3 番目のレイヤーがパターンを認識するように数値を意図的に 微調整することを想像することです。",
  "model": "google_nmt",
  "from_community_srt": "意図的に数字を微調整して、2番目のレイヤーがエッジでピックアップし、3番目のレイヤーがパターンなどでピックアップするようにします。",
  "n_reviews": 0,
  "start": 762.62,
  "end": 776.58
 },
 {
  "input": "I personally find this satisfying rather than just treating the network as a total black box, because when the network doesn't perform the way you anticipate, if you've built up a little bit of a relationship with what those weights and biases actually mean, you have a starting place for experimenting with how to change the structure to improve.",
  "translatedText": "等私個人としては、ネットワークを完全なブラック ボ ックスとして扱うよりも、これで満足できると感じています。 なぜなら、ネットワークが期待 どおりに動作しないとき、それらの重みやバイアスが実際に何を意味するのかについて少しでも 関係を構築していれば、 、改善するために構造を変更する方法を実験するための出発点が得 られます。",
  "model": "google_nmt",
  "from_community_srt": "私は個人的には、ネットワーク全体をブラックボックスとして読むのではなく、 ネットワークがあなたの それらの重みや偏りが実際にあなたが出発点を持つことを意味するものと少しの関係を築き上げているかどうかを予測してください ネットワークを改善するために、あるいはネットワークが機能するように構造を変更する方法を試してみてください。",
  "n_reviews": 0,
  "start": 776.98,
  "end": 794.18
 },
 {
  "input": "Or when the network does work but not for the reasons you might expect, digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible solutions.",
  "translatedText": "あるいは、ネットワークが機能しているものの、予想どおりの理由ではない場合、重 みとバイアスが何をしているのかを掘り下げることは、自分の仮定に疑問を投げかけ、考えられ る解決策の全領域を実際に明らかにする良い方法です。",
  "model": "google_nmt",
  "from_community_srt": "しかし、あなたが期待する理由ではありません 重みと偏りが何をしているのかを掘り下げることは、あなたの前提に挑戦し、可能な限り完全な空間を明らかにする良い方法です ソリューション ところで、ここの実際の機能は書き留めるのが少し面倒です。",
  "n_reviews": 0,
  "start": 794.96,
  "end": 805.82
 },
 {
  "input": "By the way, the actual function here is a little cumbersome to write down, don't you think?",
  "translatedText": "ところで、ここで実際の関 数を書くのは少し面倒ですよね。",
  "model": "google_nmt",
  "from_community_srt": "あなたは思いませんか？",
  "n_reviews": 0,
  "start": 806.84,
  "end": 810.68
 },
 {
  "input": "So let me show you a more notationally compact way that these connections are represented.",
  "translatedText": "そこで、これらの接続をよりコン パクトに表記する方法を示しましょう。",
  "model": "google_nmt",
  "from_community_srt": "ですから、これらの接続が表現されるより表記的にコンパクトな方法を私に教えてください。",
  "n_reviews": 0,
  "start": 812.5,
  "end": 817.14
 },
 {
  "input": "This is how you'd see it if you choose to read up more about neural networks.",
  "translatedText": "ニューラル ネットワークについてさらに詳しく読ん でみると、次のようになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 817.66,
  "end": 820.52
 },
 {
  "input": "Organize all of the activations from one layer into a column as a vector.",
  "translatedText": "1 つのレイヤーのすべてのアクティベーションをベクトルとして列に編成 します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.38,
  "end": 820.52
 },
 {
  "input": "Then organize all of the weights as a matrix, where each row of that matrix corresponds to the connections between one layer and a particular neuron in the next layer.",
  "translatedText": "次に、すべての重みを行列として編成します。 行列の各行は、あ る層と次の層の特定のニューロンの間の接続に対応します。",
  "model": "google_nmt",
  "from_community_srt": "これはあなたがそれを見る方法です あなたがニューラルネットワークの詳細を読むことを選択した場合 1つのレイヤーからすべてのアクティベーションをベクトルとして列に整理する 次に、すべての重みを行列として編成し、その行列の各行",
  "n_reviews": 0,
  "start": 821.38,
  "end": 838.0
 },
 {
  "input": "What that means is that taking the weighted sum of the activations in the first layer according to these weights corresponds to one of the terms in the matrix vector product of everything we have on the left here.",
  "translatedText": "これが意味 するのは、これらの重みに従って最初の層のアクティベーションの重み付き合計を取 得すると、左側にあるすべての行列ベクトル積の項の 1 つに対応するということ です。",
  "model": "google_nmt",
  "from_community_srt": "1つの層と次の層の特定のニューロンとの間の接続に対応する その意味は、これらの重みに従って第1層の活性化の加重和を取ることですか？",
  "n_reviews": 0,
  "start": 838.54,
  "end": 849.88
 },
 {
  "input": "By the way, so much of machine learning just comes down to having a good grasp of linear algebra, so for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means, take a look at the series I did on linear algebra, especially chapter 3.",
  "translatedText": "ちなみに、機械学習の多くは線形代数をよく理解することに尽き るので、行列と行列ベクトルの乗算の意味を視覚的に理解したい人は、 私が行ったシリーズを見てください。 線形代数、特に第 3 章。",
  "model": "google_nmt",
  "from_community_srt": "ここにあるすべてのものの行列ベクトル積の項の1つに対応します ところで、機械学習の多くは、線形代数をよく理解することに帰着します ですから、マトリックスの視覚的な理解を望む人や、行列ベクトルの乗算が線形代数で行ったシリーズ",
  "n_reviews": 0,
  "start": 854.0,
  "end": 868.6
 },
 {
  "input": "Back to our expression, instead of talking about adding the bias to each one of these values independently, we represent it by organizing all those biases into a vector, and adding the entire vector to the previous matrix vector product.",
  "translatedText": "式に戻 ると、これらの値のそれぞれに独立してバイアスを追加することについて話すのではなく、こ れらすべてのバイアスをベクトルに編成し、ベクトル全体を前の行列ベクトル積に追加する ことによってそれを表します。",
  "model": "google_nmt",
  "from_community_srt": "特に第3章 これらの値のそれぞれにバイアスを追加することについて話すのではなく、私たちの表現に戻ると、 それらのバイアスを全てベクトル化し、ベクトル全体を前の行列ベクトル積に加える 次に、最終ステップとして",
  "n_reviews": 0,
  "start": 869.24,
  "end": 882.3
 },
 {
  "input": "Then as a final step, I'll wrap a sigmoid around the outside here, and what that's supposed to represent is that you're going to apply the sigmoid function to each specific component of the resulting vector inside.",
  "translatedText": "次に、最後のステップとして、ここで外側にシグモイドを巻き 付けます。 これが表すことは、結果として得られるベクトルの内部の特定の各コンポーネン トにシグモイド関数を適用することになります。",
  "model": "google_nmt",
  "from_community_srt": "私はここの外側のSigmoidをラップします そして、それが表すはずのものは、あなたがシグモイド関数をそれぞれの固有の関数に適用することです 結果として生じるベクトルの内部のコンポーネント",
  "n_reviews": 0,
  "start": 883.28,
  "end": 894.74
 },
 {
  "input": "So once you write down this weight matrix and these vectors as their own symbols, you can communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression, and this makes the relevant code both a lot simpler and a lot faster, since many libraries optimize the heck out of matrix multiplication.",
  "translatedText": "したがって、この重み行列とこれらのベク トルを独自のシンボルとして書き留めると、ある層から次の層へのアクティベーションの完 全な移行を非常に厳密できちんとした小さな式で伝えることができ、これにより関連するコ ードがはるかに単純になり、多くのライブラリが行列の乗算を最適化しているため、はるかに 高速になります。",
  "model": "google_nmt",
  "from_community_srt": "したがって、この重み行列とこれらのベクトルを独自の記号として書き留めると、次のことができます。 非常にタイトできちんとした小さな表情で、あるレイヤーから次のレイヤーへのアクティベーションの完全な移行を伝え、 これにより、多くのライブラリが行列の乗算を最適化するので、関連するコードがずっと簡単で多く高速になります",
  "n_reviews": 0,
  "start": 895.94,
  "end": 915.66
 },
 {
  "input": "Remember how earlier I said these neurons are simply things that hold numbers?",
  "translatedText": "先ほど、これらのニューロンは単に数字を保持するものであると述べたことを覚えていますか?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 917.82,
  "end": 921.46
 },
 {
  "input": "Well of course the specific numbers that they hold depends on the image you feed in, so it's actually more accurate to think of each neuron as a function, one that takes in the outputs of all the neurons in the previous layer and spits out a number between 0 and 1.",
  "translatedText": "もちろん、それらが保持する具体的な数値は、入力した画像によって異なり ます。 そのため、実際には、各ニューロンを関数として考える方が正確で す。 関数は、前の層のすべてのニューロンの出力を受け取り、 0から1ま での数値。",
  "model": "google_nmt",
  "from_community_srt": "私がこれらのニューロンが単に数字を保持するもの もちろん、それらが保持する特定の数値は、あなたがフィードした画像に依存します したがって、実際には、各ニューロンを 前の層のすべてのニューロンの出力と0と1の間の数を吐き出す",
  "n_reviews": 0,
  "start": 922.22,
  "end": 938.34
 },
 {
  "input": "Really the entire network is just a function, one that takes in 784 numbers as an input and spits out 10 numbers as an output.",
  "translatedText": "実際、ネットワーク全体は単なる関数であり、784 個の数値を入力として受 け取り、10 個の数値を出力として吐き出します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 939.2,
  "end": 947.06
 },
 {
  "input": "It's an absurdly complicated function, one that involves 13,000 parameters in the forms of these weights and biases that pick up on certain patterns, and which involves iterating many matrix vector products and the sigmoid squishification function, but it's just a function nonetheless.",
  "translatedText": "これは途方もなく複雑な関数であ り、特定のパターンを検出する重みとバイアスの形式で 13,000 のパラメーターが含まれ、多くの行列ベクトル積とシグモイド潰し関数 の反復が含まれますが、それでも単なる関数です。",
  "model": "google_nmt",
  "from_community_srt": "実際には、ネットワーク全体が機能しているだけです 784個の数字を入力とし、10個の数字を出力として吐き出す それはばかげている 特定のパターンを拾い上げるこれらの重みおよび偏りの形で13,000のパラメータを含む複雑な関数1",
  "n_reviews": 0,
  "start": 947.56,
  "end": 962.64
 },
 {
  "input": "And in a way it's kind of reassuring that it looks complicated.",
  "translatedText": "そして、それが複雑に見える ことは、ある意味、安心感を与えます。",
  "model": "google_nmt",
  "from_community_srt": "多くの行列ベクトル積とシグモイドスカッシュ喚起関数を反復する それにもかかわらず、それは単なる関数であり、ある意味では、それは複雑に見える 私たちが数字を認識することの挑戦に就くことができると思ったのは、もっと簡単なことでしたか？",
  "n_reviews": 0,
  "start": 963.4,
  "end": 966.66
 },
 {
  "input": "I mean if it were any simpler, what hope would we have that it could take on the challenge of recognizing digits?",
  "translatedText": "つまり、もしそれがもっと単純だったら、数字を認識するとい う課題に挑戦できるなんて、どんな希望が持てるでしょうか?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 967.34,
  "end": 972.28
 },
 {
  "input": "And how does it take on that challenge?",
  "translatedText": "そして、その課題にどのように取り組むのでしょうか?",
  "model": "google_nmt",
  "from_community_srt": "そしてそれはどのようにその挑戦にかかりますか？",
  "n_reviews": 0,
  "start": 973.34,
  "end": 974.7
 },
 {
  "input": "How does this network learn the appropriate weights and biases just by looking at data?",
  "translatedText": "このネットワークは、データを見るだけで適切な重みとバイアスをどのように学習するのでしょうか?",
  "model": "google_nmt",
  "from_community_srt": "このネットワークは、データを見るだけで適切な重みとバイアスをどのように学習しますか？",
  "n_reviews": 0,
  "start": 975.08,
  "end": 979.36
 },
 {
  "input": "Well that's what I'll show in the next video, and I'll also dig a little more into what this particular network we're seeing is really doing.",
  "translatedText": "それについては次のビデオで説明します。 また、この特定のネットワークが実際に何をしているのかに ついてももう少し詳しく説明します。",
  "model": "google_nmt",
  "from_community_srt": "ああ？ それは私が次のビデオで見せてくれるものです。",
  "n_reviews": 0,
  "start": 980.14,
  "end": 986.12
 },
 {
  "input": "Now is the point I suppose I should say subscribe to stay notified about when that video or any new videos come out, but realistically most of you don't actually receive notifications from YouTube, do you?",
  "translatedText": "ここで重要なのは、そのビデオや新しいビデオが公開されたときの 通知を受け取るために購読すると言うべきだと思いますが、現実的には、ほとんどの人が実際に Yo uTube からの通知を受け取っていませんよね。",
  "model": "google_nmt",
  "from_community_srt": "私たちが見ているこの特定のネットワークが本当にやっていることをもう少し詳しく見ていきます 今は、そのビデオや新しいビデオがいつ出てくるかを知らせてくれるように申し込むといいでしょう。 しかし、現実的にはほとんどの人が実際にYouTubeからの通知を受け取っていません。",
  "n_reviews": 0,
  "start": 987.58,
  "end": 997.42
 },
 {
  "input": "Maybe more honestly I should say subscribe so that the neural networks that underlie YouTube's recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you.",
  "translatedText": "おそらくもっと正直に言うと、YouTube の推奨アルゴリズムの基礎をなすニューラル ネットワークが、ユーザーがこのチャンネルのコンテンツを 推奨されると信じ込むように、チャンネル登録するというべきでしょう。",
  "model": "google_nmt",
  "from_community_srt": "もっと正直なところ、私はサブスクリプションと言って、YouTubeの基盤となるニューラルネットワーク 推奨アルゴリズムは、あなたがこのチャンネルからのコンテンツをあなたに推薦されて見たいと思っている",
  "n_reviews": 0,
  "start": 998.02,
  "end": 1007.88
 },
 {
  "input": "Anyway, stay posted for more.",
  "translatedText": "とにかく、続報をお待ちください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1008.56,
  "end": 1009.94
 },
 {
  "input": "Thank you very much to everyone supporting these videos on Patreon.",
  "translatedText": "Patreon でこれらのビデオをサポートしてくださった皆様に心より感謝いたします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1010.76,
  "end": 1013.5
 },
 {
  "input": "I've been a little slow to progress in the probability series this summer, but I'm jumping back into it after this project, so patrons you can look out for updates there.",
  "translatedText": "この夏は確率シリーズの進歩が少 し遅れていましたが、このプロジェクトの後はまた本格的に取り組むつもりですので、パトロンの皆様はそこ で最新情報をチェックしていただければ幸いです。",
  "model": "google_nmt",
  "from_community_srt": "とにかく続きを続ける これらのビデオをパトリオンでサポートしてくださったみなさん、本当にありがとう 私はこの夏の確率シリーズでは少し遅れています しかし、私はこのプロジェクトの後にそれにジャンプしているので、あなたはそこにある更新を見極めることができます",
  "n_reviews": 0,
  "start": 1014.0,
  "end": 1021.9
 },
 {
  "input": "To close things off here I have with me Lisha Li who did her PhD work on the theoretical side of deep learning and who currently works at a venture capital firm called Amplify Partners who kindly provided some of the funding for this video.",
  "translatedText": "最後に、深層学習の理論面で博士号の研究をし、 現在はこのビデオに資金の一部を提供してくれた Amplify Partners と いうベンチャー キャピタル会社で働いている Leesha Lee に話を聞きます。",
  "model": "google_nmt",
  "from_community_srt": "私はここにいるものを閉じるには私と一緒にいるLisha Li リー氏は、博士号を博士課程で学び、理論的な学習の面で仕事をしています。 現在、ベンチャーキャピタルで働く 誰がこのビデオのための資金の一部を親切に提供したのですか？",
  "n_reviews": 0,
  "start": 1023.6,
  "end": 1034.62
 },
 {
  "input": "So Lisha one thing I think we should quickly bring up is this sigmoid function.",
  "translatedText": "そこで、リーシャ、すぐに取り上げるべきだと思うのが、このシグモイド関数です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1035.46,
  "end": 1039.12
 },
 {
  "input": "As I understand it early networks use this to squish the relevant weighted sum into that interval between zero and one, you know kind of motivated by this biological analogy of neurons either being inactive or active.",
  "translatedText": "私が理解しているところによると、初期のネットワークはこれを使用して、関連する重み付けされた合計を 0 と 1 の間の区間に押し込みます。 ニューロンが非アクティブまたはアクティブであるという生物学的な類似によって動 機づけられているのはご存知でしょう。",
  "model": "google_nmt",
  "from_community_srt": "Lishaのこと 私はすぐにこのシグモイド関数を呼び出すべきだと思います 私が理解しているように、初期のネットワークはこれを使って関連する加重和をゼロと1の間の区間 あなたは、この生物学的なニューロンの類推によって動かされていることを知っています。",
  "n_reviews": 0,
  "start": 1039.7,
  "end": 1049.84
 },
 {
  "input": "Exactly.",
  "translatedText": "その通り。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1050.28,
  "end": 1050.3
 },
 {
  "input": "But relatively few modern networks actually use sigmoid anymore.",
  "translatedText": "しかし、実際にシグモイドを使用している現代のネットワークは 比較的少数です。",
  "model": "google_nmt",
  "from_community_srt": "（Lisha） - まさに しかし、現代のネットワークでは実際にシグモイドを実際に使用している人はほとんどいません（3B1B）。",
  "n_reviews": 0,
  "start": 1050.56,
  "end": 1054.04
 },
 {
  "input": "Yeah.",
  "translatedText": "うん。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.32,
  "end": 1054.32
 },
 {
  "input": "It's kind of old school right?",
  "translatedText": "それはちょっと古い学校ですよね？",
  "model": "google_nmt",
  "from_community_srt": "それは古い学校の権利のようなものですか？",
  "n_reviews": 0,
  "start": 1054.44,
  "end": 1055.54
 },
 {
  "input": "Yeah or rather ReLU seems to be much easier to train.",
  "translatedText": "そうですね、むしろレルのほうが訓練しや すいようです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1055.76,
  "end": 1058.98
 },
 {
  "input": "And ReLU, ReLU stands for rectified linear unit?",
  "translatedText": "そして、relu、reluは整流リニアユニットの略ですか？",
  "model": "google_nmt",
  "from_community_srt": "（Lisha） - そうか、むしろ ReLUは訓練がはるかに簡単だと思われる （3B1B） - そしてReLUは実際に整流されたリニアユニット （Lisha） - はい、この種の関数です。",
  "n_reviews": 0,
  "start": 1059.4,
  "end": 1062.34
 },
 {
  "input": "Yes it's this kind of function where you're just taking a max of zero and a where a is given by what you were explaining in the video",
  "translatedText": "はい、これはこの種の関数 で、最大ゼロと a を取得するだけです。 ここで、a はビデオで説明した内容によって与え られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1062.68,
  "end": 1070.92
 },
 {
  "input": "and what this was sort of motivated from I think was a partially by a biological analogy with how neurons would either be activated or not.",
  "translatedText": "これは、ニューロンがどのように活性化されるか活性化されないかに関する生物学的 な類似から部分的に動機付けられたものだと思います。",
  "model": "google_nmt",
  "from_community_srt": "ここでは、最大値0をとり、 あなたがビデオで何を説明していたのか、これが何かの動機付けだったのは 部分的には生物学的 方法のアナロジー ニューロンは活性化されていてもいなくてもよいので、ある閾値を超えると",
  "n_reviews": 0,
  "start": 1070.92,
  "end": 1081.36
 },
 {
  "input": "And so if it passes a certain threshold it would be the identity function but if it did not then it would just not be activated so it'd be zero",
  "translatedText": "したがって、特定のしきい値を超えた場 合は恒等関数になりますが、超えなかった場合はアクティブ化されないため、ゼロになります。",
  "model": "google_nmt",
  "from_community_srt": "アイデンティティ関数 しかし、そうでなければ、それは単に活性化されないのでゼロになるので、それは単純化の一種です シグモイドを使用しても訓練には役に立たなかったか、訓練するのが非常に困難でした",
  "n_reviews": 0,
  "start": 1081.36,
  "end": 1089.46
 },
 {
  "input": "so it's kind of a simplification.",
  "translatedText": "つまり、一種の簡略化です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1089.46,
  "end": 1090.84
 },
 {
  "input": "Using sigmoids didn't help training or it was very difficult to train at some point and people just tried ReLU and it happened to work very well for these incredibly deep neural networks.",
  "translatedText": "シグモイドの使用はトレーニングに役立たなかったり、ある時点でトレ ーニングが非常に困難になったりしていましたが、人々は relu を試してみたところ、たまたまこれらの信じられな いほど深いニューラル ネットワークで非常にうまく機能しました。",
  "model": "google_nmt",
  "from_community_srt": "それはある時点であり、人々はただ気分を変えようとしたばかりで、うまくいった 非常にこれらの信じられないほどのために 深いニューラルネットワーク。",
  "n_reviews": 0,
  "start": 1091.16,
  "end": 1104.62
 },
 {
  "input": "All right thank you Lisha.",
  "translatedText": "わかりました、リーシャ、ありがとう。",
  "model": "google_nmt",
  "from_community_srt": "（3B1B） - すべての権利 ありがとうございますLisha",
  "n_reviews": 0,
  "start": 1105.1,
  "end": 1105.64
 }
]