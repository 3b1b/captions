1
00:00:04,220 --> 00:00:05,400
これは3だ。

2
00:00:06,060 --> 00:00:08,538
ぞんざいに書かれ、28x28ピクセルという極

3
00:00:08,538 --> 00:00:11,016
めて低い解像度でレンダリングされているが、あ

4
00:00:11,016 --> 00:00:13,720
なたの脳はそれを3として認識するのに問題はない。

5
00:00:14,340 --> 00:00:16,650
そして、頭脳がこのように難なくこなせることがいか

6
00:00:16,650 --> 00:00:18,960
にクレイジーなことなのか、少し考えてみてほしい。

7
00:00:19,700 --> 00:00:24,009
つまり、各ピクセルの具体的な値は画像によって大きく異なるに

8
00:00:24,009 --> 00:00:28,320
もかかわらず、これとこれとこれも3Sとして認識できるのだ。

9
00:00:28,900 --> 00:00:32,846
この3を見たときに発火している眼球内の特殊な光感受性細

10
00:00:32,846 --> 00:00:36,940
胞は、この3を見たときに発火しているものとは全く異なる。

11
00:00:37,520 --> 00:00:41,003
しかし、あなたのそのクレイジーなほど賢い視覚野の

12
00:00:41,003 --> 00:00:44,486
何かが、これらを同じ考えを表していると解し、同時

13
00:00:44,486 --> 00:00:48,260
に他のイメージをそれ自身の異なる考えとして認識する。

14
00:00:49,220 --> 00:00:52,583
しかし、このように28×28ピクセルのグリッドを

15
00:00:52,583 --> 00:00:55,947
取り込み、0から10までの数字を1つだけ出力し、

16
00:00:55,947 --> 00:00:59,311
その数字が何であるかを教えてくれるプログラムを書

17
00:00:59,311 --> 00:01:02,675
いてくれ、と言ったら、その仕事は滑稽なほど簡単な

18
00:01:02,675 --> 00:01:06,180
ものから、気が遠くなるほど難しいものになるだろう。

19
00:01:07,160 --> 00:01:09,576
あなたが岩の下で生きてきたのでなければ、機

20
00:01:09,576 --> 00:01:11,993
械学習とニューラルネットワークの現在と将来

21
00:01:11,993 --> 00:01:14,640
への関連性と重要性を説明する必要はないだろう。

22
00:01:15,120 --> 00:01:17,455
しかし、ここで私がしたいことは、バックグラウンドがな

23
00:01:17,455 --> 00:01:19,789
いことを前提に、ニューラルネットワークが実際にどのよ

24
00:01:19,789 --> 00:01:22,125
うなものなのかをお見せし、それが何をやっているのかを

25
00:01:22,125 --> 00:01:24,460
、流行語ではなく、数学の一部として視覚化することだ。

26
00:01:25,020 --> 00:01:27,302
私の望みは、構造そのものがやる気を起こさせるもの

27
00:01:27,302 --> 00:01:29,584
だと感じてもらい、ニューラルネットワークの引用学

28
00:01:29,584 --> 00:01:31,867
習について読んだり聞いたりしたときに、それが何を

29
00:01:31,867 --> 00:01:34,340
意味するのかわかったような気分になってもらうことだ。

30
00:01:35,360 --> 00:01:37,810
このビデオはその構造的な要素に焦点を当てたも

31
00:01:37,810 --> 00:01:40,260
ので、次のビデオは学習に焦点を当てたものだ。

32
00:01:40,960 --> 00:01:43,500
私たちがやろうとしているのは、手書きの数字を認識するこ

33
00:01:43,500 --> 00:01:46,040
とを学習できるニューラルネットワークを構築することだ。

34
00:01:49,360 --> 00:01:52,082
これは、このトピックを紹介するためのやや古典的な例

35
00:01:52,082 --> 00:01:54,042
であり、ここでは現状維持で構わない。

36
00:01:54,042 --> 00:01:56,764
なぜなら、2本のビデオの最後に、もっと詳しく学べる

37
00:01:56,764 --> 00:01:59,486
いくつかの良いリソースと、これを実行するコードをダ

38
00:01:59,486 --> 00:02:02,208
ウンロードして自分のコンピューターで遊べる場所を紹

39
00:02:02,208 --> 00:02:03,080
介したいからだ。

40
00:02:05,040 --> 00:02:08,540
ニューラルネットワークには様々な種類があり、近年は

41
00:02:08,540 --> 00:02:12,040
そのような種類の研究がある種のブームになっているが

42
00:02:12,040 --> 00:02:15,540
、この2つの入門ビデオでは、あなたと私は、飾り気の

43
00:02:15,540 --> 00:02:19,180
ない最もシンプルなプレーン・バニラ形式を見るだけだ。

44
00:02:19,860 --> 00:02:24,230
これは、より強力な現代の変種を理解するために

45
00:02:24,230 --> 00:02:28,600
必要な前提条件のようなもので、信じてほしい。

46
00:02:29,120 --> 00:02:32,733
しかし、この最も単純な形であっても、手書き

47
00:02:32,733 --> 00:02:36,520
の数字を認識することを学習することができる。

48
00:02:37,480 --> 00:02:39,880
そして同時に、私たちが期待していたような2、3の

49
00:02:39,880 --> 00:02:42,280
期待をいかに下回るものであったかもわかるだろう。

50
00:02:43,380 --> 00:02:45,940
その名の通り、ニューラルネットワークは脳か

51
00:02:45,940 --> 00:02:48,500
ら着想を得ているが、それを分解してみよう。

52
00:02:48,520 --> 00:02:51,660
神経細胞とは何なのか、どのような意味でつながっているのか。

53
00:02:52,500 --> 00:02:56,392
私が今ニューロンと言ったとき、皆さんに考えてほしい

54
00:02:56,392 --> 00:03:00,440
のは、数字、特に0から1の間の数字を保持するものだ。

55
00:03:00,680 --> 00:03:02,560
それ以上のことはない。

56
00:03:03,780 --> 00:03:07,152
例えば、このネットワークは、入力画像の28

57
00:03:07,152 --> 00:03:10,525
x28ピクセルそれぞれに対応するニューロン

58
00:03:10,525 --> 00:03:14,220
の束から始まり、合計で784ニューロンとなる。

59
00:03:14,700 --> 00:03:17,827
これらのピクセルはそれぞれ、対応するピクセ

60
00:03:17,827 --> 00:03:20,954
ルのグレースケール値を表す数値を保持してお

61
00:03:20,954 --> 00:03:24,380
り、黒ピクセルの0から白ピクセルの1まである。

62
00:03:25,300 --> 00:03:28,166
ニューロン内部のこの数値は活性化と呼ばれ、こ

63
00:03:28,166 --> 00:03:31,032
こで思い浮かべるイメージは、活性化が高い数値

64
00:03:31,032 --> 00:03:34,160
のときに各ニューロンが点灯するというものだろう。

65
00:03:36,720 --> 00:03:39,290
つまり、これら784個のニューロンはすべて

66
00:03:39,290 --> 00:03:41,860
、ネットワークの第1層を構成しているのだ。

67
00:03:46,500 --> 00:03:48,870
最後の層にジャンプすると、10個のニュー

68
00:03:48,870 --> 00:03:51,360
ロンがあり、それぞれが数字を1つずつ表す。

69
00:03:52,040 --> 00:03:55,291
これらのニューロンの活性化は、やはり0か

70
00:03:55,291 --> 00:03:58,543
ら1の間の数値で、ある画像がある数字に対

71
00:03:58,543 --> 00:04:02,120
応するとシステムが考える度合いを表している。

72
00:04:03,040 --> 00:04:06,507
また、その間に隠れ層と呼ばれる層があり、当面

73
00:04:06,507 --> 00:04:09,974
はこの数字を認識するプロセスが一体どのように

74
00:04:09,974 --> 00:04:13,600
処理されるのか、巨大な疑問符がつくだけだろう。

75
00:04:14,260 --> 00:04:17,409
このネットワークでは2つの隠れ層を選

76
00:04:17,409 --> 00:04:20,560
び、それぞれ16のニューロンを持つ。

77
00:04:21,019 --> 00:04:23,413
正直なところ、2つのレイヤーを選んだのは、ほんの一瞬

78
00:04:23,413 --> 00:04:25,806
の間に構造をどのように動機づけたいかを考えてのことで

79
00:04:25,806 --> 00:04:28,200
、16は、まあ、画面に収まるちょうどいい数字だった。

80
00:04:28,780 --> 00:04:32,340
実際には、ここで具体的な構造を実験する余地は大いにある。

81
00:04:33,020 --> 00:04:35,750
ネットワークの動作は、ある層の活

82
00:04:35,750 --> 00:04:38,480
性化が次の層の活性化を決定する。

83
00:04:39,200 --> 00:04:42,234
そしてもちろん、情報処理メカニズムとしてのネ

84
00:04:42,234 --> 00:04:45,269
ットワークの核心は、ある層の活性化が次の層の

85
00:04:45,269 --> 00:04:48,580
活性化をどのようにもたらすかということに尽きる。

86
00:04:49,140 --> 00:04:51,820
これは、ニューロンの生物学的ネットワークにお

87
00:04:51,820 --> 00:04:54,500
いて、あるニューロン群の発火が、他の特定のニ

88
00:04:54,500 --> 00:04:57,180
ューロン群の発火を引き起こすことに似ている。

89
00:04:58,120 --> 00:05:00,688
ここでお見せするネットワークは、数字

90
00:05:00,688 --> 00:05:03,400
を認識するようにすでに訓練されている。

91
00:05:03,640 --> 00:05:07,328
つまり、画像を入力し、画像の各ピクセルの明るさに応じ

92
00:05:07,328 --> 00:05:11,015
て入力層の784個のニューロンをすべて点灯させると、

93
00:05:11,015 --> 00:05:14,704
その活性化パターンが次の層にある特定のパターンを引き

94
00:05:14,704 --> 00:05:18,392
起こし、それがさらに次の層にあるパターンを引き起こし

95
00:05:18,392 --> 00:05:22,080
、最終的に出力層にあるパターンを与えるということだ。

96
00:05:22,560 --> 00:05:24,802
そして、その出力層の最も明るいニューロン

97
00:05:24,802 --> 00:05:27,045
は、いわば、この画像が何桁の数字を表して

98
00:05:27,045 --> 00:05:29,400
いるかをネットワークが選択したことになる。

99
00:05:32,560 --> 00:05:35,275
そして、ある層が次の層にどのような影響を与えるのか、ある

100
00:05:35,275 --> 00:05:37,991
いはトレーニングがどのように機能するのか、その計算に飛び

101
00:05:37,991 --> 00:05:40,707
つく前に、なぜこのような層構造がインテリジェントに振る舞

102
00:05:40,707 --> 00:05:43,520
うことを期待するのが合理的なのかについてだけ話しておこう。

103
00:05:44,060 --> 00:05:45,220
ここで何を期待しているのか？

104
00:05:45,400 --> 00:05:47,600
そうした中間層にとって、最善の希望は何だろうか？

105
00:05:48,920 --> 00:05:51,154
私やあなたが数字を認識するとき、私

106
00:05:51,154 --> 00:05:53,520
たちはさまざまな要素を組み合わせる。

107
00:05:54,200 --> 00:05:56,820
9は上にループがあり、右にラインがある。

108
00:05:57,380 --> 00:06:01,180
8もトップにループがあるが、低めのループと対になっている。

109
00:06:01,980 --> 00:06:06,820
4は基本的に3つのラインに分かれている。

110
00:06:07,600 --> 00:06:11,610
完璧な世界であれば、最後から2番目のレイヤーの各ニューロン

111
00:06:11,610 --> 00:06:15,620
が、これらのサブコンポーネントのいずれかに対応し、たとえば

112
00:06:15,620 --> 00:06:19,631
9や8のようなループを持つ画像を入力すると、その活性化が1

113
00:06:19,631 --> 00:06:23,780
に近くなる特定のニューロンが存在することを望むかもしれない。

114
00:06:24,500 --> 00:06:26,853
この特定のピクセルのループという意味ではなく、

115
00:06:26,853 --> 00:06:29,206
上部に向かう全体的にループしたパターンが、この

116
00:06:29,206 --> 00:06:31,560
ニューロンを作動させることを期待しているのだ。

117
00:06:32,440 --> 00:06:34,973
そうすれば、3番目のレイヤーから最後のレイヤ

118
00:06:34,973 --> 00:06:37,506
ーに行くには、どのサブコンポーネントの組み合

119
00:06:37,506 --> 00:06:40,040
わせがどの数字に対応するかを学習すればいい。

120
00:06:41,000 --> 00:06:43,213
というのも、これらのサブコンポーネントをどうやっ

121
00:06:43,213 --> 00:06:45,426
て認識するのか、あるいは正しいサブコンポーネント

122
00:06:45,426 --> 00:06:47,640
が何であるべきかを学ぶことさえできるのだろうか？

123
00:06:48,060 --> 00:06:50,504
そして、1つのレイヤーが次のレイヤーにどのよ

124
00:06:50,504 --> 00:06:53,060
うな影響を与えるかについてはまだ話していない。

125
00:06:53,680 --> 00:06:56,680
ループを認識することは、サブ問題に分解することもできる。

126
00:06:57,280 --> 00:07:00,030
そのための合理的な方法のひとつは、まずそれを構成

127
00:07:00,030 --> 00:07:02,780
するさまざまな小さなエッジを認識することだろう。

128
00:07:03,780 --> 00:07:08,921
同様に、数字の1や4や7に見られるような

129
00:07:08,921 --> 00:07:14,320
長い線は、実際には単なる長いエッジである。

130
00:07:15,140 --> 00:07:18,930
つまり、ネットワークの第2層の各ニューロンが、関連するさまざ

131
00:07:18,930 --> 00:07:22,720
まな小さなエッジに対応していることが望ましいのかもしれない。

132
00:07:23,540 --> 00:07:27,547
おそらく、このような画像が入ってくると、8～10個の特

133
00:07:27,547 --> 00:07:31,555
定の小さなエッジに関連するニューロンがすべて点灯し、そ

134
00:07:31,555 --> 00:07:35,563
れが上部のループと長い縦線に関連するニューロンを点灯さ

135
00:07:35,563 --> 00:07:39,720
せ、それらが9に関連するニューロンを点灯させるのだろう。

136
00:07:40,680 --> 00:07:43,617
これが最終的なネットワークが実際に行うことなのかどうか

137
00:07:43,617 --> 00:07:46,555
はまた別の問題で、ネットワークのトレーニング方法を確認

138
00:07:46,555 --> 00:07:49,493
したらまた話をするつもりだが、このようなレイヤー構造を

139
00:07:49,493 --> 00:07:52,540
持つことで、ある種の目標を持つことができるかもしれない。

140
00:07:53,160 --> 00:07:56,669
さらに、このようにエッジやパターンを検出できるようになれば

141
00:07:56,669 --> 00:08:00,300
、他の画像認識タスクにも大いに役立つことは想像できるだろう。

142
00:08:00,880 --> 00:08:04,019
また、画像認識だけでなく、抽象化されたレイヤーに分類

143
00:08:04,019 --> 00:08:07,280
される、あらゆる種類のインテリジェントなことができる。

144
00:08:08,040 --> 00:08:11,045
例えば、音声の解析では、生の音声を聞き取り、明瞭な音

145
00:08:11,045 --> 00:08:14,050
を選び出し、それが組み合わさって特定の音節を作り、そ

146
00:08:14,050 --> 00:08:17,055
れが組み合わさって単語を作り、それが組み合わさってフ

147
00:08:17,055 --> 00:08:20,060
レーズやより抽象的な思考を作る、といった作業を行う。

148
00:08:21,100 --> 00:08:24,040
しかし、これが実際にどのように機能するかに話を戻すと、

149
00:08:24,040 --> 00:08:26,980
ある層の活性化が次の層の活性化をどのように決定するかを

150
00:08:26,980 --> 00:08:29,920
、今自分がデザインしているところを思い浮かべてほしい。

151
00:08:30,860 --> 00:08:34,833
ピクセルをエッジに、エッジをパターンに、パター

152
00:08:34,833 --> 00:08:38,980
ンを数字に結合するようなメカニズムが考えられる。

153
00:08:39,440 --> 00:08:44,900
具体的な例を挙げると、第2層のあるニューロ

154
00:08:44,900 --> 00:08:50,620
ンが、画像にエッジがあるかどうかを検知する。

155
00:08:51,440 --> 00:08:53,270
目下の問題は、ネットワークはどのような

156
00:08:53,270 --> 00:08:55,100
パラメータを持つべきか、ということだ。

157
00:08:55,640 --> 00:08:59,581
このパターンや他のピクセルパターン、複数のエッジが

158
00:08:59,581 --> 00:09:03,523
ループを作るパターンなどを捉える可能性を十分に表現

159
00:09:03,523 --> 00:09:07,780
できるように、どんなダイヤルやノブをいじればいいのか？

160
00:09:08,720 --> 00:09:12,140
さて、これからやることは、ニューロンと第1層のニュー

161
00:09:12,140 --> 00:09:15,560
ロンとの間のそれぞれの接続に重みを割り当てることだ。

162
00:09:16,320 --> 00:09:17,700
これらのウェイトは単なる数字に過ぎない。

163
00:09:18,540 --> 00:09:21,925
次に、第1層からすべての活性を取り出

164
00:09:21,925 --> 00:09:25,500
し、その重みに従って加重和を計算する。

165
00:09:27,700 --> 00:09:32,393
緑色のピクセルは正のウエイトを、赤色のピク

166
00:09:32,393 --> 00:09:37,086
セルは負のウエイトを示し、そのピクセルの明

167
00:09:37,086 --> 00:09:41,780
るさがウエイトの値をゆるやかに表している。

168
00:09:42,780 --> 00:09:45,690
ここで、私たちが気にしているこの領域で正の重みを

169
00:09:45,690 --> 00:09:48,601
いくつか除いて、ほとんどすべてのピクセルに関連す

170
00:09:48,601 --> 00:09:51,512
る重みをゼロにすると、すべてのピクセルの値の重み

171
00:09:51,512 --> 00:09:54,423
付き合計を取ることは、実際には、私たちが気にして

172
00:09:54,423 --> 00:09:57,820
いる領域内のピクセルの値だけを合計することに等しくなる。

173
00:09:59,140 --> 00:10:02,793
もし本当にエッジがあるかどうかを調べたいのであれ

174
00:10:02,793 --> 00:10:06,600
ば、周囲のピクセルに負のウェイトを設定すればいい。

175
00:10:07,480 --> 00:10:10,090
そうすると、中央のピクセルが明るく、周囲

176
00:10:10,090 --> 00:10:12,700
のピクセルが暗いとき、合計が最大になる。

177
00:10:14,260 --> 00:10:17,353
このように加重和を計算すると、どのような数値で

178
00:10:17,353 --> 00:10:20,446
も出てくるかもしれないが、このネットワークに必

179
00:10:20,446 --> 00:10:23,540
要なのは、活性度が0と1の間の値になることだ。

180
00:10:24,120 --> 00:10:28,051
そこで一般的に行われるのは、この加重和を、実数線を

181
00:10:28,051 --> 00:10:32,140
0から1の範囲に押し込める関数に取り込むことである。

182
00:10:32,460 --> 00:10:34,940
これを行う一般的な関数はシグモイド関数と呼ば

183
00:10:34,940 --> 00:10:37,420
れ、ロジスティック曲線としても知られている。

184
00:10:38,000 --> 00:10:42,218
基本的に、非常にネガティブな入力は0に近く、ポジティ

185
00:10:42,218 --> 00:10:46,600
ブな入力は1に近く、入力0を中心に着実に増加していく。

186
00:10:49,120 --> 00:10:52,669
つまり、ここでのニューロンの活性化とは、基本的に、

187
00:10:52,669 --> 00:10:56,360
関連する加重和がどれだけ正であるかを示すものである。

188
00:10:57,540 --> 00:10:59,659
しかし、加重和が0より大きいときにニューロ

189
00:10:59,659 --> 00:11:01,880
ンを点灯させたいわけではないのかもしれない。

190
00:11:02,280 --> 00:11:06,360
例えば、合計が10より大きいときだけアクティブにしたい。

191
00:11:06,840 --> 00:11:08,550
つまり、アクティブでないためには

192
00:11:08,550 --> 00:11:10,260
、何らかのバイアスが必要なのだ。

193
00:11:11,380 --> 00:11:15,520
この加重和にマイナス10などの数字を加えてから

194
00:11:15,520 --> 00:11:19,660
、シグモイド・スクイッシュ化関数にかけるのだ。

195
00:11:20,580 --> 00:11:22,440
その追加の数値がバイアスと呼ばれる。

196
00:11:23,460 --> 00:11:26,293
つまり重みは、第2層のニューロンがどのような

197
00:11:26,293 --> 00:11:29,126
ピクセルパターンを拾っているかを示し、バイア

198
00:11:29,126 --> 00:11:31,960
スは、ニューロンが意味のある活動を始める前に

199
00:11:31,960 --> 00:11:35,180
、重みの合計がどの程度高くなる必要があるかを示す。

200
00:11:36,120 --> 00:11:37,680
そして、それはたった1つのニューロンに過ぎない。

201
00:11:38,280 --> 00:11:42,500
この層の他のすべてのニューロンは、第1層の784個のピ

202
00:11:42,500 --> 00:11:46,720
クセル・ニューロンすべてに接続され、その784個の接続

203
00:11:46,720 --> 00:11:50,940
のひとつひとつに、それぞれの重みが関連づけられている。

204
00:11:51,600 --> 00:11:57,600
また、シグモイドでつぶす前に加重和に加えるバイアスもある。

205
00:11:58,110 --> 00:11:59,540
そして、それは考えることがたくさんある！

206
00:11:59,960 --> 00:12:03,970
この隠れ層は16ニューロンで、合計78

207
00:12:03,970 --> 00:12:07,980
4×16の重みと16のバイアスを持つ。

208
00:12:08,840 --> 00:12:10,345
そしてこれらはすべて、第1レイヤー

209
00:12:10,345 --> 00:12:11,940
から第2レイヤーへの接続にすぎない。

210
00:12:12,520 --> 00:12:14,930
他のレイヤー間のコネクションにも、たくさ

211
00:12:14,930 --> 00:12:17,340
んの重みとバイアスが関連付けられている。

212
00:12:18,340 --> 00:12:21,069
このネットワークは、重みとバイアスの

213
00:12:21,069 --> 00:12:23,800
合計がほぼ正確に13,000である。

214
00:12:23,800 --> 00:12:25,792
13,000ものノブやダイヤルがあり、それら

215
00:12:25,792 --> 00:12:27,785
を微調整したり回したりすることで、このネット

216
00:12:27,785 --> 00:12:29,960
ワークをさまざまな方法で動作させることができる。

217
00:12:31,040 --> 00:12:34,394
つまり、学習について話すとき、それはコンピュータに、

218
00:12:34,394 --> 00:12:37,748
実際に目の前の問題を解決できるように、これら多くの数

219
00:12:37,748 --> 00:12:41,360
すべてについて有効な設定を見つけさせることを指している。

220
00:12:42,620 --> 00:12:46,076
楽しくもあり、恐ろしくもある思考実験のひとつは、座っ

221
00:12:46,076 --> 00:12:49,533
てこれらの重みとバイアスをすべて手作業で設定し、第2

222
00:12:49,533 --> 00:12:52,990
レイヤーがエッジを拾い、第3レイヤーがパターンを拾う

223
00:12:52,990 --> 00:12:56,580
ように、意図的に数字を微調整することを想像することだ。

224
00:12:56,980 --> 00:13:00,420
私自身は、ネットワークを完全にブラックボックスとして扱うより

225
00:13:00,420 --> 00:13:02,484
も、この方が満足できると思っている。

226
00:13:02,484 --> 00:13:05,924
なぜなら、ネットワークが予想通りに機能しないとき、その重みと

227
00:13:05,924 --> 00:13:09,363
バイアスが実際に何を意味するのか、少しでも関係が築けていれば

228
00:13:09,363 --> 00:13:12,803
、どのように構造を変えれば改善されるのか、実験するためのスタ

229
00:13:12,803 --> 00:13:14,180
ート地点に立てるからだ。

230
00:13:14,960 --> 00:13:17,617
あるいは、ネットワークは機能するが、期待するよ

231
00:13:17,617 --> 00:13:20,274
うな理由ではない場合、重みとバイアスが何をして

232
00:13:20,274 --> 00:13:22,931
いるかを掘り下げることは、あなたの仮定に挑戦し

233
00:13:22,931 --> 00:13:25,820
、可能な解決策の全容を明らかにする良い方法である。

234
00:13:26,840 --> 00:13:30,680
ところで、ここで実際の機能を書くのは少し面倒だと思わないか？

235
00:13:32,500 --> 00:13:34,819
そこで、これらのコネクションをよりコ

236
00:13:34,819 --> 00:13:37,140
ンパクトに表記する方法を紹介しよう。

237
00:13:37,660 --> 00:13:39,089
ニューラル・ネットワークについてもっと

238
00:13:39,089 --> 00:13:40,520
詳しく読もうと思えば、こうなるだろう。

239
00:13:41,380 --> 00:13:49,690
ある層と次の層の特定のニューロンとの間の接続に対応する

240
00:13:49,690 --> 00:13:58,000
行列として、ある層からのすべての活性化を列に整理する。

241
00:13:58,540 --> 00:14:02,260
つまり、これらの重みに応じて第1層の活性度

242
00:14:02,260 --> 00:14:05,981
の加重和を取ると、左側のすべての行列のベク

243
00:14:05,981 --> 00:14:09,880
トル積の項のひとつに相当する、ということだ。

244
00:14:14,000 --> 00:14:17,650
ところで、機械学習の多くは線形代数をよく理解して

245
00:14:17,650 --> 00:14:21,300
いるかどうかにかかっている。行列と行列のベクトル

246
00:14:21,300 --> 00:14:24,950
乗算の意味を視覚的に理解したい人は、私が線形代数

247
00:14:24,950 --> 00:14:28,600
について書いたシリーズ、特に第3章を見てほしい。

248
00:14:29,240 --> 00:14:33,542
式に戻ると、これらの値にそれぞれ独立にバイアスを加えるの

249
00:14:33,542 --> 00:14:37,844
ではなく、これらのバイアスをすべてベクトルに整理し、その

250
00:14:37,844 --> 00:14:42,300
ベクトル全体を前の行列のベクトル積に加えることで表現する。

251
00:14:43,280 --> 00:14:48,114
そして最後の段階として、シグモイドを外側に巻いてみる。

252
00:14:48,114 --> 00:14:51,875
これは、シグモイド関数を内側のベクトルの各

253
00:14:51,875 --> 00:14:54,740
成分に適用することを表している。

254
00:14:55,940 --> 00:15:00,816
つまり、重み行列とベクトルをそれぞれのシンボル

255
00:15:00,816 --> 00:15:05,693
として書き出せば、1つのレイヤーから次のレイヤ

256
00:15:05,693 --> 00:15:10,570
ーへのアクティベーションの完全な遷移を、非常に

257
00:15:10,570 --> 00:15:15,660
タイトで整然とした小さな式で伝えることができる。

258
00:15:17,820 --> 00:15:19,640
先ほど、ニューロンは単に数字を保持する

259
00:15:19,640 --> 00:15:21,460
ものだと言ったのを覚えているだろうか？

260
00:15:22,220 --> 00:15:30,100
各ニューロンは、前の層のすべてのニューロンの

261
00:15:30,100 --> 00:15:38,340
出力を取り込み、0から1の間の数値を出力する。

262
00:15:39,200 --> 00:15:43,130
ネットワーク全体は単なる関数で、784個の数字を入力と

263
00:15:43,130 --> 00:15:47,060
して受け取り、10個の数字を出力として吐き出すものだ。

264
00:15:47,560 --> 00:15:52,214
この関数は、特定のパターンをピックアップする重みとバイアス

265
00:15:52,214 --> 00:15:56,869
の形で13,000ものパラメータを含み、多くの行列ベクトル

266
00:15:56,869 --> 00:16:01,523
積とシグモイド二乗化関数を反復する、とんでもなく複雑な関数

267
00:16:01,523 --> 00:16:06,178
だが、それでもただの関数であり、複雑に見えるのはある意味心

268
00:16:06,178 --> 00:16:06,660
強い。

269
00:16:07,340 --> 00:16:09,755
つまり、もっと単純なものであれば、数字を認識

270
00:16:09,755 --> 00:16:12,280
することに挑戦できるという望みがあるだろうか？

271
00:16:13,340 --> 00:16:14,700
そして、その挑戦はどのように行われるのだろうか？

272
00:16:15,080 --> 00:16:17,220
このネットワークは、データを見るだけで、どのよう

273
00:16:17,220 --> 00:16:19,360
にして適切な重みとバイアスを学習するのだろうか？

274
00:16:20,140 --> 00:16:22,133
それは次のビデオでお見せするとして、私たち

275
00:16:22,133 --> 00:16:24,126
が見ているこの特別なネットワークが実際に何

276
00:16:24,126 --> 00:16:26,120
をしているのか、もう少し掘り下げてみよう。

277
00:16:27,580 --> 00:16:30,860
さて、ここでビデオや新しいビデオが公開されたときに通知を受

278
00:16:30,860 --> 00:16:34,140
け取れるように購読を申し込むべきだと思うが、現実的にはYo

279
00:16:34,140 --> 00:16:37,420
uTubeからの通知を受け取っていない人がほとんどだろう。

280
00:16:38,020 --> 00:16:40,413
もっと正直に言うと、YouTubeの推薦アルゴリズ

281
00:16:40,413 --> 00:16:42,806
ムの根底にあるニューラルネットワークが、このチャン

282
00:16:42,806 --> 00:16:45,199
ネルからのコンテンツがあなたに推薦されるのを見たい

283
00:16:45,199 --> 00:16:47,880
と思うようにするために、購読すると言うべきかもしれない。

284
00:16:48,560 --> 00:16:49,940
とにかく、続報をお待ちいただきたい。

285
00:16:50,760 --> 00:16:52,130
Patreonでこのビデオをサポートし

286
00:16:52,130 --> 00:16:53,500
てくれている皆さん、本当にありがとう。

287
00:16:54,000 --> 00:16:56,569
この夏、私は確率のシリーズの進行が少し遅かったが、この

288
00:16:56,569 --> 00:16:59,139
プロジェクトの後、再びこのシリーズに飛び込んでいるので

289
00:16:59,139 --> 00:17:01,900
、パトロンの皆さんはそちらの更新を楽しみにしていてほしい。

290
00:17:03,600 --> 00:17:07,273
最後に、ディープラーニングの理論的側面について博士号を

291
00:17:07,273 --> 00:17:10,946
取得し、現在はアンプリファイ・パートナーズというベンチ

292
00:17:10,946 --> 00:17:14,619
ャーキャピタルで働いているリーシャ・リーを紹介しよう。

293
00:17:15,460 --> 00:17:17,290
そこでリーシャ、ひとつ手っ取り早くこのシ

294
00:17:17,290 --> 00:17:19,119
グモイド関数について話しておこうと思う。

295
00:17:19,700 --> 00:17:24,674
私が理解するところでは、初期のネットワークは、この加

296
00:17:24,674 --> 00:17:29,840
重和をゼロと1の間の区間に押し込めるためにこれを使う。

297
00:17:30,280 --> 00:17:30,300
その通りだ。

298
00:17:30,560 --> 00:17:32,300
しかし、実際にシグモイドを使用している

299
00:17:32,300 --> 00:17:34,040
最新のネットワークはもう比較的少ない。

300
00:17:34,320 --> 00:17:34,320
そうだ。

301
00:17:34,440 --> 00:17:35,540
古臭いだろ？

302
00:17:35,760 --> 00:17:38,980
そう、いや、むしろレルーの方がトレーニングしやすいようだ。

303
00:17:39,400 --> 00:17:42,340
reluはrectified linear unitの略か？

304
00:17:42,680 --> 00:17:48,229
そうだね、ゼロとaの最大値をとるだけの関数で、aはビデ

305
00:17:48,229 --> 00:17:51,724
オで説明していたもので与えられる。

306
00:17:51,724 --> 00:17:57,273
これは、ニューロンが活性化されるかされないかという生物

307
00:17:57,273 --> 00:18:02,823
学的な類推から、ある閾値を超えると同一関数になるが、そ

308
00:18:02,823 --> 00:18:08,373
うでなければ活性化されないのでゼロになる、というような

309
00:18:08,373 --> 00:18:10,840
単純化したものだと思う。

310
00:18:11,160 --> 00:18:15,646
シグモイドを使ってもトレーニングがうまくい

311
00:18:15,646 --> 00:18:20,133
かなかったり、トレーニングが非常に難しかっ

312
00:18:20,133 --> 00:18:24,620
たりしたので、人々はreluを試してみた。

313
00:18:25,100 --> 00:18:25,640
アリシア、ありがとう。

