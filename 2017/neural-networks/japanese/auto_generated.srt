1
00:00:00,000 --> 00:00:01,310
これは 3 です。

2
00:00:01,310 --> 00:00:05,387
雑に書かれ、28x28 ピクセルという非

3
00:00:05,387 --> 00:00:08,852
常に低い解像度でレンダリングされ

4
00:00:08,852 --> 00:00:13,743
ていますが、脳は問題なく 3 として認識します。

5
00:00:13,743 --> 00:00:15,745
そして、脳がこれを簡単に実行でき

6
00:00:15,745 --> 00:00:18,687
ることがどれほどクレイジーであるかを少し理解しても

7
00:00:18,687 --> 00:00:19,629
らいたいのです。

8
00:00:19,629 --> 00:00:24,078
つまり、各ピ クセルの具体的な値は画像ごとに大きく異な

9
00:00:24,078 --> 00:00:28,691
りますが、これ、これ、こ れも 3 として認識できます。

10
00:00:28,691 --> 00:00:31,368
この 3 つを見たときに発火する目の特

11
00:00:31,368 --> 00:00:35,383
定の光感受性細胞は、この 3 つを見たときに発火する細胞とは

12
00:00:35,383 --> 00:00:36,722
大きく異なり ます。

13
00:00:36,722 --> 00:00:40,859
しかし、あなたのその非常に賢い視覚野の何かは、これ

14
00:00:40,859 --> 00:00:44,997
らを同じアイデアを表す ものとして解決し、同時に他

15
00:00:44,997 --> 00:00:49,300
の画像をそれら自身の異なるアイデアとして認識します。

16
00:00:49,300 --> 00:00:52,445
しかし、もし私が、「おい、座って、28x28

17
00:00:52,445 --> 00:00:55,591
のグリッドを取り込んで 0 から 10 まで

18
00:00:55,591 --> 00:00:58,873
の 1 つの数値を出力し、その数字が何であるかを

19
00:00:58,873 --> 00:01:01,882
示すプログラムを書いてくれ」と言ったら、そ

20
00:01:01,882 --> 00:01:05,165
のタスクは滑稽なほど簡単なものから、気の遠くなる

21
00:01:05,165 --> 00:01:06,122
ような難しい。

22
00:01:06,122 --> 00:01:09,496
岩の下で生きている人で ない限り、機械学習とニューラル

23
00:01:09,496 --> 00:01:12,147
ネットワークの現在と将来に対する関連性と重

24
00:01:12,147 --> 00:01:14,557
要性を説く必要はほとんどないと思います。

25
00:01:14,557 --> 00:01:16,822
しかし、私がここでやりたいのは、背景が何もないことを前提

26
00:01:16,822 --> 00:01:18,696
として、ニューラル ネットワークが実際にどのよう

27
00:01:18,696 --> 00:01:20,571
なものであるかを示し、バズワードとしてではなく数

28
00:01:20,571 --> 00:01:21,976
学の一部として、ニューラル ネット

29
00:01:21,976 --> 00:01:24,241
ワークが何を行っているのかを視覚化するのに役立つことです。

30
00:01:24,241 --> 00:01:26,984
私の願いは、構造そのものが動機づけられていると感じて帰っ

31
00:01:26,984 --> 00:01:29,537
てきて、ニューラル ネットワークのクォートアンクォート

32
00:01:29,537 --> 00:01:31,806
学習について読んだり聞いたりしたときに、それが

33
00:01:31,806 --> 00:01:34,549
何を意味するのかわかったように感じてもらえることだけです。

34
00:01:34,549 --> 00:01:37,424
このビデオではその構造コンポ ーネントについて

35
00:01:37,424 --> 00:01:40,300
のみ説明し、次のビデオでは学習に取り組みます。

36
00:01:40,300 --> 00:01:43,147
私たちがやろうとしているのは、手書きの数字の認識を学

37
00:01:43,147 --> 00:01:46,104
習できるニューラル ネットワークを構築すること です。

38
00:01:46,104 --> 00:01:49,908
これは、このトピックを紹介するためのやや古典的な例です。

39
00:01:49,908 --> 00:01:52,354
ここでは現状のま まで構いません。

40
00:01:52,354 --> 00:01:55,886
2 つのビデオの最後に、詳細を学ぶことができるいく

41
00:01:55,886 --> 00:01:58,739
つかの優れ たリソースを示したいからです。

42
00:01:58,739 --> 00:02:01,999
これを行うコードをダウンロードして、自分のコ

43
00:02:01,999 --> 00:02:04,173
ンピュータで試すことができます。

44
00:02:04,173 --> 00:02:07,434
ニューラル ネットワークには多くの亜種があり

45
00:02:07,434 --> 00:02:11,121
、近年、これらの亜種に対する研究が一種のブームになっ

46
00:02:11,121 --> 00:02:14,807
ていますが、これら 2 つの紹介ビデオでは、余分な装

47
00:02:14,807 --> 00:02:18,920
飾のない、最も単純なプレーン バニラ形式を見 ていきます。

48
00:02:18,920 --> 00:02:22,108
これは、より強力な最新の亜種を理解するために必要

49
00:02:22,108 --> 00:02:25,297
な前提条件のよ うなものですが、私たちが理解する

50
00:02:25,297 --> 00:02:28,485
にはまだ複雑さがたくさんあると信じてくださ い。

51
00:02:28,485 --> 00:02:31,200
しかし、この最も単純な形式であっても、手書きの

52
00:02:31,200 --> 00:02:33,206
数字を認識することを学習できます。

53
00:02:33,206 --> 00:02:36,628
これは コンピューターにとって非常に素晴らしいことです。

54
00:02:36,628 --> 00:02:39,559
そして同時に、それが私たちがそれに対して抱くかもしれ

55
00:02:39,559 --> 00:02:42,707
ないいくつかの希望をいかに満たしていないのかもわかるでしょ

56
00:02:42,707 --> 00:02:42,925
う。

57
00:02:42,925 --> 00:02:45,512
名前が示すように、ニューラル ネットワークは脳からイン

58
00:02:45,512 --> 00:02:48,193
スピレーションを得ていますが、それを詳しく見てみましょう。

59
00:02:48,193 --> 00:02:50,013
ニューロンとは何ですか? それらはどのよ

60
00:02:50,013 --> 00:02:51,833
うな意味でつながって いるのでしょうか?

61
00:02:51,833 --> 00:02:56,332
今、私がニューロンと言うときに考えていただきたいのは、数値

62
00:02:56,332 --> 00:03:00,987
、具体的には 0 と 1 の間の数値を保持するものだけです。

63
00:03:00,987 --> 00:03:03,377
本当にそれ以上ではありません。

64
00:03:03,377 --> 00:03:06,759
たとえば、ネッ トワークは、入力画像の 28 x

65
00:03:06,759 --> 00:03:10,683
28 ピクセルのそれぞれに対応するニューロンの束から始ま

66
00:03:10,683 --> 00:03:13,660
り、合計 784 個のニューロンになります。

67
00:03:13,660 --> 00:03:17,151
これらのそれぞれには、黒ピクセルの 0 か

68
00:03:17,151 --> 00:03:20,642
ら白ピクセルの 1 までの、対応するピクセル

69
00:03:20,642 --> 00:03:24,293
のグレースケール値を表す数値が保持されま す。

70
00:03:24,293 --> 00:03:28,113
ニューロン内のこの数値は活性化と呼ばれ、活

71
00:03:28,113 --> 00:03:31,933
性化の数値が高いと各ニュー ロンが点灯する

72
00:03:31,933 --> 00:03:35,935
というイメージを思い浮かべるかもしれません。

73
00:03:35,935 --> 00:03:39,667
したがって、これら 7 84 個のニューロン

74
00:03:39,667 --> 00:03:43,399
すべてがネットワークの最初の層を構成します。

75
00:03:43,399 --> 00:03:46,901
最後の層に飛びます。 これには 10 個

76
00:03:46,901 --> 00:03:51,570
のニューロンがあり、それぞれが数字の 1 つを表します。

77
00:03:51,570 --> 00:03:54,920
これらのニューロンの活性化は、 やはり 0 と

78
00:03:54,920 --> 00:03:58,410
1 の間の数値であり、特定の画像が特定の数字に対応

79
00:03:58,410 --> 00:04:02,180
しているとシステ ムがどの程度考えているかを表します。

80
00:04:02,180 --> 00:04:05,202
間には隠れ層と呼ばれるいくつかの層も

81
00:04:05,202 --> 00:04:09,020
ありますが、当面はこの数字を認識するプロセスが一

82
00:04:09,020 --> 00:04:12,838
体どのように処理され るのかという大きな疑問符が

83
00:04:12,838 --> 00:04:13,952
付くはずです。

84
00:04:13,952 --> 00:04:16,187
このネットワークでは、それぞれ 16 個の

85
00:04:16,187 --> 00:04:18,422
ニューロンを持つ 2 つの隠れ層を選択しまし

86
00:04:18,422 --> 00:04:20,657
たが、確かに、これは一種の恣意的な選択です。

87
00:04:20,657 --> 00:04:23,274
正直に言うと、 一瞬のうちに構造をどのように動か

88
00:04:23,274 --> 00:04:26,109
したいかに基づいて 2 つのレイヤーを選択しました。

89
00:04:26,109 --> 00:04:28,617
16 は、画面に収まるちょうどいい数でした。

90
00:04:28,617 --> 00:04:31,350
実際には、ここには特定の構造を実験する余

91
00:04:31,350 --> 00:04:32,782
地がたくさんあります。

92
00:04:32,782 --> 00:04:35,696
ネットワークの動作方法では、ある層でのアクティベー

93
00:04:35,696 --> 00:04:38,610
ションが次の層のアク ティベーションを決定します。

94
00:04:38,610 --> 00:04:42,476
そしてもちろん、情報処理メカニズムとしてのネットワーク

95
00:04:42,476 --> 00:04:45,791
の中心は、ある層からの活性化が次の層の活性化をど

96
00:04:45,791 --> 00:04:48,829
のように引き起こすかということ になります。

97
00:04:48,829 --> 00:04:51,498
これは、ニューロンの生物学的ネットワークにおいて、あ

98
00:04:51,498 --> 00:04:53,654
るニューロンのグループの発火が他のニュー

99
00:04:53,654 --> 00:04:56,324
ロンの発火を引き起こす方法に大まかに似ていることを意

100
00:04:56,324 --> 00:04:57,043
図しています。

101
00:04:57,043 --> 00:04:59,216
ここで示しているネットワークはすでに数

102
00:04:59,216 --> 00:05:01,172
字を認識するように訓練されています。

103
00:05:01,172 --> 00:05:03,455
これが何を意味するのかを説明しましょう。

104
00:05:03,455 --> 00:05:06,951
これは、画像内の 各ピクセルの明るさに応じて入力層の

105
00:05:06,951 --> 00:05:10,318
784 個のニューロンすべてを点灯する画像を入力す

106
00:05:10,318 --> 00:05:13,944
ると、その活性化パターンが次の層で非常に特殊なパターンを

107
00:05:13,944 --> 00:05:16,016
引き起こし、その次の層で何らか

108
00:05:16,016 --> 00:05:18,477
のパターンが発生することを意味します。

109
00:05:18,477 --> 00:05:22,103
これにより、最終的に出力層にパターンが与えられま す。

110
00:05:22,103 --> 00:05:26,193
そして、その出力層の最も明るいニューロンは、いわば、こ

111
00:05:26,193 --> 00:05:30,434
の画像が表す桁をネットワ ークが選択することになります。

112
00:05:30,434 --> 00:05:33,599
ある層が次の層にどのような影響を与えるか、またはトレ

113
00:05:33,599 --> 00:05:36,764
ーニングがど のように機能するかについての数学に入る

114
00:05:36,764 --> 00:05:39,928
前に、このような層構造がインテリジェントに動作すると

115
00:05:39,928 --> 00:05:43,458
期待する のが合理的である理由について少しお話しましょう。

116
00:05:43,458 --> 00:05:45,046
ここで私たちは何を期待しているのでしょうか？

117
00:05:45,046 --> 00:05:46,985
これらの中間層がやっている 可能性のあ

118
00:05:46,985 --> 00:05:49,026
ることに対する最善の希望は何でしょうか?

119
00:05:49,026 --> 00:05:51,308
さて、あなたまたは私が数字を認識するとき、私た

120
00:05:51,308 --> 00:05:53,689
ちはさまざまなコンポーネ ントを組み合わせます。

121
00:05:53,689 --> 00:05:56,934
A 9 には上部にループがあり、右側にラインがあります。

122
00:05:56,934 --> 00:05:59,067
8 にもトップにループがあります

123
00:05:59,067 --> 00:06:01,827
が、ローに別のループが組み合わされています。

124
00:06:01,827 --> 00:06:06,673
A 4 は基本的に 3 つの特定の行に分 かれています。

125
00:06:06,673 --> 00:06:08,856
これで完璧な世界では、最後から 2

126
00:06:08,856 --> 00:06:12,252
番目の層の各ニューロンがこれらのサブコン ポーネントの

127
00:06:12,252 --> 00:06:14,556
1 つに対応し、たとえば 9 や 8

128
00:06:14,556 --> 00:06:17,224
などの上部にループがある画像を入力するたび

129
00:06:17,224 --> 00:06:20,619
に、いくつかのサブコンポーネントが存在することを期待でき

130
00:06:20,619 --> 00:06:23,651
ます。 活性化が 1 に近づく特定のニュ ーロン。

131
00:06:23,651 --> 00:06:26,334
そして、私はこの特定のピクセルのループを意味するの

132
00:06:26,334 --> 00:06:28,695
ではなく、上部に向かう一般的なループ状のパ

133
00:06:28,695 --> 00:06:31,700
ターンがこのニューロンを引き起こすことを期待しています。

134
00:06:31,700 --> 00:06:34,057
そうすれば、3 番目の層から最後の層

135
00:06:34,057 --> 00:06:36,787
に進むには、サブコンポーネントのどの組み合わ

136
00:06:36,787 --> 00:06:40,012
せがどの数字に対応するかを学習するだけで 済みます。

137
00:06:40,012 --> 00:06:41,843
もちろん、これは将来の問題を引き起こすだけです。

138
00:06:41,843 --> 00:06:43,979
なぜなら、これらのサブコンポーネントをどうやって認識

139
00:06:43,979 --> 00:06:45,887
するのでしょうか、あるいは、適切なサブコンポーネン

140
00:06:45,887 --> 00:06:47,794
トが何であるべきかを知ることさえできるでしょうか?

141
00:06:47,794 --> 00:06:49,469
あるレイヤーが次 のレイヤーにどのような影

142
00:06:49,469 --> 00:06:51,144
響を与えるかについてはまだ話していませんが

143
00:06:51,144 --> 00:06:52,900
、この点について少し一緒に考えてみましょう。

144
00:06:52,900 --> 00:06:56,598
ループを認識すると、サブ問題に分解することもできます。

145
00:06:56,598 --> 00:06:59,340
これを行うための合理的な方法の 1 つは

146
00:06:59,340 --> 00:07:02,733
、まずそれを構成するさまざまな小さなエッジを認識する

147
00:07:02,733 --> 00:07:03,386
ことです。

148
00:07:03,386 --> 00:07:05,847
同様に、数字の 1、4、または 7

149
00:07:05,847 --> 00:07:09,812
に見られるような長い線も、実際には単なる長いエッジであるか

150
00:07:09,812 --> 00:07:13,777
、あるいはいくつかの小さなエ ッジからなる特定のパターンと

151
00:07:13,777 --> 00:07:15,281
考えることもできます。

152
00:07:15,281 --> 00:07:18,313
したがって、おそらく私たちの希望は、ネットワーク

153
00:07:18,313 --> 00:07:20,981
の 2 番目の層の各ニューロンが、関連するさ

154
00:07:20,981 --> 00:07:23,406
まざまな小さなエッジに対応することです。

155
00:07:23,406 --> 00:07:26,643
おそらく、このよう な画像が入力されると、約 8

156
00:07:26,643 --> 00:07:30,527
～ 10 個の特定の小さなエッジに関連付けられたすべてのニ

157
00:07:30,527 --> 00:07:33,764
ューロンが点灯し、次に上部のループと長い垂直線に関

158
00:07:33,764 --> 00:07:36,354
連付けられたニューロンが点灯し、それら

159
00:07:36,354 --> 00:07:39,720
のニューロンが点灯します。 9に関連するニューロン。

160
00:07:39,720 --> 00:07:41,811
これが最終的なネットワークが実際に行うこ

161
00:07:41,811 --> 00:07:44,299
とであるかどうかは別の問題であり、ネットワークをト

162
00:07:44,299 --> 00:07:47,186
レーニングする方法がわかったら、またこの問題に 戻ります。

163
00:07:47,186 --> 00:07:49,760
しかし、これは私たちが持つかもしれない希望であ

164
00:07:49,760 --> 00:07:52,334
り、このような階層構造を持つ一種の目 標です。

165
00:07:52,334 --> 00:07:56,289
さらに、このようにエッジやパターンを検出できれば、他

166
00:07:56,289 --> 00:08:00,397
の画像認識タ スクにも非常に役立つことが想像できます。

167
00:08:00,397 --> 00:08:03,784
画像認識を超えて、抽象化の層 に分割して実行したい

168
00:08:03,784 --> 00:08:07,306
あらゆる種類のインテリジェントな処理が存在し ます。

169
00:08:07,306 --> 00:08:10,443
たとえば、音声の解析には、生の音声を取り込み、

170
00:08:10,443 --> 00:08:13,579
特定の音節を形成するために 結合したり、単語を

171
00:08:13,579 --> 00:08:16,716
形成したり結合してフレーズやより抽象的な思考を

172
00:08:16,716 --> 00:08:20,262
構成したりする個 別の音を抽出することが含まれます。

173
00:08:20,262 --> 00:08:23,604
しかし、これが実際にどのように機能するかに戻って、ある層

174
00:08:23,604 --> 00:08:27,062
のアクティベーションが次の層のアクティベーションをどのように

175
00:08:27,062 --> 00:08:29,367
正確に決定するかを今設計している自分を

176
00:08:29,367 --> 00:08:30,635
想像してみてください。

177
00:08:30,635 --> 00:08:33,300
目標は、ピクセルをエッジに結合したり、エッジ

178
00:08:33,300 --> 00:08:35,238
をパターンに結合したり、パター

179
00:08:35,238 --> 00:08:38,872
ンを数字に結合したりできる何らかのメカニズムを持つことです。

180
00:08:38,872 --> 00:08:41,491
そして、1 つの非常に具体的な例にズー

181
00:08:41,491 --> 00:08:43,980
ムインするために、2 番目の層の 1

182
00:08:43,980 --> 00:08:47,909
つの特定のニューロンが、画像のこの領域にエッジがあるかどう

183
00:08:47,909 --> 00:08:50,660
かを検出することが期待されているとします。

184
00:08:50,660 --> 00:08:52,929
ここでの問題は、ネットワークにどのよう

185
00:08:52,929 --> 00:08:55,197
なパラメータが必要かとい うことです。

186
00:08:55,197 --> 00:08:57,383
このパターンやその他のピクセル

187
00:08:57,383 --> 00:09:00,252
パターン、あるいは複数のエッジがループを

188
00:09:00,252 --> 00:09:03,394
作るパターンなどを潜在的に捉えるのに十分な表現

189
00:09:03,394 --> 00:09:05,990
力を持たせるためには、どのダイヤルや

190
00:09:05,990 --> 00:09:08,175
ノブを調整すればよいでしょうか?

191
00:09:08,175 --> 00:09:11,686
さて、これから行うことは、ニューロンと最初の層の

192
00:09:11,686 --> 00:09:15,759
ニューロンの間の接続のそれぞれに重みを割り当てることです。

193
00:09:15,759 --> 00:09:17,666
これらの重みは単なる数値 です。

194
00:09:17,666 --> 00:09:21,614
次に、最初の層からこれらのアクティベーションをすべて取得

195
00:09:21,614 --> 00:09:25,704
し、これらの重みに従って重み付け された合計を計算します。

196
00:09:25,704 --> 00:09:29,664
これらの重みが独自の小さなグリッドに編成されていると考える

197
00:09:29,664 --> 00:09:32,832
とわかりやすいと思います。 正の重みを示すために

198
00:09:32,832 --> 00:09:35,208
緑のピクセルを使用し、負の重みを示

199
00:09:35,208 --> 00:09:37,453
すために赤のピクセルを使用します。

200
00:09:37,453 --> 00:09:39,961
そのピクセルの明るさはある程度です。

201
00:09:39,961 --> 00:09:41,941
重みの値 のゆるやかな描写。

202
00:09:41,941 --> 00:09:46,088
関心のあるこの領域のいくつかの正の重みを除いて、ほぼすべて

203
00:09:46,088 --> 00:09:50,097
のピクセルに関連付けられた重みをゼロにすると、すべてのピク

204
00:09:50,097 --> 00:09:54,105
セル値の重み付き 合計を取ることは、実際にはピクセルの値を

205
00:09:54,105 --> 00:09:57,976
合計することになります。 私たちが大 切にしている地域。

206
00:09:57,976 --> 00:10:02,449
ここにエッジがあるかどうかを本当に確認したい場合は

207
00:10:02,449 --> 00:10:07,440
、周囲のピクセルに負の重みを関連付けることが考えられます。

208
00:10:07,440 --> 00:10:09,989
中央のピ クセルが明るく、周囲のピク

209
00:10:09,989 --> 00:10:12,680
セルが暗い場合、合計は最大になります。

210
00:10:12,680 --> 00:10:16,307
このように加重合計を計算すると、任意の数値が得られる可

211
00:10:16,307 --> 00:10:19,129
能性がありますが、このネットワークで必要

212
00:10:19,129 --> 00:10:21,951
なのは、アクティベーションが 0 と 1

213
00:10:21,951 --> 00:10:23,563
の間の値であることです。

214
00:10:23,563 --> 00:10:26,191
したがって、一般的に行うべき ことは、この重み付

215
00:10:26,191 --> 00:10:28,381
けされた合計を、実数直線を 0 と 1

216
00:10:28,381 --> 00:10:31,447
の間の範囲に押し込む何らかの関数にポンプ することです。

217
00:10:31,447 --> 00:10:34,448
これを行う一般的な関数はシグモイド関数と呼ばれ

218
00:10:34,448 --> 00:10:37,449
、ロジスティック曲線としても知 られています。

219
00:10:37,449 --> 00:10:40,449
基本的に、非常に負の入力は 0

220
00:10:40,449 --> 00:10:45,700
に近くなり、非常に正の入力は 1 に近くなり、入力 0

221
00:10:45,700 --> 00:10:48,137
の周りで着実に増加します。

222
00:10:48,137 --> 00:10:51,470
したがって、ここでのニューロンの活性化は

223
00:10:51,470 --> 00:10:55,754
、基本的に、関連する加重合計がどれだけ正であるかの尺度

224
00:10:55,754 --> 00:10:56,706
になります。

225
00:10:56,706 --> 00:10:58,755
しかし、重み付けされた合計が 0 よ

226
00:10:58,755 --> 00:11:01,558
り大きいときにニューロンを点灯させたいわけではないか

227
00:11:01,558 --> 00:11:02,313
もしれません。

228
00:11:02,313 --> 00:11:04,703
おそらく、合計が 10 よりも大きい場合に

229
00:11:04,703 --> 00:11:06,768
のみアクティブにしたい場合があります。

230
00:11:06,768 --> 00:11:10,663
つまり、非アクティブにするために何らかのバイアスが必要です。

231
00:11:10,663 --> 00:11:15,648
次に行うこ とは、シグモイド圧縮関数に接続する前に、この加

232
00:11:15,648 --> 00:11:20,634
重合計にマイナス 10 などの他の数値を加算することです。

233
00:11:20,634 --> 00:11:23,382
この追加の数はバイアスと呼ばれます。

234
00:11:23,382 --> 00:11:26,232
したがっ て、重みは、2 番目の層のこのニューロンがどの

235
00:11:26,232 --> 00:11:29,083
ピクセル パターンを認識しているかを示し、バイアスは、ニ

236
00:11:29,083 --> 00:11:31,933
ュー ロンが意味のあるアクティブになり始める前に、重み付

237
00:11:31,933 --> 00:11:34,784
けされた合計がどのくらい大きくなければならないかを示し

238
00:11:34,784 --> 00:11:35,089
ます。

239
00:11:35,089 --> 00:11:37,188
そしてそれは単なる 1 つのニューロンです。

240
00:11:37,188 --> 00:11:41,640
この層の 1 つおきのニューロンは、最初の層の 784

241
00:11:41,640 --> 00:11:45,934
個 のピクセル ニューロンすべてに接続され、これらの

242
00:11:45,934 --> 00:11:50,705
784 個の接続のそれぞれに独自の重 みが関連付けられます。

243
00:11:50,705 --> 00:11:54,332
また、それぞれには何らかのバイアスがあり、シグモイドで押

244
00:11:54,332 --> 00:11:57,960
しつぶす前 に加重和に加算する他の数値が含まれています。

245
00:11:57,960 --> 00:11:59,684
そして、それは考えるべきことがたくさんあります！

246
00:11:59,684 --> 00:12:03,929
この 16 ニュー ロンの隠れ層では、合計 784

247
00:12:03,929 --> 00:12:07,848
× 16 の重みと 16 のバイアスになります。

248
00:12:07,848 --> 00:12:10,242
これらはすべて 、第 1 層から第

249
00:12:10,242 --> 00:12:12,104
2 層への接続にすぎません。

250
00:12:12,104 --> 00:12:14,996
他の層間の接続にも、それ らに関連

251
00:12:14,996 --> 00:12:18,059
する一連の重みとバイアスがあります。

252
00:12:18,059 --> 00:12:21,463
結局のところ、このネットワーク には、合計でほぼ正確に

253
00:12:21,463 --> 00:12:24,015
13,000 の重みとバイアスがあります。

254
00:12:24,015 --> 00:12:26,515
13,000 個のノブとダイヤルを調整した

255
00:12:26,515 --> 00:12:29,697
り回して、このネットワークをさまざまな方法で動作させるこ

256
00:12:29,697 --> 00:12:30,493
とができます。

257
00:12:30,493 --> 00:12:32,682
したがって、学習について話すとき

258
00:12:32,682 --> 00:12:36,416
、それが指しているのは、目前の問題を実際に解決できるように

259
00:12:36,416 --> 00:12:38,605
、これらの多くの数値すべてに対し

260
00:12:38,605 --> 00:12:41,953
て有効な設定をコンピューターに見つけさせることです。

261
00:12:41,953 --> 00:12:44,960
楽しくもあり、ある意味恐ろしくもある 思考実験の

262
00:12:44,960 --> 00:12:47,847
1 つは、座ってこれらの重みとバイアスをすべて手

263
00:12:47,847 --> 00:12:51,095
動で設定し、2 番目の レイヤーがエッジを認識し、3

264
00:12:51,095 --> 00:12:54,464
番目のレイヤーがパターンを認識するように数値を意図的に

265
00:12:54,464 --> 00:12:56,509
微調整することを想像することです。

266
00:12:56,509 --> 00:12:59,308
等私個人としては、ネットワークを完全なブラック ボ

267
00:12:59,308 --> 00:13:02,323
ックスとして扱うよりも、これで満足できると感じています。

268
00:13:02,323 --> 00:13:04,045
なぜなら、ネットワークが期待

269
00:13:04,045 --> 00:13:06,952
どおりに動作しないとき、それらの重みやバイアスが実際に

270
00:13:06,952 --> 00:13:10,074
何を意味するのかについて少しでも 関係を構築していれば、

271
00:13:10,074 --> 00:13:12,981
、改善するために構造を変更する方法を実験するための出発

272
00:13:12,981 --> 00:13:13,950
点が得 られます。

273
00:13:13,950 --> 00:13:17,013
あるいは、ネットワークが機能しているものの、予想どおり

274
00:13:17,013 --> 00:13:20,076
の理由ではない場合、重 みとバイアスが何をしているのか

275
00:13:20,076 --> 00:13:23,365
を掘り下げることは、自分の仮定に疑問を投げかけ、考えられ

276
00:13:23,365 --> 00:13:26,201
る解決策の全領域を実際に明らかにする良い方法です。

277
00:13:26,201 --> 00:13:32,177
ところで、ここで実際の関 数を書くのは少し面倒ですよね。

278
00:13:32,177 --> 00:13:34,488
そこで、これらの接続をよりコン

279
00:13:34,488 --> 00:13:37,087
パクトに表記する方法を示しましょう。

280
00:13:37,087 --> 00:13:39,608
ニューラル ネットワークについてさらに詳しく読ん

281
00:13:39,608 --> 00:13:41,120
でみると、次のようになります。

282
00:13:41,120 --> 00:13:43,566
1 つのレイヤーのすべてのアクティベー

283
00:13:43,566 --> 00:13:46,140
ションをベクトルとして列に編成 します。

284
00:13:46,140 --> 00:13:50,463
次に、すべての重みを行列として編成します。

285
00:13:50,463 --> 00:13:56,433
行列の各行は、あ る層と次の層の特定のニューロンの間の接

286
00:13:56,433 --> 00:13:58,080
続に対応します。

287
00:13:58,080 --> 00:14:01,954
これが意味 するのは、これらの重みに従って最初の層のアクテ

288
00:14:01,954 --> 00:14:04,091
ィベーションの重み付き合計を取

289
00:14:04,091 --> 00:14:07,698
得すると、左側にあるすべての行列ベクトル積の項の 1

290
00:14:07,698 --> 00:14:09,702
つに対応するということ です。

291
00:14:09,702 --> 00:14:15,860
ちなみに、機械学習の多くは線形代数をよく理解することに尽き

292
00:14:15,860 --> 00:14:20,581
るので、行列と行列ベクトルの乗算の意味を視覚的

293
00:14:20,581 --> 00:14:26,123
に理解したい人は、 私が行ったシリーズを見てください。

294
00:14:26,123 --> 00:14:28,997
線形代数、特に第 3 章。

295
00:14:28,997 --> 00:14:32,330
式に戻 ると、これらの値のそれぞれに独立してバイア

296
00:14:32,330 --> 00:14:35,263
スを追加することについて話すのではなく、こ

297
00:14:35,263 --> 00:14:38,596
れらすべてのバイアスをベクトルに編成し、ベクトル全

298
00:14:38,596 --> 00:14:40,862
体を前の行列ベクトル積に追加する

299
00:14:40,862 --> 00:14:42,729
ことによってそれを表します。

300
00:14:42,729 --> 00:14:46,581
次に、最後のステップとして、ここで外側にシグモイドを巻き

301
00:14:46,581 --> 00:14:49,636
付けます。 これが表すことは、結果として得られ

302
00:14:49,636 --> 00:14:52,292
るベクトルの内部の特定の各コンポーネン

303
00:14:52,292 --> 00:14:55,215
トにシグモイド関数を適用することになります。

304
00:14:55,215 --> 00:14:57,902
したがって、この重み行列とこれらのベク

305
00:14:57,902 --> 00:15:01,261
トルを独自のシンボルとして書き留めると、ある層から

306
00:15:01,261 --> 00:15:03,545
次の層へのアクティベーションの完

307
00:15:03,545 --> 00:15:06,905
全な移行を非常に厳密できちんとした小さな式で伝える

308
00:15:06,905 --> 00:15:09,189
ことができ、これにより関連するコ

309
00:15:09,189 --> 00:15:12,548
ードがはるかに単純になり、多くのライブラリが行列の

310
00:15:12,548 --> 00:15:16,042
乗算を最適化しているため、はるかに 高速になります。

311
00:15:16,042 --> 00:15:19,010
先ほど、これらのニューロンは単に数字を保持

312
00:15:19,010 --> 00:15:22,120
するものであると述べたことを覚えていますか?

313
00:15:22,120 --> 00:15:26,060
もちろん、それらが保持する具体的な数値は、入力した画像

314
00:15:26,060 --> 00:15:30,001
によって異なり ます。 そのため、実際には、各ニューロ

315
00:15:30,001 --> 00:15:32,628
ンを関数として考える方が正確で す。

316
00:15:32,628 --> 00:15:36,715
関数は、前の層のすべてのニューロンの出力を受け取り、

317
00:15:36,715 --> 00:15:38,321
0から1ま での数値。

318
00:15:38,321 --> 00:15:41,917
実際、ネットワーク全体は単なる関数であり、784

319
00:15:41,917 --> 00:15:44,650
個の数値を入力として受 け取り、10

320
00:15:44,650 --> 00:15:47,096
個の数値を出力として吐き出します。

321
00:15:47,096 --> 00:15:49,550
これは途方もなく複雑な関数であ

322
00:15:49,550 --> 00:15:53,538
り、特定のパターンを検出する重みとバイアスの形式で

323
00:15:53,538 --> 00:15:57,526
13,000 のパラメーターが含まれ、多くの行列ベク

324
00:15:57,526 --> 00:16:01,514
トル積とシグモイド潰し関数 の反復が含まれますが、そ

325
00:16:01,514 --> 00:16:03,201
れでも単なる関数です。

326
00:16:03,201 --> 00:16:05,009
そして、それが複雑に見える こと

327
00:16:05,009 --> 00:16:06,817
は、ある意味、安心感を与えます。

328
00:16:06,817 --> 00:16:09,865
つまり、もしそれがもっと単純だったら、数字を認識するとい

329
00:16:09,865 --> 00:16:12,807
う課題に挑戦できるなんて、どんな希望が持てるでしょうか?

330
00:16:12,807 --> 00:16:14,920
そして、その課題にどのように取り組むのでしょうか?

331
00:16:14,920 --> 00:16:17,400
このネットワークは、データを見るだけで適切な重

332
00:16:17,400 --> 00:16:19,880
みとバイアスをどのように学習するのでしょうか?

333
00:16:19,880 --> 00:16:21,712
それについては次のビデオで説明します。

334
00:16:21,712 --> 00:16:24,508
また、この特定のネットワークが実際に何をしているのかに

335
00:16:24,508 --> 00:16:26,147
ついてももう少し詳しく説明します。

336
00:16:26,147 --> 00:16:28,948
ここで重要なのは、そのビデオや新しいビデオが公開さ

337
00:16:28,948 --> 00:16:31,749
れたときの 通知を受け取るために購読すると言うべき

338
00:16:31,749 --> 00:16:34,549
だと思いますが、現実的には、ほとんどの人が実際に

339
00:16:34,549 --> 00:16:37,574
Yo uTube からの通知を受け取っていませんよね。

340
00:16:37,574 --> 00:16:39,838
おそらくもっと正直に言うと、YouTube

341
00:16:39,838 --> 00:16:42,000
の推奨アルゴリズムの基礎をなすニューラル

342
00:16:42,000 --> 00:16:44,881
ネットワークが、ユーザーがこのチャンネルのコンテンツを

343
00:16:44,881 --> 00:16:47,557
推奨されると信じ込むように、チャンネル登録するという

344
00:16:47,557 --> 00:16:48,278
べきでしょう。

345
00:16:48,278 --> 00:16:50,600
とにかく、続報をお待ちください。

346
00:16:50,600 --> 00:16:52,104
Patreon でこれらのビデオをサポート

347
00:16:52,104 --> 00:16:53,609
してくださった皆様に心より感謝いたします。

348
00:16:53,609 --> 00:16:55,176
この夏は確率シリーズの進歩が少

349
00:16:55,176 --> 00:16:58,018
し遅れていましたが、このプロジェクトの後はまた本格的に取り

350
00:16:58,018 --> 00:17:00,075
組むつもりですので、パトロンの皆様はそこ

351
00:17:00,075 --> 00:17:02,329
で最新情報をチェックしていただければ幸いです。

352
00:17:02,329 --> 00:17:05,113
最後に、深層学習の理論面で博士号の研究をし、

353
00:17:05,113 --> 00:17:07,897
現在はこのビデオに資金の一部を提供してくれた

354
00:17:07,897 --> 00:17:11,165
Amplify Partners と いうベンチャー

355
00:17:11,165 --> 00:17:14,191
キャピタル会社で働いている Leesha Lee

356
00:17:14,191 --> 00:17:15,160
に話を聞きます。

357
00:17:15,160 --> 00:17:17,320
そこで、リーシャ、すぐに取り上げるべき

358
00:17:17,320 --> 00:17:19,480
だと思うのが、このシグモイド関数です。

359
00:17:19,480 --> 00:17:21,566
私が理解しているところによると、初期のネットワーク

360
00:17:21,566 --> 00:17:23,569
はこれを使用して、関連する重み付けされた合計を

361
00:17:23,569 --> 00:17:25,155
0 と 1 の間の区間に押し込みます。

362
00:17:25,155 --> 00:17:27,241
ニューロンが非アクティブまたはアクティブであると

363
00:17:27,241 --> 00:17:29,328
いう生物学的な類似によって動 機づけられているのは

364
00:17:29,328 --> 00:17:29,995
ご存知でしょう。

365
00:17:29,995 --> 00:17:30,459
その通り。

366
00:17:30,459 --> 00:17:33,275
しかし、実際にシグモイドを使用している現代のネットワークは

367
00:17:33,275 --> 00:17:34,026
比較的少数です。

368
00:17:34,026 --> 00:17:34,288
うん。

369
00:17:34,288 --> 00:17:35,860
それはちょっと古い学校ですよね？

370
00:17:35,860 --> 00:17:38,872
そうですね、むしろレルのほうが訓練しや すいようです。

371
00:17:38,872 --> 00:17:42,306
そして、relu、reluは整流リニアユニットの略ですか？

372
00:17:42,306 --> 00:17:45,229
はい、これはこの種の関数 で、最大ゼロと

373
00:17:45,229 --> 00:17:47,873
a を取得するだけです。 ここで、a

374
00:17:47,873 --> 00:17:51,074
はビデオで説明した内容によって与え られます。

375
00:17:51,074 --> 00:17:54,223
これは、ニューロンがどのように活性化される

376
00:17:54,223 --> 00:17:56,922
か活性化されないかに関する生物学的

377
00:17:56,922 --> 00:18:00,670
な類似から部分的に動機付けられたものだと思います。

378
00:18:00,670 --> 00:18:03,311
したがって、特定のしきい値を超えた場

379
00:18:03,311 --> 00:18:06,090
合は恒等関数になりますが、超えなかった場

380
00:18:06,090 --> 00:18:09,287
合はアクティブ化されないため、ゼロになります。

381
00:18:09,287 --> 00:18:10,997
つまり、一種の簡略化です。

382
00:18:10,997 --> 00:18:14,261
シグモイドの使用はトレーニングに役立たなかったり、ある時点で

383
00:18:14,261 --> 00:18:17,524
トレ ーニングが非常に困難になったりしていましたが、人々は

384
00:18:17,524 --> 00:18:20,679
relu を試してみたところ、たまたまこれらの信じられな

385
00:18:20,679 --> 00:18:23,943
いほど深いニューラル ネットワークで非常にうまく機能しました

386
00:18:23,943 --> 00:18:24,052
。

387
00:18:24,052 --> 00:18:39,080
わかりました、リーシャ、ありがとう。

