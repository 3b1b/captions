1
00:00:04,219 --> 00:00:05,400
これは 3 です。

2
00:00:06,060 --> 00:00:08,571
雑に書かれ、28x28 ピクセルという非

3
00:00:08,571 --> 00:00:10,706
常に低い解像度でレンダリングされ 

4
00:00:10,706 --> 00:00:13,720
ていますが、脳は問題なく 3 として認識します。

5
00:00:14,340 --> 00:00:15,910
そして、脳がこれを簡単に実行でき 

6
00:00:15,910 --> 00:00:18,220
ることがどれほどクレイジーであるかを少し理解しても

7
00:00:18,220 --> 00:00:18,960
らいたいのです。

8
00:00:19,700 --> 00:00:23,931
つまり、各ピ クセルの具体的な値は画像ごとに大きく異な

9
00:00:23,931 --> 00:00:28,320
りますが、これ、これ、こ れも 3 として認識できます。

10
00:00:28,900 --> 00:00:31,580
この 3 つを見たときに発火する目の特 

11
00:00:31,580 --> 00:00:35,599
定の光感受性細胞は、この 3 つを見たときに発火する細胞とは

12
00:00:35,599 --> 00:00:36,940
大きく異なり ます。

13
00:00:37,520 --> 00:00:41,052
しかし、あなたのその非常に賢い視覚野の何かは、これ

14
00:00:41,052 --> 00:00:44,585
らを同じアイデアを表す ものとして解決し、同時に他

15
00:00:44,585 --> 00:00:48,260
の画像をそれら自身の異なるアイデアとして認識します。

16
00:00:49,220 --> 00:00:52,391
しかし、もし私が、「おい、座って、28x28 

17
00:00:52,391 --> 00:00:55,562
のグリッドを取り込んで 0 から 10 まで 

18
00:00:55,562 --> 00:00:58,872
の 1 つの数値を出力し、その数字が何であるかを

19
00:00:58,872 --> 00:01:01,905
示すプログラムを書いてくれ」と言ったら、そ 

20
00:01:01,905 --> 00:01:05,214
のタスクは滑稽なほど簡単なものから、気の遠くなる

21
00:01:05,214 --> 00:01:06,180
ような難しい。

22
00:01:07,160 --> 00:01:10,152
岩の下で生きている人で ない限り、機械学習とニューラル 

23
00:01:10,152 --> 00:01:12,502
ネットワークの現在と将来に対する関連性と重 

24
00:01:12,502 --> 00:01:14,640
要性を説く必要はほとんどないと思います。

25
00:01:15,120 --> 00:01:17,229
しかし、私がここでやりたいのは、背景が何もないことを前提 

26
00:01:17,229 --> 00:01:18,975
として、ニューラル ネットワークが実際にどのよう

27
00:01:18,975 --> 00:01:20,721
なものであるかを示し、バズワードとしてではなく数

28
00:01:20,721 --> 00:01:22,030
学の一部として、ニューラル ネット 

29
00:01:22,030 --> 00:01:24,140
ワークが何を行っているのかを視覚化するのに役立つことです。

30
00:01:24,140 --> 00:01:26,853
私の願いは、構造そのものが動機づけられていると感じて帰っ 

31
00:01:26,853 --> 00:01:29,380
てきて、ニューラル ネットワークのクォートアンクォート

32
00:01:29,380 --> 00:01:31,626
学習について読んだり聞いたりしたときに、それが 

33
00:01:31,626 --> 00:01:34,340
何を意味するのかわかったように感じてもらえることだけです。

34
00:01:35,360 --> 00:01:37,810
このビデオではその構造コンポ ーネントについて

35
00:01:37,810 --> 00:01:40,260
のみ説明し、次のビデオでは学習に取り組みます。

36
00:01:40,960 --> 00:01:43,452
私たちがやろうとしているのは、手書きの数字の認識を学

37
00:01:43,452 --> 00:01:46,040
習できるニューラル ネットワークを構築すること です。

38
00:01:49,360 --> 00:01:52,248
これは、このトピックを紹介するためのやや古典的な例です。

39
00:01:52,248 --> 00:01:54,105
 ここでは現状のま まで構いません。

40
00:01:54,105 --> 00:01:56,787
 2 つのビデオの最後に、詳細を学ぶことができるいく

41
00:01:56,787 --> 00:01:58,953
つかの優れ たリソースを示したいからです。

42
00:01:58,953 --> 00:02:01,429
 これを行うコードをダウンロードして、自分のコ 

43
00:02:01,429 --> 00:02:03,080
ンピュータで試すことができます。

44
00:02:05,040 --> 00:02:08,167
ニューラル ネットワークには多くの亜種があり 

45
00:02:08,167 --> 00:02:11,702
、近年、これらの亜種に対する研究が一種のブームになっ

46
00:02:11,702 --> 00:02:15,237
ていますが、これら 2 つの紹介ビデオでは、余分な装

47
00:02:15,237 --> 00:02:19,180
飾のない、最も単純なプレーン バニラ形式を見 ていきます。

48
00:02:19,860 --> 00:02:22,773
これは、より強力な最新の亜種を理解するために必要

49
00:02:22,773 --> 00:02:25,686
な前提条件のよ うなものですが、私たちが理解する

50
00:02:25,686 --> 00:02:28,600
にはまだ複雑さがたくさんあると信じてくださ い。

51
00:02:29,120 --> 00:02:31,586
しかし、この最も単純な形式であっても、手書きの

52
00:02:31,586 --> 00:02:33,409
数字を認識することを学習できます。

53
00:02:33,409 --> 00:02:36,520
 これは コンピューターにとって非常に素晴らしいことです。

54
00:02:37,480 --> 00:02:39,714
そして同時に、それが私たちがそれに対して抱くかもしれ 

55
00:02:39,714 --> 00:02:42,114
ないいくつかの希望をいかに満たしていないのかもわかるでしょ

56
00:02:42,114 --> 00:02:42,280
う。

57
00:02:43,380 --> 00:02:45,895
名前が示すように、ニューラル ネットワークは脳からイン 

58
00:02:45,895 --> 00:02:48,500
スピレーションを得ていますが、それを詳しく見てみましょう。

59
00:02:48,520 --> 00:02:50,090
ニューロンとは何ですか? それらはどのよ

60
00:02:50,090 --> 00:02:51,660
うな意味でつながって いるのでしょうか?

61
00:02:52,500 --> 00:02:56,402
今、私がニューロンと言うときに考えていただきたいのは、数値

62
00:02:56,402 --> 00:03:00,440
、具体的には 0 と 1 の間の数値を保持するものだけです。

63
00:03:00,680 --> 00:03:02,000
本当にそれ以上ではありません。

64
00:03:02,000 --> 00:03:06,019
たとえば、ネッ トワークは、入力画像の 28 x 

65
00:03:06,019 --> 00:03:10,682
28 ピクセルのそれぞれに対応するニューロンの束から始ま 

66
00:03:10,682 --> 00:03:14,220
り、合計 784 個のニューロンになります。

67
00:03:14,700 --> 00:03:17,878
これらのそれぞれには、黒ピクセルの 0 か 

68
00:03:17,878 --> 00:03:21,057
ら白ピクセルの 1 までの、対応するピクセル

69
00:03:21,057 --> 00:03:24,380
のグレースケール値を表す数値が保持されま す。

70
00:03:25,300 --> 00:03:28,207
ニューロン内のこの数値は活性化と呼ばれ、活

71
00:03:28,207 --> 00:03:31,114
性化の数値が高いと各ニュー ロンが点灯する

72
00:03:31,114 --> 00:03:34,160
というイメージを思い浮かべるかもしれません。

73
00:03:36,720 --> 00:03:39,290
したがって、これら 7 84 個のニューロン

74
00:03:39,290 --> 00:03:41,860
すべてがネットワークの最初の層を構成します。

75
00:03:46,500 --> 00:03:48,582
最後の層に飛びます。 これには 10 個 

76
00:03:48,582 --> 00:03:51,360
のニューロンがあり、それぞれが数字の 1 つを表します。

77
00:03:52,040 --> 00:03:55,223
これらのニューロンの活性化は、 やはり 0 と 

78
00:03:55,223 --> 00:03:58,538
1 の間の数値であり、特定の画像が特定の数字に対応

79
00:03:58,538 --> 00:04:02,120
しているとシステ ムがどの程度考えているかを表します。

80
00:04:03,040 --> 00:04:05,751
間には隠れ層と呼ばれるいくつかの層も 

81
00:04:05,751 --> 00:04:09,176
ありますが、当面はこの数字を認識するプロセスが一

82
00:04:09,176 --> 00:04:12,601
体どのように処理され るのかという大きな疑問符が

83
00:04:12,601 --> 00:04:13,600
付くはずです。

84
00:04:14,260 --> 00:04:16,360
このネットワークでは、それぞれ 16 個の 

85
00:04:16,360 --> 00:04:18,459
ニューロンを持つ 2 つの隠れ層を選択しまし

86
00:04:18,459 --> 00:04:20,560
たが、確かに、これは一種の恣意的な選択です。

87
00:04:21,019 --> 00:04:23,380
正直に言うと、 一瞬のうちに構造をどのように動か

88
00:04:23,380 --> 00:04:25,937
したいかに基づいて 2 つのレイヤーを選択しました。

89
00:04:25,937 --> 00:04:28,200
 16 は、画面に収まるちょうどいい数でした。

90
00:04:28,780 --> 00:04:31,116
実際には、ここには特定の構造を実験する余 

91
00:04:31,116 --> 00:04:32,340
地がたくさんあります。

92
00:04:33,020 --> 00:04:35,750
ネットワークの動作方法では、ある層でのアクティベー

93
00:04:35,750 --> 00:04:38,480
ションが次の層のアク ティベーションを決定します。

94
00:04:39,200 --> 00:04:42,749
そしてもちろん、情報処理メカニズムとしてのネットワーク 

95
00:04:42,749 --> 00:04:45,791
の中心は、ある層からの活性化が次の層の活性化をど

96
00:04:45,791 --> 00:04:48,580
のように引き起こすかということ になります。

97
00:04:49,140 --> 00:04:51,753
これは、ニューロンの生物学的ネットワークにおいて、あ

98
00:04:51,753 --> 00:04:53,863
るニューロンのグループの発火が他のニュー 

99
00:04:53,863 --> 00:04:56,476
ロンの発火を引き起こす方法に大まかに似ていることを意

100
00:04:56,476 --> 00:04:57,180
図しています。

101
00:04:58,120 --> 00:04:59,909
ここで示しているネットワークはすでに数 

102
00:04:59,909 --> 00:05:01,520
字を認識するように訓練されています。

103
00:05:01,520 --> 00:05:03,400
 これが何を意味するのかを説明しましょう。

104
00:05:03,640 --> 00:05:07,097
これは、画像内の 各ピクセルの明るさに応じて入力層の 

105
00:05:07,097 --> 00:05:10,426
784 個のニューロンすべてを点灯する画像を入力す 

106
00:05:10,426 --> 00:05:14,012
ると、その活性化パターンが次の層で非常に特殊なパターンを

107
00:05:14,012 --> 00:05:16,061
引き起こし、その次の層で何らか 

108
00:05:16,061 --> 00:05:18,494
のパターンが発生することを意味します。

109
00:05:18,494 --> 00:05:22,080
 これにより、最終的に出力層にパターンが与えられま す。

110
00:05:22,560 --> 00:05:25,917
そして、その出力層の最も明るいニューロンは、いわば、こ

111
00:05:25,917 --> 00:05:29,400
の画像が表す桁をネットワ ークが選択することになります。

112
00:05:32,560 --> 00:05:35,223
ある層が次の層にどのような影響を与えるか、またはトレ

113
00:05:35,223 --> 00:05:37,886
ーニングがど のように機能するかについての数学に入る

114
00:05:37,886 --> 00:05:40,549
前に、このような層構造がインテリジェントに動作すると

115
00:05:40,549 --> 00:05:43,520
期待する のが合理的である理由について少しお話しましょう。

116
00:05:44,060 --> 00:05:45,220
ここで私たちは何を期待しているのでしょうか？

117
00:05:45,400 --> 00:05:46,471
これらの中間層がやっている 可能性のあ

118
00:05:46,471 --> 00:05:47,600
ることに対する最善の希望は何でしょうか?

119
00:05:48,920 --> 00:05:51,171
さて、あなたまたは私が数字を認識するとき、私た

120
00:05:51,171 --> 00:05:53,520
ちはさまざまなコンポーネ ントを組み合わせます。

121
00:05:54,200 --> 00:05:56,820
A 9 には上部にループがあり、右側にラインがあります。

122
00:05:57,380 --> 00:05:59,036
8 にもトップにループがあります 

123
00:05:59,036 --> 00:06:01,180
が、ローに別のループが組み合わされています。

124
00:06:01,980 --> 00:06:06,820
A 4 は基本的に 3 つの特定の行に分 かれています。

125
00:06:07,600 --> 00:06:09,680
これで完璧な世界では、最後から 2 

126
00:06:09,680 --> 00:06:12,916
番目の層の各ニューロンがこれらのサブコン ポーネントの 

127
00:06:12,916 --> 00:06:15,112
1 つに対応し、たとえば 9 や 8 

128
00:06:15,112 --> 00:06:17,654
などの上部にループがある画像を入力するたび 

129
00:06:17,654 --> 00:06:20,890
に、いくつかのサブコンポーネントが存在することを期待でき

130
00:06:20,890 --> 00:06:23,780
ます。 活性化が 1 に近づく特定のニュ ーロン。

131
00:06:24,500 --> 00:06:26,853
そして、私はこの特定のピクセルのループを意味するの

132
00:06:26,853 --> 00:06:28,924
ではなく、上部に向かう一般的なループ状のパ 

133
00:06:28,924 --> 00:06:31,560
ターンがこのニューロンを引き起こすことを期待しています。

134
00:06:32,440 --> 00:06:34,595
そうすれば、3 番目の層から最後の層 

135
00:06:34,595 --> 00:06:37,090
に進むには、サブコンポーネントのどの組み合わ

136
00:06:37,090 --> 00:06:40,040
せがどの数字に対応するかを学習するだけで 済みます。

137
00:06:41,000 --> 00:06:42,562
もちろん、これは将来の問題を引き起こすだけです。

138
00:06:42,562 --> 00:06:44,385
 なぜなら、これらのサブコンポーネントをどうやって認識 

139
00:06:44,385 --> 00:06:46,012
するのでしょうか、あるいは、適切なサブコンポーネン

140
00:06:46,012 --> 00:06:47,640
トが何であるべきかを知ることさえできるでしょうか?

141
00:06:48,060 --> 00:06:49,700
あるレイヤーが次 のレイヤーにどのような影

142
00:06:49,700 --> 00:06:51,341
響を与えるかについてはまだ話していませんが

143
00:06:51,341 --> 00:06:53,060
、この点について少し一緒に考えてみましょう。

144
00:06:53,680 --> 00:06:56,680
ループを認識すると、サブ問題に分解することもできます。

145
00:06:57,280 --> 00:06:59,299
これを行うための合理的な方法の 1 つは 

146
00:06:59,299 --> 00:07:01,799
、まずそれを構成するさまざまな小さなエッジを認識する

147
00:07:01,799 --> 00:07:02,280
ことです。

148
00:07:02,280 --> 00:07:04,771
同様に、数字の 1、4、または 7 

149
00:07:04,771 --> 00:07:08,784
に見られるような長い線も、実際には単なる長いエッジであるか

150
00:07:08,784 --> 00:07:12,797
、あるいはいくつかの小さなエ ッジからなる特定のパターンと

151
00:07:12,797 --> 00:07:14,320
考えることもできます。

152
00:07:15,140 --> 00:07:17,968
したがって、おそらく私たちの希望は、ネットワーク 

153
00:07:17,968 --> 00:07:20,457
の 2 番目の層の各ニューロンが、関連するさ

154
00:07:20,457 --> 00:07:22,720
まざまな小さなエッジに対応することです。

155
00:07:23,540 --> 00:07:26,750
おそらく、このよう な画像が入力されると、約 8 

156
00:07:26,750 --> 00:07:30,602
～ 10 個の特定の小さなエッジに関連付けられたすべてのニ 

157
00:07:30,602 --> 00:07:33,813
ューロンが点灯し、次に上部のループと長い垂直線に関

158
00:07:33,813 --> 00:07:36,381
連付けられたニューロンが点灯し、それら 

159
00:07:36,381 --> 00:07:39,720
のニューロンが点灯します。 9に関連するニューロン。

160
00:07:40,680 --> 00:07:42,500
これが最終的なネットワークが実際に行うこ 

161
00:07:42,500 --> 00:07:44,666
とであるかどうかは別の問題であり、ネットワークをト

162
00:07:44,666 --> 00:07:47,180
レーニングする方法がわかったら、またこの問題に 戻ります。

163
00:07:47,500 --> 00:07:50,020
しかし、これは私たちが持つかもしれない希望であ

164
00:07:50,020 --> 00:07:52,540
り、このような階層構造を持つ一種の目 標です。

165
00:07:53,160 --> 00:07:56,662
さらに、このようにエッジやパターンを検出できれば、他

166
00:07:56,662 --> 00:08:00,300
の画像認識タ スクにも非常に役立つことが想像できます。

167
00:08:00,880 --> 00:08:04,017
画像認識を超えて、抽象化の層 に分割して実行したい

168
00:08:04,017 --> 00:08:07,280
あらゆる種類のインテリジェントな処理が存在し ます。

169
00:08:08,040 --> 00:08:10,950
たとえば、音声の解析には、生の音声を取り込み、

170
00:08:10,950 --> 00:08:13,860
特定の音節を形成するために 結合したり、単語を

171
00:08:13,860 --> 00:08:16,770
形成したり結合してフレーズやより抽象的な思考を

172
00:08:16,770 --> 00:08:20,060
構成したりする個 別の音を抽出することが含まれます。

173
00:08:21,100 --> 00:08:23,942
しかし、これが実際にどのように機能するかに戻って、ある層 

174
00:08:23,942 --> 00:08:26,882
のアクティベーションが次の層のアクティベーションをどのように

175
00:08:26,882 --> 00:08:28,842
正確に決定するかを今設計している自分を 

176
00:08:28,842 --> 00:08:29,920
想像してみてください。

177
00:08:30,860 --> 00:08:33,487
目標は、ピクセルをエッジに結合したり、エッジ

178
00:08:33,487 --> 00:08:35,397
をパターンに結合したり、パター 

179
00:08:35,397 --> 00:08:38,980
ンを数字に結合したりできる何らかのメカニズムを持つことです。

180
00:08:39,440 --> 00:08:41,924
そして、1 つの非常に具体的な例にズー 

181
00:08:41,924 --> 00:08:44,284
ムインするために、2 番目の層の 1 

182
00:08:44,284 --> 00:08:48,011
つの特定のニューロンが、画像のこの領域にエッジがあるかどう 

183
00:08:48,011 --> 00:08:50,620
かを検出することが期待されているとします。

184
00:08:51,440 --> 00:08:53,270
ここでの問題は、ネットワークにどのよう

185
00:08:53,270 --> 00:08:55,100
なパラメータが必要かとい うことです。

186
00:08:55,640 --> 00:08:57,684
このパターンやその他のピクセル 

187
00:08:57,684 --> 00:09:00,368
パターン、あるいは複数のエッジがループを 

188
00:09:00,368 --> 00:09:03,307
作るパターンなどを潜在的に捉えるのに十分な表現

189
00:09:03,307 --> 00:09:05,735
力を持たせるためには、どのダイヤルや 

190
00:09:05,735 --> 00:09:07,780
ノブを調整すればよいでしょうか?

191
00:09:08,720 --> 00:09:11,886
さて、これから行うことは、ニューロンと最初の層の 

192
00:09:11,886 --> 00:09:15,560
ニューロンの間の接続のそれぞれに重みを割り当てることです。

193
00:09:16,320 --> 00:09:17,700
これらの重みは単なる数値 です。

194
00:09:18,540 --> 00:09:21,958
次に、最初の層からこれらのアクティベーションをすべて取得

195
00:09:21,958 --> 00:09:25,500
し、これらの重みに従って重み付け された合計を計算します。

196
00:09:27,700 --> 00:09:31,417
これらの重みが独自の小さなグリッドに編成されていると考える 

197
00:09:31,417 --> 00:09:34,390
とわかりやすいと思います。 正の重みを示すために

198
00:09:34,390 --> 00:09:36,620
緑のピクセルを使用し、負の重みを示 

199
00:09:36,620 --> 00:09:38,727
すために赤のピクセルを使用します。

200
00:09:38,727 --> 00:09:41,081
 そのピクセルの明るさはある程度です。

201
00:09:41,081 --> 00:09:42,940
 重みの値 のゆるやかな描写。

202
00:09:42,940 --> 00:09:46,788
関心のあるこの領域のいくつかの正の重みを除いて、ほぼすべて 

203
00:09:46,788 --> 00:09:50,508
のピクセルに関連付けられた重みをゼロにすると、すべてのピク

204
00:09:50,508 --> 00:09:54,228
セル値の重み付き 合計を取ることは、実際にはピクセルの値を

205
00:09:54,228 --> 00:09:57,820
合計することになります。 私たちが大 切にしている地域。

206
00:09:59,140 --> 00:10:02,666
ここにエッジがあるかどうかを本当に確認したい場合は 

207
00:10:02,666 --> 00:10:06,600
、周囲のピクセルに負の重みを関連付けることが考えられます。

208
00:10:07,480 --> 00:10:10,019
中央のピ クセルが明るく、周囲のピク

209
00:10:10,019 --> 00:10:12,700
セルが暗い場合、合計は最大になります。

210
00:10:14,260 --> 00:10:17,353
このように加重合計を計算すると、任意の数値が得られる可

211
00:10:17,353 --> 00:10:19,759
能性がありますが、このネットワークで必要 

212
00:10:19,759 --> 00:10:22,165
なのは、アクティベーションが 0 と 1 

213
00:10:22,165 --> 00:10:23,540
の間の値であることです。

214
00:10:24,120 --> 00:10:26,793
したがって、一般的に行うべき ことは、この重み付

215
00:10:26,793 --> 00:10:29,021
けされた合計を、実数直線を 0 と 1 

216
00:10:29,021 --> 00:10:32,140
の間の範囲に押し込む何らかの関数にポンプ することです。

217
00:10:32,460 --> 00:10:34,720
これを行う一般的な関数はシグモイド関数と呼ばれ

218
00:10:34,720 --> 00:10:36,980
、ロジスティック曲線としても知 られています。

219
00:10:36,980 --> 00:10:39,680
基本的に、非常に負の入力は 0 

220
00:10:39,680 --> 00:10:44,405
に近くなり、非常に正の入力は 1 に近くなり、入力 0 

221
00:10:44,405 --> 00:10:46,600
の周りで着実に増加します。

222
00:10:49,120 --> 00:10:51,935
したがって、ここでのニューロンの活性化は 

223
00:10:51,935 --> 00:10:55,555
、基本的に、関連する加重合計がどれだけ正であるかの尺度

224
00:10:55,555 --> 00:10:56,360
になります。

225
00:10:57,540 --> 00:10:59,125
しかし、重み付けされた合計が 0 よ 

226
00:10:59,125 --> 00:11:01,295
り大きいときにニューロンを点灯させたいわけではないか

227
00:11:01,295 --> 00:11:01,880
もしれません。

228
00:11:02,280 --> 00:11:04,469
おそらく、合計が 10 よりも大きい場合に 

229
00:11:04,469 --> 00:11:06,360
のみアクティブにしたい場合があります。

230
00:11:06,840 --> 00:11:10,260
つまり、非アクティブにするために何らかのバイアスが必要です。

231
00:11:11,380 --> 00:11:15,520
次に行うこ とは、シグモイド圧縮関数に接続する前に、この加

232
00:11:15,520 --> 00:11:19,660
重合計にマイナス 10 などの他の数値を加算することです。

233
00:11:20,580 --> 00:11:22,440
この追加の数はバイアスと呼ばれます。

234
00:11:23,460 --> 00:11:26,313
したがっ て、重みは、2 番目の層のこのニューロンがどの

235
00:11:26,313 --> 00:11:29,167
ピクセル パターンを認識しているかを示し、バイアスは、ニ

236
00:11:29,167 --> 00:11:32,020
ュー ロンが意味のあるアクティブになり始める前に、重み付

237
00:11:32,020 --> 00:11:34,874
けされた合計がどのくらい大きくなければならないかを示し 

238
00:11:34,874 --> 00:11:35,180
ます。

239
00:11:36,120 --> 00:11:37,680
そしてそれは単なる 1 つのニューロンです。

240
00:11:38,280 --> 00:11:42,450
この層の 1 つおきのニューロンは、最初の層の 784 

241
00:11:42,450 --> 00:11:46,471
個 のピクセル ニューロンすべてに接続され、これらの 

242
00:11:46,471 --> 00:11:50,940
784 個の接続のそれぞれに独自の重 みが関連付けられます。

243
00:11:51,600 --> 00:11:54,600
また、それぞれには何らかのバイアスがあり、シグモイドで押

244
00:11:54,600 --> 00:11:57,600
しつぶす前 に加重和に加算する他の数値が含まれています。

245
00:11:58,110 --> 00:11:59,540
そして、それは考えるべきことがたくさんあります！

246
00:11:59,960 --> 00:12:04,130
この 16 ニュー ロンの隠れ層では、合計 784 

247
00:12:04,130 --> 00:12:07,980
× 16 の重みと 16 のバイアスになります。

248
00:12:08,840 --> 00:12:10,583
これらはすべて 、第 1 層から第 

249
00:12:10,583 --> 00:12:11,940
2 層への接続にすぎません。

250
00:12:12,520 --> 00:12:14,861
他の層間の接続にも、それ らに関連

251
00:12:14,861 --> 00:12:17,340
する一連の重みとバイアスがあります。

252
00:12:18,340 --> 00:12:21,460
結局のところ、このネットワーク には、合計でほぼ正確に 

253
00:12:21,460 --> 00:12:23,800
13,000 の重みとバイアスがあります。

254
00:12:23,800 --> 00:12:26,177
13,000 個のノブとダイヤルを調整した 

255
00:12:26,177 --> 00:12:29,203
り回して、このネットワークをさまざまな方法で動作させるこ

256
00:12:29,203 --> 00:12:29,960
とができます。

257
00:12:31,040 --> 00:12:33,011
したがって、学習について話すとき 

258
00:12:33,011 --> 00:12:36,373
、それが指しているのは、目前の問題を実際に解決できるように

259
00:12:36,373 --> 00:12:38,345
、これらの多くの数値すべてに対し 

260
00:12:38,345 --> 00:12:41,360
て有効な設定をコンピューターに見つけさせることです。

261
00:12:42,620 --> 00:12:45,504
楽しくもあり、ある意味恐ろしくもある 思考実験の 

262
00:12:45,504 --> 00:12:48,273
1 つは、座ってこれらの重みとバイアスをすべて手

263
00:12:48,273 --> 00:12:51,388
動で設定し、2 番目の レイヤーがエッジを認識し、3 

264
00:12:51,388 --> 00:12:54,618
番目のレイヤーがパターンを認識するように数値を意図的に 

265
00:12:54,618 --> 00:12:56,580
微調整することを想像することです。

266
00:12:56,980 --> 00:12:59,740
等私個人としては、ネットワークを完全なブラック ボ 

267
00:12:59,740 --> 00:13:02,713
ックスとして扱うよりも、これで満足できると感じています。

268
00:13:02,713 --> 00:13:04,412
 なぜなら、ネットワークが期待 

269
00:13:04,412 --> 00:13:07,278
どおりに動作しないとき、それらの重みやバイアスが実際に

270
00:13:07,278 --> 00:13:10,357
何を意味するのかについて少しでも 関係を構築していれば、 

271
00:13:10,357 --> 00:13:13,224
、改善するために構造を変更する方法を実験するための出発

272
00:13:13,224 --> 00:13:14,180
点が得 られます。

273
00:13:14,960 --> 00:13:17,675
あるいは、ネットワークが機能しているものの、予想どおり

274
00:13:17,675 --> 00:13:20,390
の理由ではない場合、重 みとバイアスが何をしているのか

275
00:13:20,390 --> 00:13:23,306
を掘り下げることは、自分の仮定に疑問を投げかけ、考えられ 

276
00:13:23,306 --> 00:13:25,820
る解決策の全領域を実際に明らかにする良い方法です。

277
00:13:26,840 --> 00:13:30,680
ところで、ここで実際の関 数を書くのは少し面倒ですよね。

278
00:13:32,500 --> 00:13:34,683
そこで、これらの接続をよりコン 

279
00:13:34,683 --> 00:13:37,140
パクトに表記する方法を示しましょう。

280
00:13:37,660 --> 00:13:39,447
ニューラル ネットワークについてさらに詳しく読ん 

281
00:13:39,447 --> 00:13:40,520
でみると、次のようになります。

282
00:13:41,380 --> 00:13:42,861
1 つのレイヤーのすべてのアクティベー

283
00:13:42,861 --> 00:13:44,420
ションをベクトルとして列に編成 します。

284
00:13:44,420 --> 00:13:50,053
次に、すべての重みを行列として編成します。

285
00:13:50,053 --> 00:13:57,833
 行列の各行は、あ る層と次の層の特定のニューロンの間の接

286
00:13:57,833 --> 00:13:59,980
続に対応します。

287
00:13:59,980 --> 00:14:04,753
これが意味 するのは、これらの重みに従って最初の層のアクテ

288
00:14:04,753 --> 00:14:07,386
ィベーションの重み付き合計を取 

289
00:14:07,386 --> 00:14:11,831
得すると、左側にあるすべての行列ベクトル積の項の 1 

290
00:14:11,831 --> 00:14:14,300
つに対応するということ です。

291
00:14:14,660 --> 00:14:19,108
ちなみに、機械学習の多くは線形代数をよく理解することに尽き 

292
00:14:19,108 --> 00:14:22,519
るので、行列と行列ベクトルの乗算の意味を視覚的

293
00:14:22,519 --> 00:14:26,523
に理解したい人は、 私が行ったシリーズを見てください。

294
00:14:26,523 --> 00:14:28,600
 線形代数、特に第 3 章。

295
00:14:29,240 --> 00:14:32,409
式に戻 ると、これらの値のそれぞれに独立してバイア

296
00:14:32,409 --> 00:14:35,199
スを追加することについて話すのではなく、こ 

297
00:14:35,199 --> 00:14:38,369
れらすべてのバイアスをベクトルに編成し、ベクトル全

298
00:14:38,369 --> 00:14:40,524
体を前の行列ベクトル積に追加する 

299
00:14:40,524 --> 00:14:42,300
ことによってそれを表します。

300
00:14:43,280 --> 00:14:46,815
次に、最後のステップとして、ここで外側にシグモイドを巻き 

301
00:14:46,815 --> 00:14:49,619
付けます。 これが表すことは、結果として得られ

302
00:14:49,619 --> 00:14:52,057
るベクトルの内部の特定の各コンポーネン 

303
00:14:52,057 --> 00:14:54,740
トにシグモイド関数を適用することになります。

304
00:14:55,940 --> 00:14:58,484
したがって、この重み行列とこれらのベク 

305
00:14:58,484 --> 00:15:01,665
トルを独自のシンボルとして書き留めると、ある層から

306
00:15:01,665 --> 00:15:03,828
次の層へのアクティベーションの完 

307
00:15:03,828 --> 00:15:07,008
全な移行を非常に厳密できちんとした小さな式で伝える

308
00:15:07,008 --> 00:15:09,171
ことができ、これにより関連するコ 

309
00:15:09,171 --> 00:15:12,352
ードがはるかに単純になり、多くのライブラリが行列の

310
00:15:12,352 --> 00:15:15,660
乗算を最適化しているため、はるかに 高速になります。

311
00:15:17,820 --> 00:15:19,597
先ほど、これらのニューロンは単に数字を保持

312
00:15:19,597 --> 00:15:21,460
するものであると述べたことを覚えていますか?

313
00:15:22,220 --> 00:15:26,141
もちろん、それらが保持する具体的な数値は、入力した画像

314
00:15:26,141 --> 00:15:30,062
によって異なり ます。 そのため、実際には、各ニューロ

315
00:15:30,062 --> 00:15:32,676
ンを関数として考える方が正確で す。

316
00:15:32,676 --> 00:15:36,742
 関数は、前の層のすべてのニューロンの出力を受け取り、 

317
00:15:36,742 --> 00:15:38,340
0から1ま での数値。

318
00:15:39,200 --> 00:15:42,421
実際、ネットワーク全体は単なる関数であり、784 

319
00:15:42,421 --> 00:15:44,869
個の数値を入力として受 け取り、10 

320
00:15:44,869 --> 00:15:47,060
個の数値を出力として吐き出します。

321
00:15:47,560 --> 00:15:49,857
これは途方もなく複雑な関数であ 

322
00:15:49,857 --> 00:15:53,592
り、特定のパターンを検出する重みとバイアスの形式で 

323
00:15:53,592 --> 00:15:57,326
13,000 のパラメーターが含まれ、多くの行列ベク

324
00:15:57,326 --> 00:16:01,060
トル積とシグモイド潰し関数 の反復が含まれますが、そ

325
00:16:01,060 --> 00:16:02,640
れでも単なる関数です。

326
00:16:03,400 --> 00:16:05,030
そして、それが複雑に見える こと

327
00:16:05,030 --> 00:16:06,660
は、ある意味、安心感を与えます。

328
00:16:07,340 --> 00:16:09,853
つまり、もしそれがもっと単純だったら、数字を認識するとい 

329
00:16:09,853 --> 00:16:12,280
う課題に挑戦できるなんて、どんな希望が持てるでしょうか?

330
00:16:13,340 --> 00:16:14,700
そして、その課題にどのように取り組むのでしょうか?

331
00:16:15,080 --> 00:16:17,220
このネットワークは、データを見るだけで適切な重

332
00:16:17,220 --> 00:16:19,360
みとバイアスをどのように学習するのでしょうか?

333
00:16:20,140 --> 00:16:21,776
それについては次のビデオで説明します。

334
00:16:21,776 --> 00:16:24,275
 また、この特定のネットワークが実際に何をしているのかに 

335
00:16:24,275 --> 00:16:25,740
ついてももう少し詳しく説明します。

336
00:16:25,740 --> 00:16:28,602
ここで重要なのは、そのビデオや新しいビデオが公開さ

337
00:16:28,602 --> 00:16:31,465
れたときの 通知を受け取るために購読すると言うべき

338
00:16:31,465 --> 00:16:34,328
だと思いますが、現実的には、ほとんどの人が実際に 

339
00:16:34,328 --> 00:16:37,420
Yo uTube からの通知を受け取っていませんよね。

340
00:16:38,020 --> 00:16:40,105
おそらくもっと正直に言うと、YouTube 

341
00:16:40,105 --> 00:16:42,096
の推奨アルゴリズムの基礎をなすニューラル 

342
00:16:42,096 --> 00:16:44,751
ネットワークが、ユーザーがこのチャンネルのコンテンツを 

343
00:16:44,751 --> 00:16:47,216
推奨されると信じ込むように、チャンネル登録するという

344
00:16:47,216 --> 00:16:47,880
べきでしょう。

345
00:16:48,560 --> 00:16:49,940
とにかく、続報をお待ちください。

346
00:16:50,760 --> 00:16:52,130
Patreon でこれらのビデオをサポート

347
00:16:52,130 --> 00:16:53,500
してくださった皆様に心より感謝いたします。

348
00:16:54,000 --> 00:16:55,420
この夏は確率シリーズの進歩が少 

349
00:16:55,420 --> 00:16:57,994
し遅れていましたが、このプロジェクトの後はまた本格的に取り

350
00:16:57,994 --> 00:16:59,858
組むつもりですので、パトロンの皆様はそこ 

351
00:16:59,858 --> 00:17:01,900
で最新情報をチェックしていただければ幸いです。

352
00:17:03,600 --> 00:17:05,991
最後に、深層学習の理論面で博士号の研究をし、 

353
00:17:05,991 --> 00:17:08,382
現在はこのビデオに資金の一部を提供してくれた 

354
00:17:08,382 --> 00:17:11,189
Amplify Partners と いうベンチャー 

355
00:17:11,189 --> 00:17:13,788
キャピタル会社で働いている Leesha Lee 

356
00:17:13,788 --> 00:17:14,619
に話を聞きます。

357
00:17:15,460 --> 00:17:17,290
そこで、リーシャ、すぐに取り上げるべき

358
00:17:17,290 --> 00:17:19,119
だと思うのが、このシグモイド関数です。

359
00:17:19,700 --> 00:17:21,711
私が理解しているところによると、初期のネットワーク

360
00:17:21,711 --> 00:17:23,643
はこれを使用して、関連する重み付けされた合計を 

361
00:17:23,643 --> 00:17:25,172
0 と 1 の間の区間に押し込みます。

362
00:17:25,172 --> 00:17:27,184
 ニューロンが非アクティブまたはアクティブであると

363
00:17:27,184 --> 00:17:29,196
いう生物学的な類似によって動 機づけられているのは

364
00:17:29,196 --> 00:17:29,840
ご存知でしょう。

365
00:17:30,280 --> 00:17:30,300
その通り。

366
00:17:30,560 --> 00:17:33,307
しかし、実際にシグモイドを使用している現代のネットワークは 

367
00:17:33,307 --> 00:17:34,040
比較的少数です。

368
00:17:34,320 --> 00:17:34,320
うん。

369
00:17:34,440 --> 00:17:35,540
それはちょっと古い学校ですよね？

370
00:17:35,760 --> 00:17:38,580
そうですね、むしろレルのほうが訓練しや すいようです。

371
00:17:38,580 --> 00:17:42,340
そして、relu、reluは整流リニアユニットの略ですか？

372
00:17:42,680 --> 00:17:45,426
はい、これはこの種の関数 で、最大ゼロと 

373
00:17:45,426 --> 00:17:47,911
a を取得するだけです。 ここで、a 

374
00:17:47,911 --> 00:17:50,920
はビデオで説明した内容によって与え られます。

375
00:17:50,920 --> 00:17:54,345
これは、ニューロンがどのように活性化される

376
00:17:54,345 --> 00:17:57,281
か活性化されないかに関する生物学的 

377
00:17:57,281 --> 00:18:01,360
な類似から部分的に動機付けられたものだと思います。

378
00:18:01,360 --> 00:18:03,842
したがって、特定のしきい値を超えた場 

379
00:18:03,842 --> 00:18:06,455
合は恒等関数になりますが、超えなかった場

380
00:18:06,455 --> 00:18:09,460
合はアクティブ化されないため、ゼロになります。

381
00:18:09,460 --> 00:18:10,840
つまり、一種の簡略化です。

382
00:18:11,160 --> 00:18:14,525
シグモイドの使用はトレーニングに役立たなかったり、ある時点で

383
00:18:14,525 --> 00:18:17,889
トレ ーニングが非常に困難になったりしていましたが、人々は 

384
00:18:17,889 --> 00:18:21,142
relu を試してみたところ、たまたまこれらの信じられな 

385
00:18:21,142 --> 00:18:24,507
いほど深いニューラル ネットワークで非常にうまく機能しました

386
00:18:24,507 --> 00:18:24,620
。

387
00:18:25,100 --> 00:18:25,640
わかりました、リーシャ、ありがとう。

