1
00:00:00,000 --> 00:00:11,200
这是一个3。它以 28x28 像素的极低分辨率编写和渲染，但你的

2
00:00:11,200 --> 00:00:15,340
大脑可以毫不费力地将其识别为 3。我希望你能花点时间体会

3
00:00:15,340 --> 00:00:20,500
一下大脑可以如此轻松地做到这一点是多么疯狂。我的意思是

4
00:00:20,500 --> 00:00:26,180
，这个、这个和这个也可以被识别为 3 秒，尽管每个像素的具

5
00:00:26,180 --> 00:00:31,260
体值与下一个图像有很大不同。当您看到这 3 时，您眼睛中

6
00:00:31,260 --> 00:00:36,020
发射的特定光敏细胞与您看到这 3 时发射的光敏细胞非常不

7
00:00:36,020 --> 00:00:42,900
同。但你那疯狂聪明的视觉皮层中的某些东西将这些图像解析为

8
00:00:42,900 --> 00:00:49,300
代表相同的想法，同时将其他图像识别为它们自己独特的想法。

9
00:00:49,300 --> 00:00:55,820
但如果我告诉你，嘿，坐下来为我写一个程序，它接受 28x28 的网

10
00:00:56,340 --> 00:01:01,780
格并输出 0 到 10 之间的单个数字，告诉你它认为这个数字是什

11
00:01:01,780 --> 00:01:07,860
么，那么这个任务就会从可笑的琐碎变成极其困难。除非你一直生活在

12
00:01:07,860 --> 00:01:12,020
岩石下，否则我认为我几乎不需要激发机器学习和神经网络对

13
00:01:12,020 --> 00:01:16,460
于现在和未来的相关性和重要性。但我在这里想做的是在没有背景的情

14
00:01:16,460 --> 00:01:22,020
况下向您展示神经网络实际上是什么，并帮助您可视化它正在做什么，而不是作

15
00:01:22,060 --> 00:01:26,860
为一个流行语，而是作为一段数学。我的希望只是，当你读完或听到神

16
00:01:26,860 --> 00:01:31,460
经网络引用-非引用学习时，你会感觉结构本身是有动机的

17
00:01:31,460 --> 00:01:36,780
，并且感觉你知道它意味着什么。本视频将专门讨论

18
00:01:36,780 --> 00:01:40,300
其结构部分，下面的视频将讨论学习问题。

19
00:01:40,300 --> 00:01:45,580
我们要做的就是建立一个可以学习识别手写数字的神经网络

20
00:01:45,580 --> 00:01:53,540
。这是介绍该主题的一个有点经典的示例，我很高兴在这里坚持

21
00:01:53,540 --> 00:01:57,340
现状，因为在两个视频的末尾，我想向您指出一些很好的资源，您

22
00:01:57,340 --> 00:02:01,420
可以在其中了解更多信息，以及您可以下载执行此操作的代码并在

23
00:02:01,420 --> 00:02:07,820
您自己的计算机上使用它。神经网络有很多变体，近年来，

24
00:02:07,820 --> 00:02:12,900
对这些变体的研究蓬勃发展，但在这两个介绍性视频

25
00:02:12,940 --> 00:02:18,100
中，你和我将只看最简单的普通形式，没有任何额外的

26
00:02:18,100 --> 00:02:23,020
装饰。这是理解任何更强大的现代变体的必要先决

27
00:02:23,020 --> 00:02:28,140
条件，相信我，它仍然有很多复杂性需要我们去思考

28
00:02:28,140 --> 00:02:33,440
。但即使是这种最简单的形式，它也可以学习识别手写数字，这对

29
00:02:33,440 --> 00:02:39,380
于计算机来说是一件非常酷的事情。同时你会发现它确实没

30
00:02:39,460 --> 00:02:45,620
有达到我们的一些希望。顾名思义，神经网络受到大脑

31
00:02:45,620 --> 00:02:50,820
的启发，但让我们来分解一下。什么是神经元，它们以什么方式连接

32
00:02:50,820 --> 00:02:56,900
在一起？现在，当我说神经元时，我想让你想到的是一个包含数字的东西

33
00:02:56,900 --> 00:03:04,380
，特别是 0 到 1 之间的数字。确实仅此而已。例如，网络

34
00:03:04,420 --> 00:03:10,060
从对应于输入图像的 28×28 像素中的每一个像素的一堆神经元

35
00:03:10,060 --> 00:03:17,260
开始，总共 784 个神经元。其中每一个都包含一个数字，表示

36
00:03:17,260 --> 00:03:23,900
相应像素的灰度值，范围从黑色像素的 0 到白色像素的 1

37
00:03:23,900 --> 00:03:30,060
。神经元内部的这个数字称为它的激活值，您可能想到的图像是

38
00:03:30,060 --> 00:03:37,260
，当每个神经元的激活值很高时，它就会被点亮。因此，所有这

39
00:03:37,260 --> 00:03:47,820
784 个神经元构成了我们网络的第一层。现在跳到最后一层，它有 1

40
00:03:47,820 --> 00:03:53,780
0 个神经元，每个神经元代表一个数字。这些神经元的激活（同

41
00:03:53,780 --> 00:03:59,460
样是 0 到 1 之间的某个数字）表示系统认为给定图

42
00:03:59,500 --> 00:04:05,180
像与给定数字的对应程度。中间还有一些称为隐藏层的

43
00:04:05,180 --> 00:04:10,780
层，目前这应该只是一个巨大的问号，不知道如

44
00:04:10,780 --> 00:04:15,900
何处理这个数字识别过程。在这个网络中，我选择了两个隐

45
00:04:15,900 --> 00:04:21,460
藏层，每个隐藏层有 16 个神经元，诚然，这是一种任意选择。说实话，我

46
00:04:21,460 --> 00:04:26,620
选择了两层，基于我想如何在短时间内激发结构，而 16 层，

47
00:04:26,620 --> 00:04:30,940
这只是一个适合屏幕的数字。实际上，这里有很大的空间可以对特

48
00:04:30,940 --> 00:04:37,020
定结构进行实验。网络运行的方式，一层的激活决定下

49
00:04:37,020 --> 00:04:42,340
一层的激活。当然，作为一种信息处理机制，网

50
00:04:42,340 --> 00:04:47,820
络的核心归结为一层的激活如何引起下一层的

51
00:04:47,820 --> 00:04:53,340
激活。它大致类似于神经元生物网络中某些神经元群的放

52
00:04:53,380 --> 00:04:59,380
电导致某些其他神经元放电的方式。现在我在这里展示的网络已

53
00:04:59,380 --> 00:05:04,260
经接受过识别数字的训练，让我向您展示我的意思。这意味着，如

54
00:05:04,260 --> 00:05:10,900
果您输入一张图像，根据图像中每个像素的亮度照亮输入层的所有

55
00:05:10,900 --> 00:05:16,860
784 个神经元，则该激活模式会在下一层中导致一些非常特定的

56
00:05:16,860 --> 00:05:21,740
模式，从而在下一层中导致一些模式它最终在输出层中给出了一些模式

57
00:05:21,780 --> 00:05:27,540
。输出层最亮的神经元是网络的选择，可以说，该图

58
00:05:27,540 --> 00:05:35,420
像代表什么数字。在深入了解一层如何影响下一层或训

59
00:05:35,420 --> 00:05:40,460
练如何进行之前，我们先来谈谈为什么期望这样的分层结构

60
00:05:40,460 --> 00:05:46,340
能够智能地运行是合理的。我们在这里期待什么？这些中间层可能

61
00:05:46,420 --> 00:05:52,420
做的事情的最大希望是什么？好吧，当你或我识别数字时，我们会将各种组件

62
00:05:52,420 --> 00:05:58,980
拼凑在一起。9 的顶部有一个环，右侧有一条线。8 也有一个顶部环，

63
00:05:58,980 --> 00:06:05,420
但它与另一个底部环配对。4 基本上可以分为三行，诸

64
00:06:05,420 --> 00:06:11,500
如此类。现在在一个完美的世界中，我们可能希望倒数第二层中的

65
00:06:11,740 --> 00:06:17,460
每个神经元都与这些子组件之一相对应，每当您输入带有顶部循环（例

66
00:06:17,460 --> 00:06:23,060
如 9 或 8）的图像时，都会有一些其激活值接近 1 的特定神

67
00:06:23,060 --> 00:06:28,620
经元。我并不是指这种特定的像素循环，而是希望任何朝向顶部

68
00:06:28,620 --> 00:06:33,980
的普遍循环模式都会引发该神经元。这样，从第三层到最

69
00:06:33,980 --> 00:06:39,380
后一层只需要学习哪个子组件组合对应于哪个数

70
00:06:39,380 --> 00:06:44,020
字。当然，这只是解决问题，因为您如何识别这些子

71
00:06:44,020 --> 00:06:48,340
组件，甚至如何了解正确的子组件应该是什么？我什至还

72
00:06:48,340 --> 00:06:52,900
没有讨论一层如何影响下一层，但请跟我一起讨论一下这一层。

73
00:06:52,900 --> 00:06:59,020
识别循环也可以分解为子问题。一种合理的方法是首

74
00:06:59,020 --> 00:07:05,640
先识别构成它的各种小边缘。类似地，一条长线，就像您

75
00:07:05,640 --> 00:07:11,280
在数字 1、4 或 7 中看到的那种，那实际上只是一条长边，或者

76
00:07:11,280 --> 00:07:18,440
您可能将其视为几个较小边的某种图案。因此，也许我们希望网络第二

77
00:07:18,440 --> 00:07:24,680
层中的每个神经元都与各种相关的小边相对应。也许当像这样的

78
00:07:24,680 --> 00:07:30,760
图像出现时，它会照亮与大约 8 到 10 个特定小边缘相关的所有

79
00:07:31,040 --> 00:07:36,480
神经元，这些边缘又会照亮与上环和一条长垂直线相关的神经元，而这些

80
00:07:36,480 --> 00:07:41,960
神经元又会照亮与 9 相关的神经元。这是否是我们最终网络实际上

81
00:07:41,960 --> 00:07:46,560
所做的事情是另一个问题，一旦我们了解如何训练网络，我就会回到这个

82
00:07:46,560 --> 00:07:51,800
问题。但这是我们可能拥有的一个希望，一种像这样的分层结构的目

83
00:07:51,800 --> 00:07:57,440
标。此外，您可以想象能够检测这样的边缘和图案对于

84
00:07:57,480 --> 00:08:02,440
其他图像识别任务将非常有用。甚至除了图像识别之外，

85
00:08:02,440 --> 00:08:06,640
您可能还想做各种各样的智能事情，这些事情可以分解为抽象

86
00:08:06,640 --> 00:08:12,640
层。例如，解析语音涉及获取原始音频并挑选出不同的声音，这些声音

87
00:08:12,640 --> 00:08:17,760
结合起来形成某些音节，这些音节结合起来形成单词，这些声音结合起来

88
00:08:17,760 --> 00:08:23,360
组成短语和更抽象的想法等等。但回到这些实际上是如何工作的，想

89
00:08:23,400 --> 00:08:29,160
象一下你自己现在正在设计一层中的激活如何准确地确定下一

90
00:08:29,160 --> 00:08:35,320
层中的激活。我们的目标是拥有某种机制，可以将像素组合成边缘，或者

91
00:08:35,320 --> 00:08:41,040
将边缘组合成图案，或者将图案组合成数字。放大一个非常具体的示

92
00:08:41,040 --> 00:08:47,440
例，假设希望第二层中的一个特定神经元能够识别图像在

93
00:08:47,680 --> 00:08:54,440
此区域中是否有边缘。当前的问题是，网络应该具有哪些

94
00:08:54,440 --> 00:09:00,440
参数？您应该能够调整哪些转盘和旋钮，以便具有足够的表现力来潜在

95
00:09:00,440 --> 00:09:05,880
地捕捉这种图案，或任何其他像素图案，或多个边缘可以形成循环的图

96
00:09:05,880 --> 00:09:11,680
案，以及其他类似的东西？好吧，我们要做的就是为我们的神经元和

97
00:09:11,680 --> 00:09:17,160
第一层神经元之间的每个连接分配一个权重。这些重量只是数

98
00:09:17,160 --> 00:09:23,960
字。然后从第一层获取所有这些激活并根据这些权重计算它

99
00:09:23,960 --> 00:09:30,400
们的加权和。我发现将这些权重视为被组织成自己的小网

100
00:09:30,400 --> 00:09:35,200
格是有帮助的，我将使用绿色像素来表示正权重，使用红

101
00:09:35,200 --> 00:09:40,760
色像素来表示负权重，其中该像素的亮度是一些对重量值的

102
00:09:40,760 --> 00:09:45,880
宽松描述。如果我们将与几乎所有像素相关的权重设为零（

103
00:09:45,880 --> 00:09:51,200
除了我们关心的该区域中的一些正权重），那么对所有像素

104
00:09:51,200 --> 00:09:56,360
值进行加权和实际上相当于将刚刚在其中的像素值相加。我们

105
00:09:56,360 --> 00:10:02,760
关心的地区。如果您确实想知道这里是否有边缘，您

106
00:10:02,760 --> 00:10:07,960
可能会做的是与周围像素相关的一些负权重。当中间

107
00:10:08,000 --> 00:10:12,680
的像素较亮而周围的像素较暗时，总和最大。

108
00:10:12,680 --> 00:10:19,200
当你计算这样的加权和时，你可能会得到任何数字，但对于这个网络，我

109
00:10:19,200 --> 00:10:25,200
们想要的是激活值是 0 到 1 之间的某个值。因此，常见的做法是

110
00:10:25,200 --> 00:10:30,560
将这个加权和注入某个函数，将实数轴压缩到 0 到 1 之间的

111
00:10:30,560 --> 00:10:36,360
范围内。执行此操作的常见函数称为 sigmoid 函数，也称为

112
00:10:36,360 --> 00:10:42,760
逻辑曲线。基本上，非常负的输入最终接近 0，非常正的输入最终接近

113
00:10:42,760 --> 00:10:51,400
1，并且它只是在输入 0 附近稳定增加。所以这里神经元的激活

114
00:10:51,400 --> 00:10:59,320
基本上是衡量相关加权和的正值程度。但也许你并不希望

115
00:10:59,320 --> 00:11:04,080
神经元在加权和大于0时亮起。也许您只希望当总和大于 1

116
00:11:04,120 --> 00:11:11,520
0 时它才处于活动状态。也就是说，您希望对其不活动有一些偏差。然后我们

117
00:11:11,520 --> 00:11:17,560
要做的就是在这个加权和中添加一些其他数字，比如负 10，然后将其插

118
00:11:17,560 --> 00:11:23,840
入 sigmoid 压缩函数。这个额外的数字称为偏差。因此

119
00:11:23,840 --> 00:11:29,080
，权重告诉您第二层中的该神经元正在接收什么像素模式，而偏

120
00:11:29,120 --> 00:11:34,640
差则告诉您在神经元开始有意义地活动之前，加权和需要多高

121
00:11:34,640 --> 00:11:41,760
。那只是一个神经元。该层中的每个其他神经元都将连接到第一层的所

122
00:11:41,760 --> 00:11:49,080
有 784 个像素神经元，并且这 784 个连接中的每个连接都有其

123
00:11:49,080 --> 00:11:55,320
自己的关联权重。此外，每个值都有一些偏差，即在用 sigmoid 压缩

124
00:11:55,320 --> 00:12:00,600
之前将其添加到加权和中的其他数字。这需要考虑很多！对于这个包含 16

125
00:12:00,600 --> 00:12:09,280
个神经元的隐藏层，总共有 784 个权重乘以 16 个权重，以及 16 个偏差。所有这些

126
00:12:09,280 --> 00:12:13,760
只是从第一层到第二层的连接。其他层之间的连接

127
00:12:13,760 --> 00:12:19,600
也有一堆与之相关的权重和偏差。总而言之，这个网络

128
00:12:19,600 --> 00:12:26,680
几乎有 13,000 个总权重和偏差。13,000 个旋钮和转盘可

129
00:12:26,680 --> 00:12:32,400
以进行调整和转动，以使该网络以不同的方式运行。因此，当我们谈论学

130
00:12:32,400 --> 00:12:38,440
习时，指的是让计算机为所有这些许多数字找到有效的设

131
00:12:38,440 --> 00:12:44,400
置，以便真正解决手头的问题。一个既有趣又有点可怕

132
00:12:44,400 --> 00:12:49,440
的思想实验是想象坐下来手动设置所有这些权重和偏差

133
00:12:49,440 --> 00:12:53,960
，有目的地调整数字，以便第二层拾取边缘，第三层

134
00:12:53,960 --> 00:12:59,680
拾取模式， ETC。我个人认为这令人满意，而不仅仅

135
00:12:59,680 --> 00:13:04,400
是将网络视为一个完全的黑匣子，因为当网络没有按照您预

136
00:13:04,400 --> 00:13:09,040
期的方式运行时，如果您已经与这些权重和偏差的实际含义

137
00:13:09,040 --> 00:13:13,440
建立了一些关系，您有一个开始尝试如何改变结构以进行

138
00:13:13,440 --> 00:13:17,680
改进。或者，当网络确实工作，但不是出于您可能期望的原因

139
00:13:17,680 --> 00:13:22,760
时，深入研究权重和偏差的作用是挑战您的假设并真正揭示可

140
00:13:22,760 --> 00:13:28,560
能解决方案的全部空间的好方法。顺便说一句，这里的实际功

141
00:13:28,560 --> 00:13:34,840
能写起来有点麻烦，你不觉得吗？因此，让我向您展示一种更

142
00:13:34,840 --> 00:13:39,200
紧凑的表示这些连接的方式。如果您选择阅读更多有关神经网络的内容

143
00:13:39,200 --> 00:13:45,360
，您会看到这样的结果。将一层的所有激活组织为一列作为向量

144
00:13:45,480 --> 00:13:53,400
。然后将所有权重组织为一个矩阵，其中该矩阵的每一

145
00:13:53,400 --> 00:13:58,680
行对应于一层与下一层中特定神经元之间的连接。这意味

146
00:13:58,680 --> 00:14:03,360
着根据这些权重求第一层激活的加权和对应

147
00:14:03,360 --> 00:14:08,880
于我们左边所有内容的矩阵向量乘积中的一项

148
00:14:08,880 --> 00:14:17,840
。顺便说一句，机器学习的大部分内容都归结为对线性代数的良好

149
00:14:17,840 --> 00:14:23,000
掌握，因此对于任何想要对矩阵以及矩阵向量乘法的含义有很好的视

150
00:14:23,000 --> 00:14:29,320
觉理解的人，请看一下我所做的系列线性代数，特别是第三章。回到

151
00:14:29,320 --> 00:14:34,200
我们的表达式，我们不是谈论将偏差独立地添加到这些值中的每一个，而

152
00:14:34,200 --> 00:14:40,440
是通过将所有这些偏差组织到一个向量中，并将整个向量添加到前一个矩

153
00:14:40,440 --> 00:14:47,240
阵向量乘积来表示它。然后，作为最后一步，我将在此处的外部包裹一

154
00:14:47,240 --> 00:14:51,480
个 sigmoid 函数，这应该表示您要将 sigmoid 函数

155
00:14:51,480 --> 00:14:58,120
应用于内部结果向量的每个特定组件。因此，一旦你写下这个权重矩

156
00:14:58,120 --> 00:15:03,320
阵和这些向量作为它们自己的符号，你就可以用一个非常紧凑和整

157
00:15:03,480 --> 00:15:08,840
洁的小表达式来传达从一层到下一层的激活的完整转换，这使得相

158
00:15:08,840 --> 00:15:14,600
关代码变得更加简单和方便。速度要快得多，因为许多库都优化

159
00:15:14,600 --> 00:15:21,400
了矩阵乘法。还记得我之前说过这些神经元只是保存数字的东西吗？

160
00:15:22,120 --> 00:15:26,280
当然，它们保存的具体数字取决于您输入的图像，因此将

161
00:15:28,120 --> 00:15:31,960
每个神经元视为一个函数实际上更准确，该函数接收前一

162
00:15:31,960 --> 00:15:37,240
层中所有神经元的输出，并吐出一个0 到 1 之间

163
00:15:37,240 --> 00:15:43,800
的数字。实际上，整个网络只是一个函数，它接受 784 个数字作为

164
00:15:43,800 --> 00:15:49,720
输入，并输出 10 个数字作为输出。这是一个极其复杂的函数，涉及

165
00:15:49,720 --> 00:15:54,520
13,000 个参数，这些参数以这些权重和偏差的形式出现，

166
00:15:54,520 --> 00:15:59,000
并根据某些模式进行选取，并且涉及迭代许多矩阵向量乘积和 si

167
00:15:59,000 --> 00:16:04,760
gmoid 压缩函数，但它仍然只是一个函数。从某种程度上来说，它

168
00:16:04,760 --> 00:16:09,720
看起来很复杂，这让人感到安心。我的意思是，如果它再简单一点，我们对它

169
00:16:09,720 --> 00:16:14,920
能够应对识别数字的挑战还有什么希望呢？它如何应对这一挑战？

170
00:16:14,920 --> 00:16:19,320
该网络如何仅通过查看数据来学习适当的权重和偏差？

171
00:16:19,880 --> 00:16:23,960
这就是我将在下一个视频中展示的内容，我还将进一步深入了解这个

172
00:16:23,960 --> 00:16:29,880
特定网络的真正作用。现在我想我应该说订阅以便在该视频或任何新视

173
00:16:29,880 --> 00:16:34,840
频发布时随时收到通知，但实际上你们中的大多数人实际上并没有收到来

174
00:16:34,840 --> 00:16:39,880
自 YouTube 的通知，是吗？也许更诚实地说，我应该说

175
00:16:39,880 --> 00:16:44,920
订阅，这样 YouTube 推荐算法的神经网络就会相信

176
00:16:44,920 --> 00:16:49,800
您希望看到该频道的内容被推荐给您。无论如何，请继续关注更多信息。

177
00:16:50,600 --> 00:16:54,840
非常感谢 Patreon 上支持这些视频的所有人。今年夏天我在概率

178
00:16:54,840 --> 00:16:59,160
系列上的进展有点慢，但在这个项目之后我会重新开始，所以

179
00:16:59,160 --> 00:17:05,640
顾客们可以在那里留意更新。最后，我邀请 Leesha Lee

180
00:17:05,640 --> 00:17:09,880
进行深度学习理论方面的博士研究，目前在一家名为 Amplify P

181
00:17:09,880 --> 00:17:14,520
artners 的风险投资公司工作，该公司为该视频提供了部分资金。

182
00:17:15,160 --> 00:17:19,480
Leesha，我认为我们应该快速提出的一件事是这个 sigmoid 函数。

183
00:17:19,480 --> 00:17:23,400
据我了解，早期网络使用它来将相关加权和压缩到零和一

184
00:17:23,400 --> 00:17:28,200
之间的区间，你知道这种生物类比的动机是神经元要么不

185
00:17:28,200 --> 00:17:33,240
活跃，要么活跃。确切地。但真正使用 sigmoid 的现代网络

186
00:17:33,240 --> 00:17:37,800
相对较少。是的。这有点老派吧？是的，或者更确切地说，relu 似

187
00:17:37,800 --> 00:17:43,880
乎更容易训练。还有relu，relu代表整流线性单元？是的，在这种函数

188
00:17:43,880 --> 00:17:50,280
中，您只需取最大值为零，而 a 则由您在视频中解释的内容

189
00:17:50,280 --> 00:17:56,440
给出。我认为这部分是出于对神经元如何激活

190
00:17:56,440 --> 00:18:03,640
或不激活的生物学类比。因此，如果它通过了某个阈值，

191
00:18:03,640 --> 00:18:09,080
它将成为恒等函数，但如果它没有通过，那么它就不会被激活，因此它会为零

192
00:18:09,080 --> 00:18:13,640
。所以这是一种简化。使用 sigmoid 对训练没有帮助，或

193
00:18:13,640 --> 00:18:21,320
者在某些时候训练起来非常困难，人们只是尝试了 relu，它恰好对这

194
00:18:21,320 --> 00:18:26,120
些令人难以置信的深层神经网络非常有效。好的，谢谢莉莎。

195
00:18:39,080 --> 00:18:40,060


