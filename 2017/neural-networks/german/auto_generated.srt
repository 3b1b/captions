1
00:00:04,220 --> 00:00:05,400
Dies ist eine 3.

2
00:00:06,060 --> 00:00:09,890
Sie ist schlampig geschrieben und mit einer extrem niedrigen Auflösung von 28x28 

3
00:00:09,890 --> 00:00:13,720
Pixeln gerendert, aber dein Gehirn hat keine Probleme, es als eine 3 zu erkennen.

4
00:00:14,340 --> 00:00:17,210
Nimm dir einen Moment Zeit um zu erkennen, wie verrückt es ist, 

5
00:00:17,210 --> 00:00:18,960
dass Gehirne das so mühelos tun können.

6
00:00:19,700 --> 00:00:22,992
Ich meine, das, das und das sind auch als 3 erkennbar, 

7
00:00:22,992 --> 00:00:28,320
auch wenn die spezifischen Werte jedes Pixels von Bild zu Bild sehr unterschiedlich sind.

8
00:00:28,900 --> 00:00:32,550
Die besonderen lichtempfindlichen Zellen in deinem Auge, die aktiviert werden, 

9
00:00:32,550 --> 00:00:35,877
wenn du diese 3 siehst, sind ganz anders als die, die aktiviert werden, 

10
00:00:35,877 --> 00:00:36,940
wenn du diese 3 siehst.

11
00:00:37,520 --> 00:00:42,828
Aber irgendetwas in deinem unglaublich cleveren visuellen Kortex löst diese Bilder als 

12
00:00:42,828 --> 00:00:48,260
dieselbe Idee auf, während es gleichzeitig andere Bilder als eigenständige Ideen erkennt.

13
00:00:49,220 --> 00:00:53,488
Aber wenn ich dir sage: "Hey, setz dich hin und schreibe mir ein Programm, 

14
00:00:53,488 --> 00:00:56,504
das ein Raster von 28x28 Pixeln wie dieses aufnimmt, 

15
00:00:56,504 --> 00:01:00,204
eine einzelne Zahl zwischen 0 und 10 ausgibt und dir somit sagt, 

16
00:01:00,204 --> 00:01:04,301
welche Zahl es annimmt", dann wird aus einer kinderleichten Aufgabe ein 

17
00:01:04,301 --> 00:01:06,180
höchst kompliziertes Unterfangen.

18
00:01:07,160 --> 00:01:09,214
Wenn du nicht gerade hinter dem Mond gelebt hast, 

19
00:01:09,214 --> 00:01:11,804
muss ich wohl kaum die Relevanz und Bedeutung von maschinellem 

20
00:01:11,804 --> 00:01:14,640
Lernen und neuronalen Netzen für die Gegenwart und Zukunft begründen.

21
00:01:15,120 --> 00:01:18,337
Dennoch möchte ich dir hier, ohne Vorkenntnisse vorauszusetzen, zeigen, 

22
00:01:18,337 --> 00:01:21,421
was ein neuronales Netzwerk eigentlich ist und dir veranschaulichen, 

23
00:01:21,421 --> 00:01:24,460
was es tut - nicht als Schlagwort, sondern als ein Stück Mathematik.

24
00:01:25,020 --> 00:01:28,062
Meine Hoffnung ist, dass du das Gefühl hast, dass die Struktur 

25
00:01:28,062 --> 00:01:30,814
selbst motiviert ist und dass du weißt, was es bedeutet, 

26
00:01:30,814 --> 00:01:34,340
wenn du von einem neuronalen Netzwerk, Zitat: "Lernen", liest oder hörst.

27
00:01:35,360 --> 00:01:37,835
In diesem Video geht es nur um die strukturelle 

28
00:01:37,835 --> 00:01:40,260
Komponente und im nächsten Video um das Lernen.

29
00:01:40,960 --> 00:01:44,141
Wir werden ein neuronales Netzwerk aufbauen, das lernen kann, 

30
00:01:44,141 --> 00:01:46,040
handgeschriebene Ziffern zu erkennen.

31
00:01:49,360 --> 00:01:52,163
Das ist ein klassisches Beispiel für die Einführung in das Thema, 

32
00:01:52,163 --> 00:01:55,604
und ich bleibe hier gerne beim Status quo, denn am Ende der beiden Videos möchte 

33
00:01:55,604 --> 00:01:57,685
ich dich auf ein paar gute Ressourcen verweisen, 

34
00:01:57,685 --> 00:02:00,276
wo du mehr erfahren kannst und wo du den Code, der dies tut, 

35
00:02:00,276 --> 00:02:03,080
herunterladen und auf deinem eigenen Computer ausprobieren kannst.

36
00:02:05,040 --> 00:02:07,888
Es gibt viele verschiedene Varianten neuronaler Netze, 

37
00:02:07,888 --> 00:02:11,514
und in den letzten Jahren hat die Forschung zu diesen Varianten einen 

38
00:02:11,514 --> 00:02:15,036
regelrechten Boom erlebt. In diesen beiden Einführungsvideos werden 

39
00:02:15,036 --> 00:02:19,180
wir uns jedoch nur die einfachste Form ohne zusätzlichen Schnickschnack ansehen.

40
00:02:19,860 --> 00:02:24,289
Das ist eine notwendige Voraussetzung, um die leistungsstärkeren modernen 

41
00:02:24,289 --> 00:02:28,600
Varianten zu verstehen, und glaub mir, sie sind immer noch sehr komplex.

42
00:02:29,120 --> 00:02:31,907
Aber selbst in dieser einfachsten Form kann er lernen, 

43
00:02:31,907 --> 00:02:35,709
handgeschriebene Ziffern zu erkennen, was für einen Computer eine ziemlich 

44
00:02:35,709 --> 00:02:36,520
coole Sache ist.

45
00:02:37,480 --> 00:02:40,449
Und gleichzeitig wirst du sehen, dass er einige Hoffnungen, 

46
00:02:40,449 --> 00:02:42,280
die wir in ihn setzen, nicht erfüllt.

47
00:02:43,380 --> 00:02:46,744
Wie der Name schon sagt, sind neuronale Netze vom Gehirn inspiriert, 

48
00:02:46,744 --> 00:02:48,500
aber lass uns das mal aufschlüsseln.

49
00:02:48,520 --> 00:02:51,660
Was sind die Neuronen und in welchem Sinne sind sie miteinander verbunden?

50
00:02:52,500 --> 00:02:56,944
Wenn ich jetzt von einem Neuron spreche, sollst du nur an ein Ding denken, 

51
00:02:56,944 --> 00:03:00,440
das eine Zahl enthält, und zwar eine Zahl zwischen 0 und 1.

52
00:03:00,680 --> 00:03:02,560
Mehr als das ist es wirklich nicht.

53
00:03:03,780 --> 00:03:08,274
Zum Beispiel beginnt das Netzwerk mit einem Bündel von Neuronen, 

54
00:03:08,274 --> 00:03:14,220
die jedem der 28x28 Pixel des Eingangsbildes entsprechen, also insgesamt 784 Neuronen.

55
00:03:14,700 --> 00:03:19,631
Jedes dieser Felder enthält eine Zahl, die den Graustufenwert des entsprechenden 

56
00:03:19,631 --> 00:03:24,380
Pixels darstellt und von 0 für schwarze Pixel bis zu 1 für weiße Pixel reicht.

57
00:03:25,300 --> 00:03:28,844
Diese Zahl innerhalb des Neurons wird als Aktivierung bezeichnet. 

58
00:03:28,844 --> 00:03:31,904
Du kannst dir vorstellen, dass jedes Neuron aufleuchtet, 

59
00:03:31,904 --> 00:03:34,160
wenn seine Aktivierung eine hohe Zahl ist.

60
00:03:36,720 --> 00:03:41,860
All diese 784 Neuronen bilden also die erste Schicht unseres Netzwerks.

61
00:03:46,500 --> 00:03:51,360
Die letzte Schicht besteht aus 10 Neuronen, die jeweils eine der Ziffern repräsentieren.

62
00:03:52,040 --> 00:03:56,336
Die Aktivierung dieser Neuronen, die wiederum eine Zahl zwischen 0 und 1 ist, 

63
00:03:56,336 --> 00:03:59,806
zeigt an, wie sehr das System glaubt, dass ein bestimmtes Bild 

64
00:03:59,806 --> 00:04:02,120
mit einer bestimmten Ziffer übereinstimmt.

65
00:04:03,040 --> 00:04:07,388
Dazwischen gibt es noch ein paar Schichten, die so genannten versteckten Schichten, 

66
00:04:07,388 --> 00:04:10,597
die vorerst nur ein riesiges Fragezeichen dafür sein sollten, 

67
00:04:10,597 --> 00:04:13,600
wie das Erkennen von Ziffern überhaupt funktionieren soll.

68
00:04:14,260 --> 00:04:18,476
In diesem Netzwerk habe ich zwei versteckte Schichten mit jeweils 16 Neuronen gewählt, 

69
00:04:18,476 --> 00:04:20,560
was zugegebenermaßen etwas willkürlich ist.

70
00:04:21,019 --> 00:04:23,237
Um ehrlich zu sein, habe ich mich für zwei Ebenen entschieden, 

71
00:04:23,237 --> 00:04:25,665
basierend darauf, wie ich die Struktur im Anschluss motivieren will, 

72
00:04:25,665 --> 00:04:28,200
und 16, das war einfach eine schöne Zahl, die auf den Bildschirm passte.

73
00:04:28,780 --> 00:04:32,340
In der Praxis gibt es hier viel Raum für Experimente mit einer bestimmten Struktur.

74
00:04:33,020 --> 00:04:35,888
So wie das Netzwerk funktioniert, bestimmen die Aktivierungen 

75
00:04:35,888 --> 00:04:38,480
in einer Schicht die Aktivierungen der nächsten Schicht.

76
00:04:39,200 --> 00:04:42,775
Das Herzstück des Netzwerks als Informationsverarbeitungsmechanismus 

77
00:04:42,775 --> 00:04:45,677
ist natürlich die Frage, wie die Aktivierungen in einer 

78
00:04:45,677 --> 00:04:48,580
Schicht zu Aktivierungen in der nächsten Schicht führen.

79
00:04:49,140 --> 00:04:52,992
Das ist in etwa so, wie wenn in biologischen Netzwerken von Neuronen 

80
00:04:52,992 --> 00:04:57,180
einige Gruppen von Neuronen feuern, die wiederum andere zum Feuern bringen.

81
00:04:58,120 --> 00:05:00,740
Das Netzwerk, das ich hier zeige, wurde bereits darauf trainiert, 

82
00:05:00,740 --> 00:05:03,400
Ziffern zu erkennen, und ich werde dir zeigen, was ich damit meine.

83
00:05:03,640 --> 00:05:07,802
Das heißt, wenn du ein Bild einspeist und alle 784 Neuronen der Eingabeschicht 

84
00:05:07,802 --> 00:05:11,068
entsprechend der Helligkeit jedes Pixels im Bild beleuchtest, 

85
00:05:11,068 --> 00:05:15,757
verursacht dieses Aktivierungsmuster ein ganz bestimmtes Muster in der nächsten Schicht, 

86
00:05:15,757 --> 00:05:19,129
das wiederum ein Muster in der übernächsten Schicht verursacht, 

87
00:05:19,129 --> 00:05:22,080
das schließlich ein Muster in der Ausgabeschicht ergibt.

88
00:05:22,560 --> 00:05:27,330
Und das hellste Neuron dieser Ausgabeschicht ist sozusagen die Wahl des Netzwerks, 

89
00:05:27,330 --> 00:05:29,400
welche Ziffer dieses Bild darstellt.

90
00:05:32,560 --> 00:05:34,564
Bevor wir uns mit der Mathematik beschäftigen, 

91
00:05:34,564 --> 00:05:37,848
wie eine Schicht die nächste beeinflusst oder wie das Training funktioniert, 

92
00:05:37,848 --> 00:05:40,364
lass uns darüber reden, warum es überhaupt vernünftig ist, 

93
00:05:40,364 --> 00:05:43,520
von einer solchen Schichtstruktur ein intelligentes Verhalten zu erwarten.

94
00:05:44,060 --> 00:05:45,220
Was erwarten wir hier?

95
00:05:45,400 --> 00:05:47,600
Was ist die beste Hoffnung für diese mittleren Schichten?

96
00:05:48,920 --> 00:05:53,520
Wenn du oder ich Zahlen erkennen, setzen wir verschiedene Komponenten zusammen.

97
00:05:54,200 --> 00:05:56,820
Eine 9 hat oben eine Schlaufe und rechts eine Linie.

98
00:05:57,380 --> 00:05:59,426
Eine 8 hat auch eine Schlaufe oben, aber sie ist 

99
00:05:59,426 --> 00:06:01,180
mit einer weiteren Schlaufe unten gepaart.

100
00:06:01,980 --> 00:06:06,820
Eine 4 besteht im Grunde aus drei bestimmten Linien und so weiter.

101
00:06:07,600 --> 00:06:11,586
In einer perfekten Welt würden wir hoffen, dass jedes Neuron in der 

102
00:06:11,586 --> 00:06:15,162
vorletzten Schicht einer dieser Unterkomponenten entspricht. 

103
00:06:15,162 --> 00:06:19,266
Wenn du also ein Bild mit einer Schleife, z. B. einer 9 oder einer 8, 

104
00:06:19,266 --> 00:06:23,780
eingibst, gibt es ein bestimmtes Neuron, dessen Aktivierung nahe bei 1 liegt.

105
00:06:24,500 --> 00:06:28,256
Und ich meine nicht diese spezielle Schleife von Pixeln, sondern die Hoffnung ist, 

106
00:06:28,256 --> 00:06:31,560
dass ein allgemeines Schleifenmuster an der Spitze dieses Neuron auslöst.

107
00:06:32,440 --> 00:06:36,556
Um von der dritten zur letzten Schicht zu gelangen, musst du also nur lernen, 

108
00:06:36,556 --> 00:06:40,040
welche Kombination von Teilkomponenten welchen Ziffern entspricht.

109
00:06:41,000 --> 00:06:42,980
Damit ist das Problem natürlich noch nicht gelöst, 

110
00:06:42,980 --> 00:06:45,698
denn wie würdest du diese Teilkomponenten erkennen oder sogar lernen, 

111
00:06:45,698 --> 00:06:47,640
welche die richtigen Teilkomponenten sein sollten?

112
00:06:48,060 --> 00:06:49,980
Und ich habe noch gar nicht darüber gesprochen, 

113
00:06:49,980 --> 00:06:53,060
wie eine Schicht die nächste beeinflusst, aber komm einen Moment mit mir mit.

114
00:06:53,680 --> 00:06:56,680
Das Erkennen einer Schleife kann auch in Teilprobleme zerfallen.

115
00:06:57,280 --> 00:06:59,856
Ein vernünftiger Weg, dies zu tun, wäre, zuerst die 

116
00:06:59,856 --> 00:07:02,780
verschiedenen kleinen Kanten zu erkennen, die es ausmachen.

117
00:07:03,780 --> 00:07:07,772
Genauso ist eine lange Linie, wie sie in den Ziffern 1, 4 oder 7 vorkommt, 

118
00:07:07,772 --> 00:07:11,179
eigentlich nur eine lange Kante, oder du kannst sie dir als ein 

119
00:07:11,179 --> 00:07:14,320
bestimmtes Muster aus mehreren kleineren Kanten vorstellen.

120
00:07:15,140 --> 00:07:18,984
Unsere Hoffnung ist also, dass jedes Neuron in der zweiten Schicht des 

121
00:07:18,984 --> 00:07:22,720
Netzes mit den verschiedenen relevanten kleinen Kanten übereinstimmt.

122
00:07:23,540 --> 00:07:27,433
Wenn ein Bild wie dieses hereinkommt, leuchten vielleicht alle Neuronen auf, 

123
00:07:27,433 --> 00:07:30,669
die mit etwa 8 bis 10 bestimmten kleinen Kanten verbunden sind, 

124
00:07:30,669 --> 00:07:34,866
was wiederum die Neuronen beleuchtet, die mit der oberen Schleife und einer langen 

125
00:07:34,866 --> 00:07:38,203
vertikalen Linie verbunden sind, und diese beleuchten das Neuron, 

126
00:07:38,203 --> 00:07:39,720
das mit einer 9 verbunden ist.

127
00:07:40,680 --> 00:07:43,426
Ob es das ist, was unser endgültiges Netzwerk tatsächlich tut, 

128
00:07:43,426 --> 00:07:46,304
ist eine andere Frage, auf die ich zurückkomme, sobald wir sehen, 

129
00:07:46,304 --> 00:07:48,964
wie wir das Netzwerk trainieren, aber das ist eine Hoffnung, 

130
00:07:48,964 --> 00:07:52,540
die wir vielleicht haben, eine Art Ziel mit der geschichteten Struktur wie dieser.

131
00:07:53,160 --> 00:07:57,173
Außerdem kannst du dir vorstellen, dass die Fähigkeit, Kanten und Muster zu erkennen, 

132
00:07:57,173 --> 00:08:00,300
auch für andere Aufgaben der Bilderkennung sehr nützlich sein kann.

133
00:08:00,880 --> 00:08:04,411
Und auch jenseits der Bilderkennung gibt es alle möglichen intelligenten Dinge, 

134
00:08:04,411 --> 00:08:07,280
die du tun möchtest und die sich in Abstraktionsebenen aufteilen.

135
00:08:08,040 --> 00:08:10,444
Beim Parsen von Sprache geht es zum Beispiel darum, 

136
00:08:10,444 --> 00:08:13,680
aus dem unbearbeiteten Audiomaterial bestimmte Laute herauszufiltern, 

137
00:08:13,680 --> 00:08:17,101
die sich zu bestimmten Silben zusammensetzen, die wiederum Wörter bilden, 

138
00:08:17,101 --> 00:08:20,060
die sich zu Sätzen und abstrakteren Gedanken zusammensetzen usw.

139
00:08:21,100 --> 00:08:25,401
Aber um darauf zurückzukommen, wie das eigentlich funktioniert, stell dir vor, 

140
00:08:25,401 --> 00:08:29,920
wie genau die Aktivierungen in einer Schicht die nächste Schicht bestimmen könnten.

141
00:08:30,860 --> 00:08:35,217
Das Ziel ist es, einen Mechanismus zu haben, der Pixel zu Kanten, 

142
00:08:35,217 --> 00:08:38,980
Kanten zu Mustern oder Muster zu Zahlen kombinieren kann.

143
00:08:39,440 --> 00:08:43,012
Um ein ganz konkretes Beispiel zu nennen: Wir hoffen, 

144
00:08:43,012 --> 00:08:46,915
dass ein bestimmtes Neuron in der zweiten Schicht erkennt, 

145
00:08:46,915 --> 00:08:50,620
ob das Bild in diesem Bereich einen Rand hat oder nicht.

146
00:08:51,440 --> 00:08:55,100
Die Frage, die sich stellt, ist: Welche Parameter sollte das Netzwerk haben?

147
00:08:55,640 --> 00:08:58,649
Welche Regler und Knöpfe solltest du so einstellen können, 

148
00:08:58,649 --> 00:09:02,883
dass sie ausdrucksstark genug sind, um dieses Muster oder jedes andere Pixelmuster 

149
00:09:02,883 --> 00:09:06,912
zu erfassen, oder das Muster, dass mehrere Kanten eine Schleife bilden können, 

150
00:09:06,912 --> 00:09:07,780
und andere Dinge?

151
00:09:08,720 --> 00:09:11,920
Wir weisen jeder der Verbindungen zwischen unserem 

152
00:09:11,920 --> 00:09:15,560
Neuron und den Neuronen der ersten Schicht ein Gewicht zu.

153
00:09:16,320 --> 00:09:17,700
Diese Gewichte sind nur Zahlen.

154
00:09:18,540 --> 00:09:21,991
Dann nimmst du alle Aktivierungen aus der ersten Schicht und 

155
00:09:21,991 --> 00:09:25,500
berechnest ihre gewichtete Summe entsprechend dieser Gewichte.

156
00:09:27,700 --> 00:09:32,393
Ich finde es hilfreich, wenn du dir diese Gewichte in einem eigenen kleinen Raster 

157
00:09:32,393 --> 00:09:37,143
vorstellst. Ich werde grüne Pixel für positive Gewichte und rote Pixel für negative 

158
00:09:37,143 --> 00:09:41,780
Gewichte verwenden, wobei die Helligkeit des Pixels den Wert des Gewichts anzeigt.

159
00:09:42,780 --> 00:09:46,070
Wenn wir nun die Gewichte für fast alle Pixel auf Null setzen, 

160
00:09:46,070 --> 00:09:50,091
mit Ausnahme einiger positiver Gewichte in der Region, die uns interessiert, 

161
00:09:50,091 --> 00:09:54,216
dann läuft die gewichtete Summe aller Pixelwerte eigentlich nur darauf hinaus, 

162
00:09:54,216 --> 00:09:57,820
die Werte der Pixel in der Region, die uns interessiert, zu addieren.

163
00:09:59,140 --> 00:10:03,096
Und wenn du wirklich herausfinden willst, ob es hier eine Kante gibt, 

164
00:10:03,096 --> 00:10:06,600
könntest du den umliegenden Pixeln negative Gewichte zuweisen.

165
00:10:07,480 --> 00:10:10,652
Dann ist die Summe am größten, wenn die mittleren Pixel hell, 

166
00:10:10,652 --> 00:10:12,700
die umliegenden Pixel aber dunkler sind.

167
00:10:14,260 --> 00:10:16,911
Wenn du eine gewichtete Summe wie diese berechnest, 

168
00:10:16,911 --> 00:10:20,633
kannst du eine beliebige Zahl erhalten, aber für dieses Netz wollen wir, 

169
00:10:20,633 --> 00:10:23,540
dass die Aktivierungen einen Wert zwischen 0 und 1 haben.

170
00:10:24,120 --> 00:10:28,299
Deshalb ist es üblich, diese gewichtete Summe in eine Funktion zu pumpen, 

171
00:10:28,299 --> 00:10:32,140
die die reelle Zahlenreihe in den Bereich zwischen 0 und 1 quetscht.

172
00:10:32,460 --> 00:10:35,630
Eine gängige Funktion, die dies tut, ist die Sigmoidfunktion, 

173
00:10:35,630 --> 00:10:37,420
auch bekannt als logistische Kurve.

174
00:10:38,000 --> 00:10:41,486
Im Grunde genommen enden sehr negative Eingaben nahe bei 0, 

175
00:10:41,486 --> 00:10:46,600
positive Eingaben enden nahe bei 1 und der Wert steigt um die Eingabe 0 herum stetig an.

176
00:10:49,120 --> 00:10:53,230
Die Aktivierung des Neurons ist hier also im Grunde ein Maß dafür, 

177
00:10:53,230 --> 00:10:56,360
wie positiv die entsprechende gewichtete Summe ist.

178
00:10:57,540 --> 00:11:00,102
Aber vielleicht willst du nicht, dass das Neuron aufleuchtet, 

179
00:11:00,102 --> 00:11:01,880
wenn die gewichtete Summe größer als 0 ist.

180
00:11:02,280 --> 00:11:06,360
Vielleicht möchtest du, dass sie nur aktiv ist, wenn die Summe größer als 10 ist.

181
00:11:06,840 --> 00:11:10,260
Das heißt, du willst, dass es inaktiv ist.

182
00:11:11,380 --> 00:11:15,441
Dann fügen wir dieser gewichteten Summe einfach eine andere Zahl hinzu, z. B. 

183
00:11:15,441 --> 00:11:19,660
eine negative 10, bevor wir sie in die Sigmoid-Squishification-Funktion einfügen.

184
00:11:20,580 --> 00:11:22,440
Diese zusätzliche Zahl wird als Verzerrung bezeichnet.

185
00:11:23,460 --> 00:11:27,386
Die Gewichte sagen dir also, welches Pixelmuster dieses Neuron in 

186
00:11:27,386 --> 00:11:30,539
der zweiten Schicht aufnimmt, und der Bias sagt dir, 

187
00:11:30,539 --> 00:11:35,180
wie hoch die gewichtete Summe sein muss, damit das Neuron sinnvoll aktiv wird.

188
00:11:36,120 --> 00:11:37,680
Und das ist nur ein Neuron.

189
00:11:38,280 --> 00:11:44,413
Jedes andere Neuron in dieser Schicht ist mit allen 784 Pixelneuronen aus der 

190
00:11:44,413 --> 00:11:50,940
ersten Schicht verbunden, und jede dieser 784 Verbindungen hat ihr eigenes Gewicht.

191
00:11:51,600 --> 00:11:54,261
Außerdem hat jede von ihnen eine Verzerrung, eine andere Zahl, 

192
00:11:54,261 --> 00:11:57,600
die du zur gewichteten Summe hinzufügst, bevor du sie mit dem Sigmoid stauchst.

193
00:11:58,110 --> 00:11:59,540
Und das ist eine Menge, über die man nachdenken muss!

194
00:11:59,960 --> 00:12:03,894
Bei dieser versteckten Schicht mit 16 Neuronen sind 

195
00:12:03,894 --> 00:12:07,980
das insgesamt 784 mal 16 Gewichte und 16 Verzerrungen.

196
00:12:08,840 --> 00:12:11,940
Und all das sind nur die Verbindungen von der ersten zur zweiten Schicht.

197
00:12:12,520 --> 00:12:14,874
Die Verbindungen zwischen den anderen Schichten haben auch eine 

198
00:12:14,874 --> 00:12:17,340
Reihe von Gewichten und Verzerrungen, die mit ihnen verbunden sind.

199
00:12:18,340 --> 00:12:23,800
Alles in allem hat dieses Netzwerk fast genau 13.000 Gewichte und Verzerrungen.

200
00:12:23,800 --> 00:12:27,141
13.000 Knöpfe und Regler, an denen du drehen und wenden kannst, 

201
00:12:27,141 --> 00:12:29,960
um das Netzwerk auf unterschiedliche Weise zu steuern.

202
00:12:31,040 --> 00:12:34,971
Wenn wir also von Lernen sprechen, geht es darum, den Computer dazu zu bringen, 

203
00:12:34,971 --> 00:12:38,509
eine gültige Einstellung für all diese vielen, vielen Zahlen zu finden, 

204
00:12:38,509 --> 00:12:41,360
so dass er das vorliegende Problem tatsächlich lösen kann.

205
00:12:42,620 --> 00:12:46,295
Ein Gedankenexperiment, das gleichzeitig Spaß macht und irgendwie erschreckend ist, 

206
00:12:46,295 --> 00:12:49,534
ist die Vorstellung, dass du dich hinsetzt und all diese Gewichtungen und 

207
00:12:49,534 --> 00:12:53,079
Verzerrungen von Hand einstellst, indem du die Zahlen absichtlich so veränderst, 

208
00:12:53,079 --> 00:12:56,580
dass die zweite Schicht die Kanten aufnimmt, die dritte Schicht die Muster, usw.

209
00:12:56,980 --> 00:13:00,937
Ich persönlich finde das befriedigender, als das Netzwerk als totale Blackbox zu 

210
00:13:00,937 --> 00:13:04,993
behandeln, denn wenn das Netzwerk nicht so funktioniert, wie du es dir vorstellst, 

211
00:13:04,993 --> 00:13:09,342
hast du, wenn du eine kleine Beziehung zu den Gewichten und Verzerrungen aufgebaut hast, 

212
00:13:09,342 --> 00:13:13,153
eine Ausgangsbasis, um zu experimentieren, wie du die Struktur ändern kannst, 

213
00:13:13,153 --> 00:13:14,180
um sie zu verbessern.

214
00:13:14,960 --> 00:13:18,669
Wenn das Netzwerk zwar funktioniert, aber nicht aus den Gründen, die du erwartest, 

215
00:13:18,669 --> 00:13:21,976
ist die Untersuchung der Gewichte und Verzerrungen eine gute Möglichkeit, 

216
00:13:21,976 --> 00:13:25,820
deine Annahmen in Frage zu stellen und den ganzen Raum möglicher Lösungen aufzudecken.

217
00:13:26,840 --> 00:13:29,985
Übrigens ist die eigentliche Funktion hier etwas umständlich aufzuschreiben, 

218
00:13:29,985 --> 00:13:30,680
findest du nicht?

219
00:13:32,500 --> 00:13:37,140
Ich zeige dir jetzt eine kompaktere Art, diese Verbindungen darzustellen.

220
00:13:37,660 --> 00:13:40,520
So würdest du es sehen, wenn du mehr über neuronale Netze nachlesen würdest.

221
00:13:40,653 --> 00:13:40,520
Organisiere alle Aktivierungen einer Schicht in einer Spalte, 

222
00:13:41,083 --> 00:13:40,653
da eine Matrix den Verbindungen zwischen einer Schicht und einem bestimmten Neuron in der 

223
00:13:41,380 --> 00:13:41,083
nächsten Schicht entspricht.

224
00:13:41,380 --> 00:13:47,024
Das bedeutet, dass die gewichtete Summe der Aktivierungen in der ersten 

225
00:13:47,024 --> 00:13:53,923
Schicht entsprechend dieser Gewichte einem der Terme im Matrix-Vektorprodukt von allem, 

226
00:13:53,923 --> 00:13:58,000
was wir hier auf der linken Seite haben, entspricht.

227
00:13:58,540 --> 00:14:01,528
Übrigens: Ein Großteil des maschinellen Lernens beruht auf einem guten Verständnis 

228
00:14:01,528 --> 00:14:04,263
der linearen Algebra. Wer also ein gutes visuelles Verständnis für Matrizen 

229
00:14:04,263 --> 00:14:06,604
und die Bedeutung der Matrix-Vektor-Multiplikation haben möchte, 

230
00:14:06,604 --> 00:14:08,908
sollte einen Blick auf meine Serie über lineare Algebra werfen, 

231
00:14:08,908 --> 00:14:09,880
insbesondere auf Kapitel 3.

232
00:14:14,000 --> 00:14:18,911
Zurück zu unserem Ausdruck: Anstatt die Verzerrung zu jedem dieser Werte 

233
00:14:18,911 --> 00:14:23,957
einzeln zu addieren, fassen wir alle Verzerrungen in einem Vektor zusammen 

234
00:14:23,957 --> 00:14:28,600
und addieren den gesamten Vektor zum vorherigen Matrix-Vektorprodukt.

235
00:14:29,240 --> 00:14:33,821
Als letzten Schritt wickle ich hier ein Sigmoid um die Außenseite. 

236
00:14:33,821 --> 00:14:38,265
Das soll bedeuten, dass du die Sigmoidfunktion auf jede einzelne 

237
00:14:38,265 --> 00:14:42,300
Komponente des resultierenden Vektors im Inneren anwendest.

238
00:14:43,280 --> 00:14:45,719
Wenn du also diese Gewichtsmatrix und diese Vektoren als eigene Symbole aufschreibst, 

239
00:14:45,719 --> 00:14:48,017
kannst du den gesamten Übergang der Aktivierungen von einer Schicht zur nächsten 

240
00:14:48,017 --> 00:14:50,173
in einem extrem knappen und übersichtlichen kleinen Ausdruck kommunizieren, 

241
00:14:50,173 --> 00:14:52,584
und das macht den entsprechenden Code sowohl viel einfacher als auch viel schneller, 

242
00:14:52,584 --> 00:14:54,740
da viele Bibliotheken die Matrixmultiplikation bis aufs Äußerste optimieren.

243
00:14:55,940 --> 00:15:05,199
Erinnerst du dich daran, dass ich vorhin gesagt habe, 

244
00:15:05,199 --> 00:15:15,660
dass diese Neuronen einfach Dinge sind, die Zahlen enthalten?

245
00:15:17,820 --> 00:15:19,341
Deshalb ist es genauer, sich jedes Neuron als eine Funktion vorzustellen, 

246
00:15:19,341 --> 00:15:21,130
die die Ausgaben aller Neuronen der vorherigen Schicht aufnimmt und eine Zahl zwischen 

247
00:15:21,130 --> 00:15:21,460
0 und 1 ausgibt.

248
00:15:22,220 --> 00:15:29,312
Das gesamte Netzwerk ist eigentlich nur eine Funktion, 

249
00:15:29,312 --> 00:15:38,340
die 784 Zahlen als Eingabe erhält und 10 Zahlen als Ausgabe ausspuckt.

250
00:15:39,200 --> 00:15:41,136
Es ist eine absurd komplizierte Funktion mit 13.000 Parametern in Form von Gewichten 

251
00:15:41,136 --> 00:15:42,298
und Verzerrungen, die bestimmte Muster aufgreifen, 

252
00:15:42,298 --> 00:15:43,870
und die die Iteration vieler Matrix-Vektor-Produkte und die sigmoide 

253
00:15:43,870 --> 00:15:45,624
Squishification-Funktion beinhaltet, aber es ist trotzdem nur eine Funktion, 

254
00:15:45,624 --> 00:15:47,060
und irgendwie ist es beruhigend, dass sie kompliziert aussieht.

255
00:15:47,560 --> 00:15:55,152
Ich meine, wenn es noch einfacher wäre, welche Hoffnung hätten wir dann, 

256
00:15:55,152 --> 00:16:02,640
dass es die Herausforderung des Erkennens von Ziffern bewältigen könnte?

257
00:16:03,400 --> 00:16:06,660
Und wie nimmt sie diese Herausforderung an?

258
00:16:07,340 --> 00:16:10,616
Wie lernt dieses Netz die richtigen Gewichte und Verzerrungen, 

259
00:16:10,616 --> 00:16:12,280
indem es sich die Daten ansieht?

260
00:16:13,340 --> 00:16:14,041
Das zeige ich dir im nächsten Video, und ich werde auch ein bisschen mehr darauf 

261
00:16:14,041 --> 00:16:14,700
eingehen, was dieses spezielle Netzwerk, das wir hier sehen, wirklich macht.

262
00:16:15,080 --> 00:16:16,719
Jetzt sollte ich wohl sagen, dass du dich anmelden sollst, damit du benachrichtigt wirst, 

263
00:16:16,719 --> 00:16:18,194
wenn ein Video oder neue Videos erscheinen, aber realistisch betrachtet erhalten 

264
00:16:18,194 --> 00:16:19,360
die meisten von euch keine Benachrichtigungen von YouTube, oder?

265
00:16:20,140 --> 00:16:22,239
Vielleicht sollte ich ehrlicher sagen: Abonnieren, damit die neuronalen Netze, 

266
00:16:22,239 --> 00:16:24,498
die dem Empfehlungsalgorithmus von YouTube zugrunde liegen, darauf eingestellt sind, 

267
00:16:24,498 --> 00:16:26,120
dass du Inhalte von diesem Kanal empfohlen bekommen möchtest.

268
00:16:27,580 --> 00:16:37,420
Bleib auf jeden Fall auf dem Laufenden für mehr.

269
00:16:38,020 --> 00:16:47,880
Vielen Dank an alle, die diese Videos auf Patreon unterstützen.

270
00:16:48,560 --> 00:16:49,183
In der Wahrscheinlichkeitsreihe bin ich diesen Sommer etwas langsamer vorangekommen, 

271
00:16:49,183 --> 00:16:49,580
aber ich werde nach diesem Projekt wieder einsteigen, 

272
00:16:49,580 --> 00:16:49,940
also kannst du dort nach Updates Ausschau halten.

273
00:16:50,760 --> 00:16:51,464
Zum Abschluss habe ich Leisha Lee bei mir, die sich in ihrer Doktorarbeit 

274
00:16:51,464 --> 00:16:52,168
mit der theoretischen Seite des Deep Learning beschäftigt hat und derzeit 

275
00:16:52,168 --> 00:16:52,681
bei der Risikokapitalfirma Amplify Partners arbeitet, 

276
00:16:52,681 --> 00:16:53,500
die freundlicherweise einen Teil der Finanzierung für dieses Video bereitgestellt hat.

277
00:16:54,000 --> 00:17:01,900
Leisha, eine Sache, die wir schnell erwähnen sollten, ist die sigmoide Funktion.

278
00:17:03,600 --> 00:17:06,054
So wie ich es verstehe, nutzen frühe Netzwerke dies, 

279
00:17:06,054 --> 00:17:10,036
um die relevante gewichtete Summe in das Intervall zwischen Null und Eins zu pressen, 

280
00:17:10,036 --> 00:17:12,443
sozusagen motiviert durch die biologische Analogie, 

281
00:17:12,443 --> 00:17:14,619
dass Neuronen entweder inaktiv oder aktiv sind.

282
00:17:15,460 --> 00:17:19,119
Ganz genau.

283
00:17:19,700 --> 00:17:29,840
Aber nur noch relativ wenige moderne Netze verwenden das Sigmoid.

284
00:17:30,280 --> 00:17:30,300
Ja.

285
00:17:30,560 --> 00:17:34,040
Das ist doch ganz schön altmodisch, oder?

286
00:17:34,320 --> 00:17:34,320
Ja, oder besser gesagt, Relu scheint viel einfacher zu trainieren zu sein.

287
00:17:34,440 --> 00:17:35,540
Und relu steht für rectified linear unit?

288
00:17:35,760 --> 00:17:36,299
Ja, es ist diese Art von Funktion, bei der du einfach einen Maximalwert aus 

289
00:17:36,299 --> 00:17:36,866
Null und A nimmst, wobei A durch das gegeben ist, was du im Video erklärt hast. 

290
00:17:36,866 --> 00:17:37,426
Ich glaube, das wurde zum Teil durch eine biologische Analogie dazu motiviert, 

291
00:17:37,426 --> 00:17:37,788
wie Neuronen entweder aktiviert werden oder nicht, 

292
00:17:37,788 --> 00:17:38,206
und wenn sie einen bestimmten Schwellenwert überschreiten, 

293
00:17:38,206 --> 00:17:38,554
wäre es die Identitätsfunktion, aber wenn nicht, 

294
00:17:38,554 --> 00:17:38,980
würde sie einfach nicht aktiviert werden, also wäre es Null.

295
00:17:39,400 --> 00:17:40,425
Die Verwendung von Sigmoiden hat beim Training nicht geholfen oder war irgendwann 

296
00:17:40,425 --> 00:17:41,364
sehr schwierig zu trainieren und die Leute haben einfach Relu ausprobiert, 

297
00:17:41,364 --> 00:17:42,340
was bei diesen unglaublich tiefen neuronalen Netzen sehr gut funktioniert hat.

298
00:17:42,680 --> 00:18:01,360
Vielen Dank, Alicia.

299
00:18:01,360 --> 00:18:04,686
Wenn sie also einen bestimmten Schwellenwert überschreitet, 

300
00:18:04,686 --> 00:18:07,458
wäre sie die Identitätsfunktion, aber wenn nicht, 

301
00:18:07,458 --> 00:18:10,840
würde sie einfach nicht aktiviert werden, also wäre sie Null.

302
00:18:11,160 --> 00:18:15,683
Die Verwendung von Sigmoiden hat beim Training nicht geholfen oder war irgendwann 

303
00:18:15,683 --> 00:18:20,151
sehr schwierig zu trainieren und die Leute haben einfach ReLU ausprobiert und es 

304
00:18:20,151 --> 00:18:24,620
hat zufällig sehr gut für diese unglaublich tiefen neuronalen Netze funktioniert.

305
00:18:25,100 --> 00:18:25,640
Vielen Dank, Lisha.

