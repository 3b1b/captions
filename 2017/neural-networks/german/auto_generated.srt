1
00:00:04,220 --> 00:00:05,400
Dies ist eine 3.

2
00:00:06,060 --> 00:00:09,866
Es ist schlampig geschrieben und mit einer extrem niedrigen Auflösung von 28x28 

3
00:00:09,866 --> 00:00:13,720
Pixeln gerendert, aber dein Gehirn hat keine Probleme, es als eine 3 zu erkennen.

4
00:00:14,340 --> 00:00:16,827
Und ich möchte, dass du dir einen Moment Zeit nimmst, um zu erkennen, 

5
00:00:16,827 --> 00:00:18,960
wie verrückt es ist, dass Gehirne das so mühelos tun können.

6
00:00:19,700 --> 00:00:23,065
Ich meine, das, das und das sind auch als 3er erkennbar, 

7
00:00:23,065 --> 00:00:28,320
auch wenn die spezifischen Werte jedes Pixels von Bild zu Bild sehr unterschiedlich sind.

8
00:00:28,900 --> 00:00:32,502
Die besonderen lichtempfindlichen Zellen in deinem Auge, die feuern, 

9
00:00:32,502 --> 00:00:36,940
wenn du diese 3 siehst, sind ganz anders als die, die feuern, wenn du diese 3 siehst.

10
00:00:37,520 --> 00:00:43,117
Aber irgendetwas in deinem verrückten visuellen Kortex löst diese Bilder als dieselbe 

11
00:00:43,117 --> 00:00:48,260
Idee auf, während er gleichzeitig andere Bilder als ihre eigenen Ideen erkennt.

12
00:00:49,220 --> 00:00:53,683
Aber wenn ich dir sage: "Hey, setz dich hin und schreibe mir ein Programm, 

13
00:00:53,683 --> 00:00:57,848
das ein Raster von 28x28 Pixeln wie dieses aufnimmt und eine einzelne 

14
00:00:57,848 --> 00:01:02,133
Zahl zwischen 0 und 10 ausgibt und dir sagt, was es für eine Zahl hält, 

15
00:01:02,133 --> 00:01:06,180
dann wird die Aufgabe von komisch trivial zu erschreckend schwierig.

16
00:01:07,160 --> 00:01:09,273
Wenn du nicht gerade unter einem Stein gelebt hast, 

17
00:01:09,273 --> 00:01:11,835
muss ich wohl kaum die Relevanz und Bedeutung von maschinellem 

18
00:01:11,835 --> 00:01:14,640
Lernen und neuronalen Netzen für die Gegenwart und Zukunft begründen.

19
00:01:15,120 --> 00:01:18,577
Aber ich möchte dir hier zeigen, was ein neuronales Netzwerk eigentlich ist, 

20
00:01:18,577 --> 00:01:21,406
ohne dass du einen Hintergrund hast, und dir veranschaulichen, 

21
00:01:21,406 --> 00:01:24,460
was es tut - nicht als Schlagwort, sondern als ein Stück Mathematik.

22
00:01:25,020 --> 00:01:28,062
Meine Hoffnung ist, dass du das Gefühl hast, dass die Struktur 

23
00:01:28,062 --> 00:01:30,814
selbst motiviert ist und dass du weißt, was es bedeutet, 

24
00:01:30,814 --> 00:01:34,340
wenn du von einem neuronalen Netzwerk, Zitat: "Lernen", liest oder hörst.

25
00:01:35,360 --> 00:01:37,835
In diesem Video geht es nur um die strukturelle 

26
00:01:37,835 --> 00:01:40,260
Komponente und im nächsten Video um das Lernen.

27
00:01:40,960 --> 00:01:44,141
Wir werden ein neuronales Netzwerk aufbauen, das lernen kann, 

28
00:01:44,141 --> 00:01:46,040
handgeschriebene Ziffern zu erkennen.

29
00:01:49,360 --> 00:01:52,163
Das ist ein klassisches Beispiel für die Einführung in das Thema, 

30
00:01:52,163 --> 00:01:55,604
und ich bleibe hier gerne beim Status quo, denn am Ende der beiden Videos möchte 

31
00:01:55,604 --> 00:01:57,685
ich dich auf ein paar gute Ressourcen verweisen, 

32
00:01:57,685 --> 00:02:00,276
wo du mehr erfahren kannst und wo du den Code, der dies tut, 

33
00:02:00,276 --> 00:02:03,080
herunterladen und auf deinem eigenen Computer ausprobieren kannst.

34
00:02:05,040 --> 00:02:07,888
Es gibt viele verschiedene Varianten neuronaler Netze, 

35
00:02:07,888 --> 00:02:11,514
und in den letzten Jahren hat die Forschung zu diesen Varianten einen 

36
00:02:11,514 --> 00:02:15,036
regelrechten Boom erlebt. In diesen beiden Einführungsvideos werden 

37
00:02:15,036 --> 00:02:19,180
wir uns jedoch nur die einfachste Form ohne zusätzlichen Schnickschnack ansehen.

38
00:02:19,860 --> 00:02:24,289
Das ist eine notwendige Voraussetzung, um die leistungsstärkeren modernen 

39
00:02:24,289 --> 00:02:28,600
Varianten zu verstehen, und glaub mir, sie sind immer noch sehr komplex.

40
00:02:29,120 --> 00:02:31,907
Aber selbst in dieser einfachsten Form kann er lernen, 

41
00:02:31,907 --> 00:02:35,709
handgeschriebene Ziffern zu erkennen, was für einen Computer eine ziemlich 

42
00:02:35,709 --> 00:02:36,520
coole Sache ist.

43
00:02:37,480 --> 00:02:40,467
Und gleichzeitig wirst du sehen, dass sie einige Hoffnungen, 

44
00:02:40,467 --> 00:02:42,280
die wir in sie setzen, nicht erfüllt.

45
00:02:43,380 --> 00:02:46,744
Wie der Name schon sagt, sind neuronale Netze vom Gehirn inspiriert, 

46
00:02:46,744 --> 00:02:48,500
aber lass uns das mal aufschlüsseln.

47
00:02:48,520 --> 00:02:51,660
Was sind die Neuronen und in welchem Sinne sind sie miteinander verbunden?

48
00:02:52,500 --> 00:02:56,944
Wenn ich jetzt von einem Neuron spreche, sollst du nur an ein Ding denken, 

49
00:02:56,944 --> 00:03:00,440
das eine Zahl enthält, und zwar eine Zahl zwischen 0 und 1.

50
00:03:00,680 --> 00:03:02,560
Mehr als das ist es wirklich nicht.

51
00:03:03,780 --> 00:03:08,274
Zum Beispiel beginnt das Netzwerk mit einem Bündel von Neuronen, 

52
00:03:08,274 --> 00:03:14,220
die jedem der 28x28 Pixel des Eingangsbildes entsprechen, also insgesamt 784 Neuronen.

53
00:03:14,700 --> 00:03:19,631
Jedes dieser Felder enthält eine Zahl, die den Graustufenwert des entsprechenden 

54
00:03:19,631 --> 00:03:24,380
Pixels darstellt und von 0 für schwarze Pixel bis zu 1 für weiße Pixel reicht.

55
00:03:25,300 --> 00:03:28,844
Diese Zahl innerhalb des Neurons wird als Aktivierung bezeichnet. 

56
00:03:28,844 --> 00:03:31,904
Du kannst dir vorstellen, dass jedes Neuron aufleuchtet, 

57
00:03:31,904 --> 00:03:34,160
wenn seine Aktivierung eine hohe Zahl ist.

58
00:03:36,720 --> 00:03:41,860
All diese 784 Neuronen bilden also die erste Schicht unseres Netzwerks.

59
00:03:46,500 --> 00:03:51,360
Die letzte Schicht besteht aus 10 Neuronen, die jeweils eine der Ziffern repräsentieren.

60
00:03:52,040 --> 00:03:56,336
Die Aktivierung dieser Neuronen, die wiederum eine Zahl zwischen 0 und 1 ist, 

61
00:03:56,336 --> 00:03:59,806
zeigt an, wie sehr das System glaubt, dass ein bestimmtes Bild 

62
00:03:59,806 --> 00:04:02,120
mit einer bestimmten Ziffer übereinstimmt.

63
00:04:03,040 --> 00:04:07,388
Dazwischen gibt es noch ein paar Schichten, die so genannten versteckten Schichten, 

64
00:04:07,388 --> 00:04:10,597
die vorerst nur ein riesiges Fragezeichen dafür sein sollten, 

65
00:04:10,597 --> 00:04:13,600
wie das Erkennen von Ziffern überhaupt funktionieren soll.

66
00:04:14,260 --> 00:04:18,476
In diesem Netzwerk habe ich zwei versteckte Schichten mit jeweils 16 Neuronen gewählt, 

67
00:04:18,476 --> 00:04:20,560
was zugegebenermaßen etwas willkürlich ist.

68
00:04:21,019 --> 00:04:23,491
Um ehrlich zu sein, habe ich mich für zwei Ebenen entschieden, 

69
00:04:23,491 --> 00:04:25,688
weil ich die Struktur gleich motivieren wollte, und 16, 

70
00:04:25,688 --> 00:04:28,200
das war einfach eine schöne Zahl, die auf den Bildschirm passte.

71
00:04:28,780 --> 00:04:32,340
In der Praxis gibt es hier viel Raum für Experimente mit einer bestimmten Struktur.

72
00:04:33,020 --> 00:04:35,888
So wie das Netzwerk funktioniert, bestimmen die Aktivierungen 

73
00:04:35,888 --> 00:04:38,480
in einer Schicht die Aktivierungen der nächsten Schicht.

74
00:04:39,200 --> 00:04:42,775
Das Herzstück des Netzwerks als Informationsverarbeitungsmechanismus 

75
00:04:42,775 --> 00:04:45,677
ist natürlich die Frage, wie die Aktivierungen in einer 

76
00:04:45,677 --> 00:04:48,580
Schicht zu Aktivierungen in der nächsten Schicht führen.

77
00:04:49,140 --> 00:04:52,992
Das ist in etwa so, wie wenn in biologischen Netzwerken von Neuronen 

78
00:04:52,992 --> 00:04:57,180
einige Gruppen von Neuronen feuern, die wiederum andere zum Feuern bringen.

79
00:04:58,120 --> 00:05:00,740
Das Netzwerk, das ich hier zeige, wurde bereits darauf trainiert, 

80
00:05:00,740 --> 00:05:03,400
Ziffern zu erkennen, und ich werde dir zeigen, was ich damit meine.

81
00:05:03,640 --> 00:05:07,802
Das heißt, wenn du ein Bild einspeist und alle 784 Neuronen der Eingabeschicht 

82
00:05:07,802 --> 00:05:11,068
entsprechend der Helligkeit jedes Pixels im Bild beleuchtest, 

83
00:05:11,068 --> 00:05:15,757
verursacht dieses Aktivierungsmuster ein ganz bestimmtes Muster in der nächsten Schicht, 

84
00:05:15,757 --> 00:05:19,129
das wiederum ein Muster in der übernächsten Schicht verursacht, 

85
00:05:19,129 --> 00:05:22,080
das schließlich ein Muster in der Ausgabeschicht ergibt.

86
00:05:22,560 --> 00:05:27,330
Und das hellste Neuron dieser Ausgabeschicht ist sozusagen die Wahl des Netzwerks, 

87
00:05:27,330 --> 00:05:29,400
welche Ziffer dieses Bild darstellt.

88
00:05:32,560 --> 00:05:34,564
Bevor wir uns mit der Mathematik beschäftigen, 

89
00:05:34,564 --> 00:05:37,848
wie eine Schicht die nächste beeinflusst oder wie das Training funktioniert, 

90
00:05:37,848 --> 00:05:40,364
lass uns darüber reden, warum es überhaupt vernünftig ist, 

91
00:05:40,364 --> 00:05:43,520
von einer solchen Schichtstruktur ein intelligentes Verhalten zu erwarten.

92
00:05:44,060 --> 00:05:45,220
Was erwarten wir hier?

93
00:05:45,400 --> 00:05:47,600
Was ist die beste Hoffnung für diese mittleren Schichten?

94
00:05:48,920 --> 00:05:53,520
Wenn du oder ich Zahlen erkennen, setzen wir verschiedene Komponenten zusammen.

95
00:05:54,200 --> 00:05:56,820
Eine 9 hat oben eine Schlaufe und rechts eine Linie.

96
00:05:57,380 --> 00:05:59,426
Eine 8 hat auch eine Schlaufe oben, aber sie ist 

97
00:05:59,426 --> 00:06:01,180
mit einer weiteren Schlaufe unten gepaart.

98
00:06:01,980 --> 00:06:06,820
Eine 4 besteht im Grunde aus drei bestimmten Linien und so weiter.

99
00:06:07,600 --> 00:06:11,586
In einer perfekten Welt würden wir hoffen, dass jedes Neuron in der 

100
00:06:11,586 --> 00:06:15,162
vorletzten Schicht einer dieser Unterkomponenten entspricht. 

101
00:06:15,162 --> 00:06:19,266
Wenn du also ein Bild mit einer Schleife, z. B. einer 9 oder einer 8, 

102
00:06:19,266 --> 00:06:23,780
eingibst, gibt es ein bestimmtes Neuron, dessen Aktivierung nahe bei 1 liegt.

103
00:06:24,500 --> 00:06:28,256
Und ich meine nicht diese spezielle Schleife von Pixeln, sondern die Hoffnung ist, 

104
00:06:28,256 --> 00:06:31,560
dass ein allgemeines Schleifenmuster an der Spitze dieses Neuron auslöst.

105
00:06:32,440 --> 00:06:36,556
Um von der dritten zur letzten Schicht zu gelangen, musst du also nur lernen, 

106
00:06:36,556 --> 00:06:40,040
welche Kombination von Teilkomponenten welchen Ziffern entspricht.

107
00:06:41,000 --> 00:06:42,980
Damit ist das Problem natürlich noch nicht gelöst, 

108
00:06:42,980 --> 00:06:45,698
denn wie würdest du diese Teilkomponenten erkennen oder sogar lernen, 

109
00:06:45,698 --> 00:06:47,640
welche die richtigen Teilkomponenten sein sollten?

110
00:06:48,060 --> 00:06:49,980
Und ich habe noch gar nicht darüber gesprochen, 

111
00:06:49,980 --> 00:06:53,060
wie eine Schicht die nächste beeinflusst, aber komm einen Moment mit mir mit.

112
00:06:53,680 --> 00:06:56,680
Das Erkennen einer Schleife kann auch in Teilprobleme zerfallen.

113
00:06:57,280 --> 00:06:59,856
Ein vernünftiger Weg, dies zu tun, wäre, zuerst die 

114
00:06:59,856 --> 00:07:02,780
verschiedenen kleinen Kanten zu erkennen, die es ausmachen.

115
00:07:03,780 --> 00:07:07,772
Genauso ist eine lange Linie, wie sie in den Ziffern 1, 4 oder 7 vorkommt, 

116
00:07:07,772 --> 00:07:11,179
eigentlich nur eine lange Kante, oder du kannst sie dir als ein 

117
00:07:11,179 --> 00:07:14,320
bestimmtes Muster aus mehreren kleineren Kanten vorstellen.

118
00:07:15,140 --> 00:07:18,984
Unsere Hoffnung ist also, dass jedes Neuron in der zweiten Schicht des 

119
00:07:18,984 --> 00:07:22,720
Netzes mit den verschiedenen relevanten kleinen Kanten übereinstimmt.

120
00:07:23,540 --> 00:07:27,433
Wenn ein Bild wie dieses hereinkommt, leuchten vielleicht alle Neuronen auf, 

121
00:07:27,433 --> 00:07:30,669
die mit etwa 8 bis 10 bestimmten kleinen Kanten verbunden sind, 

122
00:07:30,669 --> 00:07:34,866
was wiederum die Neuronen beleuchtet, die mit der oberen Schleife und einer langen 

123
00:07:34,866 --> 00:07:38,203
vertikalen Linie verbunden sind, und diese beleuchten das Neuron, 

124
00:07:38,203 --> 00:07:39,720
das mit einer 9 verbunden ist.

125
00:07:40,680 --> 00:07:43,426
Ob es das ist, was unser endgültiges Netzwerk tatsächlich tut, 

126
00:07:43,426 --> 00:07:46,304
ist eine andere Frage, auf die ich zurückkomme, sobald wir sehen, 

127
00:07:46,304 --> 00:07:48,964
wie wir das Netzwerk trainieren, aber das ist eine Hoffnung, 

128
00:07:48,964 --> 00:07:52,540
die wir vielleicht haben, eine Art Ziel mit der geschichteten Struktur wie dieser.

129
00:07:53,160 --> 00:07:57,173
Außerdem kannst du dir vorstellen, dass die Fähigkeit, Kanten und Muster zu erkennen, 

130
00:07:57,173 --> 00:08:00,300
auch für andere Aufgaben der Bilderkennung sehr nützlich sein kann.

131
00:08:00,880 --> 00:08:04,411
Und auch jenseits der Bilderkennung gibt es alle möglichen intelligenten Dinge, 

132
00:08:04,411 --> 00:08:07,280
die du tun möchtest und die sich in Abstraktionsebenen aufteilen.

133
00:08:08,040 --> 00:08:10,444
Beim Parsen von Sprache geht es zum Beispiel darum, 

134
00:08:10,444 --> 00:08:13,680
aus dem unbearbeiteten Audiomaterial bestimmte Laute herauszufiltern, 

135
00:08:13,680 --> 00:08:17,101
die sich zu bestimmten Silben zusammensetzen, die wiederum Wörter bilden, 

136
00:08:17,101 --> 00:08:20,060
die sich zu Sätzen und abstrakteren Gedanken zusammensetzen usw.

137
00:08:21,100 --> 00:08:25,401
Aber um darauf zurückzukommen, wie das eigentlich funktioniert, stell dir vor, 

138
00:08:25,401 --> 00:08:29,920
wie genau die Aktivierungen in einer Schicht die nächste Schicht bestimmen könnten.

139
00:08:30,860 --> 00:08:35,217
Das Ziel ist es, einen Mechanismus zu haben, der Pixel zu Kanten, 

140
00:08:35,217 --> 00:08:38,980
Kanten zu Mustern oder Muster zu Zahlen kombinieren kann.

141
00:08:39,440 --> 00:08:43,012
Um ein ganz konkretes Beispiel zu nennen: Wir hoffen, 

142
00:08:43,012 --> 00:08:46,915
dass ein bestimmtes Neuron in der zweiten Schicht erkennt, 

143
00:08:46,915 --> 00:08:50,620
ob das Bild in diesem Bereich einen Rand hat oder nicht.

144
00:08:51,440 --> 00:08:55,100
Die Frage, die sich stellt, ist: Welche Parameter sollte das Netzwerk haben?

145
00:08:55,640 --> 00:08:58,649
Welche Regler und Knöpfe solltest du so einstellen können, 

146
00:08:58,649 --> 00:09:02,883
dass sie ausdrucksstark genug sind, um dieses Muster oder jedes andere Pixelmuster 

147
00:09:02,883 --> 00:09:06,912
zu erfassen, oder das Muster, dass mehrere Kanten eine Schleife bilden können, 

148
00:09:06,912 --> 00:09:07,780
und andere Dinge?

149
00:09:08,720 --> 00:09:11,920
Wir weisen jeder der Verbindungen zwischen unserem 

150
00:09:11,920 --> 00:09:15,560
Neuron und den Neuronen der ersten Schicht ein Gewicht zu.

151
00:09:16,320 --> 00:09:17,700
Diese Gewichte sind nur Zahlen.

152
00:09:18,540 --> 00:09:21,991
Dann nimmst du alle Aktivierungen aus der ersten Schicht und 

153
00:09:21,991 --> 00:09:25,500
berechnest ihre gewichtete Summe entsprechend dieser Gewichte.

154
00:09:27,700 --> 00:09:32,393
Ich finde es hilfreich, wenn du dir diese Gewichte in einem eigenen kleinen Raster 

155
00:09:32,393 --> 00:09:37,143
vorstellst. Ich werde grüne Pixel für positive Gewichte und rote Pixel für negative 

156
00:09:37,143 --> 00:09:41,780
Gewichte verwenden, wobei die Helligkeit des Pixels den Wert des Gewichts anzeigt.

157
00:09:42,780 --> 00:09:46,070
Wenn wir nun die Gewichte für fast alle Pixel auf Null setzen, 

158
00:09:46,070 --> 00:09:50,091
mit Ausnahme einiger positiver Gewichte in der Region, die uns interessiert, 

159
00:09:50,091 --> 00:09:54,216
dann läuft die gewichtete Summe aller Pixelwerte eigentlich nur darauf hinaus, 

160
00:09:54,216 --> 00:09:57,820
die Werte der Pixel in der Region, die uns interessiert, zu addieren.

161
00:09:59,140 --> 00:10:03,096
Und wenn du wirklich herausfinden willst, ob es hier eine Kante gibt, 

162
00:10:03,096 --> 00:10:06,600
könntest du den umliegenden Pixeln negative Gewichte zuweisen.

163
00:10:07,480 --> 00:10:10,652
Dann ist die Summe am größten, wenn die mittleren Pixel hell, 

164
00:10:10,652 --> 00:10:12,700
die umliegenden Pixel aber dunkler sind.

165
00:10:14,260 --> 00:10:16,911
Wenn du eine gewichtete Summe wie diese berechnest, 

166
00:10:16,911 --> 00:10:20,633
kannst du eine beliebige Zahl erhalten, aber für dieses Netz wollen wir, 

167
00:10:20,633 --> 00:10:23,540
dass die Aktivierungen einen Wert zwischen 0 und 1 haben.

168
00:10:24,120 --> 00:10:28,299
Deshalb ist es üblich, diese gewichtete Summe in eine Funktion zu pumpen, 

169
00:10:28,299 --> 00:10:32,140
die die reelle Zahlenreihe in den Bereich zwischen 0 und 1 quetscht.

170
00:10:32,460 --> 00:10:35,630
Eine gängige Funktion, die dies tut, ist die Sigmoidfunktion, 

171
00:10:35,630 --> 00:10:37,420
auch bekannt als logistische Kurve.

172
00:10:38,000 --> 00:10:41,486
Im Grunde genommen enden sehr negative Eingaben nahe bei 0, 

173
00:10:41,486 --> 00:10:46,600
positive Eingaben enden nahe bei 1 und der Wert steigt um die Eingabe 0 herum stetig an.

174
00:10:49,120 --> 00:10:53,230
Die Aktivierung des Neurons ist hier also im Grunde ein Maß dafür, 

175
00:10:53,230 --> 00:10:56,360
wie positiv die entsprechende gewichtete Summe ist.

176
00:10:57,540 --> 00:11:00,102
Aber vielleicht willst du nicht, dass das Neuron aufleuchtet, 

177
00:11:00,102 --> 00:11:01,880
wenn die gewichtete Summe größer als 0 ist.

178
00:11:02,280 --> 00:11:06,360
Vielleicht möchtest du, dass sie nur aktiv ist, wenn die Summe größer als 10 ist.

179
00:11:06,840 --> 00:11:10,260
Das heißt, du willst, dass es inaktiv ist.

180
00:11:11,380 --> 00:11:15,441
Dann fügen wir dieser gewichteten Summe einfach eine andere Zahl hinzu, z. B. 

181
00:11:15,441 --> 00:11:19,660
eine negative 10, bevor wir sie in die Sigmoid-Squishification-Funktion einfügen.

182
00:11:20,580 --> 00:11:22,440
Diese zusätzliche Zahl wird als Verzerrung bezeichnet.

183
00:11:23,460 --> 00:11:27,386
Die Gewichte sagen dir also, welches Pixelmuster dieses Neuron in 

184
00:11:27,386 --> 00:11:30,539
der zweiten Schicht aufnimmt, und der Bias sagt dir, 

185
00:11:30,539 --> 00:11:35,180
wie hoch die gewichtete Summe sein muss, damit das Neuron sinnvoll aktiv wird.

186
00:11:36,120 --> 00:11:37,680
Und das ist nur ein Neuron.

187
00:11:38,280 --> 00:11:44,413
Jedes andere Neuron in dieser Schicht ist mit allen 784 Pixelneuronen aus der 

188
00:11:44,413 --> 00:11:50,940
ersten Schicht verbunden, und jede dieser 784 Verbindungen hat ihr eigenes Gewicht.

189
00:11:51,600 --> 00:11:54,225
Außerdem hat jede von ihnen eine Verzerrung, eine andere Zahl, 

190
00:11:54,225 --> 00:11:57,600
die du zur gewichteten Summe hinzufügst, bevor du sie mit dem Sigmoid zerdrückst.

191
00:11:58,110 --> 00:11:59,540
Und das ist eine Menge, über die man nachdenken muss!

192
00:11:59,960 --> 00:12:03,894
Bei dieser versteckten Schicht mit 16 Neuronen sind 

193
00:12:03,894 --> 00:12:07,980
das insgesamt 784 mal 16 Gewichte und 16 Verzerrungen.

194
00:12:08,840 --> 00:12:11,940
Und all das sind nur die Verbindungen von der ersten zur zweiten Schicht.

195
00:12:12,520 --> 00:12:14,874
Die Verbindungen zwischen den anderen Schichten haben auch eine 

196
00:12:14,874 --> 00:12:17,340
Reihe von Gewichten und Verzerrungen, die mit ihnen verbunden sind.

197
00:12:18,340 --> 00:12:23,800
Alles in allem hat dieses Netzwerk fast genau 13.000 Gewichte und Verzerrungen.

198
00:12:23,800 --> 00:12:27,141
13.000 Knöpfe und Regler, an denen du drehen und wenden kannst, 

199
00:12:27,141 --> 00:12:29,960
um das Netzwerk auf unterschiedliche Weise zu steuern.

200
00:12:31,040 --> 00:12:34,971
Wenn wir also von Lernen sprechen, geht es darum, den Computer dazu zu bringen, 

201
00:12:34,971 --> 00:12:38,509
eine gültige Einstellung für all diese vielen, vielen Zahlen zu finden, 

202
00:12:38,509 --> 00:12:41,360
so dass er das vorliegende Problem tatsächlich lösen kann.

203
00:12:42,620 --> 00:12:46,295
Ein Gedankenexperiment, das gleichzeitig Spaß macht und irgendwie erschreckend ist, 

204
00:12:46,295 --> 00:12:49,534
ist die Vorstellung, dass du dich hinsetzt und all diese Gewichtungen und 

205
00:12:49,534 --> 00:12:53,079
Verzerrungen von Hand einstellst, indem du die Zahlen absichtlich so veränderst, 

206
00:12:53,079 --> 00:12:56,580
dass die zweite Schicht die Kanten aufnimmt, die dritte Schicht die Muster, usw.

207
00:12:56,980 --> 00:13:00,937
Ich persönlich finde das befriedigender, als das Netzwerk als totale Blackbox zu 

208
00:13:00,937 --> 00:13:04,993
behandeln, denn wenn das Netzwerk nicht so funktioniert, wie du es dir vorstellst, 

209
00:13:04,993 --> 00:13:09,342
hast du, wenn du eine kleine Beziehung zu den Gewichten und Verzerrungen aufgebaut hast, 

210
00:13:09,342 --> 00:13:13,153
eine Ausgangsbasis, um zu experimentieren, wie du die Struktur ändern kannst, 

211
00:13:13,153 --> 00:13:14,180
um sie zu verbessern.

212
00:13:14,960 --> 00:13:18,669
Wenn das Netzwerk zwar funktioniert, aber nicht aus den Gründen, die du erwartest, 

213
00:13:18,669 --> 00:13:21,976
ist die Untersuchung der Gewichte und Verzerrungen eine gute Möglichkeit, 

214
00:13:21,976 --> 00:13:25,820
deine Annahmen in Frage zu stellen und den ganzen Raum möglicher Lösungen aufzudecken.

215
00:13:26,840 --> 00:13:29,985
Übrigens ist die eigentliche Funktion hier etwas umständlich aufzuschreiben, 

216
00:13:29,985 --> 00:13:30,680
findest du nicht?

217
00:13:32,500 --> 00:13:37,140
Ich zeige dir jetzt eine kompaktere Art, diese Verbindungen darzustellen.

218
00:13:37,660 --> 00:13:40,520
So würdest du es sehen, wenn du mehr über neuronale Netze nachlesen würdest.

219
00:13:41,380 --> 00:13:47,104
Organisiere alle Aktivierungen einer Schicht in einer Spalte, 

220
00:13:47,104 --> 00:13:55,414
da eine Matrix den Verbindungen zwischen einer Schicht und einem bestimmten Neuron in der 

221
00:13:55,414 --> 00:13:58,000
nächsten Schicht entspricht.

222
00:13:58,540 --> 00:14:02,391
Das bedeutet, dass die gewichtete Summe der Aktivierungen in der ersten 

223
00:14:02,391 --> 00:14:07,098
Schicht entsprechend dieser Gewichte einem der Terme im Matrix-Vektorprodukt von allem, 

224
00:14:07,098 --> 00:14:09,880
was wir hier auf der linken Seite haben, entspricht.

225
00:14:14,000 --> 00:14:17,846
Übrigens: Ein Großteil des maschinellen Lernens beruht auf einem guten Verständnis 

226
00:14:17,846 --> 00:14:21,369
der linearen Algebra. Wer also ein gutes visuelles Verständnis für Matrizen 

227
00:14:21,369 --> 00:14:24,382
und die Bedeutung der Matrix-Vektor-Multiplikation haben möchte, 

228
00:14:24,382 --> 00:14:27,348
sollte einen Blick auf meine Serie über lineare Algebra werfen, 

229
00:14:27,348 --> 00:14:28,600
insbesondere auf Kapitel 3.

230
00:14:29,240 --> 00:14:33,633
Zurück zu unserem Ausdruck: Anstatt die Verzerrung zu jedem dieser Werte 

231
00:14:33,633 --> 00:14:38,147
einzeln zu addieren, fassen wir alle Verzerrungen in einem Vektor zusammen 

232
00:14:38,147 --> 00:14:42,300
und addieren den gesamten Vektor zum vorherigen Matrix-Vektorprodukt.

233
00:14:43,280 --> 00:14:47,300
Als letzten Schritt wickle ich hier ein Sigmoid um die Außenseite. 

234
00:14:47,300 --> 00:14:51,199
Das soll bedeuten, dass du die Sigmoidfunktion auf jede einzelne 

235
00:14:51,199 --> 00:14:54,740
Komponente des resultierenden Vektors im Inneren anwendest.

236
00:14:55,940 --> 00:15:00,137
Wenn du also diese Gewichtsmatrix und diese Vektoren als eigene Symbole aufschreibst, 

237
00:15:00,137 --> 00:15:04,091
kannst du den gesamten Übergang der Aktivierungen von einer Schicht zur nächsten 

238
00:15:04,091 --> 00:15:07,801
in einem extrem knappen und übersichtlichen kleinen Ausdruck kommunizieren, 

239
00:15:07,801 --> 00:15:11,950
und das macht den entsprechenden Code sowohl viel einfacher als auch viel schneller, 

240
00:15:11,950 --> 00:15:15,660
da viele Bibliotheken die Matrixmultiplikation bis aufs Äußerste optimieren.

241
00:15:17,820 --> 00:15:19,529
Erinnerst du dich daran, dass ich vorhin gesagt habe, 

242
00:15:19,529 --> 00:15:21,460
dass diese Neuronen einfach Dinge sind, die Zahlen enthalten?

243
00:15:22,220 --> 00:15:28,959
Deshalb ist es genauer, sich jedes Neuron als eine Funktion vorzustellen, 

244
00:15:28,959 --> 00:15:36,882
die die Ausgaben aller Neuronen der vorherigen Schicht aufnimmt und eine Zahl zwischen 

245
00:15:36,882 --> 00:15:38,340
0 und 1 ausgibt.

246
00:15:39,200 --> 00:15:42,658
Das gesamte Netzwerk ist eigentlich nur eine Funktion, 

247
00:15:42,658 --> 00:15:47,060
die 784 Zahlen als Eingabe erhält und 10 Zahlen als Ausgabe ausspuckt.

248
00:15:47,560 --> 00:15:52,265
Es ist eine absurd komplizierte Funktion mit 13.000 Parametern in Form von Gewichten 

249
00:15:52,265 --> 00:15:55,089
und Verzerrungen, die bestimmte Muster aufgreifen, 

250
00:15:55,089 --> 00:15:58,909
und die die Iteration vieler Matrix-Vektor-Produkte und die sigmoide 

251
00:15:58,909 --> 00:16:03,172
Squishification-Funktion beinhaltet, aber es ist trotzdem nur eine Funktion, 

252
00:16:03,172 --> 00:16:06,660
und irgendwie ist es beruhigend, dass sie kompliziert aussieht.

253
00:16:07,340 --> 00:16:09,827
Ich meine, wenn es noch einfacher wäre, welche Hoffnung hätten wir dann, 

254
00:16:09,827 --> 00:16:12,280
dass es die Herausforderung des Erkennens von Ziffern bewältigen könnte?

255
00:16:13,340 --> 00:16:14,700
Und wie nimmt sie diese Herausforderung an?

256
00:16:15,080 --> 00:16:17,918
Wie lernt dieses Netz die richtigen Gewichte und Verzerrungen, 

257
00:16:17,918 --> 00:16:19,360
indem es sich die Daten ansieht?

258
00:16:20,140 --> 00:16:23,225
Das zeige ich dir im nächsten Video, und ich werde auch ein bisschen mehr darauf 

259
00:16:23,225 --> 00:16:26,120
eingehen, was dieses spezielle Netzwerk, das wir hier sehen, wirklich macht.

260
00:16:27,580 --> 00:16:31,348
Jetzt sollte ich wohl sagen, dass du dich anmelden sollst, damit du benachrichtigt wirst, 

261
00:16:31,348 --> 00:16:34,740
wenn ein Video oder neue Videos erscheinen, aber realistisch betrachtet erhalten 

262
00:16:34,740 --> 00:16:37,420
die meisten von euch keine Benachrichtigungen von YouTube, oder?

263
00:16:38,020 --> 00:16:41,481
Vielleicht sollte ich ehrlicher sagen: Abonnieren, damit die neuronalen Netze, 

264
00:16:41,481 --> 00:16:45,206
die dem Empfehlungsalgorithmus von YouTube zugrunde liegen, darauf eingestellt sind, 

265
00:16:45,206 --> 00:16:47,880
dass du Inhalte von diesem Kanal empfohlen bekommen möchtest.

266
00:16:48,560 --> 00:16:49,940
Bleib auf jeden Fall auf dem Laufenden für mehr.

267
00:16:50,760 --> 00:16:53,500
Vielen Dank an alle, die diese Videos auf Patreon unterstützen.

268
00:16:54,000 --> 00:16:57,571
In der Wahrscheinlichkeitsreihe bin ich diesen Sommer etwas langsamer vorangekommen, 

269
00:16:57,571 --> 00:16:59,840
aber ich werde nach diesem Projekt wieder einsteigen, 

270
00:16:59,840 --> 00:17:01,900
also kannst du dort nach Updates Ausschau halten.

271
00:17:03,600 --> 00:17:06,431
Zum Abschluss habe ich Leisha Lee bei mir, die sich in ihrer Doktorarbeit 

272
00:17:06,431 --> 00:17:09,263
mit der theoretischen Seite des Deep Learning beschäftigt hat und derzeit 

273
00:17:09,263 --> 00:17:11,329
bei der Risikokapitalfirma Amplify Partners arbeitet, 

274
00:17:11,329 --> 00:17:14,619
die freundlicherweise einen Teil der Finanzierung für dieses Video bereitgestellt hat.

275
00:17:15,460 --> 00:17:19,119
Leisha, eine Sache, die wir schnell erwähnen sollten, ist die sigmoide Funktion.

276
00:17:19,700 --> 00:17:21,958
So wie ich es verstehe, nutzen frühe Netzwerke dies, 

277
00:17:21,958 --> 00:17:25,622
um die relevante gewichtete Summe in das Intervall zwischen Null und Eins zu pressen, 

278
00:17:25,622 --> 00:17:27,837
sozusagen motiviert durch die biologische Analogie, 

279
00:17:27,837 --> 00:17:29,840
dass Neuronen entweder inaktiv oder aktiv sind.

280
00:17:30,280 --> 00:17:30,300
Ganz genau.

281
00:17:30,560 --> 00:17:34,040
Aber nur noch relativ wenige moderne Netze verwenden das Sigmoid.

282
00:17:34,320 --> 00:17:34,320
Ja.

283
00:17:34,440 --> 00:17:35,540
Das ist doch ganz schön altmodisch, oder?

284
00:17:35,760 --> 00:17:38,980
Ja, oder besser gesagt, Relu scheint viel einfacher zu trainieren zu sein.

285
00:17:39,400 --> 00:17:42,340
Und relu steht für rectified linear unit?

286
00:17:42,680 --> 00:17:47,394
Ja, es ist diese Art von Funktion, bei der du einfach einen Maximalwert aus 

287
00:17:47,394 --> 00:17:52,356
Null und A nimmst, wobei A durch das gegeben ist, was du im Video erklärt hast. 

288
00:17:52,356 --> 00:17:57,256
Ich glaube, das wurde zum Teil durch eine biologische Analogie dazu motiviert, 

289
00:17:57,256 --> 00:18:00,419
wie Neuronen entweder aktiviert werden oder nicht, 

290
00:18:00,419 --> 00:18:04,079
und wenn sie einen bestimmten Schwellenwert überschreiten, 

291
00:18:04,079 --> 00:18:07,118
wäre es die Identitätsfunktion, aber wenn nicht, 

292
00:18:07,118 --> 00:18:10,840
würde sie einfach nicht aktiviert werden, also wäre es Null.

293
00:18:11,160 --> 00:18:15,856
Die Verwendung von Sigmoiden hat beim Training nicht geholfen oder war irgendwann 

294
00:18:15,856 --> 00:18:20,152
sehr schwierig zu trainieren und die Leute haben einfach Relu ausprobiert, 

295
00:18:20,152 --> 00:18:24,620
was bei diesen unglaublich tiefen neuronalen Netzen sehr gut funktioniert hat.

296
00:18:25,100 --> 00:18:25,640
Vielen Dank, Alicia.

