1
00:00:04,020 --> 00:00:10,680
To jest trójka. Jest niedbale napisana i renderowana w wyjątkowo niskiej rozdzielczości 28 na 28 pikseli.

2
00:00:10,680 --> 00:00:15,660
Ale twój mózg nie ma problemu z rozpoznaniem jej jako trójki i chce, żebyś poświęcił chwilę, by to docenić

3
00:00:15,900 --> 00:00:18,949
Jak szalone jest to, że mózg może robić to bez wysiłku?

4
00:00:18,949 --> 00:00:23,160
Mam na myśli to, że to i to są tak samo rozpoznawalne jako trójka,

5
00:00:23,160 --> 00:00:28,060
nawet jeśli konkretne wartości każdego piksela różnią się bardzo, jeden obraz od drugiego.

6
00:00:28,080 --> 00:00:33,780
Szczególne wrażliwe na światło komórki w twoim oku, które strzelają, kiedy widzisz tę trójkę

7
00:00:33,780 --> 00:00:36,800
bardzo różnią się od tych strzelających, gdy widzisz tę trójkę.

8
00:00:37,140 --> 00:00:40,610
Ale coś w tej szalonej, inteligentnej kory wzrokowej

9
00:00:41,129 --> 00:00:48,139
rozpatruje je jako reprezentujące ten sam pomysł, a jednocześnie rozpoznaje inne obrazy jako własne odrębne pomysły

10
00:00:48,840 --> 00:00:55,039
Ale gdybym ci powiedział, hej usiądź i napisz mi program, który ma siatkę 28 na 28

11
00:00:55,379 --> 00:01:01,759
takie piksele i wyprowadza pojedynczą liczbę z przedziału od 0 do 10, informującą o tym, co uważa za cyfrę

12
00:01:02,250 --> 00:01:06,139
Cóż, zadanie staje się od komedii trywialnej do zniechęcająco trudnej

13
00:01:06,750 --> 00:01:08,270
Chyba, że ​​żyjesz pod kamieniem

14
00:01:08,270 --> 00:01:14,599
Myślę, że nie muszę motywować znaczenia uczenia maszynowego i sieci neuronowych do teraźniejszości w przyszłości

15
00:01:14,640 --> 00:01:18,410
Ale to, co chcę tutaj zrobić, to pokazać, czym właściwie jest sieć neuronowa

16
00:01:18,660 --> 00:01:24,229
Zakładając, że nie ma tła i nie wyobrażamy sobie, co robi, nie jako modne słowo, ale jako element matematyki

17
00:01:24,570 --> 00:01:28,310
Mam nadzieję, że po prostu odejdziesz, czując, że ta struktura sama w sobie jest

18
00:01:28,380 --> 00:01:34,399
Zmotywowani i czujący, jakbyś wiedział, co to znaczy, kiedy czytasz lub słyszysz o sieci neuronowej - nie licz na naukę

19
00:01:34,950 --> 00:01:40,249
Ten film będzie poświęcony komponentowi struktury tego, a następny będzie dotyczył uczenia się

20
00:01:40,530 --> 00:01:45,950
Zamierzamy stworzyć sieć neuronową, która nauczy się rozpoznawać ręcznie pisane cyfry

21
00:01:49,270 --> 00:01:51,329
Jest to dość klasyczny przykład dla

22
00:01:51,520 --> 00:01:56,759
Przedstawiam temat i cieszę się, że trzymam tutaj status quo, ponieważ na końcu tych dwóch filmów chcę wskazać

23
00:01:56,760 --> 00:02:02,099
Jesteś parą dobrych zasobów, gdzie możesz dowiedzieć się więcej i gdzie możesz pobrać kod, który to robi i grać z nim?

24
00:02:02,100 --> 00:02:04,100
na własnym komputerze

25
00:02:04,750 --> 00:02:08,970
Istnieje wiele różnych wariantów sieci neuronowych iw ostatnich latach

26
00:02:08,970 --> 00:02:11,970
Nastąpił pewien boom w badaniach nad tymi wariantami

27
00:02:12,130 --> 00:02:19,019
Ale w tych dwóch filmach wprowadzających, ty i ja będziemy patrzeć na najprostszą formę czysto-waniliową bez dodanych dodatków

28
00:02:19,300 --> 00:02:21,040
Jest to konieczne

29
00:02:21,040 --> 00:02:24,510
warunkiem wstępnym zrozumienia któregokolwiek z potężniejszych nowoczesnych wariantów i

30
00:02:24,760 --> 00:02:28,199
Uwierzcie mi, że wciąż ma on wiele komplikacji, abyśmy mogli otoczyć nasze umysły

31
00:02:28,690 --> 00:02:32,820
Ale nawet w tej najprostszej formie może nauczyć się rozpoznawać ręcznie pisane cyfry

32
00:02:32,820 --> 00:02:36,180
Co jest całkiem fajnym rozwiązaniem dla komputera.

33
00:02:37,120 --> 00:02:41,960
A jednocześnie zobaczysz, jak to się nie udaje, że mamy parę nadziei na to

34
00:02:43,090 --> 00:02:48,179
Jak sama nazwa wskazuje sieci neuronowe są inspirowane przez mózg, ale załóżmy to

35
00:02:48,520 --> 00:02:51,389
Jakie są neurony iw jakim sensie są ze sobą powiązane?

36
00:02:52,090 --> 00:02:57,750
Teraz, kiedy mówię neuron, wszystko, co chcę, żebyś pomyślał, to rzecz, która ma numer

37
00:02:58,209 --> 00:03:02,129
W szczególności liczba między 0 a 1 to naprawdę nie więcej

38
00:03:03,430 --> 00:03:11,130
Na przykład sieć zaczyna się od garści neuronów odpowiadających każdemu z 28 razy 28 pikseli obrazu wejściowego

39
00:03:11,400 --> 00:03:12,460
który jest

40
00:03:12,460 --> 00:03:20,240
Wszystkich 784 neuronów w każdym z nich znajduje się liczba reprezentująca wartość skali szarości odpowiedniego piksela

41
00:03:20,769 --> 00:03:24,299
od 0 dla czarnych pikseli do 1 dla białych pikseli

42
00:03:24,910 --> 00:03:30,419
Ta liczba wewnątrz neuronu jest nazywana jego aktywacją i obrazem, który możesz mieć tutaj na myśli

43
00:03:30,420 --> 00:03:33,959
Czy każdy neuron jest oświetlony, gdy jego aktywacja jest duża?

44
00:03:36,260 --> 00:03:41,559
Zatem wszystkie te 784 neurony tworzą pierwszą warstwę naszej sieci

45
00:03:45,990 --> 00:03:51,289
Teraz przeskakując do ostatniej warstwy, ma dziesięć neuronów, z których każdy reprezentuje jedną z cyfr

46
00:03:51,570 --> 00:03:56,239
aktywacja w tych neuronach znowu kilka liczb, które są między zero i jeden

47
00:03:56,880 --> 00:04:00,049
Reprezentuje, jak bardzo system myśli, że dany obraz?

48
00:04:00,720 --> 00:04:05,990
Odpowiada danej cyfrze. Istnieje również kilka warstw pomiędzy warstwami

49
00:04:06,180 --> 00:04:07,770
Który na razie?

50
00:04:07,770 --> 00:04:13,549
Powinien być po prostu olbrzymim znakiem zapytania, jak na Ziemi zostanie potraktowany proces rozpoznawania cyfr

51
00:04:13,740 --> 00:04:20,209
W tej sieci wybrałem dwie ukryte warstwy, każda z 16 neuronami i oczywiście jest to rodzaj arbitralnego wyboru

52
00:04:20,609 --> 00:04:24,889
szczerze mówiąc wybrałem dwie warstwy w oparciu o to, jak chcę zmotywować strukturę w jednej chwili i

53
00:04:25,350 --> 00:04:29,179
16-studzienkowe to była po prostu fajna liczba, którą można dopasować na ekranie w praktyce

54
00:04:29,180 --> 00:04:32,209
Jest tu dużo miejsca na eksperymentowanie z określoną strukturą

55
00:04:32,730 --> 00:04:38,329
Sposób, w jaki sieć działa aktywacje w jednej warstwie, determinuje aktywacje następnej warstwy

56
00:04:38,760 --> 00:04:45,349
I oczywiście serce sieci jako mechanizm przetwarzania informacji sprowadza się do dokładnie tego, jak te

57
00:04:45,570 --> 00:04:48,409
Aktywacje z jednej warstwy powodują aktywacje w następnej warstwie

58
00:04:48,900 --> 00:04:54,859
To ma być luźno analogiczne do tego, jak w sieciach biologicznych neuronów strzelają niektóre grupy neuronów

59
00:04:55,410 --> 00:04:57,410
spowodować, że inni wystrzelą

60
00:04:57,570 --> 00:04:58,340
Teraz sieć

61
00:04:58,340 --> 00:05:03,019
Pokazuję tutaj został już przeszkolony do rozpoznawania cyfr i pozwól mi pokazać, co mam na myśli przez to

62
00:05:03,140 --> 00:05:06,580
Oznacza to, że jeśli dodasz obraz, rozświetlisz wszystko

63
00:05:06,640 --> 00:05:11,780
784 neuronów warstwy wejściowej zgodnie z jasnością każdego piksela na obrazie

64
00:05:12,330 --> 00:05:17,029
Ten wzorzec aktywacji powoduje bardzo specyficzny wzór w następnej warstwie

65
00:05:17,190 --> 00:05:19,309
Co powoduje pewien wzorzec w następnym?

66
00:05:19,440 --> 00:05:22,190
Który ostatecznie daje pewien wzór w warstwie wyjściowej i?

67
00:05:22,350 --> 00:05:29,359
Najjaśniejszym neuronem tej warstwy wyjściowej jest wybór sieci, a więc jaka cyfra reprezentuje ten obraz?

68
00:05:32,070 --> 00:05:36,859
A przed wskoczeniem do matematyki, jak jedna warstwa wpływa na następną lub jak działa trening?

69
00:05:37,140 --> 00:05:43,069
Porozmawiajmy tylko o tym, dlaczego rozsądnie jest oczekiwać inteligentnego zachowania się w warstwowej strukturze

70
00:05:43,800 --> 00:05:48,260
Czego spodziewamy się tutaj? Jaka jest najlepsza nadzieja na to, co robią te środkowe warstwy?

71
00:05:48,860 --> 00:05:56,720
Cóż, kiedy rozpoznajemy cyfry, łączymy różne komponenty, a dziewięć ma pętlę na górze i linię po prawej

72
00:05:57,260 --> 00:06:01,280
8 ma również pętlę do góry, ale jest sparowana z kolejną niską pętlą

73
00:06:02,020 --> 00:06:06,599
A 4 zasadniczo dzieli się na trzy konkretne linie i takie rzeczy

74
00:06:07,180 --> 00:06:11,970
Teraz w idealnym świecie możemy mieć nadzieję, że każdy neuron w drugiej i ostatniej warstwie

75
00:06:12,640 --> 00:06:14,729
odpowiada jednemu z tych podelementów

76
00:06:14,890 --> 00:06:19,740
To zawsze, gdy karmisz obraz, powiedzmy, że pętla jest na górze jak 9 lub 8

77
00:06:19,870 --> 00:06:21,220
Jest jakiś konkretny

78
00:06:21,220 --> 00:06:27,749
Neuron, którego aktywacja będzie zbliżona do jednej i nie mam na myśli tej konkretnej pętli pikseli, która byłaby nadzieją, że jakakolwiek

79
00:06:28,090 --> 00:06:35,039
Zasadniczo, pętla w kierunku góry powoduje, że neuron przechodzi z trzeciej warstwy na ostatnią

80
00:06:35,380 --> 00:06:39,960
wymaga jedynie nauki, która kombinacja pod-elementów odpowiada której cyfrze

81
00:06:40,510 --> 00:06:42,810
Oczywiście, że po prostu kopie problem w dół drogi

82
00:06:42,910 --> 00:06:49,019
Bo jak rozpoznałbyś te podskładniki, a nawet dowiedziałbyś się, jakie powinny być właściwe pod-komponenty, a ja wciąż jeszcze o tym nie rozmawiałem

83
00:06:49,020 --> 00:06:52,829
Jak jedna warstwa wpływa na następną, ale działa ze mną przez chwilę

84
00:06:53,650 --> 00:06:56,340
rozpoznanie pętli może również rozbić się na podproblemy

85
00:06:56,860 --> 00:07:02,550
Jednym rozsądnym sposobem na to byłoby najpierw rozpoznać różne małe krawędzie, które go tworzą

86
00:07:03,520 --> 00:07:08,910
Podobnie długa linia, taka jak ta, którą możesz zobaczyć w cyfrach 1, 4 lub 7

87
00:07:08,910 --> 00:07:14,279
Cóż, to naprawdę długa krawędź, a może uważasz ją za pewien wzór kilku mniejszych krawędzi

88
00:07:14,740 --> 00:07:19,379
Więc może naszą nadzieją jest to, że każdy neuron w drugiej warstwie sieci

89
00:07:20,290 --> 00:07:22,650
odpowiada różnym odpowiednim małym krawędziom

90
00:07:23,230 --> 00:07:28,259
Może kiedy taki obraz pojawi się w środku, zapali wszystkie neurony

91
00:07:28,720 --> 00:07:31,649
związane z około ośmiu do dziesięciu określonych małych krawędzi

92
00:07:31,930 --> 00:07:36,930
co z kolei włącza neurony powiązane z górną pętlą i długą pionową linią i

93
00:07:37,300 --> 00:07:39,599
Te rozświetlają neuron związany z dziewięcioma

94
00:07:40,300 --> 00:07:41,100
niezależnie od tego, czy

95
00:07:41,100 --> 00:07:47,070
Tak właśnie działa nasza ostatnia sieć, to kolejne pytanie, do którego wrócę, gdy zobaczymy, jak szkolić sieć

96
00:07:47,350 --> 00:07:52,170
Ale to jest nadzieja, którą możemy mieć. Rodzaj celu z warstwową strukturą w ten sposób

97
00:07:53,020 --> 00:07:59,340
Co więcej, możesz sobie wyobrazić, że rozpoznawanie krawędzi i wzorów może być przydatne w innych zadaniach związanych z rozpoznawaniem obrazów

98
00:07:59,740 --> 00:08:06,749
A nawet poza rozpoznawaniem obrazów istnieją różne inteligentne rzeczy, które możesz chcieć zrobić, które rozpadają się na warstwy abstrakcji

99
00:08:07,690 --> 00:08:14,670
Parsowanie mowy wymaga na przykład robienia surowego dźwięku i wybierania odrębnych dźwięków, które łączą się, tworząc pewne sylaby

100
00:08:15,070 --> 00:08:19,829
Które łączą się, tworząc słowa, które łączą się w frazę i bardziej abstrakcyjne myśli itp

101
00:08:20,770 --> 00:08:25,710
Ale wracając do tego, jak to wszystko działa, wyobraź sobie teraz projektowanie

102
00:08:25,710 --> 00:08:30,449
Jak dokładnie aktywacje w jednej warstwie mogą decydować o aktywacjach w następnej?

103
00:08:30,670 --> 00:08:35,879
Celem jest posiadanie mechanizmu, który mógłby łączyć piksele w krawędzie

104
00:08:35,880 --> 00:08:41,430
Lub krawędzie we wzorce lub wzory na cyfry i powiększenie jednego bardzo konkretnego przykładu

105
00:08:41,950 --> 00:08:44,189
Powiedzmy, że nadzieja dotyczy jednego konkretnego

106
00:08:44,380 --> 00:08:50,430
Neuron w drugiej warstwie, aby sprawdzić, czy obraz ma tu krawędź w tym regionie

107
00:08:50,950 --> 00:08:54,960
Pytanie, jakie parametry powinna mieć sieć

108
00:08:55,270 --> 00:09:02,490
jakie pokrętła i pokrętła powinny być w stanie dostroić, tak aby był wystarczająco ekspresyjny, aby potencjalnie uchwycić ten wzór lub

109
00:09:02,590 --> 00:09:07,290
Każdy inny wzór pikseli lub wzór, który kilka krawędzi może zrobić pętlę i inne podobne rzeczy?

110
00:09:08,290 --> 00:09:15,389
Cóż, to co zrobimy, to przypisać wagę do każdego połączenia między naszym neuronem a neuronami z pierwszej warstwy

111
00:09:15,850 --> 00:09:17,850
Te ciężary to tylko liczby

112
00:09:18,190 --> 00:09:25,590
następnie weź wszystkie te aktywacje z pierwszej warstwy i obliczaj ich ważoną sumę zgodnie z tymi wagami I

113
00:09:27,370 --> 00:09:31,680
Pomyśl o tym, że te wagi są zorganizowane w małą siatkę

114
00:09:31,680 --> 00:09:37,079
I zamierzam użyć zielonych pikseli, aby wskazać pozytywne ciężary i czerwone piksele wskazujące ujemne wagi

115
00:09:37,240 --> 00:09:41,670
Gdzie jasność tego piksela stanowi jakiś luźny obraz wartości ciężaru?

116
00:09:42,400 --> 00:09:45,840
Teraz, jeśli zrobimy wagi powiązane z prawie wszystkimi pikselami zero

117
00:09:46,150 --> 00:09:49,079
z wyjątkiem niektórych pozytywnych wag w tym regionie, na których nam zależy

118
00:09:49,480 --> 00:09:51,310
następnie biorąc ważoną sumę

119
00:09:51,310 --> 00:09:57,690
wszystkie wartości pikseli są po prostu sumujące wartości pikseli w regionie, który nas interesuje

120
00:09:58,870 --> 00:10:04,440
A jeśli naprawdę chcesz, żeby się dowiedział, czy jest tu coś, co możesz zrobić, to masz ujemne wagi

121
00:10:04,900 --> 00:10:06,900
związane z otaczającymi pikselami

122
00:10:07,030 --> 00:10:12,660
Wtedy suma jest największa, gdy te środkowe piksele są jasne, ale otaczające piksele są ciemniejsze

123
00:10:14,279 --> 00:10:18,169
Kiedy obliczysz ważoną sumę w ten sposób, możesz wyjść z dowolnym numerem

124
00:10:18,240 --> 00:10:23,180
ale dla tej sieci potrzebujemy, aby aktywacje miały pewną wartość między 0 a 1

125
00:10:23,730 --> 00:10:26,599
więc powszechną rzeczą jest pompowanie tej ważonej sumy

126
00:10:26,910 --> 00:10:32,000
Do funkcji, która redukuje rzeczywistą linię liczbową do zakresu pomiędzy 0 a 1 i

127
00:10:32,190 --> 00:10:37,249
Typową funkcją, która to robi, jest funkcja sigmoidowa, znana również jako krzywa logistyczna

128
00:10:37,980 --> 00:10:43,339
zasadniczo bardzo ujemne dane wejściowe kończą się blisko zera bardzo pozytywne wejścia kończą się blisko 1

129
00:10:43,339 --> 00:10:46,398
i stale wzrasta wokół wejścia 0

130
00:10:49,080 --> 00:10:56,029
Tak więc aktywacja neuronu jest zasadniczo miarą tego, jak pozytywna jest odpowiednia ważona suma

131
00:10:57,450 --> 00:11:01,819
Ale może to nie jest tak, że chcesz, aby neuron zapalił się, gdy ważona suma jest większa niż 0

132
00:11:02,100 --> 00:11:06,260
Może chcesz, aby była aktywna tylko wtedy, gdy suma jest większa niż 10

133
00:11:06,630 --> 00:11:10,279
To jest coś, co chcesz, żeby było nieaktywne

134
00:11:10,860 --> 00:11:16,099
to, co zrobimy, to po prostu dodaj inną liczbę, na przykład ujemną 10, do tej ważonej sumy

135
00:11:16,529 --> 00:11:19,669
Przed podłączeniem go przez funkcję esmozy

136
00:11:20,220 --> 00:11:22,730
Ta dodatkowa liczba nazywa się uprzedzeniem

137
00:11:23,310 --> 00:11:29,060
Ciężary mówią ci, jaki wzorzec pikseli ten neuron w drugiej warstwie odbiera i jest nastawiony

138
00:11:29,220 --> 00:11:35,450
mówi, jak wysoka musi być ważona suma, zanim neuron zacznie być znacząco aktywny

139
00:11:35,910 --> 00:11:37,910
A to tylko jeden neuron

140
00:11:38,120 --> 00:11:41,940
Każdy inny neuron w tej warstwie zostanie połączony ze wszystkimi

141
00:11:42,320 --> 00:11:50,620
784 piksele neuronów z pierwszej warstwy i każde z tych 784 połączeń ma przypisaną własną wagę

142
00:11:51,330 --> 00:11:57,739
również każdy ma jakieś odchylenie, jakąś inną liczbę, którą dodajesz do sumy ważonej, przed zgniataniem jej sigmoidą i

143
00:11:58,020 --> 00:12:01,909
O tym ukrytym warstwie 16 neuronów trzeba się dużo zastanowić

144
00:12:02,010 --> 00:12:08,270
to łącznie 784 razy 16 ciężarów wraz z 16 uprzedzeniami

145
00:12:08,490 --> 00:12:14,029
A wszystko to tylko połączenia od pierwszej warstwy do drugiej połączenia między innymi warstwami

146
00:12:14,029 --> 00:12:17,208
Ponadto, wiąże się z nimi wiązka wag i błędów

147
00:12:17,760 --> 00:12:20,680
Wszystkie powiedziane i zrobione ta sieć ma prawie dokładnie

148
00:12:21,280 --> 00:12:23,920
13 000 całkowitych wag i błędów

149
00:12:24,280 --> 00:12:29,540
13 000 pokręteł i pokręteł, które można modyfikować i obracać, aby ta sieć działała na różne sposoby

150
00:12:30,520 --> 00:12:32,520
Więc kiedy mówimy o nauce?

151
00:12:32,530 --> 00:12:40,199
Chodzi o to, aby komputer znalazł prawidłowe ustawienie dla wszystkich tych wielu liczb, tak aby to faktycznie rozwiązało

152
00:12:40,200 --> 00:12:42,190
problem w zasięgu ręki

153
00:12:42,190 --> 00:12:43,000
jedna myśl

154
00:12:43,000 --> 00:12:49,979
Eksperyment, który jest jednocześnie zabawny i przerażający, polega na wyobrażaniu sobie siedzenia i ustawiania wszystkich tych wag i odchyleń ręcznie.

155
00:12:50,380 --> 00:12:56,159
Celowo szykując liczby tak, aby druga warstwa znajdowała się na krawędziach, trzecia warstwa zbiera się na wzorach itp

156
00:12:56,350 --> 00:13:01,440
Osobiście uważam to za satysfakcjonujące, a nie tylko czytanie sieci jako całkowitej czarnej skrzynki

157
00:13:01,870 --> 00:13:04,349
Ponieważ, gdy sieć nie działa tak, jak ty

158
00:13:04,600 --> 00:13:11,370
spodziewaj się, że zbudowałeś trochę związku z tym, co te wagi i uprzedzenia oznaczają, że masz do czego zacząć

159
00:13:11,680 --> 00:13:16,289
Eksperymentowanie z tym, jak zmienić strukturę, by poprawić lub kiedy sieć działa?

160
00:13:16,290 --> 00:13:18,290
Ale nie z powodów, których możesz się spodziewać

161
00:13:18,310 --> 00:13:25,169
Wbijanie się w to, co robią wagi i uprzedzenia, jest dobrym sposobem na zakwestionowanie twoich przypuszczeń i naprawdę odsłonięcie całej przestrzeni możliwej.

162
00:13:25,180 --> 00:13:26,350
rozwiązania

163
00:13:26,350 --> 00:13:30,600
Przy okazji faktyczna funkcja tutaj jest trochę kłopotliwa do zapisania. Nie sądzisz?

164
00:13:32,350 --> 00:13:38,460
Pozwól, że pokażę ci bardziej kompaktowy sposób, w jaki reprezentowane są te połączenia. Tak to zobaczysz

165
00:13:38,460 --> 00:13:40,460
Jeśli zdecydujesz się przeczytać więcej o sieciach neuronowych

166
00:13:41,110 --> 00:13:45,810
Zorganizuj wszystkie aktywacje z jednej warstwy w kolumnę jako wektor

167
00:13:47,470 --> 00:13:52,320
Następnie uporządkuj wszystkie wagi jako macierz, gdzie każdy rząd tej macierzy

168
00:13:52,900 --> 00:13:57,659
odpowiada połączeniom między warstwą a określonym neuronem w następnej warstwie

169
00:13:58,060 --> 00:14:03,599
Co to oznacza, że ​​biorąc ważoną sumę aktywacji w pierwszej warstwie według tych wag?

170
00:14:04,000 --> 00:14:09,330
Odpowiada jednemu z warunków w macierzowym produkcie wektorowym wszystkiego, co mamy po lewej stronie

171
00:14:13,540 --> 00:14:18,380
Tak przy okazji, uczenie maszynowe sprowadza się do dobrego zrozumienia algebry liniowej

172
00:14:18,380 --> 00:14:26,940
Tak więc dla każdego z was, kto chce miłego wizualnego zrozumienia matryc i jakie mnożenie wektora macierzy należy spojrzeć na serię, którą zrobiłem w algebrze liniowej

173
00:14:27,250 --> 00:14:28,839
szczególnie rozdział trzeci

174
00:14:28,839 --> 00:14:35,759
Wracając do naszej ekspresji, zamiast mówić o dodawaniu uprzedzeń do każdej z tych wartości niezależnie, reprezentujemy ją

175
00:14:36,010 --> 00:14:42,209
Porządkowanie wszystkich tych odchyleń w wektor i dodanie całego wektora do poprzedniego produktu wektorowego macierzy

176
00:14:42,910 --> 00:14:44,040
Następnie jako ostatni krok

177
00:14:44,040 --> 00:14:47,250
Wymienię sigmoidę na zewnątrz

178
00:14:47,250 --> 00:14:51,899
A to, co ma reprezentować, polega na tym, że zastosujesz funkcję sigmoid do każdej konkretnej

179
00:14:52,420 --> 00:14:54,570
element wektora wynikowego w środku

180
00:14:55,510 --> 00:15:00,749
Kiedy więc zapiszesz macierz wagi i te wektory jako swoje własne symbole, możesz

181
00:15:01,000 --> 00:15:07,589
przekazać pełne przejście aktywacji z jednej warstwy na drugą w niezwykle ciasnym i schludnym wyrazie i

182
00:15:07,930 --> 00:15:15,000
Dzięki temu odpowiedni kod jest znacznie prostszy i szybszy, ponieważ wiele bibliotek optymalizuje mnożenie macierzy poza macierz

183
00:15:17,560 --> 00:15:21,359
Pamiętasz, jak wcześniej mówiłem, że te neurony to po prostu rzeczy, które trzymają liczby

184
00:15:21,790 --> 00:15:26,250
No cóż, poszczególne numery, które trzymają, zależą od obrazu, który wysyłasz

185
00:15:27,790 --> 00:15:32,940
Tak więc dokładniej jest myśleć o każdym neuronie jako funkcji, która bierze w sobie

186
00:15:33,070 --> 00:15:38,070
wyniki wszystkich neuronów w poprzedniej warstwie i wypluwa liczbę między zero a jeden

187
00:15:38,800 --> 00:15:42,270
Naprawdę cała sieć jest tylko funkcją, która się przyjmuje

188
00:15:42,760 --> 00:15:47,010
784 liczby jako dane wejściowe i wyprowadza dziesięć liczb jako wynik

189
00:15:47,470 --> 00:15:48,700
To absurd

190
00:15:48,700 --> 00:15:56,249
Skomplikowana funkcja, która obejmuje trzynaście tysięcy parametrów w postaci tych wag i odchyleń, które wychwytują określone wzorce i które obejmują

191
00:15:56,250 --> 00:16:00,270
iterowanie wielu produktów wektorów macierzy i funkcji esmozy squmo esmozy

192
00:16:00,610 --> 00:16:06,390
Ale to tylko funkcja, ale w pewnym sensie jest to trochę uspokajające, że wygląda na skomplikowaną

193
00:16:06,390 --> 00:16:12,239
Chodzi mi o to, że gdyby było prostsze, jaką nadzieję mielibyśmy, moglibyśmy podjąć wyzwanie rozpoznawania cyfr?

194
00:16:12,960 --> 00:16:19,559
A w jaki sposób podejmuje to wyzwanie? W jaki sposób ta sieć poznaje odpowiednie wagi i błędy, patrząc na dane? O?

195
00:16:20,080 --> 00:16:26,039
Właśnie to pokażę w następnym filmie, a także jeszcze bardziej zagłębię się w to, co ta konkretna sieć, którą oglądamy, naprawdę robi

196
00:16:27,130 --> 00:16:32,640
Chyba powinienem powiedzieć, że subskrybuj, aby otrzymywać powiadomienia o tym, kiedy pojawi się ten film lub nowe filmy

197
00:16:32,760 --> 00:16:37,560
Ale realistycznie większość z was nie otrzymuje powiadomień z YouTube, prawda?

198
00:16:37,560 --> 00:16:42,260
Może bardziej uczciwie powinienem powiedzieć, że subskrybuj, żeby sieci neuronowe leżały u podstaw YouTube

199
00:16:42,459 --> 00:16:47,639
Algorytm rekomendacji jest przygotowany, aby sądzić, że chcesz, aby treści z tego kanału były Ci polecane

200
00:16:48,250 --> 00:16:50,250
w każdym razie zostań wysłany po więcej

201
00:16:50,410 --> 00:16:53,550
Dziękuję wszystkim, którzy wspierają te filmy na patreonie

202
00:16:53,589 --> 00:16:56,759
Byłem trochę powolny, aby osiągnąć postępy w serii prawdopodobieństwa tego lata

203
00:16:56,760 --> 00:17:01,379
Ale po tym projekcie wskakuję z powrotem w to, więc patronami możecie tam szukać aktualizacji

204
00:17:03,310 --> 00:17:05,550
Aby zamknąć tutaj, mam ze sobą Lisha Li

205
00:17:05,550 --> 00:17:12,029
Lee, która wykonała swoją pracę doktorską na teoretycznej stronie głębokiego uczenia się i która obecnie pracuje w firmie typu venture capital, wzmacnia partnerów

206
00:17:12,030 --> 00:17:16,530
Kto łaskawie zapewnił część funduszy na ten film, więc Lisha jedno

207
00:17:16,530 --> 00:17:19,109
Myślę, że powinniśmy szybko poruszyć tę funkcję sigmoidalną

208
00:17:19,180 --> 00:17:24,780
Jak rozumiem, wczesne sieci wykorzystały to, aby zgnieść odpowiednią sumę ważoną w tym przedziale między zero a jeden

209
00:17:24,980 --> 00:17:30,340
Wiesz, że jesteś zmotywowany tą biologiczną analogią neuronów, które są nieaktywne lub aktywne
(Lisha) - Dokładnie

210
00:17:30,360 --> 00:17:36,320
(3B1B) - Ale stosunkowo niewiele nowoczesnych sieci faktycznie używa sigmoidów. To trochę stara szkoła, prawda?
(Lisha) - Tak, a raczej

211
00:17:36,370 --> 00:17:42,780
ReLU wydaje się być znacznie łatwiejsze do wyszkolenia
(3B1B) - A ReLU naprawdę oznacza prostowaną jednostkę liniową

212
00:17:42,780 --> 00:17:48,839
(Lisha) - Tak, to jest funkcja, w której bierzesz maksymalnie 0 i gdzie jest podane

213
00:17:49,120 --> 00:17:53,670
to, co tłumaczyłeś w filmie i jakie to było motywowanie, z mojego punktu widzenia było

214
00:17:54,610 --> 00:17:56,610
częściowo przez biologię

215
00:17:56,620 --> 00:17:58,179
Analogia z tym, jak

216
00:17:58,179 --> 00:18:03,089
Neurony będą aktywowane lub nie, a więc jeśli przekroczą pewien próg

217
00:18:03,250 --> 00:18:05,250
Byłaby to funkcja tożsamości

218
00:18:05,290 --> 00:18:10,439
Ale jeśli nie, to po prostu nie zostanie aktywowany, więc zero, więc jest to swego rodzaju uproszczenie

219
00:18:10,720 --> 00:18:14,429
Używanie sigmoidów nie pomagało w treningu lub było bardzo ciężko trenować

220
00:18:14,429 --> 00:18:19,589
W pewnym momencie ludzie próbowali relu i to się udało

221
00:18:20,110 --> 00:18:22,140
Bardzo dobrze dla nich niesamowicie

222
00:18:22,690 --> 00:18:25,090
Głębokie sieci neuronowe.
(3B1B) - W porządku

223
00:18:25,090 --> 00:18:26,060
Dziękuję Lisha

