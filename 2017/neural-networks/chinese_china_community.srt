1
00:00:04,350 --> 00:00:05,570
这 是一个3

2
00:00:05,780 --> 00:00:11,050
一个字迹歪斜 28x28像素 超低分辨率的3

3
00:00:11,050 --> 00:00:13,960
但你的大脑把它辨认成3 一点问题也没有

4
00:00:14,180 --> 00:00:19,380
我希望大家能稍微感叹一下 人脑"竟然"能如此轻而易举地完成这项工作

5
00:00:19,580 --> 00:00:23,360
你看这个 这个 和这个图 我们都认作3

6
00:00:23,360 --> 00:00:28,540
但他们每个图中各个像素的值是大相径庭的

7
00:00:28,670 --> 00:00:34,050
你眼睛中 看到这张3时激发的光感细胞

8
00:00:34,050 --> 00:00:37,160
和看到这张3时激发的细胞有着千差万别

9
00:00:37,460 --> 00:00:41,060
但你大脑皮层中小小的一块处理视觉的智能区域

10
00:00:41,060 --> 00:00:44,170
却居然能够把这些图像处理成相同信息

11
00:00:44,170 --> 00:00:48,520
同时还能把其他的图像解释成各自不同的信息

12
00:00:49,120 --> 00:00:52,960
不过 要是我现在叫你 嘿 你滴 给我写个程序

13
00:00:52,960 --> 00:00:59,810
输入一个28x28像素的表格 输出一个0到9之间的个位数

14
00:00:59,950 --> 00:01:02,160
来判断一个图片里到底写着哪个数字呢

15
00:01:02,590 --> 00:01:06,580
好吧 这个原本轻松的工作一下就变得如登天一般难了

16
00:01:07,100 --> 00:01:08,650
大家又不是山顶洞人

17
00:01:08,650 --> 00:01:11,330
想必我没有必要再给大家展望一遍

18
00:01:11,330 --> 00:01:15,040
机器学习和神经网络对当下和未来有多么息息相关

19
00:01:15,040 --> 00:01:16,810
但假设大家对神经网络没有任何背景知识

20
00:01:16,810 --> 00:01:20,210
我想来给大家介绍下神经网络究竟是什么

21
00:01:20,210 --> 00:01:22,360
用视觉化的方式展示它的工作机制

22
00:01:22,360 --> 00:01:24,560
把它作为一门数学 而不是单纯的网红热词来对待

23
00:01:24,900 --> 00:01:29,090
这系列讲完 我希望大家能够理解神经网络为什么会是这样的结构

24
00:01:29,090 --> 00:01:34,720
让大家再度听说神经网络"学习"之时 能够明白这究竟代表了什么

25
00:01:35,300 --> 00:01:38,610
本期视频仅仅会介绍神经网络的结构

26
00:01:38,610 --> 00:01:40,630
下一期将会讨论它的学习原理

27
00:01:40,840 --> 00:01:43,350
接下来我们将会手把手搭建一个

28
00:01:43,350 --> 00:01:46,260
能够识别手写数字的神经网络

29
00:01:49,570 --> 00:01:52,820
这是一个用于入门的经典范例

30
00:01:52,820 --> 00:01:54,650
而这里我并不会标新立异的原因

31
00:01:54,650 --> 00:01:56,380
是因为这两期结束的时候

32
00:01:56,380 --> 00:01:59,350
我会附上一些不错的补充学习资料

33
00:01:59,350 --> 00:02:03,390
你能下载它们的代码 在自己的电脑上捣鼓

34
00:02:05,040 --> 00:02:08,040
神经网络的变种非常非常之多

35
00:02:08,040 --> 00:02:12,310
近些年来对于这些变种的研究更呈爆发的态势

36
00:02:12,450 --> 00:02:14,770
不过在这几期入门介绍的视频中

37
00:02:14,770 --> 00:02:19,460
咱们只会讨论最简单  不加料的原味版(多层感知器MLP)

38
00:02:19,650 --> 00:02:24,680
我们得先理解经典的原版  才好理解功能更强大的现代变种

39
00:02:24,680 --> 00:02:28,790
而且相信我 光理解原版就够咱们吃一壶的了

40
00:02:29,020 --> 00:02:30,830
不过即便这最简单的版本

41
00:02:30,830 --> 00:02:33,170
就已经能来识别手写数字

42
00:02:33,170 --> 00:02:36,550
对于电脑而言已经很棒了

43
00:02:37,480 --> 00:02:38,430
与此同时

44
00:02:38,430 --> 00:02:42,160
你们也会看到神经网络有时也会有不尽如人意的地方

45
00:02:43,470 --> 00:02:47,130
顾名思义 神经网络之名来源自人的大脑结构

46
00:02:47,300 --> 00:02:48,720
我们来一层层剖析一下

47
00:02:48,800 --> 00:02:52,000
它的神经元是什么 神经元又是如何连接起来的

48
00:02:52,420 --> 00:02:54,500
目前说到神经元

49
00:02:54,610 --> 00:02:58,020
我想要大家把它暂时理解成一个用来装数字的容器

50
00:02:58,180 --> 00:03:00,700
装着一个0到1之间的数字

51
00:03:00,700 --> 00:03:02,450
仅此而已

52
00:03:03,770 --> 00:03:06,880
看例子 这个网络一开始的地方有很多神经元

53
00:03:06,880 --> 00:03:11,650
分别对应了28x28的输入图像里的每一个像素

54
00:03:11,750 --> 00:03:14,440
总计784个神经元

55
00:03:14,560 --> 00:03:20,690
神经元中装着的数字代表对应像素的灰度值

56
00:03:20,860 --> 00:03:22,860
0表示纯黑像素

57
00:03:22,860 --> 00:03:24,570
1表示纯白像素

58
00:03:25,260 --> 00:03:28,690
我们把神经元里装着的数 叫做"激活值"

59
00:03:29,060 --> 00:03:30,840
大家可以想象这么一个画面

60
00:03:30,840 --> 00:03:34,210
激活值越大 那么那个神经元就点着越亮

61
00:03:36,580 --> 00:03:41,930
这么些784个神经元就组成了网络的第一层

62
00:03:46,340 --> 00:03:48,200
现在我们跳到网络的最后一层

63
00:03:48,200 --> 00:03:51,620
这一层的十个神经元分别代表0到9这十个数字

64
00:03:51,920 --> 00:03:56,530
它们的激活值 同理都处在0到1之间

65
00:03:56,800 --> 00:04:02,320
这些值表示系统认为输入的图像对应着哪个数字的可能性

66
00:04:02,830 --> 00:04:06,560
网络中间还有几层"隐含层"

67
00:04:06,560 --> 00:04:09,920
暂时我们就把它看做一个大黑箱

68
00:04:09,920 --> 00:04:13,880
里面就进行着处理识别数字的具体工作

69
00:04:14,180 --> 00:04:18,120
这个网络中 我选择加两层隐含层 每层有16个神经元

70
00:04:18,120 --> 00:04:21,470
我得承认 这些设置都是随便选的

71
00:04:21,470 --> 00:04:25,010
结构选择两层隐含层的理由过一会我自会来解释

72
00:04:25,310 --> 00:04:28,430
而选择16个神经元无非是显得好看罢了

73
00:04:28,690 --> 00:04:32,500
实际应用中 在网络的结构上 我们有很大的调整实验的余地

74
00:04:33,130 --> 00:04:34,480
神经网络运作的时候

75
00:04:34,480 --> 00:04:38,710
上一层的激活值将决定下一层的激活值

76
00:04:39,140 --> 00:04:43,170
所以说 神经网络处理信息的核心机制正是

77
00:04:43,260 --> 00:04:48,740
一层的激活值是通过怎样的运算 算出下一层的激活值的

78
00:04:49,220 --> 00:04:53,700
某种程度上讲 它想模仿的是生物中神经元组成的网络

79
00:04:53,700 --> 00:04:57,410
某些神经元的激发 就会促使另一些神经元激发

80
00:04:57,910 --> 00:05:01,650
当前我给你展示的神经网络已经被训练好  可以识别数字了

81
00:05:01,650 --> 00:05:03,450
我来解释下这是什么意思

82
00:05:03,530 --> 00:05:09,650
这表示 如果你在网络输入层的784个神经元处

83
00:05:09,650 --> 00:05:12,440
输入了784个代表输入图像各像素的灰度值

84
00:05:12,660 --> 00:05:17,540
那么 这层激活值的图案会让下层的激活值产生某些特殊的图案

85
00:05:17,540 --> 00:05:19,780
再让再下层的产生特殊的图案

86
00:05:19,780 --> 00:05:22,290
最终在输出层得到某种结果

87
00:05:22,590 --> 00:05:27,730
而输出层最亮的那个神经元就表示神经网络的"选择"

88
00:05:27,730 --> 00:05:29,950
它认为输入图像里写着这个数字

89
00:05:32,470 --> 00:05:37,260
在我介绍网络每层间如何影响 训练过程的数学原理之前

90
00:05:37,380 --> 00:05:43,860
我们先讨论下 凭什么我们就觉得这种层状结构可以做到智能判断

91
00:05:44,110 --> 00:05:45,390
我们在期待什么呢

92
00:05:45,390 --> 00:05:48,610
我们到底期望这些中间层最好能做些什么呢

93
00:05:49,200 --> 00:05:51,870
当我们人类在识别数字的时候

94
00:05:51,870 --> 00:05:53,970
我们是在组合数字的各个部件

95
00:05:54,110 --> 00:05:57,090
9 就是上边一个圈 右边再一竖

96
00:05:57,320 --> 00:06:01,380
8 就是上边一个圈 搭着下边一个圈

97
00:06:01,950 --> 00:06:06,890
4 就当能拆分成3条直线  诸如此类

98
00:06:07,570 --> 00:06:12,500
在理想的情况下 我们希望倒数第二层中的各个神经元

99
00:06:12,500 --> 00:06:15,000
能分别对应上一个笔画部件

100
00:06:15,190 --> 00:06:20,040
这样一来 当我们输入一个9或者8这种带圈的数字时

101
00:06:20,240 --> 00:06:23,990
某一个神经元中的激活值就会接近1

102
00:06:24,420 --> 00:06:26,770
而且我并不特指某种样子的圈

103
00:06:26,770 --> 00:06:31,810
我是希望 所有这种位于图像顶部的圆圈图案都能点亮这个神经元

104
00:06:32,330 --> 00:06:35,370
这样一来 从第三层到最后一层

105
00:06:35,370 --> 00:06:40,320
我们只需要学习哪些部件能组合出哪个数字即可

106
00:06:40,930 --> 00:06:43,140
当然 这样一来我们就引来了更多的问题

107
00:06:43,140 --> 00:06:45,460
例如  要如何识别这些部件呢

108
00:06:45,460 --> 00:06:47,860
甚至哪些部件才算正确的呢

109
00:06:47,920 --> 00:06:51,120
而且我还没提到上一层网络是如何影响下一层的

110
00:06:51,120 --> 00:06:53,310
不过暂时请先陪我把话题讨论完

111
00:06:53,490 --> 00:06:56,930
识别圆圈的任务同理可以拆分成更细微的问题

112
00:06:57,160 --> 00:07:02,880
一种合理的方法便是首先识别出数字图形中更小的边

113
00:07:03,560 --> 00:07:08,560
比如像1  4  7中的这种长条

114
00:07:08,800 --> 00:07:10,800
就是一条长边嘛

115
00:07:10,800 --> 00:07:14,690
或者把它当做几条短边组合起来的图案也可以

116
00:07:15,010 --> 00:07:20,050
于是我们希望 也许网络第二层的各个神经元

117
00:07:20,130 --> 00:07:23,040
就能对应上这些各种各样的短边

118
00:07:23,470 --> 00:07:26,330
没准当这样子的图像输入进来的时候

119
00:07:26,530 --> 00:07:32,040
它就能把所有关联短边的八到十个神经元都给点亮

120
00:07:32,270 --> 00:07:37,100
接着就能点亮对应顶部圆圈和长竖条的神经元

121
00:07:37,410 --> 00:07:40,000
最后就能点亮对应9字的神经元

122
00:07:40,580 --> 00:07:44,480
至于咱们的网络是否真的能做到这一步

123
00:07:44,610 --> 00:07:47,510
等我解释完网络如何训练 再来回头讨论吧

124
00:07:47,650 --> 00:07:49,680
但这就是我们的希望

125
00:07:49,680 --> 00:07:52,630
希望这种层状结构能完成的目标

126
00:07:53,060 --> 00:07:57,320
更进一步讲 假如神经网络真能识别出这类边缘和图案

127
00:07:57,320 --> 00:08:00,640
它就能很好地运用到其他图像的识别任务上来

128
00:08:00,880 --> 00:08:02,630
甚至不光是图像识别

129
00:08:02,630 --> 00:08:05,190
世界上各种人工智能的任务

130
00:08:05,190 --> 00:08:07,550
都可以转化为抽象元素 一层层的抽丝剥茧

131
00:08:07,850 --> 00:08:09,640
就比如说语音识别

132
00:08:09,640 --> 00:08:12,930
就是要从原音频中识别出特殊的声音

133
00:08:13,010 --> 00:08:15,280
组合成特定的音节

134
00:08:15,420 --> 00:08:17,030
组合成单词

135
00:08:17,030 --> 00:08:20,450
再组合成短语 以及更加抽象的概念

136
00:08:21,120 --> 00:08:23,780
回到神经网络工作原理的话题上来

137
00:08:23,910 --> 00:08:28,340
试想一下 你要设计上一层中的激活值

138
00:08:28,340 --> 00:08:30,690
到底会如何决定下一层中的激活值

139
00:08:31,040 --> 00:08:36,260
我们需要设计一个机制 可以把像素拼成短边

140
00:08:36,320 --> 00:08:37,730
把短边拼成图案

141
00:08:37,730 --> 00:08:39,180
或者把图案拼成数字 等等

142
00:08:39,290 --> 00:08:42,230
这个例子里 我们来放大关注其中一个

143
00:08:42,230 --> 00:08:46,280
我们来设计 让第二层中的这一个神经元

144
00:08:46,280 --> 00:08:50,750
能够正确识别出图像中的这块区域里 是否存在一条边

145
00:08:51,380 --> 00:08:55,370
现在我们就需要知道这个网络的参数

146
00:08:55,610 --> 00:08:58,580
你应该如何调整网络上的旋钮开关

147
00:08:58,640 --> 00:09:02,360
才能让它足以表示出 要么这种图案

148
00:09:02,530 --> 00:09:04,490
要么别的像素图案

149
00:09:04,490 --> 00:09:07,970
要么是几条边组合成圆圈的图案之类

150
00:09:08,620 --> 00:09:12,520
我们需要给这个神经元和第一层所有神经元间的每一条接线

151
00:09:12,520 --> 00:09:15,800
都赋上一个权重值

152
00:09:16,230 --> 00:09:18,090
这些权重都不过是数字而已

153
00:09:18,520 --> 00:09:22,450
然后 我们拿起第一层所有的激活值

154
00:09:22,450 --> 00:09:25,730
和它们对应权重值一起 算出它们的加权和

155
00:09:27,510 --> 00:09:32,100
我觉得把这些权重值看做一个表格更好理解

156
00:09:32,370 --> 00:09:35,060
我会把正的权重值标记成绿色

157
00:09:35,060 --> 00:09:37,410
负的标记成红色

158
00:09:37,570 --> 00:09:41,930
颜色越暗 就大致表示它的权重越接近于0

159
00:09:42,760 --> 00:09:46,160
现在我们如果把关注区域的权重赋为正值

160
00:09:46,160 --> 00:09:49,620
而其他所有的权重值一律赋为0

161
00:09:49,850 --> 00:09:53,050
这样一来 对所有的像素值取加权和

162
00:09:53,050 --> 00:09:57,670
就只会累加我们关注区域的像素值了

163
00:09:58,960 --> 00:10:02,290
此时如果你真的想识别出这里是否存在一条边

164
00:10:02,560 --> 00:10:06,880
你只需要给周围一圈的像素赋予负的权重

165
00:10:07,360 --> 00:10:13,060
这样当中间的像素亮 周围的像素暗时 加权和就能达到最大

166
00:10:14,650 --> 00:10:18,610
这样计算出来的加权和可以是任意大小

167
00:10:18,610 --> 00:10:23,720
但这个网络中 我们需要激活值都处在0与1之间

168
00:10:24,100 --> 00:10:28,130
那么 我们就可以顺其自然把这个加权和输进某个函数

169
00:10:28,130 --> 00:10:32,060
把这条实数轴挤压进0到1的区间内

170
00:10:32,520 --> 00:10:35,810
其中一个叫sigmoid的函数非常常用

171
00:10:35,810 --> 00:10:37,650
它又叫logistic/逻辑斯蒂曲线

172
00:10:38,010 --> 00:10:41,120
简而言之 它能把非常大的负值变成接近0

173
00:10:41,370 --> 00:10:43,720
非常大的正值变成接近1

174
00:10:43,870 --> 00:10:46,720
而在取值0附近则是平稳增长的

175
00:10:49,420 --> 00:10:51,760
所以这个神经元中的激活值

176
00:10:51,760 --> 00:10:56,610
实际上就是一个对加权和到底有多正的打分

177
00:10:57,760 --> 00:11:02,090
但有时 即使加权和大于0时 你也不想把神经元点亮

178
00:11:02,370 --> 00:11:06,580
可能只有当和大于例如10的时候才让它激发

179
00:11:06,970 --> 00:11:10,730
此时你就需要加上一个偏置值 保证不能随便激发

180
00:11:11,200 --> 00:11:16,560
而我们只需要在加权和之后加上一个负10之类的数

181
00:11:16,560 --> 00:11:19,950
再把它送进sigmoid压缩/映射函数即可

182
00:11:20,570 --> 00:11:22,900
这个附加的数就叫做偏置

183
00:11:23,330 --> 00:11:28,610
总而言之 权重告诉你这个第二层的神经元关注什么样的像素图案

184
00:11:28,610 --> 00:11:32,450
偏置则告诉你加权和得有多大

185
00:11:32,450 --> 00:11:35,510
才能让神经元的激发变得有意义

186
00:11:36,130 --> 00:11:37,940
我们这就解说完了其中一个神经元

187
00:11:38,150 --> 00:11:45,300
但这一层的每一个神经元 都会和第一层全部的784个神经元相连接

188
00:11:45,420 --> 00:11:50,930
每一个的784个接线上都带着一个权重

189
00:11:51,500 --> 00:11:55,910
而且每一个神经元都会在计算自己的加权和后加上自己的偏置

190
00:11:55,910 --> 00:11:57,860
再通过sigmoid压缩输出自己的结果

191
00:11:58,210 --> 00:11:59,680
一下子要考虑的就多起来了

192
00:11:59,970 --> 00:12:02,380
这层隐含层的16个神经元

193
00:12:02,380 --> 00:12:08,370
就需要总计784 x 16个权重值和16个偏置值

194
00:12:08,740 --> 00:12:12,240
而且这还是单单第一层和第二层之间的连接

195
00:12:12,450 --> 00:12:17,530
别的层之间的连接还有它们分别自带的权重和偏置

196
00:12:18,240 --> 00:12:24,070
一套下来整个网络一共会用上将近13000个权重加偏置

197
00:12:24,270 --> 00:12:27,590
相当于这个网络上有13000多个旋钮开关让你调整

198
00:12:27,590 --> 00:12:30,260
从而带来不一样的结果

199
00:12:30,800 --> 00:12:32,840
所以 当我们讨论机器如何学习的时候

200
00:12:32,840 --> 00:12:39,140
我们其实在讲 电脑应该如何设置这一大坨的数字参数

201
00:12:39,140 --> 00:12:41,600
才能让它正确地解决问题

202
00:12:42,480 --> 00:12:46,290
这里有个细思极恐的思想实验

203
00:12:46,290 --> 00:12:51,940
想象一下你自己手动调整这些权重还有偏置参数

204
00:12:51,940 --> 00:12:56,920
让第二层识别短边 第三层识别图案

205
00:12:57,250 --> 00:13:01,800
比起把网络完全当做一个黑箱 我个人觉得这么考虑更加令人满足

206
00:13:02,030 --> 00:13:05,330
毕竟当网络的输出和期望出了偏差的时候

207
00:13:05,550 --> 00:13:10,020
如果你一定程度上了解了这些权重和偏置的意义

208
00:13:10,290 --> 00:13:14,410
那么你再尝试对结构进行修正就有出发点了

209
00:13:14,870 --> 00:13:18,210
或许你的神经网络能输出正确的结果 但过程和你想象的不一样

210
00:13:18,470 --> 00:13:23,010
那么 深挖权重和偏置的实际意义 就可以有效挑战你的假设

211
00:13:23,010 --> 00:13:26,130
进而探索出所有可能的解决方案

212
00:13:26,680 --> 00:13:30,960
顺便一提 整个函数这么写下来是不是很难懂

213
00:13:32,690 --> 00:13:37,410
我这里就给大家展示个既能表示所有的连线 看着又清爽的符号表达好了

214
00:13:37,670 --> 00:13:40,740
你之后学习神经网络你就会一直见到这种符号

215
00:13:41,220 --> 00:13:46,210
我们把某一层中所有的激活值统一成一列向量

216
00:13:47,810 --> 00:13:50,520
再把它和下一层间所有的权重放到一个矩阵中

217
00:13:50,730 --> 00:13:55,380
矩阵第n行就是这一层的所有神经元

218
00:13:55,380 --> 00:13:58,040
和下一层第n个神经元间所有连线的权重

219
00:13:58,400 --> 00:14:03,960
这样权重矩阵和向量乘积的第n项

220
00:14:03,960 --> 00:14:09,650
就是这一层所有的激活值 和下一层第n个神经元间连线权重的加权和

221
00:14:13,870 --> 00:14:18,610
顺带一句 机器学习到头来和线性代数是不分家的

222
00:14:18,660 --> 00:14:24,330
如果大家想更形象地理解矩阵和矩阵乘法的意义的话

223
00:14:24,390 --> 00:14:27,320
不妨去看下我之前做的线代本质系列吧

224
00:14:27,320 --> 00:14:28,570
特别是第三章

225
00:14:29,140 --> 00:14:30,420
回到符号表达的话题

226
00:14:30,420 --> 00:14:34,640
表达偏置值的时候 我们并不会把一个个值都拎出来单独讨论

227
00:14:34,940 --> 00:14:38,690
相反 我们会把它们都放到一个向量里

228
00:14:38,690 --> 00:14:42,570
然后把它和之前的矩阵乘法的结果相加

229
00:14:43,230 --> 00:14:47,490
最后一步 我们把整个表达式用一个sigmoid包起来

230
00:14:47,690 --> 00:14:49,830
所谓包起来就是指

231
00:14:49,830 --> 00:14:55,070
对表达式结果向量中的每一项都取一次sigmoid

232
00:14:55,780 --> 00:15:00,570
现在只要我们一写下权重矩阵和相应向量的符号

233
00:15:00,570 --> 00:15:05,060
神经网络各层之间激活值的转化

234
00:15:05,060 --> 00:15:07,650
就可以表达得清晰简洁明了了

235
00:15:08,170 --> 00:15:12,340
这种表达也让我们写程序变得简便了许多

236
00:15:12,340 --> 00:15:16,010
因为很多库在矩阵乘法方面做了十足的优化(比如屏幕里用的numpy)

237
00:15:17,680 --> 00:15:21,760
还记得之前 我要叫大家把神经元看作数字的容器吗

238
00:15:22,100 --> 00:15:26,640
实际上 神经元中装着的值是取决于你的输入图像的

239
00:15:28,140 --> 00:15:31,830
所以我们把神经元看作一个函数才更加准确

240
00:15:31,980 --> 00:15:36,260
它输入的是上一层所有的神经元的输出

241
00:15:36,260 --> 00:15:38,590
而它的输出是一个0到1之间的值

242
00:15:39,130 --> 00:15:41,600
其实整个神经网络就是一个函数

243
00:15:41,600 --> 00:15:47,260
一个输入784个值 输出10个值的函数

244
00:15:47,800 --> 00:15:49,800
不过这个函数极其的复杂

245
00:15:49,800 --> 00:15:53,920
它用了13000个权重参数偏置参数

246
00:15:53,920 --> 00:15:55,480
来识别特殊图案

247
00:15:55,640 --> 00:16:00,550
又要循环不停地用到矩阵乘法和sigmoid映射运算

248
00:16:00,930 --> 00:16:03,010
但它终究只是个函数而已

249
00:16:03,620 --> 00:16:06,900
而它的复杂程度可以稍微让人安点心

250
00:16:07,240 --> 00:16:08,830
如果它没这么复杂的话

251
00:16:08,830 --> 00:16:12,570
我们恐怕就不大能指望它数字识别能多准了

252
00:16:13,260 --> 00:16:14,990
那么 它是如何处理这项艰巨任务的

253
00:16:14,990 --> 00:16:19,710
神经网络是如何通过数据来获得合适的权重和偏置的

254
00:16:20,040 --> 00:16:21,930
这就是下一集的内容了

255
00:16:22,160 --> 00:16:26,400
我们会继续探究这一种特殊的神经网络的运作方式

256
00:16:27,440 --> 00:16:32,950
虽说又到了提醒大家赶紧订阅来获得下一期新视频推送的时间

257
00:16:33,070 --> 00:16:37,540
但大家伙基本上都没获得过YouTube的推送消息吧

258
00:16:37,940 --> 00:16:39,910
也许我更该说 大家赶紧订阅我的频道

259
00:16:39,910 --> 00:16:43,770
这样YouTube后端推荐算法用的神经网络

260
00:16:43,770 --> 00:16:47,950
才会相信大家想看到这个频道的内容出现在推荐栏

261
00:16:48,380 --> 00:16:50,160
总之 我们不见不散

262
00:16:50,650 --> 00:16:53,870
非常感谢所有在Patreon上资助我的人

263
00:16:53,930 --> 00:16:57,090
这个夏天我稍微有点偏离了概率论系列的计划

264
00:16:57,090 --> 00:16:59,280
但做完这个系列我会回到计划上来

265
00:16:59,280 --> 00:17:01,800
所以资助的大家可以随时在那获取更新

266
00:17:03,640 --> 00:17:06,030
节目的结尾 我请到了Lisha Li

267
00:17:06,030 --> 00:17:09,010
她在博士期间做了深度学习理论方面的研究

268
00:17:09,010 --> 00:17:12,300
而她现在在一家叫Amplify Partners的风投公司工作

269
00:17:12,300 --> 00:17:14,780
他们慷慨地为本期视频提供了一部分资金资助

270
00:17:15,240 --> 00:17:19,410
那么Lisha 有个事情我想稍微提到一下 就是这个sigmoid函数

271
00:17:19,570 --> 00:17:21,320
以我的理解 早期的网络都是用这个

272
00:17:21,320 --> 00:17:25,190
把加权和映射到0至1的区间内的

273
00:17:25,190 --> 00:17:29,850
当年正是这么来模仿生物学上的神经元是否激发的来着

274
00:17:29,850 --> 00:17:30,480
没错

275
00:17:30,480 --> 00:17:34,260
但现在的神经网络基本上都不用sigmoid了

276
00:17:34,260 --> 00:17:35,610
就很过时

277
00:17:35,610 --> 00:17:39,270
没错没错 ReLU应该更好训练

278
00:17:39,270 --> 00:17:42,540
ReLU全称叫"线性整流函数"吧

279
00:17:42,540 --> 00:17:44,540
是的 这种函数就是返回

280
00:17:44,540 --> 00:17:49,260
0和a的最大值 其中a就是函数的输入

281
00:17:49,450 --> 00:17:53,230
你在视频中解释 神经元之所以会采用这种函数

282
00:17:53,230 --> 00:18:01,380
我认为一部分是为了模仿生物学上的神经元什么时候会激发

283
00:18:01,380 --> 00:18:05,640
所以 当超过一个阈值的时候 ReLU就和恒定函数一样

284
00:18:05,640 --> 00:18:09,460
而没过这个阈值 那么就不激发 输出0

285
00:18:09,460 --> 00:18:10,980
可以当作是一种简化版

286
00:18:10,980 --> 00:18:13,090
Sigmoid并没有让训练结果变得更好

287
00:18:13,090 --> 00:18:15,610
或者某种程度上讲它很难训练

288
00:18:15,610 --> 00:18:17,820
后来有人就拿ReLU试了试

289
00:18:17,820 --> 00:18:24,820
结果发现在特别深的神经网络上效果特别的好

290
00:18:24,880 --> 00:18:26,050
非常感谢Lisha

