1
00:00:04,220 --> 00:00:05,400
Il s'agit d'un 3.

2
00:00:06,060 --> 00:00:10,445
Il est mal écrit et rendu à une résolution extrêmement faible de 28x28 pixels, 

3
00:00:10,445 --> 00:00:13,720
mais ton cerveau n'a aucun mal à le reconnaître comme un 3.

4
00:00:14,340 --> 00:00:16,724
Et je veux que tu prennes un moment pour apprécier à quel point 

5
00:00:16,724 --> 00:00:18,960
il est fou que les cerveaux puissent faire cela sans effort.

6
00:00:19,700 --> 00:00:23,709
Je veux dire que ceci, ceci et ceci sont également reconnaissables comme des 3, 

7
00:00:23,709 --> 00:00:27,919
même si les valeurs spécifiques de chaque pixel sont très différentes d'une image à 

8
00:00:27,919 --> 00:00:28,320
l'autre.

9
00:00:28,900 --> 00:00:32,944
Les cellules photosensibles particulières de ton œil qui se déclenchent lorsque tu 

10
00:00:32,944 --> 00:00:36,940
vois ce 3 sont très différentes de celles qui se déclenchent lorsque tu vois ce 3.

11
00:00:37,520 --> 00:00:41,654
Mais quelque chose dans ton cortex visuel, qui est d'une intelligence folle, 

12
00:00:41,654 --> 00:00:44,447
considère que ces images représentent la même idée, 

13
00:00:44,447 --> 00:00:48,260
tout en reconnaissant d'autres images comme étant des idées distinctes.

14
00:00:49,220 --> 00:00:53,297
Mais si je te disais, hé, assieds-toi et écris pour moi un programme 

15
00:00:53,297 --> 00:00:57,552
qui prend une grille de 28x28 pixels comme celle-ci et qui sort un seul 

16
00:00:57,552 --> 00:01:01,511
nombre entre 0 et 10, en te disant ce qu'il pense être le chiffre, 

17
00:01:01,511 --> 00:01:06,180
eh bien la tâche passe d'une trivialité comique à une difficulté décourageante.

18
00:01:07,160 --> 00:01:09,529
À moins que tu n'aies vécu sous une roche, je pense que je n'ai 

19
00:01:09,529 --> 00:01:12,233
guère besoin de motiver la pertinence et l'importance de l'apprentissage 

20
00:01:12,233 --> 00:01:14,640
automatique et des réseaux neuronaux pour le présent et l'avenir.

21
00:01:15,120 --> 00:01:18,526
Mais ce que je veux faire ici, c'est te montrer ce qu'est réellement un réseau neuronal, 

22
00:01:18,526 --> 00:01:21,780
en supposant qu'il n'y ait pas d'antécédents, et t'aider à visualiser ce qu'il fait, 

23
00:01:21,780 --> 00:01:24,460
non pas comme un mot à la mode mais comme un morceau de mathématiques.

24
00:01:25,020 --> 00:01:28,721
J'espère que tu repartiras avec le sentiment que la structure elle-même est motivée, 

25
00:01:28,721 --> 00:01:31,683
et que tu sauras ce que cela signifie lorsque tu liras ou entendras 

26
00:01:31,683 --> 00:01:34,340
parler d'un réseau neuronal entre guillemets d'apprentissage.

27
00:01:35,360 --> 00:01:38,308
Cette vidéo va juste être consacrée à la composante structure de cela, 

28
00:01:38,308 --> 00:01:40,260
et la suivante va s'attaquer à l'apprentissage.

29
00:01:40,960 --> 00:01:43,586
Ce que nous allons faire, c'est assembler un réseau neuronal 

30
00:01:43,586 --> 00:01:46,040
qui peut apprendre à reconnaître des chiffres manuscrits.

31
00:01:49,360 --> 00:01:51,783
C'est un exemple assez classique pour introduire le sujet, 

32
00:01:51,783 --> 00:01:55,316
et je suis heureux de m'en tenir au statu quo ici, parce qu'à la fin des deux vidéos, 

33
00:01:55,316 --> 00:01:58,766
je veux t'indiquer quelques bonnes ressources où tu pourras en apprendre davantage, 

34
00:01:58,766 --> 00:02:02,176
et où tu pourras télécharger le code qui permet de faire cela et de jouer avec sur 

35
00:02:02,176 --> 00:02:03,080
ton propre ordinateur.

36
00:02:05,040 --> 00:02:09,345
Il existe de nombreuses variantes de réseaux neuronaux, et ces dernières années, 

37
00:02:09,345 --> 00:02:12,428
la recherche sur ces variantes a connu une sorte de boom, 

38
00:02:12,428 --> 00:02:17,160
mais dans ces deux vidéos d'introduction, toi et moi allons nous contenter d'examiner la 

39
00:02:17,160 --> 00:02:19,180
forme la plus simple, sans fioritures.

40
00:02:19,860 --> 00:02:22,802
C'est en quelque sorte un prérequis nécessaire pour comprendre les 

41
00:02:22,802 --> 00:02:25,174
variantes modernes les plus puissantes, et crois-moi, 

42
00:02:25,174 --> 00:02:28,600
il y a encore beaucoup de complexité pour que nous puissions nous y retrouver.

43
00:02:29,120 --> 00:02:32,771
Mais même sous cette forme la plus simple, il peut apprendre à reconnaître 

44
00:02:32,771 --> 00:02:36,520
des chiffres manuscrits, ce qui est une chose plutôt cool pour un ordinateur.

45
00:02:37,480 --> 00:02:39,808
Et en même temps, tu verras qu'il n'est pas à la 

46
00:02:39,808 --> 00:02:42,280
hauteur de certains espoirs que nous pouvions avoir.

47
00:02:43,380 --> 00:02:47,301
Comme leur nom l'indique, les réseaux neuronaux s'inspirent du cerveau, 

48
00:02:47,301 --> 00:02:48,500
mais décomposons cela.

49
00:02:48,520 --> 00:02:51,660
Quels sont les neurones, et dans quel sens sont-ils reliés entre eux ?

50
00:02:52,500 --> 00:02:56,517
Pour l'instant, lorsque je parle de neurones, tout ce à quoi je veux que tu penses, 

51
00:02:56,517 --> 00:03:00,440
c'est à une chose qui contient un nombre, plus précisément un nombre entre 0 et 1.

52
00:03:00,680 --> 00:03:02,560
Ce n'est vraiment pas plus que cela.

53
00:03:03,780 --> 00:03:08,964
Par exemple, le réseau commence avec un groupe de neurones correspondant 

54
00:03:08,964 --> 00:03:14,220
à chacun des 28x28 pixels de l'image d'entrée, soit 784 neurones au total.

55
00:03:14,700 --> 00:03:19,511
Chacun d'entre eux contient un nombre qui représente la valeur de l'échelle de gris 

56
00:03:19,511 --> 00:03:24,380
du pixel correspondant, allant de 0 pour les pixels noirs à 1 pour les pixels blancs.

57
00:03:25,300 --> 00:03:28,334
Ce nombre à l'intérieur du neurone est appelé son activation, 

58
00:03:28,334 --> 00:03:31,222
et l'image que tu peux avoir à l'esprit ici est que chaque 

59
00:03:31,222 --> 00:03:34,160
neurone s'allume lorsque son activation est un nombre élevé.

60
00:03:36,720 --> 00:03:41,860
Tous ces 784 neurones constituent donc la première couche de notre réseau.

61
00:03:46,500 --> 00:03:49,601
Passons maintenant à la dernière couche, qui comporte 10 neurones, 

62
00:03:49,601 --> 00:03:51,360
chacun représentant l'un des chiffres.

63
00:03:52,040 --> 00:03:56,692
L'activation de ces neurones, encore une fois un nombre compris entre 0 et 1, 

64
00:03:56,692 --> 00:04:01,762
représente à quel point le système pense qu'une image donnée correspond à un chiffre 

65
00:04:01,762 --> 00:04:02,120
donné.

66
00:04:03,040 --> 00:04:06,829
Il y a également deux couches intermédiaires appelées couches cachées qui, 

67
00:04:06,829 --> 00:04:10,164
pour l'instant, ne sont qu'un énorme point d'interrogation sur la 

68
00:04:10,164 --> 00:04:13,600
façon dont ce processus de reconnaissance des chiffres va être géré.

69
00:04:14,260 --> 00:04:18,000
Dans ce réseau, j'ai choisi deux couches cachées, chacune avec 16 neurones, 

70
00:04:18,000 --> 00:04:20,560
et il est vrai que c'est un peu un choix arbitraire.

71
00:04:21,019 --> 00:04:23,362
Pour être honnête, j'ai choisi deux couches en fonction de la 

72
00:04:23,362 --> 00:04:25,781
façon dont je veux motiver la structure dans un instant, et 16, 

73
00:04:25,781 --> 00:04:28,200
eh bien c'était juste un joli chiffre à faire tenir sur l'écran.

74
00:04:28,780 --> 00:04:30,541
Dans la pratique, il y a beaucoup de place pour 

75
00:04:30,541 --> 00:04:32,340
l'expérimentation d'une structure spécifique ici.

76
00:04:33,020 --> 00:04:35,660
Selon le mode de fonctionnement du réseau, les activations 

77
00:04:35,660 --> 00:04:38,480
d'une couche déterminent les activations de la couche suivante.

78
00:04:39,200 --> 00:04:42,507
Et bien sûr, le cœur du réseau en tant que mécanisme de traitement 

79
00:04:42,507 --> 00:04:45,716
de l'information se résume à la façon dont ces activations d'une 

80
00:04:45,716 --> 00:04:48,580
couche entraînent des activations dans la couche suivante.

81
00:04:49,140 --> 00:04:51,483
Il s'agit d'une analogie vague avec la façon dont, 

82
00:04:51,483 --> 00:04:55,388
dans les réseaux biologiques de neurones, l'allumage de certains groupes de neurones 

83
00:04:55,388 --> 00:04:57,180
entraîne l'allumage de certains autres.

84
00:04:58,120 --> 00:05:01,330
Le réseau que je montre ici a déjà été entraîné à reconnaître les chiffres, 

85
00:05:01,330 --> 00:05:03,400
et je vais te montrer ce que je veux dire par là.

86
00:05:03,640 --> 00:05:08,358
Cela signifie que si tu introduis une image, en allumant les 784 neurones de la couche 

87
00:05:08,358 --> 00:05:11,938
d'entrée en fonction de la luminosité de chaque pixel de l'image, 

88
00:05:11,938 --> 00:05:16,493
ce modèle d'activations provoque un modèle très spécifique dans la couche suivante, 

89
00:05:16,493 --> 00:05:19,042
qui provoque un modèle dans la couche d'après, 

90
00:05:19,042 --> 00:05:22,080
qui donne finalement un modèle dans la couche de sortie.

91
00:05:22,560 --> 00:05:26,461
Et le neurone le plus brillant de cette couche de sortie est le choix du réseau, 

92
00:05:26,461 --> 00:05:29,400
pour ainsi dire, quant au chiffre que cette image représente.

93
00:05:32,560 --> 00:05:35,367
Avant de nous lancer dans les calculs pour déterminer comment une couche 

94
00:05:35,367 --> 00:05:37,636
influence la suivante, ou comment fonctionne la formation, 

95
00:05:37,636 --> 00:05:40,366
parlons des raisons pour lesquelles il est raisonnable de s'attendre à 

96
00:05:40,366 --> 00:05:43,520
ce qu'une structure en couches comme celle-ci se comporte de manière intelligente.

97
00:05:44,060 --> 00:05:45,220
Qu'attendons-nous ici ?

98
00:05:45,400 --> 00:05:47,600
Quel est le meilleur espoir pour ces couches intermédiaires ?

99
00:05:48,920 --> 00:05:53,520
Eh bien, lorsque toi ou moi reconnaissons des chiffres, nous rassemblons divers éléments.

100
00:05:54,200 --> 00:05:56,820
Un 9 a une boucle en haut et une ligne à droite.

101
00:05:57,380 --> 00:06:01,180
Un 8 a également une boucle en haut, mais elle est associée à une autre boucle en bas.

102
00:06:01,980 --> 00:06:06,820
Un 4 se décompose essentiellement en trois lignes spécifiques, et des choses comme ça.

103
00:06:07,600 --> 00:06:11,255
Dans un monde parfait, nous pourrions espérer que chaque neurone de 

104
00:06:11,255 --> 00:06:14,964
l'avant-dernière couche corresponde à l'une de ces sous-composantes, 

105
00:06:14,964 --> 00:06:19,157
qu'à chaque fois que tu introduis une image avec, disons, une boucle en haut, 

106
00:06:19,157 --> 00:06:23,780
comme un 9 ou un 8, il y ait un neurone spécifique dont l'activation sera proche de 1.

107
00:06:24,500 --> 00:06:27,275
Et je ne parle pas de cette boucle spécifique de pixels, 

108
00:06:27,275 --> 00:06:31,560
l'espoir serait que tout motif généralement en boucle vers le haut déclenche ce neurone.

109
00:06:32,440 --> 00:06:35,427
Ainsi, pour passer de la troisième couche à la dernière, 

110
00:06:35,427 --> 00:06:40,040
il suffit d'apprendre quelle combinaison de sous-composants correspond à quels chiffres.

111
00:06:41,000 --> 00:06:43,075
Bien sûr, cela ne fait que repousser le problème, 

112
00:06:43,075 --> 00:06:46,353
car comment reconnaître ces sous-composants, ou même apprendre quels devraient 

113
00:06:46,353 --> 00:06:47,640
être les bons sous-composants ?

114
00:06:48,060 --> 00:06:51,502
Et je n'ai même pas encore parlé de la façon dont une couche influence la suivante, 

115
00:06:51,502 --> 00:06:53,060
mais suis-moi un instant sur ce point.

116
00:06:53,680 --> 00:06:56,680
La reconnaissance d'une boucle peut également se décomposer en sous-problèmes.

117
00:06:57,280 --> 00:07:00,105
Une façon raisonnable de le faire serait de reconnaître 

118
00:07:00,105 --> 00:07:02,780
d'abord les différents petits bords qui le composent.

119
00:07:03,780 --> 00:07:07,758
De même, une longue ligne, comme celle que tu peux voir dans les chiffres 1, 

120
00:07:07,758 --> 00:07:11,168
4 ou 7, n'est en fait qu'une longue arête, ou peut-être penses-tu 

121
00:07:11,168 --> 00:07:14,320
qu'il s'agit d'un certain modèle de plusieurs petites arêtes.

122
00:07:15,140 --> 00:07:18,903
Alors peut-être que notre espoir est que chaque neurone de la deuxième 

123
00:07:18,903 --> 00:07:22,720
couche du réseau corresponde aux différentes petites arêtes pertinentes.

124
00:07:23,540 --> 00:07:26,605
Peut-être que lorsqu'une image comme celle-ci arrive, 

125
00:07:26,605 --> 00:07:31,260
elle allume tous les neurones associés à environ 8 à 10 petits bords spécifiques, 

126
00:07:31,260 --> 00:07:35,235
ce qui allume à son tour les neurones associés à la boucle supérieure 

127
00:07:35,235 --> 00:07:39,720
et à une longue ligne verticale, et ceux-ci allument le neurone associé à un 9.

128
00:07:40,680 --> 00:07:44,306
Que ce soit ou non ce que notre réseau final fait réellement est une autre question, 

129
00:07:44,306 --> 00:07:47,761
sur laquelle je reviendrai une fois que nous aurons vu comment former le réseau, 

130
00:07:47,761 --> 00:07:49,766
mais c'est un espoir que nous pourrions avoir, 

131
00:07:49,766 --> 00:07:52,540
une sorte d'objectif avec la structure en couches comme celle-ci.

132
00:07:53,160 --> 00:07:56,750
De plus, tu peux imaginer comment le fait de pouvoir détecter des bords et des motifs 

133
00:07:56,750 --> 00:08:00,300
comme celui-ci serait vraiment utile pour d'autres tâches de reconnaissance d'images.

134
00:08:00,880 --> 00:08:02,659
Et même au-delà de la reconnaissance d'images, 

135
00:08:02,659 --> 00:08:05,878
il y a toutes sortes de choses intelligentes que tu pourrais vouloir faire et qui se 

136
00:08:05,878 --> 00:08:07,280
décomposent en couches d'abstraction.

137
00:08:08,040 --> 00:08:10,928
L'analyse de la parole, par exemple, consiste à prendre des données 

138
00:08:10,928 --> 00:08:13,051
audio brutes et à en extraire des sons distincts, 

139
00:08:13,051 --> 00:08:16,789
qui se combinent pour former certaines syllabes, qui se combinent pour former des mots, 

140
00:08:16,789 --> 00:08:20,060
qui se combinent pour former des phrases et des pensées plus abstraites, etc.

141
00:08:21,100 --> 00:08:24,689
Mais pour en revenir à la façon dont tout cela fonctionne réellement, 

142
00:08:24,689 --> 00:08:29,304
imagine-toi en train de concevoir comment les activations d'une couche peuvent déterminer 

143
00:08:29,304 --> 00:08:29,920
la suivante.

144
00:08:30,860 --> 00:08:35,957
L'objectif est de disposer d'un mécanisme qui pourrait combiner les pixels en arêtes, 

145
00:08:35,957 --> 00:08:38,980
ou les arêtes en motifs, ou les motifs en chiffres.

146
00:08:39,440 --> 00:08:42,176
Et pour zoomer sur un exemple très spécifique, 

147
00:08:42,176 --> 00:08:45,903
disons que l'on espère qu'un neurone particulier de la deuxième 

148
00:08:45,903 --> 00:08:50,620
couche saura déterminer si l'image présente ou non un bord dans cette région ici.

149
00:08:51,440 --> 00:08:53,308
La question qui se pose est la suivante : quels 

150
00:08:53,308 --> 00:08:55,100
sont les paramètres que le réseau doit avoir ?

151
00:08:55,640 --> 00:08:59,842
Quels cadrans et boutons devrais-tu pouvoir régler pour qu'il soit suffisamment expressif 

152
00:08:59,842 --> 00:09:03,110
pour capturer potentiellement ce motif, ou tout autre motif de pixel, 

153
00:09:03,110 --> 00:09:06,285
ou le motif selon lequel plusieurs bords peuvent former une boucle, 

154
00:09:06,285 --> 00:09:07,780
et d'autres choses de ce genre ?

155
00:09:08,720 --> 00:09:12,068
Eh bien, ce que nous allons faire, c'est attribuer un poids à chacune 

156
00:09:12,068 --> 00:09:15,560
des connexions entre notre neurone et les neurones de la première couche.

157
00:09:16,320 --> 00:09:17,700
Ces poids ne sont que des chiffres.

158
00:09:18,540 --> 00:09:22,140
Prends ensuite toutes ces activations de la première couche 

159
00:09:22,140 --> 00:09:25,500
et calcule leur somme pondérée en fonction de ces poids.

160
00:09:27,700 --> 00:09:31,231
Je trouve utile de penser que ces poids sont organisés en une petite grille 

161
00:09:31,231 --> 00:09:34,763
qui leur est propre, et je vais utiliser des pixels verts pour indiquer les 

162
00:09:34,763 --> 00:09:38,062
poids positifs, et des pixels rouges pour indiquer les poids négatifs, 

163
00:09:38,062 --> 00:09:41,780
où la luminosité de ce pixel est une représentation libre de la valeur du poids.

164
00:09:42,780 --> 00:09:46,371
Si les poids associés à la quasi-totalité des pixels sont nuls, 

165
00:09:46,371 --> 00:09:50,636
à l'exception de quelques poids positifs dans la région qui nous intéresse, 

166
00:09:50,636 --> 00:09:55,631
la somme pondérée de toutes les valeurs des pixels revient à additionner les valeurs des 

167
00:09:55,631 --> 00:09:57,820
pixels de la région qui nous intéresse.

168
00:09:59,140 --> 00:10:02,587
Et si tu voulais vraiment savoir s'il y a un bord ici, 

169
00:10:02,587 --> 00:10:06,600
tu pourrais associer des poids négatifs aux pixels environnants.

170
00:10:07,480 --> 00:10:10,170
La somme est alors la plus importante lorsque les pixels du milieu 

171
00:10:10,170 --> 00:10:12,700
sont clairs mais que les pixels environnants sont plus sombres.

172
00:10:14,260 --> 00:10:16,799
Lorsque tu calcules une somme pondérée comme celle-ci, 

173
00:10:16,799 --> 00:10:19,569
tu peux obtenir n'importe quel nombre, mais pour ce réseau, 

174
00:10:19,569 --> 00:10:23,540
ce que nous voulons, c'est que les activations aient une valeur comprise entre 0 et 1.

175
00:10:24,120 --> 00:10:28,130
Il est donc courant de pomper cette somme pondérée dans une fonction qui 

176
00:10:28,130 --> 00:10:32,140
écrase la ligne des nombres réels dans l'intervalle compris entre 0 et 1.

177
00:10:32,460 --> 00:10:35,370
Et une fonction courante qui fait cela s'appelle la fonction sigmoïde, 

178
00:10:35,370 --> 00:10:37,420
également connue sous le nom de courbe logistique.

179
00:10:38,000 --> 00:10:41,265
En gros, les entrées très négatives se terminent près de 0, 

180
00:10:41,265 --> 00:10:45,456
les entrées positives se terminent près de 1, et tout augmente régulièrement 

181
00:10:45,456 --> 00:10:46,600
autour de l'entrée 0.

182
00:10:49,120 --> 00:10:52,647
L'activation du neurone ici est donc essentiellement une 

183
00:10:52,647 --> 00:10:56,360
mesure de la positivité de la somme pondérée correspondante.

184
00:10:57,540 --> 00:10:59,747
Mais peut-être que ce n'est pas que tu veux que le neurone 

185
00:10:59,747 --> 00:11:01,880
s'allume lorsque la somme pondérée est plus grande que 0.

186
00:11:02,280 --> 00:11:05,827
Peut-être veux-tu qu'il ne soit actif que lorsque la somme est supérieure à 10, 

187
00:11:05,827 --> 00:11:06,360
par exemple.

188
00:11:06,840 --> 00:11:10,260
C'est-à-dire que tu veux qu'il y ait un biais pour qu'il soit inactif.

189
00:11:11,380 --> 00:11:14,006
Ce que nous ferons alors, c'est d'ajouter un autre nombre, 

190
00:11:14,006 --> 00:11:16,677
par exemple un nombre négatif de 10, à cette somme pondérée 

191
00:11:16,677 --> 00:11:19,660
avant de l'introduire dans la fonction de squishification sigmoïde.

192
00:11:20,580 --> 00:11:22,440
Ce nombre supplémentaire s'appelle le biais.

193
00:11:23,460 --> 00:11:27,971
Les poids t'indiquent donc quel motif de pixel ce neurone de la deuxième couche capte, 

194
00:11:27,971 --> 00:11:31,912
et le biais t'indique à quel point la somme pondérée doit être élevée avant 

195
00:11:31,912 --> 00:11:35,180
que le neurone ne commence à être actif de façon significative.

196
00:11:36,120 --> 00:11:37,680
Et il ne s'agit là que d'un seul neurone.

197
00:11:38,280 --> 00:11:44,500
Tous les autres neurones de cette couche vont être connectés aux 784 neurones pixels 

198
00:11:44,500 --> 00:11:50,940
de la première couche, et chacune de ces 784 connexions est associée à son propre poids.

199
00:11:51,600 --> 00:11:54,678
De plus, chacun a un biais, un autre nombre que tu ajoutes 

200
00:11:54,678 --> 00:11:57,600
à la somme pondérée avant de l'écraser avec la sigmoïde.

201
00:11:58,110 --> 00:11:59,540
Et c'est beaucoup de choses à penser !

202
00:11:59,960 --> 00:12:06,471
Avec cette couche cachée de 16 neurones, cela fait un total de 784 fois 16 poids, 

203
00:12:06,471 --> 00:12:07,980
ainsi que 16 biais.

204
00:12:08,840 --> 00:12:11,940
Et tout cela ne représente que les connexions de la première couche à la seconde.

205
00:12:12,520 --> 00:12:15,171
Les connexions entre les autres couches sont également 

206
00:12:15,171 --> 00:12:17,340
associées à un ensemble de poids et de biais.

207
00:12:18,340 --> 00:12:23,800
Tout compte fait, ce réseau a presque exactement 13 000 poids et biais au total.

208
00:12:23,800 --> 00:12:27,039
13 000 boutons et cadrans qui peuvent être réglés et tournés 

209
00:12:27,039 --> 00:12:29,960
pour que ce réseau se comporte de différentes manières.

210
00:12:31,040 --> 00:12:34,534
Ainsi, lorsque nous parlons d'apprentissage, il s'agit d'amener 

211
00:12:34,534 --> 00:12:38,138
l'ordinateur à trouver un paramètre valide pour tous ces nombreux 

212
00:12:38,138 --> 00:12:41,360
nombres afin qu'il puisse résoudre le problème en question.

213
00:12:42,620 --> 00:12:46,036
Une expérience de pensée à la fois amusante et horrifiante consiste à 

214
00:12:46,036 --> 00:12:49,746
s'asseoir et à définir toutes ces pondérations et tous ces biais à la main, 

215
00:12:49,746 --> 00:12:53,114
en modifiant délibérément les chiffres de façon à ce que la deuxième 

216
00:12:53,114 --> 00:12:56,580
couche prenne en compte les bords, la troisième couche les motifs, etc.

217
00:12:56,980 --> 00:13:00,278
Personnellement, je trouve cela satisfaisant plutôt que de traiter le 

218
00:13:00,278 --> 00:13:03,483
réseau comme une boîte noire totale, parce que lorsque le réseau ne 

219
00:13:03,483 --> 00:13:07,017
fonctionne pas comme tu l'avais prévu, si tu as établi une petite relation 

220
00:13:07,017 --> 00:13:09,750
avec ce que ces poids et ces biais signifient réellement, 

221
00:13:09,750 --> 00:13:13,378
tu as un point de départ pour expérimenter la façon de modifier la structure 

222
00:13:13,378 --> 00:13:14,180
pour l'améliorer.

223
00:13:14,960 --> 00:13:18,524
Ou lorsque le réseau fonctionne, mais pas pour les raisons auxquelles tu t'attendais, 

224
00:13:18,524 --> 00:13:22,130
le fait de creuser sur ce que font les poids et les biais est un bon moyen de remettre 

225
00:13:22,130 --> 00:13:25,820
en question tes hypothèses et d'exposer réellement tout l'espace des solutions possibles.

226
00:13:26,840 --> 00:13:30,680
Au fait, la fonction réelle ici est un peu lourde à écrire, tu ne crois pas ?

227
00:13:32,500 --> 00:13:37,140
Je vais donc te montrer une façon plus compacte de représenter ces connexions.

228
00:13:37,660 --> 00:13:39,119
C'est ainsi que tu le verras si tu choisis de te 

229
00:13:39,119 --> 00:13:40,520
documenter davantage sur les réseaux neuronaux.

230
00:13:41,380 --> 00:13:50,132
Organise toutes les activations d'une couche dans une colonne car une matrice correspond 

231
00:13:50,132 --> 00:13:58,000
aux connexions entre une couche et un neurone particulier de la couche suivante.

232
00:13:58,540 --> 00:14:02,301
Cela signifie que la somme pondérée des activations de la première 

233
00:14:02,301 --> 00:14:05,894
couche en fonction de ces poids correspond à l'un des termes du 

234
00:14:05,894 --> 00:14:09,880
produit vectoriel de la matrice de tout ce que nous avons à gauche ici.

235
00:14:14,000 --> 00:14:16,807
Au fait, une grande partie de l'apprentissage automatique se résume à 

236
00:14:16,807 --> 00:14:19,655
une bonne maîtrise de l'algèbre linéaire, alors pour ceux d'entre vous 

237
00:14:19,655 --> 00:14:22,503
qui veulent une bonne compréhension visuelle des matrices et de ce que 

238
00:14:22,503 --> 00:14:24,629
signifie la multiplication vectorielle des matrices, 

239
00:14:24,629 --> 00:14:27,436
jetez un coup d'œil à la série que j'ai faite sur l'algèbre linéaire, 

240
00:14:27,436 --> 00:14:28,600
en particulier le chapitre 3.

241
00:14:29,240 --> 00:14:33,626
Pour en revenir à notre expression, au lieu de parler de l'ajout du biais à chacune de 

242
00:14:33,626 --> 00:14:37,862
ces valeurs indépendamment, nous le représentons en organisant tous ces biais en un 

243
00:14:37,862 --> 00:14:42,300
vecteur, et en ajoutant le vecteur entier au produit vectoriel de la matrice précédente.

244
00:14:43,280 --> 00:14:47,442
Puis, comme dernière étape, je vais enrouler une sigmoïde autour de l'extérieur ici, 

245
00:14:47,442 --> 00:14:51,066
et ce que c'est censé représenter, c'est que tu vas appliquer la fonction 

246
00:14:51,066 --> 00:14:54,740
sigmoïde à chaque composante spécifique du vecteur résultant à l'intérieur.

247
00:14:55,940 --> 00:14:59,980
Une fois que tu as écrit cette matrice de poids et ces vecteurs comme leurs propres 

248
00:14:59,980 --> 00:15:03,924
symboles, tu peux communiquer la transition complète des activations d'une couche 

249
00:15:03,924 --> 00:15:07,194
à l'autre dans une petite expression extrêmement serrée et soignée, 

250
00:15:07,194 --> 00:15:11,523
ce qui rend le code correspondant à la fois beaucoup plus simple et beaucoup plus rapide, 

251
00:15:11,523 --> 00:15:15,660
étant donné que de nombreuses bibliothèques optimisent la multiplication des matrices.

252
00:15:17,820 --> 00:15:19,702
Tu te souviens que j'ai dit tout à l'heure que ces neurones 

253
00:15:19,702 --> 00:15:21,460
sont simplement des choses qui contiennent des nombres ?

254
00:15:22,220 --> 00:15:28,296
Il est donc plus exact de considérer chaque neurone comme une fonction, 

255
00:15:28,296 --> 00:15:33,866
qui prend en compte les sorties de tous les neurones de la couche 

256
00:15:33,866 --> 00:15:38,340
précédente et produit un nombre compris entre 0 et 1.

257
00:15:39,200 --> 00:15:42,849
En réalité, le réseau entier n'est qu'une fonction, 

258
00:15:42,849 --> 00:15:47,060
qui prend 784 nombres en entrée et en recrache 10 en sortie.

259
00:15:47,560 --> 00:15:51,359
C'est une fonction absurdement compliquée, qui implique 13 000 paramètres 

260
00:15:51,359 --> 00:15:54,799
sous la forme de poids et de biais qui détectent certains modèles, 

261
00:15:54,799 --> 00:15:58,598
et qui implique l'itération de nombreux produits vectoriels matriciels et 

262
00:15:58,598 --> 00:16:03,065
de la fonction de squishification sigmoïde, mais ce n'est qu'une fonction malgré tout, 

263
00:16:03,065 --> 00:16:06,660
et d'une certaine façon, c'est rassurant qu'elle ait l'air compliquée.

264
00:16:07,340 --> 00:16:09,969
Je veux dire que s'il était plus simple, quel espoir aurions-nous 

265
00:16:09,969 --> 00:16:12,280
qu'il puisse relever le défi de reconnaître les chiffres ?

266
00:16:13,340 --> 00:16:14,700
Et comment relève-t-il ce défi ?

267
00:16:15,080 --> 00:16:17,305
Comment ce réseau apprend-il les poids et les biais 

268
00:16:17,305 --> 00:16:19,360
appropriés simplement en regardant les données ?

269
00:16:20,140 --> 00:16:22,311
C'est ce que je vais te montrer dans la prochaine vidéo, 

270
00:16:22,311 --> 00:16:25,053
et je vais aussi creuser un peu plus sur ce que fait vraiment ce réseau 

271
00:16:25,053 --> 00:16:26,120
particulier que nous voyons.

272
00:16:27,580 --> 00:16:30,973
Je suppose que je devrais te dire de t'abonner pour rester informé de la sortie 

273
00:16:30,973 --> 00:16:33,390
d'une vidéo ou de toute nouvelle vidéo, mais en réalité, 

274
00:16:33,390 --> 00:16:36,826
la plupart d'entre vous ne reçoivent pas de notifications de la part de YouTube, 

275
00:16:36,826 --> 00:16:37,420
n'est-ce pas ?

276
00:16:38,020 --> 00:16:41,306
Peut-être que plus honnêtement, je devrais dire s'abonner pour que les réseaux 

277
00:16:41,306 --> 00:16:44,468
neuronaux qui sous-tendent l'algorithme de recommandation de YouTube soient 

278
00:16:44,468 --> 00:16:47,880
amorcés pour croire que tu veux que le contenu de cette chaîne te soit recommandé.

279
00:16:48,560 --> 00:16:49,940
Quoi qu'il en soit, reste connecté pour en savoir plus.

280
00:16:50,760 --> 00:16:53,500
Merci beaucoup à tous ceux qui soutiennent ces vidéos sur Patreon.

281
00:16:54,000 --> 00:16:57,459
J'ai été un peu lent à progresser dans la série des probabilités cet été, 

282
00:16:57,459 --> 00:16:59,843
mais je m'y remets après ce projet, alors patrons, 

283
00:16:59,843 --> 00:17:01,900
vous pouvez guetter des mises à jour là-bas.

284
00:17:03,600 --> 00:17:06,206
Pour terminer, j'ai avec moi Leisha Lee qui a fait son travail de 

285
00:17:06,206 --> 00:17:08,813
doctorat sur l'aspect théorique de l'apprentissage profond et qui 

286
00:17:08,813 --> 00:17:11,420
travaille actuellement dans une société de capital-risque appelée 

287
00:17:11,420 --> 00:17:14,619
Amplify Partners qui a gentiment fourni une partie du financement de cette vidéo.

288
00:17:15,460 --> 00:17:17,433
Alors Leisha, une chose que je pense que nous devrions 

289
00:17:17,433 --> 00:17:19,119
rapidement évoquer est cette fonction sigmoïde.

290
00:17:19,700 --> 00:17:23,024
Si j'ai bien compris, les réseaux précoces utilisent cette méthode pour réduire 

291
00:17:23,024 --> 00:17:25,268
la somme pondérée dans l'intervalle entre zéro et un, 

292
00:17:25,268 --> 00:17:28,717
ce qui est en quelque sorte motivé par l'analogie biologique des neurones qui sont 

293
00:17:28,717 --> 00:17:29,840
soit inactifs, soit actifs.

294
00:17:30,280 --> 00:17:30,300
Exactement.

295
00:17:30,560 --> 00:17:34,040
Mais relativement peu de réseaux modernes utilisent encore la sigmoïde.

296
00:17:34,320 --> 00:17:34,320
Oui.

297
00:17:34,440 --> 00:17:35,540
C'est un peu la vieille école, n'est-ce pas ?

298
00:17:35,760 --> 00:17:38,980
Oui ou plutôt relu semble être beaucoup plus facile à former.

299
00:17:39,400 --> 00:17:42,340
Et relu est l'abréviation de rectified linear unit ?

300
00:17:42,680 --> 00:17:47,284
Oui, c'est ce genre de fonction où tu prends juste le maximum de zéro et de a 

301
00:17:47,284 --> 00:17:51,889
où a est donné par ce que tu expliquais dans la vidéo et ce qui a été motivé, 

302
00:17:51,889 --> 00:17:56,612
je pense, en partie par une analogie biologique avec la façon dont les neurones 

303
00:17:56,612 --> 00:18:01,276
seraient soit activés soit non activés et donc si cela passe un certain seuil, 

304
00:18:01,276 --> 00:18:04,936
ce serait la fonction d'identité mais si ce n'est pas le cas, 

305
00:18:04,936 --> 00:18:08,537
il ne serait tout simplement pas activé donc ce serait zéro, 

306
00:18:08,537 --> 00:18:10,840
donc c'est une sorte de simplification.

307
00:18:11,160 --> 00:18:15,610
L'utilisation de sigmoïdes n'a pas aidé à l'entraînement ou était très difficile à 

308
00:18:15,610 --> 00:18:20,169
entraîner à un moment donné et les gens ont simplement essayé relu et il s'est avéré 

309
00:18:20,169 --> 00:18:24,620
que cela fonctionnait très bien pour ces réseaux neuronaux incroyablement profonds.

310
00:18:25,100 --> 00:18:25,640
Très bien, merci Alicia.

