1
00:00:04,220 --> 00:00:05,400
C'est un 3.

2
00:00:06,060 --> 00:00:10,309
Il est mal écrit et rendu à une résolution extrêmement faible de 28 x 28 pixels, 

3
00:00:10,309 --> 00:00:13,720
mais votre cerveau n'a aucun mal à le reconnaître comme un 3.

4
00:00:14,340 --> 00:00:16,612
Et je veux que vous preniez un moment pour apprécier à quel 

5
00:00:16,612 --> 00:00:18,960
point il est fou que le cerveau puisse faire cela sans effort.

6
00:00:19,700 --> 00:00:23,337
Je veux dire, ceci, ceci et cela sont également reconnaissables comme 3, 

7
00:00:23,337 --> 00:00:27,622
même si les valeurs spécifiques de chaque pixel sont très différentes d'une image 

8
00:00:27,622 --> 00:00:28,320
à l'autre.

9
00:00:28,900 --> 00:00:32,799
Les cellules sensibles à la lumière de votre œil qui se déclenchent lorsque vous 

10
00:00:32,799 --> 00:00:36,940
voyez ce 3 sont très différentes de celles qui se déclenchent lorsque vous voyez ce 3.

11
00:00:37,520 --> 00:00:41,186
Mais quelque chose dans votre cortex visuel incroyablement intelligent 

12
00:00:41,186 --> 00:00:44,748
les résout comme représentant la même idée, tout en reconnaissant en 

13
00:00:44,748 --> 00:00:48,260
même temps d'autres images comme leurs propres idées distinctes.

14
00:00:49,220 --> 00:00:53,475
Mais si je vous disais, asseyez-vous et écrivez pour moi un programme 

15
00:00:53,475 --> 00:00:57,730
qui prend une grille de 28x28 pixels comme celle-ci et génère un seul 

16
00:00:57,730 --> 00:01:01,985
nombre entre 0 et 10, vous disant à quoi il pense que le chiffre est, 

17
00:01:01,985 --> 00:01:06,180
eh bien, la tâche va de comiquement trivial à terriblement difficile.

18
00:01:07,160 --> 00:01:08,781
À moins que vous n'ayez vécu sous un rocher, 

19
00:01:08,781 --> 00:01:11,528
je pense que je n'ai guère besoin de motiver la pertinence et l'importance 

20
00:01:11,528 --> 00:01:14,110
de l'apprentissage automatique et des réseaux de neurones pour le présent 

21
00:01:14,110 --> 00:01:14,640
et l'avenir.

22
00:01:15,120 --> 00:01:18,356
Mais ce que je veux faire ici, c'est vous montrer ce qu'est réellement 

23
00:01:18,356 --> 00:01:22,043
un réseau de neurones, sans aucun contexte, et vous aider à visualiser ce qu'il fait, 

24
00:01:22,043 --> 00:01:24,460
non pas comme un mot à la mode mais comme une mathématique.

25
00:01:25,020 --> 00:01:28,180
J'espère que vous repartirez avec le sentiment que la structure elle-même 

26
00:01:28,180 --> 00:01:31,138
est motivée et que vous savez ce que cela signifie lorsque vous lisez ou 

27
00:01:31,138 --> 00:01:34,340
entendez parler d'un apprentissage entre guillemets par un réseau neuronal.

28
00:01:35,360 --> 00:01:38,471
Cette vidéo va simplement être consacrée à la composante structure de celle-ci, 

29
00:01:38,471 --> 00:01:40,260
et la suivante va aborder l'apprentissage.

30
00:01:40,960 --> 00:01:43,479
Ce que nous allons faire, c'est créer un réseau neuronal 

31
00:01:43,479 --> 00:01:46,040
capable d'apprendre à reconnaître les chiffres manuscrits.

32
00:01:49,360 --> 00:01:52,556
Il s'agit d'un exemple quelque peu classique d'introduction au sujet, 

33
00:01:52,556 --> 00:01:55,869
et je suis heureux de m'en tenir au statu quo ici, car à la fin des deux vidéos, 

34
00:01:55,869 --> 00:01:59,026
je souhaite vous indiquer quelques bonnes ressources où vous pouvez en apprendre 

35
00:01:59,026 --> 00:02:02,378
davantage et où vous pouvez télécharger le code qui fait cela et jouer avec sur votre 

36
00:02:02,378 --> 00:02:03,080
propre ordinateur.

37
00:02:05,040 --> 00:02:08,952
Il existe de nombreuses variantes de réseaux de neurones, et ces dernières années, 

38
00:02:08,952 --> 00:02:12,110
il y a eu une sorte d'essor de la recherche sur ces variantes, 

39
00:02:12,110 --> 00:02:15,692
mais dans ces deux vidéos d'introduction, vous et moi allons simplement 

40
00:02:15,692 --> 00:02:19,180
examiner la forme vanille la plus simple, sans fioritures supplémentaires.

41
00:02:19,860 --> 00:02:22,669
C’est en quelque sorte une condition préalable nécessaire pour 

42
00:02:22,669 --> 00:02:25,389
comprendre l’une des variantes modernes les plus puissantes, 

43
00:02:25,389 --> 00:02:28,600
et croyez-moi, il nous reste encore beaucoup de complexité à comprendre.

44
00:02:29,120 --> 00:02:32,773
Mais même sous cette forme la plus simple, il peut apprendre à reconnaître les 

45
00:02:32,773 --> 00:02:36,520
chiffres manuscrits, ce qui est une chose plutôt intéressante pour un ordinateur.

46
00:02:37,480 --> 00:02:39,799
Et en même temps, vous verrez à quel point il est loin de 

47
00:02:39,799 --> 00:02:42,280
répondre aux quelques espoirs que nous pourrions avoir en lui.

48
00:02:43,380 --> 00:02:47,395
Comme leur nom l’indique, les réseaux de neurones sont inspirés par le cerveau, 

49
00:02:47,395 --> 00:02:48,500
mais décomposons cela.

50
00:02:48,520 --> 00:02:51,660
Que sont les neurones et dans quel sens sont-ils liés entre eux ?

51
00:02:52,500 --> 00:02:56,241
En ce moment, quand je parle de neurone, tout ce à quoi je veux que vous pensiez, 

52
00:02:56,241 --> 00:03:00,211
c'est à une chose qui contient un nombre, en particulier un nombre compris entre 0 

53
00:03:00,211 --> 00:03:00,440
et 1.

54
00:03:00,680 --> 00:03:02,560
Ce n'est vraiment pas plus que ça.

55
00:03:03,780 --> 00:03:08,796
Par exemple, le réseau commence par un groupe de neurones correspondant à 

56
00:03:08,796 --> 00:03:14,220
chacun des 28x28 pixels de l'image d'entrée, soit 784 neurones au total.

57
00:03:14,700 --> 00:03:19,376
Chacun d'eux contient un nombre qui représente la valeur d'échelle de gris du 

58
00:03:19,376 --> 00:03:23,999
pixel correspondant, allant de 0 pour les pixels noirs jusqu'à 1 pour les pixels 

59
00:03:23,999 --> 00:03:24,380
blancs.

60
00:03:25,300 --> 00:03:28,253
Ce nombre à l’intérieur du neurone est appelé son activation, 

61
00:03:28,253 --> 00:03:31,206
et l’image que vous pourriez avoir en tête ici est que chaque 

62
00:03:31,206 --> 00:03:34,160
neurone est allumé lorsque son activation est un nombre élevé.

63
00:03:36,720 --> 00:03:41,860
L’ensemble de ces 784 neurones constitue donc la première couche de notre réseau.

64
00:03:46,500 --> 00:03:49,569
Passons maintenant à la dernière couche, celle-ci comporte 10 neurones, 

65
00:03:49,569 --> 00:03:51,360
chacun représentant l'un des chiffres.

66
00:03:52,040 --> 00:03:56,709
L'activation de ces neurones, encore une fois un nombre compris entre 0 et 1, 

67
00:03:56,709 --> 00:04:01,778
représente à quel point le système pense qu'une image donnée correspond à un chiffre 

68
00:04:01,778 --> 00:04:02,120
donné.

69
00:04:03,040 --> 00:04:06,360
Il y a aussi quelques couches entre les deux, appelées couches cachées, 

70
00:04:06,360 --> 00:04:10,049
qui pour le moment ne devraient être qu'un énorme point d'interrogation 

71
00:04:10,049 --> 00:04:13,600
sur la manière dont ce processus de reconnaissance des chiffres va être géré.

72
00:04:14,260 --> 00:04:16,798
Dans ce réseau, j'ai choisi deux couches cachées, 

73
00:04:16,798 --> 00:04:20,560
chacune contenant 16 neurones, et il est vrai que c'est un choix arbitraire.

74
00:04:21,019 --> 00:04:23,425
Pour être honnête, j'ai choisi deux couches en fonction de la 

75
00:04:23,425 --> 00:04:25,685
façon dont je veux motiver la structure en un instant, et 16, 

76
00:04:25,685 --> 00:04:28,200
eh bien, c'était juste un joli nombre à afficher sur l'écran.

77
00:04:28,780 --> 00:04:32,340
Dans la pratique, il y a ici beaucoup de marge pour expérimenter une structure spécifique.

78
00:04:33,020 --> 00:04:35,633
Selon le fonctionnement du réseau, les activations dans 

79
00:04:35,633 --> 00:04:38,480
une couche déterminent les activations de la couche suivante.

80
00:04:39,200 --> 00:04:42,358
Et bien sûr, le cœur du réseau en tant que mécanisme de traitement 

81
00:04:42,358 --> 00:04:45,563
de l’information se résume à la manière exacte dont ces activations 

82
00:04:45,563 --> 00:04:48,580
d’une couche entraînent des activations dans la couche suivante.

83
00:04:49,140 --> 00:04:51,485
Cela est censé être vaguement analogue à la façon dont, 

84
00:04:51,485 --> 00:04:54,081
dans les réseaux biologiques de neurones, certains groupes de 

85
00:04:54,081 --> 00:04:57,180
neurones se déclenchent en provoquant le déclenchement de certains autres.

86
00:04:58,120 --> 00:05:01,369
Maintenant, le réseau que je montre ici a déjà été formé pour reconnaître les chiffres, 

87
00:05:01,369 --> 00:05:03,400
et laissez-moi vous montrer ce que je veux dire par là.

88
00:05:03,640 --> 00:05:06,054
Cela signifie que si vous alimentez une image, 

89
00:05:06,054 --> 00:05:10,420
éclairant les 784 neurones de la couche d'entrée en fonction de la luminosité de 

90
00:05:10,420 --> 00:05:14,580
chaque pixel de l'image, ce modèle d'activations provoque un modèle très 

91
00:05:14,580 --> 00:05:19,100
spécifique dans la couche suivante qui provoque un modèle dans celui d'après. cela, 

92
00:05:19,100 --> 00:05:22,080
ce qui donne finalement un motif dans la couche de sortie.

93
00:05:22,560 --> 00:05:26,489
Et le neurone le plus brillant de cette couche de sortie est le choix du réseau, 

94
00:05:26,489 --> 00:05:29,400
pour ainsi dire, pour le chiffre que représente cette image.

95
00:05:32,560 --> 00:05:35,155
Et avant de nous lancer dans les calculs sur la manière dont une couche 

96
00:05:35,155 --> 00:05:37,463
influence la suivante ou sur le fonctionnement de la formation, 

97
00:05:37,463 --> 00:05:40,131
parlons simplement des raisons pour lesquelles il est même raisonnable de 

98
00:05:40,131 --> 00:05:42,979
s'attendre à ce qu'une structure en couches comme celle-ci se comporte 

99
00:05:42,979 --> 00:05:43,520
intelligemment.

100
00:05:44,060 --> 00:05:45,220
Qu’attend-on ici ?

101
00:05:45,400 --> 00:05:47,600
Quel est le meilleur espoir pour ces couches intermédiaires ?

102
00:05:48,920 --> 00:05:51,770
Eh bien, lorsque vous ou moi reconnaissons des chiffres, 

103
00:05:51,770 --> 00:05:53,520
nous rassemblons divers composants.

104
00:05:54,200 --> 00:05:56,820
Un 9 a une boucle en haut et une ligne à droite.

105
00:05:57,380 --> 00:06:01,180
Un 8 a également une boucle en haut, mais il est associé à une autre boucle en bas.

106
00:06:01,980 --> 00:06:06,820
Un 4 se décompose essentiellement en trois lignes spécifiques, et des choses comme ça.

107
00:06:07,600 --> 00:06:11,522
Maintenant, dans un monde parfait, nous pourrions espérer que chaque neurone de 

108
00:06:11,522 --> 00:06:15,199
l'avant-dernière couche corresponde à l'un de ces sous-composants, 

109
00:06:15,199 --> 00:06:19,318
et que chaque fois que vous introduisez une image avec, disons, une boucle en haut, 

110
00:06:19,318 --> 00:06:23,191
comme un 9 ou un 8, il y en a neurone spécifique dont l'activation va être 

111
00:06:23,191 --> 00:06:23,780
proche de 1.

112
00:06:24,500 --> 00:06:27,200
Et je ne parle pas de cette boucle spécifique de pixels, 

113
00:06:27,200 --> 00:06:30,564
l'espoir serait que tout motif généralement en boucle vers le haut 

114
00:06:30,564 --> 00:06:31,560
déclenche ce neurone.

115
00:06:32,440 --> 00:06:36,311
De cette façon, passer de la troisième couche à la dernière nécessite simplement 

116
00:06:36,311 --> 00:06:40,040
d’apprendre quelle combinaison de sous-composants correspond à quels chiffres.

117
00:06:41,000 --> 00:06:43,114
Bien sûr, cela ne fait que repousser le problème, 

118
00:06:43,114 --> 00:06:46,328
car comment reconnaître ces sous-composants, ou même savoir quels devraient 

119
00:06:46,328 --> 00:06:47,640
être les bons sous-composants ?

120
00:06:48,060 --> 00:06:51,255
Et je n'ai toujours pas parlé de la façon dont une couche influence la suivante, 

121
00:06:51,255 --> 00:06:53,060
mais suivez-moi sur celle-ci pendant un instant.

122
00:06:53,680 --> 00:06:56,680
Reconnaître une boucle peut également se décomposer en sous-problèmes.

123
00:06:57,280 --> 00:07:00,130
Une façon raisonnable d’y parvenir serait de reconnaître 

124
00:07:00,130 --> 00:07:02,780
d’abord les différents petits bords qui le composent.

125
00:07:03,780 --> 00:07:07,702
De même, une longue ligne, comme celle que vous pourriez voir dans les chiffres 1, 

126
00:07:07,702 --> 00:07:10,066
4 ou 7, n'est en réalité qu'un long bord, 

127
00:07:10,066 --> 00:07:14,320
ou peut-être que vous la considérez comme un certain motif de plusieurs bords plus petits.

128
00:07:15,140 --> 00:07:19,011
Alors peut-être que notre espoir est que chaque neurone de la deuxième 

129
00:07:19,011 --> 00:07:22,720
couche du réseau corresponde aux différents petits bords pertinents.

130
00:07:23,540 --> 00:07:26,842
Peut-être que lorsqu'une image comme celle-ci apparaît, 

131
00:07:26,842 --> 00:07:31,409
elle éclaire tous les neurones associés à environ 8 à 10 petits bords spécifiques, 

132
00:07:31,409 --> 00:07:35,482
ce qui à son tour éclaire les neurones associés à la boucle supérieure et 

133
00:07:35,482 --> 00:07:39,720
à une longue ligne verticale, et ceux-ci éclairent le neurone associé à un 9.

134
00:07:40,680 --> 00:07:44,267
Que ce soit ou non ce que fait réellement notre réseau final est une autre question, 

135
00:07:44,267 --> 00:07:47,686
sur laquelle je reviendrai une fois que nous aurons vu comment former le réseau, 

136
00:07:47,686 --> 00:07:49,838
mais c'est un espoir que nous pourrions avoir, 

137
00:07:49,838 --> 00:07:52,540
une sorte d'objectif avec la structure en couches. comme ça.

138
00:07:53,160 --> 00:07:56,646
De plus, vous pouvez imaginer à quel point la capacité de détecter des bords et des 

139
00:07:56,646 --> 00:08:00,300
motifs comme celui-ci serait très utile pour d’autres tâches de reconnaissance d’images.

140
00:08:00,880 --> 00:08:02,795
Et même au-delà de la reconnaissance d’images, 

141
00:08:02,795 --> 00:08:05,771
vous pourriez vouloir faire toutes sortes de choses intelligentes qui se 

142
00:08:05,771 --> 00:08:07,280
décomposent en couches d’abstraction.

143
00:08:08,040 --> 00:08:11,245
L'analyse de la parole, par exemple, implique de prendre de l'audio 

144
00:08:11,245 --> 00:08:14,324
brut et de sélectionner des sons distincts, qui se combinent pour former 

145
00:08:14,324 --> 00:08:16,812
certaines syllabes, qui se combinent pour former des mots, 

146
00:08:16,812 --> 00:08:20,060
qui se combinent pour former des phrases et des pensées plus abstraites, etc.

147
00:08:21,100 --> 00:08:24,315
Mais pour en revenir à la façon dont tout cela fonctionne réellement, 

148
00:08:24,315 --> 00:08:27,025
imaginez-vous en train de concevoir comment exactement les 

149
00:08:27,025 --> 00:08:29,920
activations d'une couche pourraient déterminer la suivante.

150
00:08:30,860 --> 00:08:34,706
L'objectif est d'avoir un mécanisme qui pourrait éventuellement 

151
00:08:34,706 --> 00:08:38,980
combiner des pixels en bords, ou des bords en motifs, ou des motifs en chiffres.

152
00:08:39,440 --> 00:08:42,191
Et pour zoomer sur un exemple très spécifique, 

153
00:08:42,191 --> 00:08:45,878
disons que l'espoir est qu'un neurone particulier dans 

154
00:08:45,878 --> 00:08:50,620
la deuxième couche détecte si l'image a ou non un bord dans cette région ici.

155
00:08:51,440 --> 00:08:55,100
La question qui se pose est de savoir quels paramètres le réseau doit-il avoir ?

156
00:08:55,640 --> 00:08:58,762
Quels cadrans et boutons devriez-vous pouvoir modifier pour qu'ils 

157
00:08:58,762 --> 00:09:01,885
soient suffisamment expressifs pour potentiellement capturer ce motif, 

158
00:09:01,885 --> 00:09:04,788
ou tout autre motif de pixels, ou le motif selon lequel plusieurs 

159
00:09:04,788 --> 00:09:07,780
bords peuvent former une boucle, et d'autres choses similaires ?

160
00:09:08,720 --> 00:09:12,163
Eh bien, ce que nous allons faire, c'est attribuer un poids à chacune 

161
00:09:12,163 --> 00:09:15,560
des connexions entre notre neurone et les neurones de la première couche.

162
00:09:16,320 --> 00:09:17,700
Ces poids ne sont que des chiffres.

163
00:09:18,540 --> 00:09:22,109
Prenez ensuite toutes ces activations de la première couche 

164
00:09:22,109 --> 00:09:25,500
et calculez leur somme pondérée en fonction de ces poids.

165
00:09:27,700 --> 00:09:31,153
Je trouve utile de considérer ces poids comme étant organisés dans une petite 

166
00:09:31,153 --> 00:09:34,651
grille qui leur est propre, et je vais utiliser des pixels verts pour indiquer 

167
00:09:34,651 --> 00:09:37,972
des poids positifs, et des pixels rouges pour indiquer des poids négatifs, 

168
00:09:37,972 --> 00:09:41,780
où la luminosité de ce pixel est certaine. représentation vague de la valeur du poids.

169
00:09:42,780 --> 00:09:46,481
Maintenant, si nous rendons nuls les poids associés à presque tous les pixels, 

170
00:09:46,481 --> 00:09:50,370
à l'exception de certains poids positifs dans cette région qui nous intéresse, 

171
00:09:50,370 --> 00:09:54,259
alors prendre la somme pondérée de toutes les valeurs de pixels revient simplement 

172
00:09:54,259 --> 00:09:57,820
à additionner les valeurs du pixel juste en la région qui nous tient à cœur.

173
00:09:59,140 --> 00:10:02,583
Et si vous voulez vraiment savoir s'il y a un bord ici, 

174
00:10:02,583 --> 00:10:06,600
vous pouvez avoir des poids négatifs associés aux pixels environnants.

175
00:10:07,480 --> 00:10:10,150
Ensuite, la somme est la plus grande lorsque ces pixels du milieu 

176
00:10:10,150 --> 00:10:12,700
sont clairs mais que les pixels environnants sont plus sombres.

177
00:10:14,260 --> 00:10:16,720
Lorsque vous calculez une somme pondérée comme celle-ci, 

178
00:10:16,720 --> 00:10:19,655
vous pouvez obtenir n'importe quel nombre, mais pour ce réseau, 

179
00:10:19,655 --> 00:10:23,540
ce que nous voulons, c'est que les activations aient une valeur comprise entre 0 et 1.

180
00:10:24,120 --> 00:10:28,018
Il est donc courant d’injecter cette somme pondérée dans une fonction 

181
00:10:28,018 --> 00:10:32,140
qui écrase la droite numérique réelle dans la plage comprise entre 0 et 1.

182
00:10:32,460 --> 00:10:35,353
Et une fonction courante qui fait cela est appelée fonction sigmoïde, 

183
00:10:35,353 --> 00:10:37,420
également connue sous le nom de courbe logistique.

184
00:10:38,000 --> 00:10:41,440
Fondamentalement, les entrées très négatives finissent près de 0, 

185
00:10:41,440 --> 00:10:45,661
les entrées positives finissent près de 1, et cela augmente régulièrement autour 

186
00:10:45,661 --> 00:10:46,600
de l'entrée 0.

187
00:10:49,120 --> 00:10:52,650
Ainsi, l’activation du neurone ici est essentiellement une 

188
00:10:52,650 --> 00:10:56,360
mesure du degré de positivité de la somme pondérée pertinente.

189
00:10:57,540 --> 00:10:59,580
Mais ce n’est peut-être pas que vous souhaitiez que le 

190
00:10:59,580 --> 00:11:01,880
neurone s’allume lorsque la somme pondérée est supérieure à 0.

191
00:11:02,280 --> 00:11:04,453
Peut-être souhaitez-vous qu'il soit actif uniquement 

192
00:11:04,453 --> 00:11:06,360
lorsque la somme est supérieure à 10, par exemple.

193
00:11:06,840 --> 00:11:10,260
Autrement dit, vous voulez qu'il y ait un certain biais pour qu'il soit inactif.

194
00:11:11,380 --> 00:11:15,472
Ce que nous ferons alors, c'est simplement ajouter un autre nombre comme moins 10 

195
00:11:15,472 --> 00:11:19,660
à cette somme pondérée avant de la brancher via la fonction de squishification sigmoïde.

196
00:11:20,580 --> 00:11:22,440
Ce nombre supplémentaire est appelé le biais.

197
00:11:23,460 --> 00:11:27,317
Ainsi, les poids vous indiquent quel motif de pixels ce neurone de la deuxième 

198
00:11:27,317 --> 00:11:31,224
couche capte, et le biais vous indique à quel point la somme pondérée doit être 

199
00:11:31,224 --> 00:11:35,180
élevée avant que le neurone ne commence à devenir actif de manière significative.

200
00:11:36,120 --> 00:11:37,680
Et ce n'est qu'un neurone.

201
00:11:38,280 --> 00:11:44,500
Tous les autres neurones de cette couche seront connectés aux 784 neurones de pixels 

202
00:11:44,500 --> 00:11:50,940
de la première couche, et chacune de ces 784 connexions est associée à son propre poids.

203
00:11:51,600 --> 00:11:54,624
De plus, chacun a un biais, un autre nombre que vous ajoutez 

204
00:11:54,624 --> 00:11:57,600
à la somme pondérée avant de l'écraser avec le sigmoïde.

205
00:11:58,110 --> 00:11:59,540
Et ça fait beaucoup de choses à penser !

206
00:11:59,960 --> 00:12:06,555
Avec cette couche cachée de 16 neurones, cela représente un total de 784 fois 16 poids, 

207
00:12:06,555 --> 00:12:07,980
ainsi que 16 biais.

208
00:12:08,840 --> 00:12:11,940
Et tout cela ne concerne que les connexions de la première couche à la seconde.

209
00:12:12,520 --> 00:12:15,020
Les connexions entre les autres couches sont également 

210
00:12:15,020 --> 00:12:17,340
associées à un certain nombre de poids et de biais.

211
00:12:18,340 --> 00:12:23,800
Tout compte fait, ce réseau compte presque exactement 13 000 poids et biais au total.

212
00:12:23,800 --> 00:12:26,671
13 000 boutons et cadrans qui peuvent être modifiés et 

213
00:12:26,671 --> 00:12:29,960
tournés pour que ce réseau se comporte de différentes manières.

214
00:12:31,040 --> 00:12:33,402
Ainsi, lorsque nous parlons d'apprentissage, 

215
00:12:33,402 --> 00:12:36,730
cela fait référence à amener l'ordinateur à trouver un paramètre 

216
00:12:36,730 --> 00:12:40,202
valide pour tous ces nombreux nombres afin qu'il résolve réellement 

217
00:12:40,202 --> 00:12:41,360
le problème en question.

218
00:12:42,620 --> 00:12:45,970
Une expérience de pensée à la fois amusante et plutôt horrifiante 

219
00:12:45,970 --> 00:12:50,031
consiste à imaginer s'asseoir et définir tous ces poids et biais à la main, 

220
00:12:50,031 --> 00:12:54,447
en ajustant délibérément les chiffres de sorte que la deuxième couche capte les bords, 

221
00:12:54,447 --> 00:12:56,580
la troisième couche capte les motifs, etc.

222
00:12:56,980 --> 00:13:00,199
Personnellement, je trouve cela satisfaisant plutôt que de simplement 

223
00:13:00,199 --> 00:13:02,406
traiter le réseau comme une boîte noire totale, 

224
00:13:02,406 --> 00:13:04,844
car lorsque le réseau ne fonctionne pas comme prévu, 

225
00:13:04,844 --> 00:13:08,385
si vous avez construit une petite relation avec ce que signifient réellement 

226
00:13:08,385 --> 00:13:12,018
ces poids et ces biais. , vous disposez d’un point de départ pour expérimenter 

227
00:13:12,018 --> 00:13:14,180
comment modifier la structure pour l’améliorer.

228
00:13:14,960 --> 00:13:18,635
Ou lorsque le réseau fonctionne mais pas pour les raisons auxquelles vous pourriez vous 

229
00:13:18,635 --> 00:13:22,227
attendre, examiner l’effet des pondérations et des biais est un bon moyen de remettre 

230
00:13:22,227 --> 00:13:25,820
en question vos hypothèses et d’exposer réellement l’ensemble des solutions possibles.

231
00:13:26,840 --> 00:13:30,680
Au fait, la fonction réelle ici est un peu lourde à écrire, vous ne trouvez pas ?

232
00:13:32,500 --> 00:13:37,140
Alors laissez-moi vous montrer une manière plus compacte de représenter ces connexions.

233
00:13:37,660 --> 00:13:39,172
C'est ainsi que vous le verriez si vous choisissez 

234
00:13:39,172 --> 00:13:40,520
d'en savoir plus sur les réseaux de neurones.

235
00:13:41,380 --> 00:13:50,076
Organisez toutes les activations d’une couche dans une colonne car une matrice correspond 

236
00:13:50,076 --> 00:13:58,000
aux connexions entre une couche et un neurone particulier dans la couche suivante.

237
00:13:58,540 --> 00:14:02,177
Cela signifie que prendre la somme pondérée des activations dans la 

238
00:14:02,177 --> 00:14:05,761
première couche en fonction de ces poids correspond à l'un des 

239
00:14:05,761 --> 00:14:09,880
termes du produit vectoriel matriciel de tout ce que nous avons à gauche ici.

240
00:14:14,000 --> 00:14:17,005
Soit dit en passant, une grande partie de l'apprentissage automatique se résume 

241
00:14:17,005 --> 00:14:19,510
simplement à avoir une bonne compréhension de l'algèbre linéaire, 

242
00:14:19,510 --> 00:14:22,445
donc pour tous ceux d'entre vous qui veulent une bonne compréhension visuelle 

243
00:14:22,445 --> 00:14:25,236
des matrices et de ce que signifie la multiplication vectorielle matricielle, 

244
00:14:25,236 --> 00:14:27,562
jetez un œil à la série que j'ai faite sur algèbre linéaire, 

245
00:14:27,562 --> 00:14:28,600
en particulier le chapitre 3.

246
00:14:29,240 --> 00:14:33,558
Revenons à notre expression, au lieu de parler d'ajouter le biais à chacune de 

247
00:14:33,558 --> 00:14:37,877
ces valeurs indépendamment, nous le représentons en organisant tous ces biais dans 

248
00:14:37,877 --> 00:14:42,300
un vecteur et en ajoutant le vecteur entier au produit vectoriel matriciel précédent.

249
00:14:43,280 --> 00:14:46,618
Ensuite, comme dernière étape, je vais enrouler ici un sigmoïde autour de 

250
00:14:46,618 --> 00:14:49,100
l'extérieur, et ce que cela est censé représenter, 

251
00:14:49,100 --> 00:14:52,980
c'est que vous allez appliquer la fonction sigmoïde à chaque composant spécifique 

252
00:14:52,980 --> 00:14:54,740
du vecteur résultant à l'intérieur.

253
00:14:55,940 --> 00:14:59,734
Ainsi, une fois que vous avez écrit cette matrice de poids et ces vecteurs comme 

254
00:14:59,734 --> 00:15:03,809
leurs propres symboles, vous pouvez communiquer la transition complète des activations 

255
00:15:03,809 --> 00:15:08,024
d'une couche à la suivante dans une petite expression extrêmement précise et soignée, 

256
00:15:08,024 --> 00:15:12,240
ce qui rend le code correspondant à la fois beaucoup plus simple et beaucoup plus rapide, 

257
00:15:12,240 --> 00:15:15,660
car de nombreuses bibliothèques optimisent la multiplication matricielle.

258
00:15:17,820 --> 00:15:19,732
Vous souvenez-vous que j'ai dit plus tôt que ces neurones 

259
00:15:19,732 --> 00:15:21,460
étaient simplement des éléments contenant des chiffres ?

260
00:15:22,220 --> 00:15:25,993
Bien sûr, les nombres spécifiques qu'ils contiennent dépendent de 

261
00:15:25,993 --> 00:15:30,145
l'image que vous alimentez, il est donc plus précis de considérer chaque 

262
00:15:30,145 --> 00:15:34,080
neurone comme une fonction, une fonction qui prend en compte les sorties 

263
00:15:34,080 --> 00:15:38,340
de tous les neurones de la couche précédente et crache un nombre. entre 0 et 1.

264
00:15:39,200 --> 00:15:42,911
En réalité, l'ensemble du réseau n'est qu'une fonction, 

265
00:15:42,911 --> 00:15:47,060
une fonction qui prend 784 nombres en entrée et crache 10 nombres en sortie.

266
00:15:47,560 --> 00:15:49,804
C'est une fonction absurdement compliquée, 

267
00:15:49,804 --> 00:15:53,719
qui implique 13 000 paramètres sous la forme de ces poids et biais qui reprennent 

268
00:15:53,719 --> 00:15:57,635
certains modèles, et qui implique l'itération de nombreux produits vectoriels 

269
00:15:57,635 --> 00:16:00,261
matriciels et la fonction de squishification sigmoïde, 

270
00:16:00,261 --> 00:16:03,890
mais ce n'est néanmoins qu'une fonction, et dans un de toute façon, 

271
00:16:03,890 --> 00:16:06,660
c'est plutôt rassurant que ça ait l'air compliqué.

272
00:16:07,340 --> 00:16:09,791
Je veux dire, si c'était plus simple, quel espoir aurions-nous 

273
00:16:09,791 --> 00:16:12,280
qu'il puisse relever le défi de la reconnaissance des chiffres ?

274
00:16:13,340 --> 00:16:14,700
Et comment relève-t-il ce défi ?

275
00:16:15,080 --> 00:16:17,200
Comment ce réseau apprend-il les pondérations et les 

276
00:16:17,200 --> 00:16:19,360
biais appropriés simplement en examinant les données ?

277
00:16:20,140 --> 00:16:22,455
Eh bien, c'est ce que je vais montrer dans la prochaine vidéo, 

278
00:16:22,455 --> 00:16:25,566
et je vais également approfondir un peu plus ce que fait réellement ce réseau particulier 

279
00:16:25,566 --> 00:16:26,120
que nous voyons.

280
00:16:27,580 --> 00:16:29,933
C'est maintenant le point que je suppose que je devrais dire, 

281
00:16:29,933 --> 00:16:33,141
abonnez-vous pour rester informé de la sortie d'une vidéo ou de toute nouvelle vidéo, 

282
00:16:33,141 --> 00:16:35,744
mais en réalité, la plupart d'entre vous ne reçoivent pas réellement 

283
00:16:35,744 --> 00:16:37,420
de notifications de YouTube, n'est-ce pas ?

284
00:16:38,020 --> 00:16:41,332
Peut-être plus honnêtement, je devrais dire de vous abonner pour que les réseaux de 

285
00:16:41,332 --> 00:16:44,685
neurones qui sous-tendent l'algorithme de recommandation de YouTube soient prêts 

286
00:16:44,685 --> 00:16:47,880
à croire que vous souhaitez voir le contenu de cette chaîne vous être recommandé.

287
00:16:48,560 --> 00:16:49,940
Quoi qu'il en soit, restez informé pour en savoir plus.

288
00:16:50,760 --> 00:16:53,500
Merci beaucoup à tous ceux qui soutiennent ces vidéos sur Patreon.

289
00:16:54,000 --> 00:16:57,456
J'ai été un peu lent à progresser dans la série de probabilités cet été, 

290
00:16:57,456 --> 00:17:01,316
mais je m'y remets après ce projet, afin que les clients puissent y consulter les 

291
00:17:01,316 --> 00:17:01,900
mises à jour.

292
00:17:03,600 --> 00:17:06,305
Pour conclure, j'ai avec moi Leisha Lee qui a fait son doctorat 

293
00:17:06,305 --> 00:17:09,090
sur le côté théorique de l'apprentissage profond et qui travaille 

294
00:17:09,090 --> 00:17:11,994
actuellement dans une société de capital-risque appelée Amplify Partners 

295
00:17:11,994 --> 00:17:14,619
qui a aimablement fourni une partie du financement de cette vidéo.

296
00:17:15,460 --> 00:17:17,433
Donc, Leisha, une chose que je pense que nous devrions 

297
00:17:17,433 --> 00:17:19,119
rapidement évoquer est cette fonction sigmoïde.

298
00:17:19,700 --> 00:17:23,211
Si je comprends bien, les premiers réseaux utilisent cela pour écraser la somme pondérée 

299
00:17:23,211 --> 00:17:25,618
pertinente dans cet intervalle entre zéro et un, vous savez, 

300
00:17:25,618 --> 00:17:28,893
en quelque sorte motivés par cette analogie biologique selon laquelle les neurones 

301
00:17:28,893 --> 00:17:29,840
sont inactifs ou actifs.

302
00:17:30,280 --> 00:17:30,300
Exactement.

303
00:17:30,560 --> 00:17:34,040
Mais relativement peu de réseaux modernes utilisent encore le sigmoïde.

304
00:17:34,320 --> 00:17:34,320
Ouais.

305
00:17:34,440 --> 00:17:35,540
C'est un peu old school, non ?

306
00:17:35,760 --> 00:17:38,980
Ouais ou plutôt Relu semble être beaucoup plus facile à entraîner.

307
00:17:39,400 --> 00:17:42,340
Et relu signifie unité linéaire rectifiée ?

308
00:17:42,680 --> 00:17:47,300
Oui, c'est ce genre de fonction où vous prenez juste un maximum de zéro et a où 

309
00:17:47,300 --> 00:17:52,030
a est donné par ce que vous expliquiez dans la vidéo et ce qui était en quelque sorte 

310
00:17:52,030 --> 00:17:56,815
motivé, je pense, c'était en partie par une analogie biologique avec la façon dont 

311
00:17:56,815 --> 00:18:01,050
les neurones serait activé ou non et donc s'il dépasse un certain seuil, 

312
00:18:01,050 --> 00:18:05,064
ce serait la fonction d'identité, mais si ce n'était pas le cas, 

313
00:18:05,064 --> 00:18:08,475
il ne serait tout simplement pas activé, donc ce serait zéro, 

314
00:18:08,475 --> 00:18:10,840
donc c'est une sorte de simplification.

315
00:18:11,160 --> 00:18:15,682
L'utilisation des sigmoïdes n'a pas aidé à l'entraînement ou c'était 

316
00:18:15,682 --> 00:18:20,257
très difficile de s'entraîner à un moment donné et les gens ont juste essayé Relu 

317
00:18:20,257 --> 00:18:24,620
et cela a très bien fonctionné pour ces réseaux neuronaux incroyablement profonds.

318
00:18:25,100 --> 00:18:25,640
Très bien, merci Alicia.

