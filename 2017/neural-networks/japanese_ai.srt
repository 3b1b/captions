1
00:00:00,000 --> 00:00:11,200
これは 3 です。雑に書かれ、28x28 ピクセルという非常に低い解像度でレンダリングされ

2
00:00:11,200 --> 00:00:15,340
ていますが、脳は問題なく 3 として認識します。そして、脳がこれを簡単に実行でき

3
00:00:15,340 --> 00:00:20,500
ることがどれほどクレイジーであるかを少し理解してもらいたいのです。つまり、各ピ

4
00:00:20,500 --> 00:00:26,180
クセルの具体的な値は画像ごとに大きく異なりますが、これ、これ、こ

5
00:00:26,180 --> 00:00:31,260
れも 3 として認識できます。この 3 つを見たときに発火する目の特

6
00:00:31,260 --> 00:00:36,020
定の光感受性細胞は、この 3 つを見たときに発火する細胞とは大きく異なり

7
00:00:36,020 --> 00:00:42,900
ます。しかし、あなたのその非常に賢い視覚野の何かは、これらを同じアイデアを表す

8
00:00:42,900 --> 00:00:49,300
ものとして解決し、同時に他の画像をそれら自身の異なるアイデアとして認識します。

9
00:00:49,300 --> 00:00:55,820
しかし、もし私が、「おい、座って、28x28 のグリッドを取り込んで 0 から 10 まで

10
00:00:56,340 --> 00:01:01,780
の 1 つの数値を出力し、その数字が何であるかを示すプログラムを書いてくれ」と言ったら、そ

11
00:01:01,780 --> 00:01:07,860
のタスクは滑稽なほど簡単なものから、気の遠くなるような難しい。岩の下で生きている人で

12
00:01:07,860 --> 00:01:12,020
ない限り、機械学習とニューラル ネットワークの現在と将来に対する関連性と重

13
00:01:12,020 --> 00:01:16,460
要性を説く必要はほとんどないと思います。しかし、私がここでやりたいのは、背景が何もないことを前提

14
00:01:16,460 --> 00:01:22,020
として、ニューラル ネットワークが実際にどのようなものであるかを示し、バズワードとしてではなく数学の一部として、ニューラル ネット

15
00:01:22,060 --> 00:01:26,860
ワークが何を行っているのかを視覚化するのに役立つことです。私の願いは、構造そのものが動機づけられていると感じて帰っ

16
00:01:26,860 --> 00:01:31,460
てきて、ニューラル ネットワークのクォートアンクォート学習について読んだり聞いたりしたときに、それが

17
00:01:31,460 --> 00:01:36,780
何を意味するのかわかったように感じてもらえることだけです。このビデオではその構造コンポ

18
00:01:36,780 --> 00:01:40,300
ーネントについてのみ説明し、次のビデオでは学習に取り組みます。

19
00:01:40,300 --> 00:01:45,580
私たちがやろうとしているのは、手書きの数字の認識を学習できるニューラル ネットワークを構築すること

20
00:01:45,580 --> 00:01:53,540
です。これは、このトピックを紹介するためのやや古典的な例です。ここでは現状のま

21
00:01:53,540 --> 00:01:57,340
まで構いません。2 つのビデオの最後に、詳細を学ぶことができるいくつかの優れ

22
00:01:57,340 --> 00:02:01,420
たリソースを示したいからです。これを行うコードをダウンロードして、自分のコ

23
00:02:01,420 --> 00:02:07,820
ンピュータで試すことができます。ニューラル ネットワークには多くの亜種があり

24
00:02:07,820 --> 00:02:12,900
、近年、これらの亜種に対する研究が一種のブームになっていますが、これら 2

25
00:02:12,940 --> 00:02:18,100
つの紹介ビデオでは、余分な装飾のない、最も単純なプレーン バニラ形式を見

26
00:02:18,100 --> 00:02:23,020
ていきます。これは、より強力な最新の亜種を理解するために必要な前提条件のよ

27
00:02:23,020 --> 00:02:28,140
うなものですが、私たちが理解するにはまだ複雑さがたくさんあると信じてくださ

28
00:02:28,140 --> 00:02:33,440
い。しかし、この最も単純な形式であっても、手書きの数字を認識することを学習できます。これは

29
00:02:33,440 --> 00:02:39,380
コンピューターにとって非常に素晴らしいことです。そして同時に、それが私たちがそれに対して抱くかもしれ

30
00:02:39,460 --> 00:02:45,620
ないいくつかの希望をいかに満たしていないのかもわかるでしょう。名前が示すように、ニューラル ネットワークは脳からイン

31
00:02:45,620 --> 00:02:50,820
スピレーションを得ていますが、それを詳しく見てみましょう。ニューロンとは何ですか?それらはどのような意味でつながって

32
00:02:50,820 --> 00:02:56,900
いるのでしょうか?今、私がニューロンと言うときに考えていただきたいのは、数値、具体的には

33
00:02:56,900 --> 00:03:04,380
0 と 1 の間の数値を保持するものだけです。本当にそれ以上ではありません。たとえば、ネッ

34
00:03:04,420 --> 00:03:10,060
トワークは、入力画像の 28 x 28 ピクセルのそれぞれに対応するニューロンの束から始ま

35
00:03:10,060 --> 00:03:17,260
り、合計 784 個のニューロンになります。これらのそれぞれには、黒ピクセルの 0 か

36
00:03:17,260 --> 00:03:23,900
ら白ピクセルの 1 までの、対応するピクセルのグレースケール値を表す数値が保持されま

37
00:03:23,900 --> 00:03:30,060
す。ニューロン内のこの数値は活性化と呼ばれ、活性化の数値が高いと各ニュー

38
00:03:30,060 --> 00:03:37,260
ロンが点灯するというイメージを思い浮かべるかもしれません。したがって、これら 7

39
00:03:37,260 --> 00:03:47,820
84 個のニューロンすべてがネットワークの最初の層を構成します。最後の層に飛びます。これには 10 個

40
00:03:47,820 --> 00:03:53,780
のニューロンがあり、それぞれが数字の 1 つを表します。これらのニューロンの活性化は、

41
00:03:53,780 --> 00:03:59,460
やはり 0 と 1 の間の数値であり、特定の画像が特定の数字に対応しているとシステ

42
00:03:59,500 --> 00:04:05,180
ムがどの程度考えているかを表します。間には隠れ層と呼ばれるいくつかの層も

43
00:04:05,180 --> 00:04:10,780
ありますが、当面はこの数字を認識するプロセスが一体どのように処理され

44
00:04:10,780 --> 00:04:15,900
るのかという大きな疑問符が付くはずです。このネットワークでは、それぞれ 16 個の

45
00:04:15,900 --> 00:04:21,460
ニューロンを持つ 2 つの隠れ層を選択しましたが、確かに、これは一種の恣意的な選択です。正直に言うと、

46
00:04:21,460 --> 00:04:26,620
一瞬のうちに構造をどのように動かしたいかに基づいて 2 つのレイヤーを選択しました。16

47
00:04:26,620 --> 00:04:30,940
は、画面に収まるちょうどいい数でした。実際には、ここには特定の構造を実験する余

48
00:04:30,940 --> 00:04:37,020
地がたくさんあります。ネットワークの動作方法では、ある層でのアクティベーションが次の層のアク

49
00:04:37,020 --> 00:04:42,340
ティベーションを決定します。そしてもちろん、情報処理メカニズムとしてのネットワーク

50
00:04:42,340 --> 00:04:47,820
の中心は、ある層からの活性化が次の層の活性化をどのように引き起こすかということ

51
00:04:47,820 --> 00:04:53,340
になります。これは、ニューロンの生物学的ネットワークにおいて、あるニューロンのグループの発火が他のニュー

52
00:04:53,380 --> 00:04:59,380
ロンの発火を引き起こす方法に大まかに似ていることを意図しています。ここで示しているネットワークはすでに数

53
00:04:59,380 --> 00:05:04,260
字を認識するように訓練されています。これが何を意味するのかを説明しましょう。これは、画像内の

54
00:05:04,260 --> 00:05:10,900
各ピクセルの明るさに応じて入力層の 784 個のニューロンすべてを点灯する画像を入力す

55
00:05:10,900 --> 00:05:16,860
ると、その活性化パターンが次の層で非常に特殊なパターンを引き起こし、その次の層で何らか

56
00:05:16,860 --> 00:05:21,740
のパターンが発生することを意味します。これにより、最終的に出力層にパターンが与えられま

57
00:05:21,780 --> 00:05:27,540
す。そして、その出力層の最も明るいニューロンは、いわば、この画像が表す桁をネットワ

58
00:05:27,540 --> 00:05:35,420
ークが選択することになります。ある層が次の層にどのような影響を与えるか、またはトレーニングがど

59
00:05:35,420 --> 00:05:40,460
のように機能するかについての数学に入る前に、このような層構造がインテリジェントに動作すると期待する

60
00:05:40,460 --> 00:05:46,340
のが合理的である理由について少しお話しましょう。ここで私たちは何を期待しているのでしょうか？これらの中間層がやっている

61
00:05:46,420 --> 00:05:52,420
可能性のあることに対する最善の希望は何でしょうか?さて、あなたまたは私が数字を認識するとき、私たちはさまざまなコンポーネ

62
00:05:52,420 --> 00:05:58,980
ントを組み合わせます。A 9 には上部にループがあり、右側にラインがあります。8 にもトップにループがあります

63
00:05:58,980 --> 00:06:05,420
が、ローに別のループが組み合わされています。A 4 は基本的に 3 つの特定の行に分

64
00:06:05,420 --> 00:06:11,500
かれています。これで完璧な世界では、最後から 2 番目の層の各ニューロンがこれらのサブコン

65
00:06:11,740 --> 00:06:17,460
ポーネントの 1 つに対応し、たとえば 9 や 8 などの上部にループがある画像を入力するたび

66
00:06:17,460 --> 00:06:23,060
に、いくつかのサブコンポーネントが存在することを期待できます。活性化が 1 に近づく特定のニュ

67
00:06:23,060 --> 00:06:28,620
ーロン。そして、私はこの特定のピクセルのループを意味するのではなく、上部に向かう一般的なループ状のパ

68
00:06:28,620 --> 00:06:33,980
ターンがこのニューロンを引き起こすことを期待しています。そうすれば、3 番目の層から最後の層

69
00:06:33,980 --> 00:06:39,380
に進むには、サブコンポーネントのどの組み合わせがどの数字に対応するかを学習するだけで

70
00:06:39,380 --> 00:06:44,020
済みます。もちろん、これは将来の問題を引き起こすだけです。なぜなら、これらのサブコンポーネントをどうやって認識

71
00:06:44,020 --> 00:06:48,340
するのでしょうか、あるいは、適切なサブコンポーネントが何であるべきかを知ることさえできるでしょうか?あるレイヤーが次

72
00:06:48,340 --> 00:06:52,900
のレイヤーにどのような影響を与えるかについてはまだ話していませんが、この点について少し一緒に考えてみましょう。

73
00:06:52,900 --> 00:06:59,020
ループを認識すると、サブ問題に分解することもできます。これを行うための合理的な方法の 1 つは

74
00:06:59,020 --> 00:07:05,640
、まずそれを構成するさまざまな小さなエッジを認識することです。同様に、数字の 1、4、または

75
00:07:05,640 --> 00:07:11,280
7 に見られるような長い線も、実際には単なる長いエッジであるか、あるいはいくつかの小さなエ

76
00:07:11,280 --> 00:07:18,440
ッジからなる特定のパターンと考えることもできます。したがって、おそらく私たちの希望は、ネットワーク

77
00:07:18,440 --> 00:07:24,680
の 2 番目の層の各ニューロンが、関連するさまざまな小さなエッジに対応することです。おそらく、このよう

78
00:07:24,680 --> 00:07:30,760
な画像が入力されると、約 8 ～ 10 個の特定の小さなエッジに関連付けられたすべてのニ

79
00:07:31,040 --> 00:07:36,480
ューロンが点灯し、次に上部のループと長い垂直線に関連付けられたニューロンが点灯し、それら

80
00:07:36,480 --> 00:07:41,960
のニューロンが点灯します。 9に関連するニューロン。これが最終的なネットワークが実際に行うこ

81
00:07:41,960 --> 00:07:46,560
とであるかどうかは別の問題であり、ネットワークをトレーニングする方法がわかったら、またこの問題に

82
00:07:46,560 --> 00:07:51,800
戻ります。しかし、これは私たちが持つかもしれない希望であり、このような階層構造を持つ一種の目

83
00:07:51,800 --> 00:07:57,440
標です。さらに、このようにエッジやパターンを検出できれば、他の画像認識タ

84
00:07:57,480 --> 00:08:02,440
スクにも非常に役立つことが想像できます。画像認識を超えて、抽象化の層

85
00:08:02,440 --> 00:08:06,640
に分割して実行したいあらゆる種類のインテリジェントな処理が存在し

86
00:08:06,640 --> 00:08:12,640
ます。たとえば、音声の解析には、生の音声を取り込み、特定の音節を形成するために

87
00:08:12,640 --> 00:08:17,760
結合したり、単語を形成したり結合してフレーズやより抽象的な思考を構成したりする個

88
00:08:17,760 --> 00:08:23,360
別の音を抽出することが含まれます。しかし、これが実際にどのように機能するかに戻って、ある層

89
00:08:23,400 --> 00:08:29,160
のアクティベーションが次の層のアクティベーションをどのように正確に決定するかを今設計している自分を

90
00:08:29,160 --> 00:08:35,320
想像してみてください。目標は、ピクセルをエッジに結合したり、エッジをパターンに結合したり、パター

91
00:08:35,320 --> 00:08:41,040
ンを数字に結合したりできる何らかのメカニズムを持つことです。そして、1 つの非常に具体的な例にズー

92
00:08:41,040 --> 00:08:47,440
ムインするために、2 番目の層の 1 つの特定のニューロンが、画像のこの領域にエッジがあるかどう

93
00:08:47,680 --> 00:08:54,440
かを検出することが期待されているとします。ここでの問題は、ネットワークにどのようなパラメータが必要かとい

94
00:08:54,440 --> 00:09:00,440
うことです。このパターンやその他のピクセル パターン、あるいは複数のエッジがループを

95
00:09:00,440 --> 00:09:05,880
作るパターンなどを潜在的に捉えるのに十分な表現力を持たせるためには、どのダイヤルや

96
00:09:05,880 --> 00:09:11,680
ノブを調整すればよいでしょうか?さて、これから行うことは、ニューロンと最初の層の

97
00:09:11,680 --> 00:09:17,160
ニューロンの間の接続のそれぞれに重みを割り当てることです。これらの重みは単なる数値

98
00:09:17,160 --> 00:09:23,960
です。次に、最初の層からこれらのアクティベーションをすべて取得し、これらの重みに従って重み付け

99
00:09:23,960 --> 00:09:30,400
された合計を計算します。これらの重みが独自の小さなグリッドに編成されていると考える

100
00:09:30,400 --> 00:09:35,200
とわかりやすいと思います。正の重みを示すために緑のピクセルを使用し、負の重みを示

101
00:09:35,200 --> 00:09:40,760
すために赤のピクセルを使用します。そのピクセルの明るさはある程度です。重みの値

102
00:09:40,760 --> 00:09:45,880
のゆるやかな描写。関心のあるこの領域のいくつかの正の重みを除いて、ほぼすべて

103
00:09:45,880 --> 00:09:51,200
のピクセルに関連付けられた重みをゼロにすると、すべてのピクセル値の重み付き

104
00:09:51,200 --> 00:09:56,360
合計を取ることは、実際にはピクセルの値を合計することになります。私たちが大

105
00:09:56,360 --> 00:10:02,760
切にしている地域。ここにエッジがあるかどうかを本当に確認したい場合は

106
00:10:02,760 --> 00:10:07,960
、周囲のピクセルに負の重みを関連付けることが考えられます。中央のピ

107
00:10:08,000 --> 00:10:12,680
クセルが明るく、周囲のピクセルが暗い場合、合計は最大になります。

108
00:10:12,680 --> 00:10:19,200
このように加重合計を計算すると、任意の数値が得られる可能性がありますが、このネットワークで必要

109
00:10:19,200 --> 00:10:25,200
なのは、アクティベーションが 0 と 1 の間の値であることです。したがって、一般的に行うべき

110
00:10:25,200 --> 00:10:30,560
ことは、この重み付けされた合計を、実数直線を 0 と 1 の間の範囲に押し込む何らかの関数にポンプ

111
00:10:30,560 --> 00:10:36,360
することです。これを行う一般的な関数はシグモイド関数と呼ばれ、ロジスティック曲線としても知

112
00:10:36,360 --> 00:10:42,760
られています。基本的に、非常に負の入力は 0 に近くなり、非常に正の入力は 1

113
00:10:42,760 --> 00:10:51,400
に近くなり、入力 0 の周りで着実に増加します。したがって、ここでのニューロンの活性化は

114
00:10:51,400 --> 00:10:59,320
、基本的に、関連する加重合計がどれだけ正であるかの尺度になります。しかし、重み付けされた合計が 0 よ

115
00:10:59,320 --> 00:11:04,080
り大きいときにニューロンを点灯させたいわけではないかもしれません。おそらく、合計が 10 よりも大きい場合に

116
00:11:04,120 --> 00:11:11,520
のみアクティブにしたい場合があります。つまり、非アクティブにするために何らかのバイアスが必要です。次に行うこ

117
00:11:11,520 --> 00:11:17,560
とは、シグモイド圧縮関数に接続する前に、この加重合計にマイナス 10

118
00:11:17,560 --> 00:11:23,840
などの他の数値を加算することです。この追加の数はバイアスと呼ばれます。したがっ

119
00:11:23,840 --> 00:11:29,080
て、重みは、2 番目の層のこのニューロンがどのピクセル パターンを認識しているかを示し、バイアスは、ニュー

120
00:11:29,120 --> 00:11:34,640
ロンが意味のあるアクティブになり始める前に、重み付けされた合計がどのくらい大きくなければならないかを示し

121
00:11:34,640 --> 00:11:41,760
ます。そしてそれは単なる 1 つのニューロンです。この層の 1 つおきのニューロンは、最初の層の 784 個

122
00:11:41,760 --> 00:11:49,080
のピクセル ニューロンすべてに接続され、これらの 784 個の接続のそれぞれに独自の重

123
00:11:49,080 --> 00:11:55,320
みが関連付けられます。また、それぞれには何らかのバイアスがあり、シグモイドで押しつぶす前

124
00:11:55,320 --> 00:12:00,600
に加重和に加算する他の数値が含まれています。そして、それは考えるべきことがたくさんあります！この 16 ニュー

125
00:12:00,600 --> 00:12:09,280
ロンの隠れ層では、合計 784 × 16 の重みと 16 のバイアスになります。これらはすべて

126
00:12:09,280 --> 00:12:13,760
、第 1 層から第 2 層への接続にすぎません。他の層間の接続にも、それ

127
00:12:13,760 --> 00:12:19,600
らに関連する一連の重みとバイアスがあります。結局のところ、このネットワーク

128
00:12:19,600 --> 00:12:26,680
には、合計でほぼ正確に 13,000 の重みとバイアスがあります。13,000 個のノブとダイヤルを調整した

129
00:12:26,680 --> 00:12:32,400
り回して、このネットワークをさまざまな方法で動作させることができます。したがって、学習について話すとき

130
00:12:32,400 --> 00:12:38,440
、それが指しているのは、目前の問題を実際に解決できるように、これらの多くの数値すべてに対し

131
00:12:38,440 --> 00:12:44,400
て有効な設定をコンピューターに見つけさせることです。楽しくもあり、ある意味恐ろしくもある

132
00:12:44,400 --> 00:12:49,440
思考実験の 1 つは、座ってこれらの重みとバイアスをすべて手動で設定し、2 番目の

133
00:12:49,440 --> 00:12:53,960
レイヤーがエッジを認識し、3 番目のレイヤーがパターンを認識するように数値を意図的に

134
00:12:53,960 --> 00:12:59,680
微調整することを想像することです。等私個人としては、ネットワークを完全なブラック ボ

135
00:12:59,680 --> 00:13:04,400
ックスとして扱うよりも、これで満足できると感じています。なぜなら、ネットワークが期待

136
00:13:04,400 --> 00:13:09,040
どおりに動作しないとき、それらの重みやバイアスが実際に何を意味するのかについて少しでも

137
00:13:09,040 --> 00:13:13,440
関係を構築していれば、 、改善するために構造を変更する方法を実験するための出発点が得

138
00:13:13,440 --> 00:13:17,680
られます。あるいは、ネットワークが機能しているものの、予想どおりの理由ではない場合、重

139
00:13:17,680 --> 00:13:22,760
みとバイアスが何をしているのかを掘り下げることは、自分の仮定に疑問を投げかけ、考えられ

140
00:13:22,760 --> 00:13:28,560
る解決策の全領域を実際に明らかにする良い方法です。ところで、ここで実際の関

141
00:13:28,560 --> 00:13:34,840
数を書くのは少し面倒ですよね。そこで、これらの接続をよりコン

142
00:13:34,840 --> 00:13:39,200
パクトに表記する方法を示しましょう。ニューラル ネットワークについてさらに詳しく読ん

143
00:13:39,200 --> 00:13:45,360
でみると、次のようになります。1 つのレイヤーのすべてのアクティベーションをベクトルとして列に編成

144
00:13:45,480 --> 00:13:53,400
します。次に、すべての重みを行列として編成します。行列の各行は、あ

145
00:13:53,400 --> 00:13:58,680
る層と次の層の特定のニューロンの間の接続に対応します。これが意味

146
00:13:58,680 --> 00:14:03,360
するのは、これらの重みに従って最初の層のアクティベーションの重み付き合計を取

147
00:14:03,360 --> 00:14:08,880
得すると、左側にあるすべての行列ベクトル積の項の 1 つに対応するということ

148
00:14:08,880 --> 00:14:17,840
です。ちなみに、機械学習の多くは線形代数をよく理解することに尽き

149
00:14:17,840 --> 00:14:23,000
るので、行列と行列ベクトルの乗算の意味を視覚的に理解したい人は、

150
00:14:23,000 --> 00:14:29,320
私が行ったシリーズを見てください。線形代数、特に第 3 章。式に戻

151
00:14:29,320 --> 00:14:34,200
ると、これらの値のそれぞれに独立してバイアスを追加することについて話すのではなく、こ

152
00:14:34,200 --> 00:14:40,440
れらすべてのバイアスをベクトルに編成し、ベクトル全体を前の行列ベクトル積に追加する

153
00:14:40,440 --> 00:14:47,240
ことによってそれを表します。次に、最後のステップとして、ここで外側にシグモイドを巻き

154
00:14:47,240 --> 00:14:51,480
付けます。これが表すことは、結果として得られるベクトルの内部の特定の各コンポーネン

155
00:14:51,480 --> 00:14:58,120
トにシグモイド関数を適用することになります。したがって、この重み行列とこれらのベク

156
00:14:58,120 --> 00:15:03,320
トルを独自のシンボルとして書き留めると、ある層から次の層へのアクティベーションの完

157
00:15:03,480 --> 00:15:08,840
全な移行を非常に厳密できちんとした小さな式で伝えることができ、これにより関連するコ

158
00:15:08,840 --> 00:15:14,600
ードがはるかに単純になり、多くのライブラリが行列の乗算を最適化しているため、はるかに

159
00:15:14,600 --> 00:15:21,400
高速になります。先ほど、これらのニューロンは単に数字を保持するものであると述べたことを覚えていますか?

160
00:15:22,120 --> 00:15:26,280
もちろん、それらが保持する具体的な数値は、入力した画像によって異なり

161
00:15:28,120 --> 00:15:31,960
ます。そのため、実際には、各ニューロンを関数として考える方が正確で

162
00:15:31,960 --> 00:15:37,240
す。関数は、前の層のすべてのニューロンの出力を受け取り、 0から1ま

163
00:15:37,240 --> 00:15:43,800
での数値。実際、ネットワーク全体は単なる関数であり、784 個の数値を入力として受

164
00:15:43,800 --> 00:15:49,720
け取り、10 個の数値を出力として吐き出します。これは途方もなく複雑な関数であ

165
00:15:49,720 --> 00:15:54,520
り、特定のパターンを検出する重みとバイアスの形式で 13,000

166
00:15:54,520 --> 00:15:59,000
のパラメーターが含まれ、多くの行列ベクトル積とシグモイド潰し関数

167
00:15:59,000 --> 00:16:04,760
の反復が含まれますが、それでも単なる関数です。そして、それが複雑に見える

168
00:16:04,760 --> 00:16:09,720
ことは、ある意味、安心感を与えます。つまり、もしそれがもっと単純だったら、数字を認識するとい

169
00:16:09,720 --> 00:16:14,920
う課題に挑戦できるなんて、どんな希望が持てるでしょうか?そして、その課題にどのように取り組むのでしょうか?

170
00:16:14,920 --> 00:16:19,320
このネットワークは、データを見るだけで適切な重みとバイアスをどのように学習するのでしょうか?

171
00:16:19,880 --> 00:16:23,960
それについては次のビデオで説明します。また、この特定のネットワークが実際に何をしているのかに

172
00:16:23,960 --> 00:16:29,880
ついてももう少し詳しく説明します。ここで重要なのは、そのビデオや新しいビデオが公開されたときの

173
00:16:29,880 --> 00:16:34,840
通知を受け取るために購読すると言うべきだと思いますが、現実的には、ほとんどの人が実際に Yo

174
00:16:34,840 --> 00:16:39,880
uTube からの通知を受け取っていませんよね。おそらくもっと正直に言うと、YouTube

175
00:16:39,880 --> 00:16:44,920
の推奨アルゴリズムの基礎をなすニューラル ネットワークが、ユーザーがこのチャンネルのコンテンツを

176
00:16:44,920 --> 00:16:49,800
推奨されると信じ込むように、チャンネル登録するというべきでしょう。とにかく、続報をお待ちください。

177
00:16:50,600 --> 00:16:54,840
Patreon でこれらのビデオをサポートしてくださった皆様に心より感謝いたします。この夏は確率シリーズの進歩が少

178
00:16:54,840 --> 00:16:59,160
し遅れていましたが、このプロジェクトの後はまた本格的に取り組むつもりですので、パトロンの皆様はそこ

179
00:16:59,160 --> 00:17:05,640
で最新情報をチェックしていただければ幸いです。最後に、深層学習の理論面で博士号の研究をし、

180
00:17:05,640 --> 00:17:09,880
現在はこのビデオに資金の一部を提供してくれた Amplify Partners と

181
00:17:09,880 --> 00:17:14,520
いうベンチャー キャピタル会社で働いている Leesha Lee に話を聞きます。

182
00:17:15,160 --> 00:17:19,480
そこで、リーシャ、すぐに取り上げるべきだと思うのが、このシグモイド関数です。

183
00:17:19,480 --> 00:17:23,400
私が理解しているところによると、初期のネットワークはこれを使用して、関連する重み付けされた合計を 0 と

184
00:17:23,400 --> 00:17:28,200
1 の間の区間に押し込みます。ニューロンが非アクティブまたはアクティブであるという生物学的な類似によって動

185
00:17:28,200 --> 00:17:33,240
機づけられているのはご存知でしょう。その通り。しかし、実際にシグモイドを使用している現代のネットワークは

186
00:17:33,240 --> 00:17:37,800
比較的少数です。うん。それはちょっと古い学校ですよね？そうですね、むしろレルのほうが訓練しや

187
00:17:37,800 --> 00:17:43,880
すいようです。そして、relu、reluは整流リニアユニットの略ですか？はい、これはこの種の関数

188
00:17:43,880 --> 00:17:50,280
で、最大ゼロと a を取得するだけです。ここで、a はビデオで説明した内容によって与え

189
00:17:50,280 --> 00:17:56,440
られます。これは、ニューロンがどのように活性化されるか活性化されないかに関する生物学的

190
00:17:56,440 --> 00:18:03,640
な類似から部分的に動機付けられたものだと思います。したがって、特定のしきい値を超えた場

191
00:18:03,640 --> 00:18:09,080
合は恒等関数になりますが、超えなかった場合はアクティブ化されないため、ゼロになります

192
00:18:09,080 --> 00:18:13,640
。つまり、一種の簡略化です。シグモイドの使用はトレーニングに役立たなかったり、ある時点でトレ

193
00:18:13,640 --> 00:18:21,320
ーニングが非常に困難になったりしていましたが、人々は relu を試してみたところ、たまたまこれらの信じられな

194
00:18:21,320 --> 00:18:26,120
いほど深いニューラル ネットワークで非常にうまく機能しました。わかりました、リーシャ、ありがとう。

195
00:18:39,080 --> 00:18:40,060


