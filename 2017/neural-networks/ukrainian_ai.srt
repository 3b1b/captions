1
00:00:00,000 --> 00:00:11,200
Це 3. Він неохайно написаний і відтворений із надзвичайно низькою роздільною здатністю 28x28 пікселів, але

2
00:00:11,200 --> 00:00:15,340
ваш мозок без проблем розпізнає його як 3. І я хочу, щоб ви знайшли хвилинку,

3
00:00:15,340 --> 00:00:20,500
щоб оцінити, наскільки це божевілля, що мозок може робити це так легко. Я маю на

4
00:00:20,500 --> 00:00:26,180
увазі, що це, це і це також розпізнаються як 3s, навіть якщо конкретні значення кожного пікселя

5
00:00:26,180 --> 00:00:31,260
сильно відрізняються від одного зображення до іншого. Конкретні світлочутливі клітини у вашому оці, які спрацьовують,

6
00:00:31,260 --> 00:00:36,020
коли ви бачите це 3, дуже відрізняються від тих, що спрацьовують, коли ви бачите це

7
00:00:36,020 --> 00:00:42,900
3. Але щось у вашій шалено-розумній зоровій корі розпізнає їх як такі, що представляють

8
00:00:42,900 --> 00:00:49,300
одну і ту саму ідею, водночас розпізнаючи інші зображення як власні окремі ідеї.

9
00:00:49,300 --> 00:00:55,820
Але якби я сказав тобі, сядьте і напишіть для мене програму, яка приймає сітку 28x28 і

10
00:00:56,340 --> 00:01:01,780
виводить єдине число від 0 до 10, повідомляючи вам, якою, на її думку, є ця

11
00:01:01,780 --> 00:01:07,860
цифра, то завдання перетворюється з комічно тривіального на страшно важко. Якщо ви не жили

12
00:01:07,860 --> 00:01:12,020
під каменем, я думаю, що мені навряд чи потрібно мотивувати актуальність і важливість машинного

13
00:01:12,020 --> 00:01:16,460
навчання та нейронних мереж для сьогодення та майбутнього. Але я хочу тут показати вам, що

14
00:01:16,460 --> 00:01:22,020
насправді таке нейронна мережа, не припускаючи жодної передісторії, і допомогти візуалізувати, що вона робить, не

15
00:01:22,060 --> 00:01:26,860
як модне слово, а як шматок математики. Я сподіваюся, що ви відчуєте, що сама структура

16
00:01:26,860 --> 00:01:31,460
вмотивована, і ви відчуєте, що знаєте, що це означає, коли ви читаєте або

17
00:01:31,460 --> 00:01:36,780
чуєте про нейронну мережу, яка вивчає цитати-розриви. Це відео буде присвячено

18
00:01:36,780 --> 00:01:40,300
лише структурному компоненту, а наступне – навчанню.

19
00:01:40,300 --> 00:01:45,580
Ми збираємося створити нейронну мережу, яка навчиться розпізнавати рукописні цифри.

20
00:01:45,580 --> 00:01:53,540
Це дещо класичний приклад представлення теми, і я радий дотримуватися статус-кво тут, тому що

21
00:01:53,540 --> 00:01:57,340
в кінці двох відео я хочу вказати вам на кілька хороших ресурсів, де ви

22
00:01:57,340 --> 00:02:01,420
можете дізнатися більше, і де ви можете завантажити код, який це робить, і

23
00:02:01,420 --> 00:02:07,820
пограти з ним на своєму комп’ютері. Існує багато різноманітних варіантів нейронних мереж, і останніми

24
00:02:07,820 --> 00:02:12,900
роками спостерігався певний бум у дослідженні цих варіантів, але в цих двох вступних

25
00:02:12,940 --> 00:02:18,100
відео ми з вами просто подивимося на найпростішу звичайну ванільну форму без

26
00:02:18,100 --> 00:02:23,020
додаткових вишукувань. Це начебто необхідна передумова для розуміння будь-якого з потужніших сучасних варіантів, і,

27
00:02:23,020 --> 00:02:28,140
повірте мені, у нас все ще є багато складності, про яку ми можемо подумати.

28
00:02:28,140 --> 00:02:33,440
Але навіть у цій найпростішій формі він може навчитися розпізнавати рукописні

29
00:02:33,440 --> 00:02:39,380
цифри, що для комп’ютера дуже круто. І в той же час ви побачите, як він

30
00:02:39,460 --> 00:02:45,620
не виправдовує кількох надій, які ми можемо покладати на нього. Як випливає з назви, нейронні мережі

31
00:02:45,620 --> 00:02:50,820
надихає мозок, але давайте розберемо це. Що таке нейрони і в якому сенсі вони пов’язані

32
00:02:50,820 --> 00:02:56,900
між собою? Зараз, коли я кажу про нейрон, все, про що я хочу, щоб ви думали, це річ,

33
00:02:56,900 --> 00:03:04,380
яка містить число, зокрема число від 0 до 1. Насправді це не більше того. Наприклад, мережа

34
00:03:04,420 --> 00:03:10,060
починається з групи нейронів, що відповідають кожному з 28 по 28 пікселів

35
00:03:10,060 --> 00:03:17,260
вхідного зображення, тобто загалом 784 нейрони. Кожне з них містить число, яке представляє значення

36
00:03:17,260 --> 00:03:23,900
градацій сірого відповідного пікселя в діапазоні від 0 для чорних пікселів до 1 для білих пікселів.

37
00:03:23,900 --> 00:03:30,060
Це число всередині нейрона називається його активацією, і ви можете мати на

38
00:03:30,060 --> 00:03:37,260
увазі, що кожен нейрон світиться, коли його активація висока. Отже, усі ці

39
00:03:37,260 --> 00:03:47,820
784 нейрони складають перший рівень нашої мережі. Переходячи до останнього шару, ми маємо

40
00:03:47,820 --> 00:03:53,780
10 нейронів, кожен з яких представляє одну з цифр. Активація в цих нейронах, знову

41
00:03:53,780 --> 00:03:59,460
якесь число від 0 до 1, показує, наскільки система вважає, що

42
00:03:59,500 --> 00:04:05,180
дане зображення відповідає даній цифрі. Існує також пара проміжних шарів, які називаються

43
00:04:05,180 --> 00:04:10,780
прихованими шарами, які на даний момент повинні бути просто величезним знаком питання про

44
00:04:10,780 --> 00:04:15,900
те, як цей процес розпізнавання цифр відбуватиметься. У цій мережі я вибрав два

45
00:04:15,900 --> 00:04:21,460
прихованих шари, кожен з яких містить 16 нейронів, і, правда, це довільний вибір. Чесно кажучи, я

46
00:04:21,460 --> 00:04:26,620
вибрав два шари, виходячи з того, як я хочу мотивувати структуру за мить, і 16, це було

47
00:04:26,620 --> 00:04:30,940
просто гарне число, щоб поміститися на екрані. На практиці тут є багато можливостей для експериментів

48
00:04:30,940 --> 00:04:37,020
із конкретною структурою. Спосіб роботи мережі, активація на одному рівні визначає

49
00:04:37,020 --> 00:04:42,340
активацію наступного рівня. І, звичайно, суть мережі як механізму обробки інформації зводиться

50
00:04:42,340 --> 00:04:47,820
до того, як саме ці активації з одного рівня призводять до активацій на

51
00:04:47,820 --> 00:04:53,340
наступному рівні. Це приблизно аналогічно тому, як у біологічних мережах нейронів

52
00:04:53,380 --> 00:04:59,380
деякі групи нейронів викликають активацію інших. Тепер мережа, яку я тут показую, уже

53
00:04:59,380 --> 00:05:04,260
навчена розпізнавати цифри, і дозвольте мені показати вам, що я маю на увазі під цим. Це означає, що

54
00:05:04,260 --> 00:05:10,900
якщо ви подаєте зображення, яке освітлює всі 784 нейрони вхідного шару відповідно до яскравості

55
00:05:10,900 --> 00:05:16,860
кожного пікселя на зображенні, цей шаблон активації спричиняє дуже специфічний шаблон у наступному шарі, який

56
00:05:16,860 --> 00:05:21,740
викликає певний шаблон у наступному шарі. це, що нарешті дає певний візерунок у вихідному шарі.

57
00:05:21,780 --> 00:05:27,540
І найяскравішим нейроном цього вихідного рівня є вибір мережі, так би мовити, яку

58
00:05:27,540 --> 00:05:35,420
цифру представляє це зображення. І перш ніж перейти до математики щодо того, як один шар

59
00:05:35,420 --> 00:05:40,460
впливає на наступний, або як працює навчання, давайте просто поговоримо про те, чому взагалі розумно

60
00:05:40,460 --> 00:05:46,340
очікувати, що така багатошарова структура буде поводитися розумно. Чого ми тут очікуємо? Яка найкраща надія на

61
00:05:46,420 --> 00:05:52,420
те, що можуть робити ці середні верстви? Що ж, коли ви чи я розпізнаємо цифри, ми збираємо

62
00:05:52,420 --> 00:05:58,980
разом різні компоненти. 9 має петлю вгорі та лінію праворуч. 8 також має петлю зверху,

63
00:05:58,980 --> 00:06:05,420
але вона поєднана з іншою петлею внизу. 4 в основному розбивається на три конкретні

64
00:06:05,420 --> 00:06:11,500
рядки тощо. Тепер, в ідеальному світі, ми можемо сподіватися, що кожен нейрон у передостанньому

65
00:06:11,740 --> 00:06:17,460
шарі відповідає одному з цих підкомпонентів, що кожного разу, коли ви подаєте зображення, скажімо, із

66
00:06:17,460 --> 00:06:23,060
петлею вгорі, як-от 9 чи 8, є деякі специфічний нейрон, чия активація буде близькою до

67
00:06:23,060 --> 00:06:28,620
1. І я не маю на увазі цю конкретну петлю пікселів, я сподіваюся, що будь-який загалом

68
00:06:28,620 --> 00:06:33,980
петлевий візерунок у напрямку до вершини виділяє цей нейрон. Таким чином, щоб перейти від

69
00:06:33,980 --> 00:06:39,380
третього рівня до останнього, потрібно просто дізнатися, яка комбінація підкомпонентів відповідає яким

70
00:06:39,380 --> 00:06:44,020
цифрам. Звісно, це тільки підштовхує проблему, тому що як ви розпізнаєте

71
00:06:44,020 --> 00:06:48,340
ці підкомпоненти чи навіть дізнаєтеся, якими мають бути правильні підкомпоненти? І я ще

72
00:06:48,340 --> 00:06:52,900
навіть не говорив про те, як один шар впливає на наступний, але поговоримо про це на мить.

73
00:06:52,900 --> 00:06:59,020
Розпізнавання циклу також може розпадатися на підпроблеми. Одним із розумних способів зробити це

74
00:06:59,020 --> 00:07:05,640
було б спочатку розпізнати різні маленькі грані, які його складають. Подібним чином, довга лінія, подібна до тієї,

75
00:07:05,640 --> 00:07:11,280
яку ви можете побачити в цифрах 1, 4 або 7, насправді це просто довга грань, або, можливо, ви думаєте

76
00:07:11,280 --> 00:07:18,440
про неї як про певний візерунок із кількох менших ребер. Тому, можливо, ми сподіваємося, що кожен

77
00:07:18,440 --> 00:07:24,680
нейрон у другому шарі мережі відповідає різним відповідним маленьким краям. Можливо, коли з’являється

78
00:07:24,680 --> 00:07:30,760
таке зображення, воно висвітлює всі нейрони, пов’язані з приблизно 8-10 певними маленькими краями,

79
00:07:31,040 --> 00:07:36,480
що, у свою чергу, висвітлює нейрони, пов’язані з верхньою петлею та довгою вертикальною лінією,

80
00:07:36,480 --> 00:07:41,960
а ті висвітлюють нейрон, пов&#39;язаний з 9. Інше питання, до якого я повернуся,

81
00:07:41,960 --> 00:07:46,560
як тільки ми побачимо, як навчити мережу, чи це те, що насправді робить наша остання

82
00:07:46,560 --> 00:07:51,800
мережа. Але це надія, яку ми могли б мати, свого роду мета з такою багатошаровою

83
00:07:51,800 --> 00:07:57,440
структурою. Крім того, ви можете собі уявити, як можливість виявлення країв і візерунків, як це,

84
00:07:57,480 --> 00:08:02,440
була б справді корисною для інших завдань розпізнавання зображень. І навіть за межами розпізнавання зображення,

85
00:08:02,440 --> 00:08:06,640
є всілякі інтелектуальні речі, які ви можете зробити, які розбиваються на шари

86
00:08:06,640 --> 00:08:12,640
абстракції. Розбір мовлення, наприклад, передбачає взяття необробленого аудіо та виділення чітких звуків, які поєднуються,

87
00:08:12,640 --> 00:08:17,760
щоб створити певні склади, які поєднуються, щоб утворити слова, які поєднуються, щоб скласти

88
00:08:17,760 --> 00:08:23,360
фрази та більш абстрактні думки тощо. Але повертаючись до того, як все це насправді працює,

89
00:08:23,400 --> 00:08:29,160
уявіть себе зараз, коли ви проектуєте, як саме активації на одному рівні можуть визначати

90
00:08:29,160 --> 00:08:35,320
активації на наступному. Мета полягає в тому, щоб мати якийсь механізм, який міг би поєднувати пікселі

91
00:08:35,320 --> 00:08:41,040
в краї, або краї в візерунки, або візерунки в цифри. І щоб збільшити масштаб на одному

92
00:08:41,040 --> 00:08:47,440
дуже конкретному прикладі, скажімо, ми сподіваємося, що один конкретний нейрон у другому шарі зрозуміє, чи

93
00:08:47,680 --> 00:08:54,440
має зображення перевагу в цій області. Виникає питання, які параметри повинна мати

94
00:08:54,440 --> 00:09:00,440
мережа? Які циферблати та ручки ви повинні мати можливість налаштувати, щоб вони були достатньо

95
00:09:00,440 --> 00:09:05,880
виразними, щоб потенційно захопити цей візерунок, або будь-який інший піксельний візерунок, або візерунок, який кілька

96
00:09:05,880 --> 00:09:11,680
країв можуть утворювати петлю, тощо? Що ж, ми призначимо вагу кожному з’єднанням

97
00:09:11,680 --> 00:09:17,160
між нашим нейроном і нейронами з першого шару. Ці ваги - просто

98
00:09:17,160 --> 00:09:23,960
цифри. Потім візьміть усі ці активації з першого рівня та обчисліть їх зважену суму відповідно

99
00:09:23,960 --> 00:09:30,400
до цих ваг. Я вважаю корисним розглядати ці ваги як організовані у невелику

100
00:09:30,400 --> 00:09:35,200
власну сітку, і я збираюся використовувати зелені пікселі для позначення позитивних ваг, а червоні

101
00:09:35,200 --> 00:09:40,760
пікселі — для позначення від’ємних ваг, де яскравість цього пікселя є деякою вільне

102
00:09:40,760 --> 00:09:45,880
зображення значення ваги. Якщо ми зробили вагові коефіцієнти, пов’язані майже з усіма пікселями, нульовими,

103
00:09:45,880 --> 00:09:51,200
за винятком деяких додатних вагових коефіцієнтів у цій області, які нас цікавлять, тоді взяття зваженої

104
00:09:51,200 --> 00:09:56,360
суми всіх значень пікселів справді означає лише додавання значень пікселя просто в регіон, про

105
00:09:56,360 --> 00:10:02,760
який ми дбаємо. І якщо ви дійсно хочете зрозуміти, чи є тут перевага,

106
00:10:02,760 --> 00:10:07,960
ви можете зробити деякі від’ємні ваги, пов’язані з оточуючими пікселями. Тоді сума

107
00:10:08,000 --> 00:10:12,680
найбільша, коли ті середні пікселі яскраві, а оточуючі пікселі темніші.

108
00:10:12,680 --> 00:10:19,200
Коли ви обчислюєте таку зважену суму, ви можете отримати будь-яке число, але для цієї мережі ми

109
00:10:19,200 --> 00:10:25,200
хочемо, щоб активації були деякими значеннями від 0 до 1. Отже, звичайна річ – це

110
00:10:25,200 --> 00:10:30,560
закачати цю зважену суму в якусь функцію, яка розміщує дійсну числову лінію в діапазоні між 0

111
00:10:30,560 --> 00:10:36,360
і 1. І звичайна функція, яка це робить, називається сигмоподібною функцією, також відомою як

112
00:10:36,360 --> 00:10:42,760
логістична крива. В основному дуже негативні вхідні дані закінчуються близькими до 0, дуже позитивні вхідні дані закінчуються близькими

113
00:10:42,760 --> 00:10:51,400
до 1, і вони просто постійно зростають навколо вхідних даних 0. Таким чином, активація нейрона тут є

114
00:10:51,400 --> 00:10:59,320
в основному показником того, наскільки позитивною є відповідна зважена сума. Але, можливо, ви не хочете,

115
00:10:59,320 --> 00:11:04,080
щоб нейрон світився, коли зважена сума більша за 0. Можливо, ви хочете, щоб він був активним лише

116
00:11:04,120 --> 00:11:11,520
тоді, коли сума перевищує, скажімо, 10. Тобто ви хочете, щоб він був неактивним. Тоді ми

117
00:11:11,520 --> 00:11:17,560
просто додамо якесь інше число, як-от мінус 10, до цієї зваженої суми перед тим,

118
00:11:17,560 --> 00:11:23,840
як підключити її до функції сигмоїдного здавлювання. Це додаткове число називається зміщенням. Отже, вагові

119
00:11:23,840 --> 00:11:29,080
коефіцієнти показують вам, який піксельний шаблон уловлює цей нейрон у другому шарі, а зсув

120
00:11:29,120 --> 00:11:34,640
говорить вам, наскільки високою має бути зважена сума, перш ніж нейрон почне ставати значно активнішим.

121
00:11:34,640 --> 00:11:41,760
І це лише один нейрон. Кожен інший нейрон цього шару буде з’єднаний з усіма

122
00:11:41,760 --> 00:11:49,080
784 піксельними нейронами з першого шару, і кожне з цих 784 з’єднань

123
00:11:49,080 --> 00:11:55,320
має власну вагу. Крім того, у кожного з них є деяке зміщення, якесь інше число, яке ви додаєте

124
00:11:55,320 --> 00:12:00,600
до зваженої суми перед тим, як стиснути її сигмою. І це над чим подумати! З цим прихованим шаром

125
00:12:00,600 --> 00:12:09,280
із 16 нейронів це загалом 784 помножити на 16 ваг разом із 16 упередженнями. І все це

126
00:12:09,280 --> 00:12:13,760
лише зв’язки від першого шару до другого. Зв’язки між іншими шарами також

127
00:12:13,760 --> 00:12:19,600
мають купу ваг і упереджень, пов’язаних з ними. Усе сказано та зроблено, ця

128
00:12:19,600 --> 00:12:26,680
мережа має майже рівно 13 000 загальних ваг і упереджень. 13 000 ручок і циферблатів, які

129
00:12:26,680 --> 00:12:32,400
можна налаштовувати та повертати, щоб ця мережа працювала по-різному. Отже, коли ми говоримо про

130
00:12:32,400 --> 00:12:38,440
навчання, це стосується того, щоб змусити комп’ютер знайти дійсне налаштування для всіх цих

131
00:12:38,440 --> 00:12:44,400
багатьох чисел, щоб він фактично вирішив проблему. Один уявний експеримент, який водночас веселий

132
00:12:44,400 --> 00:12:49,440
і жахливий, полягає в тому, щоб уявити, як ви сідаєте й вручну встановлюєте

133
00:12:49,440 --> 00:12:53,960
всі ці ваги та зміщення, цілеспрямовано змінюючи числа так, щоб другий шар підбирав краї,

134
00:12:53,960 --> 00:12:59,680
третій шар підбирав шаблони, тощо Особисто я вважаю це задовільним, а не ставлюся

135
00:12:59,680 --> 00:13:04,400
до мережі як до чорної скриньки, тому що коли мережа не працює так,

136
00:13:04,400 --> 00:13:09,040
як ви очікуєте, якщо ви трохи зрозумієте, що насправді означають ці ваги та

137
00:13:09,040 --> 00:13:13,440
упередження , у вас є відправна точка для експериментів зі зміною структури для

138
00:13:13,440 --> 00:13:17,680
покращення. Або коли мережа працює, але не з тих причин, які ви могли б

139
00:13:17,680 --> 00:13:22,760
очікувати, копання в тому, що роблять ваги та упередження, є хорошим способом кинути виклик вашим

140
00:13:22,760 --> 00:13:28,560
припущенням і дійсно відкрити повний простір можливих рішень. До речі, фактичну функцію тут

141
00:13:28,560 --> 00:13:34,840
трохи громіздко записати, вам не здається? Тож дозвольте мені показати вам

142
00:13:34,840 --> 00:13:39,200
більш компактний спосіб представлення цих зв’язків. Ось як ви побачите це, якщо вирішите прочитати

143
00:13:39,200 --> 00:13:45,360
більше про нейронні мережі. Організуйте всі активації з одного шару в стовпець як

144
00:13:45,480 --> 00:13:53,400
вектор. Потім організуйте всі ваги як матрицю, де кожен рядок цієї матриці відповідає

145
00:13:53,400 --> 00:13:58,680
зв’язкам між одним шаром і певним нейроном на наступному шарі. Це означає,

146
00:13:58,680 --> 00:14:03,360
що взяття зваженої суми активацій у першому шарі відповідно до цих ваг відповідає

147
00:14:03,360 --> 00:14:08,880
одному з доданків у векторному добутку матриці всього, що ми маємо тут

148
00:14:08,880 --> 00:14:17,840
зліва. До речі, значна частина машинного навчання зводиться лише до гарного розуміння лінійної алгебри,

149
00:14:17,840 --> 00:14:23,000
тому будь-хто з вас, хто хоче добре візуально розуміти матриці та значення векторного множення

150
00:14:23,000 --> 00:14:29,320
матриць, погляньте на серію, яку я робив на лінійна алгебра, особливо розділ 3. Повертаючись

151
00:14:29,320 --> 00:14:34,200
до нашого виразу, замість того, щоб говорити про додавання зміщення до кожного з цих значень

152
00:14:34,200 --> 00:14:40,440
незалежно, ми представляємо це, організовуючи всі ці зміщення у вектор і додаючи весь вектор до

153
00:14:40,440 --> 00:14:47,240
попереднього векторного добутку матриці. Потім, як останній крок, я оберну сигмовид навколо зовнішнього

154
00:14:47,240 --> 00:14:51,480
боку, і це має означати те, що ви збираєтеся застосувати функцію сигмоіда

155
00:14:51,480 --> 00:14:58,120
до кожного конкретного компонента результуючого вектора всередині. Отже, як тільки ви запишете цю

156
00:14:58,120 --> 00:15:03,320
вагову матрицю та ці вектори як їхні власні символи, ви зможете повідомити про повний

157
00:15:03,480 --> 00:15:08,840
перехід активацій від одного шару до наступного в надзвичайно вузькому та акуратному маленькому виразі,

158
00:15:08,840 --> 00:15:14,600
і це зробить відповідний код набагато простішим і набагато швидше, оскільки багато бібліотек оптимізують

159
00:15:14,600 --> 00:15:21,400
матричне множення. Пам’ятаєте, як раніше я казав, що ці нейрони — це просто речі, які містять числа?

160
00:15:22,120 --> 00:15:26,280
Ну, звичайно, конкретні числа, які вони зберігають, залежать від зображення, яке ви

161
00:15:28,120 --> 00:15:31,960
подаєте, тому насправді точніше розглядати кожен нейрон як функцію, яка приймає

162
00:15:31,960 --> 00:15:37,240
вихідні дані всіх нейронів попереднього шару та викидає число від 0 до

163
00:15:37,240 --> 00:15:43,800
1. Насправді вся мережа — це лише функція, яка приймає 784 числа як

164
00:15:43,800 --> 00:15:49,720
вхідні дані та видає 10 чисел як вихідні. Це абсурдно складна функція, яка включає

165
00:15:49,720 --> 00:15:54,520
13 000 параметрів у формі цих ваг і зміщень, які вловлюють певні

166
00:15:54,520 --> 00:15:59,000
шаблони, і яка включає ітерацію багатьох векторних добутків матриці та функцію сигмоїдного

167
00:15:59,000 --> 00:16:04,760
здавлювання, але це, тим не менш, лише функція. І те, що це

168
00:16:04,760 --> 00:16:09,720
виглядає складним, певною мірою заспокоює. Я маю на увазі, якби це було простіше, яка б у

169
00:16:09,720 --> 00:16:14,920
нас була надія, що воно зможе впоратися з проблемою розпізнавання цифр? І як він приймає цей виклик?

170
00:16:14,920 --> 00:16:19,320
Як ця мережа дізнається відповідні ваги та упередження, просто переглядаючи дані?

171
00:16:19,880 --> 00:16:23,960
Що ж, це те, що я покажу в наступному відео, а також трохи детальніше розберусь про те,

172
00:16:23,960 --> 00:16:29,880
що насправді робить ця конкретна мережа. Тепер я вважаю, що я повинен сказати, що підписатись, щоб отримувати

173
00:16:29,880 --> 00:16:34,840
сповіщення про те, коли з’явиться це відео чи будь-які нові відео, але насправді більшість із вас насправді

174
00:16:34,840 --> 00:16:39,880
не отримує сповіщень від YouTube, чи не так? Можливо, відверто кажучи, я повинен сказати, що

175
00:16:39,880 --> 00:16:44,920
підписуйтесь, щоб нейронні мережі, які лежать в основі алгоритму рекомендацій YouTube, переконалися, що

176
00:16:44,920 --> 00:16:49,800
ви хочете, щоб вам рекомендували вміст із цього каналу. У будь-якому разі залишайтеся в курсі, щоб дізнатися більше.

177
00:16:50,600 --> 00:16:54,840
Велике спасибі всім, хто підтримує ці відео на Patreon. Я трохи повільно просувався в

178
00:16:54,840 --> 00:16:59,160
серії ймовірностей цього літа, але я повертаюся до цього після цього проекту, тож патрони,

179
00:16:59,160 --> 00:17:05,640
ви можете стежити за оновленнями там. Щоб закінчити, зі мною є Ліша Лі,

180
00:17:05,640 --> 00:17:09,880
яка захистила докторську роботу з теоретичної сторони глибокого навчання та зараз працює у

181
00:17:09,880 --> 00:17:14,520
фірмі венчурного капіталу Amplify Partners, яка люб’язно надала частину фінансування для цього відео.

182
00:17:15,160 --> 00:17:19,480
Отже, Ліша, я вважаю, що ми повинні швидко згадати одну річ, це цю сигмоподібну функцію.

183
00:17:19,480 --> 00:17:23,400
Наскільки я розумію, ранні мережі використовували це, щоб стиснути відповідну зважену суму в інтервал

184
00:17:23,400 --> 00:17:28,200
між нулем і одиницею, ви знаєте, начебто вмотивовані цією біологічною аналогією нейронів, які

185
00:17:28,200 --> 00:17:33,240
або неактивні, або активні. точно. Але порівняно небагато сучасних мереж фактично використовують

186
00:17:33,240 --> 00:17:37,800
sigmoid. так Це стара школа, чи не так? Так, точніше relu, здається, набагато

187
00:17:37,800 --> 00:17:43,880
легше тренувати. А relu, relu означає випрямлену лінійну одиницю? Так, це така функція, де

188
00:17:43,880 --> 00:17:50,280
ви просто берете максимальне значення нуль і a, де a задано тим, що ви пояснювали

189
00:17:50,280 --> 00:17:56,440
у відео. Я вважаю, що це було частково мотивовано біологічною аналогією з

190
00:17:56,440 --> 00:18:03,640
тим, як нейрони активуються чи ні. Отже, якщо він перевищить певний поріг, це

191
00:18:03,640 --> 00:18:09,080
буде функція ідентифікації, але якщо ні, то вона просто не буде активована, тому буде нульовою.

192
00:18:09,080 --> 00:18:13,640
Так що це якесь спрощення. Використання сигмовидів не допомогло в навчанні або в якийсь

193
00:18:13,640 --> 00:18:21,320
момент було дуже важко тренуватися, і люди просто спробували relu, і сталося, що це дуже

194
00:18:21,320 --> 00:18:26,120
добре спрацювало для цих неймовірно глибоких нейронних мереж. Гаразд, дякую Ліша.

195
00:18:39,080 --> 00:18:40,060
you

