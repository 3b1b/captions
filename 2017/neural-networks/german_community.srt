1
00:00:04,020 --> 00:00:10,680
Dies ist eine Drei. Sie ist schlampig geschrieben und ist bei einer extrem niedrigen Auflösung von 28 um 28 Pixel gerendert.

2
00:00:10,680 --> 00:00:13,620
Aber dein Gehirn hat keine Mühe, es als eine Drei zu Erkennen.

3
00:00:13,620 --> 00:00:18,860
Nimm doch einen Moment Zeit zum Wertschätzen, wie verrückt es ist, dass Gehirne dies so mühelos tun können?

4
00:00:18,940 --> 00:00:23,160
Ich meine, diese sind auch erkennbar als Dreien,

5
00:00:23,160 --> 00:00:28,060
obwohl die spezifischen Werte von jedem Pixel von einem Bild zum Nächsten sehr verschieden sind.

6
00:00:28,080 --> 00:00:33,780
Die speziellen lichtempfindlichen Zellen im Auge, die aktiviert werden, wenn Sie die diese Drei sehen,

7
00:00:33,780 --> 00:00:36,800
sind sehr verschieden sind von denen, die aktiviert werden, wenn Sie die andere Drei sehen.

8
00:00:37,140 --> 00:00:40,880
Aber etwas in Ihrem verrückten intelligenten visuellen Kortex

9
00:00:40,920 --> 00:00:48,600
erkennt diese als dieselbe Idee, während zugleich andere Bilder als ihre ganz eigenen Ideen erkannt werden.

10
00:00:48,840 --> 00:00:55,340
Aber wenn ich Ihnen sagte, schreiben Sie für mich ein Programm, das ein 28x28 Raster nimmt

11
00:00:55,380 --> 00:01:01,760
die Zahl darin erkennt und diese Zahl zwischen 0 und 10 ausgibt...

12
00:01:02,250 --> 00:01:06,139
... dann wird aus einer kinderleichten Aufgabe eine höchst kompliziertes Unterfangen.

13
00:01:06,750 --> 00:01:08,270
Es sei denn, man lebe hinter dem Mond,

14
00:01:08,270 --> 00:01:14,599
muss ich kaum die Relevanz und Bedeutung des maschinellen Lernens und neuronaler Netze für die Gegenwart die Zukunft betonen.

15
00:01:14,640 --> 00:01:18,640
Aber was will ich hier zu tun will, ist Ihnen zu zeigen, was ein neuronales Netz tatsächlich ist.

16
00:01:18,660 --> 00:01:24,220
Ich gehe von keine Vorwissen aus und das Thema wird nicht als Schlagwort visualisiert, sondern als ein Stück Mathematik

17
00:01:24,560 --> 00:01:28,620
Meine Hoffnung ist nur, dass Sie das Gefühl verlieren, dass diese Struktur selbstmotivert ist

18
00:01:28,620 --> 00:01:34,400
und anfangen zu wissen, was es bedeutet, wenn Sie lesen oder hören, dass ein neuronales Netz "lernt".

19
00:01:34,940 --> 00:01:40,240
Dieses Video widmet sich nur den Strukturkomponenten und das folgende Video wird das "Lernen" bewältigen.

20
00:01:40,460 --> 00:01:45,940
Wir setzen ein neuronales Netz zusammen, das lernen soll handgeschriebene Ziffern zu erkennen.

21
00:01:49,260 --> 00:01:52,580
Dies ist ein klassisches Beispiel als Einführung für das Thema

22
00:01:52,580 --> 00:01:56,760
und ich bin zufrieden mit dem Status quo hier, denn am Ende der beiden Videos will ich Sie auf

23
00:01:56,760 --> 00:02:04,480
ein paar gute Ressourcen hinweisen, wo Sie mehr lernen und den Code herunterladen können und mit ihm auf Ihrem PC spielen können.

24
00:02:04,740 --> 00:02:07,880
Es gibt viele, viele Varianten von neuronalen Netzen.

25
00:02:07,880 --> 00:02:11,960
Und in den letzten Jahren gab es bei der Erforschung dieser Varianten einen Boom.

26
00:02:12,130 --> 00:02:19,019
Aber in diesen beiden einleitenden Videos, werden wir die einfachste Form ohne Extrazusätze betrachten.

27
00:02:19,300 --> 00:02:21,440
Dies ist eine Art notwendige Voraussetzung,

28
00:02:21,440 --> 00:02:24,520
um eine der leistungsfähigeren modernen Varianten zu verstehen und

29
00:02:24,760 --> 00:02:28,199
Vertrauen Sie mir. Es noch viel Komplexität hat für uns, unseren Geist umschlingen

30
00:02:28,690 --> 00:02:32,820
Aber auch in dieser einfachsten Form kann es lernen, handgeschriebene Ziffern zu erkennen.

31
00:02:32,820 --> 00:02:36,180
Was eine ziemlich coole Fähigkeit ist für einen Computer.

32
00:02:37,120 --> 00:02:41,960
Trotzdem werden Sie sehen, wie einige Hoffnungen nicht erfüllt werden können.

33
00:02:43,090 --> 00:02:48,179
Wie der Name suggeriert, ziehen neurale Netzwerke ihre Inspiration vom tatsächlichen Gehirn. Lassen Sie uns das etwas herunterbrechen.

34
00:02:48,520 --> 00:02:51,389
Was sind Neuronen und wie sind sie miteinander verbunden?

35
00:02:52,090 --> 00:02:57,750
Vorerst, wenn ich von Neuronen spreche, möchte ich, dass Sie an ein Konstrukt denken, das eine Nummer beinhalten kann.

36
00:02:58,209 --> 00:03:02,129
Insbesondere eine Zahl zwischen 0 und 1. Es ist wirklich nicht mehr als das.

37
00:03:03,430 --> 00:03:11,130
Zum Beispiel: Das Netzwerk beginnt mit einem Bündel von Neuronen zu jedem der 28 mal 28 Bildpunkten des Eingabebildes,

38
00:03:11,400 --> 00:03:12,460
das macht:

39
00:03:12,460 --> 00:03:20,240
784 Neuronen insgesamt. Jedes enthält eine Zahl, die den Grauwert des entsprechenden Pixels darstellt,

40
00:03:20,769 --> 00:03:24,299
und dabei im Bereich von 0 für schwarze Pixel, bis 1 für weisse Pixel liegt.

41
00:03:24,900 --> 00:03:29,440
Diese Zahl innerhalb des Neurons ist seine sogenannte Aktivierung.

42
00:03:29,540 --> 00:03:33,960
Das Bild, das Sie hierfür vor Augen haben könnten ist, dass das Neuron leuchtet, wenn es eine hohe Zahl hat.

43
00:03:36,260 --> 00:03:41,560
Somit bilden diese ersten 784 Neuronen die erste Stufe des Netzwerks.

44
00:03:45,990 --> 00:03:51,289
Wenn wir nun zur letzten Stufe springen, verbleiben 10 Neuronen für die 10 Ziffern 0-9.

45
00:03:51,560 --> 00:03:56,220
Die Aktivierung dieser Neuronen liegt wieder zwischen 0 und 1.

46
00:03:56,700 --> 00:04:02,540
Diese Zahl zeigt an, wie sehr das System glaubt, dass es sich bei dem Bild um die entsprechende Zahl handelt.

47
00:04:02,540 --> 00:04:06,620
Zwischen diesen beiden Stufen gibt es ein paar weitere - die verborgenen Stufen.

48
00:04:06,620 --> 00:04:09,540
Welche im Moment nur ein grosses Fragezeichen sind.

49
00:04:09,800 --> 00:04:13,540
Wie in aller Welt soll dieser Erkennungsprozess vom Bild zur Zahl gehandhabt werden?

50
00:04:13,740 --> 00:04:20,600
In diesem Netzwerk wählte ich zwei verborgene Stufen, jeweils mit 16 Neuronen, wobei dies eine willkürliche Zahl ist.

51
00:04:20,940 --> 00:04:24,880
Ehrlich gesagt, habe ich zwei Stufen gewählt, um besser auf die Funktionsweise eingehen zu können.

52
00:04:25,340 --> 00:04:29,160
Und 16, das war einfach eine nette Zahl die auch auf den Bildschirm passt.

53
00:04:29,180 --> 00:04:32,209
Die gezeigte Struktur erlaubt einiges an Raum zum experimentieren.

54
00:04:32,730 --> 00:04:38,329
Und so funktioniert's: Die Aktivierung auf einer Stufe bereitet die Aktivierung der nächsten Stufe vor.

55
00:04:38,760 --> 00:04:45,349
Und natürlich ist das Herz des Netzes als Mechanismus der Informationsverarbeitung, ist die Bestimmung

56
00:04:45,570 --> 00:04:48,409
wie genau die Aktivierung einer Stufe die Aktivierung der nächsten herbeiführt.

57
00:04:48,900 --> 00:04:54,859
Die Idee ist, dass es ungefähr analog zu biologischen neuronalen Netzwerken ist - wo somit die Aktivierung gewisser Neuronen

58
00:04:55,400 --> 00:04:57,340
zur Aktivierung anderer Neuronen führt.

59
00:04:57,560 --> 00:04:59,940
Das Netzwerk hier wurde bereits trainiert

60
00:04:59,980 --> 00:05:03,060
um  Ziffern zu erkennen. Lassen Sie mich erläutern, was ich damit meine:

61
00:05:03,140 --> 00:05:05,100
Das bedeutet, dass wenn ich ein Bild einfüttere,

62
00:05:05,300 --> 00:05:11,780
jede der 784 Neuronen der Eingangsstufe einen entsprechenden Wert für die Helligkeit des Bildpunktes erhält.

63
00:05:12,330 --> 00:05:17,029
Dieses Muster der Aktivierungen verursacht einige sehr spezifische Muster in der nächsten Schicht.

64
00:05:17,190 --> 00:05:19,309
Was ein bestimmtes Muster in der nächsten Stufe herbeiführt,

65
00:05:19,440 --> 00:05:22,200
welches schlussendlich ein bestimmtes Muster für die letzte Stufe erstellt.

66
00:05:22,340 --> 00:05:30,020
Das hellste Neuron dieser Ausgabestufe ist gewissermassen die beste Schätzung des Netzwerkes dessen, was das Bild beinhaltet.

67
00:05:32,060 --> 00:05:36,840
Und bevor wir in die Mathematik springen und uns anschauen,  wie eine Schicht die andere beeinflusst, oder wie das Training funktioniert,

68
00:05:37,140 --> 00:05:43,069
sollten wir uns überlegen, warum es überhaupt sinnvoll ist, von dieser Schichtenstruktur intelligentes Verhalten zu erwarten.

69
00:05:43,800 --> 00:05:48,260
Was erwarten wir denn hier? Was ist die beste Hoffnung dessen, was diese Mittelschichten tun könnten?

70
00:05:48,860 --> 00:05:56,720
Nun, wenn Sie und ich Zahlen erkennen, so fügen wir einzelne Komponenten zusammen. Ein Kreis oben und eine Linie unten im Fall der Neun, beispielsweise.

71
00:05:57,260 --> 00:06:01,280
Eine 8 hat ebenfalls einen Kreis oben, wird aber mit einem zweiten Kreis darunter kombiniert.

72
00:06:02,020 --> 00:06:06,599
Eine 4 kann im Grunde in drei Linien heruntergebrochen werden - und so weiter.

73
00:06:07,180 --> 00:06:11,970
In einer perfekten Welt könnten wir hoffen, dass jedes Neuron in der zweitletzten Schicht

74
00:06:12,640 --> 00:06:15,120
einem dieser Unterkomponenten entspricht.

75
00:06:15,640 --> 00:06:19,720
Dass also für jedes Bild mit - sagen wir einem Kreis oben, wie bei der 9 oder 8,

76
00:06:19,870 --> 00:06:21,220
ein spezifisches Neuron besteht,

77
00:06:21,220 --> 00:06:27,749
dessen Aktivierung in der Nähe von 1.0 liegt. Und ich meine nicht diesen spezifischen Kreis,

78
00:06:28,090 --> 00:06:35,039
sondern alle Kreis-artigen Muster im oberen Bereich dieses Neuron aktiviert. Somit ist der letzte Schritt

79
00:06:35,380 --> 00:06:39,960
nur noch eine Sache des Lernens, welche Kombinationen von Teilkomponenten welchen Ziffern entspricht.

80
00:06:40,510 --> 00:06:42,810
Natürlich verschiebt dies das Problem nur nach hinten:

81
00:06:42,910 --> 00:06:49,019
Denn wie würden Sie diese Subkomponenten erkennen oder nur schon lernen, was die richtigen Subkomponenten sind, und ich habe noch nicht einmal darüber gesprochen,

82
00:06:49,020 --> 00:06:52,829
wie eine Schicht die nächste beeinflusst. Aber bleiben Sie bei mir für den Moment.

83
00:06:53,650 --> 00:06:56,340
Denn das Erkennen eines Kreises kann auch in Teilschritte aufgetrennt werden.

84
00:06:56,860 --> 00:07:02,550
Eine vernünftige Art dies zu tun wäre, zunächst verschiedene kleine Kanten zu erkennen.

85
00:07:03,520 --> 00:07:08,910
Diese ähneln den längeren Strichen wie sie bspw. in den Zahlen 1, 4 oder 7 vorkommen.

86
00:07:08,910 --> 00:07:14,279
Nun, das sind wirklich nur länge Kanten. Stellen Sie sich also vor, dass bestimmtes Muster aus mehreren kleineren Kanten bestehen.

87
00:07:14,740 --> 00:07:19,379
Somit wäre unsere Hoffnung, dass jedes Neuron in der zweiten Schicht des Netzes

88
00:07:20,290 --> 00:07:22,650
mit verschiedenen relevanten Kanten der ersten Schicht korrespondiert.

89
00:07:23,230 --> 00:07:28,259
Wenn also ein Bild w ie dieses hier verwendet wird, werden alle Neuronen aktiviert,

90
00:07:28,720 --> 00:07:31,649
welche mit etwa acht bis zehn spezifischen Kanten verbunden sind.

91
00:07:31,920 --> 00:07:36,920
welche wiederum die zugehörigen Neuronen der oberen Schleife und einer langen vertikalen Linie aktiviert

92
00:07:37,280 --> 00:07:39,880
und diese aktivieren schlussendlich die Neuronen, welche mit der 9 assoziiert werden.

93
00:07:40,280 --> 00:07:40,760
 

94
00:07:40,760 --> 00:07:47,040
Ob unser endgültiges Netzwerk dies tatsächlich so macht, ist eine andere Frage, auf welche ich zurückkommen werde, wenn wir das Netzwerk trainieren.

95
00:07:47,360 --> 00:07:52,200
Aber das ist eine Hoffnung, welche wir haben könnten. Eine Art Zielvorstellung einer Schichtenstruktur.

96
00:07:53,000 --> 00:07:59,320
Außerdem können Sie sich vorstellen, wie das Erkennen dieser Kanten und Muster auch für andere Bilderkennungsaufgaben nützlich sein könnte.

97
00:08:00,160 --> 00:08:07,160
Und auch über Bilderkennung hinaus, gibt es alle Arten von intelligenten Dingen, die Sie in Abstraktionsebenen herunterbrechen könnten.

98
00:08:07,680 --> 00:08:15,040
Sprachanalyse zum Beispiel beinhaltet die Verwendung roher Audiodaten und aus welchen markante Klänge und daraus wiederum Silben gewonnen werden.

99
00:08:15,440 --> 00:08:20,360
Diese werden zu Wörtern kombiniert, welche Sätze bilden und abstraktere Gedanken ausdrücken, usw.

100
00:08:20,760 --> 00:08:25,710
Zurück dazu, wie das alles tatsächlich funktioniert. Stellen Sie sich vor den Prozess zu gestalten.

101
00:08:25,710 --> 00:08:30,449
Wie genau könnte die Aktivierungen in einer Schicht die Aktivierungen in den nächsten bestimmen?

102
00:08:30,670 --> 00:08:35,879
Das Ziel ist es, einen Mechanismus zu haben, welcher Pixel zu Kanten kombinieren kann.

103
00:08:35,880 --> 00:08:41,430
Oder Kanten zu Mustern oder Muster zu Ziffern. Schauen wir uns das mal an einem ganz bestimmten Beispiel an:

104
00:08:41,950 --> 00:08:44,189
Versuchen wir mal für ein ganz bestimmtes

105
00:08:44,380 --> 00:08:50,430
Neuron der zweiten Schicht zu prüfen, ob oder nicht das Bild eine Kante in dieser Region hat.

106
00:08:50,950 --> 00:08:54,960
Die zentrale Frage ist, welche Parameter sollte das Netzwerk haben und welche Grössen

107
00:08:55,280 --> 00:09:02,520
verändert und angepasst werden sollten, sodass sie ausdrucksstark genug sind, um möglicherweise dieses Muster zu erfassen.

108
00:09:03,080 --> 00:09:07,800
Oder jedes andere Pixelmuster, oder das Muster, dank welchem einige Kanten eine Schleife bilden können etc.

109
00:09:08,280 --> 00:09:15,360
Was wir nun tun, ist jeder Verbindung zwischen unserem Neuron und jedem Neuron der ersten Stufe ein Gewicht zu geben.

110
00:09:15,850 --> 00:09:17,850
Diese Gewichte sind beliebige Zahlen.

111
00:09:18,190 --> 00:09:25,590
Dann nehmen wir alle Aktivierungswerte der ersten Stufe und berechnen die gewichtete Summe ihres Einflusses.

112
00:09:27,360 --> 00:09:32,000
Ich finde es nützlich, mir die einzelnen Gewichte als ein eigenes Muster vorzustellen

113
00:09:32,240 --> 00:09:37,080
Und ich verwende grüne Pixel für positive Gewichte und rote Pixel für negative Gewichte,

114
00:09:37,240 --> 00:09:41,680
wobei die Helligkeit jedes Pixels eine lose Darstellung des Wertes des Gewichts ist.

115
00:09:42,400 --> 00:09:45,840
Wenn wir nun die Gewichte fast aller Pixel auf Null setzen,

116
00:09:46,150 --> 00:09:49,079
mit Ausnahme einiger positiver Gewichte in der Region, welche uns interessiert,

117
00:09:49,480 --> 00:09:51,310
und die gewichtete Summe nehmen,

118
00:09:51,310 --> 00:09:57,690
dann entspricht diese nur den Pixelwerten in der Region, die uns interessiert.

119
00:09:58,870 --> 00:10:04,440
Wenn Sie nun wirklich bestätigen wollen, ob es dort eine Kante hat, so können Sie einige negative Gewichte

120
00:10:04,900 --> 00:10:06,900
mit den umliegenden Pixeln assoziieren.

121
00:10:07,030 --> 00:10:12,660
So ist die Summe dann am größten, wenn die mittleren Pixel hell sind und die umgebenden Pixel dunkler.

122
00:10:14,279 --> 00:10:18,169
Wenn Sie eine gewichtete Summe berechnen, kann eine beliebige Zahl dabei raus kommen.

123
00:10:18,240 --> 00:10:23,180
Aber für dieses Netzwerk wollen wir Aktivierungen erhalten, welche einen Wert zwischen 0 und 1 einnehmen.

124
00:10:23,730 --> 00:10:26,599
Eine übliche Art, mit diesem Problem umzugehen ist, diese gewichtete Summe zu skalieren.

125
00:10:26,910 --> 00:10:32,000
Dies passiert in einer Funktion, welche die Auswahl reeller Zahlen in den Bereich zwischen 0 und 1 quetscht.

126
00:10:32,190 --> 00:10:37,249
Eine bekannte Funktion, welche dies tut, ist die Sigmoid-Funktion (auch logistische Kurve genannt).

127
00:10:37,980 --> 00:10:43,339
Kurz gesagt: Sehr negative Werte fallen in die Nähe von Null, während sehr positive Werte sich 1 nähern

128
00:10:43,339 --> 00:10:46,398
und die Werte steigen stetig an im Bereich um Null.

129
00:10:49,080 --> 00:10:56,029
Aktivierung dieses Neurons ist also im Grunde ein Maß dafür, wie positiv die gewichtete Summe ist.

130
00:10:57,450 --> 00:11:01,819
Aber vielleicht möchten Sie gar nicht, dass das Neuron aufleuchtet, sobald es einen höheren Wert als 0 hat.

131
00:11:02,100 --> 00:11:06,260
Vielleicht möchten Sie es nur dann aktiv haben, wenn die gewichtete Summe grösser als 10 ist.

132
00:11:06,630 --> 00:11:10,279
Das heißt, Sie wollen eine gewisse Tendenz dazu, inaktiv zu sein.

133
00:11:10,860 --> 00:11:16,099
Was wir dann tun ist, eine andere Zahl hinzufügen - wie bspw. (-10),

134
00:11:16,529 --> 00:11:19,669
bevor sie durch die S-förmige Skalierungsfunktion wandert.

135
00:11:20,220 --> 00:11:22,730
Diese zusätzliche Zahl wird Verzerrung (oder Bias) genannt.

136
00:11:23,320 --> 00:11:28,480
Somit sagen die Gewichte, welche Pixelmuster dieses Neuron in der zweiten Schicht aufgreifen soll

137
00:11:28,560 --> 00:11:35,440
und die Verzerrung gibt an, wie hoch die gewichtete Summe sein muss, damit eine Aktivierung eingeleitet wird.

138
00:11:35,920 --> 00:11:37,920
Und das ist erst ein Neuron.

139
00:11:38,120 --> 00:11:41,940
Jedes andere Neuron in dieser Schicht ist ebenfalls mit allen

140
00:11:42,320 --> 00:11:50,620
784 Pixelneuronen aus der ersten Schicht verbunden und jeder dieser Anschlüsse hat ein eigenes Gewicht, welches damit verbundenen ist.

141
00:11:51,330 --> 00:11:57,739
Ebenfalls hat jedes eine Verzerrung, eine Zahl, welche hinzugefügt wird, bevor die Sigmoid-Funktion angewendet wird (Skalierung auf 0-1).

142
00:11:58,020 --> 00:12:01,909
Das ergibt viele Zwischenresultate. Mit 16 Schichten dieser versteckten Schichten

143
00:12:02,010 --> 00:12:08,270
ergibt das 784 Pixel * 16 Gewichte mit 16 Verzerrungskonstanten.

144
00:12:08,490 --> 00:12:14,029
Und all dies ergibt erst die Verbindung der ersten und zweiten Schicht. Die Verbinungen zu den weiteren

145
00:12:14,029 --> 00:12:17,208
Schichten haben ebenfalls einen Haufen Gewichte und Verzerrungen angehängt.

146
00:12:17,760 --> 00:12:20,680
Schlussendlich hat dieses Netzwerk fast aufs Loch genau

147
00:12:21,280 --> 00:12:23,920
13,000 Gewichte und Verzerrungen.

148
00:12:24,280 --> 00:12:29,540
13.000 Knöpfe und Regler, die gedrückt und gedreht werden können, umd das Verhalten dieses Netzwerkes zu beeinflussen.

149
00:12:30,520 --> 00:12:32,520
Wenn wir also von 'lernen' sprechen,

150
00:12:32,520 --> 00:12:39,120
beziehen wir uns darauf, dass der Computer herausfinden soll, welche Ausprägung all dieser Stellschrauben

151
00:12:39,300 --> 00:12:41,700
das Problem schlussendlich lösen kann.

152
00:12:41,960 --> 00:12:43,420
Stellen Sie sich vor:

153
00:12:43,600 --> 00:12:49,980
Gleichzeitig lustig und schrecklich: Sie müssten alle diese Gewichte und Verzerrungen von Hand eingeben!

154
00:12:50,380 --> 00:12:56,160
Gezielt die Zahlen optimieren, sodass die zweite Schicht Kanten erkennt, die dritte dann Muster usw.

155
00:12:56,340 --> 00:13:01,440
Ich persönlich finde diese Idee befriedigend - um das Netzwerk nicht als komplette Blackbox behandeln zu müssen.

156
00:13:01,860 --> 00:13:05,220
Denn wenn das Netzwerk nicht wunschgemäss funktioniert,

157
00:13:05,760 --> 00:13:10,100
und Sie ein minimales Verständnis davon haben, was diese Gewichte und Verzerrungen überhaupt bedeuten,

158
00:13:10,360 --> 00:13:14,400
haben Sie Handlungsmöglichkeiten und können erahnen, wie das Netzwerk verbessert werden könnte.

159
00:13:14,720 --> 00:13:18,280
Oder: Wenn das Netzwerk funktioniert, aber vielleicht nicht aus den erwarteten Gründen,

160
00:13:18,300 --> 00:13:25,840
könnten Sie die Gewichte und Verzerrungen überprüfen um Ihre Annahmen zu überdenken und ggf. den Lösungsraum erweitern.

161
00:13:25,840 --> 00:13:26,340
 

162
00:13:26,580 --> 00:13:31,500
Zudem: Die tatsächliche Funktion zu notieren erscheint etwas mühsam, finden Sie nicht?

163
00:13:32,340 --> 00:13:38,280
Lassen Sie mich also  eine kompaktere Notierung verwenden. So würden Sie es denn auch sehen,

164
00:13:38,460 --> 00:13:40,900
wenn Sie sich weiter zum Thema neuronale Netze informieren möchten.

165
00:13:41,100 --> 00:13:45,800
Organisieren Sie alle Aktivierungswerte einer Schicht in die Spalte eines Vektors.

166
00:13:47,470 --> 00:13:52,320
Dann stellen Sie alle Gewichte in der Form einer Matrix dar, in welcher jede Zeile

167
00:13:52,900 --> 00:13:57,659
der Verbindung zwischen einer Schicht und einem bestimmten Neuron der nächsten Schicht entspricht

168
00:13:58,060 --> 00:14:03,599
Das bedeutet, daß die gewichtete Summe der Aktivierungen in der ersten Schich gemäss dieser Gewichte

169
00:14:04,000 --> 00:14:09,330
einem Ausdruck des Matrizen-Vektor-Produkts von allem auf der linken Seite entspricht.

170
00:14:13,540 --> 00:14:18,380
Nebenbei: Der Grossteil dessen, was bei Machine Learning passiert, kann durch ein gutes Verständnis von linearer Algebra erklärt werden.

171
00:14:18,380 --> 00:14:26,940
Wer also eine hübsche Visualisierung dessen wünscht was Matrizen sind, oder wie Vektor-Multiplikation funktioniert, sollte sich meine Serie zur linearen Algebra anschauen.

172
00:14:27,250 --> 00:14:28,839
Vor allem das 3. Kapitel!

173
00:14:28,840 --> 00:14:34,680
Zurück zu unserer Veranschaulichung: Statt die Verzerrung jeder einzelnen Zeile anzufügen, können wir

174
00:14:34,960 --> 00:14:42,200
sie ebenfalls in Form eines Vektors organisieren, welchen wir dem vorangegangenen Matrizen-Vektor-Produkt hinzufügen können.

175
00:14:42,910 --> 00:14:44,040
Als letzten Schritt,

176
00:14:44,040 --> 00:14:47,250
packe ich das Ganze in eine Sigmoid-Funktion.

177
00:14:47,250 --> 00:14:55,200
Und was das darstellen soll ist, dass Sie die Sigmoid-Funktion auf jeden der resultierenden Vektorkomponenten anwenden.

178
00:14:55,500 --> 00:15:00,740
Wenn Sie also diese Gewichtsmatrix und diese Vektoren als eigene Symbole darstellen, können Sie

179
00:15:01,000 --> 00:15:07,589
den gesamten Aktivierungs-Übergang von einer Stufe zur nächsten in kürzester Form notieren.

180
00:15:07,920 --> 00:15:15,940
Dies macht den entsprechenden Code sowohl viel einfacher zu schreiben, als auch viel schneller, da viele Bibliotheken Matrixmultiplikation optimiert haben.

181
00:15:17,560 --> 00:15:21,360
Erinnern Sie sich, wie ich sagte, dass diese Neuronen einfach Dinge seien, die Zahlen halten können?

182
00:15:21,790 --> 00:15:26,250
Nun, natürlich hängt dabei die spezifische Zahl davon ab, was man dem Netzwerk als Input liefert.

183
00:15:27,790 --> 00:15:32,940
Es wäre aber präziser zu sagen, dass jedes Neuron einer Funktion entspricht. Eine Funktion, die als eigenen Input

184
00:15:33,070 --> 00:15:38,070
die Resultate aller vorangegangenen Neuronen erhält, und eine Zahl zwischen 0 und 1 ausspuckt.

185
00:15:38,800 --> 00:15:42,270
Eigentlich ist das Ganze Netzwerk einfach eine grosse Funktion, welche

186
00:15:42,760 --> 00:15:47,010
784 Zahlen als Input erhält und zehn Zahlen als Output serviert.

187
00:15:47,460 --> 00:15:49,620
Es ist eine absurd komlizierte Funktion,

188
00:15:49,820 --> 00:15:56,240
eine die 13'000 Parameter in Form von Gewichten und Verzerrungen beinhaltet und gewisse Muster erkennt,

189
00:15:56,249 --> 00:16:00,260
in dem sie immer wieder Matrizen-Vektor-Produkte und Skalierungen auswertet.

190
00:16:00,600 --> 00:16:06,380
Aber es ist nur eine Funktion. Und gewissermassen ist die augenscheinliche Komplexität beruhigend:

191
00:16:06,940 --> 00:16:12,780
Wenn es irgendwie einfacher wäre, welche Hoffnung hätten wir, dass es der Herausforderung der Zahlenerkennung gewachsen wäre?

192
00:16:12,960 --> 00:16:19,560
Und wie nimmte es sich dieser Herausforderung überhaupt an? Wie erkennt das Netzwerk die entsprechenden Gewichte und Verzerrungen, wenn es nur entsprechende Daten erhält?

193
00:16:20,080 --> 00:16:26,039
Genau das werde ich im nächsten Video aufzeigen. Ebenfalls werde ich etwas detaillierter darauf eingehen, was im gezeigten Netzwerk tatsächlich abläuft.

194
00:16:27,130 --> 00:16:32,640
Jetzt ist der Zeitpunkt da, an dem ich vermutlich an das Abonnieren und 'Gefällt-Mir-Drücken' erinnern sollte, damit Sie auf dem Laufenden bleiben können.

195
00:16:32,760 --> 00:16:37,560
Aber realistisch gesehen erhalten die meisten von Ihnen wahrscheinlich sowieso keine Youtube-Benachrichtigungen, nicht wahr?

196
00:16:37,560 --> 00:16:42,260
Vielleicht sollte ich eher sagen: "Abonniere, damit Youtube's Neuronales Netzwerk, welches die

197
00:16:42,459 --> 00:16:47,639
Empfehlungen generiert, dazu erzogen wird zu glauben, dass Du mehr von diesem Kanal sehen möchtest.

198
00:16:48,250 --> 00:16:50,250
 

199
00:16:50,410 --> 00:16:53,550
Vielen Dank für alle Unterstützer von Patreon.

200
00:16:53,589 --> 00:16:56,759
Ich war ein wenig langsam diesen Sommer was die Serie zum Thema Wahrscheinlichkeit betrifft,

201
00:16:56,760 --> 00:17:01,379
Aber ich mache gleich nach diesem Projekt wieder weiter. Patreons, Ihr könnt also nach Updates Ausschau halten.

202
00:17:03,310 --> 00:17:05,550
Zu guter Letzt habe ich nun noch Lisha Li bei mir.

203
00:17:05,550 --> 00:17:12,029
Lee hat ihre Doktorarbeit über die theoretische Seite von Deep Learning geschrieben und arbeitet zurzeit bei einer Venture Capital Firma namens Amplify Partners, welche

204
00:17:12,030 --> 00:17:16,530
freundlicherweise die Produktion dieses Videos mitfinanziert habt. Also, Lisha:

205
00:17:16,530 --> 00:17:19,109
Ich denke, wir sollten schnell diese Sigmoidfunktion hervorholen.

206
00:17:19,180 --> 00:17:24,780
Wie ich es verstanden habe, nutzten frühere Netzwerke diese Funktion um die relevanten gewichteten Summen in einen Intervall von 0-1 zu quetschen.

207
00:17:24,980 --> 00:17:29,680
Sie wissen schon, von dieser biologischen Analogie motiviert, dass ein Neuron aktiv oder inaktiv ist.

208
00:17:29,900 --> 00:17:35,680
(Lisha:) Ja, genau!
(3B1B:) Aber inzwischen nutzen nur wenige moderne Netzwerke die Sigmoid-Funktion. Ist das veraltet?

209
00:17:35,880 --> 00:17:42,780
Lisha: Ja. Bzw. scheint Relu viel leichter zu sein für's Training. (3B1B:) Und Relu steht für entzerrte Lineareinheit? (frei übersetzt)

210
00:17:42,780 --> 00:17:48,839
(Lisha:) - Ja, es ist diese Art Funktion, wo Sie ein Maximum 0 und a definieren, wobei a so definiert wird,

211
00:17:49,120 --> 00:17:53,670
wie Du es im Video erklärt hast und woher dies stammt ist, so glaube ich,

212
00:17:54,600 --> 00:17:57,760
ist teilweise durch eine Analogie aus der Biologie, wonach

213
00:17:57,820 --> 00:18:03,100
Neuronen entweder aktiviert oder nicht aktiviert sind, je nach dem, ob eine bestimmte Schwelle überwunden ist.

214
00:18:03,240 --> 00:18:05,660
Wenn dem so ist, verwenden wir die Identitätsfunktion,

215
00:18:05,840 --> 00:18:10,420
Wenn nicht, dann ist die Aktivierung einfach gleich Null. Das ist also eine Vereinfachung.

216
00:18:10,720 --> 00:18:15,080
Sigmoid funktionierte nicht gut - oder eher: es war schwierig Verbesserungen zu erzielen.

217
00:18:15,300 --> 00:18:19,580
Irgendwann hat man Relu ausprobiert und es schien zu funktionieren.

218
00:18:20,100 --> 00:18:24,920
Sehr gut sogar - für diese unglaublich tiefgründigen Neuronalen Netzwerke.

219
00:18:25,080 --> 00:18:26,060
(3B1B:) Danke, Lisha!

