1
00:00:04,220 --> 00:00:05,400
Đây là số 3.

2
00:00:06,060 --> 00:00:10,233
Nó được viết và hiển thị một cách cẩu thả ở độ phân giải cực thấp 28x28 pixel, 

3
00:00:10,233 --> 00:00:13,720
nhưng bộ não của bạn không gặp khó khăn gì khi nhận ra nó là số 3.

4
00:00:14,340 --> 00:00:16,670
Và tôi muốn bạn dành chút thời gian để đánh giá cao cách 

5
00:00:16,670 --> 00:00:18,960
bộ não có thể làm điều này một cách dễ dàng đến thế nào.

6
00:00:19,700 --> 00:00:23,926
Ý tôi là, cái này, cái này và cái này cũng có thể được nhận dạng là 3 giây, 

7
00:00:23,926 --> 00:00:28,320
mặc dù giá trị cụ thể của từng pixel rất khác nhau giữa các hình ảnh tiếp theo.

8
00:00:28,900 --> 00:00:32,920
Các tế bào nhạy cảm với ánh sáng cụ thể trong mắt bạn đang hoạt động khi bạn nhìn 

9
00:00:32,920 --> 00:00:36,940
thấy 3 cái này rất khác với các tế bào đang hoạt động khi bạn nhìn thấy cái này 3.

10
00:00:37,520 --> 00:00:41,063
Nhưng có điều gì đó trong vỏ não thị giác cực kỳ thông minh của 

11
00:00:41,063 --> 00:00:44,384
bạn phân giải những điều này như thể hiện cùng một ý tưởng, 

12
00:00:44,384 --> 00:00:48,260
đồng thời nhận ra những hình ảnh khác là ý tưởng riêng biệt của chúng.

13
00:00:49,220 --> 00:00:53,337
Nhưng nếu tôi nói với bạn, này, hãy ngồi xuống và viết cho tôi một 

14
00:00:53,337 --> 00:00:57,515
chương trình lấy một lưới 28x28 pixel như thế này và xuất ra một số 

15
00:00:57,515 --> 00:01:02,001
duy nhất trong khoảng từ 0 đến 10, cho bạn biết nó nghĩ chữ số đó là gì, 

16
00:01:02,001 --> 00:01:06,180
thì nhiệm vụ sẽ bắt đầu từ hài hước tầm thường đến vô cùng khó khăn.

17
00:01:07,160 --> 00:01:09,680
Trừ khi bạn đang sống dưới một hang đá, tôi nghĩ tôi hầu như 

18
00:01:09,680 --> 00:01:12,119
không cần phải thúc đẩy sự liên quan và tầm quan trọng của 

19
00:01:12,119 --> 00:01:14,640
học máy và mạng lưới thần kinh đối với hiện tại và tương lai.

20
00:01:15,120 --> 00:01:18,638
Nhưng điều tôi muốn làm ở đây là cho bạn thấy mạng lưới thần kinh thực sự là gì, 

21
00:01:18,638 --> 00:01:21,940
giả sử không có kiến thức nền tảng, và giúp hình dung những gì nó đang làm, 

22
00:01:21,940 --> 00:01:24,460
không phải như một từ thông dụng mà như một phần toán học.

23
00:01:25,020 --> 00:01:29,625
Tôi hy vọng rằng bạn sẽ cảm thấy như chính cấu trúc đó đã được thúc đẩy và cảm thấy 

24
00:01:29,625 --> 00:01:34,340
như bạn biết ý nghĩa của nó khi đọc hoặc bạn nghe về một mạng lưới thần kinh "học tập"

25
00:01:35,360 --> 00:01:40,260
Video này sẽ chỉ dành cho thành phần cấu trúc của nó và video sau sẽ đề cập đến việc học.

26
00:01:40,960 --> 00:01:43,381
Những gì chúng ta sắp làm là tập hợp một mạng lưới 

27
00:01:43,381 --> 00:01:46,040
thần kinh có thể học cách nhận dạng các chữ số viết tay.

28
00:01:49,360 --> 00:01:53,898
Đây là một ví dụ khá cổ điển để giới thiệu chủ đề và tôi rất vui được giữ nguyên hiện 

29
00:01:53,898 --> 00:01:57,697
trạng ở đây vì ở cuối hai video tôi muốn chỉ cho bạn các nguồn hữu ích, 

30
00:01:57,697 --> 00:02:02,341
nơi bạn có thể học và tải xuống code để thực hiện điều này và chơi với nó trên máy tính 

31
00:02:02,341 --> 00:02:03,080
của riêng bạn.

32
00:02:05,040 --> 00:02:09,692
Có rất nhiều biến thể của mạng lưới thần kinh và trong những năm gần đây đã 

33
00:02:09,692 --> 00:02:15,201
có sự bùng nổ trong nghiên cứu về các biến thể này, nhưng trong hai video giới thiệu này, 

34
00:02:15,201 --> 00:02:19,180
bạn và tôi sẽ chỉ xem xét dạng vani đơn giản nhất, không rườm rà.

35
00:02:19,860 --> 00:02:24,202
Đây là điều kiện tiên quyết cần thiết để hiểu bất kỳ biến thể hiện đại mạnh mẽ 

36
00:02:24,202 --> 00:02:28,600
nào và tin tôi đi, nó vẫn còn rất nhiều điều phức tạp để chúng ta phải suy nghĩ.

37
00:02:29,120 --> 00:02:32,766
Nhưng ngay cả ở dạng đơn giản nhất này, nó vẫn có thể học cách nhận 

38
00:02:32,766 --> 00:02:36,520
dạng các chữ số viết tay, đây là một điều khá thú vị đối với máy tính.

39
00:02:37,480 --> 00:02:39,782
Và đồng thời, bạn sẽ thấy nó không còn như thế 

40
00:02:39,782 --> 00:02:42,280
nào với một vài hy vọng mà ta có thể đặt ra cho nó.

41
00:02:43,380 --> 00:02:47,108
Đúng như tên gọi, mạng lưới thần kinh được lấy cảm hứng từ bộ não, 

42
00:02:47,108 --> 00:02:48,500
nhưng hãy chia nhỏ nó ra.

43
00:02:48,520 --> 00:02:51,660
Tế bào thần kinh là gì và chúng liên kết với nhau theo nghĩa nào?

44
00:02:52,500 --> 00:02:56,376
Ngay bây giờ khi tôi nói đến nơ-ron, tất cả những gì tôi muốn 

45
00:02:56,376 --> 00:03:00,440
bạn nghĩ đến là một thứ chứa một số, cụ thể là một số từ 0 đến 1.

46
00:03:00,680 --> 00:03:02,560
Nó thực sự không nhiều hơn thế.

47
00:03:03,780 --> 00:03:09,038
Ví dụ: mạng bắt đầu với một loạt các nơ-ron tương ứng với mỗi pixel 

48
00:03:09,038 --> 00:03:14,220
trong số 28x28 pixel của hình ảnh đầu vào, tổng cộng có 784 nơ-ron.

49
00:03:14,700 --> 00:03:20,958
Mỗi cái chứa một số đại diện cho giá trị thang độ xám của pixel tương ứng, 

50
00:03:20,958 --> 00:03:24,380
từ 0 cho pixel đen đến 1 cho pixel trắng.

51
00:03:25,300 --> 00:03:28,511
Con số này bên trong nơ-ron được gọi là kích hoạt của nó, 

52
00:03:28,511 --> 00:03:32,997
và hình ảnh mà bạn có thể nghĩ đến ở đây là mỗi nơ-ron sẽ sáng lên khi kích hoạt 

53
00:03:32,997 --> 00:03:34,160
của nó là một số cao.

54
00:03:36,720 --> 00:03:41,860
Vậy tất cả 784 nơ-ron này tạo thành lớp đầu tiên trong mạng lưới của chúng ta.

55
00:03:46,500 --> 00:03:49,511
Bây giờ chuyển sang lớp cuối cùng, lớp này có 10 nơ-ron, 

56
00:03:49,511 --> 00:03:51,360
mỗi nơ-ron đại diện cho một chữ số.

57
00:03:52,040 --> 00:03:56,696
Sự kích hoạt trong các nơ-ron này, cũng là một số nằm trong khoảng từ 0 đến 1, 

58
00:03:56,696 --> 00:04:01,825
thể hiện mức độ hệ thống cho rằng một hình ảnh nhất định tương ứng với một chữ số nhất 

59
00:04:01,825 --> 00:04:02,120
định.

60
00:04:03,040 --> 00:04:06,739
Ngoài ra còn có một vài lớp ở giữa được gọi là lớp ẩn, 

61
00:04:06,739 --> 00:04:12,052
hiện tại chỉ là một dấu hỏi khổng lồ về việc quá trình nhận dạng chữ số này sẽ 

62
00:04:12,052 --> 00:04:13,600
được xử lý như thế nào.

63
00:04:14,260 --> 00:04:17,381
Trong mạng này, tôi đã chọn hai lớp ẩn, mỗi lớp có 16 

64
00:04:17,381 --> 00:04:20,560
nơ-ron và phải thừa nhận rằng đó là một lựa chọn tùy ý.

65
00:04:21,019 --> 00:04:24,560
Thành thật mà nói, tôi đã chọn hai lớp dựa trên cách tôi muốn thúc đẩy 

66
00:04:24,560 --> 00:04:28,200
cấu trúc chỉ một lát và 16, đó chỉ là một con số đẹp để vừa với màn hình.

67
00:04:28,780 --> 00:04:32,340
Trong thực tế, có rất nhiều chỗ để thử nghiệm một cấu trúc cụ thể ở đây.

68
00:04:33,020 --> 00:04:35,691
Cách thức hoạt động của mạng, kích hoạt trong 

69
00:04:35,691 --> 00:04:38,480
một lớp sẽ xác định kích hoạt của lớp tiếp theo.

70
00:04:39,200 --> 00:04:43,890
Và tất nhiên, trung tâm của mạng với tư cách là một cơ chế xử lý thông tin phụ thuộc vào 

71
00:04:43,890 --> 00:04:48,580
cách chính xác những kích hoạt đó từ một lớp sẽ mang lại những kích hoạt ở lớp tiếp theo.

72
00:04:49,140 --> 00:04:53,211
Nó có ý nghĩa tương tự một cách lỏng lẻo với cách trong mạng lưới sinh học của 

73
00:04:53,211 --> 00:04:57,180
các nơ-ron, một số nhóm nơ-ron kích hoạt sẽ khiến một số nhóm khác kích hoạt.

74
00:04:58,120 --> 00:05:00,670
Mạng mà tôi đang trình bày ở đây đã được đào tạo để nhận 

75
00:05:00,670 --> 00:05:03,400
dạng các chữ số và để tôi cho bạn biết ý tôi khi nói điều đó.

76
00:05:03,640 --> 00:05:06,723
Điều đó có nghĩa là nếu bạn đưa vào một hình ảnh, 

77
00:05:06,723 --> 00:05:12,212
chiếu sáng tất cả 784 nơ-ron của lớp đầu vào theo độ sáng của từng pixel trong hình ảnh, 

78
00:05:12,212 --> 00:05:16,652
thì kiểu kích hoạt đó sẽ gây ra một số kiểu rất cụ thể ở lớp tiếp theo, 

79
00:05:16,652 --> 00:05:22,080
tạo ra một số kiểu ở lớp tiếp theo. nó, cuối cùng nó đưa ra một số mẫu trong lớp đầu ra.

80
00:05:22,560 --> 00:05:26,307
Và nơ-ron sáng nhất của lớp đầu ra đó là sự lựa chọn của mạng, 

81
00:05:26,307 --> 00:05:29,400
có thể nói, đối với chữ số mà hình ảnh này đại diện.

82
00:05:32,560 --> 00:05:36,160
Và trước khi chuyển sang bài toán về một lớp ảnh hưởng đến lớp tiếp 

83
00:05:36,160 --> 00:05:39,866
theo thế nào hoặc cách đào tạo hoạt động, ta chỉ nói về lý do tại sao 

84
00:05:39,866 --> 00:05:43,520
lại hợp lý khi chờ đợi một cấu trúc lớp thế này hoạt động thông minh.

85
00:05:44,060 --> 00:05:45,220
Chúng ta đang mong đợi điều gì ở đây?

86
00:05:45,400 --> 00:05:47,600
Hy vọng tốt nhất cho những lớp giữa đó là gì?

87
00:05:48,920 --> 00:05:51,268
Chà, khi bạn hoặc tôi nhận ra các chữ số, chúng 

88
00:05:51,268 --> 00:05:53,520
ta ghép các thành phần khác nhau lại với nhau.

89
00:05:54,200 --> 00:05:56,820
Số 9 có một vòng ở trên và một đường ở bên phải.

90
00:05:57,380 --> 00:06:01,180
Số 8 cũng có một vòng lặp ở trên, nhưng nó được ghép với một vòng khác ở phía dưới.

91
00:06:01,980 --> 00:06:06,820
Về cơ bản, số 4 được chia thành ba dòng cụ thể và những thứ tương tự.

92
00:06:07,600 --> 00:06:11,645
Trong một thế giới hoàn hảo hiện tại, ta có thể hy vọng rằng mỗi nơ-ron ở lớp 

93
00:06:11,645 --> 00:06:15,482
thứ hai đến lớp cuối cùng tương ứng với một trong các thành phần phụ này, 

94
00:06:15,482 --> 00:06:19,216
rằng bất cứ khi nào bạn đưa vào một hình ảnh với một vòng lặp lên trên, 

95
00:06:19,216 --> 00:06:23,780
chẳng hạn như số 9 hoặc số 8, sẽ có một số nơ-ron cụ thể có mức kích hoạt sẽ gần bằng 1.

96
00:06:24,500 --> 00:06:27,194
Và ý tôi không phải là vòng lặp pixel cụ thể này, 

97
00:06:27,194 --> 00:06:31,560
hy vọng là bất kỳ mô hình lặp nói chung nào về phía trên sẽ kích hoạt nơ-ron này.

98
00:06:32,440 --> 00:06:36,148
Bằng cách đó, đi từ lớp thứ ba đến lớp cuối cùng chỉ cần tìm 

99
00:06:36,148 --> 00:06:40,040
hiểu sự kết hợp của các thành phần phụ tương ứng với chữ số nào.

100
00:06:41,000 --> 00:06:42,913
Tất nhiên, điều đó chỉ giải quyết vấn đề tạm thời, 

101
00:06:42,913 --> 00:06:46,139
vì làm thế nào bạn có thể nhận ra những thành phần phụ này hoặc thậm chí tìm hiểu xem 

102
00:06:46,139 --> 00:06:47,640
những thành phần phụ phù hợp phải là gì?

103
00:06:48,060 --> 00:06:51,546
Và tôi thậm chí còn chưa nói về việc một lớp ảnh hưởng đến lớp sau thế nào, 

104
00:06:51,546 --> 00:06:53,060
nhưng hãy nói về lớp này một lát.

105
00:06:53,680 --> 00:06:56,680
Nhận biết một vòng lặp cũng có thể chia thành các bài toán con.

106
00:06:57,280 --> 00:07:02,780
Một cách hợp lý để làm điều này là trước tiên nhận dạng các cạnh nhỏ khác nhau tạo nên nó.

107
00:07:03,780 --> 00:07:07,972
Tương tự, một đường dài, giống như loại bạn có thể thấy ở các chữ số 1, 

108
00:07:07,972 --> 00:07:11,466
4 hoặc 7, thực sự chỉ là một cạnh dài, hoặc có thể bạn nghĩ 

109
00:07:11,466 --> 00:07:14,320
nó như một mẫu nhất định của một số cạnh nhỏ hơn.

110
00:07:15,140 --> 00:07:18,993
Vì vậy, có lẽ chúng ta hy vọng rằng mỗi nơ-ron ở lớp thứ hai 

111
00:07:18,993 --> 00:07:22,720
của mạng tương ứng với các cạnh nhỏ có liên quan khác nhau.

112
00:07:23,540 --> 00:07:26,442
Có thể khi một hình ảnh như thế này xuất hiện, 

113
00:07:26,442 --> 00:07:31,444
nó sẽ chiếu sáng tất cả các nơ-ron liên kết với khoảng 8 đến 10 cạnh nhỏ cụ thể, 

114
00:07:31,444 --> 00:07:36,261
từ đó làm sáng các nơ-ron liên kết với vòng trên và một đường thẳng đứng dài, 

115
00:07:36,261 --> 00:07:39,720
và những nơ-ron đó sẽ sáng lên nơ-ron liên kết với số 9.

116
00:07:40,680 --> 00:07:44,617
Liệu đây có phải là điều mà mạng cuối cùng của chúng ta thực sự làm hay không lại 

117
00:07:44,617 --> 00:07:48,410
là một câu hỏi khác, câu hỏi mà tôi sẽ quay lại khi ta biết cách đào tạo mạng, 

118
00:07:48,410 --> 00:07:52,540
nhưng đây là hy vọng mà ta có thể có, một loại mục tiêu với cấu trúc phân lớp thế này.

119
00:07:53,160 --> 00:07:56,755
Hơn nữa, bạn có thể tưởng tượng việc phát hiện các cạnh và mẫu như thế 

120
00:07:56,755 --> 00:08:00,300
này sẽ thực sự hữu ích thế nào cho các tác vụ nhận dạng hình ảnh khác.

121
00:08:00,880 --> 00:08:03,028
Và thậm chí ngoài khả năng nhận dạng hình ảnh, 

122
00:08:03,028 --> 00:08:06,137
có tất cả những thứ thông minh mà bạn có thể muốn thực hiện để chia 

123
00:08:06,137 --> 00:08:07,280
thành các lớp trừu tượng.

124
00:08:08,040 --> 00:08:11,974
Ví dụ: phân tích lời nói bao gồm việc lấy âm thanh thô và chọn ra các âm 

125
00:08:11,974 --> 00:08:16,502
thanh riêng biệt, kết hợp để tạo ra các âm tiết nhất định, kết hợp để tạo thành từ, 

126
00:08:16,502 --> 00:08:20,060
kết hợp để tạo thành cụm từ và những suy nghĩ trừu tượng hơn, v.v.

127
00:08:21,100 --> 00:08:25,589
Nhưng quay lại cách hoạt động thực sự này, hãy hình dung ngay bây giờ chính bạn đang 

128
00:08:25,589 --> 00:08:29,920
thiết kế cách chính xác các kích hoạt trong một lớp có thể xác định lớp tiếp theo.

129
00:08:30,860 --> 00:08:34,778
Mục tiêu là có một số cơ chế có thể kết hợp các pixel thành các cạnh 

130
00:08:34,778 --> 00:08:38,980
hoặc các cạnh thành các mẫu hoặc các mẫu thành các chữ số một cách hợp lý.

131
00:08:39,440 --> 00:08:44,991
Và để phóng to một ví dụ rất cụ thể, giả sử hy vọng là một nơ-ron cụ thể 

132
00:08:44,991 --> 00:08:50,620
ở lớp thứ hai sẽ xác định xem hình ảnh có cạnh ở vùng này ở đây hay không.

133
00:08:51,440 --> 00:08:55,100
Câu hỏi đặt ra là mạng nên có những thông số nào?

134
00:08:55,640 --> 00:08:59,591
Bạn có thể điều chỉnh các mặt số và nút xoay nào sao cho đủ biểu cảm 

135
00:08:59,591 --> 00:09:03,656
để có thể nắm bắt được mẫu này hoặc bất kỳ mẫu pixel nào khác hoặc mẫu 

136
00:09:03,656 --> 00:09:07,780
mà một số cạnh có thể tạo thành một vòng lặp và những thứ tương tự khác?

137
00:09:08,720 --> 00:09:12,169
Chà, những gì chúng ta sẽ làm là gán trọng số cho từng kết 

138
00:09:12,169 --> 00:09:15,560
nối giữa nơ-ron của chúng ta và các nơ-ron ở lớp đầu tiên.

139
00:09:16,320 --> 00:09:17,700
Những trọng số này chỉ là những con số.

140
00:09:18,540 --> 00:09:21,920
Sau đó lấy tất cả các kích hoạt đó từ lớp đầu tiên 

141
00:09:21,920 --> 00:09:25,500
và tính tổng trọng số của chúng theo các trọng số này.

142
00:09:27,700 --> 00:09:31,231
Tôi thấy hữu ích khi coi các trọng số này được sắp xếp thành một lưới nhỏ 

143
00:09:31,231 --> 00:09:34,668
của riêng chúng và tôi sẽ sử dụng các pixel màu xanh lá cây để biểu thị 

144
00:09:34,668 --> 00:09:37,913
các trọng số dương và các pixel màu đỏ để biểu thị các trọng số âm, 

145
00:09:37,913 --> 00:09:41,780
trong đó độ sáng của pixel đó là một chút mô tả lỏng lẻo về giá trị của trọng số.

146
00:09:42,780 --> 00:09:46,607
Giờ nếu ta đặt các trọng số được liên kết với hầu hết tất cả các pixel 

147
00:09:46,607 --> 00:09:50,380
bằng 0 ngoại trừ một số trọng số dương trong vùng mà ta quan tâm này, 

148
00:09:50,380 --> 00:09:54,046
thì việc lấy tổng trọng số của tất cả các giá trị pixel thực sự chỉ 

149
00:09:54,046 --> 00:09:57,820
bằng việc cộng các giá trị của pixel chỉ trong khu vực mà ta quan tâm.

150
00:09:59,140 --> 00:10:02,426
Và nếu bạn thực sự muốn biết liệu có cạnh nào ở đây hay không, 

151
00:10:02,426 --> 00:10:06,600
điều bạn có thể làm là có một số trọng số âm liên quan đến các pixel xung quanh.

152
00:10:07,480 --> 00:10:12,700
Khi đó tổng lớn nhất khi các pixel ở giữa đó sáng nhưng các pixel xung quanh tối hơn.

153
00:10:14,260 --> 00:10:18,508
Khi bạn tính tổng có trọng số như thế này, bạn có thể đưa ra bất kỳ số nào, 

154
00:10:18,508 --> 00:10:23,540
nhưng đối với mạng này, điều chúng ta muốn là kích hoạt phải có giá trị nào đó từ 0 đến 1.

155
00:10:24,120 --> 00:10:28,066
Vì vậy, một điều thông thường cần làm là đưa tổng có trọng số 

156
00:10:28,066 --> 00:10:32,140
này vào một hàm nào đó để ép trục số thực vào khoảng từ 0 đến 1.

157
00:10:32,460 --> 00:10:35,634
Và một hàm phổ biến thực hiện điều này được gọi là hàm sigmoid, 

158
00:10:35,634 --> 00:10:37,420
còn được gọi là đường cong logistic.

159
00:10:38,000 --> 00:10:41,344
Về cơ bản, đầu vào rất âm có giá trị gần bằng 0, 

160
00:10:41,344 --> 00:10:46,600
đầu vào dương có giá trị gần bằng 1 và chỉ tăng đều đặn xung quanh đầu vào 0.

161
00:10:49,120 --> 00:10:52,703
Vì vậy, việc kích hoạt nơ-ron ở đây về cơ bản là 

162
00:10:52,703 --> 00:10:56,360
thước đo mức độ dương của tổng trọng số liên quan.

163
00:10:57,540 --> 00:11:01,880
Nhưng có lẽ không phải là bạn muốn nơ-ron sáng lên khi tổng trọng số lớn hơn 0.

164
00:11:02,280 --> 00:11:06,360
Có thể bạn chỉ muốn nó hoạt động khi tổng lớn hơn 10.

165
00:11:06,840 --> 00:11:10,260
Tức là bạn muốn có một số độ lệch để nó không hoạt động.

166
00:11:11,380 --> 00:11:15,551
Những gì chúng ta sẽ làm sau đó chỉ là thêm một số số khác như âm 

167
00:11:15,551 --> 00:11:19,660
10 vào tổng có trọng số này trước khi thay nó vào hàm ép sigmoid.

168
00:11:20,580 --> 00:11:22,440
Số bổ sung đó được gọi là độ lệch.

169
00:11:23,460 --> 00:11:27,306
Vì vậy, các trọng số cho bạn biết mô hình pixel mà nơ-ron này ở 

170
00:11:27,306 --> 00:11:31,153
lớp thứ hai đang chọn và độ lệch cho bạn biết tổng trọng số cần 

171
00:11:31,153 --> 00:11:35,180
phải cao đến mức nào trước khi nơ-ron bắt đầu hoạt động có ý nghĩa.

172
00:11:36,120 --> 00:11:37,680
Và đó chỉ là một tế bào thần kinh.

173
00:11:38,280 --> 00:11:44,571
Mọi nơ-ron khác trong lớp này sẽ được kết nối với tất cả các nơ-ron 784 pixel từ 

174
00:11:44,571 --> 00:11:50,940
lớp đầu tiên và mỗi một trong số 784 kết nối đó có trọng số riêng gắn liền với nó.

175
00:11:51,600 --> 00:11:54,467
Ngoài ra, mỗi số có một số độ lệch, một số số khác mà 

176
00:11:54,467 --> 00:11:57,600
bạn cộng vào tổng có trọng số trước khi ép nó bằng sigmoid.

177
00:11:58,110 --> 00:11:59,540
Và đó là rất nhiều điều để suy nghĩ!

178
00:11:59,960 --> 00:12:06,217
Với lớp ẩn gồm 16 nơ-ron này, tức là có tổng cộng 784 lần 16 trọng số, 

179
00:12:06,217 --> 00:12:07,980
cùng với 16 độ lệch.

180
00:12:08,840 --> 00:12:11,940
Và tất cả đó chỉ là sự kết nối từ lớp thứ nhất đến lớp thứ hai.

181
00:12:12,520 --> 00:12:17,340
Các kết nối giữa các lớp khác cũng có nhiều trọng số và độ lệch liên quan đến chúng.

182
00:12:18,340 --> 00:12:23,800
Nói chung, mạng này có gần như chính xác tổng cộng 13.000 trọng số và độ lệch.

183
00:12:23,800 --> 00:12:26,907
13.000 nút và con số có thể được điều chỉnh và đổi hướng 

184
00:12:26,907 --> 00:12:29,960
để làm cho mạng này hoạt động theo những cách khác nhau.

185
00:12:31,040 --> 00:12:36,200
Vì vậy, khi chúng ta nói về việc học, điều được đề cập đến là làm cho máy tính tìm ra một 

186
00:12:36,200 --> 00:12:41,360
cài đặt hợp lệ cho tất cả những con số này để nó thực sự giải quyết được vấn đề trước mắt.

187
00:12:42,620 --> 00:12:47,117
Một thử nghiệm tưởng tượng vừa thú vị vừa hơi đáng sợ là hãy tưởng tượng bạn 

188
00:12:47,117 --> 00:12:51,147
ngồi xuống và thiết lập tất cả các trọng số và độ lệch này bằng tay, 

189
00:12:51,147 --> 00:12:56,112
cố tình điều chỉnh các con số để lớp thứ hai chọn các cạnh, lớp thứ ba chọn các mẫu, 

190
00:12:56,112 --> 00:12:56,580
vân vân.

191
00:12:56,980 --> 00:13:01,680
Cá nhân tôi thấy thỏa mái về điều này hơn là chỉ coi mạng như một hộp đen hoàn toàn, 

192
00:13:01,680 --> 00:13:04,722
bởi vì khi mạng không hoạt động theo cách bạn dự đoán, 

193
00:13:04,722 --> 00:13:08,925
nếu bạn đã xây dựng được một chút mối quan hệ với ý nghĩa thực sự của những 

194
00:13:08,925 --> 00:13:13,184
trọng số và độ lệch đó, bạn có điểm khởi đầu để thử nghiệm cách thay đổi cấu 

195
00:13:13,184 --> 00:13:14,180
trúc để cải thiện.

196
00:13:14,960 --> 00:13:18,444
Hoặc khi mạng hoạt động nhưng không phải vì những lý do bạn có thể mong đợi, 

197
00:13:18,444 --> 00:13:22,064
việc tìm hiểu xem trọng số và độ lệch đang làm gì là một cách hay để thách thức 

198
00:13:22,064 --> 00:13:25,820
các giả định của bạn và thực sự khám phá toàn bộ không gian của các nghiệm khả thi.

199
00:13:26,840 --> 00:13:30,680
Nhân tiện, hàm số thực tế ở đây hơi phức tạp khi viết ra, bạn có nghĩ vậy không?

200
00:13:32,500 --> 00:13:37,140
Vì vậy, hãy để tôi chỉ cho bạn một cách biểu diễn các kết nối này một cách nhỏ gọn hơn.

201
00:13:37,660 --> 00:13:40,520
Đây là cách bạn sẽ thấy nếu bạn chọn đọc thêm về mạng lưới thần kinh.

202
00:13:40,967 --> 00:13:40,520
Sắp xếp tất cả các kích hoạt từ một lớp vào một cột dưới dạng ma trận 

203
00:13:41,380 --> 00:13:40,967
tương ứng với các kết nối giữa một lớp và một nơ-ron cụ thể ở lớp tiếp theo.

204
00:13:41,380 --> 00:13:46,736
Điều đó có nghĩa là việc lấy tổng có trọng số của các lần kích hoạt 

205
00:13:46,736 --> 00:13:52,171
trong lớp đầu tiên theo các trọng số này tương ứng với một trong các 

206
00:13:52,171 --> 00:13:58,000
số hạng trong tích vectơ ma trận của mọi thứ chúng ta có ở bên trái ở đây.

207
00:13:58,540 --> 00:14:02,177
Điều đó có nghĩa là việc lấy tổng có trọng số của các lần kích hoạt 

208
00:14:02,177 --> 00:14:05,868
trong lớp đầu tiên theo các trọng số này tương ứng với một trong các 

209
00:14:05,868 --> 00:14:09,880
số hạng trong tích vectơ ma trận của mọi thứ chúng ta có ở bên trái ở đây. 

210
00:14:14,000 --> 00:14:18,652
Nhân tiện, phần lớn việc học máy đều phụ thuộc vào việc nắm bắt tốt đại số tuyến tính, 

211
00:14:18,652 --> 00:14:22,342
vì vậy đối với bất kỳ ai trong số các bạn muốn hiểu rõ về ma trận và 

212
00:14:22,342 --> 00:14:25,872
ý nghĩa của phép nhân vectơ ma trận, hãy xem loạt bài tôi đã thực 

213
00:14:25,872 --> 00:14:28,600
hiện trên đại số tuyến tính, đặc biệt là chương 3. 

214
00:14:29,240 --> 00:14:33,575
Quay lại biểu thức của chúng ta, thay vì nói về việc thêm độ lệch cho từng giá 

215
00:14:33,575 --> 00:14:37,964
trị này một cách độc lập, chúng ta biểu diễn nó bằng cách tổ chức tất cả các độ 

216
00:14:37,964 --> 00:14:42,300
lệch đó thành một vectơ và thêm toàn bộ vectơ vào tích vectơ ma trận trước đó. 

217
00:14:43,280 --> 00:14:47,000
Sau đó, bước cuối cùng, tôi sẽ bọc một sigmoid xung quanh bên 

218
00:14:47,000 --> 00:14:50,660
ngoài ở đây và điều được cho là thể hiện rằng bạn sẽ áp dụng 

219
00:14:50,660 --> 00:14:54,740
hàm sigmoid cho từng thành phần cụ thể của vectơ kết quả bên trong. 

220
00:14:55,940 --> 00:15:00,260
Vì vậy, khi bạn viết ma trận trọng số này và các vectơ này làm ký hiệu riêng, 

221
00:15:00,260 --> 00:15:05,024
bạn có thể truyền đạt toàn bộ quá trình chuyển đổi kích hoạt từ lớp này sang lớp tiếp 

222
00:15:05,024 --> 00:15:07,960
theo bằng một biểu thức cực kỳ chặt chẽ và gọn gàng, 

223
00:15:07,960 --> 00:15:12,834
và điều này làm cho mã liên quan trở nên đơn giản hơn rất nhiều và nhanh hơn rất nhiều, 

224
00:15:12,834 --> 00:15:15,660
vì nhiều thư viện đã tối ưu hóa phép nhân ma trận. 

225
00:15:17,820 --> 00:15:19,620
Nhớ rằng trước đó tôi đã nói những tế bào thần 

226
00:15:19,620 --> 00:15:21,460
kinh này chỉ đơn giản là những thứ chứa các số? 

227
00:15:22,220 --> 00:15:28,128
Tất nhiên, những con số cụ thể mà chúng chứa phụ thuộc vào hình ảnh bạn đưa vào, 

228
00:15:28,128 --> 00:15:32,212
vì vậy sẽ chính xác hơn khi coi mỗi nơ-ron như một hàm, 

229
00:15:32,212 --> 00:15:38,340
một hàm nhận đầu ra của tất cả các nơ-ron ở lớp trước và tạo ra một số giữa 0 và 1. 

230
00:15:39,200 --> 00:15:43,010
Thực sự toàn bộ mạng chỉ là một hàm số, một hàm 

231
00:15:43,010 --> 00:15:47,060
lấy 784 số làm đầu vào và đưa ra 10 số làm đầu ra. 

232
00:15:47,560 --> 00:15:52,544
Đó là một hàm phức tạp đến mức vô lý, một hàm bao gồm 13.000 tham số dưới dạng 

233
00:15:52,544 --> 00:15:57,403
các trọng số và độ lệch theo các mẫu nhất định và liên quan đến việc lặp lại 

234
00:15:57,403 --> 00:16:02,640
nhiều tích vectơ ma trận và hàm ép sigmoid, nhưng dù sao nó cũng chỉ là một hàm số.

235
00:16:03,400 --> 00:16:06,660
Và trong một theo cách đó thì có vẻ yên tâm hơn vì nó trông phức tạp. 

236
00:16:07,340 --> 00:16:09,706
Ý tôi là nếu nó đơn giản hơn chút nữa, ta còn hy vọng gì 

237
00:16:09,706 --> 00:16:12,280
nữa rằng nó có thể đương đầu với thách thức nhận dạng chữ số? 

238
00:16:13,340 --> 00:16:14,700
Và nó thực hiện thử thách đó như thế nào? 

239
00:16:15,080 --> 00:16:17,197
Làm cách nào để mạng này tìm hiểu các trọng số 

240
00:16:17,197 --> 00:16:19,360
và độ lệch thích hợp chỉ bằng cách xem dữ liệu? 

241
00:16:20,140 --> 00:16:23,209
Đó là những gì tôi sẽ trình bày trong video sau và tôi cũng sẽ tìm hiểu thêm 

242
00:16:23,209 --> 00:16:26,120
một chút về hoạt động thực sự của mạng cụ thể mà chúng ta đang thấy này. 

243
00:16:27,580 --> 00:16:30,827
Bây giờ là lúc tôi nghĩ mình nên đăng ký để được thông báo khi có 

244
00:16:30,827 --> 00:16:33,976
video hoặc bất kỳ video mới nào xuất hiện, nhưng thực tế là hầu 

245
00:16:33,976 --> 00:16:37,420
hết các bạn không thực sự nhận được thông báo từ YouTube, phải không? 

246
00:16:38,020 --> 00:16:41,219
Có lẽ thành thật hơn tôi nên nói là đăng ký để các mạng lưới 

247
00:16:41,219 --> 00:16:44,418
thần kinh làm nền tảng cho thuật toán đề xuất của YouTube có 

248
00:16:44,418 --> 00:16:47,880
cơ sở tin rằng bạn muốn xem nội dung từ kênh này đề xuất cho bạn. 

249
00:16:48,560 --> 00:16:49,940
Dù sao hãy đăng để biết thêm. 

250
00:16:50,760 --> 00:16:53,500
Cảm ơn mọi người rất nhiều vì đã ủng hộ những video này trên Patreon. 

251
00:16:54,000 --> 00:16:56,833
Tôi tiến triển hơi chậm trong loạt video xác suất vào mùa hè này, 

252
00:16:56,833 --> 00:16:59,538
nhưng tôi sẽ quay lại với nó sau dự án này, vì vậy những khách 

253
00:16:59,538 --> 00:17:01,900
hàng quen có thể theo dõi các thông tin cập nhật ở đó. 

254
00:17:03,600 --> 00:17:05,813
Để kết thúc mọi chuyện ở đây, tôi có Leisha Lee, 

255
00:17:05,813 --> 00:17:09,426
người đã làm luận án tiến sĩ về mặt lý thuyết của học sâu và hiện đang làm việc 

256
00:17:09,426 --> 00:17:12,000
tại một công ty đầu tư mạo hiểm tên là Amplify Partners, 

257
00:17:12,000 --> 00:17:14,619
người đã vui lòng cung cấp một số kinh phí cho video này. 

258
00:17:15,460 --> 00:17:19,119
Vì vậy, Leisha có một điều tôi nghĩ chúng ta nên nhanh chóng đưa ra là hàm sigmoid này. 

259
00:17:19,700 --> 00:17:23,123
Theo tôi hiểu, các mạng ban đầu sử dụng điều này để gộp tổng trọng số có liên 

260
00:17:23,123 --> 00:17:26,547
quan vào khoảng giữa 0 và 1, bạn biết đấy, loại được thúc đẩy bởi sự tương tự 

261
00:17:26,547 --> 00:17:29,840
sinh học này của các tế bào thần kinh không hoạt động hoặc đang hoạt động. 

262
00:17:30,280 --> 00:17:30,300
Chính xác. 

263
00:17:30,560 --> 00:17:34,040
Nhưng tương đối ít mạng hiện đại thực sự sử dụng sigmoid nữa. 

264
00:17:34,320 --> 00:17:34,320
Vâng

265
00:17:34,440 --> 00:17:35,540
Đó là loại trường học cũ phải không? 

266
00:17:35,760 --> 00:17:38,980
Vâng hay đúng hơn là relu có vẻ dễ huấn luyện hơn nhiều. 

267
00:17:39,400 --> 00:17:42,340
Và relu là viết tắt của đơn vị tuyến tính chỉnh lưu?  

268
00:17:42,680 --> 00:17:47,282
Đúng, đây là loại hàm số trong đó bạn chỉ lấy giá trị tối đa bằng 0 

269
00:17:47,282 --> 00:17:52,087
và trong đó a được đưa ra bởi những gì bạn đang giải thích trong video 

270
00:17:52,087 --> 00:17:56,825
và điều này được thúc đẩy từ đâu. Tôi nghĩ một phần là do sự tương tự 

271
00:17:56,825 --> 00:18:01,360
sinh học với cách các tế bào thần kinh sẽ được kích hoạt hay không.

272
00:18:01,360 --> 00:18:06,127
Và vì vậy nếu nó vượt qua một ngưỡng nhất định thì đó sẽ là chức năng nhận dạng nhưng 

273
00:18:06,127 --> 00:18:10,840
nếu không thì nó sẽ không được kích hoạt nên sẽ bằng 0 nên đó là một sự đơn giản hóa.

274
00:18:11,160 --> 00:18:15,603
 Việc sử dụng sigmoid không giúp ích gì cho việc đào tạo hoặc ở một 

275
00:18:15,603 --> 00:18:20,176
thời điểm nào đó rất khó đào tạo và mọi người chỉ thử relu và nó tình 

276
00:18:20,176 --> 00:18:24,620
cờ hoạt động rất tốt đối với các mạng lưới thần kinh cực kỳ sâu này.

277
00:18:25,100 --> 00:18:25,640
Được rồi, cảm ơn Alicia.

