1
00:00:04,220 --> 00:00:05,400
Đây là số 3.

2
00:00:06,060 --> 00:00:10,233
Nó được viết và hiển thị một cách cẩu thả ở độ phân giải cực thấp 28x28 pixel, 

3
00:00:10,233 --> 00:00:13,720
nhưng bộ não của bạn không gặp khó khăn gì khi nhận ra nó là số 3.

4
00:00:14,340 --> 00:00:16,551
Và tôi muốn bạn dành một chút thời gian để đánh giá cao 

5
00:00:16,551 --> 00:00:18,960
việc bộ não có thể làm điều này một cách dễ dàng đến mức nào.

6
00:00:19,700 --> 00:00:23,926
Ý tôi là, cái này, cái này và cái này cũng có thể được nhận dạng là 3 giây, 

7
00:00:23,926 --> 00:00:28,320
mặc dù giá trị cụ thể của từng pixel rất khác nhau giữa các hình ảnh tiếp theo.

8
00:00:28,900 --> 00:00:32,920
Các tế bào nhạy cảm với ánh sáng cụ thể trong mắt bạn đang hoạt động khi bạn nhìn 

9
00:00:32,920 --> 00:00:36,940
thấy 3 cái này rất khác với các tế bào đang hoạt động khi bạn nhìn thấy cái này 3.

10
00:00:37,520 --> 00:00:41,063
Nhưng có điều gì đó trong vỏ não thị giác cực kỳ thông minh của 

11
00:00:41,063 --> 00:00:44,384
bạn phân giải những điều này như thể hiện cùng một ý tưởng, 

12
00:00:44,384 --> 00:00:48,260
đồng thời nhận ra những hình ảnh khác là ý tưởng riêng biệt của chúng.

13
00:00:49,220 --> 00:00:53,263
Nhưng nếu tôi nói với bạn, này, hãy ngồi xuống và viết cho tôi một 

14
00:00:53,263 --> 00:00:57,368
chương trình lấy một lưới 28x28 pixel như thế này và xuất ra một số 

15
00:00:57,368 --> 00:01:01,774
duy nhất trong khoảng từ 0 đến 10, cho bạn biết nó nghĩ chữ số đó là gì, 

16
00:01:01,774 --> 00:01:06,180
thì nhiệm vụ sẽ bắt đầu từ hài hước tầm thường đến khó khăn đến khó khăn.

17
00:01:07,160 --> 00:01:09,680
Trừ khi bạn đang sống dưới một tảng đá, tôi nghĩ tôi hầu như 

18
00:01:09,680 --> 00:01:12,119
không cần phải thúc đẩy sự liên quan và tầm quan trọng của 

19
00:01:12,119 --> 00:01:14,640
học máy và mạng lưới thần kinh đối với hiện tại và tương lai.

20
00:01:15,120 --> 00:01:18,638
Nhưng điều tôi muốn làm ở đây là cho bạn thấy mạng lưới thần kinh thực sự là gì, 

21
00:01:18,638 --> 00:01:21,940
giả sử không có kiến thức nền tảng, và giúp hình dung những gì nó đang làm, 

22
00:01:21,940 --> 00:01:24,460
không phải như một từ thông dụng mà như một phần toán học.

23
00:01:25,020 --> 00:01:28,009
Tôi hy vọng rằng bạn sẽ cảm thấy như chính cấu trúc đó đã được thúc 

24
00:01:28,009 --> 00:01:31,042
đẩy và cảm thấy như bạn biết ý nghĩa của nó khi đọc hoặc bạn nghe về 

25
00:01:31,042 --> 00:01:34,340
một phương pháp học tập trích dẫn-không trích dẫn trên mạng lưới thần kinh.

26
00:01:35,360 --> 00:01:40,260
Video này sẽ chỉ dành cho thành phần cấu trúc của nó và video sau sẽ đề cập đến việc học.

27
00:01:40,960 --> 00:01:43,381
Những gì chúng ta sắp làm là tập hợp một mạng lưới 

28
00:01:43,381 --> 00:01:46,040
thần kinh có thể học cách nhận dạng các chữ số viết tay.

29
00:01:49,360 --> 00:01:52,710
Đây là một ví dụ khá cổ điển để giới thiệu chủ đề và tôi rất vui được giữ 

30
00:01:52,710 --> 00:01:56,152
nguyên hiện trạng ở đây vì ở cuối hai video tôi muốn chỉ cho bạn một số tài 

31
00:01:56,152 --> 00:01:59,185
nguyên hữu ích để bạn có thể tìm hiểu thêm và tìm hiểu thêm ở đâu. 

32
00:01:59,185 --> 00:02:03,080
bạn có thể tải xuống mã thực hiện việc này và chơi với nó trên máy tính của riêng bạn.

33
00:02:05,040 --> 00:02:09,692
Có rất nhiều biến thể của mạng lưới thần kinh và trong những năm gần đây đã 

34
00:02:09,692 --> 00:02:15,201
có sự bùng nổ trong nghiên cứu về các biến thể này, nhưng trong hai video giới thiệu này, 

35
00:02:15,201 --> 00:02:19,180
bạn và tôi sẽ chỉ xem xét dạng vani đơn giản nhất, không rườm rà.

36
00:02:19,860 --> 00:02:24,202
Đây là điều kiện tiên quyết cần thiết để hiểu bất kỳ biến thể hiện đại mạnh mẽ 

37
00:02:24,202 --> 00:02:28,600
nào và tin tôi đi, nó vẫn còn rất nhiều điều phức tạp để chúng ta phải suy nghĩ.

38
00:02:29,120 --> 00:02:32,766
Nhưng ngay cả ở dạng đơn giản nhất này, nó vẫn có thể học cách nhận 

39
00:02:32,766 --> 00:02:36,520
dạng các chữ số viết tay, đây là một điều khá thú vị đối với máy tính.

40
00:02:37,480 --> 00:02:39,833
Và đồng thời, bạn sẽ thấy nó không còn như thế nào 

41
00:02:39,833 --> 00:02:42,280
với một vài hy vọng mà chúng ta có thể đặt ra cho nó.

42
00:02:43,380 --> 00:02:47,108
Đúng như tên gọi, mạng lưới thần kinh được lấy cảm hứng từ bộ não, 

43
00:02:47,108 --> 00:02:48,500
nhưng hãy chia nhỏ nó ra.

44
00:02:48,520 --> 00:02:51,660
Tế bào thần kinh là gì và chúng liên kết với nhau theo nghĩa nào?

45
00:02:52,500 --> 00:02:56,376
Ngay bây giờ khi tôi nói đến nơ-ron, tất cả những gì tôi muốn 

46
00:02:56,376 --> 00:03:00,440
bạn nghĩ đến là một thứ chứa một số, cụ thể là một số từ 0 đến 1.

47
00:03:00,680 --> 00:03:02,560
Nó thực sự không nhiều hơn thế.

48
00:03:03,780 --> 00:03:09,038
Ví dụ: mạng bắt đầu với một loạt các nơ-ron tương ứng với mỗi pixel 

49
00:03:09,038 --> 00:03:14,220
trong số 28x28 pixel của hình ảnh đầu vào, tổng cộng có 784 nơ-ron.

50
00:03:14,700 --> 00:03:20,958
Mỗi cái chứa một số đại diện cho giá trị thang độ xám của pixel tương ứng, 

51
00:03:20,958 --> 00:03:24,380
từ 0 cho pixel đen đến 1 cho pixel trắng.

52
00:03:25,300 --> 00:03:28,511
Con số này bên trong nơ-ron được gọi là kích hoạt của nó, 

53
00:03:28,511 --> 00:03:32,997
và hình ảnh mà bạn có thể nghĩ đến ở đây là mỗi nơ-ron sẽ sáng lên khi kích hoạt 

54
00:03:32,997 --> 00:03:34,160
của nó là một số cao.

55
00:03:36,720 --> 00:03:41,860
Vậy tất cả 784 nơ-ron này tạo thành lớp đầu tiên trong mạng lưới của chúng ta.

56
00:03:46,500 --> 00:03:49,511
Bây giờ chuyển sang lớp cuối cùng, lớp này có 10 nơ-ron, 

57
00:03:49,511 --> 00:03:51,360
mỗi nơ-ron đại diện cho một chữ số.

58
00:03:52,040 --> 00:03:56,696
Sự kích hoạt trong các nơ-ron này, cũng là một số nằm trong khoảng từ 0 đến 1, 

59
00:03:56,696 --> 00:04:01,825
thể hiện mức độ hệ thống cho rằng một hình ảnh nhất định tương ứng với một chữ số nhất 

60
00:04:01,825 --> 00:04:02,120
định.

61
00:04:03,040 --> 00:04:06,739
Ngoài ra còn có một vài lớp ở giữa được gọi là lớp ẩn, 

62
00:04:06,739 --> 00:04:12,052
hiện tại chỉ là một dấu hỏi khổng lồ về việc quá trình nhận dạng chữ số này sẽ 

63
00:04:12,052 --> 00:04:13,600
được xử lý như thế nào.

64
00:04:14,260 --> 00:04:17,381
Trong mạng này, tôi đã chọn hai lớp ẩn, mỗi lớp có 16 

65
00:04:17,381 --> 00:04:20,560
nơ-ron và phải thừa nhận rằng đó là một lựa chọn tùy ý.

66
00:04:21,019 --> 00:04:24,586
Thành thật mà nói, tôi đã chọn hai lớp dựa trên cách tôi muốn thúc đẩy cấu 

67
00:04:24,586 --> 00:04:28,200
trúc chỉ trong giây lát và 16, đó chỉ là một con số đẹp để vừa với màn hình.

68
00:04:28,780 --> 00:04:32,340
Trong thực tế, có rất nhiều chỗ để thử nghiệm một cấu trúc cụ thể ở đây.

69
00:04:33,020 --> 00:04:35,691
Cách thức hoạt động của mạng, kích hoạt trong 

70
00:04:35,691 --> 00:04:38,480
một lớp sẽ xác định kích hoạt của lớp tiếp theo.

71
00:04:39,200 --> 00:04:43,890
Và tất nhiên, trung tâm của mạng với tư cách là một cơ chế xử lý thông tin phụ thuộc vào 

72
00:04:43,890 --> 00:04:48,580
cách chính xác những kích hoạt đó từ một lớp sẽ mang lại những kích hoạt ở lớp tiếp theo.

73
00:04:49,140 --> 00:04:53,211
Nó có ý nghĩa tương tự một cách lỏng lẻo với cách trong mạng lưới sinh học của 

74
00:04:53,211 --> 00:04:57,180
các nơ-ron, một số nhóm nơ-ron kích hoạt sẽ khiến một số nhóm khác kích hoạt.

75
00:04:58,120 --> 00:05:00,670
Mạng mà tôi đang trình bày ở đây đã được đào tạo để nhận 

76
00:05:00,670 --> 00:05:03,400
dạng các chữ số và để tôi cho bạn biết ý tôi khi nói điều đó.

77
00:05:03,640 --> 00:05:06,723
Điều đó có nghĩa là nếu bạn đưa vào một hình ảnh, 

78
00:05:06,723 --> 00:05:12,212
chiếu sáng tất cả 784 nơ-ron của lớp đầu vào theo độ sáng của từng pixel trong hình ảnh, 

79
00:05:12,212 --> 00:05:16,652
thì kiểu kích hoạt đó sẽ gây ra một số kiểu rất cụ thể ở lớp tiếp theo, 

80
00:05:16,652 --> 00:05:22,080
tạo ra một số kiểu ở lớp tiếp theo. nó, cuối cùng nó đưa ra một số mẫu trong lớp đầu ra.

81
00:05:22,560 --> 00:05:26,307
Và nơ-ron sáng nhất của lớp đầu ra đó là sự lựa chọn của mạng, 

82
00:05:26,307 --> 00:05:29,400
có thể nói, đối với chữ số mà hình ảnh này đại diện.

83
00:05:32,560 --> 00:05:36,258
Và trước khi chuyển sang tính toán xem lớp này ảnh hưởng như thế nào đến lớp tiếp 

84
00:05:36,258 --> 00:05:38,423
theo hoặc cách hoạt động của quá trình đào tạo, 

85
00:05:38,423 --> 00:05:42,121
chúng ta hãy nói về lý do tại sao việc mong đợi một cấu trúc lớp như thế này hoạt 

86
00:05:42,121 --> 00:05:43,520
động thông minh là điều hợp lý.

87
00:05:44,060 --> 00:05:45,220
Chúng ta đang mong đợi điều gì ở đây?

88
00:05:45,400 --> 00:05:47,600
Hy vọng tốt nhất cho những lớp giữa đó là gì?

89
00:05:48,920 --> 00:05:51,268
Chà, khi bạn hoặc tôi nhận ra các chữ số, chúng 

90
00:05:51,268 --> 00:05:53,520
ta ghép các thành phần khác nhau lại với nhau.

91
00:05:54,200 --> 00:05:56,820
Số 9 có một vòng ở trên và một đường ở bên phải.

92
00:05:57,380 --> 00:06:01,180
Số 8 cũng có một vòng lặp ở trên, nhưng nó được ghép với một vòng khác ở phía dưới.

93
00:06:01,980 --> 00:06:06,820
Về cơ bản, số 4 được chia thành ba dòng cụ thể và những thứ tương tự.

94
00:06:07,600 --> 00:06:11,568
Bây giờ, trong một thế giới hoàn hảo, chúng ta có thể hy vọng rằng mỗi nơ-ron 

95
00:06:11,568 --> 00:06:15,639
ở lớp thứ hai đến lớp cuối cùng tương ứng với một trong các thành phần phụ này, 

96
00:06:15,639 --> 00:06:19,302
rằng bất cứ khi nào bạn đưa vào một hình ảnh với một vòng lặp lên trên, 

97
00:06:19,302 --> 00:06:23,780
chẳng hạn như số 9 hoặc số 8, sẽ có một số nơ-ron cụ thể có mức kích hoạt sẽ gần bằng 1.

98
00:06:24,500 --> 00:06:27,194
Và ý tôi không phải là vòng lặp pixel cụ thể này, 

99
00:06:27,194 --> 00:06:31,560
hy vọng là bất kỳ mô hình lặp nói chung nào về phía trên sẽ kích hoạt nơ-ron này.

100
00:06:32,440 --> 00:06:36,148
Bằng cách đó, đi từ lớp thứ ba đến lớp cuối cùng chỉ cần tìm 

101
00:06:36,148 --> 00:06:40,040
hiểu sự kết hợp của các thành phần phụ tương ứng với chữ số nào.

102
00:06:41,000 --> 00:06:44,320
Tất nhiên, điều đó chỉ giải quyết được vấn đề vì làm thế nào bạn có thể nhận ra những 

103
00:06:44,320 --> 00:06:47,640
thành phần phụ này hoặc thậm chí tìm hiểu xem những thành phần phụ phù hợp phải là gì?

104
00:06:48,060 --> 00:06:51,419
Và tôi thậm chí còn chưa nói về việc một lớp ảnh hưởng đến lớp tiếp theo như thế nào, 

105
00:06:51,419 --> 00:06:53,060
nhưng hãy cùng tôi nói về lớp này một lát.

106
00:06:53,680 --> 00:06:56,680
Nhận biết một vòng lặp cũng có thể chia thành các bài toán con.

107
00:06:57,280 --> 00:07:00,030
Một cách hợp lý để làm điều này là trước tiên 

108
00:07:00,030 --> 00:07:02,780
hãy nhận ra các cạnh nhỏ khác nhau tạo nên nó.

109
00:07:03,780 --> 00:07:07,972
Tương tự, một đường dài, giống như loại bạn có thể thấy ở các chữ số 1, 

110
00:07:07,972 --> 00:07:11,466
4 hoặc 7, thực sự chỉ là một cạnh dài, hoặc có thể bạn nghĩ 

111
00:07:11,466 --> 00:07:14,320
nó như một mẫu nhất định của một số cạnh nhỏ hơn.

112
00:07:15,140 --> 00:07:18,993
Vì vậy, có lẽ chúng ta hy vọng rằng mỗi nơ-ron ở lớp thứ hai 

113
00:07:18,993 --> 00:07:22,720
của mạng tương ứng với các cạnh nhỏ có liên quan khác nhau.

114
00:07:23,540 --> 00:07:26,442
Có thể khi một hình ảnh như thế này xuất hiện, 

115
00:07:26,442 --> 00:07:31,444
nó sẽ chiếu sáng tất cả các nơ-ron liên kết với khoảng 8 đến 10 cạnh nhỏ cụ thể, 

116
00:07:31,444 --> 00:07:36,261
từ đó làm sáng các nơ-ron liên kết với vòng trên và một đường thẳng đứng dài, 

117
00:07:36,261 --> 00:07:39,720
và những nơ-ron đó sẽ sáng lên nơ-ron liên kết với số 9.

118
00:07:40,680 --> 00:07:43,655
Liệu đây có phải là điều mà mạng cuối cùng của chúng ta thực sự làm 

119
00:07:43,655 --> 00:07:46,544
hay không lại là một câu hỏi khác, câu hỏi mà tôi sẽ quay lại khi 

120
00:07:46,544 --> 00:07:50,264
chúng ta biết cách huấn luyện mạng, nhưng đây là niềm hy vọng mà chúng ta có thể có, 

121
00:07:50,264 --> 00:07:52,540
một loại mục tiêu với cấu trúc phân lớp như thế này.

122
00:07:53,160 --> 00:07:56,656
Hơn nữa, bạn có thể tưởng tượng việc phát hiện các cạnh và mẫu như thế 

123
00:07:56,656 --> 00:08:00,300
này sẽ thực sự hữu ích như thế nào cho các tác vụ nhận dạng hình ảnh khác.

124
00:08:00,880 --> 00:08:03,028
Và thậm chí ngoài khả năng nhận dạng hình ảnh, 

125
00:08:03,028 --> 00:08:06,137
có tất cả những thứ thông minh mà bạn có thể muốn thực hiện để chia 

126
00:08:06,137 --> 00:08:07,280
thành các lớp trừu tượng.

127
00:08:08,040 --> 00:08:11,974
Ví dụ: phân tích lời nói bao gồm việc lấy âm thanh thô và chọn ra các âm 

128
00:08:11,974 --> 00:08:16,502
thanh riêng biệt, kết hợp để tạo ra các âm tiết nhất định, kết hợp để tạo thành từ, 

129
00:08:16,502 --> 00:08:20,060
kết hợp để tạo thành cụm từ và những suy nghĩ trừu tượng hơn, v.v.

130
00:08:21,100 --> 00:08:24,712
Nhưng quay lại cách thức hoạt động của bất kỳ thứ nào trong số này thực sự hoạt động, 

131
00:08:24,712 --> 00:08:27,652
hãy hình dung ngay bây giờ chính bạn đang thiết kế cách chính xác các 

132
00:08:27,652 --> 00:08:29,920
kích hoạt trong một lớp có thể xác định lớp tiếp theo.

133
00:08:30,860 --> 00:08:34,778
Mục tiêu là có một số cơ chế có thể kết hợp các pixel thành các cạnh 

134
00:08:34,778 --> 00:08:38,980
hoặc các cạnh thành các mẫu hoặc các mẫu thành các chữ số một cách hợp lý.

135
00:08:39,440 --> 00:08:44,991
Và để phóng to một ví dụ rất cụ thể, giả sử hy vọng là một nơ-ron cụ thể 

136
00:08:44,991 --> 00:08:50,620
ở lớp thứ hai sẽ xác định xem hình ảnh có cạnh ở vùng này ở đây hay không.

137
00:08:51,440 --> 00:08:55,100
Câu hỏi đặt ra là mạng nên có những thông số nào?

138
00:08:55,640 --> 00:08:59,591
Bạn có thể điều chỉnh các mặt số và nút xoay nào sao cho đủ biểu cảm 

139
00:08:59,591 --> 00:09:03,656
để có thể nắm bắt được mẫu này hoặc bất kỳ mẫu pixel nào khác hoặc mẫu 

140
00:09:03,656 --> 00:09:07,780
mà một số cạnh có thể tạo thành một vòng lặp và những thứ tương tự khác?

141
00:09:08,720 --> 00:09:12,169
Chà, những gì chúng ta sẽ làm là gán trọng số cho từng kết 

142
00:09:12,169 --> 00:09:15,560
nối giữa nơ-ron của chúng ta và các nơ-ron ở lớp đầu tiên.

143
00:09:16,320 --> 00:09:17,700
Những trọng số này chỉ là những con số.

144
00:09:18,540 --> 00:09:21,920
Sau đó lấy tất cả các kích hoạt đó từ lớp đầu tiên 

145
00:09:21,920 --> 00:09:25,500
và tính tổng trọng số của chúng theo các trọng số này.

146
00:09:27,700 --> 00:09:31,196
Tôi thấy hữu ích khi coi các trọng số này được sắp xếp thành một lưới nhỏ 

147
00:09:31,196 --> 00:09:34,598
của riêng chúng và tôi sẽ sử dụng các pixel màu xanh lá cây để biểu thị 

148
00:09:34,598 --> 00:09:37,811
các trọng số dương và các pixel màu đỏ để biểu thị các trọng số âm, 

149
00:09:37,811 --> 00:09:41,780
trong đó độ sáng của pixel đó là một chút mô tả lỏng lẻo về giá trị của trọng lượng.

150
00:09:42,780 --> 00:09:46,552
Bây giờ, nếu chúng ta đặt các trọng số được liên kết với hầu hết tất cả các 

151
00:09:46,552 --> 00:09:50,622
pixel bằng 0 ngoại trừ một số trọng số dương trong vùng mà chúng ta quan tâm này, 

152
00:09:50,622 --> 00:09:54,246
thì việc lấy tổng trọng số của tất cả các giá trị pixel thực sự chỉ bằng 

153
00:09:54,246 --> 00:09:57,820
việc cộng các giá trị của pixel chỉ trong khu vực mà chúng tôi quan tâm.

154
00:09:59,140 --> 00:10:02,426
Và nếu bạn thực sự muốn biết liệu có cạnh nào ở đây hay không, 

155
00:10:02,426 --> 00:10:06,600
điều bạn có thể làm là có một số trọng số âm liên quan đến các pixel xung quanh.

156
00:10:07,480 --> 00:10:12,700
Khi đó tổng lớn nhất khi các pixel ở giữa đó sáng nhưng các pixel xung quanh tối hơn.

157
00:10:14,260 --> 00:10:18,483
Khi bạn tính tổng có trọng số như thế này, bạn có thể đưa ra bất kỳ số nào, 

158
00:10:18,483 --> 00:10:23,095
nhưng đối với mạng này, điều chúng tôi muốn là kích hoạt phải có giá trị nào đó từ 

159
00:10:23,095 --> 00:10:23,540
0 đến 1.

160
00:10:24,120 --> 00:10:28,066
Vì vậy, một điều thông thường cần làm là bơm tổng có trọng số 

161
00:10:28,066 --> 00:10:32,140
này vào một hàm nào đó để ép trục số thực vào khoảng từ 0 đến 1.

162
00:10:32,460 --> 00:10:35,634
Và một hàm phổ biến thực hiện điều này được gọi là hàm sigmoid, 

163
00:10:35,634 --> 00:10:37,420
còn được gọi là đường cong logistic.

164
00:10:38,000 --> 00:10:41,344
Về cơ bản, đầu vào rất âm có giá trị gần bằng 0, 

165
00:10:41,344 --> 00:10:46,600
đầu vào dương có giá trị gần bằng 1 và chỉ tăng đều đặn xung quanh đầu vào 0.

166
00:10:49,120 --> 00:10:52,703
Vì vậy, việc kích hoạt nơ-ron ở đây về cơ bản là 

167
00:10:52,703 --> 00:10:56,360
thước đo mức độ dương của tổng trọng số liên quan.

168
00:10:57,540 --> 00:11:01,880
Nhưng có lẽ không phải là bạn muốn nơ-ron sáng lên khi tổng trọng số lớn hơn 0.

169
00:11:02,280 --> 00:11:06,360
Có thể bạn chỉ muốn nó hoạt động khi tổng lớn hơn 10.

170
00:11:06,840 --> 00:11:10,260
Tức là bạn muốn có một số thành kiến để nó không hoạt động.

171
00:11:11,380 --> 00:11:15,375
Những gì chúng ta sẽ làm sau đó chỉ là thêm một số số khác như âm 10 

172
00:11:15,375 --> 00:11:19,660
vào tổng có trọng số này trước khi cắm nó vào hàm sigmoid squishification.

173
00:11:20,580 --> 00:11:22,440
Con số bổ sung đó được gọi là độ lệch.

174
00:11:23,460 --> 00:11:27,306
Vì vậy, các trọng số cho bạn biết mô hình pixel mà nơ-ron này ở 

175
00:11:27,306 --> 00:11:31,153
lớp thứ hai đang chọn và độ lệch cho bạn biết tổng trọng số cần 

176
00:11:31,153 --> 00:11:35,180
phải cao đến mức nào trước khi nơ-ron bắt đầu hoạt động có ý nghĩa.

177
00:11:36,120 --> 00:11:37,680
Và đó chỉ là một tế bào thần kinh.

178
00:11:38,280 --> 00:11:44,571
Mọi nơ-ron khác trong lớp này sẽ được kết nối với tất cả các nơ-ron 784 pixel từ 

179
00:11:44,571 --> 00:11:50,940
lớp đầu tiên và mỗi một trong số 784 kết nối đó có trọng số riêng gắn liền với nó.

180
00:11:51,600 --> 00:11:54,467
Ngoài ra, mỗi số có một số độ lệch, một số số khác mà 

181
00:11:54,467 --> 00:11:57,600
bạn cộng vào tổng có trọng số trước khi ép nó bằng sigmoid.

182
00:11:58,110 --> 00:11:59,540
Và đó là rất nhiều điều để suy nghĩ!

183
00:11:59,960 --> 00:12:06,017
Với lớp ẩn gồm 16 nơ-ron này, tức là có tổng cộng 784 lần 16 trọng số, 

184
00:12:06,017 --> 00:12:07,980
cùng với 16 thành kiến.

185
00:12:08,840 --> 00:12:11,940
Và tất cả những điều đó chỉ là sự kết nối từ lớp thứ nhất đến lớp thứ hai.

186
00:12:12,520 --> 00:12:17,340
Các kết nối giữa các lớp khác cũng có nhiều trọng số và độ lệch liên quan đến chúng.

187
00:12:18,340 --> 00:12:23,800
Nói chung, mạng này có gần như chính xác tổng cộng 13.000 trọng số và độ lệch.

188
00:12:23,800 --> 00:12:26,880
13.000 nút bấm và mặt số có thể được điều chỉnh và xoay 

189
00:12:26,880 --> 00:12:29,960
để làm cho mạng này hoạt động theo những cách khác nhau.

190
00:12:31,040 --> 00:12:36,200
Vì vậy, khi chúng ta nói về việc học, điều được đề cập đến là làm cho máy tính tìm ra một 

191
00:12:36,200 --> 00:12:41,360
cài đặt hợp lệ cho tất cả những con số này để nó thực sự giải quyết được vấn đề trước mắt.

192
00:12:42,620 --> 00:12:47,117
Một thử nghiệm tưởng tượng vừa thú vị vừa hơi đáng sợ là hãy tưởng tượng bạn 

193
00:12:47,117 --> 00:12:51,147
ngồi xuống và thiết lập tất cả các trọng số và độ lệch này bằng tay, 

194
00:12:51,147 --> 00:12:56,112
cố tình điều chỉnh các con số để lớp thứ hai chọn các cạnh, lớp thứ ba chọn các mẫu, 

195
00:12:56,112 --> 00:12:56,580
vân vân.

196
00:12:56,980 --> 00:13:01,526
Cá nhân tôi thấy điều này thỏa mãn thay vì chỉ coi mạng như một hộp đen hoàn toàn, 

197
00:13:01,526 --> 00:13:04,539
bởi vì khi mạng không hoạt động theo cách bạn dự đoán, 

198
00:13:04,539 --> 00:13:08,702
nếu bạn đã xây dựng được một chút mối quan hệ với ý nghĩa thực sự của những 

199
00:13:08,702 --> 00:13:12,974
trọng số và thành kiến đó. , bạn có điểm khởi đầu để thử nghiệm cách thay đổi 

200
00:13:12,974 --> 00:13:14,180
cấu trúc để cải thiện.

201
00:13:14,960 --> 00:13:18,359
Hoặc khi mạng hoạt động nhưng không phải vì những lý do bạn có thể mong đợi, 

202
00:13:18,359 --> 00:13:22,023
việc tìm hiểu xem trọng số và thành kiến đang làm gì là một cách hay để thách thức 

203
00:13:22,023 --> 00:13:25,820
các giả định của bạn và thực sự khám phá toàn bộ không gian của các giải pháp khả thi.

204
00:13:26,840 --> 00:13:30,680
Nhân tiện, chức năng thực tế ở đây hơi phức tạp khi viết ra, bạn có nghĩ vậy không?

205
00:13:32,500 --> 00:13:37,140
Vì vậy, hãy để tôi chỉ cho bạn một cách biểu diễn các kết nối này một cách nhỏ gọn hơn.

206
00:13:37,660 --> 00:13:40,520
Đây là cách bạn sẽ thấy nếu bạn chọn đọc thêm về mạng lưới thần kinh.

207
00:13:41,380 --> 00:13:49,348
Sắp xếp tất cả các kích hoạt từ một lớp vào một cột dưới dạng ma trận 

208
00:13:49,348 --> 00:13:58,000
tương ứng với các kết nối giữa một lớp và một nơ-ron cụ thể ở lớp tiếp theo.

209
00:13:58,540 --> 00:14:02,194
Điều đó có nghĩa là việc lấy tổng có trọng số của các lần kích hoạt 

210
00:14:02,194 --> 00:14:05,902
trong lớp đầu tiên theo các trọng số này tương ứng với một trong các 

211
00:14:05,902 --> 00:14:09,880
số hạng trong tích vectơ ma trận của mọi thứ chúng ta có ở bên trái ở đây.

212
00:14:14,000 --> 00:14:18,669
Nhân tiện, phần lớn việc học máy đều phụ thuộc vào việc nắm bắt tốt đại số tuyến tính, 

213
00:14:18,669 --> 00:14:22,373
vì vậy đối với bất kỳ ai trong số các bạn muốn hiểu rõ về ma trận và 

214
00:14:22,373 --> 00:14:25,916
ý nghĩa của phép nhân vectơ ma trận, hãy xem loạt bài tôi đã thực 

215
00:14:25,916 --> 00:14:28,600
hiện trên đại số tuyến tính, đặc biệt là chương 3.

216
00:14:29,240 --> 00:14:33,593
Quay lại biểu thức của chúng ta, thay vì nói về việc thêm độ lệch cho từng giá 

217
00:14:33,593 --> 00:14:38,001
trị này một cách độc lập, chúng ta biểu diễn nó bằng cách tổ chức tất cả các độ 

218
00:14:38,001 --> 00:14:42,300
lệch đó thành một vectơ và thêm toàn bộ vectơ vào tích vectơ ma trận trước đó.

219
00:14:43,280 --> 00:14:47,019
Sau đó, bước cuối cùng, tôi sẽ bọc một sigmoid xung quanh bên 

220
00:14:47,019 --> 00:14:50,698
ngoài ở đây và điều được cho là thể hiện rằng bạn sẽ áp dụng 

221
00:14:50,698 --> 00:14:54,740
hàm sigmoid cho từng thành phần cụ thể của vectơ kết quả bên trong.

222
00:14:55,940 --> 00:15:00,272
Vì vậy, khi bạn viết ma trận trọng số này và các vectơ này làm ký hiệu riêng, 

223
00:15:00,272 --> 00:15:05,050
bạn có thể truyền đạt toàn bộ quá trình chuyển đổi kích hoạt từ lớp này sang lớp tiếp 

224
00:15:05,050 --> 00:15:07,994
theo bằng một biểu thức cực kỳ chặt chẽ và gọn gàng, 

225
00:15:07,994 --> 00:15:12,882
và điều này làm cho mã liên quan trở nên đơn giản hơn rất nhiều và nhanh hơn rất nhiều, 

226
00:15:12,882 --> 00:15:15,660
vì nhiều thư viện đã tối ưu hóa phép nhân ma trận.

227
00:15:17,820 --> 00:15:19,605
Hãy nhớ trước đó tôi đã nói những tế bào thần kinh 

228
00:15:19,605 --> 00:15:21,460
này chỉ đơn giản là những thứ chứa đựng những con số?

229
00:15:22,220 --> 00:15:28,155
Tất nhiên, những con số cụ thể mà chúng chứa phụ thuộc vào hình ảnh bạn đưa vào, 

230
00:15:28,155 --> 00:15:32,258
vì vậy sẽ chính xác hơn khi coi mỗi nơ-ron như một hàm, 

231
00:15:32,258 --> 00:15:38,340
một hàm nhận đầu ra của tất cả các nơ-ron ở lớp trước và tạo ra một số giữa 0 và 1.

232
00:15:39,200 --> 00:15:43,019
Thực sự toàn bộ mạng chỉ là một chức năng, một chức 

233
00:15:43,019 --> 00:15:47,060
năng lấy 784 số làm đầu vào và đưa ra 10 số làm đầu ra.

234
00:15:47,560 --> 00:15:52,163
Đó là một hàm phức tạp đến mức vô lý, một hàm bao gồm 13.000 tham số dưới 

235
00:15:52,163 --> 00:15:56,767
dạng các trọng số và độ lệch theo các mẫu nhất định và liên quan đến việc 

236
00:15:56,767 --> 00:16:02,367
lặp lại nhiều tích vectơ ma trận và hàm nén sigmoid, nhưng dù sao nó cũng chỉ là một hàm, 

237
00:16:02,367 --> 00:16:06,660
và trong một theo cách đó thì có vẻ yên tâm hơn vì nó trông phức tạp.

238
00:16:07,340 --> 00:16:09,849
Ý tôi là nếu nó đơn giản hơn chút nữa, chúng ta còn hy vọng gì 

239
00:16:09,849 --> 00:16:12,280
nữa rằng nó có thể đương đầu với thách thức nhận dạng chữ số?

240
00:16:13,340 --> 00:16:14,700
Và nó thực hiện thử thách đó như thế nào?

241
00:16:15,080 --> 00:16:17,220
Làm cách nào để mạng này tìm hiểu các trọng số 

242
00:16:17,220 --> 00:16:19,360
và độ lệch thích hợp chỉ bằng cách xem dữ liệu?

243
00:16:20,140 --> 00:16:23,149
Đó là những gì tôi sẽ trình bày trong video tiếp theo và tôi cũng sẽ tìm hiểu 

244
00:16:23,149 --> 00:16:26,120
thêm một chút về hoạt động thực sự của mạng cụ thể mà chúng ta đang thấy này.

245
00:16:27,580 --> 00:16:30,843
Bây giờ là lúc tôi nghĩ mình nên đăng ký để được thông báo khi có 

246
00:16:30,843 --> 00:16:34,008
video hoặc bất kỳ video mới nào xuất hiện, nhưng thực tế là hầu 

247
00:16:34,008 --> 00:16:37,420
hết các bạn không thực sự nhận được thông báo từ YouTube, phải không?

248
00:16:38,020 --> 00:16:41,175
Có lẽ thành thật mà nói hơn, tôi nên nói là đăng ký để các mạng 

249
00:16:41,175 --> 00:16:44,429
lưới thần kinh làm nền tảng cho thuật toán đề xuất của YouTube có 

250
00:16:44,429 --> 00:16:47,880
cơ sở tin rằng bạn muốn xem nội dung từ kênh này được đề xuất cho bạn.

251
00:16:48,560 --> 00:16:49,940
Dù sao hãy đăng để biết thêm.

252
00:16:50,760 --> 00:16:53,500
Cảm ơn mọi người rất nhiều vì đã ủng hộ những video này trên Patreon.

253
00:16:54,000 --> 00:16:56,707
Tôi tiến triển hơi chậm trong chuỗi xác suất vào mùa hè này, 

254
00:16:56,707 --> 00:17:00,657
nhưng tôi sẽ quay lại với nó sau dự án này, vì vậy những khách hàng quen có thể theo dõi 

255
00:17:00,657 --> 00:17:01,900
các thông tin cập nhật ở đó.

256
00:17:03,600 --> 00:17:05,822
Để kết thúc mọi chuyện ở đây, tôi có Leisha Lee, 

257
00:17:05,822 --> 00:17:09,450
người đã làm luận án tiến sĩ về mặt lý thuyết của học sâu và hiện đang làm việc 

258
00:17:09,450 --> 00:17:12,035
tại một công ty đầu tư mạo hiểm tên là Amplify Partners, 

259
00:17:12,035 --> 00:17:14,619
người đã vui lòng cung cấp một số kinh phí cho video này.

260
00:17:15,460 --> 00:17:19,119
Vì vậy, Leisha có một điều tôi nghĩ chúng ta nên nhanh chóng đưa ra là hàm sigmoid này.

261
00:17:19,700 --> 00:17:23,138
Theo tôi hiểu, các mạng ban đầu sử dụng điều này để gộp tổng trọng số có liên 

262
00:17:23,138 --> 00:17:26,445
quan vào khoảng giữa 0 và 1, bạn biết đấy, loại được thúc đẩy bởi sự tương 

263
00:17:26,445 --> 00:17:29,840
tự sinh học này của các tế bào thần kinh không hoạt động hoặc đang hoạt động.

264
00:17:30,280 --> 00:17:30,300
Chính xác.

265
00:17:30,560 --> 00:17:34,040
Nhưng tương đối ít mạng hiện đại thực sự sử dụng sigmoid nữa.

266
00:17:34,320 --> 00:17:34,320
Vâng.

267
00:17:34,440 --> 00:17:35,540
Đó là loại trường học cũ phải không?

268
00:17:35,760 --> 00:17:38,980
Vâng hay đúng hơn là relu có vẻ dễ huấn luyện hơn nhiều.

269
00:17:39,400 --> 00:17:42,340
Và relu là viết tắt của đơn vị tuyến tính chỉnh lưu?

270
00:17:42,680 --> 00:17:48,312
Đúng, đây là loại chức năng trong đó bạn chỉ lấy giá trị tối đa bằng 0 và trong đó a được 

271
00:17:48,312 --> 00:17:53,693
đưa ra bởi những gì bạn đang giải thích trong video và điều này được thúc đẩy từ đâu. 

272
00:17:53,693 --> 00:17:59,200
Tôi nghĩ một phần là do sự tương tự sinh học với cách các tế bào thần kinh sẽ được kích 

273
00:17:59,200 --> 00:18:04,832
hoạt hay không và vì vậy nếu nó vượt qua một ngưỡng nhất định thì đó sẽ là chức năng nhận 

274
00:18:04,832 --> 00:18:10,276
dạng nhưng nếu không thì nó sẽ không được kích hoạt nên sẽ bằng 0 nên đó là một sự đơn 

275
00:18:10,276 --> 00:18:10,840
giản hóa.

276
00:18:11,160 --> 00:18:15,559
Việc sử dụng sigmoid không giúp ích gì cho việc đào tạo hoặc ở một 

277
00:18:15,559 --> 00:18:20,155
thời điểm nào đó rất khó đào tạo và mọi người chỉ thử relu và nó tình 

278
00:18:20,155 --> 00:18:24,620
cờ hoạt động rất tốt đối với các mạng lưới thần kinh cực kỳ sâu này.

279
00:18:25,100 --> 00:18:25,640
Được rồi, cảm ơn Alicia.

