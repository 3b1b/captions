1
00:00:04,020 --> 00:00:10,680
นี่คือเลขสาม เขียนอย่างลวกๆ และแสดงผลที่ความละเอียดต่ำที่ 28 x 28 pixel

2
00:00:10,680 --> 00:00:15,660
แต่ก็ไม่ใช่ปัญหาที่สมองคุณจะเห็นว่ามันเป็นเลขสาม ผมอยากให้คุณตระหนักถึงคุณค่า

3
00:00:15,900 --> 00:00:18,940
ไม่น่าเชื่อว่าสมองเราสามารถทำมันได้อย่างง่ายดาย

4
00:00:18,940 --> 00:00:23,160
อันนี้ อันนี้ แล้วก็อันนี้ ก็ยังเห็นว่าเป็นเลขสาม

5
00:00:23,160 --> 00:00:28,060
แม้ว่าข้อมูลในแต่ละpixelในแต่ละรูปจะไม่เหมือนกันก็ตาม

6
00:00:28,080 --> 00:00:33,780
เซลล์ที่ไวต่อแสงในดวงตาของคุณส่งสัญญาณเมื่อคุณเห็นเลขสามนี้

7
00:00:33,780 --> 00:00:36,800
ก็เป็นคนละอันกับที่ส่งสัญญาณเมื่อคุณเห็นเลขสามรูปนี้

8
00:00:37,140 --> 00:00:40,610
แต่สิ่งที่อยู่ในvisual cortex ที่ฉลาดมากของคุณ

9
00:00:41,129 --> 00:00:48,139
สามารถแก้ปัญหาเหล่านี้เพื่อแสดงถึงแนวคิดเดียวกันในขณะที่จดจำภาพอื่น ๆ เป็นความคิดที่แตกต่างออกไป

10
00:00:48,840 --> 00:00:55,039
แต่ถ้าผมบอกคุณ ให้นั่งลงและเขียนโปรแกรมให้ผม โดยใช้ข้อมูลจากตารางจาก 28 x 28 pixels

11
00:00:55,379 --> 00:01:01,759
เช่นนี้และคืนตัวเลขเดียวระหว่าง 0 ถึง 10 บอกคุณว่ามันคิดว่ามันเป็นตัวเลขอะไร

12
00:01:02,250 --> 00:01:06,139
งานนี้จากตลกง่ายดายกลายเป็นยากเย็นแสนเข็ญ

13
00:01:06,750 --> 00:01:08,270
นอกจากคุณอยู่ใต้หิน

14
00:01:08,270 --> 00:01:14,599
ผมคิดว่าผมแทบจะไม่จำเป็นต้องพูดถึงความเกี่ยวข้องและความสำคัญของ Machine Learning และ Neural Networkในปัจจุบันต่อไปในอนาคต

15
00:01:14,640 --> 00:01:18,410
แต่สิ่งที่ผมต้องการจะทำคือแสดงให้คุณเห็นว่า neural network มันคืออะไรกันแน่

16
00:01:18,660 --> 00:01:24,229
สมมติว่าไม่มีความรู้พื้นหลังและให้ช่วยเห็นภาพว่ามันทำอะไร ไม่ใช่buzzword แต่ราวกับเป็นส่วนหนึ่งของคณิตศาสตร์

17
00:01:24,570 --> 00:01:28,310
ผมหวังว่าคุณจะออกมารู้สึกเหมือนโครงสร้างนี้เอง

18
00:01:28,380 --> 00:01:34,399
มีแรงบันดาลใจและรู้สึกเหมือนคุณรู้ว่ามันหมายถึงอะไรเมื่อคุณอ่านหรือได้ยินเกี่ยวกับ neural network

19
00:01:34,950 --> 00:01:40,249
วิดีโอนี้เป็นจะทุ่มเทให้กับองค์ประกอบโครงสร้างของมันและอันต่อไปเป็นไปเพื่อจัดการกับการเรียนรู้

20
00:01:40,530 --> 00:01:45,950
สิ่งที่เรากำลังจะทำคือใส่กัน neural network ที่สามารถเรียนรู้ที่จะอ่านเลขที่เขียนด้วยลายมือ

21
00:01:49,270 --> 00:01:51,329
นี่เป็นตัวอย่างคลาสสิกสำหรับ

22
00:01:51,520 --> 00:01:56,759
แนะนำหัวข้อนี้และผมก็ยินดีที่จะปฏิบัติตามเดิมที่นี่เพราะในตอนท้ายของวิดีโอทั้งสองแบบที่ฉันต้องการชี้

23
00:01:56,760 --> 00:02:02,099
ไปยังแหล่งข้อมูลที่ดีที่คุณสามารถเรียนรู้ได้เพิ่มเติมและที่ใดที่คุณสามารถดาวน์โหลดโค้ดที่ทำแบบนี้และเล่นกับมันได้

24
00:02:02,100 --> 00:02:04,100
บนคอมพิวเตอร์ของคุณเอง

25
00:02:04,750 --> 00:02:08,970
มันมี neural network หลาย หลายรูปแบบ และในช่วงไม่กี่ปีที่ผ่านมา

26
00:02:08,970 --> 00:02:11,970
มีบูมในการค้นคว้าวิจัยเกี่ยวกับรูปแบบเหล่านี้

27
00:02:12,130 --> 00:02:19,019
แต่ในวิดีโอแนะนำทั้งสองแบบนี้เราจะมองไปที่รูปแบบวานิลลาธรรมดาเรียบง่ายที่ไม่มีการแต่งเติม

28
00:02:19,300 --> 00:02:21,040
มันค่อนข้างเป็นสิ่งที่จำเป็น

29
00:02:21,040 --> 00:02:24,510
ก่อนการทำความเข้าใจรูปแบบที่ทันสมัยและมีประสิทธิภาพ

30
00:02:24,760 --> 00:02:28,199
เชื่อผม มันยังมีความซับซ้อนมากมายสำหรับเราที่จะต้องเข้าใจ

31
00:02:28,690 --> 00:02:32,820
แม้ในรูปแบบที่ง่ายที่สุดนี้ก็สามารถเรียนรู้เพื่อจดจำตัวเลขที่เขียนด้วยลายมือ

32
00:02:32,820 --> 00:02:36,180
ซึ่งเป็นความสามารถที่ค่อนข้างเจ๋งสำหรับคอมพิวเตอร์

33
00:02:37,120 --> 00:02:41,960
และในเวลาเดียวกันคุณจะเห็นว่ามันขาดอะไรจากความคาดหมายที่เราตั้งไว้

34
00:02:43,090 --> 00:02:48,179
เป็นชื่อที่เกริ่นไว้ neural network เครือข่ายประสาทได้รับแรงบันดาลใจจากสมอง  เรามาดูกัน

35
00:02:48,520 --> 00:02:51,389
อะไรคือเซลล์ประสาทและพวกเขาเชื่อมโยงกันอย่างไร?

36
00:02:52,090 --> 00:02:57,750
ตอนนี้เมื่อผมพูดเซลล์ประสาท ผมต้องการให้คุณคิดเป็นสิ่งที่เก็บตัวเลข

37
00:02:58,209 --> 00:03:02,129
โดยเฉพาะตัวเลขระหว่าง 0 ถึง 1 จะไม่มากไปกว่านั้น

38
00:03:03,430 --> 00:03:11,130
ตัวอย่างเช่นเครือข่ายเริ่มต้นด้วยเซลล์ประสาทที่มาจาก 28 x 28 pixel ของภาพที่นำเข้า

39
00:03:11,400 --> 00:03:12,460
ซึ่งคือ

40
00:03:12,460 --> 00:03:20,240
784 เซลล์ประสาททั้งหมดในแต่ละเซลล์มีจำนวนที่แสดงถึงค่า grayscale ของpixelที่ตรงกัน

41
00:03:20,769 --> 00:03:24,299
ตั้งแต่ 0 สำหรับ pixel สีดำถึง 1 สำหรับ pixel สีขาว

42
00:03:24,910 --> 00:03:30,419
เลขที่อยู่ภายใน neuron เรียกว่า activation คุณอาจจะนึกภาพตามได้ว่า

43
00:03:30,420 --> 00:03:33,959
neuron แต่ละอันจะสว่างขึ้นเมื่อเลข activation นั้นสูง

44
00:03:36,260 --> 00:03:41,559
neuron ทั้ง 784 อันรวมกันเป็น layer แรกใน network ของเรา

45
00:03:45,990 --> 00:03:51,289
ข้ามไปที่ layer สุดท้าย ซึ่งมีทั้งหมดสิบเ neuron แต่ละอันแทนเลขหนึ่งตัว

46
00:03:51,570 --> 00:03:56,239
เลข activation ใน neuron เหล่านี้ อีกครั้งเลขที่อยู่ระหว่างศูนย์และหนึ่ง

47
00:03:56,880 --> 00:04:00,049
หมายถึง ค่าที่ระบบคิดว่าภาพที่เห็น

48
00:04:00,720 --> 00:04:05,990
ตรงกับตัวเลขที่ระบุ นอกจากนี้ยังมี layer อีกสอง layer ตรงกลางที่เรียกว่า hidden layer

49
00:04:06,180 --> 00:04:07,770
ซึ่งในขณะนี้

50
00:04:07,770 --> 00:04:13,549
ควรเป็นแค่เครื่องหมายคำถามยักษ์ ทดแทนวิธีการใดในโลกนี้ ที่จะจัดการกับวิธีการจดจำตัวเลข

51
00:04:13,740 --> 00:04:20,209
ใน network นี้ผมเลือก hidden layer ไว้สองชั้นซึ่งมี 16 neuron และยอมรับว่าค่อนข้างเป็นทางเลือกที่ไม่มีเหตุผล

52
00:04:20,609 --> 00:04:24,889
ในความเป็นจริงผมเลือกสอง layer ขึ้นอยู่กับว่าวิธีที่ผมต้องการจะกระตุ้นโครงสร้างในสักครู่

53
00:04:25,350 --> 00:04:29,179
และ 16 มันเป็นเพียงตัวเลขที่ดีเพื่อให้พอดีกับบนหน้าจอ ในทางปฏิบัติ

54
00:04:29,180 --> 00:04:32,209
ยังมีพื้นที่จำนวนมากสำหรับการทดลองกับการโครงสร้าง

55
00:04:32,730 --> 00:04:38,329
วิธีที่เครือข่ายเปิดใช้งานการเปิดใช้งานในชั้นเดียวจะเป็นตัวกำหนดการเปิดใช้งานของเลเยอร์ถัดไป

56
00:04:38,760 --> 00:04:45,349
และแน่นอนว่าหัวใจของเครือข่ายเป็นกลไกในการประมวลผลข้อมูลจะลดลงอย่างแน่นอน

57
00:04:45,570 --> 00:04:48,409
การเปิดใช้งานจากเลเยอร์จะทำให้เกิดการเปิดใช้งานในชั้นถัดไป

58
00:04:48,900 --> 00:04:54,859
มันหมายถึงการหลวมคล้ายคลึงกับวิธีการในเครือข่ายทางชีววิทยาของเซลล์ประสาทบางกลุ่มของเซลล์ประสาทยิง

59
00:04:55,410 --> 00:04:57,410
ทำให้คนอื่นบางคนดับเพลิง

60
00:04:57,570 --> 00:04:58,340
ตอนนี้เครือข่าย

61
00:04:58,340 --> 00:05:03,019
ฉันกำลังแสดงอยู่ที่นี่ได้รับการฝึกฝนให้จดจำตัวเลขแล้วแจ้งให้ฉันทราบว่าฉันหมายถึงอะไร

62
00:05:03,140 --> 00:05:06,580
นั่นหมายความว่าถ้าคุณให้อาหารภาพแสงทั้งหมด

63
00:05:06,640 --> 00:05:11,780
784 เซลล์ประสาทของชั้นข้อมูลเข้าตามความสว่างของแต่ละพิกเซลในภาพ

64
00:05:12,330 --> 00:05:17,029
รูปแบบการเปิดใช้งานทำให้รูปแบบที่เฉพาะเจาะจงบางอย่างในชั้นถัดไป

65
00:05:17,190 --> 00:05:19,309
ซึ่งทำให้รูปแบบบางอย่างในหนึ่งหลังจากนั้นหรือไม่

66
00:05:19,440 --> 00:05:22,190
ซึ่งสุดท้ายให้รูปแบบในชั้นส่งออกและ?

67
00:05:22,350 --> 00:05:29,359
เซลล์ประสาทที่สว่างที่สุดของชั้นส่งออกนั้นเป็นทางเลือกของเครือข่ายเพื่อที่จะพูดว่ารูปภาพนี้เป็นภาพอะไร?

68
00:05:32,070 --> 00:05:36,859
และก่อนที่จะกระโดดลงไปในวิชาคณิตศาสตร์สำหรับวิธีการหนึ่งชั้นมีอิทธิพลต่อการฝึกอบรมต่อไปหรืออย่างไร?

69
00:05:37,140 --> 00:05:43,069
ลองพูดถึงเหตุผลว่าทำไมถึงสมควรที่จะคาดหวังให้โครงสร้างแบบเลเยอร์แบบนี้ทำงานได้อย่างชาญฉลาด

70
00:05:43,800 --> 00:05:48,260
เราคาดหวังอะไรที่นี่? อะไรคือความหวังที่ดีที่สุดสำหรับสิ่งที่ชั้นกลางเหล่านี้อาจจะทำ?

71
00:05:48,860 --> 00:05:56,720
เมื่อคุณหรือฉันรู้จักตัวเลขที่เรารวบรวมส่วนประกอบต่างๆไว้เก้าชิ้นจะมีวงแหวนขึ้นด้านบนและมีสายด้านขวา

72
00:05:57,260 --> 00:06:01,280
8 ยังมีวงขึ้นด้านบน แต่ก็จับคู่กับอีกหนึ่งวงต่ำลง

73
00:06:02,020 --> 00:06:06,599
A 4 โดยทั่วไปแบ่งออกเป็นสามบรรทัดที่เฉพาะเจาะจงและสิ่งที่ต้องการที่

74
00:06:07,180 --> 00:06:11,970
ตอนนี้ในโลกที่สมบูรณ์แบบเราอาจหวังว่าแต่ละเซลล์ประสาทจะอยู่ในชั้นที่สองถึงชั้นสุดท้าย

75
00:06:12,640 --> 00:06:14,729
ตรงกับคอมโพเนนต์ย่อยเหล่านี้

76
00:06:14,890 --> 00:06:19,740
ทุกครั้งที่คุณป้อนข้อมูลในรูปด้วยการพูดว่าห่วงขึ้นด้านบนเช่น 9 หรือ 8

77
00:06:19,870 --> 00:06:21,220
มีบางอย่างที่เฉพาะเจาะจง

78
00:06:21,220 --> 00:06:27,749
Neuron ที่มีการเปิดใช้งานกำลังจะใกล้เคียงกับหนึ่งและฉันไม่ได้หมายถึงวงพิกเซลที่เฉพาะเจาะจงนี้หวังว่าจะเป็นที่ใด

79
00:06:28,090 --> 00:06:35,039
รูปแบบ loopy โดยทั่วไปไปทางด้านบนจะตั้งเซลล์ประสาทชนิดนี้ออกจากชั้นที่ 3 เป็นครั้งสุดท้าย

80
00:06:35,380 --> 00:06:39,960
เพียงต้องการการเรียนรู้ซึ่งการรวมส่วนประกอบย่อยสอดคล้องกับตัวเลขใด

81
00:06:40,510 --> 00:06:42,810
แน่นอนว่าเพียงแค่เตะปัญหาลงที่ถนน

82
00:06:42,910 --> 00:06:49,019
เนื่องจากคุณจะจำแนกองค์ประกอบย่อยเหล่านี้ได้อย่างไรหรือเรียนรู้ว่าองค์ประกอบย่อยที่ถูกต้องควรเป็นอย่างไรและฉันยังไม่ได้พูดถึงเรื่องนี้

83
00:06:49,020 --> 00:06:52,829
ชั้นหนึ่งมีอิทธิพลต่อสิ่งใดต่อไป แต่ให้วิ่งไปกับฉันในช่วงนี้สักครู่

84
00:06:53,650 --> 00:06:56,340
ตระหนักถึงห่วงยังสามารถแบ่งย่อยเป็นปัญหาย่อยได้

85
00:06:56,860 --> 00:07:02,550
วิธีหนึ่งที่สมเหตุสมผลในการทำเช่นนี้คือการจำแนกขอบเล็ก ๆ ต่างๆที่ทำให้เกิดขึ้นได้

86
00:07:03,520 --> 00:07:08,910
เช่นเดียวกับเส้นยาวเช่นเดียวกับที่คุณอาจเห็นในตัวเลข 1 หรือ 4 หรือ 7

87
00:07:08,910 --> 00:07:14,279
ดีที่จริงๆเพียงขอบยาวหรือบางทีคุณอาจคิดว่ามันเป็นรูปแบบบางส่วนของขอบเล็ก ๆ

88
00:07:14,740 --> 00:07:19,379
บางทีความหวังของเราก็คือเซลล์ประสาทแต่ละตัวอยู่ในชั้นที่สองของเครือข่าย

89
00:07:20,290 --> 00:07:22,650
สอดคล้องกับขอบเล็ก ๆ ที่เกี่ยวข้อง

90
00:07:23,230 --> 00:07:28,259
บางทีเมื่อภาพเช่นนี้มาในนั้นไฟขึ้นทั้งหมดของเซลล์ประสาท

91
00:07:28,720 --> 00:07:31,649
ที่เกี่ยวข้องกับรอบแปดถึงสิบเฉพาะขอบเล็ก ๆ

92
00:07:31,930 --> 00:07:36,930
ซึ่งจะหมุนเวียนเซลล์ประสาทที่เกี่ยวข้องกับวงบนและเส้นแนวตั้งยาวและ

93
00:07:37,300 --> 00:07:39,599
ผู้ที่สว่างขึ้นเซลล์ประสาทที่เกี่ยวข้องกับเก้า

94
00:07:40,300 --> 00:07:41,100
ไม่ว่าจะเป็น

95
00:07:41,100 --> 00:07:47,070
นี่คือสิ่งที่เครือข่ายสุดท้ายของเรามีอยู่จริงคือคำถามหนึ่งที่เราจะกลับมาดูเมื่อเราดูวิธีการฝึกอบรมเครือข่าย

96
00:07:47,350 --> 00:07:52,170
แต่นี่เป็นความหวังที่เราอาจมี การจัดเรียงของเป้าหมายที่มีโครงสร้างแบบชั้นเช่นนี้

97
00:07:53,020 --> 00:07:59,340
นอกจากนี้คุณสามารถจินตนาการได้ว่าการตรวจจับขอบและรูปแบบเช่นนี้จะเป็นประโยชน์อย่างแท้จริงสำหรับงานการจดจำภาพอื่น ๆ หรือไม่

98
00:07:59,740 --> 00:08:06,749
แม้กระทั่งการรับรู้ภาพจะมีทุกสิ่งที่ชาญฉลาดที่คุณอาจต้องการทำซึ่งแบ่งออกเป็นชั้นของสิ่งที่เป็นนามธรรม

99
00:08:07,690 --> 00:08:14,670
การแยกเสียงพูดเช่นการหยิบเสียงดิบและการเลือกเสียงที่แตกต่างซึ่งรวมกันเพื่อให้พยางค์บางเสียง

100
00:08:15,070 --> 00:08:19,829
ซึ่งรวมกันเป็นคำที่รวมกันเพื่อทำเป็นวลีและความคิดที่เป็นนามธรรมมากขึ้นเป็นต้น

101
00:08:20,770 --> 00:08:25,710
แต่กลับไปที่วิธีการใด ๆ นี้จริงการทำงานภาพตัวเองในขณะนี้การออกแบบ

102
00:08:25,710 --> 00:08:30,449
วิธี activations ในชั้นเดียวอาจกำหนด activations ในถัดไปหรือไม่

103
00:08:30,670 --> 00:08:35,879
เป้าหมายคือการมีกลไกบางอย่างที่น่าจะสามารถรวมพิกเซลเข้ากับขอบได้

104
00:08:35,880 --> 00:08:41,430
หรือขอบเป็นรูปแบบหรือรูปแบบเป็นตัวเลขและเพื่อซูมเข้าในตัวอย่างที่เฉพาะเจาะจงมาก

105
00:08:41,950 --> 00:08:44,189
สมมุติว่าความหวังสำหรับคนพิเศษ

106
00:08:44,380 --> 00:08:50,430
Neuron ในชั้นที่สองเพื่อรับภาพที่มีขอบในภูมิภาคนี้หรือไม่

107
00:08:50,950 --> 00:08:54,960
คำถามในมือคือสิ่งที่ควรจะเป็นเครือข่ายของเครือข่าย

108
00:08:55,270 --> 00:09:02,490
สิ่งที่หมุนและลูกบิดคุณควรจะสามารถปรับแต่งเพื่อให้มันแสดงออกมากพอที่จะสามารถจับภาพรูปแบบนี้หรือ

109
00:09:02,590 --> 00:09:07,290
รูปแบบพิกเซลอื่นหรือรูปแบบที่ขอบหลาย ๆ สามารถสร้างลูปและสิ่งอื่น ๆ ได้หรือไม่?

110
00:09:08,290 --> 00:09:15,389
ดีสิ่งที่เราจะทำคือกำหนดน้ำหนักให้กับแต่ละการเชื่อมต่อระหว่างเซลล์ประสาทและเซลล์ประสาทของเราจากชั้นแรก

111
00:09:15,850 --> 00:09:17,850
น้ำหนักเหล่านี้เป็นตัวเลขเพียงอย่างเดียว

112
00:09:18,190 --> 00:09:25,590
จากนั้นใช้การกระตุ้นทั้งหมดจากเลเยอร์แรกและคำนวณผลรวมน้ำหนักตามน้ำหนักเหล่านี้ I

113
00:09:27,370 --> 00:09:31,680
หาข้อมูลที่เป็นประโยชน์ในการพิจารณาน้ำหนักเหล่านี้ว่าเป็นการจัดเป็นตารางย่อย ๆ ของตนเอง

114
00:09:31,680 --> 00:09:37,079
และฉันจะใช้พิกเซลสีเขียวเพื่อระบุน้ำหนักบวกและพิกเซลสีแดงเพื่อระบุน้ำหนักที่เป็นค่าลบ

115
00:09:37,240 --> 00:09:41,670
ในกรณีที่ความสว่างของพิกเซลนั้นเป็นภาพของค่าน้ำหนักที่หลวม?

116
00:09:42,400 --> 00:09:45,840
ตอนนี้ถ้าเราทำน้ำหนักที่สัมพันธ์กับจุดพิกเซลเกือบทั้งหมด

117
00:09:46,150 --> 00:09:49,079
ยกเว้นน้ำหนักที่เป็นบวกบางส่วนในภูมิภาคนี้ที่เราใส่ใจ

118
00:09:49,480 --> 00:09:51,310
แล้วนำผลรวมถ่วงน้ำหนักของ

119
00:09:51,310 --> 00:09:57,690
ค่าพิกเซลทั้งหมดจริงๆเพียงจำนวนเงินที่จะเพิ่มค่าของพิกเซลเพียงในพื้นที่ที่เราใส่ใจ

120
00:09:58,870 --> 00:10:04,440
และหากคุณต้องการรับข้อมูลว่ามีขอบอยู่ตรงนี้สิ่งที่คุณควรทำคือมีน้ำหนักเชิงลบ

121
00:10:04,900 --> 00:10:06,900
เกี่ยวข้องกับพิกเซลแวดล้อม

122
00:10:07,030 --> 00:10:12,660
ยอดรวมนั้นใหญ่ที่สุดเมื่อพิกเซลกลางสว่าง แต่พิกเซลแวดล้อมมืด

123
00:10:14,279 --> 00:10:18,169
เมื่อคุณคำนวณจำนวนเงินที่ถ่วงน้ำหนักเช่นนี้คุณอาจออกมาพร้อมกับหมายเลขใดก็ได้

124
00:10:18,240 --> 00:10:23,180
แต่สำหรับเครือข่ายนี้สิ่งที่เราต้องการคือการเปิดใช้งานให้มีค่าระหว่าง 0 และ 1

125
00:10:23,730 --> 00:10:26,599
ดังนั้นสิ่งที่ต้องทำก็คือการทุ่มเงินจำนวนนี้

126
00:10:26,910 --> 00:10:32,000
เป็นฟังก์ชันบางอย่างที่ squishes เส้นจำนวนจริงในช่วงระหว่าง 0 และ 1 และ

127
00:10:32,190 --> 00:10:37,249
ฟังก์ชันทั่วไปที่เรียกว่า sigmoid function หรือที่เรียกว่า logistic curve

128
00:10:37,980 --> 00:10:43,339
ปัจจัยการผลิตเชิงลบโดยส่วนมากจะจบลงใกล้ศูนย์ปัจจัยการผลิตที่เป็นบวกมากใกล้เคียงกับ 1

129
00:10:43,339 --> 00:10:46,398
และเพิ่มขึ้นเรื่อย ๆ ประมาณ 0 อินพุท

130
00:10:49,080 --> 00:10:56,029
ดังนั้นการเปิดใช้งานของเซลล์ประสาทที่นี่เป็นพื้นวัดของวิธีการบวกบวกถ่วงน้ำหนักที่เกี่ยวข้องคือ

131
00:10:57,450 --> 00:11:01,819
แต่อาจไม่ใช่ว่าคุณต้องการให้เซลล์ประสาทสว่างขึ้นเมื่อผลรวมที่มีค่ามากกว่า 0

132
00:11:02,100 --> 00:11:06,260
บางทีคุณอาจต้องการให้ใช้งานได้เมื่อผลรวมใหญ่กว่าพูด 10

133
00:11:06,630 --> 00:11:10,279
นั่นคือคุณต้องการอคติบางอย่างที่จะไม่ใช้งาน

134
00:11:10,860 --> 00:11:16,099
สิ่งที่เราจะทำก็คือเพิ่มจำนวนอื่นเช่นลบ 10 เป็นจำนวนรวมที่ถ่วงนี้

135
00:11:16,529 --> 00:11:19,669
ก่อนที่จะเสียบผ่านฟังก์ชัน squishification sigmoid

136
00:11:20,220 --> 00:11:22,730
จำนวนเพิ่มเติมที่เรียกว่าอคติ

137
00:11:23,310 --> 00:11:29,060
ดังนั้นน้ำหนักบอกคุณว่ารูปแบบพิกเซลนี้เซลล์ประสาทในชั้นที่สองคือการรับขึ้นและอคติ

138
00:11:29,220 --> 00:11:35,450
บอกคุณว่าผลรวมที่ถ่วงน้ำหนักต้องเป็นอย่างไรก่อนที่เซลล์ประสาทจะเริ่มทำงานอย่างมีนัยสำคัญ

139
00:11:35,910 --> 00:11:37,910
และนั่นเป็นเพียงเซลล์ประสาทตัวเดียว

140
00:11:38,120 --> 00:11:41,940
เซลล์ประสาททุกตัวในชั้นนี้จะเชื่อมต่อกับทุกเซลล์

141
00:11:42,320 --> 00:11:50,620
784 พิกเซลเซลล์ประสาทจากชั้นแรกและหนึ่งในนั้น 784 การเชื่อมต่อมีน้ำหนักของตัวเองที่เกี่ยวข้องกับมัน

142
00:11:51,330 --> 00:11:57,739
แต่ละคนมีอคติบางจำนวนอื่น ๆ ที่คุณเพิ่มไปยังผลรวมถัวเฉลี่ยก่อน squishing กับ sigmoid และ

143
00:11:58,020 --> 00:12:01,909
นั่นเป็นจำนวนมากที่จะคิดเกี่ยวกับกับชั้นที่ซ่อนอยู่นี้ของ 16 เซลล์ประสาท

144
00:12:02,010 --> 00:12:08,270
นั่นคือทั้งหมด 784 ครั้ง 16 น้ำหนักพร้อมกับ 16 อคติ

145
00:12:08,490 --> 00:12:14,029
ทั้งหมดนี้เป็นเพียงการเชื่อมต่อจากเลเยอร์แรกไปจนถึงจุดเชื่อมต่อระหว่างชั้นอื่น ๆ

146
00:12:14,029 --> 00:12:17,208
นอกจากนี้ยังมีน้ำหนักและอคติที่เกี่ยวข้องอีกด้วย

147
00:12:17,760 --> 00:12:20,680
ทั้งหมดกล่าวและทำเครือข่ายนี้ได้เกือบจะแน่นอน

148
00:12:21,280 --> 00:12:23,920
น้ำหนักและอคติทั้งสิ้น 13,000

149
00:12:24,280 --> 00:12:29,540
ปุ่มหมุนและลูกบิด 13,000 ปุ่มที่สามารถปรับแต่งและหมุนเพื่อทำให้เครือข่ายนี้ทำงานได้หลายวิธี

150
00:12:30,520 --> 00:12:32,520
ดังนั้นเมื่อเราพูดถึงการเรียนรู้?

151
00:12:32,530 --> 00:12:40,199
สิ่งที่หมายถึงคือการทำให้คอมพิวเตอร์หาการตั้งค่าที่ถูกต้องสำหรับตัวเลขจำนวนมากทั้งหมดเหล่านี้จำนวนมากเพื่อให้สามารถแก้ปัญหาได้จริง

152
00:12:40,200 --> 00:12:42,190
ปัญหาอยู่ในมือ

153
00:12:42,190 --> 00:12:43,000
หนึ่งคิด

154
00:12:43,000 --> 00:12:49,979
การทดลองที่สนุกสนานและน่าสะพรึงกลัวคือการจินตนาการให้นั่งลงและตั้งค่าทั้งหมดของน้ำหนักและอคติเหล่านี้ด้วยมือ

155
00:12:50,380 --> 00:12:56,159
ปรับแต่งตัวเลขอย่างรอบคอบเพื่อให้ชั้นที่สองหยิบขึ้นมาบนขอบชั้นที่สามจะหยิบขึ้นมาบนรูปแบบอื่น ๆ

156
00:12:56,350 --> 00:13:01,440
ฉันเองพบความพึงพอใจมากกว่าเพียงแค่อ่านเครือข่ายเป็นกล่องสีดำทั้งหมด

157
00:13:01,870 --> 00:13:04,349
เพราะเมื่อเครือข่ายไม่ทำงานตามที่คุณต้องการ

158
00:13:04,600 --> 00:13:11,370
คาดหวังว่าคุณจะมีความสัมพันธ์กับสิ่งที่น้ำหนักและความอคติเหล่านี้จริง ๆ หมายความว่าคุณมีสถานที่เริ่มต้น

159
00:13:11,680 --> 00:13:16,289
ทดลองใช้วิธีเปลี่ยนโครงสร้างเพื่อปรับปรุงหรือเมื่อเครือข่ายทำงาน

160
00:13:16,290 --> 00:13:18,290
ไม่ใช่เหตุผลที่คุณคาดหวัง

161
00:13:18,310 --> 00:13:25,169
การขุดเจาะสิ่งที่น้ำหนักและอคติกำลังทำอยู่เป็นวิธีที่ดีในการท้าทายสมมติฐานของคุณและเปิดเผยพื้นที่เต็มรูปแบบที่เป็นไปได้

162
00:13:25,180 --> 00:13:26,350
การแก้ปัญหา

163
00:13:26,350 --> 00:13:30,600
โดยวิธีการทำงานจริงที่นี่เป็นเพียงเล็กน้อยยุ่งยากในการเขียนลง คุณไม่คิดเหรอ?

164
00:13:32,350 --> 00:13:38,460
ดังนั้นให้ฉันแสดงวิธีการที่มีขนาดกะทัดรัดมากขึ้นในรูปแบบ notationally ซึ่งแสดงว่ามีการเชื่อมต่อเหล่านี้ นี่คือสิ่งที่คุณจะเห็น

165
00:13:38,460 --> 00:13:40,460
ถ้าคุณเลือกที่จะอ่านเพิ่มเติมเกี่ยวกับเครือข่ายประสาท

166
00:13:41,110 --> 00:13:45,810
จัดระเบียบการเปิดใช้งานทั้งหมดจากชั้นหนึ่งเป็นคอลัมน์เป็นเวกเตอร์

167
00:13:47,470 --> 00:13:52,320
จากนั้นจัดระเบียบน้ำหนักทั้งหมดเป็นเมตริกซ์ที่แต่ละแถวของเมทริกซ์นั้น

168
00:13:52,900 --> 00:13:57,659
สอดคล้องกับการเชื่อมต่อระหว่างชั้นหนึ่งกับเซลล์ประสาทที่เฉพาะเจาะจงในชั้นถัดไป

169
00:13:58,060 --> 00:14:03,599
สิ่งที่หมายถึงคือการใช้น้ำหนักของการกระตุ้นในชั้นแรกตามน้ำหนักเหล่านี้หรือไม่?

170
00:14:04,000 --> 00:14:09,330
สอดคล้องกับเงื่อนไขข้อใดข้อหนึ่งในผลิตภัณฑ์เมตริกซ์เวกเตอร์ของทุกสิ่งทุกอย่างที่เรามีด้านซ้ายที่นี่

171
00:14:13,540 --> 00:14:18,380
โดยวิธีการมากของการเรียนรู้เครื่องเพียงแค่ลงมามีความเข้าใจที่ดีของพีชคณิตเชิงเส้น

172
00:14:18,380 --> 00:14:26,940
ดังนั้นสำหรับคุณที่ต้องการความเข้าใจภาพที่ดีสำหรับเมทริกซ์และสิ่งที่การคูณเวกเตอร์เมทริกซ์หมายถึงดูที่ชุดที่ฉันทำในพีชคณิตเชิงเส้น

173
00:14:27,250 --> 00:14:28,839
โดยเฉพาะบทที่สาม

174
00:14:28,839 --> 00:14:35,759
กลับไปที่การแสดงออกของเราแทนที่จะพูดถึงการเพิ่มความลำเอียงให้กับแต่ละค่าเหล่านี้อย่างอิสระ

175
00:14:36,010 --> 00:14:42,209
การจัดอคติเหล่านี้ให้เป็นเวกเตอร์และเพิ่มเวกเตอร์ทั้งหมดลงในผลิตภัณฑ์เวคเตอร์เมทริกซ์ก่อนหน้านี้

176
00:14:42,910 --> 00:14:44,040
จากนั้นเป็นขั้นตอนสุดท้าย

177
00:14:44,040 --> 00:14:47,250
ฉันจะแร็พ sigmoid รอบด้านนอกที่นี่

178
00:14:47,250 --> 00:14:51,899
และสิ่งที่ควรจะหมายถึงคือคุณจะใช้ฟังก์ชัน sigmoid กับแต่ละเฉพาะ

179
00:14:52,420 --> 00:14:54,570
ส่วนประกอบของเวกเตอร์ที่เกิดขึ้นภายใน

180
00:14:55,510 --> 00:15:00,749
ดังนั้นเมื่อคุณจดเมทริกซ์น้ำหนักนี้และเวกเตอร์เหล่านี้เป็นสัญลักษณ์ของตัวเองที่คุณสามารถทำได้

181
00:15:01,000 --> 00:15:07,589
สื่อสารการเปลี่ยนจากการเปิดใช้งานทั้งหมดจากชั้นหนึ่งไปยังอีกชั้นหนึ่งในการแสดงออกน้อยมากและแน่นและ

182
00:15:07,930 --> 00:15:15,000
นี้ทำให้รหัสที่เกี่ยวข้องทั้งง่ายมากและรวดเร็วยิ่งขึ้นเนื่องจากหลายห้องสมุดเพิ่มประสิทธิภาพ heck ออกจากการคูณเมทริกซ์

183
00:15:17,560 --> 00:15:21,359
โปรดจำไว้ว่าก่อนหน้านี้ฉันกล่าวว่าเซลล์ประสาทเหล่านี้เป็นเพียงสิ่งที่ถือตัวเลข

184
00:15:21,790 --> 00:15:26,250
แน่นอนตัวเลขที่เฉพาะเจาะจงที่พวกเขาถือขึ้นอยู่กับภาพที่คุณฟีด

185
00:15:27,790 --> 00:15:32,940
ดังนั้นจึงเป็นจริงถูกต้องมากขึ้นในการคิดของเซลล์ประสาทแต่ละตัวเป็นฟังก์ชันที่ใช้เวลาใน

186
00:15:33,070 --> 00:15:38,070
เอาท์พุทของเซลล์ประสาททั้งหมดในชั้นก่อนหน้าและ spits ออกจำนวนระหว่างศูนย์และหนึ่ง

187
00:15:38,800 --> 00:15:42,270
จริงๆเครือข่ายทั้งหมดเป็นเพียงหนึ่งฟังก์ชันที่ใช้เวลาค่ะ

188
00:15:42,760 --> 00:15:47,010
784 เป็น input และ spits ออกสิบตัวเลขเป็นเอาท์พุท

189
00:15:47,470 --> 00:15:48,700
มันไร้สาระ

190
00:15:48,700 --> 00:15:56,249
ฟังก์ชันซับซ้อนหนึ่งที่เกี่ยวข้องกับพารามิเตอร์สามหมื่นในรูปแบบของน้ำหนักและอคติเหล่านี้ที่รับในรูปแบบบางอย่างและเกี่ยวข้องกับ

191
00:15:56,250 --> 00:16:00,270
iterating หลายผลิตภัณฑ์เวกเตอร์เมทริกซ์และฟังก์ชัน sigmoid squish evocation

192
00:16:00,610 --> 00:16:06,390
แต่มันก็เป็นเพียงแค่ฟังก์ชั่นและในแบบที่มันเป็นความมั่นใจว่ามันดูซับซ้อน

193
00:16:06,390 --> 00:16:12,239
ฉันหมายความว่าถ้าเราหวังว่าเราจะมีความหวังที่ง่ายกว่านี้

194
00:16:12,960 --> 00:16:19,559
และวิธีการที่จะใช้เวลาในการท้าทายที่? เครือข่ายนี้จะเรียนรู้น้ำหนักและความลำเอียงที่เหมาะสมเพียงแค่ดูข้อมูล โอ้?

195
00:16:20,080 --> 00:16:26,039
นั่นคือสิ่งที่ฉันจะแสดงในวิดีโอถัดไปและฉันจะขุดเพิ่มเติมอีกนิดในสิ่งที่เครือข่ายเฉพาะที่เรากำลังดูอยู่นี้กำลังทำอยู่จริงๆ

196
00:16:27,130 --> 00:16:32,640
ตอนนี้เป็นจุดที่ฉันคิดว่าฉันควรบอกว่าสมัครรับข้อมูลเพื่อรับทราบเมื่อวิดีโอหรือวิดีโอใหม่ ๆ ออกมา

197
00:16:32,760 --> 00:16:37,560
แต่ในความสมจริงส่วนใหญ่คุณไม่ได้รับการแจ้งเตือนจาก YouTube จริงๆใช่ไหม

198
00:16:37,560 --> 00:16:42,260
อาจจะสุจริตมากขึ้นฉันควรบอกว่าสมัครรับข้อมูลเพื่อให้เครือข่ายประสาทที่รองรับ YouTube

199
00:16:42,459 --> 00:16:47,639
อัลกอริทึมคำแนะนำแนะนำให้เชื่อว่าคุณต้องการดูเนื้อหาจากช่องนี้ขอแนะนำให้คุณ

200
00:16:48,250 --> 00:16:50,250
ยังคงโพสต์มากขึ้น

201
00:16:50,410 --> 00:16:53,550
ขอบคุณทุกคนที่สนับสนุนวิดีโอเหล่านี้ใน patreon

202
00:16:53,589 --> 00:16:56,759
ฉันได้รับความคืบหน้าในชุดความน่าจะเป็นช่วงฤดูร้อนนี้เล็กน้อย

203
00:16:56,760 --> 00:17:01,379
แต่ฉันกระโดดกลับเข้ามาหลังจากโครงการนี้เพื่อคุ้มครองคุณสามารถมองหาการปรับปรุงที่นั่น

204
00:17:03,310 --> 00:17:05,550
เพื่อปิดสิ่งต่างๆออกจากที่นี่ฉันมีกับฉัน Lisha Li

205
00:17:05,550 --> 00:17:12,029
ลีผู้ซึ่งทำผลงานด้านการศึกษาระดับปริญญาเอกของเธอในด้านทฤษฎีของการเรียนรู้ลึก ๆ และปัจจุบันทำงานใน บริษัท ร่วมทุนที่เรียกว่าพันธมิตรขยายกิจการ

206
00:17:12,030 --> 00:17:16,530
ใครให้ความกรุณาในการระดมทุนสำหรับวิดีโอนี้ดังนั้น Lisha จึงเป็นสิ่งหนึ่ง

207
00:17:16,530 --> 00:17:19,109
ฉันคิดว่าเราควรจะนำมาขึ้นเป็นฟังก์ชัน sigmoid นี้

208
00:17:19,180 --> 00:17:24,780
ตามที่ฉันเข้าใจว่าเครือข่ายแรก ๆ ใช้วิธีนี้เพื่อหาค่าน้ำหนักที่เกี่ยวข้องในช่วงระหว่างศูนย์กับหนึ่ง

209
00:17:24,980 --> 00:17:30,340
คุณรู้ชนิดของแรงบันดาลใจจากการเปรียบเทียบทางชีววิทยาของเซลล์ประสาทนี้ไม่ว่าจะเป็นงานหรือใช้งาน
(Lisha) - ถูกต้อง

210
00:17:30,360 --> 00:17:36,320
(3B1B) - แต่ค่อนข้างน้อยเครือข่ายสมัยใหม่ใช้ sigmoid อีกต่อไป นั่นเป็นเรื่องของโรงเรียนเก่าใช่มั้ย?
(Lisha) - ใช่หรือค่อนข้าง

211
00:17:36,370 --> 00:17:42,780
ReLU ดูเหมือนว่าจะฝึกได้ง่ายกว่ามาก
(3B1B) - และ ReLU ย่อมาจาก rectified linear unit

212
00:17:42,780 --> 00:17:48,839
(Lisha) - ใช่นี่เป็นฟังก์ชันแบบนี้ที่คุณใช้เวลาเพียงแค่ไม่ถึง 0 และที่ a จะได้รับโดย

213
00:17:49,120 --> 00:17:53,670
สิ่งที่คุณกำลังอธิบายในวิดีโอและสิ่งที่เป็นแรงบันดาลใจแบบนี้จากที่ฉันคิดว่าเป็น

214
00:17:54,610 --> 00:17:56,610
บางส่วนโดยทางชีวภาพ

215
00:17:56,620 --> 00:17:58,179
คล้ายคลึงกับวิธี

216
00:17:58,179 --> 00:18:03,089
เซลล์ประสาทจะทำงานได้หรือไม่และถ้าผ่านเกณฑ์หนึ่ง

217
00:18:03,250 --> 00:18:05,250
มันจะเป็นฟังก์ชันการระบุตัวตน

218
00:18:05,290 --> 00:18:10,439
แต่ถ้ามันไม่ได้แล้วมันก็จะไม่ได้รับการเปิดใช้งานเพื่อให้เป็นศูนย์ดังนั้นจึงเป็นชนิดของการทำให้เข้าใจง่าย

219
00:18:10,720 --> 00:18:14,429
การใช้ sigmaids ไม่ได้ช่วยในการฝึกอบรมหรือการฝึกอบรมทำได้ยากมาก

220
00:18:14,429 --> 00:18:19,589
ในบางประเด็นและผู้คนเพิ่งลอง relu และมันก็เกิดขึ้นกับการทำงาน

221
00:18:20,110 --> 00:18:22,140
เป็นอย่างดีสำหรับคนเหล่านี้อย่างไม่น่าเชื่อ

222
00:18:22,690 --> 00:18:25,090
เครือข่ายประสาทเทียมลึก
(3B1B) - ถูกต้อง

223
00:18:25,090 --> 00:18:26,060
ขอบคุณ Lisha

