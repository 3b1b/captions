1
00:00:04,220 --> 00:00:05,400
Ez egy 3.

2
00:00:06,060 --> 00:00:10,550
Hanyagul van megírva, és rendkívül alacsony, 28x28 pixeles felbontásban jelenik meg, 

3
00:00:10,550 --> 00:00:13,720
de az agyadnak nem okoz gondot felismerni, hogy ez egy 3-as.

4
00:00:14,340 --> 00:00:17,085
És szeretném, ha egy pillanatra értékelnétek, milyen őrületes, 

5
00:00:17,085 --> 00:00:18,960
hogy az agyak ilyen könnyedén képesek erre.

6
00:00:19,700 --> 00:00:24,044
Úgy értem, ez, ez és ez is felismerhető 3-asnak, még akkor is, 

7
00:00:24,044 --> 00:00:28,320
ha az egyes pixelek konkrét értékei képenként nagyon eltérőek.

8
00:00:28,900 --> 00:00:33,191
A szemedben lévő fényérzékeny sejtek, amelyek akkor tüzelnek, amikor ezt a 3-at látod, 

9
00:00:33,191 --> 00:00:36,940
nagyon különböznek azoktól, amelyek akkor tüzelnek, amikor ezt a 3-at látod.

10
00:00:37,520 --> 00:00:41,791
De valami az ön őrülten okos vizuális kéregében úgy oldja fel ezeket, 

11
00:00:41,791 --> 00:00:46,124
mintha ugyanazt az elképzelést ábrázolnák, miközben más képeket saját, 

12
00:00:46,124 --> 00:00:48,260
különálló elképzelésként ismer fel.

13
00:00:49,220 --> 00:00:53,543
De ha azt mondanám, hogy hé, ülj le, és írj nekem egy programot, 

14
00:00:53,543 --> 00:00:59,129
amely egy ilyen 28x28 pixeles rácsot vesz fel, és egy 0 és 10 közötti számot ad ki, 

15
00:00:59,129 --> 00:01:04,583
megmondva, hogy szerinte mi az a számjegy, akkor a feladat komikusan triviálisból 

16
00:01:04,583 --> 00:01:06,180
ijesztően nehézzé válik.

17
00:01:07,160 --> 00:01:09,240
Hacsak nem éltél egy szikla alatt, azt hiszem, 

18
00:01:09,240 --> 00:01:12,692
aligha kell motiválnom a gépi tanulás és a neurális hálózatok jelentőségét és 

19
00:01:12,692 --> 00:01:14,640
fontosságát a jelen és a jövő szempontjából.

20
00:01:15,120 --> 00:01:18,847
De amit itt szeretnék tenni, az az, hogy megmutatom, mi is valójában a neurális hálózat, 

21
00:01:18,847 --> 00:01:22,030
feltételezve, hogy nincs háttér, és hogy segítsek szemléltetni, mit csinál, 

22
00:01:22,030 --> 00:01:24,460
nem mint egy divatos szó, hanem mint egy darab matematika.

23
00:01:25,020 --> 00:01:27,938
Remélem, hogy úgy érzed, hogy maga a struktúra motivált, 

24
00:01:27,938 --> 00:01:30,704
és úgy érzed, hogy tudod, mit jelent, amikor olvasol, 

25
00:01:30,704 --> 00:01:34,340
vagy hallasz egy idézőjeles neurális hálózatról, idézőjeles tanulásról.

26
00:01:35,360 --> 00:01:38,066
Ez a videó csak a struktúra komponensének lesz szentelve, 

27
00:01:38,066 --> 00:01:40,260
a következő pedig a tanulással fog foglalkozni.

28
00:01:40,960 --> 00:01:43,342
Mi most egy olyan neurális hálózatot állítunk össze, 

29
00:01:43,342 --> 00:01:46,040
amely képes megtanulni a kézzel írt számjegyek felismerését.

30
00:01:49,360 --> 00:01:51,970
Ez egy kissé klasszikus példa a téma bemutatására, 

31
00:01:51,970 --> 00:01:56,271
és örömmel ragaszkodom a status quo-hoz, mert a két videó végén szeretnék rámutatni 

32
00:01:56,271 --> 00:02:00,059
néhány jó forrásra, ahol többet megtudhatsz, és ahol letöltheted a kódot, 

33
00:02:00,059 --> 00:02:03,080
amely ezt teszi, és játszhatsz vele a saját számítógépeden.

34
00:02:05,040 --> 00:02:07,701
A neurális hálózatoknak sokféle változata létezik, 

35
00:02:07,701 --> 00:02:11,301
és az utóbbi években egyfajta fellendülés tapasztalható a kutatásban 

36
00:02:11,301 --> 00:02:15,892
e változatok irányába, de ebben a két bevezető videóban mi ketten csak a legegyszerűbb, 

37
00:02:15,892 --> 00:02:19,180
sima vanília formát fogjuk megnézni, mindenféle sallang nélkül.

38
00:02:19,860 --> 00:02:22,538
Ez egyfajta szükséges előfeltétele annak, hogy megértsük 

39
00:02:22,538 --> 00:02:24,934
bármelyik erősebb modern változatot, és higgye el, 

40
00:02:24,934 --> 00:02:28,600
még mindig rengeteg komplexitással rendelkezik, amit nekünk kell átgondolnunk.

41
00:02:29,120 --> 00:02:32,686
De még ebben a legegyszerűbb formában is képes megtanulni a kézzel 

42
00:02:32,686 --> 00:02:36,520
írt számjegyek felismerését, ami egy számítógép számára elég menő dolog.

43
00:02:37,480 --> 00:02:42,280
És ugyanakkor látni fogjátok, hogy elmarad néhány reménytől, amit hozzá fűztünk.

44
00:02:43,380 --> 00:02:48,500
Ahogy a neve is sugallja, a neurális hálózatokat az agy ihlette, de ezt most bontsuk le.

45
00:02:48,520 --> 00:02:51,660
Mik azok a neuronok, és milyen értelemben kapcsolódnak egymáshoz?

46
00:02:52,500 --> 00:02:57,275
Most, amikor azt mondom, hogy neuron, azt akarom, hogy egy olyan dologra gondoljatok, 

47
00:02:57,275 --> 00:03:00,440
ami egy számot tart, konkrétan egy 0 és 1 közötti számot.

48
00:03:00,680 --> 00:03:02,560
Ennél többről szó sincs.

49
00:03:03,780 --> 00:03:09,042
A hálózat például a bemeneti kép minden egyes 28x28 pixelének 

50
00:03:09,042 --> 00:03:14,220
megfelelő neuronokkal indul, ami összesen 784 neuront jelent.

51
00:03:14,700 --> 00:03:19,570
Ezek mindegyike egy számot tartalmaz, amely a megfelelő pixel szürkeárnyalatos 

52
00:03:19,570 --> 00:03:24,380
értékét jelöli, a fekete pixelek esetében 0-tól a fehér pixelek esetében 1-ig.

53
00:03:25,300 --> 00:03:29,418
Ezt a számot a neuronon belül aktivációnak nevezzük, és az a kép, 

54
00:03:29,418 --> 00:03:34,160
ami itt eszedbe juthat, hogy minden neuron világít, ha az aktivációja magas.

55
00:03:36,720 --> 00:03:41,860
Tehát mindezek a 784 neuron alkotják a hálózatunk első rétegét.

56
00:03:46,500 --> 00:03:49,029
Az utolsó rétegre ugorva, ez 10 neuront tartalmaz, 

57
00:03:49,029 --> 00:03:51,360
amelyek mindegyike egy-egy számjegyet képvisel.

58
00:03:52,040 --> 00:03:57,079
Ezeknek az idegsejteknek az aktivációja - ismét egy 0 és 1 közötti szám - azt jelzi, 

59
00:03:57,079 --> 00:04:02,120
hogy a rendszer mennyire gondolja, hogy egy adott kép megfelel egy adott számjegynek.

60
00:04:03,040 --> 00:04:06,930
Van még egy pár réteg a kettő között, az úgynevezett rejtett rétegek, 

61
00:04:06,930 --> 00:04:10,154
amelyek egyelőre csak egy hatalmas kérdőjelet jelentenek, 

62
00:04:10,154 --> 00:04:13,600
hogy a számjegyek felismerésének folyamata hogyan fog működni.

63
00:04:14,260 --> 00:04:17,019
Ebben a hálózatban két rejtett réteget választottam, 

64
00:04:17,019 --> 00:04:20,560
egyenként 16 neuronnal, és bevallom, ez egyfajta önkényes választás.

65
00:04:21,019 --> 00:04:23,223
Hogy őszinte legyek, két réteget választottam az alapján, 

66
00:04:23,223 --> 00:04:25,616
hogy hogyan akarom motiválni a szerkezetet egy pillanat alatt, 

67
00:04:25,616 --> 00:04:28,200
és a 16, nos, ez csak egy szép szám volt, hogy elférjen a képernyőn.

68
00:04:28,780 --> 00:04:32,340
A gyakorlatban itt sok lehetőség van a konkrét struktúrával való kísérletezésre.

69
00:04:33,020 --> 00:04:35,646
A hálózat működésének módja szerint az egyik réteg 

70
00:04:35,646 --> 00:04:38,480
aktivációi meghatározzák a következő réteg aktivációit.

71
00:04:39,200 --> 00:04:43,836
És természetesen a hálózat, mint információfeldolgozó mechanizmus szíve pontosan azon 

72
00:04:43,836 --> 00:04:48,580
múlik, hogy az egyik réteg aktivációi hogyan hozzák létre a következő réteg aktivációit.

73
00:04:49,140 --> 00:04:53,129
Ez nagyjából analóg azzal, ahogyan a biológiai neuronhálózatokban 

74
00:04:53,129 --> 00:04:57,180
az idegsejtek egyes csoportjai bizonyos neuronok tüzelését okozzák.

75
00:04:58,120 --> 00:05:01,656
Az itt bemutatott hálózatot már betanítottuk a számjegyek felismerésére, 

76
00:05:01,656 --> 00:05:03,400
és hadd mutassam meg, mire gondolok.

77
00:05:03,640 --> 00:05:06,067
Ez azt jelenti, hogy ha betáplálunk egy képet, 

78
00:05:06,067 --> 00:05:10,613
és a bemeneti réteg mind a 784 neuronját a kép minden egyes pixelének fényereje szerint 

79
00:05:10,613 --> 00:05:14,951
világítjuk meg, akkor az aktiválásoknak ez a mintázata valamilyen nagyon specifikus 

80
00:05:14,951 --> 00:05:19,445
mintát okoz a következő rétegben, amely valamilyen mintát okoz az azt követő rétegben, 

81
00:05:19,445 --> 00:05:22,080
ami végül valamilyen mintát ad a kimeneti rétegben.

82
00:05:22,560 --> 00:05:26,265
És a kimeneti réteg legvilágosabb neuronja a hálózat választása, 

83
00:05:26,265 --> 00:05:29,400
hogy úgy mondjam, melyik számjegyet képviseli ez a kép.

84
00:05:32,560 --> 00:05:36,260
És mielőtt belemennénk a matematikába, hogy az egyik réteg hogyan befolyásolja 

85
00:05:36,260 --> 00:05:39,164
a következőt, vagy hogyan működik a képzés, beszéljünk arról, 

86
00:05:39,164 --> 00:05:42,068
hogy miért is ésszerű elvárni egy ilyen réteges struktúrától, 

87
00:05:42,068 --> 00:05:43,520
hogy intelligensen viselkedjen.

88
00:05:44,060 --> 00:05:45,220
Mit várunk itt?

89
00:05:45,400 --> 00:05:47,600
Mi a legjobb remény ezeknek a középső rétegeknek?

90
00:05:48,920 --> 00:05:53,520
Nos, amikor ön vagy én számjegyeket ismerünk fel, különböző összetevőket rakunk össze.

91
00:05:54,200 --> 00:05:56,820
A 9-esnek van egy hurok felül és egy vonal a jobb oldalon.

92
00:05:57,380 --> 00:06:01,180
A 8-asnak is van egy hurok a tetején, de ez egy másik hurokkal párosul lent.

93
00:06:01,980 --> 00:06:06,820
A 4-es alapvetően három konkrét vonalra bomlik, meg ilyesmi.

94
00:06:07,600 --> 00:06:11,549
Egy tökéletes világban remélhetnénk, hogy az utolsó előtti réteg minden 

95
00:06:11,549 --> 00:06:14,839
neuronja megfelel valamelyik alkomponensnek, hogy bármikor, 

96
00:06:14,839 --> 00:06:18,350
amikor egy képet táplálunk be, mondjuk, egy hurokkal a tetején, 

97
00:06:18,350 --> 00:06:21,531
mint például egy 9-es vagy 8-as, van egy bizonyos neuron, 

98
00:06:21,531 --> 00:06:23,780
amelynek aktivációja közel lesz az 1-hez.

99
00:06:24,500 --> 00:06:27,512
És nem erre a konkrét pixelhurokra gondolok, a remény az lenne, 

100
00:06:27,512 --> 00:06:31,560
hogy bármilyen általánosan hurkolt minta a csúcs felé haladva beindítja ezt a neuront.

101
00:06:32,440 --> 00:06:36,418
Így a harmadik rétegtől az utolsóig való eljutáshoz csak azt kell megtanulni, 

102
00:06:36,418 --> 00:06:40,040
hogy az alkomponensek melyik kombinációja melyik számjegynek felel meg.

103
00:06:41,000 --> 00:06:43,368
Ez persze csak tovább rúgja a problémát, mert hogyan ismerhetnénk 

104
00:06:43,368 --> 00:06:45,953
fel ezeket az alkomponenseket, vagy egyáltalán hogyan tanulhatnánk meg, 

105
00:06:45,953 --> 00:06:47,640
hogy melyek legyenek a megfelelő alkomponensek?

106
00:06:48,060 --> 00:06:51,631
És még nem is beszéltem arról, hogy az egyik réteg hogyan befolyásolja a következőt, 

107
00:06:51,631 --> 00:06:53,060
de egy pillanatra tartsatok velem.

108
00:06:53,680 --> 00:06:56,680
A hurok felismerése részproblémákra is bontható.

109
00:06:57,280 --> 00:07:01,880
Ennek egyik ésszerű módja az lenne, ha először is felismernénk a különböző kis éleket, 

110
00:07:01,880 --> 00:07:02,780
amelyek alkotják.

111
00:07:03,780 --> 00:07:06,701
Hasonlóképpen, egy hosszú vonal, mint amilyeneket az 1, 

112
00:07:06,701 --> 00:07:10,041
4 vagy 7 számjegyekben láthatunk, valójában csak egy hosszú él, 

113
00:07:10,041 --> 00:07:14,320
vagy talán úgy gondolunk rá, mint egy bizonyos, több kisebb élből álló mintázatra.

114
00:07:15,140 --> 00:07:18,839
Tehát talán az a reményünk, hogy a hálózat második rétegének 

115
00:07:18,839 --> 00:07:22,720
minden egyes neuronja megfelel a különböző releváns kis éleknek.

116
00:07:23,540 --> 00:07:27,846
Talán amikor egy ilyen kép érkezik be, akkor az összes neuron felgyullad, 

117
00:07:27,846 --> 00:07:31,106
amely körülbelül 8-10 specifikus kis éllel kapcsolatos, 

118
00:07:31,106 --> 00:07:34,889
ami viszont felgyújtja a felső hurokkal és egy hosszú függőleges 

119
00:07:34,889 --> 00:07:39,720
vonallal kapcsolatos neuronokat, és ezek felgyújtják a 9-essel kapcsolatos neuront.

120
00:07:40,680 --> 00:07:44,878
Hogy a végleges hálózatunk valójában ezt teszi-e vagy sem, az egy másik kérdés, 

121
00:07:44,878 --> 00:07:48,866
amire még visszatérek, amint meglátjuk, hogyan kell betanítani a hálózatot, 

122
00:07:48,866 --> 00:07:52,540
de ez egy remény, egyfajta célunk lehet az ilyen réteges struktúrával.

123
00:07:53,160 --> 00:07:56,784
Sőt, el lehet képzelni, hogy az élek és minták ilyen módon történő 

124
00:07:56,784 --> 00:08:00,300
felismerése nagyon hasznos lenne más képfelismerési feladatoknál.

125
00:08:00,880 --> 00:08:04,326
És még a képfelismerésen túl is vannak mindenféle intelligens dolgok, 

126
00:08:04,326 --> 00:08:07,280
amelyeket az absztrakciós rétegekre bontva akarsz elvégezni.

127
00:08:08,040 --> 00:08:12,340
A beszéd elemzése például azt jelenti, hogy a nyers hanganyagból kiválogatjuk 

128
00:08:12,340 --> 00:08:16,806
az egyes hangokat, amelyek egyes szótagokat alkotnak, amelyek szavakat alkotnak, 

129
00:08:16,806 --> 00:08:20,060
amelyek mondatokká és elvont gondolatokká állnak össze stb.

130
00:08:21,100 --> 00:08:24,104
De visszatérve arra, hogy mindez hogyan is működik valójában, 

131
00:08:24,104 --> 00:08:27,157
képzelje el magát most, amint megtervezi, hogy pontosan hogyan 

132
00:08:27,157 --> 00:08:29,920
határozhatják meg az egyik réteg aktivációi a következőt.

133
00:08:30,860 --> 00:08:34,785
A cél az, hogy legyen valamilyen mechanizmus, amely elképzelhető módon a 

134
00:08:34,785 --> 00:08:38,980
pixeleket élekké, az éleket mintákká, vagy a mintákat számjegyekké kombinálja.

135
00:08:39,440 --> 00:08:43,207
És hogy egy nagyon konkrét példára ráközelítsünk, tegyük fel, 

136
00:08:43,207 --> 00:08:47,581
hogy a remény az, hogy a második réteg egy bizonyos neuronja felismeri, 

137
00:08:47,581 --> 00:08:50,620
hogy a képnek van-e éle ebben a régióban vagy sem.

138
00:08:51,440 --> 00:08:55,100
A kérdés az, hogy milyen paraméterekkel kell rendelkeznie a hálózatnak?

139
00:08:55,640 --> 00:09:00,140
Milyen tárcsákat és gombokat kell tudnod beállítani, hogy elég kifejező legyen ahhoz, 

140
00:09:00,140 --> 00:09:04,117
hogy potenciálisan megragadja ezt a mintát, vagy bármilyen más pixelmintát, 

141
00:09:04,117 --> 00:09:07,780
vagy azt a mintát, hogy több élből hurok lehet, és más ilyen dolgokat?

142
00:09:08,720 --> 00:09:12,173
Nos, a neuronunk és az első réteg neuronjai közötti 

143
00:09:12,173 --> 00:09:15,560
kapcsolatok mindegyikéhez hozzárendelünk egy súlyt.

144
00:09:16,320 --> 00:09:17,700
Ezek a súlyok csak számok.

145
00:09:18,540 --> 00:09:21,752
Ezután vegyük az első réteg összes aktiválását, 

146
00:09:21,752 --> 00:09:25,500
és számítsuk ki a súlyozott összegüket e súlyok szerint.

147
00:09:27,700 --> 00:09:30,536
Hasznosnak találom, ha úgy gondolok ezekre a súlyokra, 

148
00:09:30,536 --> 00:09:33,992
mintha egy saját kis rácsba rendeződnének, és zöld pixeleket fogok 

149
00:09:33,992 --> 00:09:38,479
használni a pozitív súlyok jelölésére, és piros pixeleket a negatív súlyok jelölésére, 

150
00:09:38,479 --> 00:09:41,780
ahol az adott pixel fényessége a súly értékének laza ábrázolása.

151
00:09:42,780 --> 00:09:46,156
Ha most a szinte minden pixelhez tartozó súlyokat nullává tettük, 

152
00:09:46,156 --> 00:09:49,379
kivéve néhány pozitív súlyt ebben a számunkra fontos régióban, 

153
00:09:49,379 --> 00:09:53,880
akkor az összes pixelérték súlyozott összegének kiszámítása valójában csak annyit tesz, 

154
00:09:53,880 --> 00:09:57,820
hogy összeadjuk a pixelértékeket csak abban a régióban, amely minket érdekel.

155
00:09:59,140 --> 00:10:02,964
És ha tényleg meg akarjuk állapítani, hogy van-e itt egy él, 

156
00:10:02,964 --> 00:10:06,600
akkor a környező pixelekhez negatív súlyokat rendelhetünk.

157
00:10:07,480 --> 00:10:10,886
Az összeg akkor a legnagyobb, ha a középső pixelek világosak, 

158
00:10:10,886 --> 00:10:12,700
de a környező pixelek sötétebbek.

159
00:10:14,260 --> 00:10:18,556
Amikor egy ilyen súlyozott összeget számolunk, bármilyen számot kaphatunk, 

160
00:10:18,556 --> 00:10:23,540
de ebben a hálózatban azt szeretnénk, ha az aktivációk 0 és 1 közötti értéket kapnának.

161
00:10:24,120 --> 00:10:28,776
Ezért gyakran előfordul, hogy ezt a súlyozott összeget beletöltjük valamilyen függvénybe, 

162
00:10:28,776 --> 00:10:32,140
amely a valós számok sorát a 0 és 1 közötti tartományba szorítja.

163
00:10:32,460 --> 00:10:37,420
Egy gyakori függvény, amely ezt teszi, a szigmoid függvény, más néven logisztikus görbe.

164
00:10:38,000 --> 00:10:41,178
Alapvetően a nagyon negatív bemenetek közel 0-hoz, 

165
00:10:41,178 --> 00:10:46,600
a pozitív bemenetek közel 1-hez végződnek, és a 0 bemenet körül folyamatosan növekszik.

166
00:10:49,120 --> 00:10:52,775
Tehát a neuron aktivációja itt alapvetően azt méri, 

167
00:10:52,775 --> 00:10:56,360
hogy a vonatkozó súlyozott összeg mennyire pozitív.

168
00:10:57,540 --> 00:10:59,268
De talán nem is arról van szó, hogy azt akarjuk, 

169
00:10:59,268 --> 00:11:01,880
hogy a neuron akkor világítson, amikor a súlyozott összeg nagyobb, mint 0.

170
00:11:02,280 --> 00:11:06,360
Talán csak akkor szeretné, ha aktív lenne, ha az összeg nagyobb, mint mondjuk 10.

171
00:11:06,840 --> 00:11:10,260
Azaz, azt akarod, hogy legyen némi előítélet, hogy inaktív legyen.

172
00:11:11,380 --> 00:11:14,087
Ezután egyszerűen csak hozzáadunk egy másik számot, 

173
00:11:14,087 --> 00:11:16,743
például negatív 10-et ehhez a súlyozott összeghez, 

174
00:11:16,743 --> 00:11:19,660
mielőtt átdugnánk a szigmoid squishification függvényen.

175
00:11:20,580 --> 00:11:22,440
Ezt a további számot nevezzük torzításnak.

176
00:11:23,460 --> 00:11:27,189
Tehát a súlyok megmondják, hogy a második rétegben lévő neuron milyen 

177
00:11:27,189 --> 00:11:29,746
pixelmintát vesz fel, az előfeszítés pedig azt, 

178
00:11:29,746 --> 00:11:33,049
hogy a súlyozott összegnek milyen magasnak kell lennie ahhoz, 

179
00:11:33,049 --> 00:11:35,180
hogy a neuron értelmesen aktívvá váljon.

180
00:11:36,120 --> 00:11:37,680
És ez csak egy neuron.

181
00:11:38,280 --> 00:11:44,334
Minden más neuron ebben a rétegben az első réteg mind a 784 pixel 

182
00:11:44,334 --> 00:11:50,940
neuronjához kapcsolódik, és mind a 784 kapcsolathoz saját súly tartozik.

183
00:11:51,600 --> 00:11:54,099
Mindegyiknek van valamilyen torzítása, egy másik szám, 

184
00:11:54,099 --> 00:11:57,600
amelyet hozzáadunk a súlyozott összeghez, mielőtt a szigmoiddal összenyomjuk.

185
00:11:58,110 --> 00:11:59,540
És ez rengeteg gondolkodnivaló!

186
00:11:59,960 --> 00:12:06,266
Ezzel a 16 neuronból álló rejtett réteggel ez összesen 784-szer 16 súlyt jelent, 

187
00:12:06,266 --> 00:12:07,980
16 torzítással együtt.

188
00:12:08,840 --> 00:12:11,940
És mindez csak az első réteg és a második réteg közötti kapcsolatok.

189
00:12:12,520 --> 00:12:17,340
A többi réteg közötti kapcsolatokhoz szintén egy csomó súly és torzítás tartozik.

190
00:12:18,340 --> 00:12:21,275
Mindent összevetve, ez a hálózat majdnem pontosan 

191
00:12:21,275 --> 00:12:23,800
13 000 összes súlyt és torzítást tartalmaz.

192
00:12:23,800 --> 00:12:29,960
13 000 gomb és tárcsa, amelyekkel és elforgatásával a hálózat különböző módon viselkedhet.

193
00:12:31,040 --> 00:12:34,236
Amikor tehát tanulásról beszélünk, akkor ez azt jelenti, 

194
00:12:34,236 --> 00:12:38,836
hogy a számítógépnek meg kell találnia a sok-sok számnak az érvényes beállítását, 

195
00:12:38,836 --> 00:12:41,360
hogy ténylegesen megoldja az adott problémát.

196
00:12:42,620 --> 00:12:47,291
Egy egyszerre szórakoztató és rémisztő gondolatkísérlet, ha elképzeljük, hogy leülünk, 

197
00:12:47,291 --> 00:12:50,351
és kézzel állítjuk be ezeket a súlyokat és torzításokat, 

198
00:12:50,351 --> 00:12:54,271
és célzottan úgy állítjuk be a számokat, hogy a második réteg az éleket, 

199
00:12:54,271 --> 00:12:56,580
a harmadik réteg a mintákat stb. vegye fel.

200
00:12:56,980 --> 00:12:59,437
Én személy szerint ezt kielégítőnek találom, ahelyett, 

201
00:12:59,437 --> 00:13:03,279
hogy a hálózatot teljes fekete dobozként kezelném, mert ha a hálózat nem úgy működik, 

202
00:13:03,279 --> 00:13:06,585
ahogyan azt elvártuk, és ha már kialakítottunk egy kis kapcsolatot azzal, 

203
00:13:06,585 --> 00:13:09,221
hogy ezek a súlyok és torzítások valójában mit jelentenek, 

204
00:13:09,221 --> 00:13:11,410
akkor van egy kiindulópontunk a kísérletezéshez, 

205
00:13:11,410 --> 00:13:14,180
hogy hogyan változtassuk meg a struktúrát a javítás érdekében.

206
00:13:14,960 --> 00:13:17,936
Vagy amikor a hálózat működik, de nem a várt okok miatt, 

207
00:13:17,936 --> 00:13:20,494
a súlyok és torzítások feltárása jó módja annak, 

208
00:13:20,494 --> 00:13:23,992
hogy megkérdőjelezzük a feltételezéseinket, és valóban feltárjuk a 

209
00:13:23,992 --> 00:13:25,820
lehetséges megoldások teljes terét.

210
00:13:26,840 --> 00:13:30,680
Egyébként a tényleges funkciót itt egy kicsit nehézkes leírni, nem gondolja?

211
00:13:32,500 --> 00:13:37,140
Hadd mutassam meg tehát, hogyan lehet ezeket a kapcsolatokat tömörebben ábrázolni.

212
00:13:37,660 --> 00:13:40,520
Így láthatod, ha úgy döntesz, hogy többet olvasol a neurális hálózatokról.

213
00:13:41,380 --> 00:13:47,338
Rendezze az összes aktivációt egy rétegből egy oszlopba, 

214
00:13:47,338 --> 00:13:55,595
mivel a mátrix megfelel az egyik réteg és a következő réteg egy adott neuronja 

215
00:13:55,595 --> 00:13:58,000
közötti kapcsolatoknak.

216
00:13:58,540 --> 00:14:04,414
Ez azt jelenti, hogy az első réteg aktivációinak súlyozott összege az említett súlyok 

217
00:14:04,414 --> 00:14:09,880
szerint megfelel a mátrix vektorproduktumának egyik tagjának, ami itt balra van.

218
00:14:14,000 --> 00:14:18,231
Egyébként a gépi tanulás nagy része csak a lineáris algebra jó ismeretén múlik, 

219
00:14:18,231 --> 00:14:22,834
így azok számára, akik szeretnének egy szép vizuális megértést a mátrixokról és arról, 

220
00:14:22,834 --> 00:14:27,277
hogy mit jelent a mátrix-vektor szorzás, nézzék meg a lineáris algebra sorozatomat, 

221
00:14:27,277 --> 00:14:28,600
különösen a 3. fejezetet.

222
00:14:29,240 --> 00:14:32,442
Visszatérve a kifejezésünkhöz, ahelyett, hogy arról beszélnénk, 

223
00:14:32,442 --> 00:14:36,445
hogy minden egyes értékhez függetlenül adjuk hozzá a torzítást, úgy ábrázoljuk, 

224
00:14:36,445 --> 00:14:38,947
hogy az összes torzítást egy vektorba szervezzük, 

225
00:14:38,947 --> 00:14:42,300
és a teljes vektort hozzáadjuk az előző mátrix vektorproduktumához.

226
00:14:43,280 --> 00:14:47,495
Utolsó lépésként egy szigmoidot tekercselek körbe itt kívülről, 

227
00:14:47,495 --> 00:14:53,093
és ennek azt kell ábrázolnia, hogy a szigmoid függvényt a kapott vektor minden egyes 

228
00:14:53,093 --> 00:14:54,740
komponensére alkalmazzuk.

229
00:14:55,940 --> 00:15:00,999
Tehát ha egyszer leírjuk ezt a súlymátrixot és ezeket a vektorokat saját szimbólumként, 

230
00:15:00,999 --> 00:15:05,828
akkor az aktivációk teljes átmenetét az egyik rétegből a másikba egy rendkívül szűk 

231
00:15:05,828 --> 00:15:10,945
és takaros kis kifejezésben kommunikálhatjuk, és ez a vonatkozó kódot sokkal egyszerűbbé 

232
00:15:10,945 --> 00:15:15,660
és gyorsabbá teszi, mivel sok könyvtár a mátrixszorzást a fenébe is optimalizálja.

233
00:15:17,820 --> 00:15:19,698
Emlékszel, hogy korábban azt mondtam, hogy ezek 

234
00:15:19,698 --> 00:15:21,460
a neuronok egyszerűen számokat tároló dolgok?

235
00:15:22,220 --> 00:15:27,716
Nos, természetesen a konkrét számok, amelyeket tartalmaznak, a betáplált képtől függnek, 

236
00:15:27,716 --> 00:15:32,657
így valójában pontosabb, ha minden neuronra úgy gondolunk, mint egy függvényre, 

237
00:15:32,657 --> 00:15:36,178
amely az előző réteg összes neuronjának kimenetét veszi, 

238
00:15:36,178 --> 00:15:38,340
és egy 0 és 1 közötti számot ad ki.

239
00:15:39,200 --> 00:15:44,959
Valójában az egész hálózat csak egy függvény, amely bemenetként 784 számot vesz fel, 

240
00:15:44,959 --> 00:15:47,060
és kimenetként 10 számot ad ki.

241
00:15:47,560 --> 00:15:52,252
Ez egy abszurdan bonyolult függvény, amely 13 000 paramétert foglal magában a súlyok 

242
00:15:52,252 --> 00:15:55,840
és torzítások formájában, amelyek bizonyos mintákat vesznek fel, 

243
00:15:55,840 --> 00:16:00,642
és amely számos mátrixvektor-termék és a szigmoid squishification függvény iterációját 

244
00:16:00,642 --> 00:16:05,335
foglalja magában, de ez mégis csak egy függvény, és bizonyos értelemben megnyugtató, 

245
00:16:05,335 --> 00:16:06,660
hogy bonyolultnak tűnik.

246
00:16:07,340 --> 00:16:09,830
Úgy értem, ha egyszerűbb lenne, milyen reményünk lenne arra, 

247
00:16:09,830 --> 00:16:12,280
hogy meg tudná oldani a számjegyek felismerésének kihívását?

248
00:16:13,340 --> 00:16:14,700
És hogyan veszi fel ezt a kihívást?

249
00:16:15,080 --> 00:16:17,282
Hogyan tanulja meg ez a hálózat a megfelelő súlyokat 

250
00:16:17,282 --> 00:16:19,360
és torzításokat pusztán az adatok megtekintésével?

251
00:16:20,140 --> 00:16:21,968
Nos, ezt fogom megmutatni a következő videóban, 

252
00:16:21,968 --> 00:16:24,901
és egy kicsit jobban beleásom magam abba is, hogy mit is csinál valójában ez 

253
00:16:24,901 --> 00:16:26,120
a bizonyos hálózat, amit látunk.

254
00:16:27,580 --> 00:16:31,133
Most van az a pont, ahol azt hiszem, azt kellene mondanom, hogy feliratkozom, 

255
00:16:31,133 --> 00:16:34,413
hogy értesüljek arról, hogy mikor jön ki videó vagy bármilyen új videó, 

256
00:16:34,413 --> 00:16:37,420
de reálisan a legtöbben nem kapnak értesítést a YouTube-tól, ugye?

257
00:16:38,020 --> 00:16:40,803
Talán őszintébben azt kellene mondanom, hogy feliratkozni, 

258
00:16:40,803 --> 00:16:44,907
hogy a YouTube ajánló algoritmusának alapjául szolgáló neurális hálózatok azt higgyék, 

259
00:16:44,907 --> 00:16:47,880
hogy szeretné, ha az adott csatorna tartalmait ajánlanák Önnek.

260
00:16:48,560 --> 00:16:49,940
Mindenesetre maradj postázva a továbbiakért.

261
00:16:50,760 --> 00:16:53,500
Nagyon köszönöm mindenkinek, aki támogatja ezeket a videókat a Patreonon.

262
00:16:54,000 --> 00:16:57,590
Ezen a nyáron kicsit lassan haladtam a valószínűségi sorozatban, 

263
00:16:57,590 --> 00:17:01,900
de a projekt után újra belevágok, úgyhogy a patrónusok várják a frissítéseket.

264
00:17:03,600 --> 00:17:05,767
A dolgok lezárásaként itt van velem Leisha Lee, 

265
00:17:05,767 --> 00:17:07,935
aki a mélytanulás elméleti oldaláról doktorált, 

266
00:17:07,935 --> 00:17:11,684
és aki jelenleg egy Amplify Partners nevű kockázati tőkebefektető cégnél dolgozik, 

267
00:17:11,684 --> 00:17:14,619
amely szívesen biztosította a videó finanszírozásának egy részét.

268
00:17:15,460 --> 00:17:18,261
Szóval Leisha, az egyik dolog, amit szerintem gyorsan fel kellene hoznunk, 

269
00:17:18,261 --> 00:17:19,119
az a szigmoid függvény.

270
00:17:19,700 --> 00:17:22,109
Ha jól értem, a korai hálózatok ezt arra használják, 

271
00:17:22,109 --> 00:17:25,884
hogy a releváns súlyozott összeget a nulla és egy közötti intervallumba szorítsák, 

272
00:17:25,884 --> 00:17:29,840
tudod, a biológiai analógia alapján, miszerint a neuronok vagy inaktívak, vagy aktívak.

273
00:17:30,280 --> 00:17:30,300
Pontosan.

274
00:17:30,560 --> 00:17:34,040
De viszonylag kevés modern hálózat használ már szigmoidot.

275
00:17:34,320 --> 00:17:34,320
Igen, igen, igen.

276
00:17:34,440 --> 00:17:35,540
Ez egy kicsit régimódi, nem?

277
00:17:35,760 --> 00:17:38,980
Igen, vagy inkább a relu sokkal könnyebben képezhetőnek tűnik.

278
00:17:39,400 --> 00:17:42,340
És a relu az egyenirányított lineáris egységet jelenti?

279
00:17:42,680 --> 00:17:48,739
Igen, ez egy ilyen függvény, ahol csak a nulla és a maximumát vesszük, ahol a adott, 

280
00:17:48,739 --> 00:17:53,801
amit a videóban magyaráztál, és amit ez a fajta motiváció, azt hiszem, 

281
00:17:53,801 --> 00:17:59,647
részben egy biológiai analógia volt, hogy a neuronok vagy aktiválódnak, vagy nem, 

282
00:17:59,647 --> 00:18:05,564
és ha átlép egy bizonyos küszöböt, akkor ez lenne az identitásfüggvény, de ha nem, 

283
00:18:05,564 --> 00:18:10,840
akkor nem aktiválódna, tehát nulla lenne, tehát ez egyfajta egyszerűsítés.

284
00:18:11,160 --> 00:18:13,999
A szigmoidok használata nem segített a képzésben, 

285
00:18:13,999 --> 00:18:17,009
vagy nagyon nehéz volt a képzés egy bizonyos ponton, 

286
00:18:17,009 --> 00:18:19,678
és az emberek egyszerűen kipróbálták a relu-t, 

287
00:18:19,678 --> 00:18:24,620
ami történetesen nagyon jól működött ezeknél a hihetetlenül mély neurális hálózatoknál.

288
00:18:25,100 --> 00:18:25,640
Rendben, köszönöm Alicia.

