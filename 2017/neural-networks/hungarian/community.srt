1
00:00:04,020 --> 00:00:10,680
Ez egy hármas szám. Hanyagul írt és alacsony, 28 × 28 képpontos felbontásban lett beolvasva.

2
00:00:10,680 --> 00:00:15,660
Az agyadnak mégsem esik nehezére 3-as számként felismernie. Szerintem érdemes egy pillanatra elgondolkodni azon,

3
00:00:15,900 --> 00:00:18,949
mennyire elképesztő, hogy az agyad minden megerőltetés nélkül képes erre.

4
00:00:18,949 --> 00:00:23,160
Hiszen ez a 3 ábra mind felismerhető hármasként

5
00:00:23,160 --> 00:00:28,060
annak ellenére, hogy az egyes képpontok nagyon különböznek egymástól.

6
00:00:28,080 --> 00:00:33,780
A szemedben lévő fényérzékeny sejtek, amelyek akkor "tüzelnek", mikor ezt a hármast látod

7
00:00:33,780 --> 00:00:36,800
nagyrészt eltérnek azoktól, amelyek ennél a hármasnál kapnak jelet.

8
00:00:37,140 --> 00:00:40,620
De valami az őrülten intelligens vizuális kérgedben felismeri, hogy

9
00:00:41,129 --> 00:00:48,139
ezek egy és ugyanazon jelentés megnyilvánulásai, miközben más képeket meg más jelentésekkel társít.

10
00:00:48,840 --> 00:00:55,040
De ha most azt mondanám neked, ülj le és írj nekem egy programot, amely beolvas egy 28 x 28-as rácsot

11
00:00:55,380 --> 00:01:01,760
és egyetlen számot ad ki 0-9 között, azt a számjegyet, amit felismerni vél?

12
00:01:02,240 --> 00:01:06,120
Nos, a feladat akkor a nevetségesen triviálisból hirtelen vért izzasztóan bonyolulttá válna.

13
00:01:06,750 --> 00:01:08,270
Hacsak nem egy szikla alatt éltél eddig,

14
00:01:08,270 --> 00:01:14,599
valószínűleg aligha kell hangsúlyoznom a gépi tanulás és a neurális hálózatok fontosságát mind a jelenben mind a jövőre nézve.

15
00:01:14,640 --> 00:01:18,420
Most arról fogok beszélni, hogy pontosan mi is az a neurális hálózat.

16
00:01:18,660 --> 00:01:24,220
Feltételezve az előismeretek hiányát és megpróbálva matekosan vizualizálni a működést.

17
00:01:24,560 --> 00:01:28,300
Mindössze annyit remélek, hogy a végén már nem gondolod azt, hogy ez egy motivációval rendelkező struktúra,

18
00:01:28,380 --> 00:01:34,399
és úgy érzed, tudod mire gondolj, amikor valahol olvasod vagy hallod a "neurális hálózat" kifejezést.

19
00:01:34,950 --> 00:01:40,249
Ez a videó csak a struktúrával foglalkozik, a következő szól a tanulásról.

20
00:01:40,530 --> 00:01:45,950
Itt és most megpróbálunk egy olyan neurális hálózatot létrehozni, amely megtanulja felismerni a kézzel írott számokat.

21
00:01:49,270 --> 00:01:51,329
Ez egy klasszikus példa a téma bemutatására.

22
00:01:51,520 --> 00:01:56,759
És itt örömmel ragaszkodom a status quo-hoz, mert a két videó végén mutatok majd néhány forrást,

23
00:01:56,760 --> 00:02:02,099
ahonnan még többet lehet megtudni, illetve ahonnan ez a program is letölthető majd,

24
00:02:02,100 --> 00:02:04,100
hogy bárki elbabrálhasson vele a számítógépén.

25
00:02:04,750 --> 00:02:08,970
A neurális hálózatoknak nagyon sok változata létezik,

26
00:02:08,970 --> 00:02:11,970
és az elmúlt években nagyon felggyorsult ezeknek a kutatása,

27
00:02:12,130 --> 00:02:19,019
de e két bevezető videóban én csak az alapfelszereltségű, extrák nélküli verzióval foglalkozom.

28
00:02:19,300 --> 00:02:21,040
Ez az előfeltétele a leghatékonyabb

29
00:02:21,040 --> 00:02:24,510
modern változatok megértésének és

30
00:02:24,760 --> 00:02:28,199
hidd el, hogy ez még így is elég bonyolult lesz.

31
00:02:28,690 --> 00:02:32,820
Mégis ebben a legegyszerűbb formájában is képes lesz megtanulni a kézzel írt számjegyek felismerését.

32
00:02:32,820 --> 00:02:36,180
Ami azért elég menő képesség egy számítógép részéről.

33
00:02:37,120 --> 00:02:41,960
Ugyanakkor azt is látni fogod, hogy hogyan nem fogja beteljesíteni néhány reményünket.

34
00:02:43,090 --> 00:02:48,179
Mint a neve is sugallja, a neurális hálózatokat az agy inspirálta.

35
00:02:48,520 --> 00:02:51,389
De nézzük közelebbről: mik azok a neuronok és hogyan kapcsolódnak egymáshoz?

36
00:02:52,090 --> 00:02:57,750
Amikor azt mondom, neuron, egy olyan dologra gondolj, ami egy számot tartalmaz.

37
00:02:58,209 --> 00:03:02,129
Méghozzá egy 0 és 1 közötti számot - ez az egész tényleg nem több ennél.

38
00:03:03,430 --> 00:03:11,130
A hálózat egy halom neuronnal kezdődik, melyek mindegyike a bemeneti 28x28-as kép egy képpontjának felel meg.

39
00:03:11,400 --> 00:03:12,460
Ez összesen 784 neuron.

40
00:03:12,460 --> 00:03:20,240
Mindegyik olyan számot tartalmaz, amely a megfelelő pixel szürkeárnyalatos értékét mutatja.

41
00:03:20,769 --> 00:03:24,299
A 0-s, fekete pixeltől egészen az 1-es, fehér pixelig.

42
00:03:24,910 --> 00:03:30,419
Ezt a számot a neuronban aktivitásnak nevezik.

43
00:03:30,420 --> 00:03:33,959
Ha a neuron aktivitása magas, akkor a neuron tüzel.

44
00:03:36,260 --> 00:03:41,559
Ez a 784 neuron alkotja a hálózatunk első rétegét.

45
00:03:45,990 --> 00:03:51,289
Most az utolsó rétegre ugrunk: itt tíz neuron van, mindegyik egy számjegyet képvisel.

46
00:03:51,570 --> 00:03:56,239
Az aktivitása ezeknek a neuronoknak is egy 0 és 1 közötti szám, ami azt mutatja meg,

47
00:03:56,880 --> 00:04:00,049
mennyire gondolja úgy a rendszer, hogy ennek a számnak felel meg a bemeneti kép.

48
00:04:00,720 --> 00:04:05,990
Van még néhány "rejtett réteg" is a hálózatban,

49
00:04:06,180 --> 00:04:07,770
amit egyelőre jejlöljünk csak egy

50
00:04:07,770 --> 00:04:13,549
óriási kérdőjellel, hogy mégis hogy a manóba fogjuk megvalósítani a számfelismerés folyamatát…

51
00:04:13,740 --> 00:04:20,209
Ehhez a hálózathoz két rejtett réteget választottam, egyenként 16 neuronnal, és ez valójában egy önkényes választás.

52
00:04:20,609 --> 00:04:24,889
Amiatt lett két réteg, hogy pontosan minek is akarom kitenni a struktúrát mindjárt, 

53
00:04:25,350 --> 00:04:29,179
a 16 meg csak egyszerűen csak egy szép szám, jól illeszkedik a képernyőnkre.

54
00:04:29,180 --> 00:04:32,209
Valójában egy csomót lehet kisérletezni a legmegfelelőbb struktúrával…

55
00:04:32,730 --> 00:04:38,329
Az aktivitások a hálózat adott rétegében meghatározzák a következő réteg aktivitásait.

56
00:04:38,760 --> 00:04:45,349
És persze a hálózat mint információfeldolgozó mechanizmus lényegi kérdése pontosan az,

57
00:04:45,570 --> 00:04:48,409
hogy hogyan eredményez az egyik réteg aktivitása a következő rétegben aktivitást.

58
00:04:48,900 --> 00:04:54,859
Az egész nagyjából ahhoz hasonló elven működik, ahogy a biológiai hálózatokban is egy neuron-csoport tüzelésekor

59
00:04:55,410 --> 00:04:57,410
más neuronok is tüzelni kezdenek.

60
00:04:57,570 --> 00:04:58,340
A hálózat,  amit most bemutatok

61
00:04:58,340 --> 00:05:03,019
már be lett tanítva arra, hogy felismerje a számjegyeket. Hadd mutassam meg, mit értek ezalatt.

62
00:05:03,140 --> 00:05:06,580
Ha betáplálunk egy képet, 

63
00:05:06,640 --> 00:05:11,780
ami a bemeneti réteg mind a 784 neuronját aktivizálja, a pixelek fényerőssége szerint,

64
00:05:12,330 --> 00:05:17,029
akkor az aktivitások mintázata egy nagyon specifikus mintát generál a következő rétegben,

65
00:05:17,190 --> 00:05:19,309
ami a rákövetkező rétegben generál egy aktivitási mintázatot,

66
00:05:19,440 --> 00:05:22,190
ami végül a kimeneti rétegben hoz létre egy mintát.

67
00:05:22,350 --> 00:05:29,359
És a kimenti réteg legfényesebb neuronja lesz végül a rendszer tippje arra nézve, hogy milyen szám van a képen.

68
00:05:32,070 --> 00:05:36,859
És mielőtt belevetnénk magunkat a matekba, hogy hogyan hatnak egymásra a rétegek vagy hogyan működik a betanítás,

69
00:05:37,140 --> 00:05:43,069
beszéljünk inkább arról, hogy egyáltalán miért lehet értelme elvárni egy réteges struktúrától, hogy intelligensen viselkedjen?

70
00:05:43,800 --> 00:05:48,260
Mire számítunk itt? Mégis milyen viselkedést remélünk a középső rétegektől?

71
00:05:48,860 --> 00:05:56,720
Nos, amikor mi emberek számjegyeket ismerünk fel, különböző komponenseket illesztünk össze: a 9-es felül egy karika, jobb oldalon meg egy vonal.

72
00:05:57,260 --> 00:06:01,280
A 8-as szintén egy karika felül, plusz még egy karika alul.

73
00:06:02,020 --> 00:06:06,599
A 4-est meg lényegében 3 konkrét vonal alkotja, stb.

74
00:06:07,180 --> 00:06:11,970
Namármost egy tökéletes világban remélhetnénk pl. azt, hogy az utolsó előtti rétegben minden neuron

75
00:06:12,640 --> 00:06:14,729
egy ilyen komponensnek felel meg, és 

76
00:06:14,890 --> 00:06:19,740
ha bármikor olyan képet töltünk be, aminek karika van a tetején, pl. 8-ast vagy 9-est

77
00:06:19,870 --> 00:06:21,220
akkor lesz olyan konkrét neuron

78
00:06:21,220 --> 00:06:27,749
aminek az aktivitása közel lesz az egyhez. És persze nem csak egyetlen féle karika, hanem mindenféle

79
00:06:28,090 --> 00:06:35,039
felül elhelyezkedő hurok-szerű mintázat aktivizálja ezt a neuront. Így aztán a harmadik rétegből az utolsóba lépve

80
00:06:35,380 --> 00:06:39,960
csak azt kell megtanulnia, hogy mely részkomponensek kombinációja felel meg egy számjegynek.

81
00:06:40,510 --> 00:06:42,810
Ezzel persze csak kicsit odébbgörgettük a problémát,

82
00:06:42,910 --> 00:06:49,019
Mert hogyan ismerné fel a komponenseket, vagy hogyan tudná, melyikhez melyik szám tartozik, és még mindig nem is beszéltem róla,

83
00:06:49,020 --> 00:06:52,829
hogyan hat egy réteg a következőre, de még egy kicsit időzzünk el itt.

84
00:06:53,650 --> 00:06:56,340
Egy hurok felismerése is tovább bontható.

85
00:06:56,860 --> 00:07:02,550
Ennek egyik ésszerű módja az lenne, hogy először felismerjük a különböző kis éleket, amelyek felépítik.

86
00:07:03,520 --> 00:07:08,910
Hasonlóképpen azt a hosszú vonalat, ami az 1-es, 4-es és 7-es számjegyekben is szerepel,

87
00:07:08,910 --> 00:07:14,279
is tekinthetjük egy hosszú élnek, vagy akár több kisebb él meghatározott mintázatának is.

88
00:07:14,740 --> 00:07:19,379
Szóval talán reménykedhetünk pl. abban, hogy a második réteg minden neuronja

89
00:07:20,290 --> 00:07:22,650
ezeknek a kis meghatározott éleknek felel meg.

90
00:07:23,230 --> 00:07:28,259
Lehet, hogy amikor ez a kép érkezik be, az összes olyan neuron tüzel

91
00:07:28,720 --> 00:07:31,649
amely ezt a nyolc-tíz egyedi kis élt reprezentálja, 

92
00:07:31,930 --> 00:07:36,930
amelyek aktiválják a felső hurokhoz és a hosszú függőleges vonalhoz kapcsolódó neuronokat,

93
00:07:37,300 --> 00:07:39,599
Amelyek végül a 9-eshez tartozó neuront kapcsolják be.

94
00:07:40,300 --> 00:07:41,100
Az már más kérdés,

95
00:07:41,100 --> 00:07:47,070
hogy ez tényleg így zajlik-e le a hálózatunkban - de erre még visszatérek, miután megnéztük, hogyan kell betanítani a hálózatot.

96
00:07:47,350 --> 00:07:52,170
De ettől még persze reménykedhetünk benne, hogy ez ennek a réteges felépítésnek a célja.

97
00:07:53,020 --> 00:07:59,340
Ráadásul gyanús, hogy a szélek és minták ilyesféle felismerése nagyon jól jönne más képfelismerési feladatokhoz is.

98
00:07:59,740 --> 00:08:06,749
És még a képfelismerésen túl is mindenféle intelligens dolog lebontható több absztrakciós rétegre.

99
00:08:07,690 --> 00:08:14,670
A beszédfelismerés például a nyers hanganyagból a különféle hangok kiválasztását jelenti, amelyek egyes szótagokat építenek fel,

100
00:08:15,070 --> 00:08:19,829
melyek együttese szavakat alkot, melyek együttese pedig kifejezéseket és absztrakt gondolatokat stb.

101
00:08:20,770 --> 00:08:25,710
De térjünk vissza arra, hogy mindez hogy is működik. Képzeld el, 

102
00:08:25,710 --> 00:08:30,449
hogy azt tervezed éppen meg, hogyan határozza meg egy réteg aktivitása a következő rétegét.

103
00:08:30,670 --> 00:08:35,879
A cél létrehozni egy olyan mechanizmust, amely elképzelhetően kombinálja a képpontokat élekbe,

104
00:08:35,880 --> 00:08:41,430
vagy éleket mintákba vagy mintákat számjegyekbe. Vegyünk egy jól meghatározott esetet:

105
00:08:41,950 --> 00:08:44,189
Tegyük fel, hogy azt próbáljuk elérni,

106
00:08:44,380 --> 00:08:50,430
hogy a második réteg egyik neuronja meg tudja mondani, hogy van-e itt él vagy sem.

107
00:08:50,950 --> 00:08:54,960
A kérdés az, hogy milyen paraméterek kellenek a hálózatnak ahhoz,

108
00:08:55,270 --> 00:09:02,490
milyen tárcsákat kell tudjunk úgy tekergetni, hogy azzal képesek legyünk kifejezni ezt a mintát.

109
00:09:02,590 --> 00:09:07,290
Vagy bármilyen másik képpontmintát, olyanokat, amiket élekből vagy hurkokból összeállíthatunk?

110
00:09:08,290 --> 00:09:15,389
Azt fogjuk tenni, hogy súlyokat rendelünk e neuron és az első réteg neuronjai közé.

111
00:09:15,850 --> 00:09:17,850
Ezek a súlyok csak számok.

112
00:09:18,190 --> 00:09:25,590
Aztán vesszük az összes aktivitást az első rétegből és kiszámoljuk a súlyozott összegüket.

113
00:09:27,370 --> 00:09:31,680
Szerintem szemléletes ezekre a súlyokra is úgy tekinteni, mintha ők is a saját kis rácsukba rendeződnének:

114
00:09:31,680 --> 00:09:37,079
zöld pixeket fogok használni a pozitív súlyok jelölésére, és pirosat a negatívokéra, 

115
00:09:37,240 --> 00:09:41,670
míg a pixelek fényereje a súlyok abszolút értékét jelöli.

116
00:09:42,400 --> 00:09:45,840
Na most, ha szinte az összes képponthoz tartozó súlyunk nulla,

117
00:09:46,150 --> 00:09:49,079
kivéve néhány pozitív súlyt ebben a régióban, amelyet vizsgálunk,

118
00:09:49,480 --> 00:09:51,310
akkor lényegében a súlyozott összeg kiszámítása

119
00:09:51,310 --> 00:09:57,690
nem más, mint egyszerűen csak összegyűjteni a pixelértékeket ebben a régióban.

120
00:09:58,870 --> 00:10:04,440
És ha tényleg azt akarod, hogy itt egy élt találj, akkor célszerű negatív súlyt társítani

121
00:10:04,900 --> 00:10:06,900
a környező képpontokhoz.

122
00:10:07,030 --> 00:10:12,660
Ezután az összeg akkor lesz a legnagyobb, ha a középső képpontok világosak, de a környező képpontok sötétek.

123
00:10:14,279 --> 00:10:18,169
Ilyen súlyozott összeg számításakor, bármilyen érték kijöhet,

124
00:10:18,240 --> 00:10:23,180
de ehhez a hálózathoz az értékeknek 0 és 1 közé kell esnie.

125
00:10:23,730 --> 00:10:26,599
Így aztán az összeget valamiféle függvénybe kell csomagolnunk,

126
00:10:26,910 --> 00:10:32,000
ami megoldja ezt az "összenyomást".

127
00:10:32,190 --> 00:10:37,249
Az egyik általános függvény, amivel ez megoldható a szigmoid, vagy más néven logisztikus függvény.

128
00:10:37,980 --> 00:10:43,339
Itt lényegében a nagyon negatív értékekből egy nulla közeli érték, nagyon pozitívakból 1 közeli érték lesz,

129
00:10:43,339 --> 00:10:46,398
0 környezetében pedig monoton nő.

130
00:10:49,080 --> 00:10:56,029
Vagyis a neuron aktivitása itt alapvetően attól függ, hogy mennyire pozitív a súlyozott összeg.

131
00:10:57,450 --> 00:11:01,819
Lehet viszont, hogy nem akarjuk, hogy a neuron már akkor tüzeljen, ha a súlyozott összeg nagyobb 0-nál.

132
00:11:02,100 --> 00:11:06,260
Lehet, hogy azt szeretnénk, hogy ez csak akkor történjen meg, ha 10-nél is nagyobb.

133
00:11:06,630 --> 00:11:10,279
Vagyis kell egy küszöbérték, ameddig a neuron inaktív marad.

134
00:11:10,860 --> 00:11:16,099
Ennek érdekében csak hozzá kell adnunk a kifejezéshez egy negatív számot,

135
00:11:16,529 --> 00:11:19,669
mielőtt beküldenénk a szigmoid függvénybe.

136
00:11:20,220 --> 00:11:22,730
Ezt a számot torzításnak nevezzük.

137
00:11:23,310 --> 00:11:29,060
Tehát a súlyok megmondják, milyen pixel-mintázatra reagál a neuron a második rétegben, 

138
00:11:29,220 --> 00:11:35,450
míg a torzítás meghatározza, hogy mi az a minimális súlyozott összeg, ami felett a neuron tüzelni kezd.

139
00:11:35,910 --> 00:11:37,910
És ez csak egyetlen neuron.

140
00:11:38,120 --> 00:11:41,940
A második réteg összes többi neuronja is ugyanúgy kapcsolódik

141
00:11:42,320 --> 00:11:50,620
az első réteg 784 neuronjához, és mind a 784 kapcsolathoz tartozik egy súly is.

142
00:11:51,330 --> 00:11:57,739
Továbbá mindegyiknek van egy torzítási értéke is.

143
00:11:58,020 --> 00:12:01,909
Ez azért nem kevés számítanivaló: ez a rejtett réteg 16 neuronnal

144
00:12:02,010 --> 00:12:08,270
összesen 784 x 16 súly + még 16 torzítás.

145
00:12:08,490 --> 00:12:14,029
És mindez csak az első rétegtől a másodikig tartó út. A többi réteg között is van

146
00:12:14,029 --> 00:12:17,208
egy halom súly és torzítás.

147
00:12:17,760 --> 00:12:20,680
Mindent számbavébe ennek a hálózatnak összesen

148
00:12:21,280 --> 00:12:23,920
nagyjából 13.000 súlya és torzítása van.

149
00:12:24,280 --> 00:12:29,540
13.000 tekergethető tárcsa, amivel a hálózat működése finomhangolható.

150
00:12:30,520 --> 00:12:32,520
Tehát, amikor a tanulásról beszélünk, 

151
00:12:32,530 --> 00:12:40,199
az lényegében az a folyamat, ahogy a számítógép megtalálja azt a beállítást,

152
00:12:40,200 --> 00:12:42,190
amivel az összes megadott problémára jó megoldást tud adni.

153
00:12:42,190 --> 00:12:43,000
Egy gondolatkísérlet -

154
00:12:43,000 --> 00:12:49,979
szórakoztató és szörnyűséges: milyen lenne leülni és ezeket a súlyokat és torzításokat mind kézzel

155
00:12:50,380 --> 00:12:56,159
beállítgatni úgy, hogy a második réteg felismerje a széleket, a harmadik réteg a mintákat stb.

156
00:12:56,350 --> 00:13:01,440
Számomra ez a kielégítőbb gondolat annál, mintha a hálózat ezen részére csak mint egy fekete dobozra gondolnék.

157
00:13:01,870 --> 00:13:04,349
Mert amikor a hálózat nem végzi jól a munkáját,

158
00:13:04,600 --> 00:13:11,370
de mi mégis tisztában vagyunk azzal, hogy a súlyok és torzítások hogyan épülnek a rendszerbe, van ötletünk, 

159
00:13:11,680 --> 00:13:16,289
hogy hol kezdjünk neki a struktúra javításának. Vagy amikor bár működik a hálózat,

160
00:13:16,290 --> 00:13:18,290
de nem azért és nem úgy, mint ahogy előre elképzeltük.

161
00:13:18,310 --> 00:13:25,169
A hálózat működésének a mélyére ásni segít letesztelni a kezdeti feltételezéseinket, és feltárni

162
00:13:25,180 --> 00:13:26,350
 a lehetséges megoldások teljes spektrumát.

163
00:13:26,350 --> 00:13:30,600
Amúgy a tényleges függvényt kicsit fárasztó lenne leírni, nem?

164
00:13:32,350 --> 00:13:38,460
Hadd mutassak egy tömörebb jelölést a kapcsolatok leírására. Ha később olvasol

165
00:13:38,460 --> 00:13:40,460
a témában, akkor is így fogsz ezzel találkozni.

166
00:13:41,110 --> 00:13:45,810
Rendezzük a réteg összes aktivitását egy oszlopvektorba.

167
00:13:47,470 --> 00:13:52,320
Aztán rendezzük az összes súlyt egy mátrixba, aminek egy sora

168
00:13:52,900 --> 00:13:57,659
a hálózat egy rétege és a következő réteg egy neuronja közötti kapcsolatot írja le.

169
00:13:58,060 --> 00:14:03,599
Ami azt jelenti, hogy az aktiválások súlyozott összege az első rétegben megegyezik 

170
00:14:04,000 --> 00:14:09,330
a mátrix megfelelő sorvektorának és az oszlopvektornak a szorzatával.

171
00:14:13,540 --> 00:14:18,380
Mellesleg a gépi tanulás nagy része főleg a lineáris algebra jó megértésére épül.

172
00:14:18,380 --> 00:14:26,940
Szóval ha bárki a mátrixok vizuális megértésére és mátrix szorzás megértésére vágyna, nézze meg a lineáris algebra sorozatomat is.

173
00:14:27,250 --> 00:14:28,839
Kiváltképp a harmadik fejezetét.

174
00:14:28,839 --> 00:14:35,759
A kifejezésünkre visszatérve - ahelyett, hogy egyenként hozzáadnánk minden torzítást,

175
00:14:36,010 --> 00:14:42,209
inkább vektorba szervezzük ezeket is, és hozzáadjuk a mátrix-szorzathoz.

176
00:14:42,910 --> 00:14:44,040
Aztán végső lépésként

177
00:14:44,040 --> 00:14:47,250
az egész kifejezés bekerül a szigmoid függvénybe.

178
00:14:47,250 --> 00:14:51,899
És ez azt jelenti, hogy a szigmoid függvényt az eredményként előálló vektor 

179
00:14:52,420 --> 00:14:54,570
mindegyik elemére alkalmazni fogjuk.

180
00:14:55,510 --> 00:15:00,749
Végül ha a súlymátrixot és ezeket a vektorokat saját szimbólumukkal jelöljük,

181
00:15:01,000 --> 00:15:07,589
tiszta és tömör formában tudjuk leírni az aktivitás átadását az egyik rétegről a másikra.

182
00:15:07,930 --> 00:15:15,000
Ezáltal a kód fontos része sokkal egyszerűbbé és gyorsabbá válik, mivel számos könyvtár optimalizálja a mátrix-műveleteket.

183
00:15:17,560 --> 00:15:21,359
Korábban megegyeztünk, hogy ezek a neuronok egyszerűen olyan dolgok, amelyek számokat tartalmaznak.

184
00:15:21,790 --> 00:15:26,250
Persze a számok attól függnek, hogy milyen képet töltöttünk be.

185
00:15:27,790 --> 00:15:32,940
Valójában pontosabb úgy fogalmazni, hogy minden egyes neuron egy függvény.

186
00:15:33,070 --> 00:15:38,070
Amelynek az előző réteg összes neuronja egy változója, és egy nulla és egy közötti számot ad eredményül.

187
00:15:38,800 --> 00:15:42,270
Valójában az egész neurális hálózat csak egy függvény.

188
00:15:42,760 --> 00:15:47,010
Ami 784 számot kap inputként és 10-et ad vissza.

189
00:15:47,470 --> 00:15:48,700
Egy rettenetesen komplikált függvény,

190
00:15:48,700 --> 00:15:56,249
amely 13.000 paramétert tartalmaz súlyok és torzítások formájában, és amely

191
00:15:56,250 --> 00:16:00,270
számos mátrixszorzaton alkalmazza a szigmoid függvényt,

192
00:16:00,610 --> 00:16:06,390
de ettől még csak egy függvény. És amúgy meg még jó is, ha bonyolultnak tűnik.

193
00:16:06,390 --> 00:16:12,239
Hiszen ha egyszerű lenne, akkor hogyan várhatnánk tőle, hogy felismerjen számjegyeket?

194
00:16:12,960 --> 00:16:19,559
És hogyan teszi ezt? Hogyan tudja beállítani saját magát pusztán azáltal, hogy adatokat olvas be?

195
00:16:20,080 --> 00:16:26,039
Ez az, amit a következő videóban mutatok be, illetve elmagyarázom, hogy pontosan mit is csinál ez a különleges hálózat.

196
00:16:27,130 --> 00:16:32,640
Most kéne azt mondanom, hogy iratkozz fel, hogy ne maradj le semmilyen újdonságról,

197
00:16:32,760 --> 00:16:37,560
de legtöbbünk gondolom amúgy sem kap értesítést a Youtube-tól.

198
00:16:37,560 --> 00:16:42,260
Vagy még őszintében kérhetném ezt azért, hogy hadd higgye

199
00:16:42,459 --> 00:16:47,639
a Youtube videó-ajánló neurális hálózata azt, hogy ettől a csatornától szeretnél tartalmat látni.

200
00:16:48,250 --> 00:16:50,250
Akárhogy is, figyeljétek a posztjaimat!

201
00:16:50,410 --> 00:16:53,550
Köszönjük mindenkinek, aki támogatja ezeket a videókat a patreonon!

202
00:16:53,589 --> 00:16:56,759
A nyár folyamán kicsit lassan haladtam a valószínűségi sorozatommal.

203
00:16:56,760 --> 00:17:01,379
De ez után a projekt után folytatom újra, így a patreonosok is ott tudják figyelni a frissítéseket.

204
00:17:03,310 --> 00:17:05,550
A téma lezárásaként itt van velem Lisha Li, aki a PhD-jét 

205
00:17:05,550 --> 00:17:12,029
a deep learning (mély tanulás) elméleti oldalából írta, és jelenleg az Amplify Partners kockázatitőke-cégnél dolgozik,

206
00:17:12,030 --> 00:17:16,530
amely részben ezt a videót is finanszírozta.

207
00:17:16,530 --> 00:17:19,109
Lisha, amiről gyorsan szót kell ejtenünk, az a szigmoid függvény.

208
00:17:19,180 --> 00:17:24,780
A korai hálózatok ezt használták, hogy a súlyozott összeget 0 és 1 közé normalizálják,

209
00:17:24,980 --> 00:17:30,340
a biológiai neuronok analógiájaként az inaktív és aktív állapot leképezésére …
(Lisha) - Pontosan

210
00:17:30,360 --> 00:17:36,320
(3B1B) - De viszonylag kevés modern hálózat használ manapság szigmoidot. Kicsit elavult már?
(Lisha) - Igen, vagy inkább

211
00:17:36,370 --> 00:17:42,780
a ReLU sokkal könnyebben tanítható.
(3B1B) - A "ReLU"  korrigált lineáris egységet jelent, ugye?

212
00:17:42,780 --> 00:17:48,839
(Lisha) - Igen, ez egy olyan függvény, ami max(a,0)-t ad.

213
00:17:49,120 --> 00:17:53,670
Ahogy a videóban is rámutattál, és ami ezt motiválta,

214
00:17:54,610 --> 00:17:56,610
az részben biológiai,

215
00:17:56,620 --> 00:17:58,179
és azzal analóg,

216
00:17:58,179 --> 00:18:03,089
ahogy a neuronok vagy aktiválódnak vagy sem. Ha egy bizonyos küszöbérték fölé megy, 

217
00:18:03,250 --> 00:18:05,250
akkor a függvény saját magát adja vissza,

218
00:18:05,290 --> 00:18:10,439
de ha nem, akkor nem akarunk aktiválást, így az érték legyen nulla - ez egyfajta egyszerűsítés.

219
00:18:10,720 --> 00:18:14,429
A szigmoidot egy ponton túl túl nehéz volt betanítani,

220
00:18:14,429 --> 00:18:19,589
így aztán kipróbálták a ReLU-t, és az meg működött,

221
00:18:20,110 --> 00:18:22,140
méghozzá nagyon jól

222
00:18:22,690 --> 00:18:25,090
a mély neurális hálózatoknál.
(3B1B) - Rendben.

223
00:18:25,090 --> 00:18:26,060
Köszönöm, Lisha.

