1
00:00:04,020 --> 00:00:10,680
這是一個隨意書寫的28*28像素、解析度很低的數字 3

2
00:00:10,680 --> 00:00:15,660
但你的大腦一看見就能輕鬆辨識出來 ，我想要你好好欣賞這點

3
00:00:15,900 --> 00:00:18,949
人腦能夠毫無障礙地辨識是非常厲害的

4
00:00:18,949 --> 00:00:23,160
我的意思是，這個、這個、還有這個，都能被識別為 3

5
00:00:23,160 --> 00:00:28,060
即使前後圖像的圖形組成有很大差異

6
00:00:28,080 --> 00:00:33,780
當你看到這張 3 在眼中所激發的感光細胞

7
00:00:33,780 --> 00:00:36,800
跟當你看到這張 3 所激發的感光細胞是非常不同的

8
00:00:37,140 --> 00:00:40,610
但在你驚人聰明的視覺皮層的處理下

9
00:00:41,120 --> 00:00:48,140
能將這兩個 3 視為同一個概念，同時將其他圖像視為不同的概念

10
00:00:48,840 --> 00:00:52,760
要是我要你：「嘿！坐下來幫我寫個程式， 」

11
00:00:52,760 --> 00:00:56,720
「輸入像這個 28*28 像素的數字圖像」

12
00:00:56,720 --> 00:01:02,220
「接著輸出該程式認為的 0 到 10 之間的一個數字 ，必須跟你認為的一樣。」

13
00:01:02,220 --> 00:01:06,120
這個任務將不再是家常便飯，而變得嚇死人的困難

14
00:01:06,750 --> 00:01:08,270
除非你是山頂洞人

15
00:01:08,270 --> 00:01:14,599
我想不用再強調機器學習和神經網路之間，對未來發展的關聯性和重要性

16
00:01:14,640 --> 00:01:18,410
我現在要向你展示神經網路究竟是什麼

17
00:01:18,660 --> 00:01:20,000
假設你沒有相關背景知識

18
00:01:20,000 --> 00:01:22,000
我會視覺化神經網路的運作

19
00:01:22,000 --> 00:01:24,260
並且把它當作一門數學，不僅僅是當下流行詞語

20
00:01:24,860 --> 00:01:29,080
我希望你將能理解為什麼神經網路是長這個樣子

21
00:01:29,080 --> 00:01:34,400
當你看到或聽到機器藉著神經網路來「學習」時
是了解其意涵的

22
00:01:35,220 --> 00:01:38,420
這支影片將解釋神經網路的構造

23
00:01:38,440 --> 00:01:40,360
而下一部影片將解釋機器學習

24
00:01:40,520 --> 00:01:45,940
我們要做的是打造一個可以辨識手寫數字的神經網路

25
00:01:49,420 --> 00:01:52,500
這是介紹這種主題很典型的範例

26
00:01:52,500 --> 00:01:56,420
我樂於保持這種模式，因為在看完兩支影片後

27
00:01:56,420 --> 00:02:01,000
我會給你一些很好的網站，你可以在那裡學到很多，並且下載程式碼

28
00:02:01,100 --> 00:02:03,560
在你的電腦裡好好研究

29
00:02:04,740 --> 00:02:08,960
神經網路發展成很多很多不同類型

30
00:02:08,970 --> 00:02:11,970
而且近年來對這些的研究有爆炸性的趨勢

31
00:02:12,130 --> 00:02:19,019
但這兩支入門影片只會帶你來認識，最簡單的一種神經網路：「多層感知機」(MLP) 最基本的樣子

32
00:02:19,300 --> 00:02:21,660
這是必要的入門知識

33
00:02:21,660 --> 00:02:24,520
對於將來要理解現在任何一種強大的神經網路

34
00:02:24,760 --> 00:02:28,199
而且相信我，今天的主題已經夠複雜了，足以讓你腦袋打結

35
00:02:28,690 --> 00:02:32,820
即使是這麼簡單的神經網路也可以經由學習來分辨手寫數字

36
00:02:32,820 --> 00:02:36,180
這對電腦來說是非常酷的一件事

37
00:02:37,120 --> 00:02:41,960
而且與此同時你也將看到神經網路不盡人意的地方

38
00:02:43,090 --> 00:02:46,900
神經網路一如其名，是啟發自生物的大腦神經結構

39
00:02:46,900 --> 00:02:48,520
讓我們來剖析它吧

40
00:02:48,520 --> 00:02:51,620
何謂神經元，又是什麼機制讓它們連在一起的?

41
00:02:52,080 --> 00:02:57,740
現在，當我說「神經元」，我要你聯想到它是乘載一個數字的容器

42
00:02:58,200 --> 00:03:02,120
基本是介於 0 和 1 之間的數字，但實際上不止於此

43
00:03:03,420 --> 00:03:11,120
例如：神經網路以輸入圖像的每個像素，對應到每個神經元作為輸入

44
00:03:11,720 --> 00:03:20,240
也就是說輸入層總共有 784 個神經元，每個都有乘載數字 ，每個數字代表了對應像素的灰階值

45
00:03:20,760 --> 00:03:24,300
灰階值 0 即黑色，1 即白色

46
00:03:24,900 --> 00:03:28,480
這些在神經元中的數字稱為「激勵值」

47
00:03:28,480 --> 00:03:30,420
在此你可能注意到

48
00:03:30,420 --> 00:03:33,960
每當神經元激勵值越高，該神經元就越亮

49
00:03:36,260 --> 00:03:41,559
於是全部的 784 個神經元，組成了神經網路的第一層

50
00:03:45,990 --> 00:03:51,289
我們現在跳到最後一層，這層有 10 個神經元，各自表示 0 到 9 的數字

51
00:03:51,570 --> 00:03:56,239
同樣在這邊的神經元也各自有著介於 0 到 1 的激勵值

52
00:03:56,880 --> 00:04:02,400
表示對於給定的圖像，神經網路對於實際數字的判斷結果

53
00:04:02,460 --> 00:04:06,180
在輸入層和輸出層之間，有數個「隱藏層」

54
00:04:06,180 --> 00:04:07,380
現在在本入門影片裡

55
00:04:07,380 --> 00:04:13,540
對於神經網路是如何進行判斷的，我們只能先把它看做是巨大的問號

56
00:04:13,740 --> 00:04:17,660
本影片展示的視覺化神經網路，我設計了兩個隱層，個別搭載16個神經元

57
00:04:17,660 --> 00:04:20,600
這只是擺好看的設定

58
00:04:20,600 --> 00:04:24,880
老實說之所以選擇兩個隱層，是基於視覺化讓你看得清楚的考量，待會解釋

59
00:04:25,340 --> 00:04:28,120
而安排 16 個神經元，只是為了符合版面，也是為了讓你看得清楚

60
00:04:28,540 --> 00:04:32,200
在實際應用上，神經網路的結構經實驗不斷調整可以變得非常巨大且特殊

61
00:04:32,730 --> 00:04:38,329
神經網路操作一層激勵值的方式，會決定下一層的激勵值

62
00:04:38,760 --> 00:04:45,349
這正是神經網路的核心價值，不同以往的資料處理技術，現在發展成只要輸入

63
00:04:45,570 --> 00:04:48,409
激勵值從上一層傳到下一層，最後輸出足夠正確的結果

64
00:04:48,900 --> 00:04:54,859
神經網路的本質就是模仿生物的大腦，就像一叢腦細胞被激發

65
00:04:55,410 --> 00:04:57,410
引發其他神經細胞的串聯反應

66
00:04:57,570 --> 00:04:58,340
我現在展示的

67
00:04:58,340 --> 00:05:01,560
這個神經網路已經訓練完成了，可以準確辨識圖像中的數字

68
00:05:01,560 --> 00:05:03,560
讓我來解釋「傳遞激勵值」這點

69
00:05:03,560 --> 00:05:06,580
意思是：當你輸入一張 28x28 像素的圖像，它將點亮

70
00:05:06,640 --> 00:05:11,780
所有 784 個神經元 每個都比照對應像素的灰階值，來決定自己的激勵值

71
00:05:12,330 --> 00:05:17,029
決定的數值分布狀態會影響下一層被啟動的神經元的分布

72
00:05:17,190 --> 00:05:19,309
又會導致下一層不同的分布

73
00:05:19,440 --> 00:05:22,190
最後抵達輸出層，輸出層的神經元也會有特定的分布

74
00:05:22,350 --> 00:05:29,359
而最亮的那個就是神經網路所認為最有可能答對的答案

75
00:05:32,070 --> 00:05:36,859
但在一腳踏進數學之前，要先知道上層如何影響下層，而且機器學習為什麼會有用

76
00:05:37,140 --> 00:05:43,440
為什麼我們認為層狀結構會像這樣聰明地運作是非常合理的？

77
00:05:43,800 --> 00:05:48,260
我們在期待什麼呢？ 我們最想要神經網路的隱藏層怎麼運作呢？

78
00:05:48,860 --> 00:05:53,860
在你或我在辨識圖中的數字時我們會把各種筆畫拼湊在一起。

79
00:05:54,320 --> 00:05:57,200
一個 9 字，上面有圓圈，而在右邊有一條直線

80
00:05:57,260 --> 00:06:01,280
一個 8 字，上面也有一个圓圈但在下面與另一個圓圈相連

81
00:06:02,020 --> 00:06:06,599
一個 4 基本上可以拆解成就像那些特定的筆畫

82
00:06:07,180 --> 00:06:11,970
一個理想的情況中我們會希望第二層的每個神經元

83
00:06:12,640 --> 00:06:14,729
能識別這些筆劃的其中之一

84
00:06:14,880 --> 00:06:19,680
每次你輸入一個有頂部有個圓圈的圖像如 9 或 8 時

85
00:06:19,860 --> 00:06:24,000
隱層第二層的某些特定神經元的激勵值就會接近 1

86
00:06:24,000 --> 00:06:31,620
而我要的並不是單單適用這種圓圈，而是更廣泛的各種圓圈皆適用

87
00:06:32,240 --> 00:06:35,020
如此，在隱層第二層到輸出層的神經元

88
00:06:35,380 --> 00:06:39,960
只需要學習對應於數字的筆畫的組合

89
00:06:40,500 --> 00:06:42,800
當然這又丟出了一道難題

90
00:06:42,880 --> 00:06:48,000
因為你怎麼讓那些神經元知道那些數字該對應到那些特定的筆畫？

91
00:06:48,000 --> 00:06:52,820
而我甚至還沒開始講上一層怎麼影響下一層，但是再聽我解釋一下這裡

92
00:06:53,640 --> 00:06:56,340
辨識一個圓圈的問題也可以分解成辨識一些較小零件的問題

93
00:06:56,860 --> 00:07:02,550
一個合理的方法是認出組成它的各式各樣的邊

94
00:07:03,520 --> 00:07:08,910
同樣的道理，你在數字 1，4 或者 7 中所看到的一條長線

95
00:07:08,910 --> 00:07:14,279
真的就是好幾小條的短線，根據特定筆畫順序，組合成的長線

96
00:07:14,740 --> 00:07:19,379
所以我們期望在這網絡第二層

97
00:07:20,290 --> 00:07:22,650
對應著各式各樣的一些小邊

98
00:07:23,220 --> 00:07:27,420
也許出現一個像這樣的一個圖像就點亮

99
00:07:27,420 --> 00:07:31,640
所有大約有 8 到 10 種有關的特定神經元

100
00:07:31,930 --> 00:07:36,930
它接著點亮後上方的圓圈和一根垂直的長線以及點亮和一條線相聯的神經元

101
00:07:37,300 --> 00:07:39,599
最後點亮數字 9 的神經元

102
00:07:40,300 --> 00:07:44,280
不管這個是不是我們最終的網絡實際上的實施是另一個問題

103
00:07:44,280 --> 00:07:47,080
這個我們在知道怎樣了訓練網絡之後我在回過來講

104
00:07:47,350 --> 00:07:52,170
但至少我們可能有點希望
像是一種以這樣分層結構為目標的

105
00:07:53,020 --> 00:08:00,160
你可以進一步想像怎樣能來檢測像這樣的邊和式樣對其他的圖像識別功能真是有用的

106
00:08:00,800 --> 00:08:07,200
並甚至在圖像識別之外做各種各樣智能的東西也許你也想分解成一些抽象的層

107
00:08:07,680 --> 00:08:14,660
例如句子的分析涉及到把原始的語音提出一些獨特的聲音構成一些音節再構成

108
00:08:15,070 --> 00:08:19,829
詞再構成詞組以及更為抽象的思想等。

109
00:08:20,770 --> 00:08:25,710
但回到這些實際是怎樣工作的把你自己現在就放到這個的情景怎樣來設計

110
00:08:25,710 --> 00:08:30,449
如何在讓這層中的激勵函數可以決定下一層的激勵函數呢？

111
00:08:30,660 --> 00:08:35,860
這目標是有一些機能它想起來可以集中到一個特定的樣本來把一些像素結合成

112
00:08:35,880 --> 00:08:41,800
邊或者把邊結合成式樣或者式樣成爲數字
在這個特別的例子裡面

113
00:08:41,800 --> 00:08:44,700
我們希望第二層的這一個神經元

114
00:08:44,700 --> 00:08:50,580
可以正確的辨識出這個圖像裡有沒有一條邊

115
00:08:50,940 --> 00:08:54,960
現在我們想知道的是網路裡有哪些參數

116
00:08:55,260 --> 00:09:02,260
要怎麼調整這些參數才能讓完整的表達出是這個圖案

117
00:09:02,380 --> 00:09:07,780
還是其他的圖案或是由數個邊組合成的圓圈之類的

118
00:09:08,280 --> 00:09:15,380
我們會分配給神經元和輸入層間的每一個連接線一個權重

119
00:09:15,840 --> 00:09:17,940
權重單純只是一個數字而已

120
00:09:18,180 --> 00:09:25,580
然後計算所有激勵函數的加權總和

121
00:09:27,370 --> 00:09:31,680
把這些權重整理成一個圖像應該更好理解

122
00:09:31,680 --> 00:09:37,079
我把正的權重值標記為綠色
負的權重值標記為紅色

123
00:09:37,240 --> 00:09:41,670
當顏色越亮代表它的值跟 0 差距越大

124
00:09:42,400 --> 00:09:45,840
除了我們所關注的區域以外

125
00:09:45,840 --> 00:09:49,080
所有的權重值都改為 0

126
00:09:49,480 --> 00:09:51,500
然後去取得所有像素的加權總合

127
00:09:51,500 --> 00:09:57,680
幾乎就等於只有我們所關注的區域的值提升了

128
00:09:58,860 --> 00:10:02,620
如果知道這裡是不是真的存在一條邊

129
00:10:02,620 --> 00:10:06,660
你只需要在周圍加上負的權重

130
00:10:07,020 --> 00:10:12,660
這樣當中間的像素亮但是周圍的像素暗 就可以得到最大的加權總和

131
00:10:14,279 --> 00:10:18,169
當你計算加權總和時 它的值可能是任意實數

132
00:10:18,240 --> 00:10:23,180
但是在這裡我們想要計算完的結果介於 0 跟 1 之間

133
00:10:23,730 --> 00:10:26,599
所以我們通常會把這個值丟進一個函數裡面

134
00:10:26,910 --> 00:10:32,000
把這個實數軸壓縮成一個介於 0 到 1 之間

135
00:10:32,190 --> 00:10:37,249
有一個常見的函數叫做「Sigmoid」也被稱為「邏輯函數」

136
00:10:37,980 --> 00:10:43,339
基本上越小的數會越來越接近 0
越大的數會越來越接近 1

137
00:10:43,340 --> 00:10:46,600
輸入值在 0 附近的會平穩增長

138
00:10:49,080 --> 00:10:56,020
所以從神經網路得到的激勵函數基本上就代表加權總和的大小

139
00:10:57,450 --> 00:11:01,819
但是不是每次只要加權總和大於零的時候就點亮神經元

140
00:11:02,100 --> 00:11:06,260
也許你只想要在它大於 10 的時候啟動

141
00:11:06,630 --> 00:11:10,279
所以要加入一個門檻來確保它不會隨便啟動

142
00:11:10,860 --> 00:11:16,099
我們只要在加權總和後面加上一個像是 負10 之類的數

143
00:11:16,520 --> 00:11:19,660
再把它塞進邏輯函數裡

144
00:11:20,220 --> 00:11:22,740
這個附加的數字就叫做偏置

145
00:11:23,300 --> 00:11:29,060
所以權重告訴我們下一層的神經元所關注的圖樣

146
00:11:29,220 --> 00:11:35,460
偏置則告訴我們加權總和要超過什麼程度才是有意義的

147
00:11:35,940 --> 00:11:37,940
以上只是一個神經元的情況

148
00:11:38,120 --> 00:11:45,060
在這一層的每個神經元都會連接第一層共 784 個神經元

149
00:11:45,060 --> 00:11:50,720
而且這 784  條連接線都各有一個屬於自己的權重

150
00:11:51,320 --> 00:11:57,720
還有每一個神經元都會在計算完加權總和後再加上自己的偏置再用邏輯函數輸出自己的結果

151
00:11:58,020 --> 00:12:01,909
讓我們看看這個有著 16 個神經元的隱藏層

152
00:12:02,000 --> 00:12:08,260
這 16 個神經元都各有 784 個自己的權重和 16 個偏置

153
00:12:08,480 --> 00:12:12,560
這些還只是第一層和第二層的連接而已

154
00:12:12,560 --> 00:12:17,200
在其他層裡還有他們各自的權重和偏置

155
00:12:17,760 --> 00:12:21,200
整體來說整個網路使用了

156
00:12:21,280 --> 00:12:23,920
大約 13,000 個權重和偏置

157
00:12:24,280 --> 00:12:29,540
13,000 個可以調整的參數來讓網路可以呈現不同的結果

158
00:12:30,520 --> 00:12:32,520
所以當我們談到學習的時候

159
00:12:32,520 --> 00:12:39,100
就是在說如何讓電腦去找到一大堆正確的參數

160
00:12:39,100 --> 00:12:41,700
讓它解決問題

161
00:12:42,380 --> 00:12:46,060
有一個仔細想想會很嚇人的情況

162
00:12:46,060 --> 00:12:49,920
想像一下如果你需要手動調整這些權重和偏置

163
00:12:50,380 --> 00:12:56,600
設定這些數字來讓第二層識別一條邊
然後讓第三層識別圖案

164
00:12:57,080 --> 00:13:01,440
我個人認為這樣想像會比把它整個當成一個黑盒子更好

165
00:13:01,860 --> 00:13:05,160
因為當網路的輸出和你的認知有所差異時

166
00:13:05,520 --> 00:13:11,040
如果你能足夠了解權重與偏置的關係

167
00:13:11,080 --> 00:13:14,760
就更容易該怎麼改變結構來修正

168
00:13:14,760 --> 00:13:18,280
或是網路能輸出正確的結果但是過程跟你想像中有差異

169
00:13:18,300 --> 00:13:23,880
那麼去挖掘權重和偏置的實際境況對於測試你的認知有幫助

170
00:13:23,880 --> 00:13:26,360
和找出所有可能的解決方案

171
00:13:26,360 --> 00:13:30,700
順帶一提
你不覺得這個公式看起來有點複雜嗎？

172
00:13:32,340 --> 00:13:37,740
所以讓我來示範以簡單的符號來表達整個公式

173
00:13:37,740 --> 00:13:40,460
如果你以後想要繼續鑽研神經網路就會很常看到

174
00:13:41,110 --> 00:13:45,810
我們把這一層的激勵函數放到一個向量中

175
00:13:47,460 --> 00:13:50,600
然後把所有的權重放到矩陣中

176
00:13:50,600 --> 00:13:57,660
在這個陣列裡的每一列將會對應到這一層和下一層的所有連線的權重

177
00:13:58,060 --> 00:14:03,599
這代表矩陣相乘後的每一項都代表其中一個神經元的激勵函數

178
00:14:04,000 --> 00:14:09,330
矩陣相乘的結果就是對應的神經元激勵函數

179
00:14:13,540 --> 00:14:18,380
順道一提 學習機器學習需要對線性代數有一定的了解

180
00:14:18,380 --> 00:14:26,940
所以任何想要透過視覺化的教學理解矩陣和矩陣乘法可以去看我的以前做的線性代數系列影片

181
00:14:27,250 --> 00:14:28,839
特別是第三篇

182
00:14:28,840 --> 00:14:34,640
會到正題 當我要加上偏置時也會把每一個偏置

183
00:14:34,880 --> 00:14:42,200
放到矩陣裡在和前面算出來的結果做矩陣加法

184
00:14:42,910 --> 00:14:44,040
最後一步

185
00:14:44,040 --> 00:14:47,250
我會用邏輯函數把整個結果包起來

186
00:14:47,250 --> 00:14:51,899
意思是把最後得到的向量一個一個丟進邏輯函數中

187
00:14:52,420 --> 00:14:54,570
來計算出每一個結果

188
00:14:55,500 --> 00:15:00,520
現在當我們用簡單的符號來寫下公式

189
00:15:00,520 --> 00:15:07,580
就可以很清楚完整的表達每一層之間的關係

190
00:15:07,920 --> 00:15:15,720
還有這也讓我們更簡單快速的編寫程式碼
像是很多的對於矩陣運算有最佳化的函式庫

191
00:15:17,560 --> 00:15:21,360
還記得我一開始說神經元只是一個簡單承載數字的東西嗎

192
00:15:21,790 --> 00:15:26,250
當然 它裡面所裝得數字取決於你給他的圖像

193
00:15:27,780 --> 00:15:31,880
所以更準確地說應該把神經元當作一個函數

194
00:15:31,960 --> 00:15:38,060
它的輸出是取決於上一層所有的神經元然後轉換成一個介於 0 到 1 的數字

195
00:15:38,800 --> 00:15:41,680
其實也可以把整個神經網路當作一個函數

196
00:15:41,680 --> 00:15:47,080
有著 784 個輸入值和 10 輸出值的函數

197
00:15:47,460 --> 00:15:49,580
它是一個非常複雜的函數

198
00:15:49,580 --> 00:15:56,240
用來選擇正確圖案的權重和偏置參數就有 13000 個

199
00:15:56,250 --> 00:16:00,270
還會不停地用到矩陣乘法和邏輯函數

200
00:16:00,610 --> 00:16:06,390
即便它看起來很複雜但卻是很可靠的一個函數

201
00:16:06,390 --> 00:16:12,239
我的意思是如果它看起來很簡單 我們怎麼能期望它辨識數字呢

202
00:16:12,960 --> 00:16:19,559
但是它是如何完成這個任務的？
這個網路是如何只透過讀取數據來學習如何調整權重和偏置的？

203
00:16:20,080 --> 00:16:26,039
這就是下一集的內容了
我們也會更深入更多關於網路運作的細節

204
00:16:27,130 --> 00:16:32,640
現在又到了提醒大家如果要獲得更多新影片的通知趕快訂閱

205
00:16:32,760 --> 00:16:37,560
但是大部分的時候你都沒有收到來自 YouTube 的通知吧？

206
00:16:37,560 --> 00:16:43,640
也許我該說趕快訂閱讓 YouYube 的推薦演算法神經網路

207
00:16:43,640 --> 00:16:47,640
去相信你想看到關於這個頻道的消息

208
00:16:48,250 --> 00:16:50,250
總之留意更多消息

209
00:16:50,410 --> 00:16:53,550
感謝大家在 Patreon 平台上的支持

210
00:16:53,589 --> 00:16:56,759
我的機率系列影片在這個夏天進展得有點慢

211
00:16:56,760 --> 00:17:01,379
但是在這個系列結束後我會回到那個系列
所以大家可以留意新消息

212
00:17:03,310 --> 00:17:05,550
在影片的結尾
我邀請了 Lisha Li

213
00:17:05,550 --> 00:17:12,029
她在博士研究是關於深度學習的理論
現在一個叫做 Amplify Partners 的創投公司任職

214
00:17:12,029 --> 00:17:15,040
他們慷慨的贊助了這個影片

215
00:17:15,380 --> 00:17:19,100
所以 Lisha 我們想要快速的提一下「Sigmoid」這個函數

216
00:17:19,180 --> 00:17:24,780
據我所知早期的神經網路都會使用這個函數來讓權重介於 0 到 1 之間

217
00:17:24,980 --> 00:17:30,340
用來模仿生物學上的神經元是處於活耀還是不活耀的狀態
(Lisha) 沒錯

218
00:17:30,360 --> 00:17:36,120
但是現在神經網路幾乎沒有使用「Sigmoid」函數，是因為它已經太老套了是嗎？

219
00:17:36,120 --> 00:17:42,660
(Lisha) 沒錯，更準確的說是使用 ReLU 更容易訓練
(3B1B) ReLU 的全名是「線性整流函數」 對吧

220
00:17:42,660 --> 00:17:48,840
(Lisha) 對，這個函數會返回 0 跟輸入值的最大值

221
00:17:49,120 --> 00:17:53,680
我的解釋是使用這個函數比較符合

222
00:17:54,600 --> 00:17:56,600
生物學的原理

223
00:17:56,620 --> 00:17:58,179
類似於

224
00:17:58,179 --> 00:18:03,089
神經元在甚麼時候活耀或不活耀
當它超過一個門檻的時候

225
00:18:03,250 --> 00:18:05,250
它就像恆等函數一樣

226
00:18:05,290 --> 00:18:10,439
但如果它沒有超過則輸出 0
所以它是一個很簡單的函數

227
00:18:10,720 --> 00:18:14,429
使用「Sigmoid」並沒有幫助訓練或是說它很難以訓練

228
00:18:14,429 --> 00:18:17,760
後來有人嘗試了 ReLU

229
00:18:17,760 --> 00:18:22,140
發現它的表現難以置信的好

230
00:18:22,690 --> 00:18:25,090
在深度神經網路

231
00:18:25,090 --> 00:18:26,060
謝謝 Lisha

