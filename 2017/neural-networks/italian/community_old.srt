1
00:00:04,020 --> 00:00:10,680
Questo è un tre. È scritto male e visualizzato ad una risoluzione estremamente bassa di 28x28 pixel.

2
00:00:10,680 --> 00:00:15,660
Ma il tuo cervello non ha problemi a riconoscerlo come un tre e voglio che ti prenda un momento per apprezzare

3
00:00:15,900 --> 00:00:18,949
quanto è pazzesco il fatto che il cervello riesca a farlo questa cosa così facilmente?

4
00:00:18,949 --> 00:00:23,160
Intendo questo, questo e questo sono anch'essi riconoscibili come tre,

5
00:00:23,160 --> 00:00:28,060
anche se i valori specifici di ciascun pixel sono molto diversi da un'immagine all'altra.

6
00:00:28,080 --> 00:00:33,780
Le specifiche cellule fotosensibili dell'occhio che si attivano quando vedi questo tre

7
00:00:33,780 --> 00:00:36,800
sono molto diverse da quelle che si attivano quando vedi questo tre.

8
00:00:37,140 --> 00:00:40,610
Ma qualcosa in quella tua corteccia visiva

9
00:00:41,129 --> 00:00:48,139
li riconosce come entità che rappresentano la stessa idea, mentre allo stesso tempo riconosce le altre immagini come idee distinte

10
00:00:48,840 --> 00:00:55,039
Ma se ti dicessi, siediti e scrivimi un programma che contiene una griglia di 28x28

11
00:00:55,379 --> 00:01:01,759
pixel come questa che genera un numero singolo tra 0 e 10 e che ti dica quale cifra pensa sia

12
00:01:02,250 --> 00:01:06,139
Allora il compito passa dall'essere banale all'essere tremendamente difficile

13
00:01:06,750 --> 00:01:08,270
A meno che tu non abbia vissuto sotto terra

14
00:01:08,270 --> 00:01:14,599
penso che non avrò bisogno di motivare la rilevanza e l'importanza dell'apprendimento automatico e delle reti neurali nel presente nel futuro

15
00:01:14,640 --> 00:01:18,410
Ma quello che voglio fare qui è mostrarti che cos'è una rete neurale

16
00:01:18,660 --> 00:01:24,229
Senza pretendere conoscenze pregresse, in modo da aiutarti a visualizzare cosa sta facendo, non in modo divulgativo, ma come un pezzo di matematica.

17
00:01:24,570 --> 00:01:28,310
La mia speranza è che tu capisca come come questa struttura stessa

18
00:01:28,380 --> 00:01:34,399
è motivata e ti senta come se sapessi cosa significa quando leggi o senti parlare di una di una rete neurale

19
00:01:34,950 --> 00:01:40,249
Questo video sarà dedicato alla sua componente strutturale ed il successivo affronterà l'apprendimento.

20
00:01:40,530 --> 00:01:45,950
Quello che faremo è costruire una rete neurale che possa imparare a riconoscere cifre scritte a mano

21
00:01:49,270 --> 00:01:51,329
Questo è il classico esempio che si usa per

22
00:01:51,520 --> 00:01:56,759
introdurre l'argomento e sono felice di riproporlo qui perché alla fine dei due video voglio indicarti

23
00:01:56,760 --> 00:02:02,099
un paio di buone fonti dove puoi imparare di più e dove puoi scaricare il codice che fa tutto ciò e persino giocarci

24
00:02:02,100 --> 00:02:04,100
sul tuo computer.

25
00:02:04,750 --> 00:02:08,970
Ci sono molte molte varianti delle reti neurali e negli ultimi anni

26
00:02:08,970 --> 00:02:11,970
c'è stata una sorta di "boom" nella ricerca di queste varianti.

27
00:02:12,130 --> 00:02:19,019
Ma in questi due video introduttivi, io e te stiamo per dare un'occhiata alla più semplice delle varianti, "vanilla", senza fronzoli aggiunti

28
00:02:19,300 --> 00:02:21,040
Questo è un requisito necessario

29
00:02:21,040 --> 00:02:24,510
per comprendere le varianti più moderne e,

30
00:02:24,760 --> 00:02:28,199
fidati di me, è abbastanza complessa da darci grattacapi

31
00:02:28,690 --> 00:02:32,820
Ma anche in questa forma più semplice può imparare a riconoscere cifre scritte a mano

32
00:02:32,820 --> 00:02:36,180
Che è una cosa abbastanza interessante da far fare ad un computer.

33
00:02:37,120 --> 00:02:41,960
E allo stesso tempo vedrai quali sono le speranze che abbiamo per essa

34
00:02:43,090 --> 00:02:48,179
Come suggerisce il nome, le reti neurali sono ispirate al cervello, ma approfondiamo l'argomento

35
00:02:48,520 --> 00:02:51,389
Cosa sono i neuroni e in che significa che sono collegati tra loro?

36
00:02:52,090 --> 00:02:57,750
In questo momento quando dico neurone tutto quello a cui vorrei tu pensassi è una cosa che contiene un numero

37
00:02:58,209 --> 00:03:02,129
Specificamente un numero compreso tra 0 e 1, davvero, non è molto più di questo

38
00:03:03,430 --> 00:03:11,130
Ad esempio, la rete inizia con un gruppo di neuroni corrispondenti a ciascuna dei 28x28 pixel dell'immagine in ingresso

39
00:03:11,400 --> 00:03:12,460
che è

40
00:03:12,460 --> 00:03:20,240
784 neuroni in totale, ognuno di questi contiene un numero che rappresenta il valore in scala di grigi del pixel corrispondente

41
00:03:20,769 --> 00:03:24,299
va da 0 per i pixel neri fino ad 1 per i pixel bianchi

42
00:03:24,910 --> 00:03:30,419
Questo numero all'interno del neurone è chiamato "attivazione" e l'immagine che potresti avere in mente qui

43
00:03:30,420 --> 00:03:33,959
è che ogni neurone è si illumina quando la sua attivazione è un numero elevato.

44
00:03:36,260 --> 00:03:41,559
Così tutti questi 784 neuroni costituiscono il primo strato della nostra rete

45
00:03:45,990 --> 00:03:51,289
Ora saltiamo all'ultimo livello, questo ha dieci neuroni ciascuno dei quali rappresenta una delle cifre,

46
00:03:51,570 --> 00:03:56,239
l'attivazione in questi neuroni di nuovo un numero compreso tra zero e uno

47
00:03:56,880 --> 00:04:00,049
Rappresenta quanto il sistema "pensi" che una determinata immagine

48
00:04:00,720 --> 00:04:05,990
corrisponda ad una determinata cifra. C'è anche un paio di livelli in mezzo chiamati "livelli nascosti"

49
00:04:06,180 --> 00:04:07,770
che per il momento

50
00:04:07,770 --> 00:04:13,549
saranno un gigante punto interrogativo su come faccia questo processo di riconoscimento delle cifre ad essere gestito

51
00:04:13,740 --> 00:04:20,209
In questa rete ho scelto due strati nascosti ognuno con 16 neuroni e ammetto che è stata una scelta arbitraria

52
00:04:20,609 --> 00:04:24,889
ad essere onesto ho scelto due livelli in per riuscire a spiegare la struttura con un'occhiata e

53
00:04:25,350 --> 00:04:29,179
16 è in pratica un bel numero fare in modo che la rete fosse contenuta nello schermo

54
00:04:29,180 --> 00:04:32,209
C'è molto spazio per sperimentare con una struttura specifica

55
00:04:32,730 --> 00:04:38,329
Il modo in cui la rete gestisce le attivazioni in un livello determina le attivazioni del livello successivo

56
00:04:38,760 --> 00:04:45,349
E naturalmente il cuore della rete come meccanismo di elaborazione delle informazioni si riduce esattamente a come quelle

57
00:04:45,570 --> 00:04:48,409
attivazioni di un livello determinano le attivazioni di quello successivo

58
00:04:48,900 --> 00:04:54,859
È pensato per essere simile al meccanismo con cui alcuni gruppi di neuroni nelle reti biologiche si attivano

59
00:04:55,410 --> 00:04:57,410
facendo in modo che si attivino altri neuroni

60
00:04:57,570 --> 00:04:58,340
Ora la rete

61
00:04:58,340 --> 00:05:03,019
che sto mostrando qui è già stata addestrata a riconoscere le cifre e, lascia che ti mostri cosa intendo con questo

62
00:05:03,140 --> 00:05:06,580
Significa che se si alimenta la rete con un'immagine si illuminano tutti

63
00:05:06,640 --> 00:05:11,780
i 784 neuroni del livello di input, in base alla luminosità di ciascun pixel nell'immagine.

64
00:05:12,330 --> 00:05:17,029
Quel modello di attivazioni provoca l'attivazione di alcuni schemi specifici nel livello successivo

65
00:05:17,190 --> 00:05:19,309
il quale causa qualche altro schema in quello che segue

66
00:05:19,440 --> 00:05:22,190
che alla fine dà qualche altro schema nel livello di output e

67
00:05:22,350 --> 00:05:29,359
il neurone più brillante di questo livello di output è la cifra scelta della rete  che rappresenta questa immagine

68
00:05:32,070 --> 00:05:36,859
E prima di trattare la matematica di come uno strato influenza il prossimo o di come funziona l'allenamento

69
00:05:37,140 --> 00:05:43,069
parliamo del perché è ragionevole aspettarsi che una struttura a strati come questa si comporti in modo intelligente

70
00:05:43,800 --> 00:05:48,260
Cosa ci possiamo aspettare qui? Qual è il migliore obiettivo che questi strati potrebbero ottenere?

71
00:05:48,860 --> 00:05:56,720
Bene, quando io o te riconosciamo le cifre, mettiamo insieme vari componenti, un nove ha un cerchio in alto e una linea a destra

72
00:05:57,260 --> 00:06:01,280
un 8 ha anche esso un cerchio in alto, ma è abbinato a un altro cerchio in basso

73
00:06:02,020 --> 00:06:06,599
Un 4 fondamentalmente si divide in tre linee e cose altre considerazioni del genere per gli altri numeri

74
00:06:07,180 --> 00:06:11,970
Ora in un mondo perfetto potremmo sperare che ogni neurone nel penultimo strato

75
00:06:12,640 --> 00:06:14,729
corrisponda a uno di questi sotto componenti

76
00:06:14,890 --> 00:06:19,740
Che ogni volta che sottoponi un'immagine con un cerchio in alto come un 9 o un 8

77
00:06:19,870 --> 00:06:21,220
ci siano alcuni specifici

78
00:06:21,220 --> 00:06:27,749
Neuroni la cui attivazione sia vicina a uno e non intendo questo specifico cerchio di pixel la speranza sarebbe che ogni

79
00:06:28,090 --> 00:06:35,039
modello di cerchio in alto inneschi questo neurone in questo modo passando dal terzo strato all'ultimo

80
00:06:35,380 --> 00:06:39,960
è richiesto semplicemente di imparare quale combinazione di sub componenti corrisponda a quale cifra

81
00:06:40,510 --> 00:06:42,810
Ovviamente ciò dà l'avvio al problema

82
00:06:42,910 --> 00:06:49,019
Non ho ancora parlato di come riconoscere questi sub-componenti o addirittura di come imparare quali sono i sotto componenti giusti

83
00:06:49,020 --> 00:06:52,829
Di come uno strato influenza il prossimo ma, seguimi un momento

84
00:06:53,650 --> 00:06:56,340
riconoscere un cerchio è un problema che puo' essere scomposto in sottoproblemi

85
00:06:56,860 --> 00:07:02,550
Un modo ragionevole per farlo sarebbe riconoscere innanzitutto i vari piccoli bordi che lo compongono

86
00:07:03,520 --> 00:07:08,910
Allo stesso modo per una linea lunga come quella che potresti vedere nelle cifre 1 o 4 o 7

87
00:07:08,910 --> 00:07:14,279
Beh, questo è solo un lungo tratto, ma potresti pensarlo come un tratto diviso in parti più piccole

88
00:07:14,740 --> 00:07:19,379
Quindi la nostra speranza è che ogni neurone nel secondo strato della rete

89
00:07:20,290 --> 00:07:22,650
corrisponda ai vari piccoli tratti rilevanti

90
00:07:23,230 --> 00:07:28,259
Forse quando arriva un'immagine come questa, si accendono tutti i neuroni

91
00:07:28,720 --> 00:07:31,649
associati a circa otto o dieci tratti specifici

92
00:07:31,930 --> 00:07:36,930
che a sua volta illumina i neuroni associati al cerchio in alto e una lunga linea verticale e

93
00:07:37,300 --> 00:07:39,599
Quelli illuminano il neurone associato a un nove

94
00:07:40,300 --> 00:07:41,100
 

95
00:07:41,100 --> 00:07:47,070
se questo è ciò che fa effettivamente la nostra rete finale è un'altra domanda, su cui tornerò una volta che vedremo come addestrare la rete.

96
00:07:47,350 --> 00:07:52,170
Ma questa è una speranza che potremmo avere, una sorta di obiettivo con una struttura a strati come questa.

97
00:07:53,020 --> 00:07:59,340
Inoltre puoi immaginare come essere in grado di rilevare bordi e motivi come questi sarebbe davvero utile per altre attività di riconoscimento dell'immagine

98
00:07:59,740 --> 00:08:06,749
Oltre al riconoscimento delle immagini ci sono tante altre cose che potresti voler fare che possono essere divise in strati di astrazione

99
00:08:07,690 --> 00:08:14,670
L'analisi del parlato, ad esempio, implica l'acquisizione di audio non elaborato e l'individuazione di suoni distinti che si combinano per comporre le sillabe

100
00:08:15,070 --> 00:08:19,829
Che si combinano per formare parole che si combinano per comporre frasi e pensieri più astratti, ecc

101
00:08:20,770 --> 00:08:25,710
Ma torniamo a vedere come funzionano raffigurandole visivamente

102
00:08:25,710 --> 00:08:30,449
In che modo esattamente le attivazioni in un livello potrebbero determinare le attivazioni nel prossimo?

103
00:08:30,670 --> 00:08:35,879
L'obiettivo è di avere un meccanismo che possa in teoria combinare i pixel in tratti

104
00:08:35,880 --> 00:08:41,430
o i tratti in modelli o modelli in cifre e come esempio vediamo un caso molto specifico

105
00:08:41,950 --> 00:08:44,189
Focalizziamo l'attenzione su un particolare

106
00:08:44,380 --> 00:08:50,430
neurone nel secondo livello per capire se l'immagine ha o meno un tratto in questa regione qui

107
00:08:50,950 --> 00:08:54,960
La domanda in questione è quali parametri dovrebbe avere la rete,

108
00:08:55,270 --> 00:09:02,490
quali quadretti dovresti essere in grado di modificare in modo che sia capace di catturare questo modello o

109
00:09:02,590 --> 00:09:07,290
qualunque altro schema di pixel o lo schema in cui diversi bordi possono creare un cerchio e altre cose simili?

110
00:09:08,290 --> 00:09:15,389
Bene, quello che faremo è assegnare un peso a ciascuna delle connessioni tra il nostro neurone e i neuroni del primo strato

111
00:09:15,850 --> 00:09:17,850
Questi pesi sono solo numeri

112
00:09:18,190 --> 00:09:25,590
quindi prendi tutte quelle attivazioni dal primo livello e calcola la loro somma ponderata in base a questi pesi

113
00:09:27,370 --> 00:09:31,680
Trovo utile pensare a questi pesi come se fossero organizzati in una piccola griglia

114
00:09:31,680 --> 00:09:37,079
E ho intenzione di utilizzare i pixel verdi per indicare pesi positivi e pixel rossi per indicare i pesi negativi

115
00:09:37,240 --> 00:09:41,670
Dove la luminosità di quel pixel è una rappresentazione approssimativa del valore dei pesi

116
00:09:42,400 --> 00:09:45,840
Ora se abbiamo messo i pesi associati a quasi tutti i pixel zero

117
00:09:46,150 --> 00:09:49,079
salvo per alcuni pesi positivi in ​​questa regione che ci interessa

118
00:09:49,480 --> 00:09:51,310
allora prendere la somma ponderata di

119
00:09:51,310 --> 00:09:57,690
tutti i valori dei pixel in realtà equivale a sommare i valori del pixel solo nella regione che ci interessa

120
00:09:58,870 --> 00:10:04,440
E, se vuoi davvero che capisca se c'è un tratto qui, quello che potresti fare è avere dei pesi negativi

121
00:10:04,900 --> 00:10:06,900
associati ai pixel circostanti

122
00:10:07,030 --> 00:10:12,660
Quindi la somma è maggiore quando i pixel centrali sono luminosi, ma i pixel circostanti sono più scuri

123
00:10:14,279 --> 00:10:18,169
Quando calcoli una somma ponderata come questa potresti ottenere qualsiasi numero

124
00:10:18,240 --> 00:10:23,180
ma per questa rete ciò che vogliamo è che le attivazioni abbiano un valore compreso tra 0 e 1

125
00:10:23,730 --> 00:10:26,599
quindi una cosa comune da fare è mettere questa somma ponderata

126
00:10:26,910 --> 00:10:32,000
In una funzione che schiaccia la linea deli numeri reali nell'intervallo compreso tra 0 e 1 e

127
00:10:32,190 --> 00:10:37,249
Una tipica funzione che fa ciò è chiamata la funzione sigmoide conosciuta anche come curva logistica

128
00:10:37,980 --> 00:10:43,339
input molto negativi finiscono vicino a zero. Gli input molto positivi finiscono vicino a 1

129
00:10:43,339 --> 00:10:46,398
e aumenta fortemente attorno all'input 0

130
00:10:49,080 --> 00:10:56,029
Quindi l'attivazione del neurone qui è fondamentalmente una misura di quanto sia positiva la somma ponderata

131
00:10:57,450 --> 00:11:01,819
Ma forse non vuoi che il neurone si accenda quando la somma ponderata è maggiore di 0

132
00:11:02,100 --> 00:11:06,260
Forse vuoi solo che si accenda quando la somma è più grande di 10

133
00:11:06,630 --> 00:11:10,279
Quello che vuoi è una soglia entro la quale sia inattivo

134
00:11:10,860 --> 00:11:16,099
quello che faremo allora è aggiungere un altro numero come il -10 a questa somma ponderata

135
00:11:16,529 --> 00:11:19,669
Prima di collegarlo alla funzione sigmoide

136
00:11:20,220 --> 00:11:22,730
Quel numero aggiuntivo è chiamato "soglia" [bias]

137
00:11:23,310 --> 00:11:29,060
Quindi i pesi ti dicono quale schema di pixel sta cercando questo neurone del secondo strato e la soglia

138
00:11:29,220 --> 00:11:35,450
ti dice quanto deve essere alta la somma ponderata prima che il neurone inizi a diventare significativamente attivo

139
00:11:35,910 --> 00:11:37,910
E questo è solo un neurone

140
00:11:38,120 --> 00:11:41,940
Ogni altro neurone in questo strato sarà collegato a tutti

141
00:11:42,320 --> 00:11:50,620
i 784 neuroni del primo strato e ciascuno di questi 784 collegamenti ha il proprio peso associato

142
00:11:51,330 --> 00:11:57,739
inoltre ognuno ha come soglia un altro numero che aggiungi alla somma ponderata prima di sottoporlo alla funzione sigmoide e

143
00:11:58,020 --> 00:12:01,909
C'è molto da discutere in questo strato nascosto di 16 neuroni

144
00:12:02,010 --> 00:12:08,270
è un totale di 784 per 16 pesi con 16 soglie

145
00:12:08,490 --> 00:12:14,029
E tutto ciò consiste solo nelle connessioni dal primo strato al secondo, le connessioni tra gli altri livelli

146
00:12:14,029 --> 00:12:17,208
hanno anche esse un parecchi pesi e soglie

147
00:12:17,760 --> 00:12:20,680
Alla fine questa rete ha quasi esattamente

148
00:12:21,280 --> 00:12:23,920
13.000 pesi e soglie in totale

149
00:12:24,280 --> 00:12:29,540
13.000 manopole e quadrati che possono essere modificati e trasformati per far sì che questa rete si comporti in modi diversi

150
00:12:30,520 --> 00:12:32,520
Quindi quando parliamo di apprendimento?

151
00:12:32,530 --> 00:12:40,199
Ciò a cui ci si riferisce è far sì che il computer trovi un'impostazione valida per tutti questi numeri in modo che possa effettivamente risolvere

152
00:12:40,200 --> 00:12:42,190
il problema.

153
00:12:42,190 --> 00:12:43,000
un pensiero

154
00:12:43,000 --> 00:12:49,979
Un esperimento che è allo stesso tempo divertente e quasi terrificante è immaginare di sedersi e mettere a punto tutti questi pesi e pregiudizi a mano

155
00:12:50,380 --> 00:12:56,159
Modificando in modo mirato i numeri in modo che il secondo livello riprenda i tratti, il terzo strato riprenda schemi ecc

156
00:12:56,350 --> 00:13:01,440
Personalmente trovo tutto ciò soddisfacente piuttosto che vedere la rete come una scatola nera

157
00:13:01,870 --> 00:13:04,349
Perché quando la rete non funziona come ti aspettavi

158
00:13:04,600 --> 00:13:11,370
se hai un po' di confidenza con ciò che questi pesi e soglie significano hai un punto di partenza per

159
00:13:11,680 --> 00:13:16,289
Sperimentare come migliorare la struttura, o quando la rete funziona

160
00:13:16,290 --> 00:13:18,290
Ma non per le cose che potresti aspettarti

161
00:13:18,310 --> 00:13:25,169
Andare a guardare ciò che stanno facendo i pesi e le soglie è un buon modo per mettere alla prova le tue ipotesi e rivelare davvero l'intero spazio delle

162
00:13:25,180 --> 00:13:26,350
soluzioni

163
00:13:26,350 --> 00:13:30,600
A proposito, la funzione attuale qui è un po' macchinosa da scrivere. Non pensi?

164
00:13:32,350 --> 00:13:38,460
Quindi lascia che ti mostri un modo più compatto per rappresentare queste connessioni. Questo è come lo vedresti

165
00:13:38,460 --> 00:13:40,460
scegliessi di leggere qualcosa in più sulle reti neurali

166
00:13:41,110 --> 00:13:45,810
Organizza tutte le attivazioni da un livello in una colonna come un vettore

167
00:13:47,470 --> 00:13:52,320
Quindi organizza tutti i pesi come una matrice in cui ogni riga di quella matrice

168
00:13:52,900 --> 00:13:57,659
corrisponde alle connessioni tra uno strato e un particolare neurone nello strato successivo

169
00:13:58,060 --> 00:14:03,599
Che cosa significa prendere la somma ponderata delle attivazioni nel primo strato in base a questi pesi?

170
00:14:04,000 --> 00:14:09,330
Corrisponde a uno dei termini nel prodotto matrice-vettore di tutto ciò che abbiamo qui a sinistra

171
00:14:13,540 --> 00:14:18,380
gran parte del machine learning si riduce ad avere una buona conoscenza dell'algebra lineare

172
00:14:18,380 --> 00:14:26,940
Chiunque di voi desideri capire graficamente le matrici e sapere che significa la moltiplicazione matrice-vettore puo' dare un'occhiata alla serie che ho fatto sull'algebra lineare

173
00:14:27,250 --> 00:14:28,839
specialmente il terzo capitolo

174
00:14:28,839 --> 00:14:35,759
Tornando alla nostra espressione, invece di di aggiungere la soglia a ciascuno di questi valori, la rappresentiamo in modo indipendente

175
00:14:36,010 --> 00:14:42,209
organizzando tutte quelle soglie in un vettore e poi sommiamo l'intero vettore al precedente prodotto vettore-matrice

176
00:14:42,910 --> 00:14:44,040
Quindi come passo finale

177
00:14:44,040 --> 00:14:47,250
Mettiamo la sigmoide all'esterno di tutto

178
00:14:47,250 --> 00:14:51,899
rappresentando che applicheremo la funzione sigmoide ad ogni specifico

179
00:14:52,420 --> 00:14:54,570
componente interno del vettore risultante

180
00:14:55,510 --> 00:15:00,749
Quindi, una volta scritta questa matrice dei pesi e questi vettori come simboli a sè, è possibile

181
00:15:01,000 --> 00:15:07,589
comunicare la transizione completa delle attivazioni da uno strato all'altro in una piccola espressione estremamente stretta e accurata e

182
00:15:07,930 --> 00:15:15,000
Questo rende il codice sia molto più semplice che molto più veloce poiché molte librerie ottimizzano la moltiplicazione delle matrici

183
00:15:17,560 --> 00:15:21,359
Ricorda quanto prima ho detto, che questi neuroni sono semplicemente cose che contengono numeri

184
00:15:21,790 --> 00:15:26,250
Beh, i numeri specifici che tengono dipende dall'immagine iniziale

185
00:15:27,790 --> 00:15:32,940
Quindi in realtà è più accurato pensare a ciascun neurone come a una funzione che accetta l'output

186
00:15:33,070 --> 00:15:38,070
di tutti i neuroni nel livello precedente e sputa un numero compreso tra zero e uno

187
00:15:38,800 --> 00:15:42,270
In realtà l'intera rete è solo una funzione che accetta

188
00:15:42,760 --> 00:15:47,010
784 numeri come input e sputa dieci numeri come output

189
00:15:47,470 --> 00:15:48,700
È una funzione

190
00:15:48,700 --> 00:15:56,249
complicata che coinvolge tredici mila parametri sotto forma di questi pesi e soglie che riconoscono determinati schemi che finiscono

191
00:15:56,250 --> 00:16:00,270
in molti prodotti matrice-vettore e l'applicazione della funzione sigmoide

192
00:16:00,610 --> 00:16:06,390
Ma è pur sempre una funzione e in un certo senso è rassicurante il fatto che sembra complicato

193
00:16:06,390 --> 00:16:12,239
Voglio dire se fosse più semplice che speranza avremmo che possa riconoscere le cifre?

194
00:16:12,960 --> 00:16:19,559
E come risolve quel problema? In che modo questa rete impara i pesi e le soglie appropriate semplicemente osservando i dati? Oh?

195
00:16:20,080 --> 00:16:26,039
Questo è quello che vi mostrerò nel prossimo video e approfondirò un po' ciò che questa particolare rete che stiamo vedendo sta facendo

196
00:16:27,130 --> 00:16:32,640
E' arrivato il momento in cui vi dovrei dire di iscrivervi per rimanere informati su quando quel video o eventuali nuovi video escono

197
00:16:32,760 --> 00:16:37,560
Ma realisticamente la maggior parte di voi in realtà non riceve le notifiche da YouTube, vero?

198
00:16:37,560 --> 00:16:42,260
Forse più onestamente dovrei dire iscrivervi in modo che le reti neurali che stanno alla base di YouTube,

199
00:16:42,459 --> 00:16:47,639
Gli algoritmi di raccomandazione siano convinti che tu voglia vedere i contenuti di questo canale raccomandandoteli

200
00:16:48,250 --> 00:16:50,250
comunque rimani connesso ancora

201
00:16:50,410 --> 00:16:53,550
Grazie mille a tutti coloro che supportano questi video su patreon

202
00:16:53,589 --> 00:16:56,759
Sono stato un po' lento a progredire nella serie di probabilità questa estate

203
00:16:56,760 --> 00:17:01,379
Ma ci sto tornando dopo questo progetto, quindi i patrons possono cercare aggiornamenti lì

204
00:17:03,310 --> 00:17:05,550
Per chiudere le cose qui ho con me Lisha Li

205
00:17:05,550 --> 00:17:12,029
Lee che ha fatto il suo dottorato di ricerca sul lato teorico del deep learning e che attualmente lavora in una società di venture capital chiamata amplify partners

206
00:17:12,030 --> 00:17:16,530
Chi ha gentilmente fornito alcuni fondi per questo video, quindi Lisha una cosa

207
00:17:16,530 --> 00:17:19,109
Penso che dovremmo migliorare rapidamente questa funzione sigmoide

208
00:17:19,180 --> 00:17:24,780
Come capisco, le prime reti usavano questo per schiacciare la somma ponderata rilevante in quell'intervallo tra zero e uno

209
00:17:24,980 --> 00:17:30,340
Sapete che è motivata da questa analogia biologica dei neuroni che sono inattivi o attivi
(Lisha) - Esattamente

210
00:17:30,360 --> 00:17:36,320
(3B1B) - Ma relativamente poche reti moderne usano effettivamente sigmoid. È una specie di vecchia scuola, giusto?
(Lisha) - Sì o piuttosto

211
00:17:36,370 --> 00:17:42,780
ReLU sembra essere molto più facile da addestrare
(3B1B) - E ReLU è l'acronimo di unità lineare rettificata

212
00:17:42,780 --> 00:17:48,839
(Lisha) - Sì, è questo tipo di funzione in cui stai solo prendendo un massimo di 0 e a dove a è dato da

213
00:17:49,120 --> 00:17:53,670
quello che stavi spiegando nel video e ciò che era motivato da quello che penso fosse un

214
00:17:54,610 --> 00:17:56,610
parzialmente da un'analogia biologica

215
00:17:56,620 --> 00:17:58,179
con come

216
00:17:58,179 --> 00:18:03,089
I neuroni potrebbero essere attivati ​​o meno e se passa una certa soglia

217
00:18:03,250 --> 00:18:05,250
Sarebbe la funzione di identità

218
00:18:05,290 --> 00:18:10,439
Ma se così non fosse, non sarebbe stato attivato in modo da essere zero quindi è una specie di semplificazione

219
00:18:10,720 --> 00:18:14,429
L'uso dela sigmoide non ha aiutato l'allenamento, o era difficile da allenare

220
00:18:14,429 --> 00:18:19,589
Ad un certo punto e le persone hanno provato relu ed è successo che funzionava

221
00:18:20,110 --> 00:18:22,140
Molto bene

222
00:18:22,690 --> 00:18:25,090
Profonde reti neurali.
(3B1B) - Va bene

223
00:18:25,090 --> 00:18:26,060
Grazie Lisha

