1
00:00:04,220 --> 00:00:05,400
Questo è un 3.

2
00:00:06,060 --> 00:00:09,769
È scritto in modo approssimato e visualizzato a una risoluzione estremamente 

3
00:00:09,769 --> 00:00:13,720
bassa di 28x28 pixel, ma il tuo cervello non ha problemi a riconoscerlo come un 3.

4
00:00:14,340 --> 00:00:16,757
E voglio che ti prenda un momento per apprezzare quanto 

5
00:00:16,757 --> 00:00:18,960
sia assurdo che il cervello lo faccia senza fatica.

6
00:00:19,700 --> 00:00:23,442
Insomma, anche questo, questo e questo sono riconoscibili come 3, 

7
00:00:23,442 --> 00:00:28,320
anche se i valori specifici di ogni pixel sono molto diversi da un'immagine all'altra.

8
00:00:28,900 --> 00:00:32,816
Le particolari cellule foto-sensibili del tuo occhio che si attivano quando 

9
00:00:32,816 --> 00:00:36,940
vedi questo 3 sono molto diverse da quelle che si attivano quando vedi questo 3.

10
00:00:37,520 --> 00:00:42,591
Ma qualcosa in quella tua corteccia visiva così intelligente porta queste immagini a 

11
00:00:42,591 --> 00:00:47,723
rappresentare la stessa idea, riconoscendo allo stesso tempo altre immagini come idee 

12
00:00:47,723 --> 00:00:48,260
distinte.

13
00:00:49,220 --> 00:00:54,873
Ma se ti dicessi: "Ehi, siediti e scrivi per me un programma che prenda una griglia di 

14
00:00:54,873 --> 00:00:59,032
28x28 pixel come questa e produca un singolo numero tra 0 e 10, 

15
00:00:59,032 --> 00:01:04,490
dicendoti quale cifra pensa che sia", beh, il compito passa da comicamente banale a 

16
00:01:04,490 --> 00:01:06,180
spaventosamente difficile.

17
00:01:07,160 --> 00:01:09,506
A meno che tu non abbia vissuto in una caverna, 

18
00:01:09,506 --> 00:01:13,271
credo serva motivare l'importanza del deep learning e delle reti neurali per 

19
00:01:13,271 --> 00:01:14,640
il presente e per il futuro.

20
00:01:15,120 --> 00:01:18,974
Quello che voglio fare qui è mostrarti cos'è effettivamente una rete neurale, 

21
00:01:18,974 --> 00:01:21,692
senza conoscenze di base, visualizzando quello che fa, 

22
00:01:21,692 --> 00:01:24,460
non in modo divulgativo, ma come un pezzo di matematica.

23
00:01:25,020 --> 00:01:29,706
La mia speranza è che porti a casa una conoscenza della struttura stessa e che tu sappia 

24
00:01:29,706 --> 00:01:34,340
cosa significa quando leggi o senti dire che una rete neurale, tra virgolette, "impara".

25
00:01:35,360 --> 00:01:37,746
Questo video sarà dedicato alla componente strutturale, 

26
00:01:37,746 --> 00:01:40,260
mentre il successivo affronterà il tema dell'apprendimento.

27
00:01:40,960 --> 00:01:43,594
Quello che faremo sarà mettere insieme una rete neurale 

28
00:01:43,594 --> 00:01:46,040
che impara a riconoscere delle cifre scritte a mano.

29
00:01:49,360 --> 00:01:53,933
Questo è un esempio un po' classico per introdurre l'argomento e sono felice di attenermi 

30
00:01:53,933 --> 00:01:58,303
alla letteratura, perché alla fine dei due video voglio indicarti alcune risorse dove 

31
00:01:58,303 --> 00:02:02,622
puoi trovare di più e dove puoi scaricare il codice che fa questo e giocarci sul tuo 

32
00:02:02,622 --> 00:02:03,080
computer.

33
00:02:05,040 --> 00:02:09,731
Esistono molte varianti delle reti neurali e negli ultimi anni c'è stato 

34
00:02:09,731 --> 00:02:14,745
un boom della ricerca su queste varianti, ma in questi due video introduttivi 

35
00:02:14,745 --> 00:02:19,180
io e te ci limiteremo a vedere la forma più semplice, senza fronzoli.

36
00:02:19,860 --> 00:02:25,492
Questo è un prerequisito necessario per comprendere le varianti moderne più potenti e, 

37
00:02:25,492 --> 00:02:28,600
credimi, abbiamo tanto materiale per divertirci.

38
00:02:29,120 --> 00:02:33,650
Ma anche in questa forma più semplice può imparare a riconoscere le cifre scritte a mano, 

39
00:02:33,650 --> 00:02:36,520
il che è una cosa piuttosto interessante per un computer.

40
00:02:37,480 --> 00:02:40,309
E allo stesso tempo vedrai come non riesce a soddisfare 

41
00:02:40,309 --> 00:02:42,280
un paio di speranze che potremmo avere.

42
00:02:43,380 --> 00:02:46,974
Come suggerisce il nome, le reti neurali si ispirano al cervello, 

43
00:02:46,974 --> 00:02:48,500
ma vediamo di capire meglio.

44
00:02:48,520 --> 00:02:51,660
Cosa sono i neuroni, e in che modo sono collegati tra loro?

45
00:02:52,500 --> 00:02:56,252
In questo momento, quando parlo di neurone, voglio che tu pensi a un 

46
00:02:56,252 --> 00:03:00,440
oggetto che contiene un numero, nello specifico un numero compreso tra 0 e 1.

47
00:03:00,680 --> 00:03:02,560
Davvero, non è molto più di questo.

48
00:03:03,780 --> 00:03:09,208
Ad esempio, la rete inizia con un gruppo di neuroni corrispondenti a ciascuno 

49
00:03:09,208 --> 00:03:14,220
dei 28x28 pixel dell'immagine di ingresso, per un totale di 784 neuroni.

50
00:03:14,700 --> 00:03:19,511
Ognuno di questi contiene un numero che rappresenta il valore della scala di grigi 

51
00:03:19,511 --> 00:03:24,380
del pixel corrispondente, che va da 0 per i pixel neri fino a 1 per i pixel bianchi.

52
00:03:25,300 --> 00:03:29,943
Questo numero all'interno del neurone è chiamato attivazione e l'immagine che potresti 

53
00:03:29,943 --> 00:03:34,160
avere in mente è che ogni neurone si illumina quando la sua attivazione è alta.

54
00:03:36,720 --> 00:03:41,860
Quindi tutti questi 784 neuroni costituiscono il primo strato della nostra rete.

55
00:03:46,500 --> 00:03:49,057
Passando all'ultimo strato, questo ha 10 neuroni, 

56
00:03:49,057 --> 00:03:51,360
ognuno dei quali rappresenta una delle cifre.

57
00:03:52,040 --> 00:03:56,166
L'attivazione di questi neuroni, sempre un numero compreso tra 0 e 1, 

58
00:03:56,166 --> 00:04:01,058
rappresenta quanto il sistema pensa che una determinata immagine corrisponda a una 

59
00:04:01,058 --> 00:04:02,120
determinata cifra.

60
00:04:03,040 --> 00:04:06,594
Ci sono anche un paio di strati intermedi chiamati livelli nascosti, 

61
00:04:06,594 --> 00:04:10,303
che per il momento dovrebbero essere solo un enorme punto interrogativo 

62
00:04:10,303 --> 00:04:13,600
su come verrà gestito il processo di riconoscimento delle cifre.

63
00:04:14,260 --> 00:04:17,032
In questa rete ho scelto di avere due strati nascosti, 

64
00:04:17,032 --> 00:04:20,560
ciascuno con 16 neuroni, si tratta di una scelta piuttosto arbitraria.

65
00:04:21,019 --> 00:04:25,308
A dire il vero ho scelto due strato per come voglio motivare la struttura tra un attimo, 

66
00:04:25,308 --> 00:04:28,200
e 16, beh, era solo un bel numero da inserire sullo schermo.

67
00:04:28,780 --> 00:04:32,340
In pratica c'è molto spazio per sperimentare una struttura specifica.

68
00:04:33,020 --> 00:04:35,877
Per come funziona la rete, le attivazioni di uno strato 

69
00:04:35,877 --> 00:04:38,480
determinano le attivazioni dello strato successivo.

70
00:04:39,200 --> 00:04:42,172
Il cuore della rete come meccanismo di elaborazione delle 

71
00:04:42,172 --> 00:04:45,504
informazioni si riduce esattamente al modo in cui le attivazioni 

72
00:04:45,504 --> 00:04:48,580
di uno strato producono attivazioni nello strato successivo.

73
00:04:49,140 --> 00:04:53,810
Si tratta di un'analogia con il modo in cui, nelle reti biologiche di neuroni, 

74
00:04:53,810 --> 00:04:57,180
alcuni gruppi di neuroni si attivano e ne attivano altri.

75
00:04:58,120 --> 00:05:02,080
La rete che ti mostro è già stata addestrata a riconoscere le cifre. 

76
00:05:02,080 --> 00:05:03,400
Ti mostro cosa intendo.

77
00:05:03,640 --> 00:05:08,137
Ciò significa che se inserisci un'immagine, attivando tutti i 784 neuroni dello 

78
00:05:08,137 --> 00:05:12,016
strato di input in base alla luminosità di ogni pixel dell'immagine, 

79
00:05:12,016 --> 00:05:16,626
quel modello di attivazioni provoca una propagazione molto specifica nello strato 

80
00:05:16,626 --> 00:05:19,381
successivo, che si propaga in quello successivo, 

81
00:05:19,381 --> 00:05:22,080
che infine dà un pattern nello strato di output.

82
00:05:22,560 --> 00:05:26,525
E il neurone più luminoso di questo strato di uscita è la decisione della rete, 

83
00:05:26,525 --> 00:05:29,400
per così dire, su quale cifra rappresenta questa immagine.

84
00:05:32,560 --> 00:05:36,161
Prima della matematica di come uno strato influenza l'altro o di come 

85
00:05:36,161 --> 00:05:39,815
funziona l'addestramento, parliamo del perché è ragionevole aspettarsi 

86
00:05:39,815 --> 00:05:43,520
che una struttura a strati come questa si comporti in modo intelligente.

87
00:05:44,060 --> 00:05:45,220
Cosa ci aspettiamo qui?

88
00:05:45,400 --> 00:05:47,600
Cosa speriamo che stiano facendo questi strati intermedi?

89
00:05:48,920 --> 00:05:53,520
Quando tu o io riconosciamo delle cifre, mettiamo insieme vari pezzetti.

90
00:05:54,200 --> 00:05:56,820
Un 9 ha un anello in alto e una linea a destra.

91
00:05:57,380 --> 00:06:01,180
Anche l'8 ha un anello in alto, ma è abbinato a un altro anello in basso.

92
00:06:01,980 --> 00:06:06,820
Un 4 si divide fondamentalmente in tre linee specifiche e cose del genere.

93
00:06:07,600 --> 00:06:11,685
Ora, in un mondo perfetto, potremmo sperare che ogni neurone del penultimo 

94
00:06:11,685 --> 00:06:14,518
strato corrisponda a una di queste sottocomponenti, 

95
00:06:14,518 --> 00:06:18,114
in modo che ogni volta che inserisci un'immagine con, ad esempio, 

96
00:06:18,114 --> 00:06:22,036
un ciclo in alto, come un 9 o un 8, ci sia qualche neurone specifico la 

97
00:06:22,036 --> 00:06:23,780
cui attivazione sarà vicina a 1.

98
00:06:24,500 --> 00:06:27,656
E non mi riferisco a questo specifico anello di pixel, 

99
00:06:27,656 --> 00:06:31,560
ma la speranza è che qualsiasi anello in alto attivi questo neurone.

100
00:06:32,440 --> 00:06:36,187
In questo modo, per passare dal terzo all'ultimo livello è sufficiente 

101
00:06:36,187 --> 00:06:40,040
imparare quale combinazione di sottocomponenti corrisponde a quali cifre.

102
00:06:41,000 --> 00:06:43,110
Naturalmente, questo non fa che aggravare il problema, 

103
00:06:43,110 --> 00:06:46,373
perché come si fa a riconoscere queste sottocomponenti o a imparare quali dovrebbero 

104
00:06:46,373 --> 00:06:47,640
essere le sottocomponenti giusti?

105
00:06:48,060 --> 00:06:53,060
E non ho ancora parlato di come uno strato influenzi l'altro, ma seguimi per un momento.

106
00:06:53,680 --> 00:06:56,680
Anche il riconoscimento di un anello può essere diviso in sottoproblemi.

107
00:06:57,280 --> 00:07:00,310
Un modo ragionevole per farlo è quello di riconoscere 

108
00:07:00,310 --> 00:07:02,780
innanzitutto i vari bordi che lo compongono.

109
00:07:03,780 --> 00:07:08,129
Allo stesso modo, una linea lunga, come quella che puoi vedere nelle cifre 1, 

110
00:07:08,129 --> 00:07:11,531
4 o 7, è in realtà solo un bordo lungo, o forse la consideri 

111
00:07:11,531 --> 00:07:14,320
come un certo schema di diversi bordi più piccoli.

112
00:07:15,140 --> 00:07:18,703
Quindi forse la nostra speranza è che ogni neurone del 

113
00:07:18,703 --> 00:07:22,720
secondo strato della rete corrisponda ai vari bordi rilevanti.

114
00:07:23,540 --> 00:07:28,772
Magari quando arriva un'immagine come questa, si accendono tutti i neuroni associati a 

115
00:07:28,772 --> 00:07:33,765
circa 8-10 bordi specifici piccoli, che a loro volta accendono i neuroni associati 

116
00:07:33,765 --> 00:07:36,893
all'anello superiore e a una lunga linea verticale, 

117
00:07:36,893 --> 00:07:39,720
e questi accendono il neurone associato a un 9.

118
00:07:40,680 --> 00:07:44,800
Se questo è ciò che la nostra rete finale effettivamente fa è un'altra questione, 

119
00:07:44,800 --> 00:07:47,162
su cui tornerò quando vedremo l'addestramento, 

120
00:07:47,162 --> 00:07:50,982
ma questa è una speranza che potremmo avere, una sorta di obiettivo con una 

121
00:07:50,982 --> 00:07:52,540
struttura a strati come questa.

122
00:07:53,160 --> 00:07:56,704
Inoltre, puoi immaginare come la capacità di rilevare bordi e pattern 

123
00:07:56,704 --> 00:08:00,300
di questo tipo possa essere utile per il riconoscimento delle immagini.

124
00:08:00,880 --> 00:08:03,363
E anche al di là del riconoscimento delle immagini, 

125
00:08:03,363 --> 00:08:07,280
ci sono tante altre cose interessanti che si suddividono in livelli di astrazione.

126
00:08:08,040 --> 00:08:11,811
Il parsing del parlato, ad esempio, consiste nel prendere l'audio grezzo e 

127
00:08:11,811 --> 00:08:15,835
individuare i suoni distinti, che si combinano per formare determinate sillabe, 

128
00:08:15,835 --> 00:08:20,060
che si combinano per formare parole, che formano frasi e pensieri più astratti, ecc.

129
00:08:21,100 --> 00:08:24,093
Ma tornando al funzionamento effettivo di tutto questo, 

130
00:08:24,093 --> 00:08:28,530
immagina di progettare in questo momento come le attivazioni di uno strato possano 

131
00:08:28,530 --> 00:08:29,920
determinare il successivo.

132
00:08:30,860 --> 00:08:35,943
L'obiettivo è avere un meccanismo che possa combinare i pixel in bordi, 

133
00:08:35,943 --> 00:08:38,980
o i bordi in schemi, o gli schemi in cifre.

134
00:08:39,440 --> 00:08:45,061
Per ingrandire un esempio molto specifico, diciamo che la speranza è che un particolare 

135
00:08:45,061 --> 00:08:50,620
neurone del secondo livello capisca se l'immagine ha o meno un bordo in questa regione.

136
00:08:51,440 --> 00:08:55,100
La domanda da porsi è: quali parametri deve avere la rete?

137
00:08:55,640 --> 00:08:59,652
Quali sono le manopole che dovresti poter regolare in modo che sia abbastanza 

138
00:08:59,652 --> 00:09:03,561
espressivo da catturare questo pattern, o qualsiasi altro pattern di pixel, 

139
00:09:03,561 --> 00:09:07,780
o il pattern per cui più bordi possono formare un anello, e altre cose del genere?

140
00:09:08,720 --> 00:09:12,140
Allora, quello che faremo è assegnare un peso a ciascuna delle 

141
00:09:12,140 --> 00:09:15,560
connessioni tra il nostro neurone e i neuroni del primo strato.

142
00:09:16,320 --> 00:09:17,700
Questi pesi sono solo numeri.

143
00:09:18,540 --> 00:09:21,954
Poi prendi tutte le attivazioni del primo livello e 

144
00:09:21,954 --> 00:09:25,500
calcola la loro somma ponderata in base a questi pesi.

145
00:09:27,700 --> 00:09:31,091
Trovo utile pensare a questi pesi come organizzati in una piccola 

146
00:09:31,091 --> 00:09:34,534
griglia a sé stante e utilizzerò i pixel verdi per indicare i pesi 

147
00:09:34,534 --> 00:09:37,360
positivi e i pixel rossi per indicare i pesi negativi, 

148
00:09:37,360 --> 00:09:41,780
la luminosità di quel pixel è una rappresentazione approssimativa del valore del peso.

149
00:09:42,780 --> 00:09:46,369
Se i pesi associati a quasi tutti i pixel fossero pari a zero, 

150
00:09:46,369 --> 00:09:50,300
ad eccezione di alcuni pesi positivi nella regione che ci interessa, 

151
00:09:50,300 --> 00:09:55,199
allora la somma ponderata di tutti i valori dei pixel equivarrebbe a sommare i valori 

152
00:09:55,199 --> 00:09:57,820
dei pixel solo nella regione che ci interessa.

153
00:09:59,140 --> 00:10:02,374
E se volessi davvero capire se c'è un bordo qui, 

154
00:10:02,374 --> 00:10:06,600
potresti avere dei pesi negativi associati ai pixel circostanti.

155
00:10:07,480 --> 00:10:10,064
Quindi la somma è maggiore quando i pixel centrali 

156
00:10:10,064 --> 00:10:12,700
sono luminosi ma i pixel circostanti sono più scuri.

157
00:10:14,260 --> 00:10:18,928
Quando calcoli una somma ponderata come questa, puoi ottenere un numero qualsiasi, 

158
00:10:18,928 --> 00:10:23,540
ma per questa rete vogliamo che le attivazioni siano un valore compreso tra 0 e 1.

159
00:10:24,120 --> 00:10:27,933
Quindi, una cosa comune da fare è dare questa somma ponderata a una 

160
00:10:27,933 --> 00:10:32,140
funzione che schiaccia la linea dei numeri reali nell'intervallo tra 0 e 1.

161
00:10:32,460 --> 00:10:37,420
Una funzione comune che fa questo è la funzione sigmoide, nota anche come curva logistica.

162
00:10:38,000 --> 00:10:41,677
In pratica, gli input molto negativi finiscono per avvicinarsi a 0, 

163
00:10:41,677 --> 00:10:45,788
quelli positivi per avvicinarsi a 1 e il tutto cresce costantemente intorno 

164
00:10:45,788 --> 00:10:46,600
all'ingresso 0.

165
00:10:49,120 --> 00:10:52,676
Quindi l'attivazione del neurone è fondamentalmente una 

166
00:10:52,676 --> 00:10:56,360
misura di quanto sia positiva la relativa somma ponderata.

167
00:10:57,540 --> 00:10:59,619
Ma forse non è che si vuole che il neurone si 

168
00:10:59,619 --> 00:11:01,880
accenda quando la somma ponderata è maggiore di 0.

169
00:11:02,280 --> 00:11:06,360
Forse vuoi che sia attivo solo quando la somma è maggiore di 10, ad esempio.

170
00:11:06,840 --> 00:11:10,260
In altre parole, vuoi che ci sia un pregiudizio perché sia inattivo.

171
00:11:11,380 --> 00:11:15,552
A questo punto aggiungeremo un altro numero, ad esempio meno 10, 

172
00:11:15,552 --> 00:11:19,660
alla somma ponderata prima di inserirla nella funzione sigmoide.

173
00:11:20,580 --> 00:11:22,440
Questo numero aggiuntivo è chiamato bias.

174
00:11:23,460 --> 00:11:27,438
Quindi i pesi ti dicono quale modello di pixel questo neurone del secondo 

175
00:11:27,438 --> 00:11:31,362
livello sta rilevando e il bias ti dice quanto deve essere alta la somma 

176
00:11:31,362 --> 00:11:35,180
dei pesi prima che il neurone inizi ad attivarsi in modo significativo.

177
00:11:36,120 --> 00:11:37,680
E questo è solo un neurone.

178
00:11:38,280 --> 00:11:44,530
Ogni altro neurone di questo strato sarà collegato a tutti i 784 neuroni pixel 

179
00:11:44,530 --> 00:11:50,940
del primo strato e ognuna di queste 784 connessioni ha un proprio peso associato.

180
00:11:51,600 --> 00:11:54,387
Inoltre, ognuno di essi ha un bias, un altro numero che si 

181
00:11:54,387 --> 00:11:57,600
aggiunge alla somma ponderata prima di schiacciarla con la sigmoide.

182
00:11:58,110 --> 00:11:59,540
E sono tante le cose a cui pensare!

183
00:11:59,960 --> 00:12:05,935
Con questo strato nascosto di 16 neuroni, il totale è di 784 volte 16 pesi, 

184
00:12:05,935 --> 00:12:07,980
oltre a 16 polarizzazioni.

185
00:12:08,840 --> 00:12:11,940
E tutto questo è solo il collegamento tra il primo strato e il secondo.

186
00:12:12,520 --> 00:12:17,340
Anche le connessioni tra gli altri livelli hanno una serie di pesi e bias associati.

187
00:12:18,340 --> 00:12:23,800
Tutto sommato, questa rete ha quasi esattamente 13.000 pesi e bias totali.

188
00:12:23,800 --> 00:12:26,968
13.000 manopole che possono essere regolate e ruotate 

189
00:12:26,968 --> 00:12:29,960
per far sì che la rete si comporti in modi diversi.

190
00:12:31,040 --> 00:12:34,516
Quindi, quando parliamo di apprendimento, ci riferiamo al fatto 

191
00:12:34,516 --> 00:12:38,861
che il computer deve trovare una configurazione valida per tutti questi numeri, 

192
00:12:38,861 --> 00:12:41,360
in modo da risolvere il problema in questione.

193
00:12:42,620 --> 00:12:46,019
Un esperimento a cui pensare che è allo stesso tempo divertente e 

194
00:12:46,019 --> 00:12:49,419
terrificante è quello di immaginare di sedersi e impostare a mano 

195
00:12:49,419 --> 00:12:52,819
tutti questi pesi e pregiudizi, modificando di proposito i numeri 

196
00:12:52,819 --> 00:12:56,580
in modo che il secondo strato raccolga i bordi, il terzo gli schemi, ecc.

197
00:12:56,980 --> 00:13:01,280
Personalmente lo trovo soddisfacente piuttosto che trattare la rete come una totale 

198
00:13:01,280 --> 00:13:04,863
scatola nera, perché quando la rete non funziona come avevi previsto, 

199
00:13:04,863 --> 00:13:09,112
se hai costruito un po' di relazione con il significato effettivo dei pesi e delle 

200
00:13:09,112 --> 00:13:13,412
distorsioni, hai un punto di partenza per sperimentare come modificare la struttura 

201
00:13:13,412 --> 00:13:14,180
per migliorare.

202
00:13:14,960 --> 00:13:18,131
Oppure, quando la rete funziona ma non per i motivi che ci si aspettava, 

203
00:13:18,131 --> 00:13:21,693
scavare per capire cosa stanno facendo i pesi e le distorsioni è un buon modo per 

204
00:13:21,693 --> 00:13:25,385
mettere in discussione le proprie ipotesi ed esporre l'intero spazio delle possibili 

205
00:13:25,385 --> 00:13:25,820
soluzioni.

206
00:13:26,840 --> 00:13:30,680
A proposito, la funzione attuale è un po' complicata da scrivere, non credi?

207
00:13:32,500 --> 00:13:37,140
Ti mostrerò quindi un modo più compatto di rappresentare queste connessioni.

208
00:13:37,660 --> 00:13:40,520
Questo è come lo vedresti se decidessi di studiare di più le reti neurali.

209
00:13:41,380 --> 00:13:40,520
Organizza tutte le attivazioni di uno strato in un vettore colonna.

210
00:13:41,380 --> 00:13:49,744
Poi organizza tutti i pesi come una matrice, dove ogni riga corrisponde alle 

211
00:13:49,744 --> 00:13:58,000
connessioni tra uno strato e un particolare neurone dello strato successivo.

212
00:13:58,540 --> 00:14:02,189
Ciò significa che la somma ponderata delle attivazioni del primo 

213
00:14:02,189 --> 00:14:05,725
strato in base a questi pesi corrisponde a uno dei termini del 

214
00:14:05,725 --> 00:14:09,880
prodotto vettoriale della matrice di tutto ciò che abbiamo qui a sinistra.

215
00:14:14,000 --> 00:14:17,833
Gran parte dell'apprendimento automatico si basa su una buona conoscenza 

216
00:14:17,833 --> 00:14:21,352
dell'algebra lineare, quindi se vuoi una comprensione visiva delle 

217
00:14:21,352 --> 00:14:24,503
matrici e del significato della moltiplicazione vettoriale, 

218
00:14:24,503 --> 00:14:28,600
dai un'occhiata alla serie sull'algebra lineare, in particolare al capitolo 3.

219
00:14:29,240 --> 00:14:33,680
Tornando alla nostra espressione, invece di parlare di aggiungere il bias a ciascuno 

220
00:14:33,680 --> 00:14:38,173
di questi valori in modo indipendente, lo rappresentiamo organizzandoli in un vettore 

221
00:14:38,173 --> 00:14:42,300
e aggiungendo l'intero vettore al prodotto vettoriale della matrice precedente.

222
00:14:43,280 --> 00:14:47,081
Poi, come ultimo passo, avvolgerò una sigmoide intorno all'esterno 

223
00:14:47,081 --> 00:14:50,655
e ciò che dovrebbe rappresentare è che applicherai la funzione 

224
00:14:50,655 --> 00:14:54,740
sigmoide a ogni componente specifica del vettore risultante all'interno.

225
00:14:55,940 --> 00:15:00,627
Quindi, una volta scritta questa matrice di pesi e questi vettori come simboli propri, 

226
00:15:00,627 --> 00:15:04,345
puoi comunicare l'intera transizione delle attivazioni da uno strato 

227
00:15:04,345 --> 00:15:08,116
all'altro in una piccola espressione estremamente stretta e ordinata, 

228
00:15:08,116 --> 00:15:11,349
che rende il codice pertinente molto più semplice e veloce, 

229
00:15:11,349 --> 00:15:15,660
dato che molte librerie ottimizzano al massimo la moltiplicazione delle matrici.

230
00:15:17,820 --> 00:15:21,460
Ricordi che ho detto che i neuroni sono semplicemente oggetti che contengono numeri?

231
00:15:22,220 --> 00:15:27,593
Ovviamente i numeri specifici che contengono dipendono dall'immagine che inserisci, 

232
00:15:27,593 --> 00:15:33,094
quindi è più corretto pensare a ogni neurone come a una funzione che riceve le uscite 

233
00:15:33,094 --> 00:15:38,340
di tutti i neuroni dello strato precedente e produce un numero compreso tra 0 e 1.

234
00:15:39,200 --> 00:15:43,166
In realtà l'intera rete è solo una funzione che riceve 

235
00:15:43,166 --> 00:15:47,060
784 numeri come input e ne restituisce 10 come output.

236
00:15:47,560 --> 00:15:50,564
Si tratta di una funzione assurdamente complicata, 

237
00:15:50,564 --> 00:15:55,571
con 13.000 parametri sotto forma di pesi e bias che individuano determinati schemi e 

238
00:15:55,571 --> 00:16:00,460
che comporta l'iterazione di molti prodotti vettoriali di matrici e della funzione 

239
00:16:00,460 --> 00:16:02,640
sigmoide, ma è comunque una funzione.

240
00:16:03,400 --> 00:16:06,660
In un certo senso è rassicurante il fatto che sembri complicato.

241
00:16:07,340 --> 00:16:09,786
Insomma, se fosse più semplice, che speranza avremmo 

242
00:16:09,786 --> 00:16:12,280
di affrontare la sfida del riconoscimento delle cifre?

243
00:16:13,340 --> 00:16:14,700
E come affronta questa sfida?

244
00:16:15,080 --> 00:16:17,356
Come fa questa rete ad apprendere i pesi e i bias 

245
00:16:17,356 --> 00:16:19,360
appropriati semplicemente osservando i dati?

246
00:16:20,140 --> 00:16:23,103
Ebbene, questo è ciò che mostrerò nel prossimo video e 

247
00:16:23,103 --> 00:16:26,120
approfondirò anche l'aspetto di questa particolare rete.

248
00:16:27,580 --> 00:16:32,703
A questo punto dovrei dire di iscrivervi per essere avvisati quando escono nuovi video, 

249
00:16:32,703 --> 00:16:37,420
ma realisticamente la maggior parte di voi non riceve notifiche da YouTube, vero?

250
00:16:38,020 --> 00:16:40,910
Forse, più onestamente, dovrei dire di iscrivervi, 

251
00:16:40,910 --> 00:16:45,896
in modo che le reti neurali alla base dell'algoritmo di YouTube siano indotte a credere 

252
00:16:45,896 --> 00:16:47,880
che tu voglia altri contenuti così.

253
00:16:48,560 --> 00:16:49,940
In ogni caso, restate aggiornati.

254
00:16:50,760 --> 00:16:53,500
Grazie mille a tutti coloro che sostengono questi video su Patreon.

255
00:16:54,000 --> 00:16:58,740
Quest'estate sono stato un po' lento nel portare avanti la serie di probabilità, 

256
00:16:58,740 --> 00:17:01,900
ma dopo questo progetto mi ci sto butterò a capofitto.

257
00:17:03,600 --> 00:17:07,046
Per concludere, c'è qui Lisha Li, che ha svolto il suo dottorato di ricerca 

258
00:17:07,046 --> 00:17:10,538
sull'aspetto teorico del deep learning e che ora lavora presso la società di 

259
00:17:10,538 --> 00:17:14,619
venture capital Amplify Partners, che ha fornito parte dei finanziamenti per questo video.

260
00:17:15,460 --> 00:17:19,119
Quindi, Lisha, una cosa di cui dobbiamo parlare un momento è la funzione sigmoide.

261
00:17:19,700 --> 00:17:23,303
A quanto ho capito, le prime reti usano questo metodo per schiacciare 

262
00:17:23,303 --> 00:17:26,597
la somma ponderata in quell'intervallo compreso tra zero e uno, 

263
00:17:26,597 --> 00:17:29,840
motivato dall'analogia biologica tra neuroni inattivi o attivi.

264
00:17:30,280 --> 00:17:30,300
Esattamente.

265
00:17:30,560 --> 00:17:34,040
Ma sono relativamente poche le reti moderne che utilizzano la sigmoide.

266
00:17:34,320 --> 00:17:34,320
Sì.

267
00:17:34,440 --> 00:17:35,540
È una vecchia scuola, giusto?

268
00:17:35,760 --> 00:17:38,980
Sì, o meglio, il ReLU sembra essere molto più facile da addestrare.

269
00:17:39,400 --> 00:17:42,340
E ReLU sta per unità lineare rettificata?

270
00:17:42,680 --> 00:17:47,282
Sì, è questo tipo di funzione in cui stai solo prendendo un massimo 

271
00:17:47,282 --> 00:17:51,952
di 0 e a dove a è dato da quello che stavi spiegando nel video e ciò 

272
00:17:51,952 --> 00:17:56,960
che era motivato da quello che penso fosse un parzialmente da un'analogia 

273
00:17:56,960 --> 00:18:01,360
biologica con come I neuroni potrebbero essere attivati ​​o meno.

274
00:18:01,360 --> 00:18:06,157
E se passa una certa soglia sarebbe la funzione di identità, ma se così non fosse, 

275
00:18:06,157 --> 00:18:10,840
non sarebbe stato attivato e sarebbe zero, quindi è un po' una semplificazione.


276
00:18:11,160 --> 00:18:16,503
L'uso delle sigmoidi non aiutava l'addestramento o lo rendeva molto difficile, 

277
00:18:16,503 --> 00:18:20,967
quindi si è provato con la ReLU e si è visto che funzionava molto 

278
00:18:20,967 --> 00:18:24,620
bene per queste reti neurali incredibilmente profonde.

279
00:18:25,100 --> 00:18:25,640
Va bene grazie Lisha.

