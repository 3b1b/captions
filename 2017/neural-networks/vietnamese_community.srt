1
00:00:04,020 --> 00:00:10,680
Đây là số ba. Nó được viết phức tạp/rắc rối và có độ phân giải rất thấp 28 28 pixel.

2
00:00:10,680 --> 00:00:15,660
Nhưng não của bạn không có vấn đề khi nhận dạng đó là số ba. Tôi hy vọng bạn có thể dành một chút thời gian để đánh giá vấn đề này.

3
00:00:15,900 --> 00:00:18,949
Làm thế nào bộ não có thể làm điều này dễ dàng?

4
00:00:18,949 --> 00:00:23,160
Ý tôi là, nó đây. Đây cũng được nhận dạng là số ba,

5
00:00:23,160 --> 00:00:28,060
mặc dù các giá trị cụ thể của mỗi điểm ảnh là rất khác nhau trong những hình ảnh khác.

6
00:00:28,080 --> 00:00:33,780
Các tế bào nhạy sáng đặc biệt trong mắt của bạn sẽ phát ra tín hiệu khi bạn nhìn thấy số ba này.

7
00:00:33,780 --> 00:00:36,800
Khi bạn nhìn thấy số ba này thì sẽ tạo ra tín hiệu rất khác nhau.

8
00:00:37,140 --> 00:00:40,610
Nhưng có một cái gì đó ở trong vỏ não thị giác
thông minh của bạn ...

9
00:00:41,129 --> 00:00:48,139
đã giải quyết chúng bằng cách đưa ra hình mẫu tương tự trong khi cùng lúc nhận dạng hình ảnh khác bằng cách  riêng biệt của mình.

10
00:00:48,840 --> 00:00:55,039
Nhưng nếu tôi nói với bạn: Hãy ngồi xuống và viết cho tôi một chương trình mà đầu vào là một bức ảnh 28x28 pixel ...

11
00:00:55,379 --> 00:01:01,759
... như thế này và kết quả đầu ra là một số duy nhất nằm trong khoảng từ 0 đến 10 giống với số trong bức hình nhất.

12
00:01:02,250 --> 00:01:06,139
Nhiệm vụ này có vẻ khó khăn.

13
00:01:06,740 --> 00:01:08,260
Trừ khi bạn đã được sống dưới một tảng đá (?????)

14
00:01:08,260 --> 00:01:14,580
Tôi nghĩ là tôi không cần phải giải thích sự liên quan.Các phần quan trọng của Học máy và Mạng neural, sẽ được giới thiệu trong tương lai.

15
00:01:14,640 --> 00:01:18,420
Nhưng những gì tôi muốn làm ở đây là giúp bạn hiểu ntn là một mạng Neural thực sự

16
00:01:18,660 --> 00:01:24,229
Giả sử không có nền tảng và để giúp hình dung những gì nó hoạt động hãy biểu diễn chúng như 1 mảng của toán học.

17
00:01:24,570 --> 00:01:28,310
Hy vọng của tôi chỉ là bạn có thể hình dung được cấu trúc của 1 mạng neural.

18
00:01:28,380 --> 00:01:34,399
Thúc đẩy và giúp bạn hình dung được khi bạn đọc hoặc nghe về quá trình học của một mạng neural.

19
00:01:34,950 --> 00:01:40,249
Video này sẽ chỉ giới thiệu các thành phần cấu trúc chính của mạng neural và video sau sẽ giải thích quá trình học của nó.

20
00:01:40,530 --> 00:01:45,950
Những gì chúng ta sẽ làm là phân tích một mạng lưới thần kinh có thể học cách nhận dạng chữ số viết tay.

21
00:01:49,270 --> 00:01:51,329
Đây là một ví dụ khá cơ bản.

22
00:01:51,520 --> 00:01:56,759
Giới thiệu chủ đề ( tôi rất vui khi được gắn bó với hiện trạng ở đây vì ở phần cuối của video thứ 2 tôi muốn giới thiệu ...

23
00:01:56,760 --> 00:02:02,099
Bạn có một vài tài nguyên tốt, nơi bạn có thể tìm hiểu thêm và tải về mã nguồn và thực thi nó ...

24
00:02:02,100 --> 00:02:04,100
trên máy tính của riêng bạn.

25
00:02:04,750 --> 00:02:08,970
Có rất nhiều nhiều biến thể của mạng lưới thần kinh trong những năm gần đây.

26
00:02:08,970 --> 00:02:11,970
Đã có một sự bùng nổ trong nghiên cứu đối với các biến thể.

27
00:02:12,130 --> 00:02:19,019
Nhưng trong 2 video giới thiệu này chỉ giới thiễu những hình thức đơn giản nhất mà không có bổ sung gì thêm.

28
00:02:19,300 --> 00:02:21,040
Đây là một điều cần thiết - ...

29
00:02:21,040 --> 00:02:24,510
... điều kiện tiên quyết cho sự tìm hiểu bất kỳ biến thể hiện đại và mạnh mẽ hơn của mạng neural.

30
00:02:24,760 --> 00:02:28,199
Tin tôi đi, nó vẫn có rất điều nhiều phức tạp làm rối loạn suy nghĩ của chúng tôi.

31
00:02:28,690 --> 00:02:32,820
Nhưng ngay cả ở dạng đơn giản nhất này nó vẫn có thể học cách nhận dạng chữ số viết tay

32
00:02:32,820 --> 00:02:36,180
Đó là một điều khá tốt để một máy tính để có thể làm được.

33
00:02:37,120 --> 00:02:41,960
Và đồng thời, bạn sẽ thấy một vài nhược điểm của nó.

34
00:02:43,090 --> 00:02:48,179
Giống như tên gọi, mạng neural được lấy cảm hứng từ bộ não, nhưng chúng ta hãy cùng mổ xẻ nó nào ;)

35
00:02:48,520 --> 00:02:51,389
các tế bào thần kinh là gì và chúng được họ liên kết với nhau như thế nào?

36
00:02:52,090 --> 00:02:57,750
Ngay bây giờ khi tôi nói về neuron, tôi muốn tất cả các bạn suy nghĩ về một thứ có chứa 1 chữ số

37
00:02:58,209 --> 00:03:02,129
Cụ thể, một số từ 0 & 1 và thực sự không nhiều hơn.

38
00:03:03,430 --> 00:03:11,130
Ví dụ mạng bắt đầu với một loạt các tế bào thần kinh tương ứng với mỗi 28x28 pixel của ảnh đầu vào

39
00:03:11,400 --> 00:03:12,460
có nghĩa là

40
00:03:12,460 --> 00:03:20,240
784 tế bào thần kinh - mỗi 1 tế bào đại diện cho giá trị màu xám của điểm ảnh tương ứng

41
00:03:20,769 --> 00:03:24,299
từ 0 (đối với pixel đen) đến 1 ( cho pixel trắng).

42
00:03:24,910 --> 00:03:30,419
Con số này bên trong tế bào thần kinh được gọi là giá trị kích hoạt của nó và hình ảnh mà bạn có thể có trong tâm trí là đây.

43
00:03:30,420 --> 00:03:33,959
Điều đó có nghĩa là tế bào thần kinh được được kích hoạt khi đầu vào của nó là một số lượng lớn?

44
00:03:36,260 --> 00:03:41,559
Vì vậy, tất cả 784 các tế bào thần kinh tạo nên lớp đầu tiên của mạng neuron.

45
00:03:45,990 --> 00:03:51,289
Bây giờ nhảy qua đến lớp cuối cùng , lớp này có mười tế bào thần kinh, mỗi tế bào đại diện cho một số tương ứng (1 -10).

46
00:03:51,570 --> 00:03:56,239
giá trị kích hoạt của các tế bào thần kinh là một số số 0 & 1.

47
00:03:56,880 --> 00:04:00,049
Đại diện cho bao nhiêu hệ thống cho rằng một hình ảnh nhất định?

48
00:04:00,720 --> 00:04:05,990
Tương ứng với một chữ số nhất định. Ngoài ra còn có một vài lớp ở giữa gọi là lớp ẩn

49
00:04:06,180 --> 00:04:07,770
Mà trong thời gian này?

50
00:04:07,770 --> 00:04:13,549
Nên chỉ là một dấu hỏi khổng lồ rằng quá trình xử lý này nhận dạng chữ số sẽ được thực hiện ntn?

51
00:04:13,740 --> 00:04:20,209
Trong mạng này tôi đã chọn hai lớp ẩn mỗi người với 16 tế bào thần kinh và thừa nhận đó là loại một sự lựa chọn tùy ý

52
00:04:20,609 --> 00:04:24,889
phải trung thực tôi đã chọn hai lớp dựa trên cách tôi muốn thúc đẩy cấu trúc chỉ trong một khoảnh khắc và

53
00:04:25,350 --> 00:04:29,179
16 rõ rằng chỉ là một số đẹp để phù hợp trên màn hình trong thực tế

54
00:04:29,180 --> 00:04:32,209
Có rất nhiều chỗ cho thử nghiệm với một cấu trúc cụ thể ở đây

55
00:04:32,730 --> 00:04:38,329
Cách mạng hoạt động kích hoạt trong một lớp xác định kích hoạt của lớp tiếp theo

56
00:04:38,760 --> 00:04:45,349
Và dĩ nhiên trung tâm của mạng như một cơ chế xử lý thông tin đi xuống đến chính xác cách thức những

57
00:04:45,570 --> 00:04:48,409
kích hoạt từ một lớp mang về kích hoạt trong lớp tiếp theo

58
00:04:48,900 --> 00:04:54,859
Nó có nghĩa là phải lỏng lẻo tương tự như thế nào trong các mạng sinh học của tế bào thần kinh một số nhóm tế bào thần kinh bắn

59
00:04:55,410 --> 00:04:57,410
gây ra một số người khác để bắn

60
00:04:57,570 --> 00:04:58,340
Bây giờ mạng

61
00:04:58,340 --> 00:05:03,019
Tôi thấy ở đây đã được huấn luyện để nhận ra chữ số và để tôi chỉ cho bạn những gì tôi có ý nghĩa bởi đó

62
00:05:03,140 --> 00:05:06,580
Nó có nghĩa là nếu bạn ăn trong một ánh sáng hình ảnh lên tất cả

63
00:05:06,640 --> 00:05:11,780
784 tế bào thần kinh của lớp đầu vào theo độ sáng của mỗi điểm ảnh trong hình ảnh

64
00:05:12,330 --> 00:05:17,029
Đó là mô hình của kích hoạt gây ra một số mô hình rất cụ thể trong lớp tiếp theo

65
00:05:17,190 --> 00:05:19,309
Gây ra một số mô hình trong một sau nó?

66
00:05:19,440 --> 00:05:22,190
Mà cuối cùng đưa ra một số mô hình trong lớp đầu ra và?

67
00:05:22,350 --> 00:05:29,359
Các tế bào thần kinh sáng nhất mà lớp đầu ra là sự lựa chọn của mạng như vậy để nói về những gì chữ số hình ảnh này thể hiện?

68
00:05:32,070 --> 00:05:36,859
Và trước khi nhảy vào toán học trong bao một lớp ảnh hưởng đến các công trình đào tạo tiếp theo hoặc làm thế nào?

69
00:05:37,140 --> 00:05:43,069
Chúng ta hãy chỉ nói về lý do tại sao nó thậm chí còn hợp lý để mong đợi một cấu trúc lớp như thế này để hành xử một cách thông minh

70
00:05:43,800 --> 00:05:48,260
chúng tôi đang mong đợi gì ở đây? hy vọng tốt nhất cho những gì các lớp trung lưu có thể làm là gì?

71
00:05:48,860 --> 00:05:56,720
Vâng khi bạn hoặc tôi nhận ra chữ số chúng ta mảnh thành phần với nhau khác nhau chín có một vòng lên phía trên và một đường ở bên phải

72
00:05:57,260 --> 00:06:01,280
8 cũng có một vòng lặp lên hàng đầu, nhưng nó đi kèm với một vòng lặp xuống thấp

73
00:06:02,020 --> 00:06:06,599
Một 4 về cơ bản bị phá vỡ thành ba dòng cụ thể và những thứ tương tự

74
00:06:07,180 --> 00:06:11,970
Bây giờ trong một thế giới hoàn hảo chúng ta có thể hy vọng rằng mỗi tế bào thần kinh trong lớp thứ hai-to-cuối cùng

75
00:06:12,640 --> 00:06:14,729
tương ứng với một trong những thành phần phụ

76
00:06:14,890 --> 00:06:19,740
bất cứ lúc nào mà bạn ăn trong một hình ảnh với nói một vòng lên đầu như một 9 hoặc 8

77
00:06:19,870 --> 00:06:21,220
Có một số cụ thể

78
00:06:21,220 --> 00:06:27,749
Neuron mà kích hoạt sẽ được gần gũi với một và tôi không có nghĩa là vòng lặp cụ thể này của pixel với hy vọng sẽ là bất kỳ

79
00:06:28,090 --> 00:06:35,039
mô hình thường điên rồ đối với bộ đầu ra tế bào thần kinh này mà cách đi từ lớp ba đến người cuối cùng

80
00:06:35,380 --> 00:06:39,960
chỉ đòi hỏi học tập mà sự kết hợp của các thành phần phụ nào tương ứng với chữ số

81
00:06:40,510 --> 00:06:42,810
Tất nhiên đó chỉ là vấn đề đá xuống đường

82
00:06:42,910 --> 00:06:49,019
Bởi vì làm thế nào bạn có thể nhận ra các thành phần phụ hoặc thậm chí tìm hiểu những gì các thành phần phụ phải nên và tôi vẫn chưa thậm chí đã nói về

83
00:06:49,020 --> 00:06:52,829
Làm thế nào một lớp ảnh hưởng đến sau nhưng chạy với tôi trên một này trong chốc lát

84
00:06:53,650 --> 00:06:56,340
công nhận một vòng lặp cũng có thể phá vỡ thành bài toán

85
00:06:56,860 --> 00:07:02,550
Một cách hợp lý để làm điều này là lần đầu tiên nhận ra các cạnh nhỏ khác nhau mà làm cho nó lên

86
00:07:03,520 --> 00:07:08,910
Tương tự như vậy một đường dài như các loại bạn có thể thấy trong các chữ số 1 hoặc 4 hoặc 7

87
00:07:08,910 --> 00:07:14,279
Vâng đó là thực sự chỉ là một cạnh dài hoặc có thể bạn nghĩ về nó như một khuôn mẫu nhất định của nhiều cạnh nhỏ

88
00:07:14,740 --> 00:07:19,379
Vì vậy, có lẽ hy vọng của chúng tôi là mỗi tế bào thần kinh trong lớp thứ hai của mạng

89
00:07:20,290 --> 00:07:22,650
tương ứng với nhiều cạnh ít liên quan

90
00:07:23,230 --> 00:07:28,259
Có lẽ khi một hình ảnh như thế này do thỏa thuận hợp nó sáng lên tất cả các tế bào thần kinh

91
00:07:28,720 --> 00:07:31,649
gắn liền với khoảng tám đến mười cạnh ít cụ thể

92
00:07:31,930 --> 00:07:36,930
mà lần lượt sáng lên các tế bào thần kinh liên quan đến vòng lặp trên và một đường thẳng đứng dài và

93
00:07:37,300 --> 00:07:39,599
Những ánh sáng lên các tế bào thần kinh liên kết với một chín

94
00:07:40,300 --> 00:07:41,100
có hay không

95
00:07:41,100 --> 00:07:47,070
Đây là những gì mạng chính thức của chúng tôi thực sự không là một câu hỏi, một trong đó tôi sẽ trở lại một lần chúng ta thấy làm thế nào để đào tạo mạng

96
00:07:47,350 --> 00:07:52,170
Nhưng đây là một hy vọng rằng chúng tôi có thể có. Một loại mục tiêu với cấu trúc lớp như thế này

97
00:07:53,020 --> 00:07:59,340
Hơn nữa bạn có thể tưởng tượng như thế nào có thể phát hiện các cạnh và các mẫu như thế này sẽ thực sự hữu ích cho các nhiệm vụ nhận dạng hình ảnh khác

98
00:07:59,740 --> 00:08:06,749
Và ngay cả khi không còn nhận ra hình ảnh có tất cả các loại vật thông minh bạn có thể muốn làm phá vỡ mà xuống thành các lớp trừu tượng

99
00:08:07,690 --> 00:08:14,670
Phân tích bài phát biểu ví dụ liên quan đến việc dùng âm thanh thô và chọn ra những âm thanh khác nhau kết hợp để tạo âm tiết nhất định

100
00:08:15,070 --> 00:08:19,829
Trong đó kết hợp để tạo thành lời mà kết hợp để tạo nên cụm từ và những suy nghĩ trừu tượng hơn vv

101
00:08:20,770 --> 00:08:25,710
Nhưng nhận được trở lại như thế nào những điều này thực sự hoạt động hình ảnh chính mình ngay bây giờ thiết kế

102
00:08:25,710 --> 00:08:30,449
Làm thế nào chính xác kích hoạt trong một lớp có thể xác định kích hoạt trong tiếp theo?

103
00:08:30,670 --> 00:08:35,879
Mục đích là để có một số cơ chế mà hình dung có thể kết hợp điểm ảnh vào mép

104
00:08:35,880 --> 00:08:41,430
Hoặc cạnh vào mẫu hoặc mô hình thành chữ số và để phóng to trên một ví dụ rất cụ thể

105
00:08:41,950 --> 00:08:44,189
Hãy nói rằng hy vọng là một đặc biệt

106
00:08:44,380 --> 00:08:50,430
Neuron trong lớp thứ hai để chọn lên trên hay không hình ảnh có lợi thế trong khu vực này đây

107
00:08:50,950 --> 00:08:54,960
Câu hỏi trong tầm tay là những gì các thông số nên mạng có

108
00:08:55,270 --> 00:09:02,490
những gì quay và nút bấm bạn có thể tinh chỉnh sao cho nó đủ biểu cảm để có khả năng nắm bắt được mô hình này hoặc

109
00:09:02,590 --> 00:09:07,290
Bất kỳ mô hình điểm ảnh khác hoặc mô hình mà nhiều cạnh có thể làm cho một vòng lặp và những thứ khác như vậy?

110
00:09:08,290 --> 00:09:15,389
Vâng, những gì chúng tôi sẽ làm là gán một trọng lượng cho mỗi một trong những mối liên hệ giữa tế bào thần kinh của chúng tôi và các tế bào thần kinh từ lớp đầu tiên

111
00:09:15,850 --> 00:09:17,850
Những trọng lượng chỉ là con số

112
00:09:18,190 --> 00:09:25,590
sau đó đi tất cả những kích hoạt từ lớp đầu tiên và tính tổng trọng của họ theo những trọng tôi

113
00:09:27,370 --> 00:09:31,680
Thấy hữu ích khi nghĩ về những trọng như đang được tổ chức thành một mạng lưới nhỏ của riêng mình

114
00:09:31,680 --> 00:09:37,079
Và tôi sẽ sử dụng pixel màu xanh lá cây để chỉ trọng lượng tích cực và điểm ảnh màu đỏ để chỉ ra trọng âm

115
00:09:37,240 --> 00:09:41,670
Trong trường hợp độ sáng của điểm ảnh đó là một số mô tả lỏng lẻo của các giá trị trọng lượng?

116
00:09:42,400 --> 00:09:45,840
Bây giờ nếu chúng tôi thực hiện các trọng liên quan đến hầu hết các pixel zero

117
00:09:46,150 --> 00:09:49,079
ngoại trừ một số trọng lượng tích cực trong khu vực này mà chúng tôi quan tâm

118
00:09:49,480 --> 00:09:51,310
sau đó lấy tổng trọng số của

119
00:09:51,310 --> 00:09:57,690
tất cả các giá trị pixel thực sự chỉ là số tiền cộng dồn các giá trị của pixel chỉ trong khu vực mà chúng tôi quan tâm

120
00:09:58,870 --> 00:10:04,440
Và, nếu bạn thực sự muốn nó nhặt về việc liệu có là một cạnh ở đây những gì bạn có thể làm là có một số trọng lượng tiêu cực

121
00:10:04,900 --> 00:10:06,900
kết hợp với các điểm ảnh xung quanh

122
00:10:07,030 --> 00:10:12,660
Sau đó tổng hợp là lớn nhất khi những điểm ảnh giữa sáng, nhưng các điểm ảnh xung quanh cũng sẫm

123
00:10:14,279 --> 00:10:18,169
Khi bạn tính toán một tổng trọng như thế này bạn có thể đi ra với bất kỳ số

124
00:10:18,240 --> 00:10:23,180
nhưng cho mạng này những gì chúng tôi muốn là cho kích hoạt được một số giá trị giữa 0 và 1

125
00:10:23,730 --> 00:10:26,599
do đó, một điều phổ biến để làm là để bơm tổng trọng này

126
00:10:26,910 --> 00:10:32,000
Vào một số chức năng mà squishes dòng số thực vào khoảng từ 0 & 1 và

127
00:10:32,190 --> 00:10:37,249
Một chức năng phổ biến mà thực hiện điều này được gọi là hàm sigmoid còn được gọi là một đường cong logistic

128
00:10:37,980 --> 00:10:43,339
về cơ bản đầu vào rất tiêu cực kết thúc gần bằng không đóng góp rất tích cực kết thúc gần 1

129
00:10:43,339 --> 00:10:46,398
và nó chỉ đều đặn tăng xung quanh các đầu vào 0

130
00:10:49,080 --> 00:10:56,029
Vì vậy, sự hoạt hóa của tế bào thần kinh ở đây về cơ bản là một biện pháp như thế nào dương tính tổng trọng có liên quan là

131
00:10:57,450 --> 00:11:01,819
Nhưng có lẽ nó không phải là bạn muốn neuron sáng lên khi tổng trọng lớn hơn 0

132
00:11:02,100 --> 00:11:06,260
Có lẽ bạn chỉ muốn nó được kích hoạt khi tổng lớn hơn nói 10

133
00:11:06,630 --> 00:11:10,279
Đó là bạn muốn có một số thiên vị cho nó không hoạt động

134
00:11:10,860 --> 00:11:16,099
những gì chúng tôi sẽ làm sau đó chỉ cần được thêm vào một số số khác như tiêu cực từ 10 đến tổng trọng này

135
00:11:16,529 --> 00:11:19,669
Trước khi cắm nó thông qua hàm sigmoid squishification

136
00:11:20,220 --> 00:11:22,730
Đó là số bổ sung được gọi là thiên vị

137
00:11:23,310 --> 00:11:29,060
Vì vậy, các trọng số cho bạn biết điểm ảnh mẫu tế bào thần kinh này trong lớp thứ hai được chọn lên trên và thiên vị

138
00:11:29,220 --> 00:11:35,450
cho bạn cao bao nhiêu tổng trọng cần thực hiện trước các tế bào thần kinh bắt đầu nhận được ý nghĩa tích cực

139
00:11:35,910 --> 00:11:37,910
Và đó chỉ là một tế bào thần kinh

140
00:11:38,120 --> 00:11:41,940
Mỗi tế bào thần kinh khác trong lớp này sẽ được kết nối với tất cả

141
00:11:42,320 --> 00:11:50,620
784 pixel tế bào thần kinh từ lớp đầu tiên và mỗi một trong những kết nối 784 đã sức nặng của nó liên kết với nó

142
00:11:51,330 --> 00:11:57,739
cũng mỗi người có một số thiên vị một số số khác mà bạn thêm vào tổng trọng trước khi nứt nó với sigmoid và

143
00:11:58,020 --> 00:12:01,909
Đó là rất nhiều suy nghĩ về với lớp ẩn này của 16 tế bào thần kinh

144
00:12:02,010 --> 00:12:08,270
đó là tổng cộng 784 lần 16 tạ cùng với 16 thành kiến

145
00:12:08,490 --> 00:12:14,029
Và tất cả điều đó chỉ là sự kết nối từ lớp đầu tiên vào thứ hai các kết nối giữa các lớp khác

146
00:12:14,029 --> 00:12:17,208
Ngoài ra, có một loạt các trọng lượng và những thành kiến ​​liên kết với chúng

147
00:12:17,760 --> 00:12:20,680
Tất cả nói và thực hiện mạng này có gần như chính xác

148
00:12:21,280 --> 00:12:23,920
13.000 tổng trọng lượng và những thành kiến

149
00:12:24,280 --> 00:12:29,540
13.000 nút bấm và quay số có thể được tinh chỉnh và quay sang làm cư xử mạng này theo những cách khác nhau

150
00:12:30,520 --> 00:12:32,520
Vì vậy, khi chúng ta nói về học tập?

151
00:12:32,530 --> 00:12:40,199
Có gì đó là đề cập đến là nhận được máy tính để tìm một thiết lập có giá trị cho tất cả các số nhiều nhiều để nó thực sự sẽ giải quyết

152
00:12:40,200 --> 00:12:42,190
vấn đề trong tầm tay

153
00:12:42,190 --> 00:12:43,000
Một ý nghĩ

154
00:12:43,000 --> 00:12:49,979
Thử nghiệm có nghĩa là cùng một lúc vui vẻ và tốt bụng của kinh hoàng là để tưởng tượng ngồi xuống và cài đặt tất cả các trọng lượng và những thành kiến ​​bằng tay

155
00:12:50,380 --> 00:12:56,159
Cố tinh chỉnh các con số để các lớp thứ hai nhặt trên mép lớp thứ ba nhặt trên các mẫu vv

156
00:12:56,350 --> 00:13:01,440
Cá nhân tôi tìm thấy điều này thỏa mãn hơn là chỉ đọc mạng như một tổng hộp đen

157
00:13:01,870 --> 00:13:04,349
Bởi vì khi mạng không thực hiện theo cách bạn

158
00:13:04,600 --> 00:13:11,370
dự đoán nếu bạn đã xây dựng lên một chút của một mối quan hệ với những gì các trọng lượng và những thành kiến ​​thực sự có nghĩa là bạn có một vị trí bắt đầu cho

159
00:13:11,680 --> 00:13:16,289
Thử nghiệm với làm thế nào để thay đổi cấu trúc để cải thiện hoặc khi mạng không làm việc?

160
00:13:16,290 --> 00:13:18,290
Nhưng không phải vì những lý do bạn có thể mong đợi

161
00:13:18,310 --> 00:13:25,169
Đào sâu vào những gì các trọng lượng và những thành kiến ​​đang làm là một cách tốt để thách thức các giả định của bạn và thực sự tiếp xúc với không gian đầy đủ các thể

162
00:13:25,180 --> 00:13:26,350
các giải pháp

163
00:13:26,350 --> 00:13:30,600
Bằng cách này các chức năng thực tế đây là một cồng kềnh chút để viết xuống. Đừng nghĩ rằng bạn?

164
00:13:32,350 --> 00:13:38,460
Vì vậy, hãy để tôi chỉ cho bạn một cách notationally nhỏ gọn hơn mà những kết nối này được đại diện. Đây là cách bạn muốn nhìn thấy nó

165
00:13:38,460 --> 00:13:40,460
Nếu bạn chọn để đọc lên thêm về mạng nơ-ron

166
00:13:41,110 --> 00:13:45,810
Tổ chức tất cả các kích hoạt từ một lớp nào thành một cột như một vector

167
00:13:47,470 --> 00:13:52,320
Sau đó, tổ chức tất cả các trọng như một ma trận trong đó mỗi hàng ma trận

168
00:13:52,900 --> 00:13:57,659
tương ứng với các mối liên hệ giữa một lớp và một tế bào thần kinh đặc biệt trong lớp tiếp theo

169
00:13:58,060 --> 00:14:03,599
Điều đó có nghĩa là lấy tổng trọng số của kích hoạt trong lớp đầu tiên theo các trọng?

170
00:14:04,000 --> 00:14:09,330
Tương ứng với một trong các điều khoản trong sản phẩm vector ma trận của tất cả mọi thứ chúng tôi có ở bên trái ở đây

171
00:14:13,540 --> 00:14:18,380
Bằng cách quá nhiều học máy chỉ đi xuống để có một nắm bắt tốt các đại số tuyến tính

172
00:14:18,380 --> 00:14:26,940
Vì vậy, đối với bất kỳ bạn của những người muốn có một sự hiểu biết trực quan tốt đẹp cho các ma trận và những gì nhân vector ma trận có nghĩa là hãy nhìn vào dòng tôi đã làm trên đại số tuyến tính

173
00:14:27,250 --> 00:14:28,839
đặc biệt là chương ba

174
00:14:28,839 --> 00:14:35,759
Về biểu hiện của chúng tôi thay vì nói về thêm thiên vị cho mỗi một trong những giá trị độc lập chúng tôi đại diện nó bằng cách

175
00:14:36,010 --> 00:14:42,209
Tổ chức tất cả những thành kiến ​​vào một vector và thêm toàn bộ vector cho sản phẩm ma trận vector trước

176
00:14:42,910 --> 00:14:44,040
Sau đó, như một bước cuối cùng

177
00:14:44,040 --> 00:14:47,250
Tôi sẽ rap một sigmoid xung quanh bên ngoài ở đây

178
00:14:47,250 --> 00:14:51,899
Và điều đó là nghĩa vụ phải thể hiện là bạn đang đi để áp dụng hàm sigmoid cho mỗi cụ thể

179
00:14:52,420 --> 00:14:54,570
thành phần của vector kết quả bên trong

180
00:14:55,510 --> 00:15:00,749
Vì vậy, một khi bạn viết ra ma trận trọng số này và những vector như là biểu tượng riêng của họ, bạn có thể

181
00:15:01,000 --> 00:15:07,589
giao tiếp quá trình chuyển đổi toàn bộ kích hoạt từ một lớp tiếp theo trong một biểu thức ít cực kỳ chặt chẽ và gọn gàng và

182
00:15:07,930 --> 00:15:15,000
Điều này làm cho các mã có liên quan cả hai rất nhiều đơn giản và nhanh hơn rất nhiều kể từ nhiều thư viện tối ưu hóa các heck ra khỏi nhân ma trận

183
00:15:17,560 --> 00:15:21,359
Ghi như thế nào trước đó tôi đã nói những tế bào thần kinh chỉ đơn giản là những điều mà giữ số

184
00:15:21,790 --> 00:15:26,250
Vâng tất nhiên những con số cụ thể mà họ nắm giữ phụ thuộc vào hình ảnh bạn nuôi trong

185
00:15:27,790 --> 00:15:32,940
Vì vậy, nó thực sự chính xác hơn để suy nghĩ của mỗi tế bào thần kinh như một chức năng mà mất trong các

186
00:15:33,070 --> 00:15:38,070
kết quả đầu ra của tất cả các tế bào thần kinh trong lớp trước và spits ra một số từ zero và một

187
00:15:38,800 --> 00:15:42,270
Thực sự toàn bộ mạng chỉ là một chức năng mà mất trong

188
00:15:42,760 --> 00:15:47,010
784 số như một đầu vào và spits ra mười con số như một đầu ra

189
00:15:47,470 --> 00:15:48,700
Đây là một ngớ ngẩn

190
00:15:48,700 --> 00:15:56,249
chức năng một phức tạp có liên quan đến mười ba ngàn thông số dưới các hình thức của những trọng lượng và những thành kiến ​​mà nhặt trên các mẫu nhất định và trong đó bao gồm

191
00:15:56,250 --> 00:16:00,270
lặp lại nhiều sản phẩm vector ma trận và sigmoid chức năng gọi hồn squish

192
00:16:00,610 --> 00:16:06,390
Nhưng nó chỉ là một chức năng dù sao và trong một cách đó là loại yên tâm rằng nó có vẻ phức tạp

193
00:16:06,390 --> 00:16:12,239
Ý tôi là nếu nó là bất kỳ đơn giản hơn những gì hy vọng chúng ta sẽ phải rằng nó có thể đảm nhận những thách thức của công nhận chữ số?

194
00:16:12,960 --> 00:16:19,559
Và như thế nào đảm nhận thách thức đó? Làm thế nào để mạng này tìm hiểu những trọng lượng và những thành kiến ​​thích hợp chỉ cần nhìn vào dữ liệu? Oh?

195
00:16:20,080 --> 00:16:26,039
Đó là những gì tôi sẽ hiển thị trong video tiếp theo, và tôi cũng sẽ đào thêm một chút vào những gì mạng đặc biệt này, chúng ta đang nhìn thấy được thực sự làm

196
00:16:27,130 --> 00:16:32,640
Bây giờ là thời điểm tôi cho rằng tôi nên nói đăng ký lại thông báo về khi mà video hoặc bất kỳ video mới đi ra

197
00:16:32,760 --> 00:16:37,560
Nhưng thực tế hầu hết các bạn không thực sự nhận được thông báo từ YouTube, phải không?

198
00:16:37,560 --> 00:16:42,260
Có lẽ một cách trung thực hơn tôi nên nói đăng ký để các mạng thần kinh làm nền tảng của YouTube

199
00:16:42,459 --> 00:16:47,639
thuật toán đề nghị được sơn lót để tin rằng bạn muốn xem nội dung từ kênh này được khuyến khích để bạn

200
00:16:48,250 --> 00:16:50,250
nào ở lại gửi để biết thêm

201
00:16:50,410 --> 00:16:53,550
Cảm ơn bạn rất nhiều để mọi người hỗ trợ các video trên patreon

202
00:16:53,589 --> 00:16:56,759
Tôi đã là một chút chậm để tiến bộ trong loạt khả năng mùa hè này

203
00:16:56,760 --> 00:17:01,379
Nhưng tôi nhảy trở lại vào nó sau khi dự án này, do đó khách hàng quen, bạn có thể tìm cho ra bản cập nhật có

204
00:17:03,310 --> 00:17:05,550
Để đóng vật tắt ở đây tôi có với tôi Lisha Li

205
00:17:05,550 --> 00:17:12,029
Lee người đã làm công việc tiến sĩ của mình về mặt lý thuyết của học sâu và những người hiện đang làm việc tại một công ty đầu tư mạo hiểm gọi là khuếch đại đối tác

206
00:17:12,030 --> 00:17:16,530
Ai vui lòng cung cấp một số các nguồn tài trợ cho video này để Lisha một điều

207
00:17:16,530 --> 00:17:19,109
Tôi nghĩ chúng ta nên nhanh chóng đưa lên là hàm sigmoid này

208
00:17:19,180 --> 00:17:24,780
Theo tôi được biết mạng đầu sử dụng này để squish các tổng trọng có liên quan vào khoảng thời gian đó giữa zero và một

209
00:17:24,980 --> 00:17:30,340
Bạn biết loại động cơ bằng cách tương tự sinh học này của tế bào thần kinh hoặc là không hoạt động hoặc hoạt động
(Lisha) - Chính xác

210
00:17:30,360 --> 00:17:36,320
(3B1B) - Nhưng tương đối ít mạng hiện đại thực sự sử dụng sigmoid nữa. Đó là loại ngay trường cũ?
(Lisha) - Ừ hay đúng hơn

211
00:17:36,370 --> 00:17:42,780
ReLU có vẻ là dễ dàng hơn nhiều để đào tạo
(3B1B) - Và ReLU thực sự đại diện cho đơn vị tuyến tính sửa chữa

212
00:17:42,780 --> 00:17:48,839
(Lisha) - Có nó là loại chức năng mà bạn chỉ cần tham gia một tối đa của 0 và một nơi được cho bởi

213
00:17:49,120 --> 00:17:53,670
những gì bạn đã giải thích trong đoạn video và điều này là loại động cơ từ tôi nghĩ là một

214
00:17:54,610 --> 00:17:56,610
một phần bởi một sinh học

215
00:17:56,620 --> 00:17:58,179
Tương tự với cách

216
00:17:58,179 --> 00:18:03,089
Tế bào thần kinh hoặc là sẽ được kích hoạt hay không và vì vậy nếu nó vượt qua một ngưỡng nhất định

217
00:18:03,250 --> 00:18:05,250
Nó sẽ là chức năng nhận diện

218
00:18:05,290 --> 00:18:10,439
Nhưng nếu nó không thì nó sẽ chỉ không được kích hoạt vì vậy hãy zero vì vậy nó loại đơn giản hóa

219
00:18:10,720 --> 00:18:14,429
Sử dụng sigmoids không giúp đào tạo, hoặc nó đã rất khó khăn để đào tạo

220
00:18:14,429 --> 00:18:19,589
Đó là tại một số điểm và mọi người chỉ cố gắng relu và nó đã xảy ra để làm việc

221
00:18:20,110 --> 00:18:22,140
Rất tốt cho những khó tin

222
00:18:22,690 --> 00:18:25,090
mạng thần kinh sâu.
(3B1B) - Được rồi

223
00:18:25,090 --> 00:18:26,060
Cảm ơn Lisha

