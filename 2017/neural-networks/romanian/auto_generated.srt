1
00:00:04,220 --> 00:00:05,400
Acesta este un 3.

2
00:00:06,060 --> 00:00:10,118
Este scrisă neglijent și redată la o rezoluție extrem de mică, de 28x28 pixeli, 

3
00:00:10,118 --> 00:00:13,720
dar creierul tău nu are nicio problemă în a o recunoaște ca fiind un 3.

4
00:00:14,340 --> 00:00:16,763
Și vreau să vă luați un moment pentru a aprecia cât de nebunesc 

5
00:00:16,763 --> 00:00:18,960
este faptul că creierul poate face acest lucru fără efort.

6
00:00:19,700 --> 00:00:23,636
Adică, aceasta, aceasta și aceasta sunt, de asemenea, recunoscute ca fiind 3s, 

7
00:00:23,636 --> 00:00:27,921
chiar dacă valorile specifice ale fiecărui pixel sunt foarte diferite de la o imagine 

8
00:00:27,921 --> 00:00:28,320
la alta.

9
00:00:28,900 --> 00:00:32,793
Celulele sensibile la lumină din ochiul dumneavoastră care se activează când 

10
00:00:32,793 --> 00:00:36,940
vedeți acest 3 sunt foarte diferite de cele care se activează când vedeți acest 3.

11
00:00:37,520 --> 00:00:41,296
Dar ceva în cortexul tău vizual nebunește deștept și inteligent 

12
00:00:41,296 --> 00:00:44,424
rezolvă aceste imagini ca reprezentând aceeași idee, 

13
00:00:44,424 --> 00:00:48,260
recunoscând în același timp alte imagini ca fiind idei distincte.

14
00:00:49,220 --> 00:00:54,695
Dar dacă ți-aș spune: "Hei, stai jos și scrie-mi un program care să ia o grilă de 

15
00:00:54,695 --> 00:00:59,235
28x28 pixeli ca aceasta și să scoată un singur număr între 0 și 10, 

16
00:00:59,235 --> 00:01:04,911
spunându-ți ce crede că este cifra", ei bine, sarcina devine de la comic trivială la 

17
00:01:04,911 --> 00:01:06,180
extrem de dificilă.

18
00:01:07,160 --> 00:01:10,877
Dacă nu cumva ați trăit sub o stâncă, cred că nu este nevoie să motivez relevanța 

19
00:01:10,877 --> 00:01:14,640
și importanța învățării automate și a rețelelor neuronale pentru prezent și viitor.

20
00:01:15,120 --> 00:01:18,512
Dar ceea ce vreau să fac aici este să vă arăt ce este de fapt o rețea neuronală, 

21
00:01:18,512 --> 00:01:22,198
presupunând că nu aveți niciun fel de cunoștințe, și să vă ajut să vizualizați ce face, 

22
00:01:22,198 --> 00:01:24,460
nu ca un cuvânt la modă, ci ca o bucată de matematică.

23
00:01:25,020 --> 00:01:29,627
Speranța mea este ca voi să aveți sentimentul că structura în sine este motivată și să 

24
00:01:29,627 --> 00:01:34,340
simțiți că știți ce înseamnă când citiți sau auziți despre o rețea neuronală de învățare.

25
00:01:35,360 --> 00:01:38,506
Acest videoclip va fi dedicat doar componentei de structură, 

26
00:01:38,506 --> 00:01:40,260
iar următorul va aborda învățarea.

27
00:01:40,960 --> 00:01:43,597
Ceea ce vom face este să creăm o rețea neuronală care 

28
00:01:43,597 --> 00:01:46,040
poate învăța să recunoască cifrele scrise de mână.

29
00:01:49,360 --> 00:01:52,736
Acesta este un exemplu oarecum clasic de introducere a subiectului și mă bucur 

30
00:01:52,736 --> 00:01:56,241
să rămân la status quo aici, pentru că la sfârșitul celor două videoclipuri vreau 

31
00:01:56,241 --> 00:01:59,660
să vă indic câteva resurse bune de unde puteți afla mai multe și de unde puteți 

32
00:01:59,660 --> 00:02:03,080
descărca codul care face acest lucru și să vă jucați cu el pe propriul computer.

33
00:02:05,040 --> 00:02:07,566
Există multe variante ale rețelelor neuronale, 

34
00:02:07,566 --> 00:02:11,975
iar în ultimii ani a avut loc o adevărată explozie a cercetării acestor variante, 

35
00:02:11,975 --> 00:02:16,760
dar în aceste două videoclipuri introductive, noi doi ne vom uita doar la cea mai simplă 

36
00:02:16,760 --> 00:02:19,180
formă, cea mai simplă, fără alte complicații.

37
00:02:19,860 --> 00:02:24,008
Aceasta este o condiție necesară pentru a înțelege oricare dintre cele mai 

38
00:02:24,008 --> 00:02:28,600
puternice variante moderne și, credeți-mă, este încă destul de complexă pentru noi.

39
00:02:29,120 --> 00:02:32,866
Dar chiar și în această formă cea mai simplă poate învăța să recunoască cifrele 

40
00:02:32,866 --> 00:02:36,520
scrise de mână, ceea ce este un lucru destul de interesant pentru un computer.

41
00:02:37,480 --> 00:02:39,880
Și, în același timp, veți vedea cum nu se ridică la nivelul 

42
00:02:39,880 --> 00:02:42,280
câtorva speranțe pe care le-am putea avea în legătură cu el.

43
00:02:43,380 --> 00:02:47,108
După cum sugerează și numele, rețelele neuronale sunt inspirate de creier, 

44
00:02:47,108 --> 00:02:48,500
dar să analizăm acest lucru.

45
00:02:48,520 --> 00:02:51,660
Care sunt neuronii și în ce sens sunt legați între ei?

46
00:02:52,500 --> 00:02:56,406
În momentul de față, când spun neuron, vreau să vă gândiți la 

47
00:02:56,406 --> 00:03:00,440
un lucru care conține un număr, mai exact un număr între 0 și 1.

48
00:03:00,680 --> 00:03:02,560
Nu este mai mult de atât.

49
00:03:03,780 --> 00:03:08,967
De exemplu, rețeaua începe cu un grup de neuroni care corespund fiecăruia dintre 

50
00:03:08,967 --> 00:03:14,220
cei 28x28 pixeli ai imaginii de intrare, ceea ce înseamnă 784 de neuroni în total.

51
00:03:14,700 --> 00:03:19,232
Fiecare dintre acestea conține un număr care reprezintă valoarea scalei de gri a 

52
00:03:19,232 --> 00:03:24,100
pixelului corespunzător, variind de la 0 pentru pixelii negri până la 1 pentru pixelii 

53
00:03:24,100 --> 00:03:24,380
albi.

54
00:03:25,300 --> 00:03:28,095
Acest număr din interiorul neuronului se numește activare, 

55
00:03:28,095 --> 00:03:31,175
iar imaginea pe care o puteți avea în minte aici este că fiecare 

56
00:03:31,175 --> 00:03:34,160
neuron este aprins atunci când activarea sa este un număr mare.

57
00:03:36,720 --> 00:03:41,860
Așadar, toți acești 784 de neuroni alcătuiesc primul strat al rețelei noastre.

58
00:03:46,500 --> 00:03:49,374
Acum, trecând la ultimul strat, acesta are 10 neuroni, 

59
00:03:49,374 --> 00:03:51,360
fiecare reprezentând una dintre cifre.

60
00:03:52,040 --> 00:03:56,332
Activarea acestor neuroni, din nou un număr cuprins între 0 și 1, 

61
00:03:56,332 --> 00:04:02,120
reprezintă cât de mult crede sistemul că o anumită imagine corespunde cu o anumită cifră.

62
00:04:03,040 --> 00:04:06,529
Există, de asemenea, câteva straturi intermediare, numite straturi ascunse, 

63
00:04:06,529 --> 00:04:09,881
care, pentru moment, ar trebui să fie doar un uriaș semn de întrebare cu 

64
00:04:09,881 --> 00:04:13,600
privire la modul în care va fi gestionat acest proces de recunoaștere a cifrelor.

65
00:04:14,260 --> 00:04:16,698
În această rețea am ales două straturi ascunse, 

66
00:04:16,698 --> 00:04:20,560
fiecare cu câte 16 neuroni, și recunosc că este o alegere oarecum arbitrară.

67
00:04:21,019 --> 00:04:24,269
Ca să fiu sincer, am ales două straturi pe baza modului în care vreau să motivez 

68
00:04:24,269 --> 00:04:27,838
structura în câteva momente, iar 16, ei bine, a fost doar un număr frumos care să încapă 

69
00:04:27,838 --> 00:04:28,200
pe ecran.

70
00:04:28,780 --> 00:04:32,340
În practică, există mult spațiu pentru a experimenta o structură specifică.

71
00:04:33,020 --> 00:04:35,595
În modul în care funcționează rețeaua, activările 

72
00:04:35,595 --> 00:04:38,480
dintr-un strat determină activările din stratul următor.

73
00:04:39,200 --> 00:04:43,726
Și, bineînțeles, inima rețelei ca mecanism de procesare a informației se reduce la 

74
00:04:43,726 --> 00:04:48,580
modul exact în care aceste activări dintr-un strat determină activări în stratul următor.

75
00:04:49,140 --> 00:04:53,209
Se dorește a fi oarecum analog cu modul în care, în rețelele biologice de neuroni, 

76
00:04:53,209 --> 00:04:57,180
unele grupuri de neuroni care se activează determină alte grupuri să se activeze.

77
00:04:58,120 --> 00:05:00,586
Rețeaua pe care v-o prezint aici a fost deja antrenată pentru a 

78
00:05:00,586 --> 00:05:03,400
recunoaște cifrele și permiteți-mi să vă arăt ce vreau să spun prin asta.

79
00:05:03,640 --> 00:05:06,259
Aceasta înseamnă că, dacă introduceți o imagine, 

80
00:05:06,259 --> 00:05:10,802
aprinzând toți cei 784 de neuroni din stratul de intrare în funcție de luminozitatea 

81
00:05:10,802 --> 00:05:15,452
fiecărui pixel din imagine, acest model de activări determină un model foarte specific 

82
00:05:15,452 --> 00:05:19,942
în stratul următor, care determină un model în cel următor, care, în cele din urmă, 

83
00:05:19,942 --> 00:05:22,080
determină un model în stratul de ieșire.

84
00:05:22,560 --> 00:05:26,048
Iar cel mai strălucitor neuron din stratul de ieșire este alegerea rețelei, 

85
00:05:26,048 --> 00:05:29,400
ca să spunem așa, pentru a determina ce cifră reprezintă această imagine.

86
00:05:32,560 --> 00:05:35,280
Și înainte de a intra în calculele matematice privind modul în care 

87
00:05:35,280 --> 00:05:38,240
un strat îl influențează pe următorul sau cum funcționează antrenamentul, 

88
00:05:38,240 --> 00:05:40,919
să vorbim despre motivul pentru care este rezonabil să ne așteptăm 

89
00:05:40,919 --> 00:05:43,520
ca o structură stratificată ca aceasta să se comporte inteligent.

90
00:05:44,060 --> 00:05:45,220
La ce ne așteptăm aici?

91
00:05:45,400 --> 00:05:47,600
Care este cea mai bună speranță pentru aceste straturi de mijloc?

92
00:05:48,920 --> 00:05:53,520
Ei bine, atunci când noi doi recunoaștem cifrele, punem cap la cap diverse componente.

93
00:05:54,200 --> 00:05:56,820
Un 9 are o buclă în partea de sus și o linie în dreapta.

94
00:05:57,380 --> 00:05:59,260
Un 8 are, de asemenea, o buclă în partea de sus, 

95
00:05:59,260 --> 00:06:01,180
dar este asociat cu o altă buclă în partea de jos.

96
00:06:01,980 --> 00:06:06,820
Un 4 se împarte practic în trei linii specifice și lucruri de genul acesta.

97
00:06:07,600 --> 00:06:11,712
Acum, într-o lume perfectă, am putea spera că fiecare neuron din penultimul 

98
00:06:11,712 --> 00:06:14,526
strat corespunde uneia dintre aceste subcomponente, 

99
00:06:14,526 --> 00:06:19,180
că de fiecare dată când introduceți o imagine cu, să zicem, o buclă în partea de sus, 

100
00:06:19,180 --> 00:06:23,780
cum ar fi un 9 sau un 8, există un anumit neuron a cărui activare va fi aproape de 1.

101
00:06:24,500 --> 00:06:27,116
Și nu mă refer la această buclă specifică de pixeli, 

102
00:06:27,116 --> 00:06:31,560
ci sperăm că orice model general de buclă spre partea superioară declanșează acest neuron.

103
00:06:32,440 --> 00:06:35,986
În acest fel, pentru a trece de la al treilea strat la ultimul strat, 

104
00:06:35,986 --> 00:06:40,040
trebuie doar să înveți ce combinație de subcomponente corespunde fiecărei cifre.

105
00:06:41,000 --> 00:06:43,608
Bineînțeles, acest lucru nu face decât să ridice problema în josul drumului, 

106
00:06:43,608 --> 00:06:45,336
deoarece cum veți recunoaște aceste subcomponente, 

107
00:06:45,336 --> 00:06:47,640
sau chiar să învățați care ar trebui să fie subcomponentele corecte?

108
00:06:48,060 --> 00:06:51,352
Și încă nu am vorbit despre modul în care un strat îl influențează pe următorul, 

109
00:06:51,352 --> 00:06:53,060
dar fiți alături de mine pentru un moment.

110
00:06:53,680 --> 00:06:56,680
Recunoașterea unei bucle poate fi, de asemenea, împărțită în subprobleme.

111
00:06:57,280 --> 00:06:59,910
O modalitate rezonabilă de a face acest lucru ar fi să 

112
00:06:59,910 --> 00:07:02,780
recunoaștem mai întâi diferitele margini mici care o compun.

113
00:07:03,780 --> 00:07:08,037
În mod similar, o linie lungă, cum ar fi cea pe care o puteți vedea în cifrele 1, 

114
00:07:08,037 --> 00:07:11,516
4 sau 7, este de fapt doar o muchie lungă, sau poate vă gândiți la 

115
00:07:11,516 --> 00:07:14,320
ea ca la un anumit model de mai multe muchii mai mici.

116
00:07:15,140 --> 00:07:19,054
Poate că speranța noastră este că fiecare neuron din al doilea 

117
00:07:19,054 --> 00:07:22,720
strat al rețelei corespunde cu diferitele muchii relevante.

118
00:07:23,540 --> 00:07:26,492
Poate că, atunci când o imagine ca aceasta intră, 

119
00:07:26,492 --> 00:07:31,511
se aprind toți neuronii asociați cu aproximativ 8 până la 10 margini mici specifice, 

120
00:07:31,511 --> 00:07:35,586
care, la rândul lor, aprind neuronii asociați cu bucla superioară și 

121
00:07:35,586 --> 00:07:39,720
o linie verticală lungă, iar aceștia aprind neuronii asociați cu un 9.

122
00:07:40,680 --> 00:07:44,648
Dacă acest lucru este sau nu ceea ce face rețeaua noastră finală este o altă întrebare, 

123
00:07:44,648 --> 00:07:47,444
la care voi reveni după ce vom vedea cum să antrenăm rețeaua, 

124
00:07:47,444 --> 00:07:49,969
dar aceasta este o speranță pe care am putea să o avem, 

125
00:07:49,969 --> 00:07:52,540
un fel de obiectiv cu o astfel de structură stratificată.

126
00:07:53,160 --> 00:07:56,527
În plus, vă puteți imagina cum ar fi foarte utilă capacitatea de a detecta 

127
00:07:56,527 --> 00:08:00,300
marginile și modelele în acest fel pentru alte sarcini de recunoaștere a imaginilor.

128
00:08:00,880 --> 00:08:02,671
Și chiar și dincolo de recunoașterea imaginilor, 

129
00:08:02,671 --> 00:08:05,853
există tot felul de lucruri inteligente pe care ați putea dori să le faceți și care se 

130
00:08:05,853 --> 00:08:07,280
descompun în straturi de abstractizare.

131
00:08:08,040 --> 00:08:11,793
Analiza vorbirii, de exemplu, implică luarea de date audio brute și selectarea 

132
00:08:11,793 --> 00:08:15,023
sunetelor distincte, care se combină pentru a forma anumite silabe, 

133
00:08:15,023 --> 00:08:18,824
care se combină pentru a forma cuvinte, care se combină pentru a forma fraze și 

134
00:08:18,824 --> 00:08:20,060
gânduri mai abstracte etc.

135
00:08:21,100 --> 00:08:24,888
Dar, revenind la modul în care funcționează de fapt toate acestea, 

136
00:08:24,888 --> 00:08:29,920
imaginați-vă chiar acum cum anume activările dintr-un strat ar putea determina următorul.

137
00:08:30,860 --> 00:08:36,065
Scopul este de a avea un mecanism care ar putea combina pixelii în muchii, 

138
00:08:36,065 --> 00:08:38,980
sau muchii în modele, sau modele în cifre.

139
00:08:39,440 --> 00:08:42,923
Și pentru a ne concentra asupra unui exemplu foarte specific, 

140
00:08:42,923 --> 00:08:46,574
să spunem că se speră ca un anumit neuron din al doilea strat să 

141
00:08:46,574 --> 00:08:50,620
detecteze dacă imaginea are sau nu o margine în această regiune de aici.

142
00:08:51,440 --> 00:08:55,100
Întrebarea care se pune este ce parametri ar trebui să aibă rețeaua?

143
00:08:55,640 --> 00:08:59,721
Ce cadrane și butoane ar trebui să puteți ajusta astfel încât să fie suficient 

144
00:08:59,721 --> 00:09:03,388
de expresiv pentru a capta acest model, sau orice alt model de pixeli, 

145
00:09:03,388 --> 00:09:07,780
sau modelul în care mai multe muchii pot forma o buclă, și alte lucruri de acest gen?

146
00:09:08,720 --> 00:09:12,293
Ei bine, ceea ce vom face este să atribuim o pondere fiecăreia dintre 

147
00:09:12,293 --> 00:09:15,560
conexiunile dintre neuronul nostru și neuronii din primul strat.

148
00:09:16,320 --> 00:09:17,700
Aceste greutăți sunt doar numere.

149
00:09:18,540 --> 00:09:21,960
Apoi se iau toate aceste activări din primul strat și se 

150
00:09:21,960 --> 00:09:25,500
calculează suma lor ponderată în funcție de aceste ponderi.

151
00:09:27,700 --> 00:09:31,291
Mi se pare util să mă gândesc la aceste ponderi ca fiind organizate într-o 

152
00:09:31,291 --> 00:09:34,883
mică grilă proprie, iar eu voi folosi pixeli verzi pentru a indica ponderi 

153
00:09:34,883 --> 00:09:37,709
pozitive și pixeli roșii pentru a indica ponderi negative, 

154
00:09:37,709 --> 00:09:41,780
unde luminozitatea pixelului respectiv este o reprezentare liberă a valorii ponderii.

155
00:09:42,780 --> 00:09:46,948
Acum, dacă am făcut ca ponderile asociate cu aproape toți pixelii să fie zero, 

156
00:09:46,948 --> 00:09:50,854
cu excepția unor ponderi pozitive în această regiune care ne interesează, 

157
00:09:50,854 --> 00:09:54,548
atunci suma ponderată a tuturor valorilor pixelilor se rezumă doar la 

158
00:09:54,548 --> 00:09:57,820
adunarea valorilor pixelilor din regiunea care ne interesează.

159
00:09:59,140 --> 00:10:02,898
Și dacă ați dori cu adevărat să aflați dacă există o margine aici, 

160
00:10:02,898 --> 00:10:06,600
ați putea avea unele ponderi negative asociate cu pixelii din jur.

161
00:10:07,480 --> 00:10:10,884
Apoi, suma este cea mai mare atunci când pixelii din mijloc sunt luminoși, 

162
00:10:10,884 --> 00:10:12,700
dar pixelii din jur sunt mai întunecați.

163
00:10:14,260 --> 00:10:18,960
Atunci când calculați o sumă ponderată ca aceasta, puteți obține orice număr, 

164
00:10:18,960 --> 00:10:23,540
dar pentru această rețea dorim ca activările să aibă o valoare între 0 și 1.

165
00:10:24,120 --> 00:10:27,962
Deci, un lucru obișnuit este să pompăm această sumă ponderată într-o 

166
00:10:27,962 --> 00:10:32,140
funcție care să comprime linia numerelor reale în intervalul dintre 0 și 1.

167
00:10:32,460 --> 00:10:35,612
Iar o funcție obișnuită care face acest lucru se numește funcția sigmoidă, 

168
00:10:35,612 --> 00:10:37,420
cunoscută și sub numele de curbă logistică.

169
00:10:38,000 --> 00:10:41,583
Practic, intrările foarte negative ajung aproape de 0, 

170
00:10:41,583 --> 00:10:46,600
intrările pozitive ajung aproape de 1 și crește constant în jurul intrării 0.

171
00:10:49,120 --> 00:10:52,638
Astfel, activarea neuronului de aici este practic o 

172
00:10:52,638 --> 00:10:56,360
măsură a cât de pozitivă este suma ponderată relevantă.

173
00:10:57,540 --> 00:10:59,745
Dar poate că nu este vorba de faptul că doriți ca neuronii să 

174
00:10:59,745 --> 00:11:01,880
se aprindă atunci când suma ponderată este mai mare decât 0.

175
00:11:02,280 --> 00:11:05,877
Poate că doriți ca acesta să fie activ doar atunci când suma este mai mare de 10, 

176
00:11:05,877 --> 00:11:06,360
de exemplu.

177
00:11:06,840 --> 00:11:10,260
Adică, doriți o anumită prejudecată pentru ca acesta să fie inactiv.

178
00:11:11,380 --> 00:11:15,315
Ceea ce vom face atunci este să adăugăm un alt număr, cum ar fi un minus 10, 

179
00:11:15,315 --> 00:11:19,660
la această sumă ponderată înainte de a o introduce în funcția sigmoidală de strivire.

180
00:11:20,580 --> 00:11:22,440
Acest număr suplimentar se numește bias.

181
00:11:23,460 --> 00:11:28,105
Deci, ponderile vă spun ce model de pixeli detectează acest neuron din al doilea strat, 

182
00:11:28,105 --> 00:11:32,065
iar polarizarea vă spune cât de mare trebuie să fie suma ponderată înainte 

183
00:11:32,065 --> 00:11:35,180
ca neuronul să înceapă să devină activ în mod semnificativ.

184
00:11:36,120 --> 00:11:37,680
Și acesta este doar un singur neuron.

185
00:11:38,280 --> 00:11:42,336
Fiecare alt neuron din acest strat va fi conectat la toți 

186
00:11:42,336 --> 00:11:45,624
cei 784 de neuroni de pixeli din primul strat, 

187
00:11:45,624 --> 00:11:50,940
iar fiecare dintre aceste 784 de conexiuni are propria sa greutate asociată.

188
00:11:51,600 --> 00:11:53,828
De asemenea, fiecare dintre ele are un anumit bias, 

189
00:11:53,828 --> 00:11:57,600
un alt număr pe care îl adăugați la suma ponderată înainte de a o comprima cu sigmoidul.

190
00:11:58,110 --> 00:11:59,540
Și sunt multe lucruri la care trebuie să te gândești!

191
00:11:59,960 --> 00:12:05,955
Cu acest strat ascuns de 16 neuroni, rezultă un total de 784 ori 16 ponderi, 

192
00:12:05,955 --> 00:12:07,980
împreună cu 16 polarizări.

193
00:12:08,840 --> 00:12:11,940
Și toate acestea reprezintă doar conexiunile de la primul strat la cel de-al doilea.

194
00:12:12,520 --> 00:12:15,044
Conexiunile dintre celelalte straturi au, de asemenea, 

195
00:12:15,044 --> 00:12:17,340
o serie de ponderi și prejudecăți asociate cu ele.

196
00:12:18,340 --> 00:12:23,800
În concluzie, această rețea are aproape exact 13 000 de ponderi și prejudecăți totale.

197
00:12:23,800 --> 00:12:26,756
13.000 de butoane și cadrane care pot fi ajustate și rotite 

198
00:12:26,756 --> 00:12:29,960
pentru a face ca această rețea să se comporte în moduri diferite.

199
00:12:31,040 --> 00:12:34,463
Așadar, atunci când vorbim despre învățare, ne referim la faptul că 

200
00:12:34,463 --> 00:12:37,886
trebuie să determinăm computerul să găsească o setare validă pentru 

201
00:12:37,886 --> 00:12:41,360
toate aceste multe numere, astfel încât să rezolve problema în cauză.

202
00:12:42,620 --> 00:12:46,086
Un experiment de gândire care este în același timp amuzant și înfricoșător 

203
00:12:46,086 --> 00:12:50,108
este să ne imaginăm că stăm jos și stabilim manual toate aceste ponderi și polarizări, 

204
00:12:50,108 --> 00:12:53,760
modificând intenționat numerele astfel încât al doilea strat să se concentreze 

205
00:12:53,760 --> 00:12:56,580
pe margini, al treilea strat să se concentreze pe modele etc.

206
00:12:56,980 --> 00:13:00,448
Personal, consider că acest lucru este mai degrabă satisfăcător decât să 

207
00:13:00,448 --> 00:13:02,681
tratez rețeaua ca pe o cutie neagră, deoarece, 

208
00:13:02,681 --> 00:13:05,484
atunci când rețeaua nu funcționează așa cum ați anticipat, 

209
00:13:05,484 --> 00:13:09,571
dacă ați construit o relație cu ceea ce înseamnă de fapt acele ponderi și polarizări, 

210
00:13:09,571 --> 00:13:13,134
aveți un punct de plecare pentru a experimenta cum să modificați structura 

211
00:13:13,134 --> 00:13:14,180
pentru a o îmbunătăți.

212
00:13:14,960 --> 00:13:18,438
Sau, atunci când rețeaua funcționează, dar nu din motivele la care v-ați aștepta, 

213
00:13:18,438 --> 00:13:22,086
săpătura în ceea ce fac ponderile și polarizările este o modalitate bună de a vă pune 

214
00:13:22,086 --> 00:13:25,820
la îndoială ipotezele și de a expune cu adevărat întregul spațiu al soluțiilor posibile.

215
00:13:26,840 --> 00:13:30,680
Apropo, funcția reală de aici este un pic cam greoaie de scris, nu credeți?

216
00:13:32,500 --> 00:13:34,623
Așadar, permiteți-mi să vă arăt un mod mai compact de 

217
00:13:34,623 --> 00:13:37,140
reprezentare a acestor conexiuni din punct de vedere notațional.

218
00:13:37,660 --> 00:13:40,520
Iată cum o veți vedea dacă veți alege să citiți mai multe despre rețelele neuronale.

219
00:13:41,380 --> 00:13:47,586
Organizați toate activările dintr-un strat într-o coloană, 

220
00:13:47,586 --> 00:13:55,896
deoarece o matrice corespunde conexiunilor dintre un strat și un anumit neuron 

221
00:13:55,896 --> 00:13:58,000
din stratul următor.

222
00:13:58,540 --> 00:14:02,378
Aceasta înseamnă că suma ponderată a activărilor din primul strat 

223
00:14:02,378 --> 00:14:06,216
în funcție de aceste ponderi corespunde unuia dintre termenii din 

224
00:14:06,216 --> 00:14:09,880
produsul vectorial matricial a tot ceea ce avem aici în stânga.

225
00:14:14,000 --> 00:14:17,614
Apropo, o mare parte din învățarea automată se reduce la o bună înțelegere a 

226
00:14:17,614 --> 00:14:20,994
algebrei liniare, așa că, pentru cei care doresc o înțelegere vizuală a 

227
00:14:20,994 --> 00:14:24,046
matricelor și a ceea ce înseamnă înmulțirea matricei cu vectori, 

228
00:14:24,046 --> 00:14:27,379
aruncați o privire la seria pe care am făcut-o despre algebra liniară, 

229
00:14:27,379 --> 00:14:28,600
în special la capitolul 3.

230
00:14:29,240 --> 00:14:32,279
Revenind la expresia noastră, în loc să vorbim despre adăugarea 

231
00:14:32,279 --> 00:14:35,366
prejudecății la fiecare dintre aceste valori în mod independent, 

232
00:14:35,366 --> 00:14:38,548
o reprezentăm prin organizarea tuturor acestor prejudecăți într-un 

233
00:14:38,548 --> 00:14:42,300
vector și adăugarea întregului vector la produsul vectorial matricial anterior.

234
00:14:43,280 --> 00:14:47,457
Apoi, ca un pas final, voi înfășura un sigmoid în jurul exteriorului de aici, 

235
00:14:47,457 --> 00:14:51,205
ceea ce ar trebui să reprezinte faptul că veți aplica funcția sigmoid 

236
00:14:51,205 --> 00:14:54,740
la fiecare componentă specifică a vectorului rezultat în interior.

237
00:14:55,940 --> 00:15:00,609
Astfel, odată ce scrieți această matrice de ponderare și acești vectori ca simboluri 

238
00:15:00,609 --> 00:15:05,497
proprii, puteți comunica tranziția completă a activărilor de la un strat la altul într-o 

239
00:15:05,497 --> 00:15:10,276
expresie extrem de strânsă și de clară, ceea ce face ca codul relevant să fie mult mai 

240
00:15:10,276 --> 00:15:15,055
simplu și mult mai rapid, deoarece multe biblioteci optimizează foarte mult înmulțirea 

241
00:15:15,055 --> 00:15:15,660
matricelor.

242
00:15:17,820 --> 00:15:19,511
Vă amintiți cum am spus mai devreme că acești 

243
00:15:19,511 --> 00:15:21,460
neuroni sunt pur și simplu lucruri care rețin numere?

244
00:15:22,220 --> 00:15:27,112
Bineînțeles, numerele specifice pe care le dețin depind de imaginea pe care o 

245
00:15:27,112 --> 00:15:32,381
introduceți, așa că este mai corect să ne gândim la fiecare neuron ca la o funcție, 

246
00:15:32,381 --> 00:15:37,524
una care preia ieșirile tuturor neuronilor din stratul anterior și emite un număr 

247
00:15:37,524 --> 00:15:38,340
între 0 și 1.

248
00:15:39,200 --> 00:15:43,303
De fapt, întreaga rețea este doar o funcție, care primește 

249
00:15:43,303 --> 00:15:47,060
784 de numere ca intrare și emite 10 numere ca ieșire.

250
00:15:47,560 --> 00:15:52,446
Este o funcție absurd de complicată, care implică 13.000 de parametri sub forma acestor 

251
00:15:52,446 --> 00:15:57,221
ponderi și prejudecăți care se bazează pe anumite tipare și care implică iterația mai 

252
00:15:57,221 --> 00:16:01,662
multor produse vectoriale matriciale și a funcției sigmoidale de squishificare, 

253
00:16:01,662 --> 00:16:06,660
dar este totuși doar o funcție și, într-un fel, este liniștitor faptul că pare complicată.

254
00:16:07,340 --> 00:16:09,875
Adică, dacă ar fi fost mai simplu, ce speranță am fi avut 

255
00:16:09,875 --> 00:16:12,280
că ar putea accepta provocarea de a recunoaște cifrele?

256
00:16:13,340 --> 00:16:14,700
Și cum își asumă această provocare?

257
00:16:15,080 --> 00:16:17,351
Cum învață această rețea ponderile și prejudecățile 

258
00:16:17,351 --> 00:16:19,360
adecvate doar prin simpla observare a datelor?

259
00:16:20,140 --> 00:16:22,793
Ei bine, asta vă voi arăta în următorul videoclip și, de asemenea, 

260
00:16:22,793 --> 00:16:26,120
voi cerceta puțin mai mult despre ce face cu adevărat această rețea pe care o vedem.

261
00:16:27,580 --> 00:16:30,801
Acum este momentul în care ar trebui să vă spun să vă abonați pentru a fi 

262
00:16:30,801 --> 00:16:34,328
anunțați când apar videoclipuri sau orice videoclipuri noi, dar, în mod realist, 

263
00:16:34,328 --> 00:16:37,420
cei mai mulți dintre voi nu primesc notificări de la YouTube, nu-i așa?

264
00:16:38,020 --> 00:16:40,375
Poate că, mai sincer, ar trebui să spun "abonați-vă", 

265
00:16:40,375 --> 00:16:43,778
astfel încât rețelele neuronale care stau la baza algoritmului de recomandare 

266
00:16:43,778 --> 00:16:47,094
al YouTube să fie pregătite să creadă că doriți să vi se recomande conținut 

267
00:16:47,094 --> 00:16:47,880
de pe acest canal.

268
00:16:48,560 --> 00:16:49,940
Oricum, rămâneți la curent pentru mai multe.

269
00:16:50,760 --> 00:16:53,500
Mulțumesc foarte mult tuturor celor care susțin aceste videoclipuri pe Patreon.

270
00:16:54,000 --> 00:16:57,790
Am fost puțin cam lent în această vară în ceea ce privește seria de probabilități, 

271
00:16:57,790 --> 00:17:01,900
dar mă apuc din nou de treabă după acest proiect, așa că puteți aștepta actualizări acolo.

272
00:17:03,600 --> 00:17:05,811
Pentru a încheia aici, o am alături de mine pe Leisha Lee, 

273
00:17:05,811 --> 00:17:08,472
care și-a făcut doctoratul pe partea teoretică a învățării profunde și 

274
00:17:08,472 --> 00:17:11,471
care în prezent lucrează la o firmă de capital de risc numită Amplify Partners, 

275
00:17:11,471 --> 00:17:14,619
care a avut amabilitatea de a furniza o parte din finanțarea pentru acest videoclip.

276
00:17:15,460 --> 00:17:17,272
Așadar, Leisha, un lucru pe care cred că ar trebui 

277
00:17:17,272 --> 00:17:19,119
să-l aducem rapid în discuție este funcția sigmoidă.

278
00:17:19,700 --> 00:17:23,079
Din câte am înțeles, rețelele timpurii folosesc acest lucru pentru a comprima 

279
00:17:23,079 --> 00:17:25,636
suma ponderată relevantă în intervalul dintre zero și unu, 

280
00:17:25,636 --> 00:17:29,363
într-un fel motivat de această analogie biologică în care neuronii sunt fie inactivi, 

281
00:17:29,363 --> 00:17:29,840
fie activi.

282
00:17:30,280 --> 00:17:30,300
Exact.

283
00:17:30,560 --> 00:17:34,040
Dar relativ puține rețele moderne mai folosesc de fapt sigmoidul.

284
00:17:34,320 --> 00:17:34,320
Da.

285
00:17:34,440 --> 00:17:35,540
E un fel de școală veche, nu?

286
00:17:35,760 --> 00:17:38,980
Da, sau mai degrabă relu pare a fi mult mai ușor de antrenat.

287
00:17:39,400 --> 00:17:42,340
Și relu înseamnă unitate liniară rectificată?

288
00:17:42,680 --> 00:17:47,574
Da, este un fel de funcție în care se ia doar un maxim de zero și a, 

289
00:17:47,574 --> 00:17:53,674
unde a este dat de ceea ce ați explicat în videoclip, iar acest lucru a fost motivat, 

290
00:17:53,674 --> 00:17:59,916
cred, parțial de o analogie biologică cu modul în care neuronii ar fi activi sau nu și, 

291
00:17:59,916 --> 00:18:05,520
astfel, dacă trece un anumit prag, ar fi o funcție de identitate, dar dacă nu, 

292
00:18:05,520 --> 00:18:10,840
atunci nu ar fi activat, deci ar fi zero, deci este un fel de simplificare.

293
00:18:11,160 --> 00:18:15,607
Folosirea sigmoidelor nu a ajutat la instruire sau a fost foarte dificil de 

294
00:18:15,607 --> 00:18:20,055
instruit la un moment dat, iar oamenii au încercat relu și s-a întâmplat să 

295
00:18:20,055 --> 00:18:24,620
funcționeze foarte bine pentru aceste rețele neuronale incredibil de profunde.

296
00:18:25,100 --> 00:18:25,640
În regulă, mulțumesc Alicia.

