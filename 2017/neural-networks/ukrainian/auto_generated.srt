1
00:00:04,220 --> 00:00:05,400
Це 3.

2
00:00:06,060 --> 00:00:09,863
Він недбало написаний і відрендерений з надзвичайно низькою роздільною 

3
00:00:09,863 --> 00:00:13,720
здатністю 28x28 пікселів, але ваш мозок без проблем розпізнає його як 3.

4
00:00:14,340 --> 00:00:17,087
І я хочу, щоб ви замислилися над тим, наскільки божевільним є те, 

5
00:00:17,087 --> 00:00:18,960
що мозок може робити це без особливих зусиль.

6
00:00:19,700 --> 00:00:23,055
Я маю на увазі, що це, це і це також розпізнаються як 3s, 

7
00:00:23,055 --> 00:00:27,105
хоча конкретні значення кожного пікселя дуже відрізняються від одного 

8
00:00:27,105 --> 00:00:28,320
зображення до іншого.

9
00:00:28,900 --> 00:00:32,056
Конкретні світлочутливі клітини у вашому оці, які спрацьовують, 

10
00:00:32,056 --> 00:00:35,657
коли ви бачите цю цифру 3, дуже відрізняються від тих, які спрацьовують, 

11
00:00:35,657 --> 00:00:36,940
коли ви бачите цю цифру 2.

12
00:00:37,520 --> 00:00:41,431
Але щось у вашій божевільно-розумній зоровій корі вирішує, 

13
00:00:41,431 --> 00:00:46,801
що вони представляють одну й ту ж ідею, і в той же час розпізнає інші зображення 

14
00:00:46,801 --> 00:00:48,260
як власні окремі ідеї.

15
00:00:49,220 --> 00:00:53,585
Але якщо я скажу вам: "Сідайте і напишіть для мене програму, 

16
00:00:53,585 --> 00:00:58,809
яка бере сітку 28x28 пікселів, як ця, і виводить одне число від 0 до 10, 

17
00:00:58,809 --> 00:01:04,820
повідомляючи вам, що це за цифра, то завдання перетворюється з комічно тривіального 

18
00:01:04,820 --> 00:01:06,180
на лякаюче складне.

19
00:01:07,160 --> 00:01:11,056
Якщо ви не жили під скелею, я думаю, що навряд чи потрібно пояснювати вам актуальність 

20
00:01:11,056 --> 00:01:14,640
і важливість машинного навчання та нейронних мереж для сьогодення і майбутнього.

21
00:01:15,120 --> 00:01:19,422
Але я хочу показати вам, що таке нейронна мережа, не маючи ніякого досвіду, 

22
00:01:19,422 --> 00:01:24,460
і допомогти візуалізувати те, що вона робить, не як модне слово, а як частину математики.

23
00:01:25,020 --> 00:01:28,032
Я сподіваюся, що ви відчуєте, що сама структура є вмотивованою, 

24
00:01:28,032 --> 00:01:32,127
і відчуєте, що знаєте, що це означає, коли прочитаєте або почуєте про нейронну мережу, 

25
00:01:32,127 --> 00:01:34,340
яка навчається за принципом "цитати-без-цитат".

26
00:01:35,360 --> 00:01:38,160
Це відео якраз буде присвячене структурному компоненту, 

27
00:01:38,160 --> 00:01:40,260
а наступне відео буде присвячене навчанню.

28
00:01:40,960 --> 00:01:45,046
Ми збираємося створити нейронну мережу, яка навчиться розпізнавати цифри, 

29
00:01:45,046 --> 00:01:46,040
написані від руки.

30
00:01:49,360 --> 00:01:51,863
Це дещо класичний приклад для ознайомлення з темою, 

31
00:01:51,863 --> 00:01:55,233
і я буду радий дотримуватися статус-кво, тому що в кінці двох відео я 

32
00:01:55,233 --> 00:01:58,891
хочу вказати вам на кілька хороших ресурсів, де ви можете дізнатися більше, 

33
00:01:58,891 --> 00:02:03,080
і де ви можете завантажити код, який це робить, і погратися з ним на своєму комп'ютері.

34
00:02:05,040 --> 00:02:07,832
Існує багато-багато варіантів нейронних мереж, 

35
00:02:07,832 --> 00:02:12,466
і в останні роки спостерігається своєрідний бум у дослідженнях цих варіантів, 

36
00:02:12,466 --> 00:02:17,160
але в цих двох вступних відео ми з вами просто розглянемо найпростішу ванільну 

37
00:02:17,160 --> 00:02:19,180
форму без додаткових надмірностей.

38
00:02:19,860 --> 00:02:24,040
Це свого роду необхідна передумова для розуміння будь-якого з більш потужних 

39
00:02:24,040 --> 00:02:28,600
сучасних варіантів, і, повірте мені, це все ще досить складно для нас, щоб осягнути.

40
00:02:29,120 --> 00:02:33,200
Але навіть у цій найпростішій формі він може навчитися розпізнавати цифри, 

41
00:02:33,200 --> 00:02:36,520
написані від руки, що для комп'ютера є досить крутим умінням.

42
00:02:37,480 --> 00:02:40,711
І в той же час ви побачите, як він не виправдовує деяких сподівань, 

43
00:02:40,711 --> 00:02:42,280
які ми могли б на нього покласти.

44
00:02:43,380 --> 00:02:46,357
Як випливає з назви, нейронні мережі надихаються мозком, 

45
00:02:46,357 --> 00:02:48,500
але давайте розберемося з цим детальніше.

46
00:02:48,520 --> 00:02:51,660
Що таке нейрони, і в якому сенсі вони пов'язані між собою?

47
00:02:52,500 --> 00:02:56,609
Зараз, коли я кажу "нейрон", я хочу, щоб ви думали про те, 

48
00:02:56,609 --> 00:03:00,440
що це річ, яка зберігає число, а саме число від 0 до 1.

49
00:03:00,680 --> 00:03:02,560
Насправді це не більше.

50
00:03:03,780 --> 00:03:07,464
Наприклад, мережа починається з групи нейронів, 

51
00:03:07,464 --> 00:03:14,220
що відповідають кожному з 28x28 пікселів вхідного зображення, тобто загалом 784 нейрони.

52
00:03:14,700 --> 00:03:19,540
Кожен з них містить число, яке представляє значення відтінку сірого для 

53
00:03:19,540 --> 00:03:24,380
відповідного пікселя, від 0 для чорних пікселів до 1 для білих пікселів.

54
00:03:25,300 --> 00:03:30,249
Це число всередині нейрона називається його активацією, і ви можете уявити собі, 

55
00:03:30,249 --> 00:03:34,160
що кожен нейрон світиться, коли його активація має велике число.

56
00:03:36,720 --> 00:03:41,860
Отже, всі ці 784 нейрони складають перший рівень нашої мережі.

57
00:03:46,500 --> 00:03:49,467
Тепер переходимо до останнього шару, він має 10 нейронів, 

58
00:03:49,467 --> 00:03:51,360
кожен з яких представляє одну з цифр.

59
00:03:52,040 --> 00:03:57,188
Активація цих нейронів, знову ж таки, деяке число від 0 до 1, показує, 

60
00:03:57,188 --> 00:04:02,120
наскільки система вважає, що дане зображення відповідає даній цифрі.

61
00:04:03,040 --> 00:04:06,622
Між ними є ще кілька шарів, які називаються прихованими, 

62
00:04:06,622 --> 00:04:10,394
і які поки що мають бути просто гігантським знаком питання, 

63
00:04:10,394 --> 00:04:13,600
як саме буде відбуватися процес розпізнавання цифр.

64
00:04:14,260 --> 00:04:18,346
У цій мережі я вибрав два приховані шари, кожен з яких має 16 нейронів, 

65
00:04:18,346 --> 00:04:20,560
і, зізнаюся, це досить довільний вибір.

66
00:04:21,019 --> 00:04:23,366
Чесно кажучи, я вибрав два шари, виходячи з того, 

67
00:04:23,366 --> 00:04:26,979
як я хочу мотивувати структуру в одну мить, і 16, ну, це просто гарне число, 

68
00:04:26,979 --> 00:04:28,200
щоб поміститися на екрані.

69
00:04:28,780 --> 00:04:32,340
На практиці тут є багато простору для експериментів з конкретною структурою.

70
00:04:33,020 --> 00:04:38,480
Як працює мережа, активації на одному рівні визначають активації на наступному рівні.

71
00:04:39,200 --> 00:04:43,890
І, звичайно, суть мережі як механізму обробки інформації зводиться до того, 

72
00:04:43,890 --> 00:04:48,580
як саме активації на одному рівні спричиняють активації на наступному рівні.

73
00:04:49,140 --> 00:04:54,372
Це можна порівняти з тим, як у біологічних мережах нейронів деякі групи нейронів, 

74
00:04:54,372 --> 00:04:57,180
що стріляють, викликають стрілянину в інших.

75
00:04:58,120 --> 00:05:01,130
Зараз мережа, яку я показую тут, вже навчена розпізнавати цифри, 

76
00:05:01,130 --> 00:05:03,400
і дозвольте мені показати вам, що я маю на увазі.

77
00:05:03,640 --> 00:05:08,250
Це означає, що якщо ви подаєте зображення, підсвічуючи всі 784 нейрони вхідного 

78
00:05:08,250 --> 00:05:11,765
шару відповідно до яскравості кожного пікселя на зображенні, 

79
00:05:11,765 --> 00:05:16,375
цей шаблон активацій викликає певний дуже специфічний шаблон у наступному шарі, 

80
00:05:16,375 --> 00:05:20,063
який викликає певний шаблон у наступному за ним, який, зрештою, 

81
00:05:20,063 --> 00:05:22,080
дає певний шаблон у вихідному шарі.

82
00:05:22,560 --> 00:05:26,035
І найяскравіший нейрон цього вихідного шару - це вибір мережі, 

83
00:05:26,035 --> 00:05:29,400
так би мовити, для того, яку цифру представляє це зображення.

84
00:05:32,560 --> 00:05:36,567
І перш ніж перейти до математичних розрахунків того, як один шар впливає на інший, 

85
00:05:36,567 --> 00:05:40,912
або як працює навчання, давайте просто поговоримо про те, чому взагалі розумно очікувати, 

86
00:05:40,912 --> 00:05:43,520
що шарувата структура, як ця, буде поводитися розумно.

87
00:05:44,060 --> 00:05:45,220
Чого ми очікуємо тут?

88
00:05:45,400 --> 00:05:47,600
Яка найкраща надія для цих середніх верств?

89
00:05:48,920 --> 00:05:53,520
Коли ви або я розпізнаємо цифри, ми складаємо їх з різних компонентів.

90
00:05:54,200 --> 00:05:56,820
Дев'ятка має петлю зверху і лінію праворуч.

91
00:05:57,380 --> 00:06:01,180
Вісімка також має петлю вгорі, але вона поєднана з іншою петлею внизу.

92
00:06:01,980 --> 00:06:06,820
А 4 в основному розбивається на три конкретні рядки і тому подібні речі.

93
00:06:07,600 --> 00:06:12,532
В ідеальному світі ми могли б сподіватися, що кожен нейрон у передостанньому шарі 

94
00:06:12,532 --> 00:06:17,945
відповідає одному з цих підкомпонентів, що кожного разу, коли ви завантажуєте зображення, 

95
00:06:17,945 --> 00:06:21,734
скажімо, з петлею зверху, наприклад, 9 або 8, є певний нейрон, 

96
00:06:21,734 --> 00:06:23,780
активація якого буде близька до 1.

97
00:06:24,500 --> 00:06:27,426
І я не маю на увазі цю конкретну петлю пікселів, а сподіваюся, 

98
00:06:27,426 --> 00:06:31,560
що будь-який загальний петлеподібний візерунок у напрямку до вершини запускає цей нейрон.

99
00:06:32,440 --> 00:06:37,176
Таким чином, для переходу від третього рівня до останнього потрібно лише запам'ятати, 

100
00:06:37,176 --> 00:06:40,040
яка комбінація підкомпонентів відповідає якій цифрі.

101
00:06:41,000 --> 00:06:43,141
Звісно, це лише відкидає проблему на задній план, 

102
00:06:43,141 --> 00:06:45,883
адже як ви розпізнаєте ці підкомпоненти, або навіть дізнаєтесь, 

103
00:06:45,883 --> 00:06:47,640
якими мають бути правильні підкомпоненти?

104
00:06:48,060 --> 00:06:51,118
І я ще навіть не говорив про те, як один шар впливає на інший, 

105
00:06:51,118 --> 00:06:53,060
але давайте поміркуємо над цим питанням.

106
00:06:53,680 --> 00:06:56,680
Розпізнавання циклу також може розбиватися на підпроблеми.

107
00:06:57,280 --> 00:07:01,510
Один з розумних способів зробити це - спочатку розпізнати різні маленькі грані, 

108
00:07:01,510 --> 00:07:02,780
з яких вона складається.

109
00:07:03,780 --> 00:07:07,477
Аналогічно, довга лінія, подібна до тієї, що ви бачите у цифрах 1, 

110
00:07:07,477 --> 00:07:10,567
4 або 7, насправді є просто довгим краєм, або, можливо, 

111
00:07:10,567 --> 00:07:14,320
ви думаєте про неї як про певний візерунок з декількох менших країв.

112
00:07:15,140 --> 00:07:18,864
Тож, можливо, ми сподіваємося, що кожен нейрон у другому 

113
00:07:18,864 --> 00:07:22,720
шарі мережі відповідає різним відповідним маленьким ребрам.

114
00:07:23,540 --> 00:07:28,424
Можливо, коли надходить зображення, подібне до цього, воно запалює всі нейрони, 

115
00:07:28,424 --> 00:07:33,431
пов'язані з 8-10 певними маленькими краями, які, в свою чергу, запалюють нейрони, 

116
00:07:33,431 --> 00:07:38,437
пов'язані з верхньою петлею і довгою вертикальною лінією, а ті запалюють нейрони, 

117
00:07:38,437 --> 00:07:39,720
пов'язані з цифрою 9.

118
00:07:40,680 --> 00:07:44,668
Чи це те, чим насправді займається наша остаточна мережа - це інше питання, 

119
00:07:44,668 --> 00:07:48,499
до якого я повернуся, коли ми побачимо, як навчати мережу, але це надія, 

120
00:07:48,499 --> 00:07:52,540
яку ми можемо мати, свого роду мета з такою багаторівневою структурою, як ця.

121
00:07:53,160 --> 00:07:56,517
Більше того, ви можете собі уявити, наскільки корисною буде можливість 

122
00:07:56,517 --> 00:08:00,300
виявляти краї та шаблони подібним чином для інших задач розпізнавання зображень.

123
00:08:00,880 --> 00:08:04,311
І навіть за межами розпізнавання зображень є всілякі інтелектуальні речі, 

124
00:08:04,311 --> 00:08:07,280
які ви можете захотіти зробити, розбиваючись на шари абстракції.

125
00:08:08,040 --> 00:08:11,127
Наприклад, синтаксичний аналіз мовлення полягає в тому, 

126
00:08:11,127 --> 00:08:15,648
що ми беремо сире аудіо і виділяємо окремі звуки, які поєднуються в певні склади, 

127
00:08:15,648 --> 00:08:20,060
які поєднуються в слова, які поєднуються в фрази і більш абстрактні думки, тощо.

128
00:08:21,100 --> 00:08:24,845
Але повертаючись до того, як все це насправді працює, уявіть, 

129
00:08:24,845 --> 00:08:29,920
що ви зараз розробляєте, як саме активації в одному шарі можуть визначати наступний.

130
00:08:30,860 --> 00:08:33,729
Мета полягає в тому, щоб мати якийсь механізм, 

131
00:08:33,729 --> 00:08:38,980
який міг би об'єднувати пікселі в ребра, або ребра в візерунки, або візерунки в цифри.

132
00:08:39,440 --> 00:08:44,387
А щоб наблизити один дуже конкретний приклад, скажімо, надія полягає в тому, 

133
00:08:44,387 --> 00:08:47,728
що один конкретний нейрон у другому шарі визначить, 

134
00:08:47,728 --> 00:08:50,620
чи є на зображенні край у цій області, чи ні.

135
00:08:51,440 --> 00:08:55,100
Питання в тому, які параметри повинна мати мережа?

136
00:08:55,640 --> 00:08:58,720
Які регулятори і ручки ви повинні мати можливість налаштувати так, 

137
00:08:58,720 --> 00:09:02,123
щоб вони були достатньо виразними, щоб потенційно захопити цей візерунок, 

138
00:09:02,123 --> 00:09:04,745
або будь-який інший піксельний візерунок, або візерунок, 

139
00:09:04,745 --> 00:09:07,780
який може утворювати петлю з декількох країв, і інші подібні речі?

140
00:09:08,720 --> 00:09:15,560
Отже, ми призначимо вагу кожному зв'язку між нашим нейроном і нейронами з першого шару.

141
00:09:16,320 --> 00:09:17,700
Ці ваги - лише цифри.

142
00:09:18,540 --> 00:09:21,984
Потім візьміть всі ці активації з першого шару і 

143
00:09:21,984 --> 00:09:25,500
обчисліть їхню зважену суму відповідно до цих ваг.

144
00:09:27,700 --> 00:09:31,411
Я вважаю, що корисно уявити ці ваги як організовані у власну сітку, 

145
00:09:31,411 --> 00:09:35,176
і буду використовувати зелені пікселі для позначення позитивних ваг, 

146
00:09:35,176 --> 00:09:37,959
а червоні пікселі - для позначення негативних ваг, 

147
00:09:37,959 --> 00:09:41,780
де яскравість пікселя є деяким приблизним відображенням значення ваги.

148
00:09:42,780 --> 00:09:46,292
Тепер, якщо ми зробили ваги, пов'язані майже з усіма пікселями, 

149
00:09:46,292 --> 00:09:49,586
нульовими, за винятком деяких позитивних ваг у цій області, 

150
00:09:49,586 --> 00:09:53,264
яка нас цікавить, то отримання зваженої суми всіх значень пікселів 

151
00:09:53,264 --> 00:09:57,820
насправді зводиться до додавання значень пікселів лише в області, яка нас цікавить.

152
00:09:59,140 --> 00:10:02,222
І якщо ви дійсно хочете визначити, чи є тут край, 

153
00:10:02,222 --> 00:10:06,600
ви можете встановити від'ємні ваги, пов'язані з навколишніми пікселями.

154
00:10:07,480 --> 00:10:11,265
Тоді сума буде найбільшою, коли середні пікселі будуть яскравими, 

155
00:10:11,265 --> 00:10:12,700
а навколишні - темнішими.

156
00:10:14,260 --> 00:10:19,209
Коли ви обчислюєте зважену суму таким чином, ви можете отримати будь-яке число, 

157
00:10:19,209 --> 00:10:23,540
але для цієї мережі ми хочемо, щоб активації були значенням між 0 і 1.

158
00:10:24,120 --> 00:10:29,111
Отже, звичайна річ, яку потрібно зробити, це перекачати цю зважену суму в деяку функцію, 

159
00:10:29,111 --> 00:10:32,140
яка втискає дійсну числову лінію в діапазон між 0 і 1.

160
00:10:32,460 --> 00:10:35,766
І загальна функція, яка робить це, називається сигмоїдною функцією, 

161
00:10:35,766 --> 00:10:37,420
також відомою як логістична крива.

162
00:10:38,000 --> 00:10:41,297
В основному дуже від'ємні входи наближаються до 0, 

163
00:10:41,297 --> 00:10:46,600
позитивні входи наближаються до 1, і вона просто постійно зростає навколо входу 0.

164
00:10:49,120 --> 00:10:56,360
Отже, активація нейрона тут є мірою того, наскільки позитивною є відповідна зважена сума.

165
00:10:57,540 --> 00:11:01,880
Але, можливо, ви не хочете, щоб нейрон світився, коли зважена сума більша за 0.

166
00:11:02,280 --> 00:11:06,360
Можливо, ви хочете, щоб він був активний лише тоді, коли сума більша, скажімо, за 10.

167
00:11:06,840 --> 00:11:10,260
Тобто, ви хочете, щоб він був неактивним.

168
00:11:11,380 --> 00:11:15,406
Далі ми просто додамо до цієї зваженої суми ще якесь число, наприклад, 

169
00:11:15,406 --> 00:11:19,660
від'ємне 10, перш ніж пропустити її через функцію сигмоїдного згладжування.

170
00:11:20,580 --> 00:11:22,440
Це додаткове число називається зміщенням.

171
00:11:23,460 --> 00:11:28,730
Таким чином, ваги показують, на який піксельний шаблон реагує нейрон у другому шарі, 

172
00:11:28,730 --> 00:11:32,823
а зсув показує, наскільки великою має бути сума зважених значень, 

173
00:11:32,823 --> 00:11:35,180
щоб нейрон почав проявляти активність.

174
00:11:36,120 --> 00:11:37,680
І це лише один нейрон.

175
00:11:38,280 --> 00:11:44,816
Кожен інший нейрон цього шару буде з'єднаний з усіма 784 піксельними нейронами 

176
00:11:44,816 --> 00:11:50,940
першого шару, і кожен з цих 784 зв'язків має власну вагу, пов'язану з ним.

177
00:11:51,600 --> 00:11:55,761
Крім того, кожне з них має деяке зміщення, яке ви додаєте до зваженої суми перед тим, 

178
00:11:55,761 --> 00:11:57,600
як розчавити її за допомогою сигмоїда.

179
00:11:58,110 --> 00:11:59,540
І тут є над чим подумати!

180
00:11:59,960 --> 00:12:06,142
З цим прихованим шаром з 16 нейронів, це загалом 784 помножені на 16 ваг, 

181
00:12:06,142 --> 00:12:07,980
разом з 16 зміщеннями.

182
00:12:08,840 --> 00:12:11,940
І все це - лише з'єднання першого шару з другим.

183
00:12:12,520 --> 00:12:17,340
Зв'язки між іншими шарами також мають купу ваг і упереджень, пов'язаних з ними.

184
00:12:18,340 --> 00:12:23,800
Загалом, ця мережа має майже рівно 13 000 загальних ваг та зміщень.

185
00:12:23,800 --> 00:12:27,473
13 000 ручок і циферблатів, які можна налаштовувати і повертати, 

186
00:12:27,473 --> 00:12:29,960
щоб змусити цю мережу поводитися по-різному.

187
00:12:31,040 --> 00:12:34,018
Отже, коли ми говоримо про навчання, ми маємо на увазі, 

188
00:12:34,018 --> 00:12:38,381
що комп'ютер може знайти правильне значення для всіх цих багатьох багатьох чисел, 

189
00:12:38,381 --> 00:12:41,360
так що він дійсно розв'яже задачу, яка стоїть перед ним.

190
00:12:42,620 --> 00:12:47,468
Один з уявних експериментів, який одночасно веселий і трохи жахливий, - це уявити, 

191
00:12:47,468 --> 00:12:50,738
як ви сидите і встановлюєте всі ці ваги і зсуви вручну, 

192
00:12:50,738 --> 00:12:54,886
цілеспрямовано підлаштовуючи цифри так, щоб другий шар вловлював краї, 

193
00:12:54,886 --> 00:12:56,580
третій шар - візерунки і т.д.

194
00:12:56,980 --> 00:12:59,466
Особисто я знаходжу в цьому більше задоволення, 

195
00:12:59,466 --> 00:13:02,471
ніж у ставленні до мережі як до тотального чорного ящика, 

196
00:13:02,471 --> 00:13:06,875
тому що коли мережа не працює так, як ви очікуєте, якщо ви трохи розібралися в тому, 

197
00:13:06,875 --> 00:13:11,434
що насправді означають ці ваги і упередження, у вас є відправна точка для експериментів 

198
00:13:11,434 --> 00:13:14,180
з тим, як змінити структуру, щоб поліпшити її роботу.

199
00:13:14,960 --> 00:13:18,114
Якщо ж мережа працює, але не з тих причин, які ви очікували, 

200
00:13:18,114 --> 00:13:21,476
то аналіз вагових коефіцієнтів та упереджень - це хороший спосіб 

201
00:13:21,476 --> 00:13:25,820
поставити під сумнів ваші припущення і дійсно розкрити весь простір можливих рішень.

202
00:13:26,840 --> 00:13:30,680
До речі, фактична функція тут трохи громіздка для запису, вам не здається?

203
00:13:32,500 --> 00:13:37,140
Тож дозвольте мені показати вам більш компактний спосіб представлення цих зв'язків.

204
00:13:37,660 --> 00:13:40,520
Ось як ви це побачите, якщо вирішите почитати більше про нейронні мережі.

205
00:13:41,380 --> 00:13:47,509
Організуйте всі активації з одного шару в стовпчик, 

206
00:13:47,509 --> 00:13:58,000
оскільки матриця відповідає зв'язкам між одним шаром і певним нейроном у наступному шарі.

207
00:13:58,540 --> 00:14:04,076
Це означає, що взяття зваженої суми активацій у першому шарі відповідно до цих ваг 

208
00:14:04,076 --> 00:14:09,880
відповідає одному з доданків у матричному векторному добутку всього, що ми маємо зліва.

209
00:14:14,000 --> 00:14:17,858
До речі, багато чого в машинному навчанні зводиться до хорошого розуміння 

210
00:14:17,858 --> 00:14:21,404
лінійної алгебри, тому для тих з вас, хто хоче мати гарне візуальне 

211
00:14:21,404 --> 00:14:25,106
уявлення про матриці і про те, що означає множення матричних векторів, 

212
00:14:25,106 --> 00:14:28,600
подивіться серію моїх статей про лінійну алгебру, особливо главу 3.

213
00:14:29,240 --> 00:14:33,559
Повертаючись до нашого виразу, замість того, щоб говорити про додавання зміщення до 

214
00:14:33,559 --> 00:14:37,826
кожного з цих значень незалежно, ми представляємо його шляхом організації всіх цих 

215
00:14:37,826 --> 00:14:42,300
зміщень у вектор і додавання всього вектора до попереднього векторного добутку матриці.

216
00:14:43,280 --> 00:14:47,119
На останньому кроці я обгорну сигмоїд навколо зовнішньої сторони, 

217
00:14:47,119 --> 00:14:50,667
і це означатиме, що ви застосуєте функцію сигмоїда до кожної 

218
00:14:50,667 --> 00:14:54,740
конкретної компоненти результуючого вектора, що знаходиться всередині.

219
00:14:55,940 --> 00:15:00,562
Отже, як тільки ви записуєте цю вагову матрицю і ці вектори як власні символи, 

220
00:15:00,562 --> 00:15:05,478
ви можете передати повний перехід активацій від одного шару до іншого у надзвичайно 

221
00:15:05,478 --> 00:15:10,159
стислому і акуратному маленькому виразі, і це робить відповідний код і набагато 

222
00:15:10,159 --> 00:15:15,191
простішим, і набагато швидшим, оскільки багато бібліотек до біса оптимізують множення 

223
00:15:15,191 --> 00:15:15,660
матриць.

224
00:15:17,820 --> 00:15:21,460
Пам'ятаєте, як раніше я говорив, що ці нейрони - це просто речі, які зберігають числа?

225
00:15:22,220 --> 00:15:27,520
Звичайно, конкретні числа, які вони зберігають, залежать від зображення, 

226
00:15:27,520 --> 00:15:32,530
яке ви подаєте, тому точніше думати про кожен нейрон як про функцію, 

227
00:15:32,530 --> 00:15:38,340
яка приймає виходи всіх нейронів попереднього шару і випльовує число від 0 до 1.

228
00:15:39,200 --> 00:15:43,320
Насправді вся мережа - це просто функція, яка приймає 

229
00:15:43,320 --> 00:15:47,060
на вхід 784 числа і випльовує 10 чисел на виході.

230
00:15:47,560 --> 00:15:52,801
Це абсурдно складна функція, яка включає 13 000 параметрів у вигляді цих ваг і зсувів, 

231
00:15:52,801 --> 00:15:57,742
які враховують певні закономірності, і яка передбачає ітерацію багатьох матричних 

232
00:15:57,742 --> 00:16:02,020
векторних добутків і сигмоїдної функції згладжування, але тим не менш, 

233
00:16:02,020 --> 00:16:06,660
це просто функція, і в певному сенсі це заспокоює, що вона виглядає складною.

234
00:16:07,340 --> 00:16:10,367
Я маю на увазі, що якби він був ще простішим, то чи могли б ми сподіватися, 

235
00:16:10,367 --> 00:16:12,280
що він зможе прийняти виклик розпізнавання цифр?

236
00:16:13,340 --> 00:16:14,700
І як він приймає цей виклик?

237
00:16:15,080 --> 00:16:19,360
Як ця мережа вивчає відповідні ваги та упередження, просто дивлячись на дані?

238
00:16:20,140 --> 00:16:23,520
Саме це я покажу в наступному відео, а також трохи докладніше розкажу про те, 

239
00:16:23,520 --> 00:16:26,120
чим насправді займається ця конкретна мережа, яку ми бачимо.

240
00:16:27,580 --> 00:16:30,356
Тепер, напевно, варто сказати, що ви можете підписатися, 

241
00:16:30,356 --> 00:16:33,717
щоб отримувати сповіщення про вихід відео або будь-яких нових відео, 

242
00:16:33,717 --> 00:16:37,420
але насправді більшість з вас не отримують сповіщень від YouTube, чи не так?

243
00:16:38,020 --> 00:16:41,138
Можливо, чесніше було б сказати "підписатися", щоб нейронні мережі, 

244
00:16:41,138 --> 00:16:44,623
які лежать в основі алгоритму рекомендацій YouTube, були налаштовані на те, 

245
00:16:44,623 --> 00:16:47,880
що ви хочете бачити контент з цього каналу, який вам рекомендуватимуть.

246
00:16:48,560 --> 00:16:49,940
У будь-якому випадку, слідкуйте за новинами.

247
00:16:50,760 --> 00:16:53,500
Щиро дякуємо всім, хто підтримав ці відео на Patreon.

248
00:16:54,000 --> 00:16:57,234
Цього літа я трохи повільно просувався в імовірнісних рядах, 

249
00:16:57,234 --> 00:17:01,900
але після цього проекту я повернуся до них, тож ви можете слідкувати за оновленнями там.

250
00:17:03,600 --> 00:17:07,154
На завершення зі мною Лейша Лі, яка захистила докторську дисертацію з 

251
00:17:07,154 --> 00:17:11,725
теоретичної сторони глибокого навчання і зараз працює у венчурній фірмі Amplify Partners, 

252
00:17:11,725 --> 00:17:14,619
яка люб'язно надала частину фінансування для цього відео.

253
00:17:15,460 --> 00:17:19,119
Отже, Лейшо, я думаю, що ми повинні швидко згадати про цю сигмоїдну функцію.

254
00:17:19,700 --> 00:17:21,980
Як я розумію, ранні мережі використовували це, 

255
00:17:21,980 --> 00:17:25,376
щоб втиснути відповідну зважену суму в інтервал між нулем і одиницею, 

256
00:17:25,376 --> 00:17:29,160
мотивуючи це біологічною аналогією нейронів, які можуть бути або неактивними, 

257
00:17:29,160 --> 00:17:29,840
або активними.

258
00:17:30,280 --> 00:17:30,300
Саме так.

259
00:17:30,560 --> 00:17:34,040
Але відносно небагато сучасних мереж насправді використовують сигмоїд.

260
00:17:34,320 --> 00:17:34,320
Так.

261
00:17:34,440 --> 00:17:35,540
Це типу стара школа, чи не так?

262
00:17:35,760 --> 00:17:38,980
Так, точніше, релу, здається, набагато легше тренувати.

263
00:17:39,400 --> 00:17:42,340
А relu - це випрямлена лінійна одиниця?

264
00:17:42,680 --> 00:17:48,543
Так, це така функція, де ви просто берете максимум нуля і a, де a задається тим, 

265
00:17:48,543 --> 00:17:52,959
що ви пояснювали у відео, і чим це було мотивовано, я думаю, 

266
00:17:52,959 --> 00:17:58,316
частково біологічною аналогією з тим, як нейрони або активуються, або ні, 

267
00:17:58,316 --> 00:18:04,252
і якщо він переходить певний поріг, то це буде функція ідентичності, але якщо ні, 

268
00:18:04,252 --> 00:18:08,595
то він просто не буде активований, і буде дорівнювати нулю, 

269
00:18:08,595 --> 00:18:10,840
так що це свого роду спрощення.

270
00:18:11,160 --> 00:18:14,230
Використання сигмоїдів не допомагало навчанню, 

271
00:18:14,230 --> 00:18:19,327
або в якийсь момент було дуже важко навчатися, і люди просто спробували Relu, 

272
00:18:19,327 --> 00:18:24,620
і виявилося, що це дуже добре працює для цих неймовірно глибоких нейронних мереж.

273
00:18:25,100 --> 00:18:25,640
Гаразд, дякую, Алісіє.

