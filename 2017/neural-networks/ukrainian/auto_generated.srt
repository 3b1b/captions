1
00:00:00,000 --> 00:00:01,516
Це 3.

2
00:00:01,516 --> 00:00:07,563
Він неохайно написаний і відтворений із надзвичайно низькою роздільною

3
00:00:07,563 --> 00:00:13,781
здатністю 28x28 пікселів, але ваш мозок без проблем розпізнає його як 3.

4
00:00:13,781 --> 00:00:16,414
І я хочу, щоб ви знайшли хвилинку, щоб оцінити,

5
00:00:16,414 --> 00:00:19,650
наскільки це божевілля, що мозок може робити це так легко.

6
00:00:19,650 --> 00:00:24,227
Я маю на увазі, що це, це і це також розпізнаються як 3s, навіть якщо конкретні

7
00:00:24,227 --> 00:00:28,747
значення кожного пікселя сильно відрізняються від одного зображення до іншого.

8
00:00:28,747 --> 00:00:32,609
Конкретні світлочутливі клітини у вашому оці, які спрацьовують, коли ви

9
00:00:32,609 --> 00:00:36,845
бачите це 3, дуже відрізняються від тих, що спрацьовують, коли ви бачите це 3.

10
00:00:36,845 --> 00:00:43,222
Але щось у вашій шалено-розумній зоровій корі розпізнає їх як такі, що представляють

11
00:00:43,222 --> 00:00:49,300
одну і ту саму ідею, водночас розпізнаючи інші зображення як власні окремі ідеї.

12
00:00:49,300 --> 00:00:54,896
Але якби я сказав тобі, сядьте і напишіть для мене програму, яка приймає сітку

13
00:00:54,896 --> 00:01:00,492
28x28 і виводить єдине число від 0 до 10, повідомляючи вам, якою, на її думку,

14
00:01:00,492 --> 00:01:06,160
є ця цифра, то завдання перетворюється з комічно тривіального на страшно важко.

15
00:01:06,160 --> 00:01:10,551
Якщо ви не жили під каменем, я думаю, що мені навряд чи потрібно мотивувати актуальність

16
00:01:10,551 --> 00:01:14,598
і важливість машинного навчання та нейронних мереж для сьогодення та майбутнього.

17
00:01:14,598 --> 00:01:17,936
Але я хочу тут показати вам, що насправді таке нейронна мережа,

18
00:01:17,936 --> 00:01:21,222
не припускаючи жодної передісторії, і допомогти візуалізувати,

19
00:01:21,222 --> 00:01:24,300
що вона робить, не як модне слово, а як шматок математики.

20
00:01:24,300 --> 00:01:29,389
Я сподіваюся, що ви відчуєте, що сама структура вмотивована, і ви відчуєте, що знаєте,

21
00:01:29,389 --> 00:01:34,596
що це означає, коли ви читаєте або чуєте про нейронну мережу, яка вивчає цитати-розриви.

22
00:01:34,596 --> 00:01:40,300
Це відео буде присвячено лише структурному компоненту, а наступне – навчанню.

23
00:01:40,300 --> 00:01:46,264
Ми збираємося створити нейронну мережу, яка навчиться розпізнавати рукописні цифри.

24
00:01:46,264 --> 00:01:50,811
Це дещо класичний приклад представлення теми, і я радий дотримуватися

25
00:01:50,811 --> 00:01:55,487
статус-кво тут, тому що в кінці двох відео я хочу вказати вам на кілька

26
00:01:55,487 --> 00:01:59,644
хороших ресурсів, де ви можете дізнатися більше, і де ви можете

27
00:01:59,644 --> 00:02:04,256
завантажити код, який це робить, і пограти з ним на своєму комп’ютері.

28
00:02:04,256 --> 00:02:09,369
Існує багато різноманітних варіантів нейронних мереж, і останніми роками спостерігався

29
00:02:09,369 --> 00:02:14,071
певний бум у дослідженні цих варіантів, але в цих двох вступних відео ми з вами

30
00:02:14,071 --> 00:02:19,009
просто подивимося на найпростішу звичайну ванільну форму без додаткових вишукувань.

31
00:02:19,009 --> 00:02:23,488
Це начебто необхідна передумова для розуміння будь-якого з потужніших сучасних

32
00:02:23,488 --> 00:02:28,591
варіантів, і, повірте мені, у нас все ще є багато складності, про яку ми можемо подумати.

33
00:02:28,591 --> 00:02:32,462
Але навіть у цій найпростішій формі він може навчитися

34
00:02:32,462 --> 00:02:36,685
розпізнавати рукописні цифри, що для комп’ютера дуже круто.

35
00:02:36,685 --> 00:02:39,864
І в той же час ви побачите, як він не виправдовує

36
00:02:39,864 --> 00:02:42,980
кількох надій, які ми можемо покладати на нього.

37
00:02:42,980 --> 00:02:48,246
Як випливає з назви, нейронні мережі надихає мозок, але давайте розберемо це.

38
00:02:48,246 --> 00:02:51,943
Що таке нейрони і в якому сенсі вони пов’язані між собою?

39
00:02:51,943 --> 00:02:56,364
Зараз, коли я кажу про нейрон, все, про що я хочу, щоб ви

40
00:02:56,364 --> 00:03:01,014
думали, це річ, яка містить число, зокрема число від 0 до 1.

41
00:03:01,014 --> 00:03:03,407
Насправді це не більше того.

42
00:03:03,407 --> 00:03:08,717
Наприклад, мережа починається з групи нейронів, що відповідають кожному

43
00:03:08,717 --> 00:03:13,732
з 28 по 28 пікселів вхідного зображення, тобто загалом 784 нейрони.

44
00:03:13,732 --> 00:03:19,424
Кожне з них містить число, яке представляє значення градацій сірого відповідного

45
00:03:19,424 --> 00:03:24,413
пікселя в діапазоні від 0 для чорних пікселів до 1 для білих пікселів.

46
00:03:24,413 --> 00:03:30,105
Це число всередині нейрона називається його активацією, і ви можете

47
00:03:30,105 --> 00:03:35,965
мати на увазі, що кожен нейрон світиться, коли його активація висока.

48
00:03:35,965 --> 00:03:43,500
Отже, усі ці 784 нейрони складають перший рівень нашої мережі.

49
00:03:43,500 --> 00:03:48,228
Переходячи до останнього шару, ми маємо 10 нейронів,

50
00:03:48,228 --> 00:03:51,618
кожен з яких представляє одну з цифр.

51
00:03:51,618 --> 00:03:56,773
Активація в цих нейронах, знову якесь число від 0 до 1, показує,

52
00:03:56,773 --> 00:04:02,246
наскільки система вважає, що дане зображення відповідає даній цифрі.

53
00:04:02,246 --> 00:04:06,000
Існує також пара проміжних шарів, які називаються прихованими

54
00:04:06,000 --> 00:04:09,997
шарами, які на даний момент повинні бути просто величезним знаком

55
00:04:09,997 --> 00:04:13,993
питання про те, як цей процес розпізнавання цифр відбуватиметься.

56
00:04:13,993 --> 00:04:17,457
У цій мережі я вибрав два прихованих шари, кожен з яких

57
00:04:17,457 --> 00:04:20,673
містить 16 нейронів, і, правда, це довільний вибір.

58
00:04:20,673 --> 00:04:24,431
Чесно кажучи, я вибрав два шари, виходячи з того, як я хочу мотивувати

59
00:04:24,431 --> 00:04:28,666
структуру за мить, і 16, це було просто гарне число, щоб поміститися на екрані.

60
00:04:28,666 --> 00:04:32,866
На практиці тут є багато можливостей для експериментів із конкретною структурою.

61
00:04:32,866 --> 00:04:38,685
Спосіб роботи мережі, активація на одному рівні визначає активацію наступного рівня.

62
00:04:38,685 --> 00:04:43,837
І, звичайно, суть мережі як механізму обробки інформації зводиться до того, як

63
00:04:43,837 --> 00:04:48,924
саме ці активації з одного рівня призводять до активацій на наступному рівні.

64
00:04:48,924 --> 00:04:52,899
Це приблизно аналогічно тому, як у біологічних мережах

65
00:04:52,899 --> 00:04:57,091
нейронів деякі групи нейронів викликають активацію інших.

66
00:04:57,091 --> 00:05:00,099
Тепер мережа, яку я тут показую, уже навчена розпізнавати

67
00:05:00,099 --> 00:05:03,471
цифри, і дозвольте мені показати вам, що я маю на увазі під цим.

68
00:05:03,471 --> 00:05:08,262
Це означає, що якщо ви подаєте зображення, яке освітлює всі 784 нейрони вхідного

69
00:05:08,262 --> 00:05:13,113
шару відповідно до яскравості кожного пікселя на зображенні, цей шаблон активації

70
00:05:13,113 --> 00:05:17,904
спричиняє дуже специфічний шаблон у наступному шарі, який викликає певний шаблон

71
00:05:17,904 --> 00:05:22,223
у наступному шарі. це, що нарешті дає певний візерунок у вихідному шарі.

72
00:05:22,223 --> 00:05:26,198
І найяскравішим нейроном цього вихідного рівня є вибір

73
00:05:26,198 --> 00:05:30,534
мережі, так би мовити, яку цифру представляє це зображення.

74
00:05:30,534 --> 00:05:35,044
І перш ніж перейти до математики щодо того, як один шар впливає на наступний,

75
00:05:35,044 --> 00:05:39,149
або як працює навчання, давайте просто поговоримо про те, чому взагалі

76
00:05:39,149 --> 00:05:43,485
розумно очікувати, що така багатошарова структура буде поводитися розумно.

77
00:05:43,485 --> 00:05:45,084
Чого ми тут очікуємо?

78
00:05:45,084 --> 00:05:49,093
Яка найкраща надія на те, що можуть робити ці середні верстви?

79
00:05:49,093 --> 00:05:53,786
Що ж, коли ви чи я розпізнаємо цифри, ми збираємо разом різні компоненти.

80
00:05:53,786 --> 00:05:56,998
9 має петлю вгорі та лінію праворуч.

81
00:05:56,998 --> 00:06:01,901
8 також має петлю зверху, але вона поєднана з іншою петлею внизу.

82
00:06:01,901 --> 00:06:06,771
4 в основному розбивається на три конкретні рядки тощо.

83
00:06:06,771 --> 00:06:10,955
Тепер, в ідеальному світі, ми можемо сподіватися, що кожен нейрон у

84
00:06:10,955 --> 00:06:15,386
передостанньому шарі відповідає одному з цих підкомпонентів, що кожного

85
00:06:15,386 --> 00:06:19,570
разу, коли ви подаєте зображення, скажімо, із петлею вгорі, як-от 9

86
00:06:19,570 --> 00:06:23,755
чи 8, є деякі специфічний нейрон, чия активація буде близькою до 1.

87
00:06:23,755 --> 00:06:27,946
І я не маю на увазі цю конкретну петлю пікселів, я сподіваюся, що будь-який

88
00:06:27,946 --> 00:06:31,751
загалом петлевий візерунок у напрямку до вершини виділяє цей нейрон.

89
00:06:31,751 --> 00:06:35,807
Таким чином, щоб перейти від третього рівня до останнього, потрібно

90
00:06:35,807 --> 00:06:40,101
просто дізнатися, яка комбінація підкомпонентів відповідає яким цифрам.

91
00:06:40,101 --> 00:06:43,689
Звісно, це тільки підштовхує проблему, тому що як ви розпізнаєте ці

92
00:06:43,689 --> 00:06:47,805
підкомпоненти чи навіть дізнаєтеся, якими мають бути правильні підкомпоненти?

93
00:06:47,805 --> 00:06:50,560
І я ще навіть не говорив про те, як один шар впливає

94
00:06:50,560 --> 00:06:52,900
на наступний, але поговоримо про це на мить.

95
00:06:52,900 --> 00:06:56,650
Розпізнавання циклу також може розпадатися на підпроблеми.

96
00:06:56,650 --> 00:07:00,073
Одним із розумних способів зробити це було б спочатку

97
00:07:00,073 --> 00:07:03,433
розпізнати різні маленькі грані, які його складають.

98
00:07:03,433 --> 00:07:07,441
Подібним чином, довга лінія, подібна до тієї, яку ви можете побачити

99
00:07:07,441 --> 00:07:11,391
в цифрах 1, 4 або 7, насправді це просто довга грань, або, можливо,

100
00:07:11,391 --> 00:07:15,341
ви думаєте про неї як про певний візерунок із кількох менших ребер.

101
00:07:15,341 --> 00:07:19,352
Тому, можливо, ми сподіваємося, що кожен нейрон у другому

102
00:07:19,352 --> 00:07:23,432
шарі мережі відповідає різним відповідним маленьким краям.

103
00:07:23,432 --> 00:07:28,529
Можливо, коли з’являється таке зображення, воно висвітлює всі нейрони, пов’язані з

104
00:07:28,529 --> 00:07:33,995
приблизно 8-10 певними маленькими краями, що, у свою чергу, висвітлює нейрони, пов’язані

105
00:07:33,995 --> 00:07:39,460
з верхньою петлею та довгою вертикальною лінією, а ті висвітлюють нейрон, пов&#39;язаний

106
00:07:39,460 --> 00:07:39,768
з 9.

107
00:07:39,768 --> 00:07:43,380
Інше питання, до якого я повернуся, як тільки ми побачимо, як

108
00:07:43,380 --> 00:07:47,284
навчити мережу, чи це те, що насправді робить наша остання мережа.

109
00:07:47,284 --> 00:07:52,439
Але це надія, яку ми могли б мати, свого роду мета з такою багатошаровою структурою.

110
00:07:52,439 --> 00:07:56,547
Крім того, ви можете собі уявити, як можливість виявлення країв і візерунків,

111
00:07:56,547 --> 00:08:00,444
як це, була б справді корисною для інших завдань розпізнавання зображень.

112
00:08:00,444 --> 00:08:04,030
І навіть за межами розпізнавання зображення, є всілякі інтелектуальні

113
00:08:04,030 --> 00:08:07,412
речі, які ви можете зробити, які розбиваються на шари абстракції.

114
00:08:07,412 --> 00:08:11,628
Розбір мовлення, наприклад, передбачає взяття необробленого аудіо та виділення

115
00:08:11,628 --> 00:08:15,897
чітких звуків, які поєднуються, щоб створити певні склади, які поєднуються, щоб

116
00:08:15,897 --> 00:08:20,326
утворити слова, які поєднуються, щоб скласти фрази та більш абстрактні думки тощо.

117
00:08:20,326 --> 00:08:25,313
Але повертаючись до того, як все це насправді працює, уявіть себе зараз, коли ви

118
00:08:25,313 --> 00:08:30,731
проектуєте, як саме активації на одному рівні можуть визначати активації на наступному.

119
00:08:30,731 --> 00:08:35,105
Мета полягає в тому, щоб мати якийсь механізм, який міг би поєднувати

120
00:08:35,105 --> 00:08:38,917
пікселі в краї, або краї в візерунки, або візерунки в цифри.

121
00:08:38,917 --> 00:08:44,828
І щоб збільшити масштаб на одному дуже конкретному прикладі, скажімо, ми сподіваємося, що

122
00:08:44,828 --> 00:08:50,739
один конкретний нейрон у другому шарі зрозуміє, чи має зображення перевагу в цій області.

123
00:08:50,739 --> 00:08:55,305
Виникає питання, які параметри повинна мати мережа?

124
00:08:55,305 --> 00:08:59,442
Які циферблати та ручки ви повинні мати можливість налаштувати, щоб вони були

125
00:08:59,442 --> 00:09:03,686
достатньо виразними, щоб потенційно захопити цей візерунок, або будь-який інший

126
00:09:03,686 --> 00:09:08,247
піксельний візерунок, або візерунок, який кілька країв можуть утворювати петлю, тощо?

127
00:09:08,247 --> 00:09:12,225
Що ж, ми призначимо вагу кожному з’єднанням між

128
00:09:12,225 --> 00:09:15,790
нашим нейроном і нейронами з першого шару.

129
00:09:15,790 --> 00:09:17,797
Ці ваги - просто цифри.

130
00:09:17,797 --> 00:09:21,878
Потім візьміть усі ці активації з першого рівня та

131
00:09:21,878 --> 00:09:25,800
обчисліть їх зважену суму відповідно до цих ваг.

132
00:09:25,800 --> 00:09:30,020
Я вважаю корисним розглядати ці ваги як організовані у невелику власну

133
00:09:30,020 --> 00:09:33,942
сітку, і я збираюся використовувати зелені пікселі для позначення

134
00:09:33,942 --> 00:09:38,043
позитивних ваг, а червоні пікселі — для позначення від’ємних ваг, де

135
00:09:38,043 --> 00:09:42,026
яскравість цього пікселя є деякою вільне зображення значення ваги.

136
00:09:42,026 --> 00:09:45,861
Якщо ми зробили вагові коефіцієнти, пов’язані майже з усіма пікселями,

137
00:09:45,861 --> 00:09:49,913
нульовими, за винятком деяких додатних вагових коефіцієнтів у цій області,

138
00:09:49,913 --> 00:09:53,965
які нас цікавлять, тоді взяття зваженої суми всіх значень пікселів справді

139
00:09:53,965 --> 00:09:58,070
означає лише додавання значень пікселя просто в регіон, про який ми дбаємо.

140
00:09:58,070 --> 00:10:02,798
І якщо ви дійсно хочете зрозуміти, чи є тут перевага, ви можете

141
00:10:02,798 --> 00:10:07,451
зробити деякі від’ємні ваги, пов’язані з оточуючими пікселями.

142
00:10:07,451 --> 00:10:12,680
Тоді сума найбільша, коли ті середні пікселі яскраві, а оточуючі пікселі темніші.

143
00:10:12,680 --> 00:10:18,138
Коли ви обчислюєте таку зважену суму, ви можете отримати будь-яке число, але

144
00:10:18,138 --> 00:10:23,596
для цієї мережі ми хочемо, щоб активації були деякими значеннями від 0 до 1.

145
00:10:23,596 --> 00:10:27,280
Отже, звичайна річ – це закачати цю зважену суму в якусь

146
00:10:27,280 --> 00:10:31,546
функцію, яка розміщує дійсну числову лінію в діапазоні між 0 і 1.

147
00:10:31,546 --> 00:10:34,212
І звичайна функція, яка це робить, називається

148
00:10:34,212 --> 00:10:37,560
сигмоподібною функцією, також відомою як логістична крива.

149
00:10:37,560 --> 00:10:41,185
В основному дуже негативні вхідні дані закінчуються близькими

150
00:10:41,185 --> 00:10:44,635
до 0, дуже позитивні вхідні дані закінчуються близькими до

151
00:10:44,635 --> 00:10:48,203
1, і вони просто постійно зростають навколо вхідних даних 0.

152
00:10:48,203 --> 00:10:52,705
Таким чином, активація нейрона тут є в основному показником

153
00:10:52,705 --> 00:10:56,757
того, наскільки позитивною є відповідна зважена сума.

154
00:10:56,757 --> 00:11:02,349
Але, можливо, ви не хочете, щоб нейрон світився, коли зважена сума більша за 0.

155
00:11:02,349 --> 00:11:06,838
Можливо, ви хочете, щоб він був активним лише тоді, коли сума перевищує, скажімо, 10.

156
00:11:06,838 --> 00:11:10,689
Тобто ви хочете, щоб він був неактивним.

157
00:11:10,689 --> 00:11:15,819
Тоді ми просто додамо якесь інше число, як-от мінус 10, до цієї зваженої

158
00:11:15,819 --> 00:11:20,668
суми перед тим, як підключити її до функції сигмоїдного здавлювання.

159
00:11:20,668 --> 00:11:23,395
Це додаткове число називається зміщенням.

160
00:11:23,395 --> 00:11:27,413
Отже, вагові коефіцієнти показують вам, який піксельний шаблон уловлює

161
00:11:27,413 --> 00:11:31,373
цей нейрон у другому шарі, а зсув говорить вам, наскільки високою має

162
00:11:31,373 --> 00:11:35,221
бути зважена сума, перш ніж нейрон почне ставати значно активнішим.

163
00:11:35,221 --> 00:11:37,328
І це лише один нейрон.

164
00:11:37,328 --> 00:11:44,015
Кожен інший нейрон цього шару буде з’єднаний з усіма 784 піксельними

165
00:11:44,015 --> 00:11:50,799
нейронами з першого шару, і кожне з цих 784 з’єднань має власну вагу.

166
00:11:50,799 --> 00:11:54,309
Крім того, у кожного з них є деяке зміщення, якесь інше число,

167
00:11:54,309 --> 00:11:57,986
яке ви додаєте до зваженої суми перед тим, як стиснути її сигмою.

168
00:11:57,986 --> 00:11:59,711
І це над чим подумати!

169
00:11:59,711 --> 00:12:03,712
З цим прихованим шаром із 16 нейронів це загалом

170
00:12:03,712 --> 00:12:07,877
784 помножити на 16 ваг разом із 16 упередженнями.

171
00:12:07,877 --> 00:12:12,139
І все це лише зв’язки від першого шару до другого.

172
00:12:12,139 --> 00:12:18,092
Зв’язки між іншими шарами також мають купу ваг і упереджень, пов’язаних з ними.

173
00:12:18,092 --> 00:12:24,071
Усе сказано та зроблено, ця мережа має майже рівно 13 000 загальних ваг і упереджень.

174
00:12:24,071 --> 00:12:27,365
13 000 ручок і циферблатів, які можна налаштовувати

175
00:12:27,365 --> 00:12:30,532
та повертати, щоб ця мережа працювала по-різному.

176
00:12:30,532 --> 00:12:36,267
Отже, коли ми говоримо про навчання, це стосується того, щоб змусити комп’ютер знайти

177
00:12:36,267 --> 00:12:42,003
дійсне налаштування для всіх цих багатьох чисел, щоб він фактично розв’язав проблему.

178
00:12:42,003 --> 00:12:46,664
Один уявний експеримент, який водночас веселий і жахливий, полягає в тому, щоб

179
00:12:46,664 --> 00:12:51,620
уявити, як ви сідаєте й вручну встановлюєте всі ці ваги та зміщення, цілеспрямовано

180
00:12:51,620 --> 00:12:56,576
змінюючи числа так, щоб другий шар підбирав краї, третій шар підбирав шаблони, тощо

181
00:12:56,576 --> 00:13:00,942
Особисто я вважаю це задовільним, а не ставлюся до мережі як до чорної

182
00:13:00,942 --> 00:13:05,184
скриньки, тому що коли мережа не працює так, як ви очікуєте, якщо ви

183
00:13:05,184 --> 00:13:09,611
трохи зрозумієте, що насправді означають ці ваги та упередження , у вас

184
00:13:09,611 --> 00:13:14,038
є відправна точка для експериментів зі зміною структури для покращення.

185
00:13:14,038 --> 00:13:17,980
Або коли мережа працює, але не з тих причин, які ви могли б очікувати,

186
00:13:17,980 --> 00:13:22,088
копання в тому, що роблять ваги та упередження, є хорошим способом кинути

187
00:13:22,088 --> 00:13:26,252
виклик вашим припущенням і дійсно відкрити повний простір можливих рішень.

188
00:13:26,252 --> 00:13:32,234
До речі, фактичну функцію тут трохи громіздко записати, вам не здається?

189
00:13:32,234 --> 00:13:37,130
Тож дозвольте мені показати вам більш компактний спосіб представлення цих зв’язків.

190
00:13:37,130 --> 00:13:41,210
Ось як ви побачите це, якщо вирішите прочитати більше про нейронні мережі.

191
00:13:41,210 --> 00:13:46,288
Організуйте всі активації з одного шару в стовпець як вектор.

192
00:13:46,288 --> 00:13:51,897
Потім організуйте всі ваги як матрицю, де кожен рядок цієї матриці

193
00:13:51,897 --> 00:13:58,093
відповідає зв’язкам між одним шаром і певним нейроном на наступному шарі.

194
00:13:58,093 --> 00:14:03,774
Це означає, що взяття зваженої суми активацій у першому шарі відповідно до цих ваг

195
00:14:03,774 --> 00:14:09,865
відповідає одному з доданків у векторному добутку матриці всього, що ми маємо тут зліва.

196
00:14:09,865 --> 00:14:14,720
До речі, значна частина машинного навчання зводиться лише до гарного

197
00:14:14,720 --> 00:14:19,223
розуміння лінійної алгебри, тому будь-хто з вас, хто хоче добре

198
00:14:19,223 --> 00:14:24,008
візуально розуміти матриці та значення векторного множення матриць,

199
00:14:24,008 --> 00:14:29,004
погляньте на серію, яку я робив на лінійна алгебра, особливо розділ 3.

200
00:14:29,004 --> 00:14:33,608
Повертаючись до нашого виразу, замість того, щоб говорити про додавання зміщення

201
00:14:33,608 --> 00:14:38,041
до кожного з цих значень незалежно, ми представляємо це, організовуючи всі ці

202
00:14:38,041 --> 00:14:42,816
зміщення у вектор і додаючи весь вектор до попереднього векторного добутку матриці.

203
00:14:42,816 --> 00:14:47,030
Потім, як останній крок, я оберну сигмовид навколо зовнішнього боку,

204
00:14:47,030 --> 00:14:51,182
і це має означати те, що ви збираєтеся застосувати функцію сигмоіда

205
00:14:51,182 --> 00:14:55,274
до кожного конкретного компонента результуючого вектора всередині.

206
00:14:55,274 --> 00:15:00,534
Отже, як тільки ви запишете цю вагову матрицю та ці вектори як їхні власні символи, ви

207
00:15:00,534 --> 00:15:05,371
зможете повідомити про повний перехід активацій від одного шару до наступного в

208
00:15:05,371 --> 00:15:10,389
надзвичайно вузькому та акуратному маленькому виразі, і це зробить відповідний код

209
00:15:10,389 --> 00:15:15,528
набагато простішим і набагато швидше, оскільки багато бібліотек оптимізують матричне

210
00:15:15,528 --> 00:15:16,133
множення.

211
00:15:16,133 --> 00:15:21,400
Пам’ятаєте, як раніше я казав, що ці нейрони — це просто речі, які містять числа?

212
00:15:21,400 --> 00:15:26,912
Ну, звичайно, конкретні числа, які вони зберігають, залежать від зображення,

213
00:15:26,912 --> 00:15:32,568
яке ви подаєте, тому насправді точніше розглядати кожен нейрон як функцію, яка

214
00:15:32,568 --> 00:15:38,439
приймає вихідні дані всіх нейронів попереднього шару та викидає число від 0 до 1.

215
00:15:38,439 --> 00:15:42,674
Насправді вся мережа — це лише функція, яка приймає

216
00:15:42,674 --> 00:15:47,154
784 числа як вхідні дані та видає 10 чисел як вихідні.

217
00:15:47,154 --> 00:15:52,209
Це абсурдно складна функція, яка включає 13 000 параметрів у формі цих ваг і

218
00:15:52,209 --> 00:15:57,460
зміщень, які вловлюють певні шаблони, і яка включає ітерацію багатьох векторних

219
00:15:57,460 --> 00:16:03,237
добутків матриці та функцію сигмоїдного здавлювання, але це, тим не менш, лише функція.

220
00:16:03,237 --> 00:16:06,878
І те, що це виглядає складним, певною мірою заспокоює.

221
00:16:06,878 --> 00:16:09,673
Я маю на увазі, якби це було простіше, яка б у нас була

222
00:16:09,673 --> 00:16:12,818
надія, що воно зможе впоратися з проблемою розпізнавання цифр?

223
00:16:12,818 --> 00:16:14,920
І як він приймає цей виклик?

224
00:16:14,920 --> 00:16:19,320
Як ця мережа дізнається відповідні ваги та упередження, просто переглядаючи дані?

225
00:16:19,320 --> 00:16:22,479
Що ж, це те, що я покажу в наступному відео, а також трохи

226
00:16:22,479 --> 00:16:26,227
детальніше розберусь про те, що насправді робить ця конкретна мережа.

227
00:16:26,227 --> 00:16:29,769
Тепер я вважаю, що я повинен сказати, що підписатись, щоб отримувати

228
00:16:29,769 --> 00:16:33,413
сповіщення про те, коли з’явиться це відео чи будь-які нові відео, але

229
00:16:33,413 --> 00:16:37,622
насправді більшість із вас насправді не отримує сповіщень від YouTube, чи не так?

230
00:16:37,622 --> 00:16:41,002
Можливо, відверто кажучи, я повинен сказати, що підписуйтесь, щоб

231
00:16:41,002 --> 00:16:44,536
нейронні мережі, які лежать в основі алгоритму рекомендацій YouTube,

232
00:16:44,536 --> 00:16:48,275
переконалися, що ви хочете, щоб вам рекомендували вміст із цього каналу.

233
00:16:48,275 --> 00:16:49,800
У будь-якому разі залишайтеся в курсі, щоб дізнатися більше.

234
00:16:49,800 --> 00:16:53,634
Велике спасибі всім, хто підтримує ці відео на Patreon.

235
00:16:53,634 --> 00:16:57,934
Я трохи повільно просувався в серії ймовірностей цього літа, але я повертаюся

236
00:16:57,934 --> 00:17:02,400
до цього після цього проекту, тож патрони, ви можете стежити за оновленнями там.

237
00:17:02,400 --> 00:17:06,671
Щоб закінчити, зі мною є Ліша Лі, яка захистила докторську роботу з теоретичної

238
00:17:06,671 --> 00:17:10,462
сторони глибокого навчання та зараз працює у фірмі венчурного капіталу

239
00:17:10,462 --> 00:17:14,520
Amplify Partners, яка люб’язно надала частину фінансування для цього відео.

240
00:17:14,520 --> 00:17:19,480
Отже, Ліша, я вважаю, що ми повинні швидко згадати одну річ, це цю сигмоподібну функцію.

241
00:17:19,480 --> 00:17:23,159
Наскільки я розумію, ранні мережі використовували це, щоб стиснути відповідну

242
00:17:23,159 --> 00:17:26,792
зважену суму в інтервал між нулем і одиницею, ви знаєте, начебто вмотивовані

243
00:17:26,792 --> 00:17:30,048
цією біологічною аналогією нейронів, які або неактивні, або активні.

244
00:17:30,048 --> 00:17:30,552
Точно.

245
00:17:30,552 --> 00:17:34,091
Але порівняно небагато сучасних мереж фактично використовують sigmoid.

246
00:17:34,091 --> 00:17:34,392
так

247
00:17:34,392 --> 00:17:35,945
Це стара школа, чи не так?

248
00:17:35,945 --> 00:17:38,974
Так, точніше relu, здається, набагато легше тренувати.

249
00:17:38,974 --> 00:17:42,360
А relu, relu означає випрямлену лінійну одиницю?

250
00:17:42,360 --> 00:17:46,469
Так, це така функція, де ви просто берете максимальне

251
00:17:46,469 --> 00:17:51,187
значення нуль і a, де a задано тим, що ви пояснювали у відео.

252
00:17:51,187 --> 00:17:56,245
Я вважаю, що це було частково мотивовано біологічною

253
00:17:56,245 --> 00:18:00,730
аналогією з тим, як нейрони активуються чи ні.

254
00:18:00,730 --> 00:18:05,149
Отже, якщо він перевищить певний поріг, це буде функція ідентифікації,

255
00:18:05,149 --> 00:18:09,380
але якщо ні, то вона просто не буде активована, тому буде нульовою.

256
00:18:09,380 --> 00:18:11,084
Так що це якесь спрощення.

257
00:18:11,084 --> 00:18:15,250
Використання сигмовидів не допомогло в навчанні або в якийсь момент

258
00:18:15,250 --> 00:18:19,600
було дуже важко тренуватися, і люди просто спробували relu, і сталося,

259
00:18:19,600 --> 00:18:24,072
що це дуже добре спрацювало для цих неймовірно глибоких нейронних мереж.

260
00:18:24,072 --> 00:18:26,120
Гаразд, дякую Ліша.

