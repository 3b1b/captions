1
00:00:00,000 --> 00:00:01,516
Це 3.

2
00:00:01,516 --> 00:00:07,563
Він неохайно написаний і відтворений із надзвичайно низькою роздільною

3
00:00:07,563 --> 00:00:13,781
здатністю 28x28 пікселів, але ваш мозок без проблем розпізнає його як 3.

4
00:00:13,781 --> 00:00:16,414
І я хочу, щоб ви знайшли хвилинку, щоб оцінити,

5
00:00:16,414 --> 00:00:19,650
наскільки це божевілля, що мозок може робити це так легко.

6
00:00:19,650 --> 00:00:22,968
Я маю на увазі, що це, це і це також розпізнаються як 3s,

7
00:00:22,968 --> 00:00:27,489
навіть якщо конкретні значення кожного пікселя сильно відрізняються від одного

8
00:00:27,489 --> 00:00:28,747
зображення до іншого.

9
00:00:28,747 --> 00:00:33,306
Конкретні світлочутливі клітини у вашому оці, які спрацьовують, коли ви бачите це 3,

10
00:00:33,306 --> 00:00:36,845
дуже відрізняються від тих, що спрацьовують, коли ви бачите це 3.

11
00:00:36,845 --> 00:00:41,947
Але щось у вашій шалено-розумній зоровій корі розпізнає їх як такі,

12
00:00:41,947 --> 00:00:48,324
що представляють одну і ту саму ідею, водночас розпізнаючи інші зображення як власні

13
00:00:48,324 --> 00:00:49,300
окремі ідеї.

14
00:00:49,300 --> 00:00:53,621
Але якби я сказав тобі, сядьте і напишіть для мене програму,

15
00:00:53,621 --> 00:00:59,146
яка приймає сітку 28x28 і виводить єдине число від 0 до 10, повідомляючи вам,

16
00:00:59,146 --> 00:01:04,885
якою, на її думку, є ця цифра, то завдання перетворюється з комічно тривіального

17
00:01:04,885 --> 00:01:06,160
на страшно важко.

18
00:01:06,160 --> 00:01:10,551
Якщо ви не жили під каменем, я думаю, що мені навряд чи потрібно мотивувати актуальність

19
00:01:10,551 --> 00:01:14,598
і важливість машинного навчання та нейронних мереж для сьогодення та майбутнього.

20
00:01:14,598 --> 00:01:17,936
Але я хочу тут показати вам, що насправді таке нейронна мережа,

21
00:01:17,936 --> 00:01:21,222
не припускаючи жодної передісторії, і допомогти візуалізувати,

22
00:01:21,222 --> 00:01:24,300
що вона робить, не як модне слово, а як шматок математики.

23
00:01:24,300 --> 00:01:29,389
Я сподіваюся, що ви відчуєте, що сама структура вмотивована, і ви відчуєте, що знаєте,

24
00:01:29,389 --> 00:01:34,596
що це означає, коли ви читаєте або чуєте про нейронну мережу, яка вивчає цитати-розриви.

25
00:01:34,596 --> 00:01:40,300
Це відео буде присвячено лише структурному компоненту, а наступне – навчанню.

26
00:01:40,300 --> 00:01:46,264
Ми збираємося створити нейронну мережу, яка навчиться розпізнавати рукописні цифри.

27
00:01:46,264 --> 00:01:51,850
Це дещо класичний приклад представлення теми, і я радий дотримуватися статус-кво тут,

28
00:01:51,850 --> 00:01:56,657
тому що в кінці двох відео я хочу вказати вам на кілька хороших ресурсів,

29
00:01:56,657 --> 00:02:00,748
де ви можете дізнатися більше, і де ви можете завантажити код,

30
00:02:00,748 --> 00:02:04,256
який це робить, і пограти з ним на своєму комп’ютері.

31
00:02:04,256 --> 00:02:07,430
Існує багато різноманітних варіантів нейронних мереж,

32
00:02:07,430 --> 00:02:11,720
і останніми роками спостерігався певний бум у дослідженні цих варіантів,

33
00:02:11,720 --> 00:02:16,540
але в цих двох вступних відео ми з вами просто подивимося на найпростішу звичайну

34
00:02:16,540 --> 00:02:19,009
ванільну форму без додаткових вишукувань.

35
00:02:19,009 --> 00:02:24,111
Це начебто необхідна передумова для розуміння будь-якого з потужніших сучасних варіантів,

36
00:02:24,111 --> 00:02:28,591
і, повірте мені, у нас все ще є багато складності, про яку ми можемо подумати.

37
00:02:28,591 --> 00:02:34,573
Але навіть у цій найпростішій формі він може навчитися розпізнавати рукописні цифри,

38
00:02:34,573 --> 00:02:36,685
що для комп’ютера дуже круто.

39
00:02:36,685 --> 00:02:40,818
І в той же час ви побачите, як він не виправдовує кількох надій,

40
00:02:40,818 --> 00:02:42,980
які ми можемо покладати на нього.

41
00:02:42,980 --> 00:02:48,246
Як випливає з назви, нейронні мережі надихає мозок, але давайте розберемо це.

42
00:02:48,246 --> 00:02:51,943
Що таке нейрони і в якому сенсі вони пов’язані між собою?

43
00:02:51,943 --> 00:02:56,974
Зараз, коли я кажу про нейрон, все, про що я хочу, щоб ви думали,

44
00:02:56,974 --> 00:03:01,014
це річ, яка містить число, зокрема число від 0 до 1.

45
00:03:01,014 --> 00:03:03,407
Насправді це не більше того.

46
00:03:03,407 --> 00:03:06,947
Наприклад, мережа починається з групи нейронів,

47
00:03:06,947 --> 00:03:11,740
що відповідають кожному з 28 по 28 пікселів вхідного зображення,

48
00:03:11,740 --> 00:03:13,732
тобто загалом 784 нейрони.

49
00:03:13,732 --> 00:03:19,424
Кожне з них містить число, яке представляє значення градацій сірого відповідного

50
00:03:19,424 --> 00:03:24,413
пікселя в діапазоні від 0 для чорних пікселів до 1 для білих пікселів.

51
00:03:24,413 --> 00:03:29,101
Це число всередині нейрона називається його активацією,

52
00:03:29,101 --> 00:03:35,965
і ви можете мати на увазі, що кожен нейрон світиться, коли його активація висока.

53
00:03:35,965 --> 00:03:43,500
Отже, усі ці 784 нейрони складають перший рівень нашої мережі.

54
00:03:43,500 --> 00:03:48,228
Переходячи до останнього шару, ми маємо 10 нейронів,

55
00:03:48,228 --> 00:03:51,618
кожен з яких представляє одну з цифр.

56
00:03:51,618 --> 00:03:56,773
Активація в цих нейронах, знову якесь число від 0 до 1, показує,

57
00:03:56,773 --> 00:04:02,246
наскільки система вважає, що дане зображення відповідає даній цифрі.

58
00:04:02,246 --> 00:04:06,485
Існує також пара проміжних шарів, які називаються прихованими шарами,

59
00:04:06,485 --> 00:04:10,965
які на даний момент повинні бути просто величезним знаком питання про те,

60
00:04:10,965 --> 00:04:13,993
як цей процес розпізнавання цифр відбуватиметься.

61
00:04:13,993 --> 00:04:18,756
У цій мережі я вибрав два прихованих шари, кожен з яких містить 16 нейронів,

62
00:04:18,756 --> 00:04:20,673
і, правда, це довільний вибір.

63
00:04:20,673 --> 00:04:25,437
Чесно кажучи, я вибрав два шари, виходячи з того, як я хочу мотивувати структуру за мить,

64
00:04:25,437 --> 00:04:28,666
і 16, це було просто гарне число, щоб поміститися на екрані.

65
00:04:28,666 --> 00:04:32,866
На практиці тут є багато можливостей для експериментів із конкретною структурою.

66
00:04:32,866 --> 00:04:38,685
Спосіб роботи мережі, активація на одному рівні визначає активацію наступного рівня.

67
00:04:38,685 --> 00:04:43,641
І, звичайно, суть мережі як механізму обробки інформації зводиться до того,

68
00:04:43,641 --> 00:04:48,924
як саме ці активації з одного рівня призводять до активацій на наступному рівні.

69
00:04:48,924 --> 00:04:52,899
Це приблизно аналогічно тому, як у біологічних мережах

70
00:04:52,899 --> 00:04:57,091
нейронів деякі групи нейронів викликають активацію інших.

71
00:04:57,091 --> 00:05:00,462
Тепер мережа, яку я тут показую, уже навчена розпізнавати цифри,

72
00:05:00,462 --> 00:05:03,471
і дозвольте мені показати вам, що я маю на увазі під цим.

73
00:05:03,471 --> 00:05:08,262
Це означає, що якщо ви подаєте зображення, яке освітлює всі 784 нейрони вхідного

74
00:05:08,262 --> 00:05:11,871
шару відповідно до яскравості кожного пікселя на зображенні,

75
00:05:11,871 --> 00:05:16,248
цей шаблон активації спричиняє дуже специфічний шаблон у наступному шарі,

76
00:05:16,248 --> 00:05:19,265
який викликає певний шаблон у наступному шарі. це,

77
00:05:19,265 --> 00:05:22,223
що нарешті дає певний візерунок у вихідному шарі.

78
00:05:22,223 --> 00:05:26,776
І найяскравішим нейроном цього вихідного рівня є вибір мережі,

79
00:05:26,776 --> 00:05:30,534
так би мовити, яку цифру представляє це зображення.

80
00:05:30,534 --> 00:05:35,044
І перш ніж перейти до математики щодо того, як один шар впливає на наступний,

81
00:05:35,044 --> 00:05:38,397
або як працює навчання, давайте просто поговоримо про те,

82
00:05:38,397 --> 00:05:43,485
чому взагалі розумно очікувати, що така багатошарова структура буде поводитися розумно.

83
00:05:43,485 --> 00:05:45,084
Чого ми тут очікуємо?

84
00:05:45,084 --> 00:05:49,093
Яка найкраща надія на те, що можуть робити ці середні верстви?

85
00:05:49,093 --> 00:05:53,786
Що ж, коли ви чи я розпізнаємо цифри, ми збираємо разом різні компоненти.

86
00:05:53,786 --> 00:05:56,998
9 має петлю вгорі та лінію праворуч.

87
00:05:56,998 --> 00:06:01,901
8 також має петлю зверху, але вона поєднана з іншою петлею внизу.

88
00:06:01,901 --> 00:06:06,771
4 в основному розбивається на три конкретні рядки тощо.

89
00:06:06,771 --> 00:06:09,847
Тепер, в ідеальному світі, ми можемо сподіватися,

90
00:06:09,847 --> 00:06:14,709
що кожен нейрон у передостанньому шарі відповідає одному з цих підкомпонентів,

91
00:06:14,709 --> 00:06:19,078
що кожного разу, коли ви подаєте зображення, скажімо, із петлею вгорі,

92
00:06:19,078 --> 00:06:23,755
як-от 9 чи 8, є деякі специфічний нейрон, чия активація буде близькою до 1.

93
00:06:23,755 --> 00:06:27,229
І я не маю на увазі цю конкретну петлю пікселів, я сподіваюся,

94
00:06:27,229 --> 00:06:31,751
що будь-який загалом петлевий візерунок у напрямку до вершини виділяє цей нейрон.

95
00:06:31,751 --> 00:06:35,270
Таким чином, щоб перейти від третього рівня до останнього,

96
00:06:35,270 --> 00:06:40,101
потрібно просто дізнатися, яка комбінація підкомпонентів відповідає яким цифрам.

97
00:06:40,101 --> 00:06:43,689
Звісно, це тільки підштовхує проблему, тому що як ви розпізнаєте ці

98
00:06:43,689 --> 00:06:47,805
підкомпоненти чи навіть дізнаєтеся, якими мають бути правильні підкомпоненти?

99
00:06:47,805 --> 00:06:51,288
І я ще навіть не говорив про те, як один шар впливає на наступний,

100
00:06:51,288 --> 00:06:52,900
але поговоримо про це на мить.

101
00:06:52,900 --> 00:06:56,650
Розпізнавання циклу також може розпадатися на підпроблеми.

102
00:06:56,650 --> 00:07:02,165
Одним із розумних способів зробити це було б спочатку розпізнати різні маленькі грані,

103
00:07:02,165 --> 00:07:03,433
які його складають.

104
00:07:03,433 --> 00:07:08,138
Подібним чином, довга лінія, подібна до тієї, яку ви можете побачити в цифрах 1,

105
00:07:08,138 --> 00:07:11,391
4 або 7, насправді це просто довга грань, або, можливо,

106
00:07:11,391 --> 00:07:15,341
ви думаєте про неї як про певний візерунок із кількох менших ребер.

107
00:07:15,341 --> 00:07:19,352
Тому, можливо, ми сподіваємося, що кожен нейрон у другому

108
00:07:19,352 --> 00:07:23,432
шарі мережі відповідає різним відповідним маленьким краям.

109
00:07:23,432 --> 00:07:27,792
Можливо, коли з’являється таке зображення, воно висвітлює всі нейрони,

110
00:07:27,792 --> 00:07:32,214
пов’язані з приблизно 8-10 певними маленькими краями, що, у свою чергу,

111
00:07:32,214 --> 00:07:37,004
висвітлює нейрони, пов’язані з верхньою петлею та довгою вертикальною лінією,

112
00:07:37,004 --> 00:07:39,768
а ті висвітлюють нейрон, пов&#39;язаний з 9.

113
00:07:39,768 --> 00:07:43,205
Інше питання, до якого я повернуся, як тільки ми побачимо,

114
00:07:43,205 --> 00:07:47,284
як навчити мережу, чи це те, що насправді робить наша остання мережа.

115
00:07:47,284 --> 00:07:52,439
Але це надія, яку ми могли б мати, свого роду мета з такою багатошаровою структурою.

116
00:07:52,439 --> 00:07:56,547
Крім того, ви можете собі уявити, як можливість виявлення країв і візерунків,

117
00:07:56,547 --> 00:08:00,444
як це, була б справді корисною для інших завдань розпізнавання зображень.

118
00:08:00,444 --> 00:08:04,338
І навіть за межами розпізнавання зображення, є всілякі інтелектуальні речі,

119
00:08:04,338 --> 00:08:07,412
які ви можете зробити, які розбиваються на шари абстракції.

120
00:08:07,412 --> 00:08:11,628
Розбір мовлення, наприклад, передбачає взяття необробленого аудіо та виділення

121
00:08:11,628 --> 00:08:15,683
чітких звуків, які поєднуються, щоб створити певні склади, які поєднуються,

122
00:08:15,683 --> 00:08:20,326
щоб утворити слова, які поєднуються, щоб скласти фрази та більш абстрактні думки тощо.

123
00:08:20,326 --> 00:08:24,821
Але повертаючись до того, як все це насправді працює, уявіть себе зараз,

124
00:08:24,821 --> 00:08:29,992
коли ви проектуєте, як саме активації на одному рівні можуть визначати активації на

125
00:08:29,992 --> 00:08:30,731
наступному.

126
00:08:30,731 --> 00:08:33,668
Мета полягає в тому, щоб мати якийсь механізм,

127
00:08:33,668 --> 00:08:38,917
який міг би поєднувати пікселі в краї, або краї в візерунки, або візерунки в цифри.

128
00:08:38,917 --> 00:08:44,631
І щоб збільшити масштаб на одному дуже конкретному прикладі, скажімо, ми сподіваємося,

129
00:08:44,631 --> 00:08:47,981
що один конкретний нейрон у другому шарі зрозуміє,

130
00:08:47,981 --> 00:08:50,739
чи має зображення перевагу в цій області.

131
00:08:50,739 --> 00:08:55,305
Виникає питання, які параметри повинна мати мережа?

132
00:08:55,305 --> 00:08:58,700
Які циферблати та ручки ви повинні мати можливість налаштувати,

133
00:08:58,700 --> 00:09:02,625
щоб вони були достатньо виразними, щоб потенційно захопити цей візерунок,

134
00:09:02,625 --> 00:09:05,648
або будь-який інший піксельний візерунок, або візерунок,

135
00:09:05,648 --> 00:09:08,247
який кілька країв можуть утворювати петлю, тощо?

136
00:09:08,247 --> 00:09:15,707
Що ж, ми призначимо вагу кожному з’єднанням між нашим нейроном і нейронами з першого шару.

137
00:09:15,707 --> 00:09:15,790


138
00:09:15,790 --> 00:09:17,797
Ці ваги - просто цифри.

139
00:09:17,797 --> 00:09:21,878
Потім візьміть усі ці активації з першого рівня та

140
00:09:21,878 --> 00:09:25,800
обчисліть їх зважену суму відповідно до цих ваг.

141
00:09:25,800 --> 00:09:30,436
Я вважаю корисним розглядати ці ваги як організовані у невелику власну сітку,

142
00:09:30,436 --> 00:09:34,893
і я збираюся використовувати зелені пікселі для позначення позитивних ваг,

143
00:09:34,893 --> 00:09:37,865
а червоні пікселі — для позначення від’ємних ваг,

144
00:09:37,865 --> 00:09:42,026
де яскравість цього пікселя є деякою вільне зображення значення ваги.

145
00:09:42,026 --> 00:09:45,861
Якщо ми зробили вагові коефіцієнти, пов’язані майже з усіма пікселями,

146
00:09:45,861 --> 00:09:49,913
нульовими, за винятком деяких додатних вагових коефіцієнтів у цій області,

147
00:09:49,913 --> 00:09:53,965
які нас цікавлять, тоді взяття зваженої суми всіх значень пікселів справді

148
00:09:53,965 --> 00:09:58,070
означає лише додавання значень пікселя просто в регіон, про який ми дбаємо.

149
00:09:58,070 --> 00:10:02,059
І якщо ви дійсно хочете зрозуміти, чи є тут перевага,

150
00:10:02,059 --> 00:10:07,451
ви можете зробити деякі від’ємні ваги, пов’язані з оточуючими пікселями.

151
00:10:07,451 --> 00:10:12,680
Тоді сума найбільша, коли ті середні пікселі яскраві, а оточуючі пікселі темніші.

152
00:10:12,680 --> 00:10:17,854
Коли ви обчислюєте таку зважену суму, ви можете отримати будь-яке число,

153
00:10:17,854 --> 00:10:23,596
але для цієї мережі ми хочемо, щоб активації були деякими значеннями від 0 до 1.

154
00:10:23,596 --> 00:10:27,861
Отже, звичайна річ – це закачати цю зважену суму в якусь функцію,

155
00:10:27,861 --> 00:10:31,546
яка розміщує дійсну числову лінію в діапазоні між 0 і 1.

156
00:10:31,546 --> 00:10:35,574
І звичайна функція, яка це робить, називається сигмоподібною функцією,

157
00:10:35,574 --> 00:10:37,560
також відомою як логістична крива.

158
00:10:37,560 --> 00:10:41,536
В основному дуже негативні вхідні дані закінчуються близькими до 0,

159
00:10:41,536 --> 00:10:44,811
дуже позитивні вхідні дані закінчуються близькими до 1,

160
00:10:44,811 --> 00:10:48,203
і вони просто постійно зростають навколо вхідних даних 0.

161
00:10:48,203 --> 00:10:53,155
Таким чином, активація нейрона тут є в основному показником того,

162
00:10:53,155 --> 00:10:56,757
наскільки позитивною є відповідна зважена сума.

163
00:10:56,757 --> 00:11:02,349
Але, можливо, ви не хочете, щоб нейрон світився, коли зважена сума більша за 0.

164
00:11:02,349 --> 00:11:06,838
Можливо, ви хочете, щоб він був активним лише тоді, коли сума перевищує, скажімо, 10.

165
00:11:06,838 --> 00:11:10,689
Тобто ви хочете, щоб він був неактивним.

166
00:11:10,689 --> 00:11:14,624
Тоді ми просто додамо якесь інше число, як-от мінус 10,

167
00:11:14,624 --> 00:11:20,668
до цієї зваженої суми перед тим, як підключити її до функції сигмоїдного здавлювання.

168
00:11:20,668 --> 00:11:23,395
Це додаткове число називається зміщенням.

169
00:11:23,395 --> 00:11:27,413
Отже, вагові коефіцієнти показують вам, який піксельний шаблон уловлює

170
00:11:27,413 --> 00:11:32,448
цей нейрон у другому шарі, а зсув говорить вам, наскільки високою має бути зважена сума,

171
00:11:32,448 --> 00:11:35,221
перш ніж нейрон почне ставати значно активнішим.

172
00:11:35,221 --> 00:11:37,328
І це лише один нейрон.

173
00:11:37,328 --> 00:11:44,015
Кожен інший нейрон цього шару буде з’єднаний з усіма 784 піксельними

174
00:11:44,015 --> 00:11:50,799
нейронами з першого шару, і кожне з цих 784 з’єднань має власну вагу.

175
00:11:50,799 --> 00:11:54,309
Крім того, у кожного з них є деяке зміщення, якесь інше число,

176
00:11:54,309 --> 00:11:57,986
яке ви додаєте до зваженої суми перед тим, як стиснути її сигмою.

177
00:11:57,986 --> 00:11:59,711
І це над чим подумати!

178
00:11:59,711 --> 00:12:03,712
З цим прихованим шаром із 16 нейронів це загалом

179
00:12:03,712 --> 00:12:07,877
784 помножити на 16 ваг разом із 16 упередженнями.

180
00:12:07,877 --> 00:12:12,139
І все це лише зв’язки від першого шару до другого.

181
00:12:12,139 --> 00:12:18,092
Зв’язки між іншими шарами також мають купу ваг і упереджень, пов’язаних з ними.

182
00:12:18,092 --> 00:12:24,071
Усе сказано та зроблено, ця мережа має майже рівно 13 000 загальних ваг і упереджень.

183
00:12:24,071 --> 00:12:28,252
13 000 ручок і циферблатів, які можна налаштовувати та повертати,

184
00:12:28,252 --> 00:12:30,532
щоб ця мережа працювала по-різному.

185
00:12:30,532 --> 00:12:34,333
Отже, коли ми говоримо про навчання, це стосується того,

186
00:12:34,333 --> 00:12:39,536
щоб змусити комп’ютер знайти дійсне налаштування для всіх цих багатьох чисел,

187
00:12:39,536 --> 00:12:42,003
щоб він фактично розв’язав проблему.

188
00:12:42,003 --> 00:12:47,136
Один уявний експеримент, який водночас веселий і жахливий, полягає в тому, щоб уявити,

189
00:12:47,136 --> 00:12:50,735
як ви сідаєте й вручну встановлюєте всі ці ваги та зміщення,

190
00:12:50,735 --> 00:12:54,570
цілеспрямовано змінюючи числа так, щоб другий шар підбирав краї,

191
00:12:54,570 --> 00:12:56,576
третій шар підбирав шаблони, тощо

192
00:12:56,576 --> 00:13:01,556
Особисто я вважаю це задовільним, а не ставлюся до мережі як до чорної скриньки,

193
00:13:01,556 --> 00:13:06,291
тому що коли мережа не працює так, як ви очікуєте, якщо ви трохи зрозумієте,

194
00:13:06,291 --> 00:13:09,242
що насправді означають ці ваги та упередження ,

195
00:13:09,242 --> 00:13:14,038
у вас є відправна точка для експериментів зі зміною структури для покращення.

196
00:13:14,038 --> 00:13:17,980
Або коли мережа працює, але не з тих причин, які ви могли б очікувати,

197
00:13:17,980 --> 00:13:20,645
копання в тому, що роблять ваги та упередження,

198
00:13:20,645 --> 00:13:24,864
є хорошим способом кинути виклик вашим припущенням і дійсно відкрити повний

199
00:13:24,864 --> 00:13:26,252
простір можливих рішень.

200
00:13:26,252 --> 00:13:32,234
До речі, фактичну функцію тут трохи громіздко записати, вам не здається?

201
00:13:32,234 --> 00:13:37,130
Тож дозвольте мені показати вам більш компактний спосіб представлення цих зв’язків.

202
00:13:37,130 --> 00:13:41,210
Ось як ви побачите це, якщо вирішите прочитати більше про нейронні мережі.

203
00:13:41,210 --> 00:13:46,288
Організуйте всі активації з одного шару в стовпець як вектор.

204
00:13:46,288 --> 00:13:51,897
Потім організуйте всі ваги як матрицю, де кожен рядок цієї матриці

205
00:13:51,897 --> 00:13:58,093
відповідає зв’язкам між одним шаром і певним нейроном на наступному шарі.

206
00:13:58,093 --> 00:14:03,774
Це означає, що взяття зваженої суми активацій у першому шарі відповідно до цих ваг

207
00:14:03,774 --> 00:14:09,865
відповідає одному з доданків у векторному добутку матриці всього, що ми маємо тут зліва.

208
00:14:09,865 --> 00:14:14,720
До речі, значна частина машинного навчання зводиться лише до гарного

209
00:14:14,720 --> 00:14:18,168
розуміння лінійної алгебри, тому будь-хто з вас,

210
00:14:18,168 --> 00:14:24,008
хто хоче добре візуально розуміти матриці та значення векторного множення матриць,

211
00:14:24,008 --> 00:14:29,004
погляньте на серію, яку я робив на лінійна алгебра, особливо розділ 3.

212
00:14:29,004 --> 00:14:33,608
Повертаючись до нашого виразу, замість того, щоб говорити про додавання зміщення

213
00:14:33,608 --> 00:14:36,848
до кожного з цих значень незалежно, ми представляємо це,

214
00:14:36,848 --> 00:14:41,225
організовуючи всі ці зміщення у вектор і додаючи весь вектор до попереднього

215
00:14:41,225 --> 00:14:42,816
векторного добутку матриці.

216
00:14:42,816 --> 00:14:47,030
Потім, як останній крок, я оберну сигмовид навколо зовнішнього боку,

217
00:14:47,030 --> 00:14:51,182
і це має означати те, що ви збираєтеся застосувати функцію сигмоіда

218
00:14:51,182 --> 00:14:55,274
до кожного конкретного компонента результуючого вектора всередині.

219
00:14:55,274 --> 00:15:00,353
Отже, як тільки ви запишете цю вагову матрицю та ці вектори як їхні власні символи,

220
00:15:00,353 --> 00:15:05,371
ви зможете повідомити про повний перехід активацій від одного шару до наступного в

221
00:15:05,371 --> 00:15:08,636
надзвичайно вузькому та акуратному маленькому виразі,

222
00:15:08,636 --> 00:15:12,687
і це зробить відповідний код набагато простішим і набагато швидше,

223
00:15:12,687 --> 00:15:16,133
оскільки багато бібліотек оптимізують матричне множення.

224
00:15:16,133 --> 00:15:21,400
Пам’ятаєте, як раніше я казав, що ці нейрони — це просто речі, які містять числа?

225
00:15:21,400 --> 00:15:26,912
Ну, звичайно, конкретні числа, які вони зберігають, залежать від зображення,

226
00:15:26,912 --> 00:15:32,282
яке ви подаєте, тому насправді точніше розглядати кожен нейрон як функцію,

227
00:15:32,282 --> 00:15:38,439
яка приймає вихідні дані всіх нейронів попереднього шару та викидає число від 0 до 1.

228
00:15:38,439 --> 00:15:42,674
Насправді вся мережа — це лише функція, яка приймає

229
00:15:42,674 --> 00:15:47,154
784 числа як вхідні дані та видає 10 чисел як вихідні.

230
00:15:47,154 --> 00:15:52,799
Це абсурдно складна функція, яка включає 13 000 параметрів у формі цих ваг і зміщень,

231
00:15:52,799 --> 00:15:58,051
які вловлюють певні шаблони, і яка включає ітерацію багатьох векторних добутків

232
00:15:58,051 --> 00:16:03,237
матриці та функцію сигмоїдного здавлювання, але це, тим не менш, лише функція.

233
00:16:03,237 --> 00:16:06,878
І те, що це виглядає складним, певною мірою заспокоює.

234
00:16:06,878 --> 00:16:10,023
Я маю на увазі, якби це було простіше, яка б у нас була надія,

235
00:16:10,023 --> 00:16:12,818
що воно зможе впоратися з проблемою розпізнавання цифр?

236
00:16:12,818 --> 00:16:14,920
І як він приймає цей виклик?

237
00:16:14,920 --> 00:16:19,320
Як ця мережа дізнається відповідні ваги та упередження, просто переглядаючи дані?

238
00:16:19,320 --> 00:16:24,031
Що ж, це те, що я покажу в наступному відео, а також трохи детальніше розберусь про те,

239
00:16:24,031 --> 00:16:26,227
що насправді робить ця конкретна мережа.

240
00:16:26,227 --> 00:16:30,744
Тепер я вважаю, що я повинен сказати, що підписатись, щоб отримувати сповіщення про те,

241
00:16:30,744 --> 00:16:33,208
коли з’явиться це відео чи будь-які нові відео,

242
00:16:33,208 --> 00:16:37,622
але насправді більшість із вас насправді не отримує сповіщень від YouTube, чи не так?

243
00:16:37,622 --> 00:16:40,797
Можливо, відверто кажучи, я повинен сказати, що підписуйтесь,

244
00:16:40,797 --> 00:16:44,536
щоб нейронні мережі, які лежать в основі алгоритму рекомендацій YouTube,

245
00:16:44,536 --> 00:16:48,275
переконалися, що ви хочете, щоб вам рекомендували вміст із цього каналу.

246
00:16:48,275 --> 00:16:49,800
У будь-якому разі залишайтеся в курсі, щоб дізнатися більше.

247
00:16:49,800 --> 00:16:53,634
Велике спасибі всім, хто підтримує ці відео на Patreon.

248
00:16:53,634 --> 00:16:56,997
Я трохи повільно просувався в серії ймовірностей цього літа,

249
00:16:56,997 --> 00:17:00,305
але я повертаюся до цього після цього проекту, тож патрони,

250
00:17:00,305 --> 00:17:02,400
ви можете стежити за оновленнями там.

251
00:17:02,400 --> 00:17:06,671
Щоб закінчити, зі мною є Ліша Лі, яка захистила докторську роботу з теоретичної

252
00:17:06,671 --> 00:17:11,423
сторони глибокого навчання та зараз працює у фірмі венчурного капіталу Amplify Partners,

253
00:17:11,423 --> 00:17:14,520
яка люб’язно надала частину фінансування для цього відео.

254
00:17:14,520 --> 00:17:19,480
Отже, Ліша, я вважаю, що ми повинні швидко згадати одну річ, це цю сигмоподібну функцію.

255
00:17:19,480 --> 00:17:22,027
Наскільки я розумію, ранні мережі використовували це,

256
00:17:22,027 --> 00:17:25,330
щоб стиснути відповідну зважену суму в інтервал між нулем і одиницею,

257
00:17:25,330 --> 00:17:28,538
ви знаєте, начебто вмотивовані цією біологічною аналогією нейронів,

258
00:17:28,538 --> 00:17:30,048
які або неактивні, або активні.

259
00:17:30,048 --> 00:17:30,552
Точно.

260
00:17:30,552 --> 00:17:34,091
Але порівняно небагато сучасних мереж фактично використовують sigmoid.

261
00:17:34,091 --> 00:17:34,392
так

262
00:17:34,392 --> 00:17:35,945
Це стара школа, чи не так?

263
00:17:35,945 --> 00:17:38,974
Так, точніше relu, здається, набагато легше тренувати.

264
00:17:38,974 --> 00:17:42,360
А relu, relu означає випрямлену лінійну одиницю?

265
00:17:42,360 --> 00:17:47,915
Так, це така функція, де ви просто берете максимальне значення нуль і a,

266
00:17:47,915 --> 00:17:51,187
де a задано тим, що ви пояснювали у відео.

267
00:17:51,187 --> 00:17:57,867
Я вважаю, що це було частково мотивовано біологічною аналогією з тим,

268
00:17:57,867 --> 00:18:00,730
як нейрони активуються чи ні.

269
00:18:00,730 --> 00:18:05,149
Отже, якщо він перевищить певний поріг, це буде функція ідентифікації,

270
00:18:05,149 --> 00:18:09,380
але якщо ні, то вона просто не буде активована, тому буде нульовою.

271
00:18:09,380 --> 00:18:11,084
Так що це якесь спрощення.

272
00:18:11,084 --> 00:18:15,250
Використання сигмовидів не допомогло в навчанні або в якийсь момент

273
00:18:15,250 --> 00:18:19,600
було дуже важко тренуватися, і люди просто спробували relu, і сталося,

274
00:18:19,600 --> 00:18:24,072
що це дуже добре спрацювало для цих неймовірно глибоких нейронних мереж.

275
00:18:24,072 --> 00:18:26,120
Гаразд, дякую Ліша.

