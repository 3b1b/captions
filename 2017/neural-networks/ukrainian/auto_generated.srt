1
00:00:04,219 --> 00:00:05,400
Це 3. 

2
00:00:06,060 --> 00:00:09,836
Він неохайно написаний і відтворений із надзвичайно низькою роздільною 

3
00:00:09,836 --> 00:00:13,720
здатністю 28x28 пікселів, але ваш мозок без проблем розпізнає його як 3. 

4
00:00:14,340 --> 00:00:16,412
І я хочу, щоб ви знайшли хвилинку, щоб оцінити, 

5
00:00:16,412 --> 00:00:18,960
наскільки це божевілля, що мозок може робити це так легко. 

6
00:00:19,700 --> 00:00:22,844
Я маю на увазі, що це, це і це також розпізнаються як 3s, 

7
00:00:22,844 --> 00:00:27,127
навіть якщо конкретні значення кожного пікселя сильно відрізняються від одного 

8
00:00:27,127 --> 00:00:28,320
зображення до іншого. 

9
00:00:28,900 --> 00:00:33,425
Конкретні світлочутливі клітини у вашому оці, які спрацьовують, коли ви бачите це 3, 

10
00:00:33,425 --> 00:00:36,940
дуже відрізняються від тих, що спрацьовують, коли ви бачите це 3. 

11
00:00:37,520 --> 00:00:41,919
Але щось у вашій шалено-розумній зоровій корі розпізнає їх як такі, 

12
00:00:41,919 --> 00:00:47,418
що представляють одну і ту саму ідею, водночас розпізнаючи інші зображення як власні 

13
00:00:47,418 --> 00:00:48,260
окремі ідеї. 

14
00:00:49,220 --> 00:00:53,566
Але якби я сказав тобі, сядьте і напишіть для мене програму, 

15
00:00:53,566 --> 00:00:59,125
яка приймає сітку 28x28 і виводить єдине число від 0 до 10, повідомляючи вам, 

16
00:00:59,125 --> 00:01:04,897
якою, на її думку, є ця цифра, то завдання перетворюється з комічно тривіального 

17
00:01:04,897 --> 00:01:06,180
на страшно важко. 

18
00:01:07,160 --> 00:01:11,053
Якщо ви не жили під каменем, я думаю, що мені навряд чи потрібно мотивувати актуальність 

19
00:01:11,053 --> 00:01:14,640
і важливість машинного навчання та нейронних мереж для сьогодення та майбутнього. 

20
00:01:15,120 --> 00:01:18,223
Але я хочу тут показати вам, що насправді таке нейронна мережа, 

21
00:01:18,223 --> 00:01:21,278
не припускаючи жодної передісторії, і допомогти візуалізувати, 

22
00:01:21,278 --> 00:01:24,140
що вона робить, не як модне слово, а як шматок математики. 

23
00:01:24,140 --> 00:01:29,182
Я сподіваюся, що ви відчуєте, що сама структура вмотивована, і ви відчуєте, що знаєте, 

24
00:01:29,182 --> 00:01:34,340
що це означає, коли ви читаєте або чуєте про нейронну мережу, яка вивчає цитати-розриви. 

25
00:01:35,360 --> 00:01:40,260
Це відео буде присвячено лише структурному компоненту, а наступне – навчанню. 

26
00:01:40,960 --> 00:01:46,040
Ми збираємося створити нейронну мережу, яка навчиться розпізнавати рукописні цифри. 

27
00:01:49,360 --> 00:01:53,619
Це дещо класичний приклад представлення теми, і я радий дотримуватися статус-кво тут, 

28
00:01:53,619 --> 00:01:57,284
тому що в кінці двох відео я хочу вказати вам на кілька хороших ресурсів, 

29
00:01:57,284 --> 00:02:00,405
де ви можете дізнатися більше, і де ви можете завантажити код, 

30
00:02:00,405 --> 00:02:03,080
який це робить, і пограти з ним на своєму комп’ютері. 

31
00:02:05,040 --> 00:02:08,082
Існує багато різноманітних варіантів нейронних мереж, 

32
00:02:08,082 --> 00:02:12,194
і останніми роками спостерігався певний бум у дослідженні цих варіантів, 

33
00:02:12,194 --> 00:02:16,813
але в цих двох вступних відео ми з вами просто подивимося на найпростішу звичайну 

34
00:02:16,813 --> 00:02:19,180
ванільну форму без додаткових вишукувань. 

35
00:02:19,860 --> 00:02:24,514
Це начебто необхідна передумова для розуміння будь-якого з потужніших сучасних варіантів, 

36
00:02:24,514 --> 00:02:28,600
і, повірте мені, у нас все ще є багато складності, про яку ми можемо подумати. 

37
00:02:29,120 --> 00:02:34,589
Але навіть у цій найпростішій формі він може навчитися розпізнавати рукописні цифри, 

38
00:02:34,589 --> 00:02:36,520
що для комп’ютера дуже круто. 

39
00:02:37,480 --> 00:02:40,631
І в той же час ви побачите, як він не виправдовує кількох надій, 

40
00:02:40,631 --> 00:02:42,280
які ми можемо покладати на нього. 

41
00:02:43,380 --> 00:02:48,500
Як випливає з назви, нейронні мережі надихає мозок, але давайте розберемо це. 

42
00:02:48,520 --> 00:02:51,660
Що таке нейрони і в якому сенсі вони пов’язані між собою? 

43
00:02:52,500 --> 00:02:56,903
Зараз, коли я кажу про нейрон, все, про що я хочу, щоб ви думали, 

44
00:02:56,903 --> 00:03:00,440
це річ, яка містить число, зокрема число від 0 до 1. 

45
00:03:00,680 --> 00:03:02,000
Насправді це не більше того. 

46
00:03:02,000 --> 00:03:06,189
Наприклад, мережа починається з групи нейронів, 

47
00:03:06,189 --> 00:03:11,863
що відповідають кожному з 28 по 28 пікселів вхідного зображення, 

48
00:03:11,863 --> 00:03:14,220
тобто загалом 784 нейрони. 

49
00:03:14,700 --> 00:03:19,858
Кожне з них містить число, яке представляє значення градацій сірого відповідного 

50
00:03:19,858 --> 00:03:24,380
пікселя в діапазоні від 0 для чорних пікселів до 1 для білих пікселів. 

51
00:03:25,300 --> 00:03:28,895
Це число всередині нейрона називається його активацією, 

52
00:03:28,895 --> 00:03:34,160
і ви можете мати на увазі, що кожен нейрон світиться, коли його активація висока. 

53
00:03:36,720 --> 00:03:41,860
Отже, усі ці 784 нейрони складають перший рівень нашої мережі. 

54
00:03:46,500 --> 00:03:49,330
Переходячи до останнього шару, ми маємо 10 нейронів, 

55
00:03:49,330 --> 00:03:51,360
кожен з яких представляє одну з цифр. 

56
00:03:52,040 --> 00:03:56,929
Активація в цих нейронах, знову якесь число від 0 до 1, показує, 

57
00:03:56,929 --> 00:04:02,120
наскільки система вважає, що дане зображення відповідає даній цифрі. 

58
00:04:03,040 --> 00:04:06,850
Існує також пара проміжних шарів, які називаються прихованими шарами, 

59
00:04:06,850 --> 00:04:10,878
які на даний момент повинні бути просто величезним знаком питання про те, 

60
00:04:10,878 --> 00:04:13,600
як цей процес розпізнавання цифр відбуватиметься. 

61
00:04:14,260 --> 00:04:18,751
У цій мережі я вибрав два прихованих шари, кожен з яких містить 16 нейронів, 

62
00:04:18,751 --> 00:04:20,560
і, правда, це довільний вибір. 

63
00:04:21,019 --> 00:04:25,299
Чесно кажучи, я вибрав два шари, виходячи з того, як я хочу мотивувати структуру за мить, 

64
00:04:25,299 --> 00:04:28,200
і 16, це було просто гарне число, щоб поміститися на екрані. 

65
00:04:28,780 --> 00:04:32,340
На практиці тут є багато можливостей для експериментів із конкретною структурою. 

66
00:04:33,020 --> 00:04:38,480
Спосіб роботи мережі, активація на одному рівні визначає активацію наступного рівня. 

67
00:04:39,200 --> 00:04:43,740
І, звичайно, суть мережі як механізму обробки інформації зводиться до того, 

68
00:04:43,740 --> 00:04:48,580
як саме ці активації з одного рівня призводять до активацій на наступному рівні. 

69
00:04:49,140 --> 00:04:53,053
Це приблизно аналогічно тому, як у біологічних мережах 

70
00:04:53,053 --> 00:04:57,180
нейронів деякі групи нейронів викликають активацію інших. 

71
00:04:58,120 --> 00:05:00,910
Тепер мережа, яку я тут показую, уже навчена розпізнавати цифри, 

72
00:05:00,910 --> 00:05:03,400
і дозвольте мені показати вам, що я маю на увазі під цим. 

73
00:05:03,640 --> 00:05:08,351
Це означає, що якщо ви подаєте зображення, яке освітлює всі 784 нейрони вхідного 

74
00:05:08,351 --> 00:05:11,900
шару відповідно до яскравості кожного пікселя на зображенні, 

75
00:05:11,900 --> 00:05:16,204
цей шаблон активації спричиняє дуже специфічний шаблон у наступному шарі, 

76
00:05:16,204 --> 00:05:19,171
який викликає певний шаблон у наступному шарі. це, 

77
00:05:19,171 --> 00:05:22,080
що нарешті дає певний візерунок у вихідному шарі. 

78
00:05:22,560 --> 00:05:26,307
І найяскравішим нейроном цього вихідного рівня є вибір мережі, 

79
00:05:26,307 --> 00:05:29,400
так би мовити, яку цифру представляє це зображення. 

80
00:05:32,560 --> 00:05:36,376
І перш ніж перейти до математики щодо того, як один шар впливає на наступний, 

81
00:05:36,376 --> 00:05:39,214
або як працює навчання, давайте просто поговоримо про те, 

82
00:05:39,214 --> 00:05:43,520
чому взагалі розумно очікувати, що така багатошарова структура буде поводитися розумно. 

83
00:05:44,060 --> 00:05:45,220
Чого ми тут очікуємо? 

84
00:05:45,400 --> 00:05:47,600
Яка найкраща надія на те, що можуть робити ці середні верстви? 

85
00:05:48,920 --> 00:05:53,520
Що ж, коли ви чи я розпізнаємо цифри, ми збираємо разом різні компоненти. 

86
00:05:54,200 --> 00:05:56,820
9 має петлю вгорі та лінію праворуч. 

87
00:05:57,380 --> 00:06:01,180
8 також має петлю зверху, але вона поєднана з іншою петлею внизу. 

88
00:06:01,980 --> 00:06:06,820
4 в основному розбивається на три конкретні рядки тощо. 

89
00:06:07,600 --> 00:06:10,531
Тепер, в ідеальному світі, ми можемо сподіватися, 

90
00:06:10,531 --> 00:06:15,162
що кожен нейрон у передостанньому шарі відповідає одному з цих підкомпонентів, 

91
00:06:15,162 --> 00:06:19,324
що кожного разу, коли ви подаєте зображення, скажімо, із петлею вгорі, 

92
00:06:19,324 --> 00:06:23,780
як-от 9 чи 8, є деякі специфічний нейрон, чия активація буде близькою до 1. 

93
00:06:24,500 --> 00:06:27,567
І я не маю на увазі цю конкретну петлю пікселів, я сподіваюся, 

94
00:06:27,567 --> 00:06:31,560
що будь-який загалом петлевий візерунок у напрямку до вершини виділяє цей нейрон. 

95
00:06:32,440 --> 00:06:35,642
Таким чином, щоб перейти від третього рівня до останнього, 

96
00:06:35,642 --> 00:06:40,040
потрібно просто дізнатися, яка комбінація підкомпонентів відповідає яким цифрам. 

97
00:06:41,000 --> 00:06:44,092
Звісно, це тільки підштовхує проблему, тому що як ви розпізнаєте ці 

98
00:06:44,092 --> 00:06:47,640
підкомпоненти чи навіть дізнаєтеся, якими мають бути правильні підкомпоненти? 

99
00:06:48,060 --> 00:06:51,478
І я ще навіть не говорив про те, як один шар впливає на наступний, 

100
00:06:51,478 --> 00:06:53,060
але поговоримо про це на мить. 

101
00:06:53,680 --> 00:06:56,680
Розпізнавання циклу також може розпадатися на підпроблеми. 

102
00:06:57,280 --> 00:07:01,345
Одним із розумних способів зробити це було б спочатку розпізнати різні маленькі грані, 

103
00:07:01,345 --> 00:07:02,280
які його складають. 

104
00:07:02,280 --> 00:07:07,037
Подібним чином, довга лінія, подібна до тієї, яку ви можете побачити в цифрах 1, 

105
00:07:07,037 --> 00:07:10,326
4 або 7, насправді це просто довга грань, або, можливо, 

106
00:07:10,326 --> 00:07:14,320
ви думаєте про неї як про певний візерунок із кількох менших ребер. 

107
00:07:15,140 --> 00:07:18,897
Тому, можливо, ми сподіваємося, що кожен нейрон у другому 

108
00:07:18,897 --> 00:07:22,720
шарі мережі відповідає різним відповідним маленьким краям. 

109
00:07:23,540 --> 00:07:27,858
Можливо, коли з’являється таке зображення, воно висвітлює всі нейрони, 

110
00:07:27,858 --> 00:07:32,238
пов’язані з приблизно 8-10 певними маленькими краями, що, у свою чергу, 

111
00:07:32,238 --> 00:07:36,982
висвітлює нейрони, пов’язані з верхньою петлею та довгою вертикальною лінією, 

112
00:07:36,982 --> 00:07:39,720
а ті висвітлюють нейрон, пов'язаний з 9. 

113
00:07:40,680 --> 00:07:43,652
Інше питання, до якого я повернуся, як тільки ми побачимо, 

114
00:07:43,652 --> 00:07:47,180
як навчити мережу, чи це те, що насправді робить наша остання мережа. 

115
00:07:47,500 --> 00:07:52,540
Але це надія, яку ми могли б мати, свого роду мета з такою багатошаровою структурою. 

116
00:07:53,160 --> 00:07:56,823
Крім того, ви можете собі уявити, як можливість виявлення країв і візерунків, 

117
00:07:56,823 --> 00:08:00,300
як це, була б справді корисною для інших завдань розпізнавання зображень. 

118
00:08:00,880 --> 00:08:04,456
І навіть за межами розпізнавання зображення, є всілякі інтелектуальні речі, 

119
00:08:04,456 --> 00:08:07,280
які ви можете зробити, які розбиваються на шари абстракції. 

120
00:08:08,040 --> 00:08:11,963
Розбір мовлення, наприклад, передбачає взяття необробленого аудіо та виділення 

121
00:08:11,963 --> 00:08:15,738
чітких звуків, які поєднуються, щоб створити певні склади, які поєднуються, 

122
00:08:15,738 --> 00:08:20,060
щоб утворити слова, які поєднуються, щоб скласти фрази та більш абстрактні думки тощо. 

123
00:08:21,100 --> 00:08:24,909
Але повертаючись до того, як все це насправді працює, уявіть себе зараз, 

124
00:08:24,909 --> 00:08:29,293
коли ви проектуєте, як саме активації на одному рівні можуть визначати активації на 

125
00:08:29,293 --> 00:08:29,920
наступному. 

126
00:08:30,860 --> 00:08:33,773
Мета полягає в тому, щоб мати якийсь механізм, 

127
00:08:33,773 --> 00:08:38,980
який міг би поєднувати пікселі в краї, або краї в візерунки, або візерунки в цифри. 

128
00:08:39,440 --> 00:08:44,843
І щоб збільшити масштаб на одному дуже конкретному прикладі, скажімо, ми сподіваємося, 

129
00:08:44,843 --> 00:08:48,011
що один конкретний нейрон у другому шарі зрозуміє, 

130
00:08:48,011 --> 00:08:50,620
чи має зображення перевагу в цій області. 

131
00:08:51,440 --> 00:08:55,100
Виникає питання, які параметри повинна мати мережа? 

132
00:08:55,640 --> 00:08:58,824
Які циферблати та ручки ви повинні мати можливість налаштувати, 

133
00:08:58,824 --> 00:09:02,506
щоб вони були достатньо виразними, щоб потенційно захопити цей візерунок, 

134
00:09:02,506 --> 00:09:05,342
або будь-який інший піксельний візерунок, або візерунок, 

135
00:09:05,342 --> 00:09:07,780
який кілька країв можуть утворювати петлю, тощо? 

136
00:09:08,720 --> 00:09:15,484
Що ж, ми призначимо вагу кожному з’єднанням між нашим нейроном і нейронами з першого шару.

137
00:09:15,484 --> 00:09:15,560
 

138
00:09:16,320 --> 00:09:17,700
Ці ваги - просто цифри. 

139
00:09:18,540 --> 00:09:22,089
Потім візьміть усі ці активації з першого рівня та 

140
00:09:22,089 --> 00:09:25,500
обчисліть їх зважену суму відповідно до цих ваг. 

141
00:09:27,700 --> 00:09:32,054
Я вважаю корисним розглядати ці ваги як організовані у невелику власну сітку, 

142
00:09:32,054 --> 00:09:36,241
і я збираюся використовувати зелені пікселі для позначення позитивних ваг, 

143
00:09:36,241 --> 00:09:39,032
а червоні пікселі — для позначення від’ємних ваг, 

144
00:09:39,032 --> 00:09:42,940
де яскравість цього пікселя є деякою вільне зображення значення ваги. 

145
00:09:42,940 --> 00:09:46,497
Якщо ми зробили вагові коефіцієнти, пов’язані майже з усіма пікселями, 

146
00:09:46,497 --> 00:09:50,254
нульовими, за винятком деяких додатних вагових коефіцієнтів у цій області, 

147
00:09:50,254 --> 00:09:54,012
які нас цікавлять, тоді взяття зваженої суми всіх значень пікселів справді 

148
00:09:54,012 --> 00:09:57,820
означає лише додавання значень пікселя просто в регіон, про який ми дбаємо. 

149
00:09:59,140 --> 00:10:02,311
І якщо ви дійсно хочете зрозуміти, чи є тут перевага, 

150
00:10:02,311 --> 00:10:06,600
ви можете зробити деякі від’ємні ваги, пов’язані з оточуючими пікселями. 

151
00:10:07,480 --> 00:10:12,700
Тоді сума найбільша, коли ті середні пікселі яскраві, а оточуючі пікселі темніші. 

152
00:10:14,260 --> 00:10:18,658
Коли ви обчислюєте таку зважену суму, ви можете отримати будь-яке число, 

153
00:10:18,658 --> 00:10:23,540
але для цієї мережі ми хочемо, щоб активації були деякими значеннями від 0 до 1. 

154
00:10:24,120 --> 00:10:28,423
Отже, звичайна річ – це закачати цю зважену суму в якусь функцію, 

155
00:10:28,423 --> 00:10:32,140
яка розміщує дійсну числову лінію в діапазоні між 0 і 1. 

156
00:10:32,460 --> 00:10:35,487
І звичайна функція, яка це робить, називається сигмоподібною функцією, 

157
00:10:35,487 --> 00:10:36,980
також відомою як логістична крива. 

158
00:10:36,980 --> 00:10:40,574
В основному дуже негативні вхідні дані закінчуються близькими до 0, 

159
00:10:40,574 --> 00:10:43,534
дуже позитивні вхідні дані закінчуються близькими до 1, 

160
00:10:43,534 --> 00:10:46,600
і вони просто постійно зростають навколо вхідних даних 0. 

161
00:10:49,120 --> 00:10:53,311
Таким чином, активація нейрона тут є в основному показником того, 

162
00:10:53,311 --> 00:10:56,360
наскільки позитивною є відповідна зважена сума. 

163
00:10:57,540 --> 00:11:01,880
Але, можливо, ви не хочете, щоб нейрон світився, коли зважена сума більша за 0. 

164
00:11:02,280 --> 00:11:06,360
Можливо, ви хочете, щоб він був активним лише тоді, коли сума перевищує, скажімо, 10. 

165
00:11:06,840 --> 00:11:10,260
Тобто ви хочете, щоб він був неактивним. 

166
00:11:11,380 --> 00:11:14,645
Тоді ми просто додамо якесь інше число, як-от мінус 10, 

167
00:11:14,645 --> 00:11:19,660
до цієї зваженої суми перед тим, як підключити її до функції сигмоїдного здавлювання. 

168
00:11:20,580 --> 00:11:22,440
Це додаткове число називається зміщенням. 

169
00:11:23,460 --> 00:11:27,441
Отже, вагові коефіцієнти показують вам, який піксельний шаблон уловлює 

170
00:11:27,441 --> 00:11:32,432
цей нейрон у другому шарі, а зсув говорить вам, наскільки високою має бути зважена сума, 

171
00:11:32,432 --> 00:11:35,180
перш ніж нейрон почне ставати значно активнішим. 

172
00:11:36,120 --> 00:11:37,680
І це лише один нейрон. 

173
00:11:38,280 --> 00:11:44,564
Кожен інший нейрон цього шару буде з’єднаний з усіма 784 піксельними 

174
00:11:44,564 --> 00:11:50,940
нейронами з першого шару, і кожне з цих 784 з’єднань має власну вагу. 

175
00:11:51,600 --> 00:11:54,530
Крім того, у кожного з них є деяке зміщення, якесь інше число, 

176
00:11:54,530 --> 00:11:57,600
яке ви додаєте до зваженої суми перед тим, як стиснути її сигмою. 

177
00:11:58,110 --> 00:11:59,540
І це над чим подумати! 

178
00:11:59,960 --> 00:12:03,889
З цим прихованим шаром із 16 нейронів це загалом 

179
00:12:03,889 --> 00:12:07,980
784 помножити на 16 ваг разом із 16 упередженнями. 

180
00:12:08,840 --> 00:12:11,940
І все це лише зв’язки від першого шару до другого. 

181
00:12:12,520 --> 00:12:17,340
Зв’язки між іншими шарами також мають купу ваг і упереджень, пов’язаних з ними. 

182
00:12:18,340 --> 00:12:23,800
Усе сказано та зроблено, ця мережа має майже рівно 13 000 загальних ваг і упереджень. 

183
00:12:23,800 --> 00:12:27,785
13 000 ручок і циферблатів, які можна налаштовувати та повертати, 

184
00:12:27,785 --> 00:12:29,960
щоб ця мережа працювала по-різному. 

185
00:12:31,040 --> 00:12:34,460
Отже, коли ми говоримо про навчання, це стосується того, 

186
00:12:34,460 --> 00:12:39,140
щоб змусити комп’ютер знайти дійсне налаштування для всіх цих багатьох чисел, 

187
00:12:39,140 --> 00:12:41,360
щоб він фактично розв’язав проблему. 

188
00:12:42,620 --> 00:12:47,537
Один уявний експеримент, який водночас веселий і жахливий, полягає в тому, щоб уявити, 

189
00:12:47,537 --> 00:12:50,984
як ви сідаєте й вручну встановлюєте всі ці ваги та зміщення, 

190
00:12:50,984 --> 00:12:54,658
цілеспрямовано змінюючи числа так, щоб другий шар підбирав краї, 

191
00:12:54,658 --> 00:12:56,580
третій шар підбирав шаблони, тощо 

192
00:12:56,980 --> 00:13:01,885
Особисто я вважаю це задовільним, а не ставлюся до мережі як до чорної скриньки, 

193
00:13:01,885 --> 00:13:06,549
тому що коли мережа не працює так, як ви очікуєте, якщо ви трохи зрозумієте, 

194
00:13:06,549 --> 00:13:09,456
що насправді означають ці ваги та упередження , 

195
00:13:09,456 --> 00:13:14,180
у вас є відправна точка для експериментів зі зміною структури для покращення. 

196
00:13:14,960 --> 00:13:18,464
Або коли мережа працює, але не з тих причин, які ви могли б очікувати, 

197
00:13:18,464 --> 00:13:20,834
копання в тому, що роблять ваги та упередження, 

198
00:13:20,834 --> 00:13:24,585
є хорошим способом кинути виклик вашим припущенням і дійсно відкрити повний 

199
00:13:24,585 --> 00:13:25,820
простір можливих рішень. 

200
00:13:26,840 --> 00:13:30,680
До речі, фактичну функцію тут трохи громіздко записати, вам не здається? 

201
00:13:32,500 --> 00:13:37,140
Тож дозвольте мені показати вам більш компактний спосіб представлення цих зв’язків. 

202
00:13:37,660 --> 00:13:40,520
Ось як ви побачите це, якщо вирішите прочитати більше про нейронні мережі. 

203
00:13:41,380 --> 00:13:44,780
Організуйте всі активації з одного шару в стовпець як вектор. 

204
00:13:44,780 --> 00:13:52,002
Потім організуйте всі ваги як матрицю, де кожен рядок цієї матриці 

205
00:13:52,002 --> 00:13:59,980
відповідає зв’язкам між одним шаром і певним нейроном на наступному шарі. 

206
00:13:59,980 --> 00:14:07,121
Це означає, що взяття зваженої суми активацій у першому шарі відповідно до цих ваг 

207
00:14:07,121 --> 00:14:14,780
відповідає одному з доданків у векторному добутку матриці всього, що ми маємо тут зліва. 

208
00:14:14,780 --> 00:14:18,285
До речі, значна частина машинного навчання зводиться лише до гарного 

209
00:14:18,285 --> 00:14:20,775
розуміння лінійної алгебри, тому будь-хто з вас, 

210
00:14:20,775 --> 00:14:24,992
хто хоче добре візуально розуміти матриці та значення векторного множення матриць, 

211
00:14:24,992 --> 00:14:28,600
погляньте на серію, яку я робив на лінійна алгебра, особливо розділ 3. 

212
00:14:29,240 --> 00:14:33,593
Повертаючись до нашого виразу, замість того, щоб говорити про додавання зміщення 

213
00:14:33,593 --> 00:14:36,656
до кожного з цих значень незалежно, ми представляємо це, 

214
00:14:36,656 --> 00:14:40,795
організовуючи всі ці зміщення у вектор і додаючи весь вектор до попереднього 

215
00:14:40,795 --> 00:14:42,300
векторного добутку матриці. 

216
00:14:43,280 --> 00:14:47,156
Потім, як останній крок, я оберну сигмовид навколо зовнішнього боку, 

217
00:14:47,156 --> 00:14:50,976
і це має означати те, що ви збираєтеся застосувати функцію сигмоіда 

218
00:14:50,976 --> 00:14:54,740
до кожного конкретного компонента результуючого вектора всередині. 

219
00:14:55,940 --> 00:15:00,741
Отже, як тільки ви запишете цю вагову матрицю та ці вектори як їхні власні символи, 

220
00:15:00,741 --> 00:15:05,485
ви зможете повідомити про повний перехід активацій від одного шару до наступного в 

221
00:15:05,485 --> 00:15:08,572
надзвичайно вузькому та акуратному маленькому виразі, 

222
00:15:08,572 --> 00:15:12,401
і це зробить відповідний код набагато простішим і набагато швидше, 

223
00:15:12,401 --> 00:15:15,660
оскільки багато бібліотек оптимізують матричне множення. 

224
00:15:17,820 --> 00:15:21,460
Пам’ятаєте, як раніше я казав, що ці нейрони — це просто речі, які містять числа? 

225
00:15:22,220 --> 00:15:27,435
Ну, звичайно, конкретні числа, які вони зберігають, залежать від зображення, 

226
00:15:27,435 --> 00:15:32,515
яке ви подаєте, тому насправді точніше розглядати кожен нейрон як функцію, 

227
00:15:32,515 --> 00:15:38,340
яка приймає вихідні дані всіх нейронів попереднього шару та викидає число від 0 до 1. 

228
00:15:39,200 --> 00:15:43,019
Насправді вся мережа — це лише функція, яка приймає 

229
00:15:43,019 --> 00:15:47,060
784 числа як вхідні дані та видає 10 чисел як вихідні. 

230
00:15:47,560 --> 00:15:52,853
Це абсурдно складна функція, яка включає 13 000 параметрів у формі цих ваг і зміщень, 

231
00:15:52,853 --> 00:15:57,777
які вловлюють певні шаблони, і яка включає ітерацію багатьох векторних добутків 

232
00:15:57,777 --> 00:16:02,640
матриці та функцію сигмоїдного здавлювання, але це, тим не менш, лише функція. 

233
00:16:03,400 --> 00:16:06,660
І те, що це виглядає складним, певною мірою заспокоює. 

234
00:16:07,340 --> 00:16:09,955
Я маю на увазі, якби це було простіше, яка б у нас була надія, 

235
00:16:09,955 --> 00:16:12,280
що воно зможе впоратися з проблемою розпізнавання цифр? 

236
00:16:13,340 --> 00:16:14,700
І як він приймає цей виклик? 

237
00:16:15,080 --> 00:16:19,360
Як ця мережа дізнається відповідні ваги та упередження, просто переглядаючи дані? 

238
00:16:20,140 --> 00:16:23,960
Що ж, це те, що я покажу в наступному відео, а також трохи детальніше розберусь про те, 

239
00:16:23,960 --> 00:16:25,740
що насправді робить ця конкретна мережа. 

240
00:16:25,740 --> 00:16:30,369
Тепер я вважаю, що я повинен сказати, що підписатись, щоб отримувати сповіщення про те, 

241
00:16:30,369 --> 00:16:32,895
коли з’явиться це відео чи будь-які нові відео, 

242
00:16:32,895 --> 00:16:37,420
але насправді більшість із вас насправді не отримує сповіщень від YouTube, чи не так? 

243
00:16:38,020 --> 00:16:40,959
Можливо, відверто кажучи, я повинен сказати, що підписуйтесь, 

244
00:16:40,959 --> 00:16:44,419
щоб нейронні мережі, які лежать в основі алгоритму рекомендацій YouTube, 

245
00:16:44,419 --> 00:16:47,880
переконалися, що ви хочете, щоб вам рекомендували вміст із цього каналу. 

246
00:16:48,560 --> 00:16:49,940
У будь-якому разі залишайтеся в курсі, щоб дізнатися більше. 

247
00:16:50,760 --> 00:16:53,500
Велике спасибі всім, хто підтримує ці відео на Patreon. 

248
00:16:54,000 --> 00:16:57,030
Я трохи повільно просувався в серії ймовірностей цього літа, 

249
00:16:57,030 --> 00:17:00,011
але я повертаюся до цього після цього проекту, тож патрони, 

250
00:17:00,011 --> 00:17:01,900
ви можете стежити за оновленнями там. 

251
00:17:03,600 --> 00:17:07,483
Щоб закінчити, зі мною є Ліша Лі, яка захистила докторську роботу з теоретичної 

252
00:17:07,483 --> 00:17:11,804
сторони глибокого навчання та зараз працює у фірмі венчурного капіталу Amplify Partners, 

253
00:17:11,804 --> 00:17:14,619
яка люб’язно надала частину фінансування для цього відео. 

254
00:17:15,460 --> 00:17:19,119
Отже, Ліша, я вважаю, що ми повинні швидко згадати одну річ, це цю сигмоподібну функцію. 

255
00:17:19,700 --> 00:17:22,144
Наскільки я розумію, ранні мережі використовували це, 

256
00:17:22,144 --> 00:17:25,313
щоб стиснути відповідну зважену суму в інтервал між нулем і одиницею, 

257
00:17:25,313 --> 00:17:28,391
ви знаєте, начебто вмотивовані цією біологічною аналогією нейронів, 

258
00:17:28,391 --> 00:17:29,840
які або неактивні, або активні. 

259
00:17:30,280 --> 00:17:30,300
Точно. 

260
00:17:30,560 --> 00:17:34,040
Але порівняно небагато сучасних мереж фактично використовують sigmoid. 

261
00:17:34,320 --> 00:17:34,320
так 

262
00:17:34,440 --> 00:17:35,540
Це стара школа, чи не так? 

263
00:17:35,760 --> 00:17:38,580
Так, точніше relu, здається, набагато легше тренувати. 

264
00:17:38,580 --> 00:17:42,340
А relu, relu означає випрямлену лінійну одиницю? 

265
00:17:42,680 --> 00:17:47,865
Так, це така функція, де ви просто берете максимальне значення нуль і a, 

266
00:17:47,865 --> 00:17:50,920
де a задано тим, що ви пояснювали у відео. 

267
00:17:50,920 --> 00:17:58,228
Я вважаю, що це було частково мотивовано біологічною аналогією з тим, 

268
00:17:58,228 --> 00:18:01,360
як нейрони активуються чи ні. 

269
00:18:01,360 --> 00:18:05,497
Отже, якщо він перевищить певний поріг, це буде функція ідентифікації, 

270
00:18:05,497 --> 00:18:09,460
але якщо ні, то вона просто не буде активована, тому буде нульовою. 

271
00:18:09,460 --> 00:18:10,840
Так що це якесь спрощення. 

272
00:18:11,160 --> 00:18:15,477
Використання сигмовидів не допомогло в навчанні або в якийсь момент 

273
00:18:15,477 --> 00:18:19,985
було дуже важко тренуватися, і люди просто спробували relu, і сталося, 

274
00:18:19,985 --> 00:18:24,620
що це дуже добре спрацювало для цих неймовірно глибоких нейронних мереж. 

275
00:18:25,100 --> 00:18:25,640
Гаразд, дякую Ліша. 

