[
 {
  "input": "This is a 3.",
  "translatedText": "",
  "from_community_srt": "هذه هى ثلاثة,",
  "n_reviews": 0,
  "start": 4.22,
  "end": 5.4
 },
 {
  "input": "It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3.",
  "translatedText": "",
  "from_community_srt": "مكتوبة ومقدمة بإهمال وبدقة منخفضة جدا 28*28 بكسل ولكن عقلك لا يوجد لديه مشكلة في التعرف على أنها ثلاثة وأريد منك أن تأخذ لحظة لنقدر",
  "n_reviews": 0,
  "start": 6.06,
  "end": 13.72
 },
 {
  "input": "And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 14.34,
  "end": 18.96
 },
 {
  "input": "I mean, this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next.",
  "translatedText": "",
  "from_community_srt": "كيف أن عقلك يستطيع فعل هذا بكل سلاسة وبدون مجهود أعنى هذا وهذا وهذا يمكن أيضا التعرف عليهم على أنهم الرقم ثلاثة",
  "n_reviews": 0,
  "start": 19.7,
  "end": 28.32
 },
 {
  "input": "The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3.",
  "translatedText": "",
  "from_community_srt": "على الرغم من أن القيم المحددة لكل بكسل مختلفة جدا من صورة واحدة إلى أخرى الخلايا الحساسة للضوء في عينيك التى تعمل عندما ترى هذه الثلاثة تختلف كثيرا عن تلك التي تعمل عندما ترى تلك الثلاثة.",
  "n_reviews": 0,
  "start": 28.9,
  "end": 36.94
 },
 {
  "input": "But something in that crazy-smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas.",
  "translatedText": "",
  "from_community_srt": "ولكن شيئا في القشرة البصرية الذكية التي تمتلكها يحل هذا على أنه تقديم لنفس الفكرة وهى الرقم ثلاثة ولكنه بنفس الوقت يقوم بالتعرف على الصور الأخرى على أنها أفكار أخرى تختلف عن الرقم ثلاثة",
  "n_reviews": 0,
  "start": 37.52,
  "end": 48.26
 },
 {
  "input": "But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this and outputs a single number between 0 and 10, telling you what it thinks the digit is, well the task goes from comically trivial to dauntingly difficult.",
  "translatedText": "",
  "from_community_srt": "ولكن إذا قلت لك مهلا اجلس واكتب لى برنامجا تكون مدخلاته شبكة من  28 بكسل*28بكسل مثل هذا ومخرجاته رقم بين الصفر وال10 ويستطيع التوقع ماذا يكون هذا الرقم عندها تتحول المهمة من شيء غاية فى التفاهة لشئ صعب بطريقة مروعة إذا لم تكن تعيش فى العصر الحجرى فأعتقد",
  "n_reviews": 0,
  "start": 49.22,
  "end": 66.18
 },
 {
  "input": "Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future.",
  "translatedText": "",
  "from_community_srt": "أننى بالكاد أحتاج لتنبيهك للعلاقة الوثيقة للحاضر والمستقبل أيضا بعلم الشبكات العصبية وتعلم الآلة",
  "n_reviews": 0,
  "start": 67.16,
  "end": 74.64
 },
 {
  "input": "But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing, not as a buzzword but as a piece of math.",
  "translatedText": "",
  "from_community_srt": "ولكن ما أريد فعله حقا هو توضيح لك ما هى الشبكات العصبية ؟ بافتراض عدم وجود خلفية مسبقة ومساعدتك لتخيل ماتفعله ليس كمصطلحات رنانة ولكن بلغة الرياضيات",
  "n_reviews": 0,
  "start": 75.12,
  "end": 84.46
 },
 {
  "input": "My hope is that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote-unquote learning.",
  "translatedText": "",
  "from_community_srt": "ما أتمناه أن تخرج من هنا وأنت تشعر أن الهيكل نفسه محفز وأن تشعر بأنك تعرف معنى ما تقرأ أو تسمع من اقتباسات عن الشبكات العصبية",
  "n_reviews": 0,
  "start": 85.02,
  "end": 94.34
 },
 {
  "input": "This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning.",
  "translatedText": "",
  "from_community_srt": "هذا الفيديو سيكون فقط مكرسا للهيكل المكون للشبكات العصبية أما الذى يليه فسيكون لمعالجة وتوضيح التعلم نفسه بالنسبة للشبكات العصبية",
  "n_reviews": 0,
  "start": 95.36,
  "end": 100.26
 },
 {
  "input": "What we're going to do is put together a neural network that can learn to recognize handwritten digits.",
  "translatedText": "",
  "from_community_srt": "ما نحن بصدد القيام به هو وضع شبكة عصبية للتعرف على الأرقام المكتوبة بخط اليد هذا مثال كلاسيكى",
  "n_reviews": 0,
  "start": 100.96,
  "end": 106.04
 },
 {
  "input": "This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quo here, because at the end of the two videos I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer.",
  "translatedText": "",
  "from_community_srt": "لتقديم الموضوع وسأكون سعيدا بالتمسك به فى الوضع الراهن لأنه فى نهاية الفيديوهين أريد أن أشير لبعض المصادر الجيدة التى تستطيع التعلم منها أكثر والأماكن التى يمكنك من خلالها تحميل الكود الذى يقوم بذلك وتشغيله",
  "n_reviews": 0,
  "start": 109.36,
  "end": 123.08
 },
 {
  "input": "There are many many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos you and I are just going to look at the simplest plain vanilla form with no added frills.",
  "translatedText": "",
  "from_community_srt": "على جهاز الكمبيوتر الخاص بك هناك العديد والعديد من الأشكال المختلفة للشبكات العصبية  وفى السنوات الأخيرة كان هناك نوع من الطفرة في البحث نحو هذه الأشكال ولكن في هذين الفيديوهين التمهيديين سنقوم أنا وأنت فقط بالنظر إلى شكل مبسط جدا بدون إضافات معقدة",
  "n_reviews": 0,
  "start": 125.04,
  "end": 139.18
 },
 {
  "input": "This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds around.",
  "translatedText": "",
  "from_community_srt": "هذا ضروري نوعا ما لأجل فهم أي من  الأشكال الأقوى والأحدث وصدقنى لازال لدينا الكثير من التعقيد لنحاول فهمه بعقولنا",
  "n_reviews": 0,
  "start": 139.86,
  "end": 148.6
 },
 {
  "input": "But even in this simplest form it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do.",
  "translatedText": "",
  "from_community_srt": "ولكن حتى فى هذا الشكل البسيط تستطيع الشبكة تعلم كيف تتعرف على الأرقام المكتوبة بخط اليد وهو شيء رائع جدا أن يكون جهاز الكمبيوتر قادرا على هذا",
  "n_reviews": 0,
  "start": 149.12,
  "end": 156.52
 },
 {
  "input": "And at the same time you'll see how it does fall short of a couple hopes that we might have for it.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 157.48,
  "end": 162.28
 },
 {
  "input": "As the name suggests neural networks are inspired by the brain, but let's break that down.",
  "translatedText": "",
  "from_community_srt": "فى الوقت نفسه سترى كيف أنها لا ترقى لبعض الآمال التى نضعها عليها كما يوحي اسمها الشبكات العصبية مستوحاة من الدماغ، ولكن دعنا نوضح ذلك أكثر",
  "n_reviews": 0,
  "start": 163.38,
  "end": 168.5
 },
 {
  "input": "What are the neurons, and in what sense are they linked together?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 168.52,
  "end": 171.66
 },
 {
  "input": "Right now when I say neuron all I want you to think about is a thing that holds a number, specifically a number between 0 and 1.",
  "translatedText": "",
  "from_community_srt": "ما هي الخلايا العصبية وبأي منطق ترتبط ببعضها البعض ؟ الآن عندما أقول الخلايا العصبية كل ما أريدك أن تفكر فيه هو شيء يحمل عددا",
  "n_reviews": 0,
  "start": 172.5,
  "end": 180.44
 },
 {
  "input": "It's really not more than that.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 180.68,
  "end": 182.56
 },
 {
  "input": "For example the network starts with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784 neurons in total.",
  "translatedText": "",
  "from_community_srt": "رقم بين ال0 وال 1 تحديدا إنها حقا ليست شيئا أكثر من ذلك على سبيل المثال الشبكة تبدأ بحزمة من الخلايا تستجيب لكل مدخل من مدخلات  صورة 28*28 بكسل",
  "n_reviews": 0,
  "start": 183.78,
  "end": 194.22
 },
 {
  "input": "Each one of these holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels.",
  "translatedText": "",
  "from_community_srt": "الذي هو 784 خلية عصبية  في المجموع كل واحد منها يحمل رقم  يمثل قيمة التدرج الرمادى الذى تستجيب به كل بكسل",
  "n_reviews": 0,
  "start": 194.7,
  "end": 204.38
 },
 {
  "input": "This number inside the neuron is called its activation, and the image you might have in mind here is that each neuron is lit up when its activation is a high number.",
  "translatedText": "",
  "from_community_srt": "تتراوح من 0 للبكسل السوداء وحتى 1 للبكسل البيضاء هذا الرقم داخل الخلية العصبية يسمى تفعيل والصورة التى يمكنك تخيلها بعقلك الآن",
  "n_reviews": 0,
  "start": 205.3,
  "end": 214.16
 },
 {
  "input": "So all of these 784 neurons make up the first layer of our network.",
  "translatedText": "",
  "from_community_srt": "هى أن كل خلية تضيء عندما يتم تفعيلها برقم ذي قيمة كبيرة لذلك هذه ال784 خلية عصبية تشكل الطبقة الأولى من الشبكة",
  "n_reviews": 0,
  "start": 216.72,
  "end": 221.86
 },
 {
  "input": "Now jumping over to the last layer, this has 10 neurons, each representing one of the digits.",
  "translatedText": "",
  "from_community_srt": "الآن بالنظر للطبقة الأخيرة فهى تمتلك عشر خلايا عصبية كل منها تشير لرقم من الأرقام تفعيل هذه الخلايا كما قلنا من قبل هو رقم بين الصفر والواحد",
  "n_reviews": 0,
  "start": 226.5,
  "end": 231.36
 },
 {
  "input": "The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit.",
  "translatedText": "",
  "from_community_srt": "يمثل لأى مدى تعتقد الشبكة أن الصورة المقدمة إليها تمثل هذا الرقم تحديدا هناك أيضا طبقتين فى المنتصف تسمى الطبقات المخفية",
  "n_reviews": 0,
  "start": 232.04,
  "end": 242.12
 },
 {
  "input": "There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark for how on earth this process of recognizing digits is going to be handled.",
  "translatedText": "",
  "from_community_srt": "والتي في الوقت الحاضر ستكون فقط علامة استفهام كبيرة إذا هل يجب أن يكون هناك علامة استفهام كبيرة كيف بحق الإله ستتم عملية التعرف على الأرقام هذه ؟!",
  "n_reviews": 0,
  "start": 243.04,
  "end": 253.6
 },
 {
  "input": "In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice.",
  "translatedText": "",
  "from_community_srt": "في هذه الشبكة اخترت طبقتين مخفيتين كل واحدة  16 خلية عصبية  وأعترف أن هذا نوع من الاختيار التعسفي",
  "n_reviews": 0,
  "start": 254.26,
  "end": 260.56
 },
 {
  "input": "To be honest I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that was just a nice number to fit on the screen.",
  "translatedText": "",
  "from_community_srt": "لنكون صادقين اخترت طبقتين بناء على لأي مدى أريد تحفيز الهيكل فى لحظة واحدة أما عن 16 ؟ فهذا فقط كان رقما جيدا ليلائم الشاشة أثناء التدريب :D",
  "n_reviews": 0,
  "start": 261.02,
  "end": 268.2
 },
 {
  "input": "In practice there is a lot of room for experiment with a specific structure here.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 268.78,
  "end": 272.34
 },
 {
  "input": "The way the network operates, activations in one layer determine the activations of the next layer.",
  "translatedText": "",
  "from_community_srt": "عند التطبيق هناك الكثير من المتسع لتجربة هيكل معين هنا حيث تعمل الشبكة بالطريقة الآتية فتفعيل طبقة واحدة يؤدى إلى تحديد كيفية تفعيل الطبقة التي تليها",
  "n_reviews": 0,
  "start": 273.02,
  "end": 278.48
 },
 {
  "input": "And of course the heart of the network as an information processing mechanism comes down to exactly how those activations from one layer bring about activations in the next layer.",
  "translatedText": "",
  "from_community_srt": "وبالتأكيد فقلب الشبكة يمثل آلية معالجة المعلومات والذى يمثل بدقة الكيفية التى يكون بها تفعيل طبقة واحدة يسبب تفعيل الطبقة التي تليها",
  "n_reviews": 0,
  "start": 279.2,
  "end": 288.58
 },
 {
  "input": "It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire.",
  "translatedText": "",
  "from_community_srt": "من المفترض أن يكون هذا قريبا بعض الشيء للكيفية التي يؤدى فيها تفعيل  بعض الخلايا العصبية البيولوجية",
  "n_reviews": 0,
  "start": 289.14,
  "end": 297.18
 },
 {
  "input": "Now the network I'm showing here has already been trained to recognize digits, and let me show you what I mean by that.",
  "translatedText": "",
  "from_community_srt": "لتفعيل بعض من الخلايا الأخرى الآن الشبكة التي أستعرضها هنا تم بالفعل تدريبها للتعرف على الأرقام ودعني أريك ما أعنيه بذلك",
  "n_reviews": 0,
  "start": 298.12,
  "end": 303.4
 },
 {
  "input": "It means if you feed in an image, lighting up all 784 neurons of the input layer according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer which causes some pattern in the one after it, which finally gives some pattern in the output layer.",
  "translatedText": "",
  "from_community_srt": "هذا يعنى إذا قمت بتغذيتها بصورة معطى فيها كل قيم  ال 784 خلية كمدخلات فإنه طبقا لقيمة تلك القيم يتم تحديد مدى سطوع كل بكسل فى الصورة هذا النمط من التفعيل يسبب نما محددا فى التفعيل للطبقة التي تليها والذى بدوره يسبب نمطا محددا آخر فى الطبقة التي تليها",
  "n_reviews": 0,
  "start": 303.64,
  "end": 322.08
 },
 {
  "input": "And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents.",
  "translatedText": "",
  "from_community_srt": "والذى فى النهاية يعطى نمطا ما فى الطبقة الخارجية والخلية الأكثر لمعانا تكون هى اختيار الشبكة لما تظنه يمثل الرقم الصحيح للصورة التى أدخلتها إليها",
  "n_reviews": 0,
  "start": 322.56,
  "end": 329.4
 },
 {
  "input": "And before jumping into the math for how one layer influences the next, or how training works, let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently.",
  "translatedText": "",
  "from_community_srt": "وقبل الذهاب إلى الرياضيات وراء كيف أن طبقة واحدة تؤثر فى الطبقة التي تليها أو الكيفية التي يتم بها تدريب الشبكات دعنا فقط نوضح لماذا من المنطقي أن نتوقع من هيكل طبقي كهذا أن يتصرف بذكاء ما الذي نتوقعه هنا؟ ما هو أفضل توقع لما يمكن أن تفعله تلك الطبقات الوسطى؟",
  "n_reviews": 0,
  "start": 332.56,
  "end": 343.52
 },
 {
  "input": "What are we expecting here?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 344.06,
  "end": 345.22
 },
 {
  "input": "What is the best hope for what those middle layers might be doing?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 345.4,
  "end": 347.6
 },
 {
  "input": "Well, when you or I recognize digits, we piece together various components.",
  "translatedText": "",
  "from_community_srt": "حسنا عندما نتعرف أنا وأنت على الأرقام فإننا نجمع معا بعض القطع مثلا ال9 تمتلك حلقة فى الأعلى وخط فى الأسفل إلى اليمين قليلا",
  "n_reviews": 0,
  "start": 348.92,
  "end": 353.52
 },
 {
  "input": "A 9 has a loop up top and a line on the right.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 354.2,
  "end": 356.82
 },
 {
  "input": "An 8 also has a loop up top, but it's paired with another loop down low.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 357.38,
  "end": 361.18
 },
 {
  "input": "A 4 basically breaks down into three specific lines, and things like that.",
  "translatedText": "",
  "from_community_srt": "و 8 لديها أيضا حلقة أعلى، لكنه يقترن بحلقة أخرى فى الأسفل أما ال4 فهى أساسا تقسم إلى ثلاث خطوط كالتي بالشكل",
  "n_reviews": 0,
  "start": 361.98,
  "end": 366.82
 },
 {
  "input": "Now in a perfect world, we might hope that each neuron in the second to last layer corresponds with one of these subcomponents, that anytime you feed in an image with, say, a loop up top, like a 9 or an 8, there's some specific neuron whose activation is going to be close to 1.",
  "translatedText": "",
  "from_community_srt": "فى عالم مثالى ربما نأمل أن كل طبقة من الثانية وحتى الأخيرة تستجيب لواحدة من هذه المكونات نأمل أنه فى أى وقت تغذيها بصورة بها حلقة فى الأعلى مثل ال9 أو  8",
  "n_reviews": 0,
  "start": 367.6,
  "end": 383.78
 },
 {
  "input": "And I don't mean this specific loop of pixels, the hope would be that any generally loopy pattern towards the top sets off this neuron.",
  "translatedText": "",
  "from_community_srt": "هناك بعض الخلايا العصبية المحددة التى يكون تفعيلها قريبا من الواحد وأنا لا أعنى حلقة محددة من البكسلات فالأمل أن يكون",
  "n_reviews": 0,
  "start": 384.5,
  "end": 391.56
 },
 {
  "input": "That way, going from the third layer to the last one just requires learning which combination of subcomponents corresponds to which digits.",
  "translatedText": "",
  "from_community_srt": "أى نمط حلقى عام فى الأعلى قادر على تفعيل الخلايا العصبية بنفس الطريقة التى تتخذها للتعرف على الحلقة بهذه الطريقة وبمراقبة كيفية التفعيل من الطبقة الثالثة للطبقة الأخيرة للتعرف على أى مزيج من المكونات الفرعية يستجيب مع أى عدد",
  "n_reviews": 0,
  "start": 392.44,
  "end": 400.04
 },
 {
  "input": "Of course, that just kicks the problem down the road, because how would you recognize these subcomponents, or even learn what the right subcomponents should be?",
  "translatedText": "",
  "from_community_srt": "بالطبع هناك بعض العقبات فى الطريق إذ كيف يمكنك التعرف على هذه  المكونات الفرعية أو حتى تعلم أيها سيكون صحيحا وأنا لم أتحدث بعد عن الطريقة التى",
  "n_reviews": 0,
  "start": 401.0,
  "end": 407.64
 },
 {
  "input": "And I still haven't even talked about how one layer influences the next, but run with me on this one for a moment.",
  "translatedText": "",
  "from_community_srt": "تؤثر بها الطبقة الواحدة بالطبقة الأخرى لكن جارني  في هذا للحظة التعرف على حلقة يمكنه أن يقسم أيضا إلى عدة مشكلات صغيرة",
  "n_reviews": 0,
  "start": 408.06,
  "end": 413.06
 },
 {
  "input": "Recognizing a loop can also break down into subproblems.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 413.68,
  "end": 416.68
 },
 {
  "input": "One reasonable way to do this would be to first recognize the various little edges that make it up.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 417.28,
  "end": 422.78
 },
 {
  "input": "Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7, is really just a long edge, or maybe you think of it as a certain pattern of several smaller edges.",
  "translatedText": "",
  "from_community_srt": "إحدى الطرق المعقولة للقيام بهذا هى أن تتعرف أولا على الحواف الصغيرة المختلفة التى تكونها بالمثل الخط الطويل كالذى ربما تراه فى الرقم 1 أو 4 أو 7 ليس سوى حافة طويلة أو ربما تفكر به على أنه نمط محدد من عدة حواف صغيرة متعددة لذا أملنا أنه ربما كل خلية عصبية فى الطبقة الثانية من الشبكة",
  "n_reviews": 0,
  "start": 423.78,
  "end": 434.32
 },
 {
  "input": "So maybe our hope is that each neuron in the second layer of the network corresponds with the various relevant little edges.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 435.14,
  "end": 442.72
 },
 {
  "input": "Maybe when an image like this one comes in, it lights up all of the neurons associated with around 8 to 10 specific little edges, which in turn lights up the neurons associated with the upper loop and a long vertical line, and those light up the neuron associated with a 9.",
  "translatedText": "",
  "from_community_srt": "تستجيب مع مختلف الحواف الصغيرة ذات الصلة ربما عندما تغذى بصورة كهذه تقوم بتفعيل جميع الخلايا العصبية المرتبطة بحوالي ثمانية إلى عشرة حواف صغيرة محددة والتي بدورها تضيء الخلايا العصبية المرتبطة بالحلقة العليا والخط العمودي الطويل وهؤلاء يفعلون الخلية المرتبطة بالرقم 9 سواء كان ذلك ما تفعله الشبكة فعلا أو لا هو سؤال آخر ..",
  "n_reviews": 0,
  "start": 443.54,
  "end": 459.72
 },
 {
  "input": "Whether or not this is what our final network actually does is another question, one that I'll come back to once we see how to train the network, but this is a hope that we might have, a sort of goal with the layered structure like this.",
  "translatedText": "",
  "from_community_srt": "سأعود إليه بمجرد أن نرى كيف يمكننا تدريب الشبكات ولكن هذا نوع ما من الأمل نوع من الهدف مع طبقات مهيكلة كهذه",
  "n_reviews": 0,
  "start": 460.68,
  "end": 472.54
 },
 {
  "input": "Moreover, you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks.",
  "translatedText": "",
  "from_community_srt": "علاوة على ذلك يمكنك أن تتخيل كيف يمكن أن تكون القابلية لتحديد هذه الحواف والأنماط مفيدة فى مهمات أخرى للتعرف على الصور",
  "n_reviews": 0,
  "start": 473.16,
  "end": 480.3
 },
 {
  "input": "And even beyond image recognition, there are all sorts of intelligent things you might want to do that break down into layers of abstraction.",
  "translatedText": "",
  "from_community_srt": "وخلاف التعرف على الصور هناك الكثير من الأشياء الذكية التى بإمكانك فعلها يمكن تقسميها إلى طبقات تجريدية",
  "n_reviews": 0,
  "start": 480.88,
  "end": 487.28
 },
 {
  "input": "Parsing speech, for example, involves taking raw audio and picking out distinct sounds, which combine to make certain syllables, which combine to form words, which combine to make up phrases and more abstract thoughts, etc.",
  "translatedText": "",
  "from_community_srt": "تحليل الكلام على سبيل المثال ينطوي على أخذ الصوت الخام واختيار الأصوات المميزة التي تتحد لتكوين منهاج معينة والتي تتحد لتشكيل الكلمات التي تتضافر لتكوين العبارات والمزيد من الأفكار المجردة وغيرها بالعودة إلى كيفية عمل أى من هذا ,",
  "n_reviews": 0,
  "start": 488.04,
  "end": 500.06
 },
 {
  "input": "But getting back to how any of this actually works, picture yourself right now designing how exactly the activations in one layer might determine the next.",
  "translatedText": "",
  "from_community_srt": "تصور نفسك الآن تصمم كيف بالضبط  أن تفعيل بعض الخلايا العصبية فى طبقة واحدة يؤثر على تفعيل الخلايا العصبية فى الطبقة التي تليها",
  "n_reviews": 0,
  "start": 501.1,
  "end": 509.92
 },
 {
  "input": "The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits.",
  "translatedText": "",
  "from_community_srt": "الهدف هو إيجاد آلية يمكنها تحديد الحواف فى البكسلات وتحويل الحواف إلى أنماط والأنماط إلى أرقام وللتركيز على مثال واحد",
  "n_reviews": 0,
  "start": 510.86,
  "end": 518.98
 },
 {
  "input": "And to zoom in on one very specific example, let's say the hope is for one particular neuron in the second layer to pick up on whether or not the image has an edge in this region here.",
  "translatedText": "",
  "from_community_srt": "دعونا نقول أننا نأمل أن واحدا معينا من الخلايا العصبية فى الطبقة الثانية لتحديد ما إذا كانت الصورة لديها حواف فى هذه المنطقة أو لا",
  "n_reviews": 0,
  "start": 519.44,
  "end": 530.62
 },
 {
  "input": "The question at hand is what parameters should the network have?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 531.44,
  "end": 535.1
 },
 {
  "input": "What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern, or any other pixel pattern, or the pattern that several edges can make a loop, and other such things?",
  "translatedText": "",
  "from_community_srt": "والسؤال المطروح هو ما هي المعاملات  التي ينبغي أن تكون لدى الشبكة ما النقاط التى يجب عليك طرقها لكى تكون معبرة كفاية لإمساك النمط المطلوب",
  "n_reviews": 0,
  "start": 535.64,
  "end": 547.78
 },
 {
  "input": "Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer.",
  "translatedText": "",
  "from_community_srt": "أو أى نمط بكسلى آخر أو النمط الذى يحدد الحواف المختلفة التى تكون حلقة أو غيرها من الأشياء حسنا، ما سنفعله هو تعيين وزن لكل واحد من الروابط بين الخلية العصبية التى لدينا  والخلايا العصبية من الطبقة الأولى",
  "n_reviews": 0,
  "start": 548.72,
  "end": 555.56
 },
 {
  "input": "These weights are just numbers.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 556.32,
  "end": 557.7
 },
 {
  "input": "Then take all of those activations from the first layer and compute their weighted sum according to these weights.",
  "translatedText": "",
  "from_community_srt": "هذه الأوزان هي فقط مجرد  أرقام ثم تأخذ كل تلك التفعيلات من الطبقة الأولى وتحسب مجموعها طبقا لتلك المجموعات",
  "n_reviews": 0,
  "start": 558.54,
  "end": 565.5
 },
 {
  "input": "I find it helpful to think of these weights as being organized into a little grid of their own, and I'm going to use green pixels to indicate positive weights, and red pixels to indicate negative weights, where the brightness of that pixel is some loose depiction of the weight's value.",
  "translatedText": "",
  "from_community_srt": "أجد أنه من المفيد أن تفكر فى هذه المجموعات على أنها منظمة على هيئة شبكات صغيرة خاصة بهم وسأستخدم بكسل خضراء للإشارة إلى الأوزان الموجبة والبكسل الأحمر للإشارة إلى الأوزان السالبة حيث سطوع هذا البكسل هو تصوير فضفاض لقيم الأوزان الآن إذا جعلنا كل الأوزان مرتبطة مع تقريبا كل البكسلات التى قيمتها بصفر",
  "n_reviews": 0,
  "start": 567.7,
  "end": 581.78
 },
 {
  "input": "Now if we made the weights associated with almost all of the pixels zero except for some positive weights in this region that we care about, then taking the weighted sum of all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about.",
  "translatedText": "",
  "from_community_srt": "باستثناء بعض الأوزان الإيجابية في هذه المنطقة التي نهتم بها ثم نأخذ الوزن المجموع كل قيم البكسل هى فقط كميات لإضافتها لقيم البكسل فى المنطقة التى نهتم بها",
  "n_reviews": 0,
  "start": 582.78,
  "end": 597.82
 },
 {
  "input": "And if you really wanted to pick up on whether there's an edge here, what you might do is have some negative weights associated with the surrounding pixels.",
  "translatedText": "",
  "from_community_srt": "وإن كنت تريد معرفة ما إذا كان هناك حافة هنا أو لا ربما عليك أن تمتلك بعض الاوزان السالبة مرتبطة بالبكسل المحيطة",
  "n_reviews": 0,
  "start": 599.14,
  "end": 606.6
 },
 {
  "input": "Then the sum is largest when those middle pixels are bright but the surrounding pixels are darker.",
  "translatedText": "",
  "from_community_srt": "ثم يكون المجموع أكبر عندما تكون هذه البكسل فى المنتصف  ساطعة ، ولكن البكسل المحيطة بها أكثر قتامة",
  "n_reviews": 0,
  "start": 607.48,
  "end": 612.7
 },
 {
  "input": "When you compute a weighted sum like this, you might come out with any number, but for this network what we want is for activations to be some value between 0 and 1.",
  "translatedText": "",
  "from_community_srt": "عند حساب وزن المجموع بهذه الطريقة يمكنك الحصول على أى عدد ولكن لهذه الشبكة ما نريده هو أن تكون للتفعيلات قيمة بين 0 وال 1",
  "n_reviews": 0,
  "start": 614.26,
  "end": 623.54
 },
 {
  "input": "So a common thing to do is to pump this weighted sum into some function that squishes the real number line into the range between 0 and 1.",
  "translatedText": "",
  "from_community_srt": "الشيء الشائع الذى نستخدمه هو تغذية دالة ما بهذا الوزن تلك الدالة تقوم بتحويل هذا العدد أيا كانت قيمته إلى قيمة بين الصفر والواحد",
  "n_reviews": 0,
  "start": 624.12,
  "end": 632.14
 },
 {
  "input": "And a common function that does this is called the sigmoid function, also known as a logistic curve.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 632.46,
  "end": 637.42
 },
 {
  "input": "Basically very negative inputs end up close to 0, positive inputs end up close to 1, and it just steadily increases around the input 0.",
  "translatedText": "",
  "from_community_srt": "هناك دالة شائعة تقوم بذلك بالفعل تسمى sigmoid وهى أيضا معروفة باسم logistic curve بالأساس أى مدخل عال في قيمته السالبة  ينتهى به المطاف إلى الصفر وأى مدخل عال في قيمته الموجبة ينتهي به المطاف إلى الواحد",
  "n_reviews": 0,
  "start": 638.0,
  "end": 646.6
 },
 {
  "input": "So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is.",
  "translatedText": "",
  "from_community_srt": "وهى تزيد بطريقة طردية فقط حول النقطة صفر لذلك تفعيل الخلايا العصبية هنا هو بالأساس قياس للانحياز الموجب لوزن المجموع",
  "n_reviews": 0,
  "start": 649.12,
  "end": 656.36
 },
 {
  "input": "But maybe it's not that you want the neuron to light up when the weighted sum is bigger than 0.",
  "translatedText": "",
  "from_community_srt": "ولكن ربما لا تريد أن تضيء الخلية العصبية  عندما يكون الوزن  أكبر من 0 ربما كنت تريد فقط أن تكون نشطة عندما يكون الوزن أكبر من 10",
  "n_reviews": 0,
  "start": 657.54,
  "end": 661.88
 },
 {
  "input": "Maybe you only want it to be active when the sum is bigger than say 10.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 662.28,
  "end": 666.36
 },
 {
  "input": "That is, you want some bias for it to be inactive.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 666.84,
  "end": 670.26
 },
 {
  "input": "What we'll do then is just add in some other number like negative 10 to this weighted sum before plugging it through the sigmoid squishification function.",
  "translatedText": "",
  "from_community_srt": "أنك تريد أن تجعلها متحيزة قليلا لأن تكون غير نشطة ما ستفعله فقط هو إضافة رقم سالب لمجموع الأوزان مثل -10",
  "n_reviews": 0,
  "start": 671.38,
  "end": 679.66
 },
 {
  "input": "That additional number is called the bias.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 680.58,
  "end": 682.44
 },
 {
  "input": "So the weights tell you what pixel pattern this neuron in the second layer is picking up on, and the bias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active.",
  "translatedText": "",
  "from_community_srt": "قبل إدخالها إلى دالة ال sigmoid ويسمى هذا العدد الإضافي بالانحياز لذا فالأوزان تخبرك ما هو نمط البكسل الذى تتخذه الخلية العصبية فى الطبقة الثانية أما الانحياز فيخبرك القيمة التى يحتاج أن يكون عليها الوزن لكى تصبح الخلية العصبية نشطة وهذا كله فقط لخلية واحدة",
  "n_reviews": 0,
  "start": 683.46,
  "end": 695.18
 },
 {
  "input": "And that is just one neuron.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 696.12,
  "end": 697.68
 },
 {
  "input": "Every other neuron in this layer is going to be connected to all 784 pixel neurons from the first layer, and each one of those 784 connections has its own weight associated with it.",
  "translatedText": "",
  "from_community_srt": "كل خلية عصبية أخرى فى هذه الطبقة ستكون مرتبطة بكل ال784 خلية عصبية الموجودة فى الطبقة الأولى ولكل ارتباط من هذه ال784 ارتباطات له وزنه الخاص المرتبط به أيضا لكل واحد انحياز ..رقم ما تضيفه للوزن قبل سحقه فى دالة الsigmoid",
  "n_reviews": 0,
  "start": 698.28,
  "end": 710.94
 },
 {
  "input": "Also, each one has some bias, some other number that you add on to the weighted sum before squishing it with the sigmoid.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 711.6,
  "end": 717.6
 },
 {
  "input": "And that's a lot to think about!",
  "translatedText": "",
  "n_reviews": 0,
  "start": 718.11,
  "end": 719.54
 },
 {
  "input": "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights, along with 16 biases.",
  "translatedText": "",
  "from_community_srt": "وهذا كثير جدا لو فكرت به مليا فى هذه الطبقات المخفية يوجد 784 * 16 وزن جنبا إلى جنب مع 16 انحياز",
  "n_reviews": 0,
  "start": 719.96,
  "end": 727.98
 },
 {
  "input": "And all of that is just the connections from the first layer to the second.",
  "translatedText": "",
  "from_community_srt": "وكل ذلك هو مجرد اتصال من الطبقة الأولى إلى الثانية الروابط بين الطبقات الأخرى أيضا لديها أوزان وانحيازات",
  "n_reviews": 0,
  "start": 728.84,
  "end": 731.94
 },
 {
  "input": "The connections between the other layers also have a bunch of weights and biases associated with them.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 732.52,
  "end": 737.34
 },
 {
  "input": "All said and done, this network has almost exactly 13,000 total weights and biases.",
  "translatedText": "",
  "from_community_srt": "بأخذ كل ذلك بعين الاعتبار هذه الشبكة تمتلك حوالى 13,000 وزن وانحياز 13,000 نقطة لتطرقها والطرق على تلك النقاط بشكل مختلف سيؤدى بالشبكة للتصرف بشكل مختلف",
  "n_reviews": 0,
  "start": 738.34,
  "end": 743.8
 },
 {
  "input": "13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 743.8,
  "end": 749.96
 },
 {
  "input": "So when we talk about learning, what that's referring to is getting the computer to find a valid setting for all of these many many numbers so that it'll actually solve the problem at hand.",
  "translatedText": "",
  "from_community_srt": "لذلك عندما نتحدث عن التعلم ما يعنيه هذا هو جعل جهاز الكمبيوتر يجد وضع فعال لكل هذه الأرقام المتعددة بشرط أن يكون هذا الوضع فعلا",
  "n_reviews": 0,
  "start": 751.04,
  "end": 761.36
 },
 {
  "input": "One thought experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand, purposefully tweaking the numbers so that the second layer picks up on edges, the third layer picks up on patterns, etc.",
  "translatedText": "",
  "from_community_srt": "يحل المشكلة المقدمة إليه شيء مرعب ومسلى أن تتخيل نفسك تجلس وتقوم بحساب كل تلك الأوزان والانحيازات باليد .. تقوم بتغيير وتبديل الأرقام لكى تقوم الطبقة الثانية بتحديد الحواف والثالثة بتحديد الأنماط إلخ",
  "n_reviews": 0,
  "start": 762.62,
  "end": 776.58
 },
 {
  "input": "I personally find this satisfying rather than just treating the network as a total black box, because when the network doesn't perform the way you anticipate, if you've built up a little bit of a relationship with what those weights and biases actually mean, you have a starting place for experimenting with how to change the structure to improve.",
  "translatedText": "",
  "from_community_srt": "أجد هذا مرضِِ على المستوى الشخصي بدلا من تخيل الشبكة كصندوق أسود كبير لأنه عندما لا تؤدى الشبكة وظيفتها بشكل جيد إذا كان لديك توقع عن ما تعنيه تلك الأنماط والأوزان فسيكون لديك نقطة تبدأ منها لتجريب كيفية تغيير الهيكل لتحسينه",
  "n_reviews": 0,
  "start": 776.98,
  "end": 794.18
 },
 {
  "input": "Or when the network does work but not for the reasons you might expect, digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible solutions.",
  "translatedText": "",
  "from_community_srt": "أو عندما تعمل الشبكة وتؤدى وظيفتها ولكن ليس للأسباب التى تتوقعها التعمق في ما تعنيه الأوزان والانحيازات هو طريقة جيدة لتحدى الفروض التي وضعتها وعرض كل الحلول الممكنة",
  "n_reviews": 0,
  "start": 794.96,
  "end": 805.82
 },
 {
  "input": "By the way, the actual function here is a little cumbersome to write down, don't you think?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 806.84,
  "end": 810.68
 },
 {
  "input": "So let me show you a more notationally compact way that these connections are represented.",
  "translatedText": "",
  "from_community_srt": "بالمناسبة أظن أن الدالة هنا صعبة قليلا فى الكتابة أليس كذلك ؟ لذا دعنى أريك طريقة أفضل لكتابة هذه الروابط بشكل أسهل ..هكذا ستراها",
  "n_reviews": 0,
  "start": 812.5,
  "end": 817.14
 },
 {
  "input": "This is how you'd see it if you choose to read up more about neural networks.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 817.66,
  "end": 820.52
 },
 {
  "input": "Organize all of the activations from one layer into a column as a vector.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 821.38,
  "end": 820.52
 },
 {
  "input": "Then organize all of the weights as a matrix, where each row of that matrix corresponds to the connections between one layer and a particular neuron in the next layer.",
  "translatedText": "",
  "from_community_srt": "إذا اخترت قراءة المزيد عن الشبكات العصبية قم بتنظيم كل التفعيلات فى الطبقة الواحدة لعمود كمتجه ثم تنظيم جميع الأوزان كمصفوفة حيث كل صف من تلك المصفوفة يتوافق مع وصلات بين طبقة واحدة وخلية عصبية معينة في الطبقة التالية",
  "n_reviews": 0,
  "start": 821.38,
  "end": 838.0
 },
 {
  "input": "What that means is that taking the weighted sum of the activations in the first layer according to these weights corresponds to one of the terms in the matrix vector product of everything we have on the left here.",
  "translatedText": "",
  "from_community_srt": "مايعنيه هذا أخذ مجموع أوزان التفعيلات من الطبقة الأولى وهذه الأوزان تتوافق مع واحدة من من عناصر المصفوفة التى نراها هنا على اليسار",
  "n_reviews": 0,
  "start": 838.54,
  "end": 849.88
 },
 {
  "input": "By the way, so much of machine learning just comes down to having a good grasp of linear algebra, so for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means, take a look at the series I did on linear algebra, especially chapter 3.",
  "translatedText": "",
  "from_community_srt": "بالمناسبة الكثير من تعلم الآلة له علاقة وثيقة بالجبر الخطي لذلك لأي شخص منكم يريد تصورا بصريا لطيفا لضرب المصفوفات بإمكانك أن تلقي نظرة على هذه السلسلة التى قدمتها فى الجبر الخطى",
  "n_reviews": 0,
  "start": 854.0,
  "end": 868.6
 },
 {
  "input": "Back to our expression, instead of talking about adding the bias to each one of these values independently, we represent it by organizing all those biases into a vector, and adding the entire vector to the previous matrix vector product.",
  "translatedText": "",
  "from_community_srt": "وخاصة الفصل الثالث بالعودة إلى ما كنا نتحدث فيه بدلا من إضافة الانحياز لكل قيمة على حدى نقوم بتنظيم كل الانحيازات على هيئة متجه ونضيف هذا المتجه إلى حاصل الضرب السابق للمصفوفتين",
  "n_reviews": 0,
  "start": 869.24,
  "end": 882.3
 },
 {
  "input": "Then as a final step, I'll wrap a sigmoid around the outside here, and what that's supposed to represent is that you're going to apply the sigmoid function to each specific component of the resulting vector inside.",
  "translatedText": "",
  "from_community_srt": "ثم كخطوة نهائية نضع sigmoid حولها بهذه الطريقة ومايفترض أن يقدمه هذا أنك تطبق الsigmoid على كل عنصر داخل  المتجه الناتج",
  "n_reviews": 0,
  "start": 883.28,
  "end": 894.74
 },
 {
  "input": "So once you write down this weight matrix and these vectors as their own symbols, you can communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression, and this makes the relevant code both a lot simpler and a lot faster, since many libraries optimize the heck out of matrix multiplication.",
  "translatedText": "",
  "from_community_srt": "لذلك بمجرد كتابة مصفوفة الوزن هذه وهذه المتجهات كرموز تستطيع ربط كل الانتقال من طبقة للطبقة الأخرى بتعبير بسيط وصغير وهذا يجعل الأوامر البرمجية أبسط وأكثر سرعة باعتبار أن كثير من المكتبات تقوم بتحسين شكل ضرب المصفوفات",
  "n_reviews": 0,
  "start": 895.94,
  "end": 915.66
 },
 {
  "input": "Remember how earlier I said these neurons are simply things that hold numbers?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 917.82,
  "end": 921.46
 },
 {
  "input": "Well of course the specific numbers that they hold depends on the image you feed in, so it's actually more accurate to think of each neuron as a function, one that takes in the outputs of all the neurons in the previous layer and spits out a number between 0 and 1.",
  "translatedText": "",
  "from_community_srt": "هل تذكر سابقا عندما قلنا أن الخلايا العصبية هى ببساطة أشياء تحمل أرقاما ؟ حسنا بالتأكيد الأرقام التى يحملونها تعتمد على الصورة التى تغذيهم بها لذا ربما تكون أكثر دقة لو فكرت بكل خلية عصبية على أنها دالة  تأخذ المخرجات من جميع الخلايا العصبية فى الطبقة السابقة وتظهرها على أنها رقم بين الصفر والواحد",
  "n_reviews": 0,
  "start": 922.22,
  "end": 938.34
 },
 {
  "input": "Really the entire network is just a function, one that takes in 784 numbers as an input and spits out 10 numbers as an output.",
  "translatedText": "",
  "from_community_srt": "فى الحقيقة الشبكة بالكامل هى أيضا دالة تأخذ 784 كمدخل وتخرج 10 أرقام كمخرجات إنها معقدة قليلا",
  "n_reviews": 0,
  "start": 939.2,
  "end": 947.06
 },
 {
  "input": "It's an absurdly complicated function, one that involves 13,000 parameters in the forms of these weights and biases that pick up on certain patterns, and which involves iterating many matrix vector products and the sigmoid squishification function, but it's just a function nonetheless.",
  "translatedText": "",
  "from_community_srt": "لدرجة أن لديها 13,000 معامل على شكل أوزان وانحيازات تتشكل بأنماط معينة والتى تتضمن تكرار الكثير من ضرب المصفوفات ثم سحق الرقم فى دالة الsigmoid",
  "n_reviews": 0,
  "start": 947.56,
  "end": 962.64
 },
 {
  "input": "And in a way it's kind of reassuring that it looks complicated.",
  "translatedText": "",
  "from_community_srt": "ولكنها مجرد دالة فى النهاية وبطريقة ما إنه شيء مطمئن أنها تبدو معقدة أعنى لو كانت أبسط لما كان سيكون لدينا أمل فى أن تتغلب على تحدى التعرف على الأرقام أليس كذلك ؟",
  "n_reviews": 0,
  "start": 963.4,
  "end": 966.66
 },
 {
  "input": "I mean if it were any simpler, what hope would we have that it could take on the challenge of recognizing digits?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 967.34,
  "end": 972.28
 },
 {
  "input": "And how does it take on that challenge?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 973.34,
  "end": 974.7
 },
 {
  "input": "How does this network learn the appropriate weights and biases just by looking at data?",
  "translatedText": "",
  "from_community_srt": "ولكن كيف تتعلم هذه الشبكة؟ كيف تعرف الأوزان والانحيازات المناسبة فقط بالنظر للبيانات التى لديها ؟",
  "n_reviews": 0,
  "start": 975.08,
  "end": 979.36
 },
 {
  "input": "Well that's what I'll show in the next video, and I'll also dig a little more into what this particular network we're seeing is really doing.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 980.14,
  "end": 986.12
 },
 {
  "input": "Now is the point I suppose I should say subscribe to stay notified about when that video or any new videos come out, but realistically most of you don't actually receive notifications from YouTube, do you?",
  "translatedText": "",
  "from_community_srt": "هذا ما سأريه لك فى الفيديو القادم كما سنتعمق قليلا فيما تفعله تلك الشبكات على أرض الواقع الآن هو الوقت الذى أقول فيه اشترك فى القناة وما إلى ذلك لتبقا متطلعا على الفيديوهات الجديدة",
  "n_reviews": 0,
  "start": 987.58,
  "end": 997.42
 },
 {
  "input": "Maybe more honestly I should say subscribe so that the neural networks that underlie YouTube's recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you.",
  "translatedText": "",
  "from_community_srt": "ولكن الواقع أن معظمكم لا يتلقون إشعارات من يوتوب فعلا، أليس كذلك؟ ربما يجب أن أقول اشترك حتى تستطيع الشبكات العصبية لليوتيوب التصديق بأنك تريد رؤية فيديوهات من هذه القناة وبالتالى ترشيحها لك",
  "n_reviews": 0,
  "start": 998.02,
  "end": 1007.88
 },
 {
  "input": "Anyway, stay posted for more.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1008.56,
  "end": 1009.94
 },
 {
  "input": "Thank you very much to everyone supporting these videos on Patreon.",
  "translatedText": "",
  "from_community_srt": "على أى حال ابق مستعدا للمزيد شكرا جزيلا للجميع على دعم هذه أشرطة الفيديو على باترون لقد كان قليلا بطيئا للتقدم في سلسلة الاحتمالات هذا الصيف",
  "n_reviews": 0,
  "start": 1010.76,
  "end": 1013.5
 },
 {
  "input": "I've been a little slow to progress in the probability series this summer, but I'm jumping back into it after this project, so patrons you can look out for updates there.",
  "translatedText": "",
  "from_community_srt": "ولكن أنا سأعود إليها  مرة أخرى في ذلك بعد هذا المشروع حتى ذلك  يمكنك البحث عن التحديثات هناك",
  "n_reviews": 0,
  "start": 1014.0,
  "end": 1021.9
 },
 {
  "input": "To close things off here I have with me Lisha Li who did her PhD work on the theoretical side of deep learning and who currently works at a venture capital firm called Amplify Partners who kindly provided some of the funding for this video.",
  "translatedText": "",
  "from_community_srt": "وللإنهاء هنا لدى هنا ليشا لى التى قامت بتأدية الدكتوراه فى الجانب النظرى من التعلم العميق والتى تعمل حاليا فى شركة استثمارية تسمى amplify partners",
  "n_reviews": 0,
  "start": 1023.6,
  "end": 1034.62
 },
 {
  "input": "So Lisha one thing I think we should quickly bring up is this sigmoid function.",
  "translatedText": "",
  "from_community_srt": "الذين تقدموا بتقديم بعض التمويل لهذا الفيديو لذا ليشا شئ ما أعتقد أنه يجب أن نذكره بسرعة هنا هو هذه الsigmoid function كما فهمت لقد استخدمت قديما لسحق الأرقام إلى رقم ما بين الصفر والواحد",
  "n_reviews": 0,
  "start": 1035.46,
  "end": 1039.12
 },
 {
  "input": "As I understand it early networks use this to squish the relevant weighted sum into that interval between zero and one, you know kind of motivated by this biological analogy of neurons either being inactive or active.",
  "translatedText": "",
  "from_community_srt": "محفزة نوعا ما بالخلايا العصبية الحقيقية التى إما تكون مفعلة أو غير مفعلة -بالضبط   * لكن الشبكات العصبية الحديثة لاتستخدمها بعد الآن هذا نوع من المدارس القديمة أليس كذلك ؟ - نعم",
  "n_reviews": 0,
  "start": 1039.7,
  "end": 1049.84
 },
 {
  "input": "Exactly.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1050.28,
  "end": 1050.3
 },
 {
  "input": "But relatively few modern networks actually use sigmoid anymore.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1050.56,
  "end": 1054.04
 },
 {
  "input": "Yeah.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1054.32,
  "end": 1054.32
 },
 {
  "input": "It's kind of old school right?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1054.44,
  "end": 1055.54
 },
 {
  "input": "Yeah or rather ReLU seems to be much easier to train.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1055.76,
  "end": 1058.98
 },
 {
  "input": "And ReLU, ReLU stands for rectified linear unit?",
  "translatedText": "",
  "from_community_srt": "Relu يبدو أسهل بكثير وRelu هو اختصار ل Really stands for rectified linear unit نعم إنها نوع من الدوال حيث تأخذ فقط القيمة الأكبر من ال a هى المفعلة والأقل تكون غير مفعلة",
  "n_reviews": 0,
  "start": 1059.4,
  "end": 1062.34
 },
 {
  "input": "Yes it's this kind of function where you're just taking a max of zero and a where a is given by what you were explaining in the video and what this was sort of motivated from I think was a partially by a biological analogy with how neurons would either be activated or not.",
  "translatedText": "",
  "from_community_srt": "كما كنت تشرح فى الفيديو وهى مستوحاة أيضا جزئيا من قبل البيولوجية الخلايا العصبية إما أن يتم تفعيلها أو لا، إذا مرت بعتبة معينة أو لا",
  "n_reviews": 0,
  "start": 1062.68,
  "end": 1081.36
 },
 {
  "input": "And so if it passes a certain threshold it would be the identity function but if it did not then it would just not be activated so it'd be zero so it's kind of a simplification.",
  "translatedText": "",
  "from_community_srt": "فإذا مرت تصبح مفعلة وإذا لم تمر تصبح غير مفعلة استخدام sigmoid جعل من الصعب جدا تدريبها أما Relu فهى أسهل فى التدريب",
  "n_reviews": 0,
  "start": 1081.36,
  "end": 1090.84
 },
 {
  "input": "Using sigmoids didn't help training or it was very difficult to train at some point and people just tried ReLU and it happened to work very well for these incredibly deep neural networks.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1091.16,
  "end": 1104.62
 },
 {
  "input": "All right thank you Lisha.",
  "translatedText": "",
  "from_community_srt": "شكرا ليشا",
  "n_reviews": 0,
  "start": 1105.1,
  "end": 1105.64
 }
]