1
00:00:04,220 --> 00:00:05,400
Este es un 3.

2
00:00:06,060 --> 00:00:09,964
Está escrito de forma chapucera y representado a una resolución extremadamente 

3
00:00:09,964 --> 00:00:13,720
baja de 28x28 píxeles, pero a tu cerebro no le cuesta reconocerlo como un 3.

4
00:00:14,340 --> 00:00:16,650
Y quiero que te tomes un momento para apreciar lo loco 

5
00:00:16,650 --> 00:00:18,960
que es que los cerebros puedan hacer esto sin esfuerzo.

6
00:00:19,700 --> 00:00:23,311
Es decir, esto, esto y esto también son reconocibles como 3s, 

7
00:00:23,311 --> 00:00:28,320
aunque los valores específicos de cada píxel sean muy diferentes de una imagen a otra.

8
00:00:28,900 --> 00:00:33,031
Las células sensibles a la luz concretas de tu ojo que se disparan cuando 

9
00:00:33,031 --> 00:00:36,940
ves este 3 son muy distintas de las que se disparan cuando ves este 3.

10
00:00:37,520 --> 00:00:42,890
Pero algo en tu corteza visual locamente inteligente resuelve que representan la 

11
00:00:42,890 --> 00:00:48,260
misma idea, al tiempo que reconoce otras imágenes como ideas propias y distintas.

12
00:00:49,220 --> 00:00:54,851
Pero si te dijera, oye, siéntate y escribe para mí un programa que tome una cuadrícula 

13
00:00:54,851 --> 00:00:59,059
de 28x28 píxeles como ésta y emita un único número entre 0 y 10, 

14
00:00:59,059 --> 00:01:04,432
diciéndote cuál cree que es el dígito, pues la tarea pasa de cómicamente trivial a 

15
00:01:04,432 --> 00:01:06,180
desalentadoramente difícil.

16
00:01:07,160 --> 00:01:09,099
A menos que hayas estado viviendo bajo una roca, 

17
00:01:09,099 --> 00:01:11,513
creo que apenas necesito motivar la relevancia e importancia 

18
00:01:11,513 --> 00:01:14,640
del aprendizaje automático y las redes neuronales para el presente y el futuro.

19
00:01:15,120 --> 00:01:18,467
Pero lo que quiero hacer aquí es mostrarte lo que es realmente una red neuronal, 

20
00:01:18,467 --> 00:01:21,980
suponiendo que no tengas conocimientos previos, y ayudarte a visualizar lo que hace, 

21
00:01:21,980 --> 00:01:24,460
no como una palabra de moda, sino como una pieza matemática.

22
00:01:25,020 --> 00:01:28,608
Mi esperanza es que salgas sintiendo que la propia estructura está motivada, 

23
00:01:28,608 --> 00:01:31,590
y que sientas que sabes lo que significa cuando lees o escuchas 

24
00:01:31,590 --> 00:01:34,340
hablar de una red neuronal, entre comillas, de aprendizaje.

25
00:01:35,360 --> 00:01:38,422
Este vídeo sólo va a estar dedicado al componente de estructura, 

26
00:01:38,422 --> 00:01:40,260
y el siguiente abordará el aprendizaje.

27
00:01:40,960 --> 00:01:43,423
Lo que vamos a hacer es montar una red neuronal 

28
00:01:43,423 --> 00:01:46,040
que pueda aprender a reconocer dígitos manuscritos.

29
00:01:49,360 --> 00:01:52,362
Se trata de un ejemplo un tanto clásico para introducir el tema, 

30
00:01:52,362 --> 00:01:55,827
y me complace ceñirme al statu quo aquí, porque al final de los dos vídeos 

31
00:01:55,827 --> 00:01:59,061
quiero indicarte un par de buenos recursos donde puedes aprender más, 

32
00:01:59,061 --> 00:02:03,080
y donde puedes descargar el código que hace esto y jugar con él en tu propio ordenador.

33
00:02:05,040 --> 00:02:09,865
Hay muchas variantes de redes neuronales, y en los últimos años ha habido una especie 

34
00:02:09,865 --> 00:02:12,558
de auge en la investigación de estas variantes, 

35
00:02:12,558 --> 00:02:17,328
pero en estos dos vídeos introductorios tú y yo sólo vamos a ver la forma más simple 

36
00:02:17,328 --> 00:02:19,180
y sencilla, sin adornos añadidos.

37
00:02:19,860 --> 00:02:22,531
Se trata de una especie de requisito previo necesario para 

38
00:02:22,531 --> 00:02:25,339
comprender cualquiera de las variantes modernas más potentes, 

39
00:02:25,339 --> 00:02:28,600
y créeme que aún tiene mucha complejidad para que nos hagamos a la idea.

40
00:02:29,120 --> 00:02:33,838
Pero incluso en su forma más simple puede aprender a reconocer dígitos escritos a mano, 

41
00:02:33,838 --> 00:02:36,520
lo cual es algo muy interesante para un ordenador.

42
00:02:37,480 --> 00:02:39,880
Y, al mismo tiempo, verás cómo no cumple un par 

43
00:02:39,880 --> 00:02:42,280
de esperanzas que podríamos tener puestas en él.

44
00:02:43,380 --> 00:02:47,166
Como su nombre indica, las redes neuronales se inspiran en el cerebro, 

45
00:02:47,166 --> 00:02:48,500
pero vamos a desglosarlo.

46
00:02:48,520 --> 00:02:51,660
¿Qué son las neuronas y en qué sentido están relacionadas entre sí?

47
00:02:52,500 --> 00:02:56,388
Ahora mismo, cuando digo neurona, lo único en lo que quiero que pienses 

48
00:02:56,388 --> 00:03:00,440
es en una cosa que contiene un número, concretamente un número entre 0 y 1.

49
00:03:00,680 --> 00:03:02,560
En realidad no es más que eso.

50
00:03:03,780 --> 00:03:09,000
Por ejemplo, la red empieza con un montón de neuronas correspondientes a cada uno 

51
00:03:09,000 --> 00:03:14,220
de los 28x28 píxeles de la imagen de entrada, lo que supone 784 neuronas en total.

52
00:03:14,700 --> 00:03:19,567
Cada uno de ellos contiene un número que representa el valor de la escala de grises del 

53
00:03:19,567 --> 00:03:24,380
píxel correspondiente, que va desde 0 para los píxeles negros hasta 1 para los blancos.

54
00:03:25,300 --> 00:03:28,270
Este número dentro de la neurona se llama su activación, 

55
00:03:28,270 --> 00:03:32,648
y la imagen que puedes tener en mente aquí es que cada neurona se ilumina cuando su 

56
00:03:32,648 --> 00:03:34,160
activación es un número alto.

57
00:03:36,720 --> 00:03:41,860
Así pues, todas estas 784 neuronas constituyen la primera capa de nuestra red.

58
00:03:46,500 --> 00:03:49,018
Saltando ahora a la última capa, ésta tiene 10 neuronas, 

59
00:03:49,018 --> 00:03:51,360
cada una de las cuales representa uno de los dígitos.

60
00:03:52,040 --> 00:03:56,450
La activación en estas neuronas, de nuevo algún número que está entre 0 y 1, 

61
00:03:56,450 --> 00:04:01,432
representa hasta qué punto el sistema piensa que una imagen dada se corresponde con un 

62
00:04:01,432 --> 00:04:02,120
dígito dado.

63
00:04:03,040 --> 00:04:06,453
También hay un par de capas intermedias llamadas capas ocultas, 

64
00:04:06,453 --> 00:04:09,813
que por el momento deberían ser un gran signo de interrogación 

65
00:04:09,813 --> 00:04:13,600
sobre cómo se va a gestionar este proceso de reconocimiento de dígitos.

66
00:04:14,260 --> 00:04:17,540
En esta red elegí dos capas ocultas, cada una con 16 neuronas, 

67
00:04:17,540 --> 00:04:20,560
y hay que admitir que es una elección un tanto arbitraria.

68
00:04:21,019 --> 00:04:24,631
Para ser sincero, elegí dos capas basándome en cómo quiero motivar la estructura en 

69
00:04:24,631 --> 00:04:28,200
un momento, y 16, bueno, era sólo un número bonito para que cupiera en la pantalla.

70
00:04:28,780 --> 00:04:32,340
En la práctica, aquí hay mucho margen para experimentar con una estructura específica.

71
00:04:33,020 --> 00:04:35,677
Según el funcionamiento de la red, las activaciones de 

72
00:04:35,677 --> 00:04:38,480
una capa determinan las activaciones de la capa siguiente.

73
00:04:39,200 --> 00:04:42,624
Y, por supuesto, el núcleo de la red como mecanismo de procesamiento 

74
00:04:42,624 --> 00:04:45,850
de la información se reduce exactamente a cómo esas activaciones 

75
00:04:45,850 --> 00:04:48,580
de una capa provocan activaciones en la capa siguiente.

76
00:04:49,140 --> 00:04:53,242
Pretende ser vagamente análogo a cómo en las redes biológicas de neuronas, 

77
00:04:53,242 --> 00:04:57,180
algunos grupos de neuronas que se disparan provocan el disparo de otras.

78
00:04:58,120 --> 00:05:01,241
Ahora bien, la red que muestro aquí ya ha sido entrenada para reconocer dígitos, 

79
00:05:01,241 --> 00:05:03,400
y permíteme que te muestre lo que quiero decir con ello.

80
00:05:03,640 --> 00:05:08,279
Significa que si introduces una imagen, iluminando las 784 neuronas de la capa 

81
00:05:08,279 --> 00:05:11,509
de entrada según el brillo de cada píxel de la imagen, 

82
00:05:11,509 --> 00:05:16,383
ese patrón de activaciones provoca algún patrón muy concreto en la capa siguiente, 

83
00:05:16,383 --> 00:05:21,022
que provoca algún patrón en la que le sigue, que finalmente da algún patrón en 

84
00:05:21,022 --> 00:05:22,080
la capa de salida.

85
00:05:22,560 --> 00:05:26,536
Y la neurona más brillante de esa capa de salida es la elección de la red, 

86
00:05:26,536 --> 00:05:29,400
por así decirlo, de qué dígito representa esta imagen.

87
00:05:32,560 --> 00:05:36,408
Y antes de entrar en las matemáticas de cómo una capa influye en la siguiente, 

88
00:05:36,408 --> 00:05:40,061
o cómo funciona el entrenamiento, hablemos de por qué es razonable esperar 

89
00:05:40,061 --> 00:05:43,520
que una estructura de capas como ésta se comporte de forma inteligente.

90
00:05:44,060 --> 00:05:45,220
¿Qué esperamos aquí?

91
00:05:45,400 --> 00:05:47,600
¿Cuál es la mejor esperanza para esas capas intermedias?

92
00:05:48,920 --> 00:05:53,520
Bien, cuando tú o yo reconocemos dígitos, unimos varios componentes.

93
00:05:54,200 --> 00:05:56,820
Un 9 tiene un bucle arriba y una línea a la derecha.

94
00:05:57,380 --> 00:06:01,180
Un 8 también tiene un bucle arriba, pero está emparejado con otro bucle abajo.

95
00:06:01,980 --> 00:06:06,820
Un 4 se divide básicamente en tres líneas específicas, y cosas por el estilo.

96
00:06:07,600 --> 00:06:11,549
Ahora bien, en un mundo perfecto, podríamos esperar que cada neurona de 

97
00:06:11,549 --> 00:06:15,168
la penúltima capa se corresponda con uno de estos subcomponentes, 

98
00:06:15,168 --> 00:06:18,953
que cada vez que alimentes una imagen con, digamos, un bucle arriba, 

99
00:06:18,953 --> 00:06:23,780
como un 9 o un 8, haya alguna neurona específica cuya activación vaya a ser cercana a 1.

100
00:06:24,500 --> 00:06:26,868
Y no me refiero a este bucle específico de píxeles, 

101
00:06:26,868 --> 00:06:30,557
la esperanza sería que cualquier patrón general de bucle hacia la parte superior 

102
00:06:30,557 --> 00:06:31,560
activara esta neurona.

103
00:06:32,440 --> 00:06:36,126
De este modo, pasar de la tercera capa a la última sólo requiere 

104
00:06:36,126 --> 00:06:40,040
aprender qué combinación de subcomponentes corresponde a qué dígitos.

105
00:06:41,000 --> 00:06:43,051
Por supuesto, eso no hace más que agravar el problema, 

106
00:06:43,051 --> 00:06:44,804
porque ¿cómo reconocerías esos subcomponentes, 

107
00:06:44,804 --> 00:06:47,640
o incluso cómo aprenderías cuáles deberían ser los subcomponentes correctos?

108
00:06:48,060 --> 00:06:51,327
Y todavía no he hablado de cómo una capa influye en la siguiente, 

109
00:06:51,327 --> 00:06:53,060
pero acompáñame en esto un momento.

110
00:06:53,680 --> 00:06:56,680
Reconocer un bucle también puede dividirse en subproblemas.

111
00:06:57,280 --> 00:07:00,059
Una forma razonable de hacerlo sería reconocer 

112
00:07:00,059 --> 00:07:02,780
primero las distintas aristas que lo componen.

113
00:07:03,780 --> 00:07:08,312
Del mismo modo, una línea larga, como las que puedes ver en los dígitos 1 ó 4 ó 7, 

114
00:07:08,312 --> 00:07:11,862
es en realidad una arista larga, o quizá pienses en ella como un 

115
00:07:11,862 --> 00:07:14,320
cierto patrón de varias aristas más pequeñas.

116
00:07:15,140 --> 00:07:19,008
Así que tal vez nuestra esperanza sea que cada neurona de la segunda capa 

117
00:07:19,008 --> 00:07:22,720
de la red se corresponda con las distintas pequeñas aristas relevantes.

118
00:07:23,540 --> 00:07:29,036
Quizá cuando entra una imagen como ésta, se iluminan todas las neuronas asociadas a unos 

119
00:07:29,036 --> 00:07:34,223
8 ó 10 pequeños bordes concretos, lo que a su vez ilumina las neuronas asociadas al 

120
00:07:34,223 --> 00:07:39,720
bucle superior y a una larga línea vertical, y éstas iluminan la neurona asociada a un 9.

121
00:07:40,680 --> 00:07:44,262
Si esto es o no lo que hace realmente nuestra red final es otra cuestión, 

122
00:07:44,262 --> 00:07:47,263
sobre la que volveré una vez que veamos cómo entrenar la red, 

123
00:07:47,263 --> 00:07:49,587
pero ésta es una esperanza que podríamos tener, 

124
00:07:49,587 --> 00:07:52,540
una especie de objetivo con la estructura en capas como ésta.

125
00:07:53,160 --> 00:07:56,705
Además, puedes imaginar que ser capaz de detectar bordes y patrones como 

126
00:07:56,705 --> 00:08:00,300
éste sería realmente útil para otras tareas de reconocimiento de imágenes.

127
00:08:00,880 --> 00:08:03,027
E incluso más allá del reconocimiento de imágenes, 

128
00:08:03,027 --> 00:08:06,269
hay todo tipo de cosas inteligentes que puedes querer hacer y que se dividen 

129
00:08:06,269 --> 00:08:07,280
en capas de abstracción.

130
00:08:08,040 --> 00:08:10,101
El análisis sintáctico del habla, por ejemplo, 

131
00:08:10,101 --> 00:08:13,128
consiste en tomar el audio en bruto y seleccionar sonidos distintos, 

132
00:08:13,128 --> 00:08:15,322
que se combinan para formar determinadas sílabas, 

133
00:08:15,322 --> 00:08:18,217
que se combinan para formar palabras, que se combinan para formar 

134
00:08:18,217 --> 00:08:20,060
frases y pensamientos más abstractos, etc.

135
00:08:21,100 --> 00:08:23,931
Pero volviendo a cómo funciona realmente todo esto, 

136
00:08:23,931 --> 00:08:28,177
imagínate ahora mismo diseñando cómo exactamente las activaciones de una capa 

137
00:08:28,177 --> 00:08:29,920
podrían determinar la siguiente.

138
00:08:30,860 --> 00:08:36,102
El objetivo es disponer de algún mecanismo que pueda combinar píxeles en aristas, 

139
00:08:36,102 --> 00:08:38,980
o aristas en patrones, o patrones en dígitos.

140
00:08:39,440 --> 00:08:45,030
Y para centrarnos en un ejemplo muy concreto, digamos que la esperanza es que una neurona 

141
00:08:45,030 --> 00:08:50,620
concreta de la segunda capa capte si la imagen tiene o no un borde en esta región de aquí.

142
00:08:51,440 --> 00:08:55,100
La cuestión que se plantea es ¿qué parámetros debe tener la red?

143
00:08:55,640 --> 00:08:59,799
¿Qué diales y mandos deberías poder ajustar para que sea lo suficientemente expresivo 

144
00:08:59,799 --> 00:09:03,717
como para captar potencialmente este patrón, o cualquier otro patrón de píxeles, 

145
00:09:03,717 --> 00:09:07,780
o el patrón de que varios bordes pueden hacer un bucle, y otras cosas por el estilo?

146
00:09:08,720 --> 00:09:11,951
Bien, lo que haremos será asignar un peso a cada una de las 

147
00:09:11,951 --> 00:09:15,560
conexiones entre nuestra neurona y las neuronas de la primera capa.

148
00:09:16,320 --> 00:09:17,700
Estos pesos son sólo números.

149
00:09:18,540 --> 00:09:21,796
A continuación, toma todas esas activaciones de la 

150
00:09:21,796 --> 00:09:25,500
primera capa y calcula su suma ponderada según esos pesos.

151
00:09:27,700 --> 00:09:31,148
Me resulta útil pensar en estos pesos como si estuvieran organizados en 

152
00:09:31,148 --> 00:09:34,787
una pequeña cuadrícula propia, y voy a utilizar píxeles verdes para indicar 

153
00:09:34,787 --> 00:09:37,805
pesos positivos, y píxeles rojos para indicar pesos negativos, 

154
00:09:37,805 --> 00:09:41,780
donde el brillo de ese píxel es una representación poco precisa del valor del peso.

155
00:09:42,780 --> 00:09:46,859
Ahora bien, si hacemos que los pesos asociados a casi todos los píxeles sean cero, 

156
00:09:46,859 --> 00:09:49,955
salvo algunos pesos positivos en esta región que nos interesa, 

157
00:09:49,955 --> 00:09:53,494
entonces tomar la suma ponderada de todos los valores de los píxeles en 

158
00:09:53,494 --> 00:09:57,820
realidad sólo equivale a sumar los valores del píxel sólo en la región que nos interesa.

159
00:09:59,140 --> 00:10:01,981
Y si realmente quisieras detectar si hay un borde aquí, 

160
00:10:01,981 --> 00:10:05,534
lo que podrías hacer es tener algunos pesos negativos asociados a los 

161
00:10:05,534 --> 00:10:06,600
píxeles circundantes.

162
00:10:07,480 --> 00:10:10,161
Entonces la suma es mayor cuando esos píxeles del medio 

163
00:10:10,161 --> 00:10:12,700
son brillantes pero los de alrededor son más oscuros.

164
00:10:14,260 --> 00:10:18,597
Cuando calculas una suma ponderada como ésta, puedes obtener cualquier número, 

165
00:10:18,597 --> 00:10:23,540
pero para esta red lo que queremos es que las activaciones tengan algún valor entre 0 y 1.

166
00:10:24,120 --> 00:10:27,953
Así que lo más habitual es bombear esta suma ponderada en alguna 

167
00:10:27,953 --> 00:10:32,140
función que aplaste la recta numérica real en el intervalo entre 0 y 1.

168
00:10:32,460 --> 00:10:35,590
Y una función habitual que hace esto se llama función sigmoidea, 

169
00:10:35,590 --> 00:10:37,420
también conocida como curva logística.

170
00:10:38,000 --> 00:10:41,050
Básicamente, las entradas muy negativas terminan cerca de 0, 

171
00:10:41,050 --> 00:10:45,300
las entradas positivas terminan cerca de 1, y simplemente aumenta de forma constante 

172
00:10:45,300 --> 00:10:46,600
alrededor de la entrada 0.

173
00:10:49,120 --> 00:10:52,585
Así que la activación de la neurona aquí es básicamente 

174
00:10:52,585 --> 00:10:56,360
una medida de lo positiva que es la suma ponderada relevante.

175
00:10:57,540 --> 00:10:59,664
Pero quizá no es que quieras que la neurona se 

176
00:10:59,664 --> 00:11:01,880
ilumine cuando la suma ponderada sea mayor que 0.

177
00:11:02,280 --> 00:11:06,360
Tal vez sólo quieras que se active cuando la suma sea mayor que, digamos, 10.

178
00:11:06,840 --> 00:11:10,260
Es decir, quieres algún sesgo para que esté inactivo.

179
00:11:11,380 --> 00:11:15,273
Lo que haremos entonces es añadir algún otro número, como 10 negativo, 

180
00:11:15,273 --> 00:11:19,660
a esta suma ponderada antes de pasarla por la función de aplastamiento sigmoide.

181
00:11:20,580 --> 00:11:22,440
Ese número adicional se llama sesgo.

182
00:11:23,460 --> 00:11:27,122
Así pues, los pesos te dicen qué patrón de píxeles está captando esta 

183
00:11:27,122 --> 00:11:30,941
neurona de la segunda capa, y el sesgo te dice lo alta que tiene que ser 

184
00:11:30,941 --> 00:11:35,180
la suma ponderada para que la neurona empiece a activarse de forma significativa.

185
00:11:36,120 --> 00:11:37,680
Y eso es sólo una neurona.

186
00:11:38,280 --> 00:11:44,284
Todas las demás neuronas de esta capa van a estar conectadas a las 784 neuronas de 

187
00:11:44,284 --> 00:11:50,578
píxeles de la primera capa, y cada una de esas 784 conexiones tiene asociado su propio 

188
00:11:50,578 --> 00:11:50,940
peso.

189
00:11:51,600 --> 00:11:54,600
Además, cada una tiene un sesgo, otro número que añades 

190
00:11:54,600 --> 00:11:57,600
a la suma ponderada antes de aplastarla con la sigmoide.

191
00:11:58,110 --> 00:11:59,540
¡Y eso es mucho en lo que pensar!

192
00:11:59,960 --> 00:12:06,255
Con esta capa oculta de 16 neuronas, hay un total de 784 veces 16 pesos, 

193
00:12:06,255 --> 00:12:07,980
junto con 16 sesgos.

194
00:12:08,840 --> 00:12:11,940
Y todo eso son sólo las conexiones de la primera capa a la segunda.

195
00:12:12,520 --> 00:12:17,340
Las conexiones entre las demás capas también tienen asociados un montón de pesos y sesgos.

196
00:12:18,340 --> 00:12:23,800
Dicho todo esto, esta red tiene casi exactamente 13.000 pesos y sesgos totales.

197
00:12:23,800 --> 00:12:26,909
13.000 mandos y diales que se pueden ajustar y girar 

198
00:12:26,909 --> 00:12:29,960
para que esta red se comporte de diferentes maneras.

199
00:12:31,040 --> 00:12:34,463
Así que, cuando hablamos de aprendizaje, nos referimos a conseguir que 

200
00:12:34,463 --> 00:12:38,707
el ordenador encuentre una configuración válida para todos estos muchos muchos números, 

201
00:12:38,707 --> 00:12:41,360
de modo que realmente resuelva el problema en cuestión.

202
00:12:42,620 --> 00:12:47,218
Un experimento mental que resulta divertido y horripilante a la vez es imaginar que 

203
00:12:47,218 --> 00:12:50,448
te sientas y estableces a mano todos estos pesos y sesgos, 

204
00:12:50,448 --> 00:12:54,663
ajustando a propósito los números para que la segunda capa capte los bordes, 

205
00:12:54,663 --> 00:12:56,580
la tercera capte los patrones, etc.

206
00:12:56,980 --> 00:13:01,559
Personalmente, esto me satisface más que tratar la red como una caja negra total, 

207
00:13:01,559 --> 00:13:04,295
porque cuando la red no funciona como esperabas, 

208
00:13:04,295 --> 00:13:08,539
si has construido una pequeña relación con lo que significan realmente esos 

209
00:13:08,539 --> 00:13:12,839
pesos y sesgos, tienes un punto de partida para experimentar cómo cambiar la 

210
00:13:12,839 --> 00:13:14,180
estructura para mejorar.

211
00:13:14,960 --> 00:13:18,119
O cuando la red funciona pero no por las razones que esperabas, 

212
00:13:18,119 --> 00:13:21,525
indagar en lo que hacen los pesos y los sesgos es una buena forma de 

213
00:13:21,525 --> 00:13:25,820
cuestionar tus suposiciones y exponer realmente todo el espacio de soluciones posibles.

214
00:13:26,840 --> 00:13:30,680
Por cierto, la función real aquí es un poco engorrosa de escribir, ¿no crees?

215
00:13:32,500 --> 00:13:37,140
Permíteme mostrarte una forma más compacta de representar estas conexiones.

216
00:13:37,660 --> 00:13:40,520
Así es como lo verías si decides leer más sobre las redes neuronales.

217
00:13:41,380 --> 00:13:47,387
Organiza todas las activaciones de una capa en una columna, 

218
00:13:47,387 --> 00:13:55,897
ya que una matriz corresponde a las conexiones entre una capa y una neurona concreta 

219
00:13:55,897 --> 00:13:58,000
de la capa siguiente.

220
00:13:58,540 --> 00:14:02,268
Lo que eso significa es que tomar la suma ponderada de las activaciones 

221
00:14:02,268 --> 00:14:05,944
de la primera capa según estos pesos corresponde a uno de los términos 

222
00:14:05,944 --> 00:14:09,880
del producto vectorial matricial de todo lo que tenemos aquí a la izquierda.

223
00:14:14,000 --> 00:14:17,743
Por cierto, gran parte del aprendizaje automático se reduce a tener una buena comprensión 

224
00:14:17,743 --> 00:14:21,403
del álgebra lineal, así que para cualquiera de vosotros que desee una buena comprensión 

225
00:14:21,403 --> 00:14:24,981
visual de las matrices y de lo que significa la multiplicación vectorial de matrices, 

226
00:14:24,981 --> 00:14:28,600
echad un vistazo a la serie que hice sobre álgebra lineal, especialmente el capítulo 3.

227
00:14:29,240 --> 00:14:33,503
Volviendo a nuestra expresión, en lugar de hablar de sumar el sesgo a cada uno 

228
00:14:33,503 --> 00:14:37,982
de esos valores independientemente, lo representamos organizando todos esos sesgos 

229
00:14:37,982 --> 00:14:42,300
en un vector, y sumando todo el vector al producto vectorial matricial anterior.

230
00:14:43,280 --> 00:14:46,984
Luego, como paso final, envolveré una sigmoidea por aquí fuera, 

231
00:14:46,984 --> 00:14:50,804
y lo que se supone que representa es que vas a aplicar la función 

232
00:14:50,804 --> 00:14:54,740
sigmoidea a cada componente específico del vector resultante dentro.

233
00:14:55,940 --> 00:15:00,272
Así, una vez que escribes esta matriz de pesos y estos vectores como símbolos propios, 

234
00:15:00,272 --> 00:15:04,106
puedes comunicar la transición completa de las activaciones de una capa a la 

235
00:15:04,106 --> 00:15:07,642
siguiente en una pequeña expresión extremadamente ajustada y ordenada, 

236
00:15:07,642 --> 00:15:11,875
y esto hace que el código correspondiente sea mucho más sencillo y mucho más rápido, 

237
00:15:11,875 --> 00:15:15,660
ya que muchas bibliotecas optimizan muchísimo la multiplicación de matrices.

238
00:15:17,820 --> 00:15:21,460
¿Recuerdas que antes dije que estas neuronas son simplemente cosas que contienen números?

239
00:15:22,220 --> 00:15:27,780
Por supuesto, los números concretos que contienen dependen de la imagen que introduzcas, 

240
00:15:27,780 --> 00:15:32,466
así que en realidad es más exacto pensar en cada neurona como una función, 

241
00:15:32,466 --> 00:15:37,965
que toma las salidas de todas las neuronas de la capa anterior y escupe un número entre 

242
00:15:37,965 --> 00:15:38,340
0 y 1.

243
00:15:39,200 --> 00:15:42,663
En realidad, toda la red no es más que una función, 

244
00:15:42,663 --> 00:15:47,060
que toma 784 números como entrada y escupe 10 números como salida.

245
00:15:47,560 --> 00:15:52,155
Es una función absurdamente complicada, en la que intervienen 13.000 parámetros en 

246
00:15:52,155 --> 00:15:55,919
forma de ponderaciones y sesgos que detectan determinados patrones, 

247
00:15:55,919 --> 00:16:00,237
y que implica iterar muchos productos vectoriales matriciales y la función de 

248
00:16:00,237 --> 00:16:04,390
aplastamiento sigmoide, pero no deja de ser una función y, en cierto modo, 

249
00:16:04,390 --> 00:16:06,660
es tranquilizador que parezca complicada.

250
00:16:07,340 --> 00:16:09,989
Es decir, si fuera más sencillo, ¿qué esperanza tendríamos 

251
00:16:09,989 --> 00:16:12,280
de que pudiera asumir el reto de reconocer dígitos?

252
00:16:13,340 --> 00:16:14,700
¿Y cómo asume ese reto?

253
00:16:15,080 --> 00:16:19,360
¿Cómo aprende esta red los pesos y sesgos adecuados con sólo mirar los datos?

254
00:16:20,140 --> 00:16:22,200
Pues eso es lo que mostraré en el siguiente vídeo, 

255
00:16:22,200 --> 00:16:25,352
y también profundizaré un poco más en lo que hace realmente esta red concreta 

256
00:16:25,352 --> 00:16:26,120
que estamos viendo.

257
00:16:27,580 --> 00:16:30,799
Ahora es cuando supongo que debería decir que te suscribas para que te 

258
00:16:30,799 --> 00:16:34,291
avisen cuando salga un vídeo o cualquier vídeo nuevo, pero siendo realistas, 

259
00:16:34,291 --> 00:16:37,420
la mayoría de vosotros no recibís notificaciones de YouTube, ¿verdad?

260
00:16:38,020 --> 00:16:41,126
Tal vez debería decir más honestamente suscríbete para que las redes 

261
00:16:41,126 --> 00:16:44,323
neuronales que subyacen al algoritmo de recomendación de YouTube estén 

262
00:16:44,323 --> 00:16:47,880
preparadas para creer que quieres que se te recomiende contenido de este canal.

263
00:16:48,560 --> 00:16:49,940
En cualquier caso, mantente informado para saber más.

264
00:16:50,760 --> 00:16:53,500
Muchas gracias a todos los que apoyáis estos vídeos en Patreon.

265
00:16:54,000 --> 00:16:57,072
Este verano he avanzado un poco despacio en la serie de probabilidad, 

266
00:16:57,072 --> 00:16:59,135
pero voy a retomarla después de este proyecto, 

267
00:16:59,135 --> 00:17:01,900
así que los mecenas podéis estar atentos a las actualizaciones.

268
00:17:03,600 --> 00:17:05,495
Para terminar, tengo aquí conmigo a Leisha Lee, 

269
00:17:05,495 --> 00:17:08,339
que hizo su doctorado sobre el aspecto teórico del aprendizaje profundo 

270
00:17:08,339 --> 00:17:11,697
y que actualmente trabaja en una empresa de capital riesgo llamada Amplify Partners, 

271
00:17:11,697 --> 00:17:14,619
que amablemente ha proporcionado parte de la financiación para este vídeo.

272
00:17:15,460 --> 00:17:17,308
Así que Leisha, una cosa que creo que deberíamos 

273
00:17:17,308 --> 00:17:19,119
mencionar rápidamente es esta función sigmoidea.

274
00:17:19,700 --> 00:17:23,079
Según tengo entendido, las redes primitivas utilizan esto para aplastar la suma 

275
00:17:23,079 --> 00:17:25,826
ponderada relevante en ese intervalo entre cero y uno, ya sabes, 

276
00:17:25,826 --> 00:17:28,995
algo motivado por esta analogía biológica de que las neuronas pueden estar 

277
00:17:28,995 --> 00:17:29,840
inactivas o activas.

278
00:17:30,280 --> 00:17:30,300
Exacto.

279
00:17:30,560 --> 00:17:34,040
Pero relativamente pocas redes modernas utilizan ya la sigmoidea.

280
00:17:34,320 --> 00:17:34,320
Sí.

281
00:17:34,440 --> 00:17:35,540
Es un poco de la vieja escuela, ¿verdad?

282
00:17:35,760 --> 00:17:38,980
Sí o más bien relu parece ser mucho más fácil de entrenar.

283
00:17:39,400 --> 00:17:42,340
¿Y relu significa unidad lineal rectificada?

284
00:17:42,680 --> 00:17:47,828
Sí, es una especie de función en la que simplemente tomas un máximo de cero y a, 

285
00:17:47,828 --> 00:17:51,261
donde a viene dado por lo que explicabas en el vídeo, 

286
00:17:51,261 --> 00:17:56,855
y creo que esto estaba motivado en parte por una analogía biológica con la forma en que 

287
00:17:56,855 --> 00:18:01,686
las neuronas se activan o no, de modo que si superan un determinado umbral, 

288
00:18:01,686 --> 00:18:06,708
sería la función de identidad, pero si no lo hacen, simplemente no se activan, 

289
00:18:06,708 --> 00:18:10,840
de modo que sería cero, así que es una especie de simplificación.

290
00:18:11,160 --> 00:18:15,646
Utilizar sigmoides no ayudaba a entrenar o era muy difícil de entrenar 

291
00:18:15,646 --> 00:18:20,006
en algún momento y la gente simplemente probó con relu y resultó que 

292
00:18:20,006 --> 00:18:24,620
funcionaba muy bien para estas redes neuronales increíblemente profundas.

293
00:18:25,100 --> 00:18:25,640
Muy bien, gracias Alicia.

