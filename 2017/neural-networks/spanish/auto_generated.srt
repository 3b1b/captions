1
00:00:04,220 --> 00:00:05,400
Este es un 3.

2
00:00:06,060 --> 00:00:10,553
Está mal escrito y renderizado con una resolución extremadamente baja de 28x28 píxeles, 

3
00:00:10,553 --> 00:00:13,720
pero tu cerebro no tiene problemas para reconocerlo como un 3.

4
00:00:14,340 --> 00:00:16,650
Y quiero que te tomes un momento para apreciar lo loco 

5
00:00:16,650 --> 00:00:18,960
que es que los cerebros puedan hacer esto sin esfuerzo.

6
00:00:19,700 --> 00:00:23,435
Quiero decir, esto, esto y esto también son reconocibles como 3, 

7
00:00:23,435 --> 00:00:28,320
aunque los valores específicos de cada píxel son muy diferentes de una imagen a otra.

8
00:00:28,900 --> 00:00:33,085
Las células sensibles a la luz particulares de tu ojo que se activan cuando 

9
00:00:33,085 --> 00:00:36,940
ves este 3 son muy diferentes de las que se activan cuando ves este 3.

10
00:00:37,520 --> 00:00:41,137
Pero algo en esa increíblemente inteligente corteza visual tuya 

11
00:00:41,137 --> 00:00:44,642
resuelve que representan la misma idea, mientras que al mismo 

12
00:00:44,642 --> 00:00:48,260
tiempo reconoce otras imágenes como sus propias ideas distintas.

13
00:00:49,220 --> 00:00:53,337
Pero si te dijera, oye, siéntate y escribe para mí un programa que 

14
00:00:53,337 --> 00:00:58,621
tome una cuadrícula de 28x28 píxeles como esta y genere un único número entre 0 y 10, 

15
00:00:58,621 --> 00:01:02,923
diciéndote cuál cree que es el dígito, bueno, la tarea va de desde lo 

16
00:01:02,923 --> 00:01:06,180
cómicamente trivial hasta lo abrumadoramente difícil.

17
00:01:07,160 --> 00:01:09,109
A menos que hayas estado viviendo bajo una roca, 

18
00:01:09,109 --> 00:01:11,496
creo que no necesito motivar la relevancia y la importancia 

19
00:01:11,496 --> 00:01:14,640
del aprendizaje automático y las redes neuronales para el presente y el futuro.

20
00:01:15,120 --> 00:01:18,616
Pero lo que quiero hacer aquí es mostrarles qué es realmente una red neuronal, 

21
00:01:18,616 --> 00:01:21,848
sin asumir ningún trasfondo, y ayudar a visualizar lo que está haciendo, 

22
00:01:21,848 --> 00:01:24,460
no como una palabra de moda sino como una pieza matemática.

23
00:01:25,020 --> 00:01:28,126
Mi esperanza es que usted salga sintiendo que la estructura en sí 

24
00:01:28,126 --> 00:01:31,233
está motivada y que sienta que sabe lo que significa cuando lee o 

25
00:01:31,233 --> 00:01:34,340
escucha acerca del aprendizaje entre comillas de una red neuronal.

26
00:01:35,360 --> 00:01:38,457
Este video solo estará dedicado al componente estructural de esto, 

27
00:01:38,457 --> 00:01:40,260
y el siguiente abordará el aprendizaje.

28
00:01:40,960 --> 00:01:43,500
Lo que vamos a hacer es armar una red neuronal que 

29
00:01:43,500 --> 00:01:46,040
pueda aprender a reconocer dígitos escritos a mano.

30
00:01:49,360 --> 00:01:52,113
Este es un ejemplo un tanto clásico para presentar el tema, 

31
00:01:52,113 --> 00:01:54,315
y estoy feliz de seguir con el status quo aquí, 

32
00:01:54,315 --> 00:01:57,848
porque al final de los dos videos quiero indicarle un par de buenos recursos 

33
00:01:57,848 --> 00:02:01,244
donde puede aprender más y donde puedes descargar el código que hace esto 

34
00:02:01,244 --> 00:02:03,080
y jugar con él en tu propia computadora.

35
00:02:05,040 --> 00:02:09,827
Hay muchas variantes de redes neuronales, y en los últimos años ha habido una especie 

36
00:02:09,827 --> 00:02:12,666
de auge en la investigación sobre estas variantes, 

37
00:02:12,666 --> 00:02:17,342
pero en estos dos videos introductorios usted y yo solo veremos la forma más simple 

38
00:02:17,342 --> 00:02:19,180
y simple sin adornos adicionales.

39
00:02:19,860 --> 00:02:23,909
Este es un requisito previo necesario para comprender cualquiera de las variantes 

40
00:02:23,909 --> 00:02:28,056
modernas más poderosas y, créanme, todavía tiene mucha complejidad para que podamos 

41
00:02:28,056 --> 00:02:28,600
entenderlo.

42
00:02:29,120 --> 00:02:33,777
Pero incluso en esta forma más simple puede aprender a reconocer dígitos escritos a mano, 

43
00:02:33,777 --> 00:02:36,520
algo muy interesante que puede hacer una computadora.

44
00:02:37,480 --> 00:02:39,779
Y al mismo tiempo verán cómo no cumple con un 

45
00:02:39,779 --> 00:02:42,280
par de esperanzas que podríamos tener al respecto.

46
00:02:43,380 --> 00:02:47,444
Como sugiere el nombre, las redes neuronales están inspiradas en el cerebro, 

47
00:02:47,444 --> 00:02:48,500
pero analicemos eso.

48
00:02:48,520 --> 00:02:51,660
¿Qué son las neuronas y en qué sentido están unidas entre sí?

49
00:02:52,500 --> 00:02:56,440
Ahora, cuando digo neurona, lo único que quiero que piensen es en 

50
00:02:56,440 --> 00:03:00,440
algo que contiene un número, específicamente un número entre 0 y 1.

51
00:03:00,680 --> 00:03:02,560
Realmente no es más que eso.

52
00:03:03,780 --> 00:03:09,000
Por ejemplo, la red comienza con un grupo de neuronas correspondientes a cada uno 

53
00:03:09,000 --> 00:03:14,220
de los 28x28 píxeles de la imagen de entrada, lo que supone 784 neuronas en total.

54
00:03:14,700 --> 00:03:19,396
Cada uno de ellos tiene un número que representa el valor en escala de grises del 

55
00:03:19,396 --> 00:03:24,380
píxel correspondiente, que va desde 0 para píxeles negros hasta 1 para píxeles blancos.

56
00:03:25,300 --> 00:03:28,147
Este número dentro de la neurona se llama activación, 

57
00:03:28,147 --> 00:03:32,630
y la imagen que quizás tengas en mente aquí es que cada neurona se ilumina cuando su 

58
00:03:32,630 --> 00:03:34,160
activación es un número alto.

59
00:03:36,720 --> 00:03:41,860
Entonces, todas estas 784 neuronas constituyen la primera capa de nuestra red.

60
00:03:46,500 --> 00:03:48,906
Ahora saltando a la última capa, tiene 10 neuronas, 

61
00:03:48,906 --> 00:03:51,360
cada una de las cuales representa uno de los dígitos.

62
00:03:52,040 --> 00:03:56,513
La activación en estas neuronas, nuevamente algún número que está entre 0 y 1, 

63
00:03:56,513 --> 00:04:01,440
representa cuánto piensa el sistema que una imagen determinada corresponde a un dígito 

64
00:04:01,440 --> 00:04:02,120
determinado.

65
00:04:03,040 --> 00:04:06,289
También hay un par de capas intermedias llamadas capas ocultas, 

66
00:04:06,289 --> 00:04:09,893
que por el momento deberían ser solo un signo de interrogación gigante 

67
00:04:09,893 --> 00:04:13,600
sobre cómo diablos se manejará este proceso de reconocimiento de dígitos.

68
00:04:14,260 --> 00:04:17,652
En esta red elegí dos capas ocultas, cada una con 16 neuronas, 

69
00:04:17,652 --> 00:04:20,560
y hay que admitir que es una elección algo arbitraria.

70
00:04:21,019 --> 00:04:25,196
Para ser honesto, elegí dos capas según cómo quiero motivar la estructura en un momento, 

71
00:04:25,196 --> 00:04:28,200
y 16, bueno, ese fue un buen número para encajar en la pantalla.

72
00:04:28,780 --> 00:04:32,340
En la práctica, aquí hay mucho espacio para experimentar con una estructura específica.

73
00:04:33,020 --> 00:04:35,702
Según la forma en que opera la red, las activaciones en 

74
00:04:35,702 --> 00:04:38,480
una capa determinan las activaciones de la siguiente capa.

75
00:04:39,200 --> 00:04:42,008
Y, por supuesto, el corazón de la red como mecanismo de 

76
00:04:42,008 --> 00:04:45,169
procesamiento de información se reduce exactamente a cómo esas 

77
00:04:45,169 --> 00:04:48,580
activaciones de una capa provocan activaciones en la siguiente capa.

78
00:04:49,140 --> 00:04:53,431
Se supone que es vagamente análogo a cómo en las redes biológicas de neuronas, 

79
00:04:53,431 --> 00:04:57,180
algunos grupos de neuronas que se activan hacen que otros se activen.

80
00:04:58,120 --> 00:05:00,817
Ahora bien, la red que estoy mostrando aquí ya ha sido entrenada para 

81
00:05:00,817 --> 00:05:03,400
reconocer dígitos y déjenme mostrarles lo que quiero decir con eso.

82
00:05:03,640 --> 00:05:08,250
Significa que si alimentas una imagen, iluminando las 784 neuronas de la capa de 

83
00:05:08,250 --> 00:05:11,721
entrada de acuerdo con el brillo de cada píxel de la imagen, 

84
00:05:11,721 --> 00:05:16,331
ese patrón de activaciones provoca un patrón muy específico en la siguiente capa 

85
00:05:16,331 --> 00:05:20,884
que provoca algún patrón en la siguiente. él, lo que finalmente da algún patrón 

86
00:05:20,884 --> 00:05:22,080
en la capa de salida.

87
00:05:22,560 --> 00:05:26,536
Y la neurona más brillante de esa capa de salida es la elección de la red, 

88
00:05:26,536 --> 00:05:29,400
por así decirlo, de qué dígito representa esta imagen.

89
00:05:32,560 --> 00:05:36,291
Y antes de pasar a las matemáticas sobre cómo una capa influye en la siguiente, 

90
00:05:36,291 --> 00:05:39,788
o cómo funciona el entrenamiento, hablemos de por qué es incluso razonable 

91
00:05:39,788 --> 00:05:43,520
esperar que una estructura en capas como esta se comporte de manera inteligente.

92
00:05:44,060 --> 00:05:45,220
¿Qué esperamos aquí?

93
00:05:45,400 --> 00:05:47,600
¿Cuál es la mejor esperanza para esas capas intermedias?

94
00:05:48,920 --> 00:05:53,520
Bueno, cuando tú o yo reconocemos dígitos, juntamos varios componentes.

95
00:05:54,200 --> 00:05:56,820
Un 9 tiene un bucle arriba y una línea a la derecha.

96
00:05:57,380 --> 00:06:01,180
Un 8 también tiene un bucle arriba, pero está emparejado con otro bucle abajo.

97
00:06:01,980 --> 00:06:06,820
Un 4 básicamente se divide en tres líneas específicas y cosas así.

98
00:06:07,600 --> 00:06:11,350
Ahora, en un mundo perfecto, podríamos esperar que cada neurona en la 

99
00:06:11,350 --> 00:06:14,725
penúltima capa se corresponda con uno de estos subcomponentes, 

100
00:06:14,725 --> 00:06:19,279
que cada vez que introduzcas una imagen con, digamos, un bucle en la parte superior, 

101
00:06:19,279 --> 00:06:23,780
como un 9 o un 8, haya algo Neurona específica cuya activación va a ser cercana a 1.

102
00:06:24,500 --> 00:06:26,823
Y no me refiero a este bucle específico de píxeles, 

103
00:06:26,823 --> 00:06:30,264
la esperanza sería que cualquier patrón generalmente bucleado hacia la parte 

104
00:06:30,264 --> 00:06:31,560
superior active esta neurona.

105
00:06:32,440 --> 00:06:36,155
De esa manera, pasar de la tercera capa a la última solo requiere 

106
00:06:36,155 --> 00:06:40,040
aprender qué combinación de subcomponentes corresponde a qué dígitos.

107
00:06:41,000 --> 00:06:42,991
Por supuesto, eso simplemente retrasa el problema, 

108
00:06:42,991 --> 00:06:44,866
porque ¿cómo reconocerías estos subcomponentes, 

109
00:06:44,866 --> 00:06:47,640
o incluso aprenderías cuáles deberían ser los subcomponentes correctos?

110
00:06:48,060 --> 00:06:51,230
Y todavía ni siquiera he hablado sobre cómo una capa influye en la siguiente, 

111
00:06:51,230 --> 00:06:53,060
pero habla conmigo sobre esto por un momento.

112
00:06:53,680 --> 00:06:56,680
Reconocer un bucle también puede dividirse en subproblemas.

113
00:06:57,280 --> 00:06:59,839
Una forma razonable de hacerlo sería reconocer 

114
00:06:59,839 --> 00:07:02,780
primero los distintos pequeños bordes que lo componen.

115
00:07:03,780 --> 00:07:07,817
De manera similar, una línea larga, como la que puedes ver en los dígitos 1, 

116
00:07:07,817 --> 00:07:11,331
4 o 7, en realidad es solo un borde largo, o tal vez la consideres 

117
00:07:11,331 --> 00:07:14,320
como un patrón determinado de varios bordes más pequeños.

118
00:07:15,140 --> 00:07:18,851
Entonces, tal vez nuestra esperanza sea que cada neurona en la segunda 

119
00:07:18,851 --> 00:07:22,720
capa de la red se corresponda con los diversos pequeños bordes relevantes.

120
00:07:23,540 --> 00:07:27,807
Tal vez cuando aparece una imagen como esta, ilumina todas las neuronas 

121
00:07:27,807 --> 00:07:31,541
asociadas con alrededor de 8 a 10 pequeños bordes específicos, 

122
00:07:31,541 --> 00:07:35,512
que a su vez iluminan las neuronas asociadas con el bucle superior 

123
00:07:35,512 --> 00:07:39,720
y una larga línea vertical, y esas iluminan el Neurona asociada a un 9.

124
00:07:40,680 --> 00:07:44,306
Si esto es o no lo que nuestra red final realmente hace es otra cuestión, 

125
00:07:44,306 --> 00:07:47,149
a la que volveré una vez que veamos cómo entrenar la red, 

126
00:07:47,149 --> 00:07:49,501
pero esta es una esperanza que podríamos tener, 

127
00:07:49,501 --> 00:07:52,540
una especie de objetivo con la estructura en capas. como esto.

128
00:07:53,160 --> 00:07:56,730
Además, puedes imaginar cómo ser capaz de detectar bordes y patrones como 

129
00:07:56,730 --> 00:08:00,300
este sería realmente útil para otras tareas de reconocimiento de imágenes.

130
00:08:00,880 --> 00:08:03,013
E incluso más allá del reconocimiento de imágenes, 

131
00:08:03,013 --> 00:08:06,276
hay todo tipo de cosas inteligentes que quizás quieras hacer y que se dividen 

132
00:08:06,276 --> 00:08:07,280
en capas de abstracción.

133
00:08:08,040 --> 00:08:11,823
Analizar el habla, por ejemplo, implica tomar audio sin procesar y seleccionar 

134
00:08:11,823 --> 00:08:14,888
sonidos distintos, que se combinan para formar ciertas sílabas, 

135
00:08:14,888 --> 00:08:19,102
que se combinan para formar palabras, que se combinan para formar frases y pensamientos 

136
00:08:19,102 --> 00:08:20,060
más abstractos, etc.

137
00:08:21,100 --> 00:08:23,931
Pero volviendo a cómo funciona realmente todo esto, 

138
00:08:23,931 --> 00:08:28,177
imagínese ahora mismo diseñando cómo exactamente las activaciones en una capa 

139
00:08:28,177 --> 00:08:29,920
podrían determinar la siguiente.

140
00:08:30,860 --> 00:08:36,273
El objetivo es tener algún mecanismo que posiblemente pueda combinar píxeles en bordes, 

141
00:08:36,273 --> 00:08:38,980
o bordes en patrones, o patrones en dígitos.

142
00:08:39,440 --> 00:08:44,967
Y para ampliar un ejemplo muy específico, digamos que la esperanza es que una neurona en 

143
00:08:44,967 --> 00:08:50,309
particular en la segunda capa detecte si la imagen tiene o no un borde en esta región 

144
00:08:50,309 --> 00:08:50,620
aquí.

145
00:08:51,440 --> 00:08:55,100
La pregunta que nos ocupa es ¿qué parámetros debe tener la red?

146
00:08:55,640 --> 00:08:59,891
¿Qué diales y perillas deberías poder ajustar para que sean lo suficientemente expresivos 

147
00:08:59,891 --> 00:09:03,812
como para capturar potencialmente este patrón, o cualquier otro patrón de píxeles, 

148
00:09:03,812 --> 00:09:07,780
o el patrón en el que varios bordes pueden formar un bucle, y otras cosas similares?

149
00:09:08,720 --> 00:09:11,979
Bueno, lo que haremos será asignar un peso a cada una de las 

150
00:09:11,979 --> 00:09:15,560
conexiones entre nuestra neurona y las neuronas de la primera capa.

151
00:09:16,320 --> 00:09:17,700
Estos pesos son sólo números.

152
00:09:18,540 --> 00:09:21,988
Luego tome todas esas activaciones de la primera capa 

153
00:09:21,988 --> 00:09:25,500
y calcule su suma ponderada de acuerdo con estos pesos.

154
00:09:27,700 --> 00:09:32,076
Me resulta útil pensar que estos pesos están organizados en una pequeña cuadrícula 

155
00:09:32,076 --> 00:09:36,612
propia, y voy a usar píxeles verdes para indicar pesos positivos y píxeles rojos para 

156
00:09:36,612 --> 00:09:41,305
indicar pesos negativos, donde el brillo de ese píxel es algo descripción vaga del valor 

157
00:09:41,305 --> 00:09:41,780
del peso.

158
00:09:42,780 --> 00:09:47,042
Ahora bien, si hicimos que los pesos asociados con casi todos los píxeles fueran cero, 

159
00:09:47,042 --> 00:09:50,226
excepto algunos pesos positivos en esta región que nos interesa, 

160
00:09:50,226 --> 00:09:53,753
entonces tomar la suma ponderada de todos los valores de los píxeles en 

161
00:09:53,753 --> 00:09:57,820
realidad equivale a sumar los valores del píxel justo en la región que nos importa.

162
00:09:59,140 --> 00:10:01,931
Y si realmente quisiera saber si hay una ventaja aquí, 

163
00:10:01,931 --> 00:10:05,534
lo que podría hacer es tener algunos pesos negativos asociados con los 

164
00:10:05,534 --> 00:10:06,600
píxeles circundantes.

165
00:10:07,480 --> 00:10:09,978
Entonces la suma es mayor cuando esos píxeles del medio 

166
00:10:09,978 --> 00:10:12,700
son brillantes pero los píxeles circundantes son más oscuros.

167
00:10:14,260 --> 00:10:18,538
Cuando calcula una suma ponderada como esta, puede obtener cualquier número, 

168
00:10:18,538 --> 00:10:23,540
pero para esta red lo que queremos es que las activaciones tengan algún valor entre 0 y 1.

169
00:10:24,120 --> 00:10:28,069
Entonces, una cosa común es bombear esta suma ponderada a alguna 

170
00:10:28,069 --> 00:10:32,140
función que aplaste la recta numérica real en el rango entre 0 y 1.

171
00:10:32,460 --> 00:10:35,535
Y una función común que hace esto se llama función sigmoidea, 

172
00:10:35,535 --> 00:10:37,420
también conocida como curva logística.

173
00:10:38,000 --> 00:10:41,362
Básicamente, las entradas muy negativas terminan cerca de 0, 

174
00:10:41,362 --> 00:10:45,717
las entradas positivas terminan cerca de 1 y aumentan constantemente alrededor 

175
00:10:45,717 --> 00:10:46,600
de la entrada 0.

176
00:10:49,120 --> 00:10:52,619
Entonces, la activación de la neurona aquí es básicamente 

177
00:10:52,619 --> 00:10:56,360
una medida de qué tan positiva es la suma ponderada relevante.

178
00:10:57,540 --> 00:10:59,710
Pero tal vez no sea que quieras que la neurona se 

179
00:10:59,710 --> 00:11:01,880
encienda cuando la suma ponderada sea mayor que 0.

180
00:11:02,280 --> 00:11:06,360
Tal vez solo quieras que esté activo cuando la suma sea mayor que, por ejemplo, 10.

181
00:11:06,840 --> 00:11:10,260
Es decir, desea algún sesgo para que esté inactivo.

182
00:11:11,380 --> 00:11:15,618
Lo que haremos entonces es simplemente agregar algún otro número como menos 10 a esta 

183
00:11:15,618 --> 00:11:19,660
suma ponderada antes de conectarlo a través de la función de compresión sigmoidea.

184
00:11:20,580 --> 00:11:22,440
Ese número adicional se llama sesgo.

185
00:11:23,460 --> 00:11:27,504
Entonces, los pesos le dicen qué patrón de píxeles está captando esta neurona 

186
00:11:27,504 --> 00:11:31,601
en la segunda capa, y el sesgo le dice qué tan alta debe ser la suma ponderada 

187
00:11:31,601 --> 00:11:35,180
antes de que la neurona comience a activarse de manera significativa.

188
00:11:36,120 --> 00:11:37,680
Y esa es sólo una neurona.

189
00:11:38,280 --> 00:11:44,687
Cada dos neuronas en esta capa estarán conectadas a las neuronas de 784 píxeles de 

190
00:11:44,687 --> 00:11:50,940
la primera capa, y cada una de esas 784 conexiones tiene su propio peso asociado.

191
00:11:51,600 --> 00:11:54,600
Además, cada uno tiene algún sesgo, algún otro número que se 

192
00:11:54,600 --> 00:11:57,600
suma a la suma ponderada antes de aplastarlo con el sigmoide.

193
00:11:58,110 --> 00:11:59,540
¡Y eso es mucho en qué pensar!

194
00:11:59,960 --> 00:12:06,309
Con esta capa oculta de 16 neuronas, eso es un total de 784 veces 16 pesos, 

195
00:12:06,309 --> 00:12:07,980
junto con 16 sesgos.

196
00:12:08,840 --> 00:12:11,940
Y todo eso son sólo las conexiones de la primera capa a la segunda.

197
00:12:12,520 --> 00:12:17,340
Las conexiones entre las otras capas también tienen muchos pesos y sesgos asociados.

198
00:12:18,340 --> 00:12:23,800
Dicho y hecho, esta red tiene casi exactamente 13.000 pesos y sesgos totales.

199
00:12:23,800 --> 00:12:26,966
13.000 perillas y diales que se pueden ajustar y girar 

200
00:12:26,966 --> 00:12:29,960
para que esta red se comporte de diferentes maneras.

201
00:12:31,040 --> 00:12:34,400
Entonces, cuando hablamos de aprendizaje, a lo que nos referimos es a 

202
00:12:34,400 --> 00:12:37,856
lograr que la computadora encuentre una configuración válida para todos 

203
00:12:37,856 --> 00:12:41,360
estos muchos números para que realmente resuelva el problema en cuestión.

204
00:12:42,620 --> 00:12:47,109
Un experimento mental que es a la vez divertido y un poco aterrador es imaginarse 

205
00:12:47,109 --> 00:12:50,284
sentado y configurando todos estos pesos y sesgos a mano, 

206
00:12:50,284 --> 00:12:54,444
ajustando los números a propósito para que la segunda capa tome los bordes, 

207
00:12:54,444 --> 00:12:56,580
la tercera capa tome los patrones, etc.

208
00:12:56,980 --> 00:13:01,368
Personalmente, encuentro esto satisfactorio en lugar de simplemente tratar la red como 

209
00:13:01,368 --> 00:13:05,302
una caja negra total, porque cuando la red no funciona de la manera esperada, 

210
00:13:05,302 --> 00:13:09,489
si has construido un poco de relación con lo que realmente significan esos pesos y 

211
00:13:09,489 --> 00:13:13,776
sesgos , tiene un punto de partida para experimentar cómo cambiar la estructura para 

212
00:13:13,776 --> 00:13:14,180
mejorar.

213
00:13:14,960 --> 00:13:18,176
O cuando la red funciona pero no por las razones que cabría esperar, 

214
00:13:18,176 --> 00:13:21,718
profundizar en lo que están haciendo los pesos y sesgos es una buena manera 

215
00:13:21,718 --> 00:13:25,820
de desafiar sus suposiciones y exponer realmente todo el espacio de posibles soluciones.

216
00:13:26,840 --> 00:13:30,680
Por cierto, la función real aquí es un poco engorrosa de escribir, ¿no crees?

217
00:13:32,500 --> 00:13:34,797
Así que déjame mostrarte una manera más compacta de 

218
00:13:34,797 --> 00:13:37,140
representar estas conexiones en términos de notación.

219
00:13:37,660 --> 00:13:40,520
Así es como lo verás si eliges leer más sobre redes neuronales.

220
00:13:41,380 --> 00:13:47,315
Organice todas las activaciones de una capa en una columna, 

221
00:13:47,315 --> 00:13:55,922
ya que una matriz corresponde a las conexiones entre una capa y una neurona particular 

222
00:13:55,922 --> 00:13:58,000
en la siguiente capa.

223
00:13:58,540 --> 00:14:02,237
Lo que eso significa es que tomar la suma ponderada de las activaciones en 

224
00:14:02,237 --> 00:14:06,034
la primera capa de acuerdo con estos pesos corresponde a uno de los términos 

225
00:14:06,034 --> 00:14:09,880
en el producto vectorial matricial de todo lo que tenemos aquí a la izquierda.

226
00:14:14,000 --> 00:14:17,797
Por cierto, gran parte del aprendizaje automático se reduce a tener una buena comprensión 

227
00:14:17,797 --> 00:14:21,510
del álgebra lineal, así que para cualquiera de ustedes que quiera una buena comprensión 

228
00:14:21,510 --> 00:14:25,097
visual de las matrices y lo que significa la multiplicación de vectores matriciales, 

229
00:14:25,097 --> 00:14:28,600
eche un vistazo a la serie que hice en Álgebra lineal, especialmente el capítulo 3.

230
00:14:29,240 --> 00:14:33,472
Volviendo a nuestra expresión, en lugar de hablar de sumar el sesgo a cada uno de 

231
00:14:33,472 --> 00:14:37,860
estos valores de forma independiente, lo representamos organizando todos esos sesgos 

232
00:14:37,860 --> 00:14:42,300
en un vector y sumando el vector completo al producto vectorial de la matriz anterior.

233
00:14:43,280 --> 00:14:47,169
Luego, como paso final, envolveré un sigmoide alrededor del exterior aquí, 

234
00:14:47,169 --> 00:14:50,799
y lo que se supone que eso representa es que vas a aplicar la función 

235
00:14:50,799 --> 00:14:54,740
sigmoidea a cada componente específico del vector resultante en el interior.

236
00:14:55,940 --> 00:14:59,792
Entonces, una vez que escribe esta matriz de peso y estos vectores como sus 

237
00:14:59,792 --> 00:15:03,645
propios símbolos, puede comunicar la transición completa de activaciones de 

238
00:15:03,645 --> 00:15:07,903
una capa a la siguiente en una pequeña expresión extremadamente precisa y ordenada, 

239
00:15:07,903 --> 00:15:11,807
y esto hace que el código relevante sea mucho más simple y mucho más rápido, 

240
00:15:11,807 --> 00:15:15,660
ya que muchas bibliotecas optimizan al máximo la multiplicación de matrices.

241
00:15:17,820 --> 00:15:21,460
¿Recuerdas que antes dije que estas neuronas son simplemente cosas que contienen números?

242
00:15:22,220 --> 00:15:26,220
Bueno, por supuesto, los números específicos que contienen dependen 

243
00:15:26,220 --> 00:15:30,103
de la imagen que introduces, por lo que en realidad es más exacto 

244
00:15:30,103 --> 00:15:34,162
pensar en cada neurona como una función, una que toma las salidas de 

245
00:15:34,162 --> 00:15:38,340
todas las neuronas en la capa anterior y escupe un número. entre 0 y 1.

246
00:15:39,200 --> 00:15:43,197
En realidad, toda la red es solo una función, una que toma 

247
00:15:43,197 --> 00:15:47,060
784 números como entrada y escupe 10 números como salida.

248
00:15:47,560 --> 00:15:52,334
Es una función absurdamente complicada, que involucra 13.000 parámetros en las formas de 

249
00:15:52,334 --> 00:15:55,124
estos pesos y sesgos que detectan ciertos patrones, 

250
00:15:55,124 --> 00:15:59,899
y que implica iterar muchos productos vectoriales matriciales y la función de compresión 

251
00:15:59,899 --> 00:16:02,689
sigmoidea, pero de todos modos es solo una función, 

252
00:16:02,689 --> 00:16:06,660
y en un sentido De alguna manera es tranquilizador que parezca complicado.

253
00:16:07,340 --> 00:16:09,980
Quiero decir, si fuera más simple, ¿qué esperanzas tendríamos 

254
00:16:09,980 --> 00:16:12,280
de que pudiera asumir el desafío de reconocer dígitos?

255
00:16:13,340 --> 00:16:14,700
¿Y cómo afronta ese desafío?

256
00:16:15,080 --> 00:16:19,360
¿Cómo aprende esta red los pesos y sesgos apropiados con solo mirar los datos?

257
00:16:20,140 --> 00:16:23,185
Bueno, eso es lo que mostraré en el siguiente video y también profundizaré un poco 

258
00:16:23,185 --> 00:16:26,120
más en lo que realmente está haciendo esta red en particular que estamos viendo.

259
00:16:27,580 --> 00:16:30,710
Ahora es el punto. Supongo que debería decir suscribirse para recibir 

260
00:16:30,710 --> 00:16:33,573
notificaciones sobre cuándo salen videos o videos nuevos, pero, 

261
00:16:33,573 --> 00:16:37,420
siendo realistas, la mayoría de ustedes no reciben notificaciones de YouTube, ¿verdad?

262
00:16:38,020 --> 00:16:41,159
Tal vez, más honestamente, debería decir suscríbete para que las redes 

263
00:16:41,159 --> 00:16:44,298
neuronales que subyacen al algoritmo de recomendación de YouTube estén 

264
00:16:44,298 --> 00:16:47,880
preparadas para creer que deseas ver contenido de este canal y te lo recomienden.

265
00:16:48,560 --> 00:16:49,940
De todos modos, mantente informado para más.

266
00:16:50,760 --> 00:16:53,500
Muchas gracias a todos los que apoyan estos videos en Patreon.

267
00:16:54,000 --> 00:16:57,186
He tardado un poco en progresar en la serie de probabilidad este verano, 

268
00:16:57,186 --> 00:16:59,717
pero volveré a ella después de este proyecto, así que los 

269
00:16:59,717 --> 00:17:01,900
patrocinadores pueden buscar actualizaciones allí.

270
00:17:03,600 --> 00:17:06,273
Para cerrar, tengo conmigo a Leisha Lee, quien hizo su trabajo de 

271
00:17:06,273 --> 00:17:09,191
doctorado en el lado teórico del aprendizaje profundo y que actualmente 

272
00:17:09,191 --> 00:17:11,946
trabaja en una firma de capital de riesgo llamada Amplify Partners, 

273
00:17:11,946 --> 00:17:14,619
quien amablemente proporcionó parte de los fondos para este video.

274
00:17:15,460 --> 00:17:17,345
Entonces, Leisha, una cosa que creo que deberíamos 

275
00:17:17,345 --> 00:17:19,119
mencionar rápidamente es esta función sigmoidea.

276
00:17:19,700 --> 00:17:23,063
Según tengo entendido, las primeras redes usan esto para aplastar la 

277
00:17:23,063 --> 00:17:26,476
suma ponderada relevante en ese intervalo entre cero y uno, ya sabes, 

278
00:17:26,476 --> 00:17:29,840
motivado por esta analogía biológica de neuronas activas o inactivas.

279
00:17:30,280 --> 00:17:30,300
Exactamente.

280
00:17:30,560 --> 00:17:34,040
Pero relativamente pocas redes modernas ya utilizan sigmoide.

281
00:17:34,320 --> 00:17:34,320
Sí.

282
00:17:34,440 --> 00:17:35,540
Es algo de la vieja escuela, ¿verdad?

283
00:17:35,760 --> 00:17:38,980
Sí, o mejor dicho, relu parece ser mucho más fácil de entrenar.

284
00:17:39,400 --> 00:17:42,340
¿Y relu significa unidad lineal rectificada?

285
00:17:42,680 --> 00:17:48,324
Sí, es este tipo de función en la que simplemente tomas un máximo de cero y donde a está 

286
00:17:48,324 --> 00:17:53,905
dado por lo que estabas explicando en el video y creo que fue parcialmente motivado por 

287
00:17:53,905 --> 00:17:58,789
una analogía biológica con cómo funcionan las neuronas. se activaría o no y, 

288
00:17:58,789 --> 00:18:03,292
por lo tanto, si pasa un cierto umbral, sería la función de identidad, 

289
00:18:03,292 --> 00:18:08,049
pero si no lo hiciera, simplemente no se activaría, por lo que sería cero, 

290
00:18:08,049 --> 00:18:10,840
por lo que es una especie de simplificación.

291
00:18:11,160 --> 00:18:15,532
El uso de sigmoides no ayudó al entrenamiento o fue muy difícil 

292
00:18:15,532 --> 00:18:19,768
entrenar en algún momento y la gente simplemente probó relu y 

293
00:18:19,768 --> 00:18:24,620
funcionó muy bien para estas redes neuronales increíblemente profundas.

294
00:18:25,100 --> 00:18:25,640
Muy bien, gracias Alicia.

