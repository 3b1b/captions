1
00:00:00,000 --> 00:00:01,310
यह 3 है.

2
00:00:01,310 --> 00:00:07,527
इसे 28x28 पिक्सल के बेहद कम रिज़ॉल्यूशन पर लापरवाही से लिखा और प्रस्तुत किया गया

3
00:00:07,527 --> 00:00:13,743
है, लेकिन आपके मस्तिष्क को इसे 3 के रूप में पहचानने में कोई परेशानी नहीं होती है।

4
00:00:13,743 --> 00:00:16,713
और मैं चाहता हूं कि आप एक पल के लिए इसकी सराहना करें कि

5
00:00:16,713 --> 00:00:19,629
यह कितना अजीब है कि दिमाग इतनी आसानी से ऐसा कर सकता है।

6
00:00:19,629 --> 00:00:24,097
मेरा मतलब है, यह, यह और यह भी 3s के रूप में पहचाने जाने योग्य हैं, भले

7
00:00:24,097 --> 00:00:28,691
ही प्रत्येक पिक्सेल का विशिष्ट मान एक छवि से दूसरी छवि में बहुत भिन्न हो।

8
00:00:28,691 --> 00:00:32,811
जब आप इस 3 को देखते हैं तो आपकी आंख में विशेष प्रकाश-संवेदनशील कोशिकाएं सक्रिय

9
00:00:32,811 --> 00:00:36,722
हो जाती हैं, जब आप इस 3 को देखते हैं तो चमकती कोशिकाएं बहुत भिन्न होती हैं।

10
00:00:36,722 --> 00:00:40,809
लेकिन आपके उस पागल-स्मार्ट विज़ुअल कॉर्टेक्स में कुछ चीज़ इन्हें

11
00:00:40,809 --> 00:00:44,897
एक ही विचार का प्रतिनिधित्व करने के रूप में हल करती है, जबकि साथ

12
00:00:44,897 --> 00:00:49,300
ही अन्य छवियों को अपने स्वयं के विशिष्ट विचारों के रूप में पहचानती है।

13
00:00:49,300 --> 00:00:54,863
लेकिन अगर मैंने आपसे कहा, अरे, बैठो और मेरे लिए एक प्रोग्राम लिखो जो 28x28 की ग्रिड

14
00:00:54,863 --> 00:01:00,360
लेता है और 0 और 10 के बीच एक एकल संख्या आउटपुट करता है, और आपको बताता है कि वह अंक

15
00:01:00,360 --> 00:01:06,122
के बारे में क्या सोचता है, तो यह कार्य हास्यास्पद रूप से मामूली हो जाता है अत्यंत कठिन.

16
00:01:06,122 --> 00:01:08,904
जब तक आप किसी चट्टान के नीचे नहीं रह रहे हैं, मुझे लगता है कि

17
00:01:08,904 --> 00:01:11,730
मुझे वर्तमान और भविष्य के लिए मशीन लर्निंग और तंत्रिका नेटवर्क

18
00:01:11,730 --> 00:01:14,557
की प्रासंगिकता और महत्व को प्रेरित करने की शायद ही आवश्यकता है।

19
00:01:14,557 --> 00:01:17,872
लेकिन मैं यहां आपको यह दिखाना चाहता हूं कि तंत्रिका नेटवर्क वास्तव में क्या

20
00:01:17,872 --> 00:01:21,057
है, बिना किसी पृष्ठभूमि के, और यह कल्पना करने में मदद करना कि यह क्या कर

21
00:01:21,057 --> 00:01:24,241
रहा है, एक चर्चा शब्द के रूप में नहीं बल्कि गणित के एक टुकड़े के रूप में।

22
00:01:24,241 --> 00:01:27,444
मेरी आशा बस इतनी है कि आप यह महसूस करते हुए आएं कि संरचना स्वयं

23
00:01:27,444 --> 00:01:30,846
प्रेरित है, और जब आप तंत्रिका नेटवर्क उद्धरण-अनउद्धरण सीखने के बारे

24
00:01:30,846 --> 00:01:34,549
में पढ़ते हैं या सुनते हैं तो आपको ऐसा महसूस होता है कि इसका क्या मतलब है।

25
00:01:34,549 --> 00:01:40,300
यह वीडियो केवल इसके संरचना घटक के लिए समर्पित होगा, और अगला वीडियो सीखने से संबंधित होगा।

26
00:01:40,300 --> 00:01:43,119
हम जो करने जा रहे हैं वह एक तंत्रिका नेटवर्क को एक

27
00:01:43,119 --> 00:01:46,104
साथ रखना है जो हस्तलिखित अंकों को पहचानना सीख सकता है।

28
00:01:46,104 --> 00:01:50,830
विषय को प्रस्तुत करने के लिए यह कुछ हद तक उत्कृष्ट उदाहरण है, और मुझे यहां यथास्थिति

29
00:01:50,830 --> 00:01:55,278
पर बने रहने में खुशी हो रही है, क्योंकि दो वीडियो के अंत में मैं आपको कुछ अच्छे

30
00:01:55,278 --> 00:01:59,781
संसाधनों की ओर इंगित करना चाहता हूं जहां आप और अधिक सीख सकते हैं, और कहां आप ऐसा

31
00:01:59,781 --> 00:02:04,173
करने वाले कोड को डाउनलोड कर सकते हैं और अपने कंप्यूटर पर उसके साथ खेल सकते हैं।

32
00:02:04,173 --> 00:02:08,932
तंत्रिका नेटवर्क के कई प्रकार हैं, और हाल के वर्षों में इन प्रकारों के

33
00:02:08,932 --> 00:02:13,691
प्रति अनुसंधान में तेजी आई है, लेकिन इन दो परिचयात्मक वीडियो में आप और

34
00:02:13,691 --> 00:02:18,920
मैं बिना किसी अतिरिक्त तामझाम के सबसे सरल सादे वेनिला रूप को देखने जा रहे हैं।

35
00:02:18,920 --> 00:02:23,672
किसी भी अधिक शक्तिशाली आधुनिक संस्करण को समझने के लिए यह एक आवश्यक शर्त है, और

36
00:02:23,672 --> 00:02:28,485
मेरा विश्वास करें कि इसमें अभी भी हमारे दिमाग को समझने के लिए काफी जटिलताएं हैं।

37
00:02:28,485 --> 00:02:32,669
लेकिन इस सरलतम रूप में भी यह हस्तलिखित अंकों को पहचानना

38
00:02:32,669 --> 00:02:36,628
सीख सकता है, जो एक कंप्यूटर के लिए बहुत अच्छी बात है।

39
00:02:36,628 --> 00:02:39,913
और साथ ही आप देखेंगे कि कैसे यह कुछ उम्मीदों से

40
00:02:39,913 --> 00:02:42,925
कम हो जाता है जो हमारी इसके लिए हो सकती हैं।

41
00:02:42,925 --> 00:02:45,827
जैसा कि नाम से पता चलता है, तंत्रिका नेटवर्क मस्तिष्क

42
00:02:45,827 --> 00:02:48,193
से प्रेरित होते हैं, लेकिन आइए इसे तोड़ दें।

43
00:02:48,193 --> 00:02:51,833
न्यूरॉन्स क्या हैं, और वे किस अर्थ में एक साथ जुड़े हुए हैं?

44
00:02:51,833 --> 00:02:56,378
अभी जब मैं न्यूरॉन कहता हूं, तो मैं चाहता हूं कि आप एक ऐसी चीज के बारे

45
00:02:56,378 --> 00:03:00,987
में सोचें जो एक संख्या रखती है, विशेष रूप से 0 और 1 के बीच की एक संख्या।

46
00:03:00,987 --> 00:03:03,377
यह वास्तव में उससे अधिक नहीं है.

47
00:03:03,377 --> 00:03:08,318
उदाहरण के लिए, नेटवर्क इनपुट छवि के 28 गुना 28 पिक्सेल में से प्रत्येक के

48
00:03:08,318 --> 00:03:13,660
अनुरूप न्यूरॉन्स के एक समूह के साथ शुरू होता है, जो कुल मिलाकर 784 न्यूरॉन्स है।

49
00:03:13,660 --> 00:03:19,010
इनमें से प्रत्येक में एक संख्या होती है जो संबंधित पिक्सेल के ग्रेस्केल मान को

50
00:03:19,010 --> 00:03:24,293
दर्शाती है, जो काले पिक्सेल के लिए 0 से लेकर सफेद पिक्सेल के लिए 1 तक होती है।

51
00:03:24,293 --> 00:03:29,979
न्यूरॉन के अंदर की इस संख्या को इसकी सक्रियता कहा जाता है, और आपके मन में यह छवि हो

52
00:03:29,979 --> 00:03:35,935
सकती है कि प्रत्येक न्यूरॉन तब प्रकाशित होता है जब इसकी सक्रियता एक उच्च संख्या होती है।

53
00:03:35,935 --> 00:03:43,399
तो ये सभी 784 न्यूरॉन्स हमारे नेटवर्क की पहली परत बनाते हैं।

54
00:03:43,399 --> 00:03:47,832
अब अंतिम परत पर जाएं, इसमें 10 न्यूरॉन हैं, जिनमें

55
00:03:47,832 --> 00:03:51,570
से प्रत्येक एक अंक का प्रतिनिधित्व करता है।

56
00:03:51,570 --> 00:03:57,053
इन न्यूरॉन्स में सक्रियता, फिर से कुछ संख्या जो 0 और 1 के बीच है, यह दर्शाती

57
00:03:57,053 --> 00:04:02,180
है कि सिस्टम कितना सोचता है कि दी गई छवि किसी दिए गए अंक से मेल खाती है।

58
00:04:02,180 --> 00:04:07,863
बीच में कुछ परतें भी होती हैं जिन्हें छिपी हुई परतें कहा जाता है, जो फिलहाल एक बड़ा

59
00:04:07,863 --> 00:04:13,952
प्रश्नचिह्न होना चाहिए कि पृथ्वी पर अंकों को पहचानने की इस प्रक्रिया को कैसे संभाला जाएगा।

60
00:04:13,952 --> 00:04:17,364
इस नेटवर्क में मैंने दो छिपी हुई परतों को चुना, प्रत्येक

61
00:04:17,364 --> 00:04:20,657
में 16 न्यूरॉन्स थे, और माना कि यह एक मनमाना विकल्प है।

62
00:04:20,657 --> 00:04:24,661
ईमानदारी से कहूं तो, मैंने इस आधार पर दो परतें चुनीं कि मैं संरचना को एक पल में कैसे

63
00:04:24,661 --> 00:04:28,617
प्रेरित करना चाहता हूं, और 16, खैर यह स्क्रीन पर फिट होने के लिए एक अच्छी संख्या थी।

64
00:04:28,617 --> 00:04:32,782
व्यवहार में यहां एक विशिष्ट संरचना के साथ प्रयोग के लिए बहुत जगह है।

65
00:04:32,782 --> 00:04:35,665
जिस तरह से नेटवर्क संचालित होता है, एक परत में

66
00:04:35,665 --> 00:04:38,610
सक्रियता अगली परत की सक्रियता निर्धारित करती है।

67
00:04:38,610 --> 00:04:43,787
और निश्चित रूप से एक सूचना प्रसंस्करण तंत्र के रूप में नेटवर्क का मूल इस बात

68
00:04:43,787 --> 00:04:48,829
पर निर्भर करता है कि कैसे एक परत से सक्रियता अगली परत में सक्रियता लाती है।

69
00:04:48,829 --> 00:04:52,596
इसका मतलब यह है कि न्यूरॉन्स के जैविक नेटवर्क में

70
00:04:52,596 --> 00:04:57,043
न्यूरॉन्स के कुछ समूह किस तरह से दूसरों को सक्रिय करते हैं।

71
00:04:57,043 --> 00:05:00,292
अब जो नेटवर्क मैं यहां दिखा रहा हूं उसे पहले से ही अंकों को पहचानने के लिए

72
00:05:00,292 --> 00:05:03,455
प्रशिक्षित किया गया है, और मैं आपको दिखाता हूं कि इससे मेरा क्या मतलब है।

73
00:05:03,455 --> 00:05:08,162
इसका मतलब है कि यदि आप छवि में प्रत्येक पिक्सेल की चमक के अनुसार इनपुट परत के

74
00:05:08,162 --> 00:05:12,688
सभी 784 न्यूरॉन्स को रोशन करते हुए एक छवि में फ़ीड करते हैं, तो सक्रियण का

75
00:05:12,688 --> 00:05:17,335
पैटर्न अगली परत में कुछ बहुत विशिष्ट पैटर्न का कारण बनता है, जो एक के बाद एक

76
00:05:17,335 --> 00:05:22,103
में कुछ पैटर्न का कारण बनता है यह, जो अंततः आउटपुट लेयर में कुछ पैटर्न देता है।

77
00:05:22,103 --> 00:05:26,304
और उस आउटपुट परत का सबसे चमकीला न्यूरॉन नेटवर्क की पसंद है,

78
00:05:26,304 --> 00:05:30,434
इसलिए बोलने के लिए, यह छवि किस अंक का प्रतिनिधित्व करती है।

79
00:05:30,434 --> 00:05:34,696
और गणित में कूदने से पहले कि एक परत अगली परत को कैसे प्रभावित करती है,

80
00:05:34,696 --> 00:05:39,017
या प्रशिक्षण कैसे काम करता है, आइए बस इस बारे में बात करें कि इस तरह की

81
00:05:39,017 --> 00:05:43,458
स्तरित संरचना से बुद्धिमानी से व्यवहार करने की अपेक्षा करना क्यों उचित है।

82
00:05:43,458 --> 00:05:45,046
हम यहाँ क्या उम्मीद कर रहे हैं?

83
00:05:45,046 --> 00:05:49,026
वे मध्य परतें क्या कर रही होंगी, इसके लिए सबसे अच्छी आशा क्या है?

84
00:05:49,026 --> 00:05:53,689
खैर, जब आप या मैं अंकों को पहचानते हैं, तो हम विभिन्न घटकों को एक साथ जोड़ते हैं।

85
00:05:53,689 --> 00:05:56,934
9 में ऊपर एक लूप और दाहिनी ओर एक रेखा होती है।

86
00:05:56,934 --> 00:06:01,827
8 में ऊपर की ओर एक लूप भी होता है, लेकिन इसे नीचे की ओर एक अन्य लूप के साथ जोड़ा जाता है।

87
00:06:01,827 --> 00:06:06,673
A 4 मूलतः तीन विशिष्ट रेखाओं और इसी तरह की चीज़ों में टूट जाता है।

88
00:06:06,673 --> 00:06:10,753
अब एक आदर्श दुनिया में, हम उम्मीद कर सकते हैं कि दूसरी से आखिरी परत

89
00:06:10,753 --> 00:06:14,832
में प्रत्येक न्यूरॉन इन उप-घटकों में से एक से मेल खाता है, जब भी आप

90
00:06:14,832 --> 00:06:19,032
किसी छवि में फ़ीड करते हैं, उदाहरण के लिए, शीर्ष पर एक लूप, जैसे 9 या

91
00:06:19,032 --> 00:06:23,651
8, तो वहां कुछ होता है विशिष्ट न्यूरॉन जिसकी सक्रियता 1 के करीब होने वाली है।

92
00:06:23,651 --> 00:06:27,526
और मेरा मतलब पिक्सल के इस विशिष्ट लूप से नहीं है, आशा यह होगी कि

93
00:06:27,526 --> 00:06:31,700
शीर्ष की ओर कोई भी आम तौर पर लूपी पैटर्न इस न्यूरॉन को बंद कर देता है।

94
00:06:31,700 --> 00:06:35,724
इस तरह, तीसरी परत से अंतिम परत तक जाने के लिए बस यह सीखने की

95
00:06:35,724 --> 00:06:40,012
आवश्यकता है कि उपघटकों का कौन सा संयोजन किन अंकों से मेल खाता है।

96
00:06:40,012 --> 00:06:43,872
बेशक, इससे समस्या ख़त्म हो जाती है, क्योंकि आप इन उप-घटकों को

97
00:06:43,872 --> 00:06:47,794
कैसे पहचानेंगे, या यह भी सीखेंगे कि सही उप-घटक क्या होने चाहिए?

98
00:06:47,794 --> 00:06:50,366
और मैंने अभी तक इस बारे में बात भी नहीं की है कि एक परत दूसरी परत

99
00:06:50,366 --> 00:06:52,900
को कैसे प्रभावित करती है, लेकिन एक पल के लिए इस पर मेरे साथ चलें।

100
00:06:52,900 --> 00:06:56,598
लूप को पहचानने से उप-समस्याएँ भी विभाजित हो सकती हैं।

101
00:06:56,598 --> 00:06:59,851
ऐसा करने का एक उचित तरीका यह होगा कि पहले इसे

102
00:06:59,851 --> 00:07:03,386
बनाने वाले विभिन्न छोटे किनारों को पहचान लिया जाए।

103
00:07:03,386 --> 00:07:07,307
इसी प्रकार, एक लंबी रेखा जैसी कि आप अंक 1 या 4 या 7 में देख

104
00:07:07,307 --> 00:07:11,229
सकते हैं, वास्तव में यह सिर्फ एक लंबा किनारा है, या शायद आप

105
00:07:11,229 --> 00:07:15,281
इसे कई छोटे किनारों के एक निश्चित पैटर्न के रूप में सोचते हैं।

106
00:07:15,281 --> 00:07:19,591
तो शायद हमारी आशा यह है कि नेटवर्क की दूसरी परत में प्रत्येक

107
00:07:19,591 --> 00:07:23,406
न्यूरॉन विभिन्न प्रासंगिक छोटे किनारों से मेल खाता है।

108
00:07:23,406 --> 00:07:28,844
हो सकता है कि जब इस तरह की कोई छवि आती है, तो यह लगभग 8 से 10 विशिष्ट छोटे किनारों

109
00:07:28,844 --> 00:07:34,217
से जुड़े सभी न्यूरॉन्स को रोशन करती है, जो बदले में ऊपरी लूप और एक लंबी ऊर्ध्वाधर

110
00:07:34,217 --> 00:07:39,720
रेखा से जुड़े न्यूरॉन्स को रोशन करती है, और वे प्रकाश डालते हैं 9 से संबद्ध न्यूरॉन।

111
00:07:39,720 --> 00:07:43,359
हमारा अंतिम नेटवर्क वास्तव में ऐसा करता है या नहीं, यह एक और सवाल है, जिस पर

112
00:07:43,359 --> 00:07:47,186
मैं एक बार फिर विचार करूंगा जब हम देखेंगे कि नेटवर्क को कैसे प्रशिक्षित किया जाए।

113
00:07:47,186 --> 00:07:49,869
लेकिन यह एक आशा है कि हमारे पास इस तरह की स्तरित

114
00:07:49,869 --> 00:07:52,334
संरचना के साथ एक प्रकार का लक्ष्य हो सकता है।

115
00:07:52,334 --> 00:07:56,420
इसके अलावा, आप कल्पना कर सकते हैं कि इस तरह किनारों और पैटर्न का पता लगाने

116
00:07:56,420 --> 00:08:00,397
में सक्षम होना अन्य छवि पहचान कार्यों के लिए वास्तव में कैसे उपयोगी होगा।

117
00:08:00,397 --> 00:08:03,822
और छवि पहचान से परे भी, सभी प्रकार की बुद्धिमान चीजें हैं

118
00:08:03,822 --> 00:08:07,306
जो आप करना चाहते हैं जो अमूर्तता की परतों में टूट जाती हैं।

119
00:08:07,306 --> 00:08:11,477
उदाहरण के लिए, भाषण को पार्स करने में कच्चा ऑडियो लेना और अलग-अलग

120
00:08:11,477 --> 00:08:15,838
ध्वनियों को चुनना शामिल है, जो मिलकर कुछ शब्दांश बनाते हैं, जो मिलकर

121
00:08:15,838 --> 00:08:20,262
शब्द बनाते हैं, जो मिलकर वाक्यांश और अधिक अमूर्त विचार बनाते हैं, आदि।

122
00:08:20,262 --> 00:08:25,536
लेकिन इनमें से कोई भी वास्तव में कैसे काम करता है, इस पर वापस लौटते हुए, अभी स्वयं कल्पना

123
00:08:25,536 --> 00:08:30,635
करें कि एक परत में सक्रियता वास्तव में अगली परत में सक्रियता कैसे निर्धारित कर सकती है।

124
00:08:30,635 --> 00:08:34,718
लक्ष्य कुछ ऐसा तंत्र बनाना है जो पिक्सेल को किनारों में,

125
00:08:34,718 --> 00:08:38,872
या किनारों को पैटर्न में, या पैटर्न को अंकों में जोड़ सके।

126
00:08:38,872 --> 00:08:44,766
और एक बहुत ही विशिष्ट उदाहरण पर ज़ूम करने के लिए, मान लें कि दूसरी परत में एक विशेष

127
00:08:44,766 --> 00:08:50,660
न्यूरॉन से यह पता लगाने की उम्मीद है कि छवि का इस क्षेत्र में कोई किनारा है या नहीं।

128
00:08:50,660 --> 00:08:55,197
सवाल यह है कि नेटवर्क में कौन से पैरामीटर होने चाहिए?

129
00:08:55,197 --> 00:08:59,483
आपको किस डायल और नॉब को बदलने में सक्षम होना चाहिए ताकि यह इस पैटर्न,

130
00:08:59,483 --> 00:09:03,768
या किसी अन्य पिक्सेल पैटर्न, या पैटर्न को कैप्चर करने के लिए पर्याप्त

131
00:09:03,768 --> 00:09:08,175
रूप से अभिव्यंजक हो, जिससे कई किनारे एक लूप बना सकें, और ऐसी अन्य चीजें?

132
00:09:08,175 --> 00:09:11,686
खैर, हम जो करेंगे वह हमारे न्यूरॉन और पहली परत के

133
00:09:11,686 --> 00:09:15,759
न्यूरॉन्स के बीच प्रत्येक कनेक्शन को एक भार प्रदान करेंगे।

134
00:09:15,759 --> 00:09:17,666
ये वज़न महज़ संख्याएँ हैं।

135
00:09:17,666 --> 00:09:21,817
फिर उन सभी सक्रियताओं को पहली परत से लें और इन

136
00:09:21,817 --> 00:09:25,704
भारों के अनुसार उनके भारित योग की गणना करें।

137
00:09:25,704 --> 00:09:29,659
मुझे इन वजनों को अपने स्वयं के एक छोटे ग्रिड में व्यवस्थित करने के बारे में

138
00:09:29,659 --> 00:09:33,666
सोचने में मदद मिलती है, और मैं सकारात्मक वजन को इंगित करने के लिए हरे पिक्सल

139
00:09:33,666 --> 00:09:37,726
का उपयोग करने जा रहा हूं, और नकारात्मक वजन को इंगित करने के लिए लाल पिक्सल का

140
00:09:37,726 --> 00:09:41,941
उपयोग करने जा रहा हूं, जहां उस पिक्सेल की चमक कुछ है वजन के मूल्य का ढीला चित्रण.

141
00:09:41,941 --> 00:09:47,265
यदि हमने इस क्षेत्र में कुछ सकारात्मक भारों को छोड़कर, जिनकी हम परवाह करते हैं, लगभग

142
00:09:47,265 --> 00:09:52,589
सभी पिक्सेल से जुड़े भार को शून्य कर दिया है, तो सभी पिक्सेल मानों का भारित योग लेना

143
00:09:52,589 --> 00:09:57,976
वास्तव में केवल पिक्सेल के मानों को जोड़ने के समान है। वह क्षेत्र जिसकी हमें परवाह है।

144
00:09:57,976 --> 00:10:02,708
और यदि आप वास्तव में यह जानना चाहते हैं कि क्या यहां कोई बढ़त है,

145
00:10:02,708 --> 00:10:07,440
तो आप आसपास के पिक्सेल से जुड़े कुछ नकारात्मक भार को समझ सकते हैं।

146
00:10:07,440 --> 00:10:10,195
तब योग सबसे बड़ा होता है जब वे मध्य पिक्सेल चमकीले

147
00:10:10,195 --> 00:10:12,680
होते हैं लेकिन आसपास के पिक्सेल गहरे होते हैं।

148
00:10:12,680 --> 00:10:18,190
जब आप इस तरह एक भारित राशि की गणना करते हैं, तो आप किसी भी संख्या के साथ आ सकते

149
00:10:18,190 --> 00:10:23,563
हैं, लेकिन इस नेटवर्क के लिए हम चाहते हैं कि सक्रियण 0 और 1 के बीच कुछ मान हो।

150
00:10:23,563 --> 00:10:27,505
तो एक सामान्य बात यह है कि इस भारित योग को किसी फ़ंक्शन में पंप किया

151
00:10:27,505 --> 00:10:31,447
जाए जो वास्तविक संख्या रेखा को 0 और 1 के बीच की सीमा में दबा देता है।

152
00:10:31,447 --> 00:10:34,255
और ऐसा करने वाला एक सामान्य कार्य सिग्मॉइड फ़ंक्शन

153
00:10:34,255 --> 00:10:37,449
कहलाता है, जिसे लॉजिस्टिक वक्र के रूप में भी जाना जाता है।

154
00:10:37,449 --> 00:10:42,687
मूल रूप से बहुत नकारात्मक इनपुट 0 के करीब समाप्त होते हैं, बहुत सकारात्मक

155
00:10:42,687 --> 00:10:48,137
इनपुट 1 के करीब समाप्त होते हैं, और यह इनपुट 0 के आसपास लगातार बढ़ता जाता है।

156
00:10:48,137 --> 00:10:52,422
तो यहां न्यूरॉन की सक्रियता मूल रूप से इस बात का

157
00:10:52,422 --> 00:10:56,706
माप है कि प्रासंगिक भारित योग कितना सकारात्मक है।

158
00:10:56,706 --> 00:10:59,814
लेकिन शायद ऐसा नहीं है कि आप चाहते हैं कि जब भारित

159
00:10:59,814 --> 00:11:02,313
योग 0 से बड़ा हो तो न्यूरॉन प्रकाशमान हो।

160
00:11:02,313 --> 00:11:06,768
हो सकता है कि आप इसे केवल तभी सक्रिय करना चाहते हों जब योग 10 से बड़ा हो।

161
00:11:06,768 --> 00:11:10,663
यानी, आप इसके निष्क्रिय होने के लिए कुछ पूर्वाग्रह चाहते हैं।

162
00:11:10,663 --> 00:11:15,578
इसके बाद हम इस भारित योग को सिग्मॉइड स्क्विशिफिकेशन फ़ंक्शन के माध्यम

163
00:11:15,578 --> 00:11:20,634
से प्लग करने से पहले इसमें कोई अन्य संख्या, जैसे ऋणात्मक 10, जोड़ देंगे।

164
00:11:20,634 --> 00:11:23,382
उस अतिरिक्त संख्या को पूर्वाग्रह कहा जाता है।

165
00:11:23,382 --> 00:11:27,162
तो वज़न आपको बताता है कि दूसरी परत में यह न्यूरॉन किस पिक्सेल

166
00:11:27,162 --> 00:11:31,065
पैटर्न को उठा रहा है, और पूर्वाग्रह आपको बताता है कि न्यूरॉन के

167
00:11:31,065 --> 00:11:35,089
सार्थक रूप से सक्रिय होने से पहले भारित योग कितना अधिक होना चाहिए।

168
00:11:35,089 --> 00:11:37,188
और वह सिर्फ एक न्यूरॉन है.

169
00:11:37,188 --> 00:11:43,706
इस परत का हर दूसरा न्यूरॉन पहली परत के सभी 784 पिक्सेल न्यूरॉन्स से

170
00:11:43,706 --> 00:11:50,705
जुड़ा होगा, और उन 784 कनेक्शनों में से प्रत्येक का अपना वजन जुड़ा हुआ है।

171
00:11:50,705 --> 00:11:54,332
इसके अलावा, प्रत्येक में कुछ पूर्वाग्रह होते हैं, कुछ अन्य संख्याएं

172
00:11:54,332 --> 00:11:57,960
जिन्हें आप सिग्मॉइड के साथ कुचलने से पहले भारित राशि में जोड़ते हैं।

173
00:11:57,960 --> 00:11:59,684
और यह बहुत सोचने वाली बात है!

174
00:11:59,684 --> 00:12:07,848
16 न्यूरॉन्स की इस छिपी हुई परत के साथ, 16 पूर्वाग्रहों के साथ, यह कुल 784 गुना 16 भार है।

175
00:12:07,848 --> 00:12:12,104
और यह सब पहली परत से दूसरी परत तक का कनेक्शन मात्र है।

176
00:12:12,104 --> 00:12:18,059
अन्य परतों के बीच के संबंधों में बहुत सारे वजन और पूर्वाग्रह भी जुड़े हुए हैं।

177
00:12:18,059 --> 00:12:24,015
सब कुछ कहा और किया गया, इस नेटवर्क में लगभग 13,000 कुल भार और पूर्वाग्रह हैं।

178
00:12:24,015 --> 00:12:27,038
13,000 नॉब और डायल जिन्हें इस नेटवर्क को अलग-अलग

179
00:12:27,038 --> 00:12:30,493
तरीकों से संचालित करने के लिए बदला और घुमाया जा सकता है।

180
00:12:30,493 --> 00:12:35,971
इसलिए जब हम सीखने के बारे में बात करते हैं, तो इसका मतलब कंप्यूटर को इन सभी

181
00:12:35,971 --> 00:12:41,953
संख्याओं के लिए एक वैध सेटिंग ढूंढना है ताकि वह वास्तव में समस्या का समाधान कर सके।

182
00:12:41,953 --> 00:12:46,738
एक विचार प्रयोग जो मजेदार भी है और भयावह भी, वह है बैठकर कल्पना करना और

183
00:12:46,738 --> 00:12:51,391
इन सभी वजनों और पूर्वाग्रहों को हाथ से सेट करना, जानबूझकर संख्याओं को

184
00:12:51,391 --> 00:12:56,509
बदलना ताकि दूसरी परत किनारों को पकड़ ले, तीसरी परत पैटर्न को पहचान ले, वगैरह।

185
00:12:56,509 --> 00:12:59,987
मैं व्यक्तिगत रूप से नेटवर्क को संपूर्ण ब्लैक बॉक्स के रूप में मानने के

186
00:12:59,987 --> 00:13:03,369
बजाय इसे संतोषजनक मानता हूं, क्योंकि जब नेटवर्क आपके अनुमान के अनुसार

187
00:13:03,369 --> 00:13:06,751
प्रदर्शन नहीं करता है, तो यदि आपने उन भारों और पूर्वाग्रहों का वास्तव

188
00:13:06,751 --> 00:13:10,375
में क्या मतलब है, इसके साथ थोड़ा सा संबंध बना लिया है , आपके पास यह प्रयोग

189
00:13:10,375 --> 00:13:13,950
करने के लिए एक प्रारंभिक स्थान है कि सुधार के लिए संरचना को कैसे बदला जाए।

190
00:13:13,950 --> 00:13:18,117
या जब नेटवर्क काम करता है, लेकिन उन कारणों के लिए नहीं जिनकी आप उम्मीद कर सकते हैं,

191
00:13:18,117 --> 00:13:22,134
तो वज़न और पूर्वाग्रह क्या कर रहे हैं, इसकी खोज करना आपकी धारणाओं को चुनौती देने

192
00:13:22,134 --> 00:13:26,201
और वास्तव में संभावित समाधानों की पूरी गुंजाइश को उजागर करने का एक अच्छा तरीका है।

193
00:13:26,201 --> 00:13:32,177
वैसे, यहाँ वास्तविक कार्य को लिखना थोड़ा बोझिल है, क्या आपको नहीं लगता?

194
00:13:32,177 --> 00:13:34,568
तो आइए मैं आपको एक अधिक सांकेतिक रूप से संक्षिप्त तरीका

195
00:13:34,568 --> 00:13:37,087
दिखाता हूं जिससे इन कनेक्शनों का प्रतिनिधित्व किया जाता है।

196
00:13:37,087 --> 00:13:41,120
यदि आप तंत्रिका नेटवर्क के बारे में अधिक पढ़ना चुनते हैं तो आप इसे इसी तरह देखेंगे।

197
00:13:41,120 --> 00:13:46,140
सभी सक्रियणों को एक परत से एक वेक्टर के रूप में एक कॉलम में व्यवस्थित करें।

198
00:13:46,140 --> 00:13:51,818
फिर सभी भारों को एक मैट्रिक्स के रूप में व्यवस्थित करें, जहां उस मैट्रिक्स की

199
00:13:51,818 --> 00:13:58,080
प्रत्येक पंक्ति एक परत और अगली परत में एक विशेष न्यूरॉन के बीच कनेक्शन से मेल खाती है।

200
00:13:58,080 --> 00:14:03,960
इसका मतलब यह है कि इन भारों के अनुसार पहली परत में सक्रियणों का भारित योग लेना हमारे

201
00:14:03,960 --> 00:14:09,702
यहां बाईं ओर मौजूद हर चीज के मैट्रिक्स वेक्टर उत्पाद में से एक शब्द से मेल खाता है।

202
00:14:09,702 --> 00:14:14,473
वैसे, मशीन सीखने का इतना सारा काम सिर्फ रैखिक बीजगणित की अच्छी समझ

203
00:14:14,473 --> 00:14:19,172
के कारण होता है, इसलिए आप में से जो लोग मैट्रिक्स के लिए एक अच्छी

204
00:14:19,172 --> 00:14:23,942
दृश्य समझ चाहते हैं और मैट्रिक्स वेक्टर गुणन का क्या मतलब है, मेरे

205
00:14:23,942 --> 00:14:28,997
द्वारा की गई श्रृंखला पर एक नज़र डालें रैखिक बीजगणित, विशेषकर अध्याय 3।

206
00:14:28,997 --> 00:14:33,489
अपनी अभिव्यक्ति पर वापस जाएं, इन मूल्यों में से प्रत्येक में पूर्वाग्रह को स्वतंत्र रूप

207
00:14:33,489 --> 00:14:38,083
से जोड़ने के बारे में बात करने के बजाय, हम उन सभी पूर्वाग्रहों को एक वेक्टर में व्यवस्थित

208
00:14:38,083 --> 00:14:42,524
करके और पूरे वेक्टर को पिछले मैट्रिक्स वेक्टर उत्पाद में जोड़कर इसका प्रतिनिधित्व करते

209
00:14:42,524 --> 00:14:42,729
हैं।

210
00:14:42,729 --> 00:14:46,608
फिर अंतिम चरण के रूप में, मैं यहां बाहर के चारों ओर एक सिग्मॉइड

211
00:14:46,608 --> 00:14:50,850
लपेटूंगा, और जो प्रतिनिधित्व करने वाला है वह यह है कि आप अंदर परिणामी

212
00:14:50,850 --> 00:14:55,215
वेक्टर के प्रत्येक विशिष्ट घटक पर सिग्मॉइड फ़ंक्शन लागू करने जा रहे हैं।

213
00:14:55,215 --> 00:15:00,466
इसलिए एक बार जब आप इस वेट मैट्रिक्स और इन वैक्टरों को अपने स्वयं के प्रतीकों के रूप में

214
00:15:00,466 --> 00:15:05,718
लिख लेते हैं, तो आप एक परत से दूसरी परत तक सक्रियताओं के पूर्ण संक्रमण को बेहद चुस्त और

215
00:15:05,718 --> 00:15:10,969
साफ-सुथरी छोटी अभिव्यक्ति में संप्रेषित कर सकते हैं, और यह प्रासंगिक कोड को बहुत सरल और

216
00:15:10,969 --> 00:15:16,042
सरल बना देता है। बहुत तेज़, क्योंकि कई लाइब्रेरी मैट्रिक्स गुणन को अनुकूलित करती हैं।

217
00:15:16,042 --> 00:15:19,210
याद रखें कि मैंने पहले कैसे कहा था कि ये न्यूरॉन

218
00:15:19,210 --> 00:15:22,120
केवल ऐसी चीज़ें हैं जिनमें संख्याएँ होती हैं?

219
00:15:22,120 --> 00:15:27,314
बेशक, उनके पास मौजूद विशिष्ट संख्याएं आपके द्वारा फीड की गई छवि पर निर्भर करती हैं,

220
00:15:27,314 --> 00:15:32,755
इसलिए वास्तव में प्रत्येक न्यूरॉन को एक फ़ंक्शन के रूप में सोचना अधिक सटीक है, जो पिछली

221
00:15:32,755 --> 00:15:38,321
परत के सभी न्यूरॉन्स के आउटपुट लेता है, और एक को बाहर निकालता है। 0 और 1 के बीच की संख्या.

222
00:15:38,321 --> 00:15:42,708
वास्तव में पूरा नेटवर्क सिर्फ एक फ़ंक्शन है, जो इनपुट के रूप

223
00:15:42,708 --> 00:15:47,096
में 784 नंबर लेता है और आउटपुट के रूप में 10 नंबर निकालता है।

224
00:15:47,096 --> 00:15:52,444
यह एक बेतुका जटिल फ़ंक्शन है, जिसमें इन वज़न और पूर्वाग्रहों के रूप में 13,000 पैरामीटर

225
00:15:52,444 --> 00:15:57,914
शामिल हैं जो कुछ पैटर्न पर आधारित हैं, और जिसमें कई मैट्रिक्स वेक्टर उत्पादों और सिग्मॉइड

226
00:15:57,914 --> 00:16:03,201
स्क्विशिफिकेशन फ़ंक्शन को पुनरावृत्त करना शामिल है, लेकिन फिर भी यह केवल एक फ़ंक्शन है।

227
00:16:03,201 --> 00:16:06,817
और एक तरह से यह आश्वस्त करने वाला है कि यह जटिल दिखता है।

228
00:16:06,817 --> 00:16:09,839
मेरा मतलब है कि यदि यह और भी सरल होता, तो हमें क्या आशा

229
00:16:09,839 --> 00:16:12,807
होती कि यह अंकों को पहचानने की चुनौती का सामना कर सकता?

230
00:16:12,807 --> 00:16:14,920
और यह उस चुनौती को कैसे स्वीकार करता है?

231
00:16:14,920 --> 00:16:19,880
यह नेटवर्क केवल डेटा को देखकर उचित भार और पूर्वाग्रह कैसे सीखता है?

232
00:16:19,880 --> 00:16:22,954
खैर, यही मैं अगले वीडियो में दिखाऊंगा, और मैं यह भी

233
00:16:22,954 --> 00:16:26,147
जानूंगा कि यह विशेष नेटवर्क वास्तव में क्या कर रहा है।

234
00:16:26,147 --> 00:16:29,864
अब मुद्दा यह है कि मुझे लगता है कि मुझे यह कहना चाहिए कि जब वह वीडियो या कोई नया

235
00:16:29,864 --> 00:16:33,627
वीडियो आता है तो उसके बारे में सूचित रहने के लिए सदस्यता लें, लेकिन वास्तविकता यह

236
00:16:33,627 --> 00:16:37,574
है कि आप में से अधिकांश को वास्तव में YouTube से सूचनाएं प्राप्त नहीं होती हैं, है ना?

237
00:16:37,574 --> 00:16:41,192
शायद अधिक ईमानदारी से मुझे यह कहना चाहिए कि सदस्यता लें ताकि YouTube की

238
00:16:41,192 --> 00:16:44,810
अनुशंसा एल्गोरिदम के अंतर्गत आने वाले तंत्रिका नेटवर्क को यह विश्वास हो

239
00:16:44,810 --> 00:16:48,278
सके कि आप इस चैनल की सामग्री देखना चाहते हैं जो आपके लिए अनुशंसित है।

240
00:16:48,278 --> 00:16:50,600
वैसे भी, अधिक जानकारी के लिए पोस्ट करते रहें।

241
00:16:50,600 --> 00:16:53,609
पैट्रियन पर इन वीडियो का समर्थन करने वाले सभी लोगों को बहुत-बहुत धन्यवाद।

242
00:16:53,609 --> 00:16:57,942
मैं इस गर्मी में संभाव्यता श्रृंखला में प्रगति करने में थोड़ा धीमा रहा हूं, लेकिन

243
00:16:57,942 --> 00:17:02,329
इस परियोजना के बाद मैं इसमें वापस कूद रहा हूं, ताकि संरक्षक आप वहां अपडेट देख सकें।

244
00:17:02,329 --> 00:17:06,358
यहां चीजों को बंद करने के लिए मेरे साथ लीशा ली हैं, जिन्होंने गहन शिक्षण के

245
00:17:06,358 --> 00:17:10,600
सैद्धांतिक पक्ष पर पीएचडी की है, और जो वर्तमान में एम्प्लीफाई पार्टनर्स नामक एक

246
00:17:10,600 --> 00:17:15,160
उद्यम पूंजी फर्म में काम करती हैं, जिन्होंने इस वीडियो के लिए कुछ फंडिंग प्रदान की है।

247
00:17:15,160 --> 00:17:17,343
तो लीशा, मुझे लगता है कि एक चीज़ जो हमें जल्दी

248
00:17:17,343 --> 00:17:19,480
से सामने लानी चाहिए वह है यह सिग्मॉइड फ़ंक्शन।

249
00:17:19,480 --> 00:17:22,954
जैसा कि मैं इसे समझता हूं, शुरुआती नेटवर्क शून्य और एक के बीच के अंतराल में

250
00:17:22,954 --> 00:17:26,475
प्रासंगिक भारित राशि को निचोड़ने के लिए इसका उपयोग करते हैं, आप जानते हैं कि

251
00:17:26,475 --> 00:17:29,995
न्यूरॉन्स के निष्क्रिय या सक्रिय होने के इस जैविक सादृश्य से प्रेरित होता है।

252
00:17:29,995 --> 00:17:30,459
बिल्कुल।

253
00:17:30,459 --> 00:17:34,026
लेकिन अपेक्षाकृत कुछ आधुनिक नेटवर्क वास्तव में अब सिग्मॉइड का उपयोग करते हैं।

254
00:17:34,026 --> 00:17:34,288
हाँ।

255
00:17:34,288 --> 00:17:35,860
यह एक तरह का पुराना स्कूल है ना?

256
00:17:35,860 --> 00:17:38,872
हाँ या यूँ कहें कि रेलू को प्रशिक्षित करना बहुत आसान लगता है।

257
00:17:38,872 --> 00:17:42,306
और रेलू, रेलू का मतलब रेक्टिफाइड लीनियर यूनिट है?

258
00:17:42,306 --> 00:17:46,576
हाँ, यह इस प्रकार का फ़ंक्शन है जहाँ आप अधिकतम शून्य ले

259
00:17:46,576 --> 00:17:51,074
रहे हैं और जहाँ a दिया गया है जो आप वीडियो में समझा रहे थे।

260
00:17:51,074 --> 00:17:55,910
और मुझे लगता है कि यह किस तरह से प्रेरित था, यह आंशिक रूप से एक

261
00:17:55,910 --> 00:18:00,670
जैविक सादृश्य द्वारा था कि न्यूरॉन्स कैसे सक्रिय होंगे या नहीं।

262
00:18:00,670 --> 00:18:05,009
और इसलिए यदि यह एक निश्चित सीमा पार कर जाता है तो यह पहचान कार्य होगा,

263
00:18:05,009 --> 00:18:09,287
लेकिन यदि ऐसा नहीं होता है तो यह सक्रिय नहीं होगा इसलिए यह शून्य होगा।

264
00:18:09,287 --> 00:18:10,997
तो यह एक तरह का सरलीकरण है।

265
00:18:10,997 --> 00:18:15,212
सिग्मोइड्स का उपयोग करने से प्रशिक्षण में मदद नहीं मिली या कुछ बिंदु पर

266
00:18:15,212 --> 00:18:19,485
प्रशिक्षित करना बहुत मुश्किल था और लोगों ने बस रिले की कोशिश की और यह इन

267
00:18:19,485 --> 00:18:24,052
अविश्वसनीय रूप से गहरे तंत्रिका नेटवर्क के लिए बहुत अच्छी तरह से काम करने लगा।

268
00:18:24,052 --> 00:18:39,080
ठीक है, धन्यवाद लीशा।

