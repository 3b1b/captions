1
00:00:04,219 --> 00:00:05,400
यह 3 है.

2
00:00:06,060 --> 00:00:10,079
इसे 28x28 पिक्सल के बेहद कम रिज़ॉल्यूशन पर लापरवाही से लिखा और प्रस्तुत किया गया है, 

3
00:00:10,079 --> 00:00:13,720
लेकिन आपके मस्तिष्क को इसे 3 के रूप में पहचानने में कोई परेशानी नहीं होती है।

4
00:00:14,340 --> 00:00:16,670
और मैं चाहता हूं कि आप एक पल के लिए इसकी सराहना करें कि 

5
00:00:16,670 --> 00:00:18,960
यह कितना अजीब है कि दिमाग इतनी आसानी से ऐसा कर सकता है।

6
00:00:19,700 --> 00:00:23,710
मेरा मतलब है, यह, यह और यह भी 3s के रूप में पहचाने जाने योग्य हैं, 

7
00:00:23,710 --> 00:00:28,320
भले ही प्रत्येक पिक्सेल का विशिष्ट मान एक छवि से दूसरी छवि में बहुत भिन्न हो।

8
00:00:28,900 --> 00:00:33,024
जब आप इस 3 को देखते हैं तो आपकी आंख में विशेष प्रकाश-संवेदनशील कोशिकाएं सक्रिय 

9
00:00:33,024 --> 00:00:36,940
हो जाती हैं, जब आप इस 3 को देखते हैं तो चमकती कोशिकाएं बहुत भिन्न होती हैं।

10
00:00:37,520 --> 00:00:41,010
लेकिन आपके उस पागल-स्मार्ट विज़ुअल कॉर्टेक्स में कुछ चीज़ इन्हें 

11
00:00:41,010 --> 00:00:44,017
एक ही विचार का प्रतिनिधित्व करने के रूप में हल करती है, 

12
00:00:44,017 --> 00:00:48,260
जबकि साथ ही अन्य छवियों को अपने स्वयं के विशिष्ट विचारों के रूप में पहचानती है।

13
00:00:49,220 --> 00:00:54,828
लेकिन अगर मैंने आपसे कहा, अरे, बैठो और मेरे लिए एक प्रोग्राम लिखो जो 28x28 की ग्रिड 

14
00:00:54,828 --> 00:00:58,568
लेता है और 0 और 10 के बीच एक एकल संख्या आउटपुट करता है, 

15
00:00:58,568 --> 00:01:02,173
और आपको बताता है कि वह अंक के बारे में क्या सोचता है, 

16
00:01:02,173 --> 00:01:06,180
तो यह कार्य हास्यास्पद रूप से मामूली हो जाता है अत्यंत कठिन.

17
00:01:07,160 --> 00:01:09,626
जब तक आप किसी चट्टान के नीचे नहीं रह रहे हैं, मुझे लगता है कि 

18
00:01:09,626 --> 00:01:12,133
मुझे वर्तमान और भविष्य के लिए मशीन लर्निंग और तंत्रिका नेटवर्क 

19
00:01:12,133 --> 00:01:14,640
की प्रासंगिकता और महत्व को प्रेरित करने की शायद ही आवश्यकता है।

20
00:01:15,120 --> 00:01:18,370
लेकिन मैं यहां आपको यह दिखाना चाहता हूं कि तंत्रिका नेटवर्क वास्तव में क्या है, 

21
00:01:18,370 --> 00:01:21,499
बिना किसी पृष्ठभूमि के, और यह कल्पना करने में मदद करना कि यह क्या कर रहा है, 

22
00:01:21,499 --> 00:01:24,140
एक चर्चा शब्द के रूप में नहीं बल्कि गणित के एक टुकड़े के रूप में।

23
00:01:24,140 --> 00:01:27,903
मेरी आशा बस इतनी है कि आप यह महसूस करते हुए आएं कि संरचना स्वयं प्रेरित है, 

24
00:01:27,903 --> 00:01:31,171
और जब आप तंत्रिका नेटवर्क उद्धरण-अनउद्धरण सीखने के बारे में पढ़ते 

25
00:01:31,171 --> 00:01:34,340
हैं या सुनते हैं तो आपको ऐसा महसूस होता है कि इसका क्या मतलब है।

26
00:01:35,360 --> 00:01:40,260
यह वीडियो केवल इसके संरचना घटक के लिए समर्पित होगा, और अगला वीडियो सीखने से संबंधित होगा।

27
00:01:40,960 --> 00:01:43,427
हम जो करने जा रहे हैं वह एक तंत्रिका नेटवर्क को एक 

28
00:01:43,427 --> 00:01:46,040
साथ रखना है जो हस्तलिखित अंकों को पहचानना सीख सकता है।

29
00:01:49,360 --> 00:01:51,977
विषय को प्रस्तुत करने के लिए यह कुछ हद तक उत्कृष्ट उदाहरण है, 

30
00:01:51,977 --> 00:01:54,299
और मुझे यहां यथास्थिति पर बने रहने में खुशी हो रही है, 

31
00:01:54,299 --> 00:01:57,676
क्योंकि दो वीडियो के अंत में मैं आपको कुछ अच्छे संसाधनों की ओर इंगित करना चाहता 

32
00:01:57,676 --> 00:02:01,138
हूं जहां आप और अधिक सीख सकते हैं, और कहां आप ऐसा करने वाले कोड को डाउनलोड कर सकते 

33
00:02:01,138 --> 00:02:03,080
हैं और अपने कंप्यूटर पर उसके साथ खेल सकते हैं।

34
00:02:05,040 --> 00:02:09,603
तंत्रिका नेटवर्क के कई प्रकार हैं, और हाल के वर्षों में इन प्रकारों के 

35
00:02:09,603 --> 00:02:14,166
प्रति अनुसंधान में तेजी आई है, लेकिन इन दो परिचयात्मक वीडियो में आप और 

36
00:02:14,166 --> 00:02:19,180
मैं बिना किसी अतिरिक्त तामझाम के सबसे सरल सादे वेनिला रूप को देखने जा रहे हैं।

37
00:02:19,860 --> 00:02:24,037
किसी भी अधिक शक्तिशाली आधुनिक संस्करण को समझने के लिए यह एक आवश्यक शर्त है, 

38
00:02:24,037 --> 00:02:28,600
और मेरा विश्वास करें कि इसमें अभी भी हमारे दिमाग को समझने के लिए काफी जटिलताएं हैं।

39
00:02:29,120 --> 00:02:33,804
लेकिन इस सरलतम रूप में भी यह हस्तलिखित अंकों को पहचानना सीख सकता है, 

40
00:02:33,804 --> 00:02:36,520
जो एक कंप्यूटर के लिए बहुत अच्छी बात है।

41
00:02:37,480 --> 00:02:39,984
और साथ ही आप देखेंगे कि कैसे यह कुछ उम्मीदों से 

42
00:02:39,984 --> 00:02:42,280
कम हो जाता है जो हमारी इसके लिए हो सकती हैं।

43
00:02:43,380 --> 00:02:47,298
जैसा कि नाम से पता चलता है, तंत्रिका नेटवर्क मस्तिष्क से प्रेरित होते हैं, 

44
00:02:47,298 --> 00:02:48,500
लेकिन आइए इसे तोड़ दें।

45
00:02:48,520 --> 00:02:51,660
न्यूरॉन्स क्या हैं, और वे किस अर्थ में एक साथ जुड़े हुए हैं?

46
00:02:52,500 --> 00:02:56,442
अभी जब मैं न्यूरॉन कहता हूं, तो मैं चाहता हूं कि आप एक ऐसी चीज के बारे 

47
00:02:56,442 --> 00:03:00,440
में सोचें जो एक संख्या रखती है, विशेष रूप से 0 और 1 के बीच की एक संख्या।

48
00:03:00,680 --> 00:03:02,000
यह वास्तव में उससे अधिक नहीं है.

49
00:03:02,000 --> 00:03:07,871
उदाहरण के लिए, नेटवर्क इनपुट छवि के 28 गुना 28 पिक्सेल में से प्रत्येक के 

50
00:03:07,871 --> 00:03:14,220
अनुरूप न्यूरॉन्स के एक समूह के साथ शुरू होता है, जो कुल मिलाकर 784 न्यूरॉन्स है।

51
00:03:14,700 --> 00:03:19,570
इनमें से प्रत्येक में एक संख्या होती है जो संबंधित पिक्सेल के ग्रेस्केल मान को 

52
00:03:19,570 --> 00:03:24,380
दर्शाती है, जो काले पिक्सेल के लिए 0 से लेकर सफेद पिक्सेल के लिए 1 तक होती है।

53
00:03:25,300 --> 00:03:28,339
न्यूरॉन के अंदर की इस संख्या को इसकी सक्रियता कहा जाता है, 

54
00:03:28,339 --> 00:03:32,975
और आपके मन में यह छवि हो सकती है कि प्रत्येक न्यूरॉन तब प्रकाशित होता है जब इसकी सक्रियता 

55
00:03:32,975 --> 00:03:34,160
एक उच्च संख्या होती है।

56
00:03:36,720 --> 00:03:41,860
तो ये सभी 784 न्यूरॉन्स हमारे नेटवर्क की पहली परत बनाते हैं।

57
00:03:46,500 --> 00:03:49,136
अब अंतिम परत पर जाएं, इसमें 10 न्यूरॉन हैं, जिनमें 

58
00:03:49,136 --> 00:03:51,360
से प्रत्येक एक अंक का प्रतिनिधित्व करता है।

59
00:03:52,040 --> 00:03:56,504
इन न्यूरॉन्स में सक्रियता, फिर से कुछ संख्या जो 0 और 1 के बीच है, 

60
00:03:56,504 --> 00:04:02,120
यह दर्शाती है कि सिस्टम कितना सोचता है कि दी गई छवि किसी दिए गए अंक से मेल खाती है।

61
00:04:03,040 --> 00:04:07,045
बीच में कुछ परतें भी होती हैं जिन्हें छिपी हुई परतें कहा जाता है, 

62
00:04:07,045 --> 00:04:12,325
जो फिलहाल एक बड़ा प्रश्नचिह्न होना चाहिए कि पृथ्वी पर अंकों को पहचानने की इस प्रक्रिया 

63
00:04:12,325 --> 00:04:13,600
को कैसे संभाला जाएगा।

64
00:04:14,260 --> 00:04:16,959
इस नेटवर्क में मैंने दो छिपी हुई परतों को चुना, 

65
00:04:16,959 --> 00:04:20,560
प्रत्येक में 16 न्यूरॉन्स थे, और माना कि यह एक मनमाना विकल्प है।

66
00:04:21,019 --> 00:04:24,631
ईमानदारी से कहूं तो, मैंने इस आधार पर दो परतें चुनीं कि मैं संरचना को एक पल में कैसे 

67
00:04:24,631 --> 00:04:28,200
प्रेरित करना चाहता हूं, और 16, खैर यह स्क्रीन पर फिट होने के लिए एक अच्छी संख्या थी।

68
00:04:28,780 --> 00:04:32,340
व्यवहार में यहां एक विशिष्ट संरचना के साथ प्रयोग के लिए बहुत जगह है।

69
00:04:33,020 --> 00:04:35,721
जिस तरह से नेटवर्क संचालित होता है, एक परत में 

70
00:04:35,721 --> 00:04:38,480
सक्रियता अगली परत की सक्रियता निर्धारित करती है।

71
00:04:39,200 --> 00:04:43,951
और निश्चित रूप से एक सूचना प्रसंस्करण तंत्र के रूप में नेटवर्क का मूल इस बात 

72
00:04:43,951 --> 00:04:48,580
पर निर्भर करता है कि कैसे एक परत से सक्रियता अगली परत में सक्रियता लाती है।

73
00:04:49,140 --> 00:04:52,828
इसका मतलब यह है कि न्यूरॉन्स के जैविक नेटवर्क में 

74
00:04:52,828 --> 00:04:57,180
न्यूरॉन्स के कुछ समूह किस तरह से दूसरों को सक्रिय करते हैं।

75
00:04:58,120 --> 00:05:00,795
अब जो नेटवर्क मैं यहां दिखा रहा हूं उसे पहले से ही अंकों को पहचानने के लिए 

76
00:05:00,795 --> 00:05:03,400
प्रशिक्षित किया गया है, और मैं आपको दिखाता हूं कि इससे मेरा क्या मतलब है।

77
00:05:03,640 --> 00:05:08,294
इसका मतलब है कि यदि आप छवि में प्रत्येक पिक्सेल की चमक के अनुसार इनपुट परत के 

78
00:05:08,294 --> 00:05:11,935
सभी 784 न्यूरॉन्स को रोशन करते हुए एक छवि में फ़ीड करते हैं, 

79
00:05:11,935 --> 00:05:16,410
तो सक्रियण का पैटर्न अगली परत में कुछ बहुत विशिष्ट पैटर्न का कारण बनता है, 

80
00:05:16,410 --> 00:05:19,454
जो एक के बाद एक में कुछ पैटर्न का कारण बनता है यह, 

81
00:05:19,454 --> 00:05:22,080
जो अंततः आउटपुट लेयर में कुछ पैटर्न देता है।

82
00:05:22,560 --> 00:05:26,008
और उस आउटपुट परत का सबसे चमकीला न्यूरॉन नेटवर्क की पसंद है, 

83
00:05:26,008 --> 00:05:29,400
इसलिए बोलने के लिए, यह छवि किस अंक का प्रतिनिधित्व करती है।

84
00:05:32,560 --> 00:05:36,145
और गणित में कूदने से पहले कि एक परत अगली परत को कैसे प्रभावित करती है, 

85
00:05:36,145 --> 00:05:39,782
या प्रशिक्षण कैसे काम करता है, आइए बस इस बारे में बात करें कि इस तरह की 

86
00:05:39,782 --> 00:05:43,520
स्तरित संरचना से बुद्धिमानी से व्यवहार करने की अपेक्षा करना क्यों उचित है।

87
00:05:44,060 --> 00:05:45,220
हम यहाँ क्या उम्मीद कर रहे हैं?

88
00:05:45,400 --> 00:05:47,600
वे मध्य परतें क्या कर रही होंगी, इसके लिए सबसे अच्छी आशा क्या है?

89
00:05:48,920 --> 00:05:53,520
खैर, जब आप या मैं अंकों को पहचानते हैं, तो हम विभिन्न घटकों को एक साथ जोड़ते हैं।

90
00:05:54,200 --> 00:05:56,820
9 में ऊपर एक लूप और दाहिनी ओर एक रेखा होती है।

91
00:05:57,380 --> 00:06:01,180
8 में ऊपर की ओर एक लूप भी होता है, लेकिन इसे नीचे की ओर एक अन्य लूप के साथ जोड़ा जाता है।

92
00:06:01,980 --> 00:06:06,820
A 4 मूलतः तीन विशिष्ट रेखाओं और इसी तरह की चीज़ों में टूट जाता है।

93
00:06:07,600 --> 00:06:11,487
अब एक आदर्श दुनिया में, हम उम्मीद कर सकते हैं कि दूसरी से आखिरी परत 

94
00:06:11,487 --> 00:06:14,860
में प्रत्येक न्यूरॉन इन उप-घटकों में से एक से मेल खाता है, 

95
00:06:14,860 --> 00:06:18,805
जब भी आप किसी छवि में फ़ीड करते हैं, उदाहरण के लिए, शीर्ष पर एक लूप, 

96
00:06:18,805 --> 00:06:23,780
जैसे 9 या 8, तो वहां कुछ होता है विशिष्ट न्यूरॉन जिसकी सक्रियता 1 के करीब होने वाली है।

97
00:06:24,500 --> 00:06:27,114
और मेरा मतलब पिक्सल के इस विशिष्ट लूप से नहीं है, 

98
00:06:27,114 --> 00:06:31,560
आशा यह होगी कि शीर्ष की ओर कोई भी आम तौर पर लूपी पैटर्न इस न्यूरॉन को बंद कर देता है।

99
00:06:32,440 --> 00:06:36,119
इस तरह, तीसरी परत से अंतिम परत तक जाने के लिए बस यह सीखने की 

100
00:06:36,119 --> 00:06:40,040
आवश्यकता है कि उपघटकों का कौन सा संयोजन किन अंकों से मेल खाता है।

101
00:06:41,000 --> 00:06:45,143
बेशक, इससे समस्या ख़त्म हो जाती है, क्योंकि आप इन उप-घटकों को कैसे पहचानेंगे, 

102
00:06:45,143 --> 00:06:47,640
या यह भी सीखेंगे कि सही उप-घटक क्या होने चाहिए?

103
00:06:48,060 --> 00:06:50,579
और मैंने अभी तक इस बारे में बात भी नहीं की है कि एक परत दूसरी परत 

104
00:06:50,579 --> 00:06:53,060
को कैसे प्रभावित करती है, लेकिन एक पल के लिए इस पर मेरे साथ चलें।

105
00:06:53,680 --> 00:06:56,680
लूप को पहचानने से उप-समस्याएँ भी विभाजित हो सकती हैं।

106
00:06:57,280 --> 00:06:59,675
ऐसा करने का एक उचित तरीका यह होगा कि पहले इसे 

107
00:06:59,675 --> 00:07:02,280
बनाने वाले विभिन्न छोटे किनारों को पहचान लिया जाए।

108
00:07:02,280 --> 00:07:06,910
इसी प्रकार, एक लंबी रेखा जैसी कि आप अंक 1 या 4 या 7 में देख सकते हैं, 

109
00:07:06,910 --> 00:07:11,012
वास्तव में यह सिर्फ एक लंबा किनारा है, या शायद आप इसे कई छोटे 

110
00:07:11,012 --> 00:07:14,320
किनारों के एक निश्चित पैटर्न के रूप में सोचते हैं।

111
00:07:15,140 --> 00:07:19,160
तो शायद हमारी आशा यह है कि नेटवर्क की दूसरी परत में प्रत्येक 

112
00:07:19,160 --> 00:07:22,720
न्यूरॉन विभिन्न प्रासंगिक छोटे किनारों से मेल खाता है।

113
00:07:23,540 --> 00:07:28,933
हो सकता है कि जब इस तरह की कोई छवि आती है, तो यह लगभग 8 से 10 विशिष्ट छोटे किनारों 

114
00:07:28,933 --> 00:07:34,261
से जुड़े सभी न्यूरॉन्स को रोशन करती है, जो बदले में ऊपरी लूप और एक लंबी ऊर्ध्वाधर 

115
00:07:34,261 --> 00:07:39,720
रेखा से जुड़े न्यूरॉन्स को रोशन करती है, और वे प्रकाश डालते हैं 9 से संबद्ध न्यूरॉन।

116
00:07:40,680 --> 00:07:43,559
हमारा अंतिम नेटवर्क वास्तव में ऐसा करता है या नहीं, यह एक और सवाल है, 

117
00:07:43,559 --> 00:07:47,180
जिस पर मैं एक बार फिर विचार करूंगा जब हम देखेंगे कि नेटवर्क को कैसे प्रशिक्षित किया जाए।

118
00:07:47,500 --> 00:07:50,127
लेकिन यह एक आशा है कि हमारे पास इस तरह की स्तरित 

119
00:07:50,127 --> 00:07:52,540
संरचना के साथ एक प्रकार का लक्ष्य हो सकता है।

120
00:07:53,160 --> 00:07:56,778
इसके अलावा, आप कल्पना कर सकते हैं कि इस तरह किनारों और पैटर्न का पता लगाने 

121
00:07:56,778 --> 00:08:00,300
में सक्षम होना अन्य छवि पहचान कार्यों के लिए वास्तव में कैसे उपयोगी होगा।

122
00:08:00,880 --> 00:08:04,052
और छवि पहचान से परे भी, सभी प्रकार की बुद्धिमान चीजें हैं 

123
00:08:04,052 --> 00:08:07,280
जो आप करना चाहते हैं जो अमूर्तता की परतों में टूट जाती हैं।

124
00:08:08,040 --> 00:08:11,909
उदाहरण के लिए, भाषण को पार्स करने में कच्चा ऑडियो लेना और अलग-अलग 

125
00:08:11,909 --> 00:08:15,427
ध्वनियों को चुनना शामिल है, जो मिलकर कुछ शब्दांश बनाते हैं, 

126
00:08:15,427 --> 00:08:20,060
जो मिलकर शब्द बनाते हैं, जो मिलकर वाक्यांश और अधिक अमूर्त विचार बनाते हैं, आदि।

127
00:08:21,100 --> 00:08:24,737
लेकिन इनमें से कोई भी वास्तव में कैसे काम करता है, इस पर वापस लौटते हुए, 

128
00:08:24,737 --> 00:08:28,873
अभी स्वयं कल्पना करें कि एक परत में सक्रियता वास्तव में अगली परत में सक्रियता कैसे 

129
00:08:28,873 --> 00:08:29,920
निर्धारित कर सकती है।

130
00:08:30,860 --> 00:08:34,884
लक्ष्य कुछ ऐसा तंत्र बनाना है जो पिक्सेल को किनारों में, 

131
00:08:34,884 --> 00:08:38,980
या किनारों को पैटर्न में, या पैटर्न को अंकों में जोड़ सके।

132
00:08:39,440 --> 00:08:42,767
और एक बहुत ही विशिष्ट उदाहरण पर ज़ूम करने के लिए, 

133
00:08:42,767 --> 00:08:48,357
मान लें कि दूसरी परत में एक विशेष न्यूरॉन से यह पता लगाने की उम्मीद है कि छवि का इस 

134
00:08:48,357 --> 00:08:50,620
क्षेत्र में कोई किनारा है या नहीं।

135
00:08:51,440 --> 00:08:55,100
सवाल यह है कि नेटवर्क में कौन से पैरामीटर होने चाहिए?

136
00:08:55,640 --> 00:08:59,648
आपको किस डायल और नॉब को बदलने में सक्षम होना चाहिए ताकि यह इस पैटर्न, 

137
00:08:59,648 --> 00:09:03,656
या किसी अन्य पिक्सेल पैटर्न, या पैटर्न को कैप्चर करने के लिए पर्याप्त 

138
00:09:03,656 --> 00:09:07,780
रूप से अभिव्यंजक हो, जिससे कई किनारे एक लूप बना सकें, और ऐसी अन्य चीजें?

139
00:09:08,720 --> 00:09:11,886
खैर, हम जो करेंगे वह हमारे न्यूरॉन और पहली परत के 

140
00:09:11,886 --> 00:09:15,560
न्यूरॉन्स के बीच प्रत्येक कनेक्शन को एक भार प्रदान करेंगे।

141
00:09:16,320 --> 00:09:17,700
ये वज़न महज़ संख्याएँ हैं।

142
00:09:18,540 --> 00:09:22,134
फिर उन सभी सक्रियताओं को पहली परत से लें और इन 

143
00:09:22,134 --> 00:09:25,500
भारों के अनुसार उनके भारित योग की गणना करें।

144
00:09:27,700 --> 00:09:31,412
मुझे इन वजनों को अपने स्वयं के एक छोटे ग्रिड में व्यवस्थित करने के बारे में 

145
00:09:31,412 --> 00:09:35,173
सोचने में मदद मिलती है, और मैं सकारात्मक वजन को इंगित करने के लिए हरे पिक्सल 

146
00:09:35,173 --> 00:09:38,983
का उपयोग करने जा रहा हूं, और नकारात्मक वजन को इंगित करने के लिए लाल पिक्सल का 

147
00:09:38,983 --> 00:09:42,940
उपयोग करने जा रहा हूं, जहां उस पिक्सेल की चमक कुछ है वजन के मूल्य का ढीला चित्रण.

148
00:09:42,940 --> 00:09:47,590
यदि हमने इस क्षेत्र में कुछ सकारात्मक भारों को छोड़कर, जिनकी हम परवाह करते हैं, 

149
00:09:47,590 --> 00:09:50,554
लगभग सभी पिक्सेल से जुड़े भार को शून्य कर दिया है, 

150
00:09:50,554 --> 00:09:55,495
तो सभी पिक्सेल मानों का भारित योग लेना वास्तव में केवल पिक्सेल के मानों को जोड़ने के 

151
00:09:55,495 --> 00:09:57,820
समान है। वह क्षेत्र जिसकी हमें परवाह है।

152
00:09:59,140 --> 00:10:02,870
और यदि आप वास्तव में यह जानना चाहते हैं कि क्या यहां कोई बढ़त है, 

153
00:10:02,870 --> 00:10:06,600
तो आप आसपास के पिक्सेल से जुड़े कुछ नकारात्मक भार को समझ सकते हैं।

154
00:10:07,480 --> 00:10:10,224
तब योग सबसे बड़ा होता है जब वे मध्य पिक्सेल चमकीले 

155
00:10:10,224 --> 00:10:12,700
होते हैं लेकिन आसपास के पिक्सेल गहरे होते हैं।

156
00:10:14,260 --> 00:10:19,252
जब आप इस तरह एक भारित राशि की गणना करते हैं, तो आप किसी भी संख्या के साथ आ सकते हैं, 

157
00:10:19,252 --> 00:10:23,540
लेकिन इस नेटवर्क के लिए हम चाहते हैं कि सक्रियण 0 और 1 के बीच कुछ मान हो।

158
00:10:24,120 --> 00:10:28,130
तो एक सामान्य बात यह है कि इस भारित योग को किसी फ़ंक्शन में पंप किया 

159
00:10:28,130 --> 00:10:32,140
जाए जो वास्तविक संख्या रेखा को 0 और 1 के बीच की सीमा में दबा देता है।

160
00:10:32,460 --> 00:10:35,031
और ऐसा करने वाला एक सामान्य कार्य सिग्मॉइड फ़ंक्शन कहलाता है, 

161
00:10:35,031 --> 00:10:36,980
जिसे लॉजिस्टिक वक्र के रूप में भी जाना जाता है।

162
00:10:36,980 --> 00:10:40,738
मूल रूप से बहुत नकारात्मक इनपुट 0 के करीब समाप्त होते हैं, 

163
00:10:40,738 --> 00:10:43,796
बहुत सकारात्मक इनपुट 1 के करीब समाप्त होते हैं, 

164
00:10:43,796 --> 00:10:46,600
और यह इनपुट 0 के आसपास लगातार बढ़ता जाता है।

165
00:10:49,120 --> 00:10:52,740
तो यहां न्यूरॉन की सक्रियता मूल रूप से इस बात का 

166
00:10:52,740 --> 00:10:56,360
माप है कि प्रासंगिक भारित योग कितना सकारात्मक है।

167
00:10:57,540 --> 00:10:59,945
लेकिन शायद ऐसा नहीं है कि आप चाहते हैं कि जब भारित 

168
00:10:59,945 --> 00:11:01,880
योग 0 से बड़ा हो तो न्यूरॉन प्रकाशमान हो।

169
00:11:02,280 --> 00:11:06,360
हो सकता है कि आप इसे केवल तभी सक्रिय करना चाहते हों जब योग 10 से बड़ा हो।

170
00:11:06,840 --> 00:11:10,260
यानी, आप इसके निष्क्रिय होने के लिए कुछ पूर्वाग्रह चाहते हैं।

171
00:11:11,380 --> 00:11:15,461
इसके बाद हम इस भारित योग को सिग्मॉइड स्क्विशिफिकेशन फ़ंक्शन के माध्यम 

172
00:11:15,461 --> 00:11:19,660
से प्लग करने से पहले इसमें कोई अन्य संख्या, जैसे ऋणात्मक 10, जोड़ देंगे।

173
00:11:20,580 --> 00:11:22,440
उस अतिरिक्त संख्या को पूर्वाग्रह कहा जाता है।

174
00:11:23,460 --> 00:11:28,587
तो वज़न आपको बताता है कि दूसरी परत में यह न्यूरॉन किस पिक्सेल पैटर्न को उठा रहा है, 

175
00:11:28,587 --> 00:11:32,433
और पूर्वाग्रह आपको बताता है कि न्यूरॉन के सार्थक रूप से सक्रिय 

176
00:11:32,433 --> 00:11:35,180
होने से पहले भारित योग कितना अधिक होना चाहिए।

177
00:11:36,120 --> 00:11:37,680
और वह सिर्फ एक न्यूरॉन है.

178
00:11:38,280 --> 00:11:45,462
इस परत का हर दूसरा न्यूरॉन पहली परत के सभी 784 पिक्सेल न्यूरॉन्स से जुड़ा होगा, 

179
00:11:45,462 --> 00:11:50,940
और उन 784 कनेक्शनों में से प्रत्येक का अपना वजन जुड़ा हुआ है।

180
00:11:51,600 --> 00:11:53,805
इसके अलावा, प्रत्येक में कुछ पूर्वाग्रह होते हैं, 

181
00:11:53,805 --> 00:11:57,600
कुछ अन्य संख्याएं जिन्हें आप सिग्मॉइड के साथ कुचलने से पहले भारित राशि में जोड़ते हैं।

182
00:11:58,110 --> 00:11:59,540
और यह बहुत सोचने वाली बात है!

183
00:11:59,960 --> 00:12:07,980
16 न्यूरॉन्स की इस छिपी हुई परत के साथ, 16 पूर्वाग्रहों के साथ, यह कुल 784 गुना 16 भार है।

184
00:12:08,840 --> 00:12:11,940
और यह सब पहली परत से दूसरी परत तक का कनेक्शन मात्र है।

185
00:12:12,520 --> 00:12:17,340
अन्य परतों के बीच के संबंधों में बहुत सारे वजन और पूर्वाग्रह भी जुड़े हुए हैं।

186
00:12:18,340 --> 00:12:23,800
सब कुछ कहा और किया गया, इस नेटवर्क में लगभग 13,000 कुल भार और पूर्वाग्रह हैं।

187
00:12:23,800 --> 00:12:26,674
13,000 नॉब और डायल जिन्हें इस नेटवर्क को अलग-अलग 

188
00:12:26,674 --> 00:12:29,960
तरीकों से संचालित करने के लिए बदला और घुमाया जा सकता है।

189
00:12:31,040 --> 00:12:35,972
इसलिए जब हम सीखने के बारे में बात करते हैं, तो इसका मतलब कंप्यूटर को इन सभी 

190
00:12:35,972 --> 00:12:41,360
संख्याओं के लिए एक वैध सेटिंग ढूंढना है ताकि वह वास्तव में समस्या का समाधान कर सके।

191
00:12:42,620 --> 00:12:47,209
एक विचार प्रयोग जो मजेदार भी है और भयावह भी, वह है बैठकर कल्पना करना और 

192
00:12:47,209 --> 00:12:50,333
इन सभी वजनों और पूर्वाग्रहों को हाथ से सेट करना, 

193
00:12:50,333 --> 00:12:54,285
जानबूझकर संख्याओं को बदलना ताकि दूसरी परत किनारों को पकड़ ले, 

194
00:12:54,285 --> 00:12:56,580
तीसरी परत पैटर्न को पहचान ले, वगैरह।

195
00:12:56,980 --> 00:13:00,410
मैं व्यक्तिगत रूप से नेटवर्क को संपूर्ण ब्लैक बॉक्स के रूप में मानने के 

196
00:13:00,410 --> 00:13:03,745
बजाय इसे संतोषजनक मानता हूं, क्योंकि जब नेटवर्क आपके अनुमान के अनुसार 

197
00:13:03,745 --> 00:13:07,938
प्रदर्शन नहीं करता है, तो यदि आपने उन भारों और पूर्वाग्रहों का वास्तव में क्या मतलब है, 

198
00:13:07,938 --> 00:13:11,368
इसके साथ थोड़ा सा संबंध बना लिया है , आपके पास यह प्रयोग करने के लिए एक 

199
00:13:11,368 --> 00:13:14,180
प्रारंभिक स्थान है कि सुधार के लिए संरचना को कैसे बदला जाए।

200
00:13:14,960 --> 00:13:18,653
या जब नेटवर्क काम करता है, लेकिन उन कारणों के लिए नहीं जिनकी आप उम्मीद कर सकते हैं, 

201
00:13:18,653 --> 00:13:22,214
तो वज़न और पूर्वाग्रह क्या कर रहे हैं, इसकी खोज करना आपकी धारणाओं को चुनौती देने 

202
00:13:22,214 --> 00:13:25,820
और वास्तव में संभावित समाधानों की पूरी गुंजाइश को उजागर करने का एक अच्छा तरीका है।

203
00:13:26,840 --> 00:13:30,680
वैसे, यहाँ वास्तविक कार्य को लिखना थोड़ा बोझिल है, क्या आपको नहीं लगता?

204
00:13:32,500 --> 00:13:34,759
तो आइए मैं आपको एक अधिक सांकेतिक रूप से संक्षिप्त तरीका 

205
00:13:34,759 --> 00:13:37,140
दिखाता हूं जिससे इन कनेक्शनों का प्रतिनिधित्व किया जाता है।

206
00:13:37,660 --> 00:13:40,520
यदि आप तंत्रिका नेटवर्क के बारे में अधिक पढ़ना चुनते हैं तो आप इसे इसी तरह देखेंगे।

207
00:13:41,380 --> 00:13:44,420
सभी सक्रियणों को एक परत से एक वेक्टर के रूप में एक कॉलम में व्यवस्थित करें।

208
00:13:44,420 --> 00:13:49,828
फिर सभी भारों को एक मैट्रिक्स के रूप में व्यवस्थित करें, 

209
00:13:49,828 --> 00:13:57,418
जहां उस मैट्रिक्स की प्रत्येक पंक्ति एक परत और अगली परत में एक विशेष न्यूरॉन के 

210
00:13:57,418 --> 00:13:59,980
बीच कनेक्शन से मेल खाती है।

211
00:13:59,980 --> 00:14:07,225
इसका मतलब यह है कि इन भारों के अनुसार पहली परत में सक्रियणों का भारित योग लेना हमारे 

212
00:14:07,225 --> 00:14:14,300
यहां बाईं ओर मौजूद हर चीज के मैट्रिक्स वेक्टर उत्पाद में से एक शब्द से मेल खाता है।

213
00:14:14,660 --> 00:14:18,980
वैसे, मशीन सीखने का इतना सारा काम सिर्फ रैखिक बीजगणित की अच्छी समझ के कारण होता है, 

214
00:14:18,980 --> 00:14:22,324
इसलिए आप में से जो लोग मैट्रिक्स के लिए एक अच्छी दृश्य समझ चाहते 

215
00:14:22,324 --> 00:14:25,616
हैं और मैट्रिक्स वेक्टर गुणन का क्या मतलब है, मेरे द्वारा की गई 

216
00:14:25,616 --> 00:14:28,600
श्रृंखला पर एक नज़र डालें रैखिक बीजगणित, विशेषकर अध्याय 3।

217
00:14:29,240 --> 00:14:33,512
अपनी अभिव्यक्ति पर वापस जाएं, इन मूल्यों में से प्रत्येक में पूर्वाग्रह को स्वतंत्र रूप 

218
00:14:33,512 --> 00:14:37,881
से जोड़ने के बारे में बात करने के बजाय, हम उन सभी पूर्वाग्रहों को एक वेक्टर में व्यवस्थित 

219
00:14:37,881 --> 00:14:42,105
करके और पूरे वेक्टर को पिछले मैट्रिक्स वेक्टर उत्पाद में जोड़कर इसका प्रतिनिधित्व करते 

220
00:14:42,105 --> 00:14:42,300
हैं।

221
00:14:43,280 --> 00:14:47,396
फिर अंतिम चरण के रूप में, मैं यहां बाहर के चारों ओर एक सिग्मॉइड लपेटूंगा, 

222
00:14:47,396 --> 00:14:51,123
और जो प्रतिनिधित्व करने वाला है वह यह है कि आप अंदर परिणामी वेक्टर 

223
00:14:51,123 --> 00:14:54,740
के प्रत्येक विशिष्ट घटक पर सिग्मॉइड फ़ंक्शन लागू करने जा रहे हैं।

224
00:14:55,940 --> 00:15:00,912
इसलिए एक बार जब आप इस वेट मैट्रिक्स और इन वैक्टरों को अपने स्वयं के प्रतीकों के रूप में 

225
00:15:00,912 --> 00:15:05,884
लिख लेते हैं, तो आप एक परत से दूसरी परत तक सक्रियताओं के पूर्ण संक्रमण को बेहद चुस्त और 

226
00:15:05,884 --> 00:15:08,879
साफ-सुथरी छोटी अभिव्यक्ति में संप्रेषित कर सकते हैं, 

227
00:15:08,879 --> 00:15:12,439
और यह प्रासंगिक कोड को बहुत सरल और सरल बना देता है। बहुत तेज़, 

228
00:15:12,439 --> 00:15:15,660
क्योंकि कई लाइब्रेरी मैट्रिक्स गुणन को अनुकूलित करती हैं।

229
00:15:17,820 --> 00:15:19,717
याद रखें कि मैंने पहले कैसे कहा था कि ये न्यूरॉन 

230
00:15:19,717 --> 00:15:21,460
केवल ऐसी चीज़ें हैं जिनमें संख्याएँ होती हैं?

231
00:15:22,220 --> 00:15:27,388
बेशक, उनके पास मौजूद विशिष्ट संख्याएं आपके द्वारा फीड की गई छवि पर निर्भर करती हैं, 

232
00:15:27,388 --> 00:15:32,248
इसलिए वास्तव में प्रत्येक न्यूरॉन को एक फ़ंक्शन के रूप में सोचना अधिक सटीक है, 

233
00:15:32,248 --> 00:15:36,801
जो पिछली परत के सभी न्यूरॉन्स के आउटपुट लेता है, और एक को बाहर निकालता है।

234
00:15:36,801 --> 00:15:38,340
 0 और 1 के बीच की संख्या.

235
00:15:39,200 --> 00:15:43,130
वास्तव में पूरा नेटवर्क सिर्फ एक फ़ंक्शन है, जो इनपुट के रूप 

236
00:15:43,130 --> 00:15:47,060
में 784 नंबर लेता है और आउटपुट के रूप में 10 नंबर निकालता है।

237
00:15:47,560 --> 00:15:52,567
यह एक बेतुका जटिल फ़ंक्शन है, जिसमें इन वज़न और पूर्वाग्रहों के रूप में 13,000 पैरामीटर 

238
00:15:52,567 --> 00:15:57,689
शामिल हैं जो कुछ पैटर्न पर आधारित हैं, और जिसमें कई मैट्रिक्स वेक्टर उत्पादों और सिग्मॉइड 

239
00:15:57,689 --> 00:16:02,640
स्क्विशिफिकेशन फ़ंक्शन को पुनरावृत्त करना शामिल है, लेकिन फिर भी यह केवल एक फ़ंक्शन है।

240
00:16:03,400 --> 00:16:06,660
और एक तरह से यह आश्वस्त करने वाला है कि यह जटिल दिखता है।

241
00:16:07,340 --> 00:16:09,832
मेरा मतलब है कि यदि यह और भी सरल होता, तो हमें क्या आशा 

242
00:16:09,832 --> 00:16:12,280
होती कि यह अंकों को पहचानने की चुनौती का सामना कर सकता?

243
00:16:13,340 --> 00:16:14,700
और यह उस चुनौती को कैसे स्वीकार करता है?

244
00:16:15,080 --> 00:16:19,360
यह नेटवर्क केवल डेटा को देखकर उचित भार और पूर्वाग्रह कैसे सीखता है?

245
00:16:20,140 --> 00:16:22,887
खैर, यही मैं अगले वीडियो में दिखाऊंगा, और मैं यह भी 

246
00:16:22,887 --> 00:16:25,740
जानूंगा कि यह विशेष नेटवर्क वास्तव में क्या कर रहा है।

247
00:16:25,740 --> 00:16:29,539
अब मुद्दा यह है कि मुझे लगता है कि मुझे यह कहना चाहिए कि जब वह वीडियो या कोई नया 

248
00:16:29,539 --> 00:16:32,447
वीडियो आता है तो उसके बारे में सूचित रहने के लिए सदस्यता लें, 

249
00:16:32,447 --> 00:16:36,434
लेकिन वास्तविकता यह है कि आप में से अधिकांश को वास्तव में YouTube से सूचनाएं प्राप्त 

250
00:16:36,434 --> 00:16:37,420
नहीं होती हैं, है ना?

251
00:16:38,020 --> 00:16:41,352
शायद अधिक ईमानदारी से मुझे यह कहना चाहिए कि सदस्यता लें ताकि YouTube की 

252
00:16:41,352 --> 00:16:44,685
अनुशंसा एल्गोरिदम के अंतर्गत आने वाले तंत्रिका नेटवर्क को यह विश्वास हो 

253
00:16:44,685 --> 00:16:47,880
सके कि आप इस चैनल की सामग्री देखना चाहते हैं जो आपके लिए अनुशंसित है।

254
00:16:48,560 --> 00:16:49,940
वैसे भी, अधिक जानकारी के लिए पोस्ट करते रहें।

255
00:16:50,760 --> 00:16:53,500
पैट्रियन पर इन वीडियो का समर्थन करने वाले सभी लोगों को बहुत-बहुत धन्यवाद।

256
00:16:54,000 --> 00:16:57,638
मैं इस गर्मी में संभाव्यता श्रृंखला में प्रगति करने में थोड़ा धीमा रहा हूं, 

257
00:16:57,638 --> 00:17:01,900
लेकिन इस परियोजना के बाद मैं इसमें वापस कूद रहा हूं, ताकि संरक्षक आप वहां अपडेट देख सकें।

258
00:17:03,600 --> 00:17:05,967
यहां चीजों को बंद करने के लिए मेरे साथ लीशा ली हैं, 

259
00:17:05,967 --> 00:17:08,563
जिन्होंने गहन शिक्षण के सैद्धांतिक पक्ष पर पीएचडी की है, 

260
00:17:08,563 --> 00:17:12,297
और जो वर्तमान में एम्प्लीफाई पार्टनर्स नामक एक उद्यम पूंजी फर्म में काम करती हैं, 

261
00:17:12,297 --> 00:17:14,619
जिन्होंने इस वीडियो के लिए कुछ फंडिंग प्रदान की है।

262
00:17:15,460 --> 00:17:17,309
तो लीशा, मुझे लगता है कि एक चीज़ जो हमें जल्दी 

263
00:17:17,309 --> 00:17:19,119
से सामने लानी चाहिए वह है यह सिग्मॉइड फ़ंक्शन।

264
00:17:19,700 --> 00:17:23,050
जैसा कि मैं इसे समझता हूं, शुरुआती नेटवर्क शून्य और एक के बीच के अंतराल में 

265
00:17:23,050 --> 00:17:25,739
प्रासंगिक भारित राशि को निचोड़ने के लिए इसका उपयोग करते हैं, 

266
00:17:25,739 --> 00:17:29,134
आप जानते हैं कि न्यूरॉन्स के निष्क्रिय या सक्रिय होने के इस जैविक सादृश्य से 

267
00:17:29,134 --> 00:17:29,840
प्रेरित होता है।

268
00:17:30,280 --> 00:17:30,300
बिल्कुल।

269
00:17:30,560 --> 00:17:34,040
लेकिन अपेक्षाकृत कुछ आधुनिक नेटवर्क वास्तव में अब सिग्मॉइड का उपयोग करते हैं।

270
00:17:34,320 --> 00:17:34,320
हाँ।

271
00:17:34,440 --> 00:17:35,540
यह एक तरह का पुराना स्कूल है ना?

272
00:17:35,760 --> 00:17:38,580
हाँ या यूँ कहें कि रेलू को प्रशिक्षित करना बहुत आसान लगता है।

273
00:17:38,580 --> 00:17:42,340
और रेलू, रेलू का मतलब रेक्टिफाइड लीनियर यूनिट है?

274
00:17:42,680 --> 00:17:46,692
हाँ, यह इस प्रकार का फ़ंक्शन है जहाँ आप अधिकतम शून्य ले 

275
00:17:46,692 --> 00:17:50,920
रहे हैं और जहाँ a दिया गया है जो आप वीडियो में समझा रहे थे।

276
00:17:50,920 --> 00:17:56,181
और मुझे लगता है कि यह किस तरह से प्रेरित था, यह आंशिक रूप से एक 

277
00:17:56,181 --> 00:18:01,360
जैविक सादृश्य द्वारा था कि न्यूरॉन्स कैसे सक्रिय होंगे या नहीं।

278
00:18:01,360 --> 00:18:05,438
और इसलिए यदि यह एक निश्चित सीमा पार कर जाता है तो यह पहचान कार्य होगा, 

279
00:18:05,438 --> 00:18:09,460
लेकिन यदि ऐसा नहीं होता है तो यह सक्रिय नहीं होगा इसलिए यह शून्य होगा।

280
00:18:09,460 --> 00:18:10,840
तो यह एक तरह का सरलीकरण है।

281
00:18:11,160 --> 00:18:15,505
सिग्मोइड्स का उपयोग करने से प्रशिक्षण में मदद नहीं मिली या कुछ बिंदु पर 

282
00:18:15,505 --> 00:18:19,912
प्रशिक्षित करना बहुत मुश्किल था और लोगों ने बस रिले की कोशिश की और यह इन 

283
00:18:19,912 --> 00:18:24,620
अविश्वसनीय रूप से गहरे तंत्रिका नेटवर्क के लिए बहुत अच्छी तरह से काम करने लगा।

284
00:18:25,100 --> 00:18:25,640
ठीक है, धन्यवाद लीशा।

