1
00:00:00,000 --> 00:00:01,310
यह 3 है.

2
00:00:01,310 --> 00:00:07,757
इसे 28x28 पिक्सल के बेहद कम रिज़ॉल्यूशन पर लापरवाही से लिखा और प्रस्तुत किया गया है,

3
00:00:07,757 --> 00:00:13,743
लेकिन आपके मस्तिष्क को इसे 3 के रूप में पहचानने में कोई परेशानी नहीं होती है।

4
00:00:13,743 --> 00:00:16,713
और मैं चाहता हूं कि आप एक पल के लिए इसकी सराहना करें कि

5
00:00:16,713 --> 00:00:19,629
यह कितना अजीब है कि दिमाग इतनी आसानी से ऐसा कर सकता है।

6
00:00:19,629 --> 00:00:23,782
मेरा मतलब है, यह, यह और यह भी 3s के रूप में पहचाने जाने योग्य हैं,

7
00:00:23,782 --> 00:00:28,691
भले ही प्रत्येक पिक्सेल का विशिष्ट मान एक छवि से दूसरी छवि में बहुत भिन्न हो।

8
00:00:28,691 --> 00:00:31,455
जब आप इस 3 को देखते हैं तो आपकी आंख में विशेष प्रकाश-

9
00:00:31,455 --> 00:00:35,679
संवेदनशील कोशिकाएं सक्रिय हो जाती हैं, जब आप इस 3 को देखते हैं तो चमकती कोशिकाएं

10
00:00:35,679 --> 00:00:36,722
बहुत भिन्न होती हैं।

11
00:00:36,722 --> 00:00:40,809
लेकिन आपके उस पागल-स्मार्ट विज़ुअल कॉर्टेक्स में कुछ चीज़ इन्हें

12
00:00:40,809 --> 00:00:44,268
एक ही विचार का प्रतिनिधित्व करने के रूप में हल करती है,

13
00:00:44,268 --> 00:00:49,300
जबकि साथ ही अन्य छवियों को अपने स्वयं के विशिष्ट विचारों के रूप में पहचानती है।

14
00:00:49,300 --> 00:00:54,863
लेकिन अगर मैंने आपसे कहा, अरे, बैठो और मेरे लिए एक प्रोग्राम लिखो जो 28x28 की ग्रिड

15
00:00:54,863 --> 00:00:58,506
लेता है और 0 और 10 के बीच एक एकल संख्या आउटपुट करता है,

16
00:00:58,506 --> 00:01:02,082
और आपको बताता है कि वह अंक के बारे में क्या सोचता है,

17
00:01:02,082 --> 00:01:06,122
तो यह कार्य हास्यास्पद रूप से मामूली हो जाता है अत्यंत कठिन.

18
00:01:06,122 --> 00:01:08,904
जब तक आप किसी चट्टान के नीचे नहीं रह रहे हैं, मुझे लगता है कि

19
00:01:08,904 --> 00:01:11,730
मुझे वर्तमान और भविष्य के लिए मशीन लर्निंग और तंत्रिका नेटवर्क

20
00:01:11,730 --> 00:01:14,557
की प्रासंगिकता और महत्व को प्रेरित करने की शायद ही आवश्यकता है।

21
00:01:14,557 --> 00:01:18,003
लेकिन मैं यहां आपको यह दिखाना चाहता हूं कि तंत्रिका नेटवर्क वास्तव में क्या है,

22
00:01:18,003 --> 00:01:21,362
बिना किसी पृष्ठभूमि के, और यह कल्पना करने में मदद करना कि यह क्या कर रहा है,

23
00:01:21,362 --> 00:01:24,241
एक चर्चा शब्द के रूप में नहीं बल्कि गणित के एक टुकड़े के रूप में।

24
00:01:24,241 --> 00:01:27,994
मेरी आशा बस इतनी है कि आप यह महसूस करते हुए आएं कि संरचना स्वयं प्रेरित है,

25
00:01:27,994 --> 00:01:31,346
और जब आप तंत्रिका नेटवर्क उद्धरण-अनउद्धरण सीखने के बारे में पढ़ते

26
00:01:31,346 --> 00:01:34,549
हैं या सुनते हैं तो आपको ऐसा महसूस होता है कि इसका क्या मतलब है।

27
00:01:34,549 --> 00:01:40,300
यह वीडियो केवल इसके संरचना घटक के लिए समर्पित होगा, और अगला वीडियो सीखने से संबंधित होगा।

28
00:01:40,300 --> 00:01:43,119
हम जो करने जा रहे हैं वह एक तंत्रिका नेटवर्क को एक

29
00:01:43,119 --> 00:01:46,104
साथ रखना है जो हस्तलिखित अंकों को पहचानना सीख सकता है।

30
00:01:46,104 --> 00:01:49,496
विषय को प्रस्तुत करने के लिए यह कुछ हद तक उत्कृष्ट उदाहरण है,

31
00:01:49,496 --> 00:01:52,553
और मुझे यहां यथास्थिति पर बने रहने में खुशी हो रही है,

32
00:01:52,553 --> 00:01:57,057
क्योंकि दो वीडियो के अंत में मैं आपको कुछ अच्छे संसाधनों की ओर इंगित करना चाहता

33
00:01:57,057 --> 00:02:01,616
हूं जहां आप और अधिक सीख सकते हैं, और कहां आप ऐसा करने वाले कोड को डाउनलोड कर सकते

34
00:02:01,616 --> 00:02:04,173
हैं और अपने कंप्यूटर पर उसके साथ खेल सकते हैं।

35
00:02:04,173 --> 00:02:08,932
तंत्रिका नेटवर्क के कई प्रकार हैं, और हाल के वर्षों में इन प्रकारों के

36
00:02:08,932 --> 00:02:13,691
प्रति अनुसंधान में तेजी आई है, लेकिन इन दो परिचयात्मक वीडियो में आप और

37
00:02:13,691 --> 00:02:18,920
मैं बिना किसी अतिरिक्त तामझाम के सबसे सरल सादे वेनिला रूप को देखने जा रहे हैं।

38
00:02:18,920 --> 00:02:23,432
किसी भी अधिक शक्तिशाली आधुनिक संस्करण को समझने के लिए यह एक आवश्यक शर्त है,

39
00:02:23,432 --> 00:02:28,485
और मेरा विश्वास करें कि इसमें अभी भी हमारे दिमाग को समझने के लिए काफी जटिलताएं हैं।

40
00:02:28,485 --> 00:02:33,565
लेकिन इस सरलतम रूप में भी यह हस्तलिखित अंकों को पहचानना सीख सकता है,

41
00:02:33,565 --> 00:02:36,628
जो एक कंप्यूटर के लिए बहुत अच्छी बात है।

42
00:02:36,628 --> 00:02:39,913
और साथ ही आप देखेंगे कि कैसे यह कुछ उम्मीदों से

43
00:02:39,913 --> 00:02:42,925
कम हो जाता है जो हमारी इसके लिए हो सकती हैं।

44
00:02:42,925 --> 00:02:46,903
जैसा कि नाम से पता चलता है, तंत्रिका नेटवर्क मस्तिष्क से प्रेरित होते हैं,

45
00:02:46,903 --> 00:02:48,193
लेकिन आइए इसे तोड़ दें।

46
00:02:48,193 --> 00:02:51,833
न्यूरॉन्स क्या हैं, और वे किस अर्थ में एक साथ जुड़े हुए हैं?

47
00:02:51,833 --> 00:02:56,378
अभी जब मैं न्यूरॉन कहता हूं, तो मैं चाहता हूं कि आप एक ऐसी चीज के बारे

48
00:02:56,378 --> 00:03:00,987
में सोचें जो एक संख्या रखती है, विशेष रूप से 0 और 1 के बीच की एक संख्या।

49
00:03:00,987 --> 00:03:03,377
यह वास्तव में उससे अधिक नहीं है.

50
00:03:03,377 --> 00:03:08,318
उदाहरण के लिए, नेटवर्क इनपुट छवि के 28 गुना 28 पिक्सेल में से प्रत्येक के

51
00:03:08,318 --> 00:03:13,660
अनुरूप न्यूरॉन्स के एक समूह के साथ शुरू होता है, जो कुल मिलाकर 784 न्यूरॉन्स है।

52
00:03:13,660 --> 00:03:19,755
इनमें से प्रत्येक में एक संख्या होती है जो संबंधित पिक्सेल के ग्रेस्केल मान को दर्शाती है,

53
00:03:19,755 --> 00:03:24,293
जो काले पिक्सेल के लिए 0 से लेकर सफेद पिक्सेल के लिए 1 तक होती है।

54
00:03:24,293 --> 00:03:28,219
न्यूरॉन के अंदर की इस संख्या को इसकी सक्रियता कहा जाता है,

55
00:03:28,219 --> 00:03:33,769
और आपके मन में यह छवि हो सकती है कि प्रत्येक न्यूरॉन तब प्रकाशित होता है जब इसकी

56
00:03:33,769 --> 00:03:35,935
सक्रियता एक उच्च संख्या होती है।

57
00:03:35,935 --> 00:03:43,399
तो ये सभी 784 न्यूरॉन्स हमारे नेटवर्क की पहली परत बनाते हैं।

58
00:03:43,399 --> 00:03:47,832
अब अंतिम परत पर जाएं, इसमें 10 न्यूरॉन हैं, जिनमें

59
00:03:47,832 --> 00:03:51,570
से प्रत्येक एक अंक का प्रतिनिधित्व करता है।

60
00:03:51,570 --> 00:03:56,198
इन न्यूरॉन्स में सक्रियता, फिर से कुछ संख्या जो 0 और 1 के बीच है,

61
00:03:56,198 --> 00:04:02,180
यह दर्शाती है कि सिस्टम कितना सोचता है कि दी गई छवि किसी दिए गए अंक से मेल खाती है।

62
00:04:02,180 --> 00:04:06,577
बीच में कुछ परतें भी होती हैं जिन्हें छिपी हुई परतें कहा जाता है,

63
00:04:06,577 --> 00:04:12,531
जो फिलहाल एक बड़ा प्रश्नचिह्न होना चाहिए कि पृथ्वी पर अंकों को पहचानने की इस प्रक्रिया

64
00:04:12,531 --> 00:04:13,952
को कैसे संभाला जाएगा।

65
00:04:13,952 --> 00:04:16,766
इस नेटवर्क में मैंने दो छिपी हुई परतों को चुना,

66
00:04:16,766 --> 00:04:20,657
प्रत्येक में 16 न्यूरॉन्स थे, और माना कि यह एक मनमाना विकल्प है।

67
00:04:20,657 --> 00:04:24,661
ईमानदारी से कहूं तो, मैंने इस आधार पर दो परतें चुनीं कि मैं संरचना को एक पल में कैसे

68
00:04:24,661 --> 00:04:28,617
प्रेरित करना चाहता हूं, और 16, खैर यह स्क्रीन पर फिट होने के लिए एक अच्छी संख्या थी।

69
00:04:28,617 --> 00:04:32,782
व्यवहार में यहां एक विशिष्ट संरचना के साथ प्रयोग के लिए बहुत जगह है।

70
00:04:32,782 --> 00:04:35,665
जिस तरह से नेटवर्क संचालित होता है, एक परत में

71
00:04:35,665 --> 00:04:38,610
सक्रियता अगली परत की सक्रियता निर्धारित करती है।

72
00:04:38,610 --> 00:04:43,787
और निश्चित रूप से एक सूचना प्रसंस्करण तंत्र के रूप में नेटवर्क का मूल इस बात

73
00:04:43,787 --> 00:04:48,829
पर निर्भर करता है कि कैसे एक परत से सक्रियता अगली परत में सक्रियता लाती है।

74
00:04:48,829 --> 00:04:52,596
इसका मतलब यह है कि न्यूरॉन्स के जैविक नेटवर्क में

75
00:04:52,596 --> 00:04:57,043
न्यूरॉन्स के कुछ समूह किस तरह से दूसरों को सक्रिय करते हैं।

76
00:04:57,043 --> 00:05:00,292
अब जो नेटवर्क मैं यहां दिखा रहा हूं उसे पहले से ही अंकों को पहचानने के लिए

77
00:05:00,292 --> 00:05:03,455
प्रशिक्षित किया गया है, और मैं आपको दिखाता हूं कि इससे मेरा क्या मतलब है।

78
00:05:03,455 --> 00:05:08,162
इसका मतलब है कि यदि आप छवि में प्रत्येक पिक्सेल की चमक के अनुसार इनपुट परत के

79
00:05:08,162 --> 00:05:11,783
सभी 784 न्यूरॉन्स को रोशन करते हुए एक छवि में फ़ीड करते हैं,

80
00:05:11,783 --> 00:05:16,309
तो सक्रियण का पैटर्न अगली परत में कुछ बहुत विशिष्ट पैटर्न का कारण बनता है,

81
00:05:16,309 --> 00:05:19,387
जो एक के बाद एक में कुछ पैटर्न का कारण बनता है यह,

82
00:05:19,387 --> 00:05:22,103
जो अंततः आउटपुट लेयर में कुछ पैटर्न देता है।

83
00:05:22,103 --> 00:05:26,234
और उस आउटपुट परत का सबसे चमकीला न्यूरॉन नेटवर्क की पसंद है,

84
00:05:26,234 --> 00:05:30,434
इसलिए बोलने के लिए, यह छवि किस अंक का प्रतिनिधित्व करती है।

85
00:05:30,434 --> 00:05:34,636
और गणित में कूदने से पहले कि एक परत अगली परत को कैसे प्रभावित करती है,

86
00:05:34,636 --> 00:05:39,017
या प्रशिक्षण कैसे काम करता है, आइए बस इस बारे में बात करें कि इस तरह की

87
00:05:39,017 --> 00:05:43,458
स्तरित संरचना से बुद्धिमानी से व्यवहार करने की अपेक्षा करना क्यों उचित है।

88
00:05:43,458 --> 00:05:45,046
हम यहाँ क्या उम्मीद कर रहे हैं?

89
00:05:45,046 --> 00:05:49,026
वे मध्य परतें क्या कर रही होंगी, इसके लिए सबसे अच्छी आशा क्या है?

90
00:05:49,026 --> 00:05:53,689
खैर, जब आप या मैं अंकों को पहचानते हैं, तो हम विभिन्न घटकों को एक साथ जोड़ते हैं।

91
00:05:53,689 --> 00:05:56,934
9 में ऊपर एक लूप और दाहिनी ओर एक रेखा होती है।

92
00:05:56,934 --> 00:06:01,827
8 में ऊपर की ओर एक लूप भी होता है, लेकिन इसे नीचे की ओर एक अन्य लूप के साथ जोड़ा जाता है।

93
00:06:01,827 --> 00:06:06,673
A 4 मूलतः तीन विशिष्ट रेखाओं और इसी तरह की चीज़ों में टूट जाता है।

94
00:06:06,673 --> 00:06:10,753
अब एक आदर्श दुनिया में, हम उम्मीद कर सकते हैं कि दूसरी से आखिरी परत

95
00:06:10,753 --> 00:06:14,232
में प्रत्येक न्यूरॉन इन उप-घटकों में से एक से मेल खाता है,

96
00:06:14,232 --> 00:06:18,372
जब भी आप किसी छवि में फ़ीड करते हैं, उदाहरण के लिए, शीर्ष पर एक लूप,

97
00:06:18,372 --> 00:06:23,651
जैसे 9 या 8, तो वहां कुछ होता है विशिष्ट न्यूरॉन जिसकी सक्रियता 1 के करीब होने वाली है।

98
00:06:23,651 --> 00:06:26,572
और मेरा मतलब पिक्सल के इस विशिष्ट लूप से नहीं है,

99
00:06:26,572 --> 00:06:31,700
आशा यह होगी कि शीर्ष की ओर कोई भी आम तौर पर लूपी पैटर्न इस न्यूरॉन को बंद कर देता है।

100
00:06:31,700 --> 00:06:35,724
इस तरह, तीसरी परत से अंतिम परत तक जाने के लिए बस यह सीखने की

101
00:06:35,724 --> 00:06:40,012
आवश्यकता है कि उपघटकों का कौन सा संयोजन किन अंकों से मेल खाता है।

102
00:06:40,012 --> 00:06:43,312
बेशक, इससे समस्या ख़त्म हो जाती है, क्योंकि आप इन उप-

103
00:06:43,312 --> 00:06:47,794
घटकों को कैसे पहचानेंगे, या यह भी सीखेंगे कि सही उप-घटक क्या होने चाहिए?

104
00:06:47,794 --> 00:06:50,366
और मैंने अभी तक इस बारे में बात भी नहीं की है कि एक परत दूसरी परत

105
00:06:50,366 --> 00:06:52,900
को कैसे प्रभावित करती है, लेकिन एक पल के लिए इस पर मेरे साथ चलें।

106
00:06:52,900 --> 00:06:56,598
लूप को पहचानने से उप-समस्याएँ भी विभाजित हो सकती हैं।

107
00:06:56,598 --> 00:06:59,851
ऐसा करने का एक उचित तरीका यह होगा कि पहले इसे

108
00:06:59,851 --> 00:07:03,386
बनाने वाले विभिन्न छोटे किनारों को पहचान लिया जाए।

109
00:07:03,386 --> 00:07:07,895
इसी प्रकार, एक लंबी रेखा जैसी कि आप अंक 1 या 4 या 7 में देख सकते हैं,

110
00:07:07,895 --> 00:07:11,686
वास्तव में यह सिर्फ एक लंबा किनारा है, या शायद आप इसे कई

111
00:07:11,686 --> 00:07:15,281
छोटे किनारों के एक निश्चित पैटर्न के रूप में सोचते हैं।

112
00:07:15,281 --> 00:07:19,591
तो शायद हमारी आशा यह है कि नेटवर्क की दूसरी परत में प्रत्येक

113
00:07:19,591 --> 00:07:23,406
न्यूरॉन विभिन्न प्रासंगिक छोटे किनारों से मेल खाता है।

114
00:07:23,406 --> 00:07:28,844
हो सकता है कि जब इस तरह की कोई छवि आती है, तो यह लगभग 8 से 10 विशिष्ट छोटे किनारों

115
00:07:28,844 --> 00:07:34,217
से जुड़े सभी न्यूरॉन्स को रोशन करती है, जो बदले में ऊपरी लूप और एक लंबी ऊर्ध्वाधर

116
00:07:34,217 --> 00:07:39,720
रेखा से जुड़े न्यूरॉन्स को रोशन करती है, और वे प्रकाश डालते हैं 9 से संबद्ध न्यूरॉन।

117
00:07:39,720 --> 00:07:42,981
हमारा अंतिम नेटवर्क वास्तव में ऐसा करता है या नहीं, यह एक और सवाल है,

118
00:07:42,981 --> 00:07:47,186
जिस पर मैं एक बार फिर विचार करूंगा जब हम देखेंगे कि नेटवर्क को कैसे प्रशिक्षित किया जाए।

119
00:07:47,186 --> 00:07:49,869
लेकिन यह एक आशा है कि हमारे पास इस तरह की स्तरित

120
00:07:49,869 --> 00:07:52,334
संरचना के साथ एक प्रकार का लक्ष्य हो सकता है।

121
00:07:52,334 --> 00:07:56,420
इसके अलावा, आप कल्पना कर सकते हैं कि इस तरह किनारों और पैटर्न का पता लगाने

122
00:07:56,420 --> 00:08:00,397
में सक्षम होना अन्य छवि पहचान कार्यों के लिए वास्तव में कैसे उपयोगी होगा।

123
00:08:00,397 --> 00:08:03,822
और छवि पहचान से परे भी, सभी प्रकार की बुद्धिमान चीजें हैं

124
00:08:03,822 --> 00:08:07,306
जो आप करना चाहते हैं जो अमूर्तता की परतों में टूट जाती हैं।

125
00:08:07,306 --> 00:08:11,224
उदाहरण के लिए, भाषण को पार्स करने में कच्चा ऑडियो लेना और अलग-

126
00:08:11,224 --> 00:08:15,206
अलग ध्वनियों को चुनना शामिल है, जो मिलकर कुछ शब्दांश बनाते हैं,

127
00:08:15,206 --> 00:08:20,262
जो मिलकर शब्द बनाते हैं, जो मिलकर वाक्यांश और अधिक अमूर्त विचार बनाते हैं, आदि।

128
00:08:20,262 --> 00:08:24,481
लेकिन इनमें से कोई भी वास्तव में कैसे काम करता है, इस पर वापस लौटते हुए,

129
00:08:24,481 --> 00:08:29,405
अभी स्वयं कल्पना करें कि एक परत में सक्रियता वास्तव में अगली परत में सक्रियता कैसे

130
00:08:29,405 --> 00:08:30,635
निर्धारित कर सकती है।

131
00:08:30,635 --> 00:08:34,646
लक्ष्य कुछ ऐसा तंत्र बनाना है जो पिक्सेल को किनारों में,

132
00:08:34,646 --> 00:08:38,872
या किनारों को पैटर्न में, या पैटर्न को अंकों में जोड़ सके।

133
00:08:38,872 --> 00:08:42,310
और एक बहुत ही विशिष्ट उदाहरण पर ज़ूम करने के लिए,

134
00:08:42,310 --> 00:08:48,274
मान लें कि दूसरी परत में एक विशेष न्यूरॉन से यह पता लगाने की उम्मीद है कि छवि का इस

135
00:08:48,274 --> 00:08:50,660
क्षेत्र में कोई किनारा है या नहीं।

136
00:08:50,660 --> 00:08:55,197
सवाल यह है कि नेटवर्क में कौन से पैरामीटर होने चाहिए?

137
00:08:55,197 --> 00:08:59,421
आपको किस डायल और नॉब को बदलने में सक्षम होना चाहिए ताकि यह इस पैटर्न,

138
00:08:59,421 --> 00:09:03,768
या किसी अन्य पिक्सेल पैटर्न, या पैटर्न को कैप्चर करने के लिए पर्याप्त

139
00:09:03,768 --> 00:09:08,175
रूप से अभिव्यंजक हो, जिससे कई किनारे एक लूप बना सकें, और ऐसी अन्य चीजें?

140
00:09:08,175 --> 00:09:11,686
खैर, हम जो करेंगे वह हमारे न्यूरॉन और पहली परत के

141
00:09:11,686 --> 00:09:15,759
न्यूरॉन्स के बीच प्रत्येक कनेक्शन को एक भार प्रदान करेंगे।

142
00:09:15,759 --> 00:09:17,666
ये वज़न महज़ संख्याएँ हैं।

143
00:09:17,666 --> 00:09:21,817
फिर उन सभी सक्रियताओं को पहली परत से लें और इन

144
00:09:21,817 --> 00:09:25,704
भारों के अनुसार उनके भारित योग की गणना करें।

145
00:09:25,704 --> 00:09:29,659
मुझे इन वजनों को अपने स्वयं के एक छोटे ग्रिड में व्यवस्थित करने के बारे में

146
00:09:29,659 --> 00:09:33,666
सोचने में मदद मिलती है, और मैं सकारात्मक वजन को इंगित करने के लिए हरे पिक्सल

147
00:09:33,666 --> 00:09:37,726
का उपयोग करने जा रहा हूं, और नकारात्मक वजन को इंगित करने के लिए लाल पिक्सल का

148
00:09:37,726 --> 00:09:41,941
उपयोग करने जा रहा हूं, जहां उस पिक्सेल की चमक कुछ है वजन के मूल्य का ढीला चित्रण.

149
00:09:41,941 --> 00:09:46,889
यदि हमने इस क्षेत्र में कुछ सकारात्मक भारों को छोड़कर, जिनकी हम परवाह करते हैं,

150
00:09:46,889 --> 00:09:50,084
लगभग सभी पिक्सेल से जुड़े भार को शून्य कर दिया है,

151
00:09:50,084 --> 00:09:55,470
तो सभी पिक्सेल मानों का भारित योग लेना वास्तव में केवल पिक्सेल के मानों को जोड़ने के

152
00:09:55,470 --> 00:09:57,976
समान है। वह क्षेत्र जिसकी हमें परवाह है।

153
00:09:57,976 --> 00:10:02,636
और यदि आप वास्तव में यह जानना चाहते हैं कि क्या यहां कोई बढ़त है,

154
00:10:02,636 --> 00:10:07,440
तो आप आसपास के पिक्सेल से जुड़े कुछ नकारात्मक भार को समझ सकते हैं।

155
00:10:07,440 --> 00:10:10,195
तब योग सबसे बड़ा होता है जब वे मध्य पिक्सेल चमकीले

156
00:10:10,195 --> 00:10:12,680
होते हैं लेकिन आसपास के पिक्सेल गहरे होते हैं।

157
00:10:12,680 --> 00:10:18,466
जब आप इस तरह एक भारित राशि की गणना करते हैं, तो आप किसी भी संख्या के साथ आ सकते हैं,

158
00:10:18,466 --> 00:10:23,563
लेकिन इस नेटवर्क के लिए हम चाहते हैं कि सक्रियण 0 और 1 के बीच कुछ मान हो।

159
00:10:23,563 --> 00:10:27,505
तो एक सामान्य बात यह है कि इस भारित योग को किसी फ़ंक्शन में पंप किया

160
00:10:27,505 --> 00:10:31,447
जाए जो वास्तविक संख्या रेखा को 0 और 1 के बीच की सीमा में दबा देता है।

161
00:10:31,447 --> 00:10:34,806
और ऐसा करने वाला एक सामान्य कार्य सिग्मॉइड फ़ंक्शन कहलाता है,

162
00:10:34,806 --> 00:10:37,449
जिसे लॉजिस्टिक वक्र के रूप में भी जाना जाता है।

163
00:10:37,449 --> 00:10:41,554
मूल रूप से बहुत नकारात्मक इनपुट 0 के करीब समाप्त होते हैं,

164
00:10:41,554 --> 00:10:44,952
बहुत सकारात्मक इनपुट 1 के करीब समाप्त होते हैं,

165
00:10:44,952 --> 00:10:48,137
और यह इनपुट 0 के आसपास लगातार बढ़ता जाता है।

166
00:10:48,137 --> 00:10:52,422
तो यहां न्यूरॉन की सक्रियता मूल रूप से इस बात का

167
00:10:52,422 --> 00:10:56,706
माप है कि प्रासंगिक भारित योग कितना सकारात्मक है।

168
00:10:56,706 --> 00:10:59,814
लेकिन शायद ऐसा नहीं है कि आप चाहते हैं कि जब भारित

169
00:10:59,814 --> 00:11:02,313
योग 0 से बड़ा हो तो न्यूरॉन प्रकाशमान हो।

170
00:11:02,313 --> 00:11:06,768
हो सकता है कि आप इसे केवल तभी सक्रिय करना चाहते हों जब योग 10 से बड़ा हो।

171
00:11:06,768 --> 00:11:10,663
यानी, आप इसके निष्क्रिय होने के लिए कुछ पूर्वाग्रह चाहते हैं।

172
00:11:10,663 --> 00:11:15,578
इसके बाद हम इस भारित योग को सिग्मॉइड स्क्विशिफिकेशन फ़ंक्शन के माध्यम

173
00:11:15,578 --> 00:11:20,634
से प्लग करने से पहले इसमें कोई अन्य संख्या, जैसे ऋणात्मक 10, जोड़ देंगे।

174
00:11:20,634 --> 00:11:23,382
उस अतिरिक्त संख्या को पूर्वाग्रह कहा जाता है।

175
00:11:23,382 --> 00:11:28,443
तो वज़न आपको बताता है कि दूसरी परत में यह न्यूरॉन किस पिक्सेल पैटर्न को उठा रहा है,

176
00:11:28,443 --> 00:11:32,345
और पूर्वाग्रह आपको बताता है कि न्यूरॉन के सार्थक रूप से सक्रिय

177
00:11:32,345 --> 00:11:35,089
होने से पहले भारित योग कितना अधिक होना चाहिए।

178
00:11:35,089 --> 00:11:37,188
और वह सिर्फ एक न्यूरॉन है.

179
00:11:37,188 --> 00:11:44,761
इस परत का हर दूसरा न्यूरॉन पहली परत के सभी 784 पिक्सेल न्यूरॉन्स से जुड़ा होगा,

180
00:11:44,761 --> 00:11:50,705
और उन 784 कनेक्शनों में से प्रत्येक का अपना वजन जुड़ा हुआ है।

181
00:11:50,705 --> 00:11:53,318
इसके अलावा, प्रत्येक में कुछ पूर्वाग्रह होते हैं,

182
00:11:53,318 --> 00:11:57,960
कुछ अन्य संख्याएं जिन्हें आप सिग्मॉइड के साथ कुचलने से पहले भारित राशि में जोड़ते हैं।

183
00:11:57,960 --> 00:11:59,684
और यह बहुत सोचने वाली बात है!

184
00:11:59,684 --> 00:12:07,848
16 न्यूरॉन्स की इस छिपी हुई परत के साथ, 16 पूर्वाग्रहों के साथ, यह कुल 784 गुना 16 भार है।

185
00:12:07,848 --> 00:12:12,104
और यह सब पहली परत से दूसरी परत तक का कनेक्शन मात्र है।

186
00:12:12,104 --> 00:12:18,059
अन्य परतों के बीच के संबंधों में बहुत सारे वजन और पूर्वाग्रह भी जुड़े हुए हैं।

187
00:12:18,059 --> 00:12:24,015
सब कुछ कहा और किया गया, इस नेटवर्क में लगभग 13,000 कुल भार और पूर्वाग्रह हैं।

188
00:12:24,015 --> 00:12:27,038
13,000 नॉब और डायल जिन्हें इस नेटवर्क को अलग-अलग

189
00:12:27,038 --> 00:12:30,493
तरीकों से संचालित करने के लिए बदला और घुमाया जा सकता है।

190
00:12:30,493 --> 00:12:35,971
इसलिए जब हम सीखने के बारे में बात करते हैं, तो इसका मतलब कंप्यूटर को इन सभी

191
00:12:35,971 --> 00:12:41,953
संख्याओं के लिए एक वैध सेटिंग ढूंढना है ताकि वह वास्तव में समस्या का समाधान कर सके।

192
00:12:41,953 --> 00:12:46,738
एक विचार प्रयोग जो मजेदार भी है और भयावह भी, वह है बैठकर कल्पना करना और

193
00:12:46,738 --> 00:12:49,929
इन सभी वजनों और पूर्वाग्रहों को हाथ से सेट करना,

194
00:12:49,929 --> 00:12:54,049
जानबूझकर संख्याओं को बदलना ताकि दूसरी परत किनारों को पकड़ ले,

195
00:12:54,049 --> 00:12:56,509
तीसरी परत पैटर्न को पहचान ले, वगैरह।

196
00:12:56,509 --> 00:12:59,987
मैं व्यक्तिगत रूप से नेटवर्क को संपूर्ण ब्लैक बॉक्स के रूप में मानने के

197
00:12:59,987 --> 00:13:03,369
बजाय इसे संतोषजनक मानता हूं, क्योंकि जब नेटवर्क आपके अनुमान के अनुसार

198
00:13:03,369 --> 00:13:07,573
प्रदर्शन नहीं करता है, तो यदि आपने उन भारों और पूर्वाग्रहों का वास्तव में क्या मतलब है,

199
00:13:07,573 --> 00:13:11,100
इसके साथ थोड़ा सा संबंध बना लिया है , आपके पास यह प्रयोग करने के लिए एक

200
00:13:11,100 --> 00:13:13,950
प्रारंभिक स्थान है कि सुधार के लिए संरचना को कैसे बदला जाए।

201
00:13:13,950 --> 00:13:18,067
या जब नेटवर्क काम करता है, लेकिन उन कारणों के लिए नहीं जिनकी आप उम्मीद कर सकते हैं,

202
00:13:18,067 --> 00:13:22,134
तो वज़न और पूर्वाग्रह क्या कर रहे हैं, इसकी खोज करना आपकी धारणाओं को चुनौती देने

203
00:13:22,134 --> 00:13:26,201
और वास्तव में संभावित समाधानों की पूरी गुंजाइश को उजागर करने का एक अच्छा तरीका है।

204
00:13:26,201 --> 00:13:32,177
वैसे, यहाँ वास्तविक कार्य को लिखना थोड़ा बोझिल है, क्या आपको नहीं लगता?

205
00:13:32,177 --> 00:13:34,568
तो आइए मैं आपको एक अधिक सांकेतिक रूप से संक्षिप्त तरीका

206
00:13:34,568 --> 00:13:37,087
दिखाता हूं जिससे इन कनेक्शनों का प्रतिनिधित्व किया जाता है।

207
00:13:37,087 --> 00:13:41,120
यदि आप तंत्रिका नेटवर्क के बारे में अधिक पढ़ना चुनते हैं तो आप इसे इसी तरह देखेंगे।

208
00:13:41,120 --> 00:13:46,140
सभी सक्रियणों को एक परत से एक वेक्टर के रूप में एक कॉलम में व्यवस्थित करें।

209
00:13:46,140 --> 00:13:50,217
फिर सभी भारों को एक मैट्रिक्स के रूप में व्यवस्थित करें,

210
00:13:50,217 --> 00:13:56,114
जहां उस मैट्रिक्स की प्रत्येक पंक्ति एक परत और अगली परत में एक विशेष न्यूरॉन के

211
00:13:56,114 --> 00:13:58,080
बीच कनेक्शन से मेल खाती है।

212
00:13:58,080 --> 00:14:03,960
इसका मतलब यह है कि इन भारों के अनुसार पहली परत में सक्रियणों का भारित योग लेना हमारे

213
00:14:03,960 --> 00:14:09,702
यहां बाईं ओर मौजूद हर चीज के मैट्रिक्स वेक्टर उत्पाद में से एक शब्द से मेल खाता है।

214
00:14:09,702 --> 00:14:15,612
वैसे, मशीन सीखने का इतना सारा काम सिर्फ रैखिक बीजगणित की अच्छी समझ के कारण होता है,

215
00:14:15,612 --> 00:14:20,311
इसलिए आप में से जो लोग मैट्रिक्स के लिए एक अच्छी दृश्य समझ चाहते

216
00:14:20,311 --> 00:14:24,868
हैं और मैट्रिक्स वेक्टर गुणन का क्या मतलब है, मेरे द्वारा की गई

217
00:14:24,868 --> 00:14:28,997
श्रृंखला पर एक नज़र डालें रैखिक बीजगणित, विशेषकर अध्याय 3।

218
00:14:28,997 --> 00:14:33,489
अपनी अभिव्यक्ति पर वापस जाएं, इन मूल्यों में से प्रत्येक में पूर्वाग्रह को स्वतंत्र रूप

219
00:14:33,489 --> 00:14:38,083
से जोड़ने के बारे में बात करने के बजाय, हम उन सभी पूर्वाग्रहों को एक वेक्टर में व्यवस्थित

220
00:14:38,083 --> 00:14:42,524
करके और पूरे वेक्टर को पिछले मैट्रिक्स वेक्टर उत्पाद में जोड़कर इसका प्रतिनिधित्व करते

221
00:14:42,524 --> 00:14:42,729
हैं।

222
00:14:42,729 --> 00:14:47,153
फिर अंतिम चरण के रूप में, मैं यहां बाहर के चारों ओर एक सिग्मॉइड लपेटूंगा,

223
00:14:47,153 --> 00:14:51,275
और जो प्रतिनिधित्व करने वाला है वह यह है कि आप अंदर परिणामी वेक्टर

224
00:14:51,275 --> 00:14:55,215
के प्रत्येक विशिष्ट घटक पर सिग्मॉइड फ़ंक्शन लागू करने जा रहे हैं।

225
00:14:55,215 --> 00:15:00,466
इसलिए एक बार जब आप इस वेट मैट्रिक्स और इन वैक्टरों को अपने स्वयं के प्रतीकों के रूप में

226
00:15:00,466 --> 00:15:05,718
लिख लेते हैं, तो आप एक परत से दूसरी परत तक सक्रियताओं के पूर्ण संक्रमण को बेहद चुस्त और

227
00:15:05,718 --> 00:15:08,821
साफ-सुथरी छोटी अभिव्यक्ति में संप्रेषित कर सकते हैं,

228
00:15:08,821 --> 00:15:12,581
और यह प्रासंगिक कोड को बहुत सरल और सरल बना देता है। बहुत तेज़,

229
00:15:12,581 --> 00:15:16,042
क्योंकि कई लाइब्रेरी मैट्रिक्स गुणन को अनुकूलित करती हैं।

230
00:15:16,042 --> 00:15:19,210
याद रखें कि मैंने पहले कैसे कहा था कि ये न्यूरॉन

231
00:15:19,210 --> 00:15:22,120
केवल ऐसी चीज़ें हैं जिनमें संख्याएँ होती हैं?

232
00:15:22,120 --> 00:15:27,252
बेशक, उनके पास मौजूद विशिष्ट संख्याएं आपके द्वारा फीड की गई छवि पर निर्भर करती हैं,

233
00:15:27,252 --> 00:15:32,137
इसलिए वास्तव में प्रत्येक न्यूरॉन को एक फ़ंक्शन के रूप में सोचना अधिक सटीक है,

234
00:15:32,137 --> 00:15:36,775
जो पिछली परत के सभी न्यूरॉन्स के आउटपुट लेता है, और एक को बाहर निकालता है।

235
00:15:36,775 --> 00:15:38,321
0 और 1 के बीच की संख्या.

236
00:15:38,321 --> 00:15:42,708
वास्तव में पूरा नेटवर्क सिर्फ एक फ़ंक्शन है, जो इनपुट के रूप

237
00:15:42,708 --> 00:15:47,096
में 784 नंबर लेता है और आउटपुट के रूप में 10 नंबर निकालता है।

238
00:15:47,096 --> 00:15:51,654
यह एक बेतुका जटिल फ़ंक्शन है, जिसमें इन वज़न और पूर्वाग्रहों के रूप में 13,

239
00:15:51,654 --> 00:15:54,753
000 पैरामीटर शामिल हैं जो कुछ पैटर्न पर आधारित हैं,

240
00:15:54,753 --> 00:16:00,162
और जिसमें कई मैट्रिक्स वेक्टर उत्पादों और सिग्मॉइड स्क्विशिफिकेशन फ़ंक्शन को पुनरावृत्त

241
00:16:00,162 --> 00:16:03,201
करना शामिल है, लेकिन फिर भी यह केवल एक फ़ंक्शन है।

242
00:16:03,201 --> 00:16:06,817
और एक तरह से यह आश्वस्त करने वाला है कि यह जटिल दिखता है।

243
00:16:06,817 --> 00:16:09,839
मेरा मतलब है कि यदि यह और भी सरल होता, तो हमें क्या आशा

244
00:16:09,839 --> 00:16:12,807
होती कि यह अंकों को पहचानने की चुनौती का सामना कर सकता?

245
00:16:12,807 --> 00:16:14,920
और यह उस चुनौती को कैसे स्वीकार करता है?

246
00:16:14,920 --> 00:16:19,880
यह नेटवर्क केवल डेटा को देखकर उचित भार और पूर्वाग्रह कैसे सीखता है?

247
00:16:19,880 --> 00:16:22,954
खैर, यही मैं अगले वीडियो में दिखाऊंगा, और मैं यह भी

248
00:16:22,954 --> 00:16:26,147
जानूंगा कि यह विशेष नेटवर्क वास्तव में क्या कर रहा है।

249
00:16:26,147 --> 00:16:29,864
अब मुद्दा यह है कि मुझे लगता है कि मुझे यह कहना चाहिए कि जब वह वीडियो या कोई नया

250
00:16:29,864 --> 00:16:32,664
वीडियो आता है तो उसके बारे में सूचित रहने के लिए सदस्यता लें,

251
00:16:32,664 --> 00:16:36,610
लेकिन वास्तविकता यह है कि आप में से अधिकांश को वास्तव में YouTube से सूचनाएं प्राप्त

252
00:16:36,610 --> 00:16:37,574
नहीं होती हैं, है ना?

253
00:16:37,574 --> 00:16:41,192
शायद अधिक ईमानदारी से मुझे यह कहना चाहिए कि सदस्यता लें ताकि YouTube की

254
00:16:41,192 --> 00:16:44,810
अनुशंसा एल्गोरिदम के अंतर्गत आने वाले तंत्रिका नेटवर्क को यह विश्वास हो

255
00:16:44,810 --> 00:16:48,278
सके कि आप इस चैनल की सामग्री देखना चाहते हैं जो आपके लिए अनुशंसित है।

256
00:16:48,278 --> 00:16:50,600
वैसे भी, अधिक जानकारी के लिए पोस्ट करते रहें।

257
00:16:50,600 --> 00:16:53,609
पैट्रियन पर इन वीडियो का समर्थन करने वाले सभी लोगों को बहुत-बहुत धन्यवाद।

258
00:16:53,609 --> 00:16:57,572
मैं इस गर्मी में संभाव्यता श्रृंखला में प्रगति करने में थोड़ा धीमा रहा हूं,

259
00:16:57,572 --> 00:17:02,329
लेकिन इस परियोजना के बाद मैं इसमें वापस कूद रहा हूं, ताकि संरक्षक आप वहां अपडेट देख सकें।

260
00:17:02,329 --> 00:17:05,033
यहां चीजों को बंद करने के लिए मेरे साथ लीशा ली हैं,

261
00:17:05,033 --> 00:17:08,055
जिन्होंने गहन शिक्षण के सैद्धांतिक पक्ष पर पीएचडी की है,

262
00:17:08,055 --> 00:17:12,403
और जो वर्तमान में एम्प्लीफाई पार्टनर्स नामक एक उद्यम पूंजी फर्म में काम करती हैं,

263
00:17:12,403 --> 00:17:15,160
जिन्होंने इस वीडियो के लिए कुछ फंडिंग प्रदान की है।

264
00:17:15,160 --> 00:17:17,343
तो लीशा, मुझे लगता है कि एक चीज़ जो हमें जल्दी

265
00:17:17,343 --> 00:17:19,480
से सामने लानी चाहिए वह है यह सिग्मॉइड फ़ंक्शन।

266
00:17:19,480 --> 00:17:22,954
जैसा कि मैं इसे समझता हूं, शुरुआती नेटवर्क शून्य और एक के बीच के अंतराल में

267
00:17:22,954 --> 00:17:25,698
प्रासंगिक भारित राशि को निचोड़ने के लिए इसका उपयोग करते हैं,

268
00:17:25,698 --> 00:17:29,127
आप जानते हैं कि न्यूरॉन्स के निष्क्रिय या सक्रिय होने के इस जैविक सादृश्य

269
00:17:29,127 --> 00:17:29,995
से प्रेरित होता है।

270
00:17:29,995 --> 00:17:30,459
बिल्कुल।

271
00:17:30,459 --> 00:17:34,026
लेकिन अपेक्षाकृत कुछ आधुनिक नेटवर्क वास्तव में अब सिग्मॉइड का उपयोग करते हैं।

272
00:17:34,026 --> 00:17:34,288
हाँ।

273
00:17:34,288 --> 00:17:35,860
यह एक तरह का पुराना स्कूल है ना?

274
00:17:35,860 --> 00:17:38,872
हाँ या यूँ कहें कि रेलू को प्रशिक्षित करना बहुत आसान लगता है।

275
00:17:38,872 --> 00:17:42,306
और रेलू, रेलू का मतलब रेक्टिफाइड लीनियर यूनिट है?

276
00:17:42,306 --> 00:17:46,576
हाँ, यह इस प्रकार का फ़ंक्शन है जहाँ आप अधिकतम शून्य ले

277
00:17:46,576 --> 00:17:51,074
रहे हैं और जहाँ a दिया गया है जो आप वीडियो में समझा रहे थे।

278
00:17:51,074 --> 00:17:55,910
और मुझे लगता है कि यह किस तरह से प्रेरित था, यह आंशिक रूप से एक

279
00:17:55,910 --> 00:18:00,670
जैविक सादृश्य द्वारा था कि न्यूरॉन्स कैसे सक्रिय होंगे या नहीं।

280
00:18:00,670 --> 00:18:04,948
और इसलिए यदि यह एक निश्चित सीमा पार कर जाता है तो यह पहचान कार्य होगा,

281
00:18:04,948 --> 00:18:09,287
लेकिन यदि ऐसा नहीं होता है तो यह सक्रिय नहीं होगा इसलिए यह शून्य होगा।

282
00:18:09,287 --> 00:18:10,997
तो यह एक तरह का सरलीकरण है।

283
00:18:10,997 --> 00:18:15,212
सिग्मोइड्स का उपयोग करने से प्रशिक्षण में मदद नहीं मिली या कुछ बिंदु पर

284
00:18:15,212 --> 00:18:19,485
प्रशिक्षित करना बहुत मुश्किल था और लोगों ने बस रिले की कोशिश की और यह इन

285
00:18:19,485 --> 00:18:24,052
अविश्वसनीय रूप से गहरे तंत्रिका नेटवर्क के लिए बहुत अच्छी तरह से काम करने लगा।

286
00:18:24,052 --> 00:18:39,080
ठीक है, धन्यवाद लीशा।

