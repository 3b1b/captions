[
 {
  "input": "This is a 3.",
  "translatedText": "이것은 3입니다.",
  "model": "DeepL",
  "time_range": [
   4.22,
   5.4
  ]
 },
 {
  "input": "It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3.",
  "translatedText": "28x28 픽셀의 매우 낮은 해상도로 조잡하게 작성되고 렌더링되었지만 뇌는 이를 3으로 인식하는 데 아무런 문제가 없습니다.",
  "model": "DeepL",
  "time_range": [
   6.06,
   13.72
  ]
 },
 {
  "input": "And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly.",
  "translatedText": "그리고 두뇌가 이렇게 쉽게 이 일을 해낼 수 있다는 것이 얼마나 놀라운 일인지 잠시 생각해 보셨으면 합니다.",
  "model": "DeepL",
  "time_range": [
   14.34,
   18.96
  ]
 },
 {
  "input": "I mean, this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next.",
  "translatedText": "각 픽셀의 특정 값은 이미지마다 매우 다르지만 이것, 이것, 이것도 3초로 인식할 수 있습니다.",
  "model": "DeepL",
  "time_range": [
   19.7,
   28.32
  ]
 },
 {
  "input": "The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3.",
  "translatedText": "이 3을 볼 때 발화하는 눈의 특정 빛에 민감한 세포는 이 3을 볼 때 발화하는 세포와 매우 다릅니다.",
  "model": "DeepL",
  "time_range": [
   28.9,
   36.94
  ]
 },
 {
  "input": "But something in that crazy-smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas.",
  "translatedText": "하지만 당신의 미친 듯이 똑똑한 시각 피질은 이 이미지들을 동일한 아이디어를 나타내는 것으로 인식하는 동시에 다른 이미지들을 고유한 아이디어로 인식합니다.",
  "model": "DeepL",
  "time_range": [
   37.52,
   48.26
  ]
 },
 {
  "input": "But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this and outputs a single number between 0 and 10, telling you what it thinks the digit is, well the task goes from comically trivial to dauntingly difficult.",
  "translatedText": "하지만 제가 여러분에게 앉아서 이렇게 28x28 픽셀의 격자를 받아 0에서 10 사이의 숫자를 출력하고 그 숫자가 무엇이라고 생각하는지 알려주는 프로그램을 작성하라고 하면, 이 작업은 코믹하게 사소한 것에서 엄청나게 어려운 것으로 바뀝니다.",
  "model": "DeepL",
  "time_range": [
   49.22,
   66.18
  ]
 },
 {
  "input": "Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future.",
  "translatedText": "기계 학습과 신경망의 현재와 미래에 대한 관련성과 중요성은 바위 밑에서 살아온 사람이 아니라면 굳이 설명할 필요가 없을 것 같습니다.",
  "model": "DeepL",
  "time_range": [
   67.16,
   74.64
  ]
 },
 {
  "input": "But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing, not as a buzzword but as a piece of math.",
  "translatedText": "하지만 여기서 제가 하고 싶은 것은 배경 지식이 없다고 가정하고 신경망이 실제로 무엇인지, 그리고 신경망이 하는 일을 유행어가 아닌 수학의 한 부분으로 시각화하는 데 도움을 주고자 하는 것입니다.",
  "model": "DeepL",
  "time_range": [
   75.12,
   84.46
  ]
 },
 {
  "input": "My hope is that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote-unquote learning.",
  "translatedText": "제 바람은 여러분이 이 구조 자체에 동기를 부여받고, 신경망 인용-비인용 학습에 대해 읽거나 들었을 때 그 의미를 알 것 같다는 느낌을 받는 것입니다.",
  "model": "DeepL",
  "time_range": [
   85.02,
   94.34
  ]
 },
 {
  "input": "This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning.",
  "translatedText": "이 동영상에서는 구조 구성 요소에 대해서만 다루고 다음 동영상에서는 학습에 대해 다룰 예정입니다.",
  "model": "DeepL",
  "time_range": [
   95.36,
   100.26
  ]
 },
 {
  "input": "What we're going to do is put together a neural network that can learn to recognize handwritten digits.",
  "translatedText": "우리가 할 일은 손으로 쓴 숫자를 인식하는 방법을 학습할 수 있는 신경망을 구성하는 것입니다.",
  "model": "DeepL",
  "time_range": [
   100.96,
   106.04
  ]
 },
 {
  "input": "This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quo here, because at the end of the two videos I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer.",
  "translatedText": "이것은 주제를 소개하는 데 있어 다소 고전적인 예시이며, 두 동영상의 마지막에 더 많은 것을 배울 수 있는 몇 가지 좋은 리소스와 이를 수행하는 코드를 다운로드하여 자신의 컴퓨터에서 재생할 수 있는 곳을 소개하고자 하므로 여기서는 현재 상황을 그대로 유지하겠습니다.",
  "model": "DeepL",
  "time_range": [
   109.36,
   123.08
  ]
 },
 {
  "input": "There are many many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos you and I are just going to look at the simplest plain vanilla form with no added frills.",
  "translatedText": "신경망에는 다양한 변형이 있으며, 최근 몇 년 동안 이러한 변형에 대한 연구가 붐을 이루고 있지만, 이 두 개의 소개 동영상에서는 가장 단순한 플레인 바닐라 형태에 대해서만 살펴볼 것입니다.",
  "model": "DeepL",
  "time_range": [
   125.04,
   139.18
  ]
 },
 {
  "input": "This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds around.",
  "translatedText": "이것은 더 강력한 최신 변형을 이해하기 위한 필수 전제 조건이며, 여전히 우리가 이해하기에는 복잡한 부분이 많이 있습니다.",
  "model": "DeepL",
  "time_range": [
   139.86,
   148.6
  ]
 },
 {
  "input": "But even in this simplest form it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do.",
  "translatedText": "하지만 이렇게 간단한 형태라도 컴퓨터가 손으로 쓴 숫자를 인식하는 법을 배울 수 있다는 것은 컴퓨터가 할 수 있는 꽤 멋진 일입니다.",
  "model": "DeepL",
  "time_range": [
   149.12,
   156.52
  ]
 },
 {
  "input": "And at the same time you'll see how it does fall short of a couple hopes that we might have for it.",
  "translatedText": "동시에 우리가 기대했던 몇 가지 기대에 얼마나 못 미치는지 알게 될 것입니다.",
  "model": "DeepL",
  "time_range": [
   157.48,
   162.28
  ]
 },
 {
  "input": "As the name suggests neural networks are inspired by the brain, but let's break that down.",
  "translatedText": "이름에서 알 수 있듯이 신경망은 뇌에서 영감을 얻었지만, 이를 좀 더 세분화해 보겠습니다.",
  "model": "DeepL",
  "time_range": [
   163.38,
   168.5
  ]
 },
 {
  "input": "What are the neurons, and in what sense are they linked together?",
  "translatedText": "뉴런이란 무엇이며, 어떤 의미에서 서로 연결되어 있을까요?",
  "model": "DeepL",
  "time_range": [
   168.52,
   171.66
  ]
 },
 {
  "input": "Right now when I say neuron all I want you to think about is a thing that holds a number, specifically a number between 0 and 1.",
  "translatedText": "지금 제가 뉴런이라고 할 때 여러분이 생각했으면 하는 것은 숫자, 특히 0과 1 사이의 숫자를 담고 있는 것뿐입니다.",
  "model": "DeepL",
  "time_range": [
   172.5,
   180.44
  ]
 },
 {
  "input": "It's really not more than that.",
  "translatedText": "그 이상도 이하도 아닙니다.",
  "model": "DeepL",
  "time_range": [
   180.68,
   182.56
  ]
 },
 {
  "input": "For example the network starts with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784 neurons in total.",
  "translatedText": "예를 들어 네트워크는 입력 이미지의 28x28 픽셀 각각에 해당하는 뉴런 무리로 시작하며, 총 784개의 뉴런으로 구성됩니다.",
  "model": "DeepL",
  "time_range": [
   183.78,
   194.22
  ]
 },
 {
  "input": "Each one of these holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels.",
  "translatedText": "각각의 숫자는 해당 픽셀의 그레이스케일 값을 나타내는 숫자로, 검은색 픽셀의 경우 0에서 흰색 픽셀의 경우 1까지 다양합니다.",
  "model": "DeepL",
  "time_range": [
   194.7,
   204.38
  ]
 },
 {
  "input": "This number inside the neuron is called its activation, and the image you might have in mind here is that each neuron is lit up when its activation is a high number.",
  "translatedText": "뉴런 내부의 이 숫자를 활성화라고 하며, 활성화가 높을 때 각 뉴런에 불이 켜지는 이미지를 떠올릴 수 있습니다.",
  "model": "DeepL",
  "time_range": [
   205.3,
   214.16
  ]
 },
 {
  "input": "So all of these 784 neurons make up the first layer of our network.",
  "translatedText": "따라서 이 784개의 뉴런이 모두 네트워크의 첫 번째 계층을 구성합니다.",
  "model": "DeepL",
  "time_range": [
   216.72,
   221.86
  ]
 },
 {
  "input": "Now jumping over to the last layer, this has 10 neurons, each representing one of the digits.",
  "translatedText": "이제 마지막 레이어로 넘어가서, 여기에는 각각 숫자 하나를 나타내는 10개의 뉴런이 있습니다.",
  "model": "DeepL",
  "time_range": [
   226.5,
   231.36
  ]
 },
 {
  "input": "The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit.",
  "translatedText": "이러한 뉴런의 활성화는 다시 0과 1 사이의 숫자로, 시스템이 주어진 이미지가 주어진 숫자와 얼마나 일치한다고 생각하는지를 나타냅니다.",
  "model": "DeepL",
  "time_range": [
   232.04,
   242.12
  ]
 },
 {
  "input": "There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark for how on earth this process of recognizing digits is going to be handled.",
  "translatedText": "또한 그 사이에는 숨겨진 레이어라고 불리는 두 개의 레이어가 있는데, 당분간은 도대체 숫자를 인식하는 이 프로세스가 어떻게 처리될 것인지에 대한 거대한 물음표가 될 것입니다.",
  "model": "DeepL",
  "time_range": [
   243.04,
   253.6
  ]
 },
 {
  "input": "In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice.",
  "translatedText": "이 네트워크에서는 각각 16개의 뉴런이 있는 두 개의 숨겨진 레이어를 선택했는데, 이는 다소 자의적인 선택이라는 점을 인정합니다.",
  "model": "DeepL",
  "time_range": [
   254.26,
   260.56
  ]
 },
 {
  "input": "To be honest I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that was just a nice number to fit on the screen.",
  "translatedText": "솔직히 말해서 저는 순간적으로 구조를 어떻게 동기 부여하고 싶은지에 따라 두 개의 레이어를 선택했고, 16은 화면에 맞추기 좋은 숫자였습니다.",
  "model": "DeepL",
  "time_range": [
   261.02,
   268.2
  ]
 },
 {
  "input": "In practice there is a lot of room for experiment with a specific structure here.",
  "translatedText": "실제로 여기에는 특정 구조로 실험할 여지가 많습니다.",
  "model": "DeepL",
  "time_range": [
   268.78,
   272.34
  ]
 },
 {
  "input": "The way the network operates, activations in one layer determine the activations of the next layer.",
  "translatedText": "네트워크가 작동하는 방식에 따라 한 계층의 활성화가 다음 계층의 활성화를 결정합니다.",
  "model": "DeepL",
  "time_range": [
   273.02,
   278.48
  ]
 },
 {
  "input": "And of course the heart of the network as an information processing mechanism comes down to exactly how those activations from one layer bring about activations in the next layer.",
  "translatedText": "물론 정보 처리 메커니즘으로서 네트워크의 핵심은 한 계층의 활성화가 다음 계층의 활성화를 가져오는 방식에 달려 있습니다.",
  "model": "DeepL",
  "time_range": [
   279.2,
   288.58
  ]
 },
 {
  "input": "It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire.",
  "translatedText": "이는 생물학적 뉴런 네트워크에서 일부 뉴런 그룹이 발화하면 다른 뉴런도 발화하는 것과 느슨하게 비유할 수 있습니다.",
  "model": "DeepL",
  "time_range": [
   289.14,
   297.18
  ]
 },
 {
  "input": "Now the network I'm showing here has already been trained to recognize digits, and let me show you what I mean by that.",
  "translatedText": "여기서 보여드리는 네트워크는 이미 숫자를 인식하도록 훈련된 네트워크입니다.",
  "model": "DeepL",
  "time_range": [
   298.12,
   303.4
  ]
 },
 {
  "input": "It means if you feed in an image, lighting up all 784 neurons of the input layer according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer which causes some pattern in the one after it, which finally gives some pattern in the output layer.",
  "translatedText": "즉, 이미지를 입력하면 이미지의 각 픽셀 밝기에 따라 입력 레이어의 784개 뉴런을 모두 비추면 그 활성화 패턴이 다음 레이어에서 어떤 특정한 패턴을 일으키고, 그 다음 레이어에서 어떤 패턴을 일으키고, 최종적으로 출력 레이어에서 어떤 패턴이 만들어집니다.",
  "model": "DeepL",
  "time_range": [
   303.64,
   322.08
  ]
 },
 {
  "input": "And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents.",
  "translatedText": "그리고 해당 출력 레이어에서 가장 밝은 뉴런은 말하자면 이 이미지가 나타내는 숫자에 대한 네트워크의 선택입니다.",
  "model": "DeepL",
  "time_range": [
   322.56,
   329.4
  ]
 },
 {
  "input": "And before jumping into the math for how one layer influences the next, or how training works, let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently.",
  "translatedText": "한 레이어가 다음 레이어에 어떤 영향을 미치는지 또는 트레이닝이 어떻게 작동하는지에 대한 계산에 들어가기 전에, 이와 같은 레이어 구조가 지능적으로 작동할 것으로 기대하는 것이 왜 합리적인지에 대해 먼저 이야기해 보겠습니다.",
  "model": "DeepL",
  "time_range": [
   332.56,
   343.52
  ]
 },
 {
  "input": "What are we expecting here?",
  "translatedText": "여기서 무엇을 기대할 수 있을까요?",
  "model": "DeepL",
  "time_range": [
   344.06,
   345.22
  ]
 },
 {
  "input": "What is the best hope for those middle layers?",
  "translatedText": "이러한 중간 계층을 위한 최선의 희망은 무엇일까요?",
  "model": "DeepL",
  "time_range": [
   345.4,
   347.6
  ]
 },
 {
  "input": "Well, when you or I recognize digits, we piece together various components.",
  "translatedText": "여러분이나 제가 숫자를 인식할 때 우리는 다양한 구성 요소를 조합합니다.",
  "model": "DeepL",
  "time_range": [
   348.92,
   353.52
  ]
 },
 {
  "input": "A 9 has a loop up top and a line on the right.",
  "translatedText": "9는 위쪽에 고리가 있고 오른쪽에 선이 있습니다.",
  "model": "DeepL",
  "time_range": [
   354.2,
   356.82
  ]
 },
 {
  "input": "An 8 also has a loop up top, but it's paired with another loop down low.",
  "translatedText": "8도 위쪽에는 루프가 있지만 아래쪽에는 다른 루프와 짝을 이룹니다.",
  "model": "DeepL",
  "time_range": [
   357.38,
   361.18
  ]
 },
 {
  "input": "A 4 basically breaks down into three specific lines, and things like that.",
  "translatedText": "4는 기본적으로 세 개의 특정 라인으로 나뉘며, 그런 식으로 세분화됩니다.",
  "model": "DeepL",
  "time_range": [
   361.98,
   366.82
  ]
 },
 {
  "input": "Now in a perfect world, we might hope that each neuron in the second to last layer corresponds with one of these subcomponents, that anytime you feed in an image with, say, a loop up top, like a 9 or an 8, there's some specific neuron whose activation is going to be close to 1.",
  "translatedText": "완벽한 세상이라면 두 번째부터 마지막 레이어의 각 뉴런이 이러한 하위 구성 요소 중 하나에 대응하여, 예를 들어 9 또는 8과 같은 루프가 위에 있는 이미지를 입력할 때마다 활성화가 1에 가까운 특정 뉴런이 있기를 바랄 수 있습니다.",
  "model": "DeepL",
  "time_range": [
   367.6,
   383.78
  ]
 },
 {
  "input": "And I don't mean this specific loop of pixels, the hope would be that any generally loopy pattern towards the top sets off this neuron.",
  "translatedText": "이 특정 픽셀 루프를 말하는 것이 아니라, 일반적으로 상단을 향한 루핑 패턴이 이 뉴런을 자극한다는 의미입니다.",
  "model": "DeepL",
  "time_range": [
   384.5,
   391.56
  ]
 },
 {
  "input": "That way, going from the third layer to the last one just requires learning which combination of subcomponents corresponds to which digits.",
  "translatedText": "이렇게 하면 세 번째 레이어에서 마지막 레이어로 이동할 때 어떤 하위 구성 요소의 조합이 어떤 숫자에 해당하는지 학습하기만 하면 됩니다.",
  "model": "DeepL",
  "time_range": [
   392.44,
   400.04
  ]
 },
 {
  "input": "Of course, that just kicks the problem down the road, because how would you recognize these subcomponents, or even learn what the right subcomponents should be?",
  "translatedText": "물론, 이러한 하위 구성 요소를 어떻게 인식하고 올바른 하위 구성 요소가 무엇인지 알아낼 수 있을까요?",
  "model": "DeepL",
  "time_range": [
   401.0,
   407.64
  ]
 },
 {
  "input": "And I still haven't even talked about how one layer influences the next, but run with me on this one for a moment.",
  "translatedText": "한 레이어가 다음 레이어에 어떤 영향을 미치는지에 대해서는 아직 이야기하지 않았지만, 이 레이어에 대해 잠시 설명해 보겠습니다.",
  "model": "DeepL",
  "time_range": [
   408.06,
   413.06
  ]
 },
 {
  "input": "Recognizing a loop can also break down into subproblems.",
  "translatedText": "루프를 인식하는 것도 하위 문제로 세분화할 수 있습니다.",
  "model": "DeepL",
  "time_range": [
   413.68,
   416.68
  ]
 },
 {
  "input": "One reasonable way to do this would be to first recognize the various little edges that make it up.",
  "translatedText": "이를 수행하는 한 가지 합리적인 방법은 먼저 그것을 구성하는 다양한 작은 가장자리를 인식하는 것입니다.",
  "model": "DeepL",
  "time_range": [
   417.28,
   422.78
  ]
 },
 {
  "input": "Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7, is really just a long edge, or maybe you think of it as a certain pattern of several smaller edges.",
  "translatedText": "마찬가지로 숫자 1, 4, 7에서 볼 수 있는 긴 선은 실제로는 긴 가장자리일 뿐이며, 여러 개의 작은 가장자리로 이루어진 특정 패턴으로 생각할 수도 있습니다.",
  "model": "DeepL",
  "time_range": [
   423.78,
   434.32
  ]
 },
 {
  "input": "So maybe our hope is that each neuron in the second layer of the network corresponds with the various relevant little edges.",
  "translatedText": "따라서 네트워크의 두 번째 레이어에 있는 각 뉴런이 다양한 관련 작은 가장자리와 대응하는 것이 우리의 희망일 수 있습니다.",
  "model": "DeepL",
  "time_range": [
   435.14,
   442.72
  ]
 },
 {
  "input": "Maybe when an image like this one comes in, it lights up all of the neurons associated with around 8 to 10 specific little edges, which in turn lights up the neurons associated with the upper loop and a long vertical line, and those light up the neuron associated with a 9.",
  "translatedText": "이런 이미지가 들어오면 8~10개의 특정 작은 가장자리와 관련된 모든 뉴런에 불이 들어오고, 이는 다시 위쪽 루프와 긴 수직선과 관련된 뉴런에 불을 켜고, 9와 관련된 뉴런에 불이 들어오는 식입니다.",
  "model": "DeepL",
  "time_range": [
   443.54,
   459.72
  ]
 },
 {
  "input": "Whether or not this is what our final network actually does is another question, one that I'll come back to once we see how to train the network, but this is a hope that we might have, a sort of goal with the layered structure like this.",
  "translatedText": "이것이 우리의 최종 네트워크가 실제로 하는 일인지 여부는 네트워크를 훈련하는 방법을 알게 되면 다시 생각해 볼 문제이지만, 이것은 우리가 가질 수 있는 희망이며, 이와 같은 계층적 구조를 가진 일종의 목표입니다.",
  "model": "DeepL",
  "time_range": [
   460.68,
   472.54
  ]
 },
 {
  "input": "Moreover, you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks.",
  "translatedText": "또한 이와 같은 가장자리와 패턴을 감지할 수 있다면 다른 이미지 인식 작업에 얼마나 유용할지 상상할 수 있습니다.",
  "model": "DeepL",
  "time_range": [
   473.16,
   480.3
  ]
 },
 {
  "input": "And even beyond image recognition, there are all sorts of intelligent things you might want to do that break down into layers of abstraction.",
  "translatedText": "이미지 인식 외에도 추상화 계층으로 세분화할 수 있는 모든 종류의 지능적인 작업을 수행할 수 있습니다.",
  "model": "DeepL",
  "time_range": [
   480.88,
   487.28
  ]
 },
 {
  "input": "Parsing speech, for example, involves taking raw audio and picking out distinct sounds, which combine to make certain syllables, which combine to form words, which combine to make up phrases and more abstract thoughts, etc.",
  "translatedText": "예를 들어, 음성 구문 분석은 원시 오디오를 가져와 뚜렷한 소리를 골라내고, 이 소리가 결합하여 특정 음절을 만들고, 이 음절이 결합하여 단어를 만들고, 이 단어가 결합하여 구문과 더 추상적인 생각을 구성하는 등의 작업을 포함합니다.",
  "model": "DeepL",
  "time_range": [
   488.04,
   500.06
  ]
 },
 {
  "input": "But getting back to how any of this actually works, picture yourself right now designing how exactly the activations in one layer might determine the next.",
  "translatedText": "하지만 이 모든 것이 실제로 어떻게 작동하는지 다시 돌아가서, 한 레이어의 활성화가 다음 레이어를 어떻게 결정할지 설계하는 모습을 상상해 보세요.",
  "model": "DeepL",
  "time_range": [
   501.1,
   509.92
  ]
 },
 {
  "input": "The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits.",
  "translatedText": "목표는 픽셀을 가장자리로, 가장자리를 패턴으로, 패턴을 숫자로 결합할 수 있는 메커니즘을 갖추는 것입니다.",
  "model": "DeepL",
  "time_range": [
   510.86,
   518.98
  ]
 },
 {
  "input": "And to zoom in on one very specific example, let's say the hope is for one particular neuron in the second layer to pick up on whether or not the image has an edge in this region here.",
  "translatedText": "아주 구체적인 예를 하나 더 들어보자면, 두 번째 레이어의 특정 뉴런이 이미지의 이 영역에 가장자리가 있는지 없는지를 감지하는 것이 목표라고 가정해 보겠습니다.",
  "model": "DeepL",
  "time_range": [
   519.44,
   530.62
  ]
 },
 {
  "input": "The question at hand is what parameters should the network have?",
  "translatedText": "당면한 문제는 네트워크에 어떤 매개변수가 있어야 하는가 하는 것입니다.",
  "model": "DeepL",
  "time_range": [
   531.44,
   535.1
  ]
 },
 {
  "input": "What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern, or any other pixel pattern, or the pattern that several edges can make a loop, and other such things?",
  "translatedText": "이 패턴이나 다른 픽셀 패턴, 여러 모서리가 루프를 만들 수 있는 패턴 등을 잠재적으로 캡처할 수 있을 만큼 표현력을 발휘하려면 어떤 다이얼과 노브를 조정할 수 있어야 하나요?",
  "model": "DeepL",
  "time_range": [
   535.64,
   547.78
  ]
 },
 {
  "input": "Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer.",
  "translatedText": "우리가 할 일은 뉴런과 첫 번째 레이어의 뉴런 사이의 각 연결에 가중치를 할당하는 것입니다.",
  "model": "DeepL",
  "time_range": [
   548.72,
   555.56
  ]
 },
 {
  "input": "These weights are just numbers.",
  "translatedText": "이 가중치는 단지 숫자에 불과합니다.",
  "model": "DeepL",
  "time_range": [
   556.32,
   557.7
  ]
 },
 {
  "input": "Then take all of those activations from the first layer and compute their weighted sum according to these weights.",
  "translatedText": "그런 다음 첫 번째 레이어에서 이러한 모든 활성화를 가져와 가중치에 따라 가중치 합계를 계산합니다.",
  "model": "DeepL",
  "time_range": [
   558.54,
   565.5
  ]
 },
 {
  "input": "I find it helpful to think of these weights as being organized into a little grid of their own, and I'm going to use green pixels to indicate positive weights, and red pixels to indicate negative weights, where the brightness of that pixel is some loose depiction of the weight's value.",
  "translatedText": "저는 이러한 가중치를 작은 격자로 정리한 것으로 생각하면 도움이 된다고 생각하며, 녹색 픽셀은 양의 가중치를 나타내고 빨간색 픽셀은 음의 가중치를 나타내며, 해당 픽셀의 밝기는 가중치의 값을 느슨하게 묘사하는 것입니다.",
  "model": "DeepL",
  "time_range": [
   567.7,
   581.78
  ]
 },
 {
  "input": "Now if we made the weights associated with almost all of the pixels zero except for some positive weights in this region that we care about, then taking the weighted sum of all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about.",
  "translatedText": "이제 관심 있는 영역의 일부 양수 가중치를 제외하고 거의 모든 픽셀과 관련된 가중치를 0으로 만들었으므로 모든 픽셀 값의 가중치 합계를 구하면 관심 있는 영역의 픽셀 값만 합산하는 것과 같습니다.",
  "model": "DeepL",
  "time_range": [
   582.78,
   597.82
  ]
 },
 {
  "input": "And if you really wanted to pick up on whether there's an edge here, what you might do is have some negative weights associated with the surrounding pixels.",
  "translatedText": "여기에 가장자리가 있는지 여부를 파악하고 싶다면 주변 픽셀에 음수 가중치를 적용하면 됩니다.",
  "model": "DeepL",
  "time_range": [
   599.14,
   606.6
  ]
 },
 {
  "input": "Then the sum is largest when those middle pixels are bright but the surrounding pixels are darker.",
  "translatedText": "그런 다음 중간 픽셀이 밝고 주변 픽셀이 어두울 때 합이 가장 큽니다.",
  "model": "DeepL",
  "time_range": [
   607.48,
   612.7
  ]
 },
 {
  "input": "When you compute a weighted sum like this, you might come out with any number, but for this network what we want is for activations to be some value between 0 and 1.",
  "translatedText": "이렇게 가중 합계를 계산하면 어떤 숫자가 나올 수도 있지만, 이 네트워크에서 우리가 원하는 것은 활성화가 0과 1 사이의 값이 되는 것입니다.",
  "model": "DeepL",
  "time_range": [
   614.26,
   623.54
  ]
 },
 {
  "input": "So a common thing to do is to pump this weighted sum into some function that squishes the real number line into the range between 0 and 1.",
  "translatedText": "따라서 이 가중 합계를 0과 1 사이의 범위로 실수선을 쪼개는 함수로 펌핑하는 것이 일반적입니다.",
  "model": "DeepL",
  "time_range": [
   624.12,
   632.14
  ]
 },
 {
  "input": "And a common function that does this is called the sigmoid function, also known as a logistic curve.",
  "translatedText": "이를 수행하는 일반적인 함수를 로지스틱 곡선이라고도 하는 시그모이드 함수라고 합니다.",
  "model": "DeepL",
  "time_range": [
   632.46,
   637.42
  ]
 },
 {
  "input": "Basically very negative inputs end up close to 0, positive inputs end up close to 1, and it just steadily increases around the input 0.",
  "translatedText": "기본적으로 매우 음수인 입력은 0에 가까워지고, 양수인 입력은 1에 가까워지며, 입력 0을 중심으로 꾸준히 증가합니다.",
  "model": "DeepL",
  "time_range": [
   638.0,
   646.6
  ]
 },
 {
  "input": "So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is.",
  "translatedText": "따라서 여기서 뉴런의 활성화는 기본적으로 관련 가중치 합이 얼마나 양수인지를 측정하는 척도입니다.",
  "model": "DeepL",
  "time_range": [
   649.12,
   656.36
  ]
 },
 {
  "input": "But maybe it's not that you want the neuron to light up when the weighted sum is bigger than 0.",
  "translatedText": "하지만 가중치 합이 0보다 클 때 뉴런에 불이 켜지기를 원하는 것은 아닐 수도 있습니다.",
  "model": "DeepL",
  "time_range": [
   657.54,
   661.88
  ]
 },
 {
  "input": "Maybe you only want it to be active when the sum is bigger than say 10.",
  "translatedText": "합이 10보다 클 때만 활성화되도록 하고 싶을 수도 있습니다.",
  "model": "DeepL",
  "time_range": [
   662.28,
   666.36
  ]
 },
 {
  "input": "That is, you want some bias for it to be inactive.",
  "translatedText": "즉, 비활성화를 위해 약간의 편향성을 원합니다.",
  "model": "DeepL",
  "time_range": [
   666.84,
   670.26
  ]
 },
 {
  "input": "What we'll do then is just add in some other number like negative 10 to this weighted sum before plugging it through the sigmoid squishification function.",
  "translatedText": "그런 다음 이 가중 합계에 음수 10과 같은 다른 숫자를 더한 다음 시그모이드 스퀴시화 함수를 통해 연결하면 됩니다.",
  "model": "DeepL",
  "time_range": [
   671.38,
   679.66
  ]
 },
 {
  "input": "That additional number is called the bias.",
  "translatedText": "이 추가 숫자를 바이어스라고 합니다.",
  "model": "DeepL",
  "time_range": [
   680.58,
   682.44
  ]
 },
 {
  "input": "So the weights tell you what pixel pattern this neuron in the second layer is picking up on, and the bias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active.",
  "translatedText": "따라서 가중치는 두 번째 레이어의 뉴런이 어떤 픽셀 패턴을 포착하고 있는지 알려주고, 바이어스는 뉴런이 의미 있게 활성화되기 시작하려면 가중치 합이 얼마나 높아야 하는지를 알려줍니다.",
  "model": "DeepL",
  "time_range": [
   683.46,
   695.18
  ]
 },
 {
  "input": "And that is just one neuron.",
  "translatedText": "그리고 그것은 하나의 뉴런에 불과합니다.",
  "model": "DeepL",
  "time_range": [
   696.12,
   697.68
  ]
 },
 {
  "input": "Every other neuron in this layer is going to be connected to all 784 pixel neurons from the first layer, and each one of those 784 connections has its own weight associated with it.",
  "translatedText": "이 레이어의 다른 모든 뉴런은 첫 번째 레이어의 784개 픽셀 뉴런에 모두 연결되며, 784개의 연결에는 각각 고유한 가중치가 부여됩니다.",
  "model": "DeepL",
  "time_range": [
   698.28,
   710.94
  ]
 },
 {
  "input": "Also, each one has some bias, some other number that you add on to the weighted sum before squishing it with the sigmoid.",
  "translatedText": "또한 각각에는 시그모이드로 쪼개기 전에 가중 합계에 더하는 다른 숫자, 즉 편향이 있습니다.",
  "model": "DeepL",
  "time_range": [
   711.6,
   717.6
  ]
 },
 {
  "input": "And that's a lot to think about!",
  "translatedText": "생각해야 할 것이 많습니다!",
  "model": "DeepL",
  "time_range": [
   718.11,
   719.54
  ]
 },
 {
  "input": "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights, along with 16 biases.",
  "translatedText": "16개의 뉴런으로 구성된 이 숨겨진 레이어는 16개의 가중치와 함께 총 784개의 가중치를 16개의 편향으로 계산한 것입니다.",
  "model": "DeepL",
  "time_range": [
   719.96,
   727.98
  ]
 },
 {
  "input": "And all of that is just the connections from the first layer to the second.",
  "translatedText": "그리고 이 모든 것은 첫 번째 레이어에서 두 번째 레이어로 연결되는 것일 뿐입니다.",
  "model": "DeepL",
  "time_range": [
   728.84,
   731.94
  ]
 },
 {
  "input": "The connections between the other layers also have a bunch of weights and biases associated with them.",
  "translatedText": "다른 레이어 간의 연결에도 여러 가중치와 편향이 연관되어 있습니다.",
  "model": "DeepL",
  "time_range": [
   732.52,
   737.34
  ]
 },
 {
  "input": "All said and done, this network has almost exactly 13,000 total weights and biases.",
  "translatedText": "이 네트워크의 총 가중치와 편향성은 거의 정확히 13,000개에 달합니다.",
  "model": "DeepL",
  "time_range": [
   738.34,
   743.8
  ]
 },
 {
  "input": "13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways.",
  "translatedText": "13,000개의 노브와 다이얼을 조정하고 돌려서 이 네트워크가 다양한 방식으로 작동하도록 설정할 수 있습니다.",
  "model": "DeepL",
  "time_range": [
   743.8,
   749.96
  ]
 },
 {
  "input": "So when we talk about learning, what that's referring to is getting the computer to find a valid setting for all of these many many numbers so that it'll actually solve the problem at hand.",
  "translatedText": "따라서 학습이란 컴퓨터가 수많은 숫자에 대해 유효한 설정을 찾아내어 실제로 당면한 문제를 해결할 수 있도록 하는 것을 의미합니다.",
  "model": "DeepL",
  "time_range": [
   751.04,
   761.36
  ]
 },
 {
  "input": "One thought experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand, purposefully tweaking the numbers so that the second layer picks up on edges, the third layer picks up on patterns, etc.",
  "translatedText": "재미와 공포를 동시에 느낄 수 있는 한 가지 사고 실험은 앉아서 이 모든 가중치와 편향을 손으로 직접 설정하고, 의도적으로 숫자를 조정하여 두 번째 레이어가 가장자리를 포착하고 세 번째 레이어가 패턴을 포착하는 등의 작업을 상상해 보는 것입니다.",
  "model": "DeepL",
  "time_range": [
   762.62,
   776.58
  ]
 },
 {
  "input": "I personally find this satisfying rather than just treating the network as a total black box, because when the network doesn't perform the way you anticipate, if you've built up a little bit of a relationship with what those weights and biases actually mean, you have a starting place for experimenting with how to change the structure to improve.",
  "translatedText": "네트워크가 예상한 대로 작동하지 않을 때 이러한 가중치와 편향성이 실제로 무엇을 의미하는지에 대한 관계를 조금이라도 구축했다면, 구조를 개선하기 위해 구조를 변경하는 방법을 실험할 수 있는 출발점이 생기기 때문에 개인적으로 네트워크를 완전한 블랙박스로 취급하는 것보다 만족스럽다고 생각합니다.",
  "model": "DeepL",
  "time_range": [
   776.98,
   794.18
  ]
 },
 {
  "input": "Or when the network does work but not for the reasons you might expect, digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible solutions.",
  "translatedText": "또는 네트워크가 작동하지만 예상과 달리 작동하지 않는 경우, 가중치와 편향이 어떤 역할을 하는지 파헤치는 것은 가정에 도전하고 가능한 솔루션의 전체 공간을 실제로 드러내는 좋은 방법입니다.",
  "model": "DeepL",
  "time_range": [
   794.96,
   805.82
  ]
 },
 {
  "input": "By the way, the actual function here is a little cumbersome to write down, don't you think?",
  "translatedText": "그런데 여기의 실제 기능은 적기에는 조금 번거롭지 않나요?",
  "model": "DeepL",
  "time_range": [
   806.84,
   810.68
  ]
 },
 {
  "input": "So let me show you a more notationally compact way that these connections are represented.",
  "translatedText": "이러한 연결을 보다 간결하게 표현하는 표기법을 보여드리겠습니다.",
  "model": "DeepL",
  "time_range": [
   812.5,
   817.14
  ]
 },
 {
  "input": "This is how you'd see it if you choose to read up more about neural networks.",
  "translatedText": "신경망에 대해 더 자세히 알아보려면 이렇게 하면 됩니다.",
  "model": "DeepL",
  "time_range": [
   817.66,
   820.52
  ]
 },
 {
  "input": "Organize all of the activations from one layer into a column as a matrix corresponds to the connections between one layer and a particular neuron in the next layer.",
  "translatedText": "한 레이어의 모든 활성화는 한 레이어와 다음 레이어의 특정 뉴런 사이의 연결에 해당하는 매트릭스로 열로 구성합니다.",
  "model": "DeepL",
  "time_range": [
   821.38,
   838.0
  ]
 },
 {
  "input": "What that means is that taking the weighted sum of the activations in the first layer according to these weights corresponds to one of the terms in the matrix vector product of everything we have on the left here.",
  "translatedText": "즉, 이러한 가중치에 따라 첫 번째 레이어에서 활성화의 가중치 합을 구하면 여기 왼쪽에 있는 모든 항목의 행렬 벡터 곱의 항 중 하나에 해당한다는 뜻입니다.",
  "model": "DeepL",
  "time_range": [
   838.54,
   849.88
  ]
 },
 {
  "input": "By the way, so much of machine learning just comes down to having a good grasp of linear algebra, so for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means, take a look at the series I did on linear algebra, especially chapter 3.",
  "translatedText": "머신러닝의 많은 부분이 선형 대수학을 잘 이해하는 데 달려 있기 때문에 행렬과 행렬 벡터 곱셈의 의미를 시각적으로 잘 이해하고 싶은 분들은 제가 작성한 선형 대수학 시리즈, 특히 3장을 살펴보시기 바랍니다.",
  "model": "DeepL",
  "time_range": [
   854.0,
   868.6
  ]
 },
 {
  "input": "Back to our expression, instead of talking about adding the bias to each one of these values independently, we represent it by organizing all those biases into a vector, and adding the entire vector to the previous matrix vector product.",
  "translatedText": "다시 표현식으로 돌아가서, 각각의 값에 편향을 독립적으로 더하는 대신 모든 편향을 벡터로 구성하고 전체 벡터를 이전 행렬 벡터 곱에 더하는 방식으로 표현합니다.",
  "model": "DeepL",
  "time_range": [
   869.24,
   882.3
  ]
 },
 {
  "input": "Then as a final step, I'll wrap a sigmoid around the outside here, and what that's supposed to represent is that you're going to apply the sigmoid function to each specific component of the resulting vector inside.",
  "translatedText": "그런 다음 마지막 단계로 여기 바깥쪽을 시그모이드로 감싸고, 이것이 나타내는 것은 결과 벡터의 각 특정 구성 요소에 시그모이드 함수를 내부에 적용한다는 것입니다.",
  "model": "DeepL",
  "time_range": [
   883.28,
   894.74
  ]
 },
 {
  "input": "So once you write down this weight matrix and these vectors as their own symbols, you can communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression, and this makes the relevant code both a lot simpler and a lot faster, since many libraries optimize the heck out of matrix multiplication.",
  "translatedText": "따라서 이 가중치 행렬과 벡터를 자체 심볼로 적어두면 한 레이어에서 다음 레이어로의 전체 활성화 전환을 매우 간결하고 깔끔한 표현으로 전달할 수 있으며, 많은 라이브러리가 행렬 곱셈을 최적화하기 때문에 관련 코드가 훨씬 간단하고 훨씬 빨라집니다.",
  "model": "DeepL",
  "time_range": [
   895.94,
   915.66
  ]
 },
 {
  "input": "Remember how earlier I said these neurons are simply things that hold numbers?",
  "translatedText": "앞서 뉴런은 단순히 숫자를 저장하는 존재라고 말씀드린 것을 기억하시나요?",
  "model": "DeepL",
  "time_range": [
   917.82,
   921.46
  ]
 },
 {
  "input": "Well of course the specific numbers that they hold depends on the image you feed in, so it's actually more accurate to think of each neuron as a function, one that takes in the outputs of all the neurons in the previous layer and spits out a number between 0 and 1.",
  "translatedText": "물론 뉴런이 보유하는 구체적인 숫자는 입력하는 이미지에 따라 달라지므로 각 뉴런을 이전 레이어의 모든 뉴런의 출력을 받아 0과 1 사이의 숫자를 뱉어내는 함수로 생각하는 것이 더 정확합니다.",
  "model": "DeepL",
  "time_range": [
   922.22,
   938.34
  ]
 },
 {
  "input": "Really the entire network is just a function, one that takes in 784 numbers as an input and spits out 10 numbers as an output.",
  "translatedText": "실제로 전체 네트워크는 784개의 숫자를 입력으로 받아 10개의 숫자를 출력으로 뱉어내는 하나의 함수에 불과합니다.",
  "model": "DeepL",
  "time_range": [
   939.2,
   947.06
  ]
 },
 {
  "input": "It's an absurdly complicated function, one that involves 13,000 parameters in the forms of these weights and biases that pick up on certain patterns, and which involves iterating many matrix vector products and the sigmoid squishification function, but it's just a function nonetheless, and in a way it's kind of reassuring that it looks complicated.",
  "translatedText": "특정 패턴을 포착하는 가중치와 편향의 형태로 13,000개의 매개 변수가 포함되고 많은 행렬 벡터 곱과 시그모이드 스퀴시화 함수를 반복하는 엄청나게 복잡한 함수이지만, 그럼에도 불구하고 그냥 함수일 뿐이며 어떤 면에서는 복잡해 보이는 것이 오히려 안심이 되기도 합니다.",
  "model": "DeepL",
  "time_range": [
   947.56,
   966.66
  ]
 },
 {
  "input": "I mean if it were any simpler, what hope would we have that it could take on the challenge of recognizing digits?",
  "translatedText": "이보다 더 간단했다면 숫자를 인식하는 데 어떤 희망을 가질 수 있었을까요?",
  "model": "DeepL",
  "time_range": [
   967.34,
   972.28
  ]
 },
 {
  "input": "And how does it take on that challenge?",
  "translatedText": "그렇다면 그 도전은 어떻게 이루어질까요?",
  "model": "DeepL",
  "time_range": [
   973.34,
   974.7
  ]
 },
 {
  "input": "How does this network learn the appropriate weights and biases just by looking at data?",
  "translatedText": "이 네트워크는 어떻게 데이터를 보고 적절한 가중치와 편향성을 학습할 수 있을까요?",
  "model": "DeepL",
  "time_range": [
   975.08,
   979.36
  ]
 },
 {
  "input": "Well that's what I'll show in the next video, and I'll also dig a little more into what this particular network we're seeing is really doing.",
  "translatedText": "다음 동영상에서 이 내용을 보여드리고, 우리가 보고 있는 이 특정 네트워크가 실제로 어떤 일을 하고 있는지 조금 더 자세히 살펴보겠습니다.",
  "model": "DeepL",
  "time_range": [
   980.14,
   986.12
  ]
 },
 {
  "input": "Now is the point I suppose I should say subscribe to stay notified about when video or any new videos come out, but realistically most of you don't actually receive notifications from YouTube, do you?",
  "translatedText": "이제 동영상이나 새 동영상이 언제 나오는지 알림을 받으려면 구독을 신청해야 한다고 생각하지만, 현실적으로 여러분 대부분은 실제로 YouTube에서 알림을 받지 않으시죠?",
  "model": "DeepL",
  "time_range": [
   987.58,
   997.42
  ]
 },
 {
  "input": "Maybe more honestly I should say subscribe so that the neural networks that underlie YouTube's recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you.",
  "translatedText": "더 솔직하게 말하자면 구독을 하면 YouTube 추천 알고리즘의 기반이 되는 신경망이 사용자가 이 채널의 콘텐츠를 보고 싶어한다고 믿도록 준비된 상태로 만들 수 있습니다.",
  "model": "DeepL",
  "time_range": [
   998.02,
   1007.88
  ]
 },
 {
  "input": "Anyway stay posted for more.",
  "translatedText": "자세한 내용은 계속 확인해 주세요.",
  "model": "DeepL",
  "time_range": [
   1008.56,
   1009.94
  ]
 },
 {
  "input": "Thank you very much to everyone supporting these videos on Patreon.",
  "translatedText": "Patreon에서 이 동영상을 후원해주시는 모든 분들께 진심으로 감사드립니다.",
  "model": "DeepL",
  "time_range": [
   1010.76,
   1013.5
  ]
 },
 {
  "input": "I've been a little slow to progress in the probability series this summer, but I'm jumping back into it after this project, so patrons you can look out for updates there.",
  "translatedText": "이번 여름에 확률 시리즈 진행이 조금 더뎠지만, 이 프로젝트가 끝나면 다시 시작하려고 하니 고객 여러분도 업데이트를 기대해 주세요.",
  "model": "DeepL",
  "time_range": [
   1014.0,
   1021.9
  ]
 },
 {
  "input": "To close things off here I have with me Leisha Lee who did her PhD work on the theoretical side of deep learning and who currently works at a venture capital firm called Amplify Partners who kindly provided some of the funding for this video.",
  "translatedText": "마지막으로 딥러닝의 이론적 측면에서 박사 학위를 취득하고 현재 Amplify Partners라는 벤처 캐피탈 회사에서 일하고 있으며 이 동영상에 대한 자금 일부를 친절하게 제공한 레이샤 리와 함께 이야기를 마무리합니다.",
  "model": "DeepL",
  "time_range": [
   1023.6,
   1034.62
  ]
 },
 {
  "input": "So Leisha one thing I think we should quickly bring up is this sigmoid function.",
  "translatedText": "그래서 레이샤는 이 시그모이드 함수를 빨리 언급해야 한다고 생각합니다.",
  "model": "DeepL",
  "time_range": [
   1035.46,
   1039.12
  ]
 },
 {
  "input": "As I understand it early networks use this to squish the relevant weighted sum into that interval between zero and one, you know kind of motivated by this biological analogy of neurons either being inactive or active.",
  "translatedText": "제가 알기로 초기 네트워크에서는 이를 사용하여 관련 가중치를 0과 1 사이의 간격으로 쪼개는데, 이는 뉴런이 비활성 또는 활성 상태라는 생물학적 비유에서 착안한 것입니다.",
  "model": "DeepL",
  "time_range": [
   1039.7,
   1049.84
  ]
 },
 {
  "input": "Exactly.",
  "translatedText": "맞습니다.",
  "model": "DeepL",
  "time_range": [
   1050.28,
   1050.3
  ]
 },
 {
  "input": "But relatively few modern networks actually use sigmoid anymore.",
  "translatedText": "하지만 최신 네트워크에서 시그모이드를 실제로 사용하는 경우는 상대적으로 적습니다.",
  "model": "DeepL",
  "time_range": [
   1050.56,
   1054.04
  ]
 },
 {
  "input": "Yeah.",
  "translatedText": "네",
  "model": "DeepL",
  "time_range": [
   1054.32,
   1054.32
  ]
 },
 {
  "input": "It's kind of old school right?",
  "translatedText": "좀 구식이지 않나요?",
  "model": "DeepL",
  "time_range": [
   1054.44,
   1055.54
  ]
 },
 {
  "input": "Yeah or rather relu seems to be much easier to train.",
  "translatedText": "네, 오히려 릴루가 훨씬 더 훈련하기 쉬운 것 같습니다.",
  "model": "DeepL",
  "time_range": [
   1055.76,
   1058.98
  ]
 },
 {
  "input": "And relu stands for rectified linear unit?",
  "translatedText": "릴루는 정류된 선형 유닛의 약자입니까?",
  "model": "DeepL",
  "time_range": [
   1059.4,
   1062.34
  ]
 },
 {
  "input": "Yes it's this kind of function where you're just taking a max of zero and a where a is given by what you were explaining in the video and what this was sort of motivated from I think was a partially by a biological analogy with how neurons would either be activated or not and so if it passes a certain threshold it would be the identity function but if it did not then it would just not be activated so it'd be zero so it's kind of a simplification.",
  "translatedText": "예, 동영상에서 설명하신 것처럼 최대값 0과 a를 취하는 이런 종류의 함수이며, 이 함수의 동기는 부분적으로 뉴런이 활성화되거나 활성화되지 않는 방식에 대한 생물학적 유추에서 비롯된 것으로, 특정 임계값을 통과하면 정체 함수가 되지만 그렇지 않으면 활성화되지 않아 0이 되므로 일종의 단순화라고 할 수 있습니다.",
  "model": "DeepL",
  "time_range": [
   1062.68,
   1090.84
  ]
 },
 {
  "input": "Using sigmoids didn't help training or it was very difficult to train at some point and people just tried relu and it happened to work very well for these incredibly deep neural networks.",
  "translatedText": "시그모이드를 사용하면 훈련에 도움이 되지 않거나 훈련이 매우 어려웠고, 사람들은 릴루를 시도했는데 놀랍도록 깊은 신경망에 매우 잘 작동했습니다.",
  "model": "DeepL",
  "time_range": [
   1091.16,
   1104.62
  ]
 },
 {
  "input": "All right thank you Alicia.",
  "translatedText": "알겠습니다, 알리샤 감사합니다.",
  "model": "DeepL",
  "time_range": [
   1105.1,
   1105.64
  ]
 }
]