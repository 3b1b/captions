[
 {
  "input": "This is a 3.",
  "translatedText": "",
  "from_community_srt": "여기에 숫자 3이 있습니다.",
  "n_reviews": 0,
  "start": 4.22,
  "end": 5.4
 },
 {
  "input": "It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3.",
  "translatedText": "",
  "from_community_srt": "다소 삐뚤삐뚤하게 쓰였고, 28x28 픽셀 밖에 안되는 저해상도로  기록되어 있지만, 여러분의 뇌는 이 이미지를 아주 간단히  3이라고 인식합니다.",
  "n_reviews": 0,
  "start": 6.06,
  "end": 13.72
 },
 {
  "input": "And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly.",
  "translatedText": "",
  "from_community_srt": "뇌는 대체 어떻게 이런 어마어마한 일을 간단히 해내는 것일까요? 그러니까,",
  "n_reviews": 0,
  "start": 14.34,
  "end": 18.96
 },
 {
  "input": "I mean, this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next.",
  "translatedText": "",
  "from_community_srt": "여러분의 뇌는 오른쪽의 이 그림들도 전부 3이라고 인식해냅니다. 각 픽셀들의 밝기가 이미지마다 서로 다른데도 말이죠.",
  "n_reviews": 0,
  "start": 19.7,
  "end": 28.32
 },
 {
  "input": "The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3.",
  "translatedText": "",
  "from_community_srt": "여러분의 눈 안에 있는 시지각 세포들이 빛 신호를 받아들이는 패턴은, 이 그림을 볼 때와, 이 그림을 볼 때 서로 다른 양상을 가집니다.",
  "n_reviews": 0,
  "start": 28.9,
  "end": 36.94
 },
 {
  "input": "But something in that crazy-smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas.",
  "translatedText": "",
  "from_community_srt": "그러나, 여러분의 뇌에 있는 시각피질(visual cortex)은 이러한 차이에도 불구하고 두 이미지가 같은 개념을 가리킨다고 판단합니다. 동시에, 다른 개념을 가리키는 이미지들도 구별해내지요.",
  "n_reviews": 0,
  "start": 37.52,
  "end": 48.26
 },
 {
  "input": "But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this and outputs a single number between 0 and 10, telling you what it thinks the digit is, well the task goes from comically trivial to dauntingly difficult.",
  "translatedText": "",
  "from_community_srt": "하지만 만약, 제가 여러분에게 이 그림과 같이 28x28개의 입력값들을 받아 0에서 9 범위의 정수 값 하나를 내놓는 프로그램을 짜 보라고 한다면 어떨까요. 문제가 갑자기 어려워집니다.",
  "n_reviews": 0,
  "start": 49.22,
  "end": 66.18
 },
 {
  "input": "Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future.",
  "translatedText": "",
  "from_community_srt": "여러분이 동굴 속에서 살던 분이 아니라면, 굳이 현재부터 미래까지의 기계학습(Machine Learning)과 신경망(Neural Network)의 중요성을 다시 강조할 필요는 없을 듯 합니다.",
  "n_reviews": 0,
  "start": 67.16,
  "end": 74.64
 },
 {
  "input": "But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing, not as a buzzword but as a piece of math.",
  "translatedText": "",
  "from_community_srt": "여기에서는 여러분이 배경지식이 없다고 간주하고 신경망이 구체적으로 무엇인지를 알아보고, 유행어가 아닌, 수학으로서 신경망이 도대체 무엇인지를 보여드릴 것입니다.",
  "n_reviews": 0,
  "start": 75.12,
  "end": 84.46
 },
 {
  "input": "My hope is that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote-unquote learning.",
  "translatedText": "",
  "from_community_srt": "이 동영상을 통해 여러분이 신경망 구조가 만들어진 토대를 깨닫고 그 구조 속에서 \"학습\"이 어떤 의미를 갖는지 알게 되었으면 좋겠습니다.",
  "n_reviews": 0,
  "start": 85.02,
  "end": 94.34
 },
 {
  "input": "This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning.",
  "translatedText": "",
  "from_community_srt": "이 영상에서는 신경망의 구조에 대해 먼저 다룰 것입니다. 학습은 다음 영상에서 다룰거고요.",
  "n_reviews": 0,
  "start": 95.36,
  "end": 100.26
 },
 {
  "input": "What we're going to do is put together a neural network that can learn to recognize handwritten digits.",
  "translatedText": "",
  "from_community_srt": "이제, 필기체 숫자 이미지에서 숫자 인식을 학습할 수 있는신경망의 구조를 알아보겠습니다.",
  "n_reviews": 0,
  "start": 100.96,
  "end": 106.04
 },
 {
  "input": "This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quo here, because at the end of the two videos I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer.",
  "translatedText": "",
  "from_community_srt": "이것은 신경망이론을 소개하는 고전적 예시중 하나입니다. 이 영상과 다음 영상의 말미에 인공신경망에 대한통찰력을 기를 수 있는 자료가 포함되어 있는 웹페이지의 링크를 드릴 것입니다. 예제에 사용된 코드 또한 여러분에게 제공됩니다.",
  "n_reviews": 0,
  "start": 109.36,
  "end": 123.08
 },
 {
  "input": "There are many many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos you and I are just going to look at the simplest plain vanilla form with no added frills.",
  "translatedText": "",
  "from_community_srt": "신경망에는 정말 다양한 종류가 있습니다. 최근에 많은 개발이 이루어지고 있기도  하지요. 그렇지만, 이번 영상에서는 제일 기본적인 형태의 신경망을 집중적으로 다룰 것입니다.",
  "n_reviews": 0,
  "start": 125.04,
  "end": 139.18
 },
 {
  "input": "This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds around.",
  "translatedText": "",
  "from_community_srt": "이것들을 잘 이해해야 비로소 더 강력하고 복잡한 신경망들을 이해할 수 있을 것입니다. 사실, 기본 형태도 꽤 복잡합니다.",
  "n_reviews": 0,
  "start": 139.86,
  "end": 148.6
 },
 {
  "input": "But even in this simplest form it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do.",
  "translatedText": "",
  "from_community_srt": "중요한건, 기본 신경망으로도 숫자 손글씨를 인식하는데는 충분하다는 겁니다. 사람이 아닌 컴퓨터가 말이지요. 정말 놀라운 일입니다.",
  "n_reviews": 0,
  "start": 149.12,
  "end": 156.52
 },
 {
  "input": "And at the same time you'll see how it does fall short of a couple hopes that we might have for it.",
  "translatedText": "",
  "from_community_srt": "동시에, 여러분은 컴퓨터가 우리의 기대를 저버리는 일도 볼지 모르겠습니다.",
  "n_reviews": 0,
  "start": 157.48,
  "end": 162.28
 },
 {
  "input": "As the name suggests neural networks are inspired by the brain, but let's break that down.",
  "translatedText": "",
  "from_community_srt": "이름에서 알 수 있는 것처럼, 신경망이라는 아이디어는 사람의 뇌에서 착안한 것입니다. 뇌의 어떠한 점을 본딴 걸까요?",
  "n_reviews": 0,
  "start": 163.38,
  "end": 168.5
 },
 {
  "input": "What are the neurons, and in what sense are they linked together?",
  "translatedText": "",
  "from_community_srt": "뇌와 신경망이 어떤 유사점을 가지는 걸까요? 먼저,",
  "n_reviews": 0,
  "start": 168.52,
  "end": 171.66
 },
 {
  "input": "Right now when I say neuron all I want you to think about is a thing that holds a number, specifically a number between 0 and 1.",
  "translatedText": "",
  "from_community_srt": "뉴런에 대해 알아봅시다. 뉴런은 하나의 숫자를 담는다는 사실을 떠올리는 것으로 충분합니다. 0.0에서 1.0까지의 숫자만요.",
  "n_reviews": 0,
  "start": 172.5,
  "end": 180.44
 },
 {
  "input": "It's really not more than that.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 180.68,
  "end": 182.56
 },
 {
  "input": "For example the network starts with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784 neurons in total.",
  "translatedText": "",
  "from_community_srt": "그 뿐입니다. 예를 들어, 이미지의 28x28개 픽셀들을  입력값으로 취하는 신경망을 생각해봅시다. 총 784개입니다.",
  "n_reviews": 0,
  "start": 183.78,
  "end": 194.22
 },
 {
  "input": "Each one of these holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels.",
  "translatedText": "",
  "from_community_srt": "각 뉴런들은 각 픽셀의 밝기를 나타냅니다. 검은 픽셀은 0.0, 하얀 픽셀은 1.0에 해당합니다.",
  "n_reviews": 0,
  "start": 194.7,
  "end": 204.38
 },
 {
  "input": "This number inside the neuron is called its activation, and the image you might have in mind here is that each neuron is lit up when its activation is a high number.",
  "translatedText": "",
  "from_community_srt": "신경망 안에서 이러한 숫자들은 입력값이라고 불립니다. 큰 입력값이 주어질수록 각각의 신경망이 더 큰 정도로 활성화 됩니다.",
  "n_reviews": 0,
  "start": 205.3,
  "end": 214.16
 },
 {
  "input": "So all of these 784 neurons make up the first layer of our network.",
  "translatedText": "",
  "from_community_srt": "이 모든 784개의 뉴런이 신경망의 입력층을 구성하게 됩니다.",
  "n_reviews": 0,
  "start": 216.72,
  "end": 221.86
 },
 {
  "input": "Now jumping over to the last layer, this has 10 neurons, each representing one of the digits.",
  "translatedText": "",
  "from_community_srt": "출력층은 총 10개의 뉴런을 가지고 있습니다.",
  "n_reviews": 0,
  "start": 226.5,
  "end": 231.36
 },
 {
  "input": "The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit.",
  "translatedText": "",
  "from_community_srt": "각각의 뉴런은 0 부터 9 까지의 숫자를 대표하는데요, 이 뉴런들은 0과 1사이의 어떤 값을 취하고. 그 값은 뉴런이 대표하는 숫자와 입력값이 일치하는 정도를 나타냅니다. 그 값은 주어진 입력값과  각 뉴런이 대표하는 숫자 사이의 일치 정도를 나타냅니다.",
  "n_reviews": 0,
  "start": 232.04,
  "end": 242.12
 },
 {
  "input": "There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark for how on earth this process of recognizing digits is going to be handled.",
  "translatedText": "",
  "from_community_srt": "입력층과 출력층 사이에는 숨겨진 층이라고 불리는 몇 개의 층들이 있는데요, 지금 당장은 이 층이 어떻게 숫자를 인식할 수 있는지 중요하지 않으므로 물음표 표시만 해놓고 넘어가도록 합시다.",
  "n_reviews": 0,
  "start": 243.04,
  "end": 253.6
 },
 {
  "input": "In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice.",
  "translatedText": "",
  "from_community_srt": "이 신경망에선 각각 16개의 뉴런을 가진 두 개의 숨겨진 층을 사용할건데 솔직히 그냥 아무 숫자나 부른겁니다 두 개의 층을 선택한 이유는 조금 있다 이 구조를 구성할 방법 때문이고",
  "n_reviews": 0,
  "start": 254.26,
  "end": 260.56
 },
 {
  "input": "To be honest I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that was just a nice number to fit on the screen.",
  "translatedText": "",
  "from_community_srt": "그리고 16개는... 그냥 화면에 잘 들어가서 넣은 겁니다.",
  "n_reviews": 0,
  "start": 261.02,
  "end": 268.2
 },
 {
  "input": "In practice there is a lot of room for experiment with a specific structure here.",
  "translatedText": "",
  "from_community_srt": "같은 역할을 하는 신경망의 형태는 더 많이 있습니다.",
  "n_reviews": 0,
  "start": 268.78,
  "end": 272.34
 },
 {
  "input": "The way the network operates, activations in one layer determine the activations of the next layer.",
  "translatedText": "",
  "from_community_srt": "이 신경망은 기본적으로한 층에서의 활성화가 다음 층의 활성화를 유도하는 방식으로 작동합니다.",
  "n_reviews": 0,
  "start": 273.02,
  "end": 278.48
 },
 {
  "input": "And of course the heart of the network as an information processing mechanism comes down to exactly how those activations from one layer bring about activations in the next layer.",
  "translatedText": "",
  "from_community_srt": "정보 처리에 있어서의 신경망의 가장 중요한 점은 도대체 어떻게 한 층에서의 활성화가 다른 층의 활성화를 불러일으키는지에 관한 점입니다.",
  "n_reviews": 0,
  "start": 279.2,
  "end": 288.58
 },
 {
  "input": "It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire.",
  "translatedText": "",
  "from_community_srt": "이러한 과정은 생물의 뉴런이 작동하는 방식과도 닮아있는데, 몇몇 뉴런의 활성화가 다른 뉴런의 활성화를 수반한다는 점이죠.",
  "n_reviews": 0,
  "start": 289.14,
  "end": 297.18
 },
 {
  "input": "Now the network I'm showing here has already been trained to recognize digits, and let me show you what I mean by that.",
  "translatedText": "",
  "from_community_srt": "제가 지금 보여드리는 신경망은 이미 숫자를 인식하도록 훈련되어 있습니다.",
  "n_reviews": 0,
  "start": 298.12,
  "end": 303.4
 },
 {
  "input": "It means if you feed in an image, lighting up all 784 neurons of the input layer according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer which causes some pattern in the one after it, which finally gives some pattern in the output layer.",
  "translatedText": "",
  "from_community_srt": "사진의 픽셀인 784개에 해당하는 입력 뉴런들을 모두 활성화 시킬 때, 이 때 활성화되는 뉴런들이 특정 패턴이 다음 층이 활성화 되게끔 합니다. 그 다음 열도 마찬가지로 활성화가 되고 마지막으로 출력 층에도 전달됩니다.",
  "n_reviews": 0,
  "start": 303.64,
  "end": 322.08
 },
 {
  "input": "And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents.",
  "translatedText": "",
  "from_community_srt": "출력 층에서 가장 빛나는 뉴런이 이 신경망에서 선택된 출력값입니다.",
  "n_reviews": 0,
  "start": 322.56,
  "end": 329.4
 },
 {
  "input": "And before jumping into the math for how one layer influences the next, or how training works, let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently.",
  "translatedText": "",
  "from_community_srt": "어떻게 한 층의 활성화가 다른 층의 활성화를 불러일으키는지에 대해 수학적인 접근을 하기 이전에 어떻게 이러한 이러한 구조가 지적으로 행동한다고 볼 수 있는 건지 생각해봅시다..",
  "n_reviews": 0,
  "start": 332.56,
  "end": 343.52
 },
 {
  "input": "What are we expecting here?",
  "translatedText": "",
  "from_community_srt": "우리는 지금 뭘 기대하고 있는 걸까요?",
  "n_reviews": 0,
  "start": 344.06,
  "end": 345.22
 },
 {
  "input": "What is the best hope for what those middle layers might be doing?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 345.4,
  "end": 347.6
 },
 {
  "input": "Well, when you or I recognize digits, we piece together various components.",
  "translatedText": "",
  "from_community_srt": "가운데의 층들은 무슨 역할을 하는 걸까요? 우리가 숫자를 인식할 때는 각 부분을 합칩니다.",
  "n_reviews": 0,
  "start": 348.92,
  "end": 353.52
 },
 {
  "input": "A 9 has a loop up top and a line on the right.",
  "translatedText": "",
  "from_community_srt": "9같은 경우 동그라미가 위에,",
  "n_reviews": 0,
  "start": 354.2,
  "end": 356.82
 },
 {
  "input": "An 8 also has a loop up top, but it's paired with another loop down low.",
  "translatedText": "",
  "from_community_srt": "직선이 오른쪽에 8은 동그라미가 똑같이 위에 있지만 아래에도 동그라미가 있습니다.",
  "n_reviews": 0,
  "start": 357.38,
  "end": 361.18
 },
 {
  "input": "A 4 basically breaks down into three specific lines, and things like that.",
  "translatedText": "",
  "from_community_srt": "4는 세개의 직선으로 이루어지며 이런 모양이 될 것입니다.",
  "n_reviews": 0,
  "start": 361.98,
  "end": 366.82
 },
 {
  "input": "Now in a perfect world, we might hope that each neuron in the second to last layer corresponds with one of these subcomponents, that anytime you feed in an image with, say, a loop up top, like a 9 or an 8, there's some specific neuron whose activation is going to be close to 1.",
  "translatedText": "",
  "from_community_srt": "이상적으로는 두 번째 층의 각 뉴런들이 이러한 '부분'들에 대해 대응하길 원하죠. 동그라미가 위에 있는 9나 8같은 숫자를 넣었을 때 이상적으로는 특정한 뉴런의 활성치가 1에 가까워질겁니다.",
  "n_reviews": 0,
  "start": 367.6,
  "end": 383.78
 },
 {
  "input": "And I don't mean this specific loop of pixels, the hope would be that any generally loopy pattern towards the top sets off this neuron.",
  "translatedText": "",
  "from_community_srt": "물론 동그라미가 들어가 있는 모든 경우에 이 신경이 활성화 되길 바라는 것은 아닙니다.",
  "n_reviews": 0,
  "start": 384.5,
  "end": 391.56
 },
 {
  "input": "That way, going from the third layer to the last one just requires learning which combination of subcomponents corresponds to which digits.",
  "translatedText": "",
  "from_community_srt": "이런식으로 하면 세 번째 층에서 마지막 층으로 갈 때에는 어떤 부분들의 결합이 어떤 숫자를 가르키는지만 보면 충분하겠죠.",
  "n_reviews": 0,
  "start": 392.44,
  "end": 400.04
 },
 {
  "input": "Of course, that just kicks the problem down the road, because how would you recognize these subcomponents, or even learn what the right subcomponents should be?",
  "translatedText": "",
  "from_community_srt": "물론 이러한 과정은 '어떻게 각각의 부분들을 어떻게 인지할 것인가?' 혹은 '어떠한 위치에 있어야 하는가?' 아직 한 층에서의 활성화가 어떻게 다음 층의 활성화로 진행하는 과정에 대해 말하지 않았지만,",
  "n_reviews": 0,
  "start": 401.0,
  "end": 407.64
 },
 {
  "input": "And I still haven't even talked about how one layer influences the next, but run with me on this one for a moment.",
  "translatedText": "",
  "from_community_srt": "이 문제에 대해 잠시 생각해봅시다.",
  "n_reviews": 0,
  "start": 408.06,
  "end": 413.06
 },
 {
  "input": "Recognizing a loop can also break down into subproblems.",
  "translatedText": "",
  "from_community_srt": "동그라미를 인식하는 것 또한 부가적인 질문들을 품게 합니다.",
  "n_reviews": 0,
  "start": 413.68,
  "end": 416.68
 },
 {
  "input": "One reasonable way to do this would be to first recognize the various little edges that make it up.",
  "translatedText": "",
  "from_community_srt": "한 가지 합리적인 방법은 여러개의 작은 부분으로 나누어서 인식하는 것입니다.",
  "n_reviews": 0,
  "start": 417.28,
  "end": 422.78
 },
 {
  "input": "Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7, is really just a long edge, or maybe you think of it as a certain pattern of several smaller edges.",
  "translatedText": "",
  "from_community_srt": "비슷한 방식으로 1,4,7에서 볼 수 있는 기다란 선은.. 이건 그냥 긴 선이라서 짧은 선 여러개로 쪼갤 수 있다고 생각할 수 있습니다.",
  "n_reviews": 0,
  "start": 423.78,
  "end": 434.32
 },
 {
  "input": "So maybe our hope is that each neuron in the second layer of the network corresponds with the various relevant little edges.",
  "translatedText": "",
  "from_community_srt": "그래서 어쩌면 두 번째 층에 위치한 뉴런들이 수 많은 자그마한 조각들에 대응된다고 생각할 수 있습니다.",
  "n_reviews": 0,
  "start": 435.14,
  "end": 442.72
 },
 {
  "input": "Maybe when an image like this one comes in, it lights up all of the neurons associated with around 8 to 10 specific little edges, which in turn lights up the neurons associated with the upper loop and a long vertical line, and those light up the neuron associated with a 9.",
  "translatedText": "",
  "from_community_srt": "예를 들어서 이런 사진이 들어왔을 때 이 그림과 관련되어 있는 8개에서 10개에 이르는 뉴런들을 활성화 시킨 다음에 위에는 동그라미, 밑에는 수직선과 관련된 뉴런을 활성화 시킵니다. 그리고 이렇게 활성화된 뉴런들은 9로 이어집니다.",
  "n_reviews": 0,
  "start": 443.54,
  "end": 459.72
 },
 {
  "input": "Whether or not this is what our final network actually does is another question, one that I'll come back to once we see how to train the network, but this is a hope that we might have, a sort of goal with the layered structure like this.",
  "translatedText": "",
  "from_community_srt": "최종적으로 이 신경망이 이렇게 행동하는지는 또다른 문제입니다. 일단 신경망을 어떻게 훈련시키는지에 대해 알아보겠습니다. 이렇게 층의 구조를 갖게 하는 것이 우리가 원하는 것이죠.",
  "n_reviews": 0,
  "start": 460.68,
  "end": 472.54
 },
 {
  "input": "Moreover, you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks.",
  "translatedText": "",
  "from_community_srt": "이러한 방식으로 일정한 형태나 테두리를 알아내는 것은 다른 이미지 처리 작업에서도 매우 유용하리라 생각할 수 있습니다.",
  "n_reviews": 0,
  "start": 473.16,
  "end": 480.3
 },
 {
  "input": "And even beyond image recognition, there are all sorts of intelligent things you might want to do that break down into layers of abstraction.",
  "translatedText": "",
  "from_community_srt": "이미지 처리 작업 이외에도, 이렇게 여러 추상화된 단계로 나눌 수 있 는 복잡한 것들은 더 있습니다.",
  "n_reviews": 0,
  "start": 480.88,
  "end": 487.28
 },
 {
  "input": "Parsing speech, for example, involves taking raw audio and picking out distinct sounds, which combine to make certain syllables, which combine to form words, which combine to make up phrases and more abstract thoughts, etc.",
  "translatedText": "",
  "from_community_srt": "예를 들면 음성 분석이 있지요. 음성 분석은 특정한 소리들을 합쳐 음절을 만들고, 음절을 합쳐 단어를 만들고, 단어를 합쳐 문장과 추상적인 생각들을 구성합니다.",
  "n_reviews": 0,
  "start": 488.04,
  "end": 500.06
 },
 {
  "input": "But getting back to how any of this actually works, picture yourself right now designing how exactly the activations in one layer might determine the next.",
  "translatedText": "",
  "from_community_srt": "다시 돌아와서 지금 당신이 이것이 어떻게 작동할 것인지 설계한다고 상상해봅시다. 한 층의 활성이 어떻게 다음 층에서의 정확한 활성을 이끌어 내는 걸까요? 목표는 픽셀을 테두리로 결합시키거나,",
  "n_reviews": 0,
  "start": 501.1,
  "end": 509.92
 },
 {
  "input": "The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits.",
  "translatedText": "",
  "from_community_srt": "테두리를 패턴으로 결합시키거나, 패턴을 숫자로 결합하는 매커니즘을 만드는 것입니다. 목표는 픽셀을 테두리로 결합시키거나, 테두리를 패턴으로 결합시키거나, 패턴을 숫자로 결합하는 매커니즘을 만드는 것입니다. 목표는 픽셀을 테두리로 결합시키거나, 테두리를 패턴으로 결합시키거나, 패턴을 숫자로 결합하는 매커니즘을 만드는 것입니다.",
  "n_reviews": 0,
  "start": 510.86,
  "end": 518.98
 },
 {
  "input": "And to zoom in on one very specific example, let's say the hope is for one particular neuron in the second layer to pick up on whether or not the image has an edge in this region here.",
  "translatedText": "",
  "from_community_srt": "이것의 매우 구체적인 예를 하나 들여다 봅시다. 우리가 원하는 것이 두 번째 층의 하나의 특정한 뉴런이 이미지가 외곽선이 있는지 없는지 판별하는 것이라고 합시다. 이부분에서 말이죠.",
  "n_reviews": 0,
  "start": 519.44,
  "end": 530.62
 },
 {
  "input": "The question at hand is what parameters should the network have?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 531.44,
  "end": 535.1
 },
 {
  "input": "What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern, or any other pixel pattern, or the pattern that several edges can make a loop, and other such things?",
  "translatedText": "",
  "from_community_srt": "여기서 문제는 과연 신경망이 어떤 변수들을 가지고 있어야 할까요? 또한 그런 판별을 하기 위해 무엇을 조정할 수 있어야 할까요? 이런패턴이나 아니면 다른 패턴이나 여러 테두리가 모여 고리를 만드는 산황 등등을 판별할 수 있을까요? 여러 테두리가 모여 고리를 만드는 상황 등등을 판별할 수 있을까요?",
  "n_reviews": 0,
  "start": 535.64,
  "end": 547.78
 },
 {
  "input": "Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer.",
  "translatedText": "",
  "from_community_srt": "여기서 우리는 첫 층의 뉴런과 현재 뉴런을 잇는 신경에 가중치를 부여할 것입니다. 여기서 우리는 첫 층의 뉴런과 현재 뉴런을 잇는 신경에 가중치를 부여할 것입니다. 여기서 우리는 첫 층의 뉴런과 현재 뉴런을 잇는 신경에 가중치를 부여할 것입니다.",
  "n_reviews": 0,
  "start": 548.72,
  "end": 555.56
 },
 {
  "input": "These weights are just numbers.",
  "translatedText": "",
  "from_community_srt": "가중치는 그냥 숫자로 생각해주세요.",
  "n_reviews": 0,
  "start": 556.32,
  "end": 557.7
 },
 {
  "input": "Then take all of those activations from the first layer and compute their weighted sum according to these weights.",
  "translatedText": "",
  "from_community_srt": "그리고 첫 층 뉴런에서의 모든 활성치를 가져와 각 신경의 가중치를 주고 모두 더합니다.",
  "n_reviews": 0,
  "start": 558.54,
  "end": 565.5
 },
 {
  "input": "I find it helpful to think of these weights as being organized into a little grid of their own, and I'm going to use green pixels to indicate positive weights, and red pixels to indicate negative weights, where the brightness of that pixel is some loose depiction of the weight's value.",
  "translatedText": "",
  "from_community_srt": "그런 가중치를 이렇게 평면상에 표현하면 더욱 이해가 쉬워집니다. 양수는 초록색 픽셀로, 음수는 빨강색 픽셀로 표현하겠습니다. 픽셀의 밝기는 뭔가 가중치의 느슨한 표현이라고 해야할까요? 이제 그 부분을 제외한 다른 모든 픽셀의 가중치를 0에 가깝게 만들고",
  "n_reviews": 0,
  "start": 567.7,
  "end": 581.78
 },
 {
  "input": "Now if we made the weights associated with almost all of the pixels zero except for some positive weights in this region that we care about, then taking the weighted sum of all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about.",
  "translatedText": "",
  "from_community_srt": "이제 그 부분을 제외한 다른 모든 픽셀의 가중치를 0에 가깝게 만들고 각 픽셀에 가중치를 준 값의 합을 구하면 그 영역의 픽셀에만 가중치를 주어 더한 것과 같은 상황이 됩니다.",
  "n_reviews": 0,
  "start": 582.78,
  "end": 597.82
 },
 {
  "input": "And if you really wanted to pick up on whether there's an edge here, what you might do is have some negative weights associated with the surrounding pixels.",
  "translatedText": "",
  "from_community_srt": "그리고 그 부분이 정말 테두리가 맞는지 확인하고 싶다면 주변의 픽셀에 음수 가중치를 주면 됩니다. 주변의 픽셀에 음수 가중치를 주면 됩니다.",
  "n_reviews": 0,
  "start": 599.14,
  "end": 606.6
 },
 {
  "input": "Then the sum is largest when those middle pixels are bright but the surrounding pixels are darker.",
  "translatedText": "",
  "from_community_srt": "그러면 주변의 픽셀이 어두우면서 중앙의 픽셀이 밝을 때 최대치를 얻을 수 있습니다. 그러면 주변의 픽셀이 어두우면서 중앙의 픽셀이 밝을 때 최대치를 얻을 수 있습니다.",
  "n_reviews": 0,
  "start": 607.48,
  "end": 612.7
 },
 {
  "input": "When you compute a weighted sum like this, you might come out with any number, but for this network what we want is for activations to be some value between 0 and 1.",
  "translatedText": "",
  "from_community_srt": "이처럼 가중치를 준 값의 합을 계산해 보면 어떤 값이라도 나올 수 있습니다. 하지만 우리가 이 신경망에서 원하는 건 0과 1 사이의 값이죠. 하지만 우리가 이 신경망에서 원하는 건 0과 1 사이의 값이죠.",
  "n_reviews": 0,
  "start": 614.26,
  "end": 623.54
 },
 {
  "input": "So a common thing to do is to pump this weighted sum into some function that squishes the real number line into the range between 0 and 1.",
  "translatedText": "",
  "from_community_srt": "따라서 이 가중치를 준 값의 합을 0과 1 사이의 숫자로 만들어 주는 함수에 넣을 겁니다. 따라서 이 가중치를 준 값의 합을 0과 1 사이의 숫자로 만들어 주는 함수에 넣을 겁니다. 따라서 이 가중치를 준 값의 합을 0과 1 사이의 숫자로 만들어 주는 함수에 넣을 겁니다.",
  "n_reviews": 0,
  "start": 624.12,
  "end": 632.14
 },
 {
  "input": "And a common function that does this is called the sigmoid function, also known as a logistic curve.",
  "translatedText": "",
  "from_community_srt": "이러한 함수로는 로지스틱 방정식으로도 알려진 시그모이드 함수가 잘 알려져 있습니다. 이러한 함수로는 로지스틱 방정식으로도 알려진 시그모이드 함수가 잘 알려져 있습니다.",
  "n_reviews": 0,
  "start": 632.46,
  "end": 637.42
 },
 {
  "input": "Basically very negative inputs end up close to 0, positive inputs end up close to 1, and it just steadily increases around the input 0.",
  "translatedText": "",
  "from_community_srt": "간단히 설명하면 매우 작은 음수는 0에 매우 가깝게 대응되고 매우 큰 양수는 1에 매우 가깝게 대응되며 0 주위에서는 계속 증가합니다.",
  "n_reviews": 0,
  "start": 638.0,
  "end": 646.6
 },
 {
  "input": "So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is.",
  "translatedText": "",
  "from_community_srt": "그래서 이 뉴런의 활성화는 기본적으로 관련있는 가중치의 합이 얼마나 더 양에 가까운지에 따라 정해져있습니다.",
  "n_reviews": 0,
  "start": 649.12,
  "end": 656.36
 },
 {
  "input": "But maybe it's not that you want the neuron to light up when the weighted sum is bigger than 0.",
  "translatedText": "",
  "from_community_srt": "하지만 당신은 가중치의 합이 0을 넘을때 뉴런이 활성화 되는 것을 원하는 게 아닐 수 있지요.",
  "n_reviews": 0,
  "start": 657.54,
  "end": 661.88
 },
 {
  "input": "Maybe you only want it to be active when the sum is bigger than say 10.",
  "translatedText": "",
  "from_community_srt": "예를 들어 합이 10보다 클 때 활성화 되기를 원할 수 도 있습니다.",
  "n_reviews": 0,
  "start": 662.28,
  "end": 666.36
 },
 {
  "input": "That is, you want some bias for it to be inactive.",
  "translatedText": "",
  "from_community_srt": "그것이 활성화되지 않기 위한 조건을 다는 것이죠.",
  "n_reviews": 0,
  "start": 666.84,
  "end": 670.26
 },
 {
  "input": "What we'll do then is just add in some other number like negative 10 to this weighted sum before plugging it through the sigmoid squishification function.",
  "translatedText": "",
  "from_community_srt": "그럴때는 이 가중치에  -10 처럼 다른 음의 숫자를 더해줍니다. 시그모이드 함수에 값을 집어넣기 전에 말이죠.",
  "n_reviews": 0,
  "start": 671.38,
  "end": 679.66
 },
 {
  "input": "That additional number is called the bias.",
  "translatedText": "",
  "from_community_srt": "그 더해지는 숫자를  Bias라고 합니다.",
  "n_reviews": 0,
  "start": 680.58,
  "end": 682.44
 },
 {
  "input": "So the weights tell you what pixel pattern this neuron in the second layer is picking up on, and the bias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active.",
  "translatedText": "",
  "from_community_srt": "가중치는 두번째 레이어가 선택하려는 뉴런의 픽셀 패턴을 알려주며 bias는 뉴런이 활성화되려면 가중치의 합이 얼마나 더 높아야 하는지를 알려줍니다.",
  "n_reviews": 0,
  "start": 683.46,
  "end": 695.18
 },
 {
  "input": "And that is just one neuron.",
  "translatedText": "",
  "from_community_srt": "지금까진 단 한 개의 뉴런에 대하여 이야기했습니다.",
  "n_reviews": 0,
  "start": 696.12,
  "end": 697.68
 },
 {
  "input": "Every other neuron in this layer is going to be connected to all 784 pixel neurons from the first layer, and each one of those 784 connections has its own weight associated with it.",
  "translatedText": "",
  "from_community_srt": "이 레이어의 모든 뉴런은 각각 첫번째 레이어의 784개 뉴런과  연결됩니다. 그리고 이 각각의 연결들은 각자의 가중치를 갖습니다.",
  "n_reviews": 0,
  "start": 698.28,
  "end": 710.94
 },
 {
  "input": "Also, each one has some bias, some other number that you add on to the weighted sum before squishing it with the sigmoid.",
  "translatedText": "",
  "from_community_srt": "또한 각각의 뉴런은 시그모이드 함수로 압축하기전에 가중치에 더한 값인 bias를 갖습니다.",
  "n_reviews": 0,
  "start": 711.6,
  "end": 717.6
 },
 {
  "input": "And that's a lot to think about!",
  "translatedText": "",
  "n_reviews": 0,
  "start": 718.11,
  "end": 719.54
 },
 {
  "input": "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights, along with 16 biases.",
  "translatedText": "",
  "from_community_srt": "이러한 16개 뉴런의 숨겨진 레이어는 각각의 16개의 bias를 가진 784 x 16개의 가중치를 의미합니다.",
  "n_reviews": 0,
  "start": 719.96,
  "end": 727.98
 },
 {
  "input": "And all of that is just the connections from the first layer to the second.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 728.84,
  "end": 731.94
 },
 {
  "input": "The connections between the other layers also have a bunch of weights and biases associated with them.",
  "translatedText": "",
  "from_community_srt": "그리고 이 수치는 첫번째 레이어와 두번째 레이어 사이의 연결은 그들과 연관된 가중치와 bias들로 이루어져있습니다.",
  "n_reviews": 0,
  "start": 732.52,
  "end": 737.34
 },
 {
  "input": "All said and done, this network has almost exactly 13,000 total weights and biases.",
  "translatedText": "",
  "from_community_srt": "이 연결은 거의 13,000개의 가중치와 bias를 갖습니다. 이 연결은 거의 13,000개의 가중치와 bias를 갖습니다.",
  "n_reviews": 0,
  "start": 738.34,
  "end": 743.8
 },
 {
  "input": "13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways.",
  "translatedText": "",
  "from_community_srt": "비틀고 변할수 있는 13,000개의 연결은 각각 다른 방법으로 행동합니다.",
  "n_reviews": 0,
  "start": 743.8,
  "end": 749.96
 },
 {
  "input": "So when we talk about learning, what that's referring to is getting the computer to find a valid setting for all of these many many numbers so that it'll actually solve the problem at hand.",
  "translatedText": "",
  "from_community_srt": "'배움' 에 대해서 얘기하자면 컴퓨터가 실제로 해당 문제를 스스로 해결하기 위해서 수많은 수치들을 찾기 위한 알맞은 환경을 얻는다는 것을 의미합니다.",
  "n_reviews": 0,
  "start": 751.04,
  "end": 761.36
 },
 {
  "input": "One thought experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand, purposefully tweaking the numbers so that the second layer picks up on edges, the third layer picks up on patterns, etc.",
  "translatedText": "",
  "from_community_srt": "재밌기도 하고 조금 무섭기도 한 사고 실험을 해봅시다. 한번 상상해 보세요. 이 가중치와 bias들을 일일히 직접 설정하는 겁니다. 의도적으로 두번째 층은 모서리를 인식하고 세번째 층은 패턴을 인식하고 등을 할 수 있게 수를 수정하는 겁니다.",
  "n_reviews": 0,
  "start": 762.62,
  "end": 776.58
 },
 {
  "input": "I personally find this satisfying rather than just treating the network as a total black box, because when the network doesn't perform the way you anticipate, if you've built up a little bit of a relationship with what those weights and biases actually mean, you have a starting place for experimenting with how to change the structure to improve.",
  "translatedText": "",
  "from_community_srt": "개인적으로 네트워크를 그냥 블랙 박스로 이해하는 것보다 이 편이 낫다고 생각합니다. 네트워크가 예상했던 것처럼 작동하지 않을 때 가중치와 bias가 실제로는 무엇을 의미할지 조금이나마 생각해 두는 것이 구조를 어떻게 바꿔야 개선시킬 수 있을지 생각해보는 시작점이 될 수 있기 때문입니다.",
  "n_reviews": 0,
  "start": 776.98,
  "end": 794.18
 },
 {
  "input": "Or when the network does work but not for the reasons you might expect, digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible solutions.",
  "translatedText": "",
  "from_community_srt": "아니면 네트워크가 작동하기는 하지만 예상했던 이유 때문이 아니라면 가중치와 bias가 어떤 것인지 파고드는 것이 세워 두었던 가정을 시험하고 무엇이 가능한지를 확실히 시험해볼 수 있게 합니다.",
  "n_reviews": 0,
  "start": 794.96,
  "end": 805.82
 },
 {
  "input": "By the way, the actual function here is a little cumbersome to write down, don't you think?",
  "translatedText": "",
  "from_community_srt": "그건 그렇고,",
  "n_reviews": 0,
  "start": 806.84,
  "end": 810.68
 },
 {
  "input": "So let me show you a more notationally compact way that these connections are represented.",
  "translatedText": "",
  "from_community_srt": "이 함수가 좀 적기 복잡하지 않습니까? 이 연결을 표현할 수 있는 훨신 간결한 방법을 보여드리겠습니다.",
  "n_reviews": 0,
  "start": 812.5,
  "end": 817.14
 },
 {
  "input": "This is how you'd see it if you choose to read up more about neural networks.",
  "translatedText": "",
  "from_community_srt": "신경망에 대해 더 알아보려고 하면 보게 될 방식입니다.",
  "n_reviews": 0,
  "start": 817.66,
  "end": 820.52
 },
 {
  "input": "Organize all of the activations from one layer into a column as a vector.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 821.38,
  "end": 820.52
 },
 {
  "input": "Then organize all of the weights as a matrix, where each row of that matrix corresponds to the connections between one layer and a particular neuron in the next layer.",
  "translatedText": "",
  "from_community_srt": "한 층이 활성화되는 정도를 열벡터로 나타냅니다. 그리고 가중치를 모두 모아 행렬로 나타냅니다. 행렬의 각 열은 한 층과 다음 층의 특정 뉴런의 연결을 나타냅니다.",
  "n_reviews": 0,
  "start": 821.38,
  "end": 838.0
 },
 {
  "input": "What that means is that taking the weighted sum of the activations in the first layer according to these weights corresponds to one of the terms in the matrix vector product of everything we have on the left here.",
  "translatedText": "",
  "from_community_srt": "이 것은 가중치들에 따라 활성화된 정도를 더한 것이 행렬 벡터곱을 하여 나오는 열벡터의 각 원소에 대응한다는 것을 의미합니다.",
  "n_reviews": 0,
  "start": 838.54,
  "end": 849.88
 },
 {
  "input": "By the way, so much of machine learning just comes down to having a good grasp of linear algebra, so for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means, take a look at the series I did on linear algebra, especially chapter 3.",
  "translatedText": "",
  "from_community_srt": "그나저나, 기계 학습의 대부분엔 선형 대수학을 잘 이해하는 것이 필요합니다. 따라서 행렬과 행렬 벡터곱이 무엇을 의미하는지를 시각적으로 이해하고 싶은 사람은 제가 만든 선형 대수학 시리즈, 특히 3장을 찾아 보세요.",
  "n_reviews": 0,
  "start": 854.0,
  "end": 868.6
 },
 {
  "input": "Back to our expression, instead of talking about adding the bias to each one of these values independently, we represent it by organizing all those biases into a vector, and adding the entire vector to the previous matrix vector product.",
  "translatedText": "",
  "from_community_srt": "설명으로 돌아가서, bias를 독립적으로 각각의 값에 더하는 것으로 표현하는 대신 bias를 전부 모아서 열벡터로 만들고 벡터 전체를 아까 전의 행렬 벡터 곱에 더하는 것으로 표현합니다.",
  "n_reviews": 0,
  "start": 869.24,
  "end": 882.3
 },
 {
  "input": "Then as a final step, I'll wrap a sigmoid around the outside here, and what that's supposed to represent is that you're going to apply the sigmoid function to each specific component of the resulting vector inside.",
  "translatedText": "",
  "from_community_srt": "마지막으로 시그모이드를 이 바깥에 감싸줍니다. 이러면 시그모이드 함수를 결과 벡터 내의 원소들에 각각 적용함을 의미합니다.",
  "n_reviews": 0,
  "start": 883.28,
  "end": 894.74
 },
 {
  "input": "So once you write down this weight matrix and these vectors as their own symbols, you can communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression, and this makes the relevant code both a lot simpler and a lot faster, since many libraries optimize the heck out of matrix multiplication.",
  "translatedText": "",
  "from_community_srt": "그래서, 가중치 행렬과 벡터들을 각각의 기호로 나타내면 활성화된 정도가 한 층에서 다른 층으로 어떻게 전달되는지를 아주 짧고 간단한 식으로 나타낼 수 있게 됩니다. 많은 라이브러리들이 행렬곱을 최적화하므로 관련 코드를 훨씬 쉽고 빠르게 만들 수 있습니다.",
  "n_reviews": 0,
  "start": 895.94,
  "end": 915.66
 },
 {
  "input": "Remember how earlier I said these neurons are simply things that hold numbers?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 917.82,
  "end": 921.46
 },
 {
  "input": "Well of course the specific numbers that they hold depends on the image you feed in, so it's actually more accurate to think of each neuron as a function, one that takes in the outputs of all the neurons in the previous layer and spits out a number between 0 and 1.",
  "translatedText": "",
  "from_community_srt": "앞에서 뉴런이 숫자를 보관하는 것이라고 간단하게 말한 것이 기억나나요? 당연히도 뉴런이 보관한 숫자들은 입력한 이미지에 따라 결정됩니다. 따라서 뉴런을 이전 층의 뉴런의 출력을 모두 받아서 0과 1 사이의 수를 만들어내는 함수라고 생각하는 것이 더 정확합니다.",
  "n_reviews": 0,
  "start": 922.22,
  "end": 938.34
 },
 {
  "input": "Really the entire network is just a function, one that takes in 784 numbers as an input and spits out 10 numbers as an output.",
  "translatedText": "",
  "from_community_srt": "사실 네트워크 전체도 784개의 수를 입력 받아서 10개의 수를 출력하는 함수입니다.",
  "n_reviews": 0,
  "start": 939.2,
  "end": 947.06
 },
 {
  "input": "It's an absurdly complicated function, one that involves 13,000 parameters in the forms of these weights and biases that pick up on certain patterns, and which involves iterating many matrix vector products and the sigmoid squishification function, but it's just a function nonetheless.",
  "translatedText": "",
  "from_community_srt": "이 터무니 없이 복잡한 함수는 특정한 패턴을 인식하기 위해 13,000여개의 가중치나 bias형태의 매개변수로 수많은 벡터 행렬 곱과 시그모이드 압축을 반복합니다.",
  "n_reviews": 0,
  "start": 947.56,
  "end": 962.64
 },
 {
  "input": "And in a way it's kind of reassuring that it looks complicated.",
  "translatedText": "",
  "from_community_srt": "그럼에도 불구하고 그것은 단지 함수에 불과하며 어떤 면에서는 복잡해서 다행이기도 합니다.",
  "n_reviews": 0,
  "start": 963.4,
  "end": 966.66
 },
 {
  "input": "I mean if it were any simpler, what hope would we have that it could take on the challenge of recognizing digits?",
  "translatedText": "",
  "from_community_srt": "내 말은 만약 그것이 단순한 함수였다면, 우리가 이 숫자들을 이해하려는 도전에 정면으로 맞서보려고나 했을까요?",
  "n_reviews": 0,
  "start": 967.34,
  "end": 972.28
 },
 {
  "input": "And how does it take on that challenge?",
  "translatedText": "",
  "n_reviews": 0,
  "start": 973.34,
  "end": 974.7
 },
 {
  "input": "How does this network learn the appropriate weights and biases just by looking at data?",
  "translatedText": "",
  "from_community_srt": "그런데 어떻게 그 도전에 맞설까요? 이 네트워크는 어떻게 데이터를 보는 것만으로 적절한 가중치와 bias들을 배우는걸까요?",
  "n_reviews": 0,
  "start": 975.08,
  "end": 979.36
 },
 {
  "input": "Well that's what I'll show in the next video, and I'll also dig a little more into what this particular network we're seeing is really doing.",
  "translatedText": "",
  "from_community_srt": "그것이 바로 제가 다음 비디오에서 알려드릴 내용입니다. 그리고 우리가 보고 있는 이 특정 네트워크가 실제로 하고 있는 일을 좀 더 깊이 파헤칠 겁니다.",
  "n_reviews": 0,
  "start": 980.14,
  "end": 986.12
 },
 {
  "input": "Now is the point I suppose I should say subscribe to stay notified about when that video or any new videos come out, but realistically most of you don't actually receive notifications from YouTube, do you?",
  "translatedText": "",
  "from_community_srt": "이제 그 비디오 또는 새로운 비디오가 언제 나올지에 대한  알림을 받기 위해 구독을 부탁할 타이밍입니다. 하지만 현실적으로 대부분의 사람들은 실제로  YouTube로부터 알림을 받지 못하고 있습니다.",
  "n_reviews": 0,
  "start": 987.58,
  "end": 997.42
 },
 {
  "input": "Maybe more honestly I should say subscribe so that the neural networks that underlie YouTube's recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you.",
  "translatedText": "",
  "from_community_srt": "YouTube의 추천 알고리즘의 기반이 되는 신경망은 구독자들이 채널의 콘텐츠를 보길 원한다고 생각하기 때문에 구독을 부탁하고 싶네요.",
  "n_reviews": 0,
  "start": 998.02,
  "end": 1007.88
 },
 {
  "input": "Anyway, stay posted for more.",
  "translatedText": "",
  "from_community_srt": "어쨌든 또 다른 영상을 지켜봐주세요.",
  "n_reviews": 0,
  "start": 1008.56,
  "end": 1009.94
 },
 {
  "input": "Thank you very much to everyone supporting these videos on Patreon.",
  "translatedText": "",
  "from_community_srt": "Patreon을 통해 영상들을 후원해주신 분들께 감사드립니다.",
  "n_reviews": 0,
  "start": 1010.76,
  "end": 1013.5
 },
 {
  "input": "I've been a little slow to progress in the probability series this summer, but I'm jumping back into it after this project, so patrons you can look out for updates there.",
  "translatedText": "",
  "from_community_srt": "올 여름엔 확률 편이 조금 더디게 진행됐군요. 이 프로젝트를 마치고 다시 진행한다면 곧 찾아 보실 수 있을 겁니다.",
  "n_reviews": 0,
  "start": 1014.0,
  "end": 1021.9
 },
 {
  "input": "To close things off here I have with me Lisha Li who did her PhD work on the theoretical side of deep learning and who currently works at a venture capital firm called Amplify Partners who kindly provided some of the funding for this video.",
  "translatedText": "",
  "from_community_srt": "마치며 저는 지금 Lisha Li와 함께 있는데요 Lisha Li는 딥러닝의 이론적인 측면으로 박사 학위를 받았고 현재 Amplify Partners라는 벤처캐피탈 회사에서 일하고 있으며 Amplify Partners는 이 비디오 제작을 후원해주신 고마운 곳입니다.",
  "n_reviews": 0,
  "start": 1023.6,
  "end": 1034.62
 },
 {
  "input": "So Lisha one thing I think we should quickly bring up is this sigmoid function.",
  "translatedText": "",
  "from_community_srt": "그래서 Lisha,",
  "n_reviews": 0,
  "start": 1035.46,
  "end": 1039.12
 },
 {
  "input": "As I understand it early networks use this to squish the relevant weighted sum into that interval between zero and one, you know kind of motivated by this biological analogy of neurons either being inactive or active.",
  "translatedText": "",
  "from_community_srt": "저는 시그모이드 함수에 대해서 얘기를 해보고 싶은데 제가 이해하기론 초기 네트워크는 시그모이드 함수를 가중치가 적용된 합들을 0부터 1사이의 값으로 압축하기 위해 사용했고 이건 뉴런이 활성화 되거나 비활성화 되는 생물학적인 현상을 모방한거죠 그렇죠.",
  "n_reviews": 0,
  "start": 1039.7,
  "end": 1049.84
 },
 {
  "input": "Exactly.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1050.28,
  "end": 1050.3
 },
 {
  "input": "But relatively few modern networks actually use sigmoid anymore.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1050.56,
  "end": 1054.04
 },
 {
  "input": "Yeah.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1054.32,
  "end": 1054.32
 },
 {
  "input": "It's kind of old school right?",
  "translatedText": "",
  "from_community_srt": "그런데 현대의 네트워크들은 시그모이드 함수를 이제 잘 안써요 이건 이제 좀 구식이잖아요? 네 대신에 ReLU함수가 더 훈련시키기 쉽죠.",
  "n_reviews": 0,
  "start": 1054.44,
  "end": 1055.54
 },
 {
  "input": "Yeah or rather ReLU seems to be much easier to train.",
  "translatedText": "",
  "n_reviews": 0,
  "start": 1055.76,
  "end": 1058.98
 },
 {
  "input": "And ReLU, ReLU stands for rectified linear unit?",
  "translatedText": "",
  "from_community_srt": "ReLU는 \"Rectified Linear Unit\"(선형 정류 유닛)이고요.",
  "n_reviews": 0,
  "start": 1059.4,
  "end": 1062.34
 },
 {
  "input": "Yes it's this kind of function where you're just taking a max of zero and a where a is given by what you were explaining in the video and what this was sort of motivated from I think was a partially by a biological analogy with how neurons would either be activated or not.",
  "translatedText": "",
  "from_community_srt": "네 ReLU는 그냥 0과 a에 max함수를 취한건데 a는 뉴런의 활성치를 나타내는 함수임을 이 비디오에서 설명했습니다. 이건 뉴런이 생물학적으로 어떻게 활성화되고 비활성화 되는지를 모방한 것과 같아서 임계값을 넘기면 항등함수를 출력하고 임계값을 넘기지 못하면 0을 출력합니다.",
  "n_reviews": 0,
  "start": 1062.68,
  "end": 1081.36
 },
 {
  "input": "And so if it passes a certain threshold it would be the identity function but if it did not then it would just not be activated so it'd be zero so it's kind of a simplification.",
  "translatedText": "",
  "from_community_srt": "임계값을 넘기지 못하면 0을 출력합니다. 단순화된 버전같은거죠.",
  "n_reviews": 0,
  "start": 1081.36,
  "end": 1090.84
 },
 {
  "input": "Using sigmoids didn't help training or it was very difficult to train at some point and people just tried ReLU and it happened to work very well for these incredibly deep neural networks.",
  "translatedText": "",
  "from_community_srt": "시그모이드 함수로는 신경망 훈련이 잘 되지 않아요 아니면 훈련 시키기 아주 어려운 걸 수도 있죠 그래서 누군가가 ReLU로 시도해 봤는데 이런 아주 깊은 신경망(Deep neural networks)에서 아주 잘 작동했던거죠.",
  "n_reviews": 0,
  "start": 1091.16,
  "end": 1104.62
 },
 {
  "input": "All right thank you Lisha.",
  "translatedText": "",
  "from_community_srt": "그렇군요 고마워요 Lisha",
  "n_reviews": 0,
  "start": 1105.1,
  "end": 1105.64
 }
]