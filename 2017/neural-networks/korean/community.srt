1
00:00:04,100 --> 00:00:07,560
여기에 숫자 3이 있습니다. 다소 삐뚤삐뚤하게 쓰였고,

2
00:00:07,560 --> 00:00:10,840
28x28 픽셀 밖에 안되는 저해상도로  기록되어 있지만,

3
00:00:10,840 --> 00:00:14,400
여러분의 뇌는 이 이미지를 아주 간단히
 3이라고 인식합니다.

4
00:00:14,400 --> 00:00:19,440
뇌는 대체 어떻게 이런 어마어마한 일을 간단히 해내는 것일까요?

5
00:00:19,460 --> 00:00:23,100
그러니까, 여러분의 뇌는 오른쪽의 이 그림들도 전부 3이라고 인식해냅니다.

6
00:00:23,100 --> 00:00:28,820
각 픽셀들의 밝기가 이미지마다 서로 다른데도 말이죠.

7
00:00:28,820 --> 00:00:32,960
여러분의 눈 안에 있는 시지각 세포들이 빛 신호를 받아들이는 패턴은,

8
00:00:32,960 --> 00:00:35,040
이 그림을 볼 때와,

9
00:00:35,040 --> 00:00:37,260
이 그림을 볼 때 서로 다른 양상을 가집니다.

10
00:00:37,260 --> 00:00:41,160
그러나,  여러분의 뇌에 있는 시각피질(visual cortex)은 이러한 차이에도 불구하고

11
00:00:41,160 --> 00:00:43,940
두 이미지가 같은 개념을 가리킨다고 판단합니다.

12
00:00:43,940 --> 00:00:49,180
동시에, 다른 개념을 가리키는 이미지들도 구별해내지요.

13
00:00:49,180 --> 00:00:56,100
하지만 만약, 제가 여러분에게 이 그림과 같이 28x28개의 입력값들을 받아

14
00:00:56,100 --> 00:01:02,200
0에서 9 범위의 정수 값 하나를 내놓는 프로그램을 짜 보라고 한다면 어떨까요.

15
00:01:02,200 --> 00:01:06,740
문제가 갑자기 어려워집니다.

16
00:01:06,740 --> 00:01:09,040
여러분이 동굴 속에서 살던 분이 아니라면,

17
00:01:09,040 --> 00:01:15,200
굳이 현재부터 미래까지의 기계학습(Machine Learning)과 신경망(Neural Network)의 중요성을 다시 강조할 필요는 없을 듯 합니다.

18
00:01:15,200 --> 00:01:19,560
여기에서는 여러분이 배경지식이 없다고 간주하고 신경망이 구체적으로 무엇인지를 알아보고,

19
00:01:19,560 --> 00:01:24,500
유행어가 아닌, 수학으로서 신경망이 도대체 무엇인지를 보여드릴 것입니다.

20
00:01:24,580 --> 00:01:28,960
이 동영상을 통해 여러분이 신경망 구조가 만들어진 토대를 깨닫고

21
00:01:29,100 --> 00:01:34,740
그 구조 속에서 "학습"이 어떤 의미를 갖는지
알게 되었으면 좋겠습니다.

22
00:01:35,040 --> 00:01:38,320
이 영상에서는 신경망의 구조에 대해 먼저 다룰 것입니다.

23
00:01:38,320 --> 00:01:40,300
학습은 다음 영상에서 다룰거고요.

24
00:01:40,700 --> 00:01:46,200
이제, 필기체 숫자 이미지에서 숫자 인식을
학습할 수 있는신경망의 구조를 알아보겠습니다.

25
00:01:49,140 --> 00:01:52,420
이것은 신경망이론을 소개하는 고전적 예시중 하나입니다.

26
00:01:52,500 --> 00:01:57,820
이 영상과 다음 영상의 말미에 인공신경망에 대한통찰력을 기를 수 있는 자료가 포함되어 있는 웹페이지의 링크를 드릴 것입니다.

27
00:01:58,040 --> 00:02:03,380
예제에 사용된 코드 또한 여러분에게 제공됩니다.

28
00:02:04,900 --> 00:02:08,200
신경망에는 정말 다양한 종류가 있습니다.

29
00:02:08,200 --> 00:02:12,580
최근에 많은 개발이 이루어지고 있기도
 하지요.

30
00:02:12,580 --> 00:02:19,080
그렇지만, 이번 영상에서는 제일 기본적인 형태의 신경망을 집중적으로 다룰 것입니다.

31
00:02:19,080 --> 00:02:24,820
이것들을 잘 이해해야 비로소 더 강력하고 복잡한 신경망들을 이해할 수 있을 것입니다.

32
00:02:24,820 --> 00:02:29,040
사실, 기본 형태도 꽤 복잡합니다.

33
00:02:29,040 --> 00:02:33,160
중요한건, 기본 신경망으로도 숫자 손글씨를 인식하는데는 충분하다는 겁니다.

34
00:02:33,160 --> 00:02:36,620
사람이 아닌 컴퓨터가 말이지요. 정말 놀라운 일입니다.

35
00:02:37,220 --> 00:02:42,040
동시에, 여러분은 컴퓨터가 우리의 기대를 저버리는 일도 볼지 모르겠습니다.

36
00:02:43,260 --> 00:02:47,020
이름에서 알 수 있는 것처럼, 신경망이라는 아이디어는 사람의 뇌에서 착안한 것입니다.

37
00:02:47,020 --> 00:02:52,340
뇌의 어떠한 점을 본딴 걸까요? 
뇌와 신경망이 어떤 유사점을 가지는 걸까요?

38
00:02:52,340 --> 00:02:58,000
먼저, 뉴런에 대해 알아봅시다. 뉴런은 하나의 숫자를 담는다는 사실을 떠올리는 것으로 충분합니다.

39
00:02:58,000 --> 00:03:03,680
0.0에서 1.0까지의 숫자만요. 그 뿐입니다.

40
00:03:03,700 --> 00:03:11,640
예를 들어, 이미지의 28x28개 픽셀들을
 입력값으로 취하는 신경망을 생각해봅시다.

41
00:03:11,640 --> 00:03:20,840
총 784개입니다. 각 뉴런들은 각 픽셀의 밝기를 나타냅니다.

42
00:03:20,840 --> 00:03:25,220
검은 픽셀은 0.0, 하얀 픽셀은 1.0에 해당합니다.

43
00:03:25,300 --> 00:03:28,780
신경망 안에서 이러한 숫자들은 입력값이라고 불립니다.

44
00:03:28,800 --> 00:03:33,860
큰 입력값이 주어질수록 각각의 신경망이 더 큰 정도로 활성화 됩니다.

45
00:03:36,360 --> 00:03:43,300
이 모든 784개의 뉴런이 신경망의 입력층을 구성하게 됩니다.

46
00:03:46,400 --> 00:03:51,500
출력층은 총 10개의 뉴런을 가지고 있습니다.
각각의 뉴런은 0 부터 9 까지의 숫자를 대표하는데요,

47
00:03:51,900 --> 00:03:57,000
이 뉴런들은 0과 1사이의 어떤 값을 취하고. 그 값은 뉴런이 대표하는 숫자와 입력값이 일치하는 정도를 나타냅니다.

48
00:03:57,500 --> 00:04:02,700
그 값은 주어진 입력값과
 각 뉴런이 대표하는 숫자 사이의 일치 정도를 나타냅니다.

49
00:04:02,920 --> 00:04:06,460
입력층과 출력층 사이에는 숨겨진 층이라고 불리는 몇 개의 층들이 있는데요,

50
00:04:06,460 --> 00:04:14,100
지금 당장은 이 층이 어떻게 숫자를 인식할 수 있는지 중요하지 않으므로 물음표 표시만 해놓고 넘어가도록 합시다.

51
00:04:14,100 --> 00:04:17,760
이 신경망에선 각각 16개의 뉴런을 가진 두 개의 숨겨진 층을 사용할건데

52
00:04:17,760 --> 00:04:20,880
솔직히 그냥 아무 숫자나 부른겁니다

53
00:04:20,880 --> 00:04:24,920
두 개의 층을 선택한 이유는 조금 있다 이 구조를 구성할 방법 때문이고

54
00:04:24,920 --> 00:04:28,500
그리고 16개는... 그냥 화면에 잘 들어가서 넣은 겁니다.

55
00:04:28,740 --> 00:04:32,340
같은 역할을 하는 신경망의 형태는 더 많이 있습니다.

56
00:04:32,880 --> 00:04:38,360
이 신경망은 기본적으로한 층에서의 활성화가 다음 층의 활성화를 유도하는 방식으로 작동합니다.

57
00:04:38,900 --> 00:04:42,960
정보 처리에 있어서의 신경망의 가장 중요한 점은

58
00:04:43,380 --> 00:04:48,560
도대체 어떻게 한 층에서의 활성화가 다른 층의 활성화를 불러일으키는지에 관한 점입니다.

59
00:04:49,120 --> 00:04:53,020
이러한 과정은
생물의 뉴런이 작동하는 방식과도 닮아있는데,

60
00:04:53,560 --> 00:04:57,260
몇몇 뉴런의 활성화가
다른 뉴런의 활성화를 수반한다는 점이죠.

61
00:04:58,020 --> 00:05:01,720
제가 지금 보여드리는 신경망은
이미 숫자를 인식하도록 훈련되어 있습니다.

62
00:05:01,800 --> 00:05:12,400
사진의 픽셀인 784개에 해당하는 입력 뉴런들을
모두 활성화 시킬 때,

63
00:05:12,400 --> 00:05:17,240
이 때 활성화되는 뉴런들이 특정 패턴이 다음 층이 활성화 되게끔 합니다.

64
00:05:17,280 --> 00:05:20,320
그 다음 열도 마찬가지로 활성화가 되고

65
00:05:20,320 --> 00:05:22,480
마지막으로 출력 층에도 전달됩니다.

66
00:05:22,720 --> 00:05:29,160
출력 층에서 가장 빛나는 뉴런이 이 신경망에서 선택된 출력값입니다.

67
00:05:32,240 --> 00:05:36,980
어떻게 한 층의 활성화가 다른 층의 활성화를 불러일으키는지에 대해 수학적인 접근을 하기 이전에

68
00:05:36,980 --> 00:05:43,440
어떻게 이러한 이러한 구조가 지적으로 행동한다고 볼 수 있는 건지 생각해봅시다..

69
00:05:43,440 --> 00:05:48,460
우리는 지금 뭘 기대하고 있는 걸까요? 가운데의 층들은 무슨 역할을 하는 걸까요?

70
00:05:48,960 --> 00:05:53,620
우리가 숫자를 인식할 때는 각 부분을 합칩니다.

71
00:05:53,860 --> 00:05:57,300
9같은 경우 동그라미가 위에, 직선이 오른쪽에

72
00:05:57,440 --> 00:06:01,320
8은 동그라미가 똑같이 위에 있지만 아래에도 동그라미가 있습니다.

73
00:06:01,680 --> 00:06:06,960
4는 세개의 직선으로 이루어지며 이런 모양이 될 것입니다.

74
00:06:07,440 --> 00:06:14,640
이상적으로는 두 번째 층의 각 뉴런들이 이러한 '부분'들에 대해 대응하길 원하죠.

75
00:06:15,500 --> 00:06:20,180
동그라미가 위에 있는 9나 8같은 숫자를 넣었을 때

76
00:06:20,180 --> 00:06:24,020
이상적으로는 특정한 뉴런의 활성치가 1에 가까워질겁니다.

77
00:06:24,400 --> 00:06:31,520
물론 동그라미가 들어가 있는 모든 경우에 이 신경이 활성화 되길 바라는 것은 아닙니다.

78
00:06:32,140 --> 00:06:40,140
이런식으로 하면 세 번째 층에서 마지막 층으로 갈 때에는 어떤 부분들의 결합이 어떤 숫자를 가르키는지만 보면 충분하겠죠.

79
00:06:41,040 --> 00:06:47,840
물론 이러한 과정은 '어떻게 각각의 부분들을 어떻게 인지할 것인가?' 혹은 '어떠한 위치에 있어야 하는가?'

80
00:06:48,620 --> 00:06:51,220
아직 한 층에서의 활성화가 어떻게 다음 층의 활성화로 진행하는 과정에 대해 말하지 않았지만,

81
00:06:51,420 --> 00:06:53,220
이 문제에 대해 잠시 생각해봅시다.

82
00:06:53,760 --> 00:06:56,940
동그라미를 인식하는 것 또한 부가적인 질문들을 품게 합니다.

83
00:06:57,080 --> 00:07:02,700
한 가지 합리적인 방법은 여러개의 작은 부분으로 나누어서 인식하는 것입니다.

84
00:07:03,320 --> 00:07:08,600
비슷한 방식으로 1,4,7에서 볼 수 있는 기다란 선은..

85
00:07:09,000 --> 00:07:14,480
이건 그냥 긴 선이라서 짧은 선 여러개로 쪼갤 수 있다고 생각할 수 있습니다.

86
00:07:15,300 --> 00:07:19,800
그래서 어쩌면 두 번째 층에 위치한 뉴런들이

87
00:07:19,880 --> 00:07:23,100
수 많은 자그마한 조각들에 대응된다고 생각할 수 있습니다.

88
00:07:23,480 --> 00:07:26,420
예를 들어서 이런 사진이 들어왔을 때

89
00:07:26,540 --> 00:07:32,020
이 그림과 관련되어 있는 8개에서 10개에 이르는 뉴런들을 활성화 시킨 다음에

90
00:07:32,180 --> 00:07:36,820
위에는 동그라미, 밑에는 수직선과 관련된 뉴런을 활성화 시킵니다.

91
00:07:37,060 --> 00:07:40,020
그리고 이렇게 활성화된 뉴런들은 9로 이어집니다.

92
00:07:40,460 --> 00:07:44,400
최종적으로 이 신경망이 이렇게 행동하는지는 또다른 문제입니다.

93
00:07:44,740 --> 00:07:47,180
일단 신경망을 어떻게
훈련시키는지에 대해 알아보겠습니다.

94
00:07:47,900 --> 00:07:52,460
이렇게 층의 구조를 갖게 하는 것이 우리가 원하는 것이죠.

95
00:07:53,120 --> 00:07:57,000
이러한 방식으로
일정한 형태나 테두리를 알아내는 것은

96
00:07:57,000 --> 00:08:00,400
다른 이미지 처리 작업에서도
매우 유용하리라 생각할 수 있습니다.

97
00:08:00,900 --> 00:08:07,500
이미지 처리 작업 이외에도, 이렇게 여러 추상화된 
단계로 나눌 수 있 는 복잡한 것들은 더 있습니다.

98
00:08:07,900 --> 00:08:09,300
예를 들면 음성 분석이 있지요.

99
00:08:09,500 --> 00:08:14,095
음성 분석은 특정한 소리들을 합쳐 음절을 만들고,

100
00:08:14,100 --> 00:08:20,060
음절을 합쳐 단어를 만들고, 단어를 합쳐 문장과 추상적인 생각들을 구성합니다.

101
00:08:20,520 --> 00:08:26,680
다시 돌아와서 지금 당신이 이것이 어떻게 작동할 것인지 설계한다고 상상해봅시다.

102
00:08:26,740 --> 00:08:31,380
한 층의 활성이 어떻게 다음 층에서의
정확한 활성을 이끌어 내는 걸까요?

103
00:08:31,385 --> 00:08:34,520
목표는 픽셀을 테두리로 결합시키거나, 테두리를 패턴으로 결합시키거나, 패턴을 숫자로 결합하는 매커니즘을 만드는 것입니다.

104
00:08:34,520 --> 00:08:37,400
목표는 픽셀을 테두리로 결합시키거나, 테두리를 패턴으로 결합시키거나, 패턴을 숫자로 결합하는 매커니즘을 만드는 것입니다.

105
00:08:37,400 --> 00:08:38,980
목표는 픽셀을 테두리로 결합시키거나, 테두리를 패턴으로 결합시키거나, 패턴을 숫자로 결합하는 매커니즘을 만드는 것입니다.

106
00:08:39,380 --> 00:08:42,580
이것의 매우 구체적인 예를 하나 들여다 봅시다.

107
00:08:43,060 --> 00:08:46,580
우리가 원하는 것이 두 번째 층의 하나의 특정한 뉴런이

108
00:08:46,585 --> 00:08:49,565
이미지가 외곽선이 있는지 없는지 판별하는 것이라고 합시다.

109
00:08:49,565 --> 00:08:50,745
이부분에서 말이죠.

110
00:08:51,100 --> 00:08:52,900
여기서 문제는

111
00:08:53,300 --> 00:08:56,120
과연 신경망이 어떤 변수들을 가지고 있어야 할까요?

112
00:08:56,260 --> 00:09:01,080
또한 그런 판별을 하기 위해 무엇을 조정할 수 있어야 할까요?

113
00:09:01,640 --> 00:09:04,540
이런패턴이나 아니면 다른 패턴이나

114
00:09:04,780 --> 00:09:05,715
여러 테두리가 모여 고리를 만드는 산황 등등을 판별할 수 있을까요?

115
00:09:05,715 --> 00:09:08,035
여러 테두리가 모여 고리를 만드는 상황 등등을 판별할 수 있을까요?

116
00:09:08,375 --> 00:09:10,945
여기서 우리는 첫 층의 뉴런과 현재 뉴런을 잇는 신경에 가중치를 부여할 것입니다.

117
00:09:10,945 --> 00:09:13,765
여기서 우리는 첫 층의 뉴런과 현재 뉴런을 잇는 신경에 가중치를 부여할 것입니다.

118
00:09:13,765 --> 00:09:15,915
여기서 우리는 첫 층의 뉴런과 현재 뉴런을 잇는 신경에 가중치를 부여할 것입니다.

119
00:09:16,660 --> 00:09:18,360
가중치는 그냥 숫자로 생각해주세요.

120
00:09:18,900 --> 00:09:22,275
그리고 첫 층 뉴런에서의 모든 활성치를 가져와

121
00:09:22,280 --> 00:09:25,480
각 신경의 가중치를 주고 모두 더합니다.

122
00:09:27,280 --> 00:09:31,860
그런 가중치를 이렇게 평면상에 표현하면 더욱 이해가 쉬워집니다.

123
00:09:32,360 --> 00:09:37,120
양수는 초록색 픽셀로, 음수는 빨강색 픽셀로 표현하겠습니다.

124
00:09:37,120 --> 00:09:41,800
픽셀의 밝기는 뭔가 가중치의 느슨한 표현이라고 해야할까요?

125
00:09:42,500 --> 00:09:45,500
이제 그 부분을 제외한 다른 모든 픽셀의 가중치를 0에 가깝게 만들고

126
00:09:45,500 --> 00:09:49,580
이제 그 부분을 제외한 다른 모든 픽셀의 가중치를 0에 가깝게 만들고

127
00:09:50,080 --> 00:09:52,880
각 픽셀에 가중치를 준 값의 합을 구하면

128
00:09:52,940 --> 00:09:57,660
그 영역의 픽셀에만 가중치를 주어 더한 것과 같은 상황이 됩니다.

129
00:09:58,960 --> 00:10:02,340
그리고 그 부분이 정말 테두리가 맞는지 확인하고 싶다면

130
00:10:02,600 --> 00:10:05,080
주변의 픽셀에 음수 가중치를 주면 됩니다.

131
00:10:05,080 --> 00:10:07,120
주변의 픽셀에 음수 가중치를 주면 됩니다.

132
00:10:07,260 --> 00:10:10,980
그러면 주변의 픽셀이 어두우면서 중앙의 픽셀이 밝을 때 최대치를 얻을 수 있습니다.

133
00:10:10,980 --> 00:10:13,120
그러면 주변의 픽셀이 어두우면서 중앙의 픽셀이 밝을 때 최대치를 얻을 수 있습니다.

134
00:10:14,400 --> 00:10:18,340
이처럼 가중치를 준 값의 합을 계산해 보면 어떤 값이라도 나올 수 있습니다.

135
00:10:18,520 --> 00:10:20,460
하지만 우리가 이 신경망에서 원하는 건 0과 1 사이의 값이죠.

136
00:10:20,460 --> 00:10:23,700
하지만 우리가 이 신경망에서 원하는 건 0과 1 사이의 값이죠.

137
00:10:23,855 --> 00:10:27,005
따라서 이 가중치를 준 값의 합을 0과 1 사이의 숫자로 만들어 주는 함수에 넣을 겁니다.

138
00:10:27,005 --> 00:10:29,785
따라서 이 가중치를 준 값의 합을 0과 1 사이의 숫자로 만들어 주는 함수에 넣을 겁니다.

139
00:10:29,785 --> 00:10:32,125
따라서 이 가중치를 준 값의 합을 0과 1 사이의 숫자로 만들어 주는 함수에 넣을 겁니다.

140
00:10:32,335 --> 00:10:35,555
이러한 함수로는 로지스틱 방정식으로도 알려진 시그모이드 함수가 잘 알려져 있습니다.

141
00:10:35,555 --> 00:10:37,685
이러한 함수로는 로지스틱 방정식으로도 알려진 시그모이드 함수가 잘 알려져 있습니다.

142
00:10:38,035 --> 00:10:41,085
간단히 설명하면 매우 작은 음수는 0에 매우 가깝게 대응되고

143
00:10:41,285 --> 00:10:44,365
매우 큰 양수는 1에 매우 가깝게 대응되며

144
00:10:44,400 --> 00:10:46,840
0 주위에서는 계속 증가합니다.

145
00:10:49,160 --> 00:10:51,920
그래서 이 뉴런의 활성화는 기본적으로

146
00:10:52,080 --> 00:10:56,600
관련있는 가중치의 합이 얼마나 더 양에 가까운지에 따라 정해져있습니다.

147
00:10:57,800 --> 00:11:02,560
하지만 당신은 가중치의 합이 0을 넘을때 뉴런이 활성화 되는 것을 원하는 게 아닐 수 있지요.

148
00:11:02,600 --> 00:11:06,520
예를 들어 합이 10보다 클 때 활성화 되기를 원할 수 도 있습니다.

149
00:11:06,640 --> 00:11:10,540
그것이 활성화되지 않기 위한 조건을 다는 것이죠.

150
00:11:11,060 --> 00:11:16,340
그럴때는 이 가중치에  -10 처럼 다른 음의 숫자를 더해줍니다.

151
00:11:16,340 --> 00:11:19,540
시그모이드 함수에 값을 집어넣기 전에 말이죠.

152
00:11:20,335 --> 00:11:22,695
그 더해지는 숫자를  Bias라고 합니다.

153
00:11:23,120 --> 00:11:28,260
가중치는 두번째 레이어가 선택하려는 뉴런의 픽셀 패턴을 알려주며

154
00:11:28,440 --> 00:11:35,100
bias는 뉴런이 활성화되려면 가중치의 합이 얼마나 더 높아야 하는지를 알려줍니다.

155
00:11:35,460 --> 00:11:37,600
지금까진 단 한 개의 뉴런에 대하여 이야기했습니다.

156
00:11:37,600 --> 00:11:40,860
이 레이어의 모든 뉴런은 각각

157
00:11:40,880 --> 00:11:44,980
첫번째 레이어의 784개 뉴런과  연결됩니다.

158
00:11:45,320 --> 00:11:50,780
그리고 이 각각의 연결들은 각자의 가중치를 갖습니다.

159
00:11:51,280 --> 00:11:58,100
또한 각각의 뉴런은 시그모이드 함수로 압축하기전에 가중치에 더한 값인 bias를 갖습니다.

160
00:11:58,380 --> 00:12:02,040
이러한 16개 뉴런의 숨겨진 레이어는

161
00:12:02,280 --> 00:12:08,140
각각의 16개의 bias를 가진 784 x 16개의 가중치를 의미합니다.

162
00:12:08,140 --> 00:12:12,060
그리고 이 수치는 첫번째 레이어와 두번째 레이어 사이의 연결은

163
00:12:12,060 --> 00:12:17,260
그들과 연관된 가중치와 bias들로 이루어져있습니다.

164
00:12:18,020 --> 00:12:21,360
이 연결은 거의 13,000개의 가중치와 bias를 갖습니다.

165
00:12:21,360 --> 00:12:24,215
이 연결은 거의 13,000개의 가중치와 bias를 갖습니다.

166
00:12:24,220 --> 00:12:29,980
비틀고 변할수 있는 13,000개의 연결은 각각 다른 방법으로 행동합니다.

167
00:12:30,620 --> 00:12:33,100
'배움' 에 대해서 얘기하자면

168
00:12:33,100 --> 00:12:37,860
컴퓨터가 실제로 해당 문제를 스스로 해결하기 위해서

169
00:12:37,860 --> 00:12:41,380
수많은 수치들을 찾기 위한 알맞은 환경을 얻는다는 것을 의미합니다.

170
00:12:42,260 --> 00:12:46,020
재밌기도 하고 조금 무섭기도 한 사고 실험을 해봅시다.

171
00:12:46,020 --> 00:12:50,080
한번 상상해 보세요. 이 가중치와 bias들을 일일히 직접 설정하는 겁니다.

172
00:12:50,080 --> 00:12:56,480
의도적으로 두번째 층은 모서리를 인식하고 세번째 층은 패턴을 인식하고 등을 할 수 있게 수를 수정하는 겁니다.

173
00:12:57,200 --> 00:13:01,700
개인적으로 네트워크를 그냥 블랙 박스로 이해하는 것보다 이 편이 낫다고 생각합니다.

174
00:13:01,700 --> 00:13:05,360
네트워크가 예상했던 것처럼 작동하지 않을 때

175
00:13:05,660 --> 00:13:10,180
가중치와 bias가 실제로는 무엇을 의미할지 조금이나마 생각해 두는 것이

176
00:13:10,320 --> 00:13:14,140
구조를 어떻게 바꿔야 개선시킬 수 있을지 생각해보는 시작점이 될 수 있기 때문입니다.

177
00:13:14,660 --> 00:13:18,400
아니면 네트워크가 작동하기는 하지만 예상했던 이유 때문이 아니라면

178
00:13:18,400 --> 00:13:22,880
가중치와 bias가 어떤 것인지 파고드는 것이 세워 두었던 가정을 시험하고

179
00:13:22,940 --> 00:13:26,180
무엇이 가능한지를 확실히 시험해볼 수 있게 합니다.

180
00:13:26,620 --> 00:13:30,760
그건 그렇고, 이 함수가 좀 적기 복잡하지 않습니까?

181
00:13:32,880 --> 00:13:37,360
이 연결을 표현할 수 있는 훨신 간결한 방법을 보여드리겠습니다.

182
00:13:37,500 --> 00:13:40,860
신경망에 대해 더 알아보려고 하면 보게 될 방식입니다.

183
00:13:40,960 --> 00:13:44,040
한 층이 활성화되는 정도를

184
00:13:44,120 --> 00:13:46,320
열벡터로 나타냅니다.

185
00:13:47,555 --> 00:13:50,755
그리고 가중치를 모두 모아 행렬로 나타냅니다.

186
00:13:50,760 --> 00:13:58,260
행렬의 각 열은 한 층과 다음 층의 특정 뉴런의 연결을 나타냅니다.

187
00:13:58,260 --> 00:14:03,900
이 것은 가중치들에 따라 활성화된 정도를 더한 것이

188
00:14:04,020 --> 00:14:09,500
행렬 벡터곱을 하여 나오는 열벡터의 각 원소에 대응한다는 것을 의미합니다.

189
00:14:14,000 --> 00:14:18,500
그나저나, 기계 학습의 대부분엔 선형 대수학을 잘 이해하는 것이 필요합니다.

190
00:14:18,500 --> 00:14:24,080
따라서 행렬과 행렬 벡터곱이 무엇을 의미하는지를 시각적으로 이해하고 싶은 사람은

191
00:14:24,180 --> 00:14:28,640
제가 만든 선형 대수학 시리즈, 특히 3장을 찾아 보세요.

192
00:14:29,120 --> 00:14:34,720
설명으로 돌아가서, bias를 독립적으로 각각의 값에 더하는 것으로 표현하는 대신

193
00:14:34,800 --> 00:14:42,220
bias를 전부 모아서 열벡터로 만들고 벡터 전체를 아까 전의 행렬 벡터 곱에 더하는 것으로 표현합니다.

194
00:14:43,000 --> 00:14:44,380
마지막으로

195
00:14:44,540 --> 00:14:47,280
시그모이드를 이 바깥에 감싸줍니다.

196
00:14:47,720 --> 00:14:51,895
이러면 시그모이드 함수를 결과 벡터 내의

197
00:14:51,895 --> 00:14:54,765
원소들에 각각 적용함을 의미합니다.

198
00:14:55,540 --> 00:15:00,280
그래서, 가중치 행렬과 벡터들을 각각의 기호로 나타내면

199
00:15:00,280 --> 00:15:07,980
활성화된 정도가 한 층에서 다른 층으로 어떻게 전달되는지를 아주 짧고 간단한 식으로 나타낼 수 있게 됩니다.

200
00:15:07,980 --> 00:15:15,820
많은 라이브러리들이 행렬곱을 최적화하므로 관련 코드를 훨씬 쉽고 빠르게 만들 수 있습니다.

201
00:15:17,460 --> 00:15:21,800
앞에서 뉴런이 숫자를 보관하는 것이라고 간단하게 말한 것이 기억나나요?

202
00:15:22,080 --> 00:15:26,480
당연히도 뉴런이 보관한 숫자들은 입력한 이미지에 따라 결정됩니다.

203
00:15:27,920 --> 00:15:33,140
따라서 뉴런을 이전 층의 뉴런의 출력을 모두 받아서

204
00:15:33,240 --> 00:15:38,360
0과 1 사이의 수를 만들어내는 함수라고 생각하는 것이 더 정확합니다.

205
00:15:38,880 --> 00:15:44,600
사실 네트워크 전체도 784개의 수를 입력 받아서

206
00:15:44,760 --> 00:15:47,020
10개의 수를 출력하는 함수입니다.

207
00:15:47,560 --> 00:15:51,460
이 터무니 없이 복잡한 함수는 특정한 패턴을 인식하기 위해

208
00:15:51,460 --> 00:15:55,660
13,000여개의 가중치나 bias형태의 매개변수로

209
00:15:55,660 --> 00:16:00,440
수많은 벡터 행렬 곱과 시그모이드 압축을 반복합니다.

210
00:16:00,820 --> 00:16:03,460
그럼에도 불구하고 그것은 단지 함수에 불과하며

211
00:16:03,580 --> 00:16:06,980
어떤 면에서는 복잡해서 다행이기도 합니다.

212
00:16:06,980 --> 00:16:09,000
내 말은 만약 그것이 단순한 함수였다면,

213
00:16:09,000 --> 00:16:12,800
우리가 이 숫자들을 이해하려는 도전에 정면으로 맞서보려고나 했을까요?

214
00:16:13,120 --> 00:16:14,940
그런데 어떻게 그 도전에 맞설까요?

215
00:16:14,940 --> 00:16:20,080
이 네트워크는 어떻게 데이터를 보는 것만으로 적절한 가중치와 bias들을 배우는걸까요?

216
00:16:20,080 --> 00:16:22,260
그것이 바로 제가 다음 비디오에서 알려드릴 내용입니다.

217
00:16:22,260 --> 00:16:23,720
그리고 우리가 보고 있는 이 특정 네트워크가

218
00:16:23,760 --> 00:16:26,360
실제로 하고 있는 일을 좀 더 깊이 파헤칠 겁니다.

219
00:16:27,380 --> 00:16:30,340
이제 그 비디오 또는 새로운 비디오가 언제 나올지에 대한  알림을 받기 위해

220
00:16:30,340 --> 00:16:33,020
구독을 부탁할 타이밍입니다.

221
00:16:33,020 --> 00:16:35,020
하지만 현실적으로 대부분의 사람들은

222
00:16:35,020 --> 00:16:37,920
실제로  YouTube로부터 알림을 받지 못하고 있습니다.

223
00:16:37,920 --> 00:16:41,340
YouTube의 추천 알고리즘의 기반이 되는 신경망은

224
00:16:41,340 --> 00:16:43,920
구독자들이 채널의 콘텐츠를 보길 원한다고 생각하기 때문에

225
00:16:43,920 --> 00:16:47,800
구독을 부탁하고 싶네요.

226
00:16:48,300 --> 00:16:50,160
어쨌든 또 다른 영상을 지켜봐주세요.

227
00:16:50,685 --> 00:16:53,785
Patreon을 통해 영상들을 후원해주신 분들께 감사드립니다.

228
00:16:53,820 --> 00:16:56,920
올 여름엔 확률 편이 조금 더디게 진행됐군요.

229
00:16:56,920 --> 00:16:59,300
이 프로젝트를 마치고 다시 진행한다면

230
00:16:59,300 --> 00:17:01,840
곧 찾아 보실 수 있을 겁니다.

231
00:17:03,420 --> 00:17:06,060
마치며 저는 지금 Lisha Li와 함께 있는데요

232
00:17:06,060 --> 00:17:09,040
Lisha Li는 딥러닝의 이론적인 측면으로 박사 학위를 받았고

233
00:17:09,040 --> 00:17:12,180
현재 Amplify Partners라는 벤처캐피탈 회사에서 일하고 있으며

234
00:17:12,180 --> 00:17:14,860
Amplify Partners는 이 비디오 제작을 후원해주신 고마운 곳입니다.

235
00:17:15,220 --> 00:17:19,400
그래서 Lisha, 저는 시그모이드 함수에 대해서 얘기를 해보고 싶은데

236
00:17:19,420 --> 00:17:23,460
제가 이해하기론 초기 네트워크는 시그모이드 함수를 가중치가 적용된 합들을

237
00:17:23,460 --> 00:17:25,080
0부터 1사이의 값으로 압축하기 위해 사용했고

238
00:17:25,080 --> 00:17:29,700
이건 뉴런이 활성화 되거나 비활성화 되는 생물학적인 현상을 모방한거죠

239
00:17:29,700 --> 00:17:30,200
그렇죠.

240
00:17:30,200 --> 00:17:34,400
그런데 현대의 네트워크들은 시그모이드 함수를 이제 잘 안써요

241
00:17:34,400 --> 00:17:35,680
이건 이제 좀 구식이잖아요?

242
00:17:35,680 --> 00:17:39,300
네 대신에 ReLU함수가 더 훈련시키기 쉽죠.

243
00:17:39,305 --> 00:17:42,275
ReLU는 "Rectified Linear Unit"(선형 정류 유닛)이고요.

244
00:17:42,280 --> 00:17:47,560
네 ReLU는 그냥 0과 a에 max함수를 취한건데

245
00:17:47,620 --> 00:17:52,120
a는 뉴런의 활성치를 나타내는 함수임을 이 비디오에서 설명했습니다.

246
00:17:52,120 --> 00:18:02,440
이건 뉴런이 생물학적으로 어떻게 활성화되고 비활성화 되는지를 모방한 것과 같아서

247
00:18:02,440 --> 00:18:05,300
임계값을 넘기면 항등함수를 출력하고

248
00:18:05,300 --> 00:18:08,120
임계값을 넘기지 못하면 0을 출력합니다.

249
00:18:08,120 --> 00:18:09,800
임계값을 넘기지 못하면 0을 출력합니다.

250
00:18:09,800 --> 00:18:10,940
단순화된 버전같은거죠.

251
00:18:10,940 --> 00:18:13,480
시그모이드 함수로는 신경망 훈련이 잘 되지 않아요

252
00:18:13,480 --> 00:18:16,100
아니면 훈련 시키기 아주 어려운 걸 수도 있죠

253
00:18:16,100 --> 00:18:19,060
그래서 누군가가 ReLU로 시도해 봤는데

254
00:18:19,060 --> 00:18:24,960
이런 아주 깊은 신경망(Deep neural networks)에서 아주 잘 작동했던거죠.

255
00:18:24,960 --> 00:18:26,120
그렇군요 고마워요 Lisha

