1
00:00:04,220 --> 00:00:05,400
이것은 3입니다.

2
00:00:06,060 --> 00:00:08,391
28x28 픽셀의 매우 낮은 해상도로 

3
00:00:08,391 --> 00:00:11,055
조잡하게 작성되고 렌더링되었지만 뇌는 이를 

4
00:00:11,055 --> 00:00:13,720
3으로 인식하는 데 아무런 문제가 없습니다.

5
00:00:14,340 --> 00:00:16,573
그리고 두뇌가 이렇게 쉽게 이 일을 해낼 수 있다는 

6
00:00:16,573 --> 00:00:18,652
것이 얼마나 놀라운 일인지 잠시 생각해 보셨으면 

7
00:00:18,652 --> 00:00:18,960
합니다.

8
00:00:19,700 --> 00:00:24,579
각 픽셀의 특정 값은 이미지마다 매우 다르지만 이것, 

9
00:00:24,579 --> 00:00:28,320
이것, 이것도 3초로 인식할 수 있습니다.

10
00:00:28,900 --> 00:00:32,708
이 3을 볼 때 발화하는 눈의 특정 빛에 민감한 

11
00:00:32,708 --> 00:00:36,940
세포는 이 3을 볼 때 발화하는 세포와 매우 다릅니다.

12
00:00:37,520 --> 00:00:40,891
하지만 당신의 미친 듯이 똑똑한 시각 피질은 이 

13
00:00:40,891 --> 00:00:44,638
이미지들을 동일한 아이디어를 나타내는 것으로 인식하는 

14
00:00:44,638 --> 00:00:48,260
동시에 다른 이미지들을 고유한 아이디어로 인식합니다.

15
00:00:49,220 --> 00:00:52,637
하지만 제가 여러분에게 앉아서 이렇게 28x28 

16
00:00:52,637 --> 00:00:55,928
픽셀의 격자를 받아 0에서 10 사이의 숫자를 

17
00:00:55,928 --> 00:00:59,471
출력하고 그 숫자가 무엇이라고 생각하는지 알려주는 

18
00:00:59,471 --> 00:01:02,889
프로그램을 작성하라고 하면, 이 작업은 코믹하게 

19
00:01:02,889 --> 00:01:06,180
사소한 것에서 엄청나게 어려운 것으로 바뀝니다.

20
00:01:07,160 --> 00:01:09,484
기계 학습과 신경망의 현재와 미래에 대한 

21
00:01:09,484 --> 00:01:12,011
관련성과 중요성은 바위 밑에서 살아온 사람이 

22
00:01:12,011 --> 00:01:14,640
아니라면 굳이 설명할 필요가 없을 것 같습니다.

23
00:01:15,120 --> 00:01:17,433
하지만 여기서 제가 하고 싶은 것은 배경 지식이 

24
00:01:17,433 --> 00:01:19,490
없다고 가정하고 신경망이 실제로 무엇인지, 

25
00:01:19,490 --> 00:01:21,803
그리고 신경망이 하는 일을 유행어가 아닌 수학의 

26
00:01:21,803 --> 00:01:24,031
한 부분으로 시각화하는 데 도움을 주고자 하는 

27
00:01:24,031 --> 00:01:24,460
것입니다.

28
00:01:25,020 --> 00:01:28,348
제 바람은 여러분이 이 구조 자체에 동기를 부여받고, 

29
00:01:28,348 --> 00:01:31,455
신경망 인용-비인용 학습에 대해 읽거나 들었을 때 

30
00:01:31,455 --> 00:01:34,340
그 의미를 알 것 같다는 느낌을 받는 것입니다.

31
00:01:35,360 --> 00:01:37,900
이 동영상에서는 구조 구성 요소에 대해서만 다루고 

32
00:01:37,900 --> 00:01:40,260
다음 동영상에서는 학습에 대해 다룰 예정입니다.

33
00:01:40,960 --> 00:01:43,304
우리가 할 일은 손으로 쓴 숫자를 인식하는 

34
00:01:43,304 --> 00:01:46,040
방법을 학습할 수 있는 신경망을 구성하는 것입니다.

35
00:01:49,360 --> 00:01:51,770
이것은 주제를 소개하는 데 있어 다소 고전적인 

36
00:01:51,770 --> 00:01:54,458
예시이며, 두 동영상의 마지막에 더 많은 것을 배울 

37
00:01:54,458 --> 00:01:57,239
수 있는 몇 가지 좋은 리소스와 이를 수행하는 코드를 

38
00:01:57,239 --> 00:01:59,928
다운로드하여 자신의 컴퓨터에서 재생할 수 있는 곳을 

39
00:01:59,928 --> 00:02:02,338
소개하고자 하므로 여기서는 현재 상황을 그대로 

40
00:02:02,338 --> 00:02:03,080
유지하겠습니다.

41
00:02:05,040 --> 00:02:07,574
신경망에는 다양한 변형이 있으며, 

42
00:02:07,574 --> 00:02:10,909
최근 몇 년 동안 이러한 변형에 대한 연구가 

43
00:02:10,909 --> 00:02:14,777
붐을 이루고 있지만, 이 두 개의 소개 동영상에서는 

44
00:02:14,777 --> 00:02:17,979
가장 단순한 플레인 바닐라 형태에 대해서만 

45
00:02:17,979 --> 00:02:19,180
살펴볼 것입니다.

46
00:02:19,860 --> 00:02:22,687
이것은 더 강력한 최신 변형을 이해하기 

47
00:02:22,687 --> 00:02:25,643
위한 필수 전제 조건이며, 여전히 우리가 

48
00:02:25,643 --> 00:02:28,600
이해하기에는 복잡한 부분이 많이 있습니다.

49
00:02:29,120 --> 00:02:31,320
하지만 이렇게 간단한 형태라도 컴퓨터가 

50
00:02:31,320 --> 00:02:33,620
손으로 쓴 숫자를 인식하는 법을 배울 수 

51
00:02:33,620 --> 00:02:36,520
있다는 것은 컴퓨터가 할 수 있는 꽤 멋진 일입니다.

52
00:02:37,480 --> 00:02:39,935
동시에 우리가 기대했던 몇 가지 기대에 

53
00:02:39,935 --> 00:02:42,280
얼마나 못 미치는지 알게 될 것입니다.

54
00:02:43,380 --> 00:02:46,042
이름에서 알 수 있듯이 신경망은 뇌에서 영감을 

55
00:02:46,042 --> 00:02:48,500
얻었지만, 이를 좀 더 세분화해 보겠습니다.

56
00:02:48,520 --> 00:02:50,384
뉴런이란 무엇이며, 어떤 의미에서 

57
00:02:50,384 --> 00:02:51,660
서로 연결되어 있을까요?

58
00:02:52,500 --> 00:02:55,065
지금 제가 뉴런이라고 할 때 여러분이 

59
00:02:55,065 --> 00:02:57,752
생각했으면 하는 것은 숫자, 특히 0과 

60
00:02:57,752 --> 00:03:00,440
1 사이의 숫자를 담고 있는 것뿐입니다.

61
00:03:00,680 --> 00:03:02,560
그 이상도 이하도 아닙니다.

62
00:03:03,780 --> 00:03:06,762
예를 들어 네트워크는 입력 이미지의 

63
00:03:06,762 --> 00:03:09,894
28x28 픽셀 각각에 해당하는 뉴런 

64
00:03:09,894 --> 00:03:14,220
무리로 시작하며, 총 784개의 뉴런으로 구성됩니다.

65
00:03:14,700 --> 00:03:17,742
각각의 숫자는 해당 픽셀의 그레이스케일 

66
00:03:17,742 --> 00:03:21,061
값을 나타내는 숫자로, 검은색 픽셀의 경우 

67
00:03:21,061 --> 00:03:24,380
0에서 흰색 픽셀의 경우 1까지 다양합니다.

68
00:03:25,300 --> 00:03:28,586
뉴런 내부의 이 숫자를 활성화라고 하며, 

69
00:03:28,586 --> 00:03:31,301
활성화가 높을 때 각 뉴런에 불이 

70
00:03:31,301 --> 00:03:34,160
켜지는 이미지를 떠올릴 수 있습니다.

71
00:03:36,720 --> 00:03:39,161
따라서 이 784개의 뉴런이 모두 

72
00:03:39,161 --> 00:03:41,860
네트워크의 첫 번째 계층을 구성합니다.

73
00:03:46,500 --> 00:03:48,182
이제 마지막 레이어로 넘어가서, 

74
00:03:48,182 --> 00:03:50,518
여기에는 각각 숫자 하나를 나타내는 10개의 

75
00:03:50,518 --> 00:03:51,360
뉴런이 있습니다.

76
00:03:52,040 --> 00:03:56,072
이러한 뉴런의 활성화는 다시 0과 1 사이의 숫자로, 

77
00:03:56,072 --> 00:03:59,566
시스템이 주어진 이미지가 주어진 숫자와 얼마나 

78
00:03:59,566 --> 00:04:02,120
일치한다고 생각하는지를 나타냅니다.

79
00:04:03,040 --> 00:04:05,652
또한 그 사이에는 숨겨진 레이어라고 불리는 

80
00:04:05,652 --> 00:04:08,265
두 개의 레이어가 있는데, 당분간은 도대체 

81
00:04:08,265 --> 00:04:10,987
숫자를 인식하는 이 프로세스가 어떻게 처리될 

82
00:04:10,987 --> 00:04:13,600
것인지에 대한 거대한 물음표가 될 것입니다.

83
00:04:14,260 --> 00:04:16,447
이 네트워크에서는 각각 16개의 뉴런이 있는 

84
00:04:16,447 --> 00:04:18,284
두 개의 숨겨진 레이어를 선택했는데, 

85
00:04:18,284 --> 00:04:20,560
이는 다소 자의적인 선택이라는 점을 인정합니다.

86
00:04:21,019 --> 00:04:23,292
솔직히 말해서 저는 순간적으로 구조를 어떻게 

87
00:04:23,292 --> 00:04:25,655
동기 부여하고 싶은지에 따라 두 개의 레이어를 

88
00:04:25,655 --> 00:04:28,200
선택했고, 16은 화면에 맞추기 좋은 숫자였습니다.

89
00:04:28,780 --> 00:04:32,340
실제로 여기에는 특정 구조로 실험할 여지가 많습니다.

90
00:04:33,020 --> 00:04:35,808
네트워크가 작동하는 방식에 따라 한 계층의 

91
00:04:35,808 --> 00:04:38,480
활성화가 다음 계층의 활성화를 결정합니다.

92
00:04:39,200 --> 00:04:42,419
물론 정보 처리 메커니즘으로서 네트워크의 

93
00:04:42,419 --> 00:04:45,500
핵심은 한 계층의 활성화가 다음 계층의 

94
00:04:45,500 --> 00:04:48,580
활성화를 가져오는 방식에 달려 있습니다.

95
00:04:49,140 --> 00:04:51,778
이는 생물학적 뉴런 네트워크에서 일부 

96
00:04:51,778 --> 00:04:54,165
뉴런 그룹이 발화하면 다른 뉴런도 

97
00:04:54,165 --> 00:04:57,180
발화하는 것과 느슨하게 비유할 수 있습니다.

98
00:04:58,120 --> 00:05:00,566
여기서 보여드리는 네트워크는 이미 

99
00:05:00,566 --> 00:05:03,400
숫자를 인식하도록 훈련된 네트워크입니다.

100
00:05:03,640 --> 00:05:07,121
즉, 이미지를 입력하면 이미지의 각 픽셀 밝기에 

101
00:05:07,121 --> 00:05:10,603
따라 입력 레이어의 784개 뉴런을 모두 비추면 

102
00:05:10,603 --> 00:05:13,956
그 활성화 패턴이 다음 레이어에서 어떤 특정한 

103
00:05:13,956 --> 00:05:17,566
패턴을 일으키고, 그 다음 레이어에서 어떤 패턴을 

104
00:05:17,566 --> 00:05:21,177
일으키고, 최종적으로 출력 레이어에서 어떤 패턴이 

105
00:05:21,177 --> 00:05:22,080
만들어집니다.

106
00:05:22,560 --> 00:05:24,656
그리고 해당 출력 레이어에서 가장 

107
00:05:24,656 --> 00:05:26,752
밝은 뉴런은 말하자면 이 이미지가 

108
00:05:26,752 --> 00:05:29,400
나타내는 숫자에 대한 네트워크의 선택입니다.

109
00:05:32,560 --> 00:05:34,927
한 레이어가 다음 레이어에 어떤 영향을 미치는지 

110
00:05:34,927 --> 00:05:36,943
또는 트레이닝이 어떻게 작동하는지에 대한 

111
00:05:36,943 --> 00:05:38,960
계산에 들어가기 전에, 이와 같은 레이어 

112
00:05:38,960 --> 00:05:41,240
구조가 지능적으로 작동할 것으로 기대하는 것이 

113
00:05:41,240 --> 00:05:43,520
왜 합리적인지에 대해 먼저 이야기해 보겠습니다.

114
00:05:44,060 --> 00:05:45,220
여기서 무엇을 기대할 수 있을까요?

115
00:05:45,400 --> 00:05:47,600
이러한 중간 계층을 위한 최선의 희망은 무엇일까요?

116
00:05:48,920 --> 00:05:51,105
여러분이나 제가 숫자를 인식할 때 

117
00:05:51,105 --> 00:05:53,520
우리는 다양한 구성 요소를 조합합니다.

118
00:05:54,200 --> 00:05:56,820
9는 위쪽에 고리가 있고 오른쪽에 선이 있습니다.

119
00:05:57,380 --> 00:05:59,023
8도 위쪽에는 루프가 있지만 

120
00:05:59,023 --> 00:06:01,180
아래쪽에는 다른 루프와 짝을 이룹니다.

121
00:06:01,980 --> 00:06:05,167
4는 기본적으로 세 개의 특정 라인으로 나뉘며, 

122
00:06:05,167 --> 00:06:06,820
그런 식으로 세분화됩니다.

123
00:06:07,600 --> 00:06:10,836
완벽한 세상이라면 두 번째부터 마지막 레이어의 

124
00:06:10,836 --> 00:06:13,947
각 뉴런이 이러한 하위 구성 요소 중 하나에 

125
00:06:13,947 --> 00:06:17,308
대응하여, 예를 들어 9 또는 8과 같은 루프가 

126
00:06:17,308 --> 00:06:20,668
위에 있는 이미지를 입력할 때마다 활성화가 1에 

127
00:06:20,668 --> 00:06:23,780
가까운 특정 뉴런이 있기를 바랄 수 있습니다.

128
00:06:24,500 --> 00:06:27,232
이 특정 픽셀 루프를 말하는 것이 아니라, 

129
00:06:27,232 --> 00:06:29,510
일반적으로 상단을 향한 루핑 패턴이 

130
00:06:29,510 --> 00:06:31,560
이 뉴런을 자극한다는 의미입니다.

131
00:06:32,440 --> 00:06:35,140
이렇게 하면 세 번째 레이어에서 마지막 레이어로 

132
00:06:35,140 --> 00:06:37,740
이동할 때 어떤 하위 구성 요소의 조합이 어떤 

133
00:06:37,740 --> 00:06:40,040
숫자에 해당하는지 학습하기만 하면 됩니다.

134
00:06:41,000 --> 00:06:44,145
물론, 이러한 하위 구성 요소를 어떻게 인식하고 

135
00:06:44,145 --> 00:06:47,640
올바른 하위 구성 요소가 무엇인지 알아낼 수 있을까요?

136
00:06:48,060 --> 00:06:49,609
한 레이어가 다음 레이어에 어떤 영향을 

137
00:06:49,609 --> 00:06:51,440
미치는지에 대해서는 아직 이야기하지 않았지만, 

138
00:06:51,440 --> 00:06:53,060
이 레이어에 대해 잠시 설명해 보겠습니다.

139
00:06:53,680 --> 00:06:55,518
루프를 인식하는 것도 하위 문제로 

140
00:06:55,518 --> 00:06:56,680
세분화할 수 있습니다.

141
00:06:57,280 --> 00:07:00,078
이를 수행하는 한 가지 합리적인 방법은 먼저 그것을 

142
00:07:00,078 --> 00:07:02,780
구성하는 다양한 작은 가장자리를 인식하는 것입니다.

143
00:07:03,780 --> 00:07:07,095
마찬가지로 숫자 1, 4, 7에서 볼 수 있는 긴 

144
00:07:07,095 --> 00:07:09,582
선은 실제로는 긴 가장자리일 뿐이며, 

145
00:07:09,582 --> 00:07:12,898
여러 개의 작은 가장자리로 이루어진 특정 패턴으로 

146
00:07:12,898 --> 00:07:14,320
생각할 수도 있습니다.

147
00:07:15,140 --> 00:07:17,703
따라서 네트워크의 두 번째 레이어에 있는 

148
00:07:17,703 --> 00:07:20,156
각 뉴런이 다양한 관련 작은 가장자리와 

149
00:07:20,156 --> 00:07:22,720
대응하는 것이 우리의 희망일 수 있습니다.

150
00:07:23,540 --> 00:07:27,329
이런 이미지가 들어오면 8~10개의 특정 작은 

151
00:07:27,329 --> 00:07:31,119
가장자리와 관련된 모든 뉴런에 불이 들어오고, 

152
00:07:31,119 --> 00:07:35,201
이는 다시 위쪽 루프와 긴 수직선과 관련된 뉴런에 

153
00:07:35,201 --> 00:07:38,991
불을 켜고, 9와 관련된 뉴런에 불이 들어오는 

154
00:07:38,991 --> 00:07:39,720
식입니다.

155
00:07:40,680 --> 00:07:43,494
이것이 우리의 최종 네트워크가 실제로 하는 일인지 

156
00:07:43,494 --> 00:07:46,308
여부는 네트워크를 훈련하는 방법을 알게 되면 다시 

157
00:07:46,308 --> 00:07:49,223
생각해 볼 문제이지만, 이것은 우리가 가질 수 있는 

158
00:07:49,223 --> 00:07:51,936
희망이며, 이와 같은 계층적 구조를 가진 일종의 

159
00:07:51,936 --> 00:07:52,540
목표입니다.

160
00:07:53,160 --> 00:07:55,313
또한 이와 같은 가장자리와 패턴을 

161
00:07:55,313 --> 00:07:57,580
감지할 수 있다면 다른 이미지 인식 

162
00:07:57,580 --> 00:08:00,300
작업에 얼마나 유용할지 상상할 수 있습니다.

163
00:08:00,880 --> 00:08:03,911
이미지 인식 외에도 추상화 계층으로 세분화할 수 

164
00:08:03,911 --> 00:08:07,280
있는 모든 종류의 지능적인 작업을 수행할 수 있습니다.

165
00:08:08,040 --> 00:08:10,351
예를 들어, 음성 구문 분석은 원시 오디오를 

166
00:08:10,351 --> 00:08:12,015
가져와 뚜렷한 소리를 골라내고, 

167
00:08:12,015 --> 00:08:14,142
이 소리가 결합하여 특정 음절을 만들고, 

168
00:08:14,142 --> 00:08:15,991
이 음절이 결합하여 단어를 만들고, 

169
00:08:15,991 --> 00:08:18,395
이 단어가 결합하여 구문과 더 추상적인 생각을 

170
00:08:18,395 --> 00:08:20,060
구성하는 등의 작업을 포함합니다.

171
00:08:21,100 --> 00:08:23,931
하지만 이 모든 것이 실제로 어떻게 작동하는지 

172
00:08:23,931 --> 00:08:27,088
다시 돌아가서, 한 레이어의 활성화가 다음 레이어를 

173
00:08:27,088 --> 00:08:29,920
어떻게 결정할지 설계하는 모습을 상상해 보세요.

174
00:08:30,860 --> 00:08:34,575
목표는 픽셀을 가장자리로, 가장자리를 패턴으로, 

175
00:08:34,575 --> 00:08:38,291
패턴을 숫자로 결합할 수 있는 메커니즘을 갖추는 

176
00:08:38,291 --> 00:08:38,980
것입니다.

177
00:08:39,440 --> 00:08:42,329
아주 구체적인 예를 하나 더 들어보자면, 

178
00:08:42,329 --> 00:08:45,846
두 번째 레이어의 특정 뉴런이 이미지의 이 영역에 

179
00:08:45,846 --> 00:08:49,363
가장자리가 있는지 없는지를 감지하는 것이 목표라고 

180
00:08:49,363 --> 00:08:50,620
가정해 보겠습니다.

181
00:08:51,440 --> 00:08:53,035
당면한 문제는 네트워크에 어떤 

182
00:08:53,035 --> 00:08:55,100
매개변수가 있어야 하는가 하는 것입니다.

183
00:08:55,640 --> 00:08:57,745
이 패턴이나 다른 픽셀 패턴, 

184
00:08:57,745 --> 00:09:00,595
여러 모서리가 루프를 만들 수 있는 패턴 

185
00:09:00,595 --> 00:09:03,815
등을 잠재적으로 캡처할 수 있을 만큼 표현력을 

186
00:09:03,815 --> 00:09:06,788
발휘하려면 어떤 다이얼과 노브를 조정할 수 

187
00:09:06,788 --> 00:09:07,780
있어야 하나요?

188
00:09:08,720 --> 00:09:12,207
우리가 할 일은 뉴런과 첫 번째 레이어의 뉴런 

189
00:09:12,207 --> 00:09:15,560
사이의 각 연결에 가중치를 할당하는 것입니다.

190
00:09:16,320 --> 00:09:17,700
이 가중치는 단지 숫자에 불과합니다.

191
00:09:18,540 --> 00:09:22,209
그런 다음 첫 번째 레이어에서 이러한 모든 활성화를 

192
00:09:22,209 --> 00:09:25,500
가져와 가중치에 따라 가중치 합계를 계산합니다.

193
00:09:27,700 --> 00:09:30,354
저는 이러한 가중치를 작은 격자로 정리한 

194
00:09:30,354 --> 00:09:33,008
것으로 생각하면 도움이 된다고 생각하며, 

195
00:09:33,008 --> 00:09:35,778
녹색 픽셀은 양의 가중치를 나타내고 빨간색 

196
00:09:35,778 --> 00:09:37,856
픽셀은 음의 가중치를 나타내며, 

197
00:09:37,856 --> 00:09:40,625
해당 픽셀의 밝기는 가중치의 값을 느슨하게 

198
00:09:40,625 --> 00:09:41,780
묘사하는 것입니다.

199
00:09:42,780 --> 00:09:46,781
이제 관심 있는 영역의 일부 양수 가중치를 제외하고 

200
00:09:46,781 --> 00:09:50,920
거의 모든 픽셀과 관련된 가중치를 0으로 만들었으므로 

201
00:09:50,920 --> 00:09:54,646
모든 픽셀 값의 가중치 합계를 구하면 관심 있는 

202
00:09:54,646 --> 00:09:57,820
영역의 픽셀 값만 합산하는 것과 같습니다.

203
00:09:59,140 --> 00:10:02,504
여기에 가장자리가 있는지 여부를 파악하고 

204
00:10:02,504 --> 00:10:06,600
싶다면 주변 픽셀에 음수 가중치를 적용하면 됩니다.

205
00:10:07,480 --> 00:10:10,023
그런 다음 중간 픽셀이 밝고 주변 

206
00:10:10,023 --> 00:10:12,700
픽셀이 어두울 때 합이 가장 큽니다.

207
00:10:14,260 --> 00:10:17,314
이렇게 가중 합계를 계산하면 어떤 숫자가 나올 

208
00:10:17,314 --> 00:10:20,250
수도 있지만, 이 네트워크에서 우리가 원하는 

209
00:10:20,250 --> 00:10:23,540
것은 활성화가 0과 1 사이의 값이 되는 것입니다.

210
00:10:24,120 --> 00:10:27,981
따라서 이 가중 합계를 0과 1 사이의 범위로 

211
00:10:27,981 --> 00:10:32,140
실수선을 쪼개는 함수로 펌핑하는 것이 일반적입니다.

212
00:10:32,460 --> 00:10:34,781
이를 수행하는 일반적인 함수를 로지스틱 

213
00:10:34,781 --> 00:10:37,420
곡선이라고도 하는 시그모이드 함수라고 합니다.

214
00:10:38,000 --> 00:10:41,518
기본적으로 매우 음수인 입력은 0에 가까워지고, 

215
00:10:41,518 --> 00:10:43,863
양수인 입력은 1에 가까워지며, 

216
00:10:43,863 --> 00:10:46,600
입력 0을 중심으로 꾸준히 증가합니다.

217
00:10:49,120 --> 00:10:52,605
따라서 여기서 뉴런의 활성화는 기본적으로 관련 

218
00:10:52,605 --> 00:10:56,360
가중치 합이 얼마나 양수인지를 측정하는 척도입니다.

219
00:10:57,540 --> 00:10:59,577
하지만 가중치 합이 0보다 클 때 뉴런에 

220
00:10:59,577 --> 00:11:01,880
불이 켜지기를 원하는 것은 아닐 수도 있습니다.

221
00:11:02,280 --> 00:11:04,680
합이 10보다 클 때만 활성화되도록 

222
00:11:04,680 --> 00:11:06,360
하고 싶을 수도 있습니다.

223
00:11:06,840 --> 00:11:10,260
즉, 비활성화를 위해 약간의 편향성을 원합니다.

224
00:11:11,380 --> 00:11:14,182
그런 다음 이 가중 합계에 음수 10과 

225
00:11:14,182 --> 00:11:16,984
같은 다른 숫자를 더한 다음 시그모이드 

226
00:11:16,984 --> 00:11:19,660
스퀴시화 함수를 통해 연결하면 됩니다.

227
00:11:20,580 --> 00:11:22,440
이 추가 숫자를 바이어스라고 합니다.

228
00:11:23,460 --> 00:11:26,447
따라서 가중치는 두 번째 레이어의 뉴런이 어떤 

229
00:11:26,447 --> 00:11:28,975
픽셀 패턴을 포착하고 있는지 알려주고, 

230
00:11:28,975 --> 00:11:31,503
바이어스는 뉴런이 의미 있게 활성화되기 

231
00:11:31,503 --> 00:11:34,490
시작하려면 가중치 합이 얼마나 높아야 하는지를 

232
00:11:34,490 --> 00:11:35,180
알려줍니다.

233
00:11:36,120 --> 00:11:37,680
그리고 그것은 하나의 뉴런에 불과합니다.

234
00:11:38,280 --> 00:11:42,719
이 레이어의 다른 모든 뉴런은 첫 번째 레이어의 

235
00:11:42,719 --> 00:11:46,171
784개 픽셀 뉴런에 모두 연결되며, 

236
00:11:46,171 --> 00:11:50,940
784개의 연결에는 각각 고유한 가중치가 부여됩니다.

237
00:11:51,600 --> 00:11:54,541
또한 각각에는 시그모이드로 쪼개기 전에 가중 

238
00:11:54,541 --> 00:11:57,600
합계에 더하는 다른 숫자, 즉 편향이 있습니다.

239
00:11:58,110 --> 00:11:59,540
생각해야 할 것이 많습니다!

240
00:11:59,960 --> 00:12:02,824
16개의 뉴런으로 구성된 이 숨겨진 레이어는 

241
00:12:02,824 --> 00:12:05,230
16개의 가중치와 함께 총 784개의 

242
00:12:05,230 --> 00:12:07,980
가중치를 16개의 편향으로 계산한 것입니다.

243
00:12:08,840 --> 00:12:10,390
그리고 이 모든 것은 첫 번째 레이어에서 

244
00:12:10,390 --> 00:12:11,940
두 번째 레이어로 연결되는 것일 뿐입니다.

245
00:12:12,520 --> 00:12:14,864
다른 레이어 간의 연결에도 여러 

246
00:12:14,864 --> 00:12:17,340
가중치와 편향이 연관되어 있습니다.

247
00:12:18,340 --> 00:12:21,003
이 네트워크의 총 가중치와 편향성은 

248
00:12:21,003 --> 00:12:23,800
거의 정확히 13,000개에 달합니다.

249
00:12:23,800 --> 00:12:25,617
13,000개의 노브와 다이얼을 

250
00:12:25,617 --> 00:12:27,738
조정하고 돌려서 이 네트워크가 다양한 

251
00:12:27,738 --> 00:12:29,960
방식으로 작동하도록 설정할 수 있습니다.

252
00:12:31,040 --> 00:12:34,623
따라서 학습이란 컴퓨터가 수많은 숫자에 대해 

253
00:12:34,623 --> 00:12:38,206
유효한 설정을 찾아내어 실제로 당면한 문제를 

254
00:12:38,206 --> 00:12:41,360
해결할 수 있도록 하는 것을 의미합니다.

255
00:12:42,620 --> 00:12:45,473
재미와 공포를 동시에 느낄 수 있는 한 가지 사고 

256
00:12:45,473 --> 00:12:48,122
실험은 앉아서 이 모든 가중치와 편향을 손으로 

257
00:12:48,122 --> 00:12:50,771
직접 설정하고, 의도적으로 숫자를 조정하여 두 

258
00:12:50,771 --> 00:12:53,726
번째 레이어가 가장자리를 포착하고 세 번째 레이어가 

259
00:12:53,726 --> 00:12:56,580
패턴을 포착하는 등의 작업을 상상해 보는 것입니다.

260
00:12:56,980 --> 00:12:59,829
네트워크가 예상한 대로 작동하지 않을 때 이러한 

261
00:12:59,829 --> 00:13:02,783
가중치와 편향성이 실제로 무엇을 의미하는지에 대한 

262
00:13:02,783 --> 00:13:04,577
관계를 조금이라도 구축했다면, 

263
00:13:04,577 --> 00:13:07,215
구조를 개선하기 위해 구조를 변경하는 방법을 

264
00:13:07,215 --> 00:13:10,170
실험할 수 있는 출발점이 생기기 때문에 개인적으로 

265
00:13:10,170 --> 00:13:12,808
네트워크를 완전한 블랙박스로 취급하는 것보다 

266
00:13:12,808 --> 00:13:14,180
만족스럽다고 생각합니다.

267
00:13:14,960 --> 00:13:17,752
또는 네트워크가 작동하지만 예상과 달리 작동하지 

268
00:13:17,752 --> 00:13:20,545
않는 경우, 가중치와 편향이 어떤 역할을 하는지 

269
00:13:20,545 --> 00:13:23,234
파헤치는 것은 가정에 도전하고 가능한 솔루션의 

270
00:13:23,234 --> 00:13:25,820
전체 공간을 실제로 드러내는 좋은 방법입니다.

271
00:13:26,840 --> 00:13:29,240
그런데 여기의 실제 기능은 적기에는 

272
00:13:29,240 --> 00:13:30,680
조금 번거롭지 않나요?

273
00:13:32,500 --> 00:13:34,621
이러한 연결을 보다 간결하게 

274
00:13:34,621 --> 00:13:37,140
표현하는 표기법을 보여드리겠습니다.

275
00:13:37,660 --> 00:13:39,505
신경망에 대해 더 자세히 알아보려면 

276
00:13:39,505 --> 00:13:40,520
이렇게 하면 됩니다.

277
00:13:41,380 --> 00:13:47,093
한 레이어의 모든 활성화는 한 레이어와 

278
00:13:47,093 --> 00:13:52,806
다음 레이어의 특정 뉴런 사이의 연결에 

279
00:13:52,806 --> 00:13:58,000
해당하는 매트릭스로 열로 구성합니다.

280
00:13:58,540 --> 00:14:01,928
즉, 이러한 가중치에 따라 첫 번째 레이어에서 

281
00:14:01,928 --> 00:14:05,708
활성화의 가중치 합을 구하면 여기 왼쪽에 있는 모든 

282
00:14:05,708 --> 00:14:09,228
항목의 행렬 벡터 곱의 항 중 하나에 해당한다는 

283
00:14:09,228 --> 00:14:09,880
뜻입니다.

284
00:14:14,000 --> 00:14:17,554
머신러닝의 많은 부분이 선형 대수학을 잘 이해하는 

285
00:14:17,554 --> 00:14:20,855
데 달려 있기 때문에 행렬과 행렬 벡터 곱셈의 

286
00:14:20,855 --> 00:14:24,283
의미를 시각적으로 잘 이해하고 싶은 분들은 제가 

287
00:14:24,283 --> 00:14:27,965
작성한 선형 대수학 시리즈, 특히 3장을 살펴보시기 

288
00:14:27,965 --> 00:14:28,600
바랍니다.

289
00:14:29,240 --> 00:14:33,055
다시 표현식으로 돌아가서, 각각의 값에 편향을 

290
00:14:33,055 --> 00:14:37,310
독립적으로 더하는 대신 모든 편향을 벡터로 구성하고 

291
00:14:37,310 --> 00:14:41,419
전체 벡터를 이전 행렬 벡터 곱에 더하는 방식으로 

292
00:14:41,419 --> 00:14:42,300
표현합니다.

293
00:14:43,280 --> 00:14:46,972
그런 다음 마지막 단계로 여기 바깥쪽을 시그모이드로 

294
00:14:46,972 --> 00:14:50,665
감싸고, 이것이 나타내는 것은 결과 벡터의 각 특정 

295
00:14:50,665 --> 00:14:54,103
구성 요소에 시그모이드 함수를 내부에 적용한다는 

296
00:14:54,103 --> 00:14:54,740
것입니다.

297
00:14:55,940 --> 00:14:59,538
따라서 이 가중치 행렬과 벡터를 자체 심볼로 

298
00:14:59,538 --> 00:15:03,137
적어두면 한 레이어에서 다음 레이어로의 전체 

299
00:15:03,137 --> 00:15:06,735
활성화 전환을 매우 간결하고 깔끔한 표현으로 

300
00:15:06,735 --> 00:15:10,765
전달할 수 있으며, 많은 라이브러리가 행렬 곱셈을 

301
00:15:10,765 --> 00:15:14,796
최적화하기 때문에 관련 코드가 훨씬 간단하고 훨씬 

302
00:15:14,796 --> 00:15:15,660
빨라집니다.

303
00:15:17,820 --> 00:15:19,640
앞서 뉴런은 단순히 숫자를 저장하는 

304
00:15:19,640 --> 00:15:21,460
존재라고 말씀드린 것을 기억하시나요?

305
00:15:22,220 --> 00:15:26,173
물론 뉴런이 보유하는 구체적인 숫자는 입력하는 

306
00:15:26,173 --> 00:15:30,432
이미지에 따라 달라지므로 각 뉴런을 이전 레이어의 

307
00:15:30,432 --> 00:15:34,538
모든 뉴런의 출력을 받아 0과 1 사이의 숫자를 

308
00:15:34,538 --> 00:15:38,340
뱉어내는 함수로 생각하는 것이 더 정확합니다.

309
00:15:39,200 --> 00:15:41,533
실제로 전체 네트워크는 784개의 

310
00:15:41,533 --> 00:15:44,112
숫자를 입력으로 받아 10개의 숫자를 

311
00:15:44,112 --> 00:15:47,060
출력으로 뱉어내는 하나의 함수에 불과합니다.

312
00:15:47,560 --> 00:15:50,743
특정 패턴을 포착하는 가중치와 편향의 형태로 

313
00:15:50,743 --> 00:15:54,563
13,000개의 매개 변수가 포함되고 많은 행렬 벡터 

314
00:15:54,563 --> 00:15:58,128
곱과 시그모이드 스퀴시화 함수를 반복하는 엄청나게 

315
00:15:58,128 --> 00:16:01,694
복잡한 함수이지만, 그럼에도 불구하고 그냥 함수일 

316
00:16:01,694 --> 00:16:05,132
뿐이며 어떤 면에서는 복잡해 보이는 것이 오히려 

317
00:16:05,132 --> 00:16:06,660
안심이 되기도 합니다.

318
00:16:07,340 --> 00:16:09,870
이보다 더 간단했다면 숫자를 인식하는 

319
00:16:09,870 --> 00:16:12,280
데 어떤 희망을 가질 수 있었을까요?

320
00:16:13,340 --> 00:16:14,700
그렇다면 그 도전은 어떻게 이루어질까요?

321
00:16:15,080 --> 00:16:16,982
이 네트워크는 어떻게 데이터를 보고 

322
00:16:16,982 --> 00:16:19,360
적절한 가중치와 편향성을 학습할 수 있을까요?

323
00:16:20,140 --> 00:16:21,848
다음 동영상에서 이 내용을 보여드리고, 

324
00:16:21,848 --> 00:16:23,790
우리가 보고 있는 이 특정 네트워크가 실제로 

325
00:16:23,790 --> 00:16:26,120
어떤 일을 하고 있는지 조금 더 자세히 살펴보겠습니다.

326
00:16:27,580 --> 00:16:30,092
이제 동영상이나 새 동영상이 언제 나오는지 

327
00:16:30,092 --> 00:16:33,128
알림을 받으려면 구독을 신청해야 한다고 생각하지만, 

328
00:16:33,128 --> 00:16:35,117
현실적으로 여러분 대부분은 실제로 

329
00:16:35,117 --> 00:16:37,420
YouTube에서 알림을 받지 않으시죠?

330
00:16:38,020 --> 00:16:39,992
더 솔직하게 말하자면 구독을 하면 

331
00:16:39,992 --> 00:16:42,482
YouTube 추천 알고리즘의 기반이 되는 

332
00:16:42,482 --> 00:16:44,973
신경망이 사용자가 이 채널의 콘텐츠를 보고 

333
00:16:44,973 --> 00:16:47,880
싶어한다고 믿도록 준비된 상태로 만들 수 있습니다.

334
00:16:48,560 --> 00:16:49,940
자세한 내용은 계속 확인해 주세요.

335
00:16:50,760 --> 00:16:52,289
Patreon에서 이 동영상을 후원해주시는 

336
00:16:52,289 --> 00:16:53,500
모든 분들께 진심으로 감사드립니다.

337
00:16:54,000 --> 00:16:56,921
이번 여름에 확률 시리즈 진행이 조금 더뎠지만, 

338
00:16:56,921 --> 00:16:59,519
이 프로젝트가 끝나면 다시 시작하려고 하니 

339
00:16:59,519 --> 00:17:01,900
고객 여러분도 업데이트를 기대해 주세요.

340
00:17:03,600 --> 00:17:05,677
마지막으로 딥러닝의 이론적 측면에서 박사 

341
00:17:05,677 --> 00:17:07,484
학위를 취득하고 현재 Amplify 

342
00:17:07,484 --> 00:17:09,561
Partners라는 벤처 캐피탈 회사에서 

343
00:17:09,561 --> 00:17:11,819
일하고 있으며 이 동영상에 대한 자금 일부를 

344
00:17:11,819 --> 00:17:13,987
친절하게 제공한 레이샤 리와 함께 이야기를 

345
00:17:13,987 --> 00:17:14,619
마무리합니다.

346
00:17:15,460 --> 00:17:17,055
그래서 레이샤는 이 시그모이드 

347
00:17:17,055 --> 00:17:19,119
함수를 빨리 언급해야 한다고 생각합니다.

348
00:17:19,700 --> 00:17:21,941
제가 알기로 초기 네트워크에서는 이를 

349
00:17:21,941 --> 00:17:24,289
사용하여 관련 가중치를 0과 1 사이의 

350
00:17:24,289 --> 00:17:26,637
간격으로 쪼개는데, 이는 뉴런이 비활성 

351
00:17:26,637 --> 00:17:29,840
또는 활성 상태라는 생물학적 비유에서 착안한 것입니다.

352
00:17:30,280 --> 00:17:30,300
맞습니다.

353
00:17:30,560 --> 00:17:32,184
하지만 최신 네트워크에서 시그모이드를 

354
00:17:32,184 --> 00:17:34,040
실제로 사용하는 경우는 상대적으로 적습니다.

355
00:17:34,320 --> 00:17:34,320
네

356
00:17:34,440 --> 00:17:35,540
좀 구식이지 않나요?

357
00:17:35,760 --> 00:17:37,421
네, 오히려 릴루가 훨씬 더 

358
00:17:37,421 --> 00:17:38,980
훈련하기 쉬운 것 같습니다.

359
00:17:39,400 --> 00:17:42,340
릴루는 정류된 선형 유닛의 약자입니까?

360
00:17:42,680 --> 00:17:47,211
예, 동영상에서 설명하신 것처럼 최대값 0과 a를 

361
00:17:47,211 --> 00:17:49,962
취하는 이런 종류의 함수이며, 

362
00:17:49,962 --> 00:17:54,332
이 함수의 동기는 부분적으로 뉴런이 활성화되거나 

363
00:17:54,332 --> 00:17:59,187
활성화되지 않는 방식에 대한 생물학적 유추에서 비롯된 

364
00:17:59,187 --> 00:18:03,880
것으로, 특정 임계값을 통과하면 정체 함수가 되지만 

365
00:18:03,880 --> 00:18:08,412
그렇지 않으면 활성화되지 않아 0이 되므로 일종의 

366
00:18:08,412 --> 00:18:10,840
단순화라고 할 수 있습니다.

367
00:18:11,160 --> 00:18:15,702
시그모이드를 사용하면 훈련에 도움이 되지 않거나 

368
00:18:15,702 --> 00:18:20,413
훈련이 매우 어려웠고, 사람들은 릴루를 시도했는데 

369
00:18:20,413 --> 00:18:24,620
놀랍도록 깊은 신경망에 매우 잘 작동했습니다.

370
00:18:25,100 --> 00:18:25,640
알겠습니다, 알리샤 감사합니다.

