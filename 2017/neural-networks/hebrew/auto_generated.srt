1
00:00:04,220 --> 00:00:05,400
זה 3. 

2
00:00:06,060 --> 00:00:10,829
זה כתוב בצורה מרושלת ומוצג ברזולוציה נמוכה מאוד של 28x28 פיקסלים, 

3
00:00:10,829 --> 00:00:13,720
אבל המוח שלכם לא מתקשה לזהות את זה כ-3. 

4
00:00:14,340 --> 00:00:18,960
ואני רוצה שתקדישו רגע כדי להעריך כמה זה מטורף שהמוח יכול לעשות את זה כל כך בקלות. 

5
00:00:19,700 --> 00:00:23,696
כלומר, זה, זה וזה כולם ניתנים לזיהוי בתור 3, למרות 

6
00:00:23,696 --> 00:00:28,320
שהערכים הספציפיים של כל פיקסל שונים מאוד מתמונה אחת לאחרת. 

7
00:00:28,900 --> 00:00:32,958
התאים הספציפיים בעין שלכם שמגיבים כשאתם רואים את ה-3 

8
00:00:32,958 --> 00:00:36,940
הזה שונים מאוד מאלה שמגיבים כשאתם רואים את ה-3 הזה. 

9
00:00:37,520 --> 00:00:43,477
אבל משהו במוח החכם בטירוף הזה שלכם מבין שכל אלה מייצגים את אותו רעיון, 

10
00:00:43,477 --> 00:00:48,260
ובו בזמן מזהה תמונות אחרות כמיצגות רעיונות נפרדים משלהם. 

11
00:00:49,220 --> 00:00:54,727
אבל אם הייתי אומר לכם, היי, שבו ותכתבו עבורי תוכנית שמקבלת רשת 

12
00:00:54,727 --> 00:01:01,546
של 28x28 פיקסלים ומוציאה מספר בודד בין 0 ל-10, שאומר לכם מה היא חושבת שהספרה, 

13
00:01:01,546 --> 00:01:06,180
ובכן, המשימה הופכת מטריוויאלית לחלוטין, לקשה להחריד. 

14
00:01:07,160 --> 00:01:10,775
אלא אם אתם מנותקים מהעולם, אני חושב שאני לא באמת צריך לתת 

15
00:01:10,775 --> 00:01:14,640
מוטיבציה לחשיבות של למידת מכונה ורשתות נוריונים להווה ולעתיד. 

16
00:01:15,120 --> 00:01:19,158
אבל מה שאני רוצה לעשות כאן זה להראות לכם מהי בעצם רשת נוירונים, 

17
00:01:19,158 --> 00:01:24,460
בלי להניח רקע מקדים, וכדי לעזור לכם להבין מה היא עושה, לא כבאזוורד אלא באופן מתמטי. 

18
00:01:25,020 --> 00:01:29,226
התקווה שלי היא רק שתצאו עם הבנה של המוטיבציה מאחורי המבנה הכללי, 

19
00:01:29,226 --> 00:01:34,340
ושתרגישו שאתם יודעים מה זה אומר כשאתם קוראים או שומעים על רשת נוירונים "לומדת".

20
00:01:35,360 --> 00:01:40,260
הסרטון הזה יוקדש רק למבנה, והסרטון הבא יעסוק בלמידה. 

21
00:01:40,960 --> 00:01:46,040
מה שאנחנו הולכים לעשות זה לבנות רשת נוירונים שיכולה ללמוד לזהות ספרות שכתובות בכתב יד. 

22
00:01:49,360 --> 00:01:54,041
זו דוגמה יחיסת סטנדרטית להצגת הנושא, ואני שמח להישאר עם המצב הקיים כאן, 

23
00:01:54,041 --> 00:01:59,308
כי בסוף שני הסרטונים אני רוצה להפנות אתכם לכמה משאבים טובים שבהם תוכל ללמוד עוד, 

24
00:01:59,308 --> 00:02:03,080
ושם תוכלו להוריד את הקוד שעושה זאת ולשחק איתו במחשב שלכם. 

25
00:02:05,040 --> 00:02:09,778
יש הרבה וריאציות של רשתות נוירונים, ובשנים האחרונות חלה סוג של 

26
00:02:09,778 --> 00:02:14,291
פריחה במחקר לכיוון הווריאציות האלה, אבל בשני סרטוני ההיכרות 

27
00:02:14,291 --> 00:02:19,180
האלה אתם ואני הולכים להסתכל על הגרסה הפשוטה ביותר בלי שום תוספות.

28
00:02:19,860 --> 00:02:24,739
זה סוג של תנאי הכרחי להבנת כל אחת מהגרסאות המודרניות והחזקות יותר, 

29
00:02:24,739 --> 00:02:28,600
ותאמינו לי שעדיין יש לה מורכבות לא פשוטה שנצטרך להבין

30
00:02:29,120 --> 00:02:33,882
אבל אפילו הגרסה הפשוטה ביותר הזו תוכל ללמוד לזהות ספרות בכתב יד, 

31
00:02:33,882 --> 00:02:36,520
וזה די מגניב שמחשב יכול לעשות את זה.

32
00:02:37,480 --> 00:02:42,280
ובאותו הזמן נראה שהיא לא משיגה כל מה שקיווינו. 

33
00:02:43,380 --> 00:02:48,500
כפי שהשם מרמז, רשתות נוירונים נוצרו בהשראת המוח, אבל בואו נפרק את זה. 

34
00:02:48,520 --> 00:02:51,660
מהם הנוירונים, ובאיזה מובן הם קשורים זה לזה? 

35
00:02:52,500 --> 00:02:58,434
ברגע כשאני אומר נוירון, כל מה שאני רוצה שתחשבו עליו זה דבר שמכיל מספר, 

36
00:02:58,434 --> 00:03:00,440
ספציפית מספר בין 0 ל-1. 

37
00:03:00,680 --> 00:03:02,560
זה באמת לא יותר מזה. 

38
00:03:03,780 --> 00:03:08,911
לדוגמה, הרשת מתחילה עם קבוצה של נוירונים שמתאימים לכל אחד 

39
00:03:08,911 --> 00:03:14,220
מ-28 על 28 פיקסלים של תמונת הקלט, שהם 784 נוירונים בסך הכל. 

40
00:03:14,700 --> 00:03:20,024
כל אחד מאלה מכיל מספר המייצג את ערך גווני האפור של הפיקסל המתאים, 

41
00:03:20,024 --> 00:03:24,380
החל מ-0 עבור פיקסלים שחורים ועד 1 עבור פיקסלים לבנים. 

42
00:03:25,300 --> 00:03:29,621
המספר הזה בתוך הנוירון נקרא האקטיבציה שלו, והתמונה שאולי יש 

43
00:03:29,621 --> 00:03:34,160
לכם בראש היא שכל נוירון נדלק כאשר האקטיבציה שלו היא מספר גבוה. 

44
00:03:36,720 --> 00:03:41,860
אז כל 784 הנוירונים האלה מהווים את השכבה הראשונה של הרשת שלנו. 

45
00:03:46,500 --> 00:03:51,360
עכשיו עם נקפוץ לשכבה האחרונה, לה יש 10 נוירונים, שכל אחד מייצג את אחת הספרות. 

46
00:03:52,040 --> 00:03:56,671
האקטיבציה בנוירונים האלה, שוב מספר שהוא בין 0 ל-1, 

47
00:03:56,671 --> 00:04:02,120
מייצגת עד כמה המערכת חושבת שתמונה נתונה מתאימה לספרה נתונה. 

48
00:04:03,040 --> 00:04:08,320
יש גם כמה שכבות ביניים שנקראות שכבות נסתרות, שכרגע אמורות להוות 

49
00:04:08,320 --> 00:04:13,600
סימן שאלה ענק לאיך לכל הרוחות התהליך הזה של זיהוי הספרות יטופל. 

50
00:04:14,260 --> 00:04:20,560
ברשת הזו בחרתי שתי שכבות נסתרות, כל אחת עם 16 נוירונים, ובכנות זו בחירה די שרירותית. 

51
00:04:21,019 --> 00:04:25,631
למען האמת, בחרתי שתי שכבות כי זה מתאים למוטיבציה שאתן למבנה בעוד רגע, 

52
00:04:25,631 --> 00:04:28,200
ו-16, זה היה סתם מספר נחמד שנכנס במסך. 

53
00:04:28,780 --> 00:04:32,340
בפועל יש כאן הרבה מקום להתנסות במבנים שונים. 

54
00:04:33,020 --> 00:04:38,480
באופן שבו הרשת פועלת, האקטיבציות בשכבה אחת קובעות את האקטיבציות של השכבה הבאה. 

55
00:04:39,200 --> 00:04:43,890
וכמובן הלב של הרשת כמנגנון עיבוד מידע מסתכם בבדיוק איך 

56
00:04:43,890 --> 00:04:48,580
אותן אקטיבציות משכבה אחת גורמות לאקטיבציות בשכבה הבאה. 

57
00:04:49,140 --> 00:04:53,222
זה אמור להקביל באופן חלקי לאופן שבו ברשתות ביולוגיות של נוירונים 

58
00:04:53,222 --> 00:04:57,180
כשקבוצות מסוימות של נוירונים מופעלות, הן מפעילות קבוצות אחרות. 

59
00:04:58,120 --> 00:05:03,400
עכשיו הרשת שאני מראה כאן כבר אומנה לזהות ספרות, ותנו לי להראות לכם למה אני מתכוון. 

60
00:05:03,640 --> 00:05:09,705
זה אומר שאם אתם מכניסים תמונה שמדליקה את כל 784 הנוירונים של שכבת הקלט לפי 

61
00:05:09,705 --> 00:05:16,499
הבהירות של כל פיקסל בתמונה, דפוס ההפעלה הזה גורם לאיזה דפוס מאוד ספציפי בשכבה הבאה, 

62
00:05:16,499 --> 00:05:22,080
שגורם לדפוס כלשהו בשכבה שאחרי, מה שלבסוף נותן דפוס כלשהו בשכבת הפלט. 

63
00:05:22,560 --> 00:05:26,092
והנוירון הבהיר ביותר של שכבת הפלט הוא, כביכול, 

64
00:05:26,092 --> 00:05:29,400
הבחירה של הרשת של באיזו ספרה התמונה מייצגת. 

65
00:05:32,560 --> 00:05:38,105
ולפני שנקפוץ למתמטיקה שמאחורי איך שכבה אחת משפיעה על השכבה הבאה, או איך האימון עובד, 

66
00:05:38,105 --> 00:05:43,520
בואו נדבר רק על למה זה בכלל הגיוני לצפות ממבנה שכבות כזה להתנהג בצורה אינטליגנטית. 

67
00:05:44,060 --> 00:05:45,220
למה אנחנו מצפים כאן? 

68
00:05:45,400 --> 00:05:47,600
מהי התקווה הטובה ביותר למה ששכבות הביניים האלה עשויות לעשות? 

69
00:05:48,920 --> 00:05:53,520
ובכן, כאשר אתם או אני מזהים ספרות, אנחנו מזהים רכיבים שונים. 

70
00:05:54,200 --> 00:05:56,820
ל-9 יש לולאה למעלה וקו מימין. 

71
00:05:57,380 --> 00:06:01,180
ל-8 יש גם לולאה למעלה, אבל מודבקת לה לולאה נוספת מלמטה. 

72
00:06:01,980 --> 00:06:06,820
4 בעצם מתפרק לשלושה קווים ספציפיים, ודברים כאלה. 

73
00:06:07,600 --> 00:06:12,993
עכשיו בעולם מושלם, אנחנו עשויים לקוות שכל נוירון בשכבה האחת לפי אחרונה 

74
00:06:12,993 --> 00:06:18,234
מתכתב עם אחד מתתי-הרכיבים האלה, שבכל פעם שאתם מזינים תמונה עם, נגיד, 

75
00:06:18,234 --> 00:06:23,780
לולאה למעלה, כמו 9 או 8, יש נוירון ספציפי שהאקטיבציה שלו תהיה קרובה ל-1. 

76
00:06:24,500 --> 00:06:27,436
ואני לא מתכוון ללולאה הספציפית הזו של פיקסלים, 

77
00:06:27,436 --> 00:06:31,560
התקווה היא שכל דפוס לולאה כללי לכיוון למעלה מפעיל את הנוירון הזה. 

78
00:06:32,440 --> 00:06:36,314
בדרך זו, מעבר מהשכבה השלישית לשכבה האחרונה רק מצריך 

79
00:06:36,314 --> 00:06:40,040
ללמוד איזה שילוב של רכיבי משנה מתאים לאילו ספרות. 

80
00:06:41,000 --> 00:06:44,909
כמובן, זה רק מעביר את הבעיה להמשך הדרך, כי איך תזהה את רכיבי המשנה האלה, 

81
00:06:44,909 --> 00:06:47,640
או אפילו תלמד מה צריכים להיות רכיבי המשנה הנכונים? 

82
00:06:48,060 --> 00:06:53,060
ועדיין לא דיברתי אפילו על איך שכבה אחת משפיעה על השניה, אבל תתקדמו איתי עם זה לרגע. 

83
00:06:53,680 --> 00:06:56,680
זיהוי לולאה יכול גם להתפרק לתת-בעיות. 

84
00:06:57,280 --> 00:07:02,780
אחת הדרכים ההגיוניות לעשות זאת תהיה לזהות תחילה את הקצוות הקטנים השונים המרכיבים אותה. 

85
00:07:03,780 --> 00:07:08,654
באופן דומה, קו ארוך כמו מהסוג שאתם עשויים לראות בספרות 1 או 4 או 7, ובכן, 

86
00:07:08,654 --> 00:07:14,320
זה באמת רק קצה ארוך, או אולי אתם חושבים על זה כעל דפוס מסוים של כמה קצוות קטנים יותר. 

87
00:07:15,140 --> 00:07:18,969
אז אולי התקווה שלנו היא שכל נוירון בשכבה השנייה 

88
00:07:18,969 --> 00:07:22,720
של הרשת מתיחס לקצוות הקטנים הרלוונטיים השונים. 

89
00:07:23,540 --> 00:07:28,591
אולי כאשר תמונה כמו זו נכנסת, היא מאירה את כל הנוירונים הקשורים 

90
00:07:28,591 --> 00:07:33,958
בסביבות 8 עד 10 קצוות קטנים ספציפיים, אשר בתורם מאירים את הנוירונים 

91
00:07:33,958 --> 00:07:39,720
הקשורים ללולאה העליונה ולקו אנכי ארוך, ואלו מאירים את הנוירון הקשור ל-9. 

92
00:07:40,680 --> 00:07:44,190
האם זה מה שהרשת הסופית שלנו עושה או לא, זו שאלה אחרת, 

93
00:07:44,190 --> 00:07:47,180
שאלה שאחזור אליה ברגע שנראה איך לאמן את הרשת. 

94
00:07:47,500 --> 00:07:52,540
אבל זו תקווה שיכולה להיות לנו, מעין מטרה עם מבנה השכבות כמו זה. 

95
00:07:53,160 --> 00:07:56,535
יתר על כן, אתם יכולים לדמיין איך היכולת לזהות קצוות 

96
00:07:56,535 --> 00:08:00,300
ודפוסים כאלו תהיה שימושית עבור משימות זיהוי תמונות אחרות. 

97
00:08:00,880 --> 00:08:04,500
ואפילו מעבר לזיהוי תמונה, יש כל מיני דברים אינטליגנטיים 

98
00:08:04,500 --> 00:08:07,280
שאולי תרצו לעשות שמתפרקים לשכבות של הפשטה. 

99
00:08:08,040 --> 00:08:12,679
ניתוח דיבור, למשל, כרוך בנטילת אודיו גולמי ובחירת צלילים מובחנים, 

100
00:08:12,679 --> 00:08:16,404
המשלבים יצירת הברות מסוימות, המשתלבות ויוצרות מילים, 

101
00:08:16,404 --> 00:08:20,060
המשתלבות ליצירת ביטויים ומחשבות מופשטות יותר, וכו'. 

102
00:08:21,100 --> 00:08:25,404
אבל אם נחזור לאופן שבו כל זה עובד בפועל, דמיינו את עצמכם כעת 

103
00:08:25,404 --> 00:08:29,920
מתכננים כיצד הפעלה בשכבה אחת עשויות לקבוע את ההפעלה בשכבה הבאה. 

104
00:08:30,860 --> 00:08:35,767
המטרה היא לקבל מנגנון כלשהו שיכול לשלב פיקסלים לקצוות, 

105
00:08:35,767 --> 00:08:38,980
או קצוות לתבניות, או דפוסים לספרות. 

106
00:08:39,440 --> 00:08:44,981
וכדי להתמקד בדוגמה אחת ספציפית, נניח שהתקווה היא שנוירון 

107
00:08:44,981 --> 00:08:50,620
מסוים בשכבה השנייה יקלוט האם לתמונה יש קצה באזור הזה כאן. 

108
00:08:51,440 --> 00:08:55,100
השאלה העומדת על הפרק היא אילו פרמטרים צריכים להיות לרשת? 

109
00:08:55,640 --> 00:09:01,710
אילו כפתורים וחוגות אתם צריכים להיות מסוגלים לכוונן כך שזה יהיה מספיק כדי ללכוד את הדפוס 

110
00:09:01,710 --> 00:09:07,780
הזה, או כל דפוס פיקסלים אחר, או את הדפוס שכמה קצוות יכולים ליצור לולאה, ועוד דברים כאלה? 

111
00:09:08,720 --> 00:09:15,560
ובכן, מה שנעשה זה להקצות משקל לכל אחד מהקשרים בין הנוירון שלנו לנוירונים מהשכבה הראשונה. 

112
00:09:16,320 --> 00:09:17,700
המשקולות האלה הן רק מספרים. 

113
00:09:18,540 --> 00:09:25,423
לאחר מכן קחו את כל ההפעלות הללו מהשכבה הראשונה וחשבו את הסכום המשוקלל שלהן לפי משקלים אלו.

114
00:09:25,423 --> 00:09:25,500
 

115
00:09:27,700 --> 00:09:32,012
אני מוצא שזה מועיל לחשוב על המשקלים האלו כמאורגנים ברשת קטנה משלהם, 

116
00:09:32,012 --> 00:09:35,754
ואני הולך להשתמש בפיקסלים ירוקים כדי לציין משקלים חיוביים, 

117
00:09:35,754 --> 00:09:40,574
ובפיקסלים אדומים כדי לציין משקלים שליליים, כאשר הבהירות של הפיקסל היא תיאור 

118
00:09:40,574 --> 00:09:41,780
רופף של ערך המשקל. 

119
00:09:42,780 --> 00:09:46,610
אם היינו מאפסים את המשקלים הקשורים כמעט לכל הפיקסלים, 

120
00:09:46,610 --> 00:09:50,016
למעט כמה משקלים חיוביים באזור זה שאכפת לנו מהם, 

121
00:09:50,016 --> 00:09:55,053
אז לקחת את הסכום המשוקלל של כל ערכי הפיקסלים פשוט מסתכם בחיבור של ערכי 

122
00:09:55,053 --> 00:09:57,820
הפיקסלים רק בתוך האזור שאכפת לנו ממנו. 

123
00:09:59,140 --> 00:10:03,005
ואם באמת רציתם להבין אם יש כאן קצה, מה שאתם יכולים לעשות 

124
00:10:03,005 --> 00:10:06,600
זה שיהיו כמה משקלים שליליים הקשורים לפיקסלים שמסביב. 

125
00:10:07,480 --> 00:10:12,642
אז הסכום הוא הגדול ביותר כאשר הפיקסלים האמצעיים האלה בהירים אבל הפיקסלים שמסביב כהים יותר.

126
00:10:12,642 --> 00:10:12,700
 

127
00:10:14,260 --> 00:10:18,310
כשאתם מחשבים סכום משוקלל כזה, אתם עשויים לקבל כל מספר, 

128
00:10:18,310 --> 00:10:23,540
אבל עבור הרשת הזו מה שאנחנו רוצים זה שההפעלה תהיה ערך כלשהו בין 0 ל-1. 

129
00:10:24,120 --> 00:10:28,059
אז דבר נפוץ לעשות הוא להעביר את הסכום המשוקלל הזה לאיזו 

130
00:10:28,059 --> 00:10:32,140
פונקציה שדוחסת את טווח המספרים האמיתיים לטווח שבין 0 ל-1. 

131
00:10:32,460 --> 00:10:37,420
ופונקציה נפוצה שעושה זאת נקראת הפונקציה הסיגמואידית, הידועה גם בתור עקומה לוגיסטית. 

132
00:10:38,000 --> 00:10:43,996
בעיקרון, לקלטים שליליים ערכה מתקרב מאוד ל-0, לקלטים חיוביים מתקרב מאוד ל-1, 

133
00:10:43,996 --> 00:10:46,600
והיא רק גדלה בהתמדה סביב הקלט 0. 

134
00:10:49,120 --> 00:10:56,360
אז ההפעלה של הנוירון כאן היא בעצם מדד למידת החיוביות של הסכום המשוקלל הרלוונטי. 

135
00:10:57,540 --> 00:11:01,880
אבל אולי זה לא שאתם רוצים שהנוירון יידלק כשהסכום המשוקלל גדול מ-0. 

136
00:11:02,280 --> 00:11:06,360
אולי אתם רוצים שהוא יהיה פעיל רק כשהסכום גדול מ-10 נניח. 

137
00:11:06,840 --> 00:11:10,260
כלומר, אתם רוצים הטיה מסוימת כדי שהוא לא יהיה פעיל. 

138
00:11:11,380 --> 00:11:15,109
מה שנעשה הוא פשוט להוסיף מספר אחר, כמו מינוס  10, 

139
00:11:15,109 --> 00:11:19,660
לסכום המשוקלל הזה לפני המעעבר לדחיסה דרך פונקציית הסיגמואיד. 

140
00:11:20,580 --> 00:11:22,440
המספר הנוסף הזה נקרא הטיה. 

141
00:11:23,460 --> 00:11:28,423
אז המשקולות אומרות לכם איזה דפוס פיקסלים הנוירון הזה בשכבה השנייה קולט, 

142
00:11:28,423 --> 00:11:34,076
וההטיה אומרת לכם כמה גבוה הסכום המשוקלל צריך להיות לפני שהנוירון יתחיל להיות פעיל 

143
00:11:34,076 --> 00:11:35,180
בצורה משמעותית. 

144
00:11:36,120 --> 00:11:37,680
וזה רק נוירון אחד. 

145
00:11:38,280 --> 00:11:45,756
כל נוירון אחר בשכבה הזו יתחבר לכל הנוירונים של 784 פיקסלים מהשכבה הראשונה, 

146
00:11:45,756 --> 00:11:50,940
ולכל אחד מאותם 784 חיבורים יש משקל משלו הקשור אליו. 

147
00:11:51,600 --> 00:11:54,360
כמו כן, לכל אחד יש הטיה מסוימת, מספר אחר שאתם 

148
00:11:54,360 --> 00:11:57,600
מוסיפים על הסכום המשוקלל לפני דחיסה שלו עם הסיגמואיד. 

149
00:11:58,110 --> 00:11:59,540
וזה הרבה לחשוב עליו! 

150
00:11:59,960 --> 00:12:07,980
עם השכבה הנסתרת הזו של 16 נוירונים, זה בסך הכל 784 כפול 16 משקלים, יחד עם 16 הטיות. 

151
00:12:08,840 --> 00:12:11,940
וכל זה הוא רק החיבורים מהשכבה הראשונה לשניה. 

152
00:12:12,520 --> 00:12:17,340
לקשרים בין השכבות האחרות יש גם קבוצה של משקלים והטיות הקשורות אליהם. 

153
00:12:18,340 --> 00:12:23,800
בכל מקרה, לרשת הזו יש כמעט בדיוק 13,000 משקלים והטיות. 

154
00:12:23,800 --> 00:12:29,960
13,000 כפתורים וחוגות שניתן לכוונן ולסובב כדי לגרום לרשת הזו להתנהג בדרכים שונות. 

155
00:12:31,040 --> 00:12:36,363
אז כשאנחנו מדברים על למידה, הכוונה היא לגרום למחשב למצוא כיוונון 

156
00:12:36,363 --> 00:12:41,360
עבור כל המספרים הרבים האלה, כך שהוא יפתור את הבעיה שעל הפרק. 

157
00:12:42,620 --> 00:12:47,247
ניסוי מחשבתי אחד שהוא מהנה ומפחיד בעת ובעונה אחת הוא לדמיין 

158
00:12:47,247 --> 00:12:51,335
איך יושבים ומגדירים את כל המשקולות וההטיות האלה ביד, 

159
00:12:51,335 --> 00:12:56,580
כך שהשכבה השנייה תופסת את הקצוות, השכבה השלישית תופסת דפוסים, וכו '.

160
00:12:56,980 --> 00:13:02,208
אני אישית מוצא את זה מספק לא רק להתייחס לרשת כאל קופסה שחורה מוחלטת, 

161
00:13:02,208 --> 00:13:07,587
כי כשהרשת לא מתפקדת כמו שאתם חושבים, אם בניתם קצת הבנה לגבי המשמעות של 

162
00:13:07,587 --> 00:13:14,180
המשקולות וההטיות האלה, יש לכם נקודת התחלה להתנסויות כיצד לשנות את המבנה כדי לשפר אותו. 

163
00:13:14,960 --> 00:13:18,353
או כשהרשת אכן עובדת, אבל לא מהסיבות שחשבתם עליהן, 

164
00:13:18,353 --> 00:13:23,919
העמקה במה שהמשקלים וההטיות עושים היא דרך טובה לאתגר את ההנחות שלכם ולחשוף את מלוא 

165
00:13:23,919 --> 00:13:25,820
המרחב של הפתרונות האפשריים. 

166
00:13:26,840 --> 00:13:30,680
אגב, הפונקציה בפועל כאן קצת מסורבלת לרישום, אתם לא חושבים? 

167
00:13:32,500 --> 00:13:37,140
אז הרשו לי להראות לכם דרך קומפקטית יותר מבחינה סימון שבה קשרים אלו מיוצגים. 

168
00:13:37,660 --> 00:13:40,520
כך תראו את זה אם תבחרו לקרוא עוד על רשתות ניורונים. 

169
00:13:41,380 --> 00:13:40,520
ארגנו את כל ההפעלות משכבה אחת לעמודה בתור וקטור. 

170
00:13:41,380 --> 00:13:49,962
לאחר מכן ארגנו את כל המשקולות כמטריצה, כאשר כל שורה של המטריצה 

171
00:13:49,962 --> 00:13:58,000
הזו מתאימה לחיבורים בין שכבה אחת לנוירון מסוים בשכבה הבאה. 

172
00:13:58,540 --> 00:14:04,094
מה שזה אומר הוא שלקיחת הסכום המשוקלל של ההפעלה בשכבה הראשונה לפי משקלים 

173
00:14:04,094 --> 00:14:09,880
אלו מתאימה לאחד האיברים במכפלת הווקטור במטריצה של כל מה שיש לנו כאן משמאל. 

174
00:14:14,000 --> 00:14:18,773
אגב, כל כך הרבה מלמידת מכונה מסתכם רק בהבנה טובה של אלגברה לינארית, 

175
00:14:18,773 --> 00:14:24,669
אז לכל אחד מכם שרוצה הבנה ויזואלית יפה של מטריצות ומה המשמעות של כפל וקטור ומטריצה, 

176
00:14:24,669 --> 00:14:28,600
תסתכלו על הסדרה שעשיתי על אלגברה לינארית, במיוחד פרק 3. 

177
00:14:29,240 --> 00:14:34,877
בחזרה לביטוי שלנו, במקום לדבר על הוספת ההטיה לכל אחד מהערכים הללו באופן עצמאי, 

178
00:14:34,877 --> 00:14:38,874
אנחנו מייצגים אותו על ידי ארגון כל ההטיות הללו בווקטור, 

179
00:14:38,874 --> 00:14:42,300
והוספת הווקטור כולו לתוצר ווקטור המטריצה הקודם. 

180
00:14:43,280 --> 00:14:49,050
ואז כשלב אחרון, אעטוף בסיגמואיד כאן, ומה שזה אמור לייצג זה שאתם הולכים 

181
00:14:49,050 --> 00:14:54,740
להחיל את פונקציית הסיגמואיד על כל רכיב ספציפי של הווקטור שנוצר בפנים. 

182
00:14:55,940 --> 00:15:01,269
אז ברגע שאתם כותבים את מטריצת המשקל הזו והווקטורים האלה כסמלים משלהם, 

183
00:15:01,269 --> 00:15:07,589
אתם יכולים לתקשר את המעבר המלא של הפעלות משכבה אחת לאחרת בביטוי קטן ומסודר במיוחד, 

184
00:15:07,589 --> 00:15:12,309
וזה הופך את הקוד הרלוונטי להרבה יותר פשוט וגם הרבה יותר מהיר, 

185
00:15:12,309 --> 00:15:15,660
מכיוון שספריות רבות מייעלות את כפל המטריצה. 

186
00:15:17,820 --> 00:15:21,460
זוכרים איך קודם לכן אמרתי שהנוירונים האלה הם פשוט דברים שמכילים מספרים? 

187
00:15:22,220 --> 00:15:27,942
ובכן, כמובן שהמספרים הספציפיים שהם מחזיקים תלויים בתמונה שאתם מכניסים, 

188
00:15:27,942 --> 00:15:31,892
כך שלמעשה נכון יותר לחשוב על כל נוירון כפונקציה, 

189
00:15:31,892 --> 00:15:38,340
כזו שמקבלת את הפלטים של כל הנוירונים בשכבה הקודמת, ופולטת החוצה מספר בין 0 ל-1. 

190
00:15:39,200 --> 00:15:47,060
באמת שכל הרשת היא רק פונקציה, כזו שמקבלת 784 מספרים כקלט ופולטת 10 מספרים כפלט. 

191
00:15:47,560 --> 00:15:52,657
זוהי פונקציה מסובכת בצורה אבסורדית, כזו שכוללת 13,000 פרמטרים בצורות של 

192
00:15:52,657 --> 00:15:57,471
משקלים והטיות אלו והמבינה דפוסים מסוימים, ואשר כוללת איטרציה של כפל 

193
00:15:57,471 --> 00:16:02,640
וקטורים ומטריצות רבים והפונקצייה הסיגמואידית, אבל בכל זאת זו רק פונקציה. 

194
00:16:03,400 --> 00:16:06,660
ובמובן מסוים זה קצת מעודד שזה נראה מסובך. 

195
00:16:07,340 --> 00:16:09,945
כלומר, אם היא היתה פשוטה יותר, איזו תקווה הייתה 

196
00:16:09,945 --> 00:16:12,280
לנו שהיא יכולה לעמוד באתגר של זיהוי ספרות? 

197
00:16:13,340 --> 00:16:14,700
ואיך היא מתמודדת עם האתגר הזה? 

198
00:16:15,080 --> 00:16:19,360
איך הרשת הזו לומדת את המשקולות וההטיות המתאימים רק על ידי התבוננות בנתונים? 

199
00:16:20,140 --> 00:16:26,120
ובכן, זה מה שאני אראה בסרטון הבא, ואני גם אעמיק קצת יותר במה באמת הרשת הספציפית הזו עושה. 

200
00:16:27,580 --> 00:16:32,527
עכשיו זה הנקודה שאני מניח שאני צריך להגיד להירשם כדי להישאר מעודכן לגבי מתי הסרטון הזה או 

201
00:16:32,527 --> 00:16:37,420
סרטונים חדשים כלשהם יוצאים, אבל באופן מציאותי רובכם באמת לא מקבלים התראות מיוטיוב, נכון? 

202
00:16:38,020 --> 00:16:42,821
אולי יותר בכנות אני צריך להגיד הירשמו כדי שהרשתות הנוירוניות שעומדות בבסיס 

203
00:16:42,821 --> 00:16:47,880
אלגוריתם ההמלצה של יוטיוב יהיו מוכנות להאמין שאתם רוצים לראות תוכן מהערוץ הזה. 

204
00:16:48,560 --> 00:16:49,940
בכל מקרה, היכונו לעוד. 

205
00:16:50,760 --> 00:16:53,500
תודה רבה לכל מי שתומך בסרטונים האלה ב-Patreon. 

206
00:16:54,000 --> 00:16:59,595
קצת איחרתי להתקדם בסדרת ההסתברות בקיץ הזה, אבל אני קופץ אליה בחזרה אחרי הפרויקט הזה, 

207
00:16:59,595 --> 00:17:01,900
אז לפטרונים תוכלו לחפש עדכונים שם. 

208
00:17:03,600 --> 00:17:07,101
כדי לסגור את העניינים נמצאת כאן איתי Lisha Li שעשתה את עבודת 

209
00:17:07,101 --> 00:17:10,774
הדוקטורט שלה בצד התיאורטי של למידה עמוקה, ועובדת כיום בחברת הון 

210
00:17:10,774 --> 00:17:14,619
סיכון בשם Amplify Partners שבאדיבותה סיפקה חלק מהמימון לסרטון הזה. 

211
00:17:15,460 --> 00:17:19,119
אז לישה, דבר אחד שאני חושב שאנחנו צריכים להעלות לדיון היא הפונקציה הסיגמואידית. 

212
00:17:19,700 --> 00:17:22,916
כפי שאני מבין את זה, רשתות מוקדמות משתמשות בה כדי לדחוס את 

213
00:17:22,916 --> 00:17:25,860
הסכום המשוקלל הרלוונטי לתוך המרווח הזה שבין אפס לאחד, 

214
00:17:25,860 --> 00:17:29,840
עם מוטיווציה מהאנלוגיה הביולוגית הזו של נוירונים שאינם פעילים או פעילים. 

215
00:17:30,280 --> 00:17:30,300
בְּדִיוּק. 

216
00:17:30,560 --> 00:17:34,040
אבל מעט יחסית מהרשתות המודרניות משתמשות עדיין בסיגמואיד. 

217
00:17:34,320 --> 00:17:34,320
כן.

218
00:17:34,440 --> 00:17:35,540
זה סוג של אסכולה ישנה, נכון? 

219
00:17:35,760 --> 00:17:38,980
כן, או ליתר דיוק, נראה שהרבה יותר קל לאמן כשמשתמשים ב-ReLU . 

220
00:17:39,400 --> 00:17:42,340
ו-ReLU היא Rectified Linear Unit (יחידה לינארית מתוקנת)? 

221
00:17:42,680 --> 00:17:47,103
כן, היא סוג של פונקציה שבה אתם פשוט לוקחים מקסימום 

222
00:17:47,103 --> 00:17:50,920
של אפס ו-a שבו a נתון לפי מה שהסברת בסרטון. 

223
00:17:50,920 --> 00:18:01,360
ולדעתי זה היה סוג של מוטיבציה בגלל אנלוגיה ביולוגית לאופן שבו נוירונים יופעלו או לא. 

224
00:18:01,360 --> 00:18:05,450
אז אם היא תעבור סף מסוים היא תהיה פונקציית הזהות, 

225
00:18:05,450 --> 00:18:09,460
אבל אם לא אז היא פשוט לא תופעל ואז היא תהיה אפס. 

226
00:18:09,460 --> 00:18:10,840
אז זהו סוג של פישוט. 

227
00:18:11,160 --> 00:18:17,597
השימוש בסיגמואידים לא עזר לאימון או שהיה קשה מאוד לאמן בשלב מסוים 

228
00:18:17,597 --> 00:18:24,620
ואנשים פשוט ניסו ReLU וזה עבד טוב מאוד עבור רשתות נוירונים עמוקות מאוד. 

229
00:18:25,100 --> 00:18:25,640
בסדר, תודה לך לישה. 

