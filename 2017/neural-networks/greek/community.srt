1
00:00:04,020 --> 00:00:10,680
Αυτό είναι το τρία. Είναι τσαπατσούλικα γραμμένο και στην υπερβολικά χαμηλή ανάλυση των 28x28 pixels.

2
00:00:10,680 --> 00:00:15,660
Το μυαλό σας όμως δεν έχει κανένα πρόβλημα να δει ότι είναι τρία και θέλω να αυτό το εκτιμήσετε για μια στιγμή:

3
00:00:15,900 --> 00:00:18,949
Πόσο τρελό είναι που οι εγκέφαλοι μπορούν να το κάνουν τόσο αβίαστα;

4
00:00:18,949 --> 00:00:23,160
Θέλω να πω, αυτό, αυτό κι αυτό τα βλέπουμε ως τρία,

5
00:00:23,160 --> 00:00:28,060
παρόλο που η συγκεκριμένη τιμή κάθε pixel διαφέρει κατά πολύ από εικόνα σε εικόνα.

6
00:00:28,080 --> 00:00:33,780
Τα εξειδικευμένα φωτοευαίσθητα κύτταρα στο μάτι σας που πυροδοτούνται όταν βλέπουν αυτό το τρία

7
00:00:33,780 --> 00:00:36,800
είναι πολύ διαφορετικά από αυτά που πυροδοτούνται όταν βλέπουν αυτό το τρία.

8
00:00:37,140 --> 00:00:40,610
Αλλά κάτι σε αυτόν τον τρελά έξυπνο φλοιό σας, τα ξεμπερδεύει

9
00:00:41,129 --> 00:00:48,139
λες και αντιπροσωπεύουν την ίδια ιδέα, ενώ την ίδια στιγμή αναγνωρίζει άλλες εικόνες ως άλλες ξεχωριστές ιδέες.

10
00:00:48,840 --> 00:00:55,039
Αν σας έλεγα όμως να καθίσετε κάτω και να μου γράψετε ένα πρόγραμμα που δέχεται ως είσοδο ένα πλέγμα 28x28 pixels

11
00:00:55,379 --> 00:01:01,759
σαν αυτό, και βγάζει ως έξοδο έναν αριθμό από το 0 ως το 10 λέγοντάς σας ποιο ψηφίο νομίζει ότι είναι...

12
00:01:02,250 --> 00:01:06,139
Λοιπόν, το πρόβλημα μετατρέπεται από κωμικά απλό σε αποθαρρυντικά δύσκολο.

13
00:01:06,750 --> 00:01:08,270
Εκτός αν ζείτε στα βουνά,

14
00:01:08,270 --> 00:01:14,599
νομίζω ότι δεν χρειάζεται να σας πείσω για τη συνάφεια και τη σημασία του machine learning και των νευρωνικών δικτύων, τώρα και στο μέλλον,

15
00:01:14,640 --> 00:01:18,410
αλλά αυτό που θέλω να κάνω εδώ, είναι να σας δείξω τι είναι πραγματικά ένα νευρωνικό δίκτυο.

16
00:01:18,660 --> 00:01:24,229
Υποθέτοντας ότι δεν έχετε ασχοληθεί καθόλου με το θέμα, και για να σας βοηθήσω να οπτικοποιήσετε τι κάνει, όχι σαν λέξη της μόδας, αλλά σαν μαθηματικό εργαλείο,

17
00:01:24,570 --> 00:01:28,310
Η ελπίδα μου είναι ότι τελειώνοντας το βίντεο θα νιώθετε τα κίνητρα που μας οδηγούν σε αυτήν τη δομή,

18
00:01:28,380 --> 00:01:34,399
και θα καταλαβαίνετε τι σημαίνει το "μαθαίνει" όταν διαβάζετε ή ακούτε για ένα νευρωνικό δίκτυο "μάθησης".

19
00:01:34,950 --> 00:01:40,249
Αυτό το βίντεο θα είναι αφιερωμένο στο κομμάτι της δομής, ενώ το επόμενο θα ασχοληθεί με το κομμάτι της "μάθησης".

20
00:01:40,530 --> 00:01:45,950
Αυτό που θα κάνουμε είναι να φτιάξουμε ένα νευρωνικό δίκτυο που θα μάθει να αναγνωρίζει χειρόγραφα ψηφία.

21
00:01:49,270 --> 00:01:51,329
Αυτό είναι ένα σχετικά κλασικό παράδειγμα για

22
00:01:51,520 --> 00:01:56,759
εισαγωγή στο θέμα και χαίρομαι που ακολουθώ την παράδοση εδώ, γιατί στο τέλος των δύο βίντεο, θέλω

23
00:01:56,760 --> 00:02:02,099
να σας παραπέμψω σε καναδυό καλές πηγές όπου μπορείτε να μάθετε περισσότερα και να κατεβάσετε τον κώδικα που κάνει αυτήν τη δουλειά

24
00:02:02,100 --> 00:02:04,100
και να "παίξετε" με αυτόν στον υπολογιστή σας.

25
00:02:04,750 --> 00:02:08,970
Υπάρχουν πάρα πολλές διαφορετικές εκδοχές των νευρωνικών δικτύων και τα τελευταία χρόνια,

26
00:02:08,970 --> 00:02:11,970
έχει υπάρξει μία, ας πούμε, έκρηξη στην έρευνα αυτών των εκδοχών.

27
00:02:12,130 --> 00:02:19,019
Όμως σε αυτά τα δύο εισαγωγικά βίντεο, εσείς κι εγώ θα εξετάσουμε την πολύ απλή μορφή, χωρίς τα πρόσθετα στολίδια.

28
00:02:19,300 --> 00:02:21,040
Αυτή η μορφή είναι κάπως απαραίτητη

29
00:02:21,040 --> 00:02:24,510
προϋπόθεση στο να καταλάβετε οποιαδήποτε από τις ισχυρότερες εκδοχές και

30
00:02:24,760 --> 00:02:28,199
πιστέψτε με, είναι και από μόνη της αρκετά περίπλοκη.

31
00:02:28,690 --> 00:02:32,820
Ακόμη όμως και αυτή η απλούστατη μορφή, μπορεί να μάθει να αναγνωρίζει χειρόγραφα ψηφία,

32
00:02:32,820 --> 00:02:36,180
κάτι πολύ εντυπωσιακό για έναν υπολογιστή.

33
00:02:37,120 --> 00:02:41,960
Ταυτόχρονα, θα δείτε πώς, ίσως να μας απογοητεύσει σε κάποια πράγματα που μπορεί να περιμένουμε από αυτήν.

34
00:02:43,090 --> 00:02:48,179
Όπως δείχνει και το όνομα, τα νευρωνικά δίκτυα έχουν εμπνευστεί από τους εγκεφάλους. Ας το αναλύσουμε όμως αυτό.

35
00:02:48,520 --> 00:02:51,389
Τι είναι οι νευρώνες, και τι εννοούμε όταν λέμε ότι "συνδέονται" μεταξύ τους;

36
00:02:52,090 --> 00:02:57,750
Τώρα, όταν λέω "νευρώνας", το μόνο που θέλω να σκέφτεστε είναι ένα πράγμα που αποθηκεύει έναν αριθμό.

37
00:02:58,209 --> 00:03:02,129
Πιο συγκεκριμένα, έναν αριθμό μεταξύ του 0 και του 1. Πραγματικά δεν είναι κάτι παραπάνω από αυτό.

38
00:03:03,430 --> 00:03:11,130
Πχ. το δίκτυο ξεκινάει από μία ομάδα νευρώνων που αντιστοιχούν σε καθένα από τα 28x28 pixels της εικόνας εισόδου,

39
00:03:11,400 --> 00:03:12,460
δηλαδή είναι 784 νευρώνες

40
00:03:12,460 --> 00:03:20,240
συνολικά, ο καθένας εκ των οποίων κρατάει έναν αριθμό που αντιπροσωπεύει την τιμή του pixel στην κλίμακα του γκρι,

41
00:03:20,769 --> 00:03:24,299
με εύρος από 0 για τα μαύρα pixel μέχρι και 1 για τα άσπρα.

42
00:03:24,910 --> 00:03:30,419
Αυτόν τον αριθμό τον λέμε ενεργοποίηση του νευρώνα, και η εικόνα που ίσως σας έρχεται στο μυαλό είναι

43
00:03:30,420 --> 00:03:33,959
ότι κάθε νευρώνας "ανάβει" όταν η ενεργοποίησή του είναι ένας μεγάλος αριθμός.

44
00:03:36,260 --> 00:03:41,559
Άρα όλοι αυτοί οι 784 νευρώνες συνθέτουν το πρώτο στρώμα του δικτύου μας.

45
00:03:45,990 --> 00:03:51,289
Τώρα ας δούμε κατευθείαν το τελευταίο στρώμα του δικτύου, που έχει 10 νευρώνες, και ο καθένας αντιπροσωπεύει ένα από τα ψηφία.

46
00:03:51,570 --> 00:03:56,239
Η ενεργοποίηση σε αυτούς τους νευρώνες, που είναι και πάλι ένας αριθμός ανάμεσα στο 0 και το 1,

47
00:03:56,880 --> 00:04:00,049
αντιπροσωπεύει το κατά πόσο το σύστημα πιστεύει ότι η δοθείσα εικόνα,

48
00:04:00,720 --> 00:04:05,990
αντιστοιχεί σε αυτό το ψηφίο. Υπάρχουν επίσης καναδυό στρώματα ανάμεσα που ονομάζονται "κρυφά στρώματα",

49
00:04:06,180 --> 00:04:07,770
στα οποία για την ώρα,

50
00:04:07,770 --> 00:04:13,549
ας βάλουμε απλά ένα τεράστιο ερωτηματικό για το πώς στο καλό λειτουργούν ώστε να επιτύχει η διαδικασία αναγνώρισης των ψηφίων.

51
00:04:13,740 --> 00:04:20,209
Σε αυτό το δίκτυο επέλεξα να έχω δύο κρυφά στρώματα, το καθένα από 16 νευρώνες, και ομολογουμένως είναι μία λίγο αυθαίρετη επιλογή.

52
00:04:20,609 --> 00:04:24,889
Για να είμαι ειλικρινής, διάλεξα να έχω δύο κρυφά στρώματα, βασιζόμενος στη δομή που θέλω να υπάρχει,

53
00:04:25,350 --> 00:04:29,179
και 16 νευρώνες... Ε, το 16 ήταν απλά ένας ωραίος αριθμός ώστε να χωράνε οι νευρώνες στην οθόνη.

54
00:04:29,180 --> 00:04:32,209
Υπάρχει πολύς χώρος για να πειραματιστείτε με τη δομή εδώ.

55
00:04:32,730 --> 00:04:38,329
Ο τρόπος με τον οποίο το δίκτυο χειρίζεται τις ενεργοποιήσεις σε ένα στρώμα, καθορίζει τις ενεργοποιήσεις στο επόμενο.

56
00:04:38,760 --> 00:04:45,349
Και φυσικά, η καρδιά του δικτύου ως ένας μηχανισμός επεξεργασίας πληροφοριών, είναι το πώς ακριβώς αυτές

57
00:04:45,570 --> 00:04:48,409
οι ενεργοποιήσεις σε ένα στρώμα, καθορίζουν τις ενεργοποιήσεις στο επόμενο.

58
00:04:48,900 --> 00:04:54,859
Έχει φτιαχτεί ώστε να είναι κάπως ανάλογο με το πώς στα βιολογικά νευρωνικά δίκτυα, κάποιες ομάδες νευρώνων πυροδοτούν

59
00:04:55,410 --> 00:04:57,410
κάποιες άλλες ομάδες με τη σειρά τους.

60
00:04:57,570 --> 00:04:58,340
Τώρα, το δίκτυο

61
00:04:58,340 --> 00:05:03,019
που δείχνω εδώ, έχει ήδη εκπαιδευτεί στο να αναγνωρίζει ψηφία, και ας σας δείξω τι εννοώ με αυτό.

62
00:05:03,140 --> 00:05:06,580
Σημαίνει ότι αν του δώσω μια εικόνα, ενεργοποιώντας έτσι

63
00:05:06,640 --> 00:05:11,780
και τους 784 νευρώνες του στρώματος εισόδου, ανάλογα με τη φωτεινότητα κάθε pixel,

64
00:05:12,330 --> 00:05:17,029
αυτό το μοτίβο ενεργοποιήσεων, θα προκαλέσει κάποιο άλλο συγκεκριμένο μοτίβο στο επόμενο στρώμα,

65
00:05:17,190 --> 00:05:19,309
που ακολούθως θα προκαλέσει κάποιο μοτίβο στο επόμενο στρώμα,

66
00:05:19,440 --> 00:05:22,190
που τελικά θα δώσει ένα μοτίβο στο στρώμα εξόδου,

67
00:05:22,350 --> 00:05:29,359
και ο φωτεινότερος νευρώνας του στρώματος εξόδου δείχνει, ας πούμε, το ψηφίο που το δίκτυο πιστεύει ότι αντιπροσωπεύει η εικόνα.

68
00:05:32,070 --> 00:05:36,859
Και πριν μπούμε στα μαθηματικά για το πώς το ένα στρώμα επηρεάζει το άλλο ή το πώς λειτουργεί η εκπαίδευση του δικτύου,

69
00:05:37,140 --> 00:05:43,069
ας μιλήσουμε απλά για το γιατί μας φαίνεται λογικό μία δομή με στρώματα να συμπεριφέρεται έξυπνα.

70
00:05:43,800 --> 00:05:48,260
Τι περιμένουμε εδώ; Ποια είναι η καλύτερη ελπίδα μας για το τι κάνουν αυτά τα μεσαία στρώματα;

71
00:05:48,860 --> 00:05:56,720
Λοιπόν, όταν εσείς ή εγώ αναγνωρίζουμε ψηφία, ενώνουμε μαζί διάφορα κομμάτια. Ένα 9 έχει έναν κύκλο πάνω και μια γραμμή στα δεξιά,

72
00:05:57,260 --> 00:06:01,280
ένα 8 έχει επίσης έναν κύκλο πάνω, αλλά και έναν κύκλο κάτω.

73
00:06:02,020 --> 00:06:06,599
Ένα 4 βασικά σπάει σε τρεις συγκεκριμένες γραμμές, και τα λοιπά.

74
00:06:07,180 --> 00:06:11,970
Τώρα, σε έναν ιδανικό κόσμο, μπορεί να ελπίζαμε ότι κάθε νευρώνας του προτελευταίου στρώματος,

75
00:06:12,640 --> 00:06:14,729
αντιστοιχεί σε καθένα από αυτά τα "κομμάτια".

76
00:06:14,890 --> 00:06:19,740
Ότι κάθε φορά που δίνουμε μια εικόνα, ας πούμε με έναν κύκλο πάνω, όπως στο 9 ή στο 8,

77
00:06:19,870 --> 00:06:21,220
υπάρχει ένας συγκεκριμένος

78
00:06:21,220 --> 00:06:27,749
νευρώνας, του οποίου η ενεργοποίηση θα είναι κοντά στο 1. Και δεν εννοώ μόνο αυτόν τον συγκεκριμένο κύκλο από pixels, η ελπίδα μας είναι ότι κάθε

79
00:06:28,090 --> 00:06:35,039
γενικότερο κυκλόμορφο μοτίβο στο πάνω μέρος, θα ενεργοποιούσε αυτόν τον νευρώνα. Έτσι, η μετάβαση από το τρίτο στο τελευταίο στρώμα,

80
00:06:35,380 --> 00:06:39,960
απαιτεί μόνο το να μάθουμε ποιος συνδυασμός κομματιών αντιστοιχεί σε ποια ψηφία.

81
00:06:40,510 --> 00:06:42,810
Φυσικά, αυτό απλά μεταφέρει το πρόβλημα παρακάτω και δεν το λύνει.

82
00:06:42,910 --> 00:06:49,019
Γιατί πώς θα αναγνωρίζατε αυτά τα κομμάτια ή ακόμη και θα μαθαίνατε ποια είναι τα σωστά κομμάτια, και ακόμη δεν έχω καν μιλήσει

83
00:06:49,020 --> 00:06:52,829
για το πώς ένα στρώμα επηρεάζει το επόμενο, αλλά ελάτε εδώ μαζί μου για μια στιγμή.

84
00:06:53,650 --> 00:06:56,340
Η αναγνώριση ενός κύκλου, μπορεί κι αυτή να διασπαστεί σε υποπροβλήματα.

85
00:06:56,860 --> 00:07:02,550
Ένας λογικός τρόπος να γίνει αυτό θα ήταν να αναγνωρίσουμε πρώτα τις διάφορες μικρές ακμές που τον συνθέτουν.

86
00:07:03,520 --> 00:07:08,910
Παρομοίως, μία μακριά γραμμή, όπως αυτή στα ψηφία 1, 4 ή 7.

87
00:07:08,910 --> 00:07:14,279
Λοιπόν, αυτή είναι απλά μια μακριά ακμή, ή μπορείτε και αυτήν να τη σκεφτείτε σαν μια ακολουθία πολλών μικρότερων ακμών.

88
00:07:14,740 --> 00:07:19,379
Άρα ίσως να ελπίζουμε ότι κάθε νευρώνας στο δεύτερο στρώμα του δικτύου,

89
00:07:20,290 --> 00:07:22,650
αντιστοιχεί στις διάφορες μικρές ακμές.

90
00:07:23,230 --> 00:07:28,259
Ίσως όταν μπαίνει μια εικόνα σαν κι αυτήν, να ενεργοποιούνται όλοι οι νευρώνες

91
00:07:28,720 --> 00:07:31,649
που αντιστοιχούν σε περίπου 8 με 10 συγκεκριμένες μικρές ακμές,

92
00:07:31,930 --> 00:07:36,930
που με τη σειρά τους ενεργοποιούν τους νευρώνες για τον πάνω κύκλο και για μία μακριά κάθετη γραμμή,

93
00:07:37,300 --> 00:07:39,599
και μετά αυτοί ενεργοποιούν τον νευρώνα που αντιστοιχεί στο 9.

94
00:07:40,300 --> 00:07:41,100
Το αν είναι ή όχι

95
00:07:41,100 --> 00:07:47,070
αυτή η πραγματική λειτουργία του τελικού μας δικτύου, είναι μια άλλη ερώτηση, στην οποία θα αναφερθώ όταν δούμε το πώς γίνεται η εκπαίδευση του δικτύου.

96
00:07:47,350 --> 00:07:52,170
Ίσως όμως αυτή να είναι μία ελπίδα μας. Κάτι σαν στόχος για το πώς θα είναι η δομή των στρωμάτων.

97
00:07:53,020 --> 00:07:59,340
Επιπλέον, σκεφτείτε το πώς η δυνατότητα ανίχνευσης ακμών και μοτίβων σαν αυτά θα ήταν πολύ χρήσιμη σε άλλα προβλήματα αναγνώρισης εικόνας.

98
00:07:59,740 --> 00:08:06,749
Και πέρα από την αναγνώριση εικόνας, υπάρχουν τόσα έξυπνα πράγματα που ίσως θέλατε να κάνετε, που μπορούν να σπάσουν σε τέτοια αφηρημένα στρώματα.

99
00:08:07,690 --> 00:08:14,670
Η αναγνώριση ομιλίας πχ, περιλαμβάνει την ανάλυση σκέτου ήχου σε ξεχωριστούς μικρούς ήχους που συνδυάζονται για τη δημιουργία συλλαβών,

100
00:08:15,070 --> 00:08:19,829
που συνδυάζονται για τον σχηματισμό λέξεων, που συνδυάζονται για να φτιάξουν φράσεις και πιο αφηρημένες σκέψεις κλπ.

101
00:08:20,770 --> 00:08:25,710
Ας γυρίσουμε όμως στο πώς δουλεύει καθετί από αυτά. Φανταστείτε τον εαυτό σας αυτήν τη στιγμή να σχεδιάζει

102
00:08:25,710 --> 00:08:30,449
το πώς ακριβώς οι ενεργοποιήσεις σε ένα στρώμα μπορεί να καθορίζουν τις ενεργοποιήσεις του επόμενου.

103
00:08:30,670 --> 00:08:35,879
Ο στόχος είναι να έχουμε κάποιον μηχανισμό που πιθανώς να συνδυάζει pixels σε ακμές,

104
00:08:35,880 --> 00:08:41,430
ή ακμές σε μοτίβα, ή μοτίβα σε ψηφία και για να επικεντρωθούμε σε ένα παράδειγμα:

105
00:08:41,950 --> 00:08:44,189
Έστω ότι ελπίζουμε πως ένας συγκεκριμένος νευρώνας

106
00:08:44,380 --> 00:08:50,430
του δεύτερου στρώματος μπορεί να ανιχνεύσει αν η εικόνα έχει ή όχι, μία ακμή σε αυτήν την περιοχή εδώ.

107
00:08:50,950 --> 00:08:54,960
Η ερώτηση που καλούμαστε να απαντήσουμε, είναι το τι παραμέτρους θα έπρεπε να έχει το δίκτυο,

108
00:08:55,270 --> 00:09:02,490
τι κουμπιά και μοχλούς θα έπρεπε να πειράξετε, ώστε να μπορεί ενδεχομένως να εντοπίσει αυτό το μοτίβο

109
00:09:02,590 --> 00:09:07,290
ή ένα άλλο οποιοδήποτε μοτίβο από pixels ή το μοτίβο όπου μερικές ακμές συνθέτουν έναν κύκλο και άλλα τέτοια.

110
00:09:08,290 --> 00:09:15,389
Λοιπόν, αυτό που θα κάνουμε είναι να ορίσουμε ένα βάρος σε καθεμία σύνδεση μεταξύ του νευρώνα και των νευρώνων του προηγούμενου στρώματος.

111
00:09:15,850 --> 00:09:17,850
Αυτά τα βάρη είναι απλά αριθμοί.

112
00:09:18,190 --> 00:09:25,590
Μετά, θα πάρουμε όλες αυτές τις ενεργοποιήσεις του πρώτου στρώματος και θα υπολογίσουμε το σταθμικό άθροισμα σύμφωνα με αυτά τα βάρη.

113
00:09:27,370 --> 00:09:31,680
Προσωπικά με βοηθάει πολύ να σκέφτομαι τα βάρη ως ένα μικρό ξεχωριστό πλέγμα,

114
00:09:31,680 --> 00:09:37,079
και εδώ χρησιμοποιώ πράσινα pixels για να δείξω τα θετικά βάρη και κόκκινα για να δείξω τα αρνητικά,

115
00:09:37,240 --> 00:09:41,670
όπου η φωτεινότητα κάποιου pixel δείχνει περίπου την τιμή του βάρους.

116
00:09:42,400 --> 00:09:45,840
Τώρα, αν μηδενίζαμε τα βάρη σχεδόν όλων των pixels,

117
00:09:46,150 --> 00:09:49,079
εκτός από κάποια θετικά βάρη σε αυτήν την περιοχή που μας ενδιαφέρει,

118
00:09:49,480 --> 00:09:51,310
τότε παίρνοντας το σταθμικό άθροισμα

119
00:09:51,310 --> 00:09:57,690
όλων των τιμών των pixel, ισοδυναμεί τελικά με το να προσθέσουμε μόνο τις τιμές των pixel της περιοχής που μας ενδιαφέρει.

120
00:09:58,870 --> 00:10:04,440
Και, αν πραγματικά θέλατε να δείτε αν υπάρχει ακμή εδώ, ίσως να βάζατε κάποια αρνητικά βάρη

121
00:10:04,900 --> 00:10:06,900
στα pixel γύρω από την περιοχή,

122
00:10:07,030 --> 00:10:12,660
ώστε το άθροισμα να είναι μεγαλύτερο μόνο αν τα μεσαία pixel είναι φωτεινά, αλλά τα τριγύρω σκοτεινά.

123
00:10:14,279 --> 00:10:18,169
Όταν υπολογίζετε ένα σταθμικό άθροισμα σαν αυτό, μπορεί να καταλήξετε σε έναν οποιονδήποτε αριθμό.

124
00:10:18,240 --> 00:10:23,180
Σε αυτό όμως το δίκτυο, αυτό που θέλουμε είναι οι ενεργοποιήσεις να έχουν μία τιμή μεταξύ του 0 και του 1,

125
00:10:23,730 --> 00:10:26,599
οπότε συχνά αυτό που κάνουμε είναι να περάσουμε αυτό το άθροισμα σε κάποια

126
00:10:26,910 --> 00:10:32,000
συνάρτηση που συμπιέζει τον πραγματικό άξονα στο διάστημα μεταξύ του 0 και του 1,

127
00:10:32,190 --> 00:10:37,249
και μία κοινή συνάρτηση που το κάνει αυτό, είναι η σιγμοειδής, γνωστή και ως λογιστική παλινδρόμηση.

128
00:10:37,980 --> 00:10:43,339
Βασικά, οι πολύ αρνητικές είσοδοι καταλήγουν κοντά στο 0, ενώ οι πολύ θετικές είσοδοι καταλήγουν κοντά στο 1,

129
00:10:43,339 --> 00:10:46,398
και στην περιοχή κοντά στην είσοδο 0 υπάρχει μία σταθερή ανοδικότητα.

130
00:10:49,080 --> 00:10:56,029
Οπότε η ενεργοποίηση ενός νευρώνα εδώ μετράει βασικά το πόσο θετικό είναι το αντίστοιχο σταθμικό άθροισμα.

131
00:10:57,450 --> 00:11:01,819
Ίσως όμως να μη θέλετε ο νευρώνας να ενεργοποιείται όταν το σταθμικό άθροισμα είναι πάνω από 0.

132
00:11:02,100 --> 00:11:06,260
Ίσως θέλετε να ενεργοποιείται μόνο όταν το άθροισμα είναι πχ. πάνω από 10.

133
00:11:06,630 --> 00:11:10,279
Θέλετε δηλαδή μία "πόλωση" που θα τον απενεργοποιεί.

134
00:11:10,860 --> 00:11:16,099
Αυτό που κάνουμε λοιπόν είναι να προσθέσουμε έναν ακόμη αριθμό, πχ. το -10 σε αυτό το σταθμικό άθροισμα,

135
00:11:16,529 --> 00:11:19,669
πριν το περάσουμε στην σιγμοειδή συνάρτηση "συμπίεσης".

136
00:11:20,220 --> 00:11:22,730
Αυτός ο πρόσθετος αριθμός λέγεται "πόλωση".

137
00:11:23,310 --> 00:11:29,060
Άρα τα βάρη, σας λένε ποιο μοτίβο από pixel ανιχνεύει αυτός ο νευρώνας στο δεύτερο στρώμα, και η πόλωση

138
00:11:29,220 --> 00:11:35,450
σας λέει το πόσο μεγάλο πρέπει να είναι το σταθμικό άθροισμα ώστε να αρχίσει ο νευρώνας να είναι σημαντικά ενεργός.

139
00:11:35,910 --> 00:11:37,910
Και αυτός είναι μόνο ένας από τους νευρώνες.

140
00:11:38,120 --> 00:11:41,940
Κάθε άλλος νευρώνας σε αυτό το δίκτυο, συνδέεται με όλους τους

141
00:11:42,320 --> 00:11:50,620
784 νευρώνες-pixel του πρώτου στρώματος, και καθεμία από αυτές τις 784 συνδέσεις έχει το δικό της βάρος,

142
00:11:51,330 --> 00:11:57,739
και κάθε νευρώνας έχει τη δική του πόλωση, δηλαδή έναν ακόμα όρο που προσθέτεις στο σταθμικό άθροισμα πριν το "συμπιέσεις" με την σιγμοειδή.

143
00:11:58,020 --> 00:12:01,909
Και αυτά είναι πολλά πράγματα! Με αυτό το κρυφό στρώμα των 16 νευρώνων,

144
00:12:02,010 --> 00:12:08,270
είναι ένα σύνολο από 784*16 βάρη μαζί με 16 πολώσεις.

145
00:12:08,490 --> 00:12:14,029
Και όλα αυτά είναι μόλις οι συνδέσεις του πρώτου με το δεύτερο στρώμα. Οι συνδέσεις των άλλων στρωμάτων

146
00:12:14,029 --> 00:12:17,208
έχουν κι αυτές ένα σωρό από βάρη και πολώσεις συσχετισμένα με αυτές.

147
00:12:17,760 --> 00:12:20,680
Τελικά, αυτό το δίκτυο έχει περίπου

148
00:12:21,280 --> 00:12:23,920
13.000 συνολικά βάρη και πολώσεις.

149
00:12:24,280 --> 00:12:29,540
13.000 κουμπιά και μοχλούς που μπορούν να πειραχτούν ώστε το δίκτυο να συμπεριφέρεται με τον τρόπο που θέλουμε.

150
00:12:30,520 --> 00:12:32,520
Οπότε, όταν μιλάμε για μάθηση,

151
00:12:32,530 --> 00:12:40,199
αυτό στο οποίο αναφερόμαστε, είναι το ότι βάζουμε τον υπολογιστή να βρει μία έγκυρη ρύθμιση όλων αυτών των αριθμών, ώστε τελικά να μας

152
00:12:40,200 --> 00:12:42,190
λύνει το πρόβλημα που θέλουμε.

153
00:12:42,190 --> 00:12:49,979
Ένα νοητικό πείραμα που είναι διασκεδαστικό αλλά και τρομακτικό μαζί, είναι να φανταστείτε ότι κάθεστε και ρυθμίζετε όλα τα βάρη και τις πολώσεις με το χέρι,

154
00:12:50,380 --> 00:12:56,159
σκόπιμα πειράζοντας τους αριθμούς, ώστε το δεύτερο στρώμα να εντοπίζει τις ακμές, το τρίτο στρώμα να εντοπίζει τα μοτίβα κλπ.

155
00:12:56,350 --> 00:13:01,440
Προσωπικά το βρίσκω πιο διασκεδαστικό από το να θεωρώ το δίκτυο σαν ένα μαύρο κουτί.

156
00:13:01,870 --> 00:13:04,349
Γιατί όταν το δίκτυο δεν συμπεριφέρεται όπως θα το περιμένατε,

157
00:13:04,600 --> 00:13:11,370
αν έχετε αποκτίσει μια μικρή επαφή με το τι σημαίνουν αυτά τα βάρη και οι πολώσεις, τότε έχετε ένα σημείο εκκίνησης

158
00:13:11,680 --> 00:13:16,289
για να πειραματιστείτε με το πώς να αλλάξετε και να βελτιώσετε τη δομή. Ή όταν δουλεύει το δίκτυο,

159
00:13:16,290 --> 00:13:18,290
όχι όμως για τους λόγους που θα περιμένατε,

160
00:13:18,310 --> 00:13:25,169
το να ψάξετε το τι κάνουν τα βάρη και οι ακμές, είναι ένας καλός τρόπος να τεστάρετε τις παραδοχές σας και να δείτε πραγματικά το πλήρες εύρος

161
00:13:25,180 --> 00:13:26,350
των πιθανών λύσεων.

162
00:13:26,350 --> 00:13:31,600
Παρεμπιπτόντως, η πραγματική συνάρτηση εδώ είναι λίγο κουραστική στη γραφή. Δεν νομίζετε;

163
00:13:32,350 --> 00:13:38,460
Αφήστε με να σας δείξω έναν σημειογραφικά πιο συμπαγή τρόπο να αναπαραστήσετε αυτές τις συνδέσεις. Έτσι θα τα συναντούσατε,

164
00:13:38,460 --> 00:13:40,460
αν επιλέγατε να διαβάσετε παραπάνω για τα νευρωνικά δίκτυα.

165
00:13:41,110 --> 00:13:45,810
Οργανώστε όλες τις ενεργοποιήσεις ενός στρώματος σε μία στήλη, ως ένα διάνυσμα.

166
00:13:47,470 --> 00:13:52,320
Μετά οργανώστε όλα τα βάρη ως έναν πίνακα, όπου κάθε γραμμή του πίνακα

167
00:13:52,900 --> 00:13:57,659
αντιστοιχεί στις συνδέσεις μεταξύ ενός στρώματος και ενός συγκεκριμένου νευρώνα του επόμενου.

168
00:13:58,060 --> 00:14:03,599
Αυτό σημαίνει ότι το σταθμικό άθροισμα των ενεργοποιήσεων στο πρώτο στρώμα, σύμφωνα με αυτά τα βάρη,

169
00:14:04,000 --> 00:14:09,330
αντιστοιχεί σε έναν από τους όρους του γινομένου των πινάκων που έχουμε στα αριστερά.

170
00:14:13,540 --> 00:14:18,380
Παρεμπιπτόντως, μεγάλο κομμάτι του machine learning απαιτεί να έχετε μια καλή επαφή με τη γραμμική άλγεβρα.

171
00:14:18,380 --> 00:14:26,940
Οπότε, αν κάποιος από εσάς θέλει μία ωραία οπτική κατανόηση των πινάκων και του τι σημαίνει το γινόμενο πινάκων, ας τσεκάρει τη σειρά που έκανα για τη γραμμική άλγεβρα,

172
00:14:27,250 --> 00:14:28,839
και ειδικά το κεφάλαιο 3.

173
00:14:28,839 --> 00:14:35,759
Πίσω στην έκφρασή μας, αντί να λέμε ότι προσθέτουμε την πόλωση σε κάθε τιμή ξεχωριστά, την αναπαριστούμε ως

174
00:14:36,010 --> 00:14:42,209
ένα διάνυσμα όλων αυτών των πολώσεων, το οποίο προσθέτουμε στο προηγούμενο γινόμενο πινάκων.

175
00:14:42,910 --> 00:14:44,040
Τέλος, θα γράψω

176
00:14:44,040 --> 00:14:47,250
τη σιγμοειδή γύρω από αυτήν την έκφραση,

177
00:14:47,250 --> 00:14:51,899
και αυτό σημαίνει ότι θα εφαρμόσετε τη σιγμοειδή συνάρτηση σε καθεμία από τις συνιστώσες

178
00:14:52,420 --> 00:14:54,570
του τελικού διανύσματος στο εσωτερικό.

179
00:14:55,510 --> 00:15:00,749
Οπότε μόλις γράψετε αυτόν τον πίνακα βαρών και αυτά τα διανύσματα με τα δικά τους σύμβολα,

180
00:15:01,000 --> 00:15:07,589
μπορείτε να συμβολίσετε την πλήρη μετάβαση των ενεργοποιήσεων από το ένα στρώμα στο επόμενο με μία υπερβολικά μικρή και όμορφη έκφραση,

181
00:15:07,930 --> 00:15:16,500
και αυτό μετά κάνει τον σχετικό κώδικα και απλούστερο και πολύ γρηγορότερο, μιας και πολλές βιβλιοθήκες βελτιστοποιούν απίστευτα το γινόμενο πινάκων.

182
00:15:17,560 --> 00:15:21,359
Θυμάστε που νωρίτερα είχα πει ότι αυτοί οι νευρώνες είναι απλά πράγματα που κρατάνε αριθμούς;

183
00:15:21,790 --> 00:15:26,250
Ε, φυσικά αυτοί οι αριθμοί εξαρτώνται από την εικόνα που δίνεται ως είσοδος.

184
00:15:27,790 --> 00:15:32,940
Οπότε είναι πιο ακριβές να σκέφτεστε κάθε νευρώνα ως μία συνάρτηση! Μία συνάρτηση που δέχεται

185
00:15:33,070 --> 00:15:38,070
ως είσοδο τις τιμές όλων των νευρώνων του προηγούμενου στρώματος, και βγάζει ως έξοδο έναν αριθμό μεταξύ του 0 και του 1.

186
00:15:38,800 --> 00:15:42,270
Πραγματικά, ολόκληρο το δίκτυο είναι απλά μία συνάρτηση που παίρνει ως είσοδο

187
00:15:42,760 --> 00:15:47,010
784 αριθμούς και βγάζει ως έξοδο 10 αριθμούς.

188
00:15:47,470 --> 00:15:48,700
Είναι μία αδιανόητα

189
00:15:48,700 --> 00:15:56,249
περίπλοκη συνάρτηση, που περιλαμβάνει 13.000 παραμέτρους με τη μορφή βαρών και πολώσεων, που ανιχνεύουν συγκεκριμένα μοτίβα και περιλαμβάνουν

190
00:15:56,250 --> 00:16:00,270
την εκτέλεση πάρα πολλών γινομένων πινάκων και της σιγμοειδούς συνάρτησης "συμπίεσης".

191
00:16:00,610 --> 00:16:06,390
Παρ' όλ' αυτά είναι απλά μία συνάρτηση, και με κάποιον τρόπο είναι καθησυχαστικό ότι είναι τόσο περίπλοκη.

192
00:16:06,390 --> 00:16:12,239
Θέλω να πω, αν ήταν έστω λίγο πιο απλή, τι ελπίδες θα είχαμε ότι θα μπορούσε να αντιμετωπίσει την πρόκληση της αναγνώρισης ψηφίων;

193
00:16:12,960 --> 00:16:19,559
Και πώς αντιμετωπίζει αυτήν την πρόκληση; Πώς μαθαίνει το δίκτυο τα κατάλληλα βάρη και τις πολώσεις, απλά κοιτώντας τα δεδομένα; Ε...

194
00:16:20,080 --> 00:16:26,039
Αυτό είναι που θα πούμε στο επόμενο βίντεο, και επίσης θα ψάξουμε λίγο ακόμα τι κάνει αυτό το δίκτυο που βλέπουμε.

195
00:16:27,130 --> 00:16:32,640
Τώρα υποθέτω είναι που πρέπει να σας πω να εγγραφείτε για να ειδοποιηθείτε όταν βγει αυτό ή οποιοδήποτε άλλο βίντεο.

196
00:16:32,760 --> 00:16:37,560
Στ' αλήθεια όμως, οι περισσότεροι από εσάς δεν παίρνετε ειδοποιήσεις από το YouTube, έτσι;

197
00:16:37,560 --> 00:16:42,260
Ίσως πρέπει να σας πω να εγγραφείτε, ώστε τα νευρωνικά δίκτυα που κρύβονται πίσω από τον αλγόριθμο προτάσεων

198
00:16:42,459 --> 00:16:47,639
του YouTube να πιστέψουν ότι θέλετε όντως να σας προτείνονται τα βίντεο αυτού του καναλιού.

199
00:16:48,250 --> 00:16:50,250
Τέλος πάντων, μείνετε συντονισμένοι για περισσότερα.

200
00:16:50,410 --> 00:16:53,550
Ευχαριστώ πάρα πολύ όλους όσους υποστηρίζουν αυτά τα βίντεο στο Patreon.

201
00:16:53,589 --> 00:16:56,759
Έχω αργήσει λίγο να προχωρήσω τη σειρά για τις Πιθανότητες αυτό το καλοκαίρι,

202
00:16:56,760 --> 00:17:01,379
αλλά θα επιστρέψω σε αυτήν μετά από αυτό το project, οπότε εσείς οι patrons θα δείτε εκεί τις ενημερώσεις.

203
00:17:03,310 --> 00:17:05,550
Για να κλείσουμε, έχω μαζί μου την Lisha Li,

204
00:17:05,550 --> 00:17:12,029
που έκανε το διδακτορικό της στο θεωρητικό μέρος του deep learning και τώρα εργάζεται σε μία εταιρεία επιχειρηματικών κεφαλαίων, την Amplify Partners,

205
00:17:12,030 --> 00:17:16,530
η οποία ευγενικά παρείχε ένα μέρος της χρηματοδότησης για αυτό το βίντεο. Οπότε Lisha, ένα πράγμα

206
00:17:16,530 --> 00:17:19,109
που πρέπει γρήγορα να συζητήσουμε είναι αυτή η σιγμοειδής συνάρτηση.

207
00:17:19,180 --> 00:17:24,780
Όπως το καταλαβαίνω εγώ, τα πρώτα δίκτυα την χρησιμοποιούσαν για να συμπιέσουν το σταθμικό άθροισμα στο διάστημα μεταξύ 0 και 1.

208
00:17:24,980 --> 00:17:30,340
- Ξέρεις, κάπως εμπνευσμένοι από τη βιολογική αναλογία των νευρώνων που είναι ενεργοί ή ανενεργοί
- Ακριβώς

209
00:17:30,360 --> 00:17:38,300
- Πλέον όμως λίγα σύγχρονα δίκτυα χρησιμοποιούν τη σιγμοειδή στην πράξη. Έχει παλιώσει λίγο ε;
- Ναι, ή μάλλον η ReLU δείχνει να είναι ευκολότερη…

210
00:17:38,300 --> 00:17:42,780
…στην εκπαίδευση.
- Και ReLU ουσιαστικά λέμε την ανορθωμένη γραμμική μοναδιαία (Rectified Linear Unit)

211
00:17:42,780 --> 00:17:48,839
- Ναι, είναι μία συνάρτηση που απλά παίρνει το μέγιστο μεταξύ του 0 και του a, όπου το a δίνεται

212
00:17:49,120 --> 00:17:53,670
από αυτό που εξηγούσες στο βίντεο. Και νομίζω ότι είναι μερικώς

213
00:17:54,610 --> 00:17:58,100
εμπνευσμένη από μία βιολογική αναλογία του πώς

214
00:17:58,179 --> 00:18:03,089
οι νευρώνες ενεργοποιούνται ή όχι, και ανάλογα με το αν ξεπερνιέται ένα συγκεκριμένο φράγμα,

215
00:18:03,250 --> 00:18:05,250
η ReLU θα ήταν απλά η ταυτοτική συνάρτηση,

216
00:18:05,290 --> 00:18:10,439
ενώ αν όχι, απλά δεν θα ενεργοποιούνταν και θα ήταν 0, οπότε είναι απλά μία απλοποίηση.

217
00:18:10,720 --> 00:18:14,429
Η χρήση των σιγμοειδών δεν βοήθησε στην εκπαίδευση, ή την έκανε πολύ δύσκολη

218
00:18:14,429 --> 00:18:19,589
σε κάποια σημεία, και οι άνθρωποι απλά δοκίμασαν την ReLU και έτυχε να δουλεύει

219
00:18:20,110 --> 00:18:22,140
πολύ καλά για αυτά τα απίστευτα

220
00:18:22,690 --> 00:18:26,000
περίπλοκα νευρωνικά δίκτυα.
- Μια χαρά, σε ευχαριστώ Lisha.

