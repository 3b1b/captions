1
00:00:04,220 --> 00:00:05,400
Ito ay isang 3.

2
00:00:06,060 --> 00:00:10,180
Ito ay palpak na nakasulat at nai-render sa napakababang resolution na 28x28 pixels, 

3
00:00:10,180 --> 00:00:13,720
ngunit ang iyong utak ay walang problema na kilalanin ito bilang isang 3.

4
00:00:14,340 --> 00:00:16,667
At gusto kong maglaan ka ng ilang sandali upang pahalagahan kung 

5
00:00:16,667 --> 00:00:18,960
gaano kabaliw na magagawa ito ng utak nang walang kahirap-hirap.

6
00:00:19,700 --> 00:00:22,964
Ibig kong sabihin, ito, ito at ito ay nakikilala rin bilang 3s, 

7
00:00:22,964 --> 00:00:27,350
kahit na ang mga partikular na halaga ng bawat pixel ay ibang-iba mula sa isang imahe 

8
00:00:27,350 --> 00:00:28,320
patungo sa susunod.

9
00:00:28,900 --> 00:00:32,791
Ang partikular na light-sensitive na mga cell sa iyong mata na nagpapaputok 

10
00:00:32,791 --> 00:00:36,940
kapag nakita mo itong 3 ay ibang-iba sa mga nagpapaputok kapag nakita mo itong 3.

11
00:00:37,520 --> 00:00:41,237
Ngunit may isang bagay sa iyong nakatutuwang visual cortex na niresolba 

12
00:00:41,237 --> 00:00:43,819
ang mga ito bilang kumakatawan sa parehong ideya, 

13
00:00:43,819 --> 00:00:48,260
habang kinikilala ang iba pang mga larawan bilang kanilang sariling natatanging ideya.

14
00:00:49,220 --> 00:00:53,216
Ngunit kung sinabi ko sa iyo, hey, umupo at sumulat para sa akin ng isang 

15
00:00:53,216 --> 00:00:57,375
programa na tumatagal sa isang grid ng 28x28 pixels tulad nito at naglalabas 

16
00:00:57,375 --> 00:01:01,642
ng isang solong numero sa pagitan ng 0 at 10, na nagsasabi sa iyo kung ano ang 

17
00:01:01,642 --> 00:01:06,180
iniisip ng digit, mabuti ang gawain mula sa nakakatawang maliit hanggang sa mahirap.

18
00:01:07,160 --> 00:01:09,039
Maliban na lang kung nakatira ka sa ilalim ng bato, 

19
00:01:09,039 --> 00:01:11,568
sa tingin ko ay hindi ko na kailangan pang hikayatin ang kaugnayan at 

20
00:01:11,568 --> 00:01:14,640
kahalagahan ng machine learning at mga neural network sa kasalukuyan at sa hinaharap.

21
00:01:15,120 --> 00:01:18,196
Ngunit ang gusto kong gawin dito ay ipakita sa iyo kung ano talaga ang isang neural 

22
00:01:18,196 --> 00:01:21,236
network, sa pag-aakalang walang background, at upang makatulong na makita kung ano 

23
00:01:21,236 --> 00:01:24,460
ang ginagawa nito, hindi bilang isang buzzword ngunit bilang isang piraso ng matematika.

24
00:01:25,020 --> 00:01:28,195
Umaasa ako na umalis ka sa pakiramdam na ang istraktura mismo ay motibasyon, 

25
00:01:28,195 --> 00:01:31,247
at pakiramdam na alam mo kung ano ang ibig sabihin nito kapag nagbasa ka, 

26
00:01:31,247 --> 00:01:34,340
o narinig mo ang tungkol sa isang neural network na quote-unquote learning.

27
00:01:35,360 --> 00:01:38,282
Ang video na ito ay ilalaan lamang sa bahagi ng istraktura nito, 

28
00:01:38,282 --> 00:01:40,260
at ang susunod ay tatalakayin ang pag-aaral.

29
00:01:40,960 --> 00:01:43,345
Ang gagawin natin ay pagsama-samahin ang isang neural 

30
00:01:43,345 --> 00:01:46,040
network na matututong kilalanin ang mga sulat-kamay na digit.

31
00:01:49,360 --> 00:01:52,111
Ito ay isang medyo klasikong halimbawa para sa pagpapakilala ng paksa, 

32
00:01:52,111 --> 00:01:55,522
at masaya akong manatili sa status quo dito, dahil sa dulo ng dalawang video gusto kong 

33
00:01:55,522 --> 00:01:58,932
ituro sa iyo ang ilang magagandang mapagkukunan kung saan maaari kang matuto nang higit 

34
00:01:58,932 --> 00:02:02,382
pa, at kung saan maaari mong i-download ang code na gumagawa nito at laruin ito sa iyong 

35
00:02:02,382 --> 00:02:03,080
sariling computer.

36
00:02:05,040 --> 00:02:08,574
Maraming mga variant ng mga neural network, at sa mga nakalipas na taon ay 

37
00:02:08,574 --> 00:02:12,487
nagkaroon ng isang uri ng pag-unlad sa pananaliksik patungo sa mga variant na ito, 

38
00:02:12,487 --> 00:02:16,022
ngunit sa dalawang panimulang video na ito ikaw at ako ay titingin lang sa 

39
00:02:16,022 --> 00:02:19,180
pinakasimpleng plain vanilla form na walang karagdagang mga frills.

40
00:02:19,860 --> 00:02:22,785
Ito ay uri ng isang kinakailangang kinakailangan para sa pag-unawa sa alinman sa 

41
00:02:22,785 --> 00:02:25,638
mga mas makapangyarihang modernong variant, at magtiwala sa akin na mayroon pa 

42
00:02:25,638 --> 00:02:28,600
itong maraming kumplikado para sa amin upang ibalot ang aming mga isip sa paligid.

43
00:02:29,120 --> 00:02:31,280
Ngunit kahit na sa pinakasimpleng anyo na ito, 

44
00:02:31,280 --> 00:02:33,992
matututunan nitong kilalanin ang mga sulat-kamay na digit, 

45
00:02:33,992 --> 00:02:36,520
na isang magandang bagay na magagawa ng isang computer.

46
00:02:37,480 --> 00:02:39,741
At sa parehong oras makikita mo kung paano ito kulang sa 

47
00:02:39,741 --> 00:02:42,280
pag-asa ng isang mag-asawa na maaaring magkaroon tayo para dito.

48
00:02:43,380 --> 00:02:47,289
Tulad ng ipinahihiwatig ng pangalan, ang mga neural network ay inspirasyon ng utak, 

49
00:02:47,289 --> 00:02:48,500
ngunit buwagin natin iyon.

50
00:02:48,520 --> 00:02:51,660
Ano ang mga neuron, at sa anong diwa sila ay magkakaugnay?

51
00:02:52,500 --> 00:02:56,496
Sa ngayon kapag sinabi kong neuron ang gusto kong isipin mo ay isang bagay 

52
00:02:56,496 --> 00:03:00,440
na mayroong isang numero, partikular na isang numero sa pagitan ng 0 at 1.

53
00:03:00,680 --> 00:03:02,560
Ito ay talagang hindi higit pa sa na.

54
00:03:03,780 --> 00:03:08,688
Halimbawa ang network ay nagsisimula sa isang bungkos ng mga neuron na 

55
00:03:08,688 --> 00:03:14,220
tumutugma sa bawat isa sa 28x28 pixels ng input image, na 784 neuron sa kabuuan.

56
00:03:14,700 --> 00:03:19,512
Ang bawat isa sa mga ito ay mayroong isang numero na kumakatawan sa grayscale na halaga 

57
00:03:19,512 --> 00:03:24,380
ng kaukulang pixel, mula 0 para sa mga itim na pixel hanggang 1 para sa mga puting pixel.

58
00:03:25,300 --> 00:03:28,470
Ang numerong ito sa loob ng neuron ay tinatawag na activation nito, 

59
00:03:28,470 --> 00:03:31,408
at ang imahe na maaaring nasa isip mo dito ay ang bawat neuron 

60
00:03:31,408 --> 00:03:34,160
ay naiilawan kapag ang activation nito ay mataas na numero.

61
00:03:36,720 --> 00:03:41,860
Kaya lahat ng 784 neuron na ito ay bumubuo sa unang layer ng aming network.

62
00:03:46,500 --> 00:03:49,316
Ngayon ay tumatalon sa huling layer, mayroon itong 10 neuron, 

63
00:03:49,316 --> 00:03:51,360
bawat isa ay kumakatawan sa isa sa mga digit.

64
00:03:52,040 --> 00:03:56,127
Ang pag-activate sa mga neuron na ito, muli ang ilang numero na nasa pagitan ng 0 at 1, 

65
00:03:56,127 --> 00:03:59,425
ay kumakatawan sa kung gaano kalaki ang iniisip ng system na ang isang 

66
00:03:59,425 --> 00:04:02,120
ibinigay na imahe ay tumutugma sa isang naibigay na digit.

67
00:04:03,040 --> 00:04:06,783
Mayroon ding ilang layer sa pagitan na tinatawag na hidden layers, 

68
00:04:06,783 --> 00:04:10,247
na sa ngayon ay dapat na isang malaking tandang pananong kung 

69
00:04:10,247 --> 00:04:13,600
paano haharapin ang prosesong ito ng pagkilala sa mga digit.

70
00:04:14,260 --> 00:04:17,941
Sa network na ito pumili ako ng dalawang nakatagong layer, bawat isa ay may 16 na neuron, 

71
00:04:17,941 --> 00:04:20,560
at tinatanggap na iyon ay uri ng isang arbitrary na pagpipilian.

72
00:04:21,019 --> 00:04:23,352
Sa totoo lang, pumili ako ng dalawang layer batay sa kung paano 

73
00:04:23,352 --> 00:04:25,794
ko gustong i-motivate ang structure sa loob lang ng ilang sandali, 

74
00:04:25,794 --> 00:04:28,200
at 16, buti na lang magandang numero iyon para magkasya sa screen.

75
00:04:28,780 --> 00:04:30,597
Sa pagsasagawa, mayroong maraming puwang para sa 

76
00:04:30,597 --> 00:04:32,340
eksperimento sa isang tiyak na istraktura dito.

77
00:04:33,020 --> 00:04:35,522
Sa paraan ng pagpapatakbo ng network, tinutukoy ng mga 

78
00:04:35,522 --> 00:04:38,480
activation sa isang layer ang mga activation ng susunod na layer.

79
00:04:39,200 --> 00:04:42,254
At siyempre ang puso ng network bilang isang mekanismo sa pagpoproseso 

80
00:04:42,254 --> 00:04:45,094
ng impormasyon ay bumaba sa eksakto kung paano nagdudulot ang mga 

81
00:04:45,094 --> 00:04:48,580
pag-activate na iyon mula sa isang layer ng mga pag-activate sa susunod na layer.

82
00:04:49,140 --> 00:04:51,781
Ito ay sinadya upang maging maluwag na kahalintulad sa kung paano sa 

83
00:04:51,781 --> 00:04:54,385
mga biological na network ng mga neuron, ang ilang mga grupo ng mga 

84
00:04:54,385 --> 00:04:57,180
neuron na nagpapaputok ay nagiging sanhi ng ilang iba pa sa pagpapaputok.

85
00:04:58,120 --> 00:05:01,030
Ngayon ang network na ipinapakita ko dito ay sinanay na upang makilala ang mga digit, 

86
00:05:01,030 --> 00:05:03,400
at hayaan mo akong ipakita sa iyo kung ano ang ibig kong sabihin doon.

87
00:05:03,640 --> 00:05:06,564
Nangangahulugan ito na kung magpapakain ka sa isang imahe, 

88
00:05:06,564 --> 00:05:10,133
na nagpapailaw sa lahat ng 784 neuron ng input layer ayon sa liwanag ng 

89
00:05:10,133 --> 00:05:13,900
bawat pixel sa larawan, ang pattern ng mga activation na iyon ay nagdudulot 

90
00:05:13,900 --> 00:05:17,569
ng ilang partikular na pattern sa susunod na layer na nagdudulot ng ilang 

91
00:05:17,569 --> 00:05:21,286
pattern sa isa pagkatapos. ito, na sa wakas ay nagbibigay ng ilang pattern 

92
00:05:21,286 --> 00:05:22,080
sa output layer.

93
00:05:22,560 --> 00:05:25,980
At ang pinakamaliwanag na neuron ng layer ng output na iyon ay ang pagpipilian 

94
00:05:25,980 --> 00:05:29,400
ng network, wika nga, para sa kung anong digit ang kinakatawan ng larawang ito.

95
00:05:32,560 --> 00:05:36,227
At bago tumalon sa matematika para sa kung paano naiimpluwensyahan ng isang layer ang 

96
00:05:36,227 --> 00:05:38,231
susunod, o kung paano gumagana ang pagsasanay, 

97
00:05:38,231 --> 00:05:41,856
pag-usapan lang natin kung bakit makatuwirang asahan ang isang layered na istraktura 

98
00:05:41,856 --> 00:05:43,520
na tulad nito na kumilos nang matalino.

99
00:05:44,060 --> 00:05:45,220
Ano ang inaasahan natin dito?

100
00:05:45,400 --> 00:05:47,600
Ano ang pinakamagandang pag-asa para sa mga gitnang layer na iyon?

101
00:05:48,920 --> 00:05:51,124
Buweno, kapag nakilala mo o ko ang mga digit, 

102
00:05:51,124 --> 00:05:53,520
pinagsasama-sama natin ang iba't ibang bahagi.

103
00:05:54,200 --> 00:05:56,820
Ang 9 ay may loop sa itaas at isang linya sa kanan.

104
00:05:57,380 --> 00:05:59,153
Ang isang 8 ay mayroon ding isang loop sa itaas, 

105
00:05:59,153 --> 00:06:01,180
ngunit ito ay ipinares sa isa pang loop pababa sa ibaba.

106
00:06:01,980 --> 00:06:05,323
Ang isang 4 ay karaniwang nahahati sa tatlong partikular na linya, 

107
00:06:05,323 --> 00:06:06,820
at mga bagay na katulad niyan.

108
00:06:07,600 --> 00:06:11,565
Ngayon sa isang perpektong mundo, maaari tayong umasa na ang bawat neuron sa pangalawa 

109
00:06:11,565 --> 00:06:14,937
hanggang sa huling layer ay tumutugma sa isa sa mga subkomponyenteng ito, 

110
00:06:14,937 --> 00:06:19,039
na anumang oras na i-feed mo ang isang imahe na may, sabihin nating, isang loop sa itaas, 

111
00:06:19,039 --> 00:06:22,777
tulad ng isang 9 o isang 8, mayroong ilang tiyak na neuron na ang pag-activate ay 

112
00:06:22,777 --> 00:06:23,780
magiging malapit sa 1.

113
00:06:24,500 --> 00:06:27,332
At hindi ko ibig sabihin ang partikular na loop ng mga pixel na ito, 

114
00:06:27,332 --> 00:06:30,821
ang pag-asa ay ang anumang pangkalahatang loopy pattern patungo sa tuktok na set off 

115
00:06:30,821 --> 00:06:31,560
ang neuron na ito.

116
00:06:32,440 --> 00:06:34,910
Sa ganoong paraan, ang pagpunta mula sa pangatlong layer hanggang 

117
00:06:34,910 --> 00:06:37,381
sa huling layer ay nangangailangan lamang ng pag-aaral kung aling 

118
00:06:37,381 --> 00:06:40,040
kumbinasyon ng mga subcomponents ang tumutugma sa kung aling mga digit.

119
00:06:41,000 --> 00:06:44,134
Syempre, pinababa lang nito ang problema, dahil paano mo makikilala ang mga 

120
00:06:44,134 --> 00:06:47,640
subcomponents na ito, o kahit na matutunan kung ano dapat ang tamang mga subkomponen?

121
00:06:48,060 --> 00:06:51,100
At hindi ko pa rin napag-uusapan kung paano naiimpluwensyahan ng isang layer ang susunod, 

122
00:06:51,100 --> 00:06:53,060
ngunit tumakbo kasama ako sa isang ito nang ilang sandali.

123
00:06:53,680 --> 00:06:56,680
Ang pagkilala sa isang loop ay maaari ding masira sa mga subproblema.

124
00:06:57,280 --> 00:06:59,916
Ang isang makatwirang paraan upang gawin ito ay ang unang 

125
00:06:59,916 --> 00:07:02,780
makilala ang iba't ibang maliliit na gilid na bumubuo dito.

126
00:07:03,780 --> 00:07:07,231
Katulad nito, ang isang mahabang linya, tulad ng uri na maaari mong makita 

127
00:07:07,231 --> 00:07:10,223
sa mga digit 1 o 4 o 7, ay talagang isang mahabang gilid lamang, 

128
00:07:10,223 --> 00:07:14,320
o marahil ay iniisip mo ito bilang isang tiyak na pattern ng ilang mas maliliit na gilid.

129
00:07:15,140 --> 00:07:18,955
Kaya marahil ang aming pag-asa ay ang bawat neuron sa pangalawang layer ng 

130
00:07:18,955 --> 00:07:22,720
network ay tumutugma sa iba't ibang mga nauugnay na maliliit na gilid.

131
00:07:23,540 --> 00:07:26,149
Marahil kapag ang isang imaheng tulad nito ay pumasok, 

132
00:07:26,149 --> 00:07:30,230
ito ay nag-iilaw sa lahat ng mga neuron na nauugnay sa humigit-kumulang 8 hanggang 10 

133
00:07:30,230 --> 00:07:34,263
partikular na maliliit na gilid, na kung saan ay nag-iilaw sa mga neuron na nauugnay 

134
00:07:34,263 --> 00:07:36,683
sa itaas na loop at isang mahabang patayong linya, 

135
00:07:36,683 --> 00:07:39,720
at ang mga iyon ay nagpapailaw sa neuron na nauugnay sa isang 9.

136
00:07:40,680 --> 00:07:44,301
Kung ito man ang aktwal na ginagawa ng aming huling network ay isa pang tanong, 

137
00:07:44,301 --> 00:07:47,515
isa na babalikan ko kapag nakita namin kung paano sanayin ang network, 

138
00:07:47,515 --> 00:07:49,959
ngunit ito ay isang pag-asa na maaaring mayroon kami, 

139
00:07:49,959 --> 00:07:52,540
isang uri ng layunin na may layered na istraktura ganito.

140
00:07:53,160 --> 00:07:55,171
Bukod dito, maaari mong isipin kung paano magiging 

141
00:07:55,171 --> 00:07:57,459
kapaki-pakinabang ang kakayahang makakita ng mga gilid at 

142
00:07:57,459 --> 00:08:00,300
pattern na tulad nito para sa iba pang mga gawain sa pagkilala ng imahe.

143
00:08:00,880 --> 00:08:03,933
At kahit na higit pa sa pagkilala sa imahe, mayroong lahat ng uri ng mga 

144
00:08:03,933 --> 00:08:07,280
matatalinong bagay na maaari mong gawin na bumagsak sa mga layer ng abstraction.

145
00:08:08,040 --> 00:08:11,004
Ang pag-parse ng pagsasalita, halimbawa, ay kinabibilangan ng pagkuha ng 

146
00:08:11,004 --> 00:08:13,116
hilaw na audio at pagpili ng mga natatanging tunog, 

147
00:08:13,116 --> 00:08:16,567
na pinagsama upang makagawa ng ilang pantig, na pinagsama upang bumuo ng mga salita, 

148
00:08:16,567 --> 00:08:20,060
na pinagsama upang bumuo ng mga parirala at higit pang abstract na mga kaisipan, atbp.

149
00:08:21,100 --> 00:08:24,522
Ngunit sa pagbabalik sa kung paano aktwal na gumagana ang alinman sa mga ito, 

150
00:08:24,522 --> 00:08:27,506
isipin ang iyong sarili ngayon na nagdidisenyo kung paano eksaktong 

151
00:08:27,506 --> 00:08:29,920
matukoy ng mga pag-activate sa isang layer ang susunod.

152
00:08:30,860 --> 00:08:34,814
Ang layunin ay magkaroon ng ilang mekanismo na maaaring mapagsama-sama ang 

153
00:08:34,814 --> 00:08:38,980
mga pixel sa mga gilid, o mga gilid sa mga pattern, o mga pattern sa mga digit.

154
00:08:39,440 --> 00:08:42,545
At para mag-zoom in sa isang napaka-espesipikong halimbawa, 

155
00:08:42,545 --> 00:08:46,065
sabihin nating ang pag-asa ay para sa isang partikular na neuron sa 

156
00:08:46,065 --> 00:08:50,620
pangalawang layer upang malaman kung ang imahe ay may gilid sa rehiyong ito dito o wala.

157
00:08:51,440 --> 00:08:55,100
Ang tanong ay kung anong mga parameter ang dapat magkaroon ng network?

158
00:08:55,640 --> 00:08:59,703
Anong mga dial at knobs ang dapat mong i-tweak upang ito ay sapat na nagpapahayag 

159
00:08:59,703 --> 00:09:03,815
upang potensyal na makuha ang pattern na ito, o anumang iba pang pattern ng pixel, 

160
00:09:03,815 --> 00:09:07,780
o ang pattern na maaaring gawing loop ng ilang mga gilid, at iba pang mga bagay?

161
00:09:08,720 --> 00:09:11,995
Well, ang gagawin natin ay magtalaga ng timbang sa bawat isa sa mga 

162
00:09:11,995 --> 00:09:15,560
koneksyon sa pagitan ng ating neuron at ng mga neuron mula sa unang layer.

163
00:09:16,320 --> 00:09:17,700
Ang mga timbang na ito ay mga numero lamang.

164
00:09:18,540 --> 00:09:21,946
Pagkatapos ay kunin ang lahat ng mga pag-activate mula sa unang layer 

165
00:09:21,946 --> 00:09:25,500
at kalkulahin ang kanilang timbang na kabuuan ayon sa mga timbang na ito.

166
00:09:27,700 --> 00:09:31,199
Nakikita kong nakakatulong na isipin ang mga timbang na ito bilang nakaayos sa sarili 

167
00:09:31,199 --> 00:09:34,577
nitong maliit na grid, at gagamit ako ng mga berdeng pixel para ipahiwatig ang mga 

168
00:09:34,577 --> 00:09:37,995
positibong timbang, at mga pulang pixel para ipahiwatig ang mga negatibong timbang, 

169
00:09:37,995 --> 00:09:39,948
kung saan ang liwanag ng pixel na iyon ay ilan. 

170
00:09:39,948 --> 00:09:41,780
maluwag na paglalarawan ng halaga ng timbang.

171
00:09:42,780 --> 00:09:46,463
Ngayon kung ginawa namin ang mga timbang na nauugnay sa halos lahat ng mga pixel na 

172
00:09:46,463 --> 00:09:50,102
zero maliban sa ilang positibong timbang sa rehiyong ito na aming pinapahalagahan, 

173
00:09:50,102 --> 00:09:54,005
kung gayon ang pagkuha ng timbang na kabuuan ng lahat ng mga halaga ng pixel ay talagang 

174
00:09:54,005 --> 00:09:57,820
katumbas ng pagdaragdag ng mga halaga ng pixel sa ang rehiyon na ating pinapahalagahan.

175
00:09:59,140 --> 00:10:02,106
At kung talagang gusto mong malaman kung mayroong isang gilid dito, 

176
00:10:02,106 --> 00:10:05,683
ang maaari mong gawin ay magkaroon ng ilang negatibong timbang na nauugnay sa mga 

177
00:10:05,683 --> 00:10:06,600
nakapaligid na pixel.

178
00:10:07,480 --> 00:10:10,071
Kung gayon ang kabuuan ay pinakamalaki kapag ang mga gitnang pixel na 

179
00:10:10,071 --> 00:10:12,700
iyon ay maliwanag ngunit ang mga nakapalibot na pixel ay mas madidilim.

180
00:10:14,260 --> 00:10:16,511
Kapag nag-compute ka ng weighted sum tulad nito, 

181
00:10:16,511 --> 00:10:19,589
maaari kang magkaroon ng anumang numero, ngunit para sa network na 

182
00:10:19,589 --> 00:10:23,540
ito ang gusto namin ay ang mga activation ay maging ilang halaga sa pagitan ng 0 at 1.

183
00:10:24,120 --> 00:10:28,040
Kaya ang isang karaniwang bagay na dapat gawin ay ang pagbomba ng timbang na kabuuan na 

184
00:10:28,040 --> 00:10:32,050
ito sa ilang function na pumuputol sa totoong linya ng numero sa hanay sa pagitan ng 0 at 

185
00:10:32,050 --> 00:10:32,140
1.

186
00:10:32,460 --> 00:10:35,853
At ang karaniwang function na gumagawa nito ay tinatawag na sigmoid function, 

187
00:10:35,853 --> 00:10:37,420
na kilala rin bilang logistic curve.

188
00:10:38,000 --> 00:10:41,382
Karaniwang napaka-negatibong mga input ay nagtatapos sa malapit sa 0, 

189
00:10:41,382 --> 00:10:44,087
ang mga positibong input ay nagtatapos sa malapit sa 1, 

190
00:10:44,087 --> 00:10:46,600
at ito ay patuloy na tumataas sa paligid ng input 0.

191
00:10:49,120 --> 00:10:52,502
Kaya ang pag-activate ng neuron dito ay karaniwang isang 

192
00:10:52,502 --> 00:10:56,360
sukatan kung gaano kapositibo ang nauugnay na timbang na kabuuan.

193
00:10:57,540 --> 00:10:59,710
Pero hindi naman siguro gusto mong umilaw ang 

194
00:10:59,710 --> 00:11:01,880
neuron kapag mas malaki sa 0 ang weighted sum.

195
00:11:02,280 --> 00:11:06,360
Baka gusto mo lang itong maging aktibo kapag ang kabuuan ay mas malaki kaysa sabihing 10.

196
00:11:06,840 --> 00:11:10,260
Ibig sabihin, gusto mo ng bias para hindi ito aktibo.

197
00:11:11,380 --> 00:11:14,026
Ang gagawin natin pagkatapos ay magdagdag lamang ng ilang iba 

198
00:11:14,026 --> 00:11:16,757
pang numero tulad ng negatibong 10 sa timbang na kabuuan na ito 

199
00:11:16,757 --> 00:11:19,660
bago ito isaksak sa pamamagitan ng sigmoid squishification function.

200
00:11:20,580 --> 00:11:22,440
Ang karagdagang bilang na iyon ay tinatawag na bias.

201
00:11:23,460 --> 00:11:27,413
Kaya't ang mga timbang ay nagsasabi sa iyo kung anong pixel pattern ang kinukuha 

202
00:11:27,413 --> 00:11:31,226
ng neuron na ito sa pangalawang layer, at ang bias ay nagsasabi sa iyo kung gaano 

203
00:11:31,226 --> 00:11:35,180
kataas ang timbang na kabuuan bago magsimulang maging makabuluhang aktibo ang neuron.

204
00:11:36,120 --> 00:11:37,680
At iyon ay isang neuron lamang.

205
00:11:38,280 --> 00:11:44,610
Ang bawat iba pang neuron sa layer na ito ay ikokonekta sa lahat ng 784 pixel neuron mula 

206
00:11:44,610 --> 00:11:50,940
sa unang layer, at bawat isa sa 784 na koneksyon ay may sariling timbang na nauugnay dito.

207
00:11:51,600 --> 00:11:54,535
Gayundin, ang bawat isa ay may ilang bias, ilang iba pang numero na 

208
00:11:54,535 --> 00:11:57,600
idaragdag mo sa timbang na kabuuan bago ito i-squish gamit ang sigmoid.

209
00:11:58,110 --> 00:11:59,540
At iyon ay maraming dapat isipin!

210
00:11:59,960 --> 00:12:06,390
Sa nakatagong layer na ito ng 16 na neuron, iyon ay kabuuang 784 beses na 16 na timbang, 

211
00:12:06,390 --> 00:12:07,980
kasama ang 16 na bias.

212
00:12:08,840 --> 00:12:11,940
At lahat ng iyon ay ang mga koneksyon lamang mula sa unang layer hanggang sa pangalawa.

213
00:12:12,520 --> 00:12:14,930
Ang mga koneksyon sa pagitan ng iba pang mga layer ay mayroon 

214
00:12:14,930 --> 00:12:17,340
ding isang grupo ng mga timbang at bias na nauugnay sa kanila.

215
00:12:18,340 --> 00:12:21,043
Lahat ng sinabi at tapos na, ang network na ito ay 

216
00:12:21,043 --> 00:12:23,800
may halos eksaktong 13,000 kabuuang timbang at bias.

217
00:12:23,800 --> 00:12:26,908
13,000 knobs at dial na maaaring i-tweak at i-on para 

218
00:12:26,908 --> 00:12:29,960
kumilos ang network na ito sa iba't ibang paraan.

219
00:12:31,040 --> 00:12:33,891
Kaya't kapag pinag-uusapan natin ang tungkol sa pag-aaral, 

220
00:12:33,891 --> 00:12:37,331
ang tinutukoy nito ay ang paghahanap ng computer ng wastong setting para sa 

221
00:12:37,331 --> 00:12:41,360
lahat ng napakaraming numerong ito upang talagang malutas nito ang problemang nasa kamay.

222
00:12:42,620 --> 00:12:46,218
Ang isang pag-iisip na eksperimento na sabay-sabay na masaya at uri ng nakakatakot 

223
00:12:46,218 --> 00:12:49,643
ay ang isipin na nakaupo at itinatakda ang lahat ng mga timbang at bias na ito 

224
00:12:49,643 --> 00:12:53,198
sa pamamagitan ng kamay, sinasadyang i-tweak ang mga numero upang ang pangalawang 

225
00:12:53,198 --> 00:12:56,580
layer ay kunin sa mga gilid, ang ikatlong layer ay kunin sa mga pattern, atbp.

226
00:12:56,980 --> 00:13:00,526
Personal kong natutuwa itong kasiya-siya sa halip na ituring lamang ang network 

227
00:13:00,526 --> 00:13:03,806
bilang isang kabuuang black box, dahil kapag hindi gumanap ang network sa 

228
00:13:03,806 --> 00:13:07,308
paraang inaasahan mo, kung nakabuo ka ng kaunting kaugnayan sa kung ano talaga 

229
00:13:07,308 --> 00:13:09,569
ang ibig sabihin ng mga timbang at bias na iyon. , 

230
00:13:09,569 --> 00:13:13,027
mayroon kang panimulang lugar para sa pag-eksperimento kung paano baguhin ang 

231
00:13:13,027 --> 00:13:14,180
istraktura upang mapabuti.

232
00:13:14,960 --> 00:13:18,322
O kapag gumagana ang network ngunit hindi para sa mga kadahilanang maaari mong asahan, 

233
00:13:18,322 --> 00:13:21,104
ang paghuhukay sa kung ano ang ginagawa ng mga timbang at bias ay isang 

234
00:13:21,104 --> 00:13:23,810
magandang paraan upang hamunin ang iyong mga pagpapalagay at talagang 

235
00:13:23,810 --> 00:13:25,820
ilantad ang buong espasyo ng mga posibleng solusyon.

236
00:13:26,840 --> 00:13:30,680
Sa pamamagitan ng paraan, ang aktwal na pag-andar dito ay medyo mahirap isulat, hindi ba?

237
00:13:32,500 --> 00:13:34,998
Kaya hayaan mo akong ipakita sa iyo ang isang mas notationally 

238
00:13:34,998 --> 00:13:37,140
compact na paraan na ang mga koneksyon ay kinakatawan.

239
00:13:37,660 --> 00:13:39,103
Ito ay kung paano mo ito makikita kung pipiliin mong 

240
00:13:39,103 --> 00:13:40,520
magbasa nang higit pa tungkol sa mga neural network.

241
00:13:41,380 --> 00:13:46,730
Ayusin ang lahat ng mga activation mula sa isang layer patungo sa 

242
00:13:46,730 --> 00:13:52,081
isang column bilang isang matrix ay tumutugma sa mga koneksyon sa 

243
00:13:52,081 --> 00:13:58,000
pagitan ng isang layer at isang partikular na neuron sa susunod na layer.

244
00:13:58,540 --> 00:14:02,521
Ang ibig sabihin nito ay ang pagkuha ng timbang na kabuuan ng mga pag-activate 

245
00:14:02,521 --> 00:14:06,402
sa unang layer ayon sa mga timbang na ito ay tumutugma sa isa sa mga termino 

246
00:14:06,402 --> 00:14:09,880
sa produkto ng matrix vector ng lahat ng mayroon tayo sa kaliwa dito.

247
00:14:14,000 --> 00:14:17,598
Oo nga pala, napakaraming machine learning ay nagmumula lamang sa pagkakaroon ng isang 

248
00:14:17,598 --> 00:14:21,279
mahusay na kaalaman sa linear algebra, kaya para sa sinuman sa inyo na nais ng magandang 

249
00:14:21,279 --> 00:14:24,836
visual na pag-unawa para sa mga matrice at kung ano ang ibig sabihin ng matrix vector 

250
00:14:24,836 --> 00:14:27,648
multiplication, tingnan ang seryeng ginawa ko noong linear algebra, 

251
00:14:27,648 --> 00:14:28,600
lalo na ang kabanata 3.

252
00:14:29,240 --> 00:14:32,393
Bumalik sa aming expression, sa halip na pag-usapan ang pagdaragdag ng 

253
00:14:32,393 --> 00:14:34,925
bias sa bawat isa sa mga halagang ito nang nakapag-iisa, 

254
00:14:34,925 --> 00:14:38,213
kinakatawan namin ito sa pamamagitan ng pag-aayos ng lahat ng mga bias na 

255
00:14:38,213 --> 00:14:41,544
iyon sa isang vector, at pagdaragdag ng buong vector sa nakaraang produkto 

256
00:14:41,544 --> 00:14:42,300
ng matrix vector.

257
00:14:43,280 --> 00:14:47,658
Pagkatapos bilang pangwakas na hakbang, magbabalot ako ng sigmoid sa labas dito, 

258
00:14:47,658 --> 00:14:51,604
at kung ano ang dapat na kinakatawan ay ilalapat mo ang sigmoid function 

259
00:14:51,604 --> 00:14:54,740
sa bawat partikular na bahagi ng resultang vector sa loob.

260
00:14:55,940 --> 00:14:59,748
Kaya't sa sandaling isulat mo ang weight matrix na ito at ang mga vector na ito 

261
00:14:59,748 --> 00:15:03,556
bilang kanilang sariling mga simbolo, maaari mong ipaalam ang buong paglipat ng mga 

262
00:15:03,556 --> 00:15:07,409
activation mula sa isang layer patungo sa susunod sa isang napakahigpit at maayos na 

263
00:15:07,409 --> 00:15:11,398
maliit na expression, at ginagawa nitong mas simple at mas simple ang nauugnay na code. 

264
00:15:11,398 --> 00:15:14,980
mas mabilis, dahil maraming mga aklatan ang nag-optimize ng heck out ng matrix 

265
00:15:14,980 --> 00:15:15,660
multiplication.

266
00:15:17,820 --> 00:15:19,583
Tandaan kung paano ko sinabi na ang mga neuron 

267
00:15:19,583 --> 00:15:21,460
na ito ay mga bagay lamang na mayroong mga numero?

268
00:15:22,220 --> 00:15:26,335
Siyempre, ang mga tukoy na numero na hawak nila ay nakasalalay sa imahe 

269
00:15:26,335 --> 00:15:31,308
na iyong pinapakain, kaya mas tumpak na isipin ang bawat neuron bilang isang function, 

270
00:15:31,308 --> 00:15:35,424
isa na kumukuha ng mga output ng lahat ng mga neuron sa nakaraang layer 

271
00:15:35,424 --> 00:15:38,340
at naglalabas ng isang numero sa pagitan ng 0 at 1.

272
00:15:39,200 --> 00:15:42,053
Talagang ang buong network ay isang function lamang, 

273
00:15:42,053 --> 00:15:45,983
isa na kumukuha ng 784 na numero bilang input at naglalabas ng 10 numero 

274
00:15:45,983 --> 00:15:47,060
bilang isang output.

275
00:15:47,560 --> 00:15:50,166
Ito ay isang walang katotohanan na kumplikadong function, 

276
00:15:50,166 --> 00:15:54,031
isa na nagsasangkot ng 13,000 mga parameter sa mga anyo ng mga timbang at bias na ito 

277
00:15:54,031 --> 00:15:57,941
na nakakakuha sa ilang partikular na pattern, at kung saan ay nagsasangkot ng pag-ulit 

278
00:15:57,941 --> 00:16:01,491
ng maraming matrix vector na produkto at ang sigmoid squishification function, 

279
00:16:01,491 --> 00:16:03,648
ngunit ito ay isang function lamang gayunpaman, 

280
00:16:03,648 --> 00:16:06,660
at sa isang paraan na ito ay uri ng pagtiyak na mukhang kumplikado.

281
00:16:07,340 --> 00:16:09,669
Ibig kong sabihin kung ito ay mas simple, ano ang pag-asa 

282
00:16:09,669 --> 00:16:12,280
natin na kaya nitong harapin ang hamon ng pagkilala ng mga digit?

283
00:16:13,340 --> 00:16:14,700
At paano ito humaharap sa hamon na iyon?

284
00:16:15,080 --> 00:16:17,125
Paano natututo ang network na ito ng mga naaangkop na 

285
00:16:17,125 --> 00:16:19,360
timbang at bias sa pamamagitan lamang ng pagtingin sa data?

286
00:16:20,140 --> 00:16:21,937
Well, iyon ang ipapakita ko sa susunod na video, 

287
00:16:21,937 --> 00:16:24,909
at maghuhukay din ako ng kaunti sa kung ano talaga ang ginagawa ng partikular na 

288
00:16:24,909 --> 00:16:26,120
network na ito na nakikita natin.

289
00:16:27,580 --> 00:16:30,664
Ngayon ang punto na dapat kong sabihin na mag-subscribe upang manatiling maabisuhan 

290
00:16:30,664 --> 00:16:33,197
tungkol sa kung kailan lumabas ang video o anumang mga bagong video, 

291
00:16:33,197 --> 00:16:36,465
ngunit sa totoo lang karamihan sa inyo ay hindi talaga nakakatanggap ng mga notification 

292
00:16:36,465 --> 00:16:37,420
mula sa YouTube, hindi ba?

293
00:16:38,020 --> 00:16:41,211
Marahil sa totoo lang, dapat kong sabihing mag-subscribe upang ang mga neural 

294
00:16:41,211 --> 00:16:44,443
network na sumasailalim sa algorithm ng rekomendasyon ng YouTube ay mauunawaan 

295
00:16:44,443 --> 00:16:47,880
na gusto mong makakita ng nilalaman mula sa channel na ito na mairerekomenda sa iyo.

296
00:16:48,560 --> 00:16:49,940
Anyway manatiling naka-post para sa higit pa.

297
00:16:50,760 --> 00:16:53,500
Maraming salamat sa lahat ng sumusuporta sa mga video na ito sa Patreon.

298
00:16:54,000 --> 00:16:57,119
Medyo naging mabagal ako sa pag-usad sa probability series ngayong tag-init, 

299
00:16:57,119 --> 00:16:59,307
ngunit babalikan ko ito pagkatapos ng proyektong ito, 

300
00:16:59,307 --> 00:17:01,900
kaya mga parokyano ay maaari kayong tumingin ng mga update doon.

301
00:17:03,600 --> 00:17:06,258
Upang isara ang mga bagay dito, kasama ko si Leisha Lee na gumawa ng 

302
00:17:06,258 --> 00:17:09,187
kanyang PhD na trabaho sa theoretical side ng deep learning at kasalukuyang 

303
00:17:09,187 --> 00:17:11,768
nagtatrabaho sa isang venture capital firm na tinatawag na Amplify 

304
00:17:11,768 --> 00:17:14,619
Partners na mabait na nagbigay ng ilan sa pagpopondo para sa video na ito.

305
00:17:15,460 --> 00:17:17,218
Kaya Leisha isang bagay na sa tingin ko ay dapat 

306
00:17:17,218 --> 00:17:19,119
nating mabilis na ilabas ang sigmoid function na ito.

307
00:17:19,700 --> 00:17:23,118
Sa pagkakaintindi ko, ginagamit ito ng mga maagang network para i-squish ang nauugnay na 

308
00:17:23,118 --> 00:17:26,267
timbang na kabuuan sa pagitan ng zero at isa, alam mo na ang uri ng motibasyon ng 

309
00:17:26,267 --> 00:17:29,571
biological na pagkakatulad na ito ng mga neuron ay alinman sa pagiging hindi aktibo o 

310
00:17:29,571 --> 00:17:29,840
aktibo.

311
00:17:30,280 --> 00:17:30,300
Eksakto.

312
00:17:30,560 --> 00:17:34,040
Ngunit medyo kakaunting modernong network ang aktwal na gumagamit ng sigmoid.

313
00:17:34,320 --> 00:17:34,320
Oo.

314
00:17:34,440 --> 00:17:35,540
Parang old school diba?

315
00:17:35,760 --> 00:17:38,980
Oo, o sa halip, tila mas madaling sanayin ang relu.

316
00:17:39,400 --> 00:17:42,340
At ang relu ay kumakatawan sa rectified linear unit?

317
00:17:42,680 --> 00:17:47,223
Oo, ito ang uri ng pag-andar kung saan kumukuha ka lamang ng maximum na zero at isang 

318
00:17:47,223 --> 00:17:51,925
kung saan ang a ay ibinibigay sa pamamagitan ng kung ano ang iyong ipinaliwanag sa video 

319
00:17:51,925 --> 00:17:56,627
at kung saan ito ay uri ng motivated mula sa tingin ko ay isang bahagyang sa pamamagitan 

320
00:17:56,627 --> 00:18:01,065
ng isang biological analogy sa kung paano neurons maa-activate o hindi at kaya kung 

321
00:18:01,065 --> 00:18:05,503
pumasa ito sa isang tiyak na threshold ito ang magiging function ng pagkakakilanlan 

322
00:18:05,503 --> 00:18:10,100
ngunit kung hindi, hindi ito maa-activate kaya ito ay magiging zero kaya ito ay uri ng 

323
00:18:10,100 --> 00:18:10,840
pagpapasimple.

324
00:18:11,160 --> 00:18:15,754
Ang paggamit ng mga sigmoid ay hindi nakatulong sa pagsasanay o napakahirap magsanay 

325
00:18:15,754 --> 00:18:20,187
sa ilang mga punto at sinubukan lang ng mga tao ang relu at nangyari itong gumana 

326
00:18:20,187 --> 00:18:24,620
nang mahusay para sa mga hindi kapani-paniwalang malalim na neural network na ito.

327
00:18:25,100 --> 00:18:25,640
Sige salamat Alicia.

