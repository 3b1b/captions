1
00:00:00,000 --> 00:00:04,040
Le jeu Wurdle est devenu assez viral au cours des deux derniers mois, et

2
00:00:04,040 --> 00:00:07,880
n'a jamais négligé une opportunité de cours de mathématiques. Il me semble que

3
00:00:07,880 --> 00:00:12,120
ce jeu constitue un très bon exemple central dans une leçon sur la

4
00:00:12,120 --> 00:00:13,120
théorie de l'information, et en particulier un sujet connu sous le nom d’entropie.

5
00:00:13,120 --> 00:00:17,120
Vous voyez, comme beaucoup de gens, je me suis laissé entraîner dans le puzzle, et

6
00:00:17,120 --> 00:00:21,200
comme beaucoup de programmeurs, je me suis également laissé entraîner à essayer d'écrire un

7
00:00:21,200 --> 00:00:23,200
algorithme qui permettrait de jouer au jeu de la manière la plus optimale possible.

8
00:00:23,200 --> 00:00:26,400
Et ce que j'ai pensé faire ici, c'est simplement parler avec vous

9
00:00:26,400 --> 00:00:29,980
de certains de mes processus et expliquer certains des calculs qui y

10
00:00:29,980 --> 00:00:32,080
sont liés, puisque tout l'algorithme est centré sur cette idée d'entropie.

11
00:00:32,080 --> 00:00:42,180
Tout d’abord, au cas où vous n’en auriez pas entendu parler, qu’est-ce que Wurdle ?

12
00:00:42,180 --> 00:00:45,380
Et pour faire d'une pierre deux coups pendant que nous examinons les règles

13
00:00:45,380 --> 00:00:48,980
du jeu, permettez-moi également de vous montrer où nous allons avec cela,

14
00:00:48,980 --> 00:00:51,380
c'est-à-dire développer un petit algorithme qui jouera essentiellement le jeu pour nous.

15
00:00:51,380 --> 00:00:54,860
Bien que je n'aie pas fait le Wurdle d'aujourd'hui, nous sommes

16
00:00:54,860 --> 00:00:55,860
le 4 février et nous verrons comment le bot se comporte.

17
00:00:55,860 --> 00:00:59,580
Le but de Wurdle est de deviner un mot mystérieux de

18
00:00:59,580 --> 00:01:00,860
cinq lettres, et vous avez six chances différentes de le deviner.

19
00:01:00,860 --> 00:01:05,240
Par exemple, mon robot Wurdle me suggère de commencer par la grue à devinettes.

20
00:01:05,240 --> 00:01:09,300
Chaque fois que vous faites une supposition, vous obtenez des informations

21
00:01:09,300 --> 00:01:10,940
sur la proximité de votre supposition avec la vraie réponse.

22
00:01:10,940 --> 00:01:14,540
Ici, la case grise me dit qu'il n'y a pas de C dans la réponse réelle.

23
00:01:14,540 --> 00:01:18,340
La case jaune m'indique qu'il y a un R, mais il n'est pas dans cette position.

24
00:01:18,340 --> 00:01:21,820
La case verte m'indique que le mot secret a

25
00:01:21,820 --> 00:01:22,820
un A et qu'il est en troisième position.

26
00:01:22,820 --> 00:01:24,300
Et puis il n’y a ni N ni E.

27
00:01:24,300 --> 00:01:27,420
Alors laissez-moi entrer et donner cette information au robot Wurdle.

28
00:01:27,420 --> 00:01:31,500
Nous avons commencé avec la grue, nous avons eu du gris, du jaune, du vert, du gris, du gris.

29
00:01:31,500 --> 00:01:35,460
Ne vous inquiétez pas de toutes les données qu'il affiche en ce moment, je vous l'expliquerai en temps voulu.

30
00:01:35,460 --> 00:01:39,700
Mais sa principale suggestion pour notre deuxième choix est shtick.

31
00:01:39,700 --> 00:01:43,500
Et votre supposition doit être un véritable mot de cinq lettres, mais comme vous

32
00:01:43,500 --> 00:01:45,700
le verrez, elle est assez libérale quant à ce qu'elle vous laissera réellement deviner.

33
00:01:45,700 --> 00:01:48,860
Dans ce cas, nous essayons shtick.

34
00:01:48,860 --> 00:01:50,260
Et bien, les choses s’annoncent plutôt bien.

35
00:01:50,260 --> 00:01:54,580
On touche le S et le H, donc on connaît les trois premières lettres, on sait qu'il y a un R.

36
00:01:54,740 --> 00:01:59,740
Et donc ça va être comme SHA quelque chose de R, ou SHA R quelque chose.

37
00:01:59,740 --> 00:02:03,200
Et il semble que le robot Wurdle sache qu'il ne

38
00:02:03,200 --> 00:02:05,220
reste que deux possibilités, soit un fragment, soit un tranchant.

39
00:02:05,220 --> 00:02:08,620
C'est une sorte de mélange entre eux à ce stade, donc je suppose que c'est

40
00:02:08,620 --> 00:02:11,260
probablement simplement parce que c'est par ordre alphabétique que cela va avec le fragment.

41
00:02:11,260 --> 00:02:13,000
Quelle hourra, c'est la vraie réponse.

42
00:02:13,000 --> 00:02:14,660
Nous l'avons donc eu en trois.

43
00:02:14,660 --> 00:02:17,740
Si vous vous demandez si c'est bon, la façon dont j'ai entendu une

44
00:02:17,740 --> 00:02:20,820
personne dire qu'avec Wurdle, quatre est la normale et trois est un birdie.

45
00:02:20,820 --> 00:02:22,960
Ce qui, je pense, est une analogie assez pertinente.

46
00:02:22,960 --> 00:02:27,560
Il faut être constamment sur son jeu pour en obtenir quatre, mais ce n'est certainement pas fou.

47
00:02:27,560 --> 00:02:30,000
Mais quand vous l'obtenez en trois, ça fait du bien.

48
00:02:30,000 --> 00:02:33,800
Donc, si vous êtes partant, ce que j'aimerais faire ici, c'est simplement parler de mon

49
00:02:33,800 --> 00:02:36,600
processus de réflexion depuis le début sur la façon dont j'aborde le robot Wurdle.

50
00:02:36,600 --> 00:02:39,800
Et comme je l'ai dit, c'est en réalité une excuse pour un cours de théorie de l'information.

51
00:02:39,800 --> 00:02:43,160
L’objectif principal est d’expliquer ce qu’est l’information et ce qu’est l’entropie.

52
00:02:48,560 --> 00:02:52,080
Ma première pensée en abordant ce sujet a été de jeter un

53
00:02:52,080 --> 00:02:53,560
œil aux fréquences relatives des différentes lettres de la langue anglaise.

54
00:02:53,560 --> 00:02:57,800
Alors j'ai pensé, d'accord, y a-t-il une supposition d'ouverture ou une paire de

55
00:02:57,800 --> 00:02:59,960
suppositions d'ouverture qui touche beaucoup de ces lettres les plus fréquentes ?

56
00:02:59,960 --> 00:03:03,780
Et celui que j'aimais beaucoup était d'en faire d'autres suivis de clous.

57
00:03:03,780 --> 00:03:06,980
L’idée est que si vous frappez une lettre, vous savez, vous

58
00:03:06,980 --> 00:03:07,980
obtenez un vert ou un jaune, ça fait toujours du bien.

59
00:03:07,980 --> 00:03:09,460
C'est comme si vous receviez des informations.

60
00:03:09,460 --> 00:03:13,140
Mais dans ces cas-là, même si vous ne frappez pas et que vous

61
00:03:13,140 --> 00:03:16,640
obtenez toujours des gris, cela vous donne quand même beaucoup d'informations puisqu'il est

62
00:03:16,640 --> 00:03:17,640
assez rare de trouver un mot qui n'a aucune de ces lettres.

63
00:03:17,640 --> 00:03:21,840
Mais même quand même, cela ne semble pas super systématique, car

64
00:03:21,840 --> 00:03:23,520
par exemple, cela ne fait rien pour considérer l'ordre des lettres.

65
00:03:23,520 --> 00:03:26,080
Pourquoi taper ongles quand je pourrais taper escargot ?

66
00:03:26,080 --> 00:03:27,720
Est-il préférable d'avoir ce S à la fin ?

67
00:03:27,720 --> 00:03:28,720
Je ne suis pas vraiment sûr.

68
00:03:28,720 --> 00:03:33,500
Maintenant, un de mes amis m'a dit qu'il aimait commencer avec le mot fatigué, ce qui

69
00:03:33,500 --> 00:03:37,160
m'a un peu surpris car il contient des lettres inhabituelles comme le W et le Y.

70
00:03:37,160 --> 00:03:39,400
Mais qui sait, c'est peut-être une meilleure ouverture.

71
00:03:39,400 --> 00:03:43,920
Existe-t-il une sorte de score quantitatif que nous pouvons

72
00:03:43,920 --> 00:03:44,920
attribuer pour juger de la qualité d’une supposition potentielle ?

73
00:03:44,920 --> 00:03:48,640
Maintenant, pour définir la manière dont nous allons classer les suppositions possibles, revenons en arrière

74
00:03:48,640 --> 00:03:51,800
et ajoutons un peu de clarté à la manière exacte dont le jeu est configuré.

75
00:03:51,800 --> 00:03:55,880
Il y a donc une liste de mots qu'il vous permettra de saisir et

76
00:03:55,880 --> 00:03:57,920
qui sont considérés comme des suppositions valables et qui fait environ 13 000 mots.

77
00:03:57,920 --> 00:04:01,560
Mais quand on y regarde, il y a beaucoup de choses vraiment inhabituelles, comme une tête ou

78
00:04:01,560 --> 00:04:07,040
Ali et ARG, le genre de mots qui provoquent des disputes familiales dans une partie de Scrabble.

79
00:04:07,040 --> 00:04:10,600
Mais l’ambiance du jeu est que la réponse sera toujours un mot assez courant.

80
00:04:10,600 --> 00:04:16,080
Et en fait, il existe une autre liste d’environ 2 300 mots qui constituent les réponses possibles.

81
00:04:16,080 --> 00:04:20,320
Et il s'agit d'une liste organisée par des humains, je pense spécifiquement par

82
00:04:20,320 --> 00:04:21,800
la petite amie du créateur du jeu, ce qui est plutôt amusant.

83
00:04:21,800 --> 00:04:25,560
Mais ce que j'aimerais faire, notre défi pour ce projet est de voir si nous

84
00:04:25,560 --> 00:04:30,720
pouvons écrire un programme résolvant Wordle qui n'intègre pas les connaissances antérieures sur cette liste.

85
00:04:30,720 --> 00:04:34,560
D’une part, il existe de nombreux mots de cinq lettres

86
00:04:34,560 --> 00:04:35,560
assez courants que vous ne trouverez pas dans cette liste.

87
00:04:35,560 --> 00:04:38,360
Il serait donc préférable d'écrire un programme un peu plus résistant et qui permettrait

88
00:04:38,360 --> 00:04:41,960
de jouer à Wordle contre n'importe qui, pas seulement contre le site officiel.

89
00:04:41,960 --> 00:04:45,900
Et aussi la raison pour laquelle nous connaissons cette liste de

90
00:04:45,900 --> 00:04:47,440
réponses possibles, c'est parce qu'elle est visible dans le code source.

91
00:04:47,440 --> 00:04:51,620
Mais la manière dont cela est visible dans le code source dépend

92
00:04:51,620 --> 00:04:52,840
de l'ordre spécifique dans lequel les réponses apparaissent de jour en jour.

93
00:04:52,840 --> 00:04:56,400
Vous pouvez donc toujours simplement rechercher quelle sera la réponse de demain.

94
00:04:56,400 --> 00:04:59,140
Il est donc clair qu'il y a un certain sens dans lequel l'utilisation de la liste constitue de la triche.

95
00:04:59,140 --> 00:05:02,900
Et ce qui rend un casse-tête plus intéressant et une leçon de théorie de l'information plus

96
00:05:02,900 --> 00:05:07,640
riche est d'utiliser à la place des données plus universelles comme les fréquences relatives des

97
00:05:07,640 --> 00:05:11,640
mots en général pour capturer cette intuition d'avoir une préférence pour des mots plus courants.

98
00:05:11,640 --> 00:05:16,560
Alors, parmi ces 13 000 possibilités, comment devrions-nous choisir la première supposition ?

99
00:05:16,560 --> 00:05:19,960
Par exemple, si mon ami propose fatigué, comment analyser sa qualité ?

100
00:05:19,960 --> 00:05:25,040
Eh bien, la raison pour laquelle il a dit qu'il aime ce W improbable est qu'il

101
00:05:25,040 --> 00:05:27,880
aime la nature à long terme de la sensation de bien-être si vous frappez ce W.

102
00:05:27,880 --> 00:05:31,400
Par exemple, si le premier modèle révélé ressemblait à ceci, alors il s’avère qu’il

103
00:05:31,400 --> 00:05:36,080
n’y a que 58 mots dans ce lexique géant qui correspondent à ce modèle.

104
00:05:36,080 --> 00:05:38,900
Cela représente donc une énorme réduction par rapport à 13 000.

105
00:05:38,900 --> 00:05:43,320
Mais le revers de la médaille, bien sûr, c’est qu’il est très rare d’avoir un motif comme celui-ci.

106
00:05:43,360 --> 00:05:47,600
Plus précisément, si chaque mot avait la même probabilité d’être la réponse, la

107
00:05:47,600 --> 00:05:51,680
probabilité de trouver ce modèle serait de 58 divisée par environ 13 000.

108
00:05:51,680 --> 00:05:53,880
Bien sûr, il n’est pas également probable qu’elles constituent des réponses.

109
00:05:53,880 --> 00:05:56,680
La plupart de ces mots sont très obscurs, voire discutables.

110
00:05:56,680 --> 00:05:59,560
Mais au moins pour notre première tentative, supposons qu'ils sont

111
00:05:59,560 --> 00:06:02,040
tous également probables, puis affinons cela un peu plus tard.

112
00:06:02,040 --> 00:06:07,360
Le fait est qu’un modèle contenant beaucoup d’informations est, de par sa nature même, peu susceptible de se produire.

113
00:06:07,360 --> 00:06:11,320
En fait, ce que signifie être informatif, c'est que c'est peu probable.

114
00:06:11,920 --> 00:06:16,720
Un modèle beaucoup plus probable à voir avec cette ouverture serait quelque chose

115
00:06:16,720 --> 00:06:18,360
comme ceci, où bien sûr il n'y a pas de W dedans.

116
00:06:18,360 --> 00:06:22,080
Peut-être qu'il y a un E, et peut-être qu'il n'y a pas de A, qu'il n'y a pas de R, qu'il n'y a pas de Y.

117
00:06:22,080 --> 00:06:24,640
Dans ce cas, il y a 1 400 correspondances possibles.

118
00:06:24,640 --> 00:06:29,600
Si toutes les probabilités étaient égales, la probabilité que ce

119
00:06:29,600 --> 00:06:30,680
soit la tendance que vous obtiendriez serait d’environ 11 %.

120
00:06:30,680 --> 00:06:34,320
Les résultats les plus probables sont donc aussi les moins informatifs.

121
00:06:34,320 --> 00:06:38,440
Pour avoir une vue plus globale, permettez-moi de vous montrer la répartition

122
00:06:38,440 --> 00:06:42,000
complète des probabilités sur tous les différents modèles que vous pourriez observer.

123
00:06:42,000 --> 00:06:46,000
Ainsi, chaque barre que vous regardez correspond à un motif possible de couleurs qui pourrait

124
00:06:46,000 --> 00:06:50,500
être révélé, parmi lesquels il y a 3 à la 5ème possibilités, et elles

125
00:06:50,500 --> 00:06:52,960
sont organisées de gauche à droite, de la plus courante à la moins courante.

126
00:06:52,960 --> 00:06:56,200
La possibilité la plus courante ici est donc que vous obteniez uniquement des gris.

127
00:06:56,200 --> 00:06:58,800
Cela se produit environ 14 % du temps.

128
00:06:58,800 --> 00:07:02,040
Et ce que vous espérez lorsque vous faites une supposition, c'est que vous vous

129
00:07:02,040 --> 00:07:06,360
retrouviez quelque part dans cette longue queue, comme ici où il n'y a que

130
00:07:06,360 --> 00:07:09,920
18 possibilités pour ce qui correspond à ce modèle qui ressemble évidemment à ceci.

131
00:07:09,920 --> 00:07:14,080
Ou si nous nous aventurons un peu plus à gauche, vous savez, peut-être que nous irons jusqu'ici.

132
00:07:14,080 --> 00:07:16,560
D'accord, voici un bon puzzle pour vous.

133
00:07:16,560 --> 00:07:20,600
Quels sont les trois mots de la langue anglaise qui commencent par un

134
00:07:20,600 --> 00:07:22,040
W, se terminent par un Y et contiennent un R quelque part ?

135
00:07:22,040 --> 00:07:27,560
Il s’avère que les réponses sont, voyons, verbeuses, vermifuges et ironiques.

136
00:07:27,560 --> 00:07:32,720
Donc, pour juger de la qualité globale de ce mot, nous voulons une sorte

137
00:07:32,720 --> 00:07:35,720
de mesure de la quantité d'informations attendue que vous allez obtenir de cette distribution.

138
00:07:36,360 --> 00:07:41,080
Si nous examinons chaque modèle et multiplions sa probabilité d'apparition par quelque chose

139
00:07:41,080 --> 00:07:46,000
qui mesure son caractère informatif, cela peut peut-être nous donner un score objectif.

140
00:07:46,000 --> 00:07:50,280
Maintenant, votre premier instinct pour savoir ce que devrait être quelque chose pourrait être le nombre de correspondances.

141
00:07:50,280 --> 00:07:52,960
Vous souhaitez un nombre moyen de correspondances inférieur.

142
00:07:52,960 --> 00:07:57,400
Mais j'aimerais plutôt utiliser une mesure plus universelle que nous attribuons souvent à l'information, et

143
00:07:57,400 --> 00:08:01,040
qui sera plus flexible une fois que nous aurons une probabilité différente attribuée à

144
00:08:01,040 --> 00:08:04,320
chacun de ces 13 000 mots pour savoir s'ils constituent ou non la réponse.

145
00:08:10,600 --> 00:08:14,760
L'unité d'information standard est le bit, qui a une formule un peu

146
00:08:14,760 --> 00:08:17,800
amusante, mais qui est vraiment intuitive si l'on regarde simplement des exemples.

147
00:08:17,800 --> 00:08:21,880
Si vous avez une observation qui réduit de moitié votre espace

148
00:08:21,880 --> 00:08:24,200
des possibles, on dit qu’elle ne contient qu’un bit d’information.

149
00:08:24,200 --> 00:08:27,680
Dans notre exemple, l'espace des possibilités est constitué de tous les mots possibles, et il s'avère qu'environ la

150
00:08:27,760 --> 00:08:31,560
moitié des mots de cinq lettres ont un S, un peu moins que cela, mais environ la moitié.

151
00:08:31,560 --> 00:08:35,200
Cette observation vous donnerait donc une information.

152
00:08:35,200 --> 00:08:39,640
Si, au contraire, un nouveau fait réduit cet espace de possibilités

153
00:08:39,640 --> 00:08:42,000
d’un facteur quatre, nous disons qu’il contient deux éléments d’information.

154
00:08:42,000 --> 00:08:45,120
Par exemple, il s’avère qu’environ un quart de ces mots ont un T.

155
00:08:45,120 --> 00:08:49,720
Si l’observation divise cet espace par huit, nous disons qu’il

156
00:08:49,720 --> 00:08:50,920
s’agit de trois éléments d’information, et ainsi de suite.

157
00:08:50,920 --> 00:08:55,000
Quatre bits le coupent en 16ème, cinq bits le coupent en 32ème.

158
00:08:55,000 --> 00:09:00,160
Alors maintenant, vous voudrez peut-être faire une pause et vous demander quelle est la formule

159
00:09:00,160 --> 00:09:04,520
pour obtenir des informations sur le nombre de bits en termes de probabilité d'occurrence ?

160
00:09:04,520 --> 00:09:07,920
Ce que nous disons ici, c'est que lorsque vous prenez la moitié du nombre de bits,

161
00:09:07,920 --> 00:09:11,680
cela équivaut à la même chose que la probabilité, ce qui revient à dire que

162
00:09:11,680 --> 00:09:16,200
deux puissance du nombre de bits est un sur la probabilité, ce qui se réorganise

163
00:09:16,200 --> 00:09:19,680
en disant que l'information est la base logarithmique deux de un divisée par la probabilité.

164
00:09:19,680 --> 00:09:23,200
Et parfois, vous voyez cela avec encore un réarrangement supplémentaire, où

165
00:09:23,200 --> 00:09:25,680
l'information est le log négatif en base deux de la probabilité.

166
00:09:25,680 --> 00:09:29,120
Exprimé ainsi, cela peut paraître un peu bizarre aux non-initiés, mais

167
00:09:29,120 --> 00:09:33,400
il s'agit en réalité de l'idée très intuitive de se demander

168
00:09:33,400 --> 00:09:35,120
combien de fois vous avez réduit de moitié vos possibilités.

169
00:09:35,120 --> 00:09:37,840
Maintenant, si vous vous demandez, vous savez, je pensais que nous jouions juste

170
00:09:37,840 --> 00:09:39,920
à un jeu de mots amusant, pourquoi les logarithmes entrent-ils en scène ?

171
00:09:39,920 --> 00:09:43,920
L'une des raisons pour lesquelles cette unité est plus intéressante est qu'il est beaucoup plus facile de

172
00:09:43,920 --> 00:09:48,120
parler d'événements très improbables, beaucoup plus facile de dire qu'une observation contient 20 bits d'information que

173
00:09:48,120 --> 00:09:53,480
de dire que la probabilité que tel ou tel se produise est de 0. 0000095.

174
00:09:53,480 --> 00:09:57,360
Mais une raison plus importante pour laquelle cette expression logarithmique s’est avérée être un complément

175
00:09:57,360 --> 00:10:02,000
très utile à la théorie des probabilités est la manière dont les informations s’additionnent.

176
00:10:02,000 --> 00:10:05,560
Par exemple, si une observation vous donne deux bits d'information, réduisant

177
00:10:05,560 --> 00:10:10,120
votre espace de quatre, puis qu'une deuxième observation comme votre deuxième

178
00:10:10,120 --> 00:10:14,480
estimation dans Wordle vous donne trois autres bits d'information, vous réduisant

179
00:10:14,480 --> 00:10:17,360
encore d'un facteur huit, le deux ensemble vous donnent cinq informations.

180
00:10:17,360 --> 00:10:21,200
De la même manière que les probabilités aiment se multiplier, les informations aiment s’ajouter.

181
00:10:21,200 --> 00:10:24,920
Ainsi, dès que nous sommes dans le domaine de quelque chose comme une valeur attendue,

182
00:10:24,920 --> 00:10:28,660
où nous additionnons un tas de nombres, les journaux rendent la gestion beaucoup plus agréable.

183
00:10:28,660 --> 00:10:32,600
Revenons à notre distribution pour Weary et ajoutons un autre petit

184
00:10:32,600 --> 00:10:35,560
tracker ici, nous montrant la quantité d'informations disponibles pour chaque modèle.

185
00:10:35,560 --> 00:10:38,760
La principale chose que je veux que vous remarquiez est que plus la probabilité est élevée à mesure

186
00:10:38,760 --> 00:10:43,500
que nous arrivons à ces modèles les plus probables, plus l'information est faible, moins vous gagnez de bits.

187
00:10:43,500 --> 00:10:47,360
La façon dont nous mesurons la qualité de cette supposition sera de prendre la

188
00:10:47,360 --> 00:10:51,620
valeur attendue de cette information, où nous examinons chaque modèle, nous disons quelle est

189
00:10:51,620 --> 00:10:54,940
sa probabilité, puis nous la multiplions par le nombre d'éléments d'information que nous obtenons.

190
00:10:54,940 --> 00:10:58,480
Et dans l’exemple de Weary, cela s’avère être 4. 9 bits.

191
00:10:58,480 --> 00:11:02,800
Ainsi, en moyenne, les informations que vous obtenez de cette supposition d’ouverture

192
00:11:02,800 --> 00:11:05,660
équivaut à réduire de moitié votre espace des possibilités environ cinq fois.

193
00:11:05,660 --> 00:11:10,260
En revanche, un exemple de supposition avec une valeur

194
00:11:10,260 --> 00:11:13,220
d’information attendue plus élevée serait quelque chose comme Slate.

195
00:11:13,220 --> 00:11:16,180
Dans ce cas, vous remarquerez que la distribution semble beaucoup plus plate.

196
00:11:16,180 --> 00:11:20,780
En particulier, l'occurrence la plus probable de tous les gris n'a qu'environ 6 % de chances de

197
00:11:20,780 --> 00:11:25,940
se produire, donc au minimum vous en obtenez évidemment 3. 9 informations.

198
00:11:25,940 --> 00:11:29,140
Mais c’est un minimum, vous obtiendrez généralement quelque chose de mieux que cela.

199
00:11:29,140 --> 00:11:33,380
Et il s'avère que lorsque vous analysez les chiffres sur celui-ci et

200
00:11:33,380 --> 00:11:36,420
additionnez tous les termes pertinents, l'information moyenne est d'environ 5. 8.

201
00:11:36,420 --> 00:11:42,140
Ainsi, contrairement à Weary, votre espace de possibilités sera en

202
00:11:42,140 --> 00:11:43,940
moyenne environ deux fois plus grand après cette première hypothèse.

203
00:11:43,940 --> 00:11:49,540
Il existe en fait une histoire amusante sur le nom de cette valeur attendue de la quantité d'informations.

204
00:11:49,540 --> 00:11:52,580
La théorie de l'information a été développée par Claude Shannon, qui travaillait aux Bell Labs

205
00:11:52,580 --> 00:11:57,620
dans les années 1940, mais il discutait de certaines de ses idées encore non

206
00:11:57,620 --> 00:12:01,500
publiées avec John von Neumann, qui était ce géant intellectuel de l'époque, très en vue.

207
00:12:01,500 --> 00:12:04,180
en mathématiques et en physique et les débuts de ce qui allait devenir l'informatique.

208
00:12:04,180 --> 00:12:07,260
Et lorsqu'il a mentionné qu'il n'avait pas vraiment un bon nom pour

209
00:12:07,260 --> 00:12:12,540
cette valeur attendue de la quantité d'information, von Neumann aurait dit, selon

210
00:12:12,540 --> 00:12:14,720
l'histoire, eh bien, vous devriez appeler cela entropie, et pour deux raisons.

211
00:12:14,720 --> 00:12:18,400
En premier lieu, votre fonction d'incertitude a été utilisée en mécanique statistique sous ce nom,

212
00:12:18,400 --> 00:12:23,100
donc elle a déjà un nom, et en deuxième lieu, et plus important encore, personne

213
00:12:23,100 --> 00:12:26,940
ne sait ce qu'est réellement l'entropie, donc dans un débat vous aurez toujours ont l'avantage.

214
00:12:26,940 --> 00:12:31,420
Donc, si le nom semble un peu mystérieux, et si

215
00:12:31,420 --> 00:12:33,420
l’on en croit cette histoire, c’est en quelque sorte intentionnel.

216
00:12:33,420 --> 00:12:36,740
De plus, si vous vous interrogez sur sa relation avec toutes ces deuxièmes

217
00:12:36,740 --> 00:12:40,820
lois de la thermodynamique issues de la physique, il y a certainement un

218
00:12:40,820 --> 00:12:44,780
lien, mais à l'origine, Shannon ne traitait que de la théorie des probabilités

219
00:12:44,780 --> 00:12:49,340
pures, et pour nos besoins ici, lorsque j'utilise le mot entropie, je veux

220
00:12:49,340 --> 00:12:50,820
juste que vous réfléchissiez à la valeur informationnelle attendue d'une supposition particulière.

221
00:12:50,820 --> 00:12:54,380
Vous pouvez considérer l’entropie comme la mesure de deux choses simultanément.

222
00:12:54,380 --> 00:12:57,420
La première concerne la platitude de la distribution.

223
00:12:57,420 --> 00:13:01,700
Plus une distribution est proche de l’uniforme, plus cette entropie sera élevée.

224
00:13:01,700 --> 00:13:06,340
Dans notre cas, où il y a 3 modèles au total sur 5, pour une distribution uniforme, l'observation de

225
00:13:06,340 --> 00:13:11,340
l'un d'entre eux aurait un journal d'informations de base 2 sur 3 au 5, qui se trouve être

226
00:13:11,340 --> 00:13:17,860
7. 92, c'est donc le maximum absolu que vous pourriez avoir pour cette entropie.

227
00:13:17,860 --> 00:13:21,900
Mais l’entropie est aussi en quelque sorte une mesure

228
00:13:21,900 --> 00:13:22,900
du nombre de possibilités qui existent en premier lieu.

229
00:13:22,900 --> 00:13:26,980
Par exemple, si vous avez un mot dans lequel il n'y a que 16 modèles

230
00:13:26,980 --> 00:13:32,760
possibles, et chacun est également probable, cette entropie, cette information attendue, serait de 4 bits.

231
00:13:32,760 --> 00:13:36,880
Mais si vous avez un autre mot où il y a 64 modèles possibles qui

232
00:13:36,880 --> 00:13:41,000
pourraient apparaître, et ils sont tous également probables, alors l'entropie serait de 6 bits.

233
00:13:41,000 --> 00:13:45,800
Donc, si vous voyez une distribution dans la nature qui a une entropie de 6 bits, c'est

234
00:13:45,800 --> 00:13:50,000
un peu comme si cela disait qu'il y a autant de variation et d'incertitude dans ce

235
00:13:50,000 --> 00:13:54,400
qui est sur le point de se produire que s'il y avait 64 résultats également probables.

236
00:13:54,400 --> 00:13:58,360
Pour mon premier passage au Wurtelebot, je l'ai fait faire comme ça.

237
00:13:58,360 --> 00:14:03,560
Il passe en revue toutes les suppositions possibles que vous pourriez avoir, les 13 000 mots,

238
00:14:03,560 --> 00:14:08,580
calcule l'entropie pour chacun d'entre eux, ou plus précisément, l'entropie de la distribution à travers

239
00:14:08,580 --> 00:14:13,040
tous les modèles que vous pourriez voir, pour chacun d'entre eux, et choisit le plus

240
00:14:13,040 --> 00:14:17,200
élevé, puisque c'est celui qui est susceptible de réduire au maximum votre espace des possibles.

241
00:14:17,200 --> 00:14:20,120
Et même si je n’ai parlé ici que de la première

242
00:14:20,120 --> 00:14:21,680
supposition, cela fait la même chose pour les prochaines suppositions.

243
00:14:21,680 --> 00:14:25,100
Par exemple, après avoir vu un modèle sur cette première supposition, qui vous limiterait à un

244
00:14:25,100 --> 00:14:29,300
plus petit nombre de mots possibles en fonction de ce qui correspond à cela, vous

245
00:14:29,300 --> 00:14:32,300
jouez simplement au même jeu en ce qui concerne ce plus petit ensemble de mots.

246
00:14:32,300 --> 00:14:36,500
Pour une seconde supposition proposée, vous examinez la distribution de tous les modèles qui

247
00:14:36,500 --> 00:14:41,540
pourraient survenir à partir de cet ensemble plus restreint de mots, vous recherchez

248
00:14:41,540 --> 00:14:45,480
parmi les 13 000 possibilités et vous trouvez celle qui maximise cette entropie.

249
00:14:45,480 --> 00:14:48,980
Pour vous montrer comment cela fonctionne en action, permettez-moi de vous présenter une petite variante de

250
00:14:48,980 --> 00:14:54,060
Wurtele que j'ai écrite et qui montre les points saillants de cette analyse dans les marges.

251
00:14:54,460 --> 00:14:57,820
Après avoir effectué tous ses calculs d'entropie, à droite, il

252
00:14:57,820 --> 00:15:00,340
nous montre lesquels ont les informations attendues les plus élevées.

253
00:15:00,340 --> 00:15:04,940
Il s'avère que la première réponse, du moins pour le moment, nous affinerons cela plus tard,

254
00:15:04,940 --> 00:15:11,140
est Tares, ce qui signifie, euh, bien sûr, une vesce, la vesce la plus courante.

255
00:15:11,140 --> 00:15:14,180
Chaque fois que nous faisons une supposition ici, où peut-être j'ignore en quelque sorte

256
00:15:14,180 --> 00:15:19,220
ses recommandations et opte pour Slate, parce que j'aime Slate, nous pouvons voir combien

257
00:15:19,220 --> 00:15:23,300
d'informations attendues il contenait, mais ensuite à droite du mot ici, cela nous montre

258
00:15:23,340 --> 00:15:24,980
combien informations réelles que nous avons obtenues, compte tenu de ce modèle particulier.

259
00:15:24,980 --> 00:15:28,660
Alors là, on dirait que nous n’avons pas eu de chance, on s’attendait à en avoir 5. 8, mais

260
00:15:28,660 --> 00:15:30,660
nous avons obtenu quelque chose avec moins que cela.

261
00:15:30,660 --> 00:15:34,020
Et puis sur le côté gauche, ici, cela nous montre tous

262
00:15:34,020 --> 00:15:35,860
les différents mots possibles donnés là où nous en sommes actuellement.

263
00:15:35,860 --> 00:15:39,820
Les barres bleues nous indiquent la probabilité qu'il pense à chaque mot, donc pour le moment, il

264
00:15:39,820 --> 00:15:44,140
suppose que chaque mot a la même probabilité d'apparaître, mais nous affinerons cela dans un instant.

265
00:15:44,140 --> 00:15:48,580
Et puis cette mesure d'incertitude nous indique l'entropie de cette distribution parmi les

266
00:15:48,580 --> 00:15:53,220
mots possibles, ce qui, à l'heure actuelle, parce qu'il s'agit d'une distribution

267
00:15:53,300 --> 00:15:55,940
uniforme, n'est qu'une manière inutilement compliquée de compter le nombre de possibilités.

268
00:15:55,940 --> 00:16:01,700
Par exemple, si nous prenons 2 puissance 13. 66, cela devrait

269
00:16:01,700 --> 00:16:02,700
être autour des 13 000 possibilités.

270
00:16:02,700 --> 00:16:06,780
Je suis un peu en retrait ici, mais uniquement parce que je n'affiche pas toutes les décimales.

271
00:16:06,780 --> 00:16:10,260
Pour le moment, cela peut sembler redondant et compliquer excessivement les choses, mais

272
00:16:10,260 --> 00:16:12,780
vous comprendrez pourquoi il est utile d'avoir les deux chiffres en une minute.

273
00:16:12,780 --> 00:16:16,780
Donc, ici, il semble que cela suggère que l'entropie la plus élevée pour notre deuxième

274
00:16:16,780 --> 00:16:19,700
hypothèse est Ramen, ce qui, encore une fois, ne ressemble vraiment pas à un mot.

275
00:16:19,700 --> 00:16:25,660
Donc, pour prendre le dessus sur le plan moral ici, je vais aller de l'avant et taper Rains.

276
00:16:25,660 --> 00:16:27,540
Et encore une fois, on dirait que nous n’avons pas eu de chance.

277
00:16:27,540 --> 00:16:32,100
Nous en attendions 4. 3 bits et nous n’en avons que 3. 39 bits d'informations.

278
00:16:32,100 --> 00:16:35,060
Cela nous ramène donc à 55 possibilités.

279
00:16:35,060 --> 00:16:38,860
Et ici, je vais peut-être simplement suivre ce que cela suggère,

280
00:16:38,860 --> 00:16:40,200
à savoir un combo, peu importe ce que cela signifie.

281
00:16:40,200 --> 00:16:43,300
Et d'accord, c'est en fait une bonne occasion de résoudre un casse-tête.

282
00:16:43,300 --> 00:16:47,020
Cela nous dit que ce modèle nous donne 4. 7 informations.

283
00:16:47,020 --> 00:16:52,400
Mais sur la gauche, avant de voir ce schéma, il y en avait 5. 78 bits d'incertitude.

284
00:16:52,400 --> 00:16:56,860
Alors, comme quiz pour vous, qu'est-ce que cela signifie sur le nombre de possibilités restantes ?

285
00:16:56,860 --> 00:17:02,280
Eh bien, cela signifie que nous en sommes réduits à un peu

286
00:17:02,280 --> 00:17:04,700
d’incertitude, ce qui revient à dire qu’il y a deux réponses possibles.

287
00:17:04,700 --> 00:17:06,520
C'est un choix 50-50.

288
00:17:06,520 --> 00:17:09,860
Et à partir de là, parce que vous et moi savons quels mots

289
00:17:09,860 --> 00:17:11,220
sont les plus courants, nous savons que la réponse devrait être abyssale.

290
00:17:11,220 --> 00:17:13,540
Mais tel qu’il est écrit actuellement, le programme ne le sait pas.

291
00:17:13,540 --> 00:17:17,560
Alors il continue, essayant d'obtenir autant d'informations que possible, jusqu'à ce

292
00:17:17,560 --> 00:17:20,360
qu'il ne reste plus qu'une possibilité, puis il la devine.

293
00:17:20,360 --> 00:17:22,700
Nous avons donc évidemment besoin d’une meilleure stratégie de fin de partie.

294
00:17:22,700 --> 00:17:26,540
Mais disons que nous appelons cette version l'un de nos solveurs de

295
00:17:26,540 --> 00:17:30,740
mots, puis que nous exécutons quelques simulations pour voir comment cela fonctionne.

296
00:17:30,740 --> 00:17:34,240
Donc, la façon dont cela fonctionne est de jouer à tous les jeux de mots possibles.

297
00:17:34,240 --> 00:17:38,780
Il passe en revue tous ces 2315 mots qui sont les véritables réponses aux mots.

298
00:17:38,780 --> 00:17:41,340
Il s’agit essentiellement de l’utiliser comme ensemble de tests.

299
00:17:41,340 --> 00:17:45,820
Et avec cette méthode naïve qui consiste à ne pas considérer à quel point un mot est courant et à

300
00:17:45,820 --> 00:17:50,480
essayer simplement de maximiser l'information à chaque étape du processus, jusqu'à ce qu'il s'agisse d'un et d'un seul choix.

301
00:17:50,480 --> 00:17:55,100
À la fin de la simulation, le score moyen s’élève à environ 4. 124.

302
00:17:55,100 --> 00:17:59,780
Ce qui n’est pas mal, pour être honnête, je m’attendais à faire pire.

303
00:17:59,780 --> 00:18:03,040
Mais les gens qui jouent aux mots vous diront qu’ils peuvent généralement l’obtenir en 4.

304
00:18:03,040 --> 00:18:05,260
Le véritable défi est d’en obtenir autant en 3 que possible.

305
00:18:05,260 --> 00:18:08,920
C'est un écart assez important entre le score de 4 et le score de 3.

306
00:18:08,920 --> 00:18:13,300
Le fruit évident ici est d'incorporer d'une manière ou d'une autre si

307
00:18:13,300 --> 00:18:23,160
un mot est courant ou non, et comment faire exactement cela.

308
00:18:23,160 --> 00:18:26,860
La façon dont je l'ai abordé consiste à obtenir une liste

309
00:18:26,860 --> 00:18:28,560
des fréquences relatives de tous les mots de la langue anglaise.

310
00:18:28,560 --> 00:18:32,560
Et je viens d'utiliser la fonction de données de fréquence des mots de Mathematica,

311
00:18:32,560 --> 00:18:35,520
qui elle-même est extraite de l'ensemble de données publiques Google Books English Ngram.

312
00:18:35,520 --> 00:18:38,680
Et c'est plutôt amusant à regarder, par exemple si nous les

313
00:18:38,680 --> 00:18:40,120
trions des mots les plus courants aux mots les moins courants.

314
00:18:40,120 --> 00:18:43,740
De toute évidence, ce sont les mots de 5 lettres les plus courants dans la langue anglaise.

315
00:18:43,740 --> 00:18:46,480
Ou plutôt, c'est le 8ème plus courant.

316
00:18:46,480 --> 00:18:49,440
Le premier est lequel, puis il y a là et là.

317
00:18:49,440 --> 00:18:53,020
Premier en lui-même n'est pas premier, mais 9ème, et il est logique

318
00:18:53,020 --> 00:18:57,840
que ces autres mots apparaissent plus souvent, là où ceux qui suivent

319
00:18:57,840 --> 00:18:59,000
premier sont après, où et ceux-ci étant juste un peu moins courants.

320
00:18:59,000 --> 00:19:04,400
Désormais, en utilisant ces données pour modéliser la probabilité que chacun de ces mots

321
00:19:04,400 --> 00:19:06,760
soit la réponse finale, cela ne devrait pas être simplement proportionnel à la fréquence.

322
00:19:07,020 --> 00:19:12,560
Par exemple, à qui on attribue une note de 0. 002 dans cet ensemble de données, alors que

323
00:19:12,560 --> 00:19:15,200
le mot tresse est en quelque sorte environ 1 000 fois moins probable.

324
00:19:15,200 --> 00:19:19,400
Mais ces deux mots sont suffisamment courants pour qu’ils valent certainement la peine d’être pris en compte.

325
00:19:19,400 --> 00:19:21,900
Nous voulons donc davantage un seuil binaire.

326
00:19:21,900 --> 00:19:26,520
La façon dont j'ai procédé est d'imaginer prendre toute cette liste triée de mots, puis

327
00:19:26,520 --> 00:19:31,060
de la disposer sur un axe des x, puis d'appliquer la fonction sigmoïde, qui est

328
00:19:31,060 --> 00:19:35,540
la manière standard d'avoir une fonction dont la sortie est fondamentalement binaire, c'est soit 0,

329
00:19:35,540 --> 00:19:38,500
soit 1, mais il y a un lissage entre les deux pour cette région d'incertitude.

330
00:19:38,500 --> 00:19:43,900
Donc, essentiellement, la probabilité que j'attribue à chaque mot d'être dans la liste finale sera la valeur

331
00:19:43,900 --> 00:19:49,540
de la fonction sigmoïde ci-dessus, quel que soit l'endroit où elle se trouve sur l'axe des x.

332
00:19:49,540 --> 00:19:53,940
Cela dépend évidemment de quelques paramètres, par exemple la largeur de l'espace sur l'axe des x

333
00:19:53,940 --> 00:19:59,660
que ces mots remplissent détermine la façon dont nous passons progressivement ou abruptement de 1

334
00:19:59,660 --> 00:20:03,000
à 0, et l'endroit où nous les situons de gauche à droite détermine le seuil.

335
00:20:03,160 --> 00:20:07,340
Pour être honnête, j’ai simplement fait cela en me léchant le doigt et en le mettant face au vent.

336
00:20:07,340 --> 00:20:10,800
J'ai parcouru la liste triée et essayé de trouver une fenêtre dans laquelle,

337
00:20:10,800 --> 00:20:15,280
lorsque je l'ai regardée, j'ai pensé qu'environ la moitié de ces mots étaient

338
00:20:15,280 --> 00:20:17,680
plus susceptibles qu'improbables d'être la réponse finale, et je l'ai utilisé comme seuil.

339
00:20:17,680 --> 00:20:21,840
Une fois que nous avons une distribution comme celle-ci entre les mots, cela

340
00:20:21,840 --> 00:20:24,460
nous donne une autre situation dans laquelle l'entropie devient cette mesure vraiment utile.

341
00:20:24,460 --> 00:20:28,480
Par exemple, disons que nous jouons à un jeu et que nous commençons avec mes

342
00:20:28,480 --> 00:20:32,480
anciens premiers mots, qui étaient une plume et des ongles, et que nous nous

343
00:20:32,480 --> 00:20:33,760
retrouvons avec une situation où il y a quatre mots possibles qui y correspondent.

344
00:20:33,760 --> 00:20:36,440
Et disons que nous les considérons tous également probables.

345
00:20:36,440 --> 00:20:40,000
Laissez-moi vous demander, quelle est l'entropie de cette distribution ?

346
00:20:40,000 --> 00:20:45,920
Eh bien, les informations associées à chacune de ces possibilités seront la base log

347
00:20:45,920 --> 00:20:50,800
2 sur 4, puisque chacune vaut 1 et 4, et cela fait 2.

348
00:20:50,800 --> 00:20:52,780
Deux informations, quatre possibilités.

349
00:20:52,780 --> 00:20:54,360
Tout cela est très bien.

350
00:20:54,360 --> 00:20:58,320
Mais et si je vous disais qu'en réalité il y a plus de quatre matches ?

351
00:20:58,320 --> 00:21:02,600
En réalité, lorsque nous parcourons la liste complète de mots, il y a 16 mots qui y correspondent.

352
00:21:02,600 --> 00:21:07,260
Mais supposons que notre modèle attribue une très faible probabilité à ces 12 autres mots d'être

353
00:21:07,260 --> 00:21:11,440
réellement la réponse finale, quelque chose comme 1 sur 1 000 parce qu'ils sont vraiment obscurs.

354
00:21:11,440 --> 00:21:15,480
Maintenant, laissez-moi vous demander, quelle est l'entropie de cette distribution ?

355
00:21:15,480 --> 00:21:19,600
Si l'entropie mesurait uniquement le nombre de correspondances ici, alors vous pourriez

356
00:21:19,600 --> 00:21:24,760
vous attendre à ce qu'elle ressemble à la base logarithmique 2 sur

357
00:21:24,760 --> 00:21:26,200
16, qui serait 4, soit deux bits d'incertitude de plus qu'auparavant.

358
00:21:26,200 --> 00:21:30,320
Mais bien entendu, l’incertitude réelle n’est pas vraiment différente de celle que nous connaissions auparavant.

359
00:21:30,320 --> 00:21:33,840
Ce n'est pas parce qu'il y a ces 12 mots vraiment obscurs qu'il

360
00:21:33,840 --> 00:21:38,200
serait d'autant plus surprenant d'apprendre que la réponse finale est charme, par exemple.

361
00:21:38,200 --> 00:21:42,080
Ainsi, lorsque vous effectuez réellement le calcul ici et que vous additionnez la probabilité

362
00:21:42,080 --> 00:21:45,960
de chaque occurrence multipliée par les informations correspondantes, vous obtenez 2. 11 bits.

363
00:21:45,960 --> 00:21:50,280
Je dis juste qu'il s'agit essentiellement de deux éléments, essentiellement de ces quatre possibilités,

364
00:21:50,280 --> 00:21:54,240
mais il y a un peu plus d'incertitude à cause de tous ces événements

365
00:21:54,240 --> 00:21:57,120
hautement improbables, même si si vous les appreniez, vous en tireriez une tonne d'informations.

366
00:21:57,120 --> 00:22:00,800
Donc, en effectuant un zoom arrière, cela fait partie de ce qui fait

367
00:22:00,800 --> 00:22:01,800
de Wordle un si bel exemple pour une leçon de théorie de l'information.

368
00:22:01,800 --> 00:22:05,280
Nous avons ces deux applications distinctes de sensation pour l’entropie.

369
00:22:05,280 --> 00:22:09,640
Le premier nous dit quelle est l'information attendue que nous obtiendrons

370
00:22:09,640 --> 00:22:14,560
à partir d'une supposition donnée, et le second nous dit

371
00:22:14,560 --> 00:22:16,480
: pouvons-nous mesurer l'incertitude restante parmi tous les mots possibles.

372
00:22:16,480 --> 00:22:19,800
Et je dois souligner que, dans le premier cas où nous examinons les informations attendues d'une supposition,

373
00:22:19,800 --> 00:22:25,000
une fois que nous avons une pondération inégale des mots, cela affecte le calcul de l'entropie.

374
00:22:25,000 --> 00:22:28,600
Par exemple, permettez-moi de reprendre le même cas que nous avons examiné

375
00:22:28,600 --> 00:22:33,560
plus tôt concernant la distribution associée à Weary, mais cette fois

376
00:22:33,560 --> 00:22:34,560
en utilisant une distribution non uniforme sur tous les mots possibles.

377
00:22:34,560 --> 00:22:39,360
Alors laissez-moi voir si je peux trouver ici une partie qui l’illustre assez bien.

378
00:22:39,360 --> 00:22:42,480
Ok, ici, c'est plutôt bien.

379
00:22:42,480 --> 00:22:46,360
Ici, nous avons deux modèles adjacents qui sont à peu près également probables,

380
00:22:46,360 --> 00:22:49,480
mais l'un d'eux, nous dit-on, a 32 mots possibles qui lui correspondent.

381
00:22:49,480 --> 00:22:54,080
Et si nous vérifions ce qu’ils sont, ce sont ces 32 mots, qui

382
00:22:54,080 --> 00:22:55,600
ne sont que des mots très improbables lorsque vous les parcourez des yeux.

383
00:22:55,600 --> 00:23:00,400
Il est difficile de trouver des réponses qui semblent plausibles, peut-être des cris, mais

384
00:23:00,400 --> 00:23:04,440
si nous regardons le modèle voisin dans la distribution, qui est considéré comme

385
00:23:04,440 --> 00:23:08,920
tout aussi probable, on nous dit qu'il n'y a que 8 correspondances possibles, donc

386
00:23:08,920 --> 00:23:09,920
un quart comme de nombreux matches, mais c'est à peu près aussi probable.

387
00:23:09,920 --> 00:23:12,520
Et lorsque nous récupérons ces matchs, nous pouvons comprendre pourquoi.

388
00:23:12,520 --> 00:23:17,840
Certaines d’entre elles sont des réponses réellement plausibles, comme la sonnerie, la colère ou les raps.

389
00:23:17,840 --> 00:23:22,000
Pour illustrer comment nous intégrons tout cela, permettez-moi d'afficher ici la version 2 de Wordlebot, et il

390
00:23:22,000 --> 00:23:25,960
y a deux ou trois différences principales par rapport à la première que nous avons vue.

391
00:23:25,960 --> 00:23:29,460
Tout d'abord, comme je viens de le dire, la façon dont nous calculons ces

392
00:23:29,460 --> 00:23:34,800
entropies, ces valeurs attendues de l'information, utilise désormais des distributions plus raffinées entre

393
00:23:34,800 --> 00:23:39,300
les modèles qui intègrent la probabilité qu'un mot donné soit réellement la réponse.

394
00:23:39,300 --> 00:23:44,160
Il se trouve que les larmes sont toujours au premier rang, même si les suivantes sont un peu différentes.

395
00:23:44,160 --> 00:23:47,920
Deuxièmement, lorsqu'il classera ses meilleurs choix, il conservera désormais un modèle de probabilité que chaque

396
00:23:47,920 --> 00:23:52,600
mot soit la réponse réelle, et il l'intégrera dans sa décision, qui est plus

397
00:23:52,600 --> 00:23:55,520
facile à voir une fois que nous avons quelques suppositions sur la réponse. tableau.

398
00:23:55,520 --> 00:24:01,120
Encore une fois, nous ignorons sa recommandation, car nous ne pouvons pas laisser les machines diriger nos vies.

399
00:24:01,120 --> 00:24:05,160
Et je suppose que je devrais mentionner une autre chose différente ici, à gauche, que la

400
00:24:05,160 --> 00:24:10,080
valeur d'incertitude, ce nombre de bits, n'est plus seulement redondante avec le nombre de correspondances possibles.

401
00:24:10,080 --> 00:24:16,520
Maintenant, si nous le retirons et calculons 2 puissance 8. 02, qui est un peu au-dessus de

402
00:24:16,520 --> 00:24:22,640
256, je suppose 259, ce qu'il dit, c'est que même s'il y a 526 mots

403
00:24:22,640 --> 00:24:26,400
au total qui correspondent réellement à ce modèle, le degré d'incertitude qu'il a est

404
00:24:26,400 --> 00:24:29,760
plus proche de ce qu'il serait s'il y avait 259 mots également probables. résultats.

405
00:24:29,760 --> 00:24:31,100
Vous pouvez y penser comme ceci.

406
00:24:31,100 --> 00:24:35,560
Il sait que le borx n'est pas la réponse, pareil pour les yorts, le zorl

407
00:24:35,560 --> 00:24:37,840
et le zorus, donc c'est un peu moins incertain que dans le cas précédent.

408
00:24:37,840 --> 00:24:40,220
Ce nombre de bits sera plus petit.

409
00:24:40,220 --> 00:24:44,040
Et si je continue à jouer au jeu, j'affine cela avec quelques

410
00:24:44,040 --> 00:24:48,680
suppositions qui sont à propos de ce que je voudrais expliquer ici.

411
00:24:48,680 --> 00:24:52,520
À la quatrième hypothèse, si vous regardez ses meilleurs choix, vous

412
00:24:52,520 --> 00:24:53,800
pouvez voir qu'il ne s'agit plus seulement de maximiser l'entropie.

413
00:24:53,800 --> 00:24:58,480
Donc à ce stade, il y a techniquement sept possibilités, mais les

414
00:24:58,480 --> 00:25:00,780
seules qui ont une chance significative sont les dortoirs et les mots.

415
00:25:00,780 --> 00:25:04,760
Et vous pouvez voir qu'il classe ces deux valeurs au-dessus de

416
00:25:04,760 --> 00:25:07,560
toutes ces autres valeurs, qui, à proprement parler, donneraient plus d'informations.

417
00:25:07,560 --> 00:25:11,200
La toute première fois que j'ai fait cela, j'ai simplement additionné ces deux nombres pour mesurer la

418
00:25:11,200 --> 00:25:14,580
qualité de chaque supposition, ce qui a en fait mieux fonctionné que vous ne le pensez.

419
00:25:14,580 --> 00:25:17,600
Mais cela ne semblait vraiment pas systématique, et je suis sûr qu'il existe d'autres

420
00:25:17,600 --> 00:25:19,880
approches que les gens pourraient adopter, mais voici celle sur laquelle j'ai atterri.

421
00:25:19,880 --> 00:25:24,200
Si nous envisageons la perspective d'une prochaine supposition, comme dans ce cas les mots, ce

422
00:25:24,200 --> 00:25:28,440
qui nous importe vraiment, c'est le score attendu de notre jeu si nous faisons cela.

423
00:25:28,440 --> 00:25:32,880
Et pour calculer ce score attendu, nous disons quelle est la probabilité que

424
00:25:32,880 --> 00:25:35,640
les mots soient la réponse réelle, ce qui est actuellement décrit à 58 %.

425
00:25:36,080 --> 00:25:40,400
Nous disons qu'avec 58 % de chances, notre score dans ce jeu serait de 4.

426
00:25:40,400 --> 00:25:46,240
Et puis avec la probabilité de 1 moins 58 %, notre score sera supérieur à 4.

427
00:25:46,240 --> 00:25:50,640
Nous n’en savons pas encore plus, mais nous pouvons l’estimer en fonction du

428
00:25:50,640 --> 00:25:52,920
degré d’incertitude qu’il y aura probablement une fois arrivé à ce point.

429
00:25:52,920 --> 00:25:56,600
Concrètement, pour le moment, il y en a 1. 44 bits d'incertitude.

430
00:25:56,600 --> 00:26:01,560
Si nous devinons des mots, cela nous indique que l'information attendue que nous obtiendrons est 1. 27 bits.

431
00:26:01,560 --> 00:26:06,280
Donc, si nous devinons les mots, cette différence représente le degré

432
00:26:06,280 --> 00:26:08,280
d’incertitude qui nous restera probablement après que cela se produise.

433
00:26:08,280 --> 00:26:12,500
Ce dont nous avons besoin, c'est d'une sorte de fonction, que

434
00:26:12,500 --> 00:26:13,880
j'appelle ici f, qui associe cette incertitude à un score attendu.

435
00:26:13,880 --> 00:26:18,040
Et la façon dont cela s'est déroulé consistait simplement à tracer un tas de

436
00:26:18,040 --> 00:26:23,920
données des jeux précédents basés sur la version 1 du bot pour dire quel

437
00:26:23,920 --> 00:26:27,040
était le score réel après différents points avec certaines quantités d'incertitude très mesurables.

438
00:26:27,040 --> 00:26:31,120
Par exemple, ces points de données ici se situent au-dessus d’une valeur d’environ 8. Environ

439
00:26:31,120 --> 00:26:36,840
7 disent pour certains matchs après un point où il y en avait 8. 7 bits d'incertitude,

440
00:26:36,840 --> 00:26:39,340
il a fallu deux suppositions pour obtenir la réponse finale.

441
00:26:39,340 --> 00:26:43,180
Pour les autres jeux, il fallait trois tentatives, pour les autres jeux, il fallait quatre tentatives.

442
00:26:43,180 --> 00:26:46,920
Si nous passons ici vers la gauche, tous les points au-dessus de zéro indiquent

443
00:26:46,920 --> 00:26:51,620
que lorsqu'il n'y a aucun élément d'incertitude, c'est-à-dire qu'il n'y a qu'une seule possibilité,

444
00:26:51,620 --> 00:26:55,000
alors le nombre de suppositions requis est toujours d'une seule, ce qui est rassurant.

445
00:26:55,000 --> 00:26:59,020
Chaque fois qu'il y avait un peu d'incertitude, ce qui

446
00:26:59,020 --> 00:27:02,360
signifiait qu'il ne s'agissait essentiellement que de deux possibilités, il

447
00:27:02,360 --> 00:27:03,940
fallait parfois une supposition supplémentaire, parfois deux suppositions supplémentaires.

448
00:27:03,940 --> 00:27:05,980
Et ainsi de suite, ici.

449
00:27:05,980 --> 00:27:11,020
Un moyen un peu plus simple de visualiser ces données consiste peut-être à les regrouper et à prendre des moyennes.

450
00:27:11,020 --> 00:27:15,940
Par exemple, cette barre indique que parmi tous les points pour lesquels nous avions

451
00:27:15,940 --> 00:27:22,420
un peu d'incertitude, le nombre moyen de nouvelles suppositions requises était d'environ 1. 5.

452
00:27:22,420 --> 00:27:25,920
Et la barre ici indique que parmi tous les différents jeux, où à

453
00:27:25,920 --> 00:27:30,480
un moment donné l'incertitude était un peu supérieure à quatre bits, ce qui

454
00:27:30,480 --> 00:27:35,120
revient à la réduire à 16 possibilités différentes, alors en moyenne, cela nécessite

455
00:27:35,120 --> 00:27:36,240
un peu plus de deux suppositions à partir de ce point. avant.

456
00:27:36,240 --> 00:27:40,080
Et à partir de là, j'ai juste fait une régression pour adapter une fonction qui semblait raisonnable à cela.

457
00:27:40,080 --> 00:27:44,160
Et rappelez-vous que l’intérêt de faire tout cela est de pouvoir quantifier cette intuition selon

458
00:27:44,160 --> 00:27:49,720
laquelle plus nous obtenons d’informations à partir d’un mot, plus le score attendu sera bas.

459
00:27:49,720 --> 00:27:54,380
Donc avec ceci comme version 2. 0, si nous revenons en arrière et exécutons le même ensemble de

460
00:27:54,380 --> 00:27:59,820
simulations, en le faisant jouer avec les 2315 réponses de mots possibles, comment cela se passe-t-il ?

461
00:27:59,820 --> 00:28:04,060
Et bien contrairement à notre première version, c'est nettement mieux, ce qui est rassurant.

462
00:28:04,060 --> 00:28:08,780
Tout compte fait, la moyenne est d’environ 3. 6, bien que contrairement à la première version,

463
00:28:08,780 --> 00:28:12,820
il perd plusieurs fois et en nécessite plus de six dans ce cas.

464
00:28:12,820 --> 00:28:15,980
Vraisemblablement parce qu'il y a des moments où il faut faire

465
00:28:15,980 --> 00:28:18,980
un compromis pour atteindre l'objectif plutôt que de maximiser l'information.

466
00:28:18,980 --> 00:28:22,140
Alors pouvons-nous faire mieux que 3. 6 ?

467
00:28:22,140 --> 00:28:23,260
Nous le pouvons certainement.

468
00:28:23,260 --> 00:28:27,120
Maintenant, j'ai dit au début qu'il était très amusant d'essayer de ne pas incorporer la

469
00:28:27,120 --> 00:28:29,980
vraie liste de réponses en mots dans la manière dont il construit son modèle.

470
00:28:29,980 --> 00:28:35,180
Mais si nous l’intégrons, la meilleure performance que j’ai pu obtenir était d’environ 3. 43.

471
00:28:35,180 --> 00:28:39,520
Donc, si nous essayons d'être plus sophistiqués que d'utiliser simplement les données de fréquence des mots pour choisir cette

472
00:28:39,520 --> 00:28:44,220
distribution a priori, cette 3. 43 donne probablement un maximum de la qualité que nous pourrions

473
00:28:44,220 --> 00:28:46,360
obtenir avec cela, ou du moins de la qualité que je pourrais obtenir avec cela.

474
00:28:46,360 --> 00:28:50,240
Cette meilleure performance utilise essentiellement les idées dont j'ai parlé ici, mais

475
00:28:50,240 --> 00:28:53,400
elle va un peu plus loin, comme si elle effectuait une

476
00:28:53,400 --> 00:28:55,660
recherche des informations attendues deux pas en avant plutôt qu'un seul.

477
00:28:55,660 --> 00:28:58,720
Au départ, j'avais prévu d'en parler davantage, mais je me

478
00:28:58,720 --> 00:29:00,580
rends compte que nous avons en fait été assez longs.

479
00:29:00,580 --> 00:29:03,520
La seule chose que je dirai, c'est qu'après avoir effectué cette recherche en

480
00:29:03,520 --> 00:29:07,720
deux étapes, puis exécuté quelques exemples de simulations sur les meilleurs candidats, jusqu'à

481
00:29:07,720 --> 00:29:09,500
présent, pour moi au moins, il semble que Crane soit le meilleur ouvreur.

482
00:29:09,500 --> 00:29:11,080
Qui l'aurait deviné ?

483
00:29:11,080 --> 00:29:15,680
De plus, si vous utilisez la vraie liste de mots pour déterminer votre espace de

484
00:29:15,680 --> 00:29:17,920
possibilités, alors l'incertitude avec laquelle vous commencez est d'un peu plus de 11 bits.

485
00:29:18,160 --> 00:29:22,760
Et il s'avère que, rien qu'à partir d'une recherche par force brute, le

486
00:29:22,760 --> 00:29:26,580
maximum d'informations attendues après les deux premières suppositions est d'environ 10 bits.

487
00:29:26,580 --> 00:29:31,720
Ce qui suggère que dans le meilleur des cas, après vos deux premières

488
00:29:31,720 --> 00:29:35,220
suppositions, avec un jeu parfaitement optimal, il vous restera environ un peu d'incertitude.

489
00:29:35,220 --> 00:29:37,400
Ce qui revient à se limiter à deux suppositions possibles.

490
00:29:37,400 --> 00:29:41,440
Je pense donc qu'il est juste et probablement assez conservateur de dire que vous ne pourrez

491
00:29:41,440 --> 00:29:45,620
jamais écrire un algorithme qui abaisse cette moyenne à 3, car avec les mots dont vous

492
00:29:45,620 --> 00:29:50,460
disposez, il n'y a tout simplement pas de place pour obtenir suffisamment d'informations après seulement deux

493
00:29:50,460 --> 00:29:53,820
étapes. capable de garantir la réponse dans le troisième créneau à chaque fois sans faute.

