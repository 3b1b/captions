1
00:00:00,000 --> 00:00:03,187
Wurdle oyunu son bir iki ayda oldukça viral hale geldi ve hiçbir zaman 

2
00:00:03,187 --> 00:00:05,791
bir matematik dersi fırsatını gözden kaçıran biri olmadı. 

3
00:00:05,791 --> 00:00:08,888
Bana öyle geliyor ki bu oyun bilgi teorisi ile ilgili bir derste çok 

4
00:00:08,888 --> 00:00:12,660
iyi bir merkezi örnek teşkil ediyor ve özellikle de entropi olarak bilinen bir konu.

5
00:00:13,920 --> 00:00:18,254
Görüyorsunuz, pek çok insan gibi ben de bulmacanın içine kapıldım ve birçok programcı 

6
00:00:18,254 --> 00:00:22,740
gibi ben de oyunu olabildiğince optimum şekilde oynatacak bir algoritma yazmaya kapıldım.

7
00:00:23,180 --> 00:00:25,813
Ve burada yapmayı düşündüğüm şey, sizinle bu konudaki sürecimin 

8
00:00:25,813 --> 00:00:28,981
bir kısmını konuşmak ve bunun içine giren matematiğin bir kısmını açıklamak, 

9
00:00:28,981 --> 00:00:31,080
çünkü tüm algoritma bu entropi fikrine odaklanıyor.

10
00:00:38,700 --> 00:00:41,640
İlk olarak, duymadıysanız söyleyeyim, Wurdle nedir?

11
00:00:42,040 --> 00:00:45,283
Ve burada oyunun kurallarını incelerken bir taşla iki kuş vurmak için, 

12
00:00:45,283 --> 00:00:47,750
bununla nereye gittiğimizi de ön izlememe izin verin, 

13
00:00:47,750 --> 00:00:51,040
yani temelde oyunu bizim için oynayacak küçük bir algoritma geliştirmek.

14
00:00:51,360 --> 00:00:55,100
Bugünkü Wurdle'ı yapmamış olsam da, bugün 4 Şubat ve botun nasıl yapacağını göreceğiz.

15
00:00:55,480 --> 00:00:57,870
Wurdle'ın amacı, beş harfli gizemli bir kelimeyi tahmin 

16
00:00:57,870 --> 00:01:00,340
etmektir ve size tahmin etmeniz için altı farklı şans verilir.

17
00:01:00,840 --> 00:01:04,379
Örneğin, Wurdle botum tahmin vinciyle başlamamı öneriyor.

18
00:01:05,180 --> 00:01:07,676
Her tahmin yaptığınızda, tahmininizin gerçek cevaba 

19
00:01:07,676 --> 00:01:10,220
ne kadar yakın olduğuna dair bazı bilgiler alırsınız.

20
00:01:10,920 --> 00:01:14,100
Burada gri kutu bana asıl cevapta C'nin olmadığını söylüyor.

21
00:01:14,520 --> 00:01:17,840
Sarı kutu bana bir R olduğunu söylüyor ama o konumda değil.

22
00:01:18,240 --> 00:01:20,326
Yeşil kutu bana gizli kelimenin A harfine sahip 

23
00:01:20,326 --> 00:01:22,240
olduğunu ve üçüncü sırada olduğunu söylüyor.

24
00:01:22,720 --> 00:01:24,580
Ve sonra N yok ve E yok.

25
00:01:25,200 --> 00:01:27,340
O halde içeri girip Wurdle botuna bu bilgiyi söyleyeyim.

26
00:01:27,340 --> 00:01:30,320
Turnayla başladık, gri, sarı, yeşil, gri, gri elde ettik.

27
00:01:31,420 --> 00:01:33,539
Şu anda gösterdiği tüm veriler hakkında endişelenmeyin, 

28
00:01:33,539 --> 00:01:34,940
bunları zamanı gelince açıklayacağım.

29
00:01:35,460 --> 00:01:38,820
Ancak ikinci seçimimiz için en önemli önerisi saçmalıktır.

30
00:01:39,560 --> 00:01:42,886
Ve tahmininizin gerçekten beş harfli bir kelime olması gerekiyor, ancak göreceğiniz gibi, 

31
00:01:42,886 --> 00:01:45,400
aslında tahmin etmenize izin vereceği şey konusunda oldukça liberal.

32
00:01:46,200 --> 00:01:47,440
Bu durumda, saçmalamayı deneriz.

33
00:01:48,780 --> 00:01:50,180
Ve tamam, işler oldukça iyi görünüyor.

34
00:01:50,260 --> 00:01:53,980
S ve H'ye basıyoruz, yani ilk üç harfi biliyoruz, bir R olduğunu biliyoruz.

35
00:01:53,980 --> 00:01:58,700
Ve böylece SHA bir R gibi olacak veya SHA R bir şey olacak.

36
00:01:59,620 --> 00:02:03,344
Görünüşe göre Wurdle botu bunun yalnızca iki olasılığa bağlı olduğunu biliyor; 

37
00:02:03,344 --> 00:02:04,240
kırık ya da keskin.

38
00:02:05,100 --> 00:02:07,499
Bu noktada aralarında bir tür çekişme var, bu yüzden 

39
00:02:07,499 --> 00:02:10,080
sanırım muhtemelen alfabetik olduğu için parçayla uyumlu.

40
00:02:11,220 --> 00:02:12,860
Hangi yaşasın, asıl cevap bu.

41
00:02:12,960 --> 00:02:13,780
Böylece üçe çıktık.

42
00:02:14,600 --> 00:02:17,500
Bunun iyi olup olmadığını merak ediyorsanız, bir kişiden duyduğuma göre 

43
00:02:17,500 --> 00:02:20,360
Wurdle'da dört eşit ve üç birdie'dir şeklinde bir cümle duydum.

44
00:02:20,680 --> 00:02:22,480
Oldukça uygun bir benzetme olduğunu düşünüyorum.

45
00:02:22,480 --> 00:02:25,477
Dört puan almak için oyununuzda tutarlı bir şekilde çalışmalısınız, 

46
00:02:25,477 --> 00:02:27,020
ancak bu kesinlikle çılgınca değil.

47
00:02:27,180 --> 00:02:29,920
Ama üçe böldüğünüzde harika hissettiriyor.

48
00:02:30,880 --> 00:02:33,437
Yani eğer istekliyseniz, burada yapmak istediğim şey Wurdle botuna nasıl 

49
00:02:33,437 --> 00:02:35,960
yaklaştığım konusunda en başından itibaren düşünce sürecimden bahsetmek.

50
00:02:36,480 --> 00:02:39,440
Ve dediğim gibi bu aslında bilgi teorisi dersi için bir bahane.

51
00:02:39,740 --> 00:02:42,820
Temel amaç bilgi nedir, entropinin ne olduğunu açıklamaktır.

52
00:02:48,220 --> 00:02:50,998
Bu konuya yaklaşırken ilk düşüncem İngilizcedeki 

53
00:02:50,998 --> 00:02:53,720
farklı harflerin göreceli sıklıklarına bakmaktı.

54
00:02:54,380 --> 00:02:56,729
Ben de düşündüm ki, tamam, bu en sık kullanılan harflerin çoğuna 

55
00:02:56,729 --> 00:02:59,260
karşılık gelen bir açılış tahmini veya bir açılış tahmin çifti var mı?

56
00:02:59,960 --> 00:03:03,000
Ve oldukça hoşuma giden bir şey de diğerini ve ardından tırnakları yapmaktı.

57
00:03:03,760 --> 00:03:05,492
Buradaki düşünce şu ki, eğer bir harfe rastlarsanız, 

58
00:03:05,492 --> 00:03:07,520
yeşil ya da sarı elde edersiniz, bu her zaman iyi hissettirir.

59
00:03:07,520 --> 00:03:08,840
Bilgi alıyormuşsunuz gibi geliyor.

60
00:03:09,340 --> 00:03:12,293
Ancak bu durumlarda, vurmasanız ve her zaman gri tonlar alsanız bile, 

61
00:03:12,293 --> 00:03:15,036
bu size yine de birçok bilgi verir, çünkü bu harflerden herhangi 

62
00:03:15,036 --> 00:03:17,400
birine sahip olmayan bir kelime bulmak oldukça nadirdir.

63
00:03:18,140 --> 00:03:20,693
Ancak yine de bu pek sistematik gelmiyor çünkü örneğin 

64
00:03:20,693 --> 00:03:23,200
harflerin sırasını dikkate almanın hiçbir faydası yok.

65
00:03:23,560 --> 00:03:25,300
Salyangoz yazabilecekken neden çivi yazayım ki?

66
00:03:26,080 --> 00:03:27,500
Sonunda S olması daha mı iyi?

67
00:03:27,820 --> 00:03:28,680
Gerçekten emin değilim.

68
00:03:29,240 --> 00:03:32,538
Bir arkadaşım yorgun sözcüğüyle başlamayı sevdiğini söyledi, 

69
00:03:32,538 --> 00:03:36,540
bu beni biraz şaşırttı çünkü içinde W ve Y gibi alışılmadık harfler vardı.

70
00:03:37,120 --> 00:03:39,000
Ama kim bilir, belki bu daha iyi bir açılıştır.

71
00:03:39,320 --> 00:03:41,770
Potansiyel bir tahminin kalitesini değerlendirmek 

72
00:03:41,770 --> 00:03:44,320
için verebileceğimiz bir tür niceliksel puan var mı?

73
00:03:45,340 --> 00:03:48,402
Şimdi olası tahminleri sıralama yöntemimizi belirlemek için geriye 

74
00:03:48,402 --> 00:03:51,420
dönüp oyunun tam olarak nasıl kurulduğuna biraz açıklık getirelim.

75
00:03:51,420 --> 00:03:54,496
Yani, geçerli tahminler olarak kabul edilen, girmenize izin 

76
00:03:54,496 --> 00:03:57,880
verecek yaklaşık 13.000 kelime uzunluğunda bir kelime listesi var.

77
00:03:58,320 --> 00:04:02,805
Ama baktığınızda pek çok sıra dışı şey var; kafa ya da Ali ve ARG gibi şeyler, 

78
00:04:02,805 --> 00:04:06,440
Scrabble oyununda aile tartışmalarına yol açan türden kelimeler.

79
00:04:06,960 --> 00:04:10,540
Ancak oyunun havası, cevabın her zaman oldukça yaygın bir kelime olacağıdır.

80
00:04:10,960 --> 00:04:15,360
Ve aslında olası yanıtlar olan yaklaşık 2300 kelimeden oluşan başka bir liste daha var.

81
00:04:15,940 --> 00:04:18,509
Ve bu, insanların hazırladığı bir liste, sanırım özellikle oyun 

82
00:04:18,509 --> 00:04:21,160
yaratıcısının kız arkadaşı tarafından, ki bu da oldukça eğlenceli.

83
00:04:21,820 --> 00:04:24,518
Ancak yapmak istediğim şey, bu projedeki amacımız, 

84
00:04:24,518 --> 00:04:27,269
bu listeyle ilgili önceki bilgileri birleştirmeyen, 

85
00:04:27,269 --> 00:04:30,180
Wordle çözen bir program yazıp yazamayacağımızı görmek.

86
00:04:30,720 --> 00:04:34,640
Öncelikle, bu listede bulamayacağınız pek çok yaygın beş harfli kelime var.

87
00:04:34,940 --> 00:04:38,222
Bu nedenle, biraz daha dayanıklı olan ve sadece resmi web sitesine değil, 

88
00:04:38,222 --> 00:04:41,460
herkese karşı Wordle oynayabilecek bir program yazmak daha iyi olacaktır.

89
00:04:41,920 --> 00:04:45,130
Ayrıca bu olası yanıtlar listesinin ne olduğunu bilmemizin nedeni, 

90
00:04:45,130 --> 00:04:47,000
bunun kaynak kodunda görünür olmasıdır.

91
00:04:47,000 --> 00:04:53,260
Ancak kaynak kodunda görünme şekli, cevapların günden güne ortaya çıkma sırasına göredir.

92
00:04:53,260 --> 00:04:55,840
Böylece her zaman yarının cevabının ne olacağına bakabilirsiniz.

93
00:04:56,420 --> 00:04:58,880
Açıkça görülüyor ki, listeyi kullanmanın hile yapmak olduğu bir anlam taşıyor.

94
00:04:59,100 --> 00:05:03,518
Ve daha ilginç bir bulmacayı ve daha zengin bir bilgi teorisi dersini ortaya çıkaran şey, 

95
00:05:03,518 --> 00:05:07,101
daha yaygın sözcükleri tercih etme sezgisini yakalamak için genel olarak 

96
00:05:07,101 --> 00:05:10,440
göreceli sözcük sıklıkları gibi daha evrensel verileri kullanmaktır.

97
00:05:11,600 --> 00:05:15,900
Peki bu 13.000 olasılık arasından açılış tahminini nasıl seçmeliyiz?

98
00:05:16,400 --> 00:05:19,780
Mesela arkadaşım bıkkınlık teklif ediyorsa kalitesini nasıl analiz etmeliyiz?

99
00:05:20,520 --> 00:05:23,325
Pek olası olmayan W'yi sevdiğini söylemesinin nedeni, 

100
00:05:23,325 --> 00:05:27,340
o W'ye vurmanın ne kadar iyi hissettireceğinin uzun vadede doğasını sevmesidir.

101
00:05:27,920 --> 00:05:31,110
Örneğin, ortaya çıkan ilk kalıp bunun gibi bir şeyse, 

102
00:05:31,110 --> 00:05:35,600
bu dev sözlükte bu kalıpla eşleşen yalnızca 58 kelime olduğu ortaya çıkıyor.

103
00:05:36,060 --> 00:05:38,400
Yani bu 13.000'den çok büyük bir azalma.

104
00:05:38,780 --> 00:05:43,020
Ancak bunun diğer tarafı da elbette ki böyle bir model elde etmenin çok nadir olmasıdır.

105
00:05:43,020 --> 00:05:47,622
Spesifik olarak, her kelimenin cevap olma olasılığı eşit olsaydı, 

106
00:05:47,622 --> 00:05:51,040
bu kalıba ulaşma olasılığı 58 bölü 13.000 olurdu.

107
00:05:51,580 --> 00:05:53,600
Elbette bunların cevap olma ihtimali eşit değil.

108
00:05:53,720 --> 00:05:56,220
Bunların çoğu çok belirsiz ve hatta şüpheli kelimelerdir.

109
00:05:56,600 --> 00:05:58,938
Ama en azından tüm bunlara ilk geçişimizde, hepsinin eşit 

110
00:05:58,938 --> 00:06:01,600
derecede olası olduğunu varsayalım ve bunu biraz sonra düzeltelim.

111
00:06:02,020 --> 00:06:04,394
Mesele şu ki, çok fazla bilgi içeren bir modelin 

112
00:06:04,394 --> 00:06:06,720
doğası gereği ortaya çıkması pek olası değildir.

113
00:06:07,280 --> 00:06:10,800
Aslında bilgilendirici olmanın anlamı bunun olası olmadığıdır.

114
00:06:11,719 --> 00:06:16,308
Bu açılışta görülecek çok daha olası bir model bunun gibi bir şey olabilir, 

115
00:06:16,308 --> 00:06:18,120
burada elbette W harfi yoktur.

116
00:06:18,240 --> 00:06:21,400
Belki bir E vardır ve belki A yoktur, R yoktur, Y yoktur.

117
00:06:22,080 --> 00:06:24,560
Bu durumda 1400 olası eşleşme vardır.

118
00:06:25,080 --> 00:06:30,600
Hepsi eşit olasılıkta olsaydı, göreceğiniz modelin bu olma olasılığı yaklaşık %11 olurdu.

119
00:06:30,900 --> 00:06:33,340
Dolayısıyla en olası sonuçlar aynı zamanda en az bilgilendirici olanlardır.

120
00:06:34,240 --> 00:06:36,660
Burada daha küresel bir bakış elde etmek için, 

121
00:06:36,660 --> 00:06:41,140
görebileceğiniz tüm farklı kalıplara göre olasılıkların tam dağılımını size göstereyim.

122
00:06:41,740 --> 00:06:46,563
Yani baktığınız her çubuk, ortaya çıkabilecek olası bir renk düzenine karşılık gelir, 

123
00:06:46,563 --> 00:06:49,872
bunlardan 3 üzeri 5 olasılık vardır ve bunlar soldan sağa, 

124
00:06:49,872 --> 00:06:52,340
en yaygından en az yaygına doğru düzenlenir.

125
00:06:52,920 --> 00:06:56,000
Yani buradaki en yaygın olasılık, tüm grileri elde etmenizdir.

126
00:06:56,100 --> 00:06:58,120
Bu, zamanın yaklaşık %14'ünde gerçekleşir.

127
00:06:58,580 --> 00:07:01,897
Ve bir tahmin yaptığınızda umduğunuz şey, kendinizi bu uzun 

128
00:07:01,897 --> 00:07:04,772
kuyrukta bir yerde bulmanızdır; burada olduğu gibi, 

129
00:07:04,772 --> 00:07:09,140
açıkça buna benzeyen bu kalıba uyan şey için yalnızca 18 olasılığın olduğu yer.

130
00:07:09,920 --> 00:07:13,800
Ya da biraz daha sola gidersek belki buraya kadar gidebiliriz.

131
00:07:14,940 --> 00:07:16,180
Tamam, işte sana güzel bir bulmaca.

132
00:07:16,540 --> 00:07:19,270
İngilizce dilinde W ile başlayan, Y ile biten 

133
00:07:19,270 --> 00:07:22,000
ve içinde bir yerde R bulunan üç kelime nedir?

134
00:07:22,480 --> 00:07:26,800
Cevapların, bakalım, uzun uzun, kurtlu ve alaycı olduğu ortaya çıktı.

135
00:07:27,500 --> 00:07:31,307
Bu kelimenin genel olarak ne kadar iyi olduğuna karar vermek için, 

136
00:07:31,307 --> 00:07:35,740
bu dağılımdan alacağınız beklenen bilgi miktarının bir tür ölçüsünü istiyoruz.

137
00:07:35,740 --> 00:07:40,342
Her bir modeli incelersek ve onun gerçekleşme olasılığını ne kadar bilgilendirici 

138
00:07:40,342 --> 00:07:44,720
olduğunu ölçen bir şeyle çarparsak, bu bize belki objektif bir puan verebilir.

139
00:07:45,960 --> 00:07:49,840
Şimdi bir şeyin ne olması gerektiğine dair ilk içgüdünüz eşleşme sayısı olabilir.

140
00:07:50,160 --> 00:07:52,400
Daha düşük bir ortalama eşleşme sayısı istiyorsunuz.

141
00:07:52,800 --> 00:07:56,590
Ancak bunun yerine, genellikle bilgiye atfettiğimiz daha evrensel bir ölçüm kullanmak 

142
00:07:56,590 --> 00:08:00,381
istiyorum ve bu 13.000 kelimenin her birine, bunların gerçekten cevap olup olmadığına 

143
00:08:00,381 --> 00:08:04,260
ilişkin farklı bir olasılık atandığında daha esnek olacak bir ölçüm kullanmak istiyorum.

144
00:08:10,320 --> 00:08:14,102
Standart bilgi birimi, biraz komik bir formüle sahip olan bit'tir, 

145
00:08:14,102 --> 00:08:16,980
ancak sadece örneklere bakarsak gerçekten sezgiseldir.

146
00:08:17,780 --> 00:08:21,261
Olasılık alanınızı yarıya indiren bir gözleminiz varsa, 

147
00:08:21,261 --> 00:08:23,500
onun bir bit bilgisi vardır diyoruz.

148
00:08:24,180 --> 00:08:27,560
Örneğimizde, olasılıklar uzayı tüm olası kelimelerden oluşuyor ve ortaya çıkıyor ki, 

149
00:08:27,560 --> 00:08:29,748
beş harfli kelimelerin yaklaşık yarısının S harfi var, 

150
00:08:29,748 --> 00:08:31,260
bundan biraz daha az ama yarısı kadar.

151
00:08:31,780 --> 00:08:34,320
Yani bu gözlem size biraz bilgi verecektir.

152
00:08:34,880 --> 00:08:38,807
Bunun yerine yeni bir olgu bu olasılıklar alanını dört kat azaltırsa, 

153
00:08:38,807 --> 00:08:41,500
onun iki bitlik bilgiye sahip olduğunu söyleriz.

154
00:08:41,980 --> 00:08:44,460
Örneğin, bu kelimelerin yaklaşık dörtte birinde T harfinin olduğu ortaya çıktı.

155
00:08:45,020 --> 00:08:47,788
Eğer gözlem bu alanı sekiz kat azaltırsa, bunun üç 

156
00:08:47,788 --> 00:08:50,720
bitlik bilgi olduğunu söyleriz ve bu böyle devam eder.

157
00:08:50,900 --> 00:08:55,060
Dört bit onu 16'ya, beş bit ise 32'ye böler.

158
00:08:55,060 --> 00:08:57,932
Şimdi durup kendinize şu soruyu sorabilirsiniz: 

159
00:08:57,932 --> 00:09:02,660
Bir olayın gerçekleşme olasılığı açısından bit sayısı bilgisinin formülü nedir?

160
00:09:02,660 --> 00:09:08,245
Burada söylediğimiz şey, bit sayısının yarısını aldığınızda, bu olasılık ile aynı şeydir; 

161
00:09:08,245 --> 00:09:13,272
bu, bit sayısının iki üssünün bir bölü olasılık olduğunu söylemekle aynı şeydir; 

162
00:09:13,272 --> 00:09:18,547
Bilginin log tabanının iki bölü olasılığa eşit olduğunu söyleyerek yeniden düzenleme 

163
00:09:18,547 --> 00:09:18,920
yapar.

164
00:09:19,620 --> 00:09:22,283
Ve bazen bunu bir yeniden düzenlemeyle daha görürsünüz; 

165
00:09:22,283 --> 00:09:24,900
burada bilgi, olasılığın negatif logaritması tabanıdır.

166
00:09:25,660 --> 00:09:29,511
Bu şekilde ifade edilirse, bu konuya yeni başlayan biri için biraz tuhaf görünebilir, 

167
00:09:29,511 --> 00:09:32,378
ancak aslında bu, olasılıklarınızı kaç kez yarıya indirdiğinizi 

168
00:09:32,378 --> 00:09:34,080
sormak gibi çok sezgisel bir fikirdir.

169
00:09:35,180 --> 00:09:38,012
Şimdi merak ediyorsanız, eğlenceli bir kelime oyunu oynadığımızı sanıyordum, 

170
00:09:38,012 --> 00:09:39,300
neden logaritmalar devreye giriyor?

171
00:09:39,780 --> 00:09:42,213
Bunun daha güzel bir birim olmasının bir nedeni, 

172
00:09:42,213 --> 00:09:45,788
pek olası olmayan olaylar hakkında konuşmanın çok daha kolay olmasıdır; 

173
00:09:45,788 --> 00:09:48,569
bir gözlemin 20 bitlik bilgiye sahip olduğunu söylemek, 

174
00:09:48,569 --> 00:09:52,940
şunun şunun meydana gelme olasılığının 0 olduğunu söylemekten çok daha kolaydır.0000095.

175
00:09:53,300 --> 00:09:57,431
Ancak bu logaritmik ifadenin olasılık teorisine çok yararlı bir katkı olduğunun 

176
00:09:57,431 --> 00:10:01,460
ortaya çıkmasının daha önemli bir nedeni, bilgilerin bir araya gelme şeklidir.

177
00:10:02,060 --> 00:10:04,743
Örneğin, bir gözlem size iki bitlik bilgi verirse, 

178
00:10:04,743 --> 00:10:08,268
alanınızı dört katına çıkarırsa ve ardından Wordle'deki ikinci 

179
00:10:08,268 --> 00:10:11,688
tahmininiz gibi ikinci bir gözlem size başka bir üç bitlik bilgi 

180
00:10:11,688 --> 00:10:14,477
verirse ve sizi başka bir sekiz kat daha küçültürse, 

181
00:10:14,477 --> 00:10:16,740
ikisi birlikte size beş bitlik bilgi verir.

182
00:10:17,160 --> 00:10:21,020
Olasılıklar çoğalmayı sevdiği gibi, bilgi de eklemeyi sever.

183
00:10:21,960 --> 00:10:24,811
Dolayısıyla, bir dizi sayıyı topladığımız beklenen değer gibi bir şeyin 

184
00:10:24,811 --> 00:10:27,980
alanına girdiğimizde, günlükler bununla uğraşmayı çok daha güzel hale getiriyor.

185
00:10:28,480 --> 00:10:31,685
Weary dağıtımımıza geri dönelim ve buraya her model için ne kadar 

186
00:10:31,685 --> 00:10:34,940
bilgi bulunduğunu bize gösteren başka bir küçük izleyici ekleyelim.

187
00:10:35,580 --> 00:10:40,047
Fark etmenizi istediğim asıl şey, daha olası modellere ulaşma olasılığımız arttıkça, 

188
00:10:40,047 --> 00:10:42,780
bilgi ne kadar düşükse, o kadar az bit kazanırsınız.

189
00:10:43,500 --> 00:10:47,675
Bu tahminin kalitesini ölçmemizin yolu, bu bilginin beklenen değerini almak olacaktır; 

190
00:10:47,675 --> 00:10:50,939
burada her bir modeli inceliyoruz, bunun ne kadar muhtemel olduğunu 

191
00:10:50,939 --> 00:10:54,060
söylüyoruz ve sonra bunu kaç bitlik bilgi aldığımızla çarpıyoruz.

192
00:10:54,710 --> 00:10:58,120
Ve Weary örneğinde bunun 4 olduğu ortaya çıkıyor.9 bit.

193
00:10:58,560 --> 00:11:01,885
Yani ortalama olarak, bu açılış tahmininden alacağınız bilgi, 

194
00:11:01,885 --> 00:11:05,480
olasılıklar alanınızı yaklaşık beş kez yarıya indirmeye eşdeğerdir.

195
00:11:05,960 --> 00:11:08,743
Bunun tersine, beklenen bilgi değeri daha yüksek 

196
00:11:08,743 --> 00:11:11,640
olan bir tahmin örneği Slate gibi bir şey olabilir.

197
00:11:13,120 --> 00:11:15,620
Bu durumda dağılımın çok daha düz göründüğünü fark edeceksiniz.

198
00:11:15,940 --> 00:11:20,447
Özellikle, tüm grilerin en muhtemel oluşumu yalnızca %6 civarında meydana 

199
00:11:20,447 --> 00:11:25,260
gelme şansına sahiptir, yani en azından açıkça 3 elde edersiniz.9 bitlik bilgi.

200
00:11:25,920 --> 00:11:28,560
Ancak bu minimumdur, daha genel olarak bundan daha iyi bir şey elde edersiniz.

201
00:11:29,100 --> 00:11:32,109
Ve buradaki rakamları hesaplayıp ilgili tüm terimleri 

202
00:11:32,109 --> 00:11:35,900
topladığınızda ortalama bilginin yaklaşık 5 olduğu ortaya çıkıyor.8.

203
00:11:37,360 --> 00:11:40,290
Yani Weary'nin aksine, olasılıklar alanınız bu ilk 

204
00:11:40,290 --> 00:11:43,540
tahminden sonra ortalama olarak yarısı kadar büyük olacaktır.

205
00:11:44,420 --> 00:11:49,120
Bilgi miktarının bu beklenen değerinin adı hakkında aslında eğlenceli bir hikaye var.

206
00:11:49,200 --> 00:11:52,924
Bilgi teorisi, 1940'larda Bell Laboratuarlarında çalışan Claude Shannon 

207
00:11:52,924 --> 00:11:56,404
tarafından geliştirildi, ancak henüz yayınlanmamış bazı fikirlerinden, 

208
00:11:56,404 --> 00:12:00,178
zamanın entelektüel devi, çok öne çıkan John von Neumann'la konuşuyordu. 

209
00:12:00,178 --> 00:12:03,560
matematik ve fizikte ve bilgisayar bilimine dönüşen şeyin başlangıcı.

210
00:12:04,100 --> 00:12:07,337
Ve von Neumann, bilgi miktarının bu beklenen değeri için gerçekten 

211
00:12:07,337 --> 00:12:10,382
iyi bir isme sahip olmadığını söylediğinde, söylendiğine göre, 

212
00:12:10,382 --> 00:12:14,200
hikaye şöyle devam ediyor, buna entropi diyebilirsiniz ve bunun iki nedeni var.

213
00:12:14,540 --> 00:12:18,581
İlk olarak, belirsizlik fonksiyonunuz istatistiksel mekanikte bu isimle kullanıldı, 

214
00:12:18,581 --> 00:12:21,852
dolayısıyla zaten bir adı var ve ikinci olarak ve daha da önemlisi, 

215
00:12:21,852 --> 00:12:24,354
hiç kimse entropinin gerçekte ne olduğunu bilmiyor, 

216
00:12:24,354 --> 00:12:26,760
dolayısıyla bir tartışmada her zaman avantajı var.

217
00:12:27,700 --> 00:12:31,036
Yani eğer isim biraz gizemli görünüyorsa ve eğer bu hikayeye inanılacaksa, 

218
00:12:31,036 --> 00:12:32,460
bu bir bakıma tasarım gereğidir.

219
00:12:33,280 --> 00:12:37,367
Ayrıca, eğer bunun fizikteki termodinamiğin ikinci yasasıyla ilgili tüm o şeylerle 

220
00:12:37,367 --> 00:12:40,272
ilişkisini merak ediyorsanız, kesinlikle bir bağlantı var, 

221
00:12:40,272 --> 00:12:44,163
ama kökeninde Shannon sadece saf olasılık teorisiyle ilgileniyordu ve buradaki 

222
00:12:44,163 --> 00:12:48,053
amaçlarımız için, Kelime entropisi, sadece belirli bir tahminin beklenen bilgi 

223
00:12:48,053 --> 00:12:49,580
değerini düşünmenizi istiyorum.

224
00:12:50,700 --> 00:12:53,780
Entropiyi iki şeyin aynı anda ölçülmesi olarak düşünebilirsiniz.

225
00:12:54,240 --> 00:12:56,780
Bunlardan ilki dağılımın ne kadar düz olduğudur.

226
00:12:57,320 --> 00:13:01,120
Dağılım düzgünlüğe ne kadar yakınsa entropi o kadar yüksek olur.

227
00:13:01,580 --> 00:13:05,469
Bizim durumumuzda, 3 üzeri 5'lik toplam örüntülerin olduğu durumda, 

228
00:13:05,469 --> 00:13:09,088
düzgün bir dağılım için, bunlardan herhangi birinin gözlemlenmesi, 

229
00:13:09,088 --> 00:13:12,600
3 üzeri 5'lik bilgi günlüğü tabanı 2'ye sahip olacaktır; 

230
00:13:12,600 --> 00:13:17,300
bu da 7 olur.92, yani bu entropi için sahip olabileceğiniz mutlak maksimum değer budur.

231
00:13:17,840 --> 00:13:22,080
Ancak entropi aynı zamanda ilk etapta ne kadar olasılığın bulunduğunun da bir ölçüsüdür.

232
00:13:22,320 --> 00:13:27,057
Örneğin, yalnızca 16 olası örüntünün olduğu ve her birinin eşit olasılığa 

233
00:13:27,057 --> 00:13:32,180
sahip olduğu bir kelimeniz varsa, bu entropi, bu beklenen bilgi 4 bit olacaktır.

234
00:13:32,579 --> 00:13:36,730
Ancak 64 olası örüntünün ortaya çıkabileceği başka bir kelimeniz varsa ve bunların 

235
00:13:36,730 --> 00:13:40,480
hepsi eşit derecede olasıysa, o zaman entropi 6 bit olarak hesaplanacaktır.

236
00:13:41,500 --> 00:13:45,738
Yani, eğer doğada 6 bitlik bir entropiye sahip bir dağılım görürseniz, 

237
00:13:45,738 --> 00:13:48,544
bu sanki 64 eşit olasılıklı sonuç varmış gibi, 

238
00:13:48,544 --> 00:13:53,500
olacaklar konusunda çok fazla değişkenlik ve belirsizlik olduğunu söylemek gibidir.

239
00:13:54,360 --> 00:13:59,320
Wurtelebot'a ilk geçişimde temelde bunu yapmasını sağladım.

240
00:13:59,320 --> 00:14:03,773
Yapabileceğiniz tüm olası tahminleri, yani 13.000 kelimenin tamamını gözden geçirir, 

241
00:14:03,773 --> 00:14:06,446
her biri için entropiyi veya daha spesifik olarak, 

242
00:14:06,446 --> 00:14:10,428
her biri için görebileceğiniz tüm kalıplar arasındaki dağılımın entropisini 

243
00:14:10,428 --> 00:14:14,463
hesaplar ve en yüksek olanı seçer, çünkü bu Olasılık alanınızı mümkün olduğu 

244
00:14:14,463 --> 00:14:16,140
kadar daraltması muhtemel olanı.

245
00:14:17,140 --> 00:14:19,099
Burada sadece ilk tahminden bahsetmiş olsam da 

246
00:14:19,099 --> 00:14:21,100
sonraki birkaç tahmin için de aynı şeyi yapıyor.

247
00:14:21,560 --> 00:14:24,903
Örneğin, ilk tahminde, hangi kelimeyle eşleştiğine bağlı olarak 

248
00:14:24,903 --> 00:14:28,874
sizi daha az sayıda olası kelimeyle sınırlayacak bir model gördükten sonra, 

249
00:14:28,874 --> 00:14:31,800
aynı oyunu o daha küçük kelime kümesine göre oynarsınız.

250
00:14:32,260 --> 00:14:36,081
Önerilen ikinci bir tahmin için, bu daha kısıtlı kelime grubundan 

251
00:14:36,081 --> 00:14:39,034
oluşabilecek tüm kalıpların dağılımına bakarsınız, 

252
00:14:39,034 --> 00:14:43,840
13.000 olasılığın tamamını ararsınız ve bu entropiyi maksimuma çıkaranı bulursunuz.

253
00:14:45,420 --> 00:14:49,958
Bunun nasıl çalıştığını size göstermek için, kenarlarda bu analizin önemli noktalarını 

254
00:14:49,958 --> 00:14:54,080
gösteren, yazdığım Wurtele'nin küçük bir versiyonunu ele almama izin verin.

255
00:14:54,080 --> 00:14:56,760
Tüm entropi hesaplamalarını yaptıktan sonra sağ tarafta bize 

256
00:14:56,760 --> 00:14:59,660
hangilerinin en yüksek beklenen bilgiye sahip olduğunu gösteriyor.

257
00:15:00,280 --> 00:15:06,051
En azından şimdilik en önemli cevabın Tares olduğu ortaya çıktı, 

258
00:15:06,051 --> 00:15:10,580
bu da tabii ki fiğ, en yaygın fiğ anlamına geliyor.

259
00:15:11,040 --> 00:15:14,395
Burada her tahmin yaptığımızda, belki de önerilerini göz ardı edip slate'i 

260
00:15:14,395 --> 00:15:17,538
tercih ederim, çünkü slate'i severim, ne kadar beklenen bilgiye sahip 

261
00:15:17,538 --> 00:15:21,021
olduğunu görebiliriz, ancak burada kelimenin sağında bize ne kadar bilgi olduğunu 

262
00:15:21,021 --> 00:15:24,420
gösteriyor. Bu özel model göz önüne alındığında, elde ettiğimiz gerçek bilgiler.

263
00:15:25,000 --> 00:15:28,063
Yani burada biraz şanssızız gibi görünüyor, 5 almamız bekleniyordu.8, 

264
00:15:28,063 --> 00:15:30,120
ama bundan daha azına sahip bir şey elde ettik.

265
00:15:30,600 --> 00:15:32,741
Ve sol tarafta, şu anda bulunduğumuz yere göre 

266
00:15:32,741 --> 00:15:35,020
bize mümkün olan tüm farklı kelimeleri gösteriyor.

267
00:15:35,800 --> 00:15:38,892
Mavi çubuklar bize her kelimenin ne kadar olası olduğunu düşündüğünü gösteriyor; 

268
00:15:38,892 --> 00:15:42,099
dolayısıyla şu anda her kelimenin eşit derecede gerçekleşme olasılığını varsayıyor, 

269
00:15:42,099 --> 00:15:43,360
ancak bunu birazdan düzelteceğiz.

270
00:15:44,060 --> 00:15:48,084
Ve bu belirsizlik ölçümü bize bu dağılımın olası kelimeler arasındaki 

271
00:15:48,084 --> 00:15:51,878
entropisini anlatıyor; bu şu anda tekdüze bir dağılım olduğu için 

272
00:15:51,878 --> 00:15:55,960
olasılıkların sayısını saymanın gereksiz derecede karmaşık bir yoludur.

273
00:15:56,560 --> 00:16:02,180
Örneğin 2 üssü 13'ü alırsak.66, bu 13.000 olasılık civarında olmalı.

274
00:16:02,900 --> 00:16:06,140
Burada biraz yanlışım var ama bunun nedeni ondalık basamakların tamamını göstermemem.

275
00:16:06,720 --> 00:16:09,451
Şu anda bu size gereksiz gelebilir ve işleri aşırı derecede karmaşık hale getirebilir, 

276
00:16:09,451 --> 00:16:11,931
ancak bir dakika içinde her iki sayıya da sahip olmanın neden yararlı olduğunu 

277
00:16:11,931 --> 00:16:12,340
göreceksiniz.

278
00:16:12,760 --> 00:16:16,201
Yani burada ikinci tahminimiz için en yüksek entropinin Ramen olduğunu 

279
00:16:16,201 --> 00:16:19,400
öne sürüyor gibi görünüyor ki bu da yine tek kelime gibi gelmiyor.

280
00:16:19,980 --> 00:16:24,060
Burada ahlaki açıdan yüksek bir yer edinmek için, devam edip Rains yazacağım.

281
00:16:25,440 --> 00:16:27,340
Ve yine biraz şanssızmışız gibi görünüyor.

282
00:16:27,520 --> 00:16:31,360
Biz 4 bekliyorduk.3 bit ve elimizde sadece 3 var.39 bit bilgi.

283
00:16:31,940 --> 00:16:33,940
Bu da bizi 55 olasılığa indiriyor.

284
00:16:34,900 --> 00:16:36,778
Ve burada belki de aslında onun önerdiği şeyle, 

285
00:16:36,778 --> 00:16:39,440
yani birleşik olanla, her ne anlama geliyorsa onunla devam edeceğim.

286
00:16:40,040 --> 00:16:42,920
Ve tamam, bu aslında bir bulmaca için iyi bir şans.

287
00:16:42,920 --> 00:16:46,380
Bu modelin bize 4 verdiğini söylüyor.7 bitlik bilgi.

288
00:16:47,060 --> 00:16:51,720
Ama sol tarafta, bu modeli görmeden önce 5 tane vardı.78 bitlik belirsizlik.

289
00:16:52,420 --> 00:16:56,340
Peki sizin için bir test olarak, kalan olasılıkların sayısı ne anlama geliyor?

290
00:16:58,040 --> 00:17:01,803
Bu, bir miktar belirsizliğe indirgendiğimiz anlamına gelir ki bu, 

291
00:17:01,803 --> 00:17:04,540
iki olası yanıt olduğunu söylemekle aynı şeydir.

292
00:17:04,700 --> 00:17:05,700
Bu 50-50'lik bir seçim.

293
00:17:06,500 --> 00:17:08,524
Ve buradan yola çıkarak, sen ve ben hangi kelimelerin daha yaygın 

294
00:17:08,524 --> 00:17:10,640
olduğunu bildiğimiz için cevabın uçurum olması gerektiğini biliyoruz.

295
00:17:11,180 --> 00:17:13,280
Ancak şu anda yazıldığı gibi, program bunu bilmiyor.

296
00:17:13,540 --> 00:17:16,755
Böylece tek bir olasılık kalana kadar mümkün olduğu kadar 

297
00:17:16,755 --> 00:17:19,859
çok bilgi toplamaya devam eder ve sonra onu tahmin eder.

298
00:17:20,380 --> 00:17:22,339
Açıkçası daha iyi bir oyunsonu stratejisine ihtiyacımız var.

299
00:17:22,599 --> 00:17:25,410
Ancak diyelim ki bu sürüme kelime çözücülerimizden biri adını verdik ve 

300
00:17:25,410 --> 00:17:28,260
sonra gidip nasıl çalıştığını görmek için bazı simülasyonlar çalıştırdık.

301
00:17:30,360 --> 00:17:34,120
Yani bunun çalışma şekli mümkün olan her kelime oyununu oynamaktır.

302
00:17:34,240 --> 00:17:38,540
Gerçek wordle cevapları olan 2315 kelimenin tamamının üzerinden geçiyor.

303
00:17:38,540 --> 00:17:40,580
Temel olarak bunu bir test seti olarak kullanıyor.

304
00:17:41,360 --> 00:17:44,105
Ve bir kelimenin ne kadar yaygın olduğunu dikkate almamak ve 

305
00:17:44,105 --> 00:17:46,940
tek ve tek bir seçeneğe varıncaya kadar yol boyunca her adımda 

306
00:17:46,940 --> 00:17:49,820
bilgiyi en üst düzeye çıkarmaya çalışmak gibi naif bir yöntemle.

307
00:17:50,360 --> 00:17:54,300
Simülasyonun sonunda ortalama puan 4 civarında çıkıyor.124.

308
00:17:55,319 --> 00:17:59,240
Ki bu fena değil, dürüst olmak gerekirse, daha kötüsünü bekliyordum.

309
00:17:59,660 --> 00:18:02,600
Ancak wordle oynayanlar size genellikle 4'te alabileceklerini söyleyecektir.

310
00:18:02,860 --> 00:18:05,380
Asıl zorluk 3'te mümkün olduğunca çok sayıda elde etmektir.

311
00:18:05,380 --> 00:18:08,080
4'lük skor ile 3'lük skor arasında oldukça büyük bir sıçrama var.

312
00:18:08,860 --> 00:18:11,920
Buradaki bariz düşük sonuç, bir kelimenin yaygın olup olmadığını 

313
00:18:11,920 --> 00:18:14,980
ve bunu tam olarak nasıl yapacağımızı bir şekilde dahil etmektir.

314
00:18:22,800 --> 00:18:25,519
Benim yaklaşımım İngilizce dilindeki tüm kelimelerin 

315
00:18:25,519 --> 00:18:27,880
göreceli frekanslarının bir listesini almaktı.

316
00:18:28,220 --> 00:18:31,588
Ve az önce Mathematica'nın Google Kitaplar İngilizce Ngram genel 

317
00:18:31,588 --> 00:18:34,860
veri kümesinden alınan kelime frekansı veri fonksiyonunu kullandım.

318
00:18:35,460 --> 00:18:37,587
Ve buna bakmak oldukça eğlenceli, örneğin en yaygın 

319
00:18:37,587 --> 00:18:39,960
sözcüklerden en az kullanılan sözcüklere doğru sıralarsak.

320
00:18:40,120 --> 00:18:43,080
Açıkçası bunlar İngilizce dilinde en yaygın 5 harfli kelimelerdir.

321
00:18:43,700 --> 00:18:45,840
Daha doğrusu bunlar en yaygın 8'incisidir.

322
00:18:46,280 --> 00:18:48,880
İlki hangisi, sonrasında orası ve orası var.

323
00:18:49,260 --> 00:18:52,203
Birincinin kendisi birinci değil, 9'uncudur ve bu diğer 

324
00:18:52,203 --> 00:18:55,293
kelimelerin daha sık ortaya çıkabileceği, ilk gelenlerin sonra 

325
00:18:55,293 --> 00:18:58,580
olduğu ve nerede olduğu ve biraz daha az yaygın olduğu mantıklıdır.

326
00:18:59,160 --> 00:19:03,195
Şimdi, bu verileri, bu kelimelerin her birinin nihai cevap olma olasılığını 

327
00:19:03,195 --> 00:19:06,860
modellemek için kullanırken, bu sadece sıklıkla orantılı olmamalıdır.

328
00:19:06,860 --> 00:19:10,471
Örneğin 0 puan verilir.Bu veri setinde 002 var, 

329
00:19:10,471 --> 00:19:15,060
oysa örgü kelimesinin olasılığı bir anlamda 1000 kat daha az.

330
00:19:15,560 --> 00:19:17,107
Ancak bunların her ikisi de, neredeyse kesinlikle 

331
00:19:17,107 --> 00:19:18,840
dikkate alınmaya değer olacak kadar yaygın kelimelerdir.

332
00:19:19,340 --> 00:19:21,000
Bu yüzden daha fazla ikili kesinti istiyoruz.

333
00:19:21,860 --> 00:19:25,329
Bu konuda izlediğim yol, tüm bu sıralanmış kelime listesini alıp, 

334
00:19:25,329 --> 00:19:29,429
bunu bir x eksenine göre düzenlediğimi ve ardından çıktısı temelde ikili olan 

335
00:19:29,429 --> 00:19:33,739
bir fonksiyona sahip olmanın standart yolu olan sigmoid fonksiyonunu uyguladığımı 

336
00:19:33,739 --> 00:19:38,260
hayal etmekti. ya 0 ya da 1, ancak bu belirsizlik bölgesi için arada bir yumuşama var.

337
00:19:39,160 --> 00:19:43,855
Yani esas olarak, her bir kelimeye son listede yer almaları için atadığım olasılık, 

338
00:19:43,855 --> 00:19:48,440
x ekseninde nerede olursa olsun yukarıdaki sigmoid fonksiyonunun değeri olacaktır.

339
00:19:49,520 --> 00:19:52,138
Açıkçası bu birkaç parametreye bağlıdır; örneğin, 

340
00:19:52,138 --> 00:19:55,856
bu kelimelerin x ekseni üzerinde ne kadar geniş bir alanı dolduracağı, 

341
00:19:55,856 --> 00:20:00,464
1'den 0'a ne kadar kademeli veya dik bir şekilde düştüğümüzü belirler ve onları 

342
00:20:00,464 --> 00:20:03,240
soldan sağa nereye yerleştirdiğimiz kesmeyi belirler.

343
00:20:03,240 --> 00:20:06,920
Dürüst olmak gerekirse, bunu yapma şeklim sadece parmağımı yalayıp rüzgara doğru tutmaktı.

344
00:20:07,140 --> 00:20:09,805
Sıralanmış listeye baktım ve bir pencere bulmaya çalıştım; 

345
00:20:09,805 --> 00:20:13,239
ona baktığımda bu kelimelerin yaklaşık yarısının son cevap olma ihtimalinin 

346
00:20:13,239 --> 00:20:17,260
olmama ihtimalinden daha yüksek olduğunu düşündüm ve bunu kesme noktası olarak kullandım.

347
00:20:17,260 --> 00:20:19,928
Kelimeler arasında böyle bir dağılıma sahip olduğumuzda, 

348
00:20:19,928 --> 00:20:23,860
bu bize entropinin gerçekten yararlı bir ölçüm haline geldiği başka bir durum verir.

349
00:20:24,500 --> 00:20:28,921
Örneğin, diyelim ki bir oyun oynuyoruz ve tüy ve çivilerden oluşan eski açılışlarımla 

350
00:20:28,921 --> 00:20:33,240
başlıyoruz ve onunla eşleşen dört olası kelimenin olduğu bir durumla karşılaşıyoruz.

351
00:20:33,560 --> 00:20:35,620
Ve diyelim ki hepsinin eşit derecede olası olduğunu düşünüyoruz.

352
00:20:36,220 --> 00:20:38,880
Size şunu sorayım, bu dağılımın entropisi nedir?

353
00:20:41,080 --> 00:20:46,555
Bu olasılıkların her biriyle ilgili bilgi, 2/4'ün logaritması olacaktır, 

354
00:20:46,555 --> 00:20:50,040
çünkü her biri 1 ve 4'tür ve bu da 2'dir.

355
00:20:50,040 --> 00:20:52,460
İki bit bilgi, dört olasılık.

356
00:20:52,760 --> 00:20:53,580
Hepsi çok iyi ve güzel.

357
00:20:54,300 --> 00:20:57,800
Peki ya size aslında dörtten fazla eşleşme olduğunu söylesem?

358
00:20:58,260 --> 00:21:02,460
Gerçekte kelime listesinin tamamına baktığımızda onunla eşleşen 16 kelime var.

359
00:21:02,580 --> 00:21:06,506
Ancak modelimizin, diğer 12 kelimenin aslında nihai cevap olma ihtimalini gerçekten 

360
00:21:06,506 --> 00:21:09,124
düşük tuttuğunu varsayalım; 1000'de 1 gibi bir şey, 

361
00:21:09,124 --> 00:21:10,760
çünkü bunlar gerçekten belirsizdir.

362
00:21:11,500 --> 00:21:14,260
Şimdi size şunu sorayım, bu dağılımın entropisi nedir?

363
00:21:15,420 --> 00:21:18,037
Eğer entropi burada sadece eşleşme sayısını ölçüyorsa, 

364
00:21:18,037 --> 00:21:21,797
o zaman bunun 16'nın logaritması 2 gibi bir şey olmasını bekleyebilirsiniz 

365
00:21:21,797 --> 00:21:25,700
ki bu da 4 olur, daha önce sahip olduğumuz belirsizlikten iki bit daha fazla olur.

366
00:21:26,180 --> 00:21:29,860
Ancak elbette gerçek belirsizlik daha önce yaşadığımızdan çok da farklı değil.

367
00:21:30,160 --> 00:21:33,870
Bu 12 belirsiz kelimenin var olması, örneğin son cevabın çekicilik 

368
00:21:33,870 --> 00:21:37,360
olduğunu öğrenmenin çok daha şaşırtıcı olacağı anlamına gelmez.

369
00:21:38,180 --> 00:21:41,941
Yani buradaki hesaplamayı gerçekten yaptığınızda ve her bir olayın olasılığını 

370
00:21:41,941 --> 00:21:45,560
karşılık gelen bilgilerle topladığınızda elde ettiğiniz sonuç 2 olur.11 bit.

371
00:21:45,560 --> 00:21:49,137
Sadece şunu söylüyorum, temelde iki parça, temelde bu dört olasılık, 

372
00:21:49,137 --> 00:21:53,129
ancak tüm bu pek olası olmayan olaylar nedeniyle biraz daha belirsizlik var, 

373
00:21:53,129 --> 00:21:56,500
gerçi bunları öğrenmiş olsaydınız bundan bir ton bilgi alırsınız.

374
00:21:57,160 --> 00:21:59,240
Yani uzaklaştırma, Wordle'u bilgi teorisi dersi 

375
00:21:59,240 --> 00:22:01,400
için bu kadar güzel bir örnek yapan şeyin bir parçası.

376
00:22:01,600 --> 00:22:04,640
Entropi için iki ayrı duygu uygulamasına sahibiz.

377
00:22:05,160 --> 00:22:10,280
Birincisi bize belirli bir tahminden alacağımız beklenen bilginin ne olduğunu söylüyor, 

378
00:22:10,280 --> 00:22:15,460
ikincisi ise mümkün olan tüm kelimeler arasında kalan belirsizliği ölçebilir miyiz diyor.

379
00:22:16,460 --> 00:22:20,500
Ve şunu vurgulamalıyım ki, bir tahminin beklenen bilgisine baktığımız ilk durumda, 

380
00:22:20,500 --> 00:22:24,540
kelimelere eşit olmayan bir ağırlık verdiğimizde, bu entropi hesaplamasını etkiler.

381
00:22:24,980 --> 00:22:29,271
Örneğin, daha önce Weary ile ilişkili dağıtıma baktığımız aynı durumu ele alayım, 

382
00:22:29,271 --> 00:22:33,720
ancak bu sefer tüm olası kelimeler arasında tekdüze olmayan bir dağılım kullanıyorum.

383
00:22:34,500 --> 00:22:38,280
Bakalım burada bunu oldukça iyi gösteren bir bölüm bulabilecek miyim?

384
00:22:40,940 --> 00:22:42,360
Tamam, işte bu oldukça iyi.

385
00:22:42,360 --> 00:22:45,355
Burada birbirine eşit olasılıklara sahip iki bitişik modelimiz var, 

386
00:22:45,355 --> 00:22:49,100
ancak bize söylenenlerden birinin kendisiyle eşleşen 32 olası kelime olduğu söylendi.

387
00:22:49,280 --> 00:22:52,055
Ve bunların ne olduğunu kontrol edersek, bunlar şu 32 kelimedir, 

388
00:22:52,055 --> 00:22:55,600
gözlerinizi üzerlerine taradığınızda bunların hepsi pek olası olmayan kelimelerdir.

389
00:22:55,840 --> 00:22:58,786
Belki bağırmak gibi akla yatkın cevaplar bulmak zordur, 

390
00:22:58,786 --> 00:23:03,364
ancak dağılımdaki komşu düzene bakarsak, ki bu da hemen hemen aynı olası kabul edilir, 

391
00:23:03,364 --> 00:23:07,783
bize sadece 8 olası eşleşme olduğu söylendi, yani çeyrek olarak birçok eşleşme var, 

392
00:23:07,783 --> 00:23:09,520
ancak bu da bir o kadar muhtemel.

393
00:23:09,860 --> 00:23:12,140
Ve bu kibritleri çıkardığımızda nedenini görebiliriz.

394
00:23:12,500 --> 00:23:16,300
Bunlardan bazıları, zil sesi, gazap veya tecavüz gibi gerçekten makul yanıtlardır.

395
00:23:17,900 --> 00:23:21,590
Tüm bunları nasıl dahil ettiğimizi göstermek için, burada Wordlebot'un 2. 

396
00:23:21,590 --> 00:23:25,280
versiyonunu ele almama izin verin; ilk gördüğümüzden iki veya üç ana fark var.

397
00:23:25,860 --> 00:23:28,634
Öncelikle, az önce söylediğim gibi, bu entropileri, 

398
00:23:28,634 --> 00:23:31,409
bu beklenen bilgi değerlerini hesaplama yöntemimiz, 

399
00:23:31,409 --> 00:23:35,678
artık belirli bir kelimenin gerçekten cevap olma olasılığını da içeren kalıplar 

400
00:23:35,678 --> 00:23:38,240
arasındaki daha hassas dağılımları kullanmaktır.

401
00:23:38,879 --> 00:23:43,820
Aslına bakılırsa gözyaşları hala 1 numara, ancak sonrakiler biraz farklı.

402
00:23:44,360 --> 00:23:46,650
İkincisi, en çok tercih edilenleri sıraladığında, 

403
00:23:46,650 --> 00:23:50,132
artık her kelimenin asıl cevap olma ihtimaline ilişkin bir model tutacak ve 

404
00:23:50,132 --> 00:23:53,659
bunu kararına dahil edecek; bunu, konuyla ilgili birkaç tahminimiz olduğunda 

405
00:23:53,659 --> 00:23:55,080
görmek daha kolay olacak. masa.

406
00:23:55,860 --> 00:23:58,105
Yine tavsiyelerini göz ardı ediyoruz çünkü makinelerin 

407
00:23:58,105 --> 00:23:59,780
hayatlarımızı yönetmesine izin veremeyiz.

408
00:24:01,140 --> 00:24:04,884
Ve sanırım burada solda farklı olan başka bir şeyden bahsetmem gerekiyor; 

409
00:24:04,884 --> 00:24:09,336
belirsizlik değeri, yani bit sayısı, artık sadece olası eşleşmelerin sayısıyla gereksiz 

410
00:24:09,336 --> 00:24:09,640
değil.

411
00:24:10,080 --> 00:24:14,938
Şimdi yukarı çekip 2 üzeri 8'i hesaplarsak.02, ki bu da 256'nın, 

412
00:24:14,938 --> 00:24:19,530
sanırım 259'un biraz üzerinde, aslında bu kalıpla eşleşen toplam 

413
00:24:19,530 --> 00:24:23,656
526 kelime olmasına rağmen, sahip olduğu belirsizlik miktarı, 

414
00:24:23,656 --> 00:24:28,980
eşit derecede olası 259 kelime olsaydı ne olacağına daha çok benziyor. sonuçlar.

415
00:24:29,720 --> 00:24:30,740
Bunu şöyle düşünebilirsiniz.

416
00:24:31,020 --> 00:24:35,054
Borx'un cevap olmadığını biliyor, yorts, zorl ve zorus için de aynısı geçerli, 

417
00:24:35,054 --> 00:24:37,680
dolayısıyla önceki duruma göre biraz daha az belirsiz.

418
00:24:37,820 --> 00:24:39,280
Bu bit sayısı daha az olacaktır.

419
00:24:40,220 --> 00:24:43,242
Ve eğer oyunu oynamaya devam edersem, burada açıklamak 

420
00:24:43,242 --> 00:24:46,540
istediğim şeye uygun birkaç tahminle bunu detaylandıracağım.

421
00:24:48,360 --> 00:24:50,999
Dördüncü tahmine göre, eğer en çok tercih edilenlere bakarsanız, 

422
00:24:50,999 --> 00:24:53,760
bunun artık sadece entropiyi maksimuma çıkarmadığını görebilirsiniz.

423
00:24:54,460 --> 00:24:57,263
Yani bu noktada teknik olarak yedi olasılık var 

424
00:24:57,263 --> 00:25:00,300
ama anlamlı şansı olan tek şey yurtlar ve kelimeler.

425
00:25:00,300 --> 00:25:03,983
Ve her ikisini de seçmenin bu diğer değerlerin üzerinde yer aldığını, 

426
00:25:03,983 --> 00:25:06,720
açıkçası daha fazla bilgi vereceğini görebilirsiniz.

427
00:25:07,240 --> 00:25:10,641
Bunu ilk yaptığımda, her tahminin kalitesini ölçmek için bu iki sayıyı 

428
00:25:10,641 --> 00:25:13,900
topladım ve bu aslında tahmin edebileceğinizden daha iyi işe yaradı.

429
00:25:14,300 --> 00:25:17,091
Ama bu pek sistematik gelmedi ve eminim ki insanların benimseyebileceği 

430
00:25:17,091 --> 00:25:19,340
başka yaklaşımlar da vardır, ama ben bu yaklaşıma ulaştım.

431
00:25:19,760 --> 00:25:23,636
Bir sonraki tahmin olasılığını düşünürsek, bu durumda kelimeler gibi, 

432
00:25:23,636 --> 00:25:27,900
gerçekten umursadığımız şey, eğer bunu yaparsak oyunumuzun beklenen puanıdır.

433
00:25:28,230 --> 00:25:31,893
Beklenen puanı hesaplamak için de kelimelerin gerçek cevap olma 

434
00:25:31,893 --> 00:25:35,900
olasılığının ne olduğunu söylüyoruz ki bu şu anda %58'i açıklıyor.

435
00:25:36,040 --> 00:25:39,540
Bu maçta puanımızın %58 ihtimalle 4 olacağını söylüyoruz.

436
00:25:40,320 --> 00:25:45,640
Ve 1 eksi %58 olasılıkla puanımız 4'ten fazla olacaktır.

437
00:25:46,220 --> 00:25:49,222
Daha ne kadarını bilmiyoruz ama o noktaya geldiğimizde ne kadar 

438
00:25:49,222 --> 00:25:52,460
belirsizliğin ortaya çıkabileceğine dayanarak bunu tahmin edebiliriz.

439
00:25:52,960 --> 00:25:55,940
Özellikle şu anda 1 tane var.44 bit belirsizlik.

440
00:25:56,440 --> 00:26:01,120
Kelimeleri tahmin edersek, bu bize alacağımız beklenen bilginin 1 olduğunu söyler.27 bit.

441
00:26:01,620 --> 00:26:04,639
Yani kelimeleri tahmin edersek, bu fark, bu olay gerçekleştikten 

442
00:26:04,639 --> 00:26:07,660
sonra ne kadar belirsizlikle baş başa kalacağımızı temsil ediyor.

443
00:26:08,260 --> 00:26:10,424
İhtiyacımız olan şey, burada f adını verdiğim, 

444
00:26:10,424 --> 00:26:13,740
bu belirsizliği beklenen bir puanla ilişkilendiren bir tür fonksiyondur.

445
00:26:14,240 --> 00:26:18,168
Ve bunu gerçekleştirmenin yolu, botun 1. versiyonuna dayalı olarak 

446
00:26:18,168 --> 00:26:22,625
önceki oyunlardan bir grup veriyi çizerek, çok ölçülebilir belirsizliklerle 

447
00:26:22,625 --> 00:26:26,320
çeşitli noktalardan sonra gerçek puanın ne olduğunu söylemekti.

448
00:26:27,020 --> 00:26:31,394
Örneğin, buradaki veri noktaları 8 civarındaki bir değerin üzerinde duruyor.8'in 

449
00:26:31,394 --> 00:26:35,460
olduğu bir noktadan sonra bazı oyunlar için 7 ya da öylesine diyorlar.7 bitlik 

450
00:26:35,460 --> 00:26:38,960
belirsizlik, nihai cevaba ulaşmak için iki tahmin yapılması gerekti.

451
00:26:39,320 --> 00:26:42,240
Diğer oyunlar için üç tahmin gerekiyordu, diğer oyunlar için ise dört tahmin gerekiyordu.

452
00:26:43,140 --> 00:26:46,276
Burada sola kayarsak, sıfırın üzerindeki tüm noktalar, 

453
00:26:46,276 --> 00:26:49,869
ne zaman sıfır belirsizlik varsa, yani tek bir olasılık varsa, 

454
00:26:49,869 --> 00:26:54,260
o zaman gereken tahmin sayısı her zaman sadece birdir, bu da güven vericidir.

455
00:26:54,780 --> 00:26:58,745
Ne zaman bir miktar belirsizlik olsa, yani esasen iki olasılığa 

456
00:26:58,745 --> 00:27:03,020
bağlıysa bazen bir tahmin daha, bazen de iki tahmin daha gerekiyordu.

457
00:27:03,080 --> 00:27:05,240
Burada da böyle devam ediyor.

458
00:27:05,740 --> 00:27:08,215
Belki bu verileri görselleştirmenin biraz daha kolay bir yolu, 

459
00:27:08,215 --> 00:27:10,220
bunları bir araya toplayıp ortalamalarını almaktır.

460
00:27:11,000 --> 00:27:15,620
Örneğin buradaki çubuk, bir miktar belirsizliğimizin olduğu tüm noktalar arasında 

461
00:27:15,620 --> 00:27:19,960
ortalama olarak gereken yeni tahmin sayısının yaklaşık 1 olduğunu söylüyor.5.

462
00:27:22,140 --> 00:27:26,450
Ve buradaki çubuk, tüm farklı oyunlar arasında bir noktada belirsizliğin dört bitin 

463
00:27:26,450 --> 00:27:30,556
biraz üzerinde olduğunu söylüyor, bu da onu 16 farklı olasılığa daraltmak gibi, 

464
00:27:30,556 --> 00:27:35,072
o zaman ortalama olarak o noktadan itibaren ikiden biraz daha fazla tahmin gerektiriyor 

465
00:27:35,072 --> 00:27:35,380
ileri.

466
00:27:36,060 --> 00:27:39,460
Ve buradan itibaren buna makul görünen bir fonksiyona uyacak bir regresyon yaptım.

467
00:27:39,980 --> 00:27:44,219
Ve unutmayın, bunları yapmanın asıl amacı, bir kelimeden ne kadar çok bilgi 

468
00:27:44,219 --> 00:27:48,960
kazanırsak beklenen puanın o kadar düşük olacağı şeklindeki bu sezgiyi ölçebilmektir.

469
00:27:49,680 --> 00:27:54,912
Yani bu sürüm 2 olarak.0'a dönersek ve aynı simülasyon setini çalıştırırsak, 

470
00:27:54,912 --> 00:27:59,240
2315 olası sözcük yanıtının tümüne karşı oynatırsak, bu nasıl olur?

471
00:28:00,280 --> 00:28:03,420
İlk versiyonumuzun aksine kesinlikle daha iyi, bu da güven verici.

472
00:28:04,020 --> 00:28:06,407
Tüm söylenen ve yapılan ortalama 3 civarındadır.6, 

473
00:28:06,407 --> 00:28:10,340
ilk versiyondan farklı olarak birkaç kez kaybettiği ve bu durumda altıdan fazlasını 

474
00:28:10,340 --> 00:28:12,120
gerektirdiği durumlar olmasına rağmen.

475
00:28:12,639 --> 00:28:15,220
Muhtemelen bilgiyi en üst düzeye çıkarmak yerine hedefe 

476
00:28:15,220 --> 00:28:17,940
ulaşmak için bu ödünleşimin yapıldığı zamanlar olduğu için.

477
00:28:19,040 --> 00:28:21,000
Peki 3'ten daha iyisini yapabilir miyiz?6?

478
00:28:22,080 --> 00:28:22,920
Kesinlikle yapabiliriz.

479
00:28:23,280 --> 00:28:26,060
Başlangıçta, kelime cevaplarının gerçek listesini modelini 

480
00:28:26,060 --> 00:28:29,360
oluşturma biçimine dahil etmemenin çok eğlenceli olduğunu söylemiştim.

481
00:28:29,880 --> 00:28:34,180
Ancak bunu dahil edersek alabileceğim en iyi performans 3 civarındaydı.43.

482
00:28:35,160 --> 00:28:38,700
Dolayısıyla, bu önceki dağılımı seçmek için kelime sıklığı verilerini kullanmaktan daha 

483
00:28:38,700 --> 00:28:41,797
karmaşık hale gelmeye çalışırsak, bu 3.43 muhtemelen bunda ne kadar başarılı 

484
00:28:41,797 --> 00:28:45,418
olabileceğimizin veya en azından benim bunda ne kadar başarılı olabileceğimin maksimumunu 

485
00:28:45,418 --> 00:28:45,740
veriyor.

486
00:28:46,240 --> 00:28:50,024
Bu en iyi performans aslında sadece burada bahsettiğim fikirleri kullanır, 

487
00:28:50,024 --> 00:28:54,565
ancak biraz daha ileri gider, sanki beklenen bilgiyi tek bir adım yerine iki adım ileriye 

488
00:28:54,565 --> 00:28:55,120
doğru arar.

489
00:28:55,620 --> 00:28:57,958
Başlangıçta bunun hakkında daha fazla konuşmayı planlıyordum 

490
00:28:57,958 --> 00:29:00,220
ama aslında oldukça uzun bir yol kat ettiğimizi fark ettim.

491
00:29:00,580 --> 00:29:03,327
Söyleyeceğim tek şey, bu iki adımlı aramayı yaptıktan ve ardından en 

492
00:29:03,327 --> 00:29:05,994
iyi adaylar üzerinde birkaç örnek simülasyon çalıştırdıktan sonra, 

493
00:29:05,994 --> 00:29:09,100
şu ana kadar benim için en azından Crane'in en iyi açıcı olduğu görünüyor.

494
00:29:09,100 --> 00:29:10,060
Kim tahmin ederdi?

495
00:29:10,920 --> 00:29:14,862
Ayrıca olasılıklar alanınızı belirlemek için gerçek sözcük listesini kullanırsanız, 

496
00:29:14,862 --> 00:29:17,820
o zaman başlangıçtaki belirsizlik 11 bitin biraz üzerinde olur.

497
00:29:18,300 --> 00:29:21,949
Ve sadece kaba kuvvet aramasından sonra, ilk iki tahminden sonra 

498
00:29:21,949 --> 00:29:25,880
beklenen maksimum olası bilginin 10 bit civarında olduğu ortaya çıktı.

499
00:29:26,500 --> 00:29:29,203
Bu da en iyi senaryoda, ilk iki tahmininizden sonra, 

500
00:29:29,203 --> 00:29:33,335
mükemmel derecede optimal bir oyunla, yaklaşık bir miktar belirsizlikle baş başa 

501
00:29:33,335 --> 00:29:34,560
kalacağınızı gösteriyor.

502
00:29:34,800 --> 00:29:37,960
Bu, iki olası tahminde bulunmakla aynı şeydir.

503
00:29:37,960 --> 00:29:42,128
Bu yüzden, bu ortalamayı 3'e kadar düşüren bir algoritmayı asla yazamayacağınızı 

504
00:29:42,128 --> 00:29:45,954
söylemek adil ve muhtemelen oldukça muhafazakar olur çünkü kullanabileceğiniz 

505
00:29:45,954 --> 00:29:49,975
kelimelerle, sadece iki adımdan sonra yeterli bilgiyi elde etmek için yer yoktur. 

506
00:29:49,975 --> 00:29:53,360
her seferinde üçüncü slottaki cevabı hatasız olarak garanti edebilir.

