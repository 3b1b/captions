1
00:00:00,000 --> 00:00:04,040
Wurdle oyunu son bir iki ayda oldukça viral hale geldi ve hiçbir

2
00:00:04,040 --> 00:00:07,880
zaman bir matematik dersi fırsatını gözden kaçıran biri olmadı. Bana öyle geliyor

3
00:00:07,880 --> 00:00:12,120
ki bu oyun bilgi teorisi ile ilgili bir derste çok iyi bir

4
00:00:12,120 --> 00:00:13,120
merkezi örnek teşkil ediyor ve özellikle de entropi olarak bilinen bir konu.

5
00:00:13,120 --> 00:00:17,120
Görüyorsunuz, pek çok insan gibi ben de bulmacanın içine

6
00:00:17,120 --> 00:00:21,200
kapıldım ve birçok programcı gibi ben de oyunu

7
00:00:21,200 --> 00:00:23,200
olabildiğince optimum şekilde oynatacak bir algoritma yazmaya kapıldım.

8
00:00:23,200 --> 00:00:26,400
Ve burada yapmayı düşündüğüm şey, sizinle bu konudaki sürecimin

9
00:00:26,400 --> 00:00:29,980
bir kısmını konuşmak ve bunun içine giren matematiğin bir

10
00:00:29,980 --> 00:00:32,080
kısmını açıklamak, çünkü tüm algoritma bu entropi fikrine odaklanıyor.

11
00:00:32,080 --> 00:00:42,180
İlk olarak, duymadıysanız söyleyeyim, Wurdle nedir?

12
00:00:42,180 --> 00:00:45,380
Ve burada oyunun kurallarını incelerken bir taşla iki kuş vurmak

13
00:00:45,380 --> 00:00:48,980
için, bununla nereye gittiğimizi de ön izlememe izin verin, yani

14
00:00:48,980 --> 00:00:51,380
temelde oyunu bizim için oynayacak küçük bir algoritma geliştirmek.

15
00:00:51,380 --> 00:00:54,860
Bugünkü Wurdle&#39;ı yapmamış olsam da, bugün 4

16
00:00:54,860 --> 00:00:55,860
Şubat ve botun nasıl yapacağını göreceğiz.

17
00:00:55,860 --> 00:00:59,580
Wurdle&#39;ın amacı, beş harfli gizemli bir kelimeyi tahmin etmektir

18
00:00:59,580 --> 00:01:00,860
ve size tahmin etmeniz için altı farklı şans verilir.

19
00:01:00,860 --> 00:01:05,240
Örneğin, Wurdle botum tahmin vinciyle başlamamı öneriyor.

20
00:01:05,240 --> 00:01:09,300
Her tahmin yaptığınızda, tahmininizin gerçek cevaba ne

21
00:01:09,300 --> 00:01:10,940
kadar yakın olduğuna dair bazı bilgiler alırsınız.

22
00:01:10,940 --> 00:01:14,540
Burada gri kutu bana asıl cevapta C&#39;nin olmadığını söylüyor.

23
00:01:14,540 --> 00:01:18,340
Sarı kutu bana bir R olduğunu söylüyor ama o konumda değil.

24
00:01:18,340 --> 00:01:21,820
Yeşil kutu bana gizli kelimenin A harfine

25
00:01:21,820 --> 00:01:22,820
sahip olduğunu ve üçüncü sırada olduğunu söylüyor.

26
00:01:22,820 --> 00:01:24,300
Ve sonra N yok ve E yok.

27
00:01:24,300 --> 00:01:27,420
O halde içeri girip Wurdle botuna bu bilgiyi söyleyeyim.

28
00:01:27,420 --> 00:01:31,500
Turnayla başladık, gri, sarı, yeşil, gri, gri elde ettik.

29
00:01:31,500 --> 00:01:35,460
Şu anda gösterdiği tüm veriler hakkında endişelenmeyin, bunları zamanı gelince açıklayacağım.

30
00:01:35,460 --> 00:01:39,700
Ancak ikinci seçimimiz için en önemli önerisi saçmalıktır.

31
00:01:39,700 --> 00:01:43,500
Ve tahmininizin gerçekten beş harfli bir kelime olması gerekiyor, ancak göreceğiniz

32
00:01:43,500 --> 00:01:45,700
gibi, aslında tahmin etmenize izin vereceği şey konusunda oldukça liberal.

33
00:01:45,700 --> 00:01:48,860
Bu durumda, saçmalamayı deneriz.

34
00:01:48,860 --> 00:01:50,260
Ve tamam, işler oldukça iyi görünüyor.

35
00:01:50,260 --> 00:01:54,580
S ve H&#39;ye basıyoruz, yani ilk üç harfi biliyoruz, bir R olduğunu biliyoruz.

36
00:01:54,740 --> 00:01:59,740
Ve böylece SHA bir R gibi olacak veya SHA R bir şey olacak.

37
00:01:59,740 --> 00:02:03,200
Görünüşe göre Wurdle botu bunun yalnızca iki olasılığa

38
00:02:03,200 --> 00:02:05,220
bağlı olduğunu biliyor; kırık ya da keskin.

39
00:02:05,220 --> 00:02:08,620
Bu noktada aralarında bir tür çekişme var, bu

40
00:02:08,620 --> 00:02:11,260
yüzden sanırım muhtemelen alfabetik olduğu için parçayla uyumlu.

41
00:02:11,260 --> 00:02:13,000
Hangi yaşasın, asıl cevap bu.

42
00:02:13,000 --> 00:02:14,660
Böylece üçe çıktık.

43
00:02:14,660 --> 00:02:17,740
Bunun iyi olup olmadığını merak ediyorsanız, bir kişiden duyduğuma göre

44
00:02:17,740 --> 00:02:20,820
Wurdle&#39;da dört eşit ve üç birdie&#39;dir şeklinde bir cümle duydum.

45
00:02:20,820 --> 00:02:22,960
Oldukça uygun bir benzetme olduğunu düşünüyorum.

46
00:02:22,960 --> 00:02:27,560
Dört puan almak için oyununuzda tutarlı bir şekilde çalışmalısınız, ancak bu kesinlikle çılgınca değil.

47
00:02:27,560 --> 00:02:30,000
Ama üçe böldüğünüzde harika hissettiriyor.

48
00:02:30,000 --> 00:02:33,800
Yani eğer istekliyseniz, burada yapmak istediğim şey Wurdle botuna

49
00:02:33,800 --> 00:02:36,600
nasıl yaklaştığım konusunda en başından itibaren düşünce sürecimden bahsetmek.

50
00:02:36,600 --> 00:02:39,800
Ve dediğim gibi bu aslında bilgi teorisi dersi için bir bahane.

51
00:02:39,800 --> 00:02:43,160
Temel amaç bilgi nedir, entropinin ne olduğunu açıklamaktır.

52
00:02:48,560 --> 00:02:52,080
Bu konuya yaklaşırken ilk düşüncem İngilizcedeki

53
00:02:52,080 --> 00:02:53,560
farklı harflerin göreceli sıklıklarına bakmaktı.

54
00:02:53,560 --> 00:02:57,800
Ben de düşündüm ki, tamam, bu en sık kullanılan harflerin çoğuna karşılık

55
00:02:57,800 --> 00:02:59,960
gelen bir açılış tahmini veya bir açılış tahmin çifti var mı?

56
00:02:59,960 --> 00:03:03,780
Ve oldukça hoşuma giden bir şey de diğerini ve ardından tırnakları yapmaktı.

57
00:03:03,780 --> 00:03:06,980
Buradaki düşünce şu ki, eğer bir harfe rastlarsanız, yeşil ya

58
00:03:06,980 --> 00:03:07,980
da sarı elde edersiniz, bu her zaman iyi hissettirir.

59
00:03:07,980 --> 00:03:09,460
Bilgi alıyormuşsunuz gibi geliyor.

60
00:03:09,460 --> 00:03:13,140
Ancak bu durumlarda, vurmasanız ve her zaman gri tonlar alsanız

61
00:03:13,140 --> 00:03:16,640
bile, bu size yine de birçok bilgi verir, çünkü bu

62
00:03:16,640 --> 00:03:17,640
harflerden herhangi birine sahip olmayan bir kelime bulmak oldukça nadirdir.

63
00:03:17,640 --> 00:03:21,840
Ancak yine de bu pek sistematik gelmiyor çünkü

64
00:03:21,840 --> 00:03:23,520
örneğin harflerin sırasını dikkate almanın hiçbir faydası yok.

65
00:03:23,520 --> 00:03:26,080
Salyangoz yazabilecekken neden çivi yazayım ki?

66
00:03:26,080 --> 00:03:27,720
Sonunda S olması daha mı iyi?

67
00:03:27,720 --> 00:03:28,720
Gerçekten emin değilim.

68
00:03:28,720 --> 00:03:33,500
Bir arkadaşım yorgun sözcüğüyle başlamayı sevdiğini söyledi, bu beni biraz

69
00:03:33,500 --> 00:03:37,160
şaşırttı çünkü içinde W ve Y gibi alışılmadık harfler vardı.

70
00:03:37,160 --> 00:03:39,400
Ama kim bilir, belki bu daha iyi bir açılıştır.

71
00:03:39,400 --> 00:03:43,920
Potansiyel bir tahminin kalitesini değerlendirmek için verebileceğimiz

72
00:03:43,920 --> 00:03:44,920
bir tür niceliksel puan var mı?

73
00:03:44,920 --> 00:03:48,640
Şimdi olası tahminleri sıralama yöntemimizi belirlemek için geriye dönüp

74
00:03:48,640 --> 00:03:51,800
oyunun tam olarak nasıl kurulduğuna biraz açıklık getirelim.

75
00:03:51,800 --> 00:03:55,880
Yani, geçerli tahminler olarak kabul edilen, girmenize izin verecek

76
00:03:55,880 --> 00:03:57,920
yaklaşık 13.000 kelime uzunluğunda bir kelime listesi var.

77
00:03:57,920 --> 00:04:01,560
Ama baktığınızda pek çok sıra dışı şey var; kafa ya da Ali

78
00:04:01,560 --> 00:04:07,040
ve ARG gibi şeyler, Scrabble oyununda aile tartışmalarına yol açan türden kelimeler.

79
00:04:07,040 --> 00:04:10,600
Ancak oyunun havası, cevabın her zaman oldukça yaygın bir kelime olacağıdır.

80
00:04:10,600 --> 00:04:16,080
Ve aslında olası yanıtlar olan yaklaşık 2300 kelimeden oluşan başka bir liste daha var.

81
00:04:16,080 --> 00:04:20,320
Ve bu, insanların hazırladığı bir liste, sanırım özellikle oyun

82
00:04:20,320 --> 00:04:21,800
yaratıcısının kız arkadaşı tarafından, ki bu da oldukça eğlenceli.

83
00:04:21,800 --> 00:04:25,560
Ancak yapmak istediğim şey, bu projedeki amacımız, bu listeyle ilgili

84
00:04:25,560 --> 00:04:30,720
önceki bilgileri birleştirmeyen, Wordle çözen bir program yazıp yazamayacağımızı görmek.

85
00:04:30,720 --> 00:04:34,560
Öncelikle, bu listede bulamayacağınız pek çok

86
00:04:34,560 --> 00:04:35,560
yaygın beş harfli kelime var.

87
00:04:35,560 --> 00:04:38,360
Bu nedenle, biraz daha dayanıklı olan ve sadece resmi web sitesine

88
00:04:38,360 --> 00:04:41,960
değil, herkese karşı Wordle oynayabilecek bir program yazmak daha iyi olacaktır.

89
00:04:41,960 --> 00:04:45,900
Ayrıca bu olası yanıtlar listesinin ne olduğunu

90
00:04:45,900 --> 00:04:47,440
bilmemizin nedeni, bunun kaynak kodunda görünür olmasıdır.

91
00:04:47,440 --> 00:04:51,620
Ancak kaynak kodunda görünme şekli, cevapların

92
00:04:51,620 --> 00:04:52,840
günden güne ortaya çıkma sırasına göredir.

93
00:04:52,840 --> 00:04:56,400
Böylece her zaman yarının cevabının ne olacağına bakabilirsiniz.

94
00:04:56,400 --> 00:04:59,140
Açıkça görülüyor ki, listeyi kullanmanın hile yapmak olduğu bir anlam taşıyor.

95
00:04:59,140 --> 00:05:02,900
Ve daha ilginç bir bulmacayı ve daha zengin bir bilgi teorisi

96
00:05:02,900 --> 00:05:07,640
dersini ortaya çıkaran şey, daha yaygın sözcükleri tercih etme sezgisini yakalamak

97
00:05:07,640 --> 00:05:11,640
için genel olarak göreceli sözcük sıklıkları gibi daha evrensel verileri kullanmaktır.

98
00:05:11,640 --> 00:05:16,560
Peki bu 13.000 olasılık arasından açılış tahminini nasıl seçmeliyiz?

99
00:05:16,560 --> 00:05:19,960
Mesela arkadaşım bıkkınlık teklif ediyorsa kalitesini nasıl analiz etmeliyiz?

100
00:05:19,960 --> 00:05:25,040
Pek olası olmayan W&#39;yi sevdiğini söylemesinin nedeni, o W&#39;ye

101
00:05:25,040 --> 00:05:27,880
vurmanın ne kadar iyi hissettireceğinin uzun vadede doğasını sevmesidir.

102
00:05:27,880 --> 00:05:31,400
Örneğin, ortaya çıkan ilk kalıp bunun gibi bir şeyse, bu dev

103
00:05:31,400 --> 00:05:36,080
sözlükte bu kalıpla eşleşen yalnızca 58 kelime olduğu ortaya çıkıyor.

104
00:05:36,080 --> 00:05:38,900
Yani bu 13.000&#39;den çok büyük bir azalma.

105
00:05:38,900 --> 00:05:43,320
Ancak bunun diğer tarafı da elbette ki böyle bir model elde etmenin çok nadir olmasıdır.

106
00:05:43,360 --> 00:05:47,600
Spesifik olarak, her kelimenin cevap olma olasılığı eşit olsaydı,

107
00:05:47,600 --> 00:05:51,680
bu kalıba ulaşma olasılığı 58 bölü 13.000 olurdu.

108
00:05:51,680 --> 00:05:53,880
Elbette bunların cevap olma ihtimali eşit değil.

109
00:05:53,880 --> 00:05:56,680
Bunların çoğu çok belirsiz ve hatta şüpheli kelimelerdir.

110
00:05:56,680 --> 00:05:59,560
Ama en azından tüm bunlara ilk geçişimizde, hepsinin eşit

111
00:05:59,560 --> 00:06:02,040
derecede olası olduğunu varsayalım ve bunu biraz sonra düzeltelim.

112
00:06:02,040 --> 00:06:07,360
Mesele şu ki, çok fazla bilgi içeren bir modelin doğası gereği ortaya çıkması pek olası değildir.

113
00:06:07,360 --> 00:06:11,320
Aslında bilgilendirici olmanın anlamı bunun olası olmadığıdır.

114
00:06:11,920 --> 00:06:16,720
Bu açılışta görülecek çok daha olası bir model bunun

115
00:06:16,720 --> 00:06:18,360
gibi bir şey olabilir, burada elbette W harfi yoktur.

116
00:06:18,360 --> 00:06:22,080
Belki bir E vardır ve belki A yoktur, R yoktur, Y yoktur.

117
00:06:22,080 --> 00:06:24,640
Bu durumda 1400 olası eşleşme vardır.

118
00:06:24,640 --> 00:06:29,600
Hepsi eşit olasılıkta olsaydı, göreceğiniz modelin

119
00:06:29,600 --> 00:06:30,680
bu olma olasılığı yaklaşık %11 olurdu.

120
00:06:30,680 --> 00:06:34,320
Dolayısıyla en olası sonuçlar aynı zamanda en az bilgilendirici olanlardır.

121
00:06:34,320 --> 00:06:38,440
Burada daha küresel bir bakış elde etmek için, görebileceğiniz

122
00:06:38,440 --> 00:06:42,000
tüm farklı kalıplara göre olasılıkların tam dağılımını size göstereyim.

123
00:06:42,000 --> 00:06:46,000
Yani baktığınız her çubuk, ortaya çıkabilecek olası bir renk düzenine

124
00:06:46,000 --> 00:06:50,500
karşılık gelir, bunlardan 3 üzeri 5 olasılık vardır ve bunlar

125
00:06:50,500 --> 00:06:52,960
soldan sağa, en yaygından en az yaygına doğru düzenlenir.

126
00:06:52,960 --> 00:06:56,200
Yani buradaki en yaygın olasılık, tüm grileri elde etmenizdir.

127
00:06:56,200 --> 00:06:58,800
Bu, zamanın yaklaşık %14&#39;ünde gerçekleşir.

128
00:06:58,800 --> 00:07:02,040
Ve bir tahmin yaptığınızda umduğunuz şey, kendinizi bu uzun kuyrukta

129
00:07:02,040 --> 00:07:06,360
bir yerde bulmanızdır; burada olduğu gibi, açıkça buna benzeyen bu

130
00:07:06,360 --> 00:07:09,920
kalıba uyan şey için yalnızca 18 olasılığın olduğu yer.

131
00:07:09,920 --> 00:07:14,080
Ya da biraz daha sola gidersek belki buraya kadar gidebiliriz.

132
00:07:14,080 --> 00:07:16,560
Tamam, işte sana güzel bir bulmaca.

133
00:07:16,560 --> 00:07:20,600
İngilizce dilinde W ile başlayan, Y ile biten ve

134
00:07:20,600 --> 00:07:22,040
içinde bir yerde R bulunan üç kelime nedir?

135
00:07:22,040 --> 00:07:27,560
Cevapların, bakalım, uzun uzun, kurtlu ve alaycı olduğu ortaya çıktı.

136
00:07:27,560 --> 00:07:32,720
Bu kelimenin genel olarak ne kadar iyi olduğuna karar vermek için,

137
00:07:32,720 --> 00:07:35,720
bu dağılımdan alacağınız beklenen bilgi miktarının bir tür ölçüsünü istiyoruz.

138
00:07:36,360 --> 00:07:41,080
Her bir modeli incelersek ve onun gerçekleşme olasılığını ne kadar bilgilendirici olduğunu

139
00:07:41,080 --> 00:07:46,000
ölçen bir şeyle çarparsak, bu bize belki objektif bir puan verebilir.

140
00:07:46,000 --> 00:07:50,280
Şimdi bir şeyin ne olması gerektiğine dair ilk içgüdünüz eşleşme sayısı olabilir.

141
00:07:50,280 --> 00:07:52,960
Daha düşük bir ortalama eşleşme sayısı istiyorsunuz.

142
00:07:52,960 --> 00:07:57,400
Ancak bunun yerine, genellikle bilgiye atfettiğimiz daha evrensel bir ölçüm kullanmak istiyorum

143
00:07:57,400 --> 00:08:01,040
ve bu 13.000 kelimenin her birine, bunların gerçekten cevap olup olmadığına ilişkin

144
00:08:01,040 --> 00:08:04,320
farklı bir olasılık atandığında daha esnek olacak bir ölçüm kullanmak istiyorum.

145
00:08:10,600 --> 00:08:14,760
Standart bilgi birimi, biraz komik bir formüle sahip

146
00:08:14,760 --> 00:08:17,800
olan bit&#39;tir, ancak sadece örneklere bakarsak gerçekten sezgiseldir.

147
00:08:17,800 --> 00:08:21,880
Olasılık alanınızı yarıya indiren bir gözleminiz varsa,

148
00:08:21,880 --> 00:08:24,200
onun bir bit bilgisi vardır diyoruz.

149
00:08:24,200 --> 00:08:27,680
Örneğimizde, olasılıklar uzayı tüm olası kelimelerden oluşuyor ve ortaya çıkıyor ki, beş harfli

150
00:08:27,760 --> 00:08:31,560
kelimelerin yaklaşık yarısının S harfi var, bundan biraz daha az ama yarısı kadar.

151
00:08:31,560 --> 00:08:35,200
Yani bu gözlem size biraz bilgi verecektir.

152
00:08:35,200 --> 00:08:39,640
Bunun yerine yeni bir olgu bu olasılıklar alanını dört

153
00:08:39,640 --> 00:08:42,000
kat azaltırsa, onun iki bitlik bilgiye sahip olduğunu söyleriz.

154
00:08:42,000 --> 00:08:45,120
Örneğin, bu kelimelerin yaklaşık dörtte birinde T harfinin olduğu ortaya çıktı.

155
00:08:45,120 --> 00:08:49,720
Eğer gözlem bu alanı sekiz kat azaltırsa, bunun üç

156
00:08:49,720 --> 00:08:50,920
bitlik bilgi olduğunu söyleriz ve bu böyle devam eder.

157
00:08:50,920 --> 00:08:55,000
Dört bit onu 16&#39;ya, beş bit ise 32&#39;ye böler.

158
00:08:55,000 --> 00:09:00,160
Şimdi durup kendinize şu soruyu sorabilirsiniz: Bir olayın

159
00:09:00,160 --> 00:09:04,520
gerçekleşme olasılığı açısından bit sayısı bilgisinin formülü nedir?

160
00:09:04,520 --> 00:09:07,920
Burada söylediğimiz şey, bit sayısının yarısını aldığınızda, bu olasılık

161
00:09:07,920 --> 00:09:11,680
ile aynı şeydir; bu, bit sayısının iki üssünün bir

162
00:09:11,680 --> 00:09:16,200
bölü olasılık olduğunu söylemekle aynı şeydir; Bilginin log tabanının

163
00:09:16,200 --> 00:09:19,680
iki bölü olasılığa eşit olduğunu söyleyerek yeniden düzenleme yapar.

164
00:09:19,680 --> 00:09:23,200
Ve bazen bunu bir yeniden düzenlemeyle daha

165
00:09:23,200 --> 00:09:25,680
görürsünüz; burada bilgi, olasılığın negatif logaritması tabanıdır.

166
00:09:25,680 --> 00:09:29,120
Bu şekilde ifade edilirse, bu konuya yeni başlayan biri

167
00:09:29,120 --> 00:09:33,400
için biraz tuhaf görünebilir, ancak aslında bu, olasılıklarınızı kaç

168
00:09:33,400 --> 00:09:35,120
kez yarıya indirdiğinizi sormak gibi çok sezgisel bir fikirdir.

169
00:09:35,120 --> 00:09:37,840
Şimdi merak ediyorsanız, eğlenceli bir kelime oyunu

170
00:09:37,840 --> 00:09:39,920
oynadığımızı sanıyordum, neden logaritmalar devreye giriyor?

171
00:09:39,920 --> 00:09:43,920
Bunun daha güzel bir birim olmasının bir nedeni, pek olası olmayan olaylar hakkında

172
00:09:43,920 --> 00:09:48,120
konuşmanın çok daha kolay olmasıdır; bir gözlemin 20 bitlik bilgiye sahip olduğunu söylemek,

173
00:09:48,120 --> 00:09:53,480
şunun şunun meydana gelme olasılığının 0 olduğunu söylemekten çok daha kolaydır. 0000095.

174
00:09:53,480 --> 00:09:57,360
Ancak bu logaritmik ifadenin olasılık teorisine çok yararlı bir katkı olduğunun

175
00:09:57,360 --> 00:10:02,000
ortaya çıkmasının daha önemli bir nedeni, bilgilerin bir araya gelme şeklidir.

176
00:10:02,000 --> 00:10:05,560
Örneğin, bir gözlem size iki bitlik bilgi verirse, alanınızı dört katına

177
00:10:05,560 --> 00:10:10,120
çıkarırsa ve ardından Wordle&#39;deki ikinci tahmininiz gibi ikinci bir gözlem size

178
00:10:10,120 --> 00:10:14,480
başka bir üç bitlik bilgi verirse ve sizi başka bir sekiz

179
00:10:14,480 --> 00:10:17,360
kat daha küçültürse, ikisi birlikte size beş bitlik bilgi verir.

180
00:10:17,360 --> 00:10:21,200
Olasılıklar çoğalmayı sevdiği gibi, bilgi de eklemeyi sever.

181
00:10:21,200 --> 00:10:24,920
Dolayısıyla, bir dizi sayıyı topladığımız beklenen değer gibi bir şeyin

182
00:10:24,920 --> 00:10:28,660
alanına girdiğimizde, günlükler bununla uğraşmayı çok daha güzel hale getiriyor.

183
00:10:28,660 --> 00:10:32,600
Weary dağıtımımıza geri dönelim ve buraya her model için ne

184
00:10:32,600 --> 00:10:35,560
kadar bilgi bulunduğunu bize gösteren başka bir küçük izleyici ekleyelim.

185
00:10:35,560 --> 00:10:38,760
Fark etmenizi istediğim asıl şey, daha olası modellere ulaşma olasılığımız

186
00:10:38,760 --> 00:10:43,500
arttıkça, bilgi ne kadar düşükse, o kadar az bit kazanırsınız.

187
00:10:43,500 --> 00:10:47,360
Bu tahminin kalitesini ölçmemizin yolu, bu bilginin beklenen değerini almak

188
00:10:47,360 --> 00:10:51,620
olacaktır; burada her bir modeli inceliyoruz, bunun ne kadar muhtemel

189
00:10:51,620 --> 00:10:54,940
olduğunu söylüyoruz ve sonra bunu kaç bitlik bilgi aldığımızla çarpıyoruz.

190
00:10:54,940 --> 00:10:58,480
Ve Weary örneğinde bunun 4 olduğu ortaya çıkıyor. 9 bit.

191
00:10:58,480 --> 00:11:02,800
Yani ortalama olarak, bu açılış tahmininden alacağınız bilgi,

192
00:11:02,800 --> 00:11:05,660
olasılıklar alanınızı yaklaşık beş kez yarıya indirmeye eşdeğerdir.

193
00:11:05,660 --> 00:11:10,260
Bunun tersine, beklenen bilgi değeri daha yüksek olan

194
00:11:10,260 --> 00:11:13,220
bir tahmin örneği Slate gibi bir şey olabilir.

195
00:11:13,220 --> 00:11:16,180
Bu durumda dağılımın çok daha düz göründüğünü fark edeceksiniz.

196
00:11:16,180 --> 00:11:20,780
Özellikle, tüm grilerin en muhtemel oluşumu yalnızca %6 civarında meydana gelme şansına

197
00:11:20,780 --> 00:11:25,940
sahiptir, yani en azından açıkça 3 elde edersiniz. 9 bitlik bilgi.

198
00:11:25,940 --> 00:11:29,140
Ancak bu minimumdur, daha genel olarak bundan daha iyi bir şey elde edersiniz.

199
00:11:29,140 --> 00:11:33,380
Ve buradaki rakamları hesaplayıp ilgili tüm terimleri topladığınızda

200
00:11:33,380 --> 00:11:36,420
ortalama bilginin yaklaşık 5 olduğu ortaya çıkıyor. 8.

201
00:11:36,420 --> 00:11:42,140
Yani Weary&#39;nin aksine, olasılıklar alanınız bu ilk tahminden

202
00:11:42,140 --> 00:11:43,940
sonra ortalama olarak yarısı kadar büyük olacaktır.

203
00:11:43,940 --> 00:11:49,540
Bilgi miktarının bu beklenen değerinin adı hakkında aslında eğlenceli bir hikaye var.

204
00:11:49,540 --> 00:11:52,580
Bilgi teorisi, 1940&#39;larda Bell Laboratuarlarında çalışan Claude Shannon tarafından

205
00:11:52,580 --> 00:11:57,620
geliştirildi, ancak henüz yayınlanmamış bazı fikirlerinden, zamanın entelektüel

206
00:11:57,620 --> 00:12:01,500
devi, çok öne çıkan John von Neumann&#39;la konuşuyordu. matematik

207
00:12:01,500 --> 00:12:04,180
ve fizikte ve bilgisayar bilimine dönüşen şeyin başlangıcı.

208
00:12:04,180 --> 00:12:07,260
Ve von Neumann, bilgi miktarının bu beklenen değeri için gerçekten

209
00:12:07,260 --> 00:12:12,540
iyi bir isme sahip olmadığını söylediğinde, söylendiğine göre, hikaye şöyle

210
00:12:12,540 --> 00:12:14,720
devam ediyor, buna entropi diyebilirsiniz ve bunun iki nedeni var.

211
00:12:14,720 --> 00:12:18,400
İlk olarak, belirsizlik fonksiyonunuz istatistiksel mekanikte bu isimle kullanıldı, dolayısıyla zaten bir

212
00:12:18,400 --> 00:12:23,100
adı var ve ikinci olarak ve daha da önemlisi, hiç kimse entropinin

213
00:12:23,100 --> 00:12:26,940
gerçekte ne olduğunu bilmiyor, dolayısıyla bir tartışmada her zaman avantajı var.

214
00:12:26,940 --> 00:12:31,420
Yani eğer isim biraz gizemli görünüyorsa ve eğer

215
00:12:31,420 --> 00:12:33,420
bu hikayeye inanılacaksa, bu bir bakıma tasarım gereğidir.

216
00:12:33,420 --> 00:12:36,740
Ayrıca, eğer bunun fizikteki termodinamiğin ikinci yasasıyla ilgili tüm

217
00:12:36,740 --> 00:12:40,820
o şeylerle ilişkisini merak ediyorsanız, kesinlikle bir bağlantı

218
00:12:40,820 --> 00:12:44,780
var, ama kökeninde Shannon sadece saf olasılık teorisiyle

219
00:12:44,780 --> 00:12:49,340
ilgileniyordu ve buradaki amaçlarımız için, Kelime entropisi, sadece

220
00:12:49,340 --> 00:12:50,820
belirli bir tahminin beklenen bilgi değerini düşünmenizi istiyorum.

221
00:12:50,820 --> 00:12:54,380
Entropiyi iki şeyin aynı anda ölçülmesi olarak düşünebilirsiniz.

222
00:12:54,380 --> 00:12:57,420
Bunlardan ilki dağılımın ne kadar düz olduğudur.

223
00:12:57,420 --> 00:13:01,700
Dağılım düzgünlüğe ne kadar yakınsa entropi o kadar yüksek olur.

224
00:13:01,700 --> 00:13:06,340
Bizim durumumuzda, 3 üzeri 5&#39;lik toplam örüntülerin olduğu durumda, düzgün bir dağılım için, bunlardan herhangi

225
00:13:06,340 --> 00:13:11,340
birinin gözlemlenmesi, 3 üzeri 5&#39;lik bilgi günlüğü tabanı 2&#39;ye sahip olacaktır; bu da 7

226
00:13:11,340 --> 00:13:17,860
olur. 92, yani bu entropi için sahip olabileceğiniz mutlak maksimum değer budur.

227
00:13:17,860 --> 00:13:21,900
Ancak entropi aynı zamanda ilk etapta ne

228
00:13:21,900 --> 00:13:22,900
kadar olasılığın bulunduğunun da bir ölçüsüdür.

229
00:13:22,900 --> 00:13:26,980
Örneğin, yalnızca 16 olası örüntünün olduğu ve her birinin eşit olasılığa sahip

230
00:13:26,980 --> 00:13:32,760
olduğu bir kelimeniz varsa, bu entropi, bu beklenen bilgi 4 bit olacaktır.

231
00:13:32,760 --> 00:13:36,880
Ancak 64 olası örüntünün ortaya çıkabileceği başka bir kelimeniz varsa ve bunların

232
00:13:36,880 --> 00:13:41,000
hepsi eşit derecede olasıysa, o zaman entropi 6 bit olarak hesaplanacaktır.

233
00:13:41,000 --> 00:13:45,800
Yani, eğer doğada 6 bitlik bir entropiye sahip bir dağılım

234
00:13:45,800 --> 00:13:50,000
görürseniz, bu sanki 64 eşit olasılıklı sonuç varmış gibi, olacaklar

235
00:13:50,000 --> 00:13:54,400
konusunda çok fazla değişkenlik ve belirsizlik olduğunu söylemek gibidir.

236
00:13:54,400 --> 00:13:58,360
Wurtelebot&#39;a ilk geçişimde temelde bunu yapmasını sağladım.

237
00:13:58,360 --> 00:14:03,560
Yapabileceğiniz tüm olası tahminleri, yani 13.000 kelimenin tamamını gözden geçirir, her

238
00:14:03,560 --> 00:14:08,580
biri için entropiyi veya daha spesifik olarak, her biri için görebileceğiniz

239
00:14:08,580 --> 00:14:13,040
tüm kalıplar arasındaki dağılımın entropisini hesaplar ve en yüksek olanı seçer,

240
00:14:13,040 --> 00:14:17,200
çünkü bu Olasılık alanınızı mümkün olduğu kadar daraltması muhtemel olanı.

241
00:14:17,200 --> 00:14:20,120
Burada sadece ilk tahminden bahsetmiş olsam da sonraki

242
00:14:20,120 --> 00:14:21,680
birkaç tahmin için de aynı şeyi yapıyor.

243
00:14:21,680 --> 00:14:25,100
Örneğin, ilk tahminde, hangi kelimeyle eşleştiğine bağlı olarak sizi daha

244
00:14:25,100 --> 00:14:29,300
az sayıda olası kelimeyle sınırlayacak bir model gördükten sonra,

245
00:14:29,300 --> 00:14:32,300
aynı oyunu o daha küçük kelime kümesine göre oynarsınız.

246
00:14:32,300 --> 00:14:36,500
Önerilen ikinci bir tahmin için, bu daha kısıtlı kelime

247
00:14:36,500 --> 00:14:41,540
grubundan oluşabilecek tüm kalıpların dağılımına bakarsınız, 13.000 olasılığın

248
00:14:41,540 --> 00:14:45,480
tamamını ararsınız ve bu entropiyi maksimuma çıkaranı bulursunuz.

249
00:14:45,480 --> 00:14:48,980
Bunun nasıl çalıştığını size göstermek için, kenarlarda bu analizin önemli noktalarını

250
00:14:48,980 --> 00:14:54,060
gösteren, yazdığım Wurtele&#39;nin küçük bir versiyonunu ele almama izin verin.

251
00:14:54,460 --> 00:14:57,820
Tüm entropi hesaplamalarını yaptıktan sonra sağ tarafta bize

252
00:14:57,820 --> 00:15:00,340
hangilerinin en yüksek beklenen bilgiye sahip olduğunu gösteriyor.

253
00:15:00,340 --> 00:15:04,940
En azından şimdilik en önemli cevabın Tares olduğu ortaya çıktı,

254
00:15:04,940 --> 00:15:11,140
bu da tabii ki fiğ, en yaygın fiğ anlamına geliyor.

255
00:15:11,140 --> 00:15:14,180
Burada her tahmin yaptığımızda, belki de önerilerini göz ardı edip slate&#39;i

256
00:15:14,180 --> 00:15:19,220
tercih ederim, çünkü slate&#39;i severim, ne kadar beklenen bilgiye sahip olduğunu

257
00:15:19,220 --> 00:15:23,300
görebiliriz, ancak burada kelimenin sağında bize ne kadar bilgi olduğunu gösteriyor.

258
00:15:23,340 --> 00:15:24,980
Bu özel model göz önüne alındığında, elde ettiğimiz gerçek bilgiler.

259
00:15:24,980 --> 00:15:28,660
Yani burada biraz şanssızız gibi görünüyor, 5 almamız bekleniyordu. 8, ama

260
00:15:28,660 --> 00:15:30,660
bundan daha azına sahip bir şey elde ettik.

261
00:15:30,660 --> 00:15:34,020
Ve sol tarafta, şu anda bulunduğumuz yere göre

262
00:15:34,020 --> 00:15:35,860
bize mümkün olan tüm farklı kelimeleri gösteriyor.

263
00:15:35,860 --> 00:15:39,820
Mavi çubuklar bize her kelimenin ne kadar olası olduğunu düşündüğünü gösteriyor; dolayısıyla şu

264
00:15:39,820 --> 00:15:44,140
anda her kelimenin eşit derecede gerçekleşme olasılığını varsayıyor, ancak bunu birazdan düzelteceğiz.

265
00:15:44,140 --> 00:15:48,580
Ve bu belirsizlik ölçümü bize bu dağılımın olası kelimeler arasındaki

266
00:15:48,580 --> 00:15:53,220
entropisini anlatıyor; bu şu anda tekdüze bir dağılım olduğu

267
00:15:53,300 --> 00:15:55,940
için olasılıkların sayısını saymanın gereksiz derecede karmaşık bir yoludur.

268
00:15:55,940 --> 00:16:01,700
Örneğin 2 üssü 13&#39;ü alırsak. 66, bu

269
00:16:01,700 --> 00:16:02,700
13.000 olasılık civarında olmalı.

270
00:16:02,700 --> 00:16:06,780
Burada biraz yanlışım var ama bunun nedeni ondalık basamakların tamamını göstermemem.

271
00:16:06,780 --> 00:16:10,260
Şu anda bu size gereksiz gelebilir ve işleri aşırı derecede karmaşık hale getirebilir, ancak

272
00:16:10,260 --> 00:16:12,780
bir dakika içinde her iki sayıya da sahip olmanın neden yararlı olduğunu göreceksiniz.

273
00:16:12,780 --> 00:16:16,780
Yani burada ikinci tahminimiz için en yüksek entropinin Ramen olduğunu öne

274
00:16:16,780 --> 00:16:19,700
sürüyor gibi görünüyor ki bu da yine tek kelime gibi gelmiyor.

275
00:16:19,700 --> 00:16:25,660
Burada ahlaki açıdan yüksek bir yer edinmek için, devam edip Rains yazacağım.

276
00:16:25,660 --> 00:16:27,540
Ve yine biraz şanssızmışız gibi görünüyor.

277
00:16:27,540 --> 00:16:32,100
Biz 4 bekliyorduk. 3 bit ve elimizde sadece 3 var. 39 bit bilgi.

278
00:16:32,100 --> 00:16:35,060
Bu da bizi 55 olasılığa indiriyor.

279
00:16:35,060 --> 00:16:38,860
Ve burada belki de aslında onun önerdiği şeyle, yani

280
00:16:38,860 --> 00:16:40,200
birleşik olanla, her ne anlama geliyorsa onunla devam edeceğim.

281
00:16:40,200 --> 00:16:43,300
Ve tamam, bu aslında bir bulmaca için iyi bir şans.

282
00:16:43,300 --> 00:16:47,020
Bu modelin bize 4 verdiğini söylüyor. 7 bitlik bilgi.

283
00:16:47,020 --> 00:16:52,400
Ama sol tarafta, bu modeli görmeden önce 5 tane vardı. 78 bitlik belirsizlik.

284
00:16:52,400 --> 00:16:56,860
Peki sizin için bir test olarak, kalan olasılıkların sayısı ne anlama geliyor?

285
00:16:56,860 --> 00:17:02,280
Bu, bir miktar belirsizliğe indirgendiğimiz anlamına gelir ki

286
00:17:02,280 --> 00:17:04,700
bu, iki olası yanıt olduğunu söylemekle aynı şeydir.

287
00:17:04,700 --> 00:17:06,520
Bu 50-50&#39;lik bir seçim.

288
00:17:06,520 --> 00:17:09,860
Ve buradan yola çıkarak, sen ve ben hangi kelimelerin daha

289
00:17:09,860 --> 00:17:11,220
yaygın olduğunu bildiğimiz için cevabın uçurum olması gerektiğini biliyoruz.

290
00:17:11,220 --> 00:17:13,540
Ancak şu anda yazıldığı gibi, program bunu bilmiyor.

291
00:17:13,540 --> 00:17:17,560
Böylece tek bir olasılık kalana kadar mümkün olduğu kadar çok

292
00:17:17,560 --> 00:17:20,360
bilgi toplamaya devam eder ve sonra onu tahmin eder.

293
00:17:20,360 --> 00:17:22,700
Açıkçası daha iyi bir oyunsonu stratejisine ihtiyacımız var.

294
00:17:22,700 --> 00:17:26,540
Ancak diyelim ki bu sürüme kelime çözücülerimizden biri adını verdik

295
00:17:26,540 --> 00:17:30,740
ve sonra gidip nasıl çalıştığını görmek için bazı simülasyonlar çalıştırdık.

296
00:17:30,740 --> 00:17:34,240
Yani bunun çalışma şekli mümkün olan her kelime oyununu oynamaktır.

297
00:17:34,240 --> 00:17:38,780
Gerçek wordle cevapları olan 2315 kelimenin tamamının üzerinden geçiyor.

298
00:17:38,780 --> 00:17:41,340
Temel olarak bunu bir test seti olarak kullanıyor.

299
00:17:41,340 --> 00:17:45,820
Ve bir kelimenin ne kadar yaygın olduğunu dikkate almamak ve tek ve tek bir seçeneğe varıncaya

300
00:17:45,820 --> 00:17:50,480
kadar yol boyunca her adımda bilgiyi en üst düzeye çıkarmaya çalışmak gibi naif bir yöntemle.

301
00:17:50,480 --> 00:17:55,100
Simülasyonun sonunda ortalama puan 4 civarında çıkıyor. 124.

302
00:17:55,100 --> 00:17:59,780
Ki bu fena değil, dürüst olmak gerekirse, daha kötüsünü bekliyordum.

303
00:17:59,780 --> 00:18:03,040
Ancak wordle oynayanlar size genellikle 4&#39;te alabileceklerini söyleyecektir.

304
00:18:03,040 --> 00:18:05,260
Asıl zorluk 3&#39;te mümkün olduğunca çok sayıda elde etmektir.

305
00:18:05,260 --> 00:18:08,920
4&#39;lük skor ile 3&#39;lük skor arasında oldukça büyük bir sıçrama var.

306
00:18:08,920 --> 00:18:13,300
Buradaki bariz düşük sonuç, bir kelimenin yaygın olup olmadığını ve

307
00:18:13,300 --> 00:18:23,160
bunu tam olarak nasıl yapacağımızı bir şekilde dahil etmektir.

308
00:18:23,160 --> 00:18:26,860
Benim yaklaşımım İngilizce dilindeki tüm kelimelerin

309
00:18:26,860 --> 00:18:28,560
göreceli frekanslarının bir listesini almaktı.

310
00:18:28,560 --> 00:18:32,560
Ve az önce Mathematica&#39;nın Google Kitaplar İngilizce Ngram genel

311
00:18:32,560 --> 00:18:35,520
veri kümesinden alınan kelime frekansı veri fonksiyonunu kullandım.

312
00:18:35,520 --> 00:18:38,680
Ve buna bakmak oldukça eğlenceli, örneğin en yaygın

313
00:18:38,680 --> 00:18:40,120
sözcüklerden en az kullanılan sözcüklere doğru sıralarsak.

314
00:18:40,120 --> 00:18:43,740
Açıkçası bunlar İngilizce dilinde en yaygın 5 harfli kelimelerdir.

315
00:18:43,740 --> 00:18:46,480
Daha doğrusu bunlar en yaygın 8&#39;incisidir.

316
00:18:46,480 --> 00:18:49,440
İlki hangisi, sonrasında orası ve orası var.

317
00:18:49,440 --> 00:18:53,020
Birincinin kendisi birinci değil, 9&#39;uncudur ve bu diğer kelimelerin

318
00:18:53,020 --> 00:18:57,840
daha sık ortaya çıkabileceği, ilk gelenlerin sonra olduğu ve

319
00:18:57,840 --> 00:18:59,000
nerede olduğu ve biraz daha az yaygın olduğu mantıklıdır.

320
00:18:59,000 --> 00:19:04,400
Şimdi, bu verileri, bu kelimelerin her birinin nihai cevap olma

321
00:19:04,400 --> 00:19:06,760
olasılığını modellemek için kullanırken, bu sadece sıklıkla orantılı olmamalıdır.

322
00:19:07,020 --> 00:19:12,560
Örneğin 0 puan verilir. Bu veri setinde 002 var, oysa

323
00:19:12,560 --> 00:19:15,200
örgü kelimesinin olasılığı bir anlamda 1000 kat daha az.

324
00:19:15,200 --> 00:19:19,400
Ancak bunların her ikisi de, neredeyse kesinlikle dikkate alınmaya değer olacak kadar yaygın kelimelerdir.

325
00:19:19,400 --> 00:19:21,900
Bu yüzden daha fazla ikili kesinti istiyoruz.

326
00:19:21,900 --> 00:19:26,520
Bu konuda izlediğim yol, tüm bu sıralanmış kelime listesini alıp, bunu bir

327
00:19:26,520 --> 00:19:31,060
x eksenine göre düzenlediğimi ve ardından çıktısı temelde ikili olan bir fonksiyona

328
00:19:31,060 --> 00:19:35,540
sahip olmanın standart yolu olan sigmoid fonksiyonunu uyguladığımı hayal etmekti. ya 0

329
00:19:35,540 --> 00:19:38,500
ya da 1, ancak bu belirsizlik bölgesi için arada bir yumuşama var.

330
00:19:38,500 --> 00:19:43,900
Yani esas olarak, her bir kelimeye son listede yer almaları için atadığım

331
00:19:43,900 --> 00:19:49,540
olasılık, x ekseninde nerede olursa olsun yukarıdaki sigmoid fonksiyonunun değeri olacaktır.

332
00:19:49,540 --> 00:19:53,940
Açıkçası bu birkaç parametreye bağlıdır; örneğin, bu kelimelerin x ekseni üzerinde ne

333
00:19:53,940 --> 00:19:59,660
kadar geniş bir alanı dolduracağı, 1&#39;den 0&#39;a ne kadar kademeli veya dik

334
00:19:59,660 --> 00:20:03,000
bir şekilde düştüğümüzü belirler ve onları soldan sağa nereye yerleştirdiğimiz kesmeyi belirler.

335
00:20:03,160 --> 00:20:07,340
Dürüst olmak gerekirse, bunu yapma şeklim sadece parmağımı yalayıp rüzgara doğru tutmaktı.

336
00:20:07,340 --> 00:20:10,800
Sıralanmış listeye baktım ve bir pencere bulmaya çalıştım; ona baktığımda

337
00:20:10,800 --> 00:20:15,280
bu kelimelerin yaklaşık yarısının son cevap olma ihtimalinin olmama ihtimalinden

338
00:20:15,280 --> 00:20:17,680
daha yüksek olduğunu düşündüm ve bunu kesme noktası olarak kullandım.

339
00:20:17,680 --> 00:20:21,840
Kelimeler arasında böyle bir dağılıma sahip olduğumuzda, bu bize entropinin

340
00:20:21,840 --> 00:20:24,460
gerçekten yararlı bir ölçüm haline geldiği başka bir durum verir.

341
00:20:24,460 --> 00:20:28,480
Örneğin, diyelim ki bir oyun oynuyoruz ve tüy

342
00:20:28,480 --> 00:20:32,480
ve çivilerden oluşan eski açılışlarımla başlıyoruz ve onunla

343
00:20:32,480 --> 00:20:33,760
eşleşen dört olası kelimenin olduğu bir durumla karşılaşıyoruz.

344
00:20:33,760 --> 00:20:36,440
Ve diyelim ki hepsinin eşit derecede olası olduğunu düşünüyoruz.

345
00:20:36,440 --> 00:20:40,000
Size şunu sorayım, bu dağılımın entropisi nedir?

346
00:20:40,000 --> 00:20:45,920
Bu olasılıkların her biriyle ilgili bilgi, 2/4&#39;ün logaritması olacaktır, çünkü

347
00:20:45,920 --> 00:20:50,800
her biri 1 ve 4&#39;tür ve bu da 2&#39;dir.

348
00:20:50,800 --> 00:20:52,780
İki bit bilgi, dört olasılık.

349
00:20:52,780 --> 00:20:54,360
Hepsi çok iyi ve güzel.

350
00:20:54,360 --> 00:20:58,320
Peki ya size aslında dörtten fazla eşleşme olduğunu söylesem?

351
00:20:58,320 --> 00:21:02,600
Gerçekte kelime listesinin tamamına baktığımızda onunla eşleşen 16 kelime var.

352
00:21:02,600 --> 00:21:07,260
Ancak modelimizin, diğer 12 kelimenin aslında nihai cevap olma ihtimalini gerçekten düşük

353
00:21:07,260 --> 00:21:11,440
tuttuğunu varsayalım; 1000&#39;de 1 gibi bir şey, çünkü bunlar gerçekten belirsizdir.

354
00:21:11,440 --> 00:21:15,480
Şimdi size şunu sorayım, bu dağılımın entropisi nedir?

355
00:21:15,480 --> 00:21:19,600
Eğer entropi burada sadece eşleşme sayısını ölçüyorsa, o zaman bunun 16&#39;nın

356
00:21:19,600 --> 00:21:24,760
logaritması 2 gibi bir şey olmasını bekleyebilirsiniz ki bu da 4

357
00:21:24,760 --> 00:21:26,200
olur, daha önce sahip olduğumuz belirsizlikten iki bit daha fazla olur.

358
00:21:26,200 --> 00:21:30,320
Ancak elbette gerçek belirsizlik daha önce yaşadığımızdan çok da farklı değil.

359
00:21:30,320 --> 00:21:33,840
Bu 12 belirsiz kelimenin var olması, örneğin son cevabın

360
00:21:33,840 --> 00:21:38,200
çekicilik olduğunu öğrenmenin çok daha şaşırtıcı olacağı anlamına gelmez.

361
00:21:38,200 --> 00:21:42,080
Yani buradaki hesaplamayı gerçekten yaptığınızda ve her bir olayın olasılığını karşılık

362
00:21:42,080 --> 00:21:45,960
gelen bilgilerle topladığınızda elde ettiğiniz sonuç 2 olur. 11 bit.

363
00:21:45,960 --> 00:21:50,280
Sadece şunu söylüyorum, temelde iki parça, temelde bu dört olasılık, ancak

364
00:21:50,280 --> 00:21:54,240
tüm bu pek olası olmayan olaylar nedeniyle biraz daha belirsizlik

365
00:21:54,240 --> 00:21:57,120
var, gerçi bunları öğrenmiş olsaydınız bundan bir ton bilgi alırsınız.

366
00:21:57,120 --> 00:22:00,800
Yani uzaklaştırma, Wordle&#39;u bilgi teorisi dersi için bu

367
00:22:00,800 --> 00:22:01,800
kadar güzel bir örnek yapan şeyin bir parçası.

368
00:22:01,800 --> 00:22:05,280
Entropi için iki ayrı duygu uygulamasına sahibiz.

369
00:22:05,280 --> 00:22:09,640
Birincisi bize belirli bir tahminden alacağımız beklenen bilginin

370
00:22:09,640 --> 00:22:14,560
ne olduğunu söylüyor, ikincisi ise mümkün olan tüm

371
00:22:14,560 --> 00:22:16,480
kelimeler arasında kalan belirsizliği ölçebilir miyiz diyor.

372
00:22:16,480 --> 00:22:19,800
Ve şunu vurgulamalıyım ki, bir tahminin beklenen bilgisine baktığımız ilk durumda,

373
00:22:19,800 --> 00:22:25,000
kelimelere eşit olmayan bir ağırlık verdiğimizde, bu entropi hesaplamasını etkiler.

374
00:22:25,000 --> 00:22:28,600
Örneğin, daha önce Weary ile ilişkili dağıtıma baktığımız

375
00:22:28,600 --> 00:22:33,560
aynı durumu ele alayım, ancak bu sefer tüm

376
00:22:33,560 --> 00:22:34,560
olası kelimeler arasında tekdüze olmayan bir dağılım kullanıyorum.

377
00:22:34,560 --> 00:22:39,360
Bakalım burada bunu oldukça iyi gösteren bir bölüm bulabilecek miyim?

378
00:22:39,360 --> 00:22:42,480
Tamam, işte bu oldukça iyi.

379
00:22:42,480 --> 00:22:46,360
Burada birbirine eşit olasılıklara sahip iki bitişik modelimiz var, ancak

380
00:22:46,360 --> 00:22:49,480
bize söylenenlerden birinin kendisiyle eşleşen 32 olası kelime olduğu söylendi.

381
00:22:49,480 --> 00:22:54,080
Ve bunların ne olduğunu kontrol edersek, bunlar şu 32 kelimedir,

382
00:22:54,080 --> 00:22:55,600
gözlerinizi üzerlerine taradığınızda bunların hepsi pek olası olmayan kelimelerdir.

383
00:22:55,600 --> 00:23:00,400
Belki bağırmak gibi akla yatkın cevaplar bulmak zordur, ancak dağılımdaki komşu

384
00:23:00,400 --> 00:23:04,440
düzene bakarsak, ki bu da hemen hemen aynı olası kabul

385
00:23:04,440 --> 00:23:08,920
edilir, bize sadece 8 olası eşleşme olduğu söylendi, yani çeyrek olarak

386
00:23:08,920 --> 00:23:09,920
birçok eşleşme var, ancak bu da bir o kadar muhtemel.

387
00:23:09,920 --> 00:23:12,520
Ve bu kibritleri çıkardığımızda nedenini görebiliriz.

388
00:23:12,520 --> 00:23:17,840
Bunlardan bazıları, zil sesi, gazap veya tecavüz gibi gerçekten makul yanıtlardır.

389
00:23:17,840 --> 00:23:22,000
Tüm bunları nasıl dahil ettiğimizi göstermek için, burada Wordlebot&#39;un 2. versiyonunu ele

390
00:23:22,000 --> 00:23:25,960
almama izin verin; ilk gördüğümüzden iki veya üç ana fark var.

391
00:23:25,960 --> 00:23:29,460
Öncelikle, az önce söylediğim gibi, bu entropileri, bu beklenen bilgi

392
00:23:29,460 --> 00:23:34,800
değerlerini hesaplama yöntemimiz, artık belirli bir kelimenin gerçekten cevap olma

393
00:23:34,800 --> 00:23:39,300
olasılığını da içeren kalıplar arasındaki daha hassas dağılımları kullanmaktır.

394
00:23:39,300 --> 00:23:44,160
Aslına bakılırsa gözyaşları hala 1 numara, ancak sonrakiler biraz farklı.

395
00:23:44,160 --> 00:23:47,920
İkincisi, en çok tercih edilenleri sıraladığında, artık her kelimenin asıl cevap

396
00:23:47,920 --> 00:23:52,600
olma ihtimaline ilişkin bir model tutacak ve bunu kararına dahil edecek;

397
00:23:52,600 --> 00:23:55,520
bunu, konuyla ilgili birkaç tahminimiz olduğunda görmek daha kolay olacak. masa.

398
00:23:55,520 --> 00:24:01,120
Yine tavsiyelerini göz ardı ediyoruz çünkü makinelerin hayatlarımızı yönetmesine izin veremeyiz.

399
00:24:01,120 --> 00:24:05,160
Ve sanırım burada solda farklı olan başka bir şeyden bahsetmem gerekiyor; belirsizlik

400
00:24:05,160 --> 00:24:10,080
değeri, yani bit sayısı, artık sadece olası eşleşmelerin sayısıyla gereksiz değil.

401
00:24:10,080 --> 00:24:16,520
Şimdi yukarı çekip 2 üzeri 8&#39;i hesaplarsak. 02, ki bu da 256&#39;nın,

402
00:24:16,520 --> 00:24:22,640
sanırım 259&#39;un biraz üzerinde, aslında bu kalıpla eşleşen toplam 526

403
00:24:22,640 --> 00:24:26,400
kelime olmasına rağmen, sahip olduğu belirsizlik miktarı, eşit derecede olası

404
00:24:26,400 --> 00:24:29,760
259 kelime olsaydı ne olacağına daha çok benziyor. sonuçlar.

405
00:24:29,760 --> 00:24:31,100
Bunu şöyle düşünebilirsiniz.

406
00:24:31,100 --> 00:24:35,560
Borx&#39;un cevap olmadığını biliyor, yorts, zorl ve zorus için de

407
00:24:35,560 --> 00:24:37,840
aynısı geçerli, dolayısıyla önceki duruma göre biraz daha az belirsiz.

408
00:24:37,840 --> 00:24:40,220
Bu bit sayısı daha az olacaktır.

409
00:24:40,220 --> 00:24:44,040
Ve eğer oyunu oynamaya devam edersem, burada açıklamak

410
00:24:44,040 --> 00:24:48,680
istediğim şeye uygun birkaç tahminle bunu detaylandıracağım.

411
00:24:48,680 --> 00:24:52,520
Dördüncü tahmine göre, eğer en çok tercih edilenlere

412
00:24:52,520 --> 00:24:53,800
bakarsanız, bunun artık sadece entropiyi maksimuma çıkarmadığını görebilirsiniz.

413
00:24:53,800 --> 00:24:58,480
Yani bu noktada teknik olarak yedi olasılık var ama

414
00:24:58,480 --> 00:25:00,780
anlamlı şansı olan tek şey yurtlar ve kelimeler.

415
00:25:00,780 --> 00:25:04,760
Ve her ikisini de seçmenin bu diğer değerlerin üzerinde

416
00:25:04,760 --> 00:25:07,560
yer aldığını, açıkçası daha fazla bilgi vereceğini görebilirsiniz.

417
00:25:07,560 --> 00:25:11,200
Bunu ilk yaptığımda, her tahminin kalitesini ölçmek için bu iki sayıyı

418
00:25:11,200 --> 00:25:14,580
topladım ve bu aslında tahmin edebileceğinizden daha iyi işe yaradı.

419
00:25:14,580 --> 00:25:17,600
Ama bu pek sistematik gelmedi ve eminim ki insanların benimseyebileceği

420
00:25:17,600 --> 00:25:19,880
başka yaklaşımlar da vardır, ama ben bu yaklaşıma ulaştım.

421
00:25:19,880 --> 00:25:24,200
Bir sonraki tahmin olasılığını düşünürsek, bu durumda kelimeler gibi,

422
00:25:24,200 --> 00:25:28,440
gerçekten umursadığımız şey, eğer bunu yaparsak oyunumuzun beklenen puanıdır.

423
00:25:28,440 --> 00:25:32,880
Beklenen puanı hesaplamak için de kelimelerin gerçek cevap olma olasılığının

424
00:25:32,880 --> 00:25:35,640
ne olduğunu söylüyoruz ki bu şu anda %58&#39;i açıklıyor.

425
00:25:36,080 --> 00:25:40,400
Bu maçta puanımızın %58 ihtimalle 4 olacağını söylüyoruz.

426
00:25:40,400 --> 00:25:46,240
Ve 1 eksi %58 olasılıkla puanımız 4&#39;ten fazla olacaktır.

427
00:25:46,240 --> 00:25:50,640
Daha ne kadarını bilmiyoruz ama o noktaya geldiğimizde ne

428
00:25:50,640 --> 00:25:52,920
kadar belirsizliğin ortaya çıkabileceğine dayanarak bunu tahmin edebiliriz.

429
00:25:52,920 --> 00:25:56,600
Özellikle şu anda 1 tane var. 44 bit belirsizlik.

430
00:25:56,600 --> 00:26:01,560
Kelimeleri tahmin edersek, bu bize alacağımız beklenen bilginin 1 olduğunu söyler. 27 bit.

431
00:26:01,560 --> 00:26:06,280
Yani kelimeleri tahmin edersek, bu fark, bu olay gerçekleştikten

432
00:26:06,280 --> 00:26:08,280
sonra ne kadar belirsizlikle baş başa kalacağımızı temsil ediyor.

433
00:26:08,280 --> 00:26:12,500
İhtiyacımız olan şey, burada f adını verdiğim, bu

434
00:26:12,500 --> 00:26:13,880
belirsizliği beklenen bir puanla ilişkilendiren bir tür fonksiyondur.

435
00:26:13,880 --> 00:26:18,040
Ve bunu gerçekleştirmenin yolu, botun 1. versiyonuna dayalı olarak

436
00:26:18,040 --> 00:26:23,920
önceki oyunlardan bir grup veriyi çizerek, çok ölçülebilir belirsizliklerle

437
00:26:23,920 --> 00:26:27,040
çeşitli noktalardan sonra gerçek puanın ne olduğunu söylemekti.

438
00:26:27,040 --> 00:26:31,120
Örneğin, buradaki veri noktaları 8 civarındaki bir değerin üzerinde duruyor. 8&#39;in

439
00:26:31,120 --> 00:26:36,840
olduğu bir noktadan sonra bazı oyunlar için 7 ya da öylesine diyorlar. 7 bitlik belirsizlik,

440
00:26:36,840 --> 00:26:39,340
nihai cevaba ulaşmak için iki tahmin yapılması gerekti.

441
00:26:39,340 --> 00:26:43,180
Diğer oyunlar için üç tahmin gerekiyordu, diğer oyunlar için ise dört tahmin gerekiyordu.

442
00:26:43,180 --> 00:26:46,920
Burada sola kayarsak, sıfırın üzerindeki tüm noktalar, ne zaman sıfır

443
00:26:46,920 --> 00:26:51,620
belirsizlik varsa, yani tek bir olasılık varsa, o zaman gereken

444
00:26:51,620 --> 00:26:55,000
tahmin sayısı her zaman sadece birdir, bu da güven vericidir.

445
00:26:55,000 --> 00:26:59,020
Ne zaman bir miktar belirsizlik olsa, yani

446
00:26:59,020 --> 00:27:02,360
esasen iki olasılığa bağlıysa bazen bir tahmin

447
00:27:02,360 --> 00:27:03,940
daha, bazen de iki tahmin daha gerekiyordu.

448
00:27:03,940 --> 00:27:05,980
Burada da böyle devam ediyor.

449
00:27:05,980 --> 00:27:11,020
Belki bu verileri görselleştirmenin biraz daha kolay bir yolu, bunları bir araya toplayıp ortalamalarını almaktır.

450
00:27:11,020 --> 00:27:15,940
Örneğin buradaki çubuk, bir miktar belirsizliğimizin olduğu tüm noktalar arasında ortalama

451
00:27:15,940 --> 00:27:22,420
olarak gereken yeni tahmin sayısının yaklaşık 1 olduğunu söylüyor. 5.

452
00:27:22,420 --> 00:27:25,920
Ve buradaki çubuk, tüm farklı oyunlar arasında bir noktada belirsizliğin

453
00:27:25,920 --> 00:27:30,480
dört bitin biraz üzerinde olduğunu söylüyor, bu da onu

454
00:27:30,480 --> 00:27:35,120
16 farklı olasılığa daraltmak gibi, o zaman ortalama olarak o

455
00:27:35,120 --> 00:27:36,240
noktadan itibaren ikiden biraz daha fazla tahmin gerektiriyor ileri.

456
00:27:36,240 --> 00:27:40,080
Ve buradan itibaren buna makul görünen bir fonksiyona uyacak bir regresyon yaptım.

457
00:27:40,080 --> 00:27:44,160
Ve unutmayın, bunları yapmanın asıl amacı, bir kelimeden ne kadar çok bilgi

458
00:27:44,160 --> 00:27:49,720
kazanırsak beklenen puanın o kadar düşük olacağı şeklindeki bu sezgiyi ölçebilmektir.

459
00:27:49,720 --> 00:27:54,380
Yani bu sürüm 2 olarak. 0&#39;a dönersek ve aynı simülasyon setini çalıştırırsak, 2315

460
00:27:54,380 --> 00:27:59,820
olası sözcük yanıtının tümüne karşı oynatırsak, bu nasıl olur?

461
00:27:59,820 --> 00:28:04,060
İlk versiyonumuzun aksine kesinlikle daha iyi, bu da güven verici.

462
00:28:04,060 --> 00:28:08,780
Tüm söylenen ve yapılan ortalama 3 civarındadır. 6, ilk versiyondan farklı olarak birkaç

463
00:28:08,780 --> 00:28:12,820
kez kaybettiği ve bu durumda altıdan fazlasını gerektirdiği durumlar olmasına rağmen.

464
00:28:12,820 --> 00:28:15,980
Muhtemelen bilgiyi en üst düzeye çıkarmak yerine hedefe

465
00:28:15,980 --> 00:28:18,980
ulaşmak için bu ödünleşimin yapıldığı zamanlar olduğu için.

466
00:28:18,980 --> 00:28:22,140
Peki 3&#39;ten daha iyisini yapabilir miyiz? 6?

467
00:28:22,140 --> 00:28:23,260
Kesinlikle yapabiliriz.

468
00:28:23,260 --> 00:28:27,120
Başlangıçta, kelime cevaplarının gerçek listesini modelini oluşturma

469
00:28:27,120 --> 00:28:29,980
biçimine dahil etmemenin çok eğlenceli olduğunu söylemiştim.

470
00:28:29,980 --> 00:28:35,180
Ancak bunu dahil edersek alabileceğim en iyi performans 3 civarındaydı. 43.

471
00:28:35,180 --> 00:28:39,520
Dolayısıyla, bu önceki dağılımı seçmek için kelime sıklığı verilerini kullanmaktan daha karmaşık hale gelmeye

472
00:28:39,520 --> 00:28:44,220
çalışırsak, bu 3. 43 muhtemelen bunda ne kadar başarılı olabileceğimizin veya

473
00:28:44,220 --> 00:28:46,360
en azından benim bunda ne kadar başarılı olabileceğimin maksimumunu veriyor.

474
00:28:46,360 --> 00:28:50,240
Bu en iyi performans aslında sadece burada bahsettiğim fikirleri

475
00:28:50,240 --> 00:28:53,400
kullanır, ancak biraz daha ileri gider, sanki beklenen bilgiyi

476
00:28:53,400 --> 00:28:55,660
tek bir adım yerine iki adım ileriye doğru arar.

477
00:28:55,660 --> 00:28:58,720
Başlangıçta bunun hakkında daha fazla konuşmayı planlıyordum ama aslında

478
00:28:58,720 --> 00:29:00,580
oldukça uzun bir yol kat ettiğimizi fark ettim.

479
00:29:00,580 --> 00:29:03,520
Söyleyeceğim tek şey, bu iki adımlı aramayı yaptıktan ve ardından en

480
00:29:03,520 --> 00:29:07,720
iyi adaylar üzerinde birkaç örnek simülasyon çalıştırdıktan sonra, şu ana kadar

481
00:29:07,720 --> 00:29:09,500
benim için en azından Crane&#39;in en iyi açıcı olduğu görünüyor.

482
00:29:09,500 --> 00:29:11,080
Kim tahmin ederdi?

483
00:29:11,080 --> 00:29:15,680
Ayrıca olasılıklar alanınızı belirlemek için gerçek sözcük listesini kullanırsanız,

484
00:29:15,680 --> 00:29:17,920
o zaman başlangıçtaki belirsizlik 11 bitin biraz üzerinde olur.

485
00:29:18,160 --> 00:29:22,760
Ve sadece kaba kuvvet aramasından sonra, ilk iki tahminden sonra

486
00:29:22,760 --> 00:29:26,580
beklenen maksimum olası bilginin 10 bit civarında olduğu ortaya çıktı.

487
00:29:26,580 --> 00:29:31,720
Bu da en iyi senaryoda, ilk iki tahmininizden sonra, mükemmel derecede

488
00:29:31,720 --> 00:29:35,220
optimal bir oyunla, yaklaşık bir miktar belirsizlikle baş başa kalacağınızı gösteriyor.

489
00:29:35,220 --> 00:29:37,400
Bu, iki olası tahminde bulunmakla aynı şeydir.

490
00:29:37,400 --> 00:29:41,440
Bu yüzden, bu ortalamayı 3&#39;e kadar düşüren bir algoritmayı asla yazamayacağınızı

491
00:29:41,440 --> 00:29:45,620
söylemek adil ve muhtemelen oldukça muhafazakar olur çünkü kullanabileceğiniz kelimelerle,

492
00:29:45,620 --> 00:29:50,460
sadece iki adımdan sonra yeterli bilgiyi elde etmek için yer

493
00:29:50,460 --> 00:29:53,820
yoktur. her seferinde üçüncü slottaki cevabı hatasız olarak garanti edebilir.

