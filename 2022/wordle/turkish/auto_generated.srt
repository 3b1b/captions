1
00:00:00,000 --> 00:00:03,303
Wurdle oyunu son bir iki ayda oldukça viral hale geldi ve hiçbir zaman

2
00:00:03,303 --> 00:00:06,001
bir matematik dersi fırsatını gözden kaçıran biri olmadı.

3
00:00:06,001 --> 00:00:09,211
Bana öyle geliyor ki bu oyun bilgi teorisi ile ilgili bir derste çok

4
00:00:09,211 --> 00:00:13,120
iyi bir merkezi örnek teşkil ediyor ve özellikle de entropi olarak bilinen bir konu.

5
00:00:13,120 --> 00:00:18,073
Görüyorsunuz, pek çok insan gibi ben de bulmacanın içine kapıldım ve birçok programcı

6
00:00:18,073 --> 00:00:23,200
gibi ben de oyunu olabildiğince optimum şekilde oynatacak bir algoritma yazmaya kapıldım.

7
00:00:23,200 --> 00:00:26,160
Ve burada yapmayı düşündüğüm şey, sizinle bu konudaki sürecimin

8
00:00:26,160 --> 00:00:29,721
bir kısmını konuşmak ve bunun içine giren matematiğin bir kısmını açıklamak,

9
00:00:29,721 --> 00:00:32,080
çünkü tüm algoritma bu entropi fikrine odaklanıyor.

10
00:00:32,080 --> 00:00:42,180
İlk olarak, duymadıysanız söyleyeyim, Wurdle nedir?

11
00:00:42,180 --> 00:00:45,495
Ve burada oyunun kurallarını incelerken bir taşla iki kuş vurmak için,

12
00:00:45,495 --> 00:00:48,017
bununla nereye gittiğimizi de ön izlememe izin verin,

13
00:00:48,017 --> 00:00:51,380
yani temelde oyunu bizim için oynayacak küçük bir algoritma geliştirmek.

14
00:00:51,380 --> 00:00:55,860
Bugünkü Wurdle&#39;ı yapmamış olsam da, bugün 4 Şubat ve botun nasıl yapacağını göreceğiz.

15
00:00:55,860 --> 00:00:58,319
Wurdle&#39;ın amacı, beş harfli gizemli bir kelimeyi tahmin

16
00:00:58,319 --> 00:01:00,860
etmektir ve size tahmin etmeniz için altı farklı şans verilir.

17
00:01:00,860 --> 00:01:05,240
Örneğin, Wurdle botum tahmin vinciyle başlamamı öneriyor.

18
00:01:05,240 --> 00:01:08,062
Her tahmin yaptığınızda, tahmininizin gerçek cevaba

19
00:01:08,062 --> 00:01:10,940
ne kadar yakın olduğuna dair bazı bilgiler alırsınız.

20
00:01:10,940 --> 00:01:14,540
Burada gri kutu bana asıl cevapta C&#39;nin olmadığını söylüyor.

21
00:01:14,540 --> 00:01:18,340
Sarı kutu bana bir R olduğunu söylüyor ama o konumda değil.

22
00:01:18,340 --> 00:01:20,677
Yeşil kutu bana gizli kelimenin A harfine sahip

23
00:01:20,677 --> 00:01:22,820
olduğunu ve üçüncü sırada olduğunu söylüyor.

24
00:01:22,820 --> 00:01:24,300
Ve sonra N yok ve E yok.

25
00:01:24,300 --> 00:01:27,420
O halde içeri girip Wurdle botuna bu bilgiyi söyleyeyim.

26
00:01:27,420 --> 00:01:31,500
Turnayla başladık, gri, sarı, yeşil, gri, gri elde ettik.

27
00:01:31,500 --> 00:01:33,884
Şu anda gösterdiği tüm veriler hakkında endişelenmeyin,

28
00:01:33,884 --> 00:01:35,460
bunları zamanı gelince açıklayacağım.

29
00:01:35,460 --> 00:01:39,700
Ancak ikinci seçimimiz için en önemli önerisi saçmalıktır.

30
00:01:39,700 --> 00:01:43,117
Ve tahmininizin gerçekten beş harfli bir kelime olması gerekiyor, ancak göreceğiniz gibi,

31
00:01:43,117 --> 00:01:45,700
aslında tahmin etmenize izin vereceği şey konusunda oldukça liberal.

32
00:01:45,700 --> 00:01:48,860
Bu durumda, saçmalamayı deneriz.

33
00:01:48,860 --> 00:01:50,260
Ve tamam, işler oldukça iyi görünüyor.

34
00:01:50,260 --> 00:01:54,740
S ve H&#39;ye basıyoruz, yani ilk üç harfi biliyoruz, bir R olduğunu biliyoruz.

35
00:01:54,740 --> 00:01:59,740
Ve böylece SHA bir R gibi olacak veya SHA R bir şey olacak.

36
00:01:59,740 --> 00:02:04,157
Görünüşe göre Wurdle botu bunun yalnızca iki olasılığa bağlı olduğunu biliyor;

37
00:02:04,157 --> 00:02:05,220
kırık ya da keskin.

38
00:02:05,220 --> 00:02:08,130
Bu noktada aralarında bir tür çekişme var, bu yüzden

39
00:02:08,130 --> 00:02:11,260
sanırım muhtemelen alfabetik olduğu için parçayla uyumlu.

40
00:02:11,260 --> 00:02:13,000
Hangi yaşasın, asıl cevap bu.

41
00:02:13,000 --> 00:02:14,660
Böylece üçe çıktık.

42
00:02:14,660 --> 00:02:17,761
Bunun iyi olup olmadığını merak ediyorsanız, bir kişiden duyduğuma göre

43
00:02:17,761 --> 00:02:20,820
Wurdle&#39;da dört eşit ve üç birdie&#39;dir şeklinde bir cümle duydum.

44
00:02:20,820 --> 00:02:22,960
Oldukça uygun bir benzetme olduğunu düşünüyorum.

45
00:02:22,960 --> 00:02:25,996
Dört puan almak için oyununuzda tutarlı bir şekilde çalışmalısınız,

46
00:02:25,996 --> 00:02:27,560
ancak bu kesinlikle çılgınca değil.

47
00:02:27,560 --> 00:02:30,000
Ama üçe böldüğünüzde harika hissettiriyor.

48
00:02:30,000 --> 00:02:33,322
Yani eğer istekliyseniz, burada yapmak istediğim şey Wurdle botuna nasıl

49
00:02:33,322 --> 00:02:36,600
yaklaştığım konusunda en başından itibaren düşünce sürecimden bahsetmek.

50
00:02:36,600 --> 00:02:39,800
Ve dediğim gibi bu aslında bilgi teorisi dersi için bir bahane.

51
00:02:39,800 --> 00:02:48,560
Temel amaç bilgi nedir, entropinin ne olduğunu açıklamaktır.

52
00:02:48,560 --> 00:02:51,085
Bu konuya yaklaşırken ilk düşüncem İngilizcedeki

53
00:02:51,085 --> 00:02:53,560
farklı harflerin göreceli sıklıklarına bakmaktı.

54
00:02:53,560 --> 00:02:56,641
Ben de düşündüm ki, tamam, bu en sık kullanılan harflerin çoğuna

55
00:02:56,641 --> 00:02:59,960
karşılık gelen bir açılış tahmini veya bir açılış tahmin çifti var mı?

56
00:02:59,960 --> 00:03:03,780
Ve oldukça hoşuma giden bir şey de diğerini ve ardından tırnakları yapmaktı.

57
00:03:03,780 --> 00:03:05,715
Buradaki düşünce şu ki, eğer bir harfe rastlarsanız,

58
00:03:05,715 --> 00:03:07,980
yeşil ya da sarı elde edersiniz, bu her zaman iyi hissettirir.

59
00:03:07,980 --> 00:03:09,460
Bilgi alıyormuşsunuz gibi geliyor.

60
00:03:09,460 --> 00:03:12,457
Ancak bu durumlarda, vurmasanız ve her zaman gri tonlar alsanız bile,

61
00:03:12,457 --> 00:03:15,241
bu size yine de birçok bilgi verir, çünkü bu harflerden herhangi

62
00:03:15,241 --> 00:03:17,640
birine sahip olmayan bir kelime bulmak oldukça nadirdir.

63
00:03:17,640 --> 00:03:20,606
Ancak yine de bu pek sistematik gelmiyor çünkü örneğin

64
00:03:20,606 --> 00:03:23,520
harflerin sırasını dikkate almanın hiçbir faydası yok.

65
00:03:23,520 --> 00:03:26,080
Salyangoz yazabilecekken neden çivi yazayım ki?

66
00:03:26,080 --> 00:03:27,720
Sonunda S olması daha mı iyi?

67
00:03:27,720 --> 00:03:28,720
Gerçekten emin değilim.

68
00:03:28,720 --> 00:03:32,533
Bir arkadaşım yorgun sözcüğüyle başlamayı sevdiğini söyledi,

69
00:03:32,533 --> 00:03:37,160
bu beni biraz şaşırttı çünkü içinde W ve Y gibi alışılmadık harfler vardı.

70
00:03:37,160 --> 00:03:39,400
Ama kim bilir, belki bu daha iyi bir açılıştır.

71
00:03:39,400 --> 00:03:42,105
Potansiyel bir tahminin kalitesini değerlendirmek

72
00:03:42,105 --> 00:03:44,920
için verebileceğimiz bir tür niceliksel puan var mı?

73
00:03:44,920 --> 00:03:48,385
Şimdi olası tahminleri sıralama yöntemimizi belirlemek için geriye

74
00:03:48,385 --> 00:03:51,800
dönüp oyunun tam olarak nasıl kurulduğuna biraz açıklık getirelim.

75
00:03:51,800 --> 00:03:54,714
Yani, geçerli tahminler olarak kabul edilen, girmenize izin

76
00:03:54,714 --> 00:03:57,920
verecek yaklaşık 13.000 kelime uzunluğunda bir kelime listesi var.

77
00:03:57,920 --> 00:04:02,958
Ama baktığınızda pek çok sıra dışı şey var; kafa ya da Ali ve ARG gibi şeyler,

78
00:04:02,958 --> 00:04:07,040
Scrabble oyununda aile tartışmalarına yol açan türden kelimeler.

79
00:04:07,040 --> 00:04:10,600
Ancak oyunun havası, cevabın her zaman oldukça yaygın bir kelime olacağıdır.

80
00:04:10,600 --> 00:04:16,080
Ve aslında olası yanıtlar olan yaklaşık 2300 kelimeden oluşan başka bir liste daha var.

81
00:04:16,080 --> 00:04:18,896
Ve bu, insanların hazırladığı bir liste, sanırım özellikle oyun

82
00:04:18,896 --> 00:04:21,800
yaratıcısının kız arkadaşı tarafından, ki bu da oldukça eğlenceli.

83
00:04:21,800 --> 00:04:24,679
Ancak yapmak istediğim şey, bu projedeki amacımız,

84
00:04:24,679 --> 00:04:27,614
bu listeyle ilgili önceki bilgileri birleştirmeyen,

85
00:04:27,614 --> 00:04:30,720
Wordle çözen bir program yazıp yazamayacağımızı görmek.

86
00:04:30,720 --> 00:04:35,560
Öncelikle, bu listede bulamayacağınız pek çok yaygın beş harfli kelime var.

87
00:04:35,560 --> 00:04:38,781
Bu nedenle, biraz daha dayanıklı olan ve sadece resmi web sitesine değil,

88
00:04:38,781 --> 00:04:41,960
herkese karşı Wordle oynayabilecek bir program yazmak daha iyi olacaktır.

89
00:04:41,960 --> 00:04:45,423
Ayrıca bu olası yanıtlar listesinin ne olduğunu bilmemizin nedeni,

90
00:04:45,423 --> 00:04:47,440
bunun kaynak kodunda görünür olmasıdır.

91
00:04:47,440 --> 00:04:52,840
Ancak kaynak kodunda görünme şekli, cevapların günden güne ortaya çıkma sırasına göredir.

92
00:04:52,840 --> 00:04:56,400
Böylece her zaman yarının cevabının ne olacağına bakabilirsiniz.

93
00:04:56,400 --> 00:04:59,140
Açıkça görülüyor ki, listeyi kullanmanın hile yapmak olduğu bir anlam taşıyor.

94
00:04:59,140 --> 00:05:04,010
Ve daha ilginç bir bulmacayı ve daha zengin bir bilgi teorisi dersini ortaya çıkaran şey,

95
00:05:04,010 --> 00:05:07,960
daha yaygın sözcükleri tercih etme sezgisini yakalamak için genel olarak

96
00:05:07,960 --> 00:05:11,640
göreceli sözcük sıklıkları gibi daha evrensel verileri kullanmaktır.

97
00:05:11,640 --> 00:05:16,560
Peki bu 13.000 olasılık arasından açılış tahminini nasıl seçmeliyiz?

98
00:05:16,560 --> 00:05:19,960
Mesela arkadaşım bıkkınlık teklif ediyorsa kalitesini nasıl analiz etmeliyiz?

99
00:05:19,960 --> 00:05:23,217
Pek olası olmayan W&#39;yi sevdiğini söylemesinin nedeni,

100
00:05:23,217 --> 00:05:27,880
o W&#39;ye vurmanın ne kadar iyi hissettireceğinin uzun vadede doğasını sevmesidir.

101
00:05:27,880 --> 00:05:31,286
Örneğin, ortaya çıkan ilk kalıp bunun gibi bir şeyse,

102
00:05:31,286 --> 00:05:36,080
bu dev sözlükte bu kalıpla eşleşen yalnızca 58 kelime olduğu ortaya çıkıyor.

103
00:05:36,080 --> 00:05:38,900
Yani bu 13.000&#39;den çok büyük bir azalma.

104
00:05:38,900 --> 00:05:43,360
Ancak bunun diğer tarafı da elbette ki böyle bir model elde etmenin çok nadir olmasıdır.

105
00:05:43,360 --> 00:05:48,134
Spesifik olarak, her kelimenin cevap olma olasılığı eşit olsaydı,

106
00:05:48,134 --> 00:05:51,680
bu kalıba ulaşma olasılığı 58 bölü 13.000 olurdu.

107
00:05:51,680 --> 00:05:53,880
Elbette bunların cevap olma ihtimali eşit değil.

108
00:05:53,880 --> 00:05:56,680
Bunların çoğu çok belirsiz ve hatta şüpheli kelimelerdir.

109
00:05:56,680 --> 00:05:59,187
Ama en azından tüm bunlara ilk geçişimizde, hepsinin eşit

110
00:05:59,187 --> 00:06:02,040
derecede olası olduğunu varsayalım ve bunu biraz sonra düzeltelim.

111
00:06:02,040 --> 00:06:04,727
Mesele şu ki, çok fazla bilgi içeren bir modelin

112
00:06:04,727 --> 00:06:07,360
doğası gereği ortaya çıkması pek olası değildir.

113
00:06:07,360 --> 00:06:11,920
Aslında bilgilendirici olmanın anlamı bunun olası olmadığıdır.

114
00:06:11,920 --> 00:06:16,537
Bu açılışta görülecek çok daha olası bir model bunun gibi bir şey olabilir,

115
00:06:16,537 --> 00:06:18,360
burada elbette W harfi yoktur.

116
00:06:18,360 --> 00:06:22,080
Belki bir E vardır ve belki A yoktur, R yoktur, Y yoktur.

117
00:06:22,080 --> 00:06:24,640
Bu durumda 1400 olası eşleşme vardır.

118
00:06:24,640 --> 00:06:30,680
Hepsi eşit olasılıkta olsaydı, göreceğiniz modelin bu olma olasılığı yaklaşık %11 olurdu.

119
00:06:30,680 --> 00:06:34,320
Dolayısıyla en olası sonuçlar aynı zamanda en az bilgilendirici olanlardır.

120
00:06:34,320 --> 00:06:37,013
Burada daha küresel bir bakış elde etmek için,

121
00:06:37,013 --> 00:06:42,000
görebileceğiniz tüm farklı kalıplara göre olasılıkların tam dağılımını size göstereyim.

122
00:06:42,000 --> 00:06:46,987
Yani baktığınız her çubuk, ortaya çıkabilecek olası bir renk düzenine karşılık gelir,

123
00:06:46,987 --> 00:06:50,408
bunlardan 3 üzeri 5 olasılık vardır ve bunlar soldan sağa,

124
00:06:50,408 --> 00:06:52,960
en yaygından en az yaygına doğru düzenlenir.

125
00:06:52,960 --> 00:06:56,200
Yani buradaki en yaygın olasılık, tüm grileri elde etmenizdir.

126
00:06:56,200 --> 00:06:58,800
Bu, zamanın yaklaşık %14&#39;ünde gerçekleşir.

127
00:06:58,800 --> 00:07:02,293
Ve bir tahmin yaptığınızda umduğunuz şey, kendinizi bu uzun

128
00:07:02,293 --> 00:07:05,320
kuyrukta bir yerde bulmanızdır; burada olduğu gibi,

129
00:07:05,320 --> 00:07:09,920
açıkça buna benzeyen bu kalıba uyan şey için yalnızca 18 olasılığın olduğu yer.

130
00:07:09,920 --> 00:07:14,080
Ya da biraz daha sola gidersek belki buraya kadar gidebiliriz.

131
00:07:14,080 --> 00:07:16,560
Tamam, işte sana güzel bir bulmaca.

132
00:07:16,560 --> 00:07:19,300
İngilizce dilinde W ile başlayan, Y ile biten

133
00:07:19,300 --> 00:07:22,040
ve içinde bir yerde R bulunan üç kelime nedir?

134
00:07:22,040 --> 00:07:27,560
Cevapların, bakalım, uzun uzun, kurtlu ve alaycı olduğu ortaya çıktı.

135
00:07:27,560 --> 00:07:31,626
Bu kelimenin genel olarak ne kadar iyi olduğuna karar vermek için,

136
00:07:31,626 --> 00:07:36,360
bu dağılımdan alacağınız beklenen bilgi miktarının bir tür ölçüsünü istiyoruz.

137
00:07:36,360 --> 00:07:41,300
Her bir modeli incelersek ve onun gerçekleşme olasılığını ne kadar bilgilendirici

138
00:07:41,300 --> 00:07:46,000
olduğunu ölçen bir şeyle çarparsak, bu bize belki objektif bir puan verebilir.

139
00:07:46,000 --> 00:07:50,280
Şimdi bir şeyin ne olması gerektiğine dair ilk içgüdünüz eşleşme sayısı olabilir.

140
00:07:50,280 --> 00:07:52,960
Daha düşük bir ortalama eşleşme sayısı istiyorsunuz.

141
00:07:52,960 --> 00:07:58,794
Ancak bunun yerine, genellikle bilgiye atfettiğimiz daha evrensel bir ölçüm kullanmak

142
00:07:58,794 --> 00:08:04,629
istiyorum ve bu 13.000 kelimenin her birine, bunların gerçekten cevap olup olmadığına

143
00:08:04,629 --> 00:08:10,600
ilişkin farklı bir olasılık atandığında daha esnek olacak bir ölçüm kullanmak istiyorum.

144
00:08:10,600 --> 00:08:14,689
Standart bilgi birimi, biraz komik bir formüle sahip olan bit&#39;tir,

145
00:08:14,689 --> 00:08:17,800
ancak sadece örneklere bakarsak gerçekten sezgiseldir.

146
00:08:17,800 --> 00:08:21,695
Olasılık alanınızı yarıya indiren bir gözleminiz varsa,

147
00:08:21,695 --> 00:08:24,200
onun bir bit bilgisi vardır diyoruz.

148
00:08:24,200 --> 00:08:27,714
Örneğimizde, olasılıklar uzayı tüm olası kelimelerden oluşuyor ve ortaya çıkıyor ki,

149
00:08:27,714 --> 00:08:29,988
beş harfli kelimelerin yaklaşık yarısının S harfi var,

150
00:08:29,988 --> 00:08:31,560
bundan biraz daha az ama yarısı kadar.

151
00:08:31,560 --> 00:08:35,200
Yani bu gözlem size biraz bilgi verecektir.

152
00:08:35,200 --> 00:08:39,233
Bunun yerine yeni bir olgu bu olasılıklar alanını dört kat azaltırsa,

153
00:08:39,233 --> 00:08:42,000
onun iki bitlik bilgiye sahip olduğunu söyleriz.

154
00:08:42,000 --> 00:08:45,120
Örneğin, bu kelimelerin yaklaşık dörtte birinde T harfinin olduğu ortaya çıktı.

155
00:08:45,120 --> 00:08:47,937
Eğer gözlem bu alanı sekiz kat azaltırsa, bunun üç

156
00:08:47,937 --> 00:08:50,920
bitlik bilgi olduğunu söyleriz ve bu böyle devam eder.

157
00:08:50,920 --> 00:08:55,000
Dört bit onu 16&#39;ya, beş bit ise 32&#39;ye böler.

158
00:08:55,000 --> 00:08:58,598
Şimdi durup kendinize şu soruyu sorabilirsiniz:

159
00:08:58,598 --> 00:09:04,520
Bir olayın gerçekleşme olasılığı açısından bit sayısı bilgisinin formülü nedir?

160
00:09:04,520 --> 00:09:09,727
Burada söylediğimiz şey, bit sayısının yarısını aldığınızda, bu olasılık ile aynı şeydir;

161
00:09:09,727 --> 00:09:14,414
bu, bit sayısının iki üssünün bir bölü olasılık olduğunu söylemekle aynı şeydir;

162
00:09:14,414 --> 00:09:19,332
Bilginin log tabanının iki bölü olasılığa eşit olduğunu söyleyerek yeniden düzenleme

163
00:09:19,332 --> 00:09:19,680
yapar.

164
00:09:19,680 --> 00:09:22,707
Ve bazen bunu bir yeniden düzenlemeyle daha görürsünüz;

165
00:09:22,707 --> 00:09:25,680
burada bilgi, olasılığın negatif logaritması tabanıdır.

166
00:09:25,680 --> 00:09:29,998
Bu şekilde ifade edilirse, bu konuya yeni başlayan biri için biraz tuhaf görünebilir,

167
00:09:29,998 --> 00:09:33,211
ancak aslında bu, olasılıklarınızı kaç kez yarıya indirdiğinizi

168
00:09:33,211 --> 00:09:35,120
sormak gibi çok sezgisel bir fikirdir.

169
00:09:35,120 --> 00:09:38,420
Şimdi merak ediyorsanız, eğlenceli bir kelime oyunu oynadığımızı sanıyordum,

170
00:09:38,420 --> 00:09:39,920
neden logaritmalar devreye giriyor?

171
00:09:39,920 --> 00:09:42,427
Bunun daha güzel bir birim olmasının bir nedeni,

172
00:09:42,427 --> 00:09:46,111
pek olası olmayan olaylar hakkında konuşmanın çok daha kolay olmasıdır;

173
00:09:46,111 --> 00:09:48,977
bir gözlemin 20 bitlik bilgiye sahip olduğunu söylemek,

174
00:09:48,977 --> 00:09:53,480
şunun şunun meydana gelme olasılığının 0 olduğunu söylemekten çok daha kolaydır.0000095.

175
00:09:53,480 --> 00:09:57,793
Ancak bu logaritmik ifadenin olasılık teorisine çok yararlı bir katkı olduğunun

176
00:09:57,793 --> 00:10:02,000
ortaya çıkmasının daha önemli bir nedeni, bilgilerin bir araya gelme şeklidir.

177
00:10:02,000 --> 00:10:04,807
Örneğin, bir gözlem size iki bitlik bilgi verirse,

178
00:10:04,807 --> 00:10:08,496
alanınızı dört katına çıkarırsa ve ardından Wordle&#39;deki ikinci

179
00:10:08,496 --> 00:10:12,074
tahmininiz gibi ikinci bir gözlem size başka bir üç bitlik bilgi

180
00:10:12,074 --> 00:10:14,992
verirse ve sizi başka bir sekiz kat daha küçültürse,

181
00:10:14,992 --> 00:10:17,360
ikisi birlikte size beş bitlik bilgi verir.

182
00:10:17,360 --> 00:10:21,200
Olasılıklar çoğalmayı sevdiği gibi, bilgi de eklemeyi sever.

183
00:10:21,200 --> 00:10:24,733
Dolayısıyla, bir dizi sayıyı topladığımız beklenen değer gibi bir şeyin

184
00:10:24,733 --> 00:10:28,660
alanına girdiğimizde, günlükler bununla uğraşmayı çok daha güzel hale getiriyor.

185
00:10:28,660 --> 00:10:32,084
Weary dağıtımımıza geri dönelim ve buraya her model için ne kadar

186
00:10:32,084 --> 00:10:35,560
bilgi bulunduğunu bize gösteren başka bir küçük izleyici ekleyelim.

187
00:10:35,560 --> 00:10:40,486
Fark etmenizi istediğim asıl şey, daha olası modellere ulaşma olasılığımız arttıkça,

188
00:10:40,486 --> 00:10:43,500
bilgi ne kadar düşükse, o kadar az bit kazanırsınız.

189
00:10:43,500 --> 00:10:48,024
Bu tahminin kalitesini ölçmemizin yolu, bu bilginin beklenen değerini almak olacaktır;

190
00:10:48,024 --> 00:10:51,560
burada her bir modeli inceliyoruz, bunun ne kadar muhtemel olduğunu

191
00:10:51,560 --> 00:10:54,940
söylüyoruz ve sonra bunu kaç bitlik bilgi aldığımızla çarpıyoruz.

192
00:10:54,940 --> 00:10:58,480
Ve Weary örneğinde bunun 4 olduğu ortaya çıkıyor.9 bit.

193
00:10:58,480 --> 00:11:01,930
Yani ortalama olarak, bu açılış tahmininden alacağınız bilgi,

194
00:11:01,930 --> 00:11:05,660
olasılıklar alanınızı yaklaşık beş kez yarıya indirmeye eşdeğerdir.

195
00:11:05,660 --> 00:11:09,364
Bunun tersine, beklenen bilgi değeri daha yüksek

196
00:11:09,364 --> 00:11:13,220
olan bir tahmin örneği Slate gibi bir şey olabilir.

197
00:11:13,220 --> 00:11:16,180
Bu durumda dağılımın çok daha düz göründüğünü fark edeceksiniz.

198
00:11:16,180 --> 00:11:20,900
Özellikle, tüm grilerin en muhtemel oluşumu yalnızca %6 civarında meydana

199
00:11:20,900 --> 00:11:25,940
gelme şansına sahiptir, yani en azından açıkça 3 elde edersiniz.9 bitlik bilgi.

200
00:11:25,940 --> 00:11:29,140
Ancak bu minimumdur, daha genel olarak bundan daha iyi bir şey elde edersiniz.

201
00:11:29,140 --> 00:11:32,362
Ve buradaki rakamları hesaplayıp ilgili tüm terimleri

202
00:11:32,362 --> 00:11:36,420
topladığınızda ortalama bilginin yaklaşık 5 olduğu ortaya çıkıyor.8.

203
00:11:36,420 --> 00:11:39,985
Yani Weary&#39;nin aksine, olasılıklar alanınız bu ilk

204
00:11:39,985 --> 00:11:43,940
tahminden sonra ortalama olarak yarısı kadar büyük olacaktır.

205
00:11:43,940 --> 00:11:49,540
Bilgi miktarının bu beklenen değerinin adı hakkında aslında eğlenceli bir hikaye var.

206
00:11:49,540 --> 00:11:53,337
Bilgi teorisi, 1940&#39;larda Bell Laboratuarlarında çalışan Claude Shannon

207
00:11:53,337 --> 00:11:56,884
tarafından geliştirildi, ancak henüz yayınlanmamış bazı fikirlerinden,

208
00:11:56,884 --> 00:12:00,732
zamanın entelektüel devi, çok öne çıkan John von Neumann&#39;la konuşuyordu.

209
00:12:00,732 --> 00:12:04,180
matematik ve fizikte ve bilgisayar bilimine dönüşen şeyin başlangıcı.

210
00:12:04,180 --> 00:12:07,558
Ve von Neumann, bilgi miktarının bu beklenen değeri için gerçekten

211
00:12:07,558 --> 00:12:10,735
iyi bir isme sahip olmadığını söylediğinde, söylendiğine göre,

212
00:12:10,735 --> 00:12:14,720
hikaye şöyle devam ediyor, buna entropi diyebilirsiniz ve bunun iki nedeni var.

213
00:12:14,720 --> 00:12:18,761
İlk olarak, belirsizlik fonksiyonunuz istatistiksel mekanikte bu isimle kullanıldı,

214
00:12:18,761 --> 00:12:22,032
dolayısıyla zaten bir adı var ve ikinci olarak ve daha da önemlisi,

215
00:12:22,032 --> 00:12:24,534
hiç kimse entropinin gerçekte ne olduğunu bilmiyor,

216
00:12:24,534 --> 00:12:26,940
dolayısıyla bir tartışmada her zaman avantajı var.

217
00:12:26,940 --> 00:12:31,482
Yani eğer isim biraz gizemli görünüyorsa ve eğer bu hikayeye inanılacaksa,

218
00:12:31,482 --> 00:12:33,420
bu bir bakıma tasarım gereğidir.

219
00:12:33,420 --> 00:12:37,783
Ayrıca, eğer bunun fizikteki termodinamiğin ikinci yasasıyla ilgili tüm o şeylerle

220
00:12:37,783 --> 00:12:40,884
ilişkisini merak ediyorsanız, kesinlikle bir bağlantı var,

221
00:12:40,884 --> 00:12:45,037
ama kökeninde Shannon sadece saf olasılık teorisiyle ilgileniyordu ve buradaki

222
00:12:45,037 --> 00:12:49,190
amaçlarımız için, Kelime entropisi, sadece belirli bir tahminin beklenen bilgi

223
00:12:49,190 --> 00:12:50,820
değerini düşünmenizi istiyorum.

224
00:12:50,820 --> 00:12:54,380
Entropiyi iki şeyin aynı anda ölçülmesi olarak düşünebilirsiniz.

225
00:12:54,380 --> 00:12:57,420
Bunlardan ilki dağılımın ne kadar düz olduğudur.

226
00:12:57,420 --> 00:13:01,700
Dağılım düzgünlüğe ne kadar yakınsa entropi o kadar yüksek olur.

227
00:13:01,700 --> 00:13:05,698
Bizim durumumuzda, 3 üzeri 5&#39;lik toplam örüntülerin olduğu durumda,

228
00:13:05,698 --> 00:13:09,419
düzgün bir dağılım için, bunlardan herhangi birinin gözlemlenmesi,

229
00:13:09,419 --> 00:13:13,028
3 üzeri 5&#39;lik bilgi günlüğü tabanı 2&#39;ye sahip olacaktır;

230
00:13:13,028 --> 00:13:17,860
bu da 7 olur.92, yani bu entropi için sahip olabileceğiniz mutlak maksimum değer budur.

231
00:13:17,860 --> 00:13:22,900
Ancak entropi aynı zamanda ilk etapta ne kadar olasılığın bulunduğunun da bir ölçüsüdür.

232
00:13:22,900 --> 00:13:27,637
Örneğin, yalnızca 16 olası örüntünün olduğu ve her birinin eşit olasılığa

233
00:13:27,637 --> 00:13:32,760
sahip olduğu bir kelimeniz varsa, bu entropi, bu beklenen bilgi 4 bit olacaktır.

234
00:13:32,760 --> 00:13:37,088
Ancak 64 olası örüntünün ortaya çıkabileceği başka bir kelimeniz varsa ve bunların

235
00:13:37,088 --> 00:13:41,000
hepsi eşit derecede olasıysa, o zaman entropi 6 bit olarak hesaplanacaktır.

236
00:13:41,000 --> 00:13:45,733
Yani, eğer doğada 6 bitlik bir entropiye sahip bir dağılım görürseniz,

237
00:13:45,733 --> 00:13:48,866
bu sanki 64 eşit olasılıklı sonuç varmış gibi,

238
00:13:48,866 --> 00:13:54,400
olacaklar konusunda çok fazla değişkenlik ve belirsizlik olduğunu söylemek gibidir.

239
00:13:54,400 --> 00:13:58,360
Wurtelebot&#39;a ilk geçişimde temelde bunu yapmasını sağladım.

240
00:13:58,360 --> 00:14:03,348
Yapabileceğiniz tüm olası tahminleri, yani 13.000 kelimenin tamamını gözden geçirir,

241
00:14:03,348 --> 00:14:06,342
her biri için entropiyi veya daha spesifik olarak,

242
00:14:06,342 --> 00:14:10,802
her biri için görebileceğiniz tüm kalıplar arasındaki dağılımın entropisini

243
00:14:10,802 --> 00:14:15,321
hesaplar ve en yüksek olanı seçer, çünkü bu Olasılık alanınızı mümkün olduğu

244
00:14:15,321 --> 00:14:17,200
kadar daraltması muhtemel olanı.

245
00:14:17,200 --> 00:14:19,416
Burada sadece ilk tahminden bahsetmiş olsam da

246
00:14:19,416 --> 00:14:21,680
sonraki birkaç tahmin için de aynı şeyi yapıyor.

247
00:14:21,680 --> 00:14:25,147
Örneğin, ilk tahminde, hangi kelimeyle eşleştiğine bağlı olarak

248
00:14:25,147 --> 00:14:29,265
sizi daha az sayıda olası kelimeyle sınırlayacak bir model gördükten sonra,

249
00:14:29,265 --> 00:14:32,300
aynı oyunu o daha küçük kelime kümesine göre oynarsınız.

250
00:14:32,300 --> 00:14:36,649
Önerilen ikinci bir tahmin için, bu daha kısıtlı kelime grubundan

251
00:14:36,649 --> 00:14:40,010
oluşabilecek tüm kalıpların dağılımına bakarsınız,

252
00:14:40,010 --> 00:14:45,480
13.000 olasılığın tamamını ararsınız ve bu entropiyi maksimuma çıkaranı bulursunuz.

253
00:14:45,480 --> 00:14:50,186
Bunun nasıl çalıştığını size göstermek için, kenarlarda bu analizin önemli noktalarını

254
00:14:50,186 --> 00:14:54,460
gösteren, yazdığım Wurtele&#39;nin küçük bir versiyonunu ele almama izin verin.

255
00:14:54,460 --> 00:14:57,284
Tüm entropi hesaplamalarını yaptıktan sonra sağ tarafta bize

256
00:14:57,284 --> 00:15:00,340
hangilerinin en yüksek beklenen bilgiye sahip olduğunu gösteriyor.

257
00:15:00,340 --> 00:15:06,391
En azından şimdilik en önemli cevabın Tares olduğu ortaya çıktı,

258
00:15:06,391 --> 00:15:11,140
bu da tabii ki fiğ, en yaygın fiğ anlamına geliyor.

259
00:15:11,140 --> 00:15:14,610
Burada her tahmin yaptığımızda, belki de önerilerini göz ardı edip slate&#39;i

260
00:15:14,610 --> 00:15:17,862
tercih ederim, çünkü slate&#39;i severim, ne kadar beklenen bilgiye sahip

261
00:15:17,862 --> 00:15:21,465
olduğunu görebiliriz, ancak burada kelimenin sağında bize ne kadar bilgi olduğunu

262
00:15:21,465 --> 00:15:24,980
gösteriyor. Bu özel model göz önüne alındığında, elde ettiğimiz gerçek bilgiler.

263
00:15:24,980 --> 00:15:28,378
Yani burada biraz şanssızız gibi görünüyor, 5 almamız bekleniyordu.8,

264
00:15:28,378 --> 00:15:30,660
ama bundan daha azına sahip bir şey elde ettik.

265
00:15:30,660 --> 00:15:33,179
Ve sol tarafta, şu anda bulunduğumuz yere göre

266
00:15:33,179 --> 00:15:35,860
bize mümkün olan tüm farklı kelimeleri gösteriyor.

267
00:15:35,860 --> 00:15:39,247
Mavi çubuklar bize her kelimenin ne kadar olası olduğunu düşündüğünü gösteriyor;

268
00:15:39,247 --> 00:15:42,760
dolayısıyla şu anda her kelimenin eşit derecede gerçekleşme olasılığını varsayıyor,

269
00:15:42,760 --> 00:15:44,140
ancak bunu birazdan düzelteceğiz.

270
00:15:44,140 --> 00:15:48,130
Ve bu belirsizlik ölçümü bize bu dağılımın olası kelimeler arasındaki

271
00:15:48,130 --> 00:15:51,892
entropisini anlatıyor; bu şu anda tekdüze bir dağılım olduğu için

272
00:15:51,892 --> 00:15:55,940
olasılıkların sayısını saymanın gereksiz derecede karmaşık bir yoludur.

273
00:15:55,940 --> 00:16:02,700
Örneğin 2 üssü 13&#39;ü alırsak.66, bu 13.000 olasılık civarında olmalı.

274
00:16:02,700 --> 00:16:06,780
Burada biraz yanlışım var ama bunun nedeni ondalık basamakların tamamını göstermemem.

275
00:16:06,780 --> 00:16:09,696
Şu anda bu size gereksiz gelebilir ve işleri aşırı derecede karmaşık hale getirebilir,

276
00:16:09,696 --> 00:16:12,344
ancak bir dakika içinde her iki sayıya da sahip olmanın neden yararlı olduğunu

277
00:16:12,344 --> 00:16:12,780
göreceksiniz.

278
00:16:12,780 --> 00:16:16,366
Yani burada ikinci tahminimiz için en yüksek entropinin Ramen olduğunu

279
00:16:16,366 --> 00:16:19,700
öne sürüyor gibi görünüyor ki bu da yine tek kelime gibi gelmiyor.

280
00:16:19,700 --> 00:16:25,660
Burada ahlaki açıdan yüksek bir yer edinmek için, devam edip Rains yazacağım.

281
00:16:25,660 --> 00:16:27,540
Ve yine biraz şanssızmışız gibi görünüyor.

282
00:16:27,540 --> 00:16:32,100
Biz 4 bekliyorduk.3 bit ve elimizde sadece 3 var.39 bit bilgi.

283
00:16:32,100 --> 00:16:35,060
Bu da bizi 55 olasılığa indiriyor.

284
00:16:35,060 --> 00:16:37,186
Ve burada belki de aslında onun önerdiği şeyle,

285
00:16:37,186 --> 00:16:40,200
yani birleşik olanla, her ne anlama geliyorsa onunla devam edeceğim.

286
00:16:40,200 --> 00:16:43,300
Ve tamam, bu aslında bir bulmaca için iyi bir şans.

287
00:16:43,300 --> 00:16:47,020
Bu modelin bize 4 verdiğini söylüyor.7 bitlik bilgi.

288
00:16:47,020 --> 00:16:52,400
Ama sol tarafta, bu modeli görmeden önce 5 tane vardı.78 bitlik belirsizlik.

289
00:16:52,400 --> 00:16:56,860
Peki sizin için bir test olarak, kalan olasılıkların sayısı ne anlama geliyor?

290
00:16:56,860 --> 00:17:01,398
Bu, bir miktar belirsizliğe indirgendiğimiz anlamına gelir ki bu,

291
00:17:01,398 --> 00:17:04,700
iki olası yanıt olduğunu söylemekle aynı şeydir.

292
00:17:04,700 --> 00:17:06,520
Bu 50-50&#39;lik bir seçim.

293
00:17:06,520 --> 00:17:08,817
Ve buradan yola çıkarak, sen ve ben hangi kelimelerin daha yaygın

294
00:17:08,817 --> 00:17:11,220
olduğunu bildiğimiz için cevabın uçurum olması gerektiğini biliyoruz.

295
00:17:11,220 --> 00:17:13,540
Ancak şu anda yazıldığı gibi, program bunu bilmiyor.

296
00:17:13,540 --> 00:17:17,009
Böylece tek bir olasılık kalana kadar mümkün olduğu kadar

297
00:17:17,009 --> 00:17:20,360
çok bilgi toplamaya devam eder ve sonra onu tahmin eder.

298
00:17:20,360 --> 00:17:22,700
Açıkçası daha iyi bir oyunsonu stratejisine ihtiyacımız var.

299
00:17:22,700 --> 00:17:26,692
Ancak diyelim ki bu sürüme kelime çözücülerimizden biri adını verdik ve

300
00:17:26,692 --> 00:17:30,740
sonra gidip nasıl çalıştığını görmek için bazı simülasyonlar çalıştırdık.

301
00:17:30,740 --> 00:17:34,240
Yani bunun çalışma şekli mümkün olan her kelime oyununu oynamaktır.

302
00:17:34,240 --> 00:17:38,780
Gerçek wordle cevapları olan 2315 kelimenin tamamının üzerinden geçiyor.

303
00:17:38,780 --> 00:17:41,340
Temel olarak bunu bir test seti olarak kullanıyor.

304
00:17:41,340 --> 00:17:44,305
Ve bir kelimenin ne kadar yaygın olduğunu dikkate almamak ve

305
00:17:44,305 --> 00:17:47,368
tek ve tek bir seçeneğe varıncaya kadar yol boyunca her adımda

306
00:17:47,368 --> 00:17:50,480
bilgiyi en üst düzeye çıkarmaya çalışmak gibi naif bir yöntemle.

307
00:17:50,480 --> 00:17:55,100
Simülasyonun sonunda ortalama puan 4 civarında çıkıyor.124.

308
00:17:55,100 --> 00:17:59,780
Ki bu fena değil, dürüst olmak gerekirse, daha kötüsünü bekliyordum.

309
00:17:59,780 --> 00:18:03,040
Ancak wordle oynayanlar size genellikle 4&#39;te alabileceklerini söyleyecektir.

310
00:18:03,040 --> 00:18:05,260
Asıl zorluk 3&#39;te mümkün olduğunca çok sayıda elde etmektir.

311
00:18:05,260 --> 00:18:08,920
4&#39;lük skor ile 3&#39;lük skor arasında oldukça büyük bir sıçrama var.

312
00:18:08,920 --> 00:18:16,040
Buradaki bariz düşük sonuç, bir kelimenin yaygın olup olmadığını

313
00:18:16,040 --> 00:18:23,160
ve bunu tam olarak nasıl yapacağımızı bir şekilde dahil etmektir.

314
00:18:23,160 --> 00:18:26,050
Benim yaklaşımım İngilizce dilindeki tüm kelimelerin

315
00:18:26,050 --> 00:18:28,560
göreceli frekanslarının bir listesini almaktı.

316
00:18:28,560 --> 00:18:32,091
Ve az önce Mathematica&#39;nın Google Kitaplar İngilizce Ngram genel

317
00:18:32,091 --> 00:18:35,520
veri kümesinden alınan kelime frekansı veri fonksiyonunu kullandım.

318
00:18:35,520 --> 00:18:37,694
Ve buna bakmak oldukça eğlenceli, örneğin en yaygın

319
00:18:37,694 --> 00:18:40,120
sözcüklerden en az kullanılan sözcüklere doğru sıralarsak.

320
00:18:40,120 --> 00:18:43,740
Açıkçası bunlar İngilizce dilinde en yaygın 5 harfli kelimelerdir.

321
00:18:43,740 --> 00:18:46,480
Daha doğrusu bunlar en yaygın 8&#39;incisidir.

322
00:18:46,480 --> 00:18:49,440
İlki hangisi, sonrasında orası ve orası var.

323
00:18:49,440 --> 00:18:52,458
Birincinin kendisi birinci değil, 9&#39;uncudur ve bu diğer

324
00:18:52,458 --> 00:18:55,628
kelimelerin daha sık ortaya çıkabileceği, ilk gelenlerin sonra

325
00:18:55,628 --> 00:18:59,000
olduğu ve nerede olduğu ve biraz daha az yaygın olduğu mantıklıdır.

326
00:18:59,000 --> 00:19:03,203
Şimdi, bu verileri, bu kelimelerin her birinin nihai cevap olma olasılığını

327
00:19:03,203 --> 00:19:07,020
modellemek için kullanırken, bu sadece sıklıkla orantılı olmamalıdır.

328
00:19:07,020 --> 00:19:10,622
Örneğin 0 puan verilir.Bu veri setinde 002 var,

329
00:19:10,622 --> 00:19:15,200
oysa örgü kelimesinin olasılığı bir anlamda 1000 kat daha az.

330
00:19:15,200 --> 00:19:17,181
Ancak bunların her ikisi de, neredeyse kesinlikle

331
00:19:17,181 --> 00:19:19,400
dikkate alınmaya değer olacak kadar yaygın kelimelerdir.

332
00:19:19,400 --> 00:19:21,900
Bu yüzden daha fazla ikili kesinti istiyoruz.

333
00:19:21,900 --> 00:19:25,411
Bu konuda izlediğim yol, tüm bu sıralanmış kelime listesini alıp,

334
00:19:25,411 --> 00:19:29,561
bunu bir x eksenine göre düzenlediğimi ve ardından çıktısı temelde ikili olan

335
00:19:29,561 --> 00:19:33,924
bir fonksiyona sahip olmanın standart yolu olan sigmoid fonksiyonunu uyguladığımı

336
00:19:33,924 --> 00:19:38,500
hayal etmekti. ya 0 ya da 1, ancak bu belirsizlik bölgesi için arada bir yumuşama var.

337
00:19:38,500 --> 00:19:44,086
Yani esas olarak, her bir kelimeye son listede yer almaları için atadığım olasılık,

338
00:19:44,086 --> 00:19:49,540
x ekseninde nerede olursa olsun yukarıdaki sigmoid fonksiyonunun değeri olacaktır.

339
00:19:49,540 --> 00:19:52,139
Açıkçası bu birkaç parametreye bağlıdır; örneğin,

340
00:19:52,139 --> 00:19:55,830
bu kelimelerin x ekseni üzerinde ne kadar geniş bir alanı dolduracağı,

341
00:19:55,830 --> 00:20:00,404
1&#39;den 0&#39;a ne kadar kademeli veya dik bir şekilde düştüğümüzü belirler ve onları

342
00:20:00,404 --> 00:20:03,160
soldan sağa nereye yerleştirdiğimiz kesmeyi belirler.

343
00:20:03,160 --> 00:20:07,340
Dürüst olmak gerekirse, bunu yapma şeklim sadece parmağımı yalayıp rüzgara doğru tutmaktı.

344
00:20:07,340 --> 00:20:10,063
Sıralanmış listeye baktım ve bir pencere bulmaya çalıştım;

345
00:20:10,063 --> 00:20:13,571
ona baktığımda bu kelimelerin yaklaşık yarısının son cevap olma ihtimalinin

346
00:20:13,571 --> 00:20:17,680
olmama ihtimalinden daha yüksek olduğunu düşündüm ve bunu kesme noktası olarak kullandım.

347
00:20:17,680 --> 00:20:20,420
Kelimeler arasında böyle bir dağılıma sahip olduğumuzda,

348
00:20:20,420 --> 00:20:24,460
bu bize entropinin gerçekten yararlı bir ölçüm haline geldiği başka bir durum verir.

349
00:20:24,460 --> 00:20:29,164
Örneğin, diyelim ki bir oyun oynuyoruz ve tüy ve çivilerden oluşan eski açılışlarımla

350
00:20:29,164 --> 00:20:33,760
başlıyoruz ve onunla eşleşen dört olası kelimenin olduğu bir durumla karşılaşıyoruz.

351
00:20:33,760 --> 00:20:36,440
Ve diyelim ki hepsinin eşit derecede olası olduğunu düşünüyoruz.

352
00:20:36,440 --> 00:20:40,000
Size şunu sorayım, bu dağılımın entropisi nedir?

353
00:20:40,000 --> 00:20:46,600
Bu olasılıkların her biriyle ilgili bilgi, 2/4&#39;ün logaritması olacaktır,

354
00:20:46,600 --> 00:20:50,800
çünkü her biri 1 ve 4&#39;tür ve bu da 2&#39;dir.

355
00:20:50,800 --> 00:20:52,780
İki bit bilgi, dört olasılık.

356
00:20:52,780 --> 00:20:54,360
Hepsi çok iyi ve güzel.

357
00:20:54,360 --> 00:20:58,320
Peki ya size aslında dörtten fazla eşleşme olduğunu söylesem?

358
00:20:58,320 --> 00:21:02,600
Gerçekte kelime listesinin tamamına baktığımızda onunla eşleşen 16 kelime var.

359
00:21:02,600 --> 00:21:06,843
Ancak modelimizin, diğer 12 kelimenin aslında nihai cevap olma ihtimalini gerçekten

360
00:21:06,843 --> 00:21:09,672
düşük tuttuğunu varsayalım; 1000&#39;de 1 gibi bir şey,

361
00:21:09,672 --> 00:21:11,440
çünkü bunlar gerçekten belirsizdir.

362
00:21:11,440 --> 00:21:15,480
Şimdi size şunu sorayım, bu dağılımın entropisi nedir?

363
00:21:15,480 --> 00:21:18,209
Eğer entropi burada sadece eşleşme sayısını ölçüyorsa,

364
00:21:18,209 --> 00:21:22,130
o zaman bunun 16&#39;nın logaritması 2 gibi bir şey olmasını bekleyebilirsiniz

365
00:21:22,130 --> 00:21:26,200
ki bu da 4 olur, daha önce sahip olduğumuz belirsizlikten iki bit daha fazla olur.

366
00:21:26,200 --> 00:21:30,320
Ancak elbette gerçek belirsizlik daha önce yaşadığımızdan çok da farklı değil.

367
00:21:30,320 --> 00:21:34,381
Bu 12 belirsiz kelimenin var olması, örneğin son cevabın çekicilik

368
00:21:34,381 --> 00:21:38,200
olduğunu öğrenmenin çok daha şaşırtıcı olacağı anlamına gelmez.

369
00:21:38,200 --> 00:21:42,155
Yani buradaki hesaplamayı gerçekten yaptığınızda ve her bir olayın olasılığını

370
00:21:42,155 --> 00:21:45,960
karşılık gelen bilgilerle topladığınızda elde ettiğiniz sonuç 2 olur.11 bit.

371
00:21:45,960 --> 00:21:49,609
Sadece şunu söylüyorum, temelde iki parça, temelde bu dört olasılık,

372
00:21:49,609 --> 00:21:53,682
ancak tüm bu pek olası olmayan olaylar nedeniyle biraz daha belirsizlik var,

373
00:21:53,682 --> 00:21:57,120
gerçi bunları öğrenmiş olsaydınız bundan bir ton bilgi alırsınız.

374
00:21:57,120 --> 00:21:59,415
Yani uzaklaştırma, Wordle&#39;u bilgi teorisi dersi

375
00:21:59,415 --> 00:22:01,800
için bu kadar güzel bir örnek yapan şeyin bir parçası.

376
00:22:01,800 --> 00:22:05,280
Entropi için iki ayrı duygu uygulamasına sahibiz.

377
00:22:05,280 --> 00:22:10,848
Birincisi bize belirli bir tahminden alacağımız beklenen bilginin ne olduğunu söylüyor,

378
00:22:10,848 --> 00:22:16,480
ikincisi ise mümkün olan tüm kelimeler arasında kalan belirsizliği ölçebilir miyiz diyor.

379
00:22:16,480 --> 00:22:20,740
Ve şunu vurgulamalıyım ki, bir tahminin beklenen bilgisine baktığımız ilk durumda,

380
00:22:20,740 --> 00:22:25,000
kelimelere eşit olmayan bir ağırlık verdiğimizde, bu entropi hesaplamasını etkiler.

381
00:22:25,000 --> 00:22:29,694
Örneğin, daha önce Weary ile ilişkili dağıtıma baktığımız aynı durumu ele alayım,

382
00:22:29,694 --> 00:22:34,560
ancak bu sefer tüm olası kelimeler arasında tekdüze olmayan bir dağılım kullanıyorum.

383
00:22:34,560 --> 00:22:39,360
Bakalım burada bunu oldukça iyi gösteren bir bölüm bulabilecek miyim?

384
00:22:39,360 --> 00:22:42,480
Tamam, işte bu oldukça iyi.

385
00:22:42,480 --> 00:22:45,591
Burada birbirine eşit olasılıklara sahip iki bitişik modelimiz var,

386
00:22:45,591 --> 00:22:49,480
ancak bize söylenenlerden birinin kendisiyle eşleşen 32 olası kelime olduğu söylendi.

387
00:22:49,480 --> 00:22:52,167
Ve bunların ne olduğunu kontrol edersek, bunlar şu 32 kelimedir,

388
00:22:52,167 --> 00:22:55,600
gözlerinizi üzerlerine taradığınızda bunların hepsi pek olası olmayan kelimelerdir.

389
00:22:55,600 --> 00:22:58,684
Belki bağırmak gibi akla yatkın cevaplar bulmak zordur,

390
00:22:58,684 --> 00:23:03,476
ancak dağılımdaki komşu düzene bakarsak, ki bu da hemen hemen aynı olası kabul edilir,

391
00:23:03,476 --> 00:23:08,102
bize sadece 8 olası eşleşme olduğu söylendi, yani çeyrek olarak birçok eşleşme var,

392
00:23:08,102 --> 00:23:09,920
ancak bu da bir o kadar muhtemel.

393
00:23:09,920 --> 00:23:12,520
Ve bu kibritleri çıkardığımızda nedenini görebiliriz.

394
00:23:12,520 --> 00:23:17,840
Bunlardan bazıları, zil sesi, gazap veya tecavüz gibi gerçekten makul yanıtlardır.

395
00:23:17,840 --> 00:23:21,900
Tüm bunları nasıl dahil ettiğimizi göstermek için, burada Wordlebot&#39;un 2.

396
00:23:21,900 --> 00:23:25,960
versiyonunu ele almama izin verin; ilk gördüğümüzden iki veya üç ana fark var.

397
00:23:25,960 --> 00:23:28,950
Öncelikle, az önce söylediğim gibi, bu entropileri,

398
00:23:28,950 --> 00:23:31,940
bu beklenen bilgi değerlerini hesaplama yöntemimiz,

399
00:23:31,940 --> 00:23:36,540
artık belirli bir kelimenin gerçekten cevap olma olasılığını da içeren kalıplar

400
00:23:36,540 --> 00:23:39,300
arasındaki daha hassas dağılımları kullanmaktır.

401
00:23:39,300 --> 00:23:44,160
Aslına bakılırsa gözyaşları hala 1 numara, ancak sonrakiler biraz farklı.

402
00:23:44,160 --> 00:23:46,587
İkincisi, en çok tercih edilenleri sıraladığında,

403
00:23:46,587 --> 00:23:50,276
artık her kelimenin asıl cevap olma ihtimaline ilişkin bir model tutacak ve

404
00:23:50,276 --> 00:23:54,015
bunu kararına dahil edecek; bunu, konuyla ilgili birkaç tahminimiz olduğunda

405
00:23:54,015 --> 00:23:55,520
görmek daha kolay olacak. masa.

406
00:23:55,520 --> 00:23:58,728
Yine tavsiyelerini göz ardı ediyoruz çünkü makinelerin

407
00:23:58,728 --> 00:24:01,120
hayatlarımızı yönetmesine izin veremeyiz.

408
00:24:01,120 --> 00:24:05,066
Ve sanırım burada solda farklı olan başka bir şeyden bahsetmem gerekiyor;

409
00:24:05,066 --> 00:24:09,760
belirsizlik değeri, yani bit sayısı, artık sadece olası eşleşmelerin sayısıyla gereksiz

410
00:24:09,760 --> 00:24:10,080
değil.

411
00:24:10,080 --> 00:24:15,138
Şimdi yukarı çekip 2 üzeri 8&#39;i hesaplarsak.02, ki bu da 256&#39;nın,

412
00:24:15,138 --> 00:24:19,920
sanırım 259&#39;un biraz üzerinde, aslında bu kalıpla eşleşen toplam

413
00:24:19,920 --> 00:24:24,216
526 kelime olmasına rağmen, sahip olduğu belirsizlik miktarı,

414
00:24:24,216 --> 00:24:29,760
eşit derecede olası 259 kelime olsaydı ne olacağına daha çok benziyor. sonuçlar.

415
00:24:29,760 --> 00:24:31,100
Bunu şöyle düşünebilirsiniz.

416
00:24:31,100 --> 00:24:35,183
Borx&#39;un cevap olmadığını biliyor, yorts, zorl ve zorus için de aynısı geçerli,

417
00:24:35,183 --> 00:24:37,840
dolayısıyla önceki duruma göre biraz daha az belirsiz.

418
00:24:37,840 --> 00:24:40,220
Bu bit sayısı daha az olacaktır.

419
00:24:40,220 --> 00:24:44,266
Ve eğer oyunu oynamaya devam edersem, burada açıklamak

420
00:24:44,266 --> 00:24:48,680
istediğim şeye uygun birkaç tahminle bunu detaylandıracağım.

421
00:24:48,680 --> 00:24:51,182
Dördüncü tahmine göre, eğer en çok tercih edilenlere bakarsanız,

422
00:24:51,182 --> 00:24:53,800
bunun artık sadece entropiyi maksimuma çıkarmadığını görebilirsiniz.

423
00:24:53,800 --> 00:24:57,150
Yani bu noktada teknik olarak yedi olasılık var

424
00:24:57,150 --> 00:25:00,780
ama anlamlı şansı olan tek şey yurtlar ve kelimeler.

425
00:25:00,780 --> 00:25:04,670
Ve her ikisini de seçmenin bu diğer değerlerin üzerinde yer aldığını,

426
00:25:04,670 --> 00:25:07,560
açıkçası daha fazla bilgi vereceğini görebilirsiniz.

427
00:25:07,560 --> 00:25:11,145
Bunu ilk yaptığımda, her tahminin kalitesini ölçmek için bu iki sayıyı

428
00:25:11,145 --> 00:25:14,580
topladım ve bu aslında tahmin edebileceğinizden daha iyi işe yaradı.

429
00:25:14,580 --> 00:25:17,515
Ama bu pek sistematik gelmedi ve eminim ki insanların benimseyebileceği

430
00:25:17,515 --> 00:25:19,880
başka yaklaşımlar da vardır, ama ben bu yaklaşıma ulaştım.

431
00:25:19,880 --> 00:25:23,956
Bir sonraki tahmin olasılığını düşünürsek, bu durumda kelimeler gibi,

432
00:25:23,956 --> 00:25:28,440
gerçekten umursadığımız şey, eğer bunu yaparsak oyunumuzun beklenen puanıdır.

433
00:25:28,440 --> 00:25:32,088
Beklenen puanı hesaplamak için de kelimelerin gerçek cevap olma

434
00:25:32,088 --> 00:25:36,080
olasılığının ne olduğunu söylüyoruz ki bu şu anda %58&#39;i açıklıyor.

435
00:25:36,080 --> 00:25:40,400
Bu maçta puanımızın %58 ihtimalle 4 olacağını söylüyoruz.

436
00:25:40,400 --> 00:25:46,240
Ve 1 eksi %58 olasılıkla puanımız 4&#39;ten fazla olacaktır.

437
00:25:46,240 --> 00:25:49,454
Daha ne kadarını bilmiyoruz ama o noktaya geldiğimizde ne kadar

438
00:25:49,454 --> 00:25:52,920
belirsizliğin ortaya çıkabileceğine dayanarak bunu tahmin edebiliriz.

439
00:25:52,920 --> 00:25:56,600
Özellikle şu anda 1 tane var.44 bit belirsizlik.

440
00:25:56,600 --> 00:26:01,560
Kelimeleri tahmin edersek, bu bize alacağımız beklenen bilginin 1 olduğunu söyler.27 bit.

441
00:26:01,560 --> 00:26:04,920
Yani kelimeleri tahmin edersek, bu fark, bu olay gerçekleştikten

442
00:26:04,920 --> 00:26:08,280
sonra ne kadar belirsizlikle baş başa kalacağımızı temsil ediyor.

443
00:26:08,280 --> 00:26:10,491
İhtiyacımız olan şey, burada f adını verdiğim,

444
00:26:10,491 --> 00:26:13,880
bu belirsizliği beklenen bir puanla ilişkilendiren bir tür fonksiyondur.

445
00:26:13,880 --> 00:26:18,160
Ve bunu gerçekleştirmenin yolu, botun 1. versiyonuna dayalı olarak

446
00:26:18,160 --> 00:26:23,015
önceki oyunlardan bir grup veriyi çizerek, çok ölçülebilir belirsizliklerle

447
00:26:23,015 --> 00:26:27,040
çeşitli noktalardan sonra gerçek puanın ne olduğunu söylemekti.

448
00:26:27,040 --> 00:26:31,546
Örneğin, buradaki veri noktaları 8 civarındaki bir değerin üzerinde duruyor.8&#39;in

449
00:26:31,546 --> 00:26:35,734
olduğu bir noktadan sonra bazı oyunlar için 7 ya da öylesine diyorlar.7 bitlik

450
00:26:35,734 --> 00:26:39,340
belirsizlik, nihai cevaba ulaşmak için iki tahmin yapılması gerekti.

451
00:26:39,340 --> 00:26:43,180
Diğer oyunlar için üç tahmin gerekiyordu, diğer oyunlar için ise dört tahmin gerekiyordu.

452
00:26:43,180 --> 00:26:46,513
Burada sola kayarsak, sıfırın üzerindeki tüm noktalar,

453
00:26:46,513 --> 00:26:50,332
ne zaman sıfır belirsizlik varsa, yani tek bir olasılık varsa,

454
00:26:50,332 --> 00:26:55,000
o zaman gereken tahmin sayısı her zaman sadece birdir, bu da güven vericidir.

455
00:26:55,000 --> 00:26:59,301
Ne zaman bir miktar belirsizlik olsa, yani esasen iki olasılığa

456
00:26:59,301 --> 00:27:03,940
bağlıysa bazen bir tahmin daha, bazen de iki tahmin daha gerekiyordu.

457
00:27:03,940 --> 00:27:05,980
Burada da böyle devam ediyor.

458
00:27:05,980 --> 00:27:08,765
Belki bu verileri görselleştirmenin biraz daha kolay bir yolu,

459
00:27:08,765 --> 00:27:11,020
bunları bir araya toplayıp ortalamalarını almaktır.

460
00:27:11,020 --> 00:27:16,899
Örneğin buradaki çubuk, bir miktar belirsizliğimizin olduğu tüm noktalar arasında

461
00:27:16,899 --> 00:27:22,420
ortalama olarak gereken yeni tahmin sayısının yaklaşık 1 olduğunu söylüyor.5.

462
00:27:22,420 --> 00:27:26,919
Ve buradaki çubuk, tüm farklı oyunlar arasında bir noktada belirsizliğin dört bitin

463
00:27:26,919 --> 00:27:31,204
biraz üzerinde olduğunu söylüyor, bu da onu 16 farklı olasılığa daraltmak gibi,

464
00:27:31,204 --> 00:27:35,918
o zaman ortalama olarak o noktadan itibaren ikiden biraz daha fazla tahmin gerektiriyor

465
00:27:35,918 --> 00:27:36,240
ileri.

466
00:27:36,240 --> 00:27:40,080
Ve buradan itibaren buna makul görünen bir fonksiyona uyacak bir regresyon yaptım.

467
00:27:40,080 --> 00:27:44,630
Ve unutmayın, bunları yapmanın asıl amacı, bir kelimeden ne kadar çok bilgi

468
00:27:44,630 --> 00:27:49,720
kazanırsak beklenen puanın o kadar düşük olacağı şeklindeki bu sezgiyi ölçebilmektir.

469
00:27:49,720 --> 00:27:55,247
Yani bu sürüm 2 olarak.0&#39;a dönersek ve aynı simülasyon setini çalıştırırsak,

470
00:27:55,247 --> 00:27:59,820
2315 olası sözcük yanıtının tümüne karşı oynatırsak, bu nasıl olur?

471
00:27:59,820 --> 00:28:04,060
İlk versiyonumuzun aksine kesinlikle daha iyi, bu da güven verici.

472
00:28:04,060 --> 00:28:06,642
Tüm söylenen ve yapılan ortalama 3 civarındadır.6,

473
00:28:06,642 --> 00:28:10,895
ilk versiyondan farklı olarak birkaç kez kaybettiği ve bu durumda altıdan fazlasını

474
00:28:10,895 --> 00:28:12,820
gerektirdiği durumlar olmasına rağmen.

475
00:28:12,820 --> 00:28:15,819
Muhtemelen bilgiyi en üst düzeye çıkarmak yerine hedefe

476
00:28:15,819 --> 00:28:18,980
ulaşmak için bu ödünleşimin yapıldığı zamanlar olduğu için.

477
00:28:18,980 --> 00:28:22,140
Peki 3&#39;ten daha iyisini yapabilir miyiz?6?

478
00:28:22,140 --> 00:28:23,260
Kesinlikle yapabiliriz.

479
00:28:23,260 --> 00:28:26,333
Başlangıçta, kelime cevaplarının gerçek listesini modelini

480
00:28:26,333 --> 00:28:29,980
oluşturma biçimine dahil etmemenin çok eğlenceli olduğunu söylemiştim.

481
00:28:29,980 --> 00:28:35,180
Ancak bunu dahil edersek alabileceğim en iyi performans 3 civarındaydı.43.

482
00:28:35,180 --> 00:28:38,920
Dolayısıyla, bu önceki dağılımı seçmek için kelime sıklığı verilerini kullanmaktan daha

483
00:28:38,920 --> 00:28:42,194
karmaşık hale gelmeye çalışırsak, bu 3.43 muhtemelen bunda ne kadar başarılı

484
00:28:42,194 --> 00:28:46,019
olabileceğimizin veya en azından benim bunda ne kadar başarılı olabileceğimin maksimumunu

485
00:28:46,019 --> 00:28:46,360
veriyor.

486
00:28:46,360 --> 00:28:50,323
Bu en iyi performans aslında sadece burada bahsettiğim fikirleri kullanır,

487
00:28:50,323 --> 00:28:55,078
ancak biraz daha ileri gider, sanki beklenen bilgiyi tek bir adım yerine iki adım ileriye

488
00:28:55,078 --> 00:28:55,660
doğru arar.

489
00:28:55,660 --> 00:28:58,161
Başlangıçta bunun hakkında daha fazla konuşmayı planlıyordum

490
00:28:58,161 --> 00:29:00,580
ama aslında oldukça uzun bir yol kat ettiğimizi fark ettim.

491
00:29:00,580 --> 00:29:03,456
Söyleyeceğim tek şey, bu iki adımlı aramayı yaptıktan ve ardından en

492
00:29:03,456 --> 00:29:06,248
iyi adaylar üzerinde birkaç örnek simülasyon çalıştırdıktan sonra,

493
00:29:06,248 --> 00:29:09,500
şu ana kadar benim için en azından Crane&#39;in en iyi açıcı olduğu görünüyor.

494
00:29:09,500 --> 00:29:11,080
Kim tahmin ederdi?

495
00:29:11,080 --> 00:29:15,125
Ayrıca olasılıklar alanınızı belirlemek için gerçek sözcük listesini kullanırsanız,

496
00:29:15,125 --> 00:29:18,160
o zaman başlangıçtaki belirsizlik 11 bitin biraz üzerinde olur.

497
00:29:18,160 --> 00:29:22,214
Ve sadece kaba kuvvet aramasından sonra, ilk iki tahminden sonra

498
00:29:22,214 --> 00:29:26,580
beklenen maksimum olası bilginin 10 bit civarında olduğu ortaya çıktı.

499
00:29:26,580 --> 00:29:29,478
Bu da en iyi senaryoda, ilk iki tahmininizden sonra,

500
00:29:29,478 --> 00:29:33,907
mükemmel derecede optimal bir oyunla, yaklaşık bir miktar belirsizlikle baş başa

501
00:29:33,907 --> 00:29:35,220
kalacağınızı gösteriyor.

502
00:29:35,220 --> 00:29:37,400
Bu, iki olası tahminde bulunmakla aynı şeydir.

503
00:29:37,400 --> 00:29:40,935
Bu yüzden, bu ortalamayı 3&#39;e kadar düşüren bir algoritmayı asla yazamayacağınızı

504
00:29:40,935 --> 00:29:44,179
söylemek adil ve muhtemelen oldukça muhafazakar olur çünkü kullanabileceğiniz

505
00:29:44,179 --> 00:29:47,590
kelimelerle, sadece iki adımdan sonra yeterli bilgiyi elde etmek için yer yoktur.

506
00:29:47,590 --> 00:29:50,460
her seferinde üçüncü slottaki cevabı hatasız olarak garanti edebilir.

