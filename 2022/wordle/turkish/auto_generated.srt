1
00:00:00,000 --> 00:00:03,303
Wurdle oyunu son bir iki ayda oldukça viral hale geldi ve hiçbir zaman

2
00:00:03,303 --> 00:00:06,466
bir matematik dersi fırsatını gözden kaçıran biri olmadı. Bana öyle

3
00:00:06,466 --> 00:00:09,584
geliyor ki bu oyun bilgi teorisi ile ilgili bir derste çok iyi bir

4
00:00:09,584 --> 00:00:13,120
merkezi örnek teşkil ediyor ve özellikle de entropi olarak bilinen bir konu.

5
00:00:13,120 --> 00:00:18,073
Görüyorsunuz, pek çok insan gibi ben de bulmacanın içine kapıldım ve birçok programcı

6
00:00:18,073 --> 00:00:23,200
gibi ben de oyunu olabildiğince optimum şekilde oynatacak bir algoritma yazmaya kapıldım.

7
00:00:23,200 --> 00:00:26,160
Ve burada yapmayı düşündüğüm şey, sizinle bu konudaki sürecimin

8
00:00:26,160 --> 00:00:29,212
bir kısmını konuşmak ve bunun içine giren matematiğin bir kısmını

9
00:00:29,212 --> 00:00:32,080
açıklamak, çünkü tüm algoritma bu entropi fikrine odaklanıyor.

10
00:00:32,080 --> 00:00:42,180
İlk olarak, duymadıysanız söyleyeyim, Wurdle nedir?

11
00:00:42,180 --> 00:00:45,215
Ve burada oyunun kurallarını incelerken bir taşla iki kuş vurmak

12
00:00:45,215 --> 00:00:48,251
için, bununla nereye gittiğimizi de ön izlememe izin verin, yani

13
00:00:48,251 --> 00:00:51,380
temelde oyunu bizim için oynayacak küçük bir algoritma geliştirmek.

14
00:00:51,380 --> 00:00:55,860
Bugünkü Wurdle&#39;ı yapmamış olsam da, bugün 4 Şubat ve botun nasıl yapacağını göreceğiz.

15
00:00:55,860 --> 00:00:58,319
Wurdle&#39;ın amacı, beş harfli gizemli bir kelimeyi tahmin

16
00:00:58,319 --> 00:01:00,860
etmektir ve size tahmin etmeniz için altı farklı şans verilir.

17
00:01:00,860 --> 00:01:05,240
Örneğin, Wurdle botum tahmin vinciyle başlamamı öneriyor.

18
00:01:05,240 --> 00:01:08,062
Her tahmin yaptığınızda, tahmininizin gerçek cevaba

19
00:01:08,062 --> 00:01:10,940
ne kadar yakın olduğuna dair bazı bilgiler alırsınız.

20
00:01:10,940 --> 00:01:14,540
Burada gri kutu bana asıl cevapta C&#39;nin olmadığını söylüyor.

21
00:01:14,540 --> 00:01:18,340
Sarı kutu bana bir R olduğunu söylüyor ama o konumda değil.

22
00:01:18,340 --> 00:01:20,677
Yeşil kutu bana gizli kelimenin A harfine sahip

23
00:01:20,677 --> 00:01:22,820
olduğunu ve üçüncü sırada olduğunu söylüyor.

24
00:01:22,820 --> 00:01:24,300
Ve sonra N yok ve E yok.

25
00:01:24,300 --> 00:01:27,420
O halde içeri girip Wurdle botuna bu bilgiyi söyleyeyim.

26
00:01:27,420 --> 00:01:31,500
Turnayla başladık, gri, sarı, yeşil, gri, gri elde ettik.

27
00:01:31,500 --> 00:01:33,884
Şu anda gösterdiği tüm veriler hakkında endişelenmeyin,

28
00:01:33,884 --> 00:01:35,460
bunları zamanı gelince açıklayacağım.

29
00:01:35,460 --> 00:01:39,700
Ancak ikinci seçimimiz için en önemli önerisi saçmalıktır.

30
00:01:39,700 --> 00:01:42,889
Ve tahmininizin gerçekten beş harfli bir kelime olması gerekiyor, ancak göreceğiniz

31
00:01:42,889 --> 00:01:45,700
gibi, aslında tahmin etmenize izin vereceği şey konusunda oldukça liberal.

32
00:01:45,700 --> 00:01:48,860
Bu durumda, saçmalamayı deneriz.

33
00:01:48,860 --> 00:01:50,260
Ve tamam, işler oldukça iyi görünüyor.

34
00:01:50,260 --> 00:01:54,740
S ve H&#39;ye basıyoruz, yani ilk üç harfi biliyoruz, bir R olduğunu biliyoruz.

35
00:01:54,740 --> 00:01:59,740
Ve böylece SHA bir R gibi olacak veya SHA R bir şey olacak.

36
00:01:59,740 --> 00:02:02,815
Görünüşe göre Wurdle botu bunun yalnızca iki olasılığa

37
00:02:02,815 --> 00:02:05,220
bağlı olduğunu biliyor; kırık ya da keskin.

38
00:02:05,220 --> 00:02:08,130
Bu noktada aralarında bir tür çekişme var, bu yüzden

39
00:02:08,130 --> 00:02:11,260
sanırım muhtemelen alfabetik olduğu için parçayla uyumlu.

40
00:02:11,260 --> 00:02:13,000
Hangi yaşasın, asıl cevap bu.

41
00:02:13,000 --> 00:02:14,660
Böylece üçe çıktık.

42
00:02:14,660 --> 00:02:17,761
Bunun iyi olup olmadığını merak ediyorsanız, bir kişiden duyduğuma göre

43
00:02:17,761 --> 00:02:20,820
Wurdle&#39;da dört eşit ve üç birdie&#39;dir şeklinde bir cümle duydum.

44
00:02:20,820 --> 00:02:22,960
Oldukça uygun bir benzetme olduğunu düşünüyorum.

45
00:02:22,960 --> 00:02:25,282
Dört puan almak için oyununuzda tutarlı bir şekilde

46
00:02:25,282 --> 00:02:27,560
çalışmalısınız, ancak bu kesinlikle çılgınca değil.

47
00:02:27,560 --> 00:02:30,000
Ama üçe böldüğünüzde harika hissettiriyor.

48
00:02:30,000 --> 00:02:33,322
Yani eğer istekliyseniz, burada yapmak istediğim şey Wurdle botuna nasıl

49
00:02:33,322 --> 00:02:36,600
yaklaştığım konusunda en başından itibaren düşünce sürecimden bahsetmek.

50
00:02:36,600 --> 00:02:39,800
Ve dediğim gibi bu aslında bilgi teorisi dersi için bir bahane.

51
00:02:39,800 --> 00:02:48,560
Temel amaç bilgi nedir, entropinin ne olduğunu açıklamaktır.

52
00:02:48,560 --> 00:02:51,085
Bu konuya yaklaşırken ilk düşüncem İngilizcedeki

53
00:02:51,085 --> 00:02:53,560
farklı harflerin göreceli sıklıklarına bakmaktı.

54
00:02:53,560 --> 00:02:56,641
Ben de düşündüm ki, tamam, bu en sık kullanılan harflerin çoğuna

55
00:02:56,641 --> 00:02:59,960
karşılık gelen bir açılış tahmini veya bir açılış tahmin çifti var mı?

56
00:02:59,960 --> 00:03:03,780
Ve oldukça hoşuma giden bir şey de diğerini ve ardından tırnakları yapmaktı.

57
00:03:03,780 --> 00:03:05,934
Buradaki düşünce şu ki, eğer bir harfe rastlarsanız, yeşil

58
00:03:05,934 --> 00:03:07,980
ya da sarı elde edersiniz, bu her zaman iyi hissettirir.

59
00:03:07,980 --> 00:03:09,460
Bilgi alıyormuşsunuz gibi geliyor.

60
00:03:09,460 --> 00:03:12,200
Ancak bu durumlarda, vurmasanız ve her zaman gri tonlar alsanız

61
00:03:12,200 --> 00:03:14,856
bile, bu size yine de birçok bilgi verir, çünkü bu harflerden

62
00:03:14,856 --> 00:03:17,640
herhangi birine sahip olmayan bir kelime bulmak oldukça nadirdir.

63
00:03:17,640 --> 00:03:20,606
Ancak yine de bu pek sistematik gelmiyor çünkü örneğin

64
00:03:20,606 --> 00:03:23,520
harflerin sırasını dikkate almanın hiçbir faydası yok.

65
00:03:23,520 --> 00:03:26,080
Salyangoz yazabilecekken neden çivi yazayım ki?

66
00:03:26,080 --> 00:03:27,720
Sonunda S olması daha mı iyi?

67
00:03:27,720 --> 00:03:28,720
Gerçekten emin değilim.

68
00:03:28,720 --> 00:03:33,033
Bir arkadaşım yorgun sözcüğüyle başlamayı sevdiğini söyledi, bu beni

69
00:03:33,033 --> 00:03:37,160
biraz şaşırttı çünkü içinde W ve Y gibi alışılmadık harfler vardı.

70
00:03:37,160 --> 00:03:39,400
Ama kim bilir, belki bu daha iyi bir açılıştır.

71
00:03:39,400 --> 00:03:42,105
Potansiyel bir tahminin kalitesini değerlendirmek

72
00:03:42,105 --> 00:03:44,920
için verebileceğimiz bir tür niceliksel puan var mı?

73
00:03:44,920 --> 00:03:48,385
Şimdi olası tahminleri sıralama yöntemimizi belirlemek için geriye

74
00:03:48,385 --> 00:03:51,800
dönüp oyunun tam olarak nasıl kurulduğuna biraz açıklık getirelim.

75
00:03:51,800 --> 00:03:54,714
Yani, geçerli tahminler olarak kabul edilen, girmenize izin

76
00:03:54,714 --> 00:03:57,920
verecek yaklaşık 13.000 kelime uzunluğunda bir kelime listesi var.

77
00:03:57,920 --> 00:04:02,448
Ama baktığınızda pek çok sıra dışı şey var; kafa ya da Ali ve ARG gibi

78
00:04:02,448 --> 00:04:07,040
şeyler, Scrabble oyununda aile tartışmalarına yol açan türden kelimeler.

79
00:04:07,040 --> 00:04:10,600
Ancak oyunun havası, cevabın her zaman oldukça yaygın bir kelime olacağıdır.

80
00:04:10,600 --> 00:04:16,080
Ve aslında olası yanıtlar olan yaklaşık 2300 kelimeden oluşan başka bir liste daha var.

81
00:04:16,080 --> 00:04:18,896
Ve bu, insanların hazırladığı bir liste, sanırım özellikle oyun

82
00:04:18,896 --> 00:04:21,800
yaratıcısının kız arkadaşı tarafından, ki bu da oldukça eğlenceli.

83
00:04:21,800 --> 00:04:26,147
Ancak yapmak istediğim şey, bu projedeki amacımız, bu listeyle ilgili önceki

84
00:04:26,147 --> 00:04:30,720
bilgileri birleştirmeyen, Wordle çözen bir program yazıp yazamayacağımızı görmek.

85
00:04:30,720 --> 00:04:35,560
Öncelikle, bu listede bulamayacağınız pek çok yaygın beş harfli kelime var.

86
00:04:35,560 --> 00:04:38,781
Bu nedenle, biraz daha dayanıklı olan ve sadece resmi web sitesine değil,

87
00:04:38,781 --> 00:04:41,960
herkese karşı Wordle oynayabilecek bir program yazmak daha iyi olacaktır.

88
00:04:41,960 --> 00:04:44,441
Ayrıca bu olası yanıtlar listesinin ne olduğunu

89
00:04:44,441 --> 00:04:47,440
bilmemizin nedeni, bunun kaynak kodunda görünür olmasıdır.

90
00:04:47,440 --> 00:04:52,840
Ancak kaynak kodunda görünme şekli, cevapların günden güne ortaya çıkma sırasına göredir.

91
00:04:52,840 --> 00:04:56,400
Böylece her zaman yarının cevabının ne olacağına bakabilirsiniz.

92
00:04:56,400 --> 00:04:59,140
Açıkça görülüyor ki, listeyi kullanmanın hile yapmak olduğu bir anlam taşıyor.

93
00:04:59,140 --> 00:05:03,306
Ve daha ilginç bir bulmacayı ve daha zengin bir bilgi teorisi dersini ortaya

94
00:05:03,306 --> 00:05:07,581
çıkaran şey, daha yaygın sözcükleri tercih etme sezgisini yakalamak için genel

95
00:05:07,581 --> 00:05:11,640
olarak göreceli sözcük sıklıkları gibi daha evrensel verileri kullanmaktır.

96
00:05:11,640 --> 00:05:16,560
Peki bu 13.000 olasılık arasından açılış tahminini nasıl seçmeliyiz?

97
00:05:16,560 --> 00:05:19,960
Mesela arkadaşım bıkkınlık teklif ediyorsa kalitesini nasıl analiz etmeliyiz?

98
00:05:19,960 --> 00:05:23,835
Pek olası olmayan W&#39;yi sevdiğini söylemesinin nedeni, o W&#39;ye

99
00:05:23,835 --> 00:05:27,880
vurmanın ne kadar iyi hissettireceğinin uzun vadede doğasını sevmesidir.

100
00:05:27,880 --> 00:05:31,727
Örneğin, ortaya çıkan ilk kalıp bunun gibi bir şeyse, bu dev

101
00:05:31,727 --> 00:05:36,080
sözlükte bu kalıpla eşleşen yalnızca 58 kelime olduğu ortaya çıkıyor.

102
00:05:36,080 --> 00:05:38,900
Yani bu 13.000&#39;den çok büyük bir azalma.

103
00:05:38,900 --> 00:05:43,360
Ancak bunun diğer tarafı da elbette ki böyle bir model elde etmenin çok nadir olmasıdır.

104
00:05:43,360 --> 00:05:47,483
Spesifik olarak, her kelimenin cevap olma olasılığı eşit

105
00:05:47,483 --> 00:05:51,680
olsaydı, bu kalıba ulaşma olasılığı 58 bölü 13.000 olurdu.

106
00:05:51,680 --> 00:05:53,880
Elbette bunların cevap olma ihtimali eşit değil.

107
00:05:53,880 --> 00:05:56,680
Bunların çoğu çok belirsiz ve hatta şüpheli kelimelerdir.

108
00:05:56,680 --> 00:05:59,187
Ama en azından tüm bunlara ilk geçişimizde, hepsinin eşit

109
00:05:59,187 --> 00:06:02,040
derecede olası olduğunu varsayalım ve bunu biraz sonra düzeltelim.

110
00:06:02,040 --> 00:06:04,727
Mesele şu ki, çok fazla bilgi içeren bir modelin

111
00:06:04,727 --> 00:06:07,360
doğası gereği ortaya çıkması pek olası değildir.

112
00:06:07,360 --> 00:06:11,920
Aslında bilgilendirici olmanın anlamı bunun olası olmadığıdır.

113
00:06:11,920 --> 00:06:15,140
Bu açılışta görülecek çok daha olası bir model bunun

114
00:06:15,140 --> 00:06:18,360
gibi bir şey olabilir, burada elbette W harfi yoktur.

115
00:06:18,360 --> 00:06:22,080
Belki bir E vardır ve belki A yoktur, R yoktur, Y yoktur.

116
00:06:22,080 --> 00:06:24,640
Bu durumda 1400 olası eşleşme vardır.

117
00:06:24,640 --> 00:06:30,680
Hepsi eşit olasılıkta olsaydı, göreceğiniz modelin bu olma olasılığı yaklaşık %11 olurdu.

118
00:06:30,680 --> 00:06:34,320
Dolayısıyla en olası sonuçlar aynı zamanda en az bilgilendirici olanlardır.

119
00:06:34,320 --> 00:06:38,160
Burada daha küresel bir bakış elde etmek için, görebileceğiniz tüm

120
00:06:38,160 --> 00:06:42,000
farklı kalıplara göre olasılıkların tam dağılımını size göstereyim.

121
00:06:42,000 --> 00:06:45,537
Yani baktığınız her çubuk, ortaya çıkabilecek olası bir renk

122
00:06:45,537 --> 00:06:49,248
düzenine karşılık gelir, bunlardan 3 üzeri 5 olasılık vardır ve

123
00:06:49,248 --> 00:06:52,960
bunlar soldan sağa, en yaygından en az yaygına doğru düzenlenir.

124
00:06:52,960 --> 00:06:56,200
Yani buradaki en yaygın olasılık, tüm grileri elde etmenizdir.

125
00:06:56,200 --> 00:06:58,800
Bu, zamanın yaklaşık %14&#39;ünde gerçekleşir.

126
00:06:58,800 --> 00:07:02,293
Ve bir tahmin yaptığınızda umduğunuz şey, kendinizi bu uzun

127
00:07:02,293 --> 00:07:06,019
kuyrukta bir yerde bulmanızdır; burada olduğu gibi, açıkça buna

128
00:07:06,019 --> 00:07:09,920
benzeyen bu kalıba uyan şey için yalnızca 18 olasılığın olduğu yer.

129
00:07:09,920 --> 00:07:14,080
Ya da biraz daha sola gidersek belki buraya kadar gidebiliriz.

130
00:07:14,080 --> 00:07:16,560
Tamam, işte sana güzel bir bulmaca.

131
00:07:16,560 --> 00:07:19,300
İngilizce dilinde W ile başlayan, Y ile biten

132
00:07:19,300 --> 00:07:22,040
ve içinde bir yerde R bulunan üç kelime nedir?

133
00:07:22,040 --> 00:07:27,560
Cevapların, bakalım, uzun uzun, kurtlu ve alaycı olduğu ortaya çıktı.

134
00:07:27,560 --> 00:07:31,808
Bu kelimenin genel olarak ne kadar iyi olduğuna karar vermek için, bu

135
00:07:31,808 --> 00:07:36,360
dağılımdan alacağınız beklenen bilgi miktarının bir tür ölçüsünü istiyoruz.

136
00:07:36,360 --> 00:07:41,300
Her bir modeli incelersek ve onun gerçekleşme olasılığını ne kadar bilgilendirici

137
00:07:41,300 --> 00:07:46,000
olduğunu ölçen bir şeyle çarparsak, bu bize belki objektif bir puan verebilir.

138
00:07:46,000 --> 00:07:50,280
Şimdi bir şeyin ne olması gerektiğine dair ilk içgüdünüz eşleşme sayısı olabilir.

139
00:07:50,280 --> 00:07:52,960
Daha düşük bir ortalama eşleşme sayısı istiyorsunuz.

140
00:07:52,960 --> 00:07:58,794
Ancak bunun yerine, genellikle bilgiye atfettiğimiz daha evrensel bir ölçüm kullanmak

141
00:07:58,794 --> 00:08:04,629
istiyorum ve bu 13.000 kelimenin her birine, bunların gerçekten cevap olup olmadığına

142
00:08:04,629 --> 00:08:10,600
ilişkin farklı bir olasılık atandığında daha esnek olacak bir ölçüm kullanmak istiyorum.

143
00:08:10,600 --> 00:08:13,940
Standart bilgi birimi, biraz komik bir formüle sahip olan

144
00:08:13,940 --> 00:08:17,800
bit&#39;tir, ancak sadece örneklere bakarsak gerçekten sezgiseldir.

145
00:08:17,800 --> 00:08:21,208
Olasılık alanınızı yarıya indiren bir gözleminiz

146
00:08:21,208 --> 00:08:24,200
varsa, onun bir bit bilgisi vardır diyoruz.

147
00:08:24,200 --> 00:08:27,880
Örneğimizde, olasılıklar uzayı tüm olası kelimelerden oluşuyor ve ortaya çıkıyor ki, beş

148
00:08:27,880 --> 00:08:31,560
harfli kelimelerin yaklaşık yarısının S harfi var, bundan biraz daha az ama yarısı kadar.

149
00:08:31,560 --> 00:08:35,200
Yani bu gözlem size biraz bilgi verecektir.

150
00:08:35,200 --> 00:08:38,600
Bunun yerine yeni bir olgu bu olasılıklar alanını dört kat

151
00:08:38,600 --> 00:08:42,000
azaltırsa, onun iki bitlik bilgiye sahip olduğunu söyleriz.

152
00:08:42,000 --> 00:08:45,120
Örneğin, bu kelimelerin yaklaşık dörtte birinde T harfinin olduğu ortaya çıktı.

153
00:08:45,120 --> 00:08:47,937
Eğer gözlem bu alanı sekiz kat azaltırsa, bunun üç

154
00:08:47,937 --> 00:08:50,920
bitlik bilgi olduğunu söyleriz ve bu böyle devam eder.

155
00:08:50,920 --> 00:08:55,000
Dört bit onu 16&#39;ya, beş bit ise 32&#39;ye böler.

156
00:08:55,000 --> 00:08:59,422
Şimdi durup kendinize şu soruyu sorabilirsiniz: Bir olayın

157
00:08:59,422 --> 00:09:04,520
gerçekleşme olasılığı açısından bit sayısı bilgisinin formülü nedir?

158
00:09:04,520 --> 00:09:09,727
Burada söylediğimiz şey, bit sayısının yarısını aldığınızda, bu olasılık ile aynı şeydir;

159
00:09:09,727 --> 00:09:14,935
bu, bit sayısının iki üssünün bir bölü olasılık olduğunu söylemekle aynı şeydir; Bilginin

160
00:09:14,935 --> 00:09:19,680
log tabanının iki bölü olasılığa eşit olduğunu söyleyerek yeniden düzenleme yapar.

161
00:09:19,680 --> 00:09:22,707
Ve bazen bunu bir yeniden düzenlemeyle daha görürsünüz;

162
00:09:22,707 --> 00:09:25,680
burada bilgi, olasılığın negatif logaritması tabanıdır.

163
00:09:25,680 --> 00:09:28,742
Bu şekilde ifade edilirse, bu konuya yeni başlayan biri için

164
00:09:28,742 --> 00:09:31,755
biraz tuhaf görünebilir, ancak aslında bu, olasılıklarınızı

165
00:09:31,755 --> 00:09:35,120
kaç kez yarıya indirdiğinizi sormak gibi çok sezgisel bir fikirdir.

166
00:09:35,120 --> 00:09:37,348
Şimdi merak ediyorsanız, eğlenceli bir kelime oyunu

167
00:09:37,348 --> 00:09:39,920
oynadığımızı sanıyordum, neden logaritmalar devreye giriyor?

168
00:09:39,920 --> 00:09:44,124
Bunun daha güzel bir birim olmasının bir nedeni, pek olası olmayan olaylar hakkında

169
00:09:44,124 --> 00:09:48,279
konuşmanın çok daha kolay olmasıdır; bir gözlemin 20 bitlik bilgiye sahip olduğunu

170
00:09:48,279 --> 00:09:52,785
söylemek, şunun şunun meydana gelme olasılığının 0 olduğunu söylemekten çok daha kolaydır.

171
00:09:52,785 --> 00:09:53,480
0000095.

172
00:09:53,480 --> 00:09:57,793
Ancak bu logaritmik ifadenin olasılık teorisine çok yararlı bir katkı olduğunun

173
00:09:57,793 --> 00:10:02,000
ortaya çıkmasının daha önemli bir nedeni, bilgilerin bir araya gelme şeklidir.

174
00:10:02,000 --> 00:10:05,633
Örneğin, bir gözlem size iki bitlik bilgi verirse, alanınızı dört

175
00:10:05,633 --> 00:10:09,377
katına çıkarırsa ve ardından Wordle&#39;deki ikinci tahmininiz gibi

176
00:10:09,377 --> 00:10:13,286
ikinci bir gözlem size başka bir üç bitlik bilgi verirse ve sizi başka

177
00:10:13,286 --> 00:10:17,360
bir sekiz kat daha küçültürse, ikisi birlikte size beş bitlik bilgi verir.

178
00:10:17,360 --> 00:10:21,200
Olasılıklar çoğalmayı sevdiği gibi, bilgi de eklemeyi sever.

179
00:10:21,200 --> 00:10:24,733
Dolayısıyla, bir dizi sayıyı topladığımız beklenen değer gibi bir şeyin

180
00:10:24,733 --> 00:10:28,660
alanına girdiğimizde, günlükler bununla uğraşmayı çok daha güzel hale getiriyor.

181
00:10:28,660 --> 00:10:32,084
Weary dağıtımımıza geri dönelim ve buraya her model için ne kadar

182
00:10:32,084 --> 00:10:35,560
bilgi bulunduğunu bize gösteren başka bir küçük izleyici ekleyelim.

183
00:10:35,560 --> 00:10:39,153
Fark etmenizi istediğim asıl şey, daha olası modellere ulaşma

184
00:10:39,153 --> 00:10:43,500
olasılığımız arttıkça, bilgi ne kadar düşükse, o kadar az bit kazanırsınız.

185
00:10:43,500 --> 00:10:47,140
Bu tahminin kalitesini ölçmemizin yolu, bu bilginin beklenen değerini

186
00:10:47,140 --> 00:10:51,092
almak olacaktır; burada her bir modeli inceliyoruz, bunun ne kadar muhtemel

187
00:10:51,092 --> 00:10:54,940
olduğunu söylüyoruz ve sonra bunu kaç bitlik bilgi aldığımızla çarpıyoruz.

188
00:10:54,940 --> 00:10:58,107
Ve Weary örneğinde bunun 4 olduğu ortaya çıkıyor.

189
00:10:58,107 --> 00:10:58,480
9 bit.

190
00:10:58,480 --> 00:11:01,930
Yani ortalama olarak, bu açılış tahmininden alacağınız bilgi,

191
00:11:01,930 --> 00:11:05,660
olasılıklar alanınızı yaklaşık beş kez yarıya indirmeye eşdeğerdir.

192
00:11:05,660 --> 00:11:09,364
Bunun tersine, beklenen bilgi değeri daha yüksek

193
00:11:09,364 --> 00:11:13,220
olan bir tahmin örneği Slate gibi bir şey olabilir.

194
00:11:13,220 --> 00:11:16,180
Bu durumda dağılımın çok daha düz göründüğünü fark edeceksiniz.

195
00:11:16,180 --> 00:11:20,128
Özellikle, tüm grilerin en muhtemel oluşumu yalnızca %6 civarında

196
00:11:20,128 --> 00:11:24,435
meydana gelme şansına sahiptir, yani en azından açıkça 3 elde edersiniz.

197
00:11:24,435 --> 00:11:25,940
9 bitlik bilgi.

198
00:11:25,940 --> 00:11:29,140
Ancak bu minimumdur, daha genel olarak bundan daha iyi bir şey elde edersiniz.

199
00:11:29,140 --> 00:11:32,376
Ve buradaki rakamları hesaplayıp ilgili tüm terimleri

200
00:11:32,376 --> 00:11:36,333
topladığınızda ortalama bilginin yaklaşık 5 olduğu ortaya çıkıyor.

201
00:11:36,333 --> 00:11:36,420
8.

202
00:11:36,420 --> 00:11:39,985
Yani Weary&#39;nin aksine, olasılıklar alanınız bu ilk

203
00:11:39,985 --> 00:11:43,940
tahminden sonra ortalama olarak yarısı kadar büyük olacaktır.

204
00:11:43,940 --> 00:11:49,540
Bilgi miktarının bu beklenen değerinin adı hakkında aslında eğlenceli bir hikaye var.

205
00:11:49,540 --> 00:11:53,337
Bilgi teorisi, 1940&#39;larda Bell Laboratuarlarında çalışan Claude Shannon

206
00:11:53,337 --> 00:11:56,884
tarafından geliştirildi, ancak henüz yayınlanmamış bazı fikirlerinden,

207
00:11:56,884 --> 00:12:00,732
zamanın entelektüel devi, çok öne çıkan John von Neumann&#39;la konuşuyordu.

208
00:12:00,732 --> 00:12:04,180
matematik ve fizikte ve bilgisayar bilimine dönüşen şeyin başlangıcı.

209
00:12:04,180 --> 00:12:07,558
Ve von Neumann, bilgi miktarının bu beklenen değeri için gerçekten

210
00:12:07,558 --> 00:12:11,088
iyi bir isme sahip olmadığını söylediğinde, söylendiğine göre, hikaye

211
00:12:11,088 --> 00:12:14,720
şöyle devam ediyor, buna entropi diyebilirsiniz ve bunun iki nedeni var.

212
00:12:14,720 --> 00:12:18,761
İlk olarak, belirsizlik fonksiyonunuz istatistiksel mekanikte bu isimle kullanıldı,

213
00:12:18,761 --> 00:12:23,043
dolayısıyla zaten bir adı var ve ikinci olarak ve daha da önemlisi, hiç kimse entropinin

214
00:12:23,043 --> 00:12:26,940
gerçekte ne olduğunu bilmiyor, dolayısıyla bir tartışmada her zaman avantajı var.

215
00:12:26,940 --> 00:12:30,089
Yani eğer isim biraz gizemli görünüyorsa ve eğer bu

216
00:12:30,089 --> 00:12:33,420
hikayeye inanılacaksa, bu bir bakıma tasarım gereğidir.

217
00:12:33,420 --> 00:12:37,783
Ayrıca, eğer bunun fizikteki termodinamiğin ikinci yasasıyla ilgili tüm o şeylerle

218
00:12:37,783 --> 00:12:42,041
ilişkisini merak ediyorsanız, kesinlikle bir bağlantı var, ama kökeninde Shannon

219
00:12:42,041 --> 00:12:46,351
sadece saf olasılık teorisiyle ilgileniyordu ve buradaki amaçlarımız için, Kelime

220
00:12:46,351 --> 00:12:50,820
entropisi, sadece belirli bir tahminin beklenen bilgi değerini düşünmenizi istiyorum.

221
00:12:50,820 --> 00:12:54,380
Entropiyi iki şeyin aynı anda ölçülmesi olarak düşünebilirsiniz.

222
00:12:54,380 --> 00:12:57,420
Bunlardan ilki dağılımın ne kadar düz olduğudur.

223
00:12:57,420 --> 00:13:01,700
Dağılım düzgünlüğe ne kadar yakınsa entropi o kadar yüksek olur.

224
00:13:01,700 --> 00:13:05,070
Bizim durumumuzda, 3 üzeri 5&#39;lik toplam örüntülerin olduğu durumda,

225
00:13:05,070 --> 00:13:08,300
düzgün bir dağılım için, bunlardan herhangi birinin gözlemlenmesi, 3

226
00:13:08,300 --> 00:13:11,858
üzeri 5&#39;lik bilgi günlüğü tabanı 2&#39;ye sahip olacaktır; bu da 7 olur.

227
00:13:11,858 --> 00:13:17,860
92, yani bu entropi için sahip olabileceğiniz mutlak maksimum değer budur.

228
00:13:17,860 --> 00:13:22,900
Ancak entropi aynı zamanda ilk etapta ne kadar olasılığın bulunduğunun da bir ölçüsüdür.

229
00:13:22,900 --> 00:13:27,637
Örneğin, yalnızca 16 olası örüntünün olduğu ve her birinin eşit olasılığa

230
00:13:27,637 --> 00:13:32,760
sahip olduğu bir kelimeniz varsa, bu entropi, bu beklenen bilgi 4 bit olacaktır.

231
00:13:32,760 --> 00:13:37,088
Ancak 64 olası örüntünün ortaya çıkabileceği başka bir kelimeniz varsa ve bunların

232
00:13:37,088 --> 00:13:41,000
hepsi eşit derecede olasıysa, o zaman entropi 6 bit olarak hesaplanacaktır.

233
00:13:41,000 --> 00:13:45,733
Yani, eğer doğada 6 bitlik bir entropiye sahip bir dağılım görürseniz,

234
00:13:45,733 --> 00:13:50,200
bu sanki 64 eşit olasılıklı sonuç varmış gibi, olacaklar konusunda

235
00:13:50,200 --> 00:13:54,400
çok fazla değişkenlik ve belirsizlik olduğunu söylemek gibidir.

236
00:13:54,400 --> 00:13:58,360
Wurtelebot&#39;a ilk geçişimde temelde bunu yapmasını sağladım.

237
00:13:58,360 --> 00:14:02,820
Yapabileceğiniz tüm olası tahminleri, yani 13.000 kelimenin tamamını gözden

238
00:14:02,820 --> 00:14:07,163
geçirir, her biri için entropiyi veya daha spesifik olarak, her biri için

239
00:14:07,163 --> 00:14:11,682
görebileceğiniz tüm kalıplar arasındaki dağılımın entropisini hesaplar ve en

240
00:14:11,682 --> 00:14:16,319
yüksek olanı seçer, çünkü bu Olasılık alanınızı mümkün olduğu kadar daraltması

241
00:14:16,319 --> 00:14:17,200
muhtemel olanı.

242
00:14:17,200 --> 00:14:19,416
Burada sadece ilk tahminden bahsetmiş olsam da

243
00:14:19,416 --> 00:14:21,680
sonraki birkaç tahmin için de aynı şeyi yapıyor.

244
00:14:21,680 --> 00:14:25,147
Örneğin, ilk tahminde, hangi kelimeyle eşleştiğine bağlı olarak

245
00:14:25,147 --> 00:14:28,886
sizi daha az sayıda olası kelimeyle sınırlayacak bir model gördükten

246
00:14:28,886 --> 00:14:32,300
sonra, aynı oyunu o daha küçük kelime kümesine göre oynarsınız.

247
00:14:32,300 --> 00:14:36,649
Önerilen ikinci bir tahmin için, bu daha kısıtlı kelime grubundan

248
00:14:36,649 --> 00:14:41,196
oluşabilecek tüm kalıpların dağılımına bakarsınız, 13.000 olasılığın

249
00:14:41,196 --> 00:14:45,480
tamamını ararsınız ve bu entropiyi maksimuma çıkaranı bulursunuz.

250
00:14:45,480 --> 00:14:50,186
Bunun nasıl çalıştığını size göstermek için, kenarlarda bu analizin önemli noktalarını

251
00:14:50,186 --> 00:14:54,460
gösteren, yazdığım Wurtele&#39;nin küçük bir versiyonunu ele almama izin verin.

252
00:14:54,460 --> 00:14:57,284
Tüm entropi hesaplamalarını yaptıktan sonra sağ tarafta bize

253
00:14:57,284 --> 00:15:00,340
hangilerinin en yüksek beklenen bilgiye sahip olduğunu gösteriyor.

254
00:15:00,340 --> 00:15:05,740
En azından şimdilik en önemli cevabın Tares olduğu ortaya

255
00:15:05,740 --> 00:15:11,140
çıktı, bu da tabii ki fiğ, en yaygın fiğ anlamına geliyor.

256
00:15:11,140 --> 00:15:14,610
Burada her tahmin yaptığımızda, belki de önerilerini göz ardı edip slate&#39;i

257
00:15:14,610 --> 00:15:17,862
tercih ederim, çünkü slate&#39;i severim, ne kadar beklenen bilgiye sahip

258
00:15:17,862 --> 00:15:21,465
olduğunu görebiliriz, ancak burada kelimenin sağında bize ne kadar bilgi olduğunu

259
00:15:21,465 --> 00:15:24,980
gösteriyor. Bu özel model göz önüne alındığında, elde ettiğimiz gerçek bilgiler.

260
00:15:24,980 --> 00:15:27,932
Yani burada biraz şanssızız gibi görünüyor, 5 almamız bekleniyordu.

261
00:15:27,932 --> 00:15:30,660
8, ama bundan daha azına sahip bir şey elde ettik.

262
00:15:30,660 --> 00:15:33,179
Ve sol tarafta, şu anda bulunduğumuz yere göre

263
00:15:33,179 --> 00:15:35,860
bize mümkün olan tüm farklı kelimeleri gösteriyor.

264
00:15:35,860 --> 00:15:38,745
Mavi çubuklar bize her kelimenin ne kadar olası olduğunu düşündüğünü

265
00:15:38,745 --> 00:15:41,254
gösteriyor; dolayısıyla şu anda her kelimenin eşit derecede

266
00:15:41,254 --> 00:15:44,140
gerçekleşme olasılığını varsayıyor, ancak bunu birazdan düzelteceğiz.

267
00:15:44,140 --> 00:15:48,130
Ve bu belirsizlik ölçümü bize bu dağılımın olası kelimeler arasındaki

268
00:15:48,130 --> 00:15:51,892
entropisini anlatıyor; bu şu anda tekdüze bir dağılım olduğu için

269
00:15:51,892 --> 00:15:55,940
olasılıkların sayısını saymanın gereksiz derecede karmaşık bir yoludur.

270
00:15:55,940 --> 00:15:59,343
Örneğin 2 üssü 13&#39;ü alırsak.

271
00:15:59,343 --> 00:16:02,700
66, bu 13.000 olasılık civarında olmalı.

272
00:16:02,700 --> 00:16:06,780
Burada biraz yanlışım var ama bunun nedeni ondalık basamakların tamamını göstermemem.

273
00:16:06,780 --> 00:16:09,696
Şu anda bu size gereksiz gelebilir ve işleri aşırı derecede karmaşık hale getirebilir,

274
00:16:09,696 --> 00:16:12,344
ancak bir dakika içinde her iki sayıya da sahip olmanın neden yararlı olduğunu

275
00:16:12,344 --> 00:16:12,780
göreceksiniz.

276
00:16:12,780 --> 00:16:16,366
Yani burada ikinci tahminimiz için en yüksek entropinin Ramen olduğunu

277
00:16:16,366 --> 00:16:19,700
öne sürüyor gibi görünüyor ki bu da yine tek kelime gibi gelmiyor.

278
00:16:19,700 --> 00:16:25,660
Burada ahlaki açıdan yüksek bir yer edinmek için, devam edip Rains yazacağım.

279
00:16:25,660 --> 00:16:27,540
Ve yine biraz şanssızmışız gibi görünüyor.

280
00:16:27,540 --> 00:16:28,872
Biz 4 bekliyorduk.

281
00:16:28,872 --> 00:16:30,556
3 bit ve elimizde sadece 3 var.

282
00:16:30,556 --> 00:16:32,100
39 bit bilgi.

283
00:16:32,100 --> 00:16:35,060
Bu da bizi 55 olasılığa indiriyor.

284
00:16:35,060 --> 00:16:37,807
Ve burada belki de aslında onun önerdiği şeyle, yani birleşik

285
00:16:37,807 --> 00:16:40,200
olanla, her ne anlama geliyorsa onunla devam edeceğim.

286
00:16:40,200 --> 00:16:43,300
Ve tamam, bu aslında bir bulmaca için iyi bir şans.

287
00:16:43,300 --> 00:16:45,718
Bu modelin bize 4 verdiğini söylüyor.

288
00:16:45,718 --> 00:16:47,020
7 bitlik bilgi.

289
00:16:47,020 --> 00:16:50,990
Ama sol tarafta, bu modeli görmeden önce 5 tane vardı.

290
00:16:50,990 --> 00:16:52,400
78 bitlik belirsizlik.

291
00:16:52,400 --> 00:16:56,860
Peki sizin için bir test olarak, kalan olasılıkların sayısı ne anlama geliyor?

292
00:16:56,860 --> 00:17:00,917
Bu, bir miktar belirsizliğe indirgendiğimiz anlamına gelir

293
00:17:00,917 --> 00:17:04,700
ki bu, iki olası yanıt olduğunu söylemekle aynı şeydir.

294
00:17:04,700 --> 00:17:06,520
Bu 50-50&#39;lik bir seçim.

295
00:17:06,520 --> 00:17:08,817
Ve buradan yola çıkarak, sen ve ben hangi kelimelerin daha yaygın

296
00:17:08,817 --> 00:17:11,220
olduğunu bildiğimiz için cevabın uçurum olması gerektiğini biliyoruz.

297
00:17:11,220 --> 00:17:13,540
Ancak şu anda yazıldığı gibi, program bunu bilmiyor.

298
00:17:13,540 --> 00:17:17,009
Böylece tek bir olasılık kalana kadar mümkün olduğu kadar

299
00:17:17,009 --> 00:17:20,360
çok bilgi toplamaya devam eder ve sonra onu tahmin eder.

300
00:17:20,360 --> 00:17:22,700
Açıkçası daha iyi bir oyunsonu stratejisine ihtiyacımız var.

301
00:17:22,700 --> 00:17:26,692
Ancak diyelim ki bu sürüme kelime çözücülerimizden biri adını verdik ve

302
00:17:26,692 --> 00:17:30,740
sonra gidip nasıl çalıştığını görmek için bazı simülasyonlar çalıştırdık.

303
00:17:30,740 --> 00:17:34,240
Yani bunun çalışma şekli mümkün olan her kelime oyununu oynamaktır.

304
00:17:34,240 --> 00:17:38,780
Gerçek wordle cevapları olan 2315 kelimenin tamamının üzerinden geçiyor.

305
00:17:38,780 --> 00:17:41,340
Temel olarak bunu bir test seti olarak kullanıyor.

306
00:17:41,340 --> 00:17:44,305
Ve bir kelimenin ne kadar yaygın olduğunu dikkate almamak ve

307
00:17:44,305 --> 00:17:47,368
tek ve tek bir seçeneğe varıncaya kadar yol boyunca her adımda

308
00:17:47,368 --> 00:17:50,480
bilgiyi en üst düzeye çıkarmaya çalışmak gibi naif bir yöntemle.

309
00:17:50,480 --> 00:17:54,912
Simülasyonun sonunda ortalama puan 4 civarında çıkıyor.

310
00:17:54,912 --> 00:17:55,100
124.

311
00:17:55,100 --> 00:17:59,780
Ki bu fena değil, dürüst olmak gerekirse, daha kötüsünü bekliyordum.

312
00:17:59,780 --> 00:18:03,040
Ancak wordle oynayanlar size genellikle 4&#39;te alabileceklerini söyleyecektir.

313
00:18:03,040 --> 00:18:05,260
Asıl zorluk 3&#39;te mümkün olduğunca çok sayıda elde etmektir.

314
00:18:05,260 --> 00:18:08,920
4&#39;lük skor ile 3&#39;lük skor arasında oldukça büyük bir sıçrama var.

315
00:18:08,920 --> 00:18:16,040
Buradaki bariz düşük sonuç, bir kelimenin yaygın olup olmadığını

316
00:18:16,040 --> 00:18:23,160
ve bunu tam olarak nasıl yapacağımızı bir şekilde dahil etmektir.

317
00:18:23,160 --> 00:18:26,050
Benim yaklaşımım İngilizce dilindeki tüm kelimelerin

318
00:18:26,050 --> 00:18:28,560
göreceli frekanslarının bir listesini almaktı.

319
00:18:28,560 --> 00:18:32,091
Ve az önce Mathematica&#39;nın Google Kitaplar İngilizce Ngram genel

320
00:18:32,091 --> 00:18:35,520
veri kümesinden alınan kelime frekansı veri fonksiyonunu kullandım.

321
00:18:35,520 --> 00:18:37,694
Ve buna bakmak oldukça eğlenceli, örneğin en yaygın

322
00:18:37,694 --> 00:18:40,120
sözcüklerden en az kullanılan sözcüklere doğru sıralarsak.

323
00:18:40,120 --> 00:18:43,740
Açıkçası bunlar İngilizce dilinde en yaygın 5 harfli kelimelerdir.

324
00:18:43,740 --> 00:18:46,480
Daha doğrusu bunlar en yaygın 8&#39;incisidir.

325
00:18:46,480 --> 00:18:49,440
İlki hangisi, sonrasında orası ve orası var.

326
00:18:49,440 --> 00:18:52,458
Birincinin kendisi birinci değil, 9&#39;uncudur ve bu diğer

327
00:18:52,458 --> 00:18:55,628
kelimelerin daha sık ortaya çıkabileceği, ilk gelenlerin sonra

328
00:18:55,628 --> 00:18:59,000
olduğu ve nerede olduğu ve biraz daha az yaygın olduğu mantıklıdır.

329
00:18:59,000 --> 00:19:03,203
Şimdi, bu verileri, bu kelimelerin her birinin nihai cevap olma olasılığını

330
00:19:03,203 --> 00:19:07,020
modellemek için kullanırken, bu sadece sıklıkla orantılı olmamalıdır.

331
00:19:07,020 --> 00:19:09,596
Örneğin 0 puan verilir.

332
00:19:09,596 --> 00:19:15,200
Bu veri setinde 002 var, oysa örgü kelimesinin olasılığı bir anlamda 1000 kat daha az.

333
00:19:15,200 --> 00:19:17,181
Ancak bunların her ikisi de, neredeyse kesinlikle

334
00:19:17,181 --> 00:19:19,400
dikkate alınmaya değer olacak kadar yaygın kelimelerdir.

335
00:19:19,400 --> 00:19:21,900
Bu yüzden daha fazla ikili kesinti istiyoruz.

336
00:19:21,900 --> 00:19:25,996
Bu konuda izlediğim yol, tüm bu sıralanmış kelime listesini alıp, bunu bir x

337
00:19:25,996 --> 00:19:30,359
eksenine göre düzenlediğimi ve ardından çıktısı temelde ikili olan bir fonksiyona

338
00:19:30,359 --> 00:19:34,722
sahip olmanın standart yolu olan sigmoid fonksiyonunu uyguladığımı hayal etmekti.

339
00:19:34,722 --> 00:19:38,500
ya 0 ya da 1, ancak bu belirsizlik bölgesi için arada bir yumuşama var.

340
00:19:38,500 --> 00:19:44,086
Yani esas olarak, her bir kelimeye son listede yer almaları için atadığım olasılık,

341
00:19:44,086 --> 00:19:49,540
x ekseninde nerede olursa olsun yukarıdaki sigmoid fonksiyonunun değeri olacaktır.

342
00:19:49,540 --> 00:19:54,010
Açıkçası bu birkaç parametreye bağlıdır; örneğin, bu kelimelerin x ekseni üzerinde ne

343
00:19:54,010 --> 00:19:58,377
kadar geniş bir alanı dolduracağı, 1&#39;den 0&#39;a ne kadar kademeli veya dik bir

344
00:19:58,377 --> 00:20:02,692
şekilde düştüğümüzü belirler ve onları soldan sağa nereye yerleştirdiğimiz kesmeyi

345
00:20:02,692 --> 00:20:03,160
belirler.

346
00:20:03,160 --> 00:20:07,340
Dürüst olmak gerekirse, bunu yapma şeklim sadece parmağımı yalayıp rüzgara doğru tutmaktı.

347
00:20:07,340 --> 00:20:10,755
Sıralanmış listeye baktım ve bir pencere bulmaya çalıştım; ona baktığımda

348
00:20:10,755 --> 00:20:13,894
bu kelimelerin yaklaşık yarısının son cevap olma ihtimalinin olmama

349
00:20:13,894 --> 00:20:17,680
ihtimalinden daha yüksek olduğunu düşündüm ve bunu kesme noktası olarak kullandım.

350
00:20:17,680 --> 00:20:20,805
Kelimeler arasında böyle bir dağılıma sahip olduğumuzda, bu bize

351
00:20:20,805 --> 00:20:24,460
entropinin gerçekten yararlı bir ölçüm haline geldiği başka bir durum verir.

352
00:20:24,460 --> 00:20:29,164
Örneğin, diyelim ki bir oyun oynuyoruz ve tüy ve çivilerden oluşan eski açılışlarımla

353
00:20:29,164 --> 00:20:33,760
başlıyoruz ve onunla eşleşen dört olası kelimenin olduğu bir durumla karşılaşıyoruz.

354
00:20:33,760 --> 00:20:36,440
Ve diyelim ki hepsinin eşit derecede olası olduğunu düşünüyoruz.

355
00:20:36,440 --> 00:20:40,000
Size şunu sorayım, bu dağılımın entropisi nedir?

356
00:20:40,000 --> 00:20:45,657
Bu olasılıkların her biriyle ilgili bilgi, 2/4&#39;ün logaritması

357
00:20:45,657 --> 00:20:50,800
olacaktır, çünkü her biri 1 ve 4&#39;tür ve bu da 2&#39;dir.

358
00:20:50,800 --> 00:20:52,780
İki bit bilgi, dört olasılık.

359
00:20:52,780 --> 00:20:54,360
Hepsi çok iyi ve güzel.

360
00:20:54,360 --> 00:20:58,320
Peki ya size aslında dörtten fazla eşleşme olduğunu söylesem?

361
00:20:58,320 --> 00:21:02,600
Gerçekte kelime listesinin tamamına baktığımızda onunla eşleşen 16 kelime var.

362
00:21:02,600 --> 00:21:06,843
Ancak modelimizin, diğer 12 kelimenin aslında nihai cevap olma ihtimalini gerçekten

363
00:21:06,843 --> 00:21:10,833
düşük tuttuğunu varsayalım; 1000&#39;de 1 gibi bir şey, çünkü bunlar gerçekten

364
00:21:10,833 --> 00:21:11,440
belirsizdir.

365
00:21:11,440 --> 00:21:15,480
Şimdi size şunu sorayım, bu dağılımın entropisi nedir?

366
00:21:15,480 --> 00:21:18,904
Eğer entropi burada sadece eşleşme sayısını ölçüyorsa, o zaman bunun

367
00:21:18,904 --> 00:21:22,428
16&#39;nın logaritması 2 gibi bir şey olmasını bekleyebilirsiniz ki bu

368
00:21:22,428 --> 00:21:26,200
da 4 olur, daha önce sahip olduğumuz belirsizlikten iki bit daha fazla olur.

369
00:21:26,200 --> 00:21:30,320
Ancak elbette gerçek belirsizlik daha önce yaşadığımızdan çok da farklı değil.

370
00:21:30,320 --> 00:21:34,381
Bu 12 belirsiz kelimenin var olması, örneğin son cevabın çekicilik

371
00:21:34,381 --> 00:21:38,200
olduğunu öğrenmenin çok daha şaşırtıcı olacağı anlamına gelmez.

372
00:21:38,200 --> 00:21:42,104
Yani buradaki hesaplamayı gerçekten yaptığınızda ve her bir olayın olasılığını

373
00:21:42,104 --> 00:21:45,514
karşılık gelen bilgilerle topladığınızda elde ettiğiniz sonuç 2 olur.

374
00:21:45,514 --> 00:21:45,960
11 bit.

375
00:21:45,960 --> 00:21:49,609
Sadece şunu söylüyorum, temelde iki parça, temelde bu dört olasılık,

376
00:21:49,609 --> 00:21:53,417
ancak tüm bu pek olası olmayan olaylar nedeniyle biraz daha belirsizlik

377
00:21:53,417 --> 00:21:57,120
var, gerçi bunları öğrenmiş olsaydınız bundan bir ton bilgi alırsınız.

378
00:21:57,120 --> 00:21:59,415
Yani uzaklaştırma, Wordle&#39;u bilgi teorisi dersi

379
00:21:59,415 --> 00:22:01,800
için bu kadar güzel bir örnek yapan şeyin bir parçası.

380
00:22:01,800 --> 00:22:05,280
Entropi için iki ayrı duygu uygulamasına sahibiz.

381
00:22:05,280 --> 00:22:10,848
Birincisi bize belirli bir tahminden alacağımız beklenen bilginin ne olduğunu söylüyor,

382
00:22:10,848 --> 00:22:16,480
ikincisi ise mümkün olan tüm kelimeler arasında kalan belirsizliği ölçebilir miyiz diyor.

383
00:22:16,480 --> 00:22:20,740
Ve şunu vurgulamalıyım ki, bir tahminin beklenen bilgisine baktığımız ilk durumda,

384
00:22:20,740 --> 00:22:25,000
kelimelere eşit olmayan bir ağırlık verdiğimizde, bu entropi hesaplamasını etkiler.

385
00:22:25,000 --> 00:22:29,694
Örneğin, daha önce Weary ile ilişkili dağıtıma baktığımız aynı durumu ele alayım,

386
00:22:29,694 --> 00:22:34,560
ancak bu sefer tüm olası kelimeler arasında tekdüze olmayan bir dağılım kullanıyorum.

387
00:22:34,560 --> 00:22:39,360
Bakalım burada bunu oldukça iyi gösteren bir bölüm bulabilecek miyim?

388
00:22:39,360 --> 00:22:42,480
Tamam, işte bu oldukça iyi.

389
00:22:42,480 --> 00:22:45,865
Burada birbirine eşit olasılıklara sahip iki bitişik modelimiz var, ancak

390
00:22:45,865 --> 00:22:49,480
bize söylenenlerden birinin kendisiyle eşleşen 32 olası kelime olduğu söylendi.

391
00:22:49,480 --> 00:22:52,664
Ve bunların ne olduğunu kontrol edersek, bunlar şu 32 kelimedir, gözlerinizi

392
00:22:52,664 --> 00:22:55,600
üzerlerine taradığınızda bunların hepsi pek olası olmayan kelimelerdir.

393
00:22:55,600 --> 00:23:00,391
Belki bağırmak gibi akla yatkın cevaplar bulmak zordur, ancak dağılımdaki komşu düzene

394
00:23:00,391 --> 00:23:05,018
bakarsak, ki bu da hemen hemen aynı olası kabul edilir, bize sadece 8 olası eşleşme

395
00:23:05,018 --> 00:23:09,920
olduğu söylendi, yani çeyrek olarak birçok eşleşme var, ancak bu da bir o kadar muhtemel.

396
00:23:09,920 --> 00:23:12,520
Ve bu kibritleri çıkardığımızda nedenini görebiliriz.

397
00:23:12,520 --> 00:23:17,840
Bunlardan bazıları, zil sesi, gazap veya tecavüz gibi gerçekten makul yanıtlardır.

398
00:23:17,840 --> 00:23:21,900
Tüm bunları nasıl dahil ettiğimizi göstermek için, burada Wordlebot&#39;un 2.

399
00:23:21,900 --> 00:23:25,960
versiyonunu ele almama izin verin; ilk gördüğümüzden iki veya üç ana fark var.

400
00:23:25,960 --> 00:23:30,675
Öncelikle, az önce söylediğim gibi, bu entropileri, bu beklenen bilgi değerlerini

401
00:23:30,675 --> 00:23:34,757
hesaplama yöntemimiz, artık belirli bir kelimenin gerçekten cevap olma

402
00:23:34,757 --> 00:23:39,300
olasılığını da içeren kalıplar arasındaki daha hassas dağılımları kullanmaktır.

403
00:23:39,300 --> 00:23:44,160
Aslına bakılırsa gözyaşları hala 1 numara, ancak sonrakiler biraz farklı.

404
00:23:44,160 --> 00:23:47,801
İkincisi, en çok tercih edilenleri sıraladığında, artık her kelimenin asıl

405
00:23:47,801 --> 00:23:51,636
cevap olma ihtimaline ilişkin bir model tutacak ve bunu kararına dahil edecek;

406
00:23:51,636 --> 00:23:55,520
bunu, konuyla ilgili birkaç tahminimiz olduğunda görmek daha kolay olacak. masa.

407
00:23:55,520 --> 00:23:58,728
Yine tavsiyelerini göz ardı ediyoruz çünkü makinelerin

408
00:23:58,728 --> 00:24:01,120
hayatlarımızı yönetmesine izin veremeyiz.

409
00:24:01,120 --> 00:24:05,706
Ve sanırım burada solda farklı olan başka bir şeyden bahsetmem gerekiyor; belirsizlik

410
00:24:05,706 --> 00:24:10,080
değeri, yani bit sayısı, artık sadece olası eşleşmelerin sayısıyla gereksiz değil.

411
00:24:10,080 --> 00:24:13,489
Şimdi yukarı çekip 2 üzeri 8&#39;i hesaplarsak.

412
00:24:13,489 --> 00:24:18,981
02, ki bu da 256&#39;nın, sanırım 259&#39;un biraz üzerinde, aslında bu kalıpla

413
00:24:18,981 --> 00:24:24,267
eşleşen toplam 526 kelime olmasına rağmen, sahip olduğu belirsizlik miktarı,

414
00:24:24,267 --> 00:24:29,760
eşit derecede olası 259 kelime olsaydı ne olacağına daha çok benziyor. sonuçlar.

415
00:24:29,760 --> 00:24:31,100
Bunu şöyle düşünebilirsiniz.

416
00:24:31,100 --> 00:24:34,396
Borx&#39;un cevap olmadığını biliyor, yorts, zorl ve zorus için de

417
00:24:34,396 --> 00:24:37,840
aynısı geçerli, dolayısıyla önceki duruma göre biraz daha az belirsiz.

418
00:24:37,840 --> 00:24:40,220
Bu bit sayısı daha az olacaktır.

419
00:24:40,220 --> 00:24:44,266
Ve eğer oyunu oynamaya devam edersem, burada açıklamak

420
00:24:44,266 --> 00:24:48,680
istediğim şeye uygun birkaç tahminle bunu detaylandıracağım.

421
00:24:48,680 --> 00:24:51,182
Dördüncü tahmine göre, eğer en çok tercih edilenlere bakarsanız,

422
00:24:51,182 --> 00:24:53,800
bunun artık sadece entropiyi maksimuma çıkarmadığını görebilirsiniz.

423
00:24:53,800 --> 00:24:57,150
Yani bu noktada teknik olarak yedi olasılık var

424
00:24:57,150 --> 00:25:00,780
ama anlamlı şansı olan tek şey yurtlar ve kelimeler.

425
00:25:00,780 --> 00:25:04,114
Ve her ikisini de seçmenin bu diğer değerlerin üzerinde yer

426
00:25:04,114 --> 00:25:07,560
aldığını, açıkçası daha fazla bilgi vereceğini görebilirsiniz.

427
00:25:07,560 --> 00:25:11,145
Bunu ilk yaptığımda, her tahminin kalitesini ölçmek için bu iki sayıyı

428
00:25:11,145 --> 00:25:14,580
topladım ve bu aslında tahmin edebileceğinizden daha iyi işe yaradı.

429
00:25:14,580 --> 00:25:17,515
Ama bu pek sistematik gelmedi ve eminim ki insanların benimseyebileceği

430
00:25:17,515 --> 00:25:19,880
başka yaklaşımlar da vardır, ama ben bu yaklaşıma ulaştım.

431
00:25:19,880 --> 00:25:23,956
Bir sonraki tahmin olasılığını düşünürsek, bu durumda kelimeler gibi,

432
00:25:23,956 --> 00:25:28,440
gerçekten umursadığımız şey, eğer bunu yaparsak oyunumuzun beklenen puanıdır.

433
00:25:28,440 --> 00:25:32,088
Beklenen puanı hesaplamak için de kelimelerin gerçek cevap olma

434
00:25:32,088 --> 00:25:36,080
olasılığının ne olduğunu söylüyoruz ki bu şu anda %58&#39;i açıklıyor.

435
00:25:36,080 --> 00:25:40,400
Bu maçta puanımızın %58 ihtimalle 4 olacağını söylüyoruz.

436
00:25:40,400 --> 00:25:46,240
Ve 1 eksi %58 olasılıkla puanımız 4&#39;ten fazla olacaktır.

437
00:25:46,240 --> 00:25:49,454
Daha ne kadarını bilmiyoruz ama o noktaya geldiğimizde ne kadar

438
00:25:49,454 --> 00:25:52,920
belirsizliğin ortaya çıkabileceğine dayanarak bunu tahmin edebiliriz.

439
00:25:52,920 --> 00:25:55,227
Özellikle şu anda 1 tane var.

440
00:25:55,227 --> 00:25:56,600
44 bit belirsizlik.

441
00:25:56,600 --> 00:26:01,131
Kelimeleri tahmin edersek, bu bize alacağımız beklenen bilginin 1 olduğunu söyler.

442
00:26:01,131 --> 00:26:01,560
27 bit.

443
00:26:01,560 --> 00:26:04,920
Yani kelimeleri tahmin edersek, bu fark, bu olay gerçekleştikten

444
00:26:04,920 --> 00:26:08,280
sonra ne kadar belirsizlikle baş başa kalacağımızı temsil ediyor.

445
00:26:08,280 --> 00:26:11,244
İhtiyacımız olan şey, burada f adını verdiğim, bu belirsizliği

446
00:26:11,244 --> 00:26:13,880
beklenen bir puanla ilişkilendiren bir tür fonksiyondur.

447
00:26:13,880 --> 00:26:18,160
Ve bunu gerçekleştirmenin yolu, botun 1. versiyonuna dayalı olarak

448
00:26:18,160 --> 00:26:23,015
önceki oyunlardan bir grup veriyi çizerek, çok ölçülebilir belirsizliklerle

449
00:26:23,015 --> 00:26:27,040
çeşitli noktalardan sonra gerçek puanın ne olduğunu söylemekti.

450
00:26:27,040 --> 00:26:31,073
Örneğin, buradaki veri noktaları 8 civarındaki bir değerin üzerinde duruyor.

451
00:26:31,073 --> 00:26:35,426
8&#39;in olduğu bir noktadan sonra bazı oyunlar için 7 ya da öylesine diyorlar.

452
00:26:35,426 --> 00:26:39,340
7 bitlik belirsizlik, nihai cevaba ulaşmak için iki tahmin yapılması gerekti.

453
00:26:39,340 --> 00:26:43,180
Diğer oyunlar için üç tahmin gerekiyordu, diğer oyunlar için ise dört tahmin gerekiyordu.

454
00:26:43,180 --> 00:26:47,059
Burada sola kayarsak, sıfırın üzerindeki tüm noktalar, ne zaman

455
00:26:47,059 --> 00:26:50,817
sıfır belirsizlik varsa, yani tek bir olasılık varsa, o zaman

456
00:26:50,817 --> 00:26:55,000
gereken tahmin sayısı her zaman sadece birdir, bu da güven vericidir.

457
00:26:55,000 --> 00:26:59,301
Ne zaman bir miktar belirsizlik olsa, yani esasen iki olasılığa

458
00:26:59,301 --> 00:27:03,940
bağlıysa bazen bir tahmin daha, bazen de iki tahmin daha gerekiyordu.

459
00:27:03,940 --> 00:27:05,980
Burada da böyle devam ediyor.

460
00:27:05,980 --> 00:27:08,500
Belki bu verileri görselleştirmenin biraz daha kolay bir

461
00:27:08,500 --> 00:27:11,020
yolu, bunları bir araya toplayıp ortalamalarını almaktır.

462
00:27:11,020 --> 00:27:16,915
Örneğin buradaki çubuk, bir miktar belirsizliğimizin olduğu tüm noktalar arasında

463
00:27:16,915 --> 00:27:22,308
ortalama olarak gereken yeni tahmin sayısının yaklaşık 1 olduğunu söylüyor.

464
00:27:22,308 --> 00:27:22,420
5.

465
00:27:22,420 --> 00:27:26,919
Ve buradaki çubuk, tüm farklı oyunlar arasında bir noktada belirsizliğin dört bitin

466
00:27:26,919 --> 00:27:31,633
biraz üzerinde olduğunu söylüyor, bu da onu 16 farklı olasılığa daraltmak gibi, o zaman

467
00:27:31,633 --> 00:27:36,240
ortalama olarak o noktadan itibaren ikiden biraz daha fazla tahmin gerektiriyor ileri.

468
00:27:36,240 --> 00:27:40,080
Ve buradan itibaren buna makul görünen bir fonksiyona uyacak bir regresyon yaptım.

469
00:27:40,080 --> 00:27:44,630
Ve unutmayın, bunları yapmanın asıl amacı, bir kelimeden ne kadar çok bilgi

470
00:27:44,630 --> 00:27:49,720
kazanırsak beklenen puanın o kadar düşük olacağı şeklindeki bu sezgiyi ölçebilmektir.

471
00:27:49,720 --> 00:27:51,043
Yani bu sürüm 2 olarak.

472
00:27:51,043 --> 00:27:55,467
0&#39;a dönersek ve aynı simülasyon setini çalıştırırsak, 2315

473
00:27:55,467 --> 00:27:59,820
olası sözcük yanıtının tümüne karşı oynatırsak, bu nasıl olur?

474
00:27:59,820 --> 00:28:04,060
İlk versiyonumuzun aksine kesinlikle daha iyi, bu da güven verici.

475
00:28:04,060 --> 00:28:06,284
Tüm söylenen ve yapılan ortalama 3 civarındadır.

476
00:28:06,284 --> 00:28:09,473
6, ilk versiyondan farklı olarak birkaç kez kaybettiği ve bu

477
00:28:09,473 --> 00:28:12,820
durumda altıdan fazlasını gerektirdiği durumlar olmasına rağmen.

478
00:28:12,820 --> 00:28:15,819
Muhtemelen bilgiyi en üst düzeye çıkarmak yerine hedefe

479
00:28:15,819 --> 00:28:18,980
ulaşmak için bu ödünleşimin yapıldığı zamanlar olduğu için.

480
00:28:18,980 --> 00:28:22,022
Peki 3&#39;ten daha iyisini yapabilir miyiz?

481
00:28:22,022 --> 00:28:22,140
6?

482
00:28:22,140 --> 00:28:23,260
Kesinlikle yapabiliriz.

483
00:28:23,260 --> 00:28:26,333
Başlangıçta, kelime cevaplarının gerçek listesini modelini

484
00:28:26,333 --> 00:28:29,980
oluşturma biçimine dahil etmemenin çok eğlenceli olduğunu söylemiştim.

485
00:28:29,980 --> 00:28:35,043
Ancak bunu dahil edersek alabileceğim en iyi performans 3 civarındaydı.

486
00:28:35,043 --> 00:28:35,180
43.

487
00:28:35,180 --> 00:28:37,864
Dolayısıyla, bu önceki dağılımı seçmek için kelime sıklığı

488
00:28:37,864 --> 00:28:40,957
verilerini kullanmaktan daha karmaşık hale gelmeye çalışırsak, bu 3.

489
00:28:40,957 --> 00:28:43,777
43 muhtemelen bunda ne kadar başarılı olabileceğimizin veya en azından

490
00:28:43,777 --> 00:28:46,360
benim bunda ne kadar başarılı olabileceğimin maksimumunu veriyor.

491
00:28:46,360 --> 00:28:50,957
Bu en iyi performans aslında sadece burada bahsettiğim fikirleri kullanır, ancak biraz

492
00:28:50,957 --> 00:28:55,660
daha ileri gider, sanki beklenen bilgiyi tek bir adım yerine iki adım ileriye doğru arar.

493
00:28:55,660 --> 00:28:58,161
Başlangıçta bunun hakkında daha fazla konuşmayı planlıyordum

494
00:28:58,161 --> 00:29:00,580
ama aslında oldukça uzun bir yol kat ettiğimizi fark ettim.

495
00:29:00,580 --> 00:29:03,456
Söyleyeceğim tek şey, bu iki adımlı aramayı yaptıktan ve ardından en

496
00:29:03,456 --> 00:29:06,373
iyi adaylar üzerinde birkaç örnek simülasyon çalıştırdıktan sonra, şu

497
00:29:06,373 --> 00:29:09,500
ana kadar benim için en azından Crane&#39;in en iyi açıcı olduğu görünüyor.

498
00:29:09,500 --> 00:29:11,080
Kim tahmin ederdi?

499
00:29:11,080 --> 00:29:14,403
Ayrıca olasılıklar alanınızı belirlemek için gerçek sözcük listesini

500
00:29:14,403 --> 00:29:18,160
kullanırsanız, o zaman başlangıçtaki belirsizlik 11 bitin biraz üzerinde olur.

501
00:29:18,160 --> 00:29:22,214
Ve sadece kaba kuvvet aramasından sonra, ilk iki tahminden sonra

502
00:29:22,214 --> 00:29:26,580
beklenen maksimum olası bilginin 10 bit civarında olduğu ortaya çıktı.

503
00:29:26,580 --> 00:29:30,900
Bu da en iyi senaryoda, ilk iki tahmininizden sonra, mükemmel derecede optimal

504
00:29:30,900 --> 00:29:35,220
bir oyunla, yaklaşık bir miktar belirsizlikle baş başa kalacağınızı gösteriyor.

505
00:29:35,220 --> 00:29:37,400
Bu, iki olası tahminde bulunmakla aynı şeydir.

506
00:29:37,400 --> 00:29:40,935
Bu yüzden, bu ortalamayı 3&#39;e kadar düşüren bir algoritmayı asla yazamayacağınızı

507
00:29:40,935 --> 00:29:44,179
söylemek adil ve muhtemelen oldukça muhafazakar olur çünkü kullanabileceğiniz

508
00:29:44,179 --> 00:29:47,257
kelimelerle, sadece iki adımdan sonra yeterli bilgiyi elde etmek için yer

509
00:29:47,257 --> 00:29:50,460
yoktur. her seferinde üçüncü slottaki cevabı hatasız olarak garanti edebilir.

