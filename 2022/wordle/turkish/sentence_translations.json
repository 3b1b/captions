[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy.",
  "translatedText": "Wurdle oyunu son bir iki ayda oldukça viral hale geldi ve hiçbir zaman bir matematik dersi fırsatını gözden kaçıran biri olmadı. Bana öyle geliyor ki bu oyun bilgi teorisi ile ilgili bir derste çok iyi bir merkezi örnek teşkil ediyor ve özellikle de entropi olarak bilinen bir konu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could.",
  "translatedText": "Görüyorsunuz, pek çok insan gibi ben de bulmacanın içine kapıldım ve birçok programcı gibi ben de oyunu olabildiğince optimum şekilde oynatacak bir algoritma yazmaya kapıldım.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy.",
  "translatedText": "Ve burada yapmayı düşündüğüm şey, sizinle bu konudaki sürecimin bir kısmını konuşmak ve bunun içine giren matematiğin bir kısmını açıklamak, çünkü tüm algoritma bu entropi fikrine odaklanıyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle?",
  "translatedText": "İlk olarak, duymadıysanız söyleyeyim, Wurdle nedir?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us.",
  "translatedText": "Ve burada oyunun kurallarını incelerken bir taşla iki kuş vurmak için, bununla nereye gittiğimizi de ön izlememe izin verin, yani temelde oyunu bizim için oynayacak küçük bir algoritma geliştirmek.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does.",
  "translatedText": "Bugünkü Wurdle'ı yapmamış olsam da, bugün 4 Şubat ve botun nasıl yapacağını göreceğiz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess.",
  "translatedText": "Wurdle'ın amacı, beş harfli gizemli bir kelimeyi tahmin etmektir ve size tahmin etmeniz için altı farklı şans verilir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane.",
  "translatedText": "Örneğin, Wurdle botum tahmin vinciyle başlamamı öneriyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer.",
  "translatedText": "Her tahmin yaptığınızda, tahmininizin gerçek cevaba ne kadar yakın olduğuna dair bazı bilgiler alırsınız.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer.",
  "translatedText": "Burada gri kutu bana asıl cevapta C'nin olmadığını söylüyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position.",
  "translatedText": "Sarı kutu bana bir R olduğunu söylüyor ama o konumda değil.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position.",
  "translatedText": "Yeşil kutu bana gizli kelimenin A harfine sahip olduğunu ve üçüncü sırada olduğunu söylüyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E.",
  "translatedText": "Ve sonra N yok ve E yok.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information.",
  "translatedText": "O halde içeri girip Wurdle botuna bu bilgiyi söyleyeyim.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey.",
  "translatedText": "Turnayla başladık, gri, sarı, yeşil, gri, gri elde ettik.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time.",
  "translatedText": "Şu anda gösterdiği tüm veriler hakkında endişelenmeyin, bunları zamanı gelince açıklayacağım.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick.",
  "translatedText": "Ancak ikinci seçimimiz için en önemli önerisi saçmalıktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess.",
  "translatedText": "Ve tahmininizin gerçekten beş harfli bir kelime olması gerekiyor, ancak göreceğiniz gibi, aslında tahmin etmenize izin vereceği şey konusunda oldukça liberal.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick.",
  "translatedText": "Bu durumda, saçmalamayı deneriz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good.",
  "translatedText": "Ve tamam, işler oldukça iyi görünüyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R.",
  "translatedText": "S ve H'ye basıyoruz, yani ilk üç harfi biliyoruz, bir R olduğunu biliyoruz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something.",
  "translatedText": "Ve böylece SHA bir R gibi olacak veya SHA R bir şey olacak.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp.",
  "translatedText": "Görünüşe göre Wurdle botu bunun yalnızca iki olasılığa bağlı olduğunu biliyor; kırık ya da keskin.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a toss up between them at this point, so I guess probably just because it's alphabetical it goes with shard.",
  "translatedText": "Bu noktada aralarında bir tür çekişme var, bu yüzden sanırım muhtemelen alfabetik olduğu için parçayla uyumlu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer.",
  "translatedText": "Hangi yaşasın, asıl cevap bu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.22,
  "end": 132.86
 },
 {
  "input": "So we got it in three.",
  "translatedText": "Böylece üçe çıktık.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 132.96,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle four is par and three is birdie.",
  "translatedText": "Bunun iyi olup olmadığını merak ediyorsanız, bir kişiden duyduğuma göre Wurdle'da dört eşit ve üç birdie'dir şeklinde bir cümle duydum.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy.",
  "translatedText": "Oldukça uygun bir benzetme olduğunu düşünüyorum.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy.",
  "translatedText": "Dört puan almak için oyununuzda tutarlı bir şekilde çalışmalısınız, ancak bu kesinlikle çılgınca değil.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great.",
  "translatedText": "Ama üçe böldüğünüzde harika hissettiriyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot.",
  "translatedText": "Yani eğer istekliyseniz, burada yapmak istediğim şey Wurdle botuna nasıl yaklaştığım konusunda en başından itibaren düşünce sürecimden bahsetmek.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson.",
  "translatedText": "Ve dediğim gibi bu aslında bilgi teorisi dersi için bir bahane.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy.",
  "translatedText": "Temel amaç bilgi nedir, entropinin ne olduğunu açıklamaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language.",
  "translatedText": "Bu konuya yaklaşırken ilk düşüncem İngilizcedeki farklı harflerin göreceli sıklıklarına bakmaktı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters?",
  "translatedText": "Ben de düşündüm ki, tamam, bu en sık kullanılan harflerin çoğuna karşılık gelen bir açılış tahmini veya bir açılış tahmin çifti var mı?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails.",
  "translatedText": "Ve oldukça hoşuma giden bir şey de diğerini ve ardından tırnakları yapmaktı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good.",
  "translatedText": "Buradaki düşünce şu ki, eğer bir harfe rastlarsanız, yeşil ya da sarı elde edersiniz, bu her zaman iyi hissettirir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.76,
  "end": 187.52
 },
 {
  "input": "It feels like you're getting information.",
  "translatedText": "Bilgi alıyormuşsunuz gibi geliyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.52,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information since it's pretty rare to find a word that doesn't have any of these letters.",
  "translatedText": "Ancak bu durumlarda, vurmasanız ve her zaman gri tonlar alsanız bile, bu size yine de birçok bilgi verir, çünkü bu harflerden herhangi birine sahip olmayan bir kelime bulmak oldukça nadirdir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters.",
  "translatedText": "Ancak yine de bu pek sistematik gelmiyor çünkü örneğin harflerin sırasını dikkate almanın hiçbir faydası yok.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail?",
  "translatedText": "Salyangoz yazabilecekken neden çivi yazayım ki?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end?",
  "translatedText": "Sonunda S olması daha mı iyi?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure.",
  "translatedText": "Gerçekten emin değilim.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y.",
  "translatedText": "Bir arkadaşım yorgun sözcüğüyle başlamayı sevdiğini söyledi, bu beni biraz şaşırttı çünkü içinde W ve Y gibi alışılmadık harfler vardı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener.",
  "translatedText": "Ama kim bilir, belki bu daha iyi bir açılıştır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess?",
  "translatedText": "Potansiyel bir tahminin kalitesini değerlendirmek için verebileceğimiz bir tür niceliksel puan var mı?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up.",
  "translatedText": "Şimdi olası tahminleri sıralama yöntemimizi belirlemek için geriye dönüp oyunun tam olarak nasıl kurulduğuna biraz açıklık getirelim.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long.",
  "translatedText": "Yani, geçerli tahminler olarak kabul edilen, girmenize izin verecek yaklaşık 13.000 kelime uzunluğunda bir kelime listesi var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble.",
  "translatedText": "Ama baktığınızda pek çok sıra dışı şey var; kafa ya da Ali ve ARG gibi şeyler, Scrabble oyununda aile tartışmalarına yol açan türden kelimeler.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word.",
  "translatedText": "Ancak oyunun havası, cevabın her zaman oldukça yaygın bir kelime olacağıdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers.",
  "translatedText": "Ve aslında olası yanıtlar olan yaklaşık 2300 kelimeden oluşan başka bir liste daha var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creator's girlfriend, which is kind of fun.",
  "translatedText": "Ve bu, insanların hazırladığı bir liste, sanırım özellikle oyun yaratıcısının kız arkadaşı tarafından, ki bu da oldukça eğlenceli.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving Wordle that doesn't incorporate previous knowledge about this list.",
  "translatedText": "Ancak yapmak istediğim şey, bu projedeki amacımız, bu listeyle ilgili önceki bilgileri birleştirmeyen, Wordle çözen bir program yazıp yazamayacağımızı görmek.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list.",
  "translatedText": "Öncelikle, bu listede bulamayacağınız pek çok yaygın beş harfli kelime var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play Wordle against anyone, not just what happens to be the official website.",
  "translatedText": "Bu nedenle, biraz daha dayanıklı olan ve sadece resmi web sitesine değil, herkese karşı Wordle oynayabilecek bir program yazmak daha iyi olacaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also the reason that we know what this list of possible answers is, is because it's visible in the source code.",
  "translatedText": "Ayrıca bu olası yanıtlar listesinin ne olduğunu bilmemizin nedeni, bunun kaynak kodunda görünür olmasıdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day.",
  "translatedText": "Ancak kaynak kodunda görünme şekli, cevapların günden güne ortaya çıkma sırasına göredir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 292.8
 },
 {
  "input": "So you could always just look up what tomorrow's answer will be.",
  "translatedText": "Böylece her zaman yarının cevabının ne olacağına bakabilirsiniz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.06,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating.",
  "translatedText": "Açıkça görülüyor ki, listeyi kullanmanın hile yapmak olduğu bir anlam taşıyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words.",
  "translatedText": "Ve daha ilginç bir bulmacayı ve daha zengin bir bilgi teorisi dersini ortaya çıkaran şey, daha yaygın sözcükleri tercih etme sezgisini yakalamak için genel olarak göreceli sözcük sıklıkları gibi daha evrensel verileri kullanmaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess?",
  "translatedText": "Peki bu 13.000 olasılık arasından açılış tahminini nasıl seçmeliyiz?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality?",
  "translatedText": "Mesela arkadaşım bıkkınlık teklif ediyorsa kalitesini nasıl analiz etmeliyiz?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W.",
  "translatedText": "Pek olası olmayan W'yi sevdiğini söylemesinin nedeni, o W'ye vurmanın ne kadar iyi hissettireceğinin uzun vadede doğasını sevmesidir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern.",
  "translatedText": "Örneğin, ortaya çıkan ilk kalıp bunun gibi bir şeyse, bu dev sözlükte bu kalıpla eşleşen yalnızca 58 kelime olduğu ortaya çıkıyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000.",
  "translatedText": "Yani bu 13.000'den çok büyük bir azalma.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this.",
  "translatedText": "Ancak bunun diğer tarafı da elbette ki böyle bir model elde etmenin çok nadir olmasıdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000.",
  "translatedText": "Spesifik olarak, her kelimenin cevap olma olasılığı eşit olsaydı, bu kalıba ulaşma olasılığı 58 bölü 13.000 olurdu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers.",
  "translatedText": "Elbette bunların cevap olma ihtimali eşit değil.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words.",
  "translatedText": "Bunların çoğu çok belirsiz ve hatta şüpheli kelimelerdir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later.",
  "translatedText": "Ama en azından tüm bunlara ilk geçişimizde, hepsinin eşit derecede olası olduğunu varsayalım ve bunu biraz sonra düzeltelim.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is the pattern with a lot of information is by its very nature unlikely to occur.",
  "translatedText": "Mesele şu ki, çok fazla bilgi içeren bir modelin doğası gereği ortaya çıkması pek olası değildir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely.",
  "translatedText": "Aslında bilgilendirici olmanın anlamı bunun olası olmadığıdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where of course there's not a W in it.",
  "translatedText": "Bu açılışta görülecek çok daha olası bir model bunun gibi bir şey olabilir, burada elbette W harfi yoktur.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y.",
  "translatedText": "Belki bir E vardır ve belki A yoktur, R yoktur, Y yoktur.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches.",
  "translatedText": "Bu durumda 1400 olası eşleşme vardır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see.",
  "translatedText": "Hepsi eşit olasılıkta olsaydı, göreceğiniz modelin bu olma olasılığı yaklaşık %11 olurdu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative.",
  "translatedText": "Dolayısıyla en olası sonuçlar aynı zamanda en az bilgilendirici olanlardır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see.",
  "translatedText": "Burada daha küresel bir bakış elde etmek için, görebileceğiniz tüm farklı kalıplara göre olasılıkların tam dağılımını size göstereyim.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common.",
  "translatedText": "Yani baktığınız her çubuk, ortaya çıkabilecek olası bir renk düzenine karşılık gelir, bunlardan 3 üzeri 5 olasılık vardır ve bunlar soldan sağa, en yaygından en az yaygına doğru düzenlenir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays.",
  "translatedText": "Yani buradaki en yaygın olasılık, tüm grileri elde etmenizdir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time.",
  "translatedText": "Bu, zamanın yaklaşık %14'ünde gerçekleşir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here where there's only 18 possibilities for what matches this pattern that evidently look like this.",
  "translatedText": "Ve bir tahmin yaptığınızda umduğunuz şey, kendinizi bu uzun kuyrukta bir yerde bulmanızdır; burada olduğu gibi, açıkça buna benzeyen bu kalıba uyan şey için yalnızca 18 olasılığın olduğu yer.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here.",
  "translatedText": "Ya da biraz daha sola gidersek belki buraya kadar gidebiliriz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you.",
  "translatedText": "Tamam, işte sana güzel bir bulmaca.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them?",
  "translatedText": "İngilizce dilinde W ile başlayan, Y ile biten ve içinde bir yerde R bulunan üç kelime nedir?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly.",
  "translatedText": "Cevapların, bakalım, uzun uzun, kurtlu ve alaycı olduğu ortaya çıktı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution.",
  "translatedText": "Bu kelimenin genel olarak ne kadar iyi olduğuna karar vermek için, bu dağılımdan alacağınız beklenen bilgi miktarının bir tür ölçüsünü istiyoruz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score.",
  "translatedText": "Her bir modeli incelersek ve onun gerçekleşme olasılığını ne kadar bilgilendirici olduğunu ölçen bir şeyle çarparsak, bu bize belki objektif bir puan verebilir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches.",
  "translatedText": "Şimdi bir şeyin ne olması gerektiğine dair ilk içgüdünüz eşleşme sayısı olabilir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches.",
  "translatedText": "Daha düşük bir ortalama eşleşme sayısı istiyorsunuz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer.",
  "translatedText": "Ancak bunun yerine, genellikle bilgiye atfettiğimiz daha evrensel bir ölçüm kullanmak istiyorum ve bu 13.000 kelimenin her birine, bunların gerçekten cevap olup olmadığına ilişkin farklı bir olasılık atandığında daha esnek olacak bir ölçüm kullanmak istiyorum.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but it's really intuitive if we just look at examples.",
  "translatedText": "Standart bilgi birimi, biraz komik bir formüle sahip olan bit'tir, ancak sadece örneklere bakarsak gerçekten sezgiseldir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information.",
  "translatedText": "Olasılık alanınızı yarıya indiren bir gözleminiz varsa, onun bir bit bilgisi vardır diyoruz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about Half of the five letter words have an S, a little less than that, but about half.",
  "translatedText": "Örneğimizde, olasılıklar uzayı tüm olası kelimelerden oluşuyor ve ortaya çıkıyor ki, beş harfli kelimelerin yaklaşık yarısının S harfi var, bundan biraz daha az ama yarısı kadar.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information.",
  "translatedText": "Yani bu gözlem size biraz bilgi verecektir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information.",
  "translatedText": "Bunun yerine yeni bir olgu bu olasılıklar alanını dört kat azaltırsa, onun iki bitlik bilgiye sahip olduğunu söyleriz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T.",
  "translatedText": "Örneğin, bu kelimelerin yaklaşık dörtte birinde T harfinin olduğu ortaya çıktı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth.",
  "translatedText": "Eğer gözlem bu alanı sekiz kat azaltırsa, bunun üç bitlik bilgi olduğunu söyleriz ve bu böyle devam eder.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a 16th, five bits cuts it into a 32nd.",
  "translatedText": "Dört bit onu 16'ya, beş bit ise 32'ye böler.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.52
 },
 {
  "input": "So now you might want to pause and ask yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence?",
  "translatedText": "Şimdi durup kendinize şu soruyu sorabilirsiniz: Bir olayın gerçekleşme olasılığı açısından bit sayısı bilgisinin formülü nedir?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 533.52,
  "end": 542.98
 },
 {
  "input": "What we're saying here is that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability.",
  "translatedText": "Burada söylediğimiz şey, bit sayısının yarısını aldığınızda, bu olasılık ile aynı şeydir; bu, bit sayısının iki üssünün bir bölü olasılık olduğunu söylemekle aynı şeydir; Bilginin log tabanının iki bölü olasılığa eşit olduğunu söyleyerek yeniden düzenleme yapar.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still, where the information is the negative log base two of the probability.",
  "translatedText": "Ve bazen bunu bir yeniden düzenlemeyle daha görürsünüz; burada bilgi, olasılığın negatif logaritması tabanıdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half.",
  "translatedText": "Bu şekilde ifade edilirse, bu konuya yeni başlayan biri için biraz tuhaf görünebilir, ancak aslında bu, olasılıklarınızı kaç kez yarıya indirdiğinizi sormak gibi çok sezgisel bir fikirdir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture?",
  "translatedText": "Şimdi merak ediyorsanız, eğlenceli bir kelime oyunu oynadığımızı sanıyordum, neden logaritmalar devreye giriyor?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095.",
  "translatedText": "Bunun daha güzel bir birim olmasının bir nedeni, pek olası olmayan olaylar hakkında konuşmanın çok daha kolay olmasıdır; bir gözlemin 20 bitlik bilgiye sahip olduğunu söylemek, şunun şunun meydana gelme olasılığının 0 olduğunu söylemekten çok daha kolaydır.0000095.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together.",
  "translatedText": "Ancak bu logaritmik ifadenin olasılık teorisine çok yararlı bir katkı olduğunun ortaya çıkmasının daha önemli bir nedeni, bilgilerin bir araya gelme şeklidir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information.",
  "translatedText": "Örneğin, bir gözlem size iki bitlik bilgi verirse, alanınızı dört katına çıkarırsa ve ardından Wordle'deki ikinci tahmininiz gibi ikinci bir gözlem size başka bir üç bitlik bilgi verirse ve sizi başka bir sekiz kat daha küçültürse, ikisi birlikte size beş bitlik bilgi verir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add.",
  "translatedText": "Olasılıklar çoğalmayı sevdiği gibi, bilgi de eklemeyi sever.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with.",
  "translatedText": "Dolayısıyla, bir dizi sayıyı topladığımız beklenen değer gibi bir şeyin alanına girdiğimizde, günlükler bununla uğraşmayı çok daha güzel hale getiriyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for Weary and add another little tracker on here, showing us how much information there is for each pattern.",
  "translatedText": "Weary dağıtımımıza geri dönelim ve buraya her model için ne kadar bilgi bulunduğunu bize gösteren başka bir küçük izleyici ekleyelim.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain.",
  "translatedText": "Fark etmenizi istediğim asıl şey, daha olası modellere ulaşma olasılığımız arttıkça, bilgi ne kadar düşükse, o kadar az bit kazanırsınız.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information, where we go through each pattern, we say how probable is it, and then we multiply that by how many bits of information do we get.",
  "translatedText": "Bu tahminin kalitesini ölçmemizin yolu, bu bilginin beklenen değerini almak olacaktır; burada her bir modeli inceliyoruz, bunun ne kadar muhtemel olduğunu söylüyoruz ve sonra bunu kaç bitlik bilgi aldığımızla çarpıyoruz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 643.5,
  "end": 654.06
 },
 {
  "input": "And in the example of Weary, that turns out to be 4.9 bits.",
  "translatedText": "Ve Weary örneğinde bunun 4 olduğu ortaya çıkıyor.9 bit.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times.",
  "translatedText": "Yani ortalama olarak, bu açılış tahmininden alacağınız bilgi, olasılıklar alanınızı yaklaşık beş kez yarıya indirmeye eşdeğerdir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like Slate.",
  "translatedText": "Bunun tersine, beklenen bilgi değeri daha yüksek olan bir tahmin örneği Slate gibi bir şey olabilir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case you'll notice the distribution looks a lot flatter.",
  "translatedText": "Bu durumda dağılımın çok daha düz göründüğünü fark edeceksiniz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information.",
  "translatedText": "Özellikle, tüm grilerin en muhtemel oluşumu yalnızca %6 civarında meydana gelme şansına sahiptir, yani en azından açıkça 3 elde edersiniz.9 bitlik bilgi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that.",
  "translatedText": "Ancak bu minimumdur, daha genel olarak bundan daha iyi bir şey elde edersiniz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8.",
  "translatedText": "Ve buradaki rakamları hesaplayıp ilgili tüm terimleri topladığınızda ortalama bilginin yaklaşık 5 olduğu ortaya çıkıyor.8.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with Weary, your space of possibilities will be about half as big after this first guess, on average.",
  "translatedText": "Yani Weary'nin aksine, olasılıklar alanınız bu ilk tahminden sonra ortalama olarak yarısı kadar büyük olacaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity.",
  "translatedText": "Bilgi miktarının bu beklenen değerinin adı hakkında aslında eğlenceli bir hikaye var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "Information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science.",
  "translatedText": "Bilgi teorisi, 1940'larda Bell Laboratuarlarında çalışan Claude Shannon tarafından geliştirildi, ancak henüz yayınlanmamış bazı fikirlerinden, zamanın entelektüel devi, çok öne çıkan John von Neumann'la konuşuyordu. matematik ve fizikte ve bilgisayar bilimine dönüşen şeyin başlangıcı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well you should call it entropy, and for two reasons.",
  "translatedText": "Ve von Neumann, bilgi miktarının bu beklenen değeri için gerçekten iyi bir isme sahip olmadığını söylediğinde, söylendiğine göre, hikaye şöyle devam ediyor, buna entropi diyebilirsiniz ve bunun iki nedeni var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage.",
  "translatedText": "İlk olarak, belirsizlik fonksiyonunuz istatistiksel mekanikte bu isimle kullanıldı, dolayısıyla zaten bir adı var ve ikinci olarak ve daha da önemlisi, hiç kimse entropinin gerçekte ne olduğunu bilmiyor, dolayısıyla bir tartışmada her zaman avantajı var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design.",
  "translatedText": "Yani eğer isim biraz gizemli görünüyorsa ve eğer bu hikayeye inanılacaksa, bu bir bakıma tasarım gereğidir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess.",
  "translatedText": "Ayrıca, eğer bunun fizikteki termodinamiğin ikinci yasasıyla ilgili tüm o şeylerle ilişkisini merak ediyorsanız, kesinlikle bir bağlantı var, ama kökeninde Shannon sadece saf olasılık teorisiyle ilgileniyordu ve buradaki amaçlarımız için, Kelime entropisi, sadece belirli bir tahminin beklenen bilgi değerini düşünmenizi istiyorum.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously.",
  "translatedText": "Entropiyi iki şeyin aynı anda ölçülmesi olarak düşünebilirsiniz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution.",
  "translatedText": "Bunlardan ilki dağılımın ne kadar düz olduğudur.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be.",
  "translatedText": "Dağılım düzgünlüğe ne kadar yakınsa entropi o kadar yüksek olur.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy.",
  "translatedText": "Bizim durumumuzda, 3 üzeri 5'lik toplam örüntülerin olduğu durumda, düzgün bir dağılım için, bunlardan herhangi birinin gözlemlenmesi, 3 üzeri 5'lik bilgi günlüğü tabanı 2'ye sahip olacaktır; bu da 7 olur.92, yani bu entropi için sahip olabileceğiniz mutlak maksimum değer budur.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place.",
  "translatedText": "Ancak entropi aynı zamanda ilk etapta ne kadar olasılığın bulunduğunun da bir ölçüsüdür.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits.",
  "translatedText": "Örneğin, yalnızca 16 olası örüntünün olduğu ve her birinin eşit olasılığa sahip olduğu bir kelimeniz varsa, bu entropi, bu beklenen bilgi 4 bit olacaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits.",
  "translatedText": "Ancak 64 olası örüntünün ortaya çıkabileceği başka bir kelimeniz varsa ve bunların hepsi eşit derecede olasıysa, o zaman entropi 6 bit olarak hesaplanacaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes.",
  "translatedText": "Yani, eğer doğada 6 bitlik bir entropiye sahip bir dağılım görürseniz, bu sanki 64 eşit olasılıklı sonuç varmış gibi, olacaklar konusunda çok fazla değişkenlik ve belirsizlik olduğunu söylemek gibidir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this.",
  "translatedText": "Wurtelebot'a ilk geçişimde temelde bunu yapmasını sağladım.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the possible guesses you could have, all 13,000 words, computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns you might see, for each one, and picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible.",
  "translatedText": "Yapabileceğiniz tüm olası tahminleri, yani 13.000 kelimenin tamamını gözden geçirir, her biri için entropiyi veya daha spesifik olarak, her biri için görebileceğiniz tüm kalıplar arasındaki dağılımın entropisini hesaplar ve en yüksek olanı seçer, çünkü bu Olasılık alanınızı mümkün olduğu kadar daraltması muhtemel olanı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses.",
  "translatedText": "Burada sadece ilk tahminden bahsetmiş olsam da sonraki birkaç tahmin için de aynı şeyi yapıyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words.",
  "translatedText": "Örneğin, ilk tahminde, hangi kelimeyle eşleştiğine bağlı olarak sizi daha az sayıda olası kelimeyle sınırlayacak bir model gördükten sonra, aynı oyunu o daha küçük kelime kümesine göre oynarsınız.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy.",
  "translatedText": "Önerilen ikinci bir tahmin için, bu daha kısıtlı kelime grubundan oluşabilecek tüm kalıpların dağılımına bakarsınız, 13.000 olasılığın tamamını ararsınız ve bu entropiyi maksimuma çıkaranı bulursunuz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins.",
  "translatedText": "Bunun nasıl çalıştığını size göstermek için, kenarlarda bu analizin önemli noktalarını gösteren, yazdığım Wurtele'nin küçük bir versiyonunu ele almama izin verin.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "After doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information.",
  "translatedText": "Tüm entropi hesaplamalarını yaptıktan sonra sağ tarafta bize hangilerinin en yüksek beklenen bilgiye sahip olduğunu gösteriyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch.",
  "translatedText": "En azından şimdilik en önemli cevabın Tares olduğu ortaya çıktı, bu da tabii ki fiğ, en yaygın fiğ anlamına geliyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got, given this particular pattern.",
  "translatedText": "Burada her tahmin yaptığımızda, belki de önerilerini göz ardı edip slate'i tercih ederim, çünkü slate'i severim, ne kadar beklenen bilgiye sahip olduğunu görebiliriz, ancak burada kelimenin sağında bize ne kadar bilgi olduğunu gösteriyor. Bu özel model göz önüne alındığında, elde ettiğimiz gerçek bilgiler.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that.",
  "translatedText": "Yani burada biraz şanssızız gibi görünüyor, 5 almamız bekleniyordu.8, ama bundan daha azına sahip bir şey elde ettik.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now.",
  "translatedText": "Ve sol tarafta, şu anda bulunduğumuz yere göre bize mümkün olan tüm farklı kelimeleri gösteriyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment.",
  "translatedText": "Mavi çubuklar bize her kelimenin ne kadar olası olduğunu düşündüğünü gösteriyor; dolayısıyla şu anda her kelimenin eşit derecede gerçekleşme olasılığını varsayıyor, ancak bunu birazdan düzelteceğiz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities.",
  "translatedText": "Ve bu belirsizlik ölçümü bize bu dağılımın olası kelimeler arasındaki entropisini anlatıyor; bu şu anda tekdüze bir dağılım olduğu için olasılıkların sayısını saymanın gereksiz derecede karmaşık bir yoludur.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities.",
  "translatedText": "Örneğin 2 üssü 13'ü alırsak.66, bu 13.000 olasılık civarında olmalı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "I'm a little bit off here, but only because I'm not showing all the decimal places.",
  "translatedText": "Burada biraz yanlışım var ama bunun nedeni ondalık basamakların tamamını göstermemem.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute.",
  "translatedText": "Şu anda bu size gereksiz gelebilir ve işleri aşırı derecede karmaşık hale getirebilir, ancak bir dakika içinde her iki sayıya da sahip olmanın neden yararlı olduğunu göreceksiniz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Ramen, which again just really doesn't feel like a word.",
  "translatedText": "Yani burada ikinci tahminimiz için en yüksek entropinin Ramen olduğunu öne sürüyor gibi görünüyor ki bu da yine tek kelime gibi gelmiyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here, I'm going to go ahead and type in Rains.",
  "translatedText": "Burada ahlaki açıdan yüksek bir yer edinmek için, devam edip Rains yazacağım.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky.",
  "translatedText": "Ve yine biraz şanssızmışız gibi görünüyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information.",
  "translatedText": "Biz 4 bekliyorduk.3 bit ve elimizde sadece 3 var.39 bit bilgi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities.",
  "translatedText": "Bu da bizi 55 olasılığa indiriyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means.",
  "translatedText": "Ve burada belki de aslında onun önerdiği şeyle, yani birleşik olanla, her ne anlama geliyorsa onunla devam edeceğim.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And okay, this is actually a good chance for a puzzle.",
  "translatedText": "Ve tamam, bu aslında bir bulmaca için iyi bir şans.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information.",
  "translatedText": "Bu modelin bize 4 verdiğini söylüyor.7 bitlik bilgi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty.",
  "translatedText": "Ama sol tarafta, bu modeli görmeden önce 5 tane vardı.78 bitlik belirsizlik.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities?",
  "translatedText": "Peki sizin için bir test olarak, kalan olasılıkların sayısı ne anlama geliyor?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well, it means that we're reduced down to one bit of uncertainty, which is the same thing as saying that there's two possible answers.",
  "translatedText": "Bu, bir miktar belirsizliğe indirgendiğimiz anlamına gelir ki bu, iki olası yanıt olduğunu söylemekle aynı şeydir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice.",
  "translatedText": "Bu 50-50'lik bir seçim.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss.",
  "translatedText": "Ve buradan yola çıkarak, sen ve ben hangi kelimelerin daha yaygın olduğunu bildiğimiz için cevabın uçurum olması gerektiğini biliyoruz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that.",
  "translatedText": "Ancak şu anda yazıldığı gibi, program bunu bilmiyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it.",
  "translatedText": "Böylece tek bir olasılık kalana kadar mümkün olduğu kadar çok bilgi toplamaya devam eder ve sonra onu tahmin eder.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy.",
  "translatedText": "Açıkçası daha iyi bir oyunsonu stratejisine ihtiyacımız var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1042.34
 },
 {
  "input": "But let's say we call this version one of our wordle solver, and then we go and run some simulations to see how it does.",
  "translatedText": "Ancak diyelim ki bu sürüme kelime çözücülerimizden biri adını verdik ve sonra gidip nasıl çalıştığını görmek için bazı simülasyonlar çalıştırdık.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1042.6,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game.",
  "translatedText": "Yani bunun çalışma şekli mümkün olan her kelime oyununu oynamaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers.",
  "translatedText": "Gerçek wordle cevapları olan 2315 kelimenin tamamının üzerinden geçiyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set.",
  "translatedText": "Temel olarak bunu bir test seti olarak kullanıyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice.",
  "translatedText": "Ve bir kelimenin ne kadar yaygın olduğunu dikkate almamak ve tek ve tek bir seçeneğe varıncaya kadar yol boyunca her adımda bilgiyi en üst düzeye çıkarmaya çalışmak gibi naif bir yöntemle.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124.",
  "translatedText": "Simülasyonun sonunda ortalama puan 4 civarında çıkıyor.124.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expected to do worse.",
  "translatedText": "Ki bu fena değil, dürüst olmak gerekirse, daha kötüsünü bekliyordum.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4.",
  "translatedText": "Ancak wordle oynayanlar size genellikle 4'te alabileceklerini söyleyecektir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can.",
  "translatedText": "Asıl zorluk 3'te mümkün olduğunca çok sayıda elde etmektir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3.",
  "translatedText": "4'lük skor ile 3'lük skor arasında oldukça büyük bir sıçrama var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that.",
  "translatedText": "Buradaki bariz düşük sonuç, bir kelimenin yaygın olup olmadığını ve bunu tam olarak nasıl yapacağımızı bir şekilde dahil etmektir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language.",
  "translatedText": "Benim yaklaşımım İngilizce dilindeki tüm kelimelerin göreceli frekanslarının bir listesini almaktı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1107.88
 },
 {
  "input": "And I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset.",
  "translatedText": "Ve az önce Mathematica'nın Google Kitaplar İngilizce Ngram genel veri kümesinden alınan kelime frekansı veri fonksiyonunu kullandım.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words.",
  "translatedText": "Ve buna bakmak oldukça eğlenceli, örneğin en yaygın sözcüklerden en az kullanılan sözcüklere doğru sıralarsak.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5 letter words in the English language.",
  "translatedText": "Açıkçası bunlar İngilizce dilinde en yaygın 5 harfli kelimelerdir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common.",
  "translatedText": "Daha doğrusu bunlar en yaygın 8'incisidir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there.",
  "translatedText": "İlki hangisi, sonrasında orası ve orası var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common.",
  "translatedText": "Birincinin kendisi birinci değil, 9'uncudur ve bu diğer kelimelerin daha sık ortaya çıkabileceği, ilk gelenlerin sonra olduğu ve nerede olduğu ve biraz daha az yaygın olduğu mantıklıdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency.",
  "translatedText": "Şimdi, bu verileri, bu kelimelerin her birinin nihai cevap olma olasılığını modellemek için kullanırken, bu sadece sıklıkla orantılı olmamalıdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1146.34
 },
 {
  "input": "For example, which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely.",
  "translatedText": "Örneğin 0 puan verilir.Bu veri setinde 002 var, oysa örgü kelimesinin olasılığı bir anlamda 1000 kat daha az.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.7,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering.",
  "translatedText": "Ancak bunların her ikisi de, neredeyse kesinlikle dikkate alınmaya değer olacak kadar yaygın kelimelerdir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1158.84
 },
 {
  "input": "So we want more of a binary cutoff.",
  "translatedText": "Bu yüzden daha fazla ikili kesinti istiyoruz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1159.34,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty.",
  "translatedText": "Bu konuda izlediğim yol, tüm bu sıralanmış kelime listesini alıp, bunu bir x eksenine göre düzenlediğimi ve ardından çıktısı temelde ikili olan bir fonksiyona sahip olmanın standart yolu olan sigmoid fonksiyonunu uyguladığımı hayal etmekti. ya 0 ya da 1, ancak bu belirsizlik bölgesi için arada bir yumuşama var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis.",
  "translatedText": "Yani esas olarak, her bir kelimeye son listede yer almaları için atadığım olasılık, x ekseninde nerede olursa olsun yukarıdaki sigmoid fonksiyonunun değeri olacaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff.",
  "translatedText": "Açıkçası bu birkaç parametreye bağlıdır; örneğin, bu kelimelerin x ekseni üzerinde ne kadar geniş bir alanı dolduracağı, 1'den 0'a ne kadar kademeli veya dik bir şekilde düştüğümüzü belirler ve onları soldan sağa nereye yerleştirdiğimiz kesmeyi belirler.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "To be honest, the way I did this was just licking my finger and sticking it into the wind.",
  "translatedText": "Dürüst olmak gerekirse, bunu yapma şeklim sadece parmağımı yalayıp rüzgara doğru tutmaktı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff.",
  "translatedText": "Sıralanmış listeye baktım ve bir pencere bulmaya çalıştım; ona baktığımda bu kelimelerin yaklaşık yarısının son cevap olma ihtimalinin olmama ihtimalinden daha yüksek olduğunu düşündüm ve bunu kesme noktası olarak kullandım.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement.",
  "translatedText": "Kelimeler arasında böyle bir dağılıma sahip olduğumuzda, bu bize entropinin gerçekten yararlı bir ölçüm haline geldiği başka bir durum verir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were a feather and nails, and we end up with a situation where there's four possible words that match it.",
  "translatedText": "Örneğin, diyelim ki bir oyun oynuyoruz ve tüy ve çivilerden oluşan eski açılışlarımla başlıyoruz ve onunla eşleşen dört olası kelimenin olduğu bir durumla karşılaşıyoruz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely.",
  "translatedText": "Ve diyelim ki hepsinin eşit derecede olası olduğunu düşünüyoruz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1235.62
 },
 {
  "input": "Let me ask you, what is the entropy of this distribution?",
  "translatedText": "Size şunu sorayım, bu dağılımın entropisi nedir?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1236.22,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2.",
  "translatedText": "Bu olasılıkların her biriyle ilgili bilgi, 2/4'ün logaritması olacaktır, çünkü her biri 1 ve 4'tür ve bu da 2'dir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "Two bits of information, four possibilities.",
  "translatedText": "İki bit bilgi, dört olasılık.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good.",
  "translatedText": "Hepsi çok iyi ve güzel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than four matches?",
  "translatedText": "Peki ya size aslında dörtten fazla eşleşme olduğunu söylesem?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it.",
  "translatedText": "Gerçekte kelime listesinin tamamına baktığımızda onunla eşleşen 16 kelime var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure.",
  "translatedText": "Ancak modelimizin, diğer 12 kelimenin aslında nihai cevap olma ihtimalini gerçekten düşük tuttuğunu varsayalım; 1000'de 1 gibi bir şey, çünkü bunlar gerçekten belirsizdir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution?",
  "translatedText": "Şimdi size şunu sorayım, bu dağılımın entropisi nedir?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before.",
  "translatedText": "Eğer entropi burada sadece eşleşme sayısını ölçüyorsa, o zaman bunun 16'nın logaritması 2 gibi bir şey olmasını bekleyebilirsiniz ki bu da 4 olur, daha önce sahip olduğumuz belirsizlikten iki bit daha fazla olur.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before.",
  "translatedText": "Ancak elbette gerçek belirsizlik daha önce yaşadığımızdan çok da farklı değil.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example.",
  "translatedText": "Bu 12 belirsiz kelimenin var olması, örneğin son cevabın çekicilik olduğunu öğrenmenin çok daha şaşırtıcı olacağı anlamına gelmez.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here, and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits.",
  "translatedText": "Yani buradaki hesaplamayı gerçekten yaptığınızda ve her bir olayın olasılığını karşılık gelen bilgilerle topladığınızda elde ettiğiniz sonuç 2 olur.11 bit.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "I'm just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it.",
  "translatedText": "Sadece şunu söylüyorum, temelde iki parça, temelde bu dört olasılık, ancak tüm bu pek olası olmayan olaylar nedeniyle biraz daha belirsizlik var, gerçi bunları öğrenmiş olsaydınız bundan bir ton bilgi alırsınız.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson.",
  "translatedText": "Yani uzaklaştırma, Wordle'u bilgi teorisi dersi için bu kadar güzel bir örnek yapan şeyin bir parçası.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy.",
  "translatedText": "Entropi için iki ayrı duygu uygulamasına sahibiz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words that we have possible.",
  "translatedText": "Birincisi bize belirli bir tahminden alacağımız beklenen bilginin ne olduğunu söylüyor, ikincisi ise mümkün olan tüm kelimeler arasında kalan belirsizliği ölçebilir miyiz diyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation.",
  "translatedText": "Ve şunu vurgulamalıyım ki, bir tahminin beklenen bilgisine baktığımız ilk durumda, kelimelere eşit olmayan bir ağırlık verdiğimizde, bu entropi hesaplamasını etkiler.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words.",
  "translatedText": "Örneğin, daha önce Weary ile ilişkili dağıtıma baktığımız aynı durumu ele alayım, ancak bu sefer tüm olası kelimeler arasında tekdüze olmayan bir dağılım kullanıyorum.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well.",
  "translatedText": "Bakalım burada bunu oldukça iyi gösteren bir bölüm bulabilecek miyim?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here this is pretty good.",
  "translatedText": "Tamam, işte bu oldukça iyi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it.",
  "translatedText": "Burada birbirine eşit olasılıklara sahip iki bitişik modelimiz var, ancak bize söylenenlerden birinin kendisiyle eşleşen 32 olası kelime olduğu söylendi.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them.",
  "translatedText": "Ve bunların ne olduğunu kontrol edersek, bunlar şu 32 kelimedir, gözlerinizi üzerlerine taradığınızda bunların hepsi pek olası olmayan kelimelerdir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches, so a quarter as many matches, but it's about as likely.",
  "translatedText": "Belki bağırmak gibi akla yatkın cevaplar bulmak zordur, ancak dağılımdaki komşu düzene bakarsak, ki bu da hemen hemen aynı olası kabul edilir, bize sadece 8 olası eşleşme olduğu söylendi, yani çeyrek olarak birçok eşleşme var, ancak bu da bir o kadar muhtemel.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why.",
  "translatedText": "Ve bu kibritleri çıkardığımızda nedenini görebiliriz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers, like ring, or wrath, or raps.",
  "translatedText": "Bunlardan bazıları, zil sesi, gazap veya tecavüz gibi gerçekten makul yanıtlardır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version 2 of the Wordlebot here, and there are two or three main differences from the first one that we saw.",
  "translatedText": "Tüm bunları nasıl dahil ettiğimizi göstermek için, burada Wordlebot'un 2. versiyonunu ele almama izin verin; ilk gördüğümüzden iki veya üç ana fark var.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer.",
  "translatedText": "Öncelikle, az önce söylediğim gibi, bu entropileri, bu beklenen bilgi değerlerini hesaplama yöntemimiz, artık belirli bir kelimenin gerçekten cevap olma olasılığını da içeren kalıplar arasındaki daha hassas dağılımları kullanmaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number 1, though the ones following are a bit different.",
  "translatedText": "Aslına bakılırsa gözyaşları hala 1 numara, ancak sonrakiler biraz farklı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table.",
  "translatedText": "İkincisi, en çok tercih edilenleri sıraladığında, artık her kelimenin asıl cevap olma ihtimaline ilişkin bir model tutacak ve bunu kararına dahil edecek; bunu, konuyla ilgili birkaç tahminimiz olduğunda görmek daha kolay olacak. masa.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives.",
  "translatedText": "Yine tavsiyelerini göz ardı ediyoruz çünkü makinelerin hayatlarımızı yönetmesine izin veremeyiz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches.",
  "translatedText": "Ve sanırım burada solda farklı olan başka bir şeyden bahsetmem gerekiyor; belirsizlik değeri, yani bit sayısı, artık sadece olası eşleşmelerin sayısıyla gereksiz değil.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which is a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes.",
  "translatedText": "Şimdi yukarı çekip 2 üzeri 8'i hesaplarsak.02, ki bu da 256'nın, sanırım 259'un biraz üzerinde, aslında bu kalıpla eşleşen toplam 526 kelime olmasına rağmen, sahip olduğu belirsizlik miktarı, eşit derecede olası 259 kelime olsaydı ne olacağına daha çok benziyor. sonuçlar.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this.",
  "translatedText": "Bunu şöyle düşünebilirsiniz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borx is not the answer, same with yorts and zorl and zorus, so it's a little less uncertain than it was in the previous case.",
  "translatedText": "Borx'un cevap olmadığını biliyor, yorts, zorl ve zorus için de aynısı geçerli, dolayısıyla önceki duruma göre biraz daha az belirsiz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller.",
  "translatedText": "Bu bit sayısı daha az olacaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here.",
  "translatedText": "Ve eğer oyunu oynamaya devam edersem, burada açıklamak istediğim şeye uygun birkaç tahminle bunu detaylandıracağım.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy.",
  "translatedText": "Dördüncü tahmine göre, eğer en çok tercih edilenlere bakarsanız, bunun artık sadece entropiyi maksimuma çıkarmadığını görebilirsiniz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words.",
  "translatedText": "Yani bu noktada teknik olarak yedi olasılık var ama anlamlı şansı olan tek şey yurtlar ve kelimeler.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values, that strictly speaking would give more information.",
  "translatedText": "Ve her ikisini de seçmenin bu diğer değerlerin üzerinde yer aldığını, açıkçası daha fazla bilgi vereceğini görebilirsiniz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect.",
  "translatedText": "Bunu ilk yaptığımda, her tahminin kalitesini ölçmek için bu iki sayıyı topladım ve bu aslında tahmin edebileceğinizden daha iyi işe yaradı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic, and I'm sure there's other approaches people could take but here's the one I landed on.",
  "translatedText": "Ama bu pek sistematik gelmedi ve eminim ki insanların benimseyebileceği başka yaklaşımlar da vardır, ama ben bu yaklaşıma ulaştım.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that.",
  "translatedText": "Bir sonraki tahmin olasılığını düşünürsek, bu durumda kelimeler gibi, gerçekten umursadığımız şey, eğer bunu yaparsak oyunumuzun beklenen puanıdır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to.",
  "translatedText": "Beklenen puanı hesaplamak için de kelimelerin gerçek cevap olma olasılığının ne olduğunu söylüyoruz ki bu şu anda %58'i açıklıyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be 4.",
  "translatedText": "Bu maçta puanımızın %58 ihtimalle 4 olacağını söylüyoruz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of 1 minus that 58%, our score will be more than that 4.",
  "translatedText": "Ve 1 eksi %58 olasılıkla puanımız 4'ten fazla olacaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point.",
  "translatedText": "Daha ne kadarını bilmiyoruz ama o noktaya geldiğimizde ne kadar belirsizliğin ortaya çıkabileceğine dayanarak bunu tahmin edebiliriz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment there's 1.44 bits of uncertainty.",
  "translatedText": "Özellikle şu anda 1 tane var.44 bit belirsizlik.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits.",
  "translatedText": "Kelimeleri tahmin edersek, bu bize alacağımız beklenen bilginin 1 olduğunu söyler.27 bit.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens.",
  "translatedText": "Yani kelimeleri tahmin edersek, bu fark, bu olay gerçekleştikten sonra ne kadar belirsizlikle baş başa kalacağımızı temsil ediyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score.",
  "translatedText": "İhtiyacımız olan şey, burada f adını verdiğim, bu belirsizliği beklenen bir puanla ilişkilendiren bir tür fonksiyondur.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version 1 of the bot to say hey what was the actual score after various points with certain very measurable amounts of uncertainty.",
  "translatedText": "Ve bunu gerçekleştirmenin yolu, botun 1. versiyonuna dayalı olarak önceki oyunlardan bir grup veriyi çizerek, çok ölçülebilir belirsizliklerle çeşitli noktalardan sonra gerçek puanın ne olduğunu söylemekti.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer.",
  "translatedText": "Örneğin, buradaki veri noktaları 8 civarındaki bir değerin üzerinde duruyor.8'in olduğu bir noktadan sonra bazı oyunlar için 7 ya da öylesine diyorlar.7 bitlik belirsizlik, nihai cevaba ulaşmak için iki tahmin yapılması gerekti.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games it took three guesses, for other games it took four guesses.",
  "translatedText": "Diğer oyunlar için üç tahmin gerekiyordu, diğer oyunlar için ise dört tahmin gerekiyordu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring.",
  "translatedText": "Burada sola kayarsak, sıfırın üzerindeki tüm noktalar, ne zaman sıfır belirsizlik varsa, yani tek bir olasılık varsa, o zaman gereken tahmin sayısı her zaman sadece birdir, bu da güven vericidir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses.",
  "translatedText": "Ne zaman bir miktar belirsizlik olsa, yani esasen iki olasılığa bağlıysa bazen bir tahmin daha, bazen de iki tahmin daha gerekiyordu.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1623.02
 },
 {
  "input": "And so on and so forth here.",
  "translatedText": "Burada da böyle devam ediyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1623.08,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages.",
  "translatedText": "Belki bu verileri görselleştirmenin biraz daha kolay bir yolu, bunları bir araya toplayıp ortalamalarını almaktır.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example this bar here saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5.",
  "translatedText": "Örneğin buradaki çubuk, bir miktar belirsizliğimizin olduğu tüm noktalar arasında ortalama olarak gereken yeni tahmin sayısının yaklaşık 1 olduğunu söylüyor.5.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward.",
  "translatedText": "Ve buradaki çubuk, tüm farklı oyunlar arasında bir noktada belirsizliğin dört bitin biraz üzerinde olduğunu söylüyor, bu da onu 16 farklı olasılığa daraltmak gibi, o zaman ortalama olarak o noktadan itibaren ikiden biraz daha fazla tahmin gerektiriyor ileri.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this.",
  "translatedText": "Ve buradan itibaren buna makul görünen bir fonksiyona uyacak bir regresyon yaptım.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be.",
  "translatedText": "Ve unutmayın, bunları yapmanın asıl amacı, bir kelimeden ne kadar çok bilgi kazanırsak beklenen puanın o kadar düşük olacağı şeklindeki bu sezgiyi ölçebilmektir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So with this as version 2.0, if we go back and we run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do?",
  "translatedText": "Yani bu sürüm 2 olarak.0'a dönersek ve aynı simülasyon setini çalıştırırsak, 2315 olası sözcük yanıtının tümüne karşı oynatırsak, bu nasıl olur?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version it's definitely better, which is reassuring.",
  "translatedText": "İlk versiyonumuzun aksine kesinlikle daha iyi, bu da güven verici.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done the average is around 3.6, although unlike the first version there are a couple times that it loses and requires more than six in this circumstance.",
  "translatedText": "Tüm söylenen ve yapılan ortalama 3 civarındadır.6, ilk versiyondan farklı olarak birkaç kez kaybettiği ve bu durumda altıdan fazlasını gerektirdiği durumlar olmasına rağmen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information.",
  "translatedText": "Muhtemelen bilgiyi en üst düzeye çıkarmak yerine hedefe ulaşmak için bu ödünleşimin yapıldığı zamanlar olduğu için.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6?",
  "translatedText": "Peki 3'ten daha iyisini yapabilir miyiz?6?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can.",
  "translatedText": "Kesinlikle yapabiliriz.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model.",
  "translatedText": "Başlangıçta, kelime cevaplarının gerçek listesini modelini oluşturma biçimine dahil etmemenin çok eğlenceli olduğunu söylemiştim.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43.",
  "translatedText": "Ancak bunu dahil edersek alabileceğim en iyi performans 3 civarındaydı.43.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that.",
  "translatedText": "Dolayısıyla, bu önceki dağılımı seçmek için kelime sıklığı verilerini kullanmaktan daha karmaşık hale gelmeye çalışırsak, bu 3.43 muhtemelen bunda ne kadar başarılı olabileceğimizin veya en azından benim bunda ne kadar başarılı olabileceğimin maksimumunu veriyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one.",
  "translatedText": "Bu en iyi performans aslında sadece burada bahsettiğim fikirleri kullanır, ancak biraz daha ileri gider, sanki beklenen bilgiyi tek bir adım yerine iki adım ileriye doğru arar.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is.",
  "translatedText": "Başlangıçta bunun hakkında daha fazla konuşmayı planlıyordum ama aslında oldukça uzun bir yol kat ettiğimizi fark ettim.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least it's looking like Crane is the best opener.",
  "translatedText": "Söyleyeceğim tek şey, bu iki adımlı aramayı yaptıktan ve ardından en iyi adaylar üzerinde birkaç örnek simülasyon çalıştırdıktan sonra, şu ana kadar benim için en azından Crane'in en iyi açıcı olduğu görünüyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed?",
  "translatedText": "Kim tahmin ederdi?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits.",
  "translatedText": "Ayrıca olasılıklar alanınızı belirlemek için gerçek sözcük listesini kullanırsanız, o zaman başlangıçtaki belirsizlik 11 bitin biraz üzerinde olur.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out, just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits.",
  "translatedText": "Ve sadece kaba kuvvet aramasından sonra, ilk iki tahminden sonra beklenen maksimum olası bilginin 10 bit civarında olduğu ortaya çıktı.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty.",
  "translatedText": "Bu da en iyi senaryoda, ilk iki tahmininizden sonra, mükemmel derecede optimal bir oyunla, yaklaşık bir miktar belirsizlikle baş başa kalacağınızı gösteriyor.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses.",
  "translatedText": "Bu, iki olası tahminde bulunmakla aynı şeydir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "So I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as 3, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail.",
  "translatedText": "Bu yüzden, bu ortalamayı 3'e kadar düşüren bir algoritmayı asla yazamayacağınızı söylemek adil ve muhtemelen oldukça muhafazakar olur çünkü kullanabileceğiniz kelimelerle, sadece iki adımdan sonra yeterli bilgiyi elde etmek için yer yoktur. her seferinde üçüncü slottaki cevabı hatasız olarak garanti edebilir.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]