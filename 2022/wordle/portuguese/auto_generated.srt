1
00:00:00,000 --> 00:00:03,327
O jogo Wurdle se tornou bastante viral nos últimos dois meses e, como

2
00:00:03,327 --> 00:00:06,702
nunca desperdiço uma oportunidade de uma aula de matemática, me ocorre

3
00:00:06,702 --> 00:00:10,030
que este jogo é um exemplo central muito bom em uma aula sobre teoria

4
00:00:10,030 --> 00:00:13,120
da informação e, em particular um tópico conhecido como entropia.

5
00:00:13,120 --> 00:00:16,532
Veja, como muitas pessoas, fui sugado pelo quebra-cabeça e, como

6
00:00:16,532 --> 00:00:19,840
muitos programadores, também fui sugado por tentar escrever um

7
00:00:19,840 --> 00:00:23,200
algoritmo que jogasse o jogo da maneira mais otimizada possível.

8
00:00:23,200 --> 00:00:26,191
E o que pensei em fazer aqui é apenas conversar com vocês sobre

9
00:00:26,191 --> 00:00:29,182
meu processo nisso e explicar um pouco da matemática envolvida,

10
00:00:29,182 --> 00:00:32,080
já que todo o algoritmo está centrado nessa ideia de entropia.

11
00:00:32,080 --> 00:00:42,180
Primeiramente, caso você ainda não tenha ouvido falar, o que é Wurdle?

12
00:00:42,180 --> 00:00:45,146
E para matar dois coelhos com uma cajadada só enquanto analisamos as

13
00:00:45,146 --> 00:00:48,198
regras do jogo, deixe-me também prever onde estamos indo com isso, que

14
00:00:48,198 --> 00:00:51,380
é desenvolver um pequeno algoritmo que basicamente jogará o jogo para nós.

15
00:00:51,380 --> 00:00:55,860
Embora eu não tenha feito o Wurdle de hoje, é 4 de fevereiro e veremos como o bot se sai.

16
00:00:55,860 --> 00:00:58,278
O objetivo do Wurdle é adivinhar uma palavra misteriosa de

17
00:00:58,278 --> 00:01:00,860
cinco letras, e você terá seis chances diferentes de adivinhar.

18
00:01:00,860 --> 00:01:05,240
Por exemplo, meu bot Wurdle sugere que eu comece com o guindaste de adivinhação.

19
00:01:05,240 --> 00:01:08,158
Cada vez que você dá um palpite, você obtém algumas informações

20
00:01:08,158 --> 00:01:10,940
sobre o quão próximo seu palpite está da resposta verdadeira.

21
00:01:10,940 --> 00:01:14,540
Aqui, a caixa cinza me diz que não há C na resposta real.

22
00:01:14,540 --> 00:01:18,340
A caixa amarela está me dizendo que existe um R, mas não está nessa posição.

23
00:01:18,340 --> 00:01:22,820
A caixa verde está me dizendo que a palavra secreta tem um A e está na terceira posição.

24
00:01:22,820 --> 00:01:24,300
E então não há N e não há E.

25
00:01:24,300 --> 00:01:27,420
Então deixe-me entrar e contar essa informação ao bot Wurdle.

26
00:01:27,420 --> 00:01:31,500
Começamos com guindaste, ficamos cinza, amarelo, verde, cinza, cinza.

27
00:01:31,500 --> 00:01:33,817
Não se preocupe com todos os dados que estão mostrando

28
00:01:33,817 --> 00:01:35,460
agora, explicarei isso no devido tempo.

29
00:01:35,460 --> 00:01:39,700
Mas sua principal sugestão para nossa segunda escolha é uma besteira.

30
00:01:39,700 --> 00:01:42,720
E seu palpite precisa ser uma palavra real de cinco letras, mas como você

31
00:01:42,720 --> 00:01:45,700
verá, é bastante liberal com o que realmente permitirá que você adivinhe.

32
00:01:45,700 --> 00:01:48,860
Neste caso, tentamos shtick.

33
00:01:48,860 --> 00:01:50,260
E tudo bem, as coisas estão parecendo muito boas.

34
00:01:50,260 --> 00:01:54,740
Atingimos o S e o H, então conhecemos as três primeiras letras, sabemos que existe um R.

35
00:01:54,740 --> 00:01:59,740
E então será como SHA algo R, ou SHA R alguma coisa.

36
00:01:59,740 --> 00:02:02,750
E parece que o bot Wurdle sabe que existem apenas

37
00:02:02,750 --> 00:02:05,220
duas possibilidades: fragmento ou afiado.

38
00:02:05,220 --> 00:02:08,282
Isso é uma espécie de confusão entre eles neste momento, então acho que

39
00:02:08,282 --> 00:02:11,260
provavelmente só porque está em ordem alfabética, vai com o fragmento.

40
00:02:11,260 --> 00:02:13,000
Qual, viva, é a resposta real.

41
00:02:13,000 --> 00:02:14,660
Então conseguimos isso em três.

42
00:02:14,660 --> 00:02:17,740
Se você está se perguntando se isso é bom, a maneira como ouvi

43
00:02:17,740 --> 00:02:20,820
uma pessoa dizer é que com Wurdle quatro é par e três é birdie.

44
00:02:20,820 --> 00:02:22,960
O que considero uma analogia bastante adequada.

45
00:02:22,960 --> 00:02:25,304
Você tem que estar consistentemente no seu jogo para

46
00:02:25,304 --> 00:02:27,560
conseguir quatro, mas certamente não é uma loucura.

47
00:02:27,560 --> 00:02:30,000
Mas quando você consegue isso em três, é ótimo.

48
00:02:30,000 --> 00:02:33,366
Então, se você quiser, o que eu gostaria de fazer aqui é apenas falar sobre

49
00:02:33,366 --> 00:02:36,600
meu processo de pensamento desde o início sobre como abordo o bot Wurdle.

50
00:02:36,600 --> 00:02:39,800
E como eu disse, na verdade é uma desculpa para uma aula de teoria da informação.

51
00:02:39,800 --> 00:02:48,560
O objetivo principal é explicar o que é informação e o que é entropia.

52
00:02:48,560 --> 00:02:51,100
Meu primeiro pensamento ao abordar isso foi dar uma olhada nas

53
00:02:51,100 --> 00:02:53,560
frequências relativas de diferentes letras na língua inglesa.

54
00:02:53,560 --> 00:02:56,812
Então pensei, ok, existe um palpite inicial ou um par inicial

55
00:02:56,812 --> 00:02:59,960
de palpites que acerta muitas dessas letras mais frequentes?

56
00:02:59,960 --> 00:03:03,780
E uma que eu gostava muito era fazer outra seguida de unhas.

57
00:03:03,780 --> 00:03:05,860
A ideia é que se você acertar uma letra, você sabe,

58
00:03:05,860 --> 00:03:07,980
você ganha um verde ou um amarelo, isso sempre é bom.

59
00:03:07,980 --> 00:03:09,460
Parece que você está obtendo informações.

60
00:03:09,460 --> 00:03:13,550
Mas nesses casos, mesmo que você não acerte e sempre fique cinza, isso ainda lhe dá muita

61
00:03:13,550 --> 00:03:17,640
informação, já que é muito raro encontrar uma palavra que não tenha nenhuma dessas letras.

62
00:03:17,640 --> 00:03:20,681
Mas mesmo assim, isso não parece super sistemático, porque,

63
00:03:20,681 --> 00:03:23,520
por exemplo, não faz nada considerar a ordem das letras.

64
00:03:23,520 --> 00:03:26,080
Por que digitar pregos quando eu poderia digitar caracol?

65
00:03:26,080 --> 00:03:27,720
É melhor ter aquele S no final?

66
00:03:27,720 --> 00:03:28,720
Eu não tenho certeza.

67
00:03:28,720 --> 00:03:32,940
Agora, um amigo meu disse que gostava de começar com a palavra cansado,

68
00:03:32,940 --> 00:03:37,160
o que me surpreendeu porque tem algumas letras incomuns, como o W e o Y.

69
00:03:37,160 --> 00:03:39,400
Mas quem sabe, talvez seja uma abertura melhor.

70
00:03:39,400 --> 00:03:42,064
Existe algum tipo de pontuação quantitativa que podemos

71
00:03:42,064 --> 00:03:44,920
atribuir para julgar a qualidade de uma possível estimativa?

72
00:03:44,920 --> 00:03:48,298
Agora, para definir a maneira como classificaremos as possíveis suposições, vamos

73
00:03:48,298 --> 00:03:51,800
voltar e adicionar um pouco de clareza sobre como exatamente o jogo está configurado.

74
00:03:51,800 --> 00:03:54,749
Portanto, há uma lista de palavras que permitirá que você insira e

75
00:03:54,749 --> 00:03:57,920
que sejam consideradas suposições válidas, com cerca de 13.000 palavras.

76
00:03:57,920 --> 00:04:00,860
Mas quando você olha para isso, há muitas coisas realmente

77
00:04:00,860 --> 00:04:03,700
incomuns, coisas como uma cabeça ou Ali e ARG, o tipo de

78
00:04:03,700 --> 00:04:07,040
palavras que provocam discussões familiares em um jogo de Scrabble.

79
00:04:07,040 --> 00:04:10,600
Mas a vibração do jogo é que a resposta sempre será uma palavra decentemente comum.

80
00:04:10,600 --> 00:04:16,080
E, de fato, há outra lista de cerca de 2.300 palavras que são as respostas possíveis.

81
00:04:16,080 --> 00:04:18,545
E esta é uma lista com curadoria humana, acho que

82
00:04:18,545 --> 00:04:21,800
especificamente da namorada do criador do jogo, o que é divertido.

83
00:04:21,800 --> 00:04:26,035
Mas o que eu gostaria de fazer, nosso desafio para este projeto é ver se conseguimos

84
00:04:26,035 --> 00:04:30,421
escrever um programa resolvendo Wordle que não incorpore conhecimento prévio sobre esta

85
00:04:30,421 --> 00:04:30,720
lista.

86
00:04:30,720 --> 00:04:33,043
Por um lado, há muitas palavras de cinco letras

87
00:04:33,043 --> 00:04:35,560
bastante comuns que você não encontrará nessa lista.

88
00:04:35,560 --> 00:04:38,864
Portanto, seria melhor escrever um programa que fosse um pouco mais resiliente

89
00:04:38,864 --> 00:04:41,960
e que jogasse Wordle contra qualquer um, não apenas contra o site oficial.

90
00:04:41,960 --> 00:04:44,560
E também a razão pela qual sabemos qual é essa lista de

91
00:04:44,560 --> 00:04:47,440
respostas possíveis é porque ela está visível no código-fonte.

92
00:04:47,440 --> 00:04:50,186
Mas a forma como isso fica visível no código-fonte está na

93
00:04:50,186 --> 00:04:52,840
ordem específica em que as respostas surgem no dia a dia.

94
00:04:52,840 --> 00:04:56,400
Então você pode sempre procurar qual será a resposta de amanhã.

95
00:04:56,400 --> 00:04:59,140
Então, claramente, há algum sentido em que usar a lista é trapaça.

96
00:04:59,140 --> 00:05:03,337
E o que torna um quebra-cabeça mais interessante e uma lição de teoria da informação mais

97
00:05:03,337 --> 00:05:07,395
rica é, em vez disso, usar alguns dados mais universais, como frequências relativas de

98
00:05:07,395 --> 00:05:11,313
palavras em geral, para capturar essa intuição de ter preferência por palavras mais

99
00:05:11,313 --> 00:05:11,640
comuns.

100
00:05:11,640 --> 00:05:16,560
Então, destas 13.000 possibilidades, como devemos escolher o palpite inicial?

101
00:05:16,560 --> 00:05:19,960
Por exemplo, se meu amigo propõe cansado, como devemos analisar sua qualidade?

102
00:05:19,960 --> 00:05:23,977
Bem, a razão pela qual ele disse que gosta daquele W improvável é que

103
00:05:23,977 --> 00:05:27,880
ele gosta da natureza remota de como é bom se você acertar aquele W.

104
00:05:27,880 --> 00:05:31,953
Por exemplo, se o primeiro padrão revelado for algo assim, então acontece que

105
00:05:31,953 --> 00:05:36,080
existem apenas 58 palavras neste léxico gigante que correspondem a esse padrão.

106
00:05:36,080 --> 00:05:38,900
Portanto, é uma redução enorme em relação aos 13.000.

107
00:05:38,900 --> 00:05:43,360
Mas o outro lado disso, claro, é que é muito incomum obter um padrão como este.

108
00:05:43,360 --> 00:05:47,571
Especificamente, se cada palavra tivesse a mesma probabilidade de ser a resposta,

109
00:05:47,571 --> 00:05:51,680
a probabilidade de atingir esse padrão seria de 58 dividido por cerca de 13.000.

110
00:05:51,680 --> 00:05:53,880
É claro que não têm a mesma probabilidade de serem respostas.

111
00:05:53,880 --> 00:05:56,680
A maioria destas são palavras muito obscuras e até questionáveis.

112
00:05:56,680 --> 00:05:59,325
Mas pelo menos para a nossa primeira passagem por tudo isso, vamos supor que

113
00:05:59,325 --> 00:06:02,040
todas elas sejam igualmente prováveis e então refinar isso um pouco mais tarde.

114
00:06:02,040 --> 00:06:04,619
A questão é que o padrão com muitas informações

115
00:06:04,619 --> 00:06:07,360
é, por sua própria natureza, improvável de ocorrer.

116
00:06:07,360 --> 00:06:11,920
Na verdade, o que significa ser informativo é que é improvável.

117
00:06:11,920 --> 00:06:15,230
Um padrão muito mais provável de se ver nesta abertura

118
00:06:15,230 --> 00:06:18,360
seria algo assim, onde é claro que não há um W nela.

119
00:06:18,360 --> 00:06:22,080
Talvez haja um E, e talvez não haja A, não haja R, não haja Y.

120
00:06:22,080 --> 00:06:24,640
Neste caso, existem 1.400 correspondências possíveis.

121
00:06:24,640 --> 00:06:27,808
Se todos fossem igualmente prováveis, haveria uma probabilidade

122
00:06:27,808 --> 00:06:30,680
de cerca de 11% de que esse fosse o padrão que você veria.

123
00:06:30,680 --> 00:06:34,320
Portanto, os resultados mais prováveis são também os menos informativos.

124
00:06:34,320 --> 00:06:38,029
Para obter uma visão mais global aqui, deixe-me mostrar a distribuição

125
00:06:38,029 --> 00:06:42,000
completa de probabilidades em todos os diferentes padrões que você pode ver.

126
00:06:42,000 --> 00:06:45,621
Então cada barra que você está olhando corresponde a um possível padrão de

127
00:06:45,621 --> 00:06:49,145
cores que podem ser reveladas, das quais existem 3 a 5 possibilidades, e

128
00:06:49,145 --> 00:06:52,960
estão organizadas da esquerda para a direita, da mais comum para a menos comum.

129
00:06:52,960 --> 00:06:56,200
Portanto, a possibilidade mais comum aqui é que você obtenha todos os tons de cinza.

130
00:06:56,200 --> 00:06:58,800
Isso acontece cerca de 14% das vezes.

131
00:06:58,800 --> 00:07:02,489
E o que você espera quando faz uma suposição é que você acabe em algum

132
00:07:02,489 --> 00:07:06,074
lugar nesta cauda longa, como aqui, onde há apenas 18 possibilidades

133
00:07:06,074 --> 00:07:09,920
para o que corresponde a esse padrão que evidentemente se parece com este.

134
00:07:09,920 --> 00:07:12,335
Ou se nos aventurarmos um pouco mais para a esquerda,

135
00:07:12,335 --> 00:07:14,080
você sabe, talvez possamos ir até aqui.

136
00:07:14,080 --> 00:07:16,560
Ok, aqui está um bom quebra-cabeça para você.

137
00:07:16,560 --> 00:07:19,117
Quais são as três palavras da língua inglesa que

138
00:07:19,117 --> 00:07:22,040
começam com W, terminam com Y e têm um R em algum lugar?

139
00:07:22,040 --> 00:07:27,560
Acontece que as respostas são, vejamos, prolixas, minhocas e ironicamente.

140
00:07:27,560 --> 00:07:31,960
Então, para avaliar o quão boa esta palavra é em geral, queremos algum tipo de

141
00:07:31,960 --> 00:07:36,360
medida da quantidade esperada de informação que você obterá desta distribuição.

142
00:07:36,360 --> 00:07:41,208
Se analisarmos cada padrão e multiplicarmos sua probabilidade de ocorrência por algo

143
00:07:41,208 --> 00:07:46,000
que meça o quão informativo ele é, isso talvez possa nos dar uma pontuação objetiva.

144
00:07:46,000 --> 00:07:48,076
Agora, seu primeiro instinto sobre o que deveria

145
00:07:48,076 --> 00:07:50,280
ser esse algo pode ser o número de correspondências.

146
00:07:50,280 --> 00:07:52,960
Você deseja um número médio menor de correspondências.

147
00:07:52,960 --> 00:07:58,455
Mas, em vez disso, gostaria de usar uma medida mais universal que frequentemente

148
00:07:58,455 --> 00:08:04,154
atribuímos à informação, e que será mais flexível quando tivermos uma probabilidade

149
00:08:04,154 --> 00:08:09,989
diferente atribuída a cada uma destas 13.000 palavras para determinar se são ou não a

150
00:08:09,989 --> 00:08:10,600
resposta.

151
00:08:10,600 --> 00:08:14,010
A unidade padrão de informação é o bit, que tem uma fórmula um

152
00:08:14,010 --> 00:08:17,800
pouco engraçada, mas é muito intuitiva se olharmos apenas os exemplos.

153
00:08:17,800 --> 00:08:20,974
Se você tem uma observação que reduz pela metade o seu espaço

154
00:08:20,974 --> 00:08:24,200
de possibilidades, dizemos que ela contém um bit de informação.

155
00:08:24,200 --> 00:08:26,755
No nosso exemplo, o espaço de possibilidades são todas as palavras

156
00:08:26,755 --> 00:08:29,195
possíveis, e acontece que cerca de metade das palavras de cinco

157
00:08:29,195 --> 00:08:31,560
letras têm um S, um pouco menos que isso, mas cerca de metade.

158
00:08:31,560 --> 00:08:35,200
Portanto, essa observação lhe daria um pouco de informação.

159
00:08:35,200 --> 00:08:38,725
Se, em vez disso, um facto novo reduzir esse espaço de possibilidades

160
00:08:38,725 --> 00:08:42,000
por um factor de quatro, dizemos que tem dois bits de informação.

161
00:08:42,000 --> 00:08:45,120
Por exemplo, cerca de um quarto dessas palavras tem T.

162
00:08:45,120 --> 00:08:47,970
Se a observação reduzir esse espaço por um fator de oito,

163
00:08:47,970 --> 00:08:50,920
dizemos que são três bits de informação, e assim por diante.

164
00:08:50,920 --> 00:08:55,000
Quatro bits equivalem a um 16º, cinco bits equivalem a um 32º.

165
00:08:55,000 --> 00:08:59,670
Então agora você pode querer fazer uma pausa e se perguntar: qual é a fórmula

166
00:08:59,670 --> 00:09:04,520
da informação para o número de bits em termos da probabilidade de uma ocorrência?

167
00:09:04,520 --> 00:09:08,264
O que estamos dizendo aqui é que quando você eleva metade do número de bits, isso

168
00:09:08,264 --> 00:09:12,100
é a mesma coisa que a probabilidade, que é a mesma coisa que dizer que dois elevado

169
00:09:12,100 --> 00:09:15,890
à potência do número de bits é um sobre a probabilidade, que reorganiza ainda mais

170
00:09:15,890 --> 00:09:19,680
para dizer que a informação é o log de base dois de um dividido pela probabilidade.

171
00:09:19,680 --> 00:09:22,629
E às vezes você vê isso com mais um rearranjo ainda, onde

172
00:09:22,629 --> 00:09:25,680
a informação é o log negativo na base dois da probabilidade.

173
00:09:25,680 --> 00:09:28,874
Expressado desta forma, pode parecer um pouco estranho para os não

174
00:09:28,874 --> 00:09:31,830
iniciados, mas na verdade é apenas a ideia muito intuitiva de

175
00:09:31,830 --> 00:09:35,120
perguntar quantas vezes você reduziu suas possibilidades pela metade.

176
00:09:35,120 --> 00:09:37,384
Agora, se você está se perguntando, você sabe, pensei que estávamos apenas

177
00:09:37,384 --> 00:09:39,920
jogando um divertido jogo de palavras, por que os logaritmos estão entrando em cena?

178
00:09:39,920 --> 00:09:44,226
Uma razão pela qual esta unidade é melhor é que é muito mais fácil falar sobre

179
00:09:44,226 --> 00:09:48,369
eventos muito improváveis, muito mais fácil dizer que uma observação tem 20

180
00:09:48,369 --> 00:09:52,785
bits de informação do que dizer que a probabilidade de tal ou tal ocorrência é 0.

181
00:09:52,785 --> 00:09:53,480
0000095.

182
00:09:53,480 --> 00:09:57,714
Mas uma razão mais substantiva pela qual esta expressão logarítmica se revelou um

183
00:09:57,714 --> 00:10:02,000
acréscimo muito útil à teoria da probabilidade é a forma como a informação se soma.

184
00:10:02,000 --> 00:10:05,866
Por exemplo, se uma observação fornece dois bits de informação, reduzindo

185
00:10:05,866 --> 00:10:09,575
seu espaço em quatro, e então uma segunda observação, como sua segunda

186
00:10:09,575 --> 00:10:13,337
estimativa no Wordle, fornece outros três bits de informação, reduzindo

187
00:10:13,337 --> 00:10:17,360
ainda mais por outro fator de oito, o dois juntos fornecem cinco informações.

188
00:10:17,360 --> 00:10:19,343
Da mesma forma que as probabilidades gostam de

189
00:10:19,343 --> 00:10:21,200
se multiplicar, a informação gosta de somar.

190
00:10:21,200 --> 00:10:25,002
Então, assim que estamos no reino de algo como um valor esperado, onde estamos

191
00:10:25,002 --> 00:10:28,660
somando um monte de números, os logs tornam muito mais fácil lidar com isso.

192
00:10:28,660 --> 00:10:32,159
Vamos voltar à nossa distribuição para Weary e adicionar outro pequeno

193
00:10:32,159 --> 00:10:35,560
rastreador aqui, mostrando quanta informação existe para cada padrão.

194
00:10:35,560 --> 00:10:38,076
A principal coisa que quero que você observe é que quanto

195
00:10:38,076 --> 00:10:40,679
maior a probabilidade à medida que chegamos a esses padrões

196
00:10:40,679 --> 00:10:43,500
mais prováveis, quanto menor a informação, menos bits você ganha.

197
00:10:43,500 --> 00:10:47,186
A forma como medimos a qualidade dessa suposição será pegar o valor

198
00:10:47,186 --> 00:10:50,982
esperado dessa informação, onde percorremos cada padrão, dizemos quão

199
00:10:50,982 --> 00:10:54,940
provável é, e então multiplicamos por quantos bits de informação obtemos.

200
00:10:54,940 --> 00:10:58,107
E no exemplo de Weary, isso resulta em 4.

201
00:10:58,107 --> 00:10:58,480
9 bits.

202
00:10:58,480 --> 00:11:02,091
Então, em média, as informações que você obtém com essa estimativa inicial são tão

203
00:11:02,091 --> 00:11:05,660
boas quanto cortar seu espaço de possibilidades pela metade, cerca de cinco vezes.

204
00:11:05,660 --> 00:11:09,370
Por outro lado, um exemplo de suposição com um valor

205
00:11:09,370 --> 00:11:13,220
de informação esperado mais alto seria algo como Slate.

206
00:11:13,220 --> 00:11:16,180
Neste caso você notará que a distribuição parece muito mais plana.

207
00:11:16,180 --> 00:11:20,307
Em particular, a ocorrência mais provável de todos os tons de cinza tem apenas

208
00:11:20,307 --> 00:11:24,435
cerca de 6% de chance de ocorrer, então, no mínimo, você obtém evidentemente 3.

209
00:11:24,435 --> 00:11:25,940
9 bits de informação.

210
00:11:25,940 --> 00:11:29,140
Mas isso é o mínimo, mais normalmente você conseguiria algo melhor que isso.

211
00:11:29,140 --> 00:11:32,764
E acontece que quando você analisa os números deste aqui e soma

212
00:11:32,764 --> 00:11:36,333
todos os termos relevantes, a informação média é de cerca de 5.

213
00:11:36,333 --> 00:11:36,420
8.

214
00:11:36,420 --> 00:11:40,207
Portanto, em contraste com Weary, seu espaço de possibilidades será

215
00:11:40,207 --> 00:11:43,940
cerca de metade do tamanho após essa primeira estimativa, em média.

216
00:11:43,940 --> 00:11:46,796
Na verdade, há uma história divertida sobre o nome

217
00:11:46,796 --> 00:11:49,540
desse valor esperado da quantidade de informação.

218
00:11:49,540 --> 00:11:53,179
A teoria da informação foi desenvolvida por Claude Shannon, que trabalhava no Bell Labs

219
00:11:53,179 --> 00:11:56,860
na década de 1940, mas ele estava conversando sobre algumas de suas ideias ainda a serem

220
00:11:56,860 --> 00:12:00,168
publicadas com John von Neumann, que era um gigante intelectual da época, muito

221
00:12:00,168 --> 00:12:03,725
proeminente. em matemática e física e o início do que estava se tornando a ciência da

222
00:12:03,725 --> 00:12:04,180
computação.

223
00:12:04,180 --> 00:12:07,572
E quando ele mencionou que não tinha realmente um bom nome para esse valor

224
00:12:07,572 --> 00:12:11,010
esperado da quantidade de informação, von Neumann supostamente disse, assim

225
00:12:11,010 --> 00:12:14,720
continua a história, bem, você deveria chamar isso de entropia, e por duas razões.

226
00:12:14,720 --> 00:12:18,809
Em primeiro lugar, a sua função de incerteza tem sido usada na mecânica estatística

227
00:12:18,809 --> 00:12:22,655
com esse nome, por isso já tem um nome, e em segundo lugar, e mais importante,

228
00:12:22,655 --> 00:12:26,940
ninguém sabe o que realmente é entropia, por isso num debate você sempre tem a vantagem.

229
00:12:26,940 --> 00:12:30,116
Então, se o nome parece um pouco misterioso, e se

230
00:12:30,116 --> 00:12:33,420
é para acreditar nessa história, isso é intencional.

231
00:12:33,420 --> 00:12:36,955
Além disso, se você está se perguntando sobre sua relação com toda a segunda

232
00:12:36,955 --> 00:12:40,444
lei da termodinâmica da física, definitivamente há uma conexão, mas em suas

233
00:12:40,444 --> 00:12:43,841
origens Shannon estava apenas lidando com a teoria pura da probabilidade,

234
00:12:43,841 --> 00:12:47,376
e para nossos propósitos aqui, quando eu uso o entropia de palavra, só quero

235
00:12:47,376 --> 00:12:50,820
que você pense no valor de informação esperado de uma suposição específica.

236
00:12:50,820 --> 00:12:54,380
Você pode pensar na entropia como uma medida de duas coisas simultaneamente.

237
00:12:54,380 --> 00:12:57,420
A primeira é quão plana é a distribuição.

238
00:12:57,420 --> 00:13:01,700
Quanto mais próxima a distribuição estiver da uniformidade, maior será a entropia.

239
00:13:01,700 --> 00:13:05,014
No nosso caso, onde há 3 elevado a 5 padrões totais, para uma

240
00:13:05,014 --> 00:13:08,383
distribuição uniforme, a observação de qualquer um deles teria

241
00:13:08,383 --> 00:13:11,858
log de informações de base 2 de 3 elevado a 5, que passa a ser 7.

242
00:13:11,858 --> 00:13:17,860
92, então esse é o máximo absoluto que você poderia ter para esta entropia.

243
00:13:17,860 --> 00:13:22,900
Mas a entropia também é uma espécie de medida de quantas possibilidades existem.

244
00:13:22,900 --> 00:13:27,830
Por exemplo, se acontecer de você ter alguma palavra onde há apenas 16 padrões possíveis,

245
00:13:27,830 --> 00:13:32,760
e cada um é igualmente provável, essa entropia, essa informação esperada, seria de 4 bits.

246
00:13:32,760 --> 00:13:37,009
Mas se você tiver outra palavra onde há 64 padrões possíveis que poderiam surgir,

247
00:13:37,009 --> 00:13:41,000
e todos eles são igualmente prováveis, então a entropia resultaria em 6 bits.

248
00:13:41,000 --> 00:13:45,466
Então, se você vir alguma distribuição que tenha uma entropia de 6 bits,

249
00:13:45,466 --> 00:13:49,994
é como se estivesse dizendo que há tanta variação e incerteza no que está

250
00:13:49,994 --> 00:13:54,400
prestes a acontecer como se houvesse 64 resultados igualmente prováveis.

251
00:13:54,400 --> 00:13:58,360
Para minha primeira passagem no Wurtelebot, basicamente fiz isso.

252
00:13:58,360 --> 00:14:02,886
Ele analisa todas as suposições possíveis que você poderia ter, todas as 13.000

253
00:14:02,886 --> 00:14:07,468
palavras, calcula a entropia de cada uma, ou mais especificamente, a entropia da

254
00:14:07,468 --> 00:14:12,221
distribuição em todos os padrões que você pode ver, para cada uma, e escolhe o mais

255
00:14:12,221 --> 00:14:17,200
alto, já que é aquele que provavelmente reduzirá ao máximo seu espaço de possibilidades.

256
00:14:17,200 --> 00:14:19,280
E mesmo que eu tenha falado apenas sobre a primeira

257
00:14:19,280 --> 00:14:21,680
suposição aqui, acontece o mesmo com as próximas suposições.

258
00:14:21,680 --> 00:14:24,990
Por exemplo, depois de ver algum padrão nessa primeira suposição, que o

259
00:14:24,990 --> 00:14:28,714
restringiria a um número menor de palavras possíveis com base no que corresponde

260
00:14:28,714 --> 00:14:32,300
a isso, basta jogar o mesmo jogo em relação a esse conjunto menor de palavras.

261
00:14:32,300 --> 00:14:36,620
Para uma segunda suposição proposta, você olha para a distribuição de todos os

262
00:14:36,620 --> 00:14:40,831
padrões que podem ocorrer a partir desse conjunto mais restrito de palavras,

263
00:14:40,831 --> 00:14:45,480
pesquisa todas as 13.000 possibilidades e encontra aquela que maximiza essa entropia.

264
00:14:45,480 --> 00:14:49,887
Para mostrar como isso funciona em ação, deixe-me apenas apresentar uma pequena

265
00:14:49,887 --> 00:14:54,460
variante de Wurtele que escrevi, que mostra os destaques desta análise nas margens.

266
00:14:54,460 --> 00:14:57,523
Depois de fazer todos os cálculos de entropia, aqui à direita

267
00:14:57,523 --> 00:15:00,340
ele nos mostra quais possuem a maior informação esperada.

268
00:15:00,340 --> 00:15:05,740
Acontece que a resposta principal, pelo menos no momento, vamos refinar isso

269
00:15:05,740 --> 00:15:11,140
mais tarde, é Tares, que significa, claro, ervilhaca, a ervilhaca mais comum.

270
00:15:11,140 --> 00:15:14,242
Cada vez que fazemos um palpite aqui, onde talvez eu ignore suas

271
00:15:14,242 --> 00:15:17,821
recomendações e opte pelo slate, porque gosto do slate, podemos ver quanta

272
00:15:17,821 --> 00:15:21,162
informação esperada ele tinha, mas à direita da palavra aqui está nos

273
00:15:21,162 --> 00:15:24,980
mostrando o quanto informações reais que obtivemos, dado esse padrão específico.

274
00:15:24,980 --> 00:15:27,932
Então aqui parece que tivemos um pouco de azar, era esperado que tivéssemos 5.

275
00:15:27,932 --> 00:15:30,660
8, mas conseguimos algo com menos que isso.

276
00:15:30,660 --> 00:15:33,190
E então no lado esquerdo aqui está nos mostrando todas

277
00:15:33,190 --> 00:15:35,860
as diferentes palavras possíveis dadas onde estamos agora.

278
00:15:35,860 --> 00:15:38,364
As barras azuis nos dizem a probabilidade de cada palavra ser

279
00:15:38,364 --> 00:15:41,191
considerada, portanto, no momento, estamos assumindo que cada palavra

280
00:15:41,191 --> 00:15:44,140
tem a mesma probabilidade de ocorrer, mas refinaremos isso em um momento.

281
00:15:44,140 --> 00:15:47,963
E então esta medição de incerteza está a dizer-nos a entropia desta distribuição

282
00:15:47,963 --> 00:15:51,880
entre as palavras possíveis, que neste momento, por ser uma distribuição uniforme,

283
00:15:51,880 --> 00:15:55,940
é apenas uma forma desnecessariamente complicada de contar o número de possibilidades.

284
00:15:55,940 --> 00:15:59,343
Por exemplo, se elevarmos 2 elevado a 13.

285
00:15:59,343 --> 00:16:02,700
66, isso deveria estar em torno das 13.000 possibilidades.

286
00:16:02,700 --> 00:16:06,780
Estou um pouco errado aqui, mas só porque não estou mostrando todas as casas decimais.

287
00:16:06,780 --> 00:16:09,622
No momento, isso pode parecer redundante e complicar demais as

288
00:16:09,622 --> 00:16:12,780
coisas, mas você verá por que é útil ter os dois números em um minuto.

289
00:16:12,780 --> 00:16:16,164
Então aqui parece que está sugerindo que a entropia mais alta para

290
00:16:16,164 --> 00:16:19,700
nosso segundo palpite é Ramen, o que novamente não parece uma palavra.

291
00:16:19,700 --> 00:16:25,660
Então, para ter uma moral elevada aqui, vou prosseguir e digitar Rains.

292
00:16:25,660 --> 00:16:27,540
E novamente parece que tivemos um pouco de azar.

293
00:16:27,540 --> 00:16:28,872
Estávamos esperando 4.

294
00:16:28,872 --> 00:16:30,556
3 bits e só temos 3.

295
00:16:30,556 --> 00:16:32,100
39 bits de informação.

296
00:16:32,100 --> 00:16:35,060
Então isso nos leva a 55 possibilidades.

297
00:16:35,060 --> 00:16:40,200
E aqui talvez eu siga o que está sugerindo, que é combo, seja lá o que isso signifique.

298
00:16:40,200 --> 00:16:43,300
E tudo bem, esta é realmente uma boa chance para um quebra-cabeça.

299
00:16:43,300 --> 00:16:45,718
Está nos dizendo que esse padrão nos dá 4.

300
00:16:45,718 --> 00:16:47,020
7 bits de informação.

301
00:16:47,020 --> 00:16:50,990
Mas à esquerda, antes de vermos esse padrão, havia 5.

302
00:16:50,990 --> 00:16:52,400
78 bits de incerteza.

303
00:16:52,400 --> 00:16:54,862
Então, como um teste para você, o que isso significa

304
00:16:54,862 --> 00:16:56,860
sobre o número de possibilidades restantes?

305
00:16:56,860 --> 00:17:01,062
Bem, isso significa que estamos reduzidos a um pouco de incerteza,

306
00:17:01,062 --> 00:17:04,700
o que é o mesmo que dizer que há duas respostas possíveis.

307
00:17:04,700 --> 00:17:06,520
É uma escolha 50-50.

308
00:17:06,520 --> 00:17:08,849
E a partir daqui, porque você e eu sabemos quais palavras

309
00:17:08,849 --> 00:17:11,220
são mais comuns, sabemos que a resposta deveria ser abismo.

310
00:17:11,220 --> 00:17:13,540
Mas como está escrito agora, o programa não sabe disso.

311
00:17:13,540 --> 00:17:17,135
Então ele continua tentando obter o máximo de informações possível,

312
00:17:17,135 --> 00:17:20,360
até que reste apenas uma possibilidade, e então ele adivinha.

313
00:17:20,360 --> 00:17:22,700
Então, obviamente, precisamos de uma estratégia de final de jogo melhor.

314
00:17:22,700 --> 00:17:26,476
Mas digamos que chamamos esta versão de nosso solucionador de

315
00:17:26,476 --> 00:17:30,740
palavras e então executamos algumas simulações para ver como funciona.

316
00:17:30,740 --> 00:17:34,240
Então, a maneira como isso funciona é jogando todos os jogos de palavras possíveis.

317
00:17:34,240 --> 00:17:36,703
Ele está passando por todas aquelas 2.315 palavras

318
00:17:36,703 --> 00:17:38,780
que são as verdadeiras respostas do wordle.

319
00:17:38,780 --> 00:17:41,340
Basicamente, estamos usando isso como um conjunto de testes.

320
00:17:41,340 --> 00:17:45,962
E com esse método ingênuo de não considerar o quão comum uma palavra é, e apenas tentar

321
00:17:45,962 --> 00:17:50,480
maximizar a informação a cada passo do caminho, até chegar a uma e apenas uma escolha.

322
00:17:50,480 --> 00:17:54,912
Ao final da simulação, a pontuação média é de cerca de 4.

323
00:17:54,912 --> 00:17:55,100
124.

324
00:17:55,100 --> 00:17:59,780
O que não é ruim, para ser honesto, eu esperava fazer pior.

325
00:17:59,780 --> 00:18:03,040
Mas as pessoas que jogam wordle dirão que geralmente conseguem em 4.

326
00:18:03,040 --> 00:18:05,260
O verdadeiro desafio é conseguir o máximo possível em 3.

327
00:18:05,260 --> 00:18:08,920
É um salto muito grande entre a pontuação de 4 e a pontuação de 3.

328
00:18:08,920 --> 00:18:15,785
O objetivo óbvio aqui é incorporar de alguma forma se

329
00:18:15,785 --> 00:18:23,160
uma palavra é comum ou não e como exatamente fazemos isso.

330
00:18:23,160 --> 00:18:25,592
A forma como abordei isso foi obter uma lista das

331
00:18:25,592 --> 00:18:28,560
frequências relativas de todas as palavras da língua inglesa.

332
00:18:28,560 --> 00:18:32,205
E acabei de usar a função de dados de frequência de palavras do Mathematica,

333
00:18:32,205 --> 00:18:35,520
que extrai do conjunto de dados público Ngram do Google Books English.

334
00:18:35,520 --> 00:18:37,982
E é divertido de ver, por exemplo, se classificarmos

335
00:18:37,982 --> 00:18:40,120
das palavras mais comuns para as menos comuns.

336
00:18:40,120 --> 00:18:43,740
Evidentemente, essas são as palavras de 5 letras mais comuns na língua inglesa.

337
00:18:43,740 --> 00:18:46,480
Ou melhor, estes são os 8º mais comuns.

338
00:18:46,480 --> 00:18:49,440
O primeiro é qual, depois do qual existe ali e ali.

339
00:18:49,440 --> 00:18:52,596
O primeiro em si não é o primeiro, mas o 9º, e faz sentido que essas

340
00:18:52,596 --> 00:18:55,660
outras palavras possam surgir com mais frequência, onde as que vêm

341
00:18:55,660 --> 00:18:59,000
depois do primeiro são depois, onde, e aquelas são um pouco menos comuns.

342
00:18:59,000 --> 00:19:02,957
Agora, ao usar estes dados para modelar a probabilidade de cada uma destas

343
00:19:02,957 --> 00:19:07,020
palavras ser a resposta final, não deve ser apenas proporcional à frequência.

344
00:19:07,020 --> 00:19:09,596
Por exemplo, que recebe uma pontuação de 0.

345
00:19:09,596 --> 00:19:12,373
002 neste conjunto de dados, enquanto a palavra trança

346
00:19:12,373 --> 00:19:15,200
é, em certo sentido, cerca de 1000 vezes menos provável.

347
00:19:15,200 --> 00:19:17,300
Mas ambas são palavras comuns o suficiente para

348
00:19:17,300 --> 00:19:19,400
que quase certamente valha a pena considerá-las.

349
00:19:19,400 --> 00:19:21,900
Então, queremos mais um corte binário.

350
00:19:21,900 --> 00:19:26,102
A maneira como fiz isso foi imaginar pegar toda essa lista ordenada de palavras

351
00:19:26,102 --> 00:19:30,410
e, em seguida, organizá-la em um eixo x e, em seguida, aplicar a função sigmóide,

352
00:19:30,410 --> 00:19:34,612
que é a maneira padrão de ter uma função cuja saída é basicamente binária, é ou

353
00:19:34,612 --> 00:19:38,500
0 ou 1, mas há uma suavização intermediária para essa região de incerteza.

354
00:19:38,500 --> 00:19:44,083
Então, essencialmente, a probabilidade que estou atribuindo a cada palavra por estar na

355
00:19:44,083 --> 00:19:49,540
lista final será o valor da função sigmóide acima, onde quer que ela esteja no eixo x.

356
00:19:49,540 --> 00:19:53,915
Agora, obviamente, isso depende de alguns parâmetros, por exemplo, a largura do

357
00:19:53,915 --> 00:19:58,565
espaço no eixo x que essas palavras preenchem determina quão gradual ou abruptamente

358
00:19:58,565 --> 00:20:03,160
caímos de 1 para 0, e onde as situamos da esquerda para a direita determina o corte.

359
00:20:03,160 --> 00:20:07,340
Para ser sincero, fiz isso apenas lambendo o dedo e apontando-o contra o vento.

360
00:20:07,340 --> 00:20:10,850
Examinei a lista classificada e tentei encontrar uma janela onde, quando

361
00:20:10,850 --> 00:20:14,265
olhei para ela, descobri que cerca de metade dessas palavras têm maior

362
00:20:14,265 --> 00:20:17,680
probabilidade de ser a resposta final, e usei isso como ponto de corte.

363
00:20:17,680 --> 00:20:20,856
Uma vez que tenhamos uma distribuição como esta entre as palavras,

364
00:20:20,856 --> 00:20:24,460
teremos outra situação em que a entropia se torna uma medida realmente útil.

365
00:20:24,460 --> 00:20:27,467
Por exemplo, digamos que estamos jogando um jogo e começamos com

366
00:20:27,467 --> 00:20:30,521
meus antigos abridores, que eram penas e pregos, e terminamos com

367
00:20:30,521 --> 00:20:33,760
uma situação em que há quatro palavras possíveis que combinam com ele.

368
00:20:33,760 --> 00:20:36,440
E digamos que consideramos todos igualmente prováveis.

369
00:20:36,440 --> 00:20:40,000
Deixe-me perguntar: qual é a entropia dessa distribuição?

370
00:20:40,000 --> 00:20:45,312
Bem, a informação associada a cada uma dessas possibilidades

371
00:20:45,312 --> 00:20:50,800
será o log de base 2 de 4, já que cada uma é 1 e 4, e isso é 2.

372
00:20:50,800 --> 00:20:52,780
Duas informações, quatro possibilidades.

373
00:20:52,780 --> 00:20:54,360
Tudo muito bem e bom.

374
00:20:54,360 --> 00:20:58,320
Mas e se eu te dissesse que na verdade são mais de quatro partidas?

375
00:20:58,320 --> 00:21:00,482
Na realidade, quando olhamos a lista completa de

376
00:21:00,482 --> 00:21:02,600
palavras, há 16 palavras que correspondem a ela.

377
00:21:02,600 --> 00:21:05,468
Mas suponha que nosso modelo atribua uma probabilidade muito

378
00:21:05,468 --> 00:21:08,430
baixa a essas outras 12 palavras de serem realmente a resposta

379
00:21:08,430 --> 00:21:11,440
final, algo como 1 em 1.000, porque elas são realmente obscuras.

380
00:21:11,440 --> 00:21:15,480
Agora deixe-me perguntar: qual é a entropia desta distribuição?

381
00:21:15,480 --> 00:21:19,035
Se a entropia medisse puramente o número de correspondências aqui,

382
00:21:19,035 --> 00:21:22,538
então você poderia esperar que fosse algo como o log de base 2 de

383
00:21:22,538 --> 00:21:26,200
16, que seria 4, dois bits a mais de incerteza do que tínhamos antes.

384
00:21:26,200 --> 00:21:30,320
Mas é claro que a incerteza real não é muito diferente daquela que tínhamos antes.

385
00:21:30,320 --> 00:21:34,079
Só porque existem essas 12 palavras realmente obscuras não significa que

386
00:21:34,079 --> 00:21:38,200
seria ainda mais surpreendente saber que a resposta final é charme, por exemplo.

387
00:21:38,200 --> 00:21:41,907
Então, quando você realmente faz o cálculo aqui e soma a probabilidade de

388
00:21:41,907 --> 00:21:45,514
cada ocorrência vezes a informação correspondente, o que você obtém é 2.

389
00:21:45,514 --> 00:21:45,960
11 bits.

390
00:21:45,960 --> 00:21:49,870
Só estou dizendo que são basicamente dois bits, basicamente essas quatro possibilidades,

391
00:21:49,870 --> 00:21:53,824
mas há um pouco mais de incerteza por causa de todos esses eventos altamente improváveis,

392
00:21:53,824 --> 00:21:57,120
embora se você os aprendesse, obteria uma tonelada de informações com isso.

393
00:21:57,120 --> 00:21:59,481
Diminuindo o zoom, isso é parte do que torna o Wordle

394
00:21:59,481 --> 00:22:01,800
um bom exemplo para uma aula de teoria da informação.

395
00:22:01,800 --> 00:22:05,280
Temos essas duas aplicações de sentimento distintas para a entropia.

396
00:22:05,280 --> 00:22:08,934
O primeiro nos diz qual é a informação esperada que obteremos

397
00:22:08,934 --> 00:22:12,648
de uma determinada suposição, e o segundo diz se podemos medir

398
00:22:12,648 --> 00:22:16,480
a incerteza restante entre todas as palavras que temos possíveis.

399
00:22:16,480 --> 00:22:19,229
E devo enfatizar, nesse primeiro caso em que estamos olhando

400
00:22:19,229 --> 00:22:22,069
para a informação esperada de um palpite, uma vez que temos um

401
00:22:22,069 --> 00:22:25,000
peso desigual para as palavras, isso afeta o cálculo da entropia.

402
00:22:25,000 --> 00:22:28,481
Por exemplo, deixe-me abordar o mesmo caso que vimos anteriormente

403
00:22:28,481 --> 00:22:31,598
da distribuição associada a Weary, mas desta vez usando uma

404
00:22:31,598 --> 00:22:34,560
distribuição não uniforme em todas as palavras possíveis.

405
00:22:34,560 --> 00:22:39,360
Então deixe-me ver se consigo encontrar uma parte aqui que ilustre isso muito bem.

406
00:22:39,360 --> 00:22:42,480
Ok, aqui isso é muito bom.

407
00:22:42,480 --> 00:22:46,028
Aqui temos dois padrões adjacentes que são igualmente prováveis, mas nos

408
00:22:46,028 --> 00:22:49,480
disseram que um deles tem 32 palavras possíveis que correspondem a ele.

409
00:22:49,480 --> 00:22:52,489
E se verificarmos o que são, estas são aquelas 32, que são

410
00:22:52,489 --> 00:22:55,600
apenas palavras muito improváveis quando você olha para elas.

411
00:22:55,600 --> 00:22:59,061
É difícil encontrar alguma que pareça ser uma resposta plausível, talvez

412
00:22:59,061 --> 00:23:02,333
gritos, mas se olharmos para o padrão vizinho na distribuição, que é

413
00:23:02,333 --> 00:23:05,605
considerado quase tão provável, somos informados de que ele só tem 8

414
00:23:05,605 --> 00:23:09,019
correspondências possíveis, então um quarto como muitas partidas, mas é

415
00:23:09,019 --> 00:23:09,920
quase tão provável.

416
00:23:09,920 --> 00:23:12,520
E quando puxamos esses fósforos, podemos ver porquê.

417
00:23:12,520 --> 00:23:17,840
Algumas delas são respostas realmente plausíveis, como anel, ou ira, ou batidas.

418
00:23:17,840 --> 00:23:22,026
Para ilustrar como incorporamos tudo isso, deixe-me trazer a versão 2 do Wordlebot

419
00:23:22,026 --> 00:23:25,960
aqui, e há duas ou três diferenças principais em relação à primeira que vimos.

420
00:23:25,960 --> 00:23:30,406
Em primeiro lugar, como acabei de dizer, a forma como calculamos estas entropias, estes

421
00:23:30,406 --> 00:23:34,701
valores esperados de informação, utiliza agora distribuições mais refinadas entre os

422
00:23:34,701 --> 00:23:38,845
padrões que incorporam a probabilidade de uma determinada palavra ser realmente a

423
00:23:38,845 --> 00:23:39,300
resposta.

424
00:23:39,300 --> 00:23:41,756
Acontece que as lágrimas ainda são o número 1,

425
00:23:41,756 --> 00:23:44,160
embora as seguintes sejam um pouco diferentes.

426
00:23:44,160 --> 00:23:47,917
Em segundo lugar, quando classificar as suas principais escolhas, irá manter um modelo

427
00:23:47,917 --> 00:23:51,502
da probabilidade de cada palavra ser a resposta real, e irá incorporar isso na sua

428
00:23:51,502 --> 00:23:55,304
decisão, o que é mais fácil de ver quando tivermos algumas suposições sobre a resposta.

429
00:23:55,304 --> 00:23:55,520
mesa.

430
00:23:55,520 --> 00:23:58,220
Mais uma vez, ignorando a sua recomendação porque não

431
00:23:58,220 --> 00:24:01,120
podemos permitir que as máquinas governem as nossas vidas.

432
00:24:01,120 --> 00:24:03,899
E suponho que devo mencionar outra coisa diferente aqui à

433
00:24:03,899 --> 00:24:06,821
esquerda, que o valor da incerteza, esse número de bits, não

434
00:24:06,821 --> 00:24:10,080
é mais apenas redundante com o número de correspondências possíveis.

435
00:24:10,080 --> 00:24:13,489
Agora, se puxarmos para cima e calcularmos 2 elevado a 8.

436
00:24:13,489 --> 00:24:17,616
02, que está um pouco acima de 256, acho que 259, o que está dizendo

437
00:24:17,616 --> 00:24:21,804
é que embora haja um total de 526 palavras que realmente correspondam

438
00:24:21,804 --> 00:24:25,931
a esse padrão, a quantidade de incerteza que ele tem é mais parecida

439
00:24:25,931 --> 00:24:29,760
com o que seria se houvesse 259 igualmente prováveis resultados.

440
00:24:29,760 --> 00:24:31,100
Você pode pensar assim.

441
00:24:31,100 --> 00:24:34,343
Ele sabe que borx não é a resposta, o mesmo acontece com yorts,

442
00:24:34,343 --> 00:24:37,840
zorl e zorus, então é um pouco menos incerto do que no caso anterior.

443
00:24:37,840 --> 00:24:40,220
Este número de bits será menor.

444
00:24:40,220 --> 00:24:44,313
E se eu continuar jogando, estou refinando isso com algumas

445
00:24:44,313 --> 00:24:48,680
suposições que são pertinentes ao que gostaria de explicar aqui.

446
00:24:48,680 --> 00:24:51,240
Na quarta estimativa, se você olhar as principais opções,

447
00:24:51,240 --> 00:24:53,800
verá que não se trata mais apenas de maximizar a entropia.

448
00:24:53,800 --> 00:24:57,290
Então, neste ponto, existem tecnicamente sete possibilidades, mas

449
00:24:57,290 --> 00:25:00,780
as únicas com uma chance significativa são dormitórios e palavras.

450
00:25:00,780 --> 00:25:04,071
E você pode ver que ele classifica escolhendo ambos acima de todos

451
00:25:04,071 --> 00:25:07,560
esses outros valores, que estritamente falando dariam mais informações.

452
00:25:07,560 --> 00:25:10,935
Na primeira vez que fiz isso, apenas somei esses dois números para medir a

453
00:25:10,935 --> 00:25:14,580
qualidade de cada palpite, o que na verdade funcionou melhor do que você imagina.

454
00:25:14,580 --> 00:25:17,211
Mas realmente não parecia sistemático, e tenho certeza de que há outras

455
00:25:17,211 --> 00:25:19,880
abordagens que as pessoas poderiam adotar, mas aqui está a que encontrei.

456
00:25:19,880 --> 00:25:24,359
Se estivermos considerando a perspectiva de um próximo palpite, como neste caso palavras,

457
00:25:24,359 --> 00:25:28,440
o que realmente nos importa é a pontuação esperada do nosso jogo se fizermos isso.

458
00:25:28,440 --> 00:25:32,344
E para calcular a pontuação esperada, dizemos qual é a probabilidade

459
00:25:32,344 --> 00:25:36,080
de as palavras serem a resposta real, que no momento descreve 58%.

460
00:25:36,080 --> 00:25:40,400
Dizemos que com 58% de chance nossa pontuação neste jogo seria 4.

461
00:25:40,400 --> 00:25:46,240
E então, com a probabilidade de 1 menos 58%, nossa pontuação será maior que 4.

462
00:25:46,240 --> 00:25:49,628
Quanto mais não sabemos, mas podemos estimá-lo com base na quantidade

463
00:25:49,628 --> 00:25:52,920
de incerteza que provavelmente haverá quando chegarmos a esse ponto.

464
00:25:52,920 --> 00:25:55,227
Especificamente, no momento há 1.

465
00:25:55,227 --> 00:25:56,600
44 bits de incerteza.

466
00:25:56,600 --> 00:26:01,131
Se adivinharmos as palavras, isso nos diz que a informação esperada que obteremos é 1.

467
00:26:01,131 --> 00:26:01,560
27 bits.

468
00:26:01,560 --> 00:26:04,819
Portanto, se adivinharmos as palavras, esta diferença representa

469
00:26:04,819 --> 00:26:08,280
quanta incerteza provavelmente nos restará depois que isso acontecer.

470
00:26:08,280 --> 00:26:11,008
O que precisamos é de algum tipo de função, que chamo de

471
00:26:11,008 --> 00:26:13,880
f aqui, que associe essa incerteza a uma pontuação esperada.

472
00:26:13,880 --> 00:26:18,383
E a maneira como isso aconteceu foi apenas traçar um monte de dados de jogos

473
00:26:18,383 --> 00:26:22,828
anteriores com base na versão 1 do bot para dizer, ei, qual foi a pontuação

474
00:26:22,828 --> 00:26:27,040
real após vários pontos com certas quantidades mensuráveis de incerteza.

475
00:26:27,040 --> 00:26:31,073
Por exemplo, esses pontos de dados aqui estão acima de um valor próximo a 8.

476
00:26:31,073 --> 00:26:35,426
7 ou mais são o que dizem para alguns jogos depois de um ponto em que havia 8.

477
00:26:35,426 --> 00:26:39,340
7 bits de incerteza, foram necessárias duas tentativas para obter a resposta final.

478
00:26:39,340 --> 00:26:41,241
Para outros jogos foram necessários três palpites,

479
00:26:41,241 --> 00:26:43,180
para outros jogos foram necessários quatro palpites.

480
00:26:43,180 --> 00:26:47,103
Se mudarmos para a esquerda aqui, todos os pontos acima de zero dizem que sempre

481
00:26:47,103 --> 00:26:50,979
que há zero bits de incerteza, o que significa que há apenas uma possibilidade,

482
00:26:50,979 --> 00:26:55,000
então o número de suposições necessárias é sempre apenas um, o que é reconfortante.

483
00:26:55,000 --> 00:26:57,918
Sempre que havia um pouco de incerteza, o que significa que se

484
00:26:57,918 --> 00:27:00,651
resumia essencialmente a duas possibilidades, às vezes era

485
00:27:00,651 --> 00:27:03,940
necessário mais um palpite, às vezes era necessário mais dois palpites.

486
00:27:03,940 --> 00:27:05,980
E assim por diante aqui.

487
00:27:05,980 --> 00:27:08,762
Talvez uma maneira um pouco mais fácil de visualizar

488
00:27:08,762 --> 00:27:11,020
esses dados seja agrupá-los e tirar médias.

489
00:27:11,020 --> 00:27:16,629
Por exemplo, esta barra aqui diz que entre todos os pontos onde tivemos um pouco

490
00:27:16,629 --> 00:27:22,308
de incerteza, em média o número de novas suposições necessárias foi de cerca de 1.

491
00:27:22,308 --> 00:27:22,420
5.

492
00:27:22,420 --> 00:27:27,009
E a barra aqui dizendo entre todos os jogos diferentes onde em algum momento a incerteza

493
00:27:27,009 --> 00:27:31,186
estava um pouco acima de quatro bits, o que é como reduzi-la a 16 possibilidades

494
00:27:31,186 --> 00:27:35,827
diferentes, então, em média, requer um pouco mais de duas suposições a partir desse ponto

495
00:27:35,827 --> 00:27:36,240
avançar.

496
00:27:36,240 --> 00:27:38,063
E a partir daqui fiz apenas uma regressão para

497
00:27:38,063 --> 00:27:40,080
ajustar uma função que parecesse razoável para isso.

498
00:27:40,080 --> 00:27:44,818
E lembre-se que o objetivo de fazer isso é para que possamos quantificar essa intuição

499
00:27:44,818 --> 00:27:49,720
de que quanto mais informações obtivermos de uma palavra, menor será a pontuação esperada.

500
00:27:49,720 --> 00:27:51,043
Então, com isso como versão 2.

501
00:27:51,043 --> 00:27:55,401
0, se voltarmos e executarmos o mesmo conjunto de simulações, fazendo-o

502
00:27:55,401 --> 00:27:59,820
jogar contra todas as 2.315 respostas possíveis do Wordle, como funciona?

503
00:27:59,820 --> 00:28:01,917
Bem, em contraste com a nossa primeira versão,

504
00:28:01,917 --> 00:28:04,060
é definitivamente melhor, o que é reconfortante.

505
00:28:04,060 --> 00:28:06,284
Tudo dito e feito, a média é de cerca de 3.

506
00:28:06,284 --> 00:28:09,465
6, embora ao contrário da primeira versão haja algumas

507
00:28:09,465 --> 00:28:12,820
vezes que perde e requer mais de seis nesta circunstância.

508
00:28:12,820 --> 00:28:15,877
Presumivelmente porque há momentos em que é preciso fazer essa troca

509
00:28:15,877 --> 00:28:18,980
para realmente atingir o objetivo, em vez de maximizar as informações.

510
00:28:18,980 --> 00:28:22,022
Então, podemos fazer melhor que 3.

511
00:28:22,022 --> 00:28:22,140
6?

512
00:28:22,140 --> 00:28:23,260
Nós definitivamente podemos.

513
00:28:23,260 --> 00:28:26,396
Agora eu disse no início que é mais divertido tentar não incorporar a

514
00:28:26,396 --> 00:28:29,980
verdadeira lista de respostas do Wordle na maneira como ela constrói seu modelo.

515
00:28:29,980 --> 00:28:35,043
Mas se incorporarmos isso, o melhor desempenho que consegui foi em torno de 3.

516
00:28:35,043 --> 00:28:35,180
43.

517
00:28:35,180 --> 00:28:38,028
Então, se tentarmos ser mais sofisticados do que apenas usar dados de

518
00:28:38,028 --> 00:28:40,957
frequência de palavras para escolher esta distribuição anterior, este 3.

519
00:28:40,957 --> 00:28:43,658
43 provavelmente dá um máximo de quão bom poderíamos ser

520
00:28:43,658 --> 00:28:46,360
com isso, ou pelo menos quão bom eu poderia ser com isso.

521
00:28:46,360 --> 00:28:49,510
Esse melhor desempenho utiliza essencialmente as ideias de que

522
00:28:49,510 --> 00:28:52,610
falei aqui, mas vai um pouco mais longe, como se procurasse a

523
00:28:52,610 --> 00:28:55,660
informação esperada dois passos à frente em vez de apenas um.

524
00:28:55,660 --> 00:28:58,017
Originalmente eu estava planejando falar mais

525
00:28:58,017 --> 00:29:00,580
sobre isso, mas percebo que já demoramos bastante.

526
00:29:00,580 --> 00:29:03,566
A única coisa que direi é que depois de fazer essa pesquisa em duas etapas

527
00:29:03,566 --> 00:29:06,314
e, em seguida, executar algumas simulações de amostra nos principais

528
00:29:06,314 --> 00:29:09,500
candidatos, até agora, pelo menos para mim, parece que Crane é o melhor abridor.

529
00:29:09,500 --> 00:29:11,080
Quem teria adivinhado?

530
00:29:11,080 --> 00:29:14,598
Além disso, se você usar a verdadeira lista de palavras para determinar seu espaço

531
00:29:14,598 --> 00:29:18,160
de possibilidades, a incerteza com a qual você começa será de pouco mais de 11 bits.

532
00:29:18,160 --> 00:29:22,395
E acontece que, apenas a partir de uma pesquisa de força bruta, o máximo possível

533
00:29:22,395 --> 00:29:26,580
de informações esperadas após as duas primeiras tentativas é de cerca de 10 bits.

534
00:29:26,580 --> 00:29:31,157
O que sugere que, na melhor das hipóteses, após suas duas primeiras suposições,

535
00:29:31,157 --> 00:29:35,220
com um jogo perfeitamente ideal, você ficará com um pouco de incerteza.

536
00:29:35,220 --> 00:29:37,400
O que é o mesmo que ter duas suposições possíveis.

537
00:29:37,400 --> 00:29:40,447
Então eu acho que é justo e provavelmente bastante conservador dizer que você nunca

538
00:29:40,447 --> 00:29:43,676
poderia escrever um algoritmo que obtivesse essa média tão baixa quanto 3, porque com as

539
00:29:43,676 --> 00:29:46,723
palavras disponíveis, simplesmente não há espaço para obter informações suficientes

540
00:29:46,723 --> 00:29:49,807
depois de apenas duas etapas. capaz de garantir a resposta no terceiro slot todas as

541
00:29:49,807 --> 00:29:50,460
vezes, sem falhar.

