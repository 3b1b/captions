[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy. ",
  "translatedText": "لقد انتشرت لعبة Wurdle بشكل كبير في الشهر أو الشهرين الماضيين، ولم يغفل أحد أبدًا فرصة لدرس الرياضيات، ويخطر لي أن هذه اللعبة تمثل مثالًا مركزيًا جيدًا جدًا في درس حول نظرية المعلومات، وعلى وجه الخصوص موضوع يعرف باسم الانتروبيا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could. ",
  "translatedText": "كما ترى، مثل الكثير من الأشخاص، انغمست في اللغز، ومثل الكثير من المبرمجين، انغمست أيضًا في محاولة كتابة خوارزمية من شأنها أن تلعب اللعبة على النحو الأمثل قدر الإمكان. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy. ",
  "translatedText": "وما اعتقدت أنني سأفعله هنا هو مجرد التحدث معكم عن بعض العمليات التي قمت بها في ذلك، وشرح بعض العمليات الحسابية التي أجريت عليها، حيث أن الخوارزمية بأكملها تركز على فكرة الإنتروبيا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle? ",
  "translatedText": "أول الأشياء أولاً، في حالة أنك لم تسمع بها، ما هو Wurdle؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us. ",
  "translatedText": "ولقتل عصفورين بحجر واحد هنا بينما نراجع قواعد اللعبة، اسمحوا لي أيضًا أن أستعرض إلى أين نتجه بهذا، وهو تطوير خوارزمية صغيرة ستلعب اللعبة أساسًا بالنسبة لنا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does. ",
  "translatedText": "على الرغم من أنني لم أقم بـWurdle اليوم، إلا أن هذا هو الرابع من فبراير، وسنرى كيف سيعمل الروبوت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess. ",
  "translatedText": "هدف Wurdle هو تخمين كلمة غامضة مكونة من خمسة أحرف، ويتم منحك ست فرص مختلفة للتخمين. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane. ",
  "translatedText": "على سبيل المثال، يقترح روبوت Wurdle الخاص بي أن أبدأ برافعة التخمين. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer. ",
  "translatedText": "في كل مرة تقوم فيها بالتخمين، تحصل على بعض المعلومات حول مدى قرب تخمينك من الإجابة الحقيقية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer. ",
  "translatedText": "هنا يخبرني المربع الرمادي أنه لا يوجد حرف C في الإجابة الفعلية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position. ",
  "translatedText": "يخبرني المربع الأصفر بوجود حرف R، لكنه ليس في هذا الموضع. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position. ",
  "translatedText": "يخبرني الصندوق الأخضر أن الكلمة السرية بها حرف A، وهي في المركز الثالث. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E. ",
  "translatedText": "ومن ثم لا يوجد N ولا يوجد E. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information. ",
  "translatedText": "لذلك اسمحوا لي أن أدخل وأخبر روبوت Wurdle بهذه المعلومات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey. ",
  "translatedText": "لقد بدأنا بالرافعة، وحصلنا على اللون الرمادي والأصفر والأخضر والرمادي والرمادي. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time. ",
  "translatedText": "لا تقلق بشأن جميع البيانات التي تظهرها الآن، سأشرح ذلك في الوقت المناسب. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick. ",
  "translatedText": "لكن أهم اقتراح لاختيارنا الثاني هو أسلوب هزلي. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess. ",
  "translatedText": "ويجب أن يكون تخمينك عبارة عن كلمة فعلية مكونة من خمسة أحرف، ولكن كما سترى، فهي متحررة جدًا فيما يتعلق بما ستتيح لك تخمينه بالفعل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick. ",
  "translatedText": "في هذه الحالة، نحاول shtick. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good. ",
  "translatedText": "حسنًا، تبدو الأمور جيدة جدًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R. ",
  "translatedText": "نضغط على S وH، حتى نعرف الأحرف الثلاثة الأولى، ونعلم أن هناك حرف R. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something. ",
  "translatedText": "وبالتالي سيكون مثل SHA شيء R، أو SHA R شيء ما. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp. ",
  "translatedText": "ويبدو أن روبوت Wurdle يعرف أن الأمر يرجع إلى احتمالين فقط، إما شظية أو حادة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a toss up between them at this point, so I guess probably just because it's alphabetical it goes with shard. ",
  "translatedText": "هذا نوع من التقلب بينهما في هذه المرحلة، لذلك أعتقد أنه ربما فقط لأنه ترتيب أبجدي فإنه يتناسب مع القشرة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer. ",
  "translatedText": "يا للهول، هذا هو الجواب الفعلي. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.22,
  "end": 132.86
 },
 {
  "input": "So we got it in three. ",
  "translatedText": "لذلك حصلنا عليه في ثلاثة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 132.96,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle four is par and three is birdie. ",
  "translatedText": "إذا كنت تتساءل عما إذا كان هذا أمرًا جيدًا، فالطريقة التي سمعت بها عبارة شخص واحد هي أنه مع Wurdle أربعة متساوية وثلاثة طائر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy. ",
  "translatedText": "والذي أعتقد أنه تشبيه مناسب جدًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy. ",
  "translatedText": "يجب أن تكون في مستوى لعبتك باستمرار حتى تحصل على المركز الرابع، لكن هذا بالتأكيد ليس جنونًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great. ",
  "translatedText": "ولكن عندما تحصل عليه في ثلاثة، فإنه يشعر بالارتياح. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot. ",
  "translatedText": "لذا، إذا كنت ترغب في ذلك، ما أود القيام به هنا هو مجرد التحدث خلال عملية تفكيري منذ البداية حول كيفية التعامل مع روبوت Wurdle. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson. ",
  "translatedText": "وكما قلت، إنه حقًا عذر لدرس نظرية المعلومات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy. ",
  "translatedText": "الهدف الرئيسي هو شرح ما هي المعلومات وما هو الإنتروبيا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language. ",
  "translatedText": "كانت فكرتي الأولى في التعامل مع هذا الأمر هي إلقاء نظرة على التكرارات النسبية للحروف المختلفة في اللغة الإنجليزية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters? ",
  "translatedText": "لذا فكرت، حسنًا، هل هناك تخمين افتتاحي أو زوج من التخمينات الافتتاحية يصل إلى الكثير من هذه الحروف الأكثر شيوعًا؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails. ",
  "translatedText": "وكان أحد الأشياء التي كنت مغرمًا بها هو القيام بأشياء أخرى متبوعة بالأظافر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good. ",
  "translatedText": "الفكرة هي أنه إذا ضربت حرفًا، فستحصل على اللون الأخضر أو الأصفر، وهذا شعور جيد دائمًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.76,
  "end": 187.52
 },
 {
  "input": "It feels like you're getting information. ",
  "translatedText": "يبدو الأمر وكأنك تحصل على معلومات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.52,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information since it's pretty rare to find a word that doesn't have any of these letters. ",
  "translatedText": "لكن في هذه الحالات، حتى لو لم تضرب وظهرت لك علامات رمادية دائمًا، فلا يزال هذا يوفر لك الكثير من المعلومات لأنه من النادر جدًا العثور على كلمة لا تحتوي على أي من هذه الأحرف. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters. ",
  "translatedText": "لكن حتى مع ذلك، لا يبدو هذا منهجيًا للغاية، لأنه على سبيل المثال، لا يفعل شيئًا للنظر في ترتيب الحروف. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail? ",
  "translatedText": "لماذا أكتب الأظافر عندما أستطيع كتابة الحلزون؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end? ",
  "translatedText": "هل من الأفضل أن يكون لديك S في النهاية؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure. ",
  "translatedText": "أنا غير متأكد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y. ",
  "translatedText": "الآن، قال أحد أصدقائي إنه يحب أن يبدأ بكلمة &quot;ضجر&quot;، الأمر الذي فاجأني نوعًا ما لأنها تحتوي على بعض الحروف غير المألوفة مثل W وY. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener. ",
  "translatedText": "لكن من يدري، ربما تكون هذه افتتاحية أفضل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess? ",
  "translatedText": "هل هناك نوع من الدرجة الكمية التي يمكننا تقديمها للحكم على جودة التخمين المحتمل؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up. ",
  "translatedText": "الآن، للإعداد للطريقة التي سنقوم بها بترتيب التخمينات المحتملة، دعنا نعود ونضيف القليل من الوضوح حول كيفية إعداد اللعبة بالضبط. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long. ",
  "translatedText": "لذا، هناك قائمة بالكلمات التي سيسمح لك بإدخالها والتي تعتبر تخمينات صحيحة ويبلغ طولها حوالي 13000 كلمة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble. ",
  "translatedText": "لكن عندما تنظر إليها، هناك الكثير من الأشياء غير المألوفة حقًا، أشياء مثل &quot;رأس&quot; أو &quot;علي&quot; و&quot;ARG&quot;، نوع الكلمات التي تثير جدالات عائلية في لعبة &quot;سكرابل&quot;. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word. ",
  "translatedText": "لكن أجواء اللعبة هي أن الإجابة ستكون دائمًا كلمة شائعة بشكل لائق. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers. ",
  "translatedText": "وفي الواقع، هناك قائمة أخرى تضم حوالي 2300 كلمة تمثل الإجابات المحتملة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creator's girlfriend, which is kind of fun. ",
  "translatedText": "وهذه قائمة منسقة بشريًا، وأعتقد على وجه التحديد من قبل صديقة صانع اللعبة، وهي ممتعة نوعًا ما. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving Wordle that doesn't incorporate previous knowledge about this list. ",
  "translatedText": "لكن ما أود أن أفعله، التحدي الذي يواجهنا في هذا المشروع هو معرفة ما إذا كان بإمكاننا كتابة برنامج لحل Wordle لا يتضمن المعرفة السابقة حول هذه القائمة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list. ",
  "translatedText": "لسبب واحد، هناك الكثير من الكلمات الشائعة المكونة من خمسة أحرف والتي لن تجدها في تلك القائمة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play Wordle against anyone, not just what happens to be the official website. ",
  "translatedText": "لذلك سيكون من الأفضل كتابة برنامج أكثر مرونة ويمكنه تشغيل Wordle ضد أي شخص، وليس فقط ما يحدث أنه الموقع الرسمي. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also the reason that we know what this list of possible answers is, is because it's visible in the source code. ",
  "translatedText": "وأيضًا السبب وراء معرفتنا لقائمة الإجابات المحتملة هذه هو أنها مرئية في الكود المصدري. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day. ",
  "translatedText": "لكن الطريقة التي تظهر بها في الكود المصدري تكون بالترتيب المحدد الذي تظهر به الإجابات من يوم لآخر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 292.8
 },
 {
  "input": "So you could always just look up what tomorrow's answer will be. ",
  "translatedText": "لذلك يمكنك دائمًا البحث عن إجابة الغد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.06,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating. ",
  "translatedText": "لذا فمن الواضح أن هناك بعض المعنى الذي يعتبر فيه استخدام القائمة غشًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words. ",
  "translatedText": "وما يجعل اللغز أكثر إثارة للاهتمام ودرسًا أكثر ثراءً لنظرية المعلومات هو بدلاً من ذلك استخدام بعض البيانات الأكثر عالمية مثل ترددات الكلمات النسبية بشكل عام لالتقاط هذا الحدس المتمثل في تفضيل الكلمات الأكثر شيوعًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess? ",
  "translatedText": "إذن، من بين هذه الاحتمالات الـ 13000، كيف يجب أن نختار التخمين الافتتاحي؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality? ",
  "translatedText": "على سبيل المثال، إذا تقدم صديقي بطلب مرهق، فكيف ينبغي لنا أن نحلل جودته؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W. ",
  "translatedText": "حسنًا، السبب الذي جعله يحب ذلك W غير المحتمل هو أنه يحب طبيعة اللقطة الطويلة لمدى الشعور الجيد إذا ضربت ذلك W. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern. ",
  "translatedText": "على سبيل المثال، إذا كان النمط الأول الذي تم الكشف عنه شيئًا كهذا، فسيتبين أن هناك 58 كلمة فقط في هذا المعجم العملاق تتطابق مع هذا النمط. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000. ",
  "translatedText": "وهذا تخفيض كبير من 13000. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this. ",
  "translatedText": "لكن الجانب الآخر من ذلك، بالطبع، هو أنه من غير المألوف أن نحصل على نمط كهذا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000. ",
  "translatedText": "على وجه التحديد، إذا كان من المرجح أن تكون كل كلمة هي الإجابة بشكل متساوٍ، فإن احتمال الوصول إلى هذا النمط سيكون 58 مقسومًا على حوالي 13000. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers. ",
  "translatedText": "وبطبيعة الحال، ليس من المرجح أن تكون الإجابات متساوية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words. ",
  "translatedText": "معظم هذه الكلمات غامضة للغاية وحتى مشكوك فيها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later. ",
  "translatedText": "لكن على الأقل بالنسبة لمرورنا الأول في كل هذا، لنفترض أن جميعهم متساوون في الاحتمال ثم نحسن ذلك لاحقًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is the pattern with a lot of information is by its very nature unlikely to occur. ",
  "translatedText": "النقطة المهمة هي أن النمط الذي يحتوي على الكثير من المعلومات من غير المرجح أن يحدث بطبيعته. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely. ",
  "translatedText": "في الواقع، ما يعنيه أن تكون غنيًا بالمعلومات هو أنه أمر غير محتمل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where of course there's not a W in it. ",
  "translatedText": "النمط الأكثر احتمالاً الذي يمكن رؤيته مع هذه الافتتاحية سيكون شيئًا مثل هذا، حيث لا يوجد بالطبع حرف W فيه. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y. ",
  "translatedText": "ربما يوجد حرف E، وربما لا يوجد A، ولا يوجد R، ولا يوجد Y. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches. ",
  "translatedText": "في هذه الحالة، هناك 1400 تطابق محتمل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see. ",
  "translatedText": "إذا كانت جميعها متساوية في الاحتمال، فمن المرجح أن يكون هذا هو النمط الذي ستراه بنسبة 11% تقريبًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative. ",
  "translatedText": "وبالتالي فإن النتائج الأكثر ترجيحًا هي أيضًا الأقل إفادة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see. ",
  "translatedText": "للحصول على رؤية أكثر عمومية هنا، اسمحوا لي أن أعرض لكم التوزيع الكامل للاحتمالات عبر جميع الأنماط المختلفة التي قد تراها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common. ",
  "translatedText": "لذا فإن كل شريط تنظر إليه يتوافق مع نمط محتمل من الألوان التي يمكن الكشف عنها، والتي يوجد منها 3 إلى الاحتمال الخامس، وهي منظمة من اليسار إلى اليمين، من الأكثر شيوعًا إلى الأقل شيوعًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays. ",
  "translatedText": "لذا فإن الاحتمال الأكثر شيوعًا هنا هو أن تحصل على كل الألوان الرمادية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time. ",
  "translatedText": "ويحدث ذلك في حوالي 14% من الحالات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here where there's only 18 possibilities for what matches this pattern that evidently look like this. ",
  "translatedText": "وما تأمله عندما تقوم بالتخمين هو أن ينتهي بك الأمر في مكان ما في هذا الذيل الطويل، مثل هنا حيث يوجد 18 احتمالًا فقط لما يطابق هذا النمط الذي يبدو بوضوح هكذا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here. ",
  "translatedText": "أو إذا غامرنا بالتحرك إلى اليسار قليلاً، كما تعلمون، ربما نقطع كل الطريق هنا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you. ",
  "translatedText": "حسنًا، إليك لغزًا جيدًا لك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them? ",
  "translatedText": "ما هي الكلمات الثلاث في اللغة الإنجليزية التي تبدأ بحرف W، وتنتهي بحرف Y، وفيها حرف R؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly. ",
  "translatedText": "اتضح أن الإجابات هي، دعونا نرى، كلامية، ودودة، وساخرة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution. ",
  "translatedText": "لذا، للحكم على مدى جودة هذه الكلمة بشكل عام، نريد نوعًا من قياس الكمية المتوقعة من المعلومات التي ستحصل عليها من هذا التوزيع. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score. ",
  "translatedText": "إذا مررنا بكل نمط وضربنا احتمالية حدوثه في شيء يقيس مدى معلوماته، فقد يمنحنا ذلك نتيجة موضوعية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches. ",
  "translatedText": "الآن قد تكون غريزتك الأولى بشأن ما يجب أن يكون عليه هذا الشيء هي عدد التطابقات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches. ",
  "translatedText": "تريد متوسطًا أقل لعدد المطابقات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer. ",
  "translatedText": "لكن بدلاً من ذلك، أود استخدام مقياس أكثر شمولاً ننسبه غالبًا إلى المعلومات، والذي سيكون أكثر مرونة بمجرد أن يكون لدينا احتمالية مختلفة مخصصة لكل كلمة من هذه الكلمات الـ 13000 لمعرفة ما إذا كانت هي الإجابة بالفعل أم لا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but it's really intuitive if we just look at examples. ",
  "translatedText": "الوحدة القياسية للمعلومات هي البت، والتي تحتوي على صيغة مضحكة إلى حد ما، لكنها بديهية حقًا إذا نظرنا فقط إلى الأمثلة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information. ",
  "translatedText": "إذا كانت لديك ملاحظة تختصر مساحة الاحتمالات لديك إلى النصف، نقول إنها تحتوي على بت واحد من المعلومات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about Half of the five letter words have an S, a little less than that, but about half. ",
  "translatedText": "في مثالنا، مساحة الاحتمالات هي كل الكلمات الممكنة، وتبين أن حوالي نصف الكلمات المكونة من خمسة أحرف بها حرف S، أقل من ذلك بقليل، ولكن حوالي النصف. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information. ",
  "translatedText": "لذا فإن هذه الملاحظة ستمنحك معلومة واحدة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information. ",
  "translatedText": "وبدلاً من ذلك، إذا أدت حقيقة جديدة إلى تقليص مساحة الاحتمالات هذه إلى أربعة أضعاف، فسنقول إنها تحتوي على قطعتين من المعلومات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T. ",
  "translatedText": "على سبيل المثال، تبين أن حوالي ربع هذه الكلمات تحتوي على حرف T. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth. ",
  "translatedText": "إذا قطعت الملاحظة هذا الفضاء بمقدار ثمانية أضعاف، نقول إنها ثلاث أجزاء من المعلومات، وهكذا دواليك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a 16th, five bits cuts it into a 32nd. ",
  "translatedText": "أربع بتات تقطعه إلى 16، وخمس بتات تقطعه إلى 32. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.52
 },
 {
  "input": "So now you might want to pause and ask yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence? ",
  "translatedText": "والآن قد ترغب في التوقف مؤقتًا وتسأل نفسك، ما هي صيغة المعلومات لعدد البتات من حيث احتمال حدوث ذلك؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 533.52,
  "end": 542.98
 },
 {
  "input": "What we're saying here is that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability. ",
  "translatedText": "ما نقوله هنا هو أنه عندما تأخذ نصفًا لعدد البتات، فهذا هو نفس الاحتمال، وهو نفس قول أن اثنين أس عدد البتات يساوي واحدًا على الاحتمال، وهو ما يُعاد ترتيبها أيضًا لقول أن المعلومات هي سجل واحد للأساس اثنين مقسومًا على الاحتمال. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still, where the information is the negative log base two of the probability. ",
  "translatedText": "وفي بعض الأحيان ترى ذلك مع عملية إعادة ترتيب أخرى، حيث تكون المعلومات هي سالب الاحتمال للأساس اثنين. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half. ",
  "translatedText": "إذا تم التعبير عنها بهذه الطريقة، فقد تبدو غريبة بعض الشيء بالنسبة للمبتدئين، ولكنها في الحقيقة مجرد فكرة بديهية للغاية تتمثل في السؤال عن عدد المرات التي قمت فيها بتخفيض إمكانياتك إلى النصف. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture? ",
  "translatedText": "الآن إذا كنت تتساءل، كما تعلمون، اعتقدت أننا كنا نلعب لعبة كلمات ممتعة، لماذا تدخل اللوغاريتمات في الصورة؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095. ",
  "translatedText": "أحد الأسباب التي تجعل هذه الوحدة أجمل هو أنه من الأسهل كثيرًا التحدث عن أحداث غير محتملة للغاية، ومن الأسهل كثيرًا القول إن الملاحظة تحتوي على 20 بت من المعلومات مقارنة بالقول إن احتمال حدوث كذا وكذا هو 0.0000095. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together. ",
  "translatedText": "لكن السبب الأكثر أهمية وراء تحول هذا التعبير اللوغاريتمي إلى إضافة مفيدة جدًا لنظرية الاحتمال هو الطريقة التي تجمع بها المعلومات معًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information. ",
  "translatedText": "على سبيل المثال، إذا أعطتك ملاحظة واحدة قطعتين من المعلومات، مما يقلل المساحة بمقدار أربعة، ثم ملاحظة ثانية مثل تخمينك الثاني في Wordle تعطيك ثلاث أجزاء أخرى من المعلومات، مما يقلل من المساحة بمقدار عامل آخر قدره ثمانية، فإن اثنان معًا يزودانك بخمسة أجزاء من المعلومات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add. ",
  "translatedText": "بنفس الطريقة التي تحب بها الاحتمالات أن تتضاعف، تحب المعلومات أن تضيف. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with. ",
  "translatedText": "لذلك بمجرد أن نكون في عالم شيء مثل القيمة المتوقعة، حيث نقوم بإضافة مجموعة من الأرقام، فإن السجلات تجعل التعامل معها أفضل كثيرًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for Weary and add another little tracker on here, showing us how much information there is for each pattern. ",
  "translatedText": "دعنا نعود إلى توزيعتنا لـ Weary ونضيف أداة تعقب صغيرة أخرى هنا، لتوضح لنا مقدار المعلومات المتوفرة لكل نمط. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain. ",
  "translatedText": "الشيء الرئيسي الذي أريدك أن تلاحظه هو أنه كلما زادت احتمالية وصولنا إلى تلك الأنماط الأكثر احتمالية، كلما انخفضت المعلومات، وقل عدد البتات التي تكسبها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information, where we go through each pattern, we say how probable is it, and then we multiply that by how many bits of information do we get. ",
  "translatedText": "الطريقة التي نقيس بها جودة هذا التخمين هي أخذ القيمة المتوقعة لهذه المعلومات، حيث نمر عبر كل نمط، ونقول مدى احتمالية ذلك، ثم نضرب ذلك في عدد أجزاء المعلومات التي نحصل عليها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 643.5,
  "end": 654.06
 },
 {
  "input": "And in the example of Weary, that turns out to be 4.9 bits. ",
  "translatedText": "وفي مثال Weary، تبين أن الرقم 4.9 بت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times. ",
  "translatedText": "لذا، في المتوسط، فإن المعلومات التي تحصل عليها من هذا التخمين الافتتاحي جيدة مثل تقليص مساحة الاحتمالات الخاصة بك إلى النصف حوالي خمس مرات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like Slate. ",
  "translatedText": "على النقيض من ذلك، مثال على التخمين ذو قيمة معلومات متوقعة أعلى سيكون مثل Slate. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case you'll notice the distribution looks a lot flatter. ",
  "translatedText": "في هذه الحالة ستلاحظ أن التوزيع يبدو أكثر تملقًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information. ",
  "translatedText": "على وجه الخصوص، فإن احتمال حدوث جميع درجات الرمادي هو 6% فقط، لذا فمن الواضح أنك تحصل على 3% على الأقل. 9 بت من المعلومات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that. ",
  "translatedText": "ولكن هذا هو الحد الأدنى، وعادة ما تحصل على شيء أفضل من ذلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8. ",
  "translatedText": "واتضح أنه عندما تقوم بجمع الأرقام الموجودة في هذا الرقم وإضافة جميع المصطلحات ذات الصلة، فإن متوسط المعلومات هو حوالي 5.8. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with Weary, your space of possibilities will be about half as big after this first guess, on average. ",
  "translatedText": "لذا، على النقيض من Weary، فإن مساحة الاحتمالات الخاصة بك ستكون حوالي النصف بعد هذا التخمين الأول، في المتوسط. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity. ",
  "translatedText": "هناك في الواقع قصة ممتعة حول اسم هذه القيمة المتوقعة لكمية المعلومات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "Information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science. ",
  "translatedText": "تم تطوير نظرية المعلومات على يد كلود شانون، الذي كان يعمل في مختبرات بيل في الأربعينيات، لكنه كان يتحدث عن بعض أفكاره التي لم تُنشر بعد مع جون فون نيومان، الذي كان هذا العملاق الفكري في ذلك الوقت، بارزًا جدًا في الرياضيات والفيزياء وبدايات ما أصبح علوم الكمبيوتر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well you should call it entropy, and for two reasons. ",
  "translatedText": "وعندما ذكر أنه ليس لديه حقًا اسم جيد لهذه القيمة المتوقعة لكمية المعلومات، من المفترض أن فون نيومان قال، وفقًا للقصة، حسنًا، يجب أن تسميها الإنتروبيا، وذلك لسببين. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage. ",
  "translatedText": "في المقام الأول، تم استخدام دالة عدم اليقين في الميكانيكا الإحصائية تحت هذا الاسم، لذا فهي تحمل اسمًا بالفعل، وفي المقام الثاني، والأهم من ذلك، لا أحد يعرف ما هي الإنتروبيا حقًا، لذلك في أي نقاش ستظل دائمًا لدينا ميزة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design. ",
  "translatedText": "لذا، إذا كان الاسم يبدو غامضًا بعض الشيء، وإذا كان لهذه القصة أن تصدق، فهذا نوعاً ما حسب التصميم. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess. ",
  "translatedText": "وأيضًا إذا كنت تتساءل عن علاقتها بكل ما يتعلق بالقانون الثاني للديناميكا الحرارية من الفيزياء، فمن المؤكد أن هناك صلة، ولكن في أصولها كان شانون يتعامل فقط مع نظرية الاحتمالات البحتة، ولأغراضنا هنا، عندما استخدمت الإنتروبيا، أريدك فقط أن تفكر في قيمة المعلومات المتوقعة لتخمين معين. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously. ",
  "translatedText": "يمكنك التفكير في الإنتروبيا على أنها قياس شيئين في وقت واحد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution. ",
  "translatedText": "الأول هو مدى ثبات التوزيع. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be. ",
  "translatedText": "كلما كان التوزيع أقرب إلى الانتظام، كلما زادت الإنتروبيا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy. ",
  "translatedText": "في حالتنا، حيث يوجد 3 إلى 5 أنماط إجمالية، للتوزيع الموحد، فإن مراقبة أي واحد منها سيكون له قاعدة سجل معلومات 2 من 3 إلى 5، وهو ما يصادف أنه 7.92، هذا هو الحد الأقصى المطلق الذي يمكن أن تحصل عليه لهذه الإنتروبيا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place. ",
  "translatedText": "لكن الإنتروبيا هي أيضًا نوع من مقياس لعدد الاحتمالات الموجودة في المقام الأول. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits. ",
  "translatedText": "على سبيل المثال، إذا كان لديك كلمة حيث يوجد فقط 16 نمطًا ممكنًا، وكل نمط محتمل متساوٍ، فإن هذه الإنتروبيا، هذه المعلومات المتوقعة، ستكون 4 بتات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits. ",
  "translatedText": "لكن إذا كانت لديك كلمة أخرى حيث يمكن أن تظهر 64 نمطًا محتملاً، وجميعها متساوية في الاحتمال، فإن الإنتروبيا ستكون 6 بتات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes. ",
  "translatedText": "لذا، إذا رأيت بعض التوزيعات في الطبيعة والتي تحتوي على إنتروبيا تبلغ 6 بتات، فهذا يشبه القول بأن هناك قدرًا كبيرًا من الاختلاف وعدم اليقين فيما هو على وشك الحدوث كما لو كان هناك 64 نتيجة محتملة متساوية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this. ",
  "translatedText": "في أول تمريرة لي في Wurtelebot، طلبت مني فعل هذا فقط. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the possible guesses you could have, all 13,000 words, computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns you might see, for each one, and picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible. ",
  "translatedText": "إنه يمر بجميع التخمينات المحتملة التي يمكن أن تكون لديك، كل الكلمات البالغ عددها 13000 كلمة، ويحسب الإنتروبيا لكل واحدة منها، أو بشكل أكثر تحديدًا، إنتروبيا التوزيع عبر جميع الأنماط التي قد تراها، لكل واحد، ويختار الأعلى، نظرًا لأن ذلك الذي من المحتمل أن يقلل مساحة الاحتمالات الخاصة بك قدر الإمكان. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses. ",
  "translatedText": "وعلى الرغم من أنني كنت أتحدث فقط عن التخمين الأول هنا، فإنه يفعل نفس الشيء بالنسبة للتخمينات القليلة التالية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words. ",
  "translatedText": "على سبيل المثال، بعد أن ترى بعض الأنماط في هذا التخمين الأول، والتي من شأنها أن تقيدك بعدد أقل من الكلمات المحتملة بناءً على ما يتطابق مع ذلك، فإنك تلعب نفس اللعبة فيما يتعلق بتلك المجموعة الأصغر من الكلمات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy. ",
  "translatedText": "للحصول على تخمين ثانٍ مقترح، تنظر إلى توزيع جميع الأنماط التي يمكن أن تحدث من تلك المجموعة الأكثر تقييدًا من الكلمات، وتبحث في جميع الاحتمالات البالغ عددها 13000، وتجد الخيار الذي يزيد من هذه الإنتروبيا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins. ",
  "translatedText": "لكي أوضح لك كيف يتم ذلك عمليًا، اسمحوا لي أن أعرض نسخة صغيرة من كتاب Wurtele الذي كتبته والذي يوضح النقاط البارزة في هذا التحليل في الهوامش. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "After doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information. ",
  "translatedText": "بعد إجراء جميع حسابات الإنتروبيا، تظهر لنا على اليمين أي منها لديه أعلى المعلومات المتوقعة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch. ",
  "translatedText": "اتضح أن الإجابة الرئيسية، على الأقل في الوقت الحالي، وسنقوم بتحسينها لاحقًا، هي الزوان، والتي تعني، بالطبع، البيقية، البيقية الأكثر شيوعًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got, given this particular pattern. ",
  "translatedText": "في كل مرة نقوم بالتخمين هنا، حيث ربما أتجاهل توصياتها نوعًا ما وأختار القائمة، لأنني أحب القائمة، يمكننا أن نرى مقدار المعلومات المتوقعة التي تحتوي عليها، ولكن بعد ذلك على يمين الكلمة هنا تظهر لنا مقدار المعلومات المعلومات الفعلية التي حصلنا عليها، في ضوء هذا النمط بالذات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that. ",
  "translatedText": "لذا يبدو هنا أننا لم نكن محظوظين بعض الشيء، حيث كان من المتوقع أن نحصل على 5.8، لكننا حصلنا على شيء أقل من ذلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now. ",
  "translatedText": "ثم على الجانب الأيسر هنا يظهر لنا جميع الكلمات المختلفة الممكنة بالنظر إلى ما نحن فيه الآن. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment. ",
  "translatedText": "تخبرنا الأشرطة الزرقاء بمدى احتمالية التفكير في كل كلمة، لذلك في الوقت الحالي يفترض أن كل كلمة من المرجح أن تحدث بشكل متساوٍ، ولكننا سنحسن ذلك في لحظة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities. ",
  "translatedText": "ومن ثم يخبرنا قياس عدم اليقين هذا بإنتروبيا هذا التوزيع عبر الكلمات المحتملة، والتي في الوقت الحالي، نظرًا لأنه توزيع موحد، هي مجرد طريقة معقدة بلا داع لحساب عدد الاحتمالات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities. ",
  "translatedText": "على سبيل المثال، إذا أخذنا 2 أس 13.66، ينبغي أن يكون حوالي 13000 الاحتمالات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "I'm a little bit off here, but only because I'm not showing all the decimal places. ",
  "translatedText": "أنا متوقف قليلاً هنا، ولكن فقط لأنني لا أعرض جميع المنازل العشرية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute. ",
  "translatedText": "في الوقت الحالي، قد يبدو هذا زائدًا عن الحاجة ويؤدي إلى تعقيد الأمور بشكل مفرط، ولكنك سترى لماذا من المفيد الحصول على كلا الرقمين في دقيقة واحدة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Ramen, which again just really doesn't feel like a word. ",
  "translatedText": "لذا يبدو هنا أنه يشير إلى أن أعلى إنتروبيا لتخميننا الثاني هي رامين، والتي مرة أخرى لا تبدو وكأنها كلمة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here, I'm going to go ahead and type in Rains. ",
  "translatedText": "لذا، لكي أتخذ موقفًا أخلاقيًا عاليًا هنا، سأمضي قدمًا وأكتب Rains. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky. ",
  "translatedText": "ومرة أخرى يبدو أننا لم نكن محظوظين بعض الشيء. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information. ",
  "translatedText": "كنا نتوقع 4.3 بتات وحصلنا على 3 فقط. 39 بت من المعلومات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities. ",
  "translatedText": "وهذا ينقلنا إلى 55 احتمالًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means. ",
  "translatedText": "وهنا ربما سأوافق على ما يقترحه، وهو السرد، مهما كان معنى ذلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And okay, this is actually a good chance for a puzzle. ",
  "translatedText": "حسنًا، هذه في الواقع فرصة جيدة لحل اللغز. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information. ",
  "translatedText": "إنه يخبرنا أن هذا النمط يعطينا 4.7 بت من المعلومات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty. ",
  "translatedText": "لكن على اليسار، قبل أن نرى هذا النمط، كان هناك 5.78 بت من عدم اليقين. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities? ",
  "translatedText": "لذا، كاختبار لك، ماذا يعني ذلك بالنسبة لعدد الاحتمالات المتبقية؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well, it means that we're reduced down to one bit of uncertainty, which is the same thing as saying that there's two possible answers. ",
  "translatedText": "حسنًا، هذا يعني أننا قد اختزلنا إلى جزء واحد من عدم اليقين، وهو نفس القول بأن هناك إجابتين محتملتين. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice. ",
  "translatedText": "إنه خيار 50-50. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss. ",
  "translatedText": "ومن هنا، ولأنني وأنت نعرف أي الكلمات أكثر شيوعاً، فإننا نعلم أن الإجابة يجب أن تكون الهاوية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that. ",
  "translatedText": "ولكن كما هو مكتوب الآن، فإن البرنامج لا يعرف ذلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it. ",
  "translatedText": "لذلك فهو يستمر في محاولة الحصول على أكبر قدر ممكن من المعلومات، حتى يتبقى احتمال واحد فقط، ثم يخمنه. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy. ",
  "translatedText": "لذا من الواضح أننا بحاجة إلى استراتيجية أفضل لنهاية اللعبة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1042.34
 },
 {
  "input": "But let's say we call this version one of our wordle solver, and then we go and run some simulations to see how it does. ",
  "translatedText": "لكن لنفترض أننا نطلق على هذا الإصدار أحد برامج حل الكلمات لدينا، ثم نبدأ ونجري بعض عمليات المحاكاة لنرى كيف يتم ذلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1042.6,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game. ",
  "translatedText": "لذا فإن الطريقة التي يعمل بها هذا الأمر هي لعب كل لعبة كلمات ممكنة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers. ",
  "translatedText": "إنها تمر بكل تلك الكلمات البالغ عددها 2315 والتي تمثل الإجابات الفعلية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set. ",
  "translatedText": "إنها تستخدم ذلك بشكل أساسي كمجموعة اختبار. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice. ",
  "translatedText": "ومع هذه الطريقة الساذجة المتمثلة في عدم مراعاة مدى شيوع الكلمة، ومحاولة تعظيم المعلومات في كل خطوة على طول الطريق، حتى تصل إلى خيار واحد فقط. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124. ",
  "translatedText": "وبحلول نهاية المحاكاة، يبلغ متوسط الدرجات حوالي 4.124. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expected to do worse. ",
  "translatedText": "وهذا ليس سيئًا، لأكون صادقًا، كنت أتوقع نوعًا ما أن أفعل ما هو أسوأ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4. ",
  "translatedText": "لكن الأشخاص الذين يلعبون لعبة Wordle سيخبرونك أنه يمكنهم عادةً الحصول عليها خلال 4. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can. ",
  "translatedText": "التحدي الحقيقي هو الحصول على أكبر عدد ممكن من 3. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3. ",
  "translatedText": "إنها قفزة كبيرة جدًا بين النتيجة 4 والنتيجة 3. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that. ",
  "translatedText": "إن الفاكهة المعلقة الواضحة هنا هي دمج ما إذا كانت الكلمة شائعة أم لا، وكيف نفعل ذلك بالضبط. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language. ",
  "translatedText": "الطريقة التي تعاملت بها مع الأمر هي الحصول على قائمة بالتكرارات النسبية لجميع الكلمات في اللغة الإنجليزية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1107.88
 },
 {
  "input": "And I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset. ",
  "translatedText": "وقد استخدمت للتو وظيفة بيانات تردد الكلمات الخاصة بـ Mathematica، والتي تستمد نفسها من مجموعة البيانات العامة لـ Google Books English Ngram. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words. ",
  "translatedText": "ومن الممتع النظر إليه، على سبيل المثال، إذا قمنا بفرزه من الكلمات الأكثر شيوعًا إلى الكلمات الأقل شيوعًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5 letter words in the English language. ",
  "translatedText": "من الواضح أن هذه هي الكلمات الأكثر شيوعًا والمكونة من 5 أحرف في اللغة الإنجليزية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common. ",
  "translatedText": "أو بالأحرى، هذه هي المرتبة الثامنة الأكثر شيوعًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there. ",
  "translatedText": "الأول هو الذي، وبعد ذلك هناك وهناك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common. ",
  "translatedText": "الأول في حد ذاته ليس الأول، بل التاسع، ومن المنطقي أن هذه الكلمات الأخرى يمكن أن تأتي في كثير من الأحيان، حيث تكون تلك الكلمات التي تأتي بعد الأول، وأين، وتلك أقل شيوعًا قليلاً. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency. ",
  "translatedText": "الآن، عند استخدام هذه البيانات لنمذجة مدى احتمالية أن تكون كل كلمة من هذه الكلمات هي الإجابة النهائية، لا ينبغي أن تكون متناسبة مع التكرار فقط. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1146.34
 },
 {
  "input": "For example, which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely. ",
  "translatedText": "على سبيل المثال، الذي يعطى درجة 0.002 في مجموعة البيانات هذه، في حين أن كلمة جديلة أقل احتمالًا بحوالي 1000 مرة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.7,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering. ",
  "translatedText": "لكن كلتا هاتين الكلمتين شائعتان بدرجة كافية ومن المؤكد أنهما تستحقان أخذهما في الاعتبار. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1158.84
 },
 {
  "input": "So we want more of a binary cutoff. ",
  "translatedText": "لذلك نريد المزيد من القطع الثنائي. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1159.34,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty. ",
  "translatedText": "الطريقة التي اتبعتها في ذلك هي تخيل أخذ هذه القائمة الكاملة من الكلمات، ثم ترتيبها على المحور السيني، ثم تطبيق الدالة السيني، وهي الطريقة القياسية للحصول على دالة يكون ناتجها ثنائيًا بشكل أساسي، إنها إما 0 أو 1، ولكن هناك تجانس بينهما بالنسبة لتلك المنطقة من عدم اليقين. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis. ",
  "translatedText": "لذلك، بشكل أساسي، فإن الاحتمال الذي أقوم بتعيينه لكل كلمة لوجودها في القائمة النهائية سيكون قيمة الدالة السيني أعلاه أينما كانت على المحور السيني. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff. ",
  "translatedText": "من الواضح الآن أن هذا يعتمد على بعض المعلمات، على سبيل المثال مدى اتساع المساحة على المحور السيني التي تملأها تلك الكلمات يحدد مدى انخفاضنا تدريجيًا أو حادًا من 1 إلى 0، والمكان الذي نضعهم فيه من اليسار إلى اليمين يحدد القطع. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "To be honest, the way I did this was just licking my finger and sticking it into the wind. ",
  "translatedText": "لأكون صادقًا، الطريقة التي فعلت بها ذلك كانت مجرد لعق إصبعي ووضعه في مهب الريح. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff. ",
  "translatedText": "لقد بحثت في القائمة التي تم فرزها وحاولت العثور على نافذة حيث عندما نظرت إليها، اكتشفت أن حوالي نصف هذه الكلمات من المرجح أن لا تكون الإجابة النهائية، واستخدمت ذلك كنقطة قطع. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement. ",
  "translatedText": "بمجرد أن يكون لدينا توزيع مثل هذا عبر الكلمات، فهذا يعطينا موقفًا آخر تصبح فيه الإنتروبيا هذا القياس المفيد حقًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were a feather and nails, and we end up with a situation where there's four possible words that match it. ",
  "translatedText": "على سبيل المثال، لنفترض أننا كنا نلعب لعبة وبدأنا بافتتاحيتي القديمة، والتي كانت عبارة عن ريشة ومسامير، وانتهى بنا الأمر بموقف حيث توجد أربع كلمات محتملة تطابقها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely. ",
  "translatedText": "ولنفترض أننا نعتبرها كلها محتملة على قدم المساواة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1235.62
 },
 {
  "input": "Let me ask you, what is the entropy of this distribution? ",
  "translatedText": "دعني أسألك، ما هي إنتروبيا هذا التوزيع؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1236.22,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2. ",
  "translatedText": "حسنًا، المعلومات المرتبطة بكل واحد من هذه الاحتمالات ستكون السجل ذو الأساس 2 من 4، حيث أن كل واحد هو 1 و4، أي 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "Two bits of information, four possibilities. ",
  "translatedText": "قطعتان من المعلومات، وأربعة احتمالات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good. ",
  "translatedText": "كل شيء جيد جدا وجيد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than four matches? ",
  "translatedText": "ولكن ماذا لو أخبرتك أن هناك بالفعل أكثر من أربع مباريات؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it. ",
  "translatedText": "في الواقع، عندما ننظر إلى قائمة الكلمات الكاملة، نجد أن هناك 16 كلمة تطابقها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure. ",
  "translatedText": "لكن لنفترض أن نموذجنا يضع احتمالًا منخفضًا حقًا لتلك الكلمات الـ 12 الأخرى لتكون الإجابة النهائية، شيء مثل 1 في 1000 لأنها غامضة حقًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution? ",
  "translatedText": "والآن دعني أسألك، ما هي إنتروبيا هذا التوزيع؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before. ",
  "translatedText": "إذا كانت الإنتروبيا تقيس فقط عدد التطابقات هنا، فقد تتوقع أن تكون شيئًا مثل سجل الأساس 2 لـ 16، والذي سيكون 4، أي قطعتين من عدم اليقين أكثر مما كان لدينا من قبل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before. ",
  "translatedText": "لكن بطبيعة الحال، فإن عدم اليقين الفعلي لا يختلف كثيرًا عما كان لدينا من قبل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example. ",
  "translatedText": "فقط لأن هناك هذه الكلمات الـ 12 الغامضة لا يعني أنه سيكون من المثير للدهشة معرفة أن الإجابة النهائية هي السحر، على سبيل المثال. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here, and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits. ",
  "translatedText": "لذلك عندما تقوم بالحساب هنا، وتضيف احتمالية كل تكرار مضروبًا في المعلومات المقابلة، فإن ما تحصل عليه هو 2.11 بت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "I'm just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it. ",
  "translatedText": "أنا فقط أقول، إنها في الأساس جزأين، أساسًا تلك الاحتمالات الأربعة، ولكن هناك القليل من عدم اليقين بسبب كل تلك الأحداث غير المحتملة إلى حد كبير، على الرغم من أنك إذا تعلمتها، فسوف تحصل على الكثير من المعلومات منها. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson. ",
  "translatedText": "لذا، فإن التصغير هو جزء مما يجعل Wordle مثالًا رائعًا لدرس نظرية المعلومات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy. ",
  "translatedText": "لدينا هذين التطبيقين المتميزين للشعور بالإنتروبيا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words that we have possible. ",
  "translatedText": "الأول يخبرنا ما هي المعلومات المتوقعة التي سنحصل عليها من تخمين معين، والثاني يقول هل يمكننا قياس عدم اليقين المتبقي بين جميع الكلمات التي لدينا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation. ",
  "translatedText": "ويجب أن أؤكد، في تلك الحالة الأولى التي ننظر فيها إلى المعلومات المتوقعة من التخمين، بمجرد أن يكون لدينا وزن غير متساوٍ للكلمات، فإن ذلك يؤثر على حساب الإنتروبيا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words. ",
  "translatedText": "على سبيل المثال، اسمحوا لي أن أذكر نفس الحالة التي كنا ننظر إليها سابقًا للتوزيع المرتبط بـ Weary، ولكن هذه المرة باستخدام توزيع غير منتظم عبر جميع الكلمات الممكنة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well. ",
  "translatedText": "لذلك اسمحوا لي أن أرى ما إذا كان بإمكاني العثور على جزء هنا يوضح ذلك بشكل جيد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here this is pretty good. ",
  "translatedText": "حسنًا، هنا هذا جيد جدًا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it. ",
  "translatedText": "لدينا هنا نمطان متجاوران متساويان في الاحتمال، ولكن قيل لنا أن أحدهما يحتوي على 32 كلمة محتملة تتطابق معه. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them. ",
  "translatedText": "وإذا تحققنا من ماهيتها، فهذه هي تلك الكلمات الـ 32، والتي كلها مجرد كلمات غير محتملة للغاية عندما تفحصها بعينيك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches, so a quarter as many matches, but it's about as likely. ",
  "translatedText": "من الصعب العثور على أي إجابات تبدو وكأنها إجابات معقولة، وربما صيحات، ولكن إذا نظرنا إلى النمط المجاور في التوزيع، والذي يعتبر محتملًا تقريبًا، فقد قيل لنا أنه يحتوي على 8 تطابقات محتملة فقط، أي ربع العديد من المباريات، ولكن الأمر على الأرجح. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why. ",
  "translatedText": "وعندما نسحب تلك الثقاب، يمكننا أن نرى السبب. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers, like ring, or wrath, or raps. ",
  "translatedText": "بعض هذه الإجابات هي إجابات معقولة بالفعل، مثل الخاتم، أو الغضب، أو موسيقى الراب. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version 2 of the Wordlebot here, and there are two or three main differences from the first one that we saw. ",
  "translatedText": "لتوضيح كيفية دمج كل ذلك، اسمحوا لي أن أعرض الإصدار 2 من Wordlebot هنا، وهناك اختلافان أو ثلاثة اختلافات رئيسية عن الإصدار الأول الذي رأيناه. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer. ",
  "translatedText": "أولاً، كما قلت للتو، الطريقة التي نحسب بها هذه الإنتروبيا، هذه القيم المتوقعة للمعلومات، تستخدم الآن توزيعات أكثر دقة عبر الأنماط التي تتضمن احتمال أن تكون كلمة معينة هي الإجابة بالفعل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number 1, though the ones following are a bit different. ",
  "translatedText": "كما يحدث، لا تزال الدموع هي رقم 1، على الرغم من أن الدموع التالية مختلفة بعض الشيء. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table. ",
  "translatedText": "ثانيًا، عندما يقوم بتصنيف أفضل اختياراته، فإنه سيحتفظ الآن بنموذج لاحتمال أن تكون كل كلمة هي الإجابة الفعلية، وسيدمج ذلك في قراره، وهو ما يسهل رؤيته بمجرد أن يكون لدينا بعض التخمينات على طاولة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives. ",
  "translatedText": "مرة أخرى، تجاهل توصيتها لأننا لا نستطيع السماح للآلات بالتحكم في حياتنا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches. ",
  "translatedText": "وأفترض أنني يجب أن أذكر شيئًا آخر مختلفًا هنا على اليسار، وهو أن قيمة عدم اليقين، وعدد البتات هذا، لم تعد مجرد زائدة عن الحاجة مع عدد التطابقات المحتملة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which is a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes. ",
  "translatedText": "الآن إذا سحبناها للأعلى وحسابنا 2 إلى 8.02، وهو أعلى قليلاً من 256، أعتقد 259، ما يقوله هو أنه على الرغم من وجود 526 كلمة إجمالية تتطابق فعليًا مع هذا النمط، فإن مقدار عدم اليقين الموجود فيه أقرب إلى ما سيكون عليه لو كان هناك 259 كلمة محتملة على قدم المساواة النتائج. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this. ",
  "translatedText": "يمكنك التفكير في الأمر على هذا النحو. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borx is not the answer, same with yorts and zorl and zorus, so it's a little less uncertain than it was in the previous case. ",
  "translatedText": "إنه يعرف أن البورق ليس هو الحل، كما هو الحال مع yorts وzorl وzorus، لذا فهو أقل غموضًا مما كان عليه في الحالة السابقة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller. ",
  "translatedText": "سيكون هذا العدد من البتات أصغر. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here. ",
  "translatedText": "وإذا واصلت لعب اللعبة، فسوف أقوم بتحسين ذلك من خلال بعض التخمينات التي تتناسب مع ما أود أن أشرحه هنا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy. ",
  "translatedText": "من خلال التخمين الرابع، إذا نظرت إلى أفضل اختياراتها، يمكنك أن ترى أنها لم تعد مجرد تعظيم الإنتروبيا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words. ",
  "translatedText": "إذن في هذه المرحلة، هناك سبعة احتمالات من الناحية الفنية، لكن الاحتمالات الوحيدة التي لديها فرصة ذات معنى هي مساكن الطلبة والكلمات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values, that strictly speaking would give more information. ",
  "translatedText": "ويمكنك أن ترى أن اختيار كلتا القيمتين فوق كل هذه القيم الأخرى، بالمعنى الدقيق للكلمة، سيعطي المزيد من المعلومات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect. ",
  "translatedText": "في المرة الأولى التي قمت فيها بذلك، قمت فقط بجمع هذين الرقمين لقياس جودة كل تخمين، والذي كان في الواقع أفضل مما قد تظن. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic, and I'm sure there's other approaches people could take but here's the one I landed on. ",
  "translatedText": "لكن الأمر لم يكن منهجيًا حقًا، وأنا متأكد من أن هناك طرقًا أخرى يمكن للناس اتباعها ولكن هذا هو النهج الذي توصلت إليه. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that. ",
  "translatedText": "إذا كنا نفكر في احتمالية التخمين التالي، كما هو الحال في الكلمات في هذه الحالة، فإن ما نهتم به حقًا هو النتيجة المتوقعة للعبتنا إذا فعلنا ذلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to. ",
  "translatedText": "ولحساب تلك النتيجة المتوقعة، نقول ما هو احتمال أن تكون الكلمات هي الإجابة الفعلية، والتي تصف في الوقت الحالي 58%. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be 4. ",
  "translatedText": "نقول باحتمال 58% أن نتيجتنا في هذه المباراة ستكون 4. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of 1 minus that 58%, our score will be more than that 4. ",
  "translatedText": "وبعد ذلك، مع احتمال 1 ناقص 58%، ستكون درجاتنا أكثر من 4. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point. ",
  "translatedText": "لا نعرف كم من ذلك، ولكن يمكننا تقديره بناءً على مقدار عدم اليقين المحتمل عندما نصل إلى هذه النقطة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment there's 1.44 bits of uncertainty. ",
  "translatedText": "على وجه التحديد، في هذه اللحظة هناك 1.44 بت من عدم اليقين. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits. ",
  "translatedText": "إذا خمننا الكلمات، فهذا يخبرنا أن المعلومات المتوقعة التي سنحصل عليها هي 1.27 بت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens. ",
  "translatedText": "لذا، إذا خمننا الكلمات، فإن هذا الاختلاف يمثل مقدار عدم اليقين الذي من المحتمل أن نتركه بعد حدوث ذلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score. ",
  "translatedText": "ما نحتاجه هو نوع من الوظائف، والتي أسميها f هنا، والتي تربط عدم اليقين هذا بالنتيجة المتوقعة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version 1 of the bot to say hey what was the actual score after various points with certain very measurable amounts of uncertainty. ",
  "translatedText": "وكانت الطريقة التي تم بها تحقيق ذلك هي رسم مجموعة من البيانات من الألعاب السابقة استنادًا إلى الإصدار 1 من الروبوت لنقول ما هي النتيجة الفعلية بعد نقاط مختلفة مع قدر معين من عدم اليقين يمكن قياسه للغاية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer. ",
  "translatedText": "على سبيل المثال، نقاط البيانات هذه الموجودة هنا أعلى قيمة تقارب 8.7 أو نحو ذلك يقال لبعض الألعاب بعد النقطة التي كان فيها 8.7 أجزاء من عدم اليقين، استغرق الأمر تخمينين للحصول على الإجابة النهائية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games it took three guesses, for other games it took four guesses. ",
  "translatedText": "بالنسبة للألعاب الأخرى، استغرق الأمر ثلاثة تخمينات، وبالنسبة للألعاب الأخرى، استغرق الأمر أربعة تخمينات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring. ",
  "translatedText": "إذا انتقلنا إلى اليسار هنا، فإن جميع النقاط فوق الصفر تشير إلى أنه عندما يكون هناك صفر من عدم اليقين، وهو ما يعني أن هناك احتمالًا واحدًا فقط، فإن عدد التخمينات المطلوبة هو دائمًا واحد فقط، وهو أمر مطمئن. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses. ",
  "translatedText": "كلما كان هناك قدر واحد من عدم اليقين، مما يعني أن الأمر كان في الأساس يرجع إلى احتمالين فقط، كان الأمر يتطلب أحيانًا تخمينًا إضافيًا، وأحيانًا يتطلب الأمر تخمينين إضافيين. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1623.02
 },
 {
  "input": "And so on and so forth here. ",
  "translatedText": "وهكذا دواليك هنا. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1623.08,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages. ",
  "translatedText": "ربما تكون الطريقة الأسهل قليلًا لتصور هذه البيانات هي تجميعها معًا وأخذ المتوسطات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example this bar here saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5. ",
  "translatedText": "على سبيل المثال، يشير هذا الشريط هنا إلى أنه من بين جميع النقاط التي كان لدينا فيها قدر واحد من عدم اليقين، كان متوسط عدد التخمينات الجديدة المطلوبة حوالي 1.5. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward. ",
  "translatedText": "والشريط هنا يقول من بين جميع الألعاب المختلفة حيث كانت نسبة عدم اليقين في مرحلة ما أعلى بقليل من أربعة بتات، وهو ما يشبه تضييقها إلى 16 احتمالًا مختلفًا، ثم في المتوسط يتطلب الأمر ما يزيد قليلاً عن تخمينين من تلك النقطة إلى الأمام. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this. ",
  "translatedText": "ومن هنا قمت للتو بالانحدار ليناسب الوظيفة التي بدت معقولة لهذا الغرض. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be. ",
  "translatedText": "وتذكر أن الهدف الأساسي من القيام بأي من ذلك هو أن نتمكن من قياس هذا الحدس، وهو أنه كلما زادت المعلومات التي نكتسبها من كلمة ما، كلما انخفضت النتيجة المتوقعة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So with this as version 2.0, if we go back and we run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do? ",
  "translatedText": "لذا، مع هذا الإصدار 2.0، إذا عدنا وقمنا بتشغيل نفس مجموعة عمليات المحاكاة، حيث قمنا بتشغيلها مقابل جميع الإجابات اللفظية المحتملة البالغ عددها 2315، كيف سيتم ذلك؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version it's definitely better, which is reassuring. ",
  "translatedText": "حسنًا، على النقيض من نسختنا الأولى، فهي بالتأكيد أفضل، وهو أمر مطمئن. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done the average is around 3.6, although unlike the first version there are a couple times that it loses and requires more than six in this circumstance. ",
  "translatedText": "كل ما قيل وفعل هو المتوسط حوالي 3.6، على الرغم من أنه على عكس الإصدار الأول هناك عدة مرات يخسر فيها ويتطلب أكثر من ستة في هذه الظروف. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information. ",
  "translatedText": "من المفترض أنه هناك أوقات يتم فيها إجراء هذه المقايضة للوصول فعليًا إلى الهدف بدلاً من تعظيم المعلومات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6? ",
  "translatedText": "فهل يمكننا أن نفعل ما هو أفضل من 3. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can. ",
  "translatedText": "6؟ يمكننا بالتأكيد. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model. ",
  "translatedText": "لقد قلت في البداية أنه من الممتع جدًا محاولة عدم دمج القائمة الحقيقية للإجابات اللفظية في الطريقة التي يبني بها نموذجه. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43. ",
  "translatedText": "ولكن إذا قمنا بدمجها، فإن أفضل أداء يمكن أن أحصل عليه كان حوالي 3.43. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that. ",
  "translatedText": "لذا، إذا حاولنا أن نكون أكثر تعقيدًا من مجرد استخدام بيانات تكرار الكلمات لاختيار هذا التوزيع المسبق، فهذا 3.43 ربما يعطي الحد الأقصى لمدى الجودة التي يمكننا تحقيقها من خلال ذلك، أو على الأقل مدى الجودة التي يمكن أن نحققها من خلال ذلك. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one. ",
  "translatedText": "يستخدم هذا الأداء الأفضل بشكل أساسي الأفكار التي كنت أتحدث عنها هنا، ولكنه يذهب أبعد قليلاً، مثل البحث عن المعلومات المتوقعة خطوتين للأمام بدلاً من خطوة واحدة فقط. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is. ",
  "translatedText": "في الأصل كنت أخطط للحديث أكثر عن ذلك، لكنني أدركت أننا قد قطعنا وقتًا طويلاً بالفعل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least it's looking like Crane is the best opener. ",
  "translatedText": "الشيء الوحيد الذي سأقوله هو بعد إجراء هذا البحث المكون من خطوتين ثم تشغيل بعض نماذج المحاكاة في أفضل المرشحين، حتى الآن بالنسبة لي على الأقل يبدو أن Crane هو أفضل افتتاحية. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed? ",
  "translatedText": "من كان سيخمن؟ ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits. ",
  "translatedText": "وأيضًا إذا كنت تستخدم قائمة الكلمات الحقيقية لتحديد مساحة الاحتمالات الخاصة بك، فإن عدم اليقين الذي تبدأ به يزيد قليلاً عن 11 بت. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out, just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits. ",
  "translatedText": "وقد اتضح، من خلال بحث القوة الغاشمة فقط، أن الحد الأقصى الممكن للمعلومات المتوقعة بعد أول تخمينين هو حوالي 10 بتات. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty. ",
  "translatedText": "وهو ما يشير إلى أفضل سيناريو، بعد أول تخمينين، مع اللعب الأمثل تمامًا، سيتبقى لديك القليل من عدم اليقين. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses. ",
  "translatedText": "وهو نفس الأمر الذي يرجع إلى اثنين من التخمينات المحتملة. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "So I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as 3, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail. ",
  "translatedText": "لذلك أعتقد أنه من العدل وربما المحافظ جدًا أن نقول إنه لا يمكنك أبدًا كتابة خوارزمية تحصل على هذا المتوسط منخفضًا يصل إلى 3، لأنه مع الكلمات المتاحة لك، ببساطة ليس هناك مجال للحصول على معلومات كافية بعد خطوتين فقط قادر على ضمان الإجابة في الفتحة الثالثة في كل مرة دون فشل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]