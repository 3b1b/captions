[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy. ",
  "translatedText": "گیم Wurdle پچھلے ایک یا دو مہینوں میں کافی وائرل ہوا ہے، اور کبھی بھی ریاضی کے اسباق کے موقع کو نظر انداز نہیں کیا جا سکتا، مجھے یہ محسوس ہوتا ہے کہ یہ گیم انفارمیشن تھیوری کے بارے میں ایک سبق میں ایک بہت اچھی مرکزی مثال بناتی ہے، اور خاص طور پر ایک موضوع جسے اینٹروپی کہا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could. ",
  "translatedText": "آپ نے دیکھا، بہت سارے لوگوں کی طرح میں بھی اس پہیلی میں پھنس گیا تھا، اور بہت سارے پروگرامرز کی طرح میں بھی ایک الگورتھم لکھنے کی کوشش میں ڈوبا ہوا تھا جو گیم کو جتنا بہتر انداز میں کھیل سکتا تھا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy. ",
  "translatedText": "اور جو میں نے سوچا کہ میں یہاں کروں گا اس میں صرف آپ کے ساتھ اپنے کچھ عمل کے بارے میں بات کروں گا، اور اس میں شامل کچھ ریاضی کی وضاحت کروں گا، کیونکہ پورا الگورتھم انٹروپی کے اس خیال پر مرکوز ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle? ",
  "translatedText": "سب سے پہلے چیزیں، اگر آپ نے اس کے بارے میں نہیں سنا ہے، Wurdle کیا ہے؟ اور یہاں ایک پتھر سے دو پرندوں کو مارنے کے لیے جب ہم کھیل کے اصولوں سے گزرتے ہیں، مجھے یہ بھی پیش نظارہ کرنے دیں کہ ہم اس کے ساتھ کہاں جا رہے ہیں، جو کہ ایک چھوٹا الگورتھم تیار کرنا ہے جو بنیادی طور پر ہمارے لیے گیم کھیلے گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does. ",
  "translatedText": "اگرچہ میں نے آج کا Wurdle نہیں کیا ہے، یہ 4 فروری ہے، اور ہم دیکھیں گے کہ بوٹ کیسے کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess. ",
  "translatedText": "Wurdle کا مقصد ایک پراسرار پانچ حرفی لفظ کا اندازہ لگانا ہے، اور آپ کو اندازہ لگانے کے چھ مختلف مواقع فراہم کیے گئے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane. ",
  "translatedText": "مثال کے طور پر، میرا Wurdle bot تجویز کرتا ہے کہ میں اندازہ کرین سے شروع کرتا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer. ",
  "translatedText": "ہر بار جب آپ کوئی اندازہ لگاتے ہیں، آپ کو اس بارے میں کچھ معلومات ملتی ہیں کہ آپ کا اندازہ صحیح جواب کے کتنا قریب ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer. ",
  "translatedText": "یہاں گرے باکس مجھے بتا رہا ہے کہ اصل جواب میں کوئی سی نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position. ",
  "translatedText": "پیلا باکس مجھے بتا رہا ہے کہ ایک R ہے، لیکن یہ اس پوزیشن میں نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position. ",
  "translatedText": "سبز باکس مجھے بتا رہا ہے کہ خفیہ لفظ میں A ہے، اور یہ تیسری پوزیشن پر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E. ",
  "translatedText": "اور پھر کوئی N نہیں ہے اور کوئی E نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information. ",
  "translatedText": "تو مجھے اندر جانے دو اور ورڈل بوٹ کو وہ معلومات بتانے دو۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey. ",
  "translatedText": "ہم نے کرین سے آغاز کیا، ہمیں سرمئی، پیلا، سبز، سرمئی، بھوری رنگ ملا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time. ",
  "translatedText": "ان تمام اعداد و شمار کے بارے میں فکر نہ کریں جو یہ ابھی دکھا رہا ہے، میں مقررہ وقت پر اس کی وضاحت کروں گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick. ",
  "translatedText": "لیکن ہماری دوسری چننے کے لیے اس کی سب سے بڑی تجویز شٹک ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess. ",
  "translatedText": "اور آپ کا اندازہ اصل میں پانچ حروف کا لفظ ہونا ضروری ہے، لیکن جیسا کہ آپ دیکھیں گے، یہ کافی حد تک لبرل ہے جس سے یہ آپ کو حقیقت میں اندازہ لگانے دے گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick. ",
  "translatedText": "اس صورت میں، ہم shtick کی کوشش کریں. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good. ",
  "translatedText": "اور ٹھیک ہے، چیزیں بہت اچھی لگ رہی ہیں. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R. ",
  "translatedText": "ہم نے S اور H کو مارا، تو ہم پہلے تین حروف کو جانتے ہیں، ہم جانتے ہیں کہ ایک R ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something. ",
  "translatedText": "اور اس طرح یہ SHA کچھ R، یا SHA R کچھ کی طرح ہو گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp. ",
  "translatedText": "اور ایسا لگتا ہے کہ ورڈل بوٹ جانتا ہے کہ یہ صرف دو امکانات پر ہے، یا تو تیز یا تیز۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a toss up between them at this point, so I guess probably just because it's alphabetical it goes with shard. ",
  "translatedText": "یہ اس وقت ان کے درمیان ٹاس اپ کی طرح ہے، لہذا میرا اندازہ ہے کہ شاید صرف اس لیے کہ یہ حروف تہجی کے مطابق ہے یہ شارڈ کے ساتھ جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer. ",
  "translatedText": "کون سا ہورے، اصل جواب ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.22,
  "end": 132.86
 },
 {
  "input": "So we got it in three. ",
  "translatedText": "تو ہم نے اسے تین میں حاصل کیا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 132.96,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle four is par and three is birdie. ",
  "translatedText": "اگر آپ سوچ رہے ہیں کہ کیا یہ کوئی اچھا ہے، جس طرح سے میں نے ایک شخص کا جملہ سنا ہے وہ یہ ہے کہ Wurdle کے ساتھ چار برابر ہے اور تین برڈی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy. ",
  "translatedText": "جو میرے خیال میں کافی مناسب تشبیہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy. ",
  "translatedText": "آپ کو چار حاصل کرنے کے لیے اپنے کھیل پر مسلسل رہنا ہوگا، لیکن یہ یقینی طور پر پاگل نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great. ",
  "translatedText": "لیکن جب آپ اسے تین میں حاصل کرتے ہیں، تو یہ بہت اچھا لگتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot. ",
  "translatedText": "لہذا اگر آپ اس کے لئے مایوس ہیں تو، میں یہاں جو کرنا چاہتا ہوں وہ یہ ہے کہ میں شروع سے ہی اپنے سوچنے کے عمل کے ذریعے بات کروں کہ میں Wurdle bot تک کیسے پہنچتا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson. ",
  "translatedText": "اور جیسا کہ میں نے کہا، واقعی یہ انفارمیشن تھیوری کے سبق کا بہانہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy. ",
  "translatedText": "بنیادی مقصد یہ بتانا ہے کہ معلومات کیا ہے اور انٹروپی کیا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language. ",
  "translatedText": "اس تک پہنچنے میں میرا پہلا خیال انگریزی زبان میں مختلف حروف کی متعلقہ تعدد پر ایک نظر ڈالنا تھا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters? ",
  "translatedText": "تو میں نے سوچا، ٹھیک ہے، کیا کوئی ابتدائی اندازہ ہے یا اندازوں کا کوئی ابتدائی جوڑا جو ان اکثر خطوط کو متاثر کرتا ہے؟ اور ایک جس کا مجھے بہت شوق تھا وہ دوسرے کام کر رہا تھا جس کے بعد ناخن تھا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good. ",
  "translatedText": "سوچ یہ ہے کہ اگر آپ کسی خط کو مارتے ہیں، تو آپ جانتے ہیں، آپ کو سبز یا پیلا ملتا ہے، جو ہمیشہ اچھا لگتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.76,
  "end": 187.52
 },
 {
  "input": "It feels like you're getting information. ",
  "translatedText": "ایسا لگتا ہے کہ آپ معلومات حاصل کر رہے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.52,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information since it's pretty rare to find a word that doesn't have any of these letters. ",
  "translatedText": "لیکن ان صورتوں میں، یہاں تک کہ اگر آپ نہیں مارتے ہیں اور آپ ہمیشہ گرے ہو جاتے ہیں، تب بھی یہ آپ کو بہت سی معلومات فراہم کر رہا ہے کیونکہ ایسا لفظ تلاش کرنا بہت کم ہوتا ہے جس میں ان میں سے کوئی حرف نہ ہو۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters. ",
  "translatedText": "لیکن پھر بھی، یہ انتہائی منظم محسوس نہیں ہوتا، کیونکہ مثال کے طور پر، یہ حروف کی ترتیب پر غور کرنے کے لیے کچھ نہیں کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end? ",
  "translatedText": "جب میں گھونگا ٹائپ کرسکتا ہوں تو ناخن کیوں ٹائپ کریں؟ کیا آخر میں اس S کا ہونا بہتر ہے؟ مجھے واقعی یقین نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y. ",
  "translatedText": "اب، میرے ایک دوست نے کہا کہ وہ تھکے ہوئے لفظ کے ساتھ کھولنا پسند کرتا ہے، جس سے مجھے حیرت ہوئی کیونکہ اس میں کچھ غیر معمولی حروف ہیں جیسے W اور Y۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener. ",
  "translatedText": "لیکن کون جانتا ہے، شاید یہ ایک بہتر اوپنر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up. ",
  "translatedText": "کیا کسی قسم کا مقداری اسکور ہے جو ہم ممکنہ اندازے کے معیار کا فیصلہ کرنے کے لیے دے سکتے ہیں؟ اب اس طریقے کو ترتیب دینے کے لیے جس سے ہم ممکنہ اندازوں کی درجہ بندی کرنے جا رہے ہیں، آئیے واپس جائیں اور اس میں تھوڑی وضاحت شامل کریں کہ گیم بالکل کس طرح ترتیب دیا گیا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long. ",
  "translatedText": "لہذا الفاظ کی ایک فہرست ہے جو یہ آپ کو داخل کرنے کی اجازت دے گی جو درست اندازے سمجھے جاتے ہیں جو کہ تقریباً 13,000 الفاظ طویل ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble. ",
  "translatedText": "لیکن جب آپ اسے دیکھتے ہیں، تو بہت سی غیر معمولی چیزیں ہیں، سر یا علی اور ARG جیسی چیزیں، اس قسم کے الفاظ جو سکریبل کے کھیل میں خاندانی دلائل کو جنم دیتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word. ",
  "translatedText": "لیکن کھیل کی بات یہ ہے کہ جواب ہمیشہ ایک مہذب عام لفظ بنتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers. ",
  "translatedText": "اور درحقیقت، تقریباً 2300 الفاظ کی ایک اور فہرست ہے جو ممکنہ جوابات ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creator's girlfriend, which is kind of fun. ",
  "translatedText": "اور یہ ایک انسانی کیوریٹڈ لسٹ ہے، میرے خیال میں خاص طور پر گیم بنانے والے کی گرل فرینڈ کی طرف سے، جو کہ ایک طرح کی تفریحی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving Wordle that doesn't incorporate previous knowledge about this list. ",
  "translatedText": "لیکن میں کیا کرنا چاہوں گا، اس پروجیکٹ کے لیے ہمارا چیلنج یہ ہے کہ آیا ہم Wordle کو حل کرنے والا ایک پروگرام لکھ سکتے ہیں جس میں اس فہرست کے بارے میں سابقہ معلومات شامل نہ ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list. ",
  "translatedText": "ایک چیز کے لئے، بہت سارے عام پانچ حرفی الفاظ ہیں جو آپ کو اس فہرست میں نہیں مل پائیں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play Wordle against anyone, not just what happens to be the official website. ",
  "translatedText": "لہذا یہ بہتر ہوگا کہ ایک ایسا پروگرام لکھیں جو تھوڑا زیادہ لچکدار ہو اور کسی کے خلاف Wordle کھیلے، نہ کہ صرف آفیشل ویب سائٹ بننے کے لیے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also the reason that we know what this list of possible answers is, is because it's visible in the source code. ",
  "translatedText": "اور اس وجہ سے کہ ہم جانتے ہیں کہ ممکنہ جوابات کی یہ فہرست کیا ہے، کیونکہ یہ سورس کوڈ میں نظر آتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day. ",
  "translatedText": "لیکن جس طرح سے یہ سورس کوڈ میں نظر آتا ہے وہ مخصوص ترتیب میں ہے جس میں روزانہ جوابات آتے رہتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 292.8
 },
 {
  "input": "So you could always just look up what tomorrow's answer will be. ",
  "translatedText": "لہذا آپ ہمیشہ صرف یہ دیکھ سکتے ہیں کہ کل کا جواب کیا ہوگا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.06,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating. ",
  "translatedText": "تو واضح طور پر، کچھ احساس ہے جس میں فہرست کا استعمال دھوکہ دہی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words. ",
  "translatedText": "اور جو چیز زیادہ دلچسپ معمے اور معلوماتی تھیوری کے سبق کے لیے بناتی ہے وہ یہ ہے کہ اس کے بجائے کچھ اور آفاقی اعداد و شمار کا استعمال کیا جائے جیسے عام طور پر متعلقہ الفاظ کی تعدد زیادہ عام الفاظ کو ترجیح دینے کے اس وجدان کو حاصل کرنے کے لیے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality? ",
  "translatedText": "تو ان 13,000 امکانات میں سے، ہمیں ابتدائی اندازے کا انتخاب کیسے کرنا چاہیے؟ مثال کے طور پر، اگر میرا دوست تھکا ہوا تجویز کرتا ہے، تو ہمیں اس کے معیار کا تجزیہ کیسے کرنا چاہیے؟ ٹھیک ہے، اس نے جس وجہ سے کہا کہ اسے W کو پسند نہیں ہے وہ یہ ہے کہ وہ لانگ شاٹ کی نوعیت کو پسند کرتا ہے کہ اگر آپ W کو مارتے ہیں تو یہ کتنا اچھا لگتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern. ",
  "translatedText": "مثال کے طور پر، اگر سامنے آنے والا پہلا نمونہ کچھ اس طرح کا تھا، تو پتہ چلتا ہے کہ اس دیو قامت لغت میں صرف 58 الفاظ ہیں جو اس نمونے سے مماثل ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000. ",
  "translatedText": "تو یہ 13,000 سے بہت بڑی کمی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this. ",
  "translatedText": "لیکن اس کا دوسرا پہلو، یقیناً، یہ ہے کہ اس طرح کا نمونہ حاصل کرنا بہت غیر معمولی بات ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000. ",
  "translatedText": "خاص طور پر، اگر ہر لفظ کا یکساں طور پر جواب ہونے کا امکان ہے، تو اس پیٹرن کو مارنے کا امکان 58 کو تقریباً 13,000 سے تقسیم کیا جائے گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers. ",
  "translatedText": "یقینا، ان کے جوابات ہونے کا اتنا ہی امکان نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words. ",
  "translatedText": "ان میں سے اکثر انتہائی غیر واضح اور قابل اعتراض الفاظ ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later. ",
  "translatedText": "لیکن کم از کم اس سب میں ہمارے پہلے پاس کے لئے، آئیے فرض کریں کہ وہ سب یکساں طور پر امکان رکھتے ہیں اور پھر اسے تھوڑی دیر بعد بہتر کریں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is the pattern with a lot of information is by its very nature unlikely to occur. ",
  "translatedText": "نقطہ یہ ہے کہ بہت ساری معلومات کے ساتھ پیٹرن اس کی فطرت سے ہونے کا امکان نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely. ",
  "translatedText": "درحقیقت، معلوماتی ہونے کا مطلب یہ ہے کہ اس کا امکان نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where of course there's not a W in it. ",
  "translatedText": "اس اوپننگ کے ساتھ دیکھنے کے لیے ایک بہت زیادہ ممکنہ پیٹرن کچھ اس طرح ہوگا، جہاں یقیناً اس میں W نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y. ",
  "translatedText": "ہو سکتا ہے ایک E ہو، اور ہو سکتا ہے کہ کوئی A نہ ہو، کوئی R نہ ہو، Y نہیں ہو۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches. ",
  "translatedText": "اس صورت میں، 1400 ممکنہ میچز ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see. ",
  "translatedText": "اگر سب کا یکساں امکان تھا، تو یہ تقریباً 11% کا امکان ثابت ہوتا ہے کہ یہ وہ نمونہ ہے جسے آپ دیکھیں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative. ",
  "translatedText": "لہذا سب سے زیادہ ممکنہ نتائج بھی کم سے کم معلوماتی ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see. ",
  "translatedText": "یہاں مزید عالمی منظر حاصل کرنے کے لیے، میں آپ کو ان تمام مختلف نمونوں میں امکانات کی مکمل تقسیم دکھاتا ہوں جو آپ دیکھ سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common. ",
  "translatedText": "لہذا ہر بار جسے آپ دیکھ رہے ہیں وہ رنگوں کے ممکنہ پیٹرن سے مطابقت رکھتا ہے جو ظاہر ہو سکتا ہے، جن میں سے 3 سے 5 ویں امکانات ہیں، اور وہ بائیں سے دائیں تک منظم ہیں، سب سے عام سے کم سے کم عام۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays. ",
  "translatedText": "تو یہاں سب سے عام امکان یہ ہے کہ آپ کو تمام گرے مل جائیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time. ",
  "translatedText": "یہ تقریباً 14% وقت ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here where there's only 18 possibilities for what matches this pattern that evidently look like this. ",
  "translatedText": "اور جب آپ اندازہ لگاتے ہیں تو آپ جس چیز کی امید کر رہے ہیں وہ یہ ہے کہ آپ اس لمبی دم میں کہیں ختم ہو جائیں گے، جیسا کہ یہاں پر جہاں اس پیٹرن سے میل کھاتا ہے اس کے لیے صرف 18 امکانات ہیں جو ظاہر ہے کہ اس طرح نظر آتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here. ",
  "translatedText": "یا اگر ہم بائیں طرف تھوڑا سا آگے بڑھیں، تو آپ جانتے ہیں، شاید ہم یہاں سے گزر جائیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you. ",
  "translatedText": "ٹھیک ہے، یہ آپ کے لیے ایک اچھی پہیلی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them? ",
  "translatedText": "انگریزی زبان میں وہ کون سے تین الفاظ ہیں جو W سے شروع ہوتے ہیں، Y پر ختم ہوتے ہیں اور ان میں کہیں R ہوتا ہے؟ باہر کر دیتا ہے، جوابات ہیں، آئیے دیکھتے ہیں، لفظی، کیڑے، اور wryly. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution. ",
  "translatedText": "اس لیے یہ فیصلہ کرنے کے لیے کہ یہ لفظ مجموعی طور پر کتنا اچھا ہے، ہم معلومات کی متوقع مقدار کا کچھ اندازہ چاہتے ہیں جو آپ اس تقسیم سے حاصل کرنے جا رہے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score. ",
  "translatedText": "اگر ہم ہر ایک پیٹرن سے گزرتے ہیں اور ہم اس کے وقوع پذیر ہونے کے امکانات کو کسی ایسی چیز سے ضرب دیتے ہیں جس سے اندازہ ہوتا ہے کہ یہ کتنا معلوماتی ہے، تو یہ ہمیں ایک معروضی سکور دے سکتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches. ",
  "translatedText": "اب آپ کی پہلی جبلت اس کے لیے کہ کچھ ہونا چاہیے میچوں کی تعداد ہو سکتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches. ",
  "translatedText": "آپ میچوں کی کم اوسط تعداد چاہتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer. ",
  "translatedText": "لیکن اس کے بجائے میں ایک زیادہ عالمگیر پیمائش کا استعمال کرنا چاہوں گا جسے ہم اکثر معلومات سے منسوب کرتے ہیں، اور ایک بار جو ہمارے پاس ان 13,000 الفاظ میں سے ہر ایک کو مختلف امکان تفویض کرنے کے بعد زیادہ لچکدار ہو جائے گا کہ آیا وہ اصل میں جواب ہیں یا نہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but it's really intuitive if we just look at examples. ",
  "translatedText": "معلومات کی معیاری اکائی بٹ ہے، جس میں تھوڑا سا مضحکہ خیز فارمولہ ہے، لیکن اگر ہم صرف مثالوں کو دیکھیں تو یہ واقعی بدیہی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information. ",
  "translatedText": "اگر آپ کے پاس کوئی ایسا مشاہدہ ہے جو آپ کے امکانات کی جگہ کو آدھا کر دیتا ہے، تو ہم کہتے ہیں کہ اس میں تھوڑی سی معلومات ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about Half of the five letter words have an S, a little less than that, but about half. ",
  "translatedText": "ہماری مثال میں، امکانات کی جگہ تمام ممکنہ الفاظ ہیں، اور یہ پتہ چلتا ہے کہ پانچ حرفی الفاظ میں سے نصف میں S ہے، اس سے تھوڑا کم، لیکن تقریباً نصف۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information. ",
  "translatedText": "تاکہ یہ مشاہدہ آپ کو تھوڑی سی معلومات فراہم کرے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information. ",
  "translatedText": "اگر اس کے بجائے کوئی نئی حقیقت امکانات کی اس جگہ کو چار کے عنصر سے کم کر دیتی ہے، تو ہم کہتے ہیں کہ اس میں معلومات کے دو بٹس ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T. ",
  "translatedText": "مثال کے طور پر، یہ پتہ چلتا ہے کہ ان الفاظ میں سے ایک چوتھائی میں T ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth. ",
  "translatedText": "اگر مشاہدہ اس جگہ کو آٹھ کے عنصر سے کاٹتا ہے، تو ہم کہتے ہیں کہ یہ معلومات کے تین بٹس ہیں، وغیرہ وغیرہ۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a 16th, five bits cuts it into a 32nd. ",
  "translatedText": "چار بٹس اسے 16ویں میں کاٹتے ہیں، پانچ بٹس اسے 32ویں میں کاٹ دیتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.52
 },
 {
  "input": "So now you might want to pause and ask yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 533.52,
  "end": 542.98
 },
 {
  "input": "What we're saying here is that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability. ",
  "translatedText": "تو اب آپ رک کر اپنے آپ سے پوچھنا چاہیں گے، وقوع پذیر ہونے کے امکان کے لحاظ سے بٹس کی تعداد کے لیے معلومات کا فارمولا کیا ہے؟ ہم یہاں جو کہہ رہے ہیں وہ یہ ہے کہ جب آپ بٹس کی تعداد میں ایک نصف لیتے ہیں، تو یہ وہی چیز ہے جو امکان ہے، جو کہ بٹس کی تعداد کی طاقت کو دو کہنے کے برابر ہے، جو کہ امکان سے زیادہ ایک ہے۔یہ کہنے کے لیے مزید ترتیب دیتا ہے کہ انفارمیشن لاگ بیس دو ہے جسے احتمال سے تقسیم کیا جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still, where the information is the negative log base two of the probability. ",
  "translatedText": "اور کبھی کبھی آپ اسے اب بھی ایک اور ترتیب کے ساتھ دیکھتے ہیں، جہاں معلومات امکان کا منفی لاگ بیس دو ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half. ",
  "translatedText": "اس طرح کا اظہار، یہ غیر شروع کرنے والوں کو تھوڑا سا عجیب لگ سکتا ہے، لیکن یہ واقعی میں یہ پوچھنے کا صرف ایک بدیہی خیال ہے کہ آپ نے کتنی بار اپنے امکانات کو نصف میں کم کیا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095. ",
  "translatedText": "اب اگر آپ سوچ رہے ہیں، آپ کو معلوم ہے، میں نے سوچا کہ ہم صرف ایک تفریحی لفظ کا کھیل کھیل رہے ہیں، تصویر میں لوگارتھم کیوں داخل ہو رہے ہیں؟ اس کی ایک اچھی اکائی کی ایک وجہ یہ ہے کہ انتہائی غیر متوقع واقعات کے بارے میں بات کرنا بہت آسان ہے، یہ کہنا بہت آسان ہے کہ مشاہدے میں معلومات کے 20 بٹس ہوتے ہیں اس کے مقابلے میں یہ کہنا کہ فلاں اور فلاں کا امکان 0 ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together. ",
  "translatedText": "0000095 لیکن ایک زیادہ ٹھوس وجہ یہ ہے کہ یہ لوگارتھمک اظہار نظریہ امکان میں ایک بہت ہی مفید اضافہ ثابت ہوا ہے وہ طریقہ ہے جس سے معلومات کا اضافہ ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information. ",
  "translatedText": "مثال کے طور پر، اگر ایک مشاہدہ آپ کو معلومات کے دو بٹس دیتا ہے، آپ کی جگہ کو چار کر دیتا ہے، اور پھر دوسرا مشاہدہ جیسا کہ ورڈل میں آپ کے دوسرے اندازے سے آپ کو معلومات کے مزید تین بٹس ملتے ہیں، جو آپ کو آٹھ کے دوسرے عنصر سے مزید کاٹ دیتے ہیں، دو مل کر آپ کو معلومات کے پانچ بٹس دیتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add. ",
  "translatedText": "اسی طرح جس طرح امکانات بڑھنا پسند کرتے ہیں، معلومات شامل کرنا پسند کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with. ",
  "translatedText": "اس لیے جیسے ہی ہم کسی متوقع قدر جیسی چیز کے دائرے میں ہوتے ہیں، جہاں ہم نمبروں کا ایک گروپ شامل کر رہے ہوتے ہیں، نوشتہ جات اس سے نمٹنے کے لیے بہت اچھا بنا دیتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for Weary and add another little tracker on here, showing us how much information there is for each pattern. ",
  "translatedText": "آئیے Weary کے لیے اپنی تقسیم پر واپس جائیں اور یہاں ایک اور چھوٹا ٹریکر شامل کریں، جو ہمیں دکھاتے ہیں کہ ہر پیٹرن کے لیے کتنی معلومات موجود ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain. ",
  "translatedText": "اہم چیز جس پر میں آپ کو توجہ دینا چاہتا ہوں وہ یہ ہے کہ جتنا زیادہ امکان ہم ان زیادہ ممکنہ نمونوں تک پہنچیں گے، معلومات اتنی ہی کم ہوں گی، آپ کو اتنے ہی کم بٹس حاصل ہوں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information, where we go through each pattern, we say how probable is it, and then we multiply that by how many bits of information do we get. ",
  "translatedText": "جس طرح سے ہم اس اندازے کے معیار کی پیمائش کرتے ہیں وہ اس معلومات کی متوقع قدر کو لے کر جائے گا، جہاں ہم ہر پیٹرن سے گزرتے ہیں، ہم کہتے ہیں کہ یہ کتنا ممکن ہے، اور پھر ہم اس کو ضرب دیتے ہیں کہ ہمیں معلومات کے کتنے بٹس ملتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 643.5,
  "end": 654.06
 },
 {
  "input": "And in the example of Weary, that turns out to be 4.9 bits. ",
  "translatedText": "اور ویری کی مثال میں، یہ 4 نکلتا ہے۔9 بٹس۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times. ",
  "translatedText": "اس لیے اوسطاً، آپ کو اس ابتدائی اندازے سے جو معلومات ملتی ہیں وہ اتنی ہی اچھی ہے جتنی کہ آپ کے امکانات کی جگہ کو تقریباً پانچ گنا میں کاٹنا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like Slate. ",
  "translatedText": "اس کے برعکس، زیادہ متوقع معلوماتی قدر کے ساتھ اندازے کی مثال Slate کی طرح کچھ ہو گی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case you'll notice the distribution looks a lot flatter. ",
  "translatedText": "اس معاملے میں آپ دیکھیں گے کہ تقسیم بہت زیادہ چاپلوسی لگ رہی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information. ",
  "translatedText": "خاص طور پر، تمام گرے رنگوں میں سب سے زیادہ امکانی واقعہ ہونے کا صرف 6% امکان ہوتا ہے، لہذا کم از کم آپ کو واضح طور پر 3 مل رہے ہیں۔معلومات کے 9 بٹس۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that. ",
  "translatedText": "لیکن یہ ایک کم از کم ہے، زیادہ عام طور پر آپ کو اس سے بہتر کچھ ملے گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8. ",
  "translatedText": "اور یہ پتہ چلتا ہے جب آپ اس پر نمبروں کو کم کرتے ہیں اور تمام متعلقہ شرائط کو شامل کرتے ہیں، اوسط معلومات تقریبا 5 ہے. 8۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with Weary, your space of possibilities will be about half as big after this first guess, on average. ",
  "translatedText": "تو ویری کے برعکس، آپ کے امکانات کی جگہ اس پہلے اندازے کے بعد اوسطاً نصف ہو جائے گی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity. ",
  "translatedText": "اصل میں معلومات کی مقدار کی اس متوقع قدر کے نام کے بارے میں ایک دلچسپ کہانی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "Information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science. ",
  "translatedText": "انفارمیشن تھیوری کو کلاڈ شینن نے تیار کیا تھا، جو 1940 کی دہائی میں بیل لیبز میں کام کر رہے تھے، لیکن وہ جان وان نیومن کے ساتھ اپنے ابھی تک شائع ہونے والے کچھ نظریات کے بارے میں بات کر رہے تھے، جو اس وقت کا یہ دانشور دیو تھا، بہت نمایاں۔ریاضی اور طبیعیات میں اور اس کی شروعات جو کمپیوٹر سائنس بن رہی تھی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well you should call it entropy, and for two reasons. ",
  "translatedText": "اور جب اس نے ذکر کیا کہ ان کے پاس معلومات کی مقدار کی اس متوقع قدر کے لیے واقعی کوئی اچھا نام نہیں ہے، تو وان نیومن نے کہا، تو کہانی چلتی ہے، ٹھیک ہے آپ کو اسے اینٹروپی کہنا چاہیے، اور دو وجوہات کی بنا پر۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage. ",
  "translatedText": "پہلے نمبر پر، آپ کی غیر یقینیت کا فنکشن شماریاتی میکانکس میں اس نام کے تحت استعمال کیا گیا ہے، لہذا اس کا پہلے سے ہی ایک نام ہے، اور دوسری جگہ، اور اس سے بھی اہم بات، کوئی نہیں جانتا کہ اینٹروپی واقعی کیا ہے، لہذا بحث میں آپ ہمیشہ فائدہ ہے. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design. ",
  "translatedText": "لہذا اگر نام تھوڑا سا پراسرار لگتا ہے، اور اگر اس کہانی پر یقین کیا جائے، تو یہ ڈیزائن کے لحاظ سے ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess. ",
  "translatedText": "اس کے علاوہ اگر آپ سوچ رہے ہیں کہ اس کا تعلق طبیعیات سے تھرموڈینامکس کے اس دوسرے قانون سے ہے، تو یقینی طور پر ایک تعلق ہے، لیکن اس کی ابتداء میں شینن صرف خالص امکانی نظریہ کے ساتھ کام کر رہا تھا، اور یہاں ہمارے مقاصد کے لیے، جب میں استعمال کرتا ہوں۔لفظ اینٹروپی، میں صرف یہ چاہتا ہوں کہ آپ کسی خاص اندازے کی متوقع معلومات کی قدر کے بارے میں سوچیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously. ",
  "translatedText": "آپ اینٹروپی کو بیک وقت دو چیزوں کی پیمائش کے طور پر سوچ سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution. ",
  "translatedText": "پہلا یہ ہے کہ تقسیم کتنی فلیٹ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be. ",
  "translatedText": "تقسیم یکساں ہونے کے جتنی قریب ہوگی، اینٹروپی اتنی ہی زیادہ ہوگی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy. ",
  "translatedText": "ہمارے معاملے میں، جہاں 3 سے 5ویں کل پیٹرن ہیں، یکساں تقسیم کے لیے، ان میں سے کسی ایک کا مشاہدہ کرنے سے 3 سے 5ویں میں سے معلوماتی لاگ بیس 2 ہوگا، جو کہ 7 ہوتا ہے۔92، تو یہ مطلق زیادہ سے زیادہ ہے جو آپ کے پاس اس اینٹروپی کے لیے ہو سکتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place. ",
  "translatedText": "لیکن اینٹروپی بھی ایک قسم کا پیمانہ ہے کہ پہلی جگہ میں کتنے امکانات ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits. ",
  "translatedText": "مثال کے طور پر، اگر آپ کے پاس کوئی ایسا لفظ ہے جہاں صرف 16 ممکنہ نمونے ہیں، اور ہر ایک کا یکساں امکان ہے، تو یہ انٹروپی، یہ متوقع معلومات، 4 بٹس ہوں گی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits. ",
  "translatedText": "لیکن اگر آپ کے پاس کوئی اور لفظ ہے جہاں 64 ممکنہ نمونے سامنے آسکتے ہیں، اور ان سب کا یکساں امکان ہے، تو اینٹروپی 6 بٹس پر کام کرے گی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes. ",
  "translatedText": "لہذا اگر آپ جنگل میں کچھ تقسیم دیکھتے ہیں جس میں 6 بٹس کی اینٹروپی ہوتی ہے، تو یہ اس طرح ہے جیسے یہ کہہ رہا ہے کہ کیا ہونے والا ہے اس میں اتنا ہی تغیر اور غیر یقینی ہے جیسے کہ 64 یکساں طور پر ممکنہ نتائج ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this. ",
  "translatedText": "Wurtelebot میں اپنے پہلے پاس کے لئے، میں نے بنیادی طور پر اسے صرف یہ کرنا تھا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the possible guesses you could have, all 13,000 words, computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns you might see, for each one, and picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible. ",
  "translatedText": "یہ آپ کے تمام ممکنہ اندازوں سے گزرتا ہے، تمام 13,000 الفاظ، ہر ایک کے لیے انٹراپی کی گنتی کرتا ہے، یا خاص طور پر، ان تمام نمونوں میں تقسیم کی اینٹروپی جو آپ دیکھ سکتے ہیں، ہر ایک کے لیے، اور سب سے زیادہ منتخب کرتا ہے، کیونکہ یہ ہے۔ایک جو ممکنہ حد تک آپ کے امکانات کی جگہ کو کم کر سکتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses. ",
  "translatedText": "اور اگرچہ میں یہاں صرف پہلے اندازے کے بارے میں بات کر رہا ہوں، یہ اگلے چند اندازوں کے لیے بھی یہی کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words. ",
  "translatedText": "مثال کے طور پر، اس پہلے اندازے پر کچھ پیٹرن دیکھنے کے بعد، جو آپ کو اس بات کی بنیاد پر ممکنہ الفاظ کی ایک چھوٹی تعداد تک محدود کر دے گا کہ اس سے کیا مماثل ہے، آپ صرف الفاظ کے اس چھوٹے سیٹ کے حوالے سے وہی کھیل کھیلتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy. ",
  "translatedText": "دوسرے مجوزہ اندازے کے لیے، آپ ان تمام نمونوں کی تقسیم کو دیکھتے ہیں جو الفاظ کے اس زیادہ محدود مجموعے سے ہو سکتے ہیں، آپ تمام 13,000 امکانات کو تلاش کرتے ہیں، اور آپ کو وہ مل جاتا ہے جو اس اینٹروپی کو زیادہ سے زیادہ کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins. ",
  "translatedText": "آپ کو یہ بتانے کے لیے کہ یہ عمل میں کیسے کام کرتا ہے، مجھے صرف Wurtele کی ایک چھوٹی سی شکل پیش کرنے دیں جو میں نے لکھا ہے جو اس تجزیہ کی جھلکیاں حاشیے میں دکھاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "After doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information. ",
  "translatedText": "اپنے تمام اینٹروپی حسابات کرنے کے بعد، یہاں دائیں طرف یہ ہمیں دکھا رہا ہے کہ کس کے پاس سب سے زیادہ متوقع معلومات ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch. ",
  "translatedText": "سب سے اوپر کا جواب نکلتا ہے، کم از کم اس وقت، ہم اسے بعد میں بہتر کریں گے، Tares ہے، جس کا مطلب ہے، um، بلاشبہ، a vetch، سب سے عام vetch. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got, given this particular pattern. ",
  "translatedText": "ہر بار جب ہم یہاں ایک اندازہ لگاتے ہیں، جہاں شاید میں اس کی سفارشات کو نظر انداز کر دیتا ہوں اور سلیٹ کے ساتھ جاتا ہوں، کیونکہ مجھے سلیٹ پسند ہے، ہم دیکھ سکتے ہیں کہ اس میں کتنی متوقع معلومات تھیں، لیکن پھر یہاں لفظ کے دائیں طرف یہ ہمیں دکھا رہا ہے کہ کتنا اس مخصوص پیٹرن کو دیکھتے ہوئے ہمیں اصل معلومات ملی ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that. ",
  "translatedText": "تو یہاں ایسا لگتا ہے کہ ہم تھوڑا بدقسمت تھے، ہمیں 5 ملنے کی امید تھی۔8، لیکن ہمیں اس سے کم کے ساتھ کچھ ملا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now. ",
  "translatedText": "اور پھر یہاں بائیں جانب یہ ہمیں دکھا رہا ہے مختلف ممکنہ الفاظ جو کہ ہم اب کہاں ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment. ",
  "translatedText": "نیلے رنگ کی سلاخیں ہمیں بتا رہی ہیں کہ یہ ہر لفظ کے بارے میں کتنا امکان رکھتا ہے، لہذا اس وقت یہ فرض کر رہا ہے کہ ہر لفظ کے ہونے کا یکساں امکان ہے، لیکن ہم اسے ایک لمحے میں بہتر کر دیں گے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities. ",
  "translatedText": "اور پھر یہ غیر یقینی کی پیمائش ہمیں ممکنہ الفاظ میں اس تقسیم کی انٹراپی بتا رہی ہے، جو کہ فی الوقت، کیونکہ یہ یکساں تقسیم ہے، امکانات کی تعداد کو گننے کا محض ایک غیرضروری طور پر پیچیدہ طریقہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities. ",
  "translatedText": "مثال کے طور پر، اگر ہم 2 کو 13 کی طاقت پر لے جائیں۔66، یہ 13,000 امکانات کے ارد گرد ہونا چاہئے. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "I'm a little bit off here, but only because I'm not showing all the decimal places. ",
  "translatedText": "میں یہاں سے تھوڑا سا دور ہوں، لیکن صرف اس وجہ سے کہ میں تمام اعشاریہ جگہیں نہیں دکھا رہا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute. ",
  "translatedText": "اس وقت یہ بے کار محسوس ہو سکتا ہے اور جیسے کہ یہ بہت زیادہ پیچیدہ چیزیں ہیں، لیکن آپ دیکھیں گے کہ ایک منٹ میں دونوں نمبروں کا ہونا کیوں مفید ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Ramen, which again just really doesn't feel like a word. ",
  "translatedText": "تو یہاں ایسا لگتا ہے کہ یہ ہمارے دوسرے اندازے کے لیے سب سے زیادہ اینٹروپی تجویز کر رہا ہے رامین ہے، جو ایک بار پھر واقعی ایک لفظ کی طرح محسوس نہیں ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here, I'm going to go ahead and type in Rains. ",
  "translatedText": "لہذا یہاں اخلاقی بلندی حاصل کرنے کے لیے، میں آگے بڑھ کر رینز ٹائپ کرنے جا رہا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky. ",
  "translatedText": "اور پھر ایسا لگتا ہے کہ ہم کچھ بدقسمت تھے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information. ",
  "translatedText": "ہم 4 کی توقع کر رہے تھے۔3 بٹس اور ہمیں صرف 3 ملے۔معلومات کے 39 بٹس۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities. ",
  "translatedText": "تو یہ ہمیں 55 امکانات تک لے جاتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means. ",
  "translatedText": "اور یہاں شاید میں اصل میں اس کے ساتھ جاؤں گا جو یہ تجویز کر رہا ہے، جو کامبو ہے، اس کا مطلب کچھ بھی ہو۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And okay, this is actually a good chance for a puzzle. ",
  "translatedText": "اور ٹھیک ہے، یہ دراصل ایک پہیلی کے لیے ایک اچھا موقع ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information. ",
  "translatedText": "یہ ہمیں بتا رہا ہے کہ یہ نمونہ ہمیں 4 دیتا ہے۔معلومات کے 7 بٹس۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty. ",
  "translatedText": "لیکن بائیں طرف، اس سے پہلے کہ ہم اس پیٹرن کو دیکھیں، وہاں 5 تھے۔غیر یقینی صورتحال کے 78 بٹس۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well, it means that we're reduced down to one bit of uncertainty, which is the same thing as saying that there's two possible answers. ",
  "translatedText": "تو آپ کے لیے کوئز کے طور پر، بقیہ امکانات کی تعداد کے بارے میں اس کا کیا مطلب ہے؟ ٹھیک ہے، اس کا مطلب یہ ہے کہ ہم تھوڑی سی غیر یقینی صورتحال تک کم ہو گئے ہیں، جو کہ دو ممکنہ جوابات ہونے کے برابر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice. ",
  "translatedText": "یہ 50-50 کا انتخاب ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss. ",
  "translatedText": "اور یہاں سے، کیونکہ آپ اور میں جانتے ہیں کہ کون سے الفاظ زیادہ عام ہیں، ہم جانتے ہیں کہ جواب پاتال میں ہونا چاہیے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that. ",
  "translatedText": "لیکن جیسا کہ ابھی لکھا گیا ہے، پروگرام کو یہ معلوم نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it. ",
  "translatedText": "لہذا یہ صرف جاری رہتا ہے، زیادہ سے زیادہ معلومات حاصل کرنے کی کوشش کرتا ہے، جب تک کہ اس کا صرف ایک امکان باقی نہ رہ جائے، اور پھر وہ اس کا اندازہ لگا لیتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy. ",
  "translatedText": "تو ظاہر ہے کہ ہمیں ایک بہتر اینڈگیم حکمت عملی کی ضرورت ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1042.34
 },
 {
  "input": "But let's say we call this version one of our wordle solver, and then we go and run some simulations to see how it does. ",
  "translatedText": "لیکن ہم کہتے ہیں کہ ہم اس ورژن کو اپنے ورڈل سولور میں سے ایک کہتے ہیں، اور پھر ہم یہ دیکھنے کے لیے کچھ نقلیں چلاتے ہیں کہ یہ کیسے ہوتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1042.6,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game. ",
  "translatedText": "تو جس طرح سے یہ کام کر رہا ہے وہ ہر ممکن لفظی کھیل کھیل رہا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers. ",
  "translatedText": "یہ ان تمام 2315 الفاظ سے گزر رہا ہے جو اصل الفاظ کے جوابات ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set. ",
  "translatedText": "یہ بنیادی طور پر اسے ٹیسٹنگ سیٹ کے طور پر استعمال کر رہا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice. ",
  "translatedText": "اور اس سادہ طریقے کے ساتھ اس بات پر غور نہ کریں کہ کوئی لفظ کتنا عام ہے، اور صرف راستے میں ہر قدم پر معلومات کو زیادہ سے زیادہ کرنے کی کوشش کرتے ہیں، یہاں تک کہ یہ ایک اور صرف ایک انتخاب تک پہنچ جائے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124. ",
  "translatedText": "تخروپن کے اختتام تک، اوسط سکور تقریباً 4 ہو جاتا ہے۔124. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expected to do worse. ",
  "translatedText": "جو برا نہیں ہے، سچ پوچھیں تو، مجھے اس سے بھی بدتر ہونے کی توقع ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4. ",
  "translatedText": "لیکن جو لوگ ورڈل کھیلتے ہیں وہ آپ کو بتائیں گے کہ وہ اسے عام طور پر 4 میں حاصل کر سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can. ",
  "translatedText": "اصل چیلنج یہ ہے کہ آپ 3 میں زیادہ سے زیادہ حاصل کریں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3. ",
  "translatedText": "یہ 4 کے اسکور اور 3 کے اسکور کے درمیان ایک بہت بڑی چھلانگ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that. ",
  "translatedText": "یہاں واضح طور پر کم لٹکنے والا پھل کسی نہ کسی طرح شامل کرنا ہے کہ آیا کوئی لفظ عام ہے یا نہیں، اور ہم اسے کیسے کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language. ",
  "translatedText": "جس طرح میں نے اس سے رابطہ کیا وہ یہ ہے کہ انگریزی زبان کے تمام الفاظ کے لیے متعلقہ تعدد کی فہرست حاصل کی جائے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1107.88
 },
 {
  "input": "And I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset. ",
  "translatedText": "اور میں نے ابھی ریاضی کا لفظ فریکوئنسی ڈیٹا فنکشن استعمال کیا ہے، جو خود گوگل بوکس انگلش اینگرام پبلک ڈیٹاسیٹ سے کھینچتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words. ",
  "translatedText": "اور یہ دیکھنے میں ایک قسم کا مزہ ہے، مثال کے طور پر اگر ہم اسے سب سے عام الفاظ سے کم سے کم عام الفاظ میں ترتیب دیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5 letter words in the English language. ",
  "translatedText": "ظاہر ہے کہ یہ انگریزی زبان میں سب سے زیادہ عام، 5 حروف والے الفاظ ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common. ",
  "translatedText": "یا بلکہ، یہ 8 ویں سب سے زیادہ عام ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there. ",
  "translatedText": "پہلا وہ ہے جو، اس کے بعد وہاں اور وہاں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common. ",
  "translatedText": "پہلا خود پہلا نہیں، بلکہ 9واں ہے، اور یہ سمجھ میں آتا ہے کہ یہ دوسرے الفاظ زیادہ کثرت سے آ سکتے ہیں، جہاں پہلے کے بعد والے بعد میں ہیں، کہاں ہیں، اور جو تھوڑا سا کم عام ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency. ",
  "translatedText": "اب، اس ڈیٹا کا استعمال کرتے ہوئے یہ ماڈل بنانے کے لیے کہ ان الفاظ میں سے ہر ایک کے حتمی جواب ہونے کا کتنا امکان ہے، یہ صرف تعدد کے متناسب نہیں ہونا چاہیے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1146.34
 },
 {
  "input": "For example, which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely. ",
  "translatedText": "مثال کے طور پر، جس کا سکور 0 دیا گیا ہے۔اس ڈیٹاسیٹ میں 002، جبکہ لفظ braid کسی معنی میں تقریباً 1000 گنا کم ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.7,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering. ",
  "translatedText": "لیکن یہ دونوں کافی عام الفاظ ہیں کہ وہ تقریبا یقینی طور پر قابل غور ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1158.84
 },
 {
  "input": "So we want more of a binary cutoff. ",
  "translatedText": "تو ہم بائنری کٹ آف کے مزید چاہتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1159.34,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty. ",
  "translatedText": "جس طرح سے میں اس کے بارے میں گیا وہ یہ ہے کہ الفاظ کی اس پوری ترتیب شدہ فہرست کو لینے کا تصور کرنا، اور پھر اسے ایک ایکس محور پر ترتیب دینا، اور پھر سگمائڈ فنکشن کو لاگو کرنا، جو ایک فنکشن رکھنے کا معیاری طریقہ ہے جس کی آؤٹ پٹ بنیادی طور پر بائنری ہے، یہ ہے یا تو 0 یا یہ 1 ہے، لیکن غیر یقینی صورتحال کے اس خطے کے درمیان ہموار ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis. ",
  "translatedText": "لہذا بنیادی طور پر، حتمی فہرست میں شامل ہونے کے لیے میں ہر لفظ کو جو امکان تفویض کر رہا ہوں وہ اوپر موجود سگمائیڈ فنکشن کی قدر ہو گی جہاں یہ x-axis پر بیٹھے گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff. ",
  "translatedText": "اب ظاہر ہے کہ یہ چند پیرامیٹرز پر منحصر ہے، مثال کے طور پر x-axis پر ان الفاظ کی کتنی چوڑی جگہ اس بات کا تعین کرتی ہے کہ ہم 1 سے 0 تک کس طرح آہستہ آہستہ یا تیزی سے گرتے ہیں، اور ہم انہیں بائیں سے دائیں کہاں رکھتے ہیں کٹ آف کا تعین کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "To be honest, the way I did this was just licking my finger and sticking it into the wind. ",
  "translatedText": "سچ پوچھیں تو، جس طرح سے میں نے یہ کیا وہ صرف اپنی انگلی کو چاٹ کر ہوا میں چپکا رہا تھا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff. ",
  "translatedText": "میں نے ترتیب دی گئی فہرست کو دیکھا اور ایک ونڈو تلاش کرنے کی کوشش کی جہاں میں نے اسے دیکھا تو میں نے سوچا کہ ان الفاظ میں سے تقریباً نصف کے حتمی جواب نہ ہونے کا امکان زیادہ ہے، اور اسے کٹ آف کے طور پر استعمال کیا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement. ",
  "translatedText": "ایک بار جب ہم الفاظ میں اس طرح کی تقسیم کر لیتے ہیں، تو یہ ہمیں ایک اور صورت حال فراہم کرتا ہے جہاں اینٹروپی یہ واقعی مفید پیمائش بن جاتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were a feather and nails, and we end up with a situation where there's four possible words that match it. ",
  "translatedText": "مثال کے طور پر، ہم کہتے ہیں کہ ہم ایک گیم کھیل رہے تھے اور ہم اپنے پرانے اوپنرز سے شروع کرتے ہیں، جو ایک پنکھ اور ناخن تھے، اور ہم ایک ایسی صورت حال کے ساتھ ختم ہوتے ہیں جہاں چار ممکنہ الفاظ اس سے ملتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely. ",
  "translatedText": "اور ہم کہتے ہیں کہ ہم ان سب کو یکساں امکان سمجھتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1235.62
 },
 {
  "input": "Let me ask you, what is the entropy of this distribution? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1236.22,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2. ",
  "translatedText": "میں آپ سے پوچھتا ہوں، اس تقسیم کی اینٹروپی کیا ہے؟ ٹھیک ہے، ان میں سے ہر ایک سے منسلک معلومات 4 کا لاگ بیس 2 ہونے جا رہی ہے، کیونکہ ہر ایک 1 اور 4 ہے، اور وہ 2 ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "Two bits of information, four possibilities. ",
  "translatedText": "معلومات کے دو بٹس، چار امکانات۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good. ",
  "translatedText": "سب بہت اچھا اور اچھا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than four matches? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it. ",
  "translatedText": "لیکن کیا ہوگا اگر میں آپ کو بتاؤں کہ اصل میں چار سے زیادہ میچز ہیں؟ حقیقت میں، جب ہم مکمل الفاظ کی فہرست کو دیکھتے ہیں، تو اس سے 16 الفاظ ملتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure. ",
  "translatedText": "لیکن فرض کریں کہ ہمارا ماڈل ان دیگر 12 الفاظ پر بہت کم امکان رکھتا ہے جو اصل میں حتمی جواب ہیں، کچھ 1000 میں 1 کی طرح کیونکہ وہ واقعی غیر واضح ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before. ",
  "translatedText": "اب میں آپ سے پوچھتا ہوں کہ اس تقسیم کی اینٹروپی کیا ہے؟ اگر اینٹروپی خالصتاً یہاں میچوں کی تعداد کی پیمائش کر رہی تھی، تو آپ توقع کر سکتے ہیں کہ یہ 16 کے لاگ بیس 2 جیسا کچھ ہوگا، جو کہ 4 ہوگا، ہمارے پاس پہلے سے زیادہ غیر یقینی صورتحال کے دو بٹس۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before. ",
  "translatedText": "لیکن یقیناً اصل غیر یقینی صورتحال اس سے مختلف نہیں ہے جو ہمارے پاس پہلے تھی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example. ",
  "translatedText": "صرف اس وجہ سے کہ یہ 12 واقعی غیر واضح الفاظ ہیں اس کا مطلب یہ نہیں ہے کہ یہ جاننا زیادہ حیران کن ہوگا کہ حتمی جواب دلکش ہے، مثال کے طور پر۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here, and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits. ",
  "translatedText": "لہذا جب آپ اصل میں یہاں کیلکولیشن کرتے ہیں، اور آپ ہر وقوعہ کے اوقات کے امکان کو متعلقہ معلومات میں شامل کرتے ہیں، جو آپ کو ملتا ہے وہ 2 ہے۔11 بٹس۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "I'm just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it. ",
  "translatedText": "میں صرف یہ کہہ رہا ہوں، یہ بنیادی طور پر دو بٹس ہیں، بنیادی طور پر وہ چار امکانات، لیکن ان تمام انتہائی غیر متوقع واقعات کی وجہ سے کچھ زیادہ ہی غیر یقینی صورتحال ہے، حالانکہ اگر آپ انہیں سیکھ لیتے تو آپ کو اس سے بہت سی معلومات حاصل ہوتیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson. ",
  "translatedText": "لہذا زوم آؤٹ کرتے ہوئے، یہ اس چیز کا حصہ ہے جو Wordle کو معلوماتی تھیوری کے سبق کے لیے ایک عمدہ مثال بناتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy. ",
  "translatedText": "ہمارے پاس اینٹروپی کے لیے یہ دو الگ الگ احساس ایپلی کیشنز ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words that we have possible. ",
  "translatedText": "پہلا یہ بتاتا ہے کہ ہمیں دیئے گئے اندازے سے متوقع معلومات کیا حاصل ہوں گی، اور دوسرا یہ کہتا ہے کہ کیا ہم ان تمام الفاظ کے درمیان باقی ماندہ غیر یقینی کی پیمائش کر سکتے ہیں جو ہمارے پاس ممکن ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation. ",
  "translatedText": "اور مجھے اس بات پر زور دینا چاہیے کہ پہلی صورت میں جہاں ہم ایک اندازے کی متوقع معلومات کو دیکھ رہے ہیں، ایک بار جب ہمارے پاس الفاظ کا غیر مساوی وزن ہوتا ہے، جو انٹراپی کیلکولیشن کو متاثر کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words. ",
  "translatedText": "مثال کے طور پر، میں وہی کیس اٹھاتا ہوں جو ہم پہلے Weary سے وابستہ تقسیم کو دیکھ رہے تھے، لیکن اس بار تمام ممکنہ الفاظ میں غیر یکساں تقسیم کا استعمال کرتے ہوئے تو مجھے دیکھنے دو کہ کیا مجھے یہاں کوئی ایسا حصہ مل سکتا ہے جو اسے اچھی طرح سے واضح کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here this is pretty good. ",
  "translatedText": "ٹھیک ہے، یہاں یہ بہت اچھا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it. ",
  "translatedText": "یہاں ہمارے پاس دو ملحقہ نمونے ہیں جن کے بارے میں یکساں امکان ہے، لیکن ان میں سے ایک کے بارے میں ہمیں بتایا گیا ہے کہ 32 ممکنہ الفاظ ہیں جو اس سے مماثل ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them. ",
  "translatedText": "اور اگر ہم چیک کرتے ہیں کہ وہ کیا ہیں، تو یہ وہ 32 ہیں، جو کہ سب کے سب بہت ہی غیر امکان والے الفاظ ہیں جب آپ ان پر اپنی آنکھیں دیکھتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches, so a quarter as many matches, but it's about as likely. ",
  "translatedText": "ایسا کوئی بھی تلاش کرنا مشکل ہے جو قابل فہم جوابات کی طرح محسوس کرتا ہو، شاید چیختا ہو، لیکن اگر ہم تقسیم میں پڑوسی پیٹرن کو دیکھیں، جس کا امکان تقریباً سمجھا جاتا ہے، تو ہمیں بتایا جاتا ہے کہ اس کے صرف 8 ممکنہ میچز ہیں، اس لیے ایک چوتھائی بہت سے میچ، لیکن اس کا امکان اتنا ہی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why. ",
  "translatedText": "اور جب ہم ان میچوں کو کھینچتے ہیں تو ہم دیکھ سکتے ہیں کہ کیوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers, like ring, or wrath, or raps. ",
  "translatedText": "ان میں سے کچھ حقیقی معقول جوابات ہیں، جیسے انگوٹھی، یا غصہ، یا ریپس۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version 2 of the Wordlebot here, and there are two or three main differences from the first one that we saw. ",
  "translatedText": "یہ بتانے کے لیے کہ ہم ان سب کو کیسے شامل کرتے ہیں، میں یہاں Wordlebot کے ورژن 2 کو کھینچتا ہوں، اور پہلے سے جو ہم نے دیکھا اس میں دو یا تین اہم فرق ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer. ",
  "translatedText": "سب سے پہلے، جیسا کہ میں نے ابھی کہا، جس طرح سے ہم ان انٹراپیوں کو، معلومات کی ان متوقع قدروں کی گنتی کر رہے ہیں، اب ان نمونوں میں زیادہ بہتر تقسیم کا استعمال کر رہے ہیں جس میں اس امکان کو شامل کیا گیا ہے کہ ایک دیا ہوا لفظ درحقیقت جواب ہو گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number 1, though the ones following are a bit different. ",
  "translatedText": "جیسا کہ یہ ہوتا ہے، آنسو اب بھی نمبر 1 ہیں، حالانکہ اس کے بعد آنے والے کچھ مختلف ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table. ",
  "translatedText": "دوسرا، جب یہ اپنے سب سے اوپر انتخاب کی درجہ بندی کرتا ہے، تو اب یہ اس امکان کا ایک نمونہ رکھنے جا رہا ہے کہ ہر لفظ ہی اصل جواب ہے، اور یہ اسے اپنے فیصلے میں شامل کر لے گا، جو ہمارے پاس کچھ اندازے لگانے کے بعد دیکھنا آسان ہو جائے گا۔ٹیبل. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives. ",
  "translatedText": "ایک بار پھر، اس کی سفارش کو نظر انداز کرنا کیونکہ ہم مشینوں کو اپنی زندگی پر حکمرانی نہیں کرنے دے سکتے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches. ",
  "translatedText": "اور مجھے لگتا ہے کہ مجھے ایک اور چیز کا ذکر کرنا چاہئے جو یہاں بائیں طرف ختم ہو گئی ہے، وہ غیر یقینی کی قدر، بٹس کی وہ تعداد، اب صرف ممکنہ میچوں کی تعداد کے ساتھ بے کار نہیں ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which is a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes. ",
  "translatedText": "اب اگر ہم اسے اوپر کھینچیں اور 2 سے 8 کا حساب لگائیں۔02، جو کہ 256 سے تھوڑا اوپر ہے، میرا اندازہ ہے کہ 259، یہ جو کہہ رہا ہے وہ یہ ہے کہ 526 کل الفاظ ہیں جو حقیقت میں اس پیٹرن سے مماثل ہیں، اس میں جتنی غیر یقینی صورتحال ہے وہ اس سے زیادہ مماثل ہے کہ اگر 259 یکساں امکانات ہوتے تو یہ کیا ہوتا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this. ",
  "translatedText": "نتائج آپ اسے اس طرح سوچ سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borx is not the answer, same with yorts and zorl and zorus, so it's a little less uncertain than it was in the previous case. ",
  "translatedText": "یہ جانتا ہے کہ بورکس جواب نہیں ہے، جو yorts اور zorl اور zorus کے ساتھ ہے، لہذا یہ پچھلے معاملے کے مقابلے میں تھوڑا کم غیر یقینی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller. ",
  "translatedText": "بٹس کی یہ تعداد چھوٹی ہوگی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here. ",
  "translatedText": "اور اگر میں گیم کھیلتا رہتا ہوں، تو میں اسے کچھ اندازوں کے ساتھ بہتر کر رہا ہوں جو کہ میں یہاں بیان کرنا چاہوں گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy. ",
  "translatedText": "چوتھے اندازے سے، اگر آپ اس کے سب سے اوپر کے انتخاب پر نظر ڈالیں، تو آپ دیکھ سکتے ہیں کہ یہ اب صرف اینٹروپی کو زیادہ سے زیادہ نہیں کر رہا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words. ",
  "translatedText": "تو اس مقام پر، تکنیکی طور پر سات امکانات ہیں، لیکن معنی خیز موقع کے ساتھ صرف چھاترالی اور الفاظ ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values, that strictly speaking would give more information. ",
  "translatedText": "اور آپ دیکھ سکتے ہیں کہ یہ ان تمام دیگر اقدار سے اوپر دونوں کو منتخب کرتے ہوئے درجہ بندی کرتا ہے، کہ سختی سے بات کرنے سے مزید معلومات ملے گی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect. ",
  "translatedText": "پہلی بار جب میں نے یہ کیا، میں نے صرف ہر ایک اندازے کے معیار کی پیمائش کرنے کے لیے ان دو نمبروں کو شامل کیا، جو حقیقت میں آپ کے شبہ سے بہتر کام کرتے تھے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic, and I'm sure there's other approaches people could take but here's the one I landed on. ",
  "translatedText": "لیکن یہ واقعی منظم محسوس نہیں ہوا، اور مجھے یقین ہے کہ لوگ دوسرے طریقے بھی اختیار کر سکتے ہیں لیکن یہ وہی ہے جس پر میں اترا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that. ",
  "translatedText": "اگر ہم اگلے اندازے کے امکان پر غور کر رہے ہیں، جیسا کہ اس معاملے کے الفاظ میں، ہمیں واقعی جس چیز کی پرواہ ہے وہ ہمارے کھیل کا متوقع سکور ہے اگر ہم ایسا کرتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to. ",
  "translatedText": "اور اس متوقع سکور کا حساب لگانے کے لیے، ہم یہ کہتے ہیں کہ اس بات کا کیا امکان ہے کہ الفاظ ہی اصل جواب ہیں، جو اس وقت یہ 58% سے بیان کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be 4. ",
  "translatedText": "ہم 58% موقع کے ساتھ کہتے ہیں، اس گیم میں ہمارا اسکور 4 ہوگا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of 1 minus that 58%, our score will be more than that 4. ",
  "translatedText": "اور پھر 1 مائنس کے امکان کے ساتھ کہ 58%، ہمارا سکور اس 4 سے زیادہ ہو جائے گا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point. ",
  "translatedText": "ہم مزید کتنا نہیں جانتے ہیں، لیکن ہم اس بات کی بنیاد پر اندازہ لگا سکتے ہیں کہ اس مقام تک پہنچنے کے بعد کتنی غیر یقینی صورتحال ہو سکتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment there's 1.44 bits of uncertainty. ",
  "translatedText": "خاص طور پر، اس وقت 1 ہے۔غیر یقینی صورتحال کے 44 بٹس۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits. ",
  "translatedText": "اگر ہم الفاظ کا اندازہ لگاتے ہیں، تو یہ ہمیں بتا رہا ہے کہ ہمیں ملنے والی متوقع معلومات 1 ہے۔27 بٹس۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens. ",
  "translatedText": "لہذا اگر ہم الفاظ کا اندازہ لگاتے ہیں، تو یہ فرق اس بات کی نمائندگی کرتا ہے کہ اس کے ہونے کے بعد ہمارے پاس کتنی غیر یقینی صورتحال باقی رہ جائے گی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score. ",
  "translatedText": "ہمیں کسی قسم کی فنکشن کی ضرورت ہے، جسے میں یہاں f کہہ رہا ہوں، جو اس غیر یقینی صورتحال کو متوقع اسکور کے ساتھ جوڑتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version 1 of the bot to say hey what was the actual score after various points with certain very measurable amounts of uncertainty. ",
  "translatedText": "اور اس کے بارے میں جس طرح سے گزرا وہ یہ تھا کہ بوٹ کے ورژن 1 کی بنیاد پر پچھلے گیمز کے اعداد و شمار کا ایک گچھا پلاٹ کیا جائے تاکہ یہ کہا جا سکے کہ ارے مختلف پوائنٹس کے بعد اصل اسکور کیا تھا جس میں کچھ بہت ہی قابل پیمائش مقدار میں غیر یقینی صورتحال تھی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer. ",
  "translatedText": "مثال کے طور پر، یہاں یہ ڈیٹا پوائنٹس جو ایک قدر کے اوپر بیٹھے ہیں جو تقریباً 8 کی طرح ہے۔7 یا اس سے کچھ گیمز کے لیے ایک پوائنٹ کے بعد کہہ رہے ہیں جس پر 8 تھے۔غیر یقینی صورتحال کے 7 بٹس، حتمی جواب حاصل کرنے میں دو اندازے لگے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games it took three guesses, for other games it took four guesses. ",
  "translatedText": "دوسرے گیمز کے لیے اس نے تین اندازے لگائے، دوسرے گیمز کے لیے اس نے چار اندازے لگائے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring. ",
  "translatedText": "اگر ہم یہاں بائیں طرف شفٹ ہوتے ہیں، تو صفر سے زیادہ تمام پوائنٹس کہہ رہے ہیں جب بھی غیر یقینی صورتحال کے صفر بٹس ہوتے ہیں، جس کا مطلب یہ ہے کہ صرف ایک امکان ہے، پھر مطلوبہ اندازوں کی تعداد ہمیشہ صرف ایک ہوتی ہے، جو کہ یقین دہانی کراتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses. ",
  "translatedText": "جب بھی تھوڑی سی غیر یقینی صورت حال تھی، یعنی یہ بنیادی طور پر صرف دو امکانات تک تھی، پھر کبھی کبھی اسے ایک اور اندازہ کی ضرورت ہوتی تھی، کبھی کبھی اسے دو مزید اندازوں کی ضرورت ہوتی تھی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1623.02
 },
 {
  "input": "And so on and so forth here. ",
  "translatedText": "اور اسی طرح یہاں اور آگے. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1623.08,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages. ",
  "translatedText": "ہوسکتا ہے کہ اس ڈیٹا کو دیکھنے کا قدرے آسان طریقہ یہ ہے کہ اسے ایک ساتھ جمع کیا جائے اور اوسط لیا جائے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example this bar here saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5. ",
  "translatedText": "مثال کے طور پر یہاں یہ بار ان تمام نکات کے درمیان کہہ رہا ہے جہاں ہمارے پاس تھوڑی سی غیر یقینی صورتحال تھی، اوسطاً نئے اندازوں کی تعداد تقریباً 1 تھی۔5۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward. ",
  "translatedText": "اور یہاں تمام مختلف کھیلوں کے درمیان یہ کہنا کہ جہاں کسی وقت غیر یقینی صورتحال چار بٹس سے تھوڑی اوپر تھی، جو کہ اسے 16 مختلف امکانات تک محدود کرنے کے مترادف ہے، پھر اوسطاً اسے اس مقام سے دو اندازوں سے کچھ زیادہ کی ضرورت ہے۔آگے. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this. ",
  "translatedText": "اور یہاں سے میں نے صرف اس فنکشن کو فٹ کرنے کے لئے ایک رجعت کیا جو اس کے لئے مناسب لگتا تھا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be. ",
  "translatedText": "اور یاد رکھیں کہ اس میں سے کوئی بھی کام کرنے کا پورا نکتہ یہ ہے کہ ہم اس وجدان کا اندازہ لگا سکیں کہ ہم کسی لفظ سے جتنی زیادہ معلومات حاصل کریں گے، متوقع اسکور اتنا ہی کم ہوگا۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So with this as version 2.0, if we go back and we run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do? ",
  "translatedText": "تو اس کے ساتھ بطور ورژن 2۔0، اگر ہم واپس جاتے ہیں اور ہم سمیولیشن کے ایک ہی سیٹ کو چلاتے ہیں، اس کے تمام 2315 ممکنہ لفظی جوابات کے خلاف کھیلتے ہیں، یہ کیسے ہوتا ہے؟ ٹھیک ہے ہمارے پہلے ورژن کے برعکس یہ یقینی طور پر بہتر ہے، جو یقین دہانی کر رہا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version it's definitely better, which is reassuring. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done the average is around 3.6, although unlike the first version there are a couple times that it loses and requires more than six in this circumstance. ",
  "translatedText": "سبھی نے کہا اور کیا اوسط 3 کے قریب ہے۔6، اگرچہ پہلے ورژن کے برعکس ایک دو بار ایسا ہوتا ہے کہ یہ ہار جاتا ہے اور اس صورت حال میں اسے چھ سے زیادہ کی ضرورت ہوتی ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information. ",
  "translatedText": "غالباً اس لیے کہ ایسے اوقات ہوتے ہیں جب وہ معلومات کو زیادہ سے زیادہ کرنے کی بجائے اصل میں مقصد کے حصول کے لیے اس تجارت کو بناتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6? ",
  "translatedText": "تو کیا ہم 3 سے بہتر کر سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can. ",
  "translatedText": "6؟ ہم ضرور کر سکتے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model. ",
  "translatedText": "اب میں نے شروع میں کہا کہ ورڈل جوابات کی صحیح فہرست کو اس انداز میں شامل نہ کرنے کی کوشش کرنا سب سے زیادہ مزہ آتا ہے جس طرح وہ اپنا ماڈل بناتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43. ",
  "translatedText": "لیکن اگر ہم اسے شامل کرتے ہیں تو، مجھے جو بہترین کارکردگی مل سکتی تھی وہ 3 کے قریب تھی۔43. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that. ",
  "translatedText": "لہذا اگر ہم اس پیشگی تقسیم کو منتخب کرنے کے لیے صرف لفظ فریکوئنسی ڈیٹا کو استعمال کرنے سے زیادہ نفیس حاصل کرنے کی کوشش کرتے ہیں، تو یہ 3۔43 شاید زیادہ سے زیادہ بتاتا ہے کہ ہم اس کے ساتھ کتنا اچھا حاصل کرسکتے ہیں، یا کم از کم میں اس کے ساتھ کتنا اچھا حاصل کرسکتا ہوں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one. ",
  "translatedText": "وہ بہترین کارکردگی بنیادی طور پر صرف ان خیالات کا استعمال کرتی ہے جن کے بارے میں میں یہاں بات کر رہا ہوں، لیکن یہ تھوڑا آگے جاتا ہے، جیسے کہ یہ متوقع معلومات کی تلاش صرف ایک کے بجائے دو قدم آگے کرتا ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is. ",
  "translatedText": "اصل میں میں اس کے بارے میں مزید بات کرنے کا ارادہ کر رہا تھا، لیکن مجھے احساس ہے کہ ہم حقیقت میں کافی لمبا ہو چکے ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least it's looking like Crane is the best opener. ",
  "translatedText": "ایک چیز جو میں کہوں گا وہ یہ ہے کہ یہ دو قدمی تلاش کرنے کے بعد اور پھر سرفہرست امیدواروں میں ایک جوڑے کے نمونے کی نقلیں چلانا، اب تک میرے لیے کم از کم ایسا لگ رہا ہے کہ کرین بہترین اوپنر ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed? ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits. ",
  "translatedText": "کس نے اندازہ لگایا ہوگا؟ اس کے علاوہ اگر آپ اپنے امکانات کی جگہ کا تعین کرنے کے لیے حقیقی لفظی فہرست کا استعمال کرتے ہیں، تو آپ جس غیر یقینی صورتحال سے شروع کرتے ہیں وہ 11 بٹس سے کچھ زیادہ ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out, just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits. ",
  "translatedText": "اور یہ پتہ چلتا ہے، صرف ایک وحشیانہ طاقت کی تلاش سے، پہلے دو اندازوں کے بعد زیادہ سے زیادہ متوقع معلومات تقریباً 10 بٹس ہیں۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty. ",
  "translatedText": "جس سے پتہ چلتا ہے کہ بہترین صورت حال، آپ کے پہلے دو اندازوں کے بعد، بالکل بہترین کھیل کے ساتھ، آپ کو تقریباً ایک غیر یقینی صورتحال باقی رہ جائے گی۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses. ",
  "translatedText": "جو کہ دو ممکنہ اندازوں کے نیچے ہونے کے مترادف ہے۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "So I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as 3, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail. ",
  "translatedText": "تو میرے خیال میں یہ کہنا مناسب اور شاید کافی قدامت پسند ہے کہ آپ کبھی بھی ایسا الگورتھم نہیں لکھ سکتے جس کی اوسط 3 سے کم ہو، کیونکہ آپ کے لیے دستیاب الفاظ کے ساتھ، صرف دو قدموں کے بعد کافی معلومات حاصل کرنے کی گنجائش نہیں ہے۔بغیر کسی ناکامی کے ہر ایک بار تیسرے سلاٹ میں جواب کی ضمانت دینے کے قابل۔",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]