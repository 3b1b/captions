[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy.",
  "translatedText": "Игра Wurdle стала довольно вирусной за последние месяц или два, и, никогда не упуская возможности провести урок математики, мне приходит в голову, что эта игра может служить очень хорошим центральным примером в уроке по теории информации, и в частности тема, известная как энтропия.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could.",
  "translatedText": "Видите ли, как и многие люди, я был втянут в эту головоломку, и, как и многие программисты, я также был втянут в попытки написать алгоритм, который будет вести игру настолько оптимально, насколько это возможно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy.",
  "translatedText": "И я решил здесь просто обсудить с вами часть моего процесса и объяснить часть математических расчетов, которые в него вошли, поскольку весь алгоритм основан на идее энтропии.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle?",
  "translatedText": "Перво-наперво, если вы еще об этом не слышали, что такое Wurdle?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us.",
  "translatedText": "И чтобы убить двух зайцев одним выстрелом, пока мы изучаем правила игры, позвольте мне также просмотреть, чего мы собираемся добиться, а именно разработать небольшой алгоритм, который, по сути, будет играть за нас.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does.",
  "translatedText": "Хоть я и не участвовал в сегодняшнем Wurdle, сегодня 4 февраля, и посмотрим, как справится бот.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess.",
  "translatedText": "Цель Wurdle — угадать загадочное слово из пяти букв, и вам дается шесть разных шансов угадать.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane.",
  "translatedText": "Например, мой бот Wurdle предлагает мне начать с угадывающего крана.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer.",
  "translatedText": "Каждый раз, когда вы делаете предположение, вы получаете некоторую информацию о том, насколько ваше предположение близко к истинному ответу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer.",
  "translatedText": "Здесь серый прямоугольник говорит мне, что в реальном ответе нет буквы C.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position.",
  "translatedText": "Желтый квадрат сообщает мне, что есть буква R, но она не в этом положении.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position.",
  "translatedText": "Зеленый квадрат сообщает мне, что в секретном слове есть буква А и она находится на третьей позиции.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E.",
  "translatedText": "И тогда нет ни Н, ни Е.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information.",
  "translatedText": "Так что позвольте мне просто войти и сообщить эту информацию боту Wurdle.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey.",
  "translatedText": "Начали с журавля, получили серый, желтый, зеленый, серый, серый.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time.",
  "translatedText": "Не беспокойтесь обо всех данных, которые он показывает прямо сейчас, я объясню это в свое время.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick.",
  "translatedText": "Но главный совет для нашего второго выбора — это ерунда.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess.",
  "translatedText": "И ваше предположение действительно должно состоять из пяти букв, но, как вы увидите, оно довольно либерально в отношении того, что оно на самом деле позволит вам угадать.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick.",
  "translatedText": "В этом случае мы пробуем фишку.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good.",
  "translatedText": "И ладно, дела обстоят неплохо.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R.",
  "translatedText": "Мы нажимаем S и H, поэтому мы знаем первые три буквы, мы знаем, что есть R.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something.",
  "translatedText": "И это будет похоже на SHA что-то R или SHA R что-то.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp.",
  "translatedText": "И похоже, что бот Wurdle знает, что есть всего два варианта: осколочный или острый.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a toss up between them at this point, so I guess probably just because it's alphabetical it goes with shard.",
  "translatedText": "На данный момент между ними возникает своего рода спор, так что я думаю, что, вероятно, просто потому, что это алфавитный порядок, это соответствует осколку.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer.",
  "translatedText": "Какое ура, это настоящий ответ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.22,
  "end": 132.86
 },
 {
  "input": "So we got it in three.",
  "translatedText": "Итак, мы получили его за три.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 132.96,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle four is par and three is birdie.",
  "translatedText": "Если вам интересно, хорошо ли это, то я услышал от одного человека фразу: для Уёрдла четыре — это номинал, а три — птичка.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy.",
  "translatedText": "Я думаю, это довольно удачная аналогия.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy.",
  "translatedText": "Чтобы получить четыре, вам нужно постоянно работать над своей игрой, но это, конечно, не безумие.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great.",
  "translatedText": "Но когда ты получаешь это за три, это просто здорово.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot.",
  "translatedText": "Так что, если вы не против, то я хотел бы с самого начала просто рассказать о своем мыслительном процессе о том, как я подхожу к боту Wurdle.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson.",
  "translatedText": "И, как я уже сказал, на самом деле это повод для урока теории информации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy.",
  "translatedText": "Основная цель — объяснить, что такое информация и что такое энтропия.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language.",
  "translatedText": "Моей первой мыслью при подходе к этому было взглянуть на относительную частоту употребления различных букв в английском языке.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters?",
  "translatedText": "Итак, я подумал: ладно, есть ли начальная догадка или пара начальных догадок, которая соответствует многим из этих наиболее часто встречающихся букв?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails.",
  "translatedText": "И мне очень понравилось делать другое, а затем гвозди.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good.",
  "translatedText": "Идея в том, что если вы нажмете на букву, вы получите зеленый или желтый цвет, это всегда приятно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.76,
  "end": 187.52
 },
 {
  "input": "It feels like you're getting information.",
  "translatedText": "Такое ощущение, что ты получаешь информацию.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.52,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information since it's pretty rare to find a word that doesn't have any of these letters.",
  "translatedText": "Но в этих случаях, даже если вы не попадаете и всегда получаете серый цвет, это все равно дает вам много информации, поскольку довольно редко можно найти слово, в котором нет ни одной из этих букв.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters.",
  "translatedText": "Но даже несмотря на это, это не кажется суперсистематическим, потому что, например, порядок букв не учитывается.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail?",
  "translatedText": "Зачем печатать гвозди, если можно печатать улитку?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end?",
  "translatedText": "Лучше ли иметь букву S в конце?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure.",
  "translatedText": "Я не совсем уверен.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y.",
  "translatedText": "Мой друг сказал, что ему нравится начинать со слова «усталый», что меня немного удивило, потому что в нем есть несколько необычных букв, таких как W и Y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener.",
  "translatedText": "Но кто знает, может быть, это лучший дебют.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess?",
  "translatedText": "Есть ли какая-то количественная оценка, по которой мы можем судить о качестве потенциального предположения?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up.",
  "translatedText": "Теперь, чтобы настроить способ ранжирования возможных предположений, давайте вернемся и добавим немного ясности в то, как именно устроена игра.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long.",
  "translatedText": "Итак, есть список слов, которые он позволит вам ввести и которые считаются действительными догадками, длиной всего около 13 000 слов.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble.",
  "translatedText": "Но когда вы посмотрите на это, то увидите много действительно необычных вещей, таких как голова или Али и ARG, слова, которые вызывают семейные споры в игре в «Эрудит».",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word.",
  "translatedText": "Но суть игры в том, что ответом всегда будет достаточно обычное слово.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers.",
  "translatedText": "И на самом деле, есть еще один список из примерно 2300 слов, которые являются возможными ответами.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creator's girlfriend, which is kind of fun.",
  "translatedText": "И это список, составленный людьми, я думаю, конкретно девушкой создателя игры, и это довольно забавно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving Wordle that doesn't incorporate previous knowledge about this list.",
  "translatedText": "Но что я хотел бы сделать, так это то, что наша задача в этом проекте состоит в том, чтобы посмотреть, сможем ли мы написать программу, решающую Wordle, которая не будет включать предыдущие знания об этом списке.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list.",
  "translatedText": "Во-первых, существует множество довольно распространенных слов из пяти букв, которых вы не найдете в этом списке.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play Wordle against anyone, not just what happens to be the official website.",
  "translatedText": "Поэтому было бы лучше написать программу, которая была бы немного более устойчивой и могла бы играть в Wordle против кого угодно, а не только против того, что является официальным сайтом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also the reason that we know what this list of possible answers is, is because it's visible in the source code.",
  "translatedText": "А также причина, по которой мы знаем, что представляет собой этот список возможных ответов, заключается в том, что он виден в исходном коде.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day.",
  "translatedText": "Но в исходном коде это отображается в определенном порядке, в котором ответы появляются изо дня в день.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 292.8
 },
 {
  "input": "So you could always just look up what tomorrow's answer will be.",
  "translatedText": "Так что вы всегда можете просто посмотреть, каким будет завтрашний ответ.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.06,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating.",
  "translatedText": "Итак, очевидно, что в некотором смысле использование списка является мошенничеством.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words.",
  "translatedText": "И что делает головоломку более интересной и более насыщенный урок теории информации, так это использование вместо этого некоторых более универсальных данных, таких как относительная частота слов в целом, чтобы уловить интуитивное ощущение предпочтения более распространенных слов.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess?",
  "translatedText": "Итак, из этих 13 000 возможностей, как нам выбрать первую догадку?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality?",
  "translatedText": "Например, если мой друг предлагает усталость, как нам следует проанализировать ее качество?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W.",
  "translatedText": "Ну, причина, по которой он сказал, что ему нравится эта маловероятная W, заключается в том, что ему нравится дальновидность того, насколько приятно чувствовать себя, если вы действительно нажмете эту W.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern.",
  "translatedText": "Например, если первая выявленная закономерность была примерно такой, то в этом гигантском словаре всего 58 слов, соответствующих этой закономерности.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000.",
  "translatedText": "Так что это огромное сокращение по сравнению с 13 000.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this.",
  "translatedText": "Но обратной стороной этого, конечно же, является то, что такой узор встречается очень редко.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000.",
  "translatedText": "В частности, если бы каждое слово с одинаковой вероятностью было ответом, вероятность попадания в этот шаблон была бы 58, разделенная примерно на 13 000.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers.",
  "translatedText": "Конечно, они не в равной степени могут быть ответами.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words.",
  "translatedText": "Большинство из них – очень неясные и даже сомнительные слова.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later.",
  "translatedText": "Но, по крайней мере, для нашего первого этапа, давайте предположим, что все они одинаково вероятны, а затем уточним это чуть позже.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is the pattern with a lot of information is by its very nature unlikely to occur.",
  "translatedText": "Дело в том, что паттерн с большим количеством информации по своей природе маловероятен.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely.",
  "translatedText": "На самом деле, быть информативным означает то, что это маловероятно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where of course there's not a W in it.",
  "translatedText": "Гораздо более вероятной моделью, которую можно увидеть в этом открытии, было бы что-то вроде этого, где, конечно, нет буквы W.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y.",
  "translatedText": "Может быть, есть E, а может быть, нет A, нет R, нет Y.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches.",
  "translatedText": "В этом случае существует 1400 возможных совпадений.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see.",
  "translatedText": "Если бы все были одинаково вероятны, вероятность того, что вы увидите именно такую модель, составила бы около 11%.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative.",
  "translatedText": "Таким образом, наиболее вероятные результаты являются и наименее информативными.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see.",
  "translatedText": "Чтобы получить более глобальное представление, позвольте мне показать вам полное распределение вероятностей по всем различным закономерностям, которые вы можете увидеть.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common.",
  "translatedText": "Таким образом, каждая полоса, на которую вы смотрите, соответствует возможному набору цветов, которые могут быть обнаружены, из которых существует от 3 до 5 возможностей, и они расположены слева направо, от наиболее распространенного до наименее распространенного.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays.",
  "translatedText": "Таким образом, наиболее распространенной возможностью здесь является то, что вы получите все серые цвета.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time.",
  "translatedText": "Это происходит примерно в 14% случаев.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here where there's only 18 possibilities for what matches this pattern that evidently look like this.",
  "translatedText": "И когда вы делаете предположение, вы надеетесь на то, что окажетесь где-то в этом длинном хвосте, как здесь, где есть только 18 возможностей для того, что соответствует этому шаблону, который, очевидно, выглядит так.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here.",
  "translatedText": "Или, если мы рискнем пойти немного левее, знаете, может быть, мы пройдем весь путь сюда.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you.",
  "translatedText": "Хорошо, вот вам хорошая головоломка.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them?",
  "translatedText": "Какие три слова в английском языке начинаются с буквы W, заканчиваются на Y и где-то в них есть буква R?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly.",
  "translatedText": "Оказывается, ответы, посмотрим, многословные, червивые и кривые.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution.",
  "translatedText": "Итак, чтобы судить, насколько хорошо это слово в целом, нам нужна какая-то мера ожидаемого объема информации, которую вы собираетесь получить от этого распределения.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score.",
  "translatedText": "Если мы пройдемся по каждому шаблону и умножим вероятность его появления на что-то, что измеряет, насколько он информативен, это, возможно, может дать нам объективную оценку.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches.",
  "translatedText": "Теперь вашим первым инстинктом того, каким должно быть это что-то, может быть количество совпадений.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches.",
  "translatedText": "Вам нужно меньшее среднее количество совпадений.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer.",
  "translatedText": "Но вместо этого я хотел бы использовать более универсальное измерение, которое мы часто приписываем информации, и которое станет более гибким, когда каждому из этих 13 000 слов будет назначена разная вероятность того, являются ли они на самом деле ответом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but it's really intuitive if we just look at examples.",
  "translatedText": "Стандартной единицей информации является бит, формула которого немного забавна, но она действительно интуитивно понятна, если мы просто посмотрим на примеры.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information.",
  "translatedText": "Если у вас есть наблюдение, которое сокращает ваше пространство возможностей вдвое, мы говорим, что оно содержит один бит информации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about Half of the five letter words have an S, a little less than that, but about half.",
  "translatedText": "В нашем примере пространство возможностей — это все возможные слова, и получается, что примерно половина из пятибуквенных слов имеет букву S, чуть меньше, но примерно половина.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information.",
  "translatedText": "Таким образом, это наблюдение даст вам один бит информации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information.",
  "translatedText": "Если вместо этого новый факт сокращает это пространство возможностей в четыре раза, мы говорим, что он содержит два бита информации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T.",
  "translatedText": "Например, оказывается, что около четверти этих слов имеют букву Т.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth.",
  "translatedText": "Если наблюдение сокращает это пространство в восемь раз, мы говорим, что это три бита информации, и так далее, и тому подобное.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a 16th, five bits cuts it into a 32nd.",
  "translatedText": "Четыре бита превращают его в 16-й, пять битов — в 32-й.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.52
 },
 {
  "input": "So now you might want to pause and ask yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence?",
  "translatedText": "Итак, теперь вы, возможно, захотите сделать паузу и спросить себя, какова формула информации о количестве битов с точки зрения вероятности возникновения события?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 533.52,
  "end": 542.98
 },
 {
  "input": "What we're saying here is that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability.",
  "translatedText": "Мы здесь говорим о том, что когда вы принимаете половину числа битов, это то же самое, что и вероятность, а это то же самое, что сказать, что двойка в степени числа битов равна единице по сравнению с вероятностью, что далее перестраивается так, что информация представляет собой логарифм по основанию два из одного, разделенный на вероятность.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still, where the information is the negative log base two of the probability.",
  "translatedText": "И иногда вы видите это с еще одной перестановкой, где информация представляет собой отрицательный логарифм по основанию два вероятности.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half.",
  "translatedText": "Выраженный таким образом, это может показаться немного странным для непосвященных, но на самом деле это просто очень интуитивная идея спросить, сколько раз вы сократили свои возможности вдвое.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture?",
  "translatedText": "Теперь, если вам интересно, знаете, я думал, что мы просто играем в забавную словесную игру, почему на картинке появляются логарифмы?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095.",
  "translatedText": "Одна из причин, по которой эта единица более удобна, заключается в том, что гораздо проще говорить об очень маловероятных событиях, гораздо проще сказать, что наблюдение содержит 20 бит информации, чем сказать, что вероятность того или иного события равна 0.0000095.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together.",
  "translatedText": "Но более существенная причина того, что это логарифмическое выражение оказалось очень полезным дополнением к теории вероятностей, заключается в том, как информация складывается.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information.",
  "translatedText": "Например, если одно наблюдение дает вам два бита информации, сокращая ваше пространство в четыре раза, а затем второе наблюдение, такое как ваше второе предположение в Wordle, дает вам еще три бита информации, сокращая вас еще в восемь раз, два вместе дают вам пять бит информации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add.",
  "translatedText": "Точно так же, как вероятности любят умножаться, информация любит прибавляться.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with.",
  "translatedText": "Итак, как только мы попадаем в область чего-то вроде ожидаемого значения, где мы складываем кучу чисел, с журналами работать становится намного приятнее.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for Weary and add another little tracker on here, showing us how much information there is for each pattern.",
  "translatedText": "Давайте вернемся к нашему дистрибутиву Weary и добавим сюда еще один небольшой трекер, показывающий, сколько информации содержится для каждого шаблона.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain.",
  "translatedText": "Главное, что я хочу, чтобы вы заметили, это то, что чем выше вероятность того, что мы доберемся до этих более вероятных шаблонов, тем меньше информации, тем меньше битов вы получите.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information, where we go through each pattern, we say how probable is it, and then we multiply that by how many bits of information do we get.",
  "translatedText": "Чтобы измерить качество этого предположения, мы возьмем ожидаемое значение этой информации, пройдемся по каждому шаблону, скажем, насколько он вероятен, а затем умножим это на количество битов информации, которые мы получим.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 643.5,
  "end": 654.06
 },
 {
  "input": "And in the example of Weary, that turns out to be 4.9 bits.",
  "translatedText": "А в примере с Вири это число равно 4.9 бит.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times.",
  "translatedText": "Таким образом, в среднем информация, которую вы получаете из этого начального предположения, так же эффективна, как сокращение пространства ваших возможностей пополам примерно в пять раз.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like Slate.",
  "translatedText": "Напротив, примером предположения с более высокой ожидаемой информационной ценностью может быть что-то вроде Slate.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case you'll notice the distribution looks a lot flatter.",
  "translatedText": "В этом случае вы заметите, что распределение выглядит намного более плоским.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information.",
  "translatedText": "В частности, наиболее вероятное появление всех оттенков серого имеет только около 6% вероятности появления, так что как минимум вы получите очевидно 3.9 бит информации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that.",
  "translatedText": "Но это минимум, чаще всего можно получить что-то получше.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8.",
  "translatedText": "И оказывается, что если подсчитать цифры и сложить все соответствующие термины, то средняя информация составит около 5.8.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with Weary, your space of possibilities will be about half as big after this first guess, on average.",
  "translatedText": "Таким образом, в отличие от Вири, после первого предположения ваше пространство возможностей будет в среднем вдвое меньше.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity.",
  "translatedText": "На самом деле есть забавная история с названием этого ожидаемого значения количества информации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "Information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science.",
  "translatedText": "Теория информации была разработана Клодом Шенноном, который работал в Bell Labs в 1940-х годах, но о некоторых своих еще не опубликованных идеях он говорил с Джоном фон Нейманом, интеллектуальным гигантом того времени, очень выдающимся человеком. по математике и физике и положил начало тому, что впоследствии стало информатикой.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well you should call it entropy, and for two reasons.",
  "translatedText": "И когда он упомянул, что на самом деле у него нет хорошего названия для этого ожидаемого значения количества информации, фон Нейман якобы сказал, что, как гласит история, вы должны называть это энтропией, и по двум причинам.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage.",
  "translatedText": "Во-первых, ваша функция неопределенности использовалась в статистической механике под этим именем, поэтому у нее уже есть имя, а во-вторых, что более важно, никто не знает, что такое энтропия на самом деле, поэтому в дебатах вы всегда будете иметь преимущество.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design.",
  "translatedText": "Так что, если название кажется немного загадочным и если верить этой истории, то это сделано специально.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess.",
  "translatedText": "Кроме того, если вы задаетесь вопросом о его связи со всем этим вторым законом термодинамики из физики, связь определенно существует, но в ее происхождении Шеннон просто имел дело с чистой теорией вероятностей, и для наших целей здесь, когда я использую слово энтропия, я просто хочу, чтобы вы подумали об ожидаемой информационной ценности конкретного предположения.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously.",
  "translatedText": "Вы можете думать об энтропии как об измерении двух вещей одновременно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution.",
  "translatedText": "Во-первых, насколько равномерным является распределение.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be.",
  "translatedText": "Чем ближе распределение к равномерному, тем выше будет энтропия.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy.",
  "translatedText": "В нашем случае, когда общее количество шаблонов составляет от 3 до 5, для равномерного распределения наблюдение любого из них будет иметь базу данных журнала 2 из 3 до 5, что в итоге равно 7.92, так что это абсолютный максимум, который вы можете иметь для этой энтропии.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place.",
  "translatedText": "Но энтропия также является своего рода мерой того, сколько возможностей вообще существует.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits.",
  "translatedText": "Например, если у вас есть какое-то слово, в котором существует только 16 возможных шаблонов, и каждый из них равновероятен, эта энтропия, эта ожидаемая информация, будет равна 4 битам.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits.",
  "translatedText": "Но если у вас есть другое слово, в котором может возникнуть 64 возможных шаблона, и все они одинаково вероятны, тогда энтропия составит 6 бит.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes.",
  "translatedText": "Итак, если вы видите какое-то распределение, энтропия которого равна 6 битам, это как бы говорит о том, что в том, что должно произойти, существует столько же вариаций и неопределенности, как если бы было 64 равновероятных исхода.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this.",
  "translatedText": "Для моего первого использования Wurtelebot я просто сделал это.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the possible guesses you could have, all 13,000 words, computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns you might see, for each one, and picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible.",
  "translatedText": "Он перебирает все возможные предположения, все 13 000 слов, вычисляет энтропию для каждого из них, или, точнее, энтропию распределения по всем шаблонам, которые вы можете увидеть, для каждого из них, и выбирает самое высокое, поскольку это тот, который, скорее всего, максимально сократит ваше пространство возможностей.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses.",
  "translatedText": "И хотя я говорил здесь только о первой догадке, то же самое происходит и со следующими несколькими догадками.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words.",
  "translatedText": "Например, после того, как вы видите некоторый шаблон в этом первом предположении, который ограничит вас меньшим количеством возможных слов в зависимости от того, что с ним совпадает, вы просто играете в ту же игру в отношении этого меньшего набора слов.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy.",
  "translatedText": "Для предлагаемого второго предположения вы смотрите на распределение всех шаблонов, которые могут возникнуть из этого более ограниченного набора слов, вы просматриваете все 13 000 возможностей и находите тот, который максимизирует эту энтропию.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins.",
  "translatedText": "Чтобы показать вам, как это работает в действии, позвольте мне просто привести небольшой вариант написанного мной Вюртеле, в котором на полях показаны основные моменты этого анализа.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "After doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information.",
  "translatedText": "После выполнения всех расчетов энтропии справа здесь показано, какие из них имеют наиболее ожидаемую информацию.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch.",
  "translatedText": "Оказывается, самый популярный ответ, по крайней мере на данный момент, мы уточним это позже, — это Тарес, что означает, хм, конечно, вика, самая обычная вика.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got, given this particular pattern.",
  "translatedText": "Каждый раз, когда мы здесь делаем предположение, что, возможно, я игнорирую его рекомендации и использую шифер, потому что мне нравится шифер, мы можем видеть, сколько ожидаемой информации он содержал, но затем справа от слова здесь показано, сколько реальную информацию, которую мы получили, учитывая эту конкретную закономерность.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that.",
  "translatedText": "Так вот тут похоже нам немного не повезло, от нас ожидали 5.8, но нам удалось получить что-то меньшее.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now.",
  "translatedText": "А затем, слева, здесь показаны все возможные слова, учитывая, где мы сейчас находимся.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment.",
  "translatedText": "Синие полосы сообщают нам, насколько вероятно, по его мнению, каждое слово, поэтому на данный момент он предполагает, что каждое слово встречается с одинаковой вероятностью, но мы уточним это через мгновение.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities.",
  "translatedText": "И затем это измерение неопределенности говорит нам об энтропии этого распределения возможных слов, которое сейчас, поскольку это равномерное распределение, является просто излишне сложным способом подсчета количества возможностей.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities.",
  "translatedText": "Например, если бы мы возвели 2 в 13-ю степень.66, это должно быть около 13 000 возможностей.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "I'm a little bit off here, but only because I'm not showing all the decimal places.",
  "translatedText": "Я здесь немного не так, но только потому, что не показываю все десятичные знаки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute.",
  "translatedText": "На данный момент это может показаться излишним и слишком усложняющим ситуацию, но вы поймете, почему полезно иметь оба числа за минуту.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Ramen, which again just really doesn't feel like a word.",
  "translatedText": "Итак, здесь похоже, что самая высокая энтропия для нашего второго предположения — это Рамен, что опять-таки просто не похоже на слово.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here, I'm going to go ahead and type in Rains.",
  "translatedText": "Итак, чтобы занять здесь моральную позицию, я собираюсь ввести Rains.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky.",
  "translatedText": "И снова похоже, нам немного не повезло.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information.",
  "translatedText": "Мы ожидали 4.3 бита, а мы получили только 3.39 бит информации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities.",
  "translatedText": "Таким образом, мы получаем 55 возможностей.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means.",
  "translatedText": "И здесь, возможно, я просто воспользуюсь тем, что он предлагает, а именно комбо, что бы это ни значило.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And okay, this is actually a good chance for a puzzle.",
  "translatedText": "И ладно, на самом деле это хороший шанс для головоломки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information.",
  "translatedText": "Он говорит нам, что этот шаблон дает нам 4.7 бит информации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty.",
  "translatedText": "Но слева, прежде чем мы увидим эту закономерность, их было 5.78 бит неопределенности.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities?",
  "translatedText": "Итак, в качестве теста для вас: что это значит относительно количества оставшихся возможностей?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well, it means that we're reduced down to one bit of uncertainty, which is the same thing as saying that there's two possible answers.",
  "translatedText": "Ну, это означает, что мы сведены к одной частичке неопределенности, а это то же самое, что сказать, что есть два возможных ответа.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice.",
  "translatedText": "Это выбор 50 на 50.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss.",
  "translatedText": "И отсюда, поскольку мы с вами знаем, какие слова встречаются чаще, мы знаем, что ответ должен быть пропасть.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that.",
  "translatedText": "Но как сейчас написано, программа этого не знает.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it.",
  "translatedText": "Поэтому он просто продолжает работать, пытаясь получить как можно больше информации, пока не останется только одна возможность, а затем он ее угадывает.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy.",
  "translatedText": "Поэтому очевидно, что нам нужна лучшая стратегия эндшпиля.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1042.34
 },
 {
  "input": "But let's say we call this version one of our wordle solver, and then we go and run some simulations to see how it does.",
  "translatedText": "Но предположим, что мы назовем эту версию одним из наших решателей слов, а затем запустим несколько симуляций, чтобы посмотреть, как она работает.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1042.6,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game.",
  "translatedText": "Итак, это работает так: он играет во все возможные словесные игры.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers.",
  "translatedText": "Он проходит через все эти 2315 слов, которые являются реальными ответами.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set.",
  "translatedText": "По сути, он использует это как набор для тестирования.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice.",
  "translatedText": "И с помощью этого наивного метода не учитывать, насколько распространено слово, а просто пытаться максимизировать информацию на каждом этапе пути, пока не дойдет до одного и только одного варианта.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124.",
  "translatedText": "К концу моделирования средний балл составит около 4.124.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expected to do worse.",
  "translatedText": "Что неплохо, если честно, я ожидал худшего результата.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4.",
  "translatedText": "Но люди, играющие в Wordle, скажут вам, что обычно они могут получить это за 4.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can.",
  "translatedText": "Настоящая задача — собрать как можно больше очков из 3.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3.",
  "translatedText": "Это довольно большой скачок между оценкой 4 и оценкой 3.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that.",
  "translatedText": "Очевидный результат здесь — каким-то образом определить, является ли слово распространенным, и как именно мы это делаем.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language.",
  "translatedText": "Я подошел к этому с целью получить список относительных частот всех слов английского языка.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1107.88
 },
 {
  "input": "And I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset.",
  "translatedText": "И я только что использовал функцию данных частоты слов Mathematica, которая сама извлекает данные из общедоступного набора данных Google Books English Ngram.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words.",
  "translatedText": "И на это интересно смотреть, например, если мы отсортируем его от наиболее распространенных слов к наименее распространенным.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5 letter words in the English language.",
  "translatedText": "Очевидно, это самые распространенные слова из 5 букв в английском языке.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common.",
  "translatedText": "Вернее, это 8-е место по распространенности.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there.",
  "translatedText": "Сначала есть что, после чего там и там.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common.",
  "translatedText": "Первое само по себе не первое, а девятое, и имеет смысл, что эти другие слова могут встречаться чаще, где те, что после первого, находятся после, где, и эти слова встречаются немного реже.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency.",
  "translatedText": "Теперь, используя эти данные для моделирования вероятности того, что каждое из этих слов будет окончательным ответом, оно не должно быть просто пропорционально частоте.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1146.34
 },
 {
  "input": "For example, which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely.",
  "translatedText": "Например, которому присвоен балл 0.002 в этом наборе данных, тогда как слово «коса» в каком-то смысле примерно в 1000 раз менее вероятно.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.7,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering.",
  "translatedText": "Но оба эти слова достаточно распространены, поэтому их почти наверняка стоит рассмотреть.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1158.84
 },
 {
  "input": "So we want more of a binary cutoff.",
  "translatedText": "Поэтому нам нужно больше бинарного отсечения.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1159.34,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty.",
  "translatedText": "Я представил, что беру весь этот отсортированный список слов, затем располагаю его по оси X, а затем применяю сигмовидную функцию, которая является стандартным способом получить функцию, вывод которой в основном двоичный, это либо 0, либо 1, но в этой области неопределенности между ними происходит сглаживание.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis.",
  "translatedText": "По сути, вероятность того, что я назначаю каждому слову попадание в окончательный список, будет значением сигмовидной функции, указанной выше, где бы оно ни располагалось на оси X.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff.",
  "translatedText": "Очевидно, это зависит от нескольких параметров, например, насколько широкое пространство на оси X заполняют эти слова, определяет, насколько постепенно или круто мы снижаемся от 1 до 0, а то, где мы располагаем их слева направо, определяет границу.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "To be honest, the way I did this was just licking my finger and sticking it into the wind.",
  "translatedText": "Честно говоря, я просто облизнул палец и высунул его по ветру.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff.",
  "translatedText": "Я просмотрел отсортированный список и попытался найти окно, в котором, посмотрев на него, я понял, что около половины этих слов скорее всего будут окончательным ответом, и использовал его в качестве отсечения.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement.",
  "translatedText": "Как только мы получим такое распределение по словам, это даст нам еще одну ситуацию, когда энтропия становится действительно полезным измерением.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were a feather and nails, and we end up with a situation where there's four possible words that match it.",
  "translatedText": "Например, предположим, что мы играли в игру и начинаем с моих старых открывашек, которыми были перо и гвозди, и заканчиваем ситуацией, когда есть четыре возможных слова, которые ей соответствуют.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely.",
  "translatedText": "И допустим, мы считаем их всех одинаково вероятными.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1235.62
 },
 {
  "input": "Let me ask you, what is the entropy of this distribution?",
  "translatedText": "Позвольте мне спросить вас, какова энтропия этого распределения?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1236.22,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2.",
  "translatedText": "Что ж, информация, связанная с каждой из этих возможностей, будет иметь логарифмическую базу 2 из 4, поскольку каждая из них равна 1 и 4, а это 2.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "Two bits of information, four possibilities.",
  "translatedText": "Два бита информации, четыре возможности.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good.",
  "translatedText": "Все очень хорошо и хорошо.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than four matches?",
  "translatedText": "Но что, если я скажу вам, что на самом деле совпадений больше четырех?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it.",
  "translatedText": "На самом деле, когда мы просматриваем полный список слов, мы видим, что ему соответствуют 16 слов.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure.",
  "translatedText": "Но предположим, что наша модель дает очень низкую вероятность того, что остальные 12 слов действительно станут окончательным ответом, примерно 1 из 1000, потому что они действительно неясны.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution?",
  "translatedText": "Теперь позвольте мне спросить вас, какова энтропия этого распределения?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before.",
  "translatedText": "Если бы энтропия измеряла здесь просто количество совпадений, то можно было бы ожидать, что это будет что-то вроде логарифмической базы 2 из 16, что будет равно 4, то есть на два бита неопределенности больше, чем было раньше.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before.",
  "translatedText": "Но, конечно, фактическая неопределенность на самом деле не сильно отличается от того, что было раньше.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example.",
  "translatedText": "Тот факт, что есть эти 12 действительно непонятных слов, не означает, что было бы еще более удивительно узнать, например, что окончательный ответ — «очарование».",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here, and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits.",
  "translatedText": "Итак, когда вы на самом деле выполняете вычисления здесь и складываете вероятность каждого события, умноженную на соответствующую информацию, вы получаете 2.11 бит.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "I'm just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it.",
  "translatedText": "Я просто говорю, что это по сути два бита, в основном эти четыре возможности, но есть немного больше неопределенности из-за всех этих крайне маловероятных событий, хотя, если бы вы их изучили, вы бы получили из этого массу информации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson.",
  "translatedText": "Уменьшение масштаба — это часть того, что делает Wordle таким хорошим примером для урока теории информации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy.",
  "translatedText": "У нас есть два различных применения чувств к энтропии.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words that we have possible.",
  "translatedText": "Первый говорит нам, какую ожидаемую информацию мы получим в результате данного предположения, а второй говорит, можем ли мы измерить оставшуюся неопределенность среди всех возможных слов.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation.",
  "translatedText": "И я должен подчеркнуть, что в первом случае, когда мы смотрим на ожидаемую информацию предположения, когда у нас есть неравный вес слов, это влияет на вычисление энтропии.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words.",
  "translatedText": "Например, позвольте мне рассмотреть тот же случай, который мы рассматривали ранее, с распределением, связанным с Уири, но на этот раз с использованием неравномерного распределения по всем возможным словам.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well.",
  "translatedText": "Итак, давайте посмотрим, смогу ли я найти здесь часть, которая достаточно хорошо это иллюстрирует.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here this is pretty good.",
  "translatedText": "Ладно, вот это очень хорошо.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it.",
  "translatedText": "Здесь у нас есть два соседних шаблона, которые примерно одинаково вероятны, но один из них, как нам сказали, имеет 32 возможных слова, которые ему соответствуют.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them.",
  "translatedText": "И если мы проверим, что это такое, то это те 32, которые представляют собой просто очень невероятные слова, когда вы просматриваете их глазами.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches, so a quarter as many matches, but it's about as likely.",
  "translatedText": "Трудно найти какие-либо ответы, которые кажутся правдоподобными, возможно, крики, но если мы посмотрим на соседний шаблон в распределении, который считается примерно столь же вероятным, нам скажут, что он имеет только 8 возможных совпадений, то есть четверть как много совпадений, но это примерно одинаково.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why.",
  "translatedText": "И когда мы достанем эти спички, мы поймем, почему.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers, like ring, or wrath, or raps.",
  "translatedText": "Некоторые из них являются вполне правдоподобными ответами, например, «звонок», «гнев» или «стуки».",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version 2 of the Wordlebot here, and there are two or three main differences from the first one that we saw.",
  "translatedText": "Чтобы проиллюстрировать, как мы все это реализуем, позвольте мне открыть здесь вторую версию Wordlebot, и в ней есть два или три основных отличия от первой, которую мы видели.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer.",
  "translatedText": "Во-первых, как я только что сказал, способ, которым мы вычисляем эти энтропии, эти ожидаемые значения информации, теперь использует более точное распределение по шаблонам, которое учитывает вероятность того, что данное слово действительно будет ответом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number 1, though the ones following are a bit different.",
  "translatedText": "Так получилось, что слезы по-прежнему на первом месте, хотя последующие немного другие.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table.",
  "translatedText": "Во-вторых, когда он ранжирует свои лучшие варианты, он теперь будет хранить модель вероятности того, что каждое слово является фактическим ответом, и будет включать это в свое решение, что легче увидеть, если у нас есть несколько предположений по поводу ответа. стол.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives.",
  "translatedText": "Опять же, игнорируя его рекомендации, потому что мы не можем позволить машинам управлять нашей жизнью.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches.",
  "translatedText": "И я полагаю, мне следует упомянуть еще одну вещь, которая здесь слева: это значение неопределенности, это количество битов, больше не просто избыточно по сравнению с количеством возможных совпадений.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which is a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes.",
  "translatedText": "Теперь, если мы поднимем его и посчитаем 2 к 8.02, что немного выше 256, я думаю, 259, он говорит о том, что, несмотря на то, что всего 526 слов, которые на самом деле соответствуют этому шаблону, степень неопределенности, которую он имеет, больше похожа на то, что было бы, если бы было 259 равновероятных результаты.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this.",
  "translatedText": "Вы можете думать об этом так.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borx is not the answer, same with yorts and zorl and zorus, so it's a little less uncertain than it was in the previous case.",
  "translatedText": "Он знает, что боркс — это не ответ, то же самое с йортами, зорлами и зорусами, поэтому его неопределенность немного меньше, чем в предыдущем случае.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller.",
  "translatedText": "Это количество бит будет меньше.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here.",
  "translatedText": "И если я продолжу играть в игру, я уточню это с помощью пары предположений, связанных с тем, что я хотел бы объяснить здесь.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy.",
  "translatedText": "По четвертому предположению, если вы посмотрите на его лучшие варианты, вы увидите, что это уже не просто максимизация энтропии.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words.",
  "translatedText": "Итак, на данный момент технически существует семь возможностей, но единственные, у которых есть значимый шанс, — это общежития и слова.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values, that strictly speaking would give more information.",
  "translatedText": "И вы можете видеть, что выбор обоих из этих значений стоит выше всех остальных значений, которые, строго говоря, дадут больше информации.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect.",
  "translatedText": "В первый раз, когда я это сделал, я просто сложил эти два числа, чтобы измерить качество каждого предположения, и это на самом деле сработало лучше, чем вы могли подумать.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic, and I'm sure there's other approaches people could take but here's the one I landed on.",
  "translatedText": "Но на самом деле это не казалось систематическим, и я уверен, что люди могли бы использовать другие подходы, но я остановился на этом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that.",
  "translatedText": "Если мы рассматриваем перспективу следующей догадки, как в данном случае слов, нас действительно волнует ожидаемый результат нашей игры, если мы это сделаем.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to.",
  "translatedText": "И чтобы вычислить этот ожидаемый балл, мы говорим, какова вероятность того, что слова являются фактическим ответом, который на данный момент описывает 58%.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be 4.",
  "translatedText": "Мы говорим, что с вероятностью 58% наш счет в этой игре будет 4.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of 1 minus that 58%, our score will be more than that 4.",
  "translatedText": "И тогда с вероятностью 1 минус эти 58% наш результат будет больше этих 4.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point.",
  "translatedText": "Насколько больше мы не знаем, но мы можем оценить это, исходя из того, насколько велика неопределенность, вероятно, возникнет, когда мы доберемся до этой точки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment there's 1.44 bits of uncertainty.",
  "translatedText": "Конкретно на данный момент их 1.44 бита неопределенности.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits.",
  "translatedText": "Если мы угадываем слова, это означает, что ожидаемая информация, которую мы получим, равна 1.27 бит.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens.",
  "translatedText": "Итак, если мы угадываем слова, эта разница показывает, сколько неопределенности у нас, вероятно, останется после того, как это произойдет.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score.",
  "translatedText": "Нам нужна некая функция, которую я здесь называю f, которая связывает эту неопределенность с ожидаемым результатом.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version 1 of the bot to say hey what was the actual score after various points with certain very measurable amounts of uncertainty.",
  "translatedText": "И способ, которым это было сделано, заключался в том, чтобы просто построить график данных из предыдущих игр на основе версии 1 бота, чтобы сказать: «Эй, каков был фактический счет после различных точек с определенной, очень измеримой степенью неопределенности».",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer.",
  "translatedText": "Например, эти точки данных находятся выше значения примерно 8.Для некоторых игр говорят «7» или около того после момента, когда их было 8.7 бит неопределенности, чтобы получить окончательный ответ, потребовалось две догадки.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games it took three guesses, for other games it took four guesses.",
  "translatedText": "В других играх требовалось три предположения, в других — четыре предположения.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring.",
  "translatedText": "Если мы сдвинемся здесь влево, все точки выше нуля означают, что всякий раз, когда есть ноль бит неопределенности, то есть есть только одна возможность, тогда количество требуемых предположений всегда будет только одним, что обнадеживает.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses.",
  "translatedText": "Всякий раз, когда была хоть капля неопределенности, то есть, по существу, существовало всего две возможности, иногда требовалось еще одно предположение, иногда требовалось еще два предположения.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1623.02
 },
 {
  "input": "And so on and so forth here.",
  "translatedText": "И так далее и тому подобное здесь.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1623.08,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages.",
  "translatedText": "Возможно, более простой способ визуализировать эти данные — объединить их и взять средние значения.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example this bar here saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5.",
  "translatedText": "Например, эта полоса говорит о том, что среди всех точек, где у нас была одна доля неопределенности, в среднем количество требуемых новых предположений составляло около 1.5.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward.",
  "translatedText": "И вот эта полоска говорит о том, что среди всех разных игр, где в какой-то момент неопределенность была чуть выше четырех бит, что похоже на сужение ее до 16 различных возможностей, то в среднем с этой точки требуется чуть больше двух предположений. вперед.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this.",
  "translatedText": "И отсюда я просто выполнил регрессию, чтобы соответствовать функции, которая показалась мне разумной.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be.",
  "translatedText": "И помните, что весь смысл всего этого заключается в том, чтобы мы могли количественно оценить эту интуицию: чем больше информации мы получаем от слова, тем ниже будет ожидаемая оценка.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So with this as version 2.0, if we go back and we run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do?",
  "translatedText": "Итак, это версия 2.0, если мы вернемся назад и запустим тот же набор симуляций, используя все 2315 возможных словесных ответов, как это произойдет?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version it's definitely better, which is reassuring.",
  "translatedText": "Ну, в отличие от нашей первой версии, она определенно лучше, и это обнадеживает.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done the average is around 3.6, although unlike the first version there are a couple times that it loses and requires more than six in this circumstance.",
  "translatedText": "В целом средний балл составляет около 3.6, хотя в отличие от первой версии есть пару моментов, когда она проигрывает и требует больше шести в данном случае.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information.",
  "translatedText": "Вероятно, потому, что бывают случаи, когда он идет на компромисс, чтобы действительно достичь цели, а не максимизировать информацию.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6?",
  "translatedText": "Так можем ли мы сделать лучше, чем 3?6?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can.",
  "translatedText": "Мы определенно можем.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model.",
  "translatedText": "В начале я сказал, что очень интересно попытаться не включать настоящий список словесных ответов в способ построения своей модели.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43.",
  "translatedText": "Но если мы добавим это, лучший результат, который я мог бы получить, был около 3.43.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that.",
  "translatedText": "Итак, если мы попытаемся пойти более изощренно, чем просто использовать данные о частоте слов, чтобы выбрать это априорное распределение, это 3.43, вероятно, дает максимальную оценку того, насколько хорошо мы могли бы добиться этого, или, по крайней мере, насколько хорошо я мог бы добиться этого.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one.",
  "translatedText": "Эта лучшая производительность, по сути, просто использует идеи, о которых я здесь говорил, но она идет немного дальше, например, она ищет ожидаемую информацию на два шага вперед, а не на один.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is.",
  "translatedText": "Изначально я планировал поговорить об этом подробнее, но понимаю, что на самом деле мы и так зашли слишком далеко.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least it's looking like Crane is the best opener.",
  "translatedText": "Единственное, что я скажу, это то, что после этого двухэтапного поиска, а затем запуска пары выборочных симуляций с лучшими кандидатами, по крайней мере, на данный момент для меня это выглядит так, будто Крейн - лучший дебют.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed?",
  "translatedText": "Кто бы мог подумать?",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits.",
  "translatedText": "Кроме того, если вы используете настоящий список слов для определения пространства своих возможностей, то неопределенность, с которой вы начнете, составит немногим более 11 бит.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out, just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits.",
  "translatedText": "И оказывается, что при простом переборе максимально возможная ожидаемая информация после первых двух предположений составляет около 10 бит.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty.",
  "translatedText": "Это предполагает, что в лучшем случае после первых двух предположений при совершенно оптимальной игре у вас останется примерно одна доля неопределенности.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses.",
  "translatedText": "Это то же самое, что ограничиться двумя возможными предположениями.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "So I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as 3, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail.",
  "translatedText": "Поэтому я думаю, что будет справедливо и, возможно, довольно консервативно сказать, что вы никогда не сможете написать алгоритм, который бы достигал такого низкого среднего значения, как 3, потому что с доступными вам словами просто не хватит места, чтобы получить достаточно информации всего за два шага. способен гарантировать ответ в третьем слоте каждый раз в обязательном порядке.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]