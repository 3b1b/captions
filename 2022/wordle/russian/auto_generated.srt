1
00:00:00,000 --> 00:00:04,040
Игра Wurdle стала довольно вирусной за последние месяц или два, и,

2
00:00:04,040 --> 00:00:07,880
никогда не упуская возможности провести урок математики, мне приходит в голову,

3
00:00:07,880 --> 00:00:12,120
что эта игра может служить очень хорошим центральным примером в уроке

4
00:00:12,120 --> 00:00:13,120
по теории информации, и в частности тема, известная как энтропия.

5
00:00:13,120 --> 00:00:17,120
Видите ли, как и многие люди, я был втянут в эту головоломку,

6
00:00:17,120 --> 00:00:21,200
и, как и многие программисты, я также был втянут в попытки

7
00:00:21,200 --> 00:00:23,200
написать алгоритм, который будет вести игру настолько оптимально, насколько это возможно.

8
00:00:23,200 --> 00:00:26,400
И я решил здесь просто обсудить с вами часть

9
00:00:26,400 --> 00:00:29,980
моего процесса и объяснить часть математических расчетов, которые в

10
00:00:29,980 --> 00:00:32,080
него вошли, поскольку весь алгоритм основан на идее энтропии.

11
00:00:32,080 --> 00:00:42,180
Перво-наперво, если вы еще об этом не слышали, что такое Wurdle?

12
00:00:42,180 --> 00:00:45,380
И чтобы убить двух зайцев одним выстрелом, пока мы изучаем правила

13
00:00:45,380 --> 00:00:48,980
игры, позвольте мне также просмотреть, чего мы собираемся добиться, а именно

14
00:00:48,980 --> 00:00:51,380
разработать небольшой алгоритм, который, по сути, будет играть за нас.

15
00:00:51,380 --> 00:00:54,860
Хоть я и не участвовал в сегодняшнем Wurdle,

16
00:00:54,860 --> 00:00:55,860
сегодня 4 февраля, и посмотрим, как справится бот.

17
00:00:55,860 --> 00:00:59,580
Цель Wurdle — угадать загадочное слово из пяти

18
00:00:59,580 --> 00:01:00,860
букв, и вам дается шесть разных шансов угадать.

19
00:01:00,860 --> 00:01:05,240
Например, мой бот Wurdle предлагает мне начать с угадывающего крана.

20
00:01:05,240 --> 00:01:09,300
Каждый раз, когда вы делаете предположение, вы получаете некоторую информацию

21
00:01:09,300 --> 00:01:10,940
о том, насколько ваше предположение близко к истинному ответу.

22
00:01:10,940 --> 00:01:14,540
Здесь серый прямоугольник говорит мне, что в реальном ответе нет буквы C.

23
00:01:14,540 --> 00:01:18,340
Желтый квадрат сообщает мне, что есть буква R, но она не в этом положении.

24
00:01:18,340 --> 00:01:21,820
Зеленый квадрат сообщает мне, что в секретном слове есть

25
00:01:21,820 --> 00:01:22,820
буква А и она находится на третьей позиции.

26
00:01:22,820 --> 00:01:24,300
И тогда нет ни Н, ни Е.

27
00:01:24,300 --> 00:01:27,420
Так что позвольте мне просто войти и сообщить эту информацию боту Wurdle.

28
00:01:27,420 --> 00:01:31,500
Начали с журавля, получили серый, желтый, зеленый, серый, серый.

29
00:01:31,500 --> 00:01:35,460
Не беспокойтесь обо всех данных, которые он показывает прямо сейчас, я объясню это в свое время.

30
00:01:35,460 --> 00:01:39,700
Но главный совет для нашего второго выбора — это ерунда.

31
00:01:39,700 --> 00:01:43,500
И ваше предположение действительно должно состоять из пяти букв, но, как вы увидите, оно

32
00:01:43,500 --> 00:01:45,700
довольно либерально в отношении того, что оно на самом деле позволит вам угадать.

33
00:01:45,700 --> 00:01:48,860
В этом случае мы пробуем фишку.

34
00:01:48,860 --> 00:01:50,260
И ладно, дела обстоят неплохо.

35
00:01:50,260 --> 00:01:54,580
Мы нажимаем S и H, поэтому мы знаем первые три буквы, мы знаем, что есть R.

36
00:01:54,740 --> 00:01:59,740
И это будет похоже на SHA что-то R или SHA R что-то.

37
00:01:59,740 --> 00:02:03,200
И похоже, что бот Wurdle знает, что

38
00:02:03,200 --> 00:02:05,220
есть всего два варианта: осколочный или острый.

39
00:02:05,220 --> 00:02:08,620
На данный момент между ними возникает своего рода спор, так что я

40
00:02:08,620 --> 00:02:11,260
думаю, что, вероятно, просто потому, что это алфавитный порядок, это соответствует осколку.

41
00:02:11,260 --> 00:02:13,000
Какое ура, это настоящий ответ.

42
00:02:13,000 --> 00:02:14,660
Итак, мы получили его за три.

43
00:02:14,660 --> 00:02:17,740
Если вам интересно, хорошо ли это, то я услышал от одного человека

44
00:02:17,740 --> 00:02:20,820
фразу: для Уёрдла четыре — это номинал, а три — птичка.

45
00:02:20,820 --> 00:02:22,960
Я думаю, это довольно удачная аналогия.

46
00:02:22,960 --> 00:02:27,560
Чтобы получить четыре, вам нужно постоянно работать над своей игрой, но это, конечно, не безумие.

47
00:02:27,560 --> 00:02:30,000
Но когда ты получаешь это за три, это просто здорово.

48
00:02:30,000 --> 00:02:33,800
Так что, если вы не против, то я хотел бы с самого начала просто

49
00:02:33,800 --> 00:02:36,600
рассказать о своем мыслительном процессе о том, как я подхожу к боту Wurdle.

50
00:02:36,600 --> 00:02:39,800
И, как я уже сказал, на самом деле это повод для урока теории информации.

51
00:02:39,800 --> 00:02:43,160
Основная цель — объяснить, что такое информация и что такое энтропия.

52
00:02:48,560 --> 00:02:52,080
Моей первой мыслью при подходе к этому было взглянуть

53
00:02:52,080 --> 00:02:53,560
на относительную частоту употребления различных букв в английском языке.

54
00:02:53,560 --> 00:02:57,800
Итак, я подумал: ладно, есть ли начальная догадка или пара начальных

55
00:02:57,800 --> 00:02:59,960
догадок, которая соответствует многим из этих наиболее часто встречающихся букв?

56
00:02:59,960 --> 00:03:03,780
И мне очень понравилось делать другое, а затем гвозди.

57
00:03:03,780 --> 00:03:06,980
Идея в том, что если вы нажмете на букву,

58
00:03:06,980 --> 00:03:07,980
вы получите зеленый или желтый цвет, это всегда приятно.

59
00:03:07,980 --> 00:03:09,460
Такое ощущение, что ты получаешь информацию.

60
00:03:09,460 --> 00:03:13,140
Но в этих случаях, даже если вы не попадаете и всегда получаете

61
00:03:13,140 --> 00:03:16,640
серый цвет, это все равно дает вам много информации, поскольку довольно редко

62
00:03:16,640 --> 00:03:17,640
можно найти слово, в котором нет ни одной из этих букв.

63
00:03:17,640 --> 00:03:21,840
Но даже несмотря на это, это не кажется

64
00:03:21,840 --> 00:03:23,520
суперсистематическим, потому что, например, порядок букв не учитывается.

65
00:03:23,520 --> 00:03:26,080
Зачем печатать гвозди, если можно печатать улитку?

66
00:03:26,080 --> 00:03:27,720
Лучше ли иметь букву S в конце?

67
00:03:27,720 --> 00:03:28,720
Я не совсем уверен.

68
00:03:28,720 --> 00:03:33,500
Мой друг сказал, что ему нравится начинать со слова «усталый», что меня немного удивило,

69
00:03:33,500 --> 00:03:37,160
потому что в нем есть несколько необычных букв, таких как W и Y.

70
00:03:37,160 --> 00:03:39,400
Но кто знает, может быть, это лучший дебют.

71
00:03:39,400 --> 00:03:43,920
Есть ли какая-то количественная оценка, по которой

72
00:03:43,920 --> 00:03:44,920
мы можем судить о качестве потенциального предположения?

73
00:03:44,920 --> 00:03:48,640
Теперь, чтобы настроить способ ранжирования возможных предположений, давайте вернемся и

74
00:03:48,640 --> 00:03:51,800
добавим немного ясности в то, как именно устроена игра.

75
00:03:51,800 --> 00:03:55,880
Итак, есть список слов, которые он позволит вам ввести и

76
00:03:55,880 --> 00:03:57,920
которые считаются действительными догадками, длиной всего около 13 000 слов.

77
00:03:57,920 --> 00:04:01,560
Но когда вы посмотрите на это, то увидите много действительно необычных вещей, таких как

78
00:04:01,560 --> 00:04:07,040
голова или Али и ARG, слова, которые вызывают семейные споры в игре в «Эрудит».

79
00:04:07,040 --> 00:04:10,600
Но суть игры в том, что ответом всегда будет достаточно обычное слово.

80
00:04:10,600 --> 00:04:16,080
И на самом деле, есть еще один список из примерно 2300 слов, которые являются возможными ответами.

81
00:04:16,080 --> 00:04:20,320
И это список, составленный людьми, я думаю, конкретно

82
00:04:20,320 --> 00:04:21,800
девушкой создателя игры, и это довольно забавно.

83
00:04:21,800 --> 00:04:25,560
Но что я хотел бы сделать, так это то, что наша задача в этом проекте состоит в том,

84
00:04:25,560 --> 00:04:30,720
чтобы посмотреть, сможем ли мы написать программу, решающую Wordle, которая не будет включать предыдущие знания об этом списке.

85
00:04:30,720 --> 00:04:34,560
Во-первых, существует множество довольно распространенных слов из пяти

86
00:04:34,560 --> 00:04:35,560
букв, которых вы не найдете в этом списке.

87
00:04:35,560 --> 00:04:38,360
Поэтому было бы лучше написать программу, которая была бы немного более устойчивой и могла бы

88
00:04:38,360 --> 00:04:41,960
играть в Wordle против кого угодно, а не только против того, что является официальным сайтом.

89
00:04:41,960 --> 00:04:45,900
А также причина, по которой мы знаем, что представляет собой этот список

90
00:04:45,900 --> 00:04:47,440
возможных ответов, заключается в том, что он виден в исходном коде.

91
00:04:47,440 --> 00:04:51,620
Но в исходном коде это отображается в определенном порядке,

92
00:04:51,620 --> 00:04:52,840
в котором ответы появляются изо дня в день.

93
00:04:52,840 --> 00:04:56,400
Так что вы всегда можете просто посмотреть, каким будет завтрашний ответ.

94
00:04:56,400 --> 00:04:59,140
Итак, очевидно, что в некотором смысле использование списка является мошенничеством.

95
00:04:59,140 --> 00:05:02,900
И что делает головоломку более интересной и более насыщенный урок теории информации,

96
00:05:02,900 --> 00:05:07,640
так это использование вместо этого некоторых более универсальных данных, таких как относительная

97
00:05:07,640 --> 00:05:11,640
частота слов в целом, чтобы уловить интуитивное ощущение предпочтения более распространенных слов.

98
00:05:11,640 --> 00:05:16,560
Итак, из этих 13 000 возможностей, как нам выбрать первую догадку?

99
00:05:16,560 --> 00:05:19,960
Например, если мой друг предлагает усталость, как нам следует проанализировать ее качество?

100
00:05:19,960 --> 00:05:25,040
Ну, причина, по которой он сказал, что ему нравится эта маловероятная W, заключается в том,

101
00:05:25,040 --> 00:05:27,880
что ему нравится дальновидность того, насколько приятно чувствовать себя, если вы действительно нажмете эту W.

102
00:05:27,880 --> 00:05:31,400
Например, если первая выявленная закономерность была примерно такой, то в

103
00:05:31,400 --> 00:05:36,080
этом гигантском словаре всего 58 слов, соответствующих этой закономерности.

104
00:05:36,080 --> 00:05:38,900
Так что это огромное сокращение по сравнению с 13 000.

105
00:05:38,900 --> 00:05:43,320
Но обратной стороной этого, конечно же, является то, что такой узор встречается очень редко.

106
00:05:43,360 --> 00:05:47,600
В частности, если бы каждое слово с одинаковой вероятностью было ответом, вероятность

107
00:05:47,600 --> 00:05:51,680
попадания в этот шаблон была бы 58, разделенная примерно на 13 000.

108
00:05:51,680 --> 00:05:53,880
Конечно, они не в равной степени могут быть ответами.

109
00:05:53,880 --> 00:05:56,680
Большинство из них – очень неясные и даже сомнительные слова.

110
00:05:56,680 --> 00:05:59,560
Но, по крайней мере, для нашего первого этапа, давайте предположим, что

111
00:05:59,560 --> 00:06:02,040
все они одинаково вероятны, а затем уточним это чуть позже.

112
00:06:02,040 --> 00:06:07,360
Дело в том, что паттерн с большим количеством информации по своей природе маловероятен.

113
00:06:07,360 --> 00:06:11,320
На самом деле, быть информативным означает то, что это маловероятно.

114
00:06:11,920 --> 00:06:16,720
Гораздо более вероятной моделью, которую можно увидеть в этом открытии,

115
00:06:16,720 --> 00:06:18,360
было бы что-то вроде этого, где, конечно, нет буквы W.

116
00:06:18,360 --> 00:06:22,080
Может быть, есть E, а может быть, нет A, нет R, нет Y.

117
00:06:22,080 --> 00:06:24,640
В этом случае существует 1400 возможных совпадений.

118
00:06:24,640 --> 00:06:29,600
Если бы все были одинаково вероятны, вероятность того, что

119
00:06:29,600 --> 00:06:30,680
вы увидите именно такую модель, составила бы около 11%.

120
00:06:30,680 --> 00:06:34,320
Таким образом, наиболее вероятные результаты являются и наименее информативными.

121
00:06:34,320 --> 00:06:38,440
Чтобы получить более глобальное представление, позвольте мне показать вам полное

122
00:06:38,440 --> 00:06:42,000
распределение вероятностей по всем различным закономерностям, которые вы можете увидеть.

123
00:06:42,000 --> 00:06:46,000
Таким образом, каждая полоса, на которую вы смотрите, соответствует возможному набору цветов,

124
00:06:46,000 --> 00:06:50,500
которые могут быть обнаружены, из которых существует от 3 до 5 возможностей,

125
00:06:50,500 --> 00:06:52,960
и они расположены слева направо, от наиболее распространенного до наименее распространенного.

126
00:06:52,960 --> 00:06:56,200
Таким образом, наиболее распространенной возможностью здесь является то, что вы получите все серые цвета.

127
00:06:56,200 --> 00:06:58,800
Это происходит примерно в 14% случаев.

128
00:06:58,800 --> 00:07:02,040
И когда вы делаете предположение, вы надеетесь на то, что окажетесь

129
00:07:02,040 --> 00:07:06,360
где-то в этом длинном хвосте, как здесь, где есть только 18

130
00:07:06,360 --> 00:07:09,920
возможностей для того, что соответствует этому шаблону, который, очевидно, выглядит так.

131
00:07:09,920 --> 00:07:14,080
Или, если мы рискнем пойти немного левее, знаете, может быть, мы пройдем весь путь сюда.

132
00:07:14,080 --> 00:07:16,560
Хорошо, вот вам хорошая головоломка.

133
00:07:16,560 --> 00:07:20,600
Какие три слова в английском языке начинаются с буквы W,

134
00:07:20,600 --> 00:07:22,040
заканчиваются на Y и где-то в них есть буква R?

135
00:07:22,040 --> 00:07:27,560
Оказывается, ответы, посмотрим, многословные, червивые и кривые.

136
00:07:27,560 --> 00:07:32,720
Итак, чтобы судить, насколько хорошо это слово в целом, нам нужна какая-то

137
00:07:32,720 --> 00:07:35,720
мера ожидаемого объема информации, которую вы собираетесь получить от этого распределения.

138
00:07:36,360 --> 00:07:41,080
Если мы пройдемся по каждому шаблону и умножим вероятность его появления на что-то,

139
00:07:41,080 --> 00:07:46,000
что измеряет, насколько он информативен, это, возможно, может дать нам объективную оценку.

140
00:07:46,000 --> 00:07:50,280
Теперь вашим первым инстинктом того, каким должно быть это что-то, может быть количество совпадений.

141
00:07:50,280 --> 00:07:52,960
Вам нужно меньшее среднее количество совпадений.

142
00:07:52,960 --> 00:07:57,400
Но вместо этого я хотел бы использовать более универсальное измерение, которое мы часто

143
00:07:57,400 --> 00:08:01,040
приписываем информации, и которое станет более гибким, когда каждому из этих 13 000

144
00:08:01,040 --> 00:08:04,320
слов будет назначена разная вероятность того, являются ли они на самом деле ответом.

145
00:08:10,600 --> 00:08:14,760
Стандартной единицей информации является бит, формула которого немного забавна, но

146
00:08:14,760 --> 00:08:17,800
она действительно интуитивно понятна, если мы просто посмотрим на примеры.

147
00:08:17,800 --> 00:08:21,880
Если у вас есть наблюдение, которое сокращает ваше пространство возможностей

148
00:08:21,880 --> 00:08:24,200
вдвое, мы говорим, что оно содержит один бит информации.

149
00:08:24,200 --> 00:08:27,680
В нашем примере пространство возможностей — это все возможные слова, и получается, что

150
00:08:27,760 --> 00:08:31,560
примерно половина из пятибуквенных слов имеет букву S, чуть меньше, но примерно половина.

151
00:08:31,560 --> 00:08:35,200
Таким образом, это наблюдение даст вам один бит информации.

152
00:08:35,200 --> 00:08:39,640
Если вместо этого новый факт сокращает это пространство возможностей в

153
00:08:39,640 --> 00:08:42,000
четыре раза, мы говорим, что он содержит два бита информации.

154
00:08:42,000 --> 00:08:45,120
Например, оказывается, что около четверти этих слов имеют букву Т.

155
00:08:45,120 --> 00:08:49,720
Если наблюдение сокращает это пространство в восемь раз, мы говорим, что

156
00:08:49,720 --> 00:08:50,920
это три бита информации, и так далее, и тому подобное.

157
00:08:50,920 --> 00:08:55,000
Четыре бита превращают его в 16-й, пять битов — в 32-й.

158
00:08:55,000 --> 00:09:00,160
Итак, теперь вы, возможно, захотите сделать паузу и спросить себя, какова

159
00:09:00,160 --> 00:09:04,520
формула информации о количестве битов с точки зрения вероятности возникновения события?

160
00:09:04,520 --> 00:09:07,920
Мы здесь говорим о том, что когда вы принимаете половину числа битов, это то

161
00:09:07,920 --> 00:09:11,680
же самое, что и вероятность, а это то же самое, что сказать, что двойка

162
00:09:11,680 --> 00:09:16,200
в степени числа битов равна единице по сравнению с вероятностью, что далее перестраивается так,

163
00:09:16,200 --> 00:09:19,680
что информация представляет собой логарифм по основанию два из одного, разделенный на вероятность.

164
00:09:19,680 --> 00:09:23,200
И иногда вы видите это с еще одной перестановкой, где

165
00:09:23,200 --> 00:09:25,680
информация представляет собой отрицательный логарифм по основанию два вероятности.

166
00:09:25,680 --> 00:09:29,120
Выраженный таким образом, это может показаться немного странным для

167
00:09:29,120 --> 00:09:33,400
непосвященных, но на самом деле это просто очень интуитивная

168
00:09:33,400 --> 00:09:35,120
идея спросить, сколько раз вы сократили свои возможности вдвое.

169
00:09:35,120 --> 00:09:37,840
Теперь, если вам интересно, знаете, я думал, что мы просто

170
00:09:37,840 --> 00:09:39,920
играем в забавную словесную игру, почему на картинке появляются логарифмы?

171
00:09:39,920 --> 00:09:43,920
Одна из причин, по которой эта единица более удобна, заключается в том, что гораздо

172
00:09:43,920 --> 00:09:48,120
проще говорить об очень маловероятных событиях, гораздо проще сказать, что наблюдение содержит 20 бит

173
00:09:48,120 --> 00:09:53,480
информации, чем сказать, что вероятность того или иного события равна 0. 0000095.

174
00:09:53,480 --> 00:09:57,360
Но более существенная причина того, что это логарифмическое выражение оказалось очень

175
00:09:57,360 --> 00:10:02,000
полезным дополнением к теории вероятностей, заключается в том, как информация складывается.

176
00:10:02,000 --> 00:10:05,560
Например, если одно наблюдение дает вам два бита информации, сокращая ваше пространство

177
00:10:05,560 --> 00:10:10,120
в четыре раза, а затем второе наблюдение, такое как ваше второе

178
00:10:10,120 --> 00:10:14,480
предположение в Wordle, дает вам еще три бита информации, сокращая вас

179
00:10:14,480 --> 00:10:17,360
еще в восемь раз, два вместе дают вам пять бит информации.

180
00:10:17,360 --> 00:10:21,200
Точно так же, как вероятности любят умножаться, информация любит прибавляться.

181
00:10:21,200 --> 00:10:24,920
Итак, как только мы попадаем в область чего-то вроде ожидаемого значения,

182
00:10:24,920 --> 00:10:28,660
где мы складываем кучу чисел, с журналами работать становится намного приятнее.

183
00:10:28,660 --> 00:10:32,600
Давайте вернемся к нашему дистрибутиву Weary и добавим сюда еще

184
00:10:32,600 --> 00:10:35,560
один небольшой трекер, показывающий, сколько информации содержится для каждого шаблона.

185
00:10:35,560 --> 00:10:38,760
Главное, что я хочу, чтобы вы заметили, это то, что чем выше вероятность того, что

186
00:10:38,760 --> 00:10:43,500
мы доберемся до этих более вероятных шаблонов, тем меньше информации, тем меньше битов вы получите.

187
00:10:43,500 --> 00:10:47,360
Чтобы измерить качество этого предположения, мы возьмем ожидаемое значение этой

188
00:10:47,360 --> 00:10:51,620
информации, пройдемся по каждому шаблону, скажем, насколько он вероятен, а

189
00:10:51,620 --> 00:10:54,940
затем умножим это на количество битов информации, которые мы получим.

190
00:10:54,940 --> 00:10:58,480
А в примере с Вири это число равно 4. 9 бит.

191
00:10:58,480 --> 00:11:02,800
Таким образом, в среднем информация, которую вы получаете из этого начального предположения, так

192
00:11:02,800 --> 00:11:05,660
же эффективна, как сокращение пространства ваших возможностей пополам примерно в пять раз.

193
00:11:05,660 --> 00:11:10,260
Напротив, примером предположения с более высокой ожидаемой

194
00:11:10,260 --> 00:11:13,220
информационной ценностью может быть что-то вроде Slate.

195
00:11:13,220 --> 00:11:16,180
В этом случае вы заметите, что распределение выглядит намного более плоским.

196
00:11:16,180 --> 00:11:20,780
В частности, наиболее вероятное появление всех оттенков серого имеет только около 6% вероятности

197
00:11:20,780 --> 00:11:25,940
появления, так что как минимум вы получите очевидно 3. 9 бит информации.

198
00:11:25,940 --> 00:11:29,140
Но это минимум, чаще всего можно получить что-то получше.

199
00:11:29,140 --> 00:11:33,380
И оказывается, что если подсчитать цифры и сложить все

200
00:11:33,380 --> 00:11:36,420
соответствующие термины, то средняя информация составит около 5. 8.

201
00:11:36,420 --> 00:11:42,140
Таким образом, в отличие от Вири, после первого предположения

202
00:11:42,140 --> 00:11:43,940
ваше пространство возможностей будет в среднем вдвое меньше.

203
00:11:43,940 --> 00:11:49,540
На самом деле есть забавная история с названием этого ожидаемого значения количества информации.

204
00:11:49,540 --> 00:11:52,580
Теория информации была разработана Клодом Шенноном, который работал в Bell Labs в

205
00:11:52,580 --> 00:11:57,620
1940-х годах, но о некоторых своих еще не опубликованных идеях он говорил

206
00:11:57,620 --> 00:12:01,500
с Джоном фон Нейманом, интеллектуальным гигантом того времени, очень выдающимся человеком. по

207
00:12:01,500 --> 00:12:04,180
математике и физике и положил начало тому, что впоследствии стало информатикой.

208
00:12:04,180 --> 00:12:07,260
И когда он упомянул, что на самом деле у него нет хорошего

209
00:12:07,260 --> 00:12:12,540
названия для этого ожидаемого значения количества информации, фон Нейман якобы сказал, что,

210
00:12:12,540 --> 00:12:14,720
как гласит история, вы должны называть это энтропией, и по двум причинам.

211
00:12:14,720 --> 00:12:18,400
Во-первых, ваша функция неопределенности использовалась в статистической механике под этим именем, поэтому у

212
00:12:18,400 --> 00:12:23,100
нее уже есть имя, а во-вторых, что более важно, никто не знает, что

213
00:12:23,100 --> 00:12:26,940
такое энтропия на самом деле, поэтому в дебатах вы всегда будете иметь преимущество.

214
00:12:26,940 --> 00:12:31,420
Так что, если название кажется немного загадочным и

215
00:12:31,420 --> 00:12:33,420
если верить этой истории, то это сделано специально.

216
00:12:33,420 --> 00:12:36,740
Кроме того, если вы задаетесь вопросом о его связи со всем

217
00:12:36,740 --> 00:12:40,820
этим вторым законом термодинамики из физики, связь определенно существует, но в

218
00:12:40,820 --> 00:12:44,780
ее происхождении Шеннон просто имел дело с чистой теорией вероятностей, и

219
00:12:44,780 --> 00:12:49,340
для наших целей здесь, когда я использую слово энтропия, я просто

220
00:12:49,340 --> 00:12:50,820
хочу, чтобы вы подумали об ожидаемой информационной ценности конкретного предположения.

221
00:12:50,820 --> 00:12:54,380
Вы можете думать об энтропии как об измерении двух вещей одновременно.

222
00:12:54,380 --> 00:12:57,420
Во-первых, насколько равномерным является распределение.

223
00:12:57,420 --> 00:13:01,700
Чем ближе распределение к равномерному, тем выше будет энтропия.

224
00:13:01,700 --> 00:13:06,340
В нашем случае, когда общее количество шаблонов составляет от 3 до 5, для равномерного распределения наблюдение любого

225
00:13:06,340 --> 00:13:11,340
из них будет иметь базу данных журнала 2 из 3 до 5, что в итоге равно

226
00:13:11,340 --> 00:13:17,860
7. 92, так что это абсолютный максимум, который вы можете иметь для этой энтропии.

227
00:13:17,860 --> 00:13:21,900
Но энтропия также является своего рода

228
00:13:21,900 --> 00:13:22,900
мерой того, сколько возможностей вообще существует.

229
00:13:22,900 --> 00:13:26,980
Например, если у вас есть какое-то слово, в котором существует только 16 возможных шаблонов,

230
00:13:26,980 --> 00:13:32,760
и каждый из них равновероятен, эта энтропия, эта ожидаемая информация, будет равна 4 битам.

231
00:13:32,760 --> 00:13:36,880
Но если у вас есть другое слово, в котором может возникнуть 64

232
00:13:36,880 --> 00:13:41,000
возможных шаблона, и все они одинаково вероятны, тогда энтропия составит 6 бит.

233
00:13:41,000 --> 00:13:45,800
Итак, если вы видите какое-то распределение, энтропия которого равна 6 битам, это

234
00:13:45,800 --> 00:13:50,000
как бы говорит о том, что в том, что должно произойти, существует

235
00:13:50,000 --> 00:13:54,400
столько же вариаций и неопределенности, как если бы было 64 равновероятных исхода.

236
00:13:54,400 --> 00:13:58,360
Для моего первого использования Wurtelebot я просто сделал это.

237
00:13:58,360 --> 00:14:03,560
Он перебирает все возможные предположения, все 13 000 слов, вычисляет энтропию для

238
00:14:03,560 --> 00:14:08,580
каждого из них, или, точнее, энтропию распределения по всем шаблонам, которые

239
00:14:08,580 --> 00:14:13,040
вы можете увидеть, для каждого из них, и выбирает самое высокое,

240
00:14:13,040 --> 00:14:17,200
поскольку это тот, который, скорее всего, максимально сократит ваше пространство возможностей.

241
00:14:17,200 --> 00:14:20,120
И хотя я говорил здесь только о первой догадке,

242
00:14:20,120 --> 00:14:21,680
то же самое происходит и со следующими несколькими догадками.

243
00:14:21,680 --> 00:14:25,100
Например, после того, как вы видите некоторый шаблон в этом первом предположении, который ограничит

244
00:14:25,100 --> 00:14:29,300
вас меньшим количеством возможных слов в зависимости от того, что с ним совпадает,

245
00:14:29,300 --> 00:14:32,300
вы просто играете в ту же игру в отношении этого меньшего набора слов.

246
00:14:32,300 --> 00:14:36,500
Для предлагаемого второго предположения вы смотрите на распределение всех шаблонов, которые

247
00:14:36,500 --> 00:14:41,540
могут возникнуть из этого более ограниченного набора слов, вы просматриваете все

248
00:14:41,540 --> 00:14:45,480
13 000 возможностей и находите тот, который максимизирует эту энтропию.

249
00:14:45,480 --> 00:14:48,980
Чтобы показать вам, как это работает в действии, позвольте мне просто привести небольшой

250
00:14:48,980 --> 00:14:54,060
вариант написанного мной Вюртеле, в котором на полях показаны основные моменты этого анализа.

251
00:14:54,460 --> 00:14:57,820
После выполнения всех расчетов энтропии справа здесь показано,

252
00:14:57,820 --> 00:15:00,340
какие из них имеют наиболее ожидаемую информацию.

253
00:15:00,340 --> 00:15:04,940
Оказывается, самый популярный ответ, по крайней мере на данный момент, мы уточним это

254
00:15:04,940 --> 00:15:11,140
позже, — это Тарес, что означает, хм, конечно, вика, самая обычная вика.

255
00:15:11,140 --> 00:15:14,180
Каждый раз, когда мы здесь делаем предположение, что, возможно, я игнорирую его

256
00:15:14,180 --> 00:15:19,220
рекомендации и использую шифер, потому что мне нравится шифер, мы можем

257
00:15:19,220 --> 00:15:23,300
видеть, сколько ожидаемой информации он содержал, но затем справа от слова здесь

258
00:15:23,340 --> 00:15:24,980
показано, сколько реальную информацию, которую мы получили, учитывая эту конкретную закономерность.

259
00:15:24,980 --> 00:15:28,660
Так вот тут похоже нам немного не повезло, от нас ожидали 5. 8, но

260
00:15:28,660 --> 00:15:30,660
нам удалось получить что-то меньшее.

261
00:15:30,660 --> 00:15:34,020
А затем, слева, здесь показаны все возможные

262
00:15:34,020 --> 00:15:35,860
слова, учитывая, где мы сейчас находимся.

263
00:15:35,860 --> 00:15:39,820
Синие полосы сообщают нам, насколько вероятно, по его мнению, каждое слово, поэтому на данный момент

264
00:15:39,820 --> 00:15:44,140
он предполагает, что каждое слово встречается с одинаковой вероятностью, но мы уточним это через мгновение.

265
00:15:44,140 --> 00:15:48,580
И затем это измерение неопределенности говорит нам об энтропии

266
00:15:48,580 --> 00:15:53,220
этого распределения возможных слов, которое сейчас, поскольку это равномерное

267
00:15:53,300 --> 00:15:55,940
распределение, является просто излишне сложным способом подсчета количества возможностей.

268
00:15:55,940 --> 00:16:01,700
Например, если бы мы возвели 2 в 13-ю степень. 66, это должно

269
00:16:01,700 --> 00:16:02,700
быть около 13 000 возможностей.

270
00:16:02,700 --> 00:16:06,780
Я здесь немного не так, но только потому, что не показываю все десятичные знаки.

271
00:16:06,780 --> 00:16:10,260
На данный момент это может показаться излишним и слишком усложняющим ситуацию,

272
00:16:10,260 --> 00:16:12,780
но вы поймете, почему полезно иметь оба числа за минуту.

273
00:16:12,780 --> 00:16:16,780
Итак, здесь похоже, что самая высокая энтропия для нашего второго предположения

274
00:16:16,780 --> 00:16:19,700
— это Рамен, что опять-таки просто не похоже на слово.

275
00:16:19,700 --> 00:16:25,660
Итак, чтобы занять здесь моральную позицию, я собираюсь ввести Rains.

276
00:16:25,660 --> 00:16:27,540
И снова похоже, нам немного не повезло.

277
00:16:27,540 --> 00:16:32,100
Мы ожидали 4. 3 бита, а мы получили только 3. 39 бит информации.

278
00:16:32,100 --> 00:16:35,060
Таким образом, мы получаем 55 возможностей.

279
00:16:35,060 --> 00:16:38,860
И здесь, возможно, я просто воспользуюсь тем, что он

280
00:16:38,860 --> 00:16:40,200
предлагает, а именно комбо, что бы это ни значило.

281
00:16:40,200 --> 00:16:43,300
И ладно, на самом деле это хороший шанс для головоломки.

282
00:16:43,300 --> 00:16:47,020
Он говорит нам, что этот шаблон дает нам 4. 7 бит информации.

283
00:16:47,020 --> 00:16:52,400
Но слева, прежде чем мы увидим эту закономерность, их было 5. 78 бит неопределенности.

284
00:16:52,400 --> 00:16:56,860
Итак, в качестве теста для вас: что это значит относительно количества оставшихся возможностей?

285
00:16:56,860 --> 00:17:02,280
Ну, это означает, что мы сведены к одной частичке неопределенности, а

286
00:17:02,280 --> 00:17:04,700
это то же самое, что сказать, что есть два возможных ответа.

287
00:17:04,700 --> 00:17:06,520
Это выбор 50 на 50.

288
00:17:06,520 --> 00:17:09,860
И отсюда, поскольку мы с вами знаем, какие слова

289
00:17:09,860 --> 00:17:11,220
встречаются чаще, мы знаем, что ответ должен быть пропасть.

290
00:17:11,220 --> 00:17:13,540
Но как сейчас написано, программа этого не знает.

291
00:17:13,540 --> 00:17:17,560
Поэтому он просто продолжает работать, пытаясь получить как можно больше информации,

292
00:17:17,560 --> 00:17:20,360
пока не останется только одна возможность, а затем он ее угадывает.

293
00:17:20,360 --> 00:17:22,700
Поэтому очевидно, что нам нужна лучшая стратегия эндшпиля.

294
00:17:22,700 --> 00:17:26,540
Но предположим, что мы назовем эту версию одним из наших решателей

295
00:17:26,540 --> 00:17:30,740
слов, а затем запустим несколько симуляций, чтобы посмотреть, как она работает.

296
00:17:30,740 --> 00:17:34,240
Итак, это работает так: он играет во все возможные словесные игры.

297
00:17:34,240 --> 00:17:38,780
Он проходит через все эти 2315 слов, которые являются реальными ответами.

298
00:17:38,780 --> 00:17:41,340
По сути, он использует это как набор для тестирования.

299
00:17:41,340 --> 00:17:45,820
И с помощью этого наивного метода не учитывать, насколько распространено слово, а просто пытаться максимизировать

300
00:17:45,820 --> 00:17:50,480
информацию на каждом этапе пути, пока не дойдет до одного и только одного варианта.

301
00:17:50,480 --> 00:17:55,100
К концу моделирования средний балл составит около 4. 124.

302
00:17:55,100 --> 00:17:59,780
Что неплохо, если честно, я ожидал худшего результата.

303
00:17:59,780 --> 00:18:03,040
Но люди, играющие в Wordle, скажут вам, что обычно они могут получить это за 4.

304
00:18:03,040 --> 00:18:05,260
Настоящая задача — собрать как можно больше очков из 3.

305
00:18:05,260 --> 00:18:08,920
Это довольно большой скачок между оценкой 4 и оценкой 3.

306
00:18:08,920 --> 00:18:13,300
Очевидный результат здесь — каким-то образом определить, является ли

307
00:18:13,300 --> 00:18:23,160
слово распространенным, и как именно мы это делаем.

308
00:18:23,160 --> 00:18:26,860
Я подошел к этому с целью получить

309
00:18:26,860 --> 00:18:28,560
список относительных частот всех слов английского языка.

310
00:18:28,560 --> 00:18:32,560
И я только что использовал функцию данных частоты слов Mathematica, которая

311
00:18:32,560 --> 00:18:35,520
сама извлекает данные из общедоступного набора данных Google Books English Ngram.

312
00:18:35,520 --> 00:18:38,680
И на это интересно смотреть, например, если мы отсортируем

313
00:18:38,680 --> 00:18:40,120
его от наиболее распространенных слов к наименее распространенным.

314
00:18:40,120 --> 00:18:43,740
Очевидно, это самые распространенные слова из 5 букв в английском языке.

315
00:18:43,740 --> 00:18:46,480
Вернее, это 8-е место по распространенности.

316
00:18:46,480 --> 00:18:49,440
Сначала есть что, после чего там и там.

317
00:18:49,440 --> 00:18:53,020
Первое само по себе не первое, а девятое, и имеет смысл,

318
00:18:53,020 --> 00:18:57,840
что эти другие слова могут встречаться чаще, где те, что после

319
00:18:57,840 --> 00:18:59,000
первого, находятся после, где, и эти слова встречаются немного реже.

320
00:18:59,000 --> 00:19:04,400
Теперь, используя эти данные для моделирования вероятности того, что каждое из этих

321
00:19:04,400 --> 00:19:06,760
слов будет окончательным ответом, оно не должно быть просто пропорционально частоте.

322
00:19:07,020 --> 00:19:12,560
Например, которому присвоен балл 0. 002 в этом наборе данных, тогда как

323
00:19:12,560 --> 00:19:15,200
слово «коса» в каком-то смысле примерно в 1000 раз менее вероятно.

324
00:19:15,200 --> 00:19:19,400
Но оба эти слова достаточно распространены, поэтому их почти наверняка стоит рассмотреть.

325
00:19:19,400 --> 00:19:21,900
Поэтому нам нужно больше бинарного отсечения.

326
00:19:21,900 --> 00:19:26,520
Я представил, что беру весь этот отсортированный список слов, затем располагаю его

327
00:19:26,520 --> 00:19:31,060
по оси X, а затем применяю сигмовидную функцию, которая является стандартным

328
00:19:31,060 --> 00:19:35,540
способом получить функцию, вывод которой в основном двоичный, это либо 0,

329
00:19:35,540 --> 00:19:38,500
либо 1, но в этой области неопределенности между ними происходит сглаживание.

330
00:19:38,500 --> 00:19:43,900
По сути, вероятность того, что я назначаю каждому слову попадание в окончательный список, будет

331
00:19:43,900 --> 00:19:49,540
значением сигмовидной функции, указанной выше, где бы оно ни располагалось на оси X.

332
00:19:49,540 --> 00:19:53,940
Очевидно, это зависит от нескольких параметров, например, насколько широкое пространство на оси X

333
00:19:53,940 --> 00:19:59,660
заполняют эти слова, определяет, насколько постепенно или круто мы снижаемся от 1

334
00:19:59,660 --> 00:20:03,000
до 0, а то, где мы располагаем их слева направо, определяет границу.

335
00:20:03,160 --> 00:20:07,340
Честно говоря, я просто облизнул палец и высунул его по ветру.

336
00:20:07,340 --> 00:20:10,800
Я просмотрел отсортированный список и попытался найти окно, в котором, посмотрев

337
00:20:10,800 --> 00:20:15,280
на него, я понял, что около половины этих слов скорее

338
00:20:15,280 --> 00:20:17,680
всего будут окончательным ответом, и использовал его в качестве отсечения.

339
00:20:17,680 --> 00:20:21,840
Как только мы получим такое распределение по словам, это даст

340
00:20:21,840 --> 00:20:24,460
нам еще одну ситуацию, когда энтропия становится действительно полезным измерением.

341
00:20:24,460 --> 00:20:28,480
Например, предположим, что мы играли в игру и начинаем с

342
00:20:28,480 --> 00:20:32,480
моих старых открывашек, которыми были перо и гвозди, и заканчиваем

343
00:20:32,480 --> 00:20:33,760
ситуацией, когда есть четыре возможных слова, которые ей соответствуют.

344
00:20:33,760 --> 00:20:36,440
И допустим, мы считаем их всех одинаково вероятными.

345
00:20:36,440 --> 00:20:40,000
Позвольте мне спросить вас, какова энтропия этого распределения?

346
00:20:40,000 --> 00:20:45,920
Что ж, информация, связанная с каждой из этих возможностей, будет иметь логарифмическую базу 2

347
00:20:45,920 --> 00:20:50,800
из 4, поскольку каждая из них равна 1 и 4, а это 2.

348
00:20:50,800 --> 00:20:52,780
Два бита информации, четыре возможности.

349
00:20:52,780 --> 00:20:54,360
Все очень хорошо и хорошо.

350
00:20:54,360 --> 00:20:58,320
Но что, если я скажу вам, что на самом деле совпадений больше четырех?

351
00:20:58,320 --> 00:21:02,600
На самом деле, когда мы просматриваем полный список слов, мы видим, что ему соответствуют 16 слов.

352
00:21:02,600 --> 00:21:07,260
Но предположим, что наша модель дает очень низкую вероятность того, что остальные 12 слов

353
00:21:07,260 --> 00:21:11,440
действительно станут окончательным ответом, примерно 1 из 1000, потому что они действительно неясны.

354
00:21:11,440 --> 00:21:15,480
Теперь позвольте мне спросить вас, какова энтропия этого распределения?

355
00:21:15,480 --> 00:21:19,600
Если бы энтропия измеряла здесь просто количество совпадений, то можно было бы ожидать,

356
00:21:19,600 --> 00:21:24,760
что это будет что-то вроде логарифмической базы 2 из 16, что будет

357
00:21:24,760 --> 00:21:26,200
равно 4, то есть на два бита неопределенности больше, чем было раньше.

358
00:21:26,200 --> 00:21:30,320
Но, конечно, фактическая неопределенность на самом деле не сильно отличается от того, что было раньше.

359
00:21:30,320 --> 00:21:33,840
Тот факт, что есть эти 12 действительно непонятных слов, не означает, что

360
00:21:33,840 --> 00:21:38,200
было бы еще более удивительно узнать, например, что окончательный ответ — «очарование».

361
00:21:38,200 --> 00:21:42,080
Итак, когда вы на самом деле выполняете вычисления здесь и складываете вероятность

362
00:21:42,080 --> 00:21:45,960
каждого события, умноженную на соответствующую информацию, вы получаете 2. 11 бит.

363
00:21:45,960 --> 00:21:50,280
Я просто говорю, что это по сути два бита, в основном эти четыре

364
00:21:50,280 --> 00:21:54,240
возможности, но есть немного больше неопределенности из-за всех этих крайне маловероятных событий, хотя,

365
00:21:54,240 --> 00:21:57,120
если бы вы их изучили, вы бы получили из этого массу информации.

366
00:21:57,120 --> 00:22:00,800
Уменьшение масштаба — это часть того, что делает

367
00:22:00,800 --> 00:22:01,800
Wordle таким хорошим примером для урока теории информации.

368
00:22:01,800 --> 00:22:05,280
У нас есть два различных применения чувств к энтропии.

369
00:22:05,280 --> 00:22:09,640
Первый говорит нам, какую ожидаемую информацию мы получим в

370
00:22:09,640 --> 00:22:14,560
результате данного предположения, а второй говорит, можем ли

371
00:22:14,560 --> 00:22:16,480
мы измерить оставшуюся неопределенность среди всех возможных слов.

372
00:22:16,480 --> 00:22:19,800
И я должен подчеркнуть, что в первом случае, когда мы смотрим на ожидаемую информацию

373
00:22:19,800 --> 00:22:25,000
предположения, когда у нас есть неравный вес слов, это влияет на вычисление энтропии.

374
00:22:25,000 --> 00:22:28,600
Например, позвольте мне рассмотреть тот же случай, который мы рассматривали

375
00:22:28,600 --> 00:22:33,560
ранее, с распределением, связанным с Уири, но на этот

376
00:22:33,560 --> 00:22:34,560
раз с использованием неравномерного распределения по всем возможным словам.

377
00:22:34,560 --> 00:22:39,360
Итак, давайте посмотрим, смогу ли я найти здесь часть, которая достаточно хорошо это иллюстрирует.

378
00:22:39,360 --> 00:22:42,480
Ладно, вот это очень хорошо.

379
00:22:42,480 --> 00:22:46,360
Здесь у нас есть два соседних шаблона, которые примерно одинаково вероятны, но один

380
00:22:46,360 --> 00:22:49,480
из них, как нам сказали, имеет 32 возможных слова, которые ему соответствуют.

381
00:22:49,480 --> 00:22:54,080
И если мы проверим, что это такое, то это те 32, которые

382
00:22:54,080 --> 00:22:55,600
представляют собой просто очень невероятные слова, когда вы просматриваете их глазами.

383
00:22:55,600 --> 00:23:00,400
Трудно найти какие-либо ответы, которые кажутся правдоподобными, возможно, крики, но если

384
00:23:00,400 --> 00:23:04,440
мы посмотрим на соседний шаблон в распределении, который считается примерно столь

385
00:23:04,440 --> 00:23:08,920
же вероятным, нам скажут, что он имеет только 8 возможных совпадений,

386
00:23:08,920 --> 00:23:09,920
то есть четверть как много совпадений, но это примерно одинаково.

387
00:23:09,920 --> 00:23:12,520
И когда мы достанем эти спички, мы поймем, почему.

388
00:23:12,520 --> 00:23:17,840
Некоторые из них являются вполне правдоподобными ответами, например, «звонок», «гнев» или «стуки».

389
00:23:17,840 --> 00:23:22,000
Чтобы проиллюстрировать, как мы все это реализуем, позвольте мне открыть здесь вторую версию Wordlebot,

390
00:23:22,000 --> 00:23:25,960
и в ней есть два или три основных отличия от первой, которую мы видели.

391
00:23:25,960 --> 00:23:29,460
Во-первых, как я только что сказал, способ, которым мы вычисляем эти

392
00:23:29,460 --> 00:23:34,800
энтропии, эти ожидаемые значения информации, теперь использует более точное распределение по

393
00:23:34,800 --> 00:23:39,300
шаблонам, которое учитывает вероятность того, что данное слово действительно будет ответом.

394
00:23:39,300 --> 00:23:44,160
Так получилось, что слезы по-прежнему на первом месте, хотя последующие немного другие.

395
00:23:44,160 --> 00:23:47,920
Во-вторых, когда он ранжирует свои лучшие варианты, он теперь будет хранить модель вероятности того,

396
00:23:47,920 --> 00:23:52,600
что каждое слово является фактическим ответом, и будет включать это в свое решение,

397
00:23:52,600 --> 00:23:55,520
что легче увидеть, если у нас есть несколько предположений по поводу ответа. стол.

398
00:23:55,520 --> 00:24:01,120
Опять же, игнорируя его рекомендации, потому что мы не можем позволить машинам управлять нашей жизнью.

399
00:24:01,120 --> 00:24:05,160
И я полагаю, мне следует упомянуть еще одну вещь, которая здесь слева: это значение

400
00:24:05,160 --> 00:24:10,080
неопределенности, это количество битов, больше не просто избыточно по сравнению с количеством возможных совпадений.

401
00:24:10,080 --> 00:24:16,520
Теперь, если мы поднимем его и посчитаем 2 к 8. 02, что немного выше 256, я думаю,

402
00:24:16,520 --> 00:24:22,640
259, он говорит о том, что, несмотря на то, что всего 526 слов,

403
00:24:22,640 --> 00:24:26,400
которые на самом деле соответствуют этому шаблону, степень неопределенности, которую он имеет, больше

404
00:24:26,400 --> 00:24:29,760
похожа на то, что было бы, если бы было 259 равновероятных результаты.

405
00:24:29,760 --> 00:24:31,100
Вы можете думать об этом так.

406
00:24:31,100 --> 00:24:35,560
Он знает, что боркс — это не ответ, то же самое с йортами,

407
00:24:35,560 --> 00:24:37,840
зорлами и зорусами, поэтому его неопределенность немного меньше, чем в предыдущем случае.

408
00:24:37,840 --> 00:24:40,220
Это количество бит будет меньше.

409
00:24:40,220 --> 00:24:44,040
И если я продолжу играть в игру, я уточню это с помощью

410
00:24:44,040 --> 00:24:48,680
пары предположений, связанных с тем, что я хотел бы объяснить здесь.

411
00:24:48,680 --> 00:24:52,520
По четвертому предположению, если вы посмотрите на его лучшие варианты,

412
00:24:52,520 --> 00:24:53,800
вы увидите, что это уже не просто максимизация энтропии.

413
00:24:53,800 --> 00:24:58,480
Итак, на данный момент технически существует семь возможностей, но единственные,

414
00:24:58,480 --> 00:25:00,780
у которых есть значимый шанс, — это общежития и слова.

415
00:25:00,780 --> 00:25:04,760
И вы можете видеть, что выбор обоих из этих значений стоит

416
00:25:04,760 --> 00:25:07,560
выше всех остальных значений, которые, строго говоря, дадут больше информации.

417
00:25:07,560 --> 00:25:11,200
В первый раз, когда я это сделал, я просто сложил эти два числа, чтобы измерить

418
00:25:11,200 --> 00:25:14,580
качество каждого предположения, и это на самом деле сработало лучше, чем вы могли подумать.

419
00:25:14,580 --> 00:25:17,600
Но на самом деле это не казалось систематическим, и я уверен, что

420
00:25:17,600 --> 00:25:19,880
люди могли бы использовать другие подходы, но я остановился на этом.

421
00:25:19,880 --> 00:25:24,200
Если мы рассматриваем перспективу следующей догадки, как в данном случае слов,

422
00:25:24,200 --> 00:25:28,440
нас действительно волнует ожидаемый результат нашей игры, если мы это сделаем.

423
00:25:28,440 --> 00:25:32,880
И чтобы вычислить этот ожидаемый балл, мы говорим, какова вероятность того,

424
00:25:32,880 --> 00:25:35,640
что слова являются фактическим ответом, который на данный момент описывает 58%.

425
00:25:36,080 --> 00:25:40,400
Мы говорим, что с вероятностью 58% наш счет в этой игре будет 4.

426
00:25:40,400 --> 00:25:46,240
И тогда с вероятностью 1 минус эти 58% наш результат будет больше этих 4.

427
00:25:46,240 --> 00:25:50,640
Насколько больше мы не знаем, но мы можем оценить это, исходя из

428
00:25:50,640 --> 00:25:52,920
того, насколько велика неопределенность, вероятно, возникнет, когда мы доберемся до этой точки.

429
00:25:52,920 --> 00:25:56,600
Конкретно на данный момент их 1. 44 бита неопределенности.

430
00:25:56,600 --> 00:26:01,560
Если мы угадываем слова, это означает, что ожидаемая информация, которую мы получим, равна 1. 27 бит.

431
00:26:01,560 --> 00:26:06,280
Итак, если мы угадываем слова, эта разница показывает, сколько неопределенности

432
00:26:06,280 --> 00:26:08,280
у нас, вероятно, останется после того, как это произойдет.

433
00:26:08,280 --> 00:26:12,500
Нам нужна некая функция, которую я здесь называю

434
00:26:12,500 --> 00:26:13,880
f, которая связывает эту неопределенность с ожидаемым результатом.

435
00:26:13,880 --> 00:26:18,040
И способ, которым это было сделано, заключался в том, чтобы просто построить график

436
00:26:18,040 --> 00:26:23,920
данных из предыдущих игр на основе версии 1 бота, чтобы сказать: «Эй, каков

437
00:26:23,920 --> 00:26:27,040
был фактический счет после различных точек с определенной, очень измеримой степенью неопределенности».

438
00:26:27,040 --> 00:26:31,120
Например, эти точки данных находятся выше значения примерно 8. Для

439
00:26:31,120 --> 00:26:36,840
некоторых игр говорят «7» или около того после момента, когда их было 8. 7 бит

440
00:26:36,840 --> 00:26:39,340
неопределенности, чтобы получить окончательный ответ, потребовалось две догадки.

441
00:26:39,340 --> 00:26:43,180
В других играх требовалось три предположения, в других — четыре предположения.

442
00:26:43,180 --> 00:26:46,920
Если мы сдвинемся здесь влево, все точки выше нуля означают, что всякий

443
00:26:46,920 --> 00:26:51,620
раз, когда есть ноль бит неопределенности, то есть есть только одна

444
00:26:51,620 --> 00:26:55,000
возможность, тогда количество требуемых предположений всегда будет только одним, что обнадеживает.

445
00:26:55,000 --> 00:26:59,020
Всякий раз, когда была хоть капля неопределенности, то есть,

446
00:26:59,020 --> 00:27:02,360
по существу, существовало всего две возможности, иногда требовалось

447
00:27:02,360 --> 00:27:03,940
еще одно предположение, иногда требовалось еще два предположения.

448
00:27:03,940 --> 00:27:05,980
И так далее и тому подобное здесь.

449
00:27:05,980 --> 00:27:11,020
Возможно, более простой способ визуализировать эти данные — объединить их и взять средние значения.

450
00:27:11,020 --> 00:27:15,940
Например, эта полоса говорит о том, что среди всех точек, где у нас была

451
00:27:15,940 --> 00:27:22,420
одна доля неопределенности, в среднем количество требуемых новых предположений составляло около 1. 5.

452
00:27:22,420 --> 00:27:25,920
И вот эта полоска говорит о том, что среди всех разных

453
00:27:25,920 --> 00:27:30,480
игр, где в какой-то момент неопределенность была чуть выше четырех бит,

454
00:27:30,480 --> 00:27:35,120
что похоже на сужение ее до 16 различных возможностей, то в

455
00:27:35,120 --> 00:27:36,240
среднем с этой точки требуется чуть больше двух предположений. вперед.

456
00:27:36,240 --> 00:27:40,080
И отсюда я просто выполнил регрессию, чтобы соответствовать функции, которая показалась мне разумной.

457
00:27:40,080 --> 00:27:44,160
И помните, что весь смысл всего этого заключается в том, чтобы мы могли количественно оценить

458
00:27:44,160 --> 00:27:49,720
эту интуицию: чем больше информации мы получаем от слова, тем ниже будет ожидаемая оценка.

459
00:27:49,720 --> 00:27:54,380
Итак, это версия 2. 0, если мы вернемся назад и запустим тот же

460
00:27:54,380 --> 00:27:59,820
набор симуляций, используя все 2315 возможных словесных ответов, как это произойдет?

461
00:27:59,820 --> 00:28:04,060
Ну, в отличие от нашей первой версии, она определенно лучше, и это обнадеживает.

462
00:28:04,060 --> 00:28:08,780
В целом средний балл составляет около 3. 6, хотя в отличие от первой версии

463
00:28:08,780 --> 00:28:12,820
есть пару моментов, когда она проигрывает и требует больше шести в данном случае.

464
00:28:12,820 --> 00:28:15,980
Вероятно, потому, что бывают случаи, когда он идет на

465
00:28:15,980 --> 00:28:18,980
компромисс, чтобы действительно достичь цели, а не максимизировать информацию.

466
00:28:18,980 --> 00:28:22,140
Так можем ли мы сделать лучше, чем 3? 6?

467
00:28:22,140 --> 00:28:23,260
Мы определенно можем.

468
00:28:23,260 --> 00:28:27,120
В начале я сказал, что очень интересно попытаться не включать

469
00:28:27,120 --> 00:28:29,980
настоящий список словесных ответов в способ построения своей модели.

470
00:28:29,980 --> 00:28:35,180
Но если мы добавим это, лучший результат, который я мог бы получить, был около 3. 43.

471
00:28:35,180 --> 00:28:39,520
Итак, если мы попытаемся пойти более изощренно, чем просто использовать данные о частоте слов, чтобы выбрать это

472
00:28:39,520 --> 00:28:44,220
априорное распределение, это 3. 43, вероятно, дает максимальную оценку того, насколько хорошо мы могли

473
00:28:44,220 --> 00:28:46,360
бы добиться этого, или, по крайней мере, насколько хорошо я мог бы добиться этого.

474
00:28:46,360 --> 00:28:50,240
Эта лучшая производительность, по сути, просто использует идеи, о которых я

475
00:28:50,240 --> 00:28:53,400
здесь говорил, но она идет немного дальше, например, она ищет

476
00:28:53,400 --> 00:28:55,660
ожидаемую информацию на два шага вперед, а не на один.

477
00:28:55,660 --> 00:28:58,720
Изначально я планировал поговорить об этом подробнее, но понимаю, что

478
00:28:58,720 --> 00:29:00,580
на самом деле мы и так зашли слишком далеко.

479
00:29:00,580 --> 00:29:03,520
Единственное, что я скажу, это то, что после этого двухэтапного поиска, а

480
00:29:03,520 --> 00:29:07,720
затем запуска пары выборочных симуляций с лучшими кандидатами, по крайней мере, на

481
00:29:07,720 --> 00:29:09,500
данный момент для меня это выглядит так, будто Крейн - лучший дебют.

482
00:29:09,500 --> 00:29:11,080
Кто бы мог подумать?

483
00:29:11,080 --> 00:29:15,680
Кроме того, если вы используете настоящий список слов для определения пространства своих

484
00:29:15,680 --> 00:29:17,920
возможностей, то неопределенность, с которой вы начнете, составит немногим более 11 бит.

485
00:29:18,160 --> 00:29:22,760
И оказывается, что при простом переборе максимально возможная ожидаемая

486
00:29:22,760 --> 00:29:26,580
информация после первых двух предположений составляет около 10 бит.

487
00:29:26,580 --> 00:29:31,720
Это предполагает, что в лучшем случае после первых двух предположений при

488
00:29:31,720 --> 00:29:35,220
совершенно оптимальной игре у вас останется примерно одна доля неопределенности.

489
00:29:35,220 --> 00:29:37,400
Это то же самое, что ограничиться двумя возможными предположениями.

490
00:29:37,400 --> 00:29:41,440
Поэтому я думаю, что будет справедливо и, возможно, довольно консервативно сказать, что вы никогда

491
00:29:41,440 --> 00:29:45,620
не сможете написать алгоритм, который бы достигал такого низкого среднего значения, как 3, потому

492
00:29:45,620 --> 00:29:50,460
что с доступными вам словами просто не хватит места, чтобы получить достаточно информации всего

493
00:29:50,460 --> 00:29:53,820
за два шага. способен гарантировать ответ в третьем слоте каждый раз в обязательном порядке.

