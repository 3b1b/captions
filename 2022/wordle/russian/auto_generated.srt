1
00:00:00,000 --> 00:00:03,007
Игра Wurdle стала довольно вирусной за последние месяц или два, и, 

2
00:00:03,007 --> 00:00:06,599
никогда не упуская возможности провести урок математики, мне приходит в голову, 

3
00:00:06,599 --> 00:00:09,741
что эта игра может служить очень хорошим центральным примером в уроке 

4
00:00:09,741 --> 00:00:12,660
по теории информации, и в частности тема, известная как энтропия.

5
00:00:13,920 --> 00:00:16,650
Видите ли, как и многие люди, я был втянут в эту головоломку, и, 

6
00:00:16,650 --> 00:00:19,799
как и многие программисты, я также был втянут в попытки написать алгоритм, 

7
00:00:19,799 --> 00:00:22,740
который будет вести игру настолько оптимально, насколько это возможно.

8
00:00:23,180 --> 00:00:26,721
И я решил здесь просто обсудить с вами часть моего процесса и объяснить часть 

9
00:00:26,721 --> 00:00:28,855
математических расчетов, которые в него вошли, 

10
00:00:28,855 --> 00:00:31,080
поскольку весь алгоритм основан на идее энтропии.

11
00:00:38,700 --> 00:00:41,640
Перво-наперво, если вы еще об этом не слышали, что такое Wurdle?

12
00:00:42,040 --> 00:00:45,110
И чтобы убить двух зайцев одним выстрелом, пока мы изучаем правила игры, 

13
00:00:45,110 --> 00:00:47,717
позвольте мне также просмотреть, чего мы собираемся добиться, 

14
00:00:47,717 --> 00:00:51,040
а именно разработать небольшой алгоритм, который, по сути, будет играть за нас.

15
00:00:51,360 --> 00:00:53,879
Хоть я и не участвовал в сегодняшнем Wurdle, сегодня 4 февраля, 

16
00:00:53,879 --> 00:00:55,100
и посмотрим, как справится бот.

17
00:00:55,480 --> 00:00:58,220
Цель Wurdle — угадать загадочное слово из пяти букв, 

18
00:00:58,220 --> 00:01:00,340
и вам дается шесть разных шансов угадать.

19
00:01:00,840 --> 00:01:04,379
Например, мой бот Wurdle предлагает мне начать с угадывающего крана.

20
00:01:05,180 --> 00:01:08,240
Каждый раз, когда вы делаете предположение, вы получаете некоторую информацию о том, 

21
00:01:08,240 --> 00:01:10,220
насколько ваше предположение близко к истинному ответу.

22
00:01:10,920 --> 00:01:14,100
Здесь серый прямоугольник говорит мне, что в реальном ответе нет буквы C.

23
00:01:14,520 --> 00:01:17,840
Желтый квадрат сообщает мне, что есть буква R, но она не в этом положении.

24
00:01:18,240 --> 00:01:20,320
Зеленый квадрат сообщает мне, что в секретном слове 

25
00:01:20,320 --> 00:01:22,240
есть буква А и она находится на третьей позиции.

26
00:01:22,720 --> 00:01:24,580
И тогда нет ни Н, ни Е.

27
00:01:25,200 --> 00:01:27,340
Так что позвольте мне просто войти и сообщить эту информацию боту Wurdle.

28
00:01:27,340 --> 00:01:30,320
Начали с журавля, получили серый, желтый, зеленый, серый, серый.

29
00:01:31,420 --> 00:01:33,949
Не беспокойтесь обо всех данных, которые он показывает прямо сейчас, 

30
00:01:33,949 --> 00:01:34,940
я объясню это в свое время.

31
00:01:35,460 --> 00:01:38,820
Но главный совет для нашего второго выбора — это ерунда.

32
00:01:39,560 --> 00:01:42,480
И ваше предположение действительно должно состоять из пяти букв, но, как вы увидите, 

33
00:01:42,480 --> 00:01:45,400
оно довольно либерально в отношении того, что оно на самом деле позволит вам угадать.

34
00:01:46,200 --> 00:01:47,440
В этом случае мы пробуем фишку.

35
00:01:48,780 --> 00:01:50,180
И ладно, дела обстоят неплохо.

36
00:01:50,260 --> 00:01:53,980
Мы нажимаем S и H, поэтому мы знаем первые три буквы, мы знаем, что есть R.

37
00:01:53,980 --> 00:01:58,700
И это будет похоже на SHA что-то R или SHA R что-то.

38
00:01:59,620 --> 00:02:04,240
И похоже, что бот Wurdle знает, что есть всего два варианта: осколочный или острый.

39
00:02:05,100 --> 00:02:07,574
На данный момент между ними возникает своего рода спор, так что я думаю, что, 

40
00:02:07,574 --> 00:02:10,080
вероятно, просто потому, что это алфавитный порядок, это соответствует осколку.

41
00:02:11,220 --> 00:02:12,860
Какое ура, это настоящий ответ.

42
00:02:12,960 --> 00:02:13,780
Итак, мы получили его за три.

43
00:02:14,600 --> 00:02:18,093
Если вам интересно, хорошо ли это, то я услышал от одного человека фразу: 

44
00:02:18,093 --> 00:02:20,360
для Уёрдла четыре — это номинал, а три — птичка.

45
00:02:20,680 --> 00:02:22,480
Я думаю, это довольно удачная аналогия.

46
00:02:22,480 --> 00:02:25,709
Чтобы получить четыре, вам нужно постоянно работать над своей игрой, 

47
00:02:25,709 --> 00:02:27,020
но это, конечно, не безумие.

48
00:02:27,180 --> 00:02:29,920
Но когда ты получаешь это за три, это просто здорово.

49
00:02:30,880 --> 00:02:33,221
Так что, если вы не против, то я хотел бы с самого начала просто 

50
00:02:33,221 --> 00:02:35,960
рассказать о своем мыслительном процессе о том, как я подхожу к боту Wurdle.

51
00:02:36,480 --> 00:02:39,440
И, как я уже сказал, на самом деле это повод для урока теории информации.

52
00:02:39,740 --> 00:02:42,820
Основная цель — объяснить, что такое информация и что такое энтропия.

53
00:02:48,220 --> 00:02:50,708
Моей первой мыслью при подходе к этому было взглянуть на 

54
00:02:50,708 --> 00:02:53,720
относительную частоту употребления различных букв в английском языке.

55
00:02:54,380 --> 00:02:56,934
Итак, я подумал: ладно, есть ли начальная догадка или пара начальных догадок, 

56
00:02:56,934 --> 00:02:59,260
которая соответствует многим из этих наиболее часто встречающихся букв?

57
00:02:59,960 --> 00:03:03,000
И мне очень понравилось делать другое, а затем гвозди.

58
00:03:03,760 --> 00:03:06,791
Идея в том, что если вы нажмете на букву, вы получите зеленый или желтый цвет, 

59
00:03:06,791 --> 00:03:07,520
это всегда приятно.

60
00:03:07,520 --> 00:03:08,840
Такое ощущение, что ты получаешь информацию.

61
00:03:09,340 --> 00:03:12,449
Но в этих случаях, даже если вы не попадаете и всегда получаете серый цвет, 

62
00:03:12,449 --> 00:03:15,927
это все равно дает вам много информации, поскольку довольно редко можно найти слово, 

63
00:03:15,927 --> 00:03:17,400
в котором нет ни одной из этих букв.

64
00:03:18,140 --> 00:03:20,941
Но даже несмотря на это, это не кажется суперсистематическим, 

65
00:03:20,941 --> 00:03:23,200
потому что, например, порядок букв не учитывается.

66
00:03:23,560 --> 00:03:25,300
Зачем печатать гвозди, если можно печатать улитку?

67
00:03:26,080 --> 00:03:27,500
Лучше ли иметь букву S в конце?

68
00:03:27,820 --> 00:03:28,680
Я не совсем уверен.

69
00:03:29,240 --> 00:03:32,245
Мой друг сказал, что ему нравится начинать со слова «усталый», 

70
00:03:32,245 --> 00:03:36,540
что меня немного удивило, потому что в нем есть несколько необычных букв, таких как W и Y.

71
00:03:37,120 --> 00:03:39,000
Но кто знает, может быть, это лучший дебют.

72
00:03:39,320 --> 00:03:41,843
Есть ли какая-то количественная оценка, по которой мы 

73
00:03:41,843 --> 00:03:44,320
можем судить о качестве потенциального предположения?

74
00:03:45,340 --> 00:03:48,273
Теперь, чтобы настроить способ ранжирования возможных предположений, 

75
00:03:48,273 --> 00:03:51,420
давайте вернемся и добавим немного ясности в то, как именно устроена игра.

76
00:03:51,420 --> 00:03:54,577
Итак, есть список слов, которые он позволит вам ввести и которые 

77
00:03:54,577 --> 00:03:57,880
считаются действительными догадками, длиной всего около 13 000 слов.

78
00:03:58,320 --> 00:04:02,138
Но когда вы посмотрите на это, то увидите много действительно необычных вещей, 

79
00:04:02,138 --> 00:04:06,440
таких как голова или Али и ARG, слова, которые вызывают семейные споры в игре в «Эрудит».

80
00:04:06,960 --> 00:04:10,540
Но суть игры в том, что ответом всегда будет достаточно обычное слово.

81
00:04:10,960 --> 00:04:13,698
И на самом деле, есть еще один список из примерно 2300 слов, 

82
00:04:13,698 --> 00:04:15,360
которые являются возможными ответами.

83
00:04:15,940 --> 00:04:19,982
И это список, составленный людьми, я думаю, конкретно девушкой создателя игры, 

84
00:04:19,982 --> 00:04:21,160
и это довольно забавно.

85
00:04:21,820 --> 00:04:25,171
Но что я хотел бы сделать, так это то, что наша задача в этом проекте состоит в том, 

86
00:04:25,171 --> 00:04:27,853
чтобы посмотреть, сможем ли мы написать программу, решающую Wordle, 

87
00:04:27,853 --> 00:04:30,180
которая не будет включать предыдущие знания об этом списке.

88
00:04:30,720 --> 00:04:33,391
Во-первых, существует множество довольно распространенных слов из пяти букв, 

89
00:04:33,391 --> 00:04:34,640
которых вы не найдете в этом списке.

90
00:04:34,940 --> 00:04:36,962
Поэтому было бы лучше написать программу, которая была бы 

91
00:04:36,962 --> 00:04:39,472
немного более устойчивой и могла бы играть в Wordle против кого угодно, 

92
00:04:39,472 --> 00:04:41,460
а не только против того, что является официальным сайтом.

93
00:04:41,920 --> 00:04:44,568
А также причина, по которой мы знаем, что представляет собой этот список 

94
00:04:44,568 --> 00:04:47,000
возможных ответов, заключается в том, что он виден в исходном коде.

95
00:04:47,000 --> 00:04:50,646
Но в исходном коде это отображается в определенном порядке, 

96
00:04:50,646 --> 00:04:53,260
в котором ответы появляются изо дня в день.

97
00:04:53,260 --> 00:04:55,840
Так что вы всегда можете просто посмотреть, каким будет завтрашний ответ.

98
00:04:56,420 --> 00:04:58,880
Итак, очевидно, что в некотором смысле использование списка является мошенничеством.

99
00:04:59,100 --> 00:05:02,542
И что делает головоломку более интересной и более насыщенный урок теории информации, 

100
00:05:02,542 --> 00:05:05,499
так это использование вместо этого некоторых более универсальных данных, 

101
00:05:05,499 --> 00:05:08,415
таких как относительная частота слов в целом, чтобы уловить интуитивное 

102
00:05:08,415 --> 00:05:10,440
ощущение предпочтения более распространенных слов.

103
00:05:11,600 --> 00:05:15,900
Итак, из этих 13 000 возможностей, как нам выбрать первую догадку?

104
00:05:16,400 --> 00:05:18,108
Например, если мой друг предлагает усталость, 

105
00:05:18,108 --> 00:05:19,780
как нам следует проанализировать ее качество?

106
00:05:20,520 --> 00:05:22,984
Ну, причина, по которой он сказал, что ему нравится эта маловероятная W, 

107
00:05:22,984 --> 00:05:24,909
заключается в том, что ему нравится дальновидность того, 

108
00:05:24,909 --> 00:05:27,340
насколько приятно чувствовать себя, если вы действительно нажмете эту W.

109
00:05:27,920 --> 00:05:31,476
Например, если первая выявленная закономерность была примерно такой, 

110
00:05:31,476 --> 00:05:35,600
то в этом гигантском словаре всего 58 слов, соответствующих этой закономерности.

111
00:05:36,060 --> 00:05:38,400
Так что это огромное сокращение по сравнению с 13 000.

112
00:05:38,780 --> 00:05:41,222
Но обратной стороной этого, конечно же, является то, 

113
00:05:41,222 --> 00:05:43,020
что такой узор встречается очень редко.

114
00:05:43,020 --> 00:05:46,898
В частности, если бы каждое слово с одинаковой вероятностью было ответом, 

115
00:05:46,898 --> 00:05:51,040
вероятность попадания в этот шаблон была бы 58, разделенная примерно на 13 000.

116
00:05:51,580 --> 00:05:53,600
Конечно, они не в равной степени могут быть ответами.

117
00:05:53,720 --> 00:05:56,220
Большинство из них – очень неясные и даже сомнительные слова.

118
00:05:56,600 --> 00:05:59,195
Но, по крайней мере, для нашего первого этапа, давайте предположим, 

119
00:05:59,195 --> 00:06:01,600
что все они одинаково вероятны, а затем уточним это чуть позже.

120
00:06:02,020 --> 00:06:06,720
Дело в том, что паттерн с большим количеством информации по своей природе маловероятен.

121
00:06:07,280 --> 00:06:10,800
На самом деле, быть информативным означает то, что это маловероятно.

122
00:06:11,719 --> 00:06:15,377
Гораздо более вероятной моделью, которую можно увидеть в этом открытии, 

123
00:06:15,377 --> 00:06:18,120
было бы что-то вроде этого, где, конечно, нет буквы W.

124
00:06:18,240 --> 00:06:21,400
Может быть, есть E, а может быть, нет A, нет R, нет Y.

125
00:06:22,080 --> 00:06:24,560
В этом случае существует 1400 возможных совпадений.

126
00:06:25,080 --> 00:06:27,743
Если бы все были одинаково вероятны, вероятность того, 

127
00:06:27,743 --> 00:06:30,600
что вы увидите именно такую модель, составила бы около 11%.

128
00:06:30,900 --> 00:06:33,340
Таким образом, наиболее вероятные результаты являются и наименее информативными.

129
00:06:34,240 --> 00:06:36,158
Чтобы получить более глобальное представление, 

130
00:06:36,158 --> 00:06:39,384
позвольте мне показать вам полное распределение вероятностей по всем различным 

131
00:06:39,384 --> 00:06:41,140
закономерностям, которые вы можете увидеть.

132
00:06:41,740 --> 00:06:43,908
Таким образом, каждая полоса, на которую вы смотрите, 

133
00:06:43,908 --> 00:06:46,758
соответствует возможному набору цветов, которые могут быть обнаружены, 

134
00:06:46,758 --> 00:06:49,930
из которых существует от 3 до 5 возможностей, и они расположены слева направо, 

135
00:06:49,930 --> 00:06:52,340
от наиболее распространенного до наименее распространенного.

136
00:06:52,920 --> 00:06:55,061
Таким образом, наиболее распространенной возможностью здесь является то, 

137
00:06:55,061 --> 00:06:56,000
что вы получите все серые цвета.

138
00:06:56,100 --> 00:06:58,120
Это происходит примерно в 14% случаев.

139
00:06:58,580 --> 00:07:01,220
И когда вы делаете предположение, вы надеетесь на то, 

140
00:07:01,220 --> 00:07:03,908
что окажетесь где-то в этом длинном хвосте, как здесь, 

141
00:07:03,908 --> 00:07:07,575
где есть только 18 возможностей для того, что соответствует этому шаблону, 

142
00:07:07,575 --> 00:07:09,140
который, очевидно, выглядит так.

143
00:07:09,920 --> 00:07:13,800
Или, если мы рискнем пойти немного левее, знаете, может быть, мы пройдем весь путь сюда.

144
00:07:14,940 --> 00:07:16,180
Хорошо, вот вам хорошая головоломка.

145
00:07:16,540 --> 00:07:19,532
Какие три слова в английском языке начинаются с буквы W, 

146
00:07:19,532 --> 00:07:22,000
заканчиваются на Y и где-то в них есть буква R?

147
00:07:22,480 --> 00:07:26,800
Оказывается, ответы, посмотрим, многословные, червивые и кривые.

148
00:07:27,500 --> 00:07:30,296
Итак, чтобы судить, насколько хорошо это слово в целом, 

149
00:07:30,296 --> 00:07:32,993
нам нужна какая-то мера ожидаемого объема информации, 

150
00:07:32,993 --> 00:07:35,740
которую вы собираетесь получить от этого распределения.

151
00:07:35,740 --> 00:07:40,075
Если мы пройдемся по каждому шаблону и умножим вероятность его появления на что-то, 

152
00:07:40,075 --> 00:07:44,720
что измеряет, насколько он информативен, это, возможно, может дать нам объективную оценку.

153
00:07:45,960 --> 00:07:48,559
Теперь вашим первым инстинктом того, каким должно быть это что-то, 

154
00:07:48,559 --> 00:07:49,840
может быть количество совпадений.

155
00:07:50,160 --> 00:07:52,400
Вам нужно меньшее среднее количество совпадений.

156
00:07:52,800 --> 00:07:55,978
Но вместо этого я хотел бы использовать более универсальное измерение, 

157
00:07:55,978 --> 00:07:59,201
которое мы часто приписываем информации, и которое станет более гибким, 

158
00:07:59,201 --> 00:08:02,558
когда каждому из этих 13 000 слов будет назначена разная вероятность того, 

159
00:08:02,558 --> 00:08:04,260
являются ли они на самом деле ответом.

160
00:08:10,320 --> 00:08:13,713
Стандартной единицей информации является бит, формула которого немного забавна, 

161
00:08:13,713 --> 00:08:16,980
но она действительно интуитивно понятна, если мы просто посмотрим на примеры.

162
00:08:17,780 --> 00:08:21,392
Если у вас есть наблюдение, которое сокращает ваше пространство возможностей вдвое, 

163
00:08:21,392 --> 00:08:23,500
мы говорим, что оно содержит один бит информации.

164
00:08:24,180 --> 00:08:27,518
В нашем примере пространство возможностей — это все возможные слова, и получается, 

165
00:08:27,518 --> 00:08:30,415
что примерно половина из пятибуквенных слов имеет букву S, чуть меньше, 

166
00:08:30,415 --> 00:08:31,260
но примерно половина.

167
00:08:31,780 --> 00:08:34,320
Таким образом, это наблюдение даст вам один бит информации.

168
00:08:34,880 --> 00:08:39,092
Если вместо этого новый факт сокращает это пространство возможностей в четыре раза, 

169
00:08:39,092 --> 00:08:41,500
мы говорим, что он содержит два бита информации.

170
00:08:41,980 --> 00:08:44,460
Например, оказывается, что около четверти этих слов имеют букву Т.

171
00:08:45,020 --> 00:08:47,578
Если наблюдение сокращает это пространство в восемь раз, 

172
00:08:47,578 --> 00:08:50,720
мы говорим, что это три бита информации, и так далее, и тому подобное.

173
00:08:50,900 --> 00:08:55,060
Четыре бита превращают его в 16-й, пять битов — в 32-й.

174
00:08:55,060 --> 00:08:58,222
Итак, теперь вы, возможно, захотите сделать паузу и спросить себя, 

175
00:08:58,222 --> 00:09:02,282
какова формула информации о количестве битов с точки зрения вероятности возникновения 

176
00:09:02,282 --> 00:09:02,660
события?

177
00:09:02,660 --> 00:09:06,784
Мы здесь говорим о том, что когда вы принимаете половину числа битов, это то же самое, 

178
00:09:06,784 --> 00:09:09,201
что и вероятность, а это то же самое, что сказать, 

179
00:09:09,201 --> 00:09:12,804
что двойка в степени числа битов равна единице по сравнению с вероятностью, 

180
00:09:12,804 --> 00:09:16,928
что далее перестраивается так, что информация представляет собой логарифм по основанию 

181
00:09:16,928 --> 00:09:18,920
два из одного, разделенный на вероятность.

182
00:09:19,620 --> 00:09:21,561
И иногда вы видите это с еще одной перестановкой, 

183
00:09:21,561 --> 00:09:24,900
где информация представляет собой отрицательный логарифм по основанию два вероятности.

184
00:09:25,660 --> 00:09:29,299
Выраженный таким образом, это может показаться немного странным для непосвященных, 

185
00:09:29,299 --> 00:09:31,975
но на самом деле это просто очень интуитивная идея спросить, 

186
00:09:31,975 --> 00:09:34,080
сколько раз вы сократили свои возможности вдвое.

187
00:09:35,180 --> 00:09:37,224
Теперь, если вам интересно, знаете, я думал, что мы просто играем 

188
00:09:37,224 --> 00:09:39,300
в забавную словесную игру, почему на картинке появляются логарифмы?

189
00:09:39,780 --> 00:09:43,342
Одна из причин, по которой эта единица более удобна, заключается в том, 

190
00:09:43,342 --> 00:09:47,448
что гораздо проще говорить об очень маловероятных событиях, гораздо проще сказать, 

191
00:09:47,448 --> 00:09:50,218
что наблюдение содержит 20 бит информации, чем сказать, 

192
00:09:50,218 --> 00:09:52,940
что вероятность того или иного события равна 0.0000095.

193
00:09:53,300 --> 00:09:57,264
Но более существенная причина того, что это логарифмическое выражение оказалось очень 

194
00:09:57,264 --> 00:10:00,169
полезным дополнением к теории вероятностей, заключается в том, 

195
00:10:00,169 --> 00:10:01,460
как информация складывается.

196
00:10:02,060 --> 00:10:05,246
Например, если одно наблюдение дает вам два бита информации, 

197
00:10:05,246 --> 00:10:08,851
сокращая ваше пространство в четыре раза, а затем второе наблюдение, 

198
00:10:08,851 --> 00:10:13,030
такое как ваше второе предположение в Wordle, дает вам еще три бита информации, 

199
00:10:13,030 --> 00:10:16,740
сокращая вас еще в восемь раз, два вместе дают вам пять бит информации.

200
00:10:17,160 --> 00:10:21,020
Точно так же, как вероятности любят умножаться, информация любит прибавляться.

201
00:10:21,960 --> 00:10:24,871
Итак, как только мы попадаем в область чего-то вроде ожидаемого значения, 

202
00:10:24,871 --> 00:10:27,980
где мы складываем кучу чисел, с журналами работать становится намного приятнее.

203
00:10:28,480 --> 00:10:32,201
Давайте вернемся к нашему дистрибутиву Weary и добавим сюда еще один небольшой трекер, 

204
00:10:32,201 --> 00:10:34,940
показывающий, сколько информации содержится для каждого шаблона.

205
00:10:35,580 --> 00:10:37,478
Главное, что я хочу, чтобы вы заметили, это то, 

206
00:10:37,478 --> 00:10:40,722
что чем выше вероятность того, что мы доберемся до этих более вероятных шаблонов, 

207
00:10:40,722 --> 00:10:42,780
тем меньше информации, тем меньше битов вы получите.

208
00:10:43,500 --> 00:10:47,035
Чтобы измерить качество этого предположения, мы возьмем ожидаемое значение 

209
00:10:47,035 --> 00:10:50,712
этой информации, пройдемся по каждому шаблону, скажем, насколько он вероятен, 

210
00:10:50,712 --> 00:10:54,060
а затем умножим это на количество битов информации, которые мы получим.

211
00:10:54,710 --> 00:10:58,120
А в примере с Вири это число равно 4.9 бит.

212
00:10:58,560 --> 00:11:00,817
Таким образом, в среднем информация, которую вы получаете из 

213
00:11:00,817 --> 00:11:02,704
этого начального предположения, так же эффективна, 

214
00:11:02,704 --> 00:11:05,480
как сокращение пространства ваших возможностей пополам примерно в пять раз.

215
00:11:05,960 --> 00:11:08,899
Напротив, примером предположения с более высокой ожидаемой 

216
00:11:08,899 --> 00:11:11,640
информационной ценностью может быть что-то вроде Slate.

217
00:11:13,120 --> 00:11:15,620
В этом случае вы заметите, что распределение выглядит намного более плоским.

218
00:11:15,940 --> 00:11:20,655
В частности, наиболее вероятное появление всех оттенков серого имеет только около 6% 

219
00:11:20,655 --> 00:11:25,260
вероятности появления, так что как минимум вы получите очевидно 3.9 бит информации.

220
00:11:25,920 --> 00:11:28,560
Но это минимум, чаще всего можно получить что-то получше.

221
00:11:29,100 --> 00:11:33,595
И оказывается, что если подсчитать цифры и сложить все соответствующие термины, 

222
00:11:33,595 --> 00:11:35,900
то средняя информация составит около 5.8.

223
00:11:37,360 --> 00:11:40,500
Таким образом, в отличие от Вири, после первого предположения 

224
00:11:40,500 --> 00:11:43,540
ваше пространство возможностей будет в среднем вдвое меньше.

225
00:11:44,420 --> 00:11:46,770
На самом деле есть забавная история с названием 

226
00:11:46,770 --> 00:11:49,120
этого ожидаемого значения количества информации.

227
00:11:49,200 --> 00:11:51,469
Теория информации была разработана Клодом Шенноном, 

228
00:11:51,469 --> 00:11:55,266
который работал в Bell Labs в 1940-х годах, но о некоторых своих еще не опубликованных 

229
00:11:55,266 --> 00:11:58,758
идеях он говорил с Джоном фон Нейманом, интеллектуальным гигантом того времени, 

230
00:11:58,758 --> 00:12:01,988
очень выдающимся человеком. по математике и физике и положил начало тому, 

231
00:12:01,988 --> 00:12:03,560
что впоследствии стало информатикой.

232
00:12:04,100 --> 00:12:07,346
И когда он упомянул, что на самом деле у него нет хорошего названия для 

233
00:12:07,346 --> 00:12:10,683
этого ожидаемого значения количества информации, фон Нейман якобы сказал, 

234
00:12:10,683 --> 00:12:14,200
что, как гласит история, вы должны называть это энтропией, и по двум причинам.

235
00:12:14,540 --> 00:12:18,566
Во-первых, ваша функция неопределенности использовалась в статистической механике под 

236
00:12:18,566 --> 00:12:22,639
этим именем, поэтому у нее уже есть имя, а во-вторых, что более важно, никто не знает, 

237
00:12:22,639 --> 00:12:26,760
что такое энтропия на самом деле, поэтому в дебатах вы всегда будете иметь преимущество.

238
00:12:27,700 --> 00:12:31,270
Так что, если название кажется немного загадочным и если верить этой истории, 

239
00:12:31,270 --> 00:12:32,460
то это сделано специально.

240
00:12:33,280 --> 00:12:36,442
Кроме того, если вы задаетесь вопросом о его связи со всем этим вторым 

241
00:12:36,442 --> 00:12:39,247
законом термодинамики из физики, связь определенно существует, 

242
00:12:39,247 --> 00:12:42,676
но в ее происхождении Шеннон просто имел дело с чистой теорией вероятностей, 

243
00:12:42,676 --> 00:12:45,972
и для наших целей здесь, когда я использую слово энтропия, я просто хочу, 

244
00:12:45,972 --> 00:12:49,580
чтобы вы подумали об ожидаемой информационной ценности конкретного предположения.

245
00:12:50,700 --> 00:12:53,780
Вы можете думать об энтропии как об измерении двух вещей одновременно.

246
00:12:54,240 --> 00:12:56,780
Во-первых, насколько равномерным является распределение.

247
00:12:57,320 --> 00:13:01,120
Чем ближе распределение к равномерному, тем выше будет энтропия.

248
00:13:01,580 --> 00:13:05,655
В нашем случае, когда общее количество шаблонов составляет от 3 до 5, 

249
00:13:05,655 --> 00:13:10,895
для равномерного распределения наблюдение любого из них будет иметь базу данных журнала 2 

250
00:13:10,895 --> 00:13:14,854
из 3 до 5, что в итоге равно 7.92, так что это абсолютный максимум, 

251
00:13:14,854 --> 00:13:17,300
который вы можете иметь для этой энтропии.

252
00:13:17,840 --> 00:13:22,080
Но энтропия также является своего рода мерой того, сколько возможностей вообще существует.

253
00:13:22,320 --> 00:13:25,695
Например, если у вас есть какое-то слово, в котором существует 

254
00:13:25,695 --> 00:13:28,964
только 16 возможных шаблонов, и каждый из них равновероятен, 

255
00:13:28,964 --> 00:13:32,180
эта энтропия, эта ожидаемая информация, будет равна 4 битам.

256
00:13:32,579 --> 00:13:37,141
Но если у вас есть другое слово, в котором может возникнуть 64 возможных шаблона, 

257
00:13:37,141 --> 00:13:40,480
и все они одинаково вероятны, тогда энтропия составит 6 бит.

258
00:13:41,500 --> 00:13:45,569
Итак, если вы видите какое-то распределение, энтропия которого равна 6 битам, 

259
00:13:45,569 --> 00:13:48,647
это как бы говорит о том, что в том, что должно произойти, 

260
00:13:48,647 --> 00:13:51,308
существует столько же вариаций и неопределенности, 

261
00:13:51,308 --> 00:13:53,500
как если бы было 64 равновероятных исхода.

262
00:13:54,360 --> 00:13:59,320
Для моего первого использования Wurtelebot я просто сделал это.

263
00:13:59,320 --> 00:14:02,473
Он перебирает все возможные предположения, все 13 000 слов, 

264
00:14:02,473 --> 00:14:05,207
вычисляет энтропию для каждого из них, или, точнее, 

265
00:14:05,207 --> 00:14:09,832
энтропию распределения по всем шаблонам, которые вы можете увидеть, для каждого из них, 

266
00:14:09,832 --> 00:14:13,406
и выбирает самое высокое, поскольку это тот, который, скорее всего, 

267
00:14:13,406 --> 00:14:16,140
максимально сократит ваше пространство возможностей.

268
00:14:17,140 --> 00:14:18,883
И хотя я говорил здесь только о первой догадке, 

269
00:14:18,883 --> 00:14:21,100
то же самое происходит и со следующими несколькими догадками.

270
00:14:21,560 --> 00:14:24,878
Например, после того, как вы видите некоторый шаблон в этом первом предположении, 

271
00:14:24,878 --> 00:14:28,116
который ограничит вас меньшим количеством возможных слов в зависимости от того, 

272
00:14:28,116 --> 00:14:31,597
что с ним совпадает, вы просто играете в ту же игру в отношении этого меньшего набора 

273
00:14:31,597 --> 00:14:31,800
слов.

274
00:14:32,260 --> 00:14:36,246
Для предлагаемого второго предположения вы смотрите на распределение всех шаблонов, 

275
00:14:36,246 --> 00:14:39,426
которые могут возникнуть из этого более ограниченного набора слов, 

276
00:14:39,426 --> 00:14:42,178
вы просматриваете все 13 000 возможностей и находите тот, 

277
00:14:42,178 --> 00:14:43,840
который максимизирует эту энтропию.

278
00:14:45,420 --> 00:14:47,751
Чтобы показать вам, как это работает в действии, 

279
00:14:47,751 --> 00:14:51,272
позвольте мне просто привести небольшой вариант написанного мной Вюртеле, 

280
00:14:51,272 --> 00:14:54,080
в котором на полях показаны основные моменты этого анализа.

281
00:14:54,080 --> 00:14:57,218
После выполнения всех расчетов энтропии справа здесь показано, 

282
00:14:57,218 --> 00:14:59,660
какие из них имеют наиболее ожидаемую информацию.

283
00:15:00,280 --> 00:15:04,908
Оказывается, самый популярный ответ, по крайней мере на данный момент, 

284
00:15:04,908 --> 00:15:10,580
мы уточним это позже, — это Тарес, что означает, хм, конечно, вика, самая обычная вика.

285
00:15:11,040 --> 00:15:13,619
Каждый раз, когда мы здесь делаем предположение, что, возможно, 

286
00:15:13,619 --> 00:15:16,803
я игнорирую его рекомендации и использую шифер, потому что мне нравится шифер, 

287
00:15:16,803 --> 00:15:19,180
мы можем видеть, сколько ожидаемой информации он содержал, 

288
00:15:19,180 --> 00:15:22,001
но затем справа от слова здесь показано, сколько реальную информацию, 

289
00:15:22,001 --> 00:15:24,420
которую мы получили, учитывая эту конкретную закономерность.

290
00:15:25,000 --> 00:15:28,162
Так вот тут похоже нам немного не повезло, от нас ожидали 5.8, 

291
00:15:28,162 --> 00:15:30,120
но нам удалось получить что-то меньшее.

292
00:15:30,600 --> 00:15:35,020
А затем, слева, здесь показаны все возможные слова, учитывая, где мы сейчас находимся.

293
00:15:35,800 --> 00:15:38,053
Синие полосы сообщают нам, насколько вероятно, по его мнению, 

294
00:15:38,053 --> 00:15:40,088
каждое слово, поэтому на данный момент он предполагает, 

295
00:15:40,088 --> 00:15:43,360
что каждое слово встречается с одинаковой вероятностью, но мы уточним это через мгновение.

296
00:15:44,060 --> 00:15:47,629
И затем это измерение неопределенности говорит нам об энтропии этого 

297
00:15:47,629 --> 00:15:52,131
распределения возможных слов, которое сейчас, поскольку это равномерное распределение, 

298
00:15:52,131 --> 00:15:55,960
является просто излишне сложным способом подсчета количества возможностей.

299
00:15:56,560 --> 00:15:59,614
Например, если бы мы возвели 2 в 13-ю степень.66, 

300
00:15:59,614 --> 00:16:02,180
это должно быть около 13 000 возможностей.

301
00:16:02,900 --> 00:16:06,140
Я здесь немного не так, но только потому, что не показываю все десятичные знаки.

302
00:16:06,720 --> 00:16:10,008
На данный момент это может показаться излишним и слишком усложняющим ситуацию, 

303
00:16:10,008 --> 00:16:12,340
но вы поймете, почему полезно иметь оба числа за минуту.

304
00:16:12,760 --> 00:16:16,030
Итак, здесь похоже, что самая высокая энтропия для нашего второго 

305
00:16:16,030 --> 00:16:19,400
предположения — это Рамен, что опять-таки просто не похоже на слово.

306
00:16:19,980 --> 00:16:24,060
Итак, чтобы занять здесь моральную позицию, я собираюсь ввести Rains.

307
00:16:25,440 --> 00:16:27,340
И снова похоже, нам немного не повезло.

308
00:16:27,520 --> 00:16:31,360
Мы ожидали 4.3 бита, а мы получили только 3.39 бит информации.

309
00:16:31,940 --> 00:16:33,940
Таким образом, мы получаем 55 возможностей.

310
00:16:34,900 --> 00:16:37,765
И здесь, возможно, я просто воспользуюсь тем, что он предлагает, 

311
00:16:37,765 --> 00:16:39,440
а именно комбо, что бы это ни значило.

312
00:16:40,040 --> 00:16:42,920
И ладно, на самом деле это хороший шанс для головоломки.

313
00:16:42,920 --> 00:16:46,380
Он говорит нам, что этот шаблон дает нам 4.7 бит информации.

314
00:16:47,060 --> 00:16:51,720
Но слева, прежде чем мы увидим эту закономерность, их было 5.78 бит неопределенности.

315
00:16:52,420 --> 00:16:54,359
Итак, в качестве теста для вас: что это значит 

316
00:16:54,359 --> 00:16:56,340
относительно количества оставшихся возможностей?

317
00:16:58,040 --> 00:17:01,440
Ну, это означает, что мы сведены к одной частичке неопределенности, 

318
00:17:01,440 --> 00:17:04,540
а это то же самое, что сказать, что есть два возможных ответа.

319
00:17:04,700 --> 00:17:05,700
Это выбор 50 на 50.

320
00:17:06,500 --> 00:17:09,068
И отсюда, поскольку мы с вами знаем, какие слова встречаются чаще, 

321
00:17:09,068 --> 00:17:10,640
мы знаем, что ответ должен быть пропасть.

322
00:17:11,180 --> 00:17:13,280
Но как сейчас написано, программа этого не знает.

323
00:17:13,540 --> 00:17:17,074
Поэтому он просто продолжает работать, пытаясь получить как можно больше информации, 

324
00:17:17,074 --> 00:17:19,859
пока не останется только одна возможность, а затем он ее угадывает.

325
00:17:20,380 --> 00:17:22,339
Поэтому очевидно, что нам нужна лучшая стратегия эндшпиля.

326
00:17:22,599 --> 00:17:25,429
Но предположим, что мы назовем эту версию одним из наших решателей слов, 

327
00:17:25,429 --> 00:17:28,260
а затем запустим несколько симуляций, чтобы посмотреть, как она работает.

328
00:17:30,360 --> 00:17:34,120
Итак, это работает так: он играет во все возможные словесные игры.

329
00:17:34,240 --> 00:17:38,540
Он проходит через все эти 2315 слов, которые являются реальными ответами.

330
00:17:38,540 --> 00:17:40,580
По сути, он использует это как набор для тестирования.

331
00:17:41,360 --> 00:17:44,795
И с помощью этого наивного метода не учитывать, насколько распространено слово, 

332
00:17:44,795 --> 00:17:47,672
а просто пытаться максимизировать информацию на каждом этапе пути, 

333
00:17:47,672 --> 00:17:49,820
пока не дойдет до одного и только одного варианта.

334
00:17:50,360 --> 00:17:54,300
К концу моделирования средний балл составит около 4.124.

335
00:17:55,319 --> 00:17:59,240
Что неплохо, если честно, я ожидал худшего результата.

336
00:17:59,660 --> 00:18:02,600
Но люди, играющие в Wordle, скажут вам, что обычно они могут получить это за 4.

337
00:18:02,860 --> 00:18:05,380
Настоящая задача — собрать как можно больше очков из 3.

338
00:18:05,380 --> 00:18:08,080
Это довольно большой скачок между оценкой 4 и оценкой 3.

339
00:18:08,860 --> 00:18:11,766
Очевидный результат здесь — каким-то образом определить, 

340
00:18:11,766 --> 00:18:14,980
является ли слово распространенным, и как именно мы это делаем.

341
00:18:22,800 --> 00:18:25,926
Я подошел к этому с целью получить список относительных 

342
00:18:25,926 --> 00:18:27,880
частот всех слов английского языка.

343
00:18:28,220 --> 00:18:31,095
И я только что использовал функцию данных частоты слов Mathematica, 

344
00:18:31,095 --> 00:18:34,860
которая сама извлекает данные из общедоступного набора данных Google Books English Ngram.

345
00:18:35,460 --> 00:18:37,728
И на это интересно смотреть, например, если мы отсортируем его 

346
00:18:37,728 --> 00:18:39,960
от наиболее распространенных слов к наименее распространенным.

347
00:18:40,120 --> 00:18:43,080
Очевидно, это самые распространенные слова из 5 букв в английском языке.

348
00:18:43,700 --> 00:18:45,840
Вернее, это 8-е место по распространенности.

349
00:18:46,280 --> 00:18:48,880
Сначала есть что, после чего там и там.

350
00:18:49,260 --> 00:18:52,085
Первое само по себе не первое, а девятое, и имеет смысл, 

351
00:18:52,085 --> 00:18:54,713
что эти другие слова могут встречаться чаще, где те, 

352
00:18:54,713 --> 00:18:58,580
что после первого, находятся после, где, и эти слова встречаются немного реже.

353
00:18:59,160 --> 00:19:02,139
Теперь, используя эти данные для моделирования вероятности того, 

354
00:19:02,139 --> 00:19:04,568
что каждое из этих слов будет окончательным ответом, 

355
00:19:04,568 --> 00:19:06,860
оно не должно быть просто пропорционально частоте.

356
00:19:06,860 --> 00:19:10,511
Например, которому присвоен балл 0.002 в этом наборе данных, 

357
00:19:10,511 --> 00:19:15,060
тогда как слово «коса» в каком-то смысле примерно в 1000 раз менее вероятно.

358
00:19:15,560 --> 00:19:18,840
Но оба эти слова достаточно распространены, поэтому их почти наверняка стоит рассмотреть.

359
00:19:19,340 --> 00:19:21,000
Поэтому нам нужно больше бинарного отсечения.

360
00:19:21,860 --> 00:19:25,129
Я представил, что беру весь этот отсортированный список слов, 

361
00:19:25,129 --> 00:19:28,768
затем располагаю его по оси X, а затем применяю сигмовидную функцию, 

362
00:19:28,768 --> 00:19:31,721
которая является стандартным способом получить функцию, 

363
00:19:31,721 --> 00:19:34,621
вывод которой в основном двоичный, это либо 0, либо 1, 

364
00:19:34,621 --> 00:19:38,260
но в этой области неопределенности между ними происходит сглаживание.

365
00:19:39,160 --> 00:19:43,800
По сути, вероятность того, что я назначаю каждому слову попадание в окончательный список, 

366
00:19:43,800 --> 00:19:48,440
будет значением сигмовидной функции, указанной выше, где бы оно ни располагалось на оси X.

367
00:19:49,520 --> 00:19:52,728
Очевидно, это зависит от нескольких параметров, например, 

368
00:19:52,728 --> 00:19:56,767
насколько широкое пространство на оси X заполняют эти слова, определяет, 

369
00:19:56,767 --> 00:20:00,141
насколько постепенно или круто мы снижаемся от 1 до 0, а то, 

370
00:20:00,141 --> 00:20:03,240
где мы располагаем их слева направо, определяет границу.

371
00:20:03,240 --> 00:20:06,920
Честно говоря, я просто облизнул палец и высунул его по ветру.

372
00:20:07,140 --> 00:20:10,577
Я просмотрел отсортированный список и попытался найти окно, в котором, 

373
00:20:10,577 --> 00:20:13,967
посмотрев на него, я понял, что около половины этих слов скорее всего 

374
00:20:13,967 --> 00:20:17,260
будут окончательным ответом, и использовал его в качестве отсечения.

375
00:20:17,260 --> 00:20:21,128
Как только мы получим такое распределение по словам, это даст нам еще одну ситуацию, 

376
00:20:21,128 --> 00:20:23,860
когда энтропия становится действительно полезным измерением.

377
00:20:24,500 --> 00:20:28,130
Например, предположим, что мы играли в игру и начинаем с моих старых открывашек, 

378
00:20:28,130 --> 00:20:30,550
которыми были перо и гвозди, и заканчиваем ситуацией, 

379
00:20:30,550 --> 00:20:33,240
когда есть четыре возможных слова, которые ей соответствуют.

380
00:20:33,560 --> 00:20:35,620
И допустим, мы считаем их всех одинаково вероятными.

381
00:20:36,220 --> 00:20:38,880
Позвольте мне спросить вас, какова энтропия этого распределения?

382
00:20:41,080 --> 00:20:44,762
Что ж, информация, связанная с каждой из этих возможностей, 

383
00:20:44,762 --> 00:20:50,040
будет иметь логарифмическую базу 2 из 4, поскольку каждая из них равна 1 и 4, а это 2.

384
00:20:50,040 --> 00:20:52,460
Два бита информации, четыре возможности.

385
00:20:52,760 --> 00:20:53,580
Все очень хорошо и хорошо.

386
00:20:54,300 --> 00:20:57,800
Но что, если я скажу вам, что на самом деле совпадений больше четырех?

387
00:20:58,260 --> 00:21:00,745
На самом деле, когда мы просматриваем полный список слов, 

388
00:21:00,745 --> 00:21:02,460
мы видим, что ему соответствуют 16 слов.

389
00:21:02,580 --> 00:21:05,507
Но предположим, что наша модель дает очень низкую вероятность того, 

390
00:21:05,507 --> 00:21:08,349
что остальные 12 слов действительно станут окончательным ответом, 

391
00:21:08,349 --> 00:21:10,760
примерно 1 из 1000, потому что они действительно неясны.

392
00:21:11,500 --> 00:21:14,260
Теперь позвольте мне спросить вас, какова энтропия этого распределения?

393
00:21:15,420 --> 00:21:18,252
Если бы энтропия измеряла здесь просто количество совпадений, 

394
00:21:18,252 --> 00:21:22,044
то можно было бы ожидать, что это будет что-то вроде логарифмической базы 2 из 16, 

395
00:21:22,044 --> 00:21:25,700
что будет равно 4, то есть на два бита неопределенности больше, чем было раньше.

396
00:21:26,180 --> 00:21:29,282
Но, конечно, фактическая неопределенность на самом деле не сильно отличается от того, 

397
00:21:29,282 --> 00:21:29,860
что было раньше.

398
00:21:30,160 --> 00:21:33,290
Тот факт, что есть эти 12 действительно непонятных слов, не означает, 

399
00:21:33,290 --> 00:21:35,615
что было бы еще более удивительно узнать, например, 

400
00:21:35,615 --> 00:21:37,360
что окончательный ответ — «очарование».

401
00:21:38,180 --> 00:21:41,915
Итак, когда вы на самом деле выполняете вычисления здесь и складываете вероятность 

402
00:21:41,915 --> 00:21:45,560
каждого события, умноженную на соответствующую информацию, вы получаете 2.11 бит.

403
00:21:45,560 --> 00:21:49,206
Я просто говорю, что это по сути два бита, в основном эти четыре возможности, 

404
00:21:49,206 --> 00:21:53,227
но есть немного больше неопределенности из-за всех этих крайне маловероятных событий, 

405
00:21:53,227 --> 00:21:56,500
хотя, если бы вы их изучили, вы бы получили из этого массу информации.

406
00:21:57,160 --> 00:21:59,379
Уменьшение масштаба — это часть того, что делает Wordle 

407
00:21:59,379 --> 00:22:01,400
таким хорошим примером для урока теории информации.

408
00:22:01,600 --> 00:22:04,640
У нас есть два различных применения чувств к энтропии.

409
00:22:05,160 --> 00:22:08,464
Первый говорит нам, какую ожидаемую информацию мы получим в 

410
00:22:08,464 --> 00:22:11,328
результате данного предположения, а второй говорит, 

411
00:22:11,328 --> 00:22:15,460
можем ли мы измерить оставшуюся неопределенность среди всех возможных слов.

412
00:22:16,460 --> 00:22:20,523
И я должен подчеркнуть, что в первом случае, когда мы смотрим на ожидаемую информацию 

413
00:22:20,523 --> 00:22:24,540
предположения, когда у нас есть неравный вес слов, это влияет на вычисление энтропии.

414
00:22:24,980 --> 00:22:28,518
Например, позвольте мне рассмотреть тот же случай, который мы рассматривали ранее, 

415
00:22:28,518 --> 00:22:31,417
с распределением, связанным с Уири, но на этот раз с использованием 

416
00:22:31,417 --> 00:22:33,720
неравномерного распределения по всем возможным словам.

417
00:22:34,500 --> 00:22:36,621
Итак, давайте посмотрим, смогу ли я найти здесь часть, 

418
00:22:36,621 --> 00:22:38,280
которая достаточно хорошо это иллюстрирует.

419
00:22:40,940 --> 00:22:42,360
Ладно, вот это очень хорошо.

420
00:22:42,360 --> 00:22:45,541
Здесь у нас есть два соседних шаблона, которые примерно одинаково вероятны, 

421
00:22:45,541 --> 00:22:49,100
но один из них, как нам сказали, имеет 32 возможных слова, которые ему соответствуют.

422
00:22:49,280 --> 00:22:51,445
И если мы проверим, что это такое, то это те 32, 

423
00:22:51,445 --> 00:22:54,053
которые представляют собой просто очень невероятные слова, 

424
00:22:54,053 --> 00:22:55,600
когда вы просматриваете их глазами.

425
00:22:55,840 --> 00:22:59,182
Трудно найти какие-либо ответы, которые кажутся правдоподобными, возможно, 

426
00:22:59,182 --> 00:23:02,033
крики, но если мы посмотрим на соседний шаблон в распределении, 

427
00:23:02,033 --> 00:23:04,662
который считается примерно столь же вероятным, нам скажут, 

428
00:23:04,662 --> 00:23:08,361
что он имеет только 8 возможных совпадений, то есть четверть как много совпадений, 

429
00:23:08,361 --> 00:23:09,520
но это примерно одинаково.

430
00:23:09,860 --> 00:23:12,140
И когда мы достанем эти спички, мы поймем, почему.

431
00:23:12,500 --> 00:23:14,787
Некоторые из них являются вполне правдоподобными ответами, 

432
00:23:14,787 --> 00:23:16,300
например, «звонок», «гнев» или «стуки».

433
00:23:17,900 --> 00:23:20,050
Чтобы проиллюстрировать, как мы все это реализуем, 

434
00:23:20,050 --> 00:23:22,285
позвольте мне открыть здесь вторую версию Wordlebot, 

435
00:23:22,285 --> 00:23:25,280
и в ней есть два или три основных отличия от первой, которую мы видели.

436
00:23:25,860 --> 00:23:29,725
Во-первых, как я только что сказал, способ, которым мы вычисляем эти энтропии, 

437
00:23:29,725 --> 00:23:33,787
эти ожидаемые значения информации, теперь использует более точное распределение по 

438
00:23:33,787 --> 00:23:37,848
шаблонам, которое учитывает вероятность того, что данное слово действительно будет 

439
00:23:37,848 --> 00:23:38,240
ответом.

440
00:23:38,879 --> 00:23:43,820
Так получилось, что слезы по-прежнему на первом месте, хотя последующие немного другие.

441
00:23:44,360 --> 00:23:46,447
Во-вторых, когда он ранжирует свои лучшие варианты, 

442
00:23:46,447 --> 00:23:48,415
он теперь будет хранить модель вероятности того, 

443
00:23:48,415 --> 00:23:51,787
что каждое слово является фактическим ответом, и будет включать это в свое решение, 

444
00:23:51,787 --> 00:23:55,080
что легче увидеть, если у нас есть несколько предположений по поводу ответа. стол.

445
00:23:55,860 --> 00:23:57,858
Опять же, игнорируя его рекомендации, потому что мы 

446
00:23:57,858 --> 00:23:59,780
не можем позволить машинам управлять нашей жизнью.

447
00:24:01,140 --> 00:24:04,172
И я полагаю, мне следует упомянуть еще одну вещь, которая здесь слева: 

448
00:24:04,172 --> 00:24:06,436
это значение неопределенности, это количество битов, 

449
00:24:06,436 --> 00:24:09,640
больше не просто избыточно по сравнению с количеством возможных совпадений.

450
00:24:10,080 --> 00:24:14,953
Теперь, если мы поднимем его и посчитаем 2 к 8.02, что немного выше 256, я думаю, 

451
00:24:14,953 --> 00:24:18,757
259, он говорит о том, что, несмотря на то, что всего 526 слов, 

452
00:24:18,757 --> 00:24:23,333
которые на самом деле соответствуют этому шаблону, степень неопределенности, 

453
00:24:23,333 --> 00:24:26,424
которую он имеет, больше похожа на то, что было бы, 

454
00:24:26,424 --> 00:24:28,980
если бы было 259 равновероятных результаты.

455
00:24:29,720 --> 00:24:30,740
Вы можете думать об этом так.

456
00:24:31,020 --> 00:24:34,575
Он знает, что боркс — это не ответ, то же самое с йортами, зорлами и зорусами, 

457
00:24:34,575 --> 00:24:37,680
поэтому его неопределенность немного меньше, чем в предыдущем случае.

458
00:24:37,820 --> 00:24:39,280
Это количество бит будет меньше.

459
00:24:40,220 --> 00:24:44,093
И если я продолжу играть в игру, я уточню это с помощью пары предположений, 

460
00:24:44,093 --> 00:24:46,540
связанных с тем, что я хотел бы объяснить здесь.

461
00:24:48,360 --> 00:24:51,397
По четвертому предположению, если вы посмотрите на его лучшие варианты, 

462
00:24:51,397 --> 00:24:53,760
вы увидите, что это уже не просто максимизация энтропии.

463
00:24:54,460 --> 00:24:57,228
Итак, на данный момент технически существует семь возможностей, 

464
00:24:57,228 --> 00:25:00,300
но единственные, у которых есть значимый шанс, — это общежития и слова.

465
00:25:00,300 --> 00:25:04,470
И вы можете видеть, что выбор обоих из этих значений стоит выше всех остальных значений, 

466
00:25:04,470 --> 00:25:06,720
которые, строго говоря, дадут больше информации.

467
00:25:07,240 --> 00:25:09,771
В первый раз, когда я это сделал, я просто сложил эти два числа, 

468
00:25:09,771 --> 00:25:13,043
чтобы измерить качество каждого предположения, и это на самом деле сработало лучше, 

469
00:25:13,043 --> 00:25:13,900
чем вы могли подумать.

470
00:25:14,300 --> 00:25:16,631
Но на самом деле это не казалось систематическим, и я уверен, 

471
00:25:16,631 --> 00:25:19,340
что люди могли бы использовать другие подходы, но я остановился на этом.

472
00:25:19,760 --> 00:25:23,855
Если мы рассматриваем перспективу следующей догадки, как в данном случае слов, 

473
00:25:23,855 --> 00:25:27,900
нас действительно волнует ожидаемый результат нашей игры, если мы это сделаем.

474
00:25:28,230 --> 00:25:31,990
И чтобы вычислить этот ожидаемый балл, мы говорим, какова вероятность того, 

475
00:25:31,990 --> 00:25:35,900
что слова являются фактическим ответом, который на данный момент описывает 58%.

476
00:25:36,040 --> 00:25:39,540
Мы говорим, что с вероятностью 58% наш счет в этой игре будет 4.

477
00:25:40,320 --> 00:25:45,640
И тогда с вероятностью 1 минус эти 58% наш результат будет больше этих 4.

478
00:25:46,220 --> 00:25:48,989
Насколько больше мы не знаем, но мы можем оценить это, исходя из того, 

479
00:25:48,989 --> 00:25:52,460
насколько велика неопределенность, вероятно, возникнет, когда мы доберемся до этой точки.

480
00:25:52,960 --> 00:25:55,940
Конкретно на данный момент их 1.44 бита неопределенности.

481
00:25:56,440 --> 00:25:59,482
Если мы угадываем слова, это означает, что ожидаемая информация, 

482
00:25:59,482 --> 00:26:01,120
которую мы получим, равна 1.27 бит.

483
00:26:01,620 --> 00:26:04,027
Итак, если мы угадываем слова, эта разница показывает, 

484
00:26:04,027 --> 00:26:07,660
сколько неопределенности у нас, вероятно, останется после того, как это произойдет.

485
00:26:08,260 --> 00:26:10,737
Нам нужна некая функция, которую я здесь называю f, 

486
00:26:10,737 --> 00:26:13,740
которая связывает эту неопределенность с ожидаемым результатом.

487
00:26:14,240 --> 00:26:16,729
И способ, которым это было сделано, заключался в том, 

488
00:26:16,729 --> 00:26:20,418
чтобы просто построить график данных из предыдущих игр на основе версии 1 бота, 

489
00:26:20,418 --> 00:26:24,337
чтобы сказать: «Эй, каков был фактический счет после различных точек с определенной, 

490
00:26:24,337 --> 00:26:26,320
очень измеримой степенью неопределенности».

491
00:26:27,020 --> 00:26:31,071
Например, эти точки данных находятся выше значения примерно 8.Для некоторых 

492
00:26:31,071 --> 00:26:35,655
игр говорят «7» или около того после момента, когда их было 8.7 бит неопределенности, 

493
00:26:35,655 --> 00:26:38,960
чтобы получить окончательный ответ, потребовалось две догадки.

494
00:26:39,320 --> 00:26:42,240
В других играх требовалось три предположения, в других — четыре предположения.

495
00:26:43,140 --> 00:26:46,737
Если мы сдвинемся здесь влево, все точки выше нуля означают, что всякий раз, 

496
00:26:46,737 --> 00:26:50,288
когда есть ноль бит неопределенности, то есть есть только одна возможность, 

497
00:26:50,288 --> 00:26:54,260
тогда количество требуемых предположений всегда будет только одним, что обнадеживает.

498
00:26:54,780 --> 00:26:57,370
Всякий раз, когда была хоть капля неопределенности, то есть, 

499
00:26:57,370 --> 00:26:59,452
по существу, существовало всего две возможности, 

500
00:26:59,452 --> 00:27:03,020
иногда требовалось еще одно предположение, иногда требовалось еще два предположения.

501
00:27:03,080 --> 00:27:05,240
И так далее и тому подобное здесь.

502
00:27:05,740 --> 00:27:07,866
Возможно, более простой способ визуализировать 

503
00:27:07,866 --> 00:27:10,220
эти данные — объединить их и взять средние значения.

504
00:27:11,000 --> 00:27:14,003
Например, эта полоса говорит о том, что среди всех точек, 

505
00:27:14,003 --> 00:27:18,147
где у нас была одна доля неопределенности, в среднем количество требуемых новых 

506
00:27:18,147 --> 00:27:19,960
предположений составляло около 1.5.

507
00:27:22,140 --> 00:27:25,231
И вот эта полоска говорит о том, что среди всех разных игр, 

508
00:27:25,231 --> 00:27:28,682
где в какой-то момент неопределенность была чуть выше четырех бит, 

509
00:27:28,682 --> 00:27:31,516
что похоже на сужение ее до 16 различных возможностей, 

510
00:27:31,516 --> 00:27:35,380
то в среднем с этой точки требуется чуть больше двух предположений. вперед.

511
00:27:36,060 --> 00:27:38,382
И отсюда я просто выполнил регрессию, чтобы соответствовать функции, 

512
00:27:38,382 --> 00:27:39,460
которая показалась мне разумной.

513
00:27:39,980 --> 00:27:42,761
И помните, что весь смысл всего этого заключается в том, 

514
00:27:42,761 --> 00:27:45,250
чтобы мы могли количественно оценить эту интуицию: 

515
00:27:45,250 --> 00:27:48,960
чем больше информации мы получаем от слова, тем ниже будет ожидаемая оценка.

516
00:27:49,680 --> 00:27:54,882
Итак, это версия 2.0, если мы вернемся назад и запустим тот же набор симуляций, 

517
00:27:54,882 --> 00:27:59,240
используя все 2315 возможных словесных ответов, как это произойдет?

518
00:28:00,280 --> 00:28:03,420
Ну, в отличие от нашей первой версии, она определенно лучше, и это обнадеживает.

519
00:28:04,020 --> 00:28:08,173
В целом средний балл составляет около 3.6, хотя в отличие от первой версии есть 

520
00:28:08,173 --> 00:28:12,120
пару моментов, когда она проигрывает и требует больше шести в данном случае.

521
00:28:12,639 --> 00:28:15,290
Вероятно, потому, что бывают случаи, когда он идет на компромисс, 

522
00:28:15,290 --> 00:28:17,940
чтобы действительно достичь цели, а не максимизировать информацию.

523
00:28:19,040 --> 00:28:21,000
Так можем ли мы сделать лучше, чем 3?6?

524
00:28:22,080 --> 00:28:22,920
Мы определенно можем.

525
00:28:23,280 --> 00:28:26,179
В начале я сказал, что очень интересно попытаться не включать 

526
00:28:26,179 --> 00:28:29,360
настоящий список словесных ответов в способ построения своей модели.

527
00:28:29,880 --> 00:28:34,180
Но если мы добавим это, лучший результат, который я мог бы получить, был около 3.43.

528
00:28:35,160 --> 00:28:36,869
Итак, если мы попытаемся пойти более изощренно, 

529
00:28:36,869 --> 00:28:40,075
чем просто использовать данные о частоте слов, чтобы выбрать это априорное распределение, 

530
00:28:40,075 --> 00:28:41,892
это 3.43, вероятно, дает максимальную оценку того, 

531
00:28:41,892 --> 00:28:44,279
насколько хорошо мы могли бы добиться этого, или, по крайней мере, 

532
00:28:44,279 --> 00:28:45,740
насколько хорошо я мог бы добиться этого.

533
00:28:46,240 --> 00:28:49,200
Эта лучшая производительность, по сути, просто использует идеи, 

534
00:28:49,200 --> 00:28:52,206
о которых я здесь говорил, но она идет немного дальше, например, 

535
00:28:52,206 --> 00:28:55,120
она ищет ожидаемую информацию на два шага вперед, а не на один.

536
00:28:55,620 --> 00:28:57,798
Изначально я планировал поговорить об этом подробнее, 

537
00:28:57,798 --> 00:29:00,220
но понимаю, что на самом деле мы и так зашли слишком далеко.

538
00:29:00,580 --> 00:29:03,306
Единственное, что я скажу, это то, что после этого двухэтапного поиска, 

539
00:29:03,306 --> 00:29:06,411
а затем запуска пары выборочных симуляций с лучшими кандидатами, по крайней мере, 

540
00:29:06,411 --> 00:29:09,100
на данный момент для меня это выглядит так, будто Крейн - лучший дебют.

541
00:29:09,100 --> 00:29:10,060
Кто бы мог подумать?

542
00:29:10,920 --> 00:29:14,389
Кроме того, если вы используете настоящий список слов для определения пространства своих 

543
00:29:14,389 --> 00:29:17,820
возможностей, то неопределенность, с которой вы начнете, составит немногим более 11 бит.

544
00:29:18,300 --> 00:29:22,226
И оказывается, что при простом переборе максимально возможная ожидаемая 

545
00:29:22,226 --> 00:29:25,880
информация после первых двух предположений составляет около 10 бит.

546
00:29:26,500 --> 00:29:30,372
Это предполагает, что в лучшем случае после первых двух предположений при 

547
00:29:30,372 --> 00:29:34,560
совершенно оптимальной игре у вас останется примерно одна доля неопределенности.

548
00:29:34,800 --> 00:29:37,960
Это то же самое, что ограничиться двумя возможными предположениями.

549
00:29:37,960 --> 00:29:41,320
Поэтому я думаю, что будет справедливо и, возможно, довольно консервативно сказать, 

550
00:29:41,320 --> 00:29:44,520
что вы никогда не сможете написать алгоритм, который бы достигал такого низкого 

551
00:29:44,520 --> 00:29:47,959
среднего значения, как 3, потому что с доступными вам словами просто не хватит места, 

552
00:29:47,959 --> 00:29:50,200
чтобы получить достаточно информации всего за два шага. 

553
00:29:50,200 --> 00:29:53,360
способен гарантировать ответ в третьем слоте каждый раз в обязательном порядке.

