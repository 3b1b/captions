1
00:00:00,000 --> 00:00:03,117
Игра Wurdle стала довольно вирусной за последние месяц или два, и,

2
00:00:03,117 --> 00:00:06,373
никогда не упуская возможности провести урок математики, мне приходит

3
00:00:06,373 --> 00:00:09,723
в голову, что эта игра может служить очень хорошим центральным примером

4
00:00:09,723 --> 00:00:13,120
в уроке по теории информации, и в частности тема, известная как энтропия.

5
00:00:13,120 --> 00:00:16,432
Видите ли, как и многие люди, я был втянут в эту головоломку, и, как

6
00:00:16,432 --> 00:00:19,840
и многие программисты, я также был втянут в попытки написать алгоритм,

7
00:00:19,840 --> 00:00:23,200
который будет вести игру настолько оптимально, насколько это возможно.

8
00:00:23,200 --> 00:00:27,180
И я решил здесь просто обсудить с вами часть моего процесса и объяснить часть

9
00:00:27,180 --> 00:00:31,620
математических расчетов, которые в него вошли, поскольку весь алгоритм основан на идее

10
00:00:31,620 --> 00:00:32,080
энтропии.

11
00:00:32,080 --> 00:00:42,180
Перво-наперво, если вы еще об этом не слышали, что такое Wurdle?

12
00:00:42,180 --> 00:00:45,318
И чтобы убить двух зайцев одним выстрелом, пока мы изучаем правила игры,

13
00:00:45,318 --> 00:00:48,370
позвольте мне также просмотреть, чего мы собираемся добиться, а именно

14
00:00:48,370 --> 00:00:51,380
разработать небольшой алгоритм, который, по сути, будет играть за нас.

15
00:00:51,380 --> 00:00:53,879
Хоть я и не участвовал в сегодняшнем Wurdle, сегодня

16
00:00:53,879 --> 00:00:55,860
4 февраля, и посмотрим, как справится бот.

17
00:00:55,860 --> 00:00:58,360
Цель Wurdle — угадать загадочное слово из пяти

18
00:00:58,360 --> 00:01:00,860
букв, и вам дается шесть разных шансов угадать.

19
00:01:00,860 --> 00:01:05,240
Например, мой бот Wurdle предлагает мне начать с угадывающего крана.

20
00:01:05,240 --> 00:01:07,967
Каждый раз, когда вы делаете предположение, вы получаете некоторую

21
00:01:07,967 --> 00:01:10,940
информацию о том, насколько ваше предположение близко к истинному ответу.

22
00:01:10,940 --> 00:01:14,540
Здесь серый прямоугольник говорит мне, что в реальном ответе нет буквы C.

23
00:01:14,540 --> 00:01:18,340
Желтый квадрат сообщает мне, что есть буква R, но она не в этом положении.

24
00:01:18,340 --> 00:01:20,669
Зеленый квадрат сообщает мне, что в секретном слове

25
00:01:20,669 --> 00:01:22,820
есть буква А и она находится на третьей позиции.

26
00:01:22,820 --> 00:01:24,300
И тогда нет ни Н, ни Е.

27
00:01:24,300 --> 00:01:27,420
Так что позвольте мне просто войти и сообщить эту информацию боту Wurdle.

28
00:01:27,420 --> 00:01:31,500
Начали с журавля, получили серый, желтый, зеленый, серый, серый.

29
00:01:31,500 --> 00:01:33,768
Не беспокойтесь обо всех данных, которые он показывает

30
00:01:33,768 --> 00:01:35,460
прямо сейчас, я объясню это в свое время.

31
00:01:35,460 --> 00:01:39,700
Но главный совет для нашего второго выбора — это ерунда.

32
00:01:39,700 --> 00:01:42,700
И ваше предположение действительно должно состоять из пяти букв, но, как вы увидите,

33
00:01:42,700 --> 00:01:45,700
оно довольно либерально в отношении того, что оно на самом деле позволит вам угадать.

34
00:01:45,700 --> 00:01:48,860
В этом случае мы пробуем фишку.

35
00:01:48,860 --> 00:01:50,260
И ладно, дела обстоят неплохо.

36
00:01:50,260 --> 00:01:54,740
Мы нажимаем S и H, поэтому мы знаем первые три буквы, мы знаем, что есть R.

37
00:01:54,740 --> 00:01:59,740
И это будет похоже на SHA что-то R или SHA R что-то.

38
00:01:59,740 --> 00:02:05,220
И похоже, что бот Wurdle знает, что есть всего два варианта: осколочный или острый.

39
00:02:05,220 --> 00:02:08,220
На данный момент между ними возникает своего рода спор, так что я думаю, что,

40
00:02:08,220 --> 00:02:11,260
вероятно, просто потому, что это алфавитный порядок, это соответствует осколку.

41
00:02:11,260 --> 00:02:13,000
Какое ура, это настоящий ответ.

42
00:02:13,000 --> 00:02:14,660
Итак, мы получили его за три.

43
00:02:14,660 --> 00:02:17,588
Если вам интересно, хорошо ли это, то я услышал от одного

44
00:02:17,588 --> 00:02:20,820
человека фразу: для Уёрдла четыре — это номинал, а три — птичка.

45
00:02:20,820 --> 00:02:22,960
Я думаю, это довольно удачная аналогия.

46
00:02:22,960 --> 00:02:25,425
Чтобы получить четыре, вам нужно постоянно работать

47
00:02:25,425 --> 00:02:27,560
над своей игрой, но это, конечно, не безумие.

48
00:02:27,560 --> 00:02:30,000
Но когда ты получаешь это за три, это просто здорово.

49
00:02:30,000 --> 00:02:33,042
Так что, если вы не против, то я хотел бы с самого начала просто

50
00:02:33,042 --> 00:02:36,600
рассказать о своем мыслительном процессе о том, как я подхожу к боту Wurdle.

51
00:02:36,600 --> 00:02:39,800
И, как я уже сказал, на самом деле это повод для урока теории информации.

52
00:02:39,800 --> 00:02:48,560
Основная цель — объяснить, что такое информация и что такое энтропия.

53
00:02:48,560 --> 00:02:50,821
Моей первой мыслью при подходе к этому было взглянуть на

54
00:02:50,821 --> 00:02:53,560
относительную частоту употребления различных букв в английском языке.

55
00:02:53,560 --> 00:02:56,910
Итак, я подумал: ладно, есть ли начальная догадка или пара начальных догадок,

56
00:02:56,910 --> 00:02:59,960
которая соответствует многим из этих наиболее часто встречающихся букв?

57
00:02:59,960 --> 00:03:03,780
И мне очень понравилось делать другое, а затем гвозди.

58
00:03:03,780 --> 00:03:06,094
Идея в том, что если вы нажмете на букву, вы получите

59
00:03:06,094 --> 00:03:07,980
зеленый или желтый цвет, это всегда приятно.

60
00:03:07,980 --> 00:03:09,460
Такое ощущение, что ты получаешь информацию.

61
00:03:09,460 --> 00:03:12,117
Но в этих случаях, даже если вы не попадаете и всегда получаете

62
00:03:12,117 --> 00:03:14,733
серый цвет, это все равно дает вам много информации, поскольку

63
00:03:14,733 --> 00:03:17,640
довольно редко можно найти слово, в котором нет ни одной из этих букв.

64
00:03:17,640 --> 00:03:20,895
Но даже несмотря на это, это не кажется суперсистематическим,

65
00:03:20,895 --> 00:03:23,520
потому что, например, порядок букв не учитывается.

66
00:03:23,520 --> 00:03:26,080
Зачем печатать гвозди, если можно печатать улитку?

67
00:03:26,080 --> 00:03:27,720
Лучше ли иметь букву S в конце?

68
00:03:27,720 --> 00:03:28,720
Я не совсем уверен.

69
00:03:28,720 --> 00:03:32,691
Мой друг сказал, что ему нравится начинать со слова «усталый», что меня

70
00:03:32,691 --> 00:03:37,160
немного удивило, потому что в нем есть несколько необычных букв, таких как W и Y.

71
00:03:37,160 --> 00:03:39,400
Но кто знает, может быть, это лучший дебют.

72
00:03:39,400 --> 00:03:42,185
Есть ли какая-то количественная оценка, по которой мы

73
00:03:42,185 --> 00:03:44,920
можем судить о качестве потенциального предположения?

74
00:03:44,920 --> 00:03:48,239
Теперь, чтобы настроить способ ранжирования возможных предположений,

75
00:03:48,239 --> 00:03:51,800
давайте вернемся и добавим немного ясности в то, как именно устроена игра.

76
00:03:51,800 --> 00:03:54,790
Итак, есть список слов, которые он позволит вам ввести и которые

77
00:03:54,790 --> 00:03:57,920
считаются действительными догадками, длиной всего около 13 000 слов.

78
00:03:57,920 --> 00:04:02,534
Но когда вы посмотрите на это, то увидите много действительно необычных вещей, таких

79
00:04:02,534 --> 00:04:07,040
как голова или Али и ARG, слова, которые вызывают семейные споры в игре в «Эрудит».

80
00:04:07,040 --> 00:04:10,600
Но суть игры в том, что ответом всегда будет достаточно обычное слово.

81
00:04:10,600 --> 00:04:13,395
И на самом деле, есть еще один список из примерно

82
00:04:13,395 --> 00:04:16,080
2300 слов, которые являются возможными ответами.

83
00:04:16,080 --> 00:04:19,108
И это список, составленный людьми, я думаю, конкретно

84
00:04:19,108 --> 00:04:21,800
девушкой создателя игры, и это довольно забавно.

85
00:04:21,800 --> 00:04:24,745
Но что я хотел бы сделать, так это то, что наша задача в этом проекте

86
00:04:24,745 --> 00:04:27,522
состоит в том, чтобы посмотреть, сможем ли мы написать программу,

87
00:04:27,522 --> 00:04:30,720
решающую Wordle, которая не будет включать предыдущие знания об этом списке.

88
00:04:30,720 --> 00:04:33,204
Во-первых, существует множество довольно распространенных

89
00:04:33,204 --> 00:04:35,560
слов из пяти букв, которых вы не найдете в этом списке.

90
00:04:35,560 --> 00:04:37,545
Поэтому было бы лучше написать программу, которая была бы

91
00:04:37,545 --> 00:04:39,735
немного более устойчивой и могла бы играть в Wordle против кого

92
00:04:39,735 --> 00:04:41,960
угодно, а не только против того, что является официальным сайтом.

93
00:04:41,960 --> 00:04:44,817
А также причина, по которой мы знаем, что представляет собой этот список

94
00:04:44,817 --> 00:04:47,440
возможных ответов, заключается в том, что он виден в исходном коде.

95
00:04:47,440 --> 00:04:50,113
Но в исходном коде это отображается в определенном

96
00:04:50,113 --> 00:04:52,840
порядке, в котором ответы появляются изо дня в день.

97
00:04:52,840 --> 00:04:56,400
Так что вы всегда можете просто посмотреть, каким будет завтрашний ответ.

98
00:04:56,400 --> 00:04:59,140
Итак, очевидно, что в некотором смысле использование списка является мошенничеством.

99
00:04:59,140 --> 00:05:02,398
И что делает головоломку более интересной и более насыщенный урок теории

100
00:05:02,398 --> 00:05:05,211
информации, так это использование вместо этого некоторых более

101
00:05:05,211 --> 00:05:08,247
универсальных данных, таких как относительная частота слов в целом,

102
00:05:08,247 --> 00:05:11,640
чтобы уловить интуитивное ощущение предпочтения более распространенных слов.

103
00:05:11,640 --> 00:05:16,560
Итак, из этих 13 000 возможностей, как нам выбрать первую догадку?

104
00:05:16,560 --> 00:05:18,278
Например, если мой друг предлагает усталость,

105
00:05:18,278 --> 00:05:19,960
как нам следует проанализировать ее качество?

106
00:05:19,960 --> 00:05:22,704
Ну, причина, по которой он сказал, что ему нравится эта маловероятная

107
00:05:22,704 --> 00:05:25,449
W, заключается в том, что ему нравится дальновидность того, насколько

108
00:05:25,449 --> 00:05:27,880
приятно чувствовать себя, если вы действительно нажмете эту W.

109
00:05:27,880 --> 00:05:31,952
Например, если первая выявленная закономерность была примерно такой, то в

110
00:05:31,952 --> 00:05:36,080
этом гигантском словаре всего 58 слов, соответствующих этой закономерности.

111
00:05:36,080 --> 00:05:38,900
Так что это огромное сокращение по сравнению с 13 000.

112
00:05:38,900 --> 00:05:41,275
Но обратной стороной этого, конечно же, является

113
00:05:41,275 --> 00:05:43,360
то, что такой узор встречается очень редко.

114
00:05:43,360 --> 00:05:47,384
В частности, если бы каждое слово с одинаковой вероятностью было ответом,

115
00:05:47,384 --> 00:05:51,680
вероятность попадания в этот шаблон была бы 58, разделенная примерно на 13 000.

116
00:05:51,680 --> 00:05:53,880
Конечно, они не в равной степени могут быть ответами.

117
00:05:53,880 --> 00:05:56,680
Большинство из них – очень неясные и даже сомнительные слова.

118
00:05:56,680 --> 00:05:59,462
Но, по крайней мере, для нашего первого этапа, давайте предположим,

119
00:05:59,462 --> 00:06:02,040
что все они одинаково вероятны, а затем уточним это чуть позже.

120
00:06:02,040 --> 00:06:07,360
Дело в том, что паттерн с большим количеством информации по своей природе маловероятен.

121
00:06:07,360 --> 00:06:11,920
На самом деле, быть информативным означает то, что это маловероятно.

122
00:06:11,920 --> 00:06:15,088
Гораздо более вероятной моделью, которую можно увидеть в этом

123
00:06:15,088 --> 00:06:18,360
открытии, было бы что-то вроде этого, где, конечно, нет буквы W.

124
00:06:18,360 --> 00:06:22,080
Может быть, есть E, а может быть, нет A, нет R, нет Y.

125
00:06:22,080 --> 00:06:24,640
В этом случае существует 1400 возможных совпадений.

126
00:06:24,640 --> 00:06:27,554
Если бы все были одинаково вероятны, вероятность того,

127
00:06:27,554 --> 00:06:30,680
что вы увидите именно такую модель, составила бы около 11%.

128
00:06:30,680 --> 00:06:34,320
Таким образом, наиболее вероятные результаты являются и наименее информативными.

129
00:06:34,320 --> 00:06:38,000
Чтобы получить более глобальное представление, позвольте мне показать вам полное

130
00:06:38,000 --> 00:06:42,000
распределение вероятностей по всем различным закономерностям, которые вы можете увидеть.

131
00:06:42,000 --> 00:06:45,570
Таким образом, каждая полоса, на которую вы смотрите, соответствует возможному набору

132
00:06:45,570 --> 00:06:49,182
цветов, которые могут быть обнаружены, из которых существует от 3 до 5 возможностей, и

133
00:06:49,182 --> 00:06:52,212
они расположены слева направо, от наиболее распространенного до наименее

134
00:06:52,212 --> 00:06:52,960
распространенного.

135
00:06:52,960 --> 00:06:54,626
Таким образом, наиболее распространенной возможностью

136
00:06:54,626 --> 00:06:56,200
здесь является то, что вы получите все серые цвета.

137
00:06:56,200 --> 00:06:58,800
Это происходит примерно в 14% случаев.

138
00:06:58,800 --> 00:07:02,661
И когда вы делаете предположение, вы надеетесь на то, что окажетесь где-то

139
00:07:02,661 --> 00:07:06,264
в этом длинном хвосте, как здесь, где есть только 18 возможностей для

140
00:07:06,264 --> 00:07:09,920
того, что соответствует этому шаблону, который, очевидно, выглядит так.

141
00:07:09,920 --> 00:07:14,080
Или, если мы рискнем пойти немного левее, знаете, может быть, мы пройдем весь путь сюда.

142
00:07:14,080 --> 00:07:16,560
Хорошо, вот вам хорошая головоломка.

143
00:07:16,560 --> 00:07:19,405
Какие три слова в английском языке начинаются с буквы

144
00:07:19,405 --> 00:07:22,040
W, заканчиваются на Y и где-то в них есть буква R?

145
00:07:22,040 --> 00:07:27,560
Оказывается, ответы, посмотрим, многословные, червивые и кривые.

146
00:07:27,560 --> 00:07:31,826
Итак, чтобы судить, насколько хорошо это слово в целом, нам нужна какая-то мера

147
00:07:31,826 --> 00:07:36,360
ожидаемого объема информации, которую вы собираетесь получить от этого распределения.

148
00:07:36,360 --> 00:07:41,235
Если мы пройдемся по каждому шаблону и умножим вероятность его появления на что-то, что

149
00:07:41,235 --> 00:07:46,000
измеряет, насколько он информативен, это, возможно, может дать нам объективную оценку.

150
00:07:46,000 --> 00:07:48,140
Теперь вашим первым инстинктом того, каким должно

151
00:07:48,140 --> 00:07:50,280
быть это что-то, может быть количество совпадений.

152
00:07:50,280 --> 00:07:52,960
Вам нужно меньшее среднее количество совпадений.

153
00:07:52,960 --> 00:07:58,610
Но вместо этого я хотел бы использовать более универсальное измерение, которое мы

154
00:07:58,610 --> 00:08:04,536
часто приписываем информации, и которое станет более гибким, когда каждому из этих 13

155
00:08:04,536 --> 00:08:10,600
000 слов будет назначена разная вероятность того, являются ли они на самом деле ответом.

156
00:08:10,600 --> 00:08:14,268
Стандартной единицей информации является бит, формула которого немного забавна,

157
00:08:14,268 --> 00:08:17,800
но она действительно интуитивно понятна, если мы просто посмотрим на примеры.

158
00:08:17,800 --> 00:08:20,879
Если у вас есть наблюдение, которое сокращает ваше пространство

159
00:08:20,879 --> 00:08:24,200
возможностей вдвое, мы говорим, что оно содержит один бит информации.

160
00:08:24,200 --> 00:08:27,838
В нашем примере пространство возможностей — это все возможные слова, и получается, что

161
00:08:27,838 --> 00:08:31,560
примерно половина из пятибуквенных слов имеет букву S, чуть меньше, но примерно половина.

162
00:08:31,560 --> 00:08:35,200
Таким образом, это наблюдение даст вам один бит информации.

163
00:08:35,200 --> 00:08:38,754
Если вместо этого новый факт сокращает это пространство возможностей

164
00:08:38,754 --> 00:08:42,000
в четыре раза, мы говорим, что он содержит два бита информации.

165
00:08:42,000 --> 00:08:45,120
Например, оказывается, что около четверти этих слов имеют букву Т.

166
00:08:45,120 --> 00:08:47,860
Если наблюдение сокращает это пространство в восемь раз, мы

167
00:08:47,860 --> 00:08:50,920
говорим, что это три бита информации, и так далее, и тому подобное.

168
00:08:50,920 --> 00:08:55,000
Четыре бита превращают его в 16-й, пять битов — в 32-й.

169
00:08:55,000 --> 00:08:59,848
Итак, теперь вы, возможно, захотите сделать паузу и спросить себя, какова формула

170
00:08:59,848 --> 00:09:04,520
информации о количестве битов с точки зрения вероятности возникновения события?

171
00:09:04,520 --> 00:09:08,365
Мы здесь говорим о том, что когда вы принимаете половину числа битов, это то же самое,

172
00:09:08,365 --> 00:09:12,077
что и вероятность, а это то же самое, что сказать, что двойка в степени числа битов

173
00:09:12,077 --> 00:09:16,011
равна единице по сравнению с вероятностью, что далее перестраивается так, что информация

174
00:09:16,011 --> 00:09:19,680
представляет собой логарифм по основанию два из одного, разделенный на вероятность.

175
00:09:19,680 --> 00:09:22,547
И иногда вы видите это с еще одной перестановкой, где информация

176
00:09:22,547 --> 00:09:25,680
представляет собой отрицательный логарифм по основанию два вероятности.

177
00:09:25,680 --> 00:09:28,826
Выраженный таким образом, это может показаться немного странным

178
00:09:28,826 --> 00:09:32,022
для непосвященных, но на самом деле это просто очень интуитивная

179
00:09:32,022 --> 00:09:35,120
идея спросить, сколько раз вы сократили свои возможности вдвое.

180
00:09:35,120 --> 00:09:37,501
Теперь, если вам интересно, знаете, я думал, что мы просто играем

181
00:09:37,501 --> 00:09:39,920
в забавную словесную игру, почему на картинке появляются логарифмы?

182
00:09:39,920 --> 00:09:44,108
Одна из причин, по которой эта единица более удобна, заключается в том, что гораздо

183
00:09:44,108 --> 00:09:48,397
проще говорить об очень маловероятных событиях, гораздо проще сказать, что наблюдение

184
00:09:48,397 --> 00:09:52,785
содержит 20 бит информации, чем сказать, что вероятность того или иного события равна 0.

185
00:09:52,785 --> 00:09:53,480
0000095.

186
00:09:53,480 --> 00:09:57,619
Но более существенная причина того, что это логарифмическое выражение оказалось очень

187
00:09:57,619 --> 00:10:01,374
полезным дополнением к теории вероятностей, заключается в том, как информация

188
00:10:01,374 --> 00:10:02,000
складывается.

189
00:10:02,000 --> 00:10:05,826
Например, если одно наблюдение дает вам два бита информации, сокращая

190
00:10:05,826 --> 00:10:09,652
ваше пространство в четыре раза, а затем второе наблюдение, такое как

191
00:10:09,652 --> 00:10:13,479
ваше второе предположение в Wordle, дает вам еще три бита информации,

192
00:10:13,479 --> 00:10:17,360
сокращая вас еще в восемь раз, два вместе дают вам пять бит информации.

193
00:10:17,360 --> 00:10:21,200
Точно так же, как вероятности любят умножаться, информация любит прибавляться.

194
00:10:21,200 --> 00:10:24,808
Итак, как только мы попадаем в область чего-то вроде ожидаемого значения,

195
00:10:24,808 --> 00:10:28,660
где мы складываем кучу чисел, с журналами работать становится намного приятнее.

196
00:10:28,660 --> 00:10:32,269
Давайте вернемся к нашему дистрибутиву Weary и добавим сюда еще один небольшой

197
00:10:32,269 --> 00:10:35,560
трекер, показывающий, сколько информации содержится для каждого шаблона.

198
00:10:35,560 --> 00:10:38,221
Главное, что я хочу, чтобы вы заметили, это то, что чем выше

199
00:10:38,221 --> 00:10:40,795
вероятность того, что мы доберемся до этих более вероятных

200
00:10:40,795 --> 00:10:43,500
шаблонов, тем меньше информации, тем меньше битов вы получите.

201
00:10:43,500 --> 00:10:47,330
Чтобы измерить качество этого предположения, мы возьмем ожидаемое значение

202
00:10:47,330 --> 00:10:51,313
этой информации, пройдемся по каждому шаблону, скажем, насколько он вероятен,

203
00:10:51,313 --> 00:10:54,940
а затем умножим это на количество битов информации, которые мы получим.

204
00:10:54,940 --> 00:10:58,107
А в примере с Вири это число равно 4.

205
00:10:58,107 --> 00:10:58,480
9 бит.

206
00:10:58,480 --> 00:11:00,822
Таким образом, в среднем информация, которую вы получаете из

207
00:11:00,822 --> 00:11:03,356
этого начального предположения, так же эффективна, как сокращение

208
00:11:03,356 --> 00:11:05,660
пространства ваших возможностей пополам примерно в пять раз.

209
00:11:05,660 --> 00:11:09,572
Напротив, примером предположения с более высокой ожидаемой

210
00:11:09,572 --> 00:11:13,220
информационной ценностью может быть что-то вроде Slate.

211
00:11:13,220 --> 00:11:16,180
В этом случае вы заметите, что распределение выглядит намного более плоским.

212
00:11:16,180 --> 00:11:20,334
В частности, наиболее вероятное появление всех оттенков серого имеет только

213
00:11:20,334 --> 00:11:24,435
около 6% вероятности появления, так что как минимум вы получите очевидно 3.

214
00:11:24,435 --> 00:11:25,940
9 бит информации.

215
00:11:25,940 --> 00:11:29,140
Но это минимум, чаще всего можно получить что-то получше.

216
00:11:29,140 --> 00:11:32,464
И оказывается, что если подсчитать цифры и сложить все

217
00:11:32,464 --> 00:11:36,333
соответствующие термины, то средняя информация составит около 5.

218
00:11:36,333 --> 00:11:36,420
8.

219
00:11:36,420 --> 00:11:40,241
Таким образом, в отличие от Вири, после первого предположения

220
00:11:40,241 --> 00:11:43,940
ваше пространство возможностей будет в среднем вдвое меньше.

221
00:11:43,940 --> 00:11:46,740
На самом деле есть забавная история с названием

222
00:11:46,740 --> 00:11:49,540
этого ожидаемого значения количества информации.

223
00:11:49,540 --> 00:11:53,188
Теория информации была разработана Клодом Шенноном, который работал в Bell Labs в

224
00:11:53,188 --> 00:11:56,882
1940-х годах, но о некоторых своих еще не опубликованных идеях он говорил с Джоном

225
00:11:56,882 --> 00:12:00,531
фон Нейманом, интеллектуальным гигантом того времени, очень выдающимся человеком.

226
00:12:00,531 --> 00:12:04,180
по математике и физике и положил начало тому, что впоследствии стало информатикой.

227
00:12:04,180 --> 00:12:07,567
И когда он упомянул, что на самом деле у него нет хорошего названия для

228
00:12:07,567 --> 00:12:11,049
этого ожидаемого значения количества информации, фон Нейман якобы сказал,

229
00:12:11,049 --> 00:12:14,720
что, как гласит история, вы должны называть это энтропией, и по двум причинам.

230
00:12:14,720 --> 00:12:18,746
Во-первых, ваша функция неопределенности использовалась в статистической механике под

231
00:12:18,746 --> 00:12:22,819
этим именем, поэтому у нее уже есть имя, а во-вторых, что более важно, никто не знает,

232
00:12:22,819 --> 00:12:26,940
что такое энтропия на самом деле, поэтому в дебатах вы всегда будете иметь преимущество.

233
00:12:26,940 --> 00:12:30,180
Так что, если название кажется немного загадочным и

234
00:12:30,180 --> 00:12:33,420
если верить этой истории, то это сделано специально.

235
00:12:33,420 --> 00:12:36,795
Кроме того, если вы задаетесь вопросом о его связи со всем этим вторым

236
00:12:36,795 --> 00:12:40,170
законом термодинамики из физики, связь определенно существует, но в ее

237
00:12:40,170 --> 00:12:43,546
происхождении Шеннон просто имел дело с чистой теорией вероятностей, и

238
00:12:43,546 --> 00:12:46,969
для наших целей здесь, когда я использую слово энтропия, я просто хочу,

239
00:12:46,969 --> 00:12:50,820
чтобы вы подумали об ожидаемой информационной ценности конкретного предположения.

240
00:12:50,820 --> 00:12:54,380
Вы можете думать об энтропии как об измерении двух вещей одновременно.

241
00:12:54,380 --> 00:12:57,420
Во-первых, насколько равномерным является распределение.

242
00:12:57,420 --> 00:13:01,700
Чем ближе распределение к равномерному, тем выше будет энтропия.

243
00:13:01,700 --> 00:13:04,997
В нашем случае, когда общее количество шаблонов составляет от

244
00:13:04,997 --> 00:13:08,401
3 до 5, для равномерного распределения наблюдение любого из них

245
00:13:08,401 --> 00:13:11,858
будет иметь базу данных журнала 2 из 3 до 5, что в итоге равно 7.

246
00:13:11,858 --> 00:13:17,860
92, так что это абсолютный максимум, который вы можете иметь для этой энтропии.

247
00:13:17,860 --> 00:13:22,900
Но энтропия также является своего рода мерой того, сколько возможностей вообще существует.

248
00:13:22,900 --> 00:13:26,275
Например, если у вас есть какое-то слово, в котором существует

249
00:13:26,275 --> 00:13:29,544
только 16 возможных шаблонов, и каждый из них равновероятен,

250
00:13:29,544 --> 00:13:32,760
эта энтропия, эта ожидаемая информация, будет равна 4 битам.

251
00:13:32,760 --> 00:13:36,996
Но если у вас есть другое слово, в котором может возникнуть 64 возможных

252
00:13:36,996 --> 00:13:41,000
шаблона, и все они одинаково вероятны, тогда энтропия составит 6 бит.

253
00:13:41,000 --> 00:13:45,544
Итак, если вы видите какое-то распределение, энтропия которого равна 6 битам,

254
00:13:45,544 --> 00:13:50,088
это как бы говорит о том, что в том, что должно произойти, существует столько

255
00:13:50,088 --> 00:13:54,400
же вариаций и неопределенности, как если бы было 64 равновероятных исхода.

256
00:13:54,400 --> 00:13:58,360
Для моего первого использования Wurtelebot я просто сделал это.

257
00:13:58,360 --> 00:14:03,011
Он перебирает все возможные предположения, все 13 000 слов, вычисляет энтропию

258
00:14:03,011 --> 00:14:07,838
для каждого из них, или, точнее, энтропию распределения по всем шаблонам, которые

259
00:14:07,838 --> 00:14:12,490
вы можете увидеть, для каждого из них, и выбирает самое высокое, поскольку это

260
00:14:12,490 --> 00:14:17,200
тот, который, скорее всего, максимально сократит ваше пространство возможностей.

261
00:14:17,200 --> 00:14:19,419
И хотя я говорил здесь только о первой догадке, то же

262
00:14:19,419 --> 00:14:21,680
самое происходит и со следующими несколькими догадками.

263
00:14:21,680 --> 00:14:25,122
Например, после того, как вы видите некоторый шаблон в этом первом предположении,

264
00:14:25,122 --> 00:14:28,648
который ограничит вас меньшим количеством возможных слов в зависимости от того, что

265
00:14:28,648 --> 00:14:32,300
с ним совпадает, вы просто играете в ту же игру в отношении этого меньшего набора слов.

266
00:14:32,300 --> 00:14:36,837
Для предлагаемого второго предположения вы смотрите на распределение всех шаблонов,

267
00:14:36,837 --> 00:14:41,428
которые могут возникнуть из этого более ограниченного набора слов, вы просматриваете

268
00:14:41,428 --> 00:14:45,480
все 13 000 возможностей и находите тот, который максимизирует эту энтропию.

269
00:14:45,480 --> 00:14:48,391
Чтобы показать вам, как это работает в действии, позвольте

270
00:14:48,391 --> 00:14:51,548
мне просто привести небольшой вариант написанного мной Вюртеле,

271
00:14:51,548 --> 00:14:54,460
в котором на полях показаны основные моменты этого анализа.

272
00:14:54,460 --> 00:14:57,242
После выполнения всех расчетов энтропии справа здесь

273
00:14:57,242 --> 00:15:00,340
показано, какие из них имеют наиболее ожидаемую информацию.

274
00:15:00,340 --> 00:15:05,945
Оказывается, самый популярный ответ, по крайней мере на данный момент, мы уточним

275
00:15:05,945 --> 00:15:11,140
это позже, — это Тарес, что означает, хм, конечно, вика, самая обычная вика.

276
00:15:11,140 --> 00:15:14,474
Каждый раз, когда мы здесь делаем предположение, что, возможно, я игнорирую его

277
00:15:14,474 --> 00:15:17,809
рекомендации и использую шифер, потому что мне нравится шифер, мы можем видеть,

278
00:15:17,809 --> 00:15:21,269
сколько ожидаемой информации он содержал, но затем справа от слова здесь показано,

279
00:15:21,269 --> 00:15:24,980
сколько реальную информацию, которую мы получили, учитывая эту конкретную закономерность.

280
00:15:24,980 --> 00:15:27,932
Так вот тут похоже нам немного не повезло, от нас ожидали 5.

281
00:15:27,932 --> 00:15:30,660
8, но нам удалось получить что-то меньшее.

282
00:15:30,660 --> 00:15:35,860
А затем, слева, здесь показаны все возможные слова, учитывая, где мы сейчас находимся.

283
00:15:35,860 --> 00:15:38,606
Синие полосы сообщают нам, насколько вероятно, по его мнению, каждое

284
00:15:38,606 --> 00:15:41,234
слово, поэтому на данный момент он предполагает, что каждое слово

285
00:15:41,234 --> 00:15:44,140
встречается с одинаковой вероятностью, но мы уточним это через мгновение.

286
00:15:44,140 --> 00:15:47,680
И затем это измерение неопределенности говорит нам об энтропии этого

287
00:15:47,680 --> 00:15:51,373
распределения возможных слов, которое сейчас, поскольку это равномерное

288
00:15:51,373 --> 00:15:55,940
распределение, является просто излишне сложным способом подсчета количества возможностей.

289
00:15:55,940 --> 00:15:59,343
Например, если бы мы возвели 2 в 13-ю степень.

290
00:15:59,343 --> 00:16:02,700
66, это должно быть около 13 000 возможностей.

291
00:16:02,700 --> 00:16:06,780
Я здесь немного не так, но только потому, что не показываю все десятичные знаки.

292
00:16:06,780 --> 00:16:09,846
На данный момент это может показаться излишним и слишком усложняющим

293
00:16:09,846 --> 00:16:12,780
ситуацию, но вы поймете, почему полезно иметь оба числа за минуту.

294
00:16:12,780 --> 00:16:16,188
Итак, здесь похоже, что самая высокая энтропия для нашего второго

295
00:16:16,188 --> 00:16:19,700
предположения — это Рамен, что опять-таки просто не похоже на слово.

296
00:16:19,700 --> 00:16:25,660
Итак, чтобы занять здесь моральную позицию, я собираюсь ввести Rains.

297
00:16:25,660 --> 00:16:27,540
И снова похоже, нам немного не повезло.

298
00:16:27,540 --> 00:16:28,872
Мы ожидали 4.

299
00:16:28,872 --> 00:16:30,556
3 бита, а мы получили только 3.

300
00:16:30,556 --> 00:16:32,100
39 бит информации.

301
00:16:32,100 --> 00:16:35,060
Таким образом, мы получаем 55 возможностей.

302
00:16:35,060 --> 00:16:37,555
И здесь, возможно, я просто воспользуюсь тем, что

303
00:16:37,555 --> 00:16:40,200
он предлагает, а именно комбо, что бы это ни значило.

304
00:16:40,200 --> 00:16:43,300
И ладно, на самом деле это хороший шанс для головоломки.

305
00:16:43,300 --> 00:16:45,718
Он говорит нам, что этот шаблон дает нам 4.

306
00:16:45,718 --> 00:16:47,020
7 бит информации.

307
00:16:47,020 --> 00:16:50,990
Но слева, прежде чем мы увидим эту закономерность, их было 5.

308
00:16:50,990 --> 00:16:52,400
78 бит неопределенности.

309
00:16:52,400 --> 00:16:54,606
Итак, в качестве теста для вас: что это значит

310
00:16:54,606 --> 00:16:56,860
относительно количества оставшихся возможностей?

311
00:16:56,860 --> 00:17:00,960
Ну, это означает, что мы сведены к одной частичке неопределенности,

312
00:17:00,960 --> 00:17:04,700
а это то же самое, что сказать, что есть два возможных ответа.

313
00:17:04,700 --> 00:17:06,520
Это выбор 50 на 50.

314
00:17:06,520 --> 00:17:08,652
И отсюда, поскольку мы с вами знаем, какие слова

315
00:17:08,652 --> 00:17:11,220
встречаются чаще, мы знаем, что ответ должен быть пропасть.

316
00:17:11,220 --> 00:17:13,540
Но как сейчас написано, программа этого не знает.

317
00:17:13,540 --> 00:17:16,815
Поэтому он просто продолжает работать, пытаясь получить как можно больше

318
00:17:16,815 --> 00:17:20,360
информации, пока не останется только одна возможность, а затем он ее угадывает.

319
00:17:20,360 --> 00:17:22,700
Поэтому очевидно, что нам нужна лучшая стратегия эндшпиля.

320
00:17:22,700 --> 00:17:26,720
Но предположим, что мы назовем эту версию одним из наших решателей слов,

321
00:17:26,720 --> 00:17:30,740
а затем запустим несколько симуляций, чтобы посмотреть, как она работает.

322
00:17:30,740 --> 00:17:34,240
Итак, это работает так: он играет во все возможные словесные игры.

323
00:17:34,240 --> 00:17:38,780
Он проходит через все эти 2315 слов, которые являются реальными ответами.

324
00:17:38,780 --> 00:17:41,340
По сути, он использует это как набор для тестирования.

325
00:17:41,340 --> 00:17:44,030
И с помощью этого наивного метода не учитывать, насколько

326
00:17:44,030 --> 00:17:47,139
распространено слово, а просто пытаться максимизировать информацию

327
00:17:47,139 --> 00:17:50,480
на каждом этапе пути, пока не дойдет до одного и только одного варианта.

328
00:17:50,480 --> 00:17:54,912
К концу моделирования средний балл составит около 4.

329
00:17:54,912 --> 00:17:55,100
124.

330
00:17:55,100 --> 00:17:59,780
Что неплохо, если честно, я ожидал худшего результата.

331
00:17:59,780 --> 00:18:03,040
Но люди, играющие в Wordle, скажут вам, что обычно они могут получить это за 4.

332
00:18:03,040 --> 00:18:05,260
Настоящая задача — собрать как можно больше очков из 3.

333
00:18:05,260 --> 00:18:08,920
Это довольно большой скачок между оценкой 4 и оценкой 3.

334
00:18:08,920 --> 00:18:15,684
Очевидный результат здесь — каким-то образом определить,

335
00:18:15,684 --> 00:18:23,160
является ли слово распространенным, и как именно мы это делаем.

336
00:18:23,160 --> 00:18:26,483
Я подошел к этому с целью получить список относительных

337
00:18:26,483 --> 00:18:28,560
частот всех слов английского языка.

338
00:18:28,560 --> 00:18:31,929
И я только что использовал функцию данных частоты слов Mathematica, которая

339
00:18:31,929 --> 00:18:35,520
сама извлекает данные из общедоступного набора данных Google Books English Ngram.

340
00:18:35,520 --> 00:18:37,838
И на это интересно смотреть, например, если мы отсортируем его

341
00:18:37,838 --> 00:18:40,120
от наиболее распространенных слов к наименее распространенным.

342
00:18:40,120 --> 00:18:43,740
Очевидно, это самые распространенные слова из 5 букв в английском языке.

343
00:18:43,740 --> 00:18:46,480
Вернее, это 8-е место по распространенности.

344
00:18:46,480 --> 00:18:49,440
Сначала есть что, после чего там и там.

345
00:18:49,440 --> 00:18:52,541
Первое само по себе не первое, а девятое, и имеет смысл, что

346
00:18:52,541 --> 00:18:55,542
эти другие слова могут встречаться чаще, где те, что после

347
00:18:55,542 --> 00:18:59,000
первого, находятся после, где, и эти слова встречаются немного реже.

348
00:18:59,000 --> 00:19:03,010
Теперь, используя эти данные для моделирования вероятности того, что каждое из этих

349
00:19:03,010 --> 00:19:07,020
слов будет окончательным ответом, оно не должно быть просто пропорционально частоте.

350
00:19:07,020 --> 00:19:09,596
Например, которому присвоен балл 0.

351
00:19:09,596 --> 00:19:12,398
002 в этом наборе данных, тогда как слово «коса» в

352
00:19:12,398 --> 00:19:15,200
каком-то смысле примерно в 1000 раз менее вероятно.

353
00:19:15,200 --> 00:19:19,400
Но оба эти слова достаточно распространены, поэтому их почти наверняка стоит рассмотреть.

354
00:19:19,400 --> 00:19:21,900
Поэтому нам нужно больше бинарного отсечения.

355
00:19:21,900 --> 00:19:26,116
Я представил, что беру весь этот отсортированный список слов, затем располагаю

356
00:19:26,116 --> 00:19:30,440
его по оси X, а затем применяю сигмовидную функцию, которая является стандартным

357
00:19:30,440 --> 00:19:34,656
способом получить функцию, вывод которой в основном двоичный, это либо 0, либо

358
00:19:34,656 --> 00:19:38,500
1, но в этой области неопределенности между ними происходит сглаживание.

359
00:19:38,500 --> 00:19:44,020
По сути, вероятность того, что я назначаю каждому слову попадание в окончательный список,

360
00:19:44,020 --> 00:19:49,540
будет значением сигмовидной функции, указанной выше, где бы оно ни располагалось на оси X.

361
00:19:49,540 --> 00:19:53,713
Очевидно, это зависит от нескольких параметров, например, насколько широкое

362
00:19:53,713 --> 00:19:58,107
пространство на оси X заполняют эти слова, определяет, насколько постепенно или

363
00:19:58,107 --> 00:20:02,720
круто мы снижаемся от 1 до 0, а то, где мы располагаем их слева направо, определяет

364
00:20:02,720 --> 00:20:03,160
границу.

365
00:20:03,160 --> 00:20:07,340
Честно говоря, я просто облизнул палец и высунул его по ветру.

366
00:20:07,340 --> 00:20:10,852
Я просмотрел отсортированный список и попытался найти окно, в котором,

367
00:20:10,852 --> 00:20:14,315
посмотрев на него, я понял, что около половины этих слов скорее всего

368
00:20:14,315 --> 00:20:17,680
будут окончательным ответом, и использовал его в качестве отсечения.

369
00:20:17,680 --> 00:20:20,953
Как только мы получим такое распределение по словам, это даст нам еще

370
00:20:20,953 --> 00:20:24,460
одну ситуацию, когда энтропия становится действительно полезным измерением.

371
00:20:24,460 --> 00:20:27,416
Например, предположим, что мы играли в игру и начинаем с моих

372
00:20:27,416 --> 00:20:30,373
старых открывашек, которыми были перо и гвозди, и заканчиваем

373
00:20:30,373 --> 00:20:33,760
ситуацией, когда есть четыре возможных слова, которые ей соответствуют.

374
00:20:33,760 --> 00:20:36,440
И допустим, мы считаем их всех одинаково вероятными.

375
00:20:36,440 --> 00:20:40,000
Позвольте мне спросить вас, какова энтропия этого распределения?

376
00:20:40,000 --> 00:20:45,326
Что ж, информация, связанная с каждой из этих возможностей, будет иметь

377
00:20:45,326 --> 00:20:50,800
логарифмическую базу 2 из 4, поскольку каждая из них равна 1 и 4, а это 2.

378
00:20:50,800 --> 00:20:52,780
Два бита информации, четыре возможности.

379
00:20:52,780 --> 00:20:54,360
Все очень хорошо и хорошо.

380
00:20:54,360 --> 00:20:58,320
Но что, если я скажу вам, что на самом деле совпадений больше четырех?

381
00:20:58,320 --> 00:21:00,591
На самом деле, когда мы просматриваем полный список

382
00:21:00,591 --> 00:21:02,600
слов, мы видим, что ему соответствуют 16 слов.

383
00:21:02,600 --> 00:21:05,484
Но предположим, что наша модель дает очень низкую вероятность

384
00:21:05,484 --> 00:21:08,415
того, что остальные 12 слов действительно станут окончательным

385
00:21:08,415 --> 00:21:11,440
ответом, примерно 1 из 1000, потому что они действительно неясны.

386
00:21:11,440 --> 00:21:15,480
Теперь позвольте мне спросить вас, какова энтропия этого распределения?

387
00:21:15,480 --> 00:21:19,100
Если бы энтропия измеряла здесь просто количество совпадений, то можно было

388
00:21:19,100 --> 00:21:22,579
бы ожидать, что это будет что-то вроде логарифмической базы 2 из 16, что

389
00:21:22,579 --> 00:21:26,200
будет равно 4, то есть на два бита неопределенности больше, чем было раньше.

390
00:21:26,200 --> 00:21:28,260
Но, конечно, фактическая неопределенность на самом

391
00:21:28,260 --> 00:21:30,320
деле не сильно отличается от того, что было раньше.

392
00:21:30,320 --> 00:21:34,186
Тот факт, что есть эти 12 действительно непонятных слов, не означает, что было

393
00:21:34,186 --> 00:21:38,200
бы еще более удивительно узнать, например, что окончательный ответ — «очарование».

394
00:21:38,200 --> 00:21:42,067
Итак, когда вы на самом деле выполняете вычисления здесь и складываете вероятность

395
00:21:42,067 --> 00:21:45,514
каждого события, умноженную на соответствующую информацию, вы получаете 2.

396
00:21:45,514 --> 00:21:45,960
11 бит.

397
00:21:45,960 --> 00:21:49,680
Я просто говорю, что это по сути два бита, в основном эти четыре возможности,

398
00:21:49,680 --> 00:21:53,352
но есть немного больше неопределенности из-за всех этих крайне маловероятных

399
00:21:53,352 --> 00:21:57,120
событий, хотя, если бы вы их изучили, вы бы получили из этого массу информации.

400
00:21:57,120 --> 00:21:59,569
Уменьшение масштаба — это часть того, что делает Wordle

401
00:21:59,569 --> 00:22:01,800
таким хорошим примером для урока теории информации.

402
00:22:01,800 --> 00:22:05,280
У нас есть два различных применения чувств к энтропии.

403
00:22:05,280 --> 00:22:08,873
Первый говорит нам, какую ожидаемую информацию мы получим в

404
00:22:08,873 --> 00:22:12,527
результате данного предположения, а второй говорит, можем ли

405
00:22:12,527 --> 00:22:16,480
мы измерить оставшуюся неопределенность среди всех возможных слов.

406
00:22:16,480 --> 00:22:20,764
И я должен подчеркнуть, что в первом случае, когда мы смотрим на ожидаемую информацию

407
00:22:20,764 --> 00:22:25,000
предположения, когда у нас есть неравный вес слов, это влияет на вычисление энтропии.

408
00:22:25,000 --> 00:22:27,891
Например, позвольте мне рассмотреть тот же случай, который мы

409
00:22:27,891 --> 00:22:31,062
рассматривали ранее, с распределением, связанным с Уири, но на этот

410
00:22:31,062 --> 00:22:34,560
раз с использованием неравномерного распределения по всем возможным словам.

411
00:22:34,560 --> 00:22:36,911
Итак, давайте посмотрим, смогу ли я найти здесь

412
00:22:36,911 --> 00:22:39,360
часть, которая достаточно хорошо это иллюстрирует.

413
00:22:39,360 --> 00:22:42,480
Ладно, вот это очень хорошо.

414
00:22:42,480 --> 00:22:45,914
Здесь у нас есть два соседних шаблона, которые примерно одинаково вероятны, но

415
00:22:45,914 --> 00:22:49,480
один из них, как нам сказали, имеет 32 возможных слова, которые ему соответствуют.

416
00:22:49,480 --> 00:22:52,475
И если мы проверим, что это такое, то это те 32, которые представляют

417
00:22:52,475 --> 00:22:55,600
собой просто очень невероятные слова, когда вы просматриваете их глазами.

418
00:22:55,600 --> 00:22:59,098
Трудно найти какие-либо ответы, которые кажутся правдоподобными, возможно,

419
00:22:59,098 --> 00:23:02,456
крики, но если мы посмотрим на соседний шаблон в распределении, который

420
00:23:02,456 --> 00:23:05,861
считается примерно столь же вероятным, нам скажут, что он имеет только 8

421
00:23:05,861 --> 00:23:09,920
возможных совпадений, то есть четверть как много совпадений, но это примерно одинаково.

422
00:23:09,920 --> 00:23:12,520
И когда мы достанем эти спички, мы поймем, почему.

423
00:23:12,520 --> 00:23:15,180
Некоторые из них являются вполне правдоподобными

424
00:23:15,180 --> 00:23:17,840
ответами, например, «звонок», «гнев» или «стуки».

425
00:23:17,840 --> 00:23:21,830
Чтобы проиллюстрировать, как мы все это реализуем, позвольте мне открыть здесь вторую

426
00:23:21,830 --> 00:23:25,960
версию Wordlebot, и в ней есть два или три основных отличия от первой, которую мы видели.

427
00:23:25,960 --> 00:23:30,336
Во-первых, как я только что сказал, способ, которым мы вычисляем эти энтропии, эти

428
00:23:30,336 --> 00:23:34,501
ожидаемые значения информации, теперь использует более точное распределение по

429
00:23:34,501 --> 00:23:38,878
шаблонам, которое учитывает вероятность того, что данное слово действительно будет

430
00:23:38,878 --> 00:23:39,300
ответом.

431
00:23:39,300 --> 00:23:44,160
Так получилось, что слезы по-прежнему на первом месте, хотя последующие немного другие.

432
00:23:44,160 --> 00:23:47,691
Во-вторых, когда он ранжирует свои лучшие варианты, он теперь будет хранить модель

433
00:23:47,691 --> 00:23:51,435
вероятности того, что каждое слово является фактическим ответом, и будет включать это в

434
00:23:51,435 --> 00:23:54,966
свое решение, что легче увидеть, если у нас есть несколько предположений по поводу

435
00:23:54,966 --> 00:23:55,520
ответа. стол.

436
00:23:55,520 --> 00:23:58,374
Опять же, игнорируя его рекомендации, потому что мы

437
00:23:58,374 --> 00:24:01,120
не можем позволить машинам управлять нашей жизнью.

438
00:24:01,120 --> 00:24:04,001
И я полагаю, мне следует упомянуть еще одну вещь, которая здесь

439
00:24:04,001 --> 00:24:07,018
слева: это значение неопределенности, это количество битов, больше

440
00:24:07,018 --> 00:24:10,080
не просто избыточно по сравнению с количеством возможных совпадений.

441
00:24:10,080 --> 00:24:13,489
Теперь, если мы поднимем его и посчитаем 2 к 8.

442
00:24:13,489 --> 00:24:17,271
02, что немного выше 256, я думаю, 259, он говорит о том, что,

443
00:24:17,271 --> 00:24:21,594
несмотря на то, что всего 526 слов, которые на самом деле соответствуют

444
00:24:21,594 --> 00:24:25,557
этому шаблону, степень неопределенности, которую он имеет, больше

445
00:24:25,557 --> 00:24:29,760
похожа на то, что было бы, если бы было 259 равновероятных результаты.

446
00:24:29,760 --> 00:24:31,100
Вы можете думать об этом так.

447
00:24:31,100 --> 00:24:34,242
Он знает, что боркс — это не ответ, то же самое с йортами, зорлами и

448
00:24:34,242 --> 00:24:37,840
зорусами, поэтому его неопределенность немного меньше, чем в предыдущем случае.

449
00:24:37,840 --> 00:24:40,220
Это количество бит будет меньше.

450
00:24:40,220 --> 00:24:44,381
И если я продолжу играть в игру, я уточню это с помощью пары

451
00:24:44,381 --> 00:24:48,680
предположений, связанных с тем, что я хотел бы объяснить здесь.

452
00:24:48,680 --> 00:24:51,160
По четвертому предположению, если вы посмотрите на его лучшие

453
00:24:51,160 --> 00:24:53,800
варианты, вы увидите, что это уже не просто максимизация энтропии.

454
00:24:53,800 --> 00:24:57,264
Итак, на данный момент технически существует семь возможностей, но

455
00:24:57,264 --> 00:25:00,780
единственные, у которых есть значимый шанс, — это общежития и слова.

456
00:25:00,780 --> 00:25:04,194
И вы можете видеть, что выбор обоих из этих значений стоит выше всех

457
00:25:04,194 --> 00:25:07,560
остальных значений, которые, строго говоря, дадут больше информации.

458
00:25:07,560 --> 00:25:11,213
В первый раз, когда я это сделал, я просто сложил эти два числа, чтобы измерить качество

459
00:25:11,213 --> 00:25:14,580
каждого предположения, и это на самом деле сработало лучше, чем вы могли подумать.

460
00:25:14,580 --> 00:25:17,190
Но на самом деле это не казалось систематическим, и я уверен, что

461
00:25:17,190 --> 00:25:19,880
люди могли бы использовать другие подходы, но я остановился на этом.

462
00:25:19,880 --> 00:25:24,187
Если мы рассматриваем перспективу следующей догадки, как в данном случае слов,

463
00:25:24,187 --> 00:25:28,440
нас действительно волнует ожидаемый результат нашей игры, если мы это сделаем.

464
00:25:28,440 --> 00:25:32,186
И чтобы вычислить этот ожидаемый балл, мы говорим, какова вероятность того,

465
00:25:32,186 --> 00:25:36,080
что слова являются фактическим ответом, который на данный момент описывает 58%.

466
00:25:36,080 --> 00:25:40,400
Мы говорим, что с вероятностью 58% наш счет в этой игре будет 4.

467
00:25:40,400 --> 00:25:46,240
И тогда с вероятностью 1 минус эти 58% наш результат будет больше этих 4.

468
00:25:46,240 --> 00:25:49,621
Насколько больше мы не знаем, но мы можем оценить это, исходя из того, насколько

469
00:25:49,621 --> 00:25:52,920
велика неопределенность, вероятно, возникнет, когда мы доберемся до этой точки.

470
00:25:52,920 --> 00:25:55,227
Конкретно на данный момент их 1.

471
00:25:55,227 --> 00:25:56,600
44 бита неопределенности.

472
00:25:56,600 --> 00:25:59,182
Если мы угадываем слова, это означает, что ожидаемая

473
00:25:59,182 --> 00:26:01,131
информация, которую мы получим, равна 1.

474
00:26:01,131 --> 00:26:01,560
27 бит.

475
00:26:01,560 --> 00:26:04,627
Итак, если мы угадываем слова, эта разница показывает, сколько

476
00:26:04,627 --> 00:26:08,280
неопределенности у нас, вероятно, останется после того, как это произойдет.

477
00:26:08,280 --> 00:26:11,201
Нам нужна некая функция, которую я здесь называю f, которая

478
00:26:11,201 --> 00:26:13,880
связывает эту неопределенность с ожидаемым результатом.

479
00:26:13,880 --> 00:26:18,099
И способ, которым это было сделано, заключался в том, чтобы просто построить график

480
00:26:18,099 --> 00:26:22,117
данных из предыдущих игр на основе версии 1 бота, чтобы сказать: «Эй, каков был

481
00:26:22,117 --> 00:26:26,135
фактический счет после различных точек с определенной, очень измеримой степенью

482
00:26:26,135 --> 00:26:27,040
неопределенности».

483
00:26:27,040 --> 00:26:31,073
Например, эти точки данных находятся выше значения примерно 8.

484
00:26:31,073 --> 00:26:35,426
Для некоторых игр говорят «7» или около того после момента, когда их было 8.

485
00:26:35,426 --> 00:26:39,340
7 бит неопределенности, чтобы получить окончательный ответ, потребовалось две догадки.

486
00:26:39,340 --> 00:26:43,180
В других играх требовалось три предположения, в других — четыре предположения.

487
00:26:43,180 --> 00:26:47,004
Если мы сдвинемся здесь влево, все точки выше нуля означают, что всякий раз,

488
00:26:47,004 --> 00:26:50,778
когда есть ноль бит неопределенности, то есть есть только одна возможность,

489
00:26:50,778 --> 00:26:55,000
тогда количество требуемых предположений всегда будет только одним, что обнадеживает.

490
00:26:55,000 --> 00:26:57,949
Всякий раз, когда была хоть капля неопределенности, то есть, по

491
00:26:57,949 --> 00:27:00,944
существу, существовало всего две возможности, иногда требовалось

492
00:27:00,944 --> 00:27:03,940
еще одно предположение, иногда требовалось еще два предположения.

493
00:27:03,940 --> 00:27:05,980
И так далее и тому подобное здесь.

494
00:27:05,980 --> 00:27:08,372
Возможно, более простой способ визуализировать

495
00:27:08,372 --> 00:27:11,020
эти данные — объединить их и взять средние значения.

496
00:27:11,020 --> 00:27:16,499
Например, эта полоса говорит о том, что среди всех точек, где у нас была одна доля

497
00:27:16,499 --> 00:27:22,308
неопределенности, в среднем количество требуемых новых предположений составляло около 1.

498
00:27:22,308 --> 00:27:22,420
5.

499
00:27:22,420 --> 00:27:26,829
И вот эта полоска говорит о том, что среди всех разных игр, где в какой-то момент

500
00:27:26,829 --> 00:27:31,454
неопределенность была чуть выше четырех бит, что похоже на сужение ее до 16 различных

501
00:27:31,454 --> 00:27:36,240
возможностей, то в среднем с этой точки требуется чуть больше двух предположений. вперед.

502
00:27:36,240 --> 00:27:38,521
И отсюда я просто выполнил регрессию, чтобы соответствовать

503
00:27:38,521 --> 00:27:40,080
функции, которая показалась мне разумной.

504
00:27:40,080 --> 00:27:43,380
И помните, что весь смысл всего этого заключается в том, чтобы

505
00:27:43,380 --> 00:27:46,314
мы могли количественно оценить эту интуицию: чем больше

506
00:27:46,314 --> 00:27:49,720
информации мы получаем от слова, тем ниже будет ожидаемая оценка.

507
00:27:49,720 --> 00:27:51,043
Итак, это версия 2.

508
00:27:51,043 --> 00:27:55,226
0, если мы вернемся назад и запустим тот же набор симуляций,

509
00:27:55,226 --> 00:27:59,820
используя все 2315 возможных словесных ответов, как это произойдет?

510
00:27:59,820 --> 00:28:04,060
Ну, в отличие от нашей первой версии, она определенно лучше, и это обнадеживает.

511
00:28:04,060 --> 00:28:06,284
В целом средний балл составляет около 3.

512
00:28:06,284 --> 00:28:09,383
6, хотя в отличие от первой версии есть пару моментов,

513
00:28:09,383 --> 00:28:12,820
когда она проигрывает и требует больше шести в данном случае.

514
00:28:12,820 --> 00:28:15,900
Вероятно, потому, что бывают случаи, когда он идет на компромисс,

515
00:28:15,900 --> 00:28:18,980
чтобы действительно достичь цели, а не максимизировать информацию.

516
00:28:18,980 --> 00:28:22,022
Так можем ли мы сделать лучше, чем 3?

517
00:28:22,022 --> 00:28:22,140
6?

518
00:28:22,140 --> 00:28:23,260
Мы определенно можем.

519
00:28:23,260 --> 00:28:26,464
В начале я сказал, что очень интересно попытаться не включать

520
00:28:26,464 --> 00:28:29,980
настоящий список словесных ответов в способ построения своей модели.

521
00:28:29,980 --> 00:28:35,043
Но если мы добавим это, лучший результат, который я мог бы получить, был около 3.

522
00:28:35,043 --> 00:28:35,180
43.

523
00:28:35,180 --> 00:28:38,068
Итак, если мы попытаемся пойти более изощренно, чем просто использовать

524
00:28:38,068 --> 00:28:40,957
данные о частоте слов, чтобы выбрать это априорное распределение, это 3.

525
00:28:40,957 --> 00:28:43,570
43, вероятно, дает максимальную оценку того, насколько хорошо мы могли бы

526
00:28:43,570 --> 00:28:46,360
добиться этого, или, по крайней мере, насколько хорошо я мог бы добиться этого.

527
00:28:46,360 --> 00:28:49,460
Эта лучшая производительность, по сути, просто использует идеи,

528
00:28:49,460 --> 00:28:52,608
о которых я здесь говорил, но она идет немного дальше, например,

529
00:28:52,608 --> 00:28:55,660
она ищет ожидаемую информацию на два шага вперед, а не на один.

530
00:28:55,660 --> 00:28:58,120
Изначально я планировал поговорить об этом подробнее, но

531
00:28:58,120 --> 00:29:00,580
понимаю, что на самом деле мы и так зашли слишком далеко.

532
00:29:00,580 --> 00:29:03,513
Единственное, что я скажу, это то, что после этого двухэтапного поиска, а

533
00:29:03,513 --> 00:29:06,447
затем запуска пары выборочных симуляций с лучшими кандидатами, по крайней

534
00:29:06,447 --> 00:29:09,500
мере, на данный момент для меня это выглядит так, будто Крейн - лучший дебют.

535
00:29:09,500 --> 00:29:11,080
Кто бы мог подумать?

536
00:29:11,080 --> 00:29:14,640
Кроме того, если вы используете настоящий список слов для определения пространства своих

537
00:29:14,640 --> 00:29:18,160
возможностей, то неопределенность, с которой вы начнете, составит немногим более 11 бит.

538
00:29:18,160 --> 00:29:22,521
И оказывается, что при простом переборе максимально возможная ожидаемая

539
00:29:22,521 --> 00:29:26,580
информация после первых двух предположений составляет около 10 бит.

540
00:29:26,580 --> 00:29:30,731
Это предполагает, что в лучшем случае после первых двух предположений при

541
00:29:30,731 --> 00:29:35,220
совершенно оптимальной игре у вас останется примерно одна доля неопределенности.

542
00:29:35,220 --> 00:29:37,400
Это то же самое, что ограничиться двумя возможными предположениями.

543
00:29:37,400 --> 00:29:39,944
Поэтому я думаю, что будет справедливо и, возможно, довольно консервативно

544
00:29:39,944 --> 00:29:42,454
сказать, что вы никогда не сможете написать алгоритм, который бы достигал

545
00:29:42,454 --> 00:29:45,066
такого низкого среднего значения, как 3, потому что с доступными вам словами

546
00:29:45,066 --> 00:29:47,576
просто не хватит места, чтобы получить достаточно информации всего за два

547
00:29:47,576 --> 00:29:50,460
шага. способен гарантировать ответ в третьем слоте каждый раз в обязательном порядке.

