1
00:00:00,000 --> 00:00:04,060
The game Wurdle has gone pretty viral in the last month or two, and never one to overlook

2
00:00:04,060 --> 00:00:07,960
an opportunity for a math lesson, it occurs to me that this game makes for a very good

3
00:00:07,960 --> 00:00:12,300
central example in a lesson about information theory, and in particular a topic known as

4
00:00:12,300 --> 00:00:12,660
entropy.

5
00:00:13,920 --> 00:00:17,300
You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of

6
00:00:17,300 --> 00:00:21,320
programmers I also got sucked into trying to write an algorithm that would play the

7
00:00:21,320 --> 00:00:22,740
game as optimally as it could.

8
00:00:23,180 --> 00:00:26,200
And what I thought I'd do here is just talk through with you some of my process in that,

9
00:00:26,400 --> 00:00:29,980
and explain some of the math that went into it, since the whole algorithm centers on this

10
00:00:29,980 --> 00:00:31,080
idea of entropy.

11
00:00:38,700 --> 00:00:41,640
First things first, in case you haven't heard of it, what is Wurdle?

12
00:00:42,040 --> 00:00:45,020
And to kill two birds with one stone here while we go through the rules of the game,

13
00:00:45,360 --> 00:00:48,980
let me also preview where we're going with this, which is to develop a little algorithm

14
00:00:48,980 --> 00:00:51,040
that will basically play the game for us.

15
00:00:51,360 --> 00:00:54,740
Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot

16
00:00:54,740 --> 00:00:55,100
does.

17
00:00:55,480 --> 00:00:59,600
The goal of Wurdle is to guess a mystery five letter word, and you're given six different

18
00:00:59,600 --> 00:01:00,340
chances to guess.

19
00:01:00,840 --> 00:01:04,380
For example, my Wurdle bot suggests that I start with the guess crane.

20
00:01:05,180 --> 00:01:09,400
Each time that you make a guess, you get some information about how close your guess is

21
00:01:09,400 --> 00:01:10,220
to the true answer.

22
00:01:10,920 --> 00:01:14,100
Here the grey box is telling me there's no C in the actual answer.

23
00:01:14,520 --> 00:01:17,840
The yellow box is telling me there is an R, but it's not in that position.

24
00:01:18,240 --> 00:01:21,820
The green box is telling me that the secret word does have an A, and it's in the third

25
00:01:21,820 --> 00:01:22,240
position.

26
00:01:22,720 --> 00:01:24,580
And then there's no N and there's no E.

27
00:01:25,200 --> 00:01:27,340
So let me just go in and tell the Wurdle bot that information.

28
00:01:27,340 --> 00:01:30,320
We started with crane, we got grey, yellow, green, grey, grey.

29
00:01:31,420 --> 00:01:34,940
Don't worry about all the data that it's showing right now, I'll explain that in due time.

30
00:01:35,460 --> 00:01:38,820
But its top suggestion for our second pick is shtick.

31
00:01:39,560 --> 00:01:43,540
And your guess does have to be an actual five letter word, but as you'll see, it's pretty

32
00:01:43,540 --> 00:01:45,400
liberal with what it will actually let you guess.

33
00:01:46,200 --> 00:01:47,440
In this case, we try shtick.

34
00:01:48,780 --> 00:01:50,180
And alright, things are looking pretty good.

35
00:01:50,260 --> 00:01:53,980
We hit the S and the H, so we know the first three letters, we know that there's an R.

36
00:01:53,980 --> 00:01:58,700
And so it's going to be like S-H-A something R, or S-H-A R something.

37
00:01:59,620 --> 00:02:03,280
And it looks like the Wurdle bot knows that it's down to just two possibilities, either

38
00:02:03,280 --> 00:02:04,240
shard or sharp.

39
00:02:05,100 --> 00:02:08,780
That's kind of a tossup between them at this point, so I guess probably just because it's

40
00:02:08,780 --> 00:02:10,080
alphabetical it goes with shard.

41
00:02:11,220 --> 00:02:13,780
Which hooray, is the actual answer, so we got it in three.

42
00:02:14,600 --> 00:02:17,840
If you're wondering if that's any good, the way I heard one person phrase it is that with

43
00:02:17,840 --> 00:02:20,360
Wurdle, four is par and three is birdie.

44
00:02:20,680 --> 00:02:22,480
Which I think is a pretty apt analogy.

45
00:02:22,480 --> 00:02:27,020
You have to be consistently on your game to be getting four, but it's certainly not crazy.

46
00:02:27,180 --> 00:02:29,920
But when you get it in three, it just feels great.

47
00:02:30,880 --> 00:02:33,780
So if you're down for it, what I'd like to do here is just talk through my thought process

48
00:02:33,780 --> 00:02:35,960
from the beginning for how I approach the Wurdle bot.

49
00:02:36,480 --> 00:02:39,440
And like I said, really it's an excuse for an information theory lesson.

50
00:02:39,740 --> 00:02:42,820
The main goal is to explain what is information and what is entropy.

51
00:02:48,220 --> 00:02:52,120
My first thought in approaching this was to take a look at the relative frequencies of

52
00:02:52,120 --> 00:02:53,720
different letters in the English language.

53
00:02:54,380 --> 00:02:57,980
So I thought, okay, is there an opening guess or an opening pair of guesses that hits a

54
00:02:57,980 --> 00:02:59,260
lot of these most frequent letters?

55
00:02:59,960 --> 00:03:03,000
And one that I was pretty fond of was doing other followed by nails.

56
00:03:03,760 --> 00:03:07,040
The thought is that if you hit a letter, you know, you get a green or a yellow, that always

57
00:03:07,040 --> 00:03:08,840
feels good, it feels like you're getting information.

58
00:03:09,340 --> 00:03:13,180
But in these cases, even if you don't hit and you always get grays, that's still giving

59
00:03:13,180 --> 00:03:16,720
you a lot of information, since it's pretty rare to find a word that doesn't have any

60
00:03:16,720 --> 00:03:17,400
of these letters.

61
00:03:18,140 --> 00:03:21,960
But even still, that doesn't feel super systematic, because for example, it does nothing to

62
00:03:21,960 --> 00:03:23,200
consider the order of the letters.

63
00:03:23,560 --> 00:03:25,300
Why type nails when I could type snail?

64
00:03:26,080 --> 00:03:27,500
Is it better to have that S at the end?

65
00:03:27,820 --> 00:03:28,680
I'm not really sure.

66
00:03:29,240 --> 00:03:33,480
Now, a friend of mine said that he liked to open with the word weary, which kind of surprised

67
00:03:33,480 --> 00:03:36,540
me because it has some uncommon letters in there like the W and the Y.

68
00:03:37,120 --> 00:03:39,000
But who knows, maybe that is a better opener.

69
00:03:39,320 --> 00:03:43,960
Is there some kind of quantitative score that we can give to judge the quality of a potential

70
00:03:43,960 --> 00:03:44,320
guess?

71
00:03:45,340 --> 00:03:48,740
Now to set up for the way that we're going to rank possible guesses, let's go back and

72
00:03:48,740 --> 00:03:51,420
add a little clarity to how exactly the game is set up.

73
00:03:51,420 --> 00:03:55,460
So there's a list of words that it will allow you to enter that are considered valid guesses

74
00:03:55,460 --> 00:03:57,880
that's just about 13,000 words long.

75
00:03:58,320 --> 00:04:01,560
But when you look at it, there's a lot of really uncommon things, things like a head

76
00:04:01,560 --> 00:04:06,440
or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble.

77
00:04:06,960 --> 00:04:10,540
But the vibe of the game is that the answer is always going to be a decently common word.

78
00:04:10,960 --> 00:04:15,360
And in fact, there's another list of around 2300 words that are the possible answers.

79
00:04:15,940 --> 00:04:20,100
And this is a human curated list, I think specifically by the game creators girlfriend,

80
00:04:20,100 --> 00:04:21,160
which is kind of fun.

81
00:04:21,820 --> 00:04:25,620
But what I would like to do, our challenge for this project is to see if we can write

82
00:04:25,620 --> 00:04:30,180
a program solving wordle that doesn't incorporate previous knowledge about this list.

83
00:04:30,720 --> 00:04:34,340
For one thing, there's plenty of pretty common five letter words that you won't find in that

84
00:04:34,340 --> 00:04:34,640
list.

85
00:04:34,940 --> 00:04:38,440
So it would be better to write a program that's a little more resilient and would play wordle

86
00:04:38,440 --> 00:04:41,460
against anyone, not just what happens to be the official website.

87
00:04:41,920 --> 00:04:46,020
And also, the reason that we know what this list of possible answers is, is because it's

88
00:04:46,020 --> 00:04:47,000
visible in the source code.

89
00:04:47,000 --> 00:04:51,640
But the way that it's visible in the source code is in the specific order in which answers

90
00:04:51,640 --> 00:04:55,840
come up from day to day, that you could always just look up what tomorrow's answer will be.

91
00:04:56,420 --> 00:04:58,880
So clearly, there's some sense in which using the list is cheating.

92
00:04:59,100 --> 00:05:02,980
And what makes for a more interesting puzzle and a richer information theory lesson is

93
00:05:02,980 --> 00:05:07,660
to instead use some more universal data like relative word frequencies in general to capture

94
00:05:07,660 --> 00:05:10,440
this intuition of having a preference for more common words.

95
00:05:11,600 --> 00:05:15,900
So of these 13,000 possibilities, how should we choose the opening guess?

96
00:05:16,400 --> 00:05:19,780
For example, if my friend proposes weary, how should we analyze its quality?

97
00:05:20,520 --> 00:05:25,140
Well, the reason he said he likes that unlikely W is that he likes the long shot nature of

98
00:05:25,140 --> 00:05:27,340
just how good it feels if you do hit that W.

99
00:05:27,920 --> 00:05:31,480
For example, if the first pattern revealed was something like this, then it turns out

100
00:05:31,480 --> 00:05:35,600
there are only 58 words in this giant lexicon that match that pattern.

101
00:05:36,060 --> 00:05:38,400
So that's a huge reduction from 13,000.

102
00:05:38,780 --> 00:05:43,020
But the flip side of that, of course, is that it's very uncommon to get a pattern like this.

103
00:05:43,020 --> 00:05:47,600
Specifically, if each word was equally likely to be the answer, the probability of hitting

104
00:05:47,600 --> 00:05:51,040
this pattern would be 58 divided by around 13,000.

105
00:05:51,580 --> 00:05:53,600
Of course, they're not equally likely to be answers.

106
00:05:53,720 --> 00:05:56,220
Most of these are very obscure and even questionable words.

107
00:05:56,600 --> 00:05:59,560
But at least for our first pass at all of this, let's assume that they're all equally

108
00:05:59,560 --> 00:06:01,600
likely and then refine that a bit later.

109
00:06:02,020 --> 00:06:06,720
The point is, the pattern with a lot of information is, by its very nature, unlikely to occur.

110
00:06:07,280 --> 00:06:10,800
In fact, what it means to be informative is that it's unlikely.

111
00:06:11,720 --> 00:06:16,620
A much more probable pattern to see with this opening would be something like this, where,

112
00:06:16,800 --> 00:06:18,120
of course, there's not a W in it.

113
00:06:18,240 --> 00:06:21,400
Maybe there's an E, and maybe there's no A, there's no R, there's no Y.

114
00:06:22,080 --> 00:06:24,560
In this case, there are 1400 possible matches.

115
00:06:25,080 --> 00:06:29,660
If all were equally likely, it works out to be a probability of about 11% that this is

116
00:06:29,660 --> 00:06:30,600
the pattern you would see.

117
00:06:30,900 --> 00:06:33,340
So the most likely outcomes are also the least informative.

118
00:06:34,240 --> 00:06:38,060
To get a more global view here, let me show you the full distribution of probabilities

119
00:06:38,060 --> 00:06:41,140
across all of the different patterns that you might see.

120
00:06:41,740 --> 00:06:46,060
So each bar that you're looking at corresponds to a possible pattern of colors that could

121
00:06:46,060 --> 00:06:50,560
be revealed, of which there are 3 to the 5th possibilities, and they're organized from

122
00:06:50,560 --> 00:06:52,340
left to right, most common to least common.

123
00:06:52,920 --> 00:06:56,000
So the most common possibility here is that you get all grays.

124
00:06:56,100 --> 00:06:58,120
That happens about 14% of the time.

125
00:06:58,580 --> 00:07:02,080
And what you're hoping for when you make a guess is that you end up somewhere out in

126
00:07:02,080 --> 00:07:06,480
this long tail, like over here, where there's only 18 possibilities for what matches this

127
00:07:06,480 --> 00:07:09,140
pattern, that evidently look like this.

128
00:07:09,920 --> 00:07:13,800
Or if we venture a little farther to the left, you know, maybe we go all the way over here.

129
00:07:14,940 --> 00:07:16,180
Okay, here's a good puzzle for you.

130
00:07:16,540 --> 00:07:20,700
What are the three words in the English language that start with a W, end with a Y, and have

131
00:07:20,700 --> 00:07:22,000
an R somewhere in them?

132
00:07:22,480 --> 00:07:26,800
Turns out, the answers are, let's see, wordy, wormy, and wryly.

133
00:07:27,500 --> 00:07:32,760
So to judge how good this word is overall, we want some kind of measure of the expected

134
00:07:32,760 --> 00:07:35,740
amount of information that you're going to get from this distribution.

135
00:07:35,740 --> 00:07:40,860
If we go through each pattern and we multiply its probability of occurring times something

136
00:07:40,860 --> 00:07:44,720
that measures how informative it is, that can maybe give us an objective score.

137
00:07:45,960 --> 00:07:49,840
Now your first instinct for what that something should be might be the number of matches.

138
00:07:50,160 --> 00:07:52,400
You want a lower average number of matches.

139
00:07:52,800 --> 00:07:56,860
But instead I'd like to use a more universal measurement that we often ascribe to information,

140
00:07:57,320 --> 00:08:01,120
and one that will be more flexible once we have a different probability assigned to each

141
00:08:01,120 --> 00:08:04,260
of these 13,000 words for whether or not they're actually the answer.

142
00:08:10,320 --> 00:08:14,600
The standard unit of information is the bit, which has a little bit of a funny formula,

143
00:08:14,620 --> 00:08:16,980
but is really intuitive if we just look at examples.

144
00:08:17,780 --> 00:08:21,960
If you have an observation that cuts your space of possibilities in half, we say that

145
00:08:21,960 --> 00:08:23,500
it has one bit of information.

146
00:08:24,180 --> 00:08:27,760
In our example, the space of possibilities is all possible words, and it turns out about

147
00:08:27,760 --> 00:08:31,260
half of the five letter words have an S, a little less than that, but about half.

148
00:08:31,780 --> 00:08:34,320
So that observation would give you one bit of information.

149
00:08:34,880 --> 00:08:39,780
If instead a new fact chops down that space of possibilities by a factor of four, we say

150
00:08:39,780 --> 00:08:41,500
that it has two bits of information.

151
00:08:41,980 --> 00:08:44,460
For example, it turns out about a quarter of these words have a T.

152
00:08:45,020 --> 00:08:49,080
If the observation cuts that space by a factor of eight, we say it's three bits of information,

153
00:08:49,540 --> 00:08:50,720
and so on and so forth.

154
00:08:50,900 --> 00:08:53,880
Four bits cuts it into a sixteenth, five bits cuts it into a thirty second.

155
00:08:54,960 --> 00:08:58,300
So now's when you might want to take a moment and pause and ask for yourself, what is the

156
00:08:58,300 --> 00:09:02,980
formula for information for the number of bits in terms of the probability of an occurrence?

157
00:09:03,920 --> 00:09:07,080
Well, what we're saying here is basically that when you take one half to the number

158
00:09:07,080 --> 00:09:11,380
of bits, that's the same thing as the probability, which is the same thing as saying two to the

159
00:09:11,380 --> 00:09:15,460
power of the number of bits is one over the probability, which rearranges further to saying

160
00:09:15,460 --> 00:09:18,920
the information is the log base two of one divided by the probability.

161
00:09:19,620 --> 00:09:23,220
And sometimes you see this with one more rearrangement still where the information is the negative

162
00:09:23,220 --> 00:09:24,900
log base two of the probability.

163
00:09:25,660 --> 00:09:29,160
Expressed like this, it can look a little bit weird to the uninitiated, but it really

164
00:09:29,160 --> 00:09:33,360
is just the very intuitive idea of asking how many times you've cut down your possibilities

165
00:09:33,360 --> 00:09:34,080
in half.

166
00:09:35,180 --> 00:09:37,940
Now if you're wondering, you know, I thought we were just playing a fun word game, why

167
00:09:37,940 --> 00:09:39,300
are logarithms entering the picture?

168
00:09:39,780 --> 00:09:43,660
One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events,

169
00:09:43,940 --> 00:09:48,160
much easier to say that an observation has 20 bits of information than it is to say that

170
00:09:48,160 --> 00:09:52,940
the probability of such and such occurring is 0.0000095.

171
00:09:53,300 --> 00:09:57,400
But a more substantive reason that this logarithmic expression turned out to be a very useful

172
00:09:57,400 --> 00:10:01,460
addition to the theory of probability is the way that information adds together.

173
00:10:02,060 --> 00:10:05,620
For example, if one observation gives you two bits of information, cutting your space

174
00:10:05,620 --> 00:10:10,220
down by four, and then a second observation like your second guess in Wordle gives you

175
00:10:10,220 --> 00:10:14,040
another three bits of information, chopping you down further by another factor of eight,

176
00:10:14,440 --> 00:10:16,740
the two together give you five bits of information.

177
00:10:17,160 --> 00:10:21,020
In the same way that probabilities like to multiply, information likes to add.

178
00:10:21,960 --> 00:10:24,980
So as soon as we're in the realm of something like an expected value, where we're adding

179
00:10:24,980 --> 00:10:27,980
a bunch of numbers up, the logs make it a lot nicer to deal with.

180
00:10:28,480 --> 00:10:32,640
Let's go back to our distribution for weary and add another little tracker on here, showing

181
00:10:32,640 --> 00:10:34,940
us how much information there is for each pattern.

182
00:10:35,580 --> 00:10:38,820
The main thing I want you to notice is that the higher the probability as we get to those

183
00:10:38,820 --> 00:10:42,780
more likely patterns, the lower the information, the fewer bits you gain.

184
00:10:43,500 --> 00:10:47,480
The way we measure the quality of this guess will be to take the expected value of this

185
00:10:47,480 --> 00:10:48,020
information.

186
00:10:48,420 --> 00:10:52,160
When we go through each pattern, we say how probable is it and then we multiply that by

187
00:10:52,160 --> 00:10:54,060
how many bits of information do we get.

188
00:10:54,710 --> 00:10:58,120
And in the example of weary, that turns out to be 4.9 bits.

189
00:10:58,560 --> 00:11:02,800
So on average, the information you get from this opening guess is as good as chopping

190
00:11:02,800 --> 00:11:05,480
your space of possibilities in half about five times.

191
00:11:05,960 --> 00:11:10,400
By contrast, an example of a guess with a higher expected information value would be

192
00:11:10,400 --> 00:11:11,640
something like slate.

193
00:11:13,120 --> 00:11:15,620
In this case, you'll notice the distribution looks a lot flatter.

194
00:11:15,940 --> 00:11:20,840
In particular, the most probable occurrence of all grays only has about a 6% chance of

195
00:11:20,840 --> 00:11:25,260
occurring, so at minimum you're getting evidently 3.9 bits of information.

196
00:11:25,920 --> 00:11:28,560
But that's a minimum, more typically you'd get something better than that.

197
00:11:29,100 --> 00:11:32,740
And it turns out when you crunch the numbers on this one and add up all the relevant terms,

198
00:11:33,320 --> 00:11:35,900
the average information is about 5.8.

199
00:11:37,360 --> 00:11:42,180
So in contrast with weary, your space of possibilities will be about half as big after this first

200
00:11:42,180 --> 00:11:43,540
guess, on average.

201
00:11:44,420 --> 00:11:48,300
There's actually a fun story about the name for this expected value of information quantity.

202
00:11:48,300 --> 00:11:52,080
You see, information theory was developed by Claude Shannon, who was working at Bell

203
00:11:52,080 --> 00:11:56,400
Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with

204
00:11:56,400 --> 00:12:00,560
John von Neumann, who was this intellectual giant of the time, very prominent in math

205
00:12:00,560 --> 00:12:03,560
and physics and the beginnings of what was becoming computer science.

206
00:12:04,100 --> 00:12:07,360
And when he mentioned that he didn't really have a good name for this expected value of

207
00:12:07,360 --> 00:12:12,380
information quantity, von Neumann supposedly said, so the story goes, well, you should

208
00:12:12,380 --> 00:12:14,200
call it entropy, and for two reasons.

209
00:12:14,540 --> 00:12:18,460
In the first place, your uncertainty function has been used in statistical mechanics under

210
00:12:18,460 --> 00:12:23,120
that name, so it already has a name, and in the second place, and more important, nobody

211
00:12:23,120 --> 00:12:26,760
knows what entropy really is, so in a debate you'll always have the advantage.

212
00:12:27,700 --> 00:12:31,520
So if the name seems a little bit mysterious, and if this story is to be believed, that's

213
00:12:31,520 --> 00:12:32,460
kind of by design.

214
00:12:33,280 --> 00:12:36,660
Also if you're wondering about its relation to all of that second law of thermodynamics

215
00:12:36,660 --> 00:12:40,900
stuff from physics, there definitely is a connection, but in its origins Shannon was

216
00:12:40,900 --> 00:12:44,880
just dealing with pure probability theory, and for our purposes here, when I use the

217
00:12:44,880 --> 00:12:49,200
word entropy, I just want you to think the expected information value of a particular

218
00:12:49,200 --> 00:12:49,580
guess.

219
00:12:50,700 --> 00:12:53,780
You can think of entropy as measuring two things simultaneously.

220
00:12:54,240 --> 00:12:56,780
The first one is how flat is the distribution?

221
00:12:57,320 --> 00:13:01,120
The closer a distribution is to uniform, the higher that entropy will be.

222
00:13:01,580 --> 00:13:06,040
In our case, where there are 3 to the 5th total patterns, for a uniform distribution,

223
00:13:06,040 --> 00:13:11,360
observing any one of them would have information log base 2 of 3 to the 5th, which happens

224
00:13:11,360 --> 00:13:17,300
to be 7.92, so that is the absolute maximum that you could possibly have for this entropy.

225
00:13:17,840 --> 00:13:21,760
But entropy is also kind of a measure of how many possibilities there are in the first

226
00:13:21,760 --> 00:13:22,080
place.

227
00:13:22,320 --> 00:13:26,520
For example, if you happen to have some word where there's only 16 possible patterns,

228
00:13:26,860 --> 00:13:32,180
and each one is equally likely, this entropy, this expected information, would be 4 bits.

229
00:13:32,580 --> 00:13:36,840
But if you have another word where there's 64 possible patterns that could come up, and

230
00:13:36,840 --> 00:13:40,480
they're all equally likely, then the entropy would work out to be 6 bits.

231
00:13:41,500 --> 00:13:45,840
So if you see some distribution out in the wild that has an entropy of 6 bits, it's

232
00:13:45,840 --> 00:13:50,060
sort of like it's saying there's as much variation and uncertainty in what's about

233
00:13:50,060 --> 00:13:53,500
to happen as if there were 64 equally likely outcomes.

234
00:13:54,360 --> 00:13:57,960
For my first pass at the Wurtelebot, I basically had it just do this.

235
00:13:57,960 --> 00:14:02,700
It goes through all of the different possible guesses that you could have, all 13,000 words,

236
00:14:03,220 --> 00:14:07,920
it computes the entropy for each one, or more specifically, the entropy of the distribution

237
00:14:07,920 --> 00:14:12,480
across all patterns that you might see for each one, and then it picks the highest, since

238
00:14:12,480 --> 00:14:16,140
that's the one that's likely to chop down your space of possibilities as much as possible.

239
00:14:17,140 --> 00:14:20,200
And even though I've only been talking about the first guess here, it does the same thing

240
00:14:20,200 --> 00:14:21,100
for the next few guesses.

241
00:14:21,560 --> 00:14:25,180
For example, after you see some pattern on that first guess, which would restrict you

242
00:14:25,180 --> 00:14:29,360
to a smaller number of possible words based on what matches with that, you just play the

243
00:14:29,360 --> 00:14:31,800
same game with respect to that smaller set of words.

244
00:14:32,260 --> 00:14:36,360
For a proposed second guess, you look at the distribution of all patterns that could occur

245
00:14:36,360 --> 00:14:41,500
from that more restricted set of words, you search through all 13,000 possibilities, and

246
00:14:41,500 --> 00:14:43,840
you find the one that maximizes that entropy.

247
00:14:45,420 --> 00:14:48,980
To show you how this works in action, let me just pull up a little variant of Wurtele

248
00:14:48,980 --> 00:14:52,180
that I wrote that shows the highlights of this analysis in the margins.

249
00:14:53,680 --> 00:14:57,760
So after doing all its entropy calculations, on the right here it's showing us which ones

250
00:14:57,760 --> 00:14:59,660
have the highest expected information.

251
00:15:00,280 --> 00:15:04,180
Turns out the top answer, at least at the moment, we'll refine this later, is Tares,

252
00:15:04,900 --> 00:15:10,580
which means, um, of course, a vetch, the most common vetch.

253
00:15:11,040 --> 00:15:14,260
Each time we make a guess here, where maybe I kind of ignore its recommendations and go

254
00:15:14,260 --> 00:15:19,260
with slate, because I like slate, we can see how much expected information it had, but

255
00:15:19,260 --> 00:15:23,300
then on the right of the word here it's showing us how much actual information we got given

256
00:15:23,300 --> 00:15:24,420
this particular pattern.

257
00:15:25,000 --> 00:15:28,660
So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened

258
00:15:28,660 --> 00:15:30,120
to get something with less than that.

259
00:15:30,600 --> 00:15:34,020
And then on the left side here it's showing us all of the different possible words given

260
00:15:34,020 --> 00:15:35,020
where we are now.

261
00:15:35,800 --> 00:15:39,920
The blue bars are telling us how likely it thinks each word is, so at the moment it's

262
00:15:39,920 --> 00:15:43,360
assuming each word is equally likely to occur, but we'll refine that in a moment.

263
00:15:44,060 --> 00:15:48,600
And then this uncertainty measurement is telling us the entropy of this distribution across

264
00:15:48,600 --> 00:15:53,360
the possible words, which right now, because it's a uniform distribution, is just a needlessly

265
00:15:53,360 --> 00:15:55,960
complicated way to count the number of possibilities.

266
00:15:56,560 --> 00:16:01,560
For example, if we were to take 2 to the power of 13.66, that should be around the 13,000

267
00:16:01,560 --> 00:16:02,180
possibilities.

268
00:16:02,900 --> 00:16:06,140
Um, a little bit off here, but only because I'm not showing all the decimal places.

269
00:16:06,720 --> 00:16:10,160
At the moment that might feel redundant and like it's overly complicating things, but

270
00:16:10,160 --> 00:16:12,340
you'll see why it's useful to have both numbers in a minute.

271
00:16:12,760 --> 00:16:16,620
So here it looks like it's suggesting the highest entropy for our second guess is Raman,

272
00:16:16,620 --> 00:16:19,400
which again just really doesn't feel like a word.

273
00:16:19,980 --> 00:16:24,060
So to take the moral high ground here I'm going to go ahead and type in Rains.

274
00:16:25,440 --> 00:16:27,340
And again it looks like we were a little unlucky.

275
00:16:27,520 --> 00:16:31,360
We were expecting 4.3 bits and we only got 3.39 bits of information.

276
00:16:31,940 --> 00:16:33,940
So that takes us down to 55 possibilities.

277
00:16:34,900 --> 00:16:38,900
And here maybe I'll just actually go with what it's suggesting, which is combo, whatever

278
00:16:38,900 --> 00:16:39,440
that means.

279
00:16:40,040 --> 00:16:42,920
And, okay, this is actually a good chance for a puzzle.

280
00:16:42,920 --> 00:16:46,380
It's telling us this pattern gives us 4.7 bits of information.

281
00:16:47,060 --> 00:16:51,720
But over on the left, before we see that pattern, there were 5.78 bits of uncertainty.

282
00:16:52,420 --> 00:16:56,340
So as a quiz for you, what does that mean about the number of remaining possibilities?

283
00:16:58,040 --> 00:17:02,560
Well it means that we're reduced down to 1 bit of uncertainty, which is the same thing

284
00:17:02,560 --> 00:17:04,540
as saying that there's 2 possible answers.

285
00:17:04,700 --> 00:17:05,700
It's a 50-50 choice.

286
00:17:06,500 --> 00:17:09,900
And from here, because you and I know which words are more common, we know that the answer

287
00:17:09,900 --> 00:17:10,640
should be abyss.

288
00:17:11,180 --> 00:17:13,280
But as it's written right now, the program doesn't know that.

289
00:17:13,540 --> 00:17:17,640
So it just keeps going, trying to gain as much information as it can, until it's only

290
00:17:17,640 --> 00:17:19,860
one possibility left, and then it guesses it.

291
00:17:20,380 --> 00:17:24,960
So obviously we need a better endgame strategy, but let's say we call this version 1 of our

292
00:17:24,960 --> 00:17:28,260
wordle solver, and then we go and run some simulations to see how it does.

293
00:17:30,360 --> 00:17:34,120
So the way this is working is it's playing every possible wordle game.

294
00:17:34,240 --> 00:17:38,540
It's going through all of those 2315 words that are the actual wordle answers.

295
00:17:38,540 --> 00:17:40,580
It's basically using that as a testing set.

296
00:17:41,360 --> 00:17:45,740
And with this naive method of not considering how common a word is, and just trying to maximize

297
00:17:45,740 --> 00:17:49,820
the information at each step along the way, until it gets down to one and only one choice.

298
00:17:50,360 --> 00:17:54,300
By the end of the simulation, the average score works out to be about 4.124.

299
00:17:55,320 --> 00:17:59,240
Which is not bad, to be honest, I kind of expect it to do worse.

300
00:17:59,660 --> 00:18:02,600
But the people who play wordle will tell you that they can usually get it in 4.

301
00:18:02,860 --> 00:18:05,380
The real challenge is to get as many in 3 as you can.

302
00:18:05,380 --> 00:18:08,080
It's a pretty big jump between the score of 4 and the score of 3.

303
00:18:08,860 --> 00:18:12,820
The obvious low-hanging fruit here is to somehow incorporate whether or not a word is common,

304
00:18:12,820 --> 00:18:14,980
and how exactly do we do that.

305
00:18:22,800 --> 00:18:26,880
The way I approached it is to get a list of the relative frequencies for all of the words

306
00:18:26,880 --> 00:18:32,060
in the English language, and I just used Mathematica's word frequency data function, which itself

307
00:18:32,060 --> 00:18:34,860
pulls from the Google Books English Ngram public dataset.

308
00:18:35,460 --> 00:18:38,720
And it's kind of fun to look at, for example if we sort it from the most common words to

309
00:18:38,720 --> 00:18:39,960
the least common words.

310
00:18:40,120 --> 00:18:43,080
Evidently these are the most common, 5-letter words in the English language.

311
00:18:43,700 --> 00:18:45,840
Or rather, these is the 8th most common.

312
00:18:46,280 --> 00:18:48,880
First is which, after which there's there and there.

313
00:18:49,260 --> 00:18:53,080
First itself is not first, but 9th, and it makes sense that these other words could come

314
00:18:53,080 --> 00:18:57,920
about more often, where those after first are after, where, and those being just a little

315
00:18:57,920 --> 00:18:58,580
bit less common.

316
00:18:59,160 --> 00:19:04,040
Now, in using this data to model how likely each of these words is to be the final answer,

317
00:19:04,380 --> 00:19:08,240
it shouldn't just be proportional to the frequency, because for example which is given

318
00:19:08,240 --> 00:19:14,000
a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000

319
00:19:14,000 --> 00:19:15,060
times less likely.

320
00:19:15,560 --> 00:19:18,840
But both of these are common enough words that they're almost certainly worth considering,

321
00:19:19,340 --> 00:19:21,000
so we want more of a binary cutoff.

322
00:19:21,860 --> 00:19:26,500
The way I went about it is to imagine taking this whole sorted list of words, and then

323
00:19:26,500 --> 00:19:31,060
arranging it on an x-axis, and then applying the sigmoid function, which is the standard

324
00:19:31,060 --> 00:19:35,640
way to have a function whose output is basically binary, it's either 0 or it's 1, but there's

325
00:19:35,640 --> 00:19:38,260
a smoothing in between for that region of uncertainty.

326
00:19:39,160 --> 00:19:43,940
So essentially, the probability that I'm assigning to each word for being in the final list will

327
00:19:43,940 --> 00:19:48,440
be the value of the sigmoid function above wherever it sits on the x-axis.

328
00:19:49,520 --> 00:19:53,940
Now obviously this depends on a few parameters, for example how wide a space on the x-axis

329
00:19:53,940 --> 00:19:59,720
those words fill determines how gradually or steeply we drop off from 1 to 0, and where

330
00:19:59,720 --> 00:20:02,140
we situate them left to right determines the cutoff.

331
00:20:02,980 --> 00:20:06,340
And to be honest the way I did this was kind of just licking my finger and sticking it

332
00:20:06,340 --> 00:20:06,920
into the wind.

333
00:20:07,140 --> 00:20:10,860
I looked through the sorted list and tried to find a window where when I looked at it

334
00:20:10,860 --> 00:20:15,220
I figured about half of these words are more likely than not to be the final answer, and

335
00:20:15,220 --> 00:20:16,120
used that as the cutoff.

336
00:20:17,100 --> 00:20:21,680
Now once we have a distribution like this across the words, it gives us another situation

337
00:20:21,680 --> 00:20:23,860
where entropy becomes this really useful measurement.

338
00:20:24,500 --> 00:20:28,100
For example, let's say we were playing a game and we start with my old openers, which

339
00:20:28,100 --> 00:20:32,480
were other and nails, and we end up with a situation where there's four possible words

340
00:20:32,480 --> 00:20:33,240
that match it.

341
00:20:33,560 --> 00:20:37,840
And let's say we consider them all equally likely, let me ask you, what is the entropy

342
00:20:37,840 --> 00:20:38,880
of this distribution?

343
00:20:41,080 --> 00:20:46,000
Well, the information associated with each one of these possibilities is going to be

344
00:20:46,000 --> 00:20:50,260
the log base 2 of 4, since each one is 1 and 4, and that's 2.

345
00:20:50,640 --> 00:20:52,460
2 bits of information, 4 possibilities.

346
00:20:52,760 --> 00:20:53,580
All very well and good.

347
00:20:54,300 --> 00:20:57,800
But what if I told you that actually there's more than 4 matches?

348
00:20:58,260 --> 00:21:02,460
In reality, when we look through the full word list, there are 16 words that match it.

349
00:21:02,580 --> 00:21:07,300
But suppose our model puts a really low probability on those other 12 words of actually being

350
00:21:07,300 --> 00:21:10,760
the final answer, something like 1 in 1000 because they're really obscure.

351
00:21:11,500 --> 00:21:14,260
Now let me ask you, what is the entropy of this distribution?

352
00:21:15,420 --> 00:21:19,640
If entropy was purely measuring the number of matches here, then you might expect it

353
00:21:19,640 --> 00:21:24,800
to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty

354
00:21:24,800 --> 00:21:25,700
than we had before.

355
00:21:26,180 --> 00:21:29,860
But of course the actual uncertainty is not really that different from what we had before.

356
00:21:30,160 --> 00:21:33,920
Just because there's these 12 really obscure words doesn't mean that it would be all that

357
00:21:33,920 --> 00:21:37,360
more surprising to learn that the final answer is charm, for example.

358
00:21:38,180 --> 00:21:41,920
So when you actually do the calculation here and you add up the probability of each occurrence

359
00:21:41,920 --> 00:21:46,020
times the corresponding information, what you get is 2.11 bits.

360
00:21:46,020 --> 00:21:50,620
Just saying, it's basically two bits, basically those four possibilities, but there's a little

361
00:21:50,620 --> 00:21:54,760
more uncertainty because of all of those highly unlikely events, though if you did learn them

362
00:21:54,760 --> 00:21:56,500
you'd get a ton of information from it.

363
00:21:57,160 --> 00:22:00,800
So zooming out, this is part of what makes Wordle such a nice example for an information

364
00:22:00,800 --> 00:22:01,400
theory lesson.

365
00:22:01,600 --> 00:22:04,640
We have these two distinct feeling applications for entropy.

366
00:22:05,160 --> 00:22:08,980
The first one telling us what's the expected information we'll get from a given guess,

367
00:22:09,500 --> 00:22:14,580
and the second one saying can we measure the remaining uncertainty among all of the words

368
00:22:14,580 --> 00:22:15,460
we have possible.

369
00:22:16,460 --> 00:22:19,780
And I should emphasize, in that first case where we're looking at the expected information

370
00:22:19,780 --> 00:22:24,540
of a guess, once we have an unequal weighting to the words, that affects the entropy calculation.

371
00:22:24,980 --> 00:22:28,600
For example, let me pull up that same case we were looking at earlier of the distribution

372
00:22:28,600 --> 00:22:33,260
associated with Weary, but this time using a non-uniform distribution across all possible

373
00:22:33,260 --> 00:22:33,720
words.

374
00:22:34,500 --> 00:22:38,280
So let me see if I can find a part here that illustrates it pretty well.

375
00:22:40,940 --> 00:22:42,360
Okay, here, this is pretty good.

376
00:22:42,360 --> 00:22:46,520
Here we have two adjacent patterns that are about equally likely, but one of them we're

377
00:22:46,520 --> 00:22:49,100
told has 32 possible words that match it.

378
00:22:49,280 --> 00:22:54,120
And if we check what they are, these are those 32, which are all just very unlikely words

379
00:22:54,120 --> 00:22:55,600
as you scan your eyes over them.

380
00:22:55,840 --> 00:23:00,540
It's hard to find any that feel like plausible answers, maybe yells, but if we look at the

381
00:23:00,540 --> 00:23:04,500
neighboring pattern in the distribution, which is considered just about as likely, we're

382
00:23:04,500 --> 00:23:06,660
told that it only has 8 possible matches.

383
00:23:06,880 --> 00:23:09,520
So a quarter as many matches, but it's about as likely.

384
00:23:09,860 --> 00:23:12,140
And when we pull up those matches, we can see why.

385
00:23:12,500 --> 00:23:16,300
Some of these are actual plausible answers like ring or wrath or raps.

386
00:23:17,900 --> 00:23:21,760
To illustrate how we incorporate all that, let me pull up version two of the Wordlebot

387
00:23:21,760 --> 00:23:22,300
here.

388
00:23:22,560 --> 00:23:25,280
And there are two or three main differences from the first one that we saw.

389
00:23:25,860 --> 00:23:29,500
First off, like I just said, the way that we're computing these entropies, these expected

390
00:23:29,500 --> 00:23:34,620
values of information, is now using the more refined distributions across the patterns

391
00:23:34,620 --> 00:23:38,240
that incorporates the probability that a given word would actually be the answer.

392
00:23:38,880 --> 00:23:43,820
As it happens, tears is still number one, though the ones following are a bit different.

393
00:23:44,360 --> 00:23:47,920
Second, when it ranks its top picks, it's now going to keep a model of the probability

394
00:23:47,920 --> 00:23:52,500
that each word is the actual answer, and it'll incorporate that into its decision, which

395
00:23:52,500 --> 00:23:55,080
is easier to see once we have a few guesses on the table.

396
00:23:55,860 --> 00:23:59,780
Again, ignoring its recommendation because we can't let machines rule our lives.

397
00:24:01,140 --> 00:24:05,140
And I suppose I should mention another thing different here is over on the left, that uncertainty

398
00:24:05,140 --> 00:24:09,180
value, that number of bits, is no longer just redundant with the number of possible

399
00:24:09,180 --> 00:24:09,640
matches.

400
00:24:10,080 --> 00:24:16,140
Now if we pull it up and calculate 2 to the 8.02, which would be a little above 256, I

401
00:24:16,140 --> 00:24:22,160
guess 259, what it's saying is even though there are 526 total words that actually match

402
00:24:22,160 --> 00:24:26,480
this pattern, the amount of uncertainty it has is more akin to what it would be if there

403
00:24:26,900 --> 00:24:28,980
were 259 equally likely outcomes.

404
00:24:29,720 --> 00:24:30,740
You can think of it like this.

405
00:24:31,020 --> 00:24:34,660
It knows borks is not the answer, same with yorts and zorl and zorus.

406
00:24:34,660 --> 00:24:37,680
So it's a little less uncertain than it was in the previous case.

407
00:24:37,820 --> 00:24:39,280
This number of bits will be smaller.

408
00:24:40,220 --> 00:24:44,180
And if I keep playing the game, I'm refining this down with a couple guesses that are apropos

409
00:24:44,180 --> 00:24:46,540
of what I would like to explain here.

410
00:24:48,360 --> 00:24:52,580
By the fourth guess, if you look over at its top picks, you can see it's no longer just

411
00:24:52,580 --> 00:24:53,760
maximizing the entropy.

412
00:24:54,460 --> 00:24:58,480
So at this point, there's technically seven possibilities, but the only ones with a meaningful

413
00:24:58,480 --> 00:25:00,300
chance are dorms and words.

414
00:25:00,300 --> 00:25:04,680
And you can see it ranks choosing both of those above all of these other values that

415
00:25:04,680 --> 00:25:06,720
strictly speaking would give more information.

416
00:25:07,240 --> 00:25:11,220
The very first time I did this, I just added up these two numbers to measure the quality

417
00:25:11,220 --> 00:25:13,900
of each guess, which actually worked better than you might suspect.

418
00:25:14,300 --> 00:25:15,900
But it really didn't feel systematic.

419
00:25:16,100 --> 00:25:17,880
And I'm sure there's other approaches people could take.

420
00:25:17,900 --> 00:25:19,340
But here's the one I landed on.

421
00:25:19,760 --> 00:25:24,260
If we're considering the prospect of a next guess, like in this case words, what we really

422
00:25:24,260 --> 00:25:27,900
care about is the expected score of our game if we do that.

423
00:25:28,230 --> 00:25:32,620
And to calculate that expected score, we say what's the probability that words is the actual

424
00:25:32,620 --> 00:25:35,900
answer, which at the moment it describes 58% to.

425
00:25:36,040 --> 00:25:39,540
We say with a 58% chance, our score in this game would be four.

426
00:25:40,320 --> 00:25:45,360
And then with the probability of one minus that 58%, our score will be more than that

427
00:25:45,360 --> 00:25:45,640
four.

428
00:25:46,220 --> 00:25:50,660
How much more we don't know, but we can estimate it based on how much uncertainty there's likely

429
00:25:50,660 --> 00:25:52,460
to be once we get to that point.

430
00:25:52,960 --> 00:25:55,940
Specifically, at the moment, there's 1.44 bits of uncertainty.

431
00:25:56,440 --> 00:26:01,120
If we guess words, it's telling us the expected information we'll get is 1.27 bits.

432
00:26:01,620 --> 00:26:06,280
So if we guess words, this difference represents how much uncertainty we're likely to be left

433
00:26:06,280 --> 00:26:07,660
with after that happens.

434
00:26:08,260 --> 00:26:12,520
What we need is some kind of function, which I'm calling f here, that associates this uncertainty

435
00:26:12,520 --> 00:26:13,740
with an expected score.

436
00:26:14,240 --> 00:26:18,060
And the way it went about this was to just plot a bunch of the data from previous games

437
00:26:18,060 --> 00:26:23,960
based on version one of the bot to say, hey, what was the actual score after various points

438
00:26:23,960 --> 00:26:26,320
with certain very measurable amounts of uncertainty?

439
00:26:27,020 --> 00:26:31,120
For example, these data points here that are sitting above a value that's around like 8.7

440
00:26:31,120 --> 00:26:36,220
or so are saying for some games, after a point at which there were 8.7 bits of uncertainty,

441
00:26:36,760 --> 00:26:38,960
it took two guesses to get the final answer.

442
00:26:39,320 --> 00:26:40,660
For other games, it took three guesses.

443
00:26:40,820 --> 00:26:42,240
For other games, it took four guesses.

444
00:26:43,140 --> 00:26:47,100
If we shift over to the left here, all the points over zero are saying whenever there's

445
00:26:47,100 --> 00:26:51,680
zero bits of uncertainty, which is to say there's only one possibility, then the number

446
00:26:51,680 --> 00:26:54,260
of guesses required is always just one, which is reassuring.

447
00:26:54,780 --> 00:26:58,460
Whenever there was one bit of uncertainty, meaning it was essentially just down to two

448
00:26:58,460 --> 00:27:02,460
possibilities, then sometimes it required one more guess, sometimes it required two

449
00:27:02,460 --> 00:27:05,240
more guesses, and so on and so forth here.

450
00:27:05,740 --> 00:27:10,220
Maybe a slightly easier way to visualize this data is to bucket it together and take averages.

451
00:27:11,000 --> 00:27:15,340
For example, this bar here is saying among all the points where we had one bit of uncertainty,

452
00:27:15,340 --> 00:27:19,960
on average the number of new guesses required was about 1.5.

453
00:27:22,140 --> 00:27:25,840
And the bar over here is saying among all of the different games where at some point

454
00:27:25,840 --> 00:27:30,540
the uncertainty was a little above four bits, which is like narrowing it down to 16 different

455
00:27:30,540 --> 00:27:35,000
possibilities, then on average it requires a little more than two guesses from that point

456
00:27:35,000 --> 00:27:35,380
forward.

457
00:27:36,060 --> 00:27:39,460
And from here I just did a regression to fit a function that seemed reasonable to this.

458
00:27:39,980 --> 00:27:44,160
And remember, the whole point of doing any of that is so that we can quantify this intuition

459
00:27:44,160 --> 00:27:48,960
that the more information we gain from a word, the lower the expected score will be.

460
00:27:49,680 --> 00:27:54,420
So, with this as version 2.0, if we go back and run the same set of simulations, having

461
00:27:54,420 --> 00:27:59,240
it play against all 2315 possible wordle answers, how does it do?

462
00:28:00,280 --> 00:28:03,420
Well in contrast to our first version, it's definitely better, which is reassuring.

463
00:28:04,020 --> 00:28:06,180
All said and done, the average is around 3.6.

464
00:28:06,540 --> 00:28:10,560
Although unlike the first version, there are a couple times that it loses, and requires

465
00:28:10,560 --> 00:28:12,120
more than six in this circumstance.

466
00:28:12,640 --> 00:28:16,100
Presumably because there's times when it's making that tradeoff to actually go for the

467
00:28:16,100 --> 00:28:17,940
goal rather than maximizing information.

468
00:28:19,040 --> 00:28:21,000
So can we do better than 3.6?

469
00:28:22,080 --> 00:28:22,920
We definitely can.

470
00:28:23,280 --> 00:28:27,300
Now, I said at the start that it's most fun to try not incorporating the true list of

471
00:28:27,300 --> 00:28:29,360
wordle answers into the way that it builds its model.

472
00:28:29,880 --> 00:28:34,180
But if we do incorporate it, the best performance I could get was around 3.43.

473
00:28:35,160 --> 00:28:39,640
So if we try to get more sophisticated than just using word frequency data to choose this

474
00:28:39,640 --> 00:28:44,080
prior distribution, this 3.43 probably gives a max at how good we could get with that,

475
00:28:44,220 --> 00:28:45,740
or at least how good I could get with that.

476
00:28:46,240 --> 00:28:49,860
That best performance essentially just uses the ideas that I've been talking about here,

477
00:28:50,020 --> 00:28:53,500
but it goes a little farther, like it does a search for the expected information two

478
00:28:53,500 --> 00:28:55,120
steps forward rather than just one.

479
00:28:55,620 --> 00:28:58,740
Originally I was planning on talking more about that, but I realize we've actually

480
00:28:58,740 --> 00:29:00,220
gone quite long as it is.

481
00:29:00,580 --> 00:29:03,560
The one thing I'll say is after doing this two-step search and then running a couple

482
00:29:03,560 --> 00:29:07,760
sample simulations in the top candidates, so far for me at least, it's looking like

483
00:29:07,760 --> 00:29:09,100
Crane is the best opener.

484
00:29:09,100 --> 00:29:10,060
Who would have guessed?

485
00:29:10,920 --> 00:29:15,660
Also if you use the true wordle list to determine your space of possibilities, then the uncertainty

486
00:29:15,660 --> 00:29:17,820
you start with is a little over 11 bits.

487
00:29:18,300 --> 00:29:22,600
And it turns out just from a brute force search, the maximum possible expected information

488
00:29:22,600 --> 00:29:25,880
after the first two guesses is around 10 bits.

489
00:29:26,500 --> 00:29:31,540
Which suggests that best case scenario, after your first two guesses, with perfectly optimal

490
00:29:31,540 --> 00:29:34,560
play, you'll be left with around one bit of uncertainty.

491
00:29:34,800 --> 00:29:37,320
Which is the same as being down to two possible guesses.

492
00:29:37,740 --> 00:29:41,460
But I think it's fair and probably pretty conservative to say that you could never possibly

493
00:29:41,460 --> 00:29:45,640
write an algorithm that gets this average as low as three, because with the words available

494
00:29:45,640 --> 00:29:50,500
to you, there's simply not room to get enough information after only two steps to be able

495
00:29:50,500 --> 00:29:53,800
to guarantee the answer in the third slot every single time without fail.

