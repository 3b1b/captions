[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy. ",
  "translatedText": "Wurdle গেমটি গত বা দুই মাসে বেশ ভাইরাল হয়েছে, এবং গণিত পাঠের সুযোগকে কখনই উপেক্ষা করা যায় না, এটি আমার কাছে ঘটে যে এই গেমটি তথ্য তত্ত্বের পাঠে একটি খুব ভাল কেন্দ্রীয় উদাহরণ তৈরি করে, এবং বিশেষ করে এনট্রপি নামে পরিচিত একটি বিষয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could. ",
  "translatedText": "আপনি দেখুন, অনেক লোকের মতো আমিও ধাঁধার মধ্যে পড়ে গিয়েছিলাম, এবং অনেক প্রোগ্রামারদের মতো আমিও এমন একটি অ্যালগরিদম লেখার চেষ্টা করেছিলাম যা গেমটিকে যথাসম্ভব সর্বোত্তমভাবে খেলতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy. ",
  "translatedText": "এবং আমি এখানে যা করব ভেবেছিলাম তা হল আপনার সাথে আমার কিছু প্রক্রিয়ার মাধ্যমে কথা বলুন এবং এতে কিছু গণিত ব্যাখ্যা করুন, যেহেতু পুরো অ্যালগরিদম এনট্রপির এই ধারণাকে কেন্দ্র করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle? ",
  "translatedText": "প্রথম জিনিস, যদি আপনি এটি না শুনে থাকেন, Wurdle কি? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us. ",
  "translatedText": "এবং এখানে এক ঢিলে দুটি পাখি মারার জন্য যখন আমরা খেলার নিয়মের মধ্য দিয়ে যাচ্ছি, আমাকেও প্রাকদর্শন করতে দিন যেখানে আমরা এটির সাথে যাচ্ছি, যা একটি ছোট অ্যালগরিদম বিকাশ করতে হবে যা মূলত আমাদের জন্য গেমটি খেলবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does. ",
  "translatedText": "যদিও আমি আজকের Wurdle করিনি, এটি 4ঠা ফেব্রুয়ারি, এবং আমরা দেখব বটটি কেমন করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess. ",
  "translatedText": "Wurdle এর লক্ষ্য হল একটি রহস্যময় পাঁচ অক্ষরের শব্দ অনুমান করা, এবং আপনাকে অনুমান করার জন্য ছয়টি ভিন্ন সুযোগ দেওয়া হয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane. ",
  "translatedText": "উদাহরণস্বরূপ, আমার Wurdle বট পরামর্শ দেয় যে আমি অনুমান ক্রেন দিয়ে শুরু করি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer. ",
  "translatedText": "প্রতিবার যখন আপনি একটি অনুমান করেন, আপনি আপনার অনুমান সত্য উত্তরের কতটা কাছাকাছি সে সম্পর্কে কিছু তথ্য পাবেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer. ",
  "translatedText": "এখানে ধূসর বাক্সটি আমাকে বলছে প্রকৃত উত্তরে কোন সি নেই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position. ",
  "translatedText": "হলুদ বাক্সটি আমাকে বলছে একটি R আছে, কিন্তু এটি সেই অবস্থানে নেই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position. ",
  "translatedText": "সবুজ বাক্সটি আমাকে বলছে যে গোপন শব্দটিতে একটি A আছে এবং এটি তৃতীয় অবস্থানে রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E. ",
  "translatedText": "এবং তারপর কোন N নেই এবং কোন E নেই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information. ",
  "translatedText": "তাই আমাকে শুধু যান এবং Wurdle বট যে তথ্য বলুন. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey. ",
  "translatedText": "আমরা ক্রেন দিয়ে শুরু করেছি, আমরা ধূসর, হলুদ, সবুজ, ধূসর, ধূসর পেয়েছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time. ",
  "translatedText": "এখন যে সমস্ত ডেটা দেখাচ্ছে সেগুলি নিয়ে চিন্তা করবেন না, আমি যথাসময়ে এটি ব্যাখ্যা করব।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick. ",
  "translatedText": "কিন্তু আমাদের দ্বিতীয় বাছাই এর জন্য এর শীর্ষ পরামর্শ হল shtick. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess. ",
  "translatedText": "এবং আপনার অনুমান একটি প্রকৃত পাঁচ অক্ষরের শব্দ হতে হবে, কিন্তু আপনি দেখতে পাবেন, এটা আসলে কি আপনাকে অনুমান করতে দেবে তার সাথে এটি বেশ উদার।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick. ",
  "translatedText": "এই ক্ষেত্রে, আমরা shtick চেষ্টা. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good. ",
  "translatedText": "এবং ঠিক আছে, জিনিসগুলি বেশ ভাল দেখাচ্ছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R. ",
  "translatedText": "আমরা S এবং H আঘাত করি, তাই আমরা প্রথম তিনটি অক্ষর জানি, আমরা জানি যে একটি R আছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something. ",
  "translatedText": "এবং তাই এটি SHA মত হতে যাচ্ছে কিছু R, বা SHA R কিছু. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp. ",
  "translatedText": "এবং দেখে মনে হচ্ছে Wurdle বট জানে যে এটি কেবল দুটি সম্ভাবনার মধ্যে রয়েছে, হয় ধারালো বা তীক্ষ্ণ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a toss up between them at this point, so I guess probably just because it's alphabetical it goes with shard. ",
  "translatedText": "এই মুহুর্তে এটি তাদের মধ্যে টস আপ ধরনের, তাই আমি সম্ভবত অনুমান কারণ এটি বর্ণানুক্রমিক এটি শার্ডের সাথে যায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer. ",
  "translatedText": "কোন হুররে, আসল উত্তর।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.22,
  "end": 132.86
 },
 {
  "input": "So we got it in three. ",
  "translatedText": "তাই আমরা তিনে পেয়েছিলাম।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 132.96,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle four is par and three is birdie. ",
  "translatedText": "আপনি যদি ভাবছেন যে এটি কোন ভাল কিনা, আমি যেভাবে একজন ব্যক্তির বাক্যাংশ শুনেছি তা হল যে Wurdle এর সাথে চারটি সমান এবং তিনটি হল বার্ডি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy. ",
  "translatedText": "যা আমি মনে করি একটি চমত্কার উপযুক্ত উপমা. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy. ",
  "translatedText": "চারটি পাওয়ার জন্য আপনাকে আপনার খেলায় ধারাবাহিকভাবে থাকতে হবে, তবে এটি অবশ্যই পাগল নয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great. ",
  "translatedText": "কিন্তু যখন আপনি এটি তিনটিতে পান, তখন এটি দুর্দান্ত লাগে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot. ",
  "translatedText": "সুতরাং আপনি যদি এটির জন্য নিচে থাকেন, আমি এখানে যা করতে চাই তা হল শুরু থেকে আমার চিন্তা প্রক্রিয়ার মাধ্যমে কথা বলার জন্য কিভাবে আমি Wurdle বটের সাথে যোগাযোগ করি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson. ",
  "translatedText": "এবং আমি যেমন বলেছি, সত্যিই এটি একটি তথ্য তত্ত্ব পাঠের জন্য একটি অজুহাত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy. ",
  "translatedText": "মূল লক্ষ্য হল তথ্য কী এবং এনট্রপি কী তা ব্যাখ্যা করা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language. ",
  "translatedText": "এটির কাছে যাওয়ার জন্য আমার প্রথম চিন্তাটি ছিল ইংরেজি ভাষার বিভিন্ন অক্ষরের আপেক্ষিক ফ্রিকোয়েন্সিগুলির দিকে নজর দেওয়া।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters? ",
  "translatedText": "তাই আমি ভেবেছিলাম, ঠিক আছে, এই সবচেয়ে ঘন ঘন অক্ষরগুলির একটি অনেক হিট একটি খোলার অনুমান বা অনুমান একটি খোলার জোড়া আছে? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails. ",
  "translatedText": "এবং একটি যে আমি বেশ পছন্দের ছিল নখ দ্বারা অনুসরণ অন্যান্য করা. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good. ",
  "translatedText": "চিন্তা হল যে যদি আপনি একটি চিঠি আঘাত করেন, আপনি জানেন, আপনি একটি সবুজ বা একটি হলুদ পেতে, যে সবসময় ভাল লাগে. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.76,
  "end": 187.52
 },
 {
  "input": "It feels like you're getting information. ",
  "translatedText": "মনে হচ্ছে আপনি তথ্য পাচ্ছেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.52,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information since it's pretty rare to find a word that doesn't have any of these letters. ",
  "translatedText": "কিন্তু এই ক্ষেত্রে, এমনকি যদি আপনি আঘাত না করেন এবং আপনি সর্বদা ধূসর হয়ে যান, তবুও এটি আপনাকে অনেক তথ্য দিচ্ছে কারণ এই অক্ষরগুলির মধ্যে কোনটি নেই এমন একটি শব্দ খুঁজে পাওয়া খুবই বিরল।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters. ",
  "translatedText": "কিন্তু তারপরও, এটি খুব পদ্ধতিগত মনে হয় না, কারণ উদাহরণস্বরূপ, এটি অক্ষরের ক্রম বিবেচনা করার জন্য কিছুই করে না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail? ",
  "translatedText": "আমি যখন শামুক টাইপ করতে পারি তখন কেন নখ টাইপ করব? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end? ",
  "translatedText": "শেষের দিকে S থাকলে কি ভালো? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y. ",
  "translatedText": "আমি সত্যিই নিশ্চিত নই এখন, আমার এক বন্ধু বলেছেন যে তিনি ক্লান্ত শব্দটি দিয়ে খুলতে পছন্দ করেছেন, যা আমাকে অবাক করেছে কারণ এতে W এবং Y এর মতো কিছু অস্বাভাবিক অক্ষর রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener. ",
  "translatedText": "কিন্তু কে জানে, হয়তো সেটাই ভালো ওপেনার।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess? ",
  "translatedText": "কোন ধরণের পরিমাণগত স্কোর আছে যা আমরা একটি সম্ভাব্য অনুমানের গুণমান বিচার করতে পারি? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up. ",
  "translatedText": "এখন আমরা সম্ভাব্য অনুমানগুলিকে যেভাবে র‌্যাঙ্ক করতে যাচ্ছি তার জন্য সেট আপ করতে, আসুন ফিরে যাই এবং গেমটি ঠিক কীভাবে সেট আপ করা হয় সে সম্পর্কে একটু স্পষ্টতা যোগ করি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long. ",
  "translatedText": "তাই শব্দগুলির একটি তালিকা রয়েছে যা এটি আপনাকে প্রবেশ করার অনুমতি দেবে যা বৈধ অনুমান হিসাবে বিবেচিত হয় যা প্রায় 13,000 শব্দ দীর্ঘ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble. ",
  "translatedText": "কিন্তু যখন আপনি এটির দিকে তাকান, সেখানে অনেকগুলি সত্যিই অস্বাভাবিক জিনিস রয়েছে, মাথা বা আলি এবং এআরজির মতো জিনিসগুলি, স্ক্র্যাবলের একটি খেলায় পারিবারিক তর্ক নিয়ে আসে এমন ধরনের শব্দ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word. ",
  "translatedText": "কিন্তু গেমের স্পন্দন হল যে উত্তরটি সর্বদা একটি শালীনভাবে সাধারণ শব্দ হতে চলেছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers. ",
  "translatedText": "এবং আসলে, প্রায় 2300 শব্দের আরেকটি তালিকা রয়েছে যা সম্ভাব্য উত্তর।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creator's girlfriend, which is kind of fun. ",
  "translatedText": "এবং এটি একটি মানুষের কিউরেটেড তালিকা, আমি বিশেষভাবে গেম স্রষ্টার বান্ধবী দ্বারা মনে করি, যা মজার ধরনের।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving Wordle that doesn't incorporate previous knowledge about this list. ",
  "translatedText": "কিন্তু আমি যা করতে চাই, এই প্রকল্পের জন্য আমাদের চ্যালেঞ্জ হল আমরা Wordle সমাধানকারী একটি প্রোগ্রাম লিখতে পারি কিনা যা এই তালিকা সম্পর্কে পূর্ববর্তী জ্ঞানকে অন্তর্ভুক্ত করে না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list. ",
  "translatedText": "এক জিনিসের জন্য, প্রচুর সাধারণ পাঁচটি অক্ষরের শব্দ রয়েছে যা আপনি সেই তালিকায় পাবেন না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play Wordle against anyone, not just what happens to be the official website. ",
  "translatedText": "তাই এমন একটি প্রোগ্রাম লেখা ভালো হবে যা একটু বেশি স্থিতিস্থাপক এবং যে কারো বিরুদ্ধে Wordle খেলবে, শুধু অফিসিয়াল ওয়েবসাইটই হবে না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also the reason that we know what this list of possible answers is, is because it's visible in the source code. ",
  "translatedText": "এবং এছাড়াও কারণ যে আমরা জানি সম্ভাব্য উত্তরগুলির এই তালিকাটি কী, কারণ এটি সোর্স কোডে দৃশ্যমান।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day. ",
  "translatedText": "কিন্তু সোর্স কোডে এটি যেভাবে দৃশ্যমান তা নির্দিষ্ট ক্রমানুসারে যেখানে উত্তর দিন দিন আসে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 292.8
 },
 {
  "input": "So you could always just look up what tomorrow's answer will be. ",
  "translatedText": "তাই আপনি সবসময় শুধু আগামীকালের উত্তর কি হবে তা দেখতে পারেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.06,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating. ",
  "translatedText": "তাই স্পষ্টভাবে, কিছু অর্থ আছে যেখানে তালিকা ব্যবহার করে প্রতারণা করা হয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words. ",
  "translatedText": "এবং যা একটি আরও আকর্ষণীয় ধাঁধা এবং একটি সমৃদ্ধ তথ্য তত্ত্ব পাঠের জন্য তৈরি করে তা হল এর পরিবর্তে আরও কিছু সর্বজনীন ডেটা ব্যবহার করা যেমন সাধারণভাবে আপেক্ষিক শব্দ ফ্রিকোয়েন্সিগুলি আরও সাধারণ শব্দগুলির জন্য পছন্দ করার এই অন্তর্দৃষ্টি ক্যাপচার করতে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess? ",
  "translatedText": "সুতরাং এই 13,000 সম্ভাবনার মধ্যে, আমরা কীভাবে শুরুর অনুমানটি বেছে নেব? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality? ",
  "translatedText": "উদাহরণস্বরূপ, যদি আমার বন্ধু ক্লান্তিকর প্রস্তাব দেয়, তাহলে আমরা কীভাবে এর গুণমান বিশ্লেষণ করব? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W. ",
  "translatedText": "ঠিক আছে, কারণ তিনি বলেছেন যে তিনি অসম্ভাব্য ডব্লিউ পছন্দ করেন তা হল যে তিনি লম্বা শট প্রকৃতি পছন্দ করেন যে আপনি যদি ডাব্লুকে আঘাত করেন তবে এটি কতটা ভাল লাগে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern. ",
  "translatedText": "উদাহরণস্বরূপ, যদি প্রকাশিত প্রথম প্যাটার্নটি এরকম কিছু হয়, তাহলে দেখা যাচ্ছে যে এই বিশাল অভিধানে শুধুমাত্র 58টি শব্দ আছে যা সেই প্যাটার্নের সাথে মেলে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000. ",
  "translatedText": "সুতরাং এটি 13,000 থেকে একটি বিশাল হ্রাস।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this. ",
  "translatedText": "কিন্তু যে উল্টানো দিক, অবশ্যই, এটা খুব অস্বাভাবিক এই মত একটি প্যাটার্ন পেতে. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000. ",
  "translatedText": "বিশেষ করে, যদি প্রতিটি শব্দ সমানভাবে উত্তর হওয়ার সম্ভাবনা থাকে, তাহলে এই প্যাটার্নটি আঘাত করার সম্ভাবনা 58 হবে প্রায় 13,000 দ্বারা ভাগ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers. ",
  "translatedText": "অবশ্যই, তারা সমানভাবে উত্তর হতে পারে না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words. ",
  "translatedText": "এগুলোর অধিকাংশই অত্যন্ত অস্পষ্ট এবং এমনকি প্রশ্নবিদ্ধ শব্দ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later. ",
  "translatedText": "কিন্তু অন্তত আমাদের প্রথম পাসের জন্য এই সব, আসুন অনুমান করা যাক যে তারা সবাই সমানভাবে সম্ভাব্য এবং তারপর একটু পরে পরিমার্জন করুন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is the pattern with a lot of information is by its very nature unlikely to occur. ",
  "translatedText": "বিন্দু তথ্য অনেক সঙ্গে প্যাটার্ন তার প্রকৃতি দ্বারা ঘটতে অসম্ভাব্য হয়. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely. ",
  "translatedText": "আসলে, তথ্যপূর্ণ বলতে যা বোঝায় তা হল এটি অসম্ভাব্য।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where of course there's not a W in it. ",
  "translatedText": "এই খোলার সাথে দেখতে অনেক বেশি সম্ভাব্য প্যাটার্ন এরকম কিছু হবে, যেখানে অবশ্যই এটিতে একটি W নেই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y. ",
  "translatedText": "হতে পারে একটি E আছে, এবং হতে পারে কোন A নেই, R নেই, Y নেই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches. ",
  "translatedText": "এই ক্ষেত্রে, 1400টি সম্ভাব্য ম্যাচ রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see. ",
  "translatedText": "যদি সবগুলি সমানভাবে সম্ভব হয়, তবে এটি প্রায় 11% এর সম্ভাব্যতা হিসাবে কাজ করে যে এই প্যাটার্নটি আপনি দেখতে পাবেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative. ",
  "translatedText": "তাই সর্বাধিক সম্ভাব্য ফলাফলগুলিও সর্বনিম্ন তথ্যপূর্ণ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see. ",
  "translatedText": "এখানে আরও বিশ্বব্যাপী দৃষ্টিভঙ্গি পেতে, আমি আপনাকে বিভিন্ন প্যাটার্নের মধ্যে সম্ভাব্যতার সম্পূর্ণ বন্টন দেখাই যা আপনি দেখতে পারেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common. ",
  "translatedText": "সুতরাং আপনি যে প্রতিটি বারটি দেখছেন তা একটি সম্ভাব্য রঙের প্যাটার্নের সাথে মিলে যায় যা প্রকাশ করা যেতে পারে, যার মধ্যে 3 থেকে 5 তম সম্ভাবনা রয়েছে এবং সেগুলি বাম থেকে ডানে সংগঠিত, সবচেয়ে সাধারণ থেকে সর্বনিম্ন সাধারণ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays. ",
  "translatedText": "সুতরাং এখানে সবচেয়ে সাধারণ সম্ভাবনা হল যে আপনি সমস্ত ধূসর পাবেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time. ",
  "translatedText": "এটি প্রায় 14% সময় ঘটে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here where there's only 18 possibilities for what matches this pattern that evidently look like this. ",
  "translatedText": "এবং আপনি যখন অনুমান করবেন তখন আপনি যা আশা করছেন তা হল আপনি এই লম্বা লেজের মধ্যে কোথাও শেষ হয়ে যাবেন, যেমন এখানে এখানে এই প্যাটার্নের সাথে মেলে যা স্পষ্টতই এইরকম দেখায় তার জন্য মাত্র 18টি সম্ভাবনা রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here. ",
  "translatedText": "অথবা যদি আমরা বাম দিকে একটু এগিয়ে যাই, আপনি জানেন, হয়তো আমরা এখানে সব পথ যাব।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you. ",
  "translatedText": "ঠিক আছে, এখানে আপনার জন্য একটি ভাল ধাঁধা আছে. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them? ",
  "translatedText": "ইংরেজি ভাষায় যে তিনটি শব্দ W দিয়ে শুরু হয়, Y দিয়ে শেষ হয় এবং তাদের মধ্যে কোথাও R আছে কি? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly. ",
  "translatedText": "দেখা যাচ্ছে, উত্তরগুলি হল, আসুন দেখি, শব্দময়, কৃমি, এবং শুষ্কভাবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution. ",
  "translatedText": "সুতরাং এই শব্দটি সামগ্রিকভাবে কতটা ভাল তা বিচার করার জন্য, আমরা চাই প্রত্যাশিত পরিমাণের তথ্য যা আপনি এই বিতরণ থেকে পেতে চলেছেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score. ",
  "translatedText": "যদি আমরা প্রতিটি প্যাটার্নের মধ্য দিয়ে যাই এবং আমরা এর সম্ভাব্যতাকে বহুগুণ করে গুন করি যেটি কতটা তথ্যপূর্ণ তা পরিমাপ করে, এটি আমাদের একটি উদ্দেশ্যমূলক স্কোর দিতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches. ",
  "translatedText": "এখন আপনার প্রথম প্রবৃত্তি যে কিছু হওয়া উচিত ম্যাচের সংখ্যা হতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches. ",
  "translatedText": "আপনি ম্যাচের একটি কম গড় সংখ্যা চান।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer. ",
  "translatedText": "কিন্তু এর পরিবর্তে আমি একটি আরও সার্বজনীন পরিমাপ ব্যবহার করতে চাই যা আমরা প্রায়শই তথ্যের জন্য দায়ী করি, এবং যেটি আরও নমনীয় হবে একবার আমাদের কাছে এই 13,000 শব্দের প্রতিটির জন্য আলাদা সম্ভাব্যতা বরাদ্দ করা হলে তারা আসলে উত্তর কিনা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but it's really intuitive if we just look at examples. ",
  "translatedText": "তথ্যের স্ট্যান্ডার্ড একক হল বিট, যেটিতে কিছুটা মজার সূত্র রয়েছে, কিন্তু আমরা যদি উদাহরণগুলি দেখি তবে এটি সত্যিই স্বজ্ঞাত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information. ",
  "translatedText": "আপনার যদি এমন কোনো পর্যবেক্ষণ থাকে যা আপনার সম্ভাবনার স্থানকে অর্ধেক করে ফেলে, আমরা বলি যে এতে এক বিট তথ্য রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about Half of the five letter words have an S, a little less than that, but about half. ",
  "translatedText": "আমাদের উদাহরণে, সম্ভাবনার স্থান হল সব সম্ভাব্য শব্দ, এবং এটি দেখা যাচ্ছে যে পাঁচটি অক্ষরের অর্ধেক শব্দের একটি S আছে, তার থেকে একটু কম, কিন্তু প্রায় অর্ধেক।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information. ",
  "translatedText": "যাতে পর্যবেক্ষণ আপনাকে এক বিট তথ্য দেবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information. ",
  "translatedText": "পরিবর্তে যদি একটি নতুন তথ্য সম্ভাবনার সেই স্থানটিকে চারটির একটি ফ্যাক্টর দ্বারা কমিয়ে দেয়, আমরা বলি যে এতে দুটি বিট তথ্য রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T. ",
  "translatedText": "উদাহরণস্বরূপ, দেখা যাচ্ছে এই শব্দগুলির প্রায় এক চতুর্থাংশের একটি টি আছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth. ",
  "translatedText": "যদি পর্যবেক্ষণটি সেই স্থানটিকে আটের একটি ফ্যাক্টর দ্বারা কেটে দেয়, আমরা বলি এটি তথ্যের তিনটি বিট, এবং আরও অনেক কিছু।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a 16th, five bits cuts it into a 32nd. ",
  "translatedText": "চারটি বিট এটিকে 16 তম, পাঁচ বিট এটিকে 32 তম অংশে কাটে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.52
 },
 {
  "input": "So now you might want to pause and ask yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence? ",
  "translatedText": "তাই এখন আপনি বিরতি দিতে এবং নিজেকে জিজ্ঞাসা করতে চাইতে পারেন, একটি ঘটনার সম্ভাব্যতার পরিপ্রেক্ষিতে বিটের সংখ্যার জন্য তথ্যের সূত্র কী? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 533.52,
  "end": 542.98
 },
 {
  "input": "What we're saying here is that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability. ",
  "translatedText": "আমরা এখানে যা বলছি তা হল যে আপনি যখন বিট সংখ্যার এক অর্ধেক নিয়ে যান, তখন এটি সম্ভাব্যতার মতো একই জিনিস, যা বিটের সংখ্যার শক্তিকে দুটি বলার মত একই জিনিস সম্ভাবনার উপরে এক, যা সম্ভাব্যতা দ্বারা বিভক্ত একটি এর মধ্যে দুটি লগ বেস হল তথ্যটি বলার জন্য আরও পুনর্বিন্যাস করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still, where the information is the negative log base two of the probability. ",
  "translatedText": "এবং কখনও কখনও আপনি এটিকে আরও একটি পুনর্বিন্যাস সহ দেখতে পান, যেখানে তথ্যটি সম্ভাব্যতার নেতিবাচক লগ বেস দুই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half. ",
  "translatedText": "এভাবে প্রকাশ করা হলে, এটি অপ্রশিক্ষিতদের কাছে কিছুটা অদ্ভুত লাগতে পারে, কিন্তু আপনি কতবার আপনার সম্ভাবনাগুলিকে অর্ধেক করে ফেলেছেন তা জিজ্ঞাসা করার এটি সত্যিই খুব স্বজ্ঞাত ধারণা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture? ",
  "translatedText": "এখন যদি আপনি ভাবছেন, আপনি জানেন, আমি ভেবেছিলাম আমরা শুধু একটি মজার শব্দ খেলা খেলছি, কেন লগারিদম ছবিতে প্রবেশ করছে? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095. ",
  "translatedText": "এটি একটি সুন্দর ইউনিট হওয়ার একটি কারণ হল খুব অসম্ভাব্য ইভেন্টগুলি সম্পর্কে কথা বলা অনেক সহজ, একটি পর্যবেক্ষণে 20 বিট তথ্য রয়েছে তা বলা অনেক সহজ যে এইরকম এবং এইরকম ঘটার সম্ভাবনা 0।0000095।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together. ",
  "translatedText": "কিন্তু এই লগারিদমিক অভিব্যক্তিটি সম্ভাব্যতার তত্ত্বের একটি খুব দরকারী সংযোজন হিসাবে পরিণত হওয়ার একটি আরও সারগর্ভ কারণ হল তথ্যগুলিকে একত্রিত করার উপায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information. ",
  "translatedText": "উদাহরণস্বরূপ, যদি একটি পর্যবেক্ষণ আপনাকে দুই বিট তথ্য দেয়, আপনার স্থান চারটি কমিয়ে দেয়, এবং তারপরে ওয়ার্ডলেতে আপনার দ্বিতীয় অনুমানের মতো একটি দ্বিতীয় পর্যবেক্ষণ আপনাকে আরও তিন বিট তথ্য দেয়, আপনাকে আরও আটটি ফ্যাক্টর দ্বারা কমিয়ে দেয়, দুটি একসাথে আপনাকে পাঁচ বিট তথ্য দেয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add. ",
  "translatedText": "একই ভাবে যে সম্ভাব্যতা গুন করতে পছন্দ করে, তথ্য যোগ করতে পছন্দ করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with. ",
  "translatedText": "তাই যত তাড়াতাড়ি আমরা একটি প্রত্যাশিত মানের মতো কিছুর রাজ্যে আছি, যেখানে আমরা সংখ্যার একটি গুচ্ছ যোগ করছি, লগগুলি এটি মোকাবেলা করতে অনেক সুন্দর করে তোলে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for Weary and add another little tracker on here, showing us how much information there is for each pattern. ",
  "translatedText": "আসুন আমরা Weary-এর জন্য আমাদের বিতরণে ফিরে যাই এবং এখানে আরেকটি ছোট ট্র্যাকার যোগ করি, প্রতিটি প্যাটার্নের জন্য কতটা তথ্য রয়েছে তা আমাদের দেখাই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain. ",
  "translatedText": "আমি আপনাকে লক্ষ্য করতে চাই যে প্রধান জিনিসটি হল আমরা যত বেশি সম্ভাব্য প্যাটার্নগুলি পেতে পারি, তত কম তথ্য, কম বিট আপনি লাভ করেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information, where we go through each pattern, we say how probable is it, and then we multiply that by how many bits of information do we get. ",
  "translatedText": "আমরা যেভাবে এই অনুমানের গুণমান পরিমাপ করব তা হল এই তথ্যের প্রত্যাশিত মান নেওয়া, যেখানে আমরা প্রতিটি প্যাটার্নের মধ্য দিয়ে যাই, আমরা বলি এটি কতটা সম্ভাব্য, এবং তারপর আমরা কত বিট তথ্য পাই তা দ্বারা গুণ করি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 643.5,
  "end": 654.06
 },
 {
  "input": "And in the example of Weary, that turns out to be 4.9 bits. ",
  "translatedText": "এবং Weary এর উদাহরণে, এটি 4 হতে দেখা যাচ্ছে।9 বিট।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times. ",
  "translatedText": "তাই গড়ে, এই প্রারম্ভিক অনুমান থেকে আপনি যে তথ্য পাবেন তা আপনার সম্ভাবনার স্থানকে প্রায় পাঁচবার অর্ধেকের মধ্যে কেটে ফেলার মতোই ভাল।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like Slate. ",
  "translatedText": "বিপরীতে, একটি উচ্চ প্রত্যাশিত তথ্য মান সহ অনুমানের একটি উদাহরণ স্লেটের মতো কিছু হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case you'll notice the distribution looks a lot flatter. ",
  "translatedText": "এই ক্ষেত্রে আপনি লক্ষ্য করবেন যে বিতরণটি অনেক চাটুকার দেখাচ্ছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information. ",
  "translatedText": "বিশেষ করে, সমস্ত ধূসর রঙের সবচেয়ে সম্ভাব্য ঘটনাটি ঘটার প্রায় 6% সম্ভাবনা রয়েছে, তাই সর্বনিম্ন আপনি স্পষ্টতই 3 পাচ্ছেন।তথ্য 9 বিট. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that. ",
  "translatedText": "কিন্তু এটি একটি সর্বনিম্ন, আরও সাধারণত আপনি এর চেয়ে ভাল কিছু পেতে চান।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8. ",
  "translatedText": "এবং এটি দেখা যাচ্ছে যখন আপনি এটির সংখ্যাগুলিকে ক্রাঞ্চ করেন এবং সমস্ত প্রাসঙ্গিক পদ যোগ করেন, গড় তথ্য প্রায় 5।8. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with Weary, your space of possibilities will be about half as big after this first guess, on average. ",
  "translatedText": "তাই Weary এর বিপরীতে, এই প্রথম অনুমানের পরে আপনার সম্ভাবনার স্থান গড়ে প্রায় অর্ধেক হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity. ",
  "translatedText": "তথ্যের পরিমাণের এই প্রত্যাশিত মানটির নাম সম্পর্কে আসলে একটি মজার গল্প রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "Information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science. ",
  "translatedText": "তথ্য তত্ত্বটি ক্লদ শ্যানন দ্বারা তৈরি করা হয়েছিল, যিনি 1940-এর দশকে বেল ল্যাবসে কর্মরত ছিলেন, কিন্তু তিনি জন ভন নিউম্যানের সাথে তার এখনও প্রকাশিত কিছু ধারণা সম্পর্কে কথা বলছিলেন, যিনি ছিলেন সেই সময়ের এই বুদ্ধিজীবী দৈত্য, খুব বিশিষ্ট গণিত এবং পদার্থবিদ্যা এবং কম্পিউটার বিজ্ঞান হয়ে উঠছে কি শুরু. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well you should call it entropy, and for two reasons. ",
  "translatedText": "এবং যখন তিনি উল্লেখ করেছিলেন যে তথ্যের পরিমাণের এই প্রত্যাশিত মূল্যের জন্য তার আসলেই একটি ভাল নাম নেই, তখন ভন নিউম্যান অনুমিতভাবে বলেছিলেন, তাই গল্পটি চলে, আপনার এটিকে এনট্রপি বলা উচিত এবং দুটি কারণে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage. ",
  "translatedText": "প্রথম স্থানে, আপনার অনিশ্চয়তা ফাংশনটি সেই নামে পরিসংখ্যানগত বলবিদ্যায় ব্যবহার করা হয়েছে, তাই এটির ইতিমধ্যে একটি নাম রয়েছে এবং দ্বিতীয় স্থানে এবং আরও গুরুত্বপূর্ণ, কেউ জানে না এনট্রপি আসলে কী, তাই একটি বিতর্কে আপনি সর্বদা সুবিধা আছে তাই যদি নামটি একটু রহস্যময় মনে হয়, এবং যদি এই গল্পটি বিশ্বাস করা হয়, এটি ডিজাইনের ধরণের।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess. ",
  "translatedText": "এছাড়াও আপনি যদি পদার্থবিজ্ঞানের থার্মোডাইনামিক্স স্টাফের সেই দ্বিতীয় সূত্রের সাথে এর সম্পর্ক সম্পর্কে ভাবছেন তবে অবশ্যই একটি সংযোগ রয়েছে, তবে এর উত্সে শ্যানন কেবল বিশুদ্ধ সম্ভাবনা তত্ত্বের সাথে কাজ করছিলেন এবং আমাদের উদ্দেশ্যে এখানে, যখন আমি ব্যবহার করি শব্দ এনট্রপি, আমি শুধু চাই যে আপনি একটি নির্দিষ্ট অনুমানের প্রত্যাশিত তথ্য মান চিন্তা করুন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously. ",
  "translatedText": "আপনি এনট্রপিকে একই সাথে দুটি জিনিস পরিমাপ হিসাবে ভাবতে পারেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution. ",
  "translatedText": "প্রথমটি হল বিতরণ কতটা সমতল।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be. ",
  "translatedText": "একটি বন্টন ইউনিফর্মের যত কাছাকাছি হবে, এনট্রপি তত বেশি হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy. ",
  "translatedText": "আমাদের ক্ষেত্রে, যেখানে 3 থেকে 5 তম মোট প্যাটার্ন রয়েছে, একটি অভিন্ন বন্টনের জন্য, তাদের যেকোনো একটি পর্যবেক্ষণ করলে তথ্য লগ বেস 2 এর মধ্যে 3 থেকে 5 তম হবে, যা 7 হবে।92, তাই এই এনট্রপির জন্য আপনার সম্ভাব্য সর্বোচ্চ যা হতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place. ",
  "translatedText": "কিন্তু এনট্রপি হল প্রথম স্থানে কতগুলি সম্ভাবনা রয়েছে তার একটি পরিমাপ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits. ",
  "translatedText": "উদাহরণস্বরূপ, যদি আপনার কাছে এমন কিছু শব্দ থাকে যেখানে শুধুমাত্র 16টি সম্ভাব্য প্যাটার্ন আছে, এবং প্রতিটির সমান সম্ভাবনা, এই এনট্রপি, এই প্রত্যাশিত তথ্য, 4 বিট হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits. ",
  "translatedText": "কিন্তু যদি আপনার কাছে অন্য একটি শব্দ থাকে যেখানে 64টি সম্ভাব্য প্যাটার্ন আসতে পারে, এবং সেগুলি সবই সমানভাবে সম্ভব, তাহলে এনট্রপিটি 6 বিট হতে কাজ করবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes. ",
  "translatedText": "তাই যদি আপনি বন্য মধ্যে কিছু বন্টন আউট দেখতে যে 6 বিট একটি এনট্রপি আছে, এটা সাজানোর মত এটা বলছে সেখানে অনেক বৈচিত্র্য এবং অনিশ্চয়তা আছে কি ঘটতে যাচ্ছে যদি 64 সমানভাবে সম্ভাব্য ফলাফল ছিল. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this. ",
  "translatedText": "Wurtelebot এ আমার প্রথম পাসের জন্য, আমি মূলত এটি করতে পেরেছিলাম।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the possible guesses you could have, all 13,000 words, computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns you might see, for each one, and picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible. ",
  "translatedText": "এটি আপনার সম্ভাব্য সমস্ত অনুমানগুলির মধ্য দিয়ে যায়, সমস্ত 13,000 শব্দ, প্রতিটির জন্য এনট্রপি গণনা করে, বা আরও নির্দিষ্টভাবে, প্রতিটির জন্য আপনি দেখতে পারেন এমন সমস্ত প্যাটার্ন জুড়ে বিতরণের এনট্রপি গণনা করে এবং সর্বোচ্চ বাছাই করে, যেহেতু এটি যেটি আপনার সম্ভাবনার স্থান যতটা সম্ভব কমিয়ে দিতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses. ",
  "translatedText": "এবং যদিও আমি এখানে শুধুমাত্র প্রথম অনুমান সম্পর্কে কথা বলছি, এটি পরবর্তী কয়েকটি অনুমানের জন্য একই জিনিস করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words. ",
  "translatedText": "উদাহরণ স্বরূপ, আপনি সেই প্রথম অনুমানে কিছু প্যাটার্ন দেখার পরে, যা আপনাকে সম্ভাব্য শব্দের একটি ছোট সংখ্যার মধ্যে সীমাবদ্ধ করবে তার সাথে কী মিলছে তার উপর ভিত্তি করে, আপনি শব্দের সেই ছোট সেটের ক্ষেত্রে একই খেলা খেলবেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy. ",
  "translatedText": "একটি প্রস্তাবিত দ্বিতীয় অনুমানের জন্য, আপনি শব্দের আরও সীমাবদ্ধ সেট থেকে ঘটতে পারে এমন সমস্ত নিদর্শনগুলির বিতরণের দিকে তাকান, আপনি সমস্ত 13,000 সম্ভাবনার মাধ্যমে অনুসন্ধান করেন এবং আপনি সেই এনট্রপিকে সর্বাধিক করে তোলে এমন একটি খুঁজে পান।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins. ",
  "translatedText": "এটি কীভাবে কাজ করে তা আপনাকে দেখানোর জন্য, আমাকে শুধু Wurtele-এর একটি ছোট বৈকল্পিক টানতে দিন যা আমি লিখেছিলাম যা মার্জিনে এই বিশ্লেষণের হাইলাইটগুলি দেখায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "After doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information. ",
  "translatedText": "এর সমস্ত এনট্রপি গণনা করার পরে, এখানে ডানদিকে এটি আমাদের দেখায় যে কোনটির কাছে সর্বাধিক প্রত্যাশিত তথ্য রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch. ",
  "translatedText": "শীর্ষ উত্তর সক্রিয় আউট, অন্তত মুহূর্তে, আমরা পরে এটি পরিমার্জিত করব, Tares, যার মানে, um, অবশ্যই, একটি vetch, সবচেয়ে সাধারণ vetch. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got, given this particular pattern. ",
  "translatedText": "প্রতিবার আমরা এখানে অনুমান করি, যেখানে আমি হয়তো এক প্রকার এর সুপারিশ উপেক্ষা করি এবং স্লেটের সাথে যাই, কারণ আমি স্লেট পছন্দ করি, আমরা দেখতে পারি এতে কতটা প্রত্যাশিত তথ্য ছিল, কিন্তু তারপরে এখানে শব্দের ডানদিকে এটি আমাদের দেখাচ্ছে কতটা প্রকৃত তথ্য আমরা পেয়েছি, এই বিশেষ প্যাটার্ন দেওয়া. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that. ",
  "translatedText": "তাই এখানে মনে হচ্ছে আমরা একটু দুর্ভাগা ছিলাম, আমরা 5 পাওয়ার আশা করছিলাম।8, কিন্তু আমরা যে কম সঙ্গে কিছু পেতে ঘটেছে. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now. ",
  "translatedText": "এবং তারপর এখানে বাম দিকে এটি আমাদের দেখায় বিভিন্ন সম্ভাব্য শব্দের সব প্রদত্ত আমরা এখন যেখানে. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment. ",
  "translatedText": "নীল বারগুলি আমাদের বলছে যে এটি প্রতিটি শব্দকে কতটা সম্ভাব্য মনে করে, তাই এই মুহুর্তে এটি অনুমান করছে যে প্রতিটি শব্দ সমানভাবে ঘটতে পারে, তবে আমরা এটিকে এক মুহূর্তের মধ্যে পরিমার্জন করব।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities. ",
  "translatedText": "এবং তারপর এই অনিশ্চয়তা পরিমাপ আমাদের সম্ভাব্য শব্দ জুড়ে এই বিতরণের এনট্রপি বলছে, যা এই মুহূর্তে, কারণ এটি একটি অভিন্ন বন্টন, সম্ভাবনার সংখ্যা গণনা করার জন্য একটি অপ্রয়োজনীয় জটিল উপায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities. ",
  "translatedText": "উদাহরণস্বরূপ, যদি আমরা 2 কে 13 এর শক্তিতে নিয়ে যাই।66, এটি প্রায় 13,000 সম্ভাবনার হওয়া উচিত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "I'm a little bit off here, but only because I'm not showing all the decimal places. ",
  "translatedText": "আমি এখানে একটু দূরে আছি, কিন্তু শুধুমাত্র কারণ আমি সব দশমিক স্থান দেখাচ্ছি না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute. ",
  "translatedText": "এই মুহুর্তে এটি অপ্রয়োজনীয় মনে হতে পারে এবং এটি অত্যধিক জটিল জিনিসগুলি পছন্দ করতে পারে, তবে আপনি দেখতে পাবেন যে কেন এক মিনিটে উভয় নম্বর থাকা দরকারী।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Ramen, which again just really doesn't feel like a word. ",
  "translatedText": "তাই এখানে দেখে মনে হচ্ছে এটি আমাদের দ্বিতীয় অনুমানের জন্য সর্বোচ্চ এনট্রপির পরামর্শ দিচ্ছে রামেন, যা আবার সত্যিই একটি শব্দের মতো মনে হয় না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here, I'm going to go ahead and type in Rains. ",
  "translatedText": "তাই এখানে নৈতিক উচ্চ স্থল নিতে, আমি এগিয়ে যান এবং বৃষ্টি টাইপ করতে যাচ্ছি. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky. ",
  "translatedText": "এবং আবার মনে হচ্ছে আমরা একটু দুর্ভাগা ছিলাম।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information. ",
  "translatedText": "আমরা 4 আশা করছিলাম।3 বিট এবং আমরা শুধুমাত্র 3 পেয়েছি।তথ্য 39 বিট. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities. ",
  "translatedText": "তাই এটি আমাদের 55 সম্ভাবনার নিচে নিয়ে যায়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means. ",
  "translatedText": "এবং এখানে হয়তো আমি আসলে যা যা পরামর্শ দিচ্ছে তা নিয়ে যাবো, যা কম্বো, যার মানে যাই হোক না কেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And okay, this is actually a good chance for a puzzle. ",
  "translatedText": "এবং ঠিক আছে, এই আসলে একটি ধাঁধা জন্য একটি ভাল সুযোগ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information. ",
  "translatedText": "এটা আমাদের বলছে এই প্যাটার্ন আমাদের দেয় 4. তথ্য 7 বিট. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty. ",
  "translatedText": "কিন্তু বাম দিকে, আমরা সেই প্যাটার্নটি দেখার আগে, 5টি ছিল।78 বিট অনিশ্চয়তা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities? ",
  "translatedText": "তাই আপনার জন্য একটি ক্যুইজ হিসাবে, বাকি সম্ভাবনার সংখ্যা সম্পর্কে এর মানে কি? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well, it means that we're reduced down to one bit of uncertainty, which is the same thing as saying that there's two possible answers. ",
  "translatedText": "ঠিক আছে, এর মানে হল যে আমরা এক বিট অনিশ্চয়তায় নেমে গেছি, যা বলার মত একই জিনিস যে দুটি সম্ভাব্য উত্তর আছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice. ",
  "translatedText": "এটি একটি 50-50 পছন্দ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss. ",
  "translatedText": "এবং এখান থেকে, কারণ আপনি এবং আমি জানি কোন শব্দগুলি বেশি সাধারণ, আমরা জানি যে উত্তরটি অতল হওয়া উচিত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that. ",
  "translatedText": "কিন্তু এখন যেমন লেখা হয়েছে, প্রোগ্রামটি তা জানে না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it. ",
  "translatedText": "সুতরাং এটি কেবল চলতেই থাকে, যতটা সম্ভব তথ্য অর্জনের চেষ্টা করে, যতক্ষণ না এটি কেবল একটি সম্ভাবনা বাকি থাকে এবং তারপরে এটি অনুমান করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy. ",
  "translatedText": "তাই স্পষ্টতই আমাদের আরও ভালো শেষ খেলার কৌশল দরকার।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1042.34
 },
 {
  "input": "But let's say we call this version one of our wordle solver, and then we go and run some simulations to see how it does. ",
  "translatedText": "কিন্তু ধরা যাক আমরা এই সংস্করণটিকে আমাদের ওয়ার্ডল সল্ভারের একটি বলি, এবং তারপরে আমরা যাই এবং কিছু সিমুলেশন চালাই তা দেখতে কেমন হয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1042.6,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game. ",
  "translatedText": "সুতরাং যেভাবে এটি কাজ করছে তা হল এটি প্রতিটি সম্ভাব্য শব্দ খেলা খেলছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers. ",
  "translatedText": "এটি সেই 2315টি শব্দের সমস্ত মাধ্যমে যাচ্ছে যা প্রকৃত wordle উত্তর।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set. ",
  "translatedText": "এটি মূলত একটি টেস্টিং সেট হিসাবে যে ব্যবহার করে. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice. ",
  "translatedText": "এবং একটি শব্দ কতটা সাধারণ তা বিবেচনা না করার এই নিরীহ পদ্ধতির সাথে, এবং প্রতিটি ধাপে তথ্য সর্বাধিক করার চেষ্টা করা, যতক্ষণ না এটি একটি এবং শুধুমাত্র একটি পছন্দে নেমে আসে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124. ",
  "translatedText": "সিমুলেশন শেষে, গড় স্কোর প্রায় 4 হবে।124।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expected to do worse. ",
  "translatedText": "যা খারাপ নয়, সত্যি কথা বলতে, আমি আরও খারাপ করার আশা করছিলাম।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4. ",
  "translatedText": "কিন্তু যারা wordle খেলে তারা আপনাকে বলবে যে তারা সাধারণত 4 এ পেতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can. ",
  "translatedText": "আসল চ্যালেঞ্জ হল আপনি যতটা পারেন 3-তে যতগুলি পেতে পারেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3. ",
  "translatedText": "এটি 4 স্কোর এবং 3 স্কোরের মধ্যে একটি চমত্কার বড় লাফ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that. ",
  "translatedText": "এখানে সুস্পষ্ট নিম্ন ঝুলন্ত ফল হল একরকম একটি শব্দ সাধারণ কিনা তা অন্তর্ভুক্ত করা, এবং আমরা কীভাবে তা করি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language. ",
  "translatedText": "আমি যেভাবে এটির সাথে যোগাযোগ করেছি তা হল ইংরেজি ভাষার সমস্ত শব্দের আপেক্ষিক ফ্রিকোয়েন্সিগুলির একটি তালিকা পাওয়া।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1107.88
 },
 {
  "input": "And I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset. ",
  "translatedText": "এবং আমি এইমাত্র ম্যাথমেটিকার শব্দ ফ্রিকোয়েন্সি ডেটা ফাংশন ব্যবহার করেছি, যা নিজেই Google Books English Ngram পাবলিক ডেটাসেট থেকে টেনে নেয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words. ",
  "translatedText": "এবং এটি দেখতে মজার ধরনের, উদাহরণস্বরূপ যদি আমরা এটিকে সবচেয়ে সাধারণ শব্দ থেকে সর্বনিম্ন সাধারণ শব্দগুলিতে সাজাই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5 letter words in the English language. ",
  "translatedText": "স্পষ্টতই এইগুলি ইংরেজি ভাষার সবচেয়ে সাধারণ, 5 অক্ষরের শব্দ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common. ",
  "translatedText": "বা বরং, এই 8 তম সবচেয়ে সাধারণ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there. ",
  "translatedText": "প্রথমটি যা, তারপরে সেখানে এবং সেখানে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common. ",
  "translatedText": "প্রথম নিজেই প্রথম নয়, কিন্তু 9তম, এবং এটি বোঝায় যে এই অন্যান্য শব্দগুলি প্রায়শই আসতে পারে, যেখানে প্রথমের পরে থাকাগুলি পরে, কোথায় এবং যেগুলি কিছুটা কম সাধারণ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency. ",
  "translatedText": "এখন, এই প্রতিটি শব্দের চূড়ান্ত উত্তর হওয়ার সম্ভাবনা কতটা মডেল করার জন্য এই ডেটা ব্যবহার করে, এটি কেবলমাত্র ফ্রিকোয়েন্সির সমানুপাতিক হওয়া উচিত নয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1146.34
 },
 {
  "input": "For example, which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely. ",
  "translatedText": "উদাহরণস্বরূপ, যা 0 এর স্কোর দেওয়া হয়।এই ডেটাসেটে 002, যেখানে braid শব্দটি কিছু অর্থে প্রায় 1000 গুণ কম।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.7,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering. ",
  "translatedText": "কিন্তু এই দুটিই যথেষ্ট সাধারণ শব্দ যে তারা প্রায় অবশ্যই বিবেচনার যোগ্য।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1158.84
 },
 {
  "input": "So we want more of a binary cutoff. ",
  "translatedText": "তাই আমরা একটি বাইনারি কাটঅফ আরো চাই. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1159.34,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty. ",
  "translatedText": "আমি যেভাবে এটি সম্পর্কে গিয়েছিলাম তা হল শব্দের এই পুরো সাজানো তালিকাটি নিয়ে কল্পনা করা, এবং তারপরে এটিকে একটি x-অক্ষে সাজানো, এবং তারপরে সিগমায়েড ফাংশন প্রয়োগ করা, যা একটি ফাংশন রাখার আদর্শ উপায় যার আউটপুট মূলত বাইনারি, এটি হয় 0 বা এটি 1, তবে অনিশ্চয়তার সেই অঞ্চলের মধ্যে একটি মসৃণতা রয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis. ",
  "translatedText": "তাই মূলত, চূড়ান্ত তালিকায় থাকার জন্য আমি প্রতিটি শব্দের জন্য যে সম্ভাব্যতা নির্ধারণ করছি সেটি হবে উপরের সিগমায়েড ফাংশনের মান যেখানেই এটি x-অক্ষে বসে থাকে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff. ",
  "translatedText": "এখন স্পষ্টতই এটি কয়েকটি পরামিতির উপর নির্ভর করে, উদাহরণস্বরূপ, x-অক্ষের উপর কতটা প্রশস্ত স্থান এই শব্দগুলি পূরণ করে তা নির্ধারণ করে যে আমরা 1 থেকে 0 থেকে কতটা ধীরে ধীরে বা খাড়াভাবে ড্রপ করব, এবং আমরা সেগুলিকে বাম থেকে ডানে কোথায় রাখব তা কাটঅফ নির্ধারণ করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "To be honest, the way I did this was just licking my finger and sticking it into the wind. ",
  "translatedText": "সত্যি কথা বলতে, আমি যেভাবে এটি করেছি তা কেবল আমার আঙুল চাটতে এবং বাতাসে আটকে রেখেছিল।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff. ",
  "translatedText": "আমি বাছাই করা তালিকার মধ্য দিয়ে দেখেছি এবং একটি উইন্ডো খুঁজে বের করার চেষ্টা করেছি যেখানে আমি এটির দিকে তাকিয়ে দেখেছিলাম যে এই শব্দগুলির প্রায় অর্ধেকই চূড়ান্ত উত্তর না হওয়ার সম্ভাবনা বেশি, এবং এটিকে কাটঅফ হিসাবে ব্যবহার করেছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement. ",
  "translatedText": "একবার আমরা শব্দ জুড়ে এই মত একটি বন্টন আছে, এটি আমাদের আরেকটি পরিস্থিতি দেয় যেখানে এনট্রপি এই সত্যিই দরকারী পরিমাপ হয়ে ওঠে. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were a feather and nails, and we end up with a situation where there's four possible words that match it. ",
  "translatedText": "উদাহরণস্বরূপ, ধরা যাক আমরা একটি খেলা খেলছিলাম এবং আমরা আমার পুরানো ওপেনারদের দিয়ে শুরু করি, যেটি ছিল একটি পালক এবং পেরেক, এবং আমরা এমন একটি পরিস্থিতির সাথে শেষ করি যেখানে চারটি সম্ভাব্য শব্দ আছে যা এর সাথে মেলে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely. ",
  "translatedText": "এবং আসুন আমরা তাদের সকলকে সমানভাবে সম্ভাব্য বিবেচনা করি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1235.62
 },
 {
  "input": "Let me ask you, what is the entropy of this distribution? ",
  "translatedText": "আমি আপনাকে জিজ্ঞাসা করি, এই বিতরণের এনট্রপি কি? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1236.22,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2. ",
  "translatedText": "ঠিক আছে, এই সম্ভাবনার প্রতিটির সাথে যুক্ত তথ্য 4 এর লগ বেস 2 হতে চলেছে, যেহেতু প্রতিটি 1 এবং 4, এবং এটি 2।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "Two bits of information, four possibilities. ",
  "translatedText": "তথ্যের দুই বিট, চারটি সম্ভাবনা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good. ",
  "translatedText": "সব খুব ভাল এবং ভাল. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than four matches? ",
  "translatedText": "কিন্তু আমি যদি আপনাকে বলি যে আসলে চারটির বেশি ম্যাচ আছে? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it. ",
  "translatedText": "বাস্তবে, যখন আমরা সম্পূর্ণ শব্দ তালিকাটি দেখি, সেখানে 16টি শব্দ আছে যা এর সাথে মেলে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure. ",
  "translatedText": "কিন্তু ধরুন আমাদের মডেলটি সেই অন্য 12টি শব্দের বাস্তবিকই চূড়ান্ত উত্তর হওয়ার সম্ভাবনা কম রাখে, 1000 এর মধ্যে 1 এর মত কিছু কারণ সেগুলি সত্যিই অস্পষ্ট।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution? ",
  "translatedText": "এখন আমি আপনাকে জিজ্ঞাসা করি, এই বিতরণের এনট্রপি কী? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before. ",
  "translatedText": "যদি এনট্রপি এখানে ম্যাচের সংখ্যা পরিমাপ করে, তাহলে আপনি আশা করতে পারেন যে এটি 16-এর লগ বেস 2-এর মতো কিছু হবে, যা 4 হবে, আমাদের আগের চেয়ে আরও দুই বিট অনিশ্চয়তা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before. ",
  "translatedText": "তবে অবশ্যই প্রকৃত অনিশ্চয়তা আমাদের আগে যা ছিল তার থেকে আসলেই আলাদা নয়।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example. ",
  "translatedText": "এই 12টি সত্যিই অস্পষ্ট শব্দের অর্থ এই নয় যে চূড়ান্ত উত্তরটি আকর্ষণীয়, উদাহরণস্বরূপ, এটি শিখলে এটি আরও আশ্চর্যজনক হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here, and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits. ",
  "translatedText": "সুতরাং আপনি যখন প্রকৃতপক্ষে এখানে গণনা করবেন, এবং আপনি প্রতিটি ঘটনার সময়ের সম্ভাব্যতা সংশ্লিষ্ট তথ্য যোগ করবেন, আপনি যা পাবেন তা হল 2।11 বিট।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "I'm just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it. ",
  "translatedText": "আমি শুধু বলছি, এটি মূলত দুটি বিট, মূলত সেই চারটি সম্ভাবনা, তবে সেই সবগুলি অত্যন্ত অসম্ভাব্য ঘটনার কারণে একটু বেশি অনিশ্চয়তা রয়েছে, যদিও আপনি যদি সেগুলি শিখতেন তবে আপনি এটি থেকে প্রচুর তথ্য পাবেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson. ",
  "translatedText": "তাই জুম আউট, এটি একটি তথ্য তত্ত্ব পাঠের জন্য Wordle যেমন একটি চমৎকার উদাহরণ করে তোলে কি অংশ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy. ",
  "translatedText": "আমরা এনট্রপি জন্য এই দুটি স্বতন্ত্র অনুভূতি অ্যাপ্লিকেশন আছে. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words that we have possible. ",
  "translatedText": "প্রথমটি আমাদের বলে যে প্রদত্ত অনুমান থেকে আমরা কী প্রত্যাশিত তথ্য পাব, এবং দ্বিতীয়টি বলছে যে আমরা সম্ভাব্য সমস্ত শব্দের মধ্যে অবশিষ্ট অনিশ্চয়তা পরিমাপ করতে পারি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation. ",
  "translatedText": "এবং আমার জোর দেওয়া উচিত, সেই প্রথম ক্ষেত্রে যেখানে আমরা একটি অনুমানের প্রত্যাশিত তথ্য দেখছি, একবার আমাদের শব্দগুলির সাথে অসম ওজন থাকলে, যা এনট্রপি গণনাকে প্রভাবিত করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words. ",
  "translatedText": "উদাহরণ স্বরূপ, আমাকে সেই একই কেসটা তুলে ধরতে দিন যা আমরা আগে Weary এর সাথে যুক্ত ডিস্ট্রিবিউশনের দিকে দেখছিলাম, কিন্তু এবার সব সম্ভাব্য শব্দ জুড়ে একটি অ-ইউনিফর্ম ডিস্ট্রিবিউশন ব্যবহার করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well. ",
  "translatedText": "তাই আমাকে দেখতে দিন যে আমি এখানে এমন একটি অংশ খুঁজে পেতে পারি যা এটিকে সুন্দরভাবে চিত্রিত করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here this is pretty good. ",
  "translatedText": "ঠিক আছে, এখানে এটি বেশ ভাল. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it. ",
  "translatedText": "এখানে আমাদের কাছে দুটি সংলগ্ন নিদর্শন রয়েছে যা প্রায় সমানভাবে সম্ভব, তবে তাদের মধ্যে একটিতে 32টি সম্ভাব্য শব্দ রয়েছে যা এর সাথে মেলে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them. ",
  "translatedText": "এবং যদি আমরা সেগুলি কী তা পরীক্ষা করে দেখি, এগুলি হল সেই 32টি, যেগুলি আপনি তাদের উপর আপনার চোখ স্ক্যান করার সময় খুব অসম্ভাব্য শব্দ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches, so a quarter as many matches, but it's about as likely. ",
  "translatedText": "যুক্তিযুক্ত উত্তরের মতো মনে হয় এমন কোনও খুঁজে পাওয়া কঠিন, হয়ত চিৎকার করে, কিন্তু আমরা যদি বন্টনের প্রতিবেশী প্যাটার্নটি দেখি, যা প্রায় সম্ভাব্য হিসাবে বিবেচিত হয়, আমাদের বলা হয় যে এটিতে শুধুমাত্র 8টি সম্ভাব্য ম্যাচ রয়েছে, তাই এক চতুর্থাংশ অনেক ম্যাচ, কিন্তু এটা প্রায় হিসাবে সম্ভাবনা. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why. ",
  "translatedText": "এবং যখন আমরা সেই ম্যাচগুলি টানব, আমরা দেখতে পাব কেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers, like ring, or wrath, or raps. ",
  "translatedText": "এর মধ্যে কিছু প্রকৃত যুক্তিযুক্ত উত্তর, যেমন রিং, বা ক্রোধ, বা র‍্যাপ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version 2 of the Wordlebot here, and there are two or three main differences from the first one that we saw. ",
  "translatedText": "আমরা কীভাবে সেগুলিকে একত্রিত করি তা ব্যাখ্যা করার জন্য, আমাকে এখানে Wordlebot-এর সংস্করণ 2 টানতে দিন, এবং প্রথমটির থেকে দুটি বা তিনটি প্রধান পার্থক্য রয়েছে যা আমরা দেখেছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer. ",
  "translatedText": "প্রথমত, আমি যেমন বলেছি, যেভাবে আমরা এই এনট্রপিগুলি গণনা করছি, তথ্যের এই প্রত্যাশিত মানগুলি, এখন নিদর্শনগুলি জুড়ে আরও পরিমার্জিত বিতরণ ব্যবহার করছে যা একটি প্রদত্ত শব্দটি আসলে উত্তর হওয়ার সম্ভাবনাকে অন্তর্ভুক্ত করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number 1, though the ones following are a bit different. ",
  "translatedText": "এটি যেমন ঘটছে, অশ্রু এখনও 1 নম্বর, যদিও অনুসরণ করাগুলি একটু ভিন্ন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table. ",
  "translatedText": "দ্বিতীয়ত, যখন এটি তার শীর্ষ বাছাইগুলিকে র‍্যাঙ্ক করে, তখন এটি এখন সম্ভাব্যতার একটি মডেল রাখবে যে প্রতিটি শব্দই প্রকৃত উত্তর, এবং এটি এটিকে তার সিদ্ধান্তে অন্তর্ভুক্ত করবে, যা একবার আমাদের কিছু অনুমান করার পরে দেখা সহজ হবে৷ টেবিল আবার, এর সুপারিশ উপেক্ষা করা কারণ আমরা মেশিনকে আমাদের জীবন শাসন করতে দিতে পারি না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives. ",
  "translatedText": "",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches. ",
  "translatedText": "এবং আমি মনে করি আমি অন্য একটি জিনিস উল্লেখ করা উচিত এখানে ভিন্ন বাম উপর, যে অনিশ্চয়তা মান, যে সংখ্যা বিট, আর শুধুমাত্র সম্ভাব্য ম্যাচ সংখ্যা সঙ্গে অপ্রয়োজনীয় হয় না. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which is a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes. ",
  "translatedText": "এখন যদি আমরা এটিকে টেনে 2 থেকে 8 গণনা করি।02, যা 256-এর একটু উপরে, আমার ধারণা 259, এটি যা বলছে তা হল যদিও 526টি মোট শব্দ আছে যা আসলে এই প্যাটার্নের সাথে মেলে, এতে যে পরিমাণ অনিশ্চয়তা রয়েছে তা যদি 259টি সমান সম্ভাবনা থাকে তবে এটি কী হতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this. ",
  "translatedText": "ফলাফল আপনি এই মত এটা চিন্তা করতে পারেন. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borx is not the answer, same with yorts and zorl and zorus, so it's a little less uncertain than it was in the previous case. ",
  "translatedText": "এটা জানে borx উত্তর নয়, yorts এবং zorl এবং zorus এর সাথে একই, তাই এটি আগের ক্ষেত্রে ছিল তার চেয়ে কিছুটা কম অনিশ্চিত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller. ",
  "translatedText": "বিট এই সংখ্যা ছোট হবে. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here. ",
  "translatedText": "এবং যদি আমি গেমটি খেলতে থাকি, আমি এখানে যা ব্যাখ্যা করতে চাই তার অনুমানে আমি কয়েকটি অনুমান দিয়ে এটিকে পরিমার্জন করছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy. ",
  "translatedText": "চতুর্থ অনুমান দ্বারা, আপনি যদি এটির শীর্ষ বাছাইগুলি দেখেন তবে আপনি দেখতে পাবেন যে এটি আর কেবল এনট্রপিকে সর্বাধিক করছে না।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words. ",
  "translatedText": "সুতরাং এই মুহুর্তে, প্রযুক্তিগতভাবে সাতটি সম্ভাবনা রয়েছে, তবে অর্থপূর্ণ সুযোগের সাথে একমাত্র ডর্ম এবং শব্দ।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values, that strictly speaking would give more information. ",
  "translatedText": "এবং আপনি দেখতে পাচ্ছেন যে এটি এই সমস্ত অন্যান্য মানগুলির ঊর্ধ্বে উভয়কেই বেছে নেওয়ার জন্য র‌্যাঙ্ক করে, যে কঠোরভাবে বলা আরও তথ্য দেবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect. ",
  "translatedText": "প্রথমবার যখন আমি এটি করেছি, আমি প্রতিটি অনুমানের গুণমান পরিমাপ করতে এই দুটি সংখ্যা যোগ করেছি, যা আসলে আপনার সন্দেহের চেয়ে ভাল কাজ করেছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic, and I'm sure there's other approaches people could take but here's the one I landed on. ",
  "translatedText": "কিন্তু এটা সত্যিই নিয়মতান্ত্রিক মনে হয়নি, এবং আমি নিশ্চিত যে অন্যান্য পন্থাও লোকেরা নিতে পারে কিন্তু এখানেই আমি অবতরণ করেছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that. ",
  "translatedText": "আমরা যদি পরবর্তী অনুমানের সম্ভাবনা বিবেচনা করি, যেমন এই ক্ষেত্রে কথায়, আমরা যদি তা করি তবে আমাদের গেমের প্রত্যাশিত স্কোরটিই আমরা সত্যিই যত্নশীল।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to. ",
  "translatedText": "এবং সেই প্রত্যাশিত স্কোর গণনা করার জন্য, আমরা বলি যে শব্দগুলি প্রকৃত উত্তর, যা এই মুহূর্তে এটি 58% থেকে বর্ণনা করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be 4. ",
  "translatedText": "আমরা 58% সুযোগ দিয়ে বলি, এই গেমে আমাদের স্কোর হবে 4।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of 1 minus that 58%, our score will be more than that 4. ",
  "translatedText": "এবং তারপর 1 বিয়োগের সম্ভাবনা সহ যে 58%, আমাদের স্কোর 4 এর চেয়ে বেশি হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point. ",
  "translatedText": "আমরা আরও কত কিছু জানি না, তবে আমরা সেই বিন্দুতে পৌঁছানোর পরে কতটা অনিশ্চয়তা হতে পারে তার উপর ভিত্তি করে আমরা এটি অনুমান করতে পারি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment there's 1.44 bits of uncertainty. ",
  "translatedText": "বিশেষ করে, এই মুহূর্তে 1 আছে।44 বিট অনিশ্চয়তা।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits. ",
  "translatedText": "যদি আমরা শব্দগুলি অনুমান করি তবে এটি আমাদেরকে বলছে যে আমরা প্রত্যাশিত তথ্য পাব তা হল 1।27 বিট।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens. ",
  "translatedText": "সুতরাং যদি আমরা শব্দগুলি অনুমান করি তবে এই পার্থক্যটি বোঝায় যে এটি হওয়ার পরে আমাদের কতটা অনিশ্চয়তা থাকতে পারে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score. ",
  "translatedText": "আমরা কি প্রয়োজন ফাংশন কিছু ধরনের, যা আমি এখানে f কল করছি, যে একটি প্রত্যাশিত স্কোর সঙ্গে এই অনিশ্চয়তা সংযুক্ত করে. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version 1 of the bot to say hey what was the actual score after various points with certain very measurable amounts of uncertainty. ",
  "translatedText": "এবং এটি যেভাবে চলেছিল তা হল বটটির সংস্করণ 1 এর উপর ভিত্তি করে পূর্ববর্তী গেমগুলির একগুচ্ছ ডেটা প্লট করা যাতে বলা হয় আরে বিভিন্ন পয়েন্টের পরে প্রকৃত স্কোর কী ছিল নির্দিষ্ট খুব পরিমাপযোগ্য অনিশ্চয়তার সাথে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer. ",
  "translatedText": "উদাহরণস্বরূপ, এখানে এই ডেটা পয়েন্টগুলি একটি মানের উপরে বসে আছে যা প্রায় 8 এর মতো।7 বা তাই বলে কিছু খেলার জন্য একটি পয়েন্টের পরে যেখানে 8 ছিল।7 বিট অনিশ্চয়তা, চূড়ান্ত উত্তর পেতে দুটি অনুমান লেগেছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games it took three guesses, for other games it took four guesses. ",
  "translatedText": "অন্যান্য গেমের জন্য এটি তিনটি অনুমান নিয়েছে, অন্যান্য গেমগুলির জন্য এটি চারটি অনুমান নিয়েছে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring. ",
  "translatedText": "যদি আমরা এখানে বাম দিকে সরে যাই, শূন্যের উপরে সমস্ত বিন্দু বলছে যখনই অনিশ্চয়তার শূন্য বিট থাকে, যার অর্থ হল শুধুমাত্র একটি সম্ভাবনা আছে, তাহলে প্রয়োজনীয় অনুমানের সংখ্যা সর্বদা শুধুমাত্র একটি, যা আশ্বস্ত করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses. ",
  "translatedText": "যখনই এক বিট অনিশ্চয়তা ছিল, যার অর্থ এটি মূলত দুটি সম্ভাবনার মধ্যে ছিল, তখন কখনও কখনও এটি আরও একটি অনুমান প্রয়োজন, কখনও কখনও এটি আরও দুটি অনুমান প্রয়োজন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1623.02
 },
 {
  "input": "And so on and so forth here. ",
  "translatedText": "এবং তাই এবং তাই ঘোষণা এখানে. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1623.08,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages. ",
  "translatedText": "এই ডেটাটি কল্পনা করার একটি সামান্য সহজ উপায় হল এটিকে একসাথে বাকেট করা এবং গড় নেওয়া।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example this bar here saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5. ",
  "translatedText": "উদাহরণস্বরূপ এই বারটি এখানে বলা হয়েছে যে সমস্ত পয়েন্টগুলির মধ্যে যেখানে আমাদের এক বিট অনিশ্চয়তা ছিল, গড়ে নতুন অনুমানের সংখ্যা ছিল প্রায় 1।5. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward. ",
  "translatedText": "এবং এখানে বারটি বিভিন্ন গেমগুলির মধ্যে বলা হচ্ছে যেখানে কিছু সময়ে অনিশ্চয়তা চার বিটের একটু উপরে ছিল, যা এটিকে 16টি বিভিন্ন সম্ভাবনায় সংকুচিত করার মতো, তারপরে গড়ে এটির জন্য সেই বিন্দু থেকে দুটি অনুমানের চেয়ে একটু বেশি প্রয়োজন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this. ",
  "translatedText": "এগিয়ে এবং এখান থেকে আমি শুধু একটি রিগ্রেশন করেছি একটি ফাংশন মাপসই যে এই যুক্তিসঙ্গত বলে মনে হচ্ছে. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be. ",
  "translatedText": "এবং মনে রাখবেন যে কোনটি করার পুরো পয়েন্টটি হল যাতে আমরা এই অন্তর্দৃষ্টিকে পরিমাপ করতে পারি যে আমরা একটি শব্দ থেকে যত বেশি তথ্য লাভ করব, প্রত্যাশিত স্কোর তত কম হবে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So with this as version 2.0, if we go back and we run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do? ",
  "translatedText": "সুতরাং এটি 2 সংস্করণ হিসাবে।0, যদি আমরা ফিরে যাই এবং আমরা একই সেট সিমুলেশন চালাই, এটি সমস্ত 2315 সম্ভাব্য wordle উত্তরগুলির বিরুদ্ধে খেলতে পারে, এটি কীভাবে করবে? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version it's definitely better, which is reassuring. ",
  "translatedText": "আমাদের প্রথম সংস্করণের বিপরীতে এটি অবশ্যই ভাল, যা আশ্বস্ত।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done the average is around 3.6, although unlike the first version there are a couple times that it loses and requires more than six in this circumstance. ",
  "translatedText": "সব বলা এবং করা গড় প্রায় 3.6, যদিও প্রথম সংস্করণের বিপরীতে এটি কয়েকবার হারায় এবং এই পরিস্থিতিতে ছয়টির বেশি প্রয়োজন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information. ",
  "translatedText": "সম্ভবত কারণ এমন অনেক সময় আছে যখন এটি সেই ট্রেডঅফকে প্রকৃতপক্ষে তথ্যকে সর্বাধিক করার পরিবর্তে লক্ষ্যে যাওয়ার জন্য তৈরি করে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6? ",
  "translatedText": "তাই আমরা 3 এর চেয়ে ভাল করতে পারি।6? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can. ",
  "translatedText": "আমরা অবশ্যই পারি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model. ",
  "translatedText": "এখন আমি শুরুতে বলেছিলাম যে এটির মডেল তৈরি করার উপায়ে wordle উত্তরগুলির সত্যিকারের তালিকাকে অন্তর্ভুক্ত না করার চেষ্টা করা সবচেয়ে মজার।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43. ",
  "translatedText": "কিন্তু যদি আমরা এটিকে অন্তর্ভুক্ত করি, তাহলে আমার সেরা পারফরম্যান্সটি ছিল প্রায় 3টি।43. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that. ",
  "translatedText": "সুতরাং যদি আমরা এই পূর্ববর্তী বিতরণ নির্বাচন করার জন্য শুধুমাত্র শব্দ ফ্রিকোয়েন্সি ডেটা ব্যবহার করার চেয়ে আরও পরিশীলিত হওয়ার চেষ্টা করি, এই 3.43 সম্ভবত একটি সর্বোচ্চ দেয় যে আমরা এটির সাথে কতটা ভাল পেতে পারি, বা কমপক্ষে আমি এটির সাথে কতটা ভাল পেতে পারি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one. ",
  "translatedText": "সেই সেরা পারফরম্যান্সটি মূলত কেবল সেই ধারণাগুলি ব্যবহার করে যা আমি এখানে বলেছি, তবে এটি একটু দূরে চলে যায়, যেমন এটি প্রত্যাশিত তথ্যের জন্য অনুসন্ধান করে কেবল একের পরিবর্তে দুই ধাপ এগিয়ে।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is. ",
  "translatedText": "মূলত আমি এটি সম্পর্কে আরও কথা বলার পরিকল্পনা করছিলাম, তবে আমি বুঝতে পারি যে আমরা আসলে এটির মতো অনেক দীর্ঘ হয়েছি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least it's looking like Crane is the best opener. ",
  "translatedText": "আমি যা বলব তা হল এই দ্বি-পদক্ষেপ অনুসন্ধান করার পরে এবং তারপরে শীর্ষ প্রার্থীদের মধ্যে কয়েকটি নমুনা সিমুলেশন চালানোর পরে, এখন পর্যন্ত আমার জন্য অন্তত এটি মনে হচ্ছে ক্রেন সেরা ওপেনার।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed? ",
  "translatedText": "কে অনুমান করতেন? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits. ",
  "translatedText": "এছাড়াও আপনি যদি আপনার সম্ভাবনার স্থান নির্ধারণ করতে সত্যিকারের শব্দ তালিকা ব্যবহার করেন, তাহলে আপনি যে অনিশ্চয়তা দিয়ে শুরু করছেন তা হল 11 বিটের কিছু বেশি।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out, just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits. ",
  "translatedText": "এবং এটি দেখা যাচ্ছে, শুধুমাত্র একটি নৃশংস শক্তি অনুসন্ধান থেকে, প্রথম দুটি অনুমানের পরে সর্বাধিক সম্ভাব্য প্রত্যাশিত তথ্য প্রায় 10 বিট।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty. ",
  "translatedText": "যেটি প্রস্তাব করে যে সেরা কেস দৃশ্যকল্প, আপনার প্রথম দুটি অনুমানের পরে, পুরোপুরি সর্বোত্তম খেলা সহ, আপনি প্রায় এক বিট অনিশ্চয়তার সাথে থাকবেন।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses. ",
  "translatedText": "যা দুটি সম্ভাব্য অনুমান করার মতই।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "So I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as 3, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail. ",
  "translatedText": "তাই আমি মনে করি এটা বলা ন্যায্য এবং সম্ভবত বেশ রক্ষণশীল যে আপনি কখনই এমন একটি অ্যালগরিদম লিখতে পারবেন না যা এই গড় 3-এর মতো কম হয়, কারণ আপনার কাছে উপলব্ধ শব্দগুলির সাথে, শুধুমাত্র দুটি পদক্ষেপের পর পর্যাপ্ত তথ্য পাওয়ার জায়গা নেই।ব্যর্থ না হয়ে প্রতি একক সময় তৃতীয় স্লটে উত্তরের গ্যারান্টি দিতে সক্ষম।",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]