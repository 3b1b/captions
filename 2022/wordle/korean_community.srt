1
00:00:00,000 --> 00:00:04,080
Wordle 게임은 지난 한두 달 사이에 입소문이 났고 

2
00:00:04,080 --> 00:00:08,320
수학 수업의 기회를 놓칠 수 없는 게임입니다.

3
00:00:08,320 --> 00:00:12,720
이 게임은 정보 이론, 특히 엔트로피라는 주제에 대한 수업에서
매우 좋은 중심적인 예시를 만든다는 생각이 듭니다.

4
00:00:13,760 --> 00:00:17,760
많은 사람들이 이 퍼즐에 빠져들었다는 것을 알 수 있습니다.

5
00:00:17,760 --> 00:00:23,040
그리고 많은 프로그래머들처럼 저도 최대한 최상으로
게임을 할 수 있는 알고리즘을 만들려고 노력했습니다.

6
00:00:23,040 --> 00:00:26,720
제가 이번에 할 것은 여러분에게 제 과정 중 일부를 이야기하고,
거기에 관련된 수학에 대해 설명하는 것입니다.

7
00:00:26,720 --> 00:00:31,040
왜냐하면 이 전체 알고리즘이 엔트로피에 대한
아이디어를 중심으로 하기 때문입니다.

8
00:00:38,720 --> 00:00:43,120
먼저, 여러분이 아직 들어보지 못하셨을지 모르겠지만,
Wordle 이 무엇인지

9
00:00:43,120 --> 00:00:46,960
그리고 게임의 규칙을 살펴보는 동안 일석이조로,
또한 이 게임의 진행 방향을 알려드리겠습니다.

10
00:00:46,960 --> 00:00:50,800
그니까, 기본적으로 게임을 플레이할 수 있는
작은 알고리즘을 개발하는 것입니다.

11
00:00:51,440 --> 00:00:55,360
전 "지금" 2월 4일인데, Wordle을 아직 안 해 봤고,
봇이 어떻게 돌아가는지 봅시다.

12
00:00:55,360 --> 00:00:59,200
Wordle의 목표는 다섯 글자로 된 미스터리 단어를 추측하는 것이고,
당신은 여섯 번의 추측 기회가 주어집니다.

13
00:00:59,200 --> 00:01:03,920
예시로, 저의 wordlebot은 저한테
"crane"으로 추측을 시작하라고 제안합니다.

14
00:01:03,920 --> 00:01:09,120
여러분이 추측할 때마다, 여러분은 추측이
얼마나 답에 가까운지에 대한 정보를 얻게 됩니다.

15
00:01:09,120 --> 00:01:14,240
여기 회색 박스는 실제 답에 'c'가 없다고 알려 주고

16
00:01:14,240 --> 00:01:18,080
노란색 박스는 'r'이 있다고 표시 되는데
그 위치가 아니라고 알려줍니다.

17
00:01:18,080 --> 00:01:22,560
녹색 박스는 단어에 'a'가 있고
세 번째 위치에 있다고 알려줍니다.

18
00:01:22,560 --> 00:01:26,800
그리고 여기에 'n'도 없고 'e'도 없습니다.
그리고 wordlebot에게 이 정보를 전하겠습니다.

19
00:01:26,800 --> 00:01:30,240
"crane"이 회색 노란색 초록 회색 회색으로 됐죠.

20
00:01:31,200 --> 00:01:34,720
지금 보여지는 데이터는 신경쓰지 마세요,
때가 되면 설명해 드릴게요.

21
00:01:35,440 --> 00:01:41,680
두 번째 제안은 "shtik" 입니다.
여러분의 추측은 실제 5글자여야 합니다.

22
00:01:41,680 --> 00:01:45,280
하지만 보시다시피 여러분이 추측할 수 있는 단어들은
꽤 자유분방 합니다.

23
00:01:46,000 --> 00:01:50,560
이 경우엔 "shtik"를 써보죠. 그리고... 좋아요!
상황이 꽤 좋아 보입니다.

24
00:01:50,560 --> 00:01:53,840
우리는 's'와 'h'를 맞혔습니다.
이와 같이 우리는 3번째 글자까지 알게 됐고, 'r' 도 있다는 것을 압니다.

25
00:01:54,480 --> 00:02:00,160
그러면 이 단어는 's-h-a-무언가-r'
또는 's-h-a-r-무언가' 가 됩니다.

26
00:02:00,160 --> 00:02:04,160
Wordle-bot은 두 가지 가능성,
즉 "shard"와 "sharp"를 알고 있는 것 같습니다.

27
00:02:04,800 --> 00:02:08,560
이 경우엔 반반이죠.
알파벳 순으로 정렬하니까'shard'로 해보고, 

28
00:02:08,560 --> 00:02:13,680
그리고... 와!
진짜 답입니다. 우린 3번만에 해냈습니다.

29
00:02:14,400 --> 00:02:18,160
이게 과연 좋은 것인지 궁금하다면,
제가 어떤 사람 말을 들어보면 

30
00:02:18,160 --> 00:02:22,720
Wordle에서는 4는 파, 3은 버디라는 표현하는데
꽤 적절한 비유라고 생각합니다.  (골프 용어: 파, 버디)

31
00:02:22,720 --> 00:02:27,280
꾸준히 게임을 해야 4점을 받을 수 있지만
그렇게 어려운 것은 아닙니다.

32
00:02:27,280 --> 00:02:29,760
하지만 3번만에 성공하면, 그냥 기분이 좋아요.

33
00:02:30,720 --> 00:02:34,000
여러분이 괜찮으시다면 제가 여기서 하고 싶은 것은
제가 wordlebot에 어떻게 접근하는지에 대한 

34
00:02:34,000 --> 00:02:38,160
제 생각의 과정을 처음부터 쭉 살펴보는 것입니다.
그리고 제가 정말 정보이론 수업을 위한 것이라고 변명했듯이,

35
00:02:38,160 --> 00:02:42,720
주요 목표는 정보란 무엇이고
엔트로피는 무엇인지를 설명하는 것입니다.
36
00:02:48,320 --> 00:02:52,240
이것을 만들면서 제가 처음 생각한 것은
알파벳의 상대적인 빈도를 살펴보는 것이었습니다.

36
00:02:52,240 --> 00:02:56,480
저는 가장 빈번한 알파벳들이 쓰이는, 

37
00:02:56,480 --> 00:03:00,880
첫 번째의 추측 단어나 그 추측의 쌍이 있는지 생각했습니다.

38
00:03:00,880 --> 00:03:05,440
제가 좀 끌리는 것은 "other" 다음에 "nails"을 하는 것 입니다.
글자를 치면 녹색이나 노란색이 나오는데

39
00:03:05,440 --> 00:03:09,200
이것이 정보를 얻는거 같아서, 기분이 언제나 좋습니다.

40
00:03:09,200 --> 00:03:12,320
하지만 이런 경우엔, 다 못 맞추고 항상 회색을 얻는다 해도
여전히 많은 정보를 얻을 수 있습니다.

41
00:03:12,320 --> 00:03:15,840
왜냐하면 이런 글자들이 없는 단어를 찾는 것은
매우 드물기 때문입니다.

42
00:03:15,840 --> 00:03:20,320
하지만 여전히 뭔가 체계적인 느낌이 들지 않습니다.

43
00:03:20,320 --> 00:03:24,240
예를 들어, 글자의 순서를 고려하는 것은 쓸모 없기 때문입니다.
왜 제가 "snail"을 쓸 수 있었는데 왜 "nails"를 할까요?

44
00:03:24,240 --> 00:03:28,320
끝에 's'가 있는 것이 더 나을까요?
솔직히 저도 잘 몰라요.

45
00:03:28,880 --> 00:03:32,320
제 친구가 말하길 "weary"라는
단어로 시작하는 걸 좋아한다고 하더군요.

46
00:03:32,320 --> 00:03:35,840
저는 그 안에 'w'나 'y'같은
흔치 않은 글자가 들어있어서 좀 놀랐습니다.

47
00:03:35,840 --> 00:03:39,840
하지만 누가 알겠어요, 어쩌면 그게 더 나은 추측일 수도 있습니다.

48
00:03:39,840 --> 00:03:44,160
잠재적인 추측의 양질을 판단하기 위해,
우리가 줄 수 있는 양적 점수가 있을까요?

49
00:03:45,200 --> 00:03:49,040
가능한 추측의 순위를 매기는 방식을 설정하려면,
뒤로 돌아가서 게임이 정확히 어떻게 설계되었는지에 대해 

50
00:03:49,040 --> 00:03:54,000
조금 더 명확히 설명하겠습니다. 
여기에 여러분이 입력이 가능한
52
00:03:54,000 --> 00:03:58,640
약 13,000개의 단어 목록이 있는데,
모두 유효한 추측으로 여겨집니다.

51
00:03:58,640 --> 00:04:03,440
하지만 이것을 보면 "aahed"나 "aalii" 그리고 
"aargh"과 같은 정말 흔치 않은 것들이 많이 있습니다.

52
00:04:03,440 --> 00:04:06,720
이런 종류의 단어들은 Scrabble(보드게임)에서
가족끼리 말다툼을 불러오는 단어입니다.

53
00:04:06,720 --> 00:04:10,800
하지만 게임의 분위기로 봐서는 정답은
항상 꽤 흔한 단어일 것이고, 

54
00:04:10,800 --> 00:04:16,480
실제로 답이 될 수 있는 2300개의 단어 목록이 있습니다.

55
00:04:16,480 --> 00:04:21,520
이것은 인간이 만든 목록들인데, 특히 게임 제작자의
여자친구가 재미있다고 생각합니다. (어어... 여친이 있어?)

56
00:04:21,520 --> 00:04:25,360
하지만 제가 하고 싶은 것은, 이 과제에 대한 우리의 도전은,

57
00:04:25,360 --> 00:04:30,480
이 목록에 대한 지식을 포함하지 않고 Wordle을
해결하는 프로그램을 만들 수 있는지 알아보는 것입니다.

58
00:04:30,480 --> 00:04:34,640
한 가지 예로, 여러분이 그 목록에서 찾을 수 없는
꽤 흔한 다섯 글자의 단어들이 많이 있습니다.

59
00:04:34,640 --> 00:04:38,320
그래서 단지 공식 웹사이트가 아닌, 조금 더 탄력적이고 

60
00:04:38,320 --> 00:04:42,880
누구와도 wordle을 플레이 할 수 있는
프로그램을 작성하는 것이 더 나을 것입니다.

61
00:04:42,880 --> 00:04:47,200
그리고 우리가 이 가능한 답들의 목록을 알고 있는 이유는,
소스 코드에서 볼 수 있기 때문입니다.

62
00:04:47,200 --> 00:04:51,840
그래서 소스 코드를 보면, 매일매일 답이 특정한 순서로 되어 있기 때문에

63
00:04:51,840 --> 00:04:56,800
내일의 답이 무엇일지 항상 알 수 있습니다.

64
00:04:56,800 --> 00:05:00,720
그러지만 분명히 목록을 사용하는 것은 부정행위이고,
더 흥미로운 퍼즐과 더 풍부한 정보이론의 수업을

65
00:05:00,720 --> 00:05:05,760
만들기 위해 더욱 보편적인 데이터를 사용하는 것입니다.

66
00:05:05,760 --> 00:05:10,320
예를 들어, 상대적인 단어 빈도와 같은,
보다 일반적인 단어들을 선호하는 이 직관을 갖기 위해서 말이죠.

67
00:05:11,440 --> 00:05:16,240
그래서! 이 13,000개의 가능성 중에서,
첫 추측을 어떻게 선택해야 할까요?

68
00:05:16,240 --> 00:05:20,720
예를 들어, 만약 제 친구가 "weary"을 제안한다면,
그것의 양질을 어떻게 분석해야 할까요?

69
00:05:20,720 --> 00:05:25,760
글쎄, 그가 그렇게까지 좋아 할 거 같지 않은 'w'를
좋아한다고 말한 이유는, 

70
00:05:25,760 --> 00:05:30,640
그 승산이 거의 없는 'w'를 맞추면 
기분이 아주 좋기 때문입니다. 

71
00:05:30,640 --> 00:05:35,840
예를 들어, 만약 처음 패턴이 이렇게 생기면,
이 거대한 단어 목록에서 그 패턴과 일치하는 단어는 58개만 있습니다. 

72
00:05:35,840 --> 00:05:41,120
이것은 13,000개에서 엄청나게 줄어든 것입니다. 
하지만 물론 이런 패턴이 나타나는 것은 

73
00:05:41,120 --> 00:05:46,480
매우 드문 일입니다. 
구체적으로, 각 단어가 답이 될 확률이 같다면 

74
00:05:46,480 --> 00:05:50,400
이 패턴을 맞출 확률은 58을 13,000 정도로
나눈 것입니다.

75
00:05:51,440 --> 00:05:55,040
물론, 이것들이 똑같이 답이 되지는 않고,
대부분 매우 불명확하고,

76
00:05:55,040 --> 00:05:58,880
심지어 의문스러운 단어들이지만,
적어도 첫 번째 통과에 대해서는

77
00:05:58,880 --> 00:06:03,120
모두 동등하다고 가정하고,
조금 후에 다시 다듬어 봅시다.

78
00:06:03,120 --> 00:06:09,600
요점은 정보가 많은 패턴은 본질적으로 발생할 가능성이
낮다는 것입니다.

79
00:06:09,600 --> 00:06:14,480
사실 "유용한 정보"가 의미하는 바는 "그러지 않을 경우" 입니다. 
이런 시작점에서 볼 수 있는 훨씬 더 가능성 있는 패턴은 이런 것입니다.

80
00:06:14,480 --> 00:06:19,040
물론 그 안에 'w'가 없고, 'e'가 없을 수도 있고, 

81
00:06:19,040 --> 00:06:23,600
'a'가 없을 수도 있고, 'r'가 없을 수도 있고, 'y'가 없을 수도 있습니다.

82
00:06:23,600 --> 00:06:29,200
이 경우 1,400개의 가능한 일치하는 항목이 있습니다.
만약 모든 것이 동등하다면 11%의 확률로 여러분이 볼 수 있는 패턴입니다.

83
00:06:29,200 --> 00:06:34,000
그래서 가장 가능성이 높은 결과이더라도 가장 정보가 적습니다.

84
00:06:34,000 --> 00:06:38,320
좀 더 포괄적인 시야를 얻기 위해 다양한 모든 패턴의

85
00:06:38,320 --> 00:06:40,560
확률 분포를 보여드리겠습니다.

86
00:06:42,080 --> 00:06:46,880
여러분이 보고 있는 각각의 막대는 나타날 수 있는 
색깔의 패턴들과 일치하는데, 3⁵개의 가능성이 있습니다.

87
00:06:46,880 --> 00:06:50,960
그리고 그것들은 왼쪽에서 오른쪽으로, 
즉 가장 흔한 것에서 가장 흔하지 않는 것으로 구성되어 있습니다.

88
00:06:50,960 --> 00:06:55,920
여기서, 모두 회색을 얻는 가장 일반적인 가능성은
14% 정도 발생합니다.

89
00:06:55,920 --> 00:07:01,120
여러분이 희망하는 것은 이 긴 꼬리 어딘가에 있다는 것입니다. 

90
00:07:01,120 --> 00:07:05,680
이 패턴과 일치하는 것은 18개의 가능성 밖에 없는, 

91
00:07:05,680 --> 00:07:10,960
이런 단어들이 있습니다.
아니면 조금 더 오른쪽으로 간다면... 

92
00:07:10,960 --> 00:07:16,320
여기까지 가본다면... 좋아요, 
여기 여러분한테 좋은 퍼즐이 있습니다.

93
00:07:16,320 --> 00:07:20,800
영어에서 'w'로 시작하고 'y'로 끝나며
어딘가에 'r'가 있는 세 단어는 무엇일까요?

94
00:07:20,800 --> 00:07:26,640
답을 드러내보고... 한 번 보도록 하죠...
'wordy', 'wormy', 'wrily' 이네요.

95
00:07:27,440 --> 00:07:33,200
이 단어가 전체적으로 얼마나 좋은지 판단하기 위해서,
우리는 이 분포에서 얻게 될 정보의 양에 대한 

96
00:07:33,200 --> 00:07:37,520
일종의 측정값이 필요합니다. 
만약 우리가 

97
00:07:37,520 --> 00:07:42,720
각각의 패턴을 살펴보고 이 패턴이
얼마나 유용한지 측정하는 발생 확률과 "무언가"를 곱한다면,

98
00:07:42,720 --> 00:07:47,520
그 값은 우리에게 객관적인 점수로 될 것입니다. 
이제, 어떤 것이 일치해야 하는지에 대한 첫 번째 직관은

99
00:07:47,520 --> 00:07:52,720
여러분은 일치 횟수의 평균값을 낮추기 이겠지만, 

100
00:07:52,720 --> 00:07:57,120
저는 그 대신 정보에 결과를 두는,
더욱 보편적인 측정을 사용하고 싶습니다.

101
00:07:57,120 --> 00:08:00,400
이 13,000개의 단어 각각에 실제로 답인지 아닌지에 대해

102
00:08:00,400 --> 00:08:04,160
각각 다른 확률이 할당되면 더 웅통성이 있을 것입니다.

103
00:08:10,320 --> 00:08:14,560
정보의 표준 단위는 비트인데, 조금은 재미있는 

104
00:08:14,560 --> 00:08:16,880
공식을 가지고 있고, 예시를 보면 정말 직관적입니다.

105
00:08:17,600 --> 00:08:21,360
만약 여러분이 가능성의 공간을 반으로 줄인다면,

106
00:08:21,360 --> 00:08:25,680
우리는 이것을 하나의 정보가 있다고 할 수 있습니다.
이 예시에서 가능성의 공간은 모두 가능한 단어이며, 

107
00:08:25,680 --> 00:08:29,600
다섯 글자의 절반 가량이 's'로 되어 있는데,
사실 절반보다 조금 적지만 절반 정도입니다.

108
00:08:29,600 --> 00:08:34,240
이 관측은 여러분에게 1bit의 정보를 줄 것입니다.

109
00:08:34,880 --> 00:08:39,840
만약 새로운 사실이 그 공간을 4개로 나눈다면, 
우리는 그것을 2 bits의 정보를 가지고 있다고

110
00:08:39,840 --> 00:08:44,320
생각할 수 있습니다. 
예를 들어, 이 단어들의 약 4분의 1은 't'를 가지고 있습니다.

111
00:08:44,960 --> 00:08:49,360
만약 관측이 이 공간을 8배로 줄인다면, 
우리는 그 공간에 3bits의 정보가 있다고 말할 수 있고, 

112
00:08:49,360 --> 00:08:53,840
그리고 계속 이어집니다. 
4bits는 16으로 나누고 5bits는 32로 나눕니다.

113
00:08:54,720 --> 00:08:58,960
자, 지금은 잠시 스스로에게 물어보고 싶을 수 있습니다. 

114
00:08:58,960 --> 00:09:02,960
비트 수에 관련된 발생 확률과 정보의 개수 공식은 무엇일까요?

115
00:09:03,920 --> 00:09:07,440
여기서 말하는 것은 기본적으로 비트 수의 절반을 취할 때, 

116
00:09:07,440 --> 00:09:11,600
그것은 확률과 같은 것입니다. 

117
00:09:11,600 --> 00:09:15,520
즉, 2의 비트 개수 거듭제곱은 (1/가능성)과 같습니다.

118
00:09:15,520 --> 00:09:20,720
이는 더 나아가 정보는 log₂ (1/가능성)과 같습니다.
그리고 여러분은 여전히 이것을 

119
00:09:20,720 --> 00:09:24,800
다른 형태로 바꿀 수 있다는 것을 알 수 있습니다.
여기서 정보는 -log₂(확률) 입니다.

120
00:09:25,360 --> 00:09:29,200
이렇게 표현하면 처음 보는 사람들에게는
조금 이상해 보일 수 있지만, 

121
00:09:29,200 --> 00:09:33,920
이것은 그냥 얼마나 많은 확률들을 반으로 줄였는지를 
알 수 있는 매우 직관적인 발상일 뿐입니다.

122
00:09:34,880 --> 00:09:37,920
이제 여러분은 궁금할 수도 있습니다, 
우린 그냥 재미있는 단어 게임을 하는 줄 알았는데, 

123
00:09:37,920 --> 00:09:42,240
왜 로그가 이 그림에 들어올까요? 
로그가 좋은 단위인 한 가지 이유는 

124
00:09:42,240 --> 00:09:47,120
매우 있을 법하지 않은 사건에 대해 이야기하는 것이
훨씬 쉽기 때문입니다. 예시로, 어떤 관측이 20비트의 정보를 

125
00:09:47,120 --> 00:09:53,120
가지고 있다고 말하는 것은 이런 일이 일어날 확률이 
0.000095라고 말하는 것보다 훨씬 쉽습니다.

126
00:09:53,120 --> 00:09:56,640
하지만 이 로그 식이 확률론에 매우 유용한 표현이라고 

127
00:09:56,640 --> 00:10:01,920
알려진 것 보다 더욱 타당한 이유는 
로그의 덧셈 법칙 때문입니다.

128
00:10:01,920 --> 00:10:06,480
예를 들어, 한 관측이 2bits의 정보를 제공하고
즉, 공간을 네 개 나누고, 

129
00:10:06,480 --> 00:10:10,880
그 다음 두 번째 관측이 Wordle에서의 두 번째 추측처럼,
다른 3bits의 정보를 제공하면,

130
00:10:10,880 --> 00:10:15,280
그러니까 그 구역을 8개로 한번 더 나누면,
두 관측은 총 다섯 개의 정보를 제공합니다.

131
00:10:15,280 --> 00:10:19,520
확률끼리 서로 곱한 것은 정보끼리 더한 것과 같습니다.

132
00:10:19,520 --> 00:10:24,320
그래서 우리가 어떤 기대값과 같은 영역에서 

133
00:10:24,320 --> 00:10:27,760
우리가 더 많은 숫자들을 더할 때,
로그들이 훨씬 보기 좋게 만듭니다.

134
00:10:28,320 --> 00:10:32,240
다시 'weary'에 대한 분포표로 돌아가서 각 패턴에 대해 

135
00:10:32,240 --> 00:10:34,800
얼마나 많은 정보가 있는지 보여주는
작은 추적기를 여기에 하나 더 추가해 보겠습니다.

136
00:10:35,360 --> 00:10:38,080
여러분께서 주목해 주셨으면 하는 것은,
확률이 높은 패턴에 도달할수록, 
139
00:10:38,080 --> 00:10:42,640
더 낮은 정보를 얻을 수 있고,
더 적은 bits를 얻을 수 있습니다.

137
00:10:43,280 --> 00:10:48,320
우리가 이 추측의 양질을 측정하는 방법은
이 정보의 기대값을 구하는 것입니다.

138
00:10:48,320 --> 00:10:52,080
우리가 각 패턴이 있는 곳에서 얼마나 가능성 있는지 고려하고,
그 가능성에서 그 정보를 얼마나 많이 얻을 수 있는지를

139
00:10:52,080 --> 00:10:58,240
곱하는 것입니다. 
그리고 "weary"의 예시에서는 4.9bits로 나타났습니다.

140
00:10:58,240 --> 00:11:02,400
그래서 평균적으로, 이 시작점을 통해 얻은 정보는

141
00:11:02,400 --> 00:11:06,480
가능성의 공간을 5번 반으로 줄이는 것과 같습니다.

142
00:11:06,480 --> 00:11:11,600
반대로 정보의 기대값이 더 높은 추측의 예시로는
"slate"가 있습니다.

143
00:11:12,960 --> 00:11:17,040
이 경우 분포표가 훨씬 평평하게 보이며, 

144
00:11:17,040 --> 00:11:22,480
특히 모든 부분이 회색일 확률은 약 6% 정도입니다.

145
00:11:22,480 --> 00:11:27,280
그래서 여러분은 최소 3.9bits의 정보를 얻을 수 있습니다.
하지만 그건 최소고, 일반적으로 

146
00:11:27,280 --> 00:11:31,280
여러분은 그것보다 더 나은 것을 얻을 수 있습니다.
그리고 이 수치값을 계산해서 

147
00:11:31,280 --> 00:11:38,240
모든 구간들을 더하면 평균 정보는 약 5.8개 입니다.

148
00:11:38,240 --> 00:11:43,440
따라서 "weary"와는 대조적으로, 확률의 공간은
이 첫 번째 추측 이후에 평균적으로 약 절반 정도 클 것 입니다.

149
00:11:44,240 --> 00:11:48,640
이 정보량의 기댓값에 대한 이름에
재미있는 이야기가 있습니다.

150
00:11:48,640 --> 00:11:52,320
정보 이론은 1940년대에 Bell(전화기 만든 사람) 연구소에서
일하던 Calaude Shannon (클로드 섀넌)에 의해 개발되었습니다.  

151
00:11:52,320 --> 00:11:57,360
Shannon는 그 당시 지적 괴물이고 수학과 물리학에서 저명하고 

152
00:11:57,360 --> 00:12:01,440
컴퓨터 과학의 시초인 John von Neumann (폰 노이만)과 

153
00:12:01,440 --> 00:12:04,800
아직 공개되지 않은 아이디어에 대해 이야기를 나누었습니다.
And when he mentioned that he  

154
00:12:04,800 --> 00:12:08,640
그리고 그가 "정보량의 기대값"에 대한
좋은 이름이 없다고 말했을 때, 

155
00:12:08,640 --> 00:12:13,120
von Neumann은 아마도 이렇게 말했을 것입니다.
"흠, 당신은 그것을 엔트로피라고 불러야 할 것입니다. 두 가지 이유로요.

156
00:12:13,120 --> 00:12:17,600
우선 당신의 불확실성 함수는 통계 역학에서 
그 이름으로 사용되어 왔기 때문에, 이미 이름을 가지고 있습니다.

157
00:12:17,600 --> 00:12:22,640
그리고 두 번째, 그리고 더 중요한 것은 아무도
엔트로피가 실제로 무엇인지 모른다는 것입니다.

158
00:12:22,640 --> 00:12:26,720
따라서 토론에서 항상 유리할 것입니다."
(...어라?)

159
00:12:27,440 --> 00:12:31,120
그래서 만약 이름이 약간 신비롭게 느껴진다면,
그리고 이 이야기가 믿겨진다면,

160
00:12:31,120 --> 00:12:35,680
그건 일종의 "설계"입니다. 
또한, 만약 여러분이 물리학에서 나온   

161
00:12:35,680 --> 00:12:40,240
열역학 제2법칙과의 관계가 궁금한다 해도,
분명 연관성이 있긴 하지만,

162
00:12:40,240 --> 00:12:44,560
Shannon은 그냥 순수한 확률론을 다룹니다.
그래서 우리의 용도로써, 

163
00:12:44,560 --> 00:12:49,440
제가 엔트로피라는 단어를 사용할 때,
여러분은 특정한 추측의 정보의 기대값으로 생각하기를 바랍니다.

164
00:12:50,560 --> 00:12:54,160
엔트로피는 두 가지를 동시에 측정하는 것으로 생각할 수 있습니다.

165
00:12:54,160 --> 00:12:59,440
첫 번째는 분포표가 얼마나 평평한지입니다.
분포가 균일해질수록

166
00:12:59,440 --> 00:13:04,400
엔트로피는 더욱 높아집니다.
패턴이 3⁵개인 경우, 

167
00:13:04,400 --> 00:13:09,200
총 균일한 분포에서 그 중 하나를 관찰하면
정보가 log₂(3⁵)가 되는데,

168
00:13:10,720 --> 00:13:16,320
이는 7.92입니다. 
이것이 이 엔트로피가 가질 수 있는 절대적인 

169
00:13:16,320 --> 00:13:21,360
최대치입니다.
하지만 엔트로피는 애초에 얼마나 많은 가능성이 있는지를 나타내는 

170
00:13:21,360 --> 00:13:26,720
척도이기도 합니다. 
예를 들어, 16개의 패턴만 있는 단어를 가지고 있다면, 

171
00:13:26,720 --> 00:13:32,720
그리고 각각의 패턴은 동등할 가능성이 동일하다면,
이 엔트로피, 즉 예상 정보는 4bits가 될 것입니다.

172
00:13:32,720 --> 00:13:36,960
또한 64개의 가능한 패턴이 있고 있고 모두 동일할 

173
00:13:36,960 --> 00:13:42,800
가능성이 있는 단어의 경우, 엔트로피는 6bits가 됩니다.

174
00:13:42,800 --> 00:13:48,560
그래서 만약 여러분이 엔트로피가 6bits인 분포표를 본다면, 
이것은 마치 64개의 동일한 결과가 있는 것처럼 

175
00:13:48,560 --> 00:13:53,440
앞으로 일어날 일에 많은 변화와 불확실성이
있다는 것을 말하는 것과 같습니다.

176
00:13:54,160 --> 00:13:58,160
Wordlebot에서의 첫번 째 수행에서는,
기본적으로 이런 작업을 했습니다.

177
00:13:58,160 --> 00:14:01,360
13,000단어를 모두 포함하여 여러분이
가질 수 있는 다양한 추측을 모두 살펴봅니다. 

178
00:14:01,360 --> 00:14:07,120
그리고 각각의 엔트로피, 더 구체적으로는 여러분이 볼 수 있는

179
00:14:07,120 --> 00:14:12,080
모든 패턴에 걸친 분포표의 엔트로피를 계산하고 가장 높은 것을 선택합니다.

180
00:14:12,080 --> 00:14:15,840
왜냐하면, 그렇게 한다면 가능성의 공간을 최대한 줄일 수 있기 때문입니다.

181
00:14:16,880 --> 00:14:20,240
제가 여기서 첫 번째 추측에 대해서만 이야기했지만,
다음 몇 가지 추측에도 똑같은 역할을 합니다.

182
00:14:20,240 --> 00:14:24,560
예를 들어, 첫 번째 추측에서 패턴을 보고 나면,

183
00:14:24,560 --> 00:14:29,200
일치하는 단어에 따라 가능한 단어의 개수가 제한됩니다. 

184
00:14:29,200 --> 00:14:33,680
그 후에 더 작아진 단어 집합안에서 똑같이 게임을 하면 됩니다.

185
00:14:33,680 --> 00:14:37,920
두 번째 추측을 위해 제한된 단어 집합에서
발생할 수 있는 모든 패턴의 분포를 살펴봅시다.

186
00:14:38,880 --> 00:14:43,840
13,000개의 모든 가능성을 찾아보면
엔트로피를 최대화하는 것을 찾을 수 있습니다.

187
00:14:45,200 --> 00:14:49,040
이것이 어떻게 작동하는지를 보여드리기 위해 

188
00:14:49,040 --> 00:14:54,640
제가 쓴 Wordle의 변형된 부분을 여백에 표시해 보겠습니다.

189
00:14:54,640 --> 00:14:59,040
모든 엔트로피를 계산한 후, 오른쪽에는 어떤 것이
가장 높은 예상 정보를 가지고 있는지 나와 있습니다.

190
00:14:59,040 --> 00:15:03,120
적어도 나중에 이것을 구체화할 시점에서 최고의 대답은 

191
00:15:03,120 --> 00:15:10,160
"tares"입니다.  이것은...음...
가장 흔한 살갈퀴(콩과의 식물)를 의미합니다. 

192
00:15:10,800 --> 00:15:14,640
우리가 여기에 추측을 할 때마다,
전 옆에 있는 추천들을 무시할거고 "slate"를 하겠습니다. 

193
00:15:14,640 --> 00:15:19,440
왜냐하면 그냥 저는 "slate"가 좋습니다.
그리고 예상 정보들이 보입니다.

194
00:15:19,440 --> 00:15:22,880
하지만 또 여기 단어 오른쪽에 우리가 얼마나 

195
00:15:22,880 --> 00:15:26,480
많은 정보를 얻었는지 보여줍니다.
여기선 우리가 조금 운이 없었던 것 같아요. 

196
00:15:26,480 --> 00:15:30,640
5.8 정도를 얻을 것으로 예상했는데, 5.8 보다 적게 나왔습니다.

197
00:15:30,640 --> 00:15:34,800
그리고 여기 왼쪽에는 우리가 현재 있는 단계에서
가능한 모든 단어가 표시되어 있습니다. 

198
00:15:35,600 --> 00:15:39,680
파란 막대는 각 단어가 얼마나
그럴 가능성이 있다고 생각하는지 알려주기 때문에 

199
00:15:39,680 --> 00:15:43,280
이 순간에는 각 단어가 똑같이 발생할 가능성이 있다고 가정하지만,
잠시 후에 이를 재규정 할 것입니다.

200
00:15:43,840 --> 00:15:48,160
그리고 이 불확실성 값은 가능한 단어들에
대한 엔트로피 분포를 알려주고 있습니다.

201
00:15:48,160 --> 00:15:52,320
하지만 현재 이 분포는 균일한 분포이기 때문에, 

202
00:15:52,320 --> 00:15:56,240
이것은 그냥 가능성의 수를 계산하는 쓸데없이 복잡한 방법입니다.

203
00:15:56,240 --> 00:16:01,440
예를 들어, 만약 우리가 2의 13.66 거듭제곱을 한다면,
이건 약 13,000개의 가능성일 것입니다.

204
00:16:01,440 --> 00:16:06,560
여기가 좀 이상하게 보이긴 한데, 소수자리를 안 나타내서 그래요.

205
00:16:06,560 --> 00:16:10,400
불필요한 것처럼 느껴질 수도 있고, 너무 복잡한 것처럼 느껴질 수도 있지만, 

206
00:16:10,400 --> 00:16:14,240
여러분은 왜 두 숫자를 모두 가지고 있는 것이 유용한지 1분 안에 알게 될 것입니다.
두번째 추측에서 가장 높은 엔트로피는

207
00:16:14,240 --> 00:16:19,520
"ramin" 인 것 같습니다. 또 다시..
이런건 전혀 단어처럼 느껴지지 않아요. (ㅋㅋ 사전에도 안 나옴)

208
00:16:19,520 --> 00:16:26,160
그래서 조금 도덕적으로 저는 "rains"를 입력하겠습니다.

209
00:16:26,160 --> 00:16:31,760
또 운이 안 좋은거 같네요.
우리는 4.3bits를 예상했지만 오직 3.39bits 정도 얻었습니다.

210
00:16:31,760 --> 00:16:36,720
이제 55가지 가능성으로 줄여집니다. 

211
00:16:36,720 --> 00:16:41,840
그리고 여기서 저는 뭘 의미하든 간에
제안하고 있는 "kombu"를 쓰겠습니다.. 좋아요!

212
00:16:41,840 --> 00:16:46,720
이번엔 좋은 기회에요.
이 패턴은 4.78 bits의 정보를 제공하지만, 

213
00:16:46,720 --> 00:16:51,040
일단 이 패턴을 보기 전에 왼쪽 위에
5.78bits 의 불확실성이 있었다는 것을 나타냅니다.

214
00:16:51,040 --> 00:16:56,320
잠시 퀴즈로, 남은 가능성의 수는 무엇을 의미할까요?

215
00:16:57,840 --> 00:17:01,520
이것은 우리가 불확실성을 1bit로 줄였다는 것을 의미합니다. 

216
00:17:01,520 --> 00:17:05,600
즉, 두 가지 가능한 답이 있다는 것과 같습니다. 50대 50의 선택입니다.

217
00:17:06,240 --> 00:17:09,760
그리고 여기에서, 우리는 어떤 단어가 더 흔한지 알고 있기 때문에,

218
00:17:09,760 --> 00:17:14,160
우리는 그 답이 "abyss"라는 것을 알고 있습니다.
하지만 지금 보여져 있는 것처럼 프로그램은 그걸 모르고 있습니다. 

219
00:17:14,160 --> 00:17:18,800
그래서 가능한 한 많은 정보를 얻으려고 노력하죠.
단 한 가지 가능성만 남아있을 때까지요. 

220
00:17:18,800 --> 00:17:23,200
그리고 나서 추측하죠.
따라서 우린 더 나은 끝판왕 전략이 필요합니다. 

221
00:17:23,200 --> 00:17:28,000
일단 이 버전을 Wordle Solver로 부르겠습니다.
그런 다음 시뮬레이션을 실행해 보겠습니다.

222
00:17:30,640 --> 00:17:34,800
이 방법은 가능한 모든 Wordle 게임을 하는 것입니다.
Wordle의 답인 2,315개의 모든 단어를 살펴보는 것입니다. 

223
00:17:34,800 --> 00:17:40,160
기본적으로 이를 테스트 세트로 사용하고 있으며 
단어가 얼마나 흔한지 고려하지 않는

224
00:17:40,160 --> 00:17:45,760
이 순진한 방법으로 그리고 단 하나의 선택이 될 때까지 

225
00:17:45,760 --> 00:17:50,480
각 단계에서 정보를 최대화하려고 노력하면

226
00:17:50,480 --> 00:17:56,720
시뮬레이션이 끝날 때까지 평균 점수는 약 4.124가 됩니다. 
움... 그렇겐 나쁘진 않네요. (...엥?)

227
00:17:56,720 --> 00:18:01,200
솔직히 더 나쁠 줄 알았어요.
하지만 Wordle 게임을 하는 사람들은

228
00:18:01,200 --> 00:18:05,280
보통 4개 안에 맞춘다고 말합니다. 
우리의 진짜 도전은 가능한 한 3번으로 많이 맞추는 것입니다.

229
00:18:05,280 --> 00:18:09,840
4점과 3점 사이에서는 꽤 큰 도약입니다.
여기서 쉬운 목표는

230
00:18:09,840 --> 00:18:14,480
어떤 단어가 흔한지 아닌지를 어떻게든 통합시키는 것입니다.
그러면 우리는 어떻게 정확하게 할 수 있을까요?

231
00:18:22,880 --> 00:18:26,480
제가 접근한 방법은 영어의 모든 단어에 대한 상대 빈도 목록을 얻는 것입니다.

232
00:18:26,480 --> 00:18:31,280
전 Mathematica의 단어 빈도 데이터 기능을 사용했는데, 

233
00:18:31,280 --> 00:18:36,000
이 기능은 자체적으로 Google Books English n-gram
공개 데이터 세트에 있는 겁니다.

234
00:18:36,000 --> 00:18:39,840
예를 들어 가장 흔한 단어에서 가장 덜 흔한 단어로 분류하면, 

235
00:18:39,840 --> 00:18:44,480
분명 이 단어들이 영어에서 가장 흔한 다섯 글자 단어들입니다. 
"there"가 8번째로 흔합니다.

236
00:18:44,480 --> 00:18:49,840
첫 번째는 "which"이고,
그 뒤에 "their"과 "there"이 있습니다.

237
00:18:49,840 --> 00:18:54,240
"First"는 첫 번째가 아니라 아홉 번째이고,
"First" 뒤에 오는 단어들이 

238
00:18:54,240 --> 00:18:58,480
"after" "where" 그리고 "those" 인데,
이 단어들이 조금 덜 흔하게 나오는 것은 타당합니다.

239
00:18:59,120 --> 00:19:04,160
자, 이 데이터를 이용하여 각각의 단어가
최종 정답이 될 가능성을 모델링할 때, 

240
00:19:04,160 --> 00:19:07,760
그저 빈도에만 비례해서는 안 됩니다.
예를 들어 이 데이터 세트에서

241
00:19:07,760 --> 00:19:13,200
"which"에 0.002의 점수가 부여되는 반면
"braid"라는 단어는 어떤 의미에서는

242
00:19:13,200 --> 00:19:17,360
가능성이 약 천 배 적습니다. 
하지만 이 두 단어 모두 충분히 흔하기 때문에 

243
00:19:17,360 --> 00:19:20,960
충분히 고려할 가치가 있습니다.
그래서 우리는 좀 더 이분법적으로 골라내기를 할 필요가 있습니다.

244
00:19:21,600 --> 00:19:26,080
제가 한 방법은 이 모든 분류된 단어 목록을  

245
00:19:26,080 --> 00:19:30,640
x축에 배열하고 시그모이드 함수를 적용하는 것입니다.

246
00:19:30,640 --> 00:19:35,120
시그모이드 함수는 기본적으로 2진법이고 0이거나 1이긴 하지만 

247
00:19:35,120 --> 00:19:38,240
불확실성의 영역 사이에는 매끄러운 부분이 있습니다.

248
00:19:38,880 --> 00:19:43,680
그래서 제가 각 단어를 최종 목록에 배치할 확률은 

249
00:19:43,680 --> 00:19:48,400
x축 위에 있는 시그모이드 함수의 값이 될 것입니다.

250
00:19:49,280 --> 00:19:54,080
분명히 이것은 몇 가지 매개변수에 따라 달라집니다.
예를 들어,

251
00:19:54,080 --> 00:19:59,200
x축의 공백이 얼마나 넓어지면 1에서 0까지 얼마나 천천히,
얼마나 가파르게 떨어지는지가 결정되고, 

252
00:19:59,200 --> 00:20:03,440
왼쪽에서 오른쪽으로 어디에 되느냐에 따라
골라내기가 결정됩니다.

253
00:20:03,440 --> 00:20:07,600
그리고 솔직히 제가 한 방법은
그냥 손가락을 핥고 훑어보는 겁니다. (직--관)

254
00:20:07,600 --> 00:20:12,080
전 분류된 목록을 훑어보고 제가 봤을 때
이 단어들의 절반 정도가

255
00:20:12,080 --> 00:20:16,080
최종 정답이 아닐 가능성이 높은 구간을 찾으려고 했습니다.
그리고 전 이 방법으로 골라냈습니다.

256
00:20:16,880 --> 00:20:21,760
이렇게 단어 전체에 걸쳐 이와 같은 분포를 갖게 되면 

257
00:20:21,760 --> 00:20:26,320
엔트로피가 매우 유용한 측정이 되는 상황이 됩니다.

258
00:20:26,320 --> 00:20:31,120
예를 들어, 게임을 하고 있는데 "other"과 "nails"로 시작해서 

259
00:20:31,120 --> 00:20:35,520
일치하는 단어가 4개 있는 상황으로 끝난다고 가정해 보겠습니다.
그리고 우리가 이 단어들이 서로 동등하다고 가정해 봅시다.

260
00:20:36,160 --> 00:20:43,040
여러분한테 하나 물어보겠습니다,
이 분포에서 엔트로피는 얼마일까요?

261
00:20:43,040 --> 00:20:48,560
이러한 각각의 가능성과 관련된 정보는 1/4이고 log₂(4)를 하면

262
00:20:49,440 --> 00:20:53,440
2가 됩니다. 이것은 2bits의 정보,
4가지 가능성, 모두 아주 좋고 좋은 정보입니다.

263
00:20:54,080 --> 00:20:58,640
하지만! 만약 제가 여려분에게
4개 이상의 일치가 있다고 말한다면 어떻게 될까요? 

264
00:20:58,640 --> 00:21:03,360
실제로 전체 단어 목록을 살펴보면
16개의 단어가 일치합니다.

265
00:21:03,360 --> 00:21:08,320
하지만 우리의 모델이 실제로 최종 정답이 되는
나머지 12개의 단어들에 대한 확률을 

266
00:21:08,320 --> 00:21:10,720
1/1000 정도로 낮다고 합니다.
왜냐하면 그 단어들은 잘 알려지지 않은 단어이기 때문입니다.

267
00:21:11,280 --> 00:21:14,240
한번 더 물어보겠습니다,
그럼 이 분포에서 엔트로피는 얼마일까요?

268
00:21:15,200 --> 00:21:19,760
여기서 엔트로피가 단순히 일치 항목 수를 측정하는 것이라면, 

269
00:21:19,760 --> 00:21:25,920
여러분은 log₂(16)인 4 라고 예상할 수 있습니다.
우리가 그전에 얻은 것 보다, 2bits 정도가 더 불확실한 상황입니다.

270
00:21:25,920 --> 00:21:29,360
그러나 물론, 실제적인 불확실성은
이전과 크게 다르지 않습니다.

271
00:21:29,360 --> 00:21:33,360
예를 들어, 이 잘 알려지지 않은 12개의 단어들이 있다고 해서,

272
00:21:33,360 --> 00:21:38,080
우리가 최종 답이 "charm"이라는 것을
그렇게 놀랍게 생각하지 않습니다.

273
00:21:38,080 --> 00:21:41,920
여기서 실제로 계산을 하고 각 발생 확률에 

274
00:21:41,920 --> 00:21:48,080
해당 정보 수를 곱하면 2.11bits가 나옵니다.
이것은 기본적으로 2비트이고,

275
00:21:48,080 --> 00:21:51,920
기본적으로 4가지 가능성입니다.
하지만 매우 가능성이 없는 모든 사건들까지 포함하기 때문에 

276
00:21:51,920 --> 00:21:56,240
조금 더 불확실성이 있습니다. 하지만 만약 여러분이
이것들을 안다면, 여러분은 많은 정보를 얻을 수 있을 것입니다.

277
00:21:56,880 --> 00:22:00,720
그래서 멀리 본다면,
이것은 Wordle이 정보이론 수업에서 좋은 예시라는 것입니다.

278
00:22:00,720 --> 00:22:06,160
우린 엔트로피에 대한 이 두 가지
다른 느낌의 응용 프로그램을 가지고 있습니다.

279
00:22:06,160 --> 00:22:10,880
첫 번째는 주어진 추측으로부터 얻을 수 있는
예상 정보를 알려주고,

280
00:22:10,880 --> 00:22:15,360
두 번째는 가능한 모든 단어들 중에서
불확실성을 측정하는 것 입니다.

281
00:22:16,160 --> 00:22:19,920
제가 강조하는 것은, 우리가 추측에 대한 예상 정보를 볼 때, 

282
00:22:19,920 --> 00:22:24,720
단어에 대한 불균등한 가중치가 있다면
그것이 엔트로피 계산에 영향을 미친다는 것입니다.

283
00:22:24,720 --> 00:22:29,040
앞에서 살펴본 "weary"의 분포를 예로 하고,

284
00:22:29,040 --> 00:22:33,600
그러나 이번에는 가능한 모든 단어에 대해
불균등분포를 사용합니다.

285
00:22:34,240 --> 00:22:41,040
그럼 제가 잘 표현된 부분을 찾을 수 있을지...
어... 좋아요, 여기가 괜찮네요. 

286
00:22:41,040 --> 00:22:45,520
여기 두 개의 이웃한 패턴이 있는데, 

287
00:22:45,520 --> 00:22:49,120
그 중 하나는 32개의 가능한 단어를 가지고 있다고 합니다. 

288
00:22:49,120 --> 00:22:54,160
그리고 무엇인지 확인한다면, 이것들이 그 32개의 단어들입니다.
모두 매우 가능성이 낮은 단어들입니다.

289
00:22:54,160 --> 00:22:59,520
여러분이 빠르게 훑어보면,
그럴듯한 답을 찾기가 힘듭니다. 어쩌면 "yells" 일 수도? (아님말고!)

290
00:22:59,520 --> 00:23:02,960
하지만 분포에서 옆에 있는 패턴을 보면, 

291
00:23:02,960 --> 00:23:08,320
일치 가능한 단어가 8개밖에 없다고 합니다.
그래서 4분의 1정도만 일치하지만, 

292
00:23:08,320 --> 00:23:12,480
거의 그럴 것 같습니다. 그리고 우리가 그 항목들을 본다면
우리는 왜 그런지 알 수 있습니다. 

293
00:23:12,480 --> 00:23:16,160
이들 중 "wring", "wrath", "wraps"와 같은
일부는 실제적인 그럴듯한 대답입니다.

294
00:23:17,600 --> 00:23:21,840
이 모든 것을 통합하는 것을 보여주기 위해,
Wordlebot 버전 2를 보겠습니다.

295
00:23:22,400 --> 00:23:26,240
우리가 처음 본 것 보다 크게 2~3 가지 차이점이 있습니다.

296
00:23:26,240 --> 00:23:30,960
첫 번째로, 그전에 말했듯이, 우리가 엔트로피, 
즉 정보의 기대값을 계산하는 방법은
300
00:23:30,960 --> 00:23:36,160
주어진 단어가 실제로 답이 될 확률을 포함하는 모든 패턴에 관해서,

297
00:23:36,160 --> 00:23:41,920
보다 정교한 분포를 사용하는 것입니다.
그리고, 다른 것들은 조금 다르지만,

298
00:23:41,920 --> 00:23:46,160
"tares"가 여전히 1위입니다.
둘째로, 상위권 순위를 매길 때, 

299
00:23:46,160 --> 00:23:50,880
몇 가지 추측을 더 쉽게 하기 위해 각 단어가 실제 답일
304
00:23:50,880 --> 00:23:54,960
확률의 모델을 유지하고 이를 결정에 포함합니다.

300
00:23:55,680 --> 00:23:59,680
기계가 시키는 대로 살아가면 안되기 때문에,
추천을 또 무시해보겠습니다.

301
00:24:00,720 --> 00:24:04,160
그리고 여기 왼쪽에 있는 또 다른 점을 말해야 할 것 같습니다. 

302
00:24:04,160 --> 00:24:09,120
불확실성 값, bit 수는 더 이상 가능한 일치 수와 중복되지 않습니다.  

303
00:24:09,120 --> 00:24:13,840
자, 이제 계산해보면, 2의 8.02 거듭제곱을 하면, 

304
00:24:13,840 --> 00:24:21,360
256을 조금 넘습니다. 259 인거 같네요. 
이것이 의미하는 것은 실제로 

305
00:24:21,360 --> 00:24:25,840
이 패턴과 일치하는 총 단어가 526개 있음에도 불구하고,
그 불확실성의 양은 259개의 동일한 

306
00:24:25,840 --> 00:24:31,920
결과가 있는 것과 비슷하다는 것입니다.
여러분은 

307
00:24:31,920 --> 00:24:36,560
"borks", "yortz", "zoril", "zorus"는
답이 아니라는 것을 알고 있습니다.

308
00:24:36,560 --> 00:24:41,280
그래서 이전의 경우보다 조금 덜 불확실합니다,
즉, 이 bits 수는 더 적을 것입니다.

309
00:24:41,280 --> 00:24:45,840
제가 게임을 진행하고, 2개의 추측을 통해
제가 설명하고자 하는 것을 다듬어 보겠습니다.

310
00:24:48,400 --> 00:24:53,040
네 번째 추측에서, 저 위에 상위 선택들을 본다면,
더 이상 엔트로피를 최대화만 하는 것이 아니라는 것을 알 수 있습니다.

311
00:24:53,040 --> 00:24:58,000
이 시점에서, 정확히 7개의 단어가 있지만 

312
00:24:58,000 --> 00:25:02,720
의미 있는 것은 "dorms"와 "words" 뿐이고,

313
00:25:02,720 --> 00:25:06,640
엄밀히 말하자면, 다른 단어들도
정보를 더 줄 수 있게 순위에 있습니다.

314
00:25:07,360 --> 00:25:11,280
제가 이걸 처음 했을 때는, 저는 각각의 추측의 양질을 측정하기 위해 

315
00:25:11,280 --> 00:25:15,280
이 두 숫자를 더했습니다.
실제로 여러분이 의심했던 거에 비해 잘 됐습니다.

316
00:25:15,280 --> 00:25:19,600
하지만 이건 좀 체계적이지 않게 느껴집니다. 사람들이 할 수 있는
몇 가지의 방법들이 있다는 걸 알지만, 여기 제가 선택한 방법이 있습니다.

317
00:25:19,600 --> 00:25:24,400
만약 우리가 이 "words"의 경우와 같이 다음 추측을 한다면,  

318
00:25:24,400 --> 00:25:30,080
우리가 정말로 관심을 갖는 것은 "words"로 할 경우의 예상 점수입니다.
그리고 그 예상 점수를 계산하기 위해,  

319
00:25:30,080 --> 00:25:33,920
우리는 "'words'가 실제 답일 확률이 얼마나 될까?
라고 생각 할 수 있고,

320
00:25:33,920 --> 00:25:39,440
현재 58% 정도로 됩니다.
그래서 이 게임에서 58%의 확률로

321
00:25:40,080 --> 00:25:46,000
우리는 4점을 얻을 수 있고,
1에서 58%를 뺀 확률 (42%)이, 점수가 4점 초과가 됩니다.

322
00:25:46,000 --> 00:25:50,800
얼마나 그럴까요? 우리도 잘 모르지만, 일단 그 지점에
도달했을 때, 얼마나 많은 불확실성이 있을 것인지를 

323
00:25:50,800 --> 00:25:55,840
기반으로 예측할 수 있습니다.
구체적으로, 현재 1.44 bits의 불확실성이 존재하는데, 

324
00:25:56,400 --> 00:25:59,600
만약 우리가 "words"를 추측한다면,
예상 정보가 1.27 bits로 나옵니다. 

325
00:25:59,600 --> 00:26:02,720
결국, 이 차이는 이 시점 이후에

326
00:26:02,720 --> 00:26:08,000
얼마나 많은 불확실성이 남겨져 있는지를 나타냅니다.

327
00:26:08,000 --> 00:26:12,480
우리에게 필요한 것은 이 불확실성을 예상 점수와 연관시키는 

328
00:26:12,480 --> 00:26:17,120
일종의 함수, 𝒇 입니다. 
제가 이 문제를 해결하는 방법은, 버전 1의 봇에 있는 이전 게임에 

329
00:26:17,120 --> 00:26:23,520
버전 1의 봇에 있는 이전 게임에 관한 데이터를 표시하는 것 입니다.
이렇게 말이죠 "님아, 불확실성이 있는

330
00:26:23,520 --> 00:26:28,240
다양한 지점에서 그 실제 점수가 몇점이었니?"
예를 들어, 여기에 약 8.7 정도에 있는

331
00:26:28,240 --> 00:26:33,280
데이터들은 다음과 같이 말 할 수 있습니다.
"일부 게임의 경우 8.7 bits의

332
00:26:33,280 --> 00:26:38,080
불확실성이 있었던 지점 이후에 최종 답을
얻기 위해선 두 번의 추측이 필요했습니다.

333
00:26:38,080 --> 00:26:42,160
다른 게임에서는 세 번의 추측이,
또 다른 게임에서는 네 번의 추측이 필요했습니다."

334
00:26:42,960 --> 00:26:46,840
여기서 왼쪽으로 이동하면, 0을 넘는 모든 점들은 

335
00:26:46,840 --> 00:26:50,880
"0bit의 불확실성이 있을 때, 즉, 단 하나의 가능성만 있다고 할 때,

336
00:26:50,880 --> 00:26:54,560
필요한 추측의 수는 항상 하나입니다." 라고 말할 수 있습니다.
문제 될게 없죠.

337
00:26:54,560 --> 00:26:59,520
1bit의 불확실성이 있을 때마다, 즉 2개의 가능성으로 의미되며,

338
00:26:59,520 --> 00:27:02,960
때로는 한 가지 추측이 더 필요할 때도, 
두 가지 추측이 더 필요할 때도 있습니다.

339
00:27:03,520 --> 00:27:08,640
여기서 이 데이터를 시각화하는 좀 더 쉬운 방법은 데이터를 

340
00:27:08,640 --> 00:27:13,360
묶음으로 묶어서 평균을 구하는 것입니다. 
예를 들어, 이 막대는 "1 bit의 

341
00:27:13,360 --> 00:27:19,840
불확실성이 있었던 모든 지점 중에서 평균적으로
필요한 새로운 추측의 수는 약 1.5개였습니다." 라고 말할 수 있습니다. 

342
00:27:22,080 --> 00:27:26,480
그리고 저기 있는 막대는 "4 bits을 약간 넘는 

343
00:27:26,480 --> 00:27:31,280
불확실성이 있었던 모든 지점 중에서,
이는 16개의 다른 가능성으로 좁혀진 것과 같으며, 

344
00:27:31,280 --> 00:27:35,840
그 시점에서 평균적으로 2개 이상의 추측이 필요합니다."
라고 말할 수 있습니다. 

345
00:27:35,840 --> 00:27:39,760
그리고 여기서 저는 이것에 합당한
함수를 맞추기 위해 회귀 분석을 했습니다.

346
00:27:39,760 --> 00:27:44,080
그리고 이 모든 것을 할 때 중요한 점은
우리가 단어로부터 더 많은 정보를 얻을수록 

347
00:27:44,080 --> 00:27:48,320
예상 점수는 더 낮아진다는 직감을
숫자로 나타낼 수 있다는 것입니다.

348
00:27:49,520 --> 00:27:54,640
버전 2.0 봇으로 돌아가서,
2,315개의 Wordle 답이 있는 

349
00:27:54,640 --> 00:28:01,520
똑같은 시뮬레이션을 실행하면 어떻게 될까요?
첫 번째 버전과는 달리 

350
00:28:01,520 --> 00:28:06,400
확실히 더 좋습니다, 안심이 되네요.
종합해 보면 평균은 3.6 정도입니다.

351
00:28:06,400 --> 00:28:10,640
비록 첫 번째 버전과 달리, 이 상황에서는 6번 이상 

352
00:28:10,640 --> 00:28:14,480
필요한 경우가 몇 번 있는데, 아마도 정보를 극대화하기 보다는 

353
00:28:14,480 --> 00:28:17,840
실제로 목표를 달성하기 위해
균형을 맞추는 경우가 있기 때문일 것입니다.

354
00:28:18,800 --> 00:28:25,360
그럼 3.6점 보다 더 잘할 수 있을까요? 당연히 할 수 있죠.
저는 처음에 모델을 만들 때

355
00:28:25,360 --> 00:28:29,680
Wordle의 답 목록을 포함하지 않는 것이 
가장 재미있다고 말했습니다.
361
00:28:29,680 --> 00:28:35,760
하지만 만약 목록을 포함시킨다면, 제가 얻을 수 있는
최고의 점수는 3.43 정도였습니다.

356
00:28:35,760 --> 00:28:41,840
그래서 만약 우리가 단어의 빈도 데이터를 사용해서
이 사전 분포를 선택하는 것보다 

357
00:28:41,840 --> 00:28:46,000
더 정교하게 하려고 한다면, 아마도 3.43 점이 우리가 얻을 수 있는 최대값일 겁니다.
적어도 제가 잘하면 최소 3.43 정도를 얻을 수 있죠.

358
00:28:46,000 --> 00:28:49,920
그 최고의 성능은 본질적으로 제가 지금까지 말했던 

359
00:28:49,920 --> 00:28:53,680
아이디어를 사용하지만 조금 더 진행해야합니다. 
예상 정보를 한 단계 보다, 

360
00:28:53,680 --> 00:28:57,440
두 단계를 검색하는 것처럼요.
원래는 저는 이거에 대해 더 말하려고 했는데, 

361
00:28:57,440 --> 00:29:01,840
시간이 꽤 많이 흘렀다는 것을 압니다. (벌써 29분 정도 흐름 ㄷㄷ)
한 가지 말씀드리고 싶은 것은 

362
00:29:01,840 --> 00:29:05,840
이 두 단계 검색을 한 후 상위 후보들로 

363
00:29:05,840 --> 00:29:10,000
샘플 시뮬레이션을 몇 번 하면 "crane" 이 
최고의 시작 단어인 것 같습니다. 누가 상상이나 해봤을까요?

364
00:29:10,720 --> 00:29:15,600
또한 어려분이 정답 목록을 사용하여 가능성의 공간을 결정한다면, 

365
00:29:15,600 --> 00:29:20,080
당신이 시작하는 불확실성은 11비트가 조금 넘습니다. 
무차별 대입으로 

366
00:29:20,080 --> 00:29:26,320
처음 두 번 추측한 후에 예상되는 최대 정보는 약 10비트입니다. 

367
00:29:26,320 --> 00:29:30,400
즉, 처음 두 번 추측한 후에 최상의 플레이를 할 경우 

368
00:29:30,400 --> 00:29:35,520
약 1 bit의 불확실성이 남는데, 이는 두 번 추측한 것과 같습니다.

369
00:29:35,520 --> 00:29:39,520
따라서 이 평균을 3으로 낮추는 알고리즘을 결코 작성할 수 없다고 

370
00:29:39,520 --> 00:29:44,240
말하는 것이 맞다고 생각합니다. 

371
00:29:44,240 --> 00:29:48,880
왜냐하면 단 두 번의 단계만으로 세 번째 슬롯에서 실패 없이 답을 맞출 수 있는 

372
00:29:48,880 --> 00:29:53,840
충분한 정보를 얻을 수 있는 공간이 없기 때문입니다.

