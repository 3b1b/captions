[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy.",
  "translatedText": "Wurdle というゲームは、ここ 1 ～ 2 か月でかなり話題になりました。数学の授業の 機会を見逃す人はいません。このゲームは、情報理 論、特にエントロピーとして知られるトピック。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could.",
  "translatedText": "多くの人と同じように、私もパズルに夢中になりました。また、 多くのプログラマーと同じように、私もゲームを可能な限り最適 にプレイするアルゴリズムを作成することに夢中になりました。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy.",
  "translatedText": "ここで私がやろうと思ったのは、アルゴリズム全体がこのエン トロピーの考え方に中心を置いているため、そのプロセスの一 部を話し、それに使用された数学の一部を説明することです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle?",
  "translatedText": "まず最初に、Wurdle について聞いたことがない場合のために説明します。Wurdle とは何ですか? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us.",
  "translatedText": "そして、ここでゲームのルールを確認しながら一石二鳥にするために、 今後の展開についてもプレビューさせていただきます。これは、基本的 にゲームをプレイするための小さなアルゴリズムを開発することです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does.",
  "translatedText": "私は今日の Wurdle を実行していませんが、これは 2 月 4 日なので、ボットがどのように動作するか見てみましょう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess.",
  "translatedText": "Wurdle の目的は、謎の 5 文字の単語を推測す ることであり、推測する機会が 6 回与えられます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane.",
  "translatedText": "たとえば、Wurdle ボットは、推測クレーンから始めることを提案します。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer.",
  "translatedText": "推測するたびに、その推測が真の答えにどの程 度近づいているかに関する情報が得られます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer.",
  "translatedText": "ここで灰色のボックスは、実際の答えに C が存在しないことを示しています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position.",
  "translatedText": "黄色のボックスは R があることを示していますが、その位置にありません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position.",
  "translatedText": "緑色のボックスは、秘密の単語に A があり、そ れが 3 番目の位置にあることを示しています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E.",
  "translatedText": "そして、NもEもありません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information.",
  "translatedText": "それでは、早速入って、Wurdle ボットにその情報を伝えましょう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey.",
  "translatedText": "クレーンから始まり、灰色、黄色、緑色、灰色、灰色になりました。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time.",
  "translatedText": "現在表示されているすべてのデータについては心配しないでください。それについては、やがて説明します。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick.",
  "translatedText": "しかし、2番目に選ぶ一番の提案は「クソ」です。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess.",
  "translatedText": "推測は実際の 5 文字の単語である必要がありますが、 ご覧のとおり、実際に推測できる内容はかなり自由です。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick.",
  "translatedText": "この場合、shtick を試します。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good.",
  "translatedText": "さて、状況はかなり良いようです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R.",
  "translatedText": "S と H を押すと、最初の 3 文字がわかり、R があることがわかります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something.",
  "translatedText": "そして、それは SHA 何か R 、または SHA R 何かのようなものになるでしょう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp.",
  "translatedText": "そして、Wurdle ボットは、シャードかシャー プの 2 つの可能性だけを知っているようです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a tossup between them at this point, so I guess probably just because it's alphabetical it goes with shard.",
  "translatedText": "現時点では、この2つのどちらを選ぶかは微妙なところだから、おそらくアルファベット順だからシャードになるんだと思う。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer, so we got it in three.",
  "translatedText": "というわけで、3つの答えが出た。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.22,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle, four is par and three is birdie.",
  "translatedText": "ウードルでは4打がパー、3打がバーディーだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy.",
  "translatedText": "これは非常に適切な例えだと思います。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy.",
  "translatedText": "4 つを獲得するには、一貫してゲームに取り組む必要がありますが、それは確かにクレイジーではありません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great.",
  "translatedText": "しかし、3つになると、本当に素晴らしい気分になります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot.",
  "translatedText": "それで、もしあなたがそれに興味がないなら、私がここでやりたいことは、Wurdle ボットへのアプローチ方法についての私の思考プロセスを最初から説明することです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson.",
  "translatedText": "先ほども言いましたが、実際には、これは情報理論のレッスンの言い訳です。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy.",
  "translatedText": "主な目標は、情報とは何か、エントロピーとは何かを説明することです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language.",
  "translatedText": "これに取り組むにあたって私が最初に考えたのは、英語におけ るさまざまな文字の相対的な頻度を調べてみることでした。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters?",
  "translatedText": "そこで私は、これらの最も頻繁に使用される文字の多くに当てはまる 冒頭の推測または冒頭の推測のペアはあるだろうか、と考えました。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails.",
  "translatedText": "そして、私がとても気に入っていたのは、ネイルに続いて他のことをすることでした。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good, it feels like you're getting information.",
  "translatedText": "文字に当たれば、つまり、緑や黄色が出れば、それはいつもいい気分で、情報を得たような気分になる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.76,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information, since it's pretty rare to find a word that doesn't have any of these letters.",
  "translatedText": "しかし、このような場合、たとえヒットせず、常にグレーが出たとしても、これらの文字が1つもない単語を見つけるのはかなり稀なので、それはまだ多くの情報を与えていることになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters.",
  "translatedText": "しかし、それでも、これはあまり体系的とは思えません 。たとえば、文字の順序は考慮されていないからです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail?",
  "translatedText": "カタツムリを入力できるのに、なぜネイルを入力するのでしょうか? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end?",
  "translatedText": "最後にSがついたほうがいいでしょうか？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure.",
  "translatedText": "よくわかりません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y.",
  "translatedText": "さて、私の友人は、「疲れた」という単語で始めるのが好きだと言いました。それ には、W や Y などの珍しい文字が含まれているので、ちょっと驚きました。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener.",
  "translatedText": "しかし、おそらくそれはより良いオープナーであるかもしれません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess?",
  "translatedText": "潜在的な推測の質を判断するために与えることができ る、ある種の定量的なスコアはあるのでしょうか? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up.",
  "translatedText": "ここで、考えられる推測をランク付けする方法を準備するために、戻って 、ゲームがどのように正確に設定されているかを少し明確にしましょう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long.",
  "translatedText": "つまり、有効な推測として入力できる単語のリストがあ り、その長さはわずか約 13,000 単語です。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble.",
  "translatedText": "しかし、よく見てみると、本当に珍しいものがたくさんあります。頭やアリ、A RG など、スクラブル ゲームで家族の口論を引き起こすような言葉です。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word.",
  "translatedText": "しかし、ゲームの雰囲気は、答えが常にかなり一般的な単語になるということです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers.",
  "translatedText": "そして実際、答えとして考えられる約 2300 語のリストがもう 1 つあります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creators girlfriend, which is kind of fun.",
  "translatedText": "これは、ゲームクリエイターの彼女によって特別にキュレーションされたリストだと思う。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving wordle that doesn't incorporate previous knowledge about this list.",
  "translatedText": "しかし、私がやりたいことは、このプロジェクトでの挑戦は、このリストに関する以前の知識を取り入れることなく、ワードルを解くプログラムを書けるかどうかということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list.",
  "translatedText": "まず、このリストには載っていない、よく使わ れる 5 文字の単語がたくさんあります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play wordle against anyone, not just what happens to be the official website.",
  "translatedText": "だから、もう少し弾力性のあるプログラムを書いて、たまたま公式サイトだったものだけでなく、誰とでもワードルを対戦できるようにしたほうがいいだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also, the reason that we know what this list of possible answers is, is because it's visible in the source code.",
  "translatedText": "そしてまた、この可能性のある答えのリストが何であるかを我々が知っている理由は、ソースコードで見ることができるからである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day, that you could always just look up what tomorrow's answer will be.",
  "translatedText": "しかし、ソースコードで見ることができるのは、日によって答えが出てくる順番が決まっているため、明日の答えが何であるかをいつでも調べることができるのだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating.",
  "translatedText": "したがって、リストの使用には不正行為である意味があることは明らかです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words.",
  "translatedText": "そして、より興味深いパズルとより豊かな情報理論のレッスンを実現す るのは、代わりに一般的な単語の相対頻度などのより普遍的なデータ を使用して、より一般的な単語を好むという直感を捉えることです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess?",
  "translatedText": "では、これら 13,000 の可能性の中から、冒頭の推測をどのように選択すればよいのでしょうか? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality?",
  "translatedText": "たとえば、友人が疲れたプロポーズをした場合、その質をどのように分析すべきでしょうか? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W.",
  "translatedText": "まあ、彼がその可能性の低い W が好きだと言ったのは、その W が当た ったらどれだけ気持ちいいかというロングショットの性質が好きだからです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern.",
  "translatedText": "たとえば、最初に明らかにされたパターンが次のようなものであった場合、この巨大 な辞書にはそのパターンに一致する単語が 58 語しかないことがわかります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000.",
  "translatedText": "13,000からは大幅な削減です。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this.",
  "translatedText": "しかし、もちろんその裏返しとして、このようなパターンが発生するのは非常に珍しいということです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000.",
  "translatedText": "具体的には、各単語が答えとなる可能性が等しい場合、このパターンにヒ ットする確率は、58 を約 13,000 で割った値になります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers.",
  "translatedText": "もちろん、それらが同じように答えられるわけではありません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words.",
  "translatedText": "これらのほとんどは非常に曖昧で、疑わしい単語ですらあります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later.",
  "translatedText": "しかし、少なくともこのすべての最初のパスでは、それらはすべて 同じ可能性であると仮定し、後でそれを少し改良してみましょう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is, the pattern with a lot of information is, by its very nature, unlikely to occur.",
  "translatedText": "重要なのは、多くの情報を持つパターンは、その性質上、起こりにくいということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely.",
  "translatedText": "実際のところ、有益であるということは、その可能性は低いということです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where, of course, there's not a W in it.",
  "translatedText": "このオープニングで見られるより確率の高いパターンは、もちろんWは入っていないが、次のようなものだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y.",
  "translatedText": "E があるかもしれないし、A も R も Y もないかもしれません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches.",
  "translatedText": "この場合、一致する可能性は 1400 通りあります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see.",
  "translatedText": "すべてが同じ確率である場合、このパター ンになる確率は約 11% になります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative.",
  "translatedText": "したがって、最も可能性の高い結果は、最も情報が少ないものでもあります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see.",
  "translatedText": "ここでより全体的な視点を得るために、表示される可能性のあるさ まざまなパターンすべてにわたる確率の完全な分布を示します。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common.",
  "translatedText": "したがって、あなたが見ている各バーは、明らかにされる可能性のある色のパ ターンに対応しており、そのうち 3 ～ 5 番目の可能性があり、最も一 般的なものから最も一般的ではないものの順に左から右に編成されています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays.",
  "translatedText": "したがって、ここで最も一般的な可能性は、すべてが灰色になることです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time.",
  "translatedText": "それは約 14% の確率で起こります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here, where there's only 18 possibilities for what matches this pattern, that evidently look like this.",
  "translatedText": "このパターンにマッチする可能性は18個しかない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here.",
  "translatedText": "あるいは、もう少し左に行けば、おそらくここまで行けるかもしれません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you.",
  "translatedText": "さて、あなたに良いパズルをご紹介します。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them?",
  "translatedText": "W で始まり Y で終わり、どこかに R が含まれる英語の 3 つの単語は何ですか? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly.",
  "translatedText": "結局のところ、その答えは、くどい、虫食い、そして皮肉なものだった。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution.",
  "translatedText": "したがって、この単語が全体的にどの程度優れているかを判断するには、こ の分布から得られると予想される情報量を示す何らかの尺度が必要です。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score.",
  "translatedText": "各パターンを調べて、その発生確率と、そのパターンがどれだけ有益かを測定 するものを掛け合わせると、客観的なスコアが得られる可能性があります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches.",
  "translatedText": "さて、それが何であるべきかについて最初に直感するのは、一致の数かもしれません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches.",
  "translatedText": "平均一致数を減らしたいと考えています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer.",
  "translatedText": "しかし、その代わりに、私たちが情報に帰することが多い、より普遍的な尺度を使用したいと 思います。また、これらの 13,000 語のそれぞれに、それらが実際に答えであるかど うかについて異なる確率を割り当てると、より柔軟になる尺度を使用したいと考えています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but is really intuitive if we just look at examples.",
  "translatedText": "情報の標準的な単位はビットであり、ちょっとおかしな数式だが、例を見れば実に直感的だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information.",
  "translatedText": "あなたの可能性の空間を半分に減らすような観察があ る場合、それは少しの情報を持っていると言います。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about half of the five letter words have an S, a little less than that, but about half.",
  "translatedText": "この例では、可能性の空間はすべての可能な単語であり、5文字の単語の約半分がSを持つことがわかった。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information.",
  "translatedText": "したがって、この観察により、ちょっとした情報が得られるでしょう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information.",
  "translatedText": "代わりに、新しい事実がその可能性の空間を 4 分の 1 に切り 詰める場合、それは 2 ビットの情報を持っていると言います。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T.",
  "translatedText": "たとえば、これらの単語の約 4 分の 1 に T が含まれていることがわかりました。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth.",
  "translatedText": "観測によりその空間が 8 分の 1 に削減された 場合、それは 3 ビットの情報であると言います。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a sixteenth, five bits cuts it into a thirty second.",
  "translatedText": "4ビットは16分の1に、5ビットは30分の1になる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.88
 },
 {
  "input": "So now's when you might want to take a moment and pause and ask for yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence?",
  "translatedText": "では、ここで少し立ち止まり、自分自身に問いかけてみてはどうだろう。ビットの数を表す情報の公式は、発生確率の観点からどうなっているだろうか？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 534.96,
  "end": 542.98
 },
 {
  "input": "Well, what we're saying here is basically that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability.",
  "translatedText": "ここで私たちが言いたいのは、ビット数の半分を取るとき、それは 確率と同じことです。これは、ビット数の 2 乗が確率より 1 大きいと言っているのと同じことです。さらに整理すると、情報 は 2 を底とする 1 を確率で割った対数であると言えます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still where the information is the negative log base two of the probability.",
  "translatedText": "さらにもう1つ並べ替えると、情報が確率の負の対数の2乗になることもある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half.",
  "translatedText": "このように表現すると、初心者にとっては少し奇妙に見える かもしれませんが、実際には、自分の可能性を何回半減させ たかを尋ねるという非常に直感的なアイデアにすぎません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture?",
  "translatedText": "さて、疑問に思っている方は、私たちは単に楽しい言葉遊びをして いるだけだと思ったのですが、なぜ対数が登場するのでしょうか? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095.",
  "translatedText": "これがより優れた単元である理由の 1 つは、非常に起こりそうもない出来事について話 すのがはるかに簡単であり、これこれが発生する確率が 0 であると言うよりも、観測 値に 20 ビットの情報があると言う方がはるかに簡単であるためです。0000095。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together.",
  "translatedText": "しかし、この対数表現が確率論への非常に有用な追加であることが 判明したより実質的な理由は、情報が加算される方法にあります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information.",
  "translatedText": "たとえば、1 つの観測結果から 2 ビットの情報が得られ、スペースが 4 ビット減り、その後、Wordle での 2 番目の推測のような 2 番目 の観測により、さらに 3 ビットの情報が得られ、さらに 8 分の 1 に 削減された場合、 2 つを組み合わせると 5 ビットの情報が得られます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add.",
  "translatedText": "確率が増加するのと同じように、情報は追加されるのを好みます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with.",
  "translatedText": "したがって、大量の数値を加算する期待値のような領域 に入るとすぐに、ログのおかげで扱いやすくなります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for weary and add another little tracker on here, showing us how much information there is for each pattern.",
  "translatedText": "wearyのディストリビューションに戻って、ここに小さなトラッカーを追加し、各パターンの情報量を表示してみよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain.",
  "translatedText": "ここで注目していただきたいのは、可能性の高いパターンに到達する確率が高く なるほど、情報が少なくなり、得られるビットも少なくなるということです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information.",
  "translatedText": "この推測の質を測る方法は、この情報の期待値を取ることである。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 643.5,
  "end": 648.02
 },
 {
  "input": "When we go through each pattern, we say how probable is it and then we multiply that by how many bits of information do we get.",
  "translatedText": "それぞれのパターンを見て、どれくらいの確率でそうなるかを言い、それに何ビットの情報を掛け合わせる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 648.42,
  "end": 654.06
 },
 {
  "input": "And in the example of weary, that turns out to be 4.9 bits.",
  "translatedText": "ウィアリーの例では、4.9ビットになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times.",
  "translatedText": "したがって、平均すると、この冒頭の推測から得られる情報は、可能性 の空間を約 5 回半分に切り分けるのと同じくらい優れています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like slate.",
  "translatedText": "対照的に、より高い期待情報価値を持つ推測の例は、スレートのようなものだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case, you'll notice the distribution looks a lot flatter.",
  "translatedText": "この場合、分布がかなり平坦に見えることに気づくだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information.",
  "translatedText": "特に、最も可能性の高いすべての灰色の発生確率は約 6% しかないため、少 なくとも明らかに 3 が得られることになります。9ビットの情報。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that.",
  "translatedText": "しかし、これは最低限のことであり、通常はそれよりも優れたものが得られるでしょう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8.",
  "translatedText": "この数字を計算して関連するすべての用語を合計すると 、平均的な情報は約 5 であることがわかります。8. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with weary, your space of possibilities will be about half as big after this first guess, on average.",
  "translatedText": "つまり、疲れたときとは対照的に、この最初の推測の後では、可能性のスペースは平均して約半分になる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity.",
  "translatedText": "実は、この情報量の期待値の名前については面白い話があります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "You see, information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science.",
  "translatedText": "情報理論は、1940 年代にベル研究所で働いていたクロード シャノンによって 開発されましたが、彼はまだ発表されていないアイデアのいくつかについて、当時 の知的巨人で非常に著名なジョン フォン ノイマンと話し合っていました。数学と 物理学、そしてコンピューターサイエンスになりつつあったものの始まりでした。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well, you should call it entropy, and for two reasons.",
  "translatedText": "この情報量の期待値に対して、フォン・ノイマンは良い名前がないと言ったとされるが、それには2つの理由がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage.",
  "translatedText": "まず第一に、不確実性関数はその名前で統計力学で使用されているため、すでに名 前が付いています。そして第二に、そしてさらに重要なことに、エントロピーが 実際に何であるか誰も知りません。したがって、議論では常に利点があります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design.",
  "translatedText": "したがって、名前が少し謎めいているように見えても、 この話を信じられるとしても、それは一種の仕様です。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess.",
  "translatedText": "また、物理学の熱力学第 2 法則すべてとの関係について疑 問に思っているなら、間違いなく関連性がありますが、その 起源において、シャノンは純粋な確率論を扱っていただけであ り、ここでの目的のために、単語のエントロピーについては 、特定の推測の期待される情報値を考えてほしいだけです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously.",
  "translatedText": "エントロピーは、2 つのものを同時に測定すると考えることができます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution?",
  "translatedText": "まず1つ目は、分布がどの程度フラットなのかということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be.",
  "translatedText": "分布が均一に近づくほど、エントロピーは高くなります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy.",
  "translatedText": "私たちの場合、一様分布の場合、合計 3 から 5 までのパターンがあり、それらのいずれかを 観察すると、情報ログの底が 3 から 5 までの 2 になり、これはたまたま 7 になり ます。92 なので、これがこのエントロピーの絶対最大値になります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place.",
  "translatedText": "しかし、エントロピーは、そもそもどれだけの可 能性があるかを示す一種の尺度でもあります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits.",
  "translatedText": "たとえば、考えられるパターンが 16 個しかなく、それぞれの可能性が等しい単語がた またまあった場合、このエントロピー、つまり期待される情報は 4 ビットになります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits.",
  "translatedText": "しかし、64 個の考えられるパターンがあり、それらがすべて同じ確率で ある別の単語がある場合、エントロピーは 6 ビットになるでしょう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes.",
  "translatedText": "したがって、6 ビットのエントロピーを持つ分布を実際に目にした場合、 それは、同じ確率の結果が 64 個存在するのと同じくらい、これから起 こることには大きな変動と不確実性があると言っているようなものです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this.",
  "translatedText": "Wurtelebot での最初のパスでは、基本的にこれを行うだけでした。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the different possible guesses that you could have, all 13,000 words, it computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns that you might see for each one, and then it picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible.",
  "translatedText": "考えられるすべての推測 (13,000 語すべて) を調べ、各単語のエ ントロピー、より具体的には、表示される可能性のあるすべてのパターンに わたる分布のエントロピーを単語ごとに計算し、最も高いものを選択します 。それはあなたの可能性の空間を可能な限り切り詰める可能性があります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses.",
  "translatedText": "ここでは最初の推測についてのみ話しましたが、次の いくつかの推測についても同じことが起こります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words.",
  "translatedText": "たとえば、最初の推測について何らかのパターンがあり、それに一致す るものに基づいて候補となる単語の数が制限されることがわかったら、 その小さな単語のセットに関して同じゲームをプレイするだけです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy.",
  "translatedText": "提案された 2 番目の推測では、より限定された単語のセットから発生す る可能性のあるすべてのパターンの分布を調べ、13,000 の可能性 すべてを検索し、そのエントロピーを最大化するパターンを見つけます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins.",
  "translatedText": "これが実際にどのように機能するかを示すために、余白にこの分析のハイライトを 示す、私が書いた Wurtele の小さな変形版を取り出してみましょう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "So after doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information.",
  "translatedText": "エントロピーの計算をすべて行った後、右側では、どれが最も期待される情報量が多いかを示している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch.",
  "translatedText": "少なくとも現時点でのトップの答えは、後ほど詳しく説明しますが、Tar es です。つまり、もちろん、レンゲ、最も一般的なレンゲのことです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got given this particular pattern.",
  "translatedText": "スレートが好きなので、スレートの推薦を無視してスレートにするかもしれないが、ここで推測をするたびに、予想された情報量を見ることができる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that.",
  "translatedText": "つまり、ここでは少し不運だったようです。5 を獲得することが期待されていました。8 ですが 、たまたまそれ以下のものが入手できました。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now.",
  "translatedText": "そして左側には、私たちが今いる場所で考えられ るさまざまな単語がすべて表示されています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment.",
  "translatedText": "青いバーは各単語がどの程度の確率で考えられるかを示しているため、現時点では 各単語が出現する可能性が等しいと仮定していますが、これはすぐに修正します。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities.",
  "translatedText": "そして、この不確実性の測定により、考えられる単語全体にわたるこ の分布のエントロピーがわかります。これは、現時点では一様分布で あるため、可能性の数を数える不必要に複雑な方法にすぎません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities.",
  "translatedText": "たとえば、2 の 13 乗を取るとします。66、それは約 13,000 の可能性があるはずです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "Um, a little bit off here, but only because I'm not showing all the decimal places.",
  "translatedText": "少しずれているが、小数点以下が表示されていないだけだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute.",
  "translatedText": "現時点では、それは冗長で、物事が過度に複雑であるように感じるかもしれま せんが、両方の数値を取得することがなぜ便利であるかはすぐにわかります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Raman, which again just really doesn't feel like a word.",
  "translatedText": "ここでは、2番目に推測されるラマンのエントロピーが最も高いことを示唆しているように見える。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here I'm going to go ahead and type in Rains.",
  "translatedText": "だから、道徳的に優位に立つために、私は先にレインズと入力するつもりだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky.",
  "translatedText": "そしてまた少し不運だったようです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information.",
  "translatedText": "私たちは4を期待していました。3 ビットありますが、3 つしかありません。39ビットの情報。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities.",
  "translatedText": "つまり、55 の可能性が考えられます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means.",
  "translatedText": "そしてここでは、それが何を意味するにせよ、実際にそれが示唆 しているもの、つまりコンボに従うことになるかもしれません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And, okay, this is actually a good chance for a puzzle.",
  "translatedText": "そして、これはパズルのチャンスでもある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information.",
  "translatedText": "このパターンでは 4 が得られることがわかります。7ビットの情報。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty.",
  "translatedText": "しかし、左側では、そのパターンが見える前に 5 つありました。78 ビットの不確実性。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities?",
  "translatedText": "それで、あなたへのクイズですが、残りの可能性の数については何を意味しますか? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well it means that we're reduced down to 1 bit of uncertainty, which is the same thing as saying that there's 2 possible answers.",
  "translatedText": "つまり、不確定要素が1つしかないということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice.",
  "translatedText": "それは五分五分の選択だ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss.",
  "translatedText": "ここからは、あなたも私もどちらの単語がより一般的である かを知っているので、答えは深淵であることがわかります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that.",
  "translatedText": "しかし、現時点で書かれているように、プログラムはそれを知りません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it.",
  "translatedText": "したがって、可能性が 1 つだけ残されるまで、できるだけ多 くの情報を取得しようと試み続け、その後、それを推測します。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy, but let's say we call this version 1 of our wordle solver, and then we go and run some simulations to see how it does.",
  "translatedText": "だから、より良い終盤戦略が必要なのは明らかだが、これをワードル・ソルバーのバージョン1と呼ぶことにしよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game.",
  "translatedText": "つまり、これがどのように機能しているかというと、考えられるすべての単語ゲームを実行しているということです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers.",
  "translatedText": "実際の Wordle の回答である 2315 語すべてを調べます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set.",
  "translatedText": "基本的にはそれをテストセットとして使用します。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice.",
  "translatedText": "そして、単語がどれだけ一般的であるかを考慮せず、ただ 1 つの選択肢に行き 着くまで、途中の各ステップで情報を最大化しようとするこの単純な方法です。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124.",
  "translatedText": "シミュレーションが終了するまでに、平均スコアは約 4 になります。124. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expect it to do worse.",
  "translatedText": "正直なところ、もっと悪い結果を期待していた。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4.",
  "translatedText": "しかし、ワードルをプレイしている人は、通常は 4 で取得できると言います。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can.",
  "translatedText": "本当の課題は、3 つをできるだけ多く獲得することです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3.",
  "translatedText": "スコア 4 とスコア 3 の間ではかなり大きな差があります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low-hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that.",
  "translatedText": "ここでは、その単語が一般的であるかどうかをどうにかして組み込むことが重要である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language, and I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset.",
  "translatedText": "私がこの問題に取り組んだ方法は，英語の全単語の相対頻度のリストを取得することであった．",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words.",
  "translatedText": "たとえば、最も一般的な単語から最も一般的ではない 単語まで並べ替えると、見ていてとても楽しいです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5-letter words in the English language.",
  "translatedText": "これらは明らかに、英語の中で最も一般的な5文字の単語である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common.",
  "translatedText": "というか、これらは8番目に多いものです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there.",
  "translatedText": "最初にどれがあり、その後にあそことあそこがあります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common.",
  "translatedText": "first 自体は first ではなく 9th であり、これらの他の単語がよ り頻繁に出現する可能性があることは理にかなっていますが、first の後の単語 は after、where、そしてそれらの単語は少しだけ一般的ではありません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency, because for example which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely.",
  "translatedText": "例えば、このデータセットではwhichは0.002のスコアしか与えられていないが、braidという単語はある意味で1000倍も可能性が低いからだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering, so we want more of a binary cutoff.",
  "translatedText": "しかし、どちらも十分に一般的な単語であり、考慮する価値があることはほぼ間違いない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty.",
  "translatedText": "私がこれに取り組んだ方法は、このソートされた単語のリスト全体を取得し、 それを X 軸上に配置し、シグモイド関数を適用することです。これは、 出力が基本的にバイナリである関数を作成する標準的な方法です。0 か 1 のどちらかですが、その不確実性の領域の間で平滑化が行われます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis.",
  "translatedText": "つまり、基本的に、最終リストに含まれる各単語に割り当てる確率は 、x 軸上のどこに位置する上記のシグモイド関数の値になります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff.",
  "translatedText": "これは明らかにいくつかのパラメータに依存します。たとえば、これらの単語が占める x 軸上のスペースの幅によって、1 から 0 への降下がどの程度徐々にまたは急激に変 化するかが決まり、単語を左から右のどこに配置するかによってカットオフが決まります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "And to be honest the way I did this was kind of just licking my finger and sticking it into the wind.",
  "translatedText": "正直に言うと、私がこれを行った方法は、指をなめて風に突き刺すだけでした。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff.",
  "translatedText": "私はソートされたリストを調べて、これらの単語の約半分が最 終的な答えになる可能性が高いと判断できる範囲を見つけよう としました。そして、それをカットオフとして使用しました。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Now once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement.",
  "translatedText": "さて、このように単語全体に分布があると、エントロピーが実に有用な測定となる別の状況が生まれる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were other and nails, and we end up with a situation where there's four possible words that match it.",
  "translatedText": "例えば、ゲームをしていて、私が以前使っていた \"other \"と \"nails \"というオープナーから始めて、それにマッチする単語が4つある状況になったとしよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely, let me ask you, what is the entropy of this distribution?",
  "translatedText": "この分布のエントロピーはどれくらいだろうか？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2.",
  "translatedText": "さて、これらの可能性のそれぞれに関連付けられた情報は、それぞれが 1 と 4 であり、それが 2 であるため、4 の底を 2 とする対数になります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "2 bits of information, 4 possibilities.",
  "translatedText": "2ビットの情報、4つの可能性。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good.",
  "translatedText": "すべてとても順調です。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than 4 matches?",
  "translatedText": "しかし、実は4試合以上あると言ったらどうだろう？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it.",
  "translatedText": "実際、完全な単語リストに目を通すと、それに一致する単語が 16 個あります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure.",
  "translatedText": "しかし、私たちのモデルでは、他の 12 単語が実際に最終的な答えになる確率は非 常に低く、非常に曖昧であるため、1000 分の 1 程度であると仮定します。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution?",
  "translatedText": "さて、この分布のエントロピーはいくらでしょうか? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before.",
  "translatedText": "ここでエントロピーが純粋に一致の数を測定している場合、それは 16 の底 2 の対数のようなものになると予想されるかもしれません。こ れは 4 となり、以前よりも 2 ビット多くの不確実性が生じます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before.",
  "translatedText": "しかし、もちろん、実際の不確実性は以前とそれほど変わりません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example.",
  "translatedText": "これらの本当にあいまいな 12 の単語があるからといって、たとえば、最 終的な答えが魅力であると知っても、それほど驚くべきことではありません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits.",
  "translatedText": "そこで実際に計算してみると、それぞれの発生確率に対応する情報を足したものは2.11ビットになる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "Just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it.",
  "translatedText": "ただ、基本的には2ビットで、基本的にはこの4つの可能性だが、可能性の低い事象があるため、もう少し不確実性がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson.",
  "translatedText": "ズームアウトすると、これが Wordle が情報理 論のレッスンに最適な例となる理由の 1 つです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy.",
  "translatedText": "エントロピーについては、これら 2 つの異なる感覚のアプリケーションがあります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words we have possible.",
  "translatedText": "最初のものは、与えられた推測から得られる期待される情報を教えてくれるもので、2番目のものは、可能性のあるすべての言葉の中から残りの不確実性を測定することができるかというものだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation.",
  "translatedText": "そして、強調しておきたいのは、推測の期待情報を調べる最初のケースでは、単語の重 み付けが不均等になると、それがエントロピーの計算に影響を与えるということです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words.",
  "translatedText": "たとえば、Weary に関連する分布について以前に検討し たのと同じケースを取り出してみましょう。ただし、今回はす べての可能な単語にわたって不均一な分布を使用しています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well.",
  "translatedText": "それでは、それをうまく説明している部分がここに見つかるかどうか見てみましょう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here, this is pretty good.",
  "translatedText": "よし、これはなかなかいい。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it.",
  "translatedText": "ここでは、ほぼ同じ確率の 2 つの隣接するパターンがありますが、そのうちの 1 つは、それに一致する可能性のある単語が 32 個あると言われています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them.",
  "translatedText": "それらが何であるかを確認してみると、これらは 32 個であり、 目をざっと眺めると、どれも非常にありそうもない単語ばかりです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches.",
  "translatedText": "もっともらしい答えのようなものを見つけるのは難しく、もしかしたら雄叫びかもしれないが、分布の中でちょうど可能性が高いと思われる隣のパターンを見てみると、マッチする可能性は8つしかないと言われている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1386.66
 },
 {
  "input": "So a quarter as many matches, but it's about as likely.",
  "translatedText": "だから試合数は4分の1だが、可能性は同じくらいだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1386.88,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why.",
  "translatedText": "それらの一致を調べてみると、その理由がわかります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers like ring or wrath or raps.",
  "translatedText": "その中には、リングや怒り、ラップなど、実際にもっともらしい答えもある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version two of the Wordlebot here.",
  "translatedText": "そのすべてをどのように組み込んでいるかを説明するために、ここでワードルボットのバージョン2を出してみよう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1402.3
 },
 {
  "input": "And there are two or three main differences from the first one that we saw.",
  "translatedText": "そして、私たちが見た最初のものとの主な違いは2つか3つある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1402.56,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer.",
  "translatedText": "まず、先ほど述べたように、これらのエントロピー、つまり情報の期待 値を計算する方法では、特定の単語が実際に答えとなる確率を組み込 んだ、パターン全体にわたるより洗練された分布を使用しています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number one, though the ones following are a bit different.",
  "translatedText": "その結果、涙が1位であることに変わりはない。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table.",
  "translatedText": "第 2 に、上位の候補をランク付けするときに、 各単語が実際の答えである確率のモデルを保持し、 それを決定に組み込むようになります。テーブル。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives.",
  "translatedText": "繰り返しますが、私たちは機械に生活を支配させることはできないので、その推奨を無視します。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches.",
  "translatedText": "そして、ここでもう 1 つ異なる点があることを言及する必要があると思います。左側 は、不確実性の値、ビット数が、一致の可能性の数と重複するだけではなくなりました。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which would be a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes.",
  "translatedText": "8.02に2を足して計算すると、256の少し上、259になると思う。つまり、このパターンに一致する単語が全部で526個あるとしても、その不確実性は、259個の同じ可能性のある結果がある場合の不確実性に近いということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this.",
  "translatedText": "このように考えることができます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borks is not the answer, same with yorts and zorl and zorus.",
  "translatedText": "ボークスが答えでないことは分かっている。ヨーツもゾールもゾルスも同じだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1474.66
 },
 {
  "input": "So it's a little less uncertain than it was in the previous case.",
  "translatedText": "だから、以前のケースよりは少し不確実性が低くなっている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1474.66,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller.",
  "translatedText": "このビット数は小さくなります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here.",
  "translatedText": "そして、ゲームをプレイし続けると、ここで説明したいこ とと適切ないくつかの推測を加えてこれを洗練させます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy.",
  "translatedText": "4 番目の推測では、その上位の候補を見てみると、もはや エントロピーを最大化するだけではないことがわかります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words.",
  "translatedText": "つまり、現時点では技術的には 7 つの可能性がありま すが、意味のあるチャンスがあるのは寮と言葉だけです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values that strictly speaking would give more information.",
  "translatedText": "そして、厳密に言えば、より多くの情報を与えるであろう他のすべての値よりも、この2つの値を選択することをランク付けしているのがわかるだろう。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect.",
  "translatedText": "初めてこれを行ったとき、これら 2 つの数値を合計して各推測の 質を測定しました。実際、これは予想以上にうまく機能しました。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic.",
  "translatedText": "しかし、システマチックな感じはしなかった。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1515.9
 },
 {
  "input": "And I'm sure there's other approaches people could take.",
  "translatedText": "他のアプローチもあるはずだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1516.1,
  "end": 1517.88
 },
 {
  "input": "But here's the one I landed on.",
  "translatedText": "しかし、ここにたどり着いた。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1517.9,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that.",
  "translatedText": "この場合の単語のように、次の推測の見通しを考慮している場合、本 当に関心があるのは、それを行った場合のゲームの予想スコアです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to.",
  "translatedText": "そして、その期待スコアを計算するために、単語が実際の答えである確 率はいくらかを言います。現時点では 58% と説明されています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be four.",
  "translatedText": "58％の確率で、この試合のスコアは4となる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of one minus that 58%, our score will be more than that four.",
  "translatedText": "そして、その58％から1を引いた確率で、スコアは4以上になる。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point.",
  "translatedText": "それ以上はわかりませんが、その時点に到達したときにどの程度の不確 実性が存在する可能性があるかに基づいて推定することはできます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment, there's 1.44 bits of uncertainty.",
  "translatedText": "具体的には、現時点では1.44ビットの不確実性がある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits.",
  "translatedText": "単語を推測すると、得られる期待情報が 1 であることがわかります。27ビット。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens.",
  "translatedText": "したがって、言葉を推測した場合、この違いは、それが起こった 後にどの程度の不確実性が残る可能性があるかを表しています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score.",
  "translatedText": "必要なのは、この不確実性を期待されるスコアに関連付ける、 ある種の関数 (ここでは f と呼んでいます) です。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version one of the bot to say, hey, what was the actual score after various points with certain very measurable amounts of uncertainty?",
  "translatedText": "そして、その方法は、ボットのバージョン1に基づいて過去の試合のデータをプロットして、ある非常に測定可能な不確定要素を含む様々なポイントの後、実際のスコアはどうだったかと言うものだった。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games, after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer.",
  "translatedText": "例えば、8.7前後の値の上にあるこれらのデータポイントは、8.7ビットの不確実性があった時点で、最終的な答えを出すのに2回の推測が必要だったということを表している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games, it took three guesses.",
  "translatedText": "他の試合では3回当てられた。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1600.66
 },
 {
  "input": "For other games, it took four guesses.",
  "translatedText": "他の試合では4回だった。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1600.82,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring.",
  "translatedText": "ここで左にシフトすると、ゼロを超えるすべての点は、不確実性が ゼロの場合、つまり可能性が 1 つしかない場合、必要な推測の 数は常に 1 つだけであり、安心できることを示しています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses, and so on and so forth here.",
  "translatedText": "不確定要素が1つでもあると、つまり本質的に可能性が2つに絞られると、さらに1つの推測が必要になることもあれば、2つの推測が必要になることもある。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages.",
  "translatedText": "このデータを視覚化するもう少し簡単な方法は、データをまとめて平均を取ることかもしれません。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example, this bar here is saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5.",
  "translatedText": "例えば、この棒グラフは、1ビットの不確実性があるすべてのポイントの中で、平均して約1.5回の新たな推測が必要だったことを示している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here is saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward.",
  "translatedText": "このバーは、ある時点で不確実性が4ビットを少し上回った、つまり16の可能性に絞られたようなさまざまなゲームの中で、平均してその時点から2回より少し多い推測を必要とすることを示している。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this.",
  "translatedText": "ここからは、これに合理的と思われる関数を当てはめるために回帰を実行しました。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember, the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be.",
  "translatedText": "覚えておいてほしいのは、「単語から得られる情報が多ければ多いほど、期待されるスコアは低くなる」という直感を定量化するためだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So, with this as version 2.0, if we go back and run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do?",
  "translatedText": "では、これをバージョン2.0として、同じシミュレーションセットを実行し、2315通りのワードルの答えすべてと対戦させてみると、どうなるだろうか？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version, it's definitely better, which is reassuring.",
  "translatedText": "最初のバージョンとは対照的に、間違いなく良くなっている。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done, the average is around 3.6.",
  "translatedText": "平均は3.6である。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1686.18
 },
 {
  "input": "Although unlike the first version, there are a couple times that it loses, and requires more than six in this circumstance.",
  "translatedText": "しかし、最初のバージョンと違って、何度か負けることがあり、この状況では6人以上を必要とする。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1686.54,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information.",
  "translatedText": "おそらく、情報を最大化するのではなく、実際に目標を達成する ためにトレードオフが発生する場合があるからだと思われます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6?",
  "translatedText": "では、3よりもうまくできるでしょうか。6? ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can.",
  "translatedText": "間違いなくできます。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now, I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model.",
  "translatedText": "さて、私は冒頭で、ワードルのモデルを構築する方法に、ワードルの真の解答リストを取り入れないようにするのが最も楽しいと述べた。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43.",
  "translatedText": "しかし、それを組み込んだ場合、得られる最高のパフォーマンスは 3 程度でした。43. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that.",
  "translatedText": "したがって、この事前分布を選択するために単語頻度データを使用するだけではなく、より洗練されたもの にしようとすると、この 3.おそらく 43 が、これでどれだけうまくできる か、少なくともどれくらいうまくできるかという最大値を示しています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one.",
  "translatedText": "その最高のパフォーマンスは、基本的には私がここで話してきたア イデアを使用しているだけですが、期待される情報を 1 歩では なく 2 歩前進して検索するなど、さらに一歩進んだものです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is.",
  "translatedText": "当初はそれについてもっと話すつもりでしたが、実際 にはかなり長くなってしまったことに気づきました。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least, it's looking like Crane is the best opener.",
  "translatedText": "ひとつだけ言えるのは、この2段階の検索を行った後、上位候補でいくつかのサンプルシミュレーションを行った結果、少なくとも私にとっては、今のところクレインが最高の開幕投手であるように見えるということだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed?",
  "translatedText": "誰が予想したでしょうか？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits.",
  "translatedText": "また、真の Wordle リストを使用して可能性の空間を決定する場 合、最初の不確実性は 11 ビットをわずかに超える値になります。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits.",
  "translatedText": "そして、ブルートフォース・サーチの結果、最初の2回の推測の後に予想される最大情報量は約10ビットであることが判明した。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty.",
  "translatedText": "これは、最良のシナリオでは、最初の 2 つの推測の後、完全に最適なプレ イを行った場合、約 1 ビットの不確実性が残ることを示唆しています。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses.",
  "translatedText": "これは、可能な推測が 2 つに絞られるのと同じです。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "But I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as three, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail.",
  "translatedText": "というのも、利用可能な単語数では、たった2ステップで十分な情報を得る余地がなく、毎回確実に3番目の枠に答えを入れることを保証できないからだ。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]