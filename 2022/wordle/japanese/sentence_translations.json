[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy. ",
  "translatedText": "Wurdle というゲームは、ここ 1 ～ 2 か月でかなり話題になりました。数学の授業の 機会を見逃す人はいません。このゲームは、情報理 論、特にエントロピーとして知られるトピック。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could. ",
  "translatedText": "多くの人と同じように、私もパズルに夢中になりました。また、 多くのプログラマーと同じように、私もゲームを可能な限り最適 にプレイするアルゴリズムを作成することに夢中になりました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy. ",
  "translatedText": "ここで私がやろうと思ったのは、アルゴリズム全体がこのエン トロピーの考え方に中心を置いているため、そのプロセスの一 部を話し、それに使用された数学の一部を説明することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle? ",
  "translatedText": "まず最初に、Wurdle について聞いたことがない場合のために説明します。Wurdle とは何ですか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us. ",
  "translatedText": "そして、ここでゲームのルールを確認しながら一石二鳥にするために、 今後の展開についてもプレビューさせていただきます。これは、基本的 にゲームをプレイするための小さなアルゴリズムを開発することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does. ",
  "translatedText": "私は今日の Wurdle を実行していませんが、これは 2 月 4 日なので、ボットがどのように動作するか見てみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess. ",
  "translatedText": "Wurdle の目的は、謎の 5 文字の単語を推測す ることであり、推測する機会が 6 回与えられます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane. ",
  "translatedText": "たとえば、Wurdle ボットは、推測クレーンから始めることを提案します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer. ",
  "translatedText": "推測するたびに、その推測が真の答えにどの程 度近づいているかに関する情報が得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer. ",
  "translatedText": "ここで灰色のボックスは、実際の答えに C が存在しないことを示しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position. ",
  "translatedText": "黄色のボックスは R があることを示していますが、その位置にありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position. ",
  "translatedText": "緑色のボックスは、秘密の単語に A があり、そ れが 3 番目の位置にあることを示しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E. ",
  "translatedText": "そして、NもEもありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information. ",
  "translatedText": "それでは、早速入って、Wurdle ボットにその情報を伝えましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey. ",
  "translatedText": "クレーンから始まり、灰色、黄色、緑色、灰色、灰色になりました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time. ",
  "translatedText": "現在表示されているすべてのデータについては心配しないでください。それについては、やがて説明します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick. ",
  "translatedText": "しかし、2番目に選ぶ一番の提案は「クソ」です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess. ",
  "translatedText": "推測は実際の 5 文字の単語である必要がありますが、 ご覧のとおり、実際に推測できる内容はかなり自由です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick. ",
  "translatedText": "この場合、shtick を試します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good. ",
  "translatedText": "さて、状況はかなり良いようです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R. ",
  "translatedText": "S と H を押すと、最初の 3 文字がわかり、R があることがわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something. ",
  "translatedText": "そして、それは SHA 何か R 、または SHA R 何かのようなものになるでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp. ",
  "translatedText": "そして、Wurdle ボットは、シャードかシャー プの 2 つの可能性だけを知っているようです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a toss up between them at this point, so I guess probably just because it's alphabetical it goes with shard. ",
  "translatedText": "現時点ではどちらを選択するか迷っているようなものなので、おそらく アルファベット順というだけでシャードに当てはまるのだと思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer. ",
  "translatedText": "実際の答えはどれでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.22,
  "end": 132.86
 },
 {
  "input": "So we got it in three. ",
  "translatedText": "それで、3つで解決しました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 132.96,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle four is par and three is birdie. ",
  "translatedText": "それがいいのかどうか疑問に思っているなら、私が聞いたある人のフレーズで は、ワールドルでは 4 つがパー、3 つがバーディだということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy. ",
  "translatedText": "これは非常に適切な例えだと思います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy. ",
  "translatedText": "4 つを獲得するには、一貫してゲームに取り組む必要がありますが、それは確かにクレイジーではありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great. ",
  "translatedText": "しかし、3つになると、本当に素晴らしい気分になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot. ",
  "translatedText": "それで、もしあなたがそれに興味がないなら、私がここでやりたいことは、Wurdle ボットへのアプローチ方法についての私の思考プロセスを最初から説明することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson. ",
  "translatedText": "先ほども言いましたが、実際には、これは情報理論のレッスンの言い訳です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy. ",
  "translatedText": "主な目標は、情報とは何か、エントロピーとは何かを説明することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language. ",
  "translatedText": "これに取り組むにあたって私が最初に考えたのは、英語におけ るさまざまな文字の相対的な頻度を調べてみることでした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters? ",
  "translatedText": "そこで私は、これらの最も頻繁に使用される文字の多くに当てはまる 冒頭の推測または冒頭の推測のペアはあるだろうか、と考えました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails. ",
  "translatedText": "そして、私がとても気に入っていたのは、ネイルに続いて他のことをすることでした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good. ",
  "translatedText": "文字を打つと、緑か黄色が出ると、いつで も気分が良くなる、という考え方です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.76,
  "end": 187.52
 },
 {
  "input": "It feels like you're getting information. ",
  "translatedText": "情報が入ってくる感じです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.52,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information since it's pretty rare to find a word that doesn't have any of these letters. ",
  "translatedText": "ただし、このような場合、たとえヒットせず、常にグレーが表示 されたとしても、これらの文字を含まない単語を見つけること は非常にまれであるため、それでも多くの情報が得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters. ",
  "translatedText": "しかし、それでも、これはあまり体系的とは思えません 。たとえば、文字の順序は考慮されていないからです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail? ",
  "translatedText": "カタツムリを入力できるのに、なぜネイルを入力するのでしょうか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end? ",
  "translatedText": "最後にSがついたほうがいいでしょうか？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure. ",
  "translatedText": "よくわかりません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y. ",
  "translatedText": "さて、私の友人は、「疲れた」という単語で始めるのが好きだと言いました。それ には、W や Y などの珍しい文字が含まれているので、ちょっと驚きました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener. ",
  "translatedText": "しかし、おそらくそれはより良いオープナーであるかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess? ",
  "translatedText": "潜在的な推測の質を判断するために与えることができ る、ある種の定量的なスコアはあるのでしょうか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up. ",
  "translatedText": "ここで、考えられる推測をランク付けする方法を準備するために、戻って 、ゲームがどのように正確に設定されているかを少し明確にしましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long. ",
  "translatedText": "つまり、有効な推測として入力できる単語のリストがあ り、その長さはわずか約 13,000 単語です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble. ",
  "translatedText": "しかし、よく見てみると、本当に珍しいものがたくさんあります。頭やアリ、A RG など、スクラブル ゲームで家族の口論を引き起こすような言葉です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word. ",
  "translatedText": "しかし、ゲームの雰囲気は、答えが常にかなり一般的な単語になるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers. ",
  "translatedText": "そして実際、答えとして考えられる約 2300 語のリストがもう 1 つあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creator's girlfriend, which is kind of fun. ",
  "translatedText": "これは人間が厳選したリストで、特にゲームクリエイターのガールフ レンドによるものだと思いますが、これはちょっと楽しいですね。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving Wordle that doesn't incorporate previous knowledge about this list. ",
  "translatedText": "しかし、私がやりたいのは、このプロジェクトの課題は、このリストに関する事前の知識を組み 込まずに Wordle を解決するプログラムを作成できるかどうかを確認することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list. ",
  "translatedText": "まず、このリストには載っていない、よく使わ れる 5 文字の単語がたくさんあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play Wordle against anyone, not just what happens to be the official website. ",
  "translatedText": "したがって、もう少し回復力があり、公式 Web サイトだけでなく、誰と でも Wordle を対戦できるプログラムを作成する方がよいでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also the reason that we know what this list of possible answers is, is because it's visible in the source code. ",
  "translatedText": "また、この考えられる答えのリストが何であるかを私たちが知って いる理由は、それがソース コードに表示されているからです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day. ",
  "translatedText": "ただし、ソース コード内でそれが表示されるのは 、その日その日で得られる答えの特定の順序です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 292.8
 },
 {
  "input": "So you could always just look up what tomorrow's answer will be. ",
  "translatedText": "したがって、いつでも明日の答えを調べることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.06,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating. ",
  "translatedText": "したがって、リストの使用には不正行為である意味があることは明らかです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words. ",
  "translatedText": "そして、より興味深いパズルとより豊かな情報理論のレッスンを実現す るのは、代わりに一般的な単語の相対頻度などのより普遍的なデータ を使用して、より一般的な単語を好むという直感を捉えることです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess? ",
  "translatedText": "では、これら 13,000 の可能性の中から、冒頭の推測をどのように選択すればよいのでしょうか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality? ",
  "translatedText": "たとえば、友人が疲れたプロポーズをした場合、その質をどのように分析すべきでしょうか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W. ",
  "translatedText": "まあ、彼がその可能性の低い W が好きだと言ったのは、その W が当た ったらどれだけ気持ちいいかというロングショットの性質が好きだからです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern. ",
  "translatedText": "たとえば、最初に明らかにされたパターンが次のようなものであった場合、この巨大 な辞書にはそのパターンに一致する単語が 58 語しかないことがわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000. ",
  "translatedText": "13,000からは大幅な削減です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this. ",
  "translatedText": "しかし、もちろんその裏返しとして、このようなパターンが発生するのは非常に珍しいということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000. ",
  "translatedText": "具体的には、各単語が答えとなる可能性が等しい場合、このパターンにヒ ットする確率は、58 を約 13,000 で割った値になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers. ",
  "translatedText": "もちろん、それらが同じように答えられるわけではありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words. ",
  "translatedText": "これらのほとんどは非常に曖昧で、疑わしい単語ですらあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later. ",
  "translatedText": "しかし、少なくともこのすべての最初のパスでは、それらはすべて 同じ可能性であると仮定し、後でそれを少し改良してみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is the pattern with a lot of information is by its very nature unlikely to occur. ",
  "translatedText": "重要なのは、情報量が多いパターンはその性質上、発生する可能性が低いということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely. ",
  "translatedText": "実際のところ、有益であるということは、その可能性は低いということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where of course there's not a W in it. ",
  "translatedText": "このオープニングで見られる可能性の高いパターンは、次の ようなものです。もちろん、中には W がありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y. ",
  "translatedText": "E があるかもしれないし、A も R も Y もないかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches. ",
  "translatedText": "この場合、一致する可能性は 1400 通りあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see. ",
  "translatedText": "すべてが同じ確率である場合、このパター ンになる確率は約 11% になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative. ",
  "translatedText": "したがって、最も可能性の高い結果は、最も情報が少ないものでもあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see. ",
  "translatedText": "ここでより全体的な視点を得るために、表示される可能性のあるさ まざまなパターンすべてにわたる確率の完全な分布を示します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common. ",
  "translatedText": "したがって、あなたが見ている各バーは、明らかにされる可能性のある色のパ ターンに対応しており、そのうち 3 ～ 5 番目の可能性があり、最も一 般的なものから最も一般的ではないものの順に左から右に編成されています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays. ",
  "translatedText": "したがって、ここで最も一般的な可能性は、すべてが灰色になることです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time. ",
  "translatedText": "それは約 14% の確率で起こります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here where there's only 18 possibilities for what matches this pattern that evidently look like this. ",
  "translatedText": "そして、推測するときに期待しているのは、このロングテールのどこ かに行き着くということです。たとえば、このパターンに一致する可 能性が 18 個しかなく、明らかに次のように見える場所です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here. ",
  "translatedText": "あるいは、もう少し左に行けば、おそらくここまで行けるかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you. ",
  "translatedText": "さて、あなたに良いパズルをご紹介します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them? ",
  "translatedText": "W で始まり Y で終わり、どこかに R が含まれる英語の 3 つの単語は何ですか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly. ",
  "translatedText": "結局のところ、その答えは、くどい、虫食い、そして皮肉なものだった。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution. ",
  "translatedText": "したがって、この単語が全体的にどの程度優れているかを判断するには、こ の分布から得られると予想される情報量を示す何らかの尺度が必要です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score. ",
  "translatedText": "各パターンを調べて、その発生確率と、そのパターンがどれだけ有益かを測定 するものを掛け合わせると、客観的なスコアが得られる可能性があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches. ",
  "translatedText": "さて、それが何であるべきかについて最初に直感するのは、一致の数かもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches. ",
  "translatedText": "平均一致数を減らしたいと考えています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer. ",
  "translatedText": "しかし、その代わりに、私たちが情報に帰することが多い、より普遍的な尺度を使用したいと 思います。また、これらの 13,000 語のそれぞれに、それらが実際に答えであるかど うかについて異なる確率を割り当てると、より柔軟になる尺度を使用したいと考えています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but it's really intuitive if we just look at examples. ",
  "translatedText": "情報の標準単位はビットです。これには少し面白い公式が ありますが、例を見るだけであれば非常に直感的です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information. ",
  "translatedText": "あなたの可能性の空間を半分に減らすような観察があ る場合、それは少しの情報を持っていると言います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about Half of the five letter words have an S, a little less than that, but about half. ",
  "translatedText": "この例では、可能性の空間はすべての可能性のある単語であり、5 文字の単語の約 半分に S があり、それより少し少ないですが、約半分であることがわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information. ",
  "translatedText": "したがって、この観察により、ちょっとした情報が得られるでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information. ",
  "translatedText": "代わりに、新しい事実がその可能性の空間を 4 分の 1 に切り 詰める場合、それは 2 ビットの情報を持っていると言います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T. ",
  "translatedText": "たとえば、これらの単語の約 4 分の 1 に T が含まれていることがわかりました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth. ",
  "translatedText": "観測によりその空間が 8 分の 1 に削減された 場合、それは 3 ビットの情報であると言います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a 16th, five bits cuts it into a 32nd. ",
  "translatedText": "4 ビットでは 16 ビットに分割され、5 ビットでは 32 ビットに分割されます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.52
 },
 {
  "input": "So now you might want to pause and ask yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence? ",
  "translatedText": "そこで、ここで少し立ち止まって、発生確率の観点からビット数を 表す情報の公式は何なのかを自問してみてはいかがでしょうか。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 533.52,
  "end": 542.98
 },
 {
  "input": "What we're saying here is that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability. ",
  "translatedText": "ここで私たちが言いたいのは、ビット数の半分を取るとき、それは 確率と同じことです。これは、ビット数の 2 乗が確率より 1 大きいと言っているのと同じことです。さらに整理すると、情報 は 2 を底とする 1 を確率で割った対数であると言えます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still, where the information is the negative log base two of the probability. ",
  "translatedText": "そして、さらにもう 1 つ再配置すると、この情報が確率 の 2 を底とする負の対数であることが時々わかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half. ",
  "translatedText": "このように表現すると、初心者にとっては少し奇妙に見える かもしれませんが、実際には、自分の可能性を何回半減させ たかを尋ねるという非常に直感的なアイデアにすぎません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture? ",
  "translatedText": "さて、疑問に思っている方は、私たちは単に楽しい言葉遊びをして いるだけだと思ったのですが、なぜ対数が登場するのでしょうか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095. ",
  "translatedText": "これがより優れた単元である理由の 1 つは、非常に起こりそうもない出来事について話 すのがはるかに簡単であり、これこれが発生する確率が 0 であると言うよりも、観測 値に 20 ビットの情報があると言う方がはるかに簡単であるためです。0000095。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together. ",
  "translatedText": "しかし、この対数表現が確率論への非常に有用な追加であることが 判明したより実質的な理由は、情報が加算される方法にあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information. ",
  "translatedText": "たとえば、1 つの観測結果から 2 ビットの情報が得られ、スペースが 4 ビット減り、その後、Wordle での 2 番目の推測のような 2 番目 の観測により、さらに 3 ビットの情報が得られ、さらに 8 分の 1 に 削減された場合、 2 つを組み合わせると 5 ビットの情報が得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add. ",
  "translatedText": "確率が増加するのと同じように、情報は追加されるのを好みます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with. ",
  "translatedText": "したがって、大量の数値を加算する期待値のような領域 に入るとすぐに、ログのおかげで扱いやすくなります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for Weary and add another little tracker on here, showing us how much information there is for each pattern. ",
  "translatedText": "Weary のディストリビューションに戻り、ここに別の小さなトラッカ ーを追加して、各パターンにどれだけの情報があるかを示してみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain. ",
  "translatedText": "ここで注目していただきたいのは、可能性の高いパターンに到達する確率が高く なるほど、情報が少なくなり、得られるビットも少なくなるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information, where we go through each pattern, we say how probable is it, and then we multiply that by how many bits of information do we get. ",
  "translatedText": "この推測の質を測定する方法は、この情報の期待値を取得す ることです。各パターンを調べて、その確率がどれくらい かを示し、それを取得した情報のビット数で乗算します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 643.5,
  "end": 654.06
 },
 {
  "input": "And in the example of Weary, that turns out to be 4.9 bits. ",
  "translatedText": "そして、Weary の例では、それは 4 であることがわかります。9ビット。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times. ",
  "translatedText": "したがって、平均すると、この冒頭の推測から得られる情報は、可能性 の空間を約 5 回半分に切り分けるのと同じくらい優れています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like Slate. ",
  "translatedText": "対照的に、情報の期待値がより高い推測の例 としては、Slate などがあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case you'll notice the distribution looks a lot flatter. ",
  "translatedText": "この場合、分布がかなり平坦になっていることがわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information. ",
  "translatedText": "特に、最も可能性の高いすべての灰色の発生確率は約 6% しかないため、少 なくとも明らかに 3 が得られることになります。9ビットの情報。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that. ",
  "translatedText": "しかし、これは最低限のことであり、通常はそれよりも優れたものが得られるでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8. ",
  "translatedText": "この数字を計算して関連するすべての用語を合計すると 、平均的な情報は約 5 であることがわかります。8. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with Weary, your space of possibilities will be about half as big after this first guess, on average. ",
  "translatedText": "したがって、Weary とは対照的に、この最初の推 測の後、可能性の空間は平均して約半分になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity. ",
  "translatedText": "実は、この情報量の期待値の名前については面白い話があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "Information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science. ",
  "translatedText": "情報理論は、1940 年代にベル研究所で働いていたクロード シャノンによって 開発されましたが、彼はまだ発表されていないアイデアのいくつかについて、当時 の知的巨人で非常に著名なジョン フォン ノイマンと話し合っていました。数学と 物理学、そしてコンピューターサイエンスになりつつあったものの始まりでした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well you should call it entropy, and for two reasons. ",
  "translatedText": "そして、フォン・ノイマンは、この情報量の期待値にあまり良い 名前がないと述べたとき、おそらく、それをエントロピーと呼 ぶべきだ、と話は進みますが、その理由は 2 つあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage. ",
  "translatedText": "まず第一に、不確実性関数はその名前で統計力学で使用されているため、すでに名 前が付いています。そして第二に、そしてさらに重要なことに、エントロピーが 実際に何であるか誰も知りません。したがって、議論では常に利点があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design. ",
  "translatedText": "したがって、名前が少し謎めいているように見えても、 この話を信じられるとしても、それは一種の仕様です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess. ",
  "translatedText": "また、物理学の熱力学第 2 法則すべてとの関係について疑 問に思っているなら、間違いなく関連性がありますが、その 起源において、シャノンは純粋な確率論を扱っていただけであ り、ここでの目的のために、単語のエントロピーについては 、特定の推測の期待される情報値を考えてほしいだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously. ",
  "translatedText": "エントロピーは、2 つのものを同時に測定すると考えることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution. ",
  "translatedText": "1 つ目は、分布がどの程度平坦であるかです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be. ",
  "translatedText": "分布が均一に近づくほど、エントロピーは高くなります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy. ",
  "translatedText": "私たちの場合、一様分布の場合、合計 3 から 5 までのパターンがあり、それらのいずれかを 観察すると、情報ログの底が 3 から 5 までの 2 になり、これはたまたま 7 になり ます。92 なので、これがこのエントロピーの絶対最大値になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place. ",
  "translatedText": "しかし、エントロピーは、そもそもどれだけの可 能性があるかを示す一種の尺度でもあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits. ",
  "translatedText": "たとえば、考えられるパターンが 16 個しかなく、それぞれの可能性が等しい単語がた またまあった場合、このエントロピー、つまり期待される情報は 4 ビットになります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits. ",
  "translatedText": "しかし、64 個の考えられるパターンがあり、それらがすべて同じ確率で ある別の単語がある場合、エントロピーは 6 ビットになるでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes. ",
  "translatedText": "したがって、6 ビットのエントロピーを持つ分布を実際に目にした場合、 それは、同じ確率の結果が 64 個存在するのと同じくらい、これから起 こることには大きな変動と不確実性があると言っているようなものです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this. ",
  "translatedText": "Wurtelebot での最初のパスでは、基本的にこれを行うだけでした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the possible guesses you could have, all 13,000 words, computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns you might see, for each one, and picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible. ",
  "translatedText": "考えられるすべての推測 (13,000 語すべて) を調べ、各単語のエ ントロピー、より具体的には、表示される可能性のあるすべてのパターンに わたる分布のエントロピーを単語ごとに計算し、最も高いものを選択します 。それはあなたの可能性の空間を可能な限り切り詰める可能性があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses. ",
  "translatedText": "ここでは最初の推測についてのみ話しましたが、次の いくつかの推測についても同じことが起こります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words. ",
  "translatedText": "たとえば、最初の推測について何らかのパターンがあり、それに一致す るものに基づいて候補となる単語の数が制限されることがわかったら、 その小さな単語のセットに関して同じゲームをプレイするだけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy. ",
  "translatedText": "提案された 2 番目の推測では、より限定された単語のセットから発生す る可能性のあるすべてのパターンの分布を調べ、13,000 の可能性 すべてを検索し、そのエントロピーを最大化するパターンを見つけます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins. ",
  "translatedText": "これが実際にどのように機能するかを示すために、余白にこの分析のハイライトを 示す、私が書いた Wurtele の小さな変形版を取り出してみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "After doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information. ",
  "translatedText": "すべてのエントロピー計算を行った後、右側には、 期待される情報が最も高いものを示しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch. ",
  "translatedText": "少なくとも現時点でのトップの答えは、後ほど詳しく説明しますが、Tar es です。つまり、もちろん、レンゲ、最も一般的なレンゲのことです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got, given this particular pattern. ",
  "translatedText": "ここで推測するたびに、私はスレートが好きなので、推奨事項を無視してスレ ートを使用することになるのですが、そこにどれだけの期待情報が含まれてい るかがわかりますが、ここの単語の右側には、どれだけの情報が含まれている かが表示されます。この特定のパターンを考慮して、実際に得られた情報。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that. ",
  "translatedText": "つまり、ここでは少し不運だったようです。5 を獲得することが期待されていました。8 ですが 、たまたまそれ以下のものが入手できました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now. ",
  "translatedText": "そして左側には、私たちが今いる場所で考えられ るさまざまな単語がすべて表示されています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment. ",
  "translatedText": "青いバーは各単語がどの程度の確率で考えられるかを示しているため、現時点では 各単語が出現する可能性が等しいと仮定していますが、これはすぐに修正します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities. ",
  "translatedText": "そして、この不確実性の測定により、考えられる単語全体にわたるこ の分布のエントロピーがわかります。これは、現時点では一様分布で あるため、可能性の数を数える不必要に複雑な方法にすぎません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities. ",
  "translatedText": "たとえば、2 の 13 乗を取るとします。66、それは約 13,000 の可能性があるはずです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "I'm a little bit off here, but only because I'm not showing all the decimal places. ",
  "translatedText": "ここでは少しずれていますが、それは小数点以下の桁をすべて表示していないためです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute. ",
  "translatedText": "現時点では、それは冗長で、物事が過度に複雑であるように感じるかもしれま せんが、両方の数値を取得することがなぜ便利であるかはすぐにわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Ramen, which again just really doesn't feel like a word. ",
  "translatedText": "したがって、ここでは、2 番目の推測で最もエントロピーが高いのはラーメンである ことを示唆しているように見えますが、これも本当に言葉のようには感じられません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here, I'm going to go ahead and type in Rains. ",
  "translatedText": "そこで、ここで道徳的な立場を強調するために、先に進んで Rains と入力することにします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky. ",
  "translatedText": "そしてまた少し不運だったようです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information. ",
  "translatedText": "私たちは4を期待していました。3 ビットありますが、3 つしかありません。39ビットの情報。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities. ",
  "translatedText": "つまり、55 の可能性が考えられます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means. ",
  "translatedText": "そしてここでは、それが何を意味するにせよ、実際にそれが示唆 しているもの、つまりコンボに従うことになるかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And okay, this is actually a good chance for a puzzle. ",
  "translatedText": "さて、これは実際、パズルを解く良い機会です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information. ",
  "translatedText": "このパターンでは 4 が得られることがわかります。7ビットの情報。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty. ",
  "translatedText": "しかし、左側では、そのパターンが見える前に 5 つありました。78 ビットの不確実性。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities? ",
  "translatedText": "それで、あなたへのクイズですが、残りの可能性の数については何を意味しますか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well, it means that we're reduced down to one bit of uncertainty, which is the same thing as saying that there's two possible answers. ",
  "translatedText": "これは、不確実性が 1 つまで減少したことを意味します。これは 、考えられる答えが 2 つあると言っているのと同じことです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice. ",
  "translatedText": "それは五分五分の選択だ。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss. ",
  "translatedText": "ここからは、あなたも私もどちらの単語がより一般的である かを知っているので、答えは深淵であることがわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that. ",
  "translatedText": "しかし、現時点で書かれているように、プログラムはそれを知りません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it. ",
  "translatedText": "したがって、可能性が 1 つだけ残されるまで、できるだけ多 くの情報を取得しようと試み続け、その後、それを推測します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy. ",
  "translatedText": "したがって、明らかに、より良い終盤戦略が必要です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1042.34
 },
 {
  "input": "But let's say we call this version one of our wordle solver, and then we go and run some simulations to see how it does. ",
  "translatedText": "しかし、このバージョンを Wordle ソルバーの 1 つと呼び、それがどのよ うに機能するかを確認するためにいくつかのシミュレーションを実行するとします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1042.6,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game. ",
  "translatedText": "つまり、これがどのように機能しているかというと、考えられるすべての単語ゲームを実行しているということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers. ",
  "translatedText": "実際の Wordle の回答である 2315 語すべてを調べます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set. ",
  "translatedText": "基本的にはそれをテストセットとして使用します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice. ",
  "translatedText": "そして、単語がどれだけ一般的であるかを考慮せず、ただ 1 つの選択肢に行き 着くまで、途中の各ステップで情報を最大化しようとするこの単純な方法です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124. ",
  "translatedText": "シミュレーションが終了するまでに、平均スコアは約 4 になります。124. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expected to do worse. ",
  "translatedText": "それは悪いことではありませんが、正直に言うと、私はもっと悪いことをすると予想していました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4. ",
  "translatedText": "しかし、ワードルをプレイしている人は、通常は 4 で取得できると言います。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can. ",
  "translatedText": "本当の課題は、3 つをできるだけ多く獲得することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3. ",
  "translatedText": "スコア 4 とスコア 3 の間ではかなり大きな差があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that. ",
  "translatedText": "ここでの明らかに簡単な成果は、単語が一般的かどうかを何らかの方法 で組み込むことです。また、それを具体的にどのように行うかです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language. ",
  "translatedText": "私がこれにアプローチした方法は、英語のすべて の単語の相対頻度のリストを取得することです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1107.88
 },
 {
  "input": "And I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset. ",
  "translatedText": "そして、Mathematica の単語頻度データ関数を使用しました。この関数自体は、Go ogle Books English Ngram 公開データセットから取得したものです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words. ",
  "translatedText": "たとえば、最も一般的な単語から最も一般的ではない 単語まで並べ替えると、見ていてとても楽しいです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5 letter words in the English language. ",
  "translatedText": "明らかに、これらは英語で最も一般的な 5 文字の単語です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common. ",
  "translatedText": "というか、これらは8番目に多いものです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there. ",
  "translatedText": "最初にどれがあり、その後にあそことあそこがあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common. ",
  "translatedText": "first 自体は first ではなく 9th であり、これらの他の単語がよ り頻繁に出現する可能性があることは理にかなっていますが、first の後の単語 は after、where、そしてそれらの単語は少しだけ一般的ではありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency. ",
  "translatedText": "さて、このデータを使用して、これらの各単語が最終的な答えとなる可能 性をモデル化する場合、単に頻度に比例するだけであってはなりません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1146.34
 },
 {
  "input": "For example, which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely. ",
  "translatedText": "たとえば、スコア 0 が与えられます。このデータセットでは 002 が使用されますが 、braid という単語はある意味で約 1000 分の 1 の可能性が低くなります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.7,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering. ",
  "translatedText": "しかし、これらは両方とも十分に一般的な単語であるため、ほぼ確実に検討する価値があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1158.84
 },
 {
  "input": "So we want more of a binary cutoff. ",
  "translatedText": "したがって、バイナリのカットオフをさらに強化する必要があります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1159.34,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty. ",
  "translatedText": "私がこれに取り組んだ方法は、このソートされた単語のリスト全体を取得し、 それを X 軸上に配置し、シグモイド関数を適用することです。これは、 出力が基本的にバイナリである関数を作成する標準的な方法です。0 か 1 のどちらかですが、その不確実性の領域の間で平滑化が行われます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis. ",
  "translatedText": "つまり、基本的に、最終リストに含まれる各単語に割り当てる確率は 、x 軸上のどこに位置する上記のシグモイド関数の値になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff. ",
  "translatedText": "これは明らかにいくつかのパラメータに依存します。たとえば、これらの単語が占める x 軸上のスペースの幅によって、1 から 0 への降下がどの程度徐々にまたは急激に変 化するかが決まり、単語を左から右のどこに配置するかによってカットオフが決まります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "To be honest, the way I did this was just licking my finger and sticking it into the wind. ",
  "translatedText": "正直に言うと、私がこれを行った方法は、指をなめて風に突き刺すだけでした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff. ",
  "translatedText": "私はソートされたリストを調べて、これらの単語の約半分が最 終的な答えになる可能性が高いと判断できる範囲を見つけよう としました。そして、それをカットオフとして使用しました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement. ",
  "translatedText": "単語全体でこのような分布が得られると、エントロピー が非常に有用な測定値となる別の状況が得られます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were a feather and nails, and we end up with a situation where there's four possible words that match it. ",
  "translatedText": "たとえば、ゲームをプレイしていて、羽と爪だった古 いオープナーから始めて、それに一致する可能性のあ る単語が 4 つある状況に行き着いたとします。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely. ",
  "translatedText": "そして、それらはすべて同じ可能性であると考えてみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1235.62
 },
 {
  "input": "Let me ask you, what is the entropy of this distribution? ",
  "translatedText": "この分布のエントロピーはいくらですか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1236.22,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2. ",
  "translatedText": "さて、これらの可能性のそれぞれに関連付けられた情報は、それぞれが 1 と 4 であり、それが 2 であるため、4 の底を 2 とする対数になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "Two bits of information, four possibilities. ",
  "translatedText": "2 ビットの情報、4 つの可能性。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good. ",
  "translatedText": "すべてとても順調です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than four matches? ",
  "translatedText": "しかし、実際には 4 つ以上の試合があると言ったらどうなるでしょうか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it. ",
  "translatedText": "実際、完全な単語リストに目を通すと、それに一致する単語が 16 個あります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure. ",
  "translatedText": "しかし、私たちのモデルでは、他の 12 単語が実際に最終的な答えになる確率は非 常に低く、非常に曖昧であるため、1000 分の 1 程度であると仮定します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution? ",
  "translatedText": "さて、この分布のエントロピーはいくらでしょうか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before. ",
  "translatedText": "ここでエントロピーが純粋に一致の数を測定している場合、それは 16 の底 2 の対数のようなものになると予想されるかもしれません。こ れは 4 となり、以前よりも 2 ビット多くの不確実性が生じます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before. ",
  "translatedText": "しかし、もちろん、実際の不確実性は以前とそれほど変わりません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example. ",
  "translatedText": "これらの本当にあいまいな 12 の単語があるからといって、たとえば、最 終的な答えが魅力であると知っても、それほど驚くべきことではありません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here, and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits. ",
  "translatedText": "したがって、ここで実際に計算を実行し、各発生の確率と対応す る情報を合計すると、得られる値は 2 になります。11ビット。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "I'm just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it. ",
  "translatedText": "私が言いたいのは、基本的には 2 つのビット、基本的には 4 つの可能性で すが、これらの非常にありそうもない出来事がすべてあるため、もう少し不確実性 があります。ただし、それらを学べば、そこから大量の情報が得られるでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson. ",
  "translatedText": "ズームアウトすると、これが Wordle が情報理 論のレッスンに最適な例となる理由の 1 つです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy. ",
  "translatedText": "エントロピーについては、これら 2 つの異なる感覚のアプリケーションがあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words that we have possible. ",
  "translatedText": "1 つ目は、与えられた推測から得られる期待情報は何 かを示し、2 つ目は、考えられるすべての単語の中で 残っている不確実性を測定できるかどうかを示します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation. ",
  "translatedText": "そして、強調しておきたいのは、推測の期待情報を調べる最初のケースでは、単語の重 み付けが不均等になると、それがエントロピーの計算に影響を与えるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words. ",
  "translatedText": "たとえば、Weary に関連する分布について以前に検討し たのと同じケースを取り出してみましょう。ただし、今回はす べての可能な単語にわたって不均一な分布を使用しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well. ",
  "translatedText": "それでは、それをうまく説明している部分がここに見つかるかどうか見てみましょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here this is pretty good. ",
  "translatedText": "さて、これはかなり良いです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it. ",
  "translatedText": "ここでは、ほぼ同じ確率の 2 つの隣接するパターンがありますが、そのうちの 1 つは、それに一致する可能性のある単語が 32 個あると言われています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them. ",
  "translatedText": "それらが何であるかを確認してみると、これらは 32 個であり、 目をざっと眺めると、どれも非常にありそうもない単語ばかりです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches, so a quarter as many matches, but it's about as likely. ",
  "translatedText": "もっともらしい答え、おそらく叫び声のようなものを見つけるのは難し いですが、ほぼ同じ確率であると考えられる分布内の隣接するパターン を見ると、一致する可能性は 8 つしかないと言われているため、4 分の 1 は多くの試合がありますが、その可能性はほぼ同じです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why. ",
  "translatedText": "それらの一致を調べてみると、その理由がわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers, like ring, or wrath, or raps. ",
  "translatedText": "これらの中には、リング、怒り、ラップなど、実際にもっともらしい答えもあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version 2 of the Wordlebot here, and there are two or three main differences from the first one that we saw. ",
  "translatedText": "これらすべてをどのように組み込むかを説明するために、ここで Wordlebot のバージ ョン 2 を取り上げます。最初に見たものとの主な違いが 2 つまたは 3 つあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer. ",
  "translatedText": "まず、先ほど述べたように、これらのエントロピー、つまり情報の期待 値を計算する方法では、特定の単語が実際に答えとなる確率を組み込 んだ、パターン全体にわたるより洗練された分布を使用しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number 1, though the ones following are a bit different. ",
  "translatedText": "偶然にも、涙が依然としてナンバー 1 ですが、それに続くものは少し異なります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table. ",
  "translatedText": "第 2 に、上位の候補をランク付けするときに、 各単語が実際の答えである確率のモデルを保持し、 それを決定に組み込むようになります。テーブル。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives. ",
  "translatedText": "繰り返しますが、私たちは機械に生活を支配させることはできないので、その推奨を無視します。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches. ",
  "translatedText": "そして、ここでもう 1 つ異なる点があることを言及する必要があると思います。左側 は、不確実性の値、ビット数が、一致の可能性の数と重複するだけではなくなりました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which is a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes. ",
  "translatedText": "これを引き上げて、2 対 8 を計算するとします。02、これは 256 を少し上回 る、おそらく 259 です。これが言いたいのは、実際にこのパターンに 一致する単語が合計 526 個あるとしても、その不確実性の量は、同 じ可能性の 259 個があった場合の値に近いということです。結果。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this. ",
  "translatedText": "このように考えることができます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borx is not the answer, same with yorts and zorl and zorus, so it's a little less uncertain than it was in the previous case. ",
  "translatedText": "yorts、zorl、zorus の場合と同様に、borx が答えではな いことがわかっているため、前のケースよりも不確実性が少し低くなります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller. ",
  "translatedText": "このビット数は小さくなります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here. ",
  "translatedText": "そして、ゲームをプレイし続けると、ここで説明したいこ とと適切ないくつかの推測を加えてこれを洗練させます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy. ",
  "translatedText": "4 番目の推測では、その上位の候補を見てみると、もはや エントロピーを最大化するだけではないことがわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words. ",
  "translatedText": "つまり、現時点では技術的には 7 つの可能性がありま すが、意味のあるチャンスがあるのは寮と言葉だけです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values, that strictly speaking would give more information. ",
  "translatedText": "そして、これらの両方を選択すると、厳密に言えば、より多くの情報が 得られる他の値よりも上位にランク付けされていることがわかります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect. ",
  "translatedText": "初めてこれを行ったとき、これら 2 つの数値を合計して各推測の 質を測定しました。実際、これは予想以上にうまく機能しました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic, and I'm sure there's other approaches people could take but here's the one I landed on. ",
  "translatedText": "しかし、それは実際には体系的とは思えませんでした。他のアプロー チもあるとは思いますが、私がたどり着いたアプローチはこれです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that. ",
  "translatedText": "この場合の単語のように、次の推測の見通しを考慮している場合、本 当に関心があるのは、それを行った場合のゲームの予想スコアです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to. ",
  "translatedText": "そして、その期待スコアを計算するために、単語が実際の答えである確 率はいくらかを言います。現時点では 58% と説明されています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be 4. ",
  "translatedText": "58% の確率で、このゲームのスコアは 4 になるでしょう。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of 1 minus that 58%, our score will be more than that 4. ",
  "translatedText": "そして、1 から 58% を引いた確率で、スコアは 4 より大きくなります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point. ",
  "translatedText": "それ以上はわかりませんが、その時点に到達したときにどの程度の不確 実性が存在する可能性があるかに基づいて推定することはできます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment there's 1.44 bits of uncertainty. ",
  "translatedText": "具体的には、現時点では 1 です。44 ビットの不確実性。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits. ",
  "translatedText": "単語を推測すると、得られる期待情報が 1 であることがわかります。27ビット。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens. ",
  "translatedText": "したがって、言葉を推測した場合、この違いは、それが起こった 後にどの程度の不確実性が残る可能性があるかを表しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score. ",
  "translatedText": "必要なのは、この不確実性を期待されるスコアに関連付ける、 ある種の関数 (ここでは f と呼んでいます) です。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version 1 of the bot to say hey what was the actual score after various points with certain very measurable amounts of uncertainty. ",
  "translatedText": "そして、これを行う方法は、ボットのバージョン 1 に基づいて以前のゲー ムからの大量のデータをプロットして、特定の非常に測定可能な量の不確実性 を伴うさまざまなポイント後の実際のスコアが何であるかを示すことでした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer. ",
  "translatedText": "たとえば、これらのデータ ポイントは、8 程度の値よりも上にあります。8 人だった時点以降、いくつかの試合では7人ほどが発言している。7 ビットの不確 実性があり、最終的な答えを得るには 2 回の推測が必要でした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games it took three guesses, for other games it took four guesses. ",
  "translatedText": "他のゲームでは 3 回の推測が必要で、他のゲームでは 4 回の推測が必要でした。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring. ",
  "translatedText": "ここで左にシフトすると、ゼロを超えるすべての点は、不確実性が ゼロの場合、つまり可能性が 1 つしかない場合、必要な推測の 数は常に 1 つだけであり、安心できることを示しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses. ",
  "translatedText": "少しでも不確実性がある場合、つまり、本質的に可能性が 2 つ だけであることを意味する場合、さらに 1 つの推測が必要な 場合もあれば、さらに 2 つの推測が必要な場合もあります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1623.02
 },
 {
  "input": "And so on and so forth here. ",
  "translatedText": "などなど。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1623.08,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages. ",
  "translatedText": "このデータを視覚化するもう少し簡単な方法は、データをまとめて平均を取ることかもしれません。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example this bar here saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5. ",
  "translatedText": "たとえば、このバーは、少し不確実性があったすべてのポイントのうち、 必要な新しい推測の数は平均して約 1 であることを示しています。5. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward. ",
  "translatedText": "そして、ここのバーは、さまざまなゲームすべての中で、ある時点で 不確実性が 4 ビットをわずかに上回っていたことを示していま す。これは、16 の異なる可能性に絞り込むようなもので、その時 点から平均して 2 つを少し超える推測が必要ですフォワード。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this. ",
  "translatedText": "ここからは、これに合理的と思われる関数を当てはめるために回帰を実行しました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be. ",
  "translatedText": "そして、そのいずれかを行うことの要点は、単語から得られる情報が多ければ多いほど、期待され るスコアが低くなるという直感を定量化できるようにするためであることを忘れないでください。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So with this as version 2.0, if we go back and we run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do? ",
  "translatedText": "ということで、これをバージョン2とします。0、戻って同じシミュレーション セットを実行し、2 315 個の単語の答えすべてに対して実行すると、どうなるでしょうか? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version it's definitely better, which is reassuring. ",
  "translatedText": "最初のバージョンと比べると明らかに良くなっているので、安心できます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done the average is around 3.6, although unlike the first version there are a couple times that it loses and requires more than six in this circumstance. ",
  "translatedText": "全体的に見て平均は 3 程度です。ただし、最初のバージョンとは異なり 、この状況では失われ、6 つ以上が必要になることが数回あります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information. ",
  "translatedText": "おそらく、情報を最大化するのではなく、実際に目標を達成する ためにトレードオフが発生する場合があるからだと思われます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6? ",
  "translatedText": "では、3よりもうまくできるでしょうか。6? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can. ",
  "translatedText": "間違いなくできます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model. ",
  "translatedText": "さて、冒頭で、Wordle の回答の真のリストをモデル の構築方法に組み入れないのが最も楽しいと言いました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43. ",
  "translatedText": "しかし、それを組み込んだ場合、得られる最高のパフォーマンスは 3 程度でした。43. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that. ",
  "translatedText": "したがって、この事前分布を選択するために単語頻度データを使用するだけではなく、より洗練されたもの にしようとすると、この 3.おそらく 43 が、これでどれだけうまくできる か、少なくともどれくらいうまくできるかという最大値を示しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one. ",
  "translatedText": "その最高のパフォーマンスは、基本的には私がここで話してきたア イデアを使用しているだけですが、期待される情報を 1 歩では なく 2 歩前進して検索するなど、さらに一歩進んだものです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is. ",
  "translatedText": "当初はそれについてもっと話すつもりでしたが、実際 にはかなり長くなってしまったことに気づきました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least it's looking like Crane is the best opener. ",
  "translatedText": "私が言えるのは、この 2 段階の検索を行った後、上位候補でいくつかのサ ンプル シミュレーションを実行した結果、少なくとも私にとって今のとこ ろ、Crane が最良のオープナーであるように見えるということです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed? ",
  "translatedText": "誰が予想したでしょうか？",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits. ",
  "translatedText": "また、真の Wordle リストを使用して可能性の空間を決定する場 合、最初の不確実性は 11 ビットをわずかに超える値になります。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out, just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits. ",
  "translatedText": "そして、総当たり検索から、最初の 2 つの推測の後に予想さ れる最大情報は約 10 ビットであることがわかりました。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty. ",
  "translatedText": "これは、最良のシナリオでは、最初の 2 つの推測の後、完全に最適なプレ イを行った場合、約 1 ビットの不確実性が残ることを示唆しています。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses. ",
  "translatedText": "これは、可能な推測が 2 つに絞られるのと同じです。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "So I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as 3, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail. ",
  "translatedText": "したがって、この平均を 3 という低い値にするアルゴリズムを作成することは 不可能である、と言うのは公平であり、おそらくかなり保守的だと思います。な ぜなら、利用可能な単語では、たった 2 つのステップで十分な情報を取得す る余地がないからです。毎回必ず 3 番目のスロットの答えを保証できます。",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]