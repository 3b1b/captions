1
00:00:00,000 --> 00:00:04,040
Wurdle というゲームは、ここ 1 ～ 2

2
00:00:04,040 --> 00:00:07,880
か月でかなり話題になりました。数学の授業の

3
00:00:07,880 --> 00:00:12,120
機会を見逃す人はいません。このゲームは、情報理

4
00:00:12,120 --> 00:00:13,120
論、特にエントロピーとして知られるトピック。

5
00:00:13,120 --> 00:00:17,120
多くの人と同じように、私もパズルに夢中になりました。また、

6
00:00:17,120 --> 00:00:21,200
多くのプログラマーと同じように、私もゲームを可能な限り最適

7
00:00:21,200 --> 00:00:23,200
にプレイするアルゴリズムを作成することに夢中になりました。

8
00:00:23,200 --> 00:00:26,400
ここで私がやろうと思ったのは、アルゴリズム全体がこのエン

9
00:00:26,400 --> 00:00:29,980
トロピーの考え方に中心を置いているため、そのプロセスの一

10
00:00:29,980 --> 00:00:32,080
部を話し、それに使用された数学の一部を説明することです。

11
00:00:32,080 --> 00:00:42,180
まず最初に、Wurdle について聞いたことがない場合のために説明します。Wurdle とは何ですか?

12
00:00:42,180 --> 00:00:45,380
そして、ここでゲームのルールを確認しながら一石二鳥にするために、

13
00:00:45,380 --> 00:00:48,980
今後の展開についてもプレビューさせていただきます。これは、基本的

14
00:00:48,980 --> 00:00:51,380
にゲームをプレイするための小さなアルゴリズムを開発することです。

15
00:00:51,380 --> 00:00:54,860
私は今日の Wurdle を実行していませんが、これは 2 月

16
00:00:54,860 --> 00:00:55,860
4 日なので、ボットがどのように動作するか見てみましょう。

17
00:00:55,860 --> 00:00:59,580
Wurdle の目的は、謎の 5 文字の単語を推測す

18
00:00:59,580 --> 00:01:00,860
ることであり、推測する機会が 6 回与えられます。

19
00:01:00,860 --> 00:01:05,240
たとえば、Wurdle ボットは、推測クレーンから始めることを提案します。

20
00:01:05,240 --> 00:01:09,300
推測するたびに、その推測が真の答えにどの程

21
00:01:09,300 --> 00:01:10,940
度近づいているかに関する情報が得られます。

22
00:01:10,940 --> 00:01:14,540
ここで灰色のボックスは、実際の答えに C が存在しないことを示しています。

23
00:01:14,540 --> 00:01:18,340
黄色のボックスは R があることを示していますが、その位置にありません。

24
00:01:18,340 --> 00:01:21,820
緑色のボックスは、秘密の単語に A があり、そ

25
00:01:21,820 --> 00:01:22,820
れが 3 番目の位置にあることを示しています。

26
00:01:22,820 --> 00:01:24,300
そして、NもEもありません。

27
00:01:24,300 --> 00:01:27,420
それでは、早速入って、Wurdle ボットにその情報を伝えましょう。

28
00:01:27,420 --> 00:01:31,500
クレーンから始まり、灰色、黄色、緑色、灰色、灰色になりました。

29
00:01:31,500 --> 00:01:35,460
現在表示されているすべてのデータについては心配しないでください。それについては、やがて説明します。

30
00:01:35,460 --> 00:01:39,700
しかし、2番目に選ぶ一番の提案は「クソ」です。

31
00:01:39,700 --> 00:01:43,500
推測は実際の 5 文字の単語である必要がありますが、

32
00:01:43,500 --> 00:01:45,700
ご覧のとおり、実際に推測できる内容はかなり自由です。

33
00:01:45,700 --> 00:01:48,860
この場合、shtick を試します。

34
00:01:48,860 --> 00:01:50,260
さて、状況はかなり良いようです。

35
00:01:50,260 --> 00:01:54,580
S と H を押すと、最初の 3 文字がわかり、R があることがわかります。

36
00:01:54,740 --> 00:01:59,740
そして、それは SHA 何か R 、または SHA R 何かのようなものになるでしょう。

37
00:01:59,740 --> 00:02:03,200
そして、Wurdle ボットは、シャードかシャー

38
00:02:03,200 --> 00:02:05,220
プの 2 つの可能性だけを知っているようです。

39
00:02:05,220 --> 00:02:08,620
現時点ではどちらを選択するか迷っているようなものなので、おそらく

40
00:02:08,620 --> 00:02:11,260
アルファベット順というだけでシャードに当てはまるのだと思います。

41
00:02:11,260 --> 00:02:13,000
実際の答えはどれでしょう。

42
00:02:13,000 --> 00:02:14,660
それで、3つで解決しました。

43
00:02:14,660 --> 00:02:17,740
それがいいのかどうか疑問に思っているなら、私が聞いたある人のフレーズで

44
00:02:17,740 --> 00:02:20,820
は、ワールドルでは 4 つがパー、3 つがバーディだということです。

45
00:02:20,820 --> 00:02:22,960
これは非常に適切な例えだと思います。

46
00:02:22,960 --> 00:02:27,560
4 つを獲得するには、一貫してゲームに取り組む必要がありますが、それは確かにクレイジーではありません。

47
00:02:27,560 --> 00:02:30,000
しかし、3つになると、本当に素晴らしい気分になります。

48
00:02:30,000 --> 00:02:33,800
それで、もしあなたがそれに興味がないなら、私がここでやりたいことは、Wurdle

49
00:02:33,800 --> 00:02:36,600
ボットへのアプローチ方法についての私の思考プロセスを最初から説明することです。

50
00:02:36,600 --> 00:02:39,800
先ほども言いましたが、実際には、これは情報理論のレッスンの言い訳です。

51
00:02:39,800 --> 00:02:43,160
主な目標は、情報とは何か、エントロピーとは何かを説明することです。

52
00:02:48,560 --> 00:02:52,080
これに取り組むにあたって私が最初に考えたのは、英語におけ

53
00:02:52,080 --> 00:02:53,560
るさまざまな文字の相対的な頻度を調べてみることでした。

54
00:02:53,560 --> 00:02:57,800
そこで私は、これらの最も頻繁に使用される文字の多くに当てはまる

55
00:02:57,800 --> 00:02:59,960
冒頭の推測または冒頭の推測のペアはあるだろうか、と考えました。

56
00:02:59,960 --> 00:03:03,780
そして、私がとても気に入っていたのは、ネイルに続いて他のことをすることでした。

57
00:03:03,780 --> 00:03:06,980
文字を打つと、緑か黄色が出ると、いつで

58
00:03:06,980 --> 00:03:07,980
も気分が良くなる、という考え方です。

59
00:03:07,980 --> 00:03:09,460
情報が入ってくる感じです。

60
00:03:09,460 --> 00:03:13,140
ただし、このような場合、たとえヒットせず、常にグレーが表示

61
00:03:13,140 --> 00:03:16,640
されたとしても、これらの文字を含まない単語を見つけること

62
00:03:16,640 --> 00:03:17,640
は非常にまれであるため、それでも多くの情報が得られます。

63
00:03:17,640 --> 00:03:21,840
しかし、それでも、これはあまり体系的とは思えません

64
00:03:21,840 --> 00:03:23,520
。たとえば、文字の順序は考慮されていないからです。

65
00:03:23,520 --> 00:03:26,080
カタツムリを入力できるのに、なぜネイルを入力するのでしょうか?

66
00:03:26,080 --> 00:03:27,720
最後にSがついたほうがいいでしょうか？

67
00:03:27,720 --> 00:03:28,720
よくわかりません。

68
00:03:28,720 --> 00:03:33,500
さて、私の友人は、「疲れた」という単語で始めるのが好きだと言いました。それ

69
00:03:33,500 --> 00:03:37,160
には、W や Y などの珍しい文字が含まれているので、ちょっと驚きました。

70
00:03:37,160 --> 00:03:39,400
しかし、おそらくそれはより良いオープナーであるかもしれません。

71
00:03:39,400 --> 00:03:43,920
潜在的な推測の質を判断するために与えることができ

72
00:03:43,920 --> 00:03:44,920
る、ある種の定量的なスコアはあるのでしょうか?

73
00:03:44,920 --> 00:03:48,640
ここで、考えられる推測をランク付けする方法を準備するために、戻って

74
00:03:48,640 --> 00:03:51,800
、ゲームがどのように正確に設定されているかを少し明確にしましょう。

75
00:03:51,800 --> 00:03:55,880
つまり、有効な推測として入力できる単語のリストがあ

76
00:03:55,880 --> 00:03:57,920
り、その長さはわずか約 13,000 単語です。

77
00:03:57,920 --> 00:04:01,560
しかし、よく見てみると、本当に珍しいものがたくさんあります。頭やアリ、A

78
00:04:01,560 --> 00:04:07,040
RG など、スクラブル ゲームで家族の口論を引き起こすような言葉です。

79
00:04:07,040 --> 00:04:10,600
しかし、ゲームの雰囲気は、答えが常にかなり一般的な単語になるということです。

80
00:04:10,600 --> 00:04:16,080
そして実際、答えとして考えられる約 2300 語のリストがもう 1 つあります。

81
00:04:16,080 --> 00:04:20,320
これは人間が厳選したリストで、特にゲームクリエイターのガールフ

82
00:04:20,320 --> 00:04:21,800
レンドによるものだと思いますが、これはちょっと楽しいですね。

83
00:04:21,800 --> 00:04:25,560
しかし、私がやりたいのは、このプロジェクトの課題は、このリストに関する事前の知識を組み

84
00:04:25,560 --> 00:04:30,720
込まずに Wordle を解決するプログラムを作成できるかどうかを確認することです。

85
00:04:30,720 --> 00:04:34,560
まず、このリストには載っていない、よく使わ

86
00:04:34,560 --> 00:04:35,560
れる 5 文字の単語がたくさんあります。

87
00:04:35,560 --> 00:04:38,360
したがって、もう少し回復力があり、公式 Web サイトだけでなく、誰と

88
00:04:38,360 --> 00:04:41,960
でも Wordle を対戦できるプログラムを作成する方がよいでしょう。

89
00:04:41,960 --> 00:04:45,900
また、この考えられる答えのリストが何であるかを私たちが知って

90
00:04:45,900 --> 00:04:47,440
いる理由は、それがソース コードに表示されているからです。

91
00:04:47,440 --> 00:04:51,620
ただし、ソース コード内でそれが表示されるのは

92
00:04:51,620 --> 00:04:52,840
、その日その日で得られる答えの特定の順序です。

93
00:04:52,840 --> 00:04:56,400
したがって、いつでも明日の答えを調べることができます。

94
00:04:56,400 --> 00:04:59,140
したがって、リストの使用には不正行為である意味があることは明らかです。

95
00:04:59,140 --> 00:05:02,900
そして、より興味深いパズルとより豊かな情報理論のレッスンを実現す

96
00:05:02,900 --> 00:05:07,640
るのは、代わりに一般的な単語の相対頻度などのより普遍的なデータ

97
00:05:07,640 --> 00:05:11,640
を使用して、より一般的な単語を好むという直感を捉えることです。

98
00:05:11,640 --> 00:05:16,560
では、これら 13,000 の可能性の中から、冒頭の推測をどのように選択すればよいのでしょうか?

99
00:05:16,560 --> 00:05:19,960
たとえば、友人が疲れたプロポーズをした場合、その質をどのように分析すべきでしょうか?

100
00:05:19,960 --> 00:05:25,040
まあ、彼がその可能性の低い W が好きだと言ったのは、その W が当た

101
00:05:25,040 --> 00:05:27,880
ったらどれだけ気持ちいいかというロングショットの性質が好きだからです。

102
00:05:27,880 --> 00:05:31,400
たとえば、最初に明らかにされたパターンが次のようなものであった場合、この巨大

103
00:05:31,400 --> 00:05:36,080
な辞書にはそのパターンに一致する単語が 58 語しかないことがわかります。

104
00:05:36,080 --> 00:05:38,900
13,000からは大幅な削減です。

105
00:05:38,900 --> 00:05:43,320
しかし、もちろんその裏返しとして、このようなパターンが発生するのは非常に珍しいということです。

106
00:05:43,360 --> 00:05:47,600
具体的には、各単語が答えとなる可能性が等しい場合、このパターンにヒ

107
00:05:47,600 --> 00:05:51,680
ットする確率は、58 を約 13,000 で割った値になります。

108
00:05:51,680 --> 00:05:53,880
もちろん、それらが同じように答えられるわけではありません。

109
00:05:53,880 --> 00:05:56,680
これらのほとんどは非常に曖昧で、疑わしい単語ですらあります。

110
00:05:56,680 --> 00:05:59,560
しかし、少なくともこのすべての最初のパスでは、それらはすべて

111
00:05:59,560 --> 00:06:02,040
同じ可能性であると仮定し、後でそれを少し改良してみましょう。

112
00:06:02,040 --> 00:06:07,360
重要なのは、情報量が多いパターンはその性質上、発生する可能性が低いということです。

113
00:06:07,360 --> 00:06:11,320
実際のところ、有益であるということは、その可能性は低いということです。

114
00:06:11,920 --> 00:06:16,720
このオープニングで見られる可能性の高いパターンは、次の

115
00:06:16,720 --> 00:06:18,360
ようなものです。もちろん、中には W がありません。

116
00:06:18,360 --> 00:06:22,080
E があるかもしれないし、A も R も Y もないかもしれません。

117
00:06:22,080 --> 00:06:24,640
この場合、一致する可能性は 1400 通りあります。

118
00:06:24,640 --> 00:06:29,600
すべてが同じ確率である場合、このパター

119
00:06:29,600 --> 00:06:30,680
ンになる確率は約 11% になります。

120
00:06:30,680 --> 00:06:34,320
したがって、最も可能性の高い結果は、最も情報が少ないものでもあります。

121
00:06:34,320 --> 00:06:38,440
ここでより全体的な視点を得るために、表示される可能性のあるさ

122
00:06:38,440 --> 00:06:42,000
まざまなパターンすべてにわたる確率の完全な分布を示します。

123
00:06:42,000 --> 00:06:46,000
したがって、あなたが見ている各バーは、明らかにされる可能性のある色のパ

124
00:06:46,000 --> 00:06:50,500
ターンに対応しており、そのうち 3 ～ 5 番目の可能性があり、最も一

125
00:06:50,500 --> 00:06:52,960
般的なものから最も一般的ではないものの順に左から右に編成されています。

126
00:06:52,960 --> 00:06:56,200
したがって、ここで最も一般的な可能性は、すべてが灰色になることです。

127
00:06:56,200 --> 00:06:58,800
それは約 14% の確率で起こります。

128
00:06:58,800 --> 00:07:02,040
そして、推測するときに期待しているのは、このロングテールのどこ

129
00:07:02,040 --> 00:07:06,360
かに行き着くということです。たとえば、このパターンに一致する可

130
00:07:06,360 --> 00:07:09,920
能性が 18 個しかなく、明らかに次のように見える場所です。

131
00:07:09,920 --> 00:07:14,080
あるいは、もう少し左に行けば、おそらくここまで行けるかもしれません。

132
00:07:14,080 --> 00:07:16,560
さて、あなたに良いパズルをご紹介します。

133
00:07:16,560 --> 00:07:20,600
W で始まり Y で終わり、どこかに R

134
00:07:20,600 --> 00:07:22,040
が含まれる英語の 3 つの単語は何ですか?

135
00:07:22,040 --> 00:07:27,560
結局のところ、その答えは、くどい、虫食い、そして皮肉なものだった。

136
00:07:27,560 --> 00:07:32,720
したがって、この単語が全体的にどの程度優れているかを判断するには、こ

137
00:07:32,720 --> 00:07:35,720
の分布から得られると予想される情報量を示す何らかの尺度が必要です。

138
00:07:36,360 --> 00:07:41,080
各パターンを調べて、その発生確率と、そのパターンがどれだけ有益かを測定

139
00:07:41,080 --> 00:07:46,000
するものを掛け合わせると、客観的なスコアが得られる可能性があります。

140
00:07:46,000 --> 00:07:50,280
さて、それが何であるべきかについて最初に直感するのは、一致の数かもしれません。

141
00:07:50,280 --> 00:07:52,960
平均一致数を減らしたいと考えています。

142
00:07:52,960 --> 00:07:57,400
しかし、その代わりに、私たちが情報に帰することが多い、より普遍的な尺度を使用したいと

143
00:07:57,400 --> 00:08:01,040
思います。また、これらの 13,000 語のそれぞれに、それらが実際に答えであるかど

144
00:08:01,040 --> 00:08:04,320
うかについて異なる確率を割り当てると、より柔軟になる尺度を使用したいと考えています。

145
00:08:10,600 --> 00:08:14,760
情報の標準単位はビットです。これには少し面白い公式が

146
00:08:14,760 --> 00:08:17,800
ありますが、例を見るだけであれば非常に直感的です。

147
00:08:17,800 --> 00:08:21,880
あなたの可能性の空間を半分に減らすような観察があ

148
00:08:21,880 --> 00:08:24,200
る場合、それは少しの情報を持っていると言います。

149
00:08:24,200 --> 00:08:27,680
この例では、可能性の空間はすべての可能性のある単語であり、5 文字の単語の約

150
00:08:27,760 --> 00:08:31,560
半分に S があり、それより少し少ないですが、約半分であることがわかります。

151
00:08:31,560 --> 00:08:35,200
したがって、この観察により、ちょっとした情報が得られるでしょう。

152
00:08:35,200 --> 00:08:39,640
代わりに、新しい事実がその可能性の空間を 4 分の 1 に切り

153
00:08:39,640 --> 00:08:42,000
詰める場合、それは 2 ビットの情報を持っていると言います。

154
00:08:42,000 --> 00:08:45,120
たとえば、これらの単語の約 4 分の 1 に T が含まれていることがわかりました。

155
00:08:45,120 --> 00:08:49,720
観測によりその空間が 8 分の 1 に削減された

156
00:08:49,720 --> 00:08:50,920
場合、それは 3 ビットの情報であると言います。

157
00:08:50,920 --> 00:08:55,000
4 ビットでは 16 ビットに分割され、5 ビットでは 32 ビットに分割されます。

158
00:08:55,000 --> 00:09:00,160
そこで、ここで少し立ち止まって、発生確率の観点からビット数を

159
00:09:00,160 --> 00:09:04,520
表す情報の公式は何なのかを自問してみてはいかがでしょうか。

160
00:09:04,520 --> 00:09:07,920
ここで私たちが言いたいのは、ビット数の半分を取るとき、それは

161
00:09:07,920 --> 00:09:11,680
確率と同じことです。これは、ビット数の 2 乗が確率より 1

162
00:09:11,680 --> 00:09:16,200
大きいと言っているのと同じことです。さらに整理すると、情報

163
00:09:16,200 --> 00:09:19,680
は 2 を底とする 1 を確率で割った対数であると言えます。

164
00:09:19,680 --> 00:09:23,200
そして、さらにもう 1 つ再配置すると、この情報が確率

165
00:09:23,200 --> 00:09:25,680
の 2 を底とする負の対数であることが時々わかります。

166
00:09:25,680 --> 00:09:29,120
このように表現すると、初心者にとっては少し奇妙に見える

167
00:09:29,120 --> 00:09:33,400
かもしれませんが、実際には、自分の可能性を何回半減させ

168
00:09:33,400 --> 00:09:35,120
たかを尋ねるという非常に直感的なアイデアにすぎません。

169
00:09:35,120 --> 00:09:37,840
さて、疑問に思っている方は、私たちは単に楽しい言葉遊びをして

170
00:09:37,840 --> 00:09:39,920
いるだけだと思ったのですが、なぜ対数が登場するのでしょうか?

171
00:09:39,920 --> 00:09:43,920
これがより優れた単元である理由の 1 つは、非常に起こりそうもない出来事について話

172
00:09:43,920 --> 00:09:48,120
すのがはるかに簡単であり、これこれが発生する確率が 0 であると言うよりも、観測

173
00:09:48,120 --> 00:09:53,480
値に 20 ビットの情報があると言う方がはるかに簡単であるためです。0000095。

174
00:09:53,480 --> 00:09:57,360
しかし、この対数表現が確率論への非常に有用な追加であることが

175
00:09:57,360 --> 00:10:02,000
判明したより実質的な理由は、情報が加算される方法にあります。

176
00:10:02,000 --> 00:10:05,560
たとえば、1 つの観測結果から 2 ビットの情報が得られ、スペースが 4

177
00:10:05,560 --> 00:10:10,120
ビット減り、その後、Wordle での 2 番目の推測のような 2 番目

178
00:10:10,120 --> 00:10:14,480
の観測により、さらに 3 ビットの情報が得られ、さらに 8 分の 1 に

179
00:10:14,480 --> 00:10:17,360
削減された場合、 2 つを組み合わせると 5 ビットの情報が得られます。

180
00:10:17,360 --> 00:10:21,200
確率が増加するのと同じように、情報は追加されるのを好みます。

181
00:10:21,200 --> 00:10:24,920
したがって、大量の数値を加算する期待値のような領域

182
00:10:24,920 --> 00:10:28,660
に入るとすぐに、ログのおかげで扱いやすくなります。

183
00:10:28,660 --> 00:10:32,600
Weary のディストリビューションに戻り、ここに別の小さなトラッカ

184
00:10:32,600 --> 00:10:35,560
ーを追加して、各パターンにどれだけの情報があるかを示してみましょう。

185
00:10:35,560 --> 00:10:38,760
ここで注目していただきたいのは、可能性の高いパターンに到達する確率が高く

186
00:10:38,760 --> 00:10:43,500
なるほど、情報が少なくなり、得られるビットも少なくなるということです。

187
00:10:43,500 --> 00:10:47,360
この推測の質を測定する方法は、この情報の期待値を取得す

188
00:10:47,360 --> 00:10:51,620
ることです。各パターンを調べて、その確率がどれくらい

189
00:10:51,620 --> 00:10:54,940
かを示し、それを取得した情報のビット数で乗算します。

190
00:10:54,940 --> 00:10:58,480
そして、Weary の例では、それは 4 であることがわかります。9ビット。

191
00:10:58,480 --> 00:11:02,800
したがって、平均すると、この冒頭の推測から得られる情報は、可能性

192
00:11:02,800 --> 00:11:05,660
の空間を約 5 回半分に切り分けるのと同じくらい優れています。

193
00:11:05,660 --> 00:11:10,260
対照的に、情報の期待値がより高い推測の例

194
00:11:10,260 --> 00:11:13,220
としては、Slate などがあります。

195
00:11:13,220 --> 00:11:16,180
この場合、分布がかなり平坦になっていることがわかります。

196
00:11:16,180 --> 00:11:20,780
特に、最も可能性の高いすべての灰色の発生確率は約 6% しかないため、少

197
00:11:20,780 --> 00:11:25,940
なくとも明らかに 3 が得られることになります。9ビットの情報。

198
00:11:25,940 --> 00:11:29,140
しかし、これは最低限のことであり、通常はそれよりも優れたものが得られるでしょう。

199
00:11:29,140 --> 00:11:33,380
この数字を計算して関連するすべての用語を合計すると

200
00:11:33,380 --> 00:11:36,420
、平均的な情報は約 5 であることがわかります。8.

201
00:11:36,420 --> 00:11:42,140
したがって、Weary とは対照的に、この最初の推

202
00:11:42,140 --> 00:11:43,940
測の後、可能性の空間は平均して約半分になります。

203
00:11:43,940 --> 00:11:49,540
実は、この情報量の期待値の名前については面白い話があります。

204
00:11:49,540 --> 00:11:52,580
情報理論は、1940 年代にベル研究所で働いていたクロード シャノンによって

205
00:11:52,580 --> 00:11:57,620
開発されましたが、彼はまだ発表されていないアイデアのいくつかについて、当時

206
00:11:57,620 --> 00:12:01,500
の知的巨人で非常に著名なジョン フォン ノイマンと話し合っていました。数学と

207
00:12:01,500 --> 00:12:04,180
物理学、そしてコンピューターサイエンスになりつつあったものの始まりでした。

208
00:12:04,180 --> 00:12:07,260
そして、フォン・ノイマンは、この情報量の期待値にあまり良い

209
00:12:07,260 --> 00:12:12,540
名前がないと述べたとき、おそらく、それをエントロピーと呼

210
00:12:12,540 --> 00:12:14,720
ぶべきだ、と話は進みますが、その理由は 2 つあります。

211
00:12:14,720 --> 00:12:18,400
まず第一に、不確実性関数はその名前で統計力学で使用されているため、すでに名

212
00:12:18,400 --> 00:12:23,100
前が付いています。そして第二に、そしてさらに重要なことに、エントロピーが

213
00:12:23,100 --> 00:12:26,940
実際に何であるか誰も知りません。したがって、議論では常に利点があります。

214
00:12:26,940 --> 00:12:31,420
したがって、名前が少し謎めいているように見えても、

215
00:12:31,420 --> 00:12:33,420
この話を信じられるとしても、それは一種の仕様です。

216
00:12:33,420 --> 00:12:36,740
また、物理学の熱力学第 2 法則すべてとの関係について疑

217
00:12:36,740 --> 00:12:40,820
問に思っているなら、間違いなく関連性がありますが、その

218
00:12:40,820 --> 00:12:44,780
起源において、シャノンは純粋な確率論を扱っていただけであ

219
00:12:44,780 --> 00:12:49,340
り、ここでの目的のために、単語のエントロピーについては

220
00:12:49,340 --> 00:12:50,820
、特定の推測の期待される情報値を考えてほしいだけです。

221
00:12:50,820 --> 00:12:54,380
エントロピーは、2 つのものを同時に測定すると考えることができます。

222
00:12:54,380 --> 00:12:57,420
1 つ目は、分布がどの程度平坦であるかです。

223
00:12:57,420 --> 00:13:01,700
分布が均一に近づくほど、エントロピーは高くなります。

224
00:13:01,700 --> 00:13:06,340
私たちの場合、一様分布の場合、合計 3 から 5 までのパターンがあり、それらのいずれかを

225
00:13:06,340 --> 00:13:11,340
観察すると、情報ログの底が 3 から 5 までの 2 になり、これはたまたま 7 になり

226
00:13:11,340 --> 00:13:17,860
ます。92 なので、これがこのエントロピーの絶対最大値になります。

227
00:13:17,860 --> 00:13:21,900
しかし、エントロピーは、そもそもどれだけの可

228
00:13:21,900 --> 00:13:22,900
能性があるかを示す一種の尺度でもあります。

229
00:13:22,900 --> 00:13:26,980
たとえば、考えられるパターンが 16 個しかなく、それぞれの可能性が等しい単語がた

230
00:13:26,980 --> 00:13:32,760
またまあった場合、このエントロピー、つまり期待される情報は 4 ビットになります。

231
00:13:32,760 --> 00:13:36,880
しかし、64 個の考えられるパターンがあり、それらがすべて同じ確率で

232
00:13:36,880 --> 00:13:41,000
ある別の単語がある場合、エントロピーは 6 ビットになるでしょう。

233
00:13:41,000 --> 00:13:45,800
したがって、6 ビットのエントロピーを持つ分布を実際に目にした場合、

234
00:13:45,800 --> 00:13:50,000
それは、同じ確率の結果が 64 個存在するのと同じくらい、これから起

235
00:13:50,000 --> 00:13:54,400
こることには大きな変動と不確実性があると言っているようなものです。

236
00:13:54,400 --> 00:13:58,360
Wurtelebot での最初のパスでは、基本的にこれを行うだけでした。

237
00:13:58,360 --> 00:14:03,560
考えられるすべての推測 (13,000 語すべて) を調べ、各単語のエ

238
00:14:03,560 --> 00:14:08,580
ントロピー、より具体的には、表示される可能性のあるすべてのパターンに

239
00:14:08,580 --> 00:14:13,040
わたる分布のエントロピーを単語ごとに計算し、最も高いものを選択します

240
00:14:13,040 --> 00:14:17,200
。それはあなたの可能性の空間を可能な限り切り詰める可能性があります。

241
00:14:17,200 --> 00:14:20,120
ここでは最初の推測についてのみ話しましたが、次の

242
00:14:20,120 --> 00:14:21,680
いくつかの推測についても同じことが起こります。

243
00:14:21,680 --> 00:14:25,100
たとえば、最初の推測について何らかのパターンがあり、それに一致す

244
00:14:25,100 --> 00:14:29,300
るものに基づいて候補となる単語の数が制限されることがわかったら、

245
00:14:29,300 --> 00:14:32,300
その小さな単語のセットに関して同じゲームをプレイするだけです。

246
00:14:32,300 --> 00:14:36,500
提案された 2 番目の推測では、より限定された単語のセットから発生す

247
00:14:36,500 --> 00:14:41,540
る可能性のあるすべてのパターンの分布を調べ、13,000 の可能性

248
00:14:41,540 --> 00:14:45,480
すべてを検索し、そのエントロピーを最大化するパターンを見つけます。

249
00:14:45,480 --> 00:14:48,980
これが実際にどのように機能するかを示すために、余白にこの分析のハイライトを

250
00:14:48,980 --> 00:14:54,060
示す、私が書いた Wurtele の小さな変形版を取り出してみましょう。

251
00:14:54,460 --> 00:14:57,820
すべてのエントロピー計算を行った後、右側には、

252
00:14:57,820 --> 00:15:00,340
期待される情報が最も高いものを示しています。

253
00:15:00,340 --> 00:15:04,940
少なくとも現時点でのトップの答えは、後ほど詳しく説明しますが、Tar

254
00:15:04,940 --> 00:15:11,140
es です。つまり、もちろん、レンゲ、最も一般的なレンゲのことです。

255
00:15:11,140 --> 00:15:14,180
ここで推測するたびに、私はスレートが好きなので、推奨事項を無視してスレ

256
00:15:14,180 --> 00:15:19,220
ートを使用することになるのですが、そこにどれだけの期待情報が含まれてい

257
00:15:19,220 --> 00:15:23,300
るかがわかりますが、ここの単語の右側には、どれだけの情報が含まれている

258
00:15:23,340 --> 00:15:24,980
かが表示されます。この特定のパターンを考慮して、実際に得られた情報。

259
00:15:24,980 --> 00:15:28,660
つまり、ここでは少し不運だったようです。5 を獲得することが期待されていました。8 ですが

260
00:15:28,660 --> 00:15:30,660
、たまたまそれ以下のものが入手できました。

261
00:15:30,660 --> 00:15:34,020
そして左側には、私たちが今いる場所で考えられ

262
00:15:34,020 --> 00:15:35,860
るさまざまな単語がすべて表示されています。

263
00:15:35,860 --> 00:15:39,820
青いバーは各単語がどの程度の確率で考えられるかを示しているため、現時点では

264
00:15:39,820 --> 00:15:44,140
各単語が出現する可能性が等しいと仮定していますが、これはすぐに修正します。

265
00:15:44,140 --> 00:15:48,580
そして、この不確実性の測定により、考えられる単語全体にわたるこ

266
00:15:48,580 --> 00:15:53,220
の分布のエントロピーがわかります。これは、現時点では一様分布で

267
00:15:53,300 --> 00:15:55,940
あるため、可能性の数を数える不必要に複雑な方法にすぎません。

268
00:15:55,940 --> 00:16:01,700
たとえば、2 の 13 乗を取るとします。66、それは約

269
00:16:01,700 --> 00:16:02,700
13,000 の可能性があるはずです。

270
00:16:02,700 --> 00:16:06,780
ここでは少しずれていますが、それは小数点以下の桁をすべて表示していないためです。

271
00:16:06,780 --> 00:16:10,260
現時点では、それは冗長で、物事が過度に複雑であるように感じるかもしれま

272
00:16:10,260 --> 00:16:12,780
せんが、両方の数値を取得することがなぜ便利であるかはすぐにわかります。

273
00:16:12,780 --> 00:16:16,780
したがって、ここでは、2 番目の推測で最もエントロピーが高いのはラーメンである

274
00:16:16,780 --> 00:16:19,700
ことを示唆しているように見えますが、これも本当に言葉のようには感じられません。

275
00:16:19,700 --> 00:16:25,660
そこで、ここで道徳的な立場を強調するために、先に進んで Rains と入力することにします。

276
00:16:25,660 --> 00:16:27,540
そしてまた少し不運だったようです。

277
00:16:27,540 --> 00:16:32,100
私たちは4を期待していました。3 ビットありますが、3 つしかありません。39ビットの情報。

278
00:16:32,100 --> 00:16:35,060
つまり、55 の可能性が考えられます。

279
00:16:35,060 --> 00:16:38,860
そしてここでは、それが何を意味するにせよ、実際にそれが示唆

280
00:16:38,860 --> 00:16:40,200
しているもの、つまりコンボに従うことになるかもしれません。

281
00:16:40,200 --> 00:16:43,300
さて、これは実際、パズルを解く良い機会です。

282
00:16:43,300 --> 00:16:47,020
このパターンでは 4 が得られることがわかります。7ビットの情報。

283
00:16:47,020 --> 00:16:52,400
しかし、左側では、そのパターンが見える前に 5 つありました。78 ビットの不確実性。

284
00:16:52,400 --> 00:16:56,860
それで、あなたへのクイズですが、残りの可能性の数については何を意味しますか?

285
00:16:56,860 --> 00:17:02,280
これは、不確実性が 1 つまで減少したことを意味します。これは

286
00:17:02,280 --> 00:17:04,700
、考えられる答えが 2 つあると言っているのと同じことです。

287
00:17:04,700 --> 00:17:06,520
それは五分五分の選択だ。

288
00:17:06,520 --> 00:17:09,860
ここからは、あなたも私もどちらの単語がより一般的である

289
00:17:09,860 --> 00:17:11,220
かを知っているので、答えは深淵であることがわかります。

290
00:17:11,220 --> 00:17:13,540
しかし、現時点で書かれているように、プログラムはそれを知りません。

291
00:17:13,540 --> 00:17:17,560
したがって、可能性が 1 つだけ残されるまで、できるだけ多

292
00:17:17,560 --> 00:17:20,360
くの情報を取得しようと試み続け、その後、それを推測します。

293
00:17:20,360 --> 00:17:22,700
したがって、明らかに、より良い終盤戦略が必要です。

294
00:17:22,700 --> 00:17:26,540
しかし、このバージョンを Wordle ソルバーの 1 つと呼び、それがどのよ

295
00:17:26,540 --> 00:17:30,740
うに機能するかを確認するためにいくつかのシミュレーションを実行するとします。

296
00:17:30,740 --> 00:17:34,240
つまり、これがどのように機能しているかというと、考えられるすべての単語ゲームを実行しているということです。

297
00:17:34,240 --> 00:17:38,780
実際の Wordle の回答である 2315 語すべてを調べます。

298
00:17:38,780 --> 00:17:41,340
基本的にはそれをテストセットとして使用します。

299
00:17:41,340 --> 00:17:45,820
そして、単語がどれだけ一般的であるかを考慮せず、ただ 1 つの選択肢に行き

300
00:17:45,820 --> 00:17:50,480
着くまで、途中の各ステップで情報を最大化しようとするこの単純な方法です。

301
00:17:50,480 --> 00:17:55,100
シミュレーションが終了するまでに、平均スコアは約 4 になります。124.

302
00:17:55,100 --> 00:17:59,780
それは悪いことではありませんが、正直に言うと、私はもっと悪いことをすると予想していました。

303
00:17:59,780 --> 00:18:03,040
しかし、ワードルをプレイしている人は、通常は 4 で取得できると言います。

304
00:18:03,040 --> 00:18:05,260
本当の課題は、3 つをできるだけ多く獲得することです。

305
00:18:05,260 --> 00:18:08,920
スコア 4 とスコア 3 の間ではかなり大きな差があります。

306
00:18:08,920 --> 00:18:13,300
ここでの明らかに簡単な成果は、単語が一般的かどうかを何らかの方法

307
00:18:13,300 --> 00:18:23,160
で組み込むことです。また、それを具体的にどのように行うかです。

308
00:18:23,160 --> 00:18:26,860
私がこれにアプローチした方法は、英語のすべて

309
00:18:26,860 --> 00:18:28,560
の単語の相対頻度のリストを取得することです。

310
00:18:28,560 --> 00:18:32,560
そして、Mathematica の単語頻度データ関数を使用しました。この関数自体は、Go

311
00:18:32,560 --> 00:18:35,520
ogle Books English Ngram 公開データセットから取得したものです。

312
00:18:35,520 --> 00:18:38,680
たとえば、最も一般的な単語から最も一般的ではない

313
00:18:38,680 --> 00:18:40,120
単語まで並べ替えると、見ていてとても楽しいです。

314
00:18:40,120 --> 00:18:43,740
明らかに、これらは英語で最も一般的な 5 文字の単語です。

315
00:18:43,740 --> 00:18:46,480
というか、これらは8番目に多いものです。

316
00:18:46,480 --> 00:18:49,440
最初にどれがあり、その後にあそことあそこがあります。

317
00:18:49,440 --> 00:18:53,020
first 自体は first ではなく 9th であり、これらの他の単語がよ

318
00:18:53,020 --> 00:18:57,840
り頻繁に出現する可能性があることは理にかなっていますが、first の後の単語

319
00:18:57,840 --> 00:18:59,000
は after、where、そしてそれらの単語は少しだけ一般的ではありません。

320
00:18:59,000 --> 00:19:04,400
さて、このデータを使用して、これらの各単語が最終的な答えとなる可能

321
00:19:04,400 --> 00:19:06,760
性をモデル化する場合、単に頻度に比例するだけであってはなりません。

322
00:19:07,020 --> 00:19:12,560
たとえば、スコア 0 が与えられます。このデータセットでは 002 が使用されますが

323
00:19:12,560 --> 00:19:15,200
、braid という単語はある意味で約 1000 分の 1 の可能性が低くなります。

324
00:19:15,200 --> 00:19:19,400
しかし、これらは両方とも十分に一般的な単語であるため、ほぼ確実に検討する価値があります。

325
00:19:19,400 --> 00:19:21,900
したがって、バイナリのカットオフをさらに強化する必要があります。

326
00:19:21,900 --> 00:19:26,520
私がこれに取り組んだ方法は、このソートされた単語のリスト全体を取得し、

327
00:19:26,520 --> 00:19:31,060
それを X 軸上に配置し、シグモイド関数を適用することです。これは、

328
00:19:31,060 --> 00:19:35,540
出力が基本的にバイナリである関数を作成する標準的な方法です。 0 か

329
00:19:35,540 --> 00:19:38,500
1 のどちらかですが、その不確実性の領域の間で平滑化が行われます。

330
00:19:38,500 --> 00:19:43,900
つまり、基本的に、最終リストに含まれる各単語に割り当てる確率は

331
00:19:43,900 --> 00:19:49,540
、x 軸上のどこに位置する上記のシグモイド関数の値になります。

332
00:19:49,540 --> 00:19:53,940
これは明らかにいくつかのパラメータに依存します。たとえば、これらの単語が占める x

333
00:19:53,940 --> 00:19:59,660
軸上のスペースの幅によって、1 から 0 への降下がどの程度徐々にまたは急激に変

334
00:19:59,660 --> 00:20:03,000
化するかが決まり、単語を左から右のどこに配置するかによってカットオフが決まります。

335
00:20:03,160 --> 00:20:07,340
正直に言うと、私がこれを行った方法は、指をなめて風に突き刺すだけでした。

336
00:20:07,340 --> 00:20:10,800
私はソートされたリストを調べて、これらの単語の約半分が最

337
00:20:10,800 --> 00:20:15,280
終的な答えになる可能性が高いと判断できる範囲を見つけよう

338
00:20:15,280 --> 00:20:17,680
としました。そして、それをカットオフとして使用しました。

339
00:20:17,680 --> 00:20:21,840
単語全体でこのような分布が得られると、エントロピー

340
00:20:21,840 --> 00:20:24,460
が非常に有用な測定値となる別の状況が得られます。

341
00:20:24,460 --> 00:20:28,480
たとえば、ゲームをプレイしていて、羽と爪だった古

342
00:20:28,480 --> 00:20:32,480
いオープナーから始めて、それに一致する可能性のあ

343
00:20:32,480 --> 00:20:33,760
る単語が 4 つある状況に行き着いたとします。

344
00:20:33,760 --> 00:20:36,440
そして、それらはすべて同じ可能性であると考えてみましょう。

345
00:20:36,440 --> 00:20:40,000
この分布のエントロピーはいくらですか?

346
00:20:40,000 --> 00:20:45,920
さて、これらの可能性のそれぞれに関連付けられた情報は、それぞれが 1 と

347
00:20:45,920 --> 00:20:50,800
4 であり、それが 2 であるため、4 の底を 2 とする対数になります。

348
00:20:50,800 --> 00:20:52,780
2 ビットの情報、4 つの可能性。

349
00:20:52,780 --> 00:20:54,360
すべてとても順調です。

350
00:20:54,360 --> 00:20:58,320
しかし、実際には 4 つ以上の試合があると言ったらどうなるでしょうか?

351
00:20:58,320 --> 00:21:02,600
実際、完全な単語リストに目を通すと、それに一致する単語が 16 個あります。

352
00:21:02,600 --> 00:21:07,260
しかし、私たちのモデルでは、他の 12 単語が実際に最終的な答えになる確率は非

353
00:21:07,260 --> 00:21:11,440
常に低く、非常に曖昧であるため、1000 分の 1 程度であると仮定します。

354
00:21:11,440 --> 00:21:15,480
さて、この分布のエントロピーはいくらでしょうか?

355
00:21:15,480 --> 00:21:19,600
ここでエントロピーが純粋に一致の数を測定している場合、それは 16

356
00:21:19,600 --> 00:21:24,760
の底 2 の対数のようなものになると予想されるかもしれません。こ

357
00:21:24,760 --> 00:21:26,200
れは 4 となり、以前よりも 2 ビット多くの不確実性が生じます。

358
00:21:26,200 --> 00:21:30,320
しかし、もちろん、実際の不確実性は以前とそれほど変わりません。

359
00:21:30,320 --> 00:21:33,840
これらの本当にあいまいな 12 の単語があるからといって、たとえば、最

360
00:21:33,840 --> 00:21:38,200
終的な答えが魅力であると知っても、それほど驚くべきことではありません。

361
00:21:38,200 --> 00:21:42,080
したがって、ここで実際に計算を実行し、各発生の確率と対応す

362
00:21:42,080 --> 00:21:45,960
る情報を合計すると、得られる値は 2 になります。11ビット。

363
00:21:45,960 --> 00:21:50,280
私が言いたいのは、基本的には 2 つのビット、基本的には 4 つの可能性で

364
00:21:50,280 --> 00:21:54,240
すが、これらの非常にありそうもない出来事がすべてあるため、もう少し不確実性

365
00:21:54,240 --> 00:21:57,120
があります。ただし、それらを学べば、そこから大量の情報が得られるでしょう。

366
00:21:57,120 --> 00:22:00,800
ズームアウトすると、これが Wordle が情報理

367
00:22:00,800 --> 00:22:01,800
論のレッスンに最適な例となる理由の 1 つです。

368
00:22:01,800 --> 00:22:05,280
エントロピーについては、これら 2 つの異なる感覚のアプリケーションがあります。

369
00:22:05,280 --> 00:22:09,640
1 つ目は、与えられた推測から得られる期待情報は何

370
00:22:09,640 --> 00:22:14,560
かを示し、2 つ目は、考えられるすべての単語の中で

371
00:22:14,560 --> 00:22:16,480
残っている不確実性を測定できるかどうかを示します。

372
00:22:16,480 --> 00:22:19,800
そして、強調しておきたいのは、推測の期待情報を調べる最初のケースでは、単語の重

373
00:22:19,800 --> 00:22:25,000
み付けが不均等になると、それがエントロピーの計算に影響を与えるということです。

374
00:22:25,000 --> 00:22:28,600
たとえば、Weary に関連する分布について以前に検討し

375
00:22:28,600 --> 00:22:33,560
たのと同じケースを取り出してみましょう。ただし、今回はす

376
00:22:33,560 --> 00:22:34,560
べての可能な単語にわたって不均一な分布を使用しています。

377
00:22:34,560 --> 00:22:39,360
それでは、それをうまく説明している部分がここに見つかるかどうか見てみましょう。

378
00:22:39,360 --> 00:22:42,480
さて、これはかなり良いです。

379
00:22:42,480 --> 00:22:46,360
ここでは、ほぼ同じ確率の 2 つの隣接するパターンがありますが、そのうちの

380
00:22:46,360 --> 00:22:49,480
1 つは、それに一致する可能性のある単語が 32 個あると言われています。

381
00:22:49,480 --> 00:22:54,080
それらが何であるかを確認してみると、これらは 32 個であり、

382
00:22:54,080 --> 00:22:55,600
目をざっと眺めると、どれも非常にありそうもない単語ばかりです。

383
00:22:55,600 --> 00:23:00,400
もっともらしい答え、おそらく叫び声のようなものを見つけるのは難し

384
00:23:00,400 --> 00:23:04,440
いですが、ほぼ同じ確率であると考えられる分布内の隣接するパターン

385
00:23:04,440 --> 00:23:08,920
を見ると、一致する可能性は 8 つしかないと言われているため、4

386
00:23:08,920 --> 00:23:09,920
分の 1 は多くの試合がありますが、その可能性はほぼ同じです。

387
00:23:09,920 --> 00:23:12,520
それらの一致を調べてみると、その理由がわかります。

388
00:23:12,520 --> 00:23:17,840
これらの中には、リング、怒り、ラップなど、実際にもっともらしい答えもあります。

389
00:23:17,840 --> 00:23:22,000
これらすべてをどのように組み込むかを説明するために、ここで Wordlebot のバージ

390
00:23:22,000 --> 00:23:25,960
ョン 2 を取り上げます。最初に見たものとの主な違いが 2 つまたは 3 つあります。

391
00:23:25,960 --> 00:23:29,460
まず、先ほど述べたように、これらのエントロピー、つまり情報の期待

392
00:23:29,460 --> 00:23:34,800
値を計算する方法では、特定の単語が実際に答えとなる確率を組み込

393
00:23:34,800 --> 00:23:39,300
んだ、パターン全体にわたるより洗練された分布を使用しています。

394
00:23:39,300 --> 00:23:44,160
偶然にも、涙が依然としてナンバー 1 ですが、それに続くものは少し異なります。

395
00:23:44,160 --> 00:23:47,920
第 2 に、上位の候補をランク付けするときに、

396
00:23:47,920 --> 00:23:52,600
各単語が実際の答えである確率のモデルを保持し、

397
00:23:52,600 --> 00:23:55,520
それを決定に組み込むようになります。テーブル。

398
00:23:55,520 --> 00:24:01,120
繰り返しますが、私たちは機械に生活を支配させることはできないので、その推奨を無視します。

399
00:24:01,120 --> 00:24:05,160
そして、ここでもう 1 つ異なる点があることを言及する必要があると思います。左側

400
00:24:05,160 --> 00:24:10,080
は、不確実性の値、ビット数が、一致の可能性の数と重複するだけではなくなりました。

401
00:24:10,080 --> 00:24:16,520
これを引き上げて、2 対 8 を計算するとします。02、これは 256 を少し上回

402
00:24:16,520 --> 00:24:22,640
る、おそらく 259 です。これが言いたいのは、実際にこのパターンに

403
00:24:22,640 --> 00:24:26,400
一致する単語が合計 526 個あるとしても、その不確実性の量は、同

404
00:24:26,400 --> 00:24:29,760
じ可能性の 259 個があった場合の値に近いということです。結果。

405
00:24:29,760 --> 00:24:31,100
このように考えることができます。

406
00:24:31,100 --> 00:24:35,560
yorts、zorl、zorus の場合と同様に、borx が答えではな

407
00:24:35,560 --> 00:24:37,840
いことがわかっているため、前のケースよりも不確実性が少し低くなります。

408
00:24:37,840 --> 00:24:40,220
このビット数は小さくなります。

409
00:24:40,220 --> 00:24:44,040
そして、ゲームをプレイし続けると、ここで説明したいこ

410
00:24:44,040 --> 00:24:48,680
とと適切ないくつかの推測を加えてこれを洗練させます。

411
00:24:48,680 --> 00:24:52,520
4 番目の推測では、その上位の候補を見てみると、もはや

412
00:24:52,520 --> 00:24:53,800
エントロピーを最大化するだけではないことがわかります。

413
00:24:53,800 --> 00:24:58,480
つまり、現時点では技術的には 7 つの可能性がありま

414
00:24:58,480 --> 00:25:00,780
すが、意味のあるチャンスがあるのは寮と言葉だけです。

415
00:25:00,780 --> 00:25:04,760
そして、これらの両方を選択すると、厳密に言えば、より多くの情報が

416
00:25:04,760 --> 00:25:07,560
得られる他の値よりも上位にランク付けされていることがわかります。

417
00:25:07,560 --> 00:25:11,200
初めてこれを行ったとき、これら 2 つの数値を合計して各推測の

418
00:25:11,200 --> 00:25:14,580
質を測定しました。実際、これは予想以上にうまく機能しました。

419
00:25:14,580 --> 00:25:17,600
しかし、それは実際には体系的とは思えませんでした。他のアプロー

420
00:25:17,600 --> 00:25:19,880
チもあるとは思いますが、私がたどり着いたアプローチはこれです。

421
00:25:19,880 --> 00:25:24,200
この場合の単語のように、次の推測の見通しを考慮している場合、本

422
00:25:24,200 --> 00:25:28,440
当に関心があるのは、それを行った場合のゲームの予想スコアです。

423
00:25:28,440 --> 00:25:32,880
そして、その期待スコアを計算するために、単語が実際の答えである確

424
00:25:32,880 --> 00:25:35,640
率はいくらかを言います。現時点では 58% と説明されています。

425
00:25:36,080 --> 00:25:40,400
58% の確率で、このゲームのスコアは 4 になるでしょう。

426
00:25:40,400 --> 00:25:46,240
そして、1 から 58% を引いた確率で、スコアは 4 より大きくなります。

427
00:25:46,240 --> 00:25:50,640
それ以上はわかりませんが、その時点に到達したときにどの程度の不確

428
00:25:50,640 --> 00:25:52,920
実性が存在する可能性があるかに基づいて推定することはできます。

429
00:25:52,920 --> 00:25:56,600
具体的には、現時点では 1 です。44 ビットの不確実性。

430
00:25:56,600 --> 00:26:01,560
単語を推測すると、得られる期待情報が 1 であることがわかります。27ビット。

431
00:26:01,560 --> 00:26:06,280
したがって、言葉を推測した場合、この違いは、それが起こった

432
00:26:06,280 --> 00:26:08,280
後にどの程度の不確実性が残る可能性があるかを表しています。

433
00:26:08,280 --> 00:26:12,500
必要なのは、この不確実性を期待されるスコアに関連付ける、

434
00:26:12,500 --> 00:26:13,880
ある種の関数 (ここでは f と呼んでいます) です。

435
00:26:13,880 --> 00:26:18,040
そして、これを行う方法は、ボットのバージョン 1 に基づいて以前のゲー

436
00:26:18,040 --> 00:26:23,920
ムからの大量のデータをプロットして、特定の非常に測定可能な量の不確実性

437
00:26:23,920 --> 00:26:27,040
を伴うさまざまなポイント後の実際のスコアが何であるかを示すことでした。

438
00:26:27,040 --> 00:26:31,120
たとえば、これらのデータ ポイントは、8 程度の値よりも上にあります。8

439
00:26:31,120 --> 00:26:36,840
人だった時点以降、いくつかの試合では7人ほどが発言している。7 ビットの不確

440
00:26:36,840 --> 00:26:39,340
実性があり、最終的な答えを得るには 2 回の推測が必要でした。

441
00:26:39,340 --> 00:26:43,180
他のゲームでは 3 回の推測が必要で、他のゲームでは 4 回の推測が必要でした。

442
00:26:43,180 --> 00:26:46,920
ここで左にシフトすると、ゼロを超えるすべての点は、不確実性が

443
00:26:46,920 --> 00:26:51,620
ゼロの場合、つまり可能性が 1 つしかない場合、必要な推測の

444
00:26:51,620 --> 00:26:55,000
数は常に 1 つだけであり、安心できることを示しています。

445
00:26:55,000 --> 00:26:59,020
少しでも不確実性がある場合、つまり、本質的に可能性が 2 つ

446
00:26:59,020 --> 00:27:02,360
だけであることを意味する場合、さらに 1 つの推測が必要な

447
00:27:02,360 --> 00:27:03,940
場合もあれば、さらに 2 つの推測が必要な場合もあります。

448
00:27:03,940 --> 00:27:05,980
などなど。

449
00:27:05,980 --> 00:27:11,020
このデータを視覚化するもう少し簡単な方法は、データをまとめて平均を取ることかもしれません。

450
00:27:11,020 --> 00:27:15,940
たとえば、このバーは、少し不確実性があったすべてのポイントのうち、

451
00:27:15,940 --> 00:27:22,420
必要な新しい推測の数は平均して約 1 であることを示しています。5.

452
00:27:22,420 --> 00:27:25,920
そして、ここのバーは、さまざまなゲームすべての中で、ある時点で

453
00:27:25,920 --> 00:27:30,480
不確実性が 4 ビットをわずかに上回っていたことを示していま

454
00:27:30,480 --> 00:27:35,120
す。これは、16 の異なる可能性に絞り込むようなもので、その時

455
00:27:35,120 --> 00:27:36,240
点から平均して 2 つを少し超える推測が必要ですフォワード。

456
00:27:36,240 --> 00:27:40,080
ここからは、これに合理的と思われる関数を当てはめるために回帰を実行しました。

457
00:27:40,080 --> 00:27:44,160
そして、そのいずれかを行うことの要点は、単語から得られる情報が多ければ多いほど、期待され

458
00:27:44,160 --> 00:27:49,720
るスコアが低くなるという直感を定量化できるようにするためであることを忘れないでください。

459
00:27:49,720 --> 00:27:54,380
ということで、これをバージョン2とします。0、戻って同じシミュレーション セットを実行し、2

460
00:27:54,380 --> 00:27:59,820
315 個の単語の答えすべてに対して実行すると、どうなるでしょうか?

461
00:27:59,820 --> 00:28:04,060
最初のバージョンと比べると明らかに良くなっているので、安心できます。

462
00:28:04,060 --> 00:28:08,780
全体的に見て平均は 3 程度です。ただし、最初のバージョンとは異なり

463
00:28:08,780 --> 00:28:12,820
、この状況では失われ、6 つ以上が必要になることが数回あります。

464
00:28:12,820 --> 00:28:15,980
おそらく、情報を最大化するのではなく、実際に目標を達成する

465
00:28:15,980 --> 00:28:18,980
ためにトレードオフが発生する場合があるからだと思われます。

466
00:28:18,980 --> 00:28:22,140
では、3よりもうまくできるでしょうか。6?

467
00:28:22,140 --> 00:28:23,260
間違いなくできます。

468
00:28:23,260 --> 00:28:27,120
さて、冒頭で、Wordle の回答の真のリストをモデル

469
00:28:27,120 --> 00:28:29,980
の構築方法に組み入れないのが最も楽しいと言いました。

470
00:28:29,980 --> 00:28:35,180
しかし、それを組み込んだ場合、得られる最高のパフォーマンスは 3 程度でした。43.

471
00:28:35,180 --> 00:28:39,520
したがって、この事前分布を選択するために単語頻度データを使用するだけではなく、より洗練されたもの

472
00:28:39,520 --> 00:28:44,220
にしようとすると、この 3.おそらく 43 が、これでどれだけうまくできる

473
00:28:44,220 --> 00:28:46,360
か、少なくともどれくらいうまくできるかという最大値を示しています。

474
00:28:46,360 --> 00:28:50,240
その最高のパフォーマンスは、基本的には私がここで話してきたア

475
00:28:50,240 --> 00:28:53,400
イデアを使用しているだけですが、期待される情報を 1 歩では

476
00:28:53,400 --> 00:28:55,660
なく 2 歩前進して検索するなど、さらに一歩進んだものです。

477
00:28:55,660 --> 00:28:58,720
当初はそれについてもっと話すつもりでしたが、実際

478
00:28:58,720 --> 00:29:00,580
にはかなり長くなってしまったことに気づきました。

479
00:29:00,580 --> 00:29:03,520
私が言えるのは、この 2 段階の検索を行った後、上位候補でいくつかのサ

480
00:29:03,520 --> 00:29:07,720
ンプル シミュレーションを実行した結果、少なくとも私にとって今のとこ

481
00:29:07,720 --> 00:29:09,500
ろ、Crane が最良のオープナーであるように見えるということです。

482
00:29:09,500 --> 00:29:11,080
誰が予想したでしょうか？

483
00:29:11,080 --> 00:29:15,680
また、真の Wordle リストを使用して可能性の空間を決定する場

484
00:29:15,680 --> 00:29:17,920
合、最初の不確実性は 11 ビットをわずかに超える値になります。

485
00:29:18,160 --> 00:29:22,760
そして、総当たり検索から、最初の 2 つの推測の後に予想さ

486
00:29:22,760 --> 00:29:26,580
れる最大情報は約 10 ビットであることがわかりました。

487
00:29:26,580 --> 00:29:31,720
これは、最良のシナリオでは、最初の 2 つの推測の後、完全に最適なプレ

488
00:29:31,720 --> 00:29:35,220
イを行った場合、約 1 ビットの不確実性が残ることを示唆しています。

489
00:29:35,220 --> 00:29:37,400
これは、可能な推測が 2 つに絞られるのと同じです。

490
00:29:37,400 --> 00:29:41,440
したがって、この平均を 3 という低い値にするアルゴリズムを作成することは

491
00:29:41,440 --> 00:29:45,620
不可能である、と言うのは公平であり、おそらくかなり保守的だと思います。な

492
00:29:45,620 --> 00:29:50,460
ぜなら、利用可能な単語では、たった 2 つのステップで十分な情報を取得す

493
00:29:50,460 --> 00:29:53,820
る余地がないからです。毎回必ず 3 番目のスロットの答えを保証できます。

