[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy. ",
  "translatedText": "Trò chơi Wurdle đã lan truyền khá rộng rãi trong một hoặc hai tháng qua và không bao giờ người ta bỏ qua cơ hội học toán, tôi chợt nhận ra rằng trò chơi này là một ví dụ trung tâm rất hay trong bài học về lý thuyết thông tin và đặc biệt là trò chơi này. một chủ đề được gọi là entropy. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could. ",
  "translatedText": "Bạn thấy đấy, giống như nhiều người, tôi bị cuốn hút vào câu đố, và giống như nhiều lập trình viên, tôi cũng bị cuốn hút vào việc cố gắng viết một thuật toán để chơi trò chơi một cách tối ưu nhất có thể. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy. ",
  "translatedText": "Và điều tôi nghĩ tôi sẽ làm ở đây chỉ là trao đổi với bạn một số quy trình của tôi trong đó và giải thích một số phép toán liên quan đến nó, vì toàn bộ thuật toán tập trung vào ý tưởng về entropy. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle? ",
  "translatedText": "Trước hết, trong trường hợp bạn chưa từng nghe đến nó, Wurdle là gì? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us. ",
  "translatedText": "Và để một mũi tên trúng hai con chim ở đây trong khi chúng ta đi qua các quy tắc của trò chơi, hãy để tôi xem trước chúng ta sẽ đi đâu với điều này, đó là phát triển một thuật toán nhỏ về cơ bản sẽ chơi trò chơi cho chúng ta. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does. ",
  "translatedText": "Mặc dù tôi chưa thực hiện Wurdle của ngày hôm nay nhưng đây là ngày 4 tháng 2 và chúng ta sẽ xem con bot hoạt động như thế nào. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess. ",
  "translatedText": "Mục tiêu của Wurdle là đoán một từ có năm chữ cái bí ẩn và bạn có sáu cơ hội đoán khác nhau. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane. ",
  "translatedText": "Ví dụ: bot Wurdle của tôi gợi ý rằng tôi nên bắt đầu với cần cẩu đoán. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer. ",
  "translatedText": "Mỗi lần bạn đoán, bạn sẽ nhận được một số thông tin về mức độ gần đúng của suy đoán của bạn với câu trả lời đúng. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer. ",
  "translatedText": "Ở đây, hộp màu xám cho tôi biết rằng không có chữ C trong câu trả lời thực tế. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position. ",
  "translatedText": "Ô màu vàng cho tôi biết có chữ R, nhưng nó không ở vị trí đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position. ",
  "translatedText": "Ô màu xanh lá cây cho tôi biết từ bí mật có chữ A và nó ở vị trí thứ ba. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E. ",
  "translatedText": "Và rồi không có N và không có E. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information. ",
  "translatedText": "Vì vậy, hãy để tôi vào và nói với bot Wurdle thông tin đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey. ",
  "translatedText": "Chúng tôi bắt đầu với cần cẩu, chúng tôi có màu xám, vàng, xanh lá cây, xám, xám. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time. ",
  "translatedText": "Đừng lo lắng về tất cả dữ liệu mà nó đang hiển thị ngay bây giờ, tôi sẽ giải thích điều đó vào thời điểm thích hợp. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick. ",
  "translatedText": "Nhưng gợi ý hàng đầu cho lựa chọn thứ hai của chúng tôi thật tồi tệ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess. ",
  "translatedText": "Và dự đoán của bạn thực sự phải là một từ có năm chữ cái, nhưng như bạn sẽ thấy, nó khá tự do với những gì nó thực sự cho phép bạn đoán. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick. ",
  "translatedText": "Trong trường hợp này, chúng tôi thử shtick. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good. ",
  "translatedText": "Và được rồi, mọi thứ có vẻ khá tốt. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R. ",
  "translatedText": "Chúng ta nhấn chữ S và chữ H, nên chúng ta biết ba chữ cái đầu tiên, chúng ta biết rằng có chữ R. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something. ",
  "translatedText": "Và vì vậy nó sẽ giống như SHA gì đó R, hoặc SHA R gì đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp. ",
  "translatedText": "Và có vẻ như bot Wurdle biết rằng chỉ có hai khả năng, mảnh vỡ hoặc sắc nét. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a toss up between them at this point, so I guess probably just because it's alphabetical it goes with shard. ",
  "translatedText": "Vào thời điểm này, có một sự xung đột giữa chúng, vì vậy tôi đoán có lẽ chỉ vì nó được sắp xếp theo thứ tự bảng chữ cái nên nó đi kèm với phân đoạn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer. ",
  "translatedText": "Hoan hô, là câu trả lời thực tế. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.22,
  "end": 132.86
 },
 {
  "input": "So we got it in three. ",
  "translatedText": "Vì vậy, chúng tôi đã nhận được nó trong ba. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 132.96,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle four is par and three is birdie. ",
  "translatedText": "Nếu bạn đang thắc mắc liệu điều đó có tốt không, thì theo cách tôi nghe một người nói thì Wurdle bốn là par và ba là birdie. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy. ",
  "translatedText": "Mà tôi nghĩ là một sự tương tự khá thích hợp. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy. ",
  "translatedText": "Bạn phải kiên trì trong trò chơi của mình để đạt được bốn điểm, nhưng điều đó chắc chắn không điên rồ chút nào. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great. ",
  "translatedText": "Nhưng khi bạn nhận được nó trong ba, nó sẽ cảm thấy tuyệt vời. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot. ",
  "translatedText": "Vì vậy, nếu bạn không hài lòng, điều tôi muốn làm ở đây chỉ là nói về quá trình suy nghĩ của tôi ngay từ đầu về cách tôi tiếp cận bot Wurdle. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson. ",
  "translatedText": "Và như tôi đã nói, thực ra đó chỉ là cái cớ để học lý thuyết thông tin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy. ",
  "translatedText": "Mục tiêu chính là giải thích thông tin là gì và entropy là gì. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language. ",
  "translatedText": "Suy nghĩ đầu tiên của tôi khi tiếp cận vấn đề này là xem xét tần số tương đối của các chữ cái khác nhau trong tiếng Anh. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters? ",
  "translatedText": "Vì vậy, tôi nghĩ, được thôi, có một lần đoán mở đầu hoặc một cặp đoán mở đầu nào chạm được nhiều chữ cái thường gặp nhất này không? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails. ",
  "translatedText": "Và một việc mà tôi khá thích là làm những việc khác sau đó là làm móng. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good. ",
  "translatedText": "Ý nghĩ là nếu bạn nhấn vào một chữ cái, bạn biết đấy, bạn sẽ nhận được màu xanh lá cây hoặc màu vàng, điều đó luôn tạo cảm giác dễ chịu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.76,
  "end": 187.52
 },
 {
  "input": "It feels like you're getting information. ",
  "translatedText": "Có cảm giác như bạn đang nhận được thông tin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.52,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information since it's pretty rare to find a word that doesn't have any of these letters. ",
  "translatedText": "Nhưng trong những trường hợp này, ngay cả khi bạn không đánh và luôn có màu xám, điều đó vẫn cung cấp cho bạn nhiều thông tin vì rất hiếm khi tìm thấy một từ không có bất kỳ chữ cái nào trong số này. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters. ",
  "translatedText": "Nhưng dù vậy, điều đó vẫn không mang lại cảm giác siêu hệ thống, vì chẳng hạn, nó không liên quan gì đến thứ tự của các chữ cái. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail? ",
  "translatedText": "Tại sao phải gõ móng tay khi tôi có thể gõ ốc? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end? ",
  "translatedText": "Có chữ S ở cuối thì tốt hơn phải không? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure. ",
  "translatedText": "Tôi không thực sự chắc chắn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y. ",
  "translatedText": "Một người bạn của tôi nói rằng anh ấy thích mở đầu bằng từ mệt mỏi, điều này làm tôi khá ngạc nhiên vì trong đó có một số chữ cái không phổ biến như W và Y. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener. ",
  "translatedText": "Nhưng ai biết được, có lẽ đó là cách mở đầu tốt hơn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess? ",
  "translatedText": "Có loại điểm định lượng nào đó mà chúng ta có thể đưa ra để đánh giá chất lượng của một dự đoán tiềm năng không? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up. ",
  "translatedText": "Bây giờ để thiết lập cách chúng ta sắp xếp các dự đoán có thể xảy ra, hãy quay lại và thêm một chút rõ ràng về cách thiết lập chính xác trò chơi. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long. ",
  "translatedText": "Vì vậy, có một danh sách các từ mà nó sẽ cho phép bạn nhập được coi là những từ đoán hợp lệ chỉ dài khoảng 13.000 từ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble. ",
  "translatedText": "Nhưng khi bạn nhìn vào nó, có rất nhiều thứ thực sự không phổ biến, những thứ như cái đầu hay Ali và ARG, những loại từ gây ra tranh cãi trong gia đình trong trò chơi Scrabble. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word. ",
  "translatedText": "Nhưng điều thú vị của trò chơi là câu trả lời luôn là một từ khá phổ biến. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers. ",
  "translatedText": "Và trên thực tế, có một danh sách khác khoảng 2300 từ có thể là câu trả lời. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creator's girlfriend, which is kind of fun. ",
  "translatedText": "Và đây là danh sách do con người tuyển chọn, tôi nghĩ cụ thể là do bạn gái của người sáng tạo trò chơi thực hiện, điều này khá thú vị. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving Wordle that doesn't incorporate previous knowledge about this list. ",
  "translatedText": "Nhưng điều tôi muốn làm, thách thức của chúng tôi đối với dự án này là xem liệu chúng tôi có thể viết một chương trình giải Wordle mà không kết hợp kiến thức trước đây về danh sách này hay không. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list. ",
  "translatedText": "Có một điều, có rất nhiều từ có năm chữ cái khá phổ biến mà bạn sẽ không tìm thấy trong danh sách đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play Wordle against anyone, not just what happens to be the official website. ",
  "translatedText": "Vì vậy, sẽ tốt hơn nếu viết một chương trình linh hoạt hơn một chút và có thể chơi Wordle với bất kỳ ai, chứ không chỉ những gì xảy ra trên trang web chính thức. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also the reason that we know what this list of possible answers is, is because it's visible in the source code. ",
  "translatedText": "Và lý do mà chúng tôi biết danh sách các câu trả lời có thể có này là vì nó hiển thị trong mã nguồn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day. ",
  "translatedText": "Nhưng cách nó hiển thị trong mã nguồn lại theo thứ tự cụ thể mà các câu trả lời xuất hiện hàng ngày. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 292.8
 },
 {
  "input": "So you could always just look up what tomorrow's answer will be. ",
  "translatedText": "Vì vậy, bạn luôn có thể tra cứu xem câu trả lời của ngày mai sẽ là gì. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.06,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating. ",
  "translatedText": "Vì vậy, rõ ràng là việc sử dụng danh sách là gian lận. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words. ",
  "translatedText": "Và điều tạo nên một câu đố thú vị hơn và một bài học lý thuyết thông tin phong phú hơn là thay vào đó hãy sử dụng một số dữ liệu phổ quát hơn như tần số từ tương đối nói chung để nắm bắt trực giác về việc ưa thích những từ phổ biến hơn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess? ",
  "translatedText": "Vậy trong 13.000 khả năng này, chúng ta nên chọn dự đoán mở đầu như thế nào? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality? ",
  "translatedText": "Ví dụ, nếu bạn tôi đề xuất một cách mệt mỏi, chúng ta nên phân tích chất lượng của nó như thế nào? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W. ",
  "translatedText": "Chà, lý do anh ấy nói rằng anh ấy thích chữ W không chắc chắn đó là vì anh ấy thích bản chất bắn xa của cảm giác tuyệt vời như thế nào nếu bạn đánh được chữ W đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern. ",
  "translatedText": "Ví dụ: nếu mẫu đầu tiên được tiết lộ giống như thế này, thì hóa ra chỉ có 58 từ trong từ điển khổng lồ này khớp với mẫu đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000. ",
  "translatedText": "Vì vậy, đó là một mức giảm rất lớn từ 13.000. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this. ",
  "translatedText": "Nhưng mặt trái của điều đó, tất nhiên, là rất hiếm khi có được một mẫu như thế này. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000. ",
  "translatedText": "Cụ thể, nếu mỗi từ có khả năng là câu trả lời như nhau thì xác suất đạt được mẫu này sẽ là 58 chia cho khoảng 13.000. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers. ",
  "translatedText": "Tất nhiên, chúng không có khả năng là câu trả lời như nhau. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words. ",
  "translatedText": "Hầu hết trong số này là những từ rất mơ hồ và thậm chí có vấn đề. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later. ",
  "translatedText": "Nhưng ít nhất trong lần đầu tiên chúng ta vượt qua tất cả những điều này, hãy giả sử rằng chúng đều có khả năng xảy ra như nhau và sau đó tinh chỉnh điều đó một lát sau. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is the pattern with a lot of information is by its very nature unlikely to occur. ",
  "translatedText": "Vấn đề là mô hình có nhiều thông tin về bản chất khó có thể xảy ra. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely. ",
  "translatedText": "Trên thực tế, ý nghĩa của việc cung cấp nhiều thông tin là điều đó khó có thể xảy ra. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where of course there's not a W in it. ",
  "translatedText": "Một mô hình có nhiều khả năng xảy ra hơn với phần mở đầu này sẽ giống như thế này, tất nhiên là không có chữ W trong đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y. ",
  "translatedText": "Có thể có chữ E, và có thể không có chữ A, không có chữ R, không có chữ Y. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches. ",
  "translatedText": "Trong trường hợp này, có 1400 kết quả phù hợp. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see. ",
  "translatedText": "Nếu tất cả đều có khả năng như nhau thì có xác suất khoảng 11% rằng đây là mô hình bạn sẽ thấy. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative. ",
  "translatedText": "Vì vậy, những kết quả có thể xảy ra nhất cũng có ít thông tin nhất. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see. ",
  "translatedText": "Để có cái nhìn toàn diện hơn ở đây, hãy để tôi chỉ cho bạn sự phân bổ đầy đủ các xác suất trên tất cả các mẫu khác nhau mà bạn có thể thấy. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common. ",
  "translatedText": "Vì vậy, mỗi thanh bạn đang xem tương ứng với một mẫu màu có thể được tiết lộ, trong đó có từ 3 đến khả năng thứ 5 và chúng được sắp xếp từ trái sang phải, phổ biến nhất đến ít phổ biến nhất. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays. ",
  "translatedText": "Vì vậy, khả năng phổ biến nhất ở đây là bạn có toàn màu xám. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time. ",
  "translatedText": "Điều đó xảy ra khoảng 14% thời gian. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here where there's only 18 possibilities for what matches this pattern that evidently look like this. ",
  "translatedText": "Và điều bạn hy vọng khi bạn đoán là bạn sẽ kết thúc ở một nơi nào đó trong cái đuôi dài này, giống như ở đây, nơi chỉ có 18 khả năng cho những gì phù hợp với mô hình này mà rõ ràng trông như thế này. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here. ",
  "translatedText": "Hoặc nếu chúng ta mạo hiểm xa hơn một chút về phía bên trái, bạn biết đấy, có thể chúng ta sẽ đi hết quãng đường tới đây. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you. ",
  "translatedText": "Được rồi, đây là một câu đố hay dành cho bạn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them? ",
  "translatedText": "Ba từ trong tiếng Anh bắt đầu bằng chữ W, kết thúc bằng chữ Y và có chữ R ở đâu đó trong đó là gì? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly. ",
  "translatedText": "Hóa ra, câu trả lời là, hãy xem, dài dòng, sâu sắc và gượng gạo. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution. ",
  "translatedText": "Vì vậy, để đánh giá mức độ tốt của từ này nói chung, chúng tôi muốn có một số thước đo về lượng thông tin mong đợi mà bạn sẽ nhận được từ sự phân phối này. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score. ",
  "translatedText": "Nếu chúng ta xem xét từng mẫu và nhân xác suất xảy ra của nó với thứ gì đó để đo lường mức độ thông tin của nó, thì điều đó có thể cho chúng ta một điểm số khách quan. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches. ",
  "translatedText": "Bây giờ, bản năng đầu tiên của bạn về điều gì đó có thể là số lượng trận đấu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches. ",
  "translatedText": "Bạn muốn số lượng trận đấu trung bình thấp hơn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer. ",
  "translatedText": "Nhưng thay vào đó, tôi muốn sử dụng một phép đo phổ quát hơn mà chúng ta thường gán cho thông tin, và một phép đo sẽ linh hoạt hơn khi chúng ta gán một xác suất khác nhau cho mỗi từ trong số 13.000 từ này để xem liệu chúng có thực sự là câu trả lời hay không. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but it's really intuitive if we just look at examples. ",
  "translatedText": "Đơn vị thông tin tiêu chuẩn là bit, có công thức hơi buồn cười, nhưng nó thực sự trực quan nếu chúng ta chỉ nhìn vào các ví dụ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information. ",
  "translatedText": "Nếu bạn có một quan sát làm giảm một nửa không gian khả năng của bạn, thì chúng tôi nói rằng nó có một chút thông tin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about Half of the five letter words have an S, a little less than that, but about half. ",
  "translatedText": "Trong ví dụ của chúng tôi, không gian của các khả năng là tất cả các từ có thể, và hóa ra khoảng Một nửa trong số các từ có năm chữ cái có chữ S, ít hơn thế một chút, nhưng khoảng một nửa. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information. ",
  "translatedText": "Vì vậy, quan sát đó sẽ cung cấp cho bạn một chút thông tin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information. ",
  "translatedText": "Thay vào đó, nếu một sự kiện mới cắt giảm không gian khả năng đó đi bốn lần, thì chúng ta nói rằng nó có hai bit thông tin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T. ",
  "translatedText": "Ví dụ, hóa ra khoảng một phần tư những từ này có chữ T. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth. ",
  "translatedText": "Nếu sự quan sát cắt không gian đó đi tám lần, chúng ta nói đó là ba bit thông tin, v.v. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a 16th, five bits cuts it into a 32nd. ",
  "translatedText": "Bốn bit cắt nó thành phần 16, năm bit cắt nó thành phần 32. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.52
 },
 {
  "input": "So now you might want to pause and ask yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence? ",
  "translatedText": "Vì vậy, bây giờ bạn có thể muốn tạm dừng và tự hỏi, công thức thông tin về số bit theo xác suất xảy ra là gì? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 533.52,
  "end": 542.98
 },
 {
  "input": "What we're saying here is that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability. ",
  "translatedText": "Điều chúng tôi đang nói ở đây là khi bạn lấy một nửa số bit, thì nó bằng với xác suất, cũng giống như nói hai lũy thừa của số bit bằng một trên xác suất, tức là sắp xếp lại để nói rằng thông tin là log cơ số hai của một chia cho xác suất. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still, where the information is the negative log base two of the probability. ",
  "translatedText": "Và đôi khi bạn thấy điều này với một sự sắp xếp lại nữa, trong đó thông tin là log âm cơ số hai của xác suất. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half. ",
  "translatedText": "Diễn đạt như thế này, nó có thể trông hơi kỳ lạ đối với những người chưa quen, nhưng thực sự đó chỉ là một ý tưởng rất trực quan khi hỏi bạn đã cắt giảm một nửa khả năng của mình bao nhiêu lần. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture? ",
  "translatedText": "Bây giờ nếu bạn đang thắc mắc, bạn biết đấy, tôi nghĩ chúng ta chỉ đang chơi một trò chơi chữ vui nhộn, tại sao logarit lại xuất hiện trong bức tranh? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095. ",
  "translatedText": "Một lý do khiến đây là một đơn vị đẹp hơn là vì nó dễ dàng hơn rất nhiều khi nói về những sự kiện rất khó xảy ra, dễ dàng hơn nhiều khi nói rằng một quan sát có 20 bit thông tin so với việc nói rằng xác suất xảy ra điều đó và điều đó xảy ra là 0.0000095. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together. ",
  "translatedText": "Nhưng một lý do thực chất hơn khiến biểu thức logarit này hóa ra lại là một sự bổ sung rất hữu ích cho lý thuyết xác suất là cách các thông tin cộng lại với nhau. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information. ",
  "translatedText": "Ví dụ: nếu một quan sát cung cấp cho bạn hai bit thông tin, cắt giảm không gian của bạn xuống còn bốn, và sau đó quan sát thứ hai như dự đoán thứ hai của bạn trong Wordle sẽ cung cấp cho bạn ba bit thông tin khác, cắt nhỏ hơn nữa theo hệ số tám khác, thì cả hai cùng nhau cung cấp cho bạn năm thông tin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add. ",
  "translatedText": "Cũng giống như cách xác suất muốn nhân lên, thông tin cũng thích cộng thêm. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with. ",
  "translatedText": "Vì vậy, ngay khi chúng ta ở trong phạm vi của một thứ gì đó giống như giá trị kỳ vọng, nơi chúng ta cộng một loạt các số, nhật ký sẽ giúp việc xử lý dễ dàng hơn rất nhiều. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for Weary and add another little tracker on here, showing us how much information there is for each pattern. ",
  "translatedText": "Hãy quay lại bản phân phối của chúng tôi cho Weary và thêm một công cụ theo dõi nhỏ khác vào đây, cho chúng tôi biết lượng thông tin có cho mỗi mẫu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain. ",
  "translatedText": "Điều chính mà tôi muốn bạn chú ý là xác suất chúng ta đạt được những mẫu có nhiều khả năng đó càng cao thì thông tin càng thấp thì bạn thu được càng ít bit. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information, where we go through each pattern, we say how probable is it, and then we multiply that by how many bits of information do we get. ",
  "translatedText": "Cách chúng tôi đo lường chất lượng của dự đoán này là lấy giá trị kỳ vọng của thông tin này, trong đó chúng tôi xem xét từng mẫu, chúng tôi cho biết khả năng xảy ra của nó là bao nhiêu và sau đó chúng tôi nhân giá trị đó với số lượng thông tin chúng tôi nhận được. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 643.5,
  "end": 654.06
 },
 {
  "input": "And in the example of Weary, that turns out to be 4.9 bits. ",
  "translatedText": "Và trong ví dụ của Weary, kết quả là 4.9 bit. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times. ",
  "translatedText": "Vì vậy, trung bình, thông tin bạn nhận được từ lần đoán mở đầu này cũng tốt như việc cắt đôi không gian khả năng của bạn trong khoảng năm lần. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like Slate. ",
  "translatedText": "Ngược lại, một ví dụ về phỏng đoán có giá trị thông tin được mong đợi cao hơn sẽ giống như Slate. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case you'll notice the distribution looks a lot flatter. ",
  "translatedText": "Trong trường hợp này bạn sẽ nhận thấy sự phân bố trông phẳng hơn rất nhiều. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information. ",
  "translatedText": "Đặc biệt, khả năng xuất hiện nhiều nhất của tất cả các màu xám chỉ có khoảng 6% khả năng xảy ra, vì vậy rõ ràng bạn nhận được ít nhất 3.9 bit thông tin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that. ",
  "translatedText": "Nhưng đó chỉ là mức tối thiểu, thông thường bạn sẽ nhận được thứ gì đó tốt hơn thế. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8. ",
  "translatedText": "Và hóa ra là khi bạn tính toán các con số trên đây và cộng tất cả các số hạng có liên quan, thông tin trung bình là khoảng 5. số 8. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with Weary, your space of possibilities will be about half as big after this first guess, on average. ",
  "translatedText": "Vì vậy, trái ngược với Weary, trung bình không gian khả năng của bạn sẽ lớn khoảng một nửa sau lần đoán đầu tiên này. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity. ",
  "translatedText": "Thực sự có một câu chuyện thú vị về tên của giá trị kỳ vọng của lượng thông tin này. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "Information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science. ",
  "translatedText": "Lý thuyết thông tin được phát triển bởi Claude Shannon, người đang làm việc tại Bell Labs vào những năm 1940, nhưng ông ấy đang nói về một số ý tưởng chưa được công bố của mình với John von Neumann, một trí tuệ khổng lồ vào thời điểm đó, rất nổi bật. trong toán học và vật lý và sự khởi đầu của khoa học máy tính. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well you should call it entropy, and for two reasons. ",
  "translatedText": "Và khi anh ấy đề cập rằng anh ấy thực sự không có một cái tên hay cho giá trị kỳ vọng của lượng thông tin này, von Neumann được cho là đã nói, vì vậy câu chuyện diễn ra, bạn nên gọi nó là entropy, và vì hai lý do. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage. ",
  "translatedText": "Đầu tiên, hàm bất định của bạn đã được sử dụng trong cơ học thống kê dưới cái tên đó, nên nó đã có tên rồi, và thứ hai, và quan trọng hơn, không ai biết entropy thực sự là gì, vì vậy trong một cuộc tranh luận, bạn sẽ luôn luôn có lợi thế. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design. ",
  "translatedText": "Vì vậy, nếu cái tên có vẻ hơi bí ẩn và nếu câu chuyện này được tin tưởng thì đó là do thiết kế. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess. ",
  "translatedText": "Ngoài ra, nếu bạn đang thắc mắc về mối quan hệ của nó với tất cả các định luật thứ hai của nhiệt động lực học trong vật lý, thì chắc chắn có một mối liên hệ, nhưng về nguồn gốc của nó, Shannon chỉ xử lý lý thuyết xác suất thuần túy, và vì mục đích của chúng ta ở đây, khi tôi sử dụng từ entropy, tôi chỉ muốn bạn nghĩ đến giá trị thông tin mong đợi của một lần phỏng đoán cụ thể. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously. ",
  "translatedText": "Bạn có thể coi entropy như việc đo hai thứ cùng một lúc. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution. ",
  "translatedText": "Đầu tiên là mức độ phân phối bằng phẳng như thế nào. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be. ",
  "translatedText": "Sự phân bố càng gần đều thì entropy càng cao. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy. ",
  "translatedText": "Trong trường hợp của chúng tôi, khi có tổng số từ 3 đến 5 mẫu, để phân bố đồng đều, việc quan sát bất kỳ mẫu nào trong số chúng sẽ có nhật ký thông tin cơ sở 2 của 3 đến mẫu thứ 5, tức là 7.92, vậy đó là mức tối đa tuyệt đối mà bạn có thể có đối với entropy này. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place. ",
  "translatedText": "Nhưng entropy cũng là thước đo xem có bao nhiêu khả năng xảy ra ngay từ đầu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits. ",
  "translatedText": "Ví dụ: nếu bạn tình cờ có một từ nào đó trong đó chỉ có 16 mẫu có thể và mỗi mẫu đều có khả năng như nhau, thì entropy này, thông tin mong đợi này, sẽ là 4 bit. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits. ",
  "translatedText": "Nhưng nếu bạn có một từ khác trong đó có 64 mẫu có thể xuất hiện và chúng đều có khả năng như nhau, thì entropy sẽ có kết quả là 6 bit. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes. ",
  "translatedText": "Vì vậy, nếu bạn thấy một số phân phối ngoài tự nhiên có entropy 6 bit, thì điều đó giống như nói rằng có nhiều biến thể và sự không chắc chắn về những gì sắp xảy ra giống như thể có 64 kết quả có khả năng xảy ra như nhau. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this. ",
  "translatedText": "Trong lần đầu tiên tôi vượt qua Wurtelebot, về cơ bản tôi chỉ cần làm điều này. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the possible guesses you could have, all 13,000 words, computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns you might see, for each one, and picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible. ",
  "translatedText": "Nó xem xét tất cả những phỏng đoán có thể có mà bạn có thể có, tất cả 13.000 từ, tính toán entropy cho mỗi từ, hay cụ thể hơn là entropy của phân phối trên tất cả các mẫu mà bạn có thể thấy, cho mỗi mẫu và chọn mức cao nhất, vì đó là thứ có khả năng cắt giảm không gian khả năng của bạn càng nhiều càng tốt. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses. ",
  "translatedText": "Và mặc dù tôi chỉ nói về lần đoán đầu tiên ở đây, nhưng những lần đoán tiếp theo cũng diễn ra tương tự. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words. ",
  "translatedText": "Ví dụ: sau khi bạn thấy một số mẫu trong lần đoán đầu tiên đó, điều này sẽ hạn chế bạn ở số lượng từ có thể có ít hơn dựa trên những gì phù hợp với từ đó, bạn chỉ cần chơi cùng một trò chơi đối với nhóm từ nhỏ hơn đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy. ",
  "translatedText": "Đối với lần đoán thứ hai được đề xuất, bạn xem xét sự phân bố của tất cả các mẫu có thể xảy ra từ tập hợp từ hạn chế hơn đó, bạn tìm kiếm trong tất cả 13.000 khả năng và bạn tìm thấy mẫu tối đa hóa entropy đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins. ",
  "translatedText": "Để cho bạn thấy điều này hoạt động như thế nào trong thực tế, hãy để tôi đưa ra một biến thể nhỏ của Wurtele mà tôi đã viết cho thấy những điểm nổi bật của phân tích này ở bên lề. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "After doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information. ",
  "translatedText": "Sau khi thực hiện tất cả các phép tính entropy, ở bên phải nó sẽ hiển thị cho chúng ta những thông tin nào có thông tin được mong đợi cao nhất. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch. ",
  "translatedText": "Hóa ra câu trả lời hàng đầu, ít nhất là tại thời điểm này, chúng ta sẽ tinh chỉnh lại sau, là Tares, có nghĩa là, ừm, tất nhiên, đậu tằm, loại đậu tằm phổ biến nhất. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got, given this particular pattern. ",
  "translatedText": "Mỗi lần chúng ta đoán ở đây, có lẽ tôi sẽ bỏ qua các đề xuất của nó và chọn phương tiện chặn, bởi vì tôi thích phương tiện chặn, chúng ta có thể thấy nó có bao nhiêu thông tin được mong đợi, nhưng ở bên phải của từ ở đây, nó cho chúng ta thấy có bao nhiêu thông tin thông tin thực tế chúng tôi nhận được, dựa trên mẫu cụ thể này. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that. ",
  "translatedText": "Vì vậy ở đây có vẻ như chúng ta đã hơi xui xẻo một chút, chúng ta đã mong đợi nhận được 5.8, nhưng chúng tôi tình cờ nhận được thứ ít hơn thế. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now. ",
  "translatedText": "Và ở phía bên trái, nó hiển thị cho chúng ta tất cả các từ khác nhau có thể có ở vị trí hiện tại của chúng ta. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment. ",
  "translatedText": "Các thanh màu xanh lam cho chúng ta biết khả năng nó nghĩ mỗi từ là bao nhiêu, vì vậy hiện tại nó đang giả định mỗi từ đều có khả năng xảy ra như nhau, nhưng chúng ta sẽ tinh chỉnh điều đó sau. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities. ",
  "translatedText": "Và sau đó, phép đo độ không đảm bảo này cho chúng ta biết entropy của phân bố này trên các từ có thể, mà hiện tại, vì nó là phân bố đều, chỉ là một cách phức tạp không cần thiết để đếm số khả năng. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities. ",
  "translatedText": "Ví dụ: nếu chúng ta lấy 2 lũy thừa của 13.66, tức là có khoảng 13.000 khả năng. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "I'm a little bit off here, but only because I'm not showing all the decimal places. ",
  "translatedText": "Ở đây tôi hơi sai một chút, nhưng chỉ vì tôi không hiển thị tất cả các chữ số thập phân. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute. ",
  "translatedText": "Hiện tại, điều này có thể khiến bạn cảm thấy dư thừa và có vẻ như mọi thứ quá phức tạp, nhưng bạn sẽ thấy tại sao việc có cả hai con số trong một phút lại hữu ích. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Ramen, which again just really doesn't feel like a word. ",
  "translatedText": "Vì vậy, ở đây có vẻ như nó gợi ý entropy cao nhất cho lần đoán thứ hai của chúng ta là Ramen, một lần nữa nó thực sự không giống một từ nào cả. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here, I'm going to go ahead and type in Rains. ",
  "translatedText": "Vì vậy, để nâng cao nền tảng đạo đức ở đây, tôi sẽ tiếp tục và gõ Rains. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky. ",
  "translatedText": "Và một lần nữa có vẻ như chúng tôi hơi kém may mắn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information. ",
  "translatedText": "Chúng tôi đã mong đợi 4.3 bit và chúng tôi chỉ có 3.39 bit thông tin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities. ",
  "translatedText": "Vì vậy, điều đó đưa chúng ta xuống còn 55 khả năng. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means. ",
  "translatedText": "Và ở đây có lẽ tôi sẽ thực sự làm theo những gì nó gợi ý, đó là sự kết hợp, bất kể điều đó có nghĩa là gì. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And okay, this is actually a good chance for a puzzle. ",
  "translatedText": "Và được rồi, đây thực sự là một cơ hội tốt để giải câu đố. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information. ",
  "translatedText": "Nó cho chúng ta biết mẫu này cho chúng ta 4.7 bit thông tin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty. ",
  "translatedText": "Nhưng ở bên trái, trước khi chúng ta nhìn thấy mẫu đó, có 5.78 bit không chắc chắn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities? ",
  "translatedText": "Vì vậy, như một câu đố dành cho bạn, điều đó có ý nghĩa gì về số khả năng còn lại? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well, it means that we're reduced down to one bit of uncertainty, which is the same thing as saying that there's two possible answers. ",
  "translatedText": "Chà, điều đó có nghĩa là chúng ta giảm xuống còn một chút không chắc chắn, điều này cũng giống như việc nói rằng có hai câu trả lời có thể xảy ra. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice. ",
  "translatedText": "Đó là sự lựa chọn 50-50. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss. ",
  "translatedText": "Và từ đây, bởi vì bạn và tôi biết những từ nào phổ biến hơn, chúng ta biết rằng câu trả lời sẽ là vực thẳm. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that. ",
  "translatedText": "Nhưng như nó được viết bây giờ, chương trình không biết điều đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it. ",
  "translatedText": "Vì vậy, nó cứ tiếp tục, cố gắng thu thập càng nhiều thông tin càng tốt, cho đến khi chỉ còn một khả năng duy nhất, và rồi nó đoán điều đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy. ",
  "translatedText": "Vì vậy, rõ ràng là chúng ta cần một chiến lược tàn cuộc tốt hơn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1042.34
 },
 {
  "input": "But let's say we call this version one of our wordle solver, and then we go and run some simulations to see how it does. ",
  "translatedText": "Nhưng giả sử chúng tôi gọi phiên bản này là một trong những trình giải wordle của chúng tôi, sau đó chúng tôi chạy một số mô phỏng để xem nó hoạt động như thế nào. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1042.6,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game. ",
  "translatedText": "Vì vậy, cách thức hoạt động của nó là chơi mọi trò chơi chữ có thể. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers. ",
  "translatedText": "Nó sẽ trải qua tất cả 2315 từ đó là câu trả lời từng từ thực tế. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set. ",
  "translatedText": "Về cơ bản nó sử dụng nó như một bộ thử nghiệm. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice. ",
  "translatedText": "Và với phương pháp ngây thơ này là không xem xét mức độ phổ biến của một từ mà chỉ cố gắng tối đa hóa thông tin ở mỗi bước trong quá trình thực hiện, cho đến khi chỉ còn một và chỉ một lựa chọn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124. ",
  "translatedText": "Khi kết thúc mô phỏng, điểm trung bình là khoảng 4.124. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expected to do worse. ",
  "translatedText": "Điều đó không tệ, thành thật mà nói, tôi đã dự kiến sẽ làm tệ hơn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4. ",
  "translatedText": "Nhưng những người chơi wordle sẽ nói với bạn rằng họ thường có thể chơi được trong 4. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can. ",
  "translatedText": "Thử thách thực sự là lấy được càng nhiều phần 3 càng tốt. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3. ",
  "translatedText": "Đó là một bước nhảy khá lớn giữa điểm 4 và điểm 3. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that. ",
  "translatedText": "Điểm mấu chốt rõ ràng ở đây là bằng cách nào đó kết hợp xem một từ có phổ biến hay không và chính xác thì chúng ta làm điều đó như thế nào. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language. ",
  "translatedText": "Cách tôi tiếp cận là lấy danh sách tần số tương đối của tất cả các từ trong tiếng Anh. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1107.88
 },
 {
  "input": "And I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset. ",
  "translatedText": "Và tôi vừa sử dụng chức năng dữ liệu tần số từ của Mathematica, chức năng này được lấy từ tập dữ liệu công khai Ngram tiếng Anh của Google Sách. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words. ",
  "translatedText": "Và thật thú vị khi xem xét, ví dụ như nếu chúng ta sắp xếp nó từ những từ phổ biến nhất đến những từ ít phổ biến nhất. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5 letter words in the English language. ",
  "translatedText": "Rõ ràng đây là những từ có 5 chữ cái phổ biến nhất trong tiếng Anh. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common. ",
  "translatedText": "Hay đúng hơn, đây là điều phổ biến thứ 8. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there. ",
  "translatedText": "Đầu tiên là cái nào, sau đó có đó và có đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common. ",
  "translatedText": "Bản thân thứ nhất không phải là thứ nhất mà là thứ 9, và điều hợp lý là những từ khác này có thể xuất hiện thường xuyên hơn, trong đó những từ đứng sau đầu tiên là sau, ở đâu và những từ đó ít phổ biến hơn một chút. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency. ",
  "translatedText": "Bây giờ, khi sử dụng dữ liệu này để mô hình hóa khả năng mỗi từ này là câu trả lời cuối cùng, nó không nên chỉ tỷ lệ thuận với tần suất. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1146.34
 },
 {
  "input": "For example, which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely. ",
  "translatedText": "Ví dụ: được cho điểm 0.002 trong tập dữ liệu này, trong khi từ bện theo một nghĩa nào đó ít có khả năng xảy ra hơn khoảng 1000 lần. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.7,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering. ",
  "translatedText": "Nhưng cả hai đều là những từ phổ biến đến mức chúng gần như chắc chắn đáng được xem xét. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1158.84
 },
 {
  "input": "So we want more of a binary cutoff. ",
  "translatedText": "Vì vậy, chúng tôi muốn có nhiều điểm cắt nhị phân hơn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1159.34,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty. ",
  "translatedText": "Cách tôi thực hiện là tưởng tượng lấy toàn bộ danh sách các từ được sắp xếp này, sau đó sắp xếp nó theo trục x, sau đó áp dụng hàm sigmoid, đây là cách tiêu chuẩn để có một hàm có đầu ra về cơ bản là nhị phân, đó là 0 hoặc 1, nhưng có sự làm mịn ở giữa cho vùng không chắc chắn đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis. ",
  "translatedText": "Vì vậy, về cơ bản, xác suất mà tôi gán cho mỗi từ để nằm trong danh sách cuối cùng sẽ là giá trị của hàm sigmoid ở trên bất kỳ vị trí nào nó nằm trên trục x. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff. ",
  "translatedText": "Bây giờ, rõ ràng điều này phụ thuộc vào một số tham số, chẳng hạn như độ rộng của khoảng trắng trên trục x mà những từ đó điền vào sẽ xác định mức độ chúng ta giảm dần hoặc dốc từ 1 xuống 0 và vị trí chúng ta đặt chúng từ trái sang phải sẽ xác định điểm cắt. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "To be honest, the way I did this was just licking my finger and sticking it into the wind. ",
  "translatedText": "Thành thật mà nói, cách tôi làm điều này chỉ là liếm ngón tay và đưa nó theo chiều gió. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff. ",
  "translatedText": "Tôi xem qua danh sách đã sắp xếp và cố gắng tìm một cửa sổ mà khi nhìn vào nó, tôi nhận ra khoảng một nửa số từ này có nhiều khả năng là câu trả lời cuối cùng và sử dụng nó làm điểm giới hạn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement. ",
  "translatedText": "Khi chúng ta có sự phân bố như thế này trên các từ, nó sẽ cho chúng ta một tình huống khác trong đó entropy trở thành phép đo thực sự hữu ích này. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were a feather and nails, and we end up with a situation where there's four possible words that match it. ",
  "translatedText": "Ví dụ: giả sử chúng ta đang chơi một trò chơi và chúng ta bắt đầu với những từ mở đầu cũ của tôi, đó là một chiếc lông vũ và những chiếc đinh, và chúng ta kết thúc với một tình huống có thể có bốn từ phù hợp với nó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely. ",
  "translatedText": "Và giả sử chúng ta coi chúng đều có khả năng xảy ra như nhau. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1235.62
 },
 {
  "input": "Let me ask you, what is the entropy of this distribution? ",
  "translatedText": "Hãy để tôi hỏi bạn, entropy của phân phối này là gì? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1236.22,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2. ",
  "translatedText": "Chà, thông tin liên quan đến từng khả năng này sẽ là log cơ số 2 của 4, vì mỗi khả năng là 1 và 4, và đó là 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "Two bits of information, four possibilities. ",
  "translatedText": "Hai thông tin, bốn khả năng. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good. ",
  "translatedText": "Tất cả đều rất tốt và tốt. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than four matches? ",
  "translatedText": "Nhưng điều gì sẽ xảy ra nếu tôi nói với bạn rằng thực sự có nhiều hơn bốn trận đấu? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it. ",
  "translatedText": "Trên thực tế, khi chúng ta xem qua danh sách từ đầy đủ, có 16 từ phù hợp với nó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure. ",
  "translatedText": "Nhưng giả sử mô hình của chúng tôi đặt ra xác suất rất thấp cho 12 từ còn lại thực sự là câu trả lời cuối cùng, khoảng 1 trên 1000 vì chúng thực sự khó hiểu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution? ",
  "translatedText": "Bây giờ hãy để tôi hỏi bạn, entropy của phân phối này là gì? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before. ",
  "translatedText": "Nếu entropy chỉ đơn thuần là đo số lượng kết quả trùng khớp ở đây, thì bạn có thể mong đợi nó giống như log cơ số 2 của 16, tức là 4, nhiều hơn hai bit không chắc chắn so với trước đây. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before. ",
  "translatedText": "Nhưng tất nhiên, sự không chắc chắn thực tế không thực sự khác biệt so với những gì chúng ta đã có trước đây. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example. ",
  "translatedText": "Chỉ vì có 12 từ thực sự khó hiểu này không có nghĩa là sẽ ngạc nhiên hơn khi biết rằng câu trả lời cuối cùng là sự quyến rũ chẳng hạn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here, and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits. ",
  "translatedText": "Vì vậy, khi bạn thực sự thực hiện phép tính ở đây và cộng xác suất của mỗi lần xuất hiện với thông tin tương ứng, kết quả bạn nhận được là 2.11 bit. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "I'm just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it. ",
  "translatedText": "Tôi chỉ đang nói, về cơ bản nó là hai bit, về cơ bản là bốn khả năng đó, nhưng có một chút không chắc chắn hơn vì tất cả những sự kiện rất khó xảy ra đó, mặc dù nếu bạn đã tìm hiểu chúng, bạn sẽ nhận được rất nhiều thông tin từ nó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson. ",
  "translatedText": "Vì vậy, thu nhỏ, đây là một phần lý do khiến Wordle trở thành một ví dụ hay cho bài học lý thuyết thông tin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy. ",
  "translatedText": "Chúng ta có hai ứng dụng cảm nhận riêng biệt về entropy. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words that we have possible. ",
  "translatedText": "Câu đầu tiên cho chúng ta biết thông tin mong đợi mà chúng ta sẽ nhận được từ một lần phỏng đoán nhất định và câu thứ hai cho chúng ta biết liệu chúng ta có thể đo lường độ không chắc chắn còn lại trong số tất cả các từ mà chúng ta có thể có hay không. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation. ",
  "translatedText": "Và tôi nên nhấn mạnh, trong trường hợp đầu tiên khi chúng ta xem xét thông tin dự kiến của một lần đoán, một khi chúng ta có trọng số không bằng nhau đối với các từ, điều đó sẽ ảnh hưởng đến việc tính toán entropy. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words. ",
  "translatedText": "Ví dụ: hãy để tôi đưa ra trường hợp tương tự mà chúng ta đã xem xét trước đó về phân phối liên quan đến Weary, nhưng lần này sử dụng phân phối không đồng nhất trên tất cả các từ có thể. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well. ",
  "translatedText": "Vì vậy, hãy để tôi xem liệu tôi có thể tìm thấy một phần ở đây minh họa nó khá tốt không. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here this is pretty good. ",
  "translatedText": "Được rồi, ở đây khá tốt. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it. ",
  "translatedText": "Ở đây chúng ta có hai mẫu liền kề có khả năng xảy ra như nhau, nhưng một trong số chúng được cho biết có 32 từ có thể phù hợp với nó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them. ",
  "translatedText": "Và nếu chúng ta kiểm tra xem chúng là gì, thì đây là 32 từ đó, tất cả chỉ là những từ rất khó xảy ra khi bạn quét mắt qua chúng. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches, so a quarter as many matches, but it's about as likely. ",
  "translatedText": "Thật khó để tìm thấy bất kỳ câu trả lời hợp lý nào, có thể là đáng ngạc nhiên, nhưng nếu chúng ta nhìn vào mô hình lân cận trong phân phối, được coi là có khả năng xảy ra, chúng ta được biết rằng nó chỉ có 8 kết quả phù hợp có thể xảy ra, tức là một phần tư nhiều trận đấu, nhưng gần như có khả năng xảy ra. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why. ",
  "translatedText": "Và khi chúng tôi xem những trận đấu đó, chúng tôi có thể hiểu tại sao. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers, like ring, or wrath, or raps. ",
  "translatedText": "Một số trong số này là những câu trả lời thực sự hợp lý, như tiếng chuông, cơn thịnh nộ hoặc tiếng rap. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version 2 of the Wordlebot here, and there are two or three main differences from the first one that we saw. ",
  "translatedText": "Để minh họa cách chúng tôi kết hợp tất cả những điều đó, hãy để tôi đưa ra phiên bản 2 của Wordlebot ở đây và có hai hoặc ba điểm khác biệt chính so với phiên bản đầu tiên mà chúng tôi thấy. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer. ",
  "translatedText": "Trước hết, như tôi vừa nói, cách chúng ta tính toán những entropy này, những giá trị thông tin kỳ vọng này, hiện đang sử dụng những phân bố tinh tế hơn trên các mẫu kết hợp xác suất mà một từ nhất định thực sự sẽ là câu trả lời. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number 1, though the ones following are a bit different. ",
  "translatedText": "Thực tế thì nước mắt vẫn là số 1, dù những nước mắt sau có hơi khác một chút. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table. ",
  "translatedText": "Thứ hai, khi xếp hạng các lựa chọn hàng đầu, nó sẽ giữ một mô hình về xác suất mà mỗi từ là câu trả lời thực sự và nó sẽ kết hợp điều đó vào quyết định của mình, điều này sẽ dễ thấy hơn khi chúng ta có một vài dự đoán về bàn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives. ",
  "translatedText": "Một lần nữa, bỏ qua khuyến nghị của nó vì chúng ta không thể để máy móc điều khiển cuộc sống của mình. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches. ",
  "translatedText": "Và tôi cho rằng tôi nên đề cập đến một điều khác ở đây là ở bên trái, giá trị không chắc chắn đó, số bit đó, không còn dư thừa với số lượng kết quả phù hợp có thể có. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which is a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes. ",
  "translatedText": "Bây giờ nếu chúng ta kéo nó lên và tính từ 2 đến 8.02, cao hơn 256 một chút, tôi đoán là 259, điều nó nói là mặc dù có tổng cộng 526 từ thực sự khớp với mẫu này, mức độ không chắc chắn của nó gần giống với mức độ sẽ xảy ra nếu có 259 từ có khả năng như nhau kết quả. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this. ",
  "translatedText": "Bạn có thể nghĩ về nó như thế này. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borx is not the answer, same with yorts and zorl and zorus, so it's a little less uncertain than it was in the previous case. ",
  "translatedText": "Nó biết borx không phải là câu trả lời, tương tự với yorts, zorl và zorus, vì vậy nó ít chắc chắn hơn một chút so với trường hợp trước. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller. ",
  "translatedText": "Số bit này sẽ nhỏ hơn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here. ",
  "translatedText": "Và nếu tôi tiếp tục chơi trò chơi, tôi sẽ tinh chỉnh điều này bằng một vài phỏng đoán phù hợp với những gì tôi muốn giải thích ở đây. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy. ",
  "translatedText": "Đến lần đoán thứ tư, nếu bạn nhìn qua những lựa chọn hàng đầu của nó, bạn có thể thấy nó không còn chỉ tối đa hóa entropy nữa. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words. ",
  "translatedText": "Vì vậy, tại thời điểm này, về mặt kỹ thuật có bảy khả năng, nhưng những khả năng duy nhất có cơ hội có ý nghĩa là ký túc xá và từ ngữ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values, that strictly speaking would give more information. ",
  "translatedText": "Và bạn có thể thấy nó xếp hạng việc chọn cả hai giá trị đó lên trên tất cả các giá trị khác, nói đúng ra thì sẽ cung cấp thêm thông tin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect. ",
  "translatedText": "Lần đầu tiên tôi làm điều này, tôi chỉ cộng hai con số này để đo lường chất lượng của mỗi lần đoán, và chúng thực sự hoạt động tốt hơn những gì bạn có thể nghi ngờ. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic, and I'm sure there's other approaches people could take but here's the one I landed on. ",
  "translatedText": "Nhưng nó thực sự có vẻ không mang tính hệ thống và tôi chắc chắn rằng có những cách tiếp cận khác mà mọi người có thể thực hiện nhưng đây là cách tôi đã áp dụng. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that. ",
  "translatedText": "Nếu chúng ta đang xem xét khả năng xảy ra lần đoán tiếp theo, chẳng hạn như trong trường hợp này là từ ngữ, thì điều chúng ta thực sự quan tâm là điểm số kỳ vọng của trò chơi nếu chúng ta làm điều đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to. ",
  "translatedText": "Và để tính số điểm mong đợi đó, chúng tôi nói xác suất mà các từ đó là câu trả lời thực sự là bao nhiêu, hiện tại nó mô tả 58%. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be 4. ",
  "translatedText": "Chúng tôi nói với 58% cơ hội, điểm của chúng tôi trong trò chơi này sẽ là 4. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of 1 minus that 58%, our score will be more than that 4. ",
  "translatedText": "Và khi đó với xác suất 1 trừ 58% đó thì điểm của chúng ta sẽ lớn hơn 4 đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point. ",
  "translatedText": "Chúng tôi không biết còn bao nhiêu nữa, nhưng chúng tôi có thể ước tính nó dựa trên mức độ không chắc chắn có thể xảy ra khi chúng tôi đạt đến điểm đó. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment there's 1.44 bits of uncertainty. ",
  "translatedText": "Cụ thể hiện tại có 1.44 bit không chắc chắn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits. ",
  "translatedText": "Nếu chúng ta đoán các từ, nó sẽ cho chúng ta biết thông tin mong đợi mà chúng ta sẽ nhận được là 1.27 bit. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens. ",
  "translatedText": "Vì vậy, nếu chúng ta đoán từ, sự khác biệt này thể hiện mức độ không chắc chắn mà chúng ta có thể gặp phải sau khi điều đó xảy ra. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score. ",
  "translatedText": "Cái chúng ta cần là một loại hàm nào đó, mà tôi gọi là f ở đây, liên kết sự không chắc chắn này với điểm số kỳ vọng. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version 1 of the bot to say hey what was the actual score after various points with certain very measurable amounts of uncertainty. ",
  "translatedText": "Và cách nó diễn ra là chỉ vẽ một loạt dữ liệu từ các trò chơi trước dựa trên phiên bản 1 của bot để cho biết điểm thực tế sau các điểm khác nhau với mức độ không chắc chắn nhất định có thể đo lường được là bao nhiêu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer. ",
  "translatedText": "Ví dụ: những điểm dữ liệu ở đây nằm trên một giá trị khoảng 8. Khoảng 7 là nói cho một số trò chơi sau một thời điểm có 8. Có 7 điểm không chắc chắn, phải mất hai lần đoán mới có được câu trả lời cuối cùng. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games it took three guesses, for other games it took four guesses. ",
  "translatedText": "Đối với các trò chơi khác, phải mất ba lần đoán, đối với các trò chơi khác, phải mất bốn lần đoán. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring. ",
  "translatedText": "Nếu chúng ta chuyển sang bên trái ở đây, tất cả các điểm trên 0 đều cho biết bất cứ khi nào không có chút gì không chắc chắn, tức là chỉ có một khả năng, thì số lần dự đoán cần thiết luôn chỉ là một, điều này thật yên tâm. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses. ",
  "translatedText": "Bất cứ khi nào có một chút không chắc chắn, nghĩa là về cơ bản chỉ có hai khả năng xảy ra, thì đôi khi cần phải đoán thêm một lần nữa, đôi khi cần phải đoán thêm hai lần nữa. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1623.02
 },
 {
  "input": "And so on and so forth here. ",
  "translatedText": "Và vân vân và vân vân ở đây. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1623.08,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages. ",
  "translatedText": "Có lẽ cách dễ dàng hơn một chút để hình dung dữ liệu này là gộp chúng lại với nhau và lấy giá trị trung bình. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example this bar here saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5. ",
  "translatedText": "Ví dụ: thanh này ở đây cho biết trong số tất cả các điểm mà chúng tôi có một chút không chắc chắn, trung bình số lần đoán mới cần thiết là khoảng 1.5. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward. ",
  "translatedText": "Và thanh ở đây nói về tất cả các trò chơi khác nhau mà tại một thời điểm nào đó độ không chắc chắn cao hơn 4 bit một chút, giống như thu hẹp nó xuống còn 16 khả năng khác nhau, sau đó trung bình nó yêu cầu nhiều hơn hai lần đoán kể từ thời điểm đó phía trước. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this. ",
  "translatedText": "Và từ đây tôi mới thực hiện một phép hồi quy để khớp với một hàm có vẻ hợp lý với điều này. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be. ",
  "translatedText": "Và hãy nhớ rằng mục đích chung của việc thực hiện bất kỳ điều nào trong số đó là để chúng ta có thể định lượng trực giác này rằng chúng ta càng thu được nhiều thông tin từ một từ thì điểm kỳ vọng sẽ càng thấp. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So with this as version 2.0, if we go back and we run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do? ",
  "translatedText": "Vì vậy, với phiên bản này là 2.0, nếu chúng ta quay lại và chạy cùng một bộ mô phỏng, để nó đấu với tất cả 2315 câu trả lời từ có thể có, thì nó hoạt động như thế nào? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version it's definitely better, which is reassuring. ",
  "translatedText": "Ngược lại với phiên bản đầu tiên của chúng tôi, nó chắc chắn tốt hơn, điều này thật yên tâm. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done the average is around 3.6, although unlike the first version there are a couple times that it loses and requires more than six in this circumstance. ",
  "translatedText": "Tất cả đã nói và làm trung bình là khoảng 3.6, mặc dù không giống như phiên bản đầu tiên, có một vài lần nó bị mất và yêu cầu nhiều hơn sáu trong trường hợp này. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information. ",
  "translatedText": "Có lẽ bởi vì đôi khi nó thực hiện sự đánh đổi đó để thực sự đạt được mục tiêu hơn là tối đa hóa thông tin. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6? ",
  "translatedText": "Vậy chúng ta có thể làm tốt hơn 3.6? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can. ",
  "translatedText": "Chúng tôi chắc chắn có thể. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model. ",
  "translatedText": "Bây giờ tôi đã nói ngay từ đầu rằng sẽ thú vị nhất khi thử không kết hợp danh sách các câu trả lời từng từ thực sự vào cách nó xây dựng mô hình của mình. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43. ",
  "translatedText": "Nhưng nếu chúng tôi kết hợp nó, hiệu suất tốt nhất tôi có thể đạt được là khoảng 3.43. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that. ",
  "translatedText": "Vì vậy, nếu chúng ta cố gắng phức tạp hơn việc chỉ sử dụng dữ liệu tần số từ để chọn phân phối trước này, thì phân phối 3 này. 43 có lẽ cho biết mức độ tối đa mà chúng tôi có thể đạt được với điều đó, hoặc ít nhất là tôi có thể đạt được điều đó tốt đến mức nào. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one. ",
  "translatedText": "Hiệu suất tốt nhất đó về cơ bản chỉ sử dụng những ý tưởng mà tôi đã nói ở đây, nhưng nó còn đi xa hơn một chút, giống như việc tìm kiếm thông tin được mong đợi về phía trước hai bước thay vì chỉ một bước. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is. ",
  "translatedText": "Ban đầu tôi định nói nhiều hơn về vấn đề đó, nhưng tôi nhận ra rằng chúng ta thực sự đã đi khá lâu rồi. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least it's looking like Crane is the best opener. ",
  "translatedText": "Một điều tôi sẽ nói là sau khi thực hiện tìm kiếm hai bước này và sau đó chạy một vài mô phỏng mẫu trong các ứng cử viên hàng đầu, đối với tôi, ít nhất cho đến nay, có vẻ như Crane là người mở màn tốt nhất. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed? ",
  "translatedText": "Ai có thể đoán được? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits. ",
  "translatedText": "Ngoài ra, nếu bạn sử dụng danh sách từ thực sự để xác định không gian khả năng của mình, thì độ không chắc chắn mà bạn bắt đầu sẽ lớn hơn 11 bit một chút. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out, just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits. ",
  "translatedText": "Và hóa ra, chỉ từ một cuộc tìm kiếm thô bạo, thông tin mong đợi tối đa có thể có sau hai lần đoán đầu tiên là khoảng 10 bit. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty. ",
  "translatedText": "Điều này gợi ý rằng trong trường hợp tốt nhất, sau hai lần đoán đầu tiên của bạn, với lối chơi hoàn toàn tối ưu, bạn sẽ còn lại một chút không chắc chắn. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses. ",
  "translatedText": "Điều này cũng giống như việc có hai khả năng phỏng đoán. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "So I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as 3, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail. ",
  "translatedText": "Vì vậy, tôi nghĩ thật công bằng và có lẽ khá thận trọng khi nói rằng bạn không bao giờ có thể viết một thuật toán đạt mức trung bình thấp nhất là 3, bởi vì với số từ có sẵn cho bạn, đơn giản là không có đủ chỗ để có đủ thông tin chỉ sau hai bước. có thể đảm bảo câu trả lời ở ô thứ ba mọi lúc mà không thất bại. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]