[
 {
  "input": "The game Wordle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy.",
  "translatedText": " Das Spiel Wordle ist in den letzten ein oder zwei Monaten sehr populär geworden, und da ich nie eine Gelegenheit für einen Mathematikunterricht übersehe, kam mir der Gedanke, dass sich dieses Spiel sehr gut als zentrales Beispiel für eine Unterrichtsstunde über die Informationstheorie und insbesondere das Thema Entropie eignet.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could.",
  "translatedText": "Wie viele Leute bin ich in das Rätsel hineingezogen worden, und wie viele Programmierer habe ich versucht, einen Algorithmus zu schreiben, der das Spiel so optimal wie möglich spielen würde..",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy.",
  "translatedText": "Und ich dachte mir, ich erkläre dir hier einen Teil meines Vorgehens und etwas der enthaltenen Mathematik, da der gesamte Algorithmus auf dieser Idee der Entropie basiert.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wordle?",
  "translatedText": "Das Wichtigste zuerst: Falls Sie noch nichts davon gehört haben: Was ist Wordle?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us.",
  "translatedText": "Und um hier zwei Fliegen mit einer Klappe zu schlagen, während wir die Spielregeln durchgehen, möchte ich auch einen Ausblick darauf geben, worauf ich damit hinaus will. Ziel ist es einen kleinen Algorithmus zu entwickeln, der das Spiel im Grunde für uns spielt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "So, I haven't done today's Wordle, this is February 4th, and we'll see how the bot does.",
  "translatedText": "Ich habe das heutige wordle noch nicht gemacht. Heute ist es der 4. Februar und wir schauen mal, wie sich der Bot schlägt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wordle is to guess a mystery five letter word, and you're given six different chances to guess.",
  "translatedText": "Das Ziel von wordle ist es, ein geheimes Wort mit fünf Buchstaben zu erraten, und Sie haben sechs verschiedene Versuche, es zu erraten.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wordle bot suggests that I start with the guess crane.",
  "translatedText": "Beispielsweise schlägt mein wordle-Bot vor, dass ich mit \"crane\" beginnen soll.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer.",
  "translatedText": "Jedes Mal, wenn man eine Vermutung anstellt, erhält man Informationen darüber, wie nah die Eingabe an der wahren Antwort ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer.",
  "translatedText": "Hier sagt mir das graue Kästchen, dass die eigentliche Antwort kein C enthält.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position.",
  "translatedText": "Das gelbe Kästchen sagt mir, dass es zwar ein R gibt, aber es sich nicht an dieser Position befindet.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position.",
  "translatedText": "Das grüne Kästchen sagt mir, dass das geheime Wort tatsächlich ein A enthält, und zwar an dritter Stelle.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E.",
  "translatedText": "Und dann gibt es kein N und kein E.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wordle bot that information.",
  "translatedText": "Lass mich nun dem wordle-Bot diese Informationen mitteilen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey.",
  "translatedText": "Wir fingen mit crane an und erhielten Grau, Gelb, Grün, Grau, Grau.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time.",
  "translatedText": "Mach dir keine Sorgen über die Daten, die jetzt angezeigt werden, das erkläre ich zu gegebener Zeit.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick.",
  "translatedText": "Aber sein Top-Vorschlag für unsere zweite Wahl lautet shtick.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess.",
  "translatedText": "Und Ihre Antwort muss tatsächlich ein Wort mit fünf Buchstaben sein, aber wie Sie sehen werden, ist es ziemlich großzügig, was Sie tatsächlich erraten lässt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick.",
  "translatedText": "In diesem Fall versuchen wir es also mit shtick.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good.",
  "translatedText": "Und siehe da, es sieht ziemlich gut aus.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R.",
  "translatedText": "Wir geben S und H ein. Wir kennen nun die ersten drei Buchstaben und wir wissen, dass es ein R gibt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something.",
  "translatedText": "Also wird es S-H-A irgendetwas R oder S-H-A-R irgendetwas sein.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wordle bot knows that it's down to just two possibilities, either shard or sharp.",
  "translatedText": "Und es sieht so aus, als wüsste der wordle-Bot, dass es nur zwei Möglichkeiten gibt, entweder shard oder sharp.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a toss up between them at this point, so I guess probably just because it's alphabetical it goes with shard.",
  "translatedText": "Das ist im Moment eine Art Unentschieden zwischen ihnen, also nehme ich an einfach weil es alphabetisch ist, dass es shard ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer.",
  "translatedText": "welche Hurra, tatsächlich die Antwort ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 131.22,
  "end": 132.86
 },
 {
  "input": "So we got it in three.",
  "translatedText": "Also haben wir es in drei Zügen geschafft.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 132.96,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wordle four is par and three is birdie.",
  "translatedText": "Falls Sie sich fragen, ob das irgendetwas bringt: Ich habe gehört, wie eine Person es so ausgedrückt hat, dass bei wordle vier gleich Par und drei gleich Birdie sind.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy.",
  "translatedText": "Was meiner Meinung nach eine ziemlich treffende Analogie ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy.",
  "translatedText": "Um vier zu erreichen, muss man konstant sein Spiel halten, aber verrückt ist das sicher nicht.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great.",
  "translatedText": "Aber wenn man es in drei Zügen schafft, fühlt es sich einfach großartig an.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wordle bot.",
  "translatedText": "Wenn Sie also Lust darauf haben, möchte ich hier mal meinen Denkprozess von Anfang an besprechen, wie ich an den wordle-Bot herangehe.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson.",
  "translatedText": "Und wie ich schon sagte, es ist eigentlich ein Ausrede für eine Lektion in Informationstheorie.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy.",
  "translatedText": "Das Hauptziel besteht darin zu erklären, was Information und was Entropie ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language.",
  "translatedText": "Mein erster Gedanke zu diesem Thema war, einen Blick auf die relative Häufigkeit der verschiedenen Buchstaben in der englischen Sprache zu werfen",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters?",
  "translatedText": "Also dachte ich: Okay, gibt es eine Eröffnungsschätzung oder ein Eröffnungspaar, das viele dieser häufigsten Buchstaben trifft?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails.",
  "translatedText": "Und eines, das mir sehr gefiel, waren die Wörter other und danach nails ",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good.",
  "translatedText": "Der Gedanke ist, dass wenn man einen Buchstaben errät, diese dann grün oder gelb markiert werden. Das fühlt sich immer gut an",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 183.76,
  "end": 187.52
 },
 {
  "input": "It feels like you're getting information.",
  "translatedText": "Es fühlt sich an, als würdest du Informationen erhalten.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 187.52,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information since it's pretty rare to find a word that doesn't have any of these letters.",
  "translatedText": "Aber selbst wenn du in diesen Fällen kein richtigen Buchstaben erräts und nur graue Buchstaben bekommst, erhälst du dennoch viele Informationen, da es ziemlich selten ist, ein Wort zu finden, das keinen dieser Buchstaben enthält.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters.",
  "translatedText": "Aber auch das fühlt sich nicht besonders systematisch an, weil es beispielsweise nichts mit der Reihenfolge der Buchstaben zu tun hat.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail?",
  "translatedText": "Warum sollte ich nails eintippen, wenn ich auch snail eintippen könnte?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end?",
  "translatedText": "Ist es besser, das S am Ende zu haben?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure.",
  "translatedText": "Ich bin mir nicht wirklich sicher.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y.",
  "translatedText": "Nun ein Freund von mir erzählte, dass er gerne mit dem Wort weary beginnt, was mich irgendwie überraschte, weil darin einige ungewöhnliche Buchstaben wie das W und das Y enthalten sind.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener.",
  "translatedText": "Aber wer weiß, vielleicht ist das ein besserer Start.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess?",
  "translatedText": "Gibt es eine Art quantitative Bewertung, mit der wir die Qualität einer möglichen Vermutung beurteilen können?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up.",
  "translatedText": "Um nun die Art und Weise festzulegen, wie wir mögliche Vermutungen einordnen werden, gehen wir noch einmal zurück und bringen ein wenig Licht ins Dunkel, wie das Spiel genau aufgebaut ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long.",
  "translatedText": "Es gibt eine Liste von Wörtern, die Sie eingeben können und die als gültige Vermutungen gelten und die nur etwa 13.000 Wörter lang ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble.",
  "translatedText": "Aber wenn man es sich anschaut, sieht man da eine Menge wirklich ungewöhnlicher Dinge, Dinge wie einen aahed aalii und aargh, diese Art von Wörtern, die zu Familienstreitigkeiten bei einer Runde Scrabble führen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word.",
  "translatedText": "Aber die Tendenz des Spiels ist, dass die Antwort immer ein einigermaßen gebräuchliches Wort sein wird.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers.",
  "translatedText": "Und tatsächlich gibt es noch eine weitere Liste mit rund 2300 Wörtern, die mögliche Antworten darstellt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creator's girlfriend, which is kind of fun.",
  "translatedText": "Und diese Liste wurde von Menschen erstellt, ich glaube, speziell von der Freundin des Spieleentwicklers, was irgendwie lustig ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving Wordle that doesn't incorporate previous knowledge about this list.",
  "translatedText": "Aber was ich gerne machen würde, unsere Herausforderung für dieses Projekt ist zu sehen, ob wir ein  Programm schreiben können, das Wordle löst, ohne vorheriges Wissen über diese Liste zu berücksichtigen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list.",
  "translatedText": "Zum einen gibt es eine ganze Reihe von sehr gebräuchlichen Wörtern mit fünf Buchstaben, die nicht in dieser Liste zu finden sind.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play Wordle against anyone, not just what happens to be the official website.",
  "translatedText": "Daher wäre es besser, ein Programm zu schreiben, das etwas widerstandsfähiger ist und Wordle gegen jeden spielen kann, nicht nur gegen das, was zufällig die offizielle Website ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also the reason that we know what this list of possible answers is, is because it's visible in the source code.",
  "translatedText": "Und wir kennen diese Liste möglicher Antworten auch deshalb, weil sie im Quellcode sichtbar ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day.",
  "translatedText": "Aber die Art und Weise, wie es im Quellcode sichtbar ist, liegt in der spezifischen Reihenfolge, in der die Antworten von Tag zu Tag kommen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 287.0,
  "end": 292.8
 },
 {
  "input": "So you could always just look up what tomorrow's answer will be.",
  "translatedText": "Du könntest also jederzeit nachschauen, wie die Antwort für morgen lautet.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 293.06,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating.",
  "translatedText": "Es ist also klar, dass die Verwendung der Liste in gewisser Weise Betrug ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words.",
  "translatedText": "Und was zu einem interessanteren Rätsel und einer reichhaltigeren Informationstheorie-Lektion führt, ist stattdessen die Verwendung einiger universellerer Daten wie relativer Worthäufigkeiten im Allgemeinen, um die Intuition einer Vorliebe für gebräuchlichere Wörter zu erfassen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess?",
  "translatedText": "Wie sollten wir also aus diesen 13.000 Möglichkeiten den Eröffnungsvorschlag wählen?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality?",
  "translatedText": "Wenn mein Freund beispielsweise weary vorschlägt, wie sollten wir dessen Qualität analysieren?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W.",
  "translatedText": "Nun, der Grund, warum er sagte, dass er dieses unwahrscheinliche W mag, ist die Tatsache, dass das W sehr unwahrscheinlich ist. Ist das W im Wort enthalten und man hat es erraten, fühlt es sich einfach großartig an.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern.",
  "translatedText": "Zum Beispiel. wenn das erste aufgedeckte Muster so aussehen würde dann stellt sich heraus, dass es in diesem riesigen Lexikon nur 58 Wörter gibt, die diesem Muster entsprechen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000.",
  "translatedText": "Das ist also eine riesige Reduzierung von den anfangs 13.000 Wörtern.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this.",
  "translatedText": "Aber die Kehrseite davon ist natürlich, dass es sehr ungewöhnlich ist, ein solches Muster zu erhalten.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000.",
  "translatedText": "Genauer gesagt. Wenn jedes Wort mit gleicher Wahrscheinlichkeit die Antwort wäre, wäre die Wahrscheinlichkeit, dieses Muster zu treffen, 58 geteilt durch etwa 13.000.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers.",
  "translatedText": "Natürlich sind die Wörter nicht gleichermaßen wahrscheinliche Antworten.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words.",
  "translatedText": "Die meisten davon sind sehr merkwürdige und sogar fragwürdige Wörter.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later.",
  "translatedText": "Aber zumindest für unseren ersten Versuch: Lass uns annehmen, dass sie alle gleich wahrscheinlich sind, und wir verfeinern das dann etwas später.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is the pattern with a lot of information is by its very nature unlikely to occur.",
  "translatedText": "Der Punkt ist, dass es von Natur aus unwahrscheinlich ist, dass ein Muster mit vielen Informationen auftritt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely.",
  "translatedText": "Tatsächlich bedeutet informativ zu sein, dass es unwahrscheinlich ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where of course there's not a W in it.",
  "translatedText": "Ein viel wahrscheinlicheres Muster, das man bei dieser Eröffnung sehen könnte, wäre so etwas, wo natürlich kein W drin ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y.",
  "translatedText": "Vielleicht gibt es ein E, vielleicht gibt es kein A,  kein R, kein Y.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches.",
  "translatedText": "In diesem Fall gibt es 1400 mögliche Wörter.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see.",
  "translatedText": "Wenn alle gleich wahrscheinlich wären, errechnet sich daraus eine Wahrscheinlichkeit von etwa 11 %, dass du dieses Muster angezeigt bekommen würdest.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative.",
  "translatedText": "Daher sind die wahrscheinlichsten Ergebnisse auch die am wenigsten aussagekräftigen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see.",
  "translatedText": "Um hier einen umfassenderen Überblick zu erhalten, lass mich dir die vollständige Verteilung der Wahrscheinlichkeiten über alle verschiedenen Muster hinweg zeigen, die du möglicherweise siehst.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common.",
  "translatedText": "Jeder Balken, den du hier siehst, entspricht einem möglichen Farbmuster, das aufgedeckt werden könnte, von denen es 3 hoch 5 Möglichkeiten gibt, und diese sind von links nach rechts geordnet, von am häufigsten bis am seltensten.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays.",
  "translatedText": "Am Wahrscheinlichsten ist es, dass du nur Graue erhältst.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time.",
  "translatedText": "Das passiert in etwa 14 % der Fälle.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here where there's only 18 possibilities for what matches this pattern that evidently look like this.",
  "translatedText": "Woraufst du nun hoffst, wenn du ratest ist, dass du irgendwo in diesem langen Schwanz landest, so wie hier, wo es nur 18 Möglichkeiten gibt, was zu diesem Muster passt und diese Antworten enthält.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here.",
  "translatedText": "Oder wenn wir uns etwas weiter nach links wagen, vielleicht kommen wir dann bis hierher.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you.",
  "translatedText": "Okay, hier ist ein gutes Rätsel für dich.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them?",
  "translatedText": "Welche drei Wörter in der englischen Sprache beginnen mit einem W, enden mit einem Y und enthalten irgendwo ein R?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly.",
  "translatedText": "Es stellt sich heraus, dass die Antworten, mal sehen, wordy, wormy und wryly sind.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution.",
  "translatedText": "Um zu beurteilen, wie gut dieses Wort insgesamt ist, benötigen wir eine Art Maß für die erwartete Menge an Informationen, die Sie von dieser Distribution erhalten.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score.",
  "translatedText": "Wenn wir jedes Muster durchgehen und seine Auftrittswahrscheinlichkeit mit etwas multiplizieren, das misst, wie informativ es ist, kann uns das vielleicht eine objektive Bewertung geben.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches.",
  "translatedText": "Dein erster Instinkt dafür, was das sein sollte, könnte nun die Anzahl der Übereinstimmungen sein.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches.",
  "translatedText": "Du möchtest eine geringere durchschnittliche Anzahl von Übereinstimmungen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer.",
  "translatedText": "Aber Stattdessen möchte ich ein universelleres Maß verwenden, das wir Informationen oft zuschreiben und das flexibler sein wird, sobald wir jedem dieser 13.000 Wörter eine andere Wahrscheinlichkeit zugewiesen haben, um festzustellen, ob sie tatsächlich die Antwort sind oder nicht",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but it's really intuitive if we just look at examples.",
  "translatedText": "Die Standardinformationseinheit ist das Bit, dessen Formel etwas komisch ist, aber wirklich intuitiv ist, wenn wir uns nur Beispiele ansehen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information.",
  "translatedText": "Wenn Sie eine Beobachtung haben, die Ihren Möglichkeitenraum halbiert, sagen wir, dass sie eine Information enthält.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about Half of the five letter words have an S, a little less than that, but about half.",
  "translatedText": "In unserem Beispiel besteht der Raum der Möglichkeiten aus allen möglichen Wörtern, und es stellt sich heraus, dass etwa die Hälfte der Wörter mit fünf Buchstaben ein S hat, etwas weniger, aber etwa die Hälfte.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information.",
  "translatedText": "Diese Beobachtung würde Ihnen also eine kleine Information geben.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information.",
  "translatedText": "Wenn stattdessen eine neue Tatsache den Raum der Möglichkeiten um den Faktor vier verkleinert, sagen wir, dass sie zwei Informationsbits enthält.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T.",
  "translatedText": "Es stellt sich beispielsweise heraus, dass etwa ein Viertel dieser Wörter ein T hat.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth.",
  "translatedText": "Wenn die Beobachtung diesen Raum um den Faktor acht verkleinert, sagen wir, dass es sich um drei Informationsbits handelt, und so weiter und so fort.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a 16th, five bits cuts it into a 32nd.",
  "translatedText": "Vier Bits zerschneiden es in ein Sechzehntel, fünf Bits zerschneiden es in ein 32tel.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 530.9,
  "end": 533.52
 },
 {
  "input": "So now you might want to pause and ask yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence?",
  "translatedText": "Jetzt möchten Sie vielleicht innehalten und sich fragen: Wie lautet die Formel für Informationen über die Anzahl der Bits im Hinblick auf die Wahrscheinlichkeit eines Auftretens?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 533.52,
  "end": 542.98
 },
 {
  "input": "What we're saying here is that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability.",
  "translatedText": "Was wir hier sagen ist, dass wenn man die Hälfte der Anzahl der Bits hochnimmt, das dasselbe ist wie die Wahrscheinlichkeit, was dasselbe ist, als würde man sagen, dass zwei hoch die Anzahl der Bits eins über der Wahrscheinlichkeit ist, was ordnet sich weiter um und sagt, dass die Informationen die logarithmische Basis zwei von eins dividiert durch die Wahrscheinlichkeit sind.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still, where the information is the negative log base two of the probability.",
  "translatedText": "Und manchmal sieht man das noch bei einer weiteren Neuordnung, bei der die Information die negative logarithmische Basis zwei der Wahrscheinlichkeit ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half.",
  "translatedText": "So ausgedrückt, mag es für den Uneingeweihten etwas seltsam aussehen, aber es ist eigentlich nur die sehr intuitive Idee, zu fragen, wie oft man seine Möglichkeiten halbiert hat.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture?",
  "translatedText": "Wenn Sie sich jetzt fragen: Ich dachte, wir spielen nur ein lustiges Wortspiel, warum kommen dann Logarithmen ins Spiel?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095.",
  "translatedText": "Ein Grund dafür, dass dies eine schönere Einheit ist, ist, dass es einfach viel einfacher ist, über sehr unwahrscheinliche Ereignisse zu sprechen, viel einfacher zu sagen, dass eine Beobachtung 20 Bits an Informationen enthält, als zu sagen, dass die Wahrscheinlichkeit, dass dieses oder jenes eintritt, 0 ist.0000095.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together.",
  "translatedText": "Aber ein substanziellerer Grund dafür, dass sich dieser logarithmische Ausdruck als sehr nützliche Ergänzung zur Wahrscheinlichkeitstheorie erwies, ist die Art und Weise, wie Informationen addiert werden.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information.",
  "translatedText": "Wenn Ihnen beispielsweise eine Beobachtung zwei Informationsbits liefert, wodurch Ihr Platz um vier reduziert wird, und wenn Ihnen dann eine zweite Beobachtung wie Ihre zweite Schätzung in Wordle weitere drei Informationsbits liefert, wodurch Sie noch einmal um den Faktor acht reduziert werden, dann ist das der Fall zwei zusammen ergeben fünf Informationen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add.",
  "translatedText": "So wie sich Wahrscheinlichkeiten gerne vervielfachen, fügen sich auch Informationen gerne hinzu.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with.",
  "translatedText": "Sobald wir uns also im Bereich eines erwarteten Werts befinden, bei dem wir eine Reihe von Zahlen addieren, ist der Umgang mit den Protokollen viel einfacher.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for Weary and add another little tracker on here, showing us how much information there is for each pattern.",
  "translatedText": "Kehren wir zu unserer Verteilung für Weary zurück und fügen hier einen weiteren kleinen Tracker hinzu, der uns zeigt, wie viele Informationen für jedes Muster vorhanden sind.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain.",
  "translatedText": "Ich möchte Sie vor allem darauf aufmerksam machen, dass je höher die Wahrscheinlichkeit ist, wenn wir zu diesen wahrscheinlicheren Mustern gelangen, je niedriger die Informationen sind, desto weniger Bits gewinnen Sie.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information, where we go through each pattern, we say how probable is it, and then we multiply that by how many bits of information do we get.",
  "translatedText": "Wir messen die Qualität dieser Vermutung, indem wir den erwarteten Wert dieser Informationen nehmen, indem wir jedes Muster durchgehen, sagen, wie wahrscheinlich es ist, und diesen dann mit der Anzahl der Informationen multiplizieren, die wir erhalten.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 643.5,
  "end": 654.06
 },
 {
  "input": "And in the example of Weary, that turns out to be 4.9 bits.",
  "translatedText": "Und im Beispiel von Weary sind es 4.9 Bit.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times.",
  "translatedText": "Im Durchschnitt sind die Informationen, die Sie aus dieser Eröffnungsvermutung erhalten, so gut, als würden Sie Ihren Raum an Möglichkeiten etwa fünfmal halbieren.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like Slate.",
  "translatedText": "Im Gegensatz dazu wäre ein Beispiel für eine Schätzung mit einem höheren erwarteten Informationswert so etwas wie Slate.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case you'll notice the distribution looks a lot flatter.",
  "translatedText": "In diesem Fall werden Sie feststellen, dass die Verteilung viel flacher aussieht.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information.",
  "translatedText": "Insbesondere beträgt die Eintrittswahrscheinlichkeit des wahrscheinlichsten aller Grautöne nur etwa 6 %, Sie erhalten also offensichtlich mindestens 3.9 Bits an Informationen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that.",
  "translatedText": "Aber das ist ein Minimum, normalerweise bekommt man etwas Besseres.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8.",
  "translatedText": "Und es stellt sich heraus, dass die durchschnittliche Information, wenn man die Zahlen zu diesem Thema auswertet und alle relevanten Begriffe addiert, bei etwa 5 liegt.8.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with Weary, your space of possibilities will be about half as big after this first guess, on average.",
  "translatedText": "Im Gegensatz zu Weary wird Ihr Spielraum an Möglichkeiten also nach dieser ersten Vermutung im Durchschnitt etwa halb so groß sein.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity.",
  "translatedText": "Es gibt tatsächlich eine lustige Geschichte zum Namen für diesen erwarteten Wert der Informationsmenge.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "Information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science.",
  "translatedText": "Die Informationstheorie wurde von Claude Shannon entwickelt, der in den 1940er Jahren an den Bell Labs arbeitete, aber er sprach über einige seiner noch nicht veröffentlichten Ideen mit John von Neumann, dem damals prominenten intellektuellen Giganten in Mathematik und Physik und die Anfänge dessen, was später zur Informatik wurde.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well you should call it entropy, and for two reasons.",
  "translatedText": "Und als er erwähnte, dass er keinen wirklich guten Namen für diesen erwarteten Wert der Informationsmenge hatte, sagte von Neumann angeblich, man sollte es Entropie nennen, und das aus zwei Gründen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage.",
  "translatedText": "Erstens wurde Ihre Unsicherheitsfunktion in der statistischen Mechanik unter diesem Namen verwendet, sie hat also bereits einen Namen, und zweitens, und was noch wichtiger ist, weiß niemand, was Entropie wirklich ist, also werden Sie es in einer Debatte immer tun den Vorteil haben.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design.",
  "translatedText": "Wenn der Name also etwas mysteriös erscheint und man dieser Geschichte Glauben schenken darf, dann ist das sozusagen Absicht.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess.",
  "translatedText": "Auch wenn Sie sich über seine Beziehung zu all dem zweiten Hauptsatz der Thermodynamik aus der Physik wundern, gibt es definitiv einen Zusammenhang, aber in seinen Ursprüngen beschäftigte sich Shannon nur mit der reinen Wahrscheinlichkeitstheorie, und für unsere Zwecke hier, wenn ich das verwende Beim Wort Entropie möchte ich Ihnen nur den erwarteten Informationswert einer bestimmten Vermutung vorstellen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously.",
  "translatedText": "Man kann sich Entropie als die gleichzeitige Messung zweier Dinge vorstellen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution.",
  "translatedText": "Die erste Frage ist, wie flach die Verteilung ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be.",
  "translatedText": "Je näher eine Verteilung an der Gleichmäßigkeit liegt, desto höher ist die Entropie.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy.",
  "translatedText": "In unserem Fall, in dem es insgesamt 3 bis 5 Muster gibt, würde die Beobachtung eines beliebigen Musters für eine gleichmäßige Verteilung die Informationsprotokollbasis 2 von 3 bis 5 ergeben, was zufällig 7 ist.92, das ist also das absolute Maximum, das man für diese Entropie erreichen könnte.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place.",
  "translatedText": "Aber die Entropie ist auch eine Art Maß dafür, wie viele Möglichkeiten es überhaupt gibt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits.",
  "translatedText": "Wenn Sie beispielsweise ein Wort haben, bei dem es nur 16 mögliche Muster gibt und jedes davon gleich wahrscheinlich ist, beträgt diese Entropie, diese erwartete Information, 4 Bits.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits.",
  "translatedText": "Aber wenn Sie ein anderes Wort haben, bei dem 64 mögliche Muster auftauchen könnten und alle gleich wahrscheinlich sind, dann würde die Entropie 6 Bit betragen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes.",
  "translatedText": "Wenn Sie also irgendwo in freier Wildbahn eine Verteilung sehen, die eine Entropie von 6 Bit hat, dann ist das so, als ob das so wäre, als ob es so viel Variation und Ungewissheit darüber gibt, was passieren wird, als ob es 64 gleich wahrscheinliche Ergebnisse gäbe.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this.",
  "translatedText": "Bei meinem ersten Durchgang beim Wurtelebot ließ ich es im Grunde einfach so machen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the possible guesses you could have, all 13,000 words, computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns you might see, for each one, and picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible.",
  "translatedText": "Es geht alle möglichen Vermutungen durch, alle 13.000 Wörter, berechnet die Entropie für jedes einzelne, oder genauer gesagt, die Entropie der Verteilung über alle Muster, die Sie möglicherweise sehen, für jedes einzelne und wählt das höchste aus, denn das ist so diejenige, die Ihren Raum an Möglichkeiten wahrscheinlich so weit wie möglich einschränkt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses.",
  "translatedText": "Und obwohl ich hier nur über die erste Vermutung gesprochen habe, gilt das Gleiche auch für die nächsten paar Vermutungen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words.",
  "translatedText": "Wenn Sie beispielsweise bei dieser ersten Vermutung ein Muster erkennen, das Sie auf eine kleinere Anzahl möglicher Wörter beschränkt, je nachdem, was damit übereinstimmt, spielen Sie einfach dasselbe Spiel mit Bezug auf diese kleinere Gruppe von Wörtern.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy.",
  "translatedText": "Für eine vorgeschlagene zweite Vermutung betrachten Sie die Verteilung aller Muster, die aus dieser eingeschränkteren Menge von Wörtern auftreten könnten, durchsuchen alle 13.000 Möglichkeiten und finden diejenige, die diese Entropie maximiert.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins.",
  "translatedText": "Um Ihnen zu zeigen, wie das in der Praxis funktioniert, möchte ich einfach eine kleine Variante von Wurtele aufrufen, die ich geschrieben habe und die am Rand die Höhepunkte dieser Analyse zeigt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "After doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information.",
  "translatedText": "Nachdem wir alle Entropieberechnungen durchgeführt haben, zeigt es uns hier rechts, welche die höchsten erwarteten Informationen haben.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch.",
  "translatedText": "Es stellt sich heraus, dass die beste Antwort, zumindest im Moment, wir werden das später verfeinern, Tares ist, was, ähm, natürlich, eine Wicke bedeutet, die häufigste Wicke.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got, given this particular pattern.",
  "translatedText": "Jedes Mal, wenn wir hier eine Vermutung anstellen, bei der ich vielleicht die Empfehlungen ignoriere und mich für Slate entscheide, weil ich Slate mag, können wir sehen, wie viele erwartete Informationen es hatte, aber rechts vom Wort wird uns dann angezeigt, wie viele Tatsächliche Informationen, die wir aufgrund dieses besonderen Musters erhalten haben.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that.",
  "translatedText": "Hier sieht es also so aus, als hätten wir etwas Pech gehabt, man hatte erwartet, dass wir 5 bekommen.8, aber wir haben zufällig etwas mit weniger bekommen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now.",
  "translatedText": "Und dann zeigt es uns auf der linken Seite alle möglichen Wörter, je nachdem, wo wir uns gerade befinden.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment.",
  "translatedText": "Die blauen Balken geben an, wie wahrscheinlich es ist, dass jedes Wort vorkommt. Im Moment geht es also davon aus, dass jedes Wort mit gleicher Wahrscheinlichkeit vorkommt, aber wir werden das gleich verfeinern.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities.",
  "translatedText": "Und dann sagt uns diese Unsicherheitsmessung die Entropie dieser Verteilung über die möglichen Wörter, was im Moment, weil es eine gleichmäßige Verteilung ist, nur eine unnötig komplizierte Methode ist, die Anzahl der Möglichkeiten zu zählen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities.",
  "translatedText": "Wenn wir zum Beispiel 2 hoch 13 nehmen würden.66, das dürften etwa 13.000 Möglichkeiten sein.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "I'm a little bit off here, but only because I'm not showing all the decimal places.",
  "translatedText": "Ich bin hier etwas daneben, aber nur, weil ich nicht alle Dezimalstellen zeige.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute.",
  "translatedText": "Im Moment mag das überflüssig wirken und die Sache zu kompliziert machen, aber Sie werden sehen, warum es nützlich ist, beide Zahlen in einer Minute zu haben.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Ramen, which again just really doesn't feel like a word.",
  "translatedText": "Hier scheint es also darauf hinzudeuten, dass die höchste Entropie für unsere zweite Vermutung Ramen ist, was sich wiederum einfach nicht wie ein Wort anfühlt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here, I'm going to go ahead and type in Rains.",
  "translatedText": "Um hier also den moralischen Standpunkt zu vertreten, tippe ich Rains ein.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky.",
  "translatedText": "Und wieder sieht es so aus, als hätten wir etwas Pech gehabt.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information.",
  "translatedText": "Wir hatten 4 erwartet.3 Bits und wir haben nur 3.39 Bit Informationen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities.",
  "translatedText": "Damit kommen wir auf 55 Möglichkeiten.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means.",
  "translatedText": "Und hier werde ich vielleicht einfach dem folgen, was es vorschlägt, nämlich Combo, was auch immer das bedeutet.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And okay, this is actually a good chance for a puzzle.",
  "translatedText": "Und okay, das ist tatsächlich eine gute Chance für ein Rätsel.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information.",
  "translatedText": "Es sagt uns, dass dieses Muster uns 4 gibt.7 Bits an Informationen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty.",
  "translatedText": "Aber bevor wir dieses Muster sehen, waren es auf der linken Seite fünf.78 Bit Unsicherheit.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities?",
  "translatedText": "Als Quiz für Sie: Was bedeutet das über die Anzahl der verbleibenden Möglichkeiten?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well, it means that we're reduced down to one bit of uncertainty, which is the same thing as saying that there's two possible answers.",
  "translatedText": "Nun, es bedeutet, dass wir auf ein bisschen Unsicherheit reduziert sind, was dasselbe ist, als würde man sagen, dass es zwei mögliche Antworten gibt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice.",
  "translatedText": "Es ist eine 50:50-Wahl.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss.",
  "translatedText": "Und da Sie und ich wissen, welche Wörter gebräuchlicher sind, wissen wir, dass die Antwort „Abgrund“ lauten sollte.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that.",
  "translatedText": "Aber so wie es gerade geschrieben steht, weiß das Programm das nicht.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it.",
  "translatedText": "Also macht es einfach weiter und versucht, so viele Informationen wie möglich zu sammeln, bis nur noch eine Möglichkeit übrig ist, und dann errät es es.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy.",
  "translatedText": "Wir brauchen also offensichtlich eine bessere Endspielstrategie.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1042.34
 },
 {
  "input": "But let's say we call this version one of our wordle solver, and then we go and run some simulations to see how it does.",
  "translatedText": "Aber nehmen wir an, wir nennen diese Version einen unserer Wortlöser und führen dann einige Simulationen durch, um zu sehen, wie es funktioniert.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1042.6,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game.",
  "translatedText": "Das funktioniert also so, dass jedes mögliche Weltspiel gespielt wird.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers.",
  "translatedText": "Es geht darum, alle 2315 Wörter durchzugehen, die die eigentlichen Wortantworten sind.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set.",
  "translatedText": "Im Grunde wird es als Testset verwendet.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice.",
  "translatedText": "Und mit dieser naiven Methode, nicht darüber nachzudenken, wie häufig ein Wort ist, und einfach zu versuchen, die Informationen bei jedem Schritt auf dem Weg zu maximieren, bis es nur noch eine einzige Wahlmöglichkeit gibt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124.",
  "translatedText": "Am Ende der Simulation liegt die durchschnittliche Punktzahl bei etwa 4.124.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expected to do worse.",
  "translatedText": "Was nicht schlecht ist, um ehrlich zu sein, ich hatte irgendwie damit gerechnet, dass es schlechter abschneiden würde.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4.",
  "translatedText": "Aber die Leute, die Wordle spielen, werden Ihnen sagen, dass sie es normalerweise in 4 Minuten schaffen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can.",
  "translatedText": "Die eigentliche Herausforderung besteht darin, in 3 so viele wie möglich zu bekommen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3.",
  "translatedText": "Es ist ein ziemlich großer Sprung zwischen der Punktzahl 4 und der Punktzahl 3.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that.",
  "translatedText": "Die offensichtliche, niedrig hängende Frucht besteht hier darin, irgendwie einzubeziehen, ob ein Wort gebräuchlich ist oder nicht, und wie wir das genau machen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language.",
  "translatedText": "Mein Ansatz besteht darin, eine Liste der relativen Häufigkeiten aller Wörter in der englischen Sprache zu erhalten.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1102.8,
  "end": 1107.88
 },
 {
  "input": "And I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset.",
  "translatedText": "Und ich habe gerade die Worthäufigkeitsdatenfunktion von Mathematica verwendet, die ihrerseits aus dem öffentlichen Ngram-Datensatz für Englisch von Google Books stammt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1108.22,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words.",
  "translatedText": "Und es macht irgendwie Spaß, es anzusehen, wenn wir es zum Beispiel von den häufigsten Wörtern zu den am wenigsten häufigen Wörtern sortieren.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5 letter words in the English language.",
  "translatedText": "Offensichtlich sind dies die häufigsten Wörter mit fünf Buchstaben in der englischen Sprache.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common.",
  "translatedText": "Oder besser gesagt, dies ist die achthäufigste.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there.",
  "translatedText": "Zuerst ist which, danach gibt es there und there.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common.",
  "translatedText": "First selbst ist nicht first, sondern 9th, und es macht Sinn, dass diese anderen Wörter häufiger vorkommen könnten, wobei die Worte nach first nach, where sind und jene nur etwas seltener vorkommen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency.",
  "translatedText": "Wenn wir diese Daten nun verwenden, um zu modellieren, wie wahrscheinlich es ist, dass jedes dieser Wörter die endgültige Antwort ist, sollten sie nicht nur proportional zur Häufigkeit sein.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1146.34
 },
 {
  "input": "For example, which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely.",
  "translatedText": "Zum Beispiel, was mit einer Punktzahl von 0 bewertet wird.002 in diesem Datensatz, während das Wort „zopf“ in gewissem Sinne etwa 1000-mal unwahrscheinlicher ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1146.7,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering.",
  "translatedText": "Aber beide Wörter sind so häufig, dass sie mit ziemlicher Sicherheit eine Überlegung wert sind.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1155.56,
  "end": 1158.84
 },
 {
  "input": "So we want more of a binary cutoff.",
  "translatedText": "Wir wollen also eher einen binären Cutoff.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1159.34,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty.",
  "translatedText": "Meine Vorgehensweise besteht darin, mir vorzustellen, dass ich diese gesamte sortierte Liste von Wörtern nehme, sie dann auf einer x-Achse anordne und dann die Sigmoidfunktion anwende, was die Standardmethode für eine Funktion ist, deren Ausgabe grundsätzlich binär ist entweder 0 oder 1, aber dazwischen gibt es für diesen Unsicherheitsbereich eine Glättung.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis.",
  "translatedText": "Im Wesentlichen ist die Wahrscheinlichkeit, die ich jedem Wort für die Aufnahme in die endgültige Liste zuordne, der Wert der Sigmoidfunktion oben, wo auch immer sie sich auf der x-Achse befindet.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff.",
  "translatedText": "Dies hängt natürlich von einigen Parametern ab. Beispielsweise bestimmt die Breite des Raums auf der X-Achse, den diese Wörter ausfüllen, wie allmählich oder steil wir von 1 auf 0 abfallen, und wo wir sie von links nach rechts positionieren, bestimmt die Grenze.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "To be honest, the way I did this was just licking my finger and sticking it into the wind.",
  "translatedText": "Um ehrlich zu sein, habe ich das einfach so gemacht, indem ich meinen Finger abgeleckt und ihn in den Wind gehalten habe.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff.",
  "translatedText": "Ich habe die sortierte Liste durchgesehen und versucht, ein Fenster zu finden, in dem ich beim Betrachten davon ausgegangen bin, dass etwa die Hälfte dieser Wörter mit größerer Wahrscheinlichkeit die endgültige Antwort sein werden, und habe dies als Grenzwert verwendet.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement.",
  "translatedText": "Sobald wir eine solche Verteilung über die Wörter haben, ergibt sich eine weitere Situation, in der die Entropie zu diesem wirklich nützlichen Maß wird.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were a feather and nails, and we end up with a situation where there's four possible words that match it.",
  "translatedText": "Nehmen wir zum Beispiel an, wir spielen ein Spiel und beginnen mit meinen alten Eröffnungsworten, die eine Feder und Nägel waren, und enden in einer Situation, in der es vier mögliche Wörter gibt, die dazu passen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely.",
  "translatedText": "Nehmen wir an, wir halten sie alle für gleich wahrscheinlich.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1233.56,
  "end": 1235.62
 },
 {
  "input": "Let me ask you, what is the entropy of this distribution?",
  "translatedText": "Ich frage Sie: Wie groß ist die Entropie dieser Verteilung?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1236.22,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2.",
  "translatedText": "Nun, die Informationen, die jeder dieser Möglichkeiten zugeordnet sind, werden die Logbasis 2 von 4 sein, da jede davon 1 und 4 ist, und das ist 2.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "Two bits of information, four possibilities.",
  "translatedText": "Zwei Informationen, vier Möglichkeiten.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good.",
  "translatedText": "Alles sehr schön und gut.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than four matches?",
  "translatedText": "Aber was wäre, wenn ich Ihnen sagen würde, dass es tatsächlich mehr als vier Spiele sind?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it.",
  "translatedText": "Wenn wir die vollständige Wortliste durchsehen, finden wir in Wirklichkeit 16 Wörter, die dazu passen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure.",
  "translatedText": "Aber nehmen wir an, dass unser Modell den anderen 12 Wörtern eine sehr geringe Wahrscheinlichkeit zuordnet, tatsächlich die endgültige Antwort zu sein, etwa 1 zu 1000, weil sie wirklich unklar sind.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution?",
  "translatedText": "Nun möchte ich Sie fragen: Wie hoch ist die Entropie dieser Verteilung?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before.",
  "translatedText": "Wenn die Entropie hier nur die Anzahl der Übereinstimmungen messen würde, könnte man erwarten, dass sie etwa der Logarithmusbasis 2 von 16 entspricht, was 4 wäre, zwei Bits mehr Unsicherheit als zuvor.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before.",
  "translatedText": "Aber natürlich unterscheidet sich die tatsächliche Unsicherheit nicht wirklich von der, die wir zuvor hatten.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example.",
  "translatedText": "Nur weil es diese 12 wirklich obskuren Wörter gibt, heißt das nicht, dass es umso überraschender wäre, zu erfahren, dass die endgültige Antwort zum Beispiel Charme ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here, and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits.",
  "translatedText": "Wenn Sie also die Berechnung hier tatsächlich durchführen und die Wahrscheinlichkeit jedes Auftretens mal die entsprechenden Informationen addieren, erhalten Sie 2.11 Bit.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "I'm just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it.",
  "translatedText": "Ich sage nur, es sind im Grunde genommen zwei Teile, im Grunde genommen diese vier Möglichkeiten, aber aufgrund all dieser höchst unwahrscheinlichen Ereignisse gibt es etwas mehr Unsicherheit, obwohl man, wenn man sie erfahren würde, eine Menge Informationen daraus gewinnen würde.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson.",
  "translatedText": "Wenn man also herauszoomt, ist dies ein Teil dessen, was Wordle zu einem so schönen Beispiel für eine Lektion in Informationstheorie macht.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy.",
  "translatedText": "Wir haben diese beiden unterschiedlichen Gefühlsanwendungen für Entropie.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words that we have possible.",
  "translatedText": "Der erste sagt uns, welche Informationen wir von einer gegebenen Vermutung erwarten, und der zweite sagt, können wir die verbleibende Unsicherheit unter allen Wörtern messen, die uns zur Verfügung stehen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation.",
  "translatedText": "Und ich sollte betonen: Im ersten Fall, in dem wir die erwarteten Informationen einer Vermutung betrachten, wirkt sich dies auf die Entropieberechnung aus, sobald wir eine ungleiche Gewichtung der Wörter haben.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words.",
  "translatedText": "Lassen Sie mich zum Beispiel den gleichen Fall der mit „Weary“ verbundenen Verteilung aufgreifen, den wir zuvor betrachtet haben, diesmal jedoch unter Verwendung einer ungleichmäßigen Verteilung über alle möglichen Wörter.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well.",
  "translatedText": "Lassen Sie mich sehen, ob ich hier einen Teil finde, der es ziemlich gut veranschaulicht.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here this is pretty good.",
  "translatedText": "Okay, hier ist das ziemlich gut.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it.",
  "translatedText": "Hier haben wir zwei benachbarte Muster, die ungefähr gleich wahrscheinlich sind, aber für eines davon haben wir erfahren, dass es 32 mögliche Wörter gibt, die dazu passen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them.",
  "translatedText": "Und wenn wir nachsehen, was sie sind, sind das diese 32, die allesamt nur sehr unwahrscheinliche Wörter sind, wenn man sie mit den Augen überfliegt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches, so a quarter as many matches, but it's about as likely.",
  "translatedText": "Es ist schwer, irgendwelche zu finden, die sich wie plausible Antworten anfühlen, vielleicht Schreie, aber wenn wir uns das benachbarte Muster in der Verteilung ansehen, das als ungefähr genauso wahrscheinlich gilt, wird uns gesagt, dass es nur 8 mögliche Übereinstimmungen gibt, also ein Viertel viele Übereinstimmungen, aber es ist ungefähr genauso wahrscheinlich.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1375.84,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why.",
  "translatedText": "Und wenn wir diese Übereinstimmungen abrufen, können wir sehen, warum.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers, like ring, or wrath, or raps.",
  "translatedText": "Einige davon sind tatsächlich plausible Antworten, wie „Ring“, „Wrath“ oder „Raps“.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version 2 of the Wordlebot here, and there are two or three main differences from the first one that we saw.",
  "translatedText": "Um zu veranschaulichen, wie wir das alles integrieren, möchte ich hier Version 2 des Wordlebot aufrufen. Es gibt zwei oder drei Hauptunterschiede zur ersten Version, die wir gesehen haben.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1397.9,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer.",
  "translatedText": "Zunächst einmal verwendet die Art und Weise, wie wir diese Entropien, diese erwarteten Informationswerte, berechnen, wie ich gerade sagte, jetzt die verfeinerten Verteilungen über die Muster hinweg, die die Wahrscheinlichkeit berücksichtigen, dass ein bestimmtes Wort tatsächlich die Antwort wäre.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number 1, though the ones following are a bit different.",
  "translatedText": "Zufällig sind Tränen immer noch die Nummer 1, obwohl die folgenden etwas anders sind.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table.",
  "translatedText": "Zweitens wird es bei der Rangfolge seiner Top-Tipps nun ein Modell der Wahrscheinlichkeit behalten, dass jedes Wort die tatsächliche Antwort ist, und es wird dies in seine Entscheidung einbeziehen, was leichter zu erkennen ist, wenn wir ein paar Vermutungen dazu haben Tisch.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives.",
  "translatedText": "Auch hier ignorieren wir die Empfehlung, weil wir nicht zulassen können, dass Maschinen unser Leben bestimmen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches.",
  "translatedText": "Und ich denke, ich sollte noch etwas erwähnen, das hier links anders ist: Der Unsicherheitswert, diese Anzahl von Bits, ist nicht mehr nur redundant mit der Anzahl möglicher Übereinstimmungen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which is a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes.",
  "translatedText": "Wenn wir es jetzt hochziehen und 2 hoch 8 berechnen.02, was etwas über 256 liegt, ich schätze 259. Was damit gesagt wird, ist, dass, obwohl es insgesamt 526 Wörter gibt, die diesem Muster tatsächlich entsprechen, das Ausmaß der Unsicherheit eher dem entspricht, das es wäre, wenn es 259 mit gleicher Wahrscheinlichkeit gäbe Ergebnisse.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this.",
  "translatedText": "Man kann es sich so vorstellen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borx is not the answer, same with yorts and zorl and zorus, so it's a little less uncertain than it was in the previous case.",
  "translatedText": "Es weiß, dass Borx nicht die Antwort ist, das Gleiche gilt für Yorts, Zorl und Zorus, daher ist es etwas weniger unsicher als im vorherigen Fall.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1471.02,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller.",
  "translatedText": "Diese Anzahl von Bits wird kleiner sein.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here.",
  "translatedText": "Und wenn ich das Spiel weiter spiele, verfeinere ich dies mit ein paar Vermutungen, die zu dem passen, was ich hier erklären möchte.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy.",
  "translatedText": "Bei der vierten Vermutung, wenn Sie einen Blick auf die Top-Picks werfen, können Sie erkennen, dass es nicht mehr nur um die Maximierung der Entropie geht.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words.",
  "translatedText": "An diesem Punkt gibt es also technisch gesehen sieben Möglichkeiten, aber die einzigen mit einer sinnvollen Chance sind Schlafsäle und Worte.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values, that strictly speaking would give more information.",
  "translatedText": "Und Sie können sehen, dass es wichtiger ist, diese beiden Werte zu wählen als alle anderen Werte, die streng genommen mehr Informationen liefern würden.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect.",
  "translatedText": "Als ich das zum ersten Mal gemacht habe, habe ich einfach diese beiden Zahlen addiert, um die Qualität jeder Vermutung zu messen, was tatsächlich besser funktioniert hat, als Sie vielleicht vermuten.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic, and I'm sure there's other approaches people could take but here's the one I landed on.",
  "translatedText": "Aber es fühlte sich wirklich nicht systematisch an, und ich bin mir sicher, dass es andere Ansätze gibt, die die Leute verfolgen könnten, aber hier ist der, bei dem ich gelandet bin.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1514.3,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that.",
  "translatedText": "Wenn wir die Aussicht auf eine nächste Vermutung in Betracht ziehen, wie in diesem Fall Wörter, ist das, was uns wirklich interessiert, das erwartete Ergebnis unseres Spiels, wenn wir das tun.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to.",
  "translatedText": "Und um diesen erwarteten Wert zu berechnen, sagen wir, wie hoch die Wahrscheinlichkeit ist, dass Wörter die tatsächliche Antwort sind, was derzeit 58 % entspricht.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be 4.",
  "translatedText": "Wir gehen davon aus, dass unser Punktestand in diesem Spiel bei einer Chance von 58 % 4 wäre.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of 1 minus that 58%, our score will be more than that 4.",
  "translatedText": "Und dann, wenn die Wahrscheinlichkeit 1 minus 58 % beträgt, wird unser Ergebnis mehr als 4 betragen.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point.",
  "translatedText": "Wie viel mehr wissen wir nicht, aber wir können es anhand der voraussichtlichen Unsicherheit abschätzen, sobald wir diesen Punkt erreicht haben.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment there's 1.44 bits of uncertainty.",
  "translatedText": "Konkret gibt es im Moment 1.44 Bit Unsicherheit.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits.",
  "translatedText": "Wenn wir Wörter erraten, sagt uns das, dass die erwartete Information, die wir erhalten, 1 ist.27 Bit.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens.",
  "translatedText": "Wenn wir also Wörter erraten, stellt dieser Unterschied dar, wie viel Unsicherheit uns danach wahrscheinlich bleibt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score.",
  "translatedText": "Was wir brauchen, ist eine Art Funktion, die ich hier f nenne, die diese Unsicherheit mit einem erwarteten Ergebnis verknüpft.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version 1 of the bot to say hey what was the actual score after various points with certain very measurable amounts of uncertainty.",
  "translatedText": "Und die Vorgehensweise bestand darin, einfach eine Reihe von Daten aus früheren Spielen basierend auf Version 1 des Bots aufzuzeichnen, um zu sagen, wie hoch der tatsächliche Punktestand nach verschiedenen Punkten war, mit gewissen, sehr messbaren Unsicherheiten.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer.",
  "translatedText": "Zum Beispiel diese Datenpunkte hier, die über einem Wert von etwa 8 liegen.7 oder so sagen für einige Spiele nach einem Zeitpunkt, an dem es 8 waren.7 Bits Unsicherheit, es waren zwei Vermutungen erforderlich, um die endgültige Antwort zu erhalten.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games it took three guesses, for other games it took four guesses.",
  "translatedText": "Bei anderen Spielen waren drei Schätzungen erforderlich, bei anderen Spielen waren vier Schätzungen erforderlich.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring.",
  "translatedText": "Wenn wir hier nach links wechseln, sagen alle Punkte über Null, dass immer dann, wenn es null Unsicherheitsbits gibt, das heißt, dass es nur eine Möglichkeit gibt, die Anzahl der erforderlichen Schätzungen immer nur eins beträgt, was beruhigend ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses.",
  "translatedText": "Wann immer es ein bisschen Unsicherheit gab, was bedeutete, dass es sich im Wesentlichen nur um zwei Möglichkeiten handelte, war manchmal eine weitere Vermutung erforderlich, manchmal waren zwei weitere Vermutungen erforderlich.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1614.78,
  "end": 1623.02
 },
 {
  "input": "And so on and so forth here.",
  "translatedText": "Und so weiter und so fort hier.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1623.08,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages.",
  "translatedText": "Eine vielleicht etwas einfachere Möglichkeit, diese Daten zu visualisieren, besteht darin, sie zusammenzufassen und Durchschnittswerte zu bilden.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example this bar here saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5.",
  "translatedText": "Zum Beispiel dieser Balken hier, der besagt, dass von allen Punkten, bei denen wir eine gewisse Unsicherheit hatten, die Anzahl der erforderlichen neuen Vermutungen im Durchschnitt etwa 1 betrug.5.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward.",
  "translatedText": "Und der Balken hier besagt, dass bei all den verschiedenen Spielen, bei denen die Unsicherheit irgendwann etwas über vier Bit lag, was einer Eingrenzung auf 16 verschiedene Möglichkeiten entspricht, ab diesem Zeitpunkt im Durchschnitt etwas mehr als zwei Vermutungen erforderlich sind nach vorne.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this.",
  "translatedText": "Und von hier aus habe ich einfach eine Regression durchgeführt, um eine Funktion anzupassen, die hier sinnvoll erschien.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be.",
  "translatedText": "Und bedenken Sie, dass der Sinn all dessen darin besteht, dass wir die Intuition quantifizieren können, dass die erwartete Punktzahl umso niedriger sein wird, je mehr Informationen wir aus einem Wort gewinnen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So with this as version 2.0, if we go back and we run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do?",
  "translatedText": "Also hiermit als Version 2.0, wenn wir zurückgehen und den gleichen Satz Simulationen durchführen und ihn gegen alle 2315 möglichen Wortantworten spielen lassen, wie funktioniert das?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version it's definitely better, which is reassuring.",
  "translatedText": "Nun, im Gegensatz zu unserer ersten Version ist es definitiv besser, was beruhigend ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done the average is around 3.6, although unlike the first version there are a couple times that it loses and requires more than six in this circumstance.",
  "translatedText": "Alles in allem liegt der Durchschnitt bei etwa 3.6, obwohl es im Gegensatz zur ersten Version ein paar Mal Verluste gibt und in diesem Fall mehr als sechs erforderlich sind.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1684.02,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information.",
  "translatedText": "Vermutlich, weil es Zeiten gibt, in denen es darum geht, diesen Kompromiss einzugehen, um tatsächlich das Ziel zu erreichen, anstatt die Informationen zu maximieren.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6?",
  "translatedText": "Können wir es also besser machen als 3?6?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can.",
  "translatedText": "Das können wir auf jeden Fall.",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model.",
  "translatedText": "Nun habe ich zu Beginn gesagt, dass es am meisten Spaß macht, zu versuchen, nicht die wahre Liste der Wort-Antworten in die Art und Weise zu integrieren, wie das Modell erstellt wird.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43.",
  "translatedText": "Aber wenn wir es integrieren, lag die beste Leistung, die ich erzielen konnte, bei etwa 3.43.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that.",
  "translatedText": "Wenn wir also versuchen, bei der Auswahl dieser vorherigen Verteilung, dieser 3, anspruchsvoller zu werden, als nur Worthäufigkeitsdaten zu verwenden.43 gibt wahrscheinlich einen Höchstwert dafür, wie gut wir damit werden könnten, oder zumindest, wie gut ich damit werden könnte.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one.",
  "translatedText": "Diese beste Leistung nutzt im Wesentlichen nur die Ideen, über die ich hier gesprochen habe, geht aber noch ein wenig weiter, als würde die Suche nach den erwarteten Informationen zwei Schritte vorwärts statt nur einen durchführen.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is.",
  "translatedText": "Ursprünglich hatte ich vor, mehr darüber zu reden, aber mir ist klar, dass wir ohnehin schon ziemlich weit gekommen sind.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least it's looking like Crane is the best opener.",
  "translatedText": "Das Einzige, was ich sagen möchte, ist, dass nach dieser zweistufigen Suche und dem anschließenden Ausführen einiger Beispielsimulationen bei den Top-Kandidaten es für mich zumindest so aussieht, als ob Crane der beste Opener ist.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed?",
  "translatedText": "Wer hätte es gedacht?",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits.",
  "translatedText": "Auch wenn Sie die wahre Wortliste verwenden, um Ihren Möglichkeitenraum zu bestimmen, beträgt die Unsicherheit, mit der Sie beginnen, etwas mehr als 11 Bit.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out, just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits.",
  "translatedText": "Und es stellt sich heraus, dass allein bei einer Brute-Force-Suche die maximal mögliche erwartete Information nach den ersten beiden Schätzungen etwa 10 Bit beträgt.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty.",
  "translatedText": "Das deutet darauf hin, dass Sie im besten Fall nach Ihren ersten beiden Vermutungen und vollkommen optimalem Spiel mit etwa einem kleinen Unsicherheitsfaktor zurückbleiben.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses.",
  "translatedText": "Das ist das Gleiche, als ob man auf zwei mögliche Vermutungen angewiesen wäre.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "So I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as 3, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail.",
  "translatedText": "Daher denke ich, dass es fair und wahrscheinlich ziemlich konservativ ist, zu sagen, dass Sie niemals einen Algorithmus schreiben könnten, der diesen Durchschnitt auf 3 reduziert, denn mit den Wörtern, die Ihnen zur Verfügung stehen, gibt es einfach keinen Platz, um nach nur zwei Schritten genügend Informationen zu erhalten in der Lage, die Antwort im dritten Slot jedes Mal fehlerfrei zu garantieren.",
  "model": "google_nmt",
  "n_reviews": 1,
  "start": 1777.74,
  "end": 1793.36
 }
]