1
00:00:00,000 --> 00:00:02,994
Das Spiel Wordle ist in den letzten ein, zwei Monaten ziemlich viral gegangen, 

2
00:00:02,994 --> 00:00:05,837
und da ich nie eine Gelegenheit für eine Mathematikstunde auslassen würde, 

3
00:00:05,837 --> 00:00:08,831
kam mir der Gedanke, dass das Spiel ein sehr gutes zentrales Beispiel in einer 

4
00:00:08,831 --> 00:00:11,143
Lektion über Informationstheorie und insbesondere für Thema, 

5
00:00:11,143 --> 00:00:12,660
das als Entropie bekannt ist, darstellt.

6
00:00:13,920 --> 00:00:16,452
Wie viele Leute wurde auch ich in das Rätsel hineingezogen, 

7
00:00:16,452 --> 00:00:19,532
und wie viele Programmierer wurde auch ich in den Versuch hineingezogen, 

8
00:00:19,532 --> 00:00:22,740
einen Algorithmus zu schreiben, der das Spiel so optimal wie möglich spielt.

9
00:00:23,180 --> 00:00:25,651
Und ich dachte, ich würde hier einfach mit euch einen Teil meines 

10
00:00:25,651 --> 00:00:28,683
Prozesses besprechen und ein bisschen der darin enthaltenen Mathematik erklären, 

11
00:00:28,683 --> 00:00:31,080
da der gesamte Algorithmus auf dieser Idee der Entropie basiert.

12
00:00:38,700 --> 00:00:41,640
Das Wichtigste zuerst: Falls ihr noch nichts davon gehört haben: Was ist Wordle?

13
00:00:42,040 --> 00:00:44,044
Und um hier zwei Fliegen mit einer Klappe zu schlagen, 

14
00:00:44,044 --> 00:00:47,104
während wir die Spielregeln durchgehen, möchte ich auch eine Vorschau darauf geben, 

15
00:00:47,104 --> 00:00:49,618
was unser Ziel ist, nämlich einen kleinen Algorithmus zu entwickeln, 

16
00:00:49,618 --> 00:00:51,040
der das Spiel im Grunde für uns spielt.

17
00:00:51,360 --> 00:00:53,001
Also, ich das heutige Wordle noch nicht gemacht habe, 

18
00:00:53,001 --> 00:00:55,100
ist es der 4. Februar und wir werden sehen, wie sich der Bot schlägt.

19
00:00:55,480 --> 00:00:58,328
Das Ziel von Wordle ist es, ein geheimnisvolles Wort mit fünf Buchstaben zu erraten, 

20
00:00:58,328 --> 00:01:00,340
und man hat sechs verschiedene Möglichkeiten, es zu erraten.

21
00:01:00,840 --> 00:01:04,379
Beispielsweise schlägt mein Wordle-Bot vor, dass ich mit 'Crane' beginne.

22
00:01:05,180 --> 00:01:08,284
Jedes Mal, wenn man ein Wort rät anstellt, erhält man Informationen darüber, 

23
00:01:08,284 --> 00:01:10,220
wie nah die Vermutung an der wahren Antwort ist.

24
00:01:10,920 --> 00:01:14,100
Hier sagt mir das graue Kästchen, dass die Antwort kein C enthält.

25
00:01:14,520 --> 00:01:16,214
Das gelbe Kästchen sagt mir, dass es ein R gibt, 

26
00:01:16,214 --> 00:01:17,840
aber es befindet sich nicht an dieser Position.

27
00:01:18,240 --> 00:01:20,240
Das grüne Kästchen sagt mir, dass das geheime 

28
00:01:20,240 --> 00:01:22,240
Wort ein A hat und es an dritter Stelle steht.

29
00:01:22,720 --> 00:01:24,580
Und es gibt kein N und kein E.

30
00:01:25,200 --> 00:01:27,340
Lasst mich eben dem Wordle-Bot diese Informationen mitteilen.

31
00:01:27,340 --> 00:01:30,320
Wir haben mit 'Crane' angefangen und Grau, Gelb, Grün, Grau, Grau bekommen.

32
00:01:31,420 --> 00:01:33,685
Macht euch keine Sorgen wegen all der Daten, die gerade angezeigt werden, 

33
00:01:33,685 --> 00:01:34,940
ich werde das zu gegebener Zeit erklären.

34
00:01:35,460 --> 00:01:38,820
Aber sein Top-Vorschlag für unsere zweite Wahl ist 'shtick'.

35
00:01:39,560 --> 00:01:42,326
Und das Eingabewort muss tatsächlich ein Wort mit fünf Buchstaben sein, 

36
00:01:42,326 --> 00:01:45,400
aber wie wir sehen werden, ist es ziemlich großzügig, was es an Wörtern zulässt.

37
00:01:46,200 --> 00:01:47,440
In diesem Fall versuchen wir es mit shtick.

38
00:01:48,780 --> 00:01:50,180
Und es sieht ziemlich gut aus.

39
00:01:50,260 --> 00:01:53,228
Wir haben S und H, damit kennen wir die ersten drei Buchstaben und wissen, 

40
00:01:53,228 --> 00:01:53,980
dass es ein R gibt.

41
00:01:53,980 --> 00:01:58,700
Und so wird das Wort SHA irgendetwas R oder SHAR irgendetwas sein.

42
00:01:59,620 --> 00:02:01,522
Und es sieht so aus, als ob der Wordle-Bot weiß, 

43
00:02:01,522 --> 00:02:04,240
dass es nur auf zwei Möglichkeiten ankommt: entweder Shard oder Sharp.

44
00:02:05,100 --> 00:02:07,239
Es ist ein Fünfzig-Fünfzig zwischen ihnen, also vermute ich, 

45
00:02:07,239 --> 00:02:10,080
dass es sich einfach wegen der alphabetischen Ordnung für Shard entscheiden wird.

46
00:02:11,220 --> 00:02:12,860
Und - Hurra - das ist die richtige Antwort.

47
00:02:12,960 --> 00:02:13,780
Also haben wir es in drei Versuchen geschafft.

48
00:02:14,600 --> 00:02:17,544
Wenn Ihr euch fragt, ob das gut ist: Ich habe jemanden sagen hören, 

49
00:02:17,544 --> 00:02:20,360
dass Wordle in vier Versuchen ein Par und in drei ein Birdie ist.

50
00:02:20,680 --> 00:02:22,480
Was meiner Meinung nach eine ziemlich treffende Analogie ist.

51
00:02:22,480 --> 00:02:27,020
Um vier zu erreichen, muss man konstant gut spielen, aber es ist nicht undenkbar.

52
00:02:27,180 --> 00:02:29,920
Aber es fühlt sich einfach großartig an, wenn man es in drei Versuchen errät

53
00:02:30,880 --> 00:02:33,507
Wenn ihr also Lust darauf habt, möchte ich hier einfach meinen Denkprozess 

54
00:02:33,507 --> 00:02:35,960
von Anfang an besprechen, wie ich an den Wordle-Bot herangegangen bin.

55
00:02:36,480 --> 00:02:37,975
Und wie ich bereits sagte, es ist eigentlich ein 

56
00:02:37,975 --> 00:02:39,440
Vorwand für eine Lektion in Informationstheorie.

57
00:02:39,740 --> 00:02:42,820
Das Hauptziel besteht darin, zu erklären, was Informationen und was Entropie sind.

58
00:02:48,220 --> 00:02:50,269
Mein erster Gedanke bei der Annäherung an dieses Thema war, 

59
00:02:50,269 --> 00:02:53,105
einen Blick auf die relative Häufigkeit verschiedener Buchstaben in der englischen 

60
00:02:53,105 --> 00:02:53,720
Sprache zu werfen.

61
00:02:54,380 --> 00:02:57,223
Also dachte ich: Okay, gibt es ein erstes Wort oder ein Wortepaar, 

62
00:02:57,223 --> 00:02:59,260
das viele dieser häufigsten Buchstaben mitnimmt?

63
00:02:59,960 --> 00:03:03,000
Und was mir sehr gefiel, war 'other' gefolgt von 'nails'.

64
00:03:03,760 --> 00:03:06,237
Die Idee ist, dass es sich immer gut anfühlt, wenn man einen Buchstaben richtig hat, 

65
00:03:06,237 --> 00:03:07,520
also einen grünen oder einen gelben bekommt.

66
00:03:07,520 --> 00:03:08,840
Es fühlt sich an, als würde man Informationen erhalten.

67
00:03:09,340 --> 00:03:11,908
Aber selbst wenn man nichts trifft und immer grau kriegt, 

68
00:03:11,908 --> 00:03:14,875
erhält man dennoch viele Informationen, da es ziemlich selten ist, 

69
00:03:14,875 --> 00:03:17,400
ein Wort zu finden, das keinen dieser Buchstaben enthält.

70
00:03:18,140 --> 00:03:20,415
Aber auch das fühlt sich nicht besonders systematisch an, 

71
00:03:20,415 --> 00:03:23,200
weil beispielsweise die Reihenfolge der Buchstaben nicht beachtet wird.

72
00:03:23,560 --> 00:03:25,300
Warum Nägel tippen, wenn ich Schnecke tippen könnte?

73
00:03:26,080 --> 00:03:27,500
Ist es besser, das S am Ende zu haben?

74
00:03:27,820 --> 00:03:28,680
Ich bin mir nicht wirklich sicher.

75
00:03:29,240 --> 00:03:32,152
Nun sagte ein Freund von mir, dass er gerne mit dem Wort „müde“ beginnt, 

76
00:03:32,152 --> 00:03:34,705
was mich irgendwie überraschte, weil darin einige ungewöhnliche 

77
00:03:34,705 --> 00:03:36,540
Buchstaben wie das W und das Y enthalten sind.

78
00:03:37,120 --> 00:03:39,000
Aber wer weiß, vielleicht ist das ein besserer Auftakt.

79
00:03:39,320 --> 00:03:41,729
Gibt es eine Art quantitative Bewertung, mit der wir 

80
00:03:41,729 --> 00:03:44,320
die Qualität einer möglichen Vermutung beurteilen können?

81
00:03:45,340 --> 00:03:48,088
Um nun die Art und Weise festzulegen, wie wir mögliche Vermutungen einordnen werden, 

82
00:03:48,088 --> 00:03:50,320
gehen wir noch einmal zurück und bringen ein wenig Klarheit darüber, 

83
00:03:50,320 --> 00:03:51,420
wie das Spiel genau aufgebaut ist.

84
00:03:51,420 --> 00:03:54,581
Es gibt also eine Liste von Wörtern, die Sie eingeben können und die 

85
00:03:54,581 --> 00:03:57,880
als gültige Vermutungen gelten und die nur etwa 13.000 Wörter lang sind.

86
00:03:58,320 --> 00:04:01,716
Aber wenn man es sich anschaut, sieht man da eine Menge wirklich ungewöhnlicher Dinge, 

87
00:04:01,716 --> 00:04:04,058
Dinge wie einen Kopf oder Ali und ARG, die Art von Wörtern, 

88
00:04:04,058 --> 00:04:06,440
die bei einem Scrabble-Spiel Familienstreitigkeiten auslösen.

89
00:04:06,960 --> 00:04:08,750
Aber die Atmosphäre des Spiels ist, dass die Antwort 

90
00:04:08,750 --> 00:04:10,540
immer ein einigermaßen gebräuchliches Wort sein wird.

91
00:04:10,960 --> 00:04:13,935
Und tatsächlich gibt es noch eine weitere Liste mit rund 2300 Wörtern, 

92
00:04:13,935 --> 00:04:15,360
die mögliche Antworten darstellen.

93
00:04:15,940 --> 00:04:18,277
Und das ist eine von Menschen kuratierte Liste, ich glaube, 

94
00:04:18,277 --> 00:04:21,160
speziell von der Freundin des Spieleentwicklers, was irgendwie Spaß macht.

95
00:04:21,820 --> 00:04:25,438
Aber was ich gerne tun würde, unsere Herausforderung für dieses Projekt besteht darin, 

96
00:04:25,438 --> 00:04:28,141
zu sehen, ob wir ein Programm schreiben können, das Wordle löst, 

97
00:04:28,141 --> 00:04:30,180
das keine Vorkenntnisse über diese Liste enthält.

98
00:04:30,720 --> 00:04:33,345
Zum einen gibt es viele ziemlich gebräuchliche Wörter mit fünf Buchstaben, 

99
00:04:33,345 --> 00:04:34,640
die Sie in dieser Liste nicht finden.

100
00:04:34,940 --> 00:04:36,648
Daher wäre es besser, ein Programm zu schreiben, 

101
00:04:36,648 --> 00:04:39,089
das etwas widerstandsfähiger ist und Wordle gegen jeden spielen kann, 

102
00:04:39,089 --> 00:04:41,460
nicht nur gegen das, was zufällig auf der offiziellen Website steht.

103
00:04:41,920 --> 00:04:45,147
Und wir kennen diese Liste möglicher Antworten auch deshalb, 

104
00:04:45,147 --> 00:04:47,000
weil sie im Quellcode sichtbar ist.

105
00:04:47,000 --> 00:04:49,593
Aber die Art und Weise, wie es im Quellcode sichtbar ist, 

106
00:04:49,593 --> 00:04:53,260
liegt in der spezifischen Reihenfolge, in der die Antworten von Tag zu Tag kommen.

107
00:04:53,260 --> 00:04:55,840
Sie können also jederzeit nachschauen, wie die Antwort morgen lautet.

108
00:04:56,420 --> 00:04:58,880
Es ist also klar, dass die Verwendung der Liste in gewisser Weise Betrug ist.

109
00:04:59,100 --> 00:05:01,782
Und was zu einem interessanteren Rätsel und einer reichhaltigeren 

110
00:05:01,782 --> 00:05:04,465
Informationstheorie-Lektion führt, ist stattdessen die Verwendung 

111
00:05:04,465 --> 00:05:07,554
einiger universellerer Daten wie relativer Worthäufigkeiten im Allgemeinen, 

112
00:05:07,554 --> 00:05:10,440
um die Intuition einer Vorliebe für gebräuchlichere Wörter zu erfassen.

113
00:05:11,600 --> 00:05:15,900
Wie sollten wir also aus diesen 13.000 Möglichkeiten den Eröffnungstipp wählen?

114
00:05:16,400 --> 00:05:18,402
Wenn mein Freund beispielsweise müde einen Heiratsantrag macht, 

115
00:05:18,402 --> 00:05:19,780
wie sollten wir dessen Qualität analysieren?

116
00:05:20,520 --> 00:05:23,822
Nun, der Grund, warum er sagte, dass er dieses unwahrscheinliche W mag, ist, 

117
00:05:23,822 --> 00:05:27,340
dass ihm die Weitsicht gefällt, wie gut es sich anfühlt, wenn man dieses W trifft.

118
00:05:27,920 --> 00:05:30,289
Wenn zum Beispiel das erste Muster, das aufgedeckt wurde, 

119
00:05:30,289 --> 00:05:32,862
ungefähr so aussah, dann stellt sich heraus, dass es in diesem 

120
00:05:32,862 --> 00:05:35,600
riesigen Lexikon nur 58 Wörter gibt, die diesem Muster entsprechen.

121
00:05:36,060 --> 00:05:38,400
Das ist also eine enorme Reduzierung gegenüber 13.000.

122
00:05:38,780 --> 00:05:41,731
Aber die Kehrseite davon ist natürlich, dass es sehr ungewöhnlich ist, 

123
00:05:41,731 --> 00:05:43,020
ein solches Muster zu erhalten.

124
00:05:43,020 --> 00:05:46,548
Wenn jedes Wort mit gleicher Wahrscheinlichkeit die Antwort wäre, 

125
00:05:46,548 --> 00:05:51,040
wäre die Wahrscheinlichkeit, dieses Muster zu treffen, 58 geteilt durch etwa 13.000.

126
00:05:51,580 --> 00:05:52,731
Natürlich ist es nicht gleichermaßen wahrscheinlich, 

127
00:05:52,731 --> 00:05:53,600
dass es sich dabei um Antworten handelt.

128
00:05:53,720 --> 00:05:56,220
Die meisten davon sind sehr obskure und sogar fragwürdige Wörter.

129
00:05:56,600 --> 00:05:58,818
Aber zumindest für unseren ersten Versuch gehen wir davon aus, 

130
00:05:58,818 --> 00:06:01,600
dass sie alle gleich wahrscheinlich sind, und verfeinern das dann etwas später.

131
00:06:02,020 --> 00:06:04,564
Der Punkt ist, dass es von Natur aus unwahrscheinlich ist, 

132
00:06:04,564 --> 00:06:06,720
dass ein Muster mit vielen Informationen auftritt.

133
00:06:07,280 --> 00:06:10,800
Tatsächlich bedeutet es, informativ zu sein, dass es unwahrscheinlich ist.

134
00:06:11,719 --> 00:06:15,830
Ein viel wahrscheinlicheres Muster, das man bei dieser Eröffnung sehen könnte, 

135
00:06:15,830 --> 00:06:18,120
wäre so etwas, wo natürlich kein W drin ist.

136
00:06:18,240 --> 00:06:21,400
Vielleicht gibt es ein E, vielleicht gibt es kein A, es gibt kein R, es gibt kein Y.

137
00:06:22,080 --> 00:06:24,560
In diesem Fall gibt es 1400 mögliche Übereinstimmungen.

138
00:06:25,080 --> 00:06:27,950
Wenn alle gleich wahrscheinlich wären, errechnet sich eine Wahrscheinlichkeit 

139
00:06:27,950 --> 00:06:30,600
von etwa 11 %, dass es sich um das Muster handelt, das Sie sehen würden.

140
00:06:30,900 --> 00:06:33,340
Daher sind die wahrscheinlichsten Ergebnisse auch die am wenigsten aussagekräftigen.

141
00:06:34,240 --> 00:06:36,035
Um hier einen umfassenderen Überblick zu erhalten, 

142
00:06:36,035 --> 00:06:38,499
möchte ich Ihnen die vollständige Verteilung der Wahrscheinlichkeiten 

143
00:06:38,499 --> 00:06:41,140
über alle verschiedenen Muster hinweg zeigen, die Sie möglicherweise sehen.

144
00:06:41,740 --> 00:06:45,398
Jeder Balken, den Sie betrachten, entspricht also einem möglichen Farbmuster, 

145
00:06:45,398 --> 00:06:48,728
das aufgedeckt werden könnte, von denen es 3 bis 5 Möglichkeiten gibt, 

146
00:06:48,728 --> 00:06:52,340
und sie sind von links nach rechts geordnet, am häufigsten bis am seltensten.

147
00:06:52,920 --> 00:06:56,000
Die häufigste Möglichkeit besteht hier also darin, dass Sie nur Grautöne erhalten.

148
00:06:56,100 --> 00:06:58,120
Das passiert in etwa 14 % der Fälle.

149
00:06:58,580 --> 00:07:01,233
Und wenn Sie eine Vermutung anstellen, hoffen Sie, 

150
00:07:01,233 --> 00:07:04,406
dass Sie irgendwo in diesem langen Schwanz landen, wie hier, 

151
00:07:04,406 --> 00:07:07,735
wo es nur 18 Möglichkeiten gibt, was zu diesem Muster passt und 

152
00:07:07,735 --> 00:07:09,140
offensichtlich so aussieht.

153
00:07:09,920 --> 00:07:11,921
Oder wenn wir uns etwas weiter nach links wagen, 

154
00:07:11,921 --> 00:07:13,800
wissen Sie, vielleicht kommen wir bis hierher.

155
00:07:14,940 --> 00:07:16,180
Okay, hier ist ein gutes Rätsel für dich.

156
00:07:16,540 --> 00:07:19,748
Welche drei Wörter in der englischen Sprache beginnen mit einem W, 

157
00:07:19,748 --> 00:07:22,000
enden mit einem Y und enthalten irgendwo ein R?

158
00:07:22,480 --> 00:07:26,800
Es stellt sich heraus, dass die Antworten, mal sehen, wortreich, wurmig und ironisch sind.

159
00:07:27,500 --> 00:07:30,179
Um zu beurteilen, wie gut dieses Wort insgesamt ist, 

160
00:07:30,179 --> 00:07:33,667
benötigen wir eine Art Maß für die erwartete Menge an Informationen, 

161
00:07:33,667 --> 00:07:35,740
die Sie von dieser Distribution erhalten.

162
00:07:35,740 --> 00:07:39,167
Wenn wir jedes Muster durchgehen und seine Auftrittswahrscheinlichkeit 

163
00:07:39,167 --> 00:07:42,064
mit etwas multiplizieren, das misst, wie informativ es ist, 

164
00:07:42,064 --> 00:07:44,720
kann uns das vielleicht eine objektive Bewertung geben.

165
00:07:45,960 --> 00:07:47,880
Ihr erster Instinkt dafür, was das sein sollte, 

166
00:07:47,880 --> 00:07:49,840
könnte nun die Anzahl der Übereinstimmungen sein.

167
00:07:50,160 --> 00:07:52,400
Sie möchten eine geringere durchschnittliche Anzahl von Übereinstimmungen.

168
00:07:52,800 --> 00:07:55,543
Aber stattdessen möchte ich ein universelleres Maß verwenden, 

169
00:07:55,543 --> 00:07:58,596
das wir oft Informationen zuschreiben, und eines, das flexibler ist, 

170
00:07:58,596 --> 00:08:02,268
wenn wir jedem dieser 13.000 Wörter eine andere Wahrscheinlichkeit dafür zuordnen, 

171
00:08:02,268 --> 00:08:04,260
ob es tatsächlich die Antwort ist oder nicht.

172
00:08:10,320 --> 00:08:14,004
Die Standardinformationseinheit ist das Bit, dessen Formel etwas komisch ist, 

173
00:08:14,004 --> 00:08:16,980
aber wirklich intuitiv ist, wenn wir uns nur Beispiele ansehen.

174
00:08:17,780 --> 00:08:21,281
Wenn Sie eine Beobachtung haben, die Ihren Möglichkeitenraum halbiert, 

175
00:08:21,281 --> 00:08:23,500
sagen wir, dass sie eine Information enthält.

176
00:08:24,180 --> 00:08:27,012
In unserem Beispiel besteht der Raum der Möglichkeiten aus allen möglichen Wörtern, 

177
00:08:27,012 --> 00:08:30,046
und es stellt sich heraus, dass etwa die Hälfte der Wörter mit fünf Buchstaben ein S hat, 

178
00:08:30,046 --> 00:08:31,260
etwas weniger, aber etwa die Hälfte.

179
00:08:31,780 --> 00:08:34,320
Diese Beobachtung würde Ihnen also eine kleine Information geben.

180
00:08:34,880 --> 00:08:38,075
Wenn stattdessen eine neue Tatsache den Raum der Möglichkeiten um den 

181
00:08:38,075 --> 00:08:41,500
Faktor vier verkleinert, sagen wir, dass sie zwei Informationsbits enthält.

182
00:08:41,980 --> 00:08:44,460
Es stellt sich beispielsweise heraus, dass etwa ein Viertel dieser Wörter ein T hat.

183
00:08:45,020 --> 00:08:47,927
Wenn die Beobachtung diesen Raum um den Faktor acht verkleinert, sagen wir, 

184
00:08:47,927 --> 00:08:50,720
dass es sich um drei Informationsbits handelt, und so weiter und so fort.

185
00:08:50,900 --> 00:08:55,060
Vier Bits zerschneiden es in ein Sechzehntel, fünf Bits zerschneiden es in ein 32tel.

186
00:08:55,060 --> 00:08:57,493
Jetzt möchten Sie vielleicht innehalten und sich fragen: 

187
00:08:57,493 --> 00:09:01,122
Wie lautet die Formel für Informationen über die Anzahl der Bits im Hinblick auf die 

188
00:09:01,122 --> 00:09:02,660
Wahrscheinlichkeit eines Auftretens?

189
00:09:02,660 --> 00:09:06,092
Was wir hier sagen ist, dass wenn man die Hälfte der Anzahl der Bits hochnimmt, 

190
00:09:06,092 --> 00:09:09,695
das dasselbe ist wie die Wahrscheinlichkeit, was dasselbe ist, als würde man sagen, 

191
00:09:09,695 --> 00:09:12,827
dass zwei hoch die Anzahl der Bits eins über der Wahrscheinlichkeit ist, 

192
00:09:12,827 --> 00:09:16,174
was ordnet sich weiter um und sagt, dass die Informationen die logarithmische 

193
00:09:16,174 --> 00:09:18,920
Basis zwei von eins dividiert durch die Wahrscheinlichkeit sind.

194
00:09:19,620 --> 00:09:21,794
Und manchmal sieht man das noch bei einer weiteren Neuordnung, 

195
00:09:21,794 --> 00:09:24,900
bei der die Information die negative logarithmische Basis zwei der Wahrscheinlichkeit ist.

196
00:09:25,660 --> 00:09:28,971
So ausgedrückt, mag es für den Uneingeweihten etwas seltsam aussehen, 

197
00:09:28,971 --> 00:09:31,951
aber es ist eigentlich nur die sehr intuitive Idee, zu fragen, 

198
00:09:31,951 --> 00:09:34,080
wie oft man seine Möglichkeiten halbiert hat.

199
00:09:35,180 --> 00:09:37,926
Wenn Sie sich jetzt fragen: Ich dachte, wir spielen nur ein lustiges Wortspiel, 

200
00:09:37,926 --> 00:09:39,300
warum kommen dann Logarithmen ins Spiel?

201
00:09:39,780 --> 00:09:42,221
Ein Grund dafür, dass dies eine schönere Einheit ist, ist, 

202
00:09:42,221 --> 00:09:45,863
dass es einfach viel einfacher ist, über sehr unwahrscheinliche Ereignisse zu sprechen, 

203
00:09:45,863 --> 00:09:49,215
viel einfacher zu sagen, dass eine Beobachtung 20 Bits an Informationen enthält, 

204
00:09:49,215 --> 00:09:52,940
als zu sagen, dass die Wahrscheinlichkeit, dass dieses oder jenes eintritt, 0 ist.0000095.

205
00:09:53,300 --> 00:09:56,140
Aber ein substanziellerer Grund dafür, dass sich dieser logarithmische 

206
00:09:56,140 --> 00:09:59,220
Ausdruck als sehr nützliche Ergänzung zur Wahrscheinlichkeitstheorie erwies, 

207
00:09:59,220 --> 00:10:01,460
ist die Art und Weise, wie Informationen addiert werden.

208
00:10:02,060 --> 00:10:05,085
Wenn Ihnen beispielsweise eine Beobachtung zwei Informationsbits liefert, 

209
00:10:05,085 --> 00:10:08,766
wodurch Ihr Platz um vier reduziert wird, und wenn Ihnen dann eine zweite Beobachtung wie 

210
00:10:08,766 --> 00:10:11,669
Ihre zweite Schätzung in Wordle weitere drei Informationsbits liefert, 

211
00:10:11,669 --> 00:10:14,163
wodurch Sie noch einmal um den Faktor acht reduziert werden, 

212
00:10:14,163 --> 00:10:16,740
dann ist das der Fall zwei zusammen ergeben fünf Informationen.

213
00:10:17,160 --> 00:10:19,331
So wie sich Wahrscheinlichkeiten gerne vervielfachen, 

214
00:10:19,331 --> 00:10:21,020
fügen sich auch Informationen gerne hinzu.

215
00:10:21,960 --> 00:10:24,398
Sobald wir uns also im Bereich eines erwarteten Werts befinden, 

216
00:10:24,398 --> 00:10:27,408
bei dem wir eine Reihe von Zahlen addieren, ist der Umgang mit den Protokollen 

217
00:10:27,408 --> 00:10:27,980
viel einfacher.

218
00:10:28,480 --> 00:10:31,747
Kehren wir zu unserer Verteilung für Weary zurück und fügen hier einen weiteren kleinen 

219
00:10:31,747 --> 00:10:34,940
Tracker hinzu, der uns zeigt, wie viele Informationen für jedes Muster vorhanden sind.

220
00:10:35,580 --> 00:10:37,264
Ich möchte Sie vor allem darauf aufmerksam machen, 

221
00:10:37,264 --> 00:10:39,906
dass je höher die Wahrscheinlichkeit ist, wenn wir zu diesen wahrscheinlicheren 

222
00:10:39,906 --> 00:10:42,780
Mustern gelangen, je niedriger die Informationen sind, desto weniger Bits gewinnen Sie.

223
00:10:43,500 --> 00:10:46,810
Wir messen die Qualität dieser Vermutung, indem wir den erwarteten Wert dieser 

224
00:10:46,810 --> 00:10:49,492
Informationen nehmen, indem wir jedes Muster durchgehen, sagen, 

225
00:10:49,492 --> 00:10:52,677
wie wahrscheinlich es ist, und diesen dann mit der Anzahl der Informationen 

226
00:10:52,677 --> 00:10:54,060
multiplizieren, die wir erhalten.

227
00:10:54,710 --> 00:10:58,120
Und im Beispiel von Weary sind es 4.9 Bit.

228
00:10:58,560 --> 00:11:02,338
Im Durchschnitt sind die Informationen, die Sie aus dieser Eröffnungsvermutung erhalten, 

229
00:11:02,338 --> 00:11:05,480
so gut, als würden Sie Ihren Raum an Möglichkeiten etwa fünfmal halbieren.

230
00:11:05,960 --> 00:11:08,752
Im Gegensatz dazu wäre ein Beispiel für eine Schätzung mit 

231
00:11:08,752 --> 00:11:11,640
einem höheren erwarteten Informationswert so etwas wie Slate.

232
00:11:13,120 --> 00:11:15,620
In diesem Fall werden Sie feststellen, dass die Verteilung viel flacher aussieht.

233
00:11:15,940 --> 00:11:20,307
Insbesondere beträgt die Eintrittswahrscheinlichkeit des wahrscheinlichsten aller 

234
00:11:20,307 --> 00:11:24,514
Grautöne nur etwa 6 %, Sie erhalten also offensichtlich mindestens 3.9 Bits an 

235
00:11:24,514 --> 00:11:25,260
Informationen.

236
00:11:25,920 --> 00:11:28,560
Aber das ist ein Minimum, normalerweise bekommt man etwas Besseres.

237
00:11:29,100 --> 00:11:31,780
Und es stellt sich heraus, dass die durchschnittliche Information, 

238
00:11:31,780 --> 00:11:35,140
wenn man die Zahlen zu diesem Thema auswertet und alle relevanten Begriffe addiert, 

239
00:11:35,140 --> 00:11:35,900
bei etwa 5 liegt.8.

240
00:11:37,360 --> 00:11:40,332
Im Gegensatz zu Weary wird Ihr Spielraum an Möglichkeiten also 

241
00:11:40,332 --> 00:11:43,540
nach dieser ersten Vermutung im Durchschnitt etwa halb so groß sein.

242
00:11:44,420 --> 00:11:46,610
Es gibt tatsächlich eine lustige Geschichte zum 

243
00:11:46,610 --> 00:11:49,120
Namen für diesen erwarteten Wert der Informationsmenge.

244
00:11:49,200 --> 00:11:51,830
Die Informationstheorie wurde von Claude Shannon entwickelt, 

245
00:11:51,830 --> 00:11:54,116
der in den 1940er Jahren an den Bell Labs arbeitete, 

246
00:11:54,116 --> 00:11:57,997
aber er sprach über einige seiner noch nicht veröffentlichten Ideen mit John von Neumann, 

247
00:11:57,997 --> 00:12:01,490
dem damals prominenten intellektuellen Giganten in Mathematik und Physik und die 

248
00:12:01,490 --> 00:12:03,560
Anfänge dessen, was später zur Informatik wurde.

249
00:12:04,100 --> 00:12:07,568
Und als er erwähnte, dass er keinen wirklich guten Namen für diesen 

250
00:12:07,568 --> 00:12:11,343
erwarteten Wert der Informationsmenge hatte, sagte von Neumann angeblich, 

251
00:12:11,343 --> 00:12:14,200
man sollte es Entropie nennen, und das aus zwei Gründen.

252
00:12:14,540 --> 00:12:17,616
Erstens wurde Ihre Unsicherheitsfunktion in der statistischen Mechanik 

253
00:12:17,616 --> 00:12:20,389
unter diesem Namen verwendet, sie hat also bereits einen Namen, 

254
00:12:20,389 --> 00:12:23,986
und zweitens, und was noch wichtiger ist, weiß niemand, was Entropie wirklich ist, 

255
00:12:23,986 --> 00:12:26,760
also werden Sie es in einer Debatte immer tun den Vorteil haben.

256
00:12:27,700 --> 00:12:29,984
Wenn der Name also etwas mysteriös erscheint und man dieser 

257
00:12:29,984 --> 00:12:32,460
Geschichte Glauben schenken darf, dann ist das sozusagen Absicht.

258
00:12:33,280 --> 00:12:36,755
Auch wenn Sie sich über seine Beziehung zu all dem zweiten Hauptsatz der Thermodynamik 

259
00:12:36,755 --> 00:12:39,232
aus der Physik wundern, gibt es definitiv einen Zusammenhang, 

260
00:12:39,232 --> 00:12:42,069
aber in seinen Ursprüngen beschäftigte sich Shannon nur mit der reinen 

261
00:12:42,069 --> 00:12:44,306
Wahrscheinlichkeitstheorie, und für unsere Zwecke hier, 

262
00:12:44,306 --> 00:12:47,382
wenn ich das verwende Beim Wort Entropie möchte ich Ihnen nur den erwarteten 

263
00:12:47,382 --> 00:12:49,580
Informationswert einer bestimmten Vermutung vorstellen.

264
00:12:50,700 --> 00:12:53,780
Man kann sich Entropie als die gleichzeitige Messung zweier Dinge vorstellen.

265
00:12:54,240 --> 00:12:56,780
Die erste Frage ist, wie flach die Verteilung ist.

266
00:12:57,320 --> 00:13:01,120
Je näher eine Verteilung an der Gleichmäßigkeit liegt, desto höher ist die Entropie.

267
00:13:01,580 --> 00:13:04,681
In unserem Fall, in dem es insgesamt 3 bis 5 Muster gibt, 

268
00:13:04,681 --> 00:13:08,370
würde die Beobachtung eines beliebigen Musters für eine gleichmäßige 

269
00:13:08,370 --> 00:13:11,846
Verteilung die Informationsprotokollbasis 2 von 3 bis 5 ergeben, 

270
00:13:11,846 --> 00:13:14,947
was zufällig 7 ist.92, das ist also das absolute Maximum, 

271
00:13:14,947 --> 00:13:17,300
das man für diese Entropie erreichen könnte.

272
00:13:17,840 --> 00:13:22,080
Aber die Entropie ist auch eine Art Maß dafür, wie viele Möglichkeiten es überhaupt gibt.

273
00:13:22,320 --> 00:13:25,462
Wenn Sie beispielsweise ein Wort haben, bei dem es nur 16 

274
00:13:25,462 --> 00:13:28,929
mögliche Muster gibt und jedes davon gleich wahrscheinlich ist, 

275
00:13:28,929 --> 00:13:32,180
beträgt diese Entropie, diese erwartete Information, 4 Bits.

276
00:13:32,579 --> 00:13:36,332
Aber wenn Sie ein anderes Wort haben, bei dem 64 mögliche Muster auftauchen 

277
00:13:36,332 --> 00:13:40,480
könnten und alle gleich wahrscheinlich sind, dann würde die Entropie 6 Bit betragen.

278
00:13:41,500 --> 00:13:44,432
Wenn Sie also irgendwo in freier Wildbahn eine Verteilung sehen, 

279
00:13:44,432 --> 00:13:47,590
die eine Entropie von 6 Bit hat, dann ist das so, als ob das so wäre, 

280
00:13:47,590 --> 00:13:51,154
als ob es so viel Variation und Ungewissheit darüber gibt, was passieren wird, 

281
00:13:51,154 --> 00:13:53,500
als ob es 64 gleich wahrscheinliche Ergebnisse gäbe.

282
00:13:54,360 --> 00:13:59,320
Bei meinem ersten Durchgang beim Wurtelebot ließ ich es im Grunde einfach so machen.

283
00:13:59,320 --> 00:14:02,265
Es geht alle möglichen Vermutungen durch, alle 13.000 Wörter, 

284
00:14:02,265 --> 00:14:05,306
berechnet die Entropie für jedes einzelne, oder genauer gesagt, 

285
00:14:05,306 --> 00:14:08,917
die Entropie der Verteilung über alle Muster, die Sie möglicherweise sehen, 

286
00:14:08,917 --> 00:14:12,386
für jedes einzelne und wählt das höchste aus, denn das ist so diejenige, 

287
00:14:12,386 --> 00:14:16,140
die Ihren Raum an Möglichkeiten wahrscheinlich so weit wie möglich einschränkt.

288
00:14:17,140 --> 00:14:19,282
Und obwohl ich hier nur über die erste Vermutung gesprochen habe, 

289
00:14:19,282 --> 00:14:21,100
gilt das Gleiche auch für die nächsten paar Vermutungen.

290
00:14:21,560 --> 00:14:24,491
Wenn Sie beispielsweise bei dieser ersten Vermutung ein Muster erkennen, 

291
00:14:24,491 --> 00:14:27,463
das Sie auf eine kleinere Anzahl möglicher Wörter beschränkt, je nachdem, 

292
00:14:27,463 --> 00:14:31,037
was damit übereinstimmt, spielen Sie einfach dasselbe Spiel mit Bezug auf diese kleinere 

293
00:14:31,037 --> 00:14:31,800
Gruppe von Wörtern.

294
00:14:32,260 --> 00:14:36,294
Für eine vorgeschlagene zweite Vermutung betrachten Sie die Verteilung aller Muster, 

295
00:14:36,294 --> 00:14:40,565
die aus dieser eingeschränkteren Menge von Wörtern auftreten könnten, durchsuchen alle 13.

296
00:14:40,565 --> 00:14:43,840
000 Möglichkeiten und finden diejenige, die diese Entropie maximiert.

297
00:14:45,420 --> 00:14:47,906
Um Ihnen zu zeigen, wie das in der Praxis funktioniert, 

298
00:14:47,906 --> 00:14:50,660
möchte ich einfach eine kleine Variante von Wurtele aufrufen, 

299
00:14:50,660 --> 00:14:54,080
die ich geschrieben habe und die am Rand die Höhepunkte dieser Analyse zeigt.

300
00:14:54,080 --> 00:14:56,477
Nachdem wir alle Entropieberechnungen durchgeführt haben, 

301
00:14:56,477 --> 00:14:59,660
zeigt es uns hier rechts, welche die höchsten erwarteten Informationen haben.

302
00:15:00,280 --> 00:15:04,282
Es stellt sich heraus, dass die beste Antwort, zumindest im Moment, 

303
00:15:04,282 --> 00:15:09,402
wir werden das später verfeinern, Tares ist, was, ähm, natürlich, eine Wicke bedeutet, 

304
00:15:09,402 --> 00:15:10,580
die häufigste Wicke.

305
00:15:11,040 --> 00:15:12,956
Jedes Mal, wenn wir hier eine Vermutung anstellen, 

306
00:15:12,956 --> 00:15:16,001
bei der ich vielleicht die Empfehlungen ignoriere und mich für Slate entscheide, 

307
00:15:16,001 --> 00:15:19,083
weil ich Slate mag, können wir sehen, wie viele erwartete Informationen es hatte, 

308
00:15:19,083 --> 00:15:22,240
aber rechts vom Wort wird uns dann angezeigt, wie viele Tatsächliche Informationen, 

309
00:15:22,240 --> 00:15:24,420
die wir aufgrund dieses besonderen Musters erhalten haben.

310
00:15:25,000 --> 00:15:27,675
Hier sieht es also so aus, als hätten wir etwas Pech gehabt, man hatte erwartet, 

311
00:15:27,675 --> 00:15:30,120
dass wir 5 bekommen.8, aber wir haben zufällig etwas mit weniger bekommen.

312
00:15:30,600 --> 00:15:33,378
Und dann zeigt es uns auf der linken Seite alle möglichen Wörter, 

313
00:15:33,378 --> 00:15:35,020
je nachdem, wo wir uns gerade befinden.

314
00:15:35,800 --> 00:15:38,702
Die blauen Balken geben an, wie wahrscheinlich es ist, dass jedes Wort vorkommt. 

315
00:15:38,702 --> 00:15:40,959
Im Moment geht es also davon aus, dass jedes Wort mit gleicher 

316
00:15:40,959 --> 00:15:43,360
Wahrscheinlichkeit vorkommt, aber wir werden das gleich verfeinern.

317
00:15:44,060 --> 00:15:48,043
Und dann sagt uns diese Unsicherheitsmessung die Entropie dieser Verteilung über 

318
00:15:48,043 --> 00:15:51,927
die möglichen Wörter, was im Moment, weil es eine gleichmäßige Verteilung ist, 

319
00:15:51,927 --> 00:15:55,960
nur eine unnötig komplizierte Methode ist, die Anzahl der Möglichkeiten zu zählen.

320
00:15:56,560 --> 00:15:59,581
Wenn wir zum Beispiel 2 hoch 13 nehmen würden.66, 

321
00:15:59,581 --> 00:16:02,180
das dürften etwa 13.000 Möglichkeiten sein.

322
00:16:02,900 --> 00:16:06,140
Ich bin hier etwas daneben, aber nur, weil ich nicht alle Dezimalstellen zeige.

323
00:16:06,720 --> 00:16:09,352
Im Moment mag das überflüssig wirken und die Sache zu kompliziert machen, 

324
00:16:09,352 --> 00:16:12,340
aber Sie werden sehen, warum es nützlich ist, beide Zahlen in einer Minute zu haben.

325
00:16:12,760 --> 00:16:16,017
Hier scheint es also darauf hinzudeuten, dass die höchste Entropie für unsere 

326
00:16:16,017 --> 00:16:19,400
zweite Vermutung Ramen ist, was sich wiederum einfach nicht wie ein Wort anfühlt.

327
00:16:19,980 --> 00:16:24,060
Um hier also den moralischen Standpunkt zu vertreten, tippe ich Rains ein.

328
00:16:25,440 --> 00:16:27,340
Und wieder sieht es so aus, als hätten wir etwas Pech gehabt.

329
00:16:27,520 --> 00:16:31,360
Wir hatten 4 erwartet.3 Bits und wir haben nur 3.39 Bit Informationen.

330
00:16:31,940 --> 00:16:33,940
Damit kommen wir auf 55 Möglichkeiten.

331
00:16:34,900 --> 00:16:36,926
Und hier werde ich vielleicht einfach dem folgen, 

332
00:16:36,926 --> 00:16:39,440
was es vorschlägt, nämlich Combo, was auch immer das bedeutet.

333
00:16:40,040 --> 00:16:42,920
Und okay, das ist tatsächlich eine gute Chance für ein Rätsel.

334
00:16:42,920 --> 00:16:46,380
Es sagt uns, dass dieses Muster uns 4 gibt.7 Bits an Informationen.

335
00:16:47,060 --> 00:16:49,569
Aber bevor wir dieses Muster sehen, waren es auf 

336
00:16:49,569 --> 00:16:51,720
der linken Seite fünf.78 Bit Unsicherheit.

337
00:16:52,420 --> 00:16:56,340
Als Quiz für Sie: Was bedeutet das über die Anzahl der verbleibenden Möglichkeiten?

338
00:16:58,040 --> 00:17:01,224
Nun, es bedeutet, dass wir auf ein bisschen Unsicherheit reduziert sind, 

339
00:17:01,224 --> 00:17:04,540
was dasselbe ist, als würde man sagen, dass es zwei mögliche Antworten gibt.

340
00:17:04,700 --> 00:17:05,700
Es ist eine 50:50-Wahl.

341
00:17:06,500 --> 00:17:08,732
Und da Sie und ich wissen, welche Wörter gebräuchlicher sind, 

342
00:17:08,732 --> 00:17:10,640
wissen wir, dass die Antwort „Abgrund“ lauten sollte.

343
00:17:11,180 --> 00:17:13,280
Aber so wie es gerade geschrieben steht, weiß das Programm das nicht.

344
00:17:13,540 --> 00:17:17,282
Also macht es einfach weiter und versucht, so viele Informationen wie möglich zu sammeln, 

345
00:17:17,282 --> 00:17:19,859
bis nur noch eine Möglichkeit übrig ist, und dann errät es es.

346
00:17:20,380 --> 00:17:22,339
Wir brauchen also offensichtlich eine bessere Endspielstrategie.

347
00:17:22,599 --> 00:17:25,449
Aber nehmen wir an, wir nennen diese Version einen unserer Wortlöser und 

348
00:17:25,449 --> 00:17:28,260
führen dann einige Simulationen durch, um zu sehen, wie es funktioniert.

349
00:17:30,360 --> 00:17:34,120
Das funktioniert also so, dass jedes mögliche Weltspiel gespielt wird.

350
00:17:34,240 --> 00:17:38,540
Es geht darum, alle 2315 Wörter durchzugehen, die die eigentlichen Wortantworten sind.

351
00:17:38,540 --> 00:17:40,580
Im Grunde wird es als Testset verwendet.

352
00:17:41,360 --> 00:17:44,546
Und mit dieser naiven Methode, nicht darüber nachzudenken, wie häufig ein Wort ist, 

353
00:17:44,546 --> 00:17:47,923
und einfach zu versuchen, die Informationen bei jedem Schritt auf dem Weg zu maximieren, 

354
00:17:47,923 --> 00:17:49,820
bis es nur noch eine einzige Wahlmöglichkeit gibt.

355
00:17:50,360 --> 00:17:54,300
Am Ende der Simulation liegt die durchschnittliche Punktzahl bei etwa 4.124.

356
00:17:55,319 --> 00:17:58,010
Was nicht schlecht ist, um ehrlich zu sein, ich hatte irgendwie damit gerechnet, 

357
00:17:58,010 --> 00:17:59,240
dass es schlechter abschneiden würde.

358
00:17:59,660 --> 00:18:01,243
Aber die Leute, die Wordle spielen, werden Ihnen sagen, 

359
00:18:01,243 --> 00:18:02,600
dass sie es normalerweise in 4 Minuten schaffen.

360
00:18:02,860 --> 00:18:05,380
Die eigentliche Herausforderung besteht darin, in 3 so viele wie möglich zu bekommen.

361
00:18:05,380 --> 00:18:08,080
Es ist ein ziemlich großer Sprung zwischen der Punktzahl 4 und der Punktzahl 3.

362
00:18:08,860 --> 00:18:12,302
Die offensichtliche, niedrig hängende Frucht besteht hier darin, irgendwie einzubeziehen, 

363
00:18:12,302 --> 00:18:14,980
ob ein Wort gebräuchlich ist oder nicht, und wie wir das genau machen.

364
00:18:22,800 --> 00:18:25,077
Mein Ansatz besteht darin, eine Liste der relativen 

365
00:18:25,077 --> 00:18:27,880
Häufigkeiten aller Wörter in der englischen Sprache zu erhalten.

366
00:18:28,220 --> 00:18:31,363
Und ich habe gerade die Worthäufigkeitsdatenfunktion von Mathematica verwendet, 

367
00:18:31,363 --> 00:18:34,860
die ihrerseits aus dem öffentlichen Ngram-Datensatz für Englisch von Google Books stammt.

368
00:18:35,460 --> 00:18:37,741
Und es macht irgendwie Spaß, es anzusehen, wenn wir es zum Beispiel von 

369
00:18:37,741 --> 00:18:39,960
den häufigsten Wörtern zu den am wenigsten häufigen Wörtern sortieren.

370
00:18:40,120 --> 00:18:41,615
Offensichtlich sind dies die häufigsten Wörter 

371
00:18:41,615 --> 00:18:43,080
mit fünf Buchstaben in der englischen Sprache.

372
00:18:43,700 --> 00:18:45,840
Oder besser gesagt, dies ist die achthäufigste.

373
00:18:46,280 --> 00:18:48,880
Zuerst ist which, danach gibt es there und there.

374
00:18:49,260 --> 00:18:52,178
First selbst ist nicht first, sondern 9th, und es macht Sinn, 

375
00:18:52,178 --> 00:18:54,720
dass diese anderen Wörter häufiger vorkommen könnten, 

376
00:18:54,720 --> 00:18:58,580
wobei die Worte nach first nach, where sind und jene nur etwas seltener vorkommen.

377
00:18:59,160 --> 00:19:01,388
Wenn wir diese Daten nun verwenden, um zu modellieren, 

378
00:19:01,388 --> 00:19:04,631
wie wahrscheinlich es ist, dass jedes dieser Wörter die endgültige Antwort ist, 

379
00:19:04,631 --> 00:19:06,860
sollten sie nicht nur proportional zur Häufigkeit sein.

380
00:19:06,860 --> 00:19:11,061
Zum Beispiel, was mit einer Punktzahl von 0 bewertet wird.002 in diesem Datensatz, 

381
00:19:11,061 --> 00:19:15,060
während das Wort „zopf“ in gewissem Sinne etwa 1000-mal unwahrscheinlicher ist.

382
00:19:15,560 --> 00:19:17,182
Aber beide Wörter sind so häufig, dass sie mit 

383
00:19:17,182 --> 00:19:18,840
ziemlicher Sicherheit eine Überlegung wert sind.

384
00:19:19,340 --> 00:19:21,000
Wir wollen also eher einen binären Cutoff.

385
00:19:21,860 --> 00:19:24,326
Meine Vorgehensweise besteht darin, mir vorzustellen, 

386
00:19:24,326 --> 00:19:26,976
dass ich diese gesamte sortierte Liste von Wörtern nehme, 

387
00:19:26,976 --> 00:19:30,311
sie dann auf einer x-Achse anordne und dann die Sigmoidfunktion anwende, 

388
00:19:30,311 --> 00:19:32,458
was die Standardmethode für eine Funktion ist, 

389
00:19:32,458 --> 00:19:35,062
deren Ausgabe grundsätzlich binär ist entweder 0 oder 1, 

390
00:19:35,062 --> 00:19:38,260
aber dazwischen gibt es für diesen Unsicherheitsbereich eine Glättung.

391
00:19:39,160 --> 00:19:42,142
Im Wesentlichen ist die Wahrscheinlichkeit, die ich jedem Wort 

392
00:19:42,142 --> 00:19:44,510
für die Aufnahme in die endgültige Liste zuordne, 

393
00:19:44,510 --> 00:19:48,440
der Wert der Sigmoidfunktion oben, wo auch immer sie sich auf der x-Achse befindet.

394
00:19:49,520 --> 00:19:52,033
Dies hängt natürlich von einigen Parametern ab. 

395
00:19:52,033 --> 00:19:56,746
Beispielsweise bestimmt die Breite des Raums auf der X-Achse, den diese Wörter ausfüllen, 

396
00:19:56,746 --> 00:19:59,469
wie allmählich oder steil wir von 1 auf 0 abfallen, 

397
00:19:59,469 --> 00:20:03,240
und wo wir sie von links nach rechts positionieren, bestimmt die Grenze.

398
00:20:03,240 --> 00:20:04,851
Um ehrlich zu sein, habe ich das einfach so gemacht, 

399
00:20:04,851 --> 00:20:06,920
indem ich meinen Finger abgeleckt und ihn in den Wind gehalten habe.

400
00:20:07,140 --> 00:20:10,101
Ich habe die sortierte Liste durchgesehen und versucht, ein Fenster zu finden, 

401
00:20:10,101 --> 00:20:11,975
in dem ich beim Betrachten davon ausgegangen bin, 

402
00:20:11,975 --> 00:20:15,348
dass etwa die Hälfte dieser Wörter mit größerer Wahrscheinlichkeit die endgültige Antwort 

403
00:20:15,348 --> 00:20:17,260
sein werden, und habe dies als Grenzwert verwendet.

404
00:20:17,260 --> 00:20:19,735
Sobald wir eine solche Verteilung über die Wörter haben, 

405
00:20:19,735 --> 00:20:22,991
ergibt sich eine weitere Situation, in der die Entropie zu diesem wirklich 

406
00:20:22,991 --> 00:20:23,860
nützlichen Maß wird.

407
00:20:24,500 --> 00:20:27,536
Nehmen wir zum Beispiel an, wir spielen ein Spiel und beginnen mit meinen 

408
00:20:27,536 --> 00:20:29,834
alten Eröffnungsworten, die eine Feder und Nägel waren, 

409
00:20:29,834 --> 00:20:33,240
und enden in einer Situation, in der es vier mögliche Wörter gibt, die dazu passen.

410
00:20:33,560 --> 00:20:35,620
Nehmen wir an, wir halten sie alle für gleich wahrscheinlich.

411
00:20:36,220 --> 00:20:38,880
Ich frage Sie: Wie groß ist die Entropie dieser Verteilung?

412
00:20:41,080 --> 00:20:45,468
Nun, die Informationen, die jeder dieser Möglichkeiten zugeordnet sind, 

413
00:20:45,468 --> 00:20:50,040
werden die Logbasis 2 von 4 sein, da jede davon 1 und 4 ist, und das ist 2.

414
00:20:50,040 --> 00:20:52,460
Zwei Informationen, vier Möglichkeiten.

415
00:20:52,760 --> 00:20:53,580
Alles sehr schön und gut.

416
00:20:54,300 --> 00:20:57,800
Aber was wäre, wenn ich Ihnen sagen würde, dass es tatsächlich mehr als vier Spiele sind?

417
00:20:58,260 --> 00:21:00,236
Wenn wir die vollständige Wortliste durchsehen, 

418
00:21:00,236 --> 00:21:02,460
finden wir in Wirklichkeit 16 Wörter, die dazu passen.

419
00:21:02,580 --> 00:21:05,306
Aber nehmen wir an, dass unser Modell den anderen 12 Wörtern eine 

420
00:21:05,306 --> 00:21:08,859
sehr geringe Wahrscheinlichkeit zuordnet, tatsächlich die endgültige Antwort zu sein, 

421
00:21:08,859 --> 00:21:10,760
etwa 1 zu 1000, weil sie wirklich unklar sind.

422
00:21:11,500 --> 00:21:14,260
Nun möchte ich Sie fragen: Wie hoch ist die Entropie dieser Verteilung?

423
00:21:15,420 --> 00:21:19,204
Wenn die Entropie hier nur die Anzahl der Übereinstimmungen messen würde, 

424
00:21:19,204 --> 00:21:23,142
könnte man erwarten, dass sie etwa der Logarithmusbasis 2 von 16 entspricht, 

425
00:21:23,142 --> 00:21:25,700
was 4 wäre, zwei Bits mehr Unsicherheit als zuvor.

426
00:21:26,180 --> 00:21:29,151
Aber natürlich unterscheidet sich die tatsächliche Unsicherheit nicht wirklich von der, 

427
00:21:29,151 --> 00:21:29,860
die wir zuvor hatten.

428
00:21:30,160 --> 00:21:33,117
Nur weil es diese 12 wirklich obskuren Wörter gibt, heißt das nicht, 

429
00:21:33,117 --> 00:21:35,131
dass es umso überraschender wäre, zu erfahren, 

430
00:21:35,131 --> 00:21:37,360
dass die endgültige Antwort zum Beispiel Charme ist.

431
00:21:38,180 --> 00:21:41,848
Wenn Sie also die Berechnung hier tatsächlich durchführen und die Wahrscheinlichkeit 

432
00:21:41,848 --> 00:21:45,560
jedes Auftretens mal die entsprechenden Informationen addieren, erhalten Sie 2.11 Bit.

433
00:21:45,560 --> 00:21:47,623
Ich sage nur, es sind im Grunde genommen zwei Teile, 

434
00:21:47,623 --> 00:21:50,348
im Grunde genommen diese vier Möglichkeiten, aber aufgrund all dieser 

435
00:21:50,348 --> 00:21:53,073
höchst unwahrscheinlichen Ereignisse gibt es etwas mehr Unsicherheit, 

436
00:21:53,073 --> 00:21:56,500
obwohl man, wenn man sie erfahren würde, eine Menge Informationen daraus gewinnen würde.

437
00:21:57,160 --> 00:21:58,776
Wenn man also herauszoomt, ist dies ein Teil dessen, 

438
00:21:58,776 --> 00:22:01,400
was Wordle zu einem so schönen Beispiel für eine Lektion in Informationstheorie macht.

439
00:22:01,600 --> 00:22:04,640
Wir haben diese beiden unterschiedlichen Gefühlsanwendungen für Entropie.

440
00:22:05,160 --> 00:22:09,451
Der erste sagt uns, welche Informationen wir von einer gegebenen Vermutung erwarten, 

441
00:22:09,451 --> 00:22:13,995
und der zweite sagt, können wir die verbleibende Unsicherheit unter allen Wörtern messen, 

442
00:22:13,995 --> 00:22:15,460
die uns zur Verfügung stehen.

443
00:22:16,460 --> 00:22:18,999
Und ich sollte betonen: Im ersten Fall, in dem wir die erwarteten 

444
00:22:18,999 --> 00:22:22,462
Informationen einer Vermutung betrachten, wirkt sich dies auf die Entropieberechnung aus, 

445
00:22:22,462 --> 00:22:24,540
sobald wir eine ungleiche Gewichtung der Wörter haben.

446
00:22:24,980 --> 00:22:27,932
Lassen Sie mich zum Beispiel den gleichen Fall der mit „Weary“ verbundenen 

447
00:22:27,932 --> 00:22:30,098
Verteilung aufgreifen, den wir zuvor betrachtet haben, 

448
00:22:30,098 --> 00:22:33,050
diesmal jedoch unter Verwendung einer ungleichmäßigen Verteilung über alle 

449
00:22:33,050 --> 00:22:33,720
möglichen Wörter.

450
00:22:34,500 --> 00:22:38,280
Lassen Sie mich sehen, ob ich hier einen Teil finde, der es ziemlich gut veranschaulicht.

451
00:22:40,940 --> 00:22:42,360
Okay, hier ist das ziemlich gut.

452
00:22:42,360 --> 00:22:45,552
Hier haben wir zwei benachbarte Muster, die ungefähr gleich wahrscheinlich sind, 

453
00:22:45,552 --> 00:22:49,100
aber für eines davon haben wir erfahren, dass es 32 mögliche Wörter gibt, die dazu passen.

454
00:22:49,280 --> 00:22:51,714
Und wenn wir nachsehen, was sie sind, sind das diese 32, 

455
00:22:51,714 --> 00:22:53,977
die allesamt nur sehr unwahrscheinliche Wörter sind, 

456
00:22:53,977 --> 00:22:55,600
wenn man sie mit den Augen überfliegt.

457
00:22:55,840 --> 00:22:58,904
Es ist schwer, irgendwelche zu finden, die sich wie plausible Antworten anfühlen, 

458
00:22:58,904 --> 00:23:02,194
vielleicht Schreie, aber wenn wir uns das benachbarte Muster in der Verteilung ansehen, 

459
00:23:02,194 --> 00:23:04,548
das als ungefähr genauso wahrscheinlich gilt, wird uns gesagt, 

460
00:23:04,548 --> 00:23:07,875
dass es nur 8 mögliche Übereinstimmungen gibt, also ein Viertel viele Übereinstimmungen, 

461
00:23:07,875 --> 00:23:09,520
aber es ist ungefähr genauso wahrscheinlich.

462
00:23:09,860 --> 00:23:12,140
Und wenn wir diese Übereinstimmungen abrufen, können wir sehen, warum.

463
00:23:12,500 --> 00:23:16,300
Einige davon sind tatsächlich plausible Antworten, wie „Ring“, „Wrath“ oder „Raps“.

464
00:23:17,900 --> 00:23:20,059
Um zu veranschaulichen, wie wir das alles integrieren, 

465
00:23:20,059 --> 00:23:22,021
möchte ich hier Version 2 des Wordlebot aufrufen. 

466
00:23:22,021 --> 00:23:25,280
Es gibt zwei oder drei Hauptunterschiede zur ersten Version, die wir gesehen haben.

467
00:23:25,860 --> 00:23:28,777
Zunächst einmal verwendet die Art und Weise, wie wir diese Entropien, 

468
00:23:28,777 --> 00:23:31,654
diese erwarteten Informationswerte, berechnen, wie ich gerade sagte, 

469
00:23:31,654 --> 00:23:34,155
jetzt die verfeinerten Verteilungen über die Muster hinweg, 

470
00:23:34,155 --> 00:23:37,031
die die Wahrscheinlichkeit berücksichtigen, dass ein bestimmtes Wort 

471
00:23:37,031 --> 00:23:38,240
tatsächlich die Antwort wäre.

472
00:23:38,879 --> 00:23:43,820
Zufällig sind Tränen immer noch die Nummer 1, obwohl die folgenden etwas anders sind.

473
00:23:44,360 --> 00:23:46,971
Zweitens wird es bei der Rangfolge seiner Top-Tipps nun ein Modell 

474
00:23:46,971 --> 00:23:50,051
der Wahrscheinlichkeit behalten, dass jedes Wort die tatsächliche Antwort ist, 

475
00:23:50,051 --> 00:23:53,247
und es wird dies in seine Entscheidung einbeziehen, was leichter zu erkennen ist, 

476
00:23:53,247 --> 00:23:55,080
wenn wir ein paar Vermutungen dazu haben Tisch.

477
00:23:55,860 --> 00:23:58,461
Auch hier ignorieren wir die Empfehlung, weil wir nicht zulassen können, 

478
00:23:58,461 --> 00:23:59,780
dass Maschinen unser Leben bestimmen.

479
00:24:01,140 --> 00:24:04,416
Und ich denke, ich sollte noch etwas erwähnen, das hier links anders ist: 

480
00:24:04,416 --> 00:24:07,293
Der Unsicherheitswert, diese Anzahl von Bits, ist nicht mehr nur 

481
00:24:07,293 --> 00:24:09,640
redundant mit der Anzahl möglicher Übereinstimmungen.

482
00:24:10,080 --> 00:24:14,706
Wenn wir es jetzt hochziehen und 2 hoch 8 berechnen.02, was etwas über 256 liegt, 

483
00:24:14,706 --> 00:24:19,671
ich schätze 259. Was damit gesagt wird, ist, dass, obwohl es insgesamt 526 Wörter gibt, 

484
00:24:19,671 --> 00:24:24,184
die diesem Muster tatsächlich entsprechen, das Ausmaß der Unsicherheit eher dem 

485
00:24:24,184 --> 00:24:28,980
entspricht, das es wäre, wenn es 259 mit gleicher Wahrscheinlichkeit gäbe Ergebnisse.

486
00:24:29,720 --> 00:24:30,740
Man kann es sich so vorstellen.

487
00:24:31,020 --> 00:24:34,235
Es weiß, dass Borx nicht die Antwort ist, das Gleiche gilt für Yorts, 

488
00:24:34,235 --> 00:24:37,680
Zorl und Zorus, daher ist es etwas weniger unsicher als im vorherigen Fall.

489
00:24:37,820 --> 00:24:39,280
Diese Anzahl von Bits wird kleiner sein.

490
00:24:40,220 --> 00:24:44,241
Und wenn ich das Spiel weiter spiele, verfeinere ich dies mit ein paar Vermutungen, 

491
00:24:44,241 --> 00:24:46,540
die zu dem passen, was ich hier erklären möchte.

492
00:24:48,360 --> 00:24:50,938
Bei der vierten Vermutung, wenn Sie einen Blick auf die Top-Picks werfen, 

493
00:24:50,938 --> 00:24:53,760
können Sie erkennen, dass es nicht mehr nur um die Maximierung der Entropie geht.

494
00:24:54,460 --> 00:24:57,317
An diesem Punkt gibt es also technisch gesehen sieben Möglichkeiten, 

495
00:24:57,317 --> 00:25:00,300
aber die einzigen mit einer sinnvollen Chance sind Schlafsäle und Worte.

496
00:25:00,300 --> 00:25:03,425
Und Sie können sehen, dass es wichtiger ist, diese beiden Werte zu wählen 

497
00:25:03,425 --> 00:25:06,720
als alle anderen Werte, die streng genommen mehr Informationen liefern würden.

498
00:25:07,240 --> 00:25:10,151
Als ich das zum ersten Mal gemacht habe, habe ich einfach diese beiden Zahlen addiert, 

499
00:25:10,151 --> 00:25:12,962
um die Qualität jeder Vermutung zu messen, was tatsächlich besser funktioniert hat, 

500
00:25:12,962 --> 00:25:13,900
als Sie vielleicht vermuten.

501
00:25:14,300 --> 00:25:15,739
Aber es fühlte sich wirklich nicht systematisch an, 

502
00:25:15,739 --> 00:25:17,207
und ich bin mir sicher, dass es andere Ansätze gibt, 

503
00:25:17,207 --> 00:25:19,340
die die Leute verfolgen könnten, aber hier ist der, bei dem ich gelandet bin.

504
00:25:19,760 --> 00:25:22,685
Wenn wir die Aussicht auf eine nächste Vermutung in Betracht ziehen, 

505
00:25:22,685 --> 00:25:25,525
wie in diesem Fall Wörter, ist das, was uns wirklich interessiert, 

506
00:25:25,525 --> 00:25:27,900
das erwartete Ergebnis unseres Spiels, wenn wir das tun.

507
00:25:28,230 --> 00:25:30,818
Und um diesen erwarteten Wert zu berechnen, sagen wir, 

508
00:25:30,818 --> 00:25:34,582
wie hoch die Wahrscheinlichkeit ist, dass Wörter die tatsächliche Antwort sind, 

509
00:25:34,582 --> 00:25:35,900
was derzeit 58 % entspricht.

510
00:25:36,040 --> 00:25:37,808
Wir gehen davon aus, dass unser Punktestand in 

511
00:25:37,808 --> 00:25:39,540
diesem Spiel bei einer Chance von 58 % 4 wäre.

512
00:25:40,320 --> 00:25:43,512
Und dann, wenn die Wahrscheinlichkeit 1 minus 58 % beträgt, 

513
00:25:43,512 --> 00:25:45,640
wird unser Ergebnis mehr als 4 betragen.

514
00:25:46,220 --> 00:25:49,686
Wie viel mehr wissen wir nicht, aber wir können es anhand der voraussichtlichen 

515
00:25:49,686 --> 00:25:52,460
Unsicherheit abschätzen, sobald wir diesen Punkt erreicht haben.

516
00:25:52,960 --> 00:25:55,940
Konkret gibt es im Moment 1.44 Bit Unsicherheit.

517
00:25:56,440 --> 00:25:59,697
Wenn wir Wörter erraten, sagt uns das, dass die erwartete Information, 

518
00:25:59,697 --> 00:26:01,120
die wir erhalten, 1 ist.27 Bit.

519
00:26:01,620 --> 00:26:04,796
Wenn wir also Wörter erraten, stellt dieser Unterschied dar, 

520
00:26:04,796 --> 00:26:07,660
wie viel Unsicherheit uns danach wahrscheinlich bleibt.

521
00:26:08,260 --> 00:26:11,000
Was wir brauchen, ist eine Art Funktion, die ich hier f nenne, 

522
00:26:11,000 --> 00:26:13,740
die diese Unsicherheit mit einem erwarteten Ergebnis verknüpft.

523
00:26:14,240 --> 00:26:18,281
Und die Vorgehensweise bestand darin, einfach eine Reihe von Daten aus früheren Spielen 

524
00:26:18,281 --> 00:26:21,083
basierend auf Version 1 des Bots aufzuzeichnen, um zu sagen, 

525
00:26:21,083 --> 00:26:24,942
wie hoch der tatsächliche Punktestand nach verschiedenen Punkten war, mit gewissen, 

526
00:26:24,942 --> 00:26:26,320
sehr messbaren Unsicherheiten.

527
00:26:27,020 --> 00:26:30,967
Zum Beispiel diese Datenpunkte hier, die über einem Wert von etwa 8 liegen.7 oder 

528
00:26:30,967 --> 00:26:35,204
so sagen für einige Spiele nach einem Zeitpunkt, an dem es 8 waren.7 Bits Unsicherheit, 

529
00:26:35,204 --> 00:26:38,960
es waren zwei Vermutungen erforderlich, um die endgültige Antwort zu erhalten.

530
00:26:39,320 --> 00:26:40,792
Bei anderen Spielen waren drei Schätzungen erforderlich, 

531
00:26:40,792 --> 00:26:42,240
bei anderen Spielen waren vier Schätzungen erforderlich.

532
00:26:43,140 --> 00:26:46,757
Wenn wir hier nach links wechseln, sagen alle Punkte über Null, dass immer dann, 

533
00:26:46,757 --> 00:26:50,464
wenn es null Unsicherheitsbits gibt, das heißt, dass es nur eine Möglichkeit gibt, 

534
00:26:50,464 --> 00:26:54,260
die Anzahl der erforderlichen Schätzungen immer nur eins beträgt, was beruhigend ist.

535
00:26:54,780 --> 00:26:56,948
Wann immer es ein bisschen Unsicherheit gab, was bedeutete, 

536
00:26:56,948 --> 00:26:59,297
dass es sich im Wesentlichen nur um zwei Möglichkeiten handelte, 

537
00:26:59,297 --> 00:27:01,104
war manchmal eine weitere Vermutung erforderlich, 

538
00:27:01,104 --> 00:27:03,020
manchmal waren zwei weitere Vermutungen erforderlich.

539
00:27:03,080 --> 00:27:05,240
Und so weiter und so fort hier.

540
00:27:05,740 --> 00:27:08,088
Eine vielleicht etwas einfachere Möglichkeit, diese Daten zu visualisieren, 

541
00:27:08,088 --> 00:27:10,220
besteht darin, sie zusammenzufassen und Durchschnittswerte zu bilden.

542
00:27:11,000 --> 00:27:14,138
Zum Beispiel dieser Balken hier, der besagt, dass von allen Punkten, 

543
00:27:14,138 --> 00:27:16,321
bei denen wir eine gewisse Unsicherheit hatten, 

544
00:27:16,321 --> 00:27:19,960
die Anzahl der erforderlichen neuen Vermutungen im Durchschnitt etwa 1 betrug.5.

545
00:27:22,140 --> 00:27:25,181
Und der Balken hier besagt, dass bei all den verschiedenen Spielen, 

546
00:27:25,181 --> 00:27:27,999
bei denen die Unsicherheit irgendwann etwas über vier Bit lag, 

547
00:27:27,999 --> 00:27:31,041
was einer Eingrenzung auf 16 verschiedene Möglichkeiten entspricht, 

548
00:27:31,041 --> 00:27:34,082
ab diesem Zeitpunkt im Durchschnitt etwas mehr als zwei Vermutungen 

549
00:27:34,082 --> 00:27:35,380
erforderlich sind nach vorne.

550
00:27:36,060 --> 00:27:37,873
Und von hier aus habe ich einfach eine Regression durchgeführt, 

551
00:27:37,873 --> 00:27:39,460
um eine Funktion anzupassen, die hier sinnvoll erschien.

552
00:27:39,980 --> 00:27:42,472
Und bedenken Sie, dass der Sinn all dessen darin besteht, 

553
00:27:42,472 --> 00:27:45,264
dass wir die Intuition quantifizieren können, dass die erwartete 

554
00:27:45,264 --> 00:27:48,960
Punktzahl umso niedriger sein wird, je mehr Informationen wir aus einem Wort gewinnen.

555
00:27:49,680 --> 00:27:52,762
Also hiermit als Version 2.0, wenn wir zurückgehen und den 

556
00:27:52,762 --> 00:27:56,053
gleichen Satz Simulationen durchführen und ihn gegen alle 2315 

557
00:27:56,053 --> 00:27:59,240
möglichen Wortantworten spielen lassen, wie funktioniert das?

558
00:28:00,280 --> 00:28:03,420
Nun, im Gegensatz zu unserer ersten Version ist es definitiv besser, was beruhigend ist.

559
00:28:04,020 --> 00:28:06,454
Alles in allem liegt der Durchschnitt bei etwa 3.6, 

560
00:28:06,454 --> 00:28:10,574
obwohl es im Gegensatz zur ersten Version ein paar Mal Verluste gibt und in diesem Fall 

561
00:28:10,574 --> 00:28:12,120
mehr als sechs erforderlich sind.

562
00:28:12,639 --> 00:28:15,434
Vermutlich, weil es Zeiten gibt, in denen es darum geht, diesen Kompromiss einzugehen, 

563
00:28:15,434 --> 00:28:17,940
um tatsächlich das Ziel zu erreichen, anstatt die Informationen zu maximieren.

564
00:28:19,040 --> 00:28:21,000
Können wir es also besser machen als 3?6?

565
00:28:22,080 --> 00:28:22,920
Das können wir auf jeden Fall.

566
00:28:23,280 --> 00:28:25,339
Nun habe ich zu Beginn gesagt, dass es am meisten Spaß macht, 

567
00:28:25,339 --> 00:28:27,399
zu versuchen, nicht die wahre Liste der Wort-Antworten in die 

568
00:28:27,399 --> 00:28:29,360
Art und Weise zu integrieren, wie das Modell erstellt wird.

569
00:28:29,880 --> 00:28:32,376
Aber wenn wir es integrieren, lag die beste Leistung, 

570
00:28:32,376 --> 00:28:34,180
die ich erzielen konnte, bei etwa 3.43.

571
00:28:35,160 --> 00:28:37,852
Wenn wir also versuchen, bei der Auswahl dieser vorherigen Verteilung, 

572
00:28:37,852 --> 00:28:40,468
dieser 3, anspruchsvoller zu werden, als nur Worthäufigkeitsdaten zu 

573
00:28:40,468 --> 00:28:42,630
verwenden.43 gibt wahrscheinlich einen Höchstwert dafür, 

574
00:28:42,630 --> 00:28:45,740
wie gut wir damit werden könnten, oder zumindest, wie gut ich damit werden könnte.

575
00:28:46,240 --> 00:28:48,469
Diese beste Leistung nutzt im Wesentlichen nur die Ideen, 

576
00:28:48,469 --> 00:28:51,083
über die ich hier gesprochen habe, geht aber noch ein wenig weiter, 

577
00:28:51,083 --> 00:28:54,043
als würde die Suche nach den erwarteten Informationen zwei Schritte vorwärts 

578
00:28:54,043 --> 00:28:55,120
statt nur einen durchführen.

579
00:28:55,620 --> 00:28:57,558
Ursprünglich hatte ich vor, mehr darüber zu reden, 

580
00:28:57,558 --> 00:29:00,220
aber mir ist klar, dass wir ohnehin schon ziemlich weit gekommen sind.

581
00:29:00,580 --> 00:29:03,395
Das Einzige, was ich sagen möchte, ist, dass nach dieser zweistufigen Suche 

582
00:29:03,395 --> 00:29:05,988
und dem anschließenden Ausführen einiger Beispielsimulationen bei den 

583
00:29:05,988 --> 00:29:09,100
Top-Kandidaten es für mich zumindest so aussieht, als ob Crane der beste Opener ist.

584
00:29:09,100 --> 00:29:10,060
Wer hätte es gedacht?

585
00:29:10,920 --> 00:29:14,723
Auch wenn Sie die wahre Wortliste verwenden, um Ihren Möglichkeitenraum zu bestimmen, 

586
00:29:14,723 --> 00:29:17,820
beträgt die Unsicherheit, mit der Sie beginnen, etwas mehr als 11 Bit.

587
00:29:18,300 --> 00:29:21,929
Und es stellt sich heraus, dass allein bei einer Brute-Force-Suche die maximal 

588
00:29:21,929 --> 00:29:25,880
mögliche erwartete Information nach den ersten beiden Schätzungen etwa 10 Bit beträgt.

589
00:29:26,500 --> 00:29:30,436
Das deutet darauf hin, dass Sie im besten Fall nach Ihren ersten beiden Vermutungen 

590
00:29:30,436 --> 00:29:34,560
und vollkommen optimalem Spiel mit etwa einem kleinen Unsicherheitsfaktor zurückbleiben.

591
00:29:34,800 --> 00:29:37,960
Das ist das Gleiche, als ob man auf zwei mögliche Vermutungen angewiesen wäre.

592
00:29:37,960 --> 00:29:40,826
Daher denke ich, dass es fair und wahrscheinlich ziemlich konservativ ist, 

593
00:29:40,826 --> 00:29:43,271
zu sagen, dass Sie niemals einen Algorithmus schreiben könnten, 

594
00:29:43,271 --> 00:29:45,679
der diesen Durchschnitt auf 3 reduziert, denn mit den Wörtern, 

595
00:29:45,679 --> 00:29:48,048
die Ihnen zur Verfügung stehen, gibt es einfach keinen Platz, 

596
00:29:48,048 --> 00:29:50,914
um nach nur zwei Schritten genügend Informationen zu erhalten in der Lage, 

597
00:29:50,914 --> 00:29:53,360
die Antwort im dritten Slot jedes Mal fehlerfrei zu garantieren.

