1
00:00:00,000 --> 00:00:03,288
Wurdle 游戏在过去一两个月里非常 

2
00:00:03,288 --> 00:00:06,412
火爆，从来没有人会忽视数学课的机会， 

3
00:00:06,412 --> 00:00:09,700
我觉得这个游戏是信息论课程中一个非常好 

4
00:00:09,700 --> 00:00:12,660
的中心例子，特别是一个称为熵的主题。

5
00:00:13,920 --> 00:00:16,909
你看，就像很多人一样，我有点陷入了这个 

6
00:00:16,909 --> 00:00:19,899
谜题，而且像很多程序员一样，我也陷入了 

7
00:00:19,899 --> 00:00:22,740
尝试编写一种算法来尽可能最佳地玩游戏。

8
00:00:23,180 --> 00:00:25,968
我想我在这里要做的只是与你们讨论我 

9
00:00:25,968 --> 00:00:28,601
的一些过程，并解释其中的一些数学 

10
00:00:28,601 --> 00:00:31,080
，因为整个算法以熵的概念为中心。

11
00:00:38,700 --> 00:00:41,640
首先，如果您还没有听说过，那么什么是 Wurdle？

12
00:00:42,040 --> 00:00:45,099
在我们介绍游戏规则的同时，让我也 

13
00:00:45,099 --> 00:00:48,160
预览一下我们要做什么，即开发一个 

14
00:00:48,160 --> 00:00:51,040
基本上可以为我们玩游戏的小算法。

15
00:00:51,360 --> 00:00:53,271
虽然我还没有完成今天的 Wurdle，但现在 

16
00:00:53,271 --> 00:00:55,100
是 2 月 4 日，我们将看看机器人的表现。

17
00:00:55,480 --> 00:00:58,037
Wurdle 的目标是猜测一个神秘的五 

18
00:00:58,037 --> 00:01:00,340
个字母单词，您有六次不同的猜测机会。

19
00:01:00,840 --> 00:01:04,379
例如，我的 Wurdle 机器人建议我从猜测起重机开始。

20
00:01:05,180 --> 00:01:07,768
每次您进行猜测时，您都会获得一些有关 

21
00:01:07,768 --> 00:01:10,220
您的猜测与真实答案的接近程度的信息。

22
00:01:10,920 --> 00:01:14,100
灰色框告诉我实际答案中没有 C。

23
00:01:14,520 --> 00:01:17,840
黄色框告诉我有一个 R，但它不在那个位置。

24
00:01:18,240 --> 00:01:22,240
绿色框告诉我秘密词确实有一个 A，而且位于第三个位置。

25
00:01:22,720 --> 00:01:24,580
然后就没有N了，也没有E了。

26
00:01:25,200 --> 00:01:27,340
那么让我进去告诉 Wurdle 机器人该信息。

27
00:01:27,340 --> 00:01:30,320
我们从起重机开始，有灰色、黄色、绿色、灰色、灰色。

28
00:01:31,420 --> 00:01:34,940
不要担心它现在显示的所有数据，我会在适当的时候解释这一点。

29
00:01:35,460 --> 00:01:38,820
但对于我们的第二个选择来说，它的首要建议是小技巧。

30
00:01:39,560 --> 00:01:42,601
你的猜测确实必须是一个实际的五个字母的单词，但正 

31
00:01:42,601 --> 00:01:45,400
如你将看到的，它实际上让你猜测的内容相当自由。

32
00:01:46,200 --> 00:01:47,440
在这种情况下，我们尝试一下shtic。

33
00:01:48,780 --> 00:01:50,180
好吧，事情看起来相当不错。

34
00:01:50,260 --> 00:01:52,063
我们按了 S 和 H，所以我们知

35
00:01:52,063 --> 00:01:53,980
道前三个字母，我们知道有一个 R。

36
00:01:53,980 --> 00:01:58,700
所以它会像 SHA 某些 R 或 SHA R 某些东西。

37
00:01:59,620 --> 00:02:01,989
看起来 Wurdle 机器人知道它只有 

38
00:02:01,989 --> 00:02:04,240
两种可能性，要么是碎片，要么是锋利的。

39
00:02:05,100 --> 00:02:07,689
在这一点上，他们之间存在着一种折腾，所以我想可能只 

40
00:02:07,689 --> 00:02:10,080
是因为它是按字母顺序排列的，所以它与分片相匹配。

41
00:02:11,220 --> 00:02:12,860
万岁，这才是真正的答案。

42
00:02:12,960 --> 00:02:13,780
所以我们三分就搞定了。

43
00:02:14,600 --> 00:02:17,595
如果你想知道这是否有好处，我听到一个人的说法是，对 

44
00:02:17,595 --> 00:02:20,360
于Wurdle来说，四杆是标准杆，三杆是小鸟球。

45
00:02:20,680 --> 00:02:22,480
我认为这是一个非常恰当的比喻。

46
00:02:22,480 --> 00:02:27,020
你必须坚持自己的比赛才能获得四分，但这当然并不疯狂。

47
00:02:27,180 --> 00:02:29,920
但当你拿到三分的时候，感觉棒极了。

48
00:02:30,880 --> 00:02:33,521
因此，如果您愿意的话，我在这里想做的就是从一开始就 

49
00:02:33,521 --> 00:02:35,960
谈谈我如何处理 Wurdle 机器人的思考过程。

50
00:02:36,480 --> 00:02:39,440
就像我说的，这确实是信息论课程的借口。

51
00:02:39,740 --> 00:02:42,820
主要目标是解释什么是信息和什么是熵。

52
00:02:48,220 --> 00:02:51,053
在解决这个问题时，我的第一个想法 

53
00:02:51,053 --> 00:02:53,720
是看看英语中不同字母的相对频率。

54
00:02:54,380 --> 00:02:56,948
所以我想，好吧，是否有一个开局猜测或一 

55
00:02:56,948 --> 00:02:59,260
对开局猜测可以命中这些最常见的字母？

56
00:02:59,960 --> 00:03:03,000
我非常喜欢做其他事情，然后做指甲。

57
00:03:03,760 --> 00:03:05,733
我们的想法是，如果你击中一个字母，你知道 

58
00:03:05,733 --> 00:03:07,520
，你会得到绿色或黄色，这总是感觉很好。

59
00:03:07,520 --> 00:03:08,840
感觉就像你正在获取信息。

60
00:03:09,340 --> 00:03:12,124
但在这些情况下，即使你没有击中并且总 

61
00:03:12,124 --> 00:03:14,908
是得到灰色，这仍然为你提供了大量信息 

62
00:03:14,908 --> 00:03:17,400
，因为很少找到没有这些字母的单词。

63
00:03:18,140 --> 00:03:20,742
但即便如此，这仍然感觉不是超级系统 

64
00:03:20,742 --> 00:03:23,200
，因为例如，它没有考虑字母的顺序。

65
00:03:23,560 --> 00:03:25,300
当我可以输入蜗牛时，为什么还要输入指甲？

66
00:03:26,080 --> 00:03:27,500
最后加个S是不是更好？

67
00:03:27,820 --> 00:03:28,680
我不太确定。

68
00:03:29,240 --> 00:03:32,951
现在，我的一个朋友说他喜欢用“weary”这个词开头，这让 

69
00:03:32,951 --> 00:03:36,540
我有点惊讶，因为里面有一些不常见的字母，比如 W 和 Y。

70
00:03:37,120 --> 00:03:39,000
但谁知道呢，也许这是一个更好的开局。

71
00:03:39,320 --> 00:03:44,320
我们是否可以给出某种定量评 分来判断潜在猜测的质量？

72
00:03:45,340 --> 00:03:48,454
现在，为了设置我们对可能的猜测进行排名的 

73
00:03:48,454 --> 00:03:51,420
方式，让我们回顾一下游戏的具体设置方式。

74
00:03:51,420 --> 00:03:54,721
因此，您可以输入一个单词列表，这些单词被视为 

75
00:03:54,721 --> 00:03:57,880
有效的猜测，长度约为 13,000 个单词。

76
00:03:58,320 --> 00:04:02,453
但当你仔细观察时，你会发现有很多非常不常见的东西，比如 

77
00:04:02,453 --> 00:04:06,440
头像、阿里和ARG，以及拼字游戏中引发家庭争吵的词语。

78
00:04:06,960 --> 00:04:10,540
但游戏的氛围是答案总是一个相当常见的词。

79
00:04:10,960 --> 00:04:13,226
事实上，还有另一个大约 2300 

80
00:04:13,226 --> 00:04:15,360
个单词的列表，它们是可能的答案。

81
00:04:15,940 --> 00:04:18,687
这是一个人工策划的列表，我认为是由游戏 

82
00:04:18,687 --> 00:04:21,160
创建者的女朋友专门制作的，这很有趣。

83
00:04:21,820 --> 00:04:24,516
但我想做的是，我们对这个项目的挑战是看看

84
00:04:24,516 --> 00:04:27,348
我们是否可以编写一个解 决 Wordle 

85
00:04:27,348 --> 00:04:30,180
的程序，该程序不包含有关此列表的先前知识。

86
00:04:30,720 --> 00:04:34,640
一方面，有很多非常常见的五个 字母单词您在该列表中找不到。

87
00:04:34,940 --> 00:04:38,272
因此，最好编写一个更具弹性的程序，并且可以与 

88
00:04:38,272 --> 00:04:41,460
任何人玩 Wordle，而不仅仅是官方网站。

89
00:04:41,920 --> 00:04:44,541
我们之所以知道可能答案的列表是 

90
00:04:44,541 --> 00:04:47,000
什么，是因为它在源代码中可见。

91
00:04:47,000 --> 00:04:53,260
但它在源代码中可见的方式是按 照每天出现答案的特定顺序。

92
00:04:53,260 --> 00:04:55,840
所以你总是可以查找明天的答案。

93
00:04:56,420 --> 00:04:58,880
很明显，使用该列表在某种程度上是作弊行为。

94
00:04:59,100 --> 00:05:03,090
使谜题更有趣、信息论课程更丰富的方法 

95
00:05:03,090 --> 00:05:06,870
是使用一些更通用的数据，例如相对词 

96
00:05:06,870 --> 00:05:10,440
频，来捕捉对更常见词的偏好的直觉。

97
00:05:11,600 --> 00:05:15,900
那么在这13000种可能性中，我们应该如何选择开局猜测呢？

98
00:05:16,400 --> 00:05:19,780
例如，如果我的朋友提出厌烦，我们应该如何分析它的质量？

99
00:05:20,520 --> 00:05:23,996
好吧，他说他喜欢那个不太可能的 W 的原因是他喜欢 

100
00:05:23,996 --> 00:05:27,340
远射的本质，如果你击中了那个 W，那感觉是多么好。

101
00:05:27,920 --> 00:05:31,845
例如，如果第一个揭示的模式是这样的，那么这个 

102
00:05:31,845 --> 00:05:35,600
庞大的词典中只有 58 个单词与该模式匹配。

103
00:05:36,060 --> 00:05:38,400
因此，与 13,000 人相比，这是一个巨大的减少。

104
00:05:38,780 --> 00:05:43,020
但当然，另一方面是，获得这样的模式非常罕见。

105
00:05:43,020 --> 00:05:47,190
具体来说，如果每个单词作为答案的可能性相同，则达到 

106
00:05:47,190 --> 00:05:51,040
此模式的概率将为 58 除以大约 13,000。

107
00:05:51,580 --> 00:05:53,600
当然，它们成为答案的可能性并不相同。

108
00:05:53,720 --> 00:05:56,220
其中大部分都是非常晦涩甚至有问题的词语。

109
00:05:56,600 --> 00:05:59,219
但至少对于我们第一次通过这一切，我们假设它 

110
00:05:59,219 --> 00:06:01,600
们的可能性相同，然后稍后再对其进行完善。

111
00:06:02,020 --> 00:06:06,720
关键是，包含大量信息的模式本质上不太可能发生。

112
00:06:07,280 --> 00:06:10,800
事实上，提供信息意味着这是不可能的。

113
00:06:11,719 --> 00:06:18,120
在这个开口中看到的更可能的模式 是这样的，当然其中没有 W。

114
00:06:18,240 --> 00:06:21,400
也许有E，也许没有A，没有R，没有Y。

115
00:06:22,080 --> 00:06:24,560
在本例中，有 1400 个可能的匹配项。

116
00:06:25,080 --> 00:06:30,600
如果所有可能性均等，则您看到 的模式的概率约为 11%。

117
00:06:30,900 --> 00:06:33,340
因此，最可能的结果也是信息最少的。

118
00:06:34,240 --> 00:06:37,778
为了获得更全面的视角，让我向您展示您可 

119
00:06:37,778 --> 00:06:41,140
能看到的所有不同模式的概率的完整分布。

120
00:06:41,740 --> 00:06:45,383
因此，您看到的每个条形都对应于可能显示的颜 

121
00:06:45,383 --> 00:06:49,027
色模式，其中有 3 到 5 种可能性，并且 

122
00:06:49,027 --> 00:06:52,340
它们从左到右、最常见到最不常见进行组织。

123
00:06:52,920 --> 00:06:56,000
所以这里最常见的可能性是你得到的都是灰色的。

124
00:06:56,100 --> 00:06:58,120
这种情况发生的概率约为 14%。

125
00:06:58,580 --> 00:07:02,253
当你进行猜测时，你所希望的是你最终会出现在这条 

126
00:07:02,253 --> 00:07:05,773
长尾中的某个地方，就像在这里，与这个显然看起 

127
00:07:05,773 --> 00:07:09,140
来像这样的模式相匹配的可能性只有 18 种。

128
00:07:09,920 --> 00:07:11,797
或者，如果我们冒险向左走一点，

129
00:07:11,797 --> 00:07:13,800
你知道，也许我们会一直走到这里。

130
00:07:14,940 --> 00:07:16,180
好的，这是给你的一个很好的谜题。

131
00:07:16,540 --> 00:07:19,343
英语中以 W 开头、以 Y 结尾、其 

132
00:07:19,343 --> 00:07:22,000
中某个位置有 R 的三个单词是什么？

133
00:07:22,480 --> 00:07:26,800
事实证明，让我们看看，答案是罗嗦、狡猾和讽刺的。

134
00:07:27,500 --> 00:07:31,799
因此，为了判断这个词的整体效果如何，我们需要某 

135
00:07:31,799 --> 00:07:35,740
种方式来衡量您将从该分布中获得的预期信息量。

136
00:07:35,740 --> 00:07:40,417
如果我们检查每个模式，并将其发生的概率乘以衡量其 

137
00:07:40,417 --> 00:07:44,720
信息量的因素，这也许可以给我们一个客观的分数。

138
00:07:45,960 --> 00:07:49,840
现在，您对某事物应该是什么的第一直觉可能是匹配的数量。

139
00:07:50,160 --> 00:07:52,400
您想要较低的平均匹配数。

140
00:07:52,800 --> 00:07:55,570
但相反，我想使用一种更通用的衡量标准，我们通

141
00:07:55,570 --> 00:07:57,963
常将其归因于信息 ，并且一旦我们为这 

142
00:07:57,963 --> 00:08:00,607
13,000 个单词中的每一个分配不同的 

143
00:08:00,607 --> 00:08:04,260
概率来判断它们是否实际上是答案，这种衡量标准就会更加灵活。

144
00:08:10,320 --> 00:08:13,740
信息的标准单位是位，它的公式有点有趣 

145
00:08:13,740 --> 00:08:16,980
，但如果我们只看例子，它真的很直观。

146
00:08:17,780 --> 00:08:20,721
如果你的观察结果将你的可能性空间减 

147
00:08:20,721 --> 00:08:23,500
少了一半，我们就说它只有一点信息。

148
00:08:24,180 --> 00:08:27,782
在我们的示例中，可能性空间是所有可能的单词，结果表明，五 

149
00:08:27,782 --> 00:08:31,260
个字母的单词中大约一半有 S，比这个少一点，但大约一半。

150
00:08:31,780 --> 00:08:34,320
这样观察就会给你一点信息。

151
00:08:34,880 --> 00:08:38,384
相反，如果一个新事实将可能性空间减 

152
00:08:38,384 --> 00:08:41,500
少了四倍，我们就说它有两位信息。

153
00:08:41,980 --> 00:08:44,460
例如，事实证明这些单词中大约四分之一有 T。

154
00:08:45,020 --> 00:08:48,048
如果观察将该空间缩小八分之一，我 

155
00:08:48,048 --> 00:08:50,720
们就说它是三位信息，依此类推。

156
00:08:50,900 --> 00:08:55,060
四位将其切割为 16 度，五位将其切割为 32 度。

157
00:08:55,060 --> 00:08:59,240
所以现在您可能想停下来问自己，就发生 

158
00:08:59,240 --> 00:09:02,980
概率而言，比特数的信息公式是什么？

159
00:09:03,920 --> 00:09:07,761
我们在这里所说的是，当你取位数的二分之一 

160
00:09:07,761 --> 00:09:13,066
时，这与概率是一样的，这与说 2 的位数 次方等于概率的 

161
00:09:13,066 --> 00:09:18,005
1 是一样的，重新排列进 一步表示该信息是一的对数基数

162
00:09:18,005 --> 00:09:18,920
除以概率。

163
00:09:19,620 --> 00:09:22,385
有时您还会看到这种情况，还需要进行一次重新 

164
00:09:22,385 --> 00:09:24,900
排列，其中信息是以概率的负对数为底的二。

165
00:09:25,660 --> 00:09:28,574
对于外行来说，这样表达可能有点奇怪 

166
00:09:28,574 --> 00:09:31,489
，但这确实是一个非常直观的想法，即 

167
00:09:31,489 --> 00:09:34,080
询问您已将可能性减少一半的次数。

168
00:09:35,180 --> 00:09:37,287
现在，如果您想知道，您知道，我以为我们只是 

169
00:09:37,287 --> 00:09:39,300
在玩一个有趣的文字游戏，为什么要使用对数？

170
00:09:39,780 --> 00:09:43,991
这是一个更好的单元的原因之一是，谈论非常不可能 

171
00:09:43,991 --> 00:09:48,202
的事件要容易得多，说一个观察有 20 位信息比 

172
00:09:48,202 --> 00:09:52,940
说这样那样发生的概率为 0 容易得多。0000095。

173
00:09:53,300 --> 00:09:57,479
但这种对数表达式被证明是对概率论非常有用 

174
00:09:57,479 --> 00:10:01,460
的补充，更实质性的原因是信息相加的方式。

175
00:10:02,060 --> 00:10:05,871
例如，如果一个观察结果为您提供了两位信息，将您的空间 

176
00:10:05,871 --> 00:10:09,541
缩小了四分之二，然后第二个观察结果（如您在 Wor 

177
00:10:09,541 --> 00:10:13,211
dle 中的第二次猜测）为您提供了另外三位信息，将 

178
00:10:13,211 --> 00:10:16,740
您的空间进一步缩小了八倍，则两个一起给你五位信息。

179
00:10:17,160 --> 00:10:21,020
就像概率喜欢乘法一样，信息喜欢增加。

180
00:10:21,960 --> 00:10:25,043
因此，一旦我们处于预期值之类的领域，我们 

181
00:10:25,043 --> 00:10:27,980
将一堆数字相加，日志就会使其更容易处理。

182
00:10:28,480 --> 00:10:31,778
让我们回到 Weary 的发行版，并在此处添加 

183
00:10:31,778 --> 00:10:34,940
另一个小跟踪器，向我们展示每种模式有多少信息。

184
00:10:35,580 --> 00:10:39,256
我希望您注意的主要事情是，我们获得这些更有可能 

185
00:10:39,256 --> 00:10:42,780
的模式的概率越高，信息越少，您获得的位就越少。

186
00:10:43,500 --> 00:10:47,074
我们衡量这种猜测质量的方法是获取该信息的期 

187
00:10:47,074 --> 00:10:50,648
望值，我们遍历每个模式，我们说它的可能性有 

188
00:10:50,648 --> 00:10:54,060
多大，然后我们将其乘以我们获得的信息位数。

189
00:10:54,710 --> 00:10:58,120
在 Weary 的例子中，结果是 4。9 位。

190
00:10:58,560 --> 00:11:02,096
因此，平均而言，您从这个开局猜测中获得的信息 

191
00:11:02,096 --> 00:11:05,480
相当于将您的可能性空间切成两半（大约五倍）。

192
00:11:05,960 --> 00:11:08,891
相比之下，具有较高预期信息值的 

193
00:11:08,891 --> 00:11:11,640
猜测的示例类似于 Slate。

194
00:11:13,120 --> 00:11:15,620
在这种情况下，您会注意到分布看起来更加平坦。

195
00:11:15,940 --> 00:11:20,388
特别是，所有灰色中最有可能出现的概率只有 

196
00:11:20,388 --> 00:11:25,260
6% 左右，因此至少明显会得到 3。9位信息。

197
00:11:25,920 --> 00:11:28,560
但这是最低限度，更常见的是你会得到比这更好的东西。

198
00:11:29,100 --> 00:11:32,412
事实证明，当你计算这个数字并将所有相 

199
00:11:32,412 --> 00:11:35,900
关术语加起来时，平均信息约为 5。8. 

200
00:11:37,360 --> 00:11:40,590
因此，与《Weary》相比，平均而言，在第一 

201
00:11:40,590 --> 00:11:43,540
次猜测之后，你的可能性空间大约只有一半大。

202
00:11:44,420 --> 00:11:49,120
关于信息量期望值的名称，实际上有一个有趣的故事。

203
00:11:49,200 --> 00:11:52,072
信息论是由 20 世纪 40 年代在贝尔实验室工作的克劳

204
00:11:52,072 --> 00:11:54,431
德·香农 (C laude Shannon) 

205
00:11:54,431 --> 00:11:57,405
提出的，但他正在与约翰·冯·诺依曼 ( John von 

206
00:11:57,405 --> 00:12:00,072
Neumann) 谈论他尚未发表的一些想法，约翰· 

207
00:12:00,072 --> 00:12:01,918
冯·诺依曼是当时非常杰出的知识巨人。

208
00:12:01,918 --> 00:12:03,560
数学和物理以及计算机科学的开端。

209
00:12:04,100 --> 00:12:07,526
当冯·诺依曼提到他对于信息量的期望值 

210
00:12:07,526 --> 00:12:10,953
并没有一个好名字时，据说，所以故事是 

211
00:12:10,953 --> 00:12:14,200
这样的，你应该称之为熵，有两个原因。

212
00:12:14,540 --> 00:12:18,666
首先，你的不确定性函数已经在统计力学中以这个名字使 

213
00:12:18,666 --> 00:12:22,792
用了，所以它已经有一个名字了，其次，更重要的是，没 

214
00:12:22,792 --> 00:12:26,760
有人知道熵到底是什么，所以在辩论中你总是会有优势。

215
00:12:27,700 --> 00:12:30,141
因此，如果这个名字看起来有点神秘，并且 

216
00:12:30,141 --> 00:12:32,460
如果这个故事可信的话，那就是有意为之。

217
00:12:33,280 --> 00:12:36,669
另外，如果您想知道它与物理学中所有热力学 

218
00:12:36,669 --> 00:12:39,896
第二定律的关系，那么肯定存在某种联系， 

219
00:12:39,896 --> 00:12:43,285
但在其起源中，香农只是处理纯概率论，并且 

220
00:12:43,285 --> 00:12:46,513
出于我们的目的，当我使用熵这个词，我只 

221
00:12:46,513 --> 00:12:49,580
是想让你思考一个特定猜测的预期信息值。

222
00:12:50,700 --> 00:12:53,780
您可以将熵视为同时测量两个事物。

223
00:12:54,240 --> 00:12:56,780
第一个是分布的平坦程度。

224
00:12:57,320 --> 00:13:01,120
分布越接近均匀，熵就越高。

225
00:13:01,580 --> 00:13:04,832
在我们的例子中，总共有 3 到 5 

226
00:13:04,832 --> 00:13:09,891
个模式，对于均匀分布，观察其 中任何一个模式都会有 3 

227
00:13:09,891 --> 00:13:13,686
到 5 个的信息对数基数 2，恰好是 7。

228
00:13:13,686 --> 00:13:17,300
92，所以这是该熵可能具有的绝对最大值。

229
00:13:17,840 --> 00:13:22,080
但熵首先也是一种衡 量可能性的方法。

230
00:13:22,320 --> 00:13:25,391
例如，如果您碰巧有某个单词，其中只有 

231
00:13:25,391 --> 00:13:28,623
16 种可能的模式，并 且每种模式的可能

232
00:13:28,623 --> 00:13:32,180
性相同，则该熵（即该预期信息）将是 4 位。

233
00:13:32,579 --> 00:13:36,610
但如果你有另一个词，其中可能出现 64 种可能的 

234
00:13:36,610 --> 00:13:40,480
模式，并且它们的可能性相同，那么熵将是 6 位。

235
00:13:41,500 --> 00:13:45,609
因此，如果您在野外看到某个分布的熵为 6 位，这 

236
00:13:45,609 --> 00:13:49,719
就有点像是在说即将发生的事情存在同样多的变化和不 

237
00:13:49,719 --> 00:13:53,500
确定性，就好像有 64 个同样可能的结果一样。

238
00:13:54,360 --> 00:13:56,840
对于我第一次使用 Wurtele

239
00:13:56,840 --> 00:13:59,320
bot，我基本上就是让它这样做。

240
00:13:59,320 --> 00:14:03,649
它会遍历所有可能的猜测，即所有 13,000 个单 

241
00:14:03,649 --> 00:14:07,813
词，计算每个单词的熵，或者更具体地说，计算您可能 

242
00:14:07,813 --> 00:14:12,143
看到的所有模式中每个单词的分布熵，并选择最高的，因 

243
00:14:12,143 --> 00:14:16,140
为这是一个可能会尽可能地削减你的可能性空间的人。

244
00:14:17,140 --> 00:14:19,170
尽管我在这里只讨论了第一个猜测，但它对 

245
00:14:19,170 --> 00:14:21,100
于接下来的几次猜测也起到了同样的作用。

246
00:14:21,560 --> 00:14:25,023
例如，在您看到第一个猜测的某些模式后，这将根 

247
00:14:25,023 --> 00:14:28,487
据与之匹配的内容将您限制为较少数量的可能单词 

248
00:14:28,487 --> 00:14:31,800
，您只需针对该较小的单词集玩相同的游戏即可。

249
00:14:32,260 --> 00:14:36,174
对于建议的第二个猜测，您会查看从一组更受限制的 

250
00:14:36,174 --> 00:14:40,088
单词中可能出现的所有模式的分布，搜索所有 13 

251
00:14:40,088 --> 00:14:43,840
,000 种可能性，然后找到使熵最大化的一种。

252
00:14:45,420 --> 00:14:49,899
为了向您展示这是如何实际工作的，让我拿出我编写的 Wurt 

253
00:14:49,899 --> 00:14:54,080
ele 的一个小变体，它在页边空白处显示了此分析的亮点。

254
00:14:54,080 --> 00:14:59,660
完成所有熵计算后，右侧向我们展 示了哪些具有最高的预期信息。

255
00:15:00,280 --> 00:15:05,620
事实证明，至少目前最重要的答案是稗子，我们稍后会对此进 

256
00:15:05,620 --> 00:15:10,580
行完善，这意味着，嗯，当然是野豌豆，最常见的野豌豆。

257
00:15:11,040 --> 00:15:14,547
每次我们在这里进行猜测时，也许我会忽略它的建议并选择 

258
00:15:14,547 --> 00:15:17,794
slate，因为我喜欢 slate，我们可以看到 

259
00:15:17,794 --> 00:15:21,172
它有多少预期信息，但在这个词的右侧，它向我们展示了 

260
00:15:21,172 --> 00:15:24,420
多少信息考虑到这种特定的模式，我们得到的实际信息。

261
00:15:25,000 --> 00:15:27,976
所以看起来我们有点不走运，我们预计会得到 5 个。

262
00:15:27,976 --> 00:15:30,120
8，但 我们碰巧得到的东西比这个少。

263
00:15:30,600 --> 00:15:32,881
然后在左侧，它向我们展示了当前 

264
00:15:32,881 --> 00:15:35,020
所处位置的所有不同可能的单词。

265
00:15:35,800 --> 00:15:39,710
蓝色条告诉我们它认为每个单词出现的可能性有多大，因此目前它 

266
00:15:39,710 --> 00:15:43,360
假设每个单词出现的可能性相同，但我们稍后会对其进行改进。

267
00:15:44,060 --> 00:15:48,225
然后，这种不确定性测量告诉我们可能单词的 

268
00:15:48,225 --> 00:15:52,191
分布熵，因为它是均匀分布，所以现在只是 

269
00:15:52,191 --> 00:15:55,960
一种计算可能性数量的不必要的复杂方法。

270
00:15:56,560 --> 00:15:59,242
例如，如果我们要计算 2 的 13 次方。

271
00:15:59,242 --> 00:16:02,180
66，这应该是 大约 13,000 种可能性。

272
00:16:02,900 --> 00:16:06,140
我在这里有点偏离，但这只是因为我没有显示所有小数位。

273
00:16:06,720 --> 00:16:09,647
目前，这可能感觉多余，而且好像事情过于复杂，但您 

274
00:16:09,647 --> 00:16:12,340
很快就会明白为什么同时拥有这两个数字是有用的。

275
00:16:12,760 --> 00:16:16,264
所以这里看起来它表明我们第二个猜测的 

276
00:16:16,264 --> 00:16:19,400
最高熵是拉面，这又感觉不像一个词。

277
00:16:19,980 --> 00:16:24,060
因此，为了占据道德制高点，我将继续输入 Rains。

278
00:16:25,440 --> 00:16:27,340
看来我们又有点不走运了。

279
00:16:27,520 --> 00:16:31,360
我们本来期待4。3 位，但我们只得到了 3 位。39位信息。

280
00:16:31,940 --> 00:16:33,940
这样一来，我们就有 55 种可能性。

281
00:16:34,900 --> 00:16:37,311
在这里，也许我实际上会遵循它的建 

282
00:16:37,311 --> 00:16:39,440
议，即组合，无论这意味着什么。

283
00:16:40,040 --> 00:16:42,920
好吧，这实际上是一个解谜的好机会。

284
00:16:42,920 --> 00:16:46,380
它告诉我们这个模式给了我们 4。7 位信息。

285
00:16:47,060 --> 00:16:50,367
但在左边，在我们看到该模式之前，有 5 个。

286
00:16:50,367 --> 00:16:51,720
78 位不确定性。

287
00:16:52,420 --> 00:16:56,340
那么作为对你的一个测验，剩余可能性的数量意味着什么？

288
00:16:58,040 --> 00:17:01,377
嗯，这意味着我们将不确定性减少到一点 

289
00:17:01,377 --> 00:17:04,540
点，这与说有两个可能的答案是一样的。

290
00:17:04,700 --> 00:17:05,700
这是50-50的选择。

291
00:17:06,500 --> 00:17:08,629
从这里开始，因为你和我知道哪些词更 

292
00:17:08,629 --> 00:17:10,640
常见，所以我们知道答案应该是深渊。

293
00:17:11,180 --> 00:17:13,280
但正如现在所写的，程序并不知道这一点。

294
00:17:13,540 --> 00:17:16,850
所以它会继续前进，尝试获取尽可能多的信息， 

295
00:17:16,850 --> 00:17:19,859
直到只剩下一种可能性，然后它就会猜测它。

296
00:17:20,380 --> 00:17:22,339
显然我们需要更好的残局策略。

297
00:17:22,599 --> 00:17:25,538
但是，假设我们将此版本称为我们的 wordle 求解 

298
00:17:25,538 --> 00:17:28,260
器之一，然后我们运行一些模拟来看看它是如何工作的。

299
00:17:30,360 --> 00:17:34,120
所以它的工作方式是玩所有可能的文字游戏。

300
00:17:34,240 --> 00:17:38,540
它会检查所有 2315 个单词，这些单词是实际的单词答案。

301
00:17:38,540 --> 00:17:40,580
它基本上使用它作为测试集。

302
00:17:41,360 --> 00:17:45,676
采用这种天真的方法，不考虑一个词的常见程度，只是 

303
00:17:45,676 --> 00:17:49,820
试图在每一步中最大化信息，直到它只剩下一个选择。

304
00:17:50,360 --> 00:17:54,300
模拟结束时，平均得分约为 4。124. 

305
00:17:55,319 --> 00:17:59,240
老实说，这还不错，我本来以为会做得更糟。

306
00:17:59,660 --> 00:18:02,600
但玩wordle的人会告诉你，他们通常可以在4内得到它。

307
00:18:02,860 --> 00:18:05,380
真正的挑战是尽可能多地获得三分。

308
00:18:05,380 --> 00:18:08,080
4分和3分之间的差距相当大。

309
00:18:08,860 --> 00:18:12,053
这里显而易见的容易实现的目标是以某种方式纳入一 

310
00:18:12,053 --> 00:18:14,980
个单词是否常见，以及我们到底如何做到这一点。

311
00:18:22,800 --> 00:18:27,880
我的方法是获取英语中所 有单词的相对频率列表。

312
00:18:28,220 --> 00:18:29,996
我刚刚使用了 Mathematica 

313
00:18:29,996 --> 00:18:32,054
的词频数据函数，它本身是从 Go ogle 

314
00:18:32,054 --> 00:18:34,860
Books English Ngram 公共数据集中提取的。

315
00:18:35,460 --> 00:18:37,770
看起来很有趣，例如，如果我们将其从最 

316
00:18:37,770 --> 00:18:39,960
常见的单词到最不常见的单词进行排序。

317
00:18:40,120 --> 00:18:43,080
显然，这些是英语中最常见的 5 个字母单词。

318
00:18:43,700 --> 00:18:45,840
或者更确切地说，这些是第八个最常见的。

319
00:18:46,280 --> 00:18:48,880
首先是which，然后是there和there。

320
00:18:49,260 --> 00:18:52,486
First本身不是first，而是9th，并且这些其 

321
00:18:52,486 --> 00:18:55,592
他词可能更频繁地出现是有道理的，其中first之后 

322
00:18:55,592 --> 00:18:58,580
的词是after、where，而那些词则不太常见。

323
00:18:59,160 --> 00:19:03,099
现在，在使用这些数据来模拟每个单词成为最终 

324
00:19:03,099 --> 00:19:06,860
答案的可能性时，它不应该仅仅与频率成正比。

325
00:19:06,860 --> 00:19:10,879
例如，得分为 0。002 在此数据集中，而“br 

326
00:19:10,879 --> 00:19:15,060
aid”一词在某种意义上的可能性要小 1000 倍。

327
00:19:15,560 --> 00:19:18,840
但这两个词都很常见，几乎肯定值得考虑。

328
00:19:19,340 --> 00:19:21,000
所以我们想要更多的二元截止。

329
00:19:21,860 --> 00:19:26,117
我的方法是想象一下将整个排序的单词列表，然后将其排列 

330
00:19:26,117 --> 00:19:30,217
在 x 轴上，然后应用 sigmoid 函数，这是 

331
00:19:30,217 --> 00:19:34,317
输出基本上是二进制的函数的标准方法，它是要么是 0 

332
00:19:34,317 --> 00:19:38,260
，要么是 1，但对于该不确定区域，中间有一个平滑。

333
00:19:39,160 --> 00:19:43,965
所以本质上，我分配给每个单词出现在最终列表中的概率将是上 

334
00:19:43,965 --> 00:19:48,440
面的 sigmoid 函数的值，无论它位于 x 轴上。

335
00:19:49,520 --> 00:19:54,262
显然，这取决于几个参数，例如，这些单词在 x 轴上填充 

336
00:19:54,262 --> 00:19:58,836
的空间有多宽，决定了我们从 1 到 0 的逐渐或陡峭 

337
00:19:58,836 --> 00:20:03,240
程度，以及我们将它们从左到右放置的位置决定了截止值。

338
00:20:03,240 --> 00:20:06,920
说实话，我的做法就是舔手指然后把它插到风里。

339
00:20:07,140 --> 00:20:10,682
我查看了排序后的列表，并试图找到一个窗口 

340
00:20:10,682 --> 00:20:14,055
，当我查看它时，我认为这些单词中的一半 

341
00:20:14,055 --> 00:20:17,260
更有可能是最终答案，并将其用作截止值。

342
00:20:17,260 --> 00:20:20,703
一旦我们在单词之间有了这样的分布，它就会给我们 

343
00:20:20,703 --> 00:20:23,860
带来另一种情况，即熵成为这种真正有用的度量。

344
00:20:24,500 --> 00:20:27,462
例如，假设我们正在玩一个游戏，我们从我 

345
00:20:27,462 --> 00:20:30,425
的旧开场白开始，即羽毛和指甲，我们最终 

346
00:20:30,425 --> 00:20:33,240
会遇到有四个可能的单词与之匹配的情况。

347
00:20:33,560 --> 00:20:35,620
假设我们认为它们都有相同的可能性。

348
00:20:36,220 --> 00:20:38,880
我问你，这个分布的熵是多少？

349
00:20:41,080 --> 00:20:45,394
嗯，与这些可能性中的每一种相关的信息将是 4 的以 

350
00:20:45,394 --> 00:20:50,040
2 为底的对数，因为每一种都是 1 和 4，那就是 2。

351
00:20:50,040 --> 00:20:52,460
两位信息，四种可能性。

352
00:20:52,760 --> 00:20:53,580
一切都很好。

353
00:20:54,300 --> 00:20:57,800
但如果我告诉你实际上有超过四场比赛呢？

354
00:20:58,260 --> 00:21:00,969
事实上，当我们查看完整的单词列表时，有 

355
00:21:00,969 --> 00:21:02,460
16 个单词与其匹配。

356
00:21:02,580 --> 00:21:06,827
但假设我们的模型对其他 12 个单词实际成为最终答案 

357
00:21:06,827 --> 00:21:10,760
的概率非常低，大约是千分之一，因为它们真的很晦涩。

358
00:21:11,500 --> 00:21:14,260
现在我问你，这个分布的熵是多少？

359
00:21:15,420 --> 00:21:18,953
如果熵在这里纯粹测量匹配的数量，那么您可能 

360
00:21:18,953 --> 00:21:22,487
会期望它类似于 16 的以 2 为底的对数 

361
00:21:22,487 --> 00:21:25,700
，即 4，比我们之前的不确定性多了两位。

362
00:21:26,180 --> 00:21:29,860
但当然，实际的不确定性与我们之前的情况并没有太大不同。

363
00:21:30,160 --> 00:21:33,910
例如，仅仅因为有这 12 个非常晦涩的单词并不意 

364
00:21:33,910 --> 00:21:37,360
味着当得知最终答案是“魅力”时会更加令人惊讶。

365
00:21:38,180 --> 00:21:41,791
所以当你在这里实际进行计算时，将每次出现的概 

366
00:21:41,791 --> 00:21:45,560
率乘以相应的信息相加，得到的就是 2。11 位。

367
00:21:45,560 --> 00:21:49,346
我只是说，它基本上是两位，基本上是这四种可能性，但是 

368
00:21:49,346 --> 00:21:52,993
由于所有这些极不可能发生的事件，存在更多的不确定性 

369
00:21:52,993 --> 00:21:56,500
，尽管如果你确实了解了它们，你会从中获得大量信息。

370
00:21:57,160 --> 00:21:59,391
缩小范围，这就是 Wordle 成为信 

371
00:21:59,391 --> 00:22:01,400
息论课程的一个很好的例子的部分原因。

372
00:22:01,600 --> 00:22:04,640
我们对熵有两种不同的感觉应用。

373
00:22:05,160 --> 00:22:08,711
第一个告诉我们从给定的猜测中得到的预期 

374
00:22:08,711 --> 00:22:12,263
信息是什么，第二个告诉我们我们是否可以 

375
00:22:12,263 --> 00:22:15,460
衡量所有可能的单词中剩余的不确定性。

376
00:22:16,460 --> 00:22:20,579
我应该强调，在第一种情况下，我们正在查看猜测的预期 

377
00:22:20,579 --> 00:22:24,540
信息，一旦我们对单词的权重不相等，就会影响熵计算。

378
00:22:24,980 --> 00:22:28,634
例如，让我拿出我们之前查看的与 We ary 

379
00:22:28,634 --> 00:22:32,925
相关的分布的相同案例，但这次 在所有可能的单词中使用非

380
00:22:32,925 --> 00:22:33,720
均匀分布。

381
00:22:34,500 --> 00:22:38,280
所以让我看看是否可以在这里找到一个很好地说明它的部分。

382
00:22:40,940 --> 00:22:42,360
好吧，这里这很好。

383
00:22:42,360 --> 00:22:45,793
这里我们有两个相邻的模式，它们的可能性大致相同，但我 

384
00:22:45,793 --> 00:22:49,100
们被告知其中一个模式有 32 个可能的单词与其匹配。

385
00:22:49,280 --> 00:22:52,583
如果我们检查它们是什么，那就是 32 个，当 

386
00:22:52,583 --> 00:22:55,600
你扫视它们时，它们都只是非常不可能的单词。

387
00:22:55,840 --> 00:22:59,375
很难找到任何看似合理的答案，也许会大喊大叫， 

388
00:22:59,375 --> 00:23:02,756
但如果我们查看分布中的相邻模式，这被认为是 

389
00:23:02,756 --> 00:23:06,292
同样可能的，我们被告知它只有 8 个可能的匹 

390
00:23:06,292 --> 00:23:09,520
配，所以四分之一很多比赛，但可能性差不多。

391
00:23:09,860 --> 00:23:12,140
当我们拿出这些匹配项时，我们就能明白原因了。

392
00:23:12,500 --> 00:23:16,300
其中一些是实际合理的答案，例如戒指、愤怒或说唱。

393
00:23:17,900 --> 00:23:20,637
为了说明我们如何整合所有这些，让我在这里列出 

394
00:23:20,637 --> 00:23:22,542
Wordlebo t 的第 2 

395
00:23:22,542 --> 00:23:25,280
版，它与我们看到的第一个版本有两三个主要区别。

396
00:23:25,860 --> 00:23:30,109
首先，就像我刚才说的，我们计算这些熵、这些信 

397
00:23:30,109 --> 00:23:34,359
息的预期值的方式现在正在使用跨模式的更精细的 

398
00:23:34,359 --> 00:23:38,240
分布，其中包含给定单词实际上是答案的概率。

399
00:23:38,879 --> 00:23:43,820
碰巧，眼泪仍然是第一位，尽管接下来的有点不同。

400
00:23:44,360 --> 00:23:47,981
其次，当它对首选进行排名时，它现在将保留每个单词 

401
00:23:47,981 --> 00:23:51,603
是实际答案的概率模型，并将其纳入其决策中，一旦我 

402
00:23:51,603 --> 00:23:55,080
们对答案有了一些猜测，就更容易看到这一点。桌子。

403
00:23:55,860 --> 00:23:59,780
再次忽略它的建议，因为我们不能让机器统治我们的生活。

404
00:24:01,140 --> 00:24:05,574
我想我应该提到另一件不同的事情是在左边，不确定 

405
00:24:05,574 --> 00:24:09,640
性值，即位数，不再只是与可能匹配的数量冗余。

406
00:24:10,080 --> 00:24:13,860
现在，如果我们将其拉高并计算 2 的 8。

407
00:24:13,860 --> 00:24:16,739
02，略高于 256，我 猜是 

408
00:24:16,739 --> 00:24:20,340
259，它的意思是，尽管总共有 526 

409
00:24:20,340 --> 00:24:24,840
个单词实际上与此模式匹配，但它所具有的不确定性量 

410
00:24:24,840 --> 00:24:28,980
更类似于如果有 259 个同样可能的单词结果。

411
00:24:29,720 --> 00:24:30,740
你可以这样想。

412
00:24:31,020 --> 00:24:34,174
它知道 borx 不是答案，与 yorts、zorl 

413
00:24:34,174 --> 00:24:37,680
和 zorus 一样，因此它的不确定性比之前的情况要小一些。

414
00:24:37,820 --> 00:24:39,280
这个位数会更小。

415
00:24:40,220 --> 00:24:43,457
如果我继续玩这个游戏，我会用一些与我想在 

416
00:24:43,457 --> 00:24:46,540
这里解释的内容相符的猜测来完善这个游戏。

417
00:24:48,360 --> 00:24:51,218
通过第四种猜测，如果你看一下它的首 

418
00:24:51,218 --> 00:24:53,760
选，你会发现它不再只是最大化熵。

419
00:24:54,460 --> 00:24:57,463
所以在这一点上，技术上有七种可能性 

420
00:24:57,463 --> 00:25:00,300
，但唯一有意义的机会是宿舍和单词。

421
00:25:00,300 --> 00:25:03,592
您可以看到它对选择这两个值的排名高于所 

422
00:25:03,592 --> 00:25:06,720
有其他值，严格来说，这会提供更多信息。

423
00:25:07,240 --> 00:25:10,721
我第一次这样做时，我只是将这两个数字相加来衡 

424
00:25:10,721 --> 00:25:13,900
量每个猜测的质量，这实际上比你想象的要好。

425
00:25:14,300 --> 00:25:16,888
但它确实感觉不系统，而且我确信人们可 

426
00:25:16,888 --> 00:25:19,340
以采取其他方法，但这是我找到的方法。

427
00:25:19,760 --> 00:25:23,975
如果我们正在考虑下一次猜测的前景，就像在这种情况下的话， 

428
00:25:23,975 --> 00:25:27,900
我们真正关心的是如果我们这样做的话我们游戏的预期得分。

429
00:25:28,230 --> 00:25:32,163
为了计算预期分数，我们会计算单词是实际 

430
00:25:32,163 --> 00:25:35,900
答案的概率是多少，目前描述为 58%。

431
00:25:36,040 --> 00:25:39,540
我们说，有 58% 的机会，我们在这场比赛中得分为 4。

432
00:25:40,320 --> 00:25:45,640
然后，以 1 减去 58% 的概率，我们的分数将大于 4。

433
00:25:46,220 --> 00:25:49,424
我们不知道还有多少，但我们可以根据到 

434
00:25:49,424 --> 00:25:52,460
达这一点后可能存在的不确定性来估计。

435
00:25:52,960 --> 00:25:55,940
具体来说，目前有1。44 位不确定性。

436
00:25:56,440 --> 00:26:00,365
如果我们猜测单词，它会告诉我们预期得到的信息是 1。

437
00:26:00,365 --> 00:26:01,120
27 位。

438
00:26:01,620 --> 00:26:04,783
因此，如果我们猜测单词，这种差异代表了在这 

439
00:26:04,783 --> 00:26:07,660
种情况发生后我们可能会留下多少不确定性。

440
00:26:08,260 --> 00:26:10,862
我们需要的是某种函数，我在这里称之为 

441
00:26:10,862 --> 00:26:13,740
f ，它将这种不确定性与预期分数联系起来。

442
00:26:14,240 --> 00:26:18,328
它的处理方式是根据机器人的版本 1 绘制之 

443
00:26:18,328 --> 00:26:22,417
前游戏的一堆数据，以说明在具有某些非常可测 

444
00:26:22,417 --> 00:26:26,320
量的不确定性的各个点之后的实际得分是多少。

445
00:26:27,020 --> 00:26:31,180
例如，这里的这些数据点位于 8 左右的值之上。

446
00:26:31,180 --> 00:26:34,980
在 8分之后的一些比赛中，大约有7分左右。

447
00:26:34,980 --> 00:26:38,960
7位不确定 性，经过两次猜测才得到最终答案。

448
00:26:39,320 --> 00:26:42,240
对于其他游戏需要猜测三次，对于其他游戏需要猜测四次。

449
00:26:43,140 --> 00:26:46,898
如果我们在这里向左移动，所有超过零的点都表明， 

450
00:26:46,898 --> 00:26:50,657
每当不确定性为零时，也就是说只有一种可能性，那 

451
00:26:50,657 --> 00:26:54,260
么所需的猜测次数总是只有一次，这是令人放心的。

452
00:26:54,780 --> 00:26:57,679
每当有一点不确定性时，意味着基本上只 

453
00:26:57,679 --> 00:27:00,425
有两种可能性，那么有时需要再进行一 

454
00:27:00,425 --> 00:27:03,020
次猜测，有时则需要再进行两次猜测。

455
00:27:03,080 --> 00:27:05,240
这里等等等等。

456
00:27:05,740 --> 00:27:07,980
也许可视化这些数据的一种稍微简单

457
00:27:07,980 --> 00:27:10,220
的方法是将其放在一起并取平均值。

458
00:27:11,000 --> 00:27:15,384
例如，这里的这个条表示，在我们有一点不确定性 

459
00:27:15,384 --> 00:27:19,960
的所有点中，平均所需的新猜测数量约为 1。5. 

460
00:27:22,140 --> 00:27:25,578
这里的栏表示在所有不同的游戏中，在某些 

461
00:27:25,578 --> 00:27:29,705
时候不确定性略高于 4 位，这就像将 其缩小到 

462
00:27:29,705 --> 00:27:34,004
16 种不同的可能性，然后从 该点开始平均需要两个

463
00:27:34,004 --> 00:27:35,380
以上的猜测向前。

464
00:27:36,060 --> 00:27:39,460
从这里开始，我只是做了一个回归来拟合一个看起来合理的函数。

465
00:27:39,980 --> 00:27:44,649
请记住，这样做的全部目的是为了我们可以量化这种直觉 

466
00:27:44,649 --> 00:27:48,960
，即我们从单词中获得的信息越多，预期得分就越低。

467
00:27:49,680 --> 00:27:54,460
所以将此作为版本 2。0，如果我们返回并运行相同的一组模拟，

468
00:27:54,460 --> 00:27:59,240
让它 与所有 2315 个可能的单词答案进行比较，结果如何？

469
00:28:00,280 --> 00:28:03,420
与我们的第一个版本相比，它肯定更好，这令人放心。

470
00:28:04,020 --> 00:28:08,394
综上所述，平均值约为 3。6，尽管与第一个版本不同， 

471
00:28:08,394 --> 00:28:12,120
它有几次丢失并且在这种情况下需要超过 6 个。

472
00:28:12,639 --> 00:28:15,375
大概是因为有时需要进行权衡以实 

473
00:28:15,375 --> 00:28:17,940
际实现目标，而不是最大化信息。

474
00:28:19,040 --> 00:28:21,000
那么我们可以做得比 3 更好吗？6？

475
00:28:22,080 --> 00:28:22,920
我们绝对可以。

476
00:28:23,280 --> 00:28:26,464
现在我在一开始就说过，尝试不将单词答案的真 

477
00:28:26,464 --> 00:28:29,360
实列表合并到构建模型的方式中是最有趣的。

478
00:28:29,880 --> 00:28:33,625
但如果我们确实将其合并，我可以获得的最佳性能约为 3。

479
00:28:33,625 --> 00:28:34,180
43. 

480
00:28:35,160 --> 00:28:39,163
因此，如果我们尝试比仅使用词频数据更复杂地选择此先验分 

481
00:28:39,163 --> 00:28:42,594
布，则这 3.43 可能给出了我们可以做到什么 

482
00:28:42,594 --> 00:28:45,740
程度的最大值，或者至少是我可以做到什么程度。

483
00:28:46,240 --> 00:28:49,313
最佳性能本质上只是使用了我在这里讨 

484
00:28:49,313 --> 00:28:52,387
论的想法，但它更进一步，就像它向前 

485
00:28:52,387 --> 00:28:55,120
两步而不是一步搜索预期信息一样。

486
00:28:55,620 --> 00:28:57,982
本来我打算更多地讨论这个问题，但我意 

487
00:28:57,982 --> 00:29:00,220
识到我们实际上已经讨论了很长时间了。

488
00:29:00,580 --> 00:29:03,501
我要说的一件事是，在进行了两步搜索，然后在顶级 

489
00:29:03,501 --> 00:29:06,422
候选者中运行了几个样本模拟之后，至少到目前为止 

490
00:29:06,422 --> 00:29:09,100
对我来说，Crane 看起来是最好的开局者。

491
00:29:09,100 --> 00:29:10,060
谁能想到呢？

492
00:29:10,920 --> 00:29:14,443
此外，如果您使用真实的单词列表来确定您的可能性 

493
00:29:14,443 --> 00:29:17,820
空间，那么您开始的不确定性将略高于 11 位。

494
00:29:18,300 --> 00:29:22,192
事实证明，仅通过强力搜索，前两次猜测 

495
00:29:22,192 --> 00:29:25,880
后最大可能的预期信息约为 10 位。

496
00:29:26,500 --> 00:29:30,612
这表明，在最好的情况下，在您进行前两次猜测之后， 

497
00:29:30,612 --> 00:29:34,560
如果有完美的最佳玩法，您将留下大约一点不确定性。

498
00:29:34,800 --> 00:29:37,960
这与归结为两种可能的猜测相同。

499
00:29:37,960 --> 00:29:41,924
因此，我认为公平且可能相当保守地说，您永远不可能编 

500
00:29:41,924 --> 00:29:45,736
写一个使平均值低至 3 的算法，因为根据您可用的 

501
00:29:45,736 --> 00:29:49,853
单词，在仅执行两个步骤后根本没有空间获得足够的信息 。

502
00:29:49,853 --> 00:29:53,360
能够保证每次都在第三个槽中得到答案，不会失败。

