1
00:00:00,000 --> 00:00:03,407
Wurdle 游戏在过去一两个月里非常

2
00:00:03,407 --> 00:00:06,645
火爆，从来没有人会忽视数学课的机会，

3
00:00:06,645 --> 00:00:10,052
我觉得这个游戏是信息论课程中一个非常好

4
00:00:10,052 --> 00:00:13,120
的中心例子，特别是一个称为熵的主题。

5
00:00:13,120 --> 00:00:16,536
你看，就像很多人一样，我有点陷入了这个

6
00:00:16,536 --> 00:00:19,953
谜题，而且像很多程序员一样，我也陷入了

7
00:00:19,953 --> 00:00:23,200
尝试编写一种算法来尽可能最佳地玩游戏。

8
00:00:23,200 --> 00:00:26,334
我想我在这里要做的只是与你们讨论我

9
00:00:26,334 --> 00:00:29,294
的一些过程，并解释其中的一些数学

10
00:00:29,294 --> 00:00:32,080
，因为整个算法以熵的概念为中心。

11
00:00:32,080 --> 00:00:42,180
首先，如果您还没有听说过，那么什么是 Wurdle？

12
00:00:42,180 --> 00:00:45,308
在我们介绍游戏规则的同时，让我也

13
00:00:45,308 --> 00:00:48,436
预览一下我们要做什么，即开发一个

14
00:00:48,436 --> 00:00:51,380
基本上可以为我们玩游戏的小算法。

15
00:00:51,380 --> 00:00:53,669
虽然我还没有完成今天的 Wurdle，但现在

16
00:00:53,669 --> 00:00:55,860
是 2 月 4 日，我们将看看机器人的表现。

17
00:00:55,860 --> 00:00:58,491
Wurdle 的目标是猜测一个神秘的五

18
00:00:58,491 --> 00:01:00,860
个字母单词，您有六次不同的猜测机会。

19
00:01:00,860 --> 00:01:05,240
例如，我的 Wurdle 机器人建议我从猜测起重机开始。

20
00:01:05,240 --> 00:01:08,167
每次您进行猜测时，您都会获得一些有关

21
00:01:08,167 --> 00:01:10,940
您的猜测与真实答案的接近程度的信息。

22
00:01:10,940 --> 00:01:14,540
灰色框告诉我实际答案中没有 C。

23
00:01:14,540 --> 00:01:18,340
黄色框告诉我有一个 R，但它不在那个位置。

24
00:01:18,340 --> 00:01:22,820
绿色框告诉我秘密词确实有一个 A，而且位于第三个位置。

25
00:01:22,820 --> 00:01:24,300
然后就没有N了，也没有E了。

26
00:01:24,300 --> 00:01:27,420
那么让我进去告诉 Wurdle 机器人该信息。

27
00:01:27,420 --> 00:01:31,500
我们从起重机开始，有灰色、黄色、绿色、灰色、灰色。

28
00:01:31,500 --> 00:01:35,460
不要担心它现在显示的所有数据，我会在适当的时候解释这一点。

29
00:01:35,460 --> 00:01:39,700
但对于我们的第二个选择来说，它的首要建议是小技巧。

30
00:01:39,700 --> 00:01:42,825
你的猜测确实必须是一个实际的五个字母的单词，但正

31
00:01:42,825 --> 00:01:45,700
如你将看到的，它实际上让你猜测的内容相当自由。

32
00:01:45,700 --> 00:01:48,860
在这种情况下，我们尝试一下shtic。

33
00:01:48,860 --> 00:01:50,260
好吧，事情看起来相当不错。

34
00:01:50,260 --> 00:01:52,354
我们按了 S 和 H，所以我们知

35
00:01:52,354 --> 00:01:54,580
道前三个字母，我们知道有一个 R。

36
00:01:54,580 --> 00:01:59,740
所以它会像 SHA 某些 R 或 SHA R 某些东西。

37
00:01:59,740 --> 00:02:02,550
看起来 Wurdle 机器人知道它只有

38
00:02:02,550 --> 00:02:05,220
两种可能性，要么是碎片，要么是锋利的。

39
00:02:05,220 --> 00:02:08,360
在这一点上，他们之间存在着一种折腾，所以我想可能只

40
00:02:08,360 --> 00:02:11,260
是因为它是按字母顺序排列的，所以它与分片相匹配。

41
00:02:11,260 --> 00:02:13,000
万岁，这才是真正的答案。

42
00:02:13,000 --> 00:02:14,660
所以我们三分就搞定了。

43
00:02:14,660 --> 00:02:17,863
如果你想知道这是否有好处，我听到一个人的说法是，对

44
00:02:17,863 --> 00:02:20,820
于Wurdle来说，四杆是标准杆，三杆是小鸟球。

45
00:02:20,820 --> 00:02:22,960
我认为这是一个非常恰当的比喻。

46
00:02:22,960 --> 00:02:27,560
你必须坚持自己的比赛才能获得四分，但这当然并不疯狂。

47
00:02:27,560 --> 00:02:30,000
但当你拿到三分的时候，感觉棒极了。

48
00:02:30,000 --> 00:02:33,432
因此，如果您愿意的话，我在这里想做的就是从一开始就

49
00:02:33,432 --> 00:02:36,600
谈谈我如何处理 Wurdle 机器人的思考过程。

50
00:02:36,600 --> 00:02:39,800
就像我说的，这确实是信息论课程的借口。

51
00:02:39,800 --> 00:02:43,160
主要目标是解释什么是信息和什么是熵。

52
00:02:43,160 --> 00:02:48,517
在解决这个问题时，我的第一个想法

53
00:02:48,517 --> 00:02:53,560
是看看英语中不同字母的相对频率。

54
00:02:53,560 --> 00:02:56,928
所以我想，好吧，是否有一个开局猜测或一

55
00:02:56,928 --> 00:02:59,960
对开局猜测可以命中这些最常见的字母？

56
00:02:59,960 --> 00:03:03,780
我非常喜欢做其他事情，然后做指甲。

57
00:03:03,780 --> 00:03:05,985
我们的想法是，如果你击中一个字母，你知道

58
00:03:05,985 --> 00:03:07,980
，你会得到绿色或黄色，这总是感觉很好。

59
00:03:07,980 --> 00:03:09,460
感觉就像你正在获取信息。

60
00:03:09,460 --> 00:03:12,285
但在这些情况下，即使你没有击中并且总

61
00:03:12,285 --> 00:03:15,111
是得到灰色，这仍然为你提供了大量信息

62
00:03:15,111 --> 00:03:17,640
，因为很少找到没有这些字母的单词。

63
00:03:17,640 --> 00:03:20,664
但即便如此，这仍然感觉不是超级系统

64
00:03:20,664 --> 00:03:23,520
，因为例如，它没有考虑字母的顺序。

65
00:03:23,520 --> 00:03:26,080
当我可以输入蜗牛时，为什么还要输入指甲？

66
00:03:26,080 --> 00:03:27,720
最后加个S是不是更好？

67
00:03:27,720 --> 00:03:28,720
我不太确定。

68
00:03:28,720 --> 00:03:33,011
现在，我的一个朋友说他喜欢用“weary”这个词开头，这让

69
00:03:33,011 --> 00:03:37,160
我有点惊讶，因为里面有一些不常见的字母，比如 W 和 Y。

70
00:03:37,160 --> 00:03:39,400
但谁知道呢，也许这是一个更好的开局。

71
00:03:39,400 --> 00:03:44,920
我们是否可以给出某种定量评 分来判断潜在猜测的质量？

72
00:03:44,920 --> 00:03:48,443
现在，为了设置我们对可能的猜测进行排名的

73
00:03:48,443 --> 00:03:51,800
方式，让我们回顾一下游戏的具体设置方式。

74
00:03:51,800 --> 00:03:54,928
因此，您可以输入一个单词列表，这些单词被视为

75
00:03:54,928 --> 00:03:57,920
有效的猜测，长度约为 13,000 个单词。

76
00:03:57,920 --> 00:04:02,562
但当你仔细观察时，你会发现有很多非常不常见的东西，比如

77
00:04:02,562 --> 00:04:07,040
头像、阿里和ARG，以及拼字游戏中引发家庭争吵的词语。

78
00:04:07,040 --> 00:04:10,600
但游戏的氛围是答案总是一个相当常见的词。

79
00:04:10,600 --> 00:04:13,423
事实上，还有另一个大约 2300

80
00:04:13,423 --> 00:04:16,080
个单词的列表，它们是可能的答案。

81
00:04:16,080 --> 00:04:19,090
这是一个人工策划的列表，我认为是由游戏

82
00:04:19,090 --> 00:04:21,800
创建者的女朋友专门制作的，这很有趣。

83
00:04:21,800 --> 00:04:24,677
但我想做的是，我们对这个项目的挑战是看看

84
00:04:24,677 --> 00:04:27,698
我们是否可以编写一个解 决 Wordle

85
00:04:27,698 --> 00:04:30,720
的程序，该程序不包含有关此列表的先前知识。

86
00:04:30,720 --> 00:04:35,560
一方面，有很多非常常见的五个 字母单词您在该列表中找不到。

87
00:04:35,560 --> 00:04:38,831
因此，最好编写一个更具弹性的程序，并且可以与

88
00:04:38,831 --> 00:04:41,960
任何人玩 Wordle，而不仅仅是官方网站。

89
00:04:41,960 --> 00:04:44,788
我们之所以知道可能答案的列表是

90
00:04:44,788 --> 00:04:47,440
什么，是因为它在源代码中可见。

91
00:04:47,440 --> 00:04:52,840
但它在源代码中可见的方式是按 照每天出现答案的特定顺序。

92
00:04:52,840 --> 00:04:56,400
所以你总是可以查找明天的答案。

93
00:04:56,400 --> 00:04:59,140
很明显，使用该列表在某种程度上是作弊行为。

94
00:04:59,140 --> 00:05:03,538
使谜题更有趣、信息论课程更丰富的方法

95
00:05:03,538 --> 00:05:07,704
是使用一些更通用的数据，例如相对词

96
00:05:07,704 --> 00:05:11,640
频，来捕捉对更常见词的偏好的直觉。

97
00:05:11,640 --> 00:05:16,560
那么在这13000种可能性中，我们应该如何选择开局猜测呢？

98
00:05:16,560 --> 00:05:19,960
例如，如果我的朋友提出厌烦，我们应该如何分析它的质量？

99
00:05:19,960 --> 00:05:23,997
好吧，他说他喜欢那个不太可能的 W 的原因是他喜欢

100
00:05:23,997 --> 00:05:27,880
远射的本质，如果你击中了那个 W，那感觉是多么好。

101
00:05:27,880 --> 00:05:32,071
例如，如果第一个揭示的模式是这样的，那么这个

102
00:05:32,071 --> 00:05:36,080
庞大的词典中只有 58 个单词与该模式匹配。

103
00:05:36,080 --> 00:05:38,900
因此，与 13,000 人相比，这是一个巨大的减少。

104
00:05:38,900 --> 00:05:43,320
但当然，另一方面是，获得这样的模式非常罕见。

105
00:05:43,320 --> 00:05:47,667
具体来说，如果每个单词作为答案的可能性相同，则达到

106
00:05:47,667 --> 00:05:51,680
此模式的概率将为 58 除以大约 13,000。

107
00:05:51,680 --> 00:05:53,880
当然，它们成为答案的可能性并不相同。

108
00:05:53,880 --> 00:05:56,680
其中大部分都是非常晦涩甚至有问题的词语。

109
00:05:56,680 --> 00:05:59,487
但至少对于我们第一次通过这一切，我们假设它

110
00:05:59,487 --> 00:06:02,040
们的可能性相同，然后稍后再对其进行完善。

111
00:06:02,040 --> 00:06:07,360
关键是，包含大量信息的模式本质上不太可能发生。

112
00:06:07,360 --> 00:06:11,320
事实上，提供信息意味着这是不可能的。

113
00:06:11,320 --> 00:06:18,360
在这个开口中看到的更可能的模式 是这样的，当然其中没有 W。

114
00:06:18,360 --> 00:06:22,080
也许有E，也许没有A，没有R，没有Y。

115
00:06:22,080 --> 00:06:24,640
在本例中，有 1400 个可能的匹配项。

116
00:06:24,640 --> 00:06:30,680
如果所有可能性均等，则您看到 的模式的概率约为 11%。

117
00:06:30,680 --> 00:06:34,320
因此，最可能的结果也是信息最少的。

118
00:06:34,320 --> 00:06:38,258
为了获得更全面的视角，让我向您展示您可

119
00:06:38,258 --> 00:06:42,000
能看到的所有不同模式的概率的完整分布。

120
00:06:42,000 --> 00:06:45,767
因此，您看到的每个条形都对应于可能显示的颜

121
00:06:45,767 --> 00:06:49,535
色模式，其中有 3 到 5 种可能性，并且

122
00:06:49,535 --> 00:06:52,960
它们从左到右、最常见到最不常见进行组织。

123
00:06:52,960 --> 00:06:56,200
所以这里最常见的可能性是你得到的都是灰色的。

124
00:06:56,200 --> 00:06:58,800
这种情况发生的概率约为 14%。

125
00:06:58,800 --> 00:07:02,667
当你进行猜测时，你所希望的是你最终会出现在这条

126
00:07:02,667 --> 00:07:06,374
长尾中的某个地方，就像在这里，与这个显然看起

127
00:07:06,374 --> 00:07:09,920
来像这样的模式相匹配的可能性只有 18 种。

128
00:07:09,920 --> 00:07:11,932
或者，如果我们冒险向左走一点，

129
00:07:11,932 --> 00:07:14,080
你知道，也许我们会一直走到这里。

130
00:07:14,080 --> 00:07:16,560
好的，这是给你的一个很好的谜题。

131
00:07:16,560 --> 00:07:19,374
英语中以 W 开头、以 Y 结尾、其

132
00:07:19,374 --> 00:07:22,040
中某个位置有 R 的三个单词是什么？

133
00:07:22,040 --> 00:07:27,560
事实证明，让我们看看，答案是罗嗦、狡猾和讽刺的。

134
00:07:27,560 --> 00:07:31,817
因此，为了判断这个词的整体效果如何，我们需要某

135
00:07:31,817 --> 00:07:35,720
种方式来衡量您将从该分布中获得的预期信息量。

136
00:07:35,720 --> 00:07:41,074
如果我们检查每个模式，并将其发生的概率乘以衡量其

137
00:07:41,074 --> 00:07:46,000
信息量的因素，这也许可以给我们一个客观的分数。

138
00:07:46,000 --> 00:07:50,280
现在，您对某事物应该是什么的第一直觉可能是匹配的数量。

139
00:07:50,280 --> 00:07:52,960
您想要较低的平均匹配数。

140
00:07:52,960 --> 00:07:55,706
但相反，我想使用一种更通用的衡量标准，我们通

141
00:07:55,706 --> 00:07:58,078
常将其归因于信息 ，并且一旦我们为这

142
00:07:58,078 --> 00:08:00,699
13,000 个单词中的每一个分配不同的

143
00:08:00,699 --> 00:08:04,320
概率来判断它们是否实际上是答案，这种衡量标准就会更加灵活。

144
00:08:04,320 --> 00:08:11,242
信息的标准单位是位，它的公式有点有趣

145
00:08:11,242 --> 00:08:17,800
，但如果我们只看例子，它真的很直观。

146
00:08:17,800 --> 00:08:21,091
如果你的观察结果将你的可能性空间减

147
00:08:21,091 --> 00:08:24,200
少了一半，我们就说它只有一点信息。

148
00:08:24,200 --> 00:08:27,944
在我们的示例中，可能性空间是所有可能的单词，结果表明，五

149
00:08:27,944 --> 00:08:31,560
个字母的单词中大约一半有 S，比这个少一点，但大约一半。

150
00:08:31,560 --> 00:08:35,200
这样观察就会给你一点信息。

151
00:08:35,200 --> 00:08:38,800
相反，如果一个新事实将可能性空间减

152
00:08:38,800 --> 00:08:42,000
少了四倍，我们就说它有两位信息。

153
00:08:42,000 --> 00:08:45,120
例如，事实证明这些单词中大约四分之一有 T。

154
00:08:45,120 --> 00:08:48,201
如果观察将该空间缩小八分之一，我

155
00:08:48,201 --> 00:08:50,920
们就说它是三位信息，依此类推。

156
00:08:50,920 --> 00:08:55,000
四位将其切割为 16 度，五位将其切割为 32 度。

157
00:08:55,000 --> 00:09:00,024
所以现在您可能想停下来问自己，就发生

158
00:09:00,024 --> 00:09:04,520
概率而言，比特数的信息公式是什么？

159
00:09:04,520 --> 00:09:08,402
我们在这里所说的是，当你取位数的二分之一

160
00:09:08,402 --> 00:09:13,763
时，这与概率是一样的，这与说 2 的位数 次方等于概率的

161
00:09:13,763 --> 00:09:18,755
1 是一样的，重新排列进 一步表示该信息是一的对数基数

162
00:09:18,755 --> 00:09:19,680
除以概率。

163
00:09:19,680 --> 00:09:22,822
有时您还会看到这种情况，还需要进行一次重新

164
00:09:22,822 --> 00:09:25,680
排列，其中信息是以概率的负对数为底的二。

165
00:09:25,680 --> 00:09:28,947
对于外行来说，这样表达可能有点奇怪

166
00:09:28,947 --> 00:09:32,215
，但这确实是一个非常直观的想法，即

167
00:09:32,215 --> 00:09:35,120
询问您已将可能性减少一半的次数。

168
00:09:35,120 --> 00:09:37,575
现在，如果您想知道，您知道，我以为我们只是

169
00:09:37,575 --> 00:09:39,920
在玩一个有趣的文字游戏，为什么要使用对数？

170
00:09:39,920 --> 00:09:44,259
这是一个更好的单元的原因之一是，谈论非常不可能

171
00:09:44,259 --> 00:09:48,598
的事件要容易得多，说一个观察有 20 位信息比

172
00:09:48,598 --> 00:09:53,480
说这样那样发生的概率为 0 容易得多。0000095。

173
00:09:53,480 --> 00:09:57,843
但这种对数表达式被证明是对概率论非常有用

174
00:09:57,843 --> 00:10:02,000
的补充，更实质性的原因是信息相加的方式。

175
00:10:02,000 --> 00:10:05,987
例如，如果一个观察结果为您提供了两位信息，将您的空间

176
00:10:05,987 --> 00:10:09,827
缩小了四分之二，然后第二个观察结果（如您在 Wor

177
00:10:09,827 --> 00:10:13,667
dle 中的第二次猜测）为您提供了另外三位信息，将

178
00:10:13,667 --> 00:10:17,360
您的空间进一步缩小了八倍，则两个一起给你五位信息。

179
00:10:17,360 --> 00:10:21,200
就像概率喜欢乘法一样，信息喜欢增加。

180
00:10:21,200 --> 00:10:25,020
因此，一旦我们处于预期值之类的领域，我们

181
00:10:25,020 --> 00:10:28,660
将一堆数字相加，日志就会使其更容易处理。

182
00:10:28,660 --> 00:10:32,183
让我们回到 Weary 的发行版，并在此处添加

183
00:10:32,183 --> 00:10:35,560
另一个小跟踪器，向我们展示每种模式有多少信息。

184
00:10:35,560 --> 00:10:39,614
我希望您注意的主要事情是，我们获得这些更有可能

185
00:10:39,614 --> 00:10:43,500
的模式的概率越高，信息越少，您获得的位就越少。

186
00:10:43,500 --> 00:10:47,372
我们衡量这种猜测质量的方法是获取该信息的期

187
00:10:47,372 --> 00:10:51,244
望值，我们遍历每个模式，我们说它的可能性有

188
00:10:51,244 --> 00:10:54,940
多大，然后我们将其乘以我们获得的信息位数。

189
00:10:54,940 --> 00:10:58,480
在 Weary 的例子中，结果是 4。9 位。

190
00:10:58,480 --> 00:11:02,149
因此，平均而言，您从这个开局猜测中获得的信息

191
00:11:02,149 --> 00:11:05,660
相当于将您的可能性空间切成两半（大约五倍）。

192
00:11:05,660 --> 00:11:09,561
相比之下，具有较高预期信息值的

193
00:11:09,561 --> 00:11:13,220
猜测的示例类似于 Slate。

194
00:11:13,220 --> 00:11:16,180
在这种情况下，您会注意到分布看起来更加平坦。

195
00:11:16,180 --> 00:11:20,838
特别是，所有灰色中最有可能出现的概率只有

196
00:11:20,838 --> 00:11:25,940
6% 左右，因此至少明显会得到 3。9位信息。

197
00:11:25,940 --> 00:11:29,140
但这是最低限度，更常见的是你会得到比这更好的东西。

198
00:11:29,140 --> 00:11:32,686
事实证明，当你计算这个数字并将所有相

199
00:11:32,686 --> 00:11:36,420
关术语加起来时，平均信息约为 5。8.

200
00:11:36,420 --> 00:11:40,350
因此，与《Weary》相比，平均而言，在第一

201
00:11:40,350 --> 00:11:43,940
次猜测之后，你的可能性空间大约只有一半大。

202
00:11:43,940 --> 00:11:49,540
关于信息量期望值的名称，实际上有一个有趣的故事。

203
00:11:49,540 --> 00:11:52,468
信息论是由 20 世纪 40 年代在贝尔实验室工作的克劳

204
00:11:52,468 --> 00:11:54,873
德·香农 (C laude Shannon)

205
00:11:54,873 --> 00:11:57,905
提出的，但他正在与约翰·冯·诺依曼 ( John von

206
00:11:57,905 --> 00:12:00,624
Neumann) 谈论他尚未发表的一些想法，约翰·

207
00:12:00,624 --> 00:12:02,506
冯·诺依曼是当时非常杰出的知识巨人。

208
00:12:02,506 --> 00:12:04,180
数学和物理以及计算机科学的开端。

209
00:12:04,180 --> 00:12:07,756
当冯·诺依曼提到他对于信息量的期望值

210
00:12:07,756 --> 00:12:11,332
并没有一个好名字时，据说，所以故事是

211
00:12:11,332 --> 00:12:14,720
这样的，你应该称之为熵，有两个原因。

212
00:12:14,720 --> 00:12:18,846
首先，你的不确定性函数已经在统计力学中以这个名字使

213
00:12:18,846 --> 00:12:22,972
用了，所以它已经有一个名字了，其次，更重要的是，没

214
00:12:22,972 --> 00:12:26,940
有人知道熵到底是什么，所以在辩论中你总是会有优势。

215
00:12:26,940 --> 00:12:30,263
因此，如果这个名字看起来有点神秘，并且

216
00:12:30,263 --> 00:12:33,420
如果这个故事可信的话，那就是有意为之。

217
00:12:33,420 --> 00:12:37,037
另外，如果您想知道它与物理学中所有热力学

218
00:12:37,037 --> 00:12:40,483
第二定律的关系，那么肯定存在某种联系，

219
00:12:40,483 --> 00:12:44,101
但在其起源中，香农只是处理纯概率论，并且

220
00:12:44,101 --> 00:12:47,546
出于我们的目的，当我使用熵这个词，我只

221
00:12:47,546 --> 00:12:50,820
是想让你思考一个特定猜测的预期信息值。

222
00:12:50,820 --> 00:12:54,380
您可以将熵视为同时测量两个事物。

223
00:12:54,380 --> 00:12:57,420
第一个是分布的平坦程度。

224
00:12:57,420 --> 00:13:01,700
分布越接近均匀，熵就越高。

225
00:13:01,700 --> 00:13:05,043
在我们的例子中，总共有 3 到 5

226
00:13:05,043 --> 00:13:10,244
个模式，对于均匀分布，观察其 中任何一个模式都会有 3

227
00:13:10,244 --> 00:13:14,145
到 5 个的信息对数基数 2，恰好是 7。

228
00:13:14,145 --> 00:13:17,860
92，所以这是该熵可能具有的绝对最大值。

229
00:13:17,860 --> 00:13:22,900
但熵首先也是一种衡 量可能性的方法。

230
00:13:22,900 --> 00:13:25,971
例如，如果您碰巧有某个单词，其中只有

231
00:13:25,971 --> 00:13:29,203
16 种可能的模式，并 且每种模式的可能

232
00:13:29,203 --> 00:13:32,760
性相同，则该熵（即该预期信息）将是 4 位。

233
00:13:32,760 --> 00:13:36,964
但如果你有另一个词，其中可能出现 64 种可能的

234
00:13:36,964 --> 00:13:41,000
模式，并且它们的可能性相同，那么熵将是 6 位。

235
00:13:41,000 --> 00:13:45,589
因此，如果您在野外看到某个分布的熵为 6 位，这

236
00:13:45,589 --> 00:13:50,178
就有点像是在说即将发生的事情存在同样多的变化和不

237
00:13:50,178 --> 00:13:54,400
确定性，就好像有 64 个同样可能的结果一样。

238
00:13:54,400 --> 00:13:56,380
对于我第一次使用 Wurtele

239
00:13:56,380 --> 00:13:58,360
bot，我基本上就是让它这样做。

240
00:13:58,360 --> 00:14:03,209
它会遍历所有可能的猜测，即所有 13,000 个单

241
00:14:03,209 --> 00:14:07,873
词，计算每个单词的熵，或者更具体地说，计算您可能

242
00:14:07,873 --> 00:14:12,723
看到的所有模式中每个单词的分布熵，并选择最高的，因

243
00:14:12,723 --> 00:14:17,200
为这是一个可能会尽可能地削减你的可能性空间的人。

244
00:14:17,200 --> 00:14:19,497
尽管我在这里只讨论了第一个猜测，但它对

245
00:14:19,497 --> 00:14:21,680
于接下来的几次猜测也起到了同样的作用。

246
00:14:21,680 --> 00:14:25,272
例如，在您看到第一个猜测的某些模式后，这将根

247
00:14:25,272 --> 00:14:28,864
据与之匹配的内容将您限制为较少数量的可能单词

248
00:14:28,864 --> 00:14:32,300
，您只需针对该较小的单词集玩相同的游戏即可。

249
00:14:32,300 --> 00:14:36,755
对于建议的第二个猜测，您会查看从一组更受限制的

250
00:14:36,755 --> 00:14:41,210
单词中可能出现的所有模式的分布，搜索所有 13

251
00:14:41,210 --> 00:14:45,480
,000 种可能性，然后找到使熵最大化的一种。

252
00:14:45,480 --> 00:14:49,917
为了向您展示这是如何实际工作的，让我拿出我编写的 Wurt

253
00:14:49,917 --> 00:14:54,060
ele 的一个小变体，它在页边空白处显示了此分析的亮点。

254
00:14:54,060 --> 00:15:00,340
完成所有熵计算后，右侧向我们展 示了哪些具有最高的预期信息。

255
00:15:00,340 --> 00:15:05,940
事实证明，至少目前最重要的答案是稗子，我们稍后会对此进

256
00:15:05,940 --> 00:15:11,140
行完善，这意味着，嗯，当然是野豌豆，最常见的野豌豆。

257
00:15:11,140 --> 00:15:14,767
每次我们在这里进行猜测时，也许我会忽略它的建议并选择

258
00:15:14,767 --> 00:15:18,127
slate，因为我喜欢 slate，我们可以看到

259
00:15:18,127 --> 00:15:21,620
它有多少预期信息，但在这个词的右侧，它向我们展示了

260
00:15:21,620 --> 00:15:24,980
多少信息考虑到这种特定的模式，我们得到的实际信息。

261
00:15:24,980 --> 00:15:28,282
所以看起来我们有点不走运，我们预计会得到 5 个。

262
00:15:28,282 --> 00:15:30,660
8，但 我们碰巧得到的东西比这个少。

263
00:15:30,660 --> 00:15:33,343
然后在左侧，它向我们展示了当前

264
00:15:33,343 --> 00:15:35,860
所处位置的所有不同可能的单词。

265
00:15:35,860 --> 00:15:40,142
蓝色条告诉我们它认为每个单词出现的可能性有多大，因此目前它

266
00:15:40,142 --> 00:15:44,140
假设每个单词出现的可能性相同，但我们稍后会对其进行改进。

267
00:15:44,140 --> 00:15:48,270
然后，这种不确定性测量告诉我们可能单词的

268
00:15:48,270 --> 00:15:52,203
分布熵，因为它是均匀分布，所以现在只是

269
00:15:52,203 --> 00:15:55,940
一种计算可能性数量的不必要的复杂方法。

270
00:15:55,940 --> 00:15:59,166
例如，如果我们要计算 2 的 13 次方。

271
00:15:59,166 --> 00:16:02,700
66，这应该是 大约 13,000 种可能性。

272
00:16:02,700 --> 00:16:06,780
我在这里有点偏离，但这只是因为我没有显示所有小数位。

273
00:16:06,780 --> 00:16:09,905
目前，这可能感觉多余，而且好像事情过于复杂，但您

274
00:16:09,905 --> 00:16:12,780
很快就会明白为什么同时拥有这两个数字是有用的。

275
00:16:12,780 --> 00:16:16,432
所以这里看起来它表明我们第二个猜测的

276
00:16:16,432 --> 00:16:19,700
最高熵是拉面，这又感觉不像一个词。

277
00:16:19,700 --> 00:16:25,660
因此，为了占据道德制高点，我将继续输入 Rains。

278
00:16:25,660 --> 00:16:27,540
看来我们又有点不走运了。

279
00:16:27,540 --> 00:16:32,100
我们本来期待4。3 位，但我们只得到了 3 位。39位信息。

280
00:16:32,100 --> 00:16:35,060
这样一来，我们就有 55 种可能性。

281
00:16:35,060 --> 00:16:37,790
在这里，也许我实际上会遵循它的建

282
00:16:37,790 --> 00:16:40,200
议，即组合，无论这意味着什么。

283
00:16:40,200 --> 00:16:43,300
好吧，这实际上是一个解谜的好机会。

284
00:16:43,300 --> 00:16:47,020
它告诉我们这个模式给了我们 4。7 位信息。

285
00:16:47,020 --> 00:16:50,838
但在左边，在我们看到该模式之前，有 5 个。

286
00:16:50,838 --> 00:16:52,400
78 位不确定性。

287
00:16:52,400 --> 00:16:56,860
那么作为对你的一个测验，剩余可能性的数量意味着什么？

288
00:16:56,860 --> 00:17:00,885
嗯，这意味着我们将不确定性减少到一点

289
00:17:00,885 --> 00:17:04,700
点，这与说有两个可能的答案是一样的。

290
00:17:04,700 --> 00:17:06,520
这是50-50的选择。

291
00:17:06,520 --> 00:17:08,937
从这里开始，因为你和我知道哪些词更

292
00:17:08,937 --> 00:17:11,220
常见，所以我们知道答案应该是深渊。

293
00:17:11,220 --> 00:17:13,540
但正如现在所写的，程序并不知道这一点。

294
00:17:13,540 --> 00:17:17,112
所以它会继续前进，尝试获取尽可能多的信息，

295
00:17:17,112 --> 00:17:20,360
直到只剩下一种可能性，然后它就会猜测它。

296
00:17:20,360 --> 00:17:22,700
显然我们需要更好的残局策略。

297
00:17:22,700 --> 00:17:26,874
但是，假设我们将此版本称为我们的 wordle 求解

298
00:17:26,874 --> 00:17:30,740
器之一，然后我们运行一些模拟来看看它是如何工作的。

299
00:17:30,740 --> 00:17:34,240
所以它的工作方式是玩所有可能的文字游戏。

300
00:17:34,240 --> 00:17:38,780
它会检查所有 2315 个单词，这些单词是实际的单词答案。

301
00:17:38,780 --> 00:17:41,340
它基本上使用它作为测试集。

302
00:17:41,340 --> 00:17:46,003
采用这种天真的方法，不考虑一个词的常见程度，只是

303
00:17:46,003 --> 00:17:50,480
试图在每一步中最大化信息，直到它只剩下一个选择。

304
00:17:50,480 --> 00:17:55,100
模拟结束时，平均得分约为 4。124.

305
00:17:55,100 --> 00:17:59,780
老实说，这还不错，我本来以为会做得更糟。

306
00:17:59,780 --> 00:18:03,040
但玩wordle的人会告诉你，他们通常可以在4内得到它。

307
00:18:03,040 --> 00:18:05,260
真正的挑战是尽可能多地获得三分。

308
00:18:05,260 --> 00:18:08,920
4分和3分之间的差距相当大。

309
00:18:08,920 --> 00:18:16,349
这里显而易见的容易实现的目标是以某种方式纳入一

310
00:18:16,349 --> 00:18:23,160
个单词是否常见，以及我们到底如何做到这一点。

311
00:18:23,160 --> 00:18:28,560
我的方法是获取英语中所 有单词的相对频率列表。

312
00:18:28,560 --> 00:18:30,422
我刚刚使用了 Mathematica

313
00:18:30,422 --> 00:18:32,579
的词频数据函数，它本身是从 Go ogle

314
00:18:32,579 --> 00:18:35,520
Books English Ngram 公共数据集中提取的。

315
00:18:35,520 --> 00:18:37,882
看起来很有趣，例如，如果我们将其从最

316
00:18:37,882 --> 00:18:40,120
常见的单词到最不常见的单词进行排序。

317
00:18:40,120 --> 00:18:43,740
显然，这些是英语中最常见的 5 个字母单词。

318
00:18:43,740 --> 00:18:46,480
或者更确切地说，这些是第八个最常见的。

319
00:18:46,480 --> 00:18:49,440
首先是which，然后是there和there。

320
00:18:49,440 --> 00:18:52,749
First本身不是first，而是9th，并且这些其

321
00:18:52,749 --> 00:18:55,935
他词可能更频繁地出现是有道理的，其中first之后

322
00:18:55,935 --> 00:18:59,000
的词是after、where，而那些词则不太常见。

323
00:18:59,000 --> 00:19:02,970
现在，在使用这些数据来模拟每个单词成为最终

324
00:19:02,970 --> 00:19:06,760
答案的可能性时，它不应该仅仅与频率成正比。

325
00:19:06,760 --> 00:19:10,897
例如，得分为 0。002 在此数据集中，而“br

326
00:19:10,897 --> 00:19:15,200
aid”一词在某种意义上的可能性要小 1000 倍。

327
00:19:15,200 --> 00:19:19,400
但这两个词都很常见，几乎肯定值得考虑。

328
00:19:19,400 --> 00:19:21,900
所以我们想要更多的二元截止。

329
00:19:21,900 --> 00:19:26,209
我的方法是想象一下将整个排序的单词列表，然后将其排列

330
00:19:26,209 --> 00:19:30,359
在 x 轴上，然后应用 sigmoid 函数，这是

331
00:19:30,359 --> 00:19:34,509
输出基本上是二进制的函数的标准方法，它是要么是 0

332
00:19:34,509 --> 00:19:38,500
，要么是 1，但对于该不确定区域，中间有一个平滑。

333
00:19:38,500 --> 00:19:44,217
所以本质上，我分配给每个单词出现在最终列表中的概率将是上

334
00:19:44,217 --> 00:19:49,540
面的 sigmoid 函数的值，无论它位于 x 轴上。

335
00:19:49,540 --> 00:19:54,192
显然，这取决于几个参数，例如，这些单词在 x 轴上填充

336
00:19:54,192 --> 00:19:58,679
的空间有多宽，决定了我们从 1 到 0 的逐渐或陡峭

337
00:19:58,679 --> 00:20:03,000
程度，以及我们将它们从左到右放置的位置决定了截止值。

338
00:20:03,000 --> 00:20:07,340
说实话，我的做法就是舔手指然后把它插到风里。

339
00:20:07,340 --> 00:20:10,959
我查看了排序后的列表，并试图找到一个窗口

340
00:20:10,959 --> 00:20:14,405
，当我查看它时，我认为这些单词中的一半

341
00:20:14,405 --> 00:20:17,680
更有可能是最终答案，并将其用作截止值。

342
00:20:17,680 --> 00:20:21,217
一旦我们在单词之间有了这样的分布，它就会给我们

343
00:20:21,217 --> 00:20:24,460
带来另一种情况，即熵成为这种真正有用的度量。

344
00:20:24,460 --> 00:20:27,612
例如，假设我们正在玩一个游戏，我们从我

345
00:20:27,612 --> 00:20:30,765
的旧开场白开始，即羽毛和指甲，我们最终

346
00:20:30,765 --> 00:20:33,760
会遇到有四个可能的单词与之匹配的情况。

347
00:20:33,760 --> 00:20:36,440
假设我们认为它们都有相同的可能性。

348
00:20:36,440 --> 00:20:40,000
我问你，这个分布的熵是多少？

349
00:20:40,000 --> 00:20:45,200
嗯，与这些可能性中的每一种相关的信息将是 4 的以

350
00:20:45,200 --> 00:20:50,800
2 为底的对数，因为每一种都是 1 和 4，那就是 2。

351
00:20:50,800 --> 00:20:52,780
两位信息，四种可能性。

352
00:20:52,780 --> 00:20:54,360
一切都很好。

353
00:20:54,360 --> 00:20:58,320
但如果我告诉你实际上有超过四场比赛呢？

354
00:20:58,320 --> 00:21:01,081
事实上，当我们查看完整的单词列表时，有

355
00:21:01,081 --> 00:21:02,600
16 个单词与其匹配。

356
00:21:02,600 --> 00:21:07,190
但假设我们的模型对其他 12 个单词实际成为最终答案

357
00:21:07,190 --> 00:21:11,440
的概率非常低，大约是千分之一，因为它们真的很晦涩。

358
00:21:11,440 --> 00:21:15,480
现在我问你，这个分布的熵是多少？

359
00:21:15,480 --> 00:21:19,165
如果熵在这里纯粹测量匹配的数量，那么您可能

360
00:21:19,165 --> 00:21:22,850
会期望它类似于 16 的以 2 为底的对数

361
00:21:22,850 --> 00:21:26,200
，即 4，比我们之前的不确定性多了两位。

362
00:21:26,200 --> 00:21:30,320
但当然，实际的不确定性与我们之前的情况并没有太大不同。

363
00:21:30,320 --> 00:21:34,424
例如，仅仅因为有这 12 个非常晦涩的单词并不意

364
00:21:34,424 --> 00:21:38,200
味着当得知最终答案是“魅力”时会更加令人惊讶。

365
00:21:38,200 --> 00:21:41,997
所以当你在这里实际进行计算时，将每次出现的概

366
00:21:41,997 --> 00:21:45,960
率乘以相应的信息相加，得到的就是 2。11 位。

367
00:21:45,960 --> 00:21:49,823
我只是说，它基本上是两位，基本上是这四种可能性，但是

368
00:21:49,823 --> 00:21:53,543
由于所有这些极不可能发生的事件，存在更多的不确定性

369
00:21:53,543 --> 00:21:57,120
，尽管如果你确实了解了它们，你会从中获得大量信息。

370
00:21:57,120 --> 00:21:59,583
缩小范围，这就是 Wordle 成为信

371
00:21:59,583 --> 00:22:01,800
息论课程的一个很好的例子的部分原因。

372
00:22:01,800 --> 00:22:05,280
我们对熵有两种不同的感觉应用。

373
00:22:05,280 --> 00:22:09,142
第一个告诉我们从给定的猜测中得到的预期

374
00:22:09,142 --> 00:22:13,004
信息是什么，第二个告诉我们我们是否可以

375
00:22:13,004 --> 00:22:16,480
衡量所有可能的单词中剩余的不确定性。

376
00:22:16,480 --> 00:22:20,823
我应该强调，在第一种情况下，我们正在查看猜测的预期

377
00:22:20,823 --> 00:22:25,000
信息，一旦我们对单词的权重不相等，就会影响熵计算。

378
00:22:25,000 --> 00:22:28,997
例如，让我拿出我们之前查看的与 We ary

379
00:22:28,997 --> 00:22:33,690
相关的分布的相同案例，但这次 在所有可能的单词中使用非

380
00:22:33,690 --> 00:22:34,560
均匀分布。

381
00:22:34,560 --> 00:22:39,360
所以让我看看是否可以在这里找到一个很好地说明它的部分。

382
00:22:39,360 --> 00:22:42,480
好吧，这里这很好。

383
00:22:42,480 --> 00:22:46,046
这里我们有两个相邻的模式，它们的可能性大致相同，但我

384
00:22:46,046 --> 00:22:49,480
们被告知其中一个模式有 32 个可能的单词与其匹配。

385
00:22:49,480 --> 00:22:52,679
如果我们检查它们是什么，那就是 32 个，当

386
00:22:52,679 --> 00:22:55,600
你扫视它们时，它们都只是非常不可能的单词。

387
00:22:55,600 --> 00:22:59,300
很难找到任何看似合理的答案，也许会大喊大叫，

388
00:22:59,300 --> 00:23:02,840
但如果我们查看分布中的相邻模式，这被认为是

389
00:23:02,840 --> 00:23:06,541
同样可能的，我们被告知它只有 8 个可能的匹

390
00:23:06,541 --> 00:23:09,920
配，所以四分之一很多比赛，但可能性差不多。

391
00:23:09,920 --> 00:23:12,520
当我们拿出这些匹配项时，我们就能明白原因了。

392
00:23:12,520 --> 00:23:17,840
其中一些是实际合理的答案，例如戒指、愤怒或说唱。

393
00:23:17,840 --> 00:23:20,852
为了说明我们如何整合所有这些，让我在这里列出

394
00:23:20,852 --> 00:23:22,947
Wordlebo t 的第 2

395
00:23:22,947 --> 00:23:25,960
版，它与我们看到的第一个版本有两三个主要区别。

396
00:23:25,960 --> 00:23:30,539
首先，就像我刚才说的，我们计算这些熵、这些信

397
00:23:30,539 --> 00:23:35,118
息的预期值的方式现在正在使用跨模式的更精细的

398
00:23:35,118 --> 00:23:39,300
分布，其中包含给定单词实际上是答案的概率。

399
00:23:39,300 --> 00:23:44,160
碰巧，眼泪仍然是第一位，尽管接下来的有点不同。

400
00:23:44,160 --> 00:23:47,997
其次，当它对首选进行排名时，它现在将保留每个单词

401
00:23:47,997 --> 00:23:51,835
是实际答案的概率模型，并将其纳入其决策中，一旦我

402
00:23:51,835 --> 00:23:55,520
们对答案有了一些猜测，就更容易看到这一点。桌子。

403
00:23:55,520 --> 00:24:01,120
再次忽略它的建议，因为我们不能让机器统治我们的生活。

404
00:24:01,120 --> 00:24:05,794
我想我应该提到另一件不同的事情是在左边，不确定

405
00:24:05,794 --> 00:24:10,080
性值，即位数，不再只是与可能匹配的数量冗余。

406
00:24:10,080 --> 00:24:14,016
现在，如果我们将其拉高并计算 2 的 8。

407
00:24:14,016 --> 00:24:17,014
02，略高于 256，我 猜是

408
00:24:17,014 --> 00:24:20,763
259，它的意思是，尽管总共有 526

409
00:24:20,763 --> 00:24:25,449
个单词实际上与此模式匹配，但它所具有的不确定性量

410
00:24:25,449 --> 00:24:29,760
更类似于如果有 259 个同样可能的单词结果。

411
00:24:29,760 --> 00:24:31,100
你可以这样想。

412
00:24:31,100 --> 00:24:34,292
它知道 borx 不是答案，与 yorts、zorl

413
00:24:34,292 --> 00:24:37,840
和 zorus 一样，因此它的不确定性比之前的情况要小一些。

414
00:24:37,840 --> 00:24:40,220
这个位数会更小。

415
00:24:40,220 --> 00:24:44,553
如果我继续玩这个游戏，我会用一些与我想在

416
00:24:44,553 --> 00:24:48,680
这里解释的内容相符的猜测来完善这个游戏。

417
00:24:48,680 --> 00:24:51,390
通过第四种猜测，如果你看一下它的首

418
00:24:51,390 --> 00:24:53,800
选，你会发现它不再只是最大化熵。

419
00:24:53,800 --> 00:24:57,389
所以在这一点上，技术上有七种可能性

420
00:24:57,389 --> 00:25:00,780
，但唯一有意义的机会是宿舍和单词。

421
00:25:00,780 --> 00:25:04,256
您可以看到它对选择这两个值的排名高于所

422
00:25:04,256 --> 00:25:07,560
有其他值，严格来说，这会提供更多信息。

423
00:25:07,560 --> 00:25:11,229
我第一次这样做时，我只是将这两个数字相加来衡

424
00:25:11,229 --> 00:25:14,580
量每个猜测的质量，这实际上比你想象的要好。

425
00:25:14,580 --> 00:25:17,301
但它确实感觉不系统，而且我确信人们可

426
00:25:17,301 --> 00:25:19,880
以采取其他方法，但这是我找到的方法。

427
00:25:19,880 --> 00:25:24,312
如果我们正在考虑下一次猜测的前景，就像在这种情况下的话，

428
00:25:24,312 --> 00:25:28,440
我们真正关心的是如果我们这样做的话我们游戏的预期得分。

429
00:25:28,440 --> 00:25:32,132
为了计算预期分数，我们会计算单词是实际

430
00:25:32,132 --> 00:25:35,640
答案的概率是多少，目前描述为 58%。

431
00:25:35,640 --> 00:25:40,400
我们说，有 58% 的机会，我们在这场比赛中得分为 4。

432
00:25:40,400 --> 00:25:46,240
然后，以 1 减去 58% 的概率，我们的分数将大于 4。

433
00:25:46,240 --> 00:25:49,670
我们不知道还有多少，但我们可以根据到

434
00:25:49,670 --> 00:25:52,920
达这一点后可能存在的不确定性来估计。

435
00:25:52,920 --> 00:25:56,600
具体来说，目前有1。44 位不确定性。

436
00:25:56,600 --> 00:26:00,760
如果我们猜测单词，它会告诉我们预期得到的信息是 1。

437
00:26:00,760 --> 00:26:01,560
27 位。

438
00:26:01,560 --> 00:26:05,080
因此，如果我们猜测单词，这种差异代表了在这

439
00:26:05,080 --> 00:26:08,280
种情况发生后我们可能会留下多少不确定性。

440
00:26:08,280 --> 00:26:10,940
我们需要的是某种函数，我在这里称之为

441
00:26:10,940 --> 00:26:13,880
f ，它将这种不确定性与预期分数联系起来。

442
00:26:13,880 --> 00:26:18,334
它的处理方式是根据机器人的版本 1 绘制之

443
00:26:18,334 --> 00:26:22,788
前游戏的一堆数据，以说明在具有某些非常可测

444
00:26:22,788 --> 00:26:27,040
量的不确定性的各个点之后的实际得分是多少。

445
00:26:27,040 --> 00:26:31,326
例如，这里的这些数据点位于 8 左右的值之上。

446
00:26:31,326 --> 00:26:35,240
在 8分之后的一些比赛中，大约有7分左右。

447
00:26:35,240 --> 00:26:39,340
7位不确定 性，经过两次猜测才得到最终答案。

448
00:26:39,340 --> 00:26:43,180
对于其他游戏需要猜测三次，对于其他游戏需要猜测四次。

449
00:26:43,180 --> 00:26:47,175
如果我们在这里向左移动，所有超过零的点都表明，

450
00:26:47,175 --> 00:26:51,170
每当不确定性为零时，也就是说只有一种可能性，那

451
00:26:51,170 --> 00:26:55,000
么所需的猜测次数总是只有一次，这是令人放心的。

452
00:26:55,000 --> 00:26:58,145
每当有一点不确定性时，意味着基本上只

453
00:26:58,145 --> 00:27:01,125
有两种可能性，那么有时需要再进行一

454
00:27:01,125 --> 00:27:03,940
次猜测，有时则需要再进行两次猜测。

455
00:27:03,940 --> 00:27:05,980
这里等等等等。

456
00:27:05,980 --> 00:27:08,500
也许可视化这些数据的一种稍微简单

457
00:27:08,500 --> 00:27:11,020
的方法是将其放在一起并取平均值。

458
00:27:11,020 --> 00:27:16,598
例如，这里的这个条表示，在我们有一点不确定性

459
00:27:16,598 --> 00:27:22,420
的所有点中，平均所需的新猜测数量约为 1。5.

460
00:27:22,420 --> 00:27:26,009
这里的栏表示在所有不同的游戏中，在某些

461
00:27:26,009 --> 00:27:30,317
时候不确定性略高于 4 位，这就像将 其缩小到

462
00:27:30,317 --> 00:27:34,804
16 种不同的可能性，然后从 该点开始平均需要两个

463
00:27:34,804 --> 00:27:36,240
以上的猜测向前。

464
00:27:36,240 --> 00:27:40,080
从这里开始，我只是做了一个回归来拟合一个看起来合理的函数。

465
00:27:40,080 --> 00:27:45,092
请记住，这样做的全部目的是为了我们可以量化这种直觉

466
00:27:45,092 --> 00:27:49,720
，即我们从单词中获得的信息越多，预期得分就越低。

467
00:27:49,720 --> 00:27:54,770
所以将此作为版本 2。0，如果我们返回并运行相同的一组模拟，

468
00:27:54,770 --> 00:27:59,820
让它 与所有 2315 个可能的单词答案进行比较，结果如何？

469
00:27:59,820 --> 00:28:04,060
与我们的第一个版本相比，它肯定更好，这令人放心。

470
00:28:04,060 --> 00:28:08,790
综上所述，平均值约为 3。6，尽管与第一个版本不同，

471
00:28:08,790 --> 00:28:12,820
它有几次丢失并且在这种情况下需要超过 6 个。

472
00:28:12,820 --> 00:28:15,999
大概是因为有时需要进行权衡以实

473
00:28:15,999 --> 00:28:18,980
际实现目标，而不是最大化信息。

474
00:28:18,980 --> 00:28:22,140
那么我们可以做得比 3 更好吗？6？

475
00:28:22,140 --> 00:28:23,260
我们绝对可以。

476
00:28:23,260 --> 00:28:26,780
现在我在一开始就说过，尝试不将单词答案的真

477
00:28:26,780 --> 00:28:29,980
实列表合并到构建模型的方式中是最有趣的。

478
00:28:29,980 --> 00:28:34,509
但如果我们确实将其合并，我可以获得的最佳性能约为 3。

479
00:28:34,509 --> 00:28:35,180
43.

480
00:28:35,180 --> 00:28:39,410
因此，如果我们尝试比仅使用词频数据更复杂地选择此先验分

481
00:28:39,410 --> 00:28:43,036
布，则这 3.43 可能给出了我们可以做到什么

482
00:28:43,036 --> 00:28:46,360
程度的最大值，或者至少是我可以做到什么程度。

483
00:28:46,360 --> 00:28:49,579
最佳性能本质上只是使用了我在这里讨

484
00:28:49,579 --> 00:28:52,798
论的想法，但它更进一步，就像它向前

485
00:28:52,798 --> 00:28:55,660
两步而不是一步搜索预期信息一样。

486
00:28:55,660 --> 00:28:58,186
本来我打算更多地讨论这个问题，但我意

487
00:28:58,186 --> 00:29:00,580
识到我们实际上已经讨论了很长时间了。

488
00:29:00,580 --> 00:29:03,638
我要说的一件事是，在进行了两步搜索，然后在顶级

489
00:29:03,638 --> 00:29:06,696
候选者中运行了几个样本模拟之后，至少到目前为止

490
00:29:06,696 --> 00:29:09,500
对我来说，Crane 看起来是最好的开局者。

491
00:29:09,500 --> 00:29:11,080
谁能想到呢？

492
00:29:11,080 --> 00:29:14,572
此外，如果您使用真实的单词列表来确定您的可能性

493
00:29:14,572 --> 00:29:17,920
空间，那么您开始的不确定性将略高于 11 位。

494
00:29:17,920 --> 00:29:22,367
事实证明，仅通过强力搜索，前两次猜测

495
00:29:22,367 --> 00:29:26,580
后最大可能的预期信息约为 10 位。

496
00:29:26,580 --> 00:29:30,988
这表明，在最好的情况下，在您进行前两次猜测之后，

497
00:29:30,988 --> 00:29:35,220
如果有完美的最佳玩法，您将留下大约一点不确定性。

498
00:29:35,220 --> 00:29:37,400
这与归结为两种可能的猜测相同。

499
00:29:37,400 --> 00:29:41,626
因此，我认为公平且可能相当保守地说，您永远不可能编

500
00:29:41,626 --> 00:29:45,691
写一个使平均值低至 3 的算法，因为根据您可用的

501
00:29:45,691 --> 00:29:50,080
单词，在仅执行两个步骤后根本没有空间获得足够的信息 。

502
00:29:50,080 --> 00:29:53,820
能够保证每次都在第三个槽中得到答案，不会失败。

