1
00:00:00,000 --> 00:00:03,781
Wurdle 游戏在过去一两个月里非常 火爆，

2
00:00:03,781 --> 00:00:06,412
从来没有人会忽视数学课的机会， 

3
00:00:06,412 --> 00:00:10,687
我觉得这个游戏是信息论课程中一个非常好 的中心例子，

4
00:00:10,687 --> 00:00:12,660
特别是一个称为熵的主题。

5
00:00:13,920 --> 00:00:17,358
你看，就像很多人一样，我有点陷入了这个 谜题，

6
00:00:17,358 --> 00:00:19,899
而且像很多程序员一样，我也陷入了 

7
00:00:19,899 --> 00:00:22,740
尝试编写一种算法来尽可能最佳地玩游戏。

8
00:00:23,180 --> 00:00:26,897
我想我在这里要做的只是与你们讨论我 的一些过程，

9
00:00:26,897 --> 00:00:31,080
并解释其中的一些数学 ，因为整个算法以熵的概念为中心。

10
00:00:38,700 --> 00:00:41,640
首先，如果您还没有听说过，那么什么是 Wurdle？

11
00:00:42,040 --> 00:00:47,080
在我们介绍游戏规则的同时，让我也 预览一下我们要做什么，

12
00:00:47,080 --> 00:00:51,040
即开发一个 基本上可以为我们玩游戏的小算法。

13
00:00:51,360 --> 00:00:52,939
虽然我还没有完成今天的 Wurdle，

14
00:00:52,939 --> 00:00:55,100
但现在 是 2 月 4 日，我们将看看机器人的表现。

15
00:00:55,480 --> 00:00:58,805
Wurdle 的目标是猜测一个神秘的五 个字母单词，

16
00:00:58,805 --> 00:01:00,340
您有六次不同的猜测机会。

17
00:01:00,840 --> 00:01:04,379
例如，我的 Wurdle 机器人建议我从猜测起重机开始。

18
00:01:05,180 --> 00:01:07,768
每次您进行猜测时，您都会获得一些有关 

19
00:01:07,768 --> 00:01:10,220
您的猜测与真实答案的接近程度的信息。

20
00:01:10,920 --> 00:01:14,100
灰色框告诉我实际答案中没有 C。

21
00:01:14,520 --> 00:01:17,840
黄色框告诉我有一个 R，但它不在那个位置。

22
00:01:18,240 --> 00:01:22,240
绿色框告诉我秘密词确实有一个 A，而且位于第三个位置。

23
00:01:22,720 --> 00:01:24,580
然后就没有N了，也没有E了。

24
00:01:25,200 --> 00:01:27,340
那么让我进去告诉 Wurdle 机器人该信息。

25
00:01:27,340 --> 00:01:30,320
我们从起重机开始，有灰色、黄色、绿色、灰色、灰色。

26
00:01:31,420 --> 00:01:34,940
不要担心它现在显示的所有数据，我会在适当的时候解释这一点。

27
00:01:35,460 --> 00:01:38,820
但对于我们的第二个选择来说，它的首要建议是小技巧。

28
00:01:39,560 --> 00:01:42,236
你的猜测确实必须是一个实际的五个字母的单词，

29
00:01:42,236 --> 00:01:45,400
但正 如你将看到的，它实际上让你猜测的内容相当自由。

30
00:01:46,200 --> 00:01:47,440
在这种情况下，我们尝试一下shtic。

31
00:01:48,780 --> 00:01:50,180
好吧，事情看起来相当不错。

32
00:01:50,260 --> 00:01:52,852
我们按了 S 和 H，所以我们知道前三个字母，

33
00:01:52,852 --> 00:01:53,980
我们知道有一个 R。

34
00:01:53,980 --> 00:01:58,700
所以它会像 SHA 某些 R 或 SHA R 某些东西。

35
00:01:59,620 --> 00:02:02,699
看起来 Wurdle 机器人知道它只有 两种可能性，

36
00:02:02,699 --> 00:02:04,240
要么是碎片，要么是锋利的。

37
00:02:05,100 --> 00:02:06,892
在这一点上，他们之间存在着一种折腾，

38
00:02:06,892 --> 00:02:09,084
所以我想可能只 是因为它是按字母顺序排列的，

39
00:02:09,084 --> 00:02:10,080
所以它与分片相匹配。

40
00:02:11,220 --> 00:02:12,860
万岁，这才是真正的答案。

41
00:02:12,960 --> 00:02:13,780
所以我们三分就搞定了。

42
00:02:14,600 --> 00:02:17,364
如果你想知道这是否有好处，我听到一个人的说法是，

43
00:02:17,364 --> 00:02:20,360
对 于Wurdle来说，四杆是标准杆，三杆是小鸟球。

44
00:02:20,680 --> 00:02:22,480
我认为这是一个非常恰当的比喻。

45
00:02:22,480 --> 00:02:27,020
你必须坚持自己的比赛才能获得四分，但这当然并不疯狂。

46
00:02:27,180 --> 00:02:29,920
但当你拿到三分的时候，感觉棒极了。

47
00:02:30,880 --> 00:02:33,521
因此，如果您愿意的话，我在这里想做的就是从一开始就 

48
00:02:33,521 --> 00:02:35,960
谈谈我如何处理 Wurdle 机器人的思考过程。

49
00:02:36,480 --> 00:02:39,440
就像我说的，这确实是信息论课程的借口。

50
00:02:39,740 --> 00:02:42,820
主要目标是解释什么是信息和什么是熵。

51
00:02:48,220 --> 00:02:51,053
在解决这个问题时，我的第一个想法 

52
00:02:51,053 --> 00:02:53,720
是看看英语中不同字母的相对频率。

53
00:02:54,380 --> 00:02:56,948
所以我想，好吧，是否有一个开局猜测或一 

54
00:02:56,948 --> 00:02:59,260
对开局猜测可以命中这些最常见的字母？

55
00:02:59,960 --> 00:03:03,000
我非常喜欢做其他事情，然后做指甲。

56
00:03:03,760 --> 00:03:05,828
我们的想法是，如果你击中一个字母，你知道 ，

57
00:03:05,828 --> 00:03:07,520
你会得到绿色或黄色，这总是感觉很好。

58
00:03:07,520 --> 00:03:08,840
感觉就像你正在获取信息。

59
00:03:09,340 --> 00:03:13,003
但在这些情况下，即使你没有击中并且总 是得到灰色，

60
00:03:13,003 --> 00:03:17,400
这仍然为你提供了大量信息 ，因为很少找到没有这些字母的单词。

61
00:03:18,140 --> 00:03:20,886
但即便如此，这仍然感觉不是超级系统 ，

62
00:03:20,886 --> 00:03:23,200
因为例如，它没有考虑字母的顺序。

63
00:03:23,560 --> 00:03:25,300
当我可以输入蜗牛时，为什么还要输入指甲？

64
00:03:26,080 --> 00:03:27,500
最后加个S是不是更好？

65
00:03:27,820 --> 00:03:28,680
我不太确定。

66
00:03:29,240 --> 00:03:32,580
现在，我的一个朋友说他喜欢用“weary”这个词开头，

67
00:03:32,580 --> 00:03:35,426
这让 我有点惊讶，因为里面有一些不常见的字母，

68
00:03:35,426 --> 00:03:36,540
比如 W 和 Y。

69
00:03:37,120 --> 00:03:39,000
但谁知道呢，也许这是一个更好的开局。

70
00:03:39,320 --> 00:03:44,320
我们是否可以给出某种定量评 分来判断潜在猜测的质量？

71
00:03:45,340 --> 00:03:48,899
现在，为了设置我们对可能的猜测进行排名的 方式，

72
00:03:48,899 --> 00:03:51,420
让我们回顾一下游戏的具体设置方式。

73
00:03:51,420 --> 00:03:55,583
因此，您可以输入一个单词列表，这些单词被视为 有效的猜测，

74
00:03:55,583 --> 00:03:57,880
长度约为 13,000 个单词。

75
00:03:58,320 --> 00:04:02,010
但当你仔细观察时，你会发现有很多非常不常见的东西，

76
00:04:02,010 --> 00:04:06,440
比如 头像、阿里和ARG，以及拼字游戏中引发家庭争吵的词语。

77
00:04:06,960 --> 00:04:10,540
但游戏的氛围是答案总是一个相当常见的词。

78
00:04:10,960 --> 00:04:14,160
事实上，还有另一个大约 2300 个单词的列表，

79
00:04:14,160 --> 00:04:15,360
它们是可能的答案。

80
00:04:15,940 --> 00:04:18,687
这是一个人工策划的列表，我认为是由游戏 

81
00:04:18,687 --> 00:04:21,160
创建者的女朋友专门制作的，这很有趣。

82
00:04:21,820 --> 00:04:24,516
但我想做的是，我们对这个项目的挑战是看看

83
00:04:24,516 --> 00:04:27,887
我们是否可以编写一个解 决 Wordle 的程序，

84
00:04:27,887 --> 00:04:30,180
该程序不包含有关此列表的先前知识。

85
00:04:30,720 --> 00:04:34,640
一方面，有很多非常常见的五个 字母单词您在该列表中找不到。

86
00:04:34,940 --> 00:04:37,403
因此，最好编写一个更具弹性的程序，

87
00:04:37,403 --> 00:04:41,460
并且可以与 任何人玩 Wordle，而不仅仅是官方网站。

88
00:04:41,920 --> 00:04:45,033
我们之所以知道可能答案的列表是 什么，

89
00:04:45,033 --> 00:04:47,000
是因为它在源代码中可见。

90
00:04:47,000 --> 00:04:53,260
但它在源代码中可见的方式是按 照每天出现答案的特定顺序。

91
00:04:53,260 --> 00:04:55,840
所以你总是可以查找明天的答案。

92
00:04:56,420 --> 00:04:58,880
很明显，使用该列表在某种程度上是作弊行为。

93
00:04:59,100 --> 00:05:03,090
使谜题更有趣、信息论课程更丰富的方法 

94
00:05:03,090 --> 00:05:07,289
是使用一些更通用的数据，例如相对词 频，

95
00:05:07,289 --> 00:05:10,440
来捕捉对更常见词的偏好的直觉。

96
00:05:11,600 --> 00:05:15,900
那么在这13000种可能性中，我们应该如何选择开局猜测呢？

97
00:05:16,400 --> 00:05:19,780
例如，如果我的朋友提出厌烦，我们应该如何分析它的质量？

98
00:05:20,520 --> 00:05:23,996
好吧，他说他喜欢那个不太可能的 W 的原因是他喜欢 

99
00:05:23,996 --> 00:05:27,340
远射的本质，如果你击中了那个 W，那感觉是多么好。

100
00:05:27,920 --> 00:05:30,992
例如，如果第一个揭示的模式是这样的，

101
00:05:30,992 --> 00:05:35,600
那么这个 庞大的词典中只有 58 个单词与该模式匹配。

102
00:05:36,060 --> 00:05:38,400
因此，与 13,000 人相比，这是一个巨大的减少。

103
00:05:38,780 --> 00:05:43,020
但当然，另一方面是，获得这样的模式非常罕见。

104
00:05:43,020 --> 00:05:46,548
具体来说，如果每个单词作为答案的可能性相同，

105
00:05:46,548 --> 00:05:51,040
则达到 此模式的概率将为 58 除以大约 13,000。

106
00:05:51,580 --> 00:05:53,600
当然，它们成为答案的可能性并不相同。

107
00:05:53,720 --> 00:05:56,220
其中大部分都是非常晦涩甚至有问题的词语。

108
00:05:56,600 --> 00:05:58,504
但至少对于我们第一次通过这一切，

109
00:05:58,504 --> 00:06:01,600
我们假设它 们的可能性相同，然后稍后再对其进行完善。

110
00:06:02,020 --> 00:06:06,720
关键是，包含大量信息的模式本质上不太可能发生。

111
00:06:07,280 --> 00:06:10,800
事实上，提供信息意味着这是不可能的。

112
00:06:11,719 --> 00:06:18,120
在这个开口中看到的更可能的模式 是这样的，当然其中没有 W。

113
00:06:18,240 --> 00:06:21,400
也许有E，也许没有A，没有R，没有Y。

114
00:06:22,080 --> 00:06:24,560
在本例中，有 1400 个可能的匹配项。

115
00:06:25,080 --> 00:06:30,600
如果所有可能性均等，则您看到 的模式的概率约为 11%。

116
00:06:30,900 --> 00:06:33,340
因此，最可能的结果也是信息最少的。

117
00:06:34,240 --> 00:06:37,778
为了获得更全面的视角，让我向您展示您可 

118
00:06:37,778 --> 00:06:41,140
能看到的所有不同模式的概率的完整分布。

119
00:06:41,740 --> 00:06:46,046
因此，您看到的每个条形都对应于可能显示的颜 色模式，

120
00:06:46,046 --> 00:06:49,027
其中有 3 到 5 种可能性，并且 

121
00:06:49,027 --> 00:06:52,340
它们从左到右、最常见到最不常见进行组织。

122
00:06:52,920 --> 00:06:56,000
所以这里最常见的可能性是你得到的都是灰色的。

123
00:06:56,100 --> 00:06:58,120
这种情况发生的概率约为 14%。

124
00:06:58,580 --> 00:07:02,253
当你进行猜测时，你所希望的是你最终会出现在这条 

125
00:07:02,253 --> 00:07:05,773
长尾中的某个地方，就像在这里，与这个显然看起 

126
00:07:05,773 --> 00:07:09,140
来像这样的模式相匹配的可能性只有 18 种。

127
00:07:09,920 --> 00:07:12,298
或者，如果我们冒险向左走一点，你知道，

128
00:07:12,298 --> 00:07:13,800
也许我们会一直走到这里。

129
00:07:14,940 --> 00:07:16,180
好的，这是给你的一个很好的谜题。

130
00:07:16,540 --> 00:07:19,343
英语中以 W 开头、以 Y 结尾、其 

131
00:07:19,343 --> 00:07:22,000
中某个位置有 R 的三个单词是什么？

132
00:07:22,480 --> 00:07:26,800
事实证明，让我们看看，答案是罗嗦、狡猾和讽刺的。

133
00:07:27,500 --> 00:07:30,724
因此，为了判断这个词的整体效果如何，

134
00:07:30,724 --> 00:07:35,740
我们需要某 种方式来衡量您将从该分布中获得的预期信息量。

135
00:07:35,740 --> 00:07:40,417
如果我们检查每个模式，并将其发生的概率乘以衡量其 

136
00:07:40,417 --> 00:07:44,720
信息量的因素，这也许可以给我们一个客观的分数。

137
00:07:45,960 --> 00:07:49,840
现在，您对某事物应该是什么的第一直觉可能是匹配的数量。

138
00:07:50,160 --> 00:07:52,400
您想要较低的平均匹配数。

139
00:07:52,800 --> 00:07:55,192
但相反，我想使用一种更通用的衡量标准，

140
00:07:55,192 --> 00:07:57,963
我们通常将其归因于信息 ，并且一旦我们为这 

141
00:07:57,963 --> 00:08:00,607
13,000 个单词中的每一个分配不同的 

142
00:08:00,607 --> 00:08:04,260
概率来判断它们是否实际上是答案，这种衡量标准就会更加灵活。

143
00:08:10,320 --> 00:08:13,920
信息的标准单位是位，它的公式有点有趣 ，

144
00:08:13,920 --> 00:08:16,980
但如果我们只看例子，它真的很直观。

145
00:08:17,780 --> 00:08:21,538
如果你的观察结果将你的可能性空间减 少了一半，

146
00:08:21,538 --> 00:08:23,500
我们就说它只有一点信息。

147
00:08:24,180 --> 00:08:27,533
在我们的示例中，可能性空间是所有可能的单词，结果表明，

148
00:08:27,533 --> 00:08:31,260
五 个字母的单词中大约一半有 S，比这个少一点，但大约一半。

149
00:08:31,780 --> 00:08:34,320
这样观察就会给你一点信息。

150
00:08:34,880 --> 00:08:39,358
相反，如果一个新事实将可能性空间减 少了四倍，

151
00:08:39,358 --> 00:08:41,500
我们就说它有两位信息。

152
00:08:41,980 --> 00:08:44,460
例如，事实证明这些单词中大约四分之一有 T。

153
00:08:45,020 --> 00:08:49,829
如果观察将该空间缩小八分之一，我 们就说它是三位信息，

154
00:08:49,829 --> 00:08:50,720
依此类推。

155
00:08:50,900 --> 00:08:55,060
四位将其切割为 16 度，五位将其切割为 32 度。

156
00:08:55,060 --> 00:09:00,340
所以现在您可能想停下来问自己，就发生 概率而言，

157
00:09:00,340 --> 00:09:02,980
比特数的信息公式是什么？

158
00:09:03,920 --> 00:09:08,127
我们在这里所说的是，当你取位数的二分之一 时，

159
00:09:08,127 --> 00:09:13,066
这与概率是一样的，这与说 2 的位数 次方等于概率的 

160
00:09:13,066 --> 00:09:18,005
1 是一样的，重新排列进 一步表示该信息是一的对数基数

161
00:09:18,005 --> 00:09:18,920
除以概率。

162
00:09:19,620 --> 00:09:22,762
有时您还会看到这种情况，还需要进行一次重新 排列，

163
00:09:22,762 --> 00:09:24,900
其中信息是以概率的负对数为底的二。

164
00:09:25,660 --> 00:09:28,736
对于外行来说，这样表达可能有点奇怪 ，

165
00:09:28,736 --> 00:09:31,489
但这确实是一个非常直观的想法，即 

166
00:09:31,489 --> 00:09:34,080
询问您已将可能性减少一半的次数。

167
00:09:35,180 --> 00:09:37,287
现在，如果您想知道，您知道，我以为我们只是 

168
00:09:37,287 --> 00:09:39,300
在玩一个有趣的文字游戏，为什么要使用对数？

169
00:09:39,780 --> 00:09:42,587
这是一个更好的单元的原因之一是，

170
00:09:42,587 --> 00:09:45,570
谈论非常不可能 的事件要容易得多，

171
00:09:45,570 --> 00:09:50,308
说一个观察有 20 位信息比 说这样那样发生的概率为 

172
00:09:50,308 --> 00:09:52,940
0 容易得多。0000095。

173
00:09:53,300 --> 00:09:58,275
但这种对数表达式被证明是对概率论非常有用 的补充，

174
00:09:58,275 --> 00:10:01,460
更实质性的原因是信息相加的方式。

175
00:10:02,060 --> 00:10:05,024
例如，如果一个观察结果为您提供了两位信息，

176
00:10:05,024 --> 00:10:08,976
将您的空间 缩小了四分之二，然后第二个观察结果（如您在 

177
00:10:08,976 --> 00:10:12,928
Wor dle 中的第二次猜测）为您提供了另外三位信息，

178
00:10:12,928 --> 00:10:16,740
将 您的空间进一步缩小了八倍，则两个一起给你五位信息。

179
00:10:17,160 --> 00:10:21,020
就像概率喜欢乘法一样，信息喜欢增加。

180
00:10:21,960 --> 00:10:24,602
因此，一旦我们处于预期值之类的领域，

181
00:10:24,602 --> 00:10:27,980
我们 将一堆数字相加，日志就会使其更容易处理。

182
00:10:28,480 --> 00:10:30,816
让我们回到 Weary 的发行版，

183
00:10:30,816 --> 00:10:34,940
并在此处添加 另一个小跟踪器，向我们展示每种模式有多少信息。

184
00:10:35,580 --> 00:10:39,256
我希望您注意的主要事情是，我们获得这些更有可能 

185
00:10:39,256 --> 00:10:42,780
的模式的概率越高，信息越少，您获得的位就越少。

186
00:10:43,500 --> 00:10:47,561
我们衡量这种猜测质量的方法是获取该信息的期 望值，

187
00:10:47,561 --> 00:10:51,135
我们遍历每个模式，我们说它的可能性有 多大，

188
00:10:51,135 --> 00:10:54,060
然后我们将其乘以我们获得的信息位数。

189
00:10:54,710 --> 00:10:58,120
在 Weary 的例子中，结果是 4。9 位。

190
00:10:58,560 --> 00:11:02,096
因此，平均而言，您从这个开局猜测中获得的信息 

191
00:11:02,096 --> 00:11:05,480
相当于将您的可能性空间切成两半（大约五倍）。

192
00:11:05,960 --> 00:11:08,891
相比之下，具有较高预期信息值的 

193
00:11:08,891 --> 00:11:11,640
猜测的示例类似于 Slate。

194
00:11:13,120 --> 00:11:15,620
在这种情况下，您会注意到分布看起来更加平坦。

195
00:11:15,940 --> 00:11:21,659
特别是，所有灰色中最有可能出现的概率只有 6% 左右，

196
00:11:21,659 --> 00:11:25,260
因此至少明显会得到 3。9位信息。

197
00:11:25,920 --> 00:11:28,560
但这是最低限度，更常见的是你会得到比这更好的东西。

198
00:11:29,100 --> 00:11:33,807
事实证明，当你计算这个数字并将所有相 关术语加起来时，

199
00:11:33,807 --> 00:11:35,900
平均信息约为 5。8. 

200
00:11:37,360 --> 00:11:40,028
因此，与《Weary》相比，平均而言，

201
00:11:40,028 --> 00:11:43,540
在第一 次猜测之后，你的可能性空间大约只有一半大。

202
00:11:44,420 --> 00:11:49,120
关于信息量期望值的名称，实际上有一个有趣的故事。

203
00:11:49,200 --> 00:11:52,072
信息论是由 20 世纪 40 年代在贝尔实验室工作的克劳

204
00:11:52,072 --> 00:11:54,841
德·香农 (C laude Shannon) 提出的，

205
00:11:54,841 --> 00:11:57,405
但他正在与约翰·冯·诺依曼 ( John von 

206
00:11:57,405 --> 00:11:59,662
Neumann) 谈论他尚未发表的一些想法，

207
00:11:59,662 --> 00:12:01,918
约翰· 冯·诺依曼是当时非常杰出的知识巨人。

208
00:12:01,918 --> 00:12:03,560
数学和物理以及计算机科学的开端。

209
00:12:04,100 --> 00:12:09,330
当冯·诺依曼提到他对于信息量的期望值 并没有一个好名字时，

210
00:12:09,330 --> 00:12:14,200
据说，所以故事是 这样的，你应该称之为熵，有两个原因。

211
00:12:14,540 --> 00:12:19,142
首先，你的不确定性函数已经在统计力学中以这个名字使 用了，

212
00:12:19,142 --> 00:12:22,475
所以它已经有一个名字了，其次，更重要的是，

213
00:12:22,475 --> 00:12:26,760
没 有人知道熵到底是什么，所以在辩论中你总是会有优势。

214
00:12:27,700 --> 00:12:29,774
因此，如果这个名字看起来有点神秘，

215
00:12:29,774 --> 00:12:32,460
并且 如果这个故事可信的话，那就是有意为之。

216
00:12:33,280 --> 00:12:37,960
另外，如果您想知道它与物理学中所有热力学 第二定律的关系，

217
00:12:37,960 --> 00:12:42,801
那么肯定存在某种联系， 但在其起源中，香农只是处理纯概率论，

218
00:12:42,801 --> 00:12:46,029
并且 出于我们的目的，当我使用熵这个词，

219
00:12:46,029 --> 00:12:49,580
我只 是想让你思考一个特定猜测的预期信息值。

220
00:12:50,700 --> 00:12:53,780
您可以将熵视为同时测量两个事物。

221
00:12:54,240 --> 00:12:56,780
第一个是分布的平坦程度。

222
00:12:57,320 --> 00:13:01,120
分布越接近均匀，熵就越高。

223
00:13:01,580 --> 00:13:06,820
在我们的例子中，总共有 3 到 5 个模式，对于均匀分布，

224
00:13:06,820 --> 00:13:12,240
观察其 中任何一个模式都会有 3 到 5 个的信息对数基数 

225
00:13:12,240 --> 00:13:17,300
2，恰好是 7。92，所以这是该熵可能具有的绝对最大值。

226
00:13:17,840 --> 00:13:22,080
但熵首先也是一种衡 量可能性的方法。

227
00:13:22,320 --> 00:13:27,007
例如，如果您碰巧有某个单词，其中只有 16 种可能的模式，

228
00:13:27,007 --> 00:13:31,533
并 且每种模式的可能性相同，则该熵（即该预期信息）将是 

229
00:13:31,533 --> 00:13:32,180
4 位。

230
00:13:32,579 --> 00:13:37,094
但如果你有另一个词，其中可能出现 64 种可能的 模式，

231
00:13:37,094 --> 00:13:40,480
并且它们的可能性相同，那么熵将是 6 位。

232
00:13:41,500 --> 00:13:45,280
因此，如果您在野外看到某个分布的熵为 6 位，

233
00:13:45,280 --> 00:13:49,719
这 就有点像是在说即将发生的事情存在同样多的变化和不 

234
00:13:49,719 --> 00:13:53,500
确定性，就好像有 64 个同样可能的结果一样。

235
00:13:54,360 --> 00:13:57,460
对于我第一次使用 Wurtelebot，

236
00:13:57,460 --> 00:13:59,320
我基本上就是让它这样做。

237
00:13:59,320 --> 00:14:03,982
它会遍历所有可能的猜测，即所有 13,000 个单 词，

238
00:14:03,982 --> 00:14:06,814
计算每个单词的熵，或者更具体地说，

239
00:14:06,814 --> 00:14:10,644
计算您可能 看到的所有模式中每个单词的分布熵，

240
00:14:10,644 --> 00:14:14,807
并选择最高的，因 为这是一个可能会尽可能地削减你的

241
00:14:14,807 --> 00:14:16,140
可能性空间的人。

242
00:14:17,140 --> 00:14:18,764
尽管我在这里只讨论了第一个猜测，

243
00:14:18,764 --> 00:14:21,100
但它对 于接下来的几次猜测也起到了同样的作用。

244
00:14:21,560 --> 00:14:24,421
例如，在您看到第一个猜测的某些模式后，

245
00:14:24,421 --> 00:14:28,637
这将根 据与之匹配的内容将您限制为较少数量的可能单词 ，

246
00:14:28,637 --> 00:14:31,800
您只需针对该较小的单词集玩相同的游戏即可。

247
00:14:32,260 --> 00:14:36,174
对于建议的第二个猜测，您会查看从一组更受限制的 

248
00:14:36,174 --> 00:14:38,783
单词中可能出现的所有模式的分布，

249
00:14:38,783 --> 00:14:41,719
搜索所有 13 ,000 种可能性，

250
00:14:41,719 --> 00:14:43,840
然后找到使熵最大化的一种。

251
00:14:45,420 --> 00:14:47,808
为了向您展示这是如何实际工作的，

252
00:14:47,808 --> 00:14:51,541
让我拿出我编写的 Wurt ele 的一个小变体，

253
00:14:51,541 --> 00:14:54,080
它在页边空白处显示了此分析的亮点。

254
00:14:54,080 --> 00:14:59,660
完成所有熵计算后，右侧向我们展 示了哪些具有最高的预期信息。

255
00:15:00,280 --> 00:15:03,904
事实证明，至少目前最重要的答案是稗子，

256
00:15:03,904 --> 00:15:09,054
我们稍后会对此进 行完善，这意味着，嗯，当然是野豌豆，

257
00:15:09,054 --> 00:15:10,580
最常见的野豌豆。

258
00:15:11,040 --> 00:15:14,547
每次我们在这里进行猜测时，也许我会忽略它的建议并选择 

259
00:15:14,547 --> 00:15:16,885
slate，因为我喜欢 slate，

260
00:15:16,885 --> 00:15:20,133
我们可以看到 它有多少预期信息，但在这个词的右侧，

261
00:15:20,133 --> 00:15:23,120
它向我们展示了 多少信息考虑到这种特定的模式，

262
00:15:23,120 --> 00:15:24,420
我们得到的实际信息。

263
00:15:25,000 --> 00:15:27,976
所以看起来我们有点不走运，我们预计会得到 5 个。

264
00:15:27,976 --> 00:15:30,120
8，但 我们碰巧得到的东西比这个少。

265
00:15:30,600 --> 00:15:32,881
然后在左侧，它向我们展示了当前 

266
00:15:32,881 --> 00:15:35,020
所处位置的所有不同可能的单词。

267
00:15:35,800 --> 00:15:38,928
蓝色条告诉我们它认为每个单词出现的可能性有多大，

268
00:15:38,928 --> 00:15:41,665
因此目前它 假设每个单词出现的可能性相同，

269
00:15:41,665 --> 00:15:43,360
但我们稍后会对其进行改进。

270
00:15:44,060 --> 00:15:49,018
然后，这种不确定性测量告诉我们可能单词的 分布熵，

271
00:15:49,018 --> 00:15:52,191
因为它是均匀分布，所以现在只是 

272
00:15:52,191 --> 00:15:55,960
一种计算可能性数量的不必要的复杂方法。

273
00:15:56,560 --> 00:15:59,242
例如，如果我们要计算 2 的 13 次方。

274
00:15:59,242 --> 00:16:02,180
66，这应该是 大约 13,000 种可能性。

275
00:16:02,900 --> 00:16:06,140
我在这里有点偏离，但这只是因为我没有显示所有小数位。

276
00:16:06,720 --> 00:16:09,295
目前，这可能感觉多余，而且好像事情过于复杂，

277
00:16:09,295 --> 00:16:12,340
但您 很快就会明白为什么同时拥有这两个数字是有用的。

278
00:16:12,760 --> 00:16:17,555
所以这里看起来它表明我们第二个猜测的 最高熵是拉面，

279
00:16:17,555 --> 00:16:19,400
这又感觉不像一个词。

280
00:16:19,980 --> 00:16:24,060
因此，为了占据道德制高点，我将继续输入 Rains。

281
00:16:25,440 --> 00:16:27,340
看来我们又有点不走运了。

282
00:16:27,520 --> 00:16:31,360
我们本来期待4。3 位，但我们只得到了 3 位。39位信息。

283
00:16:31,940 --> 00:16:33,940
这样一来，我们就有 55 种可能性。

284
00:16:34,900 --> 00:16:37,595
在这里，也许我实际上会遵循它的建 议，

285
00:16:37,595 --> 00:16:39,440
即组合，无论这意味着什么。

286
00:16:40,040 --> 00:16:42,920
好吧，这实际上是一个解谜的好机会。

287
00:16:42,920 --> 00:16:46,380
它告诉我们这个模式给了我们 4。7 位信息。

288
00:16:47,060 --> 00:16:49,465
但在左边，在我们看到该模式之前，

289
00:16:49,465 --> 00:16:51,720
有 5 个。78 位不确定性。

290
00:16:52,420 --> 00:16:56,340
那么作为对你的一个测验，剩余可能性的数量意味着什么？

291
00:16:58,040 --> 00:17:01,729
嗯，这意味着我们将不确定性减少到一点 点，

292
00:17:01,729 --> 00:17:04,540
这与说有两个可能的答案是一样的。

293
00:17:04,700 --> 00:17:05,700
这是50-50的选择。

294
00:17:06,500 --> 00:17:08,983
从这里开始，因为你和我知道哪些词更 常见，

295
00:17:08,983 --> 00:17:10,640
所以我们知道答案应该是深渊。

296
00:17:11,180 --> 00:17:13,280
但正如现在所写的，程序并不知道这一点。

297
00:17:13,540 --> 00:17:16,699
所以它会继续前进，尝试获取尽可能多的信息，

298
00:17:16,699 --> 00:17:19,859
 直到只剩下一种可能性，然后它就会猜测它。

299
00:17:20,380 --> 00:17:22,339
显然我们需要更好的残局策略。

300
00:17:22,599 --> 00:17:25,538
但是，假设我们将此版本称为我们的 wordle 求解 

301
00:17:25,538 --> 00:17:28,260
器之一，然后我们运行一些模拟来看看它是如何工作的。

302
00:17:30,360 --> 00:17:34,120
所以它的工作方式是玩所有可能的文字游戏。

303
00:17:34,240 --> 00:17:38,540
它会检查所有 2315 个单词，这些单词是实际的单词答案。

304
00:17:38,540 --> 00:17:40,580
它基本上使用它作为测试集。

305
00:17:41,360 --> 00:17:45,158
采用这种天真的方法，不考虑一个词的常见程度，

306
00:17:45,158 --> 00:17:49,820
只是 试图在每一步中最大化信息，直到它只剩下一个选择。

307
00:17:50,360 --> 00:17:54,300
模拟结束时，平均得分约为 4。124. 

308
00:17:55,319 --> 00:17:59,240
老实说，这还不错，我本来以为会做得更糟。

309
00:17:59,660 --> 00:18:02,600
但玩wordle的人会告诉你，他们通常可以在4内得到它。

310
00:18:02,860 --> 00:18:05,380
真正的挑战是尽可能多地获得三分。

311
00:18:05,380 --> 00:18:08,080
4分和3分之间的差距相当大。

312
00:18:08,860 --> 00:18:12,053
这里显而易见的容易实现的目标是以某种方式纳入一 

313
00:18:12,053 --> 00:18:14,980
个单词是否常见，以及我们到底如何做到这一点。

314
00:18:22,800 --> 00:18:27,880
我的方法是获取英语中所 有单词的相对频率列表。

315
00:18:28,220 --> 00:18:30,745
我刚刚使用了 Mathematica 的词频数据函数，

316
00:18:30,745 --> 00:18:32,615
它本身是从 Go ogle Books 

317
00:18:32,615 --> 00:18:34,860
English Ngram 公共数据集中提取的。

318
00:18:35,460 --> 00:18:37,770
看起来很有趣，例如，如果我们将其从最 

319
00:18:37,770 --> 00:18:39,960
常见的单词到最不常见的单词进行排序。

320
00:18:40,120 --> 00:18:43,080
显然，这些是英语中最常见的 5 个字母单词。

321
00:18:43,700 --> 00:18:45,840
或者更确切地说，这些是第八个最常见的。

322
00:18:46,280 --> 00:18:48,880
首先是which，然后是there和there。

323
00:18:49,260 --> 00:18:51,769
First本身不是first，而是9th，

324
00:18:51,769 --> 00:18:54,397
并且这些其 他词可能更频繁地出现是有道理的，

325
00:18:54,397 --> 00:18:57,385
其中first之后 的词是after、where，

326
00:18:57,385 --> 00:18:58,580
而那些词则不太常见。

327
00:18:59,160 --> 00:19:04,532
现在，在使用这些数据来模拟每个单词成为最终 答案的可能性时，

328
00:19:04,532 --> 00:19:06,860
它不应该仅仅与频率成正比。

329
00:19:06,860 --> 00:19:10,075
例如，得分为 0。002 在此数据集中，

330
00:19:10,075 --> 00:19:13,934
而“br aid”一词在某种意义上的可能性要小 

331
00:19:13,934 --> 00:19:15,060
1000 倍。

332
00:19:15,560 --> 00:19:18,840
但这两个词都很常见，几乎肯定值得考虑。

333
00:19:19,340 --> 00:19:21,000
所以我们想要更多的二元截止。

334
00:19:21,860 --> 00:19:25,013
我的方法是想象一下将整个排序的单词列表，

335
00:19:25,013 --> 00:19:29,744
然后将其排列 在 x 轴上，然后应用 sigmoid 函数，

336
00:19:29,744 --> 00:19:34,475
这是 输出基本上是二进制的函数的标准方法，它是要么是 0 ，

337
00:19:34,475 --> 00:19:38,260
要么是 1，但对于该不确定区域，中间有一个平滑。

338
00:19:39,160 --> 00:19:43,965
所以本质上，我分配给每个单词出现在最终列表中的概率将是上 

339
00:19:43,965 --> 00:19:48,440
面的 sigmoid 函数的值，无论它位于 x 轴上。

340
00:19:49,520 --> 00:19:54,262
显然，这取决于几个参数，例如，这些单词在 x 轴上填充 

341
00:19:54,262 --> 00:19:59,344
的空间有多宽，决定了我们从 1 到 0 的逐渐或陡峭 程度，

342
00:19:59,344 --> 00:20:03,240
以及我们将它们从左到右放置的位置决定了截止值。

343
00:20:03,240 --> 00:20:06,920
说实话，我的做法就是舔手指然后把它插到风里。

344
00:20:07,140 --> 00:20:12,031
我查看了排序后的列表，并试图找到一个窗口 ，当我查看它时，

345
00:20:12,031 --> 00:20:15,742
我认为这些单词中的一半 更有可能是最终答案，

346
00:20:15,742 --> 00:20:17,260
并将其用作截止值。

347
00:20:17,260 --> 00:20:19,699
一旦我们在单词之间有了这样的分布，

348
00:20:19,699 --> 00:20:23,860
它就会给我们 带来另一种情况，即熵成为这种真正有用的度量。

349
00:20:24,500 --> 00:20:28,647
例如，假设我们正在玩一个游戏，我们从我 的旧开场白开始，

350
00:20:28,647 --> 00:20:32,943
即羽毛和指甲，我们最终 会遇到有四个可能的单词与之匹配的情

351
00:20:32,943 --> 00:20:33,240
况。

352
00:20:33,560 --> 00:20:35,620
假设我们认为它们都有相同的可能性。

353
00:20:36,220 --> 00:20:38,880
我问你，这个分布的熵是多少？

354
00:20:41,080 --> 00:20:45,394
嗯，与这些可能性中的每一种相关的信息将是 4 的以 

355
00:20:45,394 --> 00:20:50,040
2 为底的对数，因为每一种都是 1 和 4，那就是 2。

356
00:20:50,040 --> 00:20:52,460
两位信息，四种可能性。

357
00:20:52,760 --> 00:20:53,580
一切都很好。

358
00:20:54,300 --> 00:20:57,800
但如果我告诉你实际上有超过四场比赛呢？

359
00:20:58,260 --> 00:21:00,698
事实上，当我们查看完整的单词列表时，

360
00:21:00,698 --> 00:21:02,460
有 16 个单词与其匹配。

361
00:21:02,580 --> 00:21:06,827
但假设我们的模型对其他 12 个单词实际成为最终答案 

362
00:21:06,827 --> 00:21:10,760
的概率非常低，大约是千分之一，因为它们真的很晦涩。

363
00:21:11,500 --> 00:21:14,260
现在我问你，这个分布的熵是多少？

364
00:21:15,420 --> 00:21:17,990
如果熵在这里纯粹测量匹配的数量，

365
00:21:17,990 --> 00:21:22,648
那么您可能 会期望它类似于 16 的以 2 为底的对数 ，

366
00:21:22,648 --> 00:21:25,700
即 4，比我们之前的不确定性多了两位。

367
00:21:26,180 --> 00:21:29,860
但当然，实际的不确定性与我们之前的情况并没有太大不同。

368
00:21:30,160 --> 00:21:33,910
例如，仅仅因为有这 12 个非常晦涩的单词并不意 

369
00:21:33,910 --> 00:21:37,360
味着当得知最终答案是“魅力”时会更加令人惊讶。

370
00:21:38,180 --> 00:21:41,791
所以当你在这里实际进行计算时，将每次出现的概 

371
00:21:41,791 --> 00:21:45,560
率乘以相应的信息相加，得到的就是 2。11 位。

372
00:21:45,560 --> 00:21:48,926
我只是说，它基本上是两位，基本上是这四种可能性，

373
00:21:48,926 --> 00:21:53,133
但是 由于所有这些极不可能发生的事件，存在更多的不确定性 ，

374
00:21:53,133 --> 00:21:56,500
尽管如果你确实了解了它们，你会从中获得大量信息。

375
00:21:57,160 --> 00:21:59,391
缩小范围，这就是 Wordle 成为信 

376
00:21:59,391 --> 00:22:01,400
息论课程的一个很好的例子的部分原因。

377
00:22:01,600 --> 00:22:04,640
我们对熵有两种不同的感觉应用。

378
00:22:05,160 --> 00:22:09,777
第一个告诉我们从给定的猜测中得到的预期 信息是什么，

379
00:22:09,777 --> 00:22:14,927
第二个告诉我们我们是否可以 衡量所有可能的单词中剩余的不确

380
00:22:14,927 --> 00:22:15,460
定性。

381
00:22:16,460 --> 00:22:21,054
我应该强调，在第一种情况下，我们正在查看猜测的预期 信息，

382
00:22:21,054 --> 00:22:24,540
一旦我们对单词的权重不相等，就会影响熵计算。

383
00:22:24,980 --> 00:22:28,634
例如，让我拿出我们之前查看的与 We ary 

384
00:22:28,634 --> 00:22:32,925
相关的分布的相同案例，但这次 在所有可能的单词中使用非

385
00:22:32,925 --> 00:22:33,720
均匀分布。

386
00:22:34,500 --> 00:22:38,280
所以让我看看是否可以在这里找到一个很好地说明它的部分。

387
00:22:40,940 --> 00:22:42,360
好吧，这里这很好。

388
00:22:42,360 --> 00:22:45,412
这里我们有两个相邻的模式，它们的可能性大致相同，

389
00:22:45,412 --> 00:22:49,100
但我 们被告知其中一个模式有 32 个可能的单词与其匹配。

390
00:22:49,280 --> 00:22:52,296
如果我们检查它们是什么，那就是 32 个，

391
00:22:52,296 --> 00:22:55,600
当 你扫视它们时，它们都只是非常不可能的单词。

392
00:22:55,840 --> 00:22:59,221
很难找到任何看似合理的答案，也许会大喊大叫，

393
00:22:59,221 --> 00:23:03,679
 但如果我们查看分布中的相邻模式，这被认为是 同样可能的，

394
00:23:03,679 --> 00:23:08,290
我们被告知它只有 8 个可能的匹 配，所以四分之一很多比赛，

395
00:23:08,290 --> 00:23:09,520
但可能性差不多。

396
00:23:09,860 --> 00:23:12,140
当我们拿出这些匹配项时，我们就能明白原因了。

397
00:23:12,500 --> 00:23:16,300
其中一些是实际合理的答案，例如戒指、愤怒或说唱。

398
00:23:17,900 --> 00:23:20,637
为了说明我们如何整合所有这些，让我在这里列出 

399
00:23:20,637 --> 00:23:22,780
Wordlebo t 的第 2 版，

400
00:23:22,780 --> 00:23:25,280
它与我们看到的第一个版本有两三个主要区别。

401
00:23:25,860 --> 00:23:30,109
首先，就像我刚才说的，我们计算这些熵、这些信 

402
00:23:30,109 --> 00:23:34,914
息的预期值的方式现在正在使用跨模式的更精细的 分布，

403
00:23:34,914 --> 00:23:38,240
其中包含给定单词实际上是答案的概率。

404
00:23:38,879 --> 00:23:43,820
碰巧，眼泪仍然是第一位，尽管接下来的有点不同。

405
00:23:44,360 --> 00:23:47,981
其次，当它对首选进行排名时，它现在将保留每个单词 

406
00:23:47,981 --> 00:23:51,023
是实际答案的概率模型，并将其纳入其决策中，

407
00:23:51,023 --> 00:23:55,080
一旦我 们对答案有了一些猜测，就更容易看到这一点。桌子。

408
00:23:55,860 --> 00:23:59,780
再次忽略它的建议，因为我们不能让机器统治我们的生活。

409
00:24:01,140 --> 00:24:04,835
我想我应该提到另一件不同的事情是在左边，

410
00:24:04,835 --> 00:24:09,640
不确定 性值，即位数，不再只是与可能匹配的数量冗余。

411
00:24:10,080 --> 00:24:14,400
现在，如果我们将其拉高并计算 2 的 8。02，

412
00:24:14,400 --> 00:24:18,540
略高于 256，我 猜是 259，它的意思是，

413
00:24:18,540 --> 00:24:22,679
尽管总共有 526 个单词实际上与此模式匹配，

414
00:24:22,679 --> 00:24:27,000
但它所具有的不确定性量 更类似于如果有 259 

415
00:24:27,000 --> 00:24:28,980
个同样可能的单词结果。

416
00:24:29,720 --> 00:24:30,740
你可以这样想。

417
00:24:31,020 --> 00:24:34,174
它知道 borx 不是答案，与 yorts、zorl 

418
00:24:34,174 --> 00:24:37,680
和 zorus 一样，因此它的不确定性比之前的情况要小一些。

419
00:24:37,820 --> 00:24:39,280
这个位数会更小。

420
00:24:40,220 --> 00:24:43,457
如果我继续玩这个游戏，我会用一些与我想在 

421
00:24:43,457 --> 00:24:46,540
这里解释的内容相符的猜测来完善这个游戏。

422
00:24:48,360 --> 00:24:51,536
通过第四种猜测，如果你看一下它的首 选，

423
00:24:51,536 --> 00:24:53,760
你会发现它不再只是最大化熵。

424
00:24:54,460 --> 00:24:57,630
所以在这一点上，技术上有七种可能性 ，

425
00:24:57,630 --> 00:25:00,300
但唯一有意义的机会是宿舍和单词。

426
00:25:00,300 --> 00:25:04,415
您可以看到它对选择这两个值的排名高于所 有其他值，

427
00:25:04,415 --> 00:25:06,720
严格来说，这会提供更多信息。

428
00:25:07,240 --> 00:25:10,721
我第一次这样做时，我只是将这两个数字相加来衡 

429
00:25:10,721 --> 00:25:13,900
量每个猜测的质量，这实际上比你想象的要好。

430
00:25:14,300 --> 00:25:17,977
但它确实感觉不系统，而且我确信人们可 以采取其他方法，

431
00:25:17,977 --> 00:25:19,340
但这是我找到的方法。

432
00:25:19,760 --> 00:25:23,830
如果我们正在考虑下一次猜测的前景，就像在这种情况下的话，

433
00:25:23,830 --> 00:25:27,900
 我们真正关心的是如果我们这样做的话我们游戏的预期得分。

434
00:25:28,230 --> 00:25:33,933
为了计算预期分数，我们会计算单词是实际 答案的概率是多少，

435
00:25:33,933 --> 00:25:35,900
目前描述为 58%。

436
00:25:36,040 --> 00:25:39,540
我们说，有 58% 的机会，我们在这场比赛中得分为 4。

437
00:25:40,320 --> 00:25:45,640
然后，以 1 减去 58% 的概率，我们的分数将大于 4。

438
00:25:46,220 --> 00:25:49,424
我们不知道还有多少，但我们可以根据到 

439
00:25:49,424 --> 00:25:52,460
达这一点后可能存在的不确定性来估计。

440
00:25:52,960 --> 00:25:55,940
具体来说，目前有1。44 位不确定性。

441
00:25:56,440 --> 00:26:00,365
如果我们猜测单词，它会告诉我们预期得到的信息是 1。

442
00:26:00,365 --> 00:26:01,120
27 位。

443
00:26:01,620 --> 00:26:04,783
因此，如果我们猜测单词，这种差异代表了在这 

444
00:26:04,783 --> 00:26:07,660
种情况发生后我们可能会留下多少不确定性。

445
00:26:08,260 --> 00:26:11,274
我们需要的是某种函数，我在这里称之为 f ，

446
00:26:11,274 --> 00:26:13,740
它将这种不确定性与预期分数联系起来。

447
00:26:14,240 --> 00:26:18,328
它的处理方式是根据机器人的版本 1 绘制之 

448
00:26:18,328 --> 00:26:22,417
前游戏的一堆数据，以说明在具有某些非常可测 

449
00:26:22,417 --> 00:26:26,320
量的不确定性的各个点之后的实际得分是多少。

450
00:26:27,020 --> 00:26:31,180
例如，这里的这些数据点位于 8 左右的值之上。

451
00:26:31,180 --> 00:26:34,980
在 8分之后的一些比赛中，大约有7分左右。

452
00:26:34,980 --> 00:26:38,960
7位不确定 性，经过两次猜测才得到最终答案。

453
00:26:39,320 --> 00:26:42,240
对于其他游戏需要猜测三次，对于其他游戏需要猜测四次。

454
00:26:43,140 --> 00:26:46,742
如果我们在这里向左移动，所有超过零的点都表明，

455
00:26:46,742 --> 00:26:50,344
 每当不确定性为零时，也就是说只有一种可能性，

456
00:26:50,344 --> 00:26:54,260
那 么所需的猜测次数总是只有一次，这是令人放心的。

457
00:26:54,780 --> 00:26:58,747
每当有一点不确定性时，意味着基本上只 有两种可能性，

458
00:26:58,747 --> 00:27:03,020
那么有时需要再进行一 次猜测，有时则需要再进行两次猜测。

459
00:27:03,080 --> 00:27:05,240
这里等等等等。

460
00:27:05,740 --> 00:27:07,980
也许可视化这些数据的一种稍微简单

461
00:27:07,980 --> 00:27:10,220
的方法是将其放在一起并取平均值。

462
00:27:11,000 --> 00:27:16,528
例如，这里的这个条表示，在我们有一点不确定性 的所有点中，

463
00:27:16,528 --> 00:27:19,960
平均所需的新猜测数量约为 1。5. 

464
00:27:22,140 --> 00:27:24,891
这里的栏表示在所有不同的游戏中，

465
00:27:24,891 --> 00:27:27,986
在某些 时候不确定性略高于 4 位，

466
00:27:27,986 --> 00:27:31,597
这就像将 其缩小到 16 种不同的可能性，

467
00:27:31,597 --> 00:27:35,380
然后从 该点开始平均需要两个以上的猜测向前。

468
00:27:36,060 --> 00:27:39,460
从这里开始，我只是做了一个回归来拟合一个看起来合理的函数。

469
00:27:39,980 --> 00:27:44,829
请记住，这样做的全部目的是为了我们可以量化这种直觉 ，

470
00:27:44,829 --> 00:27:48,960
即我们从单词中获得的信息越多，预期得分就越低。

471
00:27:49,680 --> 00:27:54,460
所以将此作为版本 2。0，如果我们返回并运行相同的一组模拟，

472
00:27:54,460 --> 00:27:59,240
让它 与所有 2315 个可能的单词答案进行比较，结果如何？

473
00:28:00,280 --> 00:28:03,420
与我们的第一个版本相比，它肯定更好，这令人放心。

474
00:28:04,020 --> 00:28:08,232
综上所述，平均值约为 3。6，尽管与第一个版本不同，

475
00:28:08,232 --> 00:28:12,120
 它有几次丢失并且在这种情况下需要超过 6 个。

476
00:28:12,639 --> 00:28:16,401
大概是因为有时需要进行权衡以实 际实现目标，

477
00:28:16,401 --> 00:28:17,940
而不是最大化信息。

478
00:28:19,040 --> 00:28:21,000
那么我们可以做得比 3 更好吗？6？

479
00:28:22,080 --> 00:28:22,920
我们绝对可以。

480
00:28:23,280 --> 00:28:26,464
现在我在一开始就说过，尝试不将单词答案的真 

481
00:28:26,464 --> 00:28:29,360
实列表合并到构建模型的方式中是最有趣的。

482
00:28:29,880 --> 00:28:33,625
但如果我们确实将其合并，我可以获得的最佳性能约为 3。

483
00:28:33,625 --> 00:28:34,180
43. 

484
00:28:35,160 --> 00:28:39,449
因此，如果我们尝试比仅使用词频数据更复杂地选择此先验分 布，

485
00:28:39,449 --> 00:28:43,595
则这 3.43 可能给出了我们可以做到什么 程度的最大值，

486
00:28:43,595 --> 00:28:45,740
或者至少是我可以做到什么程度。

487
00:28:46,240 --> 00:28:50,167
最佳性能本质上只是使用了我在这里讨 论的想法，

488
00:28:50,167 --> 00:28:55,120
但它更进一步，就像它向前 两步而不是一步搜索预期信息一样。

489
00:28:55,620 --> 00:28:57,982
本来我打算更多地讨论这个问题，但我意 

490
00:28:57,982 --> 00:29:00,220
识到我们实际上已经讨论了很长时间了。

491
00:29:00,580 --> 00:29:02,770
我要说的一件事是，在进行了两步搜索，

492
00:29:02,770 --> 00:29:05,448
然后在顶级 候选者中运行了几个样本模拟之后，

493
00:29:05,448 --> 00:29:09,100
至少到目前为止 对我来说，Crane 看起来是最好的开局者。

494
00:29:09,100 --> 00:29:10,060
谁能想到呢？

495
00:29:10,920 --> 00:29:14,883
此外，如果您使用真实的单词列表来确定您的可能性 空间，

496
00:29:14,883 --> 00:29:17,820
那么您开始的不确定性将略高于 11 位。

497
00:29:18,300 --> 00:29:22,192
事实证明，仅通过强力搜索，前两次猜测 

498
00:29:22,192 --> 00:29:25,880
后最大可能的预期信息约为 10 位。

499
00:29:26,500 --> 00:29:30,447
这表明，在最好的情况下，在您进行前两次猜测之后，

500
00:29:30,447 --> 00:29:34,560
 如果有完美的最佳玩法，您将留下大约一点不确定性。

501
00:29:34,800 --> 00:29:37,960
这与归结为两种可能的猜测相同。

502
00:29:37,960 --> 00:29:40,704
因此，我认为公平且可能相当保守地说，

503
00:29:40,704 --> 00:29:44,363
您永远不可能编 写一个使平均值低至 3 的算法，

504
00:29:44,363 --> 00:29:48,175
因为根据您可用的 单词，在仅执行两个步骤后根本没有

505
00:29:48,175 --> 00:29:52,597
空间获得足够的信息 。能够保证每次都在第三个槽中得到答案，

506
00:29:52,597 --> 00:29:53,360
不会失败。

