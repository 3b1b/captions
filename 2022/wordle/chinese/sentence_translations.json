[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy.",
  "translatedText": "Wurdle 游戏在过去一两个月里非常 火爆，从来没有人会忽视数学课的机会， 我觉得这个游戏是信息论课程中一个非常好 的中心例子，特别是一个称为熵的主题。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could.",
  "translatedText": "你看，就像很多人一样，我有点陷入了这个 谜题，而且像很多程序员一样，我也陷入了 尝试编写一种算法来尽可能最佳地玩游戏。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy.",
  "translatedText": "我想我在这里要做的只是与你们讨论我 的一些过程，并解释其中的一些数学 ，因为整个算法以熵的概念为中心。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle?",
  "translatedText": "首先，如果您还没有听说过，那么什么是 Wurdle？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us.",
  "translatedText": "在我们介绍游戏规则的同时，让我也 预览一下我们要做什么，即开发一个 基本上可以为我们玩游戏的小算法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does.",
  "translatedText": "虽然我还没有完成今天的 Wurdle，但现在 是 2 月 4 日，我们将看看机器人的表现。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess.",
  "translatedText": "Wurdle 的目标是猜测一个神秘的五 个字母单词，您有六次不同的猜测机会。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane.",
  "translatedText": "例如，我的 Wurdle 机器人建议我从猜测起重机开始。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer.",
  "translatedText": "每次您进行猜测时，您都会获得一些有关 您的猜测与真实答案的接近程度的信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer.",
  "translatedText": "灰色框告诉我实际答案中没有 C。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position.",
  "translatedText": "黄色框告诉我有一个 R，但它不在那个位置。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position.",
  "translatedText": "绿色框告诉我秘密词确实有一个 A，而且位于第三个位置。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E.",
  "translatedText": "然后就没有N了，也没有E了。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information.",
  "translatedText": "那么让我进去告诉 Wurdle 机器人该信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey.",
  "translatedText": "我们从起重机开始，有灰色、黄色、绿色、灰色、灰色。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time.",
  "translatedText": "不要担心它现在显示的所有数据，我会在适当的时候解释这一点。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick.",
  "translatedText": "但对于我们的第二个选择来说，它的首要建议是小技巧。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess.",
  "translatedText": "你的猜测确实必须是一个实际的五个字母的单词，但正 如你将看到的，它实际上让你猜测的内容相当自由。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick.",
  "translatedText": "在这种情况下，我们尝试一下shtic。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good.",
  "translatedText": "好吧，事情看起来相当不错。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R.",
  "translatedText": "我们按了 S 和 H，所以我们知道前三个字母，我们知道有一个 R。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something.",
  "translatedText": "所以它会像 SHA 某些 R 或 SHA R 某些东西。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp.",
  "translatedText": "看起来 Wurdle 机器人知道它只有 两种可能性，要么是碎片，要么是锋利的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a tossup between them at this point, so I guess probably just because it's alphabetical it goes with shard.",
  "translatedText": "在这一点上，它们之间的关系有点扑朔迷离，所以我想可能因为是按字母顺序排列的，所以就和 shard 放在一起了。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer, so we got it in three.",
  "translatedText": "万幸的是，这就是真正的答案，所以我们只用了三分钟。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 131.22,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle, four is par and three is birdie.",
  "translatedText": "如果你想知道这是否有什么好处，我听一个人说过，对于沃德尔来说，四杆就是标准杆，三杆就是小鸟。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy.",
  "translatedText": "我认为这是一个非常恰当的比喻。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy.",
  "translatedText": "你必须坚持自己的比赛才能获得四分，但这当然并不疯狂。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great.",
  "translatedText": "但当你拿到三分的时候，感觉棒极了。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot.",
  "translatedText": "因此，如果您愿意的话，我在这里想做的就是从一开始就 谈谈我如何处理 Wurdle 机器人的思考过程。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson.",
  "translatedText": "就像我说的，这确实是信息论课程的借口。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy.",
  "translatedText": "主要目标是解释什么是信息和什么是熵。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language.",
  "translatedText": "在解决这个问题时，我的第一个想法 是看看英语中不同字母的相对频率。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters?",
  "translatedText": "所以我想，好吧，是否有一个开局猜测或一 对开局猜测可以命中这些最常见的字母？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails.",
  "translatedText": "我非常喜欢做其他事情，然后做指甲。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good, it feels like you're getting information.",
  "translatedText": "我们的想法是，如果你击中了一个字母，你知道，你得到了绿色或黄色，那感觉总是很好，感觉你得到了信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 183.76,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information, since it's pretty rare to find a word that doesn't have any of these letters.",
  "translatedText": "但是，在这些情况下，即使你没有命中，而且总是得到灰色，这仍然会给你提供很多信息，因为没有这些字母的单词是非常罕见的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters.",
  "translatedText": "但即便如此，这仍然感觉不是超级系统 ，因为例如，它没有考虑字母的顺序。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail?",
  "translatedText": "当我可以输入蜗牛时，为什么还要输入指甲？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end?",
  "translatedText": "最后加个S是不是更好？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure.",
  "translatedText": "我不太确定。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y.",
  "translatedText": "现在，我的一个朋友说他喜欢用“weary”这个词开头，这让 我有点惊讶，因为里面有一些不常见的字母，比如 W 和 Y。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener.",
  "translatedText": "但谁知道呢，也许这是一个更好的开局。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess?",
  "translatedText": "我们是否可以给出某种定量评 分来判断潜在猜测的质量？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up.",
  "translatedText": "现在，为了设置我们对可能的猜测进行排名的 方式，让我们回顾一下游戏的具体设置方式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long.",
  "translatedText": "因此，您可以输入一个单词列表，这些单词被视为 有效的猜测，长度约为 13,000 个单词。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble.",
  "translatedText": "但当你仔细观察时，你会发现有很多非常不常见的东西，比如 头像、阿里和ARG，以及拼字游戏中引发家庭争吵的词语。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word.",
  "translatedText": "但游戏的氛围是答案总是一个相当常见的词。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers.",
  "translatedText": "事实上，还有另一个大约 2300 个单词的列表，它们是可能的答案。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creators girlfriend, which is kind of fun.",
  "translatedText": "这是一份由人类策划的名单，我认为是由游戏创作者的女朋友专门策划的，这有点意思。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving wordle that doesn't incorporate previous knowledge about this list.",
  "translatedText": "但我想做的是，我们在这个项目中面临的挑战是，看看我们能否写出一个解决 wordle 问题的程序，而这个程序又不包含之前关于这个列表的知识。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list.",
  "translatedText": "一方面，有很多非常常见的五个 字母单词您在该列表中找不到。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play wordle against anyone, not just what happens to be the official website.",
  "translatedText": "因此，最好能编写一个更有弹性的程序，可以与任何人玩 wordle，而不仅仅是碰巧是官方网站。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also, the reason that we know what this list of possible answers is, is because it's visible in the source code.",
  "translatedText": "另外，我们之所以知道这个可能答案列表的内容，是因为它在源代码中是可见的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day, that you could always just look up what tomorrow's answer will be.",
  "translatedText": "但在源代码中可以看到的方式是，每天的答案都会按照特定的顺序出现，你可以随时查找明天的答案是什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 287.0,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating.",
  "translatedText": "很明显，使用该列表在某种程度上是作弊行为。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words.",
  "translatedText": "使谜题更有趣、信息论课程更丰富的方法 是使用一些更通用的数据，例如相对词 频，来捕捉对更常见词的偏好的直觉。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess?",
  "translatedText": "那么在这13000种可能性中，我们应该如何选择开局猜测呢？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality?",
  "translatedText": "例如，如果我的朋友提出厌烦，我们应该如何分析它的质量？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W.",
  "translatedText": "好吧，他说他喜欢那个不太可能的 W 的原因是他喜欢 远射的本质，如果你击中了那个 W，那感觉是多么好。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern.",
  "translatedText": "例如，如果第一个揭示的模式是这样的，那么这个 庞大的词典中只有 58 个单词与该模式匹配。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000.",
  "translatedText": "因此，与 13,000 人相比，这是一个巨大的减少。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this.",
  "translatedText": "但当然，另一方面是，获得这样的模式非常罕见。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000.",
  "translatedText": "具体来说，如果每个单词作为答案的可能性相同，则达到 此模式的概率将为 58 除以大约 13,000。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers.",
  "translatedText": "当然，它们成为答案的可能性并不相同。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words.",
  "translatedText": "其中大部分都是非常晦涩甚至有问题的词语。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later.",
  "translatedText": "但至少对于我们第一次通过这一切，我们假设它 们的可能性相同，然后稍后再对其进行完善。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is, the pattern with a lot of information is, by its very nature, unlikely to occur.",
  "translatedText": "问题的关键在于，从本质上讲，信息量大的模式不太可能出现。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely.",
  "translatedText": "事实上，提供信息意味着这是不可能的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where, of course, there's not a W in it.",
  "translatedText": "更有可能出现的开局模式是这样的，当然，其中没有 W。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y.",
  "translatedText": "也许有E，也许没有A，没有R，没有Y。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches.",
  "translatedText": "在本例中，有 1400 个可能的匹配项。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see.",
  "translatedText": "如果所有可能性均等，则您看到 的模式的概率约为 11%。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative.",
  "translatedText": "因此，最可能的结果也是信息最少的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see.",
  "translatedText": "为了获得更全面的视角，让我向您展示您可 能看到的所有不同模式的概率的完整分布。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common.",
  "translatedText": "因此，您看到的每个条形都对应于可能显示的颜 色模式，其中有 3 到 5 种可能性，并且 它们从左到右、最常见到最不常见进行组织。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays.",
  "translatedText": "所以这里最常见的可能性是你得到的都是灰色的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time.",
  "translatedText": "这种情况发生的概率约为 14%。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here, where there's only 18 possibilities for what matches this pattern, that evidently look like this.",
  "translatedText": "当你进行猜测时，你所希望的是，你最终会出现在这个长尾中的某个地方，就像这里，只有 18 种可能与这个模式相匹配，显然看起来是这样的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here.",
  "translatedText": "或者，如果我们冒险向左走一点，你知道，也许我们会一直走到这里。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you.",
  "translatedText": "好的，这是给你的一个很好的谜题。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them?",
  "translatedText": "英语中以 W 开头、以 Y 结尾、其 中某个位置有 R 的三个单词是什么？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly.",
  "translatedText": "事实证明，让我们看看，答案是罗嗦、狡猾和讽刺的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution.",
  "translatedText": "因此，为了判断这个词的整体效果如何，我们需要某 种方式来衡量您将从该分布中获得的预期信息量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score.",
  "translatedText": "如果我们检查每个模式，并将其发生的概率乘以衡量其 信息量的因素，这也许可以给我们一个客观的分数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches.",
  "translatedText": "现在，您对某事物应该是什么的第一直觉可能是匹配的数量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches.",
  "translatedText": "您想要较低的平均匹配数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer.",
  "translatedText": "但相反，我想使用一种更通用的衡量标准，我们通常将其归因于信息 ，并且一旦我们为这 13,000 个单词中的每一个分配不同的 概率来判断它们是否实际上是答案，这种衡量标准就会更加灵活。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but is really intuitive if we just look at examples.",
  "translatedText": "信息的标准单位是比特，它的计算公式有点滑稽，但如果我们只看例子，它确实很直观。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information.",
  "translatedText": "如果你的观察结果将你的可能性空间减 少了一半，我们就说它只有一点信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about half of the five letter words have an S, a little less than that, but about half.",
  "translatedText": "在我们的例子中，可能性的空间是所有可能的单词，结果发现大约一半的五个字母的单词都有 S，比这少一点，但也有一半左右。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information.",
  "translatedText": "这样观察就会给你一点信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information.",
  "translatedText": "相反，如果一个新事实将可能性空间减 少了四倍，我们就说它有两位信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T.",
  "translatedText": "例如，事实证明这些单词中大约四分之一有 T。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth.",
  "translatedText": "如果观察将该空间缩小八分之一，我 们就说它是三位信息，依此类推。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a sixteenth, five bits cuts it into a thirty second.",
  "translatedText": "四个比特可将其转换成十六分之一，五个比特可将其转换成三十秒。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.88
 },
 {
  "input": "So now's when you might want to take a moment and pause and ask for yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence?",
  "translatedText": "所以，现在你可能需要花点时间，停顿一下，问一问自己，从发生概率的角度来看，比特数的信息公式是什么？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 534.96,
  "end": 542.98
 },
 {
  "input": "Well, what we're saying here is basically that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability.",
  "translatedText": "我们在这里所说的是，当你取位数的二分之一 时，这与概率是一样的，这与说 2 的位数 次方等于概率的 1 是一样的，重新排列进 一步表示该信息是一的对数基数除以概率。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still where the information is the negative log base two of the probability.",
  "translatedText": "有时，你还会看到这样的情况：再重新排列一次，信息就是概率的负对数基二。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half.",
  "translatedText": "对于外行来说，这样表达可能有点奇怪 ，但这确实是一个非常直观的想法，即 询问您已将可能性减少一半的次数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture?",
  "translatedText": "现在，如果您想知道，您知道，我以为我们只是 在玩一个有趣的文字游戏，为什么要使用对数？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095.",
  "translatedText": "这是一个更好的单元的原因之一是，谈论非常不可能 的事件要容易得多，说一个观察有 20 位信息比 说这样那样发生的概率为 0 容易得多。0000095。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together.",
  "translatedText": "但这种对数表达式被证明是对概率论非常有用 的补充，更实质性的原因是信息相加的方式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information.",
  "translatedText": "例如，如果一个观察结果为您提供了两位信息，将您的空间 缩小了四分之二，然后第二个观察结果（如您在 Wor dle 中的第二次猜测）为您提供了另外三位信息，将 您的空间进一步缩小了八倍，则两个一起给你五位信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add.",
  "translatedText": "就像概率喜欢乘法一样，信息喜欢增加。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with.",
  "translatedText": "因此，一旦我们处于预期值之类的领域，我们 将一堆数字相加，日志就会使其更容易处理。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for weary and add another little tracker on here, showing us how much information there is for each pattern.",
  "translatedText": "让我们回到 \"疲倦 \"的分布图，在这里添加另一个小跟踪器，显示每种模式的信息量。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain.",
  "translatedText": "我希望您注意的主要事情是，我们获得这些更有可能 的模式的概率越高，信息越少，您获得的位就越少。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information.",
  "translatedText": "我们衡量这种猜测质量的方法是计算这种信息的预期值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 643.5,
  "end": 648.02
 },
 {
  "input": "When we go through each pattern, we say how probable is it and then we multiply that by how many bits of information do we get.",
  "translatedText": "当我们研究每一种模式时，我们会说它的可能性有多大，然后再乘以我们得到的信息位数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 648.42,
  "end": 654.06
 },
 {
  "input": "And in the example of weary, that turns out to be 4.9 bits.",
  "translatedText": "在 weary 的例子中，结果是 4.9 位。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times.",
  "translatedText": "因此，平均而言，您从这个开局猜测中获得的信息 相当于将您的可能性空间切成两半（大约五倍）。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like slate.",
  "translatedText": "相比之下，预期信息值更高的猜测的例子就是石板。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case, you'll notice the distribution looks a lot flatter.",
  "translatedText": "在这种情况下，你会发现分布看起来平缓了许多。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information.",
  "translatedText": "特别是，所有灰色中最有可能出现的概率只有 6% 左右，因此至少明显会得到 3。9位信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that.",
  "translatedText": "但这是最低限度，更常见的是你会得到比这更好的东西。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8.",
  "translatedText": "事实证明，当你计算这个数字并将所有相 关术语加起来时，平均信息约为 5。8. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with weary, your space of possibilities will be about half as big after this first guess, on average.",
  "translatedText": "因此，与 \"疲倦 \"相比，在第一次猜测之后，您的可能性空间平均会缩小一半左右。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity.",
  "translatedText": "关于信息量期望值的名称，实际上有一个有趣的故事。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "You see, information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science.",
  "translatedText": "信息论是由 20 世纪 40 年代在贝尔实验室工作的克劳德·香农 (C laude Shannon) 提出的，但他正在与约翰·冯·诺依曼 ( John von Neumann) 谈论他尚未发表的一些想法，约翰· 冯·诺依曼是当时非常杰出的知识巨人。数学和物理以及计算机科学的开端。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well, you should call it entropy, and for two reasons.",
  "translatedText": "冯-诺依曼说，\"你应该叫它熵，原因有两个。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage.",
  "translatedText": "首先，你的不确定性函数已经在统计力学中以这个名字使 用了，所以它已经有一个名字了，其次，更重要的是，没 有人知道熵到底是什么，所以在辩论中你总是会有优势。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design.",
  "translatedText": "因此，如果这个名字看起来有点神秘，并且 如果这个故事可信的话，那就是有意为之。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess.",
  "translatedText": "另外，如果您想知道它与物理学中所有热力学 第二定律的关系，那么肯定存在某种联系， 但在其起源中，香农只是处理纯概率论，并且 出于我们的目的，当我使用熵这个词，我只 是想让你思考一个特定猜测的预期信息值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously.",
  "translatedText": "您可以将熵视为同时测量两个事物。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution?",
  "translatedText": "第一个问题是，分布有多平？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be.",
  "translatedText": "分布越接近均匀，熵就越高。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy.",
  "translatedText": "在我们的例子中，总共有 3 到 5 个模式，对于均匀分布，观察其 中任何一个模式都会有 3 到 5 个的信息对数基数 2，恰好是 7。92，所以这是该熵可能具有的绝对最大值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place.",
  "translatedText": "但熵首先也是一种衡 量可能性的方法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits.",
  "translatedText": "例如，如果您碰巧有某个单词，其中只有 16 种可能的模式，并 且每种模式的可能性相同，则该熵（即该预期信息）将是 4 位。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits.",
  "translatedText": "但如果你有另一个词，其中可能出现 64 种可能的 模式，并且它们的可能性相同，那么熵将是 6 位。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes.",
  "translatedText": "因此，如果您在野外看到某个分布的熵为 6 位，这 就有点像是在说即将发生的事情存在同样多的变化和不 确定性，就好像有 64 个同样可能的结果一样。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this.",
  "translatedText": "对于我第一次使用 Wurtelebot，我基本上就是让它这样做。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the different possible guesses that you could have, all 13,000 words, it computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns that you might see for each one, and then it picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible.",
  "translatedText": "它会遍历所有可能的猜测，即所有 13,000 个单 词，计算每个单词的熵，或者更具体地说，计算您可能 看到的所有模式中每个单词的分布熵，并选择最高的，因 为这是一个可能会尽可能地削减你的可能性空间的人。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses.",
  "translatedText": "尽管我在这里只讨论了第一个猜测，但它对 于接下来的几次猜测也起到了同样的作用。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words.",
  "translatedText": "例如，在您看到第一个猜测的某些模式后，这将根 据与之匹配的内容将您限制为较少数量的可能单词 ，您只需针对该较小的单词集玩相同的游戏即可。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy.",
  "translatedText": "对于建议的第二个猜测，您会查看从一组更受限制的 单词中可能出现的所有模式的分布，搜索所有 13 ,000 种可能性，然后找到使熵最大化的一种。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins.",
  "translatedText": "为了向您展示这是如何实际工作的，让我拿出我编写的 Wurt ele 的一个小变体，它在页边空白处显示了此分析的亮点。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "So after doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information.",
  "translatedText": "在进行了所有的熵计算后，在右侧，它向我们展示了哪些信息具有最高的预期信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch.",
  "translatedText": "事实证明，至少目前最重要的答案是稗子，我们稍后会对此进 行完善，这意味着，嗯，当然是野豌豆，最常见的野豌豆。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got given this particular pattern.",
  "translatedText": "每次我们在这里进行猜测时，我可能会忽略它的建议而选择 \"石板\"，因为我喜欢 \"石板\"，我们可以看到它有多少预期信息，但在这个词的右边，它会显示我们在这个特定模式下获得了多少实际信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that.",
  "translatedText": "所以看起来我们有点不走运，我们预计会得到 5 个。8，但 我们碰巧得到的东西比这个少。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now.",
  "translatedText": "然后在左侧，它向我们展示了当前 所处位置的所有不同可能的单词。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment.",
  "translatedText": "蓝色条告诉我们它认为每个单词出现的可能性有多大，因此目前它 假设每个单词出现的可能性相同，但我们稍后会对其进行改进。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities.",
  "translatedText": "然后，这种不确定性测量告诉我们可能单词的 分布熵，因为它是均匀分布，所以现在只是 一种计算可能性数量的不必要的复杂方法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities.",
  "translatedText": "例如，如果我们要计算 2 的 13 次方。66，这应该是 大约 13,000 种可能性。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "Um, a little bit off here, but only because I'm not showing all the decimal places.",
  "translatedText": "嗯，这里有点偏差，但只是因为我没有显示所有的小数位。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute.",
  "translatedText": "目前，这可能感觉多余，而且好像事情过于复杂，但您 很快就会明白为什么同时拥有这两个数字是有用的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Raman, which again just really doesn't feel like a word.",
  "translatedText": "因此，在这里，它似乎建议我们的第二个猜测是拉曼熵值最高，但这又真的不像是一个词。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here I'm going to go ahead and type in Rains.",
  "translatedText": "所以，为了站在道德制高点上，我要继续输入 Rains。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky.",
  "translatedText": "看来我们又有点不走运了。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information.",
  "translatedText": "我们本来期待4。3 位，但我们只得到了 3 位。39位信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities.",
  "translatedText": "这样一来，我们就有 55 种可能性。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means.",
  "translatedText": "在这里，也许我实际上会遵循它的建 议，即组合，无论这意味着什么。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And, okay, this is actually a good chance for a puzzle.",
  "translatedText": "好吧，这其实也是个拼图的好机会。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information.",
  "translatedText": "它告诉我们这个模式给了我们 4。7 位信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty.",
  "translatedText": "但在左边，在我们看到该模式之前，有 5 个。78 位不确定性。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities?",
  "translatedText": "那么作为对你的一个测验，剩余可能性的数量意味着什么？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well it means that we're reduced down to 1 bit of uncertainty, which is the same thing as saying that there's 2 possible answers.",
  "translatedText": "这意味着我们只剩下一点不确定性，这和说有两种可能的答案是一回事。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice.",
  "translatedText": "这是50-50的选择。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss.",
  "translatedText": "从这里开始，因为你和我知道哪些词更 常见，所以我们知道答案应该是深渊。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that.",
  "translatedText": "但正如现在所写的，程序并不知道这一点。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it.",
  "translatedText": "所以它会继续前进，尝试获取尽可能多的信息， 直到只剩下一种可能性，然后它就会猜测它。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy, but let's say we call this version 1 of our wordle solver, and then we go and run some simulations to see how it does.",
  "translatedText": "因此，我们显然需要一个更好的终局策略，但假设我们将其称为 wordle 解算器的第一版，然后进行一些模拟，看看它的表现如何。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game.",
  "translatedText": "所以它的工作方式是玩所有可能的文字游戏。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers.",
  "translatedText": "它会检查所有 2315 个单词，这些单词是实际的单词答案。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set.",
  "translatedText": "它基本上使用它作为测试集。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice.",
  "translatedText": "采用这种天真的方法，不考虑一个词的常见程度，只是 试图在每一步中最大化信息，直到它只剩下一个选择。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124.",
  "translatedText": "模拟结束时，平均得分约为 4。124. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expect it to do worse.",
  "translatedText": "老实说，这还不错，我还真有点期待它能做得更糟。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4.",
  "translatedText": "但玩wordle的人会告诉你，他们通常可以在4内得到它。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can.",
  "translatedText": "真正的挑战是尽可能多地获得三分。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3.",
  "translatedText": "4分和3分之间的差距相当大。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low-hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that.",
  "translatedText": "显而易见，这里的 \"低垂果实 \"就是以某种方式将一个词是否常见纳入其中，而我们究竟该如何做到这一点呢？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language, and I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset.",
  "translatedText": "我的方法是获取英语中所有单词的相对词频列表，然后使用 Mathematica 的词频数据函数，该函数本身取自 Google Books English Ngram 公共数据集。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words.",
  "translatedText": "看起来很有趣，例如，如果我们将其从最 常见的单词到最不常见的单词进行排序。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5-letter words in the English language.",
  "translatedText": "显然，这些都是英语中最常见的 5 个字母的单词。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common.",
  "translatedText": "或者更确切地说，这些是第八个最常见的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there.",
  "translatedText": "首先是which，然后是there和there。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common.",
  "translatedText": "First本身不是first，而是9th，并且这些其 他词可能更频繁地出现是有道理的，其中first之后 的词是after、where，而那些词则不太常见。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency, because for example which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely.",
  "translatedText": "现在，在使用这些数据来模拟每个单词成为最终答案的可能性时，它不应该仅仅与频率成正比，因为举例来说，在这个数据集中，which 的得分是 0.002，而 braid 这个单词在某种意义上的可能性要低 1000 倍。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering, so we want more of a binary cutoff.",
  "translatedText": "但这两个词都很常见，几乎肯定值得考虑，所以我们希望采用二进制分界线。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty.",
  "translatedText": "我的方法是想象一下将整个排序的单词列表，然后将其排列 在 x 轴上，然后应用 sigmoid 函数，这是 输出基本上是二进制的函数的标准方法，它是要么是 0 ，要么是 1，但对于该不确定区域，中间有一个平滑。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis.",
  "translatedText": "所以本质上，我分配给每个单词出现在最终列表中的概率将是上 面的 sigmoid 函数的值，无论它位于 x 轴上。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff.",
  "translatedText": "显然，这取决于几个参数，例如，这些单词在 x 轴上填充 的空间有多宽，决定了我们从 1 到 0 的逐渐或陡峭 程度，以及我们将它们从左到右放置的位置决定了截止值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "And to be honest the way I did this was kind of just licking my finger and sticking it into the wind.",
  "translatedText": "说实话，我的做法就是舔手指然后把它插到风里。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff.",
  "translatedText": "我查看了排序后的列表，并试图找到一个窗口 ，当我查看它时，我认为这些单词中的一半 更有可能是最终答案，并将其用作截止值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Now once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement.",
  "translatedText": "一旦我们有了这样的词语分布，熵就会成为另一种非常有用的测量方法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were other and nails, and we end up with a situation where there's four possible words that match it.",
  "translatedText": "例如，假设我们在玩一个游戏，我们从我以前的开场白开始，分别是 \"其他 \"和 \"钉子\"，结果出现了有四个词可能与之匹配的情况。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely, let me ask you, what is the entropy of this distribution?",
  "translatedText": "假设我们认为它们的可能性相同，那么我问你，这个分布的熵是多少？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2.",
  "translatedText": "嗯，与这些可能性中的每一种相关的信息将是 4 的以 2 为底的对数，因为每一种都是 1 和 4，那就是 2。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "2 bits of information, 4 possibilities.",
  "translatedText": "2 位信息，4 种可能性。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good.",
  "translatedText": "一切都很好。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than 4 matches?",
  "translatedText": "但如果我告诉你，实际上不止 4 场比赛呢？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it.",
  "translatedText": "事实上，当我们查看完整的单词列表时，有 16 个单词与其匹配。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure.",
  "translatedText": "但假设我们的模型对其他 12 个单词实际成为最终答案 的概率非常低，大约是千分之一，因为它们真的很晦涩。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution?",
  "translatedText": "现在我问你，这个分布的熵是多少？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before.",
  "translatedText": "如果熵在这里纯粹测量匹配的数量，那么您可能 会期望它类似于 16 的以 2 为底的对数 ，即 4，比我们之前的不确定性多了两位。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before.",
  "translatedText": "但当然，实际的不确定性与我们之前的情况并没有太大不同。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example.",
  "translatedText": "例如，仅仅因为有这 12 个非常晦涩的单词并不意 味着当得知最终答案是“魅力”时会更加令人惊讶。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits.",
  "translatedText": "因此，当你在这里进行实际计算，把每次出现的概率乘以相应的信息相加，得到的结果就是 2.11 比特。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "Just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it.",
  "translatedText": "只是说，基本上是两个比特，基本上是这四种可能性，但由于所有这些极不可能发生的事件，不确定性更大一些，尽管如果你真的了解了这些事件，你会从中获得大量信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson.",
  "translatedText": "缩小范围，这就是 Wordle 成为信 息论课程的一个很好的例子的部分原因。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy.",
  "translatedText": "我们对熵有两种不同的感觉应用。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words we have possible.",
  "translatedText": "第一种是告诉我们从给定的猜测中得到的预期信息是什么，第二种是说我们能否在所有可能的词语中测量出剩余的不确定性。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation.",
  "translatedText": "我应该强调，在第一种情况下，我们正在查看猜测的预期 信息，一旦我们对单词的权重不相等，就会影响熵计算。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words.",
  "translatedText": "例如，让我拿出我们之前查看的与 We ary 相关的分布的相同案例，但这次 在所有可能的单词中使用非均匀分布。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well.",
  "translatedText": "所以让我看看是否可以在这里找到一个很好地说明它的部分。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here, this is pretty good.",
  "translatedText": "好了，这个不错",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it.",
  "translatedText": "这里我们有两个相邻的模式，它们的可能性大致相同，但我 们被告知其中一个模式有 32 个可能的单词与其匹配。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them.",
  "translatedText": "如果我们检查它们是什么，那就是 32 个，当 你扫视它们时，它们都只是非常不可能的单词。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches.",
  "translatedText": "我们很难找到任何让人觉得可信的答案，也许是吆喝声，但如果我们看一下分布中的邻近模式，它被认为同样可能，我们被告知它只有 8 种可能的匹配。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1386.66
 },
 {
  "input": "So a quarter as many matches, but it's about as likely.",
  "translatedText": "因此，比赛数量只有四分之一，但可能性差不多。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1386.88,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why.",
  "translatedText": "当我们拿出这些匹配项时，我们就能明白原因了。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers like ring or wrath or raps.",
  "translatedText": "其中有些是似是而非的答案，比如戒指、愤怒或饶舌。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version two of the Wordlebot here.",
  "translatedText": "为了说明我们是如何将所有这些功能整合在一起的，让我在这里调出 Wordlebot 的第二版。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1402.3
 },
 {
  "input": "And there are two or three main differences from the first one that we saw.",
  "translatedText": "与我们看到的第一部相比，主要有两三处不同。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1402.56,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer.",
  "translatedText": "首先，就像我刚才说的，我们计算这些熵、这些信 息的预期值的方式现在正在使用跨模式的更精细的 分布，其中包含给定单词实际上是答案的概率。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number one, though the ones following are a bit different.",
  "translatedText": "泪水仍然是第一位的，尽管后面的几位有些不同。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table.",
  "translatedText": "其次，当它对首选进行排名时，它现在将保留每个单词 是实际答案的概率模型，并将其纳入其决策中，一旦我 们对答案有了一些猜测，就更容易看到这一点。桌子。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives.",
  "translatedText": "再次忽略它的建议，因为我们不能让机器统治我们的生活。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches.",
  "translatedText": "我想我应该提到另一件不同的事情是在左边，不确定 性值，即位数，不再只是与可能匹配的数量冗余。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which would be a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes.",
  "translatedText": "现在，如果我们把它调出来，计算 2 到 8.02，也就是比 256 稍高一点，我猜是 259，它的意思是，尽管实际上有 526 个单词符合这个模式，但它的不确定性更类似于如果有 259 个同样可能的结果时的不确定性。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this.",
  "translatedText": "你可以这样想。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borks is not the answer, same with yorts and zorl and zorus.",
  "translatedText": "它知道 Borks 不是答案，Yorts、Zorl 和 Zorus 也一样。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1474.66
 },
 {
  "input": "So it's a little less uncertain than it was in the previous case.",
  "translatedText": "因此，它的不确定性比前一个案例要小一些。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1474.66,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller.",
  "translatedText": "这个位数会更小。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here.",
  "translatedText": "如果我继续玩这个游戏，我会用一些与我想在 这里解释的内容相符的猜测来完善这个游戏。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy.",
  "translatedText": "通过第四种猜测，如果你看一下它的首 选，你会发现它不再只是最大化熵。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words.",
  "translatedText": "所以在这一点上，技术上有七种可能性 ，但唯一有意义的机会是宿舍和单词。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values that strictly speaking would give more information.",
  "translatedText": "你可以看到，在所有其他严格意义上可以提供更多信息的数值中，它将这两个数值排在了前面。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect.",
  "translatedText": "我第一次这样做时，我只是将这两个数字相加来衡 量每个猜测的质量，这实际上比你想象的要好。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic.",
  "translatedText": "但我真的感觉不到它的系统性。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1515.9
 },
 {
  "input": "And I'm sure there's other approaches people could take.",
  "translatedText": "我相信人们还可以采取其他方法。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1516.1,
  "end": 1517.88
 },
 {
  "input": "But here's the one I landed on.",
  "translatedText": "但我最终选择了这里。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1517.9,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that.",
  "translatedText": "如果我们正在考虑下一次猜测的前景，就像在这种情况下的话， 我们真正关心的是如果我们这样做的话我们游戏的预期得分。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to.",
  "translatedText": "为了计算预期分数，我们会计算单词是实际 答案的概率是多少，目前描述为 58%。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be four.",
  "translatedText": "我们说，如果有 58% 的机会，我们在这场比赛中的得分为 4。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of one minus that 58%, our score will be more than that four.",
  "translatedText": "然后用 1 的概率减去 58%，我们的分数就会超过 4。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point.",
  "translatedText": "我们不知道还有多少，但我们可以根据到 达这一点后可能存在的不确定性来估计。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment, there's 1.44 bits of uncertainty.",
  "translatedText": "具体来说，目前有 1.44 比特的不确定性。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits.",
  "translatedText": "如果我们猜测单词，它会告诉我们预期得到的信息是 1。27 位。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens.",
  "translatedText": "因此，如果我们猜测单词，这种差异代表了在这 种情况发生后我们可能会留下多少不确定性。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score.",
  "translatedText": "我们需要的是某种函数，我在这里称之为 f ，它将这种不确定性与预期分数联系起来。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version one of the bot to say, hey, what was the actual score after various points with certain very measurable amounts of uncertainty?",
  "translatedText": "它的做法是，根据第一版机器人的数据，绘制前几场比赛的大量数据，然后说：\"嘿，在不确定性非常大的情况下，各点之后的实际得分是多少？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games, after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer.",
  "translatedText": "例如，这些数据点位于 8.7 左右的数值之上，说明在某些游戏中，经过 8.7 位的不确定性后，需要两次猜测才能得到最终答案。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games, it took three guesses.",
  "translatedText": "其他游戏则需要猜三次。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1600.66
 },
 {
  "input": "For other games, it took four guesses.",
  "translatedText": "其他游戏则需要猜四次。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1600.82,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring.",
  "translatedText": "如果我们在这里向左移动，所有超过零的点都表明， 每当不确定性为零时，也就是说只有一种可能性，那 么所需的猜测次数总是只有一次，这是令人放心的。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses, and so on and so forth here.",
  "translatedText": "只要有一点不确定性，也就是说基本上只有两种可能性，那么有时就需要多猜一次，有时就需要多猜两次，以此类推。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages.",
  "translatedText": "也许可视化这些数据的一种稍微简单的方法是将其放在一起并取平均值。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example, this bar here is saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5.",
  "translatedText": "例如，这个条形图表示，在我们有一点不确定性的所有点中，平均需要的新猜测次数约为 1.5。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here is saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward.",
  "translatedText": "这里的条形图表示，在所有不同的游戏中，在某一点上，不确定性略高于 4 位，也就是把范围缩小到 16 种不同的可能性，那么从那时起，平均需要猜测的次数略多于两次。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this.",
  "translatedText": "从这里开始，我只是做了一个回归来拟合一个看起来合理的函数。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember, the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be.",
  "translatedText": "记住，做这些事情的目的是为了量化我们的直觉，即从一个词中获得的信息越多，预期得分就越低。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So, with this as version 2.0, if we go back and run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do?",
  "translatedText": "那么，在 2.0 版的基础上，如果我们回去运行同一组模拟，让它与所有 2315 个可能的 wordle 答案进行博弈，结果会如何呢？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version, it's definitely better, which is reassuring.",
  "translatedText": "与我们的第一个版本相比，它肯定要好得多，这一点令人欣慰。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done, the average is around 3.6.",
  "translatedText": "综上所述，平均值约为 3.6。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1686.18
 },
 {
  "input": "Although unlike the first version, there are a couple times that it loses, and requires more than six in this circumstance.",
  "translatedText": "虽然与第一个版本不同的是，有几次它输了，在这种情况下需要六个以上。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1686.54,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information.",
  "translatedText": "大概是因为有时需要进行权衡以实 际实现目标，而不是最大化信息。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6?",
  "translatedText": "那么我们可以做得比 3 更好吗？6？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can.",
  "translatedText": "我们绝对可以。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now, I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model.",
  "translatedText": "现在，我在一开始就说过，最有趣的是尝试不把真正的 wordle 答案列表纳入它建立模型的方式。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43.",
  "translatedText": "但如果我们确实将其合并，我可以获得的最佳性能约为 3。43. ",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that.",
  "translatedText": "因此，如果我们尝试比仅使用词频数据更复杂地选择此先验分 布，则这 3.43 可能给出了我们可以做到什么 程度的最大值，或者至少是我可以做到什么程度。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one.",
  "translatedText": "最佳性能本质上只是使用了我在这里讨 论的想法，但它更进一步，就像它向前 两步而不是一步搜索预期信息一样。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is.",
  "translatedText": "本来我打算更多地讨论这个问题，但我意 识到我们实际上已经讨论了很长时间了。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least, it's looking like Crane is the best opener.",
  "translatedText": "我要说的一点是，在进行了这两步搜索，并在热门候选者中进行了几次抽样模拟后，至少到目前为止，对我来说，克莱恩看起来是最好的开局者。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed?",
  "translatedText": "谁能想到呢？",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits.",
  "translatedText": "此外，如果您使用真实的单词列表来确定您的可能性 空间，那么您开始的不确定性将略高于 11 位。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits.",
  "translatedText": "而事实证明，仅仅通过暴力搜索，前两次猜测后的最大预期信息量大约为 10 比特。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty.",
  "translatedText": "这表明，在最好的情况下，在您进行前两次猜测之后， 如果有完美的最佳玩法，您将留下大约一点不确定性。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses.",
  "translatedText": "这与归结为两种可能的猜测相同。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "But I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as three, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail.",
  "translatedText": "但我认为，如果说你永远不可能写出一个平均值低至 3 的算法，这也是公平的，而且可能是相当保守的，因为就你现有的字数而言，根本不可能只经过两步就获得足够的信息，从而保证每次都能在第三个空格中找到答案。",
  "model": "DeepL",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]