1
00:00:00,000 --> 00:00:02,934
Le jeu Wurdle est devenu assez viral au cours des deux derniers mois, 

2
00:00:02,934 --> 00:00:05,659
et n'a jamais négligé une opportunité de cours de mathématiques. 

3
00:00:05,659 --> 00:00:08,887
Il me semble que ce jeu constitue un très bon exemple central dans une leçon 

4
00:00:08,887 --> 00:00:12,660
sur la théorie de l'information, et en particulier un sujet connu sous le nom d’entropie. 

5
00:00:13,920 --> 00:00:16,644
Vous voyez, comme beaucoup de gens, je me suis laissé entraîner dans le puzzle, 

6
00:00:16,644 --> 00:00:19,470
et comme beaucoup de programmeurs, je me suis également laissé entraîner à essayer 

7
00:00:19,470 --> 00:00:22,399
d'écrire un algorithme qui permettrait de jouer au jeu de la manière la plus optimale 

8
00:00:22,399 --> 00:00:22,740
possible. 

9
00:00:23,180 --> 00:00:25,775
Et ce que j'ai pensé faire ici, c'est simplement parler avec vous de 

10
00:00:25,775 --> 00:00:28,672
certains de mes processus et expliquer certains des calculs qui y sont liés, 

11
00:00:28,672 --> 00:00:31,080
puisque tout l'algorithme est centré sur cette idée d'entropie. 

12
00:00:38,700 --> 00:00:41,640
Tout d’abord, au cas où vous n’en auriez pas entendu parler, qu’est-ce que Wurdle ? 

13
00:00:42,040 --> 00:00:45,203
Et pour faire d'une pierre deux coups pendant que nous examinons les règles du jeu, 

14
00:00:45,203 --> 00:00:47,688
permettez-moi également de vous montrer où nous allons avec cela, 

15
00:00:47,688 --> 00:00:51,040
c'est-à-dire développer un petit algorithme qui jouera essentiellement le jeu pour nous. 

16
00:00:51,360 --> 00:00:52,967
Bien que je n'aie pas fait le Wurdle d'aujourd'hui, 

17
00:00:52,967 --> 00:00:55,100
nous sommes le 4 février et nous verrons comment le bot se comporte. 

18
00:00:55,480 --> 00:00:58,216
Le but de Wurdle est de deviner un mot mystérieux de cinq lettres, 

19
00:00:58,216 --> 00:01:00,340
et vous avez six chances différentes de le deviner. 

20
00:01:00,840 --> 00:01:04,379
Par exemple, mon robot Wurdle me suggère de commencer par la grue à devinettes. 

21
00:01:05,180 --> 00:01:07,477
Chaque fois que vous faites une supposition, vous obtenez des 

22
00:01:07,477 --> 00:01:10,220
informations sur la proximité de votre supposition avec la vraie réponse. 

23
00:01:10,920 --> 00:01:14,100
Ici, la case grise me dit qu'il n'y a pas de C dans la réponse réelle. 

24
00:01:14,520 --> 00:01:17,840
La case jaune m'indique qu'il y a un R, mais il n'est pas dans cette position. 

25
00:01:18,240 --> 00:01:22,240
La case verte m'indique que le mot secret a un A et qu'il est en troisième position. 

26
00:01:22,720 --> 00:01:24,580
Et puis il n’y a ni N ni E. 

27
00:01:25,200 --> 00:01:27,340
Alors laissez-moi entrer et donner cette information au robot Wurdle. 

28
00:01:27,340 --> 00:01:29,147
Nous avons commencé avec la grue, nous avons eu du gris, 

29
00:01:29,147 --> 00:01:30,320
du jaune, du vert, du gris, du gris. 

30
00:01:31,420 --> 00:01:33,724
Ne vous inquiétez pas de toutes les données qu'il affiche en ce moment, 

31
00:01:33,724 --> 00:01:34,940
je vous l'expliquerai en temps voulu. 

32
00:01:35,460 --> 00:01:38,820
Mais sa principale suggestion pour notre deuxième choix est shtick. 

33
00:01:39,560 --> 00:01:41,806
Et votre supposition doit être un véritable mot de cinq lettres, 

34
00:01:41,806 --> 00:01:44,708
mais comme vous le verrez, elle est assez libérale quant à ce qu'elle vous laissera 

35
00:01:44,708 --> 00:01:45,400
réellement deviner. 

36
00:01:46,200 --> 00:01:47,440
Dans ce cas, nous essayons shtick. 

37
00:01:48,780 --> 00:01:50,180
Et bien, les choses s’annoncent plutôt bien. 

38
00:01:50,260 --> 00:01:53,020
On touche le S et le H, donc on connaît les trois premières lettres, 

39
00:01:53,020 --> 00:01:53,980
on sait qu'il y a un R. 

40
00:01:53,980 --> 00:01:58,700
Et donc ça va être comme SHA quelque chose de R, ou SHA R quelque chose. 

41
00:01:59,620 --> 00:02:02,740
Et il semble que le robot Wurdle sache qu'il ne reste que deux possibilités, 

42
00:02:02,740 --> 00:02:04,240
soit un fragment, soit un tranchant. 

43
00:02:05,100 --> 00:02:06,552
C'est une sorte de mélange entre eux à ce stade, 

44
00:02:06,552 --> 00:02:09,190
donc je suppose que c'est probablement simplement parce que c'est par ordre alphabétique 

45
00:02:09,190 --> 00:02:10,080
que cela va avec le fragment. 

46
00:02:11,220 --> 00:02:12,860
Quelle hourra, c'est la vraie réponse. 

47
00:02:12,960 --> 00:02:13,780
Nous l'avons donc eu en trois. 

48
00:02:14,600 --> 00:02:17,298
Si vous vous demandez si c'est bon, la façon dont j'ai entendu une 

49
00:02:17,298 --> 00:02:20,360
personne dire qu'avec Wurdle, quatre est la normale et trois est un birdie. 

50
00:02:20,680 --> 00:02:22,480
Ce qui, je pense, est une analogie assez pertinente. 

51
00:02:22,480 --> 00:02:25,335
Il faut être constamment sur son jeu pour en obtenir quatre, 

52
00:02:25,335 --> 00:02:27,020
mais ce n'est certainement pas fou. 

53
00:02:27,180 --> 00:02:29,920
Mais quand vous l'obtenez en trois, ça fait du bien. 

54
00:02:30,880 --> 00:02:32,573
Donc, si vous êtes partant, ce que j'aimerais faire ici, 

55
00:02:32,573 --> 00:02:35,039
c'est simplement parler de mon processus de réflexion depuis le début sur la façon 

56
00:02:35,039 --> 00:02:35,960
dont j'aborde le robot Wurdle. 

57
00:02:36,480 --> 00:02:38,071
Et comme je l'ai dit, c'est en réalité une excuse 

58
00:02:38,071 --> 00:02:39,440
pour un cours de théorie de l'information. 

59
00:02:39,740 --> 00:02:42,820
L’objectif principal est d’expliquer ce qu’est l’information et ce qu’est l’entropie. 

60
00:02:48,220 --> 00:02:50,928
Ma première pensée en abordant ce sujet a été de jeter un œil aux 

61
00:02:50,928 --> 00:02:53,720
fréquences relatives des différentes lettres de la langue anglaise. 

62
00:02:54,380 --> 00:02:56,805
Alors j'ai pensé, d'accord, y a-t-il une supposition d'ouverture ou une paire de 

63
00:02:56,805 --> 00:02:59,260
suppositions d'ouverture qui touche beaucoup de ces lettres les plus fréquentes ? 

64
00:02:59,960 --> 00:03:03,000
Et celui que j'aimais beaucoup était d'en faire d'autres suivis de clous. 

65
00:03:03,760 --> 00:03:05,558
L’idée est que si vous frappez une lettre, vous savez, 

66
00:03:05,558 --> 00:03:07,520
vous obtenez un vert ou un jaune, ça fait toujours du bien. 

67
00:03:07,520 --> 00:03:08,840
C'est comme si vous receviez des informations. 

68
00:03:09,340 --> 00:03:12,692
Mais dans ces cas-là, même si vous ne frappez pas et que vous obtenez toujours des gris, 

69
00:03:12,692 --> 00:03:15,366
cela vous donne quand même beaucoup d'informations puisqu'il est assez 

70
00:03:15,366 --> 00:03:17,400
rare de trouver un mot qui n'a aucune de ces lettres. 

71
00:03:18,140 --> 00:03:20,460
Mais même quand même, cela ne semble pas super systématique, 

72
00:03:20,460 --> 00:03:23,200
car par exemple, cela ne fait rien pour considérer l'ordre des lettres. 

73
00:03:23,560 --> 00:03:25,300
Pourquoi taper ongles quand je pourrais taper escargot ? 

74
00:03:26,080 --> 00:03:27,500
Est-il préférable d'avoir ce S à la fin ? 

75
00:03:27,820 --> 00:03:28,680
Je ne suis pas vraiment sûr. 

76
00:03:29,240 --> 00:03:32,693
Maintenant, un de mes amis m'a dit qu'il aimait commencer avec le mot fatigué, 

77
00:03:32,693 --> 00:03:36,540
ce qui m'a un peu surpris car il contient des lettres inhabituelles comme le W et le Y. 

78
00:03:37,120 --> 00:03:39,000
Mais qui sait, c'est peut-être une meilleure ouverture. 

79
00:03:39,320 --> 00:03:41,682
Existe-t-il une sorte de score quantitatif que nous pouvons 

80
00:03:41,682 --> 00:03:44,320
attribuer pour juger de la qualité d’une supposition potentielle ? 

81
00:03:45,340 --> 00:03:48,264
Maintenant, pour définir la manière dont nous allons classer les suppositions possibles, 

82
00:03:48,264 --> 00:03:50,302
revenons en arrière et ajoutons un peu de clarté à la manière 

83
00:03:50,302 --> 00:03:51,420
exacte dont le jeu est configuré. 

84
00:03:51,420 --> 00:03:54,584
Il y a donc une liste de mots qu'il vous permettra de saisir et qui sont 

85
00:03:54,584 --> 00:03:57,880
considérés comme des suppositions valables et qui fait environ 13 000 mots. 

86
00:03:58,320 --> 00:04:01,525
Mais quand on y regarde, il y a beaucoup de choses vraiment inhabituelles, 

87
00:04:01,525 --> 00:04:04,174
comme une tête ou Ali et ARG, le genre de mots qui provoquent 

88
00:04:04,174 --> 00:04:06,440
des disputes familiales dans une partie de Scrabble. 

89
00:04:06,960 --> 00:04:10,540
Mais l’ambiance du jeu est que la réponse sera toujours un mot assez courant. 

90
00:04:10,960 --> 00:04:13,093
Et en fait, il existe une autre liste d’environ 

91
00:04:13,093 --> 00:04:15,360
2 300 mots qui constituent les réponses possibles. 

92
00:04:15,940 --> 00:04:17,851
Et il s'agit d'une liste organisée par des humains, 

93
00:04:17,851 --> 00:04:21,160
je pense spécifiquement par la petite amie du créateur du jeu, ce qui est plutôt amusant. 

94
00:04:21,820 --> 00:04:24,651
Mais ce que j'aimerais faire, notre défi pour ce projet est de 

95
00:04:24,651 --> 00:04:27,438
voir si nous pouvons écrire un programme résolvant Wordle qui 

96
00:04:27,438 --> 00:04:30,180
n'intègre pas les connaissances antérieures sur cette liste. 

97
00:04:30,720 --> 00:04:32,611
D’une part, il existe de nombreux mots de cinq lettres 

98
00:04:32,611 --> 00:04:34,640
assez courants que vous ne trouverez pas dans cette liste. 

99
00:04:34,940 --> 00:04:38,394
Il serait donc préférable d'écrire un programme un peu plus résistant et qui permettrait 

100
00:04:38,394 --> 00:04:41,460
de jouer à Wordle contre n'importe qui, pas seulement contre le site officiel. 

101
00:04:41,920 --> 00:04:45,048
Et aussi la raison pour laquelle nous connaissons cette liste de réponses possibles, 

102
00:04:45,048 --> 00:04:47,000
c'est parce qu'elle est visible dans le code source. 

103
00:04:47,000 --> 00:04:49,997
Mais la manière dont cela est visible dans le code source dépend de 

104
00:04:49,997 --> 00:04:53,260
l'ordre spécifique dans lequel les réponses apparaissent de jour en jour. 

105
00:04:53,260 --> 00:04:55,840
Vous pouvez donc toujours simplement rechercher quelle sera la réponse de demain. 

106
00:04:56,420 --> 00:04:57,719
Il est donc clair qu'il y a un certain sens dans lequel 

107
00:04:57,719 --> 00:04:58,880
l'utilisation de la liste constitue de la triche. 

108
00:04:59,100 --> 00:05:01,865
Et ce qui rend un casse-tête plus intéressant et une leçon de théorie 

109
00:05:01,865 --> 00:05:04,671
de l'information plus riche est d'utiliser à la place des données plus 

110
00:05:04,671 --> 00:05:07,397
universelles comme les fréquences relatives des mots en général pour 

111
00:05:07,397 --> 00:05:10,440
capturer cette intuition d'avoir une préférence pour des mots plus courants. 

112
00:05:11,600 --> 00:05:13,704
Alors, parmi ces 13 000 possibilités, comment 

113
00:05:13,704 --> 00:05:15,900
devrions-nous choisir la première supposition ? 

114
00:05:16,400 --> 00:05:19,780
Par exemple, si mon ami propose fatigué, comment analyser sa qualité ? 

115
00:05:20,520 --> 00:05:23,929
Eh bien, la raison pour laquelle il a dit qu'il aime ce W improbable est qu'il 

116
00:05:23,929 --> 00:05:27,340
aime la nature à long terme de la sensation de bien-être si vous frappez ce W. 

117
00:05:27,920 --> 00:05:30,942
Par exemple, si le premier modèle révélé ressemblait à ceci, 

118
00:05:30,942 --> 00:05:34,955
alors il s’avère qu’il n’y a que 58 mots dans ce lexique géant qui correspondent 

119
00:05:34,955 --> 00:05:35,600
à ce modèle. 

120
00:05:36,060 --> 00:05:38,400
Cela représente donc une énorme réduction par rapport à 13 000. 

121
00:05:38,780 --> 00:05:40,772
Mais le revers de la médaille, bien sûr, c’est 

122
00:05:40,772 --> 00:05:43,020
qu’il est très rare d’avoir un motif comme celui-ci. 

123
00:05:43,020 --> 00:05:47,030
Plus précisément, si chaque mot avait la même probabilité d’être la réponse, 

124
00:05:47,030 --> 00:05:51,040
la probabilité de trouver ce modèle serait de 58 divisée par environ 13 000. 

125
00:05:51,580 --> 00:05:53,600
Bien sûr, il n’est pas également probable qu’elles constituent des réponses. 

126
00:05:53,720 --> 00:05:56,220
La plupart de ces mots sont très obscurs, voire discutables. 

127
00:05:56,600 --> 00:05:59,176
Mais au moins pour notre première tentative, supposons qu'ils sont 

128
00:05:59,176 --> 00:06:01,600
tous également probables, puis affinons cela un peu plus tard. 

129
00:06:02,020 --> 00:06:04,547
Le fait est qu’un modèle contenant beaucoup d’informations est, 

130
00:06:04,547 --> 00:06:06,720
de par sa nature même, peu susceptible de se produire. 

131
00:06:07,280 --> 00:06:10,800
En fait, ce que signifie être informatif, c'est que c'est peu probable. 

132
00:06:11,719 --> 00:06:15,016
Un modèle beaucoup plus probable à voir avec cette ouverture serait 

133
00:06:15,016 --> 00:06:18,120
quelque chose comme ceci, où bien sûr il n'y a pas de W dedans. 

134
00:06:18,240 --> 00:06:20,075
Peut-être qu'il y a un E, et peut-être qu'il n'y a pas de A, 

135
00:06:20,075 --> 00:06:21,400
qu'il n'y a pas de R, qu'il n'y a pas de Y. 

136
00:06:22,080 --> 00:06:24,560
Dans ce cas, il y a 1 400 correspondances possibles. 

137
00:06:25,080 --> 00:06:27,796
Si toutes les probabilités étaient égales, la probabilité que 

138
00:06:27,796 --> 00:06:30,600
ce soit la tendance que vous obtiendriez serait d’environ 11 %. 

139
00:06:30,900 --> 00:06:33,340
Les résultats les plus probables sont donc aussi les moins informatifs. 

140
00:06:34,240 --> 00:06:37,521
Pour avoir une vue plus globale, permettez-moi de vous montrer la répartition 

141
00:06:37,521 --> 00:06:41,140
complète des probabilités sur tous les différents modèles que vous pourriez observer. 

142
00:06:41,740 --> 00:06:45,302
Ainsi, chaque barre que vous regardez correspond à un motif possible de couleurs 

143
00:06:45,302 --> 00:06:48,557
qui pourrait être révélé, parmi lesquels il y a 3 à la 5ème possibilités, 

144
00:06:48,557 --> 00:06:52,340
et elles sont organisées de gauche à droite, de la plus courante à la moins courante. 

145
00:06:52,920 --> 00:06:56,000
La possibilité la plus courante ici est donc que vous obteniez uniquement des gris. 

146
00:06:56,100 --> 00:06:58,120
Cela se produit environ 14 % du temps. 

147
00:06:58,580 --> 00:07:01,231
Et ce que vous espérez lorsque vous faites une supposition, 

148
00:07:01,231 --> 00:07:04,279
c'est que vous vous retrouviez quelque part dans cette longue queue, 

149
00:07:04,279 --> 00:07:07,681
comme ici où il n'y a que 18 possibilités pour ce qui correspond à ce modèle 

150
00:07:07,681 --> 00:07:09,140
qui ressemble évidemment à ceci. 

151
00:07:09,920 --> 00:07:11,880
Ou si nous nous aventurons un peu plus à gauche, 

152
00:07:11,880 --> 00:07:13,800
vous savez, peut-être que nous irons jusqu'ici. 

153
00:07:14,940 --> 00:07:16,180
D'accord, voici un bon puzzle pour vous. 

154
00:07:16,540 --> 00:07:19,606
Quels sont les trois mots de la langue anglaise qui commencent par un W, 

155
00:07:19,606 --> 00:07:22,000
se terminent par un Y et contiennent un R quelque part ? 

156
00:07:22,480 --> 00:07:26,800
Il s’avère que les réponses sont, voyons, verbeuses, vermifuges et ironiques. 

157
00:07:27,500 --> 00:07:29,952
Donc, pour juger de la qualité globale de ce mot, 

158
00:07:29,952 --> 00:07:33,925
nous voulons une sorte de mesure de la quantité d'informations attendue que vous 

159
00:07:33,925 --> 00:07:35,740
allez obtenir de cette distribution. 

160
00:07:35,740 --> 00:07:38,816
Si nous examinons chaque modèle et multiplions sa probabilité 

161
00:07:38,816 --> 00:07:42,189
d'apparition par quelque chose qui mesure son caractère informatif, 

162
00:07:42,189 --> 00:07:44,720
cela peut peut-être nous donner un score objectif. 

163
00:07:45,960 --> 00:07:47,884
Maintenant, votre premier instinct pour savoir ce que devrait 

164
00:07:47,884 --> 00:07:49,840
être quelque chose pourrait être le nombre de correspondances. 

165
00:07:50,160 --> 00:07:52,400
Vous souhaitez un nombre moyen de correspondances inférieur. 

166
00:07:52,800 --> 00:07:56,620
Mais j'aimerais plutôt utiliser une mesure plus universelle que nous attribuons souvent à 

167
00:07:56,620 --> 00:08:00,100
l'information, et qui sera plus flexible une fois que nous aurons une probabilité 

168
00:08:00,100 --> 00:08:03,878
différente attribuée à chacun de ces 13 000 mots pour savoir s'ils constituent ou non la 

169
00:08:03,878 --> 00:08:04,260
réponse. 

170
00:08:10,320 --> 00:08:13,760
L'unité d'information standard est le bit, qui a une formule un peu amusante, 

171
00:08:13,760 --> 00:08:16,980
mais qui est vraiment intuitive si l'on regarde simplement des exemples. 

172
00:08:17,780 --> 00:08:21,212
Si vous avez une observation qui réduit de moitié votre espace des possibles, 

173
00:08:21,212 --> 00:08:23,500
on dit qu’elle ne contient qu’un bit d’information. 

174
00:08:24,180 --> 00:08:27,219
Dans notre exemple, l'espace des possibilités est constitué de tous les mots possibles, 

175
00:08:27,219 --> 00:08:29,636
et il s'avère qu'environ la moitié des mots de cinq lettres ont un S, 

176
00:08:29,636 --> 00:08:31,260
un peu moins que cela, mais environ la moitié. 

177
00:08:31,780 --> 00:08:34,320
Cette observation vous donnerait donc une information. 

178
00:08:34,880 --> 00:08:38,943
Si, au contraire, un nouveau fait réduit cet espace de possibilités d’un facteur quatre, 

179
00:08:38,943 --> 00:08:41,500
nous disons qu’il contient deux éléments d’information. 

180
00:08:41,980 --> 00:08:44,460
Par exemple, il s’avère qu’environ un quart de ces mots ont un T. 

181
00:08:45,020 --> 00:08:47,963
Si l’observation divise cet espace par huit, nous disons qu’il 

182
00:08:47,963 --> 00:08:50,720
s’agit de trois éléments d’information, et ainsi de suite. 

183
00:08:50,900 --> 00:08:55,060
Quatre bits le coupent en 16ème, cinq bits le coupent en 32ème. 

184
00:08:55,060 --> 00:08:57,587
Alors maintenant, vous voudrez peut-être faire une pause et 

185
00:08:57,587 --> 00:09:00,368
vous demander quelle est la formule pour obtenir des informations 

186
00:09:00,368 --> 00:09:02,980
sur le nombre de bits en termes de probabilité d'occurrence ? 

187
00:09:03,920 --> 00:09:07,658
Ce que nous disons ici, c'est que lorsque vous prenez la moitié du nombre de bits, 

188
00:09:07,658 --> 00:09:09,910
cela équivaut à la même chose que la probabilité, 

189
00:09:09,910 --> 00:09:13,784
ce qui revient à dire que deux puissance du nombre de bits est un sur la probabilité, 

190
00:09:13,784 --> 00:09:17,523
ce qui se réorganise en disant que l'information est la base logarithmique deux de 

191
00:09:17,523 --> 00:09:18,920
un divisée par la probabilité. 

192
00:09:19,620 --> 00:09:22,353
Et parfois, vous voyez cela avec encore un réarrangement supplémentaire, 

193
00:09:22,353 --> 00:09:24,900
où l'information est le log négatif en base deux de la probabilité. 

194
00:09:25,660 --> 00:09:28,539
Exprimé ainsi, cela peut paraître un peu bizarre aux non-initiés, 

195
00:09:28,539 --> 00:09:31,418
mais il s'agit en réalité de l'idée très intuitive de se demander 

196
00:09:31,418 --> 00:09:34,080
combien de fois vous avez réduit de moitié vos possibilités. 

197
00:09:35,180 --> 00:09:36,429
Maintenant, si vous vous demandez, vous savez, 

198
00:09:36,429 --> 00:09:38,024
je pensais que nous jouions juste à un jeu de mots amusant, 

199
00:09:38,024 --> 00:09:39,300
pourquoi les logarithmes entrent-ils en scène ? 

200
00:09:39,780 --> 00:09:42,948
L'une des raisons pour lesquelles cette unité est plus intéressante est 

201
00:09:42,948 --> 00:09:46,117
qu'il est beaucoup plus facile de parler d'événements très improbables, 

202
00:09:46,117 --> 00:09:49,594
beaucoup plus facile de dire qu'une observation contient 20 bits d'information 

203
00:09:49,594 --> 00:09:52,940
que de dire que la probabilité que tel ou tel se produise est de 0.0000095. 

204
00:09:53,300 --> 00:09:55,832
Mais une raison plus importante pour laquelle cette expression 

205
00:09:55,832 --> 00:09:58,646
logarithmique s’est avérée être un complément très utile à la théorie 

206
00:09:58,646 --> 00:10:01,460
des probabilités est la manière dont les informations s’additionnent. 

207
00:10:02,060 --> 00:10:05,249
Par exemple, si une observation vous donne deux bits d'information, 

208
00:10:05,249 --> 00:10:08,954
réduisant votre espace de quatre, puis qu'une deuxième observation comme votre 

209
00:10:08,954 --> 00:10:12,518
deuxième estimation dans Wordle vous donne trois autres bits d'information, 

210
00:10:12,518 --> 00:10:16,740
vous réduisant encore d'un facteur huit, le deux ensemble vous donnent cinq informations. 

211
00:10:17,160 --> 00:10:19,627
De la même manière que les probabilités aiment se multiplier, 

212
00:10:19,627 --> 00:10:21,020
les informations aiment s’ajouter. 

213
00:10:21,960 --> 00:10:24,821
Ainsi, dès que nous sommes dans le domaine de quelque chose comme une valeur attendue, 

214
00:10:24,821 --> 00:10:26,828
où nous additionnons un tas de nombres, les journaux rendent 

215
00:10:26,828 --> 00:10:27,980
la gestion beaucoup plus agréable. 

216
00:10:28,480 --> 00:10:31,877
Revenons à notre distribution pour Weary et ajoutons un autre petit tracker ici, 

217
00:10:31,877 --> 00:10:34,940
nous montrant la quantité d'informations disponibles pour chaque modèle. 

218
00:10:35,580 --> 00:10:37,911
La principale chose que je veux que vous remarquiez est que plus la 

219
00:10:37,911 --> 00:10:40,791
probabilité est élevée à mesure que nous arrivons à ces modèles les plus probables, 

220
00:10:40,791 --> 00:10:42,780
plus l'information est faible, moins vous gagnez de bits. 

221
00:10:43,500 --> 00:10:46,140
La façon dont nous mesurons la qualité de cette supposition sera de 

222
00:10:46,140 --> 00:10:49,323
prendre la valeur attendue de cette information, où nous examinons chaque modèle, 

223
00:10:49,323 --> 00:10:51,963
nous disons quelle est sa probabilité, puis nous la multiplions par 

224
00:10:51,963 --> 00:10:54,060
le nombre d'éléments d'information que nous obtenons. 

225
00:10:54,710 --> 00:10:58,120
Et dans l’exemple de Weary, cela s’avère être 4.9 bits. 

226
00:10:58,560 --> 00:11:02,188
Ainsi, en moyenne, les informations que vous obtenez de cette supposition d’ouverture 

227
00:11:02,188 --> 00:11:05,480
équivaut à réduire de moitié votre espace des possibilités environ cinq fois. 

228
00:11:05,960 --> 00:11:08,479
En revanche, un exemple de supposition avec une valeur 

229
00:11:08,479 --> 00:11:11,640
d’information attendue plus élevée serait quelque chose comme Slate. 

230
00:11:13,120 --> 00:11:15,620
Dans ce cas, vous remarquerez que la distribution semble beaucoup plus plate. 

231
00:11:15,940 --> 00:11:20,600
En particulier, l'occurrence la plus probable de tous les gris n'a qu'environ 6 % de 

232
00:11:20,600 --> 00:11:25,260
chances de se produire, donc au minimum vous en obtenez évidemment 3.9 informations. 

233
00:11:25,920 --> 00:11:28,560
Mais c’est un minimum, vous obtiendrez généralement quelque chose de mieux que cela. 

234
00:11:29,100 --> 00:11:32,228
Et il s'avère que lorsque vous analysez les chiffres sur celui-ci et 

235
00:11:32,228 --> 00:11:35,900
additionnez tous les termes pertinents, l'information moyenne est d'environ 5.8. 

236
00:11:37,360 --> 00:11:40,404
Ainsi, contrairement à Weary, votre espace de possibilités sera en 

237
00:11:40,404 --> 00:11:43,540
moyenne environ deux fois plus grand après cette première hypothèse. 

238
00:11:44,420 --> 00:11:46,791
Il existe en fait une histoire amusante sur le nom de 

239
00:11:46,791 --> 00:11:49,120
cette valeur attendue de la quantité d'informations. 

240
00:11:49,200 --> 00:11:51,874
La théorie de l'information a été développée par Claude Shannon, 

241
00:11:51,874 --> 00:11:54,014
qui travaillait aux Bell Labs dans les années 1940, 

242
00:11:54,014 --> 00:11:57,593
mais il discutait de certaines de ses idées encore non publiées avec John von Neumann, 

243
00:11:57,593 --> 00:11:59,980
qui était ce géant intellectuel de l'époque, très en vue. 

244
00:11:59,980 --> 00:12:03,560
en mathématiques et en physique et les débuts de ce qui allait devenir l'informatique. 

245
00:12:04,100 --> 00:12:07,363
Et lorsqu'il a mentionné qu'il n'avait pas vraiment un bon nom pour cette 

246
00:12:07,363 --> 00:12:10,451
valeur attendue de la quantité d'information, von Neumann aurait dit, 

247
00:12:10,451 --> 00:12:14,200
selon l'histoire, eh bien, vous devriez appeler cela entropie, et pour deux raisons. 

248
00:12:14,540 --> 00:12:17,373
En premier lieu, votre fonction d'incertitude a été utilisée en 

249
00:12:17,373 --> 00:12:20,030
mécanique statistique sous ce nom, donc elle a déjà un nom, 

250
00:12:20,030 --> 00:12:22,111
et en deuxième lieu, et plus important encore, 

251
00:12:22,111 --> 00:12:24,324
personne ne sait ce qu'est réellement l'entropie, 

252
00:12:24,324 --> 00:12:26,760
donc dans un débat vous aurez toujours ont l'avantage. 

253
00:12:27,700 --> 00:12:30,928
Donc, si le nom semble un peu mystérieux, et si l’on en croit cette histoire, 

254
00:12:30,928 --> 00:12:32,460
c’est en quelque sorte intentionnel. 

255
00:12:33,280 --> 00:12:36,531
De plus, si vous vous interrogez sur sa relation avec toutes ces deuxièmes lois 

256
00:12:36,531 --> 00:12:39,539
de la thermodynamique issues de la physique, il y a certainement un lien, 

257
00:12:39,539 --> 00:12:42,791
mais à l'origine, Shannon ne traitait que de la théorie des probabilités pures, 

258
00:12:42,791 --> 00:12:45,230
et pour nos besoins ici, lorsque j'utilise le mot entropie, 

259
00:12:45,230 --> 00:12:48,523
je veux juste que vous réfléchissiez à la valeur informationnelle attendue d'une 

260
00:12:48,523 --> 00:12:49,580
supposition particulière. 

261
00:12:50,700 --> 00:12:53,780
Vous pouvez considérer l’entropie comme la mesure de deux choses simultanément. 

262
00:12:54,240 --> 00:12:56,780
La première concerne la platitude de la distribution. 

263
00:12:57,320 --> 00:13:01,120
Plus une distribution est proche de l’uniforme, plus cette entropie sera élevée. 

264
00:13:01,580 --> 00:13:04,585
Dans notre cas, où il y a 3 modèles au total sur 5, 

265
00:13:04,585 --> 00:13:08,399
pour une distribution uniforme, l'observation de l'un d'entre eux 

266
00:13:08,399 --> 00:13:13,023
aurait un journal d'informations de base 2 sur 3 au 5, qui se trouve être 7.92, 

267
00:13:13,023 --> 00:13:17,300
c'est donc le maximum absolu que vous pourriez avoir pour cette entropie. 

268
00:13:17,840 --> 00:13:19,921
Mais l’entropie est aussi en quelque sorte une mesure 

269
00:13:19,921 --> 00:13:22,080
du nombre de possibilités qui existent en premier lieu. 

270
00:13:22,320 --> 00:13:26,801
Par exemple, si vous avez un mot dans lequel il n'y a que 16 modèles possibles, 

271
00:13:26,801 --> 00:13:31,171
et chacun est également probable, cette entropie, cette information attendue, 

272
00:13:31,171 --> 00:13:32,180
serait de 4 bits. 

273
00:13:32,579 --> 00:13:36,920
Mais si vous avez un autre mot où il y a 64 modèles possibles qui pourraient apparaître, 

274
00:13:36,920 --> 00:13:40,480
et ils sont tous également probables, alors l'entropie serait de 6 bits. 

275
00:13:41,500 --> 00:13:45,313
Donc, si vous voyez une distribution dans la nature qui a une entropie de 6 bits, 

276
00:13:45,313 --> 00:13:49,313
c'est un peu comme si cela disait qu'il y a autant de variation et d'incertitude dans 

277
00:13:49,313 --> 00:13:53,500
ce qui est sur le point de se produire que s'il y avait 64 résultats également probables. 

278
00:13:54,360 --> 00:13:59,320
Pour mon premier passage au Wurtelebot, je l'ai fait faire comme ça. 

279
00:13:59,320 --> 00:14:02,719
Il passe en revue toutes les suppositions possibles que vous pourriez avoir, 

280
00:14:02,719 --> 00:14:06,339
les 13 000 mots, calcule l'entropie pour chacun d'entre eux, ou plus précisément, 

281
00:14:06,339 --> 00:14:09,915
l'entropie de la distribution à travers tous les modèles que vous pourriez voir, 

282
00:14:09,915 --> 00:14:12,166
pour chacun d'entre eux, et choisit le plus élevé, 

283
00:14:12,166 --> 00:14:16,140
puisque c'est celui qui est susceptible de réduire au maximum votre espace des possibles. 

284
00:14:17,140 --> 00:14:19,169
Et même si je n’ai parlé ici que de la première supposition, 

285
00:14:19,169 --> 00:14:21,100
cela fait la même chose pour les prochaines suppositions. 

286
00:14:21,560 --> 00:14:24,349
Par exemple, après avoir vu un modèle sur cette première supposition, 

287
00:14:24,349 --> 00:14:27,656
qui vous limiterait à un plus petit nombre de mots possibles en fonction de ce qui 

288
00:14:27,656 --> 00:14:31,082
correspond à cela, vous jouez simplement au même jeu en ce qui concerne ce plus petit 

289
00:14:31,082 --> 00:14:31,800
ensemble de mots. 

290
00:14:32,260 --> 00:14:36,239
Pour une seconde supposition proposée, vous examinez la distribution de tous les modèles 

291
00:14:36,239 --> 00:14:39,503
qui pourraient survenir à partir de cet ensemble plus restreint de mots, 

292
00:14:39,503 --> 00:14:43,392
vous recherchez parmi les 13 000 possibilités et vous trouvez celle qui maximise cette 

293
00:14:43,392 --> 00:14:43,840
entropie. 

294
00:14:45,420 --> 00:14:47,703
Pour vous montrer comment cela fonctionne en action, 

295
00:14:47,703 --> 00:14:50,590
permettez-moi de vous présenter une petite variante de Wurtele que 

296
00:14:50,590 --> 00:14:54,080
j'ai écrite et qui montre les points saillants de cette analyse dans les marges. 

297
00:14:54,080 --> 00:14:56,597
Après avoir effectué tous ses calculs d'entropie, à droite, 

298
00:14:56,597 --> 00:14:59,660
il nous montre lesquels ont les informations attendues les plus élevées. 

299
00:15:00,280 --> 00:15:03,890
Il s'avère que la première réponse, du moins pour le moment, 

300
00:15:03,890 --> 00:15:08,981
nous affinerons cela plus tard, est Tares, ce qui signifie, euh, bien sûr, une vesce, 

301
00:15:08,981 --> 00:15:10,580
la vesce la plus courante. 

302
00:15:11,040 --> 00:15:12,893
Chaque fois que nous faisons une supposition ici, 

303
00:15:12,893 --> 00:15:15,821
où peut-être j'ignore en quelque sorte ses recommandations et opte pour Slate, 

304
00:15:15,821 --> 00:15:19,119
parce que j'aime Slate, nous pouvons voir combien d'informations attendues il contenait, 

305
00:15:19,119 --> 00:15:21,788
mais ensuite à droite du mot ici, cela nous montre combien informations 

306
00:15:21,788 --> 00:15:24,420
réelles que nous avons obtenues, compte tenu de ce modèle particulier. 

307
00:15:25,000 --> 00:15:28,057
Alors là, on dirait que nous n’avons pas eu de chance, on s’attendait à en avoir 5.8, 

308
00:15:28,057 --> 00:15:30,120
mais nous avons obtenu quelque chose avec moins que cela. 

309
00:15:30,600 --> 00:15:32,653
Et puis sur le côté gauche, ici, cela nous montre tous les 

310
00:15:32,653 --> 00:15:35,020
différents mots possibles donnés là où nous en sommes actuellement. 

311
00:15:35,800 --> 00:15:38,597
Les barres bleues nous indiquent la probabilité qu'il pense à chaque mot, 

312
00:15:38,597 --> 00:15:41,734
donc pour le moment, il suppose que chaque mot a la même probabilité d'apparaître, 

313
00:15:41,734 --> 00:15:43,360
mais nous affinerons cela dans un instant. 

314
00:15:44,060 --> 00:15:47,966
Et puis cette mesure d'incertitude nous indique l'entropie de cette distribution parmi 

315
00:15:47,966 --> 00:15:50,122
les mots possibles, ce qui, à l'heure actuelle, 

316
00:15:50,122 --> 00:15:52,277
parce qu'il s'agit d'une distribution uniforme, 

317
00:15:52,277 --> 00:15:55,960
n'est qu'une manière inutilement compliquée de compter le nombre de possibilités. 

318
00:15:56,560 --> 00:15:59,312
Par exemple, si nous prenons 2 puissance 13.66, 

319
00:15:59,312 --> 00:16:02,180
cela devrait être autour des 13 000 possibilités. 

320
00:16:02,900 --> 00:16:04,486
Je suis un peu en retrait ici, mais uniquement 

321
00:16:04,486 --> 00:16:06,140
parce que je n'affiche pas toutes les décimales. 

322
00:16:06,720 --> 00:16:09,513
Pour le moment, cela peut sembler redondant et compliquer excessivement les choses, 

323
00:16:09,513 --> 00:16:12,340
mais vous comprendrez pourquoi il est utile d'avoir les deux chiffres en une minute. 

324
00:16:12,760 --> 00:16:16,197
Donc, ici, il semble que cela suggère que l'entropie la plus élevée pour notre deuxième 

325
00:16:16,197 --> 00:16:19,400
hypothèse est Ramen, ce qui, encore une fois, ne ressemble vraiment pas à un mot. 

326
00:16:19,980 --> 00:16:22,261
Donc, pour prendre le dessus sur le plan moral ici, 

327
00:16:22,261 --> 00:16:24,060
je vais aller de l'avant et taper Rains. 

328
00:16:25,440 --> 00:16:27,340
Et encore une fois, on dirait que nous n’avons pas eu de chance. 

329
00:16:27,520 --> 00:16:31,360
Nous en attendions 4.3 bits et nous n’en avons que 3.39 bits d'informations. 

330
00:16:31,940 --> 00:16:33,940
Cela nous ramène donc à 55 possibilités. 

331
00:16:34,900 --> 00:16:37,400
Et ici, je vais peut-être simplement suivre ce que cela suggère, 

332
00:16:37,400 --> 00:16:39,440
à savoir un combo, peu importe ce que cela signifie. 

333
00:16:40,040 --> 00:16:42,920
Et d'accord, c'est en fait une bonne occasion de résoudre un casse-tête. 

334
00:16:42,920 --> 00:16:46,380
Cela nous dit que ce modèle nous donne 4.7 informations. 

335
00:16:47,060 --> 00:16:51,720
Mais sur la gauche, avant de voir ce schéma, il y en avait 5.78 bits d'incertitude. 

336
00:16:52,420 --> 00:16:54,320
Alors, comme quiz pour vous, qu'est-ce que cela 

337
00:16:54,320 --> 00:16:56,340
signifie sur le nombre de possibilités restantes ? 

338
00:16:58,040 --> 00:17:01,711
Eh bien, cela signifie que nous en sommes réduits à un peu d’incertitude, 

339
00:17:01,711 --> 00:17:04,540
ce qui revient à dire qu’il y a deux réponses possibles. 

340
00:17:04,700 --> 00:17:05,700
C'est un choix 50-50. 

341
00:17:06,500 --> 00:17:09,083
Et à partir de là, parce que vous et moi savons quels mots sont les plus courants, 

342
00:17:09,083 --> 00:17:10,640
nous savons que la réponse devrait être abyssale. 

343
00:17:11,180 --> 00:17:13,280
Mais tel qu’il est écrit actuellement, le programme ne le sait pas. 

344
00:17:13,540 --> 00:17:16,787
Alors il continue, essayant d'obtenir autant d'informations que possible, 

345
00:17:16,787 --> 00:17:19,859
jusqu'à ce qu'il ne reste plus qu'une possibilité, puis il la devine. 

346
00:17:20,380 --> 00:17:22,339
Nous avons donc évidemment besoin d’une meilleure stratégie de fin de partie. 

347
00:17:22,599 --> 00:17:25,319
Mais disons que nous appelons cette version l'un de nos solveurs de mots, 

348
00:17:25,319 --> 00:17:28,260
puis que nous exécutons quelques simulations pour voir comment cela fonctionne. 

349
00:17:30,360 --> 00:17:34,120
Donc, la façon dont cela fonctionne est de jouer à tous les jeux de mots possibles. 

350
00:17:34,240 --> 00:17:38,540
Il passe en revue tous ces 2315 mots qui sont les véritables réponses aux mots. 

351
00:17:38,540 --> 00:17:40,580
Il s’agit essentiellement de l’utiliser comme ensemble de tests. 

352
00:17:41,360 --> 00:17:44,154
Et avec cette méthode naïve qui consiste à ne pas considérer à quel point 

353
00:17:44,154 --> 00:17:46,874
un mot est courant et à essayer simplement de maximiser l'information à 

354
00:17:46,874 --> 00:17:49,820
chaque étape du processus, jusqu'à ce qu'il s'agisse d'un et d'un seul choix. 

355
00:17:50,360 --> 00:17:54,300
À la fin de la simulation, le score moyen s’élève à environ 4.124. 

356
00:17:55,319 --> 00:17:59,240
Ce qui n’est pas mal, pour être honnête, je m’attendais à faire pire. 

357
00:17:59,660 --> 00:18:02,600
Mais les gens qui jouent aux mots vous diront qu’ils peuvent généralement l’obtenir en 4. 

358
00:18:02,860 --> 00:18:05,380
Le véritable défi est d’en obtenir autant en 3 que possible. 

359
00:18:05,380 --> 00:18:08,080
C'est un écart assez important entre le score de 4 et le score de 3. 

360
00:18:08,860 --> 00:18:11,990
Le fruit évident ici est d'incorporer d'une manière ou d'une autre 

361
00:18:11,990 --> 00:18:14,980
si un mot est courant ou non, et comment faire exactement cela. 

362
00:18:22,800 --> 00:18:25,360
La façon dont je l'ai abordé consiste à obtenir une liste des 

363
00:18:25,360 --> 00:18:27,880
fréquences relatives de tous les mots de la langue anglaise. 

364
00:18:28,220 --> 00:18:31,425
Et je viens d'utiliser la fonction de données de fréquence des mots de Mathematica, 

365
00:18:31,425 --> 00:18:34,860
qui elle-même est extraite de l'ensemble de données publiques Google Books English Ngram. 

366
00:18:35,460 --> 00:18:37,655
Et c'est plutôt amusant à regarder, par exemple si nous les 

367
00:18:37,655 --> 00:18:39,960
trions des mots les plus courants aux mots les moins courants. 

368
00:18:40,120 --> 00:18:41,696
De toute évidence, ce sont les mots de 5 lettres 

369
00:18:41,696 --> 00:18:43,080
les plus courants dans la langue anglaise. 

370
00:18:43,700 --> 00:18:45,840
Ou plutôt, c'est le 8ème plus courant. 

371
00:18:46,280 --> 00:18:48,880
Le premier est lequel, puis il y a là et là. 

372
00:18:49,260 --> 00:18:51,511
Premier en lui-même n'est pas premier, mais 9ème, 

373
00:18:51,511 --> 00:18:54,437
et il est logique que ces autres mots apparaissent plus souvent, 

374
00:18:54,437 --> 00:18:57,544
là où ceux qui suivent premier sont après, où et ceux-ci étant juste 

375
00:18:57,544 --> 00:18:58,580
un peu moins courants. 

376
00:18:59,160 --> 00:19:02,988
Désormais, en utilisant ces données pour modéliser la probabilité que chacun de ces mots 

377
00:19:02,988 --> 00:19:06,860
soit la réponse finale, cela ne devrait pas être simplement proportionnel à la fréquence. 

378
00:19:06,860 --> 00:19:10,934
Par exemple, à qui on attribue une note de 0.002 dans cet ensemble de données, 

379
00:19:10,934 --> 00:19:15,060
alors que le mot tresse est en quelque sorte environ 1 000 fois moins probable. 

380
00:19:15,560 --> 00:19:17,289
Mais ces deux mots sont suffisamment courants pour qu’ils 

381
00:19:17,289 --> 00:19:18,840
valent certainement la peine d’être pris en compte. 

382
00:19:19,340 --> 00:19:21,000
Nous voulons donc davantage un seuil binaire. 

383
00:19:21,860 --> 00:19:25,782
La façon dont j'ai procédé est d'imaginer prendre toute cette liste triée de mots, 

384
00:19:25,782 --> 00:19:29,421
puis de la disposer sur un axe des x, puis d'appliquer la fonction sigmoïde, 

385
00:19:29,421 --> 00:19:33,439
qui est la manière standard d'avoir une fonction dont la sortie est fondamentalement 

386
00:19:33,439 --> 00:19:37,551
binaire, c'est soit 0, soit 1, mais il y a un lissage entre les deux pour cette région 

387
00:19:37,551 --> 00:19:38,260
d'incertitude. 

388
00:19:39,160 --> 00:19:42,177
Donc, essentiellement, la probabilité que j'attribue à chaque mot 

389
00:19:42,177 --> 00:19:45,742
d'être dans la liste finale sera la valeur de la fonction sigmoïde ci-dessus, 

390
00:19:45,742 --> 00:19:48,440
quel que soit l'endroit où elle se trouve sur l'axe des x. 

391
00:19:49,520 --> 00:19:51,882
Cela dépend évidemment de quelques paramètres, 

392
00:19:51,882 --> 00:19:55,098
par exemple la largeur de l'espace sur l'axe des x que ces mots 

393
00:19:55,098 --> 00:19:59,621
remplissent détermine la façon dont nous passons progressivement ou abruptement de 1 à 0, 

394
00:19:59,621 --> 00:20:03,240
et l'endroit où nous les situons de gauche à droite détermine le seuil. 

395
00:20:03,240 --> 00:20:05,024
Pour être honnête, j’ai simplement fait cela en 

396
00:20:05,024 --> 00:20:06,920
me léchant le doigt et en le mettant face au vent. 

397
00:20:07,140 --> 00:20:10,307
J'ai parcouru la liste triée et essayé de trouver une fenêtre dans laquelle, 

398
00:20:10,307 --> 00:20:13,722
lorsque je l'ai regardée, j'ai pensé qu'environ la moitié de ces mots étaient plus 

399
00:20:13,722 --> 00:20:17,260
susceptibles qu'improbables d'être la réponse finale, et je l'ai utilisé comme seuil. 

400
00:20:17,260 --> 00:20:20,055
Une fois que nous avons une distribution comme celle-ci entre les mots, 

401
00:20:20,055 --> 00:20:23,238
cela nous donne une autre situation dans laquelle l'entropie devient cette mesure 

402
00:20:23,238 --> 00:20:23,860
vraiment utile. 

403
00:20:24,500 --> 00:20:27,339
Par exemple, disons que nous jouons à un jeu et que nous commençons avec mes 

404
00:20:27,339 --> 00:20:29,552
anciens premiers mots, qui étaient une plume et des ongles, 

405
00:20:29,552 --> 00:20:32,465
et que nous nous retrouvons avec une situation où il y a quatre mots possibles 

406
00:20:32,465 --> 00:20:33,240
qui y correspondent. 

407
00:20:33,560 --> 00:20:35,620
Et disons que nous les considérons tous également probables. 

408
00:20:36,220 --> 00:20:38,880
Laissez-moi vous demander, quelle est l'entropie de cette distribution ? 

409
00:20:41,080 --> 00:20:45,334
Eh bien, les informations associées à chacune de ces possibilités 

410
00:20:45,334 --> 00:20:50,040
seront la base log 2 sur 4, puisque chacune vaut 1 et 4, et cela fait 2. 

411
00:20:50,040 --> 00:20:52,460
Deux informations, quatre possibilités. 

412
00:20:52,760 --> 00:20:53,580
Tout cela est très bien. 

413
00:20:54,300 --> 00:20:57,800
Mais et si je vous disais qu'en réalité il y a plus de quatre matches ? 

414
00:20:58,260 --> 00:21:00,932
En réalité, lorsque nous parcourons la liste complète de mots, 

415
00:21:00,932 --> 00:21:02,460
il y a 16 mots qui y correspondent. 

416
00:21:02,580 --> 00:21:05,474
Mais supposons que notre modèle attribue une très faible probabilité 

417
00:21:05,474 --> 00:21:07,907
à ces 12 autres mots d'être réellement la réponse finale, 

418
00:21:07,907 --> 00:21:10,760
quelque chose comme 1 sur 1 000 parce qu'ils sont vraiment obscurs. 

419
00:21:11,500 --> 00:21:14,260
Maintenant, laissez-moi vous demander, quelle est l'entropie de cette distribution ? 

420
00:21:15,420 --> 00:21:18,526
Si l'entropie mesurait uniquement le nombre de correspondances ici, 

421
00:21:18,526 --> 00:21:22,227
alors vous pourriez vous attendre à ce qu'elle ressemble à la base logarithmique 

422
00:21:22,227 --> 00:21:25,700
2 sur 16, qui serait 4, soit deux bits d'incertitude de plus qu'auparavant. 

423
00:21:26,180 --> 00:21:28,101
Mais bien entendu, l’incertitude réelle n’est pas vraiment 

424
00:21:28,101 --> 00:21:29,860
différente de celle que nous connaissions auparavant. 

425
00:21:30,160 --> 00:21:33,876
Ce n'est pas parce qu'il y a ces 12 mots vraiment obscurs qu'il serait d'autant 

426
00:21:33,876 --> 00:21:37,360
plus surprenant d'apprendre que la réponse finale est charme, par exemple. 

427
00:21:38,180 --> 00:21:40,613
Ainsi, lorsque vous effectuez réellement le calcul ici et que 

428
00:21:40,613 --> 00:21:43,126
vous additionnez la probabilité de chaque occurrence multipliée 

429
00:21:43,126 --> 00:21:45,560
par les informations correspondantes, vous obtenez 2.11 bits. 

430
00:21:45,560 --> 00:21:47,991
Je dis juste qu'il s'agit essentiellement de deux éléments, 

431
00:21:47,991 --> 00:21:51,637
essentiellement de ces quatre possibilités, mais il y a un peu plus d'incertitude à cause 

432
00:21:51,637 --> 00:21:54,757
de tous ces événements hautement improbables, même si si vous les appreniez, 

433
00:21:54,757 --> 00:21:56,500
vous en tireriez une tonne d'informations. 

434
00:21:57,160 --> 00:21:59,234
Donc, en effectuant un zoom arrière, cela fait partie de ce qui fait 

435
00:21:59,234 --> 00:22:01,400
de Wordle un si bel exemple pour une leçon de théorie de l'information. 

436
00:22:01,600 --> 00:22:04,640
Nous avons ces deux applications distinctes de sensation pour l’entropie. 

437
00:22:05,160 --> 00:22:08,264
Le premier nous dit quelle est l'information attendue que nous 

438
00:22:08,264 --> 00:22:11,763
obtiendrons à partir d'une supposition donnée, et le second nous dit : 

439
00:22:11,763 --> 00:22:15,460
pouvons-nous mesurer l'incertitude restante parmi tous les mots possibles. 

440
00:22:16,460 --> 00:22:19,127
Et je dois souligner que, dans le premier cas où nous examinons les 

441
00:22:19,127 --> 00:22:21,715
informations attendues d'une supposition, une fois que nous avons 

442
00:22:21,715 --> 00:22:24,540
une pondération inégale des mots, cela affecte le calcul de l'entropie. 

443
00:22:24,980 --> 00:22:28,000
Par exemple, permettez-moi de reprendre le même cas que nous avons examiné 

444
00:22:28,000 --> 00:22:30,175
plus tôt concernant la distribution associée à Weary, 

445
00:22:30,175 --> 00:22:33,720
mais cette fois en utilisant une distribution non uniforme sur tous les mots possibles. 

446
00:22:34,500 --> 00:22:38,280
Alors laissez-moi voir si je peux trouver ici une partie qui l’illustre assez bien. 

447
00:22:40,940 --> 00:22:42,360
Ok, ici, c'est plutôt bien. 

448
00:22:42,360 --> 00:22:45,884
Ici, nous avons deux modèles adjacents qui sont à peu près également probables, 

449
00:22:45,884 --> 00:22:49,100
mais l'un d'eux, nous dit-on, a 32 mots possibles qui lui correspondent. 

450
00:22:49,280 --> 00:22:51,955
Et si nous vérifions ce qu’ils sont, ce sont ces 32 mots, 

451
00:22:51,955 --> 00:22:55,600
qui ne sont que des mots très improbables lorsque vous les parcourez des yeux. 

452
00:22:55,840 --> 00:22:59,415
Il est difficile de trouver des réponses qui semblent plausibles, peut-être des cris, 

453
00:22:59,415 --> 00:23:01,993
mais si nous regardons le modèle voisin dans la distribution, 

454
00:23:01,993 --> 00:23:05,112
qui est considéré comme tout aussi probable, on nous dit qu'il n'y a que 8 

455
00:23:05,112 --> 00:23:07,939
correspondances possibles, donc un quart comme de nombreux matches, 

456
00:23:07,939 --> 00:23:09,520
mais c'est à peu près aussi probable. 

457
00:23:09,860 --> 00:23:12,140
Et lorsque nous récupérons ces matchs, nous pouvons comprendre pourquoi. 

458
00:23:12,500 --> 00:23:14,808
Certaines d’entre elles sont des réponses réellement plausibles, 

459
00:23:14,808 --> 00:23:16,300
comme la sonnerie, la colère ou les raps. 

460
00:23:17,900 --> 00:23:19,717
Pour illustrer comment nous intégrons tout cela, 

461
00:23:19,717 --> 00:23:21,793
permettez-moi d'afficher ici la version 2 de Wordlebot, 

462
00:23:21,793 --> 00:23:24,204
et il y a deux ou trois différences principales par rapport à la 

463
00:23:24,204 --> 00:23:25,280
première que nous avons vue. 

464
00:23:25,860 --> 00:23:29,801
Tout d'abord, comme je viens de le dire, la façon dont nous calculons ces entropies, 

465
00:23:29,801 --> 00:23:33,556
ces valeurs attendues de l'information, utilise désormais des distributions plus 

466
00:23:33,556 --> 00:23:37,683
raffinées entre les modèles qui intègrent la probabilité qu'un mot donné soit réellement 

467
00:23:37,683 --> 00:23:38,240
la réponse. 

468
00:23:38,879 --> 00:23:41,629
Il se trouve que les larmes sont toujours au premier rang, 

469
00:23:41,629 --> 00:23:43,820
même si les suivantes sont un peu différentes. 

470
00:23:44,360 --> 00:23:46,457
Deuxièmement, lorsqu'il classera ses meilleurs choix, 

471
00:23:46,457 --> 00:23:49,875
il conservera désormais un modèle de probabilité que chaque mot soit la réponse réelle, 

472
00:23:49,875 --> 00:23:52,477
et il l'intégrera dans sa décision, qui est plus facile à voir une 

473
00:23:52,477 --> 00:23:55,080
fois que nous avons quelques suppositions sur la réponse. tableau. 

474
00:23:55,860 --> 00:23:57,594
Encore une fois, nous ignorons sa recommandation, 

475
00:23:57,594 --> 00:23:59,780
car nous ne pouvons pas laisser les machines diriger nos vies. 

476
00:24:01,140 --> 00:24:04,096
Et je suppose que je devrais mentionner une autre chose différente ici, 

477
00:24:04,096 --> 00:24:06,478
à gauche, que la valeur d'incertitude, ce nombre de bits, 

478
00:24:06,478 --> 00:24:09,640
n'est plus seulement redondante avec le nombre de correspondances possibles. 

479
00:24:10,080 --> 00:24:13,688
Maintenant, si nous le retirons et calculons 2 puissance 8.02, 

480
00:24:13,688 --> 00:24:17,296
qui est un peu au-dessus de 256, je suppose 259, ce qu'il dit, 

481
00:24:17,296 --> 00:24:22,107
c'est que même s'il y a 526 mots au total qui correspondent réellement à ce modèle, 

482
00:24:22,107 --> 00:24:26,860
le degré d'incertitude qu'il a est plus proche de ce qu'il serait s'il y avait 259 

483
00:24:26,860 --> 00:24:28,980
mots également probables. résultats. 

484
00:24:29,720 --> 00:24:30,740
Vous pouvez y penser comme ceci. 

485
00:24:31,020 --> 00:24:33,964
Il sait que le borx n'est pas la réponse, pareil pour les yorts, 

486
00:24:33,964 --> 00:24:37,680
le zorl et le zorus, donc c'est un peu moins incertain que dans le cas précédent. 

487
00:24:37,820 --> 00:24:39,280
Ce nombre de bits sera plus petit. 

488
00:24:40,220 --> 00:24:43,234
Et si je continue à jouer au jeu, j'affine cela avec quelques 

489
00:24:43,234 --> 00:24:46,540
suppositions qui sont à propos de ce que je voudrais expliquer ici. 

490
00:24:48,360 --> 00:24:50,882
À la quatrième hypothèse, si vous regardez ses meilleurs choix, 

491
00:24:50,882 --> 00:24:53,760
vous pouvez voir qu'il ne s'agit plus seulement de maximiser l'entropie. 

492
00:24:54,460 --> 00:24:56,889
Donc à ce stade, il y a techniquement sept possibilités, 

493
00:24:56,889 --> 00:25:00,300
mais les seules qui ont une chance significative sont les dortoirs et les mots. 

494
00:25:00,300 --> 00:25:04,177
Et vous pouvez voir qu'il classe ces deux valeurs au-dessus de toutes ces autres valeurs, 

495
00:25:04,177 --> 00:25:06,720
qui, à proprement parler, donneraient plus d'informations. 

496
00:25:07,240 --> 00:25:09,297
La toute première fois que j'ai fait cela, j'ai simplement 

497
00:25:09,297 --> 00:25:11,912
additionné ces deux nombres pour mesurer la qualité de chaque supposition, 

498
00:25:11,912 --> 00:25:13,900
ce qui a en fait mieux fonctionné que vous ne le pensez. 

499
00:25:14,300 --> 00:25:15,727
Mais cela ne semblait vraiment pas systématique, 

500
00:25:15,727 --> 00:25:18,058
et je suis sûr qu'il existe d'autres approches que les gens pourraient adopter, 

501
00:25:18,058 --> 00:25:19,340
mais voici celle sur laquelle j'ai atterri. 

502
00:25:19,760 --> 00:25:22,654
Si nous envisageons la perspective d'une prochaine supposition, 

503
00:25:22,654 --> 00:25:25,277
comme dans ce cas les mots, ce qui nous importe vraiment, 

504
00:25:25,277 --> 00:25:27,900
c'est le score attendu de notre jeu si nous faisons cela. 

505
00:25:28,230 --> 00:25:31,938
Et pour calculer ce score attendu, nous disons quelle est la probabilité 

506
00:25:31,938 --> 00:25:35,900
que les mots soient la réponse réelle, ce qui est actuellement décrit à 58 %. 

507
00:25:36,040 --> 00:25:39,540
Nous disons qu'avec 58 % de chances, notre score dans ce jeu serait de 4. 

508
00:25:40,320 --> 00:25:45,640
Et puis avec la probabilité de 1 moins 58 %, notre score sera supérieur à 4. 

509
00:25:46,220 --> 00:25:49,278
Nous n’en savons pas encore plus, mais nous pouvons l’estimer en fonction 

510
00:25:49,278 --> 00:25:52,460
du degré d’incertitude qu’il y aura probablement une fois arrivé à ce point. 

511
00:25:52,960 --> 00:25:55,940
Concrètement, pour le moment, il y en a 1.44 bits d'incertitude. 

512
00:25:56,440 --> 00:25:58,563
Si nous devinons des mots, cela nous indique que 

513
00:25:58,563 --> 00:26:01,120
l'information attendue que nous obtiendrons est 1.27 bits. 

514
00:26:01,620 --> 00:26:04,597
Donc, si nous devinons les mots, cette différence représente le degré 

515
00:26:04,597 --> 00:26:07,660
d’incertitude qui nous restera probablement après que cela se produise. 

516
00:26:08,260 --> 00:26:10,723
Ce dont nous avons besoin, c'est d'une sorte de fonction, 

517
00:26:10,723 --> 00:26:13,740
que j'appelle ici f, qui associe cette incertitude à un score attendu. 

518
00:26:14,240 --> 00:26:18,347
Et la façon dont cela s'est déroulé consistait simplement à tracer un tas de données 

519
00:26:18,347 --> 00:26:22,454
des jeux précédents basés sur la version 1 du bot pour dire quel était le score réel 

520
00:26:22,454 --> 00:26:26,320
après différents points avec certaines quantités d'incertitude très mesurables. 

521
00:26:27,020 --> 00:26:31,280
Par exemple, ces points de données ici se situent au-dessus d’une valeur d’environ 8. 

522
00:26:31,280 --> 00:26:35,194
Environ 7 disent pour certains matchs après un point où il y en avait 8.7 bits 

523
00:26:35,194 --> 00:26:38,960
d'incertitude, il a fallu deux suppositions pour obtenir la réponse finale. 

524
00:26:39,320 --> 00:26:40,765
Pour les autres jeux, il fallait trois tentatives, 

525
00:26:40,765 --> 00:26:42,240
pour les autres jeux, il fallait quatre tentatives. 

526
00:26:43,140 --> 00:26:46,638
Si nous passons ici vers la gauche, tous les points au-dessus de zéro indiquent que 

527
00:26:46,638 --> 00:26:50,095
lorsqu'il n'y a aucun élément d'incertitude, c'est-à-dire qu'il n'y a qu'une seule 

528
00:26:50,095 --> 00:26:53,343
possibilité, alors le nombre de suppositions requis est toujours d'une seule, 

529
00:26:53,343 --> 00:26:54,260
ce qui est rassurant. 

530
00:26:54,780 --> 00:26:56,561
Chaque fois qu'il y avait un peu d'incertitude, 

531
00:26:56,561 --> 00:26:59,530
ce qui signifiait qu'il ne s'agissait essentiellement que de deux possibilités, 

532
00:26:59,530 --> 00:27:01,423
il fallait parfois une supposition supplémentaire, 

533
00:27:01,423 --> 00:27:03,020
parfois deux suppositions supplémentaires. 

534
00:27:03,080 --> 00:27:05,240
Et ainsi de suite, ici. 

535
00:27:05,740 --> 00:27:07,825
Un moyen un peu plus simple de visualiser ces données 

536
00:27:07,825 --> 00:27:10,220
consiste peut-être à les regrouper et à prendre des moyennes. 

537
00:27:11,000 --> 00:27:15,404
Par exemple, cette barre indique que parmi tous les points pour lesquels nous avions un 

538
00:27:15,404 --> 00:27:19,909
peu d'incertitude, le nombre moyen de nouvelles suppositions requises était d'environ 1.5.

539
00:27:19,909 --> 00:27:19,960
 

540
00:27:22,140 --> 00:27:24,898
Et la barre ici indique que parmi tous les différents jeux, 

541
00:27:24,898 --> 00:27:28,300
où à un moment donné l'incertitude était un peu supérieure à quatre bits, 

542
00:27:28,300 --> 00:27:31,840
ce qui revient à la réduire à 16 possibilités différentes, alors en moyenne, 

543
00:27:31,840 --> 00:27:35,380
cela nécessite un peu plus de deux suppositions à partir de ce point. avant. 

544
00:27:36,060 --> 00:27:37,775
Et à partir de là, j'ai juste fait une régression pour 

545
00:27:37,775 --> 00:27:39,460
adapter une fonction qui semblait raisonnable à cela. 

546
00:27:39,980 --> 00:27:43,020
Et rappelez-vous que l’intérêt de faire tout cela est de pouvoir 

547
00:27:43,020 --> 00:27:45,873
quantifier cette intuition selon laquelle plus nous obtenons 

548
00:27:45,873 --> 00:27:48,960
d’informations à partir d’un mot, plus le score attendu sera bas. 

549
00:27:49,680 --> 00:27:52,802
Donc avec ceci comme version 2.0, si nous revenons en arrière et 

550
00:27:52,802 --> 00:27:56,069
exécutons le même ensemble de simulations, en le faisant jouer avec 

551
00:27:56,069 --> 00:27:59,240
les 2315 réponses de mots possibles, comment cela se passe-t-il ? 

552
00:28:00,280 --> 00:28:01,900
Et bien contrairement à notre première version, 

553
00:28:01,900 --> 00:28:03,420
c'est nettement mieux, ce qui est rassurant. 

554
00:28:04,020 --> 00:28:06,480
Tout compte fait, la moyenne est d’environ 3.6, 

555
00:28:06,480 --> 00:28:10,325
bien que contrairement à la première version, il perd plusieurs fois et en 

556
00:28:10,325 --> 00:28:12,120
nécessite plus de six dans ce cas. 

557
00:28:12,639 --> 00:28:15,120
Vraisemblablement parce qu'il y a des moments où il faut faire un 

558
00:28:15,120 --> 00:28:17,940
compromis pour atteindre l'objectif plutôt que de maximiser l'information. 

559
00:28:19,040 --> 00:28:21,000
Alors pouvons-nous faire mieux que 3.6 ? 

560
00:28:22,080 --> 00:28:22,920
Nous le pouvons certainement. 

561
00:28:23,280 --> 00:28:26,411
Maintenant, j'ai dit au début qu'il était très amusant d'essayer de ne pas incorporer 

562
00:28:26,411 --> 00:28:29,360
la vraie liste de réponses en mots dans la manière dont il construit son modèle. 

563
00:28:29,880 --> 00:28:32,238
Mais si nous l’intégrons, la meilleure performance 

564
00:28:32,238 --> 00:28:34,180
que j’ai pu obtenir était d’environ 3.43. 

565
00:28:35,160 --> 00:28:37,735
Donc, si nous essayons d'être plus sophistiqués que d'utiliser simplement 

566
00:28:37,735 --> 00:28:40,380
les données de fréquence des mots pour choisir cette distribution a priori, 

567
00:28:40,380 --> 00:28:42,955
cette 3.43 donne probablement un maximum de la qualité que nous pourrions 

568
00:28:42,955 --> 00:28:45,740
obtenir avec cela, ou du moins de la qualité que je pourrais obtenir avec cela. 

569
00:28:46,240 --> 00:28:49,636
Cette meilleure performance utilise essentiellement les idées dont j'ai parlé ici, 

570
00:28:49,636 --> 00:28:52,501
mais elle va un peu plus loin, comme si elle effectuait une recherche 

571
00:28:52,501 --> 00:28:55,120
des informations attendues deux pas en avant plutôt qu'un seul. 

572
00:28:55,620 --> 00:28:57,591
Au départ, j'avais prévu d'en parler davantage, 

573
00:28:57,591 --> 00:29:00,220
mais je me rends compte que nous avons en fait été assez longs. 

574
00:29:00,580 --> 00:29:03,431
La seule chose que je dirai, c'est qu'après avoir effectué cette recherche en deux 

575
00:29:03,431 --> 00:29:06,282
étapes, puis exécuté quelques exemples de simulations sur les meilleurs candidats, 

576
00:29:06,282 --> 00:29:09,100
jusqu'à présent, pour moi au moins, il semble que Crane soit le meilleur ouvreur. 

577
00:29:09,100 --> 00:29:10,060
Qui l'aurait deviné ? 

578
00:29:10,920 --> 00:29:14,132
De plus, si vous utilisez la vraie liste de mots pour déterminer votre espace de 

579
00:29:14,132 --> 00:29:17,582
possibilités, alors l'incertitude avec laquelle vous commencez est d'un peu plus de 11 

580
00:29:17,582 --> 00:29:17,820
bits. 

581
00:29:18,300 --> 00:29:21,450
Et il s'avère que, rien qu'à partir d'une recherche par force brute, 

582
00:29:21,450 --> 00:29:25,012
le maximum d'informations attendues après les deux premières suppositions est 

583
00:29:25,012 --> 00:29:25,880
d'environ 10 bits. 

584
00:29:26,500 --> 00:29:30,628
Ce qui suggère que dans le meilleur des cas, après vos deux premières suppositions, 

585
00:29:30,628 --> 00:29:34,560
avec un jeu parfaitement optimal, il vous restera environ un peu d'incertitude. 

586
00:29:34,800 --> 00:29:37,960
Ce qui revient à se limiter à deux suppositions possibles. 

587
00:29:37,960 --> 00:29:41,072
Je pense donc qu'il est juste et probablement assez conservateur de dire que 

588
00:29:41,072 --> 00:29:44,103
vous ne pourrez jamais écrire un algorithme qui abaisse cette moyenne à 3, 

589
00:29:44,103 --> 00:29:47,175
car avec les mots dont vous disposez, il n'y a tout simplement pas de place 

590
00:29:47,175 --> 00:29:50,005
pour obtenir suffisamment d'informations après seulement deux étapes. 

591
00:29:50,005 --> 00:29:53,360
capable de garantir la réponse dans le troisième créneau à chaque fois sans faute. 

