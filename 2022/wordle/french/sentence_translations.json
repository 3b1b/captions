[
 {
  "input": "The game Wurdle has gone pretty viral in the last month or two, and never one to overlook an opportunity for a math lesson, it occurs to me that this game makes for a very good central example in a lesson about information theory, and in particular a topic known as entropy. ",
  "translatedText": "Le jeu Wurdle est devenu assez viral au cours des deux derniers mois, et n'a jamais négligé une opportunité de cours de mathématiques. Il me semble que ce jeu constitue un très bon exemple central dans une leçon sur la théorie de l'information, et en particulier un sujet connu sous le nom d’entropie. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 0.0,
  "end": 12.66
 },
 {
  "input": "You see, like a lot of people I got kind of sucked into the puzzle, and like a lot of programmers I also got sucked into trying to write an algorithm that would play the game as optimally as it could. ",
  "translatedText": "Vous voyez, comme beaucoup de gens, je me suis laissé entraîner dans le puzzle, et comme beaucoup de programmeurs, je me suis également laissé entraîner à essayer d'écrire un algorithme qui permettrait de jouer au jeu de la manière la plus optimale possible. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 13.92,
  "end": 22.74
 },
 {
  "input": "And what I thought I'd do here is just talk through with you some of my process in that, and explain some of the math that went into it, since the whole algorithm centers on this idea of entropy. ",
  "translatedText": "Et ce que j'ai pensé faire ici, c'est simplement parler avec vous de certains de mes processus et expliquer certains des calculs qui y sont liés, puisque tout l'algorithme est centré sur cette idée d'entropie. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 23.18,
  "end": 31.08
 },
 {
  "input": "First things first, in case you haven't heard of it, what is Wurdle? ",
  "translatedText": "Tout d’abord, au cas où vous n’en auriez pas entendu parler, qu’est-ce que Wurdle ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 38.7,
  "end": 41.64
 },
 {
  "input": "And to kill two birds with one stone here while we go through the rules of the game, let me also preview where we're going with this, which is to develop a little algorithm that will basically play the game for us. ",
  "translatedText": "Et pour faire d'une pierre deux coups pendant que nous examinons les règles du jeu, permettez-moi également de vous montrer où nous allons avec cela, c'est-à-dire développer un petit algorithme qui jouera essentiellement le jeu pour nous. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 42.04,
  "end": 51.04
 },
 {
  "input": "Though I haven't done today's Wurdle, this is February 4th, and we'll see how the bot does. ",
  "translatedText": "Bien que je n'aie pas fait le Wurdle d'aujourd'hui, nous sommes le 4 février et nous verrons comment le bot se comporte. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 51.36,
  "end": 55.1
 },
 {
  "input": "The goal of Wurdle is to guess a mystery five letter word, and you're given six different chances to guess. ",
  "translatedText": "Le but de Wurdle est de deviner un mot mystérieux de cinq lettres, et vous avez six chances différentes de le deviner. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 55.48,
  "end": 60.34
 },
 {
  "input": "For example, my Wurdle bot suggests that I start with the guess crane. ",
  "translatedText": "Par exemple, mon robot Wurdle me suggère de commencer par la grue à devinettes. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 60.84,
  "end": 64.38
 },
 {
  "input": "Each time that you make a guess, you get some information about how close your guess is to the true answer. ",
  "translatedText": "Chaque fois que vous faites une supposition, vous obtenez des informations sur la proximité de votre supposition avec la vraie réponse. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 65.18,
  "end": 70.22
 },
 {
  "input": "Here the grey box is telling me there's no C in the actual answer. ",
  "translatedText": "Ici, la case grise me dit qu'il n'y a pas de C dans la réponse réelle. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 70.92,
  "end": 74.1
 },
 {
  "input": "The yellow box is telling me there is an R, but it's not in that position. ",
  "translatedText": "La case jaune m'indique qu'il y a un R, mais il n'est pas dans cette position. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 74.52,
  "end": 77.84
 },
 {
  "input": "The green box is telling me that the secret word does have an A, and it's in the third position. ",
  "translatedText": "La case verte m'indique que le mot secret a un A et qu'il est en troisième position. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 78.24,
  "end": 82.24
 },
 {
  "input": "And then there's no N and there's no E. ",
  "translatedText": "Et puis il n’y a ni N ni E. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 82.72,
  "end": 84.58
 },
 {
  "input": "So let me just go in and tell the Wurdle bot that information. ",
  "translatedText": "Alors laissez-moi entrer et donner cette information au robot Wurdle. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 85.2,
  "end": 87.34
 },
 {
  "input": "We started with crane, we got grey, yellow, green, grey, grey. ",
  "translatedText": "Nous avons commencé avec la grue, nous avons eu du gris, du jaune, du vert, du gris, du gris. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 87.34,
  "end": 90.32
 },
 {
  "input": "Don't worry about all the data that it's showing right now, I'll explain that in due time. ",
  "translatedText": "Ne vous inquiétez pas de toutes les données qu'il affiche en ce moment, je vous l'expliquerai en temps voulu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 91.42,
  "end": 94.94
 },
 {
  "input": "But its top suggestion for our second pick is shtick. ",
  "translatedText": "Mais sa principale suggestion pour notre deuxième choix est shtick. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 95.46,
  "end": 98.82
 },
 {
  "input": "And your guess does have to be an actual five letter word, but as you'll see, it's pretty liberal with what it will actually let you guess. ",
  "translatedText": "Et votre supposition doit être un véritable mot de cinq lettres, mais comme vous le verrez, elle est assez libérale quant à ce qu'elle vous laissera réellement deviner. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 99.56,
  "end": 105.4
 },
 {
  "input": "In this case, we try shtick. ",
  "translatedText": "Dans ce cas, nous essayons shtick. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 106.2,
  "end": 107.44
 },
 {
  "input": "And alright, things are looking pretty good. ",
  "translatedText": "Et bien, les choses s’annoncent plutôt bien. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 108.78,
  "end": 110.18
 },
 {
  "input": "We hit the S and the H, so we know the first three letters, we know that there's an R. ",
  "translatedText": "On touche le S et le H, donc on connaît les trois premières lettres, on sait qu'il y a un R. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 110.26,
  "end": 113.98
 },
 {
  "input": "And so it's going to be like S-H-A something R, or S-H-A R something. ",
  "translatedText": "Et donc ça va être comme SHA quelque chose de R, ou SHA R quelque chose. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 113.98,
  "end": 118.7
 },
 {
  "input": "And it looks like the Wurdle bot knows that it's down to just two possibilities, either shard or sharp. ",
  "translatedText": "Et il semble que le robot Wurdle sache qu'il ne reste que deux possibilités, soit un fragment, soit un tranchant. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 119.62,
  "end": 124.24
 },
 {
  "input": "That's kind of a toss up between them at this point, so I guess probably just because it's alphabetical it goes with shard. ",
  "translatedText": "C'est une sorte de mélange entre eux à ce stade, donc je suppose que c'est probablement simplement parce que c'est par ordre alphabétique que cela va avec le fragment. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 125.1,
  "end": 130.08
 },
 {
  "input": "Which hooray, is the actual answer. ",
  "translatedText": "Quelle hourra, c'est la vraie réponse. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 131.22,
  "end": 132.86
 },
 {
  "input": "So we got it in three. ",
  "translatedText": "Nous l'avons donc eu en trois. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 132.96,
  "end": 133.78
 },
 {
  "input": "If you're wondering if that's any good, the way I heard one person phrase it is that with Wurdle four is par and three is birdie. ",
  "translatedText": "Si vous vous demandez si c'est bon, la façon dont j'ai entendu une personne dire qu'avec Wurdle, quatre est la normale et trois est un birdie. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 134.6,
  "end": 140.36
 },
 {
  "input": "Which I think is a pretty apt analogy. ",
  "translatedText": "Ce qui, je pense, est une analogie assez pertinente. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 140.68,
  "end": 142.48
 },
 {
  "input": "You have to be consistently on your game to be getting four, but it's certainly not crazy. ",
  "translatedText": "Il faut être constamment sur son jeu pour en obtenir quatre, mais ce n'est certainement pas fou. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 142.48,
  "end": 147.02
 },
 {
  "input": "But when you get it in three, it just feels great. ",
  "translatedText": "Mais quand vous l'obtenez en trois, ça fait du bien. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 147.18,
  "end": 149.92
 },
 {
  "input": "So if you're down for it, what I'd like to do here is just talk through my thought process from the beginning for how I approach the Wurdle bot. ",
  "translatedText": "Donc, si vous êtes partant, ce que j'aimerais faire ici, c'est simplement parler de mon processus de réflexion depuis le début sur la façon dont j'aborde le robot Wurdle. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 150.88,
  "end": 155.96
 },
 {
  "input": "And like I said, really it's an excuse for an information theory lesson. ",
  "translatedText": "Et comme je l'ai dit, c'est en réalité une excuse pour un cours de théorie de l'information. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 156.48,
  "end": 159.44
 },
 {
  "input": "The main goal is to explain what is information and what is entropy. ",
  "translatedText": "L’objectif principal est d’expliquer ce qu’est l’information et ce qu’est l’entropie. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 159.74,
  "end": 162.82
 },
 {
  "input": "My first thought in approaching this was to take a look at the relative frequencies of different letters in the English language. ",
  "translatedText": "Ma première pensée en abordant ce sujet a été de jeter un œil aux fréquences relatives des différentes lettres de la langue anglaise. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 168.22,
  "end": 173.72
 },
 {
  "input": "So I thought, okay, is there an opening guess or an opening pair of guesses that hits a lot of these most frequent letters? ",
  "translatedText": "Alors j'ai pensé, d'accord, y a-t-il une supposition d'ouverture ou une paire de suppositions d'ouverture qui touche beaucoup de ces lettres les plus fréquentes ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 174.38,
  "end": 179.26
 },
 {
  "input": "And one that I was pretty fond of was doing other followed by nails. ",
  "translatedText": "Et celui que j'aimais beaucoup était d'en faire d'autres suivis de clous. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 179.96,
  "end": 183.0
 },
 {
  "input": "The thought is that if you hit a letter, you know, you get a green or a yellow, that always feels good. ",
  "translatedText": "L’idée est que si vous frappez une lettre, vous savez, vous obtenez un vert ou un jaune, ça fait toujours du bien. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 183.76,
  "end": 187.52
 },
 {
  "input": "It feels like you're getting information. ",
  "translatedText": "C'est comme si vous receviez des informations. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 187.52,
  "end": 188.84
 },
 {
  "input": "But in these cases, even if you don't hit and you always get grays, that's still giving you a lot of information since it's pretty rare to find a word that doesn't have any of these letters. ",
  "translatedText": "Mais dans ces cas-là, même si vous ne frappez pas et que vous obtenez toujours des gris, cela vous donne quand même beaucoup d'informations puisqu'il est assez rare de trouver un mot qui n'a aucune de ces lettres. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 189.34,
  "end": 197.4
 },
 {
  "input": "But even still, that doesn't feel super systematic, because for example, it does nothing to consider the order of the letters. ",
  "translatedText": "Mais même quand même, cela ne semble pas super systématique, car par exemple, cela ne fait rien pour considérer l'ordre des lettres. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 198.14,
  "end": 203.2
 },
 {
  "input": "Why type nails when I could type snail? ",
  "translatedText": "Pourquoi taper ongles quand je pourrais taper escargot ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 203.56,
  "end": 205.3
 },
 {
  "input": "Is it better to have that S at the end? ",
  "translatedText": "Est-il préférable d'avoir ce S à la fin ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 206.08,
  "end": 207.5
 },
 {
  "input": "I'm not really sure. ",
  "translatedText": "Je ne suis pas vraiment sûr. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 207.82,
  "end": 208.68
 },
 {
  "input": "Now, a friend of mine said that he liked to open with the word weary, which kind of surprised me because it has some uncommon letters in there like the W and the Y. ",
  "translatedText": "Maintenant, un de mes amis m'a dit qu'il aimait commencer avec le mot fatigué, ce qui m'a un peu surpris car il contient des lettres inhabituelles comme le W et le Y. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 209.24,
  "end": 216.54
 },
 {
  "input": "But who knows, maybe that is a better opener. ",
  "translatedText": "Mais qui sait, c'est peut-être une meilleure ouverture. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 217.12,
  "end": 219.0
 },
 {
  "input": "Is there some kind of quantitative score that we can give to judge the quality of a potential guess? ",
  "translatedText": "Existe-t-il une sorte de score quantitatif que nous pouvons attribuer pour juger de la qualité d’une supposition potentielle ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 219.32,
  "end": 224.32
 },
 {
  "input": "Now to set up for the way that we're going to rank possible guesses, let's go back and add a little clarity to how exactly the game is set up. ",
  "translatedText": "Maintenant, pour définir la manière dont nous allons classer les suppositions possibles, revenons en arrière et ajoutons un peu de clarté à la manière exacte dont le jeu est configuré. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 225.34,
  "end": 231.42
 },
 {
  "input": "So there's a list of words that it will allow you to enter that are considered valid guesses that's just about 13,000 words long. ",
  "translatedText": "Il y a donc une liste de mots qu'il vous permettra de saisir et qui sont considérés comme des suppositions valables et qui fait environ 13 000 mots. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 231.42,
  "end": 237.88
 },
 {
  "input": "But when you look at it, there's a lot of really uncommon things, things like a head or Ali and ARG, the kind of words that bring about family arguments in a game of Scrabble. ",
  "translatedText": "Mais quand on y regarde, il y a beaucoup de choses vraiment inhabituelles, comme une tête ou Ali et ARG, le genre de mots qui provoquent des disputes familiales dans une partie de Scrabble. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 238.32,
  "end": 246.44
 },
 {
  "input": "But the vibe of the game is that the answer is always going to be a decently common word. ",
  "translatedText": "Mais l’ambiance du jeu est que la réponse sera toujours un mot assez courant. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 246.96,
  "end": 250.54
 },
 {
  "input": "And in fact, there's another list of around 2300 words that are the possible answers. ",
  "translatedText": "Et en fait, il existe une autre liste d’environ 2 300 mots qui constituent les réponses possibles. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 250.96,
  "end": 255.36
 },
 {
  "input": "And this is a human curated list, I think specifically by the game creator's girlfriend, which is kind of fun. ",
  "translatedText": "Et il s'agit d'une liste organisée par des humains, je pense spécifiquement par la petite amie du créateur du jeu, ce qui est plutôt amusant. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 255.94,
  "end": 261.16
 },
 {
  "input": "But what I would like to do, our challenge for this project is to see if we can write a program solving Wordle that doesn't incorporate previous knowledge about this list. ",
  "translatedText": "Mais ce que j'aimerais faire, notre défi pour ce projet est de voir si nous pouvons écrire un programme résolvant Wordle qui n'intègre pas les connaissances antérieures sur cette liste. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 261.82,
  "end": 270.18
 },
 {
  "input": "For one thing, there's plenty of pretty common five letter words that you won't find in that list. ",
  "translatedText": "D’une part, il existe de nombreux mots de cinq lettres assez courants que vous ne trouverez pas dans cette liste. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 270.72,
  "end": 274.64
 },
 {
  "input": "So it would be better to write a program that's a little more resilient and would play Wordle against anyone, not just what happens to be the official website. ",
  "translatedText": "Il serait donc préférable d'écrire un programme un peu plus résistant et qui permettrait de jouer à Wordle contre n'importe qui, pas seulement contre le site officiel. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 274.94,
  "end": 281.46
 },
 {
  "input": "And also the reason that we know what this list of possible answers is, is because it's visible in the source code. ",
  "translatedText": "Et aussi la raison pour laquelle nous connaissons cette liste de réponses possibles, c'est parce qu'elle est visible dans le code source. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 281.92,
  "end": 287.0
 },
 {
  "input": "But the way that it's visible in the source code is in the specific order in which answers come up from day to day. ",
  "translatedText": "Mais la manière dont cela est visible dans le code source dépend de l'ordre spécifique dans lequel les réponses apparaissent de jour en jour. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 287.0,
  "end": 292.8
 },
 {
  "input": "So you could always just look up what tomorrow's answer will be. ",
  "translatedText": "Vous pouvez donc toujours simplement rechercher quelle sera la réponse de demain. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 293.06,
  "end": 295.84
 },
 {
  "input": "So clearly, there's some sense in which using the list is cheating. ",
  "translatedText": "Il est donc clair qu'il y a un certain sens dans lequel l'utilisation de la liste constitue de la triche. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 296.42,
  "end": 298.88
 },
 {
  "input": "And what makes for a more interesting puzzle and a richer information theory lesson is to instead use some more universal data like relative word frequencies in general to capture this intuition of having a preference for more common words. ",
  "translatedText": "Et ce qui rend un casse-tête plus intéressant et une leçon de théorie de l'information plus riche est d'utiliser à la place des données plus universelles comme les fréquences relatives des mots en général pour capturer cette intuition d'avoir une préférence pour des mots plus courants. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 299.1,
  "end": 310.44
 },
 {
  "input": "So of these 13,000 possibilities, how should we choose the opening guess? ",
  "translatedText": "Alors, parmi ces 13 000 possibilités, comment devrions-nous choisir la première supposition ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 311.6,
  "end": 315.9
 },
 {
  "input": "For example, if my friend proposes weary, how should we analyze its quality? ",
  "translatedText": "Par exemple, si mon ami propose fatigué, comment analyser sa qualité ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 316.4,
  "end": 319.78
 },
 {
  "input": "Well, the reason he said he likes that unlikely W is that he likes the long shot nature of just how good it feels if you do hit that W. ",
  "translatedText": "Eh bien, la raison pour laquelle il a dit qu'il aime ce W improbable est qu'il aime la nature à long terme de la sensation de bien-être si vous frappez ce W. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 320.52,
  "end": 327.34
 },
 {
  "input": "For example, if the first pattern revealed was something like this, then it turns out there are only 58 words in this giant lexicon that match that pattern. ",
  "translatedText": "Par exemple, si le premier modèle révélé ressemblait à ceci, alors il s’avère qu’il n’y a que 58 mots dans ce lexique géant qui correspondent à ce modèle. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 327.92,
  "end": 335.6
 },
 {
  "input": "So that's a huge reduction from 13,000. ",
  "translatedText": "Cela représente donc une énorme réduction par rapport à 13 000. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 336.06,
  "end": 338.4
 },
 {
  "input": "But the flip side of that, of course, is that it's very uncommon to get a pattern like this. ",
  "translatedText": "Mais le revers de la médaille, bien sûr, c’est qu’il est très rare d’avoir un motif comme celui-ci. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 338.78,
  "end": 343.02
 },
 {
  "input": "Specifically, if each word was equally likely to be the answer, the probability of hitting this pattern would be 58 divided by around 13,000. ",
  "translatedText": "Plus précisément, si chaque mot avait la même probabilité d’être la réponse, la probabilité de trouver ce modèle serait de 58 divisée par environ 13 000. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 343.02,
  "end": 351.04
 },
 {
  "input": "Of course, they're not equally likely to be answers. ",
  "translatedText": "Bien sûr, il n’est pas également probable qu’elles constituent des réponses. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 351.58,
  "end": 353.6
 },
 {
  "input": "Most of these are very obscure and even questionable words. ",
  "translatedText": "La plupart de ces mots sont très obscurs, voire discutables. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 353.72,
  "end": 356.22
 },
 {
  "input": "But at least for our first pass at all of this, let's assume that they're all equally likely and then refine that a bit later. ",
  "translatedText": "Mais au moins pour notre première tentative, supposons qu'ils sont tous également probables, puis affinons cela un peu plus tard. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 356.6,
  "end": 361.6
 },
 {
  "input": "The point is the pattern with a lot of information is by its very nature unlikely to occur. ",
  "translatedText": "Le fait est qu’un modèle contenant beaucoup d’informations est, de par sa nature même, peu susceptible de se produire. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 362.02,
  "end": 366.72
 },
 {
  "input": "In fact, what it means to be informative is that it's unlikely. ",
  "translatedText": "En fait, ce que signifie être informatif, c'est que c'est peu probable. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 367.28,
  "end": 370.8
 },
 {
  "input": "A much more probable pattern to see with this opening would be something like this, where of course there's not a W in it. ",
  "translatedText": "Un modèle beaucoup plus probable à voir avec cette ouverture serait quelque chose comme ceci, où bien sûr il n'y a pas de W dedans. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 371.72,
  "end": 378.12
 },
 {
  "input": "Maybe there's an E, and maybe there's no A, there's no R, there's no Y. ",
  "translatedText": "Peut-être qu'il y a un E, et peut-être qu'il n'y a pas de A, qu'il n'y a pas de R, qu'il n'y a pas de Y. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 378.24,
  "end": 381.4
 },
 {
  "input": "In this case, there are 1400 possible matches. ",
  "translatedText": "Dans ce cas, il y a 1 400 correspondances possibles. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 382.08,
  "end": 384.56
 },
 {
  "input": "If all were equally likely, it works out to be a probability of about 11% that this is the pattern you would see. ",
  "translatedText": "Si toutes les probabilités étaient égales, la probabilité que ce soit la tendance que vous obtiendriez serait d’environ 11 %. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 385.08,
  "end": 390.6
 },
 {
  "input": "So the most likely outcomes are also the least informative. ",
  "translatedText": "Les résultats les plus probables sont donc aussi les moins informatifs. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 390.9,
  "end": 393.34
 },
 {
  "input": "To get a more global view here, let me show you the full distribution of probabilities across all of the different patterns that you might see. ",
  "translatedText": "Pour avoir une vue plus globale, permettez-moi de vous montrer la répartition complète des probabilités sur tous les différents modèles que vous pourriez observer. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 394.24,
  "end": 401.14
 },
 {
  "input": "So each bar that you're looking at corresponds to a possible pattern of colors that could be revealed, of which there are 3 to the 5th possibilities, and they're organized from left to right, most common to least common. ",
  "translatedText": "Ainsi, chaque barre que vous regardez correspond à un motif possible de couleurs qui pourrait être révélé, parmi lesquels il y a 3 à la 5ème possibilités, et elles sont organisées de gauche à droite, de la plus courante à la moins courante. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 401.74,
  "end": 412.34
 },
 {
  "input": "So the most common possibility here is that you get all grays. ",
  "translatedText": "La possibilité la plus courante ici est donc que vous obteniez uniquement des gris. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 412.92,
  "end": 416.0
 },
 {
  "input": "That happens about 14% of the time. ",
  "translatedText": "Cela se produit environ 14 % du temps. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 416.1,
  "end": 418.12
 },
 {
  "input": "And what you're hoping for when you make a guess is that you end up somewhere out in this long tail, like over here where there's only 18 possibilities for what matches this pattern that evidently look like this. ",
  "translatedText": "Et ce que vous espérez lorsque vous faites une supposition, c'est que vous vous retrouviez quelque part dans cette longue queue, comme ici où il n'y a que 18 possibilités pour ce qui correspond à ce modèle qui ressemble évidemment à ceci. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 418.58,
  "end": 429.14
 },
 {
  "input": "Or if we venture a little farther to the left, you know, maybe we go all the way over here. ",
  "translatedText": "Ou si nous nous aventurons un peu plus à gauche, vous savez, peut-être que nous irons jusqu'ici. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 429.92,
  "end": 433.8
 },
 {
  "input": "Okay, here's a good puzzle for you. ",
  "translatedText": "D'accord, voici un bon puzzle pour vous. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 434.94,
  "end": 436.18
 },
 {
  "input": "What are the three words in the English language that start with a W, end with a Y, and have an R somewhere in them? ",
  "translatedText": "Quels sont les trois mots de la langue anglaise qui commencent par un W, se terminent par un Y et contiennent un R quelque part ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 436.54,
  "end": 442.0
 },
 {
  "input": "Turns out, the answers are, let's see, wordy, wormy, and wryly. ",
  "translatedText": "Il s’avère que les réponses sont, voyons, verbeuses, vermifuges et ironiques. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 442.48,
  "end": 446.8
 },
 {
  "input": "So to judge how good this word is overall, we want some kind of measure of the expected amount of information that you're going to get from this distribution. ",
  "translatedText": "Donc, pour juger de la qualité globale de ce mot, nous voulons une sorte de mesure de la quantité d'informations attendue que vous allez obtenir de cette distribution. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 447.5,
  "end": 455.74
 },
 {
  "input": "If we go through each pattern and we multiply its probability of occurring times something that measures how informative it is, that can maybe give us an objective score. ",
  "translatedText": "Si nous examinons chaque modèle et multiplions sa probabilité d'apparition par quelque chose qui mesure son caractère informatif, cela peut peut-être nous donner un score objectif. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 455.74,
  "end": 464.72
 },
 {
  "input": "Now your first instinct for what that something should be might be the number of matches. ",
  "translatedText": "Maintenant, votre premier instinct pour savoir ce que devrait être quelque chose pourrait être le nombre de correspondances. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 465.96,
  "end": 469.84
 },
 {
  "input": "You want a lower average number of matches. ",
  "translatedText": "Vous souhaitez un nombre moyen de correspondances inférieur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 470.16,
  "end": 472.4
 },
 {
  "input": "But instead I'd like to use a more universal measurement that we often ascribe to information, and one that will be more flexible once we have a different probability assigned to each of these 13,000 words for whether or not they're actually the answer. ",
  "translatedText": "Mais j'aimerais plutôt utiliser une mesure plus universelle que nous attribuons souvent à l'information, et qui sera plus flexible une fois que nous aurons une probabilité différente attribuée à chacun de ces 13 000 mots pour savoir s'ils constituent ou non la réponse. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 472.8,
  "end": 484.26
 },
 {
  "input": "The standard unit of information is the bit, which has a little bit of a funny formula, but it's really intuitive if we just look at examples. ",
  "translatedText": "L'unité d'information standard est le bit, qui a une formule un peu amusante, mais qui est vraiment intuitive si l'on regarde simplement des exemples. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 490.32,
  "end": 496.98
 },
 {
  "input": "If you have an observation that cuts your space of possibilities in half, we say that it has one bit of information. ",
  "translatedText": "Si vous avez une observation qui réduit de moitié votre espace des possibles, on dit qu’elle ne contient qu’un bit d’information. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 497.78,
  "end": 503.5
 },
 {
  "input": "In our example, the space of possibilities is all possible words, and it turns out about Half of the five letter words have an S, a little less than that, but about half. ",
  "translatedText": "Dans notre exemple, l'espace des possibilités est constitué de tous les mots possibles, et il s'avère qu'environ la moitié des mots de cinq lettres ont un S, un peu moins que cela, mais environ la moitié. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 504.18,
  "end": 511.26
 },
 {
  "input": "So that observation would give you one bit of information. ",
  "translatedText": "Cette observation vous donnerait donc une information. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 511.78,
  "end": 514.32
 },
 {
  "input": "If instead a new fact chops down that space of possibilities by a factor of four, we say that it has two bits of information. ",
  "translatedText": "Si, au contraire, un nouveau fait réduit cet espace de possibilités d’un facteur quatre, nous disons qu’il contient deux éléments d’information. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 514.88,
  "end": 521.5
 },
 {
  "input": "For example, it turns out about a quarter of these words have a T. ",
  "translatedText": "Par exemple, il s’avère qu’environ un quart de ces mots ont un T. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 521.98,
  "end": 524.46
 },
 {
  "input": "If the observation cuts that space by a factor of eight, we say it's three bits of information, and so on and so forth. ",
  "translatedText": "Si l’observation divise cet espace par huit, nous disons qu’il s’agit de trois éléments d’information, et ainsi de suite. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 525.02,
  "end": 530.72
 },
 {
  "input": "Four bits cuts it into a 16th, five bits cuts it into a 32nd. ",
  "translatedText": "Quatre bits le coupent en 16ème, cinq bits le coupent en 32ème. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 530.9,
  "end": 533.52
 },
 {
  "input": "So now you might want to pause and ask yourself, what is the formula for information for the number of bits in terms of the probability of an occurrence? ",
  "translatedText": "Alors maintenant, vous voudrez peut-être faire une pause et vous demander quelle est la formule pour obtenir des informations sur le nombre de bits en termes de probabilité d'occurrence ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 533.52,
  "end": 542.98
 },
 {
  "input": "What we're saying here is that when you take one half to the number of bits, that's the same thing as the probability, which is the same thing as saying two to the power of the number of bits is one over the probability, which rearranges further to saying the information is the log base two of one divided by the probability. ",
  "translatedText": "Ce que nous disons ici, c'est que lorsque vous prenez la moitié du nombre de bits, cela équivaut à la même chose que la probabilité, ce qui revient à dire que deux puissance du nombre de bits est un sur la probabilité, ce qui se réorganise en disant que l'information est la base logarithmique deux de un divisée par la probabilité. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 543.92,
  "end": 558.92
 },
 {
  "input": "And sometimes you see this with one more rearrangement still, where the information is the negative log base two of the probability. ",
  "translatedText": "Et parfois, vous voyez cela avec encore un réarrangement supplémentaire, où l'information est le log négatif en base deux de la probabilité. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 559.62,
  "end": 564.9
 },
 {
  "input": "Expressed like this, it can look a little bit weird to the uninitiated, but it really is just the very intuitive idea of asking how many times you've cut down your possibilities in half. ",
  "translatedText": "Exprimé ainsi, cela peut paraître un peu bizarre aux non-initiés, mais il s'agit en réalité de l'idée très intuitive de se demander combien de fois vous avez réduit de moitié vos possibilités. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 565.66,
  "end": 574.08
 },
 {
  "input": "Now if you're wondering, you know, I thought we were just playing a fun word game, why are logarithms entering the picture? ",
  "translatedText": "Maintenant, si vous vous demandez, vous savez, je pensais que nous jouions juste à un jeu de mots amusant, pourquoi les logarithmes entrent-ils en scène ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 575.18,
  "end": 579.3
 },
 {
  "input": "One reason this is a nicer unit is it's just a lot easier to talk about very unlikely events, much easier to say that an observation has 20 bits of information than it is to say that the probability of such and such occurring is 0.0000095. ",
  "translatedText": "L'une des raisons pour lesquelles cette unité est plus intéressante est qu'il est beaucoup plus facile de parler d'événements très improbables, beaucoup plus facile de dire qu'une observation contient 20 bits d'information que de dire que la probabilité que tel ou tel se produise est de 0.0000095. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 579.78,
  "end": 592.94
 },
 {
  "input": "But a more substantive reason that this logarithmic expression turned out to be a very useful addition to the theory of probability is the way that information adds together. ",
  "translatedText": "Mais une raison plus importante pour laquelle cette expression logarithmique s’est avérée être un complément très utile à la théorie des probabilités est la manière dont les informations s’additionnent. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 593.3,
  "end": 601.46
 },
 {
  "input": "For example, if one observation gives you two bits of information, cutting your space down by four, and then a second observation like your second guess in Wordle gives you another three bits of information, chopping you down further by another factor of eight, the two together give you five bits of information. ",
  "translatedText": "Par exemple, si une observation vous donne deux bits d'information, réduisant votre espace de quatre, puis qu'une deuxième observation comme votre deuxième estimation dans Wordle vous donne trois autres bits d'information, vous réduisant encore d'un facteur huit, le deux ensemble vous donnent cinq informations. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 602.06,
  "end": 616.74
 },
 {
  "input": "In the same way that probabilities like to multiply, information likes to add. ",
  "translatedText": "De la même manière que les probabilités aiment se multiplier, les informations aiment s’ajouter. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 617.16,
  "end": 621.02
 },
 {
  "input": "So as soon as we're in the realm of something like an expected value, where we're adding a bunch of numbers up, the logs make it a lot nicer to deal with. ",
  "translatedText": "Ainsi, dès que nous sommes dans le domaine de quelque chose comme une valeur attendue, où nous additionnons un tas de nombres, les journaux rendent la gestion beaucoup plus agréable. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 621.96,
  "end": 627.98
 },
 {
  "input": "Let's go back to our distribution for Weary and add another little tracker on here, showing us how much information there is for each pattern. ",
  "translatedText": "Revenons à notre distribution pour Weary et ajoutons un autre petit tracker ici, nous montrant la quantité d'informations disponibles pour chaque modèle. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 628.48,
  "end": 634.94
 },
 {
  "input": "The main thing I want you to notice is that the higher the probability as we get to those more likely patterns, the lower the information, the fewer bits you gain. ",
  "translatedText": "La principale chose que je veux que vous remarquiez est que plus la probabilité est élevée à mesure que nous arrivons à ces modèles les plus probables, plus l'information est faible, moins vous gagnez de bits. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 635.58,
  "end": 642.78
 },
 {
  "input": "The way we measure the quality of this guess will be to take the expected value of this information, where we go through each pattern, we say how probable is it, and then we multiply that by how many bits of information do we get. ",
  "translatedText": "La façon dont nous mesurons la qualité de cette supposition sera de prendre la valeur attendue de cette information, où nous examinons chaque modèle, nous disons quelle est sa probabilité, puis nous la multiplions par le nombre d'éléments d'information que nous obtenons. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 643.5,
  "end": 654.06
 },
 {
  "input": "And in the example of Weary, that turns out to be 4.9 bits. ",
  "translatedText": "Et dans l’exemple de Weary, cela s’avère être 4.9 bits. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 654.71,
  "end": 658.12
 },
 {
  "input": "So on average, the information you get from this opening guess is as good as chopping your space of possibilities in half about five times. ",
  "translatedText": "Ainsi, en moyenne, les informations que vous obtenez de cette supposition d’ouverture équivaut à réduire de moitié votre espace des possibilités environ cinq fois. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 658.56,
  "end": 665.48
 },
 {
  "input": "By contrast, an example of a guess with a higher expected information value would be something like Slate. ",
  "translatedText": "En revanche, un exemple de supposition avec une valeur d’information attendue plus élevée serait quelque chose comme Slate. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 665.96,
  "end": 671.64
 },
 {
  "input": "In this case you'll notice the distribution looks a lot flatter. ",
  "translatedText": "Dans ce cas, vous remarquerez que la distribution semble beaucoup plus plate. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 673.12,
  "end": 675.62
 },
 {
  "input": "In particular, the most probable occurrence of all grays only has about a 6% chance of occurring, so at minimum you're getting evidently 3.9 bits of information. ",
  "translatedText": "En particulier, l'occurrence la plus probable de tous les gris n'a qu'environ 6 % de chances de se produire, donc au minimum vous en obtenez évidemment 3.9 informations. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 675.94,
  "end": 685.26
 },
 {
  "input": "But that's a minimum, more typically you'd get something better than that. ",
  "translatedText": "Mais c’est un minimum, vous obtiendrez généralement quelque chose de mieux que cela. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 685.92,
  "end": 688.56
 },
 {
  "input": "And it turns out when you crunch the numbers on this one and add up all the relevant terms, the average information is about 5.8. ",
  "translatedText": "Et il s'avère que lorsque vous analysez les chiffres sur celui-ci et additionnez tous les termes pertinents, l'information moyenne est d'environ 5.8. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 689.1,
  "end": 695.9
 },
 {
  "input": "So in contrast with Weary, your space of possibilities will be about half as big after this first guess, on average. ",
  "translatedText": "Ainsi, contrairement à Weary, votre espace de possibilités sera en moyenne environ deux fois plus grand après cette première hypothèse. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 697.36,
  "end": 703.54
 },
 {
  "input": "There's actually a fun story about the name for this expected value of information quantity. ",
  "translatedText": "Il existe en fait une histoire amusante sur le nom de cette valeur attendue de la quantité d'informations. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 704.42,
  "end": 708.3
 },
 {
  "input": "Information theory was developed by Claude Shannon, who was working at Bell Labs in the 1940s, but he was talking about some of his yet-to-be-published ideas with John von Neumann, who was this intellectual giant of the time, very prominent in math and physics and the beginnings of what was becoming computer science. ",
  "translatedText": "La théorie de l'information a été développée par Claude Shannon, qui travaillait aux Bell Labs dans les années 1940, mais il discutait de certaines de ses idées encore non publiées avec John von Neumann, qui était ce géant intellectuel de l'époque, très en vue. en mathématiques et en physique et les débuts de ce qui allait devenir l'informatique. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 708.3,
  "end": 723.56
 },
 {
  "input": "And when he mentioned that he didn't really have a good name for this expected value of information quantity, von Neumann supposedly said, so the story goes, well you should call it entropy, and for two reasons. ",
  "translatedText": "Et lorsqu'il a mentionné qu'il n'avait pas vraiment un bon nom pour cette valeur attendue de la quantité d'information, von Neumann aurait dit, selon l'histoire, eh bien, vous devriez appeler cela entropie, et pour deux raisons. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 724.1,
  "end": 734.2
 },
 {
  "input": "In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name, and in the second place, and more important, nobody knows what entropy really is, so in a debate you'll always have the advantage. ",
  "translatedText": "En premier lieu, votre fonction d'incertitude a été utilisée en mécanique statistique sous ce nom, donc elle a déjà un nom, et en deuxième lieu, et plus important encore, personne ne sait ce qu'est réellement l'entropie, donc dans un débat vous aurez toujours ont l'avantage. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 734.54,
  "end": 746.76
 },
 {
  "input": "So if the name seems a little bit mysterious, and if this story is to be believed, that's kind of by design. ",
  "translatedText": "Donc, si le nom semble un peu mystérieux, et si l’on en croit cette histoire, c’est en quelque sorte intentionnel. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 747.7,
  "end": 752.46
 },
 {
  "input": "Also if you're wondering about its relation to all of that second law of thermodynamics stuff from physics, there definitely is a connection, but in its origins Shannon was just dealing with pure probability theory, and for our purposes here, when I use the word entropy, I just want you to think the expected information value of a particular guess. ",
  "translatedText": "De plus, si vous vous interrogez sur sa relation avec toutes ces deuxièmes lois de la thermodynamique issues de la physique, il y a certainement un lien, mais à l'origine, Shannon ne traitait que de la théorie des probabilités pures, et pour nos besoins ici, lorsque j'utilise le mot entropie, je veux juste que vous réfléchissiez à la valeur informationnelle attendue d'une supposition particulière. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 753.28,
  "end": 769.58
 },
 {
  "input": "You can think of entropy as measuring two things simultaneously. ",
  "translatedText": "Vous pouvez considérer l’entropie comme la mesure de deux choses simultanément. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 770.7,
  "end": 773.78
 },
 {
  "input": "The first one is how flat is the distribution. ",
  "translatedText": "La première concerne la platitude de la distribution. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 774.24,
  "end": 776.78
 },
 {
  "input": "The closer a distribution is to uniform, the higher that entropy will be. ",
  "translatedText": "Plus une distribution est proche de l’uniforme, plus cette entropie sera élevée. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 777.32,
  "end": 781.12
 },
 {
  "input": "In our case, where there are 3 to the 5th total patterns, for a uniform distribution, observing any one of them would have information log base 2 of 3 to the 5th, which happens to be 7.92, so that is the absolute maximum that you could possibly have for this entropy. ",
  "translatedText": "Dans notre cas, où il y a 3 modèles au total sur 5, pour une distribution uniforme, l'observation de l'un d'entre eux aurait un journal d'informations de base 2 sur 3 au 5, qui se trouve être 7.92, c'est donc le maximum absolu que vous pourriez avoir pour cette entropie. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 781.58,
  "end": 797.3
 },
 {
  "input": "But entropy is also kind of a measure of how many possibilities there are in the first place. ",
  "translatedText": "Mais l’entropie est aussi en quelque sorte une mesure du nombre de possibilités qui existent en premier lieu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 797.84,
  "end": 802.08
 },
 {
  "input": "For example, if you happen to have some word where there's only 16 possible patterns, and each one is equally likely, this entropy, this expected information, would be 4 bits. ",
  "translatedText": "Par exemple, si vous avez un mot dans lequel il n'y a que 16 modèles possibles, et chacun est également probable, cette entropie, cette information attendue, serait de 4 bits. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 802.32,
  "end": 812.18
 },
 {
  "input": "But if you have another word where there's 64 possible patterns that could come up, and they're all equally likely, then the entropy would work out to be 6 bits. ",
  "translatedText": "Mais si vous avez un autre mot où il y a 64 modèles possibles qui pourraient apparaître, et ils sont tous également probables, alors l'entropie serait de 6 bits. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 812.58,
  "end": 820.48
 },
 {
  "input": "So if you see some distribution out in the wild that has an entropy of 6 bits, it's sort of like it's saying there's as much variation and uncertainty in what's about to happen as if there were 64 equally likely outcomes. ",
  "translatedText": "Donc, si vous voyez une distribution dans la nature qui a une entropie de 6 bits, c'est un peu comme si cela disait qu'il y a autant de variation et d'incertitude dans ce qui est sur le point de se produire que s'il y avait 64 résultats également probables. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 821.5,
  "end": 833.5
 },
 {
  "input": "For my first pass at the Wurtelebot, I basically had it just do this. ",
  "translatedText": "Pour mon premier passage au Wurtelebot, je l'ai fait faire comme ça. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 834.36,
  "end": 837.96
 },
 {
  "input": "It goes through all of the possible guesses you could have, all 13,000 words, computes the entropy for each one, or more specifically, the entropy of the distribution across all patterns you might see, for each one, and picks the highest, since that's the one that's likely to chop down your space of possibilities as much as possible. ",
  "translatedText": "Il passe en revue toutes les suppositions possibles que vous pourriez avoir, les 13 000 mots, calcule l'entropie pour chacun d'entre eux, ou plus précisément, l'entropie de la distribution à travers tous les modèles que vous pourriez voir, pour chacun d'entre eux, et choisit le plus élevé, puisque c'est celui qui est susceptible de réduire au maximum votre espace des possibles. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 837.96,
  "end": 856.14
 },
 {
  "input": "And even though I've only been talking about the first guess here, it does the same thing for the next few guesses. ",
  "translatedText": "Et même si je n’ai parlé ici que de la première supposition, cela fait la même chose pour les prochaines suppositions. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 857.14,
  "end": 861.1
 },
 {
  "input": "For example, after you see some pattern on that first guess, which would restrict you to a smaller number of possible words based on what matches with that, you just play the same game with respect to that smaller set of words. ",
  "translatedText": "Par exemple, après avoir vu un modèle sur cette première supposition, qui vous limiterait à un plus petit nombre de mots possibles en fonction de ce qui correspond à cela, vous jouez simplement au même jeu en ce qui concerne ce plus petit ensemble de mots. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 861.56,
  "end": 871.8
 },
 {
  "input": "For a proposed second guess, you look at the distribution of all patterns that could occur from that more restricted set of words, you search through all 13,000 possibilities, and you find the one that maximizes that entropy. ",
  "translatedText": "Pour une seconde supposition proposée, vous examinez la distribution de tous les modèles qui pourraient survenir à partir de cet ensemble plus restreint de mots, vous recherchez parmi les 13 000 possibilités et vous trouvez celle qui maximise cette entropie. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 872.26,
  "end": 883.84
 },
 {
  "input": "To show you how this works in action, let me just pull up a little variant of Wurtele that I wrote that shows the highlights of this analysis in the margins. ",
  "translatedText": "Pour vous montrer comment cela fonctionne en action, permettez-moi de vous présenter une petite variante de Wurtele que j'ai écrite et qui montre les points saillants de cette analyse dans les marges. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 885.42,
  "end": 892.18
 },
 {
  "input": "After doing all its entropy calculations, on the right here it's showing us which ones have the highest expected information. ",
  "translatedText": "Après avoir effectué tous ses calculs d'entropie, à droite, il nous montre lesquels ont les informations attendues les plus élevées. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 893.68,
  "end": 899.66
 },
 {
  "input": "Turns out the top answer, at least at the moment, we'll refine this later, is Tares, which means, um, of course, a vetch, the most common vetch. ",
  "translatedText": "Il s'avère que la première réponse, du moins pour le moment, nous affinerons cela plus tard, est Tares, ce qui signifie, euh, bien sûr, une vesce, la vesce la plus courante. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 900.28,
  "end": 910.58
 },
 {
  "input": "Each time we make a guess here, where maybe I kind of ignore its recommendations and go with slate, because I like slate, we can see how much expected information it had, but then on the right of the word here it's showing us how much actual information we got, given this particular pattern. ",
  "translatedText": "Chaque fois que nous faisons une supposition ici, où peut-être j'ignore en quelque sorte ses recommandations et opte pour Slate, parce que j'aime Slate, nous pouvons voir combien d'informations attendues il contenait, mais ensuite à droite du mot ici, cela nous montre combien informations réelles que nous avons obtenues, compte tenu de ce modèle particulier. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 911.04,
  "end": 924.42
 },
 {
  "input": "So here it looks like we were a little unlucky, we were expected to get 5.8, but we happened to get something with less than that. ",
  "translatedText": "Alors là, on dirait que nous n’avons pas eu de chance, on s’attendait à en avoir 5.8, mais nous avons obtenu quelque chose avec moins que cela. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 925.0,
  "end": 930.12
 },
 {
  "input": "And then on the left side here it's showing us all of the different possible words given where we are now. ",
  "translatedText": "Et puis sur le côté gauche, ici, cela nous montre tous les différents mots possibles donnés là où nous en sommes actuellement. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 930.6,
  "end": 935.02
 },
 {
  "input": "The blue bars are telling us how likely it thinks each word is, so at the moment it's assuming each word is equally likely to occur, but we'll refine that in a moment. ",
  "translatedText": "Les barres bleues nous indiquent la probabilité qu'il pense à chaque mot, donc pour le moment, il suppose que chaque mot a la même probabilité d'apparaître, mais nous affinerons cela dans un instant. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 935.8,
  "end": 943.36
 },
 {
  "input": "And then this uncertainty measurement is telling us the entropy of this distribution across the possible words, which right now, because it's a uniform distribution, is just a needlessly complicated way to count the number of possibilities. ",
  "translatedText": "Et puis cette mesure d'incertitude nous indique l'entropie de cette distribution parmi les mots possibles, ce qui, à l'heure actuelle, parce qu'il s'agit d'une distribution uniforme, n'est qu'une manière inutilement compliquée de compter le nombre de possibilités. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 944.06,
  "end": 955.96
 },
 {
  "input": "For example, if we were to take 2 to the power of 13.66, that should be around the 13,000 possibilities. ",
  "translatedText": "Par exemple, si nous prenons 2 puissance 13.66, cela devrait être autour des 13 000 possibilités. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 956.56,
  "end": 962.18
 },
 {
  "input": "I'm a little bit off here, but only because I'm not showing all the decimal places. ",
  "translatedText": "Je suis un peu en retrait ici, mais uniquement parce que je n'affiche pas toutes les décimales. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 962.9,
  "end": 966.14
 },
 {
  "input": "At the moment that might feel redundant and like it's overly complicating things, but you'll see why it's useful to have both numbers in a minute. ",
  "translatedText": "Pour le moment, cela peut sembler redondant et compliquer excessivement les choses, mais vous comprendrez pourquoi il est utile d'avoir les deux chiffres en une minute. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 966.72,
  "end": 972.34
 },
 {
  "input": "So here it looks like it's suggesting the highest entropy for our second guess is Ramen, which again just really doesn't feel like a word. ",
  "translatedText": "Donc, ici, il semble que cela suggère que l'entropie la plus élevée pour notre deuxième hypothèse est Ramen, ce qui, encore une fois, ne ressemble vraiment pas à un mot. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 972.76,
  "end": 979.4
 },
 {
  "input": "So to take the moral high ground here, I'm going to go ahead and type in Rains. ",
  "translatedText": "Donc, pour prendre le dessus sur le plan moral ici, je vais aller de l'avant et taper Rains. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 979.98,
  "end": 984.06
 },
 {
  "input": "And again it looks like we were a little unlucky. ",
  "translatedText": "Et encore une fois, on dirait que nous n’avons pas eu de chance. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 985.44,
  "end": 987.34
 },
 {
  "input": "We were expecting 4.3 bits and we only got 3.39 bits of information. ",
  "translatedText": "Nous en attendions 4.3 bits et nous n’en avons que 3.39 bits d'informations. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 987.52,
  "end": 991.36
 },
 {
  "input": "So that takes us down to 55 possibilities. ",
  "translatedText": "Cela nous ramène donc à 55 possibilités. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 991.94,
  "end": 993.94
 },
 {
  "input": "And here maybe I'll just actually go with what it's suggesting, which is combo, whatever that means. ",
  "translatedText": "Et ici, je vais peut-être simplement suivre ce que cela suggère, à savoir un combo, peu importe ce que cela signifie. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 994.9,
  "end": 999.44
 },
 {
  "input": "And okay, this is actually a good chance for a puzzle. ",
  "translatedText": "Et d'accord, c'est en fait une bonne occasion de résoudre un casse-tête. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1000.04,
  "end": 1002.92
 },
 {
  "input": "It's telling us this pattern gives us 4.7 bits of information. ",
  "translatedText": "Cela nous dit que ce modèle nous donne 4.7 informations. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1002.92,
  "end": 1006.38
 },
 {
  "input": "But over on the left, before we see that pattern, there were 5.78 bits of uncertainty. ",
  "translatedText": "Mais sur la gauche, avant de voir ce schéma, il y en avait 5.78 bits d'incertitude. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1007.06,
  "end": 1011.72
 },
 {
  "input": "So as a quiz for you, what does that mean about the number of remaining possibilities? ",
  "translatedText": "Alors, comme quiz pour vous, qu'est-ce que cela signifie sur le nombre de possibilités restantes ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1012.42,
  "end": 1016.34
 },
 {
  "input": "Well, it means that we're reduced down to one bit of uncertainty, which is the same thing as saying that there's two possible answers. ",
  "translatedText": "Eh bien, cela signifie que nous en sommes réduits à un peu d’incertitude, ce qui revient à dire qu’il y a deux réponses possibles. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1018.04,
  "end": 1024.54
 },
 {
  "input": "It's a 50-50 choice. ",
  "translatedText": "C'est un choix 50-50. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1024.7,
  "end": 1025.7
 },
 {
  "input": "And from here, because you and I know which words are more common, we know that the answer should be abyss. ",
  "translatedText": "Et à partir de là, parce que vous et moi savons quels mots sont les plus courants, nous savons que la réponse devrait être abyssale. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1026.5,
  "end": 1030.64
 },
 {
  "input": "But as it's written right now, the program doesn't know that. ",
  "translatedText": "Mais tel qu’il est écrit actuellement, le programme ne le sait pas. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1031.18,
  "end": 1033.28
 },
 {
  "input": "So it just keeps going, trying to gain as much information as it can, until it's only one possibility left, and then it guesses it. ",
  "translatedText": "Alors il continue, essayant d'obtenir autant d'informations que possible, jusqu'à ce qu'il ne reste plus qu'une possibilité, puis il la devine. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1033.54,
  "end": 1039.86
 },
 {
  "input": "So obviously we need a better endgame strategy. ",
  "translatedText": "Nous avons donc évidemment besoin d’une meilleure stratégie de fin de partie. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1040.38,
  "end": 1042.34
 },
 {
  "input": "But let's say we call this version one of our wordle solver, and then we go and run some simulations to see how it does. ",
  "translatedText": "Mais disons que nous appelons cette version l'un de nos solveurs de mots, puis que nous exécutons quelques simulations pour voir comment cela fonctionne. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1042.6,
  "end": 1048.26
 },
 {
  "input": "So the way this is working is it's playing every possible wordle game. ",
  "translatedText": "Donc, la façon dont cela fonctionne est de jouer à tous les jeux de mots possibles. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1050.36,
  "end": 1054.12
 },
 {
  "input": "It's going through all of those 2315 words that are the actual wordle answers. ",
  "translatedText": "Il passe en revue tous ces 2315 mots qui sont les véritables réponses aux mots. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1054.24,
  "end": 1058.54
 },
 {
  "input": "It's basically using that as a testing set. ",
  "translatedText": "Il s’agit essentiellement de l’utiliser comme ensemble de tests. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1058.54,
  "end": 1060.58
 },
 {
  "input": "And with this naive method of not considering how common a word is, and just trying to maximize the information at each step along the way, until it gets down to one and only one choice. ",
  "translatedText": "Et avec cette méthode naïve qui consiste à ne pas considérer à quel point un mot est courant et à essayer simplement de maximiser l'information à chaque étape du processus, jusqu'à ce qu'il s'agisse d'un et d'un seul choix. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1061.36,
  "end": 1069.82
 },
 {
  "input": "By the end of the simulation, the average score works out to be about 4.124. ",
  "translatedText": "À la fin de la simulation, le score moyen s’élève à environ 4.124. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1070.36,
  "end": 1074.3
 },
 {
  "input": "Which is not bad, to be honest, I kind of expected to do worse. ",
  "translatedText": "Ce qui n’est pas mal, pour être honnête, je m’attendais à faire pire. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1075.32,
  "end": 1079.24
 },
 {
  "input": "But the people who play wordle will tell you that they can usually get it in 4. ",
  "translatedText": "Mais les gens qui jouent aux mots vous diront qu’ils peuvent généralement l’obtenir en 4. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1079.66,
  "end": 1082.6
 },
 {
  "input": "The real challenge is to get as many in 3 as you can. ",
  "translatedText": "Le véritable défi est d’en obtenir autant en 3 que possible. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1082.86,
  "end": 1085.38
 },
 {
  "input": "It's a pretty big jump between the score of 4 and the score of 3. ",
  "translatedText": "C'est un écart assez important entre le score de 4 et le score de 3. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1085.38,
  "end": 1088.08
 },
 {
  "input": "The obvious low hanging fruit here is to somehow incorporate whether or not a word is common, and how exactly do we do that. ",
  "translatedText": "Le fruit évident ici est d'incorporer d'une manière ou d'une autre si un mot est courant ou non, et comment faire exactement cela. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1088.86,
  "end": 1094.98
 },
 {
  "input": "The way I approached it is to get a list of the relative frequencies for all of the words in the English language. ",
  "translatedText": "La façon dont je l'ai abordé consiste à obtenir une liste des fréquences relatives de tous les mots de la langue anglaise. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1102.8,
  "end": 1107.88
 },
 {
  "input": "And I just used Mathematica's word frequency data function, which itself pulls from the Google Books English Ngram public dataset. ",
  "translatedText": "Et je viens d'utiliser la fonction de données de fréquence des mots de Mathematica, qui elle-même est extraite de l'ensemble de données publiques Google Books English Ngram. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1108.22,
  "end": 1114.86
 },
 {
  "input": "And it's kind of fun to look at, for example if we sort it from the most common words to the least common words. ",
  "translatedText": "Et c'est plutôt amusant à regarder, par exemple si nous les trions des mots les plus courants aux mots les moins courants. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1115.46,
  "end": 1119.96
 },
 {
  "input": "Evidently these are the most common, 5 letter words in the English language. ",
  "translatedText": "De toute évidence, ce sont les mots de 5 lettres les plus courants dans la langue anglaise. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1120.12,
  "end": 1123.08
 },
 {
  "input": "Or rather, these is the 8th most common. ",
  "translatedText": "Ou plutôt, c'est le 8ème plus courant. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1123.7,
  "end": 1125.84
 },
 {
  "input": "First is which, after which there's there and there. ",
  "translatedText": "Le premier est lequel, puis il y a là et là. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1126.28,
  "end": 1128.88
 },
 {
  "input": "First itself is not first, but 9th, and it makes sense that these other words could come about more often, where those after first are after, where, and those being just a little bit less common. ",
  "translatedText": "Premier en lui-même n'est pas premier, mais 9ème, et il est logique que ces autres mots apparaissent plus souvent, là où ceux qui suivent premier sont après, où et ceux-ci étant juste un peu moins courants. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1129.26,
  "end": 1138.58
 },
 {
  "input": "Now, in using this data to model how likely each of these words is to be the final answer, it shouldn't just be proportional to the frequency. ",
  "translatedText": "Désormais, en utilisant ces données pour modéliser la probabilité que chacun de ces mots soit la réponse finale, cela ne devrait pas être simplement proportionnel à la fréquence. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1139.16,
  "end": 1146.34
 },
 {
  "input": "For example, which is given a score of 0.002 in this dataset, whereas the word braid is in some sense about 1000 times less likely. ",
  "translatedText": "Par exemple, à qui on attribue une note de 0.002 dans cet ensemble de données, alors que le mot tresse est en quelque sorte environ 1 000 fois moins probable. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1146.7,
  "end": 1155.06
 },
 {
  "input": "But both of these are common enough words that they're almost certainly worth considering. ",
  "translatedText": "Mais ces deux mots sont suffisamment courants pour qu’ils valent certainement la peine d’être pris en compte. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1155.56,
  "end": 1158.84
 },
 {
  "input": "So we want more of a binary cutoff. ",
  "translatedText": "Nous voulons donc davantage un seuil binaire. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1159.34,
  "end": 1161.0
 },
 {
  "input": "The way I went about it is to imagine taking this whole sorted list of words, and then arranging it on an x-axis, and then applying the sigmoid function, which is the standard way to have a function whose output is basically binary, it's either 0 or it's 1, but there's a smoothing in between for that region of uncertainty. ",
  "translatedText": "La façon dont j'ai procédé est d'imaginer prendre toute cette liste triée de mots, puis de la disposer sur un axe des x, puis d'appliquer la fonction sigmoïde, qui est la manière standard d'avoir une fonction dont la sortie est fondamentalement binaire, c'est soit 0, soit 1, mais il y a un lissage entre les deux pour cette région d'incertitude. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1161.86,
  "end": 1178.26
 },
 {
  "input": "So essentially, the probability that I'm assigning to each word for being in the final list will be the value of the sigmoid function above wherever it sits on the x-axis. ",
  "translatedText": "Donc, essentiellement, la probabilité que j'attribue à chaque mot d'être dans la liste finale sera la valeur de la fonction sigmoïde ci-dessus, quel que soit l'endroit où elle se trouve sur l'axe des x. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1179.16,
  "end": 1188.44
 },
 {
  "input": "Now obviously this depends on a few parameters, for example how wide a space on the x-axis those words fill determines how gradually or steeply we drop off from 1 to 0, and where we situate them left to right determines the cutoff. ",
  "translatedText": "Cela dépend évidemment de quelques paramètres, par exemple la largeur de l'espace sur l'axe des x que ces mots remplissent détermine la façon dont nous passons progressivement ou abruptement de 1 à 0, et l'endroit où nous les situons de gauche à droite détermine le seuil. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1189.52,
  "end": 1202.14
 },
 {
  "input": "To be honest, the way I did this was just licking my finger and sticking it into the wind. ",
  "translatedText": "Pour être honnête, j’ai simplement fait cela en me léchant le doigt et en le mettant face au vent. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1202.98,
  "end": 1206.92
 },
 {
  "input": "I looked through the sorted list and tried to find a window where when I looked at it I figured about half of these words are more likely than not to be the final answer, and used that as the cutoff. ",
  "translatedText": "J'ai parcouru la liste triée et essayé de trouver une fenêtre dans laquelle, lorsque je l'ai regardée, j'ai pensé qu'environ la moitié de ces mots étaient plus susceptibles qu'improbables d'être la réponse finale, et je l'ai utilisé comme seuil. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1207.14,
  "end": 1216.12
 },
 {
  "input": "Once we have a distribution like this across the words, it gives us another situation where entropy becomes this really useful measurement. ",
  "translatedText": "Une fois que nous avons une distribution comme celle-ci entre les mots, cela nous donne une autre situation dans laquelle l'entropie devient cette mesure vraiment utile. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1217.1,
  "end": 1223.86
 },
 {
  "input": "For example, let's say we were playing a game and we start with my old openers, which were a feather and nails, and we end up with a situation where there's four possible words that match it. ",
  "translatedText": "Par exemple, disons que nous jouons à un jeu et que nous commençons avec mes anciens premiers mots, qui étaient une plume et des ongles, et que nous nous retrouvons avec une situation où il y a quatre mots possibles qui y correspondent. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1224.5,
  "end": 1233.24
 },
 {
  "input": "And let's say we consider them all equally likely. ",
  "translatedText": "Et disons que nous les considérons tous également probables. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1233.56,
  "end": 1235.62
 },
 {
  "input": "Let me ask you, what is the entropy of this distribution? ",
  "translatedText": "Laissez-moi vous demander, quelle est l'entropie de cette distribution ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1236.22,
  "end": 1238.88
 },
 {
  "input": "Well, the information associated with each one of these possibilities is going to be the log base 2 of 4, since each one is 1 and 4, and that's 2. ",
  "translatedText": "Eh bien, les informations associées à chacune de ces possibilités seront la base log 2 sur 4, puisque chacune vaut 1 et 4, et cela fait 2. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1241.08,
  "end": 1250.26
 },
 {
  "input": "Two bits of information, four possibilities. ",
  "translatedText": "Deux informations, quatre possibilités. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1250.64,
  "end": 1252.46
 },
 {
  "input": "All very well and good. ",
  "translatedText": "Tout cela est très bien. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1252.76,
  "end": 1253.58
 },
 {
  "input": "But what if I told you that actually there's more than four matches? ",
  "translatedText": "Mais et si je vous disais qu'en réalité il y a plus de quatre matches ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1254.3,
  "end": 1257.8
 },
 {
  "input": "In reality, when we look through the full word list, there are 16 words that match it. ",
  "translatedText": "En réalité, lorsque nous parcourons la liste complète de mots, il y a 16 mots qui y correspondent. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1258.26,
  "end": 1262.46
 },
 {
  "input": "But suppose our model puts a really low probability on those other 12 words of actually being the final answer, something like 1 in 1000 because they're really obscure. ",
  "translatedText": "Mais supposons que notre modèle attribue une très faible probabilité à ces 12 autres mots d'être réellement la réponse finale, quelque chose comme 1 sur 1 000 parce qu'ils sont vraiment obscurs. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1262.58,
  "end": 1270.76
 },
 {
  "input": "Now let me ask you, what is the entropy of this distribution? ",
  "translatedText": "Maintenant, laissez-moi vous demander, quelle est l'entropie de cette distribution ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1271.5,
  "end": 1274.26
 },
 {
  "input": "If entropy was purely measuring the number of matches here, then you might expect it to be something like the log base 2 of 16, which would be 4, two more bits of uncertainty than we had before. ",
  "translatedText": "Si l'entropie mesurait uniquement le nombre de correspondances ici, alors vous pourriez vous attendre à ce qu'elle ressemble à la base logarithmique 2 sur 16, qui serait 4, soit deux bits d'incertitude de plus qu'auparavant. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1275.42,
  "end": 1285.7
 },
 {
  "input": "But of course the actual uncertainty is not really that different from what we had before. ",
  "translatedText": "Mais bien entendu, l’incertitude réelle n’est pas vraiment différente de celle que nous connaissions auparavant. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1286.18,
  "end": 1289.86
 },
 {
  "input": "Just because there's these 12 really obscure words doesn't mean that it would be all that more surprising to learn that the final answer is charm, for example. ",
  "translatedText": "Ce n'est pas parce qu'il y a ces 12 mots vraiment obscurs qu'il serait d'autant plus surprenant d'apprendre que la réponse finale est charme, par exemple. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1290.16,
  "end": 1297.36
 },
 {
  "input": "So when you actually do the calculation here, and you add up the probability of each occurrence times the corresponding information, what you get is 2.11 bits. ",
  "translatedText": "Ainsi, lorsque vous effectuez réellement le calcul ici et que vous additionnez la probabilité de chaque occurrence multipliée par les informations correspondantes, vous obtenez 2.11 bits. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1298.18,
  "end": 1306.02
 },
 {
  "input": "I'm just saying, it's basically two bits, basically those four possibilities, but there's a little more uncertainty because of all of those highly unlikely events, though if you did learn them you'd get a ton of information from it. ",
  "translatedText": "Je dis juste qu'il s'agit essentiellement de deux éléments, essentiellement de ces quatre possibilités, mais il y a un peu plus d'incertitude à cause de tous ces événements hautement improbables, même si si vous les appreniez, vous en tireriez une tonne d'informations. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1306.02,
  "end": 1316.5
 },
 {
  "input": "So zooming out, this is part of what makes Wordle such a nice example for an information theory lesson. ",
  "translatedText": "Donc, en effectuant un zoom arrière, cela fait partie de ce qui fait de Wordle un si bel exemple pour une leçon de théorie de l'information. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1317.16,
  "end": 1321.4
 },
 {
  "input": "We have these two distinct feeling applications for entropy. ",
  "translatedText": "Nous avons ces deux applications distinctes de sensation pour l’entropie. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1321.6,
  "end": 1324.64
 },
 {
  "input": "The first one telling us what's the expected information we'll get from a given guess, and the second one saying can we measure the remaining uncertainty among all of the words that we have possible. ",
  "translatedText": "Le premier nous dit quelle est l'information attendue que nous obtiendrons à partir d'une supposition donnée, et le second nous dit : pouvons-nous mesurer l'incertitude restante parmi tous les mots possibles. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1325.16,
  "end": 1335.46
 },
 {
  "input": "And I should emphasize, in that first case where we're looking at the expected information of a guess, once we have an unequal weighting to the words, that affects the entropy calculation. ",
  "translatedText": "Et je dois souligner que, dans le premier cas où nous examinons les informations attendues d'une supposition, une fois que nous avons une pondération inégale des mots, cela affecte le calcul de l'entropie. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1336.46,
  "end": 1344.54
 },
 {
  "input": "For example, let me pull up that same case we were looking at earlier of the distribution associated with Weary, but this time using a non-uniform distribution across all possible words. ",
  "translatedText": "Par exemple, permettez-moi de reprendre le même cas que nous avons examiné plus tôt concernant la distribution associée à Weary, mais cette fois en utilisant une distribution non uniforme sur tous les mots possibles. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1344.98,
  "end": 1353.72
 },
 {
  "input": "So let me see if I can find a part here that illustrates it pretty well. ",
  "translatedText": "Alors laissez-moi voir si je peux trouver ici une partie qui l’illustre assez bien. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1354.5,
  "end": 1358.28
 },
 {
  "input": "Okay, here this is pretty good. ",
  "translatedText": "Ok, ici, c'est plutôt bien. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1360.94,
  "end": 1362.36
 },
 {
  "input": "Here we have two adjacent patterns that are about equally likely, but one of them we're told has 32 possible words that match it. ",
  "translatedText": "Ici, nous avons deux modèles adjacents qui sont à peu près également probables, mais l'un d'eux, nous dit-on, a 32 mots possibles qui lui correspondent. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1362.36,
  "end": 1369.1
 },
 {
  "input": "And if we check what they are, these are those 32, which are all just very unlikely words as you scan your eyes over them. ",
  "translatedText": "Et si nous vérifions ce qu’ils sont, ce sont ces 32 mots, qui ne sont que des mots très improbables lorsque vous les parcourez des yeux. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1369.28,
  "end": 1375.6
 },
 {
  "input": "It's hard to find any that feel like plausible answers, maybe yells, but if we look at the neighboring pattern in the distribution, which is considered just about as likely, we're told that it only has 8 possible matches, so a quarter as many matches, but it's about as likely. ",
  "translatedText": "Il est difficile de trouver des réponses qui semblent plausibles, peut-être des cris, mais si nous regardons le modèle voisin dans la distribution, qui est considéré comme tout aussi probable, on nous dit qu'il n'y a que 8 correspondances possibles, donc un quart comme de nombreux matches, mais c'est à peu près aussi probable. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1375.84,
  "end": 1389.52
 },
 {
  "input": "And when we pull up those matches, we can see why. ",
  "translatedText": "Et lorsque nous récupérons ces matchs, nous pouvons comprendre pourquoi. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1389.86,
  "end": 1392.14
 },
 {
  "input": "Some of these are actual plausible answers, like ring, or wrath, or raps. ",
  "translatedText": "Certaines d’entre elles sont des réponses réellement plausibles, comme la sonnerie, la colère ou les raps. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1392.5,
  "end": 1396.3
 },
 {
  "input": "To illustrate how we incorporate all that, let me pull up version 2 of the Wordlebot here, and there are two or three main differences from the first one that we saw. ",
  "translatedText": "Pour illustrer comment nous intégrons tout cela, permettez-moi d'afficher ici la version 2 de Wordlebot, et il y a deux ou trois différences principales par rapport à la première que nous avons vue. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1397.9,
  "end": 1405.28
 },
 {
  "input": "First off, like I just said, the way that we're computing these entropies, these expected values of information, is now using the more refined distributions across the patterns that incorporates the probability that a given word would actually be the answer. ",
  "translatedText": "Tout d'abord, comme je viens de le dire, la façon dont nous calculons ces entropies, ces valeurs attendues de l'information, utilise désormais des distributions plus raffinées entre les modèles qui intègrent la probabilité qu'un mot donné soit réellement la réponse. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1405.86,
  "end": 1418.24
 },
 {
  "input": "As it happens, tears is still number 1, though the ones following are a bit different. ",
  "translatedText": "Il se trouve que les larmes sont toujours au premier rang, même si les suivantes sont un peu différentes. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1418.88,
  "end": 1423.82
 },
 {
  "input": "Second, when it ranks its top picks, it's now going to keep a model of the probability that each word is the actual answer, and it'll incorporate that into its decision, which is easier to see once we have a few guesses on the table. ",
  "translatedText": "Deuxièmement, lorsqu'il classera ses meilleurs choix, il conservera désormais un modèle de probabilité que chaque mot soit la réponse réelle, et il l'intégrera dans sa décision, qui est plus facile à voir une fois que nous avons quelques suppositions sur la réponse. tableau. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1424.36,
  "end": 1435.08
 },
 {
  "input": "Again, ignoring its recommendation because we can't let machines rule our lives. ",
  "translatedText": "Encore une fois, nous ignorons sa recommandation, car nous ne pouvons pas laisser les machines diriger nos vies. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1435.86,
  "end": 1439.78
 },
 {
  "input": "And I suppose I should mention another thing different here is over on the left, that uncertainty value, that number of bits, is no longer just redundant with the number of possible matches. ",
  "translatedText": "Et je suppose que je devrais mentionner une autre chose différente ici, à gauche, que la valeur d'incertitude, ce nombre de bits, n'est plus seulement redondante avec le nombre de correspondances possibles. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1441.14,
  "end": 1449.64
 },
 {
  "input": "Now if we pull it up and calculate 2 to the 8.02, which is a little above 256, I guess 259, what it's saying is even though there are 526 total words that actually match this pattern, the amount of uncertainty it has is more akin to what it would be if there were 259 equally likely outcomes. ",
  "translatedText": "Maintenant, si nous le retirons et calculons 2 puissance 8.02, qui est un peu au-dessus de 256, je suppose 259, ce qu'il dit, c'est que même s'il y a 526 mots au total qui correspondent réellement à ce modèle, le degré d'incertitude qu'il a est plus proche de ce qu'il serait s'il y avait 259 mots également probables. résultats. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1450.08,
  "end": 1468.98
 },
 {
  "input": "You can think of it like this. ",
  "translatedText": "Vous pouvez y penser comme ceci. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1469.72,
  "end": 1470.74
 },
 {
  "input": "It knows borx is not the answer, same with yorts and zorl and zorus, so it's a little less uncertain than it was in the previous case. ",
  "translatedText": "Il sait que le borx n'est pas la réponse, pareil pour les yorts, le zorl et le zorus, donc c'est un peu moins incertain que dans le cas précédent. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1471.02,
  "end": 1477.68
 },
 {
  "input": "This number of bits will be smaller. ",
  "translatedText": "Ce nombre de bits sera plus petit. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1477.82,
  "end": 1479.28
 },
 {
  "input": "And if I keep playing the game, I'm refining this down with a couple guesses that are apropos of what I would like to explain here. ",
  "translatedText": "Et si je continue à jouer au jeu, j'affine cela avec quelques suppositions qui sont à propos de ce que je voudrais expliquer ici. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1480.22,
  "end": 1486.54
 },
 {
  "input": "By the fourth guess, if you look over at its top picks, you can see it's no longer just maximizing the entropy. ",
  "translatedText": "À la quatrième hypothèse, si vous regardez ses meilleurs choix, vous pouvez voir qu'il ne s'agit plus seulement de maximiser l'entropie. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1488.36,
  "end": 1493.76
 },
 {
  "input": "So at this point, there's technically seven possibilities, but the only ones with a meaningful chance are dorms and words. ",
  "translatedText": "Donc à ce stade, il y a techniquement sept possibilités, mais les seules qui ont une chance significative sont les dortoirs et les mots. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1494.46,
  "end": 1500.3
 },
 {
  "input": "And you can see it ranks choosing both of those above all of these other values, that strictly speaking would give more information. ",
  "translatedText": "Et vous pouvez voir qu'il classe ces deux valeurs au-dessus de toutes ces autres valeurs, qui, à proprement parler, donneraient plus d'informations. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1500.3,
  "end": 1506.72
 },
 {
  "input": "The very first time I did this, I just added up these two numbers to measure the quality of each guess, which actually worked better than you might suspect. ",
  "translatedText": "La toute première fois que j'ai fait cela, j'ai simplement additionné ces deux nombres pour mesurer la qualité de chaque supposition, ce qui a en fait mieux fonctionné que vous ne le pensez. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1507.24,
  "end": 1513.9
 },
 {
  "input": "But it really didn't feel systematic, and I'm sure there's other approaches people could take but here's the one I landed on. ",
  "translatedText": "Mais cela ne semblait vraiment pas systématique, et je suis sûr qu'il existe d'autres approches que les gens pourraient adopter, mais voici celle sur laquelle j'ai atterri. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1514.3,
  "end": 1519.34
 },
 {
  "input": "If we're considering the prospect of a next guess, like in this case words, what we really care about is the expected score of our game if we do that. ",
  "translatedText": "Si nous envisageons la perspective d'une prochaine supposition, comme dans ce cas les mots, ce qui nous importe vraiment, c'est le score attendu de notre jeu si nous faisons cela. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1519.76,
  "end": 1527.9
 },
 {
  "input": "And to calculate that expected score, we say what's the probability that words is the actual answer, which at the moment it describes 58% to. ",
  "translatedText": "Et pour calculer ce score attendu, nous disons quelle est la probabilité que les mots soient la réponse réelle, ce qui est actuellement décrit à 58 %. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1528.23,
  "end": 1535.9
 },
 {
  "input": "We say with a 58% chance, our score in this game would be 4. ",
  "translatedText": "Nous disons qu'avec 58 % de chances, notre score dans ce jeu serait de 4. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1536.04,
  "end": 1539.54
 },
 {
  "input": "And then with the probability of 1 minus that 58%, our score will be more than that 4. ",
  "translatedText": "Et puis avec la probabilité de 1 moins 58 %, notre score sera supérieur à 4. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1540.32,
  "end": 1545.64
 },
 {
  "input": "How much more we don't know, but we can estimate it based on how much uncertainty there's likely to be once we get to that point. ",
  "translatedText": "Nous n’en savons pas encore plus, mais nous pouvons l’estimer en fonction du degré d’incertitude qu’il y aura probablement une fois arrivé à ce point. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1546.22,
  "end": 1552.46
 },
 {
  "input": "Specifically, at the moment there's 1.44 bits of uncertainty. ",
  "translatedText": "Concrètement, pour le moment, il y en a 1.44 bits d'incertitude. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1552.96,
  "end": 1555.94
 },
 {
  "input": "If we guess words, it's telling us the expected information we'll get is 1.27 bits. ",
  "translatedText": "Si nous devinons des mots, cela nous indique que l'information attendue que nous obtiendrons est 1.27 bits. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1556.44,
  "end": 1561.12
 },
 {
  "input": "So if we guess words, this difference represents how much uncertainty we're likely to be left with after that happens. ",
  "translatedText": "Donc, si nous devinons les mots, cette différence représente le degré d’incertitude qui nous restera probablement après que cela se produise. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1561.62,
  "end": 1567.66
 },
 {
  "input": "What we need is some kind of function, which I'm calling f here, that associates this uncertainty with an expected score. ",
  "translatedText": "Ce dont nous avons besoin, c'est d'une sorte de fonction, que j'appelle ici f, qui associe cette incertitude à un score attendu. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1568.26,
  "end": 1573.74
 },
 {
  "input": "And the way it went about this was to just plot a bunch of the data from previous games based on version 1 of the bot to say hey what was the actual score after various points with certain very measurable amounts of uncertainty. ",
  "translatedText": "Et la façon dont cela s'est déroulé consistait simplement à tracer un tas de données des jeux précédents basés sur la version 1 du bot pour dire quel était le score réel après différents points avec certaines quantités d'incertitude très mesurables. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1574.24,
  "end": 1586.32
 },
 {
  "input": "For example, these data points here that are sitting above a value that's around like 8.7 or so are saying for some games after a point at which there were 8.7 bits of uncertainty, it took two guesses to get the final answer. ",
  "translatedText": "Par exemple, ces points de données ici se situent au-dessus d’une valeur d’environ 8. Environ 7 disent pour certains matchs après un point où il y en avait 8.7 bits d'incertitude, il a fallu deux suppositions pour obtenir la réponse finale. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1587.02,
  "end": 1598.96
 },
 {
  "input": "For other games it took three guesses, for other games it took four guesses. ",
  "translatedText": "Pour les autres jeux, il fallait trois tentatives, pour les autres jeux, il fallait quatre tentatives. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1599.32,
  "end": 1602.24
 },
 {
  "input": "If we shift over to the left here, all the points over zero are saying whenever there's zero bits of uncertainty, which is to say there's only one possibility, then the number of guesses required is always just one, which is reassuring. ",
  "translatedText": "Si nous passons ici vers la gauche, tous les points au-dessus de zéro indiquent que lorsqu'il n'y a aucun élément d'incertitude, c'est-à-dire qu'il n'y a qu'une seule possibilité, alors le nombre de suppositions requis est toujours d'une seule, ce qui est rassurant. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1603.14,
  "end": 1614.26
 },
 {
  "input": "Whenever there was one bit of uncertainty, meaning it was essentially just down to two possibilities, then sometimes it required one more guess, sometimes it required two more guesses. ",
  "translatedText": "Chaque fois qu'il y avait un peu d'incertitude, ce qui signifiait qu'il ne s'agissait essentiellement que de deux possibilités, il fallait parfois une supposition supplémentaire, parfois deux suppositions supplémentaires. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1614.78,
  "end": 1623.02
 },
 {
  "input": "And so on and so forth here. ",
  "translatedText": "Et ainsi de suite, ici. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1623.08,
  "end": 1625.24
 },
 {
  "input": "Maybe a slightly easier way to visualize this data is to bucket it together and take averages. ",
  "translatedText": "Un moyen un peu plus simple de visualiser ces données consiste peut-être à les regrouper et à prendre des moyennes. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1625.74,
  "end": 1630.22
 },
 {
  "input": "For example this bar here saying among all the points where we had one bit of uncertainty, on average the number of new guesses required was about 1.5. ",
  "translatedText": "Par exemple, cette barre indique que parmi tous les points pour lesquels nous avions un peu d'incertitude, le nombre moyen de nouvelles suppositions requises était d'environ 1.5. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1631.0,
  "end": 1639.96
 },
 {
  "input": "And the bar over here saying among all of the different games where at some point the uncertainty was a little above four bits, which is like narrowing it down to 16 different possibilities, then on average it requires a little more than two guesses from that point forward. ",
  "translatedText": "Et la barre ici indique que parmi tous les différents jeux, où à un moment donné l'incertitude était un peu supérieure à quatre bits, ce qui revient à la réduire à 16 possibilités différentes, alors en moyenne, cela nécessite un peu plus de deux suppositions à partir de ce point. avant. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1642.14,
  "end": 1655.38
 },
 {
  "input": "And from here I just did a regression to fit a function that seemed reasonable to this. ",
  "translatedText": "Et à partir de là, j'ai juste fait une régression pour adapter une fonction qui semblait raisonnable à cela. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1656.06,
  "end": 1659.46
 },
 {
  "input": "And remember the whole point of doing any of that is so that we can quantify this intuition that the more information we gain from a word, the lower the expected score will be. ",
  "translatedText": "Et rappelez-vous que l’intérêt de faire tout cela est de pouvoir quantifier cette intuition selon laquelle plus nous obtenons d’informations à partir d’un mot, plus le score attendu sera bas. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1659.98,
  "end": 1668.96
 },
 {
  "input": "So with this as version 2.0, if we go back and we run the same set of simulations, having it play against all 2315 possible wordle answers, how does it do? ",
  "translatedText": "Donc avec ceci comme version 2.0, si nous revenons en arrière et exécutons le même ensemble de simulations, en le faisant jouer avec les 2315 réponses de mots possibles, comment cela se passe-t-il ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1669.68,
  "end": 1679.24
 },
 {
  "input": "Well in contrast to our first version it's definitely better, which is reassuring. ",
  "translatedText": "Et bien contrairement à notre première version, c'est nettement mieux, ce qui est rassurant. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1680.28,
  "end": 1683.42
 },
 {
  "input": "All said and done the average is around 3.6, although unlike the first version there are a couple times that it loses and requires more than six in this circumstance. ",
  "translatedText": "Tout compte fait, la moyenne est d’environ 3.6, bien que contrairement à la première version, il perd plusieurs fois et en nécessite plus de six dans ce cas. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1684.02,
  "end": 1692.12
 },
 {
  "input": "Presumably because there's times when it's making that tradeoff to actually go for the goal rather than maximizing information. ",
  "translatedText": "Vraisemblablement parce qu'il y a des moments où il faut faire un compromis pour atteindre l'objectif plutôt que de maximiser l'information. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1692.64,
  "end": 1697.94
 },
 {
  "input": "So can we do better than 3.6? ",
  "translatedText": "Alors pouvons-nous faire mieux que 3.6 ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1699.04,
  "end": 1701.0
 },
 {
  "input": "We definitely can. ",
  "translatedText": "Nous le pouvons certainement. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1702.08,
  "end": 1702.92
 },
 {
  "input": "Now I said at the start that it's most fun to try not incorporating the true list of wordle answers into the way that it builds its model. ",
  "translatedText": "Maintenant, j'ai dit au début qu'il était très amusant d'essayer de ne pas incorporer la vraie liste de réponses en mots dans la manière dont il construit son modèle. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1703.28,
  "end": 1709.36
 },
 {
  "input": "But if we do incorporate it, the best performance I could get was around 3.43. ",
  "translatedText": "Mais si nous l’intégrons, la meilleure performance que j’ai pu obtenir était d’environ 3.43. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1709.88,
  "end": 1714.18
 },
 {
  "input": "So if we try to get more sophisticated than just using word frequency data to choose this prior distribution, this 3.43 probably gives a max at how good we could get with that, or at least how good I could get with that. ",
  "translatedText": "Donc, si nous essayons d'être plus sophistiqués que d'utiliser simplement les données de fréquence des mots pour choisir cette distribution a priori, cette 3.43 donne probablement un maximum de la qualité que nous pourrions obtenir avec cela, ou du moins de la qualité que je pourrais obtenir avec cela. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1715.16,
  "end": 1725.74
 },
 {
  "input": "That best performance essentially just uses the ideas that I've been talking about here, but it goes a little farther, like it does a search for the expected information two steps forward rather than just one. ",
  "translatedText": "Cette meilleure performance utilise essentiellement les idées dont j'ai parlé ici, mais elle va un peu plus loin, comme si elle effectuait une recherche des informations attendues deux pas en avant plutôt qu'un seul. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1726.24,
  "end": 1735.12
 },
 {
  "input": "Originally I was planning on talking more about that, but I realize we've actually gone quite long as it is. ",
  "translatedText": "Au départ, j'avais prévu d'en parler davantage, mais je me rends compte que nous avons en fait été assez longs. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1735.62,
  "end": 1740.22
 },
 {
  "input": "The one thing I'll say is after doing this two-step search and then running a couple sample simulations in the top candidates, so far for me at least it's looking like Crane is the best opener. ",
  "translatedText": "La seule chose que je dirai, c'est qu'après avoir effectué cette recherche en deux étapes, puis exécuté quelques exemples de simulations sur les meilleurs candidats, jusqu'à présent, pour moi au moins, il semble que Crane soit le meilleur ouvreur. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1740.58,
  "end": 1749.1
 },
 {
  "input": "Who would have guessed? ",
  "translatedText": "Qui l'aurait deviné ? ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1749.1,
  "end": 1750.06
 },
 {
  "input": "Also if you use the true wordle list to determine your space of possibilities, then the uncertainty you start with is a little over 11 bits. ",
  "translatedText": "De plus, si vous utilisez la vraie liste de mots pour déterminer votre espace de possibilités, alors l'incertitude avec laquelle vous commencez est d'un peu plus de 11 bits. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1750.92,
  "end": 1757.82
 },
 {
  "input": "And it turns out, just from a brute force search, the maximum possible expected information after the first two guesses is around 10 bits. ",
  "translatedText": "Et il s'avère que, rien qu'à partir d'une recherche par force brute, le maximum d'informations attendues après les deux premières suppositions est d'environ 10 bits. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1758.3,
  "end": 1765.88
 },
 {
  "input": "Which suggests that best case scenario, after your first two guesses, with perfectly optimal play, you'll be left with around one bit of uncertainty. ",
  "translatedText": "Ce qui suggère que dans le meilleur des cas, après vos deux premières suppositions, avec un jeu parfaitement optimal, il vous restera environ un peu d'incertitude. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1766.5,
  "end": 1774.56
 },
 {
  "input": "Which is the same as being down to two possible guesses. ",
  "translatedText": "Ce qui revient à se limiter à deux suppositions possibles. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1774.8,
  "end": 1777.32
 },
 {
  "input": "So I think it's fair and probably pretty conservative to say that you could never possibly write an algorithm that gets this average as low as 3, because with the words available to you, there's simply not room to get enough information after only two steps to be able to guarantee the answer in the third slot every single time without fail. ",
  "translatedText": "Je pense donc qu'il est juste et probablement assez conservateur de dire que vous ne pourrez jamais écrire un algorithme qui abaisse cette moyenne à 3, car avec les mots dont vous disposez, il n'y a tout simplement pas de place pour obtenir suffisamment d'informations après seulement deux étapes. capable de garantir la réponse dans le troisième créneau à chaque fois sans faute. ",
  "model": "google_nmt",
  "n_reviews": 0,
  "start": 1777.74,
  "end": 1793.36
 }
]