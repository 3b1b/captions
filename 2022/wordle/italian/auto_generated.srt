1
00:00:00,000 --> 00:00:02,715
Il gioco Wurdle è diventato piuttosto virale negli ultimi due mesi, 

2
00:00:02,715 --> 00:00:05,710
e per chi non trascura mai l'opportunità di una lezione di matematica, 

3
00:00:05,710 --> 00:00:08,786
mi viene in mente che questo gioco costituisce un ottimo esempio centrale in 

4
00:00:08,786 --> 00:00:10,703
una lezione sulla teoria dell'informazione, 

5
00:00:10,703 --> 00:00:12,660
e in particolare un argomento noto come entropia.

6
00:00:13,920 --> 00:00:16,416
Vedete, come molte persone sono stato risucchiato dal puzzle, 

7
00:00:16,416 --> 00:00:19,356
e come molti programmatori sono stato anche risucchiato nel tentativo di 

8
00:00:19,356 --> 00:00:22,740
scrivere un algoritmo che potesse svolgere il gioco nel modo più ottimale possibile.

9
00:00:23,180 --> 00:00:26,239
E quello che ho pensato di fare qui è semplicemente parlarvi del mio processo, 

10
00:00:26,239 --> 00:00:28,291
e spiegare alcuni dei calcoli che ci sono implicati, 

11
00:00:28,291 --> 00:00:31,080
dato che l'intero algoritmo è incentrato su questa idea di entropia.

12
00:00:38,700 --> 00:00:41,640
Per prima cosa, nel caso non ne avessi sentito parlare, cos'è Wurdle?

13
00:00:42,040 --> 00:00:45,368
E per prendere due piccioni con una fava mentre analizziamo le regole del gioco, 

14
00:00:45,368 --> 00:00:47,587
permettetemi anche di anticipare dove stiamo andando, 

15
00:00:47,587 --> 00:00:51,040
ovvero sviluppare un piccolo algoritmo che sostanzialmente giocherà al posto nostro.

16
00:00:51,360 --> 00:00:55,100
Anche se non ho fatto il Wurdle di oggi, è il 4 febbraio e vedremo come se la cava il bot.

17
00:00:55,480 --> 00:00:57,784
L'obiettivo di Wurdle è indovinare una parola misteriosa di 

18
00:00:57,784 --> 00:01:00,340
cinque lettere e ti vengono date sei diverse possibilità di indovinare.

19
00:01:00,840 --> 00:01:04,379
Ad esempio, il mio bot Wurdle mi suggerisce di iniziare con la gru indovinata.

20
00:01:05,180 --> 00:01:07,917
Ogni volta che fai un'ipotesi, ottieni alcune informazioni 

21
00:01:07,917 --> 00:01:10,220
su quanto la tua ipotesi è vicina alla risposta vera.

22
00:01:10,920 --> 00:01:14,100
Qui la casella grigia mi dice che non c'è C nella risposta effettiva.

23
00:01:14,520 --> 00:01:17,840
La casella gialla mi dice che c'è una R, ma non è in quella posizione.

24
00:01:18,240 --> 00:01:22,240
La casella verde mi dice che la parola segreta ha una A ed è in terza posizione.

25
00:01:22,720 --> 00:01:24,580
E poi non c'è né N né E.

26
00:01:25,200 --> 00:01:27,340
Quindi lasciami entrare e riferire quell'informazione al bot Wurdle.

27
00:01:27,340 --> 00:01:30,320
Abbiamo iniziato con la gru, siamo diventati grigi, gialli, verdi, grigi, grigi.

28
00:01:31,420 --> 00:01:33,900
Non preoccuparti per tutti i dati che vengono mostrati in questo momento, 

29
00:01:33,900 --> 00:01:34,940
te lo spiegherò a tempo debito.

30
00:01:35,460 --> 00:01:38,820
Ma il suo suggerimento principale per la nostra seconda scelta è shtick.

31
00:01:39,560 --> 00:01:42,738
E la tua ipotesi deve essere una vera parola di cinque lettere, ma come vedrai, 

32
00:01:42,738 --> 00:01:45,400
è piuttosto liberale con ciò che ti farà effettivamente indovinare.

33
00:01:46,200 --> 00:01:47,440
In questo caso, proviamo shtick.

34
00:01:48,780 --> 00:01:50,180
E va bene, le cose sembrano piuttosto buone.

35
00:01:50,260 --> 00:01:53,980
Premiamo la S e la H, quindi conosciamo le prime tre lettere, sappiamo che c'è una R.

36
00:01:53,980 --> 00:01:58,700
E quindi sarà come SHA qualcosa R, o SHA R qualcosa.

37
00:01:59,620 --> 00:02:04,240
E sembra che il bot Wurdle sappia che ci sono solo due possibilità, shard o sharp.

38
00:02:05,100 --> 00:02:06,938
È una specie di scelta tra loro a questo punto, 

39
00:02:06,938 --> 00:02:10,080
quindi immagino che probabilmente solo perché è in ordine alfabetico va con shard.

40
00:02:11,220 --> 00:02:12,860
Evviva, è la vera risposta.

41
00:02:12,960 --> 00:02:13,780
Quindi ce l'abbiamo fatta in tre.

42
00:02:14,600 --> 00:02:17,480
Se ti stai chiedendo se va bene, il modo in cui ho sentito dire 

43
00:02:17,480 --> 00:02:20,360
da una persona è che con Wurdle quattro è il par e tre è birdie.

44
00:02:20,680 --> 00:02:22,480
Il che penso sia un'analogia piuttosto appropriata.

45
00:02:22,480 --> 00:02:27,020
Devi essere costantemente in gioco per ottenerne quattro, ma certamente non è pazzesco.

46
00:02:27,180 --> 00:02:29,920
Ma quando lo ottieni in tre, è semplicemente fantastico.

47
00:02:30,880 --> 00:02:33,451
Quindi, se sei d'accordo, quello che vorrei fare qui è semplicemente parlare 

48
00:02:33,451 --> 00:02:35,960
del mio processo di pensiero dall'inizio su come mi avvicino al bot Wurdle.

49
00:02:36,480 --> 00:02:39,440
E come ho detto, in realtà è una scusa per una lezione di teoria dell'informazione.

50
00:02:39,740 --> 00:02:42,820
L’obiettivo principale è spiegare cos’è l’informazione e cos’è l’entropia.

51
00:02:48,220 --> 00:02:50,656
Il mio primo pensiero nell'affrontarlo è stato quello di dare 

52
00:02:50,656 --> 00:02:53,720
un'occhiata alle frequenze relative delle diverse lettere nella lingua inglese.

53
00:02:54,380 --> 00:02:56,787
Quindi ho pensato, ok, esiste un'ipotesi di apertura o una coppia di 

54
00:02:56,787 --> 00:02:59,260
ipotesi di apertura che coincida con molte di queste lettere più frequenti?

55
00:02:59,960 --> 00:03:03,000
E uno a cui ero molto affezionato era farne altri seguiti dai chiodi.

56
00:03:03,760 --> 00:03:05,411
L'idea è che se colpisci una lettera, sai, 

57
00:03:05,411 --> 00:03:07,520
ottieni un verde o un giallo, è sempre una bella sensazione.

58
00:03:07,520 --> 00:03:08,840
Sembra che tu stia ricevendo informazioni.

59
00:03:09,340 --> 00:03:12,177
Ma in questi casi, anche se non colpisci e ottieni sempre dei grigi, 

60
00:03:12,177 --> 00:03:14,850
questo ti dà comunque molte informazioni poiché è piuttosto raro 

61
00:03:14,850 --> 00:03:17,400
trovare una parola che non contenga nessuna di queste lettere.

62
00:03:18,140 --> 00:03:20,455
Ma anche questo non sembra super sistematico, perché, 

63
00:03:20,455 --> 00:03:23,200
ad esempio, non fa nulla considerare l'ordine delle lettere.

64
00:03:23,560 --> 00:03:25,300
Perché scrivere chiodi quando potrei scrivere lumaca?

65
00:03:26,080 --> 00:03:27,500
È meglio avere quella S alla fine?

66
00:03:27,820 --> 00:03:28,680
Non sono veramente sicuro.

67
00:03:29,240 --> 00:03:32,720
Ora, un mio amico ha detto che gli piaceva aprire con la parola stanco, 

68
00:03:32,720 --> 00:03:36,540
il che mi ha sorpreso perché contiene alcune lettere insolite come la W e la Y.

69
00:03:37,120 --> 00:03:39,000
Ma chissà, forse è un'apertura migliore.

70
00:03:39,320 --> 00:03:41,713
Esiste una sorta di punteggio quantitativo che possiamo 

71
00:03:41,713 --> 00:03:44,320
assegnare per giudicare la qualità di una potenziale ipotesi?

72
00:03:45,340 --> 00:03:47,930
Ora, per impostare il modo in cui classificheremo le possibili ipotesi, 

73
00:03:47,930 --> 00:03:51,096
torniamo indietro e aggiungiamo un po' di chiarezza su come è impostato esattamente 

74
00:03:51,096 --> 00:03:51,420
il gioco.

75
00:03:51,420 --> 00:03:54,577
Quindi c'è un elenco di parole che ti permetterà di inserire 

76
00:03:54,577 --> 00:03:57,880
che sono considerate ipotesi valide che è lungo circa 13.000 parole.

77
00:03:58,320 --> 00:04:01,256
Ma quando lo guardi, ci sono un sacco di cose davvero insolite, 

78
00:04:01,256 --> 00:04:05,247
cose come una testa o Ali e ARG, il tipo di parole che provocano discussioni familiari 

79
00:04:05,247 --> 00:04:06,440
in una partita a Scarabeo.

80
00:04:06,960 --> 00:04:10,540
Ma l'atmosfera del gioco è che la risposta sarà sempre una parola abbastanza comune.

81
00:04:10,960 --> 00:04:13,183
E infatti c'è un altro elenco di circa 2300 

82
00:04:13,183 --> 00:04:15,360
parole che rappresentano le possibili risposte.

83
00:04:15,940 --> 00:04:18,530
E questa è una lista curata da persone umane, penso specificamente 

84
00:04:18,530 --> 00:04:21,160
dalla ragazza del creatore del gioco, il che è piuttosto divertente.

85
00:04:21,820 --> 00:04:24,476
Ma quello che mi piacerebbe fare, la nostra sfida per questo 

86
00:04:24,476 --> 00:04:27,262
progetto è vedere se possiamo scrivere un programma che risolva 

87
00:04:27,262 --> 00:04:30,180
Wordle che non incorpori le conoscenze precedenti su questo elenco.

88
00:04:30,720 --> 00:04:32,697
Per prima cosa, ci sono molte parole di cinque lettere 

89
00:04:32,697 --> 00:04:34,640
piuttosto comuni che non troverai in quell'elenco.

90
00:04:34,940 --> 00:04:37,624
Quindi sarebbe meglio scrivere un programma che sia un po' 

91
00:04:37,624 --> 00:04:41,460
più resistente e faccia giocare Wordle contro chiunque, non solo contro il sito ufficiale.

92
00:04:41,920 --> 00:04:44,460
E anche il motivo per cui sappiamo qual è questo elenco di 

93
00:04:44,460 --> 00:04:47,000
possibili risposte è perché è visibile nel codice sorgente.

94
00:04:47,000 --> 00:04:50,355
Ma il modo in cui è visibile nel codice sorgente è nell'ordine 

95
00:04:50,355 --> 00:04:53,260
specifico in cui le risposte emergono di giorno in giorno.

96
00:04:53,260 --> 00:04:55,840
Quindi potresti sempre cercare quale sarà la risposta di domani.

97
00:04:56,420 --> 00:04:58,880
Quindi, chiaramente, in un certo senso usare la lista è un imbroglio.

98
00:04:59,100 --> 00:05:01,893
E ciò che rende il puzzle più interessante e una lezione di teoria 

99
00:05:01,893 --> 00:05:04,561
dell’informazione più ricca è utilizzare invece alcuni dati più 

100
00:05:04,561 --> 00:05:07,354
universali come le frequenze relative delle parole in generale per 

101
00:05:07,354 --> 00:05:10,440
catturare questa intuizione di avere una preferenza per parole più comuni.

102
00:05:11,600 --> 00:05:15,900
Quindi tra queste 13.000 possibilità, come dovremmo scegliere l'ipotesi di apertura?

103
00:05:16,400 --> 00:05:19,780
Ad esempio, se il mio amico propone stanco, come dovremmo analizzarne la qualità?

104
00:05:20,520 --> 00:05:23,815
Beh, il motivo per cui ha detto che gli piace quell'improbabile W è 

105
00:05:23,815 --> 00:05:27,340
che gli piace la natura a lungo termine di quanto sia bello colpire quella W.

106
00:05:27,920 --> 00:05:31,057
Ad esempio, se il primo schema rivelato fosse qualcosa del genere, 

107
00:05:31,057 --> 00:05:34,850
si scopre che ci sono solo 58 parole in questo lessico gigante che corrispondono 

108
00:05:34,850 --> 00:05:35,600
a quello schema.

109
00:05:36,060 --> 00:05:38,400
Quindi si tratta di un'enorme riduzione rispetto a 13.000.

110
00:05:38,780 --> 00:05:40,990
Ma il rovescio della medaglia, ovviamente, è che 

111
00:05:40,990 --> 00:05:43,020
è molto raro ottenere uno schema come questo.

112
00:05:43,020 --> 00:05:47,230
Nello specifico, se ogni parola avesse la stessa probabilità di essere la risposta, 

113
00:05:47,230 --> 00:05:51,040
la probabilità di ottenere questo schema sarebbe 58 diviso per circa 13.000.

114
00:05:51,580 --> 00:05:53,600
Naturalmente, non è altrettanto probabile che siano risposte.

115
00:05:53,720 --> 00:05:56,220
La maggior parte di queste sono parole molto oscure e persino discutibili.

116
00:05:56,600 --> 00:05:58,418
Ma almeno per il nostro primo passaggio a tutto questo, 

117
00:05:58,418 --> 00:06:01,275
supponiamo che siano tutti ugualmente probabili e poi perfezioniamo il tutto un po' 

118
00:06:01,275 --> 00:06:01,600
più tardi.

119
00:06:02,020 --> 00:06:04,370
Il punto è che un modello con molte informazioni è 

120
00:06:04,370 --> 00:06:06,720
per sua stessa natura improbabile che si verifichi.

121
00:06:07,280 --> 00:06:10,800
In effetti, ciò che significa essere informativo è che è improbabile.

122
00:06:11,719 --> 00:06:16,365
Uno schema molto più probabile da vedere con questa apertura sarebbe qualcosa del genere, 

123
00:06:16,365 --> 00:06:18,120
dove ovviamente non c'è una W.

124
00:06:18,240 --> 00:06:21,400
Forse c'è una E, e forse non c'è A, non c'è R, non c'è Y.

125
00:06:22,080 --> 00:06:24,560
In questo caso ci sono 1400 corrispondenze possibili.

126
00:06:25,080 --> 00:06:27,864
Se tutti fossero ugualmente probabili, la probabilità che 

127
00:06:27,864 --> 00:06:30,600
questo sia lo schema che vedresti sarebbe di circa l’11%.

128
00:06:30,900 --> 00:06:33,340
Quindi i risultati più probabili sono anche quelli meno informativi.

129
00:06:34,240 --> 00:06:37,666
Per avere una visione più globale, lascia che ti mostri la distribuzione 

130
00:06:37,666 --> 00:06:41,140
completa delle probabilità in tutti i diversi modelli che potresti vedere.

131
00:06:41,740 --> 00:06:45,321
Quindi ogni barra che stai guardando corrisponde a un possibile schema di 

132
00:06:45,321 --> 00:06:48,903
colori che potrebbe essere rivelato, di cui ci sono da 3 a 5 possibilità, 

133
00:06:48,903 --> 00:06:52,340
e sono organizzati da sinistra a destra, dal più comune al meno comune.

134
00:06:52,920 --> 00:06:56,000
Quindi la possibilità più comune qui è che ottieni tutti i grigi.

135
00:06:56,100 --> 00:06:58,120
Ciò accade circa il 14% delle volte.

136
00:06:58,580 --> 00:07:02,197
E quello che speri quando fai un'ipotesi è di finire da qualche parte 

137
00:07:02,197 --> 00:07:05,717
in questa lunga coda, come qui dove ci sono solo 18 possibilità per ciò 

138
00:07:05,717 --> 00:07:09,140
che corrisponde a questo schema che evidentemente assomiglia a questo.

139
00:07:09,920 --> 00:07:13,800
O se ci avventuriamo un po' più a sinistra, forse arriviamo fino a qui.

140
00:07:14,940 --> 00:07:16,180
Ok, ecco un bel puzzle per te.

141
00:07:16,540 --> 00:07:19,514
Quali sono le tre parole in lingua inglese che iniziano con una W, 

142
00:07:19,514 --> 00:07:22,000
finiscono con una Y e contengono una R da qualche parte?

143
00:07:22,480 --> 00:07:26,800
Si scopre che le risposte sono, vediamo, prolisse, verminose e ironiche.

144
00:07:27,500 --> 00:07:30,720
Quindi, per giudicare quanto sia buona questa parola nel complesso, 

145
00:07:30,720 --> 00:07:34,745
vogliamo una sorta di misura della quantità prevista di informazioni che otterrai da 

146
00:07:34,745 --> 00:07:35,740
questa distribuzione.

147
00:07:35,740 --> 00:07:40,125
Se esaminiamo ogni modello e moltiplichiamo la sua probabilità che si verifichi per 

148
00:07:40,125 --> 00:07:44,720
qualcosa che misura quanto sia informativo, forse possiamo darci un punteggio oggettivo.

149
00:07:45,960 --> 00:07:47,918
Ora il tuo primo istinto su cosa dovrebbe essere quel 

150
00:07:47,918 --> 00:07:49,840
qualcosa potrebbe essere il numero di corrispondenze.

151
00:07:50,160 --> 00:07:52,400
Desideri un numero medio di partite inferiore.

152
00:07:52,800 --> 00:07:55,633
Ma mi piacerebbe invece usare una misura più universale che spesso 

153
00:07:55,633 --> 00:07:58,466
attribuiamo alle informazioni, e che sarà più flessibile una volta 

154
00:07:58,466 --> 00:08:01,257
che avremo una probabilità diversa assegnata a ciascuna di queste 

155
00:08:01,257 --> 00:08:04,260
13.000 parole per stabilire se siano o meno effettivamente la risposta.

156
00:08:10,320 --> 00:08:13,957
L'unità di informazione standard è il bit, che ha una formula un po' 

157
00:08:13,957 --> 00:08:16,980
divertente, ma è davvero intuitiva se guardiamo solo gli esempi.

158
00:08:17,780 --> 00:08:21,272
Se hai un'osservazione che dimezza il tuo spazio di possibilità, 

159
00:08:21,272 --> 00:08:23,500
diciamo che contiene un bit di informazione.

160
00:08:24,180 --> 00:08:27,411
Nel nostro esempio, lo spazio delle possibilità è composto da tutte le parole possibili, 

161
00:08:27,411 --> 00:08:30,025
e risulta che circa la metà delle parole di cinque lettere hanno una S, 

162
00:08:30,025 --> 00:08:31,260
un po' meno, ma circa la metà.

163
00:08:31,780 --> 00:08:34,320
Quindi quell'osservazione ti darebbe un po' di informazione.

164
00:08:34,880 --> 00:08:39,673
Se invece un fatto nuovo riduce di un fattore quattro quello spazio di possibilità, 

165
00:08:39,673 --> 00:08:41,500
diciamo che ha due informazioni.

166
00:08:41,980 --> 00:08:44,460
Ad esempio, risulta che circa un quarto di queste parole hanno una T.

167
00:08:45,020 --> 00:08:47,915
Se l'osservazione taglia quello spazio di un fattore otto, 

168
00:08:47,915 --> 00:08:50,720
diciamo che si tratta di tre bit di informazione, e così via.

169
00:08:50,900 --> 00:08:55,060
Quattro bit lo tagliano in un sedicesimo, cinque bit lo tagliano in un trentaduesimo.

170
00:08:55,060 --> 00:08:57,558
Quindi ora potresti voler fermarti e chiederti: 

171
00:08:57,558 --> 00:09:01,358
qual è la formula per l'informazione sul numero di bit in termini di 

172
00:09:01,358 --> 00:09:02,660
probabilità di un evento?

173
00:09:02,660 --> 00:09:06,560
Quello che stiamo dicendo qui è che quando prendi la metà del numero di bit, 

174
00:09:06,560 --> 00:09:10,764
è la stessa cosa della probabilità, che è la stessa cosa che dire due alla potenza 

175
00:09:10,764 --> 00:09:14,715
del numero di bit è uno su probabilità, che riorganizza ulteriormente dicendo 

176
00:09:14,715 --> 00:09:18,920
che l'informazione è il logaritmo in base due di uno diviso per la probabilità.

177
00:09:19,620 --> 00:09:21,849
E a volte lo vedi con un'ulteriore riorganizzazione, 

178
00:09:21,849 --> 00:09:24,900
dove l'informazione è il logaritmo negativo in base due della probabilità.

179
00:09:25,660 --> 00:09:27,902
Espresso in questo modo, può sembrare un po' 

180
00:09:27,902 --> 00:09:30,693
strano ai non iniziati, ma in realtà è solo l'idea molto 

181
00:09:30,693 --> 00:09:34,080
intuitiva di chiedersi quante volte hai ridotto a metà le tue possibilità.

182
00:09:35,180 --> 00:09:37,134
Ora, se ti stai chiedendo, sai, pensavo stessimo solo facendo un 

183
00:09:37,134 --> 00:09:39,300
divertente gioco di parole, perché i logaritmi stanno entrando in gioco?

184
00:09:39,780 --> 00:09:42,986
Uno dei motivi per cui questa è un'unità più gradevole è che è 

185
00:09:42,986 --> 00:09:45,570
molto più facile parlare di eventi molto improbabili, 

186
00:09:45,570 --> 00:09:49,015
molto più facile dire che un'osservazione ha 20 bit di informazione 

187
00:09:49,015 --> 00:09:52,940
che dire che la probabilità che si verifichi questo o quell'altro è 0.0000095.

188
00:09:53,300 --> 00:09:55,844
Ma una ragione più sostanziale per cui questa espressione 

189
00:09:55,844 --> 00:09:58,432
logaritmica si è rivelata un'aggiunta molto utile alla 

190
00:09:58,432 --> 00:10:01,460
teoria della probabilità è il modo in cui le informazioni si sommano.

191
00:10:02,060 --> 00:10:05,469
Ad esempio, se un'osservazione ti fornisce due bit di informazione, 

192
00:10:05,469 --> 00:10:09,021
riducendo il tuo spazio di quattro, e poi una seconda osservazione come la 

193
00:10:09,021 --> 00:10:12,193
tua seconda ipotesi in Wordle ti dà altri tre bit di informazione, 

194
00:10:12,193 --> 00:10:14,656
riducendoti ulteriormente di un altro fattore otto, 

195
00:10:14,656 --> 00:10:16,740
il due insieme ti danno cinque informazioni.

196
00:10:17,160 --> 00:10:19,705
Allo stesso modo in cui le probabilità amano moltiplicarsi, 

197
00:10:19,705 --> 00:10:21,020
le informazioni amano sommarsi.

198
00:10:21,960 --> 00:10:24,674
Quindi non appena siamo nel campo di qualcosa come un valore atteso, 

199
00:10:24,674 --> 00:10:27,980
dove stiamo sommando un sacco di numeri, i log rendono molto più piacevole gestirli.

200
00:10:28,480 --> 00:10:32,364
Torniamo alla nostra distribuzione per Weary e aggiungiamo qui un altro piccolo tracker, 

201
00:10:32,364 --> 00:10:34,940
che ci mostra quante informazioni ci sono per ogni pattern.

202
00:10:35,580 --> 00:10:39,052
La cosa principale che voglio farti notare è che maggiore è la probabilità quando 

203
00:10:39,052 --> 00:10:42,780
arriviamo a quegli schemi più probabili, minore è l'informazione, meno bit guadagni.

204
00:10:43,500 --> 00:10:46,960
Il modo in cui misuriamo la qualità di questa ipotesi sarà quello di prendere 

205
00:10:46,960 --> 00:10:49,844
il valore atteso di queste informazioni, esaminare ogni modello, 

206
00:10:49,844 --> 00:10:53,438
dire quanto è probabile e poi moltiplicarlo per il numero di bit di informazione 

207
00:10:53,438 --> 00:10:54,060
che otteniamo.

208
00:10:54,710 --> 00:10:58,120
E nell'esempio di Weary, risulta essere 4.9 bit.

209
00:10:58,560 --> 00:11:01,975
Quindi, in media, le informazioni che ottieni da questa ipotesi di apertura 

210
00:11:01,975 --> 00:11:05,480
equivalgono a tagliare a metà il tuo spazio di possibilità circa cinque volte.

211
00:11:05,960 --> 00:11:08,565
Al contrario, un esempio di ipotesi con un valore 

212
00:11:08,565 --> 00:11:11,640
informativo atteso più elevato sarebbe qualcosa come Slate.

213
00:11:13,120 --> 00:11:15,620
In questo caso noterai che la distribuzione sembra molto più piatta.

214
00:11:15,940 --> 00:11:20,656
In particolare, l'evento più probabile di tutti i grigi ha solo una probabilità 

215
00:11:20,656 --> 00:11:25,260
del 6% circa, quindi come minimo ne ottieni evidentemente 3.9 bit di informazione.

216
00:11:25,920 --> 00:11:28,560
Ma questo è il minimo, più tipicamente otterresti qualcosa di meglio di così.

217
00:11:29,100 --> 00:11:32,525
E si scopre che quando si calcolano i numeri su questo e si sommano 

218
00:11:32,525 --> 00:11:35,900
tutti i termini rilevanti, l'informazione media è di circa 5.8.

219
00:11:37,360 --> 00:11:40,612
Quindi, a differenza di Weary, il tuo spazio di possibilità 

220
00:11:40,612 --> 00:11:43,540
sarà in media circa la metà dopo questa prima ipotesi.

221
00:11:44,420 --> 00:11:46,770
In realtà c'è una storia divertente sul nome di 

222
00:11:46,770 --> 00:11:49,120
questo valore atteso della quantità di informazioni.

223
00:11:49,200 --> 00:11:51,859
La teoria dell'informazione fu sviluppata da Claude Shannon, 

224
00:11:51,859 --> 00:11:55,500
che lavorava ai Bell Labs negli anni '40, ma stava parlando di alcune delle sue idee 

225
00:11:55,500 --> 00:11:58,773
ancora da pubblicare con John von Neumann, che era questo gigante intellettuale 

226
00:11:58,773 --> 00:12:02,373
dell'epoca, molto importante in matematica e fisica e gli inizi di quella che stava 

227
00:12:02,373 --> 00:12:03,560
diventando l'informatica.

228
00:12:04,100 --> 00:12:07,400
E quando disse che non aveva un buon nome per questo valore atteso 

229
00:12:07,400 --> 00:12:10,701
della quantità di informazioni, von Neumann presumibilmente disse, 

230
00:12:10,701 --> 00:12:14,200
così va la storia, beh, dovresti chiamarla entropia, e per due ragioni.

231
00:12:14,540 --> 00:12:18,492
In primo luogo, la tua funzione di incertezza è stata usata nella meccanica statistica 

232
00:12:18,492 --> 00:12:22,171
con quel nome, quindi ha già un nome, e in secondo luogo, e cosa più importante, 

233
00:12:22,171 --> 00:12:26,169
nessuno sa cosa sia realmente l'entropia, quindi in un dibattito sarai sempre avere 

234
00:12:26,169 --> 00:12:26,760
il vantaggio.

235
00:12:27,700 --> 00:12:29,710
Quindi, se il nome sembra un po' misterioso, 

236
00:12:29,710 --> 00:12:32,460
e se si deve credere a questa storia, è in un certo senso previsto.

237
00:12:33,280 --> 00:12:36,388
Inoltre, se ti stai chiedendo quale sia la sua relazione con tutta quella 

238
00:12:36,388 --> 00:12:38,867
seconda legge della termodinamica, materiale della fisica, 

239
00:12:38,867 --> 00:12:42,144
c'è sicuramente una connessione, ma nelle sue origini Shannon si occupava 

240
00:12:42,144 --> 00:12:44,874
solo di pura teoria della probabilità, e per i nostri scopi qui, 

241
00:12:44,874 --> 00:12:48,151
quando uso la parola entropia, voglio solo che tu pensi al valore informativo 

242
00:12:48,151 --> 00:12:49,580
atteso di una particolare ipotesi.

243
00:12:50,700 --> 00:12:53,780
Puoi pensare all'entropia come alla misurazione di due cose contemporaneamente.

244
00:12:54,240 --> 00:12:56,780
Il primo è quanto piatta è la distribuzione.

245
00:12:57,320 --> 00:13:01,120
Più una distribuzione si avvicina all’uniforme, maggiore sarà l’entropia.

246
00:13:01,580 --> 00:13:04,635
Nel nostro caso, dove ci sono da 3 a 5 modelli totali, 

247
00:13:04,635 --> 00:13:08,412
per una distribuzione uniforme, osservando uno qualsiasi di essi si 

248
00:13:08,412 --> 00:13:11,745
otterrebbe un log delle informazioni in base 2 di 3 alla 5, 

249
00:13:11,745 --> 00:13:15,855
che risulta essere 7.92, quindi questo è il massimo assoluto che potresti 

250
00:13:15,855 --> 00:13:17,300
avere per questa entropia.

251
00:13:17,840 --> 00:13:22,080
Ma l’entropia è anche una sorta di misura di quante possibilità ci sono in primo luogo.

252
00:13:22,320 --> 00:13:27,033
Ad esempio, se ti capita di avere una parola in cui ci sono solo 16 modelli possibili, 

253
00:13:27,033 --> 00:13:29,742
e ognuno è ugualmente probabile, questa entropia, 

254
00:13:29,742 --> 00:13:32,180
questa informazione attesa, sarebbe di 4 bit.

255
00:13:32,579 --> 00:13:36,195
Ma se hai un'altra parola in cui ci sono 64 possibili modelli che potrebbero 

256
00:13:36,195 --> 00:13:40,078
emergere, e sono tutti ugualmente probabili, allora l'entropia risulterebbe essere 

257
00:13:40,078 --> 00:13:40,480
di 6 bit.

258
00:13:41,500 --> 00:13:45,534
Quindi, se vedi una distribuzione in natura che ha un'entropia di 6 bit, 

259
00:13:45,534 --> 00:13:49,412
è un po' come se dicesse che ci sono tante variazioni e incertezze in 

260
00:13:49,412 --> 00:13:53,500
ciò che sta per accadere come se ci fossero 64 risultati ugualmente probabili.

261
00:13:54,360 --> 00:13:59,320
Per il mio primo passaggio al Wurtelebot, praticamente ho fatto semplicemente questo.

262
00:13:59,320 --> 00:14:03,160
Esamina tutte le possibili ipotesi che potresti avere, tutte le 13.000 parole, 

263
00:14:03,160 --> 00:14:06,417
calcola l'entropia per ciascuna di esse o, più specificamente, 

264
00:14:06,417 --> 00:14:10,743
l'entropia della distribuzione in tutti i modelli che potresti vedere, per ciascuno, 

265
00:14:10,743 --> 00:14:14,876
e sceglie il più alto, poiché è quello che probabilmente ridurrà il più possibile il 

266
00:14:14,876 --> 00:14:16,140
tuo spazio di possibilità.

267
00:14:17,140 --> 00:14:19,330
E anche se qui ho parlato solo della prima ipotesi, 

268
00:14:19,330 --> 00:14:21,100
fa la stessa cosa per le prossime ipotesi.

269
00:14:21,560 --> 00:14:24,130
Ad esempio, dopo aver visto uno schema su quella prima ipotesi, 

270
00:14:24,130 --> 00:14:27,302
che ti limiterebbe a un numero inferiore di parole possibili in base a ciò che 

271
00:14:27,302 --> 00:14:30,916
corrisponde a quello, giochi semplicemente allo stesso gioco rispetto a quell'insieme 

272
00:14:30,916 --> 00:14:31,800
più piccolo di parole.

273
00:14:32,260 --> 00:14:36,070
Per una seconda ipotesi proposta, guardi la distribuzione di tutti i modelli 

274
00:14:36,070 --> 00:14:39,683
che potrebbero verificarsi da quell'insieme di parole più ristretto, 

275
00:14:39,683 --> 00:14:43,840
cerchi tutte le 13.000 possibilità e trovi quello che massimizza quell'entropia.

276
00:14:45,420 --> 00:14:49,652
Per mostrarvi come funziona in azione, lasciatemi semplicemente richiamare una piccola 

277
00:14:49,652 --> 00:14:53,690
variante di Wurtele che ho scritto che mostra i punti salienti di questa analisi a 

278
00:14:53,690 --> 00:14:54,080
margine.

279
00:14:54,080 --> 00:14:56,491
Dopo aver fatto tutti i calcoli dell'entropia, 

280
00:14:56,491 --> 00:14:59,660
qui a destra ci mostra quali hanno le informazioni attese più alte.

281
00:15:00,280 --> 00:15:05,662
Sembra che la risposta migliore, almeno al momento, la perfezioneremo più tardi, 

282
00:15:05,662 --> 00:15:10,580
è Tares, che significa, ehm, ovviamente, una veccia, la veccia più comune.

283
00:15:11,040 --> 00:15:14,518
Ogni volta che facciamo un'ipotesi qui, dove forse ignoro i suoi consigli 

284
00:15:14,518 --> 00:15:18,131
e scelgo lo slate, perché mi piace lo slate, possiamo vedere quante informazioni 

285
00:15:18,131 --> 00:15:21,565
attese aveva, ma poi a destra della parola qui ci mostra quante informazioni 

286
00:15:21,565 --> 00:15:24,420
effettive che abbiamo ottenuto, dato questo modello particolare.

287
00:15:25,000 --> 00:15:26,985
Quindi qui sembra che siamo stati un po' sfortunati, 

288
00:15:26,985 --> 00:15:30,120
ci aspettavamo di prenderne 5.8, ma ci è capitato di ottenere qualcosa con meno di quello.

289
00:15:30,600 --> 00:15:32,862
E poi sul lato sinistro qui ci vengono mostrate tutte le diverse 

290
00:15:32,862 --> 00:15:35,020
parole possibili data la situazione in cui ci troviamo adesso.

291
00:15:35,800 --> 00:15:38,438
Le barre blu ci dicono quanto è probabile che ciascuna parola sia, 

292
00:15:38,438 --> 00:15:41,942
quindi al momento presuppone che ogni parola abbia la stessa probabilità di verificarsi, 

293
00:15:41,942 --> 00:15:43,360
ma lo perfezioneremo tra un momento.

294
00:15:44,060 --> 00:15:47,743
E poi questa misurazione dell'incertezza ci dice l'entropia di questa 

295
00:15:47,743 --> 00:15:50,671
distribuzione tra le parole possibili, che in questo momento, 

296
00:15:50,671 --> 00:15:54,779
poiché è una distribuzione uniforme, è solo un modo inutilmente complicato per contare 

297
00:15:54,779 --> 00:15:55,960
il numero di possibilità.

298
00:15:56,560 --> 00:15:59,787
Ad esempio, se dovessimo portare 2 alla potenza di 13.66, 

299
00:15:59,787 --> 00:16:02,180
dovrebbero essere circa 13.000 possibilità.

300
00:16:02,900 --> 00:16:06,140
Sono un po' fuori strada, ma solo perché non mostro tutte le cifre decimali.

301
00:16:06,720 --> 00:16:09,833
Al momento potrebbe sembrare ridondante e complicare eccessivamente le cose, 

302
00:16:09,833 --> 00:16:12,340
ma vedrai perché è utile avere entrambi i numeri in un minuto.

303
00:16:12,760 --> 00:16:16,014
Quindi qui sembra che suggerisca che l'entropia più alta per la nostra 

304
00:16:16,014 --> 00:16:19,400
seconda ipotesi sia Ramen, che ancora una volta non sembra proprio una parola.

305
00:16:19,980 --> 00:16:24,060
Quindi, per prendere una posizione morale, andrò avanti e digiterò Rains.

306
00:16:25,440 --> 00:16:27,340
E ancora una volta sembra che siamo stati un po' sfortunati.

307
00:16:27,520 --> 00:16:31,360
Ci aspettavamo 4.3 bit e ne abbiamo solo 3.39 bit di informazioni.

308
00:16:31,940 --> 00:16:33,940
Quindi questo ci porta a 55 possibilità.

309
00:16:34,900 --> 00:16:37,257
E qui forse seguirò semplicemente ciò che suggerisce, 

310
00:16:37,257 --> 00:16:39,440
che è una combinazione, qualunque cosa significhi.

311
00:16:40,040 --> 00:16:42,920
E okay, questa è in realtà una buona occasione per un puzzle.

312
00:16:42,920 --> 00:16:46,380
Ci sta dicendo che questo schema ci dà 4.7 bit di informazione.

313
00:16:47,060 --> 00:16:51,720
Ma a sinistra, prima di vedere lo schema, ce n'erano 5.78 bit di incertezza.

314
00:16:52,420 --> 00:16:56,340
Quindi, come quiz per te, cosa significa riguardo al numero di possibilità rimanenti?

315
00:16:58,040 --> 00:17:01,424
Ebbene, significa che siamo ridotti a un minimo di incertezza, 

316
00:17:01,424 --> 00:17:04,540
il che equivale a dire che ci sono due risposte possibili.

317
00:17:04,700 --> 00:17:05,700
È una scelta 50-50.

318
00:17:06,500 --> 00:17:08,926
E da qui, poiché tu ed io sappiamo quali sono le parole più comuni, 

319
00:17:08,926 --> 00:17:10,640
sappiamo che la risposta dovrebbe essere abisso.

320
00:17:11,180 --> 00:17:13,280
Ma come è scritto proprio ora, il programma non lo sa.

321
00:17:13,540 --> 00:17:17,462
Quindi continua ad andare avanti, cercando di ottenere quante più informazioni possibile, 

322
00:17:17,462 --> 00:17:19,859
finché non rimane solo una possibilità, e poi indovina.

323
00:17:20,380 --> 00:17:22,339
Quindi ovviamente abbiamo bisogno di una migliore strategia di fine gioco.

324
00:17:22,599 --> 00:17:25,582
Ma diciamo che chiamiamo questa versione uno del nostro risolutore di parole, 

325
00:17:25,582 --> 00:17:28,260
e poi andiamo ad eseguire alcune simulazioni per vedere come funziona.

326
00:17:30,360 --> 00:17:34,120
Quindi il modo in cui funziona è giocare a ogni possibile gioco di parole.

327
00:17:34,240 --> 00:17:38,540
Sta esaminando tutte quelle 2315 parole che sono le vere risposte delle parole.

328
00:17:38,540 --> 00:17:40,580
Fondamentalmente lo utilizza come set di test.

329
00:17:41,360 --> 00:17:44,432
E con questo metodo ingenuo di non considerare quanto sia comune una parola, 

330
00:17:44,432 --> 00:17:47,984
e di cercare semplicemente di massimizzare l'informazione in ogni fase del percorso, 

331
00:17:47,984 --> 00:17:49,820
finché non si arriva a una ed una sola scelta.

332
00:17:50,360 --> 00:17:54,300
Alla fine della simulazione, il punteggio medio risulta essere circa 4.124.

333
00:17:55,319 --> 00:17:59,240
Il che non è male, a dire il vero, mi aspettavo di fare di peggio.

334
00:17:59,660 --> 00:18:02,600
Ma le persone che giocano a Wordle ti diranno che di solito riescono a farlo in 4.

335
00:18:02,860 --> 00:18:05,380
La vera sfida è ottenerne il maggior numero possibile in 3.

336
00:18:05,380 --> 00:18:08,080
C'è un salto piuttosto grande tra il punteggio di 4 e il punteggio di 3.

337
00:18:08,860 --> 00:18:12,030
L’ovvio frutto a portata di mano qui è quello di incorporare in qualche 

338
00:18:12,030 --> 00:18:14,980
modo se una parola è comune o meno, e come lo facciamo esattamente.

339
00:18:22,800 --> 00:18:25,226
Il modo in cui mi sono avvicinato è stato quello di ottenere un 

340
00:18:25,226 --> 00:18:27,880
elenco delle frequenze relative per tutte le parole in lingua inglese.

341
00:18:28,220 --> 00:18:31,682
E ho appena utilizzato la funzione dati sulla frequenza delle parole di Mathematica, 

342
00:18:31,682 --> 00:18:34,860
che a sua volta estrae dal set di dati pubblici English Ngram di Google Libri.

343
00:18:35,460 --> 00:18:37,730
Ed è piuttosto divertente da guardare, ad esempio se lo 

344
00:18:37,730 --> 00:18:39,960
ordiniamo dalle parole più comuni a quelle meno comuni.

345
00:18:40,120 --> 00:18:43,080
Evidentemente queste sono le parole di 5 lettere più comuni nella lingua inglese.

346
00:18:43,700 --> 00:18:45,840
O meglio, questi sono gli 8 più comuni.

347
00:18:46,280 --> 00:18:48,880
Il primo è quale, dopodiché c'è lì e là.

348
00:18:49,260 --> 00:18:53,946
Primo in sé non è primo, ma 9°, ed è logico che queste altre parole possano comparire più 

349
00:18:53,946 --> 00:18:58,580
spesso, dove quelle dopo prima sono dopo, dove e quelle sono solo un po' meno comuni.

350
00:18:59,160 --> 00:19:02,917
Ora, utilizzando questi dati per modellare la probabilità che ciascuna di queste 

351
00:19:02,917 --> 00:19:06,860
parole sia la risposta finale, non dovrebbe essere solo proporzionale alla frequenza.

352
00:19:06,860 --> 00:19:11,113
Ad esempio, a cui viene assegnato un punteggio pari a 0.002 in questo set di dati, 

353
00:19:11,113 --> 00:19:15,060
mentre la parola treccia è in un certo senso circa 1000 volte meno probabile.

354
00:19:15,560 --> 00:19:17,186
Ma entrambe queste sono parole abbastanza comuni da valere 

355
00:19:17,186 --> 00:19:18,840
quasi sicuramente la pena di essere prese in considerazione.

356
00:19:19,340 --> 00:19:21,000
Quindi vogliamo più di un taglio binario.

357
00:19:21,860 --> 00:19:26,019
Il modo in cui ho proceduto è stato immaginare di prendere l'intero elenco ordinato 

358
00:19:26,019 --> 00:19:29,894
di parole, quindi disporlo su un asse x, e quindi applicare la funzione sigmoide, 

359
00:19:29,894 --> 00:19:34,053
che è il modo standard per avere una funzione il cui output è fondamentalmente binario, 

360
00:19:34,053 --> 00:19:38,260
è o 0 oppure è 1, ma c'è un livellamento intermedio per quella regione di incertezza.

361
00:19:39,160 --> 00:19:43,144
Quindi, in sostanza, la probabilità che assegno a ciascuna parola di essere 

362
00:19:43,144 --> 00:19:47,601
nell'elenco finale sarà il valore della funzione sigmoide sopra ovunque si trovi 

363
00:19:47,601 --> 00:19:48,440
sull'asse x.

364
00:19:49,520 --> 00:19:51,962
Ovviamente questo dipende da alcuni parametri, 

365
00:19:51,962 --> 00:19:56,535
ad esempio quanto è ampio lo spazio sull'asse x riempito da quelle parole determina 

366
00:19:56,535 --> 00:19:59,342
quanto gradualmente o ripidamente scendiamo da 1 a 0, 

367
00:19:59,342 --> 00:20:03,240
e il punto in cui le posizioniamo da sinistra a destra determina il limite.

368
00:20:03,240 --> 00:20:04,975
Ad essere onesti, il modo in cui l'ho fatto è 

369
00:20:04,975 --> 00:20:06,920
stato semplicemente leccarmi il dito e alzarlo al vento.

370
00:20:07,140 --> 00:20:10,556
Ho esaminato l'elenco ordinato e ho cercato di trovare una finestra in cui, 

371
00:20:10,556 --> 00:20:13,886
quando l'ho guardata, ho pensato che circa la metà di queste parole fosse 

372
00:20:13,886 --> 00:20:17,260
più probabile che non fossero la risposta finale, e l'ho usata come limite.

373
00:20:17,260 --> 00:20:20,020
Una volta ottenuta una distribuzione come questa tra le parole, 

374
00:20:20,020 --> 00:20:23,860
otteniamo un'altra situazione in cui l'entropia diventa una misura davvero utile.

375
00:20:24,500 --> 00:20:28,020
Ad esempio, supponiamo che stessimo giocando e iniziamo con le mie vecchie aperture, 

376
00:20:28,020 --> 00:20:30,878
che erano una piuma e chiodi, e finiamo con una situazione in cui ci 

377
00:20:30,878 --> 00:20:33,240
sono quattro possibili parole che corrispondono a quella.

378
00:20:33,560 --> 00:20:35,620
E diciamo che li consideriamo tutti ugualmente probabili.

379
00:20:36,220 --> 00:20:38,880
Lascia che ti chieda: qual è l'entropia di questa distribuzione?

380
00:20:41,080 --> 00:20:45,626
Bene, l'informazione associata a ciascuna di queste possibilità 

381
00:20:45,626 --> 00:20:50,040
sarà il logaritmo in base 2 di 4, poiché ognuna è 1 e 4, e cioè 2.

382
00:20:50,040 --> 00:20:52,460
Due informazioni, quattro possibilità.

383
00:20:52,760 --> 00:20:53,580
Tutto molto bello e buono.

384
00:20:54,300 --> 00:20:57,800
E se ti dicessi che in realtà ci sono più di quattro partite?

385
00:20:58,260 --> 00:21:00,962
In realtà, quando esaminiamo l'elenco completo delle parole, 

386
00:21:00,962 --> 00:21:02,460
ci sono 16 parole che corrispondono.

387
00:21:02,580 --> 00:21:05,292
Ma supponiamo che il nostro modello attribuisca una probabilità 

388
00:21:05,292 --> 00:21:08,598
molto bassa alle altre 12 parole di essere effettivamente la risposta finale, 

389
00:21:08,598 --> 00:21:10,760
qualcosa come 1 su 1000 perché sono davvero oscure.

390
00:21:11,500 --> 00:21:14,260
Ora lascia che ti chieda: qual è l'entropia di questa distribuzione?

391
00:21:15,420 --> 00:21:19,006
Se l'entropia misurasse semplicemente il numero di corrispondenze qui, 

392
00:21:19,006 --> 00:21:22,783
allora potresti aspettarti che sia qualcosa come il logaritmo in base 2 di 16, 

393
00:21:22,783 --> 00:21:25,700
che sarebbe 4, due bit di incertezza in più rispetto a prima.

394
00:21:26,180 --> 00:21:29,860
Ma ovviamente l’effettiva incertezza non è poi così diversa da quella che avevamo prima.

395
00:21:30,160 --> 00:21:33,692
Solo perché ci sono queste 12 parole davvero oscure non significa che sarebbe 

396
00:21:33,692 --> 00:21:37,360
ancora più sorprendente apprendere che la risposta finale è fascino, per esempio.

397
00:21:38,180 --> 00:21:40,337
Quindi, quando fai effettivamente il calcolo qui, 

398
00:21:40,337 --> 00:21:43,574
e sommi la probabilità di ogni occorrenza moltiplicata per le informazioni 

399
00:21:43,574 --> 00:21:45,560
corrispondenti, quello che ottieni è 2.11 bit.

400
00:21:45,560 --> 00:21:49,530
Dico solo che sono fondamentalmente due bit, fondamentalmente queste quattro possibilità, 

401
00:21:49,530 --> 00:21:53,103
ma c'è un po' più di incertezza a causa di tutti quegli eventi altamente 

402
00:21:53,103 --> 00:21:56,500
improbabili, anche se se li imparassi ne otterresti un sacco di informazioni.

403
00:21:57,160 --> 00:21:59,312
Quindi, rimpicciolendo, questo fa parte di ciò che rende Wordle un 

404
00:21:59,312 --> 00:22:01,400
bell'esempio per una lezione di teoria dell'informazione.

405
00:22:01,600 --> 00:22:04,640
Abbiamo queste due distinte applicazioni di sensazione per l'entropia.

406
00:22:05,160 --> 00:22:08,543
Il primo ci dice quali sono le informazioni attese che otterremo da 

407
00:22:08,543 --> 00:22:11,777
una determinata ipotesi, e il secondo dice che possiamo misurare 

408
00:22:11,777 --> 00:22:15,460
l'incertezza rimanente tra tutte le parole che abbiamo a disposizione.

409
00:22:16,460 --> 00:22:19,074
E dovrei sottolineare, nel primo caso in cui stiamo esaminando le 

410
00:22:19,074 --> 00:22:21,807
informazioni attese di un'ipotesi, una volta che abbiamo un peso 

411
00:22:21,807 --> 00:22:24,540
disuguale per le parole, ciò influisce sul calcolo dell'entropia.

412
00:22:24,980 --> 00:22:27,823
Ad esempio, vorrei richiamare lo stesso caso che stavamo esaminando 

413
00:22:27,823 --> 00:22:30,040
in precedenza della distribuzione associata a Weary, 

414
00:22:30,040 --> 00:22:33,720
ma questa volta utilizzando una distribuzione non uniforme su tutte le parole possibili.

415
00:22:34,500 --> 00:22:38,280
Quindi vediamo se riesco a trovare una parte qui che lo illustri abbastanza bene.

416
00:22:40,940 --> 00:22:42,360
Ok, ecco, questo è abbastanza buono.

417
00:22:42,360 --> 00:22:45,571
Qui abbiamo due schemi adiacenti che sono quasi altrettanto probabili, 

418
00:22:45,571 --> 00:22:49,100
ma ci viene detto che uno di essi ha 32 possibili parole che lo corrispondono.

419
00:22:49,280 --> 00:22:51,951
E se controlliamo cosa sono, queste sono quelle 32, 

420
00:22:51,951 --> 00:22:55,600
che sono tutte parole molto improbabili mentre le guardi con gli occhi.

421
00:22:55,840 --> 00:22:59,041
È difficile trovare risposte che sembrino plausibili, forse urla, 

422
00:22:59,041 --> 00:23:01,855
ma se guardiamo lo schema dei vicini nella distribuzione, 

423
00:23:01,855 --> 00:23:05,251
che è considerato altrettanto probabile, ci viene detto che ha solo 8 

424
00:23:05,251 --> 00:23:09,520
corrispondenze possibili, quindi un quarto di molte partite, ma è altrettanto probabile.

425
00:23:09,860 --> 00:23:12,140
E quando analizziamo quei fiammiferi, possiamo capire il perché.

426
00:23:12,500 --> 00:23:16,300
Alcune di queste sono risposte realmente plausibili, come ring, o ira, o rap.

427
00:23:17,900 --> 00:23:21,527
Per illustrare come incorporiamo tutto ciò, lasciatemi richiamare qui la versione 2 di 

428
00:23:21,527 --> 00:23:25,280
Wordlebot e ci sono due o tre differenze principali rispetto alla prima che abbiamo visto.

429
00:23:25,860 --> 00:23:29,533
Prima di tutto, come ho appena detto, il modo in cui calcoliamo queste entropie, 

430
00:23:29,533 --> 00:23:32,616
questi valori attesi delle informazioni, ora utilizza distribuzioni 

431
00:23:32,616 --> 00:23:35,609
più raffinate attraverso i modelli che incorporano la probabilità 

432
00:23:35,609 --> 00:23:38,240
che una determinata parola sia effettivamente la risposta.

433
00:23:38,879 --> 00:23:41,419
Si dà il caso che le lacrime siano ancora la numero 1, 

434
00:23:41,419 --> 00:23:43,820
anche se quelle che seguono sono un po' diverse.

435
00:23:44,360 --> 00:23:46,728
In secondo luogo, quando classificherà le scelte migliori, 

436
00:23:46,728 --> 00:23:50,141
manterrà un modello della probabilità che ogni parola sia la risposta effettiva e lo 

437
00:23:50,141 --> 00:23:53,554
incorporerà nella sua decisione, il che è più facile da vedere una volta che abbiamo 

438
00:23:53,554 --> 00:23:55,080
alcune ipotesi sulla risposta. tavolo.

439
00:23:55,860 --> 00:23:57,739
Ancora una volta, ignorando la sua raccomandazione perché 

440
00:23:57,739 --> 00:23:59,780
non possiamo lasciare che le macchine governino le nostre vite.

441
00:24:01,140 --> 00:24:04,311
E suppongo che dovrei menzionare un'altra cosa diversa qui a sinistra, 

442
00:24:04,311 --> 00:24:06,383
che il valore di incertezza, quel numero di bit, 

443
00:24:06,383 --> 00:24:09,640
non è più semplicemente ridondante con il numero di possibili corrispondenze.

444
00:24:10,080 --> 00:24:14,700
Ora se lo tiriamo su e calcoliamo 2^8.02, che è leggermente superiore a 256, 

445
00:24:14,700 --> 00:24:19,020
immagino 259, ciò che dice è che anche se ci sono 526 parole totali che 

446
00:24:19,020 --> 00:24:21,840
effettivamente corrispondono a questo modello, 

447
00:24:21,840 --> 00:24:26,399
la quantità di incertezza che ha è più simile a quella che sarebbe se ce ne 

448
00:24:26,399 --> 00:24:28,980
fossero 259 ugualmente probabili risultati.

449
00:24:29,720 --> 00:24:30,740
Puoi pensarla in questo modo.

450
00:24:31,020 --> 00:24:34,481
Sa che borx non è la risposta, lo stesso con yorts, zorl e zorus, 

451
00:24:34,481 --> 00:24:37,680
quindi è un po' meno incerto rispetto al caso precedente.

452
00:24:37,820 --> 00:24:39,280
Questo numero di bit sarà inferiore.

453
00:24:40,220 --> 00:24:43,297
E se continuo a giocare, lo perfezionerò con un paio di 

454
00:24:43,297 --> 00:24:46,540
ipotesi che sono appropriate a ciò che vorrei spiegare qui.

455
00:24:48,360 --> 00:24:50,736
Alla quarta ipotesi, se guardi le sue scelte migliori, 

456
00:24:50,736 --> 00:24:53,760
puoi vedere che non si tratta più solo di massimizzare l'entropia.

457
00:24:54,460 --> 00:24:57,083
Quindi a questo punto ci sono tecnicamente sette possibilità, 

458
00:24:57,083 --> 00:25:00,300
ma le uniche con una possibilità significativa sono i dormitori e le parole.

459
00:25:00,300 --> 00:25:04,123
E si vede che si colloca scegliendo entrambi al di sopra di questi altri valori, 

460
00:25:04,123 --> 00:25:06,720
che a rigor di termini darebbero maggiori informazioni.

461
00:25:07,240 --> 00:25:09,411
La prima volta che l'ho fatto, ho semplicemente sommato 

462
00:25:09,411 --> 00:25:11,547
questi due numeri per misurare la qualità di ogni ipotesi, 

463
00:25:11,547 --> 00:25:13,900
che in realtà ha funzionato meglio di quanto potresti sospettare.

464
00:25:14,300 --> 00:25:16,755
Ma in realtà non mi è sembrato sistematico e sono sicuro che ci siano altri 

465
00:25:16,755 --> 00:25:19,340
approcci che le persone potrebbero adottare, ma ecco quello a cui sono arrivato.

466
00:25:19,760 --> 00:25:22,565
Se consideriamo la prospettiva di un'ipotesi successiva, 

467
00:25:22,565 --> 00:25:26,612
come in questo caso le parole, ciò che ci interessa veramente è il punteggio atteso del 

468
00:25:26,612 --> 00:25:27,900
nostro gioco se lo facciamo.

469
00:25:28,230 --> 00:25:32,011
E per calcolare il punteggio atteso, diciamo qual è la probabilità che 

470
00:25:32,011 --> 00:25:35,900
le parole siano la risposta effettiva, che al momento corrisponde al 58%.

471
00:25:36,040 --> 00:25:39,540
Diciamo che con una probabilità del 58%, il nostro punteggio in questo gioco sarebbe 4.

472
00:25:40,320 --> 00:25:45,640
E poi con la probabilità di 1 meno quel 58%, il nostro punteggio sarà superiore a 4.

473
00:25:46,220 --> 00:25:49,193
Quanto altro non lo sappiamo, ma possiamo stimarlo in base a 

474
00:25:49,193 --> 00:25:52,460
quanta incertezza potrebbe esserci una volta arrivati a quel punto.

475
00:25:52,960 --> 00:25:55,940
Nello specifico, al momento ce n'è 1.44 bit di incertezza.

476
00:25:56,440 --> 00:25:59,371
Se indoviniamo le parole, ci dice che l'informazione 

477
00:25:59,371 --> 00:26:01,120
prevista che otterremo è 1.27 bit.

478
00:26:01,620 --> 00:26:04,510
Quindi, se indoviniamo le parole, questa differenza rappresenta la 

479
00:26:04,510 --> 00:26:07,660
quantità di incertezza che probabilmente ci resterà dopo che ciò accadrà.

480
00:26:08,260 --> 00:26:10,576
Ciò di cui abbiamo bisogno è una sorta di funzione, 

481
00:26:10,576 --> 00:26:13,740
che qui chiamerò f, che associ questa incertezza a un punteggio atteso.

482
00:26:14,240 --> 00:26:18,282
E il modo in cui è stato fatto è stato semplicemente tracciare una serie di dati dei 

483
00:26:18,282 --> 00:26:21,373
giochi precedenti basati sulla versione 1 del bot per dire, ehi, 

484
00:26:21,373 --> 00:26:25,511
qual era il punteggio effettivo dopo vari punti con determinate quantità di incertezza 

485
00:26:25,511 --> 00:26:26,320
molto misurabili.

486
00:26:27,020 --> 00:26:30,926
Ad esempio, questi punti dati qui si trovano sopra un valore intorno a 

487
00:26:30,926 --> 00:26:35,713
8.Si dice circa 7 per alcune partite dopo un punto in cui erano 8.7 bit di incertezza, 

488
00:26:35,713 --> 00:26:38,960
ci sono volute due ipotesi per ottenere la risposta finale.

489
00:26:39,320 --> 00:26:40,739
Per altri giochi sono state necessarie tre ipotesi, 

490
00:26:40,739 --> 00:26:42,240
per altri giochi sono state necessarie quattro ipotesi.

491
00:26:43,140 --> 00:26:46,921
Se qui ci spostiamo a sinistra, tutti i punti sopra lo zero indicano che ogni volta 

492
00:26:46,921 --> 00:26:50,703
che ci sono zero punti di incertezza, vale a dire che c'è solo una possibilità, 

493
00:26:50,703 --> 00:26:54,260
allora il numero di ipotesi richieste è sempre solo una, il che è rassicurante.

494
00:26:54,780 --> 00:26:56,881
Ogni volta che c'era un po' di incertezza, 

495
00:26:56,881 --> 00:26:59,806
il che significava che essenzialmente si riducevano a due possibilità, 

496
00:26:59,806 --> 00:27:03,020
a volte richiedeva un'altra ipotesi, a volte richiedeva altre due ipotesi.

497
00:27:03,080 --> 00:27:05,240
E chi più ne ha più ne metta qui.

498
00:27:05,740 --> 00:27:08,020
Forse un modo leggermente più semplice per visualizzare 

499
00:27:08,020 --> 00:27:10,220
questi dati è raggrupparli insieme e fare delle medie.

500
00:27:11,000 --> 00:27:15,749
Ad esempio, questa barra qui dice che tra tutti i punti in cui abbiamo avuto un po' 

501
00:27:15,749 --> 00:27:19,960
di incertezza, in media il numero di nuove ipotesi richieste era di circa 1.5.

502
00:27:22,140 --> 00:27:26,043
E la barra qui dice che tra tutti i diversi giochi dove ad un certo punto 

503
00:27:26,043 --> 00:27:30,421
l'incertezza era poco più di quattro bit, che è come restringere il campo a 16 

504
00:27:30,421 --> 00:27:34,852
diverse possibilità, quindi in media richiede poco più di due ipotesi da quel punto 

505
00:27:34,852 --> 00:27:35,380
inoltrare.

506
00:27:36,060 --> 00:27:37,793
E da qui ho semplicemente fatto una regressione per 

507
00:27:37,793 --> 00:27:39,460
adattare una funzione che mi sembrava ragionevole.

508
00:27:39,980 --> 00:27:44,156
E ricorda che il punto centrale di tutto ciò è che possiamo quantificare questa 

509
00:27:44,156 --> 00:27:47,132
intuizione che più informazioni otteniamo da una parola, 

510
00:27:47,132 --> 00:27:48,960
più basso sarà il punteggio atteso.

511
00:27:49,680 --> 00:27:54,460
Quindi con questo come versione 2.0, se torniamo indietro ed eseguiamo la stessa serie di 

512
00:27:54,460 --> 00:27:59,240
simulazioni, facendola giocare contro tutte le 2315 possibili risposte di parole, come va?

513
00:28:00,280 --> 00:28:02,669
Beh, a differenza della nostra prima versione è decisamente migliore, 

514
00:28:02,669 --> 00:28:03,420
il che è rassicurante.

515
00:28:04,020 --> 00:28:08,094
Tutto sommato la media è intorno a 3.6, anche se a differenza della prima versione 

516
00:28:08,094 --> 00:28:12,120
ci sono un paio di volte che perde e ne richiede più di sei in questa circostanza.

517
00:28:12,639 --> 00:28:15,274
Presumibilmente perché ci sono momenti in cui è necessario fare quel compromesso per 

518
00:28:15,274 --> 00:28:17,940
raggiungere effettivamente l'obiettivo piuttosto che massimizzare le informazioni.

519
00:28:19,040 --> 00:28:21,000
Quindi possiamo fare meglio di 3.6?

520
00:28:22,080 --> 00:28:22,920
Possiamo sicuramente.

521
00:28:23,280 --> 00:28:26,262
Ora, all'inizio ho detto che è molto divertente provare a non incorporare 

522
00:28:26,262 --> 00:28:29,360
la vera lista delle risposte di Wordle nel modo in cui costruisce il suo modello.

523
00:28:29,880 --> 00:28:34,180
Ma se lo incorporiamo, la prestazione migliore che potrei ottenere è stata di circa 3.43.

524
00:28:35,160 --> 00:28:37,643
Quindi, se proviamo a diventare più sofisticati rispetto al semplice 

525
00:28:37,643 --> 00:28:40,090
utilizzo dei dati sulla frequenza delle parole per scegliere questa 

526
00:28:40,090 --> 00:28:42,753
distribuzione a priori, questo 3.43 probabilmente dà un massimo di quanto 

527
00:28:42,753 --> 00:28:45,740
bene potremmo ottenere con quello, o almeno quanto bene potrei ottenere con quello.

528
00:28:46,240 --> 00:28:49,245
Quella prestazione migliore essenzialmente utilizza semplicemente 

529
00:28:49,245 --> 00:28:51,750
le idee di cui ho parlato qui, ma va un po' oltre, 

530
00:28:51,750 --> 00:28:55,120
come se cercasse le informazioni attese due passi avanti anziché solo uno.

531
00:28:55,620 --> 00:28:57,323
Inizialmente avevo intenzione di parlarne di più, 

532
00:28:57,323 --> 00:29:00,220
ma mi rendo conto che in realtà siamo andati avanti piuttosto a lungo così com'è.

533
00:29:00,580 --> 00:29:03,317
L'unica cosa che dirò è che dopo aver effettuato questa ricerca in 

534
00:29:03,317 --> 00:29:06,671
due passaggi e aver eseguito un paio di simulazioni di esempio sui migliori candidati, 

535
00:29:06,671 --> 00:29:09,100
finora almeno per me sembra che Crane sia il miglior apripista.

536
00:29:09,100 --> 00:29:10,060
Chi l'avrebbe mai detto?

537
00:29:10,920 --> 00:29:14,251
Inoltre, se usi l'elenco delle parole vere per determinare il tuo 

538
00:29:14,251 --> 00:29:17,820
spazio di possibilità, l'incertezza con cui inizi è poco più di 11 bit.

539
00:29:18,300 --> 00:29:21,244
E si scopre che, solo da una ricerca con forza bruta, 

540
00:29:21,244 --> 00:29:25,880
la massima informazione possibile attesa dopo le prime due ipotesi è di circa 10 bit.

541
00:29:26,500 --> 00:29:30,504
Il che suggerisce che, nella migliore delle ipotesi, dopo le prime due ipotesi, 

542
00:29:30,504 --> 00:29:34,560
con un gioco perfettamente ottimale, rimarrai con circa un po' di incertezza.

543
00:29:34,800 --> 00:29:37,960
Il che equivale ad avere solo due possibili ipotesi.

544
00:29:37,960 --> 00:29:40,989
Quindi penso che sia giusto e probabilmente piuttosto prudente dire che 

545
00:29:40,989 --> 00:29:44,019
non potresti mai scrivere un algoritmo che porti questa media fino a 3, 

546
00:29:44,019 --> 00:29:47,132
perché con le parole a tua disposizione, semplicemente non c'è spazio 

547
00:29:47,132 --> 00:29:50,162
per ottenere informazioni sufficienti dopo solo due passaggi per essere 

548
00:29:50,162 --> 00:29:53,360
in grado di garantire la risposta nella terza fascia ogni volta senza fallo.

